{
    "hands_on_practices": [
        {
            "introduction": "In Monte Carlo simulations, not all weighted event samples are statistically equal; a sample with highly variable weights possesses less statistical power than one with uniform weights, even for the same event count. This exercise introduces the concept of effective sample size, $N_{\\text{eff}}$, a crucial metric for quantifying this loss of precision . By deriving its formula and applying it to a concrete example, you will develop a practical understanding of how weight variance impacts the statistical quality of simulation results.",
            "id": "3513799",
            "problem": "In computational high-energy physics, event generators often produce simulated events with positive weights $w_i>0$ that reflect differential cross-section factors or importance sampling adjustments. Consider estimating a scalar observable using $N$ independent event-level contributions $Y_i$ that have a common finite variance $\\sigma^2$ under the target physics distribution. A standard self-normalized weighted estimator is constructed as $\\hat{\\mu}=\\sum_{i=1}^{N}\\alpha_i Y_i$ with normalized coefficients $\\alpha_i=w_i/\\sum_{j=1}^{N}w_j$. Using the laws of variance for linear combinations of independent random variables and the normalization constraint on $\\{\\alpha_i\\}$, define the notion of an effective sample size $N_{\\text{eff}}$ by matching the variance of $\\hat{\\mu}$ to that of an unweighted sample mean. Evaluate $N_{\\text{eff}}$ numerically for the weights $\\{2,1,1,0.5\\}$, and interpret $N_{\\text{eff}}$ in terms of statistical power.\n\nSelect all statements that are correct:\n\nA. For independent event contributions with common variance $\\sigma^2$, the variance of the self-normalized weighted estimator satisfies $\\operatorname{Var}(\\hat{\\mu})=\\sigma^2\\sum_{i=1}^{N}\\alpha_i^2$, and equating $\\operatorname{Var}(\\hat{\\mu})=\\sigma^2/N_{\\text{eff}}$ yields $N_{\\text{eff}}=\\left(\\sum_{i=1}^{N}w_i\\right)^2\\Big/\\left(\\sum_{i=1}^{N}w_i^2\\right)$.\n\nB. For any choice of nonnegative weights, one always has $N_{\\text{eff}}\\geq N$.\n\nC. The quantity $N_{\\text{eff}}$ equals the number of strictly positive weights.\n\nD. One has $N_{\\text{eff}}=N$ if and only if all weights are identical up to a common scaling, that is $w_i=c$ for some constant $c>0$ and all $i$.\n\nE. For the weights $\\{2,1,1,0.5\\}$, $N_{\\text{eff}}$ is $3.24$, so $\\operatorname{Var}(\\hat{\\mu})=\\sigma^2/3.24$, which matches the variance of an unweighted mean computed from $3.24$ independent, equal-weight events in the sense of statistical power.\n\nF. In acceptance-rejection unweighting with acceptance probability $\\min\\{1,w_i/w_{\\max}\\}$ for a normalization constant $w_{\\max}=\\max_i w_i$, the expected number of accepted events equals $N_{\\text{eff}}$.",
            "solution": "The problem statement poses a well-defined question in the context of statistical data analysis for computational physics. It asks for the derivation and interpretation of the effective sample size, $N_{\\text{eff}}$, for a weighted sample. All provided information is scientifically sound, self-contained, and sufficient for a rigorous solution. The problem is therefore valid.\n\nThe core task is to define the effective sample size $N_{\\text{eff}}$ by comparing the variance of a weighted mean to that of an unweighted mean.\n\nLet the set of $N$ weights be $\\{w_1, w_2, \\ldots, w_N\\}$, with $w_i > 0$. The weighted estimator for a scalar observable $\\mu$ is given by:\n$$ \\hat{\\mu} = \\sum_{i=1}^{N} \\alpha_i Y_i $$\nwhere $Y_i$ are independent contributions with a common variance $\\operatorname{Var}(Y_i) = \\sigma^2$, and the coefficients $\\alpha_i$ are normalized weights:\n$$ \\alpha_i = \\frac{w_i}{\\sum_{j=1}^{N} w_j} $$\nThese coefficients satisfy the normalization condition $\\sum_{i=1}^{N} \\alpha_i = 1$.\n\nThe variance of the estimator $\\hat{\\mu}$ is calculated using the property of variance for a linear combination of independent random variables, $\\operatorname{Var}\\left(\\sum_i c_i X_i\\right) = \\sum_i c_i^2 \\operatorname{Var}(X_i)$. In our case, the coefficients are the $\\alpha_i$, which are treated as constants for this calculation.\n$$ \\operatorname{Var}(\\hat{\\mu}) = \\operatorname{Var}\\left(\\sum_{i=1}^{N} \\alpha_i Y_i\\right) = \\sum_{i=1}^{N} \\alpha_i^2 \\operatorname{Var}(Y_i) = \\sum_{i=1}^{N} \\alpha_i^2 \\sigma^2 = \\sigma^2 \\sum_{i=1}^{N} \\alpha_i^2 $$\n\nAn unweighted mean of $M$ independent events, $\\bar{Y} = \\frac{1}{M}\\sum_{i=1}^{M} Y_i$, has a variance:\n$$ \\operatorname{Var}(\\bar{Y}) = \\operatorname{Var}\\left(\\frac{1}{M}\\sum_{i=1}^{M} Y_i\\right) = \\frac{1}{M^2} \\sum_{i=1}^{M} \\operatorname{Var}(Y_i) = \\frac{1}{M^2} (M \\sigma^2) = \\frac{\\sigma^2}{M} $$\n\nThe effective sample size, $N_{\\text{eff}}$, is defined by equating the variance of the weighted estimator $\\hat{\\mu}$ to the variance of an unweighted mean of $N_{\\text{eff}}$ events:\n$$ \\operatorname{Var}(\\hat{\\mu}) = \\frac{\\sigma^2}{N_{\\text{eff}}} $$\nSubstituting our expression for $\\operatorname{Var}(\\hat{\\mu})$:\n$$ \\sigma^2 \\sum_{i=1}^{N} \\alpha_i^2 = \\frac{\\sigma^2}{N_{\\text{eff}}} $$\n$$ N_{\\text{eff}} = \\frac{1}{\\sum_{i=1}^{N} \\alpha_i^2} $$\nNow, we substitute the definition of $\\alpha_i$ in terms of the original weights $w_i$. Let $W = \\sum_{j=1}^{N} w_j$.\n$$ N_{\\text{eff}} = \\frac{1}{\\sum_{i=1}^{N} \\left(\\frac{w_i}{W}\\right)^2} = \\frac{1}{\\frac{1}{W^2} \\sum_{i=1}^{N} w_i^2} = \\frac{W^2}{\\sum_{i=1}^{N} w_i^2} $$\nThus, the formula for the effective sample size is:\n$$ N_{\\text{eff}} = \\frac{\\left(\\sum_{i=1}^{N} w_i\\right)^2}{\\sum_{i=1}^{N} w_i^2} $$\n\nNow, we evaluate each option.\n\n**A. For independent event contributions with common variance $\\sigma^2$, the variance of the self-normalized weighted estimator satisfies $\\operatorname{Var}(\\hat{\\mu})=\\sigma^2\\sum_{i=1}^{N}\\alpha_i^2$, and equating $\\operatorname{Var}(\\hat{\\mu})=\\sigma^2/N_{\\text{eff}}$ yields $N_{\\text{eff}}=\\left(\\sum_{i=1}^{N}w_i\\right)^2\\Big/\\left(\\sum_{i=1}^{N}w_i^2\\right)$.**\nOur derivation above confirms both parts of this statement. The variance calculation is correct, and the resulting expression for $N_{\\text{eff}}$ is also correct.\nVerdict: **Correct**.\n\n**B. For any choice of nonnegative weights, one always has $N_{\\text{eff}}\\geq N$.**\nTo evaluate this, we compare $N_{\\text{eff}} = \\frac{(\\sum w_i)^2}{\\sum w_i^2}$ with $N$.\nConsider the Cauchy-Schwarz inequality for two vectors $u = (w_1, \\ldots, w_N)$ and $v = (1, \\ldots, 1)$ in $\\mathbb{R}^N$:\n$$ \\left(\\sum_{i=1}^{N} u_i v_i\\right)^2 \\leq \\left(\\sum_{i=1}^{N} u_i^2\\right) \\left(\\sum_{i=1}^{N} v_i^2\\right) $$\n$$ \\left(\\sum_{i=1}^{N} w_i \\cdot 1\\right)^2 \\leq \\left(\\sum_{i=1}^{N} w_i^2\\right) \\left(\\sum_{i=1}^{N} 1^2\\right) $$\n$$ \\left(\\sum_{i=1}^{N} w_i\\right)^2 \\leq \\left(\\sum_{i=1}^{N} w_i^2\\right) \\cdot N $$\nSince weights are positive, $\\sum w_i^2 > 0$, so we can divide by it without changing the inequality's direction:\n$$ \\frac{\\left(\\sum_{i=1}^{N} w_i\\right)^2}{\\sum_{i=1}^{N} w_i^2} \\leq N $$\nThis means $N_{\\text{eff}} \\leq N$. The statement claims $N_{\\text{eff}} \\geq N$, which is the opposite. Therefore, the statement is false. The inequality of weights reduces the effective sample size.\nVerdict: **Incorrect**.\n\n**C. The quantity $N_{\\text{eff}}$ equals the number of strictly positive weights.**\nThe problem provides a specific set of weights to test: $w = \\{2, 1, 1, 0.5\\}$. Here, the number of events is $N=4$, and all weights are strictly positive.\nLet's compute $N_{\\text{eff}}$ for this set:\n$$ \\sum_{i=1}^{4} w_i = 2 + 1 + 1 + 0.5 = 4.5 $$\n$$ \\sum_{i=1}^{4} w_i^2 = 2^2 + 1^2 + 1^2 + 0.5^2 = 4 + 1 + 1 + 0.25 = 6.25 $$\n$$ N_{\\text{eff}} = \\frac{(4.5)^2}{6.25} = \\frac{20.25}{6.25} = 3.24 $$\nThe number of strictly positive weights is $4$, but $N_{\\text{eff}} = 3.24$. Thus, $3.24 \\neq 4$. The statement is false.\nVerdict: **Incorrect**.\n\n**D. One has $N_{\\text{eff}}=N$ if and only if all weights are identical up to a common scaling, that is $w_i=c$ for some constant $c>0$ and all $i$.**\nThe phrase \"identical up to a common scaling\" is clarified by \"that is $w_i=c$ for some constant $c>0$\". This means we must test the condition $w_1 = w_2 = \\ldots = w_N = c > 0$.\nThe equality in the Cauchy-Schwarz inequality used in option B holds if and only if the vectors $u$ and $v$ are linearly dependent, i.e., $u = k v$ for some scalar $k$.\nIn our case, $u = (w_1, \\ldots, w_N)$ and $v=(1, \\ldots, 1)$. Linear dependence means $w_i = k \\cdot 1$ for all $i$. Since weights are positive, $k$ must be a positive constant, let's call it $c$. Thus, $N_{\\text{eff}} = N$ if and only if $w_i = c$ for all $i$.\nThe statement is correct. It precisely describes the condition for maximum effective sample size.\nVerdict: **Correct**.\n\n**E. For the weights $\\{2,1,1,0.5\\}$, $N_{\\text{eff}}$ is $3.24$, so $\\operatorname{Var}(\\hat{\\mu})=\\sigma^2/3.24$, which matches the variance of an unweighted mean computed from $3.24$ independent, equal-weight events in the sense of statistical power.**\nThe calculation in option C shows that $N_{\\text{eff}} = 3.24$ is correct for the given weights.\nBy the definition of $N_{\\text{eff}}$, we have $\\operatorname{Var}(\\hat{\\mu}) = \\sigma^2 / N_{\\text{eff}}$, which is $\\sigma^2 / 3.24$. This is correct.\nThe variance of an unweighted mean of $M$ events is $\\sigma^2/M$. For a hypothetical sample of $M=3.24$ events, the variance would indeed be $\\sigma^2/3.24$.\nStatistical power, for tests concerning the mean, is a function of the standard error of the estimator, $\\sqrt{\\operatorname{Var}(\\hat{\\mu})}$. Since the variance of the weighted mean is mathematically equivalent to the variance of an unweighted mean of $3.24$ events, the statistical power is identical. The interpretation is sound and represents the fundamental purpose of the $N_{\\text{eff}}$ concept.\nVerdict: **Correct**.\n\n**F. In acceptance-rejection unweighting with acceptance probability $\\min\\{1,w_i/w_{\\max}\\}$ for a normalization constant $w_{\\max}=\\max_i w_i$, the expected number of accepted events equals $N_{\\text{eff}}$.**\nThe acceptance probability for the $i$-th event is $p_i = w_i/w_{\\max}$ (since $w_i \\le w_{\\max}$, the $\\min$ is redundant). The process of accepting or rejecting each event is a Bernoulli trial. The expected number of accepted events, $E[N_{\\text{acc}}]$, is the sum of the individual acceptance probabilities over all $N$ initial events:\n$$ E[N_{\\text{acc}}] = \\sum_{i=1}^{N} p_i = \\sum_{i=1}^{N} \\frac{w_i}{w_{\\max}} = \\frac{1}{w_{\\max}} \\sum_{i=1}^{N} w_i $$\nThe statement claims $E[N_{\\text{acc}}] = N_{\\text{eff}}$. Let's test this with the given weights $w = \\{2, 1, 1, 0.5\\}$.\nWe have $w_{\\max} = \\max\\{2, 1, 1, 0.5\\} = 2$ and $\\sum w_i = 4.5$.\n$$ E[N_{\\text{acc}}] = \\frac{4.5}{2} = 2.25 $$\nFrom option C, we know $N_{\\text{eff}} = 3.24$.\nSince $2.25 \\neq 3.24$, the statement is false. These are two distinct quantities that describe different properties of the weighted sample. They are only equal in the trivial case where all weights are identical.\nVerdict: **Incorrect**.",
            "answer": "$$\\boxed{ADE}$$"
        },
        {
            "introduction": "Building on the understanding of how weight variance degrades statistical power, a primary goal in event generation is to reduce this variance. This practice explores the powerful method of control variates, where analytical knowledge of a process, such as its soft and collinear limits, is leveraged to construct a new, less volatile estimator . Through this exercise, you will derive the optimal control variate coefficient that minimizes variance and analyze how implementation inaccuracies can introduce a systematic bias, highlighting the interplay between theoretical optimization and practical application.",
            "id": "3513755",
            "problem": "Consider Monte Carlo (MC) event weighting in computational high-energy physics for a simplified final-state radiation problem. Let the phase-space point be denoted by $\\Phi = (x,y)$ with $x \\in [0,1]$ and $y \\in [0,1]$, sampled from a uniform proposal density (that is, $p(\\Phi)=1$ on the unit square). The event weight $w(\\Phi)$ approximating a Quantum Chromodynamics (QCD) real-emission matrix element contains soft/collinear structure. Suppose that, for constants $A>0$, $B>0$, a smooth remainder coefficient $q \\ge 0$, and an infrared regulator $\\delta>0$, the event weight is modeled by\n$$\nw(\\Phi) \\equiv w(x,y) = \\frac{A}{x+\\delta}+\\frac{B}{y+\\delta} + q\\,x\\,y.\n$$\nA control variate $c(\\Phi)$ is built from the analytic soft/collinear limits, using the leading singular terms and subtracting a constant to enforce zero mean under the uniform phase-space measure. Define\n$$\nc_{\\text{true}}(\\Phi) \\equiv \\frac{A}{x+\\delta}+\\frac{B}{y+\\delta}-K,\n$$\nwhere the constant $K$ equals the expectation of the singular part,\n$$\nK = \\mathbb{E}\\!\\left[\\frac{A}{x+\\delta}+\\frac{B}{y+\\delta}\\right], \n$$\nso that $\\mathbb{E}[c_{\\text{true}}]=0$. In practice, truncation of the analytic expansion or numerical approximations may shift the subtracted constant and induce a nonzero mean. Model this truncation as a constant offset $\\tau \\in \\mathbb{R}$, and let the implementable control be\n$$\nc(\\Phi) \\equiv c_{\\text{true}}(\\Phi) + \\tau.\n$$\nFor any scalar $\\lambda \\in \\mathbb{R}$, define the adjusted weight\n$$\nw'(\\Phi) = w(\\Phi) - \\lambda\\, c(\\Phi).\n$$\nYour tasks are as follows.\n\n- From first principles of variance minimization and control variates in MC estimation, derive the unique value $\\lambda^\\star$ that minimizes the variance of $w'(\\Phi)$ over the uniform distribution on the unit square, in terms of moments of $w$ and $c$.\n- Using the specific functional forms given above, evaluate all needed expectations and covariances exactly by performing the corresponding one-dimensional integrals over $x$ and $y$. Express $\\lambda^\\star$ in closed form as a function of $A$, $B$, $q$, and $\\delta$.\n- Determine the residual bias of the adjusted-weight estimator that would arise if the mean of $c(\\Phi)$ is offset by $\\tau$ (that is, if $\\mathbb{E}[c]=\\tau$ and the estimator uses the sample mean of $w'$ without any explicit mean correction). Express this bias in closed form in terms of $\\lambda^\\star$ and $\\tau$.\n- Specify the correct boundary behavior when the control variate is constant (zero variance), in which case the variance-minimizing $\\lambda^\\star$ is not identifiable from variance minimization alone. Provide a convention that avoids introducing bias.\n\nImplement a program that, for each parameter tuple $(A,B,q,\\delta,\\tau)$ in the test suite below, returns the pair $[\\lambda^\\star,\\text{bias}]$, where $\\text{bias}$ is the residual bias of the mean estimator based on $w'(\\Phi)$ as described above. Your implementation must use the exact analytic integrals implied by the definitions, not numerical sampling.\n\nUse the following test suite of parameter values, covering a general case, a truncation-induced bias case, a weak-correlation limit, and a boundary case:\n- Test $1$: $(A,B,q,\\delta,\\tau) = (\\,2,\\,1,\\,0.5,\\,10^{-3},\\,0\\,)$.\n- Test $2$: $(A,B,q,\\delta,\\tau) = (\\,2,\\,1,\\,0.5,\\,10^{-3},\\,5\\times 10^{-2}\\,)$.\n- Test $3$: $(A,B,q,\\delta,\\tau) = (\\,2,\\,1,\\,50,\\,10^{-3},\\,5\\times 10^{-2}\\,)$.\n- Test $4$: $(A,B,q,\\delta,\\tau) = (\\,0,\\,0,\\,1,\\,10^{-3},\\,5\\times 10^{-1}\\,)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-element list $[\\lambda^\\star,\\text{bias}]$ for the corresponding test case, with each floating-point number rounded to six decimal places. For example, an output with two test cases should look like\n$[[\\lambda^\\star_1,\\text{bias}_1],[\\lambda^\\star_2,\\text{bias}_2]]$.",
            "solution": "The problem is valid as it is scientifically grounded in the principles of Monte Carlo variance reduction, is mathematically well-posed, and is formulated objectively. We proceed with the solution.\n\nThe solution is organized into four parts as requested: first, the derivation of the optimal control variate coefficient $\\lambda^\\star$ from first principles; second, the evaluation of this coefficient for the specific functional forms provided; third, the derivation of the residual bias; and fourth, the treatment of the boundary case where the control variate has zero variance.\n\n### Part 1: Derivation of the Optimal Coefficient $\\lambda^\\star$\n\nLet $w$ and $c$ be two random variables representing the event weight and the control variate, respectively, defined on the phase space $\\Phi=(x,y)$ with a uniform probability measure. We wish to find the scalar coefficient $\\lambda$ that minimizes the variance of the adjusted weight $w' = w - \\lambda c$.\n\nThe variance of $w'$ is given by:\n$$\n\\text{Var}[w'] = \\text{Var}[w - \\lambda c] = \\text{Var}[w] + \\text{Var}[-\\lambda c] + 2\\text{Cov}[w, -\\lambda c]\n$$\nUsing the properties of variance and covariance, this expands to:\n$$\n\\text{Var}[w'] = \\text{Var}[w] + \\lambda^2 \\text{Var}[c] - 2\\lambda \\text{Cov}[w, c]\n$$\nThis expression is a quadratic function of $\\lambda$. To find the value $\\lambda^\\star$ that minimizes this function, we take the derivative with respect to $\\lambda$ and set it to zero:\n$$\n\\frac{d}{d\\lambda} \\text{Var}[w'] = 2\\lambda \\text{Var}[c] - 2\\text{Cov}[w, c] = 0\n$$\nSolving for $\\lambda$ yields the optimal coefficient $\\lambda^\\star$, provided that $\\text{Var}[c] \\neq 0$:\n$$\n\\lambda^\\star = \\frac{\\text{Cov}[w, c]}{\\text{Var}[c]}\n$$\nThe covariance is defined as $\\text{Cov}[w, c] = \\mathbb{E}[wc] - \\mathbb{E}[w]\\mathbb{E}[c]$, and the variance is $\\text{Var}[c] = \\mathbb{E}[c^2] - (\\mathbb{E}[c])^2$.\n\n### Part 2: Evaluation of $\\lambda^\\star$ for the Given Forms\n\nTo evaluate $\\lambda^\\star$ in closed form, we must compute the necessary expectations, variances, and covariances. The expectation of a function $f(x,y)$ over the unit square is $\\mathbb{E}[f] = \\int_0^1 \\int_0^1 f(x,y) \\,dx\\,dy$. Due to the uniform measure and the separability of many terms, we can exploit the independence of $x$ and $y$.\n\nLet us define the following elementary integrals, which will serve as building blocks:\n- $\\mathbb{E}\\left[\\frac{1}{z+\\delta}\\right] = \\int_0^1 \\frac{1}{z+\\delta} dz = [\\ln(z+\\delta)]_0^1 = \\ln(1+\\delta) - \\ln(\\delta) = \\ln\\left(\\frac{1+\\delta}{\\delta}\\right)$. Let's denote this by $h_1(\\delta)$.\n- $\\mathbb{E}\\left[\\frac{1}{(z+\\delta)^2}\\right] = \\int_0^1 \\frac{1}{(z+\\delta)^2} dz = \\left[-\\frac{1}{z+\\delta}\\right]_0^1 = \\frac{1}{\\delta} - \\frac{1}{1+\\delta} = \\frac{1}{\\delta(1+\\delta)}$. Let's denote this by $h_2(\\delta)$.\n- $\\mathbb{E}[z] = \\int_0^1 z \\,dz = 1/2$.\n- $\\mathbb{E}[z^2] = \\int_0^1 z^2 \\,dz = 1/3$.\n\nThe weight and control variates are given by:\n$w(x,y) = \\frac{A}{x+\\delta}+\\frac{B}{y+\\delta} + qxy$\n$c(x,y) = \\frac{A}{x+\\delta}+\\frac{B}{y+\\delta} - K + \\tau$, where $K = \\mathbb{E}\\left[\\frac{A}{x+\\delta}+\\frac{B}{y+\\delta}\\right]$.\n\nUsing our notation, $K = A\\,\\mathbb{E}[\\frac{1}{x+\\delta}] + B\\,\\mathbb{E}[\\frac{1}{y+\\delta}] = (A+B)h_1(\\delta)$.\nLet's define the singular part $s(x,y) = \\frac{A}{x+\\delta}+\\frac{B}{y+\\delta}$. Then $w = s + qxy$ and $c = s - K + \\tau$.\n\nFirst, we compute the variance of the control variate, $\\text{Var}[c]$. Since $K$ and $\\tau$ are constants, $\\text{Var}[c] = \\text{Var}[s - K + \\tau] = \\text{Var}[s]$.\n$$\n\\text{Var}[s] = \\text{Var}\\left[\\frac{A}{x+\\delta}+\\frac{B}{y+\\delta}\\right]\n$$\nSince $x$ and $y$ are independent, the variance of the sum is the sum of variances:\n$$\n\\text{Var}[s] = \\text{Var}\\left[\\frac{A}{x+\\delta}\\right] + \\text{Var}\\left[\\frac{B}{y+\\delta}\\right] = A^2 \\text{Var}\\left[\\frac{1}{x+\\delta}\\right] + B^2 \\text{Var}\\left[\\frac{1}{y+\\delta}\\right]\n$$\nThe variance of $1/(z+\\delta)$ is $\\mathbb{E}[1/(z+\\delta)^2] - (\\mathbb{E}[1/(z+\\delta)])^2 = h_2(\\delta) - (h_1(\\delta))^2$. By symmetry, this is the same for $x$ and $y$.\n$$\n\\text{Var}[c] = (A^2+B^2)(h_2(\\delta) - (h_1(\\delta))^2)\n$$\nNext, we compute the covariance $\\text{Cov}[w, c]$:\n$$\n\\text{Cov}[w, c] = \\text{Cov}[s + qxy, s - K + \\tau] = \\text{Cov}[s+qxy, s]\n$$\nUsing the bilinearity of covariance:\n$$\n\\text{Cov}[w, c] = \\text{Cov}[s,s] + \\text{Cov}[qxy, s] = \\text{Var}[s] + q\\text{Cov}[xy, s]\n$$\nWe already have $\\text{Var}[s] = \\text{Var}[c]$. We need to compute $\\text{Cov}[xy, s]$:\n$$\n\\text{Cov}[xy, s] = \\mathbb{E}[xys] - \\mathbb{E}[xy]\\mathbb{E}[s]\n$$\n$\\mathbb{E}[xy] = \\mathbb{E}[x]\\mathbb{E}[y] = (1/2)(1/2) = 1/4$. And $\\mathbb{E}[s] = K = (A+B)h_1(\\delta)$.\n$\\mathbb{E}[xys] = \\mathbb{E}\\left[xy\\left(\\frac{A}{x+\\delta}+\\frac{B}{y+\\delta}\\right)\\right] = A\\mathbb{E}\\left[\\frac{xy}{x+\\delta}\\right] + B\\mathbb{E}\\left[\\frac{xy}{y+\\delta}\\right]$.\nUsing independence:\n$\\mathbb{E}\\left[\\frac{xy}{x+\\delta}\\right] = \\mathbb{E}\\left[\\frac{x}{x+\\delta}\\right]\\mathbb{E}[y]$.\n$\\mathbb{E}\\left[\\frac{x}{x+\\delta}\\right] = \\int_0^1 \\frac{x}{x+\\delta} dx = \\int_0^1 \\frac{x+\\delta-\\delta}{x+\\delta} dx = \\int_0^1 \\left(1-\\frac{\\delta}{x+\\delta}\\right) dx = 1 - \\delta h_1(\\delta)$.\nSo, $\\mathbb{E}\\left[\\frac{xy}{x+\\delta}\\right] = \\frac{1}{2}(1-\\delta h_1(\\delta))$. By symmetry, $\\mathbb{E}\\left[\\frac{xy}{y+\\delta}\\right] = \\frac{1}{2}(1-\\delta h_1(\\delta))$.\nThis gives $\\mathbb{E}[xys] = \\frac{A+B}{2}(1-\\delta h_1(\\delta))$.\nNow, substitute back:\n$\\text{Cov}[xy, s] = \\frac{A+B}{2}(1-\\delta h_1(\\delta)) - \\frac{1}{4}(A+B)h_1(\\delta) = \\frac{A+B}{4}[2(1-\\delta h_1(\\delta)) - h_1(\\delta)] = \\frac{A+B}{4}[2 - (2\\delta+1)h_1(\\delta)]$.\n\nSo, the full covariance is:\n$$\n\\text{Cov}[w,c] = \\text{Var}[c] + q \\frac{A+B}{4}[2 - (2\\delta+1)h_1(\\delta)]\n$$\nFinally, we can write $\\lambda^\\star$:\n$$\n\\lambda^\\star = \\frac{\\text{Cov}[w, c]}{\\text{Var}[c]} = 1 + \\frac{q(A+B)[2 - (2\\delta+1)h_1(\\delta)]}{4(A^2+B^2)(h_2(\\delta) - (h_1(\\delta))^2)}\n$$\n\n### Part 3: Residual Bias\n\nThe bias of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is $\\text{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$. Here, we are estimating $\\mathbb{E}[w]$ using the sample mean of the adjusted weights, $\\overline{w'}$. The expectation of this estimator is:\n$$\n\\mathbb{E}[\\overline{w'}] = \\mathbb{E}[w'] = \\mathbb{E}[w - \\lambda^\\star c] = \\mathbb{E}[w] - \\lambda^\\star \\mathbb{E}[c]\n$$\nThe problem states that the implemented control variate $c$ has a mean offset by $\\tau$, so $\\mathbb{E}[c] = \\mathbb{E}[c_{\\text{true}} + \\tau] = 0 + \\tau = \\tau$.\nThe bias is therefore:\n$$\n\\text{bias} = \\mathbb{E}[\\overline{w'}] - \\mathbb{E}[w] = (\\mathbb{E}[w] - \\lambda^\\star \\tau) - \\mathbb{E}[w] = -\\lambda^\\star \\tau\n$$\nThis is a general result: any non-zero mean in a control variate, when unaccounted for, introduces a bias equal to the negative product of the optimal coefficient and the control variate's mean.\n\n### Part 4: Boundary Case with Zero Variance\n\nThe formula for $\\lambda^\\star$ involves division by $\\text{Var}[c]$. We must consider the case where $\\text{Var}[c]=0$.\nFrom its formula, $\\text{Var}[c] = (A^2+B^2)(h_2(\\delta) - (h_1(\\delta))^2)$. Since $h_2 - h_1^2 = \\text{Var}[1/(z+\\delta)] > 0$ for $\\delta > 0$ by Jensen's inequality, $\\text{Var}[c]$ is zero if and only if $A^2+B^2=0$, which implies $A=0$ and $B=0$.\n\nIf $A=0$ and $B=0$, the singular part $s(x,y)$ is identically zero, so $K=0$ and the true control variate $c_{\\text{true}}$ is zero. The implemented control variate becomes $c(\\Phi) = \\tau$, a constant. The variance of a constant is zero.\nIn this case, the expression for $\\lambda^\\star$ becomes an indeterminate form $0/0$, since $\\text{Cov}[w,c] = \\text{Cov}[qxy, \\tau] = 0$.\nThe variance minimization objective becomes $\\text{Var}[w'] = \\text{Var}[w - \\lambda \\tau] = \\text{Var}[w]$, which is independent of $\\lambda$. Any value of $\\lambda$ is a minimizer.\nHowever, a choice of $\\lambda$ has implications for the bias, which is $-\\lambda \\tau$. To ensure the estimator remains unbiased, we must have $-\\lambda \\tau = 0$. If $\\tau \\neq 0$, this forces the choice $\\lambda=0$. If $\\tau=0$, any $\\lambda$ is fine, but $\\lambda=0$ is the most natural and simplest choice.\nTherefore, the robust convention for the boundary case where $\\text{Var}[c]=0$ is to set $\\lambda^\\star=0$. This is physically sound: if a control variate does not vary, it is uncorrelated with the weight and provides no information for variance reduction and shouldn't be used. This choice guarantees zero bias from the control variate.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the control variate problem for the given test cases.\n    \"\"\"\n    \n    # Test suite of parameter tuples (A, B, q, delta, tau).\n    test_cases = [\n        # Test 1: General case\n        (2.0, 1.0, 0.5, 1e-3, 0.0),\n        # Test 2: General case with truncation bias\n        (2.0, 1.0, 0.5, 1e-3, 5e-2),\n        # Test 3: Weak-correlation limit (large q)\n        (2.0, 1.0, 50.0, 1e-3, 5e-2),\n        # Test 4: Boundary case (constant control variate)\n        (0.0, 0.0, 1.0, 1e-3, 5e-1)\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        A, B, q, delta, tau = case\n        \n        # Part 4: Handle the boundary case where the control variate is constant.\n        # This occurs if and only if A=0 and B=0, which makes Var(c)=0.\n        # In this case, we set lambda_star = 0 to avoid introducing bias.\n        if A == 0.0 and B == 0.0:\n            lambda_star = 0.0\n            # Bias is -lambda_star * tau, which is 0.\n            bias = 0.0\n            results.append([lambda_star, bias])\n            continue\n            \n        # Part 2: Evaluate lambda_star in closed form for the general case.\n        \n        # Calculate the elementary integrals/expectations.\n        # h1 = E[1/(z+delta)]\n        h1 = np.log((1.0 + delta) / delta)\n        # h2 = E[1/(z+delta)^2]\n        h2 = 1.0 / (delta * (1.0 + delta))\n\n        # Calculate Var(c) = (A^2 + B^2) * Var(1/(z+delta))\n        var_z_inv = h2 - h1**2\n        var_c = (A**2 + B**2) * var_z_inv\n        \n        # The optimal lambda is lambda_star = Cov(w, c) / Var(c).\n        # We derived Cov(w,c) = Var(c) + q * Cov(xy, s).\n        # So, lambda_star = 1 + q * Cov(xy, s) / Var(c).\n        \n        # Calculate Cov(xy, s) = (A+B)/4 * [2 - (2*delta+1)*h1]\n        cov_xy_s = (A + B) / 4.0 * (2.0 - (2.0 * delta + 1.0) * h1)\n        \n        # Calculate lambda_star\n        lambda_star = 1.0 + q * cov_xy_s / var_c\n        \n        # Part 3: Calculate the residual bias.\n        # bias = -lambda_star * tau\n        bias = -lambda_star * tau\n        \n        results.append([lambda_star, bias])\n        \n    # Format the final output string as per requirements.\n    # Each float must be rounded to six decimal places.\n    formatted_results = []\n    for lambda_val, bias_val in results:\n        formatted_pair = f'[{lambda_val:.6f},{bias_val:.6f}]'\n        formatted_results.append(formatted_pair)\n        \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Higher-order calculations in perturbative physics often generate events with negative weights, which complicates standard analysis and unweighting procedures. This practice provides hands-on experience with signed Poisson thinning, a procedure that converts events with both positive and negative weights into an unweighted sample where each event carries a simple sign of $+1$ or $-1$ . By combining theoretical derivation with a computational simulation, you will verify the method's properties and quantify the \"variance inflation\"â€”the statistical penalty incurred for accommodating sign cancellations.",
            "id": "3513801",
            "problem": "You are given the task of formalizing and testing a signed Poisson thinning procedure for handling negative event weights in computational high-energy physics. Consider a single weighted event whose weight is decomposed as $w = w_{+} - w_{-}$ with $w_{+} \\ge 0$ and $w_{-} \\ge 0$. The goal is to represent this event by a signed unweighted sample constructed from two independent Poisson random variables and to study unbiasedness and variance properties, including the effect of negative weights on variance inflation.\n\nStarting from the following foundational facts:\n- If $N \\sim \\mathrm{Poisson}(\\lambda)$, then $\\mathbb{E}[N] = \\lambda$ and $\\mathrm{Var}[N] = \\lambda$ for any $\\lambda \\ge 0$.\n- If $X$ and $Y$ are independent random variables, then $\\mathbb{E}[X \\pm Y] = \\mathbb{E}[X] \\pm \\mathbb{E}[Y]$ and $\\mathrm{Var}[X \\pm Y] = \\mathrm{Var}[X] + \\mathrm{Var}[Y]$.\n- For any constant $c$, $\\mathbb{E}[c X] = c\\,\\mathbb{E}[X]$ and $\\mathrm{Var}[c X] = c^{2}\\,\\mathrm{Var}[X]$.\n\nDefine the signed Poisson thinning estimator for a chosen unit scale $a > 0$ as follows. Draw two independent Poisson random variables $N_{+} \\sim \\mathrm{Poisson}(\\lambda_{+})$ and $N_{-} \\sim \\mathrm{Poisson}(\\lambda_{-})$ with rates $\\lambda_{+} = w_{+}/a$ and $\\lambda_{-} = w_{-}/a$. Define the signed unweighted estimator $S = a\\,(N_{+} - N_{-})$.\n\nTasks to be addressed:\n- Derive from first principles that $S$ is an unbiased estimator of $w$, and derive its variance in terms of $w_{+}$, $w_{-}$, and $a$.\n- Define and derive a variance inflation factor that isolates the effect of sign cancellations: for $|w| > 0$, define $F = \\dfrac{w_{+} + w_{-}}{|w|}$ and interpret it as the factor by which the variance $a\\,(w_{+} + w_{-})$ exceeds the variance that would occur if the same absolute net weight $|w|$ were produced using only nonnegative weights, namely $a\\,|w|$. For the special case $|w| = 0$, specify that the inflation factor is undefined and must be reported via a sentinel as described below.\n\nProgramming task:\n- Implement a program that, for a given test suite, simulates the signed thinning estimator $S$ and compares empirical mean and variance with their derived theoretical values. For each test case, use $R$ independent repetitions to generate independent samples of $S$, and compute the empirical mean $\\hat{\\mu}$ and empirical variance $\\hat{v}$ using population normalization. The theoretical mean is $\\mu = w_{+} - w_{-}$ and the theoretical variance is $v = a\\,(w_{+} + w_{-})$.\n- For $|w| > 0$, compute the empirical inflation factor $\\hat{F} = \\hat{v}/(a\\,|w|)$ and the theoretical $F = (w_{+} + w_{-})/|w|$. For $|w| = 0$, do not compute an inflation factor; instead, output the sentinel value $-1.0$ for the inflation-factor error as specified below.\n- For each test case, output the triple of differences\n  $(\\Delta_{\\mu}, \\Delta_{v}, \\Delta_{F}) = \\big(\\hat{\\mu} - \\mu,\\; \\hat{v} - v,\\; \\hat{F} - F\\big)$,\n  where for $|w| = 0$ you must output $\\Delta_{F} = -1.0$ by convention.\n\nSimulation details:\n- For each test case, generate $R$ independent pairs $(N_{+}, N_{-})$ with the specified rates, form $S = a\\,(N_{+} - N_{-})$ for each repeat, and compute $\\hat{\\mu}$ and $\\hat{v}$ using population normalization. Use independent random number generator seeds per test case to ensure reproducibility.\n\nTest suite:\n- The program must use the following test cases, each specified as a quintuple $(w_{+}, w_{-}, a, R, \\text{seed})$:\n  - Case A: $(3.0, 0.0, 1.2, 1000000, 12345)$\n  - Case B: $(0.0, 2.5, 0.8, 1000000, 271828)$\n  - Case C: $(5.0, 5.0, 1.0, 1000000, 314159)$\n  - Case D: $(4.0, 1.5, 0.7, 1000000, 1618033)$\n\nRequired final output format:\n- Your program should produce a single line of output containing the concatenated list of results from all test cases as a comma-separated list enclosed in square brackets. The results must be ordered as $[\\Delta_{\\mu}^{A}, \\Delta_{v}^{A}, \\Delta_{F}^{A}, \\Delta_{\\mu}^{B}, \\Delta_{v}^{B}, \\Delta_{F}^{B}, \\Delta_{\\mu}^{C}, \\Delta_{v}^{C}, \\Delta_{F}^{C}, \\Delta_{\\mu}^{D}, \\Delta_{v}^{D}, \\Delta_{F}^{D}]$ with each value rendered as a floating-point number. For the inflation-factor error in cases with $|w| = 0$, output the sentinel value $-1.0$.",
            "solution": "The problem statement is valid. It is scientifically grounded in probability theory and statistics, well-posed with all necessary information provided, and objective in its formulation. It represents a formalizable and relevant problem in computational high-energy physics concerning the statistical treatment of weighted Monte Carlo events. We will now proceed with the solution.\n\nThe solution consists of two parts: first, a theoretical derivation of the statistical properties of the proposed estimator, and second, a numerical implementation to verify these derivations.\n\n### Theoretical Derivations\n\nThe problem defines a signed unweighted estimator $S$ for a weighted event with weight $w = w_{+} - w_{-}$, where $w_{+} \\ge 0$ and $w_{-} \\ge 0$. The estimator is constructed using a unit scale $a > 0$ and two independent Poisson random variables, $N_{+} \\sim \\mathrm{Poisson}(\\lambda_{+})$ and $N_{-} \\sim \\mathrm{Poisson}(\\lambda_{-})$, with rates $\\lambda_{+} = w_{+}/a$ and $\\lambda_{-} = w_{-}/a$. The estimator is given by $S = a\\,(N_{+} - N_{-})$.\n\n#### 1. Unbiasedness of the Estimator $S$\nAn estimator is unbiased if its expectation equals the true value of the parameter it estimates. Here, we must show that $\\mathbb{E}[S] = w$. We use the fundamental properties of expectation. By the linearity of expectation, we have:\n$$\n\\mathbb{E}[S] = \\mathbb{E}[a\\,(N_{+} - N_{-})] = a\\,\\mathbb{E}[N_{+} - N_{-}]\n$$\nSince $N_{+}$ and $N_{-}$ are independent, the expectation of their difference is the difference of their expectations:\n$$\n\\mathbb{E}[S] = a\\,(\\mathbb{E}[N_{+}] - \\mathbb{E}[N_{-}])\n$$\nThe expectation of a Poisson random variable $N \\sim \\mathrm{Poisson}(\\lambda)$ is $\\mathbb{E}[N] = \\lambda$. Applying this to $N_{+}$ and $N_{-}$:\n$$\n\\mathbb{E}[S] = a\\,(\\lambda_{+} - \\lambda_{-})\n$$\nSubstituting the given definitions for the rates, $\\lambda_{+} = w_{+}/a$ and $\\lambda_{-} = w_{-}/a$:\n$$\n\\mathbb{E}[S] = a\\,\\left(\\frac{w_{+}}{a} - \\frac{w_{-}}{a}\\right) = a\\,\\left(\\frac{w_{+} - w_{-}}{a}\\right) = w_{+} - w_{-}\n$$\nBy definition, $w = w_{+} - w_{-}$, so we have demonstrated that $\\mathbb{E}[S] = w$. Thus, $S$ is an unbiased estimator of $w$.\n\n#### 2. Variance of the Estimator $S$\nNext, we derive the variance of $S$, denoted as $\\mathrm{Var}(S)$. We use the property $\\mathrm{Var}(cX) = c^2\\,\\mathrm{Var}(X)$ for any constant $c$:\n$$\n\\mathrm{Var}(S) = \\mathrm{Var}(a\\,(N_{+} - N_{-})) = a^2\\,\\mathrm{Var}(N_{+} - N_{-})\n$$\nFor independent random variables $X$ and $Y$, the variance of their sum or difference is the sum of their variances: $\\mathrm{Var}(X \\pm Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y)$. Since $N_{+}$ and $N_{-}$ are independent:\n$$\n\\mathrm{Var}(S) = a^2\\,(\\mathrm{Var}(N_{+}) + \\mathrm{Var}(N_{-}))\n$$\nThe variance of a Poisson random variable $N \\sim \\mathrm{Poisson}(\\lambda)$ is $\\mathrm{Var}(N) = \\lambda$. Applying this property:\n$$\n\\mathrm{Var}(S) = a^2\\,(\\lambda_{+} + \\lambda_{-})\n$$\nSubstituting the definitions for the rates $\\lambda_{+}$ and $\\lambda_{-}$:\n$$\n\\mathrm{Var}(S) = a^2\\,\\left(\\frac{w_{+}}{a} + \\frac{w_{-}}{a}\\right) = a^2\\,\\left(\\frac{w_{+} + w_{-}}{a}\\right) = a\\,(w_{+} + w_{-})\n$$\nThis is the theoretical variance of the estimator, $v = a\\,(w_{+} + w_{-})$, as stated in the problem.\n\n#### 3. Variance Inflation Factor $F$\nThe variance inflation factor is defined for $|w| > 0$ as $F = \\dfrac{w_{+} + w_{-}}{|w|}$. To interpret this, we compare the actual variance $v = a\\,(w_{+} + w_{-})$ with the variance of an idealized estimator that achieves the same net weight magnitude $|w|$ but without any cancellations from negative weights.\n\nIn such an ideal case, the total weight magnitude $|w|$ would be constructed entirely from positive weights (or entirely from negative weights, which is equivalent in terms of variance). This corresponds to a weight decomposition $w'_{\\text{total}} = w'_+ + w'_- = |w|$, where one component is zero. The variance of the estimator in this ideal scenario, $v_{\\text{ideal}}$, would be:\n$$\nv_{\\text{ideal}} = a\\,(w'_+ + w'_-) = a\\,|w|\n$$\nThis represents the minimal variance achievable for a net weight of magnitude $|w|$ using this estimation scheme.\n\nThe ratio of the actual variance to this ideal minimum variance is:\n$$\n\\frac{v}{v_{\\text{ideal}}} = \\frac{a\\,(w_{+} + w_{-})}{a\\,|w|} = \\frac{w_{+} + w_{-}}{|w|} = F\n$$\nThus, $F$ is precisely the factor by which the variance is inflated due to the simultaneous presence of positive and negative weight components ($w_{+} > 0$ and $w_{-} > 0$), which lead to cancellations in the net weight $w$. By the triangle inequality, $w_{+} + w_{-} \\ge |w_{+} - w_{-}| = |w|$, which implies $F \\ge 1$. The factor $F$ is equal to $1$ if and only if either $w_{+}$ or $w_{-}$ is zero (no cancellation), and $F > 1$ when both are positive, indicating variance inflation.\n\nFor the case $|w| = 0$, which occurs when $w_{+} = w_{-}$, the denominator of $F$ is $0$, making the factor undefined. The problem correctly specifies that this case must be handled with a sentinel value. In this situation, the variance is $v = a\\,(w_{+} + w_{-}) = 2aw_{+}$, which is non-zero, while the net weight is zero, leading to an infinite relative uncertainty.\n\nThe following Python program implements the simulation to numerically verify these theoretical findings.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates a signed Poisson thinning procedure for several test cases,\n    compares empirical and theoretical statistics, and outputs the differences.\n    \"\"\"\n    # Test cases are given as quintuples: (w+, w-, a, R, seed)\n    test_cases = [\n        # Case A\n        (3.0, 0.0, 1.2, 1000000, 12345),\n        # Case B\n        (0.0, 2.5, 0.8, 1000000, 271828),\n        # Case C\n        (5.0, 5.0, 1.0, 1000000, 314159),\n        # Case D\n        (4.0, 1.5, 0.7, 1000000, 1618033),\n    ]\n\n    results = []\n    for w_plus, w_minus, a, R, seed in test_cases:\n        # Create a dedicated random number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # 1. Theoretical calculations\n        w_net = w_plus - w_minus\n        mu_th = w_net\n        v_th = a * (w_plus + w_minus)\n\n        # 2. Simulation\n        # Define Poisson rates\n        lambda_plus = w_plus / a\n        lambda_minus = w_minus / a\n\n        # Generate R independent Poisson-distributed random numbers\n        N_plus_samples = rng.poisson(lam=lambda_plus, size=R)\n        N_minus_samples = rng.poisson(lam=lambda_minus, size=R)\n\n        # Construct samples of the estimator S\n        S_samples = a * (N_plus_samples - N_minus_samples)\n\n        # 3. Compute empirical statistics\n        # Empirical mean\n        mu_emp = np.mean(S_samples)\n        \n        # Empirical variance using population normalization (ddof=0)\n        v_emp = np.var(S_samples, ddof=0)\n\n        # 4. Calculate differences\n        delta_mu = mu_emp - mu_th\n        delta_v = v_emp - v_th\n\n        # Handle the inflation factor F\n        # Use np.isclose for robust floating-point comparison to zero\n        if not np.isclose(w_net, 0.0):\n            # Theoretical inflation factor\n            F_th = (w_plus + w_minus) / abs(w_net)\n            \n            # Empirical inflation factor\n            F_emp = v_emp / (a * abs(w_net))\n            \n            delta_F = F_emp - F_th\n        else:\n            # As per problem specification, use sentinel value for delta_F when w=0\n            delta_F = -1.0\n        \n        results.extend([delta_mu, delta_v, delta_F])\n    \n    # Format and print the final output in the required single-line format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}