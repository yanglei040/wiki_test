{
    "hands_on_practices": [
        {
            "introduction": "将连续数据分箱是高能物理分析中一种常见的数据压缩方法，但这个过程不可避免地会损失信息。本练习将通过一个理想化的指数衰减模型，引导你使用费雪信息(Fisher Information)这一严格的统计工具来量化这种信息损失。通过推导和比较无箱(unbinned)和有箱(binned)两种情况下的费雪信息，你将能直观地理解分箱策略（如箱数 $m$）与最终参数测量精度之间的深刻联系 。",
            "id": "3540423",
            "problem": "您正在研究在有限观测窗口内，指数寿命模型的非分箱与分箱最大似然拟合的信息论效率。考虑从定义在非负数轴上的指数概率密度函数 $f(x \\mid \\lambda) = \\lambda e^{-\\lambda x}$（$x \\ge 0$，参数 $\\lambda > 0$）中抽取的独立同分布样本，但假设数据采集仅限于一个有限窗口 $[0, T]$。以在该窗口内观测到恰好 $N$ 个事件为条件。对于非分箱情况，似然函数是在 $[0, T]$ 上的截断密度的乘积。对于分箱情况，将窗口 $[0, T]$ 分割成 $m$ 个等宽的箱，并使用以 $N$ 为条件的总计数下，各箱计数的的多项式似然。使用下方的基本定义，推导解析表达式并进行数值评估。\n\n基本定义：\n- 对于密度为 $f(x \\mid \\lambda)$ 的独立观测值 $x_1, \\dots, x_N$，其对数似然函数为 $\\ell(\\lambda) = \\sum_{i=1}^N \\log f(x_i \\mid \\lambda)$。\n- 来自 $N$ 个独立样本的参数 $\\lambda$ 的费雪信息为 $I(\\lambda) = - \\mathbb{E}\\left[\\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2}\\right]$，其中期望是针对真实数据生成分布计算的。\n- 对于一个总计数为 $N$、概率 $p_k(\\lambda)$ 依赖于 $\\lambda$ 的多项式模型，参数 $\\lambda$ 的费雪信息为 $I(\\lambda) = N \\sum_{k=1}^m \\frac{1}{p_k(\\lambda)} \\left(\\frac{\\partial p_k(\\lambda)}{\\partial \\lambda}\\right)^2$，前提是对于所有 $k$ 都有 $p_k(\\lambda) > 0$。\n\n任务：\n1. 截断情况下的非分箱信息。给定观测窗口 $[0, T]$，将数据视为从 $[0, T]$ 上的截断指数分布中抽取，即条件密度为 $f_T(x \\mid \\lambda) = \\frac{\\lambda e^{-\\lambda x}}{1 - e^{-\\lambda T}}$（$0 \\le x \\le T$）。从费雪信息的定义出发，推导在此截断条件下单个事件对 $\\lambda$ 的费雪信息，并由此得到 $[0, T]$ 内 $N$ 个观测值的总费雪信息。\n2. 等宽分箱的信息。将 $[0, T]$ 分割成 $m$ 个等宽的箱，其边界为 $0, \\Delta, 2\\Delta, \\dots, T$，其中 $\\Delta = T/m$。令 $p_k(\\lambda)$ 表示在 $f_T(x \\mid \\lambda)$ 分布下，事件落入第 $k$ 个箱（$k \\in \\{1, \\dots, m\\}$）的概率。从上述多项式费雪信息的定义出发，推导以 $p_k(\\lambda)$ 和 $\\frac{\\partial p_k(\\lambda)}{\\partial \\lambda}$ 表示的 $\\lambda$ 的总费雪信息的表达式。\n3. 在最小期望占据数约束下对 $m$ 进行优化。为保证实际分箱拟合的数值稳定性，强制要求每个箱的期望计数必须至少为一个正常数阈值 $c_{\\min}$。对于总共 $N$ 个事件，这意味着对于所有 $k \\in \\{1, \\dots, m\\}$，都有 $N \\cdot p_k(\\lambda) \\ge c_{\\min}$。在所有满足此约束的整数 $m \\in \\{1, 2, \\dots, N\\}$ 中，选择能够最大化分箱费雪信息（等效于最小化相对于非分箱情况的信息损失）的 $m^\\star$。如果多个 $m$ 在浮点数容差范围内达到相同的最大值，则选择其中最大的 $m$。\n4. 实现一个程序，对下面指定的每个测试用例，计算：\n   - 总非分箱费雪信息 $I_{\\text{unb}}(\\lambda, T, N)$。\n   - 使用在占据数约束下优化得到的 $m^\\star$ 计算的总分箱费雪信息 $I_{\\text{bin}}(\\lambda, T, N, m^\\star)$。\n   - 比率 $r = I_{\\text{bin}} / I_{\\text{unb}}$。\n   - 优化后的箱数 $m^\\star$。\n   报告浮点数结果时，四舍五入至六位小数。\n\n假设与说明：\n- 所有量都是无量纲的；不使用物理单位。\n- 不涉及角度。\n- 所有概率之和必须为一，且所有 $p_k(\\lambda)$ 必须为严格正数，以确保费雪信息为有限值。\n- 在非分箱和分箱两种情况下，均需一致地使用以 $N$ 为条件的框架。\n\n测试套件：\n计算以下参数集 $(\\lambda, T, N, c_{\\min})$ 的输出：\n- 案例 A: $(0.5, 6.0, 1000, 5)$\n- 案例 B: $(0.2, 20.0, 500, 10)$\n- 案例 C: $(1.0, 3.0, 30, 5)$\n\n最终输出格式：\n您的程序应生成单行文本，其中包含结果，格式为一个逗号分隔的列表的列表，每个测试用例一个列表，顺序为 [案例 A, 案例 B, 案例 C]。每个内部列表的形式必须为 $[m^\\star, I_{\\text{unb}}, I_{\\text{bin}}, r]$，其中 $m^\\star$ 是一个整数，其他条目是四舍五入到六位小数的浮点数。例如：[[mA, IunbA, IbinA, rA],[mB, IunbB, IbinB, rB],[mC, IunbC, IbinC, rC]]。",
            "solution": "根据指定标准对问题进行验证。\n\n### 步骤 1：提取已知条件\n- **概率密度函数 (PDF)**: 非负数轴上的指数分布, $f(x \\mid \\lambda) = \\lambda e^{-\\lambda x}$ for $x \\ge 0$ and $\\lambda > 0$。\n- **观测窗口**: 数据采集限于区间 $[0, T]$。\n- **截断条件密度**: 对于在 $[0, T]$ 内观测到的 $N$ 个事件，其密度为 $f_T(x \\mid \\lambda) = \\frac{\\lambda e^{-\\lambda x}}{1 - e^{-\\lambda T}}$ for $0 \\le x \\le T$。\n- **对数似然定义**: 对于独立同分布观测值 $x_1, \\dots, x_N$，$\\ell(\\lambda) = \\sum_{i=1}^N \\log f(x_i \\mid \\lambda)$。\n- **非分箱费雪信息定义**: $I(\\lambda) = - \\mathbb{E}\\left[\\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2}\\right]$。\n- **分箱模型**: 窗口 $[0, T]$ 被划分为 $m$ 个等宽的箱。箱宽为 $\\Delta = T/m$。\n- **多项式分箱费雪信息定义**: 对于总计数 $N$ 和箱概率 $p_k(\\lambda)$，$I(\\lambda) = N \\sum_{k=1}^m \\frac{1}{p_k(\\lambda)} \\left(\\frac{\\partial p_k(\\lambda)}{\\partial \\lambda}\\right)^2$，其中 $p_k(\\lambda) > 0$。\n- **优化约束**: 对于选定的箱数 $m$，每个箱中的期望计数必须至少为 $c_{\\min}$，即对于所有 $k \\in \\{1, \\dots, m\\}$ 都有 $N \\cdot p_k(\\lambda) \\ge c_{\\min}$。\n- **优化目标**: 寻找满足约束条件且能最大化分箱费雪信息的 $m^\\star \\in \\{1, 2, \\dots, N\\}$。若有并列，则选择最大的 $m$。\n- **测试用例**:\n    - 案例 A: $(\\lambda, T, N, c_{\\min}) = (0.5, 6.0, 1000, 5)$\n    - 案例 B: $(\\lambda, T, N, c_{\\min}) = (0.2, 20.0, 500, 10)$\n    - 案例 C: $(\\lambda, T, N, c_{\\min}) = (1.0, 3.0, 30, 5)$\n- **输出要求**: 对每个测试用例，计算 $[m^\\star, I_{\\text{unb}}, I_{\\text{bin}}, r]$，其中 $I_{\\text{unb}}$, $I_{\\text{bin}}$ 和 $r = I_{\\text{bin}} / I_{\\text{unb}}$ 四舍五入到六位小数。\n\n### 步骤 2：使用提取的已知条件进行验证\n对问题的有效性进行评估：\n- **科学依据**: 该问题基于统计推断的基本原理，即最大似然估计和费雪信息理论，并将其应用于指数分布。这是计算物理学和统计学中一个标准且易于理解的课题。该设置在科学上是合理的。\n- **良态问题**: 问题明确了所有必要的定义、约束和目标。推导任务表述清晰。$m^\\star$ 的优化任务有明确的目标函数（最大化 $I_{\\text{bin}}$）、明确定义的约束集和打破并列的规则，确保存在唯一解。\n- **客观性**: 问题使用精确的数学语言陈述，没有主观或含糊的术语。\n- **其他缺陷**: 问题没有违反任何无效性标准。它是自洽的、逻辑一致的，并且计算上是可行的。数学框架是标准的。提供的测试用例是合理的，没有引入矛盾。\n\n### 步骤 3：结论与行动\n问题被判定为**有效**。下面提供一个完整的、有理有据的解决方案。\n\n解决方案的步骤是先推导所需量的解析表达式，然后进行数值实现。\n\n**1. 非分箱费雪信息, $I_{\\text{unb}}$**\n\n来自截断密度 $f_T(x \\mid \\lambda) = \\frac{\\lambda e^{-\\lambda x}}{1 - e^{-\\lambda T}}$ 的单个观测值 $x$ 的对数似然为：\n$$ \\ell_1(\\lambda) = \\log(f_T(x \\mid \\lambda)) = \\log(\\lambda) - \\lambda x - \\log(1 - e^{-\\lambda T}) $$\n关于 $\\lambda$ 的一阶导数（得分函数）是：\n$$ \\frac{\\partial \\ell_1(\\lambda)}{\\partial \\lambda} = \\frac{1}{\\lambda} - x - \\frac{1}{1 - e^{-\\lambda T}} \\cdot (-e^{-\\lambda T}) \\cdot (-T) = \\frac{1}{\\lambda} - x - \\frac{T e^{-\\lambda T}}{1 - e^{-\\lambda T}} $$\n二阶导数是：\n$$ \\frac{\\partial^2 \\ell_1(\\lambda)}{\\partial \\lambda^2} = -\\frac{1}{\\lambda^2} - \\frac{\\partial}{\\partial \\lambda}\\left(\\frac{T e^{-\\lambda T}}{1 - e^{-\\lambda T}}\\right) $$\n使用商法则，第二项的导数是 $\\frac{-T^2 e^{-\\lambda T}}{(1 - e^{-\\lambda T})^2}$。因此：\n$$ \\frac{\\partial^2 \\ell_1(\\lambda)}{\\partial \\lambda^2} = -\\frac{1}{\\lambda^2} + \\frac{T^2 e^{-\\lambda T}}{(1 - e^{-\\lambda T})^2} $$\n此表达式与随机变量 $x$ 无关。因此，期望 $\\mathbb{E}\\left[\\frac{\\partial^2 \\ell_1(\\lambda)}{\\partial \\lambda^2}\\right]$ 就是表达式本身。单个事件的费雪信息是：\n$$ I_1(\\lambda) = -\\mathbb{E}\\left[\\frac{\\partial^2 \\ell_1(\\lambda)}{\\partial \\lambda^2}\\right] = - \\left(-\\frac{1}{\\lambda^2} + \\frac{T^2 e^{-\\lambda T}}{(1 - e^{-\\lambda T})^2}\\right) = \\frac{1}{\\lambda^2} - \\frac{T^2 e^{-\\lambda T}}{(1 - e^{-\\lambda T})^2} $$\n对于 $N$ 个独立同分布的观测值，总费雪信息是可加的：\n$$ I_{\\text{unb}}(\\lambda, T, N) = N \\cdot I_1(\\lambda) = N \\left(\\frac{1}{\\lambda^2} - \\frac{T^2 e^{-\\lambda T}}{(1 - e^{-\\lambda T})^2}\\right) $$\n该表达式是非负的，这可以通过不等式 $\\sinh(v) \\ge v$（对于 $v \\ge 0$）来证明。\n\n**2. 分箱费雪信息, $I_{\\text{bin}}$**\n\n对于 $m$ 个等宽的箱（宽度 $\\Delta = T/m$），一个事件落入第 $k$ 个箱（$k \\in \\{1, \\dots, m\\}$，边界为 $[(k-1)\\Delta, k\\Delta]$）的概率由截断密度的积分给出：\n$$ p_k(\\lambda) = \\int_{(k-1)\\Delta}^{k\\Delta} f_T(x \\mid \\lambda) dx = \\frac{1}{1 - e^{-\\lambda T}} \\int_{(k-1)\\Delta}^{k\\Delta} \\lambda e^{-\\lambda x} dx $$\n$$ p_k(\\lambda) = \\frac{[-e^{-\\lambda x}]_{(k-1)\\Delta}^{k\\Delta}}{1 - e^{-\\lambda T}} = \\frac{e^{-\\lambda(k-1)\\Delta} - e^{-\\lambda k\\Delta}}{1 - e^{-\\lambda T}} $$\n为保证数值稳定性，可以使用 `expm1` 函数重写此式，其中 $\\text{expm1}(z) = e^z - 1$：\n$$ p_k(\\lambda) = \\frac{e^{-\\lambda k\\Delta}(e^{\\lambda\\Delta} - 1)}{e^{-\\lambda T}(e^{\\lambda T} - 1)} = e^{\\lambda(T-k\\Delta)} \\frac{\\text{expm1}(\\lambda\\Delta)}{\\text{expm1}(\\lambda T)} $$\n分箱模型的费雪信息为 $I_{\\text{bin}}(\\lambda) = N \\sum_{k=1}^m \\frac{1}{p_k(\\lambda)} \\left(\\frac{\\partial p_k(\\lambda)}{\\partial \\lambda}\\right)^2$。这可以重写为 $N \\sum_{k=1}^m p_k(\\lambda) \\left(\\frac{\\partial \\log p_k(\\lambda)}{\\partial \\lambda}\\right)^2$。对 $p_k(\\lambda)$ 的稳定表达式取对数：\n$$ \\log p_k(\\lambda) = \\lambda(T-k\\Delta) + \\log(\\text{expm1}(\\lambda\\Delta)) - \\log(\\text{expm1}(\\lambda T)) $$\n对 $\\lambda$ 求导：\n$$ \\frac{\\partial \\log p_k(\\lambda)}{\\partial \\lambda} = (T - k\\Delta) + \\frac{\\Delta e^{\\lambda\\Delta}}{\\text{expm1}(\\lambda\\Delta)} - \\frac{T e^{\\lambda T}}{\\text{expm1}(\\lambda T)} $$\n这提供了一种数值稳定的方法来计算 $I_{\\text{bin}}$ 求和式中的各项。\n\n**3. 箱数优化, $m^\\star$**\n\n约束条件是 $N \\cdot p_k(\\lambda) \\ge c_{\\min}$ for all $k = 1, \\dots, m$。箱概率 $p_k(\\lambda)$ 是关于 $k$ 的单调递减函数。因此，约束对于最后一个箱 $k=m$ 是最严格的。该条件简化为：\n$$ N \\cdot p_m(\\lambda) \\ge c_{\\min} $$\n使用 $k=m$ 和 $\\Delta=T/m$ 时 $p_k(\\lambda)$ 的稳定表达式：\n$$ p_m(\\lambda) = \\frac{\\text{expm1}(\\lambda T/m)}{\\text{expm1}(\\lambda T)} $$\n费雪信息 $I_{\\text{bin}}(m)$ 预期是 $m$ 的单调递增函数（因为更细的分箱保留了更多信息），当 $m \\to \\infty$ 时趋近于 $I_{\\text{unb}}$。因此，要在约束条件下最大化 $I_{\\text{bin}}(m)$，我们应选择满足条件的最大可能的 $m$。函数 $p_m(\\lambda)$ 是 $m$ 的单调递减函数。这意味着我们可以搜索满足 $N \\cdot \\frac{\\text{expm1}(\\lambda T/m)}{\\text{expm1}(\\lambda T)} \\ge c_{\\min}$ 的最大整数 $m \\in \\{1, \\dots, N\\}$。对于给定的问题约束，从 $m=1$ 开始向上进行线性扫描是足够高效的。第一个违反约束的 $m$ 值确定了上界，而 $m^\\star$ 则是紧邻它之前的值。\n\n实现将遵循这些推导出的公式。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all specified test cases.\n    It orchestrates the calculation for each case and formats the final output.\n    \"\"\"\n    test_cases = [\n        (0.5, 6.0, 1000, 5),    # Case A\n        (0.2, 20.0, 500, 10),   # Case B\n        (1.0, 3.0, 30, 5),      # Case C\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = compute_results_for_case(*case)\n        all_results.append(result)\n\n    # Format the final output string as specified\n    formatted_results = []\n    for res in all_results:\n        # res is [m_star, i_unb, i_bin, ratio]\n        s = f'[{res[0]},{res[1]:.6f},{res[2]:.6f},{res[3]:.6f}]'\n        formatted_results.append(s)\n    print(f\"[{','.join(formatted_results)}]\")\n\n\ndef compute_results_for_case(lam, T, N, c_min):\n    \"\"\"\n    Computes all required quantities for a single test case.\n\n    Args:\n        lam (float): The lambda parameter of the exponential distribution.\n        T (float): The upper bound of the observation window.\n        N (int): The total number of events.\n        c_min (int): The minimum required expected occupancy per bin.\n\n    Returns:\n        list: A list containing [m_star, I_unb, I_bin, ratio].\n    \"\"\"\n    # 1. Compute the unbinned Fisher information\n    i_unb = calculate_i_unb(lam, T, N)\n\n    # 2. Find the optimal number of bins m_star\n    m_star = find_m_star(lam, T, N, c_min)\n\n    # 3. Compute the binned Fisher information for m_star\n    i_bin = 0.0\n    if m_star > 0:\n        i_bin = calculate_i_bin(lam, T, N, m_star)\n\n    # 4. Compute the ratio of binned to unbinned information\n    ratio = i_bin / i_unb if i_unb > 0 else 0.0\n\n    # 5. Return the results\n    return [m_star, i_unb, i_bin, ratio]\n\n\ndef calculate_i_unb(lam, T, N):\n    \"\"\"\n    Calculates the total unbinned Fisher information for N events.\n    Formula: N * (1/lambda^2 - (T^2 * exp(-lambda*T)) / (1 - exp(-lambda*T))^2)\n    \"\"\"\n    lam_T = lam * T\n    # The parameters ensure lam > 0, T > 0, so lam_T > 0.\n    # No risk of division by zero in the denominator for lam_T > 0.\n    exp_m_lam_T = np.exp(-lam_T)\n    one_minus_exp = 1.0 - exp_m_lam_T\n    \n    term2 = (T**2 * exp_m_lam_T) / (one_minus_exp**2)\n    i_one_event = (1.0 / lam**2) - term2\n    \n    return N * i_one_event\n\n\ndef find_m_star(lam, T, N, c_min):\n    \"\"\"\n    Finds the optimal number of bins m_star by finding the largest m\n    that satisfies the minimum occupancy constraint.\n    \"\"\"\n    m_star_found = 0\n    lam_T = lam * T\n\n    # Constraint: N * p_m >= c_min => expm1(lam*T/m) >= (c_min/N_tot) * expm1(lam*T)\n    # Target for comparison to avoid recomputing the RHS in the loop\n    if np.isclose(lam_T, 0): # Should not happen with given parameters\n        return 1 if N >= c_min else 0\n        \n    threshold = (c_min / N) * np.expm1(lam_T)\n\n    # Linearly scan for m from 1 to N.\n    # p_m is monotonically decreasing in m, so we can stop when the constraint fails.\n    for m in range(1, N + 1):\n        lam_T_over_m = lam_T / m\n        expm1_val = np.expm1(lam_T_over_m)\n        if expm1_val >= threshold:\n            m_star_found = m\n        else:\n            break\n            \n    return m_star_found\n\n\ndef calculate_i_bin(lam, T, N, m):\n    \"\"\"\n    Calculates the total binned Fisher information for a given number of bins m.\n    \"\"\"\n    delta = T / m\n    lam_T = lam * T\n    lam_delta = lam * delta\n\n    def h(x):\n        \"\"\"Helper function for x*exp(x)/(exp(x)-1), stable for x near 0.\"\"\"\n        if np.isclose(x, 0):\n            return 1.0\n        return (x * np.exp(x)) / np.expm1(x)\n\n    h_lam_T = h(lam_T)\n    h_lam_delta = h(lam_delta)\n    \n    if np.isclose(lam_T, 0):\n        return 0.0\n        \n    expm1_lam_T = np.expm1(lam_T)\n    expm1_lam_delta = np.expm1(lam_delta)\n    \n    common_pk_factor = expm1_lam_delta / expm1_lam_T\n    common_dpk_term = (h_lam_delta - h_lam_T) / lam\n\n    info_sum = 0.0\n    for k in range(1, m + 1):\n        T_minus_k_delta = T - k * delta\n        \n        # Calculate p_k = exp(lam*(T-k*delta)) * common_pk_factor\n        pk = np.exp(lam * T_minus_k_delta) * common_pk_factor\n        \n        # Calculate d(log p_k)/d(lam) = (T-k*delta) + common_dpk_term\n        dlogpk_dlam = T_minus_k_delta + common_dpk_term\n        \n        # Term in sum for info is p_k * (d(log p_k)/d(lam))^2\n        if pk > 0:\n            info_sum += pk * (dlogpk_dlam**2)\n    \n    return N * info_sum\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "在探索新物理现象的边界时，我们常常面临数据稀疏的挑战，即许多计数箱的事件数非常少甚至为零。本练习模拟了这样一个在陡峭下降能谱下的稀疏分箱场景，旨在揭示此时极大似然估计量的行为。通过将估计参数的经验方差与理论上的克拉默-拉奥下限(Cramér-Rao Lower Bound)进行比较，你将深入理解在稀疏数据下，标准统计工具的局限性以及“方差膨胀”现象的实际意义 。",
            "id": "3540404",
            "problem": "你的任务是研究稀疏分箱似然中斜率参数的可识别性，从第一性原理出发，并实现一个完整的模拟和拟合研究。考虑一个急剧下降的谱，由有界区间 $x \\in [x_{\\min}, x_{\\max}]$ 上的归一化幂律概率密度函数（PDF）$f(x \\mid \\alpha) \\propto x^{-\\alpha}$ 建模，其中 $\\alpha$ 是一个未知的斜率参数。将区间 $[x_{\\min}, x_{\\max}]$ 划分成 $B$ 个不相交的分箱，其边界为 $\\{x_0, x_1, \\dots, x_B\\}$，其中 $x_0 = x_{\\min}$ 且 $x_B = x_{\\max}$，并且对于所有 $i$ 都有 $x_i < x_{i+1}$。假设总期望事件数已知且等于 $\\mu$，其中 $\\mu > 0$。\n\n基本基础：\n- 每个分箱 $b \\in \\{1, \\dots, B\\}$ 接收一个观测计数 $n_b \\in \\{0,1,2,\\dots\\}$，该计数被建模为从一个独立的、均值为 $\\lambda_b(\\alpha)$ 的泊松分布中抽取的样本。泊松概率质量函数（PMF）为 $P(N=n) = e^{-\\lambda} \\lambda^n / n!$，且独立的泊松变量可进行因子分解。\n- 预期的分箱均值由 $\\lambda_b(\\alpha) = \\mu \\, p_b(\\alpha)$ 给出，其中 $p_b(\\alpha)$ 是模型预测的分箱概率，即 $p_b(\\alpha) = \\int_{x_{b-1}}^{x_b} f(x \\mid \\alpha) \\, dx$，并归一化以使 $\\sum_{b=1}^B p_b(\\alpha) = 1$。\n- 对于 $[x_{\\min}, x_{\\max}]$ 上的幂律模型，分箱概率 $p_b(\\alpha)$ 由 $x^{-\\alpha}$ 在每个分箱上的精确积分除以在整个范围上的积分给出。当 $\\alpha \\neq 1$ 时，$\\int x^{-\\alpha}\\, dx = \\frac{x^{1-\\alpha}}{1-\\alpha}$。当 $\\alpha = 1$ 时，$\\int x^{-1}\\, dx = \\ln x$。使用这些公式从第一性原理计算归一化的 $p_b(\\alpha)$。\n\n任务：\n1. 从独立的泊松分布假设和 $\\lambda_b(\\alpha) = \\mu p_b(\\alpha)$ 出发，推导分箱似然。证明当 $\\mu$ 已知时，在 $\\alpha$ 上最大化似然等价于最大化多项式形式 $\\sum_{b=1}^B n_b \\ln p_b(\\alpha)$，两者相差一个与 $\\alpha$ 无关的加性常数。你的实现必须在指定的有界网格上最大化此目标函数以获得最大似然估计（MLE）$\\hat{\\alpha}$。\n2. 使用费雪信息 (Fisher information) 的定义，推导当 $\\mu$ 已知时关于 $\\alpha$ 的总费雪信息的表达式。从定义 $I(\\alpha) = \\mathbb{E}\\left[-\\frac{\\partial^2}{\\partial \\alpha^2} \\ln L(\\alpha)\\right]$ 和独立的泊松似然出发，证明费雪信息可以写成标准的“每事件”形式 $I(\\alpha)=\\mu\\, I_1(\\alpha)$，其中 $I_1(\\alpha)$ 是对所有分箱求和的 $p_b(\\alpha)\\left(\\frac{\\partial}{\\partial \\alpha}\\ln p_b(\\alpha)\\right)^2$。你的程序必须使用中心有限差分法以一个小的步长 $h$ 来数值计算 $\\frac{\\partial}{\\partial \\alpha} \\ln p_b(\\alpha)$，从而计算 $I_1(\\alpha)$，然后计算克拉默-拉奥下界（CRLB）方差为 $\\operatorname{Var}_{\\text{CRLB}}(\\hat{\\alpha}) = \\frac{1}{\\mu I_1(\\alpha)}$，并在真实的 $\\alpha$ 值处进行评估。\n3. 对每个分箱，在指定的真实斜率 $\\alpha_{\\text{true}}$ 下，通过从 $n_b \\sim \\text{Poisson}(\\mu p_b(\\alpha_{\\text{true}}))$ 中抽样（各分箱之间独立），实现一个包含 $R$ 次伪实验的蒙特卡洛模拟。对于每次伪实验，通过在固定网格 $\\{\\alpha_j\\}_{j=1}^{N_\\alpha}$（该网格在 $[\\alpha_{\\min}, \\alpha_{\\max}]$ 上线性间隔分布）上最大化 $\\sum_{b=1}^B n_b \\ln p_b(\\alpha)$ 来计算 $\\hat{\\alpha}$。为概率使用一个严格为正的下限值（例如，在取对数前将 $p_b(\\alpha)$ 截断在一个非常小的阈值以上），以避免数值问题，同时不改变归一化条件，并在所有情况下保持相同的 $\\mu$。如果拟合结果落在网格的边界上，保持原样；不要丢弃或重新拟合。\n4. 将“方差膨胀”量化为伪实验中 $\\hat{\\alpha}$ 的经验方差除以在真实 $\\alpha$ 处计算的 CRLB 方差。也就是说，为每个测试用例计算比率 $\\frac{\\operatorname{Var}_{\\text{emp}}(\\hat{\\alpha})}{\\operatorname{Var}_{\\text{CRLB}}(\\hat{\\alpha})}$。\n\n你的程序必须遵循以下约束：\n- 所有计算必须是自包含且可复现的。在要求的地方使用固定的随机种子。\n- 不使用任何角度；不需要角度单位。不需要物理单位；将所有量视为无量纲。\n- 每个测试用例的最终输出必须是一个浮点数，代表上面定义的方差膨胀比。\n\n测试套件：\n实现并对以下四个用例运行你的代码。在所有用例中，使用一个在 $[\\alpha_{\\min}, \\alpha_{\\max}] = [0.5, 6.0]$ 范围内包含 $N_\\alpha = 1001$ 个点的线性间隔 $\\alpha$ 网格，一个用于导数计算的中心有限差分步长 $h = 10^{-5}$，以及每个用例 $R = 600$ 次伪实验。使用一个基础伪随机种子 $s_0 = 1729$，并且为了可复现性，将用例索引 $k \\in \\{0,1,2,3\\}$ 的种子设置为 $s_k = s_0 + k$。对每个用例，定义真实参数 $(\\alpha_{\\text{true}}, \\mu, B, x_{\\min}, x_{\\max})$ 如下：\n- 用例 1（理想路径，样本充足）：$(\\alpha_{\\text{true}}, \\mu, B, x_{\\min}, x_{\\max}) = (3.0, 1000.0, 20, 1.0, 10.0)$。\n- 用例 2（稀疏分箱）：$(\\alpha_{\\text{true}}, \\mu, B, x_{\\min}, x_{\\max}) = (3.0, 50.0, 80, 1.0, 20.0)$。\n- 用例 3（极度稀疏且更陡峭）：$(\\alpha_{\\text{true}}, \\mu, B, x_{\\min}, x_{\\max}) = (4.0, 15.0, 80, 1.0, 50.0)$。\n- 用例 4（陡峭且可能存在边界效应）：$(\\alpha_{\\text{true}}, \\mu, B, x_{\\min}, x_{\\max}) = (5.0, 15.0, 40, 1.0, 50.0)$。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含按所列顺序排列的四个用例的方差膨胀结果，格式为方括号内以逗号分隔的列表，例如 $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$。每个元素必须是一个浮点数。",
            "solution": "目标是对幂律指数 $\\alpha$ 的分箱最大似然估计进行详细分析，重点关注估计量的方差及其与克拉默-拉奥下界（CRLB）的关系。这包括从第一性原理推导似然和费雪信息的理论表达式，然后通过蒙特卡洛模拟研究，在不同的统计条件下（特别是在稀疏数据情况下）凭经验测量估计量的性能。\n\n### 1. 理论框架\n\n#### 1.1. 幂律模型和分箱概率\n\n模型是在有界区间 $[x_{\\min}, x_{\\max}]$ 上定义的幂律概率密度函数（PDF）：\n$$\nf(x \\mid \\alpha) = C(\\alpha) x^{-\\alpha}\n$$\n其中 $C(\\alpha)$ 是归一化常数。该常数由条件 $\\int_{x_{\\min}}^{x_{\\max}} f(x \\mid \\alpha) \\, dx = 1$ 确定，可得：\n$$\nC(\\alpha) = \\left( \\int_{x_{\\min}}^{x_{\\max}} x^{-\\alpha} \\, dx \\right)^{-1}\n$$\n积分取决于 $\\alpha$ 的值。当 $\\alpha \\neq 1$ 时，不定积分为 $\\int x^{-\\alpha} \\, dx = \\frac{x^{1-\\alpha}}{1-\\alpha}$。当 $\\alpha=1$ 时，不定积分为 $\\int x^{-1} \\, dx = \\ln x$。因此，在整个范围上的定积分为：\n$$\n\\int_{x_{\\min}}^{x_{\\max}} x^{-\\alpha} \\, dx = \\begin{cases} \\frac{x_{\\max}^{1-\\alpha} - x_{\\min}^{1-\\alpha}}{1-\\alpha}  \\text{ if } \\alpha \\neq 1 \\\\ \\ln(x_{\\max}) - \\ln(x_{\\min})  \\text{ if } \\alpha = 1 \\end{cases}\n$$\n区间 $[x_{\\min}, x_{\\max}]$ 被划分为 $B$ 个分箱，其边界为 $\\{x_0, x_1, \\dots, x_B\\}$。虽然没有具体说明，我们假设采用线性分箱方案，其中分箱边界由 $x_i = x_{\\min} + i \\cdot \\frac{x_{\\max}-x_{\\min}}{B}$ 给出，对于 $i=0, \\dots, B$。\n\n一个事件落入给定分箱 $b$（其跨度为区间 $[x_{b-1}, x_b]$）的概率 $p_b(\\alpha)$ 由该分箱上的 PDF 积分给出：\n$$\np_b(\\alpha) = \\int_{x_{b-1}}^{x_b} f(x \\mid \\alpha) \\, dx = C(\\alpha) \\int_{x_{b-1}}^{x_b} x^{-\\alpha} \\, dx\n$$\n代入积分和归一化常数的表达式，我们得到归一化的分箱概率：\n$$\np_b(\\alpha) = \\begin{cases} \\frac{x_b^{1-\\alpha} - x_{b-1}^{1-\\alpha}}{x_{\\max}^{1-\\alpha} - x_{\\min}^{1-\\alpha}}  \\text{ if } \\alpha \\neq 1 \\\\ \\frac{\\ln(x_b) - \\ln(x_{b-1})}{\\ln(x_{\\max}) - \\ln(x_{\\min})}  \\text{ if } \\alpha = 1 \\end{cases}\n$$\n根据构造，这些概率的总和为 1：$\\sum_{b=1}^B p_b(\\alpha) = 1$。\n\n#### 1.2. 分箱似然函数（任务1）\n\n每个分箱 $b$ 中的观测计数 $n_b$ 被建模为一个独立的泊松随机变量，其均值为 $\\lambda_b(\\alpha) = \\mu \\, p_b(\\alpha)$，其中 $\\mu$ 是已知的总期望事件数。对于一组观测计数 $\\{n_1, \\dots, n_B\\}$，联合似然函数 $L(\\alpha)$ 是各个泊松概率的乘积：\n$$\nL(\\alpha) = \\prod_{b=1}^B P(n_b \\mid \\lambda_b(\\alpha)) = \\prod_{b=1}^B \\frac{e^{-\\lambda_b(\\alpha)} \\lambda_b(\\alpha)^{n_b}}{n_b!}\n$$\n为了找到 $\\alpha$ 的最大似然估计（MLE），我们最大化对数似然 $\\ln L(\\alpha)$：\n$$\n\\ln L(\\alpha) = \\sum_{b=1}^B \\left( n_b \\ln[\\lambda_b(\\alpha)] - \\lambda_b(\\alpha) - \\ln(n_b!) \\right)\n$$\n代入 $\\lambda_b(\\alpha) = \\mu p_b(\\alpha)$：\n$$\n\\ln L(\\alpha) = \\sum_{b=1}^B \\left( n_b \\ln[\\mu p_b(\\alpha)] - \\mu p_b(\\alpha) - \\ln(n_b!) \\right)\n$$\n我们可以将依赖于 $\\alpha$ 的项与不依赖于 $\\alpha$ 的项分开：\n$$\n\\ln L(\\alpha) = \\sum_{b=1}^B n_b \\ln p_b(\\alpha) + \\sum_{b=1}^B n_b \\ln \\mu - \\sum_{b=1}^B \\mu p_b(\\alpha) - \\sum_{b=1}^B \\ln(n_b!)\n$$\n使用恒等式 $\\sum_{b=1}^B p_b(\\alpha) = 1$ 并定义总观测计数 $N = \\sum_{b=1}^B n_b$：\n$$\n\\ln L(\\alpha) = \\left( \\sum_{b=1}^B n_b \\ln p_b(\\alpha) \\right) + N \\ln \\mu - \\mu - \\sum_{b=1}^B \\ln(n_b!)\n$$\n最后三项（$N \\ln \\mu$、$-\\mu$ 和 $-\\sum \\ln(n_b!)$）相对于参数 $\\alpha$ 是常数。因此，最大化 $\\ln L(\\alpha)$ 完全等同于最大化以下项：\n$$\nS(\\alpha) = \\sum_{b=1}^B n_b \\ln p_b(\\alpha)\n$$\n这是具有概率 $\\{p_b(\\alpha)\\}$ 的计数 $\\{n_b\\}$ 的多项分布的对数似然。此推导完成了任务1。\n\n#### 1.3. 费雪信息和克拉默-拉奥下界（任务2）\n\n费雪信息 $I(\\alpha)$ 量化了可观测数据中包含的关于未知参数 $\\alpha$ 的信息量。其定义为：\n$$\nI(\\alpha) = \\mathbb{E}\\left[ \\left( \\frac{\\partial}{\\partial \\alpha} \\ln L(\\alpha) \\right)^2 \\right] = -\\mathbb{E}\\left[ \\frac{\\partial^2}{\\partial \\alpha^2} \\ln L(\\alpha) \\right]\n$$\n我们使用第二种形式。对数似然关于 $\\alpha$ 的二阶导数是：\n$$\n\\frac{\\partial^2 \\ln L}{\\partial \\alpha^2} = \\frac{\\partial^2}{\\partial \\alpha^2} \\sum_{b=1}^B \\left( n_b \\ln p_b(\\alpha) - \\mu p_b(\\alpha) \\right) = \\sum_{b=1}^B \\left[ n_b \\left( \\frac{1}{p_b} \\frac{\\partial^2 p_b}{\\partial \\alpha^2} - \\frac{1}{p_b^2} \\left(\\frac{\\partial p_b}{\\partial \\alpha}\\right)^2 \\right) - \\mu \\frac{\\partial^2 p_b}{\\partial \\alpha^2} \\right]\n$$\n取期望值，我们使用事实 $\\mathbb{E}[n_b] = \\lambda_b(\\alpha) = \\mu p_b(\\alpha)$：\n$$\n\\mathbb{E}\\left[\\frac{\\partial^2 \\ln L}{\\partial \\alpha^2}\\right] = \\sum_{b=1}^B \\left[ \\mu p_b \\left( \\frac{1}{p_b} \\frac{\\partial^2 p_b}{\\partial \\alpha^2} - \\frac{1}{p_b^2} \\left(\\frac{\\partial p_b}{\\partial \\alpha}\\right)^2 \\right) - \\mu \\frac{\\partial^2 p_b}{\\partial \\alpha^2} \\right]\n$$\n简化表达式：\n$$\n\\mathbb{E}\\left[\\frac{\\partial^2 \\ln L}{\\partial \\alpha^2}\\right] = \\sum_{b=1}^B \\left[ \\mu \\frac{\\partial^2 p_b}{\\partial \\alpha^2} - \\frac{\\mu}{p_b} \\left(\\frac{\\partial p_b}{\\partial \\alpha}\\right)^2 - \\mu \\frac{\\partial^2 p_b}{\\partial \\alpha^2} \\right] = - \\mu \\sum_{b=1}^B \\frac{1}{p_b(\\alpha)} \\left(\\frac{\\partial p_b(\\alpha)}{\\partial \\alpha}\\right)^2\n$$\n费雪信息是这个量的负值：\n$$\nI(\\alpha) = \\mu \\sum_{b=1}^B \\frac{1}{p_b(\\alpha)} \\left(\\frac{\\partial p_b(\\alpha)}{\\partial \\alpha}\\right)^2\n$$\n使用恒等式 $\\frac{\\partial \\ln(f)}{\\partial x} = \\frac{1}{f} \\frac{\\partial f}{\\partial x}$，我们可以将其写成标准的“每事件”形式：\n$$\nI(\\alpha) = \\mu \\sum_{b=1}^B p_b(\\alpha) \\left( \\frac{1}{p_b(\\alpha)} \\frac{\\partial p_b(\\alpha)}{\\partial \\alpha} \\right)^2 = \\mu \\sum_{b=1}^B p_b(\\alpha) \\left( \\frac{\\partial \\ln p_b(\\alpha)}{\\partial \\alpha} \\right)^2\n$$\n这是 $I(\\alpha) = \\mu I_1(\\alpha)$ 的形式，其中 $I_1(\\alpha) = \\sum_b p_b(\\alpha) (\\frac{\\partial \\ln p_b(\\alpha)}{\\partial \\alpha})^2$ 是每事件的费雪信息。此推导完成了任务2。\n\n克拉默-拉奥下界（CRLB）指出，对于任何无偏估计量 $\\hat{\\alpha}$，其方差的下界是费雪信息的倒数：$\\operatorname{Var}(\\hat{\\alpha}) \\ge I(\\alpha)^{-1}$。对于大样本量，最大似然估计的方差 $\\operatorname{Var}(\\hat{\\alpha})$ 会渐近地接近这个界限。因此，理论最小方差为：\n$$\n\\operatorname{Var}_{\\text{CRLB}}(\\hat{\\alpha}) = \\frac{1}{I(\\alpha)} = \\frac{1}{\\mu I_1(\\alpha)}\n$$\n\n### 2. 计算方法\n\n该研究通过在模拟中实现理论框架来进行。\n\n#### 2.1. 蒙特卡洛模拟（任务3）\n\n对于每个测试用例，我们执行 $R$ 次伪实验。单次实验包括：\n1.  定义物理设置：$\\alpha_{\\text{true}}, \\mu, B, x_{\\min}, x_{\\max}$。\n2.  计算真实的分箱概率 $p_b(\\alpha_{\\text{true}})$ 和真实的期望分箱计数 $\\lambda_b = \\mu p_b(\\alpha_{\\text{true}})$。\n3.  通过从独立的泊松分布中抽样生成一组伪观测计数 $\\{n_b\\}_{b=1}^B$：$n_b \\sim \\text{Poisson}(\\lambda_b)$。\n\n#### 2.2. 估计与性能度量（任务3和4）\n\n对于每个伪实验数据集 $\\{n_b\\}$，通过在预定义的 $\\alpha$ 值网格上最大化目标函数 $S(\\alpha) = \\sum_{b} n_b \\ln p_b(\\alpha)$ 来找到最大似然估计 $\\hat{\\alpha}$。这是通过为网格上的每个点 $\\alpha_j$ 计算 $S(\\alpha_j)$ 并找出产生最大值的 $\\alpha_j$ 来实现的。为了避免 $\\ln(0)$ 带来的数值错误，在取对数之前对所有 $p_b$ 值应用一个小的正下限。\n\n在运行所有 $R$ 次实验后，我们得到一个包含 $R$ 个估计值的分布 $\\{\\hat{\\alpha}_1, \\dots, \\hat{\\alpha}_R\\}$。经验方差 $\\operatorname{Var}_{\\text{emp}}(\\hat{\\alpha})$ 从这个样本中计算得出。\n\nCRLB 方差 $\\operatorname{Var}_{\\text{CRLB}}(\\hat{\\alpha})$ 在真实参数值 $\\alpha_{\\text{true}}$ 处计算。计算费雪信息 $I_1(\\alpha)$ 所需的导数 $\\frac{\\partial \\ln p_b(\\alpha)}{\\partial \\alpha}$ 使用二阶中心有限差分近似法进行数值计算：\n$$\n\\frac{\\partial \\ln p_b(\\alpha)}{\\partial \\alpha} \\approx \\frac{\\ln p_b(\\alpha+h) - \\ln p_b(\\alpha-h)}{2h}\n$$\n其中 $h$ 为一个小的步长。\n\n最后，“方差膨胀”因子计算为经验方差与理论下限的比率：\n$$\n\\text{方差膨胀} = \\frac{\\operatorname{Var}_{\\text{emp}}(\\hat{\\alpha})}{\\operatorname{Var}_{\\text{CRLB}}(\\hat{\\alpha})}\n$$\n这个比率量化了估计量的实际性能与 CRLB 预测的理想渐近极限的接近程度。接近 1 的比率表示一个有效的估计量，而大于 1 的比率则表明 CRLB 对真实不确定性的估计过于乐观，这在稀疏数据情况下是常见现象。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs a simulation study of binned maximum likelihood estimation for a power-law spectrum.\n    It calculates the variance inflation factor, which is the ratio of the empirical variance\n    of the estimated slope parameter to its theoretical Cramér-Rao lower bound.\n    \"\"\"\n\n    test_cases = [\n        # (alpha_true, mu, B, x_min, x_max)\n        (3.0, 1000.0, 20, 1.0, 10.0),\n        (3.0, 50.0, 80, 1.0, 20.0),\n        (4.0, 15.0, 80, 1.0, 50.0),\n        (5.0, 15.0, 40, 1.0, 50.0)\n    ]\n\n    # Global parameters from the problem statement\n    alpha_min, alpha_max = 0.5, 6.0\n    N_alpha = 1001\n    h = 1e-5\n    R = 600\n    s0 = 1729\n    prob_floor = 1e-30\n\n    alpha_grid = np.linspace(alpha_min, alpha_max, N_alpha)\n    results = []\n\n    def calculate_probabilities(alphas, x_edges):\n        \"\"\"\n        Calculates bin probabilities for a power law f(x) ~ x^-alpha for a vector of alphas.\n        \"\"\"\n        alphas = np.atleast_1d(alphas)\n        num_alphas = len(alphas)\n        num_bins = len(x_edges) - 1\n        \n        x_b = x_edges[1:]   # Shape (B,)\n        x_bm1 = x_edges[:-1] # Shape (B,)\n        \n        probs = np.zeros((num_alphas, num_bins), dtype=np.float64)\n        \n        # --- Case alpha != 1 ---\n        mask_ne1 = ~np.isclose(alphas, 1.0)\n        if np.any(mask_ne1):\n            alpha_ne1 = alphas[mask_ne1]\n            power = 1.0 - alpha_ne1\n            \n            # Use broadcasting for bin integrals\n            unnormalized_integrals = (np.power.outer(x_b, power) - np.power.outer(x_bm1, power)) / power\n            \n            # Total integral for normalization\n            total_integral = (np.power(x_edges[-1], power) - np.power(x_edges[0], power)) / power\n            \n            probs[mask_ne1, :] = (unnormalized_integrals / total_integral).T\n\n        # --- Case alpha == 1 ---\n        mask_eq1 = np.isclose(alphas, 1.0)\n        if np.any(mask_eq1):\n            unnormalized_integrals = np.log(x_b) - np.log(x_bm1)\n            total_integral = np.log(x_edges[-1]) - np.log(x_edges[0])\n            probs[mask_eq1, :] = unnormalized_integrals / total_integral\n            \n        return probs.squeeze()\n\n    for k, case in enumerate(test_cases):\n        alpha_true, mu, B, x_min, x_max = case\n        seed = s0 + k\n        rng = np.random.default_rng(seed)\n\n        x_edges = np.linspace(x_min, x_max, B + 1)\n        \n        # --- 1. Calculate CRLB Variance ---\n        # Get probabilities around alpha_true for finite difference\n        p_alpha_plus_h = calculate_probabilities(alpha_true + h, x_edges)\n        p_alpha_minus_h = calculate_probabilities(alpha_true - h, x_edges)\n        p_true = calculate_probabilities(alpha_true, x_edges)\n\n        # Apply floor to avoid log(0)\n        p_alpha_plus_h_clipped = np.maximum(p_alpha_plus_h, prob_floor)\n        p_alpha_minus_h_clipped = np.maximum(p_alpha_minus_h, prob_floor)\n        p_true_clipped = np.maximum(p_true, prob_floor)\n\n        # Numerical derivative of log probabilities\n        d_logp_d_alpha = (np.log(p_alpha_plus_h_clipped) - np.log(p_alpha_minus_h_clipped)) / (2 * h)\n\n        # Per-event Fisher Information I_1(alpha)\n        I1_true = np.sum(p_true_clipped * (d_logp_d_alpha**2))\n\n        # CRLB variance\n        var_crlb = 1.0 / (mu * I1_true)\n\n        # --- 2. Monte Carlo Simulation ---\n        # Pre-calculate probabilities for all alpha grid points\n        prob_matrix = calculate_probabilities(alpha_grid, x_edges)\n        prob_matrix_clipped = np.maximum(prob_matrix, prob_floor)\n        log_prob_matrix = np.log(prob_matrix_clipped)\n\n        # True expected counts\n        lambda_true = mu * p_true\n        \n        alpha_estimates = []\n        for _ in range(R):\n            # Generate pseudo-data\n            n_b = rng.poisson(lambda_true)\n            \n            # Calculate objective function over the alpha grid\n            # This is equivalent to sum(n_b * log(p_b(alpha))) for each alpha\n            log_likelihoods = log_prob_matrix @ n_b\n            \n            # Find MLE for alpha\n            best_alpha_idx = np.argmax(log_likelihoods)\n            hat_alpha = alpha_grid[best_alpha_idx]\n            alpha_estimates.append(hat_alpha)\n\n        # --- 3. Compute Variance Inflation ---\n        # Empirical variance of the estimator (using ddof=1 for sample variance)\n        var_emp = np.var(alpha_estimates, ddof=1)\n        \n        variance_inflation = var_emp / var_crlb\n        results.append(str(variance_inflation))\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "我们可以将分箱看作是一种选择“概要统计量”(summary statistics)的特例，即将完整的事件样本压缩为一组箱计数。本练习将这一概念推广，探讨如何从所有箱中选择一个子集作为概要统计量，并量化由此带来的灵敏度损失。通过对比“最优”选择策略和“朴素”选择策略，本练习强调了明智地选择信息丰富的概要统计量对于保持分析的统计功效至关重要 。",
            "id": "3540393",
            "problem": "考虑一个计算高能物理中典型的一维分箱计数实验，有 $m$ 个等宽的箱（bin）覆盖了 $x \\in [0,1]$ 的范围。事件被建模为每个箱中独立的泊松计数。第 $i$ 个箱（其箱边界为 $x_{i-1} = (i-1)/m$ 和 $x_i = i/m$）中的期望计数由下式给出\n$$\n\\mu_i(\\theta) \\;=\\; \\lambda_b \\,\\Delta \\;+\\; \\lambda_s \\int_{x_{i-1}}^{x_i} \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right)\\,dx,\n$$\n其中 $\\Delta = 1/m$ 是箱宽，$\\lambda_b$ 是均匀本底率，$\\lambda_s$ 是信号幅度，$\\sigma$ 是信号的固定宽度，而 $\\theta \\in (0,1)$ 是信号峰值的未知位置参数。所有量均为无量纲。假设每个箱中的泊松涨落是独立的。\n\n将期望计数 $\\mu_i(\\theta)$ 和观测计数 $n_i$ 的完整分箱泊松似然定义为\n$$\n\\mathcal{L}_{\\text{full}}(\\theta) \\;=\\; \\prod_{i=1}^{m} \\mathrm{Poisson}\\!\\left(n_i \\,\\middle|\\, \\mu_i(\\theta)\\right).\n$$\n在 Asimov 机制（逐点将 $n_i$ 替换为 $\\mu_i(\\theta)$）下，完整分箱泊松模型中 $\\theta$ 的费雪信息为\n$$\nI_{\\text{full}}(\\theta) \\;=\\; \\sum_{i=1}^{m} \\frac{\\left(\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}\\right)^2}{\\mu_i(\\theta)}.\n$$\n\n现在考虑一个近似贝叶斯计算（ABC）风格的汇总统计拟合：我们不使用完整的计数向量，而是选择一个维度为 $p$ 的汇总向量，该向量由索引集 $S \\subset \\{1,\\dots,m\\}$（其中 $|S|=p$）中各箱的计数 $n_i$ 组成。用高斯合成似然对这些汇总统计量进行建模，其对角协方差等于相应的泊松方差，\n$$\n\\mathcal{L}_{\\text{sum}}(\\theta) \\;\\propto\\; \\exp\\!\\left(-\\frac{1}{2}\\sum_{i \\in S} \\frac{\\big(n_i - \\mu_i(\\theta)\\big)^2}{\\mu_i(\\theta)}\\right),\n$$\n这得出了汇总统计模型下 $\\theta$ 的费雪信息\n$$\nI_{\\text{sum}}(\\theta) \\;=\\; \\sum_{i \\in S} \\frac{\\left(\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}\\right)^2}{\\mu_i(\\theta)}.\n$$\n\n定义灵敏度比率\n$$\nR(p, S, \\theta) \\;=\\; \\frac{I_{\\text{sum}}(\\theta)}{I_{\\text{full}}(\\theta)},\n$$\n以及相应的灵敏度损失\n$$\nL(p, S, \\theta) \\;=\\; 1 - R(p, S, \\theta).\n$$\n目标是量化当汇总维度 $p$ 变化以及索引集 $S$ 的选择改变时，灵敏度的损失。\n\n您的任务是实现一个完整的程序，该程序：\n- 使用 $m = 64$、$\\sigma = 0.05$、$\\lambda_b = 400$ 和 $\\lambda_s = 1200$ 作为基线模型参数。\n- 通过中点求积法对每个箱进行数值计算，求得 $\\mu_i(\\theta)$ 和 $\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}$，每个箱至少使用 $200$ 个均匀样本。\n- 使用 Asimov 约定计算 $I_{\\text{full}}(\\theta)$。\n- 对于给定的汇总维度 $p$ 和 $S$ 的选择规则，计算 $I_{\\text{sum}}(\\theta)$ 和灵敏度比率 $R(p, S, \\theta)$。\n\n必须支持两种 $S$ 的选择规则：\n- 最佳-$p$（Best-$p$）：选择由 $|\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}|$ 值最大的 $p$ 个箱组成的索引集 $S$。\n- 朴素前-$p$（Naive-first-$p$）：选择 $S = \\{1,2,\\dots,p\\}$（按索引顺序的前 $p$ 个箱）。\n\n为了测试解决方案的不同方面，请使用以下参数元组 $(p, \\theta, \\text{rule}, \\alpha)$ 的测试套件，其中 `rule` 为 \"best\" 或 \"first\"，且 $\\alpha$ 通过将 $\\lambda_s$ 替换为 $\\alpha \\lambda_s$ 来缩放信号幅度：\n1. $(4, 0.50, \\text{\"best\"}, 1.0)$\n2. $(16, 0.50, \\text{\"best\"}, 1.0)$\n3. $(64, 0.50, \\text{\"best\"}, 1.0)$\n4. $(16, 0.05, \\text{\"best\"}, 1.0)$\n5. $(16, 0.50, \\text{\"first\"}, 1.0)$\n6. $(16, 0.50, \\text{\"best\"}, 0.2)$\n\n对于每个测试用例，以浮点数形式输出灵敏度比率 $R(p, S, \\theta)$。您的程序应生成单行输出，其中包含六个结果，以逗号分隔的列表形式包含在方括号内，每个浮点数四舍五入到六位小数（例如，$[0.123456,0.654321,\\dots]$）。不涉及物理单位；所有量均为无量纲。不出现角度。不得使用百分比；输出为原始十进制小数。",
            "solution": "该问题陈述清晰，科学上合理，并为得出唯一解提供了所有必要信息。这是计算高能物理统计学中的一个标准练习，涉及分箱似然、费雪信息和汇总统计量。因此，该问题被认为是有效的。\n\n问题的核心是计算灵敏度比率 $R(p, S, \\theta) = I_{\\text{sum}}(\\theta) / I_{\\text{full}}(\\theta)$，该比率量化了从完整的箱计数集转移到较小的汇总统计子集时的信息损失。这需要计算一个分箱计数实验的两种不同统计模型的费雪信息。\n\n首先，我们定义第 $i$ 个箱中预期事件数 $\\mu_i(\\theta)$ 的模型。$m$ 个箱将域 $x \\in [0, 1]$ 分割成区间 $[x_{i-1}, x_i]$，其中 $x_i = i/m$，$i=1, \\dots, m$。箱宽为 $\\Delta = 1/m$。期望计数是均匀本底分量和高斯信号分量的总和：\n$$\n\\mu_i(\\theta) \\;=\\; \\lambda_b \\,\\Delta \\;+\\; \\lambda_s \\int_{x_{i-1}}^{x_i} \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right)\\,dx\n$$\n在这里，$\\lambda_b$ 是本底率，$\\lambda_s$ 是信号幅度，$\\sigma$ 是信号宽度，$\\theta$ 是信号峰值位置。\n\n在 Asimov 机制中，观测计数 $n_i$ 被其期望值 $\\mu_i(\\theta)$ 替代，费雪信息由下式给出：\n$$\nI(\\theta) \\;=\\; \\sum_{i} \\frac{\\left(\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}\\right)^2}{\\mu_i(\\theta)}\n$$\n对于完整模型（$I_{\\text{full}}$），求和遍及所有箱；对于汇总模型（$I_{\\text{sum}}$），求和遍及所选的箱子集 $S$。为了计算这个值，我们需要导数 $\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}$。由于箱的边界不依赖于 $\\theta$，我们可以在积分符号下进行微分（莱布尼茨法则）：\n$$\n\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta} \\;=\\; \\frac{\\partial}{\\partial \\theta} \\left( \\lambda_s \\int_{x_{i-1}}^{x_i} \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right)\\,dx \\right) \\;=\\; \\lambda_s \\int_{x_{i-1}}^{x_i} \\frac{\\partial}{\\partial \\theta} \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right)\\,dx\n$$\n被积函数的导数是：\n$$\n\\frac{\\partial}{\\partial \\theta} \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right) \\;=\\; \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right) \\cdot \\left( -\\frac{2(x-\\theta)(-1)}{2\\sigma^2} \\right) \\;=\\; \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right) \\frac{x-\\theta}{\\sigma^2}\n$$\n因此，期望计数的导数表示为另一个积分：\n$$\n\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta} \\;=\\; \\lambda_s \\int_{x_{i-1}}^{x_i} \\exp\\!\\left(-\\frac{(x - \\theta)^2}{2\\sigma^2}\\right)\\frac{x-\\theta}{\\sigma^2}\\,dx\n$$\n问题指定，计算 $\\mu_i(\\theta)$ 的积分和其导数的积分都必须使用中点求积法进行数值计算，每个箱使用 $N_{quad} \\ge 200$ 个样本。对于一个通用积分 $\\int_a^b f(x)dx$，该方法将其值近似为 $(b-a) \\cdot \\frac{1}{N_{quad}} \\sum_{k=1}^{N_{quad}} f(x_k)$，其中点 $x_k$ 是将 $[a,b]$ 分割成 $N_{quad}$ 个相等子区间的中点。对于宽度为 $\\Delta$ 的第 $i$ 个箱，我们使用 $N_{quad}=200$ 个点。\n\n对于每个测试用例 $(p, \\theta, \\text{rule}, \\alpha)$，总体算法如下：\n1. 设置模型参数：$m=64$、$\\sigma=0.05$、$\\lambda_b=400$ 和 $\\lambda_s = \\alpha \\cdot 1200$。\n2. 对于每个箱 $i=1, \\dots, m$：\n   a. 定义箱边界 $x_{i-1}=(i-1)/m$ 和 $x_i=i/m$。\n   b. 在 $[x_{i-1}, x_i]$ 内生成 $N_{quad}=200$ 个求积点。\n   c. 数值计算 $\\mu_i(\\theta)$ 的信号积分部分以及 $\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}$ 的积分。\n   d. 将 $\\mu_i(\\theta)$ 和 $\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}$ 的完整值存储在数组中。\n3. 计算所有 $m$ 个箱的每箱费雪信息项 $\\left(\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}\\right)^2 / \\mu_i(\\theta)$。\n4. 通过对所有 $m$ 个箱的这些项求和来计算完整的费雪信息 $I_{\\text{full}}(\\theta)$。\n5. 根据给定的 `rule` 确定 $p$ 个箱的索引集 $S$：\n   - `\"best\"`：选择 $|\\frac{\\partial \\mu_i(\\theta)}{\\partial \\theta}|$ 值最大的 $p$ 个箱。这对应于对 $\\theta$ 变化最敏感的箱。\n   - `\"first\"`：选择前 $p$ 个箱，即 $S = \\{1, 2, \\dots, p\\}$。\n6. 通过对集合 $S$ 中的箱的每箱信息项求和，计算汇总统计费雪信息 $I_{\\text{sum}}(\\theta)$。\n7. 计算灵敏度比率 $R(p, S, \\theta) = I_{\\text{sum}}(\\theta) / I_{\\text{full}}(\\theta)$。对于 $p=m$ 的情况，该比率应为 $1$。\n\n此过程在以下 Python 程序中实现。它遍历指定的测试用例，为每个用例计算比率，并将结果格式化为所需的单行输出。",
            "answer": "```python\nimport numpy as np\n\ndef compute_sensitivity_ratio(p, theta, rule, alpha, m, sigma, lambda_b, lambda_s_base, n_quad):\n    \"\"\"\n    Computes the sensitivity ratio R(p, S, theta) for a given set of parameters.\n    \"\"\"\n    # Set model parameters for the current test case\n    lambda_s = alpha * lambda_s_base\n    delta = 1.0 / m\n    sigma2 = sigma**2\n\n    # Arrays to store bin-wise calculations\n    mu = np.zeros(m)\n    dmu_dtheta = np.zeros(m)\n    \n    # Pre-calculate quadrature offsets for re-use in each bin\n    # These are the midpoints of n_quad sub-intervals of width 1.\n    quad_offsets = (np.arange(n_quad) + 0.5) / n_quad\n\n    # Loop over each bin to calculate mu_i and its derivative\n    for i in range(m):\n        bin_edge_low = i / m\n        \n        # Generate N_quad sample points at the midpoints of sub-intervals within the current bin\n        x_samples = bin_edge_low + quad_offsets * delta\n        \n        # Evaluate the integrands at the sample points\n        signal_integrand_values = np.exp(-(x_samples - theta)**2 / (2 * sigma2))\n        deriv_integrand_values = signal_integrand_values * (x_samples - theta) / sigma2\n        \n        # Approximate the integrals using the mean of the integrand values\n        integral_signal = np.mean(signal_integrand_values) * delta\n        integral_deriv = np.mean(deriv_integrand_values) * delta\n        \n        # Compute the final mu_i and d(mu_i)/d(theta)\n        mu[i] = lambda_b * delta + lambda_s * integral_signal\n        dmu_dtheta[i] = lambda_s * integral_deriv\n\n    # Calculate the per-bin terms for Fisher information\n    # Adding a small epsilon to the denominator for numerical stability, though mu > 0 is guaranteed by background.\n    info_terms = (dmu_dtheta**2) / (mu + 1e-15)\n\n    # Calculate the full Fisher Information by summing over all bins\n    I_full = np.sum(info_terms)\n\n    # If I_full is zero, the ratio is ill-defined. This should not happen with the given parameters.\n    # Return a meaningful value based on p.\n    if I_full == 0:\n        return 1.0 if p == m else 0.0\n\n    # Determine the index set S based on the selection rule\n    if rule == \"best\":\n        # Select indices of the p bins with the largest absolute derivative\n        abs_dmu = np.abs(dmu_dtheta)\n        # argsort sorts in ascending order, so we take the last p indices\n        indices_S = np.argsort(abs_dmu)[-p:]\n    elif rule == \"first\":\n        # Select the first p bins\n        indices_S = np.arange(p)\n    else:\n        # This case should not be reached with the given problem statement\n        raise ValueError(f\"Unknown selection rule: {rule}\")\n\n    # Calculate the summary-statistic Fisher Information by summing over the selected bins\n    I_sum = np.sum(info_terms[indices_S])\n\n    # Compute the final sensitivity ratio\n    R = I_sum / I_full\n    \n    return R\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Baseline model parameters from the problem statement\n    m = 64\n    sigma = 0.05\n    lambda_b = 400.0\n    lambda_s_base = 1200.0\n    n_quad = 200\n\n    # Test suite of parameter tuples (p, theta, rule, alpha)\n    test_cases = [\n        (4, 0.50, \"best\", 1.0),\n        (16, 0.50, \"best\", 1.0),\n        (64, 0.50, \"best\", 1.0),\n        (16, 0.05, \"best\", 1.0),\n        (16, 0.50, \"first\", 1.0),\n        (16, 0.50, \"best\", 0.2),\n    ]\n\n    results = []\n    for p, theta, rule, alpha in test_cases:\n        ratio = compute_sensitivity_ratio(\n            p, theta, rule, alpha,\n            m, sigma, lambda_b, lambda_s_base, n_quad\n        )\n        # Format the result to six decimal places\n        results.append(f\"{ratio:.6f}\")\n\n    # Print the final output in the required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}