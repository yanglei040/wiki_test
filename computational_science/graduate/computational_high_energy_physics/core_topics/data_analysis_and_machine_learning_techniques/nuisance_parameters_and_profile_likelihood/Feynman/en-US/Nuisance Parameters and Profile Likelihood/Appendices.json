{
    "hands_on_practices": [
        {
            "introduction": "To build a solid intuition for profile likelihood, we begin with a foundational exercise. This practice  strips the problem down to its mathematical core by using a generic quadratic log-likelihood, which is a common and powerful local approximation for many real-world likelihood functions near their maximum. By working through this simplified case, you will see precisely how the procedure of \"profiling out\" a nuisance parameter corresponds to solving the standard optimization conditions and will derive the analytical form of the resulting profiled likelihood.",
            "id": "3524873",
            "problem": "In a simplified computational High-Energy Physics (HEP) signal-strength fit, consider a two-parameter log-likelihood approximation for a parameter of interest $\\mu$ (signal strength) and a single nuisance parameter $\\theta$ (a background normalization), obtained from a second-order Taylor expansion around the global maximum. The log-likelihood is modeled as a strictly concave quadratic of the form\n$$\n\\ell(\\mu,\\theta) \\equiv \\ln L(\\mu,\\theta) \\;=\\; -\\frac{1}{2}\\left[a\\,\\mu^{2} + 2 b\\,\\mu\\,\\theta + c\\,\\theta^{2}\\right] \\;+\\; s\\,\\mu \\;+\\; t\\,\\theta \\;+\\; k,\n$$\nwhere $a$, $b$, $c$, $s$, $t$, and $k$ are real constants determined by the data and the local curvature of the likelihood; the constants satisfy $c>0$ and $a c - b^{2} > 0$ so that the quadratic form is strictly concave in $(\\mu,\\theta)$. The profile likelihood for $\\mu$ is defined by the foundational definition of profiling in maximum likelihood estimation as\n$$\n\\tilde{L}(\\mu) \\;=\\; \\max_{\\theta} \\, L(\\mu,\\theta) \\;=\\; \\exp\\!\\left(\\max_{\\theta}\\, \\ell(\\mu,\\theta)\\right).\n$$\nUsing only the definitions above and standard calculus of optimization for concave functions, show that profiling corresponds to eliminating $\\theta$ by solving the normal equation obtained from $\\partial \\ell/\\partial \\theta = 0$, and derive a closed-form expression for the profiled likelihood $\\tilde{L}(\\mu)$ as a function of $\\mu$ and the constants $a$, $b$, $c$, $s$, $t$, and $k$. Provide the final expression for $\\tilde{L}(\\mu)$ in closed form. Your final answer must be a single analytic expression. Do not include units. Do not round.",
            "solution": "The problem is to derive a closed-form expression for the profile likelihood $\\tilde{L}(\\mu)$ for a bivariate log-likelihood function $\\ell(\\mu, \\theta)$ that is quadratic in the parameter of interest $\\mu$ and the nuisance parameter $\\theta$.\n\nThe provided log-likelihood function is:\n$$\n\\ell(\\mu,\\theta) = -\\frac{1}{2}\\left[a\\,\\mu^{2} + 2 b\\,\\mu\\,\\theta + c\\,\\theta^{2}\\right] + s\\,\\mu + t\\,\\theta + k\n$$\nThe constants satisfy $c>0$ and $a c - b^{2} > 0$, which ensures that the quadratic form is strictly concave, meaning the function has a unique global maximum.\n\nThe profile likelihood for $\\mu$, denoted $\\tilde{L}(\\mu)$, is defined as the likelihood function maximized over the space of the nuisance parameter $\\theta$ for a fixed value of $\\mu$:\n$$\n\\tilde{L}(\\mu) = \\max_{\\theta} L(\\mu,\\theta)\n$$\nSince the natural logarithm is a strictly monotonically increasing function, maximizing the likelihood $L(\\mu,\\theta)$ is equivalent to maximizing the log-likelihood $\\ell(\\mu,\\theta) = \\ln L(\\mu,\\theta)$. Therefore, we can write:\n$$\n\\tilde{L}(\\mu) = \\exp\\left(\\max_{\\theta} \\ell(\\mu,\\theta)\\right)\n$$\nLet us define the profiled log-likelihood as $\\tilde{\\ell}(\\mu) = \\max_{\\theta} \\ell(\\mu,\\theta)$. Our task is to first find $\\tilde{\\ell}(\\mu)$.\n\nFor a fixed value of $\\mu$, the log-likelihood $\\ell(\\mu,\\theta)$ can be viewed as a function of $\\theta$ alone:\n$$\n\\ell(\\theta | \\mu) = \\left(-\\frac{c}{2}\\right)\\theta^2 + (t - b\\mu)\\theta + \\left(-\\frac{a}{2}\\mu^2 + s\\mu + k\\right)\n$$\nThis is a quadratic function of $\\theta$. Since we are given that $c > 0$, the coefficient of the $\\theta^2$ term, $-c/2$, is negative. This confirms that the function is a strictly downward-opening parabola in $\\theta$, and thus is strictly concave with a unique maximum.\n\nThe maximum of a differentiable concave function is found at the critical point where its first derivative is zero. This is the origin of the \"normal equations\" in least-squares and maximum likelihood problems. This step demonstrates that the procedure of profiling is equivalent to eliminating the nuisance parameter by solving the normal equation $\\partial \\ell/\\partial \\theta = 0$.\n\nWe compute the partial derivative of $\\ell(\\mu,\\theta)$ with respect to $\\theta$:\n$$\n\\frac{\\partial \\ell}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left(-\\frac{1}{2}a\\mu^2 - b\\mu\\theta - \\frac{1}{2}c\\theta^2 + s\\mu + t\\theta + k \\right)\n$$\nTreating $\\mu$ as a constant, we get:\n$$\n\\frac{\\partial \\ell}{\\partial \\theta} = -b\\mu - c\\theta + t\n$$\nSetting this derivative to zero gives the normal equation for $\\theta$:\n$$\n-b\\mu - c\\theta + t = 0\n$$\nSolving for $\\theta$ gives the value that maximizes $\\ell$ for a given $\\mu$. Let us denote this conditional maximum likelihood estimate of $\\theta$ as $\\hat{\\hat{\\theta}}(\\mu)$:\n$$\nc\\theta = t - b\\mu \\implies \\hat{\\hat{\\theta}}(\\mu) = \\frac{t - b\\mu}{c}\n$$\nNow, we find the profiled log-likelihood $\\tilde{\\ell}(\\mu)$ by substituting this expression for $\\theta$ back into the original log-likelihood function $\\ell(\\mu,\\theta)$:\n$$\n\\tilde{\\ell}(\\mu) = \\ell(\\mu, \\hat{\\hat{\\theta}}(\\mu)) = -\\frac{1}{2}a\\mu^2 - b\\mu\\left(\\frac{t-b\\mu}{c}\\right) - \\frac{1}{2}c\\left(\\frac{t-b\\mu}{c}\\right)^2 + s\\mu + t\\left(\\frac{t-b\\mu}{c}\\right) + k\n$$\nWe now perform the algebraic expansion and simplification:\n$$\n\\tilde{\\ell}(\\mu) = -\\frac{a}{2}\\mu^2 - \\frac{bt\\mu - b^2\\mu^2}{c} - \\frac{c}{2}\\frac{t^2 - 2bt\\mu + b^2\\mu^2}{c^2} + s\\mu + \\frac{t^2 - bt\\mu}{c} + k\n$$\n$$\n\\tilde{\\ell}(\\mu) = -\\frac{a}{2}\\mu^2 - \\frac{bt\\mu}{c} + \\frac{b^2\\mu^2}{c} - \\frac{t^2 - 2bt\\mu + b^2\\mu^2}{2c} + s\\mu + \\frac{t^2}{c} - \\frac{bt\\mu}{c} + k\n$$\nTo simplify, let's group terms by powers of $\\mu$:\nThe coefficient of $\\mu^2$:\n$$\n-\\frac{a}{2} + \\frac{b^2}{c} - \\frac{b^2}{2c} = -\\frac{a}{2} + \\frac{b^2}{2c} = \\frac{-ac + b^2}{2c} = -\\frac{ac - b^2}{2c}\n$$\nThe coefficient of $\\mu$:\n$$\n-\\frac{bt}{c} + s - \\frac{bt}{c} + \\frac{2bt}{2c} = s - \\frac{2bt}{c} + \\frac{bt}{c} = s - \\frac{bt}{c} = \\frac{sc - bt}{c}\n$$\nThe constant terms (independent of $\\mu$):\n$$\n-\\frac{t^2}{2c} + \\frac{t^2}{c} + k = \\frac{t^2}{2c} + k\n$$\nCombining these gives the profiled log-likelihood:\n$$\n\\tilde{\\ell}(\\mu) = -\\left(\\frac{ac - b^2}{2c}\\right)\\mu^2 + \\left(\\frac{sc - bt}{c}\\right)\\mu + \\left(k + \\frac{t^2}{2c}\\right)\n$$\nThis expression can be written over a common denominator of $2c$:\n$$\n\\tilde{\\ell}(\\mu) = \\frac{-(ac-b^2)\\mu^2 + 2(sc-bt)\\mu + t^2 + 2ck}{2c}\n$$\nFinally, the profile likelihood $\\tilde{L}(\\mu)$ is the exponential of the profiled log-likelihood $\\tilde{\\ell}(\\mu)$:\n$$\n\\tilde{L}(\\mu) = \\exp(\\tilde{\\ell}(\\mu)) = \\exp\\left( -\\frac{ac - b^2}{2c}\\mu^2 + \\left(s - \\frac{bt}{c}\\right)\\mu + k + \\frac{t^2}{2c} \\right)\n$$\nThis is the required closed-form expression for the profiled likelihood.",
            "answer": "$$\\boxed{\\exp\\left( -\\frac{ac - b^2}{2c}\\mu^2 + \\left(s - \\frac{bt}{c}\\right)\\mu + k + \\frac{t^2}{2c} \\right)}$$"
        },
        {
            "introduction": "Having established the mechanics of profiling, we now apply the concept to a more realistic scenario common in high-energy physics. This exercise  moves from an abstract function to a concrete statistical model based on Poisson-distributed counts, where a signal measurement is affected by an efficiency uncertainty. Your task is to compute the profiled Fisher information, a key quantity that directly measures how much statistical precision on our parameter of interest is lost due to the uncertainty in the nuisance parameter, thereby connecting the abstract concept of profiling to the practical reality of measurement precision.",
            "id": "3524856",
            "problem": "Consider two statistically independent counting channels (bins) in a high-energy physics measurement, each modeled by a Poisson distribution. Let the expected count in bin $i \\in \\{1,2\\}$ be\n$$\nm_i(\\mu,\\theta) = \\mu\\, s_i\\,(1+\\theta) + b_i,\n$$\nwhere $\\mu \\ge 0$ is the signal strength parameter of interest, $s_i > 0$ is the known signal yield template for bin $i$ at unit strength, $b_i \\ge 0$ is the known background expectation in bin $i$, and $\\theta$ is a common multiplicative efficiency nuisance parameter modeled by a Gaussian constraint $\\theta \\sim \\mathcal{N}(0,\\sigma_\\theta)$ with known $\\sigma_\\theta > 0$. The observed counts $n_i$ are independent with $n_i \\sim \\text{Poisson}(m_i(\\mu,\\theta))$. The joint likelihood including the Gaussian constraint is\n$$\nL(\\mu,\\theta) \\;=\\; \\prod_{i=1}^{2} \\left[ \\frac{m_i(\\mu,\\theta)^{n_i}\\,\\exp\\!\\big(-m_i(\\mu,\\theta)\\big)}{n_i!} \\right] \\times \\exp\\!\\left(-\\frac{\\theta^2}{2\\,\\sigma_\\theta^2}\\right).\n$$\n\nIn the large-sample (asymptotic) regime, compute the profiled Fisher information $I_{\\mu\\mu}^{\\text{prof}}$ for $\\mu$ at the true parameter values $\\mu=\\mu_0>0$ and $\\theta=0$, using the expected Fisher information derived from the Asimov data set (i.e., replace $n_i$ by $m_i(\\mu_0,0)$ in expectations). Express your final result as a single closed-form analytic expression in terms of $s_1, s_2, b_1, b_2, \\mu_0$, and $\\sigma_\\theta$.\n\nFinally, briefly interpret how $I_{\\mu\\mu}^{\\text{prof}}$ depends on $\\sigma_\\theta$, including the limiting cases $\\sigma_\\theta \\to 0$ and $\\sigma_\\theta \\to \\infty$.\n\nYour final answer must be the analytic expression for $I_{\\mu\\mu}^{\\text{prof}}$. No numerical evaluation is required.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a complete derivation. We proceed to the solution.\n\nThe goal is to compute the profiled Fisher information for the parameter of interest $\\mu$, denoted as $I_{\\mu\\mu}^{\\text{prof}}$. This involves first calculating the full $2 \\times 2$ expected Fisher information matrix for the parameters $(\\mu, \\theta)$ and then using the formula for profiling.\n\nThe joint log-likelihood $\\ell(\\mu,\\theta) = \\ln L(\\mu,\\theta)$ is given by:\n$$\n\\ell(\\mu,\\theta) = \\sum_{i=1}^{2} \\left[ n_i \\ln m_i(\\mu,\\theta) - m_i(\\mu,\\theta) - \\ln(n_i!) \\right] - \\frac{\\theta^2}{2\\sigma_\\theta^2}\n$$\nwhere $m_i(\\mu,\\theta) = \\mu s_i (1+\\theta) + b_i$.\n\nThe elements of the expected Fisher information matrix $I$ are defined as $I_{jk} = -E\\left[ \\frac{\\partial^2 \\ell}{\\partial \\phi_j \\partial \\phi_k} \\right]$, where $\\phi = (\\mu, \\theta)$. In the asymptotic limit, we use the Asimov dataset, which means we replace the observed counts $n_i$ with their expected values, $E[n_i] = m_i$. This greatly simplifies the calculation. The second partial derivative of the log-likelihood is:\n$$\n\\frac{\\partial^2 \\ell}{\\partial \\phi_j \\partial \\phi_k} = \\sum_{i=1}^{2} \\left[ -\\frac{n_i}{m_i^2} \\frac{\\partial m_i}{\\partial \\phi_j} \\frac{\\partial m_i}{\\partial \\phi_k} + \\left(\\frac{n_i}{m_i} - 1\\right) \\frac{\\partial^2 m_i}{\\partial \\phi_j \\partial \\phi_k} \\right] - \\frac{\\partial^2}{\\partial \\phi_j \\partial \\phi_k} \\left( \\frac{\\theta^2}{2\\sigma_\\theta^2} \\right)\n$$\nSetting $n_i = m_i$ for the Asimov dataset, the term $(\\frac{n_i}{m_i} - 1)$ becomes zero. The Fisher information matrix elements are then calculated as:\n$$\nI_{jk} = - \\left. \\frac{\\partial^2 \\ell}{\\partial \\phi_j \\partial \\phi_k} \\right|_{\\substack{n_i=m_i}} = \\sum_{i=1}^{2} \\frac{1}{m_i} \\frac{\\partial m_i}{\\partial \\phi_j} \\frac{\\partial m_i}{\\partial \\phi_k} + (\\text{from constraint})\n$$\nThe constraint term $-\\frac{\\theta^2}{2\\sigma_\\theta^2}$ only contributes to the $I_{\\theta\\theta}$ element, with its second derivative with respect to $\\theta$ being $-\\frac{1}{\\sigma_\\theta^2}$. Thus, the contribution to $I_{\\theta\\theta}$ is $\\frac{1}{\\sigma_\\theta^2}$.\n\nFirst, we compute the partial derivatives of $m_i(\\mu,\\theta)$:\n$$\n\\frac{\\partial m_i}{\\partial \\mu} = s_i(1+\\theta)\n$$\n$$\n\\frac{\\partial m_i}{\\partial \\theta} = \\mu s_i\n$$\nThe problem asks for evaluation at the true parameter values $(\\mu, \\theta) = (\\mu_0, 0)$. At this point, the expected counts are $m_{i0} \\equiv m_i(\\mu_0, 0) = \\mu_0 s_i + b_i$. The derivatives evaluated at this point are:\n$$\n\\left. \\frac{\\partial m_i}{\\partial \\mu} \\right|_{(\\mu_0,0)} = s_i\n$$\n$$\n\\left. \\frac{\\partial m_i}{\\partial \\theta} \\right|_{(\\mu_0,0)} = \\mu_0 s_i\n$$\nNow we compute the elements of the Fisher information matrix $I$, evaluated at $(\\mu_0, 0)$ using the Asimov data $m_{i0}$:\n$$\nI_{\\mu\\mu} = \\sum_{i=1}^{2} \\frac{1}{m_{i0}} \\left( \\left. \\frac{\\partial m_i}{\\partial \\mu} \\right|_{(\\mu_0,0)} \\right)^2 = \\sum_{i=1}^{2} \\frac{s_i^2}{\\mu_0 s_i + b_i}\n$$\n$$\nI_{\\theta\\theta} = \\sum_{i=1}^{2} \\frac{1}{m_{i0}} \\left( \\left. \\frac{\\partial m_i}{\\partial \\theta} \\right|_{(\\mu_0,0)} \\right)^2 + \\frac{1}{\\sigma_\\theta^2} = \\sum_{i=1}^{2} \\frac{(\\mu_0 s_i)^2}{\\mu_0 s_i + b_i} + \\frac{1}{\\sigma_\\theta^2}\n$$\n$$\nI_{\\mu\\theta} = I_{\\theta\\mu} = \\sum_{i=1}^{2} \\frac{1}{m_{i0}} \\left( \\left. \\frac{\\partial m_i}{\\partial \\mu} \\right|_{(\\mu_0,0)} \\right) \\left( \\left. \\frac{\\partial m_i}{\\partial \\theta} \\right|_{(\\mu_0,0)} \\right) = \\sum_{i=1}^{2} \\frac{s_i (\\mu_0 s_i)}{\\mu_0 s_i + b_i} = \\mu_0 \\sum_{i=1}^{2} \\frac{s_i^2}{\\mu_0 s_i + b_i}\n$$\nWe can observe that $I_{\\mu\\theta} = \\mu_0 I_{\\mu\\mu}$. Let's define the quantity $A = I_{\\mu\\mu} = \\sum_{i=1}^{2} \\frac{s_i^2}{\\mu_0 s_i + b_i}$. The Fisher information matrix is:\n$$\nI = \\begin{pmatrix} I_{\\mu\\mu} & I_{\\mu\\theta} \\\\ I_{\\theta\\mu} & I_{\\theta\\theta} \\end{pmatrix} = \\begin{pmatrix} A & \\mu_0 A \\\\ \\mu_0 A & \\mu_0^2 A + \\frac{1}{\\sigma_\\theta^2} \\end{pmatrix}\n$$\nThe profiled Fisher information for $\\mu$ is obtained by eliminating the nuisance parameter $\\theta$. The formula is:\n$$\nI_{\\mu\\mu}^{\\text{prof}} = I_{\\mu\\mu} - I_{\\mu\\theta} (I_{\\theta\\theta})^{-1} I_{\\theta\\mu}\n$$\nSubstituting the expressions for the matrix elements:\n$$\nI_{\\mu\\mu}^{\\text{prof}} = A - (\\mu_0 A) \\left( \\mu_0^2 A + \\frac{1}{\\sigma_\\theta^2} \\right)^{-1} (\\mu_0 A) = A - \\frac{\\mu_0^2 A^2}{\\mu_0^2 A + \\frac{1}{\\sigma_\\theta^2}}\n$$\nTo simplify, we find a common denominator:\n$$\nI_{\\mu\\mu}^{\\text{prof}} = \\frac{A \\left( \\mu_0^2 A + \\frac{1}{\\sigma_\\theta^2} \\right) - \\mu_0^2 A^2}{\\mu_0^2 A + \\frac{1}{\\sigma_\\theta^2}} = \\frac{A \\mu_0^2 A + \\frac{A}{\\sigma_\\theta^2} - \\mu_0^2 A^2}{\\mu_0^2 A + \\frac{1}{\\sigma_\\theta^2}} = \\frac{\\frac{A}{\\sigma_\\theta^2}}{\\mu_0^2 A + \\frac{1}{\\sigma_\\theta^2}}\n$$\nMultiplying the numerator and denominator by $\\sigma_\\theta^2$ gives:\n$$\nI_{\\mu\\mu}^{\\text{prof}} = \\frac{A}{1 + \\mu_0^2 \\sigma_\\theta^2 A}\n$$\nFinally, substituting the expression for $A$ back in, we obtain the full closed-form analytical expression:\n$$\nI_{\\mu\\mu}^{\\text{prof}} = \\frac{\\sum_{i=1}^2 \\frac{s_i^2}{\\mu_0 s_i + b_i}}{1 + \\mu_0^2 \\sigma_\\theta^2 \\sum_{i=1}^2 \\frac{s_i^2}{\\mu_0 s_i + b_i}}\n$$\nThis expression represents the information on the signal strength $\\mu$ after accounting for the uncertainty in the nuisance parameter $\\theta$.\n\nInterpretation:\nThe denominator shows how information about $\\mu$ is degraded by the uncertainty in $\\theta$. The term $I_{\\mu\\mu} = \\sum_{i=1}^2 \\frac{s_i^2}{\\mu_0 s_i + b_i}$ is the Fisher information on $\\mu$ if $\\theta$ were known perfectly (i.e., not a parameter but a fixed constant). The profiled information $I_{\\mu\\mu}^{\\text{prof}}$ is always less than or equal to $I_{\\mu\\mu}$, which illustrates that uncertainty in nuisance parameters reduces the statistical power to measure the parameter of interest.\n\n1.  Case $\\sigma_\\theta \\to 0$: This limit corresponds to a perfectly known nuisance parameter $\\theta$ (since it is constrained by a Gaussian of zero width centered at zero). The denominator approaches $1$, and we have:\n    $$\n    \\lim_{\\sigma_\\theta \\to 0} I_{\\mu\\mu}^{\\text{prof}} = \\sum_{i=1}^2 \\frac{s_i^2}{\\mu_0 s_i + b_i} = I_{\\mu\\mu}\n    $$\n    The information on $\\mu$ is maximal, as there is no uncertainty from $\\theta$ to account for.\n\n2.  Case $\\sigma_\\theta \\to \\infty$: This limit corresponds to a completely unknown nuisance parameter (the Gaussian constraint becomes flat and provides no information). The denominator grows infinitely large.\n    $$\n    \\lim_{\\sigma_\\theta \\to \\infty} I_{\\mu\\mu}^{\\text{prof}} = 0\n    $$\n    The information on $\\mu$ drops to zero. This is because, in the absence of a constraint on $\\theta$, there is a perfect degeneracy between $\\mu$ and $\\theta$ in the term $\\mu(1+\\theta)$. A change in signal strength $\\mu$ can be perfectly mimicked by a compensating change in the efficiency parameter $\\theta$, making it impossible to distinguish between the two effects and thus extracting no information on $\\mu$ alone.\nThe value of $I_{\\mu\\mu}^{\\text{prof}}$ is a monotonically decreasing function of $\\sigma_\\theta$, gracefully interpolating between these two physical limits.",
            "answer": "$$\n\\boxed{\\frac{\\sum_{i=1}^2 \\frac{s_i^2}{\\mu_0 s_i + b_i}}{1 + \\mu_0^2 \\sigma_\\theta^2 \\sum_{i=1}^2 \\frac{s_i^2}{\\mu_0 s_i + b_i}}}\n$$"
        },
        {
            "introduction": "Complex statistical models can sometimes suffer from pathologies where the data cannot distinguish between the effects of different parameters, a situation known as non-identifiability or degeneracy. This practice  provides a hands-on look at this critical issue, setting up a scenario where two distinct nuisance parameters have an identical effect on the predicted yields. By analyzing the Hessian matrix of the log-likelihood, you will learn to mathematically diagnose this degeneracy by identifying a \"flat direction,\" a crucial skill for building robust and stable statistical models in any scientific analysis.",
            "id": "3524818",
            "problem": "Consider a two-channel counting analysis in computational high-energy physics at the Large Hadron Collider (LHC) to measure a signal strength parameter $\\mu$ for a rare process. Each channel $i \\in \\{1,2\\}$ has known background expectation $b_i > 0$ and known signal template yield $s_i > 0$. The expected event count in channel $i$ is modeled as\n$$\n\\nu_i(\\mu,\\theta_1,\\theta_2) = b_i + \\mu\\,s_i\\,(1+\\alpha),\n$$\nwhere the two nuisance parameters $\\theta_1$ and $\\theta_2$ represent a global luminosity calibration shift and a global trigger efficiency shift, respectively, and their combined effect enters the yields only through the sum\n$$\n\\alpha \\equiv \\theta_1 + \\theta_2.\n$$\nThere is an external calibration providing a single auxiliary Gaussian constraint on $\\alpha$ with known width $\\sigma_{\\alpha} > 0$, but no auxiliary constraint on any orthogonal combination of $\\theta_1$ and $\\theta_2$. The likelihood is constructed as the product of independent Poisson probabilities for the observed counts $n_i \\in \\mathbb{N}$ in each channel and the Gaussian auxiliary constraint on $\\alpha$:\n$$\nL(\\mu,\\theta_1,\\theta_2) \\equiv \\left[\\prod_{i=1}^{2} \\frac{\\nu_i(\\mu,\\theta_1,\\theta_2)^{n_i}\\,\\exp\\!\\big(-\\nu_i(\\mu,\\theta_1,\\theta_2)\\big)}{n_i!} \\right] \\times \\exp\\!\\left(-\\frac{\\alpha^2}{2\\,\\sigma_{\\alpha}^2}\\right).\n$$\nAssume the observed counts equal the nominal expectations at some true $\\mu_0 > 0$ and $\\alpha_0 = 0$, that is, $n_i = b_i + \\mu_0 s_i$.\n\nStarting from the fundamental definitions of the Poisson likelihood and Gaussian constraints, and using only standard calculus and linear algebra, do the following:\n- Derive the log-likelihood $\\ell(\\mu,\\theta_1,\\theta_2) \\equiv \\ln L(\\mu,\\theta_1,\\theta_2)$ and show explicitly that its dependence on $(\\theta_1,\\theta_2)$ is only through $\\alpha$.\n- Compute the Hessian matrix of $\\ell$ with respect to $(\\theta_1,\\theta_2)$, evaluated at the maximum-likelihood point $(\\mu,\\alpha) = (\\mu_0,0)$.\n- Demonstrate that there is a flat direction in the $(\\theta_1,\\theta_2)$ space, and find the normalized eigenvector of the Hessian corresponding to the zero eigenvalue (the flat direction).\n\nAnswer specification:\n- Provide the normalized eigenvector corresponding to the flat direction as a single row vector using the LaTeX $\\verb|\\pmatrix|$ environment.\n- No rounding is required.\n- There are no physical units associated with the final answer.",
            "solution": "The problem statement is scientifically grounded, well-posed, and objective, rooted in the standard statistical methods used in high-energy physics. All necessary components are provided, and there are no internal contradictions or ambiguities. The problem is therefore deemed valid and a full solution will be provided.\n\nThe primary task is to analyze the structure of the log-likelihood function with respect to the nuisance parameters $\\theta_1$ and $\\theta_2$. The analysis proceeds in three steps as requested.\n\nFirst, we derive the log-likelihood function $\\ell(\\mu, \\theta_1, \\theta_2)$ and demonstrate its dependence on $\\theta_1$ and $\\theta_2$ is solely through their sum, $\\alpha \\equiv \\theta_1 + \\theta_2$. The likelihood function is given as:\n$$\nL(\\mu,\\theta_1,\\theta_2) = \\left[\\prod_{i=1}^{2} \\frac{\\nu_i(\\mu,\\theta_1,\\theta_2)^{n_i}\\,\\exp(-\\nu_i(\\mu,\\theta_1,\\theta_2))}{n_i!} \\right] \\times \\exp\\left(-\\frac{\\alpha^2}{2\\,\\sigma_{\\alpha}^2}\\right)\n$$\nThe log-likelihood, $\\ell \\equiv \\ln L$, is the natural logarithm of this expression. Using the properties of logarithms, we can write $\\ell$ as a sum of terms:\n$$\n\\ell(\\mu,\\theta_1,\\theta_2) = \\ln\\left(\\prod_{i=1}^{2} \\frac{\\nu_i^{n_i}\\,e^{-\\nu_i}}{n_i!}\\right) + \\ln\\left(\\exp\\left(-\\frac{\\alpha^2}{2\\,\\sigma_{\\alpha}^2}\\right)\\right)\n$$\n$$\n\\ell(\\mu,\\theta_1,\\theta_2) = \\sum_{i=1}^{2} \\ln\\left(\\frac{\\nu_i^{n_i}\\,e^{-\\nu_i}}{n_i!}\\right) - \\frac{\\alpha^2}{2\\,\\sigma_{\\alpha}^2}\n$$\n$$\n\\ell(\\mu,\\theta_1,\\theta_2) = \\sum_{i=1}^{2} \\left( n_i \\ln \\nu_i - \\nu_i - \\ln(n_i!) \\right) - \\frac{\\alpha^2}{2\\,\\sigma_{\\alpha}^2}\n$$\nThe expected event count $\\nu_i$ is defined as $\\nu_i(\\mu,\\theta_1,\\theta_2) = b_i + \\mu\\,s_i\\,(1+\\alpha)$, where $\\alpha = \\theta_1 + \\theta_2$. It is evident that $\\nu_i$ depends on $\\theta_1$ and $\\theta_2$ only through their sum $\\alpha$. The auxiliary constraint term also depends only on $\\alpha$. The terms involving the observed counts $n_i$ are constants with respect to the model parameters. Therefore, the entire log-likelihood function $\\ell(\\mu, \\theta_1, \\theta_2)$ can be written as a function of $\\mu$ and $\\alpha$, i.e., $\\ell(\\mu, \\theta_1, \\theta_2) = f(\\mu, \\alpha(\\theta_1, \\theta_2)) = f(\\mu, \\theta_1+\\theta_2)$ for some function $f$. This explicitly shows that its dependence on $(\\theta_1, \\theta_2)$ is only through $\\alpha$.\n\nSecond, we compute the Hessian matrix of $\\ell$ with respect to $(\\theta_1, \\theta_2)$, evaluated at the point $(\\mu, \\alpha) = (\\mu_0, 0)$. The Hessian matrix $H$ has elements $H_{jk} = \\frac{\\partial^2 \\ell}{\\partial \\theta_j \\partial \\theta_k}$ for $j, k \\in \\{1, 2\\}$.\nSince $\\ell$ is a function of $\\alpha = \\theta_1 + \\theta_2$, we use the chain rule.\nThe first partial derivatives are:\n$$\n\\frac{\\partial \\ell}{\\partial \\theta_j} = \\frac{d \\ell}{d \\alpha} \\frac{\\partial \\alpha}{\\partial \\theta_j}\n$$\nSince $\\frac{\\partial \\alpha}{\\partial \\theta_1} = 1$ and $\\frac{\\partial \\alpha}{\\partial \\theta_2} = 1$, we have:\n$$\n\\frac{\\partial \\ell}{\\partial \\theta_1} = \\frac{\\partial \\ell}{\\partial \\theta_2} = \\frac{d \\ell}{d \\alpha}\n$$\nThe second partial derivatives are:\n$$\nH_{jk} = \\frac{\\partial^2 \\ell}{\\partial \\theta_j \\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_j}\\left(\\frac{d \\ell}{d \\alpha}\\right) = \\frac{d^2 \\ell}{d \\alpha^2} \\frac{\\partial \\alpha}{\\partial \\theta_j} = \\frac{d^2 \\ell}{d \\alpha^2}\n$$\nThis holds for all combinations of $j, k \\in \\{1, 2\\}$, so all four elements of the Hessian matrix are identical and equal to $\\frac{d^2 \\ell}{d \\alpha^2}$.\nTo find this second derivative, we first compute the first derivative of $\\ell$ with respect to $\\alpha$:\n$$\n\\frac{d \\ell}{d \\alpha} = \\sum_{i=1}^{2} \\left( \\frac{n_i}{\\nu_i} \\frac{d \\nu_i}{d \\alpha} - \\frac{d \\nu_i}{d \\alpha} \\right) - \\frac{\\alpha}{\\sigma_{\\alpha}^2}\n$$\nWith $\\nu_i = b_i + \\mu s_i (1+\\alpha)$, we have $\\frac{d \\nu_i}{d \\alpha} = \\mu s_i$. Substituting this in:\n$$\n\\frac{d \\ell}{d \\alpha} = \\sum_{i=1}^{2} \\left( \\frac{n_i}{\\nu_i} - 1 \\right) \\mu s_i - \\frac{\\alpha}{\\sigma_{\\alpha}^2}\n$$\nNow, we compute the second derivative:\n$$\n\\frac{d^2 \\ell}{d \\alpha^2} = \\frac{d}{d\\alpha}\\left[ \\sum_{i=1}^{2} \\left( \\frac{n_i}{\\nu_i} - 1 \\right) \\mu s_i - \\frac{\\alpha}{\\sigma_{\\alpha}^2} \\right] = \\sum_{i=1}^{2} \\left( -\\frac{n_i}{\\nu_i^2} \\left(\\frac{d\\nu_i}{d\\alpha}\\right)^2 \\right) - \\frac{1}{\\sigma_{\\alpha}^2}\n$$\nSubstituting $\\frac{d\\nu_i}{d\\alpha} = \\mu s_i$ again:\n$$\n\\frac{d^2 \\ell}{d \\alpha^2} = -\\sum_{i=1}^{2} \\frac{n_i (\\mu s_i)^2}{\\nu_i^2} - \\frac{1}{\\sigma_{\\alpha}^2}\n$$\nWe must evaluate this expression at the specified point $(\\mu, \\alpha) = (\\mu_0, 0)$. At this point, the observed counts are given as $n_i = b_i + \\mu_0 s_i$. The model expectation at $(\\mu_0, 0)$ is $\\nu_i = b_i + \\mu_0 s_i(1+0) = b_i + \\mu_0 s_i$. Thus, at the evaluation point, we have $n_i = \\nu_i$.\nSubstituting $\\mu = \\mu_0$ and $n_i = \\nu_i = b_i + \\mu_0 s_i$ into the expression for the second derivative:\n$$\n\\left. \\frac{d^2 \\ell}{d \\alpha^2} \\right|_{(\\mu_0,0)} = -\\sum_{i=1}^{2} \\frac{(b_i + \\mu_0 s_i) (\\mu_0 s_i)^2}{(b_i + \\mu_0 s_i)^2} - \\frac{1}{\\sigma_{\\alpha}^2} = -\\sum_{i=1}^{2} \\frac{(\\mu_0 s_i)^2}{b_i + \\mu_0 s_i} - \\frac{1}{\\sigma_{\\alpha}^2}\n$$\nLet us denote this constant value as $C_{\\alpha\\alpha}$. The Hessian matrix evaluated at this point is:\n$$\nH = \\begin{pmatrix} C_{\\alpha\\alpha} & C_{\\alpha\\alpha} \\\\ C_{\\alpha\\alpha} & C_{\\alpha\\alpha} \\end{pmatrix}\n$$\n\nThird, we demonstrate the existence of a flat direction and find its corresponding normalized eigenvector. A flat direction in the parameter space corresponds to a zero eigenvalue of the Hessian matrix. We find the eigenvalues $\\lambda$ of $H$ by solving the characteristic equation $\\det(H - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} C_{\\alpha\\alpha} - \\lambda & C_{\\alpha\\alpha} \\\\ C_{\\alpha\\alpha} & C_{\\alpha\\alpha} - \\lambda \\end{pmatrix} = 0\n$$\n$$\n(C_{\\alpha\\alpha} - \\lambda)^2 - C_{\\alpha\\alpha}^2 = 0\n$$\n$$\n\\lambda^2 - 2 \\lambda C_{\\alpha\\alpha} + C_{\\alpha\\alpha}^2 - C_{\\alpha\\alpha}^2 = 0\n$$\n$$\n\\lambda (\\lambda - 2 C_{\\alpha\\alpha}) = 0\n$$\nThe eigenvalues are $\\lambda_1 = 0$ and $\\lambda_2 = 2C_{\\alpha\\alpha}$. The existence of the zero eigenvalue $\\lambda_1 = 0$ proves that there is a flat direction in the $(\\theta_1, \\theta_2)$ parameter space.\n\nTo find the eigenvector $v = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}$ corresponding to the zero eigenvalue, we solve the equation $(H - 0 \\cdot I)v = 0$, which is $Hv=0$:\n$$\n\\begin{pmatrix} C_{\\alpha\\alpha} & C_{\\alpha\\alpha} \\\\ C_{\\alpha\\alpha} & C_{\\alpha\\alpha} \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis yields the single equation $C_{\\alpha\\alpha} v_1 + C_{\\alpha\\alpha} v_2 = 0$. Since $b_i, s_i, \\mu_0, \\sigma_\\alpha > 0$, it is clear that $C_{\\alpha\\alpha} < 0$. Thus, we can divide by $C_{\\alpha\\alpha}$ to get $v_1 + v_2 = 0$, or $v_1 = -v_2$. The eigenspace is spanned by any vector of the form $\\begin{pmatrix} k \\\\ -k \\end{pmatrix}$ for $k \\ne 0$. A representative eigenvector is $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\nTo find the normalized eigenvector, we divide this vector by its Euclidean norm:\n$$\n\\|v\\| = \\sqrt{1^2 + (-1)^2} = \\sqrt{2}\n$$\nThe normalized eigenvector corresponding to the flat direction is:\n$$\n\\hat{v} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix}\n$$\nThis vector represents the direction in the $(\\theta_1, \\theta_2)$ plane along which the log-likelihood does not change, to second order. This is a direct consequence of the model's dependence on these parameters only through their sum $\\alpha = \\theta_1 + \\theta_2$.\n\nThe problem asks for the answer as a single row vector.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\end{pmatrix}}\n$$"
        }
    ]
}