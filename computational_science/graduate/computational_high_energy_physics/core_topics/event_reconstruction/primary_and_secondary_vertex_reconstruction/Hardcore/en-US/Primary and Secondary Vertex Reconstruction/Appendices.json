{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of vertex reconstruction is understanding the precision of the final estimate. This exercise lays the statistical groundwork by guiding you through a first-principles derivation of vertex resolution. By starting with a linearized, weighted least-squares model, you will use the Fisher Information Matrix to analytically determine how individual track uncertainties combine to define the overall precision of the reconstructed primary vertex .",
            "id": "3528910",
            "problem": "Consider a simplified linearized primary vertex reconstruction in a collider detector where the transverse coordinate along the local detector $x$-axis and the longitudinal coordinate $z$ are estimated independently from track-level constraints. Each of $N$ reconstructed charged-particle tracks provides two one-dimensional residuals relative to the unknown primary vertex location, modeled as\n$$\ny_{x,i} \\;=\\; x \\;+\\; \\epsilon_{x,i}, \\qquad y_{z,i} \\;=\\; z \\;+\\; \\epsilon_{z,i}, \\qquad i = 1,\\dots,N,\n$$\nwhere $x$ and $z$ are the true primary vertex coordinates to be estimated, and $\\epsilon_{x,i}$ and $\\epsilon_{z,i}$ are independent Gaussian noises with zero mean and known variances $\\sigma_{d0,i}^{2}$ and $\\sigma_{z0,i}^{2}$, respectively. Assume all tracks are mutually independent, and that the transverse and longitudinal fits are decoupled so that cross-derivatives vanish in the linearization.\n\nA robust vertex fit assigns to each track a deterministic weight $w_i \\in [0,1]$ that modulates its contribution to the objective without altering the per-track Gaussian noise model. The fit minimizes the weighted sum-of-squares objective\n$$\n\\mathcal{L}(x,z) \\;=\\; \\frac{1}{2}\\sum_{i=1}^{N} w_i\\left(\\frac{(y_{x,i}-x)^{2}}{\\sigma_{d0,i}^{2}} \\;+\\; \\frac{(y_{z,i}-z)^{2}}{\\sigma_{z0,i}^{2}}\\right).\n$$\nUnder the standard Gaussian approximation near the optimum, the covariance of the estimator obtained by Maximum Likelihood Estimation (MLE) is the inverse of the Fisher Information Matrix (FIM) accumulated from the independent track constraints.\n\nStarting solely from the above Gaussian measurement model, independence assumptions, and the definition of the Fisher Information Matrix for Gaussian noise, derive the closed-form analytical expressions for the expected primary vertex resolutions (one standard deviation) in $x$ and $z$ as functions of $N$, the track weights $w_i$, and the per-track uncertainties $\\sigma_{d0,i}$ and $\\sigma_{z0,i}$. Express your final answer as a single row matrix containing the two resolutions. Do not substitute any numerical values; provide the analytic expressions only. No unit conversion is required. For clarity, present the final expressions in terms of $w_i$, $\\sigma_{d0,i}$, and $\\sigma_{z0,i}$.",
            "solution": "The problem asks for the resolutions of the estimated vertex coordinates, $\\hat{x}$ and $\\hat{z}$, which are the standard deviations of these estimators. The problem states that the covariance matrix of the estimators is the inverse of the Fisher Information Matrix (FIM). For a weighted least-squares problem with Gaussian noise, the objective function $\\mathcal{L}(x,z)$ is equivalent to the negative log-likelihood (up to a constant). The parameters to estimate are $\\theta = (x, z)$. The FIM, $\\mathbf{I}$, is a $2 \\times 2$ matrix with elements $I_{jk} = -E\\left[\\frac{\\partial^2 \\ln L}{\\partial \\theta_j \\partial \\theta_k}\\right]$, where $L$ is the likelihood. Using $l_{\\text{eff}} = -\\mathcal{L}$, we compute the FIM.\n\nFirst, we compute the second partial derivatives of $l_{\\text{eff}}(x,z) = -\\frac{1}{2}\\sum_{i=1}^{N} w_i\\left(\\frac{(y_{x,i}-x)^{2}}{\\sigma_{d0,i}^{2}} + \\frac{(y_{z,i}-z)^{2}}{\\sigma_{z0,i}^{2}}\\right)$.\nThe derivative with respect to $x$ is:\n$$\n\\frac{\\partial^2 l_{\\text{eff}}}{\\partial x^2} = \\frac{\\partial}{\\partial x} \\left( \\sum_{i=1}^{N} w_i \\frac{y_{x,i}-x}{\\sigma_{d0,i}^{2}} \\right) = - \\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{d0,i}^{2}}\n$$\nThis is a constant, so its expectation is the value itself. The $(1,1)$ element of the FIM is:\n$$\nI_{xx} = -E\\left[\\frac{\\partial^2 l_{\\text{eff}}}{\\partial x^2}\\right] = \\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{d0,i}^{2}}\n$$\nBy symmetry, the $(2,2)$ element is:\n$$\nI_{zz} = -E\\left[\\frac{\\partial^2 l_{\\text{eff}}}{\\partial z^2}\\right] = \\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{z0,i}^{2}}\n$$\nThe problem states that the fits for $x$ and $z$ are decoupled, which is reflected in the separable objective function. The mixed partial derivative is zero:\n$$\n\\frac{\\partial^2 l_{\\text{eff}}}{\\partial x \\partial z} = 0 \\implies I_{xz} = I_{zx} = 0\n$$\nThe FIM is therefore a diagonal matrix:\n$$\n\\mathbf{I} = \\begin{pmatrix} \\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{d0,i}^{2}}  0 \\\\ 0  \\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{z0,i}^{2}} \\end{pmatrix}\n$$\nThe covariance matrix of the estimators, $\\mathbf{C}$, is the inverse of the FIM:\n$$\n\\mathbf{C} = \\mathbf{I}^{-1} = \\begin{pmatrix} \\left(\\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{d0,i}^{2}}\\right)^{-1}  0 \\\\ 0  \\left(\\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{z0,i}^{2}}\\right)^{-1} \\end{pmatrix}\n$$\nThe variances of the estimators are the diagonal elements of this matrix. The resolutions (standard deviations) are the square roots of the variances.\nThe resolution in $x$, $\\sigma_{\\hat{x}}$, is:\n$$\n\\sigma_{\\hat{x}} = \\sqrt{\\text{Var}(\\hat{x})} = \\left(\\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{d0,i}^{2}}\\right)^{-1/2}\n$$\nThe resolution in $z$, $\\sigma_{\\hat{z}}$, is:\n$$\n\\sigma_{\\hat{z}} = \\sqrt{\\text{Var}(\\hat{z})} = \\left(\\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{z0,i}^{2}}\\right)^{-1/2}\n$$\nThese are the requested analytical expressions for the vertex resolutions.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\left(\\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{d0,i}^{2}}\\right)^{-1/2}  \\left(\\sum_{i=1}^{N} \\frac{w_i}{\\sigma_{z0,i}^{2}}\\right)^{-1/2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While the weighted least-squares estimator is optimal for pure, Gaussian-distributed data, real-world track samples are often contaminated by outliers, such as tracks from displaced secondary decays. This practice explores the consequences of such model violations by having you derive and calculate the systematic bias introduced in the vertex estimate . Understanding the source and magnitude of this bias is a critical step in appreciating the need for the robust fitting algorithms used in modern experiments.",
            "id": "3528935",
            "problem": "You are tasked with developing a mathematically principled and computationally robust maximum-likelihood primary vertex fitter for charged-particle tracks in high-energy physics, integrating a Newton method update and an analysis of estimator bias in the presence of displaced secondary decays. Each track provides a three-dimensional measurement of its point-of-closest-approach to a common vertex. For track $i$, its measured position is the vector $\\vec{r}_i \\in \\mathbb{R}^3$ (in $\\mathrm{mm}$) with a symmetric positive-definite covariance matrix $C_i \\in \\mathbb{R}^{3 \\times 3}$ (in $\\mathrm{mm}^2$). Assume independent Gaussian measurement errors for all tracks. The primary vertex position is $\\vec{v} \\in \\mathbb{R}^3$ (in $\\mathrm{mm}$). The goal is to estimate $\\vec{v}$ by minimizing the quadratic objective\n$$\n\\chi^2(\\vec{v}) = \\sum_{i=1}^N (\\vec{r}_i - \\vec{v})^\\top C_i^{-1} (\\vec{r}_i - \\vec{v}),\n$$\nwhich is the negative log-likelihood (up to an additive constant), consistent with independent Gaussian uncertainties.\n\nStarting from the definitions of the Gaussian likelihood, the independence of measurements, and the properties of symmetric positive-definite covariance matrices, derive from first principles the stationary condition for the estimator $\\hat{\\vec{v}}$ that minimizes $\\chi^2(\\vec{v})$, and derive the Newton method update using the gradient and Hessian of $\\chi^2(\\vec{v})$. Explain why for a quadratic objective the Newton method converges in one step to the unique minimizer. Implement the Newton update to compute $\\hat{\\vec{v}}$ for the provided test suites.\n\nThen, analyze the estimator bias when a fraction $\\epsilon$ of tracks originate from a displaced secondary vertex at $\\vec{v}_0 + \\vec{d}$, while the remaining fraction $1 - \\epsilon$ originate from the true primary vertex at $\\vec{v}_0$. Assume that the displaced tracks have measurements centered at $\\vec{v}_0 + \\vec{d}$ and the primary tracks at $\\vec{v}_0$, with covariances $C_i$ as specified. Derive the expected estimator bias vector $\\mathbb{E}[\\hat{\\vec{v}} - \\vec{v}_0]$ in terms of the inverse covariances $W_i = C_i^{-1}$, the displacement $\\vec{d}$, and the contamination set $S$ of displaced tracks. Explain under what conditions the bias reduces to $\\epsilon \\vec{d}$, and how covariance heterogeneity changes the bias. For each test case below, compute the fitted $\\hat{\\vec{v}}$ via the Newton update, and report the bias magnitude $\\|\\hat{\\vec{v}} - \\vec{v}_0\\|_2$ in $\\mathrm{mm}$.\n\nUse strictly the following test suite, where all vectors are in $\\mathrm{mm}$ and all covariance matrices are in $\\mathrm{mm}^2$. For every test case, the program must construct the observations as follows: for tracks in the primary set $P$, set $\\vec{r}_i = \\vec{v}_0$; for tracks in the displaced secondary set $S$, set $\\vec{r}_i = \\vec{v}_0 + \\vec{d}$. No random sampling is permitted; all inputs are deterministic. In all cases, the Newton method must be applied in three dimensions with full $3 \\times 3$ matrices.\n\nTest Case $1$ (isotropic, equal weights, partial contamination):\n- $N = 12$\n- $\\vec{v}_0 = (0.0, 0.0, 0.0)$\n- $\\vec{d} = (0.4, -0.1, 0.2)$\n- $\\epsilon = 0.25$, with $S = \\{0, 1, 2\\}$ and $P = \\{3, 4, 5, 6, 7, 8, 9, 10, 11\\}$\n- $C_i = \\operatorname{diag}(0.05^2, 0.05^2, 0.05^2)$ for all $i$\n\nTest Case $2$ (isotropic, equal weights, no contamination):\n- $N = 8$\n- $\\vec{v}_0 = (1.0, -1.0, 0.5)$\n- $\\vec{d} = (0.3, 0.3, -0.1)$\n- $\\epsilon = 0.0$, with $S = \\emptyset$ and $P = \\{0, 1, 2, 3, 4, 5, 6, 7\\}$\n- $C_i = \\operatorname{diag}(0.02^2, 0.02^2, 0.02^2)$ for all $i$\n\nTest Case $3$ (isotropic, equal weights, full contamination):\n- $N = 5$\n- $\\vec{v}_0 = (0.0, 0.0, 0.0)$\n- $\\vec{d} = (-0.2, 0.5, 0.1)$\n- $\\epsilon = 1.0$, with $S = \\{0, 1, 2, 3, 4\\}$ and $P = \\emptyset$\n- $C_i = \\operatorname{diag}(0.02^2, 0.02^2, 0.02^2)$ for all $i$\n\nTest Case $4$ (anisotropic, heterogeneous weights, contamination correlated with larger covariance):\n- $N = 10$\n- $\\vec{v}_0 = (0.2, -0.3, 0.1)$\n- $\\vec{d} = (0.8, 0.0, -0.2)$\n- $\\epsilon = 0.4$, with $S = \\{6, 7, 8, 9\\}$ and $P = \\{0, 1, 2, 3, 4, 5\\}$\n- For $i \\in P$: $C_i = \\operatorname{diag}(0.01^2, 0.01^2, 0.01^2)$\n- For $i \\in S$: $C_i = \\operatorname{diag}(0.10^2, 0.05^2, 0.02^2)$\n\nTest Case $5$ (anisotropic, direction-dependent weights, contamination among tracks precise along $\\vec{d}$):\n- $N = 9$\n- $\\vec{v}_0 = (0.0, 0.0, 0.0)$\n- $\\vec{d} = (0.05, 0.60, 0.0)$\n- $\\epsilon = \\tfrac{1}{3}$, with $S = \\{0, 1, 2\\}$ and $P = \\{3, 4, 5, 6, 7, 8\\}$\n- For $i \\in \\{0, 1, 2\\}$: $C_i = \\operatorname{diag}(0.2^2, 0.02^2, 0.02^2)$\n- For $i \\in \\{3, 4, 5\\}$: $C_i = \\operatorname{diag}(0.02^2, 0.2^2, 0.02^2)$\n- For $i \\in \\{6, 7, 8\\}$: $C_i = \\operatorname{diag}(0.02^2, 0.02^2, 0.2^2)$\n\nYour program must:\n- For each test case, construct the set of measurements $\\{\\vec{r}_i\\}_{i=1}^N$ and covariances $\\{C_i\\}_{i=1}^N$ as specified.\n- Compute the Newton update for $\\chi^2(\\vec{v})$ starting from any initial guess $\\vec{v}^{(0)}$ and obtain the fitted vertex $\\hat{\\vec{v}}$.\n- Compute the bias magnitude $\\|\\hat{\\vec{v}} - \\vec{v}_0\\|_2$ in $\\mathrm{mm}$ for each test case.\n\nFinal Output Format:\nYour program should produce a single line of output containing the bias magnitudes for the five test cases as a comma-separated list enclosed in square brackets, in $\\mathrm{mm}$, each rounded to six decimal places (for example, $[\\text{bias}_1,\\text{bias}_2,\\text{bias}_3,\\text{bias}_4,\\text{bias}_5]$).",
            "solution": "The problem is solved by first deriving the analytical form of the weighted least-squares estimator and its properties, and then implementing it to compute the required bias magnitudes.\n\n**1. Stationary Condition and Newton's Method**\nThe objective function is $\\chi^2(\\vec{v}) = \\sum_{i=1}^N (\\vec{r}_i - \\vec{v})^\\top W_i (\\vec{r}_i - \\vec{v})$, where $W_i = C_i^{-1}$. To find the minimum, we set the gradient $\\vec{g}(\\vec{v}) = \\nabla_{\\vec{v}} \\chi^2(\\vec{v})$ to zero.\n$$\n\\vec{g}(\\vec{v}) = \\sum_{i=1}^N -2 W_i (\\vec{r}_i - \\vec{v}) = 2 \\left( \\sum_{i=1}^N W_i \\right) \\vec{v} - 2 \\left( \\sum_{i=1}^N W_i \\vec{r}_i \\right)\n$$\nSetting $\\vec{g}(\\hat{\\vec{v}}) = \\vec{0}$ gives the stationary condition:\n$$\n\\left( \\sum_{i=1}^N W_i \\right) \\hat{\\vec{v}} = \\sum_{i=1}^N W_i \\vec{r}_i\n$$\nThis leads to the unique solution $\\hat{\\vec{v}} = \\left( \\sum_i W_i \\right)^{-1} \\left( \\sum_i W_i \\vec{r}_i \\right)$, which is a generalized weighted average.\n\nThe Hessian matrix is $H(\\vec{v}) = \\nabla_{\\vec{v}} (\\vec{g}(\\vec{v})^\\top) = 2 \\sum_i W_i = 2W$. Since the Hessian is constant, the objective function is quadratic. Newton's method update, $\\vec{v}^{(k+1)} = \\vec{v}^{(k)} - [H(\\vec{v}^{(k)})]^{-1} \\vec{g}(\\vec{v}^{(k)})$, converges in one step from any starting point $\\vec{v}^{(0)}$ to the exact solution $\\hat{\\vec{v}}$:\n$$\n\\vec{v}^{(1)} = \\vec{v}^{(0)} - (2W)^{-1} \\left( 2W\\vec{v}^{(0)} - 2 \\sum_i W_i \\vec{r}_i \\right) = W^{-1} \\sum_i W_i \\vec{r}_i = \\hat{\\vec{v}}\n$$\n\n**2. Estimator Bias Analysis**\nWe analyze the bias $\\mathbb{E}[\\hat{\\vec{v}} - \\vec{v}_0]$ when tracks in set $P$ have expected position $\\vec{v}_0$ and tracks in set $S$ have expected position $\\vec{v}_0 + \\vec{d}$. The expectation of the estimator is:\n$$\n\\mathbb{E}[\\hat{\\vec{v}}] = W^{-1} \\sum_{i=1}^N W_i \\mathbb{E}[\\vec{r}_i] = W^{-1} \\left( \\sum_{i \\in P} W_i \\vec{v}_0 + \\sum_{i \\in S} W_i (\\vec{v}_0 + \\vec{d}) \\right)\n$$\nLet $W_P = \\sum_{i \\in P} W_i$ and $W_S = \\sum_{i \\in S} W_i$. The total weight is $W = W_P + W_S$.\n$$\n\\mathbb{E}[\\hat{\\vec{v}}] = W^{-1} \\left( (W_P + W_S) \\vec{v}_0 + W_S \\vec{d} \\right) = \\vec{v}_0 + W^{-1} W_S \\vec{d}\n$$\nThe bias vector is therefore:\n$$\n\\text{Bias} = \\mathbb{E}[\\hat{\\vec{v}}] - \\vec{v}_0 = \\left( \\sum_{i=1}^N W_i \\right)^{-1} \\left( \\sum_{i \\in S} W_i \\right) \\vec{d}\n$$\nThis expression shows how the bias is a linear transformation of the true displacement $\\vec{d}$. The bias reduces to the simple form $\\epsilon \\vec{d}$ if all tracks have identical covariance matrices ($C_i=C_0$ for all $i$). In this case, $W_i=W_0$, so $W_S = |S| W_0$ and $W = N W_0$, leading to $\\text{Bias} = (N W_0)^{-1}(|S| W_0)\\vec{d} = \\frac{|S|}{N}\\vec{d} = \\epsilon \\vec{d}$. If covariances are heterogeneous, tracks with higher precision (larger $W_i$) have a stronger influence, pulling the vertex estimate towards their positions and altering the bias from the simple $\\epsilon \\vec{d}$ scaling. The implementation calculates this bias for the deterministic inputs provided.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the primary vertex fit and bias for several test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"name\": \"Case 1: Isotropic, equal weights, partial contamination\",\n            \"N\": 12,\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n            \"d\": np.array([0.4, -0.1, 0.2]),\n            \"S_indices\": {0, 1, 2},\n            \"P_indices\": {3, 4, 5, 6, 7, 8, 9, 10, 11},\n            \"covariances\": {\n                \"all\": np.diag([0.05**2, 0.05**2, 0.05**2])\n            }\n        },\n        {\n            \"name\": \"Case 2: Isotropic, equal weights, no contamination\",\n            \"N\": 8,\n            \"v0\": np.array([1.0, -1.0, 0.5]),\n            \"d\": np.array([0.3, 0.3, -0.1]),\n            \"S_indices\": set(),\n            \"P_indices\": {0, 1, 2, 3, 4, 5, 6, 7},\n            \"covariances\": {\n                \"all\": np.diag([0.02**2, 0.02**2, 0.02**2])\n            }\n        },\n        {\n            \"name\": \"Case 3: Isotropic, equal weights, full contamination\",\n            \"N\": 5,\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n            \"d\": np.array([-0.2, 0.5, 0.1]),\n            \"S_indices\": {0, 1, 2, 3, 4},\n            \"P_indices\": set(),\n            \"covariances\": {\n                \"all\": np.diag([0.02**2, 0.02**2, 0.02**2])\n            }\n        },\n        {\n            \"name\": \"Case 4: Anisotropic, heterogeneous weights\",\n            \"N\": 10,\n            \"v0\": np.array([0.2, -0.3, 0.1]),\n            \"d\": np.array([0.8, 0.0, -0.2]),\n            \"S_indices\": {6, 7, 8, 9},\n            \"P_indices\": {0, 1, 2, 3, 4, 5},\n            \"covariances\": {\n                \"primary\": np.diag([0.01**2, 0.01**2, 0.01**2]),\n                \"secondary\": np.diag([0.10**2, 0.05**2, 0.02**2])\n            }\n        },\n        {\n            \"name\": \"Case 5: Anisotropic, direction-dependent weights\",\n            \"N\": 9,\n            \"v0\": np.array([0.0, 0.0, 0.0]),\n            \"d\": np.array([0.05, 0.60, 0.0]),\n            \"S_indices\": {0, 1, 2},\n            \"P_indices\": {3, 4, 5, 6, 7, 8},\n            \"covariances\": {\n                \"group_S\": np.diag([0.2**2, 0.02**2, 0.02**2]),\n                \"group_P1\": np.diag([0.02**2, 0.2**2, 0.02**2]),\n                \"group_P2\": np.diag([0.02**2, 0.02**2, 0.2**2])\n            },\n            \"group_indices\": {\n                \"group_S\": {0, 1, 2},\n                \"group_P1\": {3, 4, 5},\n                \"group_P2\": {6, 7, 8}\n            }\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        N = case[\"N\"]\n        v0 = case[\"v0\"]\n        d = case[\"d\"]\n        S_indices = case[\"S_indices\"]\n        covariances = case[\"covariances\"]\n        \n        r_list = [np.zeros(3) for _ in range(N)]\n        C_list = [np.zeros((3, 3)) for _ in range(N)]\n\n        for i in range(N):\n            r_list[i] = v0 + d if i in S_indices else v0\n\n        if \"all\" in covariances:\n            for i in range(N):\n                C_list[i] = covariances[\"all\"]\n        elif \"group_indices\" in case: # Case 5\n            group_indices = case[\"group_indices\"]\n            for group_name, indices in group_indices.items():\n                for i in indices:\n                    C_list[i] = covariances[group_name]\n        else: # Case 4\n            P_indices = case[\"P_indices\"]\n            for i in range(N):\n                C_list[i] = covariances[\"primary\"] if i in P_indices else covariances[\"secondary\"]\n\n        W_list = [np.linalg.inv(C) for C in C_list]\n        W_total = np.sum(W_list, axis=0)\n        sum_Wr = np.sum([W @ r for W, r in zip(W_list, r_list)], axis=0)\n        \n        v_hat = np.linalg.solve(W_total, sum_Wr)\n\n        bias_magnitude = np.linalg.norm(v_hat - v0)\n        results.append(bias_magnitude)\n\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "To overcome the bias induced by outliers, physicists employ robust estimation techniques that systematically down-weight non-conforming measurements. This comprehensive practice moves from theory to implementation, tasking you with designing and evaluating robust vertex fitters based on the Huber and Student-t loss functions . By deriving the associated weight functions, implementing an iteratively reweighted least squares (IRLS) algorithm, and performing a simulation to probe the estimators' breakdown points, you will gain hands-on experience with the state-of-the-art methods that ensure reliable vertexing in complex collision environments.",
            "id": "3528981",
            "problem": "You are tasked with designing, deriving, and evaluating robust estimators for the three-dimensional primary vertex position in a collider event, where each reconstructed track provides a point-of-closest-approach position and an associated covariance. The vertex position is denoted by the three-dimensional vector $\\vec{v} \\in \\mathbb{R}^{3}$, and the $i$-th track measurement is $\\vec{r}_{i} \\in \\mathbb{R}^{3}$ with a symmetric positive-definite covariance matrix $C_{i} \\in \\mathbb{R}^{3 \\times 3}$. The Mahalanobis residual norm is defined by\n$$\n\\rho_{i}(\\vec{v}) = \\sqrt{(\\vec{r}_{i} - \\vec{v})^{\\top} C_{i}^{-1} (\\vec{r}_{i} - \\vec{v})}.\n$$\nThe robust fitting paradigm is to estimate $\\vec{v}$ by minimizing a sum of robust losses,\n$$\n\\min_{\\vec{v} \\in \\mathbb{R}^{3}} \\sum_{i=1}^{N} \\phi(\\rho_{i}(\\vec{v})),\n$$\nwhere $\\phi(\\cdot)$ is a nonnegative, differentiable loss function. Start from the following fundamental base:\n- Maximum likelihood under Gaussian errors yields least squares, i.e., $\\phi(\\rho) = \\tfrac{1}{2}\\rho^{2}$.\n- For general robust losses, the first-order optimality condition leads to an iteratively reweighted least squares scheme, based on the score function $\\psi(\\rho) = \\tfrac{d\\phi(\\rho)}{d\\rho}$ and the weight function $w(\\rho) = \\psi(\\rho)/\\rho$, with the convention that $w(0)$ is the limit $\\lim_{\\rho \\to 0^{+}} \\psi(\\rho)/\\rho$ when needed.\n- Given weights $w_{i} \\equiv w(\\rho_{i})$, one iteration of the reweighted normal equations for $\\vec{v}$ has the form\n$$\n\\left(\\sum_{i=1}^{N} w_{i} C_{i}^{-1}\\right)\\vec{v} = \\sum_{i=1}^{N} w_{i} C_{i}^{-1} \\vec{r}_{i}.\n$$\nYour tasks are:\n1) Using the definitions above, derive the weight functions $w_{i}(\\rho_{i})$ for the following two robust losses, expressed in terms of the residual norm $\\rho$:\n- Huber loss with cutoff parameter $k0$, defined by a piecewise function $\\phi(\\rho)$ that is quadratic for small $\\rho$ and linear for large $\\rho$.\n- Multivariate Student-$t$ negative log-likelihood for a $d$-dimensional observation with $\\nu0$ degrees of freedom, which yields a loss function of the form $\\phi(\\rho)$ that depends on $d$, $\\nu$, and $\\rho$ via a logarithmic term involving $1+\\rho^{2}/\\nu$.\n2) Implement an iteratively reweighted least squares algorithm to estimate the primary vertex $\\vec{v}$ given $\\{\\vec{r}_{i}, C_{i}\\}_{i=1}^{N}$ for each of the two robust losses. Use the dimension $d=3$. Ensure numerical stability by using a tolerance for convergence and a maximum number of iterations. All coordinates are in millimeters (mm); the output quantities in this part that you will be asked to report are dimensionless, but the algorithm must internally handle positions in millimeters.\n3) Study the empirical breakdown behavior under contamination by simulating data with an increasing fraction of outliers. Use the following physically and numerically sound scenario:\n- True vertex $\\vec{v}_{\\text{true}} = (0,0,0)$ mm.\n- Number of tracks $N=100$.\n- Inliers: $N_{\\text{in}} = \\lfloor (1-p) N \\rfloor$ tracks sampled independently from a normal distribution centered at $\\vec{v}_{\\text{true}}$ with isotropic covariance $\\sigma_{\\text{in}}^{2} I_{3}$, where $\\sigma_{\\text{in}} = 0.03$ mm. Each inlier has covariance $C_{i} = \\sigma_{\\text{in}}^{2} I_{3}$.\n- Outliers: $N_{\\text{out}} = N - N_{\\text{in}}$ tracks sampled independently from a normal distribution with mean $\\vec{\\mu}_{\\text{out}} = (10,-10,5)$ mm and isotropic covariance $\\sigma_{\\text{out}}^{2} I_{3}$, where $\\sigma_{\\text{out}} = 2.0$ mm. For simplicity, use the same per-track covariance $C_{i} = \\sigma_{\\text{in}}^{2} I_{3}$ also for outliers, representing underestimation of uncertainties for bad tracks.\n- Consider outlier fractions $p \\in \\{0.0, 0.2, 0.4, 0.6\\}$.\n- Define failure (breakdown) at a given $p$ if the estimated vertex $\\hat{\\vec{v}}$ satisfies $\\|\\hat{\\vec{v}} - \\vec{v}_{\\text{true}}\\|_{2}  1.0$ mm.\n- For each robust loss, report the smallest $p$ in the given set at which failure occurs. If no failure occurs for any $p$ in the set, report $1.0$.\nFor numerical reproducibility, use a fixed random seed for the simulation. Angles are not involved; no angle unit is required. All distances are in millimeters; the outputs requested below are dimensionless.\nTest suite and required outputs:\n- Weight-function evaluation tests at residual norms $\\rho \\in \\{0.5, 1.5, 3.0\\}$ with the following parameters:\n  - Huber: $k = 1.5$.\n  - Student-$t$: $\\nu = 4$, $d = 3$.\n- Breakdown study with $p \\in \\{0.0, 0.2, 0.4, 0.6\\}$ and the simulation design described above.\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order:\n$[w_{H}(0.5), w_{H}(1.5), w_{H}(3.0), w_{T}(0.5), w_{T}(1.5), w_{T}(3.0), p_{\\text{break,Huber}}, p_{\\text{break,Student}}]$,\nwhere $w_{H}(\\cdot)$ and $w_{T}(\\cdot)$ are the derived weight functions for Huber and Student-$t$, respectively, and $p_{\\text{break,Huber}}$ and $p_{\\text{break,Student}}$ are the smallest failure fractions in the test set as defined above. The eight outputs are dimensionless real numbers. The single line must contain exactly this list format with no additional text.",
            "solution": "This problem is solved by first deriving the weight functions for the Huber and Student-t losses, then describing the Iteratively Reweighted Least Squares (IRLS) algorithm used for robust estimation, and finally implementing the simulation to determine the empirical breakdown point.\n\n**1. Derivation of Weight Functions**\nThe IRLS algorithm requires a weight function $w(\\rho) = \\psi(\\rho)/\\rho$, where $\\psi(\\rho) = d\\phi/d\\rho$ is the score function derived from the loss function $\\phi(\\rho)$.\n\n**Huber Loss:** The Huber loss function for a non-negative residual norm $\\rho$ and cutoff $k$ is:\n$$ \\phi_H(\\rho) = \\begin{cases} \\frac{1}{2}\\rho^2  \\text{if } 0 \\le \\rho \\le k \\\\ k\\rho - \\frac{1}{2}k^2  \\text{if } \\rho  k \\end{cases} $$\nThe score function is its derivative, $\\psi_H(\\rho) = \\min(\\rho, k)$. The weight function is therefore:\n$$ w_H(\\rho) = \\frac{\\psi_H(\\rho)}{\\rho} = \\frac{\\min(\\rho, k)}{\\rho} = \\min\\left(1, \\frac{k}{\\rho}\\right) $$\nThis function gives full weight (1) to inliers (residuals below $k$) and down-weights outliers with a factor inversely proportional to their residual norm.\n\n**Student-t Loss:** The loss function is derived from the negative log-likelihood of a $d$-dimensional Student-t distribution with $\\nu$ degrees of freedom:\n$$ \\phi_T(\\rho) = \\frac{\\nu+d}{2} \\log\\left(1 + \\frac{\\rho^2}{\\nu}\\right) $$\nThe score function is found using the chain rule:\n$$ \\psi_T(\\rho) = \\frac{d\\phi_T}{d\\rho} = \\frac{\\nu+d}{2} \\cdot \\frac{1}{1 + \\rho^2/\\nu} \\cdot \\frac{2\\rho}{\\nu} = \\frac{(\\nu+d)\\rho}{\\nu + \\rho^2} $$\nThe corresponding weight function is:\n$$ w_T(\\rho) = \\frac{\\psi_T(\\rho)}{\\rho} = \\frac{\\nu+d}{\\nu + \\rho^2} $$\nThis function provides a smooth down-weighting for all non-zero residuals, becoming progressively stronger for larger residuals.\n\n**2. Iteratively Reweighted Least Squares (IRLS) Algorithm**\nThe IRLS algorithm solves the robust estimation problem by iteratively solving a weighted least-squares problem. For the simulation scenario where all covariance matrices are isotropic and identical ($C_i = \\sigma_{\\text{in}}^2 I_3$), the update step for the vertex estimate $\\vec{v}$ simplifies to a weighted average:\n$$ \\vec{v}^{(t+1)} = \\frac{\\sum_{i=1}^{N} w_i^{(t)} \\vec{r}_i}{\\sum_{i=1}^{N} w_i^{(t)}} $$\nwhere the weights $w_i^{(t)}$ are computed using the vertex estimate from the previous iteration, $\\vec{v}^{(t)}$, and the simplified Mahalanobis norm $\\rho_i(\\vec{v}^{(t)}) = \\|\\vec{r}_i - \\vec{v}^{(t)}\\|_2 / \\sigma_{\\text{in}}$. The algorithm starts with an initial guess (e.g., the unweighted mean) and iterates until the change in $\\vec{v}$ is below a tolerance.\n\n**3. Empirical Breakdown Study**\nThe simulation tests the robustness of each estimator against an increasing fraction $p$ of outliers. For each $p$, a dataset is generated with inliers drawn from a distribution centered at the true vertex and outliers drawn from a distribution far away. The IRLS algorithm is run for both Huber and Student-t losses. A \"breakdown\" is registered for an estimator if its final estimate $\\hat{\\vec{v}}$ is more than 1.0 mm away from the true vertex. The simulation reports the lowest outlier fraction $p$ at which this failure occurs for each estimator. This provides a practical measure of how much contamination each method can handle before its estimate becomes unreliable.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the robust vertex reconstruction problem by:\n    1. Evaluating weight functions for Huber and Student-t losses.\n    2. Implementing an Iteratively Reweighted Least Squares (IRLS) algorithm.\n    3. Performing a breakdown study to find the failure point for each estimator.\n    \"\"\"\n\n    # --- Part 1: Weight Function Definitions and Evaluation ---\n\n    def w_huber(rho, k):\n        \"\"\"Calculates the Huber weight function.\"\"\"\n        rho = np.asarray(rho)\n        weights = np.ones_like(rho, dtype=float)\n        mask = rho > 0\n        weights[mask] = np.minimum(1.0, k / rho[mask])\n        return weights\n\n    def w_student_t(rho, nu, d):\n        \"\"\"Calculates the Student-t weight function.\"\"\"\n        rho = np.asarray(rho)\n        return (nu + d) / (nu + rho**2)\n\n    rhos_test = np.array([0.5, 1.5, 3.0])\n    k_huber = 1.5\n    nu_student_t = 4.0\n    d_student_t = 3.0\n\n    weights_huber = w_huber(rhos_test, k=k_huber)\n    weights_student_t = w_student_t(rhos_test, nu=nu_student_t, d=d_student_t)\n\n    # --- Part 2: IRLS Algorithm ---\n\n    def irls_vertex_fit(r_tracks, sigma_in, weight_func, *args, max_iter=100, tol=1e-6):\n        \"\"\"\n        Performs an Iteratively Reweighted Least Squares fit for the vertex position.\n        This simplified version assumes C_i = sigma_in^2 * I_3 for all i.\n        \"\"\"\n        v = np.mean(r_tracks, axis=0)\n\n        for _ in range(max_iter):\n            v_old = v\n            distances = np.linalg.norm(r_tracks - v, axis=1)\n            rhos = distances / sigma_in\n            weights = weight_func(rhos, *args)\n            \n            sum_weights = np.sum(weights)\n            if sum_weights  1e-9:\n                v = np.mean(r_tracks, axis=0)\n            else:\n                v = np.sum(r_tracks * weights[:, np.newaxis], axis=0) / sum_weights\n\n            if np.linalg.norm(v - v_old)  tol:\n                break\n        \n        return v\n\n    # --- Part 3: Breakdown Study ---\n\n    def run_breakdown_study():\n        \"\"\"\n        Simulates track data with outliers and finds the breakdown point.\n        \"\"\"\n        rng = np.random.default_rng(12345)\n        \n        v_true = np.array([0.0, 0.0, 0.0])\n        num_tracks = 100\n        sigma_in = 0.03\n        mu_out = np.array([10.0, -10.0, 5.0])\n        sigma_out = 2.0\n        outlier_fractions = [0.0, 0.2, 0.4, 0.6]\n        breakdown_threshold = 1.0\n\n        p_break_huber = 1.0\n        p_break_student = 1.0\n        huber_failed = False\n        student_failed = False\n\n        for p in sorted(outlier_fractions):\n            num_inliers = int(np.floor((1 - p) * num_tracks))\n            num_outliers = num_tracks - num_inliers\n\n            inliers = rng.normal(loc=v_true, scale=sigma_in, size=(num_inliers, 3))\n            if num_outliers > 0:\n                outliers = rng.normal(loc=mu_out, scale=sigma_out, size=(num_outliers, 3))\n                tracks = np.vstack((inliers, outliers))\n            else:\n                tracks = inliers\n\n            if not huber_failed:\n                v_huber = irls_vertex_fit(tracks, sigma_in, w_huber, k_huber)\n                error_huber = np.linalg.norm(v_huber - v_true)\n                if error_huber > breakdown_threshold:\n                    p_break_huber = p\n                    huber_failed = True\n\n            if not student_failed:\n                v_student = irls_vertex_fit(tracks, sigma_in, w_student_t, nu_student_t, d_student_t)\n                error_student = np.linalg.norm(v_student - v_true)\n                if error_student > breakdown_threshold:\n                    p_break_student = p\n                    student_failed = True\n            \n            if huber_failed and student_failed:\n                break\n\n        return p_break_huber, p_break_student\n\n    p_break_huber, p_break_student = run_breakdown_study()\n\n    # --- Final Output Formatting ---\n    results = [\n        *weights_huber,\n        *weights_student_t,\n        p_break_huber,\n        p_break_student\n    ]\n    \n    print(f\"[{','.join(f'{r:.7f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}