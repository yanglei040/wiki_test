{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of charged lepton reconstruction is the measurement of its momentum. In a collider detector's magnetic field, a lepton's trajectory bends, and the radius of this curvature is inversely proportional to its transverse momentum, $p_T$. This hands-on derivation challenges you to connect the geometric measurement of this curvature—the sagitta—to the lepton's momentum and its ultimate resolution. By working through this problem , you will gain a fundamental understanding of how detector geometry and measurement precision dictate the performance of momentum reconstruction.",
            "id": "3520912",
            "problem": "In a high-energy collider detector, muon transverse momentum reconstruction in the bending plane relies on measuring the trajectory curvature produced by the Lorentz force in a magnetic field. Consider a charged particle of charge magnitude $|q|$ and transverse momentum $p_T$ traversing a region with an approximately track-averaged magnetic field component perpendicular to the trajectory, denoted $B$. The trajectory segment between two outer precision measurement stations is approximated as a circular arc of radius $R$ subtending a chord of length $L$ in the bending plane. The maximum deviation of the arc from the chord, the sagitta $s$, is measured using a middle precision station. Assume the small-deflection limit $L \\ll R$.\n\nFoundational relations that may be taken as starting points are:\n- The balance of Lorentz force and centripetal force in the transverse plane implies a relation between $p_T$, $B$, $R$, and $|q|$.\n- The exact geometric sagitta for a circular arc of radius $R$ and chord length $L$ is $s = R - \\sqrt{R^2 - (L/2)^2}$.\n\nSuppose the sagitta measurement has a statistical uncertainty $\\sigma_s$ due to finite spatial resolution of the tracking detectors, and, in addition, the spectrometer alignment contributes an independent, configuration-stable sagitta uncertainty $\\sigma_a$ (assumed uncorrelated with $\\sigma_s$). Both uncertainties are treated as Gaussian and independent, so that they add in quadrature. Throughout, use the practical high-energy physics unit convention $p_T \\, [\\mathrm{GeV}] = c_B \\, |q| \\, B \\, [\\mathrm{T}] \\, R \\, [\\mathrm{m}]$ with $c_B \\approx 0.3$, and take $B$ to represent the track-averaged perpendicular field relevant for bending in either a solenoidal or toroidal spectrometer.\n\nStarting only from the stated fundamental relations and approximations, derive a closed-form analytic expression for the relative transverse momentum resolution $\\sigma_{p_T}/p_T$ due to the combined sagitta measurement and alignment uncertainties, expressed in terms of $p_T$, $B$, $L$, $|q|$, $c_B$, $\\sigma_s$, and $\\sigma_a$. Your final expression must make the scaling with $L$ and $B$ explicit. Provide your final answer as a single closed-form expression. The final answer is dimensionless; no numerical evaluation is required and no units should be included in the final boxed answer.",
            "solution": "The principles of the problem are validated as scientifically grounded, well-posed, and objective. The problem describes a standard scenario in high-energy physics for momentum reconstruction of charged particles. We shall proceed with the derivation.\n\nThe starting points are the two foundational relations provided. First, the relationship between the transverse momentum $p_T$, the magnetic field $B$, and the radius of curvature $R$ for a particle with charge magnitude $|q|$:\n$$p_T = c_B |q| B R$$\nHere, $c_B$ is a constant of proportionality that handles the unit conversions as specified in the problem, approximately equal to $0.3$.\n\nSecond, the exact geometric relationship between the sagitta $s$, the radius of curvature $R$, and the chord length $L$:\n$$s = R - \\sqrt{R^2 - \\left(\\frac{L}{2}\\right)^2}$$\n\nThe problem stipulates the use of the small-deflection limit, where $L \\ll R$. We can exploit this condition to find a simplified, approximate expression for the sagitta $s$. We factor out $R$ from the square root term:\n$$s = R - R \\sqrt{1 - \\frac{L^2}{4R^2}}$$\nSince $L \\ll R$, the term $\\frac{L^2}{4R^2}$ is much smaller than $1$. We can therefore use the binomial approximation for the square root, $\\sqrt{1-x} \\approx 1 - \\frac{x}{2}$ for small $x$. Letting $x = \\frac{L^2}{4R^2}$, we get:\n$$s \\approx R - R \\left(1 - \\frac{1}{2} \\frac{L^2}{4R^2}\\right) = R - \\left(R - \\frac{RL^2}{8R^2}\\right) = \\frac{L^2}{8R}$$\nThis gives the well-known approximate relation for the sagitta:\n$$s \\approx \\frac{L^2}{8R}$$\n\nOur goal is to find the relative momentum resolution $\\frac{\\sigma_{p_T}}{p_T}$. To do this, we must first express $p_T$ in terms of the measured quantity, which is the sagitta $s$. From the sagitta approximation, we can express the radius of curvature $R$ as:\n$$R \\approx \\frac{L^2}{8s}$$\nSubstituting this expression for $R$ into the momentum equation yields:\n$$p_T \\approx c_B |q| B \\left(\\frac{L^2}{8s}\\right) = \\frac{c_B |q| B L^2}{8s}$$\n\nNow we can determine the uncertainty in $p_T$ that arises from the uncertainty in the measurement of $s$. The problem states that the total sagitta uncertainty is the quadrature sum of the statistical uncertainty $\\sigma_s$ and the alignment uncertainty $\\sigma_a$, which are independent. Let the total sagitta uncertainty be $\\sigma_{s, \\text{tot}}$.\n$$\\sigma_{s, \\text{tot}}^2 = \\sigma_s^2 + \\sigma_a^2 \\implies \\sigma_{s, \\text{tot}} = \\sqrt{\\sigma_s^2 + \\sigma_a^2}$$\n\nTo find the uncertainty in $p_T$, we use standard error propagation. In this derivation, the quantities $c_B$, $|q|$, $B$, and $L$ are treated as known constants without uncertainty. The momentum $p_T$ is a function of the single uncertain variable $s$.\n$$p_T(s) = \\left(\\frac{c_B |q| B L^2}{8}\\right) s^{-1}$$\nThe uncertainty $\\sigma_{p_T}$ is related to the uncertainty $\\sigma_{s, \\text{tot}}$ by:\n$$\\sigma_{p_T} = \\left|\\frac{dp_T}{ds}\\right| \\sigma_{s, \\text{tot}}$$\nWe compute the derivative of $p_T$ with respect to $s$:\n$$\\frac{dp_T}{ds} = \\frac{d}{ds} \\left(\\frac{c_B |q| B L^2}{8s}\\right) = -\\frac{c_B |q| B L^2}{8s^2}$$\nWe can re-express this derivative in terms of $p_T$ itself:\n$$\\frac{dp_T}{ds} = -\\frac{1}{s} \\left(\\frac{c_B |q| B L^2}{8s}\\right) = -\\frac{p_T}{s}$$\nThe magnitude of the derivative is therefore:\n$$\\left|\\frac{dp_T}{ds}\\right| = \\frac{p_T}{s}$$\nSubstituting this into the error propagation formula gives:\n$$\\sigma_{p_T} = \\frac{p_T}{s} \\sigma_{s, \\text{tot}}$$\nThe relative transverse momentum resolution is then:\n$$\\frac{\\sigma_{p_T}}{p_T} = \\frac{\\sigma_{s, \\text{tot}}}{s} = \\frac{\\sqrt{\\sigma_s^2 + \\sigma_a^2}}{s}$$\n\nThe final step is to express this result in terms of the specified independent variables, which include $p_T$ but not $s$. We use the relationship derived earlier, $p_T \\approx \\frac{c_B |q| B L^2}{8s}$, to eliminate $s$:\n$$s \\approx \\frac{c_B |q| B L^2}{8 p_T}$$\nSubstituting this expression for $s$ into our equation for the relative resolution:\n$$\\frac{\\sigma_{p_T}}{p_T} = \\frac{\\sqrt{\\sigma_s^2 + \\sigma_a^2}}{\\frac{c_B |q| B L^2}{8 p_T}}$$\nSimplifying this expression yields the final closed-form result for the relative transverse momentum resolution:\n$$\\frac{\\sigma_{p_T}}{p_T} = \\frac{8 p_T \\sqrt{\\sigma_s^2 + \\sigma_a^2}}{c_B |q| B L^2}$$\nThis expression explicitly shows the scaling of the momentum resolution with the transverse momentum $p_T$, the magnetic field $B$, and the chord length $L$. Specifically, the resolution degrades linearly with increasing $p_T$ and improves with a stronger magnetic field ($B^{-1}$) and quadratically with a longer lever arm ($L^{-2}$).",
            "answer": "$$\\boxed{\\frac{8 p_T \\sqrt{\\sigma_s^2 + \\sigma_a^2}}{c_B |q| B L^2}}$$"
        },
        {
            "introduction": "While momentum measurement is foundational, electrons present a unique challenge due to their significant energy loss via bremsstrahlung. This radiation process is highly stochastic and its effect on the electron's energy is described by a non-Gaussian probability distribution, rendering simple Kalman filters inadequate. This exercise  introduces the Gaussian-Sum Filter (GSF), an advanced technique that models the non-Gaussian energy loss as a mixture of Gaussian components, allowing for a more robust estimation. You will implement a complete prediction-update cycle, providing you with practical experience in handling the complex stochastic nature of electron interactions.",
            "id": "3520836",
            "problem": "You are tasked with constructing a computationally explicit Gaussian-sum filter (GSF) for electron energy propagation through material where the dominant stochastic process is bremsstrahlung. The target domain is lepton and photon reconstruction and identification in computational high-energy physics, where energy loss due to electron bremsstrahlung is known to be non-Gaussian. Your objective is to implement a one-step GSF prediction and measurement update in a log-energy state, and then report the posterior moments in the original energy domain.\n\nAssume the following modeling choices to maintain scientific realism while enabling a precise, testable program:\n- The state is the logarithm of energy, $x = \\ln E$, with $E$ measured in $\\mathrm{GeV}$.\n- The prior state $x$ is Gaussian, $x \\sim \\mathcal{N}(m_0, P_0)$, representing a tracker-informed estimate of the electron energy before encountering a specific material layer.\n- Passage through a material layer of thickness producing bremsstrahlung is captured by an additive increment $\\Delta x$ in log-energy with a non-Gaussian distribution. To accommodate this within a recursive filter, approximate the distribution of $\\Delta x$ by a finite Gaussian mixture with $K$ components:\n  - weights $\\alpha_k$ (non-negative and summing to $1$),\n  - component means $\\mu_k$,\n  - component variances $S_k$,\n  so that $\\Delta x$ is distributed according to the mixture $\\sum_{k=1}^{K} \\alpha_k \\,\\mathcal{N}(\\mu_k,S_k)$.\n- A direct measurement of the log-energy is available as $y = x + v$, with $v \\sim \\mathcal{N}(0,R)$ independent of the process noise, representing a logarithmic transform of an energy measurement with multiplicative resolution, which is a standard modeling device under approximately constant fractional resolution. The measurement noise variance $R$ is provided.\n\nYour program must:\n1. Construct the Gaussian-sum filter for one predict-update cycle in the log-energy domain $x$. Start from the prior $x \\sim \\mathcal{N}(m_0,P_0)$ and the process mixture $\\{(\\alpha_k,\\mu_k,S_k)\\}_{k=1}^K$, propagate to obtain a mixture posterior over $x$ after incorporating the measurement $y$ with variance $R$. The mixture weight update must be based on the marginal likelihood of $y$ under each predicted component, and all weights must be renormalized to sum to $1$.\n2. Convert the resulting posterior mixture in $x$ back to the energy domain to obtain the posterior mean $\\mathbb{E}[E]$ and variance $\\mathrm{Var}[E]$ by exact moment transformations of log-normal components and linear mixture combination.\n3. For each test case, output a list containing the posterior mean energy and posterior variance of energy, expressed respectively in $\\mathrm{GeV}$ and $\\mathrm{GeV}^2$ as floating-point numbers.\n\nFundamental base you must adhere to without using any shortcut formulas:\n- Bayesian updating of probabilities and densities.\n- The linear-Gaussian Kalman filter (KF) formulas in the scalar case as a specialization of Bayesian inference for Gaussian likelihoods and priors.\n- The transformation properties of Gaussian random variables and mixtures under affine operations.\n- The exact moments of a log-normal distribution.\n\nTest suite and parameters:\nImplement your program using the following three test cases. Each case provides $(m_0,P_0)$ for the prior in the log-energy domain, a process mixture $\\{(\\alpha_k,\\mu_k,S_k)\\}$ for the log-energy increment $\\Delta x$, and a measurement $(y,R)$ in the log-energy domain. All energies are in $\\mathrm{GeV}$, and you must express final answers in $\\mathrm{GeV}$ (for the mean) and $\\mathrm{GeV}^2$ (for the variance).\n- Case A (general non-Gaussian propagation with moderate losses, and a moderately informative measurement):\n  - Prior: $m_0 = \\ln 50$, $P_0 = (0.05)^2$.\n  - Process mixture ($K=3$):\n    - Component $1$: $\\alpha_1 = 0.6$, $\\mu_1 = \\ln 0.95$, $S_1 = (0.01)^2$.\n    - Component $2$: $\\alpha_2 = 0.3$, $\\mu_2 = \\ln 0.75$, $S_2 = (0.02)^2$.\n    - Component $3$: $\\alpha_3 = 0.1$, $\\mu_3 = \\ln 0.40$, $S_3 = (0.05)^2$.\n  - Measurement: $y = \\ln 34$, $R = (0.05)^2$.\n- Case B (boundary condition with no material effects; tests identity behavior):\n  - Prior: $m_0 = \\ln 50$, $P_0 = (0.05)^2$.\n  - Process mixture ($K=1$):\n    - Component $1$: $\\alpha_1 = 1.0$, $\\mu_1 = 0.0$, $S_1 = 0.0$.\n  - Measurement: $y = \\ln 49$, $R = (0.02)^2$.\n- Case C (edge case with strong non-Gaussian losses and relatively informative measurement):\n  - Prior: $m_0 = \\ln 20$, $P_0 = (0.07)^2$.\n  - Process mixture ($K=3$):\n    - Component $1$: $\\alpha_1 = 0.3$, $\\mu_1 = \\ln 0.90$, $S_1 = (0.015)^2$.\n    - Component $2$: $\\alpha_2 = 0.4$, $\\mu_2 = \\ln 0.60$, $S_2 = (0.03)^2$.\n    - Component $3$: $\\alpha_3 = 0.3$, $\\mu_3 = \\ln 0.20$, $S_3 = (0.08)^2$.\n  - Measurement: $y = \\ln 8$, $R = (0.04)^2$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of three entries, one per test case, where each entry is itself a two-element list containing the posterior mean energy and posterior variance of energy in the units specified. The exact format must be a single-line string of the form:\n  - $[ [m_A,v_A], [m_B,v_B], [m_C,v_C] ]$\n  with no additional whitespace requirements enforced by the grader. For example, a syntactically valid output would look like $[[m_A,v_A],[m_B,v_B],[m_C,v_C]]$ where $m_A$, $v_A$, $m_B$, $v_B$, $m_C$, $v_C$ are floating-point numbers.",
            "solution": "The problem requires the construction and application of a one-step Gaussian-sum filter (GSF) to estimate an electron's energy after it passes through a material layer, a process dominated by non-Gaussian energy loss from bremsstrahlung. The entire procedure is performed in the logarithmic energy domain, with final results transformed back to the energy domain.\n\nWe are given a prior estimate of the state, which is the logarithm of energy $x = \\ln E$ (with $E$ in $\\mathrm{GeV}$). This prior is Gaussian:\n$$\np(x_{\\text{prior}}) = \\mathcal{N}(x; m_0, P_0)\n$$\nThe change in log-energy, $\\Delta x$, due to passage through the material is modeled as a random variable drawn from a Gaussian Mixture Model (GMM) with $K$ components:\n$$\np(\\Delta x) = \\sum_{k=1}^{K} \\alpha_k \\mathcal{N}(\\Delta x; \\mu_k, S_k)\n$$\nwhere $\\alpha_k$ are the mixture weights ($\\alpha_k \\ge 0, \\sum_{k=1}^{K} \\alpha_k = 1$), $\\mu_k$ are the component means, and $S_k$ are the component variances.\n\nA measurement $y$ of the log-energy is available, with a Gaussian noise model:\n$$\np(y|x) = \\mathcal{N}(y; x, R)\n$$\nwhere $R$ is the measurement variance.\n\nThe task comprises three main stages: the prediction step, the update step, and the transformation of moments back to the energy domain.\n\n**1. GSF Prediction Step**\n\nThe prediction step propagates the prior state distribution through the process model to obtain the predicted state distribution. The state transition is $x_{\\text{pred}} = x_{\\text{prior}} + \\Delta x$. The distribution of the predicted state, $p(x_{\\text{pred}})$, is the convolution of the prior distribution and the process noise distribution:\n$$\np(x_{\\text{pred}}) = p(x_{\\text{prior}}) * p(\\Delta x)\n$$\nSince the prior is a single Gaussian, which can be seen as a GMM with one component, and the process noise is a GMM, the convolution results in another GMM.\n$$\np(x_{\\text{pred}}) = \\mathcal{N}(x; m_0, P_0) * \\left( \\sum_{k=1}^{K} \\alpha_k \\mathcal{N}(\\Delta x; \\mu_k, S_k) \\right)\n$$\nUsing the distributive property of convolution:\n$$\np(x_{\\text{pred}}) = \\sum_{k=1}^{K} \\alpha_k \\left( \\mathcal{N}(x; m_0, P_0) * \\mathcal{N}(\\Delta x; \\mu_k, S_k) \\right)\n$$\nThe convolution of two Gaussian distributions $\\mathcal{N}(m_a, P_a)$ and $\\mathcal{N}(m_b, P_b)$ is another Gaussian $\\mathcal{N}(m_a+m_b, P_a+P_b)$. Applying this to each term in the sum, we find that the predicted distribution is a GMM with $K$ components:\n$$\np(x_{\\text{pred}}) = \\sum_{k=1}^{K} w_{k, \\text{pred}} \\mathcal{N}(x; m_{k, \\text{pred}}, P_{k, \\text{pred}})\n$$\nThe parameters of the $k$-th predicted component are:\n- Weight: $w_{k, \\text{pred}} = \\alpha_k$\n- Mean: $m_{k, \\text{pred}} = m_0 + \\mu_k$\n- Variance: $P_{k, \\text{pred}} = P_0 + S_k$\n\n**2. GSF Update Step**\n\nThe update step incorporates the measurement $y$ to refine the predicted distribution, yielding the posterior distribution $p(x|y)$. This is done by applying Bayes' theorem to each component of the predicted GMM.\n\nFor each component $k$, we have a Gaussian prior $\\mathcal{N}(x; m_{k, \\text{pred}}, P_{k, \\text{pred}})$ and a Gaussian likelihood $p(y|x) = \\mathcal{N}(y; x, R)$.\n\nFirst, we calculate the marginal likelihood (or evidence) of the measurement $y$ for each component hypothesis $k$. This is the probability of observing $y$ given that the state originated from component $k$'s distribution:\n$$\nL_k \\equiv p(y|\\text{comp } k) = \\int p(y|x) p(x|\\text{comp } k) dx = \\int \\mathcal{N}(y; x, R) \\mathcal{N}(x; m_{k, \\text{pred}}, P_{k, \\text{pred}}) dx\n$$\nThis integral evaluates to a Gaussian distribution for $y$:\n$$\nL_k = \\mathcal{N}(y; m_{k, \\text{pred}}, P_{k, \\text{pred}} + R) = \\frac{1}{\\sqrt{2\\pi(P_{k, \\text{pred}} + R)}} \\exp\\left(-\\frac{(y - m_{k, \\text{pred}})^2}{2(P_{k, \\text{pred}} + R)}\\right)\n$$\nThe posterior weight for each component is proportional to its prior weight multiplied by its likelihood. The unnormalized posterior weight is:\n$$\n\\tilde{w}_{k, \\text{post}} = w_{k, \\text{pred}} L_k = \\alpha_k L_k\n$$\nThese weights are then normalized to sum to $1$:\n$$\nw_{k, \\text{post}} = \\frac{\\tilde{w}_{k, \\text{post}}}{\\sum_{j=1}^{K} \\tilde{w}_{j, \\text{post}}}\n$$\nNext, we update the mean and variance for each component using the standard scalar Kalman filter equations, which are the result of applying Bayes' rule to the Gaussian distributions.\nThe Kalman gain for component $k$ is:\n$$\nK_g^{(k)} = \\frac{P_{k, \\text{pred}}}{P_{k, \\text{pred}} + R}\n$$\nThe updated (posterior) mean for component $k$ is:\n$$\nm_{k, \\text{post}} = m_{k, \\text{pred}} + K_g^{(k)} (y - m_{k, \\text{pred}})\n$$\nThe updated (posterior) variance for component $k$ is:\n$$\nP_{k, \\text{post}} = (1 - K_g^{(k)}) P_{k, \\text{pred}}\n$$\nThe final posterior distribution of the log-energy $x$ is the new GMM formed by these updated components:\n$$\np(x|y) = \\sum_{k=1}^{K} w_{k, \\text{post}} \\mathcal{N}(x; m_{k, \\text{post}}, P_{k, \\text{post}})\n$$\n\n**3. Moment Transformation to the Energy Domain**\n\nThe posterior distribution for the energy $E = e^x$ is a mixture of log-normal distributions, since each component of the posterior for $x$ is Gaussian. If a random variable $x$ follows $\\mathcal{N}(m, P)$, then $E = e^x$ follows a log-normal distribution. The moments of this log-normal distribution are:\n- Mean: $\\mathbb{E}[E] = e^{m + P/2}$\n- Second Moment: $\\mathbb{E}[E^2] = e^{2m + 2P}$\n- Variance: $\\mathrm{Var}[E] = \\mathbb{E}[E^2] - (\\mathbb{E}[E])^2 = (e^P - 1)e^{2m+P}$\n\nFor a mixture distribution, the total moments are the weighted sum of the component moments. The posterior distribution for $E$ is $p(E|y) = \\sum_{k=1}^{K} w_{k, \\text{post}} \\text{LogNormal}(E; m_{k, \\text{post}}, P_{k, \\text{post}})$.\n\nThe total posterior mean of the energy $\\mathbb{E}[E|y]$ is the weighted sum of the means of each log-normal component:\n$$\n\\mathbb{E}[E|y] = \\sum_{k=1}^{K} w_{k, \\text{post}} \\mathbb{E}_k[E] = \\sum_{k=1}^{K} w_{k, \\text{post}} e^{m_{k, \\text{post}} + P_{k, \\text{post}}/2}\n$$\nSimilarly, the total posterior second moment $\\mathbb{E}[E^2|y]$ is:\n$$\n\\mathbb{E}[E^2|y] = \\sum_{k=1}^{K} w_{k, \\text{post}} \\mathbb{E}_k[E^2] = \\sum_{k=1}^{K} w_{k, \\text{post}} e^{2m_{k, \\text{post}} + 2P_{k, \\text{post}}}\n$$\nFinally, the posterior variance of the energy, $\\mathrm{Var}[E|y]$, is calculated using the definition of variance:\n$$\n\\mathrm{Var}[E|y] = \\mathbb{E}[E^2|y] - (\\mathbb{E}[E|y])^2\n$$\nThis procedure provides the required posterior mean energy in $\\mathrm{GeV}$ and variance in $\\mathrm{GeV}^2$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and compute solutions.\n    \"\"\"\n\n    # Test suite and parameters\n    test_cases = [\n        # Case A\n        {\n            \"prior\": {\"m0\": np.log(50), \"P0\": 0.05**2},\n            \"process_mixture\": [\n                {\"alpha\": 0.6, \"mu\": np.log(0.95), \"S\": 0.01**2},\n                {\"alpha\": 0.3, \"mu\": np.log(0.75), \"S\": 0.02**2},\n                {\"alpha\": 0.1, \"mu\": np.log(0.40), \"S\": 0.05**2},\n            ],\n            \"measurement\": {\"y\": np.log(34), \"R\": 0.05**2},\n        },\n        # Case B\n        {\n            \"prior\": {\"m0\": np.log(50), \"P0\": 0.05**2},\n            \"process_mixture\": [\n                {\"alpha\": 1.0, \"mu\": 0.0, \"S\": 0.0},\n            ],\n            \"measurement\": {\"y\": np.log(49), \"R\": 0.02**2},\n        },\n        # Case C\n        {\n            \"prior\": {\"m0\": np.log(20), \"P0\": 0.07**2},\n            \"process_mixture\": [\n                {\"alpha\": 0.3, \"mu\": np.log(0.90), \"S\": 0.015**2},\n                {\"alpha\": 0.4, \"mu\": np.log(0.60), \"S\": 0.03**2},\n                {\"alpha\": 0.3, \"mu\": np.log(0.20), \"S\": 0.08**2},\n            ],\n            \"measurement\": {\"y\": np.log(8), \"R\": 0.04**2},\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = gsf_one_step(\n            case[\"prior\"][\"m0\"],\n            case[\"prior\"][\"P0\"],\n            case[\"process_mixture\"],\n            case[\"measurement\"][\"y\"],\n            case[\"measurement\"][\"R\"],\n        )\n        results.append(list(result))\n    \n    # Format the output string exactly as specified, without extra spaces.\n    formatted_results = \",\".join([f\"[{r[0]},{r[1]}]\" for r in results])\n    print(f\"[{formatted_results}]\")\n\ndef gaussian_pdf(x, mean, var):\n    \"\"\"\n    Computes the probability density function of a Gaussian distribution.\n    \"\"\"\n    if var = 0:\n        # Handle the case of zero variance carefully, though not expected with non-zero measurement noise.\n        # This is more of a delta function. The likelihood logic will need care.\n        # For simplicity in this problem context, we can assume var > 0.\n        return np.inf if x == mean else 0.0\n    \n    prefactor = 1.0 / np.sqrt(2.0 * np.pi * var)\n    exponent = -0.5 * ((x - mean)**2 / var)\n    return prefactor * np.exp(exponent)\n\ndef gsf_one_step(m0, P0, process_mixture, y, R):\n    \"\"\"\n    Performs a single predict-update cycle of a Gaussian-sum filter.\n\n    Args:\n        m0 (float): Mean of the prior distribution in log-energy.\n        P0 (float): Variance of the prior distribution in log-energy.\n        process_mixture (list of dicts): Parameters for the process noise GMM.\n        y (float): Measurement in log-energy.\n        R (float): Variance of the measurement noise.\n\n    Returns:\n        tuple: A tuple containing the posterior mean energy and posterior variance of energy.\n    \"\"\"\n    num_components = len(process_mixture)\n    \n    # --- 1. Prediction Step ---\n    predicted_components = []\n    for k in range(num_components):\n        comp = process_mixture[k]\n        alpha_k, mu_k, S_k = comp[\"alpha\"], comp[\"mu\"], comp[\"S\"]\n        \n        m_k_pred = m0 + mu_k\n        P_k_pred = P0 + S_k\n        \n        predicted_components.append({\"weight\": alpha_k, \"mean\": m_k_pred, \"var\": P_k_pred})\n        \n    # --- 2. Update Step ---\n    posterior_components = []\n    unnormalized_weights = np.zeros(num_components)\n    \n    for k in range(num_components):\n        pred_comp = predicted_components[k]\n        w_k_pred, m_k_pred, P_k_pred = pred_comp[\"weight\"], pred_comp[\"mean\"], pred_comp[\"var\"]\n        \n        # Calculate marginal likelihood (evidence) for this component\n        innovation_var = P_k_pred + R\n        likelihood = gaussian_pdf(y, m_k_pred, innovation_var)\n        \n        unnormalized_weights[k] = w_k_pred * likelihood\n        \n        # Calculate Kalman gain and update mean/variance\n        kalman_gain = P_k_pred / innovation_var\n        m_k_post = m_k_pred + kalman_gain * (y - m_k_pred)\n        P_k_post = (1.0 - kalman_gain) * P_k_pred\n        \n        posterior_components.append({\"mean\": m_k_post, \"var\": P_k_post})\n\n    # Normalize posterior weights\n    total_weight = np.sum(unnormalized_weights)\n    if total_weight  0:\n        posterior_weights = unnormalized_weights / total_weight\n    else:\n        # Fallback to uniform weights if all likelihoods are zero (numerical underflow)\n        posterior_weights = np.ones(num_components) / num_components\n\n    for k in range(num_components):\n        posterior_components[k][\"weight\"] = posterior_weights[k]\n\n    # --- 3. Moment Transformation to Energy Domain ---\n    total_mean_E = 0.0\n    total_mean_E2 = 0.0\n\n    for k in range(num_components):\n        post_comp = posterior_components[k]\n        w_k_post, m_k_post, P_k_post = post_comp[\"weight\"], post_comp[\"mean\"], post_comp[\"var\"]\n        \n        # Mean of the k-th log-normal component\n        mean_E_k = np.exp(m_k_post + P_k_post / 2.0)\n        \n        # Second moment of the k-th log-normal component\n        mean_E2_k = np.exp(2.0 * m_k_post + 2.0 * P_k_post)\n        \n        total_mean_E += w_k_post * mean_E_k\n        total_mean_E2 += w_k_post * mean_E2_k\n        \n    # Calculate final variance\n    # Var(E) = E[E^2] - (E[E])^2\n    total_var_E = total_mean_E2 - total_mean_E**2\n\n    return total_mean_E, total_var_E\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Our final practice synthesizes track and energy information into a coherent event picture using the Particle Flow (PF) paradigm. The PF algorithm aims to reconstruct each individual particle by optimally combining measurements from all sub-detectors, such as linking a track to an electromagnetic calorimeter cluster to identify an electron. In this problem , you will implement the core decision logic to distinguish electrons from photons, identify bremsstrahlung radiation and photon conversions, and assemble the final particle candidates, gaining insight into the system-level logic of modern event reconstruction.",
            "id": "3520857",
            "problem": "You must implement, in code, a simplified but scientifically consistent Particle Flow (PF) categorization for electrons and photons in the context of lepton and photon reconstruction and identification. Start from fundamental physical and algorithmic bases and derive decision rules for linking charged-particle tracks to Electromagnetic Calorimeter (ECAL) clusters, identifying bremsstrahlung photons, recognizing photon conversions, and codifying object creation and merging. The program must process a fixed test suite and produce a single-line output aggregating the results.\n\nFoundational bases are as follows. Use energy-momentum conservation and directionality: if a charged lepton (electron) emits bremsstrahlung, the emitted photon direction is approximately along the electron’s trajectory. Let $\\vec{p}$ denote momentum, $E$ energy, $\\hat{u}$ a unit direction vector for tracks, and $\\hat{n}$ a unit direction vector for ECAL clusters. For small angles, the angular separation between a track and a cluster can be approximated by the opening angle $\\theta = \\arccos(\\hat{u} \\cdot \\hat{n})$, which in the central region is consistent with the usual $\\Delta R$ metric. Photon conversions in material yield two oppositely charged tracks of small opening angle whose initial segments have missing inner hits; their sum direction points to the ECAL energy deposit. Electrons deposit their energy predominantly in the ECAL; bremsstrahlung photons are emitted along the electron’s path and are captured by nearby ECAL clusters.\n\nImplement the following mathematically defined decision rules.\n\n1. Track–cluster angular distance: for each track $i$ and cluster $j$, compute\n$$\nd_{ij} = \\arccos\\!\\left( \\hat{u}_i \\cdot \\hat{n}_j \\right).\n$$\nA track and cluster are considered geometrically linked when $d_{ij} \\le \\Delta R_{\\mathrm{link}}$, with $\\Delta R_{\\mathrm{link}} = 0.03$ $\\mathrm{rad}$.\n\n2. Conversion identification: for any ECAL cluster $j$ linked to at least two tracks $i$ and $\\ell$, declare a converted photon if all conditions hold:\n- Opposite charges: $q_i \\cdot q_\\ell = -1$.\n- Small opening angle between the two tracks: $\\theta_{i\\ell} = \\arccos\\!\\left( \\hat{u}_i \\cdot \\hat{u}_\\ell \\right) \\le \\theta_{\\mathrm{conv}}$, with $\\theta_{\\mathrm{conv}} = 0.02$ $\\mathrm{rad}$.\n- Missing inner hits sufficient for both tracks: $h_i \\ge h_{\\mathrm{conv}}$ and $h_\\ell \\ge h_{\\mathrm{conv}}$, with $h_{\\mathrm{conv}} = 0.6$.\nIf these conditions are met, create a converted photon object with energy $E_{\\gamma}^{\\mathrm{conv}} = E_j$ and mark the two tracks and the cluster as consumed.\n\n3. Electron identification and primary cluster selection: for each remaining track $i$, among clusters $j$ not consumed, find the set with $d_{ij} \\le \\Delta R_{\\mathrm{link}}$ and choose a primary cluster $j^\\star$ as the one minimizing $d_{ij}$. A track becomes an electron if its primary cluster energy satisfies\n$$\nE_{j^\\star} \\ge \\alpha \\, p_i, \\quad \\text{with } \\alpha = 0.5.\n$$\nIf identified as an electron, mark the primary cluster as consumed.\n\n4. Bremsstrahlung cluster association and merging: for an identified electron track $i$ and its primary cluster $j^\\star$, any additional cluster $k$ not yet consumed that satisfies $d_{ik} \\le \\Delta R_{\\mathrm{brem}}$ with $\\Delta R_{\\mathrm{brem}} = 0.05$ $\\mathrm{rad}$ and is closest to track $i$ among all tracks is considered a bremsstrahlung photon cluster and is merged into the electron’s energy. The reconstructed electron energy is\n$$\nE_e = E_{j^\\star} + \\sum_{k \\in \\mathcal{B}_i} E_k,\n$$\nwhere $\\mathcal{B}_i$ is the set of bremsstrahlung clusters associated to track $i$. Mark any merged bremsstrahlung clusters as consumed.\n\n5. Unconverted photon creation: any remaining ECAL cluster not consumed by steps $2$–$4$ becomes an unconverted photon with energy $E_\\gamma = E_j$.\n\nGeometric representation of directions will use spherical coordinates to construct unit vectors. For angles $(\\theta, \\phi)$, define the unit vector\n$$\n\\hat{n}(\\theta,\\phi) = \\left( \\sin\\theta \\cos\\phi,\\; \\sin\\theta \\sin\\phi,\\; \\cos\\theta \\right).\n$$\nAngles must be provided and used in $\\mathrm{radians}$. Energies must be expressed in $\\mathrm{GeV}$.\n\nObject coding for output is as follows: electron $\\rightarrow 0$, unconverted photon $\\rightarrow 1$, converted photon $\\rightarrow 2$. Each reconstructed object must be represented as a list of the form $[\\mathrm{code}, E]$, where $E$ is the object energy in $\\mathrm{GeV}$ rounded to two decimal places.\n\nTest suite. Implement the flow on the following four cases, which test a general case, a boundary, a conversion, and an ambiguity resolution:\n\n- Case $1$ (electron with bremsstrahlung and an isolated photon):\n  - Tracks: one electron-like track with $p = 50$ $\\mathrm{GeV}$, $q = -1$, $h = 0.1$, $(\\theta, \\phi) = (1.2, 0.1)$.\n  - ECAL clusters: primary candidate $E = 45$ $\\mathrm{GeV}$ at $(\\theta, \\phi) = (1.21, 0.1)$; bremsstrahlung candidate $E = 8$ $\\mathrm{GeV}$ at $(\\theta, \\phi) = (1.23, 0.1)$; isolated photon $E = 20$ $\\mathrm{GeV}$ at $(\\theta, \\phi) = (0.3, 2.0)$.\n\n- Case $2$ (boundary failure of link: cluster just outside link threshold):\n  - Tracks: one track with $p = 20$ $\\mathrm{GeV}$, $q = +1$, $h = 0.1$, $(\\theta, \\phi) = (1.0, 0.5)$.\n  - ECAL clusters: one cluster with $E = 20$ $\\mathrm{GeV}$ at $(\\theta, \\phi) = (1.032, 0.5)$.\n\n- Case $3$ (converted photon):\n  - Tracks: two tracks with $(p, q, h, \\theta, \\phi)$ equal to $(15, +1, 0.7, 1.60, 0.2)$ and $(14, -1, 0.7, 1.61, 0.2)$.\n  - ECAL clusters: one cluster with $E = 29$ $\\mathrm{GeV}$ at $(\\theta, \\phi) = (1.605, 0.2)$.\n\n- Case $4$ (ambiguous cluster near two tracks, non-conversion, and a low-energy nearby cluster):\n  - Tracks: $(p, q, h, \\theta, \\phi)$ equal to $(40, -1, 0.1, 0.70, -1.0)$ and $(38, +1, 0.1, 0.74, -1.0)$.\n  - ECAL clusters: shared candidate $E = 42$ $\\mathrm{GeV}$ at $(\\theta, \\phi) = (0.715, -1.0)$; nearby low-energy cluster $E = 5$ $\\mathrm{GeV}$ at $(\\theta, \\phi) = (0.76, -1.0)$.\n\nRequired output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes its list of reconstructed objects. For example, the outermost list has four elements, one per case, and each element is itself a list of object lists. Energies must be in $\\mathrm{GeV}$ and rounded to two decimal places. Angles must be in $\\mathrm{radians}$. The aggregated output must be a Python-style list literal, such as $[[[0, E_1], [1, E_2]], [[1, E_3]], [[2, E_4]], [[0, E_5], [1, E_6]]]$, with all $E_k$ in $\\mathrm{GeV}$ rounded to two decimals.",
            "solution": "The problem statement is valid. It outlines a simplified, yet scientifically consistent, algorithmic procedure for particle identification based on the Particle Flow (PF) paradigm used in high-energy physics experiments. The problem is well-posed, providing a clear sequence of operations and a complete set of parameters and test data. The physical principles—such as energy-momentum conservation, the behavior of electrons and photons in a detector, and the signatures of processes like bremsstrahlung and pair conversion—are correctly, if simplified, represented. The mathematical formalism is explicit and sufficient for a unique solution to be algorithmically determined.\n\nThe solution proceeds by implementing the specified sequence of reconstruction steps. At the core of the algorithm is the systematic consumption of detector signals (charged-particle tracks and calorimeter energy deposits, or clusters) to form physically meaningful objects (electrons and photons). The process is sequential to resolve ambiguities, with specific, high-purity signatures like photon conversions being identified first.\n\nFirst, we define the geometric representation. A particle's trajectory or an energy cluster's position is given by spherical coordinates $(\\theta, \\phi)$. To compute angular separations, we convert these into three-dimensional Cartesian unit vectors $\\hat{n}$ using the standard transformation:\n$$\n\\hat{n}(\\theta,\\phi) = \\left( \\sin\\theta \\cos\\phi,\\; \\sin\\theta \\sin\\phi,\\; \\cos\\theta \\right)\n$$\nThe angular distance $d_{12}$ between two directions represented by unit vectors $\\hat{u}_1$ and $\\hat{u}_2$ is given by the arc-cosine of their dot product, which is derived from the spherical law of cosines:\n$$\nd_{12} = \\arccos(\\hat{u}_1 \\cdot \\hat{u}_2)\n$$\nThis distance metric is fundamental to all linking and association steps.\n\nThe reconstruction logic is executed for each test case as a sequence of mutually exclusive steps. Objects are marked as \"consumed\" once assigned to a reconstructed particle to prevent their reuse.\n\n1.  **Conversion Identification**: The first step searches for the distinct signature of a photon converting into an electron-positron pair within the detector material. A high-energy photon ($E \\gtrsim 2m_e c^2$) interacting with the electromagnetic field of an atomic nucleus can produce a pair of oppositely charged leptons. In a detector, this appears as an ECAL cluster with no associated track pointing to it from the interaction point, but with two nearby, oppositely charged tracks originating from a common vertex displaced from the primary interaction point. Our algorithm models this by seeking an ECAL cluster $j$ that is geometrically linked to two tracks, $i$ and $\\ell$, (i.e., $d_{ij}, d_{\\ell j} \\le \\Delta R_{\\mathrm{link}} = 0.03$). For such a configuration to be a valid conversion, three conditions reflecting the physics must be met:\n    - The tracks must have opposite charges: $q_i \\cdot q_\\ell = -1$.\n    - The opening angle between the tracks must be small, characteristic of a pair produced from a single energetic parent: $\\theta_{i\\ell} = \\arccos(\\hat{u}_i \\cdot \\hat{u}_\\ell) \\le \\theta_{\\mathrm{conv}} = 0.02$ rad.\n    - The tracks must show evidence of originating from a secondary vertex, which is simplified to a requirement on a \"missing hits\" variable: $h_i, h_\\ell \\ge h_{\\mathrm{conv}} = 0.6$.\n    If all conditions hold, a converted photon is created with energy equal to the cluster energy, $E_{\\gamma}^{\\mathrm{conv}} = E_j$. The constituent tracks and the cluster are then marked as consumed.\n\n2.  **Electron Identification**: After searching for conversions, the algorithm attempts to identify electrons from the remaining unconsumed tracks and clusters. An electron is a charged particle that deposits the majority of its energy in the ECAL. This is modeled by linking a track $i$ to a primary ECAL cluster $j^\\star$. The primary cluster is chosen from all unconsumed clusters linked to the track ($d_{ij} \\le \\Delta R_{\\mathrm{link}}$) as the one with the smallest angular distance $d_{ij}$. A track is confirmed as an electron if the energy of its primary cluster is a significant fraction of the track's momentum, satisfying $E_{j^\\star} \\ge \\alpha \\, p_i$, with the threshold factor $\\alpha = 0.5$. If a track is identified as an electron, it and its primary cluster $j^\\star$ are consumed. This step is performed sequentially for all tracks; the first track to claim a cluster consumes it, making it unavailable for other tracks.\n\n3.  **Bremsstrahlung Cluster Association**: An electron traversing material radiates photons in a process called bremsstrahlung. These photons travel in nearly the same direction as the electron and deposit their energy in the ECAL, often in separate clusters near the electron's primary impact point. The algorithm accounts for this by searching for additional unconsumed clusters, $k$, in the vicinity of an identified electron track $i$. A cluster $k$ is considered a bremsstrahlung photon and merged with the electron if:\n    - It lies within a larger cone around the electron track: $d_{ik} \\le \\Delta R_{\\mathrm{brem}} = 0.05$ rad.\n    - To resolve ambiguity when a cluster is near multiple tracks, the electron track $i$ must be the closest of all tracks to the cluster $k$.\n    The energy of any such associated bremsstrahlung cluster is added to the electron's energy: $E_e = E_{j^\\star} + \\sum_{k \\in \\mathcal{B}_i} E_k$, where $\\mathcal{B}_i$ is the set of associated bremsstrahlung clusters. These merged clusters are then consumed.\n\n4.  **Unconverted Photon Identification**: Finally, any ECAL cluster that has not been consumed in the previous steps—i.e., it was not part of a conversion, nor was it a primary or bremsstrahlung cluster for an electron—is interpreted as an unconverted photon. This assumes that any significant energy deposit in the ECAL not associated with a charged particle track is likely from a photon originating from the primary interaction. An unconverted photon object is created with energy $E_\\gamma = E_j$ for each such remaining cluster $j$.\n\nThis complete sequence ensures every cluster and track is considered in a logical, physically motivated order to produce a final, exclusive list of reconstructed electrons and photons.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements a simplified Particle Flow (PF) categorization for electrons and photons.\n    The function processes a fixed test suite of four cases and prints the aggregated results.\n    \"\"\"\n    \n    # --- Constants from the problem statement ---\n    DELTA_R_LINK = 0.03\n    THETA_CONV = 0.02\n    H_CONV = 0.6\n    ALPHA = 0.5\n    DELTA_R_BREM = 0.05\n    \n    # --- Object codes ---\n    ELECTRON_CODE = 0\n    UNCONVERTED_PHOTON_CODE = 1\n    CONVERTED_PHOTON_CODE = 2\n\n    # --- Test Suite Data ---\n    test_cases = [\n        {\n            \"tracks\": [\n                {\"p\": 50, \"q\": -1, \"h\": 0.1, \"theta\": 1.2, \"phi\": 0.1}\n            ],\n            \"clusters\": [\n                {\"E\": 45, \"theta\": 1.21, \"phi\": 0.1},\n                {\"E\": 8, \"theta\": 1.23, \"phi\": 0.1},\n                {\"E\": 20, \"theta\": 0.3, \"phi\": 2.0}\n            ]\n        },\n        {\n            \"tracks\": [\n                {\"p\": 20, \"q\": +1, \"h\": 0.1, \"theta\": 1.0, \"phi\": 0.5}\n            ],\n            \"clusters\": [\n                {\"E\": 20, \"theta\": 1.032, \"phi\": 0.5}\n            ]\n        },\n        {\n            \"tracks\": [\n                {\"p\": 15, \"q\": +1, \"h\": 0.7, \"theta\": 1.60, \"phi\": 0.2},\n                {\"p\": 14, \"q\": -1, \"h\": 0.7, \"theta\": 1.61, \"phi\": 0.2}\n            ],\n            \"clusters\": [\n                {\"E\": 29, \"theta\": 1.605, \"phi\": 0.2}\n            ]\n        },\n        {\n            \"tracks\": [\n                {\"p\": 40, \"q\": -1, \"h\": 0.1, \"theta\": 0.70, \"phi\": -1.0},\n                {\"p\": 38, \"q\": +1, \"h\": 0.1, \"theta\": 0.74, \"phi\": -1.0}\n            ],\n            \"clusters\": [\n                {\"E\": 42, \"theta\": 0.715, \"phi\": -1.0},\n                {\"E\": 5, \"theta\": 0.76, \"phi\": -1.0}\n            ]\n        }\n    ]\n\n    def unit_vector(theta, phi):\n        \"\"\"Computes the Cartesian unit vector from spherical coordinates.\"\"\"\n        return np.array([\n            np.sin(theta) * np.cos(phi),\n            np.sin(theta) * np.sin(phi),\n            np.cos(theta)\n        ])\n\n    def angular_distance(v1, v2):\n        \"\"\"Computes the angular distance between two unit vectors.\"\"\"\n        dot_product = np.clip(np.dot(v1, v2), -1.0, 1.0)\n        return np.arccos(dot_product)\n\n    def process_case(tracks_data, clusters_data):\n        \"\"\"Applies the PF algorithm to a single case.\"\"\"\n        \n        # Initialize objects with unique IDs and consumed flags\n        tracks = [dict(t, id=i, consumed=False, vec=unit_vector(t['theta'], t['phi'])) for i, t in enumerate(tracks_data)]\n        clusters = [dict(c, id=i, consumed=False, vec=unit_vector(c['theta'], c['phi'])) for i, c in enumerate(clusters_data)]\n        \n        reco_particles = []\n\n        # 1. Conversion Identification\n        # Use a copy of cluster list to allow modification while iterating\n        for j, cluster in enumerate(clusters):\n            if cluster['consumed']:\n                continue\n            \n            linked_tracks = []\n            for i, track in enumerate(tracks):\n                if not track['consumed']:\n                    dist = angular_distance(track['vec'], cluster['vec'])\n                    if dist = DELTA_R_LINK:\n                        linked_tracks.append(track)\n            \n            if len(linked_tracks) = 2:\n                # Find the first valid conversion pair\n                found_conversion = False\n                for i1 in range(len(linked_tracks)):\n                    for i2 in range(i1 + 1, len(linked_tracks)):\n                        t1 = linked_tracks[i1]\n                        t2 = linked_tracks[i2]\n                        \n                        if t1['q'] * t2['q'] == -1 and \\\n                           angular_distance(t1['vec'], t2['vec']) = THETA_CONV and \\\n                           t1['h'] = H_CONV and t2['h'] = H_CONV:\n                            \n                            energy = round(cluster['E'], 2)\n                            reco_particles.append([CONVERTED_PHOTON_CODE, energy])\n                            \n                            cluster['consumed'] = True\n                            t1['consumed'] = True\n                            t2['consumed'] = True\n                            found_conversion = True\n                            break\n                    if found_conversion:\n                        break\n\n        # 2. Electron Identification and 4. Bremsstrahlung Association\n        identified_electrons = []\n        for track in tracks:\n            if track['consumed']:\n                continue\n\n            # Find primary cluster\n            best_cluster = None\n            min_dist = float('inf')\n            \n            linked_unconsumed_clusters = []\n            for cluster in clusters:\n                if not cluster['consumed']:\n                    dist = angular_distance(track['vec'], cluster['vec'])\n                    if dist = DELTA_R_LINK:\n                        linked_unconsumed_clusters.append((dist, cluster))\n            \n            if not linked_unconsumed_clusters:\n                continue\n\n            min_dist, best_cluster = min(linked_unconsumed_clusters, key=lambda x: x[0])\n\n            # Electron check\n            if best_cluster['E'] = ALPHA * track['p']:\n                track['consumed'] = True\n                best_cluster['consumed'] = True\n                electron_energy = best_cluster['E']\n                # identified_electrons will store electron track and its growing energy\n                identified_electrons.append({'track': track, 'energy': electron_energy})\n        \n        # Now handle bremsstrahlung for the identified electrons\n        for electron in identified_electrons:\n            electron_track = electron['track']\n            \n            brem_clusters_to_add = []\n            for cluster in clusters:\n                if cluster['consumed']:\n                    continue\n                \n                dist_to_electron = angular_distance(electron_track['vec'], cluster['vec'])\n                if dist_to_electron = DELTA_R_BREM:\n                    # Check if this electron track is the closest track to the cluster\n                    closest_track_dist = float('inf')\n                    closest_track = None\n                    for any_track in tracks:\n                        d = angular_distance(any_track['vec'], cluster['vec'])\n                        if d  closest_track_dist:\n                            closest_track_dist = d\n                            closest_track = any_track\n                    \n                    if closest_track['id'] == electron_track['id']:\n                        brem_clusters_to_add.append(cluster)\n            \n            for brem_cluster in brem_clusters_to_add:\n                electron['energy'] += brem_cluster['E']\n                brem_cluster['consumed'] = True\n            \n            energy = round(electron['energy'], 2)\n            reco_particles.append([ELECTRON_CODE, energy])\n\n        # 5. Unconverted Photon Creation\n        for cluster in clusters:\n            if not cluster['consumed']:\n                energy = round(cluster['E'], 2)\n                reco_particles.append([UNCONVERTED_PHOTON_CODE, energy])\n\n        return reco_particles\n\n    # --- Main Execution Loop ---\n    all_results = []\n    for case in test_cases:\n        result = process_case(case['tracks'], case['clusters'])\n        all_results.append(result)\n\n    # Format output as a single-line Python-style list literal string\n    # E.g., [[[0, 53.0], [1, 20.0]], [[1, 20.0]], ...]\n    # str() adds spaces, so we remove them.\n    output_str = str(all_results).replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}