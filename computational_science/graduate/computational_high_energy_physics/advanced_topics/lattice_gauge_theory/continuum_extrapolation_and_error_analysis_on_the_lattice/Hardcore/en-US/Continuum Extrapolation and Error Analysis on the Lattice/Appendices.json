{
    "hands_on_practices": [
        {
            "introduction": "Raw data from lattice simulations are not independent but form a correlated time series. Before attempting any continuum extrapolation, it is imperative to correctly determine the statistical error on the mean of an observable at each lattice spacing. This practice  guides you through the complete, essential workflow: from generating a realistic, autocorrelated time series to using the blocked bootstrap method to robustly propagate statistical uncertainties into the final continuum limit estimate. Mastering this pipeline is the foundation upon which all reliable error analysis is built.",
            "id": "3509811",
            "problem": "You are given the task of implementing a statistically sound continuum extrapolation for a lattice observable measured at multiple lattice spacings, accounting for autocorrelations in the time-series data. The program you write must estimate the integrated autocorrelation time, choose a blocking size that controls residual correlations, and propagate statistical errors into the continuum limit estimate via a blocked bootstrap procedure.\n\nStart from the following fundamental base:\n- For a stationary time series with mean $m$, variance $\\sigma^{2}$, and normalized autocorrelation function $\\rho(t)$ at integer lag $t$, the variance of the sample mean of $N$ correlated observations is increased by the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ defined as\n$$\n\\tau_{\\mathrm{int}}=\\frac{1}{2}+\\sum_{t=1}^{\\infty}\\rho(t),\n$$\nand a practical estimator replaces $\\infty$ by a finite window $W$ selected by a self-consistency rule.\n- Under Symanzik effective theory assumptions for a suitably improved lattice action and operator, leading cutoff effects for many observables scale as $\\mathcal{O}(a^{2})$, so one may model the $a$-dependence as\n$$\n\\mathcal{O}(a)=\\mathcal{O}_{0}+c_{1}\\,a^{2},\n$$\nand determine the continuum limit $\\mathcal{O}_{0}$ by a linear fit in $a^{2}$.\n\nImplement the following, with all quantities dimensionless (no physical units required):\n\n1) Time-series generation. For each lattice spacing $a$ in a test case, generate a deterministic autoregressive process of order one (AR(1)) with mean given by the Symanzik model and uniformly distributed innovations from a fixed linear congruential generator (LCG). Specifically, for a given $a$, define the target mean\n$$\n\\mu(a)=\\mathcal{O}_{0}^{\\mathrm{true}}+c_{1}^{\\mathrm{true}}\\,a^{2},\n$$\nand let the AR(1) recursion for the series $\\{X_{t}^{(a)}\\}_{t=1}^{N}$ be\n$$\nX_{t+1}^{(a)}=\\mu(a)+\\phi(a)\\left(X_{t}^{(a)}-\\mu(a)\\right)+\\epsilon_{t}^{(a)},\n$$\nwith $\\phi(a)=1-\\frac{1}{L_{\\mathrm{corr}}(a)}$, and innovations $\\epsilon_{t}^{(a)}=\\sigma(a)\\left(2U_{t}^{(a)}-1\\right)$ where $\\{U_{t}^{(a)}\\}$ is a deterministic pseudo-random sequence from the LCG\n$$\ns_{t+1}=(\\alpha s_{t}+\\gamma)\\bmod M,\\quad U_{t}=\\frac{s_{t}}{M},\n$$\nwith $M=2^{32}$, $\\alpha=1664525$, $\\gamma=1013904223$. Use an $a$-specific integer seed $s_{0}$ as provided for each series. For each time series, generate $N_{\\mathrm{burn}}=50+10\\,L_{\\mathrm{corr}}(a)$ burn-in steps that are discarded, then keep the next $N$ points as the dataset for that $a$.\n\n2) Integrated autocorrelation time estimation. For each generated time series, estimate the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ using the standard windowed estimator with self-consistent window selection. Let $\\bar{X}$ be the sample mean and $C(t)$ the unbiased sample autocovariance at lag $t$. Define the normalized autocorrelation $\\rho(t)=C(t)/C(0)$. For a chosen window size $W$, define\n$$\n\\hat{\\tau}_{\\mathrm{int}}(W)=\\frac{1}{2}+\\sum_{t=1}^{W}\\rho(t).\n$$\nChoose $W$ to be the smallest lag such that $W\\geq c\\,\\hat{\\tau}_{\\mathrm{int}}(W)$ with $c=6$, subject to $W\\leq W_{\\max}=\\min\\{N-1,1000\\}$. If no such $W$ exists up to $W_{\\max}$, set $W=W_{\\max}$. Report the estimate $\\hat{\\tau}_{\\mathrm{int}}=\\max\\{\\frac{1}{2},\\hat{\\tau}_{\\mathrm{int}}(W)\\}$.\n\n3) Blocking and blocked bootstrap. For each time series:\n- Choose an integer block size $B_{\\mathrm{blk}}=\\max\\left\\{1,\\left\\lceil 2\\,\\hat{\\tau}_{\\mathrm{int}}\\right\\rceil\\right\\}$.\n- Partition the series into $M=\\left\\lfloor N/B_{\\mathrm{blk}}\\right\\rfloor$ non-overlapping blocks of length $B_{\\mathrm{blk}}$ and compute the $M$ block means. If $M=0$, use a single block equal to the overall sample mean.\n- Implement a block bootstrap with $R$ bootstrap replicates. In each replicate, resample $M$ blocks with replacement and take the average of the resampled block means to obtain one bootstrap estimate of the mean at each lattice spacing. Use the same fixed LCG as above to generate the bootstrap resampling indices, with the replicate and lattice spacing sequences continuing from a specified bootstrap seed.\n\n4) Continuum extrapolation. For each bootstrap replicate, perform an unweighted linear least-squares fit of the bootstrap means $\\{Y^{(a)}\\}$ at the given $a$ values to the model\n$$\nY^{(a)}=\\mathcal{O}_{0}+c_{1}\\,a^{2},\n$$\nand record the bootstrap value of $\\mathcal{O}_{0}$. Use the sample mean of the bootstrap distribution of $\\mathcal{O}_{0}$ as the point estimate and the sample standard deviation (with denominator $R-1$) as the standard error.\n\nNumerical requirements and test suite:\n- You must implement the above workflow for the following three test cases. All numbers below are exact and must be used as given.\n\nTest case A:\n- Lattice spacings: $a\\in\\{0.12,0.09,0.06\\}$.\n- Series length: $N=512$.\n- Correlation lengths: $L_{\\mathrm{corr}}(a)\\in\\{12,24,48\\}$ corresponding respectively to $a=0.12,0.09,0.06$.\n- True parameters: $\\mathcal{O}_{0}^{\\mathrm{true}}=1.2345$, $c_{1}^{\\mathrm{true}}=2.75$.\n- Innovation amplitudes: $\\sigma(a)\\equiv 0.25$ for all $a$.\n- Innovation seeds: $s_{0}\\in\\{123456789,987654321,42424242\\}$ corresponding respectively to $a=0.12,0.09,0.06$.\n- Bootstrap replicates: $R=499$.\n- Bootstrap seed: $s_{0}^{\\mathrm{boot}}=20250601$.\n\nTest case B:\n- Lattice spacings: $a\\in\\{0.10,0.07,0.05,0.04\\}$.\n- Series length: $N=384$.\n- Correlation lengths: $L_{\\mathrm{corr}}(a)\\in\\{8,16,24,32\\}$ corresponding respectively to $a=0.10,0.07,0.05,0.04$.\n- True parameters: $\\mathcal{O}_{0}^{\\mathrm{true}}=0.5$, $c_{1}^{\\mathrm{true}}=-1.2$.\n- Innovation amplitudes: $\\sigma(a)\\equiv 0.20$ for all $a$.\n- Innovation seeds: $s_{0}\\in\\{13579,24680,112233,998877\\}$ corresponding respectively to $a=0.10,0.07,0.05,0.04$.\n- Bootstrap replicates: $R=399$.\n- Bootstrap seed: $s_{0}^{\\mathrm{boot}}=1357911$.\n\nTest case C:\n- Lattice spacings: $a\\in\\{0.16,0.08,0.04\\}$.\n- Series length: $N=256$.\n- Correlation lengths: $L_{\\mathrm{corr}}(a)\\in\\{2,4,6\\}$ corresponding respectively to $a=0.16,0.08,0.04$.\n- True parameters: $\\mathcal{O}_{0}^{\\mathrm{true}}=-0.2$, $c_{1}^{\\mathrm{true}}=0.8$.\n- Innovation amplitudes: $\\sigma(a)\\equiv 0.15$ for all $a$.\n- Innovation seeds: $s_{0}\\in\\{314159,271828,161803\\}$ corresponding respectively to $a=0.16,0.08,0.04$.\n- Bootstrap replicates: $R=299$.\n- Bootstrap seed: $s_{0}^{\\mathrm{boot}}=424242$.\n\nImplementation prescriptions and output specification:\n- For autocovariance estimation, use the unbiased estimator $C(t)=\\frac{1}{N-t}\\sum_{i=1}^{N-t}(X_{i}-\\bar{X})(X_{i+t}-\\bar{X})$ up to $t=W_{\\max}$, and $\\rho(t)=C(t)/C(0)$ when $C(0)>0$. If $C(0)\\leq 0$, set $\\hat{\\tau}_{\\mathrm{int}}=\\frac{1}{2}$.\n- For the self-consistent window selection, use $c=6$ and $W_{\\max}=\\min\\{N-1,1000\\}$.\n- For blocking, use $B_{\\mathrm{blk}}=\\max\\left\\{1,\\left\\lceil 2\\,\\hat{\\tau}_{\\mathrm{int}}\\right\\rceil\\right\\}$ and $M=\\left\\lfloor N/B_{\\mathrm{blk}}\\right\\rfloor$. If $M=0$, use a single block whose mean is the sample mean.\n- For block bootstrap resampling, for each replicate draw $M$ independent indices uniformly from $\\{0,1,\\dots,M-1\\}$ using the specified LCG with state initialized to the given bootstrap seed and advanced sequentially across all draws and lattice spacings within that test case.\n- For the linear fit, perform an ordinary least-squares fit of $Y$ versus $a^{2}$ to estimate $\\mathcal{O}_{0}$ and $c_{1}$ for each bootstrap replicate.\n\nFinal output format:\n- For each test case, report a list consisting of:\n  1) the three or four values of $\\hat{\\tau}_{\\mathrm{int}}$ in the order of the given $a$'s, each rounded to $6$ decimal places,\n  2) the corresponding three or four integer block sizes $B_{\\mathrm{blk}}$,\n  3) the bootstrap mean of $\\mathcal{O}_{0}$ across replicates, rounded to $6$ decimal places,\n  4) the bootstrap standard deviation of $\\mathcal{O}_{0}$ across replicates, rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results for Test case A, Test case B, and Test case C, in that order, as a comma-separated list enclosed in square brackets, where each element is the per-test list described above. For example, the output structure must be of the form\n$$\n[\\,[\\hat{\\tau}_{\\mathrm{int}}(a_{1}),\\dots,\\hat{\\tau}_{\\mathrm{int}}(a_{K}),B_{\\mathrm{blk}}(a_{1}),\\dots,B_{\\mathrm{blk}}(a_{K}),\\widehat{\\mathcal{O}}_{0},\\mathrm{SE}(\\widehat{\\mathcal{O}}_{0})],\\;[\\dots],\\;[\\dots]\\,].\n$$\nAll rounding operations must be applied before printing, and each numerical entry should be a standard decimal numeral.",
            "solution": "The user-provided problem is a comprehensive and well-posed exercise in computational physics, specifically focusing on the statistical analysis of lattice simulation data. The task is to perform a continuum extrapolation of a lattice observable, which involves simulating correlated data, analyzing its autocorrelation properties, and using a blocked bootstrap method to propagate statistical uncertainties into the final continuum-limit estimate. The problem is scientifically grounded in the principles of statistical mechanics, time-series analysis, and Symanzik effective theory for lattice discretizations. All parameters, algorithms, and procedures are specified with sufficient precision to permit a unique, deterministic solution. Therefore, the problem is deemed valid and a full solution will be provided.\n\nThe solution is implemented by following a sequence of prescribed steps:\n\n1.  **Time-Series Generation**: For each given lattice spacing $a$, a synthetic time series of an observable is generated. This series is modeled as a first-order autoregressive (AR(1)) process. The process is constructed to fluctuate around a mean value $\\mu(a)$, which itself depends on the lattice spacing according to the Symanzik effective theory model, $\\mu(a)=\\mathcal{O}_{0}^{\\mathrm{true}}+c_{1}^{\\mathrm{true}}\\,a^{2}$. The AR(1) recursion is given by $X_{t+1}^{(a)}=\\mu(a)+\\phi(a)\\left(X_{t}^{(a)}-\\mu(a)\\right)+\\epsilon_{t}^{(a)}$. The parameter $\\phi(a)=1-1/L_{\\mathrm{corr}}(a)$ controls the autocorrelation time of the series, with $L_{\\mathrm{corr}}(a)$ being the given correlation length. The innovations $\\epsilon_{t}^{(a)}$ are random noise terms generated from a deterministic Linear Congruential Generator (LCG) with specified parameters ($M=2^{32}$, $\\alpha=1664525$, $\\gamma=1013904223$) and seeds. An initial portion of each series, the burn-in period $N_{\\mathrm{burn}}=50+10\\,L_{\\mathrm{corr}}(a)$, is discarded to ensure the generated data represents the stationary state of the process.\n\n2.  **Integrated Autocorrelation Time Estimation**: The key to correct error analysis for correlated data is the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$. For each generated time series, we estimate $\\tau_{\\mathrm{int}}$ using a windowed estimator. First, the sample autocovariance function $C(t)$ is computed for lags $t$ from $0$ up to a maximum $W_{\\max}$. Following the problem's prescription, we use the formula $C(t)=\\frac{1}{N-t}\\sum_{i=1}^{N-t}(X_{i}-\\bar{X})(X_{i+t}-\\bar{X})$, where $\\bar{X}$ is the mean of the entire time series of length $N$. The normalized autocorrelation function is then $\\rho(t)=C(t)/C(0)$. The estimated integrated autocorrelation time for a given window size $W$ is $\\hat{\\tau}_{\\mathrm{int}}(W)=\\frac{1}{2}+\\sum_{t=1}^{W}\\rho(t)$. The window size $W$ is chosen via a self-consistency procedure: it is the smallest integer lag $W$ that satisfies the condition $W\\geq c\\,\\hat{\\tau}_{\\mathrm{int}}(W)$, with the specified constant $c=6$. This procedure truncates the sum when the noise in $\\rho(t)$ begins to dominate the signal. If this condition is not met for any $W \\leq W_{\\max}$, then $W=W_{\\max}$ is used. The final reported estimate is $\\hat{\\tau}_{\\mathrm{int}}=\\max\\{\\frac{1}{2},\\hat{\\tau}_{\\mathrm{int}}(W)\\}$.\n\n3.  **Blocking and Blocked Bootstrap**: To handle the autocorrelations and propagate statistical errors, a blocked bootstrap method is employed. First, each time series is partitioned into non-overlapping blocks. The size of these blocks, $B_{\\mathrm{blk}}=\\max\\left\\{1,\\left\\lceil 2\\,\\hat{\\tau}_{\\mathrm{int}}\\right\\rceil\\right\\}$, is chosen to be larger than the autocorrelation time, such that the means of the blocks are approximately statistically independent. The series is divided into $M=\\left\\lfloor N/B_{\\mathrm{blk}}\\right\\rfloor$ blocks, and the mean of each block is computed. This results in a new, much shorter time series of $M$ block means for each lattice spacing $a$.\n    A bootstrap analysis is then performed. For each of $R$ bootstrap replicates, a new set of data is created. For each lattice spacing $a$, $M$ block means are resampled with replacement from the original set of $M$ block means. The average of these resampled block means gives one bootstrap estimate, $Y^{(a)}$, for the observable at that lattice spacing. This process is repeated for all lattice spacings, using a single LCG instance that advances sequentially, to generate a full set of $\\{Y^{(a)}\\}$ for one bootstrap replicate.\n\n4.  **Continuum Extrapolation**: For each of the $R$ bootstrap replicates, the continuum limit $\\mathcal{O}_{0}$ is estimated. This is done by performing an unweighted linear least-squares fit to the model $Y^{(a)}=\\mathcal{O}_{0}+c_{1}\\,a^{2}$, using the bootstrap means $Y^{(a)}$ versus the corresponding values of $a^2$. The intercept of this fit provides one bootstrap estimate of $\\mathcal{O}_{0}$.\n\n5.  **Final Estimates**: After generating $R$ bootstrap estimates for $\\mathcal{O}_{0}$, a bootstrap distribution is obtained. The final point estimate for the continuum value, $\\widehat{\\mathcal{O}}_{0}$, is the sample mean of this distribution. The corresponding statistical uncertainty, $\\mathrm{SE}(\\widehat{\\mathcal{O}}_{0})$, is given by the sample standard deviation of the distribution, using a denominator of $R-1$.\n\nThe entire procedure is deterministic due to the use of specified seeds for the LCG. The final results for each test case are collected, rounded to six decimal places, and formatted into the specified nested list structure.",
            "answer": "```python\nimport numpy as np\nimport math\n\nclass LCG:\n    \"\"\"A Linear Congruential Generator as specified in the problem.\"\"\"\n    def __init__(self, seed, M=2**32, alpha=1664525, gamma=1013904223):\n        self.state = seed\n        self.M = M\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def next_int(self):\n        \"\"\"Generates the next integer in the sequence.\"\"\"\n        self.state = (self.alpha * self.state + self.gamma) % self.M\n        return self.state\n\n    def next_float(self):\n        \"\"\"Generates the next float in [0, 1) in the sequence.\"\"\"\n        return self.next_int() / self.M\n\ndef generate_ar1_series(a, N, L_corr, O0_true, c1_true, sigma, seed):\n    \"\"\"Generates a single AR(1) time series.\"\"\"\n    rng = LCG(seed)\n    mu_a = O0_true + c1_true * a**2\n    phi_a = 1.0 - 1.0 / L_corr\n    N_burn = 50 + 10 * L_corr\n\n    # Initial value at the mean\n    x_t = mu_a\n\n    # Burn-in phase\n    for _ in range(N_burn):\n        u_t = rng.next_float()\n        epsilon_t = sigma * (2 * u_t - 1)\n        x_t = mu_a + phi_a * (x_t - mu_a) + epsilon_t\n\n    # Data generation phase\n    series = np.zeros(N)\n    for t in range(N):\n        u_t = rng.next_float()\n        epsilon_t = sigma * (2 * u_t - 1)\n        x_t = mu_a + phi_a * (x_t - mu_a) + epsilon_t\n        series[t] = x_t\n        \n    return series\n\ndef estimate_tau_int(series, c, W_max):\n    \"\"\"Estimates the integrated autocorrelation time with a self-consistent window.\"\"\"\n    N = len(series)\n    if N  2:\n        return 0.5\n        \n    mean = np.mean(series)\n    series_centered = series - mean\n\n    # Calculate C(0)\n    C0 = np.dot(series_centered, series_centered) / N\n    if C0 = 0:\n        return 0.5\n\n    tau_int_sum = 0.0\n    final_W = W_max\n    \n    for W in range(1, W_max + 1):\n        if W >= N:\n            final_W = W - 1\n            break\n        \n        # Calculate C(W)\n        CW = np.dot(series_centered[:-W], series_centered[W:]) / (N - W)\n        rho_W = CW / C0\n        tau_int_sum += rho_W\n        \n        tau_int_W = 0.5 + tau_int_sum\n        \n        if W >= c * tau_int_W:\n            final_W = W\n            break\n    \n    # Recalculate tau_int up to final_W if the loop finished\n    if W == W_max:\n        tau_int_W_final = 0.5 + tau_int_sum\n    else: # if loop broke early\n        tau_int_W_final = tau_int_W\n\n    return max(0.5, tau_int_W_final)\n\ndef solve_case(case_params):\n    \"\"\"\n    Processes a single test case from data generation to final result.\n    \"\"\"\n    a_s = np.array(case_params['a'])\n    N = case_params['N']\n    L_corrs = case_params['L_corr']\n    O0_true = case_params['O0_true']\n    c1_true = case_params['c1_true']\n    sigmas = case_params['sigma']\n    seeds = case_params['seeds']\n    R = case_params['R']\n    boot_seed = case_params['boot_seed']\n    \n    num_a = len(a_s)\n    \n    all_series = []\n    # Part 1: Generate time series for all lattice spacings\n    for i in range(num_a):\n        series = generate_ar1_series(a_s[i], N, L_corrs[i], O0_true, c1_true, sigmas[i], seeds[i])\n        all_series.append(series)\n\n    tau_ints = []\n    block_sizes = []\n    blocked_data = []\n\n    # Part 2  3: Estimate tau_int and create block means\n    for i in range(num_a):\n        series = all_series[i]\n        W_max = min(N - 1, 1000)\n        \n        tau = estimate_tau_int(series, c=6, W_max=W_max)\n        tau_ints.append(tau)\n        \n        B_blk = max(1, math.ceil(2 * tau))\n        block_sizes.append(B_blk)\n        \n        M = N // B_blk\n        if M == 0:\n            block_means = np.array([np.mean(series)])\n        else:\n            num_pts_to_use = M * B_blk\n            block_means = np.mean(series[:num_pts_to_use].reshape(M, B_blk), axis=1)\n        blocked_data.append(block_means)\n\n    # Part 3  4: Blocked bootstrap and continuum extrapolation\n    boot_rng = LCG(boot_seed)\n    O0_bootstrap_dist = []\n    a_squared = a_s**2\n\n    for _ in range(R):\n        boot_means_Y = []\n        for i in range(num_a):\n            blocks = blocked_data[i]\n            M_i = len(blocks)\n            indices = [int(boot_rng.next_float() * M_i) for _ in range(M_i)]\n            resampled_blocks = blocks[indices]\n            boot_mean = np.mean(resampled_blocks)\n            boot_means_Y.append(boot_mean)\n        \n        # OLS fit to Y = O0 + c1 * a^2\n        Y = np.array(boot_means_Y)\n        X = a_squared\n        \n        X_bar = np.mean(X)\n        Y_bar = np.mean(Y)\n        \n        # Handle case of vertical line (all a are same, not in this problem)\n        if np.sum((X - X_bar)**2) == 0:\n             c1_hat = 0\n        else:\n            c1_hat = np.sum((X - X_bar) * (Y - Y_bar)) / np.sum((X - X_bar)**2)\n        \n        O0_hat = Y_bar - c1_hat * X_bar\n        O0_bootstrap_dist.append(O0_hat)\n    \n    O0_bootstrap_dist = np.array(O0_bootstrap_dist)\n    final_O0_mean = np.mean(O0_bootstrap_dist)\n    final_O0_std_err = np.std(O0_bootstrap_dist, ddof=1)\n\n    # Assemble results\n    results = []\n    for tau in tau_ints:\n        results.append(f\"{tau:.6f}\")\n    for bs in block_sizes:\n        results.append(str(bs))\n    results.append(f\"{final_O0_mean:.6f}\")\n    results.append(f\"{final_O0_std_err:.6f}\")\n    \n    return results\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases_params = [\n        {\n            'a': [0.12, 0.09, 0.06], 'N': 512, 'L_corr': [12, 24, 48],\n            'O0_true': 1.2345, 'c1_true': 2.75, 'sigma': [0.25, 0.25, 0.25],\n            'seeds': [123456789, 987654321, 42424242], 'R': 499, 'boot_seed': 20250601\n        },\n        {\n            'a': [0.10, 0.07, 0.05, 0.04], 'N': 384, 'L_corr': [8, 16, 24, 32],\n            'O0_true': 0.5, 'c1_true': -1.2, 'sigma': [0.20, 0.20, 0.20, 0.20],\n            'seeds': [13579, 24680, 112233, 998877], 'R': 399, 'boot_seed': 1357911\n        },\n        {\n            'a': [0.16, 0.08, 0.04], 'N': 256, 'L_corr': [2, 4, 6],\n            'O0_true': -0.2, 'c1_true': 0.8, 'sigma': [0.15, 0.15, 0.15],\n            'seeds': [314159, 271828, 161803], 'R': 299, 'boot_seed': 424242\n        }\n    ]\n    \n    all_results = []\n    for params in test_cases_params:\n        case_result = solve_case(params)\n        all_results.append(f\"[{','.join(case_result)}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "With a handle on statistical errors, we next confront systematic uncertainties, chief among them being the choice of a valid fitting model. Continuum extrapolations rely on the assumption that the data lies in the asymptotic regime where a simple power-law ansatz, like $O(a) = \\beta_0 + \\beta_1 a^{p}$, holds. This exercise  addresses the common scenario where data from coarser lattices may violate this assumption, and implements a data-driven model selection technique—leave-one-out cross-validation—to decide whether to exclude suspect data points. This practice develops crucial skills in validating the domain of applicability for your theoretical models.",
            "id": "3509803",
            "problem": "Consider a sequence of lattice measurements of a dimensionless observable, modeled according to the Symanzik effective theory in the asymptotic regime. For a given action with known leading discretization exponent $p$, the observable $O(a)$ at lattice spacing $a$ is assumed to follow an expansion whose leading truncation retains only the first nontrivial power,\n$$\nO(a) = \\beta_0 + \\beta_1 \\, a^{p} + \\varepsilon,\n$$\nwhere $\\beta_0$ is the continuum limit, $\\beta_1$ encodes the leading discretization effect, and $\\varepsilon$ is a random error with independent Gaussian components of known standard deviations. In practice, the coarsest lattice spacing $a_{\\max}$ may violate the asymptotic regime, resulting in a data point that is not well described by the retained truncation.\n\nYour task is to implement a weighted linear regression and a leave-one-out cross-validation procedure to decide whether to exclude the coarsest $a$ data point from the fit, and then to report the continuum limit estimate and its statistical uncertainty from the selected fit. Use only the fundamental assumptions that independent Gaussian errors lead to a weighted least squares estimator as the maximum likelihood solution, and that cross-validation compares predictive performance on left-out data. Do not introduce any additional priors or hyperparameters.\n\nDefinitions to use:\n- For each dataset, define $x_i = a_i^{p}$ and $y_i = O_i$ with known standard deviations $\\sigma_i$.\n- Perform a weighted linear fit of $y$ versus $x$ with weights $w_i = 1/\\sigma_i^2$.\n- Define the leave-one-out predictive loss for a dataset $D = \\{(x_i,y_i,\\sigma_i)\\}_{i=1}^N$ as\n$$\n\\mathcal{L}_{\\mathrm{LOO}}(D) = \\sum_{j=1}^{N} \\left( \\frac{y_j - \\widehat{y}_j^{(-j)}}{\\sigma_j} \\right)^2,\n$$\nwhere $\\widehat{y}_j^{(-j)}$ is the predicted value at $x_j$ obtained by refitting the weighted linear model on $D \\setminus \\{(x_j,y_j,\\sigma_j)\\}$.\n- For each test case, compute $\\mathcal{L}_{\\mathrm{LOO}}$ on:\n  1. The full dataset $D_{\\mathrm{full}}$.\n  2. The dataset $D_{\\mathrm{excl}}$ obtained by removing the single point with the largest $a$ (i.e., $a_{\\max}$).\n- Decision rule: exclude the coarsest point if and only if $\\mathcal{L}_{\\mathrm{LOO}}(D_{\\mathrm{excl}})$ is strictly smaller than $\\mathcal{L}_{\\mathrm{LOO}}(D_{\\mathrm{full}})$. In the event of equality, do not exclude.\n\nAfter applying the decision rule, fit the chosen dataset and report:\n- The boolean decision whether the coarsest point was excluded.\n- The continuum estimate $\\widehat{\\beta}_0$ from the chosen fit.\n- The statistical $1$-sigma uncertainty of $\\widehat{\\beta}_0$, defined as the square root of the $(0,0)$ component of the covariance matrix of the weighted least squares estimator.\n- The two leave-one-out scores $\\mathcal{L}_{\\mathrm{LOO}}(D_{\\mathrm{full}})$ and $\\mathcal{L}_{\\mathrm{LOO}}(D_{\\mathrm{excl}})$ that were compared.\n\nAll quantities are dimensionless; no physical units are involved.\n\nInput data for the test suite:\n- Test case A (leading exponent $p = 2$):\n  - $a$ values: [$0.12$, $0.09$, $0.06$, $0.045$, $0.03$]\n  - $O(a)$ values: [$0.5230$, $0.50995$, $0.50410$, $0.50230$, $0.50120$]\n  - $\\sigma$ values: [$0.0010$, $0.0008$, $0.0007$, $0.0007$, $0.0007$]\n- Test case B (leading exponent $p = 2$):\n  - $a$ values: [$0.12$, $0.09$, $0.06$, $0.045$, $0.03$]\n  - $O(a)$ values: [$0.51336$, $0.50720$, $0.50330$, $0.50190$, $0.50078$]\n  - $\\sigma$ values: [$0.0010$, $0.0009$, $0.0008$, $0.0008$, $0.0008$]\n- Test case C (leading exponent $p = 1$):\n  - $a$ values: [$0.16$, $0.10$, $0.06$, $0.04$]\n  - $O(a)$ values: [$1.1105$, $1.1495$, $1.1702$, $1.1803$]\n  - $\\sigma$ values: [$0.0040$, $0.0030$, $0.0030$, $0.0030$]\n\nRequirements for your program:\n- For each test case, compute the two leave-one-out scores, decide whether to exclude the coarsest point, then fit the chosen dataset and extract $\\widehat{\\beta}_0$ and its statistical uncertainty.\n- Your program must produce a single line of output containing the results as a comma-separated list of three lists (one per test case), each inner list ordered as\n  [$\\text{exclude}$, $\\widehat{\\beta}_0$, $\\mathrm{uncertainty}(\\widehat{\\beta}_0)$, $\\mathcal{L}_{\\mathrm{LOO}}(D_{\\mathrm{full}})$, $\\mathcal{L}_{\\mathrm{LOO}}(D_{\\mathrm{excl}})$].\n- The final printed line must therefore have the form\n  [[exclude_A, beta0_A, s_A, LOO_full_A, LOO_excl_A],[exclude_B, beta0_B, s_B, LOO_full_B, LOO_excl_B],[exclude_C, beta0_C, s_C, LOO_full_C, LOO_excl_C]].\n- Each boolean or numerical entry is dimensionless. Angles are not used.\n\nDesign coverage:\n- Test case A is a typical situation where the coarsest point likely violates asymptotic scaling and should be excluded.\n- Test case B probes a borderline scenario where the coarsest point has only a small deviation and may be retained.\n- Test case C changes the leading exponent to $p = 1$ and tests the method with fewer points and a different slope.\n\nYour solution must be a complete, runnable program that carries out these computations and prints exactly one line in the format specified above, with no additional text.",
            "solution": "The problem requires the implementation of a model selection procedure for a continuum extrapolation fit, a standard practice in lattice field theory. The underlying physical model is derived from Symanzik effective theory, which posits that for small lattice spacings $a$, a dimensionless observable $O(a)$ can be expanded in powers of $a$. The problem specifies a truncated model,\n$$\nO(a) = \\beta_0 + \\beta_1 a^p + \\varepsilon\n$$\nwhere $\\beta_0$ is the desired continuum limit ($a \\to 0$), $\\beta_1$ is a coefficient for the leading discretization error, $p$ is a known integer exponent, and $\\varepsilon$ represents independent Gaussian measurement errors with known standard deviations $\\sigma_i$ for each data point $(a_i, O_i)$.\n\nThe primary task is to decide whether to include the data point with the coarsest lattice spacing, $a_{\\max}$, which may not lie in the asymptotic region where the truncated model is valid. This decision is based on leave-one-out cross-validation (LOOCV), a technique to assess a model's predictive accuracy.\n\nThe analysis hinges on weighted linear regression. We define the independent variable as $x_i = a_i^p$ and the dependent variable as $y_i = O_i$. The model becomes a simple linear relation $y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i$. Since the errors $\\varepsilon_i$ are independent and Gaussian with variances $\\sigma_i^2$, the maximum likelihood estimator for the parameters $\\beta = [\\beta_0, \\beta_1]^T$ is the weighted least squares (WLS) estimator.\n\nThe WLS estimator $\\widehat{\\beta}$ minimizes the weighted sum of squared residuals, or chi-squared, $\\chi^2$:\n$$\n\\chi^2(\\beta) = \\sum_{i=1}^{N} w_i (y_i - (\\beta_0 + \\beta_1 x_i))^2\n$$\nwhere the weights are $w_i = 1/\\sigma_i^2$. In matrix form, this is $\\chi^2(\\beta) = (y - X\\beta)^T W (y - X\\beta)$, where $y$ is the vector of observations, $X$ is the design matrix, and $W$ is the diagonal matrix of weights. The design matrix for this problem, with $N$ data points, is:\n$$\nX = \\begin{pmatrix} 1  x_1 \\\\ 1  x_2 \\\\ \\vdots  \\vdots \\\\ 1  x_N \\end{pmatrix}\n$$\nThe solution that minimizes $\\chi^2$ is given by the normal equations, leading to the estimator:\n$$\n\\widehat{\\beta} = (X^T W X)^{-1} X^T W y\n$$\nThe first component of $\\widehat{\\beta}$, which is $\\widehat{\\beta}_0$, is our estimate for the continuum value. The statistical uncertainty of the estimated parameters is given by the covariance matrix, $C$. For a WLS fit, this is:\n$$\nC = (X^T W X)^{-1}\n$$\nThe uncertainty on the continuum estimate $\\widehat{\\beta}_0$ is the square root of the corresponding diagonal element of the covariance matrix: $\\Delta\\widehat{\\beta}_0 = \\sqrt{C_{00}}$.\n\nTo decide whether to exclude the coarsest point, we use LOOCV. The problem defines the total predictive loss as:\n$$\n\\mathcal{L}_{\\mathrm{LOO}}(D) = \\sum_{j=1}^{N} \\left( \\frac{y_j - \\widehat{y}_j^{(-j)}}{\\sigma_j} \\right)^2\n$$\nHere, $\\widehat{y}_j^{(-j)}$ is the predicted value at $x_j$ from a WLS fit performed on the dataset $D$ with the $j$-th point removed. Calculating this by performing $N$ separate fits is computationally inefficient. A more direct method uses the \"hat\" matrix, $H$. The vector of predicted values from a fit on the full dataset is $\\widehat{y} = X\\widehat{\\beta} = X(X^T W X)^{-1} X^T W y = H y$. The hat matrix is thus $H = X(X^T W X)^{-1} X^T W$.\n\nA standard result in regression diagnostics states that the leave-one-out residual can be calculated from the full-dataset fit residual, $r_j = y_j - \\widehat{y}_j$, and the leverage of the $j$-th point, $h_{jj} = H_{jj}$:\n$$\ny_j - \\widehat{y}_j^{(-j)} = \\frac{y_j - \\widehat{y}_j}{1 - h_{jj}} = \\frac{r_j}{1 - h_{jj}}\n$$\nThis allows for the efficient computation of the LOOCV loss from a single WLS fit on a given dataset:\n$$\n\\mathcal{L}_{\\mathrm{LOO}}(D) = \\sum_{j=1}^{N} \\left( \\frac{r_j}{\\sigma_j (1 - h_{jj})} \\right)^2\n$$\nThe procedure for each test case is as follows:\n1. Define two datasets: $D_{\\mathrm{full}}$, containing all points, and $D_{\\mathrm{excl}}$, with the point corresponding to $a_{\\max}$ removed.\n2. For each dataset, perform a WLS fit to find the parameters $\\widehat{\\beta}$ and compute the LOOCV loss, $\\mathcal{L}_{\\mathrm{LOO}}$, using the formula above. This yields $\\mathcal{L}_{\\mathrm{LOO}}(D_{\\mathrm{full}})$ and $\\mathcal{L}_{\\mathrm{LOO}}(D_{\\mathrm{excl}})$.\n3. Compare the losses. If $\\mathcal{L}_{\\mathrm{LOO}}(D_{\\mathrm{excl}})  \\mathcal{L}_{\\mathrm{LOO}}(D_{\\mathrm{full}})$, the coarsest point is excluded. Otherwise, it is retained.\n4. The final continuum estimate $\\widehat{\\beta}_0$ and its uncertainty $\\sqrt{C_{00}}$ are taken from the fit corresponding to the chosen dataset (either $D_{\\mathrm{full}}$ or $D_{\\mathrm{excl}}$).\n5. All five quantities—the boolean decision, the final $\\widehat{\\beta}_0$, its uncertainty, and the two computed LOOCV losses—are reported for each case.\n\nThis procedure constitutes a complete, self-contained, and objective method for model selection in this context, adhering strictly to the principles of maximum likelihood estimation and cross-validation without introducing external information or subjective biases.",
            "answer": "```python\n# language: Python\n# version: 3.12\n# libraries:\n#   - name: numpy\n#     version: 1.23.5\n#   - name: scipy\n#     version: 1.11.4\nimport numpy as np\n\ndef calculate_fit_and_loo(a, O, sigma, p):\n    \"\"\"\n    Performs a weighted linear fit and calculates the leave-one-out cross-validation loss.\n\n    Args:\n        a (np.ndarray): Array of lattice spacings.\n        O (np.ndarray): Array of observable values.\n        sigma (np.ndarray): Array of standard deviations for O.\n        p (int): The leading discretization exponent.\n\n    Returns:\n        tuple: A tuple containing:\n            - beta0_hat (float): The continuum extrapolated value (intercept).\n            - unc_beta0 (float): The uncertainty on beta0_hat.\n            - loo_loss (float): The leave-one-out cross-validation loss.\n    \"\"\"\n    x = a**p\n    y = O\n    weights = 1 / sigma**2\n    \n    # Design matrix X\n    X = np.vstack([np.ones_like(x), x]).T\n    \n    # Weight matrix W\n    W = np.diag(weights)\n    \n    # Perform WLS fit\n    # Covariance matrix C = (X^T * W * X)^-1\n    try:\n        XTWX = X.T @ W @ X\n        C = np.linalg.inv(XTWX)\n    except np.linalg.LinAlgError:\n        # This case occurs if the matrix is singular, e.g., if there are fewer\n        # points than parameters, which is 2. The problem ensures this doesn't happen\n        # for D_excl if N_full = 4.\n        return np.nan, np.nan, np.inf\n\n    # Parameter estimates beta_hat = C * X^T * W * y\n    XTWy = X.T @ W @ y\n    beta_hat = C @ XTWy\n    \n    beta0_hat = beta_hat[0]\n    unc_beta0 = np.sqrt(C[0, 0])\n    \n    # Calculate LOOCV loss using the leverage/hat matrix method\n    # Hat matrix H = X * C * X^T * W\n    H = X @ C @ X.T @ W\n    leverages = np.diag(H)\n    \n    # Predictions from the full fit: y_hat = X * beta_hat\n    y_hat = X @ beta_hat\n    \n    # Standard residuals: r = y - y_hat\n    residuals = y - y_hat\n    \n    # LOOCV residuals, scaled by sigma\n    # The term (y_j - yhat_j-1) / sigma_j\n    loo_scaled_residuals = (residuals / sigma) / (1 - leverages)\n    \n    # LOOCV loss: sum of squares of the scaled LOOCV residuals\n    loo_loss = np.sum(loo_scaled_residuals**2)\n    \n    return beta0_hat, unc_beta0, loo_loss\n\ndef solve():\n    \"\"\"\n    Main function to process test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        # Test case A\n        {'p': 2, 'a': [0.12, 0.09, 0.06, 0.045, 0.03], 'O': [0.5230, 0.50995, 0.50410, 0.50230, 0.50120], 'sigma': [0.0010, 0.0008, 0.0007, 0.0007, 0.0007]},\n        # Test case B\n        {'p': 2, 'a': [0.12, 0.09, 0.06, 0.045, 0.03], 'O': [0.51336, 0.50720, 0.50330, 0.50190, 0.50078], 'sigma': [0.0010, 0.0009, 0.0008, 0.0008, 0.0008]},\n        # Test case C\n        {'p': 1, 'a': [0.16, 0.10, 0.06, 0.04], 'O': [1.1105, 1.1495, 1.1702, 1.1803], 'sigma': [0.0040, 0.0030, 0.0030, 0.0030]}\n    ]\n\n    results_all_cases = []\n    \n    for case in test_cases:\n        p = case['p']\n        a_full = np.array(case['a'])\n        O_full = np.array(case['O'])\n        sigma_full = np.array(case['sigma'])\n        \n        # Identify the index of the coarsest point (largest 'a')\n        idx_coarsest = np.argmax(a_full)\n        \n        # --- Analysis on the full dataset D_full ---\n        beta0_full, unc_full, loo_full = calculate_fit_and_loo(a_full, O_full, sigma_full, p)\n        \n        # --- Analysis on the dataset D_excl with the coarsest point removed ---\n        a_excl = np.delete(a_full, idx_coarsest)\n        O_excl = np.delete(O_full, idx_coarsest)\n        sigma_excl = np.delete(sigma_full, idx_coarsest)\n        \n        beta0_excl, unc_excl, loo_excl = calculate_fit_and_loo(a_excl, O_excl, sigma_excl, p)\n        \n        # --- Decision Rule ---\n        # Exclude if the loss is strictly smaller\n        exclude = loo_excl  loo_full\n        \n        if exclude:\n            final_beta0 = beta0_excl\n            final_unc = unc_excl\n        else:\n            final_beta0 = beta0_full\n            final_unc = unc_full\n            \n        case_result = [exclude, final_beta0, final_unc, loo_full, loo_excl]\n        results_all_cases.append(case_result)\n        \n    # Format the final output string as per the problem specification\n    # e.g., [[True,0.5,0.001,10.0,5.0],[...]]\n    inner_lists_str = []\n    for res_list in results_all_cases:\n        # str(bool) gives 'True'/'False', str(float) gives standard float representation.\n        str_elements = [str(x) for x in res_list]\n        inner_lists_str.append(f\"[{','.join(str_elements)}]\")\n    \n    final_output_string = f\"[{','.join(inner_lists_str)}]\"\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "A truly robust analysis requires accounting for uncertainty in the extrapolation model itself, as multiple functional forms may be theoretically plausible. Instead of selecting a single \"best\" model, Bayesian Model Averaging (BMA) offers a principled framework for combining results from several competing ansätze, weighted by the evidence for each model in the data. This advanced practice  walks you through a full BMA implementation to derive a final, model-averaged continuum value and its total credible interval. This method represents a state-of-the-art approach to quantifying both statistical and systematic uncertainties in a unified, probabilistic way.",
            "id": "3509855",
            "problem": "Implement a complete program that performs Bayesian model averaging (BMA) for continuum extrapolation of lattice data using three competing ansätze and returns the model-averaged continuum value and its equal-tailed credible interval. All quantities in this problem are dimensionless. The three models are defined by basis functions of the lattice spacing $a$ as follows:\n- $M_1$: constant plus linear, with $f(a) = f_0 + c_1\\,a$,\n- $M_2$: constant plus quadratic, with $f(a) = f_0 + c_2\\,a^2$,\n- $M_3$: constant plus quadratic times logarithm, with $f(a) = f_0 + c_3\\,a^2 \\log a$, where the logarithm is the natural logarithm and $a \\in (0,1)$ so that $\\log a$ is well-defined and negative.\n\nYou must treat the problem in a fully Bayesian linear model with Gaussian observational noise and independent Gaussian priors on the coefficients. Specifically:\n- Likelihood: given data $(a_i, y_i, \\sigma_i)$ for $i=1,\\dots,n$, model $M$ predicts $y_i$ through a linear design matrix in the coefficients, and the data likelihood is a product of Gaussians with known standard deviations $\\sigma_i$.\n- Parameter prior for each model $M_j$: the coefficient vector $\\theta = (f_0, c_j)$ is assigned a zero-mean Gaussian prior with diagonal covariance $\\mathrm{diag}(\\tau_0^2, \\tau_c^2)$, where $\\tau_0 = 10$ and $\\tau_c = 1$.\n- Model prior: models $M_1$, $M_2$, and $M_3$ have equal prior probability.\n\nYour program must:\n1. For each model $M_j$, compute the model evidence by integrating the parameters under the Gaussian prior and linear-Gaussian likelihood.\n2. For each model $M_j$, compute the posterior distribution of the continuum parameter $f_0$ (i.e., the coefficient of the constant basis function) given the data under that model.\n3. Compute the posterior probability of each model.\n4. Construct the model-averaged posterior for $f_0$ as a discrete mixture of the three model-specific Gaussian posteriors, with weights equal to the posterior model probabilities.\n5. Compute the model-averaged posterior mean of $f_0$ and its central equal-tailed credible interval with credibility level $0.68$ (i.e., lower quantile at $0.16$ and upper quantile at $0.84$) from the mixture distribution.\n\nFundamental base you must use:\n- Bayes’ theorem for model selection and parameter inference,\n- Properties of multivariate normal distributions for linear-Gaussian models with Gaussian priors,\n- Definitions of central equal-tailed credible intervals.\n\nConstraints and requirements:\n- Treat all inputs as dimensionless numbers. No angles appear in this problem; no unit conversion is needed.\n- Implement numerically stable linear algebra. You may assume the observational error covariance is diagonal with entries $\\sigma_i^2$.\n- Use the natural logarithm for $\\log a$.\n- The credible interval level is to be interpreted as a probability (e.g., $0.68$), not a percentage. Report the interval endpoints, not the half-width.\n\nTest suite:\nYour program must run on the following five datasets. Each dataset provides arrays of lattice spacings $a$, observed values $y$, and observational standard deviations $\\sigma$. Use exactly these values in the stated order.\n\n- Case 1 (typical quadratic cutoff effects):\n  - $a = [\\,0.12,\\, 0.09,\\, 0.06,\\, 0.045\\,]$\n  - $y = [\\,0.52028,\\, 0.50772,\\, 0.50582,\\, 0.50143\\,]$\n  - $\\sigma = [\\,0.005,\\, 0.004,\\, 0.003,\\, 0.003\\,]$\n\n- Case 2 (dominantly linear cutoff effects):\n  - $a = [\\,0.16,\\, 0.12,\\, 0.08,\\, 0.04\\,]$\n  - $y = [\\,0.877,\\, 0.902,\\, 0.939,\\, 0.964\\,]$\n  - $\\sigma = [\\,0.01,\\, 0.01,\\, 0.01,\\, 0.01\\,]$\n\n- Case 3 (log-enhanced quadratic corrections):\n  - $a = [\\,0.10,\\, 0.08,\\, 0.06,\\, 0.05,\\, 0.04\\,]$\n  - $y = [\\,0.189987074535,\\, 0.1909176683392,\\, 0.1954358607098302,\\, 0.1957553346575,\\, 0.19842489934\\,]$\n  - $\\sigma = [\\,0.004,\\, 0.004,\\, 0.004,\\, 0.004,\\, 0.004\\,]$\n\n- Case 4 (sparse data):\n  - $a = [\\,0.10,\\, 0.05\\,]$\n  - $y = [\\,0.311,\\, 0.29875\\,]$\n  - $\\sigma = [\\,0.02,\\, 0.02\\,]$\n\n- Case 5 (high-noise regime):\n  - $a = [\\,0.15,\\, 0.12,\\, 0.09,\\, 0.06,\\, 0.03\\,]$\n  - $y = [\\,0.845,\\, 0.78,\\, 0.805,\\, 0.765,\\, 0.77\\,]$\n  - $\\sigma = [\\,0.05,\\, 0.05,\\, 0.05,\\, 0.05,\\, 0.05\\,]$\n\nOutput specification:\n- For each case, compute the model-averaged posterior mean of $f_0$ and the endpoints of the central equal-tailed credible interval at credibility level $0.68$.\n- Round each reported number to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case contributes a sub-list in the form $[\\,\\text{mean},\\, \\text{lower},\\, \\text{upper}\\,]$. For example, the overall format must be like $[[m_1,\\ell_1,u_1],[m_2,\\ell_2,u_2],\\dots,[m_5,\\ell_5,u_5]]$ with exactly five triples in order corresponding to the cases above.",
            "solution": "The user requires the implementation of a Bayesian Model Averaging (BMA) procedure for continuum extrapolation of lattice data. The problem is well-posed, scientifically grounded, and contains all necessary information for its resolution. It falls directly within the domain of computational physics and Bayesian statistics.\n\nWe will proceed by first deriving the necessary mathematical formulas for the Bayesian linear model and then implementing them to solve the problem for the provided test cases.\n\n### Bayesian Linear Regression Formalism\n\nLet a given model $M_j$ be described by the linear relation $\\mathbf{y} = X_j \\boldsymbol{\\theta}_j + \\boldsymbol{\\epsilon}$, where:\n- $\\mathbf{y}$ is the $n \\times 1$ vector of observed values.\n- $X_j$ is the $n \\times k$ design matrix, with $k=2$ parameters for all models.\n- $\\boldsymbol{\\theta}_j$ is the $k \\times 1$ vector of model parameters, $(f_0, c_j)^T$.\n- $\\boldsymbol{\\epsilon}$ is the vector of observation errors, assumed to be drawn from a Gaussian distribution with mean $\\mathbf{0}$ and known diagonal covariance matrix $\\Sigma = \\mathrm{diag}(\\sigma_1^2, \\dots, \\sigma_n^2)$.\n\nThe likelihood of the data given the parameters is:\n$$p(\\mathbf{y} | \\boldsymbol{\\theta}_j, M_j) = \\mathcal{N}(\\mathbf{y} | X_j \\boldsymbol{\\theta}_j, \\Sigma) \\propto \\exp\\left(-\\frac{1}{2}(\\mathbf{y} - X_j \\boldsymbol{\\theta}_j)^T \\Sigma^{-1} (\\mathbf{y} - X_j \\boldsymbol{\\theta}_j)\\right)$$\n\nThe problem specifies a Gaussian prior on the parameters $\\boldsymbol{\\theta}_j$:\n$$p(\\boldsymbol{\\theta}_j | M_j) = \\mathcal{N}(\\boldsymbol{\\theta}_j | \\boldsymbol{\\mu}_0, \\Sigma_0)$$\nwhere the prior mean is $\\boldsymbol{\\mu}_0 = \\mathbf{0}$ and the prior covariance is $\\Sigma_0 = \\mathrm{diag}(\\tau_0^2, \\tau_c^2)$ with $\\tau_0 = 10$ and $\\tau_c = 1$.\n\n#### 1. Parameter Posterior Distribution\nDue to the conjugacy of the Gaussian prior with the linear-Gaussian likelihood, the posterior distribution of the parameters is also a Gaussian:\n$$p(\\boldsymbol{\\theta}_j | \\mathbf{y}, M_j) = \\mathcal{N}(\\boldsymbol{\\theta}_j | \\boldsymbol{\\mu}_{n,j}, \\Sigma_{n,j})$$\nThe posterior covariance $\\Sigma_{n,j}$ and mean $\\boldsymbol{\\mu}_{n,j}$ are given by:\n$$\n\\Sigma_{n,j} = (\\Sigma_0^{-1} + X_j^T \\Sigma^{-1} X_j)^{-1} \\\\\n\\boldsymbol{\\mu}_{n,j} = \\Sigma_{n,j} (X_j^T \\Sigma^{-1} \\mathbf{y})\n$$\nSince $\\boldsymbol{\\mu}_0=\\mathbf{0}$, the term involving the prior mean vanishes. Let $W = \\Sigma^{-1}$. The equations become:\n$$\nA_j = \\Sigma_0^{-1} + X_j^T W X_j \\\\\n\\Sigma_{n,j} = A_j^{-1} \\\\\n\\boldsymbol{\\mu}_{n,j} = \\Sigma_{n,j} (X_j^T W \\mathbf{y})\n$$\nThe parameter of interest is $f_0$, which is the first component of $\\boldsymbol{\\theta}_j$. Its marginal posterior distribution under model $M_j$ is a univariate Gaussian:\n$$p(f_0 | \\mathbf{y}, M_j) = \\mathcal{N}(f_0 | \\mu_{f_0, j}, \\sigma_{f_0, j}^2)$$\nwhere $\\mu_{f_0, j} = (\\boldsymbol{\\mu}_{n,j})_1$ and $\\sigma_{f_0, j}^2 = (\\Sigma_{n,j})_{11}$.\n\n#### 2. Model Evidence (Marginal Likelihood)\nThe evidence for model $M_j$ is the probability of the data integrated over all possible parameter values:\n$$p(\\mathbf{y} | M_j) = \\int p(\\mathbf{y} | \\boldsymbol{\\theta}_j, M_j) p(\\boldsymbol{\\theta}_j | M_j) d\\boldsymbol{\\theta}_j$$\nFor the linear-Gaussian case, this integral can be evaluated analytically. The result is:\n$$p(\\mathbf{y} | M_j) = \\mathcal{N}(\\mathbf{y} | X_j\\boldsymbol{\\mu}_0, \\Sigma + X_j\\Sigma_0 X_j^T)$$\nTo compute this stably, we use the logarithm of the evidence. A convenient and stable expression is:\n$$\n\\log p(\\mathbf{y}|M_j) = -\\frac{n}{2} \\log (2\\pi) -\\frac{1}{2}\\left( \\log\\det\\Sigma + \\log\\det\\Sigma_0 + \\log\\det A_j \\right) -\\frac{1}{2}\\left( \\mathbf{y}^T W \\mathbf{y} - \\boldsymbol{\\mu}_{n,j}^T A_j \\boldsymbol{\\mu}_{n,j} \\right)\n$$\nwhere $\\boldsymbol{\\mu}_{n,j}^T A_j \\boldsymbol{\\mu}_{n,j} = (X_j^T W \\mathbf{y})^T \\Sigma_{n,j} (X_j^T W \\mathbf{y})$.\n\n#### 3. Posterior Model Probability\nUsing Bayes' theorem for models, the posterior probability of model $M_j$ is:\n$$P(M_j | \\mathbf{y}) = \\frac{p(\\mathbf{y} | M_j) P(M_j)}{\\sum_{k=1}^3 p(\\mathbf{y} | M_k) P(M_k)}$$\nSince the prior probabilities $P(M_j)$ are equal ($1/3$), they cancel, and we get:\n$$P(M_j | \\mathbf{y}) = \\frac{p(\\mathbf{y} | M_j)}{\\sum_{k=1}^3 p(\\mathbf{y} | M_k)}$$\nTo handle potential numerical underflow or overflow, we compute these probabilities from the log-evidences $\\ell_j = \\log p(\\mathbf{y}|M_j)$ using the log-sum-exp trick:\nLet $\\ell_{\\max} = \\max(\\ell_1, \\ell_2, \\ell_3)$. Then the posterior weights $w_j = P(M_j|\\mathbf{y})$ are:\n$$w_j = \\frac{\\exp(\\ell_j - \\ell_{\\max})}{\\sum_{k=1}^3 \\exp(\\ell_k - \\ell_{\\max})}$$\n\n#### 4. BMA Posterior, Mean, and Credible Interval\nThe BMA posterior distribution for $f_0$ is a weighted mixture of the individual model posteriors:\n$$p(f_0 | \\mathbf{y}) = \\sum_{j=1}^3 w_j \\, p(f_0 | \\mathbf{y}, M_j) = \\sum_{j=1}^3 w_j \\, \\mathcal{N}(f_0 | \\mu_{f_0, j}, \\sigma_{f_0, j}^2)$$\nThe mean of this mixture distribution is the BMA posterior mean:\n$$E[f_0 | \\mathbf{y}] = \\sum_{j=1}^3 w_j \\mu_{f_0, j}$$\nThe central equal-tailed credible interval is defined by the quantiles of the mixture distribution. For a $68\\%$ credible interval, we need the $16\\%$ and $84\\%$ quantiles. The cumulative distribution function (CDF) of the mixture is:\n$$F(x) = P(f_0 \\le x | \\mathbf{y}) = \\sum_{j=1}^3 w_j \\, \\Phi\\left(\\frac{x - \\mu_{f_0, j}}{\\sigma_{f_0, j}}\\right)$$\nwhere $\\Phi$ is the standard normal CDF. We must find the values $x_{0.16}$ and $x_{0.84}$ such that $F(x_{0.16}) = 0.16$ and $F(x_{0.84}) = 0.84$. These equations are solved numerically using a root-finding algorithm.\n\n### Algorithmic Implementation\n\nFor each test case, the algorithm proceeds as follows:\n1.  Initialize prior parameters: $\\tau_0=10, \\tau_c=1$.\n2.  For each of the three models ($M_1, M_2, M_3$):\n    a. Construct the design matrix $X_j$ from the lattice spacings $a_i$ using the model's basis functions: $a_i$, $a_i^2$, and $a_i^2 \\log a_i$.\n    b. Compute the posterior mean $\\boldsymbol{\\mu}_{n,j}$ and covariance $\\Sigma_{n,j}$.\n    c. Extract the mean $\\mu_{f_0, j}$ and standard deviation $\\sigma_{f_0, j}$ for the parameter $f_0$.\n    d. Compute the log-model-evidence $\\ell_j = \\log p(\\mathbf{y}|M_j)$.\n3.  Calculate the posterior model probabilities $w_j$ from the log-evidences.\n4.  Calculate the BMA posterior mean for $f_0$.\n5.  Define the mixture CDF, $F(x)$.\n6.  Use a numerical root-finder (e.g., `scipy.optimize.brentq`) to find the lower ($x_{0.16}$) and upper ($x_{0.84}$) bounds of the $68\\%$ credible interval by solving $F(x) - q = 0$ for $q=0.16$ and $q=0.84$.\n7.  Collect and format the BMA mean, lower bound, and upper bound, rounded to six decimal places.\n\nThis procedure is systematically applied to all five test cases to produce the final result.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import brentq\n\ndef bma_extrapolation(a_data, y_data, sigma_data):\n    \"\"\"\n    Performs Bayesian Model Averaging for continuum extrapolation.\n    \"\"\"\n    # Convert inputs to numpy arrays\n    a = np.array(a_data)\n    y = np.array(y_data)\n    sigma = np.array(sigma_data)\n    \n    # Priors\n    tau_0 = 10.0\n    tau_c = 1.0\n    prior_cov_inv = np.diag([1.0 / tau_0**2, 1.0 / tau_c**2])\n    log_det_prior_cov = 2.0 * (np.log(tau_0) + np.log(tau_c))\n\n    # Data-related quantities\n    n = len(y)\n    W = np.diag(1.0 / sigma**2)\n    log_det_obs_cov = np.sum(2.0 * np.log(sigma))\n    yT_W_y = y @ W @ y\n    \n    # Basis functions for the three models\n    basis_functions = [\n        lambda x: x,\n        lambda x: x**2,\n        lambda x: x**2 * np.log(x)\n    ]\n    \n    model_posteriors = []\n    log_evidences = []\n\n    for basis_func in basis_functions:\n        # Build design matrix X\n        phi = basis_func(a)\n        X = np.vstack([np.ones_like(phi), phi]).T\n        \n        # Posterior calculation\n        A = prior_cov_inv + X.T @ W @ X\n        A_inv = np.linalg.inv(A)\n        b = X.T @ W @ y\n        \n        post_mean = A_inv @ b\n        post_cov = A_inv\n        \n        f0_mean = post_mean[0]\n        f0_var = post_cov[0, 0]\n        \n        model_posteriors.append({'mean': f0_mean, 'std': np.sqrt(f0_var)})\n        \n        # Log evidence calculation\n        log_det_A = np.log(np.linalg.det(A))\n        energy_term = yT_W_y - b @ post_mean\n        \n        log_evidence = -0.5 * (n * np.log(2 * np.pi) + log_det_prior_cov +\n                               log_det_obs_cov + log_det_A + energy_term)\n        log_evidences.append(log_evidence)\n    \n    # Model probabilities (weights)\n    log_evidences = np.array(log_evidences)\n    max_log_evidence = np.max(log_evidences)\n    evidences = np.exp(log_evidences - max_log_evidence)\n    post_probs = evidences / np.sum(evidences)\n    \n    # BMA mean\n    bma_mean = np.sum([p['mean'] * w for p, w in zip(model_posteriors, post_probs)])\n    \n    # BMA credible interval\n    def mixture_cdf(x):\n        cdf_val = 0.0\n        for i, p in enumerate(model_posteriors):\n            cdf_val += post_probs[i] * norm.cdf(x, loc=p['mean'], scale=p['std'])\n        return cdf_val\n\n    # Find a reasonable search interval for the root finder\n    all_means = [p['mean'] for p in model_posteriors]\n    all_stds = [p['std'] for p in model_posteriors]\n    search_min = min(all_means) - 10 * max(all_stds)\n    search_max = max(all_means) + 10 * max(all_stds)\n\n    # Functions to find roots for quantiles\n    lower_quantile_func = lambda x: mixture_cdf(x) - 0.16\n    upper_quantile_func = lambda x: mixture_cdf(x) - 0.84\n    \n    # Find lower and upper bounds of the 68% CI\n    ci_lower = brentq(lower_quantile_func, search_min, search_max)\n    ci_upper = brentq(upper_quantile_func, search_min, search_max)\n    \n    return [bma_mean, ci_lower, ci_upper]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        {\n            \"a\": [0.12, 0.09, 0.06, 0.045],\n            \"y\": [0.52028, 0.50772, 0.50582, 0.50143],\n            \"sigma\": [0.005, 0.004, 0.003, 0.003]\n        },\n        # Case 2\n        {\n            \"a\": [0.16, 0.12, 0.08, 0.04],\n            \"y\": [0.877, 0.902, 0.939, 0.964],\n            \"sigma\": [0.01, 0.01, 0.01, 0.01]\n        },\n        # Case 3\n        {\n            \"a\": [0.10, 0.08, 0.06, 0.05, 0.04],\n            \"y\": [0.189987074535, 0.1909176683392, 0.1954358607098302, 0.1957553346575, 0.19842489934],\n            \"sigma\": [0.004, 0.004, 0.004, 0.004, 0.004]\n        },\n        # Case 4\n        {\n            \"a\": [0.10, 0.05],\n            \"y\": [0.311, 0.29875],\n            \"sigma\": [0.02, 0.02]\n        },\n        # Case 5\n        {\n            \"a\": [0.15, 0.12, 0.09, 0.06, 0.03],\n            \"y\": [0.845, 0.78, 0.805, 0.765, 0.77],\n            \"sigma\": [0.05, 0.05, 0.05, 0.05, 0.05]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = bma_extrapolation(case[\"a\"], case[\"y\"], case[\"sigma\"])\n        results.append(result)\n\n    # Format the final output string exactly as specified.\n    result_strings = []\n    for res in results:\n        mean, lower, upper = res\n        result_strings.append(f\"[{mean:.6f},{lower:.6f},{upper:.6f}]\")\n    \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        }
    ]
}