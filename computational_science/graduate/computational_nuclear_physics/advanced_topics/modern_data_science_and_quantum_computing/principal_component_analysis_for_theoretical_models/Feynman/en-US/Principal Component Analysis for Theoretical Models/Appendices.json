{
    "hands_on_practices": [
        {
            "introduction": "Principal Component Analysis provides a new basis ordered by variance, but how stable is this basis? This exercise explores the fundamental question of how robust principal components are to the presence of noise, a ubiquitous feature of both experimental data and numerical simulations. By applying first-order perturbation theory—a powerful tool often encountered in quantum mechanics—you will derive the analytical corrections to the principal directions and eigenvalues, revealing how noise can induce mixing between the underlying components of a physical model. This practice is invaluable for developing a deeper intuition for the stability of PCA results and for understanding when a small amount of noise can significantly alter the interpretation of the dominant physical correlations.",
            "id": "3581397",
            "problem": "In computational nuclear physics, theoretical models such as chiral effective field theory emulators generate vectors of correlated observables across model parameter draws. Let the intrinsic model prediction be a random vector $s \\in \\mathbb{R}^{3}$ with zero mean and covariance matrix $C = \\mathbb{E}[s s^{\\top}]$, where $C$ is symmetric positive definite with distinct eigenvalues and orthonormal eigenvectors. Data acquisition further introduces independent additive Gaussian noise $n \\sim \\mathcal{N}(0, \\epsilon \\Sigma)$, with zero mean and covariance $\\epsilon \\Sigma$, where $\\Sigma$ is a fixed symmetric matrix and $0 < \\epsilon \\ll 1$ is a small dimensionless noise-strength parameter representing the signal-to-noise scale. The measured vector is $x = s + n$, so that by independence and bilinearity of covariance, the total covariance of $x$ is $C_{\\epsilon} = \\mathbb{E}[x x^{\\top}] = C + \\epsilon \\Sigma$.\n\nPrincipal component analysis (PCA) seeks the eigenpairs of the covariance matrix. Assume that $C$ admits the spectral decomposition $C = \\sum_{k=1}^{3} \\lambda_{k} u_{k} u_{k}^{\\top}$ with strictly ordered, nondegenerate eigenvalues $\\lambda_{1} > \\lambda_{2} > \\lambda_{3}$ and orthonormal eigenvectors $u_{1}, u_{2}, u_{3}$. Consider the eigenproblem for the perturbed covariance $C_{\\epsilon}$:\n$$(C + \\epsilon \\Sigma) \\, u_{k}(\\epsilon) = \\lambda_{k}(\\epsilon) \\, u_{k}(\\epsilon),$$\nwhere $u_{k}(\\epsilon)$ and $\\lambda_{k}(\\epsilon)$ are the perturbed principal directions and eigenvalues, respectively, chosen so that $u_{k}(0) = u_{k}$.\n\nWork in the orthonormal basis $\\{e_{1}, e_{2}, e_{3}\\}$ in which $C$ is diagonal with entries $\\lambda_{1}, \\lambda_{2}, \\lambda_{3}$, so that $u_{1} = e_{1}$, $u_{2} = e_{2}$, and $u_{3} = e_{3}$. Let the noise covariance be the symmetric matrix\n$$\\Sigma = \\begin{pmatrix}\n\\sigma_{11} & \\sigma_{12} & \\sigma_{13} \\\\\n\\sigma_{12} & \\sigma_{22} & \\sigma_{23} \\\\\n\\sigma_{13} & \\sigma_{23} & \\sigma_{33}\n\\end{pmatrix}.$$\n\nStarting only from the definitions of covariance, eigenvalues, and eigenvectors for symmetric matrices, and assuming nondegeneracy and small $\\epsilon$, derive to first order in $\\epsilon$ the perturbations to the eigenvalues and the principal directions. Then, give the explicit first-order coefficient multiplying $e_{2}$ in the normalized perturbed first principal direction $u_{1}(\\epsilon)$.\n\nYour final answer must be a single closed-form analytic expression in terms of $\\epsilon$, $\\lambda_{1}$, $\\lambda_{2}$, and $\\sigma_{12}$. Do not include any units. Do not provide any numerical approximation.",
            "solution": "The user wants me to solve the following problem.\n\nThe problem presented is a standard exercise in first-order non-degenerate perturbation theory as applied to matrices in linear algebra, framed within a context of computational physics. The task is to determine the first-order correction to an eigenvector of a covariance matrix under a small additive perturbation.\n\nLet the unperturbed operator be the covariance matrix $C$, and the perturbation be $\\epsilon \\Sigma$. The total, perturbed covariance matrix is $C_{\\epsilon} = C + \\epsilon \\Sigma$. The unperturbed eigenproblem is $C u_k = \\lambda_k u_k$ for $k \\in \\{1, 2, 3\\}$. The problem states that we work in the orthonormal basis $\\{e_{1}, e_{2}, e_{3}\\}$ where $C$ is diagonal, which means this is the eigenbasis of $C$. Thus, we have $u_k = e_k$ and $C = \\text{diag}(\\lambda_1, \\lambda_2, \\lambda_3)$. The eigenvalues $\\lambda_1, \\lambda_2, \\lambda_3$ are assumed to be distinct and ordered $\\lambda_1 > \\lambda_2 > \\lambda_3$, ensuring they are non-degenerate.\n\nThe perturbed eigenproblem is $(C + \\epsilon \\Sigma) u_k(\\epsilon) = \\lambda_k(\\epsilon) u_k(\\epsilon)$. We assume the perturbed eigenvalues and eigenvectors can be expanded as a power series in the small parameter $\\epsilon$:\n$$ \\lambda_k(\\epsilon) = \\lambda_k^{(0)} + \\epsilon \\lambda_k^{(1)} + \\epsilon^2 \\lambda_k^{(2)} + \\dots $$\n$$ u_k(\\epsilon) = u_k^{(0)} + \\epsilon u_k^{(1)} + \\epsilon^2 u_k^{(2)} + \\dots $$\nThe zeroth-order terms correspond to the unperturbed system, so $\\lambda_k^{(0)} = \\lambda_k$ and $u_k^{(0)} = u_k$. We also require the perturbed eigenvectors to remain normalized, $\\|u_k(\\epsilon)\\|^2=1$.\n\nSubstituting the series expansions into the perturbed eigenvalue equation gives:\n$$ (C + \\epsilon \\Sigma) (u_k + \\epsilon u_k^{(1)} + O(\\epsilon^2)) = (\\lambda_k + \\epsilon \\lambda_k^{(1)} + O(\\epsilon^2))(u_k + \\epsilon u_k^{(1)} + O(\\epsilon^2)) $$\nExpanding and collecting terms by powers of $\\epsilon$:\nZeroth order ($O(\\epsilon^0)$):\n$$ C u_k = \\lambda_k u_k $$\nThis is the unperturbed eigenvalue equation, which is satisfied by definition.\n\nFirst order ($O(\\epsilon^1)$):\n$$ C u_k^{(1)} + \\Sigma u_k = \\lambda_k u_k^{(1)} + \\lambda_k^{(1)} u_k $$\nThis equation allows us to determine the first-order corrections $\\lambda_k^{(1)}$ and $u_k^{(1)}$.\n\nTo find the first-order correction to the eigenvalue, $\\lambda_k^{(1)}$, we take the inner product of the first-order equation with the unperturbed eigenvector $u_k^{\\top}$:\n$$ u_k^{\\top} C u_k^{(1)} + u_k^{\\top} \\Sigma u_k = u_k^{\\top} \\lambda_k u_k^{(1)} + u_k^{\\top} \\lambda_k^{(1)} u_k $$\nSince $C$ is symmetric, $u_k^{\\top} C = (C u_k)^{\\top} = (\\lambda_k u_k)^{\\top} = \\lambda_k u_k^{\\top}$. The equation becomes:\n$$ \\lambda_k u_k^{\\top} u_k^{(1)} + u_k^{\\top} \\Sigma u_k = \\lambda_k u_k^{\\top} u_k^{(1)} + \\lambda_k^{(1)} u_k^{\\top} u_k $$\nUsing the normalization $u_k^{\\top} u_k = 1$, the terms involving $u_k^{(1)}$ cancel, yielding the standard result for the first-order energy (eigenvalue) correction:\n$$ \\lambda_k^{(1)} = u_k^{\\top} \\Sigma u_k $$\nIn the specified basis where $u_k = e_k$, this is $\\lambda_k^{(1)} = e_k^{\\top} \\Sigma e_k = \\Sigma_{kk} = \\sigma_{kk}$.\n\nTo find the first-order correction to the eigenvector, $u_k^{(1)}$, we rearrange the first-order equation:\n$$ (C - \\lambda_k I) u_k^{(1)} = (\\lambda_k^{(1)} I - \\Sigma) u_k $$\nWe can express the correction vector $u_k^{(1)}$ as a linear combination of the unperturbed eigenvectors $\\{u_j\\}_{j=1}^3$:\n$$ u_k^{(1)} = \\sum_{j=1}^{3} c_{kj} u_j $$\nThe normalization condition $\\|u_k(\\epsilon)\\|^2=1$ implies that, to first order, the correction $u_k^{(1)}$ must be orthogonal to $u_k$.\n$$ \\|u_k + \\epsilon u_k^{(1)}\\|^2 = (u_k + \\epsilon u_k^{(1)})^{\\top}(u_k + \\epsilon u_k^{(1)}) = u_k^{\\top}u_k + 2\\epsilon u_k^{\\top}u_k^{(1)} + O(\\epsilon^2) = 1 $$\nSince $u_k^{\\top}u_k = 1$, we must have $u_k^{\\top}u_k^{(1)} = 0$. This implies that the coefficient $c_{kk} = u_k^{\\top}u_k^{(1)}$ is zero. Thus, the expansion for $u_k^{(1)}$ runs over $j \\neq k$:\n$$ u_k^{(1)} = \\sum_{j \\neq k} c_{kj} u_j $$\nTo find the coefficients $c_{kj}$, we take the inner product of the rearranged first-order equation with $u_j^{\\top}$ for $j \\neq k$:\n$$ u_j^{\\top} (C - \\lambda_k I) u_k^{(1)} = u_j^{\\top} (\\lambda_k^{(1)} I - \\Sigma) u_k $$\nFor the left-hand side, since $u_j^{\\top}C=\\lambda_j u_j^{\\top}$:\n$$ u_j^{\\top} (C - \\lambda_k I) u_k^{(1)} = (\\lambda_j - \\lambda_k) u_j^{\\top} u_k^{(1)} = (\\lambda_j - \\lambda_k) u_j^{\\top} \\sum_{m \\neq k} c_{km} u_m = (\\lambda_j - \\lambda_k) c_{kj} $$\nFor the right-hand side, since $j \\neq k$, $u_j^{\\top}u_k=0$:\n$$ u_j^{\\top} (\\lambda_k^{(1)} I - \\Sigma) u_k = \\lambda_k^{(1)} u_j^{\\top}u_k - u_j^{\\top}\\Sigma u_k = - u_j^{\\top}\\Sigma u_k $$\nEquating the two sides gives:\n$$ (\\lambda_j - \\lambda_k) c_{kj} = - u_j^{\\top}\\Sigma u_k $$\nSince the eigenvalues are non-degenerate, $\\lambda_j \\neq \\lambda_k$ for $j \\neq k$, we can solve for $c_{kj}$:\n$$ c_{kj} = \\frac{- u_j^{\\top}\\Sigma u_k}{\\lambda_j - \\lambda_k} = \\frac{u_j^{\\top}\\Sigma u_k}{\\lambda_k - \\lambda_j} $$\nThe first-order correction to the eigenvector is therefore:\n$$ u_k^{(1)} = \\sum_{j \\neq k} \\frac{u_j^{\\top}\\Sigma u_k}{\\lambda_k - \\lambda_j} u_j $$\nNow we apply this general result to the specific question. We need the coefficient of $e_2$ in the perturbed first principal direction $u_1(\\epsilon)$. We set $k=1$. The unperturbed eigenvector is $u_1=e_1$. The perturbed eigenvector to first order is:\n$$ u_1(\\epsilon) \\approx u_1 + \\epsilon u_1^{(1)} = u_1 + \\epsilon \\sum_{j \\neq 1} \\frac{u_j^{\\top}\\Sigma u_1}{\\lambda_1 - \\lambda_j} u_j $$\nExpanding the sum for $j \\in \\{2, 3\\}$:\n$$ u_1(\\epsilon) \\approx u_1 + \\epsilon \\left( \\frac{u_2^{\\top}\\Sigma u_1}{\\lambda_1 - \\lambda_2} u_2 + \\frac{u_3^{\\top}\\Sigma u_1}{\\lambda_1 - \\lambda_3} u_3 \\right) $$\nIn the given basis, $u_1=e_1$, $u_2=e_2$, and $u_3=e_3$. The matrix elements of $\\Sigma$ in this basis are $u_i^\\top \\Sigma u_j = e_i^\\top \\Sigma e_j = \\sigma_{ij}$. Substituting these into the expression:\n$$ u_1(\\epsilon) \\approx e_1 + \\epsilon \\left( \\frac{\\sigma_{21}}{\\lambda_1 - \\lambda_2} e_2 + \\frac{\\sigma_{31}}{\\lambda_1 - \\lambda_3} e_3 \\right) $$\nSince $\\Sigma$ is a symmetric matrix, $\\sigma_{21} = \\sigma_{12}$.\n$$ u_1(\\epsilon) \\approx e_1 + \\epsilon \\frac{\\sigma_{12}}{\\lambda_1 - \\lambda_2} e_2 + \\epsilon \\frac{\\sigma_{13}}{\\lambda_1 - \\lambda_3} e_3 $$\nThe vector $u_1(\\epsilon)$ can be expressed as a linear combination of the basis vectors: $u_1(\\epsilon) = c_1(\\epsilon)e_1 + c_2(\\epsilon)e_2 + c_3(\\epsilon)e_3$. The coefficient multiplying $e_2$ is $c_2(\\epsilon) = e_2^{\\top}u_1(\\epsilon)$.\nTo first order in $\\epsilon$, this coefficient is:\n$$ e_2^{\\top} u_1(\\epsilon) \\approx e_2^{\\top} \\left( e_1 + \\epsilon \\frac{\\sigma_{12}}{\\lambda_1 - \\lambda_2} e_2 + \\epsilon \\frac{\\sigma_{13}}{\\lambda_1 - \\lambda_3} e_3 \\right) $$\nUsing the orthonormality of the basis vectors ($e_2^{\\top}e_1=0$, $e_2^{\\top}e_2=1$, $e_2^{\\top}e_3=0$):\n$$ e_2^{\\top} u_1(\\epsilon) \\approx 0 + \\epsilon \\frac{\\sigma_{12}}{\\lambda_1 - \\lambda_2} (1) + \\epsilon \\frac{\\sigma_{13}}{\\lambda_1 - \\lambda_3} (0) = \\epsilon \\frac{\\sigma_{12}}{\\lambda_1 - \\lambda_2} $$\nThe explicit first-order coefficient multiplying $e_2$ in the normalized perturbed first principal direction $u_1(\\epsilon)$ is the term of order $\\epsilon$ in the projection of $u_1(\\epsilon)$ onto $e_2$. This is the expression derived.",
            "answer": "$$\\boxed{\\epsilon \\frac{\\sigma_{12}}{\\lambda_{1} - \\lambda_{2}}}$$"
        },
        {
            "introduction": "One of the most powerful applications of PCA in computational physics is building fast and accurate emulators of computationally expensive theoretical models. However, a prediction from an emulator is incomplete without a robust estimate of its uncertainty. This exercise guides you through the process of constructing a statistically sound predictive interval for a PCA-based emulator, accounting for multiple distinct sources of error: the statistical uncertainty from the emulator's predictions of the PC scores and the systematic error arising from truncating the PCA expansion. You will use the Welch-Satterthwaite formula to combine these variances, a crucial technique for uncertainty quantification that properly handles the different degrees of freedom associated with each error estimate.",
            "id": "3581367",
            "problem": "You are emulating a theoretical nuclear model’s observable using principal component analysis (PCA). Let $y(\\mathbf{x})$ denote the two-neutron separation energy for a fixed nucleus at parameter vector $\\mathbf{x}$, measured in megaelectronvolts (MeV). A training ensemble of model evaluations over $\\mathbf{x}$ has been centered to produce a data matrix, and principal component analysis (PCA) has been performed on this centered training output. For a held-out parameter vector $\\mathbf{x}^{\\ast}$, you reconstruct the observable using $K=2$ retained principal components as\n$$\ny(\\mathbf{x}^{\\ast}) \\approx \\mu + \\sum_{j=1}^{2} \\ell_{j}\\, t_{j}(\\mathbf{x}^{\\ast}) + \\varepsilon_{\\text{trunc}},\n$$\nwhere $\\mu$ is the training-set mean for the observable, $\\ell_{j}$ is the loading coefficient for the observable along principal component $j$, $t_{j}(\\mathbf{x}^{\\ast})$ is the principal component score predicted by an emulator at $\\mathbf{x}^{\\ast}$, and $\\varepsilon_{\\text{trunc}}$ is a zero-mean truncation remainder from neglected principal components. Assume the following:\n- The emulator outputs for the scores are approximately Gaussian and independent across $j$, with predictive distributions $t_{j}(\\mathbf{x}^{\\ast}) \\sim \\mathcal{N}(m_{j}, v_{j})$. The variance $v_{j}$ is itself estimated from finite training data and can be treated as a scaled chi-square quantity with $ \\nu_{j}$ degrees of freedom (DoF).\n- The truncation remainder is independent of the scores and is modeled as $\\varepsilon_{\\text{trunc}} \\sim \\mathcal{N}(0, \\sigma_{\\text{trunc}}^{2})$, with $\\sigma_{\\text{trunc}}^{2}$ estimated from held-out residuals and associated with $ \\nu_{3}$ DoF.\n- All estimators referenced above are mutually independent, and normality is a good approximation due to the central limit effect in the emulator’s training regime.\n\nFrom first principles, starting from the definition of the PCA reconstruction and the law of total variance, derive a two-sided $100(1-\\alpha)\\%$ predictive interval for $y(\\mathbf{x}^{\\ast})$ that properly accounts for the estimation of multiple independent variance contributions. Your derivation must explicitly produce:\n- The analytical form of the predictive mean and variance for $y(\\mathbf{x}^{\\ast})$ in terms of $\\mu$, $\\ell_{j}$, $m_{j}$, $v_{j}$, and $\\sigma_{\\text{trunc}}^{2}$.\n- A justified construction of a single effective degrees of freedom to attach to the standardized predictive error when multiple independent, estimated variance components are combined.\n\nThen, using your derived expressions, compute the two-sided $95\\%$ predictive interval (i.e., $\\alpha = 0.05$) for the following numerical specification:\n- $\\mu = 8.5$ MeV, $K=2$, $\\ell_{1} = 1.2$, $\\ell_{2} = -0.8$,\n- $m_{1} = 1.0$, $m_{2} = -0.5$,\n- $v_{1} = 0.09$, $v_{2} = 0.16$,\n- $\\sigma_{\\text{trunc}}^{2} = 0.25$,\n- $\\nu_{1} = 40$, $\\nu_{2} = 30$, $\\nu_{3} = 50$.\n\nRound your numerical endpoints to four significant figures. Express the final energy in MeV.\n\nFinally, explain a statistically principled procedure to validate the nominal coverage of your predictive intervals empirically using out-of-sample tests, including how to quantify uncertainty in the empirical coverage estimate and what diagnostics you would examine. The explanation itself is not part of the final numerical answer but must be included in your solution reasoning.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in statistical modeling and uncertainty quantification, well-posed with a complete and consistent set of givens, and expressed in objective, formal language. There are no contradictions, ambiguities, or pseudo-scientific claims. The problem requires a standard but non-trivial application of statistical principles, specifically the construction of a predictive interval involving multiple estimated variance components.\n\nWe are tasked with deriving a two-sided $100(1-\\alpha)\\%$ predictive interval for a reconstructed observable $y(\\mathbf{x}^{\\ast})$. The model for the observable is given by\n$$\ny(\\mathbf{x}^{\\ast}) \\approx \\mu + \\sum_{j=1}^{2} \\ell_{j} t_{j}(\\mathbf{x}^{\\ast}) + \\varepsilon_{\\text{trunc}}\n$$\nLet $Y$ denote the random variable representing the predicted value of $y(\\mathbf{x}^{\\ast})$. The model can be written as\n$$\nY = \\mu + \\ell_{1} t_{1} + \\ell_{2} t_{2} + \\varepsilon_{\\text{trunc}}\n$$\nwhere $t_{1}$, $t_{2}$, and $\\varepsilon_{\\text{trunc}}$ are independent random variables. The parameters $\\mu$, $\\ell_{1}$, and $\\ell_{2}$ are treated as fixed constants in the context of this prediction.\n\nFirst, we derive the analytical form of the predictive mean and variance for $Y$.\n\nThe predictive mean, $E[Y]$, is found by taking the expectation of the expression for $Y$. By the linearity of expectation, we have:\n$$\nE[Y] = E[\\mu + \\ell_{1} t_{1} + \\ell_{2} t_{2} + \\varepsilon_{\\text{trunc}}] = \\mu + \\ell_{1} E[t_{1}] + \\ell_{2} E[t_{2}] + E[\\varepsilon_{\\text{trunc}}]\n$$\nWe are given the following information about the expectations of the random components:\n- The predictive distribution for the score $t_{j}(\\mathbf{x}^{\\ast})$ is $t_{j} \\sim \\mathcal{N}(m_{j}, v_{j})$, so $E[t_{j}] = m_{j}$.\n- The truncation remainder $\\varepsilon_{\\text{trunc}}$ is modeled as $\\varepsilon_{\\text{trunc}} \\sim \\mathcal{N}(0, \\sigma_{\\text{trunc}}^{2})$, so $E[\\varepsilon_{\\text{trunc}}] = 0$.\nSubstituting these into the expression for $E[Y]$ gives the predictive mean:\n$$\nE[Y] = \\mu + \\ell_{1} m_{1} + \\ell_{2} m_{2}\n$$\n\nThe predictive variance, $\\text{Var}(Y)$, is found by taking the variance of the expression for $Y$. Since $\\mu$, $\\ell_{1}$, and $\\ell_{2}$ are constants, and $t_1$, $t_2$, and $\\varepsilon_{\\text{trunc}}$ are mutually independent random variables, the variance of the sum is the sum of the variances:\n$$\n\\text{Var}(Y) = \\text{Var}(\\mu + \\ell_{1} t_{1} + \\ell_{2} t_{2} + \\varepsilon_{\\text{trunc}}) = \\text{Var}(\\ell_{1} t_{1}) + \\text{Var}(\\ell_{2} t_{2}) + \\text{Var}(\\varepsilon_{\\text{trunc}})\n$$\nUsing the property $\\text{Var}(aX) = a^2 \\text{Var}(X)$, we get:\n$$\n\\text{Var}(Y) = \\ell_{1}^{2} \\text{Var}(t_{1}) + \\ell_{2}^{2} \\text{Var}(t_{2}) + \\text{Var}(\\varepsilon_{\\text{trunc}})\n$$\nThe problem provides the variances of the components:\n- $\\text{Var}(t_{j}) = v_j$\n- $\\text{Var}(\\varepsilon_{\\text{trunc}}) = \\sigma_{\\text{trunc}}^{2}$\nSubstituting these gives the total predictive variance, which we denote $V_{\\text{tot}}$:\n$$\nV_{\\text{tot}} = \\text{Var}(Y) = \\ell_{1}^{2} v_{1} + \\ell_{2}^{2} v_{2} + \\sigma_{\\text{trunc}}^{2}\n$$\nSince $Y$ is a linear combination of independent (approximate) Gaussian random variables, its predictive distribution is also approximately Gaussian: $Y \\sim \\mathcal{N}(E[Y], V_{\\text{tot}})$. However, the variance $V_{\\text{tot}}$ is not known exactly but is estimated from components ($v_1, v_2, \\sigma_{\\text{trunc}}^2$) which are themselves estimates with finite degrees of freedom. Therefore, the standardized predictive error will follow a Student's t-distribution, not a standard normal distribution.\n\nThe total variance $V_{\\text{tot}}$ is a linear combination of three independently estimated variances. To find the appropriate degrees of freedom for the corresponding t-distribution, we must use the Welch-Satterthwaite approximation. This approximation provides a method for finding the effective degrees of freedom, $\\nu_{\\text{eff}}$, for a linear combination of independent mean-square estimates.\nLet the individual estimated variance components be $u_{1} = \\ell_{1}^{2} v_{1}$, $u_{2} = \\ell_{2}^{2} v_{2}$, and $u_{3} = \\sigma_{\\text{trunc}}^{2}$. The degrees of freedom associated with the estimates $v_1$, $v_2$, and $\\sigma_{\\text{trunc}}^2$ are $\\nu_1$, $\\nu_2$, and $\\nu_3$, respectively. The Welch-Satterthwaite formula for $\\nu_{\\text{eff}}$ is:\n$$\n\\nu_{\\text{eff}} = \\frac{(\\sum_{i=1}^{3} u_i)^2}{\\sum_{i=1}^{3} \\frac{u_i^2}{\\nu_i}} = \\frac{(\\ell_{1}^{2} v_{1} + \\ell_{2}^{2} v_{2} + \\sigma_{\\text{trunc}}^{2})^{2}}{\\frac{(\\ell_{1}^{2} v_{1})^{2}}{\\nu_{1}} + \\frac{(\\ell_{2}^{2} v_{2})^{2}}{\\nu_{2}} + \\frac{(\\sigma_{\\text{trunc}}^{2})^{2}}{\\nu_{3}}}\n$$\nThe standardized quantity $T = \\frac{Y - E[Y]}{\\sqrt{V_{\\text{tot}}}}$ follows an approximate Student's t-distribution with $\\nu_{\\text{eff}}$ degrees of freedom, i.e., $T \\sim t_{\\nu_{\\text{eff}}}$.\n\nA two-sided $100(1-\\alpha)\\%$ predictive interval for $Y$ is constructed as:\n$$\nE[Y] \\pm t_{1-\\alpha/2, \\nu_{\\text{eff}}} \\sqrt{V_{\\text{tot}}}\n$$\nwhere $t_{1-\\alpha/2, \\nu_{\\text{eff}}}$ is the upper $(1-\\alpha/2)$ critical value of the Student's t-distribution with $\\nu_{\\text{eff}}$ degrees of freedom.\n\nNow, we compute the $95\\%$ predictive interval ($\\alpha=0.05$) using the provided numerical values:\n$\\mu = 8.5$, $\\ell_{1} = 1.2$, $\\ell_{2} = -0.8$, $m_{1} = 1.0$, $m_{2} = -0.5$, $v_{1} = 0.09$, $v_{2} = 0.16$, $\\sigma_{\\text{trunc}}^{2} = 0.25$, $\\nu_{1} = 40$, $\\nu_{2} = 30$, $\\nu_{3} = 50$.\n\n1.  Calculate the predictive mean $E[Y]$:\n    $$\n    E[Y] = 8.5 + (1.2)(1.0) + (-0.8)(-0.5) = 8.5 + 1.2 + 0.4 = 10.1 \\, \\text{MeV}\n    $$\n2.  Calculate the total predictive variance $V_{\\text{tot}}$:\n    $$\n    V_{\\text{tot}} = (1.2)^{2}(0.09) + (-0.8)^{2}(0.16) + 0.25 = (1.44)(0.09) + (0.64)(0.16) + 0.25\n    $$\n    $$\n    V_{\\text{tot}} = 0.1296 + 0.1024 + 0.25 = 0.4820\n    $$\n3.  Calculate the effective degrees of freedom $\\nu_{\\text{eff}}$:\n    The variance components are $u_1 = 0.1296$, $u_2 = 0.1024$, $u_3 = 0.25$.\n    $$\n    \\nu_{\\text{eff}} = \\frac{(0.4820)^{2}}{\\frac{(0.1296)^{2}}{40} + \\frac{(0.1024)^{2}}{30} + \\frac{(0.25)^{2}}{50}} = \\frac{0.232324}{\\frac{0.01679616}{40} + \\frac{0.01048576}{30} + \\frac{0.0625}{50}}\n    $$\n    $$\n    \\nu_{\\text{eff}} = \\frac{0.232324}{0.000419904 + 0.0003495253\\dots + 0.00125} = \\frac{0.232324}{0.002019429\\dots} \\approx 115.045\n    $$\n4.  Construct the $95\\%$ predictive interval. We need the critical value $t_{1-0.05/2, 115.045} = t_{0.975, 115.045}$. Using a statistical calculator, this value is approximately $1.9808$.\n    The standard deviation is $\\sqrt{V_{\\text{tot}}} = \\sqrt{0.4820} \\approx 0.694262$ MeV.\n    The margin of error is $t_{0.975, 115.045} \\times \\sqrt{V_{\\text{tot}}} \\approx 1.9808 \\times 0.694262 \\approx 1.37525$ MeV.\n    The interval is $10.1 \\pm 1.37525$.\n    Lower bound: $10.1 - 1.37525 = 8.72475$ MeV.\n    Upper bound: $10.1 + 1.37525 = 11.47525$ MeV.\n    Rounding to four significant figures, the lower bound is $8.725$ and the upper bound is $11.48$.\n    The $95\\%$ predictive interval is $[8.725, 11.48]$ MeV.\n\nFinally, we are asked to explain a procedure to validate the nominal coverage of these predictive intervals.\nA statistically principled procedure for empirical validation involves the following steps:\n1.  **Out-of-Sample Test Set**: A large test set of $N$ parameter vectors, $\\{\\mathbf{x}^{\\text{test}}_i\\}_{i=1}^N$, is required. These vectors must not have been used in the training of the emulator or the construction of the PCA basis.\n2.  **Ground Truth Calculation**: For each $\\mathbf{x}^{\\text{test}}_i$, the full, high-fidelity nuclear model is run to obtain the \"true\" observable value, $y^{\\text{true}}_i = y(\\mathbf{x}^{\\text{test}}_i)$.\n3.  **Prediction and Interval Construction**: For each test point $i$, the emulator is used to generate a predictive distribution. This involves computing the predictive mean $E_i[Y]$, total variance $V_{\\text{tot}, i}$, and effective degrees of freedom $\\nu_{\\text{eff}, i}$. A $100(1-\\alpha)\\%$ predictive interval $[L_i, U_i]$ is then constructed for each point. Note that $E_i, V_{\\text{tot}, i}, \\nu_{\\text{eff}, i}$ may vary for each $\\mathbf{x}^{\\text{test}}_i$ if the emulator's predictive uncertainty changes with the input point.\n4.  **Empirical Coverage Calculation**: The empirical coverage, $\\hat{p}$, is the fraction of test points for which the true value falls within the predicted interval. Define an indicator variable $C_i = 1$ if $y^{\\text{true}}_i \\in [L_i, U_i]$ and $C_i = 0$ otherwise. The empirical coverage is $\\hat{p} = \\frac{1}{N} \\sum_{i=1}^N C_i$. For a well-calibrated model, we expect $\\hat{p} \\approx 1-\\alpha$.\n5.  **Uncertainty Quantification**: The empirical coverage $\\hat{p}$ is a statistical estimate of the true coverage probability. The number of \"successes\" (coverage events) can be modeled as a binomial random variable, $K \\sim \\text{Bin}(N, p)$. The uncertainty in $\\hat{p}$ can be quantified by a confidence interval. A simple Wald interval is $\\hat{p} \\pm z_{1-\\beta/2} \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{N}}$, where $1-\\beta$ is the confidence level for the interval on the coverage (e.g., $95\\%$). More robust methods like the Wilson score interval are preferable. The nominal coverage $1-\\alpha$ should lie within this confidence interval.\n6.  **Diagnostic Plots**: Further validation can be done through diagnostics. The most powerful is the Probability Integral Transform (PIT) histogram. For each test point $i$, we compute $z_i = F_{t, \\nu_{\\text{eff},i}}((y^{\\text{true}}_i - E_i[Y])/\\sqrt{V_{\\text{tot},i}})$, where $F_{t, \\nu_{\\text{eff},i}}$ is the CDF of the Student's t-distribution with the appropriate degrees of freedom. If the predictive distributions are well-calibrated, the set of $\\{z_i\\}$ should be uniformly distributed on $[0, 1]$. A histogram of the $z_i$ values should be approximately flat. Deviations from uniformity indicate miscalibration: a U-shape suggests predictive variance is underestimated (intervals too narrow), while a mound-shape suggests variance is overestimated (intervals too wide).",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 8.725 & 11.48 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Beyond dimensionality reduction, PCA is a formidable diagnostic tool for interrogating the structure of theoretical models themselves. In complex models such as nuclear Energy Density Functionals, it is common to encounter parameter degeneracies, where different combinations of parameters yield almost identical predictions for physical observables. This practical coding exercise tasks you with implementing a PCA-based test to detect such degeneracies by analyzing the covariance of model observables induced by parameter variations. A degeneracy reveals itself as a direction in observable space with near-zero variance, corresponding to a near-zero eigenvalue in the covariance matrix, which your program will identify. This skill is essential for diagnosing issues with model identifiability and guiding the refinement of theoretical frameworks.",
            "id": "3581425",
            "problem": "Consider a simplified calibration setting for a Skyrme Energy Density Functional (EDF) where a vector of model parameters $p \\in \\mathbb{R}^d$ determines a vector of observables $y \\in \\mathbb{R}^m$. Assume that near a calibration point $p_0$, small perturbations $\\delta p$ induce changes in observables that can be described by a linearized sensitivity model, and that the parameter variations are random with a known covariance. All quantities are treated as dimensionless by prior normalization, so no physical units are required.\n\nYour task is to design and implement a Principal Component Analysis (PCA)-based test to detect parameter degeneracies by analyzing near-zero eigenvalues in the covariance of observables induced by parameter variations. The test should be derived from first principles using the following foundational bases: linear response around a calibration point, random variables with covariance, and spectral decomposition of symmetric positive semidefinite matrices. You must not use any pre-specified formulas for the induced observable covariance or for PCA within the problem statement; instead, derive them in your solution from the stated bases.\n\nDefine a degeneracy detection criterion as follows. Let the eigenvalues of the observable covariance matrix be $\\{\\lambda_i\\}_{i=1}^m$ with $\\lambda_{\\max} = \\max_i \\lambda_i$. Given a threshold parameter $\\tau > 0$, declare a principal component to be \"degenerate\" if its eigenvalue satisfies $\\lambda_i \\le \\tau \\lambda_{\\max}$. The total number of degeneracies $D$ is the count of such eigenvalues. Larger values of $D$ indicate stronger degeneracy.\n\nImplement a program that, for each provided test case, computes the induced observable covariance matrix from the given sensitivity matrix and parameter covariance, performs PCA, and returns the degeneracy count $D$ based on the criterion above.\n\nThe test suite consists of four cases. In each case, $m = d = 6$. For clarity, denote the sensitivity matrix by $S \\in \\mathbb{R}^{6 \\times 6}$ and the parameter covariance by $C_p \\in \\mathbb{R}^{6 \\times 6}$. The threshold is $\\tau = 10^{-9}$.\n\nCase 1 (well-conditioned, no degeneracy expected):\n- Sensitivity matrix $S_1$:\n$$\nS_1 = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 0 \\\\\n0.5 & 0.2 & 0.1 & 0.3 & 1.0 & 0.0 \\\\\n0.1 & 0.4 & 0.3 & 0.2 & 0.0 & 1.0\n\\end{pmatrix}\n$$\n- Parameter covariance $C_{p,1}$:\n$$\nC_{p,1} = \\mathrm{diag}(1.0, 0.8, 1.2, 0.9, 1.1, 1.0)\n$$\n\nCase 2 (exact linear dependence between two parameter directions):\n- Sensitivity matrix $S_2$ obtained by copying column $5$ of $S_1$ into column $6$, so columns $5$ and $6$ are identical:\n$$\nS_2 = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 0 \\\\\n0.5 & 0.2 & 0.1 & 0.3 & 1.0 & 1.0 \\\\\n0.1 & 0.4 & 0.3 & 0.2 & 0.0 & 0.0\n\\end{pmatrix}\n$$\n- Parameter covariance $C_{p,2}$:\n$$\nC_{p,2} = \\mathrm{diag}(1.0, 1.0, 1.0, 1.0, 1.0, 1.0)\n$$\n\nCase 3 (anisotropic parameter covariance with a nearly frozen direction):\n- Sensitivity matrix $S_3$ is equal to $S_1$.\n- Parameter covariance $C_{p,3}$:\n$$\nC_{p,3} = \\mathrm{diag}(1.0, 1.0, 1.0, 1.0, 1.0, 10^{-12})\n$$\n\nCase 4 (near-degeneracy: two parameter directions almost linearly dependent):\n- Sensitivity matrix $S_4$ obtained by perturbing column $6$ of $S_2$ slightly so that it is nearly, but not exactly, equal to column $5$. Concretely, let the perturbation be of order $10^{-10}$ in rows $5$ and $6$:\n$$\nS_4 = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 0 \\\\\n0.5 & 0.2 & 0.1 & 0.3 & 1.0 & 1.0 + 3 \\times 10^{-11} \\\\\n0.1 & 0.4 & 0.3 & 0.2 & 0.0 & -3 \\times 10^{-11}\n\\end{pmatrix}\n$$\n- Parameter covariance $C_{p,4}$:\n$$\nC_{p,4} = \\mathrm{diag}(1.0, 1.0, 1.0, 1.0, 1.0, 1.0)\n$$\n\nYour program must, for each case $i \\in \\{1,2,3,4\\}$, compute the degeneracy count $D_i$ using the PCA-based criterion with threshold $\\tau = 10^{-9}$ and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $\\texttt{[$d_1,d_2,d_3,d_4$]}$, where each $d_i$ is an integer equal to $D_i$ for case $i$. The program must be entirely self-contained, must not read any external input, and must compute results using the specified matrices and threshold exactly as given. Angles do not appear in this problem, and percentages must not be used anywhere; any ratio should be expressed as a decimal.",
            "solution": "The problem requires the design and implementation of a Principal Component Analysis (PCA)-based test to detect parameter degeneracies in a linearized theoretical model, deriving the method from first principles. The core of the task is to analyze the covariance structure of model observables that is induced by random variations in the model parameters.\n\nThe foundational elements are:\n1.  A linear response model: Small perturbations in model parameters, $\\delta p \\in \\mathbb{R}^d$, cause changes in model observables, $\\delta y \\in \\mathbb{R}^m$, according to the relationship $\\delta y = S \\delta p$. Here, $S \\in \\mathbb{R}^{m \\times d}$ is the sensitivity matrix, whose elements $S_{ij} = \\frac{\\partial y_i}{\\partial p_j}$ represent the partial derivative of the $i$-th observable with respect to the $j$-th parameter, evaluated at a reference point $p_0$.\n2.  Stochastic parameter variations: The parameter perturbations $\\delta p$ are treated as a random vector with a known covariance matrix $C_p \\in \\mathbb{R}^{d \\times d}$. The covariance matrix is defined as $C_p = E[(\\delta p - E[\\delta p]) (\\delta p - E[\\delta p])^T]$, where $E[\\cdot]$ denotes the expectation value. For perturbations around a central value, it is standard to assume a zero mean, $E[\\delta p] = 0$, in which case $C_p = E[\\delta p \\, \\delta p^T]$.\n\nThe first step is to derive the covariance matrix of the observables, $C_y$, from the given parameter covariance $C_p$ and the linear model. The covariance matrix of $\\delta y$ is defined as:\n$$C_y = E[(\\delta y - E[\\delta y]) (\\delta y - E[\\delta y])^T]$$\nGiven the linear model, the expectation of $\\delta y$ is $E[\\delta y] = E[S \\delta p] = S E[\\delta p]$. Substituting this into the definition of $C_y$:\n$$C_y = E[(S \\delta p - S E[\\delta p]) (S \\delta p - S E[\\delta p])^T]$$\nUsing the properties of the transpose and the linearity of the expectation operator, we can refactor the expression:\n$$C_y = E[S (\\delta p - E[\\delta p]) (\\delta p - E[\\delta p])^T S^T]$$\nSince the sensitivity matrix $S$ is constant, it can be factored out of the expectation:\n$$C_y = S E[(\\delta p - E[\\delta p]) (\\delta p - E[\\delta p])^T] S^T$$\nRecognizing the term inside the expectation as the definition of the parameter covariance matrix $C_p$, we arrive at the final expression:\n$$C_y = S C_p S^T$$\nThis $m \\times m$ matrix encapsulates the variance and correlation of the observables induced by the parameter variations. Since $C_p$ is a symmetric positive semidefinite matrix, $C_y$ is also symmetric positive semidefinite. This guarantees that its eigenvalues are real and non-negative.\n\nPrincipal Component Analysis (PCA) of the observables is equivalent to performing a spectral decomposition of their covariance matrix $C_y$. According to the spectral theorem, any real symmetric matrix like $C_y$ can be diagonalized by an orthogonal matrix:\n$$C_y = V \\Lambda V^T$$\nHere, $\\Lambda$ is a diagonal matrix whose entries $\\{\\lambda_i\\}_{i=1}^m$ are the eigenvalues of $C_y$. The columns of the orthogonal matrix $V$ are the corresponding orthonormal eigenvectors $\\{v_i\\}_{i=1}^m$. These eigenvectors represent the principal components: a new basis of orthogonal directions in the observable space. The eigenvalue $\\lambda_i$ is the variance of the observable data when projected onto the principal direction $v_i$.\n\nA parameter degeneracy arises when different combinations of parameter variations lead to nearly indistinguishable changes in the observables. This manifests as a direction in the observable space having near-zero variance. An eigenvalue $\\lambda_i$ of $C_y$ that is zero or close to zero indicates that the corresponding principal component $v_i$ has almost no variance. Such an occurrence is a clear signal of degeneracy, as it implies a constraint or redundancy in how parameter variations propagate to observable variations.\n\nThe problem provides a quantitative criterion for identifying these \"degenerate\" components. Given a threshold $\\tau > 0$, an eigenvalue $\\lambda_i$ is considered to signify a degeneracy if it is exceptionally small relative to the largest eigenvalue, $\\lambda_{\\max} = \\max_j \\lambda_j$. The condition is:\n$$\\lambda_i \\le \\tau \\lambda_{\\max}$$\nThe total number of degeneracies, $D$, is the count of eigenvalues that satisfy this inequality. For this problem, the threshold is given as $\\tau = 10^{-9}$.\n\nThe algorithm to be implemented for each test case is as follows:\n1.  Given the sensitivity matrix $S$ and the parameter covariance matrix $C_p$. In the provided cases, $m=d=6$.\n2.  Compute the observable covariance matrix using the derived formula: $C_y = S C_p S^T$.\n3.  Calculate the eigenvalues of the symmetric matrix $C_y$. Let these be $\\{\\lambda_i\\}_{i=1}^6$.\n4.  Determine the maximum eigenvalue, $\\lambda_{\\max} = \\max_i \\lambda_i$.\n5.  Count the number of eigenvalues $D$ that satisfy $\\lambda_i \\le 10^{-9} \\lambda_{\\max}$. This count is the final result for the test case.\n\nThis procedure will be applied to the four test cases, each designed to probe a different scenario:\n-   **Case 1**: A well-conditioned system where no degeneracies are expected ($D_1=0$).\n-   **Case 2**: A system with an exact linear dependence in the columns of the sensitivity matrix $S_2$. This introduces a rank deficiency, which should result in exactly one zero eigenvalue for $C_y$, leading to $D_2=1$.\n-   **Case 3**: A system where one parameter's variance is artificially suppressed to a very small value ($10^{-12}$). This \"freezing\" of a parameter direction is expected to induce a near-zero variance in the observables, resulting in one very small eigenvalue and $D_3=1$.\n-   **Case 4**: A system with a near-linear dependence in the columns of $S_4$. This is a perturbation of Case 2, designed to be full-rank but ill-conditioned. This should produce one very small, but non-zero, eigenvalue, which should still be small enough to be flagged as a degeneracy, yielding $D_4=1$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs PCA-based degeneracy detection on a set of test cases for a \n    linearized model calibration problem.\n    \"\"\"\n    \n    # Define the fixed threshold parameter tau\n    tau = 1e-9\n\n    # Case 1: Well-conditioned, no degeneracy expected\n    S1 = np.array([\n        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n        [0.5, 0.2, 0.1, 0.3, 1.0, 0.0],\n        [0.1, 0.4, 0.3, 0.2, 0.0, 1.0]\n    ])\n    Cp1 = np.diag([1.0, 0.8, 1.2, 0.9, 1.1, 1.0])\n    \n    # Case 2: Exact linear dependence in sensitivity matrix\n    S2 = np.array([\n        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n        [0.5, 0.2, 0.1, 0.3, 1.0, 1.0],\n        [0.1, 0.4, 0.3, 0.2, 0.0, 0.0]\n    ])\n    Cp2 = np.identity(6)\n    \n    # Case 3: Anisotropic parameter covariance, nearly frozen direction\n    S3 = S1\n    Cp3 = np.diag([1.0, 1.0, 1.0, 1.0, 1.0, 1e-12])\n    \n    # Case 4: Near-degeneracy in sensitivity matrix\n    S4 = np.array([\n        [1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.0, 0.0],\n        [0.5, 0.2, 0.1, 0.3, 1.0, 1.0 + 3e-11],\n        [0.1, 0.4, 0.3, 0.2, 0.0, -3e-11]\n    ])\n    Cp4 = np.identity(6)\n    \n    test_cases = [\n        (S1, Cp1),\n        (S2, Cp2),\n        (S3, Cp3),\n        (S4, Cp4)\n    ]\n    \n    results = []\n    \n    for S, Cp in test_cases:\n        # Step 1: Compute the observable covariance matrix Cy = S * Cp * S^T\n        Cy = S @ Cp @ S.T\n        \n        # Step 2: Compute the eigenvalues of the symmetric matrix Cy.\n        # Use eigvalsh for numerical stability and efficiency with symmetric matrices.\n        eigenvalues = np.linalg.eigvalsh(Cy)\n        \n        # Step 3: Find the maximum eigenvalue.\n        # Check against a small positive number to handle the zero matrix case.\n        lambda_max = np.max(eigenvalues) if eigenvalues.size > 0 else 0.0\n        \n        # Step 4: Count degeneracies based on the criterion.\n        # Ensure lambda_max is not zero to avoid division by zero or spurious results.\n        if lambda_max > 1e-30:  # A small tolerance for a non-zero max eigenvalue\n            degeneracy_count = np.sum(eigenvalues = tau * lambda_max)\n        else:\n            # If all eigenvalues are effectively zero, they are all degenerate.\n            degeneracy_count = len(eigenvalues)\n            \n        results.append(int(degeneracy_count))\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}