{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of Bayesian inference is the synthesis of prior knowledge with information from data. This practice provides a foundational, analytical exploration of this interplay by deriving how changes in the prior's width directly impact the posterior's location and uncertainty in a simple conjugate model . Mastering this derivation is key to developing a deep intuition for the mechanics of Bayesian updates.",
            "id": "3544172",
            "problem": "Consider the Bayesian calibration of a single dimensionless low-energy constant $\\,\\theta\\,$ appearing in a contact interaction of a nucleon-nucleon potential within Effective Field Theory (EFT). Suppose a single emulator-predicted observable, such as a binding-energy residual, is modeled as a linear response to $\\,\\theta\\,$ with additive Gaussian noise:\n$$\ny \\mid \\theta \\sim \\mathcal{N}(a\\,\\theta,\\ \\sigma^{2}),\n$$\nwhere $\\,y\\,$ is the observed residual, $\\,a\\,$ is a known sensitivity coefficient from the linearized emulator, and $\\,\\sigma^{2}\\,$ is the known variance of combined emulator and experimental uncertainties. The prior for $\\,\\theta\\,$ is Gaussian,\n$$\n\\theta \\sim \\mathcal{N}(\\mu_{0},\\ \\tau^{2}),\n$$\nwith known prior mean $\\,\\mu_{0}\\,$ and prior variance $\\,\\tau^{2}\\,$. All quantities $\\,\\theta,\\ \\mu_{0},\\ a,\\ y\\,$ are treated as dimensionless for this calibration.\n\nYou will compare two calibrations that differ only in the prior width: one uses prior variance $\\,\\tau_{1}^{2}\\,$ and the other uses $\\,\\tau_{2}^{2}\\,$, both sharing the same prior mean $\\,\\mu_{0}\\,$. Starting from Bayes’ theorem and the definitions of Gaussian likelihood and Gaussian prior, derive the posterior for each case and then compute:\n\n1. The exact shift in the posterior mean caused by changing the prior variance from $\\,\\tau_{1}^{2}\\,$ to $\\,\\tau_{2}^{2}\\,$:\n$$\n\\Delta \\mu_{\\mathrm{post}} \\equiv \\mu_{\\mathrm{post}}(\\tau_{2}^{2}) - \\mu_{\\mathrm{post}}(\\tau_{1}^{2}).\n$$\n\n2. The exact ratio of the posterior variances:\n$$\n\\rho \\equiv \\frac{\\tau_{\\mathrm{post}}^{2}(\\tau_{2}^{2})}{\\tau_{\\mathrm{post}}^{2}(\\tau_{1}^{2})}.\n$$\n\nExpress both $\\,\\Delta \\mu_{\\mathrm{post}}\\,$ and $\\,\\rho\\,$ as closed-form analytic functions of $\\,\\mu_{0},\\,a,\\,y,\\,\\sigma,\\,\\tau_{1}^{2},\\,\\tau_{2}^{2}\\,$. No numerical evaluation is required. Provide your final answer as a single row vector containing $\\,\\Delta \\mu_{\\mathrm{post}}\\,$ and $\\,\\rho\\,$ in that order. Since $\\,\\theta\\,$ and all relevant quantities are dimensionless in this setup, no physical units are needed. No rounding is required; present exact analytic expressions.",
            "solution": "The problem is well-posed and scientifically grounded, resting on the standard principles of Bayesian inference with conjugate priors. The task is to derive the change in the posterior mean and the ratio of posterior variances for a parameter $\\,\\theta\\,$ when its Gaussian prior variance is changed.\n\nFirst, we must derive the general form of the posterior distribution for $\\,\\theta\\,$. According to Bayes' theorem, the posterior probability density function (PDF) $p(\\theta \\mid y)$ is proportional to the product of the likelihood function $p(y \\mid \\theta)$ and the prior PDF $p(\\theta)$:\n$$\np(\\theta \\mid y) \\propto p(y \\mid \\theta) \\, p(\\theta)\n$$\nThe problem states that the likelihood is Gaussian:\n$$\np(y \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(y - a\\theta)^2}{2\\sigma^2} \\right)\n$$\nAnd the prior for $\\,\\theta\\,$ is also Gaussian:\n$$\np(\\theta) = \\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left( -\\frac{(\\theta - \\mu_0)^2}{2\\tau^2} \\right)\n$$\nMultiplying these two distributions, and dropping the constant normalization factors, we get the unnormalized posterior:\n$$\np(\\theta \\mid y) \\propto \\exp\\left( -\\frac{(y - a\\theta)^2}{2\\sigma^2} \\right) \\exp\\left( -\\frac{(\\theta - \\mu_0)^2}{2\\tau^2} \\right)\n$$\n$$\np(\\theta \\mid y) \\propto \\exp\\left( -\\frac{1}{2} \\left[ \\frac{(y - a\\theta)^2}{\\sigma^2} + \\frac{(\\theta - \\mu_0)^2}{\\tau^2} \\right] \\right)\n$$\nTo identify the form of the posterior distribution, we complete the square for $\\,\\theta\\,$ in the exponent. Let the term in the square brackets be $\\,Q(\\theta)\\,$:\n$$\nQ(\\theta) = \\frac{y^2 - 2ay\\theta + a^2\\theta^2}{\\sigma^2} + \\frac{\\theta^2 - 2\\mu_0\\theta + \\mu_0^2}{\\tau^2}\n$$\nWe collect terms involving $\\,\\theta^2\\,$ and $\\,\\theta\\,$:\n$$\nQ(\\theta) = \\theta^2 \\left( \\frac{a^2}{\\sigma^2} + \\frac{1}{\\tau^2} \\right) - 2\\theta \\left( \\frac{ay}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2} \\right) + \\mathrm{const.}\n$$\nwhere \"const.\" includes terms not dependent on $\\,\\theta\\,$. The posterior distribution for $\\,\\theta\\,$ is a Gaussian of the form $\\,\\mathcal{N}(\\mu_{\\mathrm{post}}, \\tau_{\\mathrm{post}}^2)\\,$, which has a PDF proportional to $\\,\\exp\\left( -\\frac{(\\theta - \\mu_{\\mathrm{post}})^2}{2\\tau_{\\mathrm{post}}^2} \\right)\\,$. The exponent can be written as:\n$$\n-\\frac{1}{2\\tau_{\\mathrm{post}}^2} (\\theta^2 - 2\\mu_{\\mathrm{post}}\\theta + \\mu_{\\mathrm{post}}^2) = -\\frac{1}{2} \\left[ \\theta^2 \\left(\\frac{1}{\\tau_{\\mathrm{post}}^2}\\right) - 2\\theta \\left(\\frac{\\mu_{\\mathrm{post}}}{\\tau_{\\mathrm{post}}^2}\\right) + \\mathrm{const.} \\right]\n$$\nBy comparing the coefficients of the $\\,\\theta^2\\,$ term in $\\,Q(\\theta)\\,$ and the standard Gaussian form, we identify the inverse of the posterior variance, which is the posterior precision:\n$$\n\\frac{1}{\\tau_{\\mathrm{post}}^2} = \\frac{a^2}{\\sigma^2} + \\frac{1}{\\tau^2} = \\frac{a^2\\tau^2 + \\sigma^2}{\\sigma^2\\tau^2}\n$$\nFrom this, the posterior variance $\\,\\tau_{\\mathrm{post}}^2\\,$ is:\n$$\n\\tau_{\\mathrm{post}}^2 = \\frac{\\sigma^2\\tau^2}{a^2\\tau^2 + \\sigma^2}\n$$\nBy comparing the coefficients of the $\\,\\theta\\,$ term, we find the posterior mean $\\,\\mu_{\\mathrm{post}}\\,$:\n$$\n\\frac{\\mu_{\\mathrm{post}}}{\\tau_{\\mathrm{post}}^2} = \\frac{ay}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2}\n$$\n$$\n\\mu_{\\mathrm{post}} = \\tau_{\\mathrm{post}}^2 \\left( \\frac{ay}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2} \\right) = \\left( \\frac{\\sigma^2\\tau^2}{a^2\\tau^2 + \\sigma^2} \\right) \\left( \\frac{ay\\tau^2 + \\mu_0\\sigma^2}{\\sigma^2\\tau^2} \\right)\n$$\n$$\n\\mu_{\\mathrm{post}} = \\frac{ay\\tau^2 + \\mu_0\\sigma^2}{a^2\\tau^2 + \\sigma^2}\n$$\nNow we apply these general results to the two specified cases.\n\nCase 1: Prior variance $\\,\\tau_1^2\\,$.\n$$\n\\mu_{\\mathrm{post}}(\\tau_1^2) = \\frac{ay\\tau_1^2 + \\mu_0\\sigma^2}{a^2\\tau_1^2 + \\sigma^2}\n$$\n$$\n\\tau_{\\mathrm{post}}^2(\\tau_1^2) = \\frac{\\sigma^2\\tau_1^2}{a^2\\tau_1^2 + \\sigma^2}\n$$\n\nCase 2: Prior variance $\\,\\tau_2^2\\,$.\n$$\n\\mu_{\\mathrm{post}}(\\tau_2^2) = \\frac{ay\\tau_2^2 + \\mu_0\\sigma^2}{a^2\\tau_2^2 + \\sigma^2}\n$$\n$$\n\\tau_{\\mathrm{post}}^2(\\tau_2^2) = \\frac{\\sigma^2\\tau_2^2}{a^2\\tau_2^2 + \\sigma^2}\n$$\n\n1.  Compute the shift in the posterior mean, $\\,\\Delta \\mu_{\\mathrm{post}} = \\mu_{\\mathrm{post}}(\\tau_2^2) - \\mu_{\\mathrm{post}}(\\tau_1^2)\\,$:\n$$\n\\Delta \\mu_{\\mathrm{post}} = \\frac{ay\\tau_2^2 + \\mu_0\\sigma^2}{a^2\\tau_2^2 + \\sigma^2} - \\frac{ay\\tau_1^2 + \\mu_0\\sigma^2}{a^2\\tau_1^2 + \\sigma^2}\n$$\nWe find a common denominator:\n$$\n\\Delta \\mu_{\\mathrm{post}} = \\frac{(ay\\tau_2^2 + \\mu_0\\sigma^2)(a^2\\tau_1^2 + \\sigma^2) - (ay\\tau_1^2 + \\mu_0\\sigma^2)(a^2\\tau_2^2 + \\sigma^2)}{(a^2\\tau_1^2 + \\sigma^2)(a^2\\tau_2^2 + \\sigma^2)}\n$$\nExpanding the numerator:\n$$\n(a^3y\\tau_1^2\\tau_2^2 + ay\\sigma^2\\tau_2^2 + a^2\\mu_0\\sigma^2\\tau_1^2 + \\mu_0\\sigma^4) - (a^3y\\tau_1^2\\tau_2^2 + ay\\sigma^2\\tau_1^2 + a^2\\mu_0\\sigma^2\\tau_2^2 + \\mu_0\\sigma^4)\n$$\nThe terms $\\,a^3y\\tau_1^2\\tau_2^2\\,$ and $\\,\\mu_0\\sigma^4\\,$ cancel. We are left with:\n$$\nay\\sigma^2\\tau_2^2 + a^2\\mu_0\\sigma^2\\tau_1^2 - ay\\sigma^2\\tau_1^2 - a^2\\mu_0\\sigma^2\\tau_2^2 = ay\\sigma^2(\\tau_2^2 - \\tau_1^2) - a^2\\mu_0\\sigma^2(\\tau_2^2 - \\tau_1^2)\n$$\nFactoring out common terms gives:\n$$\na\\sigma^2(y - a\\mu_0)(\\tau_2^2 - \\tau_1^2)\n$$\nThus, the shift in the posterior mean is:\n$$\n\\Delta \\mu_{\\mathrm{post}} = \\frac{a\\sigma^2(y - a\\mu_0)(\\tau_2^2 - \\tau_1^2)}{(a^2\\tau_1^2 + \\sigma^2)(a^2\\tau_2^2 + \\sigma^2)}\n$$\n\n2.  Compute the ratio of posterior variances, $\\,\\rho = \\frac{\\tau_{\\mathrm{post}}^2(\\tau_2^2)}{\\tau_{\\mathrm{post}}^2(\\tau_1^2)}\\,$:\n$$\n\\rho = \\frac{\\frac{\\sigma^2\\tau_2^2}{a^2\\tau_2^2 + \\sigma^2}}{\\frac{\\sigma^2\\tau_1^2}{a^2\\tau_1^2 + \\sigma^2}}\n$$\nThe $\\,\\sigma^2\\,$ term in the numerator of each fraction cancels:\n$$\n\\rho = \\frac{\\tau_2^2}{a^2\\tau_2^2 + \\sigma^2} \\cdot \\frac{a^2\\tau_1^2 + \\sigma^2}{\\tau_1^2}\n$$\nRearranging gives the final expression for the ratio:\n$$\n\\rho = \\frac{\\tau_2^2(a^2\\tau_1^2 + \\sigma^2)}{\\tau_1^2(a^2\\tau_2^2 + \\sigma^2)}\n$$\nThe two requested quantities, $\\,\\Delta \\mu_{\\mathrm{post}}\\,$ and $\\,\\rho\\,$, are now expressed as closed-form analytic functions of the given parameters. The final answer is presented as a row vector.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{a\\sigma^2(y - a\\mu_0)(\\tau_2^2 - \\tau_1^2)}{(a^2\\tau_1^2 + \\sigma^2)(a^2\\tau_2^2 + \\sigma^2)} & \\frac{\\tau_2^2 (a^2\\tau_1^2 + \\sigma^2)}{\\tau_1^2 (a^2\\tau_2^2 + \\sigma^2)} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Real-world calibrations rarely involve simple error models. This exercise translates the foundational principles of Bayesian updating into a concrete numerical problem involving multiple observables and a structured covariance matrix that includes both experimental and theoretical (truncation) uncertainties . Completing this practice will solidify your ability to set up and solve a realistic parameter estimation problem from start to finish.",
            "id": "3544112",
            "problem": "Consider a single low-energy constant (LEC) parameter, denoted by $\\theta$, that enters an $S$-wave toy-model prediction for two phase-shift observables at laboratory energies $E_{1} = 5\\,\\mathrm{MeV}$ and $E_{2} = 10\\,\\mathrm{MeV}$. Around a reference point, assume the model predictions are linearized as $y(\\theta) \\approx y_{0} + J\\,\\theta$, where $y_{0} \\in \\mathbb{R}^{2}$ is the baseline vector at $\\theta = 0$ and $J \\in \\mathbb{R}^{2}$ is the Jacobian (sensitivity) vector. You are given\n$$\ny_{0} = \\begin{pmatrix} 0.05 \\\\ 0.09 \\end{pmatrix}, \\qquad J = \\begin{pmatrix} 0.30 \\\\ 0.50 \\end{pmatrix},\n$$\nand the experimental data vector\n$$\nd = \\begin{pmatrix} 0.12 \\\\ 0.18 \\end{pmatrix},\n$$\nwhere all observable values are measured in radians. The experimental errors are Gaussian, independent between the two energies, with standard deviations $\\sigma_{e,1} = 0.02$ and $\\sigma_{e,2} = 0.025$, giving an experimental covariance\n$$\nS_{e} = \\begin{pmatrix} \\sigma_{e,1}^{2} & 0 \\\\ 0 & \\sigma_{e,2}^{2} \\end{pmatrix} = \\begin{pmatrix} 0.0004 & 0 \\\\ 0 & 0.000625 \\end{pmatrix}.\n$$\nTo account for truncation (model) uncertainty from omitted higher-order contributions, adopt an additive Gaussian discrepancy with covariance\n$$\nS_{t} = \\sigma_{t}^{2} \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix}, \\qquad \\sigma_{t} = 0.015, \\quad \\rho = 0.60.\n$$\nThus the total data-error covariance is $\\Sigma = S_{e} + S_{t}$. Place a Gaussian prior on the LEC,\n$$\n\\theta \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2}), \\qquad \\mu_{0} = 0.50, \\quad \\sigma_{0} = 0.10,\n$$\nwith $\\theta$ dimensionless. Under the linearized Gaussian model for the likelihood with total covariance $\\Sigma$, and using Bayes’ theorem, compute the posterior mean and posterior variance of $\\theta$ that consistently include the truncation covariance in $\\Sigma$. Report your final result as the ordered pair $(\\mu_{\\mathrm{post}}, \\sigma_{\\mathrm{post}}^{2})$, rounded to four significant figures. No units are required in the final numerical answer.",
            "solution": "The problem requires the computation of the posterior mean and variance for a parameter $\\theta$ within a Bayesian framework. The problem is well-posed and scientifically grounded in the context of parameter estimation for physical models.\n\nAccording to Bayes' theorem, the posterior probability density function (PDF) for the parameter $\\theta$, given the experimental data $d$, is proportional to the product of the likelihood function $p(d|\\theta)$ and the prior PDF $p(\\theta)$:\n$$\np(\\theta|d) \\propto p(d|\\theta) \\, p(\\theta)\n$$\n\nThe problem specifies the following distributions:\n1.  **Prior Distribution**: The prior belief about the parameter $\\theta$ is described by a Gaussian distribution with mean $\\mu_0 = 0.50$ and variance $\\sigma_0^2 = (0.10)^2 = 0.01$.\n    $$\n    p(\\theta) = \\mathcal{N}(\\theta | \\mu_0, \\sigma_0^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\sigma_0^2}\\right)\n    $$\n2.  **Likelihood Function**: The data $d$ are modeled as being drawn from a multivariate Gaussian distribution. The mean of this distribution is the model prediction, $y(\\theta) = y_0 + J\\theta$, and the covariance is the total covariance $\\Sigma$, which is the sum of the experimental covariance $S_e$ and the model truncation (discrepancy) covariance $S_t$.\n    $$\n    p(d|\\theta) = \\mathcal{N}(d | y(\\theta), \\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^k \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} (d - y(\\theta))^T \\Sigma^{-1} (d - y(\\theta))\\right)\n    $$\n    where $k=2$ is the dimension of the data vector.\n\nThe posterior PDF is therefore proportional to the product of these two exponential functions. The exponent of the posterior PDF, up to a constant, is the sum of the exponents of the prior and the likelihood:\n$$\n-\\frac{1}{2} \\left[ (d - (y_0 + J\\theta))^T \\Sigma^{-1} (d - (y_0 + J\\theta)) + \\frac{(\\theta - \\mu_0)^2}{\\sigma_0^2} \\right]\n$$\nLet the residual vector be $\\Delta = d - y_0$. The expression becomes:\n$$\n-\\frac{1}{2} \\left[ (\\Delta - J\\theta)^T \\Sigma^{-1} (\\Delta - J\\theta) + \\frac{(\\theta - \\mu_0)^2}{\\sigma_0^2} \\right]\n$$\nExpanding the terms involving $\\theta$:\n$$\n(\\Delta^T - \\theta J^T) \\Sigma^{-1} (\\Delta - J\\theta) + \\frac{(\\theta - \\mu_0)^2}{\\sigma_0^2} = \\mathrm{const} - 2\\theta J^T \\Sigma^{-1} \\Delta + \\theta^2 J^T \\Sigma^{-1} J + \\frac{\\theta^2}{\\sigma_0^2} - \\frac{2\\theta\\mu_0}{\\sigma_0^2}\n$$\nCollecting terms in powers of $\\theta$:\n$$\n\\theta^2 \\left( J^T \\Sigma^{-1} J + \\frac{1}{\\sigma_0^2} \\right) - 2\\theta \\left( J^T \\Sigma^{-1} \\Delta + \\frac{\\mu_0}{\\sigma_0^2} \\right) + \\mathrm{const}\n$$\nThis is a quadratic form in $\\theta$, which means the posterior distribution is also Gaussian, $p(\\theta|d) = \\mathcal{N}(\\theta | \\mu_{\\mathrm{post}}, \\sigma_{\\mathrm{post}}^2)$. The exponent of a Gaussian PDF is $-\\frac{(\\theta-\\mu_{\\mathrm{post}})^2}{2\\sigma_{\\mathrm{post}}^2} = -\\frac{1}{2\\sigma_{\\mathrm{post}}^2}(\\theta^2 - 2\\theta\\mu_{\\mathrm{post}} + \\mu_{\\mathrm{post}}^2)$.\n\nBy comparing the coefficients of $\\theta^2$ and $\\theta$, we can identify the posterior variance $\\sigma_{\\mathrm{post}}^2$ and mean $\\mu_{\\mathrm{post}}$:\n$$\n\\frac{1}{\\sigma_{\\mathrm{post}}^2} = J^T \\Sigma^{-1} J + \\frac{1}{\\sigma_0^2}\n$$\n$$\n\\frac{\\mu_{\\mathrm{post}}}{\\sigma_{\\mathrm{post}}^2} = J^T \\Sigma^{-1} \\Delta + \\frac{\\mu_0}{\\sigma_0^2} \\implies \\mu_{\\mathrm{post}} = \\sigma_{\\mathrm{post}}^2 \\left( J^T \\Sigma^{-1} (d - y_0) + \\frac{\\mu_0}{\\sigma_0^2} \\right)\n$$\nWe now proceed with the numerical calculation.\n\nFirst, calculate the truncation covariance matrix $S_t$:\nGiven $\\sigma_t = 0.015$ and $\\rho = 0.60$, the variance is $\\sigma_t^2 = 0.015^2 = 0.000225$.\n$$\nS_t = \\sigma_t^2 \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} = 0.000225 \\begin{pmatrix} 1 & 0.60 \\\\ 0.60 & 1 \\end{pmatrix} = \\begin{pmatrix} 0.000225 & 0.000135 \\\\ 0.000135 & 0.000225 \\end{pmatrix}\n$$\nNext, calculate the total covariance matrix $\\Sigma = S_e + S_t$:\n$$\nS_e = \\begin{pmatrix} 0.0004 & 0 \\\\ 0 & 0.000625 \\end{pmatrix}\n$$\n$$\n\\Sigma = \\begin{pmatrix} 0.0004 & 0 \\\\ 0 & 0.000625 \\end{pmatrix} + \\begin{pmatrix} 0.000225 & 0.000135 \\\\ 0.000135 & 0.000225 \\end{pmatrix} = \\begin{pmatrix} 0.000625 & 0.000135 \\\\ 0.000135 & 0.000850 \\end{pmatrix}\n$$\nTo find $\\Sigma^{-1}$, we first compute its determinant:\n$$\n\\det(\\Sigma) = (0.000625)(0.000850) - (0.000135)^2 = 5.3125 \\times 10^{-7} - 1.8225 \\times 10^{-8} = 5.13025 \\times 10^{-7}\n$$\nThe inverse is:\n$$\n\\Sigma^{-1} = \\frac{1}{\\det(\\Sigma)} \\begin{pmatrix} 0.000850 & -0.000135 \\\\ -0.000135 & 0.000625 \\end{pmatrix} \\approx \\begin{pmatrix} 1656.82 & -263.14 \\\\ -263.14 & 1218.26 \\end{pmatrix}\n$$\nNow, we compute the required matrix products. The Jacobian is $J = \\begin{pmatrix} 0.30 \\\\ 0.50 \\end{pmatrix}$.\nThe term $J^T \\Sigma^{-1} J$ is:\n$$\nJ^T \\Sigma^{-1} J = \\begin{pmatrix} 0.30 & 0.50 \\end{pmatrix} \\frac{1}{5.13025 \\times 10^{-7}} \\begin{pmatrix} 0.000850 & -0.000135 \\\\ -0.000135 & 0.000625 \\end{pmatrix} \\begin{pmatrix} 0.30 \\\\ 0.50 \\end{pmatrix}\n$$\nLet's first compute the numerator: $\\begin{pmatrix} 0.30 & 0.50 \\end{pmatrix} \\begin{pmatrix} 0.000850 & -0.000135 \\\\ -0.000135 & 0.000625 \\end{pmatrix} \\begin{pmatrix} 0.30 \\\\ 0.50 \\end{pmatrix}$\n$= 0.30^2(0.000850) - 2(0.30)(0.50)(0.000135) + 0.50^2(0.000625)$\n$= 0.09(0.000850) - 0.30(0.000135) + 0.25(0.000625)$\n$= 0.0000765 - 0.0000405 + 0.00015625 = 0.00019225$\n$$\nJ^T \\Sigma^{-1} J = \\frac{0.00019225}{5.13025 \\times 10^{-7}} \\approx 374.73516\n$$\nThe prior precision is $1/\\sigma_0^2 = 1/(0.10)^2 = 1/0.01 = 100$.\nThe posterior precision is:\n$$\n\\frac{1}{\\sigma_{\\mathrm{post}}^2} = 374.73516 + 100 = 474.73516\n$$\nThe posterior variance is:\n$$\n\\sigma_{\\mathrm{post}}^2 = \\frac{1}{474.73516} \\approx 0.00210643\n$$\nNext, we compute the term for the posterior mean. The residual vector is:\n$$\nd - y_0 = \\begin{pmatrix} 0.12 \\\\ 0.18 \\end{pmatrix} - \\begin{pmatrix} 0.05 \\\\ 0.09 \\end{pmatrix} = \\begin{pmatrix} 0.07 \\\\ 0.09 \\end{pmatrix}\n$$\nThe term $J^T \\Sigma^{-1} (d - y_0)$ is:\n$$\nJ^T \\Sigma^{-1} (d - y_0) = \\begin{pmatrix} 0.30 & 0.50 \\end{pmatrix} \\frac{1}{5.13025 \\times 10^{-7}} \\begin{pmatrix} 0.000850 & -0.000135 \\\\ -0.000135 & 0.000625 \\end{pmatrix} \\begin{pmatrix} 0.07 \\\\ 0.09 \\end{pmatrix}\n$$\nNumerator calculation: $\\begin{pmatrix} 0.30 & 0.50 \\end{pmatrix} \\begin{pmatrix} 0.000850(0.07) - 0.000135(0.09) \\\\ -0.000135(0.07) + 0.000625(0.09) \\end{pmatrix}$\n$= \\begin{pmatrix} 0.30 & 0.50 \\end{pmatrix} \\begin{pmatrix} 0.0000595 - 0.00001215 \\\\ -0.00000945 + 0.00005625 \\end{pmatrix} = \\begin{pmatrix} 0.30 & 0.50 \\end{pmatrix} \\begin{pmatrix} 0.00004735 \\\\ 0.0000468 \\end{pmatrix}$\n$= 0.30(0.00004735) + 0.50(0.0000468) = 0.000014205 + 0.0000234 = 0.000037605$\n$$\nJ^T \\Sigma^{-1} (d - y_0) = \\frac{0.000037605}{5.13025 \\times 10^{-7}} \\approx 73.30026\n$$\nThe prior contribution term is $\\mu_0/\\sigma_0^2 = 0.50/0.01 = 50$.\nNow, we calculate the posterior mean:\n$$\n\\mu_{\\mathrm{post}} = \\sigma_{\\mathrm{post}}^2 \\left( J^T \\Sigma^{-1} (d - y_0) + \\frac{\\mu_0}{\\sigma_0^2} \\right) \\approx 0.00210643 \\times (73.30026 + 50)\n$$\n$$\n\\mu_{\\mathrm{post}} \\approx 0.00210643 \\times 123.30026 \\approx 0.259737\n$$\nRounding the results to four significant figures, we get:\n$$\n\\mu_{\\mathrm{post}} \\approx 0.2597\n$$\n$$\n\\sigma_{\\mathrm{post}}^2 \\approx 0.002106\n$$\nThe final result is the ordered pair $(\\mu_{\\mathrm{post}}, \\sigma_{\\mathrm{post}}^2)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.2597 & 0.002106 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Parameter estimation is only part of the scientific process; we also need to compare competing theories. This advanced computational practice guides you through the implementation of a numerically robust algorithm to calculate the Bayes factor, the primary Bayesian tool for model selection . By calculating the marginal likelihood for different chiral EFT truncation schemes, you will learn how to let the data quantitatively judge the relative merit of different physical models.",
            "id": "3544169",
            "problem": "Consider a linearized surrogate for an observable in chiral Effective Field Theory (chiral EFT) calibration at truncation order $n$, where $m$ measurements collected in the vector $y \\in \\mathbb{R}^m$ are modeled as $y \\approx X \\theta + \\varepsilon_{\\mathrm{exp}} + \\varepsilon_{\\mathrm{tr}}$. Here, $X \\in \\mathbb{R}^{m \\times d}$ is the design matrix of sensitivities with respect to $d$ low-energy constants (LECs) $\\theta \\in \\mathbb{R}^d$. The experimental noise $\\varepsilon_{\\mathrm{exp}}$ is zero-mean Gaussian with diagonal covariance $\\Sigma_{\\mathrm{exp}} = \\mathrm{diag}(s_1^2,\\dots,s_m^2)$, and the truncation error $\\varepsilon_{\\mathrm{tr}}$ is modeled as a zero-mean Gaussian with order-dependent variance $\\sigma_n^2 I_m$, where $\\sigma_n^2 = c^2 Q^{2(n+1)}$, with $c>0$ a scheme-dependent scale and $Q \\in (0,1)$ an expansion parameter. Assume a Gaussian prior for the LECs, $\\theta \\sim \\mathcal{N}(0,\\tau^2 I_d)$ with $\\tau^2>0$. Under these assumptions, the marginal likelihood (Bayesian evidence) for a model $\\mathcal{M}$ that specifies $(n,Q,c)$ is given by integrating out $\\theta$ from the Gaussian likelihood and prior. The Bayes factor between two truncation schemes $\\mathcal{M}_A$ and $\\mathcal{M}_B$ is $B_{AB} = \\dfrac{p(y \\mid \\mathcal{M}_A)}{p(y \\mid \\mathcal{M}_B)}$. Use fundamental Bayesian principles (Bayes’ theorem, Gaussian integrals, and linear-Gaussian models) to derive an algorithm that, given $(y,X,\\{s_i^2\\}_{i=1}^m,\\tau^2)$ and the two scheme specifications $(n,Q_A,c_A)$ and $(n,Q_B,c_B)$, computes the natural logarithm of the Bayes factor, $\\ln B_{AB}$, exactly and robustly using linear algebra.\n\nYour task is to implement a program that computes $\\ln B_{AB}$ for each of the following test cases. All quantities are dimensionless, and no physical units are needed. You must use the following well-tested mathematical facts as the foundational base: (i) the distributional properties of multivariate Gaussian integrals, (ii) the rule for the sum of independent Gaussian random variables, (iii) the linear transformation of Gaussian variables, and (iv) the definition of the Bayes factor as the ratio of marginal likelihoods. Do not assume any shortcut formulas beyond those that follow from these principles.\n\nFor each test case, the model evidence must be constructed from the marginal $y \\sim \\mathcal{N}(0, C)$ with covariance $C = \\Sigma_{\\mathrm{exp}} + \\sigma_n^2 I_m + X (\\tau^2 I_d) X^\\top$, and the natural logarithm of the evidence must be evaluated using a numerically stable method based on the Cholesky factorization of $C$.\n\nInput specifications for the test suite:\n- Case $1$ (happy path): $m = 3$, $d = 2$, $y = [1.05, 0.95, 1.10]$, $X = \\begin{bmatrix}1.0 & 0.2 \\\\ 1.0 & -0.1 \\\\ 1.0 & 0.3\\end{bmatrix}$, experimental variances $\\{s_i^2\\} = [0.0025, 0.0025, 0.0025]$, $\\tau^2 = 1.0$, $n = 2$, $(Q_A,c_A) = (0.3, 1.0)$, $(Q_B,c_B) = (0.5, 1.0)$.\n- Case $2$ (low-$Q$ edge): $m = 2$, $d = 1$, $y = [0.02, -0.01]$, $X = \\begin{bmatrix}1.0 \\\\ 1.0\\end{bmatrix}$, experimental variances $\\{s_i^2\\} = [0.0001, 0.0001]$, $\\tau^2 = 0.1$, $n = 3$, $(Q_A,c_A) = (0.05, 1.0)$, $(Q_B,c_B) = (0.1, 1.0)$.\n- Case $3$ (high-uncertainty regime): $m = 4$, $d = 3$, $y = [0.5, 0.7, 0.6, 0.9]$, $X = \\begin{bmatrix}1.0 & 0.3 & -0.2 \\\\ 1.0 & -0.1 & 0.4 \\\\ 1.0 & 0.2 & 0.1 \\\\ 1.0 & -0.3 & 0.2\\end{bmatrix}$, experimental variances $\\{s_i^2\\} = [0.04, 0.04, 0.01, 0.09]$, $\\tau^2 = 0.5$, $n = 1$, $(Q_A,c_A) = (0.6, 1.0)$, $(Q_B,c_B) = (0.8, 1.0)$.\n- Case $4$ (different prefactors at fixed $Q$): $m = 3$, $d = 1$, $y = [2.0, 1.8, 2.2]$, $X = \\begin{bmatrix}1.0 \\\\ 0.9 \\\\ 1.1\\end{bmatrix}$, experimental variances $\\{s_i^2\\} = [0.25, 0.16, 0.36]$, $\\tau^2 = 2.0$, $n = 0$, $(Q_A,c_A) = (0.7, 1.0)$, $(Q_B,c_B) = (0.7, 2.0)$.\n\nProgram requirements:\n- Implement the computation of $\\sigma_n^2 = c^2 Q^{2(n+1)}$ for each scheme.\n- Construct $C = \\Sigma_{\\mathrm{exp}} + \\sigma_n^2 I_m + X (\\tau^2 I_d) X^\\top$ and evaluate $\\ln p(y \\mid \\mathcal{M})$ using a Cholesky factorization of $C$ in the numerically stable closed-form for a zero-mean Gaussian: $\\ln p(y \\mid \\mathcal{M}) = -\\tfrac{1}{2}\\left( m \\ln(2\\pi) + \\ln \\det C + y^\\top C^{-1} y \\right)$.\n- Return $\\ln B_{AB} = \\ln p(y \\mid \\mathcal{M}_A) - \\ln p(y \\mid \\mathcal{M}_B)$ for each case.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where each $r_i$ is the value of $\\ln B_{AB}$ for the $i$-th test case rounded to six decimal places.",
            "solution": "The problem requires the computation of the natural logarithm of the Bayes factor, $\\ln B_{AB}$, between two competing theoretical models, $\\mathcal{M}_A$ and $\\mathcal{M}_B$, for the calibration of a chiral Effective Field Theory (EFT). The solution must be derived from fundamental principles of Bayesian statistics and implemented using numerically robust linear algebra techniques.\n\nThe statistical model for the $m$ observables, collected in the vector $y \\in \\mathbb{R}^m$, is given by the linear relation:\n$$\ny \\approx X \\theta + \\varepsilon_{\\mathrm{exp}} + \\varepsilon_{\\mathrm{tr}}\n$$\nHere, $X \\in \\mathbb{R}^{m \\times d}$ is the design matrix representing the sensitivity of the observables to the $d$ low-energy constants (LECs) contained in the vector $\\theta \\in \\mathbb{R}^d$. The model components are defined by the following distributional assumptions:\n1.  The prior distribution for the LECs is a zero-mean Gaussian: $p(\\theta) = \\mathcal{N}(\\theta \\mid 0, \\tau^2 I_d)$, where $\\tau^2 > 0$ is the prior variance and $I_d$ is the $d \\times d$ identity matrix.\n2.  The experimental error, $\\varepsilon_{\\mathrm{exp}}$, is modeled as a zero-mean Gaussian with a diagonal covariance matrix: $\\varepsilon_{\\mathrm{exp}} \\sim \\mathcal{N}(0, \\Sigma_{\\mathrm{exp}})$, where $\\Sigma_{\\mathrm{exp}} = \\mathrm{diag}(s_1^2, \\dots, s_m^2)$.\n3.  The theory truncation error, $\\varepsilon_{\\mathrm{tr}}$, is modeled as a zero-mean Gaussian with a covariance matrix $\\sigma_n^2 I_m$, where $I_m$ is the $m \\times m$ identity matrix. The variance $\\sigma_n^2$ depends on the EFT truncation order $n$, an expansion parameter $Q \\in (0,1)$, and a scale parameter $c>0$, according to the formula $\\sigma_n^2 = c^2 Q^{2(n+1)}$.\n\nA specific model, denoted $\\mathcal{M}$, is defined by the triplet $(n, Q, c)$. The Bayes factor $B_{AB}$ is the ratio of the marginal likelihoods (or evidences) of two models, $\\mathcal{M}_A = (n, Q_A, c_A)$ and $\\mathcal{M}_B = (n, Q_B, c_B)$:\n$$\nB_{AB} = \\frac{p(y \\mid \\mathcal{M}_A)}{p(y \\mid \\mathcal{M}_B)}\n$$\nOur objective is to compute its natural logarithm, $\\ln B_{AB} = \\ln p(y \\mid \\mathcal{M}_A) - \\ln p(y \\mid \\mathcal{M}_B)$.\n\nThe derivation proceeds by first determining the marginal likelihood $p(y \\mid \\mathcal{M})$. This is accomplished by integrating out the parameter vector $\\theta$ from the joint distribution:\n$$\np(y \\mid \\mathcal{M}) = \\int p(y \\mid \\theta, \\mathcal{M}) p(\\theta \\mid \\mathcal{M}) \\, d\\theta\n$$\nGiven the model structure, the vector $y$ can be expressed as the sum of three independent random vectors: $X\\theta$, $\\varepsilon_{\\mathrm{exp}}$, and $\\varepsilon_{\\mathrm{tr}}$. According to the properties of Gaussian distributions, a linear transformation of a Gaussian variable is also Gaussian. Thus, the term $X\\theta$ is a zero-mean Gaussian random vector with mean $\\mathbb{E}[X\\theta] = X \\mathbb{E}[\\theta] = 0$ and covariance $\\mathrm{Cov}(X\\theta) = X \\mathrm{Cov}(\\theta) X^\\top = X(\\tau^2 I_d)X^\\top = \\tau^2 X X^\\top$.\n\nThe sum of independent Gaussian random vectors is also a Gaussian random vector. The mean of the sum is the sum of the means, and the covariance of the sum is the sum of the covariances. Therefore, the marginal distribution of $y$ is Gaussian, $y \\sim \\mathcal{N}(0, C)$, with a total covariance matrix $C$ given by:\n$$\nC = \\mathrm{Cov}(X\\theta) + \\mathrm{Cov}(\\varepsilon_{\\mathrm{exp}}) + \\mathrm{Cov}(\\varepsilon_{\\mathrm{tr}}) = \\tau^2 X X^\\top + \\Sigma_{\\mathrm{exp}} + \\sigma_n^2 I_m\n$$\nThis derivation validates the problem's stated marginal data distribution. The probability density function for this zero-mean multivariate Gaussian distribution is:\n$$\np(y \\mid \\mathcal{M}) = \\frac{1}{\\sqrt{(2\\pi)^m \\det(C)}} \\exp\\left(-\\frac{1}{2} y^\\top C^{-1} y\\right)\n$$\nTaking the natural logarithm yields the log-marginal likelihood:\n$$\n\\ln p(y \\mid \\mathcal{M}) = -\\frac{1}{2} \\left( m \\ln(2\\pi) + \\ln(\\det(C)) + y^\\top C^{-1} y \\right)\n$$\nTo compute this quantity in a numerically stable manner, we avoid direct computation of the inverse $C^{-1}$ and the determinant $\\det(C)$, which are prone to numerical underflow/overflow and inaccuracies. The covariance matrix $C$ is symmetric and positive definite, as it is a sum of positive semi-definite and positive definite matrices. It therefore admits a unique Cholesky factorization $C = L L^\\top$, where $L$ is a lower triangular matrix with positive diagonal entries.\n\nThis factorization allows for stable computation of the required terms:\n1.  The log-determinant term: $\\ln(\\det(C)) = \\ln(\\det(L L^\\top)) = \\ln(\\det(L)^2) = 2 \\ln(\\det(L))$. Since the determinant of a triangular matrix is the product of its diagonal elements, $\\det(L) = \\prod_{i=1}^m L_{ii}$, we have $\\ln(\\det(C)) = 2 \\sum_{i=1}^m \\ln(L_{ii})$. This approach prevents the numerical issues associated with computing the product of many small or large numbers.\n2.  The quadratic form term $y^\\top C^{-1} y$: We can write this as $y^\\top (L L^\\top)^{-1} y = y^\\top (L^\\top)^{-1} L^{-1} y$. Let $z = L^{-1} y$. The quadratic form becomes $z^\\top z = \\|z\\|_2^2$. The vector $z$ is found by solving the lower triangular system $L z = y$ for $z$ via forward substitution, a computationally efficient and stable operation.\n\nThe algorithm to compute the log-evidence for a single model $\\mathcal{M}$ is as follows:\n1.  Given model parameters $(n,Q,c)$, compute the truncation variance $\\sigma_n^2 = c^2 Q^{2(n+1)}$.\n2.  Construct the total covariance matrix $C = \\tau^2 X X^\\top + \\mathrm{diag}(\\{s_i^2\\}) + \\sigma_n^2 I_m$.\n3.  Compute the Cholesky factor $L$ such that $C = L L^\\top$.\n4.  Calculate the log-determinant as $2 \\sum_i \\ln(L_{ii})$.\n5.  Solve $L z = y$ for $z$ using forward substitution.\n6.  Calculate the quadratic form as $z^\\top z$.\n7.  Combine these terms to find $\\ln p(y \\mid \\mathcal{M})$.\n\nTo find the log-Bayes factor $\\ln B_{AB}$, this algorithm is executed for both model $\\mathcal{M}_A$ (with parameters $(Q_A, c_A)$) and model $\\mathcal{M}_B$ (with parameters $(Q_B, c_B)$), and the results are subtracted:\n$$\n\\ln B_{AB} = \\ln p(y \\mid \\mathcal{M}_A) - \\ln p(y \\mid \\mathcal{M}_B)\n$$\nThis procedure is implemented for each provided test case to yield the final results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases specified.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"y\": np.array([1.05, 0.95, 1.10]),\n            \"X\": np.array([[1.0, 0.2], [1.0, -0.1], [1.0, 0.3]]),\n            \"s_sq\": np.array([0.0025, 0.0025, 0.0025]),\n            \"tau_sq\": 1.0,\n            \"n\": 2,\n            \"model_A\": {\"Q\": 0.3, \"c\": 1.0},\n            \"model_B\": {\"Q\": 0.5, \"c\": 1.0},\n        },\n        # Case 2 (low-Q edge)\n        {\n            \"y\": np.array([0.02, -0.01]),\n            \"X\": np.array([[1.0], [1.0]]),\n            \"s_sq\": np.array([0.0001, 0.0001]),\n            \"tau_sq\": 0.1,\n            \"n\": 3,\n            \"model_A\": {\"Q\": 0.05, \"c\": 1.0},\n            \"model_B\": {\"Q\": 0.1, \"c\": 1.0},\n        },\n        # Case 3 (high-uncertainty regime)\n        {\n            \"y\": np.array([0.5, 0.7, 0.6, 0.9]),\n            \"X\": np.array([[1.0, 0.3, -0.2], [1.0, -0.1, 0.4], [1.0, 0.2, 0.1], [1.0, -0.3, 0.2]]),\n            \"s_sq\": np.array([0.04, 0.04, 0.01, 0.09]),\n            \"tau_sq\": 0.5,\n            \"n\": 1,\n            \"model_A\": {\"Q\": 0.6, \"c\": 1.0},\n            \"model_B\": {\"Q\": 0.8, \"c\": 1.0},\n        },\n        # Case 4 (different prefactors at fixed Q)\n        {\n            \"y\": np.array([2.0, 1.8, 2.2]),\n            \"X\": np.array([[1.0], [0.9], [1.1]]),\n            \"s_sq\": np.array([0.25, 0.16, 0.36]),\n            \"tau_sq\": 2.0,\n            \"n\": 0,\n            \"model_A\": {\"Q\": 0.7, \"c\": 1.0},\n            \"model_B\": {\"Q\": 0.7, \"c\": 2.0},\n        }\n    ]\n\n    def compute_log_evidence(y, X, s_sq, tau_sq, n, Q, c):\n        \"\"\"\n        Computes the log-marginal likelihood for a given model.\n        \n        Args:\n            y (np.ndarray): Measurement vector.\n            X (np.ndarray): Design matrix.\n            s_sq (np.ndarray): Vector of experimental variances.\n            tau_sq (float): Prior variance for LECs.\n            n (int): Truncation order.\n            Q (float): Expansion parameter.\n            c (float): Scheme-dependent scale.\n        \n        Returns:\n            float: The natural logarithm of the marginal likelihood.\n        \"\"\"\n        m = X.shape[0]\n\n        # 1. Calculate truncation error variance\n        sigma_n_sq = c**2 * Q**(2 * (n + 1))\n\n        # 2. Construct the total covariance matrix C\n        Sigma_exp = np.diag(s_sq)\n        C = tau_sq * (X @ X.T) + Sigma_exp + sigma_n_sq * np.identity(m)\n\n        # 3. Compute Cholesky factorization: C = L L^T\n        try:\n            L = linalg.cholesky(C, lower=True)\n        except linalg.LinAlgError:\n            # Fallback for numerically challenging cases, though not expected here.\n            # This would indicate the matrix is not positive definite.\n            return -np.inf\n\n        # 4. Calculate log-determinant: log(det(C)) = 2 * sum(log(diag(L)))\n        log_det_C = 2 * np.sum(np.log(np.diag(L)))\n\n        # 5. Solve L z = y for z using forward substitution\n        z = linalg.solve_triangular(L, y, lower=True)\n\n        # 6. Calculate quadratic form: y^T C^{-1} y = z^T z\n        quad_form = np.dot(z, z)\n\n        # 7. Combine terms for the log-evidence\n        log_p = -0.5 * (m * np.log(2 * np.pi) + log_det_C + quad_form)\n        \n        return log_p\n\n    results = []\n    for case in test_cases:\n        y = case[\"y\"]\n        X = case[\"X\"]\n        s_sq = case[\"s_sq\"]\n        tau_sq = case[\"tau_sq\"]\n        n = case[\"n\"]\n        \n        # Unpack model parameters\n        params_A = case[\"model_A\"]\n        params_B = case[\"model_B\"]\n\n        # Compute log-evidence for model A\n        log_p_A = compute_log_evidence(y, X, s_sq, tau_sq, n, params_A[\"Q\"], params_A[\"c\"])\n        \n        # Compute log-evidence for model B\n        log_p_B = compute_log_evidence(y, X, s_sq, tau_sq, n, params_B[\"Q\"], params_B[\"c\"])\n        \n        # Compute log Bayes factor\n        log_B_AB = log_p_A - log_p_B\n        \n        results.append(round(log_B_AB, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}