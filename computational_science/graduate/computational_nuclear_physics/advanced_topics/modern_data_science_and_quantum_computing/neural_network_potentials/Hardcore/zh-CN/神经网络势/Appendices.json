{
    "hands_on_practices": [
        {
            "introduction": "第一个练习旨在解决构建任何神经网络势 (NNP) 过程中的一个关键初始步骤：数据预处理。我们将探讨如何正确处理物理单位，如兆电子伏 ($\\,\\mathrm{MeV}$) 和飞米 ($\\,\\mathrm{fm}$)，以确保在从能量计算力时维度的自洽性。此外，该练习将指导您选择一种合适的归一化策略，以稳定训练过程，同时不扭曲其底层的物理原理。",
            "id": "3571829",
            "problem": "一个用于核子-核子相互作用的神经网络势 (NNP) 经过训练，将源自相对坐标 $\\mathbf{r} = \\mathbf{r}_1 - \\mathbf{r}_2$ 的结构描述符映射到标量势能 $E$。训练数据集提供的能量单位为兆电子伏 ($\\,\\mathrm{MeV}$)，位置单位为飞米 ($\\,\\mathrm{fm}$)。该模型也用于力匹配，其中力由基本关系 $\\mathbf{F} = -\\nabla_{\\mathbf{r}} E$ 定义。学习到的 NNP 在推理时应为 $E$ 和 $\\mathbf{F}$ 生成物理上一致的单位，并且其训练应通过对输入和输出进行适当的归一化来稳定，同时不扭曲物理原理或造成量纲不一致。\n\n假设存在距离和能量的特征核尺度，并且在训练期间可以获得小批量（mini-batch）统计数据。选择一个选项，该选项指定了一种编码和归一化策略，该策略 (i) 通过链式法则确保 $E$（单位为 $\\,\\mathrm{MeV}$）和 $\\mathbf{F}$（单位为 $\\,\\mathrm{MeV}/\\mathrm{fm}$）的量纲一致性，并且 (ii) 证明了一种归一化的合理性，该归一化通过控制特征和目标的尺度来稳定训练，同时在推理时保留物理可解释性。\n\nA. 使用固定的物理长度尺度 $r_0$ (例如，$r_0 \\approx 1\\,\\mathrm{fm}$) 定义无量纲输入 $\\tilde{\\mathbf{r}} = \\mathbf{r}/r_0$，并使用固定的能量尺度 $E_0$ (例如，$E_0 \\approx 100\\,\\mathrm{MeV}$) 定义无量纲能量 $\\tilde{E} = E/E_0$。训练 NNP 从标准化的无量纲特征 $\\hat{\\mathbf{r}} = (\\tilde{\\mathbf{r}} - \\boldsymbol{\\mu}_{\\tilde{\\mathbf{r}}})/\\boldsymbol{\\sigma}_{\\tilde{\\mathbf{r}}}$ 预测标准化的无量纲目标 $\\hat{E} = (\\tilde{E} - \\mu_{\\tilde{E}})/\\sigma_{\\tilde{E}}$。在推理时，反转标准化和缩放以恢复 $E = E_0 \\tilde{E}$。通过链式法则计算力 $\\mathbf{F} = -\\nabla_{\\mathbf{r}} E = -(E_0/r_0)\\,\\nabla_{\\tilde{\\mathbf{r}}} \\tilde{E}$ 以确保单位为 $\\,\\mathrm{MeV}/\\mathrm{fm}$。\n\nB. 使用 $\\hbar c$ 将位置从 $\\,\\mathrm{fm}$ 转换为 $\\,\\mathrm{MeV}^{-1}$，并将单位为 $\\,\\mathrm{MeV}^{-1}$ 的 $\\mathbf{r}$ 与单位为 $\\,\\mathrm{MeV}$ 的 $E$ 一起输入。对原始（未标准化的）输入和输出进行训练。计算力时，使用 $\\mathbf{F} = -\\nabla_{\\mathbf{r}} E$ 相对于转换后的坐标求梯度，但在梯度中不包含任何转换回 $\\,\\mathrm{fm}$ 的单位转换。\n\nC. 应用最小-最大归一化 (min–max normalization) 将整个数据集中的 $r$ 和 $E$ 映射到 $[0,1]$ 区间。在这些归一化单位下训练 NNP，并在推理时将预测的能量重新缩放回 $\\,\\mathrm{MeV}$。对于力匹配，计算 $\\mathbf{F}$ 时，取其相对于归一化坐标的梯度，而不显式考虑归一化坐标与 $\\,\\mathrm{fm}$ 之间的重新缩放因子。\n\nD. 选择 $r_0$ 和 $E_0$ 分别为 $r$ 和 $E$ 的数据集标准差，设置 $\\tilde{\\mathbf{r}} = \\mathbf{r}/r_0$ 和 $\\tilde{E} = E/E_0$，并且仅对 $\\tilde{\\mathbf{r}}$ 和 $\\tilde{E}$ 进行训练，不再进行进一步的标准化。在推理时，恢复 $E = E_0 \\tilde{E}$ 并通过 $\\mathbf{F} = -(E_0/r_0)\\,\\nabla_{\\tilde{\\mathbf{r}}} \\tilde{E}$ 计算力。论证使用数据集驱动的 $r_0$ 和 $E_0$足以保证训练稳定性和量纲一致性。\n\n哪个选项最能满足上述标准 (i) 和 (ii)？",
            "solution": "该问题要求评估用于训练核子-核子相互作用的神经网络势 (NNP) 的不同编码和归一化策略。关键标准是 (i) 保持能量 $E$ 和力 $\\mathbf{F}$ 的量纲一致性，以及 (ii) 采用一种能够稳定训练同时保留物理可解释性的归一化策略。\n\n首先，我们来验证问题陈述。\n\n### 步骤 1：提取已知条件\n- 该系统是一个由 NNP 建模的核子-核子相互作用。\n- NNP 将源自相对坐标 $\\mathbf{r} = \\mathbf{r}_1 - \\mathbf{r}_2$ 的结构描述符映射到标量势能 $E$。\n- 训练数据提供的能量 $E$ 单位为兆电子伏 ($\\,\\mathrm{MeV}$)。\n- 训练数据提供的坐标 $\\mathbf{r}$ 单位为飞米 ($\\,\\mathrm{fm}$)。\n- 力由基本关系 $\\mathbf{F} = -\\nabla_{\\mathbf{r}} E$ 定义。\n- NNP 在推理时必须为 $E$ ($\\,\\mathrm{MeV}$) 和 $\\mathbf{F}$ ($\\,\\mathrm{MeV}/\\mathrm{fm}$) 生成物理上一致的单位。\n- 必须通过对输入和输出进行适当的归一化来稳定训练。\n- 归一化不能扭曲物理原理或引入量纲不一致性。\n- 假设存在距离和能量的特征核尺度。\n- 假设在训练期间可以获得小批量（mini-batch）统计数据。\n- 任务是选择一个选项，该选项指定了一种编码和归一化策略，且能满足标准 (i) 和 (ii)。\n\n### 步骤 2：使用提取的已知条件进行验证\n- **科学依据：** 该问题牢固地植根于计算核物理和机器学习。NNP 是用于模拟粒子间相互作用的标准现代工具。关系式 $\\mathbf{F} = -\\nabla E$ 是经典力学的基石，并在此处得到正确应用。单位 ($\\,\\mathrm{MeV}$, $\\,\\mathrm{fm}$) 是该领域的标准单位。用于神经网络训练的输入/输出归一化概念是机器学习的基本实践。该问题在科学上是合理的。\n- **适定性：** 该问题提供了一个明确的目标：根据两个明确定义的标准（量纲一致性和有效、可解释的归一化）评估四种不同的策略。其结构旨在从选项中找到一个唯一最佳答案。\n- **客观性：** 语言技术性强且精确。评估标准是客观的，并基于物理学和机器学习的既定原则。\n\n### 步骤 3：结论与行动\n问题陈述是有效的。它在科学上合理、适定且客观。我将通过推导核心原则然后评估每个选项来继续解答。\n\n### 基于原则的推导\n构建一个物理启发的 NNP 的稳健策略涉及几个必须正确连接在一起的步骤。\n\n1.  **量纲分析和物理缩放**：物理量具有量纲：$[\\mathbf{r}] = \\text{长度}$ (给定为 $\\,\\mathrm{fm}$) 和 $[E] = \\text{能量}$ (给定为 $\\,\\mathrm{MeV}$)。力是势能的负梯度，$\\mathbf{F} = -\\nabla_{\\mathbf{r}} E$。梯度算子 $\\nabla_{\\mathbf{r}}$ 的单位是 $\\text{长度}^{-1}$。因此，力的单位必须是 $\\text{能量}/\\text{长度}$，具体为 $\\,\\mathrm{MeV}/\\mathrm{fm}$。任何有效的方案都必须遵守这一点。\n    一个常见的首要步骤是使用特征物理尺度 $r_0$ (例如，$r_0 = 1\\,\\mathrm{fm}$) 和 $E_0$ (例如，$E_0 = 100\\,\\mathrm{MeV}$) 对输入和输出进行无量纲化。\n    令 $\\tilde{\\mathbf{r}} = \\mathbf{r}/r_0$ 和 $\\tilde{E} = E/E_0$。$\\tilde{\\mathbf{r}}$ 和 $\\tilde{E}$ 都是无量纲的。那么物理势能为 $E = E_0 \\tilde{E}$。\n    力可以通过链式法则计算：\n    $$ \\mathbf{F} = -\\nabla_{\\mathbf{r}} E = -\\nabla_{\\mathbf{r}} (E_0 \\tilde{E}(\\tilde{\\mathbf{r}}(\\mathbf{r}))) $$\n    由于 $\\tilde{\\mathbf{r}} = \\mathbf{r}/r_0$，这个变换的雅可比矩阵是 $\\frac{\\partial \\tilde{\\mathbf{r}}}{\\partial \\mathbf{r}} = \\frac{1}{r_0} \\mathbf{I}$，其中 $\\mathbf{I}$ 是单位矩阵。因此，$\\nabla_{\\mathbf{r}} = \\frac{1}{r_0} \\nabla_{\\tilde{\\mathbf{r}}}$。\n    将其代入力方程得到：\n    $$ \\mathbf{F} = -E_0 \\left( \\frac{1}{r_0} \\nabla_{\\tilde{\\mathbf{r}}} \\right) \\tilde{E} = -\\frac{E_0}{r_0} \\nabla_{\\tilde{\\mathbf{r}}} \\tilde{E} $$\n    前置因子 $E_0/r_0$ 正确地携带了 $\\,\\mathrm{MeV}/\\mathrm{fm}$ 的单位。这个框架满足标准 (i)。\n\n2.  **为训练稳定性进行归一化**：当神经网络的输入和输出在数值上表现良好时（通常以 0 为中心，标准差约为 1），其训练效果最佳。无量纲量 $\\tilde{\\mathbf{r}}$ 和 $\\tilde{E}$ 可能不满足此条件。因此，需要第二个统计归一化步骤。标准化（或 Z-score 归一化）是实现此目的的稳健方法。\n    令 $\\hat{\\mathbf{r}}$ 和 $\\hat{E}$ 为网络实际看到和预测的标准化变量：\n    $$ \\hat{\\mathbf{r}} = \\frac{\\tilde{\\mathbf{r}} - \\boldsymbol{\\mu}_{\\tilde{\\mathbf{r}}}}{\\boldsymbol{\\sigma}_{\\tilde{\\mathbf{r}}}} \\quad \\text{和} \\quad \\hat{E} = \\frac{\\tilde{E} - \\mu_{\\tilde{E}}}{\\sigma_{\\tilde{E}}} $$\n    此处，$\\boldsymbol{\\mu}$ 和 $\\boldsymbol{\\sigma}$ 分别是各自分布的均值和标准差，通常在整个训练集上计算或从小批量数据中即时计算。NNP 学习映射关系 $\\hat{E} = \\mathcal{N}(\\hat{\\mathbf{r}})$。\n    为了在推理时恢复物理量，必须反转变换：\n    $\\tilde{E} = \\hat{E} \\sigma_{\\tilde{E}} + \\mu_{\\tilde{E}}$，然后 $E = E_0 \\tilde{E}$。\n    这个两步过程（物理缩放后进行统计标准化）提供了稳定的训练（标准 ii），同时允许一条清晰的路径回到物理量，从而保留了可解释性。\n\n现在，我们根据这个框架评估每个选项。\n\n### 逐项分析\n\n**A. 使用固定的物理长度尺度 $r_0$ (例如，$r_0 \\approx 1\\,\\mathrm{fm}$), 和定义无量纲能量 $\\tilde{E} = E/E_0$ 使用固定的能量尺度 $E_0$ (例如，$E_0 \\approx 100\\,\\mathrm{MeV}$)。训练 NNP 从标准化的无量纲特征 $\\hat{\\mathbf{r}} = (\\tilde{\\mathbf{r}} - \\boldsymbol{\\mu}_{\\tilde{\\mathbf{r}}})/\\boldsymbol{\\sigma}_{\\tilde{\\mathbf{r}}}$ 预测标准化的无量纲目标 $\\hat{E} = (\\tilde{E} - \\mu_{\\tilde{E}})/\\sigma_{\\tilde{E}}$。在推理时，反转标准化和缩放以恢复 $E = E_0 \\tilde{E}$。通过链式法则计算力 $\\mathbf{F} = -\\nabla_{\\mathbf{r}} E = -(E_0/r_0)\\,\\nabla_{\\tilde{\\mathbf{r}}} \\tilde{E}$ 以确保单位为 $\\,\\mathrm{MeV}/\\mathrm{fm}$。**\n\n该选项精确地遵循了上面概述的稳健的两步程序。\n-   **(i) 量纲一致性**: 它正确地使用物理尺度 $r_0$ 和 $E_0$ 来形成无量纲量。力的计算 $\\mathbf{F} = -(E_0/r_0)\\nabla_{\\tilde{\\mathbf{r}}} \\tilde{E}$ 正确应用了链式法则，确保最终的力具有 $E_0/r_0$ 的单位，即 $\\,\\mathrm{MeV}/\\mathrm{fm}$。梯度 $\\nabla_{\\tilde{\\mathbf{r}}} \\tilde{E}$ 本身是通过对 NNP 输出 $\\hat{E} = \\mathcal{N}(\\hat{\\mathbf{r}})$ 应用链式法则穿过标准化层来计算的，这在用于训练的自动微分框架中是标准做法。\n-   **(ii) 训练稳定性和可解释性**: 它提议对无量纲、物理缩放后的输入和输出进行标准化。这是稳定神经网络训练的一种标准且高效的技术。使用固定的物理尺度 $r_0$ 和 $E_0$ 为无量纲化提供了清晰、可解释的物理基础，这与数据集的统计属性是分开的。这种分离使模型更稳健且更易于解释。\n\n**结论：** **正确**。此选项描述了一种物理上合理、量纲一致且遵循稳定机器学习最佳实践的方法。\n\n**B. 使用 $\\hbar c$ 将位置从 $\\,\\mathrm{fm}$ 转换为 $\\,\\mathrm{MeV}^{-1}$，并将单位为 $\\,\\mathrm{MeV}^{-1}$ 的 $\\mathbf{r}$ 与单位为 $\\,\\mathrm{MeV}$ 的 $E$ 一起输入。对原始（未标准化的）输入和输出进行训练。计算力时，使用 $\\mathbf{F} = -\\nabla_{\\mathbf{r}} E$ 相对于转换后的坐标求梯度，但在梯度中不包含任何转换回 $\\,\\mathrm{fm}$ 的单位转换。**\n\n-   **(i) 量纲一致性**: 令转换后的坐标为 $\\mathbf{r}' = \\mathbf{r}/(\\hbar c)$，其单位为 $\\,\\mathrm{MeV}^{-1}$。相对于此坐标的梯度 $\\nabla_{\\mathbf{r}'} E$ 的单位是 $\\,\\mathrm{MeV}/\\mathrm{MeV}^{-1} = \\mathrm{MeV}^2$。这不是力的单位。该选项明确说明不包括获得正确单位所需的转换因子 $1/(\\hbar c)$。因此，这个过程违反了量纲一致性。\n-   **(ii) 训练稳定性**: 它建议在原始、未标准化的输入和输出上进行训练。众所周知，这是次优的，可能导致训练不稳定，尤其是在处理核物理中遇到的宽范围能量和坐标时。\n\n**结论：** **不正确**。该方法在力的计算上量纲不一致，并使用了较差的训练策略。\n\n**C. 应用最小-最大归一化 (min–max normalization) 将整个数据集中的 $r$ 和 $E$ 映射到 $[0,1]$ 区间。在这些归一化单位下训练 NNP，并在推理时将预测的能量重新缩放回 $\\,\\mathrm{MeV}$。对于力匹配，计算 $\\mathbf{F}$ 时，取其相对于归一化坐标的梯度，而不显式考虑归一化坐标与 $\\,\\mathrm{fm}$ 之间的重新缩放因子。**\n\n-   **(i) 量纲一致性**: 令 $r_{norm} = (r - r_{min})/(r_{max} - r_{min})$ 和 $E_{norm} = (E - E_{min})/(E_{max} - E_{min})$。网络学习 $E_{norm}(r_{norm})$。物理力涉及因子 $\\nabla_{\\mathbf{r}} E \\propto \\frac{E_{max} - E_{min}}{r_{max} - r_{min}} \\nabla_{r_{norm}} E_{norm}$。该选项声明在计算力时不使用此重新缩放因子。项 $\\nabla_{r_{norm}} E_{norm}$ 是一个无量纲量对另一个无量纲量的梯度，因此它本身是无量纲的。这不可能是物理力。该方法量纲不一致。\n-   **(ii) 训练稳定性**: 最小-最大归一化确实可以缩放特征，但它对异常值敏感，这可能将大部分数据压缩到 $[0, 1]$ 内的一个非常小的范围内。标准化（如选项 A 所示）通常更稳健。此外，其尺度纯粹由数据驱动，缺乏使用像 $r_0$ 和 $E_0$ 这样的固定尺度所具有的清晰物理可解释性。\n\n**结论：** **不正确**。该方法在力计算的量纲一致性上失败。\n\n**D. 选择 $r_0$ 和 $E_0$ 分别为 $r$ 和 $E$ 的数据集标准差，设置 $\\tilde{\\mathbf{r}} = \\mathbf{r}/r_0$ 和 $\\tilde{E} = E/E_0$，并且仅对 $\\tilde{\\mathbf{r}}$ 和 $\\tilde{E}$ 进行训练，不再进行进一步的标准化。在推理时，恢复 $E = E_0 \\tilde{E}$ 并通过 $\\mathbf{F} = -(E_0/r_0)\\,\\nabla_{\\tilde{\\mathbf{r}}} \\tilde{E}$ 计算力。论证使用数据集驱动的 $r_0$ 和 $E_0$ 足以保证训练稳定性和量纲一致性。**\n\n-   **(i) 量纲一致性**: 力的计算公式 $\\mathbf{F} = -(E_0/r_0)\\nabla_{\\tilde{\\mathbf{r}}} \\tilde{E}$ 在量纲上是正确的，因为 $E_0 = \\sigma_E$ 的单位是 $\\,\\mathrm{MeV}$，而 $r_0 = \\sigma_r$ 的单位是 $\\,\\mathrm{fm}$。此标准得到满足。\n-   **(ii) 训练稳定性**: 此方法是一种缩放形式，但不是完全的标准化。它通过标准差缩放数据，但没有减去均值。对于许多 NNP 激活函数（如 $\\tanh$），以 0 为中心的输入对于有效训练至关重要。通过省略均值减法，与选项 A 中提出的完全标准化相比，此方法在训练稳定性方面是次优的。认为这“足够”的论点很薄弱；它不是*最佳*或最稳健的策略。选项 A 描述了一个严格更优的归一化程序。\n\n**结论：** **不正确**。虽然量纲一致，但此选项提出的归一化策略是次优的，其在稳定训练方面的效果和稳健性不如选项 A 中描述的策略。\n\n### 结论\n选项 A 描述了最完整和正确的方法论。它通过基于物理缩放正确应用链式法则，恰当地确保了量纲一致性。同时，它采用了一个稳健的两层归一化方案（物理缩放后进行统计标准化），该方案非常适合稳定神经网络训练，同时保持与底层物理学的清晰联系。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "在掌握了数据处理之后，本练习将深入探讨 NNP 的架构本身，重点关注如何编码基本的物理对称性。您将学习如何通过将势分解为同位旋标量和同位旋矢量分量来构建其结构，从而使其尊重核力的一个关键性质——同位旋对称性。这种方法不仅提升了模型的物理真实性，还能更有效地利用来自不同核子-核子散射道的数据。",
            "id": "3571865",
            "problem": "考虑一个为核子-核子相互作用设计的神经网络势 (NNP)，其旨在遵循近似的同位旋对称性。设算符基包含一个同位旋标量分量和一个与 Pauli 同位旋标积成正比的同位旋矢量分量，因此在给定的运动学构型（例如，相对距离、相对动量、自旋算符期望值）下，势可以写为\n$$\nV = V_0(\\mathbf{x}) + V_1(\\mathbf{x})\\,\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2,\n$$\n其中 $\\boldsymbol{\\tau}_i$ 是作用于核子 $i$ 的 Pauli 同位旋算符，$\\mathbf{x}$ 集合了非同位旋输入。假设训练数据来自选定分波中的质子-质子 (pp)、中子-中子 (nn) 和中子-质子 (np) 道，这些分波的总同位旋 $T$ 从量子数中已知。忽略电磁效应，但在 pp 道中有一个显式的库仑贡献除外，该贡献被视为一个已知的可加项。假设近似电荷无关性成立，因此在同一同位旋道中的 pp 和 nn 除了库仑作用和微小的电荷依赖修正外，共享相同的强相互作用。\n\n基于同位旋代数以及到单位算符和 $\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2$ 上的投影，以下哪个选项给出了一对正确的投影公式，用于从同位旋单态 ($T=0$) 和同位旋三重态 ($T=1$) 道的矩阵元中提取 $V_0$ 和 $V_1$，并同时提供一个科学上合理的训练目标和数据使用策略，该策略使用 pp、nn 和 np 数据在单个 NNP 中拟合这些分量？\n\nA. 使用恒等式 $\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2 = 2\\,\\boldsymbol{T}^2 - 3$，其中 $\\boldsymbol{T}=(\\boldsymbol{\\tau}_1+\\boldsymbol{\\tau}_2)/2$ 是总同位旋算符。那么 $\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2$ 在 $T=0$ 时的本征值为 $-3$，在 $T=1$ 时的本征值为 $+1$。因此，\n$$\nV^{(T=0)} = V_0 - 3\\,V_1,\\qquad V^{(T=1)} = V_0 + V_1,\n$$\n投影给出\n$$\nV_1 = \\frac{V^{(T=1)} - V^{(T=0)}}{4},\\qquad V_0 = \\frac{3\\,V^{(T=1)} + V^{(T=0)}}{4}.\n$$\n训练一个输出 $(V_0,V_1)$ 的单一 NNP，并定义一个复合损失函数，该函数通过同位旋分类将导出的道势与数据进行比较：对于 $T=1$ 的 pp 和 nn 分波，使用 $\\widehat{V}_{pp/nn} = \\widehat{V}_0 + \\widehat{V}_1$，对 pp 还需加上一个已知的库仑项；对于 $T=0$ 的 np 分波（例如 ${}^3S_1$–${}^3D_1$），使用 $\\widehat{V}_{np}^{(T=0)}=\\widehat{V}_0 - 3\\,\\widehat{V}_1$；对于 $T=1$ 的 np 分波（例如 ${}^1S_0$），使用 $\\widehat{V}_{np}^{(T=1)}=\\widehat{V}_0 + \\widehat{V}_1$。最小化所有道和运动学构型上残差平方的加权和，添加一个小的正则化项以在移除库仑作用后强制 pp–nn 的一致性，并包含实验或模拟器不确定性作为权重。\n\nB. 认为 $\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2$ 在双核子系统中的本征值为 $\\pm 1$，因此 $V^{(T=0)} = V_0 - V_1$ 和 $V^{(T=1)} = V_0 + V_1$，这导致 $V_1 = \\left(V^{(T=1)} - V^{(T=0)}\\right)/2$ 和 $V_0 = \\left(V^{(T=1)} + V^{(T=0)}\\right)/2$。仅在 pp 和 nn 数据上训练以学习 $(V_0,V_1)$，因为在同位旋对称性下，np 不增加独立的同位旋信息。\n\nC. 通过直接平均进行投影：定义 $V_0 = \\left(V_{pp} + V_{nn} + V_{np}\\right)/3$ 和 $V_1 = \\left(V_{pp} - V_{nn}\\right)/2$，假设电荷无关性并忽略库仑作用。通过最小化网络输出的这些线性组合与相应数据平均值之间的平方差来进行训练，而不按分波同位旋来分离 np 数据。\n\nD. 将 $\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2$ 替换为 $\\tau_{1z}\\tau_{2z}$，并确定 $V^{(pp)} = V_0 + V_1$，$V^{(nn)} = V_0 + V_1$，以及 $V^{(np)} = V_0 - V_1$。然后设 $V_1 = \\left(V^{(pp)} - V^{(np)}\\right)$ 和 $V_0 = V^{(np)} + V_1$。通过用其同位旋第三分量 $T_z$ 标记每对数据来训练 NNP，并在所有数据上对这些公式进行回归，而不进行分波分辨。\n\n选择正确的选项。",
            "solution": "用户需要对一个关于构建遵循同位旋对称性的核子-核子相互作用的神经网络势 (NNP) 的问题进行详细的验证和解答。\n\n### 问题验证\n\n#### 步骤 1：提取已知条件\n-   **系统**：一个用于核子-核子 (NN) 相互作用的神经网络势 (NNP)。\n-   **对称性**：NNP 被设计为遵循近似的同位旋对称性。\n-   **势的形式**：势表示为 $V = V_0(\\mathbf{x}) + V_1(\\mathbf{x})\\,\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2$。\n-   **定义**：$\\boldsymbol{\\tau}_i$ 是作用于核子 $i$ 的 Pauli 同位旋算符。$\\mathbf{x}$ 代表非同位旋输入的集合（例如，运动学量）。\n-   **数据来源**：训练数据来自质子-质子 (pp)、中子-中子 (nn) 和中子-质子 (np) 道。\n-   **数据结构**：数据是为总同位旋 $T$ 已知的特定分波提供的。\n-   **假设**：\n    1.  忽略电磁效应，但在 pp 道中有一个显式的、已知的可加库仑项除外。\n    2.  近似电荷无关性成立，意味着 pp 和 nn 系统在给定的同位旋道中具有相同的强相互作用（在考虑库仑效应之后）。\n\n#### 步骤 2：使用提取的已知条件进行验证\n-   **科学依据**：该问题牢固地植根于核物理原理。通过算符 $\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2$ 将 NN 相互作用参数化为同位旋标量 ($V_0$) 和同位旋矢量 ($V_1$) 分量是一种标准且物理上合理的表示方法，它遵循同位旋对称性。同位旋是强相互作用的一个基本（近似）对称性，其形式体系，包括总同位旋 $\\boldsymbol{T}$ 和 Pauli 算符 $\\boldsymbol{\\tau}$，是核结构和散射理论的核心。通过总同位旋 $T$ 对 NN 态 (pp, nn, np) 进行分类也是标准做法。问题陈述在科学上是合理的。\n-   **适定性**：该问题是适定的。它要求基于所提供的势形式推导 $V_0$ 和 $V_1$ 的投影公式，并提出一个科学上合理的训练策略。给定的信息足以推导出这些公式并评估所提出的策略。可以确定一个唯一且有意义的解。\n-   **客观性**：语言精确、专业，没有任何主观性或歧义。像“同位旋对称性”、“Pauli 同位旋算符”和“分波”这样的术语在物理学中有严格的定义。\n\n问题陈述没有违反任何无效标准。这是一个在计算核物理领域中有效且表述良好的问题。\n\n#### 步骤 3：结论和行动\n问题有效。我将继续推导解决方案并评估各个选项。\n\n### 解答推导\n\n核子-核子相互作用的势给出如下：\n$$\nV = V_0(\\mathbf{x}) + V_1(\\mathbf{x})\\,\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2\n$$\n为了确定具有确定总同位旋 $T$ 的道中的势，我们必须找到算符 $\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2$ 的本征值。\n\n双核子系统的总同位旋算符是 $\\boldsymbol{T} = \\frac{1}{2}(\\boldsymbol{\\tau}_1 + \\boldsymbol{\\tau}_2)$。它的平方是：\n$$\n\\boldsymbol{T}^2 = \\left(\\frac{1}{2}(\\boldsymbol{\\tau}_1 + \\boldsymbol{\\tau}_2)\\right) \\cdot \\left(\\frac{1}{2}(\\boldsymbol{\\tau}_1 + \\boldsymbol{\\tau}_2)\\right) = \\frac{1}{4}(\\boldsymbol{\\tau}_1^2 + \\boldsymbol{\\tau}_2^2 + 2\\,\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2)\n$$\nPauli 同位旋算符 $\\boldsymbol{\\tau}_i$ 类似于 Pauli 自旋矩阵。对于单个核子（同位旋为 $1/2$ 的粒子），Pauli 矢量算符的平方是 $\\boldsymbol{\\tau}_i^2 = \\tau_{ix}^2 + \\tau_{iy}^2 + \\tau_{iz}^2 = 3I$，其中 $I$ 是单核子同位旋空间中的单位算符。其本征值为 $3$。\n因此，算符 $\\boldsymbol{T}^2$ 变为：\n$$\n\\boldsymbol{T}^2 = \\frac{1}{4}(3 + 3 + 2\\,\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2) = \\frac{1}{2}(3 + \\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2)\n$$\n重新排列此方程以求解 $\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2$ 给出：\n$$\n\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2 = 2\\,\\boldsymbol{T}^2 - 3\n$$\n算符 $\\boldsymbol{T}^2$ 的本征值为 $T(T+1)$，其中 $T$ 是总同位旋量子数。对于双核子系统，可能的值是 $T=0$（同位旋单态）和 $T=1$（同位旋三重态）。\n\n1.  **同位旋单态道 ($T=0$):**\n    $\\boldsymbol{T}^2$ 的本征值为 $0(0+1) = 0$。\n    $\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2$ 的本征值为 $2(0) - 3 = -3$。\n    此道中的势 $V^{(T=0)}$ 为：\n    $$\n    V^{(T=0)} = V_0 - 3\\,V_1\n    $$\n\n2.  **同位旋三重态道 ($T=1$):**\n    $\\boldsymbol{T}^2$ 的本征值为 $1(1+1) = 2$。\n    $\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2$ 的本征值为 $2(2) - 3 = +1$。\n    此道中的势 $V^{(T=1)}$ 为：\n    $$\n    V^{(T=1)} = V_0 + V_1\n    $$\n\n我们现在得到了一个关于 $V_0$ 和 $V_1$ 的二元线性方程组：\n$$\n\\begin{align*}\nV^{(T=1)} = V_0 + V_1 \\quad (1) \\\\\nV^{(T=0)} = V_0 - 3\\,V_1 \\quad (2)\n\\end{align*}\n$$\n为了求 $V_1$，我们将方程 $(1)$ 减去方程 $(2)$：\n$$\nV^{(T=1)} - V^{(T=0)} = (V_0 + V_1) - (V_0 - 3\\,V_1) = 4\\,V_1\n$$\n$$\n\\implies V_1 = \\frac{V^{(T=1)} - V^{(T=0)}}{4}\n$$\n为了求 $V_0$，我们可以将 $V_1$ 代回方程 $(1)$：\n$$\nV_0 = V^{(T=1)} - V_1 = V^{(T=1)} - \\frac{V^{(T=1)} - V^{(T=0)}}{4} = \\frac{4\\,V^{(T=1)} - V^{(T=1)} + V^{(T=0)}}{4}\n$$\n$$\n\\implies V_0 = \\frac{3\\,V^{(T=1)} + V^{(T=0)}}{4}\n$$\n这些是从确定同位旋道中的势中提取同位旋标量分量 $V_0$ 和同位旋矢量分量 $V_1$ 的投影公式。\n\n### 选项评估\n\n**选项 A:**\n该选项陈述了恒等式 $\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2 = 2\\,\\boldsymbol{T}^2 - 3$，这是正确的。它正确地计算了 $\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2$ 在 $T=0$ 道的本征值为 $-3$，在 $T=1$ 道的本征值为 $+1$。这导致道势为 $V^{(T=0)} = V_0 - 3\\,V_1$ 和 $V^{(T=1)} = V_0 + V_1$，这与我们的推导相符。由此得到的投影公式 $V_1 = \\frac{V^{(T=1)} - V^{(T=0)}}{4}$ 和 $V_0 = \\frac{3\\,V^{(T=1)} + V^{(T=0)}}{4}$ 也是正确的。\n\n所提出的训练策略在科学上是合理的。它正确地指出了：\n-   pp 和 nn 散射数据仅探测 $T=1$ 道。\n-   np 散射数据必须被分离到 $T=0$ 和 $T=1$ 道中，这可以通过分波分析实现（例如，${}^3S_1$ 是 $T=0$，${}^1S_0$ 是 $T=1$）。\n-   一个单一的 NNP 应该输出基本分量 $(V_0, V_1)$。\n-   应该构建一个复合损失函数，通过将 NNP 输出根据每个数据点道的同位旋进行组合（$T=1$ 用 $\\widehat{V}_0 + \\widehat{V}_1$，$T=0$ 用 $\\widehat{V}_0 - 3\\,\\widehat{V}_1$），并与相应的实验数据进行比较。\n-   它正确地提到了为 pp 数据添加已知的库仑贡献。\n-   使用残差平方的加权和（权重来自不确定性）以及正则化的建议是机器学习中的标准和良好实践。\n\n选项 A 的全部陈述与核物理原理和稳健的机器学习方法论相一致。\n**结论：正确。**\n\n**选项 B:**\n该选项错误地陈述了 $\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2$ 的本征值为 $\\pm 1$。根据推导，本征值是 $-3$ 和 $+1$。这个根本性错误导致了不正确的投影公式。此外，它声称 np 数据不增加独立的同位旋信息，并且训练可以仅在 pp 和 nn 数据上进行。这是一个严重的误解。由于 pp 和 nn 系统都纯粹是 $T=1$ 的，它们只能约束组合 $V_0 + V_1$。如果没有来自 $T=0$ 道的数据，就不可能分离 $V_0$ 和 $V_1$，而 $T=0$ 道的数据完全由 np 散射提供。因此，所提出的训练策略存在根本性缺陷。\n**结论：错误。**\n\n**选项 C:**\n该选项提出了临时的平均公式，$V_0 = (V_{pp} + V_{nn} + V_{np})/3$ 和 $V_1 = (V_{pp} - V_{nn})/2$。这些公式并非从第一性原理推导得出。$V_1$ 的公式将意味着整个同位旋矢量势都源于 pp 和 nn 相互作用之间的电荷对称性破缺差异，这是不正确的；$V_1$ 是核力的一个主要组成部分，而电荷对称性破缺是一个小效应。$V_0$ 的公式是无意义的，因为它在没有区分 $T=0$ 和 $T=1$ 分量的情况下对 $V_{np}$ 进行了平均。关于“不按分波同位旋来分离 np 数据”进行训练的建议是一个关键缺陷，因为这种分离对于约束势至关重要。\n**结论：错误。**\n\n**选项 D:**\n该选项建议将同位旋标量算符 $\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2$ 替换为 $\\tau_{1z}\\tau_{2z}$。这是一个根本性错误。算符 $\\boldsymbol{\\tau}_1\\cdot\\boldsymbol{\\tau}_2$ 是同位旋空间中的一个旋转标量，使用它可以确保势遵循同位旋对称性（即相互作用与同位旋空间中的取向无关）。然而，算符 $\\tau_{1z}\\tau_{2z}$ 不是一个标量；它是一个二阶张量的分量。使用它会明确地破坏同位旋对称性，与问题的前提相矛盾。后续的公式和训练策略都基于这个有缺陷的前提，即相互作用依赖于同位旋投影 $T_z$ 而不是总同位旋 $T$，这对于标量相互作用来说在物理上是不正确的。\n**结论：错误。**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "一个强大的模型只有在计算上是可行的时候才具有实用价值。最后一个练习将我们的重点从模型设计转向性能分析，这是任何计算物理学家都必须掌握的一项关键技能。您将分析和比较两种嵌入了物理求解器的不同训练流程的计算复杂度，推导出它们的成本如何随网格大小、通道数和网络参数等因素扩展。",
            "id": "3571843",
            "problem": "您正在一个批处理优化的工作流程中，为耦合通道的核子-核子散射训练一个神经网络势。每个批次包含 $B$ 个独立的训练样本。对于每个样本，以及对于 $S$ 个固定能量中的每一个，您通过将一个物理求解器嵌入计算图中，并使用反向模式自动微分 (AD) 来获取关于神经网络参数的梯度，从而评估损失。\n\n考虑了两种备选的计算流程。\n\n流程 $\\mathrm{Schr}$ (坐标空间薛定谔方程)：您使用二阶有限差分格式，在每个通道有 $N$ 个点的均匀径向网格上，对耦合的径向薛定谔方程进行离散化。有 $C$ 个耦合通道，因此离散哈密顿量是一个维度为 $C N$ 的块三对角矩阵，其块为稠密的 $C \\times C$ 矩阵。您通过一个带状 $LU$ 分解来求解得到的线性系统，该分解在不同的右端项之间重复使用。反向模式 AD 对每个右端项需要一次伴随求解。\n\n流程 $\\mathrm{LS}$ (动量空间 Lippmann–Schwinger 方程)：您在每个通道有 $N$ 个点的均匀动量网格上对 Lippmann–Schwinger 方程进行离散化。有 $C$ 个耦合通道。您使用 Krylov 子空间方法（例如，广义最小残差方法 (GMRES) 或双共轭梯度稳定方法 (BiCGSTAB)）为每个右端项求解离散积分方程，每次求解需要 $I$ 次迭代。每次 Krylov 迭代都使用通过快速傅里叶变换 (FFT) 实现的卷积来应用积分算子，并包括稠密通道混合。反向模式 AD 需要与前向 Krylov 求解同阶的算子应用次数。\n\n在两种流程中，势由一个具有 $P$ 个可训练参数的全连接前馈神经网络表示，该网络将局部坐标映射到每个网格点上完整的 $C \\times C$ 通道耦合势。假设每个网格点的前向评估成本与 $P$ 成线性关系，并且对于标量损失，每个网格点的反向模式 AD 成本也与 $P$ 成线性关系。\n\n采用以下成本模型，其中依赖于算法的前置因子被归集为正常数，并且在保持这些前置因子显式的情况下，只保留 $N$ 和 $C$ 的主导阶项：\n- 每个网格点的神经网络评估：前向成本 $k_{\\mathrm{f}} P$ 和后向成本 $k_{\\mathrm{b}} P$。\n- 块三对角矩阵的坐标空间带状 $LU$ 分解：每个样本成本 $k_{\\mathrm{lu}} N C^{3}$ (分解完成一次并重复使用)。\n- 使用带状因子对每个右端项进行前向或伴随三角求解：成本 $k_{\\mathrm{sv}} N C^{2}$。\n- 在每通道 $N$ 个点的网格上，动量空间 Lippmann–Schwinger 算子的一次应用（包括基于 FFT 的卷积和通道混合）：成本 $k_{\\mathrm{mv}} C N \\ln N$。\n\n假设：\n- 在流程 $\\mathrm{Schr}$ 中，您对每个样本执行 $S$ 次前向求解和 $S$ 次伴随求解，并重复使用单次 $LU$ 分解。\n- 在流程 $\\mathrm{LS}$ 中，您对每个样本执行 $S$ 次前向 Krylov 求解和 $S$ 次伴随 Krylov 求解，每次求解需要 $I$ 次迭代，每次迭代需要一次算子应用，成本为 $k_{\\mathrm{mv}} C N \\ln N$。\n- 在两种流程中，对于前向传播和后向传播，神经网络在每个样本的每个网格点上各评估一次，总共有 $N$ 个网格点。两种流程使用相同的 $N$。\n\n仅从上述假设和关于有限差分方法离散化、带状线性代数、基于 FFT 的卷积和 Krylov 方法的标准事实出发，推导出两种流程每个批次的主导阶浮点运算次数，表示为关于 $B$、$C$、$N$、$P$、$S$、$I$ 以及常数 $k_{\\mathrm{f}}$、$k_{\\mathrm{b}}$、$k_{\\mathrm{lu}}$、$k_{\\mathrm{sv}}$、$k_{\\mathrm{mv}}$ 的符号表达式 $T_{\\mathrm{Schr}}(B,C,N,P,S)$ 和 $T_{\\mathrm{LS}}(B,C,N,P,S,I)$。\n\n答案规格：\n- 使用 $\\mathrm{pmatrix}$ 环境，将您的最终表达式对 $\\left(T_{\\mathrm{Schr}},\\,T_{\\mathrm{LS}}\\right)$ 报告为单个行矩阵。\n- 答案必须是单个闭式解析表达式，不含单位且不含不等式。",
            "solution": "用户要求推导两种计算流程（分别指定为 $\\mathrm{Schr}$ 和 $\\mathrm{LS}$）每个批次的主导阶浮点运算次数。每个流程的总成本是一个批次中的样本数量 $B$ 与每个样本的总计算成本的乘积。我们将通过将问题陈述中定义的其组成部分的成本相加来推导每个样本的成本。\n\n首先，我们确定两种流程共有的成本组成部分。问题陈述指出，对于两种流程，神经网络势在前向传播和后向传播中，对每个样本的每个网格点各评估一次。网格包含 $N$ 个点。\n每个网格点的前向评估成本为 $k_{\\mathrm{f}} P$。因此，单个样本在所有 $N$ 个网格点上神经网络 (NN) 的总前向评估成本为 $N \\cdot (k_{\\mathrm{f}} P) = k_{\\mathrm{f}} N P$。\n反向模式自动微分 (AD) 的每个网格点的后向评估成本为 $k_{\\mathrm{b}} P$。单个样本在所有 $N$ 个网格点上 NN 的总后向评估成本为 $N \\cdot (k_{\\mathrm{b}} P) = k_{\\mathrm{b}} N P$。\n因此，每个样本与神经网络评估相关的总成本（我们表示为 $T_{\\mathrm{NN}}$）是前向和后向传播成本的总和：\n$$T_{\\mathrm{NN}} = k_{\\mathrm{f}} N P + k_{\\mathrm{b}} N P = (k_{\\mathrm{f}} + k_{\\mathrm{b}}) N P$$\n对于两种流程，批次中的每个样本都会产生此成本。\n\n接下来，我们推导流程 $\\mathrm{Schr}$ 的成本，表示为 $T_{\\mathrm{Schr}}$。\n每个批次的总成本是 $T_{\\mathrm{Schr}} = B \\cdot T_{\\mathrm{Schr, sample}}$，其中 $T_{\\mathrm{Schr, sample}}$ 是每个样本的成本。\n每个样本的成本是 NN 成本和求解器成本的总和。\n$$T_{\\mathrm{Schr, sample}} = T_{\\mathrm{NN}} + T_{\\mathrm{Solver, Schr}}$$\n流程 $\\mathrm{Schr}$ 的求解器成本对每个样本包括三个部分：\n1.  **$LU$ 分解：** 对块三对角哈密顿矩阵执行一次带状 $LU$ 分解。成本为 $k_{\\mathrm{lu}} N C^{3}$。\n2.  **前向求解：** 对 $S$ 个不同的固定能量求解问题。这需要使用预先计算的 $LU$ 因子进行 $S$ 次前向求解。单次前向求解的成本为 $k_{\\mathrm{sv}} N C^{2}$。所有 $S$ 次前向求解的总成本为 $S \\cdot (k_{\\mathrm{sv}} N C^{2})$。\n3.  **伴随求解：** 对于反向模式 AD，每个右端项需要一次伴随求解，这对应于 $S$ 次前向求解中的每一次。使用带状因子进行一次伴随求解的成本与一次前向求解相同，为 $k_{\\mathrm{sv}} N C^{2}$。$S$ 次伴随求解的总成本为 $S \\cdot (k_{\\mathrm{sv}} N C^{2})$。\n流程 $\\mathrm{Schr}$ 每个样本的总求解器成本是这些部分的总和：\n$$T_{\\mathrm{Solver, Schr}} = k_{\\mathrm{lu}} N C^{3} + S k_{\\mathrm{sv}} N C^{2} + S k_{\\mathrm{sv}} N C^{2} = k_{\\mathrm{lu}} N C^{3} + 2 S k_{\\mathrm{sv}} N C^{2}$$\n结合 NN 和求解器成本，流程 $\\mathrm{Schr}$ 每个样本的总成本为：\n$$T_{\\mathrm{Schr, sample}} = (k_{\\mathrm{f}} + k_{\\mathrm{b}}) N P + k_{\\mathrm{lu}} N C^{3} + 2 S k_{\\mathrm{sv}} N C^{2}$$\n最后，通过乘以批次大小 $B$ 得到每个批次的总成本：\n$$T_{\\mathrm{Schr}}(B,C,N,P,S) = B \\left( (k_{\\mathrm{f}} + k_{\\mathrm{b}}) N P + k_{\\mathrm{lu}} N C^{3} + 2 S k_{\\mathrm{sv}} N C^{2} \\right)$$\n\n现在，我们推导流程 $\\mathrm{LS}$ 的成本，表示为 $T_{\\mathrm{LS}}$。\n每个批次的总成本是 $T_{\\mathrm{LS}} = B \\cdot T_{\\mathrm{LS, sample}}$。\n每个样本的成本是此流程的 NN 成本和求解器成本的总和。\n$$T_{\\mathrm{LS, sample}} = T_{\\mathrm{NN}} + T_{\\mathrm{Solver, LS}}$$\n流程 $\\mathrm{LS}$ 的求解器成本由用于 $S$ 个能量的 Krylov 子空间方法确定。\n1.  **前向求解：** 对于 $S$ 个能量中的每一个，执行一次前向 Krylov 求解。每次求解需要 $I$ 次迭代。每次迭代需要一次 Lippmann–Schwinger 算子应用，成本为 $k_{\\mathrm{mv}} C N \\ln N$。每个样本所有 $S$ 次前向求解的总成本为 $S \\cdot I \\cdot (k_{\\mathrm{mv}} C N \\ln N)$。\n2.  **伴随求解：** 问题规定，反向模式 AD 需要 $S$ 次伴随 Krylov 求解，每次也需要 $I$ 次迭代。每次迭代的成本与前向传播相同。每个样本所有 $S$ 次伴随求解的总成本为 $S \\cdot I \\cdot (k_{\\mathrm{mv}} C N \\ln N)$。\n流程 $\\mathrm{LS}$ 每个样本的总求解器成本是前向和伴随求解成本的总和：\n$$T_{\\mathrm{Solver, LS}} = S I k_{\\mathrm{mv}} C N \\ln N + S I k_{\\mathrm{mv}} C N \\ln N = 2 S I k_{\\mathrm{mv}} C N \\ln N$$\n结合 NN 和求解器成本，流程 $\\mathrm{LS}$ 每个样本的总成本为：\n$$T_{\\mathrm{LS, sample}} = (k_{\\mathrm{f}} + k_{\\mathrm{b}}) N P + 2 S I k_{\\mathrm{mv}} C N \\ln N$$\n那么，流程 $\\mathrm{LS}$ 每个批次的总成本为：\n$$T_{\\mathrm{LS}}(B,C,N,P,S,I) = B \\left( (k_{\\mathrm{f}} + k_{\\mathrm{b}}) N P + 2 S I k_{\\mathrm{mv}} C N \\ln N \\right)$$\n这两个关于 $T_{\\mathrm{Schr}}$ 和 $T_{\\mathrm{LS}}$ 的表达式代表了最终推导出的运算次数。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} B \\left( \\left( k_{\\mathrm{f}} + k_{\\mathrm{b}} \\right) N P + k_{\\mathrm{lu}} N C^{3} + 2 S k_{\\mathrm{sv}} N C^{2} \\right)  B \\left( \\left( k_{\\mathrm{f}} + k_{\\mathrm{b}} \\right) N P + 2 S I k_{\\mathrm{mv}} C N \\ln N \\right) \\end{pmatrix}}\n$$"
        }
    ]
}