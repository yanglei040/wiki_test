{
    "hands_on_practices": [
        {
            "introduction": "The time evolution of isotopic abundances in s-process nucleosynthesis is governed by a large system of coupled ordinary differential equations. These systems are notoriously \"stiff,\" meaning they involve processes occurring on vastly different timescales, from rapid neutron captures to slow beta decays. This stiffness renders standard explicit time-integration methods computationally prohibitive. This practice introduces the foundational technique for handling such systems: the implicit backward Euler method, coupled with a Newton-Raphson solver to handle the resulting algebraic equations. By working through the implementation for a simple two-species network , you will build the core numerical engine essential for any serious reaction network code.",
            "id": "3591040",
            "problem": "Consider a minimal reaction network representative of a segment of the slow neutron-capture process (s-process) in nuclear astrophysics, consisting of two species with a one-way flow: $Y_1 \\xrightarrow{\\lambda_{n\\gamma}} Y_2 \\xrightarrow{\\lambda_{\\beta}} \\emptyset$. Here $Y_1$ denotes the molar abundance per baryon of the seed isotope, $Y_2$ denotes the molar abundance per baryon of the neutron-capture product, $\\lambda_{n\\gamma}$ denotes the effective neutron-capture rate in $\\mathrm{s}^{-1}$, and $\\lambda_{\\beta}$ denotes the beta-decay rate in $\\mathrm{s}^{-1}$. Assume both rates are constant over the timestep. The governing Ordinary Differential Equations (ODEs) for the abundances are\n$$\n\\frac{dY_1}{dt} = -\\lambda_{n\\gamma} Y_1,\\quad\n\\frac{dY_2}{dt} = \\lambda_{n\\gamma} Y_1 - \\lambda_{\\beta} Y_2.\n$$\nLet the timestep be $\\Delta t$ in seconds, and denote by $Y_1^n$ and $Y_2^n$ the abundances at time $t^n$, with $Y_1^{n+1}$ and $Y_2^{n+1}$ the unknowns at time $t^{n+1} = t^n + \\Delta t$. Using the backward Euler method, which for a general ODE $d\\mathbf{Y}/dt = \\mathbf{f}(\\mathbf{Y})$ takes the form $\\mathbf{Y}^{n+1} = \\mathbf{Y}^{n} + \\Delta t\\, \\mathbf{f}(\\mathbf{Y}^{n+1})$, pose the implicit nonlinear system for $Y_1^{n+1}$ and $Y_2^{n+1}$ as a residual function $\\mathbf{R}(\\mathbf{U}) = \\mathbf{0}$, where $\\mathbf{U} = [Y_1^{n+1}, Y_2^{n+1}]^\\top$. Derive the residual vector $\\mathbf{R}(\\mathbf{U})$ and its Jacobian matrix $\\mathbf{J}(\\mathbf{U}) = \\partial \\mathbf{R}/\\partial \\mathbf{U}$ for this two-species chain. Design a Newton-Raphson (NR) iteration to solve for $\\mathbf{U}$, and define a mathematically precise convergence criterion based on both the residual norm and the relative step size. Implement a single backward Euler step with Newton-Raphson, and apply it to the test suite below.\n\nScientific and numerical requirements:\n- Start from the reaction-rate ODEs above. Use backward Euler discretization in time and construct the residual $\\mathbf{R}(\\mathbf{U})$. Compute the Jacobian analytically.\n- Use a Newton-Raphson solver: for iteration index $k$, update $\\mathbf{U}^{(k+1)} = \\mathbf{U}^{(k)} + \\Delta \\mathbf{U}^{(k)}$ where $\\mathbf{J}(\\mathbf{U}^{(k)})\\, \\Delta \\mathbf{U}^{(k)} = -\\mathbf{R}(\\mathbf{U}^{(k)})$.\n- Convergence criterion: terminate the Newton-Raphson iterations when both $||\\mathbf{R}(\\mathbf{U}^{(k)})||_2 \\le \\varepsilon_{\\mathrm{res}}$ and $||\\Delta \\mathbf{U}^{(k)}||_2 / \\max\\{||\\mathbf{U}^{(k)}||_2, 1\\} \\le \\varepsilon_{\\mathrm{step}}$, with tolerances $\\varepsilon_{\\mathrm{res}} = 10^{-12}$ and $\\varepsilon_{\\mathrm{step}} = 10^{-12}$.\n- If $\\Delta t = 0$, define the step to return $\\mathbf{U} = [Y_1^{n}, Y_2^{n}]^\\top$.\n- Units: $\\lambda_{n\\gamma}$ and $\\lambda_{\\beta}$ are in $\\mathrm{s}^{-1}$, $\\Delta t$ is in $\\mathrm{s}$, and abundances $Y_1$ and $Y_2$ are dimensionless. Express the output abundances as dimensionless floating-point numbers.\n\nTest suite:\n- Case $1$: $Y_1^n = 0.7$, $Y_2^n = 0.3$, $\\lambda_{n\\gamma} = 10^{-7}\\ \\mathrm{s}^{-1}$, $\\lambda_{\\beta} = 5\\times 10^{-8}\\ \\mathrm{s}^{-1}$, $\\Delta t = 10^{5}\\ \\mathrm{s}$.\n- Case $2$: $Y_1^n = 1.0$, $Y_2^n = 0.0$, $\\lambda_{n\\gamma} = 10^{-5}\\ \\mathrm{s}^{-1}$, $\\lambda_{\\beta} = 10^{-5}\\ \\mathrm{s}^{-1}$, $\\Delta t = 10^{6}\\ \\mathrm{s}$.\n- Case $3$ (boundary): $Y_1^n = 0.5$, $Y_2^n = 0.5$, $\\lambda_{n\\gamma} = 10^{-3}\\ \\mathrm{s}^{-1}$, $\\lambda_{\\beta} = 10^{-3}\\ \\mathrm{s}^{-1}$, $\\Delta t = 0\\ \\mathrm{s}$.\n- Case $4$ (edge, no beta decay): $Y_1^n = 0.8$, $Y_2^n = 0.2$, $\\lambda_{n\\gamma} = 2\\times 10^{-6}\\ \\mathrm{s}^{-1}$, $\\lambda_{\\beta} = 0\\ \\mathrm{s}^{-1}$, $\\Delta t = 10^{6}\\ \\mathrm{s}$.\n- Case $5$ (edge, no capture): $Y_1^n = 0.6$, $Y_2^n = 0.4$, $\\lambda_{n\\gamma} = 0\\ \\mathrm{s}^{-1}$, $\\lambda_{\\beta} = 10^{-7}\\ \\mathrm{s}^{-1}$, $\\Delta t = 10^{7}\\ \\mathrm{s}$.\n\nYour program must:\n- Implement the backward Euler step with Newton-Raphson using the derived residual and Jacobian.\n- Apply it to the five cases in the test suite and collect the outputs as lists $[Y_1^{n+1}, Y_2^{n+1}]$ for each case.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a two-element list of floats in the order $[Y_1^{n+1}, Y_2^{n+1}]$. For example, a valid output with two cases would look like $[[a,b],[c,d]]$.",
            "solution": "The objective is to compute a single time step for a system of two coupled ordinary differential equations (ODEs) using the backward Euler method. The resulting implicit algebraic system is to be solved using a Newton-Raphson (NR) iterative scheme.\n\nThe governing ODE system for the abundances $Y_1(t)$ and $Y_2(t)$ is given by:\n$$\n\\frac{dY_1}{dt} = -\\lambda_{n\\gamma} Y_1\n$$\n$$\n\\frac{dY_2}{dt} = \\lambda_{n\\gamma} Y_1 - \\lambda_{\\beta} Y_2\n$$\nThis can be written in vector form as $\\frac{d\\mathbf{Y}}{dt} = \\mathbf{f}(\\mathbf{Y})$, where $\\mathbf{Y} = [Y_1, Y_2]^\\top$ and the function $\\mathbf{f}(\\mathbf{Y})$ is:\n$$\n\\mathbf{f}(\\mathbf{Y}) = \\begin{bmatrix} -\\lambda_{n\\gamma} Y_1 \\\\ \\lambda_{n\\gamma} Y_1 - \\lambda_{\\beta} Y_2 \\end{bmatrix}\n$$\nThe backward Euler method discretizes this ODE system as:\n$$\n\\mathbf{Y}^{n+1} = \\mathbf{Y}^{n} + \\Delta t\\, \\mathbf{f}(\\mathbf{Y}^{n+1})\n$$\nwhere $\\mathbf{Y}^n = [Y_1^n, Y_2^n]^\\top$ represents the known abundances at time $t^n$, and $\\mathbf{Y}^{n+1} = [Y_1^{n+1}, Y_2^{n+1}]^\\top$ represents the unknown abundances at time $t^{n+1} = t^n + \\Delta t$.\n\nTo solve for $\\mathbf{Y}^{n+1}$, we frame the problem as finding the root of a residual function $\\mathbf{R}(\\mathbf{U}) = \\mathbf{0}$, where $\\mathbf{U} = \\mathbf{Y}^{n+1}$. The residual is defined by rearranging the backward Euler equation:\n$$\n\\mathbf{R}(\\mathbf{U}) = \\mathbf{U} - \\mathbf{Y}^n - \\Delta t\\, \\mathbf{f}(\\mathbf{U}) = \\mathbf{0}\n$$\nSubstituting the specific form of $\\mathbf{f}(\\mathbf{U})$, the components of the residual vector $\\mathbf{R}(\\mathbf{U}) = [R_1(\\mathbf{U}), R_2(\\mathbf{U})]^\\top$ are:\n$$\nR_1(U_1, U_2) = U_1 - Y_1^n - \\Delta t (-\\lambda_{n\\gamma} U_1) = (1 + \\Delta t \\lambda_{n\\gamma}) U_1 - Y_1^n\n$$\n$$\nR_2(U_1, U_2) = U_2 - Y_2^n - \\Delta t (\\lambda_{n\\gamma} U_1 - \\lambda_{\\beta} U_2) = -\\Delta t \\lambda_{n\\gamma} U_1 + (1 + \\Delta t \\lambda_{\\beta}) U_2 - Y_2^n\n$$\nThe Newton-Raphson method requires the Jacobian matrix, $\\mathbf{J}(\\mathbf{U}) = \\partial \\mathbf{R} / \\partial \\mathbf{U}$. The components of the Jacobian are the partial derivatives of the residual components with respect to the components of $\\mathbf{U}$:\n$$\n\\mathbf{J}(\\mathbf{U}) = \\begin{bmatrix} \\frac{\\partial R_1}{\\partial U_1} & \\frac{\\partial R_1}{\\partial U_2} \\\\ \\frac{\\partial R_2}{\\partial U_1} & \\frac{\\partial R_2}{\\partial U_2} \\end{bmatrix}\n$$\nCalculating these derivatives yields:\n$$\n\\frac{\\partial R_1}{\\partial U_1} = 1 + \\Delta t \\lambda_{n\\gamma} \\quad , \\quad \\frac{\\partial R_1}{\\partial U_2} = 0\n$$\n$$\n\\frac{\\partial R_2}{\\partial U_1} = -\\Delta t \\lambda_{n\\gamma} \\quad , \\quad \\frac{\\partial R_2}{\\partial U_2} = 1 + \\Delta t \\lambda_{\\beta}\n$$\nThus, the Jacobian matrix is:\n$$\n\\mathbf{J} = \\begin{bmatrix} 1 + \\Delta t \\lambda_{n\\gamma} & 0 \\\\ -\\Delta t \\lambda_{n\\gamma} & 1 + \\Delta t \\lambda_{\\beta} \\end{bmatrix}\n$$\nNote that because the original ODE system is linear, the Jacobian matrix $\\mathbf{J}$ is constant and does not depend on $\\mathbf{U}$. This means the algebraic system derived from the backward Euler method is also linear. The Newton-Raphson method, when applied to a linear system, converges to the exact solution in a single iteration.\n\nThe Newton-Raphson iterative scheme is defined by the update rule:\n$$\n\\mathbf{U}^{(k+1)} = \\mathbf{U}^{(k)} + \\Delta \\mathbf{U}^{(k)}\n$$\nwhere $\\mathbf{U}^{(k)}$ is the solution at iteration $k$ and the update vector $\\Delta \\mathbf{U}^{(k)}$ is obtained by solving the linear system:\n$$\n\\mathbf{J}(\\mathbf{U}^{(k)}) \\Delta \\mathbf{U}^{(k)} = -\\mathbf{R}(\\mathbf{U}^{(k)})\n$$\nThe process starts with an initial guess, typically the solution from the previous time step, $\\mathbf{U}^{(0)} = \\mathbf{Y}^n$. The iterations proceed until the convergence criteria are met. The criteria are based on both the L2-norm of the residual and the L2-norm of the relative update step:\n$$\n||\\mathbf{R}(\\mathbf{U}^{(k)})||_2 \\le \\varepsilon_{\\mathrm{res}} \\quad \\text{and} \\quad \\frac{||\\Delta \\mathbf{U}^{(k)}||_2}{\\max(||\\mathbf{U}^{(k)}||_2, 1)} \\le \\varepsilon_{\\mathrm{step}}\n$$\nwith given tolerances $\\varepsilon_{\\mathrm{res}} = 10^{-12}$ and $\\varepsilon_{\\mathrm{step}} = 10^{-12}$. For a non-zero timestep $\\Delta t > 0$, the first iteration ($k=0$) will compute an update $\\Delta \\mathbf{U}^{(0)}$ such that $\\mathbf{U}^{(1)} = \\mathbf{U}^{(0)} + \\Delta \\mathbf{U}^{(0)}$ is the exact solution to the linear system. The second iteration ($k=1$) will then find that the residual $\\mathbf{R}(\\mathbf{U}^{(1)})$ and the subsequent update $\\Delta \\mathbf{U}^{(1)}$ are both zero (to machine precision), satisfying the convergence criteria and terminating the loop.\n\nFor the special case $\\Delta t = 0$, the backward Euler scheme becomes $\\mathbf{Y}^{n+1} = \\mathbf{Y}^n$, so the solution is simply the initial state $[Y_1^n, Y_2^n]^\\top$.\n\nThe implementation will consist of a function that takes the initial abundances $\\mathbf{Y}^n$ and system parameters $\\lambda_{n\\gamma}$, $\\lambda_{\\beta}$, and $\\Delta t$. It will handle the $\\Delta t=0$ case separately. Otherwise, it will initialize $\\mathbf{U}^{(0)} = \\mathbf{Y}^n$ and enter the NR loop. Inside the loop, it will compute $\\mathbf{R}$ and $\\mathbf{J}$, solve for $\\Delta\\mathbf{U}$, and check for convergence before applying the update for the next iteration. This process is applied to each case in the provided test suite.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_step_with_newton(Yn, l_ng, l_b, dt):\n    \"\"\"\n    Computes a single backward Euler step for the 2-species reaction network\n    using a Newton-Raphson solver for the implicit system.\n\n    Args:\n        Yn (list or tuple): Initial abundances [Y1, Y2] at time t^n.\n        l_ng (float): Neutron-capture rate lambda_n,gamma in 1/s.\n        l_b (float): Beta-decay rate lambda_beta in 1/s.\n        dt (float): Timestep Delta t in s.\n\n    Returns:\n        list: Final abundances [Y1, Y2] at time t^{n+1}.\n    \"\"\"\n    # Define convergence tolerances and maximum iterations\n    eps_res = 1e-12\n    eps_step = 1e-12\n    max_iter = 10\n\n    # Handle the boundary case where the timestep is zero, as specified.\n    if dt == 0.0:\n        return list(Yn)\n\n    # Initial guess for U = [Y1(n+1), Y2(n+1)] is the value at t^n.\n    U = np.array(Yn, dtype=np.float64)\n    Yn_arr = np.array(Yn, dtype=np.float64)\n\n    # The Jacobian matrix is constant for this linear ODE system.\n    # J = [[1 + dt*l_ng,        0],\n    #      [-dt*l_ng,     1 + dt*l_b]]\n    J = np.array([\n        [1.0 + dt * l_ng, 0.0],\n        [-dt * l_ng, 1.0 + dt * l_b]\n    ], dtype=np.float64)\n\n    # Newton-Raphson iteration loop\n    for k in range(max_iter):\n        # Calculate the residual R(U) for the current iterate U.\n        # R(U) = U - Yn - dt*f(U)\n        # R1 = U1 - Y1n - dt*(-l_ng*U1) = (1 + dt*l_ng)*U1 - Y1n\n        # R2 = U2 - Y2n - dt*(l_ng*U1 - l_b*U2) = -dt*l_ng*U1 + (1 + dt*l_b)*U2 - Y2n\n        R1 = (1.0 + dt * l_ng) * U[0] - Yn_arr[0]\n        R2 = (-dt * l_ng) * U[0] + (1.0 + dt * l_b) * U[1] - Yn_arr[1]\n        R = np.array([R1, R2], dtype=np.float64)\n        \n        res_norm = np.linalg.norm(R)\n\n        # Solve the linear system J * delta_U = -R for the update vector delta_U.\n        delta_U = np.linalg.solve(J, -R)\n        \n        # Calculate the relative step size for convergence check.\n        # The norm is relative to the norm of the current solution vector U.\n        delta_U_norm = np.linalg.norm(delta_U)\n        U_norm = np.linalg.norm(U)\n        # Use max(U_norm, 1.0) to avoid division by zero if U is the zero vector.\n        step_norm = delta_U_norm / max(U_norm, 1.0)\n\n        # The problem asks to check convergence based on R(U^k) and delta_U^k.\n        # If both conditions are met, the loop terminates.\n        if res_norm = eps_res and step_norm = eps_step:\n            break\n            \n        # Update the solution vector for the next iteration.\n        U = U + delta_U\n    \n    return U.tolist()\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (Y1n, Y2n, lambda_ng, lambda_beta, dt)\n        (0.7, 0.3, 1e-7, 5e-8, 1e5),\n        (1.0, 0.0, 1e-5, 1e-5, 1e6),\n        (0.5, 0.5, 1e-3, 1e-3, 0.0),\n        (0.8, 0.2, 2e-6, 0.0, 1e6),\n        (0.6, 0.4, 0.0, 1e-7, 1e7),\n    ]\n\n    results = []\n    for case in test_cases:\n        Yn = [case[0], case[1]]\n        l_ng, l_b, dt = case[2], case[3], case[4]\n        \n        Y_next = solve_step_with_newton(Yn, l_ng, l_b, dt)\n        results.append(Y_next)\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, with no spaces, e.g., [[a,b],[c,d]].\n    # str(list) in Python adds spaces, so we build the string carefully.\n    formatted_results = ','.join(str(res).replace(' ', '') for res in results)\n    print(f\"[{formatted_results}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After implementing a numerical integrator, a crucial question arises: is the code correct? Simply obtaining a result is insufficient; we must rigorously verify that the implementation matches its theoretical specification. This practice introduces the Method of Manufactured Solutions (MMS), a powerful and standard technique for code verification. You will construct a test problem with a known, non-trivial analytic solution and use it to measure the convergence rate of two common explicit integrators . This process of comparing the observed order of accuracy with the theoretical order is a cornerstone of developing reliable and trustworthy scientific software.",
            "id": "3591084",
            "problem": "Design and implement a manufactured-solution verification of numerical convergence for a capture-decay network integrator relevant to simulations of slow neutron-capture process (s-process) nucleosynthesis in computational nuclear physics. Use the following physically motivated base and assumptions.\n\nBase equations and definitions: Consider a minimal two-isotope capture-decay network with time-dependent abundances $Y_1(t)$ and $Y_2(t)$ governed by\n$$\n\\frac{dY_1}{dt} \\;=\\; -k_c \\, Y_1 \\;+\\; S_1(t),\n$$\n$$\n\\frac{dY_2}{dt} \\;=\\; k_c \\, Y_1 \\;-\\; \\lambda_\\beta \\, Y_2 \\;+\\; S_2(t),\n$$\nwhere $k_c$ is the neutron-capture rate (in $\\mathrm{s^{-1}}$), $\\lambda_\\beta$ is the beta-decay rate (in $\\mathrm{s^{-1}}$), and $S_i(t)$ are manufactured source terms (in $\\mathrm{s^{-1}}$) constructed so that a chosen analytic abundance trajectory $(Y_1(t), Y_2(t))$ is the exact solution of the system. The initial conditions are $Y_i(0)$ equal to the chosen analytic values at $t=0$. Angles appearing in trigonometric functions must be in radians.\n\nManufactured-solution methodology: Select smooth, strictly positive functions $Y_1(t)$ and $Y_2(t)$ and define\n$$\nS_1(t) \\;=\\; \\frac{dY_1}{dt} \\;+\\; k_c \\, Y_1,\\qquad\nS_2(t) \\;=\\; \\frac{dY_2}{dt} \\;-\\; k_c \\, Y_1 \\;+\\; \\lambda_\\beta \\, Y_2,\n$$\nso that the chosen $Y_i(t)$ solve the network exactly. This construction must be implemented directly in the code.\n\nNumerical integrators to verify: Implement\n- a first-order explicit Forward Euler method, and\n- a second-order explicit Heun (improved Euler) Runge–Kutta method.\nBoth methods advance from $t_n$ to $t_{n+1}=t_n+h$ using the system right-hand side with the above $S_i(t)$, constant rates $k_c$ and $\\lambda_\\beta$, and step size $h$. Define a single-step right-hand side as\n$$\n\\mathbf{f}(t, \\mathbf{Y}) \\;=\\; \\begin{bmatrix}\n-k_c Y_1 + S_1(t) \\\\\nk_c Y_1 - \\lambda_\\beta Y_2 + S_2(t)\n\\end{bmatrix}.\n$$\nForward Euler update: $\\mathbf{Y}_{n+1} = \\mathbf{Y}_n + h \\, \\mathbf{f}(t_n, \\mathbf{Y}_n)$.\nHeun update: $\\mathbf{K}_1=\\mathbf{f}(t_n, \\mathbf{Y}_n)$, $\\mathbf{K}_2=\\mathbf{f}(t_n+h, \\mathbf{Y}_n + h \\mathbf{K}_1)$, $\\mathbf{Y}_{n+1} = \\mathbf{Y}_n + \\frac{h}{2}(\\mathbf{K}_1+\\mathbf{K}_2)$.\n\nError and observed order of accuracy: For a fixed final time $T$ and step sizes $h_1$ and $h_2$ with $h_2 = h_1/2$, define the final-time global error for step size $h$ as\n$$\nE(h) \\;=\\; \\max\\left(\\,\\left|Y_1^{\\text{num}}(T;h) - Y_1^{\\text{true}}(T)\\right|,\\,\\left|Y_2^{\\text{num}}(T;h) - Y_2^{\\text{true}}(T)\\right|\\,\\right).\n$$\nEstimate the observed order $p$ by the Richardson formula\n$$\np \\;=\\; \\frac{\\ln\\!\\left(\\frac{E(h_1)}{E(h_2)}\\right)}{\\ln\\!\\left(\\frac{h_1}{h_2}\\right)}.\n$$\n\nManufactured solutions and test suite: Use the following two analytic abundance pairs, with all trigonometric angles in radians.\n- Analytic Pair A:\n$$\nY_1^{A}(t)= e^{-0.3\\, t} \\;+\\; 0.1 \\,\\sin\\!\\left(2\\pi t\\right),\\qquad\nY_2^{A}(t)= 0.5\\, e^{-0.1\\, t} \\;+\\; 0.05 \\,\\cos\\!\\left(2\\pi t\\right).\n$$\n- Analytic Pair B:\n$$\nY_1^{B}(t)= 0.8\\, e^{-0.5\\, t} \\;+\\; 0.05 \\,\\sin\\!\\left(3 t\\right),\\qquad\nY_2^{B}(t)= 0.3\\, e^{-0.2\\, t} \\;+\\; 0.02 \\,\\cos\\!\\left(4 t\\right).\n$$\n\nConstruct $S_1(t)$ and $S_2(t)$ from each analytic pair together with the specified $k_c$ and $\\lambda_\\beta$ in each test case. For each case, initialize with the exact initial conditions at $t=0$.\n\nProvide a program that computes the observed order $p$ for each of the following four cases:\n- Case $1$ (Forward Euler, non-stiff): $k_c=0.2\\,\\mathrm{s^{-1}}$, $\\lambda_\\beta=0.4\\,\\mathrm{s^{-1}}$, $T=1.0\\,\\mathrm{s}$, $h_1=0.01\\,\\mathrm{s}$, $h_2=0.005\\,\\mathrm{s}$, Analytic Pair A.\n- Case $2$ (Heun, non-stiff): $k_c=0.2\\,\\mathrm{s^{-1}}$, $\\lambda_\\beta=0.4\\,\\mathrm{s^{-1}}$, $T=1.0\\,\\mathrm{s}$, $h_1=0.01\\,\\mathrm{s}$, $h_2=0.005\\,\\mathrm{s}$, Analytic Pair A.\n- Case $3$ (Forward Euler, moderately stiff but stable with given steps): $k_c=2.0\\,\\mathrm{s^{-1}}$, $\\lambda_\\beta=5.0\\,\\mathrm{s^{-1}}$, $T=0.5\\,\\mathrm{s}$, $h_1=\\frac{0.5}{800}\\,\\mathrm{s}$, $h_2=\\frac{0.5}{1600}\\,\\mathrm{s}$, Analytic Pair B.\n- Case $4$ (Heun, moderately stiff): $k_c=2.0\\,\\mathrm{s^{-1}}$, $\\lambda_\\beta=5.0\\,\\mathrm{s^{-1}}$, $T=0.5\\,\\mathrm{s}$, $h_1=\\frac{0.5}{800}\\,\\mathrm{s}$, $h_2=\\frac{0.5}{1600}\\,\\mathrm{s}$, Analytic Pair B.\n\nUnit conventions: Time $t$ is in seconds, rates $k_c$ and $\\lambda_\\beta$ are in $\\mathrm{s^{-1}}$, abundances $Y_i$ are dimensionless, and the observed orders $p$ are dimensionless.\n\nFinal output specification: Your program should produce a single line of output containing the four observed orders as a comma-separated list enclosed in square brackets, in the order of Cases $1$ through $4$, for example, $\\left[\\;p_1,p_2,p_3,p_4\\;\\right]$. The outputs must be real numbers (floats). No additional text must be printed.",
            "solution": "The problem is valid. It presents a clear, self-contained, and scientifically sound task based on the method of manufactured solutions, a standard verification technique in computational physics. All parameters, equations, and procedures are defined unambiguously.\n\nThe solution is designed by first implementing the mathematical constructs of the manufactured solution, then implementing the numerical integrators, and finally orchestrating the verification test suite to compute the observed order of accuracy for each specified case.\n\n**1. Mathematical Formulation and Manufactured Solution**\n\nThe core of the problem is the system of two coupled ordinary differential equations (ODEs) describing the abundances $Y_1(t)$ and $Y_2(t)$:\n$$\n\\frac{d\\mathbf{Y}}{dt} \\;=\\; \\mathbf{f}(t, \\mathbf{Y}), \\quad \\text{where} \\quad \\mathbf{Y}(t) = \\begin{bmatrix} Y_1(t) \\\\ Y_2(t) \\end{bmatrix}.\n$$\nThe right-hand side function $\\mathbf{f}(t, \\mathbf{Y})$ is defined as:\n$$\n\\mathbf{f}(t, \\mathbf{Y}) \\;=\\; \\begin{bmatrix}\n-k_c \\, Y_1(t) \\;+\\; S_1(t) \\\\\nk_c \\, Y_1(t) \\;-\\; \\lambda_\\beta \\, Y_2(t) \\;+\\; S_2(t)\n\\end{bmatrix}.\n$$\nThe method of manufactured solutions requires constructing the source terms $S_1(t)$ and $S_2(t)$ such that a chosen analytic function, $\\mathbf{Y}^{\\text{true}}(t)$, is the exact solution. This is achieved by substituting $\\mathbf{Y}^{\\text{true}}(t)$ into the ODE system and solving for the source terms:\n$$\nS_1(t) \\;=\\; \\frac{dY_1^{\\text{true}}}{dt} \\;+\\; k_c \\, Y_1^{\\text{true}}(t),\n$$\n$$\nS_2(t) \\;=\\; \\frac{dY_2^{\\text{true}}}{dt} \\;-\\; k_c \\, Y_1^{\\text{true}}(t) \\;+\\; \\lambda_\\beta \\, Y_2^{\\text{true}}(t).\n$$\nTo implement this, we need the analytic solutions and their time derivatives.\n\nFor Analytic Pair A:\n$$\nY_1^{A}(t) = e^{-0.3t} + 0.1 \\sin(2\\pi t), \\qquad \\frac{dY_1^{A}}{dt} = -0.3 e^{-0.3t} + 0.2\\pi \\cos(2\\pi t).\n$$\n$$\nY_2^{A}(t) = 0.5 e^{-0.1t} + 0.05 \\cos(2\\pi t), \\qquad \\frac{dY_2^{A}}{dt} = -0.05 e^{-0.1t} - 0.1\\pi \\sin(2\\pi t).\n$$\n\nFor Analytic Pair B:\n$$\nY_1^{B}(t) = 0.8 e^{-0.5t} + 0.05 \\sin(3t), \\qquad \\frac{dY_1^{B}}{dt} = -0.4 e^{-0.5t} + 0.15 \\cos(3t).\n$$\n$$\nY_2^{B}(t) = 0.3 e^{-0.2t} + 0.02 \\cos(4t), \\qquad \\frac{dY_2^{B}}{dt} = -0.06 e^{-0.2t} - 0.08 \\sin(4t).\n$$\nThese functions and their derivatives are implemented to allow for the dynamic calculation of the source terms $S_1(t)$ and $S_2(t)$ at any time $t$.\n\n**2. Numerical Integrators**\n\nTwo explicit numerical methods are used to solve the ODE system from $t=0$ to a final time $T$. The system is advanced in discrete time steps of size $h$.\n\nThe first-order Forward Euler method updates the solution from step $n$ to $n+1$ as:\n$$\n\\mathbf{Y}_{n+1} = \\mathbf{Y}_n + h \\, \\mathbf{f}(t_n, \\mathbf{Y}_n).\n$$\n\nThe second-order Heun method (an explicit Runge-Kutta method) uses a predictor-corrector approach for improved accuracy:\n$$\n\\mathbf{K}_1 = \\mathbf{f}(t_n, \\mathbf{Y}_n),\n$$\n$$\n\\mathbf{K}_2 = \\mathbf{f}(t_{n+1}, \\mathbf{Y}_n + h \\mathbf{K}_1), \\quad \\text{where } t_{n+1} = t_n+h,\n$$\n$$\n\\mathbf{Y}_{n+1} = \\mathbf{Y}_n + \\frac{h}{2}(\\mathbf{K}_1 + \\mathbf{K}_2).\n$$\nBoth methods are implemented in a general-purpose solver function that can be called for any of the test cases.\n\n**3. Verification Procedure and Order of Accuracy**\n\nThe goal is to verify that the implemented integrators achieve their theoretical order of accuracy. The observed order of accuracy, $p$, is computed using Richardson extrapolation on the errors from two simulations.\n\nFor each test case, the numerical solution is computed up to a final time $T$ using two different step sizes, $h_1$ and a refined step size $h_2 = h_1/2$. The global error at the final time $T$ for a given step size $h$ is defined as the maximum absolute difference between the numerical and the true analytic solutions:\n$$\nE(h) = \\max\\left(\\,\\left|Y_1^{\\text{num}}(T;h) - Y_1^{\\text{true}}(T)\\right|,\\,\\left|Y_2^{\\text{num}}(T;h) - Y_2^{\\text{true}}(T)\\right|\\,\\right).\n$$\nThis is equivalent to the infinity norm of the error vector, $\\|\\mathbf{Y}^{\\text{num}}(T;h) - \\mathbf{Y}^{\\text{true}}(T)\\|_\\infty$.\n\nThe observed order of accuracy $p$ is then estimated using the formula:\n$$\np = \\frac{\\ln(E(h_1) / E(h_2))}{\\ln(h_1 / h_2)}.\n$$\nSince $h_1/h_2 = 2$ for all test cases, this simplifies to $p = \\log_2(E(h_1) / E(h_2))$. A first-order method like Forward Euler is expected to yield $p \\approx 1$, while a second-order method like Heun's should yield $p \\approx 2$.\n\n**4. Algorithmic Structure**\n\nThe program is structured to encapsulate the different mathematical components and facilitate the execution of the test suite.\n- **Analytic Functions**: Separate functions compute the true abundance vectors $\\mathbf{Y}^{\\text{true}}(t)$ and their derivatives $d\\mathbf{Y}^{\\text{true}}/dt$ for both Analytic Pair A and B.\n- **Source and RHS Functions**: A function `source_terms` computes the vector $[S_1(t), S_2(t)]^T$ using the analytic solutions and their derivatives. This is then used by the `rhs` function, which computes $\\mathbf{f}(t, \\mathbf{Y})$ for a given state $(t, \\mathbf{Y})$.\n- **Integrator**: A single `integrate` function takes the integration method ('euler' or 'heun'), problem parameters ($k_c, \\lambda_\\beta$), and integration parameters ($T, h$) as inputs. It initializes the solution with the true values at $t=0$, i.e., $\\mathbf{Y}_0 = \\mathbf{Y}^{\\text{true}}(0)$, and iteratively applies the chosen update rule for `int(round(T/h))` steps to find $\\mathbf{Y}^{\\text{num}}(T;h)$.\n- **Main Loop**: The main part of the program defines the four test cases. It iterates through each case, running the integrator twice (for $h_1$ and $h_2$), calculates the errors $E(h_1)$ and $E(h_2)$, computes the observed order $p$, and stores the result. Finally, it prints all four computed orders in the required format.\nThis modular design ensures clarity, correctness, and adherence to the problem specification.",
            "answer": "```python\nimport numpy as np\n\ndef true_solutions(t, pair_id):\n    \"\"\"Computes the true analytic solutions Y(t) for a given pair.\"\"\"\n    if pair_id == 'A':\n        y1 = np.exp(-0.3 * t) + 0.1 * np.sin(2.0 * np.pi * t)\n        y2 = 0.5 * np.exp(-0.1 * t) + 0.05 * np.cos(2.0 * np.pi * t)\n    elif pair_id == 'B':\n        y1 = 0.8 * np.exp(-0.5 * t) + 0.05 * np.sin(3.0 * t)\n        y2 = 0.3 * np.exp(-0.2 * t) + 0.02 * np.cos(4.0 * t)\n    else:\n        raise ValueError(\"Invalid pair_id\")\n    return np.array([y1, y2])\n\ndef true_derivatives(t, pair_id):\n    \"\"\"Computes the time derivatives dY/dt of the true solutions.\"\"\"\n    if pair_id == 'A':\n        dy1_dt = -0.3 * np.exp(-0.3 * t) + 0.2 * np.pi * np.cos(2.0 * np.pi * t)\n        dy2_dt = -0.05 * np.exp(-0.1 * t) - 0.1 * np.pi * np.sin(2.0 * np.pi * t)\n    elif pair_id == 'B':\n        dy1_dt = -0.4 * np.exp(-0.5 * t) + 0.15 * np.cos(3.0 * t)\n        dy2_dt = -0.06 * np.exp(-0.2 * t) - 0.08 * np.sin(4.0 * t)\n    else:\n        raise ValueError(\"Invalid pair_id\")\n    return np.array([dy1_dt, dy2_dt])\n\ndef source_terms(t, kc, lbeta, pair_id):\n    \"\"\"Computes the manufactured source terms S(t).\"\"\"\n    y_true = true_solutions(t, pair_id)\n    dy_dt_true = true_derivatives(t, pair_id)\n    s1 = dy_dt_true[0] + kc * y_true[0]\n    s2 = dy_dt_true[1] - kc * y_true[0] + lbeta * y_true[1]\n    return np.array([s1, s2])\n\ndef rhs(t, y, kc, lbeta, pair_id):\n    \"\"\"Computes the right-hand side f(t, Y) of the ODE system.\"\"\"\n    s_vec = source_terms(t, kc, lbeta, pair_id)\n    f1 = -kc * y[0] + s_vec[0]\n    f2 = kc * y[0] - lbeta * y[1] + s_vec[1]\n    return np.array([f1, f2])\n\ndef integrate(method, kc, lbeta, T, h, pair_id):\n    \"\"\"\n    Integrates the ODE system from t=0 to T with step size h.\n    \"\"\"\n    y = true_solutions(0.0, pair_id)\n    t = 0.0\n    # Use round() to prevent floating point inaccuracies in step counting\n    num_steps = int(round(T / h))\n\n    for _ in range(num_steps):\n        if method == 'euler':\n            y = y + h * rhs(t, y, kc, lbeta, pair_id)\n        elif method == 'heun':\n            k1 = rhs(t, y, kc, lbeta, pair_id)\n            k2 = rhs(t + h, y + h * k1, kc, lbeta, pair_id)\n            y = y + (h / 2.0) * (k1 + k2)\n        t += h\n    return y\n    \ndef solve():\n    \"\"\"\n    Main function to run the test suite and compute observed orders.\n    \"\"\"\n    test_cases = [\n        # Case 1: Forward Euler, non-stiff\n        {'method': 'euler', 'kc': 0.2, 'lbeta': 0.4, 'T': 1.0, 'h1': 0.01, 'h2': 0.005, 'pair_id': 'A'},\n        # Case 2: Heun, non-stiff\n        {'method': 'heun',  'kc': 0.2, 'lbeta': 0.4, 'T': 1.0, 'h1': 0.01, 'h2': 0.005, 'pair_id': 'A'},\n        # Case 3: Forward Euler, stiff\n        {'method': 'euler', 'kc': 2.0, 'lbeta': 5.0, 'T': 0.5, 'h1': 0.5/800.0, 'h2': 0.5/1600.0, 'pair_id': 'B'},\n        # Case 4: Heun, stiff\n        {'method': 'heun',  'kc': 2.0, 'lbeta': 5.0, 'T': 0.5, 'h1': 0.5/800.0, 'h2': 0.5/1600.0, 'pair_id': 'B'}\n    ]\n\n    results = []\n    for case in test_cases:\n        method = case['method']\n        kc = case['kc']\n        lbeta = case['lbeta']\n        T = case['T']\n        h1 = case['h1']\n        h2 = case['h2']\n        pair_id = case['pair_id']\n        \n        # Integrate with step size h1\n        y_num_h1 = integrate(method, kc, lbeta, T, h1, pair_id)\n        \n        # Integrate with step size h2\n        y_num_h2 = integrate(method, kc, lbeta, T, h2, pair_id)\n\n        # Get the true solution at final time T\n        y_true_at_T = true_solutions(T, pair_id)\n        \n        # Calculate final-time global errors\n        error_h1 = np.max(np.abs(y_num_h1 - y_true_at_T))\n        error_h2 = np.max(np.abs(y_num_h2 - y_true_at_T))\n        \n        # Calculate the observed order of accuracy, p\n        # p = log(E(h1)/E(h2)) / log(h1/h2)\n        # Since h1/h2 = 2, this simplifies to log2(E(h1)/E(h2))\n        order_p = np.log(error_h1 / error_h2) / np.log(h1 / h2)\n        \n        results.append(order_p)\n\n    # Print the final result in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "A verified numerical method can, under certain conditions, still produce unphysical results, such as negative abundances. This often occurs when using higher-order methods like the Crank-Nicolson scheme on stiff problems with large time steps. This practice addresses the critical need for robustness by designing a positivity-preserving limiter. You will investigate how and why the standard Crank-Nicolson method (a $\\theta$-method with $\\theta=0.5$) can fail and then implement an adaptive scheme that increases $\\theta$ toward the more stable backward Euler method ($\\theta=1$) precisely when needed to ensure abundances remain nonnegative . This exercise demonstrates how to build physical constraints directly into your solver, a key step in transforming a basic integrator into a robust scientific tool.",
            "id": "3591070",
            "problem": "Consider a two-isotope subsystem representative of neutron-capture followed by beta decay in the slow neutron-capture process (s-process) nucleosynthesis. Let $Y_A(t)$ and $Y_B(t)$ denote dimensionless abundances (number fractions per baryon) of isotopes $A$ and $B$. The neutron-capture rate from $A$ to $B$ is $r_c$ in $\\mathrm{s}^{-1}$, and the beta-decay rate of $B$ is $\\lambda$ in $\\mathrm{s}^{-1}$. The governing Ordinary Differential Equations (ODE) are\n$$\n\\frac{dY_A}{dt} = - r_c \\, Y_A, \\qquad\n\\frac{dY_B}{dt} = r_c \\, Y_A - \\lambda \\, Y_B.\n$$\nAbundances must remain nonnegative for all times to maintain physical realism. For stiff rates and large time steps, naive implicit trapezoidal updates can violate nonnegativity.\n\nStarting from the method-of-lines formulation and the definition of the $\\theta$-method time discretization\n$$\nY^{n+1} = Y^n + \\Delta t\\left[\\theta f\\!\\left(Y^{n+1}\\right) + (1-\\theta) f\\!\\left(Y^n\\right)\\right],\n$$\nwhere $f(Y)$ is the ODE right-hand side and $0.5 \\le \\theta \\le 1$, devise a positivity-preserving limiter for the implicit abundance updates that:\n- Enforces componentwise nonnegativity of $Y_A^{n+1}$ and $Y_B^{n+1}$ at each step.\n- Reduces to the implicit trapezoidal rule (Crank–Nicolson (CN), $\\theta=0.5$) when this update is already nonnegative.\n- Otherwise adaptively increases $\\theta$ within $[0.5,1]$ so that the accepted update is nonnegative.\n- Maintains at least first-order accuracy in time in the sense that for sufficiently small $\\Delta t$, the local truncation error scales no worse than $\\mathcal{O}(\\Delta t)$ regardless of limiter activation.\n\nYour task is to implement a complete program that:\n1. Integrates the system for a specified number of steps $N$ using both:\n   - The naive CN method with $\\theta=0.5$.\n   - Your positivity-preserving limited $\\theta$-method as described above.\n2. Computes the analytical solution at final time $T=N\\Delta t$ for comparison, using the fundamental solution of the linear system\n   $$\n   Y_A(t) = Y_{A,0} e^{-r_c t}, \\quad\n   Y_B(t) =\n   \\begin{cases}\n   Y_{B,0} e^{-\\lambda t} + \\dfrac{r_c Y_{A,0}}{\\lambda - r_c}\\left(e^{-r_c t} - e^{-\\lambda t}\\right),  \\text{if } \\lambda \\ne r_c,\\\\[1em]\n   Y_{B,0} e^{-\\lambda t} + r_c Y_{A,0} \\, t \\, e^{-\\lambda t},  \\text{if } \\lambda = r_c.\n   \\end{cases}\n   $$\n3. For each test case, reports three quantities evaluated at $t=T$:\n   - The minimum of the two abundances produced by naive CN, $\\min\\{Y_A^{\\mathrm{CN}}(T), Y_B^{\\mathrm{CN}}(T)\\}$, as a float.\n   - The minimum of the two abundances produced by the positivity-limited method, $\\min\\{Y_A^{\\mathrm{lim}}(T), Y_B^{\\mathrm{lim}}(T)\\}$, as a float.\n   - The sum of absolute errors of the limited method relative to the analytical solution, $\\left|Y_A^{\\mathrm{lim}}(T) - Y_A^{\\mathrm{exact}}(T)\\right| + \\left|Y_B^{\\mathrm{lim}}(T) - Y_B^{\\mathrm{exact}}(T)\\right|$, as a float.\n\nAll abundances are dimensionless. All rates are in $\\mathrm{s}^{-1}$, and all times are in $\\mathrm{s}$. Angles are not used. Express all reported values as raw floating-point numbers (dimensionless).\n\nUse the following test suite, which spans a general case, a boundary-violating case, a strongly stiff case, and an equal-rate edge case:\n- Case 1 (general stiff, small step): $Y_{A,0}=1.0$, $Y_{B,0}=0.0$, $r_c=10^6\\,\\mathrm{s}^{-1}$, $\\lambda=1\\,\\mathrm{s}^{-1}$, $\\Delta t=10^{-7}\\,\\mathrm{s}$, $N=50$.\n- Case 2 (CN negativity expected): $Y_{A,0}=1.0$, $Y_{B,0}=0.0$, $r_c=10^6\\,\\mathrm{s}^{-1}$, $\\lambda=10^3\\,\\mathrm{s}^{-1}$, $\\Delta t=3\\times 10^{-6}\\,\\mathrm{s}$, $N=1$.\n- Case 3 (strongly stiff with large step): $Y_{A,0}=1.0$, $Y_{B,0}=0.0$, $r_c=10^4\\,\\mathrm{s}^{-1}$, $\\lambda=10^4\\,\\mathrm{s}^{-1}$, $\\Delta t=10^{-3}\\,\\mathrm{s}$, $N=3$.\n- Case 4 (equal rates edge case): $Y_{A,0}=1.0$, $Y_{B,0}=0.0$, $r_c=10^5\\,\\mathrm{s}^{-1}$, $\\lambda=10^5\\,\\mathrm{s}^{-1}$, $\\Delta t=10^{-6}\\,\\mathrm{s}$, $N=1000$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by test case, each case contributing three floats in the sequence described above. For example, the output format must be\n$$\n[\\text{CNmin}_1,\\text{LIMmin}_1,\\text{ERR}_1,\\text{CNmin}_2,\\text{LIMmin}_2,\\text{ERR}_2,\\text{CNmin}_3,\\text{LIMmin}_3,\\text{ERR}_3,\\text{CNmin}_4,\\text{LIMmin}_4,\\text{ERR}_4].\n$$",
            "solution": "We begin from the production–destruction structure for abundances in neutron-capture networks. For isotope $A$ capturing to $B$ with rate $r_c$ and isotope $B$ undergoing beta decay with rate $\\lambda$, the system of Ordinary Differential Equations (ODE) is\n$$\n\\frac{dY_A}{dt} = - r_c Y_A, \\qquad \\frac{dY_B}{dt} = r_c Y_A - \\lambda Y_B.\n$$\nThese equations embody a linear, conservative transfer from $A$ to $B$ and removal from $B$ by decay. The solution must maintain $Y_A(t) \\ge 0$ and $Y_B(t) \\ge 0$ for all $t \\ge 0$ whenever $Y_{A,0}\\ge 0$ and $Y_{B,0}\\ge 0$.\n\nTo integrate stiff systems, implicit methods are used. The $\\theta$-method for time step $\\Delta t$ and parameter $\\theta \\in [0,1]$ is defined by\n$$\nY^{n+1} = Y^n + \\Delta t\\left[\\theta f(Y^{n+1}) + (1-\\theta) f(Y^n)\\right].\n$$\nChoosing $\\theta=0.5$ yields the implicit trapezoidal rule (Crank–Nicolson (CN)), which is second-order accurate but not $L$-stable; choosing $\\theta=1$ yields Backward Euler (BE), which is first-order accurate and $A$-stable/$L$-stable.\n\nApplying the $\\theta$-method to the $A$-equation,\n$$\nY_A^{n+1} = Y_A^n + \\Delta t\\left[ \\theta(-r_c Y_A^{n+1}) + (1-\\theta)(-r_c Y_A^n) \\right],\n$$\nwe obtain\n$$\n\\left(1 + \\theta \\Delta t\\, r_c\\right) Y_A^{n+1} = \\left(1 - (1-\\theta)\\Delta t\\, r_c\\right) Y_A^n,\n$$\nhence\n$$\nY_A^{n+1} = \\frac{1 - (1-\\theta)\\Delta t\\, r_c}{1 + \\theta \\Delta t\\, r_c}\\, Y_A^n.\n$$\nFor $\\theta=0.5$, this becomes\n$$\nY_A^{n+1} = \\frac{1 - \\frac{1}{2}\\Delta t\\, r_c}{1 + \\frac{1}{2}\\Delta t\\, r_c}\\, Y_A^n.\n$$\nThe numerator $(1 - \\frac{1}{2}\\Delta t\\, r_c)$ can become negative when $\\Delta t\\, r_c  2$, causing $Y_A^{n+1}  0$, i.e., CN can violate positivity with large steps for stiff $r_c$.\n\nFor the $B$-equation,\n$$\nY_B^{n+1} = Y_B^n + \\Delta t\\left[ \\theta(r_c Y_A^{n+1} - \\lambda Y_B^{n+1}) + (1-\\theta)(r_c Y_A^n - \\lambda Y_B^n) \\right].\n$$\nRearranging gives\n$$\n\\left(1 + \\theta \\Delta t\\, \\lambda\\right) Y_B^{n+1} - \\theta \\Delta t\\, r_c\\, Y_A^{n+1}\n= Y_B^n + (1-\\theta)\\Delta t\\left(r_c Y_A^n - \\lambda Y_B^n\\right),\n$$\nso that, given $Y_A^{n+1}$,\n$$\nY_B^{n+1} = \\frac{Y_B^n + (1-\\theta)\\Delta t\\left(r_c Y_A^n - \\lambda Y_B^n\\right) + \\theta \\Delta t\\, r_c\\, Y_A^{n+1}}{1 + \\theta \\Delta t\\, \\lambda}.\n$$\nThis linear formula maintains $Y_B^{n+1} \\ge 0$ provided the numerator and denominator are nonnegative; however, when CN makes $Y_A^{n+1}  0$, $Y_B^{n+1}$ can also become negative. In contrast, the BE choice $\\theta=1$ yields\n$$\nY_A^{n+1} = \\frac{Y_A^n}{1 + \\Delta t\\, r_c}, \\qquad\nY_B^{n+1} = \\frac{Y_B^n + \\Delta t\\, r_c\\, Y_A^{n+1}}{1 + \\Delta t\\, \\lambda},\n$$\nwhich are manifestly nonnegative for $Y_A^n, Y_B^n \\ge 0$ (the denominator terms are strictly positive). More generally, for linear production–destruction networks with nonnegative off-diagonal transfers and diagonally dominant destruction, the backward Euler system matrix is an $M$-matrix; its inverse is nonnegative, ensuring positivity preservation.\n\nWe therefore construct a positivity-preserving limiter by adaptively selecting $\\theta \\in [0.5,1]$ at each step:\n- Attempt CN ($\\theta=0.5$). If the provisional update $Y^{n+1}$ is componentwise nonnegative, accept it.\n- Otherwise, increase $\\theta$ toward $1$ until nonnegativity holds. A practical approach is a bisection search on $\\theta \\in [0.5,1]$, evaluating $Y^{n+1}(\\theta)$ via the linear formulas above and shrinking the interval until a nonnegative solution is found to within a tolerance. If necessary, the method falls back to BE ($\\theta=1$), which guarantees positivity for this system.\n\nThis limiter preserves at least first-order accuracy because:\n- For sufficiently small $\\Delta t$, CN does not activate the limiter; thus, local truncation error is $\\mathcal{O}(\\Delta t^2)$.\n- When the limiter activates, $\\theta  0.5$ and the $\\theta$-method is at least first-order accurate (indeed, exactly first order for $\\theta \\ne 0.5$). Therefore, the scheme never degrades below first order in time.\n\nTo assess accuracy and the effect on positivity, we compare against the analytical solution at $t = N \\Delta t$:\n$$\nY_A(t) = Y_{A,0} e^{-r_c t}, \\quad\nY_B(t) =\n\\begin{cases}\nY_{B,0} e^{-\\lambda t} + \\dfrac{r_c Y_{A,0}}{\\lambda - r_c}\\left(e^{-r_c t} - e^{-\\lambda t}\\right),  \\text{if } \\lambda \\ne r_c,\\\\[1em]\nY_{B,0} e^{-\\lambda t} + r_c Y_{A,0} \\, t \\, e^{-\\lambda t},  \\text{if } \\lambda = r_c.\n\\end{cases}\n$$\nFor each test case, we compute:\n- $\\min\\{Y_A^{\\mathrm{CN}}(T), Y_B^{\\mathrm{CN}}(T)\\}$: an indicator of CN violating positivity when negative.\n- $\\min\\{Y_A^{\\mathrm{lim}}(T), Y_B^{\\mathrm{lim}}(T)\\}$: should be nonnegative due to the limiter.\n- The error $\\left|Y_A^{\\mathrm{lim}}(T) - Y_A^{\\mathrm{exact}}(T)\\right| + \\left|Y_B^{\\mathrm{lim}}(T) - Y_B^{\\mathrm{exact}}(T)\\right|$: quantifies the tradeoff when the limiter shifts towards BE for stability.\n\nThe program implements the update formulas for $Y_A^{n+1}$ and $Y_B^{n+1}$ under a given $\\theta$, and then applies the bisection-based limiter if CN produces any negative abundance. The final output aggregates the three floats per test case into the specified single-line, bracketed, comma-separated list, with all quantities dimensionless and computed at $t = N \\Delta t$ for the given rates (in $\\mathrm{s}^{-1}$) and time step (in $\\mathrm{s}$).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef theta_step(YA, YB, rc, lam, dt, theta):\n    \"\"\"\n    Single theta-method step for the linear two-isotope system:\n      dYA/dt = - rc * YA\n      dYB/dt = rc * YA - lam * YB\n    Using the implicit theta-method with parameter theta in [0.5, 1].\n    \"\"\"\n    # Update YA\n    num_A = 1.0 - (1.0 - theta) * dt * rc\n    den_A = 1.0 + theta * dt * rc\n    YA_next = (num_A / den_A) * YA\n\n    # Update YB given YA_next\n    den_B = 1.0 + theta * dt * lam\n    rhs_B = YB + (1.0 - theta) * dt * (rc * YA - lam * YB)\n    YB_next = (rhs_B + theta * dt * rc * YA_next) / den_B\n\n    return YA_next, YB_next\n\ndef cn_integrate(YA0, YB0, rc, lam, dt, N):\n    \"\"\"\n    Integrate using Crank-Nicolson (theta=0.5) for N steps.\n    \"\"\"\n    YA, YB = YA0, YB0\n    for _ in range(N):\n        YA, YB = theta_step(YA, YB, rc, lam, dt, 0.5)\n    return YA, YB\n\ndef limited_integrate(YA0, YB0, rc, lam, dt, N, tol=1e-14, max_iter=40):\n    \"\"\"\n    Integrate using positivity-preserving limited theta-method.\n    Attempt CN (theta=0.5); if negative, increase theta toward 1 via bisection.\n    \"\"\"\n    YA, YB = YA0, YB0\n    for _ in range(N):\n        # Try CN first\n        YA_cn, YB_cn = theta_step(YA, YB, rc, lam, dt, 0.5)\n        if (YA_cn >= -tol) and (YB_cn >= -tol):\n            YA, YB = YA_cn, YB_cn\n            continue\n        # Bisection on theta in [0.5, 1.0]\n        lo, hi = 0.5, 1.0\n        YA_hi, YB_hi = theta_step(YA, YB, rc, lam, dt, hi)  # should be positive for this linear system\n        # If even theta=1 is negative due to numerical anomaly, clamp to zero\n        if (YA_hi  -tol) or (YB_hi  -tol):\n            YA, YB = max(0.0, YA_hi), max(0.0, YB_hi)\n            continue\n        YA_mid, YB_mid = YA_cn, YB_cn\n        for _ in range(max_iter):\n            mid = 0.5 * (lo + hi)\n            YA_mid, YB_mid = theta_step(YA, YB, rc, lam, dt, mid)\n            if (YA_mid  -tol) or (YB_mid  -tol):\n                lo = mid\n            else:\n                hi = mid\n        # Accept hi (the smallest theta in [0.5, 1] that we found nonnegative)\n        YA, YB = theta_step(YA, YB, rc, lam, dt, hi)\n        # Clip tiny negative due to roundoff\n        YA = 0.0 if YA  0.0 and YA > -1e-300 else YA\n        YB = 0.0 if YB  0.0 and YB > -1e-300 else YB\n    return YA, YB\n\ndef analytic_solution(YA0, YB0, rc, lam, t):\n    \"\"\"\n    Analytical solution for the two-isotope linear system.\n    \"\"\"\n    YA_t = YA0 * np.exp(-rc * t)\n    if abs(lam - rc) > 1e-12 * max(abs(lam), abs(rc)): # Use relative tolerance for float comparison\n        YB_t = YB0 * np.exp(-lam * t) + (rc * YA0 / (lam - rc)) * (np.exp(-rc * t) - np.exp(-lam * t))\n    else:\n        # Equal rates limit\n        YB_t = YB0 * np.exp(-lam * t) + rc * YA0 * t * np.exp(-lam * t)\n    return YA_t, YB_t\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each tuple: (YA0, YB0, rc [1/s], lam [1/s], dt [s], N steps)\n    test_cases = [\n        (1.0, 0.0, 1.0e6, 1.0, 1.0e-7, 50),         # Case 1\n        (1.0, 0.0, 1.0e6, 1.0e3, 3.0e-6, 1),        # Case 2\n        (1.0, 0.0, 1.0e4, 1.0e4, 1.0e-3, 3),        # Case 3\n        (1.0, 0.0, 1.0e5, 1.0e5, 1.0e-6, 1000),     # Case 4\n    ]\n\n    results = []\n    for YA0, YB0, rc, lam, dt, N in test_cases:\n        # CN integration\n        YA_cn, YB_cn = cn_integrate(YA0, YB0, rc, lam, dt, N)\n        # Limited integration\n        YA_lim, YB_lim = limited_integrate(YA0, YB0, rc, lam, dt, N)\n        # Analytical solution at final time\n        T = dt * N\n        YA_exact, YB_exact = analytic_solution(YA0, YB0, rc, lam, T)\n        # Metrics\n        cn_min = float(min(YA_cn, YB_cn))\n        lim_min = float(min(YA_lim, YB_lim))\n        err_lim = float(abs(YA_lim - YA_exact) + abs(YB_lim - YB_exact))\n        results.extend([cn_min, lim_min, err_lim])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}