{
    "hands_on_practices": [
        {
            "introduction": "Before deploying computationally intensive iterative solvers, it is often prudent to obtain a rough estimate of the eigenvalue locations. This practice introduces Gershgorin's circle theorem as a simple, computationally inexpensive tool for this purpose . By applying the theorem to a model Hamiltonian, you will see how it provides rigorous bounds on the spectrum, a technique crucial for guiding strategies like the shift-and-invert method, where selecting a safe and effective shift is paramount.",
            "id": "3568887",
            "problem": "A nuclear shell-model Hamiltonian for a truncated configuration space is represented in a single-particle basis by the real symmetric matrix $H \\in \\mathbb{R}^{6 \\times 6}$ (Hermitian in the underlying complex formulation), whose entries are given in Mega electron Volts (MeV):\n$$\nH \\;=\\; \\begin{pmatrix}\n-8.1 & -0.25 & 0.20 & 0 & 0.05 & 0 \\\\\n-0.25 & -7.4 & -0.15 & 0 & 0 & 0 \\\\\n0.20 & -0.15 & -7.9 & 0.05 & 0.10 & 0 \\\\\n0 & 0 & 0.05 & 4.3 & -0.30 & -0.10 \\\\\n0.05 & 0 & 0.10 & -0.30 & 5.0 & -0.20 \\\\\n0 & 0 & 0 & -0.10 & -0.20 & 6.2\n\\end{pmatrix}.\n$$\nUsing first-principles tools from numerical linear algebra, compute the Gershgorin bounds for the eigenvalues of $H$ and form the union of the resulting one-dimensional Gershgorin intervals (the matrix is Hermitian, so the spectra are real). Then, in order to perform a shift-invert computation that is guaranteed to be safe (invertible factorization) while still targeting the positive-energy sector relevant to excited states, select a positive shift $\\sigma$ as the midpoint of the largest bounded open gap between adjacent positive Gershgorin intervals. Provide this $\\sigma$ in Mega electron Volts (MeV), rounded to four significant figures. The final answer must be a single real number.",
            "solution": "The problem requires the computation of a suitable shift parameter, $\\sigma$, for a shift-invert eigenvalue algorithm. This involves localizing the eigenvalues of the given Hamiltonian matrix $H$ using Gershgorin's circle theorem, identifying a gap in the positive part of the spectrum, and finding the midpoint of that gap.\n\nThe provided Hamiltonian matrix is:\n$$\nH \\;=\\; \\begin{pmatrix}\n-8.1 & -0.25 & 0.20 & 0 & 0.05 & 0 \\\\\n-0.25 & -7.4 & -0.15 & 0 & 0 & 0 \\\\\n0.20 & -0.15 & -7.9 & 0.05 & 0.10 & 0 \\\\\n0 & 0 & 0.05 & 4.3 & -0.30 & -0.10 \\\\\n0.05 & 0 & 0.10 & -0.30 & 5.0 & -0.20 \\\\\n0 & 0 & 0 & -0.10 & -0.20 & 6.2\n\\end{pmatrix}\n$$\nThe entries are in units of Mega electron Volts (MeV). Since $H$ is a real symmetric matrix, its eigenvalues are real numbers. Gershgorin's circle theorem states that every eigenvalue of a matrix $A \\in \\mathbb{C}^{n \\times n}$ lies within at least one of the Gershgorin disks $D(A_{ii}, R_i)$ in the complex plane, where $A_{ii}$ is the $i$-th diagonal entry and $R_i = \\sum_{j \\neq i} |A_{ij}|$ is the sum of the absolute values of the off-diagonal entries in the $i$-th row. For a real symmetric matrix like $H$, the eigenvalues are real, so the Gershgorin disks become closed intervals on the real line, $G_i = [H_{ii} - R_i, H_{ii} + R_i]$.\n\nFirst, we compute the radii $R_i$ for each row $i = 1, \\dots, 6$:\n$R_1 = |-0.25| + |0.20| + |0| + |0.05| + |0| = 0.25 + 0.20 + 0 + 0.05 + 0 = 0.50$\n$R_2 = |-0.25| + |-0.15| + |0| + |0| + |0| = 0.25 + 0.15 + 0 + 0 + 0 = 0.40$\n$R_3 = |0.20| + |-0.15| + |0.05| + |0.10| + |0| = 0.20 + 0.15 + 0.05 + 0.10 + 0 = 0.50$\n$R_4 = |0| + |0| + |0.05| + |-0.30| + |-0.10| = 0 + 0 + 0.05 + 0.30 + 0.10 = 0.45$\n$R_5 = |0.05| + |0| + |0.10| + |-0.30| + |-0.20| = 0.05 + 0 + 0.10 + 0.30 + 0.20 = 0.65$\n$R_6 = |0| + |0| + |0| + |-0.10| + |-0.20| = 0 + 0 + 0 + 0.10 + 0.20 = 0.30$\n\nNext, we determine the Gershgorin intervals $G_i = [H_{ii} - R_i, H_{ii} + R_i]$:\n$G_1 = [-8.1 - 0.50, -8.1 + 0.50] = [-8.60, -7.60]$\n$G_2 = [-7.4 - 0.40, -7.4 + 0.40] = [-7.80, -7.00]$\n$G_3 = [-7.9 - 0.50, -7.9 + 0.50] = [-8.40, -7.40]$\n$G_4 = [4.3 - 0.45, 4.3 + 0.45] = [3.85, 4.75]$\n$G_5 = [5.0 - 0.65, 5.0 + 0.65] = [4.35, 5.65]$\n$G_6 = [6.2 - 0.30, 6.2 + 0.30] = [5.90, 6.50]$\n\nThe full spectrum of $H$, denoted $\\Lambda(H)$, is contained in the union of these intervals: $\\Lambda(H) \\subseteq \\bigcup_{i=1}^6 G_i$. The problem asks to focus on the positive-energy sector, which corresponds to the intervals $G_4$, $G_5$, and $G_6$. We form the union of these positive intervals.\n\nThe intervals $G_4 = [3.85, 4.75]$ and $G_5 = [4.35, 5.65]$ overlap because the start of $G_5$ ($4.35$) is less than the end of $G_4$ ($4.75$). Their union is:\n$G_4 \\cup G_5 = [\\min(3.85, 4.35), \\max(4.75, 5.65)] = [3.85, 5.65]$.\n\nThe third positive interval is $G_6 = [5.90, 6.50]$. This interval is disjoint from the union $G_4 \\cup G_5$ because the upper bound of $G_4 \\cup G_5$ ($5.65$) is less than the lower bound of $G_6$ ($5.90$).\n\nThus, the region containing the positive eigenvalues, according to Gershgorin's theorem, is the union of two disjoint sets:\n$[3.85, 5.65] \\cup [5.90, 6.50]$.\n\nThe problem requires finding the midpoint of the \"largest bounded open gap between adjacent positive Gershgorin intervals\". In this case, there is only one such gap. This gap is the open interval between the two disjoint sets we found:\nGap $= (5.65, 5.90)$.\n\nThe shift $\\sigma$ is defined as the midpoint of this gap.\n$$\n\\sigma = \\frac{5.65 + 5.90}{2} = \\frac{11.55}{2} = 5.775\n$$\nThis value of $\\sigma$ lies within the gap, ensuring that $\\sigma$ is not an eigenvalue of $H$. Consequently, the matrix $(H - \\sigma I)$ is invertible, making the shift-invert procedure well-defined.\n\nThe problem requires the answer to be rounded to four significant figures. The number $5.775$ already has exactly four significant figures ($5$, $7$, $7$, $5$). Therefore, no further rounding is necessary. The value of the shift is $5.775$ MeV.",
            "answer": "$$\\boxed{5.775}$$"
        },
        {
            "introduction": "The Lanczos algorithm is a cornerstone method for tackling the massive eigenvalue problems that arise in computational nuclear physics, particularly in shell-model calculations. This hands-on exercise guides you through the implementation of this workhorse algorithm for a discretized single-nucleon Hamiltonian . By building the Krylov subspace and computing the approximate (Ritz) eigenvalues, you will gain direct experience with the core mechanics of this powerful iterative technique and the use of the residual norm to assess the accuracy of your results.",
            "id": "3568855",
            "problem": "Consider a real-symmetric matrix representation of a one-dimensional single-nucleon Hamiltonian obtained by finite-difference discretization of the time-independent Schrödinger operator on a bounded domain with Dirichlet boundary conditions. Let the Hamiltonian matrix be defined as follows. For a given number of interior grid points $N \\in \\mathbb{N}$ and a half-domain length $L \\in \\mathbb{R}_{>0}$, define the uniform mesh with spacing $h = \\dfrac{2L}{N+1}$ over the interior points $x_i = -L + i h$ for $i \\in \\{1,2,\\dots,N\\}$. The kinetic energy operator is discretized by the standard second-order central difference, producing a tridiagonal matrix $T \\in \\mathbb{R}^{N \\times N}$ with diagonal entries $T_{ii} = \\dfrac{2}{h^2}$ and first off-diagonal entries $T_{i,i+1} = T_{i+1,i} = -\\dfrac{1}{h^2}$ for all valid indices. A square-well potential of depth $V_0 \\in \\mathbb{R}_{\\ge 0}$ and half-width $a \\in \\mathbb{R}_{\\ge 0}$ is imposed by $V(x) = -V_0$ if $|x| \\le a$ and $V(x) = 0$ otherwise. Let $D \\in \\mathbb{R}^{N \\times N}$ be the diagonal matrix with $D_{ii} = V(x_i)$. The Hamiltonian is $H = T + D \\in \\mathbb{R}^{N \\times N}$.\n\nGiven a starting vector $b \\in \\mathbb{R}^N$ with $\\|b\\|_2 = 1$, the $m$-dimensional Krylov subspace is defined by $\\mathcal{K}_m(H,b) = \\operatorname{span}\\{b, Hb, H^2 b, \\dots, H^{m-1} b\\}$. Use a Lanczos construction starting from $b$ to obtain the tridiagonal Rayleigh-Ritz matrix $T_m \\in \\mathbb{R}^{m \\times m}$ associated with an orthonormal basis of $\\mathcal{K}_m(H,b)$. The Ritz values are the eigenvalues of $T_m$. The lowest Ritz value is taken to approximate the smallest eigenvalue of $H$. Let $\\theta_m \\in \\mathbb{R}$ denote the smallest eigenvalue of $T_m$ and let $y_m \\in \\mathbb{R}^m$ denote a corresponding unit-norm eigenvector. Define the Ritz vector $v_m \\in \\mathbb{R}^N$ as the projection of $y_m$ back to the full space via the Lanczos basis. The residual norm for this Ritz pair is $\\|H v_m - \\theta_m v_m\\|_2$, which serves as an a posteriori accuracy estimate for the lowest eigenvalue approximation. No physical units are required; report all numerical results as dimensionless quantities.\n\nTask: Implement a program that, for each test case specified below, constructs $H$ from $(N,L,V_0,a)$, sets $b$ to the normalized all-ones vector in $\\mathbb{R}^N$, builds the $m$-step Lanczos tridiagonal $T_m$, computes the smallest Ritz value $\\theta_m$, and estimates its accuracy by the residual norm $\\rho_m = \\|H v_m - \\theta_m v_m\\|_2$. You must compute the residual norm exactly as defined, or by an equivalent identity that is valid for a Lanczos tridiagonalization initialized with $b$. Use the Euclidean norm for all vector norms.\n\nYour implementation must be general and numerically stable for the given sizes. The following three test cases must be executed in this exact order:\n\n- Case $1$: $N = 40$, $L = 10$, $V_0 = 5$, $a = 2$, $m = 8$.\n- Case $2$: $N = 80$, $L = 8$, $V_0 = 10$, $a = 0.5$, $m = 20$.\n- Case $3$: $N = 12$, $L = 1$, $V_0 = 0$, $a = 0$, $m = 12$.\n\nFor all cases, choose $b$ to be the normalized vector with entries $b_i = 1$ for $i \\in \\{1,2,\\dots,N\\}$.\n\nFinal output format: Your program should produce a single line of output containing a list of pairs, one pair per test case in the order given. Each pair must be $[\\theta_m,\\rho_m]$ where $\\theta_m$ is the smallest Ritz value and $\\rho_m$ is the residual norm. All floating-point numbers must be printed in scientific notation with exactly $10$ significant digits, and there must be no spaces anywhere in the output. For example, an output with two cases would look like $[[1.234567890e+00,9.876543210e-05],[\\dots,\\dots]]$ but with the appropriate values for this problem. The program must not read any input and must print exactly one line.\n\nYour answer must be a complete, runnable program.",
            "solution": "The problem requires the determination of the lowest approximate eigenvalue, a Ritz value, and its associated accuracy for a discretized one-dimensional single-nucleon Hamiltonian. This analysis is performed using the Lanczos algorithm, a prominent Krylov subspace method that is exceptionally well-suited for finding extremal eigenvalues of large, symmetric matrices.\n\nFirst, we formalize the construction of the Hamiltonian matrix $H$. The physical system is modeled by the time-independent Schrödinger equation, $\\hat{H}\\psi = E\\psi$. For a single nucleon of mass $M$ in a potential $V(x)$, the Hamiltonian operator is $\\hat{H} = -\\dfrac{\\hbar^2}{2M} \\dfrac{d^2}{dx^2} + V(x)$. By adopting a system of units where the constant term $\\dfrac{\\hbar^2}{2M}$ is equal to $1$, the operator simplifies to $\\hat{H} = -\\dfrac{d^2}{dx^2} + V(x)$. We discretize this operator on a uniform grid over the bounded domain $[-L, L]$ with $N$ interior points and subject to Dirichlet boundary conditions, $\\psi(-L) = \\psi(L) = 0$.\n\nThe grid spacing is defined as $h = \\dfrac{2L}{N+1}$, and the interior grid points are located at $x_i = -L + i h$ for $i \\in \\{1, 2, \\dots, N\\}$. The second derivative operator is approximated using a second-order central difference stencil: $\\dfrac{d^2\\psi}{dx^2}\\bigg|_{x_i} \\approx \\dfrac{\\psi(x_{i+1}) - 2\\psi(x_i) + \\psi(x_{i-1})}{h^2}$. This discretization leads to a matrix representation of the kinetic energy operator, $T \\in \\mathbb{R}^{N \\times N}$, with entries:\n$$\nT_{ij} = \\begin{cases}\n    \\dfrac{2}{h^2} & \\text{if } i = j \\\\\n    -\\dfrac{1}{h^2} & \\text{if } |i - j| = 1 \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n$$\nThe potential energy operator is represented by a diagonal matrix $D \\in \\mathbb{R}^{N \\times N}$, where the diagonal entries are given by the potential function evaluated at the grid points, $D_{ii} = V(x_i)$. The specified potential is a square well of depth $V_0$ and half-width $a$:\n$$\nV(x_i) = \\begin{cases}\n    -V_0 & \\text{if } |x_i| \\le a \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n$$\nThe total discretized Hamiltonian is the sum of the kinetic and potential energy matrices, yielding the real-symmetric matrix $H = T + D \\in \\mathbb{R}^{N \\times N}$.\n\nNext, we employ the Lanczos algorithm to approximate the eigenvalues of $H$. The algorithm constructs an orthonormal basis for the Krylov subspace $\\mathcal{K}_m(H, b) = \\operatorname{span}\\{b, Hb, \\dots, H^{m-1}b\\}$, where $b$ is a given starting vector. In this process, it generates a sequence of orthonormal vectors $\\{q_j\\}_{j=1}^m$ and a real-symmetric tridiagonal matrix $T_m \\in \\mathbb{R}^{m \\times m}$. The procedure is as follows:\nInitialize: Let $q_0 = 0 \\in \\mathbb{R}^N$, $\\beta_0 = 0$, and normalize the starting vector $b$ to obtain $q_1 = b / \\|b\\|_2$.\nFor $j=1, 2, \\dots, m$:\n1. Compute the matrix-vector product $w_j = H q_j$.\n2. Compute the diagonal element of $T_m$: $\\alpha_j = q_j^T w_j$.\n3. Compute the unnormalized next vector: $\\tilde{w}_j = w_j - \\alpha_j q_j - \\beta_{j-1} q_{j-1}$. This step ensures $\\tilde{w}_j$ is orthogonal to $q_j$ and $q_{j-1}$. The three-term recurrence, a consequence of the symmetry of $H$, guarantees orthogonality to all previous basis vectors $q_k$ for $k<j$.\n4. Compute the off-diagonal element by taking the norm: $\\beta_j = \\|\\tilde{w}_j\\|_2$.\n5. If $\\beta_j$ is numerically zero, the algorithm terminates (a \"lucky breakdown\").\n6. Normalize to get the next basis vector: $q_{j+1} = \\tilde{w}_j / \\beta_j$.\n\nAfter $m$ steps, this iterative process yields the tridiagonal matrix:\n$$\nT_m = \\begin{pmatrix}\n\\alpha_1 & \\beta_1 & & & \\\\\n\\beta_1 & \\alpha_2 & \\beta_2 & & \\\\\n& \\ddots & \\ddots & \\ddots & \\\\\n& & \\beta_{m-2} & \\alpha_{m-1} & \\beta_{m-1} \\\\\n& & & \\beta_{m-1} & \\alpha_m\n\\end{pmatrix}\n$$\nThis matrix $T_m$ is the orthogonal projection of $H$ onto the Krylov subspace spanned by $\\{q_j\\}_{j=1}^m$, that is, $T_m = Q_m^T H Q_m$, where $Q_m = [q_1, q_2, \\dots, q_m]$.\n\nThe eigenvalues of the small matrix $T_m$ are called Ritz values, and they serve as approximations to the eigenvalues of the large matrix $H$. The Lanczos method is particularly effective at approximating the extremal eigenvalues of $H$. We seek the smallest eigenvalue of $H$ (the ground state energy), and thus we compute the smallest eigenvalue of $T_m$, denoted $\\theta_m$. Let $y_m$ be its corresponding unit-norm eigenvector:\n$$\nT_m y_m = \\theta_m y_m, \\quad \\|y_m\\|_2=1\n$$\nThe Ritz vector $v_m = Q_m y_m$ is the corresponding approximation to the eigenvector of $H$ in the original $N$-dimensional space.\n\nA standard a posteriori error estimate for the Ritz pair $(\\theta_m, v_m)$ is the norm of the residual vector, $\\rho_m = \\|H v_m - \\theta_m v_m\\|_2$. A direct calculation is computationally intensive. However, a fundamental property of the Lanczos algorithm provides a highly efficient method for its computation. The relationship between $H$, $Q_m$, and $T_m$ after $m$ steps is given by the identity:\n$$\nH Q_m = Q_m T_m + \\beta_m q_{m+1} e_m^T\n$$\nwhere $\\beta_m$ is the scaling factor generated at step $m$ of the algorithm, $q_{m+1}$ is the next orthonormal vector, and $e_m$ is the $m$-th standard basis vector in $\\mathbb{R}^m$. Using this identity, the residual vector simplifies dramatically:\n$$\n\\begin{aligned}\nH v_m - \\theta_m v_m &= H (Q_m y_m) - \\theta_m (Q_m y_m) \\\\\n&= (H Q_m) y_m - Q_m (\\theta_m y_m) \\\\\n&= (Q_m T_m + \\beta_m q_{m+1} e_m^T) y_m - Q_m (T_m y_m) \\\\\n&= (Q_m T_m y_m) + \\beta_m q_{m+1} (e_m^T y_m) - (Q_m T_m y_m) \\\\\n&= \\beta_m q_{m+1} (e_m^T y_m)\n\\end{aligned}\n$$\nThe term $e_m^T y_m$ is the last component of the eigenvector $y_m$, which we denote $y_{m,m}$. The residual vector is thus $r_m = \\beta_m y_{m,m} q_{m+1}$. Taking the Euclidean norm and using the fact that $\\|q_{m+1}\\|_2 = 1$, we arrive at the elegant and efficient formula for the residual norm:\n$$\n\\rho_m = \\|r_m\\|_2 = \\|\\beta_m y_{m,m} q_{m+1}\\|_2 = |\\beta_m| |y_{m,m}|\n$$\nThis formula obviates the need for explicit formation of the Ritz vector $v_m$ and the costly matrix-vector product with $H$. For the special case where $m=N$, the algorithm must terminate with $\\beta_N=0$ in exact arithmetic, leading to a residual norm of zero, as the Ritz values become the exact eigenvalues of $H$.\n\nThe implementation will execute the following steps for each test case:\n1. Construct the Hamiltonian matrix $H$ from the parameters $(N, L, V_0, a)$.\n2. Initialize and normalize the starting vector $b$.\n3. Perform $m$ iterations of the Lanczos algorithm to compute the tridiagonal matrix $T_m$ (specifically, its diagonal $\\{\\alpha_j\\}_{j=1}^m$ and off-diagonal $\\{\\beta_j\\}_{j=1}^{m-1}$ entries) and the final scalar $\\beta_m$.\n4. Solve the eigenproblem for $T_m$ using `scipy.linalg.eigh_tridiagonal` to find its eigenvalues and eigenvectors.\n5. Identify the smallest eigenvalue, $\\theta_m$, and the final component, $y_{m,m}$, of its corresponding eigenvector $y_m$.\n6. Calculate the residual norm $\\rho_m = |\\beta_m y_{m,m}|$.\n7. Format and store the pair $[\\theta_m, \\rho_m]$ for the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh_tridiagonal\n\ndef compute_ritz_pair(N, L, V0, a, m):\n    \"\"\"\n    Constructs the Hamiltonian, runs the Lanczos algorithm, and computes the\n    lowest Ritz value and its residual norm.\n    \"\"\"\n    # Step 1: Construct the Hamiltonian matrix H\n    h = (2.0 * L) / (N + 1.0)\n    x = np.array([-L + (i + 1.0) * h for i in range(N)])\n\n    # Kinetic energy matrix T\n    diag_T = np.full(N, 2.0 / h**2)\n    off_diag_T = np.full(N - 1, -1.0 / h**2)\n    T = np.diag(diag_T) + np.diag(off_diag_T, k=1) + np.diag(off_diag_T, k=-1)\n\n    # Potential energy matrix D\n    V_values = -V0 * (np.abs(x) <= a)\n    D = np.diag(V_values)\n\n    H = T + D\n\n    # Step 2: Initialize Lanczos algorithm\n    b = np.ones(N)\n    q = b / np.linalg.norm(b)\n    q_prev = np.zeros(N)\n    beta = 0.0\n\n    alphas = []\n    betas_off_diag = []\n\n    # Step 3: Perform m iterations of Lanczos algorithm\n    for j in range(m):\n        w = H @ q\n        alpha = np.dot(w, q)\n        alphas.append(alpha)\n\n        # The three-term recurrence ensures orthogonality\n        w_hat = w - alpha * q - beta * q_prev\n        beta = np.linalg.norm(w_hat)\n\n        q_prev = q\n        # Handle breakdown (beta is numerically zero)\n        if beta > 1e-15:\n            q = w_hat / beta\n        else:\n            # If breakdown occurs, subsequent beta values would be zero.\n            # beta_m will be this zero value.\n            break\n\n        if j < m - 1:\n            betas_off_diag.append(beta)\n    \n    # This is beta_m needed for the residual norm calculation\n    beta_m = beta\n    \n    # If the loop broke early due to lucky breakdown, right-pad alpha and beta lists\n    # to form a matrix of size j+1. The final m in this case is effectively j+1.\n    current_m = len(alphas)\n    if current_m < m:\n        # The remaining T_m entries are effectively undefined, but we need\n        # a matrix of size 'current_m'\n        pass\n    \n    # Step 4: Solve the tridiagonal eigenproblem for T_m\n    if not betas_off_diag: # handles m=1 case\n        eigvals = np.array(alphas)\n        eigvecs = np.array([[1.0]])\n    else:\n        eigvals, eigvecs = eigh_tridiagonal(np.array(alphas), np.array(betas_off_diag))\n    \n    # Step 5: Identify the smallest Ritz value and its eigenvector\n    min_eig_idx = np.argmin(eigvals)\n    theta_m = eigvals[min_eig_idx]\n    y_m = eigvecs[:, min_eig_idx]\n    \n    # Step 6: Calculate the residual norm\n    # The last component of the eigenvector y_m of the matrix T of size current_m\n    y_m_last = y_m[-1] if current_m > 0 else 0.0\n    rho_m = np.abs(beta_m * y_m_last)\n\n    return theta_m, rho_m\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, L, V0, a, m)\n        (40, 10.0, 5.0, 2.0, 8),\n        (80, 8.0, 10.0, 0.5, 20),\n        (12, 1.0, 0.0, 0.0, 12),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, L, V0, a, m = case\n        theta_m, rho_m = compute_ritz_pair(N, L, V0, a, m)\n        results.append([theta_m, rho_m])\n\n    # Final print statement in the exact required format.\n    results_str = [f\"[{val[0]:.10e},{val[1]:.10e}]\" for val in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Calculating the eigenvalues of a Hamiltonian is often just the starting point; understanding how these eigenvalues respond to changes in the underlying physical model is critical for deeper analysis. This practice delves into this sensitivity by exploring the connection between matrix perturbation theory and numerical computation . You will derive and implement the Hellmann-Feynman theorem to compute the analytic derivative of an eigenvalue, providing a powerful and efficient alternative to numerical approximations like finite differences.",
            "id": "3568968",
            "problem": "You are given a family of real symmetric matrices that approximate a one-dimensional Schrödinger Hamiltonian on a finite interval with Dirichlet boundary conditions. The Hamiltonian depends smoothly on a scalar parameter $\\alpha$ through a Woods–Saxon potential. Your task is to derive an expression for the rate of change of the smallest eigenvalue with respect to $\\alpha$ using Rayleigh quotient differentiation starting only from the eigenvalue problem and the normalization condition, and then to implement and compare this analytic derivative with finite-difference estimates computed via an iterative eigen-solver that uses only matrix–vector products.\n\nConsider the following construction. Let $N$ be the number of interior grid points, and let the spatial domain be $[-L,L]$ with $N$ equally spaced interior points and homogeneous Dirichlet boundary conditions at the endpoints. Let $\\Delta x = \\dfrac{2L}{N+1}$ and grid points $x_i = -L + i \\,\\Delta x$ for $i=1,2,\\dots,N$. Define the finite-difference kinetic energy matrix $T \\in \\mathbb{R}^{N \\times N}$ by the standard second-order central difference approximation to $-\\dfrac{d^2}{dx^2}$,\n$$\nT = \\frac{1}{\\Delta x^2}\\,\\mathrm{tridiag}(-1,\\,2,\\,-1),\n$$\nwhere $\\mathrm{tridiag}(-1,2,-1)$ denotes the tridiagonal matrix with main diagonal entries $2$, and first sub- and super-diagonals equal to $-1$. Define the Woods–Saxon potential at grid point $x_i$ as\n$$\nV_i(\\alpha) = -\\frac{\\alpha}{1 + \\exp\\!\\left(\\dfrac{|x_i| - R}{a}\\right)},\n$$\nwith fixed shape parameters $R>0$ and $a>0$. The Hamiltonian matrix is\n$$\nA(\\alpha) = T + \\mathrm{diag}\\!\\big(V_1(\\alpha),\\dots,V_N(\\alpha)\\big).\n$$\n\nFor a simple, isolated smallest eigenvalue $\\lambda_0(\\alpha)$ with associated normalized eigenvector $u_0(\\alpha)$ satisfying $A(\\alpha) u_0(\\alpha) = \\lambda_0(\\alpha)\\,u_0(\\alpha)$ and $u_0(\\alpha)^\\top u_0(\\alpha) = 1$, derive from first principles an expression for $\\dfrac{d\\lambda_0}{d\\alpha}$ in terms of $A(\\alpha)$, its derivative with respect to $\\alpha$, and $u_0(\\alpha)$, using only the eigenvalue equation, the normalization condition, and the definition of the Rayleigh quotient.\n\nThen implement a program that:\n- Constructs $A(\\alpha)$ on the grid for specified $(N,L,R,a)$.\n- Computes the smallest eigenpair using an iterative symmetric eigen-solver that relies on matrix–vector products (for example, a Lanczos-type method).\n- Computes the analytic derivative $\\dfrac{d\\lambda_0}{d\\alpha}$ obtained from your derivation.\n- Computes a central finite-difference estimate of $\\dfrac{d\\lambda_0}{d\\alpha}$ using\n$$\n\\frac{\\lambda_0(\\alpha+h) - \\lambda_0(\\alpha-h)}{2h},\n$$\nwhere each $\\lambda_0(\\cdot)$ is computed with the same iterative eigen-solver and requested tolerance.\n\n- Reports, for each test case, the absolute difference between the analytic derivative and the finite-difference estimate as a floating-point number.\n\nUse the following fixed discretization and physical shape parameters for all tests:\n- $N = 400$,\n- $L = 20$,\n- $R = 6$,\n- $a = 0.65$.\n\nUse an iterative symmetric eigen-solver to obtain the smallest algebraic eigenvalue and its normalized eigenvector, with an absolute convergence tolerance of $10^{-10}$ or tighter.\n\nTest suite. For each ordered pair $(\\alpha_0, h)$ below, compute the absolute error between the analytic derivative and the finite-difference estimate at $\\alpha=\\alpha_0$:\n1. $(\\alpha_0, h) = (2.0, 10^{-2})$,\n2. $(\\alpha_0, h) = (8.0, 10^{-4})$,\n3. $(\\alpha_0, h) = (12.0, 10^{-6})$,\n4. $(\\alpha_0, h) = (0.5, 5\\times 10^{-3})$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4]$), where $r_j$ is the floating-point absolute error for test case $j$. No additional text should be printed.\n\nAll computations are dimensionless; no physical units are required. Angles do not appear. Percentages do not appear. The outputs are real numbers.",
            "solution": "The problem statement has been rigorously validated and is determined to be valid. It is a well-posed problem in computational physics that is scientifically grounded, self-contained, and objective. It requires the derivation of a standard result in matrix perturbation theory and its numerical verification, which is a standard exercise in the field. All parameters and conditions are sufficiently specified to permit a unique and meaningful solution.\n\nThe core of the task is to derive an expression for the derivative of an eigenvalue with respect to a parameter, a result known as the Hellmann-Feynman theorem in quantum mechanics. The derivation is to start from the Rayleigh quotient, the eigenvalue equation, and the normalization condition. Let $\\lambda_0(\\alpha)$ be a simple, isolated real eigenvalue of the real symmetric matrix $A(\\alpha)$, with corresponding normalized eigenvector $u_0(\\alpha)$. The eigenvalue equation is $A(\\alpha)u_0(\\alpha) = \\lambda_0(\\alpha)u_0(\\alpha)$, and the normalization condition is $u_0(\\alpha)^\\top u_0(\\alpha) = 1$.\n\nThe eigenvalue $\\lambda_0(\\alpha)$ can be expressed as the Rayleigh quotient evaluated at its corresponding eigenvector:\n$$\n\\lambda_0(\\alpha) = \\frac{u_0(\\alpha)^\\top A(\\alpha) u_0(\\alpha)}{u_0(\\alpha)^\\top u_0(\\alpha)}\n$$\nGiven the normalization condition $u_0(\\alpha)^\\top u_0(\\alpha) = 1$, this simplifies to:\n$$\n\\lambda_0(\\alpha) = u_0(\\alpha)^\\top A(\\alpha) u_0(\\alpha)\n$$\nTo find the rate of change of $\\lambda_0$ with respect to $\\alpha$, we differentiate this expression. For conciseness, the explicit dependence on $\\alpha$ will be suppressed, so we write $\\lambda_0$, $A$, and $u_0$. Applying the product rule for differentiation gives:\n$$\n\\frac{d\\lambda_0}{d\\alpha} = \\left(\\frac{du_0}{d\\alpha}\\right)^\\top A u_0 + u_0^\\top \\left(\\frac{dA}{d\\alpha}\\right) u_0 + u_0^\\top A \\left(\\frac{du_0}{d\\alpha}\\right)\n$$\nSince $A$ is a real symmetric matrix ($A = A^\\top$), the third term can be rewritten. The term $u_0^\\top A \\frac{du_0}{d\\alpha}$ is a scalar, so it is equal to its transpose: $u_0^\\top A \\frac{du_0}{d\\alpha} = \\left(u_0^\\top A \\frac{du_0}{d\\alpha}\\right)^\\top = \\left(\\frac{du_0}{d\\alpha}\\right)^\\top A^\\top u_0 = \\left(\\frac{du_0}{d\\alpha}\\right)^\\top A u_0$. Thus, the first and third terms are identical. The expression becomes:\n$$\n\\frac{d\\lambda_0}{d\\alpha} = 2 \\left(\\frac{du_0}{d\\alpha}\\right)^\\top A u_0 + u_0^\\top \\left(\\frac{dA}{d\\alpha}\\right) u_0\n$$\nWe now substitute the eigenvalue equation $A u_0 = \\lambda_0 u_0$ into the expression:\n$$\n\\frac{d\\lambda_0}{d\\alpha} = 2 \\left(\\frac{du_0}{d\\alpha}\\right)^\\top (\\lambda_0 u_0) + u_0^\\top \\left(\\frac{dA}{d\\alpha}\\right) u_0 = 2 \\lambda_0 \\left(\\frac{du_0}{d\\alpha}\\right)^\\top u_0 + u_0^\\top \\left(\\frac{dA}{d\\alpha}\\right) u_0\n$$\nThe remaining unknown term, involving the derivative of the eigenvector, can be eliminated by differentiating the normalization condition $u_0^\\top u_0 = 1$ with respect to $\\alpha$:\n$$\n\\frac{d}{d\\alpha} \\left( u_0^\\top u_0 \\right) = \\left(\\frac{du_0}{d\\alpha}\\right)^\\top u_0 + u_0^\\top \\frac{du_0}{d\\alpha} = 2 \\left(\\frac{du_0}{d\\alpha}\\right)^\\top u_0 = \\frac{d}{d\\alpha}(1) = 0\n$$\nThis implies that $\\left(\\frac{du_0}{d\\alpha}\\right)^\\top u_0 = 0$. Substituting this result into the expression for $\\frac{d\\lambda_0}{d\\alpha}$ causes the first term to vanish:\n$$\n\\frac{d\\lambda_0}{d\\alpha} = 2 \\lambda_0 (0) + u_0^\\top \\left(\\frac{dA}{d\\alpha}\\right) u_0 = u_0^\\top \\left(\\frac{dA}{d\\alpha}\\right) u_0\n$$\nThis is the final analytic expression for the derivative of the eigenvalue. We now must find the derivative of the Hamiltonian matrix $A(\\alpha)$. Given $A(\\alpha) = T + \\mathrm{diag}(V_1(\\alpha), \\dots, V_N(\\alpha))$, and since the kinetic energy matrix $T$ is independent of $\\alpha$, we have:\n$$\n\\frac{dA}{d\\alpha} = \\frac{d}{d\\alpha} \\left(T + V(\\alpha)\\right) = \\frac{dV(\\alpha)}{d\\alpha} = \\mathrm{diag}\\left(\\frac{dV_1}{d\\alpha}, \\dots, \\frac{dV_N}{d\\alpha}\\right)\n$$\nThe potential is given by $V_i(\\alpha) = -\\frac{\\alpha}{1 + \\exp((|x_i| - R)/a)}$. Its derivative with respect to $\\alpha$ is simply:\n$$\n\\frac{dV_i}{d\\alpha} = -\\frac{1}{1 + \\exp\\left(\\frac{|x_i| - R}{a}\\right)}\n$$\nThe derivative of the eigenvalue is therefore the expectation value of the operator $\\frac{dV}{d\\alpha}$ in the state $u_0$:\n$$\n\\frac{d\\lambda_0}{d\\alpha} = \\sum_{i=1}^{N} (u_{0,i}(\\alpha))^2 \\frac{dV_i}{d\\alpha}\n$$\nwhere $u_{0,i}$ are the components of the eigenvector $u_0$.\n\nFor the numerical implementation, the fixed parameters are $N = 400$, $L = 20$, $R = 6$, and $a = 0.65$. The spatial grid $x_i$ and grid spacing $\\Delta x$ are constructed accordingly. The kinetic energy matrix $T$ is constructed as a sparse tridiagonal matrix. The potential $V(\\alpha)$ and its derivative $\\frac{dV}{d\\alpha}$ are diagonal, so they are stored as vectors of their diagonal elements. The full Hamiltonian $A(\\alpha)$ is handled using a `LinearOperator` from `scipy.sparse.linalg`, which encapsulates the matrix-vector product $A(\\alpha)v = T v + V(\\alpha) \\odot v$ (where $\\odot$ is element-wise product). This approach honors the problem's stipulation to use a method based on matrix-vector products. The smallest eigenpair $(\\lambda_0, u_0)$ is computed using `scipy.sparse.linalg.eigsh`, which implements a Lanczos-type algorithm suitable for large, sparse, symmetric matrices, with a tolerance set to $10^{-12}$.\n\nThe algorithm proceeds as follows for each test case $(\\alpha_0, h)$:\n$1$. The analytic derivative at $\\alpha_0$ is computed by first solving for the eigenvector $u_0(\\alpha_0)$ and then evaluating the quadratic form $u_0(\\alpha_0)^\\top \\frac{dA}{d\\alpha}(\\alpha_0) u_0(\\alpha_0)$.\n$2$. The finite-difference estimate is computed by solving for the eigenvalues $\\lambda_0(\\alpha_0+h)$ and $\\lambda_0(\\alpha_0-h)$ separately and then applying the central difference formula $\\frac{\\lambda_0(\\alpha_0+h) - \\lambda_0(\\alpha_0-h)}{2h}$.\n$3$. The absolute difference between the analytic and finite-difference results is calculated and stored.\n\nThis procedure is repeated for all test cases, and the final list of absolute errors is printed.",
            "answer": "```python\n# language: Python\n# version: 3.12\n# libraries:\n#   - name: numpy\n#     version: 1.23.5\n#   - name: scipy\n#     version: 1.11.4\n\nimport numpy as np\nfrom scipy.sparse import diags\nfrom scipy.sparse.linalg import eigsh, LinearOperator\n\ndef solve():\n    \"\"\"\n    Solves the problem by deriving, implementing, and comparing analytic \n    and finite-difference derivatives of a matrix eigenvalue.\n    \"\"\"\n    # Define fixed discretization and physical parameters\n    N = 400\n    L = 20.0\n    R = 6.0\n    a = 0.65\n    TOL = 1e-12  # Set tolerance tighter than the required 1e-10\n\n    # Define the test suite\n    test_cases = [\n        (2.0, 1e-2),\n        (8.0, 1e-4),\n        (12.0, 1e-6),\n        (0.5, 5e-3),\n    ]\n\n    # Set up the spatial grid\n    dx = 2 * L / (N + 1)\n    x = -L + np.arange(1, N + 1) * dx\n\n    # Construct the kinetic energy matrix T as a sparse matrix.\n    # This matrix is constant with respect to alpha.\n    diagonals_T = [-1.0, 2.0, -1.0]\n    offsets_T = [-1, 0, 1]\n    T = diags(diagonals_T, offsets_T, shape=(N, N), format='csr') / (dx**2)\n\n    # The diagonal of the derivative of the potential matrix, dV/d(alpha),\n    # is also constant with respect to alpha. Store it as a vector.\n    # dV_i/d(alpha) = -1 / (1 + exp((|x_i| - R)/a))\n    dV_dalpha_diag = -1.0 / (1.0 + np.exp((np.abs(x) - R) / a))\n\n    def get_potential_diag(alpha):\n        \"\"\"Returns the diagonal of the potential matrix V(alpha) as a vector.\"\"\"\n        # V_i(alpha) = alpha * (dV_i/d(alpha))\n        return alpha * dV_dalpha_diag\n\n    def solve_eigen_pair(alpha):\n        \"\"\"\n        Computes the smallest eigenvalue and its corresponding eigenvector for\n        the Hamiltonian A(alpha) using an iterative solver.\n        \"\"\"\n        V_diag = get_potential_diag(alpha)\n\n        # Define the matrix-vector product for A(alpha) * v\n        def matvec(v):\n            return T @ v + V_diag * v\n\n        # Create a LinearOperator to represent A(alpha) without forming it explicitly.\n        A_op = LinearOperator((N, N), matvec=matvec, dtype=np.float64)\n        \n        # Use eigsh to find the smallest algebraic eigenpair.\n        # This is an iterative Lanczos-based method.\n        # k=1: find one eigenpair.\n        # which='SA': find the one with the Smallest Algebraic value.\n        eigenvalues, eigenvectors = eigsh(A_op, k=1, which='SA', tol=TOL)\n        \n        # eigsh returns arrays, so we extract the scalar eigenvalue and vector.\n        return eigenvalues[0], eigenvectors[:, 0]\n\n    def compute_analytic_derivative(u_0):\n        \"\"\"\n        Computes the analytic derivative d(lambda)/d(alpha) using the\n        Hellmann-Feynman theorem result.\n        d(lambda)/d(alpha) = u_0^T * (dV/d(alpha)) * u_0\n        \"\"\"\n        # Since dV/d(alpha) is diagonal, the quadratic form is a simple sum.\n        return np.dot(u_0, dV_dalpha_diag * u_0)\n\n    results = []\n    for alpha_0, h in test_cases:\n        # 1. Compute the analytic derivative at alpha = alpha_0\n        # First, find the eigenvector u_0 at alpha_0.\n        _, u_0_at_alpha0 = solve_eigen_pair(alpha_0)\n        analytic_derivative = compute_analytic_derivative(u_0_at_alpha0)\n\n        # 2. Compute the finite-difference derivative at alpha = alpha_0\n        # Find the eigenvalues at alpha_0 + h and alpha_0 - h.\n        lambda_plus_h, _ = solve_eigen_pair(alpha_0 + h)\n        lambda_minus_h, _ = solve_eigen_pair(alpha_0 - h)\n        \n        # Apply the central difference formula.\n        finite_diff_derivative = (lambda_plus_h - lambda_minus_h) / (2.0 * h)\n        \n        # 3. Calculate the absolute error between the two methods.\n        error = abs(analytic_derivative - finite_diff_derivative)\n        results.append(error)\n\n    # Print the results in the specified format.\n    print(f\"[{','.join(f'{r:.15g}' for r in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}