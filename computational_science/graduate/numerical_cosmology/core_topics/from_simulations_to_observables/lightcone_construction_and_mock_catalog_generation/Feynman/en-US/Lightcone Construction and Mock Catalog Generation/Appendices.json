{
    "hands_on_practices": [
        {
            "introduction": "The first step in building a mock catalog is to populate a simulated volume with objects like dark matter halos, governed by theoretical models such as the halo mass function ($dn/dM$). This exercise provides fundamental practice in translating a continuous theoretical distribution into the expected number of discrete objects within a cosmic volume. Mastering this calculation, including the associated sampling variance, is essential for generating the basic components of any mock catalog .",
            "id": "3477518",
            "problem": "In the construction of a past lightcone for a mock halo catalog in numerical cosmology, consider a thin comoving shell of volume $\\Delta V$ within which the halo population is statistically homogeneous. The differential halo mass function is specified by the function $dn/dM$, defined as the comoving number density of halos per unit mass interval. Assume that halos more massive than a threshold mass $M_{\\min}$ are included with unit selection probability and halos below $M_{\\min}$ are excluded. Model the halo mass function in this shell by the form\n$$\n\\frac{dn}{dM} \\;=\\; A\\,\\left(\\frac{M}{M_{\\star}}\\right)^{-1} \\exp\\!\\left(-\\frac{M}{M_{\\star}}\\right)\\,\\frac{1}{M_{\\star}} \\quad ,\n$$\nwhere $A$ is a constant amplitude with units of $\\mathrm{Mpc}^{-3}$, and $M_{\\star}$ is a characteristic mass. You are given the following shell and model parameters:\n$$\n\\Delta V \\;=\\; 1.00 \\times 10^{8}\\,\\mathrm{Mpc}^{3} \\quad , \\quad A \\;=\\; 3.00 \\times 10^{-6}\\,\\mathrm{Mpc}^{-3} \\quad , \\quad M_{\\star} \\;=\\; 1.00 \\times 10^{13}\\,\\mathrm{M}_{\\odot} \\quad , \\quad M_{\\min} \\;=\\; 1.00 \\times 10^{13}\\,\\mathrm{M}_{\\odot} \\; .\n$$\nUsing only fundamental definitions of number counts in a volume and the Poisson point-process assumption for halo sampling, compute:\n1. The expected number of halos in the shell above $M_{\\min}$.\n2. The Poisson sampling variance of that number.\n\nExpress the final answer as two numbers in the order $(\\text{expected number}, \\text{Poisson variance})$ with no units. Round your answer to four significant figures.",
            "solution": "The problem requires the calculation of two quantities for a population of dark matter halos within a comoving volume shell: the expected number of halos above a minimum mass, and the Poisson sampling variance of this number. The solution will proceed in two parts, addressing each of these required calculations.\n\nFirst, we calculate the expected number of halos, which we denote as $N_{\\mathrm{exp}}$. The fundamental definition of the expected number of objects in a volume $\\Delta V$ is the product of their comoving number density, $n$, and the volume itself.\n$$\nN_{\\mathrm{exp}} = n \\times \\Delta V\n$$\nThe comoving number density, $n$, of halos with mass greater than a threshold mass $M_{\\min}$ is obtained by integrating the differential halo mass function, $\\frac{dn}{dM}$, over all relevant masses. Since halos below $M_{\\min}$ are excluded (i.e., have a selection probability of zero) and halos above $M_{\\min}$ are included with unit probability, the integration range is from $M_{\\min}$ to infinity.\n$$\nn(M > M_{\\min}) = \\int_{M_{\\min}}^{\\infty} \\frac{dn}{dM} \\, dM\n$$\nThe problem provides the functional form for the differential halo mass function:\n$$\n\\frac{dn}{dM} = A \\left(\\frac{M}{M_{\\star}}\\right)^{-1} \\exp\\left(-\\frac{M}{M_{\\star}}\\right) \\frac{1}{M_{\\star}}\n$$\nSubstituting this form into the integral for $n$:\n$$\nn = \\int_{M_{\\min}}^{\\infty} A \\left(\\frac{M}{M_{\\star}}\\right)^{-1} \\exp\\left(-\\frac{M}{M_{\\star}}\\right) \\frac{1}{M_{\\star}} \\, dM\n$$\nWe can simplify the expression inside the integral:\n$$\nn = A \\int_{M_{\\min}}^{\\infty} \\frac{M_{\\star}}{M} \\exp\\left(-\\frac{M}{M_{\\star}}\\right) \\frac{1}{M_{\\star}} \\, dM = A \\int_{M_{\\min}}^{\\infty} \\frac{1}{M} \\exp\\left(-\\frac{M}{M_{\\star}}\\right) \\, dM\n$$\nTo solve this integral, we perform a change of variables. Let $u = \\frac{M}{M_{\\star}}$. Then $M = u M_{\\star}$ and $dM = M_{\\star} du$. The limits of integration also change: when $M = M_{\\min}$, $u = \\frac{M_{\\min}}{M_{\\star}}$; when $M \\to \\infty$, $u \\to \\infty$. The integral becomes:\n$$\nn = A \\int_{M_{\\min}/M_{\\star}}^{\\infty} \\frac{1}{u M_{\\star}} \\exp(-u) \\, (M_{\\star} du) = A \\int_{M_{\\min}/M_{\\star}}^{\\infty} \\frac{\\exp(-u)}{u} \\, du\n$$\nThis integral is the definition of the first exponential integral function, $E_1(x)$, defined as:\n$$\nE_1(x) = \\int_{x}^{\\infty} \\frac{\\exp(-t)}{t} \\, dt\n$$\nThus, the number density is given by:\n$$\nn = A \\, E_1\\left(\\frac{M_{\\min}}{M_{\\star}}\\right)\n$$\nNow, we can find the expected number of halos, $N_{\\mathrm{exp}}$, by multiplying by the volume $\\Delta V$:\n$$\nN_{\\mathrm{exp}} = A \\, E_1\\left(\\frac{M_{\\min}}{M_{\\star}}\\right) \\, \\Delta V\n$$\nWe are given the following parameter values:\n$$\n\\Delta V = 1.00 \\times 10^{8}\\,\\mathrm{Mpc}^{3} \\quad , \\quad A = 3.00 \\times 10^{-6}\\,\\mathrm{Mpc}^{-3} \\quad , \\quad M_{\\star} = 1.00 \\times 10^{13}\\,\\mathrm{M}_{\\odot} \\quad , \\quad M_{\\min} = 1.00 \\times 10^{13}\\,\\mathrm{M}_{\\odot}\n$$\nThe ratio of the minimum mass to the characteristic mass is:\n$$\n\\frac{M_{\\min}}{M_{\\star}} = \\frac{1.00 \\times 10^{13}\\,\\mathrm{M}_{\\odot}}{1.00 \\times 10^{13}\\,\\mathrm{M}_{\\odot}} = 1\n$$\nSubstituting the numerical values into the expression for $N_{\\mathrm{exp}}$:\n$$\nN_{\\mathrm{exp}} = (3.00 \\times 10^{-6}\\,\\mathrm{Mpc}^{-3}) \\times E_1(1) \\times (1.00 \\times 10^{8}\\,\\mathrm{Mpc}^{3})\n$$\n$$\nN_{\\mathrm{exp}} = 3.00 \\times 1.00 \\times 10^{-6} \\times 10^{8} \\times E_1(1) = 300 \\, E_1(1)\n$$\nThe value of the exponential integral function at $1$ is a standard mathematical constant, $E_1(1) \\approx 0.219383934$.\n$$\nN_{\\mathrm{exp}} \\approx 300 \\times 0.219383934 \\approx 65.81518\n$$\nRounding to four significant figures, we get:\n$$\nN_{\\mathrm{exp}} \\approx 65.82\n$$\nSecond, we calculate the Poisson sampling variance of the number of halos. The problem explicitly states that we should assume a Poisson point-process for halo sampling. A fundamental property of a random variable $K$ that follows a Poisson distribution with mean (expected value) $\\lambda$ is that its variance is also equal to $\\lambda$. That is, if $K \\sim \\mathrm{Poisson}(\\lambda)$, then $E[K] = \\lambda$ and $\\mathrm{Var}(K) = \\lambda$.\nIn this problem, the number of halos, $N$, in the volume $\\Delta V$ is assumed to be a Poisson random variable. Its expected value is $E[N] = N_{\\mathrm{exp}}$. Therefore, its variance is:\n$$\n\\mathrm{Var}(N) = E[N] = N_{\\mathrm{exp}}\n$$\nUsing the value for $N_{\\mathrm{exp}}$ we just calculated:\n$$\n\\mathrm{Var}(N) \\approx 65.81518\n$$\nRounding to four significant figures, the variance is:\n$$\n\\mathrm{Var}(N) \\approx 65.82\n$$\nThe requested answer is the pair $(\\text{expected number}, \\text{Poisson variance})$. Both values are approximately $65.82$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n65.82 & 65.82\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "A lightcone mock simulates what an observer sees, meaning galaxy positions are mapped from redshift and angle to comoving coordinates using an assumed background cosmology. This practice explores the Alcock-Paczynski effect, a critical systematic in large-scale structure analysis, by quantifying how discrepancies between the universe's true cosmology and the observer's fiducial cosmology distort inferred distances. Understanding this effect is crucial for accurately interpreting measurements of the cosmic distance scale, such as from Baryon Acoustic Oscillations (BAO) .",
            "id": "3477460",
            "problem": "Consider a spatially homogeneous and isotropic universe described by the Friedmann-Lemaître-Robertson-Walker (FLRW) metric, with matter density parameter $\\Omega_m$, cosmological constant density parameter $\\Omega_\\Lambda$, and reduced Hubble constant $h$ such that $H_0 = 100\\,h$ in units of $\\mathrm{km\\,s^{-1}\\,Mpc^{-1}}$. Let $\\Omega_k = 1 - \\Omega_m - \\Omega_\\Lambda$ denote the curvature density parameter. In this setting, the expansion rate as a function of redshift $z$ is defined by the dimensionless function $E(z)$ satisfying the Friedmann equation, and the comoving radial distance $\\chi(z)$ is defined by a line-of-sight integral that depends on $E(z)$. The transverse comoving distance $D_M(z)$ is related to $\\chi(z)$ through the spatial curvature, and the line-of-sight comoving distance per unit redshift is given by $c/H(z)$, where $c$ is the speed of light.\n\nIn the construction of lightcone mock catalogs, observed angles and redshifts are mapped to three-dimensional comoving coordinates using a chosen background cosmology $(\\Omega_m^{\\mathrm{fid}}, \\Omega_\\Lambda^{\\mathrm{fid}}, h^{\\mathrm{fid}})$. If the true background cosmology is $(\\Omega_m^{\\mathrm{true}}, \\Omega_\\Lambda^{\\mathrm{true}}, h^{\\mathrm{true}})$, then the inferred Baryon Acoustic Oscillation (BAO) distances measured from the mocks will be systematically biased relative to their true values because the mapping depends on the cosmology used to construct $\\chi(z)$ and $H(z)$.\n\nStarting from the FLRW metric and the Friedmann equation, and without introducing any shortcut formulas, implement a program that:\n\n- Computes the dimensionless expansion rate $E(z)$ from the Friedmann equation for arbitrary $(\\Omega_m, \\Omega_\\Lambda, h)$, including nonzero spatial curvature $\\Omega_k$, and then computes $H(z) = H_0 E(z)$ with $H_0 = 100\\,h$ in $\\mathrm{km\\,s^{-1}\\,Mpc^{-1}}$.\n- Computes the comoving radial distance $\\chi(z) = \\dfrac{c}{H_0} \\int_0^z \\dfrac{dz'}{E(z')}$ in $\\mathrm{Mpc}$, where the speed of light $c$ must be taken as $c = 299792.458$ in $\\mathrm{km\\,s^{-1}}$.\n- Computes the transverse comoving distance $D_M(z)$ from $\\chi(z)$ with full curvature dependence. For $\\Omega_k = 0$, $D_M(z)$ equals $\\chi(z)$. For $\\Omega_k \\neq 0$, use the appropriate curved-space relation based on the standard curvature-dependent geodesic distance mapping, expressed in terms of the curvature-normalized trigonometric or hyperbolic functions.\n- Defines the inferred transverse BAO distance bias at redshift $z$ as the fractional difference between the transverse comoving distance computed with the fiducial cosmology and that computed with the true cosmology, namely $\\Delta_\\perp(z) = \\dfrac{D_M^{\\mathrm{fid}}(z)}{D_M^{\\mathrm{true}}(z)} - 1$.\n- Defines the inferred radial BAO distance bias at redshift $z$ as the fractional difference between the line-of-sight comoving distance per unit redshift computed with the fiducial cosmology and that computed with the true cosmology, namely $\\Delta_\\parallel(z) = \\dfrac{\\left(c/H^{\\mathrm{fid}}(z)\\right)}{\\left(c/H^{\\mathrm{true}}(z)\\right)} - 1$.\n\nAll intermediate distances must be computed in $\\mathrm{Mpc}$. The final outputs must be dimensionless decimal numbers. For numerical integration, you must use a method whose accuracy is controlled by absolute and relative tolerances appropriate for advanced graduate-level numerical cosmology work; ensure numerical robustness across the provided test suite.\n\nTest Suite:\n- Case $1$ (happy path, small deviations in all parameters): $\\Omega_m^{\\mathrm{true}} = 0.315$, $\\Omega_\\Lambda^{\\mathrm{true}} = 0.685$, $h^{\\mathrm{true}} = 0.674$; $\\Omega_m^{\\mathrm{fid}} = 0.310$, $\\Omega_\\Lambda^{\\mathrm{fid}} = 0.690$, $h^{\\mathrm{fid}} = 0.680$; $z = 0.8$.\n- Case $2$ (open geometry in fiducial cosmology): $\\Omega_m^{\\mathrm{true}} = 0.315$, $\\Omega_\\Lambda^{\\mathrm{true}} = 0.685$, $h^{\\mathrm{true}} = 0.674$; $\\Omega_m^{\\mathrm{fid}} = 0.280$, $\\Omega_\\Lambda^{\\mathrm{fid}} = 0.680$, $h^{\\mathrm{fid}} = 0.670$; $z = 1.2$.\n- Case $3$ (closed geometry in fiducial cosmology): $\\Omega_m^{\\mathrm{true}} = 0.315$, $\\Omega_\\Lambda^{\\mathrm{true}} = 0.685$, $h^{\\mathrm{true}} = 0.674$; $\\Omega_m^{\\mathrm{fid}} = 0.350$, $\\Omega_\\Lambda^{\\mathrm{fid}} = 0.680$, $h^{\\mathrm{fid}} = 0.700$; $z = 0.3$.\n- Case $4$ (zero curvature, pure $h$ offset): $\\Omega_m^{\\mathrm{true}} = 0.315$, $\\Omega_\\Lambda^{\\mathrm{true}} = 0.685$, $h^{\\mathrm{true}} = 0.674$; $\\Omega_m^{\\mathrm{fid}} = 0.315$, $\\Omega_\\Lambda^{\\mathrm{fid}} = 0.685$, $h^{\\mathrm{fid}} = 0.700$; $z = 0.8$.\n- Case $5$ (boundary, no bias expected): $\\Omega_m^{\\mathrm{true}} = 0.315$, $\\Omega_\\Lambda^{\\mathrm{true}} = 0.685$, $h^{\\mathrm{true}} = 0.674$; $\\Omega_m^{\\mathrm{fid}} = 0.315$, $\\Omega_\\Lambda^{\\mathrm{fid}} = 0.685$, $h^{\\mathrm{fid}} = 0.674$; $z = 0.8$.\n\nYour program must produce the biases $\\Delta_\\perp(z)$ and $\\Delta_\\parallel(z)$ for each case, rounded to six decimal places, and aggregate the results into a single line of output containing the values in the order of the test suite as a comma-separated list enclosed in square brackets. The required final output format is:\n$[\\Delta_\\perp^{(1)}, \\Delta_\\parallel^{(1)}, \\Delta_\\perp^{(2)}, \\Delta_\\parallel^{(2)}, \\Delta_\\perp^{(3)}, \\Delta_\\parallel^{(3)}, \\Delta_\\perp^{(4)}, \\Delta_\\parallel^{(4)}, \\Delta_\\perp^{(5)}, \\Delta_\\parallel^{(5)}]$.\nAll outputs must be dimensionless decimals rounded to six decimal places, without any percentage sign.",
            "solution": "The problem as stated is subjected to validation.\n\n### Step 1: Extract Givens\n- **Universe Model**: Spatially homogeneous and isotropic, described by the Friedmann-Lemaître-Robertson-Walker (FLRW) metric.\n- **Cosmological Parameters**: Matter density parameter $\\Omega_m$, cosmological constant density parameter $\\Omega_\\Lambda$, and reduced Hubble constant $h$.\n- **Hubble Constant**: $H_0 = 100\\,h$ in units of $\\mathrm{km\\,s^{-1}\\,Mpc^{-1}}$.\n- **Curvature Density Parameter**: $\\Omega_k = 1 - \\Omega_m - \\Omega_\\Lambda$.\n- **Dimensionless Expansion Rate**: $E(z)$, defined by the Friedmann equation.\n- **Hubble Parameter at Redshift z**: $H(z) = H_0 E(z)$.\n- **Speed of Light**: $c = 299792.458$ in $\\mathrm{km\\,s^{-1}}$.\n- **Comoving Radial Distance**: $\\chi(z) = \\dfrac{c}{H_0} \\int_0^z \\dfrac{dz'}{E(z')}$ in $\\mathrm{Mpc}$.\n- **Transverse Comoving Distance**: $D_M(z)$, related to $\\chi(z)$ through spatial curvature.\n- **Line-of-Sight Comoving Distance per Unit Redshift**: $c/H(z)$.\n- **Cosmologies**: True cosmology $(\\Omega_m^{\\mathrm{true}}, \\Omega_\\Lambda^{\\mathrm{true}}, h^{\\mathrm{true}})$ and a fiducial (assumed) cosmology $(\\Omega_m^{\\mathrm{fid}}, \\Omega_\\Lambda^{\\mathrm{fid}}, h^{\\mathrm{fid}})$.\n- **Transverse BAO Bias**: $\\Delta_\\perp(z) = \\dfrac{D_M^{\\mathrm{fid}}(z)}{D_M^{\\mathrm{true}}(z)} - 1$.\n- **Radial BAO Bias**: $\\Delta_\\parallel(z) = \\dfrac{\\left(c/H^{\\mathrm{fid}}(z)\\right)}{\\left(c/H^{\\mathrm{true}}(z)\\right)} - 1$.\n- **Test Suite**:\n  - Case $1$: $(\\Omega_m^{\\mathrm{true}}, \\Omega_\\Lambda^{\\mathrm{true}}, h^{\\mathrm{true}}) = (0.315, 0.685, 0.674)$; $(\\Omega_m^{\\mathrm{fid}}, \\Omega_\\Lambda^{\\mathrm{fid}}, h^{\\mathrm{fid}}) = (0.310, 0.690, 0.680)$; $z = 0.8$.\n  - Case $2$: $(\\Omega_m^{\\mathrm{true}}, \\Omega_\\Lambda^{\\mathrm{true}}, h^{\\mathrm{true}}) = (0.315, 0.685, 0.674)$; $(\\Omega_m^{\\mathrm{fid}}, \\Omega_\\Lambda^{\\mathrm{fid}}, h^{\\mathrm{fid}}) = (0.280, 0.680, 0.670)$; $z = 1.2$.\n  - Case $3$: $(\\Omega_m^{\\mathrm{true}}, \\Omega_\\Lambda^{\\mathrm{true}}, h^{\\mathrm{true}}) = (0.315, 0.685, 0.674)$; $(\\Omega_m^{\\mathrm{fid}}, \\Omega_\\Lambda^{\\mathrm{fid}}, h^{\\mathrm{fid}}) = (0.350, 0.680, 0.700)$; $z = 0.3$.\n  - Case $4$: $(\\Omega_m^{\\mathrm{true}}, \\Omega_\\Lambda^{\\mathrm{true}}, h^{\\mathrm{true}}) = (0.315, 0.685, 0.674)$; $(\\Omega_m^{\\mathrm{fid}}, \\Omega_\\Lambda^{\\mathrm{fid}}, h^{\\mathrm{fid}}) = (0.315, 0.685, 0.700)$; $z = 0.8$.\n  - Case $5$: $(\\Omega_m^{\\mathrm{true}}, \\Omega_\\Lambda^{\\mathrm{true}}, h^{\\mathrm{true}}) = (0.315, 0.685, 0.674)$; $(\\Omega_m^{\\mathrm{fid}}, \\Omega_\\Lambda^{\\mathrm{fid}}, h^{\\mathrm{fid}}) = (0.315, 0.685, 0.674)$; $z = 0.8$.\n- **Output Format**: A single-line comma-separated list of $[\\Delta_\\perp^{(1)}, \\Delta_\\parallel^{(1)}, \\dots]$ rounded to six decimal places.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is based on the standard model of cosmology ($\\Lambda$CDM) and uses the canonical FLRW metric and Friedmann equation. The definitions for comoving distances and BAO biases (which are a manifestation of the Alcock-Paczynski effect) are standard in the field. The provided value for the speed of light $c$ is correct. The cosmological parameters are physically realistic. The problem is scientifically sound.\n2.  **Well-Posed**: The problem provides all necessary inputs (cosmological parameters, redshift) and clear, unambiguous definitions for the quantities to be computed. The use of numerical integration is expected for the comoving distance calculation, which does not have a general analytic solution. A unique solution exists for each test case. The problem is well-posed.\n3.  **Objective**: The problem is stated in precise, quantitative, and objective terms, free of any subjective or opinion-based content.\n4.  **Completeness and Consistency**: All required equations, constants, and parameters are provided. The definitions are internally consistent and consistent with established cosmological formalisms. For example, the definition $\\Omega_k = 1 - \\Omega_m - \\Omega_\\Lambda$ is standard.\n5.  **No Other Flaws**: The problem is non-trivial, directly relevant to the specified topic of mock catalog generation in numerical cosmology, and verifiable through calculation.\n\n### Step 3: Verdict and Action\nThe problem is valid. A reasoned solution will be provided.\n\n### Solution Derivation\nThe problem requires the calculation of systematic biases in Baryon Acoustic Oscillation (BAO) distance measurements when an incorrect (fiducial) cosmology is assumed for data analysis. This is a classic problem in observational cosmology, related to the Alcock-Paczynski effect. The solution involves computing cosmological distances for two different sets of cosmological parameters.\n\n**1. The Friedmann Equation and Hubble Parameter**\n\nThe expansion of a homogeneous and isotropic universe is governed by the Friedmann equation. For a universe containing matter (non-relativistic), a cosmological constant $\\Lambda$, and allowing for spatial curvature, the equation is:\n$$\nH^2(z) = H_0^2 \\left[ \\Omega_{m,0} (1+z)^3 + \\Omega_{k,0} (1+z)^2 + \\Omega_{\\Lambda,0} \\right]\n$$\nwhere $H(z)$ is the Hubble parameter at redshift $z$, $H_0$ is the Hubble constant (the value of $H(z)$ at $z=0$), and $\\Omega_{m,0}$, $\\Omega_{k,0}$, and $\\Omega_{\\Lambda,0}$ are the present-day density parameters for matter, curvature, and the cosmological constant, respectively. Hereafter, the subscript $0$ is dropped for simplicity, so $\\Omega_m \\equiv \\Omega_{m,0}$, etc.\n\nThe problem defines the dimensionless expansion rate $E(z)$ as $H(z)/H_0$. Thus:\n$$\nE(z) = \\sqrt{\\Omega_m (1+z)^3 + \\Omega_k (1+z)^2 + \\Omega_\\Lambda}\n$$\nThe curvature parameter is given by the closure relation $\\Omega_k = 1 - \\Omega_m - \\Omega_\\Lambda$. Given a set of cosmological parameters $(\\Omega_m, \\Omega_\\Lambda, h)$, we can compute $H(z)$ for any redshift $z$ using $H_0 = 100\\,h \\, \\mathrm{km\\,s^{-1}\\,Mpc^{-1}}$.\n\n**2. Comoving Distances**\n\nIn an expanding universe, it is convenient to work in comoving coordinates, which factor out the cosmic expansion.\n\n**Comoving Radial Distance, $\\chi(z)$**: This is the total comoving distance along the line of sight to an object at redshift $z$. It is calculated by integrating the infinitesimal comoving distance element $d\\chi = c \\, dt / a(t)$ from the time of observation ($t_0$) to the time of emission ($t$). Expressing this in terms of redshift $z$, where $1+z = a(t_0)/a(t)$ and $dz = - (1+z) H(z) dt$, we get:\n$$\n\\chi(z) = \\int_0^z \\frac{c}{H(z')} dz' = \\frac{c}{H_0} \\int_0^z \\frac{dz'}{E(z')}\n$$\nThe speed of light $c$ must be given in $\\mathrm{km\\,s^{-1}}$ to match the units of $H_0$, resulting in $\\chi(z)$ in units of $\\mathrm{Mpc}$. This integral generally does not have an analytical solution and must be computed numerically.\n\n**Transverse Comoving Distance, $D_M(z)$**: This distance relates an object's physical transverse size to its angular size on the sky. Its definition depends on the spatial curvature of the universe. For a given comoving radial distance $\\chi$, the transverse comoving distance is:\n$$\nD_M(z) = \\begin{cases}\n\\dfrac{c}{H_0\\sqrt{\\Omega_k}} \\sinh\\left(\\sqrt{\\Omega_k} \\dfrac{H_0 \\chi(z)}{c}\\right) & \\text{if } \\Omega_k > 0 \\text{ (open)} \\\\\n\\chi(z) & \\text{if } \\Omega_k = 0 \\text{ (flat)} \\\\\n\\dfrac{c}{H_0\\sqrt{-\\Omega_k}} \\sin\\left(\\sqrt{-\\Omega_k} \\dfrac{H_0 \\chi(z)}{c}\\right) & \\text{if } \\Omega_k < 0 \\text{ (closed)}\n\\end{cases}\n$$\nNoting that $\\dfrac{H_0 \\chi(z)}{c} = \\int_0^z \\frac{dz'}{E(z')}$, these can be written more directly in terms of the integral.\n\n**3. BAO Distance Biases**\n\nWhen constructing a mock catalog, galaxy positions are mapped from observed angles and redshifts $(\\theta, \\phi, z)$ to Cartesian comoving coordinates $(x,y,z)$ using a fiducial cosmology. If this fiducial cosmology $(\\Omega_m^{\\mathrm{fid}}, \\Omega_\\Lambda^{\\mathrm{fid}}, h^{\\mathrm{fid}})$ differs from the true cosmology $(\\Omega_m^{\\mathrm{true}}, \\Omega_\\Lambda^{\\mathrm{true}}, h^{\\mathrm{true}})$ in which the mock universe \"lives\", the inferred distances will be biased.\n\n**Transverse Bias, $\\Delta_\\perp(z)$**: This bias affects distances measured perpendicular to the line of sight. It is quantified by the ratio of the transverse comoving distances:\n$$\n\\Delta_\\perp(z) = \\frac{D_M^{\\mathrm{fid}}(z)}{D_M^{\\mathrm{true}}(z)} - 1\n$$\n\n**Radial Bias, $\\Delta_\\parallel(z)$**: This bias affects distances measured along the line of sight. The fundamental quantity is the comoving distance per unit redshift, $d\\chi/dz = c/H(z)$. The bias is the fractional difference in this quantity:\n$$\n\\Delta_\\parallel(z) = \\frac{ (c/H^{\\mathrm{fid}}(z)) }{ (c/H^{\\mathrm{true}}(z)) } - 1 = \\frac{H^{\\mathrm{true}}(z)}{H^{\\mathrm{fid}}(z)} - 1\n$$\nThis simplifies to a ratio of the Hubble parameters at the given redshift $z$.\n\n**4. Computational Algorithm**\n\nThe solution requires implementing a function that, for a given cosmology $(\\Omega_m, \\Omega_\\Lambda, h)$ and redshift $z$:\n1.  Calculates the derived parameters $H_0 = 100\\,h$ and $\\Omega_k = 1 - \\Omega_m - \\Omega_\\Lambda$.\n2.  Defines the integrand $f(z') = 1/E(z') = 1/\\sqrt{\\Omega_m (1+z')^3 + \\Omega_k (1+z')^2 + \\Omega_\\Lambda}$.\n3.  Numerically computes the integral $I(z) = \\int_0^z f(z') dz'$ with high precision.\n4.  Calculates the comoving radial distance $\\chi(z) = (c/H_0) I(z)$.\n5.  Calculates the transverse comoving distance $D_M(z)$ using the appropriate formula based on the sign of $\\Omega_k$.\n6.  Calculates the Hubble parameter $H(z) = H_0 E(z)$.\n7.  Returns the required distances, $D_M(z)$ and $H(z)$.\n\nThis function is then called twice for each test case: once with the `true` parameters and once with the `fiducial` parameters. The results are used to compute $\\Delta_\\perp(z)$ and $\\Delta_\\parallel(z)$ as defined above. The final results are collected, formatted to six decimal places, and printed. The numerical integration will be performed using `scipy.integrate.quad`, with absolute and relative tolerances set to ensure high accuracy (e.g., $10^{-12}$).",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import quad\n\ndef solve():\n    \"\"\"\n    Solves the cosmological distance bias problem for the given test suite.\n    \"\"\"\n    \n    # Speed of light in km/s as specified in the problem\n    C_KM_S = 299792.458\n\n    test_cases = [\n        # Case 1 (happy path, small deviations)\n        {'true': {'omega_m': 0.315, 'omega_lambda': 0.685, 'h': 0.674},\n         'fid': {'omega_m': 0.310, 'omega_lambda': 0.690, 'h': 0.680},\n         'z': 0.8},\n        # Case 2 (open fiducial cosmology)\n        {'true': {'omega_m': 0.315, 'omega_lambda': 0.685, 'h': 0.674},\n         'fid': {'omega_m': 0.280, 'omega_lambda': 0.680, 'h': 0.670},\n         'z': 1.2},\n        # Case 3 (closed fiducial cosmology)\n        {'true': {'omega_m': 0.315, 'omega_lambda': 0.685, 'h': 0.674},\n         'fid': {'omega_m': 0.350, 'omega_lambda': 0.680, 'h': 0.700},\n         'z': 0.3},\n        # Case 4 (pure h offset, zero curvature)\n        {'true': {'omega_m': 0.315, 'omega_lambda': 0.685, 'h': 0.674},\n         'fid': {'omega_m': 0.315, 'omega_lambda': 0.685, 'h': 0.700},\n         'z': 0.8},\n        # Case 5 (boundary, no bias expected)\n        {'true': {'omega_m': 0.315, 'omega_lambda': 0.685, 'h': 0.674},\n         'fid': {'omega_m': 0.315, 'omega_lambda': 0.685, 'h': 0.674},\n         'z': 0.8},\n    ]\n\n    def compute_cosmological_quantities(omega_m, omega_lambda, h, z):\n        \"\"\"\n        Computes H(z) and D_M(z) for a given cosmology and redshift.\n\n        Args:\n            omega_m (float): Matter density parameter.\n            omega_lambda (float): Cosmological constant density parameter.\n            h (float): Reduced Hubble constant.\n            z (float): Redshift.\n\n        Returns:\n            tuple: A tuple containing (D_M(z), H(z)).\n        \"\"\"\n        H0 = 100.0 * h  # Hubble constant in km/s/Mpc\n        omega_k = 1.0 - omega_m - omega_lambda\n\n        def e_inv(zp):\n            # The integrand 1/E(z')\n            e_squared = omega_m * (1 + zp)**3 + omega_k * (1 + zp)**2 + omega_lambda\n            return 1.0 / np.sqrt(e_squared)\n\n        # Numerically integrate 1/E(z') from 0 to z\n        # High precision tolerances as required for numerical cosmology work\n        integral_val, _ = quad(e_inv, 0, z, epsabs=1e-12, epsrel=1e-12)\n\n        # Calculate D_M(z) based on curvature\n        # Small tolerance for checking if the universe is flat\n        if abs(omega_k) < 1e-9:\n            # Flat geometry: Omega_k = 0\n            # D_M(z) = chi(z)\n            chi = (C_KM_S / H0) * integral_val\n            D_M = chi\n        else:\n            # The Hubble radius divided by sqrt(|Omega_k|)\n            d_h_sqrt_k = (C_KM_S / H0) / np.sqrt(abs(omega_k))\n            # Argument for sinh/sin function\n            arg = np.sqrt(abs(omega_k)) * integral_val\n\n            if omega_k > 0:\n                # Open geometry: Omega_k > 0\n                D_M = d_h_sqrt_k * np.sinh(arg)\n            else: # omega_k < 0\n                # Closed geometry: Omega_k < 0\n                D_M = d_h_sqrt_k * np.sin(arg)\n\n        # Calculate H(z)\n        Ez = 1.0 / e_inv(z)\n        Hz = H0 * Ez\n        \n        return D_M, Hz\n\n    results = []\n    for case in test_cases:\n        p_true = case['true']\n        p_fid = case['fid']\n        z = case['z']\n\n        Dm_true, H_true = compute_cosmological_quantities(\n            p_true['omega_m'], p_true['omega_lambda'], p_true['h'], z\n        )\n        \n        Dm_fid, H_fid = compute_cosmological_quantities(\n            p_fid['omega_m'], p_fid['omega_lambda'], p_fid['h'], z\n        )\n\n        # Calculate biases\n        delta_perp = (Dm_fid / Dm_true - 1.0) if Dm_true != 0 else 0\n        delta_para = (H_true / H_fid - 1.0) if H_fid != 0 else 0\n\n        # Format and append results\n        results.append(f\"{delta_perp:.6f}\")\n        results.append(f\"{delta_para:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The ultimate purpose of mock catalogs is to interpret real observational data and test the validity of cosmological models. A key application is assessing whether a model's predictions for summary statistics, like the galaxy power spectrum, are consistent with measurements from a survey. This exercise guides you through this final stage of analysis by performing a rigorous goodness-of-fit test, using the full covariance matrix from mocks to compute a $\\chi^2$ statistic and determine if a model is statistically acceptable .",
            "id": "3477572",
            "problem": "A binned summary statistic of galaxies measured from a survey lightcone is compared to predictions from a mock catalog generator. Under the Central Limit Theorem, the binned summary vector is well approximated by a multivariate normal distribution with a mean equal to the mock prediction and a covariance estimated from an ensemble of independent lightcone mock realizations. This setting is common in numerical cosmology for assessing the goodness of fit of mock catalogs to data. Your task is to implement a program that, given several test cases of data and mock summary vectors along with their full covariance matrices estimated from mock catalogs, constructs a scalar goodness-of-fit diagnostic derived from the multivariate Gaussian log-likelihood, uses a bias-corrected inverse covariance when appropriate, and returns the associated tail probability under the null hypothesis and a boolean decision for acceptance or rejection at a specified significance level.\n\nStart from the following valid bases: the multivariate normal distribution for binned summary statistics, properties of covariance matrices, and the statistical definition of degrees of freedom. Do not use any shortcut formulas provided to you in this statement; instead, derive the necessary expressions from first principles in your solution. Your algorithm must:\n- Use the full covariance matrix for each test case.\n- Invert the covariance robustly. If the covariance is not numerically positive definite, regularize it by adding a small diagonal ridge until a stable Cholesky factorization is obtained. If a Cholesky factorization cannot be obtained after reasonable attempts, fall back to a Moore–Penrose pseudoinverse.\n- When the covariance is estimated from a finite number of independent mocks $N_s$, use the Hartlap correction to debias the inverse covariance when $N_s > n_b + 2$, where $n_b$ is the number of bins. Specifically, if applicable, multiply the inverse of the sample covariance by the factor $\\alpha = \\dfrac{N_s - n_b - 2}{N_s - 1}$; otherwise, do not apply this correction and rely on the regularized inverse described above.\n- Compute the scalar diagnostic implied by the multivariate Gaussian log-likelihood and the corresponding upper-tail probability (survival function) under the null hypothesis that the mock is correct. Use the number of bins $n_b$ as the degrees of freedom since the mock prediction is treated as fixed in this comparison.\n- Decide whether the fit is acceptable using a significance level of $0.05$. Report a boolean that is $True$ when the upper-tail probability is greater than or equal to $0.05$, and $False$ otherwise.\n\nTest suite specification:\nFor each test case, the binned summary vector of the survey data $\\boldsymbol{d}$ and mock prediction $\\boldsymbol{m}$ have $n_b = 6$ bins. The full covariance $\\boldsymbol{C}$ is specified via a correlation model and bin variances. Define a vector of per-bin standard deviations $\\boldsymbol{s}$ and a correlation coefficient $\\rho$, and construct\n$$\nC_{ij} = \\rho^{|i-j|} s_i s_j \\quad \\text{for } i,j \\in \\{1,\\dots,6\\}.\n$$\nThis yields a symmetric covariance with exponentially decaying correlations. For each test case, $N_s$ denotes the number of independent lightcone mocks used to estimate the covariance. The units for the summary statistics are not needed for the final outputs, which are dimensionless diagnostics, but for context the values may correspond to a power spectrum monopole in $(\\mathrm{Mpc}/h)^3$ across wavenumber bins in $h/\\mathrm{Mpc}$. The program does not need to use the physical units.\n\nProvide the following four test cases:\n- Case A (happy path, well-conditioned covariance and many mocks):\n  - $N_s = 500$\n  - $\\boldsymbol{s} = [50, 45, 40, 38, 35, 33]$\n  - $\\rho = 0.35$\n  - $\\boldsymbol{d} = [1000, 850, 720, 600, 500, 420]$\n  - $\\boldsymbol{m} = [980, 860, 710, 610, 510, 430]$\n- Case B (boundary condition, nearly singular covariance and too few mocks for Hartlap):\n  - $N_s = 8$\n  - $\\boldsymbol{s} = [30, 30, 30, 30, 30, 30]$\n  - $\\rho = 0.99$\n  - $\\boldsymbol{d} = [500, 480, 470, 460, 450, 440]$\n  - $\\boldsymbol{m} = [520, 500, 490, 480, 470, 460]$\n- Case C (edge case, perfect fit):\n  - $N_s = 300$\n  - $\\boldsymbol{s} = [20, 18, 17, 16, 15, 14]$\n  - $\\rho = 0.20$\n  - $\\boldsymbol{d} = [600, 550, 510, 480, 450, 420]$\n  - $\\boldsymbol{m} = [600, 550, 510, 480, 450, 420]$\n- Case D (poor fit, large residuals):\n  - $N_s = 100$\n  - $\\boldsymbol{s} = [25, 25, 25, 25, 25, 25]$\n  - $\\rho = 0.10$\n  - $\\boldsymbol{d} = [800, 820, 790, 810, 780, 805]$\n  - $\\boldsymbol{m} = [700, 940, 710, 900, 670, 935]$\n\nYour program should process these four cases in order. For each case, compute:\n- The scalar diagnostic from the multivariate Gaussian log-likelihood.\n- The degrees of freedom $n_b$.\n- The upper-tail probability under the null hypothesis.\n- The boolean decision at a significance level of $0.05$.\n\nFinal output format:\nYour program should produce a single line of output containing a list of results for the four cases. Each case’s result must be a list of four values in the order $[\\text{diagnostic}, \\text{dof}, \\text{p-value}, \\text{boolean}]$. Floats should be rounded to six decimal places, the integer degrees of freedom should be exact, and the boolean should be $True$ or $False$. For example, the output must look like:\n$[[x_1, \\text{dof}, p_1, b_1],[x_2, \\text{dof}, p_2, b_2],[x_3, \\text{dof}, p_3, b_3],[x_4, \\text{dof}, p_4, b_4]]$\nwhere $x_i$ and $p_i$ are floats rounded to six decimal places, $\\text{dof}$ is the integer $6$, and $b_i$ is a boolean.",
            "solution": "The problem requires the implementation of a statistical test to assess the goodness of fit between an observed data vector and a model prediction, a common task in cosmological data analysis. The foundation of this test is the assumption that the data vector, representing binned summary statistics, follows a multivariate normal distribution.\n\nFirst, let us establish the theoretical framework. The probability density function (PDF) for an $n_b$-dimensional data vector $\\boldsymbol{d}$ under the multivariate normal hypothesis, with mean vector $\\boldsymbol{\\mu}$ and covariance matrix $\\boldsymbol{C}$, is given by:\n$$\nP(\\boldsymbol{d} | \\boldsymbol{\\mu}, \\boldsymbol{C}) = \\frac{1}{(2\\pi)^{n_b/2} \\det(\\boldsymbol{C})^{1/2}} \\exp\\left( -\\frac{1}{2} (\\boldsymbol{d}-\\boldsymbol{\\mu})^T \\boldsymbol{C}^{-1} (\\boldsymbol{d}-\\boldsymbol{\\mu}) \\right)\n$$\nThe log-likelihood, $\\mathcal{L} = \\ln P$, is therefore:\n$$\n\\mathcal{L}(\\boldsymbol{\\mu}, \\boldsymbol{C} | \\boldsymbol{d}) = -\\frac{n_b}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det(\\boldsymbol{C})) - \\frac{1}{2} (\\boldsymbol{d}-\\boldsymbol{\\mu})^T \\boldsymbol{C}^{-1} (\\boldsymbol{d}-\\boldsymbol{\\mu})\n$$\nIn goodness-of-fit testing, we are interested in how well the data $\\boldsymbol{d}$ conform to the model, which provides a fixed prediction for the mean, $\\boldsymbol{\\mu} = \\boldsymbol{m}$. The term in the exponent of the likelihood function quantifies the squared distance between the data and the model, weighted by the inverse covariance. This quadratic form is the Mahalanobis distance squared, and it defines the chi-squared ($\\chi^2$) statistic:\n$$\n\\chi^2 = (\\boldsymbol{d}-\\boldsymbol{m})^T \\boldsymbol{C}^{-1} (\\boldsymbol{d}-\\boldsymbol{m})\n$$\nThis $\\chi^2$ value is the required scalar diagnostic. Under the null hypothesis that the data $\\boldsymbol{d}$ are a random realization from the distribution $\\mathcal{N}(\\boldsymbol{m}, \\boldsymbol{C})$, the $\\chi^2$ statistic follows a chi-squared distribution with $n_b$ degrees of freedom. The number of degrees of freedom is equal to the dimensionality of the data vector, $n_b$, because the mean vector $\\boldsymbol{m}$ is a fixed prediction from the model and is not fitted to the data $\\boldsymbol{d}$. For this problem, $n_b=6$.\n\nThe covariance matrix $\\boldsymbol{C}$ is itself estimated from a finite sample of $N_s$ mock realizations. A sample covariance matrix is an unbiased estimator of the true covariance, but its inverse, $(\\hat{\\boldsymbol{C}})^{-1}$, is a biased estimator of the true inverse, $\\boldsymbol{C}^{-1}$. To correct for this bias, Anderson (1963) and Hartlap, Simon & Schneider (2007) showed that the inverse sample covariance matrix should be rescaled by a factor $\\alpha$. This correction is applicable when the number of samples $N_s$ is greater than the number of data bins $n_b$ plus two, i.e., $N_s > n_b + 2$. The unbiased precision matrix, $\\boldsymbol{\\Psi} = \\boldsymbol{C}^{-1}$, is then estimated as:\n$$\n\\hat{\\boldsymbol{\\Psi}}_{\\text{debiased}} = \\alpha (\\hat{\\boldsymbol{C}})^{-1} = \\left(\\frac{N_s - n_b - 2}{N_s - 1}\\right) (\\hat{\\boldsymbol{C}})^{-1}\n$$\nIf the condition $N_s > n_b + 2$ is not met, this correction is not applied, and we use the (biased) inverse $(\\hat{\\boldsymbol{C}})^{-1}$.\n\nThe algorithmic procedure for each test case is as follows:\n1.  Construct the $n_b \\times n_b$ covariance matrix $\\boldsymbol{C}$ using the specified model: $C_{ij} = \\rho^{|i-j|} s_i s_j$, where $i, j \\in \\{0, 1, \\dots, 5\\}$.\n2.  Compute the inverse covariance matrix $\\boldsymbol{C}^{-1}$. This step requires robust numerical methods. The primary approach is to test for positive definiteness using Cholesky decomposition. If it fails, the matrix is not numerically positive definite. We then regularize it by adding a small diagonal component, $\\epsilon\\boldsymbol{I}$, where $\\boldsymbol{I}$ is the identity matrix. We iteratively increase $\\epsilon$ (e.g., from $10^{-12}$ upwards by factors of $10$) until a Cholesky factorization of the regularized matrix $\\boldsymbol{C} + \\epsilon\\boldsymbol{I}$ succeeds. If this regularization procedure fails after a reasonable number of attempts, we fall back to computing the Moore-Penrose pseudoinverse, $\\boldsymbol{C}^{+}$, as the effective inverse.\n3.  Apply the Hartlap correction factor $\\alpha$ to the computed inverse if $N_s > n_b + 2$. Let the final precision matrix be $\\boldsymbol{\\Psi}_{final}$.\n4.  Calculate the $\\chi^2$ statistic: $\\chi^2 = (\\boldsymbol{d}-\\boldsymbol{m})^T \\boldsymbol{\\Psi}_{final} (\\boldsymbol{d}-\\boldsymbol{m})$.\n5.  Determine the p-value, which is the upper-tail probability of the $\\chi^2$ statistic under a chi-squared distribution with $dof = n_b = 6$ degrees of freedom. This is given by the survival function, $p = SF(\\chi^2 | dof)$.\n6.  Compare the p-value to the significance level $\\alpha_{sig} = 0.05$. If $p \\geq 0.05$, the model is not rejected, and the result is $True$. Otherwise, the model is rejected, and the result is $False$.\n\nThis procedure is applied to each of the four test cases provided, yielding the required set of diagnostic values.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Solves the goodness-of-fit problem for the provided test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case A (happy path, well-conditioned covariance and many mocks)\n        {\n            \"d\": np.array([1000, 850, 720, 600, 500, 420]),\n            \"m\": np.array([980, 860, 710, 610, 510, 430]),\n            \"s\": np.array([50, 45, 40, 38, 35, 33]),\n            \"rho\": 0.35,\n            \"N_s\": 500\n        },\n        # Case B (boundary condition, nearly singular covariance and too few mocks for Hartlap)\n        {\n            \"d\": np.array([500, 480, 470, 460, 450, 440]),\n            \"m\": np.array([520, 500, 490, 480, 470, 460]),\n            \"s\": np.array([30, 30, 30, 30, 30, 30]),\n            \"rho\": 0.99,\n            \"N_s\": 8\n        },\n        # Case C (edge case, perfect fit)\n        {\n            \"d\": np.array([600, 550, 510, 480, 450, 420]),\n            \"m\": np.array([600, 550, 510, 480, 450, 420]),\n            \"s\": np.array([20, 18, 17, 16, 15, 14]),\n            \"rho\": 0.20,\n            \"N_s\": 300\n        },\n        # Case D (poor fit, large residuals)\n        {\n            \"d\": np.array([800, 820, 790, 810, 780, 805]),\n            \"m\": np.array([700, 940, 710, 900, 670, 935]),\n            \"s\": np.array([25, 25, 25, 25, 25, 25]),\n            \"rho\": 0.10,\n            \"N_s\": 100\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_goodness_of_fit(**case)\n        results.append(result)\n\n    # Format the final output string exactly as required.\n    formatted_results = []\n    for res_list in results:\n        chi2_val, dof, p_val, decision = res_list\n        formatted_list = f\"[{chi2_val:.6f},{dof},{p_val:.6f},{decision}]\"\n        formatted_results.append(formatted_list)\n        \n    final_output_string = f\"[{','.join(formatted_results)}]\"\n    print(final_output_string)\n\ndef construct_covariance(s, rho, n_b):\n    \"\"\"\n    Constructs the covariance matrix from standard deviations and a correlation model.\n    \"\"\"\n    C = np.zeros((n_b, n_b))\n    for i in range(n_b):\n        for j in range(n_b):\n            C[i, j] = (rho**abs(i - j)) * s[i] * s[j]\n    return C\n\ndef robust_inverse(C):\n    \"\"\"\n    Computes a robust inverse of a matrix, handling non-positive-definite cases.\n    \"\"\"\n    n_b = C.shape[0]\n    # First, attempt a Cholesky decomposition to check for positive-definiteness.\n    try:\n        np.linalg.cholesky(C)\n        # If the matrix is positive-definite, a standard inverse is safe and efficient.\n        return np.linalg.inv(C)\n    except np.linalg.LinAlgError:\n        # The matrix is not numerically positive-definite.\n        # Fallback 1: Add a small diagonal ridge and re-attempt Cholesky.\n        eps = 1e-12\n        for _ in range(15):  # Limit the number of regularization attempts\n            try:\n                C_reg = C + eps * np.identity(n_b)\n                np.linalg.cholesky(C_reg)\n                # If Cholesky succeeds, the regularized matrix is PD. Invert it.\n                return np.linalg.inv(C_reg)\n            except np.linalg.LinAlgError:\n                # Increase epsilon and try again.\n                eps *= 10\n        \n        # Fallback 2: If regularization repeatedly fails, use the Moore-Penrose pseudoinverse.\n        return np.linalg.pinv(C)\n\ndef compute_goodness_of_fit(d, m, s, rho, N_s, n_b=6, significance_level=0.05):\n    \"\"\"\n    Computes the goodness-of-fit diagnostics for a single case.\n    \"\"\"\n    # 1. Construct the covariance matrix\n    C = construct_covariance(s, rho, n_b)\n\n    # 2. Compute the robust inverse of the covariance matrix\n    C_inv = robust_inverse(C)\n\n    # 3. Apply the Hartlap correction to debias the inverse if applicable\n    if N_s > n_b + 2:\n        hartlap_factor = (N_s - n_b - 2) / (N_s - 1)\n        # The final precision matrix is the debiased inverse\n        precision_matrix = hartlap_factor * C_inv\n    else:\n        # If not applicable, use the computed inverse as is\n        precision_matrix = C_inv\n\n    # 4. Compute the chi-squared statistic\n    delta_v = d - m\n    chi_squared_val = delta_v.T @ precision_matrix @ delta_v\n    \n    # 5. Compute the degrees of freedom and the p-value\n    dof = n_b\n    p_value = chi2.sf(chi_squared_val, dof)\n\n    # 6. Make the acceptance/rejection decision\n    is_acceptable = p_value >= significance_level\n\n    return [chi_squared_val, dof, p_value, is_acceptable]\n\n# Execute the main function\nsolve()\n```"
        }
    ]
}