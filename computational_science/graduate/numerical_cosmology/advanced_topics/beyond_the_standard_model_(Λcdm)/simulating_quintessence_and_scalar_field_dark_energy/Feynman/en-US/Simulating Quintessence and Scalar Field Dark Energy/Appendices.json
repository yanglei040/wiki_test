{
    "hands_on_practices": [
        {
            "introduction": "The core of simulating scalar field cosmology is solving the ODEs that govern its evolution, and this practice guides you through building an adaptive-step-size integrator from scratch . You will implement a fourth-order Runge-Kutta solver in the standard dimensionless phase-space $(x, y)$ with e-folds $N = \\ln a$ as the time variable. This foundational exercise includes creating a physics-informed step-size limiter, giving you a powerful and versatile tool for exploring quintessence dynamics.",
            "id": "3488110",
            "problem": "You are to design and implement an adaptive step-size integrator in the dynamical variables $\\left(x,y\\right)$ with e-fold time $N = \\ln a$, to accurately simulate a spatially flat Friedmann–Lemaître–Robertson–Walker cosmology with a quintessence scalar field under a rapidly varying potential slope $\\lambda(\\phi)$. The system is to be simulated in units where the reduced Planck mass is set to unity, so all quantities are dimensionless.\n\nStart from the following fundamental base:\n- The Friedmann equation for a flat universe: $$H^2 = \\frac{1}{3}\\,\\rho_{\\text{tot}},$$ with $H$ the Hubble parameter and $\\rho_{\\text{tot}}$ the total energy density.\n- The Klein–Gordon equation for the scalar field $\\phi$ in an expanding universe: $$\\ddot{\\phi} + 3H\\dot{\\phi} + V_{,\\phi} = 0,$$ where $V(\\phi)$ is the potential and $V_{,\\phi} = \\mathrm{d}V/\\mathrm{d}\\phi$.\n- The acceleration equation in terms of $H$: $$\\frac{\\dot{H}}{H^2} = -\\frac{3}{2}\\left( (1+w_b)\\Omega_b + (1+w_\\phi)\\Omega_\\phi \\right),$$ with background fluid equation of state $w_b$ assumed constant, background density fraction $\\Omega_b$, scalar field equation of state $w_\\phi$, and scalar density fraction $\\Omega_\\phi$.\n\nDefine the standard dynamical variables and e-fold time derivative:\n- $$x \\equiv \\frac{\\dot{\\phi}}{\\sqrt{6}\\,H}, \\quad y \\equiv \\frac{\\sqrt{V}}{\\sqrt{3}\\,H}, \\quad \\frac{\\mathrm{d}}{\\mathrm{d}N} \\equiv \\frac{1}{H}\\frac{\\mathrm{d}}{\\mathrm{d}t}.$$\n- Let $\\Omega_\\phi = x^2 + y^2$, $w_\\phi = \\frac{x^2 - y^2}{x^2 + y^2}$ when $x^2 + y^2 \\neq 0$, and assume flatness $\\Omega_b = 1 - \\Omega_\\phi$.\n- Define the potential slope $$\\lambda(\\phi) \\equiv -\\frac{V_{,\\phi}}{V}.$$\n\nDerive from the above definitions and laws the autonomous system in $N$:\n- $$x' = -3x + \\sqrt{\\frac{3}{2}}\\,\\lambda(\\phi)\\,y^2 + \\frac{3}{2}x\\left( (1+w_b)\\left(1 - x^2 - y^2\\right) + 2x^2 \\right),$$\n- $$y' = y\\left( -\\sqrt{\\frac{3}{2}}\\,\\lambda(\\phi)\\,x + \\frac{3}{2}\\left( (1+w_b)\\left(1 - x^2 - y^2\\right) + 2x^2 \\right)\\right),$$\nsupplemented by the kinematic relation\n- $$\\phi' = \\sqrt{6}\\,x,$$\nand the definition of $\\lambda(\\phi)$ for the chosen potential. Here the prime denotes $\\mathrm{d}/\\mathrm{d}N$.\n\nYou must implement an adaptive step-size integrator over $N$ that:\n- Uses a single-step fourth-order Runge–Kutta method and step-doubling for local truncation error estimation. For a trial step of size $h$, compute one step of size $h$ and two consecutive steps of size $h/2$. Use the difference to estimate the local error.\n- Accepts a step when the infinity norm of the component-wise scaled error is below tolerance, i.e., for state vector components $u_i$, let $$\\text{err} = \\max_i \\frac{|u_i^{(h/2,h/2)} - u_i^{(h)}|}{\\text{atol} + \\text{rtol}\\,\\max\\left(|u_i^{(h/2,h/2)}|, |u_i^{(h)}|\\right)}.$$ If $\\text{err} \\leq 1$, accept the step and update the step size using a standard controller with safety factor $s$ and bounds, with order $p = 4$.\n- Enforces a $\\lambda(\\phi)$-variation limiter to handle rapidly varying $\\lambda(\\phi)$: for a trial step of size $h$, require $$|\\Delta \\lambda| \\approx \\left|\\frac{\\mathrm{d}\\lambda}{\\mathrm{d}\\phi}\\right|\\cdot |\\phi'| \\cdot h \\le \\delta_\\lambda,$$ where $\\delta_\\lambda$ is a user-specified threshold. If violated, reduce $h$ before attempting the step. Use the exact $\\mathrm{d}\\lambda/\\mathrm{d}\\phi$ implied by the chosen potential (see potentials below).\n- Ensures $y \\ge 0$ is enforced numerically to mitigate round-off drift, and that $\\Omega_\\phi \\le 1$ up to numerical tolerance; detect and report any violation.\n\nImplement the following potentials and their $\\lambda(\\phi)$ and $\\mathrm{d}\\lambda/\\mathrm{d}\\phi$:\n- Exponential: $$V(\\phi) = V_0\\,\\mathrm{e}^{-\\alpha \\phi}, \\quad \\lambda(\\phi) = \\alpha, \\quad \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}\\phi} = 0.$$\n- Gaussian: $$V(\\phi) = V_0\\,\\mathrm{e}^{-\\phi^2/\\sigma^2}, \\quad \\lambda(\\phi) = \\frac{2\\phi}{\\sigma^2}, \\quad \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}\\phi} = \\frac{2}{\\sigma^2}.$$\n- Inverse power law: $$V(\\phi) = V_0\\,\\phi^{-n} \\,\\, (\\phi > 0), \\quad \\lambda(\\phi) = \\frac{n}{\\phi}, \\quad \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}\\phi} = -\\frac{n}{\\phi^2}.$$\n\nDesign considerations:\n- Use constant background equation of state $w_b$.\n- Set numerical tolerances to $\\text{rtol} = 10^{-7}$ and $\\text{atol} = 10^{-9}$, step-size controller safety factor $s = 0.9$, growth factor bounds $f_{\\min} = 0.2$, $f_{\\max} = 5$, minimum step $h_{\\min} = 10^{-6}$, and maximum step $h_{\\max} = 0.5$. Use $\\delta_\\lambda = 0.1$ for the $\\lambda(\\phi)$-variation limiter.\n- All outputs are dimensionless.\n\nYour program must run these test cases and aggregate the required results:\n\n- Test case A (happy path, constant slope and known fixed point):\n  - Potential: exponential with $\\alpha = 5$.\n  - Background: $w_b = 0$.\n  - Domain: from $N_0 = 0$ to $N_1 = 5$.\n  - Initial conditions: $\\phi(N_0) = 0$, $x(N_0) = 10^{-6}$, $y(N_0) = 10^{-6}$.\n  - Quantity to output: the final Euclidean distance in the $\\left(x,y\\right)$-plane between the numerical state at $N_1$ and the scaling fixed point coordinates for constant $\\lambda$ and $w_b$, namely $$x_\\ast = \\sqrt{\\frac{3}{2}}\\frac{1+w_b}{\\lambda}, \\quad y_\\ast = \\sqrt{\\frac{3(1 - w_b^2)}{2\\lambda^2}}.$$ Output this distance as a float.\n\n- Test case B (rapidly varying slope near a potential maximum):\n  - Potential: Gaussian with $\\sigma = 0.1$.\n  - Background: $w_b = 0$.\n  - Domain: from $N_0 = 0$ to $N_1 = 8$.\n  - Initial conditions: $\\phi(N_0) = 0.5$, $x(N_0) = 10^{-8}$, $y(N_0) = 10^{-8}$.\n  - Quantity to output: the final value of $1 + w_\\phi(N_1)$, where $$w_\\phi = \\frac{x^2 - y^2}{x^2 + y^2}$$ if $x^2 + y^2 \\neq 0$, else use $w_\\phi = -1$. Output this as a float.\n\n- Test case C (edge case, large initial slope, physical bound preservation):\n  - Potential: inverse power law with $n = 6$, with domain restricted to $\\phi  0$.\n  - Background: $w_b = 0$.\n  - Domain: from $N_0 = 0$ to $N_1 = 5$.\n  - Initial conditions: $\\phi(N_0) = 0.5$, $x(N_0) = 10^{-6}$, $y(N_0) = 10^{-6}$.\n  - Quantity to output: a boolean indicating whether the numerical solution maintains $\\max_{N \\in [N_0,N_1]} \\Omega_\\phi(N) \\le 1 + 10^{-8}$. Output this as a boolean.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_A,result_B,result_C]$).\n- For floats, round to exactly $6$ decimal places. The values are dimensionless. For the boolean, output standard boolean text.\n- No input should be read; the program must be self-contained and produce the required output directly.",
            "solution": "The goal is to construct a numerical solver for a system of ordinary differential equations (ODEs) that describe the evolution of a quintessence scalar field in an expanding universe. The key challenge is to create an *adaptive* integrator that can adjust its step size to maintain accuracy and efficiency, especially when the physical conditions change rapidly.\n\n### Principle of Adaptive Integration\n\nAn adaptive step-size algorithm automates the process of choosing the step size $h$ for an ODE solver. The core idea is to estimate the local truncation error—the error introduced in a single step—and adjust $h$ to keep this error within a specified tolerance. If the estimated error is too large, the step is rejected and retried with a smaller $h$. If the error is very small, the step is accepted, and $h$ is increased for the next step to improve efficiency.\n\n### Implementation Strategy\n\n1.  **Error Estimation via Step Doubling**: A common method to estimate the local error of a one-step method like fourth-order Runge-Kutta (RK4) is *step doubling*. For a trial step of size $h$, we perform the integration in two ways:\n    *   One full step of size $h$ to get a solution $u_1$.\n    *   Two consecutive steps of size $h/2$ to get a more accurate solution $u_2$.\n    The difference, $|u_2 - u_1|$, provides an estimate of the local truncation error in the less accurate solution, $u_1$. The error for an RK4 method scales as $O(h^5)$.\n\n2.  **Step-Size Control**: Once the error is estimated, a controller decides whether to accept the step and how to adjust the next step size.\n    *   **Accept/Reject**: The step is accepted if the estimated error is below a predefined tolerance. The specified error metric combines both relative (`rtol`) and absolute (`atol`) tolerances, making it robust for solution components of varying magnitudes.\n    *   **New Step Size**: If a step is accepted, the optimal next step size is proportional to $h \\times (\\text{tolerance}/\\text{error})^{1/(p+1)}$, where $p=4$ is the order of the method. The formula is adjusted with a safety factor to be conservative and bounded by minimum and maximum growth factors to prevent overly aggressive or unstable changes. If a step is rejected, the same formula is used to calculate a smaller, more appropriate step size for the retry.\n\n3.  **Physics-Informed Step Limiter**: Standard error control can be inefficient if the underlying physics changes character very quickly. The problem specifies a `lambda-variation limiter`. The parameter $\\lambda(\\phi)$ represents the local slope of the potential, and some potentials (like the Gaussian or inverse power law) have slopes that can change very rapidly with $\\phi$. To prevent the integrator from taking a step so large that it completely misses this change, we add a proactive check. Before attempting an RK4 step, we estimate the change in $\\lambda$ over the proposed step $h$ and enforce $|\\Delta\\lambda| \\le \\delta_\\lambda$. If this condition is violated, we reduce $h$ *before* the expensive RK4 step is even computed.\n\n4.  **Overall Structure**: The implementation revolves around a main loop that advances the simulation from the initial to the final e-fold time, $N$. Inside the loop, the adaptive logic is executed: apply the $\\lambda$-limiter, perform the step-doubling computation, estimate the error, and based on the result, either accept the step and update the state and next step size, or reject the step and retry with a smaller step size. Physical constraints like $y \\ge 0$ and tracking of $\\Omega_\\phi$ are handled after each successful step.",
            "answer": "```python\nimport numpy as np\n\n# A small epsilon to prevent division by zero\nEPSILON = 1e-30\n\nclass ExponentialPotential:\n    \"\"\"Represents an exponential potential V = V0 * exp(-alpha * phi).\"\"\"\n    def __init__(self, alpha):\n        self.alpha = float(alpha)\n    \n    def get_lambda(self, phi):\n        return self.alpha\n        \n    def get_dlambda_dphi(self, phi):\n        return 0.0\n\nclass GaussianPotential:\n    \"\"\"Represents a Gaussian potential V = V0 * exp(-phi^2 / sigma^2).\"\"\"\n    def __init__(self, sigma):\n        self.sigma2 = float(sigma)**2\n        \n    def get_lambda(self, phi):\n        return 2.0 * phi / self.sigma2\n        \n    def get_dlambda_dphi(self, phi):\n        return 2.0 / self.sigma2\n\nclass InversePowerLawPotential:\n    \"\"\"Represents an inverse power law potential V = V0 * phi^(-n).\"\"\"\n    def __init__(self, n):\n        self.n = float(n)\n        \n    def get_lambda(self, phi):\n        if phi = 0:\n            return np.nan\n        return self.n / phi\n        \n    def get_dlambda_dphi(self, phi):\n        if phi = 0:\n            return np.nan\n        return -self.n / (phi**2)\n\ndef derivatives(N, u, w_b, potential):\n    \"\"\"\n    Computes the derivatives for the autonomous system.\n    u = [phi, x, y]\n    \"\"\"\n    phi, x, y = u\n    \n    # Get potential-dependent lambda\n    lambda_val = potential.get_lambda(phi)\n    if np.isnan(lambda_val):\n        return np.array([np.nan, np.nan, np.nan])\n\n    # Density parameter of the scalar field\n    omega_phi = x**2 + y**2\n    \n    # Ensure background density is not negative\n    omega_b = 1.0 - omega_phi if omega_phi = 1.0 else 0.0\n\n    # Common term in x' and y' equations\n    G_term = 1.5 * ((1.0 + w_b) * omega_b + 2.0 * x**2)\n    \n    phi_prime = np.sqrt(6.0) * x\n    x_prime = -3.0 * x + np.sqrt(1.5) * lambda_val * y**2 + x * G_term\n    y_prime = y * (-np.sqrt(1.5) * lambda_val * x + G_term)\n    \n    return np.array([phi_prime, x_prime, y_prime])\n\ndef rk4_step(derivs_func, N, u, h, w_b, potential):\n    \"\"\"Performs a single RK4 step.\"\"\"\n    k1 = derivs_func(N, u, w_b, potential)\n    k2 = derivs_func(N + 0.5 * h, u + 0.5 * h * k1, w_b, potential)\n    k3 = derivs_func(N + 0.5 * h, u + 0.5 * h * k2, w_b, potential)\n    k4 = derivs_func(N + h, u + h * k3, w_b, potential)\n    return u + (h / 6.0) * (k1 + 2.0 * k2 + 2.0 * k3 + k4)\n    \ndef solve_ode_adaptive(potential, w_b, N0, N1, u0, params, track_max_omega_phi=False):\n    \"\"\"\n    Adaptive step-size integrator for the quintessence ODE system.\n    \"\"\"\n    N = N0\n    u = np.array(u0, dtype=float)\n    h = params['h_init']\n    \n    max_omega_phi = u[1]**2 + u[2]**2\n    \n    max_steps = int(2 * (N1 - N0) / params['h_min'])\n    for _ in range(max_steps):\n        if N >= N1:\n            break\n\n        # Lambda variation limiter\n        dlambda_dphi = potential.get_dlambda_dphi(u[0])\n        phi_prime = np.sqrt(6.0) * u[1]\n        h_lambda = params['delta_lambda'] / (abs(dlambda_dphi * phi_prime) + EPSILON)\n        \n        h_trial = min(h, h_lambda)\n        h_trial = min(h_trial, N1 - N) # Don't overshoot\n        h_trial = max(h_trial, params['h_min'])\n\n        # Step doubling\n        u1 = rk4_step(derivatives, N, u, h_trial, w_b, potential)\n        \n        u_half = rk4_step(derivatives, N, u, h_trial / 2.0, w_b, potential)\n        u2 = rk4_step(derivatives, N + h_trial / 2.0, u_half, h_trial / 2.0, w_b, potential)\n        \n        if np.any(np.isnan(u1)) or np.any(np.isnan(u2)):\n             h = params['h_min']\n             continue\n\n        # Error estimation\n        scale = params['atol'] + params['rtol'] * np.maximum(np.abs(u1), np.abs(u2))\n        err_vec = np.abs(u2 - u1) / (scale + EPSILON)\n        err = np.max(err_vec)\n\n        # Step control logic\n        if err = 1.0: # Step accepted\n            N += h_trial\n            u = u2\n            \n            # Enforce y >= 0\n            u[2] = max(0.0, u[2])\n\n            if track_max_omega_phi:\n                current_omega_phi = u[1]**2 + u[2]**2\n                max_omega_phi = max(max_omega_phi, current_omega_phi)\n                # Physical bound check for integrity\n                if current_omega_phi > 1.1: # Generous margin for failure\n                    raise RuntimeError(f\"FATAL: Omega_phi > 1.1 at N={N}\")\n\n            # Update step size for next step\n            if err == 0.0:\n                scale_factor = params['f_max']\n            else:\n                scale_factor = params['s'] * (1.0 / err)**(1.0 / 5.0)\n            \n            h *= min(params['f_max'], max(params['f_min'], scale_factor))\n            h = min(h, params['h_max'])\n        else: # Step rejected\n            scale_factor = params['s'] * (1.0 / err)**(1.0 / 5.0)\n            h = h_trial * min(params['f_max'], max(params['f_min'], scale_factor))\n        \n        h = max(h, params['h_min'])\n    else:\n        raise RuntimeError(\"Integration failed: Maximum number of steps reached.\")\n\n    if track_max_omega_phi:\n        return u, max_omega_phi\n    return u, None\n\ndef solve():\n    \"\"\"Main function to run test cases and produce the final output.\"\"\"\n    params = {\n        'rtol': 1e-7,\n        'atol': 1e-9,\n        's': 0.9,\n        'f_min': 0.2,\n        'f_max': 5.0,\n        'h_min': 1e-6,\n        'h_max': 0.5,\n        'delta_lambda': 0.1,\n        'h_init': 0.01\n    }\n    \n    results = []\n\n    # Test Case A\n    alpha_A = 5.0\n    w_b_A = 0.0\n    pot_A = ExponentialPotential(alpha_A)\n    u0_A = [0.0, 1e-6, 1e-6]\n    u_final_A, _ = solve_ode_adaptive(pot_A, w_b_A, 0.0, 5.0, u0_A, params)\n    \n    x_star = np.sqrt(1.5) * (1.0 + w_b_A) / alpha_A\n    y_star_sq = 1.5 * (1.0 - w_b_A**2) / alpha_A**2\n    y_star = np.sqrt(y_star_sq)\n    \n    dist_A = np.sqrt((u_final_A[1] - x_star)**2 + (u_final_A[2] - y_star)**2)\n    results.append(f\"{dist_A:.6f}\")\n\n    # Test Case B\n    sigma_B = 0.1\n    w_b_B = 0.0\n    pot_B = GaussianPotential(sigma_B)\n    u0_B = [0.5, 1e-8, 1e-8]\n    u_final_B, _ = solve_ode_adaptive(pot_B, w_b_B, 0.0, 8.0, u0_B, params)\n    \n    x_f, y_f = u_final_B[1], u_final_B[2]\n    omega_phi_f = x_f**2 + y_f**2\n    w_phi_f = (x_f**2 - y_f**2) / (omega_phi_f + EPSILON) if omega_phi_f > 0 else -1.0\n    result_B = 1.0 + w_phi_f\n    results.append(f\"{result_B:.6f}\")\n\n    # Test Case C\n    n_C = 6.0\n    w_b_C = 0.0\n    pot_C = InversePowerLawPotential(n_C)\n    u0_C = [0.5, 1e-6, 1e-6]\n    _, max_omega_phi = solve_ode_adaptive(pot_C, w_b_C, 0.0, 5.0, u0_C, params, track_max_omega_phi=True)\n    \n    result_C = max_omega_phi = (1.0 + 1e-8)\n    results.append(str(result_C))\n\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "While explicit integrators are powerful, they have a critical weakness when faced with \"stiff\" systems of equations—those with widely separated dynamical timescales. This practice moves from implementation to analysis, exploring the conditions under which a quintessence simulation becomes numerically stiff . By linearizing the Klein-Gordon equation, you will discover how the physical parameters of the model, specifically the ratio of the field's effective mass to the Hubble rate ($m_{\\mathrm{eff}}/H$), directly translate into numerical instability for an explicit solver. This exercise will equip you with the ability to diagnose stiffness and understand the fundamental reasons for switching to more robust implicit methods, a key piece of expertise for tackling advanced cosmological models.",
            "id": "3488068",
            "problem": "You are to implement and evaluate a numerically robust computation of the Hubble expansion rate in a flat Friedmann–Lemaître–Robertson–Walker (FLRW) model with a canonical quintessence scalar field. The total expansion rate satisfies the Friedmann equation\n$$\nH^2 \\;=\\; \\frac{8\\pi G}{3}\\,\\sum_i \\rho_i,\n$$\nwhere $H$ is the Hubble parameter, $G$ is Newton’s constant, and $\\rho_i$ are the component energy densities. Consider a universe with three components: nonrelativistic matter of density $\\rho_m$, radiation of density $\\rho_r$, and a homogeneous scalar field $\\phi$ with canonical kinetic term. The scalar field energy density is\n$$\n\\rho_\\phi \\;=\\; K + V \\quad \\text{with} \\quad K \\equiv \\tfrac{1}{2}\\,\\dot{\\phi}^2,\n$$\nand the pressure is\n$$\np_\\phi \\;=\\; K - V.\n$$\nThe scalar field equation-of-state parameter is $w_\\phi \\equiv p_\\phi/\\rho_\\phi$. Near the cosmological constant limit $w_\\phi \\to -1$, one has $K \\ll V$, and direct floating-point evaluation of $\\rho_\\phi = K + V$ can suffer catastrophic loss of significance. This, in turn, can bias $H^2$ computed from the sum of components.\n\nWork entirely in a unit system where $(8\\pi G / 3) = 1$, so that\n$$\nH^2 \\;=\\; \\rho_m \\;+\\; \\rho_r \\;+\\; \\rho_\\phi.\n$$\nIn this problem, you will:\n- Compute a “naive” $H^2$ by direct double-precision floating-point summation.\n- Compute a “compensated” $H^2$ that uses a compensated summation strategy designed to reduce rounding error:\n  1. For the scalar field density, first produce a compensated decomposition of $\\rho_\\phi$ from $K$ and $V$ using an error-aware summation strategy so that $\\rho_\\phi$ is represented as a pair $(\\rho_{\\phi,\\text{hi}}, \\rho_{\\phi,\\text{lo}})$ satisfying $\\rho_{\\phi,\\text{hi}} + \\rho_{\\phi,\\text{lo}} \\approx K + V$ in double precision.\n  2. Then compute $H^2$ by a single pass compensated summation over the sequence $[\\rho_m, \\rho_r, \\rho_{\\phi,\\text{hi}}, \\rho_{\\phi,\\text{lo}}]$.\n- Diagnose the relative error by comparing each floating-point result against a high-precision baseline computed using arbitrary-precision decimal arithmetic.\n\nDefinitions and requirements:\n- Kinetic energy is $K = \\tfrac{1}{2}\\,\\dot{\\phi}^2$. All densities and $\\dot{\\phi}$ will be provided as real numbers in the same consistent unit system where $(8\\pi G / 3)=1$. The final diagnostic is dimensionless, so no physical unit must be reported for the outputs.\n- The naive $H^2$ is\n$$\nH^2_{\\text{naive}} \\;=\\; \\text{fl}\\big(\\rho_m + \\rho_r + \\text{fl}(K + V)\\big),\n$$\nwhere $\\text{fl}(\\cdot)$ denotes standard double-precision floating-point evaluation with left-to-right summation.\n- The compensated $H^2$ must:\n  1. Use a compensated binary summation for $\\rho_\\phi$ that outputs $(\\rho_{\\phi,\\text{hi}}, \\rho_{\\phi,\\text{lo}})$ from the pair $(V, K)$ to retain low-order information that would be lost by a single addition.\n  2. Use a compensated summation across $[\\rho_m, \\rho_r, \\rho_{\\phi,\\text{hi}}, \\rho_{\\phi,\\text{lo}}]$ to form $H^2_{\\text{comp}}$.\n- The high-precision baseline $H^2_{\\text{true}}$ must be computed using arbitrary-precision decimal arithmetic by exactly evaluating\n$$\nH^2_{\\text{true}} \\;=\\; \\rho_m \\;+\\; \\rho_r \\;+\\; V \\;+\\; \\tfrac{1}{2}\\,\\dot{\\phi}^2,\n$$\nwith sufficiently high decimal precision to make floating-point rounding error negligible in the baseline.\n\nFor each test case, compute the relative errors\n$$\n\\varepsilon_{\\text{naive}} \\;=\\; \\frac{\\left|H^2_{\\text{naive}} - H^2_{\\text{true}}\\right|}{\\left|H^2_{\\text{true}}\\right|}, \\qquad\n\\varepsilon_{\\text{comp}} \\;=\\; \\frac{\\left|H^2_{\\text{comp}} - H^2_{\\text{true}}\\right|}{\\left|H^2_{\\text{true}}\\right|}.\n$$\n\nTest suite:\nImplement your program for the following five test cases in units with $(8\\pi G/3)=1$:\n1. Case A (happy path, moderately small kinetic term): $\\rho_m = 0.3$, $\\rho_r = 1.0\\times 10^{-4}$, $V = 1.0$, $\\dot{\\phi} = 1.0\\times 10^{-4}$.\n2. Case B (extremely small kinetic term near $w_\\phi\\to -1$): $\\rho_m = 0.3$, $\\rho_r = 1.0\\times 10^{-4}$, $V = 1.0$, $\\dot{\\phi} = 1.0\\times 10^{-12}$.\n3. Case C (boundary with exactly zero kinetic term): $\\rho_m = 0.3$, $\\rho_r = 1.0\\times 10^{-4}$, $V = 1.0$, $\\dot{\\phi} = 0$.\n4. Case D (extreme dynamic range dominated by matter): $\\rho_m = 1.0\\times 10^{16}$, $\\rho_r = 1.0$, $V = 1.0$, $\\dot{\\phi} = 1.0\\times 10^{-4}$.\n5. Case E (all densities small with extremely tiny kinetic term): $\\rho_m = 1.0\\times 10^{-5}$, $\\rho_r = 1.0\\times 10^{-10}$, $V = 1.0\\times 10^{-5}$, $\\dot{\\phi} = 1.0\\times 10^{-15}$.\n\nFinal output specification:\n- For each test case, output the pair $(\\varepsilon_{\\text{naive}}, \\varepsilon_{\\text{comp}})$ as two consecutive floating-point numbers in scientific notation with exactly twelve significant digits after the decimal point.\n- Aggregate the results for all five test cases into a single list, ordered as\n$$\n[\\varepsilon_{\\text{naive},A}, \\varepsilon_{\\text{comp},A}, \\varepsilon_{\\text{naive},B}, \\varepsilon_{\\text{comp},B}, \\ldots, \\varepsilon_{\\text{naive},E}, \\varepsilon_{\\text{comp},E}],\n$$\nand print this list as a single line in the exact format\n\"[x1,x2,x3,...,x10]\" with commas and no spaces.",
            "solution": "This problem addresses a fundamental challenge in numerical computation: the loss of precision when summing floating-point numbers with vastly different magnitudes. This issue, known as \"catastrophic cancellation\" or \"absorption,\" is critical in cosmology when a scalar field's kinetic energy becomes negligible compared to its potential energy ($K \\ll V$), or when one energy component dominates the total cosmic budget.\n\n### Principle of Compensated Summation\n\nStandard floating-point addition has limited precision. When adding a small number $b$ to a large number $a$, the result might be rounded to just $a$, effectively losing the information contained in $b$. Compensated summation algorithms are designed to mitigate this by tracking the rounding error from each addition and incorporating it back into the final sum.\n\n1.  **High-Precision Baseline**: To accurately measure the error of floating-point methods, we first need a \"ground truth\" value. This is computed using arbitrary-precision arithmetic, such as Python's `decimal` module, with a precision high enough (e.g., 100 digits) that its own rounding errors are insignificant.\n\n2.  **Naive Summation**: This is the straightforward approach of adding the numbers in sequence using standard double-precision arithmetic. It is simple but susceptible to precision loss, as demonstrated by the test cases.\n\n3.  **Compensated Summation**: This robust strategy involves two steps:\n    *   **Binary Sum (FastTwoSum)**: First, we handle the sum $\\rho_\\phi = V + K$. Since $V$ is the larger term in the critical regimes, the `FastTwoSum` algorithm is highly efficient. It computes $s = V + K$ and simultaneously calculates the error term, $t = K - (s - V)$, that was lost due to rounding. The sum is now represented by an unevaluated pair of floating-point numbers, $(\\rho_{\\phi,\\text{hi}}, \\rho_{\\phi,\\text{lo}}) = (s, t)$, which preserves the original information with much higher fidelity.\n    *   **Sequence Sum (Neumaier Algorithm)**: To compute the final total, $H^2 = \\rho_m + \\rho_r + \\rho_\\phi$, we must sum the sequence $[\\rho_m, \\rho_r, \\rho_{\\phi,\\text{hi}}, \\rho_{\\phi,\\text{lo}}]$. A simple summation would again lose the low-order information contained in $\\rho_{\\phi,\\text{lo}}$ and potentially $\\rho_r$. The Neumaier algorithm is a refined version of Kahan summation that robustly handles this. It maintains a running sum `s` and a running compensation (error) term `c`. In each step, it adds the next number to the sum and intelligently updates the compensation term to capture any lost precision. The final, more accurate result is obtained by adding the final compensation `c` back to the final sum `s`.\n\nBy comparing the relative errors of the naive and compensated methods against the high-precision baseline, we can directly quantify the improvement and demonstrate the necessity of these techniques for accurate scientific computing.",
            "answer": "```python\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # Set the precision for the decimal module to a high value for baseline calculations.\n    getcontext().prec = 100\n\n    test_cases = [\n        # Case A: Happy path, moderately small kinetic term\n        {'rho_m': 0.3, 'rho_r': 1.0e-4, 'V': 1.0, 'dot_phi': 1.0e-4},\n        # Case B: Extremely small kinetic term near w_phi -> -1\n        {'rho_m': 0.3, 'rho_r': 1.0e-4, 'V': 1.0, 'dot_phi': 1.0e-12},\n        # Case C: Boundary with exactly zero kinetic term\n        {'rho_m': 0.3, 'rho_r': 1.0e-4, 'V': 1.0, 'dot_phi': 0.0},\n        # Case D: Extreme dynamic range dominated by matter\n        {'rho_m': 1.0e16, 'rho_r': 1.0, 'V': 1.0, 'dot_phi': 1.0e-4},\n        # Case E: All densities small with extremely tiny kinetic term\n        {'rho_m': 1.0e-5, 'rho_r': 1.0e-10, 'V': 1.0e-5, 'dot_phi': 1.0e-15},\n    ]\n\n    # --- Compensated Summation Algorithms ---\n\n    def fast_two_sum(a, b):\n        \"\"\"\n        Computes s = a + b and the error t, assuming abs(a) >= abs(b).\n        This is Dekker's FastTwoSum algorithm.\n        Returns a pair (s, t) that represents the sum with higher precision.\n        \"\"\"\n        s = a + b\n        t = b - (s - a)\n        return s, t\n\n    def neumaier_sum(summands):\n        \"\"\"\n        Computes the sum of a sequence of floating-point numbers using\n        Neumaier's algorithm, which is more robust than Kahan summation.\n        \"\"\"\n        s = 0.0  # The running sum\n        c = 0.0  # The running compensation term\n        for x in summands:\n            t = s + x\n            if abs(s) >= abs(x):\n                # If s is bigger, c accumulates the error in x.\n                c += (s - t) + x\n            else:\n                # If x is bigger, c accumulates the error in s.\n                c += (x - t) + s\n            s = t\n        return s + c\n\n    # --- Main Calculation Loop ---\n    \n    results = []\n    for case in test_cases:\n        rho_m_f = float(case['rho_m'])\n        rho_r_f = float(case['rho_r'])\n        V_f = float(case['V'])\n        dot_phi_f = float(case['dot_phi'])\n        K_f = 0.5 * dot_phi_f**2\n\n        # 1. High-precision baseline calculation using decimal\n        rho_m_d = Decimal(case['rho_m'])\n        rho_r_d = Decimal(case['rho_r'])\n        V_d = Decimal(case['V'])\n        dot_phi_d = Decimal(case['dot_phi'])\n        K_d = Decimal('0.5') * dot_phi_d**2\n        \n        H2_true_d = rho_m_d + rho_r_d + V_d + K_d\n        H2_true_f = float(H2_true_d)\n\n        # 2. Naive calculation using standard double-precision floats\n        rho_phi_naive = V_f + K_f\n        H2_naive = rho_m_f + rho_r_f + rho_phi_naive\n\n        # 3. Compensated calculation\n        # Step 3.1: Compensated sum for scalar field density\n        # V is always the larger term in these test cases\n        rho_phi_hi, rho_phi_lo = fast_two_sum(V_f, K_f)\n        \n        # Step 3.2: Compensated sum for all components\n        summands = [rho_m_f, rho_r_f, rho_phi_hi, rho_phi_lo]\n        H2_comp = neumaier_sum(summands)\n\n        # 4. Error calculation\n        if H2_true_f == 0.0:\n            # Avoid division by zero, although not expected for these cases\n            eps_naive = 0.0 if H2_naive == 0.0 else 1.0\n            eps_comp = 0.0 if H2_comp == 0.0 else 1.0\n        else:\n            eps_naive = abs(H2_naive - H2_true_f) / abs(H2_true_f)\n            eps_comp = abs(H2_comp - H2_true_f) / abs(H2_true_f)\n        \n        results.extend([eps_naive, eps_comp])\n\n    # Final print statement in the exact required format\n    formatted_results = [f\"{r:.12e}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While explicit integrators are powerful, they have a critical weakness when faced with \"stiff\" systems of equations—those with widely separated dynamical timescales. This practice moves from implementation to analysis, exploring the conditions under which a quintessence simulation becomes numerically stiff . By linearizing the Klein-Gordon equation, you will discover how the physical parameters of the model, specifically the ratio of the field's effective mass to the Hubble rate ($m_{\\mathrm{eff}}/H$), directly translate into numerical instability for an explicit solver. This exercise will equip you with the ability to diagnose stiffness and understand the fundamental reasons for switching to more robust implicit methods, a key piece of expertise for tackling advanced cosmological models.",
            "id": "3488076",
            "problem": "In a spatially flat Friedmann–Lemaître–Robertson–Walker (FLRW) universe with scale factor $a(t)$, consider a minimally coupled canonical scalar field $\\phi(t)$ with potential $V(\\phi)$ modeling quintessence in the thawing regime. The governing equations are the Klein–Gordon equation and the Friedmann equation,\n$$\n\\ddot{\\phi} + 3 H \\dot{\\phi} + V_{,\\phi} = 0, \\quad\nH^2 = \\frac{8\\pi G}{3}\\left(\\rho_{m0} a^{-3} + \\rho_{r0} a^{-4} + \\frac{1}{2}\\dot{\\phi}^2 + V(\\phi)\\right), \\quad \\dot{a} = H a,\n$$\nwhere $H \\equiv \\dot{a}/a$ is the Hubble rate, $V_{,\\phi} \\equiv \\partial V/\\partial \\phi$, $\\rho_{m0}$ and $\\rho_{r0}$ are present-day matter and radiation densities, and overdots denote derivatives with respect to cosmic time $t$. Define the equation-of-state parameter for the field as $w_\\phi \\equiv p_\\phi/\\rho_\\phi$ with $p_\\phi = \\frac{1}{2}\\dot{\\phi}^2 - V(\\phi)$ and $\\rho_\\phi = \\frac{1}{2}\\dot{\\phi}^2 + V(\\phi)$.\n\nYou plan to integrate this system as a set of ordinary differential equations (ODEs) for the state vector $y = (\\phi, \\pi, a)$ with $\\pi \\equiv \\dot{\\phi}$, using an adaptive explicit Runge–Kutta method, and to switch to an implicit backward differentiation formula (BDF) method when stiffness is detected. In the thawing regime, initial conditions satisfy $H \\gg m_{\\mathrm{eff}}$ and $\\pi \\approx 0$, where $m_{\\mathrm{eff}}^2 \\equiv V_{,\\phi\\phi}$ is the effective mass squared near the local field value.\n\nAssume that over a single adaptive time step of size $h$ the Hubble rate $H$ and the effective mass $m_{\\mathrm{eff}}$ can be treated as approximately constant, and that matter and radiation dominate the expansion history so that $H$ varies on a timescale $\\sim H^{-1}$. Linearize the $\\{\\phi,\\pi\\}$ subsystem about the instantaneous state to estimate the Jacobian $J$ of the right-hand side with respect to $(\\phi,\\pi)$. From first principles, use this linearization to reason about the local eigenvalues and timescales of the subsystem and the implications for numerical stability.\n\nWhich of the following statements about stiffness indicators and method switching in this problem are correct?\n\nA. Switch from explicit Runge–Kutta to backward differentiation formula (BDF) when the estimated spectral radius of the Jacobian $J$ satisfies $h \\, \\rho(J) > c_{\\mathrm{RK}}$, where $c_{\\mathrm{RK}}$ is a constant depending on the linear stability region of the chosen explicit Runge–Kutta scheme, because this indicates that the current step would leave the explicit method’s stability region.\n\nB. Use the ratio $m_{\\mathrm{eff}}/H$ as a stiffness indicator, switching to backward differentiation formula (BDF) when $m_{\\mathrm{eff}}/H \\gg 1$ (for example, exceeding a threshold such as $10$), since this signals the emergence of fast field oscillations relative to the Hubble damping timescale and a corresponding timescale disparity.\n\nC. Prefer explicit Runge–Kutta throughout the thawing regime because the Klein–Gordon equation is second order and therefore never produces stiff behavior; stiffness only arises in first-order systems.\n\nD. Trigger method switching whenever the fractional change of the equation-of-state parameter satisfies $\\lvert \\Delta w_\\phi\\rvert/\\lvert w_\\phi\\rvert > \\epsilon$ over a single explicit step, with some tolerance $\\epsilon$, because large physical variation in $w_\\phi$ is a direct indicator of stiffness in this problem.\n\nE. Use the sign of the curvature $V_{,\\phi\\phi}$ as the stiffness indicator: if $V_{,\\phi\\phi} > 0$ select backward differentiation formula (BDF), and if $V_{,\\phi\\phi}  0$ select explicit Runge–Kutta, because positive curvature makes the system numerically unstable and requires an implicit method.\n\nSelect all that apply. Provide no derivations in your final selection; base your choice on reasoning from the given equations and the concepts of linearization, eigenvalues, and stability of explicit versus implicit methods for stiff ODEs.",
            "solution": "To determine the conditions for numerical stiffness, we analyze the local behavior of the scalar field dynamics. First, we write the second-order Klein-Gordon equation as a system of two first-order ODEs by defining the state as $(\\phi, \\pi)$, where $\\pi \\equiv \\dot{\\phi}$. The system is:\n$$\n\\frac{d}{dt}\\begin{pmatrix} \\phi \\\\ \\pi \\end{pmatrix} = \\begin{pmatrix} \\pi \\\\ -V_{,\\phi}(\\phi) - 3H\\pi \\end{pmatrix}\n$$\nStiffness is determined by the eigenvalues of the Jacobian matrix of this system. Assuming $H$ and $V_{,\\phi\\phi} \\equiv m_{\\mathrm{eff}}^2$ are locally constant, the Jacobian $J$ is:\n$$\nJ = \\begin{pmatrix} 0  1 \\\\ -V_{,\\phi\\phi}  -3H \\end{pmatrix} = \\begin{pmatrix} 0  1 \\\\ -m_{\\mathrm{eff}}^2  -3H \\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ of $J$ are the roots of the characteristic equation $\\lambda^2 + 3H\\lambda + m_{\\mathrm{eff}}^2 = 0$:\n$$\n\\lambda_{\\pm} = \\frac{-3H \\pm \\sqrt{9H^2 - 4m_{\\mathrm{eff}}^2}}{2}\n$$\nNumerical stiffness arises when the system possesses timescales that are widely separated, and at least one fast timescale requires an explicit integrator to take impractically small steps for stability. This occurs when the field begins to oscillate rapidly around a potential minimum. This corresponds to the case where $4m_{\\mathrm{eff}}^2 > 9H^2$, or simply $m_{\\mathrm{eff}} \\gg H$. In this limit, the eigenvalues become complex with a large imaginary part:\n$$\n\\lambda_{\\pm} \\approx -\\frac{3H}{2} \\pm i m_{\\mathrm{eff}}\n$$\nThe system has a slow damping timescale of $\\sim 1/H$ and a very fast oscillation timescale of $\\sim 1/m_{\\mathrm{eff}}$. For an explicit Runge-Kutta method to be stable, the step size $h$ must satisfy $h \\lesssim 1/m_{\\mathrm{eff}}$. Since we are interested in the evolution on the much longer Hubble timescale ($H^{-1}$), this constraint becomes prohibitively expensive. This disparity in timescales ($1/m_{\\mathrm{eff}} \\ll 1/H$) is the definition of stiffness in this context.\n\n**Analysis of Options:**\n\n*   **A:** The stability of an explicit method is governed by the condition that $h\\lambda$ must lie within the method's stability region for all eigenvalues $\\lambda$. This is approximately bounded by the spectral radius $\\rho(J) = \\max|\\lambda_i|$. The condition $h\\,\\rho(J) > c_{\\mathrm{RK}}$ means a proposed step $h$ is too large and would lead to numerical instability. This is the formal definition of hitting the stability limit, which is a direct consequence of stiffness. Therefore, switching to a stiff-aware method like BDF is the correct strategy. This statement is correct.\n*   **B:** As shown above, the physical condition for stiffness is the emergence of fast oscillations, which occurs when the effective mass (the oscillation frequency) is much larger than the Hubble friction (the damping rate), i.e., $m_{\\mathrm{eff}} \\gg H$. The ratio $m_{\\mathrm{eff}}/H$ is therefore an excellent and physically motivated indicator of stiffness. When this ratio is large, the timescale disparity becomes large, and a switch to a stiff solver is warranted. This statement is correct.\n*   **C:** This statement is false. Any second-order ODE can be rewritten as a system of two first-order ODEs. The resulting system can absolutely be stiff, as demonstrated by the canonical example of a damped harmonic oscillator, which is precisely what the linearized Klein-Gordon equation represents.\n*   **D:** A large change in $w_\\phi$ is a *symptom* of rapid dynamics, but it is not a robust *predictor* of stiffness. Stiffness is an intrinsic property of the system's timescales (eigenvalues). A large computed $\\Delta w_\\phi$ is more likely the result of an already unstable step, which an adaptive error controller would reject anyway. A good stiffness detector should anticipate the problem based on the system's properties ($J$ or $m_{\\mathrm{eff}}/H$), not react to an inaccurate result.\n*   **E:** This confuses physical stability with numerical stiffness. If $V_{,\\phi\\phi}  0$, the system has a real, positive eigenvalue, indicating a physical (tachyonic) instability where the field rolls away exponentially. An explicit method is well-suited to accurately track this runaway behavior. If $V_{,\\phi\\phi} > 0$, the system is physically stable (oscillatory), but only becomes *numerically stiff* if $V_{,\\phi\\phi}$ is large enough ($m_{\\mathrm{eff}} \\gg H$). Simply having $V_{,\\phi\\phi} > 0$ is not a sufficient condition for stiffness. Thus, this switching criterion is incorrect.",
            "answer": "$$\\boxed{AB}$$"
        }
    ]
}