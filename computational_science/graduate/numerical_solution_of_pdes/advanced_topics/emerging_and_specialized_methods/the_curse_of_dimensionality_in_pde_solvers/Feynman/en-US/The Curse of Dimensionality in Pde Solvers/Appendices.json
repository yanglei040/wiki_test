{
    "hands_on_practices": [
        {
            "introduction": "Before we can overcome the curse of dimensionality, we must first understand its origins and mechanics. This foundational exercise guides you through a rigorous analysis of the high-dimensional heat equation, a canonical parabolic PDE. By deriving the spectrum of the discrete Laplacian and comparing the stability and computational complexity of various time-stepping schemes, you will gain a first-principles understanding of how spatial dimension $d$ severely restricts explicit methods and necessitates the use of more sophisticated implicit techniques .",
            "id": "3454722",
            "problem": "Consider the time-dependent heat equation on the $d$-dimensional torus with periodic boundary conditions,\n$$\nu_{t} = \\Delta u \\quad \\text{on} \\quad \\mathbb{T}^{d},\n$$\nwhere $\\mathbb{T}^{d}$ denotes the Cartesian product of $d$ circles of length $1$. Discretize space using a uniform grid with $m$ points per dimension (assume $m$ is even) and grid spacing $h = \\frac{1}{m}$, and approximate the Laplacian $\\Delta$ by the standard second-order central difference operator so that the semi-discrete system takes the form\n$$\n\\frac{d \\mathbf{u}(t)}{dt} = A \\, \\mathbf{u}(t),\n$$\nwhere $A$ is the $d$-dimensional discrete Laplacian with periodic boundary conditions. Let $N = m^{d}$ denote the total number of grid points.\n\nUsing only fundamental properties of Fourier modes, circulant matrices, and contraction semigroups generated by negative semidefinite operators, perform the following:\n\n1. Derive the spectrum of $A$ in terms of the discrete Fourier mode indices and identify the most negative eigenvalue as a function of $d$ and $h$.\n\n2. For the scalar linear test equation $y'(t) = \\lambda y(t)$, characterize the stability regions (sets of $z = \\Delta t \\, \\lambda$ for which the amplification factor satisfies $|G(z)| \\leq 1$) of the following time-stepping strategies when applied to the semi-discrete heat equation:\n   - Alternating Direction Implicit (ADI), understood as a standard Douglas-type splitting that sequentially solves along each spatial coordinate with implicit substeps.\n   - Exponential integrators, which compute the exact action of $\\exp(\\Delta t A)$ via the Fast Fourier Transform (FFT).\n   - Operator splitting schemes such as first-order Lie splitting and second-order Strang splitting across the $d$ coordinate directions.\n\n   Justify unconditional stability for the latter two in this setting and discuss the conditions under which ADI is unconditionally stable for diffusion operators.\n\n3. Derive the leading-order operation count per time step as a function of $d$, $m$, and $N$ for:\n   - ADI using cyclic tridiagonal solves along each coordinate direction and each grid line,\n   - Exponential integrators implemented with $d$-dimensional FFTs,\n   - Splitting methods implemented via sequential one-dimensional FFTs along each coordinate direction and across all lines.\n\n   Express these operation counts in big-$\\mathcal{O}$ notation in terms of $d$, $N$, and $\\log m$.\n\nFinally, as a baseline that quantifies the curse of dimensionality for explicit methods, determine the largest stable time step $\\Delta t_{\\max}$ for the forward Euler method applied to the semi-discrete system $d\\mathbf{u}/dt = A \\mathbf{u}$ in terms of $d$ and $h$. Provide your final $\\Delta t_{\\max}$ as a single closed-form analytic expression. No rounding is required, and no units are needed in the final expression.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of numerical analysis for partial differential equations, is well-posed with a clear and consistent setup, and is expressed in objective, formal language. It presents a standard but non-trivial problem that can be rigorously solved using established mathematical techniques.\n\nThe solution proceeds by addressing each of the tasks outlined in the problem statement.\n\n### 1. Spectrum of the Discrete Laplacian\n\nThe semi-discrete system for the heat equation $u_t = \\Delta u$ on the $d$-dimensional torus $\\mathbb{T}^d$ is given by $\\frac{d\\mathbf{u}(t)}{dt} = A \\mathbf{u}(t)$, where $A$ is the matrix representing the standard second-order finite difference approximation to the Laplacian $\\Delta$ on a uniform grid with spacing $h = \\frac{1}{m}$ and periodic boundary conditions. The total number of grid points is $N=m^d$.\n\nThe operator $A$ can be expressed as a sum of one-dimensional discrete Laplacians:\n$$\nA = \\sum_{j=1}^{d} A_j\n$$\nwhere $A_j$ represents the discrete Laplacian in the $j$-th coordinate direction. Using Kronecker product notation, this is $A = A_{1D} \\otimes I \\otimes \\dots \\otimes I + I \\otimes A_{1D} \\otimes \\dots \\otimes I + \\dots + I \\otimes \\dots \\otimes I \\otimes A_{1D}$, where $A_{1D}$ is the $m \\times m$ matrix for the 1D problem.\n\nIn one dimension, on a grid with points $x_i = i h$ for $i=0, \\dots, m-1$, the action of $A_{1D}$ on a grid function $u$ is given by:\n$$\n(A_{1D} u)_i = \\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2}\n$$\nwith indices taken modulo $m$ due to periodic boundary conditions. This structure makes $A_{1D}$ a circulant matrix. The eigenvectors of any $m \\times m$ circulant matrix are the discrete Fourier modes. A discrete Fourier mode with wave number $k \\in \\{0, 1, \\dots, m-1\\}$ is a vector $\\mathbf{v}_k$ with components $(\\mathbf{v}_k)_j = \\exp(i 2 \\pi k x_j / L) = \\exp(i 2 \\pi k j / m)$ since the length of the domain is $L=1$.\n\nApplying $A_{1D}$ to $\\mathbf{v}_k$:\n$$\n(A_{1D} \\mathbf{v}_k)_j = \\frac{\\exp(i \\frac{2\\pi k(j+1)}{m}) - 2\\exp(i \\frac{2\\pi k j}{m}) + \\exp(i \\frac{2\\pi k(j-1)}{m})}{h^2}\n$$\n$$\n= \\frac{\\exp(i \\frac{2\\pi k j}{m})}{h^2} \\left( \\exp(i \\frac{2\\pi k}{m}) - 2 + \\exp(-i \\frac{2\\pi k}{m}) \\right)\n$$\n$$\n= (\\mathbf{v}_k)_j \\frac{2}{h^2} \\left( \\cos(\\frac{2\\pi k}{m}) - 1 \\right)\n$$\nUsing the identity $1 - \\cos(2\\theta) = 2\\sin^2(\\theta)$, this becomes:\n$$\n= (\\mathbf{v}_k)_j \\left( -\\frac{4}{h^2} \\sin^2(\\frac{\\pi k}{m}) \\right)\n$$\nThus, the eigenvalues of the 1D operator $A_{1D}$ are:\n$$\n\\lambda_{1D, k} = -\\frac{4}{h^2} \\sin^2\\left(\\frac{\\pi k}{m}\\right), \\quad \\text{for } k \\in \\{0, 1, \\dots, m-1\\}\n$$\nThe eigenvalues of the $d$-dimensional operator $A$ are the sums of the eigenvalues of the constituent 1D operators. An eigenvector of $A$ is a tensor product of 1D eigenvectors $\\mathbf{v}_{\\mathbf{k}} = \\mathbf{v}_{k_1} \\otimes \\mathbf{v}_{k_2} \\otimes \\dots \\otimes \\mathbf{v}_{k_d}$, where $\\mathbf{k} = (k_1, \\dots, k_d)$ is a multi-index of wave numbers with each $k_j \\in \\{0, 1, \\dots, m-1\\}$. The corresponding eigenvalue $\\lambda_{\\mathbf{k}}$ is:\n$$\n\\lambda_{\\mathbf{k}} = \\sum_{j=1}^{d} \\lambda_{1D, k_j} = -\\frac{4}{h^2} \\sum_{j=1}^{d} \\sin^2\\left(\\frac{\\pi k_j}{m}\\right)\n$$\nThis is the spectrum of $A$. All eigenvalues are real and non-positive, so $A$ is a negative semidefinite operator.\n\nThe most negative eigenvalue, $\\lambda_{\\min}(A)$, occurs when the sum $\\sum_{j=1}^{d} \\sin^2(\\frac{\\pi k_j}{m})$ is maximized. The term $\\sin^2(\\theta)$ is maximized with value $1$ when $\\theta = \\pi/2 + n\\pi$ for integer $n$. For $k_j \\in \\{0, \\dots, m-1\\}$, the argument is $\\frac{\\pi k_j}{m}$. We need $\\frac{\\pi k_j}{m} = \\frac{\\pi}{2}$, which gives $k_j = m/2$. Since $m$ is given to be even, $k_j=m/2$ is an integer and a valid wave number. This corresponds to the highest frequency mode resolvable on the grid. To maximize the sum, we must choose $k_j = m/2$ for all $j=1, \\dots, d$.\n$$\n\\lambda_{\\min}(A) = -\\frac{4}{h^2} \\sum_{j=1}^{d} \\sin^2\\left(\\frac{\\pi (m/2)}{m}\\right) = -\\frac{4}{h^2} \\sum_{j=1}^{d} \\sin^2\\left(\\frac{\\pi}{2}\\right) = -\\frac{4}{h^2} \\sum_{j=1}^{d} 1\n$$\n$$\n\\lambda_{\\min}(A) = -\\frac{4d}{h^2}\n$$\n\n### 2. Stability of Time-Stepping Strategies\n\nA numerical method for $y' = \\lambda y$ is stable if the amplification factor $G(z)$, where $z = \\Delta t \\lambda$, satisfies $|G(z)| \\le 1$. For the system $\\mathbf{u}' = A\\mathbf{u}$, this must hold for $z_k = \\Delta t \\lambda_k$ for every eigenvalue $\\lambda_k$ of $A$. Since all $\\lambda_k \\le 0$, we only need to consider $z \\in [\\Delta t \\lambda_{\\min}, 0]$.\n\n- **Exponential Integrators:** This method computes the exact solution to the semi-discrete system over one time step: $\\mathbf{u}_{n+1} = \\exp(\\Delta t A) \\mathbf{u}_n$. The amplification operator is $\\mathcal{G} = \\exp(\\Delta t A)$, and the amplification factors are $G(z_k) = \\exp(z_k) = \\exp(\\Delta t \\lambda_k)$. Since $A$ is negative semidefinite, its eigenvalues $\\lambda_k$ are non-positive. Therefore, $\\Delta t \\lambda_k \\le 0$ for any $\\Delta t > 0$. The magnitude of the amplification factor is $|G(z_k)| = |\\exp(\\Delta t \\lambda_k)| = \\exp(\\Delta t \\lambda_k) \\le \\exp(0) = 1$. This holds for all eigenvalues and any choice of $\\Delta t > 0$. Thus, exponential integrators are unconditionally stable. This relies on the fundamental property that a negative semidefinite operator generates a contraction semigroup $\\|\\exp(tA)\\|_2 \\le 1$ for $t \\ge 0$.\n\n- **Operator Splitting Schemes:** The operator $A$ is split as $A = \\sum_{j=1}^d A_j$.\n  - First-order Lie splitting approximates $\\exp(\\Delta t A)$ by $\\mathcal{G}_{Lie} = \\prod_{j=1}^d \\exp(\\Delta t A_j)$.\n  - Second-order Strang splitting uses a symmetric product, e.g., $\\mathcal{G}_{Strang} = \\exp(\\frac{\\Delta t}{2} A_1) \\dots \\exp(\\frac{\\Delta t}{2} A_d) \\exp(\\frac{\\Delta t}{2} A_d) \\dots \\exp(\\frac{\\Delta t}{2} A_1)$.\n  Each operator $A_j$ is itself a 1D discrete Laplacian and is therefore negative semidefinite. Thus, each component operator $\\exp(s A_j)$ for $s > 0$ generates a contraction, i.e., $\\|\\exp(s A_j)\\|_2 \\le 1$. The full amplification operator for both Lie and Strang splitting is a product of such contraction operators. The $L_2$-norm of a product of operators is bounded by the product of their norms. Therefore, $\\|\\mathcal{G}_{Lie}\\|_2 \\le \\prod_{j=1}^d \\|\\exp(\\Delta t A_j)\\|_2 \\le 1$ and similarly $\\|\\mathcal{G}_{Strang}\\|_2 \\le 1$. Both schemes are unconditionally stable in the $L_2$-norm for any $\\Delta t > 0$.\n\n- **Alternating Direction Implicit (ADI):** ADI methods factor the implicit operator of a method like Crank-Nicolson, $(\\mathbf{I} - \\frac{\\Delta t}{2} A)$, into a product of easily invertible 1D operators, $\\prod_{j=1}^d (\\mathbf{I} - \\frac{\\Delta t}{2} A_j)$. For a diffusion problem with commuting operators $A_j$ (as is the case here on a Cartesian grid), the stability of the ADI scheme is linked to the stability of the underlying method it approximates. For example, a Douglas-Gunn scheme based on Crank-Nicolson is unconditionally stable. The amplification factor for each 1D Crank-Nicolson step corresponding to operator $A_j$ and eigenvalue $\\lambda_{1D,k_j}$ is $G_j = (1 + \\frac{\\Delta t}{2}\\lambda_{1D,k_j})/(1 - \\frac{\\Delta t}{2}\\lambda_{1D,k_j})$. Since $\\lambda_{1D,k_j} \\le 0$, $|G_j| \\le 1$ for any $\\Delta t > 0$. The total amplification factor in the Fourier domain is a product of such terms, and its magnitude is also bounded by $1$. Therefore, for this specific problem (linear, constant-coefficient diffusion on a Cartesian grid), ADI schemes of the Douglas type are unconditionally stable. This stability can be lost if the operators $A_j$ do not commute, which occurs for problems with mixed derivatives, on non-Cartesian grids, or with variable coefficients.\n\n### 3. Operation Count per Time Step\n\nLet $N=m^d$ be the total number of grid points.\n\n- **ADI:** The method involves solving $d$ sequences of 1D implicit systems. For each dimension $j$, one must solve $m^{d-1}$ independent 1D systems, one for each grid line along that dimension. Each system is of size $m \\times m$ and is cyclic tridiagonal due to the periodic boundary conditions. A cyclic tridiagonal system can be solved in $\\mathcal{O}(m)$ operations (e.g., using the Sherman-Morrison formula). The total cost for one direction is $m^{d-1} \\times \\mathcal{O}(m) = \\mathcal{O}(m^d) = \\mathcal{O}(N)$. Since this is done for each of the $d$ directions, the total operation count per time step is $\\mathcal{O}(d N)$.\n\n- **Exponential Integrators:** The action of $\\exp(\\Delta t A)$ is computed via FFTs. The algorithm is: (1) forward $d$-dimensional FFT of the state vector $\\mathbf{u}_n$, (2) element-wise multiplication of the transformed vector by the precomputed factors $\\exp(\\Delta t \\lambda_{\\mathbf{k}})$, and (3) inverse $d$-dimensional FFT. A $d$-dimensional FFT on an $m \\times \\dots \\times m$ grid is performed by applying 1D FFTs of size $m$ along each dimension sequentially. The cost of performing 1D FFTs of size $m$ along all $m^{d-1}$ lines of a chosen dimension is $m^{d-1} \\times \\mathcal{O}(m \\log m) = \\mathcal{O}(N \\log m)$. This is repeated for all $d$ dimensions. Thus, the cost of a $d$-dimensional FFT is $\\mathcal{O}(d N \\log m)$. The multiplication step takes $\\mathcal{O}(N)$. The dominant cost is the forward and inverse FFTs, so the total operation count per time step is $\\mathcal{O}(d N \\log m)$.\n\n- **Splitting Methods:** A splitting method like Lie splitting involves applying $d$ operators of the form $\\exp(\\Delta t A_j)$. Each application, e.g., $\\mathbf{v} \\mapsto \\exp(\\Delta t A_j)\\mathbf{v}$, is a 1D heat solve along dimension $j$ for all grid lines. This is done efficiently using 1D FFTs. For each of the $m^{d-1}$ lines in dimension $j$, one performs a forward 1D FFT, multiplication by exponential factors, and an inverse 1D FFT. The cost for one such directional solve is $m^{d-1} \\times \\mathcal{O}(m \\log m) = \\mathcal{O}(N \\log m)$. Since there are $d$ such steps in Lie splitting (and $\\mathcal{O}(d)$ steps in Strang splitting), the total operation count per time step is $\\mathcal{O}(d N \\log m)$.\n\n### Final Task: Time Step Restriction for Forward Euler\n\nThe forward Euler method applied to $\\frac{d\\mathbf{u}}{dt} = A\\mathbf{u}$ is given by:\n$$\n\\mathbf{u}_{n+1} = \\mathbf{u}_n + \\Delta t A \\mathbf{u}_n = (\\mathbf{I} + \\Delta t A) \\mathbf{u}_n\n$$\nThe amplification operator is $\\mathcal{G}(A) = \\mathbf{I} + \\Delta t A$. For stability, its spectral radius must be no greater than $1$. The eigenvalues of $\\mathcal{G}(A)$ are $g_k = 1 + \\Delta t \\lambda_k$, where $\\lambda_k$ are the eigenvalues of $A$. We require $|g_k| \\le 1$ for all $k$.\n$$\n|1 + \\Delta t \\lambda_k| \\le 1\n$$\nSince the eigenvalues $\\lambda_k$ are real and non-positive, this inequality is equivalent to:\n$$\n-1 \\le 1 + \\Delta t \\lambda_k \\le 1\n$$\nThe right-hand side, $1+\\Delta t \\lambda_k \\le 1$, implies $\\Delta t \\lambda_k \\le 0$, which is always satisfied since $\\Delta t > 0$ and $\\lambda_k \\le 0$.\nThe left-hand side, $-1 \\le 1 + \\Delta t \\lambda_k$, implies $-2 \\le \\Delta t \\lambda_k$, or $\\Delta t (-\\lambda_k) \\le 2$.\n\nThis condition must hold for all eigenvalues $\\lambda_k$. The most restrictive case is for the eigenvalue with the largest magnitude, which is $\\lambda_{\\min}(A)$.\n$$\n\\Delta t (-\\lambda_{\\min}(A)) \\le 2\n$$\n$$\n\\Delta t \\le \\frac{2}{-\\lambda_{\\min}(A)}\n$$\nThe maximum stable time step, $\\Delta t_{\\max}$, is therefore $\\frac{2}{-\\lambda_{\\min}(A)}$. From Part 1, we derived $\\lambda_{\\min}(A) = -\\frac{4d}{h^2}$. Substituting this value:\n$$\n\\Delta t_{\\max} = \\frac{2}{-(-4d/h^2)} = \\frac{2}{4d/h^2} = \\frac{2h^2}{4d} = \\frac{h^2}{2d}\n$$\nThis severe restriction, $\\Delta t_{\\max} \\propto h^2/d$, exemplifies the curse of dimensionality for explicit methods, as the time step must decrease not only with finer spatial resolution but also with increasing dimension.",
            "answer": "$$\n\\boxed{\\frac{h^{2}}{2d}}\n$$"
        },
        {
            "introduction": "Having diagnosed the curse of dimensionality, we now explore a powerful strategy to mitigate it by reformulating the problem. This practice moves from theory to implementation, contrasting a classical finite volume method for hyperbolic conservation laws, whose cost scales exponentially with dimension, against a modern probabilistic surrogate. By modeling the state variable with a probability density function (PDF) and evolving it with a low-dimensional Fokker-Planck equation, you will quantitatively demonstrate how this change in perspective breaks the exponential dependency and makes high-dimensional simulations tractable .",
            "id": "3454649",
            "problem": "Consider the hyperbolic conservation law $\\partial_t u + \\nabla \\cdot F(u) = 0$ posed on a $d$-dimensional Cartesian grid with $N$ cells per spatial dimension. A finite volume Godunov-type method advances the cell averages by evaluating numerical fluxes that, in the multidimensional case, couple $2^d$ local states at each hypercube corner. Your task is to examine how the number of primitive local problems per time step scales with the spatial dimension $d$, and to construct and analyze a probabilistic surrogate based on a Partial Differential Equation (PDE) for a probability density function that replaces the multidimensional Riemann solver.\n\nYou must complete three components:\n\n1. Complexity model for multidimensional Riemann solvers.\n   - Starting from the definition of a Godunov flux as a solution of local Riemann problems and the combinatorial structure of a $d$-dimensional hypercube, derive the per-time-step operation count,\n     $$W_{\\mathrm{R}}(d,N) = N^d \\cdot 2^d,$$\n     interpreted as the number of corner-coupled, $2^d$-state local problems per time step over the whole grid.\n   - Justify why the factor $2^d$ enters as the number of hypercube corner states and why $N^d$ is the number of cells.\n\n2. Probabilistic surrogate based on a PDE for a probability density function.\n   - Let $U(\\mathbf{x},t)$ be modeled as a random variable at each spatial point with a probability density $p(s;\\mathbf{x},t)$ on the state space variable $s \\in \\mathbb{R}$. Use conservation of the mean to write\n     $$\\partial_t \\mu(\\mathbf{x},t) + \\nabla \\cdot \\mathbb{E}[F(U(\\mathbf{x},t))] = 0,$$\n     where $\\mu(\\mathbf{x},t) = \\mathbb{E}[U(\\mathbf{x},t)]$ and $\\mathbb{E}[F(U)] = \\int_{\\mathbb{R}} F(s) p(s;\\mathbf{x},t)\\,\\mathrm{d}s.$\n   - Close the system by prescribing an evolution PDE for the density $p$ in the state variable $s$ that preserves mean and variance at leading order. One principled choice is a Fokker–Planck (also known as forward Kolmogorov) equation,\n     $$\\partial_t p(s;\\mathbf{x},t) + \\partial_s\\big(a(s;\\mu,\\sigma)\\,p(s;\\mathbf{x},t)\\big) = \\nu\\,\\partial_s^2 p(s;\\mathbf{x},t),$$\n     where $a(s;\\mu,\\sigma) = -\\gamma\\,(s-\\mu)$ is a linear drift and $\\nu \\ge 0$ is a diffusion coefficient chosen to match a target variance $\\sigma^2(\\mathbf{x},t)$, which evolves according to a closure ordinary differential equation consistent with the selected drift and diffusion. Argue that a first-order finite volume or finite difference discretization of this one-dimensional PDE in $s$ with $S$ points per cell costs a number of operations proportional to $S$ per spatial cell and per time step.\n   - Derive the surrogate operation count per time step,\n     $$W_{\\mathrm{PDF}}(d,N,S) = N^d \\cdot S,$$\n     where $S$ is the number of state-space grid points in $s$ used to approximate the integral for $\\mathbb{E}[F(U)]$ and to update $p$.\n\n3. Quantitative comparison on a benchmark flux and quadrature accuracy assessment.\n   - Consider the specific flux $F(u) = \\big(u^2/2,\\ldots,u^2/2\\big)$ with $d$ identical components.\n   - For independent test cases with prescribed $(d,N,S,\\mu,\\sigma)$, let the density $p(s)$ be a Gaussian with mean $\\mu$ and standard deviation $\\sigma$. Compute the exact mean flux magnitude per component,\n     $$\\Phi_{\\mathrm{exact}} = \\frac{1}{2}\\,\\mathbb{E}[U^2] = \\frac{1}{2}\\left(\\mu^2 + \\sigma^2\\right).$$\n   - Approximate $\\Phi_{\\mathrm{exact}}$ using a composite trapezoidal rule with $S$ nodes on the truncated interval $[\\mu - 4\\sigma,\\ \\mu + 4\\sigma]$:\n     $$\\Phi_{\\mathrm{trap}}(S;\\mu,\\sigma) = \\frac{1}{2}\\int_{\\mu-4\\sigma}^{\\mu+4\\sigma} s^2 \\, \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\!\\Big(-\\frac{(s-\\mu)^2}{2\\sigma^2}\\Big)\\,\\mathrm{d}s,$$\n     discretized with $S$ uniformly spaced points. Define the absolute quadrature error\n     $$\\mathrm{err}(S;\\mu,\\sigma) = \\big|\\Phi_{\\mathrm{trap}}(S;\\mu,\\sigma) - \\Phi_{\\mathrm{exact}}\\big|.$$\n   - For each test case, compute and report the triplet $\\big(W_{\\mathrm{R}}(d,N),\\ W_{\\mathrm{PDF}}(d,N,S),\\ \\mathrm{err}(S;\\mu,\\sigma)\\big)$.\n\nFundamental bases you must use:\n- The definition of a finite volume method for hyperbolic conservation laws and the count of hypercube corners in $d$ dimensions as $2^d$.\n- The definition of the expectation of a function under a density and the Gaussian probability density function.\n- The Fokker–Planck equation as the conservation law for probability in state space and its finite difference discretization cost scaling with the number of state grid points.\n\nYour program must implement the operation-count formulas and the quadrature error calculation. You do not need to simulate the full evolution in physical space; instead, you must quantify the costs and the quadrature error for the specified flux and Gaussian density.\n\nTest suite:\n- Case $1$: $(d,N,S,\\mu,\\sigma) = (3,16,9,0.8,0.3)$.\n- Case $2$: $(d,N,S,\\mu,\\sigma) = (5,8,17,1.0,0.5)$.\n- Case $3$: $(d,N,S,\\mu,\\sigma) = (7,4,33,1.2,0.4)$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of triplets in a single outer list. Each triplet must be of the form $[W_{\\mathrm{R}},W_{\\mathrm{PDF}},\\mathrm{err}]$ with $W_{\\mathrm{R}}$ and $W_{\\mathrm{PDF}}$ as integers and $\\mathrm{err}$ as a floating-point number. For example:\n  - $[[10,20,0.125],[\\ldots],[\\ldots]]$.",
            "solution": "The problem is valid as it is scientifically grounded, self-contained, and well-posed. We proceed with the solution, which is structured in three parts as requested.\n\n**1. Complexity Model for Multidimensional Riemann Solvers**\n\nThe problem considers a hyperbolic conservation law, $\\partial_t u + \\nabla \\cdot F(u) = 0$, solved on a $d$-dimensional Cartesian grid using a finite volume method. The grid consists of $N$ cells, or control volumes, along each of the $d$ spatial dimensions.\n\nThe total number of cells in the computational domain is the product of the number of cells in each dimension, which amounts to $N^d$. A finite volume method computes an update for the cell-averaged state variable in each of these $N^d$ cells at every time step. Therefore, the total computational work per time step, $W$, must be proportional to $N^d$.\n\nThe update for a single cell requires the evaluation of numerical fluxes across its boundaries. The problem describes a Godunov-type method where the flux evaluation is particularly complex in multiple dimensions. Specifically, it involves solving local problems at the corners (vertices) of each cell. A cell in a $d$-dimensional Cartesian grid is a hypercube. A fundamental geometric property of a $d$-dimensional hypercube is that it possesses $2^d$ vertices. For example, a line segment in $d=1$ has $2^1=2$ endpoints, a square in $d=2$ has $2^2=4$ corners, and a cube in $d=3$ has $2^3=8$ corners.\n\nThe problem states that the numerical flux calculation couples the $2^d$ local states from the cells that meet at each hypercube corner. This defines a \"local problem\" that must be resolved. The total workload is modeled by counting the number of such local problems that must be solved per time step over the entire grid. By assuming that the work for updating one cell is proportional to the number of its corners where these complex interactions occur, the work per cell scales as $2^d$.\n\nCombining these two factors—the total number of cells and the work per cell—we arrive at the overall operation count. The total number of corner-coupled local problems to be solved per time step is the product of the number of cells, $N^d$, and the number of corners per cell, $2^d$. This yields the complexity model:\n$$W_{\\mathrm{R}}(d,N) = N^d \\cdot 2^d$$\nThis expression highlights the \"curse of dimensionality\": the computational cost grows exponentially with the spatial dimension $d$, which makes standard high-order, multidimensional Godunov-type methods prohibitively expensive for $d > 3$ or $d > 4$.\n\n**2. Probabilistic Surrogate Based on a PDE for a Probability Density Function**\n\nTo circumvent the exponential scaling, a surrogate model is introduced. Instead of tracking the exact state $u(\\mathbf{x}, t)$, we model it as a random variable $U(\\mathbf{x}, t)$ characterized by a probability density function (PDF), $p(s;\\mathbf{x},t)$, where $s$ is the state-space variable. The evolution is then described in terms of the statistics of this distribution.\n\nThe evolution of the mean, $\\mu(\\mathbf{x},t) = \\mathbb{E}[U]$, is given by taking the expectation of the original conservation law, resulting in:\n$$\\partial_t \\mu(\\mathbf{x},t) + \\nabla \\cdot \\mathbb{E}[F(U(\\mathbf{x},t))] = 0$$\nwhere the expected flux is $\\mathbb{E}[F(U)] = \\int_{\\mathbb{R}} F(s) p(s;\\mathbf{x},t)\\,\\mathrm{d}s$. This equation is not closed, as the evolution of $\\mu$ depends on the full PDF $p(s)$.\n\nA closure is provided by prescribing a transport equation for the PDF $p(s)$ itself. The problem specifies a Fokker-Planck equation, which is a one-dimensional advection-diffusion equation in the state-space variable $s$:\n$$\\partial_t p(s;\\mathbf{x},t) + \\partial_s\\big(a(s;\\mu,\\sigma)\\,p(s;\\mathbf{x},t)\\big) = \\nu\\,\\partial_s^2 p(s;\\mathbf{x},t)$$\nThis equation must be solved locally within each of the $N^d$ spatial cells of the physical domain grid. Crucially, this PDE is always one-dimensional (in the variable $s$), regardless of the physical space dimension $d$.\n\nTo solve this PDE numerically, the continuous state space $s$ is discretized into a set of $S$ grid points. A standard numerical method, such as a finite difference or finite volume scheme, applied to this one-dimensional PDE on $S$ points requires a number of operations proportional to $S$ to advance one time step. For example, an explicit scheme for $p_j^{n+1}$ will depend on values at neighboring points (e.g., $p_{j-1}^n, p_j^n, p_{j+1}^n$), leading to an $O(S)$ computational cost.\n\nSimilarly, the expected flux integral $\\mathbb{E}[F(U)]$ is approximated using a quadrature rule over the same $S$ points in state space, which also incurs a cost proportional to $S$. Therefore, the total work per spatial cell consists of solving the 1D Fokker-Planck equation and computing the moment integral, both of which scale as $O(S)$.\n\nAs this work must be done for each of the $N^d$ spatial cells, the total operation count for the surrogate model per time step is the product of the number of cells and the work per cell in state space:\n$$W_{\\mathrm{PDF}}(d,N,S) = N^d \\cdot S$$\nThis model replaces the exponential dependence on dimension, $2^d$, with a linear dependence on the number of state-space discretization points, $S$. The value of $S$ is determined by the desired accuracy for representing the PDF and calculating its moments, but it is independent of $d$. This transition from exponential to algebraic scaling in the per-cell cost is the key advantage of the surrogate approach for high-dimensional problems.\n\n**3. Quantitative Comparison and Quadrature Accuracy**\n\nWe now perform a quantitative comparison for the specific flux function $F(u) = (u^2/2, \\ldots, u^2/2)$ and a Gaussian PDF for the state variable $U$.\n\nThe component of the expected flux is $\\mathbb{E}[F_i(U)] = \\mathbb{E}[U^2/2] = \\frac{1}{2}\\mathbb{E}[U^2]$. A standard result from probability theory states that for a random variable $U$ with mean $\\mu$ and variance $\\sigma^2$, its second moment is $\\mathbb{E}[U^2] = \\mu^2 + \\sigma^2$. Thus, the exact value of the mean flux component is:\n$$\\Phi_{\\mathrm{exact}} = \\frac{1}{2}\\left(\\mu^2 + \\sigma^2\\right)$$\n\nThis exact value is approximated using a numerical quadrature. The problem specifies the composite trapezoidal rule with $S$ points on the truncated interval $[\\mu - 4\\sigma, \\mu + 4\\sigma]$. The integral to be approximated is:\n$$\\Phi_{\\mathrm{approx}}(S;\\mu,\\sigma) = \\int_{\\mu-4\\sigma}^{\\mu+4\\sigma} \\frac{s^2}{2} \\, p(s;\\mu,\\sigma)\\,\\mathrm{d}s$$\nwhere $p(s;\\mu,\\sigma)$ is the Gaussian PDF:\n$$p(s;\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{(s-\\mu)^2}{2\\sigma^2}\\right)$$\nThe composite trapezoidal rule approximates this integral by discretizing the interval with $S$ uniformly spaced points $\\{s_i\\}_{i=0}^{S-1}$ and summing the areas of the resulting trapezoids.\n\nThe absolute quadrature error is then defined as the difference between the approximate and exact values:\n$$\\mathrm{err}(S;\\mu,\\sigma) = \\big|\\Phi_{\\mathrm{approx}}(S;\\mu,\\sigma) - \\Phi_{\\mathrm{exact}}\\big|$$\n\nFor each test case $(\\mathrm{d}, N, S, \\mu, \\sigma)$, we compute the triplet $(W_{\\mathrm{R}}(d,N), W_{\\mathrm{PDF}}(d,N,S), \\mathrm{err}(S;\\mu,\\sigma))$. The results, as calculated by the provided program, demonstrate the trade-off. For low dimensions (e.g., $d=3$), the conventional method can be competitive or even cheaper if $S$ is large enough ($S > 2^3 = 8$). However, as dimension $d$ increases, $W_{\\mathrm{R}}$ grows exponentially, while $W_{\\mathrm{PDF}}$ scales much more moderately, quickly establishing the computational superiority of the surrogate model, provided that a modest $S$ yields sufficient accuracy for the quadrature.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Calculates operational counts and quadrature error for three test cases\n    comparing a standard multidimensional solver with a probabilistic surrogate.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (d, N, S, mu, sigma)\n    test_cases = [\n        (3, 16, 9, 0.8, 0.3),\n        (5, 8, 17, 1.0, 0.5),\n        (7, 4, 33, 1.2, 0.4),\n    ]\n\n    results = []\n    for case in test_cases:\n        d, N, S, mu, sigma = case\n\n        # 1. Calculate the operation count for the multidimensional Riemann solver.\n        # W_R = N^d * 2^d\n        # Python's integers handle arbitrary size, so no overflow issues.\n        work_riemann = N**d * 2**d\n\n        # 2. Calculate the operation count for the probabilistic surrogate model.\n        # W_PDF = N^d * S\n        work_pdf = N**d * S\n\n        # 3. Calculate the quadrature error for the expected flux.\n        \n        # 3a. Compute the exact mean flux magnitude per component.\n        # Phi_exact = E[U^2]/2 = (mu^2 + sigma^2)/2\n        phi_exact = 0.5 * (mu**2 + sigma**2)\n\n        # 3b. Approximate the flux using the composite trapezoidal rule.\n        # The integration is over the interval [mu - 4*sigma, mu + 4*sigma].\n        # This interval captures >99.99% of the probability mass for a Gaussian.\n        a = mu - 4 * sigma\n        b = mu + 4 * sigma\n\n        # Generate S uniformly spaced nodes for the quadrature.\n        s_nodes = np.linspace(a, b, S)\n\n        # Evaluate the Gaussian PDF at these nodes.\n        pdf_values = norm.pdf(s_nodes, loc=mu, scale=sigma)\n\n        # The integrand is (s^2 / 2) * p(s), but the problem statement's integral\n        # form is for E[U^2]/2 in the text, but the integral is defined for E[U^2]\n        # with a 1/2 outside. We follow the formula:\n        # Phi_trap = (1/2) * Integral[s^2 * p(s) ds]\n        integrand = s_nodes**2 * pdf_values\n        \n        # Use numpy's trapezoidal rule for numerical integration.\n        integral_val = np.trapz(integrand, s_nodes)\n        phi_trap = 0.5 * integral_val\n\n        # 3c. Compute the absolute quadrature error.\n        quadrature_error = abs(phi_trap - phi_exact)\n\n        # Store the triplet of results.\n        # Ensure work counts are integers as specified.\n        results.append([int(work_riemann), int(work_pdf), quadrature_error])\n\n    # Final print statement in the exact required format: [[...],[...],[...]]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "This final practice explores a different paradigm for defeating the curse of dimensionality, focusing on high-dimensional inverse problems. Here, the key is not to approximate the entire state space, but to exploit an underlying structural property: sparsity. You will implement a PDE-constrained inverse problem that uses the principles of Compressed Sensing (CS) to recover a sparse, high-dimensional coefficient field from a surprisingly small number of measurements, demonstrating how the required data can scale logarithmically rather than exponentially with the system's size .",
            "id": "3454717",
            "problem": "Consider the elliptic partial differential equation on the unit hypercube $[0,1]^d$ with homogeneous Dirichlet boundary conditions,\n$$\n-\\Delta u + a(x) u = f(x),\n$$\nwhere $u$ is the state, $f$ is a known forcing, and $a(x)$ is an unknown coefficient field to be recovered. Assume $a(x)$ is sparse on a uniform grid with $n$ interior points per coordinate, so the total number of degrees of freedom is $N = n^d$. Let the discretization be defined by a classical Finite Difference (FD) scheme for the negative Laplacian $-\\Delta$ on the interior grid, yielding a symmetric positive definite matrix $K \\in \\mathbb{R}^{N \\times N}$ and discrete state $u \\in \\mathbb{R}^N$. The discrete model is\n$$\n(K + D(a)) u = f,\n$$\nwhere $D(a)$ denotes the diagonal matrix formed from the entries of the unknown vector $a \\in \\mathbb{R}^N$. Consider $P$ probing experiments with forcings $f_i \\in \\mathbb{R}^N$, and measurements of the state at $M_{\\text{sensors}}$ distinct grid nodes indexed by $\\{p_m\\}_{m=1}^{M_{\\text{sensors}}}$. The total number of scalar measurements over all probes is $M = P \\cdot M_{\\text{sensors}}$.\n\nAssume that the coefficient $a$ is small in amplitude so that linearization around the background $a=0$ is appropriate. Let $u_{0,i}$ be the background solution for probe $i$, defined by\n$$\nK u_{0,i} = f_i.\n$$\nLet $u_i$ be the solution to the full model,\n$$\n(K + D(a)) u_i = f_i.\n$$\nDefine the measurement residuals by subtracting the background,\n$$\nr_{i,m} = u_i[p_m] - u_{0,i}[p_m].\n$$\nUsing first-order perturbation analysis of the resolvent $(K + D(a))^{-1}$ and neglecting higher-order terms in $a$, one obtains the linearized relationship\n$$\nr_{i,m} \\approx -\\sum_{j=1}^N G_{p_m,j}\\, u_{0,i}[j]\\, a[j],\n$$\nwhere $G = K^{-1}$ is the discrete Green's function. Stacking all probes and sensors, the residual vector $r \\in \\mathbb{R}^{M}$ is approximately linear in $a$,\n$$\nr \\approx A a,\n$$\nwith the sensing matrix $A \\in \\mathbb{R}^{M \\times N}$ defined by rows\n$$\nA_{(i,m),j} = - G_{p_m,j}\\, u_{0,i}[j],\n$$\nfor $i \\in \\{1,\\dots,P\\}$, $m \\in \\{1,\\dots,M_{\\text{sensors}}\\}$, and $j \\in \\{1,\\dots,N\\}$. This is a compressive measurement model under Partial Differential Equation (PDE) constraints. The goal is to recover the sparse $a$ by minimizing the $\\ell_1$ norm of $a$ subject to the linearized PDE constraints. In practice, due to linearization error and numerical noise, a stable relaxation is used:\n$$\n\\min_{a \\in \\mathbb{R}^N} \\frac{1}{2}\\,\\|A a - r\\|_2^2 + \\lambda \\|a\\|_1,\n$$\nwhich is the least-squares $\\ell_1$-regularized formulation known as the LASSO in Compressed Sensing (CS).\n\nUnder the Restricted Isometry Property (RIP) for suitable random probing and sensing, theoretical compressed sensing guarantees state that the number of measurements $M$ required for exact (or stable) recovery scales as\n$$\nM \\gtrsim C\\, s \\log\\!\\left(\\frac{N}{s}\\right),\n$$\nwhere $s$ is the sparsity level of $a$ and $C>0$ is a universal constant. This logarithmic dependence on $N$ mitigates the curse of dimensionality, since $N=n^d$ grows exponentially in $d$ but $\\log(N)$ grows only linearly in $d$.\n\nYour task is to write a complete, runnable program that:\n- Constructs the FD matrix $K$ for the operator $-\\Delta$ on a uniform interior grid with $n$ points per axis in dimension $d$ (Dirichlet boundary conditions), using grid spacing $h = 1/(n+1)$ and $K = \\frac{1}{h^2}\\sum_{k=1}^d L_k$, where each $L_k$ is the Kronecker sum contribution with a $1$-dimensional second difference matrix $T \\in \\mathbb{R}^{n \\times n}$ having diagonal entries $2$ and off-diagonals $-1$.\n- Generates a sparse ground truth $a^\\star \\in \\mathbb{R}^N$ with exactly $s$ nonzero entries at random locations and amplitudes in the range $[0.02, 0.05]$, with random signs, to ensure small perturbations.\n- For each test case, selects $P$ random probe forcings $f_i$ with independent standard normal entries and $M_{\\text{sensors}} = M/P$ distinct sensor indices $p_m$ chosen uniformly at random without replacement.\n- Forms the measurement residuals $r$ by solving the full nonlinear discrete PDE $(K + D(a^\\star))u_i = f_i$ and subtracting the background $K u_{0,i} = f_i$ at the sensor nodes.\n- Builds the linearized sensing matrix $A$ using the discrete Green's rows at the sensor nodes and the background solutions $u_{0,i}$ via the formula above.\n- Recovers $a$ by solving the relaxed LASSO with an Iterative Soft Thresholding Algorithm (ISTA), defined as\n$$\na^{(k+1)} = \\mathcal{S}_{\\lambda/L}\\!\\left(a^{(k)} - \\frac{1}{L} A^\\top(A a^{(k)} - r)\\right),\n$$\nwhere $\\mathcal{S}_{\\tau}(x) = \\operatorname{sign}(x)\\cdot \\max(|x|-\\tau,0)$ is the soft-thresholding operator applied elementwise, and $L$ is a Lipschitz constant for $\\nabla \\left(\\frac{1}{2}\\|A a - r\\|_2^2\\right)$ equal to the largest eigenvalue of $A^\\top A$ (estimated by power iteration). Use $\\lambda = \\alpha \\|r\\|_2/\\sqrt{M}$ with $\\alpha = 0.05$.\n- Evaluates recovery success by the relative error\n$$\n\\epsilon = \\frac{\\|a_{\\text{rec}} - a^\\star\\|_2}{\\|a^\\star\\|_2}\n$$\nand declares success if $\\epsilon \\leq 0.2$.\n- Computes the compressed sensing threshold\n$$\nM_{\\text{CS}} = \\left\\lceil C\\, s \\log\\!\\left(\\frac{N}{s}\\right)\\right\\rceil\n$$\nwith $C=4$, and reports whether the chosen $M$ is above or below this threshold in the analysis, but the program’s final output must only contain success booleans as specified below.\n\nTest suite:\n- Case $1$: $(d,n,s,M,P) = (1,32,3,30,5)$.\n- Case $2$: $(d,n,s,M,P) = (2,10,5,60,5)$.\n- Case $3$: $(d,n,s,M,P) = (3,6,8,120,6)$.\n- Case $4$: $(d,n,s,M,P) = (3,6,8,80,5)$.\n- Case $5$: $(d,n,s,M,P) = (2,12,20,120,6)$.\n\nAll angles (none in this problem) would be in radians if present. There are no physical units in this problem. Use the natural logarithm. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[ \\text{True}, \\text{False}, \\dots ]$), representing the success booleans for the cases in the order above. Ensure the program is self-contained, deterministic, and uses a fixed random seed for reproducibility.\n\nCompressed Sensing (CS), Restricted Isometry Property (RIP), Finite Difference (FD), and Iterative Soft Thresholding Algorithm (ISTA) must be defined as above upon their first appearance.",
            "solution": "The user has provided a valid, well-posed, and scientifically grounded problem. The task is to simulate a compressed sensing experiment for parameter identification in an elliptic partial differential equation (PDE), and to report the success of the recovery for several test cases.\n\nThe problem begins with the elliptic PDE on the unit hypercube $[0,1]^d$:\n$$\n-\\Delta u + a(x) u = f(x)\n$$\nsubject to homogeneous Dirichlet boundary conditions. Here, $u$ is the solution state, $f(x)$ is a known forcing term, and $a(x)$ is an unknown coefficient field which we aim to recover. It is assumed that $a(x)$ is sparse.\n\nThe first step is to discretize the PDE. A uniform grid with $n$ interior points in each of the $d$ dimensions is used. The total number of unknowns (degrees of freedom) is thus $N = n^d$. The grid spacing is $h = 1/(n+1)$. The continuous PDE is approximated using a second-order Finite Difference (FD) scheme. The negative Laplacian operator, $-\\Delta$, is discretized into a symmetric positive definite matrix $K \\in \\mathbb{R}^{N \\times N}$. This matrix is constructed as a sum of contributions from each spatial dimension:\n$$\nK = \\frac{1}{h^2} \\sum_{k=1}^d L_k\n$$\nEach matrix $L_k$ represents the second-order difference operator along the $k$-th coordinate axis. It is formed using a Kronecker product structure:\n$$\nL_k = I \\otimes \\dots \\otimes I \\otimes T \\otimes I \\otimes \\dots \\otimes I\n$$\nwhere $T \\in \\mathbb{R}^{n \\times n}$ is the one-dimensional second-difference matrix, with $2$ on the diagonal and $-1$ on the super- and sub-diagonals, and $I$ is the $n \\times n$ identity matrix. The coefficient field $a(x)$ is discretized into a vector $a \\in \\mathbb{R}^N$. The discrete form of the PDE becomes a system of linear equations:\n$$\n(K + D(a)) u = f\n$$\nwhere $u, f, a \\in \\mathbb{R}^N$ are the discrete versions of the state, forcing, and coefficient, and $D(a)$ is the diagonal matrix formed from the elements of vector $a$.\n\nThe inverse problem is to find $a$ from measurements of $u$. The measurement process involves $P$ experiments, or probes. For each probe $i \\in \\{1, \\dots, P\\}$, a known forcing $f_i$ is applied, and the resulting state $u_i$ is measured at $M_{\\text{sensors}}$ specific grid locations, $\\{p_m\\}_{m=1}^{M_{\\text{sensors}}}$. The total number of scalar measurements is $M = P \\cdot M_{\\text{sensors}}$.\n\nTo make the problem tractable, a linearization is performed. We assume the coefficient $a$ represents a small perturbation around a background of $a=0$. The background state for probe $i$, denoted $u_{0,i}$, is the solution to the unperturbed problem:\n$$\nK u_{0,i} = f_i\n$$\nThe full solution is $u_i = (K + D(a))^{-1}f_i$. Using a first-order Taylor expansion of the resolvent $(K + D(a))^{-1}$ around $a=0$, we get $(K + D(a))^{-1} \\approx K^{-1} - K^{-1}D(a)K^{-1}$. Let $G = K^{-1}$ be the discrete Green's function. The perturbation in the solution is $\\delta u_i = u_i - u_{0,i} \\approx -K^{-1}D(a)K^{-1}f_i = -G D(a) u_{0,i}$. Applying the diagonal matrix $D(a)$ to $u_{0,i}$ is equivalent to an element-wise product. Rearranging, we can express the perturbation as a linear function of $a$: $\\delta u_i = -G D(u_{0,i}) a$.\nThe measurement residuals are defined as $r_{i,m} = u_i[p_m] - u_{0,i}[p_m] = \\delta u_i[p_m]$. This leads to the linearized relationship:\n$$\nr_{i,m} \\approx \\left[ -G D(u_{0,i}) a \\right]_{p_m} = -\\sum_{j=1}^N G_{p_m,j} \\, u_{0,i}[j] \\, a[j]\n$$\nBy stacking all $M$ measurements, we obtain a linear system $r \\approx A a$, where $r \\in \\mathbb{R}^M$ is the vector of all residuals and $A \\in \\mathbb{R}^{M \\times N}$ is the sensing matrix, whose rows are given by:\n$$\nA_{(i,m),j} = -G_{p_m,j} \\, u_{0,i}[j]\n$$\nSince $a$ is assumed to be sparse, this is a Compressed Sensing (CS) problem. Theoretical guarantees for CS, based on the **Restricted Isometry Property (RIP)** of matrix $A$, state that a sparse vector can be recovered from a number of measurements $M$ that is much smaller than its ambient dimension $N$, scaling as $M \\gtrsim s \\log(N/s)$, where $s$ is the sparsity level.\n\nDue to linearization errors and potential measurement noise, an exact solution to $Aa=r$ is not sought. Instead, we solve the LASSO (Least Absolute Shrinkage and Selection Operator) optimization problem, which provides a stable recovery:\n$$\n\\min_{a \\in \\mathbb{R}^N} \\frac{1}{2}\\|A a - r\\|_2^2 + \\lambda \\|a\\|_1\n$$\nHere, $\\lambda > 0$ is a regularization parameter that balances data fidelity (the least-squares term) and sparsity (the $\\ell_1$-norm term). This convex problem is solved using the **Iterative Soft Thresholding Algorithm (ISTA)**, a type of proximal gradient method. The update rule is:\n$$\na^{(k+1)} = \\mathcal{S}_{\\lambda/L}\\left(a^{(k)} - \\frac{1}{L} A^\\top(A a^{(k)} - r)\\right)\n$$\nwhere $k$ is the iteration index. $\\mathcal{S}_{\\tau}(x) = \\operatorname{sign}(x) \\cdot \\max(|x|-\\tau, 0)$ is the element-wise soft-thresholding operator. The step size $1/L$ is determined by $L$, a Lipschitz constant for the gradient of the least-squares term, which is the largest eigenvalue of $A^\\top A$. $L$ is estimated using the power iteration method. The regularization parameter is set to $\\lambda = \\alpha \\|r\\|_2/\\sqrt{M}$ with $\\alpha = 0.05$.\n\nThe recovery is deemed successful if the relative error between the recovered coefficient $a_{\\text{rec}}$ and the ground truth $a^\\star$ is below a threshold:\n$$\n\\epsilon = \\frac{\\|a_{\\text{rec}} - a^\\star\\|_2}{\\|a^\\star\\|_2} \\leq 0.2\n$$\n\nThe procedure for each test case is as follows:\n1.  Construct the matrix $K$ for the given dimension $d$ and grid size $n$.\n2.  Generate a ground truth sparse vector $a^\\star$ of size $N=n^d$ with $s$ non-zero entries.\n3.  Generate $P$ random forcing vectors $f_i$ and select $M_{\\text{sensors}} = M/P$ random sensor locations.\n4.  Compute the discrete Green's function $G = K^{-1}$.\n5.  For each probe, calculate the background solution $u_{0,i}=Gf_i$ and the full nonlinear solution $u_i = (K+D(a^\\star))^{-1}f_i$.\n6.  Assemble the measurement residual vector $r \\in \\mathbb{R}^M$.\n7.  Construct the sensing matrix $A \\in \\mathbb{R}^{M \\times N}$.\n8.  Run ISTA to obtain the recovered coefficient vector $a_{\\text{rec}}$.\n9.  Calculate the relative error $\\epsilon$ and determine if the recovery was successful.\nThe results for all test cases are then reported.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the PDE-constrained compressed sensing problem for a suite of test cases.\n    \"\"\"\n    # Set a fixed random seed for reproducibility.\n    rng = np.random.default_rng(0)\n\n    # Test suite: (d, n, s, M, P)\n    test_cases = [\n        (1, 32, 3, 30, 5),\n        (2, 10, 5, 60, 5),\n        (3, 6, 8, 120, 6),\n        (3, 6, 8, 80, 5),\n        (2, 12, 20, 120, 6),\n    ]\n\n    results = []\n\n    for d, n, s, M, P in test_cases:\n        # 1. Setup\n        N = n**d\n        h = 1.0 / (n + 1)\n        \n        # Construct 1D second-difference matrix T\n        T = np.diag(np.full(n, 2.0)) - np.diag(np.full(n - 1, 1.0), k=1) - np.diag(np.full(n - 1, 1.0), k=-1)\n        \n        # Construct FD matrix K for the negative Laplacian\n        K = np.zeros((N, N))\n        I_n = np.eye(n)\n        for k in range(d):\n            L_k_terms = [I_n] * d\n            L_k_terms[k] = T\n            \n            L_k = L_k_terms[0]\n            for i in range(1, d):\n                L_k = np.kron(L_k, L_k_terms[i])\n            K += L_k\n        K /= h**2\n\n        # 2. Ground Truth, Probes, and Sensors\n        # Generate sparse ground truth a_star\n        a_star = np.zeros(N)\n        support = rng.choice(N, s, replace=False)\n        amplitudes = rng.uniform(0.02, 0.05, size=s)\n        signs = rng.choice([-1, 1], size=s)\n        a_star[support] = amplitudes * signs\n\n        # Generate probes and sensors\n        probes_f = [rng.standard_normal(size=N) for _ in range(P)]\n        M_sensors = M // P\n        sensor_indices = rng.choice(N, size=M_sensors, replace=False)\n\n        # 3. Simulate Measurements\n        # Compute discrete Green's function G = K^{-1}\n        # For N up to 216, direct inversion is feasible.\n        G = np.linalg.inv(K)\n\n        r = np.zeros(M)\n        A = np.zeros((M, N))\n        \n        K_plus_Da_star = K + np.diag(a_star)\n        \n        current_row = 0\n        for i in range(P):\n            f_i = probes_f[i]\n            \n            # Background solution\n            u_0_i = G @ f_i\n            \n            # Full nonlinear solution\n            u_i = np.linalg.solve(K_plus_Da_star, f_i)\n            \n            # Measurement residuals\n            r_block = u_i[sensor_indices] - u_0_i[sensor_indices]\n            \n            # Sensing matrix block for this probe\n            A_block = -G[sensor_indices, :] * u_0_i\n            \n            # Assemble into global r and A\n            r[current_row : current_row + M_sensors] = r_block\n            A[current_row : current_row + M_sensors, :] = A_block\n            current_row += M_sensors\n\n        # 4. Solve with ISTA\n        # Estimate Lipschitz constant L via power iteration\n        AtA = A.T @ A\n        v = rng.standard_normal(size=N)\n        v /= np.linalg.norm(v)\n        for _ in range(100):\n            v_new = AtA @ v\n            v_norm = np.linalg.norm(v_new)\n            if v_norm == 0:\n                break\n            v = v_new / v_norm\n        L = np.linalg.norm(AtA @ v)\n\n        # ISTA parameters\n        alpha = 0.05\n        lambda_val = alpha * np.linalg.norm(r) / np.sqrt(M)\n        n_iter_ista = 1000\n        \n        # Soft-thresholding operator\n        def soft_threshold(x, tau):\n            return np.sign(x) * np.maximum(np.abs(x) - tau, 0)\n\n        # ISTA main loop\n        a_rec = np.zeros(N)\n        if L > 1e-12: # Avoid division by zero if A is zero\n            step_size = 1.0 / L\n            tau = lambda_val * step_size\n            for _ in range(n_iter_ista):\n                grad = A.T @ (A @ a_rec - r)\n                a_rec = soft_threshold(a_rec - step_size * grad, tau)\n\n        # 5. Evaluate and Store Result\n        norm_a_star = np.linalg.norm(a_star)\n        if norm_a_star > 1e-12:\n            rel_error = np.linalg.norm(a_rec - a_star) / norm_a_star\n        else: # Should not happen with s > 0\n            rel_error = np.linalg.norm(a_rec)\n\n        success = rel_error = 0.2\n        results.append(success)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}