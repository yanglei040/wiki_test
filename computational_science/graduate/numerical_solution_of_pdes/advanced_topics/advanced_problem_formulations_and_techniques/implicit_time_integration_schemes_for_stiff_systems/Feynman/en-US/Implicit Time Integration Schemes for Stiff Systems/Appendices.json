{
    "hands_on_practices": [
        {
            "introduction": "Designing implicit methods involves a careful balancing act between accuracy, stability, and computational cost. Singly Diagonally Implicit Runge-Kutta (SDIRK) methods represent an effective compromise, offering strong stability properties while simplifying the nonlinear algebraic systems that must be solved at each stage. This practice focuses on the crucial interplay between A-stability, L-stability, and the method's order of accuracy . By systematically deriving the stability function for a stiffly accurate SDIRK scheme and determining the specific parameter $\\gamma$ that achieves both second-order accuracy and L-stability—a property that ensures strong damping of highly stiff components—you will gain direct experience with the design principles underlying modern stiff solvers.",
            "id": "3407002",
            "problem": "Consider a stiff semi-discrete evolution system obtained from the spatial discretization of a parabolic partial differential equation, written as the ordinary differential equation (ODE) $y'(t) = F(y(t))$ with state vector $y(t) \\in \\mathbb{R}^{m}$ and $F : \\mathbb{R}^{m} \\to \\mathbb{R}^{m}$. You will construct a two-stage Singly Diagonally Implicit Runge–Kutta (SDIRK) method characterized by a common diagonal coefficient $\\gamma$ (the same in both stages) and the stiffly accurate property, meaning the numerical solution update uses the last stage weights identical to the last row of the Runge–Kutta coefficient matrix. Let the time step be $h > 0$ and denote stage values by $Y_{1}$ and $Y_{2}$, and define $k_{1} = F(Y_{1})$, $k_{2} = F(Y_{2})$.\n\nTasks:\n1. Starting from the core definitions of Runge–Kutta methods, the SDIRK structure, and the stiffly accurate property, derive the explicit stage equations $Y_{1}$ and $Y_{2}$ and the update formula $y_{n+1}$ in terms of $\\gamma$, $h$, and the previous solution $y_{n}$, ensuring the method’s abscissae satisfy $c_{1} = \\gamma$ and $c_{2} = 1$.\n2. Using the linear test equation $y'(t) = \\lambda y(t)$ with complex $\\lambda$ satisfying $\\operatorname{Re}(\\lambda) \\le 0$, derive the stability function $R(z)$ with $z = h \\lambda$ for the constructed two-stage SDIRK method. Do not assume any pre-existing form of $R(z)$; instead, derive it directly from the stage equations and the update formula.\n3. From first principles, determine the condition on $\\gamma$ that makes the method Algebraically Stable in the sense of A-stability, that is, the condition ensuring the entire closed left half-plane in the complex plane lies in the method's absolute stability region. Your answer must be stated as an analytic condition on $\\gamma$.\n4. Evaluate the large-$|z|$ asymptotic behavior of the stability function on rays with $\\operatorname{Re}(z) \\to -\\infty$ and determine whether the method is L-stable. State explicitly the condition on $\\gamma$ under which L-stability holds.\n5. Impose the classical second-order accuracy condition for two-stage Runge–Kutta methods and determine the unique $\\gamma \\in (0,1)$ that achieves both second-order accuracy and L-stability. Express your final answer as an exact closed-form analytic expression for $\\gamma$. No numerical rounding is required in the final answer.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard problem in the field of numerical analysis for ordinary differential equations, with all necessary information provided and no contradictions.\n\nA general two-stage Runge-Kutta method for the ODE $y'(t) = F(y(t))$ is defined by the equations:\n$$ Y_1 = y_n + h (a_{11} F(Y_1) + a_{12} F(Y_2)) $$\n$$ Y_2 = y_n + h (a_{21} F(Y_1) + a_{22} F(Y_2)) $$\n$$ y_{n+1} = y_n + h (b_1 F(Y_1) + b_2 F(Y_2)) $$\nThe coefficients are defined by the Butcher tableau:\n$$\n\\begin{array}{c|cc}\nc_1  a_{11}  a_{12} \\\\\nc_2  a_{21}  a_{22} \\\\\n\\hline\n      b_1  b_2\n\\end{array}\n$$\nThe problem specifies several constraints on this general form.\nFirst, it is a Singly Diagonally Implicit Runge-Kutta (SDIRK) method with a common diagonal coefficient $\\gamma$. This implies that the coefficient matrix $A = (a_{ij})$ is lower triangular ($a_{ij}=0$ for $j>i$) and the diagonal elements are equal ($a_{11}=a_{22}=\\gamma$). The tableau becomes:\n$$\n\\begin{array}{c|cc}\nc_1  \\gamma  0 \\\\\nc_2  a_{21}  \\gamma \\\\\n\\hline\n      b_1  b_2\n\\end{array}\n$$\nSecond, the abscissae are given as $c_1 = \\gamma$ and $c_2 = 1$. The abscissae must satisfy the consistency condition $c_i = \\sum_{j=1}^2 a_{ij}$. For $i=1$, we have $c_1 = a_{11} + a_{12}$, which is $\\gamma = \\gamma + 0$, a consistent relation. For $i=2$, we have $c_2 = a_{21} + a_{22}$, which gives $1 = a_{21} + \\gamma$, so $a_{21} = 1 - \\gamma$.\nThird, the method is stiffly accurate. This means the solution update uses the last stage, i.e., $y_{n+1} = Y_2$. In terms of the tableau coefficients, this property implies that the weights $b_j$ are equal to the last row of the coefficient matrix $A$. Thus, $b_1 = a_{21} = 1-\\gamma$ and $b_2 = a_{22} = \\gamma$.\nCombining these constraints, the Butcher tableau is fully determined by the parameter $\\gamma$:\n$$\n\\begin{array}{c|cc}\n\\gamma  \\gamma  0 \\\\\n1       1-\\gamma  \\gamma \\\\\n\\hline\n        1-\\gamma  \\gamma\n\\end{array}\n$$\n\n**1. Derivation of Stage Equations and Update Formula**\nUsing the determined Butcher tableau, we can write the explicit stage equations. Let $k_1 = F(Y_1)$ and $k_2 = F(Y_2)$.\nThe first stage equation is:\n$$ Y_1 = y_n + h (a_{11} k_1 + a_{12} k_2) = y_n + h (\\gamma k_1 + 0 \\cdot k_2) $$\n$$ Y_1 = y_n + h \\gamma F(Y_1) $$\nThe second stage equation is:\n$$ Y_2 = y_n + h (a_{21} k_1 + a_{22} k_2) = y_n + h ((1-\\gamma) k_1 + \\gamma k_2) $$\n$$ Y_2 = y_n + h((1-\\gamma)F(Y_1) + \\gamma F(Y_2)) $$\nThe update formula, due to the stiffly accurate property, is simply the last stage value:\n$$ y_{n+1} = Y_2 $$\nThese equations define the method. They are implicit since $Y_1$ and $Y_2$ appear on both sides of their respective defining equations.\n\n**2. Derivation of the Stability Function $R(z)$**\nTo find the stability function, we apply the method to the linear test equation $y'(t) = \\lambda y(t)$ where $\\lambda \\in \\mathbb{C}$ with $\\operatorname{Re}(\\lambda) \\le 0$. For this equation, $F(y) = \\lambda y$. Let $z = h \\lambda$.\nThe stage equations become:\n$$ Y_1 = y_n + h \\gamma (\\lambda Y_1) = y_n + z \\gamma Y_1 $$\n$$ Y_2 = y_n + h ((1-\\gamma) \\lambda Y_1 + \\gamma \\lambda Y_2) = y_n + z(1-\\gamma) Y_1 + z \\gamma Y_2 $$\nFirst, we solve the first stage equation for $Y_1$:\n$$ Y_1 (1 - z \\gamma) = y_n \\implies Y_1 = \\frac{1}{1 - z \\gamma} y_n $$\nNext, we solve the second stage equation for $Y_2$:\n$$ Y_2 (1 - z \\gamma) = y_n + z(1-\\gamma) Y_1 $$\nSubstitute the expression for $Y_1$:\n$$ Y_2 (1 - z \\gamma) = y_n + z(1-\\gamma) \\left( \\frac{1}{1 - z \\gamma} \\right) y_n $$\n$$ Y_2 = \\frac{1}{1 - z \\gamma} y_n + \\frac{z(1-\\gamma)}{(1 - z \\gamma)^2} y_n $$\n$$ Y_2 = \\left( \\frac{1 - z \\gamma + z(1-\\gamma)}{(1 - z \\gamma)^2} \\right) y_n $$\n$$ Y_2 = \\left( \\frac{1 - z \\gamma + z - z\\gamma}{(1 - z \\gamma)^2} \\right) y_n $$\n$$ Y_2 = \\left( \\frac{1 + (1 - 2\\gamma)z}{(1 - \\gamma z)^2} \\right) y_n $$\nThe numerical solution update is $y_{n+1} = Y_2$. The stability function is defined by $y_{n+1} = R(z) y_n$. Therefore:\n$$ R(z) = \\frac{1 + (1 - 2\\gamma)z}{(1 - \\gamma z)^2} $$\n\n**3. Condition for A-stability**\nA numerical method is A-stable if its region of absolute stability contains the entire left half-plane $\\mathbb{C}^- = \\{ z \\in \\mathbb{C} \\mid \\operatorname{Re}(z) \\le 0 \\}$. For a rational stability function $R(z)$, this requires two conditions:\n(i) $R(z)$ must be analytic in the open left half-plane $\\operatorname{Re}(z)  0$. The poles of $R(z)$ are given by $(1-\\gamma z)^2 = 0$, which means $z = 1/\\gamma$. For analyticity in the open left half-plane, the pole must not be there, i.e., $\\operatorname{Re}(1/\\gamma) \\ge 0$. As $\\gamma$ is real, this implies $1/\\gamma \\ge 0$, so we must have $\\gamma > 0$.\n(ii) On the boundary of the region, the imaginary axis, we must have $|R(i\\omega)| \\le 1$ for all $\\omega \\in \\mathbb{R}$.\nLet's evaluate $|R(i\\omega)|^2$:\n$$ |R(i\\omega)|^2 = \\left| \\frac{1 + (1 - 2\\gamma)i\\omega}{(1 - i\\gamma\\omega)^2} \\right|^2 = \\frac{|1 + i(1-2\\gamma)\\omega|^2}{|(1 - i\\gamma\\omega)^2|^2} = \\frac{1^2 + ((1-2\\gamma)\\omega)^2}{(|1-i\\gamma\\omega|^2)^2} $$\n$$ |R(i\\omega)|^2 = \\frac{1 + (1-2\\gamma)^2\\omega^2}{(1+\\gamma^2\\omega^2)^2} $$\nThe condition $|R(i\\omega)|^2 \\le 1$ becomes:\n$$ 1 + (1-2\\gamma)^2\\omega^2 \\le (1+\\gamma^2\\omega^2)^2 $$\n$$ 1 + (1 - 4\\gamma + 4\\gamma^2)\\omega^2 \\le 1 + 2\\gamma^2\\omega^2 + \\gamma^4\\omega^4 $$\nSubtracting $1$ from both sides and dividing by $\\omega^2$ (for $\\omega \\neq 0$):\n$$ 1 - 4\\gamma + 4\\gamma^2 \\le 2\\gamma^2 + \\gamma^4\\omega^2 $$\n$$ 1 - 4\\gamma + 2\\gamma^2 \\le \\gamma^4\\omega^2 $$\nThis inequality must hold for all $\\omega \\in \\mathbb{R}$. The right-hand side is non-negative (assuming $\\gamma$ is real) and its minimum value is $0$, which occurs at $\\omega=0$. Therefore, the inequality must hold for this minimum value, leading to the condition:\n$$ 2\\gamma^2 - 4\\gamma + 1 \\le 0 $$\nThis is the condition on $\\gamma$ for A-stability, in conjunction with $\\gamma > 0$. The roots of the quadratic $2\\gamma^2 - 4\\gamma + 1 = 0$ are $\\gamma = \\frac{4 \\pm \\sqrt{16-8}}{4} = 1 \\pm \\frac{\\sqrt{2}}{2}$. Since the quadratic opens upwards, the inequality holds between the roots. The range $1 - \\frac{\\sqrt{2}}{2} \\approx 0.293 > 0$, so the condition $\\gamma > 0$ is satisfied. The condition is $\\gamma \\in [1 - \\frac{\\sqrt{2}}{2}, 1 + \\frac{\\sqrt{2}}{2}]$.\n\n**4. L-stability Analysis**\nA method is L-stable if it is A-stable and its stability function satisfies $\\lim_{|z|\\to\\infty, \\operatorname{Re}(z) \\le 0} |R(z)| = 0$. For a rational function like $R(z)$, this is equivalent to requiring A-stability and $\\lim_{z\\to\\infty} R(z) = 0$. This latter condition holds if the degree of the numerator polynomial is strictly less than the degree of the denominator polynomial.\nOur stability function is:\n$$ R(z) = \\frac{1 + (1 - 2\\gamma)z}{(1 - \\gamma z)^2} = \\frac{(1-2\\gamma)z + 1}{\\gamma^2 z^2 - 2\\gamma z + 1} $$\nAssuming $\\gamma \\neq 0$, the degree of the denominator is $2$.\nThe degree of the numerator is $1$ if $1-2\\gamma \\neq 0$ (i.e., $\\gamma \\neq 1/2$), and it is $0$ if $1-2\\gamma = 0$ (i.e., $\\gamma=1/2$). In either case, the degree of the numerator is strictly less than the degree of the denominator.\nThus, for any $\\gamma \\neq 0$, we have $\\lim_{z\\to\\infty} R(z) = 0$.\nThe A-stability condition requires $\\gamma > 0$. Therefore, any A-stable method of this form is automatically L-stable. The condition for L-stability is the same as the condition for A-stability:\n$$ 2\\gamma^2 - 4\\gamma + 1 \\le 0 $$\n\n**5. Second-order Accuracy and L-stability**\nThe order conditions for a Runge-Kutta method up to order two are:\nOrder 1: $\\sum_{i=1}^2 b_i = 1$.\nOur method has $b_1 = 1-\\gamma$ and $b_2 = \\gamma$, so $\\sum b_i = (1-\\gamma) + \\gamma = 1$. The method is at least first-order for any $\\gamma$.\nOrder 2: $\\sum_{i=1}^2 b_i c_i = 1/2$.\nOur method has $c_1=\\gamma$ and $c_2=1$. The condition is:\n$$ b_1 c_1 + b_2 c_2 = (1-\\gamma)\\gamma + \\gamma(1) = \\gamma - \\gamma^2 + \\gamma = 2\\gamma - \\gamma^2 $$\nSetting this equal to $1/2$:\n$$ 2\\gamma - \\gamma^2 = \\frac{1}{2} $$\nMultiplying by $2$ and rearranging gives the quadratic equation:\n$$ 4\\gamma - 2\\gamma^2 = 1 \\implies 2\\gamma^2 - 4\\gamma + 1 = 0 $$\nThis is precisely the equation that defines the boundary of the A-stability and L-stability regions. The roots are $\\gamma = 1 \\pm \\frac{\\sqrt{2}}{2}$.\nThe problem requires finding the unique $\\gamma \\in (0,1)$.\nThe two roots are $\\gamma_1 = 1 - \\frac{\\sqrt{2}}{2}$ and $\\gamma_2 = 1 + \\frac{\\sqrt{2}}{2}$.\nNumerically, $\\sqrt{2} \\approx 1.414$, so $\\gamma_1 \\approx 1 - 0.707 = 0.293$ and $\\gamma_2 \\approx 1 + 0.707 = 1.707$.\nThe only root in the interval $(0,1)$ is $\\gamma_1 = 1 - \\frac{\\sqrt{2}}{2}$.\nFor this value of $\\gamma$, the L-stability condition $2\\gamma^2 - 4\\gamma + 1 \\le 0$ is satisfied because $2\\gamma^2 - 4\\gamma + 1 = 0$.\nTherefore, the unique value of $\\gamma \\in (0,1)$ that achieves both second-order accuracy and L-stability is $\\gamma = 1 - \\frac{\\sqrt{2}}{2}$.",
            "answer": "$$\\boxed{1 - \\frac{\\sqrt{2}}{2}}$$"
        },
        {
            "introduction": "Beyond Runge-Kutta methods, the family of linear multistep methods—particularly the Backward Differentiation Formulas (BDFs)—constitutes another cornerstone for integrating stiff differential equations. Their utility, however, is not unlimited, and their stability is fundamentally constrained by the Dahlquist root condition. This exercise delves into the zero-stability of the BDF family, a property essential for convergence that is independent of the specific ODE being solved . By establishing the relationship between the method's characteristic polynomial and its generating function, you will perform an analysis that uncovers the famous Dahlquist stability barrier, a profound result that dictates the maximum possible order for a stable BDF method and highlights the inherent trade-offs in numerical method design.",
            "id": "3406947",
            "problem": "Consider the semi-discrete form of a parabolic partial differential equation obtained by second-order centered finite differences in space for the diffusion equation $u_{t} = \\kappa u_{xx}$ on a uniform grid with Dirichlet boundary conditions. This yields a stiff system of ordinary differential equations (ODEs) of the form $\\dot{y}(t) = A y(t) + g(t)$, where $A \\in \\mathbb{R}^{N \\times N}$ is negative definite with large spectral radius as $N \\to \\infty$. For time integration, consider the $k$-step Backward Differentiation Formula (BDF) linear multistep method defined by\n$$\n\\sum_{j=0}^{k} \\alpha_{j} y_{n+j} = h\\, f\\!\\left(t_{n+k}, y_{n+k}\\right),\n$$\nwith $h > 0$ the time step and coefficients $\\{\\alpha_{j}\\}_{j=0}^{k}$ chosen so that the method has order $k$ and $\\beta_{k} = 1$ (and $\\beta_{j} = 0$ for $0 \\leq j  k$). Denote the characteristic polynomial associated with the homogeneous recurrence (i.e., applied to $\\dot{y} = 0$) by\n$$\n\\rho_{k}(\\xi) = \\sum_{j=0}^{k} \\alpha_{j} \\xi^{j}.\n$$\nZero-stability for a $k$-step linear multistep method requires that all roots of $\\rho_{k}(\\xi)$ satisfy $|\\xi| \\leq 1$ and that any root with $|\\xi| = 1$ is simple (Dahlquist root condition). \n\nStarting from the fundamental definitions of linear multistep methods and the Backward Differentiation Formula construction, derive the form of the BDF symbol\n$$\n\\delta_{k}(\\zeta) = \\sum_{m=1}^{k} \\frac{1}{m}\\,(1 - \\zeta)^{m},\n$$\nshow that $\\rho_{k}(\\xi) = \\xi^{k}\\,\\delta_{k}(\\xi^{-1})$, and use the root condition together with an argument principle analysis of $\\delta_{k}(\\zeta)$ on the unit circle $|\\zeta| = 1$ to determine the largest integer $k$ for which the BDF method is zero-stable. Express your final answer as a single integer.",
            "solution": "The problem asks for the largest integer $k$ for which the $k$-step Backward Differentiation Formula (BDF) is zero-stable. The solution requires deriving a key relationship between the characteristic polynomial $\\rho_k(\\xi)$ of the method and the BDF symbol $\\delta_k(\\zeta)$, and then using an argument principle analysis to apply the zero-stability condition.\n\n**Part 1: Derivation of the BDF Symbol and its Relation to $\\rho_k(\\xi)$**\n\nThe $k$-step BDF method is based on approximating the derivative $\\dot{y}(t)$ at time $t_{n+k}$ by the derivative of a unique polynomial $P(t)$ of degree at most $k$ that interpolates the solution values $(t_{n+j}, y_{n+j})$ for $j=0, 1, \\dots, k$. The derivative approximation is given by $P'(t_{n+k})$.\n\nUsing Newton's form of the interpolating polynomial with backward differences, the derivative at $t_{n+k}$ is approximated as:\n$$\n\\dot{y}(t_{n+k}) \\approx \\frac{1}{h} \\sum_{m=1}^{k} \\frac{1}{m} \\nabla^m y_{n+k}\n$$\nwhere $h$ is the constant time step and $\\nabla$ is the backward difference operator, defined by $\\nabla y_{j} = y_{j} - y_{j-1}$.\n\nThe general $k$-step linear multistep method is given by $\\sum_{j=0}^{k} \\alpha_j y_{n+j} = h \\sum_{j=0}^k \\beta_j f_{n+j}$. For a BDF method, we are given that $\\beta_k=1$ and $\\beta_j=0$ for $j  k$. The ODE is $\\dot{y}(t) = f(t, y(t))$. Therefore, the BDF method is:\n$$\n\\sum_{j=0}^{k} \\alpha_j y_{n+j} = h f(t_{n+k}, y_{n+k}) = h \\dot{y}_{n+k}\n$$\nBy comparing the two expressions for $\\dot{y}_{n+k}$, we can define the coefficients $\\alpha_j$ by the relation:\n$$\n\\sum_{j=0}^{k} \\alpha_j y_{n+j} = \\sum_{m=1}^{k} \\frac{1}{m} \\nabla^m y_{n+k}\n$$\nTo find the characteristic polynomial $\\rho_k(\\xi)$, we introduce the forward shift operator $E$, where $E y_j = y_{j+1}$. The backward difference operator can be written as $\\nabla = 1 - E^{-1}$. The above relation becomes:\n$$\n\\left(\\sum_{j=0}^{k} \\alpha_j E^j\\right) y_n = \\left(\\sum_{m=1}^{k} \\frac{1}{m} (1-E^{-1})^m \\right) y_{n+k} = \\left(\\sum_{m=1}^{k} \\frac{1}{m} (1-E^{-1})^m \\right) E^k y_n\n$$\nThe polynomial associated with the operator on the left is the characteristic polynomial $\\rho_k(\\xi) = \\sum_{j=0}^{k} \\alpha_j \\xi^j$. By replacing the operator $E$ with the complex variable $\\xi$, we establish the identity for $\\rho_k(\\xi)$:\n$$\n\\rho_k(\\xi) = \\xi^k \\sum_{m=1}^{k} \\frac{1}{m} (1-\\xi^{-1})^m\n$$\nThe problem defines the BDF symbol as $\\delta_k(\\zeta) = \\sum_{m=1}^{k} \\frac{1}{m}(1 - \\zeta)^{m}$. By setting $\\zeta = \\xi^{-1}$ in this definition, we see that the sum in the expression for $\\rho_k(\\xi)$ is precisely $\\delta_k(\\xi^{-1})$. Thus, we have shown the required relationship:\n$$\n\\rho_k(\\xi) = \\xi^k \\delta_k(\\xi^{-1})\n$$\n\n**Part 2: Zero-Stability Analysis**\n\nThe zero-stability of a linear multistep method is determined by the Dahlquist root condition applied to its first characteristic polynomial, $\\rho_k(\\xi)$. The condition states that all roots of $\\rho_k(\\xi)=0$ must lie within or on the closed unit disk in the complex plane, i.e., $|\\xi| \\le 1$, and any root on the boundary of the disk, $|\\xi| = 1$, must be simple.\n\nFrom the relationship $\\rho_k(\\xi) = \\xi^k \\delta_k(\\xi^{-1})$, we analyze the roots of $\\rho_k(\\xi)$. The expression for $\\rho_k(\\xi)$ is $\\sum_{m=1}^k \\frac{1}{m} \\xi^{k-m} (\\xi-1)^m$, which is a polynomial of degree $k$. The constant term is $\\alpha_0 = \\frac{1}{k}(-1)^k \\neq 0$, so $\\xi=0$ is not a root. Therefore, the roots of $\\rho_k(\\xi)=0$ are the same as the roots of $\\delta_k(\\xi^{-1})=0$.\n\nLet $\\xi_i$ be a root of $\\rho_k(\\xi)=0$. Let $\\zeta_i = \\xi_i^{-1}$. Then $\\delta_k(\\zeta_i) = 0$. The zero-stability condition on the roots of $\\rho_k(\\xi)$ can be translated into a condition on the roots of $\\delta_k(\\zeta)$:\n1.  $|\\xi_i| \\le 1 \\implies |\\zeta_i^{-1}| \\le 1 \\implies |\\zeta_i| \\ge 1$. All roots of $\\delta_k(\\zeta)=0$ must lie outside or on the unit circle.\n2.  If $|\\xi_i| = 1$, the root must be simple. This implies that if $|\\zeta_i| = 1$, the root of $\\delta_k(\\zeta)=0$ must also be simple.\n\nFor any $k \\ge 1$, $\\zeta = 1$ is a root of $\\delta_k(\\zeta)=0$ because $\\delta_k(1) = \\sum_{m=1}^{k} \\frac{1}{m}(1-1)^m = 0$. To check if it is simple, we compute the derivative:\n$$\n\\delta'_k(\\zeta) = \\sum_{m=1}^{k} \\frac{1}{m} \\cdot m(1-\\zeta)^{m-1}(-1) = -\\sum_{j=0}^{k-1} (1-\\zeta)^j = -\\frac{1-(1-\\zeta)^k}{\\zeta}\n$$\nEvaluating at $\\zeta=1$ (by taking the limit as $\\zeta \\to 1$) gives $\\delta'_k(1) = -1 \\neq 0$. Thus, $\\zeta=1$ is a simple root for all $k \\ge 1$. This corresponds to the necessary simple root $\\xi=1$ for $\\rho_k(\\xi)$.\n\n**Part 3: Argument Principle Analysis**\n\nTo determine for which $k$ the method is zero-stable, we must verify that $\\delta_k(\\zeta)=0$ has no roots inside the open unit disk $|\\zeta|1$. By the argument principle, the number of zeros $N$ of $\\delta_k(\\zeta)$ inside the unit circle $C = \\{\\zeta \\in \\mathbb{C} : |\\zeta|=1\\}$ is given by the winding number of the image curve $\\delta_k(C)$ around the origin:\n$$\nN = \\frac{1}{2\\pi} \\Delta_C \\arg(\\delta_k(\\zeta))\n$$\nLet the curve be parameterized by $w_k(\\theta) = \\delta_k(e^{i\\theta})$ for $\\theta \\in [0, 2\\pi]$. The BDF-$k$ method is zero-stable if and only if the winding number of this curve around the origin is $N=0$.\n\nThe curve $w_k(\\theta)$ starts and ends at the origin, since $w_k(0)=w_k(2\\pi)=\\delta_k(1)=0$. For $\\theta \\to 0^+$, $\\delta_k(e^{i\\theta}) \\approx 1-e^{i\\theta} \\approx -i\\theta$, so the curve leaves the origin along the negative imaginary axis. The curve is symmetric with respect to the real axis because $\\delta_k(\\bar{\\zeta}) = \\overline{\\delta_k(\\zeta)}$, which means $w_k(2\\pi - \\theta) = \\overline{w_k(\\theta)}$. At $\\theta=\\pi$, the curve crosses the positive real axis, as $w_k(\\pi) = \\delta_k(-1) = \\sum_{m=1}^k \\frac{2^m}{m} > 0$.\n\nThe winding number $N$ can change from $0$ to a non-zero value only if the curve moves to enclose the origin. Given its path, this requires the curve to cross the negative real axis. This means there must be some $\\theta_0 \\in (0, 2\\pi)$ for which $\\text{Im}(w_k(\\theta_0))=0$ and $\\text{Re}(w_k(\\theta_0))  0$.\n\nA detailed analysis of the curve $w_k(\\theta)$, which is standard in the advanced study of numerical methods, reveals the following behavior:\n- For $k=1, \\dots, 5$, the real part of $w_k(\\theta)$ is positive for all $\\theta \\in (0, 2\\pi)$. The curve lies entirely in the right half-plane, so the winding number is $0$.\n- For $k=6$, the curve enters the left half-plane, meaning $\\text{Re}(w_6(\\theta))$ becomes negative for some values of $\\theta$. However, for those values of $\\theta$, $\\text{Im}(w_6(\\theta))$ is non-zero. The curve creates a loop in the left half-plane but does not cross the negative real axis and does not enclose the origin. The winding number remains $0$. It can also be shown that all roots of $\\delta_6(\\zeta)=0$ apart from $\\zeta=1$ have magnitude strictly greater than $1$. Thus, BDF-6 is zero-stable.\n- For $k=7$, the loop in the left half-plane becomes larger. The curve now crosses the negative real axis, meaning there exists a $\\theta_0$ where $\\text{Im}(w_7(\\theta_0))=0$ and $\\text{Re}(w_7(\\theta_0))0$. This causes the curve to enclose the origin, resulting in a non-zero winding number ($N=2$, accounting for a complex conjugate pair of roots). This signifies that $\\delta_7(\\zeta)=0$ has roots inside the unit disk. Consequently, $\\rho_7(\\xi)=0$ has roots outside the unit disk, violating the root condition. BDF-7 is not zero-stable.\n\nSince the methods are unstable for all $k \\ge 7$, the largest integer $k$ for which the BDF method is zero-stable is $6$.",
            "answer": "$$\\boxed{6}$$"
        },
        {
            "introduction": "A method's theoretical order of accuracy, often derived under idealized assumptions, does not always translate to its observed performance on practical, stiff problems. This phenomenon, known as order reduction, is a critical consideration when selecting and applying an implicit integrator. This computational experiment uses the Method of Lines to transform the 1D heat equation into a stiff system of ODEs, which is then solved with the seemingly robust trapezoidal rule (Crank-Nicolson method), a scheme known to be A-stable and second-order accurate . By empirically measuring the convergence rate, you will observe how the method's accuracy degrades significantly in the presence of time-dependent boundary conditions, providing a tangible demonstration of order reduction and underscoring why more specialized methods are essential for reliably solving stiff PDEs.",
            "id": "3406952",
            "problem": "Consider the one-dimensional heat equation, a linear parabolic Partial Differential Equation (PDE),\n$$\n\\frac{\\partial u}{\\partial t}(x,t) = \\alpha \\frac{\\partial^2 u}{\\partial x^2}(x,t) + f(x,t), \\quad x \\in (0,1), \\ t \\in (0,T],\n$$\nsubject to Dirichlet boundary conditions\n$$\nu(0,t) = g_0(t), \\quad u(1,t) = g_1(t),\n$$\nand an initial condition\n$$\nu(x,0) = u_0(x),\n$$\nwhere $u$ is the temperature, $\\alpha$ is the thermal diffusivity, $f$ is a source term, and $g_0$, $g_1$ are prescribed boundary data. Using the Method Of Lines (MOL), discretize the spatial operator with second-order central differences on a uniform grid with $M$ interior points, and apply the trapezoidal rule (also known as the Crank–Nicolson method) for the time integration. Treat the inhomogeneous Dirichlet boundary conditions consistently by incorporating their contributions into the semi-discrete right-hand side. Quantify the empirical order of convergence in time in the maximum norm (infinity norm) and analyze order reduction for stiff systems with inhomogeneous boundary conditions.\n\nBase the derivation and implementation on the following fundamental and widely accepted facts:\n- The second-order central difference approximation of the second derivative on a uniform grid with spacing $h$ is given by the three-point stencil.\n- The trapezoidal rule is an implicit, $A$-stable, second-order scheme for Ordinary Differential Equations (ODEs).\n- The stiffness of the semi-discrete parabolic PDE increases as the spatial mesh is refined, with eigenvalues of the discrete Laplacian scaling like $h^{-2}$.\n\nImplement a manufactured solution approach to set $f$, $g_0$, $g_1$, and $u_0$ so that the exact solution is known. For each test case below, compute the numerical solution at time $T$ for a sequence of time steps and measure the error in the maximum norm over the interior grid. Then estimate the observed order of convergence $p$ by performing a least-squares linear fit of $\\log_{10}(E(\\Delta t))$ versus $\\log_{10}(\\Delta t)$ across the provided time steps, where $E(\\Delta t)$ denotes the final-time maximum norm error and $\\Delta t$ is the time step.\n\nUse the following test suite, all with $\\alpha = 1$ and $T = 1$:\n\n- Case A (inhomogeneous Dirichlet boundary conditions, smooth time dependence):\n  - Exact solution: $u(x,t) = e^{-t}\\left(\\sin(\\pi x) + x\\right)$.\n  - Boundary data: $g_0(t) = 0$, $g_1(t) = e^{-t}$.\n  - Source term: $f(x,t) = e^{-t}\\left((\\pi^2 - 1)\\sin(\\pi x) - x\\right)$.\n  - Spatial resolution: $M = 400$ interior points.\n  - Time steps: $\\Delta t \\in \\left\\{\\frac{1}{20}, \\frac{1}{40}, \\frac{1}{80}, \\frac{1}{160}\\right\\}$.\n\n- Case B (homogeneous Dirichlet boundary conditions):\n  - Exact solution: $u(x,t) = e^{-t}\\sin(\\pi x)$.\n  - Boundary data: $g_0(t) = 0$, $g_1(t) = 0$.\n  - Source term: $f(x,t) = e^{-t}(\\pi^2 - 1)\\sin(\\pi x)$.\n  - Spatial resolution: $M = 400$ interior points.\n  - Time steps: $\\Delta t \\in \\left\\{\\frac{1}{20}, \\frac{1}{40}, \\frac{1}{80}, \\frac{1}{160}\\right\\}$.\n\n- Case C (inhomogeneous Dirichlet boundary conditions, stronger stiffness via finer mesh):\n  - Exact solution: $u(x,t) = e^{-t^2}\\left(\\sin(\\pi x) + x\\right)$.\n  - Boundary data: $g_0(t) = 0$, $g_1(t) = e^{-t^2}$.\n  - Source term: $f(x,t) = e^{-t^2}\\left(\\pi^2\\sin(\\pi x) - 2t\\left(\\sin(\\pi x) + x\\right)\\right)$.\n  - Spatial resolution: $M = 1600$ interior points.\n  - Time steps: $\\Delta t \\in \\left\\{\\frac{1}{20}, \\frac{1}{40}, \\frac{1}{80}, \\frac{1}{160}\\right\\}$.\n\nFor each case, proceed as follows:\n- Construct the interior grid points $x_i = i h$ for $i = 1,2,\\dots,M$ with $h = \\frac{1}{M+1}$.\n- Form the semi-discrete stiff ODE system for the interior unknowns using the second-order central difference discretization and appropriate time-dependent boundary injection, resulting in an ODE of the form $\\mathbf{y}'(t) = A\\mathbf{y}(t) + \\mathbf{b}(t)$ where $A$ is the discrete Laplacian scaled by $\\alpha$ and $\\mathbf{b}(t)$ accounts for boundary data and source term.\n- Advance in time using the trapezoidal rule applied to $\\mathbf{y}'(t) = A\\mathbf{y}(t) + \\mathbf{b}(t)$ for each prescribed $\\Delta t$, starting from the exact interior initial condition, and compute the numerical solution at $t = T$.\n- Compute the maximum norm error at $t = T$ as $E(\\Delta t) = \\max_i \\left|y_i^{\\text{num}}(T) - u(x_i,T)\\right|$.\n- Estimate the observed order $p$ by fitting a line to the points $\\left(\\log_{10}(\\Delta t), \\log_{10}(E(\\Delta t))\\right)$ via least squares, and report the slope $p$.\n\nYour program should produce a single line of output containing the three observed orders $p$ for Case A, Case B, and Case C, respectively, as a comma-separated list enclosed in square brackets (e.g., \"[pA,pB,pC]\"). All computations are purely mathematical; no physical units are required. Angles do not appear in this problem. Percentages do not appear in this problem.",
            "solution": "To analyze the order reduction of the trapezoidal rule (Crank–Nicolson method) on the one-dimensional heat equation with inhomogeneous Dirichlet boundary conditions, we start from the standard formulation of the parabolic PDE\n$$\n\\frac{\\partial u}{\\partial t}(x,t) = \\alpha \\frac{\\partial^2 u}{\\partial x^2}(x,t) + f(x,t), \\quad x \\in (0,1), \\ t \\in (0,T],\n$$\nwith boundary conditions $u(0,t)=g_0(t)$ and $u(1,t)=g_1(t)$, and initial condition $u(x,0)=u_0(x)$.\n\nThe Method Of Lines (MOL) discretizes the spatial operator while keeping time continuous, yielding a stiff system of Ordinary Differential Equations (ODEs). Using a uniform grid of $M$ interior points $x_i = i h$ for $i=1,\\dots,M$, with $h=(M+1)^{-1}$, the second derivative is approximated by the second-order central difference\n$$\n\\frac{\\partial^2 u}{\\partial x^2}(x_i,t) \\approx \\frac{u_{i-1}(t) - 2u_i(t) + u_{i+1}(t)}{h^2}.\n$$\nWith Dirichlet boundary conditions, $u_0(t)=g_0(t)$ and $u_{M+1}(t)=g_1(t)$ are known values and enter the semi-discrete ODE through a time-dependent boundary injection vector. Denote the interior unknowns by $\\mathbf{y}(t) \\in \\mathbb{R}^M$ with components $y_i(t)=u(x_i,t)$, and define the discrete Laplacian matrix $L \\in \\mathbb{R}^{M \\times M}$ as the tridiagonal matrix with $-2/h^2$ on the diagonal and $1/h^2$ on the immediate sub- and super-diagonals. The semi-discrete ODE can be written as\n$$\n\\mathbf{y}'(t) = \\alpha L \\mathbf{y}(t) + \\alpha \\mathbf{b}_{\\text{bc}}(t) + \\mathbf{f}(t),\n$$\nwhere $\\mathbf{b}_{\\text{bc}}(t) \\in \\mathbb{R}^M$ is zero everywhere except for the first component (which receives $g_0(t)/h^2$) and the last component (which receives $g_1(t)/h^2$), and $\\mathbf{f}(t) \\in \\mathbb{R}^M$ is the interior sampling of the source term $f(x_i,t)$.\n\nWe now apply the trapezoidal rule to the non-autonomous linear ODE\n$$\n\\mathbf{y}'(t) = A \\mathbf{y}(t) + \\mathbf{b}(t), \\quad \\text{with } A = \\alpha L, \\ \\mathbf{b}(t) = \\alpha \\mathbf{b}_{\\text{bc}}(t) + \\mathbf{f}(t).\n$$\nThe trapezoidal rule advances from time $t_n$ to $t_{n+1} = t_n + \\Delta t$ via\n$$\n\\mathbf{y}_{n+1} = \\mathbf{y}_n + \\frac{\\Delta t}{2} \\left( A \\mathbf{y}_{n} + \\mathbf{b}(t_n) + A \\mathbf{y}_{n+1} + \\mathbf{b}(t_{n+1}) \\right),\n$$\nwhich can be re-arranged to the linear system\n$$\n\\left(I - \\frac{\\Delta t}{2} A \\right) \\mathbf{y}_{n+1} = \\left(I + \\frac{\\Delta t}{2} A \\right) \\mathbf{y}_n + \\frac{\\Delta t}{2} \\left( \\mathbf{b}(t_n) + \\mathbf{b}(t_{n+1}) \\right).\n$$\nBecause $A$ is stiff (its spectral radius scales like $h^{-2}$), the system must be solved implicitly at each time step. We precompute the matrix $I \\pm \\frac{\\Delta t}{2} A$ and use a direct sparse solver to efficiently handle multiple time steps with the same $\\Delta t$.\n\nTo test order reduction, we adopt manufactured solutions that define $u(x,t)$, $g_0(t)$, $g_1(t)$, and $f(x,t)$ consistently so that the exact interior solution is known at all times. We consider three cases:\n- Case A: $u(x,t) = e^{-t}\\left(\\sin(\\pi x) + x\\right)$, with $g_0(t) = 0$, $g_1(t) = e^{-t}$, and $f(x,t) = e^{-t}\\left((\\pi^2 - 1)\\sin(\\pi x) - x\\right)$.\n- Case B: $u(x,t) = e^{-t}\\sin(\\pi x)$, with $g_0(t) = 0$, $g_1(t) = 0$, and $f(x,t) = e^{-t}(\\pi^2 - 1)\\sin(\\pi x)$.\n- Case C: $u(x,t) = e^{-t^2}\\left(\\sin(\\pi x) + x\\right)$, with $g_0(t) = 0$, $g_1(t) = e^{-t^2}$, and $f(x,t) = e^{-t^2}\\left(\\pi^2\\sin(\\pi x) - 2t\\left(\\sin(\\pi x) + x\\right)\\right)$.\n\nFor each case, we set $\\alpha = 1$, $T = 1$, and use the respective $M$ values ($M=400$ for Case A and Case B, and $M=1600$ for Case C). We then integrate with $\\Delta t \\in \\left\\{\\frac{1}{20}, \\frac{1}{40}, \\frac{1}{80}, \\frac{1}{160}\\right\\}$ and compute the final-time maximum norm errors\n$$\nE(\\Delta t) = \\max_{1 \\le i \\le M} \\left| y_i^{\\text{num}}(T) - u(x_i,T) \\right|.\n$$\nTo estimate the observed order $p$, we perform a least-squares fit of a straight line to the points $\\left( \\log_{10}(\\Delta t), \\log_{10}(E(\\Delta t)) \\right)$; the slope of this line is the empirical convergence order $p$:\n$$\np \\approx \\text{slope of } \\log_{10}(E(\\Delta t)) \\text{ versus } \\log_{10}(\\Delta t).\n$$\n\nPrinciple-based explanation of order reduction: The trapezoidal rule is globally second-order accurate for sufficiently smooth non-stiff ODEs. However, for stiff systems arising from semi-discrete parabolic PDEs, the presence of time-dependent inhomogeneous boundary conditions introduces a non-autonomous forcing $\\mathbf{b}(t)$ that excites stiff spatial modes at each time step. In the infinity norm, these boundary-driven layers can dominate the global error, and unless the method is specially designed to be stiffly accurate for non-autonomous terms or employs boundary corrections, the observed temporal order of convergence in the maximum norm can reduce from second order to approximately first order. In contrast, with homogeneous Dirichlet boundary conditions, the forcing does not introduce time-dependent boundary layers, and the trapezoidal rule typically maintains second-order accuracy in the maximum norm.\n\nThe algorithm:\n- Build the discrete Laplacian $L$ and the sparse matrices $I \\pm \\frac{\\Delta t}{2}A$ for each $\\Delta t$.\n- Precompute a sparse factorization of $I - \\frac{\\Delta t}{2}A$ to solve the linear system efficiently.\n- March in time using the trapezoidal update with consistent boundary injection and source term sampling.\n- Compute the final-time maximum norm error and fit to obtain $p$.\n\nThe program finally outputs the three observed orders $p$ for Case A, Case B, and Case C, respectively, as a single line in the format \"[pA,pB,pC]\". Based on theory and empirical evidence, we expect approximately first-order convergence for inhomogeneous boundary conditions (Case A and Case C) and approximately second-order convergence for homogeneous boundary conditions (Case B).",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import diags, csc_matrix\nfrom scipy.sparse.linalg import splu\n\ndef build_laplacian_matrix(M, h, alpha=1.0):\n    \"\"\"\n    Build the discrete Laplacian with Dirichlet boundary conditions on M interior points.\n    L is tridiagonal with -2/h^2 on diagonal and 1/h^2 on off-diagonals, scaled by alpha.\n    \"\"\"\n    main = (-2.0 / h**2) * np.ones(M)\n    off = (1.0 / h**2) * np.ones(M - 1)\n    L = diags([off, main, off], offsets=[-1, 0, 1], shape=(M, M), format='csc')\n    return alpha * L\n\ndef trapezoidal_time_integration(M, T, dt, alpha, u_exact, f_fun, g0_fun, g1_fun):\n    \"\"\"\n    Integrate the semi-discrete system y' = A y + b(t), with A = alpha*L,\n    where b(t) = alpha*bc(t) + f(t), bc encodes Dirichlet boundary injection.\n    Return the infinity-norm error at time T.\n    \"\"\"\n    h = 1.0 / (M + 1)\n    x = (np.arange(1, M + 1)) * h\n\n    # Build A = alpha*L\n    A = build_laplacian_matrix(M, h, alpha=alpha)\n    I = diags([np.ones(M)], [0], shape=(M, M), format='csc')\n\n    # Precompute matrices for trapezoidal rule\n    M_lhs = (I - (dt / 2.0) * A).tocsc()\n    M_rhs = (I + (dt / 2.0) * A).tocsc()\n    # Factorize LHS matrix for repeated solves\n    lu = splu(M_lhs)\n\n    # Initial condition from exact solution at t=0\n    y = u_exact(x, 0.0)\n\n    # Number of time steps\n    N_steps = int(round(T / dt))\n    # March in time\n    t = 0.0\n    for n in range(N_steps):\n        t_n = t\n        t_np1 = t + dt\n\n        # Boundary injection vectors at t_n and t_{n+1}\n        bc_n = np.zeros(M)\n        bc_np1 = np.zeros(M)\n        g0_n = g0_fun(t_n)\n        g1_n = g1_fun(t_n)\n        g0_np1 = g0_fun(t_np1)\n        g1_np1 = g1_fun(t_np1)\n        # Injection: first and last entries incorporate boundary values scaled by 1/h^2\n        bc_n[0] = g0_n / h**2\n        bc_n[-1] = g1_n / h**2\n        bc_np1[0] = g0_np1 / h**2\n        bc_np1[-1] = g1_np1 / h**2\n\n        # Source term vectors at t_n and t_{n+1}\n        f_n = f_fun(x, t_n)\n        f_np1 = f_fun(x, t_np1)\n\n        # Assemble RHS: (I + dt/2 A) y_n + dt/2 [alpha*(bc_n + bc_np1) + (f_n + f_np1)]\n        rhs = M_rhs.dot(y) + (dt / 2.0) * (alpha * (bc_n + bc_np1) + (f_n + f_np1))\n\n        # Solve for y_{n+1}\n        y = lu.solve(rhs)\n\n        t = t_np1\n\n    # Compute exact solution at final time and error in infinity norm\n    y_exact_T = u_exact(x, T)\n    err = np.max(np.abs(y - y_exact_T))\n    return err\n\ndef observed_order(errors, dts):\n    \"\"\"\n    Estimate observed order p by least-squares fit of log10(error) vs log10(dt).\n    \"\"\"\n    log_dt = np.log10(np.array(dts))\n    log_err = np.log10(np.array(errors))\n    # Fit line: log_err = p*log_dt + c\n    p, c = np.polyfit(log_dt, log_err, 1)\n    return float(p)\n\n# Manufactured solutions and data for the test cases\ndef case_A_data():\n    alpha = 1.0\n    M = 400\n    T = 1.0\n    dts = [1/20, 1/40, 1/80, 1/160]\n    # Exact solution: u(x,t) = exp(-t) * (sin(pi x) + x)\n    def u_exact(x, t):\n        return np.exp(-t) * (np.sin(np.pi * x) + x)\n    def g0_fun(t):\n        return 0.0\n    def g1_fun(t):\n        return np.exp(-t)\n    # f(x,t) = exp(-t) * ((pi^2 - 1) sin(pi x) - x)\n    def f_fun(x, t):\n        return np.exp(-t) * ((np.pi**2 - 1.0) * np.sin(np.pi * x) - x)\n    return alpha, M, T, dts, u_exact, f_fun, g0_fun, g1_fun\n\ndef case_B_data():\n    alpha = 1.0\n    M = 400\n    T = 1.0\n    dts = [1/20, 1/40, 1/80, 1/160]\n    # Exact solution: u(x,t) = exp(-t) * sin(pi x)\n    def u_exact(x, t):\n        return np.exp(-t) * np.sin(np.pi * x)\n    def g0_fun(t):\n        return 0.0\n    def g1_fun(t):\n        return 0.0\n    # f(x,t) = exp(-t) * (pi^2 - 1) * sin(pi x)\n    def f_fun(x, t):\n        return np.exp(-t) * (np.pi**2 - 1.0) * np.sin(np.pi * x)\n    return alpha, M, T, dts, u_exact, f_fun, g0_fun, g1_fun\n\ndef case_C_data():\n    alpha = 1.0\n    M = 1600\n    T = 1.0\n    dts = [1/20, 1/40, 1/80, 1/160]\n    # Exact solution: u(x,t) = exp(-t^2) * (sin(pi x) + x)\n    def u_exact(x, t):\n        return np.exp(-t**2) * (np.sin(np.pi * x) + x)\n    def g0_fun(t):\n        return 0.0\n    def g1_fun(t):\n        return np.exp(-t**2)\n    # f(x,t) = exp(-t^2) * (pi^2 sin(pi x) - 2 t (sin(pi x) + x))\n    def f_fun(x, t):\n        return np.exp(-t**2) * (np.pi**2 * np.sin(np.pi * x) - 2.0 * t * (np.sin(np.pi * x) + x))\n    return alpha, M, T, dts, u_exact, f_fun, g0_fun, g1_fun\n\ndef run_case(alpha, M, T, dts, u_exact, f_fun, g0_fun, g1_fun):\n    errors = []\n    for dt in dts:\n        err = trapezoidal_time_integration(M, T, dt, alpha, u_exact, f_fun, g0_fun, g1_fun)\n        errors.append(err)\n    p = observed_order(errors, dts)\n    return p\n\ndef solve():\n    # Define the test cases from the problem statement.\n    cases = [case_A_data(), case_B_data(), case_C_data()]\n    results = []\n    for alpha, M, T, dts, u_exact, f_fun, g0_fun, g1_fun in cases:\n        p = run_case(alpha, M, T, dts, u_exact, f_fun, g0_fun, g1_fun)\n        # For stability in printing, limit to a reasonable number of decimals\n        results.append(f\"{p:.6f}\")\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}