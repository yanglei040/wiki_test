{
    "hands_on_practices": [
        {
            "introduction": "To appreciate the power of exponential integrators, we first need to understand the challenge they are designed to solve: stiffness. This exercise demonstrates the problem firsthand by analyzing the stability of the simple explicit Euler method when applied to the heat equation. By deriving the stability constraint from first principles, you will see how the required time step size $\\Delta t$ shrinks quadratically with the spatial grid spacing $h$, a classic symptom of stiffness that makes standard explicit methods prohibitively expensive for finely resolved simulations. ",
            "id": "3389653",
            "problem": "Consider the one-dimensional heat equation $u_{t}=\\nu u_{xx}$ with constant diffusivity $\\nu>0$ posed on a periodic domain $[0,L]$ with periodic boundary conditions. Let a uniform spatial grid of $N$ points be defined by $x_{j}=j h$ for $j=0,1,\\dots,N-1$ with $h=L/N$, and let $u_{j}(t)\\approx u(x_{j},t)$. The second-order central-difference semi-discretization yields the linear autonomous system $\\frac{d}{dt}u_{j}(t)=\\nu\\frac{u_{j-1}(t)-2u_{j}(t)+u_{j+1}(t)}{h^{2}}$ with periodic indexing. Denote the discrete Laplacian operator by $L_{h}$ so that the semi-discrete system is $\\mathbf{u}'(t)=\\nu L_{h}\\mathbf{u}(t)$.\n  \nYou are asked to proceed from fundamental definitions and well-tested facts: (i) circulant matrices on uniform periodic grids are diagonalized by discrete Fourier modes, (ii) the explicit Euler method applied to a linear system $\\mathbf{y}'=A\\mathbf{y}$ produces the amplification matrix $I+\\Delta tA$, and (iii) linear stability for such a method on a normal matrix reduces to requiring that the spectral radius of $I+\\Delta tA$ does not exceed $1$. Do not assume any pre-tabulated eigenvalue formula; derive what is needed from these bases.\n\nTasks:\n  \n1. Compute the full set of eigenvalues of $L_{h}$ by analyzing its action on discrete Fourier modes.\n2. Using the explicit Euler method with time step $\\Delta t$, derive the linear stability constraint of the form $\\Delta t\\leq C h^{2}/\\nu$ that guarantees stability for all spatial modes on the periodic grid.\n3. Determine the minimal constant $C$ (independent of $N$, $h$, $L$, and $\\nu$) that satisfies this bound for all $N\\geq 2$.\n\nProvide the final answer as the exact value of $C$. No rounding is required. Do not include units in your final answer. The answer must be a single number or a single closed-form analytic expression.",
            "solution": "The problem is validated as scientifically sound, well-posed, and self-contained. The provided information is sufficient and consistent for deriving a unique solution. We proceed with the solution by following the three specified tasks.\n\nThe semi-discretization of the heat equation $u_{t}=\\nu u_{xx}$ on a periodic domain $[0,L]$ is given by the system of ordinary differential equations:\n$$\n\\frac{d}{dt}u_{j}(t)=\\nu\\,\\frac{u_{j-1}(t)-2u_{j}(t)+u_{j+1}(t)}{h^{2}}\n$$\nfor $j=0,1,\\dots,N-1$, with grid spacing $h=L/N$ and periodic indexing (i.e., $u_{-1}=u_{N-1}$ and $u_{N}=u_{0}$). This system can be written in matrix form as $\\mathbf{u}'(t)=\\nu L_{h}\\mathbf{u}(t)$, where $\\mathbf{u}(t) = [u_0(t), u_1(t), \\dots, u_{N-1}(t)]^T$ and $L_h$ is the matrix representation of the second-order central difference operator on a periodic grid. The action of $L_h$ on a vector $\\mathbf{v}$ is $(L_h \\mathbf{v})_j = \\frac{1}{h^2}(v_{j-1} - 2v_j + v_{j+1})$. This structure makes $L_h$ a circulant matrix.\n\n**Task 1: Compute the eigenvalues of $L_h$.**\n\nAccording to the provided information, a circulant matrix on a uniform periodic grid is diagonalized by the discrete Fourier modes. The eigenvectors of an $N \\times N$ circulant matrix are the vectors $\\mathbf{w}^{(k)}$ for $k=0, 1, \\dots, N-1$, whose components are given by:\n$$\nw_j^{(k)} = \\exp\\left(i \\frac{2\\pi k j}{N}\\right) \\quad \\text{for } j=0, 1, \\dots, N-1\n$$\nTo find the eigenvalue $\\lambda_k$ corresponding to the eigenvector $\\mathbf{w}^{(k)}$, we apply the operator $L_h$ to $\\mathbf{w}^{(k)}$:\n$$\n(L_h \\mathbf{w}^{(k)})_j = \\frac{1}{h^2} \\left( w_{j-1}^{(k)} - 2w_j^{(k)} + w_{j+1}^{(k)} \\right)\n$$\nSubstituting the expression for $w_j^{(k)}$:\n$$\n(L_h \\mathbf{w}^{(k)})_j = \\frac{1}{h^2} \\left( \\exp\\left(i \\frac{2\\pi k(j-1)}{N}\\right) - 2\\exp\\left(i \\frac{2\\pi k j}{N}\\right) + \\exp\\left(i \\frac{2\\pi k(j+1)}{N}\\right) \\right)\n$$\nWe can factor out the term $\\exp(i \\frac{2\\pi k j}{N}) = w_j^{(k)}$:\n$$\n(L_h \\mathbf{w}^{(k)})_j = \\frac{1}{h^2} \\exp\\left(i \\frac{2\\pi k j}{N}\\right) \\left( \\exp\\left(-i \\frac{2\\pi k}{N}\\right) - 2 + \\exp\\left(i \\frac{2\\pi k}{N}\\right) \\right)\n$$\nUsing Euler's formula, $\\exp(i\\theta) + \\exp(-i\\theta) = 2\\cos(\\theta)$, the term in the parenthesis becomes $2\\cos\\left(\\frac{2\\pi k}{N}\\right) - 2$.\n$$\n(L_h \\mathbf{w}^{(k)})_j = \\frac{1}{h^2} w_j^{(k)} \\left( 2\\cos\\left(\\frac{2\\pi k}{N}\\right) - 2 \\right) = \\frac{2}{h^2} \\left( \\cos\\left(\\frac{2\\pi k}{N}\\right) - 1 \\right) w_j^{(k)}\n$$\nThis is an eigenvalue equation of the form $L_h \\mathbf{w}^{(k)} = \\lambda_k \\mathbf{w}^{(k)}$. The eigenvalue $\\lambda_k$ is:\n$$\n\\lambda_k = \\frac{2}{h^2} \\left( \\cos\\left(\\frac{2\\pi k}{N}\\right) - 1 \\right)\n$$\nUsing the half-angle identity $1 - \\cos(2\\theta) = 2\\sin^2(\\theta)$ with $\\theta = \\frac{\\pi k}{N}$, we can simplify the expression for $\\lambda_k$:\n$$\n\\lambda_k = -\\frac{4}{h^2} \\sin^2\\left(\\frac{\\pi k}{N}\\right)\n$$\nThe full set of eigenvalues of $L_h$ is given by this formula for $k=0, 1, \\dots, N-1$.\n\n**Task 2: Derive the linear stability constraint.**\n\nThe semi-discrete system is $\\mathbf{u}'(t) = A\\mathbf{u}(t)$ with $A = \\nu L_h$. The eigenvalues of $A$ are $\\mu_k = \\nu \\lambda_k = -\\frac{4\\nu}{h^2}\\sin^2\\left(\\frac{\\pi k}{N}\\right)$. Since $L_h$ is a real symmetric circulant matrix, it is normal, and thus $A$ is also normal.\n\nApplying the explicit Euler method with time step $\\Delta t$ yields the iterative scheme:\n$$\n\\mathbf{u}^{n+1} = \\mathbf{u}^n + \\Delta t A \\mathbf{u}^n = (I + \\Delta t A) \\mathbf{u}^n\n$$\nThe amplification matrix is $G = I + \\Delta t A$. The stability condition for a normal matrix $A$ is that the spectral radius of $G$ must not exceed $1$, i.e., $\\rho(G) \\le 1$.\nThe eigenvalues of $G$, denoted by $g_k$, are related to the eigenvalues of $A$ by $g_k = 1 + \\Delta t \\mu_k$.\n$$\ng_k = 1 - \\frac{4\\nu\\Delta t}{h^2}\\sin^2\\left(\\frac{\\pi k}{N}\\right)\n$$\nSince $\\mu_k$ are real, the eigenvalues $g_k$ are also real. The stability condition $\\rho(G) \\le 1$ simplifies to $|g_k| \\le 1$ for all $k=0, 1, \\dots, N-1$. This is equivalent to the two inequalities $-1 \\le g_k \\le 1$.\n\nThe first inequality, $g_k \\le 1$:\n$$\n1 - \\frac{4\\nu\\Delta t}{h^2}\\sin^2\\left(\\frac{\\pi k}{N}\\right) \\le 1 \\implies -\\frac{4\\nu\\Delta t}{h^2}\\sin^2\\left(\\frac{\\pi k}{N}\\right) \\le 0\n$$\nSince $\\nu > 0$, $\\Delta t > 0$, $h^2 > 0$, and $\\sin^2(\\cdot) \\ge 0$, this inequality is always satisfied.\n\nThe second inequality, $g_k \\ge -1$:\n$$\n1 - \\frac{4\\nu\\Delta t}{h^2}\\sin^2\\left(\\frac{\\pi k}{N}\\right) \\ge -1 \\implies 2 \\ge \\frac{4\\nu\\Delta t}{h^2}\\sin^2\\left(\\frac{\\pi k}{N}\\right)\n$$\nSolving for $\\Delta t$, we get:\n$$\n\\Delta t \\le \\frac{2h^2}{4\\nu\\sin^2\\left(\\frac{\\pi k}{N}\\right)} = \\frac{h^2}{2\\nu\\sin^2\\left(\\frac{\\pi k}{N}\\right)}\n$$\nThis inequality must hold for all modes $k=0, 1, \\dots, N-1$. For $k=0$, $\\sin^2(0) = 0$, and the right-hand side is undefined (or infinite), which places no restriction on $\\Delta t$. This corresponds to the constant mode (mean value), which is always stable. For all other modes ($k=1, \\dots, N-1$), the condition must be satisfied. To ensure this, $\\Delta t$ must be less than or equal to the minimum of these upper bounds:\n$$\n\\Delta t \\le \\min_{k=1,\\dots,N-1} \\left( \\frac{h^2}{2\\nu\\sin^2\\left(\\frac{\\pi k}{N}\\right)} \\right) = \\frac{h^2}{2\\nu \\max_{k=1,\\dots,N-1} \\sin^2\\left(\\frac{\\pi k}{N}\\right)}\n$$\nThis is the general stability constraint for a given grid with $N$ points.\n\n**Task 3: Determine the minimal constant $C$.**\n\nWe seek a single constant $C$ such that the stability constraint $\\Delta t \\le C \\frac{h^2}{\\nu}$ holds for all $N \\ge 2$. This requires finding a value $C$ that is valid for all possible grid configurations. A universal stability criterion must be at least as restrictive as the condition for any specific $N$. Thus, for our derived condition to be satisfied, we need:\n$$\nC \\frac{h^2}{\\nu} \\le \\frac{h^2}{2\\nu \\max_{k=1,\\dots,N-1} \\sin^2\\left(\\frac{\\pi k}{N}\\right)}\n$$\nfor all $N \\ge 2$. This simplifies to:\n$$\nC \\le \\frac{1}{2 \\max_{k=1,\\dots,N-1} \\sin^2\\left(\\frac{\\pi k}{N}\\right)}\n$$\nTo satisfy this for all $N \\ge 2$, $C$ must be less than or equal to the infimum of the right-hand side over all $N \\ge 2$:\n$$\nC \\le \\inf_{N \\ge 2} \\left( \\frac{1}{2 \\max_{k=1,\\dots,N-1} \\sin^2\\left(\\frac{\\pi k}{N}\\right)} \\right) = \\frac{1}{2 \\sup_{N \\ge 2} \\left(\\max_{k=1,\\dots,N-1} \\sin^2\\left(\\frac{\\pi k}{N}\\right)\\right)}\n$$\nLet's analyze the term $S_N = \\max_{k=1,\\dots,N-1} \\sin^2\\left(\\frac{\\pi k}{N}\\right)$. The argument $\\frac{\\pi k}{N}$ ranges from $\\frac{\\pi}{N}$ to $\\frac{\\pi(N-1)}{N}$. The function $\\sin^2(x)$ has a maximum value of $1$ at $x=\\pi/2$.\n- If $N$ is an even number, we can write $N=2M$ for some integer $M \\ge 1$. For the mode $k=M$, the argument is $\\frac{\\pi M}{2M} = \\frac{\\pi}{2}$. Thus, $\\sin^2(\\frac{\\pi}{2}) = 1$. In this case, $S_N = 1$. This occurs for all even $N \\ge 2$.\n- If $N$ is an odd number, $k$ can never be exactly $N/2$. The maximum value of $\\sin^2\\left(\\frac{\\pi k}{N}\\right)$ occurs for the integer $k$ closest to $N/2$, which are $k=(N-1)/2$ and $k=(N+1)/2$. For these values, the maximum is $\\cos^2\\left(\\frac{\\pi}{2N}\\right)$, which is strictly less than $1$ but approaches $1$ as $N \\to \\infty$.\n\nThe sequence of values $\\{S_N\\}_{N \\ge 2}$ is $\\{1, 3/4, 1, \\cos^2(\\pi/10), 1, \\dots\\}$. The supremum of this sequence is:\n$$\n\\sup_{N \\ge 2} S_N = 1\n$$\nThis supremum is achieved for any even $N$.\nSubstituting this into our inequality for $C$:\n$$\nC \\le \\frac{1}{2 \\times 1} = \\frac{1}{2}\n$$\nThe problem asks for \"the minimal constant $C$ that satisfies this bound\". This is standard, albeit slightly confusing, terminology in numerical analysis for the sharpest possible constant, i.e., the largest value of $C$ for which the stability condition holds universally. Any $C > 1/2$ would lead to a violation of the stability condition for any even $N$ (e.g., for the mode $k=N/2$). Therefore, the largest possible value for $C$ that guarantees stability for all $N \\ge 2$ is $1/2$. This is the minimal value of the stability coefficients $C_N = 1/(2S_N)$ across all $N \\ge 2$.\n\nThus, the most restrictive stability constraint, valid for any $N \\ge 2$, is $\\Delta t \\le \\frac{1}{2}\\frac{h^2}{\\nu}$. The constant $C$ is $1/2$.",
            "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$"
        },
        {
            "introduction": "Having established the problem of stiffness, we now explore the core principle of exponential integrators. For a linear system of ODEs, such as the semi-discretized heat equation, the solution over a time step $\\Delta t$ can be expressed exactly using the matrix exponential. This hands-on calculation for a small, tractable system allows you to compute this exact action and compare it directly to a naive Taylor series approximation, providing clear intuition for why and how exponential methods achieve superior accuracy for stiff problems. ",
            "id": "3389682",
            "problem": "Consider the spatial semi-discretization of the one-dimensional heat equation $\\partial_{t} u = \\partial_{xx} u$ with homogeneous Dirichlet boundary conditions at $x=0$ and $x=1$, using $n=3$ interior grid points and unit grid spacing. This yields a stiff linear ordinary differential equation (ODE) system $\\dot{y}(t) = L y(t)$ whose operator $L$ is symmetric negative definite and arises as minus the discrete Laplacian. Specifically, let\n$$\nL \\;=\\; -\\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{pmatrix}.\n$$\nExponential integrators advance the solution exactly for linear problems via $y(t+\\Delta t) = \\exp(\\Delta t L)\\, y(t)$. In time-parallel methods such as Parareal, a simple coarse propagator can be formed from a truncated Taylor series of the matrix exponential.\n\nUsing the spectral theorem and orthonormal eigendecomposition of $L$, compute the exact propagated vector $\\exp(\\Delta t L)\\, v$ for the data $v = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$ and $\\Delta t = 0.8$, without invoking any shortcut formulas. Then form the third-order truncated Taylor approximation to the exponential action,\n$$\ny_{\\mathrm{tay}} \\;=\\; \\left(I + \\Delta t L + \\frac{\\Delta t^{2}}{2} L^{2} + \\frac{\\Delta t^{3}}{6} L^{3}\\right) v,\n$$\nand compute the Euclidean norm of the error $\\| \\exp(\\Delta t L)\\, v - y_{\\mathrm{tay}} \\|_{2}$. Round your final numerical value to five significant figures. Express the answer as a pure number (no units).",
            "solution": "The user wants to compute the Euclidean norm of the error between the exact solution of an ODE system and its third-order Taylor approximation.\n\n### Step 1: Extract Givens\n- The ODE system is $\\dot{y}(t) = L y(t)$.\n- The operator $L$ is given by the matrix:\n$$\nL = -\\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} = \\begin{pmatrix} -2 & 1 & 0 \\\\ 1 & -2 & 1 \\\\ 0 & 1 & -2 \\end{pmatrix}\n$$\n- The initial data vector is $v = y(t_0) = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$.\n- The time step is $\\Delta t = 0.8$.\n- The exact propagated vector is $y_{\\mathrm{exact}} = \\exp(\\Delta t L) v$.\n- The third-order Taylor approximation is $y_{\\mathrm{tay}} = \\left(I + \\Delta t L + \\frac{\\Delta t^{2}}{2} L^{2} + \\frac{\\Delta t^{3}}{6} L^{3}\\right) v$.\n- The task is to compute $\\| y_{\\mathrm{exact}} - y_{\\mathrm{tay}} \\|_{2}$ and round the result to five significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined exercise in numerical linear algebra and the numerical solution of ordinary differential equations.\n- **Scientifically Grounded:** The problem uses standard concepts: the matrix exponential for exact solutions of linear ODEs, Taylor series approximations, and the spectral theorem for matrix functions. The matrix $L$ is a valid representation of the 1D discrete Laplacian, a common source of stiff ODEs. The problem is a standard setup in the study of exponential integrators. All premises are factually and mathematically sound.\n- **Well-Posed:** All necessary data ($L$, $v$, $\\Delta t$) are provided. The instructions are clear and lead to a unique numerical answer.\n- **Objective:** The problem is stated in precise mathematical language, free of ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution will proceed.\n\n### Part 1: Computation of the Exact Solution $y_{\\mathrm{exact}}$\n\nThe problem requires using the spectral decomposition of $L$. We have $L = QDQ^T$, where $Q$ is the orthonormal matrix of eigenvectors and $D$ is the diagonal matrix of eigenvalues. Then $\\exp(\\Delta t L) = Q \\exp(\\Delta t D) Q^T$. The exact solution is $y_{\\mathrm{exact}} = Q \\exp(\\Delta t D) Q^T v$.\n\nFirst, we find the eigenvalues and eigenvectors of $L$. The matrix $L$ is a symmetric Toeplitz matrix. The eigenvalues of the related positive definite matrix $-L$ are $\\lambda_k' = 2 - 2\\cos\\left(\\frac{k\\pi}{n+1}\\right)$ for $k=1, 2, ..., n$, where $n=3$.\n- $k=1$: $\\lambda_1' = 2 - 2\\cos(\\pi/4) = 2 - \\sqrt{2}$.\n- $k=2$: $\\lambda_2' = 2 - 2\\cos(\\pi/2) = 2$.\n- $k=3$: $\\lambda_3' = 2 - 2\\cos(3\\pi/4) = 2 + \\sqrt{2}$.\n\nThe eigenvalues of $L$ are $\\lambda_k = -\\lambda_k'$.\n- $\\lambda_1 = -2 + \\sqrt{2}$\n- $\\lambda_2 = -2$\n- $\\lambda_3 = -2 - \\sqrt{2}$\nThe diagonal matrix of eigenvalues is $D = \\mathrm{diag}(-2+\\sqrt{2}, -2, -2-\\sqrt{2})$.\n\nThe corresponding unnormalized eigenvectors have components $v_{k,j} = \\sin\\left(\\frac{jk\\pi}{n+1}\\right)$ for $j=1, 2, 3$. Normalizing these gives the columns of $Q$.\n- For $k=1$: $q_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} \\sin(\\pi/4) \\\\ \\sin(2\\pi/4) \\\\ \\sin(3\\pi/4) \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/\\sqrt{2} \\\\ 1/2 \\end{pmatrix}$.\n- For $k=2$: $q_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} \\sin(2\\pi/4) \\\\ \\sin(4\\pi/4) \\\\ \\sin(6\\pi/4) \\end{pmatrix} = \\begin{pmatrix} 1/\\sqrt{2} \\\\ 0 \\\\ -1/\\sqrt{2} \\end{pmatrix}$.\n- For $k=3$: $q_3 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} \\sin(3\\pi/4) \\\\ \\sin(6\\pi/4) \\\\ \\sin(9\\pi/4) \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ -1/\\sqrt{2} \\\\ 1/2 \\end{pmatrix}$.\n\nThe matrix of eigenvectors is $Q = \\begin{pmatrix} 1/2 & 1/\\sqrt{2} & 1/2 \\\\ 1/\\sqrt{2} & 0 & -1/\\sqrt{2} \\\\ 1/2 & -1/\\sqrt{2} & 1/2 \\end{pmatrix}$. Note that $Q$ is symmetric, so $Q^T=Q$.\n\nNext, we compute the projection of $v$ onto the eigenbasis, $c = Q^T v = Qv$.\n$c = \\begin{pmatrix} 1/2 & 1/\\sqrt{2} & 1/2 \\\\ 1/\\sqrt{2} & 0 & -1/\\sqrt{2} \\\\ 1/2 & -1/\\sqrt{2} & 1/2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 1/2 + 2/\\sqrt{2} + 3/2 \\\\ 1/\\sqrt{2} - 3/\\sqrt{2} \\\\ 1/2 - 2/\\sqrt{2} + 3/2 \\end{pmatrix} = \\begin{pmatrix} 2 + \\sqrt{2} \\\\ -\\sqrt{2} \\\\ 2 - \\sqrt{2} \\end{pmatrix}$.\n\nNow we can write $y_{\\mathrm{exact}} = c_1 \\exp(\\Delta t \\lambda_1) q_1 + c_2 \\exp(\\Delta t \\lambda_2) q_2 + c_3 \\exp(\\Delta t \\lambda_3) q_3$.\nThe exponents are, with $\\Delta t = 0.8$:\n- $\\Delta t \\lambda_1 = 0.8(-2+\\sqrt{2}) = -1.6 + 0.8\\sqrt{2} \\approx -0.468629$\n- $\\Delta t \\lambda_2 = 0.8(-2) = -1.6$\n- $\\Delta t \\lambda_3 = 0.8(-2-\\sqrt{2}) = -1.6 - 0.8\\sqrt{2} \\approx -2.731371$\n\nNumerically evaluating the components of $y_{\\mathrm{exact}}$:\n$y_{\\mathrm{exact}} = (2+\\sqrt{2})\\exp(-1.6+0.8\\sqrt{2})q_1 - \\sqrt{2}\\exp(-1.6)q_2 + (2-\\sqrt{2})\\exp(-1.6-0.8\\sqrt{2})q_3$.\nThis yields the vector:\n$y_{\\mathrm{exact}} \\approx \\begin{pmatrix} 0.8856248 \\\\ 1.4840300 \\\\ 1.2894102 \\end{pmatrix}$.\n\n### Part 2: Computation of the Taylor Approximation $y_{\\mathrm{tay}}$\n\nWe use the formula $y_{\\mathrm{tay}} = v + \\Delta t L v + \\frac{\\Delta t^{2}}{2} L^{2}v + \\frac{\\Delta t^{3}}{6}L^3 v$. We compute the terms $L^k v$ sequentially.\n- $v = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$\n- $Lv = \\begin{pmatrix} -2 & 1 & 0 \\\\ 1 & -2 & 1 \\\\ 0 & 1 & -2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} -2+2 \\\\ 1-4+3 \\\\ 2-6 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ -4 \\end{pmatrix}$\n- $L^2v = L(Lv) = \\begin{pmatrix} -2 & 1 & 0 \\\\ 1 & -2 & 1 \\\\ 0 & 1 & -2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -4 \\\\ 8 \\end{pmatrix}$\n- $L^3v = L(L^2v) = \\begin{pmatrix} -2 & 1 & 0 \\\\ 1 & -2 & 1 \\\\ 0 & 1 & -2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -4 \\\\ 8 \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ 8+8 \\\\ -4-16 \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ 16 \\\\ -20 \\end{pmatrix}$\n\nNow we assemble $y_{\\mathrm{tay}}$ using the coefficients $\\Delta t = 0.8$, $\\frac{\\Delta t^2}{2} = \\frac{0.64}{2} = 0.32$, and $\\frac{\\Delta t^3}{6} = \\frac{0.512}{6} = \\frac{0.256}{3}$.\n$y_{\\mathrm{tay}} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} + 0.8 \\begin{pmatrix} 0 \\\\ 0 \\\\ -4 \\end{pmatrix} + 0.32 \\begin{pmatrix} 0 \\\\ -4 \\\\ 8 \\end{pmatrix} + \\frac{0.256}{3} \\begin{pmatrix} -4 \\\\ 16 \\\\ -20 \\end{pmatrix}$\n$y_{\\mathrm{tay}} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 0 \\\\ -3.2 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ -1.28 \\\\ 2.56 \\end{pmatrix} + \\begin{pmatrix} -1.024/3 \\\\ 4.096/3 \\\\ -5.12/3 \\end{pmatrix}$\n- $y_{\\mathrm{tay},1} = 1 + 0 + 0 - \\frac{1.024}{3} = \\frac{3-1.024}{3} = \\frac{1.976}{3} \\approx 0.658667$\n- $y_{\\mathrm{tay},2} = 2 + 0 - 1.28 + \\frac{4.096}{3} = 0.72 + \\frac{4.096}{3} = \\frac{2.16+4.096}{3} = \\frac{6.256}{3} \\approx 2.085333$\n- $y_{\\mathrm{tay},3} = 3 - 3.2 + 2.56 - \\frac{5.12}{3} = 2.36 - \\frac{5.12}{3} = \\frac{7.08-5.12}{3} = \\frac{1.96}{3} \\approx 0.653333$\n\nSo, $y_{\\mathrm{tay}} \\approx \\begin{pmatrix} 0.6586667 \\\\ 2.0853333 \\\\ 0.6533333 \\end{pmatrix}$.\n\n### Part 3: Computation of the Error Norm\n\nThe error vector is $e = y_{\\mathrm{exact}} - y_{\\mathrm{tay}}$.\n$e_1 \\approx 0.8856248 - 0.6586667 = 0.2269581$\n$e_2 \\approx 1.4840300 - 2.0853333 = -0.6013033$\n$e_3 \\approx 1.2894102 - 0.6533333 = 0.6360769$\n\nThe Euclidean norm of the error is $\\|e\\|_2 = \\sqrt{e_1^2 + e_2^2 + e_3^2}$.\n$\\|e\\|_2^2 \\approx (0.2269581)^2 + (-0.6013033)^2 + (0.6360769)^2$\n$\\|e\\|_2^2 \\approx 0.0515098 + 0.3615658 + 0.4045939 = 0.8176695$\n$\\|e\\|_2 \\approx \\sqrt{0.8176695} \\approx 0.9042508$\n\nRounding to five significant figures gives $0.90425$.",
            "answer": "$$\\boxed{0.90425}$$"
        },
        {
            "introduction": "While computing the full matrix exponential is feasible for small toy problems, it is computationally prohibitive for the large-scale systems encountered in practice. This final practice bridges the gap from principle to a practical, state-of-the-art algorithm. You will design and implement an adaptive Krylov subspace method to efficiently approximate the action of the exponential operator on a vector, $\\varphi_1(\\Delta t L)v$, without ever forming the matrix $\\varphi_1(\\Delta t L)$ itself. This technique is the workhorse behind modern exponential integrators and showcases how theory is translated into powerful, scalable numerical tools. ",
            "id": "3389657",
            "problem": "Consider a semidiscrete stiff Partial Differential Equation (PDE) arising from method-of-lines discretization, where the linear part is represented by a matrix $L \\in \\mathbb{R}^{n \\times n}$. Exponential integrators for stiff problems rely on the family of special functions $\\varphi_k$, and in particular on the function $\\varphi_1$, defined for a matrix argument $A$ by $\\varphi_1(A) = A^{-1}(\\mathrm{e}^A - I)$, where $I$ is the identity matrix and $\\mathrm{e}^A$ denotes the matrix exponential. For a time step $\\Delta t > 0$ and a given vector $v \\in \\mathbb{R}^n$, computing the action $\\varphi_1(\\Delta t \\, L) v$ efficiently and reliably is central to exponential time-stepping schemes.\n\nA classical approach is to reduce the computation of $\\varphi_1(\\Delta t \\, L) v$ to solving an associated inhomogeneous linear Ordinary Differential Equation (ODE). Consider the ODE\n$$\ny'(s) = L\\, y(s) + v,\\quad y(0) = 0,\\quad s \\in [0,\\Delta t].\n$$\nProve that the exact solution satisfies\n$$\ny(\\Delta t) = \\Delta t \\, \\varphi_1(\\Delta t \\, L)\\, v.\n$$\nUsing this equivalence, design an adaptive Krylov subspace algorithm based on the Arnoldi process to approximate $\\varphi_1(\\Delta t \\, L) v$ by monitoring the residual of the inhomogeneous ODE. Specifically:\n\n- Construct an orthonormal basis $V_m = [v_1,\\dots,v_m]$ of the Krylov subspace $\\mathcal{K}_m(L,v) = \\mathrm{span}\\{v, L v, \\dots, L^{m-1} v\\}$ via the Arnoldi process with $v_1 = v/\\|v\\|_2$, and the Hessenberg matrix $H_m \\in \\mathbb{R}^{m \\times m}$ such that $L V_m = V_m H_m + h_{m+1,m} v_{m+1} e_m^\\top$, where $e_m \\in \\mathbb{R}^m$ is the $m$-th coordinate vector and $h_{m+1,m} \\in \\mathbb{R}$ is the Arnoldi subdiagonal coefficient.\n- Define the reduced ODE $z'(s) = H_m z(s) + \\beta\\, e_1$, $z(0)=0$, where $\\beta = \\|v\\|_2$ and $e_1 \\in \\mathbb{R}^m$ is the first coordinate vector, and set the Krylov approximation $y_m(\\Delta t) = V_m\\, z(\\Delta t)$.\n- Derive the residual of the inhomogeneous ODE associated with the Krylov approximation, $r_m(s) = y_m'(s) - L y_m(s) - v$, and show that it can be expressed in terms of $h_{m+1,m}$, $v_{m+1}$, and $z(s)$. Use this to obtain a computable a posteriori residual norm at $s=\\Delta t$, denoted $\\|r_m(\\Delta t)\\|_2$.\n- Propose a stopping criterion tied to a user-specified tolerance $\\mathrm{tol}>0$, formulated as a relative residual condition using $\\|r_m(\\Delta t)\\|_2$ and $\\|y_m(\\Delta t)\\|_2$, and justify its soundness.\n\nYour program must implement this adaptive Krylov algorithm to compute $\\varphi_1(\\Delta t \\, L) v$ and monitor the residual-based stopping criterion. For validation, compare the Krylov result to a reference obtained from a block matrix identity: for any square matrix $A$, the block exponential\n$$\n\\exp\\begin{pmatrix} A & I \\\\ 0 & 0 \\end{pmatrix}\n= \n\\begin{pmatrix} \\mathrm{e}^{A} & \\varphi_1(A) \\\\ 0 & I \\end{pmatrix}\n$$\nimplies that for $A = \\Delta t\\, L$, one can extract $\\varphi_1(A)$ as the top-right block. Use this identity to compute a reference $\\varphi_1(\\Delta t\\, L) v$ and report the relative error of the adaptive Krylov approximation in the Euclidean norm.\n\nDesign a test suite with distinct parameter sets to exercise different regimes:\n\n- Case $1$ (moderate diffusion): $n = 40$, diffusion coefficient $\\alpha = 1$, advection coefficient $\\beta = 0$, uniform grid on $[0,1]$ with Dirichlet boundary conditions, central-difference discretization for diffusion, time step $\\Delta t = 10^{-1}$, tolerance $\\mathrm{tol} = 10^{-8}$, vector $v$ with components $v_i = \\sin(\\pi i h)$ where $h = 1/(n+1)$.\n- Case $2$ (stiff diffusion): $n = 60$, diffusion coefficient $\\alpha = 10^{3}$, advection coefficient $\\beta = 0$, uniform grid and discretization as above, time step $\\Delta t = 10^{-2}$, tolerance $\\mathrm{tol} = 10^{-8}$, vector $v$ with components $v_i = \\sin(2\\pi i h)$.\n- Case $3$ (very small time step): $n = 50$, diffusion coefficient $\\alpha = 10^{3}$, advection coefficient $\\beta = 0$, time step $\\Delta t = 10^{-6}$, tolerance $\\mathrm{tol} = 10^{-12}$, vector $v$ with components $v_i = \\sin(\\pi i h)$.\n- Case $4$ (nonnormal advection–diffusion): $n = 30$, diffusion coefficient $\\alpha = 1$, advection coefficient $\\beta = 20$, uniform grid on $[0,1]$ with Dirichlet boundary conditions, central-difference for diffusion and first-order upwind for advection, time step $\\Delta t = 5 \\times 10^{-2}$, tolerance $\\mathrm{tol} = 10^{-6}$, vector $v$ with components $v_i = \\sin(3\\pi i h)$.\n- Case $5$ (degenerate linear part): $n = 25$, $L = 0_{n \\times n}$, time step $\\Delta t = 10^{-1}$, tolerance $\\mathrm{tol} = 10^{-10}$, and a nonzero vector $v$ with components $v_i = i$.\n\nFor each case, assemble the matrix $L$ according to the specified discretizations:\n- Diffusion: $L_{\\text{diff}} = \\alpha h^{-2} \\cdot \\mathrm{tridiag}(1,-2,1)$ acting on interior points.\n- Advection with positive $\\beta$: $L_{\\text{adv}} = \\beta h^{-1} \\cdot \\mathrm{upwind}$ with $(L_{\\text{adv}})_{i,i} = \\beta h^{-1}$ and $(L_{\\text{adv}})_{i,i-1} = -\\beta h^{-1}$ for $i \\ge 2$, and $(L_{\\text{adv}})_{1,*}$ respecting the Dirichlet boundary at the left endpoint.\n- Total $L$ for advection–diffusion: $L = L_{\\text{diff}} + L_{\\text{adv}}$.\n\nYour program should produce a single line of output containing the relative errors for the $5$ test cases as a comma-separated list enclosed in square brackets (e.g., $\\left[\\mathrm{err}_1,\\mathrm{err}_2,\\mathrm{err}_3,\\mathrm{err}_4,\\mathrm{err}_5\\right]$). All reported errors must be dimensionless real numbers in floating-point format.",
            "solution": "The problem asks for the derivation and implementation of an adaptive Krylov subspace algorithm to compute the action of the matrix function $\\varphi_1(\\Delta t L)$ on a vector $v$, a core task in exponential integrators for stiff PDEs. The validation and solution are presented below.\n\n### Problem Validation\n\nThe problem statement has been meticulously analyzed and is determined to be **valid**. It is scientifically sound, internally consistent, well-posed, and all necessary data and definitions for the theoretical derivations and numerical implementation are provided. The problem constitutes a standard and instructive exercise in the field of numerical analysis for differential equations.\n\n### Part 1: Equivalence between the $\\varphi_1$ Function and an ODE\n\nWe are asked to prove that the solution to the initial value problem\n$$\ny'(s) = L y(s) + v, \\quad y(0) = 0, \\quad s \\in [0, \\Delta t]\n$$\nat the final time $s=\\Delta t$ is given by $y(\\Delta t) = \\Delta t \\, \\varphi_1(\\Delta t L) v$.\n\nThe given ODE is a linear first-order inhomogeneous ordinary differential equation. Its solution can be formally expressed using the variation of constants formula (also known as Duhamel's principle). The solution is\n$$\ny(s) = \\mathrm{e}^{sL} y(0) + \\int_0^s \\mathrm{e}^{(s-\\tau)L} v \\, d\\tau.\n$$\nGiven the initial condition $y(0)=0$, this simplifies to\n$$\ny(s) = \\int_0^s \\mathrm{e}^{(s-\\tau)L} v \\, d\\tau.\n$$\nWe are interested in the solution at $s = \\Delta t$:\n$$\ny(\\Delta t) = \\int_0^{\\Delta t} \\mathrm{e}^{(\\Delta t-\\tau)L} v \\, d\\tau.\n$$\nTo relate this integral to the $\\varphi_1$ function, we perform a change of variables in the integration. Let $\\sigma = (\\Delta t - \\tau)/\\Delta t$. This implies $\\tau = \\Delta t(1-\\sigma)$ and $d\\tau = -\\Delta t \\, d\\sigma$. The integration limits change as follows: when $\\tau=0$, $\\sigma=1$; when $\\tau=\\Delta t$, $\\sigma=0$.\n\nSubstituting these into the integral, we get\n$$\ny(\\Delta t) = \\int_1^0 \\mathrm{e}^{\\sigma \\Delta t L} v \\, (-\\Delta t \\, d\\sigma) = \\Delta t \\int_0^1 \\mathrm{e}^{\\sigma \\Delta t L} v \\, d\\sigma.\n$$\nSince $v$ is a constant vector, we can move it outside the integral:\n$$\ny(\\Delta t) = \\Delta t \\left( \\int_0^1 \\mathrm{e}^{\\sigma (\\Delta t L)} \\, d\\sigma \\right) v.\n$$\nThe integral expression is the definition of the $\\varphi_1$ function applied to the matrix $\\Delta t L$. The $\\varphi_k$ functions are defined by $\\varphi_k(Z) = \\int_0^1 \\frac{(1-s)^{k-1}}{(k-1)!} \\mathrm{e}^{sZ} ds$. For $k=1$, and by a simple change of variable, this is equivalent to $\\varphi_1(Z) = \\int_0^1 \\mathrm{e}^{\\sigma Z} d\\sigma$.\nTherefore,\n$$\ny(\\Delta t) = \\Delta t \\, \\varphi_1(\\Delta t L) v.\n$$\nThis completes the proof.\n\n### Part 2: Adaptive Krylov Subspace Algorithm\n\nThe algorithm approximates the solution $y(\\Delta t)$ by projecting the ODE onto a Krylov subspace $\\mathcal{K}_m(L,v)$.\n\n#### Krylov Approximation and Reduced ODE\nThe Arnoldi process generates an orthonormal basis $V_m = [v_1, \\dots, v_m]$ for $\\mathcal{K}_m(L,v)$ such that $v_1 = v/\\|v\\|_2$. The process yields the relation $L V_m = V_m H_m + h_{m+1,m} v_{m+1} e_m^\\top$, where $H_m = V_m^\\top L V_m$ is an $m \\times m$ upper Hessenberg matrix.\n\nWe seek an approximation $y_m(s) \\in \\mathcal{K}_m(L,v)$ of the form $y_m(s) = V_m z(s)$, where $z(s) \\in \\mathbb{R}^m$ is a vector of coefficients. The initial condition $y(0)=0$ implies $y_m(0)=0$, which requires $z(0)=0$.\n\nSubstituting $y_m(s)$ into the ODE and enforcing a Galerkin condition (the residual is orthogonal to the subspace $\\mathcal{K}_m$), we get:\n$$\nV_m^\\top (y_m'(s) - L y_m(s) - v) = 0.\n$$\n$$\nV_m^\\top (V_m z'(s) - L V_m z(s) - v) = 0.\n$$\nUsing $V_m^\\top V_m = I_m$ and $V_m^\\top L V_m = H_m$:\n$$\nz'(s) - H_m z(s) - V_m^\\top v = 0.\n$$\nSince $v = \\|v\\|_2 v_1 = \\beta v_1$ and $v_1 = V_m e_1$, we have $V_m^\\top v = \\beta V_m^\\top (V_m e_1) = \\beta e_1$. This leads to the reduced $m$-dimensional ODE:\n$$\nz'(s) = H_m z(s) + \\beta e_1, \\quad z(0) = 0.\n$$\n\n#### Residual Derivation\nThe residual of the original high-dimensional ODE for the approximation $y_m(s)$ is defined as $r_m(s) = y_m'(s) - L y_m(s) - v$.\nWe substitute $y_m'(s) = V_m z'(s) = V_m (H_m z(s) + \\beta e_1) = V_m H_m z(s) + v$.\n$$\nr_m(s) = (V_m H_m z(s) + v) - L (V_m z(s)) - v = (V_m H_m - L V_m) z(s).\n$$\nUsing the Arnoldi relation $L V_m = V_m H_m + h_{m+1,m} v_{m+1} e_m^\\top$, we have $V_m H_m - L V_m = -h_{m+1,m} v_{m+1} e_m^\\top$.\nSubstituting this into the residual expression gives:\n$$\nr_m(s) = -h_{m+1,m} v_{m+1} e_m^\\top z(s).\n$$\nThe term $e_m^\\top z(s)$ is the last (m-th) component of the vector $z(s)$, let's denote it by $z_m(s)$.\n$$\nr_m(s) = -h_{m+1,m} z_m(s) v_{m+1}.\n$$\nThis shows that the residual vector is aligned with the next Arnoldi vector $v_{m+1}$.\nThe Euclidean norm of the residual at $s=\\Delta t$ is:\n$$\n\\|r_m(\\Delta t)\\|_2 = \\|-h_{m+1,m} z_m(\\Delta t) v_{m+1}\\|_2 = |h_{m+1,m} z_m(\\Delta t)| \\|v_{m+1}\\|_2.\n$$\nSince $v_{m+1}$ is a unit vector, $\\|v_{m+1}\\|_2=1$. This yields the computable a posteriori residual norm:\n$$\n\\|r_m(\\Delta t)\\|_2 = |h_{m+1,m} z_m(\\Delta t)|.\n$$\n\n#### Stopping Criterion\nThe algorithm's adaptivity comes from increasing the dimension $m$ of the Krylov subspace until the approximation is sufficiently accurate. A robust stopping criterion is essential. We use the derived residual norm to form a relative error estimate. The approximation $y_m(\\Delta t)$ should be accurate if the residual $r_m(\\Delta t)$ is small relative to the magnitude of the solution itself. This suggests the condition:\n$$\n\\|r_m(\\Delta t)\\|_2 \\le \\mathrm{tol} \\cdot \\|y_m(\\Delta t)\\|_2.\n$$\nThis criterion is sound because it measures the extent to which the approximation fails to satisfy the governing equation, normalized by the scale of the approximation. A small relative residual norm is a standard indicator of a small backward error, which, for a well-conditioned problem, implies a small forward error $\\|y(\\Delta t) - y_m(\\Delta t)\\|_2$.\nThe criterion is cheaply computable. We have $\\|r_m(\\Delta t)\\|_2 = |h_{m+1,m} z_m(\\Delta t)|$. For the solution norm, since $V_m$ has orthonormal columns, $\\|y_m(\\Delta t)\\|_2 = \\|V_m z(\\Delta t)\\|_2 = \\|z(\\Delta t)\\|_2$.\nThe final stopping criterion is:\n$$\n|h_{m+1,m} z_m(\\Delta t)| \\le \\mathrm{tol} \\cdot \\|z(\\Delta t)\\|_2.\n$$\nThe algorithm proceeds by increasing $m$, and at each step, it computes $z(\\Delta t)$ for the current $H_m$ and checks this condition. The solution $z(\\Delta t)$ for the small $m \\times m$ system is computed efficiently using a matrix exponential of an augmented $(m+1) \\times (m+1)$ matrix.\nOnce the criterion is met for a dimension $m$, the final approximation for $\\varphi_1(\\Delta t L) v$ is computed as $y_m(\\Delta t) / \\Delta t = (V_m z(\\Delta t)) / \\Delta t$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import expm\n\ndef build_L_matrix(n, alpha, beta):\n    \"\"\"\n    Constructs the an n x n matrix L for the 1D advection-diffusion problem\n    u_t = alpha * u_xx + beta * u_x on [0,1] with Dirichlet boundary conditions.\n    \"\"\"\n    if n == 0:\n        return np.array([[]])\n    if alpha == 0 and beta == 0:\n        return np.zeros((n, n))\n\n    h = 1.0 / (n + 1)\n    L = np.zeros((n, n))\n\n    # Diffusion part: L_diff = alpha/h^2 * tridiag(1, -2, 1)\n    if alpha != 0:\n        L += (alpha / h**2) * (np.diag(np.ones(n - 1), 1) + np.diag(np.ones(n - 1), -1) - 2 * np.eye(n))\n\n    # Advection part: (L_adv)_ii = beta/h, (L_adv)_{i,i-1} = -beta/h\n    # This corresponds to beta * u_x discretized with first-order backward difference.\n    if beta != 0:\n        L += (beta / h) * (np.eye(n) - np.diag(np.ones(n - 1), k=-1))\n        \n    return L\n\ndef adaptive_krylov_phi_v(L, v, dt, tol):\n    \"\"\"\n    Computes phi_1(dt*L)*v using an adaptive Arnoldi-based Krylov method.\n    \"\"\"\n    n = L.shape[0]\n    if dt == 0:\n        # phi_1(0)v = Iv = v\n        return v\n    if n == 0:\n        return np.array([])\n    \n    beta = np.linalg.norm(v)\n    if beta  1e-15:\n        return np.zeros(n)\n\n    # Maximum Krylov subspace dimension\n    max_dim = n\n\n    V = np.zeros((n, max_dim + 1))\n    H = np.zeros((max_dim + 1, max_dim))\n    \n    V[:, 0] = v / beta\n    \n    y_approx = np.zeros(n)\n\n    for k in range(1, max_dim + 1):\n        # Arnoldi iteration to build V_k and H_k\n        w = L @ V[:, k - 1]\n        \n        # Modified Gram-Schmidt to orthogonalize w against V_k\n        for j in range(k):\n            h_j_k_minus_1 = V[:, j].T @ w\n            H[j, k - 1] = h_j_k_minus_1\n            w = w - h_j_k_minus_1 * V[:, j]\n            \n        h_k_plus_1_k = np.linalg.norm(w)\n        H[k, k - 1] = h_k_plus_1_k\n\n        # Get the current small Hessenberg matrix H_k\n        Hk = H[:k, :k]\n        \n        # Solve the reduced ODE for z'(s) = Hk*z(s) + beta*e1\n        # Solution z(dt) is computed using an augmented matrix exponential\n        M_aug = np.zeros((k + 1, k + 1))\n        M_aug[:k, :k] = Hk\n        M_aug[0, k] = beta\n        \n        E = expm(dt * M_aug)\n        z_dt = E[:k, k]\n        \n        # Check stopping criterion: ||r_k(dt)|| = tol * ||y_k(dt)||\n        # ||r_k(dt)|| = |h_{k+1,k} * z_k(dt)|\n        res_norm = np.abs(h_k_plus_1_k * z_dt[-1])\n        \n        # ||y_k(dt)|| = ||V_k * z(dt)|| = ||z(dt)||\n        y_k_dt_norm = np.linalg.norm(z_dt)\n\n        if y_k_dt_norm > 1e-15 and (res_norm = tol * y_k_dt_norm):\n            y_approx = V[:, :k] @ z_dt\n            break\n\n        # Handle breakdown (exact projection)\n        if h_k_plus_1_k  1e-15:\n            y_approx = V[:, :k] @ z_dt\n            break\n\n        V[:, k] = w / h_k_plus_1_k\n\n        # If max dimension is reached, use the last approximation\n        if k == max_dim:\n           y_approx = V[:, :k] @ z_dt\n    \n    # The algorithm computes y_m(dt). We need phi_1(dt*L)*v = y_m(dt)/dt.\n    return y_approx / dt\n\ndef reference_phi_v(L, v, dt):\n    \"\"\"\n    Computes phi_1(dt*L)*v using the block matrix exponential identity.\n    \"\"\"\n    n = L.shape[0]\n    if dt == 0:\n        return v\n    if n == 0:\n        return np.array([])\n        \n    # Use: exp([[A, I], [0, 0]]) = [[exp(A), phi_1(A)], [0, I]] with A = dt*L\n    A = dt * L\n    \n    # Form the 2n x 2n block matrix\n    block_M = np.zeros((2 * n, 2 * n))\n    block_M[:n, :n] = A\n    block_M[:n, n:] = np.eye(n)\n    \n    exp_block_M = expm(block_M)\n    \n    # Extract the phi_1(A) block\n    phi1_A = exp_block_M[:n, n:]\n    \n    return phi1_A @ v\n\ndef solve():\n    test_cases = [\n        # Case 1: moderate diffusion\n        {'n': 40, 'alpha': 1, 'beta': 0, 'dt': 1e-1, 'tol': 1e-8, 'v_func': lambda i, h: np.sin(np.pi * i * h)},\n        # Case 2: stiff diffusion\n        {'n': 60, 'alpha': 1e3, 'beta': 0, 'dt': 1e-2, 'tol': 1e-8, 'v_func': lambda i, h: np.sin(2 * np.pi * i * h)},\n        # Case 3: very small time step\n        {'n': 50, 'alpha': 1e3, 'beta': 0, 'dt': 1e-6, 'tol': 1e-12, 'v_func': lambda i, h: np.sin(np.pi * i * h)},\n        # Case 4: nonnormal advection-diffusion\n        {'n': 30, 'alpha': 1, 'beta': 20, 'dt': 5e-2, 'tol': 1e-6, 'v_func': lambda i, h: np.sin(3 * np.pi * i * h)},\n        # Case 5: degenerate linear part\n        {'n': 25, 'alpha': 0, 'beta': 0, 'dt': 1e-1, 'tol': 1e-10, 'v_func': lambda i, h: i},\n    ]\n\n    results = []\n    for params in test_cases:\n        n = params['n']\n        alpha = params['alpha']\n        beta_val = params['beta']\n        dt = params['dt']\n        tol = params['tol']\n        \n        L = build_L_matrix(n, alpha, beta_val)\n\n        h = 1.0 / (n + 1)\n        v = np.array([params['v_func'](i + 1, h) for i in range(n)])\n\n        # Compute with adaptive Krylov algorithm\n        krylov_result = adaptive_krylov_phi_v(L, v, dt, tol)\n        \n        # Compute reference solution\n        if n == 25 and alpha == 0 and beta_val == 0: # Case 5\n            # For L=0, phi_1(0) = I, so phi_1(0)v = v\n            ref_result = v\n        else:\n            ref_result = reference_phi_v(L, v, dt)\n        \n        # Calculate relative error\n        norm_ref = np.linalg.norm(ref_result)\n        if norm_ref == 0:\n            error = np.linalg.norm(krylov_result)\n        else:\n            error = np.linalg.norm(krylov_result - ref_result) / norm_ref\n        \n        results.append(error)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}