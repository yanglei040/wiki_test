{
    "hands_on_practices": [
        {
            "introduction": "Before automating the continuation process in code, it is essential to grasp the underlying mechanics of the tangent calculation . This exercise provides a concrete opportunity to derive the core components of the linearized system—the Jacobian matrix $J$ and the residual's parameter derivative $R_{\\lambda}$—for a small-scale discretization of the Bratu equation. By solving for the tangent direction manually, you will build a foundational understanding of how the algorithm determines the path along the solution branch.",
            "id": "3373968",
            "problem": "Consider the steady-state one-dimensional Bratu-type Partial Differential Equation (PDE)\n$$\\frac{d^{2}u}{dx^{2}} + \\lambda \\exp(u) = 0,$$\nposed on the interval $x \\in [0,1]$ with homogeneous Dirichlet boundary conditions $u(0) = 0$ and $u(1) = 0$. Using the standard second-order central finite difference scheme on a uniform grid with spacing $h = \\frac{1}{3}$, the interior grid points are at $x_{1} = h$ and $x_{2} = 2h$, with unknowns $u_{1} \\approx u(x_{1})$ and $u_{2} \\approx u(x_{2})$. Denote $u_{0} = 0$ and $u_{3} = 0$ as the boundary values. The residual at interior node $x_{i}$ is defined by\n$$R_{i}(u,\\lambda) = \\frac{u_{i-1} - 2u_{i} + u_{i+1}}{h^{2}} + \\lambda \\exp(u_{i}), \\quad i = 1,2,$$\nso that the discrete nonlinear system is $R(u,\\lambda) = 0$ in $\\mathbb{R}^{2}$. Let $J(u,\\lambda) = \\frac{\\partial R}{\\partial u}$ be the Jacobian matrix with respect to the vector $u = (u_{1},u_{2})^{\\top}$, and let $R_{\\lambda}(u,\\lambda) = \\frac{\\partial R}{\\partial \\lambda}$ be the partial derivative of the residual with respect to the parameter $\\lambda$.\n\nIn a pseudo-arclength continuation framework, the tangent $(u_{s},\\lambda_{s})$ to the solution branch solves the linearized coupling\n$$J(u,\\lambda)\\,u_{s} + R_{\\lambda}(u,\\lambda)\\,\\lambda_{s} = 0,$$\nsupplemented by a normalization constraint\n$$c^{\\top} u_{s} + \\gamma\\,\\lambda_{s} = 1,$$\nfor a given vector $c \\in \\mathbb{R}^{2}$ and scalar $\\gamma \\in \\mathbb{R}$.\n\nStarting from the above fundamental definitions and using $h = \\frac{1}{3}$, derive explicit expressions for $R(u,\\lambda)$, $J(u,\\lambda)$, and $R_{\\lambda}(u,\\lambda)$ for this discretization. Then, at the state\n$$u_{1} = 0.2,\\quad u_{2} = 0.1,\\quad \\lambda = 2,$$\nwith normalization data\n$$c = \\begin{pmatrix}1 \\\\ -1\\end{pmatrix},\\quad \\gamma = 0,$$\nsolve the coupled linear system for the tangent and compute the scalar $\\lambda_{s}$. Round your answer to five significant figures. The final answer must be a single real number without units.",
            "solution": "The user wants to find the tangent component $\\lambda_s$ for a pseudo-arclength continuation problem applied to a discretized Bratu-type PDE.\n\n### Step 1: Problem Validation\n\nThe problem is validated against the required criteria.\n\n**Extracted Givens:**\n- PDE: $\\frac{d^{2}u}{dx^{2}} + \\lambda \\exp(u) = 0$ for $x \\in [0,1]$.\n- Boundary Conditions: $u(0) = 0$, $u(1) = 0$.\n- Discretization Scheme: Standard second-order central finite difference on a uniform grid.\n- Grid Spacing: $h = \\frac{1}{3}$.\n- Interior Grid Points: $x_1 = h$, $x_2 = 2h$.\n- Unknowns: $u_1 \\approx u(x_1)$, $u_2 \\approx u(x_2)$.\n- Boundary Values: $u_0 = 0$, $u_3 = 0$.\n- Residual Definition: $R_{i}(u,\\lambda) = \\frac{u_{i-1} - 2u_{i} + u_{i+1}}{h^{2}} + \\lambda \\exp(u_{i})$ for $i=1,2$.\n- System of Equations: $R(u,\\lambda) = 0$, where $u = (u_1, u_2)^\\top$.\n- Jacobian Matrix: $J(u,\\lambda) = \\frac{\\partial R}{\\partial u}$.\n- Residual Derivative w.r.t. Parameter: $R_{\\lambda}(u,\\lambda) = \\frac{\\partial R}{\\partial \\lambda}$.\n- Linear System for Tangent $(u_s, \\lambda_s)$: $J(u,\\lambda)\\,u_{s} + R_{\\lambda}(u,\\lambda)\\,\\lambda_{s} = 0$.\n- Normalization Constraint: $c^{\\top} u_{s} + \\gamma\\,\\lambda_{s} = 1$.\n- Specific State for Evaluation: $u_1 = 0.2$, $u_2 = 0.1$, $\\lambda = 2$.\n- Normalization Data: $c = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$, $\\gamma = 0$.\n- Required Output: The value of $\\lambda_s$, rounded to five significant figures.\n\n**Validation Verdict:**\nThe problem is scientifically grounded, well-posed, and objective. It provides a complete and consistent set of information rooted in standard numerical analysis techniques (finite differences, pseudo-arclength continuation) for solving nonlinear PDEs. The Bratu equation is a classical example in this field. All terms are clearly defined, and the task leads to a unique, computable result. The problem is valid.\n\n### Step 2: Derivation of System Components\n\nFirst, we derive the explicit forms of the residual vector $R(u, \\lambda)$, the Jacobian matrix $J(u, \\lambda)$, and the parameter derivative vector $R_{\\lambda}(u, \\lambda)$. The grid spacing is $h = \\frac{1}{3}$, so $h^2 = \\frac{1}{9}$.\n\nThe residual equations at the interior nodes $i=1$ and $i=2$ are:\n$$R_{1}(u,\\lambda) = \\frac{u_{0} - 2u_{1} + u_{2}}{h^{2}} + \\lambda \\exp(u_{1})$$\n$$R_{2}(u,\\lambda) = \\frac{u_{1} - 2u_{2} + u_{3}}{h^{2}} + \\lambda \\exp(u_{2})$$\nUsing the boundary conditions $u_0 = 0$ and $u_3 = 0$, the residual vector $R = (R_1, R_2)^\\top$ is:\n$$R(u,\\lambda) = \\begin{pmatrix} \\frac{-2u_{1} + u_{2}}{h^{2}} + \\lambda \\exp(u_{1}) \\\\ \\frac{u_{1} - 2u_{2}}{h^{2}} + \\lambda \\exp(u_{2}) \\end{pmatrix}$$\n\nThe Jacobian matrix $J(u,\\lambda) = \\frac{\\partial R}{\\partial u}$ is a $2 \\times 2$ matrix with elements $J_{ij} = \\frac{\\partial R_i}{\\partial u_j}$:\n$$J_{11} = \\frac{\\partial R_1}{\\partial u_1} = -\\frac{2}{h^2} + \\lambda \\exp(u_1)$$\n$$J_{12} = \\frac{\\partial R_1}{\\partial u_2} = \\frac{1}{h^2}$$\n$$J_{21} = \\frac{\\partial R_2}{\\partial u_1} = \\frac{1}{h^2}$$\n$$J_{22} = \\frac{\\partial R_2}{\\partial u_2} = -\\frac{2}{h^2} + \\lambda \\exp(u_2)$$\nSo, the Jacobian matrix is:\n$$J(u,\\lambda) = \\begin{pmatrix} -2/h^2 + \\lambda \\exp(u_1) & 1/h^2 \\\\ 1/h^2 & -2/h^2 + \\lambda \\exp(u_2) \\end{pmatrix}$$\n\nThe derivative of the residual with respect to $\\lambda$, $R_{\\lambda}(u,\\lambda) = \\frac{\\partial R}{\\partial \\lambda}$, is the vector:\n$$R_{\\lambda}(u,\\lambda) = \\begin{pmatrix} \\frac{\\partial R_1}{\\partial \\lambda} \\\\ \\frac{\\partial R_2}{\\partial \\lambda} \\end{pmatrix} = \\begin{pmatrix} \\exp(u_1) \\\\ \\exp(u_2) \\end{pmatrix}$$\n\n### Step 3: Setting up and Solving the Linear System for the Tangent\n\nThe tangent vector $(u_s, \\lambda_s)$, where $u_s = (u_{s1}, u_{s2})^\\top$, is found by solving the coupled system:\n1. $J(u,\\lambda)\\,u_{s} + R_{\\lambda}(u,\\lambda)\\,\\lambda_{s} = 0$\n2. $c^{\\top} u_{s} + \\gamma\\,\\lambda_{s} = 1$\n\nWe substitute the given normalization data $c = (1, -1)^\\top$ and $\\gamma=0$ into the second equation:\n$$c^{\\top} u_{s} = 1 \\implies \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} u_{s1} \\\\ u_{s2} \\end{pmatrix} = 1 \\implies u_{s1} - u_{s2} = 1$$\n\nFrom the first equation, we can express $u_s$ in terms of $\\lambda_s$:\n$$J u_s = -R_{\\lambda} \\lambda_s \\implies u_s = -\\lambda_s J^{-1} R_{\\lambda}$$\nSubstituting this into the normalization equation gives:\n$$c^{\\top} (-\\lambda_s J^{-1} R_{\\lambda}) = 1$$\n$$- \\lambda_s (c^{\\top} J^{-1} R_{\\lambda}) = 1$$\n$$\\lambda_s = \\frac{-1}{c^{\\top} J^{-1} R_{\\lambda}}$$\n\nLet's compute the term $c^{\\top} J^{-1} R_{\\lambda}$. The inverse of the Jacobian is:\n$$J^{-1} = \\frac{1}{\\det(J)} \\begin{pmatrix} J_{22} & -J_{12} \\\\ -J_{21} & J_{11} \\end{pmatrix}$$\nSo we have:\n$$c^{\\top} J^{-1} R_{\\lambda} = \\frac{1}{\\det(J)} \\begin{pmatrix} 1 & -1 \\end{pmatrix} \\begin{pmatrix} J_{22} & -J_{12} \\\\ -J_{21} & J_{11} \\end{pmatrix} \\begin{pmatrix} \\exp(u_1) \\\\ \\exp(u_2) \\end{pmatrix}$$\n$$= \\frac{1}{\\det(J)} \\begin{pmatrix} J_{22}+J_{21} & -J_{12}-J_{11} \\end{pmatrix} \\begin{pmatrix} \\exp(u_1) \\\\ \\exp(u_2) \\end{pmatrix}$$\n$$= \\frac{1}{\\det(J)} \\left[ (J_{22}+J_{21})\\exp(u_1) - (J_{12}+J_{11})\\exp(u_2) \\right]$$\nSubstituting the expressions for $J_{ij}$:\n$$= \\frac{1}{\\det(J)} \\left[ (-\\frac{2}{h^2} + \\lambda\\exp(u_2) + \\frac{1}{h^2})\\exp(u_1) - (\\frac{1}{h^2} - \\frac{2}{h^2} + \\lambda\\exp(u_1))\\exp(u_2) \\right]$$\n$$= \\frac{1}{\\det(J)} \\left[ (-\\frac{1}{h^2} + \\lambda\\exp(u_2))\\exp(u_1) - (-\\frac{1}{h^2} + \\lambda\\exp(u_1))\\exp(u_2) \\right]$$\n$$= \\frac{1}{\\det(J)} \\left[ -\\frac{1}{h^2}\\exp(u_1) + \\lambda\\exp(u_1)\\exp(u_2) + \\frac{1}{h^2}\\exp(u_2) - \\lambda\\exp(u_1)\\exp(u_2) \\right]$$\n$$= \\frac{1}{\\det(J)} \\left[ \\frac{1}{h^2}(\\exp(u_2) - \\exp(u_1)) \\right]$$\nThus,\n$$\\lambda_s = \\frac{-1}{\\frac{1}{\\det(J)} \\frac{\\exp(u_2) - \\exp(u_1)}{h^2}} = \\frac{-h^2 \\det(J)}{\\exp(u_2) - \\exp(u_1)} = \\frac{h^2 \\det(J)}{\\exp(u_1) - \\exp(u_2)}$$\n\n### Step 4: Numerical Evaluation\n\nNow we evaluate this expression at the state $u_1=0.2$, $u_2=0.1$, $\\lambda=2$, with $h=1/3$ ($1/h^2=9$).\nFirst, we compute the determinant of the Jacobian at this state:\n$$J = \\begin{pmatrix} -18 + 2 \\exp(0.2) & 9 \\\\ 9 & -18 + 2 \\exp(0.1) \\end{pmatrix}$$\n$$\\det(J) = (-18 + 2\\exp(0.2))(-18 + 2\\exp(0.1)) - (9)(9)$$\n$$\\det(J) = 324 - 36\\exp(0.1) - 36\\exp(0.2) + 4\\exp(0.3) - 81$$\n$$\\det(J) = 243 - 36(\\exp(0.1) + \\exp(0.2)) + 4\\exp(0.3)$$\n\nSubstituting $\\det(J)$ back into the expression for $\\lambda_s$:\n$$\\lambda_s = \\frac{h^2 [243 - 36(\\exp(0.1) + \\exp(0.2)) + 4\\exp(0.3)]}{\\exp(u_1) - \\exp(u_2)}$$\nUsing high-precision values:\n$\\exp(0.1) \\approx 1.105170918$\n$\\exp(0.2) \\approx 1.221402758$\n$\\exp(0.3) \\approx 1.349858808$\n\n$\\det(J) = 243 - 36(1.105170918 + 1.221402758) + 4(1.349858808)$\n$\\det(J) = 243 - 36(2.326573676) + 5.399435232$\n$\\det(J) = 243 - 83.756652336 + 5.399435232$\n$\\det(J) \\approx 164.6427829$\n\n$\\exp(u_1) - \\exp(u_2) = \\exp(0.2) - \\exp(0.1) \\approx 1.221402758 - 1.105170918 = 0.11623184$\n\n$h^2 = (1/3)^2 = 1/9$.\n\n$\\lambda_s = \\frac{(1/9) \\times 164.6427829}{0.11623184} = \\frac{18.29364254}{0.11623184} \\approx 157.3891465$\n\nThe problem requires rounding to five significant figures.\n$\\lambda_s \\approx 157.39$",
            "answer": "$$\n\\boxed{157.39}\n$$"
        },
        {
            "introduction": "With the algebraic foundation in place, we now turn to the geometric intuition behind the predictor-corrector method. The success of a continuation algorithm critically depends on the interplay between the step size and the local curvature of the solution manifold. This problem  uses the familiar unit circle as a simple yet powerful model to visualize how a predictor-corrector step operates, and more importantly, to analyze why it can fail when the prediction is too far from the true curve.",
            "id": "3217738",
            "problem": "Consider the solution manifold of the nonlinear system defined by the zero set of the smooth map $F:\\mathbb{R}^2 \\to \\mathbb{R}$, where $F(x,y) = x^2 + y^2 - 1$. The solution set $\\mathcal{M} = \\{(x,y)\\in \\mathbb{R}^2 : F(x,y) = 0\\}$ is the unit circle. A standard predictor-corrector continuation method based on pseudo-arclength uses the following components.\n\n1) Tangent computation at a known point on the curve: Let $(x_0,y_0) = (1,0) \\in \\mathcal{M}$. The tangent direction $t_0 \\in \\mathbb{R}^2$ at $(x_0,y_0)$ is defined by the condition $t_0^\\top \\nabla F(x_0,y_0) = 0$ and $\\|t_0\\|_2 = 1$, where $\\nabla F$ denotes the gradient of $F$.\n\n2) Predictor: For a step size $h > 0$, form the predicted point $z_{\\mathrm{p}} = (x_{\\mathrm{p}},y_{\\mathrm{p}}) = (x_0,y_0) + h\\, t_0$.\n\n3) Newton corrector for the augmented system: Solve for $z = (x,y)$ the $2\\times 2$ system\n$$\nH(z) = \\begin{bmatrix}\nF(x,y)\\\\\nt_0^\\top (z - z_{\\mathrm{p}})\n\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\end{bmatrix},\n$$\nusing Newton’s method initialized at $z^{(0)} = z_{\\mathrm{p}}$.\n\nYou are told that $h = 1.2$. Using only core definitions and well-tested facts about Newton’s method, smooth curves, and local linearization, analyze this continuation step. In particular, determine whether the Newton corrector can converge in this case and explain why or why not, and identify geometric features that control success for smaller $h$.\n\nSelect all statements that are correct.\n\nA) For $h = 1.2$, the augmented system $H(z) = 0$ has no solution, because the affine constraint $t_0^\\top (z - z_{\\mathrm{p}}) = 0$ defines the horizontal line $y = y_{\\mathrm{p}} = h$, which does not intersect the unit circle. Hence the Newton corrector cannot converge.\n\nB) There exists $\\delta > 0$ such that if $0 < h < \\delta$, then Newton’s method applied to $H(z) = 0$ with initial guess $z_{\\mathrm{p}}$ converges quadratically to a nearby intersection point. Geometrically, for small $h$ the hyperplane constraint is transverse to the curve and the normal distance from the predictor to the curve scales like the local curvature times $h^2$, making the predictor lie inside the local basin of attraction.\n\nC) The failure at $h = 1.2$ occurs because $\\nabla F(x_0,y_0)$ is singular, so the continuation would fail for any step size.\n\nD) Keeping $h = 1.2$ but replacing the orthogonal hyperplane constraint $t_0^\\top (z - z_{\\mathrm{p}}) = 0$ by the vertical-line constraint $x = 1$ guarantees convergence of the Newton corrector, because $x = 1$ intersects the unit circle.",
            "solution": "The user has provided a problem statement regarding a predictor-corrector continuation method. I will first validate the problem statement and then proceed to a full solution and evaluation of the given options.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- A smooth map $F:\\mathbb{R}^2 \\to \\mathbb{R}$ is defined by $F(x,y) = x^2 + y^2 - 1$.\n- The solution manifold is $\\mathcal{M} = \\{(x,y)\\in \\mathbb{R}^2 : F(x,y) = 0\\}$, which is the unit circle.\n- A known point on the curve is $(x_0,y_0) = (1,0)$.\n- The tangent direction $t_0 \\in \\mathbb{R}^2$ at $(x_0,y_0)$ is defined by $t_0^\\top \\nabla F(x_0,y_0) = 0$ and $\\|t_0\\|_2 = 1$.\n- The predictor point is $z_{\\mathrm{p}} = (x_{\\mathrm{p}},y_{\\mathrm{p}}) = (x_0,y_0) + h\\, t_0$.\n- The step size is $h = 1.2$.\n- The corrector step solves the augmented system $H(z) = \\begin{bmatrix} F(x,y)\\\\ t_0^\\top (z - z_{\\mathrm{p}}) \\end{bmatrix} = \\begin{bmatrix}0\\\\0\\end{bmatrix}$ using Newton’s method with initial guess $z^{(0)} = z_{\\mathrm{p}}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, being a standard example from the field of numerical analysis and continuation methods. The function, manifold, and method (pseudo-arclength continuation) are all well-defined and standard. The problem is objective, using precise mathematical language. It is self-contained, providing all necessary information to analyze the specified continuation step. The problem is well-posed, as the question about the convergence of the Newton corrector for the given parameters has a definite, verifiable answer. No flaws from the checklist are present.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. I will proceed with the detailed solution.\n\n**Derivation and Analysis**\n\nFirst, we determine the components of the continuation step based on the provided data.\n\n1.  **Gradient Computation**: The function is $F(x,y) = x^2 + y^2 - 1$. Its gradient is $\\nabla F(x,y) = \\begin{bmatrix} \\frac{\\partial F}{\\partial x} \\\\ \\frac{\\partial F}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} 2x \\\\ 2y \\end{bmatrix}$. At the point $(x_0,y_0) = (1,0)$, the gradient is $\\nabla F(1,0) = \\begin{bmatrix} 2(1) \\\\ 2(0) \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}$. This vector is normal to the unit circle at $(1,0)$.\n\n2.  **Tangent Computation**: The tangent vector $t_0 = \\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix}$ must satisfy $t_0^\\top \\nabla F(x_0,y_0) = 0$.\n    $$\n    \\begin{bmatrix} t_x & t_y \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix} = 2t_x = 0\n    $$\n    This implies $t_x = 0$. The second condition is normalization: $\\|t_0\\|_2 = 1$.\n    $$\n    \\sqrt{t_x^2 + t_y^2} = \\sqrt{0^2 + t_y^2} = |t_y| = 1\n    $$\n    This gives $t_y = \\pm 1$. The choice of sign determines the direction of traversal along the curve. Let's choose $t_y = 1$ for a counter-clockwise traversal. So, $t_0 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$. (The opposite choice $t_y=-1$ leads to a symmetric situation and does not alter the conclusions).\n\n3.  **Predictor Point**: With step size $h = 1.2$, the predictor point $z_{\\mathrm{p}}$ is:\n    $$\n    z_{\\mathrm{p}} = (x_{\\mathrm{p}},y_{\\mathrm{p}}) = (x_0,y_0) + h\\, t_0 = (1,0) + 1.2 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = (1, 1.2)\n    $$\n\n4.  **Corrector System**: The Newton corrector aims to find a root $z = (x,y)$ for the system $H(z)=0$:\n    $$\n    H(z) = \\begin{bmatrix} F(x,y) \\\\ t_0^\\top (z - z_{\\mathrm{p}}) \\end{bmatrix} = \\begin{bmatrix} x^2+y^2-1 \\\\ \\begin{bmatrix} 0 & 1 \\end{bmatrix} \\begin{bmatrix} x - 1 \\\\ y - 1.2 \\end{bmatrix} \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n    $$\n    The two equations are:\n    1.  $x^2 + y^2 - 1 = 0$ (The solution must be on the unit circle).\n    2.  $y - 1.2 = 0$, which means $y = 1.2$ (The solution must lie on the hyperplane orthogonal to the tangent $t_0$ and passing through the solution).\n\n5.  **Existence of Solution**: To find a solution to the system, we substitute the second equation into the first:\n    $$\n    x^2 + (1.2)^2 = 1 \\implies x^2 + 1.44 = 1 \\implies x^2 = -0.44\n    $$\n    This equation has no solution for $x$ in the real numbers $\\mathbb{R}$. Therefore, the system $H(z)=0$ has no solution. Since Newton's method is an algorithm for finding roots (solutions) of a system, it cannot converge if no root exists.\n\nWith this analysis complete, we can evaluate each option.\n\n**Option-by-Option Analysis**\n\n**A) For $h = 1.2$, the augmented system $H(z) = 0$ has no solution, because the affine constraint $t_0^\\top (z - z_{\\mathrm{p}}) = 0$ defines the horizontal line $y = y_{\\mathrm{p}} = h$, which does not intersect the unit circle. Hence the Newton corrector cannot converge.**\n\n- The augmented system is $x^2+y^2=1$ and $y=1.2$. As derived above, this system has no real solution.\n- The constraint equation is $t_0^\\top (z-z_{\\mathrm{p}})=0$. We found this simplifies to $y-1.2=0$, or $y=1.2$. This is a horizontal line.\n- The predictor point is $z_{\\mathrm{p}} = (x_{\\mathrm{p}}, y_{\\mathrm{p}}) = (1, 1.2)$, so $y_{\\mathrm{p}} = 1.2$.\n- The step size is $h=1.2$.\n- The tangent vector is $t_0 = (0,1)$. The predictor is $z_p = (x_0,y_0)+h t_0 = (1,0)+h(0,1) = (1,h)$. So $y_p=h$. The constraint $t_0^\\top(z-z_p)=0$ is $(y-y_p)=0$, which is $y=y_p=h=1.2$. This matches the statement.\n- The line $y=1.2$ does not intersect the unit circle $x^2+y^2=1$, since the maximum value of $y$ on the circle is $1$.\n- Since there is no solution, a root-finding method like Newton's method cannot converge to a solution.\n- This statement is a completely accurate description of the situation.\n\nVerdict: **Correct**\n\n**B) There exists $\\delta > 0$ such that if $0 < h < \\delta$, then Newton’s method applied to $H(z) = 0$ with initial guess $z_{\\mathrm{p}}$ converges quadratically to a nearby intersection point. Geometrically, for small $h$ the hyperplane constraint is transverse to the curve and the normal distance from the predictor to the curve scales like the local curvature times $h^2$, making the predictor lie inside the local basin of attraction.**\n\n- This statement describes the successful operation of the continuation method for a sufficiently small step size $h$. Let's analyze it for a small $h > 0$.\n- The system to solve is $x^2+y^2=1$ and $y=h$. For $0 < h < 1$, this system has two solutions, $(\\pm\\sqrt{1-h^2}, h)$. Let's consider the one near the starting point, $z^* = (\\sqrt{1-h^2}, h)$.\n- The Jacobian of the augmented system $H(z) = \\begin{bmatrix} x^2+y^2-1 \\\\ y-h \\end{bmatrix}$ is $J_H(z) = \\begin{bmatrix} 2x & 2y \\\\ 0 & 1 \\end{bmatrix}$.\n- At the solution $z^*$, the Jacobian is $J_H(z^*) = \\begin{bmatrix} 2\\sqrt{1-h^2} & 2h \\\\ 0 & 1 \\end{bmatrix}$. Its determinant is $2\\sqrt{1-h^2}$, which is non-zero for $h<1$. A non-singular Jacobian at the root is a key condition for the quadratic convergence of Newton's method.\n- The initial guess is $z_{\\mathrm{p}} = (1,h)$. The distance to the true solution $z^*$ is $\\|z_{\\mathrm{p}} - z^*\\| = \\|(1-\\sqrt{1-h^2}, 0)\\| = 1 - \\sqrt{1-h^2}$.\n- Using a Taylor expansion for small $h$, $\\sqrt{1-h^2} \\approx 1 - \\frac{1}{2}h^2$. The error is approximately $\\frac{1}{2}h^2$. This is the \"normal distance\" mentioned, and it is indeed proportional to $h^2$ and the curvature (which is $\\kappa=1$ for a unit circle).\n- According to the local convergence theory of Newton's method, if the initial guess is sufficiently close to a root where the Jacobian is non-singular, the method converges quadratically. Since the initial error scales as $h^2$, we can always choose $h$ small enough (i.e., there exists a $\\delta>0$) to place the initial guess $z_{\\mathrm{p}}$ inside the basin of attraction of $z^*$.\n- The geometric description is also correct: for small $h$, the hyperplane $y=h$ intersects the circle transversally (not tangentially) near $(1,0)$.\n- This statement accurately summarizes the theoretical foundation of why predictor-corrector methods work.\n\nVerdict: **Correct**\n\n**C) The failure at $h = 1.2$ occurs because $\\nabla F(x_0,y_0)$ is singular, so the continuation would fail for any step size.**\n\n- As computed earlier, $\\nabla F(x_0,y_0) = \\nabla F(1,0) = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}$. This is a non-zero vector. The concept of \"singular\" does not apply to a vector. If it's interpreted as the rank of the Jacobian matrix of $F$ (a $1 \\times 2$ matrix), then its rank is $1$, which is the maximum possible. The point $(1,0)$ is a regular point of the manifold.\n- The failure at $h=1.2$ is due to the non-existence of a solution for the corrector system, as explained in A. It is not due to any property of $\\nabla F$ at the starting point.\n- The claim that \"the continuation would fail for any step size\" is directly contradicted by the analysis in B, which shows that it succeeds for small enough step sizes.\n\nVerdict: **Incorrect**\n\n**D) Keeping $h = 1.2$ but replacing the orthogonal hyperplane constraint $t_0^\\top (z - z_{\\mathrm{p}}) = 0$ by the vertical-line constraint $x = 1$ guarantees convergence of the Newton corrector, because $x = 1$ intersects the unit circle.**\n\n- Let's analyze this proposed modification. The new system to solve is:\n    1.  $x^2 + y^2 - 1 = 0$\n    2.  $x - 1 = 0$\n- Substituting (2) into (1) gives $1^2 + y^2 - 1 = 0 \\implies y^2=0 \\implies y=0$. The unique solution is $z^* = (1,0)$.\n- The problem is about the convergence of Newton's method. The initial guess would still be based on the original predictor step, $z^{(0)} = z_{\\mathrm{p}} = (1, 1.2)$.\n- The new augmented function is $H_{new}(z) = \\begin{bmatrix} x^2+y^2-1 \\\\ x-1 \\end{bmatrix}$.\n- The Jacobian is $J_{H_{new}}(z) = \\begin{bmatrix} 2x & 2y \\\\ 1 & 0 \\end{bmatrix}$.\n- Let's evaluate the Jacobian at the solution $z^* = (1,0)$: $J_{H_{new}}(1,0) = \\begin{bmatrix} 2(1) & 2(0) \\\\ 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 2 & 0 \\\\ 1 & 0 \\end{bmatrix}$.\n- The determinant of this Jacobian is $2 \\cdot 0 - 0 \\cdot 1 = 0$. The Jacobian is singular at the root.\n- A singular Jacobian at the root implies that Newton's method, if it converges, will typically do so only linearly, not quadratically. More importantly, robust, guaranteed convergence is lost. Geometrically, the constraint line $x=1$ is tangent to the solution curve (the circle) at the point of intersection $(1,0)$. This non-transversal intersection is precisely what sophisticated continuation methods (like pseudo-arclength) are designed to avoid.\n- The reasoning \"because $x=1$ intersects the unit circle\" is insufficient. The mere existence of a solution does not guarantee that Newton's method will find it from a given starting point, nor does it guarantee good convergence properties.\n- Thus, the claim of \"guaranteed convergence\" is far too strong and misleading. This setup represents a pathological case for a corrector step.\n\nVerdict: **Incorrect**",
            "answer": "$$\\boxed{AB}$$"
        },
        {
            "introduction": "Having explored both the mechanical calculations and the geometric principles, you are now ready to implement a complete pseudo-arclength continuation algorithm. This final practice  challenges you to write a program that traces the entire solution branch of the discretized Bratu equation, successfully navigating its critical turning point. By implementing the full predictor-corrector loop and tracking the solution's stability, you will see how these techniques come together to form a powerful tool for exploring the full range of behaviors in complex nonlinear systems.",
            "id": "3373917",
            "problem": "Consider the steady-state boundary value problem arising from the one-dimensional Bratu model of a reaction-diffusion Partial Differential Equation (PDE) with Dirichlet boundary conditions on the unit interval. Starting from the time-dependent reaction-diffusion PDE $u_t = u_{xx} + \\lambda e^{u}$ on $x \\in (0,1)$ with $u(0,t) = 0$ and $u(1,t) = 0$, steady states satisfy the boundary value problem\n$$\nu_{xx} + \\lambda e^{u} = 0,\\quad x \\in (0,1),\\quad u(0) = 0,\\quad u(1) = 0,\n$$\nwhere $\\lambda$ is a real parameter. It is known from classical analysis that there is a fold (saddle-node) in the solution set, and solutions exist only for $\\lambda$ in an interval $\\lambda \\in (0,\\lambda_{\\mathrm{cr}})$ with $\\lambda_{\\mathrm{cr}} \\in \\mathbb{R}^{+}$. Your task is to design and implement a numerical continuation algorithm with pseudo-arclength to track solution branches of the discretized steady PDE and to assess linear stability along the branch via eigenvalues of the linearized operator.\n\nYou must proceed from the following foundational bases without using any shortcut formulas:\n\n- Finite difference discretization of the second derivative with a three-point stencil.\n- Newton’s method (Newton–Kantorovich iteration) for solving nonlinear algebraic systems.\n- Pseudo-arclength continuation that augments the steady-state residual with an arclength condition to cross folds.\n- Spectral stability determined by the sign of the rightmost (largest real part) eigenvalue of the Jacobian of the steady-state residual, which coincides with the linearization of the time-dependent PDE about the steady state in this setting.\n\nDiscretize the interval $[0,1]$ into $N$ interior grid points with uniform spacing $h = \\frac{1}{N+1}$ and enforce the Dirichlet boundary conditions $u(0)=0$ and $u(1)=0$. Let $u \\in \\mathbb{R}^{N}$ denote the vector of interior unknowns. The finite difference discretization of the steady-state residual is\n$$\nR(u,\\lambda) \\equiv \\frac{1}{h^2}Tu + \\lambda \\exp(u) = 0,\n$$\nwhere $T \\in \\mathbb{R}^{N\\times N}$ is the standard tridiagonal matrix with $-2$ on the main diagonal and $1$ on the sub- and super-diagonals, and $\\exp(u)$ applies the exponential function componentwise. The Jacobian with respect to $u$ equals\n$$\nJ(u,\\lambda) = \\frac{1}{h^2}T + \\lambda \\operatorname{diag}(\\exp(u)).\n$$\nThe derivative of $R$ with respect to $\\lambda$ equals $R_{\\lambda}(u,\\lambda) = \\exp(u) \\in \\mathbb{R}^{N}$.\n\nImplement a pseudo-arclength continuation scheme based on the following principles:\n\n- Start from a converged steady state computed by Newton’s method at an initial parameter value $\\lambda_{0} \\in \\mathbb{R}^{+}$ with initial guess $u^{(0)} = 0$.\n- At a generic continuation point $(u,\\lambda)$, compute a tangent direction $(\\tau_{u},\\tau_{\\lambda}) \\in \\mathbb{R}^{N}\\times\\mathbb{R}$ as the solution of the bordered linear system\n$$\n\\begin{bmatrix}\nJ(u,\\lambda) & R_{\\lambda}(u,\\lambda) \\\\\nR_{\\lambda}(u,\\lambda)^{\\top} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\tau_{u}\\\\\n\\tau_{\\lambda}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0\\\\\n1\n\\end{bmatrix},\n$$\nthen normalize to unit arclength by dividing by $\\sqrt{\\|\\tau_{u}\\|_{2}^{2} + \\tau_{\\lambda}^{2}}$.\n- Form a predictor $(u^{\\mathrm{pred}},\\lambda^{\\mathrm{pred}}) = (u,\\lambda) + \\Delta s\\,(\\tau_{u},\\tau_{\\lambda})$ for a given arclength step size $\\Delta s > 0$.\n- Apply Newton’s method to the augmented residual\n$$\n\\mathcal{F}(u,\\lambda) \\equiv \\begin{bmatrix}\nR(u,\\lambda) \\\\\nc(u,\\lambda)\n\\end{bmatrix} = 0,\n\\quad\nc(u,\\lambda) \\equiv \\tau_{u}^{\\top}(u - u^{\\mathrm{ref}}) + \\tau_{\\lambda}(\\lambda - \\lambda^{\\mathrm{ref}}) - \\Delta s,\n$$\nwith $(u^{\\mathrm{ref}},\\lambda^{\\mathrm{ref}})$ taken as the last accepted continuation point (the base point for the arclength constraint). In each Newton iteration for the corrector, solve the bordered linear system\n$$\n\\begin{bmatrix}\nJ(u,\\lambda) & R_{\\lambda}(u,\\lambda) \\\\\n\\tau_{u}^{\\top} & \\tau_{\\lambda}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\delta u\\\\\n\\delta\\lambda\n\\end{bmatrix}\n= - \\begin{bmatrix}\nR(u,\\lambda)\\\\\nc(u,\\lambda)\n\\end{bmatrix},\n$$\nand update $(u,\\lambda) \\leftarrow (u,\\lambda) + (\\delta u,\\delta\\lambda)$ until convergence.\n\nAt each converged continuation point, assess linear stability of the steady state for the time-dependent PDE $u_t = u_{xx} + \\lambda e^{u}$ by computing the rightmost eigenvalue of the symmetric matrix $J(u,\\lambda)$; denote it by $\\alpha_{\\max}(u,\\lambda) \\in \\mathbb{R}$. A steady state is linearly asymptotically stable if and only if $\\alpha_{\\max}(u,\\lambda) < 0$.\n\nDefine the estimated critical parameter $\\widehat{\\lambda}_{\\mathrm{cr}}$ for a given run as the maximum value of $\\lambda$ attained along the computed continuation path, i.e.,\n$$\n\\widehat{\\lambda}_{\\mathrm{cr}} \\equiv \\max\\{\\lambda^{(k)}: k=0,1,2,\\dots,K\\},\n$$\nwhere $(u^{(k)},\\lambda^{(k)})$ are the converged continuation points produced by your algorithm. This quantity approximates the turning point of the solution branch.\n\nYour program must implement the above method and run it for each parameter triplet $(N,\\Delta s,K)$ in the following test suite:\n\n- Test $1$: $(N,\\Delta s,K) = (60,\\,0.12,\\,60)$.\n- Test $2$: $(N,\\Delta s,K) = (40,\\,0.10,\\,60)$.\n- Test $3$: $(N,\\Delta s,K) = (80,\\,0.15,\\,45)$.\n\nFor each test, use the initial parameter $\\lambda_{0} = 1.0$ (unitless) and the initial Newton guess $u^{(0)} = 0 \\in \\mathbb{R}^{N}$. Use absolute tolerances for Newton solves no larger than $10^{-8}$ on the residual norms and arclength constraint; when needed, you may employ simple backtracking or reduce $\\Delta s$ locally to achieve convergence, but the nominal $\\Delta s$ used to compute the arclength constraint must match the test input.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each entry must be the floating-point value of $\\widehat{\\lambda}_{\\mathrm{cr}}$ for the corresponding test in the same order as listed above. There are no physical units in this problem. Angles do not appear. Percentages do not appear. The final output format must be exactly of the form\n$[v_{1},v_{2},v_{3}]$,\nwhere each $v_{i}$ is a real number printed in standard decimal notation.",
            "solution": "The user wants me to solve the problem statement.\n\n**1. Problem Validation**\n\nThe problem asks for the implementation of a pseudo-arclength continuation algorithm to trace the solution curve of the discretized one-dimensional Bratu equation, a standard nonlinear boundary value problem. The goal is to estimate the critical parameter $\\lambda_{\\mathrm{cr}}$ at which a turning point (fold bifurcation) occurs.\n\n- **Givens**: The problem provides the steady-state PDE, $u_{xx} + \\lambda e^{u} = 0$ with $u(0)=u(1)=0$. It specifies the finite difference discretization, leading to a system of nonlinear algebraic equations $R(u, \\lambda) = 0$. The exact forms of the residual vector $R$, its Jacobian $J$ with respect to the state vector $u$, and its derivative $R_\\lambda$ with respect to the parameter $\\lambda$ are all given. The complete algorithm for pseudo-arclength continuation is detailed, including the predictor step, the augmented residual for the corrector step, and the specific linear systems to be solved for the tangent vector and the Newton update. It also provides the initial conditions ($\\lambda_0=1.0$, $u^{(0)}=0$), convergence tolerances ($\\le 10^{-8}$), and three specific sets of parameters $(N, \\Delta s, K)$ for which the simulation must be run. The output is defined as the maximum $\\lambda$ value found for each parameter set.\n\n- **Validation**:\n    1.  **Scientifically Grounded**: The Bratu equation is a canonical problem in bifurcation theory and reaction-diffusion modeling. The methods specified—finite differences, Newton's method, and pseudo-arclength continuation—are standard and fundamental techniques in numerical analysis for solving such problems. The problem is scientifically and mathematically sound.\n    2.  **Well-Posed**: The computational task is clearly defined. Given the inputs, the algorithm is deterministic and is expected to produce a unique numerical result. The pseudo-arclength method is specifically designed to overcome the singularity at the fold, making the numerical problem well-posed.\n    3.  **Objective**: All terms are mathematically precise. The instructions are algorithmic and free of ambiguity or subjective language.\n    4.  **Completeness**: The problem statement is self-contained, providing all necessary formulas, algorithmic steps, parameters, and initial conditions.\n    5.  **Relevance**: The problem is directly on the topic of *continuation and pseudo-arclength techniques* for the *numerical solution of PDEs*.\n\n- **Verdict**: The problem is valid. I will proceed with the solution.\n\n**2. Solution Design**\n\nThe core of the solution is a direct implementation of the specified pseudo-arclength continuation method. The process can be broken down into the following steps, which will be encapsulated in a function to be run for each test case.\n\n**Step 1: Initial Point Calculation**\nThe continuation starts from a known solution. The problem specifies starting at $\\lambda_0 = 1.0$. However, the trivial solution $u=0$ is only a solution for $\\lambda=0$. Therefore, we must first find the non-trivial solution corresponding to $\\lambda_0 = 1.0$. This is achieved by solving the nonlinear system $R(u, \\lambda_0) = 0$ using a standard Newton's method, starting with the initial guess $u=0$.\nThe Newton iteration is:\n$u_{k+1} = u_k - J(u_k, \\lambda_0)^{-1} R(u_k, \\lambda_0)$, where the update is found by solving the linear system $J(u_k, \\lambda_0) \\delta u = -R(u_k, \\lambda_0)$. This iteration continues until $\\|R\\| < 10^{-8}$.\n\n**Step 2: Continuation Loop**\nStarting from the initial converged point $(u^{(0)}, \\lambda^{(0)})$, we generate a sequence of $K$ points $(u^{(k)}, \\lambda^{(k)})$ along the solution branch. Each step of the loop consists of a predictor and a corrector phase. Let the current accepted point be $(u, \\lambda)$.\n\n**Step 2a: Tangent Vector Calculation**\nTo determine the direction of the next step along the solution curve, we compute a tangent vector $(\\tau_u, \\tau_\\lambda)$. This is done by solving the bordered linear system as prescribed:\n$$\n\\begin{bmatrix}\nJ(u,\\lambda) & R_{\\lambda}(u,\\lambda) \\\\\nR_{\\lambda}(u,\\lambda)^{\\top} & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\tau_{u}\\\\\n\\tau_{\\lambda}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0\\\\\n1\n\\end{bmatrix}\n$$\nThis system is well-posed even at the turning point. After solving, the tangent vector is normalized to unit length: $(\\tau_u, \\tau_\\lambda) \\leftarrow (\\tau_u, \\tau_\\lambda) / \\sqrt{\\|\\tau_u\\|_2^2 + \\tau_\\lambda^2}$. To ensure a consistent traversal direction along the curve, the orientation of the tangent is managed. For the first step, we enforce $\\tau_\\lambda > 0$ to move towards increasing $\\lambda$. For subsequent steps, we ensure the dot product of the current tangent with the previous tangent is positive.\n\n**Step 2b: Predictor**\nA new point is predicted by taking a step of size $\\Delta s$ along the tangent direction from the current point $(u, \\lambda)$:\n$$\n(u^{\\mathrm{pred}}, \\lambda^{\\mathrm{pred}}) = (u, \\lambda) + \\Delta s (\\tau_u, \\tau_\\lambda)\n$$\nThis predicted point serves as the initial guess for the corrector step.\n\n**Step 2c: Corrector**\nThe predicted point is an approximation and does not lie exactly on the solution curve. Newton's method is used to correct it. The corrector solves the augmented system of $N+1$ equations for $N+1$ unknowns $(u, \\lambda)$:\n$$\n\\mathcal{F}(u,\\lambda) \\equiv \\begin{bmatrix}\nR(u,\\lambda) \\\\\nc(u,\\lambda)\n\\end{bmatrix} = 0\n$$\nwhere $c(u, \\lambda) = \\tau_{u}^{\\top}(u - u^{\\mathrm{ref}}) + \\tau_{\\lambda}(\\lambda - \\lambda^{\\mathrm{ref}}) - \\Delta s = 0$ is the arclength constraint. This constraint forces the corrected point to lie on a hyperplane orthogonal to the tangent vector, at a distance $\\Delta s$ from the reference point $(u^{\\mathrm{ref}}, \\lambda^{\\mathrm{ref}})$. The Newton update $(\\delta u, \\delta \\lambda)$ at each corrector iteration is found by solving the linear system:\n$$\n\\mathcal{J}_{\\mathcal{F}}(u,\\lambda)\n\\begin{bmatrix}\n\\delta u\\\\\n\\delta\\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nJ(u,\\lambda) & R_{\\lambda}(u,\\lambda) \\\\\n\\tau_{u}^{\\top} & \\tau_{\\lambda}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\delta u\\\\\n\\delta\\lambda\n\\end{bmatrix}\n= - \\begin{bmatrix}\nR(u,\\lambda)\\\\\nc(u,\\lambda)\n\\end{bmatrix}\n$$\nThe iteration $(u, \\lambda) \\leftarrow (u, \\lambda) + (\\delta u, \\delta\\lambda)$ continues until $\\|R\\| < 10^{-8}$ and $|c| < 10^{-8}$.\n\n**Step 3: Calculating the Output**\nAfter completing $K$ continuation steps, we will have a sequence of converged parameter values $\\lambda^{(0)}, \\lambda^{(1)}, \\dots, \\lambda^{(K)}$. The estimated critical parameter, $\\widehat{\\lambda}_{\\mathrm{cr}}$, is the maximum value in this sequence. This procedure is repeated for each of the three test cases. The final output is a list of these maximum values. The stability assessment via eigenvalue calculation is described for context but is not necessary for producing the final requested output, so it is omitted from the implementation to maintain focus.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bratu problem using pseudo-arclength continuation for the given test cases.\n    \"\"\"\n\n    # --- Configuration and Helper Function Definition ---\n\n    NEWTON_TOL = 1e-8\n    MAX_NEWTON_ITER = 30\n    LAMBDA0 = 1.0\n\n    def solve_continuation_case(N, ds, K):\n        \"\"\"\n        Performs a full continuation run for a given parameter set (N, ds, K).\n\n        Args:\n            N (int): Number of interior grid points.\n            ds (float): Arclength step size.\n            K (int): Number of continuation steps.\n\n        Returns:\n            float: The estimated critical lambda value (max lambda found).\n        \"\"\"\n        # --- Discretization Setup ---\n        h = 1.0 / (N + 1)\n        T = np.diag(np.ones(N - 1), 1) + np.diag(np.ones(N - 1), -1) - 2 * np.diag(np.ones(N))\n        T_h2 = T / h**2\n\n        # --- Step 1: Find the initial solution at LAMBDA0 using Newton's method ---\n        u = np.zeros(N)\n        lam = LAMBDA0\n        \n        for _ in range(MAX_NEWTON_ITER):\n            R = T_h2 @ u + lam * np.exp(u)\n            if np.linalg.norm(R)  NEWTON_TOL:\n                break\n            # Jacobian of R w.r.t. u\n            J = T_h2 + lam * np.diag(np.exp(u))\n            delta_u = np.linalg.solve(J, -R)\n            u += delta_u\n        else:\n            raise RuntimeError(f\"Initial Newton solve failed to converge for (N={N}, ds={ds}, K={K}).\")\n\n        u_current = u\n        lam_current = lam\n        all_lams = [lam_current]\n        \n        # --- Variables for tangent direction control ---\n        tau_u_prev = np.zeros_like(u)\n        tau_lam_prev = 0.0\n\n        # --- Step 2: Main Continuation Loop ---\n        for k in range(K):\n            u_ref, lam_ref = u_current, lam_current\n            \n            # --- a) Tangent Computation ---\n            J = T_h2 + lam_ref * np.diag(np.exp(u_ref))\n            R_lam = np.exp(u_ref)\n\n            # Assemble and solve the bordered system for the tangent\n            A_tan = np.zeros((N + 1, N + 1))\n            A_tan[:N, :N] = J\n            A_tan[:N, N] = R_lam\n            A_tan[N, :N] = R_lam\n            \n            b_tan = np.zeros(N + 1)\n            b_tan[N] = 1.0\n            \n            sol = np.linalg.solve(A_tan, b_tan)\n            tau_u, tau_lam = sol[:-1], sol[-1]\n\n            # --- Tangent Direction Control ---\n            if k == 0:\n                # On the first step, ensure lambda increases\n                if tau_lam  0:\n                    tau_u, tau_lam = -tau_u, -tau_lam\n            else:\n                # For subsequent steps, maintain direction relative to the previous step\n                if np.dot(tau_u, tau_u_prev) + tau_lam * tau_lam_prev  0:\n                    tau_u, tau_lam = -tau_u, -tau_lam\n            \n            # Normalize tangent and store for the next step's orientation control\n            norm_tau = np.sqrt(np.dot(tau_u, tau_u) + tau_lam**2)\n            tau_u /= norm_tau\n            tau_lam /= norm_tau\n            tau_u_prev, tau_lam_prev = tau_u, tau_lam\n\n            # --- b) Predictor Step ---\n            u_new = u_ref + ds * tau_u\n            lam_new = lam_ref + ds * tau_lam\n            \n            # --- c) Corrector Step (Augmented Newton's Method) ---\n            for _ in range(MAX_NEWTON_ITER):\n                # Calculate augmented residual\n                R = T_h2 @ u_new + lam_new * np.exp(u_new)\n                c = np.dot(tau_u, u_new - u_ref) + tau_lam * (lam_new - lam_ref) - ds\n                \n                # Check for convergence\n                if np.linalg.norm(R)  NEWTON_TOL and abs(c)  NEWTON_TOL:\n                    break\n                \n                # Assemble augmented Jacobian and RHS for the correction\n                J_new = T_h2 + lam_new * np.diag(np.exp(u_new))\n                R_lam_new = np.exp(u_new)\n                \n                A_corr = np.zeros((N + 1, N + 1))\n                A_corr[:N, :N] = J_new\n                A_corr[:N, N] = R_lam_new\n                A_corr[N, :N] = tau_u\n                A_corr[N, N] = tau_lam\n                \n                b_corr = -np.concatenate((R, [c]))\n                \n                # Solve for the update and apply it\n                try:\n                    delta = np.linalg.solve(A_corr, b_corr)\n                except np.linalg.LinAlgError:\n                    raise RuntimeError(f\"Corrector system is singular at step {k+1} for (N={N}).\")\n                \n                u_new += delta[:-1]\n                lam_new += delta[-1]\n            else:\n                raise RuntimeError(f\"Corrector Newton failed to converge at step {k+1} for (N={N}).\")\n            \n            # Update current point and record lambda\n            u_current, lam_current = u_new, lam_new\n            all_lams.append(lam_current)\n            \n        return max(all_lams)\n\n    # --- Main Execution ---\n    test_cases = [\n        (60, 0.12, 60),  # Test 1\n        (40, 0.10, 60),  # Test 2\n        (80, 0.15, 45),  # Test 3\n    ]\n\n    results = []\n    for case in test_cases:\n        N, ds, K = case\n        lambda_cr_est = solve_continuation_case(N, ds, K)\n        results.append(lambda_cr_est)\n    \n    # Print the final result in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}