{
    "hands_on_practices": [
        {
            "introduction": "In large-scale simulations, forming and storing the full Jacobian matrix is often prohibitively expensive. This practice introduces the essential technique of matrix-free methods, where we compute the action of the Jacobian on a vector, $Jv$, without assembling the matrix $J$. You will implement and compare two fundamental approaches for a nonlinear PDE: a precise analytic Jacobian-vector product derived from the Gâteaux derivative, and a more general but approximate finite-difference method. This hands-on comparison illuminates the critical trade-off between computational cost, implementation complexity, and numerical accuracy that underpins modern iterative solvers .",
            "id": "3412631",
            "problem": "Consider the steady one-dimensional nonlinear Partial Differential Equation (PDE) on the interval $[0,1]$ with homogeneous Dirichlet boundary conditions,\n$$\n-\\frac{d}{dx}\\Big( \\big(1 + a\\,u(x)^2\\big)\\,\\frac{du}{dx}(x) \\Big) + b\\,u(x)^3 = 0,\\quad x\\in(0,1),\\qquad u(0)=0,\\;u(1)=0,\n$$\nwhere $a$ and $b$ are real parameters. Let $V_h$ be the space of continuous, piecewise-linear functions on a uniform mesh of $N$ nodes with spacing $h = 1/(N-1)$ and nodal basis functions. Using the standard Galerkin weak form and a one-point midpoint quadrature on each element, define the discrete nonlinear residual operator $F(u_h)\\in\\mathbb{R}^{N-2}$ restricted to interior nodes, so that for any interior test function $v_h\\in V_h$ the element contributions are computed using the element midpoint values of $u_h$ and its gradient.\n\nYour tasks are to derive and implement a matrix-free application of the Jacobian $J(u_h)$ to a vector $v_h$ in two ways, using only local element kernels and without assembling any global sparse matrix:\n\n1. A low-order analytic linearization that computes the Gâteaux derivative of the midpoint-quadrature residual in the direction $v_h$, i.e., the action $J(u_h)v_h := \\left.\\frac{d}{d\\epsilon}F(u_h + \\epsilon v_h)\\right|_{\\epsilon=0}$ computed by summing local element contributions at the midpoint.\n\n2. A finite-difference Jacobian-vector product approximation using a single forward difference,\n$$\nJ(u_h)\\,v_h \\approx \\frac{F(u_h + \\epsilon\\,v_h) - F(u_h)}{\\epsilon},\n$$\nwith a scalar step size $\\epsilon$ chosen as\n$$\n\\epsilon = \\tau\\,\\frac{1 + \\|u_h\\|_2}{\\|v_h\\|_2},\n$$\nwhere $\\tau$ is a positive scaling parameter.\n\nFundamental base and modeling assumptions you must use:\n- Use the standard continuous Galerkin formulation in one dimension with piecewise-linear basis functions and element-wise midpoint quadrature. On a mesh element of length $h$, the gradients of the local basis functions are constant and equal to $\\pm 1/h$. The local midpoint value $u_m$ is the average of the nodal values on the element, and the local midpoint gradient is the difference of nodal values divided by $h$.\n- The discrete residual $F(u_h)$ is obtained by summing element contributions of the diffusion and reaction terms into the nodal residuals associated with interior nodes. Dirichlet boundary values are imposed strongly at $x=0$ and $x=1$, and unknowns are only the interior nodal values.\n- The analytic Jacobian-vector product $J(u_h)v_h$ is defined as the Gâteaux derivative of the discrete residual operator with respect to $u_h$ in direction $v_h$, computed consistently with the same midpoint quadrature. This must be implemented by local element kernels using only nodal values of $u_h$ and $v_h$ on each element.\n- The finite-difference approximation uses exactly one additional evaluation of $F$ at $u_h + \\epsilon v_h$ with step size $\\epsilon$ as specified.\n\nAccuracy metric to compute for each test case:\n- The relative error\n$$\n\\mathrm{err} = \\frac{\\|J_{\\mathrm{analytic}}(u_h)v_h - J_{\\mathrm{FD}}(u_h)v_h\\|_2}{\\max\\left(\\|J_{\\mathrm{analytic}}(u_h)v_h\\|_2,\\,10^{-16}\\right)}.\n$$\n\nCost model to compare methods:\n- Count only basic floating-point additions and multiplications. For a single element, assume the following per-element operation counts based on a straightforward midpoint implementation:\n  - For one evaluation of the residual $F(u_h)$: $C_F = 16$ basic operations, interpreted as $7$ additions and $9$ multiplications.\n  - For one evaluation of the analytic Jacobian-vector product $J(u_h)v_h$: $C_J = 28$ basic operations, interpreted as $11$ additions and $17$ multiplications.\n- For a mesh with $N$ nodes and $N_e = N-1$ elements, the total operation count for the analytic Jacobian-vector product is $N_e\\,C_J$.\n- For the finite-difference Jacobian-vector product using one extra residual evaluation, the total operation count is modeled as $N_e\\,C_F$ for the extra residual plus vector update costs for forming $u_h + \\epsilon v_h$ and the difference-and-scale on the interior unknowns, which add $4\\,(N-2)$ basic operations. Therefore the total is $N_e\\,C_F + 4\\,(N-2)$.\n\nFor all test cases, use the following trial fields to evaluate both methods:\n- $u(x) = \\sin\\!\\left( 2\\pi x \\right)$ so that $u(0)=u(1)=0$.\n- $v(x) = x\\,(1-x)$ so that $v(0)=v(1)=0$.\n\nDiscretize $u(x)$ and $v(x)$ at the mesh nodes, enforce the boundary values at $x=0$ and $x=1$, and treat only interior nodal values as unknowns and directions. In all computations, angles are in radians.\n\nTest suite:\n- Case $1$: $(N, a, b, \\tau) = (33, 1.0, 1.0, \\sqrt{\\epsilon_{\\mathrm{mach}}})$, where $\\epsilon_{\\mathrm{mach}}$ is machine epsilon for double precision.\n- Case $2$: $(N, a, b, \\tau) = (9, 10.0, 0.0, \\sqrt{\\epsilon_{\\mathrm{mach}}})$.\n- Case $3$: $(N, a, b, \\tau) = (65, 0.0, 5.0, \\sqrt{\\epsilon_{\\mathrm{mach}}})$.\n- Case $4$: $(N, a, b, \\tau) = (17, 1.0, 1.0, 10^{-3})$.\n\nFor each test case, compute:\n- The relative error $\\mathrm{err}$ as defined above.\n- The cost ratio $\\rho = \\dfrac{\\text{FD ops}}{\\text{analytic ops}} = \\dfrac{N_e\\,C_F + 4\\,(N-2)}{N_e\\,C_J}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists with two floating-point numbers per test case in the order of the test suite above, i.e., $[\\,[\\mathrm{err}_1,\\rho_1],[\\mathrm{err}_2,\\rho_2],\\ldots\\,]$ with no spaces.",
            "solution": "The user has provided a valid, well-posed problem in the field of numerical analysis for partial differential equations. The task is to derive, implement, and compare two methods for computing a matrix-free Jacobian-vector product for a discretized nonlinear PDE. The problem is scientifically sound and contains all necessary information. I will now proceed with a full solution.\n\nThe problem centers on the one-dimensional nonlinear boundary value problem on the interval $[0,1]$:\n$$\n-\\frac{d}{dx}\\Big( \\big(1 + a\\,u(x)^2\\big)\\,\\frac{du}{dx}(x) \\Big) + b\\,u(x)^3 = 0, \\quad u(0)=0,\\;u(1)=0.\n$$\n\nFirst, we establish the Galerkin weak form. Let $V_h$ be the space of continuous, piecewise-linear functions on a uniform mesh with nodes $x_i = i h$ for $i \\in \\{0, 1, \\dots, N-1\\}$ and mesh spacing $h = 1/(N-1)$. We seek a discrete solution $u_h \\in V_h$ that satisfies the homogeneous Dirichlet boundary conditions $u_h(0)=0$ and $u_h(1)=0$. The weak form requires that for all test functions $v_h \\in V_h$ (also satisfying the homogeneous boundary conditions), the following integral equation holds:\n$$\n\\int_0^1 \\left( \\left(1 + a\\,u_h^2\\right)\\frac{du_h}{dx}\\frac{dv_h}{dx} + b\\,u_h^3 v_h \\right) dx = 0.\n$$\nThis equation defines a system of nonlinear algebraic equations for the interior nodal values of $u_h$. Let $\\mathbf{u} \\in \\mathbb{R}^{N-2}$ be the vector of these interior nodal values. The system can be written as $F(\\mathbf{u}) = \\mathbf{0}$, where $F: \\mathbb{R}^{N-2} \\to \\mathbb{R}^{N-2}$ is the discrete residual operator.\n\nThe problem specifies that the integrals are to be approximated using a one-point midpoint quadrature rule on each mesh element $e_k = [x_k, x_{k+1}]$. This means $\\int_{e_k} g(x) dx \\approx h \\cdot g(x_{k+1/2})$, where $x_{k+1/2}$ is the midpoint of the element.\n\nFor a function $f_h \\in V_h$ with nodal values $f_k$ and $f_{k+1}$ on element $e_k$, its value and gradient at the midpoint are given by:\n- Midpoint value: $f_m = f_h(x_{k+1/2}) = \\frac{f_k + f_{k+1}}{2}$\n- Midpoint gradient: $f'_m = \\frac{df_h}{dx}(x_{k+1/2}) = \\frac{f_{k+1} - f_k}{h}$\n\nThe global residual $F(\\mathbf{u})$ is assembled by summing contributions from each element. For an element $e_k$ with nodal values $u_k$ and $u_{k+1}$, the midpoint values are $u_m = (u_k+u_{k+1})/2$ and $u'_m = (u_{k+1}-u_k)/h$. The element's contribution to the residual at nodes $k$ and $k+1$ is derived from the weak form tested with the corresponding basis functions. This results in the following elemental updates:\nThe diffusion term's contribution at the midpoint is $D_m = (1+a u_m^2)u'_m$.\nThe reaction term's contribution at the midpoint is $R_m = b u_m^3$.\nThe contribution of element $e_k$ to the residual at node $k$ (if it is an interior node) is $F_{left}^{(k)} = -D_m + \\frac{h}{2}R_m$.\nThe contribution of element $e_k$ to the residual at node $k+1$ (if it is an interior node) is $F_{right}^{(k)} = D_m + \\frac{h}{2}R_m$.\nThese contributions are summed over all elements to form the global residual vector $F(\\mathbf{u})$.\n\n**1. Analytic Jacobian-Vector Product**\n\nThe first task is to compute the Jacobian-vector product $J(\\mathbf{u})\\mathbf{v}$ analytically, defined as the Gâteaux derivative of $F(\\mathbf{u})$ in the direction $\\mathbf{v}$:\n$$\nJ(\\mathbf{u})\\mathbf{v} = \\left.\\frac{d}{d\\epsilon}F(\\mathbf{u} + \\epsilon \\mathbf{v})\\right|_{\\epsilon=0}.\n$$\nWe compute this by linearizing the elemental contributions. Let $u_h$ and $v_h$ be the discrete functions corresponding to the vectors $\\mathbf{u}$ and $\\mathbf{v}$. On an element $e_k$, let the perturbed solution be $u_h(\\epsilon) = u_h + \\epsilon v_h$. The midpoint values become functions of $\\epsilon$:\n- $u_m(\\epsilon) = \\frac{(u_k+\\epsilon v_k) + (u_{k+1}+\\epsilon v_{k+1})}{2} = u_m + \\epsilon v_m$\n- $u'_m(\\epsilon) = \\frac{(u_{k+1}+\\epsilon v_{k+1}) - (u_k+\\epsilon v_k)}{h} = u'_m + \\epsilon v'_m$\nwhere $v_m = (v_k+v_{k+1})/2$ and $v'_m = (v_{k+1}-v_k)/h$.\n\nThe Gâteaux derivative of the diffusion term $D_m(\\epsilon) = (1+a u_m(\\epsilon)^2)u'_m(\\epsilon)$ is:\n$$\n\\delta D_m = \\left.\\frac{d D_m(\\epsilon)}{d\\epsilon}\\right|_{\\epsilon=0} = (2 a u_m v_m)u'_m + (1+a u_m^2) v'_m.\n$$\nThe Gâteaux derivative of the reaction term $R_m(\\epsilon) = b u_m(\\epsilon)^3$ is:\n$$\n\\delta R_m = \\left.\\frac{d R_m(\\epsilon)}{d\\epsilon}\\right|_{\\epsilon=0} = 3 b u_m^2 v_m.\n$$\nThe contribution of element $e_k$ to the Jacobian-vector product at node $k$ is $(J\\mathbf{v})_{left}^{(k)} = -\\delta D_m + \\frac{h}{2} \\delta R_m$, and at node $k+1$ is $(J\\mathbf{v})_{right}^{(k)} = \\delta D_m + \\frac{h}{2} \\delta R_m$. These are assembled in the same way as the residual to form the global vector $J(\\mathbf{u})\\mathbf{v}$.\n\n**2. Finite-Difference Jacobian-Vector Product**\n\nThe second task is to approximate the Jacobian-vector product using a forward finite difference:\n$$\nJ(\\mathbf{u})\\,\\mathbf{v} \\approx \\frac{F(\\mathbf{u} + \\epsilon\\,\\mathbf{v}) - F(\\mathbf{u})}{\\epsilon}.\n$$\nThe step size $\\epsilon$ is chosen based on a heuristic that balances truncation and round-off errors:\n$$\n\\epsilon = \\tau\\,\\frac{1 + \\|\\mathbf{u}\\|_2}{\\|\\mathbf{v}\\|_2},\n$$\nwhere $\\mathbf{u}$ and $\\mathbf{v}$ are the vectors of interior nodal values, and $\\tau$ is a given scaling parameter. This method requires one additional evaluation of the residual function $F$ at the perturbed state $\\mathbf{u} + \\epsilon\\mathbf{v}$.\n\n**3. Comparison Metrics**\n\nThe two methods are compared using a relative error metric and a cost ratio.\n- The relative error $\\mathrm{err}$ measures the discrepancy between the analytic and finite-difference results:\n$$\n\\mathrm{err} = \\frac{\\|J_{\\mathrm{analytic}}(\\mathbf{u})\\mathbf{v} - J_{\\mathrm{FD}}(\\mathbf{u})\\mathbf{v}\\|_2}{\\max\\left(\\|J_{\\mathrm{analytic}}(\\mathbf{u})\\mathbf{v}\\|_2,\\,10^{-16}\\right)}.\n$$\n- The computational cost ratio $\\rho$ compares the operational cost of the two methods based on the provided model:\n$$\n\\rho = \\frac{\\text{FD ops}}{\\text{analytic ops}} = \\frac{N_e\\,C_F + 4\\,(N-2)}{N_e\\,C_J},\n$$\nwhere $N_e = N-1$ is the number of elements, $N$ is the number of nodes, $C_F=16$ is the operation count per element for a residual evaluation, and $C_J=28$ is the count for an analytic Jacobian-vector product evaluation.\n\nFor the tests, the functions $u(x)=\\sin(2\\pi x)$ and $v(x)=x(1-x)$ are discretized on the mesh to provide the vectors $\\mathbf{u}$ and $\\mathbf{v}$ of interior nodal values.",
            "answer": "```python\nimport numpy as np\n\ndef calculate_residual(u_int, N, a, b):\n    \"\"\"\n    Computes the discrete residual vector F(u) for the interior nodes.\n\n    Args:\n        u_int (np.ndarray): Vector of interior nodal values of u_h.\n        N (int): Total number of mesh nodes.\n        a (float): Parameter 'a' from the PDE.\n        b (float): Parameter 'b' from the PDE.\n\n    Returns:\n        np.ndarray: The residual vector F(u).\n    \"\"\"\n    h = 1.0 / (N - 1)\n    u_full = np.pad(u_int, 1, 'constant')\n    F = np.zeros(N - 2)\n\n    for k in range(N - 1):  # Loop over elements\n        u_k = u_full[k]\n        u_k1 = u_full[k + 1]\n\n        u_m = 0.5 * (u_k + u_k1)\n        u_prime_m = (u_k1 - u_k) / h\n\n        D_m = (1.0 + a * u_m**2) * u_prime_m\n        R_m = b * u_m**3\n        \n        F_left = -D_m + 0.5 * h * R_m\n        F_right = D_m + 0.5 * h * R_m\n\n        if k > 0:\n            F[k - 1] += F_left\n        if k < N - 2:\n            F[k] += F_right\n            \n    return F\n\ndef analytic_Jv(u_int, v_int, N, a, b):\n    \"\"\"\n    Computes the analytic Jacobian-vector product J(u)v.\n\n    Args:\n        u_int (np.ndarray): Vector of interior nodal values of u_h.\n        v_int (np.ndarray): Vector of interior nodal values of v_h.\n        N (int): Total number of mesh nodes.\n        a (float): Parameter 'a' from the PDE.\n        b (float): Parameter 'b' from the PDE.\n\n    Returns:\n        np.ndarray: The Jacobian-vector product J(u)v.\n    \"\"\"\n    h = 1.0 / (N - 1)\n    u_full = np.pad(u_int, 1, 'constant')\n    v_full = np.pad(v_int, 1, 'constant')\n    Jv = np.zeros(N - 2)\n\n    for k in range(N - 1):  # Loop over elements\n        u_k, u_k1 = u_full[k], u_full[k + 1]\n        v_k, v_k1 = v_full[k], v_full[k + 1]\n\n        u_m = 0.5 * (u_k + u_k1)\n        u_prime_m = (u_k1 - u_k) / h\n        v_m = 0.5 * (v_k + v_k1)\n        v_prime_m = (v_k1 - v_k) / h\n\n        dD_m = (2.0 * a * u_m * v_m) * u_prime_m + (1.0 + a * u_m**2) * v_prime_m\n        dR_m = 3.0 * b * u_m**2 * v_m\n        \n        Jv_left = -dD_m + 0.5 * h * dR_m\n        Jv_right = dD_m + 0.5 * h * dR_m\n\n        if k > 0:\n            Jv[k - 1] += Jv_left\n        if k < N - 2:\n            Jv[k] += Jv_right\n            \n    return Jv\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and compute specified metrics.\n    \"\"\"\n    # Use double precision machine epsilon\n    eps_mach = np.finfo(float).eps\n\n    test_cases = [\n        # (N, a, b, tau)\n        (33, 1.0, 1.0, np.sqrt(eps_mach)),\n        (9, 10.0, 0.0, np.sqrt(eps_mach)),\n        (65, 0.0, 5.0, np.sqrt(eps_mach)),\n        (17, 1.0, 1.0, 1e-3),\n    ]\n\n    results = []\n    \n    C_F = 16\n    C_J = 28\n\n    for N, a, b, tau in test_cases:\n        x_nodes = np.linspace(0, 1, N)\n        \n        # Discretize trial fields u(x) and v(x)\n        u_full = np.sin(2 * np.pi * x_nodes)\n        v_full = x_nodes * (1 - x_nodes)\n        \n        # Extract interior nodal values\n        u_int = u_full[1:-1]\n        v_int = v_full[1:-1]\n\n        # 1. Compute analytic Jacobian-vector product\n        jv_analytic = analytic_Jv(u_int, v_int, N, a, b)\n        \n        # 2. Compute finite-difference Jacobian-vector product\n        norm_u = np.linalg.norm(u_int)\n        norm_v = np.linalg.norm(v_int)\n\n        if norm_v < 1e-16: # Avoid division by zero\n            eps = tau * (1.0 + norm_u)\n        else:\n            eps = tau * (1.0 + norm_u) / norm_v\n\n        u_pert = u_int + eps * v_int\n        \n        F_u = calculate_residual(u_int, N, a, b)\n        F_u_pert = calculate_residual(u_pert, N, a, b)\n        \n        jv_fd = (F_u_pert - F_u) / eps\n\n        # 3. Compute accuracy metric 'err'\n        norm_jv_analytic = np.linalg.norm(jv_analytic)\n        diff_norm = np.linalg.norm(jv_analytic - jv_fd)\n        err = diff_norm / max(norm_jv_analytic, 1e-16)\n\n        # 4. Compute cost ratio 'rho'\n        N_e = N - 1\n        cost_analytic = N_e * C_J\n        cost_fd = N_e * C_F + 4 * (N - 2)\n        rho = cost_fd / cost_analytic\n\n        results.append([err, rho])\n\n    # Format output string to match requirement (no spaces)\n    def format_pair(pair):\n        return f\"[{pair[0]},{pair[1]}]\"\n    \n    formatted_results = [format_pair(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "This practice delves deeper into the nature of the \"exact\" Jacobian in discretized systems, revealing that it is itself an approximation whose accuracy depends on choices made during discretization. You will investigate how using a low-order numerical quadrature rule to integrate nonlinear terms can yield a computationally cheap but inaccurate Jacobian. The core of this exercise is to then apply a cornerstone of quasi-Newton methods—the secant condition—to derive a low-rank correction that systematically improves the Jacobian's accuracy, demonstrating a powerful technique for enhancing solver performance by repairing deficiencies in the underlying model .",
            "id": "3412714",
            "problem": "Consider the nonlinear boundary value problem for a Partial Differential Equation (PDE) in one spatial dimension on the unit interval,\n$$-u''(x) + R(u(x)) = 0 \\quad \\text{for } x \\in (0,1), \\quad u(0) = 0, \\; u(1) = 0,$$\nwhere the nonlinear reaction term is given by $R(u) = \\alpha \\exp(u)$ with parameter $\\alpha > 0$. Let a Finite Element (FE) discretization with continuous, piecewise-linear basis functions be used on a uniform mesh with $N$ elements and $N+1$ nodes. The weak form is: find $u \\in H_0^1(0,1)$ such that for all $v \\in H_0^1(0,1)$,\n$$\\int_0^1 u'(x) v'(x) \\, dx + \\int_0^1 R(u(x)) \\, v(x) \\, dx = 0.$$\nDefine the discrete residual map $F(U) \\in \\mathbb{R}^{N-1}$ for the interior nodal values $U \\in \\mathbb{R}^{N-1}$ corresponding to the FE approximation $u_h$, and its Jacobian matrix $J(U) \\in \\mathbb{R}^{N-1 \\times N-1}$ with entries\n$$F_i(U) = \\int_0^1 u_h'(x) \\, \\varphi_i'(x) \\, dx + \\int_0^1 R(u_h(x)) \\, \\varphi_i(x) \\, dx,$$\n$$J_{ij}(U) = \\frac{\\partial F_i}{\\partial U_j}(U) = \\int_0^1 \\varphi_j'(x) \\, \\varphi_i'(x) \\, dx + \\int_0^1 R'(u_h(x)) \\, \\varphi_j(x) \\, \\varphi_i(x) \\, dx,$$\nwhere $\\{\\varphi_i\\}$ are the standard hat basis functions associated with interior nodes and $R'(u) = \\alpha \\exp(u)$. For $F(U)$ and $J(U)$, the diffusion contributions are integrated exactly for piecewise-linear elements. The reaction contributions are to be computed via numerical quadrature. Define two quadrature schemes for the reaction integrals:\n- A reference quadrature using five-point Gauss–Legendre on each element, considered sufficiently accurate for this purpose.\n- An under-integrated midpoint rule on each element (single-point quadrature at the element midpoint).\n\nYour task is to examine, at a given discrete state $U$ sampled from a prescribed smooth function, how under-integration affects the accuracy of the Jacobian, and to design a quasi-Newton (QN) correction using a secant equation to compensate. Specifically:\n\n1. Mesh, basis, and state sampling:\n   - Use a uniform mesh with $N$ elements, nodes $x_k = k/N$ for $k=0,1,\\dots,N$.\n   - Prescribe a smooth interior state by sampling $u^\\star(x) = 0.5 \\sin(\\pi x) + 0.2 \\sin(2\\pi x)$ at the nodes and enforcing Dirichlet boundary values $u^\\star(0) = u^\\star(1) = 0$. Let $U \\in \\mathbb{R}^{N-1}$ be the vector of interior nodal values of this sampled function.\n2. Residual and Jacobian formation:\n   - Assemble the residual $F(U)$ using the five-point Gauss–Legendre rule for the reaction term.\n   - Assemble the Jacobian $J_{\\text{ref}}(U)$ using the five-point Gauss–Legendre rule for the reaction term and $J_{\\text{under}}(U)$ using the midpoint rule for the reaction term. In both, include the exact element stiffness for diffusion.\n3. Error quantification for under-integration:\n   - Compute the relative Frobenius-norm error\n     $$E_0 = \\frac{\\|J_{\\text{under}}(U) - J_{\\text{ref}}(U)\\|_F}{\\|J_{\\text{ref}}(U)\\|_F}.$$\n4. Quasi-Newton correction from a secant residual:\n   - Define a deterministic secant direction $s \\in \\mathbb{R}^{N-1}$ with entries $s_i = \\delta \\sin(2\\pi x_i)$ for the interior nodes $x_i$, where $\\delta = 10^{-2}$ is fixed.\n   - Form the secant residual $y = F(U + s) - F(U)$ using the five-point Gauss–Legendre rule for the reaction term in both evaluations of $F$.\n   - Construct a rank-one correction to $J_{\\text{under}}(U)$ that enforces the secant condition $J_{\\text{corr}}(U) \\, s = y$ while changing $J_{\\text{under}}(U)$ as little as possible in the Frobenius norm. You must derive and implement the least-change update that satisfies this constraint.\n5. Post-correction error:\n   - Compute the relative Frobenius-norm error after correction,\n     $$E_1 = \\frac{\\|J_{\\text{corr}}(U) - J_{\\text{ref}}(U)\\|_F}{\\|J_{\\text{ref}}(U)\\|_F},$$\n     and the reduction ratio $r = E_1 / E_0$.\n\nDesign a program that carries out the above steps for the following test suite of parameter values $(N,\\alpha)$:\n- $(N,\\alpha) = (16, 1)$, a moderate mesh with mild nonlinearity.\n- $(N,\\alpha) = (16, 50)$, a moderate mesh with strong nonlinearity.\n- $(N,\\alpha) = (64, 50)$, a fine mesh with strong nonlinearity.\n- $(N,\\alpha) = (8, 50)$, a coarse mesh with strong nonlinearity (edge case).\n- $(N,\\alpha) = (64, 1)$, a fine mesh with mild nonlinearity.\n\nFor each test case, report the triple $[E_0, E_1, r]$ as real numbers. The final program output must be a single line containing a JSON-like array of these triples in the given order, with no spaces, where each floating-point number is formatted in scientific notation with eight digits after the decimal point (Python format specifier \"%.8e\"). For example, an output for two cases would look like:\n[[1.23456789e-03,9.87654321e-04,7.99999999e-01],[...],...]\nThere are no physical units involved in this problem. All angles, when they appear in trigonometric functions, are in radians.\n\nYour program should produce a single line of output containing the results as a comma-separated list of these triples enclosed in square brackets, with no extra text.",
            "solution": "The problem is valid. It presents a well-defined task in computational science, specifically concerning the numerical solution of a nonlinear partial differential equation (PDE) using the Finite Element Method (FEM). All parameters and procedures are specified with sufficient precision to permit a unique and verifiable solution.\n\nThe problem investigates the accuracy of Jacobians assembled using different numerical quadrature rules and explores the use of a quasi-Newton correction to improve a less accurate Jacobian. The methodology proceeds as follows.\n\n**1. Finite Element Discretization**\n\nThe subject of study is the nonlinear boundary value problem (BVP):\n$$ -u''(x) + \\alpha \\exp(u(x)) = 0, \\quad x \\in (0,1) $$\nwith homogeneous Dirichlet boundary conditions $u(0) = u(1) = 0$. The parameter $\\alpha > 0$ controls the strength of the nonlinearity.\n\nThis PDE is discretized using the Galerkin Finite Element Method with continuous, piecewise-linear basis functions $\\{\\varphi_i(x)\\}_{i=1}^{N-1}$ on a uniform mesh of width $h = 1/N$. The basis functions, commonly known as \"hat functions,\" are defined such that $\\varphi_i(x_j) = \\delta_{ij}$ for interior nodes $x_j=j/N$. The finite element approximation $u_h(x)$ to the solution $u(x)$ is expressed as a linear combination of these basis functions:\n$$ u_h(x) = \\sum_{j=1}^{N-1} U_j \\varphi_j(x) $$\nwhere $U \\in \\mathbb{R}^{N-1}$ is the vector of unknown coefficients, representing the values of the solution at the interior nodes.\n\nSubstituting $u_h$ into the weak form of the PDE and testing with each basis function $\\varphi_i$ yields a system of nonlinear algebraic equations $F(U) = 0$, where the $i$-th component of the residual vector $F(U)$ is:\n$$ F_i(U) = \\int_0^1 u_h'(x) \\varphi_i'(x) \\, dx + \\int_0^1 \\alpha \\exp(u_h(x)) \\varphi_i(x) \\, dx $$\nThis system is typically solved using Newton's method, which requires the Jacobian matrix $J(U)$ of the residual $F(U)$. The entries of the Jacobian are given by $J_{ij}(U) = \\partial F_i / \\partial U_j$:\n$$ J_{ij}(U) = \\int_0^1 \\varphi_j'(x) \\varphi_i'(x) \\, dx + \\int_0^1 \\alpha \\exp(u_h(x)) \\varphi_j(x) \\varphi_i(x) \\, dx $$\n\n**2. Assembly and Numerical Quadrature**\n\nThe integrals defining $F(U)$ and $J(U)$ are computed by summing contributions from each element $[x_k, x_{k+1}]$.\nThe first term in both expressions, involving derivatives, constitutes the stiffness matrix $A$. For a uniform mesh and piecewise-linear elements, this matrix is constant, tridiagonal, and can be computed exactly. Its entries are $A_{ii} = 2/h$ and $A_{i,i\\pm 1} = -1/h$.\n\nThe second terms, which arise from the nonlinear reaction term $R(u)=\\alpha\\exp(u)$, depend on the state $U$ and require numerical integration (quadrature). The accuracy of the computed residual and Jacobian depends critically on the chosen quadrature rule. The problem specifies two rules for these reaction integrals:\n- **Reference Rule ($J_{\\text{ref}}$)**: A five-point Gauss-Legendre quadrature on each element. This is a high-order rule considered to be highly accurate for the given context.\n- **Under-integrated Rule ($J_{\\text{under}}$)**: A one-point midpoint rule on each element. This rule is computationally cheaper but less accurate, especially for nonlinear integrands or coarse meshes.\n\nThe error introduced by the under-integrated Jacobian is quantified by the relative Frobenius norm difference:\n$$ E_0 = \\frac{\\|J_{\\text{under}}(U) - J_{\\text{ref}}(U)\\|_F}{\\|J_{\\text{ref}}(U)\\|_F} $$\n\n**3. Quasi-Newton Correction**\n\nQuasi-Newton methods build an approximation of the Jacobian. We can adapt this idea to \"correct\" an inaccurate-but-cheap Jacobian, $J_{\\text{under}}(U)$. A desirable property for any Jacobian approximation $B$ is that it satisfies the secant equation, $B s = y$, where $s$ is a step vector and $y$ is the change in the residual, $y = F(U+s) - F(U)$.\n\nThe problem directs us to find a corrected Jacobian, $J_{\\text{corr}}(U)$, by performing a minimal change to $J_{\\text{under}}(U)$ in the Frobenius norm, subject to satisfying the secant equation for a prescribed direction $s$. This is a constrained optimization problem:\n$$ \\min_{J_{\\text{corr}}} \\frac{1}{2} \\|J_{\\text{corr}} - J_{\\text{under}}\\|_F^2 \\quad \\text{subject to} \\quad J_{\\text{corr}} s = y $$\nThe unique solution to this problem is a rank-one update given by the formula:\n$$ J_{\\text{corr}}(U) = J_{\\text{under}}(U) + \\frac{(y - J_{\\text{under}}(U) s) s^T}{s^T s} $$\nThis update modifies $J_{\\text{under}}(U)$ by adding the outer product of two vectors, ensuring the secant condition is met while perturbing the original matrix as little as possible. The vectors $s$ and $y$ are defined as:\n- $s_i = \\delta \\sin(2\\pi x_i)$ with $\\delta=10^{-2}$ for interior nodes $x_i$.\n- $y = F(U+s) - F(U)$, where both $F(U+s)$ and $F(U)$ are computed using the accurate five-point Gauss-Legendre rule.\n\n**4. Error after Correction**\n\nThe effectiveness of this correction is measured by computing the relative Frobenius norm error of the corrected Jacobian with respect to the reference Jacobian:\n$$ E_1 = \\frac{\\|J_{\\text{corr}}(U) - J_{\\text{ref}}(U)\\|_F}{\\|J_{\\text{ref}}(U)\\|_F} $$\nThe reduction ratio, $r = E_1 / E_0$, indicates the degree of improvement, with $r < 1$ signifying a successful correction.\n\n**5. Algorithm Summary**\n\nFor each parameter pair $(N, \\alpha)$:\n1.  Define the mesh with step size $h=1/N$ and identify interior nodes.\n2.  Sample the specified state $U$ from $u^\\star(x) = 0.5 \\sin(\\pi x) + 0.2 \\sin(2\\pi x)$.\n3.  Implement an assembly routine that computes the residual vector $F(U)$ and Jacobian matrix $J(U)$ for a given quadrature rule (number of points).\n4.  Assemble the reference Jacobian $J_{\\text{ref}}(U)$ and reference residual $F_{\\text{ref}}(U)$ using $5$-point quadrature.\n5.  Assemble the under-integrated Jacobian $J_{\\text{under}}(U)$ using $1$-point (midpoint) quadrature.\n6.  Calculate the initial error $E_0$.\n7.  Define the secant vector $s$ and compute the corresponding residual difference $y = F(U+s) - F(U)$ using $5$-point quadrature.\n8.  Compute the corrected Jacobian $J_{\\text{corr}}(U)$ using the rank-one update formula.\n9.  Calculate the corrected error $E_1$ and the reduction ratio $r=E_1/E_0$.\n10. Store the resulting triple $[E_0, E_1, r]$.\nThe final output is an aggregation of these triples for all specified test cases.",
            "answer": "```python\nimport numpy as np\n\ndef assemble(U_vec, N, alpha, num_quad_points):\n    \"\"\"\n    Assembles the residual vector F and Jacobian matrix J for the given problem.\n\n    Args:\n        U_vec (np.ndarray): Vector of interior nodal values, shape (N-1,).\n        N (int): Number of elements in the mesh.\n        alpha (float): Parameter in the nonlinear reaction term.\n        num_quad_points (int): Number of quadrature points for reaction term integration.\n\n    Returns:\n        F (np.ndarray): The residual vector, shape (N-1,).\n        J (np.ndarray): The Jacobian matrix, shape (N-1, N-1).\n    \"\"\"\n    n_dof = N - 1\n    h = 1.0 / N\n    nodes = np.linspace(0, 1, N + 1)\n    \n    # 1. Stiffness matrix (diffusion term) - exact integration\n    diag_main = np.full(n_dof, 2.0 / h)\n    diag_off = np.full(n_dof - 1, -1.0 / h)\n    A = np.diag(diag_main) + np.diag(diag_off, k=1) + np.diag(diag_off, k=-1)\n    \n    # Initialize reaction term contributions\n    F_reac = np.zeros(n_dof)\n    J_reac = np.zeros((n_dof, n_dof))\n    \n    # Create a full vector of nodal values, including boundary conditions\n    U_full = np.zeros(N + 1)\n    U_full[1:N] = U_vec\n    \n    # Get quadrature points and weights on reference interval [-1, 1]\n    if num_quad_points == 1: # Midpoint rule\n        quad_pts_ref, quad_w_ref = np.array([0.0]), np.array([2.0])\n    else:\n        quad_pts_ref, quad_w_ref = np.polynomial.legendre.leggauss(num_quad_points)\n\n    # 2. Loop over elements to assemble reaction terms\n    for e in range(N):\n        # Global indices of nodes for element e\n        node_idx_1, node_idx_2 = e, e + 1\n        \n        # Nodal values for the element\n        u1, u2 = U_full[node_idx_1], U_full[node_idx_2]\n        \n        # Map quadrature points from [-1, 1] to element [x_e, x_{e+1}]\n        x_e = nodes[e]\n        quad_pts_elem = x_e + (h / 2.0) * (quad_pts_ref + 1.0)\n        quad_w_elem = (h / 2.0) * quad_w_ref\n\n        # Quadrature loop\n        for qp, qw in zip(quad_pts_elem, quad_w_elem):\n            # Local coordinate on element \\xi in [0, h]\n            xi = qp - x_e\n            \n            # Values of local basis functions at quad point\n            phi1_val = 1.0 - xi / h\n            phi2_val = xi / h\n            \n            # Value of u_h and its derivative at quad point\n            u_h_val = u1 * phi1_val + u2 * phi2_val\n            R_val = alpha * np.exp(u_h_val)\n            R_prime_val = R_val # R'(u) = alpha * exp(u)\n            \n            # Add contributions to local reaction vector and matrix\n            # Indices i and j correspond to basis functions phi_1 and phi_2 on the element\n            f1 = qw * R_val * phi1_val\n            f2 = qw * R_val * phi2_val\n            \n            m11 = qw * R_prime_val * phi1_val * phi1_val\n            m12 = qw * R_prime_val * phi1_val * phi2_val\n            m22 = qw * R_prime_val * phi2_val * phi2_val\n            \n            # Assemble into global F_reac and J_reac\n            # node_idx_1 corresponds to DOF `node_idx_1 - 1` if interior\n            if node_idx_1 > 0: \n                dof1 = node_idx_1 - 1\n                F_reac[dof1] += f1\n                J_reac[dof1, dof1] += m11\n            \n            # node_idx_2 corresponds to DOF `node_idx_2 - 1` if interior\n            if node_idx_2 < N:\n                dof2 = node_idx_2 - 1\n                F_reac[dof2] += f2\n                J_reac[dof2, dof2] += m22\n            \n            # Off-diagonal term\n            if node_idx_1 > 0 and node_idx_2 < N:\n                dof1, dof2 = node_idx_1 - 1, node_idx_2 - 1\n                J_reac[dof1, dof2] += m12\n                J_reac[dof2, dof1] += m12\n\n    # Combine diffusion and reaction terms\n    F = A @ U_vec + F_reac\n    J = A + J_reac\n    \n    return F, J\n\ndef solve():\n    \"\"\"\n    Main driver function to run the analysis for all test cases.\n    \"\"\"\n    test_cases = [\n        (16, 1.0),\n        (16, 50.0),\n        (64, 50.0),\n        (8, 50.0),\n        (64, 1.0),\n    ]\n\n    results = []\n\n    for N, alpha in test_cases:\n        # 1. Mesh, basis, and state sampling\n        n_dof = N - 1\n        x_int = np.linspace(0, 1, N + 1)[1:-1]\n        U = 0.5 * np.sin(np.pi * x_int) + 0.2 * np.sin(2 * np.pi * x_int)\n        \n        # 2. Residual and Jacobian formation\n        F_ref, J_ref = assemble(U, N, alpha, num_quad_points=5)\n        _, J_under = assemble(U, N, alpha, num_quad_points=1)\n\n        # 3. Error quantification for under-integration\n        norm_J_ref = np.linalg.norm(J_ref, 'fro')\n        E0 = np.linalg.norm(J_under - J_ref, 'fro') / norm_J_ref\n\n        # 4. Quasi-Newton correction from a secant residual\n        delta = 1e-2\n        s = delta * np.sin(2 * np.pi * x_int)\n        U_plus_s = U + s\n        \n        F_plus_s, _ = assemble(U_plus_s, N, alpha, num_quad_points=5)\n        y = F_plus_s - F_ref\n        \n        # Least-change rank-one update\n        resid_s = y - (J_under @ s)\n        sTs = s @ s\n        J_corr = J_under + np.outer(resid_s, s) / sTs\n        \n        # 5. Post-correction error\n        E1 = np.linalg.norm(J_corr - J_ref, 'fro') / norm_J_ref\n        r = E1 / E0 if E0 > 0 else 0.0\n\n        results.append([E0, E1, r])\n\n    # Format the final output string exactly as specified\n    case_strings = []\n    for res_triple in results:\n        case_strings.append(f\"[{res_triple[0]:.8e},{res_triple[1]:.8e},{res_triple[2]:.8e}]\")\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Efficient algorithms often exploit the unique structure of a problem. This practice moves beyond global Jacobian approximations to explore targeted updates for systems with localized nonlinearities, such as those confined to boundary conditions. You will implement a \"partial\" quasi-Newton method that selectively updates only the Jacobian rows corresponding to the nonlinear boundary terms, leaving the exact, constant interior part of the matrix untouched. This exercise highlights the important principle of tailoring solver algorithms to the specific structure of a PDE, a strategy that can lead to significant computational savings in practical applications .",
            "id": "3412696",
            "problem": "Consider the nonlinear boundary value problem arising from a one-dimensional partial differential equation (PDE), specifically the Poisson equation with a nonlinear Robin-type boundary condition. Let $u : [0,1] \\to \\mathbb{R}$ be a sufficiently smooth function satisfying\n$$\n-\\frac{d^2 u}{dx^2} = f(x) \\quad \\text{for } x \\in (0,1),\n$$\nwith nonlinear boundary conditions\n$$\n\\partial_n u(0) + h(u(0)) = 0, \\qquad \\partial_n u(1) + h(u(1)) = 0,\n$$\nwhere $\\partial_n$ denotes the outward normal derivative, $h(u) = \\alpha u + \\beta u^3$, and $f(x) \\equiv 0$. The outward normal derivative satisfies $\\partial_n u(0) = -u'(0)$ and $\\partial_n u(1) = u'(1)$.\n\nYour task is to discretize the PDE on a uniform grid using $n$ intervals, so the grid has $n+1$ nodes $x_i = i h$ for $i = 0,1,\\dots,n$ with spacing $h = 1/n$. Use the second-order centered finite difference scheme for the interior points and second-order one-sided formulas for the boundary derivatives:\n$$\nu'(0) \\approx \\frac{-3 u_0 + 4 u_1 - u_2}{2h}, \\qquad u'(1) \\approx \\frac{3 u_n - 4 u_{n-1} + u_{n-2}}{2h}.\n$$\nDefine the residual vector $R(u) \\in \\mathbb{R}^{n+1}$ with components\n$$\nR_0(u) = \\frac{3 u_0 - 4 u_1 + u_2}{2h} + h(u_0),\n$$\n$$\nR_i(u) = -\\frac{u_{i-1} - 2 u_i + u_{i+1}}{h^2} \\quad \\text{for } i = 1,2,\\dots,n-1,\n$$\n$$\nR_n(u) = \\frac{3 u_n - 4 u_{n-1} + u_{n-2}}{2h} + h(u_n).\n$$\n1. Assemble the Jacobian matrix $J(u) \\in \\mathbb{R}^{(n+1)\\times(n+1)}$ by differentiating $R(u)$ with respect to $u$. Explicitly derive the nonzero entries of $J(u)$ for the interior and boundary rows in terms of $h$, $\\alpha$, $\\beta$, and $u$.\n2. Implement a Newton method to solve $R(u) = 0$, using an initial guess $u^{(0)}_i = 0.1 \\sin(\\pi x_i)$ for all $i$, with a backtracking line search that accepts a step when the Euclidean norm $\\|R(u)\\|_2$ decreases. Use a maximum of $25$ Newton iterations and declare convergence when $\\|R(u)\\|_2 \\le 10^{-10}$.\n3. Propose and implement a partial secant quasi-Newton strategy tailored for this problem: when boundary nonlinearity dominates relative to the interior linear stiffness, update only the boundary-related Jacobian rows (rows $0$ and $n$) using a secant approximation for $h'(u)$, while keeping interior rows exact and fixed. Specifically:\n   - Define the dominance criterion\n     $$\n     \\max\\left(|h'(u_0)|, |h'(u_n)|\\right) > \\kappa \\cdot \\frac{2}{h},\n     $$\n     with $h'(u) = \\alpha + 3\\beta u^2$ and fixed threshold $\\kappa = 5$.\n   - If dominance holds and a previous iterate $u^{(k-1)}$ exists, approximate the boundary derivatives via the secant slopes\n     $$\n     s_0^{(k)} = \\frac{h(u^{(k)}_0) - h(u^{(k-1)}_0)}{u^{(k)}_0 - u^{(k-1)}_0}, \\qquad\n     s_n^{(k)} = \\frac{h(u^{(k)}_n) - h(u^{(k-1)}_n)}{u^{(k)}_n - u^{(k-1)}_n},\n     $$\n     provided the denominators are nonzero; otherwise fall back to the exact $h'(u)$.\n   - Use these secant slopes only to replace $h'(u)$ in the diagonal entries of the boundary rows of $J(u)$, keeping the finite difference coefficients unchanged.\n4. Compare the full Newton method (exact Jacobian at every iteration) with the partial secant quasi-Newton method (boundary-only secant updates per the criterion). For each test case, report:\n   - The number of Newton iterations required for convergence for the full method,\n   - The number of Newton iterations required for convergence for the partial secant method,\n   - The number of times the exact boundary derivative $h'(u)$ is used in the Jacobian (i.e., the number of boundary exact evaluations) for the full method,\n   - The number of times the exact boundary derivative $h'(u)$ is used in the Jacobian for the partial secant method,\n   - The final residual norm $\\|R(u)\\|_2$ for the full method,\n   - The final residual norm $\\|R(u)\\|_2$ for the partial secant method.\n\nYour program must implement both strategies and run the following test suite, with $f(x) \\equiv 0$ in all cases:\n- Test case $1$: $n = 64$, $\\alpha = 1$, $\\beta = 5$, $\\kappa = 5$.\n- Test case $2$: $n = 64$, $\\alpha = 0.5$, $\\beta = 50$, $\\kappa = 5$.\n- Test case $3$: $n = 32$, $\\alpha = 1$, $\\beta = 0$, $\\kappa = 5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order for each test case, the six values described above, concatenated across test cases. For example, the output format is\n$$\n[\\text{it\\_full\\_1}, \\text{it\\_partial\\_1}, \\text{evals\\_full\\_1}, \\text{evals\\_partial\\_1}, \\text{norm\\_full\\_1}, \\text{norm\\_partial\\_1}, \\dots, \\text{it\\_full\\_3}, \\text{it\\_partial\\_3}, \\text{evals\\_full\\_3}, \\text{evals\\_partial\\_3}, \\text{norm\\_full\\_3}, \\text{norm\\_partial\\_3}],\n$$\nwhere each entry is either a boolean, an integer, a float, or a list of these fundamental types. No physical or angle units apply to this problem.",
            "solution": "The user has presented a valid problem in numerical analysis, concerning the solution of a nonlinear boundary value problem using finite differences and Newton-type methods. The problem is scientifically grounded, well-posed, and all necessary parameters and conditions are provided. I will therefore proceed with a full solution.\n\nThe problem requires solving the nonlinear boundary value problem (BVP) given by the differential equation\n$$\n-\\frac{d^2 u}{dx^2} = 0, \\quad x \\in (0,1)\n$$\nsubject to nonlinear Robin-type boundary conditions:\n$$\n-u'(0) + h(u(0)) = 0\n$$\n$$\nu'(1) + h(u(1)) = 0\n$$\nwhere $h(u) = \\alpha u + \\beta u^3$.\n\nThe domain $[0,1]$ is discretized into $n$ intervals of size $h=1/n$, yielding $n+1$ grid points $x_i = i h$ for $i=0, 1, \\dots, n$. The solution $u(x)$ is approximated by a vector $u \\in \\mathbb{R}^{n+1}$ where $u_i \\approx u(x_i)$.\n\nUsing the provided finite difference schemes, we formulate a system of $n+1$ nonlinear algebraic equations $R(u) = 0$, where $R(u)$ is the residual vector.\nThe components of the residual are:\n- For $i=0$ (at $x=0$): a second-order one-sided difference for $u'(0)$ is used in the first boundary condition, $-u'(0) + h(u_0) = 0$, giving\n$$\nR_0(u) = -\\left(\\frac{-3 u_0 + 4 u_1 - u_2}{2h}\\right) + h(u_0) = \\frac{3 u_0 - 4 u_1 + u_2}{2h} + \\alpha u_0 + \\beta u_0^3\n$$\n- For $i=1, 2, \\dots, n-1$ (interior points): a second-order centered difference for the PDE $-u''(x_i)=0$ gives\n$$\nR_i(u) = -\\frac{u_{i-1} - 2 u_i + u_{i+1}}{h^2}\n$$\n- For $i=n$ (at $x=1$): a second-order one-sided difference for $u'(1)$ is used in the second boundary condition, $u'(1) + h(u_n) = 0$, giving\n$$\nR_n(u) = \\frac{3 u_n - 4 u_{n-1} + u_{n-2}}{2h} + h(u_n) = \\frac{3 u_n - 4 u_{n-1} + u_{n-2}}{2h} + \\alpha u_n + \\beta u_n^3\n$$\nThis system $R(u)=0$ is solved using Newton's method.\n\n### 1. Jacobian Matrix Derivation\nNewton's method requires the Jacobian matrix $J(u) \\in \\mathbb{R}^{(n+1) \\times (n+1)}$, where $J_{ij}(u) = \\frac{\\partial R_i}{\\partial u_j}$. The derivative of the nonlinear term is $h'(u) = \\frac{d h}{d u} = \\alpha + 3\\beta u^2$.\n\nThe non-zero entries of the Jacobian are derived as follows:\n\n**Boundary Row ($i=0$):**\n$$\nJ_{00} = \\frac{\\partial R_0}{\\partial u_0} = \\frac{3}{2h} + h'(u_0) = \\frac{3}{2h} + \\alpha + 3\\beta u_0^2\n$$\n$$\nJ_{01} = \\frac{\\partial R_0}{\\partial u_1} = -\\frac{4}{2h} = -\\frac{2}{h}\n$$\n$$\nJ_{02} = \\frac{\\partial R_0}{\\partial u_2} = \\frac{1}{2h}\n$$\n\n**Interior Rows ($i=1, \\dots, n-1$):**\n$$\nJ_{i, i-1} = \\frac{\\partial R_i}{\\partial u_{i-1}} = -\\frac{1}{h^2}\n$$\n$$\nJ_{i, i} = \\frac{\\partial R_i}{\\partial u_i} = \\frac{2}{h^2}\n$$\n$$\nJ_{i, i+1} = \\frac{\\partial R_i}{\\partial u_{i+1}} = -\\frac{1}{h^2}\n$$\n\n**Boundary Row ($i=n$):**\n$$\nJ_{n, n-2} = \\frac{\\partial R_n}{\\partial u_{n-2}} = \\frac{1}{2h}\n$$\n$$\nJ_{n, n-1} = \\frac{\\partial R_n}{\\partial u_{n-1}} = -\\frac{4}{2h} = -\\frac{2}{h}\n$$\n$$\nJ_{n, n} = \\frac{\\partial R_n}{\\partial u_n} = \\frac{3}{2h} + h'(u_n) = \\frac{3}{2h} + \\alpha + 3\\beta u_n^2\n$$\n\nAll other entries of $J(u)$ are zero. The matrix has a pentadiagonal structure due to the three-point stencils used for the boundary derivatives.\n\n### 2. Numerical Solution Strategy\nNewton's method is an iterative procedure to find the root of $R(u) = 0$. Starting with an initial guess $u^{(0)}$, subsequent iterates are computed as:\n$$\nu^{(k+1)} = u^{(k)} + \\Delta u^{(k)}\n$$\nwhere the update step $\\Delta u^{(k)}$ is the solution to the linear system:\n$$\nJ(u^{(k)}) \\Delta u^{(k)} = -R(u^{(k)})\n$$\nThis process is repeated until the Euclidean norm of the residual, $\\|R(u^{(k)})\\|_2$, is below a specified tolerance ($10^{-10}$).\nA backtracking line search is employed to ensure that each step decreases the residual norm. A full step $\\Delta u^{(k)}$ is attempted first. If $\\|R(u^{(k)} + \\Delta u^{(k)})\\|_2 \\ge \\|R(u^{(k)})\\|_2$, the step size is successively halved until the norm decreases or a minimum step size is reached.\n\n### 3. Full Newton vs. Partial Secant Quasi-Newton Method\n\n**Full Newton Method:** The Jacobian matrix $J(u^{(k)})$ is computed exactly using the formulas above at every iteration $k$. This is computationally demanding if the derivative $h'(u)$ is expensive to evaluate. We track the number of iterations where this exact evaluation is performed for the boundary rows, `evals_full`, which is equal to the number of iterations `it_full`.\n\n**Partial Secant Quasi-Newton Method:** This method aims to reduce computational cost by approximating parts of the Jacobian.\n- The interior rows of the Jacobian, corresponding to the linear part of the PDE, are kept exact and constant.\n- The boundary-related entries $J_{00}$ and $J_{nn}$ are updated selectively.\n- A **dominance criterion** is checked at each iteration: $\\max(|h'(u_0)|, |h'(u_n)|) > \\kappa \\cdot \\frac{2}{h}$. This heuristic compares the \"stiffness\" from the nonlinear boundary term to the \"stiffness\" from the interior grid stencil.\n- If the criterion is met and $k > 0$, the exact derivatives $h'(u_0)$ and $h'(u_n)$ are replaced by secant approximations:\n$$\ns_0^{(k)} = \\frac{h(u^{(k)}_0) - h(u^{(k-1)}_0)}{u^{(k)}_0 - u^{(k-1)}_0}, \\qquad s_n^{(k)} = \\frac{h(u^{(k)}_n) - h(u^{(k-1)}_n)}{u^{(k)}_n - u^{(k-1)}_n}\n$$\n- An exact derivative $h'(u)$ is used for a boundary point if:\n    1. It is the first iteration ($k=0$).\n    2. The dominance criterion is not met.\n    3. The denominator for the secant approximation is close to zero, to prevent division by zero.\n- The number of exact boundary evaluations, `evals_partial`, counts the number of iterations where an exact derivative was used for at least one boundary point for any of the above reasons. This count provides a measure of how often the \"expensive\" evaluation was needed.\n\nBy comparing the number of iterations (`it_full` vs. `it_partial`) and the number of exact derivative evaluations (`evals_full` vs. `evals_partial`), we can assess the efficiency of the quasi-Newton strategy.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test suite and print results.\n    \"\"\"\n\n    def h_func(u, alpha, beta):\n        \"\"\"Computes the nonlinear boundary function h(u).\"\"\"\n        return alpha * u + beta * u**3\n\n    def h_prime_func(u, alpha, beta):\n        \"\"\"Computes the derivative of the boundary function h'(u).\"\"\"\n        return alpha + 3 * beta * u**2\n\n    def calculate_residual(u, n, h_val, alpha, beta):\n        \"\"\"Computes the residual vector R(u).\"\"\"\n        dim = n + 1\n        R = np.zeros(dim)\n        \n        # R_0\n        R[0] = (3 * u[0] - 4 * u[1] + u[2]) / (2 * h_val) + h_func(u[0], alpha, beta)\n        \n        # R_i for i = 1, ..., n-1 (since f(x)=0)\n        for i in range(1, n):\n            R[i] = -(u[i-1] - 2 * u[i] + u[i+1]) / h_val**2\n            \n        # R_n\n        R[n] = (3 * u[n] - 4 * u[n-1] + u[n-2]) / (2 * h_val) + h_func(u[n], alpha, beta)\n        \n        return R\n\n    def run_solvers_for_case(params):\n        \"\"\"\n        Runs both full Newton and partial secant methods for a given test case.\n        \"\"\"\n        n, alpha, beta, kappa = params\n        h = 1.0 / n\n        dim = n + 1\n        x = np.linspace(0, 1, dim)\n        max_iter = 25\n        tol = 1e-10\n        \n        # --- Full Newton Method ---\n        u_full = 0.1 * np.sin(np.pi * x)\n        it_full = 0\n        for k in range(max_iter):\n            R = calculate_residual(u_full, n, h, alpha, beta)\n            norm_R = np.linalg.norm(R)\n            if norm_R <= tol:\n                break\n            \n            J = np.zeros((dim, dim))\n            for i in range(1, n):\n                J[i, i-1], J[i, i], J[i, i+1] = -1/h**2, 2/h**2, -1/h**2\n            \n            J[0, 0] = 1.5/h + h_prime_func(u_full[0], alpha, beta)\n            J[0, 1], J[0, 2] = -2/h, 0.5/h\n            J[n, n] = 1.5/h + h_prime_func(u_full[n], alpha, beta)\n            J[n, n-1], J[n, n-2] = -2/h, 0.5/h\n            \n            delta_u = np.linalg.solve(J, -R)\n            \n            lambda_ls = 1.0\n            u_prev_ls = np.copy(u_full)\n            for _ in range(10): # Max 10 line search steps\n                u_new = u_prev_ls + lambda_ls * delta_u\n                if np.linalg.norm(calculate_residual(u_new, n, h, alpha, beta)) < norm_R:\n                    break\n                lambda_ls /= 2.0\n            u_full = u_new\n            it_full += 1\n        \n        norm_full = np.linalg.norm(calculate_residual(u_full, n, h, alpha, beta))\n        evals_full = it_full\n\n        # --- Partial Secant Method ---\n        u_partial = 0.1 * np.sin(np.pi * x)\n        u_prev = np.copy(u_partial)\n        it_partial = 0\n        evals_partial = 0\n\n        for k in range(max_iter):\n            R = calculate_residual(u_partial, n, h, alpha, beta)\n            norm_R = np.linalg.norm(R)\n            if norm_R <= tol:\n                break\n\n            J = np.zeros((dim, dim))\n            for i in range(1, n):\n                J[i, i-1], J[i, i], J[i, i+1] = -1/h**2, 2/h**2, -1/h**2\n            J[0, 1], J[0, 2] = -2/h, 0.5/h\n            J[n, n-1], J[n, n-2] = -2/h, 0.5/h\n\n            is_exact_update = False\n            h_prime_0 = h_prime_func(u_partial[0], alpha, beta)\n            h_prime_n = h_prime_func(u_partial[n], alpha, beta)\n            is_dominant = max(abs(h_prime_0), abs(h_prime_n)) > kappa * 2.0 / h\n\n            if k == 0 or not is_dominant:\n                is_exact_update = True\n                J[0, 0] = 1.5/h + h_prime_0\n                J[n, n] = 1.5/h + h_prime_n\n            else: # Attempt secant\n                s0, sn = None, None\n                denom0 = u_partial[0] - u_prev[0]\n                if abs(denom0) > 1e-12:\n                    s0 = (h_func(u_partial[0], alpha, beta) - h_func(u_prev[0], alpha, beta)) / denom0\n                denom_n = u_partial[n] - u_prev[n]\n                if abs(denom_n) > 1e-12:\n                    sn = (h_func(u_partial[n], alpha, beta) - h_func(u_prev[n], alpha, beta)) / denom_n\n\n                if s0 is not None:\n                    J[0, 0] = 1.5/h + s0\n                else:\n                    J[0, 0] = 1.5/h + h_prime_0\n                    is_exact_update = True\n                \n                if sn is not None:\n                    J[n, n] = 1.5/h + sn\n                else:\n                    J[n, n] = 1.5/h + h_prime_n\n                    is_exact_update = True\n            \n            if is_exact_update:\n                evals_partial += 1\n\n            u_prev = np.copy(u_partial)\n            delta_u = np.linalg.solve(J, -R)\n            \n            lambda_ls = 1.0\n            u_prev_ls = np.copy(u_partial)\n            for _ in range(10):\n                u_new = u_prev_ls + lambda_ls * delta_u\n                if np.linalg.norm(calculate_residual(u_new, n, h, alpha, beta)) < norm_R:\n                    break\n                lambda_ls /= 2.0\n            u_partial = u_new\n            it_partial += 1\n\n        norm_partial = np.linalg.norm(calculate_residual(u_partial, n, h, alpha, beta))\n\n        return [it_full, it_partial, evals_full, evals_partial, norm_full, norm_partial]\n\n    test_cases = [\n        (64, 1, 5, 5),      # Test case 1\n        (64, 0.5, 50, 5),   # Test case 2\n        (32, 1, 0, 5),      # Test case 3\n    ]\n\n    final_results = []\n    for case in test_cases:\n        case_results = run_solvers_for_case(case)\n        final_results.extend(case_results)\n\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```"
        }
    ]
}