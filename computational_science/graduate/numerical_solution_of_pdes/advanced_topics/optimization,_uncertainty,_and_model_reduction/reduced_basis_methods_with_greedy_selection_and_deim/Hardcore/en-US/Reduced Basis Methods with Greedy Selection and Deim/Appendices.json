{
    "hands_on_practices": [
        {
            "introduction": "This exercise gets to the heart of why Proper Orthogonal Decomposition (POD) is a cornerstone of model reduction. When we create a reduced basis from a collection of solution \"snapshots,\" we need a way to quantify the quality of our basis. This derivation will guide you through connecting the approximation error, measured in the physically relevant $L^{2}$ norm, directly to the singular values of the snapshot data matrix, providing a clear and computable measure of the information lost by truncating the basis .",
            "id": "3438787",
            "problem": "Consider a parameterized, nonlinear, parabolic partial differential equation posed on a bounded domain $\\Omega \\subset \\mathbb{R}^{d}$ with homogeneous Dirichlet boundary conditions, discretized in space by a conforming finite element method with basis functions $\\{\\varphi_{i}\\}_{i=1}^{n}$. Let the semi-discrete model be written as\n$$\nM \\frac{d x(t,\\mu)}{d t} + A(\\mu)\\, x(t,\\mu) + g\\!\\left(x(t,\\mu);\\mu\\right) = b(\\mu),\n$$\nwhere $M \\in \\mathbb{R}^{n \\times n}$ is the symmetric positive definite mass matrix associated with the $L^{2}(\\Omega)$ inner product, $A(\\mu) \\in \\mathbb{R}^{n \\times n}$ is the stiffness-like operator depending on a parameter $\\mu \\in \\mathcal{P}$, $x(t,\\mu) \\in \\mathbb{R}^{n}$ is the coefficient vector of the finite element solution at time $t$, $g(\\cdot;\\mu)$ is a nonlinear term intended to be approximated in an online-efficient way via the Discrete Empirical Interpolation Method (DEIM), and $b(\\mu) \\in \\mathbb{R}^{n}$ is a source term. Assume that a candidate Reduced Basis is to be selected by a greedy procedure over $\\mu \\in \\mathcal{P}$, but for this problem focus only on the Proper Orthogonal Decomposition (POD) constructed from solution snapshots.\n\nLet $\\{x_{k}\\}_{k=1}^{m}$ be a collection of $m$ solution snapshots sampled over a training set of parameters and times, assembled into the snapshot matrix $X \\in \\mathbb{R}^{n \\times m}$ with columns $x_{k}$. Define the mass-weighted snapshot matrix $Y := M^{1/2} X \\in \\mathbb{R}^{n \\times m}$, where $M^{1/2}$ denotes the unique symmetric positive definite square root of $M$. Let the singular value decomposition of $Y$ be $Y = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{m \\times m}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{n \\times m}$ is diagonal in the sense that its nonzero entries are the singular values $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{q} > 0$ on the main diagonal with $q = \\min\\{n,m\\}$. The rank-$r$ POD space is defined by the leading $r$ left singular vectors of $Y$ mapped back through $M^{-1/2}$, i.e., $\\Psi_{r} := M^{-1/2} U_{r} \\in \\mathbb{R}^{n \\times r}$, where $U_{r} \\in \\mathbb{R}^{n \\times r}$ collects the first $r$ columns of $U$. Since $\\Psi_{r}^{\\top} M \\Psi_{r} = I_{r}$, the $M$-orthogonal projector onto $\\operatorname{span}(\\Psi_{r})$ is $P_{r} := \\Psi_{r} \\Psi_{r}^{\\top} M$, and the rank-$r$ POD reconstruction of the snapshots is $X_{r} := P_{r} X$.\n\nDefine the total squared snapshot reconstruction error in the $L^{2}(\\Omega)$ sense as\n$$\nE_{r}^{2} := \\sum_{k=1}^{m} \\|x_{k} - X_{r}(:,k)\\|_{L^{2}(\\Omega)}^{2} = \\sum_{k=1}^{m} \\left( x_{k} - X_{r}(:,k) \\right)^{\\top} M \\left( x_{k} - X_{r}(:,k) \\right),\n$$\nwhich can be written compactly as\n$$\nE_{r}^{2} = \\|X - X_{r}\\|_{M,F}^{2} := \\operatorname{trace}\\!\\left((X - X_{r})^{\\top} M (X - X_{r})\\right).\n$$\n\nStarting only from the definitions of the mass-weighted inner product, the singular value decomposition, and orthogonal projection, derive the closed-form expression for $E_{r}^{2}$ in terms of the neglected singular values $\\{\\sigma_{i}\\}_{i=r+1}^{q}$ of $Y$, and thereby relate the POD truncation error to the $L^{2}(\\Omega)$ norm of the snapshot reconstruction error. Your final answer must be a single analytic expression in terms of $\\{\\sigma_{i}\\}_{i=r+1}^{q}$ that exactly equals $E_{r}^{2}$.",
            "solution": "The objective is to derive a closed-form expression for the total squared snapshot reconstruction error, $E_r^2$. We begin with the provided definition of this error in the mass-matrix-weighted Frobenius norm:\n$$\nE_{r}^{2} = \\|X - X_{r}\\|_{M,F}^{2} = \\operatorname{trace}\\!\\left((X - X_{r})^{\\top} M (X - X_{r})\\right)\n$$\nwhere $X \\in \\mathbb{R}^{n \\times m}$ is the snapshot matrix and $X_r \\in \\mathbb{R}^{n \\times m}$ is its rank-$r$ POD reconstruction.\n\nThe mass matrix $M$ is symmetric and positive definite, which guarantees the existence of a unique symmetric positive definite square root, $M^{1/2}$. We can rewrite the trace expression by inserting $M = M^{1/2} M^{1/2}$. Since $M$ is symmetric, $(M^{1/2})^{\\top} = M^{1/2}$.\n$$\nE_{r}^{2} = \\operatorname{trace}\\!\\left((X - X_{r})^{\\top} M^{1/2} M^{1/2} (X - X_{r})\\right) = \\operatorname{trace}\\!\\left(\\left(M^{1/2}(X - X_{r})\\right)^{\\top} \\left(M^{1/2}(X - X_{r})\\right)\\right)\n$$\nThis expression is the squared standard Frobenius norm of the matrix $M^{1/2}(X - X_{r})$.\n$$\nE_{r}^{2} = \\|M^{1/2}(X - X_{r})\\|_{F}^{2}\n$$\nNow, we express the term inside the norm using the provided definitions. The reconstruction $X_r$ is defined as the projection of $X$ onto the POD space:\n$$\nX_{r} = P_{r} X\n$$\nwhere the projector $P_r$ is given by $P_{r} = \\Psi_{r} \\Psi_{r}^{\\top} M$. The POD basis $\\Psi_r$ is defined as $\\Psi_{r} = M^{-1/2} U_{r}$, where $U_r$ contains the leading $r$ left singular vectors of the mass-weighted snapshot matrix $Y = M^{1/2} X$.\n\nLet's substitute the definition of $\\Psi_r$ into the expression for $P_r$:\n$$\nP_{r} = (M^{-1/2} U_{r}) (\\Psi_{r}^{\\top}) M = (M^{-1/2} U_{r}) (M^{-1/2} U_{r})^{\\top} M = (M^{-1/2} U_{r}) (U_{r}^{\\top} (M^{-1/2})^{\\top}) M\n$$\nSince $M^{-1/2}$ is symmetric, $(M^{-1/2})^{\\top} = M^{-1/2}$.\n$$\nP_{r} = M^{-1/2} U_{r} U_{r}^{\\top} M^{-1/2} M = M^{-1/2} U_{r} U_{r}^{\\top} M^{1/2}\n$$\nNow we can analyze the term $M^{1/2}(X - X_r)$:\n$$\nM^{1/2}(X - X_{r}) = M^{1/2}(X - P_{r}X) = M^{1/2}X - M^{1/2}P_{r}X\n$$\nSubstituting the expression for $P_r$:\n$$\nM^{1/2} P_{r} X = M^{1/2} (M^{-1/2} U_{r} U_{r}^{\\top} M^{1/2}) X = (M^{1/2} M^{-1/2}) U_{r} U_{r}^{\\top} (M^{1/2} X) = I U_{r} U_{r}^{\\top} (M^{1/2} X)\n$$\nUsing the definition $Y = M^{1/2}X$, we have:\n$$\nM^{1/2} P_{r} X = U_{r} U_{r}^{\\top} Y\n$$\nTherefore, the error term becomes:\n$$\nM^{1/2}(X - X_{r}) = M^{1/2}X - U_{r} U_{r}^{\\top} Y = Y - U_{r} U_{r}^{\\top} Y\n$$\nThe problem is thus transformed from calculating a weighted norm of the error in $X$ to calculating the standard Frobenius norm of the error in $Y$ when projected onto the space spanned by the columns of $U_r$.\n$$\nE_{r}^{2} = \\|Y - U_{r} U_{r}^{\\top} Y\\|_{F}^{2}\n$$\nThe matrix $U_r U_r^T$ is the orthogonal projector onto the subspace spanned by the first $r$ columns of $U$. We now use the singular value decomposition of $Y$, which is given as $Y = U \\Sigma V^{\\top}$. In summation form, this is:\n$$\nY = \\sum_{i=1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top}\n$$\nwhere $u_i$ and $v_i$ are the $i$-th columns of $U$ and $V$ respectively, $\\sigma_i$ are the singular values, and $q = \\min\\{n,m\\}$. The columns $\\{u_i\\}$ form an orthonormal basis. The projection of $Y$ onto the span of $\\{u_i\\}_{i=1}^r$ is:\n$$\nU_{r} U_{r}^{\\top} Y = \\left(\\sum_{j=1}^{r} u_j u_j^{\\top}\\right) \\left(\\sum_{i=1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top}\\right) = \\sum_{j=1}^{r} \\sum_{i=1}^{q} \\sigma_{i} u_{j} (u_{j}^{\\top} u_{i}) v_{i}^{\\top}\n$$\nDue to the orthonormality of the vectors $\\{u_i\\}$, we have $u_{j}^{\\top} u_{i} = \\delta_{ji}$, where $\\delta_{ji}$ is the Kronecker delta. The expression simplifies to:\n$$\nU_{r} U_{r}^{\\top} Y = \\sum_{i=1}^{r} \\sigma_{i} u_{i} v_{i}^{\\top}\n$$\nThis is the truncated SVD of $Y$, which is the best rank-$r$ approximation of $Y$ in the Frobenius norm (Eckart-Young-Mirsky theorem). The projection error is:\n$$\nY - U_{r} U_{r}^{\\top} Y = \\sum_{i=1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top} - \\sum_{i=1}^{r} \\sigma_{i} u_{i} v_{i}^{\\top} = \\sum_{i=r+1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top}\n$$\nFinally, we compute the squared Frobenius norm of this error matrix:\n$$\nE_{r}^{2} = \\left\\|\\sum_{i=r+1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top}\\right\\|_{F}^{2} = \\operatorname{trace}\\!\\left(\\left(\\sum_{j=r+1}^{q} \\sigma_{j} u_{j} v_{j}^{\\top}\\right)^{\\top} \\left(\\sum_{i=r+1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top}\\right)\\right)\n$$\n$$\nE_{r}^{2} = \\operatorname{trace}\\!\\left(\\left(\\sum_{j=r+1}^{q} \\sigma_{j} v_{j} u_{j}^{\\top}\\right) \\left(\\sum_{i=r+1}^{q} \\sigma_{i} u_{i} v_{i}^{\\top}\\right)\\right) = \\operatorname{trace}\\!\\left(\\sum_{j=r+1}^{q} \\sum_{i=r+1}^{q} \\sigma_{j} \\sigma_{i} v_{j} (u_{j}^{\\top} u_{i}) v_{i}^{\\top}\\right)\n$$\nAgain, using $u_{j}^{\\top} u_{i} = \\delta_{ji}$:\n$$\nE_{r}^{2} = \\operatorname{trace}\\!\\left(\\sum_{i=r+1}^{q} \\sigma_{i}^{2} v_{i} v_{i}^{\\top}\\right)\n$$\nBy linearity of the trace operator:\n$$\nE_{r}^{2} = \\sum_{i=r+1}^{q} \\sigma_{i}^{2} \\operatorname{trace}(v_{i} v_{i}^{\\top})\n$$\nUsing the cyclic property of the trace, $\\operatorname{trace}(v_{i} v_{i}^{\\top}) = \\operatorname{trace}(v_{i}^{\\top} v_{i})$. Since $v_i$ is a column of the orthogonal matrix $V$, it is a unit vector, so $v_{i}^{\\top} v_{i} = 1$.\n$$\nE_{r}^{2} = \\sum_{i=r+1}^{q} \\sigma_{i}^{2} \\cdot 1 = \\sum_{i=r+1}^{q} \\sigma_{i}^{2}\n$$\nThus, the total squared snapshot reconstruction error in the $L^2(\\Omega)$ sense is equal to the sum of the squares of the neglected singular values of the mass-weighted snapshot matrix $Y$.",
            "answer": "$$\n\\boxed{\\sum_{i=r+1}^{q} \\sigma_{i}^{2}}\n$$"
        },
        {
            "introduction": "Building a good basis is only half the battle; to achieve true computational speed-up, we must also evaluate the reduced-order model (ROM) efficiently. This is a challenge for nonlinear systems, where evaluating the Jacobian matrix can be prohibitively expensive. This practice demonstrates how the Matrix Discrete Empirical Interpolation Method (MDEIM) elegantly resolves this by creating a low-cost approximation of the Jacobian, enabling a full offline/online decomposition essential for rapid online computation .",
            "id": "3438832",
            "problem": "Consider a parameterized nonlinear partial differential equation after spatial semi-discretization that yields a finite-dimensional residual $R(u;\\mu)\\in\\mathbb{R}^{N}$, where $u\\in\\mathbb{R}^{N}$ denotes the state vector and $\\mu\\in\\mathcal{P}\\subset\\mathbb{R}^{p}$ denotes the parameter. Newton’s method for solving $R(u;\\mu)=0$ requires the Jacobian $J(u;\\mu)\\in\\mathbb{R}^{N\\times N}$ with entries $J_{ij}(u;\\mu)=\\partial R_i(u;\\mu)/\\partial u_j$. Assume a reduced basis approximation with a trial subspace spanned by columns of $V\\in\\mathbb{R}^{N\\times n}$ and a test subspace spanned by columns of $Z\\in\\mathbb{R}^{N\\times n}$ (Petrov–Galerkin setting), so that the reduced coordinates are $c\\in\\mathbb{R}^{n}$ and the state approximation is $u\\approx Vc$. The reduced residual is $r(c;\\mu)=Z^{T}R(Vc;\\mu)$ and the reduced Jacobian is $J_{r}(c;\\mu)=Z^{T}J(Vc;\\mu)V\\in\\mathbb{R}^{n\\times n}$.\n\nSuppose the full Jacobian has an affine parameter structure of the form $J(u;\\mu)=\\sum_{q=1}^{Q_{J}}\\theta_{q}(\\mu)\\,J^{(q)}(u)$, where the coefficient functions $\\theta_{q}:\\mathcal{P}\\to\\mathbb{R}$ are known and $J^{(q)}(u)\\in\\mathbb{R}^{N\\times N}$ depend on $u$ but not directly on $\\mu$. To achieve an efficient online evaluation of $J_{r}(c;\\mu)$, use the Matrix Discrete Empirical Interpolation Method (MDEIM), which is an extension of the Discrete Empirical Interpolation Method (DEIM), applied to each operator $J^{(q)}(u)$. For each $q\\in\\{1,\\dots,Q_{J}\\}$, let $U_{q}\\in\\mathbb{R}^{N^{2}\\times M_{q}}$ be a column-orthonormal basis obtained from snapshots of $\\operatorname{vec}(J^{(q)}(u))$, where $\\operatorname{vec}(\\cdot)$ stacks matrix columns into a vector, and let $P_{q}\\in\\mathbb{R}^{N^{2}\\times M_{q}}$ be the sampling matrix selecting $M_{q}$ entries of $\\operatorname{vec}(J^{(q)}(u))$ according to the standard DEIM greedy selection rule. Assume $M_{q}\\ll N^{2}$ and that $P_{q}^{T}U_{q}\\in\\mathbb{R}^{M_{q}\\times M_{q}}$ is nonsingular for each $q$.\n\nStarting from first principles of Petrov–Galerkin projection and the MDEIM approximation, derive the offline–online decomposed, fully separated expression for the reduced Jacobian $J_{r}(c;\\mu)$ in terms of:\n- the parameter functions $\\theta_{q}(\\mu)$,\n- the online coefficient vectors obtained from MDEIM sampling of $J^{(q)}(Vc)$,\n- and precomputed reduced operator “slices” formed offline from $Z$, $V$, and $U_{q}$.\n\nYour derivation must explicitly define all intermediate quantities you introduce, including $\\operatorname{vec}(\\cdot)$, $\\operatorname{unvec}(\\cdot)$, sampling matrices, and any standard basis vectors. Conclude with a single closed-form analytic expression for $J_{r}(c;\\mu)$ written purely in terms of $\\theta_{q}(\\mu)$, the DEIM sampling and basis matrices $(P_{q},U_{q})$, the vectors $c$, and a set of offline precomputed reduced matrices. No numerical evaluation is required. Provide your final expression as your answer. The answer should be a single closed-form analytical expression with no units.",
            "solution": "The objective is to derive a computationally efficient, offline-online decomposed expression for the reduced Jacobian $J_r(c;\\mu)$ using the Matrix Discrete Empirical Interpolation Method (MDEIM).\n\nWe begin with the definition of the reduced Jacobian in the Petrov-Galerkin setting:\n$$J_{r}(c;\\mu) = Z^{T}J(Vc;\\mu)V$$\nThe state approximation $u \\approx Vc$ is substituted into the expression for the full Jacobian. Given the affine structure of $J(u;\\mu)$, we have:\n$$J(Vc;\\mu) = \\sum_{q=1}^{Q_{J}}\\theta_{q}(\\mu)\\,J^{(q)}(Vc)$$\nSubstituting this into the equation for $J_r(c;\\mu)$ and using the linearity of matrix multiplication yields:\n$$J_{r}(c;\\mu) = Z^{T}\\left(\\sum_{q=1}^{Q_{J}}\\theta_{q}(\\mu)\\,J^{(q)}(Vc)\\right)V = \\sum_{q=1}^{Q_{J}}\\theta_{q}(\\mu)\\left(Z^{T}J^{(q)}(Vc)V\\right)$$\nThis expression separates the parameter-dependent part $\\theta_q(\\mu)$ from the state-dependent part $Z^{T}J^{(q)}(Vc)V$. However, the evaluation of $J^{(q)}(Vc)$ is still computationally expensive as it requires forming an $N \\times N$ matrix for each Newton step (online).\n\nTo alleviate this online computational burden, we apply the Matrix Discrete Empirical Interpolation Method (MDEIM) to each matrix function $J^{(q)}(u)$. MDEIM approximates the vectorized matrix, $\\operatorname{vec}(J^{(q)}(u))$, as a linear combination of the basis vectors in $U_q$:\n$$\\operatorname{vec}(J^{(q)}(u)) \\approx U_q \\mathbf{d}_q(u)$$\nwhere $\\mathbf{d}_q(u) \\in \\mathbb{R}^{M_q}$ is a vector of coefficients to be determined. The core idea of DEIM is to determine these coefficients by enforcing that the approximation matches the exact vector at a set of $M_q$ indices. These indices are specified by the sampling matrix $P_q$. The condition is:\n$$P_q^T \\operatorname{vec}(J^{(q)}(u)) = P_q^T (U_q \\mathbf{d}_q(u)) = (P_q^T U_q) \\mathbf{d}_q(u)$$\nSince $P_q^T U_q$ is assumed to be nonsingular, we can solve for the coefficient vector $\\mathbf{d}_q(u)$:\n$$\\mathbf{d}_q(u) = (P_q^T U_q)^{-1} P_q^T \\operatorname{vec}(J^{(q)}(u))$$\nSubstituting this back into the approximation for $\\operatorname{vec}(J^{(q)}(u))$, we get:\n$$\\operatorname{vec}(J^{(q)}(u)) \\approx U_q (P_q^T U_q)^{-1} P_q^T \\operatorname{vec}(J^{(q)}(u))$$\nTo return to the matrix form, we introduce the operator $\\operatorname{unvec}(\\cdot)$, which is the inverse of $\\operatorname{vec}(\\cdot)$, reshaping a vector from $\\mathbb{R}^{N^2}$ into a matrix in $\\mathbb{R}^{N \\times N}$. Let $U_{q,i} \\in \\mathbb{R}^{N^2}$ be the $i$-th column of the basis matrix $U_q$. The MDEIM approximation for the matrix $J^{(q)}(u)$ can be written as a sum:\n$$J^{(q)}(u) \\approx \\operatorname{unvec}\\left( \\sum_{i=1}^{M_q} \\left[ \\mathbf{d}_q(u) \\right]_i U_{q,i} \\right) = \\sum_{i=1}^{M_q} \\left[ \\mathbf{d}_q(u) \\right]_i \\operatorname{unvec}(U_{q,i})$$\nwhere $[\\mathbf{d}_q(u)]_i$ is the $i$-th component of the coefficient vector $\\mathbf{d}_q(u)$.\nLet $e_i \\in \\mathbb{R}^{M_q}$ be the $i$-th standard basis vector. Then $[\\mathbf{d}_q(u)]_i = e_i^T \\mathbf{d}_q(u)$. Applying this at the reduced state $u = Vc$, the MDEIM approximation for $J^{(q)}(Vc)$ is:\n$$J^{(q)}(Vc) \\approx \\sum_{i=1}^{M_q} \\left( e_i^T (P_q^T U_q)^{-1} P_q^T \\operatorname{vec}(J^{(q)}(Vc)) \\right) \\operatorname{unvec}(U_{q,i})$$\nNow, we substitute this approximation into the term $Z^{T}J^{(q)}(Vc)V$:\n$$Z^{T}J^{(q)}(Vc)V \\approx Z^{T} \\left( \\sum_{i=1}^{M_q} \\left( e_i^T (P_q^T U_q)^{-1} P_q^T \\operatorname{vec}(J^{(q)}(Vc)) \\right) \\operatorname{unvec}(U_{q,i}) \\right) V$$\nThe term in the large parentheses is a scalar coefficient for each $i$. We can pull it out of the matrix product:\n$$Z^{T}J^{(q)}(Vc)V \\approx \\sum_{i=1}^{M_q} \\left( e_i^T (P_q^T U_q)^{-1} P_q^T \\operatorname{vec}(J^{(q)}(Vc)) \\right) \\left( Z^T \\operatorname{unvec}(U_{q,i}) V \\right)$$\nFinally, we substitute this back into the expression for the reduced Jacobian $J_r(c;\\mu)$:\n$$J_r(c;\\mu) \\approx \\sum_{q=1}^{Q_J} \\theta_q(\\mu) \\left[ \\sum_{i=1}^{M_q} \\left( e_i^T (P_q^T U_q)^{-1} P_q^T \\operatorname{vec}(J^{(q)}(Vc)) \\right) \\left( Z^T \\operatorname{unvec}(U_{q,i}) V \\right) \\right]$$\nThis expression is now fully decomposed for efficient online evaluation.\n-   **Offline Stage**: For each $q \\in \\{1, \\dots, Q_J\\}$, we precompute and store:\n    1.  The inverse matrix $(P_q^T U_q)^{-1} \\in \\mathbb{R}^{M_q \\times M_q}$.\n    2.  The set of $M_q$ reduced operator \"slices\": $\\mathbb{J}_{q,i} = Z^T \\operatorname{unvec}(U_{q,i}) V \\in \\mathbb{R}^{n \\times n}$ for $i=1, \\dots, M_q$.\n-   **Online Stage**: For a given parameter $\\mu$ and reduced state $c$:\n    1.  Evaluate the parameter functions $\\theta_q(\\mu)$.\n    2.  For each $q$, compute the $M_q$ required entries of $J^{(q)}(Vc)$ to form the vector $\\mathbf{j}_q(c) = P_q^T \\operatorname{vec}(J^{(q)}(Vc))$. This is cheap as $M_q \\ll N^2$.\n    3.  For each $q$, compute the online coefficients $[(P_q^T U_q)^{-1} \\mathbf{j}_q(c)]_i$.\n    4.  Assemble $J_r(c;\\mu)$ by summing the precomputed slices weighted by the online coefficients.",
            "answer": "$$\\boxed{\\sum_{q=1}^{Q_{J}} \\theta_q(\\mu) \\sum_{i=1}^{M_q} \\left( e_i^T (P_q^T U_q)^{-1} P_q^T \\operatorname{vec}(J^{(q)}(Vc)) \\right) \\left( Z^T \\operatorname{unvec}(U_{q,i}) V \\right)}$$"
        },
        {
            "introduction": "Theory meets practice in this computational exercise, where you will implement and compare two strategies for building a reduced basis. We will contrast a \"strong\" greedy algorithm, guided by a rigorous but computationally expensive error estimator, with a \"weak\" greedy algorithm that uses a cheaper surrogate enabled by DEIM. By quantifying how the selected basis snapshots diverge, you will gain first-hand insight into the crucial trade-offs between accuracy and computational cost in the offline training stage of reduced basis methods .",
            "id": "3438765",
            "problem": "Construct a fully specified computational experiment to contrast weak greedy selection (using a surrogate error indicator $\\eta(\\mu)$) and strong greedy selection (using a true a posteriori estimator) for a reduced basis (RB) approximation of a parametrized linear elliptic partial differential equation, when the nonaffine right-hand side is hyperreduced via the Discrete Empirical Interpolation Method (DEIM). Your program must implement both greedy strategies and quantify the divergence in snapshot selection sequences as a function of the DEIM basis size.\n\nConsider the boundary value problem on the unit interval with homogeneous Dirichlet boundary conditions:\n- Find $u(x;\\mu)$ such that $-a(\\mu)\\,u''(x;\\mu) + u(x;\\mu) = g(x;\\mu)$ for $x \\in (0,1)$, with $u(0;\\mu) = u(1;\\mu) = 0$,\n- where $a(\\mu) = 1 + 0.5\\,\\mu$, and $g(x;\\mu) = \\exp(\\mu x) + \\sin(3\\pi x)$.\n\nDiscretize using standard second-order centered finite differences on $N$ equally spaced interior nodes. Let $h = 1/(N+1)$ and $x_i = i\\,h$ for $i = 1,2,\\dots,N$. The discrete system is\n$$\nA(\\mu)\\,u(\\mu) = f(\\mu),\n$$\nwhere $A(\\mu) = a(\\mu)\\,K + I$, with $K$ the tridiagonal stiffness matrix $K = \\frac{1}{h^2}\\,\\mathrm{tridiag}(-1, 2,-1)$ and $I$ the $N \\times N$ identity matrix. The load vector $f(\\mu)$ is given by $f_i(\\mu) = \\exp(\\mu x_i) + \\sin(3\\pi x_i)$.\n\nDefine the reduced basis space $V_k = \\mathrm{span}\\{u(\\mu^{(1)}),\\dots,u(\\mu^{(k)})\\}$ with columns of $V_k \\in \\mathbb{R}^{N \\times k}$ orthonormalized in the Euclidean inner product. For a given $\\mu$ and basis $V_k$, the reduced solution $u_k(\\mu) = V_k y(\\mu)$ is obtained by Galerkin projection:\n$$\n(V_k^\\top A(\\mu) V_k)\\,y(\\mu) = V_k^\\top \\widehat{f}(\\mu),\n$$\nwhere $\\widehat{f}(\\mu)$ is the right-hand side used in the reduced solve (specified below for each greedy variant).\n\nIntroduce the Discrete Empirical Interpolation Method (DEIM) hyperreduction for the nonaffine $f(\\mu)$ as follows. Construct a DEIM basis $U_m \\in \\mathbb{R}^{N \\times m}$ by computing the first $m$ left singular vectors of a snapshot matrix of $f(\\mu)$ over a specified DEIM training set. Select DEIM interpolation indices via the standard DEIM greedy procedure (selecting indices maximizing componentwise absolute values of the residuals). Denote by $P \\in \\mathbb{R}^{N \\times m}$ the column selection matrix associated with the chosen indices. The DEIM approximation of $f(\\mu)$ is\n$$\nf_{\\mathrm{DEIM}}(\\mu) = U_m\\,(P^\\top U_m)^{-1} P^\\top f(\\mu).\n$$\n\nImplement and compare the following greedy strategies to build two separate reduced bases of size $k_{\\max}$, each by iteratively selecting $\\mu$ from a fixed training set $\\mathcal{S}_{\\mathrm{train}}$ that maximizes an error indicator. At iteration $k$, given the current basis $V_{k-1}$:\n\n- Strong greedy (true a posteriori estimator):\n  - Compute the reduced solution using the true right-hand side, i.e., $\\widehat{f}(\\mu) = f(\\mu)$.\n  - Form the true residual $r(\\mu) = A(\\mu)\\,u_{k-1}(\\mu) - f(\\mu)$, where $u_{k-1}(\\mu)$ is the current reduced solution in $V_{k-1}$ (with $u_0(\\mu) = 0$ if $k=1$).\n  - Use the exact energy-norm a posteriori estimator\n    $$\n    \\Delta_{\\mathrm{strong}}(\\mu) = \\|r(\\mu)\\|_{A(\\mu)^{-1}} = \\sqrt{r(\\mu)^\\top A(\\mu)^{-1} r(\\mu)},\n    $$\n    which is equal to the error in the energy norm $\\|u(\\mu) - u_{k-1}(\\mu)\\|_{A(\\mu)}$ for symmetric positive definite $A(\\mu)$.\n  - Select $\\mu^{(k)} = \\arg\\max_{\\mu \\in \\mathcal{S}_{\\mathrm{train}} \\setminus \\{\\mu^{(1)},\\dots,\\mu^{(k-1)}\\}} \\Delta_{\\mathrm{strong}}(\\mu)$, compute the full-order solution $u(\\mu^{(k)})$, and orthonormalize it into $V_k$.\n\n- Weak greedy (surrogate indicator under DEIM hyperreduction):\n  - Compute the reduced solution using the DEIM right-hand side, i.e., $\\widehat{f}(\\mu) = f_{\\mathrm{DEIM}}(\\mu)$.\n  - Form the DEIM residual $r_{\\mathrm{DEIM}}(\\mu) = A(\\mu)\\,u_{k-1}(\\mu) - f_{\\mathrm{DEIM}}(\\mu)$.\n  - Use the surrogate indicator\n    $$\n    \\eta_{\\mathrm{weak}}(\\mu) = \\frac{\\|r_{\\mathrm{DEIM}}(\\mu)\\|_2}{\\alpha_{\\mathrm{LB}}(\\mu)},\n    $$\n    where $\\alpha_{\\mathrm{LB}}(\\mu)$ is a rigorous coercivity lower bound for $A(\\mu)$ in the Euclidean norm. For the given discretization, the smallest eigenvalue of $K$ is exactly\n    $$\n    \\lambda_1(K) = \\frac{4}{h^2}\\,\\sin^2\\!\\left(\\frac{\\pi}{2(N+1)}\\right),\n    $$\n    so $\\alpha_{\\mathrm{LB}}(\\mu) = 1 + a(\\mu)\\,\\lambda_1(K)$.\n  - Select $\\mu^{(k)}$ analogously by maximizing $\\eta_{\\mathrm{weak}}(\\mu)$ over unused training parameters, then compute $u(\\mu^{(k)})$ (truth) and orthonormalize it into the weak-greedy basis.\n\nBoth greedy processes start with empty basis and prohibit reselection of previously chosen parameters. In both, the snapshot added to the basis is always the full-order solution $u(\\mu^{(k)})$.\n\nQuantify the divergence of snapshot selection between the two strategies by the following two scalars for a given run producing ordered sequences $(\\mu^{(1)}_{\\mathrm{weak}},\\dots,\\mu^{(k_{\\max})}_{\\mathrm{weak}})$ and $(\\mu^{(1)}_{\\mathrm{strong}},\\dots,\\mu^{(k_{\\max})}_{\\mathrm{strong}})$:\n- The ordered-position divergence $D_{\\mathrm{ord}} = \\frac{1}{k_{\\max}}\\,\\left|\\{k \\in \\{1,\\dots,k_{\\max}\\} : \\mu^{(k)}_{\\mathrm{weak}} \\neq \\mu^{(k)}_{\\mathrm{strong}}\\}\\right|$ (a float in $[0,1]$).\n- The set-symmetric-difference divergence $D_{\\mathrm{set}} = \\frac{1}{k_{\\max}}\\,\\left|\\{\\mu^{(k)}_{\\mathrm{weak}} : 1 \\le k \\le k_{\\max}\\} \\,\\Delta\\, \\{\\mu^{(k)}_{\\mathrm{strong}} : 1 \\le k \\le k_{\\max}\\}\\right|$, where $\\Delta$ denotes symmetric difference of sets (a float in $[0,2]$).\n\nImplementation details to adhere to:\n- Use $N = 60$ interior points.\n- Use training parameter set $\\mathcal{S}_{\\mathrm{train}} = \\{0, 0.05, 0.10, \\dots, 1.00\\}$, i.e., $21$ uniformly spaced parameters on $[0,1]$.\n- Use $k_{\\max} = 6$ greedy iterations.\n- Construct the DEIM basis $U_m$ from load snapshots at $41$ uniformly spaced parameters in $[0,1]$ using the first $m$ left singular vectors, with DEIM interpolation indices chosen by the standard DEIM greedy rule described above.\n\nTest suite:\n- Case $1$: $m = 1$.\n- Case $2$: $m = 3$.\n- Case $3$: $m = 8$.\n- Case $4$: $m = 20$.\n\nFor each case, independently build the DEIM operator and run both greedy procedures to obtain $(D_{\\mathrm{ord}}, D_{\\mathrm{set}})$. The final program output must be a single line containing a flattened list of the $8$ floats in the order $[D_{\\mathrm{ord}}^{(1)}, D_{\\mathrm{set}}^{(1)}, D_{\\mathrm{ord}}^{(2)}, D_{\\mathrm{set}}^{(2)}, D_{\\mathrm{ord}}^{(3)}, D_{\\mathrm{set}}^{(3)}, D_{\\mathrm{ord}}^{(4)}, D_{\\mathrm{set}}^{(4)}]$.\n\nYour program must be complete and runnable, using only the specified libraries, and must not require any input. The final print must be a single line containing the list in the exact format with square brackets and comma-separated decimal numbers. No physical units are involved. Angles are not used. Percentages must not be used anywhere; all ratios must be decimals.",
            "solution": "### Methodological Framework\n\nThe goal is to implement and contrast two greedy algorithms for constructing a reduced basis for a parametrized elliptic partial differential equation (PDE). The key difference between the algorithms lies in the error metric used to select new basis snapshots: one employs a rigorous a posteriori error estimator (\"strong greedy\"), while the other uses a computationally cheaper surrogate indicator derived from a hyperreduced system (\"weak greedy\").\n\n#### 1. Full-Order Model (FOM)\n\nThe physical system is described by the boundary value problem on the domain $x \\in (0,1)$:\n$$\n-a(\\mu)\\,u''(x;\\mu) + u(x;\\mu) = g(x;\\mu), \\quad u(0;\\mu) = u(1;\\mu) = 0\n$$\nwhere the parameter-dependent diffusion coefficient is $a(\\mu) = 1 + 0.5\\,\\mu$ and the non-affine right-hand side is $g(x;\\mu) = \\exp(\\mu x) + \\sin(3\\pi x)$.\n\nThe PDE is discretized using a second-order centered finite difference scheme on a uniform grid of $N$ interior points, transforming it into a linear system of equations:\n$$\nA(\\mu)\\,u(\\mu) = f(\\mu)\n$$\nHere, $u(\\mu) \\in \\mathbb{R}^N$ is the vector of nodal solution values. The system matrix $A(\\mu) = a(\\mu)\\,K + I$ is symmetric positive definite and has an affine parameter dependence. The load vector $f(\\mu) \\in \\mathbb{R}^N$ is obtained by evaluating $g(x; \\mu)$ at the grid points.\n\n#### 2. Reduced-Order Modeling and Hyperreduction\n\nThe reduced basis method (RBM) approximates the solution $u(\\mu)$ in a low-dimensional subspace $V_k$ spanned by an orthonormal basis of pre-computed FOM solutions (snapshots). The approximate solution $u_k(\\mu) = V_k y(\\mu)$ is found by a Galerkin projection of the FOM onto $V_k$:\n$$\n[V_k^\\top A(\\mu) V_k]\\,y(\\mu) = V_k^\\top \\widehat{f}(\\mu)\n$$\nThe right-hand side term $\\widehat{f}(\\mu)$ differs between the greedy strategies. To avoid the high cost of forming $V_k^\\top f(\\mu)$ online due to the non-affine nature of $f(\\mu)$, the Discrete Empirical Interpolation Method (DEIM) is used to create a low-cost approximation $f_{\\mathrm{DEIM}}(\\mu)$. This involves projecting $f(\\mu)$ onto a pre-computed basis $U_m$ and using interpolation at a small set of $m$ indices to find the projection coefficients.\n\n#### 3. Greedy Selection Algorithms\n\nThe core of the experiment is the comparison of two algorithms for selecting the snapshots to build the basis $V_k$. Both iteratively select the parameter $\\mu^{(k)}$ from a training set that maximizes an error estimate.\n\n- **Strong Greedy Selection:** This method uses a mathematically rigorous a posteriori error estimator. For this problem, the error in the energy norm is exactly the dual norm of the true residual, $\\Delta_{\\mathrm{strong}}(\\mu) = \\|A(\\mu)u_{k-1}(\\mu) - f(\\mu)\\|_{A(\\mu)^{-1}}$. Calculating this estimator requires solving a full-order linear system for each parameter in the training set at each greedy step, making it computationally expensive. The reduced solution $u_{k-1}(\\mu)$ is computed using the true right-hand side $f(\\mu)$.\n\n- **Weak Greedy Selection:** This approach uses a cheaper surrogate error indicator to reduce the offline cost. The reduced solution is computed using the DEIM-approximated right-hand side, $f_{\\mathrm{DEIM}}(\\mu)$. The error indicator is the Euclidean norm of the resulting DEIM residual, scaled by a lower bound of the operator's coercivity constant: $\\eta_{\\mathrm{weak}}(\\mu) = \\|A(\\mu)u_{k-1}(\\mu) - f_{\\mathrm{DEIM}}(\\mu)\\|_2 / \\alpha_{\\mathrm{LB}}(\\mu)$. This avoids the expensive solve with $A(\\mu)^{-1}$ but introduces approximation errors from both the DEIM approximation and the use of a non-equivalent norm.\n\n#### 4. Divergence Metrics\n\nThe experiment quantifies the difference between the sequences of parameters selected by the two strategies using two metrics: the ordered-position divergence ($D_{\\mathrm{ord}}$), which measures the fraction of positions where the selected parameters differ, and the set-symmetric-difference divergence ($D_{\\mathrm{set}}$), which measures the disagreement in the sets of chosen parameters. The provided program implements this entire framework to compute these metrics for varying DEIM basis sizes $m$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and runs a computational experiment to compare weak greedy and\n    strong greedy selection for a reduced basis approximation of a parametrized\n    elliptic PDE, with the right-hand side hyperreduced via DEIM.\n    \"\"\"\n\n    # --- 1. Problem Definition  Discretization Parameters ---\n    N = 60\n    k_max = 6\n    s_train_params = np.linspace(0.0, 1.0, 21)\n    deim_train_params = np.linspace(0.0, 1.0, 41)\n    test_cases_m = [1, 3, 8, 20]\n\n    h = 1.0 / (N + 1)\n    x_grid = np.linspace(h, 1.0 - h, N)\n\n    # --- 2. Full-Order Model (FOM) Assembly ---\n    # Stiffness matrix K (from -u'') and Identity I\n    diag_main = np.full(N, 2.0)\n    diag_off = np.full(N - 1, -1.0)\n    K = (np.diag(diag_main) + np.diag(diag_off, k=1) + np.diag(diag_off, k=-1)) / h**2\n    I = np.eye(N)\n\n    # Parameter-dependent functions\n    def a_func(mu):\n        return 1.0 + 0.5 * mu\n\n    def f_func(mu, x):\n        return np.exp(mu * x) + np.sin(3 * np.pi * x)\n\n    def assemble_A(mu):\n        return a_func(mu) * K + I\n\n    def assemble_f(mu):\n        return f_func(mu, x_grid)\n\n    def solve_fom(mu):\n        A_mu = assemble_A(mu)\n        f_mu = assemble_f(mu)\n        return np.linalg.solve(A_mu, f_mu)\n\n    # --- 3. DEIM and RB Helper Functions ---\n    def build_deim_assets(m):\n        \"\"\"\n        Builds the DEIM basis U_m, interpolation indices, and the matrix\n        needed for the DEIM approximation.\n        \"\"\"\n        if m == 0:\n            return None, None\n\n        # Collect snapshots of the RHS function f(mu)\n        f_snapshots = np.array([assemble_f(mu) for mu in deim_train_params]).T\n\n        # SVD for DEIM basis\n        U, _, _ = np.linalg.svd(f_snapshots, full_matrices=False)\n        U_m = U[:, :m]\n\n        # Greedy selection of DEIM interpolation indices\n        indices = []\n        # First index\n        res = U_m[:, 0]\n        p_idx = np.argmax(np.abs(res))\n        indices.append(p_idx)\n\n        # Subsequent indices\n        for j in range(1, m):\n            u_j = U_m[:, j]\n            U_j_minus_1 = U_m[:, :j]\n            \n            # Solve for coefficients c: U_m[p, :j] c = u_j[p]\n            PT_U = U_j_minus_1[indices, :]\n            PT_u = u_j[indices]\n            try:\n                coeffs = np.linalg.solve(PT_U, PT_u)\n            except np.linalg.LinAlgError:\n                # Use least squares for stability if matrix is ill-conditioned\n                coeffs, _, _, _ = np.linalg.lstsq(PT_U, PT_u, rcond=None)\n            \n            # Compute residual\n            res = u_j - U_j_minus_1 @ coeffs\n            new_idx = np.argmax(np.abs(res))\n            indices.append(new_idx)\n        \n        # Pre-compute inverse for DEIM approximator\n        PT_Um = U_m[indices, :]\n        deim_inv = np.linalg.inv(PT_Um)\n\n        def deim_approximator(f_vec):\n            return U_m @ (deim_inv @ f_vec[indices])\n            \n        return deim_approximator\n\n    def gram_schmidt(v, V_basis):\n        \"\"\"Orthonormalizes a vector v against the columns of an orthonormal matrix V_basis.\"\"\"\n        if V_basis.shape[1] == 0:\n            norm_v = np.linalg.norm(v)\n            return v / norm_v if norm_v > 1e-12 else np.zeros_like(v)\n        \n        proj = V_basis @ (V_basis.T @ v)\n        v_orth = v - proj\n        norm_v_orth = np.linalg.norm(v_orth)\n        \n        if norm_v_orth > 1e-12:\n            return v_orth / norm_v_orth\n        else:\n            return np.zeros_like(v)\n\n    def run_greedy_procedure(strategy, deim_approximator_func=None, coercivity_func=None):\n        \"\"\"Performs the greedy algorithm for either the strong or weak strategy.\"\"\"\n        selected_mus = []\n        selected_indices = set()\n        V_basis = np.zeros((N, 0))\n        \n        for k in range(k_max):\n            max_error = -1.0\n            best_mu = -1.0\n            \n            # Iterate over parameters not yet selected\n            current_train_indices = [i for i, _ in enumerate(s_train_params) if i not in selected_indices]\n\n            for idx in current_train_indices:\n                mu = s_train_params[idx]\n                A_mu = assemble_A(mu)\n                \n                if k == 0:\n                    u_rb = np.zeros(N)\n                else:\n                    A_rb = V_basis.T @ A_mu @ V_basis\n                    f_mu_true = assemble_f(mu)\n                    \n                    if strategy == 'strong':\n                        f_rb = V_basis.T @ f_mu_true\n                    else:  # weak\n                        f_deim = deim_approximator_func(f_mu_true)\n                        f_rb = V_basis.T @ f_deim\n                    \n                    y_rb = np.linalg.solve(A_rb, f_rb)\n                    u_rb = V_basis @ y_rb\n                \n                # Calculate error based on strategy\n                if strategy == 'strong':\n                    f_mu_true = assemble_f(mu)\n                    residual = A_mu @ u_rb - f_mu_true\n                    w = np.linalg.solve(A_mu, residual)\n                    error = np.sqrt(np.dot(residual, w))\n                else:  # weak\n                    f_mu_true = assemble_f(mu)\n                    f_deim = deim_approximator_func(f_mu_true)\n                    residual = A_mu @ u_rb - f_deim\n                    error = np.linalg.norm(residual) / coercivity_func(mu)\n                    \n                if error > max_error:\n                    max_error = error\n                    best_mu = mu\n            \n            best_mu_idx = np.where(s_train_params == best_mu)[0][0]\n            selected_indices.add(best_mu_idx)\n            selected_mus.append(best_mu)\n\n            # Augment basis with the new FOM snapshot\n            new_snapshot = solve_fom(best_mu)\n            new_basis_vec = gram_schmidt(new_snapshot, V_basis)\n            V_basis = np.hstack([V_basis, new_basis_vec.reshape(-1, 1)]) if V_basis.size > 0 else new_basis_vec.reshape(-1, 1)\n\n        return selected_mus\n\n    # --- 4. Main Experiment Loop ---\n    final_results = []\n    \n    # Pre-calculate coercivity constant part\n    lambda1_K = (4.0 / h**2) * (np.sin(np.pi / (2.0 * (N + 1))))**2\n    def alpha_LB(mu):\n        return 1.0 + a_func(mu) * lambda1_K\n\n    # Run strong greedy once (results are independent of m)\n    strong_mus = run_greedy_procedure(strategy='strong')\n    \n    for m in test_cases_m:\n        # Build DEIM approximator for current m\n        deim_approximator = build_deim_assets(m)\n        \n        # Run weak greedy with the specific DEIM approximator\n        weak_mus = run_greedy_procedure(strategy='weak',\n                                        deim_approximator_func=deim_approximator,\n                                        coercivity_func=alpha_LB)\n\n        # Compute divergence metrics\n        # D_ord: Ordered-position divergence\n        ord_diff_count = sum(1 for i in range(k_max) if not np.isclose(strong_mus[i], weak_mus[i]))\n        D_ord = float(ord_diff_count) / k_max\n        \n        # D_set: Set-symmetric-difference divergence\n        set_strong = set(strong_mus)\n        set_weak = set(weak_mus)\n        sym_diff_size = len(set_strong.symmetric_difference(set_weak))\n        D_set = float(sym_diff_size) / k_max\n\n        final_results.extend([D_ord, D_set])\n        \n    print(f\"[{','.join(f'{x:.8f}' for x in final_results)}]\")\n\nsolve()\n```"
        }
    ]
}