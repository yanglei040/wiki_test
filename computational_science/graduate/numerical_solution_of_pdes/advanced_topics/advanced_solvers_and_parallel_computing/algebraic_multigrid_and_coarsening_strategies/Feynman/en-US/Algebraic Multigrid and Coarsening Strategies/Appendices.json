{
    "hands_on_practices": [
        {
            "introduction": "Theory comes to life through implementation. This first exercise provides a direct, hands-on opportunity to build a two-grid solver from its fundamental components. By implementing and comparing two distinct coarsening strategies for the canonical 1D Poisson problem—one based on structured linear interpolation and another on unstructured aggregation—you will observe firsthand how the design of the prolongation operator $P$ critically influences the method's convergence rate. This practice bridges the gap between the abstract definition of multigrid operators and their concrete performance.",
            "id": "3362508",
            "problem": "Consider the symmetric positive definite linear system arising from the standard central-difference discretization of the one-dimensional Poisson equation with homogeneous Dirichlet boundary conditions on the unit interval. The discrete operator is the tridiagonal matrix $A \\in \\mathbb{R}^{n \\times n}$ with diagonal entries $2$ and sub- and super-diagonal entries $-1$, acting on the $n$ interior degrees of freedom. \n\nA two-grid method from Algebraic Multigrid (AMG) is built from two fundamental components: a smoother and a coarse-grid correction. The weighted Jacobi smoother for weight $\\omega \\in (0,1)$ is defined by the iteration matrix \n$$\nS(\\omega) = I - \\omega D^{-1} A,\n$$\nwhere $D$ is the diagonal of $A$ and $I$ is the identity matrix of size $n$. Using $\\nu_1 \\in \\mathbb{N}_0$ pre-smoothing steps and $\\nu_2 \\in \\mathbb{N}_0$ post-smoothing steps, the overall smoothing operators are $S(\\omega)^{\\nu_1}$ and $S(\\omega)^{\\nu_2}$, respectively. The coarse-grid correction is defined by the prolongation $P \\in \\mathbb{R}^{n \\times n_c}$, the restriction $R \\in \\mathbb{R}^{n_c \\times n}$, and the Galerkin coarse operator\n$$\nA_c = R A P \\in \\mathbb{R}^{n_c \\times n_c}.\n$$\nThe two-grid error-propagation operator is \n$$\nE_{\\mathrm{TG}} = S(\\omega)^{\\nu_2} \\left( I - P A_c^{-1} R A \\right) S(\\omega)^{\\nu_1}.\n$$\nThe quantity of interest is the spectral radius\n$$\n\\rho\\left(E_{\\mathrm{TG}}\\right) = \\max\\{ |\\lambda| : \\lambda \\text{ is an eigenvalue of } E_{\\mathrm{TG}} \\},\n$$\nwhich predicts the linear convergence factor per two-grid cycle in the $\\ell_2$-norm. The coarse space dimension is $n_c$, the number of coarse degrees of freedom, which equals the number of columns of $P$. \n\nImplement and compare two algebraic coarsening strategies:\n\n- Strategy $\\mathrm{R2\\text{-}LI\\text{-}FW}$ (ratio-$2$ coarsening, linear interpolation, full weighting):\n  - Choose coarse indices as every other fine index (in one-based indexing, indices $2,4,6,\\dots$ up to $n$).\n  - Define $P$ by piecewise-linear interpolation: coarse points inject with weight $1$, and fine points between two neighboring coarse points interpolate linearly with weights summing to $1$; at boundary-adjacent fine points with only one neighboring coarse point, use weight $1$ to that neighbor.\n  - Define $R$ by full weighting: for a coarse index associated with fine index $i$, set weights $\\tfrac{1}{2}$ on fine indices $i-1$ and $i+1$ (when they exist) and weight $1$ on fine index $i$.\n- Strategy $\\mathrm{A3\\text{-}PC}$ (aggregation by contiguous blocks of size $3$, piecewise constant):\n  - Partition the ordered set of fine indices into contiguous aggregates of size $3$ (the last aggregate may be smaller); thus, the $k$-th aggregate is the set $\\{3k+1,3k+2,3k+3\\}$ in one-based indexing, for all valid $k$.\n  - Define $P$ to be piecewise constant on aggregates: $P_{ij} = 1$ if fine index $i$ is in aggregate $j$ and $P_{ij} = 0$ otherwise.\n  - Define $R = P^\\top$.\n\nYou must compute, for each test case below, the coarse space dimension $n_c$ and the spectral radius $\\rho(E_{\\mathrm{TG}})$ defined above. Use the exact $A$ described, the exact $S(\\omega)$ and $E_{\\mathrm{TG}}$ given, and the exact $A_c = R A P$ without approximation. \n\nTest suite:\n- Case $1$: $n = 63$, strategy $\\mathrm{R2\\text{-}LI\\text{-}FW}$, $\\omega = \\tfrac{2}{3}$, $\\nu_1 = 1$, $\\nu_2 = 1$.\n- Case $2$: $n = 63$, strategy $\\mathrm{A3\\text{-}PC}$, $\\omega = \\tfrac{2}{3}$, $\\nu_1 = 1$, $\\nu_2 = 1$.\n- Case $3$: $n = 31$, strategy $\\mathrm{R2\\text{-}LI\\text{-}FW}$, $\\omega = 0.8$, $\\nu_1 = 2$, $\\nu_2 = 0$.\n- Case $4$: $n = 5$, strategy $\\mathrm{A3\\text{-}PC}$, $\\omega = 0.5$, $\\nu_1 = 1$, $\\nu_2 = 1$.\n- Case $5$: $n = 3$, strategy $\\mathrm{A3\\text{-}PC}$, $\\omega = \\tfrac{2}{3}$, $\\nu_1 = 1$, $\\nu_2 = 1$.\n\nYour program must:\n- Construct $A$, $P$, and $R$ from first principles as specified.\n- Form $S(\\omega)$, $A_c$, $E_{\\mathrm{TG}}$, and compute $n_c$ and $\\rho\\left(E_{\\mathrm{TG}}\\right)$ for each case.\n- Round each $\\rho\\left(E_{\\mathrm{TG}}\\right)$ to exactly $6$ decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-element list $[n_c,\\rho]$ in the same order as the test suite. For example, an output with three cases should look like $[[n_{c,1},\\rho_1],[n_{c,2},\\rho_2],[n_{c,3},\\rho_3]]$ with each $\\rho_j$ printed with exactly $6$ decimal places and no extra whitespace. No other text should be printed.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- **System Matrix**: The matrix $A \\in \\mathbb{R}^{n \\times n}$ is the tridiagonal matrix for the $1$D Poisson equation, with diagonal entries of $2$ and sub/super-diagonal entries of $-1$.\n- **Weighted Jacobi Smoother Iteration Matrix**: $S(\\omega) = I - \\omega D^{-1} A$, where $D$ is the diagonal of $A$, $I$ is the identity matrix, and $\\omega \\in (0,1)$ is a weight.\n- **Smoothing Steps**: $\\nu_1 \\in \\mathbb{N}_0$ pre-smoothing steps and $\\nu_2 \\in \\mathbb{N}_0$ post-smoothing steps.\n- **Coarse-Grid Components**: Prolongation $P \\in \\mathbb{R}^{n \\times n_c}$, Restriction $R \\in \\mathbb{R}^{n_c \\times n}$, and Galerkin coarse operator $A_c = R A P \\in \\mathbb{R}^{n_c \\times n_c}$.\n- **Two-Grid Error-Propagation Operator**: $E_{\\mathrm{TG}} = S(\\omega)^{\\nu_2} \\left( I - P A_c^{-1} R A \\right) S(\\omega)^{\\nu_1}$.\n- **Quantity of Interest**: The spectral radius $\\rho\\left(E_{\\mathrm{TG}}\\right)$, which is the maximum absolute value of the eigenvalues of $E_{\\mathrm{TG}}$.\n- **Strategy $\\mathrm{R2\\text{-}LI\\text{-}FW}$**:\n    - **Coarsening**: Coarse indices are every other fine index, e.g., $2, 4, 6, \\dots$ (one-based).\n    - **Prolongation $P$**: Piecewise-linear interpolation. Coarse points inject (weight $1$). Fine points between two coarse points interpolate linearly. Boundary-adjacent fine points with one coarse neighbor use weight $1$ to that neighbor.\n    - **Restriction $R$**: Full weighting. For a coarse index at fine position $i$, weights are $\\tfrac{1}{2}$ on $i-1$, $1$ on $i$, and $\\tfrac{1}{2}$ on $i+1$.\n- **Strategy $\\mathrm{A3\\text{-}PC}$**:\n    - **Coarsening**: Partition fine indices into contiguous aggregates of size $3$ (last one may be smaller).\n    - **Prolongation $P$**: Piecewise constant. $P_{ij} = 1$ if fine index $i$ is in aggregate $j$, $0$ otherwise.\n    - **Restriction $R$**: $R = P^\\top$.\n- **Test Cases**:\n    1. $n = 63$, strategy $\\mathrm{R2\\text{-}LI\\text{-}FW}$, $\\omega = \\tfrac{2}{3}$, $\\nu_1 = 1$, $\\nu_2 = 1$.\n    2. $n = 63$, strategy $\\mathrm{A3\\text{-}PC}$, $\\omega = \\tfrac{2}{3}$, $\\nu_1 = 1$, $\\nu_2 = 1$.\n    3. $n = 31$, strategy $\\mathrm{R2\\text{-}LI\\text{-}FW}$, $\\omega = 0.8$, $\\nu_1 = 2$, $\\nu_2 = 0$.\n    4. $n = 5$, strategy $\\mathrm{A3\\text{-}PC}$, $\\omega = 0.5$, $\\nu_1 = 1$, $\\nu_2 = 1$.\n    5. $n = 3$, strategy $\\mathrm{A3\\text{-}PC}$, $\\omega = \\tfrac{2}{3}$, $\\nu_1 = 1$, $\\nu_2 = 1$.\n- **Output Requirement**: For each case, compute coarse dimension $n_c$ and $\\rho(E_{\\mathrm{TG}})$, with $\\rho$ rounded to $6$ decimal places.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is well-defined and scientifically sound. It resides in the field of numerical linear algebra, specifically the study of multigrid methods for solving partial differential equations. All components ($A$, $S(\\omega)$, $P$, $R$, $A_c$, $E_{\\mathrm{TG}}$) are standard elements of two-grid analysis. The coarsening strategies, while specific ( R2-LI-FW has a non-standard boundary interpolation rule), are described with sufficient precision to be implemented unambiguously. The parameters for all test cases are complete and consistent. The problem is self-contained and does not violate any mathematical or scientific principles. No flaws are identified.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Solution\nThe objective is to compute the coarse-space dimension $n_c$ and the spectral radius $\\rho(E_{\\mathrm{TG}})$ of the two-grid error-propagation operator $E_{\\mathrm{TG}}$ for several test cases. This requires constructing all constituent matrices ($A$, $S(\\omega)$, $P$, $R$, $A_c$) according to the provided specifications. We adopt zero-based indexing for all matrix and vector-related computations.\n\nFirst, we construct the system matrix $A \\in \\mathbb{R}^{n \\times n}$. It is a symmetric tridiagonal matrix with $2$ on the main diagonal and $-1$ on the first sub-diagonal and super-diagonal.\n$$A_{ij} = \\begin{cases} 2 & i=j \\\\ -1 & |i-j|=1 \\\\ 0 & \\text{otherwise} \\end{cases}$$\n\nNext, we define the weighted Jacobi smoother $S(\\omega) = I - \\omega D^{-1} A$. The matrix $A$ has a constant diagonal of $2$, so its diagonal part is $D=2I$. Thus, $D^{-1} = \\frac{1}{2}I$, and the smoother simplifies to $S(\\omega) = I - \\frac{\\omega}{2} A$. The pre- and post-smoothing operators are $S(\\omega)^{\\nu_1}$ and $S(\\omega)^{\\nu_2}$, respectively, computed via matrix exponentiation.\n\nThe core of the task is to construct the prolongation operator $P$ and restriction operator $R$ for each of the two strategies.\n\n#### Strategy $\\mathrm{R2\\text{-}LI\\text{-}FW}$ (Ratio-$2$ Coarsening, Linear Interpolation, Full Weighting)\n1.  **Coarsening**: The coarse grid points are chosen to be every other fine grid point. In one-based indexing, these are $\\{2, 4, 6, \\dots\\}$. In zero-based indexing, the coarse-point indices are $C = \\{1, 3, 5, \\dots, 2k+1, \\dots\\}$ for all $k$ such that $2k+1  n$. The number of coarse points is $n_c = \\lfloor n/2 \\rfloor$. The fine-only points are the remaining indices $F = \\{0, 1, \\dots, n-1\\} \\setminus C$.\n\n2.  **Prolongation $P \\in \\mathbb{R}^{n \\times n_c}$**: This matrix maps a coarse-grid vector to a fine-grid vector. Its columns form a basis for the coarse space.\n    - For a coarse point at fine index $i=2k+1$ (corresponding to the $k$-th coarse variable), its value is injected: a row of $P$ corresponding to a coarse point has a single $1$ in the column of that coarse variable. $P_{2k+1, k} = 1$.\n    - For an interior fine-only point $i=2k$ ($k>0$), it lies between coarse points at $i-1=2k-1$ and $i+1=2k+1$. It interpolates their values with weights $\\frac{1}{2}$. This means $P_{2k, k-1} = 0.5$ and $P_{2k, k} = 0.5$.\n    - For boundary-adjacent fine points, a special rule applies. For fine point $i=0$, its only coarse-grid neighbor is at $i=1$ (the $0$-th coarse variable). The rule specifies a weight of $1$, so $P_{0,0}=1$. Similarly, if $n$ is odd, the fine point $i=n-1$ is adjacent to coarse point $i=n-2$ (the $(n_c-1)$-th coarse variable). Its weight is $1$, so $P_{n-1, n_c-1}=1$.\n\n3.  **Restriction $R \\in \\mathbb{R}^{n_c \\times n}$**: This matrix maps a fine-grid vector to a coarse-grid vector.\n    - For the $k$-th coarse variable (associated with fine index $i_c=2k+1$), the restriction is a weighted average of values at $i_c-1$, $i_c$, and $i_c+1$. The stencil is specified as $[\\frac{1}{2}, 1, \\frac{1}{2}]$.\n    - Thus, the $k$-th row of $R$ has non-zero entries: $R_{k, i_c-1} = 0.5$, $R_{k, i_c} = 1$, and $R_{k, i_c+1} = 0.5$, provided these indices are within the grid bounds, i.e., in $\\{0, \\dots, n-1\\}$.\n\n#### Strategy $\\mathrm{A3\\text{-}PC}$ (Aggregation by contiguous blocks of size $3$, Piecewise Constant)\n1.  **Coarsening**: The fine-grid indices $\\{0, 1, \\dots, n-1\\}$ are partitioned into contiguous aggregates of size $3$. The $k$-th aggregate is $\\{3k, 3k+1, 3k+2\\}$. The last aggregate may be smaller if $n$ is not a multiple of $3$. Each aggregate defines one coarse-grid variable. The number of coarse points is $n_c = \\lceil n/3 \\rceil$.\n\n2.  **Prolongation $P \\in \\mathbb{R}^{n \\times n_c}$**: The interpolation is piecewise constant over aggregates.\n    - The $k$-th column of $P$ corresponds to the $k$-th aggregate. It has entries of $1$ for all rows (fine indices) belonging to aggregate $k$, and $0$ otherwise.\n\n3.  **Restriction $R \\in \\mathbb{R}^{n_c \\times n}$**: The restriction is defined as the transpose of the prolongation operator, $R = P^\\top$.\n\nWith $A$, $S(\\omega)$, $P$, and $R$ constructed, we form the Galerkin coarse operator $A_c = RAP$. We then compute its inverse $A_c^{-1}$. The two-grid error propagation operator is assembled as $E_{\\mathrm{TG}} = S(\\omega)^{\\nu_2} (I - P A_c^{-1} R A) S(\\omega)^{\\nu_1}$. Finally, we compute the eigenvalues of $E_{\\mathrm{TG}}$ and find the spectral radius $\\rho(E_{\\mathrm{TG}})$ by taking the maximum of their absolute values. This value is rounded to $6$ decimal places.\n\nThe results for each test case are computed by systematically applying this procedure.",
            "answer": "```python\nimport numpy as np\n\ndef _construct_A(n):\n    \"\"\"Constructs the n x n discrete 1D Poisson matrix A.\"\"\"\n    if n == 0:\n        return np.array([])\n    # Start with a matrix of zeros\n    A = np.zeros((n, n))\n    # Fill diagonal with 2 and off-diagonals with -1\n    np.fill_diagonal(A, 2)\n    if n  1:\n        np.fill_diagonal(A[1:], -1)\n        np.fill_diagonal(A[:, 1:], -1)\n    return A\n\ndef _construct_R2_LI_FW_operators(n):\n    \"\"\"Constructs P, n_c, and R for the R2-LI-FW strategy.\"\"\"\n    if n  2:\n        return np.zeros((n, 0)), 0, np.zeros((0, n))\n\n    n_c = n // 2\n    P = np.zeros((n, n_c))\n    \n    # Map from fine-grid coarse index to coarse-grid index (0-based)\n    coarse_map = {2 * k + 1: k for k in range(n_c)}\n\n    for i in range(n):\n        if i in coarse_map:  # Coarse point\n            P[i, coarse_map[i]] = 1.0\n        else:  # Fine-only point\n            if i == 0:\n                # Boundary-adjacent fine point with one coarse neighbor at index 1\n                # Coarse neighbor's coarse index is 0\n                P[i, 0] = 1.0\n            elif n % 2 != 0 and i == n - 1:\n                # Boundary-adjacent fine point (only if n is odd)\n                # Coarse neighbor is at n-2, whose coarse index is n_c-1\n                P[i, n_c - 1] = 1.0\n            else:\n                # Interior fine point, interpolate between two coarse neighbors\n                # Fine point i is even. Neighbors are i-1 and i+1.\n                coarse_neighbor1_fine_idx = i - 1\n                coarse_neighbor2_fine_idx = i + 1\n                coarse_neighbor1_coarse_idx = coarse_map[coarse_neighbor1_fine_idx]\n                coarse_neighbor2_coarse_idx = coarse_map[coarse_neighbor2_fine_idx]\n                P[i, coarse_neighbor1_coarse_idx] = 0.5\n                P[i, coarse_neighbor2_coarse_idx] = 0.5\n\n    R = np.zeros((n_c, n))\n    for j in range(n_c):\n        coarse_fine_idx = 2 * j + 1\n        if coarse_fine_idx  0:\n            R[j, coarse_fine_idx - 1] = 0.5\n        R[j, coarse_fine_idx] = 1.0\n        if coarse_fine_idx  n - 1:\n            R[j, coarse_fine_idx + 1] = 0.5\n            \n    return P, n_c, R\n\ndef _construct_A3_PC_operators(n):\n    \"\"\"Constructs P, n_c, and R for the A3-PC strategy.\"\"\"\n    if n == 0:\n        return np.zeros((0, 0)), 0, np.zeros((0, 0))\n    \n    n_c = (n + 2) // 3  # Integer ceiling division\n    P = np.zeros((n, n_c))\n    \n    for j in range(n_c):\n        start_idx = 3 * j\n        end_idx = min(start_idx + 3, n)\n        P[start_idx:end_idx, j] = 1.0\n        \n    R = P.T\n    return P, n_c, R\n\ndef compute_spectral_radius(n, strategy, omega, nu1, nu2):\n    \"\"\"Computes n_c and rho(E_TG) for a given case.\"\"\"\n    A = _construct_A(n)\n    \n    if strategy == \"R2-LI-FW\":\n        P, n_c, R = _construct_R2_LI_FW_operators(n)\n    elif strategy == \"A3-PC\":\n        P, n_c, R = _construct_A3_PC_operators(n)\n    else:\n        raise ValueError(\"Unknown strategy\")\n\n    if n_c == 0:\n        return 0, 0.0\n\n    # Smoother\n    S_omega = np.identity(n) - (omega / 2.0) * A\n    \n    S1 = np.linalg.matrix_power(S_omega, nu1)\n    S2 = np.linalg.matrix_power(S_omega, nu2)\n    \n    # Coarse grid operator\n    A_c = R @ A @ P\n    \n    # Coarse grid correction operator\n    # For very small n_c, direct inversion is stable and efficient\n    A_c_inv = np.linalg.inv(A_c)\n    CGC = np.identity(n) - P @ A_c_inv @ R @ A\n    \n    # Two-grid error propagation operator\n    E_TG = S2 @ CGC @ S1\n    \n    # Spectral radius\n    eigenvalues = np.linalg.eigvals(E_TG)\n    spectral_radius = np.max(np.abs(eigenvalues))\n    \n    return n_c, spectral_radius\n\ndef solve():\n    test_cases = [\n        {'n': 63, 'strategy': \"R2-LI-FW\", 'omega': 2/3, 'nu1': 1, 'nu2': 1},\n        {'n': 63, 'strategy': \"A3-PC\", 'omega': 2/3, 'nu1': 1, 'nu2': 1},\n        {'n': 31, 'strategy': \"R2-LI-FW\", 'omega': 0.8, 'nu1': 2, 'nu2': 0},\n        {'n': 5, 'strategy': \"A3-PC\", 'omega': 0.5, 'nu1': 1, 'nu2': 1},\n        {'n': 3, 'strategy': \"A3-PC\", 'omega': 2/3, 'nu1': 1, 'nu2': 1},\n    ]\n\n    results = []\n    for case in test_cases:\n        n_c, rho = compute_spectral_radius(\n            case['n'], case['strategy'], case['omega'], case['nu1'], case['nu2']\n        )\n        results.append(f\"[{n_c},{rho:.6f}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The previous exercise demonstrated that the choice of coarsening strategy matters. This practice delves into the 'why,' equipping you with powerful diagnostic tools to analyze the quality of a prolongation operator. Moving to the more challenging 2D Poisson problem, you will use eigenvalue analysis to quantify the crucial 'approximation property'—the ability of the coarse space to represent the smooth error components that the smoother cannot efficiently eliminate. This analysis is fundamental to designing robust and efficient Algebraic Multigrid methods for complex problems.",
            "id": "3362523",
            "problem": "You are given the task of building an eigenvalue-based diagnostic to evaluate how algebraic multigrid coarsening and prolongation affect the representation of smooth error modes for the symmetric positive definite matrix arising from the standard five-point discretization of the two-dimensional Poisson equation with homogeneous Dirichlet boundary conditions. The fundamental base for this task is:\n\n- The discretized operator on an $n \\times n$ interior grid is a symmetric positive definite matrix $A \\in \\mathbb{R}^{N \\times N}$ with $N = n^2$, constructed as the Kronecker sum of one-dimensional second-difference operators. The one-dimensional operator is the tridiagonal matrix $T_n = \\operatorname{tridiag}(-1,2,-1)$.\n- The eigenpairs $(\\lambda_i, v_i)$ of $A$ satisfy $A v_i = \\lambda_i v_i$, where $0  \\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_N$, and for the Poisson operator the smallest eigenmodes correspond to the smoothest error components.\n- A coarse space is defined by a prolongation operator $P \\in \\mathbb{R}^{N \\times N_c}$. The Galerkin coarse operator is $A_c = P^\\top A P \\in \\mathbb{R}^{N_c \\times N_c}$.\n- The $A$-orthogonal projector onto $\\operatorname{range}(P)$ is $\\Pi_P = P \\left(P^\\top A P\\right)^{-1} P^\\top A$. For any vector $x \\in \\mathbb{R}^N$, the $A$-norm is $\\lVert x \\rVert_A = \\sqrt{x^\\top A x}$.\n- The Rayleigh quotient of a nonzero vector $x$ with respect to $A$ is $\\mathcal{R}_A(x) = \\frac{x^\\top A x}{x^\\top x}$. If $y$ is an eigenvector of $A_c$, then the fine-grid vector $x = P y$ has a Rayleigh quotient that serves as a Ritz value approximating an eigenvalue of $A$ on the coarse space.\n\nYour program must implement the following diagnostics for each test case:\n\n1) Construct $A$ for an $n \\times n$ grid. Compute the $k$ smallest eigenpairs $(\\lambda_i, v_i)$ of $A$.\n\n2) Construct a prolongation $P$ corresponding to the specified coarsening strategy on a regular grid with coarsening factor $2$ in each spatial dimension:\n   - For the type labeled \"pc\" (piecewise constant), use nearest-neighbor injection in $1$D and the Kronecker product to obtain $P$ in $2$D.\n   - For the type labeled \"linear\", use standard linear interpolation in $1$D (exact restriction/prolongation for geometric coarsening by $2$ with homogeneous Dirichlet boundaries) and the Kronecker product to obtain $P$ in $2$D.\n\n3) Form $A_c = P^\\top A P$. For each of the $k$ smallest eigenvectors $v_i$ of $A$, compute the $A$-orthogonal projection error\n$$\ne_i = v_i - \\Pi_P v_i = v_i - P\\left(P^\\top A P\\right)^{-1} P^\\top A v_i,\n$$\nand the relative $A$-norm coverage metric\n$$\nc_i = \\frac{\\lVert e_i \\rVert_A}{\\lVert v_i \\rVert_A}.\n$$\nReport the mean coverage $\\overline{c} = \\frac{1}{k} \\sum_{i=1}^k c_i$ and the maximum coverage $\\max_i c_i$.\n\n4) Compute the $k_c$ smallest eigenpairs $(\\mu_j, y_j)$ of $A_c$, with $k_c = \\min\\{k, N_c - 1\\}$ to satisfy $k_c  N_c$. Map each coarse eigenvector to the fine grid via $x_j = P y_j$, compute its Rayleigh quotient $\\theta_j = \\mathcal{R}_A(x_j)$, and report the relative error in the smallest coarse Ritz value with respect to the smallest true fine-grid eigenvalue:\n$$\n\\delta = \\frac{\\min_j \\theta_j - \\lambda_1}{\\lambda_1}.\n$$\n\n5) Construct a single-step smoothed prolongation $P_s = \\left(I - \\omega D^{-1} A\\right) P$, where $D = \\operatorname{diag}(A)$ and $\\omega$ is a fixed smoothing parameter. Recompute $\\overline{c}_s$ with $P_s$ and report a refinement indicator\n$$\n\\text{improved} =\n\\begin{cases}\n1,  \\text{if } \\overline{c}_s  \\overline{c} - \\varepsilon,\\\\\n0,  \\text{otherwise,}\n\\end{cases}\n$$\nwith tolerance $\\varepsilon = 10^{-12}$.\n\nImplementation details and constraints:\n\n- Use the finite difference discretization described above to assemble $A$ as a sparse matrix. Assume homogeneous Dirichlet boundary conditions.\n- For \"pc\", build one-dimensional nearest-neighbor injection prolongation $P_1 \\in \\mathbb{R}^{n \\times m}$ with $m = \\lfloor n/2 \\rfloor$ and set $P = P_2 \\otimes P_1$, where $P_2 = P_1$ and $\\otimes$ denotes the Kronecker product. Concretely, for row index $j \\in \\{1,\\dots,n\\}$, map to coarse column $i = \\left\\lfloor \\frac{j+1}{2} \\right\\rfloor$ with unit weight, clamped to $\\{1,\\dots,m\\}$.\n- For \"linear\", build the one-dimensional linear interpolation operator $P_1 \\in \\mathbb{R}^{n \\times m}$ with $m = \\lfloor n/2 \\rfloor$ as follows: for an odd fine-grid index $j = 2i-1$ (1-based), set $(P_1)_{j,i} = 1$; for an even fine-grid index $j = 2i$, set $(P_1)_{j,i} = \\frac{1}{2}$ and $(P_1)_{j,i+1} = \\frac{1}{2}$, with appropriate modifications for boundary conditions. Then set $P = P_2 \\otimes P_1$ with $P_2 = P_1$.\n- Use $k = 6$ and smoothing parameter $\\omega = 0.67$.\n- All linear algebra must be performed in exact arithmetic of floating point types; do not introduce heuristics beyond those specified.\n\nTest suite:\n\n- Case $1$: $n = 16$, prolongation type \"pc\".\n- Case $2$: $n = 16$, prolongation type \"linear\".\n- Case $3$: $n = 20$, prolongation type \"pc\".\n\nFor each case, your program must compute and return the list $[\\overline{c}, \\max_i c_i, \\delta, \\text{improved}]$, where $\\overline{c}$, $\\max_i c_i$, and $\\delta$ are real numbers, and $\\text{improved}$ is an integer in $\\{0,1\\}$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list of lists enclosed in square brackets (e.g., \"[[a,b,c,d],[e,f,g,h],[i,j,k,l]]\"). No extra whitespace or text is permitted in the output.",
            "solution": "The user wants to implement a set of diagnostics to analyze algebraic multigrid (AMG) components for the 2D Poisson equation. The analysis involves evaluating how well different prolongation operators represent the smoothest eigenvectors of the fine-grid system matrix. This is a standard procedure in the development and analysis of multigrid methods. The problem is scientifically well-grounded, algorithmically specific, and computationally feasible.\n\nThe analysis can be decomposed into the following systematic steps:\n\n1.  **System Matrix Assembly**: The system matrix $A$ corresponds to the five-point finite difference discretization of the negative Laplacian on an $n \\times n$ grid with homogeneous Dirichlet boundary conditions. The total number of degrees of freedom is $N = n^2$. This matrix can be constructed as the Kronecker sum of two one-dimensional second-difference operators, $A = I_n \\otimes T_n + T_n \\otimes I_n$, where $T_n$ is the $n \\times n$ tridiagonal matrix $\\operatorname{tridiag}(-1, 2, -1)$. The resulting matrix $A$ is symmetric and positive definite (SPD). Its diagonal entries are all $4$, so its diagonal is $D = 4I$.\n\n2.  **Fine-Grid Eigenanalysis**: The eigenvectors of $A$ corresponding to its smallest eigenvalues are the smoothest modes on the grid. These are the modes that relaxation methods (like Jacobi or Gauss-Seidel) are slow to converge. An effective multigrid method must be able to represent these smooth modes well on the coarse grid. We compute the $k=6$ smallest eigenvalues $\\lambda_i$ and their corresponding eigenvectors $v_i$ of $A$. This is achieved using an iterative eigensolver suitable for large, sparse, symmetric matrices. The eigenvectors $v_i$ are normalized, i.e., $\\lVert v_i \\rVert_2 = 1$.\n\n3.  **Prolongation Operator Construction**: The prolongation (or interpolation) operator $P$ maps vectors from the coarse grid to the fine grid. We consider a standard coarsening strategy where the number of grid points in each dimension is halved, from $n$ to $m = \\lfloor n/2 \\rfloor$. The total number of coarse-grid points is $N_c = m^2$. The 2D prolongation operator $P \\in \\mathbb{R}^{N \\times N_c}$ is formed by the Kronecker product $P = P_1 \\otimes P_1$, where $P_1 \\in \\mathbb{R}^{n \\times m}$ is the 1D prolongation operator.\n    *   For **piecewise constant** interpolation (\"pc\"), a coarse-grid value is copied to a block of neighboring fine-grid points. Based on the provided formula $i = \\lfloor(j+1)/2\\rfloor$, the value from coarse point $i$ is assigned to fine points $2i-1$ and $2i$ (in 1-based indexing).\n    *   For **linear** interpolation (\"linear\"), values at fine-grid points are interpolated from the nearest coarse-grid points. With homogeneous Dirichlet boundary conditions, this involves weights of $\\frac{1}{2}$ and $1$. The specific rules provided correctly construct the standard 1D linear prolongation matrix.\n\n4.  **Coarse-Grid Operator**: The coarse-grid operator $A_c$ is formed using the Galerkin projection: $A_c = P^\\top A P$. Since $A$ is SPD and $P$ has full column rank, $A_c$ is also SPD.\n\n5.  **Projection Error and Coverage Metric**: The quality of the prolongation operator $P$ is measured by how well it can represent the smooth eigenvectors $v_i$. The best approximation to $v_i$ in the range of $P$, denoted as $\\operatorname{range}(P)$, is its $A$-orthogonal projection, $\\Pi_P v_i$. The projector is given by $\\Pi_P = P(P^\\top A P)^{-1} P^\\top A$. The approximation error is $e_i = v_i - \\Pi_P v_i$. A key diagnostic is the relative error in the $A$-norm, $c_i = \\frac{\\lVert e_i \\rVert_A}{\\lVert v_i \\rVert_A}$.\n    Using the properties of $A$-orthogonal projectors, for any vector $x$, $\\lVert x \\rVert_A^2 = \\lVert \\Pi_P x \\rVert_A^2 + \\lVert (I - \\Pi_P) x \\rVert_A^2$. Thus, $\\lVert e_i \\rVert_A^2 = \\lVert v_i \\rVert_A^2 - \\lVert \\Pi_P v_i \\rVert_A^2$.\n    Since $v_i$ is an eigenvector of $A$ with eigenvalue $\\lambda_i$, and is $L_2$-normalized, we have $\\lVert v_i \\rVert_A^2 = v_i^\\top A v_i = \\lambda_i v_i^\\top v_i = \\lambda_i$.\n    The projected vector's norm is $\\lVert \\Pi_P v_i \\rVert_A^2 = (\\Pi_P v_i)^\\top A (\\Pi_P v_i) = (P y_i)^\\top A (P y_i) = y_i^\\top (P^\\top A P) y_i = y_i^\\top A_c y_i$, where $y_i = (P^\\top A P)^{-1} P^\\top A v_i = A_c^{-1} (P^\\top A v_i)$. Substituting $y_i$ gives $\\lVert \\Pi_P v_i \\rVert_A^2 = (P^\\top A v_i)^\\top A_c^{-1} (P^\\top A v_i) = (P^\\top A v_i)^\\top y_i$.\n    This leads to the computationally efficient formula for the squared coverage metric:\n    $$ c_i^2 = 1 - \\frac{\\lVert \\Pi_P v_i \\rVert_A^2}{\\lVert v_i \\rVert_A^2} = 1 - \\frac{(P^\\top A v_i)^\\top y_i}{\\lambda_i} $$\n    We compute the mean $\\overline{c}$ and maximum $\\max_i c_i$ of these values over the first $k$ eigenvectors.\n\n6.  **Coarse-Grid Eigenvalue Analysis**: An alternative quality measure is to see how well the coarse-grid problem $A_c y = \\mu y$ approximates the fine-grid eigenproblem. The eigenpairs $(\\mu_j, y_j)$ of $A_c$ generate Ritz pairs $(\\theta_j, x_j)$ for $A$, where $x_j = P y_j$ and $\\theta_j$ is the Rayleigh quotient $\\mathcal{R}_A(x_j) = \\frac{x_j^\\top A x_j}{x_j^\\top x_j}$. The smallest Ritz value $\\min_j \\theta_j$ should approximate the smallest fine-grid eigenvalue $\\lambda_1$. We compute the relative error $\\delta = (\\min_j \\theta_j - \\lambda_1) / \\lambda_1$. For this, we calculate the $k_c = \\min\\{k, N_c - 1\\}$ smallest eigenpairs of $A_c$.\n\n7.  **Smoothed Prolongation**: The representation quality of $P$ can often be improved by applying a few steps of a relaxation method (smoothing). We apply one step of damped Jacobi smoothing to $P$. The smoothed prolongator is $P_s = (I - \\omega D^{-1} A) P$, with damping factor $\\omega=0.67$ and $D=\\operatorname{diag}(A)=4I$. We then re-evaluate the mean coverage $\\overline{c}_s$ using $P_s$ and determine if there was a significant improvement, indicated by $\\text{improved} = 1$ if $\\overline{c}_s  \\overline{c} - \\varepsilon$ for a small tolerance $\\varepsilon=10^{-12}$. This procedure mimics the construction of more advanced prolongation operators in AMG.\n\nThe implementation will rely on sparse matrix representations from `scipy.sparse` to manage memory and computational cost effectively. Eigenproblems will be solved using `scipy.sparse.linalg.eigsh`, and small, dense linear systems will be handled by `scipy.sparse.linalg.spsolve`.",
            "answer": "```python\nimport numpy as np\nfrom scipy import sparse\nfrom scipy.sparse import linalg as sla\n\ndef build_poisson_matrix(n):\n    \"\"\"Constructs the 2D Poisson matrix A on an n x n grid.\"\"\"\n    N = n * n\n    T_n = sparse.diags([-1, 2, -1], [-1, 0, 1], shape=(n, n), format='csc')\n    I_n = sparse.identity(n, format='csc')\n    A = sparse.kronsum(T_n, T_n)\n    return A.asformat('csc')\n\ndef build_1d_prolongation(n, p_type):\n    \"\"\"Constructs the 1D prolongation operator P1.\"\"\"\n    if n % 2 != 0:\n        # The problem spec only considers even n, but for robustness\n        # one might need a specification for odd n. We adhere to problem constraints.\n        pass\n    m = n // 2\n    \n    rows, cols, data = [], [], []\n    \n    if p_type == \"pc\":\n        # Piecewise constant interpolation\n        # Based on j_fine_idx maps to i_coarse_idx = floor((j_fine_idx+2)/2) - 1\n        # This simplifies to i_coarse_idx = j_fine_idx // 2\n        for j in range(n):\n            i = j // 2\n            if i  m:\n                rows.append(j)\n                cols.append(i)\n                data.append(1.0)\n    elif p_type == \"linear\":\n        # Linear interpolation\n        # Coarse node i is at fine position 2i+1. Fine node 2i is between i-1 and i.\n        # This corresponds to the rules after 0-based index conversion.\n        for i in range(m):\n            # Coarse node i maps to fine node 2*i+1\n            rows.append(2 * i + 1)\n            cols.append(i)\n            data.append(1.0)\n            \n            # Fine node 2*i is between coarse nodes i-1 and i\n            j_odd = 2 * i\n            if j_odd  n:\n                if i  0:\n                    rows.append(j_odd)\n                    cols.append(i - 1)\n                    data.append(0.5)\n                rows.append(j_odd)\n                cols.append(i)\n                data.append(0.5)\n\n    return sparse.coo_matrix((data, (rows, cols)), shape=(n, m)).asformat('csc')\n\ndef compute_coverage_metrics(A, P, k, lambdas, vecs):\n    \"\"\"Computes mean and max coverage for a given prolongation P.\"\"\"\n    A_c = P.T @ A @ P\n    \n    # Use a sparse solver. For small Nc, can convert to dense and use np.linalg.solve\n    # A_c_dense = A_c.toarray()\n    \n    coverages = np.zeros(k)\n    for i in range(k):\n        v = vecs[:, i]\n        lambda_i = lambdas[i]\n        \n        b_i = P.T @ (A @ v)\n        \n        # Solve Ac y = b. Use spsolve for sparse systems.\n        y_i = sla.spsolve(A_c, b_i)\n\n        # Numerator is y_i.T @ A_c @ y_i = y_i.T @ b_i\n        # Denominator is v.T @ A @ v = lambda_i * v.T @ v = lambda_i\n        # Using the formulation c_i^2 = 1 - (b_i.T @ y_i) / lambda_i\n        # This is more stable than forming the error vector e_i\n        inner_prod = b_i.T @ y_i\n        \n        if lambda_i  1e-15: # Avoid division by zero, though not expected here\n            c_i_sq = 1.0\n        else:\n            c_i_sq = 1.0 - inner_prod / lambda_i\n        \n        # Clamp to avoid small negative values from floating point errors\n        coverages[i] = np.sqrt(max(0, c_i_sq))\n        \n    mean_c = np.mean(coverages)\n    max_c = np.max(coverages)\n    \n    return mean_c, max_c\n\ndef run_case(n, p_type, k, omega, epsilon):\n    \"\"\"Runs a single test case for the AMG diagnostic problem.\"\"\"\n    # 1. Construct A and get its smallest eigenpairs\n    A = build_poisson_matrix(n)\n    N = n * n\n    # For SPD matrices, 'SM' (smallest magnitude) is equivalent to 'SA' (smallest algebraic)\n    lambdas, vecs = sla.eigsh(A, k=k, which='SM')\n\n    # 2. Construct prolongation operator P\n    m = n // 2\n    Nc = m * m\n    P1d = build_1d_prolongation(n, p_type)\n    P = sparse.kron(P1d, P1d, format='csc')\n\n    # 3. Compute coverage metrics for P\n    c_bar, c_max = compute_coverage_metrics(A, P, k, lambdas, vecs)\n    \n    # 4. Compute coarse Ritz value error\n    A_c = P.T @ A @ P\n    kc = min(k, Nc - 1)\n    \n    # For small A_c, using dense eigh is efficient and robust\n    mu, Y = np.linalg.eigh(A_c.toarray())\n    mu_smallest = mu[:kc]\n    Y_smallest = Y[:, :kc]\n    \n    thetas = np.zeros(kc)\n    for j in range(kc):\n        y_j = Y_smallest[:, j]\n        x_j = P @ y_j\n        \n        numerator = x_j.T @ A @ x_j\n        denominator = x_j.T @ x_j\n        thetas[j] = numerator / denominator\n    \n    min_theta = np.min(thetas)\n    lambda_1 = lambdas[0]\n    delta = (min_theta - lambda_1) / lambda_1\n\n    # 5. Compute smoothed prolongation and its coverage\n    D_diag = A.diagonal()\n    if np.all(D_diag == D_diag[0]):\n        # Optimized path for constant diagonal\n        d_inv = 1.0 / D_diag[0]\n        Ps = P - (omega * d_inv) * (A @ P)\n    else:\n        # General case (not needed for this problem)\n        D_inv = sparse.diags(1.0 / D_diag, format='csc')\n        Ps = P - omega * (D_inv @ A @ P)\n\n    # Recompute coverage with Ps\n    cs_bar, _ = compute_coverage_metrics(A, Ps, k, lambdas, vecs)\n    \n    improved = 1 if cs_bar  c_bar - epsilon else 0\n    \n    return [c_bar, c_max, delta, improved]\n\ndef solve():\n    \"\"\"\n    Main function to execute the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (16, \"pc\"),\n        (16, \"linear\"),\n        (20, \"pc\"),\n    ]\n    \n    k = 6\n    omega = 0.67\n    epsilon = 1e-12\n    \n    results = []\n    for n, p_type in test_cases:\n        case_result = run_case(n, p_type, k, omega, epsilon)\n        results.append(f\"[{','.join(f'{x:.6g}' for x in case_result)}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Having built and analyzed AMG components, this final practice refines your conceptual intuition. It presents a series of thought experiments on classic model problems, challenging you to apply the core principles of multigrid theory without writing new code. By reasoning about the interplay between smoothing, coarse-grid correction, interpolation quality, and problem characteristics like anisotropy, you will solidify your understanding of what makes a multigrid method effective. This exercise hones the theoretical judgment that underpins practical algorithm design.",
            "id": "3362553",
            "problem": "Consider verification of algebraic multigrid components on model problems for linear systems that arise from the numerical solution of partial differential equations. Let the fine-grid linear system be $A u = f$ where $A$ is symmetric positive definite and comes from a second-order finite-difference discretization on a uniform grid. A basic two-grid method is defined by a coarse space via a prolongation operator $P$, a restriction operator $R$, the Galerkin coarse operator $A_c = R A P$, and a smoothing iteration matrix $S = I - M^{-1} A$ where $M$ is a chosen relaxation preconditioner. One step of two-grid error propagation with one pre- and one post-smoothing sweep can be written as $E = S_{\\mathrm{post}} \\left(I - P A_c^{-1} R A\\right) S_{\\mathrm{pre}}$. The goal of verification on model problems is to confirm smoothing and coarse-grid correction principles: smoothers damp high-frequency error and coarse-grid correction eliminates the components representable on the coarse grid.\n\nYou will assess the validity of the following statements by reasoning from these principles on standard model problems. Select all statements that are correct.\n\nA. Consider the one-dimensional Poisson problem $-u'' = f$ on $(0,1)$ with homogeneous Dirichlet boundary conditions, discretized by the standard second-order centered finite difference on a uniform grid with spacing $h$. Use a two-grid method with classical coarsening by a factor of $2$, full-weighting restriction, linear interpolation, and one pre- and one post-smoothing sweep of weighted Jacobi with weight $\\omega \\in (0,2)$. There exists a choice of $\\omega \\in (0,2)$ for which the two-grid spectral radius $\\rho(E)$ is bounded strictly below $1$ independently of $h$.\n\nB. In the same one-dimensional setting as in option A, if one replaces linear interpolation with injection (piecewise constant interpolation from the coarse grid to the fine grid), the two-grid spectral radius remains bounded uniformly in $h$ because the smoother removes the low-frequency error components.\n\nC. Consider the two-dimensional anisotropic diffusion operator $-\\partial_{xx} - \\varepsilon \\partial_{yy}$ on the unit square with a uniform mesh and $0  \\varepsilon \\ll 1$. In classical algebraic multigrid, one often declares a fine-grid point $j$ to be strongly connected to a neighbor $i$ if $|a_{ji}| \\ge \\theta \\max_{k \\ne j} |a_{jk}|$ for a strength-of-connection threshold $\\theta \\in (0,1)$. If $\\theta$ is chosen so large that many physically relevant couplings are dropped as weak, then with standard interpolation aligned to the remaining strong connections the two-grid convergence can lose grid independence as $\\varepsilon \\to 0$, a degradation that verification on this model problem will reveal.\n\nD. For the one-dimensional Poisson problem with periodic boundary conditions, uniform grid spacing $h$, full-weighting restriction, and linear interpolation, any constant fine-grid error component is eliminated exactly by the coarse-grid correction (assuming exact solution of the coarse-grid problem), regardless of $h$.\n\nSelect all that apply.",
            "solution": "We start from the basic definitions. The two-grid error propagation with one pre- and one post-smoothing sweep is $E = S_{\\mathrm{post}} \\left(I - P A_c^{-1} R A\\right) S_{\\mathrm{pre}}$, where $A_c = R A P$ is the Galerkin coarse operator. The term $S = I - M^{-1} A$ describes one sweep of relaxation, which should damp high-frequency components relative to the fine grid. The coarse-grid correction $I - P A_c^{-1} R A$ is an $A$-orthogonal projector onto the $A$-orthogonal complement of the range of $P$, hence it eliminates the component of the error that lies in the range of $P$.\n\nOption A. One-dimensional Poisson, Dirichlet, full weighting, linear interpolation, weighted Jacobi smoothing, existence of $\\omega$ with $h$-independent two-grid contraction.\n\nFundamental facts for verification on the one-dimensional Poisson model:\n\n- The discrete operator $A$ on a uniform grid with spacing $h$ has stencil $\\left(-\\frac{1}{h^2}, \\frac{2}{h^2}, -\\frac{1}{h^2}\\right)$. Weighted Jacobi relaxation has iteration matrix $S = I - \\omega D^{-1} A$ with $D = \\frac{2}{h^2} I$ and $\\omega \\in (0,2)$.\n\n- To study smoothing precisely, Local Fourier Analysis (LFA) considers periodic boundary conditions on an infinite grid as an idealization. A Fourier mode $\\varphi_\\theta(j) = e^{\\mathrm{i}\\theta j}$ is an eigenvector of the symbol of $S$ with eigenvalue $\\tilde{S}(\\theta) = 1 - \\omega \\tilde{D}^{-1}(\\theta)\\tilde{A}(\\theta)$. For this model, $\\tilde{D}^{-1}(\\theta) \\tilde{A}(\\theta) = 1 - \\cos \\theta$. Hence one sweep of weighted Jacobi multiplies the amplitude of mode $\\theta$ by $1 - \\omega (1 - \\cos \\theta)$.\n\n- High-frequency modes relative to the fine grid are those with $\\theta$ near $\\pi$, for which $1 - \\cos \\theta$ is close to $2$. With a classical choice $\\omega = \\frac{2}{3}$, one obtains the high-frequency damping factor at $\\theta = \\pi$ equal to $|1 - 2 \\omega| = \\left|1 - \\frac{4}{3}\\right| = \\frac{1}{3}$. More generally, for any fixed $\\omega \\in (0,2)$ the magnitude $|\\tilde{S}(\\theta)|$ is strictly less than $1$ for all $\\theta \\in (0,\\pi)$, and the smoothing factor over the high-frequency set is bounded away from $1$ for a suitable $\\omega$ (for instance, $\\omega$ near $\\frac{2}{3}$).\n\n- The coarse space given by classical coarsening by a factor of $2$ with linear interpolation is able to represent the low-frequency components of the error. In the LFA framework, coarse-grid correction eliminates, up to aliasing, the low-frequency spectral components that lie in the range of $P$. With full-weighting restriction, linear interpolation, exact coarse solve, and one pre- and one post-smoothing sweep, standard LFA shows that the two-grid convergence factor is bounded strictly below $1$ independently of $h$ for an appropriate $\\omega$ (e.g., $\\omega = \\frac{2}{3}$).\n\nThe reasoning does not rely on any property that depends on $h$, because the Fourier symbols and the aliasing between fine and coarse grids are defined over $\\theta \\in [-\\pi,\\pi]$ independent of $h$. Therefore, there exists $\\omega \\in (0,2)$ such that $\\rho(E) \\le \\eta  1$ with $\\eta$ independent of $h$. Verdict for A: Correct.\n\nOption B. Replace linear interpolation with injection and claim uniform convergence because the smoother removes low frequencies.\n\nStart from the same two-grid structure, but set $P$ to be injection (piecewise constant) from coarse points to fine points. The approximation property needed for effective coarse-grid correction is that any sufficiently smooth (low-frequency) error $e$ on the fine grid can be well approximated by a vector in the coarse space $\\mathrm{range}(P)$. Injection fails this property for the one-dimensional Poisson model:\n\n- Consider a fine-grid error that varies slowly across the grid, such as a discrete cosine mode with small wavenumber $\\theta \\approx 0$, or more concretely $e_j = \\cos(\\theta j)$ with $\\theta = O(h)$. This is a low-frequency error component relative to the fine grid, and it is precisely the component that relaxation does not substantially reduce, because for $\\theta \\approx 0$ the Jacobi smoothing factor is $|1 - \\omega(1 - \\cos \\theta)| \\approx |1 - \\omega \\cdot \\tfrac{\\theta^2}{2}| \\approx 1 - O(\\theta^2)$, i.e., close to $1$.\n\n- With injection, the coarse space consists of vectors that are piecewise constant between coarse points. Such functions cannot approximate a smooth, slowly varying fine-grid error with $O(1)$ accuracy uniformly in $h$; the best approximation error of a smooth mode by piecewise constants decays only as the coarse mesh resolves the variation, and the coarse correction will leave a significant component of the smooth error untouched when $h$ decreases with a fixed coarsening factor.\n\n- Formally, if $e \\notin \\mathrm{range}(P)$ (as is the case for low-frequency smooth functions when $P$ is injection), then $e - P A_c^{-1} R A e$ generally retains a component along the low-frequency mode, because $R A e$ is small and $A_c^{-1}$ amplifies it, but the projector $I - P A_c^{-1} R A$ is only exact on $\\mathrm{range}(P)$.\n\nThus, the key premise in B, that the smoother removes low-frequency components, is incorrect: weighted Jacobi removes high frequencies; it does not efficiently remove low frequencies. Without a coarse space that approximates the low-frequency error, uniform two-grid convergence is lost. Verdict for B: Incorrect.\n\nOption C. Anisotropy, strength-of-connection threshold too large, loss of grid independence as $\\varepsilon \\to 0$.\n\nConsider the operator $-\\partial_{xx} - \\varepsilon \\partial_{yy}$ with $0  \\varepsilon \\ll 1$ discretized on a uniform grid. The resulting matrix $A$ has off-diagonal couplings corresponding to the $x$-direction of size approximately $\\frac{1}{h_x^2}$ and to the $y$-direction of size approximately $\\frac{\\varepsilon}{h_y^2}$. A strength-of-connection (SoC) test declares $j$ strongly connected to $i$ if $|a_{ji}| \\ge \\theta \\max_{k \\ne j} |a_{jk}|$ for $\\theta \\in (0,1)$. If $\\theta$ is chosen close to $1$, then only neighbors with magnitude comparable to the row maximum are kept as strong; in the anisotropic case, when $\\varepsilon \\ll 1$, the couplings in the weak direction $y$ have magnitude $O(\\varepsilon)$ and so are dropped when $\\theta$ is large.\n\nClassical algebraic multigrid with interpolation aligned to strong connections constructs $P$ so that a fine point interpolates primarily from its strong neighbors. When $\\theta$ is too large, the strong-coupling graph may become effectively one-dimensional, or even disconnected, in a way that does not faithfully capture the continuous anisotropy. Then the range of $P$ fails to approximate the near-nullspace of $A$ (the slowly varying error along the weak direction), violating the approximation property. As $\\varepsilon \\to 0$, the operator becomes increasingly anisotropic, and the inadequacy of $P$ becomes more severe, which verification on this model problem reveals as a deterioration of the two-grid convergence factor with mesh refinement and decreasing $\\varepsilon$. This phenomenon and its detection by model-problem verification are well established in algebraic multigrid practice. Verdict for C: Correct.\n\nOption D. Periodic one-dimensional problem, constant error eliminated exactly by coarse-grid correction with full weighting and linear interpolation.\n\nAssume the one-dimensional problem with periodic boundary conditions so that constant vectors are admissible error modes. Let $P$ be linear interpolation from the coarse grid, and $R$ be full weighting. Consider a constant error $e \\in \\mathbb{R}^n$ defined by $e_j = c$ for all fine-grid indices $j$, where $c \\in \\mathbb{R}$ is fixed. On a uniform grid, the coarse-grid constant vector $e_c$ defined by $(e_c)_J = c$ for each coarse index $J$ satisfies $P e_c = e$. Therefore, $e \\in \\mathrm{range}(P)$. For Galerkin coarsening, $A_c = R A P$, the coarse-grid correction operator is the $A$-orthogonal projector onto the $A$-orthogonal complement of $\\mathrm{range}(P)$, and for any $e \\in \\mathrm{range}(P)$ we have\n$$\n\\left(I - P A_c^{-1} R A\\right) e\n= \\left(I - P A_c^{-1} R A\\right) P e_c\n= P e_c - P A_c^{-1} (R A P) e_c\n= P e_c - P A_c^{-1} A_c e_c\n= 0.\n$$\nThus, any constant error component is eliminated exactly by the coarse-grid correction, independently of $h$, provided the coarse-grid problem is solved exactly. Verdict for D: Correct.\n\nSummary of option-by-option analysis:\n- A: Correct, by Local Fourier Analysis there exists $\\omega \\in (0,2)$, e.g., $\\omega = \\frac{2}{3}$, giving a two-grid contraction factor bounded away from $1$ independently of $h$.\n- B: Incorrect, because injection does not provide a coarse space that approximates low-frequency error, and relaxation does not remove low-frequency components.\n- C: Correct, an excessively large strength threshold can destroy the approximation property under strong anisotropy, and verification on the model problem detects the loss of grid independence as $\\varepsilon \\to 0$.\n- D: Correct, constant vectors lie in the range of $P$ and are removed exactly by Galerkin coarse-grid correction on a periodic grid.",
            "answer": "$$\\boxed{ACD}$$"
        }
    ]
}