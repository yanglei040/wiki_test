{
    "hands_on_practices": [
        {
            "introduction": "This exercise takes you to the heart of the symmetric Galerkin method. You will construct the fundamental mass and stiffness matrices, $M$ and $K$, using Chebyshev polynomials, which are the building blocks of many spectral methods. This practice is essential for understanding how a continuous differential operator is transformed into a discrete algebraic system and provides insight into the numerical properties, such as conditioning, of that system. ",
            "id": "3398055",
            "problem": "Consider the symmetric Galerkin method for a one-dimensional second-order linear elliptic operator on the interval $[-1,1]$ in the modal basis of Chebyshev polynomials of the first kind. Let the trial and test spaces be spanned by $\\{T_{0}(x),T_{1}(x),T_{2}(x)\\}$, where $T_{m}(x) = \\cos(m \\arccos(x))$. Define the mass matrix $M \\in \\mathbb{R}^{3 \\times 3}$ and the stiffness matrix $K \\in \\mathbb{R}^{3 \\times 3}$ by\n$$\nM_{mn} = \\int_{-1}^{1} T_{m}(x)\\,T_{n}(x)\\,dx, \\qquad\nK_{mn} = \\int_{-1}^{1} T_{m}'(x)\\,T_{n}'(x)\\,dx,\n$$\nfor $m,n \\in \\{0,1,2\\}$, and approximate these integrals by Clenshaw–Curtis quadrature (CC). Use $N=4$ Chebyshev–Lobatto nodes $x_{k} = \\cos\\!\\left(\\frac{k\\pi}{4}\\right)$, $k=0,1,2,3,4$, and determine the quadrature weights $\\{w_{k}\\}_{k=0}^{4}$ by the requirement that the rule is exact for all polynomials up to degree $4$.\n\nYou may use any equivalent representation to evaluate the resulting entries, but you must justify exactness of the CC quadrature for these specific entries. Then, restrict attention to the nonconstant subspace $\\operatorname{span}\\{T_{1},T_{2}\\}$, where the stiffness matrix is nonsingular, and form the generalized eigenvalue operator $A = M^{-1}K$ on this two-dimensional subspace. Finally, quantify the impact of endpoint clustering on numerical conditioning by computing the spectral condition number of $A$ on $\\operatorname{span}\\{T_{1},T_{2}\\}$, defined as the ratio of its largest to smallest eigenvalue on this subspace. Express your final answer as a single exact rational number with no units.",
            "solution": "We begin from the core definitions of the Galerkin method and Chebyshev polynomials. The Chebyshev polynomials of the first kind satisfy $T_{m}(x) = \\cos(m\\theta)$ with $x=\\cos\\theta$ for $\\theta \\in [0,\\pi]$. Their derivatives with respect to $x$ are given by\n$$\n\\frac{d}{dx}T_{m}(x) = m\\,U_{m-1}(x),\n$$\nwhere $U_{n}$ are the Chebyshev polynomials of the second kind derived from $U_{n}(\\cos\\theta)=\\frac{\\sin((n+1)\\theta)}{\\sin\\theta}$.\n\nClenshaw–Curtis quadrature (CC) uses Chebyshev–Lobatto nodes $x_{k}=\\cos\\!\\left(\\frac{k\\pi}{N}\\right)$ for $k=0,\\dots,N$ and assigns weights $\\{w_{k}\\}$ so that the interpolatory quadrature is exact for all polynomials up to degree $N$. For $N=4$, the nodes are\n$$\nx_{0}=1,\\quad x_{1}=\\frac{\\sqrt{2}}{2},\\quad x_{2}=0,\\quad x_{3}=-\\frac{\\sqrt{2}}{2},\\quad x_{4}=-1.\n$$\nWe determine weights $w_{k}$ so that, for $p=0,1,2,3,4$,\n$$\n\\sum_{k=0}^{4} w_{k}\\,x_{k}^{p} = \\int_{-1}^{1} x^{p}\\,dx =\n\\begin{cases}\n\\frac{2}{p+1}, & p \\text{ even},\\\\\n0, & p \\text{ odd}.\n\\end{cases}\n$$\nBy symmetry of the nodes, it is natural to seek $w_{0}=w_{4}=a$, $w_{1}=w_{3}=b$, $w_{2}=c$. The odd-moment equations are automatically satisfied when $w_{k}$ are symmetric. The even-moment equations yield\n$$\n\\begin{aligned}\n&2a+2b+c=2,\\\\\n&2a+b=\\frac{2}{3},\\\\\n&2a+\\frac{b}{2}=\\frac{2}{5}.\n\\end{aligned}\n$$\nSolving this system gives $b=\\frac{8}{15}$, $a=\\frac{1}{15}$, and $c=\\frac{4}{5}$. Therefore, the CC weights are\n$$\nw_{0}=w_{4}=\\frac{1}{15},\\qquad w_{1}=w_{3}=\\frac{8}{15},\\qquad w_{2}=\\frac{4}{5}.\n$$\nSince the basis functions are polynomials of degree at most $2$, and products $T_{m}T_{n}$ have degree at most $4$, and similarly $T_{m}'\\,T_{n}'$ have degree at most $2$ for $m,n\\in\\{0,1,2\\}$, the CC quadrature with $N=4$ is exact for all entries of $M$ and $K$. Thus we can compute $M$ and $K$ via analytic integrals, secure in the knowledge the quadrature would reproduce them exactly.\n\nTo evaluate the mass matrix entries, we use the change of variables $x=\\cos\\theta$, $dx=-\\sin\\theta\\,d\\theta$, and write\n$$\nM_{mn} = \\int_{-1}^{1} T_{m}(x)\\,T_{n}(x)\\,dx\n= \\int_{0}^{\\pi} \\cos(m\\theta)\\,\\cos(n\\theta)\\,\\sin\\theta\\,d\\theta.\n$$\nUsing the product-to-sum identity $\\cos(m\\theta)\\cos(n\\theta) = \\frac{1}{2}\\left[\\cos((m-n)\\theta)+\\cos((m+n)\\theta)\\right]$, we reduce the integral to\n$$\nM_{mn} = \\frac{1}{2}\\left[J(|m-n|) + J(m+n)\\right],\n$$\nwhere\n$$\nJ(k) = \\int_{0}^{\\pi} \\cos(k\\theta)\\,\\sin\\theta\\,d\\theta.\n$$\nWe compute $J(k)$ by the identity $\\sin\\theta\\cos(k\\theta)=\\frac{1}{2}\\left[\\sin((k+1)\\theta)+\\sin((1-k)\\theta)\\right]$ and use\n$$\n\\int_{0}^{\\pi} \\sin(\\ell\\theta)\\,d\\theta =\n\\begin{cases}\n\\frac{2}{\\ell}, & \\ell \\text{ odd},\\\\\n0, & \\ell \\text{ even}.\n\\end{cases}\n$$\nThis yields, for integer $k\\geq 0$,\n$$\nJ(k) =\n\\begin{cases}\n2, & k=0,\\\\\n0, & k \\text{ odd},\\\\\n\\frac{2}{1-k^{2}}, & k \\text{ even},\\ k\\geq 2.\n\\end{cases}\n$$\nThus, for $m,n\\in\\{0,1,2\\}$, we obtain\n$$\n\\begin{aligned}\n&M_{00}=\\frac{1}{2}[J(0)+J(0)]=2,\\\\\n&M_{01}=\\frac{1}{2}[J(1)+J(1)]=0,\\\\\n&M_{02}=\\frac{1}{2}[J(2)+J(2)]=\\frac{1}{2}\\left(-\\frac{2}{3}-\\frac{2}{3}\\right)=-\\frac{2}{3},\\\\\n&M_{11}=\\frac{1}{2}[J(0)+J(2)]=\\frac{1}{2}\\left(2-\\frac{2}{3}\\right)=\\frac{2}{3},\\\\\n&M_{12}=\\frac{1}{2}[J(1)+J(3)]=0,\\\\\n&M_{22}=\\frac{1}{2}[J(0)+J(4)]=\\frac{1}{2}\\left(2-\\frac{2}{15}\\right)=\\frac{14}{15}.\n\\end{aligned}\n$$\nTherefore,\n$$\nM = \\begin{pmatrix}\n2 & 0 & -\\frac{2}{3}\\\\\n0 & \\frac{2}{3} & 0\\\\\n-\\frac{2}{3} & 0 & \\frac{14}{15}\n\\end{pmatrix}.\n$$\n\nFor the stiffness matrix, we use the derivative representation and the same change of variables:\n$$\nK_{mn} = \\int_{-1}^{1} T_{m}'(x)\\,T_{n}'(x)\\,dx = \\int_{0}^{\\pi} \\frac{m\\,\\sin(m\\theta)}{\\sin\\theta}\\cdot \\frac{n\\,\\sin(n\\theta)}{\\sin\\theta}\\,\\sin\\theta\\,d\\theta\n= m n \\int_{0}^{\\pi} \\frac{\\sin(m\\theta)\\sin(n\\theta)}{\\sin\\theta}\\,d\\theta.\n$$\nFor $m,n\\in\\{0,1,2\\}$, we compute directly:\n- For $m=0$ or $n=0$, $\\sin(0\\theta)=0$, so $K_{0n}=K_{n0}=0$.\n- For $m=n=1$,\n$$\nK_{11} = 1\\cdot 1 \\int_{0}^{\\pi} \\frac{\\sin\\theta\\,\\sin\\theta}{\\sin\\theta}\\,d\\theta\n= \\int_{0}^{\\pi} \\sin\\theta\\,d\\theta = 2.\n$$\n- For $m=1,n=2$,\n$$\nK_{12} = 1\\cdot 2 \\int_{0}^{\\pi} \\frac{\\sin\\theta\\,\\sin(2\\theta)}{\\sin\\theta}\\,d\\theta\n= 2 \\int_{0}^{\\pi} \\sin(2\\theta)\\,d\\theta = 0.\n$$\n- For $m=n=2$,\n$$\nK_{22} = 2\\cdot 2 \\int_{0}^{\\pi} \\frac{\\sin(2\\theta)\\,\\sin(2\\theta)}{\\sin\\theta}\\,d\\theta\n= 4 \\int_{0}^{\\pi} \\frac{(2\\sin\\theta\\cos\\theta)^{2}}{\\sin\\theta}\\,d\\theta\n= 16 \\int_{0}^{\\pi} \\sin\\theta\\,\\cos^{2}\\theta\\,d\\theta.\n$$\nLet $u=\\cos\\theta$, $du=-\\sin\\theta\\,d\\theta$, and $\\theta:0\\to\\pi$ corresponds to $u:1\\to -1$. Then\n$$\nK_{22} = 16 \\int_{0}^{\\pi} \\sin\\theta\\,\\cos^{2}\\theta\\,d\\theta\n= -16 \\int_{1}^{-1} u^{2}\\,du\n= 16 \\int_{-1}^{1} u^{2}\\,du\n= 16 \\cdot \\frac{2}{3} = \\frac{32}{3}.\n$$\nTherefore,\n$$\nK = \\begin{pmatrix}\n0 & 0 & 0\\\\\n0 & 2 & 0\\\\\n0 & 0 & \\frac{32}{3}\n\\end{pmatrix}.\n$$\n\nWe now restrict to the nonconstant subspace $\\operatorname{span}\\{T_{1},T_{2}\\}$ where the stiffness is nonsingular. On this subspace, the mass matrix is diagonal with entries $M_{11}=\\frac{2}{3}$ and $M_{22}=\\frac{14}{15}$, and the stiffness matrix is diagonal with entries $K_{11}=2$ and $K_{22}=\\frac{32}{3}$. Hence the generalized operator $A=M^{-1}K$ is diagonal with eigenvalues\n$$\n\\lambda_{1} = \\frac{K_{11}}{M_{11}} = \\frac{2}{2/3} = 3,\\qquad\n\\lambda_{2} = \\frac{K_{22}}{M_{22}} = \\frac{32/3}{14/15} = \\frac{32}{3}\\cdot\\frac{15}{14} = \\frac{80}{7}.\n$$\nThe spectral condition number of $A$ on $\\operatorname{span}\\{T_{1},T_{2}\\}$ is the ratio of its largest to smallest eigenvalues,\n$$\n\\kappa(A) = \\frac{\\max\\{\\lambda_{1},\\lambda_{2}\\}}{\\min\\{\\lambda_{1},\\lambda_{2}\\}} = \\frac{80/7}{3} = \\frac{80}{21}.\n$$\n\nInterpretation regarding endpoint clustering: Chebyshev–Lobatto nodes cluster near $x=\\pm 1$, which weights function values and derivatives more heavily near the endpoints when measured in the energy inner product. As the polynomial degree increases, derivatives of $T_{m}$ concentrate increasingly near the endpoints, and the relative magnitude of $K_{mm}/M_{mm}$ grows with $m$, inflating the spread of the eigenvalues of $A$. In this low-order example, this manifests in $\\lambda_{2}>\\lambda_{1}$ by a factor $\\frac{80}{21}$, quantifying the impact of endpoint clustering on conditioning of the generalized eigenvalue problem associated with the Galerkin discretization using Clenshaw–Curtis quadrature.",
            "answer": "$$\\boxed{\\frac{80}{21}}$$"
        },
        {
            "introduction": "Once a numerical method is implemented, how can we be sure it is correct? This practice introduces the Method of Manufactured Solutions (MMS), a powerful and widely-used technique for code verification. By choosing a solution beforehand and deriving the corresponding problem data, you create a test case with a known answer, allowing you to rigorously validate your spectral Galerkin, tau, or collocation implementations. ",
            "id": "3397996",
            "problem": "Consider the one-dimensional, variable-coefficient, second-order elliptic operator on the interval $[-1,1]$ defined by\n$$\n\\mathcal{L}u(x) \\;=\\; -\\frac{\\mathrm{d}}{\\mathrm{d}x}\\Big(a(x)\\,u'(x)\\Big) \\;+\\; c(x)\\,u(x),\n$$\nwith smooth positive coefficient $a(x)=2+x$ and smooth nonnegative coefficient $c(x)=1+x^2$. In the method of manufactured solutions, choose a smooth target solution\n$$\nu(x) \\;=\\; \\exp(x) \\;+\\; \\sin\\!\\Big(\\frac{\\pi x}{2}\\Big).\n$$\nYour tasks are:\n- Derive the source term $f(x)$ so that the strong form $\\mathcal{L}u(x)=f(x)$ holds identically on $[-1,1]$.\n- Derive consistent Dirichlet boundary conditions by specifying $u(-1)$ and $u(1)$.\n- For verification of spectral Galerkin, tau, and collocation formulations using Legendre polynomials, compute the first Legendre coefficient of the source term $f(x)$ under the standard $L^2([-1,1])$ normalization. Specifically, with $P_1(x)=x$ and the coefficient defined by\n$$\n\\widehat{f}_1 \\;=\\; \\frac{2\\cdot 1 + 1}{2}\\int_{-1}^{1} f(x)\\,P_1(x)\\,\\mathrm{d}x \\;=\\; \\frac{3}{2}\\int_{-1}^{1} f(x)\\,x\\,\\mathrm{d}x,\n$$\ndetermine a closed-form analytic expression for $\\widehat{f}_1$.\n\nProvide all derivations starting from product rule and orthogonality principles. Express the final answer for $\\widehat{f}_1$ in exact form using elementary functions and constants (no numerical rounding).",
            "solution": "The problem is valid as it is mathematically well-posed, scientifically grounded in the theory of differential equations and numerical analysis, and provides all necessary information to derive a unique solution without ambiguity or contradiction.\n\nThe tasks are to derive the source term $f(x)$ for a manufactured solution, determine the corresponding boundary conditions, and compute the first Legendre coefficient of the source term.\n\n### Task 1: Derive the source term $f(x)$\nThe differential operator is given by\n$$\n\\mathcal{L}u(x) = -\\frac{\\mathrm{d}}{\\mathrm{d}x}\\Big(a(x)\\,u'(x)\\Big) + c(x)\\,u(x),\n$$\nwith coefficients $a(x) = 2+x$ and $c(x) = 1+x^2$. The manufactured solution is $u(x) = \\exp(x) + \\sin(\\frac{\\pi x}{2})$. The source term $f(x)$ is defined by the equation $\\mathcal{L}u(x) = f(x)$.\n\nFirst, we compute the first derivative of $u(x)$:\n$$\nu'(x) = \\frac{\\mathrm{d}}{\\mathrm{d}x}\\left(\\exp(x) + \\sin\\left(\\frac{\\pi x}{2}\\right)\\right) = \\exp(x) + \\frac{\\pi}{2}\\cos\\left(\\frac{\\pi x}{2}\\right).\n$$\nNext, we form the product $a(x)u'(x)$ and differentiate it using the product rule:\n\\begin{align*}\n\\frac{\\mathrm{d}}{\\mathrm{d}x}\\Big(a(x)u'(x)\\Big) &= \\frac{\\mathrm{d}}{\\mathrm{d}x}\\left((2+x)\\left(\\exp(x) + \\frac{\\pi}{2}\\cos\\left(\\frac{\\pi x}{2}\\right)\\right)\\right) \\\\\n&= 1 \\cdot \\left(\\exp(x) + \\frac{\\pi}{2}\\cos\\left(\\frac{\\pi x}{2}\\right)\\right) + (2+x) \\cdot \\left(\\exp(x) - \\frac{\\pi^2}{4}\\sin\\left(\\frac{\\pi x}{2}\\right)\\right) \\\\\n&= (3+x)\\exp(x) + \\frac{\\pi}{2}\\cos\\left(\\frac{\\pi x}{2}\\right) - \\frac{\\pi^2}{4}(2+x)\\sin\\left(\\frac{\\pi x}{2}\\right).\n\\end{align*}\nThe second part of the operator is $c(x)u(x)$:\n$$\nc(x)u(x) = (1+x^2)\\left(\\exp(x) + \\sin\\left(\\frac{\\pi x}{2}\\right)\\right) = (1+x^2)\\exp(x) + (1+x^2)\\sin\\left(\\frac{\\pi x}{2}\\right).\n$$\nCombining these results gives the source term $f(x) = -\\frac{\\mathrm{d}}{\\mathrm{d}x}\\big(a(x)u'(x)\\big) + c(x)u(x)$:\n\\begin{align*}\nf(x) &= -\\left((3+x)\\exp(x) + \\frac{\\pi}{2}\\cos\\left(\\frac{\\pi x}{2}\\right) - \\frac{\\pi^2}{4}(2+x)\\sin\\left(\\frac{\\pi x}{2}\\right)\\right) + (1+x^2)\\exp(x) + (1+x^2)\\sin\\left(\\frac{\\pi x}{2}\\right) \\\\\n&= (x^2-x-2)\\exp(x) - \\frac{\\pi}{2}\\cos\\left(\\frac{\\pi x}{2}\\right) + \\left(x^2 + \\frac{\\pi^2}{4}x + 1 + \\frac{\\pi^2}{2}\\right)\\sin\\left(\\frac{\\pi x}{2}\\right).\n\\end{align*}\n\n### Task 2: Derive Dirichlet boundary conditions\nThe Dirichlet boundary conditions are found by evaluating the manufactured solution $u(x)$ at the endpoints of the interval $[-1, 1]$:\n$$\nu(-1) = \\exp(-1) + \\sin\\left(-\\frac{\\pi}{2}\\right) = \\frac{1}{e} - 1.\n$$\n$$\nu(1) = \\exp(1) + \\sin\\left(\\frac{\\pi}{2}\\right) = e + 1.\n$$\n\n### Task 3: Compute the Legendre coefficient $\\widehat{f}_1$\nThe first Legendre coefficient of $f(x)$ is defined as:\n$$\n\\widehat{f}_1 = \\frac{3}{2}\\int_{-1}^{1} f(x)P_1(x)\\,\\mathrm{d}x = \\frac{3}{2}\\int_{-1}^{1} f(x)x\\,\\mathrm{d}x.\n$$\nSince $f(x) = \\mathcal{L}u(x)$, we have:\n$$\n\\int_{-1}^{1} f(x)x\\,\\mathrm{d}x = \\int_{-1}^{1} \\mathcal{L}u(x) \\cdot x\\,\\mathrm{d}x = \\int_{-1}^{1} \\left(-\\frac{\\mathrm{d}}{\\mathrm{d}x}\\Big(a(x)u'(x)\\Big) + c(x)u(x)\\right)x\\,\\mathrm{d}x.\n$$\nWe apply integration by parts to the first term, with $v(x) = x$:\n$$\n\\int_{-1}^{1} -\\frac{\\mathrm{d}}{\\mathrm{d}x}\\Big(a(x)u'(x)\\Big) v(x)\\,\\mathrm{d}x = \\left[-a(x)u'(x)v(x)\\right]_{-1}^{1} + \\int_{-1}^{1} a(x)u'(x)v'(x)\\,\\mathrm{d}x.\n$$\nWith $v(x) = x$ and $v'(x) = 1$, the integral for the coefficient becomes:\n$$\n\\int_{-1}^{1} f(x)x\\,\\mathrm{d}x = \\left[-a(x)u'(x)x\\right]_{-1}^{1} + \\int_{-1}^{1} \\left(a(x)u'(x) \\cdot 1 + c(x)u(x)x\\right)\\,\\mathrm{d}x.\n$$\nThe boundary term is:\n\\begin{align*}\n\\left[-a(x)u'(x)x\\right]_{-1}^{1} &= -a(1)u'(1)(1) - (-a(-1)u'(-1)(-1)) \\\\\n&= -a(1)u'(1) - a(-1)u'(-1).\n\\end{align*}\nWith $a(1)=3$, $a(-1)=1$, $u'(1)=e$, and $u'(-1)=e^{-1}$, the boundary term is $-3e - e^{-1}$.\n\nThe integral term is $I = \\int_{-1}^{1} \\left(a(x)u'(x) + c(x)u(x)x\\right)\\,\\mathrm{d}x$. We evaluate the integral of each component over the symmetric interval $[-1, 1]$:\n1.  The exponential terms: $\\int_{-1}^{1} \\left((2+x)e^x + (x+x^3)e^x\\right)\\,\\mathrm{d}x = \\int_{-1}^{1} (x^3+2x+2)e^x\\,\\mathrm{d}x$. Using integration by parts, the antiderivative is $[x^3-3x^2+8x-6]e^x$. Evaluating from $-1$ to $1$ gives:\n    $$ \\left[x^3-3x^2+8x-6\\right]e^x\\Big|_{-1}^{1} = (1-3+8-6)e - (-1-3-8-6)e^{-1} = 18e^{-1}. $$\n2.  The cosine terms: $\\int_{-1}^{1} (2+x)\\frac{\\pi}{2}\\cos\\left(\\frac{\\pi x}{2}\\right)\\,\\mathrm{d}x = \\int_{-1}^{1} \\pi\\cos\\left(\\frac{\\pi x}{2}\\right)\\,\\mathrm{d}x + \\int_{-1}^{1} \\frac{\\pi}{2}x\\cos\\left(\\frac{\\pi x}{2}\\right)\\,\\mathrm{d}x$. The second integral is of an odd function and is zero. The first integral is:\n    $$ \\left[2\\sin\\left(\\frac{\\pi x}{2}\\right)\\right]_{-1}^{1} = 2(\\sin(\\frac{\\pi}{2}) - \\sin(-\\frac{\\pi}{2})) = 4. $$\n3.  The sine term: $\\int_{-1}^{1} (x+x^3)\\sin\\left(\\frac{\\pi x}{2}\\right)\\,\\mathrm{d}x$. This is an even function. Let $k=\\pi/2$. Using integration by parts:\n    $$ \\int_{0}^{1} x\\sin(kx)\\,\\mathrm{d}x = \\frac{\\sin k}{k^2} - \\frac{\\cos k}{k} = \\frac{1}{k^2} = \\frac{4}{\\pi^2}. $$\n    $$ \\int_{0}^{1} x^3\\sin(kx)\\,\\mathrm{d}x = \\left(\\frac{3}{k^2}-\\frac{6}{k^4}\\right)\\sin k - \\left(\\frac{1}{k}-\\frac{6}{k^3}\\right)\\cos k = \\frac{3}{k^2}-\\frac{6}{k^4} = \\frac{12}{\\pi^2} - \\frac{96}{\\pi^4}. $$\n    The full integral is $2\\int_{0}^{1}(\\dots)\\,\\mathrm{d}x = 2\\left(\\frac{4}{\\pi^2} + \\frac{12}{\\pi^2} - \\frac{96}{\\pi^4}\\right) = \\frac{32}{\\pi^2} - \\frac{192}{\\pi^4}$.\nSumming all integral parts: $I = 18e^{-1} + 4 + \\frac{32}{\\pi^2} - \\frac{192}{\\pi^4}$.\n\nThe total value of $\\int_{-1}^{1} f(x)x\\,\\mathrm{d}x$ is the sum of the boundary and integral contributions:\n$$\n\\int_{-1}^{1} f(x)x\\,\\mathrm{d}x = (-3e - e^{-1}) + (18e^{-1} + 4 + \\frac{32}{\\pi^2} - \\frac{192}{\\pi^4}) = -3e + 17e^{-1} + 4 + \\frac{32}{\\pi^2} - \\frac{192}{\\pi^4}.\n$$\nFinally, we multiply by $\\frac{3}{2}$ to get $\\widehat{f}_1$:\n$$\n\\widehat{f}_1 = \\frac{3}{2}\\left(-3e + 17e^{-1} + 4 + \\frac{32}{\\pi^2} - \\frac{192}{\\pi^4}\\right) = 6 - \\frac{9}{2}e + \\frac{51}{2}e^{-1} + \\frac{48}{\\pi^2} - \\frac{288}{\\pi^4}.\n$$",
            "answer": "$$\n\\boxed{6 - \\frac{9}{2}\\exp(1) + \\frac{51}{2}\\exp(-1) + \\frac{48}{\\pi^2} - \\frac{288}{\\pi^4}}\n$$"
        },
        {
            "introduction": "Spectral methods can represent functions in different ways, most commonly through interpolation at specific nodes or via orthogonal projection onto a basis. This hands-on coding exercise illuminates the crucial difference between these two approaches by comparing their resulting Chebyshev coefficients. The discrepancy you will measure is known as aliasing error, a key concept for understanding the accuracy and convergence properties of spectral methods. ",
            "id": "3398051",
            "problem": "Let $[-1,1]$ be the computational domain, and let $\\{x_j\\}_{j=0}^N$ be the Chebyshev–Gauss–Lobatto nodes defined by $x_j = \\cos\\left(\\frac{\\pi j}{N}\\right)$ for integer $N \\ge 0$. Consider a smooth scalar function $f:[-1,1]\\to\\mathbb{R}$. The Lagrange interpolating polynomial of degree at most $N$ in barycentric form is constructed from the samples $f_j = f(x_j)$ as\n$$\np_N(x) = \\frac{\\displaystyle\\sum_{j=0}^N \\frac{w_j f_j}{x - x_j}}{\\displaystyle\\sum_{j=0}^N \\frac{w_j}{x - x_j}},\n$$\nwhere the barycentric weights $\\{w_j\\}_{j=0}^N$ are derived from the fundamental Lagrange polynomials by $w_j = \\left(\\prod_{m\\ne j}(x_j - x_m)\\right)^{-1}$, specialized to the Chebyshev–Gauss–Lobatto choice. The Chebyshev polynomials of the first kind $\\{T_k\\}_{k=0}^\\infty$ are defined by $T_k(\\cos\\theta) = \\cos(k\\theta)$ for all real $\\theta$ (angles measured in radians). For any polynomial $q_N(x)$ of degree at most $N$, we can expand it in the Chebyshev basis as\n$$\nq_N(x) = \\sum_{k=0}^N a_k T_k(x),\n$$\nwith unique coefficients $\\{a_k\\}_{k=0}^N$.\n\nDefine the weighted inner product associated with Chebyshev polynomials by\n$$\n\\langle u, v \\rangle = \\int_{-1}^1 \\frac{u(x)\\, v(x)}{\\sqrt{1-x^2}}\\, dx,\n$$\nwhich yields the orthogonality\n$$\n\\int_{-1}^1 \\frac{T_m(x)\\, T_n(x)}{\\sqrt{1-x^2}}\\, dx = \n\\begin{cases}\n\\pi, & n=m=0,\\\\\n\\frac{\\pi}{2}, & n=m\\ge 1,\\\\\n0, & n\\ne m.\n\\end{cases}\n$$\nThe orthogonal projection of $f$ onto $\\text{span}\\{T_0,\\dots,T_N\\}$ has coefficients $\\{a_k^{(\\mathrm{proj})}\\}_{k=0}^N$ satisfying\n$$\na_0^{(\\mathrm{proj})} = \\frac{1}{\\pi}\\int_{-1}^1 \\frac{f(x)}{\\sqrt{1-x^2}}\\, dx,\\qquad\na_k^{(\\mathrm{proj})} = \\frac{2}{\\pi}\\int_{-1}^1 \\frac{f(x)\\, T_k(x)}{\\sqrt{1-x^2}}\\, dx,\\quad k\\ge 1.\n$$\n\nTask: For each test case below, do the following.\n- Using the Chebyshev–Gauss–Lobatto samples $\\{f_j\\}_{j=0}^N$, construct the barycentric interpolant $p_N(x)$. Then compute the Chebyshev coefficients $\\{a_k^{(\\mathrm{interp})}\\}_{k=0}^N$ of $p_N(x)$ by solving the square linear system obtained from evaluating the Chebyshev basis at the Chebyshev–Gauss–Lobatto nodes, using the identity $T_k(x_j)=\\cos\\!\\left(\\frac{\\pi k j}{N}\\right)$.\n- Compute the orthogonal projection coefficients $\\{a_k^{(\\mathrm{proj})}\\}_{k=0}^N$ via high-accuracy numerical quadrature using the $M$-point Gauss–Chebyshev rule for the weight $1/\\sqrt{1-x^2}$, with nodes $x_j^{(\\mathrm{GC})}=\\cos\\!\\left(\\frac{(2j-1)\\pi}{2M}\\right)$ for $j=1,\\dots,M$ and equal weights $\\omega_j=\\frac{\\pi}{M}$, yielding\n$$\na_0^{(\\mathrm{proj})} \\approx \\frac{1}{M}\\sum_{j=1}^M f\\!\\left(x_j^{(\\mathrm{GC})}\\right),\\qquad\na_k^{(\\mathrm{proj})} \\approx \\frac{2}{M}\\sum_{j=1}^M f\\!\\left(x_j^{(\\mathrm{GC})}\\right)\\, \\cos\\!\\left(k\\, \\frac{(2j-1)\\pi}{2M}\\right),\\quad k\\ge 1.\n$$\n- Report the maximum absolute coefficient discrepancy\n$$\nE_\\infty = \\max_{0\\le k\\le N}\\left|a_k^{(\\mathrm{interp})} - a_k^{(\\mathrm{proj})}\\right|.\n$$\n\nYour program should implement the above for the following test suite (angles are in radians):\n- Test $1$: $f(x) = \\sin(\\pi x)$ with $N=32$.\n- Test $2$: $f(x) = T_{10}(x) = \\cos\\!\\left(10\\, \\arccos x\\right)$ with $N=10$.\n- Test $3$: $f(x) = x$ with $N=1$.\n- Test $4$: $f(x) = e^{3x}$ with $N=48$.\n\nDesign for coverage:\n- Test $1$ is a smooth, non-polynomial \"happy path\" case.\n- Test $2$ is an exact polynomial edge case where interpolation and projection must match for $N\\ge 10$.\n- Test $3$ is a minimal-degree boundary case.\n- Test $4$ is a highly smooth analytic case with rapidly decaying but nonzero high-order coefficients.\n\nAnswer specification:\n- For each test case, compute $E_\\infty$ as a real number (float).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[r_1,r_2,r_3,r_4]\"), where each $r_i$ is the $E_\\infty$ corresponding to Test $i$.",
            "solution": "The problem is well-defined, scientifically sound, and consistent. It presents a standard numerical analysis task involving the comparison of Chebyshev coefficients obtained through two different methods: interpolation and projection. All mathematical formulas and concepts are standard in the field of spectral methods. The problem is therefore deemed **valid**.\n\nThe solution proceeds by implementing the computational steps described in the problem statement for each test case. The core of the task is to compute two sets of Chebyshev coefficients, $\\{a_k^{(\\mathrm{interp})}\\}_{k=0}^N$ and $\\{a_k^{(\\mathrm{proj})}\\}_{k=0}^N$, and then find the maximum absolute difference between them.\n\n### Step 1: Computation of Interpolation Coefficients $\\{a_k^{(\\mathrm{interp})}\\}_{k=0}^N$\n\nThe coefficients $\\{a_k^{(\\mathrm{interp})}\\}_{k=0}^N$ are for the polynomial $p_N(x) = \\sum_{k=0}^N a_k^{(\\mathrm{interp})} T_k(x)$ which interpolates the function $f(x)$ at the $N+1$ Chebyshev–Gauss–Lobatto (CGL) nodes, $\\{x_j\\}_{j=0}^N$. The interpolation condition is $p_N(x_j) = f(x_j)$ for $j=0, \\dots, N$. Substituting the Chebyshev expansion of $p_N(x)$ and using the identity $T_k(x_j) = \\cos\\left(\\frac{\\pi kj}{N}\\right)$, we obtain the linear system:\n$$\nf(x_j) = \\sum_{k=0}^N a_k^{(\\mathrm{interp})} \\cos\\left(\\frac{\\pi kj}{N}\\right), \\quad j=0, \\dots, N.\n$$\nThis is a formulation of the Discrete Cosine Transform of Type I (DCT-I). The problem asks to solve this system for the coefficients $a_k^{(\\mathrm{interp})}$. Instead of constructing and inverting the matrix explicitly, we can use the well-known formula for the inverse DCT-I, which gives the coefficients directly:\n$$\na_k^{(\\mathrm{interp})} = \\frac{2}{N \\bar{c}_k} \\sum_{j=0}^{N} \\frac{f(x_j)}{\\bar{c}_j} \\cos\\left(\\frac{\\pi jk}{N}\\right), \\quad k=0, \\dots, N.\n$$\nHere, $\\bar{c}_k$ are weights defined as $\\bar{c}_0 = \\bar{c}_N = 2$, and $\\bar{c}_k = 1$ for $1 \\le k \\le N-1$. The same definition applies to $\\bar{c}_j$. This formula is implemented to compute $\\{a_k^{(\\mathrm{interp})}\\}$.\n\n### Step 2: Computation of Projection Coefficients $\\{a_k^{(\\mathrm{proj})}\\}_{k=0}^N$\n\nThe coefficients $\\{a_k^{(\\mathrm{proj})}\\}_{k=0}^N$ correspond to the orthogonal projection of the function $f(x)$ onto the basis of Chebyshev polynomials $\\{T_k(x)\\}_{k=0}^N$. They are the \"true\" Chebyshev coefficients of $f(x)$, defined by the integrals:\n$$\na_0^{(\\mathrm{proj})} = \\frac{1}{\\pi}\\int_{-1}^1 \\frac{f(x)}{\\sqrt{1-x^2}}\\, dx, \\qquad\na_k^{(\\mathrm{proj})} = \\frac{2}{\\pi}\\int_{-1}^1 \\frac{f(x)\\, T_k(x)}{\\sqrt{1-x^2}}\\, dx,\\quad k\\ge 1.\n$$\nAs specified, these integrals are approximated using a high-accuracy $M$-point Gauss-Chebyshev quadrature rule of the first kind. The formulas for this approximation are given as:\n$$\na_0^{(\\mathrm{proj})} \\approx \\frac{1}{M}\\sum_{j=1}^M f\\!\\left(x_j^{(\\mathrm{GC})}\\right),\\qquad\na_k^{(\\mathrm{proj})} \\approx \\frac{2}{M}\\sum_{j=1}^M f\\!\\left(x_j^{(\\mathrm{GC})}\\right)\\, T_k\\!\\left(x_j^{(\\mathrm{GC})}\\right),\\quad k\\ge 1,\n$$\nwhere $x_j^{(\\mathrm{GC})}=\\cos\\left(\\frac{(2j-1)\\pi}{2M}\\right)$ are the Gauss-Chebyshev nodes. To ensure \"high-accuracy\", the number of quadrature points $M$ is chosen to be significantly larger than $N$ for any test case. A value of $M=2048$ is used, which renders the quadrature error negligible compared to the discrepancy being measured.\n\n### Step 3: Computation of the Maximum Absolute Discrepancy $E_\\infty$\n\nThe discrepancy $E_\\infty$ is a measure of the aliasing error, which arises because the interpolating polynomial's coefficients contain contributions from higher-order modes of the original function. It is computed as the maximum norm of the difference between the two coefficient vectors:\n$$\nE_\\infty = \\max_{0\\le k\\le N}\\left|a_k^{(\\mathrm{interp})} - a_k^{(\\mathrm{proj})}\\right|.\n$$\n\n### Analysis of Test Cases\n\n- **Test 1 ($f(x) = \\sin(\\pi x), N=32$) and Test 4 ($f(x) = e^{3x}, N=48$):** For these smooth, analytic, non-polynomial functions, the true Chebyshev coefficients $a_k^{(\\mathrm{proj})}$ decay rapidly as $k \\to \\infty$. The difference $a_k^{(\\mathrm{interp})} - a_k^{(\\mathrm{proj})}$ is due to aliasing, where higher-frequency coefficients (e.g., $a_{2N \\pm k}^{(\\mathrm{proj})}$) are folded onto the lower-frequency coefficients. A small but non-zero error $E_\\infty$ is expected.\n- **Test 2 ($f(x) = T_{10}(x), N=10$) and Test 3 ($f(x) = x = T_1(x), N=1$):** In these cases, the function $f(x)$ is a polynomial of degree $D$ and we are using an interpolation degree $N \\ge D$. The Lagrange interpolating polynomial of a degree $D$ polynomial is the polynomial itself. Therefore, $p_N(x) = f(x)$, and their Chebyshev coefficients must be identical: $a_k^{(\\mathrm{interp})} = a_k^{(\\mathrm{proj})}$. The Gauss-Chebyshev quadrature is exact for polynomial integrands of sufficiently low degree, which is satisfied here with the chosen $M$. Thus, for these cases, $E_\\infty$ is expected to be zero, or a very small number on the order of machine floating-point precision.\n\nThe implementation follows these steps for each test case provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    # Define test case functions\n    def f_test1(x):\n        \"\"\"Test 1: f(x) = sin(pi*x)\"\"\"\n        return np.sin(np.pi * x)\n\n    def f_test2(x):\n        \"\"\"Test 2: f(x) = T_10(x)\"\"\"\n        # Clip input to arccos's domain to handle potential floating point errors at endpoints\n        x_safe = np.clip(x, -1.0, 1.0)\n        return np.cos(10 * np.arccos(x_safe))\n\n    def f_test3(x):\n        \"\"\"Test 3: f(x) = x, which is T_1(x)\"\"\"\n        return x\n\n    def f_test4(x):\n        \"\"\"Test 4: f(x) = exp(3x)\"\"\"\n        return np.exp(3 * x)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (f_test1, 32),\n        (f_test2, 10),\n        (f_test3, 1),\n        (f_test4, 48),\n    ]\n\n    # Number of quadrature points for projection. Chosen to be large for high accuracy.\n    M_QUADRATURE = 2048\n\n    def compute_interp_coeffs(f, N):\n        \"\"\"\n        Computes the Chebyshev coefficients a_k^{(interp)} of the polynomial\n        interpolant of f at the N+1 Chebyshev-Gauss-Lobatto nodes.\n        This is done by solving the linear system p_N(x_j) = f(x_j) using\n        the inverse Discrete Cosine Transform of Type I (DCT-I).\n        \"\"\"\n        if N == 0:\n            return np.array([f(1.0)])\n        \n        j = np.arange(N + 1)\n        x_cgl = np.cos(np.pi * j / N)\n        f_vals = f(x_cgl)\n        \n        c_bar_j = np.ones(N + 1)\n        c_bar_j[0] = c_bar_j[N] = 2.0\n        f_vals_weighted = f_vals / c_bar_j\n        \n        a_interp = np.zeros(N + 1)\n        for k in range(N + 1):\n            cos_transform = np.cos(np.pi * j * k / N)\n            summation = np.sum(f_vals_weighted * cos_transform)\n            \n            c_bar_k = 2.0 if k == 0 or k == N else 1.0\n            a_interp[k] = (2.0 / N) * summation / c_bar_k\n            \n        return a_interp\n\n    def compute_proj_coeffs(f, N, M):\n        \"\"\"\n        Computes the Chebyshev coefficients a_k^{(proj)} of the orthogonal\n        projection of f. The integrals are approximated using an M-point\n        Gauss-Chebyshev quadrature rule.\n        \"\"\"\n        j_gc = np.arange(1, M + 1)\n        theta_gc = (2 * j_gc - 1) * np.pi / (2 * M)\n        x_gc = np.cos(theta_gc)\n        f_vals_gc = f(x_gc)\n        \n        a_proj = np.zeros(N + 1)\n        \n        # k = 0 case\n        a_proj[0] = np.mean(f_vals_gc)\n        \n        # k >= 1 cases\n        for k in range(1, N + 1):\n            cos_transform = np.cos(k * theta_gc)\n            a_proj[k] = (2.0 / M) * np.sum(f_vals_gc * cos_transform)\n            \n        return a_proj\n\n    results = []\n    for f, N in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        a_interp = compute_interp_coeffs(f, N)\n        a_proj = compute_proj_coeffs(f, N, M_QUADRATURE)\n        \n        e_inf = np.max(np.abs(a_interp - a_proj))\n        results.append(e_inf)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}