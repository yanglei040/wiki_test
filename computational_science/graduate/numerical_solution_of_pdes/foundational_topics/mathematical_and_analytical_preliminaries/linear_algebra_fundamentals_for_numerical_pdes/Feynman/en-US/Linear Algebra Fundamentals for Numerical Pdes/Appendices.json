{
    "hands_on_practices": [
        {
            "introduction": "Understanding the spectral properties of discrete operators is foundational to analyzing numerical methods for PDEs. This first exercise guides you through the complete eigen-analysis of the one-dimensional discrete Laplacian, a cornerstone of finite difference methods . By solving the eigenvalue problem as a recurrence relation, you will explicitly connect the matrix's eigenvectors to discrete sine functions and its eigenvalues to their corresponding frequencies, providing a concrete link between linear algebra and Fourier analysis.",
            "id": "3416297",
            "problem": "Consider a uniform grid on the open interval $(0,1)$ with $N$ interior points located at $x_{j} = j h$ for $j \\in \\{1,2,\\dots,N\\}$ and $h = \\frac{1}{N+1}$. Impose homogeneous Dirichlet boundary conditions so that a grid function $v = (v_{1},\\dots,v_{N})^{\\top} \\in \\mathbb{R}^{N}$ satisfies $v_{0} = 0$ and $v_{N+1} = 0$ at the endpoints. Define the one-dimensional Dirichlet discrete Laplacian $A \\in \\mathbb{R}^{N \\times N}$ by its action\n$$\n(A v)_{j} = 2 v_{j} - v_{j-1} - v_{j+1}, \\quad j \\in \\{1,2,\\dots,N\\},\n$$\nwith the convention $v_{0} = 0$ and $v_{N+1} = 0$. Equivalently, $A$ is the symmetric tridiagonal Toeplitz matrix with $A_{j j} = 2$ and $A_{j, j\\pm 1} = -1$. This is the unscaled matrix representation of the centered second-order finite difference approximation of the negative second derivative with Dirichlet boundary conditions, factored by $h^{2}$ for simplicity.\n\nStarting from the fundamental definitions of eigenvalues and eigenvectors, and the finite difference model above, perform the following steps:\n\n1. Derive the eigenpairs $(\\lambda_{k}, v^{(k)})$ of $A$ by solving $A v = \\lambda v$. Use only the recurrence relations implied by the tridiagonal structure and the boundary conditions to determine the admissible discrete modes and the corresponding eigenvalues. Show that the eigenvectors are discrete sine modes and identify the allowed wavenumbers. Normalize the eigenvectors to form an orthonormal family.\n\n2. Assemble the orthonormal eigenvectors into a matrix $Q \\in \\mathbb{R}^{N \\times N}$ and form the diagonal matrix $\\Lambda = \\operatorname{diag}(\\lambda_{1},\\dots,\\lambda_{N})$. Using the properties of a symmetric matrix and orthonormal eigenvectors, use them to diagonalize $A$ explicitly as $A = Q \\Lambda Q^{\\top}$. Relate $Q$ to the Discrete Sine Transform (DST) by writing down its entries.\n\n3. Using the diagonalization and appropriate product identities for trigonometric functions, evaluate the exact determinant $\\det(A)$ as a closed-form analytic expression in terms of $N$.\n\nProvide the final answer as a single closed-form analytic expression for $\\det(A)$, with no units. Do not round; an exact expression is required.",
            "solution": "The problem requires the derivation of the eigenpairs of the discrete Laplacian matrix $A$, its diagonalization, and the subsequent calculation of its determinant. We will proceed by solving the recurrence relation that arises from the eigenvalue problem.\n\nThe matrix $A \\in \\mathbb{R}^{N \\times N}$ is defined by its action on a vector $v \\in \\mathbb{R}^{N}$ as $(A v)_{j} = 2 v_{j} - v_{j-1} - v_{j+1}$ for $j=1, \\dots, N$, with the boundary conditions $v_{0}=0$ and $v_{N+1}=0$.\n\n**1. Derivation of Eigenpairs $(\\lambda_{k}, v^{(k)})$}\n\nThe eigenvalue problem is $A v = \\lambda v$. For each component $j$, this translates to:\n$$\n2 v_{j} - v_{j-1} - v_{j+1} = \\lambda v_{j}\n$$\nRearranging the terms, we obtain a second-order linear homogeneous recurrence relation for the components $v_j$ of an eigenvector $v$:\n$$\nv_{j+1} - (2 - \\lambda) v_{j} + v_{j-1} = 0, \\quad \\text{for } j=1, \\dots, N\n$$\nwith boundary conditions $v_0 = 0$ and $v_{N+1} = 0$.\n\nWe seek a solution of the form $v_j = r^j$. Substituting this into the recurrence relation gives the characteristic equation:\n$$\nr^2 - (2-\\lambda)r + 1 = 0\n$$\nThe roots are given by the quadratic formula: $r_{1,2} = \\frac{(2-\\lambda) \\pm \\sqrt{(2-\\lambda)^2 - 4}}{2}$.\nSince $A$ is a real symmetric matrix, its eigenvalues $\\lambda$ must be real. We can analyze the nature of the roots based on the discriminant $\\Delta = (2-\\lambda)^2 - 4$.\nIf $\\Delta \\geq 0$, the roots $r_1, r_2$ are real. The general solution is $v_j = c_1 r_1^j + c_2 r_2^j$. The condition $v_0=0$ implies $c_1+c_2=0$, so $v_j = c_1(r_1^j - r_2^j)$. The condition $v_{N+1}=0$ implies $c_1(r_1^{N+1} - r_2^{N+1})=0$. Since $r_1r_2=1$, and for $\\Delta0$ we have $|r_1| \\neq |r_2|$, this would lead to $c_1=0$, giving the trivial solution $v=0$. For $\\Delta=0$, $\\lambda=0$ or $\\lambda=4$, and $r_1=r_2=\\pm 1$, which also leads to the trivial solution under the given boundary conditions.\n\nThus, we must have $\\Delta  0$, which means $(2-\\lambda)^2  4$, or $0  \\lambda  4$. In this case, the roots are a complex conjugate pair. Let's define an angle $\\theta$ such that $2-\\lambda = 2\\cos(\\theta)$. This is always possible for $0  \\lambda  4$. The roots become:\n$$\nr_{1,2} = \\cos(\\theta) \\pm i \\sin(\\theta) = e^{\\pm i\\theta}\n$$\nThe general solution for $v_j$ is a linear combination of $(e^{i\\theta})^j$ and $(e^{-i\\theta})^j$:\n$$\nv_j = c_1 e^{ij\\theta} + c_2 e^{-ij\\theta}\n$$\nApplying the first boundary condition, $v_0 = 0$:\n$$\nv_0 = c_1 e^0 + c_2 e^0 = c_1 + c_2 = 0 \\implies c_2 = -c_1\n$$\nThis simplifies the solution to:\n$$\nv_j = c_1(e^{ij\\theta} - e^{-ij\\theta}) = 2i c_1 \\sin(j\\theta)\n$$\nThis demonstrates that the eigenvectors are discrete sine modes. Let $C = 2i c_1$ be an arbitrary constant. Then $v_j = C \\sin(j\\theta)$.\n\nNow, we apply the second boundary condition, $v_{N+1}=0$:\n$$\nv_{N+1} = C \\sin((N+1)\\theta) = 0\n$$\nFor a non-trivial eigenvector, we require $C \\neq 0$, which implies $\\sin((N+1)\\theta) = 0$. This condition quantizes the possible values of $\\theta$:\n$$\n(N+1)\\theta = k\\pi \\implies \\theta_k = \\frac{k\\pi}{N+1}\n$$\nfor some integer $k$. The values $k=1, 2, \\dots, N$ produce distinct, non-trivial eigenvectors. For $k=0$ or $k=N+1$, $\\sin(j\\theta_k)=0$ for all $j$, yielding the zero vector. Other integer values of $k$ produce eigenvectors that are either identical or scalar multiples of the eigenvectors for $k \\in \\{1, \\dots, N\\}$. Thus, we have $N$ distinct eigenvectors corresponding to $k=1, 2, \\dots, N$.\n\nThe eigenvalues $\\lambda_k$ are determined by the corresponding $\\theta_k$:\n$$\n\\lambda_k = 2 - 2\\cos(\\theta_k) = 2 - 2\\cos\\left(\\frac{k\\pi}{N+1}\\right)\n$$\nUsing the half-angle identity $1-\\cos(x) = 2\\sin^2(x/2)$, we get:\n$$\n\\lambda_k = 4\\sin^2\\left(\\frac{k\\pi}{2(N+1)}\\right), \\quad k=1, 2, \\dots, N\n$$\nThe components of the corresponding unnormalized eigenvector $v^{(k)}$ are:\n$$\nv_j^{(k)} = \\sin\\left(j\\theta_k\\right) = \\sin\\left(\\frac{jk\\pi}{N+1}\\right), \\quad j=1, 2, \\dots, N\n$$\n\nTo normalize these eigenvectors, we require their Euclidean norm to be $1$. We compute the squared norm:\n$$\n\\sum_{j=1}^{N} \\left(v_j^{(k)}\\right)^2 = \\sum_{j=1}^{N} \\sin^2\\left(\\frac{jk\\pi}{N+1}\\right)\n$$\nUsing the identity $\\sin^2(x) = \\frac{1}{2}(1-\\cos(2x))$, the sum becomes:\n$$\n\\sum_{j=1}^{N} \\frac{1}{2}\\left(1 - \\cos\\left(\\frac{2jk\\pi}{N+1}\\right)\\right) = \\frac{N}{2} - \\frac{1}{2} \\sum_{j=1}^{N} \\cos\\left(\\frac{2jk\\pi}{N+1}\\right)\n$$\nThe sum of cosines can be evaluated by considering the geometric series of complex exponentials $\\sum_{j=1}^{N} z^j$ with $z = \\exp\\left(i \\frac{2k\\pi}{N+1}\\right)$. Since $k \\in \\{1, \\dots, N\\}$, $z \\neq 1$. The sum is $\\sum_{j=1}^{N} z^j = z \\frac{1-z^N}{1-z}$. As $z^{N+1}=1$, we have $z^N = z^{-1}$. The sum is $z \\frac{1-z^{-1}}{1-z} = \\frac{z-1}{1-z}=-1$.\nThus, $\\sum_{j=1}^{N} \\cos\\left(\\frac{2jk\\pi}{N+1}\\right) = \\operatorname{Re}(-1) = -1$.\nThe squared norm is therefore $\\frac{N}{2} - \\frac{1}{2}(-1) = \\frac{N+1}{2}$.\nThe normalization constant is $\\sqrt{\\frac{2}{N+1}}$. The components of the orthonormal eigenvectors $v^{(k)}$ are:\n$$\nv_j^{(k)} = \\sqrt{\\frac{2}{N+1}} \\sin\\left(\\frac{jk\\pi}{N+1}\\right), \\quad j,k = 1, \\dots, N\n$$\n\n**2. Diagonalization of $A$**\n\nWe assemble the orthonormal eigenvectors $v^{(k)}$ as columns of a matrix $Q \\in \\mathbb{R}^{N \\times N}$:\n$$\nQ_{jk} = (v^{(k)})_j = \\sqrt{\\frac{2}{N+1}} \\sin\\left(\\frac{jk\\pi}{N+1}\\right)\n$$\nThe matrix of eigenvalues $\\Lambda$ is the diagonal matrix $\\Lambda = \\operatorname{diag}(\\lambda_1, \\dots, \\lambda_N)$, where\n$$\n\\Lambda_{kk} = \\lambda_k = 4\\sin^2\\left(\\frac{k\\pi}{2(N+1)}\\right)\n$$\nThe set of eigenvalue equations $A v^{(k)} = \\lambda_k v^{(k)}$ for $k=1, \\dots, N$ can be written in matrix form as $A Q = Q \\Lambda$.\nSince $A$ is a real symmetric matrix, its eigenvectors form an orthonormal basis. The matrix $Q$ whose columns are these orthonormal eigenvectors is therefore an orthogonal matrix, satisfying $Q^{\\top}Q = I$, which implies $Q^{-1}=Q^{\\top}$.\nMultiplying $A Q = Q \\Lambda$ on the right by $Q^{\\top}$, we get $A Q Q^{\\top} = Q \\Lambda Q^{\\top}$, which simplifies to $A = Q \\Lambda Q^{\\top}$. This is the spectral decomposition (diagonalization) of $A$.\n\nThe entries of $Q$ are $Q_{jk} = \\sqrt{\\frac{2}{N+1}} \\sin\\left(\\frac{jk\\pi}{N+1}\\right)$. This matrix is, by definition, the matrix of the Type-I Discrete Sine Transform (DST-I). The scaling factor ensures the transform is orthogonal (involutary). Note that $Q$ is a symmetric matrix, i.e., $Q = Q^{\\top}$.\n\n**3. Evaluation of $\\det(A)$**\n\nUsing the property $\\det(XY) = \\det(X)\\det(Y)$ and $\\det(X^{\\top}) = \\det(X)$, we can compute the determinant of $A$ from its diagonalization:\n$$\n\\det(A) = \\det(Q \\Lambda Q^{\\top}) = \\det(Q) \\det(\\Lambda) \\det(Q^{\\top}) = \\det(\\Lambda) (\\det(Q))^2\n$$\nSince $Q$ is orthogonal, $Q Q^{\\top}=I$, so $\\det(Q Q^{\\top})=\\det(Q)\\det(Q^{\\top})=(\\det(Q))^2 = \\det(I)=1$.\nTherefore, $\\det(A) = \\det(\\Lambda)$.\nThe determinant of the diagonal matrix $\\Lambda$ is the product of its diagonal entries:\n$$\n\\det(A) = \\prod_{k=1}^{N} \\lambda_k = \\prod_{k=1}^{N} 4\\sin^2\\left(\\frac{k\\pi}{2(N+1)}\\right)\n$$\nWe can factor out the $4$'s:\n$$\n\\det(A) = 4^N \\left( \\prod_{k=1}^{N} \\sin\\left(\\frac{k\\pi}{2(N+1)}\\right) \\right)^2\n$$\nTo evaluate the product of sines, we use the well-known identity:\n$$\n\\prod_{k=1}^{m-1} \\sin\\left(\\frac{k\\pi}{m}\\right) = \\frac{m}{2^{m-1}}\n$$\nLet's set $m = 2(N+1)$. The argument of the sines in our product is $\\frac{k\\pi}{m}$. Let $P = \\prod_{k=1}^{N} \\sin\\left(\\frac{k\\pi}{m}\\right)$. The identity involves a product up to $m-1 = 2N+1$. We relate our product to the identity:\n$$\n\\frac{m}{2^{m-1}} = \\prod_{k=1}^{2N+1} \\sin\\left(\\frac{k\\pi}{m}\\right) = \\left(\\prod_{k=1}^{N} \\sin\\left(\\frac{k\\pi}{m}\\right)\\right) \\cdot \\sin\\left(\\frac{(N+1)\\pi}{m}\\right) \\cdot \\left(\\prod_{k=N+2}^{2N+1} \\sin\\left(\\frac{k\\pi}{m}\\right)\\right)\n$$\nThe middle term is $\\sin\\left(\\frac{(N+1)\\pi}{2(N+1)}\\right) = \\sin(\\pi/2) = 1$.\nFor the last term, we use the identity $\\sin(\\pi-x)=\\sin(x)$. Let $j=m-k$. As $k$ runs from $N+2$ to $2N+1$, $j$ runs from $N$ down to $1$.\n$$\n\\sin\\left(\\frac{k\\pi}{m}\\right) = \\sin\\left(\\frac{(m-j)\\pi}{m}\\right) = \\sin\\left(\\pi - \\frac{j\\pi}{m}\\right) = \\sin\\left(\\frac{j\\pi}{m}\\right)\n$$\nThus, the last product is equal to the first: $\\prod_{k=N+2}^{2N+1} \\sin\\left(\\frac{k\\pi}{m}\\right) = P$.\nWe have $\\frac{m}{2^{m-1}} = P \\cdot 1 \\cdot P = P^2$.\nSubstituting $m=2(N+1)$:\n$$\nP^2 = \\frac{2(N+1)}{2^{2(N+1)-1}} = \\frac{2(N+1)}{2^{2N+1}} = \\frac{N+1}{2^{2N}}\n$$\nFinally, we substitute this back into the expression for the determinant:\n$$\n\\det(A) = 4^N \\cdot P^2 = (2^2)^N \\cdot \\frac{N+1}{2^{2N}} = 2^{2N} \\frac{N+1}{2^{2N}} = N+1\n$$\nThe determinant of the $N \\times N$ discrete Laplacian matrix is simply $N+1$.",
            "answer": "$$\\boxed{N+1}$$"
        },
        {
            "introduction": "The quality of a numerical solution often depends on whether the discrete system inherits crucial properties from the continuous PDE, such as a maximum principle. This hands-on coding exercise explores the conditions under which a finite element stiffness matrix becomes an M-matrix, which guarantees this desirable behavior . You will assemble stiffness matrices from first principles and investigate how mesh geometry—specifically the presence of obtuse angles—can violate the necessary sign structure, leading to a potential loss of physical monotonicity.",
            "id": "3416313",
            "problem": "Consider the two-dimensional scalar diffusion boundary value problem on a polygonal domain, modeled by the elliptic partial differential equation $-\\nabla \\cdot (K(x) \\nabla u(x)) = f(x)$ with homogeneous Dirichlet boundary conditions $u(x) = 0$ on the boundary, where $K(x)$ is either a scalar field or a symmetric positive definite tensor field. Use the conforming linear Lagrange finite element method on planar triangular meshes, with piecewise constant coefficients $K_T$ per triangle. Assemble the global stiffness matrix $A$ by summing element-wise bilinear forms of the type $\\int_T \\nabla \\varphi_i(x)^\\top K_T \\nabla \\varphi_j(x) \\, \\mathrm{d}x$ for each triangle $T$ and local basis functions $\\varphi_i$. Restrict attention to the submatrix $A_{\\mathcal{I}\\mathcal{I}}$ corresponding to the interior nodes $\\mathcal{I}$, defined as those mesh nodes that lie strictly inside the domain. In this setting, the property that the stiffness matrix is a so-called $M$-matrix is characterized by the following algebraic conditions: (i) $A_{\\mathcal{I}\\mathcal{I}}$ is symmetric positive definite, (ii) all off-diagonal entries of $A_{\\mathcal{I}\\mathcal{I}}$ are nonpositive, and (iii) each interior row of $A_{\\mathcal{I}\\mathcal{I}}$ is weakly diagonally dominant. This sign structure underpins a discrete maximum principle through an inverse-positivity mechanism. \n\nYour task is to implement, in a single self-contained program, the assembly of $A$ from first principles, the extraction of $A_{\\mathcal{I}\\mathcal{I}}$, and the verification of the above $M$-matrix conditions and the nonpositivity (also known as $Z$-matrix) sign pattern. You will construct three meshes and coefficient distributions to probe the relationship between mesh geometry, coefficients, and the sign pattern of $A$, including a counterexample with obtuse triangles that breaks monotonicity of the sign pattern. All numerical values are dimensionless. Angles, where relevant, are in radians.\n\nBase assumptions and definitions to be used in your derivation:\n- The conforming linear finite element space uses nodal basis functions that are affine on each triangle and globally continuous.\n- The element-level stiffness contribution is obtained via the standard Galerkin projection of the bilinear form $\\int_T \\nabla \\varphi_i(x)^\\top K_T \\nabla \\varphi_j(x) \\, \\mathrm{d}x$ onto the local basis, resulting in constant gradients of local basis functions on each triangle and a constant coefficient $K_T$ per triangle.\n- The global stiffness matrix $A$ is assembled by summing local contributions into the appropriate entries based on element connectivity, without any shortcut formula provided to you; you must derive and compute the necessary gradients and integrals explicitly.\n- Symmetric positive definiteness is to be verified numerically by checking that all eigenvalues of $A_{\\mathcal{I}\\mathcal{I}}$ are strictly positive to within a small numerical tolerance.\n- The $Z$-matrix sign pattern is to be verified by checking that all off-diagonal entries of $A_{\\mathcal{I}\\mathcal{I}}$ are nonpositive to within a small numerical tolerance.\n- Weak diagonal dominance for the interior rows is to be verified by checking $A_{ii} \\geq \\sum_{j \\neq i} |A_{ij}|$ for each $i \\in \\mathcal{I}$ to within a small numerical tolerance.\n\nConstruct the following three test cases:\n\n- Test case $1$ (acute, isotropic): Let the domain be the square $[0,1] \\times [0,1]$. Define a uniform grid of $5 \\times 5$ nodes at coordinates $(x_i, y_j)$ where $x_i = i/4$ and $y_j = j/4$ for integers $i,j \\in \\{0,1,2,3,4\\}$. Triangulate each of the $4 \\times 4$ squares by the northeast-southwest diagonal, forming two triangles per square: $(i,j)\\text{-}(i+1,j)\\text{-}(i+1,j+1)$ and $(i,j)\\text{-}(i+1,j+1)\\text{-}(i,j+1)$ in terms of grid indices. Set the scalar coefficient $K_T = \\kappa_T \\, I$ with $\\kappa_T = 1$ for all triangles.\n\n- Test case $2$ (obtuse perturbation, isotropic, heterogeneous): Start from test case $1$ but perturb the central interior node at $(0.5,0.5)$ to $(0.1,0.5)$ while keeping all other node coordinates fixed. Use the same triangulation connectivity based on the original grid indexing. Define piecewise constant scalar coefficients by $K_T = \\kappa_T \\, I$ with $\\kappa_T = 50$ on triangles that include the perturbed central node as a vertex and $\\kappa_T = 1$ otherwise.\n\n- Test case $3$ (acute, anisotropic): Use the unperturbed mesh and triangulation of test case $1$. Define a constant anisotropic symmetric positive definite coefficient tensor per triangle by $K_T = R^\\top \\operatorname{diag}(100, 1) R$, where $R$ is the rotation matrix $$ R = \\begin{bmatrix} \\cos \\theta  -\\sin \\theta \\\\ \\sin \\theta  \\cos \\theta \\end{bmatrix} $$ with $\\theta = \\pi/4$.\n\nIn all cases, define the set of interior nodes $\\mathcal{I}$ as the nodes strictly inside the square, i.e., those with coordinates $(x,y)$ satisfying $0  x  1$ and $0  y  1$. Assemble the full stiffness matrix $A$ and then extract $A_{\\mathcal{I}\\mathcal{I}}$.\n\nFor each test case, compute two boolean indicators:\n- $Z$: whether $A_{\\mathcal{I}\\mathcal{I}}$ has the $Z$-matrix sign pattern (all off-diagonals nonpositive).\n- $M$: whether $A_{\\mathcal{I}\\mathcal{I}}$ satisfies the $M$-matrix criteria (symmetric positive definite, nonpositive off-diagonals, and weak diagonal dominance of interior rows).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[Z_1, M_1, Z_2, M_2, Z_3, M_3]$, where the subscript denotes the test case index. The outputs must be booleans.",
            "solution": "The task is to assemble the stiffness matrix for a two-dimensional scalar diffusion problem using the linear finite element method (FEM) on triangular meshes and to verify specific algebraic properties of the resulting matrix system. The properties of interest are those that characterize an $M$-matrix, which are linked to the discrete maximum principle.\n\nThe starting point is the elliptic partial differential equation on a domain $\\Omega$:\n$$-\\nabla \\cdot (K(x) \\nabla u(x)) = f(x) \\quad \\text{in } \\Omega$$\nwith homogeneous Dirichlet boundary conditions $u(x) = 0$ on $\\partial\\Omega$. Here, $u(x)$ is the scalar field, and $K(x)$ is a symmetric positive definite coefficient tensor, which is assumed to be piecewise constant on each triangle $T$ of the mesh, denoted as $K_T$.\n\nThe weak formulation of this problem leads to finding $u \\in H^1_0(\\Omega)$ such that for all $v \\in H^1_0(\\Omega)$:\n$$a(u,v) = \\int_\\Omega (\\nabla v)^\\top K (\\nabla u) \\, \\mathrm{d}x = \\int_\\Omega fv \\, \\mathrm{d}x$$\nWe discretize the space $H^1_0(\\Omega)$ using a conforming linear finite element space $V_h$. Let $\\{\\varphi_i\\}_{i \\in \\mathcal{N}}$ be the set of global nodal basis functions, where $\\mathcal{N}$ is the set of all node indices in the mesh. The approximate solution is $u_h = \\sum_{j \\in \\mathcal{N}} u_j \\varphi_j(x)$, where $u_j$ are the nodal values. The Galerkin method yields a linear system $AU = F$, where $U$ is the vector of nodal values $u_j$, and the stiffness matrix $A$ has entries $A_{ij} = a(\\varphi_j, \\varphi_i)$. Due to the homogeneous Dirichlet boundary conditions, we only solve for the nodal values at the interior nodes, indexed by the set $\\mathcal{I}$. This leads to a reduced system $A_{\\mathcal{I}\\mathcal{I}}U_{\\mathcal{I}} = F_{\\mathcal{I}}$, where $A_{\\mathcal{I}\\mathcal{I}}$ is the submatrix of $A$ corresponding to interior nodes.\n\nThe global stiffness matrix $A$ is assembled by summing up contributions from each element (triangle) $T$ in the mesh:\n$$A_{ij} = \\sum_{T} \\int_T (\\nabla \\varphi_i)^\\top K_T (\\nabla \\varphi_j) \\, \\mathrm{d}x$$\nThe integral contributes to $A_{ij}$ only if nodes $i$ and $j$ are vertices of triangle $T$. For a single triangle $T$ with vertices $p_1, p_2, p_3$, the local basis functions are affine, so their gradients are constant vectors within $T$. Let the vertices have coordinates $p_k = (x_k, y_k)$ for $k \\in \\{1,2,3\\}$. The gradient of the local basis function $\\varphi_k$ (associated with vertex $p_k$) is given by:\n$$\\nabla \\varphi_k = \\frac{1}{2|T|} \\mathbf{v}_k$$\nwhere $|T|$ is the area of the triangle and the vectors $\\mathbf{v}_k$ are determined by the vertex coordinates. Specifically:\n$$\\mathbf{v}_1 = \\begin{pmatrix} y_2 - y_3 \\\\ x_3 - x_2 \\end{pmatrix}, \\quad \\mathbf{v}_2 = \\begin{pmatrix} y_3 - y_1 \\\\ x_1 - x_3 \\end{pmatrix}, \\quad \\mathbf{v}_3 = \\begin{pmatrix} y_1 - y_2 \\\\ x_2 - x_1 \\end{pmatrix}$$\nThe area $|T|$ is calculated as $|T| = \\frac{1}{2} |x_1(y_2-y_3) + x_2(y_3-y_1) + x_3(y_1-y_2)|$.\n\nFor each triangle $T$, we compute a $3 \\times 3$ element stiffness matrix $A^T$. Since $\\nabla\\varphi_i$, $\\nabla\\varphi_j$, and $K_T$ are constant within the element, the integral simplifies to a product:\n$$A^T_{ij} = |T| (\\nabla \\varphi_i)^\\top K_T (\\nabla \\varphi_j)$$\nwhere $i,j \\in \\{1,2,3\\}$ are the local indices of the vertices. If we define a $2 \\times 3$ matrix $B_T = [\\nabla \\varphi_1, \\nabla \\varphi_2, \\nabla \\varphi_3]$, the element stiffness matrix can be expressed compactly as $A^T = |T| B_T^\\top K_T B_T$. These local entries are then added to the corresponding global positions in $A$. For a triangle with global node indices $(n_1, n_2, n_3)$, the local entry $A^T_{ij}$ is added to the global entry $A_{n_i, n_j}$.\n\nAfter assembling the global matrix $A$, we extract the submatrix $A_{\\mathcal{I}\\mathcal{I}}$ corresponding to the specified interior nodes. We then verify the following properties for $A_{\\mathcal{I}\\mathcal{I}}$ using a numerical tolerance $\\epsilon$ (e.g., $10^{-9}$):\n\n1.  **Z-matrix property**: All off-diagonal entries must be nonpositive. We check if $A_{ij} \\le \\epsilon$ for all $i \\ne j$ in the index set $\\mathcal{I}$. The boolean indicator for this is $Z$.\n\n2.  **M-matrix properties**: A matrix is an $M$-matrix if it is a Z-matrix with a positive inverse. For a symmetric matrix, this is equivalent to being a symmetric positive definite (SPD) Z-matrix. The problem adds the condition of weak diagonal dominance. Thus, we check for three conditions combined:\n    a. **Symmetric Positive Definiteness (SPD)**: We verify symmetry ($A_{\\mathcal{I}\\mathcal{I}} = A_{\\mathcal{I}\\mathcal{I}}^\\top$) and then compute the eigenvalues of $A_{\\mathcal{I}\\mathcal{I}}$. All eigenvalues must be strictly positive (greater than $\\epsilon$).\n    b. **Z-matrix property**: As defined above.\n    c. **Weak Diagonal Dominance (WDD)**: For each row $i \\in \\mathcal{I}$, we check if $A_{ii} \\ge \\sum_{j \\in \\mathcal{I}, j \\ne i} |A_{ij}|$. For a Z-matrix, where off-diagonals are nonpositive, this is equivalent to checking if the sum of each row is non-negative, $\\sum_{j \\in \\mathcal{I}} A_{ij} \\ge 0$.\n\nThe boolean indicator $M$ is true if and only if all three of these conditions (SPD, Z-matrix, and WDD) are met.\n\nThe test cases are designed to investigate the influence of mesh geometry and material coefficients on these properties.\n- **Test Case 1**: An isotropic coefficient ($K_T=I$) on a uniform mesh of acute right-angled triangles. This is the ideal case, where the Z-matrix and M-matrix properties are expected to hold.\n- **Test Case 2**: A node is perturbed, introducing obtuse angles into the mesh. This is a classic counterexample. The entry $A^T_{ij}$ is related to $-\\cot\\gamma_{ij}$ where $\\gamma_{ij}$ is the angle opposite the edge $(i, j)$. If an angle is obtuse, the cotangent is negative, leading to a positive off-diagonal contribution $A^T_{ij}  0$. This can break the Z-matrix property for the global matrix $A_{\\mathcal{I}\\mathcal{I}}$.\n- **Test Case 3**: A strong, constant anisotropy is introduced on the acute mesh. The condition for non-positive off-diagonals becomes $(\\nabla \\varphi_i)^\\top K_T (\\nabla \\varphi_j) \\le 0$. If the directions of high conductivity in $K_T$ are not aligned with the mesh, this condition can be violated even on a geometrically acute mesh, leading to a loss of the Z-matrix property.\n\nThe implementation will proceed by generating the mesh data for each case, assembling the matrix $A$ from first principles as derived above, extracting $A_{\\mathcal{I}\\mathcal{I}}$, and then programmatically checking the $Z$- and $M$-matrix conditions to produce the required boolean flags.",
            "answer": "```python\nimport numpy as np\n\ndef evaluate_case(nodes, triangles, K_func, interior_node_indices, tol=1e-9):\n    \"\"\"\n    Assembles the FEM stiffness matrix and evaluates its properties.\n\n    Args:\n        nodes (np.ndarray): Array of node coordinates, shape (num_nodes, 2).\n        triangles (list[tuple]): List of triangles, each a tuple of 3 node indices.\n        K_func (callable): Function that takes triangle nodes and returns the 2x2 K_T matrix.\n        interior_node_indices (list[int]): List of indices for interior nodes.\n        tol (float): Numerical tolerance for comparisons.\n\n    Returns:\n        tuple[bool, bool]: A tuple (is_Z, is_M) indicating Z-matrix and M-matrix properties.\n    \"\"\"\n    num_nodes = nodes.shape[0]\n    A = np.zeros((num_nodes, num_nodes))\n\n    for tri_indices in triangles:\n        # 1. Get vertex coordinates\n        p1, p2, p3 = nodes[tri_indices[0]], nodes[tri_indices[1]], nodes[tri_indices[2]]\n\n        # 2. Calculate triangle area and basis function gradients\n        # The element stiffness matrix is invariant to vertex ordering, but its\n        # derivation is cleaner if we use the absolute area.\n        matrix_for_area = np.array([\n            [p2[0] - p1[0], p2[1] - p1[1]],\n            [p3[0] - p1[0], p3[1] - p1[1]]\n        ])\n        area = 0.5 * np.abs(np.linalg.det(matrix_for_area))\n\n        if area  1e-12:  # Skip degenerate triangles\n            continue\n\n        # Gradients of local basis functions (phi_1, phi_2, phi_3)\n        # grad(phi_k) = (1 / 2*|T|) * [y_next - y_prev, x_prev - x_next]\n        grad_phi = (1.0 / (2.0 * area)) * np.array([\n            [p2[1] - p3[1], p3[1] - p1[1], p1[1] - p2[1]],  # dy components\n            [p3[0] - p2[0], p1[0] - p3[0], p2[0] - p1[0]]   # dx components\n        ])\n\n        # 3. Get coefficient K_T for the current triangle\n        K_T = K_func(tri_indices)\n\n        # 4. Compute the 3x3 element stiffness matrix A_T\n        A_T = area * grad_phi.T @ K_T @ grad_phi\n\n        # 5. Assemble A_T into the global stiffness matrix A\n        for i in range(3):\n            for j in range(3):\n                A[tri_indices[i], tri_indices[j]] += A_T[i, j]\n\n    # Extract the submatrix corresponding to interior nodes\n    A_II = A[np.ix_(interior_node_indices, interior_node_indices)]\n\n    # --- Verify Matrix Properties ---\n\n    # 1. Z-matrix property: all off-diagonal entries are non-positive\n    off_diagonal_mask = ~np.eye(A_II.shape[0], dtype=bool)\n    is_Z = np.all(A_II[off_diagonal_mask] = tol)\n\n    # 2. M-matrix properties: is a Z-matrix, SPD, and WDD.\n    is_M = False\n    if is_Z:\n        # Check for Symmetric Positive Definiteness (SPD)\n        is_symmetric = np.allclose(A_II, A_II.T, atol=tol)\n        is_spd = False\n        if is_symmetric:\n            try:\n                eigenvalues = np.linalg.eigvalsh(A_II)\n                is_spd = np.all(eigenvalues  tol)\n            except np.linalg.LinAlgError:\n                is_spd = False\n        \n        if is_spd:\n            # Check for Weak Diagonal Dominance (WDD)\n            # For a Z-matrix, WDD is equivalent to non-negative row sums.\n            row_sums = np.sum(A_II, axis=1)\n            is_wdd = np.all(row_sums = -tol)\n            \n            if is_wdd:\n                is_M = True # Only if Z, SPD, and WDD all hold\n\n    return is_Z, is_M\n\ndef solve():\n    \"\"\"\n    Sets up and runs the three test cases, then prints the results.\n    \"\"\"\n    N = 5\n    num_nodes = N * N\n    \n    # Identify interior node indices (nodes not on the boundary 0 or 1)\n    interior_node_indices = []\n    for j in range(1, N - 1):\n        for i in range(1, N - 1):\n            interior_node_indices.append(j * N + i)\n\n    # Define triangulation based on a uniform grid\n    # Each square is split by the NE-SW diagonal\n    triangles = []\n    for j in range(N - 1):\n        for i in range(N - 1):\n            p_bl = j * N + i        # Bottom-left\n            p_br = j * N + i + 1    # Bottom-right\n            p_tl = (j + 1) * N + i    # Top-left\n            p_tr = (j + 1) * N + i + 1# Top-right\n            triangles.append((p_bl, p_br, p_tr))\n            triangles.append((p_bl, p_tr, p_tl))\n    \n    results = []\n\n    # --- Test Case 1: Acute, Isotropic ---\n    nodes1 = np.array([[i / (N - 1), j / (N - 1)] for j in range(N) for i in range(N)])\n    def K_func1(tri_indices):\n        return np.identity(2)\n    \n    Z1, M1 = evaluate_case(nodes1, triangles, K_func1, interior_node_indices)\n    results.extend([Z1, M1])\n\n    # --- Test Case 2: Obtuse, Isotropic, Heterogeneous ---\n    nodes2 = np.copy(nodes1)\n    perturbed_node_idx = (N // 2) * N + (N // 2)  # Central node (2,2) - index 12\n    nodes2[perturbed_node_idx] = [0.1, 0.5]\n    \n    def K_func2(tri_indices):\n        if perturbed_node_idx in tri_indices:\n            return 50.0 * np.identity(2)\n        return np.identity(2)\n\n    Z2, M2 = evaluate_case(nodes2, triangles, K_func2, interior_node_indices)\n    results.extend([Z2, M2])\n\n    # --- Test Case 3: Acute, Anisotropic ---\n    nodes3 = nodes1 # Unperturbed mesh\n    theta = np.pi / 4.0\n    c, s = np.cos(theta), np.sin(theta)\n    R = np.array([[c, -s], [s, c]])\n    D = np.diag([100.0, 1.0])\n    K_aniso = R.T @ D @ R\n\n    def K_func3(tri_indices):\n        return K_aniso\n\n    Z3, M3 = evaluate_case(nodes3, triangles, K_func3, interior_node_indices)\n    results.extend([Z3, M3])\n\n    # Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The efficiency of solving a PDE numerically often hinges on the performance of the linear solver, but this performance is deeply tied to the algebraic properties of the system matrix. This problem examines a classic scenario where numerical methods can be deceptive: applying the GMRES algorithm to a highly non-normal matrix arising from a convection-dominated problem . You will analyze why the underlying Arnoldi process becomes unstable in finite precision and how this leads to a misleading residual norm, illustrating the critical need for robust orthogonalization techniques in practice.",
            "id": "3416269",
            "problem": "Consider the steady one-dimensional linear advection boundary value problem on the interval $[0,1]$,\n$$ -\\nu\\,u''(x) + \\beta\\,u'(x) = f(x), \\quad u(0)=0,\\; u(1)=0, $$\nwhere $\\nu \\ge 0$ and $\\beta  0$ are constants. Let $n \\in \\mathbb{N}$ interior grid points be placed uniformly with spacing $h = 1/(n+1)$, and discretize $u'(x)$ by the first-order upwind finite difference and $u''(x)$ by the standard second-order central difference. After eliminating boundary values, the resulting linear system has coefficient matrix $A \\in \\mathbb{R}^{n \\times n}$ that is tridiagonal with entries\n$$ A_{i,i} = \\frac{2\\nu}{h^2} + \\frac{\\beta}{h}, \\quad A_{i,i-1} = -\\frac{\\nu}{h^2} - \\frac{\\beta}{h}, \\quad A_{i,i+1} = -\\frac{\\nu}{h^2}, $$\nfor $i \\in \\{1,\\dots,n\\}$ with the obvious conventions at the ends. Consider the convection-dominated and purely advective limiting case $\\nu = 0$, so that\n$$ A_{i,i} = \\frac{\\beta}{h}, \\quad A_{i,i-1} = -\\frac{\\beta}{h}, \\quad A_{i,i+1} = 0, $$\ni.e., $A$ is strictly lower bidiagonal plus a diagonal. Let the right-hand side be $b = e_1 \\in \\mathbb{R}^n$ (the first canonical basis vector), and take the initial guess $x_0 = 0$.\n\nYou run the Generalized Minimal Residual method (GMRES) without restart, using the Arnoldi process with a single pass of modified Gram–Schmidt to construct an orthonormal basis $\\{v_1,\\dots,v_k\\}$, in floating-point arithmetic with unit roundoff $\\epsilon_{\\mathrm{mach}}$. No preconditioning is applied.\n\nUsing only foundational definitions, reason from first principles about the Krylov subspace $\\mathcal{K}_k(A,r_0) = \\mathrm{span}\\{r_0, Ar_0, \\dots, A^{k-1} r_0\\}$ with $r_0 = b - A x_0 = b$, the structure and non-normality of $A$ when $\\nu = 0$, and the mechanics of the Arnoldi process and floating-point orthogonalization. Then determine which statements are correct regarding the severity of loss of orthogonality in the Arnoldi vectors due to round-off, the reliability of the residual minimization reported by GMRES, and practical remedies.\n\nSelect all correct options.\n\nA. In the case $\\nu = 0$, the matrix $A$ is similar (via diagonal scaling and permutation) to a single Jordan block, so for large $n$ the Arnoldi basis built by a single pass of modified Gram–Schmidt can suffer severe loss of orthogonality. As a consequence, the residual norm computed by the projected least-squares problem in GMRES can substantially underestimate the true residual norm unless reorthogonalization is used.\n\nB. Even in the presence of loss of orthogonality among the Arnoldi vectors due to round-off, GMRES in floating-point arithmetic still exactly minimizes the true residual norm $\\|r_k\\|_2$ over $x_0 + \\mathcal{K}_k(A,r_0)$; therefore, the reported residual norm is always equal to the true residual norm.\n\nC. For this $A$ with $\\nu = 0$, selectively reorthogonalizing only against the most recently added Arnoldi vector is sufficient to fully prevent loss of orthogonality as $k$ grows, so more robust schemes like Householder orthogonalization or full reorthogonalization are unnecessary.\n\nD. Using Householder reflections in Arnoldi or performing a second pass of modified Gram–Schmidt when needed restores near machine-precision orthogonality of the basis and thereby the reliability of the reported residual norm and least-squares minimization; additionally, right preconditioning by a well-conditioned diagonal scaling that normalizes the diagonal entries of $A$ can reduce coefficient growth in orthogonalization but does not obviate the need for reorthogonalization.\n\nE. Restarting GMRES with a small restart parameter $m$ (without any reorthogonalization) eliminates the non-normality of $A$ and therefore prevents loss of orthogonality; as a result, residual norms remain fully reliable without additional stabilization.",
            "solution": "The problem requires an analysis of the behavior of the Generalized Minimal Residual method (GMRES) when applied to a linear system arising from the finite difference discretization of a one-dimensional, steady, linear advection-diffusion equation. The central issue is the impact of the matrix structure, particularly in the convection-dominated limit, on the stability of the Arnoldi process used by GMRES.\n\n### Principle-Based Derivation\n\n**1. Analysis of the Coefficient Matrix $A$**\n\nThe problem statement provides the discretized linear system's coefficient matrix $A \\in \\mathbb{R}^{n \\times n}$ with entries:\n$$ A_{i,i} = \\frac{2\\nu}{h^2} + \\frac{\\beta}{h}, \\quad A_{i,i-1} = -\\frac{\\nu}{h^2} - \\frac{\\beta}{h}, \\quad A_{i,i+1} = -\\frac{\\nu}{h^2} $$\nThe problem focuses on the purely advective case where the diffusion coefficient $\\nu = 0$. In this limit, the entries simplify to:\n$$ A_{i,i} = \\frac{\\beta}{h}, \\quad A_{i,i-1} = -\\frac{\\beta}{h}, \\quad A_{i,i+1} = 0 $$\nThis defines a lower bidiagonal matrix:\n$$ A = \\frac{\\beta}{h} \\begin{pmatrix}\n1  0  0  \\dots  0 \\\\\n-1  1  0  \\dots  0 \\\\\n0  -1  1  \\dots  0 \\\\\n\\vdots  \\ddots  \\ddots  \\ddots  \\vdots \\\\\n0  \\dots  0  -1  1\n\\end{pmatrix} $$\nTo understand the behavior of iterative methods, we must analyze the properties of $A$.\n\n*   **Eigenvalues:** Since $A$ is a lower triangular matrix, its eigenvalues are its diagonal entries. Therefore, all $n$ eigenvalues are identical:\n    $$ \\lambda_i = \\frac{\\beta}{h} \\quad \\text{for } i=1, \\dots, n $$\n    The algebraic multiplicity of the eigenvalue $\\lambda = \\beta/h$ is $n$.\n\n*   **Eigenvectors and Jordan Form:** The geometric multiplicity of an eigenvalue is the dimension of its corresponding eigenspace, which is the null space of $(A - \\lambda I)$. Let's compute this for $\\lambda = \\beta/h$:\n    $$ A - \\lambda I = A - \\frac{\\beta}{h}I = \\frac{\\beta}{h} \\begin{pmatrix}\n    0  0  0  \\dots  0 \\\\\n    -1  0  0  \\dots  0 \\\\\n    0  -1  0  \\dots  0 \\\\\n    \\vdots  \\ddots  \\ddots  \\ddots  \\vdots \\\\\n    0  \\dots  0  -1  0\n    \\end{pmatrix} $$\n    An eigenvector $v = (v_1, \\dots, v_n)^T$ must satisfy $(A - \\lambda I)v = 0$. This leads to the system of equations:\n    $$-v_1 = 0, \\quad -v_2 = 0, \\quad \\dots, \\quad -v_{n-1} = 0$$\n    The first equation of $(A - \\lambda I)v = 0$ is $0 = 0$, which places no constraint on $v_n$. Thus, any eigenvector must be of the form $(0, 0, \\dots, 0, c)^T$ for some scalar $c \\neq 0$. The eigenspace is spanned by the single vector $e_n = (0, \\dots, 0, 1)^T$.\n    The geometric multiplicity of the eigenvalue $\\lambda = \\beta/h$ is $1$.\n\n*   **Non-Normality:** A matrix is normal if it has a complete set of orthogonal eigenvectors. Since the algebraic multiplicity ($n$) is much greater than the geometric multiplicity ($1$), the matrix $A$ is defective and thus non-normal. In fact, it is a canonical example of a highly non-normal matrix. Its Jordan normal form consists of a single Jordan block of size $n$ corresponding to the eigenvalue $\\lambda = \\beta/h$. A matrix is similar to such a Jordan block if and only if its minimal polynomial is $(t-\\lambda)^n$. For our matrix $A$, $(A - \\lambda I)^k \\neq 0$ for $k  n$, so this condition holds. Such matrices cause significant transient growth of $\\|A^k r_0\\|$ before eventual decay (since for our specific $r_0$, the solution does not grow).\n\n**2. GMRES, Arnoldi Process, and Loss of Orthogonality (LOO)**\n\nGMRES constructs an approximate solution $x_k \\in x_0 + \\mathcal{K}_k(A, r_0)$ that minimizes the true residual norm $\\|b - Ax_k\\|_2$. It achieves this by building an orthonormal basis $\\{v_1, \\dots, v_k\\}$ for the Krylov subspace $\\mathcal{K}_k(A,r_0) = \\mathrm{span}\\{r_0, Ar_0, \\dots, A^{k-1}r_0\\}$ via the Arnoldi process.\n\nThe Arnoldi process, in its basic form, uses Gram-Schmidt orthogonalization. The problem specifies modified Gram-Schmidt (MGS). In floating-point arithmetic, the ability of Gram-Schmidt to produce an orthogonal basis depends on the conditioning of the original set of vectors. For our highly non-normal matrix $A$, the Krylov vectors $\\{A^j r_0\\}_{j=0}^{k-1}$ rapidly become nearly linearly dependent as $k$ increases. Orthogonalizing such a set of vectors is an ill-conditioned problem. A single pass of MGS is numerically unstable in this situation, leading to a catastrophic loss of orthogonality (LOO) in the computed Arnoldi vectors $\\hat{V}_k$. That is, for the computed basis, $\\hat{V}_k^T \\hat{V}_k$ will have off-diagonal elements significantly larger than the machine precision $\\epsilon_{\\mathrm{mach}}$.\n\n**3. Consequences of Loss of Orthogonality**\n\nIn exact arithmetic, the Arnoldi process produces the relation $AV_k = V_{k+1}\\tilde{H}_k$, where $\\tilde{H}_k$ is an upper Hessenberg matrix. GMRES uses this to transform the large residual minimization problem into a small $(k+1) \\times k$ least-squares problem:\n$$ \\|r_k\\|_2 = \\min_{y \\in \\mathbb{C}^k} \\| b - A(x_0+V_ky) \\|_2 = \\min_{y \\in \\mathbb{C}^k} \\| r_0 - AV_ky \\|_2 = \\min_{y \\in \\mathbb{C}^k} \\| V_{k+1}(\\|r_0\\|_2e_1 - \\tilde{H}_ky) \\|_2 $$\nSince $V_{k+1}$ is unitary, this simplifies to calculating $\\|r_k\\|_2 = \\min_y \\|\\,\\|r_0\\|_2e_1-\\tilde{H}_ky\\|_2$. The resulting norm is the \"reported\" or \"harmonic\" residual norm.\n\nWhen severe LOO occurs, the computed basis $\\hat{V}_k$ is not orthogonal. The fundamental Arnoldi relation breaks down and is replaced by $A\\hat{V}_k = \\hat{V}_{k+1}\\tilde{H}_k + F_k$, where $F_k$ is a non-negligible perturbation term due to round-off errors magnified by the ill-conditioning. Consequently, the true residual norm $\\|r_k\\|_2 = \\|b - A\\hat{x}_k\\|_2$ is no longer equal to the reported residual norm computed from the small least-squares problem. For highly non-normal matrices, the reported norm can be a dramatic underestimate of the true norm, giving a false impression of convergence.\n\n### Option-by-Option Analysis\n\n**A. In the case $\\nu = 0$, the matrix $A$ is similar (via diagonal scaling and permutation) to a single Jordan block, so for large $n$ the Arnoldi basis built by a single pass of modified Gram–Schmidt can suffer severe loss of orthogonality. As a consequence, the residual norm computed by the projected least-squares problem in GMRES can substantially underestimate the true residual norm unless reorthogonalization is used.**\n*   **Analysis:** This statement is a precise and accurate summary of the dynamics derived above. The matrix $A$ is indeed similar to a single Jordan block. This high degree of non-normality causes the underlying Krylov vectors to become nearly collinear, leading to severe loss of orthogonality in the Arnoldi basis computed with a single MGS pass. This, in turn, causes the reported residual norm to diverge from and underestimate the true residual norm. Reorthogonalization is a known remedy.\n*   **Verdict:** Correct.\n\n**B. Even in the presence of loss of orthogonality among the Arnoldi vectors due to round-off, GMRES in floating-point arithmetic still exactly minimizes the true residual norm $\\|r_k\\|_2$ over $x_0 + \\mathcal{K}_k(A,r_0)$; therefore, the reported residual norm is always equal to the true residual norm.**\n*   **Analysis:** This statement is false on multiple counts. In floating point arithmetic with LOO, the computed Arnoldi vectors $\\hat{V}_k$ do not span the true Krylov subspace $\\mathcal{K}_k(A,r_0)$. Furthermore, the minimization property of GMRES is compromised; the method finds an approximate solution in the span of $\\hat{V}_k$, but because the Arnoldi relation is broken, the minimization of the projected problem does not guarantee minimization of the true residual. The claim that the reported and true residuals are always equal is a well-known falsehood for GMRES on non-normal matrices without reorthogonalization.\n*   **Verdict:** Incorrect.\n\n**C. For this $A$ with $\\nu = 0$, selectively reorthogonalizing only against the most recently added Arnoldi vector is sufficient to fully prevent loss of orthogonality as $k$ grows, so more robust schemes like Householder orthogonalization or full reorthogonalization are unnecessary.**\n*   **Analysis:** Loss of orthogonality is a global phenomenon; the newly computed vector $v_{k+1}$ loses orthogonality with respect to *all* previous vectors, particularly the earliest ones. Reorthogonalizing against only the most recent vector, $v_k$, is insufficient to restore global orthogonality. For severely non-normal problems, more robust methods are required. These include full reorthogonalization (e.g., a second pass of MGS) or using an unconditionally stable method like Householder reflections. The proposed scheme is inadequate.\n*   **Verdict:** Incorrect.\n\n**D. Using Householder reflections in Arnoldi or performing a second pass of modified Gram–Schmidt when needed restores near machine-precision orthogonality of the basis and thereby the reliability of the reported residual norm and least-squares minimization; additionally, right preconditioning by a well-conditioned diagonal scaling that normalizes the diagonal entries of $A$ can reduce coefficient growth in orthogonalization but does not obviate the need for reorthogonalization.**\n*   **Analysis:** This statement correctly identifies the standard, robust remedies for LOO. Householder Arnoldi is known to be numerically stable and maintains orthogonality to near machine precision. A second pass of MGS (full reorthogonalization) has a similar effect. Restoring orthogonality restores the validity of the Arnoldi relation, making the reported residual a reliable measure of the true residual. The statement also correctly analyzes the effect of diagonal scaling. Applying a diagonal preconditioner $D = (\\beta/h)^{-1}I$ results in the matrix $AD^{-1}$, which has unit diagonal entries. This scaling does not alter the fundamental non-normal structure (similarity to a Jordan block) or the ill-conditioning of the Krylov basis. Therefore, while it might have minor beneficial effects, it does not eliminate the root cause of LOO, and a robust orthogonalization scheme remains essential.\n*   **Verdict:** Correct.\n\n**E. Restarting GMRES with a small restart parameter $m$ (without any reorthogonalization) eliminates the non-normality of $A$ and therefore prevents loss of orthogonality; as a result, residual norms remain fully reliable without additional stabilization.**\n*   **Analysis:** This statement is fundamentally flawed. An algorithmic choice like restarting cannot change an intrinsic property of the matrix $A$, such as its non-normality. While restarting with a small $m$ limits the Arnoldi process to $m$ steps and may thus limit the extent of LOO *within one cycle*, it does not prevent it. More importantly, for non-normal matrices, GMRES often requires a large number of iterations (a large Krylov subspace) to converge. Restarting discards information and typically leads to stagnation or very slow convergence for such problems. It does not make residuals \"fully reliable\" and is generally a poor strategy for this class of matrices unless combined with other techniques.\n*   **Verdict:** Incorrect.",
            "answer": "$$\\boxed{AD}$$"
        }
    ]
}