{
    "hands_on_practices": [
        {
            "introduction": "The abstract properties of coercivity and continuity are the cornerstones of the mathematical theory for solving partial differential equations. This first practice grounds these concepts by connecting them directly to the physical properties of a system, such as a material's thermal conductivity or elasticity. By deriving the coercivity and continuity constants from the pointwise bounds of the diffusion tensor $A(x)$, you will gain a concrete understanding of how these essential mathematical bounds arise from physical constraints .",
            "id": "3371888",
            "problem": "Let $\\Omega \\subset \\mathbb{R}^{d}$ be a bounded Lipschitz domain and let $V := H_{0}^{1}(\\Omega)$ denote the Sobolev space of functions with square-integrable weak gradients that vanish on $\\partial \\Omega$. Consider a measurable, symmetric matrix field $A : \\Omega \\to \\mathbb{R}^{d \\times d}$ satisfying the uniform ellipticity and boundedness condition: there exist constants $0  \\alpha_{0} \\leq \\alpha_{1}  \\infty$ such that for almost every $x \\in \\Omega$,\n$$\n\\alpha_{0} I \\leq A(x) \\leq \\alpha_{1} I,\n$$\nin the sense of symmetric matrices. Define the bilinear form $a : V \\times V \\to \\mathbb{R}$ by\n$$\na(u,v) := \\int_{\\Omega} \\nabla u(x)^{\\top} A(x) \\nabla v(x)\\, \\mathrm{d}x,\n$$\nand the associated energy norm $\\|v\\|_{a} := \\big(a(v,v)\\big)^{1/2}$. Starting from the core definitions of symmetry, positivity, and the pointwise matrix inequalities encoded by the assumption on $A(x)$, determine the largest constant $c_{\\mathrm{low}}$ and the smallest constant $c_{\\mathrm{up}}$, expressed explicitly in terms of $\\alpha_{0}$ and $\\alpha_{1}$ only, such that for every $v \\in V$,\n$$\nc_{\\mathrm{low}}\\, \\|\\nabla v\\|_{L^{2}(\\Omega)} \\leq \\|v\\|_{a} \\leq c_{\\mathrm{up}}\\, \\|\\nabla v\\|_{L^{2}(\\Omega)}.\n$$\nGive your final answer as the ordered pair $\\big(c_{\\mathrm{low}}, c_{\\mathrm{up}}\\big)$ in closed form. No numerical rounding is required. State the answer without units. For context, this question concerns norm equivalence constants that arise in the analysis of coercivity and continuity of bilinear forms for the numerical solution of partial differential equations (PDE).",
            "solution": "The objective is to determine the largest constant $c_{\\mathrm{low}}$ and the smallest constant $c_{\\mathrm{up}}$ such that the following norm equivalence holds for all $v \\in V := H_{0}^{1}(\\Omega)$:\n$$\nc_{\\mathrm{low}}\\, \\|\\nabla v\\|_{L^{2}(\\Omega)} \\leq \\|v\\|_{a} \\leq c_{\\mathrm{up}}\\, \\|\\nabla v\\|_{L^{2}(\\Omega)}.\n$$\nThe norm $\\|v\\|_{a}$ is defined as $\\|v\\|_{a} := \\big(a(v,v)\\big)^{1/2}$, where the bilinear form $a(u,v)$ is given by\n$$\na(u,v) := \\int_{\\Omega} \\nabla u(x)^{\\top} A(x) \\nabla v(x)\\, \\mathrm{d}x.\n$$\nThe problem statement provides that the symmetric matrix field $A(x)$ satisfies the condition $0  \\alpha_{0} I \\leq A(x) \\leq \\alpha_{1} I$ for almost every $x \\in \\Omega$. This inequality, interpreted in the sense of symmetric matrices (or quadratic forms), means that for any vector $\\xi \\in \\mathbb{R}^{d}$, the following holds for almost every $x \\in \\Omega$:\n$$\n\\xi^{\\top} (\\alpha_{0} I) \\xi \\leq \\xi^{\\top} A(x) \\xi \\leq \\xi^{\\top} (\\alpha_{1} I) \\xi.\n$$\nUsing the property of the identity matrix $I$, this simplifies to\n$$\n\\alpha_{0} \\xi^{\\top}\\xi \\leq \\xi^{\\top} A(x) \\xi \\leq \\alpha_{1} \\xi^{\\top}\\xi.\n$$\nRecalling that $\\xi^{\\top}\\xi$ is the squared Euclidean norm of $\\xi$, denoted by $\\|\\xi\\|^2$, the inequality is\n$$\n\\alpha_{0} \\|\\xi\\|^{2} \\leq \\xi^{\\top} A(x) \\xi \\leq \\alpha_{1} \\|\\xi\\|^{2}.\n$$\nThis pointwise inequality holds for any vector $\\xi \\in \\mathbb{R}^{d}$. For a function $v \\in V$, its gradient $\\nabla v(x)$ is a vector field, so for almost every $x \\in \\Omega$, the vector $\\nabla v(x) \\in \\mathbb{R}^{d}$. We can therefore substitute $\\xi = \\nabla v(x)$ into the inequality:\n$$\n\\alpha_{0} \\|\\nabla v(x)\\|^{2} \\leq \\nabla v(x)^{\\top} A(x) \\nabla v(x) \\leq \\alpha_{1} \\|\\nabla v(x)\\|^{2}.\n$$\nThis inequality relates the integrand of the bilinear form $a(v,v)$ to the squared norm of the gradient. Since this holds for almost every $x \\in \\Omega$, and all terms are non-negative, we can integrate over the domain $\\Omega$:\n$$\n\\int_{\\Omega} \\alpha_{0} \\|\\nabla v(x)\\|^{2} \\, \\mathrm{d}x \\leq \\int_{\\Omega} \\nabla v(x)^{\\top} A(x) \\nabla v(x) \\, \\mathrm{d}x \\leq \\int_{\\Omega} \\alpha_{1} \\|\\nabla v(x)\\|^{2} \\, \\mathrm{d}x.\n$$\nThe constants $\\alpha_0$ and $\\alpha_1$ are independent of $x$, so they can be factored out of the integrals:\n$$\n\\alpha_{0} \\int_{\\Omega} \\|\\nabla v(x)\\|^{2} \\, \\mathrm{d}x \\leq \\int_{\\Omega} \\nabla v(x)^{\\top} A(x) \\nabla v(x) \\, \\mathrm{d}x \\leq \\alpha_{1} \\int_{\\Omega} \\|\\nabla v(x)\\|^{2} \\, \\mathrm{d}x.\n$$\nWe now identify the terms with their corresponding norm definitions. The middle term is, by definition, $a(v,v) = \\|v\\|_{a}^{2}$. The integral $\\int_{\\Omega} \\|\\nabla v(x)\\|^{2} \\, \\mathrm{d}x$ is precisely the squared $L^2$-norm of the gradient of $v$, denoted by $\\|\\nabla v\\|_{L^{2}(\\Omega)}^{2}$. Substituting these definitions yields:\n$$\n\\alpha_{0} \\|\\nabla v\\|_{L^{2}(\\Omega)}^{2} \\leq \\|v\\|_{a}^{2} \\leq \\alpha_{1} \\|\\nabla v\\|_{L^{2}(\\Omega)}^{2}.\n$$\nSince norms are non-negative, we can take the square root of all parts of the inequality without changing the direction of the inequalities:\n$$\n\\sqrt{\\alpha_{0}} \\|\\nabla v\\|_{L^{2}(\\Omega)} \\leq \\|v\\|_{a} \\leq \\sqrt{\\alpha_{1}} \\|\\nabla v\\|_{L^{2}(\\Omega)}.\n$$\nThis establishes the existence of the constants and provides candidate values: $c_{\\mathrm{low}} = \\sqrt{\\alpha_{0}}$ and $c_{\\mathrm{up}} = \\sqrt{\\alpha_{1}}$.\n\nThe problem requires the largest possible value for $c_{\\mathrm{low}}$ and the smallest possible value for $c_{\\mathrm{up}}$. We must verify if the constants we found are optimal.\n\nTo show that $c_{\\mathrm{up}} = \\sqrt{\\alpha_{1}}$ is the smallest possible upper constant, we must show that for any smaller constant, the inequality would fail for some valid choice of $A(x)$. Consider the specific matrix field $A(x) = \\alpha_{1}I$. This is a valid choice because it satisfies $\\alpha_0 I \\leq \\alpha_1 I \\leq \\alpha_1 I$ since $\\alpha_0 \\leq \\alpha_1$. For this specific $A(x)$, the energy norm becomes:\n$$\n\\|v\\|_{a}^{2} = a(v,v) = \\int_{\\Omega} \\nabla v(x)^{\\top} (\\alpha_{1}I) \\nabla v(x) \\, \\mathrm{d}x = \\alpha_{1} \\int_{\\Omega} \\|\\nabla v(x)\\|^{2} \\, \\mathrm{d}x = \\alpha_{1} \\|\\nabla v\\|_{L^{2}(\\Omega)}^{2}.\n$$\nTaking the square root gives $\\|v\\|_{a} = \\sqrt{\\alpha_{1}}\\|\\nabla v\\|_{L^{2}(\\Omega)}$. The inequality $\\|v\\|_{a} \\leq c_{\\mathrm{up}}\\|\\nabla v\\|_{L^{2}(\\Omega)}$ becomes $\\sqrt{\\alpha_{1}}\\|\\nabla v\\|_{L^{2}(\\Omega)} \\leq c_{\\mathrm{up}}\\|\\nabla v\\|_{L^{2}(\\Omega)}$. For any $v \\in V$ with $\\|\\nabla v\\|_{L^{2}(\\Omega)} \\neq 0$, this implies $c_{\\mathrm{up}} \\geq \\sqrt{\\alpha_{1}}$. Since we have already shown that $c_{\\mathrm{up}} = \\sqrt{\\alpha_{1}}$ works for all valid $A(x)$, it must be the smallest possible upper bound.\n\nSimilarly, to show that $c_{\\mathrm{low}} = \\sqrt{\\alpha_{0}}$ is the largest possible lower constant, we consider the specific matrix field $A(x) = \\alpha_{0}I$. This is a valid choice as $\\alpha_0 I \\leq \\alpha_0 I \\leq \\alpha_1 I$. With this $A(x)$, the energy norm is:\n$$\n\\|v\\|_{a}^{2} = a(v,v) = \\int_{\\Omega} \\nabla v(x)^{\\top} (\\alpha_{0}I) \\nabla v(x) \\, \\mathrm{d}x = \\alpha_{0} \\int_{\\Omega} \\|\\nabla v(x)\\|^{2} \\, \\mathrm{d}x = \\alpha_{0} \\|\\nabla v\\|_{L^{2}(\\Omega)}^{2}.\n$$\nTaking the square root gives $\\|v\\|_{a} = \\sqrt{\\alpha_{0}}\\|\\nabla v\\|_{L^{2}(\\Omega)}$. The inequality $c_{\\mathrm{low}}\\|\\nabla v\\|_{L^{2}(\\Omega)} \\leq \\|v\\|_{a}$ becomes $c_{\\mathrm{low}}\\|\\nabla v\\|_{L^{2}(\\Omega)} \\leq \\sqrt{\\alpha_{0}}\\|\\nabla v\\|_{L^{2}(\\Omega)}$. For any $v \\in V$ with $\\|\\nabla v\\|_{L^{2}(\\Omega)} \\neq 0$, this implies $c_{\\mathrm{low}} \\leq \\sqrt{\\alpha_{0}}$. Since we have already shown that $c_{\\mathrm{low}} = \\sqrt{\\alpha_{0}}$ works for all valid $A(x)$, it must be the largest possible lower bound.\n\nIn conclusion, the optimal constants are $c_{\\mathrm{low}} = \\sqrt{\\alpha_{0}}$ and $c_{\\mathrm{up}} = \\sqrt{\\alpha_{1}}$.\nThe ordered pair is therefore $\\big(\\sqrt{\\alpha_{0}}, \\sqrt{\\alpha_{1}}\\big)$. These constants establish the equivalence between the energy norm $\\|\\cdot\\|_{a}$ and the Sobolev seminorm $\\|\\nabla\\cdot\\|_{L^2(\\Omega)}$, which is the foundation for proving coercivity and continuity of the bilinear form $a(\\cdot,\\cdot)$ with respect to the standard $H_0^1(\\Omega)$ norm.",
            "answer": "$$ \\boxed{(\\sqrt{\\alpha_{0}}, \\sqrt{\\alpha_{1}})} $$"
        },
        {
            "introduction": "The Lax-Milgram theorem provides a powerful guarantee for the existence and uniqueness of a solution, but it hinges on the bilinear form being both continuous and coercive. This practice challenges you to explore what happens when the coercivity condition is not met. By analyzing a series of carefully constructed scenarios, you will develop a sharp intuition for identifying non-coercive systems and understanding the functional analytic mechanisms that lead to a failure of uniqueness, a critical skill for troubleshooting variational formulations in research and application .",
            "id": "3371844",
            "problem": "Let $\\Omega \\subset \\mathbb{R}^d$ be a bounded Lipschitz domain with $d \\in \\mathbb{N}$, and let $V$ be a real Hilbert space with norm $\\|\\cdot\\|_{V}$. A bilinear form $a: V \\times V \\to \\mathbb{R}$ is called continuous if there exists $M0$ such that $|a(u,v)| \\le M \\|u\\|_{V}\\|v\\|_{V}$ for all $u,v \\in V$, and coercive if there exists $\\alpha0$ such that $a(u,u) \\ge \\alpha \\|u\\|_{V}^2$ for all $u \\in V$. Consider the variational problem: find $u \\in V$ such that\n$$\na(u,v) = f(v) \\quad \\text{for all } v \\in V,\n$$\nwhere $f \\in V'$ is a continuous linear functional. The Lax–Milgram theorem asserts existence and uniqueness if $a$ is continuous and coercive. Your goal is to identify valid counterexamples in which $a$ is continuous but not coercive, and the problem admits non-unique solutions, and to recognize the mechanism of failure in each setting from first principles of functional analysis for Partial Differential Equations (PDEs).\n\nSelect all options that present a valid counterexample and correctly identify the mechanism for non-uniqueness.\n\nA. Let $V = H^{1}(\\Omega)$ and define $a(u,v) = \\int_{\\Omega} \\nabla u \\cdot \\nabla v \\,\\mathrm{d}x$. Let $f(v) = \\int_{\\Omega} g v \\,\\mathrm{d}x$ with $g \\in L^{2}(\\Omega)$ satisfying $\\int_{\\Omega} g \\,\\mathrm{d}x = 0$. Then $a$ is continuous but not coercive on $H^{1}(\\Omega)$, the variational problem has infinitely many solutions $u$ differing by an additive constant, and the failure mechanism is the nontrivial kernel consisting of constants (equivalently, failure of the Poincaré inequality on $H^{1}(\\Omega)$), which prevents control of the full $H^{1}$-norm by $a(u,u)$.\n\nB. Let $V = H^{1}_{0}(\\Omega)$ and define $a(u,v) = \\int_{\\Omega} \\nabla u \\cdot \\nabla v \\,\\mathrm{d}x$. Let $f(v) = \\int_{\\Omega} g v \\,\\mathrm{d}x$ with $g \\in L^{2}(\\Omega)$. Then $a$ is continuous but not coercive on $H^{1}_{0}(\\Omega)$, and the variational problem has non-unique solutions because $a$ vanishes on constants; the failure mechanism is again the presence of constants in the kernel.\n\nC. Let $V = L^{2}(\\Omega)$ and define $a(u,v) = \\int_{\\Omega} u v \\,\\mathrm{d}x$. Let $f(v) = \\int_{\\Omega} h v \\,\\mathrm{d}x$ with $h \\in L^{2}(\\Omega)$. Then $a$ is continuous but not coercive on $L^{2}(\\Omega)$, and the variational problem has non-unique solutions because $a$ does not control a derivative.\n\nD. Let $V = H^{1}(\\Omega)$ and define $a(u,v) = \\int_{\\Omega} u v \\,\\mathrm{d}x$. Let $f(v) \\equiv 0$. Then $a$ is continuous but not coercive on $H^{1}(\\Omega)$, and the variational problem has non-unique solutions because the equation only involves $L^{2}$ pairings and so does not restrict the gradient, leaving many solutions.\n\nE. Let $V = H(\\mathrm{div};\\Omega) = \\{\\, \\mathbf{u} \\in L^{2}(\\Omega)^d : \\mathrm{div}\\,\\mathbf{u} \\in L^{2}(\\Omega) \\,\\}$ and define $a(\\mathbf{u},\\mathbf{v}) = \\int_{\\Omega} (\\mathrm{div}\\,\\mathbf{u}) (\\mathrm{div}\\,\\mathbf{v}) \\,\\mathrm{d}x$. Let $f(\\mathbf{v}) \\equiv 0$. Then $a$ is continuous but not coercive on $H(\\mathrm{div};\\Omega)$, the variational problem has non-unique solutions because any divergence-free field solves it, and the failure mechanism is the nontrivial kernel of the divergence operator; $a(\\mathbf{u},\\mathbf{u})$ is only a seminorm that does not control $\\|\\mathbf{u}\\|_{H(\\mathrm{div})}$.",
            "solution": "The problem asks to identify valid counterexamples to the uniqueness of solutions for variational problems where the bilinear form $a$ is continuous but not coercive. The Lax-Milgram theorem guarantees a unique solution for a given continuous linear functional $f \\in V'$ if the bilinear form $a: V \\times V \\to \\mathbb{R}$ is both continuous and coercive on the Hilbert space $V$. A failure of coercivity opens the possibility for non-existence or non-uniqueness of solutions.\n\nNon-uniqueness arises when the homogeneous problem, $a(u,v) = 0$ for all $v \\in V$, admits non-trivial solutions $u \\neq 0$. The set of all such solutions forms the kernel of the operator $A: V \\to V'$ associated with the bilinear form, defined by $(Au)(v) = a(u,v)$. If this kernel, $\\ker(A)$, is non-trivial, and a particular solution $u_p$ exists for the inhomogeneous problem $a(u,v) = f(v)$, then any function of the form $u = u_p + u_k$, where $u_k \\in \\ker(A)$, is also a solution. This leads to infinitely many solutions.\nWe will now analyze each option based on these principles.\n\n### Analysis of Option A\n- **Setup**: $V = H^{1}(\\Omega)$, $a(u,v) = \\int_{\\Omega} \\nabla u \\cdot \\nabla v \\,\\mathrm{d}x$, $f(v) = \\int_{\\Omega} g v \\,\\mathrm{d}x$ with $g \\in L^{2}(\\Omega)$ and $\\int_{\\Omega} g \\,\\mathrm{d}x = 0$.\n- **Continuity**: The bilinear form is continuous. By the Cauchy-Schwarz inequality,\n$$|a(u,v)| = \\left| \\int_{\\Omega} \\nabla u \\cdot \\nabla v \\,\\mathrm{d}x \\right| \\le \\|\\nabla u\\|_{L^2(\\Omega)} \\|\\nabla v\\|_{L^2(\\Omega)}.$$\nSince $\\|\\nabla u\\|_{L^2(\\Omega)} \\le \\|u\\|_{H^1(\\Omega)}$ and $\\|\\nabla v\\|_{L^2(\\Omega)} \\le \\|v\\|_{H^1(\\Omega)}$ by definition of the $H^1$-norm, we have $|a(u,v)| \\le \\|u\\|_{H^1(\\Omega)} \\|v\\|_{H^1(\\Omega)}$. Thus, $a$ is continuous with continuity constant $M=1$.\n- **Coercivity**: The bilinear form is not coercive on $V = H^1(\\Omega)$. We test the coercivity condition: $a(u,u) \\ge \\alpha \\|u\\|_{H^1(\\Omega)}^2$ for some $\\alpha  0$. We have\n$$a(u,u) = \\int_{\\Omega} |\\nabla u|^2 \\,\\mathrm{d}x = \\|\\nabla u\\|_{L^2(\\Omega)}^2.$$\nThe full norm is $\\|u\\|_{H^1(\\Omega)}^2 = \\|u\\|_{L^2(\\Omega)}^2 + \\|\\nabla u\\|_{L^2(\\Omega)}^2$. Consider any non-zero constant function $u(x) = c \\neq 0$. Then $u \\in H^1(\\Omega)$, but $\\nabla u = 0$. For such a function, $a(u,u) = 0$, while $\\|u\\|_{H^1(\\Omega)}^2 = \\|c\\|_{L^2(\\Omega)}^2 = c^2 \\text{meas}(\\Omega)  0$. It is impossible to find an $\\alpha  0$ such that $0 \\ge \\alpha c^2 \\text{meas}(\\Omega)$. Therefore, $a$ is not coercive. This is precisely a failure of the Poincaré inequality to hold on the entire space $H^1(\\Omega)$.\n- **Non-uniqueness**: The variational problem corresponds to the weak formulation of the Neumann problem for the Poisson equation: $-\\Delta u = g$ in $\\Omega$ with $\\frac{\\partial u}{\\partial n} = 0$ on $\\partial\\Omega$. A solution to this problem is known to exist if and only if the compatibility condition $\\int_{\\Omega} g \\,\\mathrm{d}x = 0$ is satisfied, which is given. If $u$ is a solution, then for any constant $c \\in \\mathbb{R}$, the function $u+c$ is also in $H^1(\\Omega)$ and $\\nabla(u+c) = \\nabla u$. Thus, $a(u+c, v) = a(u,v) = f(v)$ for all $v \\in H^1(\\Omega)$. Since there are infinitely many choices for $c$, the problem has infinitely many solutions.\n- **Mechanism**: The kernel of the seminorm $\\sqrt{a(u,u)}$ consists of functions $u$ for which $a(u,u) = \\|\\nabla u\\|_{L^2(\\Omega)}^2 = 0$. This implies $\\nabla u=0$, which for a connected domain $\\Omega$ means $u$ must be a constant. The kernel is the space of constant functions, which is a non-trivial subspace of $H^1(\\Omega)$. This non-trivial kernel is the source of non-uniqueness. The description is accurate.\n- **Verdict**: **Correct**.\n\n### Analysis of Option B\n- **Setup**: $V = H^{1}_{0}(\\Omega)$, $a(u,v) = \\int_{\\Omega} \\nabla u \\cdot \\nabla v \\,\\mathrm{d}x$.\n- **Coercivity**: This bilinear form *is* coercive on $H^1_0(\\Omega)$. For functions in $H^1_0(\\Omega)$, the Poincaré inequality holds: there exists a constant $C_P  0$ (depending on $\\Omega$) such that $\\|u\\|_{L^2(\\Omega)} \\le C_P \\|\\nabla u\\|_{L^2(\\Omega)}$ for all $u \\in H^1_0(\\Omega)$. Consequently,\n$$\\|u\\|_{H^1(\\Omega)}^2 = \\|u\\|_{L^2(\\Omega)}^2 + \\|\\nabla u\\|_{L^2(\\Omega)}^2 \\le (C_P^2+1)\\|\\nabla u\\|_{L^2(\\Omega)}^2 = (C_P^2+1)a(u,u).$$\nThis implies $a(u,u) \\ge \\frac{1}{C_P^2+1} \\|u\\|_{H^1(\\Omega)}^2$. With $\\alpha = (C_P^2+1)^{-1}  0$, the form is coercive. The claim that it is not coercive is false.\n- **Non-uniqueness**: Since the form is continuous and coercive on the Hilbert space $H^1_0(\\Omega)$, the Lax-Milgram theorem applies and guarantees a unique solution for any $f \\in (H^1_0(\\Omega))'$. The claim of non-unique solutions is false. The reasoning provided is also flawed; the only constant function in $H^1_0(\\Omega)$ is the zero function.\n- **Verdict**: **Incorrect**.\n\n### Analysis of Option C\n- **Setup**: $V = L^{2}(\\Omega)$, $a(u,v) = \\int_{\\Omega} u v \\,\\mathrm{d}x$.\n- **Coercivity**: This bilinear form is the standard inner product on $L^2(\\Omega)$. The norm on $V=L^2(\\Omega)$ is $\\|u\\|_V = \\|u\\|_{L^2(\\Omega)} = (\\int_\\Omega u^2 \\mathrm{d}x)^{1/2}$. We have\n$$a(u,u) = \\int_{\\Omega} u^2 \\,\\mathrm{d}x = \\|u\\|_{L^2(\\Omega)}^2.$$\nThis satisfies the coercivity condition $a(u,u) \\ge \\alpha \\|u\\|_{L^2(\\Omega)}^2$ with $\\alpha=1$. The claim that $a$ is not coercive is false.\n- **Non-uniqueness**: Since $a$ is the inner product, the problem $a(u,v)=f(v)$ has a unique solution for any $f\\in (L^2(\\Omega))'$ by the Riesz Representation Theorem (which is a special case of Lax-Milgram). The problem given is $\\int_{\\Omega} u v \\,\\mathrm{d}x = \\int_{\\Omega} h v \\,\\mathrm{d}x$, or $\\int_{\\Omega} (u-h) v \\,\\mathrm{d}x = 0$ for all $v \\in L^2(\\Omega)$. This implies $u-h=0$ a.e., so $u=h$ is the unique solution.\n- **Verdict**: **Incorrect**.\n\n### Analysis of Option D\n- **Setup**: $V = H^{1}(\\Omega)$, $a(u,v) = \\int_{\\Omega} u v \\,\\mathrm{d}x$, $f(v) \\equiv 0$.\n- **Continuity**: The form is continuous, as $|a(u,v)| = |\\int_\\Omega uv \\mathrm{d}x| \\le \\|u\\|_{L^2} \\|v\\|_{L^2} \\le \\|u\\|_{H^1} \\|v\\|_{H^1}$.\n- **Coercivity**: The form is not coercive on $H^1(\\Omega)$. We need $\\|u\\|_{L^2(\\Omega)}^2 \\ge \\alpha (\\|u\\|_{L^2(\\Omega)}^2 + \\|\\nabla u\\|_{L^2(\\Omega)}^2)$. This inequality cannot hold for a fixed $\\alpha  0$, as one can construct a sequence of functions (e.g., highly oscillatory functions) for which $\\|\\nabla u\\|_{L^2(\\Omega)}^2 / \\|u\\|_{L^2(\\Omega)}^2 \\to \\infty$. So the non-coercivity claim is correct.\n- **Non-uniqueness**: The problem is to find $u \\in H^1(\\Omega)$ such that $a(u,v) = \\int_{\\Omega} u v \\,\\mathrm{d}x = 0$ for all $v \\in H^1(\\Omega)$. The space $H^1(\\Omega)$ is dense in $L^2(\\Omega)$. So, if the inner product $\\langle u,v \\rangle_{L^2} = 0$ for all $v$ in a dense subset of $L^2(\\Omega)$, it must be zero for all functions in $L^2(\\Omega)$. Taking $v=u \\in L^2(\\Omega)$, we get $\\int_{\\Omega} u^2 \\,\\mathrm{d}x = \\|u\\|_{L^2(\\Omega)}^2 = 0$, which implies $u=0$ almost everywhere. The only function in $H^1(\\Omega)$ that is zero a.e. is the zero function. Therefore, the problem has the unique solution $u=0$. The claim of non-unique solutions is false.\n- **Verdict**: **Incorrect**.\n\n### Analysis of Option E\n- **Setup**: $V = H(\\mathrm{div};\\Omega)$, $a(\\mathbf{u},\\mathbf{v}) = \\int_{\\Omega} (\\mathrm{div}\\,\\mathbf{u}) (\\mathrm{div}\\,\\mathbf{v}) \\,\\mathrm{d}x$, $f(\\mathbf{v}) \\equiv 0$. The norm is $\\|\\mathbf{u}\\|_{H(\\mathrm{div})}^2 = \\|\\mathbf{u}\\|_{L^2(\\Omega)^d}^2 + \\|\\mathrm{div}\\,\\mathbf{u}\\|_{L^2(\\Omega)}^2$.\n- **Continuity**: The bilinear form is continuous. By the Cauchy-Schwarz inequality,\n$$|a(\\mathbf{u},\\mathbf{v})| = \\left| \\int_{\\Omega} (\\mathrm{div}\\,\\mathbf{u}) (\\mathrm{div}\\,\\mathbf{v}) \\,\\mathrm{d}x \\right| \\le \\|\\mathrm{div}\\,\\mathbf{u}\\|_{L^2} \\|\\mathrm{div}\\,\\mathbf{v}\\|_{L^2} \\le \\|\\mathbf{u}\\|_{H(\\mathrm{div})} \\|\\mathbf{v}\\|_{H(\\mathrm{div})}.$$\nThus, $a$ is continuous with $M=1$.\n- **Coercivity**: The bilinear form is not coercive. We check if $a(\\mathbf{u},\\mathbf{u}) \\ge \\alpha \\|\\mathbf{u}\\|_{H(\\mathrm{div})}^2$. We have\n$$a(\\mathbf{u},\\mathbf{u}) = \\|\\mathrm{div}\\,\\mathbf{u}\\|_{L^2(\\Omega)}^2.$$\nConsider a non-zero divergence-free vector field $\\mathbf{u} \\in H(\\mathrm{div};\\Omega)$, i.e., $\\mathbf{u} \\neq \\mathbf{0}$ but $\\mathrm{div}\\,\\mathbf{u} = 0$. Such fields exist (e.g., any constant vector field $\\mathbf{u}=\\mathbf{c}\\neq \\mathbf{0}$). For such a field, $a(\\mathbf{u},\\mathbf{u})=0$, but $\\|\\mathbf{u}\\|_{H(\\mathrm{div})}^2 = \\|\\mathbf{u}\\|_{L^2(\\Omega)^d}^2  0$. It is impossible to find an $\\alpha  0$ such that $0 \\ge \\alpha \\|\\mathbf{u}\\|_{L^2}^2$. Therefore, $a$ is not coercive.\n- **Non-uniqueness**: The problem is to find $\\mathbf{u} \\in H(\\mathrm{div};\\Omega)$ such that $a(\\mathbf{u},\\mathbf{v}) = 0$ for all $\\mathbf{v} \\in H(\\mathrm{div};\\Omega)$. This means $\\int_{\\Omega} (\\mathrm{div}\\,\\mathbf{u}) (\\mathrm{div}\\,\\mathbf{v}) \\,\\mathrm{d}x = 0$ for all $\\mathbf{v} \\in H(\\mathrm{div};\\Omega)$. The set of functions $\\{ \\mathrm{div}\\,\\mathbf{v} \\mid \\mathbf{v} \\in H(\\mathrm{div};\\Omega) \\}$ is a subspace of $L^2(\\Omega)$. In fact, for a bounded Lipschitz domain $\\Omega$, the divergence operator maps $H(\\mathrm{div};\\Omega)$ surjectively onto $L^2(\\Omega)$. Therefore, the condition becomes $\\int_{\\Omega} (\\mathrm{div}\\,\\mathbf{u}) q \\,\\mathrm{d}x = 0$ for all $q \\in L^2(\\Omega)$, which implies $\\mathrm{div}\\,\\mathbf{u} = 0$. Any divergence-free field $\\mathbf{u} \\in H(\\mathrm{div};\\Omega)$ is a solution. The space of such fields is non-trivial, so there are infinitely many solutions.\n- **Mechanism**: The source of non-uniqueness is the non-triviality of the kernel of the operator associated with $a$, which consists of all $\\mathbf{u} \\in H(\\mathrm{div};\\Omega)$ such that $\\mathrm{div}\\,\\mathbf{u} = 0$. This is precisely the kernel of the divergence operator restricted to $H(\\mathrm{div};\\Omega)$. The quantity $a(\\mathbf{u},\\mathbf{u})$ is only a seminorm because it vanishes for a non-zero function, and it fails to control the $\\|\\mathbf{u}\\|_{L^2}$ part of the $\\|\\cdot\\|_{H(\\mathrm{div})}$ norm. The description is accurate.\n- **Verdict**: **Correct**.\n\nBased on the analysis, options A and E present valid counterexamples with correct explanations for the failure mechanism.",
            "answer": "$$\\boxed{AE}$$"
        },
        {
            "introduction": "After exploring conceptual failures of coercivity, this final practice moves from theory to computation by tackling the classic Neumann-Poisson problem, whose bilinear form is not coercive over the standard $H^1$ space. You will implement and analyze two common numerical strategies—an explicit constraint and a penalty method—used to restore well-posedness by handling the problematic kernel of constant functions. This exercise bridges abstract theory with practical implementation, demonstrating how concepts like \"coercivity modulo a kernel\" translate into algorithmic choices that directly impact the stability and conditioning of the resulting linear system .",
            "id": "3371827",
            "problem": "Consider the one-dimensional Neumann Poisson model problem on the closed interval $[0,1]$ with bilinear form $a(u,v) = \\int_{0}^{1} u'(x)\\,v'(x)\\,dx$ and right-hand side functional $\\ell(v) = \\int_{0}^{1} f(x)\\,v(x)\\,dx$. The kernel of $a(\\cdot,\\cdot)$ consists of constant functions. To ensure well-posedness, one enforces a mean-zero constraint $\\int_{0}^{1} u(x)\\,dx = 0$. In numerical practice, two common enforcement strategies are: an explicit mean-zero constraint (restricting to the mean-zero subspace) and a penalty formulation (augmenting the bilinear form with a term penalizing the mean).\n\nDiscretize the interval $[0,1]$ with a uniform mesh of $n$ elements (mesh size $h = 1/n$) and continuous, piecewise linear finite elements. Let $K \\in \\mathbb{R}^{(n+1)\\times(n+1)}$ be the stiffness matrix assembled from the local contributions $\\frac{1}{h}\\begin{bmatrix}1  -1 \\\\ -1  1\\end{bmatrix}$ on each element, and let $M \\in \\mathbb{R}^{(n+1)\\times(n+1)}$ be the consistent mass matrix assembled from the local contributions $\\frac{h}{6}\\begin{bmatrix}2  1 \\\\ 1  2\\end{bmatrix}$. Define the discrete $H^{1}$ norm by $\\|u\\|_{H^{1}_h}^{2} = u^{\\top}(K+M)u$, where $u \\in \\mathbb{R}^{n+1}$ is the coefficient vector in the nodal basis. The discrete mean of $u$ is $\\mu(u) = \\dfrac{\\mathbf{1}^{\\top} M u}{\\mathbf{1}^{\\top} M \\mathbf{1}}$, where $\\mathbf{1}$ denotes the vector of all ones.\n\nFor the explicit mean-zero constraint, define the mean-zero subspace $S = \\{u \\in \\mathbb{R}^{n+1} : \\mathbf{1}^{\\top} M u = 0\\}$. For the penalty method with parameter $\\gamma  0$, define the penalized bilinear form\n$$\na_{\\gamma}(u,v) = u^{\\top} K v + \\gamma \\,\\frac{(\\mathbf{1}^{\\top} M u)\\,(\\mathbf{1}^{\\top} M v)}{\\mathbf{1}^{\\top} M \\mathbf{1}},\n$$\nwhich corresponds to the matrix $K_{\\gamma} = K + \\dfrac{\\gamma}{\\mathbf{1}^{\\top} M \\mathbf{1}}(M\\mathbf{1})(M\\mathbf{1})^{\\top}$.\n\nDefine the discrete continuity constant of a symmetric bilinear form $b(u,v) = u^{\\top} B v$ with respect to the discrete $H^{1}$ norm by\n$$\nC_{\\mathrm{cont}}(B) = \\sup_{u\\neq 0,\\,v\\neq 0} \\frac{u^{\\top} B v}{\\|u\\|_{H^{1}_h}\\,\\|v\\|_{H^{1}_h}}.\n$$\nDefine the discrete coercivity constant on a subspace $W \\subset \\mathbb{R}^{n+1}$ by\n$$\nC_{\\mathrm{coer}}(B;W) = \\inf_{u\\in W\\setminus\\{0\\}} \\frac{u^{\\top} B u}{\\|u\\|_{H^{1}_h}^{2}}.\n$$\nFor symmetric $B$ and $H := K+M$, observe that $C_{\\mathrm{cont}}(B)$ is the largest eigenvalue of the generalized eigenproblem $B x = \\lambda H x$, and $C_{\\mathrm{coer}}(B;W)$ is the smallest eigenvalue over $W$ of the same generalized eigenproblem. For the Neumann stiffness $K$, there is a zero eigenvalue with eigenvector proportional to $\\mathbf{1}$, which is excluded by restricting to $S$.\n\nYour tasks:\n\n- Assemble $K$ and $M$ for given $n$.\n- For the explicit mean-zero strategy:\n  - Compute $C_{\\mathrm{cont}}(K)$ as the largest generalized eigenvalue of $(K,H)$.\n  - Compute $C_{\\mathrm{coer}}(K;S)$ as the smallest positive generalized eigenvalue of $(K,H)$ (i.e., the smallest eigenvalue excluding the zero eigenvalue).\n  - Compute the operator condition number on $S$ given by $\\kappa_{\\mathrm{explicit}} = \\dfrac{C_{\\mathrm{cont}}(K)}{C_{\\mathrm{coer}}(K;S)}$.\n- For the penalty strategy with parameter $\\gamma$: \n  - Compute $C_{\\mathrm{cont}}(K_{\\gamma})$ as the largest generalized eigenvalue of $(K_{\\gamma},H)$.\n  - Compute $C_{\\mathrm{coer}}(K_{\\gamma};\\mathbb{R}^{n+1})$ as the smallest generalized eigenvalue of $(K_{\\gamma},H)$.\n  - Compute the operator condition number $\\kappa_{\\mathrm{penalty}}(\\gamma) = \\dfrac{C_{\\mathrm{cont}}(K_{\\gamma})}{C_{\\mathrm{coer}}(K_{\\gamma};\\mathbb{R}^{n+1})}$.\n\nDesign a program that performs these computations for the following test suite of $(n,\\text{method},\\gamma)$:\n- Case A: $(n=\\;1,\\ \\text{method}=\\ \\text{explicit},\\ \\gamma\\ \\text{ignored})$.\n- Case B: $(n=\\;8,\\ \\text{method}=\\ \\text{explicit},\\ \\gamma\\ \\text{ignored})$.\n- Case C: $(n=\\;8,\\ \\text{method}=\\ \\text{penalty},\\ \\gamma=\\ 10^{-8})$.\n- Case D: $(n=\\;8,\\ \\text{method}=\\ \\text{penalty},\\ \\gamma=\\ 1)$.\n- Case E: $(n=\\;8,\\ \\text{method}=\\ \\text{penalty},\\ \\gamma=\\ 10^{8})$.\n- Case F: $(n=\\;32,\\ \\text{method}=\\ \\text{penalty},\\ \\gamma=\\ 10^{-2})$.\n\nFor each case, produce a list of three floating-point numbers rounded to six decimal places:\n- For explicit: $[C_{\\mathrm{cont}}(K),\\ C_{\\mathrm{coer}}(K;S),\\ \\kappa_{\\mathrm{explicit}}]$.\n- For penalty: $[C_{\\mathrm{cont}}(K_{\\gamma}),\\ C_{\\mathrm{coer}}(K_{\\gamma};\\mathbb{R}^{n+1}),\\ \\kappa_{\\mathrm{penalty}}(\\gamma)]$.\n\nFinal Output Format: Your program should produce a single line of output containing the results as a comma-separated list of lists, in the order of the cases A through F, enclosed in square brackets. For example, the printed line must have the form\n$[\\,[a_{1},b_{1},c_{1}],\\,[a_{2},b_{2},c_{2}],\\,\\ldots,\\, [a_{6},b_{6},c_{6}]\\,]$,\nwith each $a_{j}, b_{j}, c_{j}$ being floating-point numbers rounded to six decimal places.\n\nNo physical units or angle units are involved in this problem, and no percentages are required. Ensure all computations are performed using the definitions and constructions given above, starting from the assembled finite element matrices.",
            "solution": "The problem requires a numerical investigation of the conditioning of two common strategies for enforcing a mean-zero constraint in the finite element solution of a one-dimensional Neumann-Poisson problem. The analysis is based on computing the continuity and coercivity constants of the associated bilinear forms, which are defined as the extreme eigenvalues of specific generalized eigenvalue problems.\n\nThe foundational steps involve the assembly of the finite element matrices and the subsequent solution of these eigenproblems for each specified case.\n\n**1. Finite Element Discretization and Matrix Assembly**\n\nThe interval $[0, 1]$ is discretized into $n$ uniform elements, each of length $h=1/n$. This creates $n+1$ nodes, indexed from $0$ to $n$. Using continuous, piecewise linear basis functions (hat functions) $\\phi_i(x)$, a function is represented by a vector of nodal coefficients $u \\in \\mathbb{R}^{n+1}$.\n\nThe bilinear form $a(u,v) = \\int_{0}^{1} u'(x)v'(x)dx$ is discretized as $a_h(u_h, v_h) = u^{\\top}Kv$, where $K \\in \\mathbb{R}^{(n+1) \\times (n+1)}$ is the global stiffness matrix. It is assembled by summing the contributions from local stiffness matrices on each element $[x_i, x_{i+1}]$. The local stiffness matrix is given as $K_{loc} = \\frac{1}{h}\\begin{bmatrix}1  -1 \\\\ -1  1\\end{bmatrix}$.\n\nThe inner product $(u,v)_{L^2} = \\int_{0}^{1} u(x)v(x)dx$ is discretized as $u^{\\top}Mv$, where $M \\in \\mathbb{R}^{(n+1) \\times (n+1)}$ is the global consistent mass matrix. It is assembled from local mass matrices $M_{loc} = \\frac{h}{6}\\begin{bmatrix}2  1 \\\\ 1  2\\end{bmatrix}$.\n\nThe assembly process for $K$ and $M$ results in symmetric, tridiagonal matrices of size $(n+1) \\times (n+1)$. For $i, j \\in \\{0, \\dots, n\\}$:\n- The stiffness matrix $K$ has entries:\n$$\nK_{ij} =\n\\begin{cases}\n    1/h  \\text{if } i=j=0 \\text{ or } i=j=n \\\\\n    2/h  \\text{if } i=j \\text{ and } 0in \\\\\n    -1/h  \\text{if } |i-j|=1 \\\\\n    0  \\text{otherwise}\n\\end{cases}\n$$\n- The mass matrix $M$ has entries:\n$$\nM_{ij} =\n\\begin{cases}\n    h/3  \\text{if } i=j=0 \\text{ or } i=j=n \\\\\n    2h/3  \\text{if } i=j \\text{ and } 0in \\\\\n    h/6  \\text{if } |i-j|=1 \\\\\n    0  \\text{otherwise}\n\\end{cases}\n$$\n\n**2. Generalized Eigenvalue Problem Formulation**\n\nThe problem defines the discrete continuity and coercivity constants with respect to the discrete $H^1$ norm $\\|u\\|_{H^1_h}$, where $\\|u\\|_{H^1_h}^2 = u^{\\top}(K+M)u$. Let $H = K+M$. For a symmetric bilinear form $b(u,v) = u^{\\top}Bv$, the constants are:\n- $C_{\\mathrm{cont}}(B) = \\sup_{u\\neq 0} \\frac{u^{\\top} B u}{u^{\\top} H u}$\n- $C_{\\mathrm{coer}}(B;W) = \\inf_{u\\in W\\setminus\\{0\\}} \\frac{u^{\\top} B u}{u^{\\top} H u}$\n\nThese correspond to the largest and smallest (on subspace $W$) eigenvalues, respectively, of the generalized eigenvalue problem $B u = \\lambda H u$. Since $H$ is symmetric and positive definite (as it's a sum of a positive semi-definite $K$ and a positive definite $M$), and the matrices $B$ considered ($K$ and $K_{\\gamma}$) are symmetric, this is a standard problem with real eigenvalues.\n\n**3. Explicit Mean-Zero Constraint**\n\nIn this method, we work with the original stiffness matrix $B=K$ but restrict the function space to the mean-zero subspace $S = \\{u \\in \\mathbb{R}^{n+1} : \\mathbf{1}^{\\top} M u = 0\\}$.\n- The continuity constant is $C_{\\mathrm{cont}}(K) = \\lambda_{\\max}(K, H)$, the largest generalized eigenvalue of the pair $(K, H)$.\n- The stiffness matrix $K$ is singular. Its kernel is spanned by the constant vector $\\mathbf{1} = (1, 1, \\dots, 1)^{\\top}$, for which $K\\mathbf{1} = \\mathbf{0}$. This corresponds to a generalized eigenvalue $\\lambda_0 = 0$.\n- The coercivity constant $C_{\\mathrm{coer}}(K;S)$ is the infimum of the Rayleigh quotient over the subspace $S$. As prescribed, this is calculated as the smallest positive generalized eigenvalue of $(K,H)$, which we denote $\\lambda_{\\min, +}(K, H)$. This is justified because all eigenvectors corresponding to non-zero eigenvalues are $M$-orthogonal to the constant functions, and thus belong to $S$.\n- The condition number is $\\kappa_{\\mathrm{explicit}} = C_{\\mathrm{cont}}(K) / C_{\\mathrm{coer}}(K;S)$.\n\n**4. Penalty Method**\n\nThis method regularizes the problem by adding a penalty term to the bilinear form. The corresponding matrix is $B = K_{\\gamma}$, where\n$$\nK_{\\gamma} = K + \\dfrac{\\gamma}{\\mathbf{1}^{\\top} M \\mathbf{1}}(M\\mathbf{1})(M\\mathbf{1})^{\\top}\n$$\nHere, $\\gamma  0$ is the penalty parameter, and $(M\\mathbf{1})(M\\mathbf{1})^{\\top}$ is a rank-one matrix representing an outer product. The vector $M\\mathbf{1}$ is the vector of row-sums of $M$, which corresponds to the vector of integrals of the basis functions. The term $\\mathbf{1}^{\\top} M \\mathbf{1}$ is the integral of the function that is identically $1$ over the domain, which is $1$. The matrix $K_{\\gamma}$ is positive definite for any $\\gamma  0$, so the problem becomes well-posed on the entire space $\\mathbb{R}^{n+1}$.\n- The continuity constant is $C_{\\mathrm{cont}}(K_{\\gamma}) = \\lambda_{\\max}(K_{\\gamma}, H)$, the largest generalized eigenvalue of $(K_{\\gamma}, H)$.\n- The coercivity constant is $C_{\\mathrm{coer}}(K_{\\gamma};\\mathbb{R}^{n+1}) = \\lambda_{\\min}(K_{\\gamma}, H)$, the smallest generalized eigenvalue of $(K_{\\gamma}, H)$.\n- The condition number is $\\kappa_{\\mathrm{penalty}}(\\gamma) = C_{\\mathrm{cont}}(K_{\\gamma}) / C_{\\mathrm{coer}}(K_{\\gamma};\\mathbb{R}^{n+1})$.\n\n**5. Computational Procedure**\n\nFor each test case $(n, \\text{method}, \\gamma)$:\n1. Determine the number of nodes $N = n+1$ and the mesh size $h=1/n$.\n2. Assemble the $(N \\times N)$ matrices $K$ and $M$ based on their element-wise definitions.\n3. Construct the norm matrix $H = K+M$.\n4. **If the method is 'explicit'**:\n    a. Let $B=K$.\n    b. Solve the generalized eigenvalue problem $K u = \\lambda H u$ to find all eigenvalues.\n    c. $C_{\\mathrm{cont}}(K)$ is the largest eigenvalue.\n    d. $C_{\\mathrm{coer}}(K;S)$ is the second-smallest eigenvalue (the smallest positive one).\n5. **If the method is 'penalty'**:\n    a. Compute the vector $c = M\\mathbf{1}$ and the scalar $d = \\mathbf{1}^{\\top}c$.\n    b. Construct the penalized matrix $K_{\\gamma} = K + (\\gamma/d) c c^{\\top}$. Let $B=K_{\\gamma}$.\n    c. Solve the generalized eigenvalue problem $K_{\\gamma} u = \\lambda H u$ to find all eigenvalues.\n    d. $C_{\\mathrm{cont}}(K_{\\gamma})$ is the largest eigenvalue.\n    e. $C_{\\mathrm{coer}}(K_{\\gamma};\\mathbb{R}^{n+1})$ is the smallest eigenvalue.\n6. Compute the condition number $\\kappa$ as the ratio of the two constants.\n7. Store the three computed values, rounded to six decimal places, for the final output.\n\nThis procedure will be implemented using numerical linear algebra routines to solve the generalized eigenvalue problems.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef assemble_matrices(n: int):\n    \"\"\"\n    Assembles the 1D finite element stiffness (K) and mass (M) matrices\n    for a uniform mesh with n elements.\n    \"\"\"\n    if n  1:\n        raise ValueError(\"Number of elements n must be at least 1.\")\n\n    num_nodes = n + 1\n    h = 1.0 / n\n\n    K = np.zeros((num_nodes, num_nodes))\n    M = np.zeros((num_nodes, num_nodes))\n\n    # Local stiffness and mass matrices\n    k_loc = (1.0 / h) * np.array([[1, -1], [-1, 1]])\n    m_loc = (h / 6.0) * np.array([[2, 1], [1, 2]])\n\n    for i in range(n):\n        # Global indices for the i-th element\n        idx = np.array([i, i + 1])\n        K[np.ix_(idx, idx)] += k_loc\n        M[np.ix_(idx, idx)] += m_loc\n\n    return K, M\n\ndef solve_case(n: int, method: str, gamma: float = 0.0):\n    \"\"\"\n    Solves a single case for the given parameters.\n    Returns a list of [C_cont, C_coer, kappa].\n    \"\"\"\n    K, M = assemble_matrices(n)\n    H = K + M\n\n    if method == 'explicit':\n        B = K\n        # Solve the generalized eigenvalue problem B*x = lambda*H*x\n        # eigh returns sorted eigenvalues and corresponding eigenvectors\n        eigenvalues = eigh(B, H, eigvals_only=True)\n        \n        # Continuity constant is the largest eigenvalue\n        c_cont = eigenvalues[-1]\n        \n        # Coercivity constant is the smallest positive eigenvalue.\n        # The smallest eigenvalue is ~0 due to the null space of K.\n        c_coer = eigenvalues[1]\n        \n    elif method == 'penalty':\n        num_nodes = n + 1\n        ones_vec = np.ones(num_nodes)\n        \n        # Construct the penalized matrix K_gamma\n        M_one_vec = M @ ones_vec\n        one_M_one = ones_vec.T @ M_one_vec\n        \n        # The penalty term is a rank-1 update\n        penalty_matrix = (gamma / one_M_one) * np.outer(M_one_vec, M_one_vec)\n        K_gamma = K + penalty_matrix\n        B = K_gamma\n\n        eigenvalues = eigh(B, H, eigvals_only=True)\n        \n        # Continuity constant is the largest eigenvalue\n        c_cont = eigenvalues[-1]\n        \n        # Coercivity constant is the smallest eigenvalue.\n        # K_gamma is positive definite for gamma  0.\n        c_coer = eigenvalues[0]\n\n    else:\n        raise ValueError(f\"Unknown method '{method}'\")\n\n    if c_coer  1e-15: # Avoid division by a near-zero number\n      kappa = np.inf\n    else:\n      kappa = c_cont / c_coer\n\n    return [c_cont, c_coer, kappa]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (n, method, gamma)\n        (1,  'explicit', 0.0),    # Case A\n        (8,  'explicit', 0.0),    # Case B\n        (8,  'penalty',  1e-8),  # Case C\n        (8,  'penalty',  1.0),   # Case D\n        (8,  'penalty',  1e8),   # Case E\n        (32, 'penalty',  1e-2),  # Case F\n    ]\n\n    results = []\n    for n, method, gamma in test_cases:\n        case_result = solve_case(n, method, gamma)\n        results.append(case_result)\n        \n    # Format the output string as per the problem specification\n    formatted_results = []\n    for res_list in results:\n        # Round to six decimal places and format as string\n        formatted_list = [f\"{x:.6f}\" for x in res_list]\n        formatted_results.append(f\"[{','.join(formatted_list)}]\")\n    \n    final_output_string = f\"[{','.join(formatted_results)}]\"\n\n    print(final_output_string)\n\nsolve()\n\n```"
        }
    ]
}