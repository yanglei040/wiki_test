{
    "hands_on_practices": [
        {
            "introduction": "The stability properties of a time integrator are fundamental to its performance on stiff systems. This exercise provides direct, hands-on practice in analyzing these properties starting from a method's definition, the Butcher tableau. By deriving the stability function $R(z)$ for a Singly Diagonally Implicit Runge-Kutta (SDIRK) method and imposing conditions for accuracy and L-stability, you will connect the algebraic structure of the method to its practical utility for problems with a wide range of time scales .",
            "id": "3459568",
            "problem": "Consider the Singly Diagonally Implicit Runge–Kutta (SDIRK) family of two-stage methods applied to the linear test problem $\\frac{dy}{dt}=\\lambda y$ with stepsize $h$ and $z=h\\lambda$. Use the stiffly accurate two-stage SDIRK tableau defined by the parameter $\\gamma$:\n$$\n\\begin{array}{c|cc}\n\\gamma & \\gamma & 0 \\\\\n1 & 1-\\gamma & \\gamma \\\\\n\\hline\n & 1-\\gamma & \\gamma\n\\end{array}\n$$\nso that $c_{1}=\\gamma$, $c_{2}=1$, $A=\\begin{pmatrix}\\gamma&0\\\\ 1-\\gamma&\\gamma\\end{pmatrix}$, and $b^{\\top}=(1-\\gamma,\\gamma)$, with $0<\\gamma$ and the stiff accuracy condition $b^{\\top}$ equal to the last row of $A$. Starting from the Runge–Kutta stage definition and the one-step update (no shortcut stability formulas permitted), derive the linear stability function $R(z)$ satisfying $y_{n+1}=R(z)\\,y_{n}$ in closed form. Then, by matching the Taylor expansion of $R(z)$ to $\\exp(z)$ up to order $\\mathcal{O}(z^{3})$, impose the second-order accuracy conditions to determine the admissible $\\gamma$. Finally, require the method to be L-stable, meaning A-stable and with $\\lim_{|z|\\to\\infty,\\,\\operatorname{Re}z<0}R(z)=0$, and identify the unique value of $\\gamma$ in the open interval $(0,1)$ that achieves L-stability. Briefly interpret why this choice is desirable for stiff Partial Differential Equation (PDE) semi-discretizations. Provide your final answer as the exact value of $\\gamma$ (no rounding).",
            "solution": "We are given a two-stage Singly Diagonally Implicit Runge–Kutta (SDIRK) method with parameter $\\gamma$ and stiff accuracy, applied to the linear test equation $\\frac{dy}{dt}=\\lambda y$ with $z=h\\lambda$. The stages $Y_{1}$ and $Y_{2}$ and the update $y_{n+1}$ are defined by the fundamental Runge–Kutta stage equations\n$$\nY_{i}=y_{n}+h\\sum_{j=1}^{s}a_{ij}f\\!\\left(Y_{j}\\right),\\quad i=1,\\dots,s,\n$$\nand the one-step formula\n$$\ny_{n+1}=y_{n}+h\\sum_{j=1}^{s}b_{j}f\\!\\left(Y_{j}\\right),\n$$\nwhere $s=2$ and $f(y)=\\lambda y$. With the tableau\n$$\nA=\\begin{pmatrix}\\gamma&0\\\\ 1-\\gamma&\\gamma\\end{pmatrix},\\quad b^{\\top}=(1-\\gamma,\\gamma),\\quad c=\\begin{pmatrix}\\gamma\\\\ 1\\end{pmatrix},\n$$\nthe stages satisfy\n$$\nY_{1}=y_{n}+h\\gamma\\lambda Y_{1},\\qquad Y_{2}=y_{n}+h\\big[(1-\\gamma)\\lambda Y_{1}+\\gamma\\lambda Y_{2}\\big].\n$$\nIntroduce $z=h\\lambda$. Solving the first stage,\n$$\n(1-\\gamma z)Y_{1}=y_{n}\\quad\\Rightarrow\\quad Y_{1}=\\frac{y_{n}}{1-\\gamma z}.\n$$\nFor the second stage,\n$$\n(1-\\gamma z)Y_{2}=y_{n}+z(1-\\gamma)Y_{1}=y_{n}+z(1-\\gamma)\\frac{y_{n}}{1-\\gamma z},\n$$\nso\n$$\nY_{2}=\\frac{y_{n}}{1-\\gamma z}+\\frac{z(1-\\gamma)}{(1-\\gamma z)^{2}}\\,y_{n}=\\frac{1+(1-2\\gamma)z}{(1-\\gamma z)^{2}}\\,y_{n}.\n$$\nBy stiff accuracy, $y_{n+1}=Y_{2}$, hence the stability function is\n$$\nR(z)=\\frac{y_{n+1}}{y_{n}}=\\frac{1+(1-2\\gamma)z}{(1-\\gamma z)^{2}}.\n$$\n\nTo impose second-order accuracy, we require that the Taylor expansion of $R(z)$ matches the Taylor expansion of $\\exp(z)$ through terms of order $\\mathcal{O}(z^{2})$, that is,\n$$\nR(z)=1+z+\\tfrac{1}{2}z^{2}+\\mathcal{O}(z^{3}).\n$$\nExpand $R(z)$ about $z=0$. Using\n$$\n(1-\\gamma z)^{-1}=1+\\gamma z+\\gamma^{2}z^{2}+\\mathcal{O}(z^{3}),\\qquad (1-\\gamma z)^{-2}=\\big((1-\\gamma z)^{-1}\\big)^{2}=1+2\\gamma z+3\\gamma^{2}z^{2}+\\mathcal{O}(z^{3}),\n$$\nwe obtain\n$$\nR(z)=\\big(1+(1-2\\gamma)z\\big)\\big(1+2\\gamma z+3\\gamma^{2}z^{2}\\big)+\\mathcal{O}(z^{3}).\n$$\nCollect coefficients:\n$$\nR(z)=1+\\big(2\\gamma+1-2\\gamma\\big)z+\\big(3\\gamma^{2}+2\\gamma(1-2\\gamma)\\big)z^{2}+\\mathcal{O}(z^{3})=1+z+\\big(2\\gamma-\\gamma^{2}\\big)z^{2}+\\mathcal{O}(z^{3}).\n$$\nMatching the $z^{2}$ coefficient to $\\tfrac{1}{2}$ yields the second-order condition\n$$\n2\\gamma-\\gamma^{2}=\\tfrac{1}{2}\\quad\\Longleftrightarrow\\quad \\gamma^{2}-2\\gamma+\\tfrac{1}{2}=0.\n$$\nSolving,\n$$\n\\gamma=1\\pm\\frac{1}{\\sqrt{2}}.\n$$\n\nNext, ensure L-stability. By definition, L-stability requires A-stability and $\\lim_{|z|\\to\\infty,\\,\\operatorname{Re}z<0}R(z)=0$. From the closed form,\n$$\nR(z)=\\frac{1+(1-2\\gamma)z}{(1-\\gamma z)^{2}},\n$$\nwe see that for any $\\gamma\\neq 0$,\n$$\n\\lim_{|z|\\to\\infty,\\,\\operatorname{Re}z<0}R(z)=0,\n$$\nbecause the numerator is of degree $1$ in $z$ while the denominator is of degree $2$.\n\nFor A-stability, it suffices to verify $|R(\\mathrm{i}y)|\\leq 1$ for all real $y$ and invoke the maximum modulus principle on the left half-plane. Compute\n$$\n|R(\\mathrm{i}y)|^{2}=\\frac{\\big|1+\\mathrm{i}(1-2\\gamma)y\\big|^{2}}{\\big|(1-\\gamma\\,\\mathrm{i}y)^{2}\\big|^{2}}=\\frac{1+(1-2\\gamma)^{2}y^{2}}{\\big(1+\\gamma^{2}y^{2}\\big)^{2}}.\n$$\nThus $|R(\\mathrm{i}y)|\\leq 1$ for all real $y$ if and only if\n$$\n1+(1-2\\gamma)^{2}y^{2}\\leq \\big(1+\\gamma^{2}y^{2}\\big)^{2}\\quad\\text{for all real }y,\n$$\nequivalently,\n$$\n\\gamma^{4}y^{4}+\\big(2\\gamma^{2}-(1-2\\gamma)^{2}\\big)y^{2}\\geq 0\\quad\\text{for all real }y.\n$$\nSince $\\gamma^{4}y^{4}\\geq 0$, this holds for all $y$ if and only if $2\\gamma^{2}-(1-2\\gamma)^{2}\\geq 0$, that is,\n$$\n2\\gamma^{2}-\\big(1-4\\gamma+4\\gamma^{2}\\big)\\geq 0\\;\\Longleftrightarrow\\;-1+4\\gamma-2\\gamma^{2}\\geq 0\\;\\Longleftrightarrow\\;2\\gamma^{2}-4\\gamma+1\\leq 0.\n$$\nBut the second-order condition we solved earlier is precisely\n$$\n2\\gamma^{2}-4\\gamma+1=0,\n$$\nso either root of the second-order condition lies on the A-stability boundary and hence yields A-stability. Combining with the limit condition, both $\\gamma=1\\pm \\frac{1}{\\sqrt{2}}$ are L-stable. Imposing the natural restriction $\\gamma\\in(0,1)$ to keep the first stage abscissa within the step and to avoid a negative weight $1-\\gamma$, we select\n$$\n\\gamma=1-\\frac{1}{\\sqrt{2}}.\n$$\n\nInterpretation for stiff Partial Differential Equation (PDE) semi-discretizations: Upon spatial discretization of a parabolic or diffusive PDE, the resulting system has eigenvalues with large negative real parts. The L-stable choice $\\gamma=1-\\frac{1}{\\sqrt{2}}$ yields a stability function $R(z)$ that is A-stable and damps high-frequency components with $|z|$ large, since $\\lim_{|z|\\to\\infty}R(z)=0$. This produces strong attenuation of stiff modes without introducing spurious oscillations, enabling unconditionally stable timestepping unconstrained by stiffness-driven step-size limits.\n\nTherefore, the unique value of $\\gamma$ in $(0,1)$ that ensures L-stability is $\\gamma=1-\\frac{1}{\\sqrt{2}}$.",
            "answer": "$$\\boxed{1-\\frac{1}{\\sqrt{2}}}$$"
        },
        {
            "introduction": "Theoretical stability analysis comes to life when its consequences are observed in a numerical simulation. This practice moves from algebraic derivations to a concrete application, highlighting the critical difference between A-stability and the stronger property of L-stability. By implementing and comparing two famous implicit methods on a stiff system arising from the heat equation, you will directly visualize why L-stability is so desirable for effectively damping high-frequency solution components that can otherwise pollute a simulation .",
            "id": "3459566",
            "problem": "Consider the one-dimensional heat Partial Differential Equation (PDE) $u_t = \\nu u_{xx}$ on the spatial interval $[0,1]$ with homogeneous Dirichlet boundary conditions $u(0,t)=0$ and $u(1,t)=0$, and a smooth initial condition $u(x,0)=u_0(x)$. Use the method of lines with a second-order central difference discretization on a uniform grid with $M$ interior nodes to obtain a linear Ordinary Differential Equation (ODE) system of the form $U'(t) = A U(t)$, where $A$ is the discrete Laplacian scaled by the diffusion coefficient $\\nu$. Specifically, let the spatial mesh width be $\\Delta x = 1/(M+1)$ and define the tridiagonal matrix $L \\in \\mathbb{R}^{M \\times M}$ with entries $L_{ii} = -2/\\Delta x^2$, $L_{i,i+1} = 1/\\Delta x^2$, and $L_{i,i-1} = 1/\\Delta x^2$ for all valid indices $i$, and zero otherwise. Then $A = \\nu L$. The resulting linear ODE system is stiff when $M$ is moderately large because the eigenvalues of $A$ have large negative real parts scaling like $\\mathcal{O}(\\nu/\\Delta x^2)$.\n\nYour task is to demonstrate, using this semidiscrete model, that the implicit midpoint (also known as the Gauss–Legendre one-stage method) is Algebraically stable (A-stable) but not L-stable, and therefore does not strongly suppress high-frequency (stiff) modes. In contrast, the backward Euler method is both A-stable and L-stable, and it does suppress those modes. To do so, implement and apply the following two time integrators to the linear system $U'(t) = A U(t)$:\n\n- Backward Euler: one time step from $U^n$ to $U^{n+1}$ solves $(I - \\Delta t\\, A) U^{n+1} = U^n$.\n- Implicit midpoint (Gauss): one time step from $U^n$ to $U^{n+1}$ solves $(I - \\tfrac{1}{2}\\Delta t\\, A) U^{n+1} = (I + \\tfrac{1}{2}\\Delta t\\, A) U^n$.\n\nWork in the discrete sine eigenbasis of $L$, whose eigenvectors have components $v_m(i) = \\sin\\!\\big(m\\pi\\, i/(M+1)\\big)$ for $i \\in \\{1,\\dots,M\\}$ and $m \\in \\{1,\\dots,M\\}$, with the discrete inner product $\\langle u, v \\rangle_{\\Delta x} = \\Delta x \\sum_{i=1}^{M} u_i v_i$, and normalize each mode to unit discrete norm. The modal amplitude (coefficient) of a vector $U \\in \\mathbb{R}^M$ in mode $m$ is defined as the absolute value of the discrete projection $a_m = |\\langle U, \\widehat{v}_m \\rangle_{\\Delta x}|$, where $\\widehat{v}_m$ is the normalized mode.\n\nDefine the modal amplification over a single time step as the ratio of the modal amplitude after the step to that before the step for a pure mode initial condition. For a mixed-mode initial condition, define the high-to-low modal amplitude ratio after one time step as $r = a_{\\text{high}}/a_{\\text{low}}$.\n\nImplement a program that constructs $A$ and the normalized eigenmodes for the following fixed configuration and test suite, applies both integrators for one time step, and reports the requested modal amplification measurements:\n\n- Fixed configuration shared by all tests:\n  - Number of interior nodes: $M = 63$.\n  - Diffusion coefficient: $\\nu = 1$.\n  - Mesh width: $\\Delta x = 1/(M+1)$.\n  - Discrete Laplacian matrix: $A = \\nu L$ as described above.\n  - Discrete inner product: $\\langle u, v \\rangle_{\\Delta x} = \\Delta x \\sum_{i=1}^{M} u_i v_i$; use this to normalize modes and to compute projections.\n  - Low-frequency mode index: $m_{\\text{low}} = 1$.\n  - High-frequency mode index: $m_{\\text{high}} = M$.\n\n- Test suite (each test uses a single time step $N_{\\text{steps}} = 1$):\n  1. Test A (stiffest emphasis): initial condition $U^0 = \\widehat{v}_{m_{\\text{high}}}$, time step $\\Delta t_A = 1000\\, \\Delta x^2$. Output the two floating-point modal amplifications after one step: the magnitude of the amplification in mode $m_{\\text{high}}$ for implicit midpoint followed by that for backward Euler, in that order.\n  2. Test B (moderately stiff): initial condition $U^0 = \\widehat{v}_{m_{\\text{high}}}$, time step $\\Delta t_B = 1\\, \\Delta x^2$. Output the two floating-point modal amplifications after one step: the magnitude of the amplification in mode $m_{\\text{high}}$ for implicit midpoint followed by that for backward Euler, in that order.\n  3. Test C (mode competition): initial condition $U^0 = \\widehat{v}_{m_{\\text{low}}} + \\widehat{v}_{m_{\\text{high}}}$, time step $\\Delta t_C = 1000\\, \\Delta x^2$. Output the two floating-point high-to-low amplitude ratios after one step: first using implicit midpoint, then using backward Euler.\n\nYour program must:\n- Construct $A$, the normalized eigenmodes $\\widehat{v}_{m_{\\text{low}}}$ and $\\widehat{v}_{m_{\\text{high}}}$, and the specified initial data for each test.\n- Advance $U^0$ by one step with each method for the specified $\\Delta t$ and compute the required modal measurements using the discrete inner product.\n- Produce a single line of output containing the six results, rounded to six decimal places, as a comma-separated list enclosed in square brackets, in the following order: $[\\text{TestA\\_IM},\\text{TestA\\_BE},\\text{TestB\\_IM},\\text{TestB\\_BE},\\text{TestC\\_IM\\_ratio},\\text{TestC\\_BE\\_ratio}]$.\n\nAll numerical values must be printed as dimensionless floating-point numbers rounded to six decimal places. No physical units or angle units are involved. The final line must be exactly the list format described above, with no other text.",
            "solution": "The problem statement is evaluated to be scientifically sound, well-posed, and self-contained. All necessary parameters, definitions, and methods are clearly specified, forming a standard exercise in the numerical analysis of partial differential equations. The problem is valid and can be solved as stated.\n\nThe problem asks to demonstrate the difference in stability properties between the backward Euler (BE) and implicit midpoint (IM) time integration schemes when applied to the stiff system of ordinary differential equations (ODEs) arising from the spatial discretization of the one-dimensional heat equation, $u_t = \\nu u_{xx}$.\n\nThe method of lines with a centered finite difference scheme on a uniform grid with $M$ interior points and mesh width $\\Delta x = 1/(M+1)$ transforms the partial differential equation (PDE) into a system of $M$ coupled ODEs of the form $U'(t) = A U(t)$. The vector $U(t) \\in \\mathbb{R}^M$ contains the solution values at the interior grid points. The matrix $A = \\nu L$, where $L$ is the discrete Laplacian, is a real symmetric tridiagonal matrix. Its eigenvalues $\\lambda_m(A)$ are real and negative, given by:\n$$\n\\lambda_m(A) = \\nu \\frac{-4}{\\Delta x^2} \\sin^2\\left(\\frac{m\\pi}{2(M+1)}\\right) \\quad \\text{for } m = 1, \\dots, M.\n$$\nThe corresponding eigenvectors $\\widehat{v}_m$ are the discrete sine modes, which form an orthogonal basis for $\\mathbb{R}^M$ under the specified discrete inner product $\\langle u, v \\rangle_{\\Delta x} = \\Delta x \\sum_{i=1}^{M} u_i v_i$.\n\nThe stability of a time integration scheme for this system can be analyzed by examining its effect on each eigenmode. For an ODE $y' = \\lambda y$, a single step of a numerical method transforms the solution from $y^n$ to $y^{n+1} = g(\\lambda \\Delta t) y^n$, where $g(z)$ is the amplification factor and $z = \\lambda \\Delta t$. For the system $U'(t) = A U(t)$, the update for the coefficient of the $m$-th eigenmode is governed by the amplification factor $g(\\lambda_m(A) \\Delta t)$.\n\nThe two methods in question have the following update rules and amplification factors:\n1.  **Backward Euler (BE)**: $(I - \\Delta t\\, A) U^{n+1} = U^n$. The amplification factor is\n    $$\n    g_{BE}(z) = \\frac{1}{1 - z}.\n    $$\n2.  **Implicit Midpoint (IM)**: $(I - \\tfrac{1}{2}\\Delta t\\, A) U^{n+1} = (I + \\tfrac{1}{2}\\Delta t\\, A) U^n$. The amplification factor is\n    $$\n    g_{IM}(z) = \\frac{1 + z/2}{1 - z/2}.\n    $$\n\nA numerical method is **A-stable** if $|g(z)| \\le 1$ for all complex numbers $z$ with a non-positive real part, $\\operatorname{Re}(z) \\le 0$. This ensures that for any stable linear ODE system, the numerical solution does not grow without bound for any time step size $\\Delta t > 0$. Both BE and IM are A-stable.\n\nA method is **L-stable** if it is A-stable and, in addition, $\\lim_{\\operatorname{Re}(z) \\to -\\infty} |g(z)| = 0$. This property is crucial for stiff systems, where some eigenvalues $\\lambda_m$ are very large and negative. L-stability ensures that the corresponding highly oscillatory or rapidly decaying modes (high-frequency modes) are strongly damped by the numerical scheme, especially when a large time step $\\Delta t$ is used.\n\nAnalyzing the amplification factors:\n- For BE: $\\lim_{\\operatorname{Re}(z) \\to -\\infty} |g_{BE}(z)| = \\lim_{\\operatorname{Re}(z) \\to -\\infty} |\\frac{1}{1-z}| = 0$. Backward Euler is L-stable.\n- For IM: $\\lim_{\\operatorname{Re}(z) \\to -\\infty} |g_{IM}(z)| = \\lim_{\\operatorname{Re}(z) \\to -\\infty} |\\frac{1+z/2}{1-z/2}| = |\\frac{z/2}{-z/2}| = 1$. Implicit midpoint is not L-stable.\n\nThis analysis predicts that BE will effectively damp high-frequency modes, while IM will preserve them, causing persistent, unphysical oscillations in the numerical solution for stiff problems. The numerical tests are designed to verify this behavior.\n\nThe implementation proceeds as follows:\n1.  Set the constants: $M=63$, $\\nu=1$, $\\Delta x=1/64$.\n2.  Construct the $M \\times M$ matrix $A$.\n3.  Construct the normalized eigenvectors $\\widehat{v}_{m_{\\text{low}}}$ and $\\widehat{v}_{m_{\\text{high}}}$ for modes $m_{\\text{low}}=1$ and $m_{\\text{high}}=63$, respectively. The normalization is performed with respect to the discrete inner product $\\langle u, v \\rangle_{\\Delta x}$. The normalization factor is found to be $\\sqrt{2}$.\n4.  For each test case, construct the initial condition vector $U^0$.\n5.  Implement functions that perform a single time step for BE and IM. These functions solve the respective linear systems, for instance, $(I - \\Delta t\\, A) U^{n+1} = U^n$ for BE.\n6.  Apply one time step to obtain $U^1$ for each method.\n7.  Compute the required modal amplifications or amplitude ratios by projecting the resulting vector $U^1$ back onto the corresponding eigenmodes using the discrete inner product. For an initial condition $U^0$ that is a pure mode $\\widehat{v}_m$, the amplification is $|\\langle U^1, \\widehat{v}_m \\rangle_{\\Delta x}| / |\\langle U^0, \\widehat{v}_m \\rangle_{\\Delta x}|$. For a mixed-mode initial condition, the high-to-low ratio is calculated from the amplitudes of the respective modes in $U^1$.\n\n- **Test A** uses a large time step relative to the stability limit of an explicit method ($\\Delta t_A = 1000 \\Delta x^2$). This corresponds to a large negative $z = \\lambda_{m_{\\text{high}}} \\Delta t_A$. As predicted, IM yields an amplification factor close to $1$, while BE yields one close to $0$.\n- **Test B** uses a smaller time step ($\\Delta t_B = \\Delta x^2$). Here, $z = \\lambda_{m_{\\text{high}}} \\Delta t_B \\approx -4$. Both methods damp the mode, but to different extents.\n- **Test C** demonstrates the practical consequence of non-L-stability. With an initial condition combining low and high-frequency modes and a large time step, BE selectively damps the stiff high-frequency component, resulting in a small high-to-low amplitude ratio. In contrast, IM fails to damp this component, leading to a large ratio, meaning the stiff component persists and pollutes the solution.\n\nThe results from the code will numerically confirm this theoretical understanding.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the given problem to demonstrate the stability properties of\n    Backward Euler and Implicit Midpoint methods.\n    \"\"\"\n    # Fixed configuration shared by all tests\n    M = 63\n    nu = 1.0\n    dx = 1.0 / (M + 1)\n    m_low = 1\n    m_high = M\n\n    # 1. Construct the matrix A = nu * L\n    A = np.zeros((M, M))\n    diag_val = -2.0 * nu / dx**2\n    off_diag_val = 1.0 * nu / dx**2\n    for i in range(M):\n        A[i, i] = diag_val\n        if i > 0:\n            A[i, i - 1] = off_diag_val\n        if i  M - 1:\n            A[i, i + 1] = off_diag_val\n\n    # 2. Construct normalized eigenmodes\n    i_vals = np.arange(1, M + 1)\n    \n    v_low_unnormalized = np.sin(m_low * np.pi * i_vals / (M + 1))\n    v_high_unnormalized = np.sin(m_high * np.pi * i_vals / (M + 1))\n\n    # The squared norm ||v_m||^2 is dx * sum(sin^2(...)) = dx * (M+1)/2 = 1/2.\n    # The normalization factor is sqrt(2).\n    norm_factor = np.sqrt(2.0)\n    v_hat_low = norm_factor * v_low_unnormalized\n    v_hat_high = norm_factor * v_high_unnormalized\n\n    # 3. Define discrete inner product\n    def inner_product(u, v):\n        return dx * np.sum(u * v)\n\n    # 4. Implement time stepping functions\n    def step_be(U0, A_matrix, dt):\n        mat = np.eye(M) - dt * A_matrix\n        return np.linalg.solve(mat, U0)\n\n    def step_im(U0, A_matrix, dt):\n        mat_lhs = np.eye(M) - 0.5 * dt * A_matrix\n        mat_rhs = np.eye(M) + 0.5 * dt * A_matrix\n        rhs_vec = mat_rhs @ U0\n        return np.linalg.solve(mat_lhs, rhs_vec)\n\n    # --- Test Suite ---\n    results = []\n\n    # Test A: Stiffest emphasis\n    dt_A = 1000.0 * dx**2\n    U0_A = v_hat_high\n    \n    U1_A_IM = step_im(U0_A, A, dt_A)\n    U1_A_BE = step_be(U0_A, A, dt_A)\n    \n    # an initial pure mode has amplitude 1 since eigenvectors are normalized\n    a0_A_high = 1.0 \n    \n    a1_A_IM_high = np.abs(inner_product(U1_A_IM, v_hat_high))\n    a1_A_BE_high = np.abs(inner_product(U1_A_BE, v_hat_high))\n\n    amp_A_IM = a1_A_IM_high / a0_A_high\n    amp_A_BE = a1_A_BE_high / a0_A_high\n    results.extend([amp_A_IM, amp_A_BE])\n\n    # Test B: Moderately stiff\n    dt_B = 1.0 * dx**2\n    U0_B = v_hat_high\n    \n    U1_B_IM = step_im(U0_B, A, dt_B)\n    U1_B_BE = step_be(U0_B, A, dt_B)\n\n    a0_B_high = 1.0\n    a1_B_IM_high = np.abs(inner_product(U1_B_IM, v_hat_high))\n    a1_B_BE_high = np.abs(inner_product(U1_B_BE, v_hat_high))\n\n    amp_B_IM = a1_B_IM_high / a0_B_high\n    amp_B_BE = a1_B_BE_high / a0_B_high\n    results.extend([amp_B_IM, amp_B_BE])\n\n    # Test C: Mode competition\n    dt_C = 1000.0 * dx**2\n    U0_C = v_hat_low + v_hat_high\n    \n    U1_C_IM = step_im(U0_C, A, dt_C)\n    U1_C_BE = step_be(U0_C, A, dt_C)\n\n    # For U0 = v_low + v_high, the initial amplitudes are both 1.\n    # After one step, the solution is a linear combination of the stepped eigenmodes.\n    # The amplitude of mode m in U1 is |U1, v_m|\n    a1_C_IM_high = np.abs(inner_product(U1_C_IM, v_hat_high))\n    a1_C_IM_low = np.abs(inner_product(U1_C_IM, v_hat_low))\n    \n    a1_C_BE_high = np.abs(inner_product(U1_C_BE, v_hat_high))\n    a1_C_BE_low = np.abs(inner_product(U1_C_BE, v_hat_low))\n\n    ratio_C_IM = a1_C_IM_high / a1_C_IM_low\n    ratio_C_BE = a1_C_BE_high / a1_C_BE_low\n    results.extend([ratio_C_IM, ratio_C_BE])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "For the most demanding stiff and non-normal systems, even L-stability may not be sufficient to guarantee robust behavior. This advanced practice delves into the concept of algebraic stability (or G-stability for linear problems), a more powerful condition that prevents transient energy growth. You will construct challenging dissipative systems where a popular L-stable SDIRK method fails to be contractive, while an algebraically stable method like Radau IIA remains robust, illustrating a key principle in the design of state-of-the-art numerical integrators .",
            "id": "3459543",
            "problem": "Consider linear autonomous partial differential equations (PDEs) after spatial semi-discretization, which yield systems of ordinary differential equations of the form $u'(t) = L u(t)$ where $u(t) \\in \\mathbb{R}^n$ and $L \\in \\mathbb{R}^{n \\times n}$. Let $H \\in \\mathbb{R}^{n \\times n}$ be a symmetric positive definite matrix defining an energy inner product $\\langle x, y \\rangle_H = x^\\top H y$ and norm $\\lVert x \\rVert_H^2 = x^\\top H x$. The operator $L$ is called dissipative with respect to $H$ if $L^\\top H + H L \\preceq 0$. For a one-step Runge–Kutta method with Butcher matrix $A \\in \\mathbb{R}^{s \\times s}$ and weights $b \\in \\mathbb{R}^s$, applied to the linear system $u' = L u$, the method defines a linear propagator $R(hL) \\in \\mathbb{R}^{n \\times n}$ such that $u_{n+1} = R(hL) u_n$ for time step $h  0$. Define the $H$-norm amplification factor of one step as\n$$\n\\mathcal{A}_H(R(hL)) \\equiv \\sup_{x \\neq 0} \\frac{\\lVert R(hL) x \\rVert_H}{\\lVert x \\rVert_H} = \\sigma_{\\max}\\!\\left(H^{\\frac{1}{2}} R(hL) H^{-\\frac{1}{2}}\\right),\n$$\nwhere $\\sigma_{\\max}(\\cdot)$ denotes the largest singular value and $H^{\\pm \\frac{1}{2}}$ denotes the symmetric square root and its inverse.\n\nYour tasks are to algorithmically investigate unconditional stability (contractivity in $H$ for all $h  0$) of lower-triangular Diagonally Implicit Runge–Kutta (DIRK) schemes versus the Radau IIA method, to determine how stage order impacts robustness, and to construct a concrete dissipative example where a DIRK scheme loses $G$-stability (fails to be contractive) while Radau IIA does not.\n\nStart from first principles:\n\n- The definition of a Runge–Kutta step for a linear system $u' = L u$ is given by stages $Y_i = u_n + h \\sum_{j=1}^s a_{ij} L Y_j$ for $i \\in \\{1,\\dots,s\\}$, and then $u_{n+1} = u_n + h \\sum_{i=1}^s b_i L Y_i$.\n- The stacked stage system can be written using the Kronecker product as\n$$\n\\left(I_{s} \\otimes I_n - h A \\otimes L\\right) \\operatorname{vec}(Y) = (e \\otimes I_n) u_n,\n$$\nwith $e \\in \\mathbb{R}^s$ the vector of ones, and the one-step propagator can be expressed as\n$$\nR(hL) = I_n + h (b^\\top \\otimes L)\\, \\left(I_{s} \\otimes I_n - h A \\otimes L\\right)^{-1} (e \\otimes I_n).\n$$\n- An $H$-dissipative $L$ means $L^\\top H + H L \\preceq 0$.\n\nImplement the following three methods (all lower-triangular in the sense that $A$ is lower triangular):\n\n- Backward Euler (one-stage DIRK, order $1$, stage order $1$): with $s = 1$, $A = [1]$, $b = [1]$, and $c = [1]$.\n- A two-stage L-stable Singly Diagonally Implicit Runge–Kutta (SDIRK) method of order $2$ with parameter $\\gamma = 1 - 1/\\sqrt{2}$, with\n$$\nA = \\begin{bmatrix} \\gamma  0 \\\\ 1-\\gamma  \\gamma \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1 - \\gamma \\\\ \\gamma \\end{bmatrix}, \\quad c = \\begin{bmatrix} \\gamma \\\\ 1 \\end{bmatrix}.\n$$\n- Radau IIA with $s = 2$ (order $3$, stage order $2$), with\n$$\nA = \\begin{bmatrix} \\frac{5}{12}  -\\frac{1}{12} \\\\[4pt] \\frac{3}{4}  \\frac{1}{4} \\end{bmatrix}, \\quad b = \\begin{bmatrix} \\frac{3}{4} \\\\[4pt] \\frac{1}{4} \\end{bmatrix}, \\quad c = \\begin{bmatrix} \\frac{1}{3} \\\\[4pt] 1 \\end{bmatrix}.\n$$\n\nUsing only the above definitions and formulas, write a program that:\n\n- Constructs three deterministic families of $H$-dissipative test operators $(H, L)$ in dimension $n = 2$:\n    1. Rank-one dissipation with anisotropic metric: for each $\\varepsilon \\in \\{0.01, 0.1\\}$ and $M \\in \\{2, 5, 10, 20\\}$, let $H = \\operatorname{diag}(1, \\varepsilon)$ and $S = \\varepsilon\\, v v^\\top$ with $v = [1,\\, M]^\\top$, and set $L = - H^{-1} S$. Verify that $L^\\top H + H L = -2 S \\preceq 0$.\n    2. Skew-damped non-normal family: for each $\\alpha \\in \\{0.1, 1.0\\}$ and $\\beta \\in \\{5.0, 10.0, 50.0\\}$, take $H = I_2$ and $L = -\\alpha I_2 + \\beta J$ with $J = \\begin{bmatrix} 0  1 \\\\ -1  0 \\end{bmatrix}$. Verify that $L^\\top + L = -2 \\alpha I_2 \\preceq 0$.\n    3. Similarity-transformed diagonal dissipation (tunable non-normality): for each $c \\in \\{5.0, 10.0, 20.0, 50.0\\}$ and $(k_1, k_2) \\in \\{(1.0, 10.0),\\, (1.0, 50.0)\\}$, set $C = \\begin{bmatrix} 1  c \\\\ 0  1 \\end{bmatrix}$, $H = C^\\top C$, $K = -\\operatorname{diag}(k_1, k_2)$, and $L = C^{-1} K C$. Show that $L^\\top H + H L = C^\\top (K^\\top + K) C \\preceq 0$.\n- For each $(H, L)$ from the above families and each time step $h \\in \\{0.1, 0.5, 1.0, 2.0, 5.0, 10.0\\}$, compute the $H$-norm amplification factor $\\mathcal{A}_H(R(hL))$ for the three methods.\n- Treat a method as contractive on a case if $\\mathcal{A}_H(R(hL)) \\le 1 + \\tau$, where $\\tau = 10^{-8}$ is a fixed numerical tolerance.\n\nUsing the computed amplification factors, your program must produce the following outputs aggregated over the entire test suite:\n\n- A boolean indicating whether Radau IIA is contractive for all cases and all $h$.\n- A boolean indicating whether the SDIRK method admits at least one violation (a case and an $h$ for which it is not contractive), thereby constructing an explicit counterexample to $G$-stability for the DIRK method while Radau IIA remains contractive.\n- Three floats giving the worst-case amplification factor (the largest observed $\\mathcal{A}_H(R(hL))$) over the test suite for, respectively, Backward Euler, SDIRK, and Radau IIA.\n- An integer giving the total number of violating cases (number of $(H, L, h)$ triples) for the SDIRK method.\n\nAngle units are not applicable. There are no physical units to report.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated Python-like list in the form $[\\text{radau\\_all}, \\text{sdirk\\_has\\_viol}, \\text{be\\_worst}, \\text{sdirk\\_worst}, \\text{radau\\_worst}, \\text{sdirk\\_viol\\_count}]$, for example, $[True,True,0.9999,1.0023,1.0,7]$.",
            "solution": "The problem requires a rigorous algorithmic investigation into the unconditional stability of specific implicit Runge-Kutta (RK) methods when applied to linear, autonomous, $H$-dissipative systems of ordinary differential equations. Unconditional stability, in this context, refers to contractivity in an energy norm defined by a symmetric positive definite matrix $H$ for any positive time step $h$.\n\n### Step 1: Preliminary Validation of the Problem Statement\nThe problem is first assessed for its validity.\n\n**Extraction of Givens:**\n- **System ODE:** $u'(t) = L u(t)$, with $u(t) \\in \\mathbb{R}^n$ and $L \\in \\mathbb{R}^{n \\times n}$.\n- **Energy Inner Product and Norm:** $\\langle x, y \\rangle_H = x^\\top H y$ and $\\lVert x \\rVert_H^2 = x^\\top H x$, where $H \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD).\n- **Dissipativity Condition:** $L$ is $H$-dissipative if $L^\\top H + H L \\preceq 0$.\n- **Runge-Kutta Propagator:** $u_{n+1} = R(hL) u_n$, where $R(hL) = I_n + h (b^\\top \\otimes L)\\, \\left(I_{s} \\otimes I_n - h A \\otimes L\\right)^{-1} (e \\otimes I_n)$.\n- **Amplification Factor:** $\\mathcal{A}_H(R(hL)) \\equiv \\sup_{x \\neq 0} \\frac{\\lVert R(hL) x \\rVert_H}{\\lVert x \\rVert_H} = \\sigma_{\\max}\\!\\left(H^{\\frac{1}{2}} R(hL) H^{-\\frac{1}{2}}\\right)$.\n- **Methods to Implement:**\n    1.  Backward Euler ($s=1$ DIRK): $A = [1]$, $b = [1]$.\n    2.  $s=2$ SDIRK (order 2, L-stable): $\\gamma = 1 - 1/\\sqrt{2}$, $A = \\begin{bmatrix} \\gamma  0 \\\\ 1-\\gamma  \\gamma \\end{bmatrix}$, $b = \\begin{bmatrix} 1 - \\gamma \\\\ \\gamma \\end{bmatrix}$.\n    3.  $s=2$ Radau IIA (order 3): $A = \\begin{bmatrix} \\tfrac{5}{12}  -\\tfrac{1}{12} \\\\ \\tfrac{3}{4}  \\tfrac{1}{4} \\end{bmatrix}$, $b = \\begin{bmatrix} \\tfrac{3}{4} \\\\ \\tfrac{1}{4} \\end{bmatrix}$.\n- **Test Problems ($n=2$):**\n    1.  Rank-one dissipation: $H = \\operatorname{diag}(1, \\varepsilon)$, $S = \\varepsilon\\, v v^\\top$ with $v = [1,\\, M]^\\top$, $L = - H^{-1} S$ for $\\varepsilon \\in \\{0.01, 0.1\\}$, $M \\in \\{2, 5, 10, 20\\}$.\n    2.  Skew-damped non-normal: $H = I_2$, $L = -\\alpha I_2 + \\beta J$ with $J = \\begin{bmatrix} 0  1 \\\\ -1  0 \\end{bmatrix}$ for $\\alpha \\in \\{0.1, 1.0\\}$, $\\beta \\in \\{5.0, 10.0, 50.0\\}$.\n    3.  Similarity-transformed: $C = \\begin{bmatrix} 1  c \\\\ 0  1 \\end{bmatrix}$, $H = C^\\top C$, $K = -\\operatorname{diag}(k_1, k_2)$, $L = C^{-1} K C$ for $c \\in \\{5.0, 10.0, 20.0, 50.0\\}$, $(k_1, k_2) \\in \\{(1.0, 10.0),\\, (1.0, 50.0)\\}$.\n- **Time Steps:** $h \\in \\{0.1, 0.5, 1.0, 2.0, 5.0, 10.0\\}$.\n- **Contractivity Tolerance:** $\\mathcal{A}_H(R(hL)) \\le 1 + \\tau$, with $\\tau = 10^{-8}$.\n- **Required Outputs:** Boolean for Radau IIA universal contractivity, boolean for SDIRK violation existence, worst-case amplification factors for all three methods, and total violation count for SDIRK.\n\n**Validation Verdict:**\nThe problem is scientifically grounded in the established theory of numerical ordinary differential equations, specifically the stability analysis of Runge-Kutta methods. The concepts of $H$-dissipativity, G-stability (of which this problem studies a linear variant), and the construction of the numerical propagator $R(hL)$ are standard. The test problems are well-defined and mathematically sound constructions of operators exhibiting properties (anisotropy, non-normality) known to be challenging for numerical integrators. The verification of dissipativity for each test family is a straightforward exercise in linear algebra, confirming their correctness. The problem is well-posed, objective, and complete. It poses a clear, verifiable, and non-trivial task. Therefore, the problem is **valid**.\n\n### Step 2: Principle-Based Solution Design\n\nThe core of the problem is to ascertain whether a numerical method preserves the non-increasing nature of the energy $\\lVert u \\rVert_H^2$ for any $H$-dissipative system. For a linear system $u' = Lu$, the analytical solution $u(t) = \\exp(tL)u_0$ has a non-increasing energy norm because the $H$-dissipativity condition $L^\\top H + H L \\preceq 0$ implies that $\\frac{d}{dt}\\lVert u(t) \\rVert_H^2 = (Lu)^\\top H u + u^\\top H (Lu) = u^\\top(L^\\top H + HL)u \\le 0$. A numerical method is contractive in the $H$-norm if $\\lVert u_{n+1} \\rVert_H \\le \\lVert u_n \\rVert_H$ for any step size $h > 0$. This is equivalent to requiring the $H$-norm amplification factor to be bounded by one: $\\mathcal{A}_H(R(hL)) \\le 1$.\n\nThe proposed algorithm systematically constructs the test problems and computes this amplification factor for each specified method, problem instance, and time step.\n\n**1. Algorithmic Construction of the Propagator $R(hL)$**\n\nGiven a Runge-Kutta method defined by its Butcher matrix $A \\in \\mathbb{R}^{s \\times s}$ and weights $b \\in \\mathbb{R}^s$, and a linear system defined by $L \\in \\mathbb{R}^{n \\times n}$, the one-step propagator $R(hL)$ is an $n \\times n$ matrix. For the specified problem dimension $n=2$, the propagator is a $2 \\times 2$ matrix. Its construction involves operations on matrices of size $sn \\times sn$.\n- With $n=2$, the identity matrices are $I_n = I_2$ and $I_s$.\n- The central matrix to be inverted is $M_{\\text{inv}} = (I_s \\otimes I_2 - h A \\otimes L)$, which has dimensions $(2s \\times 2s)$. For the one-stage Backward Euler method ($s=1$), this is a $2 \\times 2$ matrix. For the two-stage SDIRK and Radau IIA methods ($s=2$), it is a $4 \\times 4$ matrix.\n- The other components are the matrices $b^\\top \\otimes L$ (size $2 \\times 2s$) and $e \\otimes I_2$ (size $2s \\times 2$), where $e$ is the $s \\times 1$ vector of ones.\n- The propagator is then assembled as $R(hL) = I_2 + h (b^\\top \\otimes L) M_{\\text{inv}}^{-1} (e \\otimes I_2)$.\n\n**2. Algorithmic Computation of the Amplification Factor $\\mathcal{A}_H(R(hL))$**\n\nThe amplification factor is the operator norm of $R(hL)$ induced by the $H$-norm. The provided formula, $\\mathcal{A}_H(R(hL)) = \\sigma_{\\max}(H^{1/2} R(hL) H^{-1/2})$, transforms the problem into finding the largest singular value of a related matrix, which is equivalent to the standard Euclidean $2$-norm of that matrix.\n- First, the symmetric matrix square root $H^{1/2}$ and its inverse $H^{-1/2}$ are computed. Since $H$ is symmetric positive definite, these are well-defined real matrices. This is robustly handled via the eigendecomposition of $H$. If $H=PDP^\\top$ where $D$ is diagonal with positive eigenvalues, then $H^{1/2} = PD^{1/2}P^\\top$.\n- The matrix $T = H^{1/2} R(hL) H^{-1/2}$ is formed.\n- The singular values of $T$ are computed. The largest of these, $\\sigma_{\\max}(T)$, is the desired amplification factor $\\mathcal{A}_H(R(hL))$.\n\n**3. Rationale for Methods and Test Cases**\n\nThe choice of methods and test problems is purposeful.\n- **Methods**: Backward Euler and Radau IIA are known to be algebraically stable, a strong property which implies they are G-stable (and thus contractive for all $H$-dissipative linear systems). Their worst-case amplification factors are expected to be $\\le 1$. The SDIRK method, while L-stable (a desirable property for stiff equations), is not necessarily algebraically stable. Its lower stage order makes it susceptible to transient growth and loss of contractivity when the operator $L$ and metric $H$ generate highly non-normal dynamics, even if the system is dissipative.\n- **Test Cases**: The three families of $(H, L)$ pairs are designed to probe these weaknesses.\n    - Family 1 introduces non-normality through an anisotropic metric $H$ and a rank-one dissipative part.\n    - Family 2 introduces non-normality through a large skew-symmetric component in $L$.\n    - Family 3 provides a tunable way to generate a non-normal operator $L$ via a similarity transform, where the non-normality is controlled by the parameter $c$.\nThese cases create scenarios where the eigenvectors of the matrices involved are far from orthogonal, a hallmark of non-normal systems where transient growth can occur. It is in such cases that the SDIRK method is expected to fail, while the more robust Radau IIA scheme is expected to maintain contractivity.\n\n**4. Aggregation of Results**\n\nThe programmatic implementation will iterate through every combination of test case parameters and time steps. For each combination, it will compute $\\mathcal{A}_H$ for the three RK methods. The resulting list of amplification factors for each method is then analyzed to determine compliance with the contractivity condition $\\mathcal{A}_H \\le 1 + \\tau$. The final output consists of the aggregated statistics as required by the problem statement.",
            "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Solves the problem of evaluating the unconditional stability of three\n    Runge-Kutta methods for various families of H-dissipative systems.\n    \"\"\"\n    \n    # Numerical tolerance for contractivity check\n    tau = 1e-8\n    \n    # -- Method Definitions --\n    # Backward Euler (s=1, order 1)\n    A_be = np.array([[1.0]])\n    b_be = np.array([[1.0]])\n    \n    # SDIRK (s=2, order 2, L-stable)\n    gamma = 1.0 - 1.0 / np.sqrt(2.0)\n    A_sdirk = np.array([[gamma, 0.0], \n                        [1.0 - gamma, gamma]])\n    b_sdirk = np.array([[1.0 - gamma], \n                        [gamma]])\n    \n    # Radau IIA (s=2, order 3)\n    A_radau = np.array([[5.0/12.0, -1.0/12.0],\n                        [3.0/4.0,   1.0/4.0]])\n    b_radau = np.array([[3.0/4.0],\n                        [1.0/4.0]])\n    \n    methods = {\n        'BE': {'A': A_be, 'b': b_be},\n        'SDIRK': {'A': A_sdirk, 'b': b_sdirk},\n        'RadauIIA': {'A': A_radau, 'b': b_radau}\n    }\n    \n    # Time steps for analysis\n    h_vals = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n    \n    # Storage for amplification factors\n    results = {\n        'BE': [],\n        'SDIRK': [],\n        'RadauIIA': []\n    }\n\n    def compute_amplification_factor(A, b, L, H, h):\n        \"\"\"\n        Computes the H-norm amplification factor for a given RK method,\n        operator L, metric H, and step size h.\n        \"\"\"\n        s = A.shape[0]\n        n = L.shape[0]\n        \n        I_s = np.eye(s)\n        I_n = np.eye(n)\n        e = np.ones((s, 1))\n\n        # Construct the matrix to be inverted for the stage values\n        # M_inv_mat = (I_s kron I_n - h * A kron L)\n        M_to_invert = np.kron(I_s, I_n) - h * np.kron(A, L)\n        \n        # Calculate the propagator R(hL)\n        # R(hL) = I_n + h * (b^T kron L) * (M_to_invert)^-1 * (e kron I_n)\n        b_T_kron_L = np.kron(b.T, L)\n        e_kron_I_n = np.kron(e, I_n)\n        \n        try:\n            M_inv = np.linalg.inv(M_to_invert)\n        except np.linalg.LinAlgError:\n            # For extremely large h*lambda, matrix can become singular.\n            # In this case, amplification is determined by stability at infinity.\n            # All methods here are L-stable, so R(inf) is a zero or near-zero matrix.\n            # Amplification will be close to zero.\n            return 0.0\n\n        R_hL = I_n + h * b_T_kron_L @ M_inv @ e_kron_I_n\n        \n        # Compute H^(1/2) and H^(-1/2)\n        H_sqrt = scipy.linalg.sqrtm(H)\n        H_inv_sqrt = scipy.linalg.inv(H_sqrt)\n        \n        # Form the matrix for SVD\n        # T = H_sqrt @ R_hL @ H_inv_sqrt\n        T = H_sqrt @ R_hL @ H_inv_sqrt\n        \n        # The amplification factor is the largest singular value of T\n        singular_values = np.linalg.svd(T, compute_uv=False)\n        return singular_values[0]\n\n    # --- Test Case Generation and Computation ---\n    \n    # Family 1: Rank-one dissipation with anisotropic metric\n    eps_vals = [0.01, 0.1]\n    M_vals = [2.0, 5.0, 10.0, 20.0]\n    for eps in eps_vals:\n        for M in M_vals:\n            H = np.diag([1.0, eps])\n            v = np.array([1.0, M])\n            S = eps * np.outer(v, v)\n            L = -np.linalg.inv(H) @ S\n            \n            for h in h_vals:\n                for name, params in methods.items():\n                    amp = compute_amplification_factor(params['A'], params['b'], L, H, h)\n                    results[name].append(amp)\n\n    # Family 2: Skew-damped non-normal family\n    alpha_vals = [0.1, 1.0]\n    beta_vals = [5.0, 10.0, 50.0]\n    for alpha in alpha_vals:\n        for beta in beta_vals:\n            H = np.eye(2)\n            J = np.array([[0.0, 1.0], [-1.0, 0.0]])\n            L = -alpha * np.eye(2) + beta * J\n\n            for h in h_vals:\n                for name, params in methods.items():\n                    amp = compute_amplification_factor(params['A'], params['b'], L, H, h)\n                    results[name].append(amp)\n            \n    # Family 3: Similarity-transformed diagonal dissipation\n    c_vals = [5.0, 10.0, 20.0, 50.0]\n    k_pairs = [(1.0, 10.0), (1.0, 50.0)]\n    for c in c_vals:\n        for k1, k2 in k_pairs:\n             C = np.array([[1.0, c], [0.0, 1.0]])\n             H = C.T @ C\n             K = -np.diag([k1, k2])\n             C_inv = np.linalg.inv(C)\n             L = C_inv @ K @ C\n             \n             for h in h_vals:\n                for name, params in methods.items():\n                    amp = compute_amplification_factor(params['A'], params['b'], L, H, h)\n                    results[name].append(amp)\n\n    # --- Aggregation of Results ---\n    radau_amps = np.array(results['RadauIIA'])\n    sdirk_amps = np.array(results['SDIRK'])\n    be_amps = np.array(results['BE'])\n\n    radau_all_contractive = np.all(radau_amps = 1.0 + tau)\n    sdirk_has_violation = np.any(sdirk_amps  1.0 + tau)\n    sdirk_violation_count = int(np.sum(sdirk_amps  1.0 + tau))\n    \n    be_worst = np.max(be_amps)\n    sdirk_worst = np.max(sdirk_amps)\n    radau_worst = np.max(radau_amps)\n    \n    # Ensure worst-case is at least 1.0 if all values are slightly below due to precision\n    radau_worst = np.max([radau_worst, 1.0]) if radau_all_contractive else radau_worst\n\n    final_results = [\n        radau_all_contractive,\n        sdirk_has_violation,\n        be_worst,\n        sdirk_worst,\n        radau_worst,\n        sdirk_violation_count\n    ]\n    \n    # Format the final output string\n    # Booleans are capitalized in python -> str() will be 'True'/'False'\n    # The example shows 'True,True...'. Let's ensure this format\n    output_str = f\"[{final_results[0]},{final_results[1]},{final_results[2]},{final_results[3]},{final_results[4]},{final_results[5]}]\"\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}