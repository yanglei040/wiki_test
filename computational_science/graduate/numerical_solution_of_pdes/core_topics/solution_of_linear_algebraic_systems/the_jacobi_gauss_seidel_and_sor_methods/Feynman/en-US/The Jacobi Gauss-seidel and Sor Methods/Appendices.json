{
    "hands_on_practices": [
        {
            "introduction": "Iterative methods like Jacobi are not only solvers in their own right but are also critical components as \"smoothers\" in more advanced algorithms like multigrid. This exercise provides a hands-on analytical workout, using the tools of Local Fourier Analysis (LFA) to dissect the performance of the weighted Jacobi method. By determining the optimal relaxation parameter that most effectively damps high-frequency error components, you will gain a deeper, quantitative understanding of what makes an effective smoother.",
            "id": "3455516",
            "problem": "Consider the finite-difference discretization of the two-dimensional ($2$-dimensional) Poisson equation $-\\Delta u = f$ on a uniform $N \\times N$ periodic grid with mesh spacing $h$, using the standard $5$-point stencil. The resulting linear operator $A$ is circulant and diagonalized by discrete Fourier modes. Its symbol is given by $a(\\theta_1,\\theta_2) = 4 - 2\\cos\\theta_1 - 2\\cos\\theta_2$, and the diagonal of $A$ is $D = 4$. The weighted Jacobi method with weight $\\omega \\in \\mathbb{R}$ has error-propagation operator $E = I - \\omega D^{-1}A$, whose symbol on a Fourier mode with frequency $(\\theta_1,\\theta_2)$ is $S(\\theta_1,\\theta_2) = 1 - \\omega\\,a(\\theta_1,\\theta_2)/4$.\n\nUsing Local Fourier Analysis (LFA), define the high-frequency set $\\mathcal{H}$ relative to standard $2h$-coarsening as those modes with both components high:\n$$\n\\mathcal{H} = \\left\\{ (\\theta_1,\\theta_2) : |\\theta_1| \\in \\left[\\frac{\\pi}{2},\\pi\\right],\\;|\\theta_2| \\in \\left[\\frac{\\pi}{2},\\pi\\right] \\right\\}.\n$$\nThe weighted Jacobi smoothing factor is the worst-case error-amplification over $\\mathcal{H}$:\n$$\n\\mu(\\omega) = \\max_{(\\theta_1,\\theta_2) \\in \\mathcal{H}} \\left| 1 - \\omega\\,\\frac{a(\\theta_1,\\theta_2)}{4} \\right|.\n$$\n\nStarting from first principles of the symbol calculus for circulant operators and the definition of the weighted Jacobi error-propagation symbol, compute $\\mu(\\omega)$ explicitly as a function of $\\omega$ by characterizing the range of $a(\\theta_1,\\theta_2)/4$ over $\\mathcal{H}$. Then, determine the value of $\\omega$ that minimizes $\\mu(\\omega)$ over $\\omega > 0$. State your final answer as the minimizing weight $\\omega$.",
            "solution": "The objective is to determine the optimal relaxation parameter $\\omega > 0$ for the weighted Jacobi method when used as a smoother for the $2$-dimensional Poisson equation. The optimal parameter, denoted $\\omega_{\\text{opt}}$, is the one that minimizes the smoothing factor $\\mu(\\omega)$.\n\nThe problem provides the necessary definitions from Local Fourier Analysis (LFA). The error-propagation operator for the weighted Jacobi method is $E = I - \\omega D^{-1}A$. Its symbol, which acts as the amplification factor for a Fourier mode with frequency $(\\theta_1, \\theta_2)$, is given by $S(\\theta_1, \\theta_2) = 1 - \\omega\\frac{a(\\theta_1,\\theta_2)}{4}$.\nThe symbol of the discrete operator $A$ is $a(\\theta_1, \\theta_2) = 4 - 2\\cos\\theta_1 - 2\\cos\\theta_2$.\nLet's define the scaled symbol of the operator $A$ as $\\lambda(\\theta_1, \\theta_2) = \\frac{a(\\theta_1, \\theta_2)}{4}$. Substituting the expression for $a(\\theta_1, \\theta_2)$, we get:\n$$\n\\lambda(\\theta_1, \\theta_2) = \\frac{4 - 2\\cos\\theta_1 - 2\\cos\\theta_2}{4} = 1 - \\frac{1}{2}(\\cos\\theta_1 + \\cos\\theta_2)\n$$\nThe error amplification factor can then be written as $S(\\theta_1, \\theta_2) = 1 - \\omega\\lambda(\\theta_1, \\theta_2)$.\n\nThe smoothing factor $\\mu(\\omega)$ is defined as the maximum amplification factor over the set of high frequencies $\\mathcal{H}$:\n$$\n\\mu(\\omega) = \\max_{(\\theta_1, \\theta_2) \\in \\mathcal{H}} |S(\\theta_1, \\theta_2)| = \\max_{(\\theta_1, \\theta_2) \\in \\mathcal{H}} |1 - \\omega\\lambda(\\theta_1, \\theta_2)|\n$$\nThe high-frequency set is given by:\n$$\n\\mathcal{H} = \\left\\{ (\\theta_1,\\theta_2) : |\\theta_1| \\in \\left[\\frac{\\pi}{2},\\pi\\right],\\;|\\theta_2| \\in \\left[\\frac{\\pi}{2},\\pi\\right] \\right\\}\n$$\nTo evaluate $\\mu(\\omega)$, we must first find the range of values that $\\lambda(\\theta_1, \\theta_2)$ takes for all $(\\theta_1, \\theta_2) \\in \\mathcal{H}$. Let this range be denoted by $\\Lambda_{\\mathcal{H}}$.\nThe value of $\\lambda(\\theta_1, \\theta_2)$ depends on the sum $\\cos\\theta_1 + \\cos\\theta_2$. Since $\\cos(\\theta)$ is an even function, the values of $\\lambda(\\theta_1, \\theta_2)$ are identical in the four symmetric regions that constitute $\\mathcal{H}$. We can therefore confine our analysis to the region where $\\theta_1 \\in [\\frac{\\pi}{2}, \\pi]$ and $\\theta_2 \\in [\\frac{\\pi}{2}, \\pi]$.\nIn the interval $[\\frac{\\pi}{2}, \\pi]$, the function $\\cos(\\theta)$ is monotonically decreasing, ranging from $\\cos(\\frac{\\pi}{2}) = 0$ to $\\cos(\\pi) = -1$.\nThe minimum value of the sum $\\cos\\theta_1 + \\cos\\theta_2$ over this region occurs when both $\\theta_1$ and $\\theta_2$ are $\\pi$, yielding $\\cos(\\pi) + \\cos(\\pi) = -1 - 1 = -2$.\nThe maximum value occurs when both $\\theta_1$ and $\\theta_2$ are $\\frac{\\pi}{2}$, yielding $\\cos(\\frac{\\pi}{2}) + \\cos(\\frac{\\pi}{2}) = 0 + 0 = 0$.\nThus, for $(\\theta_1, \\theta_2) \\in \\mathcal{H}$, the range of $\\cos\\theta_1 + \\cos\\theta_2$ is $[-2, 0]$.\nUsing this, we can determine the range of $\\lambda(\\theta_1, \\theta_2) = 1 - \\frac{1}{2}(\\cos\\theta_1 + \\cos\\theta_2)$.\nThe minimum value of $\\lambda$ corresponds to the maximum of $\\cos\\theta_1 + \\cos\\theta_2$:\n$$\n\\lambda_{\\min} = 1 - \\frac{1}{2}(0) = 1\n$$\nThis occurs for modes where $|\\theta_1|=|\\theta_2|=\\frac{\\pi}{2}$.\nThe maximum value of $\\lambda$ corresponds to the minimum of $\\cos\\theta_1 + \\cos\\theta_2$:\n$$\n\\lambda_{\\max} = 1 - \\frac{1}{2}(-2) = 1 + 1 = 2\n$$\nThis occurs for the mode where $|\\theta_1|=|\\theta_2|=\\pi$.\nTherefore, the set of values $\\Lambda_{\\mathcal{H}}$ is the closed interval $[1, 2]$.\n\nThe expression for the smoothing factor becomes a maximization problem over this interval:\n$$\n\\mu(\\omega) = \\max_{\\lambda \\in [1, 2]} |1 - \\omega \\lambda|\n$$\nFor a fixed $\\omega$, the function $f(\\lambda) = 1 - \\omega\\lambda$ is linear in $\\lambda$. The maximum absolute value of a linear function over a closed interval must be achieved at one of the endpoints of the interval. So, we evaluate $|f(\\lambda)|$ at $\\lambda=1$ and $\\lambda=2$:\n$$\n\\mu(\\omega) = \\max\\left\\{|1 - \\omega \\cdot 1|, |1 - \\omega \\cdot 2|\\right\\} = \\max\\left\\{|1 - \\omega|, |1 - 2\\omega|\\right\\}\n$$\nTo find the optimal weight $\\omega_{\\text{opt}}$, we must find the value of $\\omega > 0$ that minimizes $\\mu(\\omega)$. This is a classic minimax problem. The minimum value of $\\max\\{|f_1(\\omega)|, |f_2(\\omega)|\\}$ is typically found at a point where $|f_1(\\omega)| = |f_2(\\omega)|$. We set the arguments of the maximum function to be equal in magnitude:\n$$\n|1 - \\omega| = |1 - 2\\omega|\n$$\nThis equation yields two possibilities:\n1. $1 - \\omega = 1 - 2\\omega$, which simplifies to $\\omega = 0$. This is not a valid solution as the problem specifies $\\omega > 0$.\n2. $1 - \\omega = -(1 - 2\\omega)$, which simplifies to $1 - \\omega = 2\\omega - 1$. Rearranging gives $3\\omega = 2$, so $\\omega = \\frac{2}{3}$.\n\nTo confirm that $\\omega = \\frac{2}{3}$ is indeed the minimizer, we can analyze the behavior of $\\mu(\\omega)$. The function $\\mu(\\omega)$ is composed of piecewise functions. The critical points for $\\omega$ are $0$, $\\frac{1}{2}$, and $1$, where the signs inside the absolute values change.\n- For $\\omega \\in (0, \\frac{2}{3})$, $\\mu(\\omega)$ is a decreasing function of $\\omega$. For instance, in $(0, \\frac{1}{2}]$, $\\mu(\\omega) = 1-\\omega$. In $(\\frac{1}{2}, \\frac{2}{3})$, $\\mu(\\omega)=1-\\omega$ as well.\n- For $\\omega \\in (\\frac{2}{3}, \\infty)$, $\\mu(\\omega)$ is an increasing function of $\\omega$. For instance, in $(\\frac{2}{3}, 1)$, $\\mu(\\omega) = 2\\omega-1$. For $\\omega \\ge 1$, $\\mu(\\omega)=2\\omega-1$ as well.\n\nSince $\\mu(\\omega)$ is decreasing for $\\omega  \\frac{2}{3}$ and increasing for $\\omega > \\frac{2}{3}$, the global minimum for $\\omega>0$ occurs precisely at $\\omega = \\frac{2}{3}$.\nThe minimal smoothing factor is $\\mu(\\frac{2}{3}) = |1 - \\frac{2}{3}| = \\frac{1}{3}$.\nThe problem asks for the value of $\\omega$ that minimizes $\\mu(\\omega)$. This optimal value is $\\frac{2}{3}$.",
            "answer": "$$\n\\boxed{\\frac{2}{3}}\n$$"
        },
        {
            "introduction": "While theoretical analysis provides foundational insights, real-world performance often hinges on tackling complex physics, such as problems with discontinuous or highly anisotropic coefficients. This coding practice explores how the smoothing performance of Gauss-Seidel and SOR can degrade in such challenging scenarios and, crucially, how this can be mitigated by aligning the update ordering with the problem's physical geometry. By implementing and comparing different strategies, you will see firsthand how thoughtful implementation choices are essential for efficiency.",
            "id": "3455542",
            "problem": "Consider the diffusion model with variable and discontinuous conductivity defined by the elliptic partial differential equation $-\\nabla \\cdot (k(x,y)\\nabla u(x,y)) = 0$ on the square domain $\\Omega = (0,1)\\times(0,1)$, subject to homogeneous Dirichlet boundary conditions $u(x,y) = 0$ for $(x,y) \\in \\partial\\Omega$. The conductivity $k(x,y)$ is piecewise constant with a curved discontinuity interface defined by a sinusoidally perturbed circle. Let the interface be given in polar coordinates relative to the center $(x_c,y_c) = (1/2,1/2)$ by $r(\\theta) = r_0 + \\varepsilon \\sin(m\\theta)$, where $\\theta(x,y) = \\mathrm{atan2}(y-y_c, x-x_c)$, $\\rho(x,y) = \\sqrt{(x-x_c)^2 + (y-y_c)^2}$, and $r_0 \\in (0,1/2)$, $\\varepsilon \\ge 0$, $m \\in \\mathbb{N}$. Define the signed distance field $\\varphi(x,y) = \\rho(x,y) - r(\\theta)$, and the conductivity as\n$$\nk(x,y) = \n\\begin{cases}\nk_{\\mathrm{in}},  \\text{if } \\varphi(x,y)  0, \\\\\nk_{\\mathrm{out}},  \\text{if } \\varphi(x,y) \\ge 0,\n\\end{cases}\n$$\nwith constants $k_{\\mathrm{in}}  0$, $k_{\\mathrm{out}}  0$.\n\nDiscretize the domain $\\Omega$ using a uniform cell-centered grid with $N\\times N$ interior unknowns and mesh spacing $h = 1/(N+1)$. For each interior cell $(i,j)$, approximate the operator $-\\nabla \\cdot (k \\nabla u)$ by a five-point finite-volume stencil with harmonic averaging of conductivity on faces to ensure flux continuity across discontinuities. Specifically, define face conductivities\n$$\nk_{i+1/2,j} = \\frac{2\\,k_{i,j}\\,k_{i+1,j}}{k_{i,j} + k_{i+1,j}},\\quad\nk_{i-1/2,j} = \\frac{2\\,k_{i,j}\\,k_{i-1,j}}{k_{i,j} + k_{i-1,j}},\n$$\n$$\nk_{i,j+1/2} = \\frac{2\\,k_{i,j}\\,k_{i,j+1}}{k_{i,j} + k_{i,j+1}},\\quad\nk_{i,j-1/2} = \\frac{2\\,k_{i,j}\\,k_{i,j-1}}{k_{i,j} + k_{i,j-1}},\n$$\nwith the convention that a missing neighbor (adjacent to the boundary) uses the interior value, and the Dirichlet boundary values are zero. The resulting discrete operator has coefficients\n$$\na_{i,j}^{\\mathrm{E}} = -\\frac{k_{i+1/2,j}}{h^2},\\quad\na_{i,j}^{\\mathrm{W}} = -\\frac{k_{i-1/2,j}}{h^2},\\quad\na_{i,j}^{\\mathrm{N}} = -\\frac{k_{i,j+1/2}}{h^2},\\quad\na_{i,j}^{\\mathrm{S}} = -\\frac{k_{i,j-1/2}}{h^2},\n$$\n$$\na_{i,j}^{\\mathrm{diag}} = -\\left(a_{i,j}^{\\mathrm{E}} + a_{i,j}^{\\mathrm{W}} + a_{i,j}^{\\mathrm{N}} + a_{i,j}^{\\mathrm{S}}\\right),\n$$\nso that the discrete operator $A$ acting on the grid function $u$ is\n$$\n(Au)_{i,j} = a_{i,j}^{\\mathrm{diag}}\\,u_{i,j}\n+ a_{i,j}^{\\mathrm{E}}\\,u_{i+1,j}\n+ a_{i,j}^{\\mathrm{W}}\\,u_{i-1,j}\n+ a_{i,j}^{\\mathrm{N}}\\,u_{i,j+1}\n+ a_{i,j}^{\\mathrm{S}}\\,u_{i,j-1}.\n$$\n\nThe classical relaxation methods for linear systems associated with partial differential equations are:\n- Jacobi: Using the diagonal part $D$ of $A$, update\n$$\nu^{(n+1)} = u^{(n)} + \\omega_{\\mathrm{J}} D^{-1}\\left(f - A u^{(n)}\\right),\n$$\nwith a scalar weight $0  \\omega_{\\mathrm{J}} \\le 1$ for stability, and here consider $f=0$.\n- Gauss-Seidel: Split $A = L + D + U$ as strict lower, diagonal, and strict upper parts, and update\n$$\n(D+L)\\,u^{(n+1)} = f - U\\,u^{(n)},\n$$\nwith $f=0$. The ordering of unknowns determines the structure of $L$ and $U$.\n- Successive Over-Relaxation (SOR): Apply relaxation with parameter $\\omega_{\\mathrm{SOR}} \\in (0,2)$ to the Gauss-Seidel update by\n$$\nu^{(n+1)} = u^{(n)} + \\omega_{\\mathrm{SOR}}\\left(u^{(n+1)}_{\\mathrm{GS}} - u^{(n)}\\right),\n$$\nwhere $u^{(n+1)}_{\\mathrm{GS}}$ denotes the Gauss-Seidel update at the current node with the chosen ordering.\n\nDefine two orderings:\n- Natural lexicographic ordering: update $(i,j)$ in increasing row-major order (for example, first $j=0$ to $N-1$, and within each row $i=0$ to $N-1$).\n- Interface-aligned ordering: compute the signed distance field $\\varphi_{i,j}$ at grid points and update nodes sorted by ascending $\\varphi_{i,j}$, i.e., sweeping through layers parallel to the interface.\n\nTo quantify “smoothing of interface-parallel modes,” construct an initial error mode localized near the interface and oscillatory along it:\n$$\ne_0(x,y) = \\exp\\left(-\\left(\\frac{\\varphi(x,y)}{\\sigma}\\right)^2\\right)\\sin\\big(q\\,\\theta(x,y)\\big),\n$$\nwhere $\\sigma > 0$ controls the localization band width, and $q \\in \\mathbb{N}$ is the angular wavenumber chosen as $q = \\min\\{32, \\lfloor N/4 \\rfloor\\}$. Let $\\mathcal{M} = \\{(x,y) \\in \\Omega : |\\varphi(x,y)| \\le 2\\sigma\\}$ be the measurement band near the interface. For a single relaxation sweep producing $e_1$ from $e_0$, define the smoothing factor\n$$\nS = \\frac{\\|e_1\\|_{2,\\mathcal{M}}}{\\|e_0\\|_{2,\\mathcal{M}}},\\quad\n\\|w\\|_{2,\\mathcal{M}} = \\left(\\sum_{(i,j)\\in \\mathcal{M}} w_{i,j}^2\\right)^{1/2},\n$$\nwith the sum taken over cell centers whose coordinates lie in $\\mathcal{M}$.\n\nImplement the discrete operator and the three relaxation methods (Jacobi, Gauss-Seidel, and Successive Over-Relaxation), and compute the smoothing factor $S$ for the five configurations:\n1. Jacobi with weight $\\omega_{\\mathrm{J}} = 0.66$.\n2. Gauss-Seidel with natural lexicographic ordering (no relaxation, i.e., $\\omega_{\\mathrm{SOR}} = 1$).\n3. Gauss-Seidel with interface-aligned ordering (no relaxation).\n4. Successive Over-Relaxation with $\\omega_{\\mathrm{SOR}} = 1.9$ and natural lexicographic ordering.\n5. Successive Over-Relaxation with $\\omega_{\\mathrm{SOR}} = 1.9$ and interface-aligned ordering.\n\nYour program must compute these five smoothing factors for each of the following test cases and return the results:\n\nTest Suite:\n- Case A (happy path): $N=96$, $r_0=0.35$, $\\varepsilon=0.04$, $m=4$, $k_{\\mathrm{in}}=10$, $k_{\\mathrm{out}}=1$, $\\sigma=0.03$, $\\omega_{\\mathrm{J}}=0.66$, $\\omega_{\\mathrm{SOR}}=1.9$.\n- Case B (high contrast): $N=96$, $r_0=0.35$, $\\varepsilon=0.08$, $m=6$, $k_{\\mathrm{in}}=1000$, $k_{\\mathrm{out}}=1$, $\\sigma=0.03$, $\\omega_{\\mathrm{J}}=0.66$, $\\omega_{\\mathrm{SOR}}=1.9$.\n- Case C (boundary and curvature variation): $N=48$, $r_0=0.35$, $\\varepsilon=0.12$, $m=8$, $k_{\\mathrm{in}}=50$, $k_{\\mathrm{out}}=1$, $\\sigma=0.04$, $\\omega_{\\mathrm{J}}=0.66$, $\\omega_{\\mathrm{SOR}}=1.9$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list of lists of floating-point numbers, in the order of cases A, B, C, and within each case in the order of the five configurations specified above. For example, the output should be of the form\n$$\n\\big[\\,[S_{A,\\mathrm{J}}, S_{A,\\mathrm{GSlex}}, S_{A,\\mathrm{GS\\varphi}}, S_{A,\\mathrm{SORlex}}, S_{A,\\mathrm{SOR\\varphi}}],\\ [S_{B,\\mathrm{J}}, \\dots],\\ [S_{C,\\mathrm{J}}, \\dots]\\,\\big]\n$$\nprinted as a single Python list literal on one line.",
            "solution": "We begin from the elliptic diffusion model $-\\nabla \\cdot (k \\nabla u) = 0$ on $\\Omega = (0,1)^2$ with homogeneous Dirichlet boundary conditions. The conductivity $k(x,y)$ is discontinuous along a curved interface $\\Gamma$ defined by the level set $\\varphi(x,y) = 0$, where the signed distance $\\varphi(x,y) = \\rho(x,y) - r(\\theta)$ uses the center $(x_c,y_c)=(1/2,1/2)$, radial coordinate $\\rho(x,y) = \\sqrt{(x-x_c)^2 + (y-y_c)^2}$, angle $\\theta(x,y) = \\mathrm{atan2}(y-y_c, x-x_c)$, and a perturbed radius $r(\\theta) = r_0 + \\varepsilon \\sin(m\\theta)$. The conductivity is $k_{\\mathrm{in}}$ inside ($\\varphi  0$) and $k_{\\mathrm{out}}$ outside ($\\varphi \\ge 0$).\n\nTo discretize, we adopt a cell-centered grid with $N \\times N$ unknowns $(i,j)$ for $i,j \\in \\{0,\\dots,N-1\\}$. The grid spacing is $h=1/(N+1)$, so the location of the cell center $(i,j)$ is $(x_i, y_j) = ((i+1)h, (j+1)h)$. We approximate the divergence operator in flux form. The normal flux across a vertical face between $(i,j)$ and $(i+1,j)$ is approximated by $-k_{i+1/2,j}\\,(u_{i+1,j} - u_{i,j})/h$, with $k_{i+1/2,j}$ a face conductivity. A well-tested approximation for discontinuous coefficients is the harmonic average across the face,\n$$\nk_{i+1/2,j} = \\frac{2\\,k_{i,j}\\,k_{i+1,j}}{k_{i,j} + k_{i+1,j}},\n$$\nwhich enforces continuity of flux and aligns with the physical interface conditions. Similar formulas apply for all faces. With zero Dirichlet boundary conditions, neighboring values outside the interior grid are zero, and we take the face conductivity at boundary-adjacent faces to equal the interior value to maintain consistency (the neighbor contribution vanishes due to zero boundary value).\n\nSumming the fluxes over the four faces of cell $(i,j)$ and dividing by $h$ yields a five-point stencil for the discrete operator $A$:\n$$\n(Au)_{i,j} =\n\\frac{k_{i+1/2,j}}{h^2}\\,(u_{i,j} - u_{i+1,j})\n+ \\frac{k_{i-1/2,j}}{h^2}\\,(u_{i,j} - u_{i-1,j})\n+ \\frac{k_{i,j+1/2}}{h^2}\\,(u_{i,j} - u_{i,j+1})\n+ \\frac{k_{i,j-1/2}}{h^2}\\,(u_{i,j} - u_{i,j-1}).\n$$\nRewriting gives the canonical form\n$$\n(Au)_{i,j} = a_{i,j}^{\\mathrm{diag}}\\,u_{i,j}\n+ a_{i,j}^{\\mathrm{E}}\\,u_{i+1,j}\n+ a_{i,j}^{\\mathrm{W}}\\,u_{i-1,j}\n+ a_{i,j}^{\\mathrm{N}}\\,u_{i,j+1}\n+ a_{i,j}^{\\mathrm{S}}\\,u_{i,j-1},\n$$\nwhere the off-diagonal coefficients are negative,\n$$\na_{i,j}^{\\mathrm{E}} = -\\frac{k_{i+1/2,j}}{h^2},\\quad\na_{i,j}^{\\mathrm{W}} = -\\frac{k_{i-1/2,j}}{h^2},\\quad\na_{i,j}^{\\mathrm{N}} = -\\frac{k_{i,j+1/2}}{h^2},\\quad\na_{i,j}^{\\mathrm{S}} = -\\frac{k_{i,j-1/2}}{h^2},\n$$\nand the diagonal coefficient is the sum of positive face weights\n$$\na_{i,j}^{\\mathrm{diag}} = -\\left(a_{i,j}^{\\mathrm{E}} + a_{i,j}^{\\mathrm{W}} + a_{i,j}^{\\mathrm{N}} + a_{i,j}^{\\mathrm{S}}\\right)\n= \\frac{k_{i+1/2,j} + k_{i-1/2,j} + k_{i,j+1/2} + k_{i,j-1/2}}{h^2}.\n$$\n\nRelaxation methods act on the linear system $A u = f$; here we study smoothing on the error equation with $f=0$ so that the update transforms the error $e^{(n)}$ into $e^{(n+1)}$. The Jacobi method uses only the diagonal $D$ of $A$:\n$$\ne^{(n+1)} = e^{(n)} - \\omega_{\\mathrm{J}}\\,D^{-1} A e^{(n)}.\n$$\nIn component form at $(i,j)$, the Jacobi update can be expressed as\n$$\ne_{i,j}^{(n+1)} = (1-\\omega_{\\mathrm{J}})\\,e_{i,j}^{(n)} + \\omega_{\\mathrm{J}}\\,\\frac{-a_{i,j}^{\\mathrm{E}}\\,e_{i+1,j}^{(n)}\n- a_{i,j}^{\\mathrm{W}}\\,e_{i-1,j}^{(n)}\n- a_{i,j}^{\\mathrm{N}}\\,e_{i,j+1}^{(n)}\n- a_{i,j}^{\\mathrm{S}}\\,e_{i,j-1}^{(n)}}{a_{i,j}^{\\mathrm{diag}}},\n$$\nwhich is derived by rearranging $D^{-1} A e^{(n)}$ and recognizing the five-point stencil structure.\n\nThe Gauss-Seidel and Successive Over-Relaxation methods use the latest available neighbor values in a prescribed ordering. An SOR sweep updates an error vector $e$ according to the formula:\n$$\ne_{i,j}^{\\text{new}} = (1-\\omega_{\\mathrm{SOR}})\\,e_{i,j}^{\\text{old}} + \\omega_{\\mathrm{SOR}}\\,e_{i,j}^{\\text{GS}},\n$$\nwhere $e_{i,j}^{\\text{GS}}$ is the Gauss-Seidel update for point $(i,j)$. This update is computed using the most recently updated values for neighbors that appear earlier in the ordering, and old values for neighbors that appear later. This is implemented by iterating through the grid points according to the specified `order` and updating an error vector `e_new` in-place. The Gauss-Seidel method is simply the case where $\\omega_{\\mathrm{SOR}}=1$.\n\nThe choice of ordering crucially affects Gauss-Seidel and Successive Over-Relaxation because it determines which neighbor values are considered “updated” and, thus, which error components the method damps more aggressively. Natural lexicographic ordering corresponds to scanning the grid row by row, which aligns with Cartesian directions. Interface-aligned ordering sorts nodes by ascending signed distance $\\varphi$, effectively sweeping through layers parallel to the interface. For modes that are oscillatory along the interface and localized near it, aligning the sweep with layers of constant $\\varphi$ preferentially uses freshly updated values in the normal direction to the interface for subsequent nodes, thereby improving damping of interface-parallel error components that would otherwise slip along the discontinuity when the ordering is misaligned.\n\nTo quantify smoothing of interface-parallel modes, we define the initial error\n$$\ne_0(x,y) = \\exp\\left(-\\left(\\frac{\\varphi(x,y)}{\\sigma}\\right)^2\\right)\\sin\\big(q\\,\\theta(x,y)\\big),\n$$\nwith $q$ chosen relative to grid resolution ($q = \\min\\{32, \\lfloor N/4 \\rfloor\\}$) and $\\sigma$ controlling localization. After one relaxation sweep of a given method and ordering producing $e_1$, we compute the smoothing factor\n$$\nS = \\frac{\\|e_1\\|_{2,\\mathcal{M}}}{\\|e_0\\|_{2,\\mathcal{M}}},\n$$\nwhere $\\mathcal{M} = \\{(x,y): |\\varphi(x,y)| \\le 2\\sigma\\}$ isolates a band around the interface. This measure isolates the behavior of the relaxation with respect to interface-parallel oscillations, avoiding the influence of boundary conditions and regions far from the discontinuity.\n\nAlgorithmic steps:\n1. Construct grid coordinates $(x_i,y_j)$ for $i,j=0,\\dots,N-1$ and compute $\\theta_{i,j}$, $\\rho_{i,j}$, $r(\\theta_{i,j})$, and $\\varphi_{i,j}$.\n2. Build $k_{i,j}$ as $k_{\\mathrm{in}}$ or $k_{\\mathrm{out}}$ according to the sign of $\\varphi_{i,j}$.\n3. Compute face conductivities by harmonic averaging for all interior faces; for faces at the boundary, use the interior conductivity.\n4. Assemble the five-point stencil coefficients $a_{i,j}^{\\mathrm{E}}$, $a_{i,j}^{\\mathrm{W}}$, $a_{i,j}^{\\mathrm{N}}$, $a_{i,j}^{\\mathrm{S}}$, and $a_{i,j}^{\\mathrm{diag}}$.\n5. Construct the initial error field $e_0$ using $\\sigma$ and $q$, and determine the measurement mask $\\mathcal{M}$ where $|\\varphi| \\le 2\\sigma$.\n6. Perform one relaxation sweep:\n   - For Jacobi, compute the update everywhere using only old neighbor values and weight $\\omega_{\\mathrm{J}}$.\n   - For Gauss-Seidel and Successive Over-Relaxation, traverse nodes in natural lexicographic order or sorted by $\\varphi$; at each node, form the Gauss-Seidel raw update using the latest available neighbor values, then apply relaxation with $\\omega_{\\mathrm{SOR}}$ (equal to $1$ for Gauss-Seidel).\n7. Compute smoothing factors $S$ as the ratio of discrete $\\ell^2$ norms over the mask $\\mathcal{M}$.\n8. Repeat for all test cases.\n\nThe program implements all steps and outputs, for each test case, five floating-point smoothing factors corresponding to Jacobi, Gauss-Seidel with lexicographic ordering, Gauss-Seidel with interface-aligned ordering, Successive Over-Relaxation with lexicographic ordering, and Successive Over-Relaxation with interface-aligned ordering, respectively, aggregated in a single list of lists printed on one line as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef harmonic(a, b):\n    s = a + b\n    if s = 0:\n        return 0.0\n    return 2.0 * a * b / s\n\ndef build_geometry(N, r0, eps, m):\n    h = 1.0 / (N + 1)\n    i = np.arange(N)\n    j = np.arange(N)\n    x = (i + 1) * h\n    y = (j + 1) * h\n    X, Y = np.meshgrid(x, y, indexing='ij')\n    xc, yc = 0.5, 0.5\n    theta = np.arctan2(Y - yc, X - xc)\n    rho = np.sqrt((X - xc) ** 2 + (Y - yc) ** 2)\n    r_theta = r0 + eps * np.sin(m * theta)\n    phi = rho - r_theta\n    return X, Y, theta, rho, r_theta, phi, h\n\ndef build_conductivity(phi, kin, kout):\n    k = np.where(phi  0.0, kin, kout)\n    return k\n\ndef assemble_stencil(k, h):\n    N = k.shape[0]\n    # Face conductivities with harmonic averaging; handle boundaries\n    kE = np.empty_like(k)\n    kW = np.empty_like(k)\n    kN = np.empty_like(k)\n    kS = np.empty_like(k)\n\n    # East faces\n    kE[:-1, :] = harmonic_array(k[:-1, :], k[1:, :])\n    kE[-1, :] = k[-1, :]  # boundary face uses interior value\n\n    # West faces\n    kW[1:, :] = harmonic_array(k[1:, :], k[:-1, :])\n    kW[0, :] = k[0, :]\n\n    # North faces\n    kN[:, :-1] = harmonic_array(k[:, :-1], k[:, 1:])\n    kN[:, -1] = k[:, -1]\n\n    # South faces\n    kS[:, 1:] = harmonic_array(k[:, 1:], k[:, :-1])\n    kS[:, 0] = k[:, 0]\n\n    aE = -kE / (h ** 2)\n    aW = -kW / (h ** 2)\n    aN = -kN / (h ** 2)\n    aS = -kS / (h ** 2)\n    aD = -(aE + aW + aN + aS)\n    return aD, aE, aW, aN, aS\n\ndef harmonic_array(a, b):\n    s = a + b\n    out = np.zeros_like(s)\n    mask = s > 0\n    out[mask] = 2.0 * a[mask] * b[mask] / s[mask]\n    return out\n\ndef initial_error(theta, phi, sigma, N):\n    q = int(min(32, N // 4))\n    e0 = np.exp(-(phi / sigma) ** 2) * np.sin(q * theta)\n    return e0\n\ndef mask_band(phi, sigma):\n    return np.abs(phi) = (2.0 * sigma)\n\ndef jacobi_sweep(u_old, aD, aE, aW, aN, aS, omegaJ):\n    N = u_old.shape[0]\n    uR = np.zeros_like(u_old)\n    uL = np.zeros_like(u_old)\n    uU = np.zeros_like(u_old)\n    uD = np.zeros_like(u_old)\n    uR[:-1, :] = u_old[1:, :]\n    uL[1:, :] = u_old[:-1, :]\n    uU[:, :-1] = u_old[:, 1:]\n    uD[:, 1:] = u_old[:, :-1]\n    s = aE * uR + aW * uL + aN * uU + aS * uD\n    raw_update = -s / aD\n    u_new = (1.0 - omegaJ) * u_old + omegaJ * raw_update\n    return u_new\n\ndef build_order_indices(N, phi, mode='lex'):\n    if mode == 'lex':\n        # row-major: j outer, i inner\n        order = [(i, j) for j in range(N) for i in range(N)]\n    elif mode == 'phi':\n        # sort by ascending phi\n        flat_indices = [(i, j) for i in range(N) for j in range(N)]\n        flat_phi = [phi[i, j] for i, j in flat_indices]\n        order = [x for _, x in sorted(zip(flat_phi, flat_indices), key=lambda t: t[0])]\n    else:\n        raise ValueError(\"Unknown ordering mode\")\n    return order\n\ndef gs_sweep(u_old, aD, aE, aW, aN, aS, order, omega=1.0):\n    u_new = u_old.copy() # This array will be updated in-place\n    N = u_old.shape[0]\n    for i, j in order:\n        # Sum over neighbors using the most up-to-date values from u_new\n        s_neighbors = 0.0\n        if i + 1  N: s_neighbors += aE[i, j] * u_new[i + 1, j]\n        if i - 1 >= 0: s_neighbors += aW[i, j] * u_new[i - 1, j]\n        if j + 1  N: s_neighbors += aN[i, j] * u_new[i, j + 1]\n        if j - 1 >= 0: s_neighbors += aS[i, j] * u_new[i, j - 1]\n        \n        # Calculate the raw Gauss-Seidel update for the error equation (f=0)\n        raw_gs_update = -s_neighbors / aD[i, j]\n        \n        # Apply SOR formula using the value from before the sweep (u_old)\n        u_new[i, j] = (1.0 - omega) * u_old[i, j] + omega * raw_gs_update\n    return u_new\n\ndef smoothing_factor(e0, e1, mask):\n    sel0 = e0[mask]\n    sel1 = e1[mask]\n    norm0 = np.sqrt(np.sum(sel0 * sel0))\n    norm1 = np.sqrt(np.sum(sel1 * sel1))\n    if norm0 == 0.0:\n        return 0.0\n    return float(norm1 / norm0)\n\ndef run_case(N, r0, eps, m, kin, kout, sigma, omegaJ, omegaSOR):\n    X, Y, theta, rho, r_theta, phi, h = build_geometry(N, r0, eps, m)\n    k = build_conductivity(phi, kin, kout)\n    aD, aE, aW, aN, aS = assemble_stencil(k, h)\n    e0 = initial_error(theta, phi, sigma, N)\n    mask = mask_band(phi, sigma)\n\n    e1_jacobi = jacobi_sweep(e0, aD, aE, aW, aN, aS, omegaJ)\n    S_jacobi = smoothing_factor(e0, e1_jacobi, mask)\n\n    order_lex = build_order_indices(N, phi, mode='lex')\n    e1_gs_lex = gs_sweep(e0, aD, aE, aW, aN, aS, order_lex, omega=1.0)\n    S_gs_lex = smoothing_factor(e0, e1_gs_lex, mask)\n\n    order_phi = build_order_indices(N, phi, mode='phi')\n    e1_gs_phi = gs_sweep(e0, aD, aE, aW, aN, aS, order_phi, omega=1.0)\n    S_gs_phi = smoothing_factor(e0, e1_gs_phi, mask)\n\n    e1_sor_lex = gs_sweep(e0, aD, aE, aW, aN, aS, order_lex, omega=omegaSOR)\n    S_sor_lex = smoothing_factor(e0, e1_sor_lex, mask)\n\n    e1_sor_phi = gs_sweep(e0, aD, aE, aW, aN, aS, order_phi, omega=omegaSOR)\n    S_sor_phi = smoothing_factor(e0, e1_sor_phi, mask)\n\n    return [S_jacobi, S_gs_lex, S_gs_phi, S_sor_lex, S_sor_phi]\n\ndef solve():\n    test_cases = [\n        {\"N\": 96, \"r0\": 0.35, \"eps\": 0.04, \"m\": 4, \"kin\": 10.0, \"kout\": 1.0, \"sigma\": 0.03, \"omegaJ\": 0.66, \"omegaSOR\": 1.9},\n        {\"N\": 96, \"r0\": 0.35, \"eps\": 0.08, \"m\": 6, \"kin\": 1000.0, \"kout\": 1.0, \"sigma\": 0.03, \"omegaJ\": 0.66, \"omegaSOR\": 1.9},\n        {\"N\": 48, \"r0\": 0.35, \"eps\": 0.12, \"m\": 8, \"kin\": 50.0, \"kout\": 1.0, \"sigma\": 0.04, \"omegaJ\": 0.66, \"omegaSOR\": 1.9},\n    ]\n\n    results = []\n    for case in test_cases:\n        res = run_case(**case)\n        results.append([float(f\"{x:.6f}\") for x in res])\n\n    print(f\"[{','.join(str(r) for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A key practical challenge with the SOR method is choosing the optimal relaxation parameter, $\\omega$, which depends on the spectral properties of the system matrix. This advanced implementation exercise guides you through creating an adaptive SOR method that dynamically estimates a near-optimal $\\omega$ at each iteration by extracting information from the current residual. Building this self-tuning solver will give you hands-on experience with a powerful technique for creating more robust and intelligent iterative methods.",
            "id": "3455492",
            "problem": "Consider the linear second-order elliptic partial differential equation (PDE)\n$$-\\nabla \\cdot \\left(\\mathbf{K}(x,y)\\,\\nabla u(x,y)\\right) = f(x,y) \\quad \\text{for } (x,y)\\in(0,1)\\times(0,1),$$\nwith homogeneous Dirichlet boundary conditions\n$$u(x,y)=0 \\quad \\text{for } (x,y)\\in\\partial\\left((0,1)\\times(0,1)\\right),$$\nwhere the symmetric, positive-definite diffusion tensor is\n$$\\mathbf{K}(x,y) = \\begin{pmatrix} k_x(x,y)  0 \\\\ 0  k_y(x,y) \\end{pmatrix}.$$\nDiscretize this PDE using a standard finite difference scheme on a uniform grid with $n\\times n$ interior points, grid spacing $h=1/(n+1)$ in both directions, and face-centered coefficients for the fluxes (arithmetic averaging at cell faces). Let the discrete linear system be\n$$A u = b,$$\nwhere $A\\in\\mathbb{R}^{N\\times N}$, $u\\in\\mathbb{R}^N$, $b\\in\\mathbb{R}^N$, and $N=n^2$. Denote by $D=\\mathrm{diag}(A)$ the diagonal of $A$, and write the standard matrix-splitting $A = D - L - U$ with strictly lower-triangular part $L$ and strictly upper-triangular part $U$.\n\nImplement the following three stationary iterative solvers for $A u = b$:\n- The Jacobi method, defined by $$u^{(k+1)} = D^{-1}(L+U)u^{(k)} + D^{-1}b.$$\n- The Gauss–Seidel method, defined lexicographically by forward substitution using the splitting $A = (D-L) - U$.\n- The Successive Over-Relaxation (SOR) method, defined by $$u^{(k+1)} = (D - \\omega L)^{-1}\\left[\\omega b - \\left(\\omega U + (\\omega - 1) D\\right)u^{(k)}\\right],$$ where the relaxation parameter $\\omega\\in(0,2)$ is chosen adaptively at each iteration using a local Rayleigh quotient estimate constructed from the current residual.\n\nAt iteration $k$, define the residual $$r^{(k)} = b - A u^{(k)}.$$ For the adaptive SOR method, compute the local Rayleigh quotient\n$$\\rho_{\\text{loc}}^{(k)} = \\frac{\\left(r^{(k)}\\right)^{\\top} A\\, r^{(k)}}{\\left(r^{(k)}\\right)^{\\top} D\\, r^{(k)}}$$\nand select the relaxation parameter $\\omega^{(k)}$ through a principled mapping from $\\rho_{\\text{loc}}^{(k)}$ to a relaxation that improves convergence, grounded in the relationship between the Jacobi iteration and SOR. The mapping must be derived from first principles in your solution, and the resulting $\\omega^{(k)}$ must satisfy $0  \\omega^{(k)}  2$ with appropriate safeguards for edge cases such as vanishing residuals.\n\nDiscretization details. For interior indices $(i,j)$ with $i=1,\\dots,n$ and $j=1,\\dots,n$, let $x_i = i h$ and $y_j = j h$, and set\n$$k_x^{i,j} = k_x(x_i,y_j),\\qquad k_y^{i,j} = k_y(x_i,y_j).$$\nDefine face-centered diffusion coefficients by arithmetic averages,\n$$k_{x}^{i+\\frac{1}{2},j} = \\begin{cases}\n\\frac{1}{2}\\left(k_x^{i,j} + k_x^{i+1,j}\\right),  i  n,\\\\\nk_x^{i,j},  i=n,\n\\end{cases}\n\\quad\nk_{x}^{i-\\frac{1}{2},j} = \\begin{cases}\n\\frac{1}{2}\\left(k_x^{i,j} + k_x^{i-1,j}\\right),  i  1,\\\\\nk_x^{i,j},  i=1,\n\\end{cases}$$\n$$k_{y}^{i,j+\\frac{1}{2}} = \\begin{cases}\n\\frac{1}{2}\\left(k_y^{i,j} + k_y^{i,j+1}\\right),  j  n,\\\\\nk_y^{i,j},  j=n,\n\\end{cases}\n\\quad\nk_{y}^{i,j-\\frac{1}{2}} = \\begin{cases}\n\\frac{1}{2}\\left(k_y^{i,j} + k_y^{i,j-1}\\right),  j  1,\\\\\nk_y^{i,j},  j=1.\n\\end{cases}$$\nThen the discrete operator at $(i,j)$ is\n$$\n(Au)^{i,j} = \\left(c_x^{+\\,i,j} + c_x^{-\\,i,j} + c_y^{+\\,i,j} + c_y^{-\\,i,j}\\right) u^{i,j}\n- c_x^{+\\,i,j}\\, u^{i+1,j} - c_x^{-\\,i,j}\\, u^{i-1,j}\n- c_y^{+\\,i,j}\\, u^{i,j+1} - c_y^{-\\,i,j}\\, u^{i,j-1},\n$$\nwhere\n$$c_x^{+\\,i,j} = \\frac{k_{x}^{i+\\frac{1}{2},j}}{h^2},\\quad c_x^{-\\,i,j} = \\frac{k_{x}^{i-\\frac{1}{2},j}}{h^2},\\quad c_y^{+\\,i,j} = \\frac{k_{y}^{i,j+\\frac{1}{2}}}{h^2},\\quad c_y^{-\\,i,j} = \\frac{k_{y}^{i,j-\\frac{1}{2}}}{h^2},$$\nand boundary values outside the interior are set to zero (homogeneous Dirichlet). The diagonal entries are\n$$D^{i,j} = c_x^{+\\,i,j} + c_x^{-\\,i,j} + c_y^{+\\,i,j} + c_y^{-\\,i,j}.$$\n\nUse the source term $f(x,y) = 1$ so that $b^{i,j} = f(x_i,y_j) = 1$.\n\nStopping criterion and initialization. Initialize with $u^{(0)} = 0$. At iteration $k$, compute the residual norm\n$$\\|r^{(k)}\\|_2 = \\left(\\sum_{i=1}^n\\sum_{j=1}^n \\left(\\left(r^{(k)}\\right)^{i,j}\\right)^2\\right)^{1/2}.$$\nStop when $\\|r^{(k)}\\|_2 \\le \\varepsilon,$ with tolerance $\\varepsilon = 10^{-6},$ or when the iteration count reaches $k_{\\max} = 5000$. All quantities are dimensionless.\n\nTest suite. Use grid size $n = 20$. Define three test cases to assess robustness under anisotropy and heterogeneity:\n- Test $1$ (anisotropic, $x$-dominant): $k_x(x,y) \\equiv 10, k_y(x,y) \\equiv 1.$\n- Test $2$ (anisotropic, $y$-dominant): $k_x(x,y) \\equiv 1, k_y(x,y) \\equiv 10.$\n- Test $3$ (heterogeneous, piecewise anisotropy): $k_x(x,y) = 1 + 9\\,\\mathbf{1}_{\\{x > 0.5\\}}, k_y(x,y) = 1 + 9\\,\\mathbf{1}_{\\{y > 0.5\\}},$ where $\\mathbf{1}_{\\{\\cdot\\}}$ is the indicator function.\n\nFor each test case, run the Jacobi method, the Gauss–Seidel method, and the adaptive SOR method that selects $\\omega$ from $\\rho_{\\text{loc}}$ at each iteration. Record the number of iterations required to meet the stopping criterion for each method. If the stopping criterion is not met within $k_{\\max}$ iterations, record $k_{\\max}$. The required final output format is a single line containing a comma-separated list of nine integers enclosed in square brackets in the order\n$$[\\text{Jacobi}_1,\\text{GaussSeidel}_1,\\text{SOR}_1,\\text{Jacobi}_2,\\text{GaussSeidel}_2,\\text{SOR}_2,\\text{Jacobi}_3,\\text{GaussSeidel}_3,\\text{SOR}_3].$$",
            "solution": "This problem requires the implementation of three iterative methods—Jacobi, Gauss-Seidel, and an adaptive Successive Over-Relaxation (SOR)—to solve a linear system $Au=b$ arising from a finite difference discretization of an elliptic PDE. The key tasks are to correctly implement the discretization, the three solvers, and particularly the adaptive scheme for choosing the SOR parameter $\\omega$.\n\n### I. Discretization and Linear System\nThe PDE $-\\nabla \\cdot (\\mathbf{K} \\nabla u) = f$ is discretized on an $n \\times n$ grid. Using central differences for the divergence and gradient operators and the specified arithmetic averaging for coefficients leads to a five-point stencil at each interior grid point $(i,j)$. The resulting linear equation for the unknown $u^{i,j}$ relates its value to its four neighbors. The collection of these $N=n^2$ equations forms the matrix system $Au=b$. The matrix $A$ is symmetric and, due to the positive diffusion coefficients, positive-definite (SPD).\n\n### II. Iterative Solvers\nAll three methods are stationary iterative solvers that refine an initial guess $u^{(0)}$.\n1.  **Jacobi Method**: The update for each component of the solution vector $u^{(k+1)}$ depends only on values from the previous iteration $u^{(k)}$. The update rule can be written compactly using the residual $r^{(k)} = b - Au^{(k)}$ as:\n    $$u^{(k+1)} = u^{(k)} + D^{-1}r^{(k)}$$\n    where $D$ is the diagonal of $A$. This is computationally simple and parallelizable.\n\n2.  **Gauss-Seidel Method**: This method improves on Jacobi by using the most recently computed values within the same iteration. When solving for $u_{i,j}^{(k+1)}$, it uses the values $u_{i-1,j}^{(k+1)}$ and $u_{i,j-1}^{(k+1)}$ which have already been updated in the current sweep (assuming lexicographical ordering). This in-place update corresponds to the matrix splitting $M=D-L$, and for SPD matrices, it typically converges faster than Jacobi.\n\n3.  **Adaptive SOR Method**: SOR accelerates Gauss-Seidel by introducing a relaxation parameter $\\omega$. The update is a weighted average of the previous iterate and the Gauss-Seidel update:\n    $$u^{(k+1)} = (1-\\omega)u^{(k)} + \\omega u_{GS}^{(k+1)}$$\n    For an SPD matrix, the method converges for any $\\omega \\in (0, 2)$. The rate of convergence is highly sensitive to the choice of $\\omega$. The optimal parameter, $\\omega_{opt}$, is related to the spectral radius $\\rho(T_J)$ of the Jacobi iteration matrix $T_J = I - D^{-1}A$. For a broad class of matrices arising from PDEs, the relationship is:\n    $$\\omega_{opt} = \\frac{2}{1 + \\sqrt{1 - \\rho(T_J)^2}}$$\n    The challenge is that $\\rho(T_J)$ is usually unknown. The adaptive strategy aims to estimate it at each step. The eigenvalues $\\mu$ of $T_J$ are related to the eigenvalues $\\lambda$ of the matrix $D^{-1}A$ by $\\mu = 1-\\lambda$. Thus, estimating $\\rho(T_J) = \\max_i|1-\\lambda_i|$ is equivalent to estimating the eigenvalues of $D^{-1}A$.\n\n    The problem suggests using the local Rayleigh quotient of the residual vector $r^{(k)}$:\n    $$\\rho_{\\text{loc}}^{(k)} = \\frac{(r^{(k)})^T A r^{(k)}}{(r^{(k)})^T D r^{(k)}}$$\n    This can be viewed as a Rayleigh quotient for the matrix $D^{-1/2} A D^{-1/2}$ with the vector $D^{1/2}r^{(k)}$. Since $D^{-1/2} A D^{-1/2}$ is similar to $D^{-1}A$, they share the same eigenvalues. Thus, $\\rho_{\\text{loc}}^{(k)}$ provides an estimate of some eigenvalue $\\lambda$ of $D^{-1}A$. As the iteration proceeds, the residual $r^{(k)}$ often becomes dominated by the eigenvector corresponding to the eigenvalue that most limits convergence. We can use $\\rho_{\\text{loc}}^{(k)}$ as an approximation for an important eigenvalue, and from that, estimate the Jacobi spectral radius as $\\hat{\\rho}_k = |1 - \\rho_{\\text{loc}}^{(k)}|$.\n\n    Substituting this estimate into the formula for $\\omega_{opt}$ gives our adaptive parameter for iteration $k$:\n    $$\\omega^{(k)} = \\frac{2}{1 + \\sqrt{1 - \\hat{\\rho}_k^2}} = \\frac{2}{1 + \\sqrt{1 - (1-\\rho_{\\text{loc}}^{(k)})^2}} = \\frac{2}{1 + \\sqrt{2\\rho_{\\text{loc}}^{(k)} - (\\rho_{\\text{loc}}^{(k)})^2}}$$\n    For an SPD matrix $A$, the eigenvalues $\\lambda$ of $D^{-1}A$ are in $(0, 2)$, which ensures the term under the square root is non-negative. Safeguards should be implemented to handle cases where $\\rho_{\\text{loc}}^{(k)}$ is close to 0 or 2, and to handle vanishing residuals.\n\n### III. Implementation Details\nThe Python code will implement these steps.\n- A function will set up the stencil coefficients for a given test case.\n- A `matvec` function will compute the matrix-vector product $Au$ efficiently using the stencil, avoiding explicit matrix storage.\n- Separate solver functions for Jacobi, Gauss-Seidel, and adaptive SOR will be created. They will loop until the $\\ell_2$-norm of the residual is below the tolerance $\\varepsilon$ or $k_{\\max}$ is reached.\n- The adaptive SOR solver will calculate $\\omega^{(k)}$ at each step using the derived formula before performing the update sweep.\n- The main script will iterate through the three test cases, call each solver, and store the number of iterations. The final result is printed as a list of nine integers.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here\n# Imports must adhere to the specified execution environment\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the specified PDE using Jacobi, Gauss-Seidel, and adaptive SOR methods\n    for three different test cases and reports the number of iterations required for convergence.\n    \"\"\"\n    n = 20\n    epsilon = 1e-6\n    k_max = 5000\n\n    def kx1(x, y): return 10.0\n    def ky1(x, y): return 1.0\n\n    def kx2(x, y): return 1.0\n    def ky2(x, y): return 10.0\n\n    def kx3(x, y): return 10.0 if x > 0.5 else 1.0\n    def ky3(x, y): return 10.0 if y > 0.5 else 1.0\n\n    test_cases = [\n        (kx1, ky1),\n        (kx2, ky2),\n        (kx3, ky3),\n    ]\n\n    def setup_coefficients(n, kx_func, ky_func):\n        h = 1.0 / (n + 1)\n        h2 = h * h\n\n        kx_grid = np.zeros((n + 2, n + 2))\n        ky_grid = np.zeros((n + 2, n + 2))\n        for i in range(n + 2):\n            for j in range(n + 2):\n                kx_grid[i, j] = kx_func(i * h, j * h)\n                ky_grid[i, j] = ky_func(i * h, j * h)\n\n        cx_p = np.zeros((n, n)); cx_m = np.zeros((n, n))\n        cy_p = np.zeros((n, n)); cy_m = np.zeros((n, n))\n\n        for i in range(n):\n            for j in range(n):\n                i_grid, j_grid = i + 1, j + 1\n                \n                k_xp = 0.5 * (kx_grid[i_grid, j_grid] + kx_grid[i_grid + 1, j_grid]) if i  n - 1 else kx_grid[i_grid, j_grid]\n                k_xm = 0.5 * (kx_grid[i_grid, j_grid] + kx_grid[i_grid - 1, j_grid]) if i > 0 else kx_grid[i_grid, j_grid]\n                k_yp = 0.5 * (ky_grid[i_grid, j_grid] + ky_grid[i_grid, j_grid + 1]) if j  n - 1 else ky_grid[i_grid, j_grid]\n                k_ym = 0.5 * (ky_grid[i_grid, j_grid] + ky_grid[i_grid, j_grid - 1]) if j > 0 else ky_grid[i_grid, j_grid]\n\n                cx_p[i, j] = k_xp / h2\n                cx_m[i, j] = k_xm / h2\n                cy_p[i, j] = k_yp / h2\n                cy_m[i, j] = k_ym / h2\n\n        D = cx_p + cx_m + cy_p + cy_m\n        return {'cx_p': cx_p, 'cx_m': cx_m, 'cy_p': cy_p, 'cy_m': cy_m, 'D': D}\n\n    def matvec(u, n, coeffs):\n        u_padded = np.pad(u, 1, 'constant')\n        u_ip1, u_im1 = u_padded[2:, 1:-1], u_padded[:-2, 1:-1]\n        u_jp1, u_jm1 = u_padded[1:-1, 2:], u_padded[1:-1, :-2]\n        \n        Au = (coeffs['D'] * u -\n              (coeffs['cx_p'] * u_ip1 + coeffs['cx_m'] * u_im1 +\n               coeffs['cy_p'] * u_jp1 + coeffs['cy_m'] * u_jm1))\n        return Au\n\n    def jacobi_solver(n, b, coeffs, k_max, epsilon):\n        u = np.zeros((n, n))\n        for k in range(1, k_max + 1):\n            r = b - matvec(u, n, coeffs)\n            if np.linalg.norm(r) = epsilon:\n                return k\n            u = u + r / coeffs['D']\n        return k_max\n\n    def gauss_seidel_solver(n, b, coeffs, k_max, epsilon):\n        u = np.zeros((n, n))\n        D = coeffs['D']\n        cx_p, cx_m, cy_p, cy_m = coeffs['cx_p'], coeffs['cx_m'], coeffs['cy_p'], coeffs['cy_m']\n\n        for k in range(1, k_max + 1):\n            r_check = b - matvec(u, n, coeffs)\n            if np.linalg.norm(r_check) = epsilon:\n                return k\n            \n            for i in range(n):\n                for j in range(n):\n                    neighbors_sum = 0\n                    if i  n - 1: neighbors_sum += cx_p[i, j] * u[i + 1, j]\n                    if i > 0:     neighbors_sum += cx_m[i, j] * u[i - 1, j]\n                    if j  n - 1: neighbors_sum += cy_p[i, j] * u[i, j + 1]\n                    if j > 0:     neighbors_sum += cy_m[i, j] * u[i, j - 1]\n                    u[i, j] = (b[i, j] + neighbors_sum) / D[i, j]\n        return k_max\n\n    def sor_solver(n, b, coeffs, k_max, epsilon):\n        u = np.zeros((n, n))\n        D = coeffs['D']\n        cx_p, cx_m, cy_p, cy_m = coeffs['cx_p'], coeffs['cx_m'], coeffs['cy_p'], coeffs['cy_m']\n\n        for k in range(1, k_max + 1):\n            r = b - matvec(u, n, coeffs)\n            if np.linalg.norm(r) = epsilon:\n                return k\n\n            # Compute adaptive omega\n            Ar = matvec(r, n, coeffs)\n            r_Dr = np.sum(r * D * r)\n            r_Ar = np.sum(r * Ar)\n\n            if abs(r_Dr)  1e-14:\n                omega = 1.0\n            else:\n                rho_loc = r_Ar / r_Dr\n                rho_loc = max(1e-12, min(rho_loc, 2.0 - 1e-12))\n                omega = 2.0 / (1.0 + np.sqrt(rho_loc * (2.0 - rho_loc)))\n            \n            u_old_iter = u.copy()\n            for i in range(n):\n                for j in range(n):\n                    neighbors_sum = 0\n                    if i  n - 1: neighbors_sum += cx_p[i, j] * u[i + 1, j]\n                    if i > 0:     neighbors_sum += cx_m[i, j] * u[i - 1, j]\n                    if j  n - 1: neighbors_sum += cy_p[i, j] * u[i, j + 1]\n                    if j > 0:     neighbors_sum += cy_m[i, j] * u[i, j - 1]\n                    \n                    u_gs = (b[i, j] + neighbors_sum) / D[i, j]\n                    u[i, j] = (1.0 - omega) * u_old_iter[i, j] + omega * u_gs\n        return k_max\n\n    results = []\n    b = np.ones((n, n))\n    \n    for kx_func, ky_func in test_cases:\n        coeffs = setup_coefficients(n, kx_func, ky_func)\n        \n        iter_jacobi = jacobi_solver(n, b, coeffs, k_max, epsilon)\n        iter_gs = gauss_seidel_solver(n, b, coeffs, k_max, epsilon)\n        iter_sor = sor_solver(n, b, coeffs, k_max, epsilon)\n        \n        results.extend([iter_jacobi, iter_gs, iter_sor])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}