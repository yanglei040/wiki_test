{
    "hands_on_practices": [
        {
            "introduction": "迭代法不仅是求解线性方程组的工具，更是多重网格等高级算法中充当“光滑算子”的关键组成部分。本练习将通过局部傅里叶分析（Local Fourier Analysis, LFA）深入探讨这一角色，分析加权雅可比法在二维泊松方程上的应用。你将推导出光滑因子作为松弛权重 $\\omega$ 的函数，并找出能最有效地衰减高频误差分量的最优值，从而为理解“光滑”特性奠定坚实的理论基础 。",
            "id": "3455516",
            "problem": "考虑在网格间距为 $h$ 的均匀 $N \\times N$ 周期性网格上，使用标准5点模板对二维 ($2$-维) 泊松方程 $-\\Delta u = f$ 进行有限差分离散化。得到的线性算子 $A$ 是一个循环算子，可被离散傅里叶模式对角化。其符号由 $a(\\theta_1,\\theta_2) = 4 - 2\\cos\\theta_1 - 2\\cos\\theta_2$ 给出，且 $A$ 的对角部分为 $D = 4$。权重为 $\\omega \\in \\mathbb{R}$ 的加权雅可比法具有误差传播算子 $E = I - \\omega D^{-1}A$，其作用于频率为 $(\\theta_1,\\theta_2)$ 的傅里叶模式上的符号为 $S(\\theta_1,\\theta_2) = 1 - \\omega\\,a(\\theta_1,\\theta_2)/4$。\n\n使用局部傅里叶分析 (LFA)，将相对于标准 $2h$-粗化的高频集 $\\mathcal{H}$ 定义为两个分量均为高频的模式：\n$$\n\\mathcal{H} = \\left\\{ (\\theta_1,\\theta_2) : |\\theta_1| \\in \\left[\\frac{\\pi}{2},\\pi\\right],\\;|\\theta_2| \\in \\left[\\frac{\\pi}{2},\\pi\\right] \\right\\}.\n$$\n加权雅可比光滑因子是在 $\\mathcal{H}$ 上的最坏情况误差放大率：\n$$\n\\mu(\\omega) = \\max_{(\\theta_1,\\theta_2) \\in \\mathcal{H}} \\left| 1 - \\omega\\,\\frac{a(\\theta_1,\\theta_2)}{4} \\right|.\n$$\n\n从循环算子的符号演算的基本原理和加权雅可比误差传播符号的定义出发，通过刻画 $a(\\theta_1,\\theta_2)/4$ 在 $\\mathcal{H}$ 上的值域，将 $\\mu(\\omega)$ 显式地计算为 $\\omega$ 的函数。然后，确定在 $\\omega > 0$ 的范围内使 $\\mu(\\omega)$ 最小化的 $\\omega$ 值。将你的最终答案表示为最小化权重 $\\omega$。",
            "solution": "目标是确定当加权雅可比法用作二维泊松方程的光滑子时，最优松弛参数 $\\omega > 0$。最优参数记为 $\\omega_{\\text{opt}}$，它是使光滑因子 $\\mu(\\omega)$ 最小化的参数。\n\n问题提供了来自局部傅里叶分析 (LFA) 的必要定义。加权雅可比法的误差传播算子是 $E = I - \\omega D^{-1}A$。其符号作为频率为 $(\\theta_1, \\theta_2)$ 的傅里叶模式的放大因子，由 $S(\\theta_1, \\theta_2) = 1 - \\omega\\frac{a(\\theta_1,\\theta_2)}{4}$ 给出。\n离散算子 $A$ 的符号是 $a(\\theta_1, \\theta_2) = 4 - 2\\cos\\theta_1 - 2\\cos\\theta_2$。\n我们将算子 $A$ 的缩放符号定义为 $\\lambda(\\theta_1, \\theta_2) = \\frac{a(\\theta_1, \\theta_2)}{4}$。代入 $a(\\theta_1, \\theta_2)$ 的表达式，我们得到：\n$$\n\\lambda(\\theta_1, \\theta_2) = \\frac{4 - 2\\cos\\theta_1 - 2\\cos\\theta_2}{4} = 1 - \\frac{1}{2}(\\cos\\theta_1 + \\cos\\theta_2)\n$$\n误差放大因子可以写成 $S(\\theta_1, \\theta_2) = 1 - \\omega\\lambda(\\theta_1, \\theta_2)$。\n\n光滑因子 $\\mu(\\omega)$ 定义为高频集 $\\mathcal{H}$ 上的最大放大因子：\n$$\n\\mu(\\omega) = \\max_{(\\theta_1, \\theta_2) \\in \\mathcal{H}} |S(\\theta_1, \\theta_2)| = \\max_{(\\theta_1, \\theta_2) \\in \\mathcal{H}} |1 - \\omega\\lambda(\\theta_1, \\theta_2)|\n$$\n高频集由下式给出：\n$$\n\\mathcal{H} = \\left\\{ (\\theta_1,\\theta_2) : |\\theta_1| \\in \\left[\\frac{\\pi}{2},\\pi\\right],\\;|\\theta_2| \\in \\left[\\frac{\\pi}{2},\\pi\\right] \\right\\}\n$$\n为了计算 $\\mu(\\omega)$，我们必须首先找到对于所有 $(\\theta_1, \\theta_2) \\in \\mathcal{H}$，$\\lambda(\\theta_1, \\theta_2)$ 所取的值域。记此值域为 $\\Lambda_{\\mathcal{H}}$。\n$\\lambda(\\theta_1, \\theta_2)$ 的值取决于和 $\\cos\\theta_1 + \\cos\\theta_2$。由于 $\\cos(\\theta)$ 是偶函数，$\\lambda(\\theta_1, \\theta_2)$ 的值在构成 $\\mathcal{H}$ 的四个对称区域中是相同的。因此，我们可以将分析限制在 $\\theta_1 \\in [\\frac{\\pi}{2}, \\pi]$ 和 $\\theta_2 \\in [\\frac{\\pi}{2}, \\pi]$ 的区域。\n在区间 $[\\frac{\\pi}{2}, \\pi]$ 内，函数 $\\cos(\\theta)$ 单调递减，其值域从 $\\cos(\\frac{\\pi}{2}) = 0$ 到 $\\cos(\\pi) = -1$。\n在此区域上，和 $\\cos\\theta_1 + \\cos\\theta_2$ 的最小值出现在 $\\theta_1$ 和 $\\theta_2$ 均为 $\\pi$ 时，得到 $\\cos(\\pi) + \\cos(\\pi) = -1 - 1 = -2$。\n最大值出现在 $\\theta_1$ 和 $\\theta_2$ 均为 $\\frac{\\pi}{2}$ 时，得到 $\\cos(\\frac{\\pi}{2}) + \\cos(\\frac{\\pi}{2}) = 0 + 0 = 0$。\n因此，对于 $(\\theta_1, \\theta_2) \\in \\mathcal{H}$，$\\cos\\theta_1 + \\cos\\theta_2$ 的值域是 $[-2, 0]$。\n利用这一点，我们可以确定 $\\lambda(\\theta_1, \\theta_2) = 1 - \\frac{1}{2}(\\cos\\theta_1 + \\cos\\theta_2)$ 的值域。\n$\\lambda$ 的最小值对应于 $\\cos\\theta_1 + \\cos\\theta_2$ 的最大值：\n$$\n\\lambda_{\\min} = 1 - \\frac{1}{2}(0) = 1\n$$\n这发生在 $|\\theta_1|=|\\theta_2|=\\frac{\\pi}{2}$ 的模式上。\n$\\lambda$ 的最大值对应于 $\\cos\\theta_1 + \\cos\\theta_2$ 的最小值：\n$$\n\\lambda_{\\max} = 1 - \\frac{1}{2}(-2) = 1 + 1 = 2\n$$\n这发生在 $|\\theta_1|=|\\theta_2|=\\pi$ 的模式上。\n因此，值集 $\\Lambda_{\\mathcal{H}}$ 是闭区间 $[1, 2]$。\n\n光滑因子的表达式变成了在这个区间上的一个最大化问题：\n$$\n\\mu(\\omega) = \\max_{\\lambda \\in [1, 2]} |1 - \\omega \\lambda|\n$$\n对于固定的 $\\omega$，函数 $f(\\lambda) = 1 - \\omega\\lambda$ 是关于 $\\lambda$ 的线性函数。一个线性函数在闭区间上的最大绝对值必在区间的某个端点处取得。所以，我们在 $\\lambda=1$ 和 $\\lambda=2$ 处计算 $|f(\\lambda)|$：\n$$\n\\mu(\\omega) = \\max\\left\\{|1 - \\omega \\cdot 1|, |1 - \\omega \\cdot 2|\\right\\} = \\max\\left\\{|1 - \\omega|, |1 - 2\\omega|\\right\\}\n$$\n为了找到最优权重 $\\omega_{\\text{opt}}$，我们必须找到使 $\\mu(\\omega)$ 最小化的 $\\omega > 0$ 的值。这是一个经典的极小化极大问题。$\\max\\{|f_1(\\omega)|, |f_2(\\omega)|\\}$ 的最小值通常在 $|f_1(\\omega)| = |f_2(\\omega)|$ 的点处取得。我们令最大值函数中的各项在绝对值上相等：\n$$\n|1 - \\omega| = |1 - 2\\omega|\n$$\n这个方程产生两种可能性：\n1. $1 - \\omega = 1 - 2\\omega$，化简得 $\\omega = 0$。这不是一个有效的解，因为问题指定 $\\omega > 0$。\n2. $1 - \\omega = -(1 - 2\\omega)$，化简得 $1 - \\omega = 2\\omega - 1$。整理得 $3\\omega = 2$，所以 $\\omega = \\frac{2}{3}$。\n\n为确认 $\\omega = \\frac{2}{3}$ 确实是最小值点，我们可以分析 $\\mu(\\omega)$ 的行为。函数 $\\mu(\\omega)$ 由分段函数组成。$\\omega$ 的临界点是 $0$、$\\frac{1}{2}$ 和 $1$，在这些点，绝对值内的符号发生变化。\n- 对于 $\\omega \\in (0, \\frac{2}{3})$，$\\mu(\\omega)$ 是 $\\omega$ 的递减函数。例如，在 $(0, \\frac{1}{2}]$ 区间内，$\\mu(\\omega) = 1-\\omega$。在 $(\\frac{1}{2}, \\frac{2}{3})$ 区间内，$\\mu(\\omega)$ 也等于 $1-\\omega$。\n- 对于 $\\omega \\in (\\frac{2}{3}, \\infty)$，$\\mu(\\omega)$ 是 $\\omega$ 的递增函数。例如，在 $(\\frac{2}{3}, 1)$ 区间内，$\\mu(\\omega) = 2\\omega-1$。对于 $\\omega \\ge 1$，$\\mu(\\omega)$ 也等于 $2\\omega-1$。\n\n由于当 $\\omega  \\frac{2}{3}$ 时 $\\mu(\\omega)$ 递减，当 $\\omega > \\frac{2}{3}$ 时 $\\mu(\\omega)$ 递增，因此对于 $\\omega > 0$ 的全局最小值恰好在 $\\omega = \\frac{2}{3}$ 处取得。\n最小光滑因子为 $\\mu(\\frac{2}{3}) = |1 - \\frac{2}{3}| = \\frac{1}{3}$。\n问题要求的是使 $\\mu(\\omega)$ 最小的 $\\omega$ 值。这个最优值是 $\\frac{2}{3}$。",
            "answer": "$$\n\\boxed{\\frac{2}{3}}\n$$"
        },
        {
            "introduction": "从理论分析转向编程实践，本练习要求你实现经典的雅可比、高斯-赛德尔和逐次超松弛（SOR）求解器。挑战在于将这些方法应用于具有各向异性和非均匀系数的扩散方程，这些情况在实践中常会减慢收敛速度。此练习的一个关键特色是实现一种自适应SOR方法，其中松弛参数 $\\omega$ 在每次迭代中根据残差动态更新，展示了一种在复杂场景下加速收敛的强大技术 。",
            "id": "3455492",
            "problem": "考虑线性二阶椭圆偏微分方程 (PDE) $$-\\nabla \\cdot \\left(\\mathbf{K}(x,y)\\,\\nabla u(x,y)\\right) = f(x,y) \\quad \\text{for } (x,y)\\in(0,1)\\times(0,1),$$ 附带齐次狄利克雷边界条件 $$u(x,y)=0 \\quad \\text{for } (x,y)\\in\\partial\\left((0,1)\\times(0,1)\\right),$$ 其中对称正定扩散张量为 $$\\mathbf{K}(x,y) = \\begin{pmatrix} k_x(x,y)  0  0 \\\\ 0  k_y(x,y)  0 \\end{pmatrix}.$$ 使用标准的有限差分格式在均匀网格上对该 PDE 进行离散化，该网格具有 $n\\times n$ 个内部点，两个方向的网格间距均为 $h=1/(n+1)$，并对通量使用面心系数（在单元面进行算术平均）。设离散线性系统为 $$A u = b,$$ 其中 $A\\in\\mathbb{R}^{N\\times N}$，$u\\in\\mathbb{R}^N$，$b\\in\\mathbb{R}^N$，且 $N=n^2$。用 $D=\\mathrm{diag}(A)$ 表示 $A$ 的对角线，并将矩阵 $A$ 写成标准矩阵分裂形式 $A = D - L - U$，其中 $L$ 为严格下三角部分，$U$ 为严格上三角部分。\n\n为 $A u = b$ 实现以下三种定常迭代求解器：\n- 雅可比法（首次出现：Jacobi method），定义为 $$u^{(k+1)} = D^{-1}(L+U)u^{(k)} + D^{-1}b.$$\n- 高斯-赛德尔法（首次出现：Gauss–Seidel method），通过使用分裂 $A = (D-L) - U$ 的前向替换按字典序定义。\n- 逐次超松弛法（首次出现：Successive Over-Relaxation (SOR) method），定义为 $$u^{(k+1)} = (D - \\omega L)^{-1}\\left[\\omega b - \\left(\\omega U + (\\omega - 1) D\\right)u^{(k)}\\right],$$ 其中松弛参数 $\\omega\\in(0,2)$ 在每次迭代中，使用根据当前残差构建的局部瑞利商估计来自适应地选择。\n\n在第 $k$ 次迭代时，定义残差 $$r^{(k)} = b - A u^{(k)}.$$ 对于自适应 SOR 方法，计算局部瑞利商 $$\\rho_{\\text{loc}}^{(k)} = \\frac{\\left(r^{(k)}\\right)^{\\top} A\\, r^{(k)}}{\\left(r^{(k)}\\right)^{\\top} D\\, r^{(k)}}$$ 并通过一个有原则的映射，从 $\\rho_{\\text{loc}}^{(k)}$ 得到一个能改善收敛性的松弛参数 $\\omega^{(k)}$，该映射基于雅可比迭代与 SOR 之间的关系。该映射必须在您的解法中从第一性原理推导出来，并且所得的 $\\omega^{(k)}$ 必须满足 $0  \\omega^{(k)}  2$，并对诸如残差消失等边缘情况有适当的保护措施。\n\n离散化细节。对于内部索引 $(i,j)$，其中 $i=1,\\dots,n$ 且 $j=1,\\dots,n$，令 $x_i = i h$ 和 $y_j = j h$，并设置 $$k_x^{i,j} = k_x(x_i,y_j),\\qquad k_y^{i,j} = k_y(x_i,y_j).$$ 通过算术平均定义面心扩散系数，\n$$k_{x}^{i+\\frac{1}{2},j} = \\begin{cases}\n\\frac{1}{2}\\left(k_x^{i,j} + k_x^{i+1,j}\\right),  i  n,\\\\\nk_x^{i,j},  i=n,\n\\end{cases}\n\\quad\nk_{x}^{i-\\frac{1}{2},j} = \\begin{cases}\n\\frac{1}{2}\\left(k_x^{i,j} + k_x^{i-1,j}\\right),  i  1,\\\\\nk_x^{i,j},  i=1,\n\\end{cases}$$\n$$k_{y}^{i,j+\\frac{1}{2}} = \\begin{cases}\n\\frac{1}{2}\\left(k_y^{i,j} + k_y^{i,j+1}\\right),  j  n,\\\\\nk_y^{i,j},  j=n,\n\\end{cases}\n\\quad\nk_{y}^{i,j-\\frac{1}{2}} = \\begin{cases}\n\\frac{1}{2}\\left(k_y^{i,j} + k_y^{i,j-1}\\right),  j  1,\\\\\nk_y^{i,j},  j=1.\n\\end{cases}$$\n那么在 $(i,j)$ 处的离散算子为\n$$\n(Au)^{i,j} = \\left(c_x^{+\\,i,j} + c_x^{-\\,i,j} + c_y^{+\\,i,j} + c_y^{-\\,i,j}\\right) u^{i,j}\n- c_x^{+\\,i,j}\\, u^{i+1,j} - c_x^{-\\,i,j}\\, u^{i-1,j}\n- c_y^{+\\,i,j}\\, u^{i,j+1} - c_y^{-\\,i,j}\\, u^{i,j-1},\n$$\n其中 $$c_x^{+\\,i,j} = \\frac{k_{x}^{i+\\frac{1}{2},j}}{h^2},\\quad c_x^{-\\,i,j} = \\frac{k_{x}^{i-\\frac{1}{2},j}}{h^2},\\quad c_y^{+\\,i,j} = \\frac{k_{y}^{i,j+\\frac{1}{2}}}{h^2},\\quad c_y^{-\\,i,j} = \\frac{k_{y}^{i,j-\\frac{1}{2}}}{h^2},$$ 内部区域外的边界值设为零（齐次狄利克雷）。对角线元素为 $$D^{i,j} = c_x^{+\\,i,j} + c_x^{-\\,i,j} + c_y^{+\\,i,j} + c_y^{-\\,i,j}.$$\n\n使用源项 $$f(x,y) = 1$$ 使得 $$b^{i,j} = f(x_i,y_j) = 1.$$\n\n停止准则与初始化。使用 $$u^{(0)} = 0.$$ 进行初始化。在第 $k$ 次迭代时，计算残差范数 $$\\|r^{(k)}\\|_2 = \\left(\\sum_{i=1}^n\\sum_{j=1}^n \\left(r^{(k)}\\right)^{i,j\\,2}\\right)^{1/2}.$$ 当 $$\\|r^{(k)}\\|_2 \\le \\varepsilon,$$ 时停止，容差为 $$\\varepsilon = 10^{-6},$$ 或当迭代次数达到 $$k_{\\max} = 5000.$$ 时停止。所有量均为无量纲。\n\n测试套件。使用网格尺寸 $$n = 20.$$ 定义三个测试用例以评估在各向异性和非均匀性下的鲁棒性：\n- 测试 $1$（各向异性，$x$ 方向主导）：$$k_x(x,y) \\equiv 10,\\qquad k_y(x,y) \\equiv 1.$$\n- 测试 $2$（各向异性，$y$ 方向主导）：$$k_x(x,y) \\equiv 1,\\qquad k_y(x,y) \\equiv 10.$$\n- 测试 $3$（非均匀，分段各向异性）：$$k_x(x,y) = 1 + 9\\,\\mathbf{1}_{\\{x > 0.5\\}},\\qquad k_y(x,y) = 1 + 9\\,\\mathbf{1}_{\\{y > 0.5\\}},$$ 其中 $\\mathbf{1}_{\\{\\cdot\\}}$ 是指示函数。\n\n对于每个测试用例，运行雅可比法、高斯-赛德尔法和在每次迭代中从 $\\rho_{\\text{loc}}$ 选择 $\\omega$ 的自适应 SOR 方法。记录每种方法达到停止准则所需的迭代次数。如果在 $k_{\\max}$ 次迭代内未达到停止准则，则记录 $$k_{\\max}.$$ 要求的最终输出格式是单行包含九个整数的逗号分隔列表，用方括号括起来，顺序为\n$$[\\text{Jacobi}_1,\\text{GaussSeidel}_1,\\text{SOR}_1,\\text{Jacobi}_2,\\text{GaussSeidel}_2,\\text{SOR}_2,\\text{Jacobi}_3,\\text{GaussSeidel}_3,\\text{SOR}_3].$$",
            "solution": "用户提供了一个问题，要求实现并比较三种迭代求解器（雅可比法、高斯-赛德尔法和自适应逐次超松弛法），用于求解由二阶椭圆偏微分方程的有限差分法离散化得到的线性系统。\n\n### 步骤 1：问题验证\n\n在尝试解决问题之前，需对问题进行严格的验证程序。\n\n- **已知条件提取**:\n    - PDE: $-\\nabla \\cdot \\left(\\mathbf{K}(x,y)\\,\\nabla u(x,y)\\right) = f(x,y)$ 在单位正方形 $(0,1)\\times(0,1)$上。\n    - 边界条件: 齐次狄利克雷，$u(x,y)=0$ 在边界上。\n    - 扩散张量: $\\mathbf{K}(x,y) = \\mathrm{diag}(k_x(x,y), k_y(x,y))$，对称正定。\n    - 源项: $f(x,y) = 1$。\n    - 离散化: 在具有 $n\\times n$ 个内部点的均匀网格上采用标准的七点有限差分格式，网格间距为 $h=1/(n+1)$。面心扩散系数使用算术平均计算。\n    - 线性系统: $A u = b$，其中 $N=n^2$。\n    - 迭代求解器: 雅可比法、高斯-赛德尔法和自适应 SOR 方法。\n    - 自适应 SOR: 松弛参数 $\\omega^{(k)}$ 在每次迭代 $k$ 中根据局部瑞利商 $\\rho_{\\text{loc}}^{(k)} = \\frac{\\left(r^{(k)}\\right)^{\\top} A\\, r^{(k)}}{\\left(r^{(k)}\\right)^{\\top} D\\, r^{(k)}}$ 确定，其中 $r^{(k)} = b - A u^{(k)}$ 且 $D = \\mathrm{diag}(A)$。\n    - 初始化: $u^{(0)} = 0$。\n    - 停止准则: $\\|r^{(k)}\\|_2 \\le \\varepsilon=10^{-6}$ 或迭代次数 $k \\ge k_{\\max}=5000$。\n    - 测试套件: $n=20$，包含三个指定的扩散张量 $\\mathbf{K}(x,y)$ 测试用例，用于测试各向异性和非均匀性。\n    - 输出: 一个包含九个整数的列表，表示每种方法在每个测试用例上的迭代次数。\n\n- **验证结论**:\n    1.  **科学依据**: 该问题是应用数值线性代数求解 PDE 的经典案例，是计算物理和工程学的基石。所用方法和概念均为标准且成熟的。\n    2.  **适定性**: 带有狄利克雷条件的椭圆 PDE 是适定的。所述的有限差分法导出一个线性系统 $Au=b$，其中矩阵 A 是对称正定 (SPD) 的，保证了唯一解的存在。指定的迭代方法对于此类系统是收敛的。SOR 的自适应方案是一种公认的（尽管是高级的）技术。\n    3.  **客观性**: 问题以数学精度进行规定，提供了所有必要的公式、参数和测试用例。它没有歧义或主观陈述。\n    4.  **完整性**: 提供了所有必需的信息。离散化方案，包括边界附近系数的处理，都有明确定义。\n\n该问题被判定为**有效**。这是一个结构良好、可解的数值分析问题。\n\n### 步骤 2：原理性解法\n\n#### I. 离散化与线性系统\n偏微分方程 $-\\frac{\\partial}{\\partial x}(k_x \\frac{\\partial u}{\\partial x}) - \\frac{\\partial}{\\partial y}(k_y \\frac{\\partial u}{\\partial y}) = f$ 在间距为 $h=1/(n+1)$ 的均匀网格上进行离散化。对导数使用中心差分近似，并对面心系数 $k_x$ 和 $k_y$ 使用指定的算术平均，我们为每个内部网格点 $(i,j)$ 获得一个线性方程，其中 $i,j \\in \\{1, \\dots, n\\}$。这个由 $N=n^2$ 个方程组成的系统可以写成矩阵形式 $Au = b$。\n\n矩阵 $A$ 对每个内部点都具有一个五点模板结构。对于一个点 $(i,j)$ (映射到单个索引 p)，矩阵 $A$ 的第 p 行将具有与该点自身及其四个邻点 $(i,j)$, $(i\\pm1, j)$, 和 $(i, j\\pm1)$ 相对应的非零项。系数在问题描述中已定义。矩阵 $A$ 是对称的，因为 $k_{x}^{i+\\frac{1}{2},j}$ 是耦合节点 $(i,j)$ 和 $(i+1,j)$ 的系数，并且它也是耦合 $(i+1,j)$ 到 $(i,j)$ 的相同系数。由于扩散张量 $\\mathbf{K}$ 是正定的，因此得到的矩阵 $A$ 是对称正定 (SPD) 的。\n\n#### II. 迭代求解器\n我们的任务是使用三种定常迭代方法求解 $Au=b$。这些方法基于将矩阵 $A$ 分裂为 $A = M - K$，从而得到迭代格式 $M u^{(k+1)} = K u^{(k)} + b$。为保证收敛，迭代矩阵 $M^{-1}K$ 的谱半径必须小于 1。\n\n1.  **雅可比法**:\n    矩阵 $A$ 分裂为 $A = D - L - U$，其中 $D$ 是 $A$ 的对角部分，$-L$ 是严格下三角部分，$-U$ 是严格上三角部分。雅可比法使用 $M=D$ 和 $K=L+U$。迭代公式为：\n    $$u^{(k+1)} = D^{-1}(L+U)u^{(k)} + D^{-1}b$$\n    这可以使用残差 $r^{(k)} = b - Au^{(k)}$ 重写为：\n    $$u^{(k+1)} = D^{-1}(D-A)u^{(k)} + D^{-1}b = (I - D^{-1}A)u^{(k)} + D^{-1}b = u^{(k)} + D^{-1}(b - Au^{(k)}) = u^{(k)} + D^{-1}r^{(k)}$$\n    在计算上，每个分量 $u_i^{(k+1)}$ 仅使用 $u^{(k)}$ 的分量来计算。这意味着整个 $u^{(k+1)}$ 向量可以并行计算，并且需要存储前一个迭代的完整副本 $u^{(k)}$。\n\n2.  **高斯-赛德尔法**:\n    高斯-赛德尔法使用分裂 $M=D-L$ 和 $K=U$。迭代公式为：\n    $$(D-L)u^{(k+1)} = Uu^{(k)} + b$$\n    由于 $D-L$ 是下三角矩阵，$u^{(k+1)}$ 可以通过前向替换求得。在实践中，这通过按字典序遍历网格点来实现，其中对 $u_{i,j}$ 的更新会立即使用来自同一次迭代 $k+1$ 的新计算出的 $u_{i-1,j}$ 和 $u_{i,j-1}$ 的值。这通常比雅可比法收敛得更快。对于 SPD 矩阵 $A$，收敛性是有保证的。\n\n3.  **自适应逐次超松弛 (SOR) 法**:\n    SOR 方法是高斯-赛德尔法的一种加速，引入了松弛参数 $\\omega$。矩阵分裂为 $M = \\frac{1}{\\omega}(D-\\omega L)$ 和 $K = \\frac{1}{\\omega}((1-\\omega)D + \\omega U)$。更新公式为：\n    $$u^{(k+1)} = (1-\\omega)u^{(k)} + \\omega u_{GS}^{(k+1)}$$\n    其中 $u_{GS}^{(k+1)}$ 是高斯-赛德尔步骤将计算出的迭代值。对于 SPD 矩阵，对任何 $\\omega \\in (0,2)$，收敛性都有保证。$\\omega$ 的最优选择，记为 $\\omega_{opt}$，可以显著加速收敛。\n\n    **ω 的自适应选择**: 问题要求在每次迭代 k 中采用自适应策略来选择 ω。对于一致有序矩阵，最优 ω 的经典公式是：\n    $$\\omega_{opt} = \\frac{2}{1 + \\sqrt{1 - \\rho(T_J)^2}}$$\n    其中 $\\rho(T_J)$ 是雅可比迭代矩阵 $T_J = D^{-1}(L+U) = I - D^{-1}A$ 的谱半径。自适应策略的核心是在每次迭代中估计 $\\rho(T_J)$。\n\n    $T_J$ 的特征值 $\\mu$ 与 $D^{-1}A$ 的特征值 $\\lambda$ 通过 $\\mu = 1-\\lambda$ 相关。由于 A 是 SPD 矩阵，特征值 $\\lambda$ 是实数且为正，为使雅可比法收敛，我们必须有 $0  \\lambda  2$。谱半径为 $\\rho(T_J) = \\max_i|\\mu_i| = \\max_i|1-\\lambda_i|$。\n\n    问题提供了局部瑞利商来估计一个特征值：\n    $$\\rho_{\\text{loc}}^{(k)} = \\frac{\\left(r^{(k)}\\right)^{\\top} A\\, r^{(k)}}{\\left(r^{(k)}\\right)^{\\top} D\\, r^{(k)}} = \\frac{(D^{1/2}r^{(k)})^T (D^{-1/2} A D^{-1/2}) (D^{1/2}r^{(k)})}{(D^{1/2}r^{(k)})^T (D^{1/2}r^{(k)})}$$\n    这是矩阵 $D^{-1/2} A D^{-1/2}$ 的瑞利商，该矩阵与 $D^{-1}A$ 具有相同的特征值 $\\{\\lambda_i\\}$。因此，$\\rho_{\\text{loc}}^{(k)}$ 是 $D^{-1}A$ 某个特征值 $\\lambda$ 的估计。经过几次迭代后，残差向量 $r^{(k)}$ 趋向于由对应于最限制收敛的特征值的特征向量所主导。对于这类问题，这些是“最平滑”的特征向量，对应于 $D^{-1}A$ 的小特征值。\n\n    让我们假设 $\\rho_{\\text{loc}}^{(k)}$ 是特征值 $\\lambda$ 的一个估计。一个常见的启发式方法是将雅可比谱半径估计为 $\\hat{\\rho}_k = |1 - \\rho_{\\text{loc}}^{(k)}|$。将此代入 $\\omega_{opt}$ 的公式中，可得到 $\\omega^{(k)}$ 的自适应估计：\n    $$\\omega^{(k)} = \\frac{2}{1 + \\sqrt{1 - \\hat{\\rho}_k^2}} = \\frac{2}{1 + \\sqrt{1 - (1-\\rho_{\\text{loc}}^{(k)})^2}} = \\frac{2}{1 + \\sqrt{\\rho_{\\text{loc}}^{(k)}(2-\\rho_{\\text{loc}}^{(k)})}}$$\n    平方根下的项是非负的，因为 $0  \\lambda  2 \\implies 0  \\rho_{\\text{loc}}^{(k)}  2$。所得的 $\\omega^{(k)}$ 将在 $[1, 2)$ 范围内，提供超松弛。该公式提供了从局部瑞利商到松弛参数的有原则的映射，正如题目所要求。\n\n#### III. 实现\n该解法将使用 Python 和 NumPy 库实现。一个主函数将协调三个测试用例。对于每个用例，它将：\n1.  设置网格和问题特定的系数 ($k_x, k_y$)。这涉及到为面心系数 ($c_x^\\pm, c_y^\\pm$) 和对角项 D 创建二维数组。\n2.  定义一个辅助函数 `matvec(u)` 来高效地计算矩阵-向量积 $Au$，这对于计算残差至关重要。\n3.  调用三个独立的函数，每个求解器一个（`jacobi_solver`、`gauss_seidel_solver`、`sor_solver`）。\n4.  每个求解器函数将初始化解向量 $u=0$，然后循环直到收敛或达到最大迭代次数。\n5.  在每个循环中，检查残差范数 $\\|r^{(k)}\\|_2$。雅可比求解器将使用一个辅助数组来存储前一个迭代值。高斯-赛德尔和 SOR 求解器将执行原地更新。SOR 求解器还将使用推导出的公式在每一步计算 $\\omega^{(k)}$。\n6.  记录每次运行的迭代次数。如果未达到收敛，则记录值 $k_{\\max}$。\n最终结果被收集并以指定格式打印。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the specified PDE using Jacobi, Gauss-Seidel, and adaptive SOR methods\n    for three different test cases and reports the number of iterations required for convergence.\n    \"\"\"\n    n = 20\n    epsilon = 1e-6\n    k_max = 5000\n\n    def kx1(x, y): return 10.0\n    def ky1(x, y): return 1.0\n\n    def kx2(x, y): return 1.0\n    def ky2(x, y): return 10.0\n\n    def kx3(x, y): return 10.0 if x  0.5 else 1.0\n    def ky3(x, y): return 10.0 if y  0.5 else 1.0\n\n    test_cases = [\n        (kx1, ky1),\n        (kx2, ky2),\n        (kx3, ky3),\n    ]\n\n    def setup_coefficients(n, kx_func, ky_func):\n        h = 1.0 / (n + 1)\n        h2 = h * h\n\n        kx_grid = np.zeros((n + 2, n + 2))\n        ky_grid = np.zeros((n + 2, n + 2))\n        for i in range(1, n + 1):\n            for j in range(1, n + 1):\n                kx_grid[i, j] = kx_func(i * h, j * h)\n                ky_grid[i, j] = ky_func(i * h, j * h)\n\n        cx_p = np.zeros((n, n)); cx_m = np.zeros((n, n))\n        cy_p = np.zeros((n, n)); cy_m = np.zeros((n, n))\n\n        for i in range(n):\n            for j in range(n):\n                i1, j1 = i + 1, j + 1\n\n                k_xp = kx_grid[i1, j1] if i1 == n else 0.5 * (kx_grid[i1, j1] + kx_grid[i1 + 1, j1])\n                k_xm = kx_grid[i1, j1] if i1 == 1 else 0.5 * (kx_grid[i1, j1] + kx_grid[i1 - 1, j1])\n                k_yp = ky_grid[i1, j1] if j1 == n else 0.5 * (ky_grid[i1, j1] + ky_grid[i1, j1 + 1])\n                k_ym = ky_grid[i1, j1] if j1 == 1 else 0.5 * (ky_grid[i1, j1] + ky_grid[i1, j1 - 1])\n                \n                cx_p[i, j] = k_xp / h2\n                cx_m[i, j] = k_xm / h2\n                cy_p[i, j] = k_yp / h2\n                cy_m[i, j] = k_ym / h2\n\n        D = cx_p + cx_m + cy_p + cy_m\n        return {'cx_p': cx_p, 'cx_m': cx_m, 'cy_p': cy_p, 'cy_m': cy_m, 'D': D}\n\n    def matvec(u, n, coeffs):\n        u_padded = np.pad(u, 1, 'constant')\n        u_ip1, u_im1 = u_padded[2:, 1:-1], u_padded[:-2, 1:-1]\n        u_jp1, u_jm1 = u_padded[1:-1, 2:], u_padded[1:-1, :-2]\n        \n        Au = (coeffs['D'] * u -\n              (coeffs['cx_p'] * u_ip1 + coeffs['cx_m'] * u_im1 +\n               coeffs['cy_p'] * u_jp1 + coeffs['cy_m'] * u_jm1))\n        return Au\n\n    def jacobi_solver(n, b, coeffs, k_max, epsilon):\n        u = np.zeros((n, n))\n        for k in range(k_max):\n            r = b - matvec(u, n, coeffs)\n            if np.linalg.norm(r) = epsilon:\n                return k\n            \n            u = u + r / coeffs['D'] # This is a parallel update, equivalent to Jacobi\n        return k_max\n\n    def gauss_seidel_solver(n, b, coeffs, k_max, epsilon):\n        u = np.zeros((n, n))\n        D = coeffs['D']\n        cx_p, cx_m = coeffs['cx_p'], coeffs['cx_m']\n        cy_p, cy_m = coeffs['cy_p'], coeffs['cy_m']\n\n        for k in range(k_max):\n            r = b - matvec(u, n, coeffs)\n            if np.linalg.norm(r) = epsilon:\n                return k\n            \n            u_padded = np.pad(u, 1, 'constant')\n            for i in range(n):\n                for j in range(n):\n                    i_p, j_p = i + 1, j + 1\n                    term_neighbors = (cx_p[i, j] * u_padded[i_p + 1, j_p] +\n                                      cx_m[i, j] * u_padded[i_p - 1, j_p] +\n                                      cy_p[i, j] * u_padded[i_p, j_p + 1] +\n                                      cy_m[i, j] * u_padded[i_p, j_p - 1])\n                    u_padded[i_p, j_p] = (b[i, j] + term_neighbors) / D[i, j]\n            u = u_padded[1:-1, 1:-1]\n        return k_max\n\n    def sor_solver(n, b, coeffs, k_max, epsilon):\n        u = np.zeros((n, n))\n        D = coeffs['D']\n        cx_p, cx_m = coeffs['cx_p'], coeffs['cx_m']\n        cy_p, cy_m = coeffs['cy_p'], coeffs['cy_m']\n\n        for k in range(k_max):\n            r = b - matvec(u, n, coeffs)\n            if np.linalg.norm(r) = epsilon:\n                return k\n\n            # Compute adaptive omega\n            Ar = matvec(r, n, coeffs)\n            r_Dr = np.sum(r * D * r)\n            r_Ar = np.sum(r * Ar)\n\n            if abs(r_Dr)  1e-14:\n                omega = 1.0 # Fallback to Gauss-Seidel\n            else:\n                rho_loc = r_Ar / r_Dr\n                rho_loc = max(1e-12, min(rho_loc, 2.0 - 1e-12))\n                omega = 2.0 / (1.0 + np.sqrt(rho_loc * (2.0 - rho_loc)))\n            \n            # SOR sweep\n            u_padded = np.pad(u, 1, 'constant')\n            for i in range(n):\n                for j in range(n):\n                    i_p, j_p = i + 1, j + 1\n                    term_neighbors = (cx_p[i, j] * u_padded[i_p + 1, j_p] +\n                                      cx_m[i, j] * u_padded[i_p - 1, j_p] +\n                                      cy_p[i, j] * u_padded[i_p, j_p + 1] +\n                                      cy_m[i, j] * u_padded[i_p, j_p - 1])\n                    \n                    u_gs = (b[i, j] + term_neighbors) / D[i, j]\n                    u_padded[i_p, j_p] += omega * (u_gs - u_padded[i_p, j_p])\n            u = u_padded[1:-1, 1:-1]\n        return k_max\n\n    results = []\n    b = np.ones((n, n))\n    \n    for kx_func, ky_func in test_cases:\n        coeffs = setup_coefficients(n, kx_func, ky_func)\n        \n        iter_jacobi = jacobi_solver(n, b, coeffs, k_max, epsilon)\n        iter_gs = gauss_seidel_solver(n, b, coeffs, k_max, epsilon)\n        iter_sor = sor_solver(n, b, coeffs, k_max, epsilon)\n        \n        results.extend([iter_jacobi, iter_gs, iter_sor])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "最后的这个练习处理一个位于数值模拟前沿的问题：在复杂的弯曲界面上，扩散系数存在巨大的不连续跳跃。这类问题对标准迭代法构成了严峻挑战。本练习聚焦于高斯-赛德尔和SOR方法中一个微妙但强大的方面：更新顺序的影响。通过实现并比较标准的字典序和与材料界面对齐的顺序，你将定量地研究如何根据问题的物理特性来定制算法，从而显著提高针对沿不连续性分布的误差的光滑性能 。",
            "id": "3455542",
            "problem": "考虑一个由椭圆偏微分方程 $-\\nabla \\cdot (k(x,y)\\nabla u(x,y)) = 0$ 定义的可变不连续电导率的扩散模型，该模型定义在方形域 $\\Omega = (0,1)\\times(0,1)$ 上，并服从齐次狄利克雷边界条件 $u(x,y) = 0$ for $(x,y) \\in \\partial\\Omega$。电导率 $k(x,y)$ 是分段常数，其不连续界面由一个正弦扰动的圆定义。设该界面在相对于中心 $(x_c,y_c) = (1/2,1/2)$ 的极坐标中由 $r(\\theta) = r_0 + \\varepsilon \\sin(m\\theta)$ 给出，其中 $\\theta(x,y) = \\operatorname{atan2}(y-y_c, x-x_c)$，$\\rho(x,y) = \\sqrt{(x-x_c)^2 + (y-y_c)^2}$，且 $r_0 \\in (0,1/2)$，$\\varepsilon \\ge 0$，$m \\in \\mathbb{N}$。定义符号距离场 $\\varphi(x,y) = \\rho(x,y) - r(\\theta)$，以及电导率为\n$$\nk(x,y) = \n\\begin{cases}\nk_{\\mathrm{in}},  \\text{若 } \\varphi(x,y)  0, \\\\\nk_{\\mathrm{out}},  \\text{若 } \\varphi(x,y) \\ge 0,\n\\end{cases}\n$$\n其中常数 $k_{\\mathrm{in}}  0$，$k_{\\mathrm{out}}  0$。\n\n使用具有 $N\\times N$ 个内部未知量和网格间距 $h = 1/(N+1)$ 的均匀单元中心网格对域 $\\Omega$ 进行离散化。对于每个内部单元 $(i,j)$，通过在面上使用电导率的调和平均，以五点有限体积模板来近似算子 $-\\nabla \\cdot (k \\nabla u)$，以确保跨不连续面的通量连续性。具体来说，定义面电导率\n$$\nk_{i+1/2,j} = \\frac{2\\,k_{i,j}\\,k_{i+1,j}}{k_{i,j} + k_{i+1,j}},\\quad\nk_{i-1/2,j} = \\frac{2\\,k_{i,j}\\,k_{i-1,j}}{k_{i,j} + k_{i-1,j}},\n$$\n$$\nk_{i,j+1/2} = \\frac{2\\,k_{i,j}\\,k_{i,j+1}}{k_{i,j} + k_{i,j+1}},\\quad\nk_{i,j-1/2} = \\frac{2\\,k_{i,j}\\,k_{i,j-1}}{k_{i,j} + k_{i,j-1}},\n$$\n约定缺失的邻居（与边界相邻）使用内部值，且狄利克雷边界值为零。得到的离散算子具有以下系数\n$$\na_{i,j}^{\\mathrm{E}} = -\\frac{k_{i+1/2,j}}{h^2},\\quad\na_{i,j}^{\\mathrm{W}} = -\\frac{k_{i-1/2,j}}{h^2},\\quad\na_{i,j}^{\\mathrm{N}} = -\\frac{k_{i,j+1/2}}{h^2},\\quad\na_{i,j}^{\\mathrm{S}} = -\\frac{k_{i,j-1/2}}{h^2},\n$$\n$$\na_{i,j}^{\\mathrm{diag}} = -\\left(a_{i,j}^{\\mathrm{E}} + a_{i,j}^{\\mathrm{W}} + a_{i,j}^{\\mathrm{N}} + a_{i,j}^{\\mathrm{S}}\\right),\n$$\n因此，作用于网格函数 $u$ 的离散算子 $A$ 为\n$$\n(Au)_{i,j} = a_{i,j}^{\\mathrm{diag}}\\,u_{i,j}\n+ a_{i,j}^{\\mathrm{E}}\\,u_{i+1,j}\n+ a_{i,j}^{\\mathrm{W}}\\,u_{i-1,j}\n+ a_{i,j}^{\\mathrm{N}}\\,u_{i,j+1}\n+ a_{i,j}^{\\mathrm{S}}\\,u_{i,j-1}.\n$$\n\n与偏微分方程相关的线性系统的经典松弛方法有：\n- 雅可比（Jacobi）法：使用 $A$ 的对角部分 $D$，更新\n$$\nu^{(n+1)} = u^{(n)} + \\omega_{\\mathrm{J}} D^{-1}\\left(f - A u^{(n)}\\right),\n$$\n其中标量权重 $0  \\omega_{\\mathrm{J}} \\le 1$ 以确保稳定性，此处考虑 $f=0$。\n- 高斯-赛德尔（Gauss-Seidel）法：将 $A$ 分解为严格下三角、对角和严格上三角部分 $A = L + D + U$，并更新\n$$\n(D+L)\\,u^{(n+1)} = f - U\\,u^{(n)},\n$$\n其中 $f=0$。未知数的排序决定了 $L$ 和 $U$ 的结构。\n- 逐次超松弛（SOR）法：通过对高斯-赛德尔更新应用松弛参数 $\\omega_{\\mathrm{SOR}} \\in (0,2)$\n$$\nu^{(n+1)} = u^{(n)} + \\omega_{\\mathrm{SOR}}\\left(u^{(n+1)}_{\\mathrm{GS}} - u^{(n)}\\right),\n$$\n其中 $u^{(n+1)}_{\\mathrm{GS}}$ 表示在当前节点上按所选排序方式进行的高斯-赛德尔更新。\n\n定义两种排序方式：\n- 自然字典序：按递增的行主序更新 $(i,j)$（例如，先从 $j=0$ 到 $N-1$，在每一行内从 $i=0$ 到 $N-1$）。\n- 界面对齐排序：计算网格点上的符号距离场 $\\varphi_{i,j}$，并按 $\\varphi_{i,j}$ 的升序对节点进行更新，即沿平行于界面的层进行扫描。\n\n为了量化“界面平行模式的光滑化”，构建一个初始误差模式，该模式局部化在界面附近并沿界面振荡：\n$$\ne_0(x,y) = \\exp\\!\\left(-\\left(\\frac{\\varphi(x,y)}{\\sigma}\\right)^2\\right)\\,\\sin\\!\\big(q\\,\\theta(x,y)\\big),\n$$\n其中 $\\sigma > 0$ 控制局部化带宽，而 $q \\in \\mathbb{N}$ 是角波数，选择为 $q = \\min\\{32, \\lfloor N/4 \\rfloor\\}$。令 $\\mathcal{M} = \\{(x,y) \\in \\Omega : |\\varphi(x,y)| \\le 2\\sigma\\}$ 为界面附近的测量带。对于从 $e_0$ 生成 $e_1$ 的单次松弛扫描，定义光滑因子\n$$\nS = \\frac{\\|e_1\\|_{2,\\mathcal{M}}}{\\|e_0\\|_{2,\\mathcal{M}}},\\quad\n\\|w\\|_{2,\\mathcal{M}} = \\left(\\sum_{(i,j)\\in \\mathcal{M}} w_{i,j}^2\\right)^{1/2},\n$$\n其中求和遍及坐标位于 $\\mathcal{M}$ 内的单元中心。\n\n实现离散算子和三种松弛方法（雅可比法、高斯-赛德尔法和逐次超松弛法），并计算以下五种配置的光滑因子 $S$：\n1. 雅可比法，权重 $\\omega_{\\mathrm{J}} = 0.66$。\n2. 高斯-赛德尔法，采用自然字典序（无松弛，即 $\\omega_{\\mathrm{SOR}} = 1$）。\n3. 高斯-赛德尔法，采用界面对齐排序（无松弛）。\n4. 逐次超松弛法，$\\omega_{\\mathrm{SOR}} = 1.9$，采用自然字典序。\n5. 逐次超松弛法，$\\omega_{\\mathrm{SOR}} = 1.9$，采用界面对齐排序。\n\n您的程序必须为以下每个测试用例计算这五个光滑因子并返回结果：\n\n测试套件：\n- 用例 A（正常路径）：$N=96$, $r_0=0.35$, $\\varepsilon=0.04$, $m=4$, $k_{\\mathrm{in}}=10$, $k_{\\mathrm{out}}=1$, $\\sigma=0.03$, $\\omega_{\\mathrm{J}}=0.66$, $\\omega_{\\mathrm{SOR}}=1.9$。\n- 用例 B（高对比度）：$N=96$, $r_0=0.35$, $\\varepsilon=0.08$, $m=6$, $k_{\\mathrm{in}}=1000$, $k_{\\mathrm{out}}=1$, $\\sigma=0.03$, $\\omega_{\\mathrm{J}}=0.66$, $\\omega_{\\mathrm{SOR}}=1.9$。\n- 用例 C（边界和曲率变化）：$N=48$, $r_0=0.35$, $\\varepsilon=0.12$, $m=8$, $k_{\\mathrm{in}}=50$, $k_{\\mathrm{out}}=1$, $\\sigma=0.04$, $\\omega_{\\mathrm{J}}=0.66$, $\\omega_{\\mathrm{SOR}}=1.9$。\n\n最终输出格式：\n您的程序应生成一行输出，其中包含一个以逗号分隔的浮点数列表的列表形式的结果，顺序为用例 A、B、C，在每个用例中，按上述五种配置的顺序排列。例如，输出形式应为\n$$\n\\big[\\,[S_{A,\\mathrm{J}}, S_{A,\\mathrm{GSlex}}, S_{A,\\mathrm{GS\\varphi}}, S_{A,\\mathrm{SORlex}}, S_{A,\\mathrm{SOR\\varphi}}],\\ [S_{B,\\mathrm{J}}, \\dots],\\ [S_{C,\\mathrm{J}}, \\dots]\\,\\big]\n$$\n以单行 Python 列表字面量的形式打印。",
            "solution": "我们从齐次狄利克雷边界条件下的椭圆扩散模型 $-\\nabla \\cdot (k \\nabla u) = 0$ on $\\Omega = (0,1)^2$ 开始。电导率 $k(x,y)$ 沿着由水平集 $\\varphi(x,y) = 0$ 定义的弯曲界面 $\\Gamma$ 不连续，其中符号距离 $\\varphi(x,y) = \\rho(x,y) - r(\\theta)$ 使用中心 $(x_c,y_c)=(1/2,1/2)$、径向坐标 $\\rho(x,y) = \\sqrt{(x-x_c)^2 + (y-y_c)^2}$、角度 $\\theta(x,y) = \\operatorname{atan2}(y-y_c, x-x_c)$ 和扰动半径 $r(\\theta) = r_0 + \\varepsilon \\sin(m\\theta)$。电导率在内部（$\\varphi  0$）为 $k_{\\mathrm{in}}$，在外部（$\\varphi \\ge 0$）为 $k_{\\mathrm{out}}$。\n\n为了进行离散化，我们采用一个具有 $N \\times N$ 个未知量 $(i,j)$（其中 $i,j \\in \\{0,\\dots,N-1\\}$）的单元中心网格。网格间距为 $h=1/(N+1)$，因此单元中心 $(i,j)$ 的位置是 $(x_i, y_j) = ((i+1)h, (j+1)h)$。我们以通量形式近似散度算子。穿过 $(i,j)$ 和 $(i+1,j)$ 之间垂直面的法向通量近似为 $-k_{i+1/2,j}\\,(u_{i+1,j} - u_{i,j})/h$，其中 $k_{i+1/2,j}$ 是面电导率。对于不连续系数，一个经过充分检验的近似是跨面的调和平均值，\n$$\nk_{i+1/2,j} = \\frac{2\\,k_{i,j}\\,k_{i+1,j}}{k_{i,j} + k_{i+1,j}},\n$$\n这强制了通量的连续性并与物理界面条件保持一致。类似的公式适用于所有面。在零狄利克雷边界条件下，内部网格之外的相邻值为零，我们取与边界相邻的面上的面电导率等于内部值以保持一致性（由于边界值为零，邻居的贡献消失）。\n\n将单元 $(i,j)$ 四个面上的通量求和并除以 $h$，得到离散算子 $A$ 的五点模板：\n$$\n(Au)_{i,j} =\n\\frac{k_{i+1/2,j}}{h^2}\\,(u_{i,j} - u_{i+1,j})\n+ \\frac{k_{i-1/2,j}}{h^2}\\,(u_{i,j} - u_{i-1,j})\n+ \\frac{k_{i,j+1/2}}{h^2}\\,(u_{i,j} - u_{i,j+1})\n+ \\frac{k_{i,j-1/2}}{h^2}\\,(u_{i,j} - u_{i,j-1}).\n$$\n重写后得到标准形式\n$$\n(Au)_{i,j} = a_{i,j}^{\\mathrm{diag}}\\,u_{i,j}\n+ a_{i,j}^{\\mathrm{E}}\\,u_{i+1,j}\n+ a_{i,j}^{\\mathrm{W}}\\,u_{i-1,j}\n+ a_{i,j}^{\\mathrm{N}}\\,u_{i,j+1}\n+ a_{i,j}^{\\mathrm{S}}\\,u_{i,j-1},\n$$\n其中非对角系数为负，\n$$\na_{i,j}^{\\mathrm{E}} = -\\frac{k_{i+1/2,j}}{h^2},\\quad\na_{i,j}^{\\mathrm{W}} = -\\frac{k_{i-1/2,j}}{h^2},\\quad\na_{i,j}^{\\mathrm{N}} = -\\frac{k_{i,j+1/2}}{h^2},\\quad\na_{i,j}^{\\mathrm{S}} = -\\frac{k_{i,j-1/2}}{h^2},\n$$\n而对角系数是正的面权重之和\n$$\na_{i,j}^{\\mathrm{diag}} = -\\left(a_{i,j}^{\\mathrm{E}} + a_{i,j}^{\\mathrm{W}} + a_{i,j}^{\\mathrm{N}} + a_{i,j}^{\\mathrm{S}}\\right)\n= \\frac{k_{i+1/2,j} + k_{i-1/2,j} + k_{i,j+1/2} + k_{i,j-1/2}}{h^2}.\n$$\n\n松弛法作用于线性系统 $A u = f$；这里我们研究误差方程的光滑化，其中 $f=0$，因此更新将误差 $e^{(n)}$ 转换为 $e^{(n+1)}$。雅可比法仅使用 $A$ 的对角部分 $D$：\n$$\ne^{(n+1)} = e^{(n)} - \\omega_{\\mathrm{J}}\\,D^{-1} A e^{(n)}.\n$$\n在分量形式下，于 $(i,j)$ 处的雅可比更新可以表示为\n$$\ne_{i,j}^{(n+1)} = (1-\\omega_{\\mathrm{J}})\\,e_{i,j}^{(n)} + \\omega_{\\mathrm{J}}\\,\\frac{-a_{i,j}^{\\mathrm{E}}\\,e_{i+1,j}^{(n)}\n- a_{i,j}^{\\mathrm{W}}\\,e_{i-1,j}^{(n)}\n- a_{i,j}^{\\mathrm{N}}\\,e_{i,j+1}^{(n)}\n- a_{i,j}^{\\mathrm{S}}\\,e_{i,j-1}^{(n)}}{a_{i,j}^{\\mathrm{diag}}},\n$$\n这是通过重新排列 $D^{-1} A e^{(n)}$ 并识别五点模板结构推导出来的。\n\n高斯-赛德尔法在预设的排序中使用最新的可用邻居值。如果我们根据所选排序将 $A$ 的下三角、对角和上三角部分表示为 $L$、$D$ 和 $U$，则更新求解\n$$\n(D+L)\\,e^{(n+1)} = -U\\,e^{(n)}.\n$$\n在分量形式下，对于按所选排序访问的节点 $(i,j)$，高斯-赛德尔法将当前扫描中已更新的邻居替换为 $e^{(n+1)}$，而尚未访问的邻居则使用 $e^{(n)}$，从而得到\n$$\ne_{i,j}^{(n+1)} = \\frac{-a_{i,j}^{\\mathrm{E}}\\,\\tilde{e}_{i+1,j}\n- a_{i,j}^{\\mathrm{W}}\\,\\tilde{e}_{i-1,j}\n- a_{i,j}^{\\mathrm{N}}\\,\\tilde{e}_{i,j+1}\n- a_{i,j}^{\\mathrm{S}}\\,\\tilde{e}_{i,j-1}}{a_{i,j}^{\\mathrm{diag}}},\n$$\n其中 $\\tilde{e}$ 表示遵循该排序的最新可用邻居值。逐次超松弛法在每个节点上对高斯-赛德尔更新应用一个松弛因子：\n$$\ne_{i,j}^{(n+1)} = (1-\\omega_{\\mathrm{SOR}})\\,e_{i,j}^{(n)} + \\omega_{\\mathrm{SOR}}\\,\\frac{-a_{i,j}^{\\mathrm{E}}\\,\\tilde{e}_{i+1,j}\n- a_{i,j}^{\\mathrm{W}}\\,\\tilde{e}_{i-1,j}\n- a_{i,j}^{\\mathrm{N}}\\,\\tilde{e}_{i,j+1}\n- a_{i,j}^{\\mathrm{S}}\\,\\tilde{e}_{i,j-1}}{a_{i,j}^{\\mathrm{diag}}}.\n$$\n\n排序的选择对高斯-赛德尔和逐次超松弛法有至关重要的影响，因为它决定了哪些邻居值被视为“已更新”，从而决定了该方法更积极地抑制哪些误差分量。自然字典序对应于逐行扫描网格，这与笛卡尔方向对齐。界面对齐排序按符号距离 $\\varphi$ 的升序对节点进行排序，有效地沿平行于界面的层进行扫描。对于沿界面振荡并局部化在其附近的模式，将扫描与常数 $\\varphi$ 层对齐，优先使用法向方向上新更新的值来处理后续节点，从而改善对界面平行误差分量的抑制，否则当排序未对齐时，这些分量会沿着不连续面滑动。\n\n为了量化界面平行模式的光滑化，我们定义初始误差\n$$\ne_0(x,y) = \\exp\\!\\left(-\\left(\\frac{\\varphi(x,y)}{\\sigma}\\right)^2\\right)\\,\\sin\\!\\big(q\\,\\theta(x,y)\\big),\n$$\n其中 $q$ 相对于网格分辨率选择（$q = \\min\\{32, \\lfloor N/4 \\rfloor\\}$），$\\sigma$ 控制局部化。在给定方法和排序下进行一次松弛扫描产生 $e_1$ 后，我们计算光滑因子\n$$\nS = \\frac{\\|e_1\\|_{2,\\mathcal{M}}}{\\|e_0\\|_{2,\\mathcal{M}}},\n$$\n其中 $\\mathcal{M} = \\{(x,y): |\\varphi(x,y)| \\le 2\\sigma\\}$ 在界面周围隔离出一个带。该度量隔离了松弛相对于界面平行振荡的行为，避免了边界条件和远离不连续面区域的影响。\n\n算法步骤：\n1. 构建网格坐标 $(x_i,y_j)$（对于 $i,j=0,\\dots,N-1$）并计算 $\\theta_{i,j}$、$\\rho_{i,j}$、$r(\\theta_{i,j})$ 和 $\\varphi_{i,j}$。\n2. 根据 $\\varphi_{i,j}$ 的符号构建 $k_{i,j}$ 为 $k_{\\mathrm{in}}$ 或 $k_{\\mathrm{out}}$。\n3. 对所有内部面，通过调和平均计算面电导率；对于边界上的面，使用内部电导率。\n4. 组装五点模板系数 $a_{i,j}^{\\mathrm{E}}$、$a_{i,j}^{\\mathrm{W}}$、$a_{i,j}^{\\mathrm{N}}$、$a_{i,j}^{\\mathrm{S}}$ 和 $a_{i,j}^{\\mathrm{diag}}$。\n5. 使用 $\\sigma$ 和 $q$ 构建初始误差场 $e_0$，并确定 $|\\varphi| \\le 2\\sigma$ 的测量掩码 $\\mathcal{M}$。\n6. 执行一次松弛扫描：\n   - 对于雅可比法，仅使用旧的邻居值和权重 $\\omega_{\\mathrm{J}}$ 来计算各处的更新。\n   - 对于高斯-赛德尔法和逐次超松弛法，按自然字典序或按 $\\varphi$ 排序遍历节点；在每个节点，使用最新的可用邻居值形成高斯-赛德尔原始更新，然后应用 $\\omega_{\\mathrm{SOR}}$（对于高斯-赛德尔法为 1）进行松弛。\n7. 计算光滑因子 $S$ 作为掩码 $\\mathcal{M}$ 上离散 $\\ell^2$ 范数的比值。\n8. 对所有测试用例重复此过程。\n\n科学真实性考虑：\n- 调和平均是处理不连续系数的适当选择，以尊重通量连续性，并广泛用于偏微分方程的数值解法中。\n- 初始模式 $e_0$ 旨在探测界面切线方向上的松弛行为，并通过高斯包络在法线方向上进行局部化，使其对不连续面附近的排序敏感。\n- 逐次超松弛参数设置为 $\\omega_{\\mathrm{SOR}} = 1.9$，以展示在稳定界限内的典型过松弛行为，而雅可比法使用 $\\omega_{\\mathrm{J}} = 0.66$ 以在不失稳定的情况下进行阻尼。\n\n该程序实现所有步骤，并为每个测试用例输出五个浮点光滑因子，分别对应于雅可比法、带字典序的高斯-赛德尔法、带界面对齐排序的高斯-赛德尔法、带字典序的逐次超松弛法和带界面对齐排序的逐次超松弛法，并按照规定格式将它们聚合在一个单行的列表的列表中打印出来。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef harmonic(a, b):\n    s = a + b\n    if s == 0:\n        return 0.0\n    return 2.0 * a * b / s\n\ndef build_geometry(N, r0, eps, m):\n    h = 1.0 / (N + 1)\n    i = np.arange(N)\n    j = np.arange(N)\n    x = (i + 0.5) * h\n    y = (j + 0.5) * h\n    X, Y = np.meshgrid(x, y, indexing='ij')\n    xc, yc = 0.5, 0.5\n    theta = np.arctan2(Y - yc, X - xc)\n    rho = np.sqrt((X - xc) ** 2 + (Y - yc) ** 2)\n    r_theta = r0 + eps * np.sin(m * theta)\n    phi = rho - r_theta\n    return X, Y, theta, rho, r_theta, phi, h\n\ndef build_conductivity(phi, kin, kout):\n    k = np.where(phi  0.0, kin, kout)\n    return k\n\ndef assemble_stencil(k, h):\n    N = k.shape[0]\n    # Face conductivities with harmonic averaging; handle boundaries\n    kE = np.empty_like(k)\n    kW = np.empty_like(k)\n    kN = np.empty_like(k)\n    kS = np.empty_like(k)\n\n    # East faces\n    kE[:-1, :] = harmonic_array(k[:-1, :], k[1:, :])\n    kE[-1, :] = k[-1, :]  # boundary face uses interior value\n\n    # West faces\n    kW[1:, :] = harmonic_array(k[1:, :], k[:-1, :])\n    kW[0, :] = k[0, :]\n\n    # North faces\n    kN[:, :-1] = harmonic_array(k[:, :-1], k[:, 1:])\n    kN[:, -1] = k[:, -1]\n\n    # South faces\n    kS[:, 1:] = harmonic_array(k[:, 1:], k[:, :-1])\n    kS[:, 0] = k[:, 0]\n\n    aE = -kE / (h ** 2)\n    aW = -kW / (h ** 2)\n    aN = -kN / (h ** 2)\n    aS = -kS / (h ** 2)\n    aD = -(aE + aW + aN + aS)\n    return aD, aE, aW, aN, aS\n\ndef harmonic_array(a, b):\n    s = a + b\n    out = np.zeros_like(s)\n    mask = s  0\n    out[mask] = 2.0 * a[mask] * b[mask] / s[mask]\n    # if s == 0 (should not in our positive kin,kout), keep zero\n    return out\n\ndef initial_error(theta, phi, sigma, N):\n    q = int(min(32, N // 4))\n    e0 = np.exp(-(phi / sigma) ** 2) * np.sin(q * theta)\n    return e0\n\ndef mask_band(phi, sigma):\n    return np.abs(phi) = (2.0 * sigma)\n\ndef jacobi_sweep(u_old, aD, aE, aW, aN, aS, omegaJ):\n    # Build neighbor arrays with zero at boundaries\n    N = u_old.shape[0]\n    uR = np.zeros_like(u_old)\n    uL = np.zeros_like(u_old)\n    uU = np.zeros_like(u_old)\n    uD = np.zeros_like(u_old)\n    uR[:-1, :] = u_old[1:, :]\n    uL[1:, :] = u_old[:-1, :]\n    uU[:, :-1] = u_old[:, 1:]\n    uD[:, 1:] = u_old[:, :-1]\n    s = aE * uR + aW * uL + aN * uU + aS * uD\n    raw = -s / aD\n    u_new = (1.0 - omegaJ) * u_old + omegaJ * raw\n    return u_new\n\ndef build_order_indices(N, phi, mode='lex'):\n    if mode == 'lex':\n        # row-major: j outer, i inner\n        order = [(i, j) for j in range(N) for i in range(N)]\n    elif mode == 'phi':\n        # sort by ascending phi\n        flat_indices = [(i, j) for i in range(N) for j in range(N)]\n        flat_phi = [phi[i, j] for i, j in flat_indices]\n        order = [x for _, x in sorted(zip(flat_phi, flat_indices), key=lambda t: t[0])]\n    else:\n        raise ValueError(\"Unknown ordering mode\")\n    return order\n\ndef gs_sweep(u_old, aD, aE, aW, aN, aS, order, omega=1.0):\n    N = u_old.shape[0]\n    u_new = u_old.copy()\n    \n    for i, j in order:\n        s = 0.0\n        # East neighbor (i+1, j)\n        if i + 1  N:\n            s += aE[i, j] * u_new[i + 1, j]\n        # West neighbor (i-1, j)\n        if i - 1 = 0:\n            s += aW[i, j] * u_new[i - 1, j]\n        # North neighbor (i, j+1)\n        if j + 1  N:\n            s += aN[i, j] * u_new[i, j + 1]\n        # South neighbor (i, j-1)\n        if j - 1 = 0:\n            s += aS[i, j] * u_new[i, j - 1]\n\n        raw = -s / aD[i, j]\n        u_new[i, j] = (1.0 - omega) * u_old[i, j] + omega * raw\n    return u_new\n\ndef smoothing_factor(e0, e1, mask):\n    # compute discrete L2 norm over mask; ratio cancels h\n    sel0 = e0[mask]\n    sel1 = e1[mask]\n    norm0 = np.sqrt(np.sum(sel0 * sel0))\n    norm1 = np.sqrt(np.sum(sel1 * sel1))\n    # To avoid division by zero, if norm0 is zero, define smoothing factor as 0.0\n    if norm0 == 0.0:\n        return 0.0\n    return float(norm1 / norm0)\n\ndef run_case(N, r0, eps, m, kin, kout, sigma, omegaJ, omegaSOR):\n    # Geometry and coefficients\n    X, Y, theta, rho, r_theta, phi, h = build_geometry(N, r0, eps, m)\n    # The original problem describes a cell-centered grid, let's adjust for that.\n    # Grid coordinates should be cell centers\n    x = (np.arange(N) + 0.5) * (1.0/N) if N>1 else np.array([0.5])\n    y = (np.arange(N) + 0.5) * (1.0/N) if N>1 else np.array([0.5])\n    h = 1.0/N\n    X, Y = np.meshgrid(x, y, indexing='ij')\n    xc, yc = 0.5, 0.5\n    theta = np.arctan2(Y - yc, X - xc)\n    rho = np.sqrt((X - xc)**2 + (Y - yc)**2)\n    r_theta = r0 + eps * np.sin(m * theta)\n    phi = rho - r_theta\n\n    k = build_conductivity(phi, kin, kout)\n    \n    aD, aE, aW, aN, aS = assemble_stencil(k, h)\n\n    # Initial error and mask\n    e0 = initial_error(theta, phi, sigma, N)\n    mask = mask_band(phi, sigma)\n\n    # Jacobi is independent of order\n    e1_jacobi = jacobi_sweep(e0, aD, aE, aW, aN, aS, omegaJ)\n    S_jacobi = smoothing_factor(e0, e1_jacobi, mask)\n\n    # Redefine GS sweep to correctly handle arbitrary ordering\n    def general_gs_sweep(u_old, aD, aE, aW, aN, aS, order, omega=1.0):\n        u_new = u_old.copy()\n        for i,j in order:\n            s = 0.0\n            if i + 1  N: s += aE[i,j] * u_new[i+1,j]\n            if i - 1 = 0: s += aW[i,j] * u_new[i-1,j]\n            if j + 1  N: s += aN[i,j] * u_new[i,j+1]\n            if j - 1 = 0: s += aS[i,j] * u_new[i,j-1]\n            u_gs_val = -s/aD[i,j]\n            u_new[i,j] = (1.0 - omega) * u_old[i,j] + omega * u_gs_val\n        return u_new\n    \n    order_lex = build_order_indices(N, phi, mode='lex')\n    e1_gs_lex = general_gs_sweep(e0, aD, aE, aW, aN, aS, order_lex, omega=1.0)\n    S_gs_lex = smoothing_factor(e0, e1_gs_lex, mask)\n\n    order_phi = build_order_indices(N, phi, mode='phi')\n    e1_gs_phi = general_gs_sweep(e0, aD, aE, aW, aN, aS, order_phi, omega=1.0)\n    S_gs_phi = smoothing_factor(e0, e1_gs_phi, mask)\n\n    e1_sor_lex = general_gs_sweep(e0, aD, aE, aW, aN, aS, order_lex, omega=omegaSOR)\n    S_sor_lex = smoothing_factor(e0, e1_sor_lex, mask)\n    \n    e1_sor_phi = general_gs_sweep(e0, aD, aE, aW, aN, aS, order_phi, omega=omegaSOR)\n    S_sor_phi = smoothing_factor(e0, e1_sor_phi, mask)\n\n    return [S_jacobi, S_gs_lex, S_gs_phi, S_sor_lex, S_sor_phi]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (happy path)\n        {\"N\": 96, \"r0\": 0.35, \"eps\": 0.04, \"m\": 4, \"kin\": 10.0, \"kout\": 1.0, \"sigma\": 0.03, \"omegaJ\": 0.66, \"omegaSOR\": 1.9},\n        # Case B (high contrast)\n        {\"N\": 96, \"r0\": 0.35, \"eps\": 0.08, \"m\": 6, \"kin\": 1000.0, \"kout\": 1.0, \"sigma\": 0.03, \"omegaJ\": 0.66, \"omegaSOR\": 1.9},\n        # Case C (boundary and curvature variation)\n        {\"N\": 48, \"r0\": 0.35, \"eps\": 0.12, \"m\": 8, \"kin\": 50.0, \"kout\": 1.0, \"sigma\": 0.04, \"omegaJ\": 0.66, \"omegaSOR\": 1.9},\n    ]\n\n    results = []\n    for case in test_cases:\n        res = run_case(\n            N=case[\"N\"],\n            r0=case[\"r0\"],\n            eps=case[\"eps\"],\n            m=case[\"m\"],\n            kin=case[\"kin\"],\n            kout=case[\"kout\"],\n            sigma=case[\"sigma\"],\n            omegaJ=case[\"omegaJ\"],\n            omegaSOR=case[\"omegaSOR\"],\n        )\n        # Round results to a reasonable number of decimals for readability\n        results.append([float(f\"{x:.6f}\") for x in res])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r).replace(' ', '') for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}