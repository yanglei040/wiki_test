{
    "hands_on_practices": [
        {
            "introduction": "The Generalized Minimal Residual (GMRES) method is a powerhouse for solving the large, non-symmetric linear systems common in computational science. Its underlying principle, while elegant, can seem abstract at first. This foundational exercise  provides a concrete starting point by guiding you through a single GMRES iteration by hand. By explicitly constructing the first Krylov subspace vector and finding the optimal solution within it, you will gain a tangible understanding of how GMRES systematically minimizes the residual at each step.",
            "id": "3338495",
            "problem": "In computational fluid dynamics (CFD), implicit time integration and linearization of transport-diffusion operators lead to linear systems of the form $A x = b$ that must be solved efficiently. Krylov subspace iterative methods construct approximations by minimizing the residual in spaces generated by repeated applications of the system matrix. Consider the linear system defined by the $3 \\times 3$ matrix\n$$\nA = \\begin{pmatrix}\n4 & 1 & 0 \\\\\n0 & 3 & 1 \\\\\n0 & 0 & 2\n\\end{pmatrix}\n$$\nand the right-hand side\n$$\nb = \\begin{pmatrix}\n1 \\\\\n1 \\\\\n1\n\\end{pmatrix},\n$$\nwith the initial approximation $x_{0} = \\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix}$. Using the Generalized Minimal Residual (GMRES) method, perform exactly one iteration (that is, construct the $1$-dimensional Krylov subspace and compute the corresponding GMRES update based on the Arnoldi process with the Euclidean inner product) to obtain $x_{1}$. Then, compute the Euclidean norm of the residual $r_{1} = b - A x_{1}$.\n\nExpress the final residual norm in exact form with no rounding. No physical units are required.",
            "solution": "The problem asks to perform one iteration of the Generalized Minimal Residual (GMRES) method and find the Euclidean norm of the resulting residual, $\\|r_1\\|_2$.\n\nFirst, we compute the initial residual, $r_0$. Since the initial guess is $x_0 = \\mathbf{0}$, the initial residual is simply the right-hand side vector $b$:\n$$\nr_0 = b - A x_0 = b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nThe one-dimensional Krylov subspace, $\\mathcal{K}_1(A, r_0)$, is the space spanned by the initial residual: $\\mathcal{K}_1(A, r_0) = \\text{span}\\{r_0\\}$.\n\nGMRES seeks an updated approximation $x_1$ of the form $x_1 = x_0 + z_1$, where $z_1 \\in \\mathcal{K}_1(A, r_0)$. This means the correction $z_1$ is a scalar multiple of $r_0$, i.e., $z_1 = \\alpha r_0$ for some scalar $\\alpha \\in \\mathbb{R}$. The coefficient $\\alpha$ is chosen to minimize the Euclidean norm of the new residual, $r_1 = b - A x_1$.\nSubstituting for $x_1$ and $r_0$, we have:\n$$\nr_1 = b - A(x_0 + \\alpha r_0) = (b - A x_0) - \\alpha A r_0 = r_0 - \\alpha A r_0\n$$\nWe need to find the value of $\\alpha$ that minimizes $\\|r_0 - \\alpha A r_0\\|_2$. This is a standard linear least-squares problem: finding the best approximation of the vector $r_0$ in the subspace spanned by the vector $A r_0$. The solution is found by making the error vector, $r_1$, orthogonal to the space we are projecting onto, which is $\\text{span}\\{A r_0\\}$. This gives the orthogonality condition:\n$$\n(A r_0)^T (r_0 - \\alpha A r_0) = 0\n$$\nSolving for $\\alpha$ gives the normal equation solution:\n$$\n\\alpha = \\frac{(A r_0)^T r_0}{\\|A r_0\\|_2^2}\n$$\nLet's compute the necessary quantities. First, the vector $A r_0$:\n$$\nA r_0 = \\begin{pmatrix} 4 & 1 & 0 \\\\ 0 & 3 & 1 \\\\ 0 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 4(1)+1(1)+0(1) \\\\ 0(1)+3(1)+1(1) \\\\ 0(1)+0(1)+2(1) \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 4 \\\\ 2 \\end{pmatrix}\n$$\nNow, we calculate the numerator for $\\alpha$:\n$$\n(A r_0)^T r_0 = \\begin{pmatrix} 5 & 4 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = 5 \\cdot 1 + 4 \\cdot 1 + 2 \\cdot 1 = 11\n$$\nNext, we calculate the denominator:\n$$\n\\|A r_0\\|_2^2 = 5^2 + 4^2 + 2^2 = 25 + 16 + 4 = 45\n$$\nSo, the optimal step length is $\\alpha = \\frac{11}{45}$.\n\nWe can now compute the updated approximation $x_1$:\n$$\nx_1 = x_0 + \\alpha r_0 = \\mathbf{0} + \\frac{11}{45} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{11}{45} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nAnd the corresponding residual $r_1 = b - A x_1$:\n$$\nr_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - A \\left( \\frac{11}{45} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\right) = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{11}{45} \\begin{pmatrix} 5 \\\\ 4 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{55}{45} \\\\ 1 - \\frac{44}{45} \\\\ 1 - \\frac{22}{45} \\end{pmatrix} = \\begin{pmatrix} \\frac{-10}{45} \\\\ \\frac{1}{45} \\\\ \\frac{23}{45} \\end{pmatrix} = \\frac{1}{45} \\begin{pmatrix} -10 \\\\ 1 \\\\ 23 \\end{pmatrix}\n$$\nFinally, we compute the Euclidean norm of the residual $r_1$:\n$$\n\\|r_1\\|_2 = \\left\\| \\frac{1}{45} \\begin{pmatrix} -10 \\\\ 1 \\\\ 23 \\end{pmatrix} \\right\\|_2 = \\frac{1}{45} \\sqrt{(-10)^2 + 1^2 + 23^2} = \\frac{1}{45} \\sqrt{100 + 1 + 529} = \\frac{\\sqrt{630}}{45}\n$$\nTo simplify, we factor the term inside the square root: $630 = 9 \\times 70 = 3^2 \\times 70$.\n$$\n\\|r_1\\|_2 = \\frac{\\sqrt{3^2 \\times 70}}{45} = \\frac{3\\sqrt{70}}{45} = \\frac{\\sqrt{70}}{15}\n$$",
            "answer": "$$\n\\boxed{\\frac{\\sqrt{70}}{15}}\n$$"
        },
        {
            "introduction": "When a linear system is symmetric positive definite (SPD), as is the case for discrete diffusion or pressure-Poisson equations, the Conjugate Gradient (CG) method offers a faster and more memory-efficient alternative to GMRES. This is thanks to its clever use of short-term recurrences to build a basis of $A$-conjugate search directions. In this practice , you will apply two iterations of CG to a canonical 1D Laplacian system. This will illuminate the step-by-step process of generating these special search directions and demonstrate why CG is so effective for this important class of problems.",
            "id": "3338514",
            "problem": "In the projection methods for incompressible flow in computational fluid dynamics, the pressure field at each timestep is obtained by solving a discrete Poisson equation with homogeneous Dirichlet boundary conditions. Consider the one-dimensional model problem on the unit interval with zero Dirichlet boundary conditions discretized on a uniform grid of $n=5$ interior points, leading to a sparse symmetric positive definite linear system $A x = b$ with the standard three-point stencil. Work with the nondimensionalized discrete operator that absorbs the grid spacing factor into the right-hand side, so that the coefficient matrix $A \\in \\mathbb{R}^{5 \\times 5}$ is the tridiagonal matrix with $2$ on the diagonal and $-1$ on the first sub- and super-diagonals, that is,\n$$\nA \\;=\\; \\begin{pmatrix}\n2 & -1 & 0 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 & 0 \\\\\n0 & -1 & 2 & -1 & 0 \\\\\n0 & 0 & -1 & 2 & -1 \\\\\n0 & 0 & 0 & -1 & 2\n\\end{pmatrix}.\n$$\nAssume a localized forcing at the first interior node, so that the right-hand side is $b = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\end{pmatrix}^{T}$, and choose the initial guess $x_{0} = \\begin{pmatrix} 0 & 0 & 0 & 0 & 0 \\end{pmatrix}^{T}$.\n\nStarting from first principles for Krylov subspace iterative methods and the Conjugate Gradient (CG) method—namely, that $A$ is symmetric positive definite, the residual is $r_{k} = b - A x_{k}$, search directions are $A$-conjugate, and each step minimizes the quadratic functional $\\phi(x) = \\tfrac{1}{2} x^{T} A x - b^{T} x$ along the affine subspace—derive the first two iterations of CG applied to this system. Specifically, construct $p_{0}$, compute the step length $\\alpha_{0}$ by enforcing the line-minimization along $p_{0}$, update $x_{1}$ and $r_{1}$, derive the coefficient $\\beta_{0}$ by enforcing $A$-conjugacy of $p_{1}$ with $p_{0}$, and then compute $\\alpha_{1}$, $x_{2}$, and $r_{2}$.\n\nReport the final result as follows:\n- Provide the five components of $x_{2}$ followed by the value of $\\|r_{2}\\|_{2}$, all as exact rational values.\n- Express your final answer as a single row using parentheses, listing the entries of $x_{2}$ in order and then $\\|r_{2}\\|_{2}$ as the last entry, with no units.",
            "solution": "The task is to compute the first two iterations of the Conjugate Gradient (CG) method for the linear system $A x = b$, where the matrix $A$, the right-hand side vector $b$, and the initial guess $x_0$ are given by:\n$$ A = \\begin{pmatrix} 2 & -1 & 0 & 0 & 0 \\\\ -1 & 2 & -1 & 0 & 0 \\\\ 0 & -1 & 2 & -1 & 0 \\\\ 0 & 0 & -1 & 2 & -1 \\\\ 0 & 0 & 0 & -1 & 2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThe matrix $A$ is symmetric and positive definite, which is a prerequisite for the CG method. The CG algorithm iteratively constructs a sequence of approximate solutions $x_k$ that minimize the quadratic functional $\\phi(x) = \\frac{1}{2} x^T A x - b^T x$.\n\n**Initialization ($k=0$)**\n\nFirst, we compute the initial residual, $r_0$. The residual is defined as $r_k = b - A x_k$.\nFor $k=0$, we have:\n$$ r_0 = b - A x_0 = b - A \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThe initial search direction, $p_0$, is set equal to the initial residual:\n$$ p_0 = r_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n\n**First Iteration ($k=0$)**\n\nThe first step is to compute the optimal step length $\\alpha_0$ that minimizes $\\phi(x_0 + \\alpha_0 p_0)$. This condition is equivalent to making the new residual $r_1$ orthogonal to the search direction $p_0$, i.e., $r_1^T p_0 = 0$. Since $r_1 = r_0 - \\alpha_0 A p_0$, we have $(r_0 - \\alpha_0 A p_0)^T p_0 = 0$, which gives the formula for $\\alpha_0$:\n$$ \\alpha_0 = \\frac{r_0^T p_0}{p_0^T A p_0} $$\nSince $p_0 = r_0$, this simplifies to $\\alpha_0 = \\frac{r_0^T r_0}{p_0^T A p_0}$. First, we compute the numerator:\n$$ r_0^T r_0 = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1 $$\nNext, we compute the matrix-vector product $A p_0$:\n$$ A p_0 = \\begin{pmatrix} 2 & -1 & 0 & 0 & 0 \\\\ -1 & 2 & -1 & 0 & 0 \\\\ 0 & -1 & 2 & -1 & 0 \\\\ 0 & 0 & -1 & 2 & -1 \\\\ 0 & 0 & 0 & -1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nNow, we compute the denominator for $\\alpha_0$:\n$$ p_0^T A p_0 = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 2 $$\nThus, the step length $\\alpha_0$ is:\n$$ \\alpha_0 = \\frac{1}{2} $$\nWe can now update the solution vector $x_1$:\n$$ x_1 = x_0 + \\alpha_0 p_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nAnd the residual vector $r_1$:\n$$ r_1 = r_0 - \\alpha_0 A p_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 - 1 \\\\ 0 - (-\\frac{1}{2}) \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n\n**Second Iteration ($k=1$)**\n\nFirst, we determine the new search direction $p_1 = r_1 + \\beta_0 p_0$. The coefficient $\\beta_0$ is chosen to enforce $A$-conjugacy between $p_1$ and $p_0$, i.e., $p_1^T A p_0 = 0$.\n$$ (r_1 + \\beta_0 p_0)^T A p_0 = 0 \\implies r_1^T A p_0 + \\beta_0 p_0^T A p_0 = 0 $$\nSolving for $\\beta_0$:\n$$ \\beta_0 = - \\frac{r_1^T A p_0}{p_0^T A p_0} $$\nA standard simplified formula is $\\beta_0 = (r_1^T r_1) / (r_0^T r_0)$. Let's use this one.\n$r_1^T r_1 = (1/2)^2 = 1/4$ and $r_0^T r_0 = 1$.\n$$ \\beta_0 = \\frac{1/4}{1} = \\frac{1}{4} $$\nNow we construct the new search direction $p_1$:\n$$ p_1 = r_1 + \\beta_0 p_0 = \\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{4} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/4 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nNext, we compute the step length $\\alpha_1$:\n$$ \\alpha_1 = \\frac{r_1^T r_1}{p_1^T A p_1} $$\nThe numerator is $r_1^T r_1 = 1/4$. For the denominator, we first need $A p_1$:\n$$ A p_1 = \\begin{pmatrix} 2 & -1 & 0 & 0 & 0 \\\\ -1 & 2 & -1 & 0 & 0 \\\\ 0 & -1 & 2 & -1 & 0 \\\\ 0 & 0 & -1 & 2 & -1 \\\\ 0 & 0 & 0 & -1 & 2 \\end{pmatrix} \\begin{pmatrix} 1/4 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2(\\frac{1}{4}) - 1(\\frac{1}{2}) \\\\ -1(\\frac{1}{4}) + 2(\\frac{1}{2}) \\\\ 0 - 1(\\frac{1}{2}) \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 3/4 \\\\ -1/2 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThen, the denominator is:\n$$ p_1^T A p_1 = \\begin{pmatrix} 1/4 & 1/2 & 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3/4 \\\\ -1/2 \\\\ 0 \\\\ 0 \\end{pmatrix} = (\\frac{1}{4})(0) + (\\frac{1}{2})(\\frac{3}{4}) = \\frac{3}{8} $$\nSo the step length $\\alpha_1$ is:\n$$ \\alpha_1 = \\frac{1/4}{3/8} = \\frac{1}{4} \\cdot \\frac{8}{3} = \\frac{2}{3} $$\nWe update the solution to get $x_2$:\n$$ x_2 = x_1 + \\alpha_1 p_1 = \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{2}{3} \\begin{pmatrix} 1/4 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1/6 \\\\ 1/3 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3/6 + 1/6 \\\\ 1/3 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2/3 \\\\ 1/3 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nFinally, we compute the new residual $r_2$:\n$$ r_2 = r_1 - \\alpha_1 A p_1 = \\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\frac{2}{3} \\begin{pmatrix} 0 \\\\ 3/4 \\\\ -1/2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1/2 \\\\ -1/3 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThe problem requires the five components of $x_2$ and the Euclidean norm of $r_2$, denoted $\\|r_2\\|_2$.\nThe components of $x_2$ are $\\frac{2}{3}$, $\\frac{1}{3}$, $0$, $0$, $0$.\nThe norm of $r_2$ is:\n$$ \\|r_2\\|_2 = \\sqrt{0^2 + 0^2 + (1/3)^2 + 0^2 + 0^2} = \\sqrt{\\frac{1}{9}} = \\frac{1}{3} $$\nThe final result consists of the five components of $x_2$ and the value of $\\|r_2\\|_2$.",
            "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{2}{3} & \\frac{1}{3} & 0 & 0 & 0 & \\frac{1}{3} \\end{pmatrix}} $$"
        },
        {
            "introduction": "A classic pitfall in applying Krylov methods is assuming that the eigenvalues of a matrix tell the whole story about convergence. For nonnormal matrices, which frequently appear in models of convection-dominated phenomena, this is far from true. This advanced practice  asks you to construct a simple $2 \\times 2$ matrix for which GMRES exhibits complete stagnation in the first step, despite having perfectly benign eigenvalues. By building this pathological case, you will directly confront the impact of nonnormality and learn to connect seemingly poor convergence behavior to the underlying structure of the operator, a critical skill for any practitioner.",
            "id": "3338518",
            "problem": "Consider a semi-discrete two-cell model of a linear convection–diffusion operator arising in computational fluid dynamics, producing a linear system of the form $A x = b$ with $A \\in \\mathbb{R}^{2 \\times 2}$. In convection-dominated regimes, the discretization matrix is often strongly nonnormal due to upwind-biased coupling. Let $A$ be constructed to have eigenvalues at $1$ and $2$, with nonnormality controlled by an off-diagonal coupling parameter. You will analyze the Generalized Minimal Residual (GMRES) method to expose stagnation caused by nonnormality, and interpret the phenomenon via Jordan form or pseudospectral considerations.\n\nTasks:\n- Construct an explicit nonnormal matrix $A \\in \\mathbb{R}^{2 \\times 2}$ with eigenvalues $1$ and $2$ that models a two-cell convection–diffusion coupling. Justify that your $A$ is nonnormal using the definition that $A$ is normal if and only if $A A^{\\top} = A^{\\top} A$ in the Euclidean inner product on $\\mathbb{R}^{2}$.\n- Choose the right-hand side $b \\in \\mathbb{R}^{2}$ and initial guess $x_{0} \\in \\mathbb{R}^{2}$ so that the initial residual $r_{0} = b - A x_{0}$ is orthogonal to its image $A r_{0}$ in the Euclidean inner product. Use only the core definitions of Krylov subspaces and GMRES: the Krylov subspace of order $k$ is $\\mathcal{K}_{k}(A, r_{0}) = \\operatorname{span}\\{r_{0}, A r_{0}, \\dots, A^{k-1} r_{0}\\}$, and GMRES chooses $x_{k} \\in x_{0} + \\mathcal{K}_{k}(A, r_{0})$ to minimize $\\|b - A x_{k}\\|_{2}$.\n- Explain, based on Jordan canonical form or pseudospectral reasoning, why such nonnormality can cause GMRES residuals to stagnate in the initial iterations even though the spectrum is benign. Your explanation must connect the algebraic structure of $A$ (e.g., similarity to a diagonal Jordan form with a poorly conditioned similarity transform) or the size of the pseudospectrum to the short-term behavior of GMRES residuals.\n- For your explicit construction, compute the one-step GMRES residual norm ratio $\\rho_{1} = \\|r_{1}\\|_{2}/\\|r_{0}\\|_{2}$, where $r_{1}$ is the residual after one GMRES iteration started at $x_{0}$ in exact arithmetic. Express your final answer for $\\rho_{1}$ as a pure number without units. No rounding is required.\n\nAnswer form requirement: Your final answer must be a single real number equal to the computed value of $\\rho_{1}$, presented without units.",
            "solution": "The problem requires the construction of a nonnormal $2 \\times 2$ matrix $A$ to demonstrate a specific failure mode of the Generalized Minimal Residual (GMRES) method, namely stagnation. The analysis involves building the matrix, selecting an initial residual vector with specific properties, explaining the cause of stagnation, and computing the one-step residual norm reduction factor.\n\nFirst, we construct a matrix $A \\in \\mathbb{R}^{2 \\times 2}$ with eigenvalues $\\lambda_1 = 1$ and $\\lambda_2 = 2$. An upper triangular matrix is a natural choice for modeling convection-dominated phenomena and for explicitly setting the eigenvalues. Let the matrix be of the form:\n$$\nA = \\begin{pmatrix} 1 & \\gamma \\\\ 0 & 2 \\end{pmatrix}\n$$\nThe eigenvalues of a triangular matrix are its diagonal entries, so they are indeed $1$ and $2$. The parameter $\\gamma \\in \\mathbb{R}$ controls the nonnormality of $A$. A matrix is normal if it commutes with its transpose, i.e., $A A^{\\top} = A^{\\top} A$. We compute both products:\n$$\nA A^{\\top} = \\begin{pmatrix} 1 & \\gamma \\\\ 0 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ \\gamma & 2 \\end{pmatrix} = \\begin{pmatrix} 1 + \\gamma^2 & 2\\gamma \\\\ 2\\gamma & 4 \\end{pmatrix}\n$$\n$$\nA^{\\top} A = \\begin{pmatrix} 1 & 0 \\\\ \\gamma & 2 \\end{pmatrix} \\begin{pmatrix} 1 & \\gamma \\\\ 0 & 2 \\end{pmatrix} = \\begin{pmatrix} 1 & \\gamma \\\\ \\gamma & \\gamma^2 + 4 \\end{pmatrix}\n$$\nFor $A$ to be normal, we must have $A A^{\\top} = A^{\\top} A$, which requires $1 + \\gamma^2 = 1$, $2\\gamma = \\gamma$, and $4 = \\gamma^2+4$. All three equations imply $\\gamma = 0$. Thus, for any non-zero value of $\\gamma$, the matrix $A$ is nonnormal.\n\nNext, we must choose a right-hand side $b \\in \\mathbb{R}^2$ and an initial guess $x_0 \\in \\mathbb{R}^2$ such that the initial residual $r_0 = b - Ax_0$ is orthogonal to its image under $A$, i.e., $r_0^{\\top} (A r_0) = 0$. It is simplest to set $x_0 = 0$, which makes $r_0 = b$. We must find a non-zero vector $r_0 = \\begin{pmatrix} u \\\\ v \\end{pmatrix}$ satisfying this orthogonality condition.\nThe condition is $r_0^{\\top} A r_0 = 0$:\n$$\n\\begin{pmatrix} u & v \\end{pmatrix} \\begin{pmatrix} 1 & \\gamma \\\\ 0 & 2 \\end{pmatrix} \\begin{pmatrix} u \\\\ v \\end{pmatrix} = \\begin{pmatrix} u & v \\end{pmatrix} \\begin{pmatrix} u + \\gamma v \\\\ 2v \\end{pmatrix} = u(u + \\gamma v) + 2v^2 = u^2 + \\gamma u v + 2v^2 = 0\n$$\nTo find a non-trivial solution (where $u$ and $v$ are not both zero), we can assume $v \\neq 0$ and divide by $v^2$ to obtain a quadratic equation for the ratio $t = u/v$:\n$$\nt^2 + \\gamma t + 2 = 0\n$$\nFor this quadratic equation to have real solutions for $t$, the discriminant must be non-negative: $\\Delta = \\gamma^2 - 4(1)(2) = \\gamma^2 - 8 \\ge 0$. This imposes a constraint on our choice of $\\gamma$: we must have $|\\gamma| \\ge \\sqrt{8} = 2\\sqrt{2}$. To make the calculations simple, we choose $\\gamma=3$, which satisfies this condition. For $\\gamma=3$, the equation for $t$ becomes:\n$$\nt^2 + 3t + 2 = 0 \\implies (t+1)(t+2) = 0\n$$\nThis gives two possible ratios: $t = -1$ or $t = -2$. We choose $t = u/v = -1$. Setting $v=1$ gives $u=-1$. Thus, an appropriate initial residual is:\n$$\nr_0 = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}\n$$\nWith this choice, we set the initial guess $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and the right-hand side $b = r_0 = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$. Our specific matrix is $A = \\begin{pmatrix} 1 & 3 \\\\ 0 & 2 \\end{pmatrix}$.\n\nThe stagnation of GMRES can be explained by analyzing the first iteration. GMRES seeks a solution $x_k \\in x_0 + \\mathcal{K}_k(A, r_0)$, where $\\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{r_0, Ar_0, \\dots, A^{k-1}r_0\\}$, that minimizes the 2-norm of the residual $\\|b-Ax_k\\|_2$.\nFor the first step ($k=1$), we seek $x_1 = x_0 + c_0 r_0$ for some scalar $c_0 \\in \\mathbb{R}$ that minimizes $\\|b - A(x_0 + c_0 r_0)\\|_2$. This is equivalent to minimizing the norm of the new residual $r_1$:\n$$\n\\|r_1\\|_2 = \\|(b - A x_0) - c_0 A r_0\\|_2 = \\|r_0 - c_0 A r_0\\|_2\n$$\nThis is a linear least-squares problem for $c_0$. The value of $c_0$ that minimizes this norm is the one that makes the new residual $r_1$ orthogonal to the space the update comes from, which is $A\\mathcal{K}_1(A,r_0) = \\text{span}\\{Ar_0\\}$. Thus, we must have $(r_1)^{\\top} (A r_0) = 0$:\n$$\n(r_0 - c_0 A r_0)^{\\top} (A r_0) = 0 \\implies r_0^{\\top}(A r_0) - c_0 (A r_0)^{\\top}(A r_0) = 0\n$$\nSolving for $c_0$ yields:\n$$\nc_0 = \\frac{r_0^{\\top}(A r_0)}{\\|A r_0\\|_2^2}\n$$\nBy our explicit construction of $r_0$, the numerator $r_0^{\\top}(A r_0)$ is zero. Thus, $c_0 = 0$.\nThis means the update to the solution is zero: $x_1 = x_0 + 0 \\cdot r_0 = x_0$. Consequently, the residual does not change: $r_1 = b - A x_1 = b - A x_0 = r_0$. This constitutes complete stagnation in the first iteration.\n\nThis phenomenon is a direct consequence of the nonnormality of $A$. For a normal matrix, its numerical range (or field of values) $W(A) = \\{ v^{\\top}Av / v^{\\top}v : v \\in \\mathbb{R}^n, v \\neq 0 \\}$ is the convex hull of its eigenvalues. For our matrix $A$, the eigenvalues are $1$ and $2$, so if $A$ were normal, $W(A)$ would be the interval $[1, 2]$. In that case, $v^{\\top}Av$ could never be zero for any non-zero vector $v$. However, because $A$ is nonnormal, its numerical range is larger than the convex hull of its eigenvalues. Our construction found a vector $r_0$ for which $r_0^{\\top}Ar_0 = 0$, proving that $0 \\in W(A)$. The existence of such a direction, where the numerical range extends far from the eigenvalues (to include $0$), is what allows for the pathological behavior observed. GMRES is known to perform poorly for matrices whose numerical range contains the origin, and our choice of $r_0$ specifically directs the algorithm into this trap. A Jordan form analysis would show that the eigenvectors of $A$ are poorly conditioned (nearly parallel for large $\\gamma$), which is another manifestation of strong nonnormality that leads to transient growth of matrix powers and poor GMRES performance.\n\nFinally, we compute the one-step GMRES residual norm ratio $\\rho_1 = \\|r_1\\|_2 / \\|r_0\\|_2$. As derived above, for our constructed problem, $c_0 = 0$, which leads to $r_1 = r_0$. Therefore:\n$$\n\\rho_1 = \\frac{\\|r_1\\|_2}{\\|r_0\\|_2} = \\frac{\\|r_0\\|_2}{\\|r_0\\|_2} = 1\n$$\nThis confirms that the residual norm does not decrease at all in the first step.",
            "answer": "$$ \\boxed{1} $$"
        }
    ]
}