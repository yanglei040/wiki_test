{
    "hands_on_practices": [
        {
            "introduction": "A fundamental skill in scientific computing is the ability to implement core numerical algorithms from first principles. This exercise guides you through the construction of the Incomplete Cholesky factorization with zero fill-in, IC(0), a classic and efficient preconditioner for symmetric positive-definite systems arising from discretized PDEs like the Poisson equation. By implementing the factorization and calculating the spectral radius of the resulting iteration matrix, you will gain direct insight into how preconditioners are built and how their effectiveness is measured .",
            "id": "3434371",
            "problem": "Construct a program that, for a symmetric positive definite (SPD) sparse matrix arising from a discrete Laplacian, builds the incomplete Cholesky factorization with zero fill, denoted as IC(0), and uses it to compute the spectral radius of the preconditioned error-propagation operator. The derivation and algorithm must be grounded in fundamental definitions and well-tested facts:\n\n- Start from the definition of the two-dimensional five-point discrete Laplacian on an $N \\times N$ interior grid with homogeneous Dirichlet boundary conditions. The canonical unscaled five-point stencil at each interior node produces a matrix $A \\in \\mathbb{R}^{n \\times n}$, with $n = N^2$, that is SPD, where each interior row has diagonal entry $4$ and up to four off-diagonal entries $-1$ corresponding to the nearest neighbors.\n- Use the definition of the Cholesky factorization for SPD matrices: for SPD $A$, there exists a unique lower-triangular $L$ with positive diagonal such that $A = LL^\\top$. The incomplete Cholesky factorization IC(0) is a factorization that enforces the sparsity pattern of $L$ to match the strictly lower-triangular part of $A$ (plus the diagonal), discarding fill-in beyond that pattern.\n- Define the left preconditioner $M = LL^\\top$ obtained from IC(0). The preconditioned operator is $M^{-1}A$, and the corresponding linear stationary error-propagation operator is $E = I - M^{-1}A$, where $I$ is the identity matrix. The spectral radius is $\\rho(E) = \\max_i |\\lambda_i(E)|$, where $\\lambda_i(E)$ are the eigenvalues of $E$.\n\nYour program must:\n\n1. Construct $A$ for the two-dimensional five-point Laplacian with homogeneous Dirichlet boundary conditions on an $N \\times N$ interior grid using the standard unscaled stencil. Use lexicographic ordering for the grid nodes unless a permutation is specified.\n2. Build the IC(0) preconditioner $M = LL^\\top$, where the sparsity pattern of $L$ is the lower-triangular part of $A$ (including the diagonal). You must compute $L$ by enforcing the zero fill-in constraint: every entry $L_{ij}$ for $i \\gt j$ is computed only if $A_{ij} \\neq 0$, and summations use only indices consistent with this sparsity pattern.\n3. Compute $\\rho(I - M^{-1}A)$ by applying $M^{-1}$ to each column of $A$ using forward and backward triangular solves with $L$ and $L^\\top$.\n4. Optionally apply the Reverse Cuthill–McKee (RCM) permutation (denote the permutation by a vector $p$ and the permutation matrix by $P$) to $A$ as $A_p = P^\\top A P$ before computing IC(0). When a permutation is applied, all computations, including IC(0) and the spectral radius, must be performed on the permuted system $A_p$.\n\nNo physical units or angles are involved. All numerical answers must be returned as floating-point values. The final output of your program must be a single line containing a Python list of floating-point spectral radii in the order of the test suite below, each rounded to eight decimal places.\n\nTest Suite:\n- Case $1$: $N = 5$, no reordering.\n- Case $2$: $N = 3$, no reordering.\n- Case $3$: $N = 5$, with Reverse Cuthill–McKee reordering applied to $A$ prior to constructing IC(0).\n- Case $4$: $N = 1$, no reordering.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For example, if there were two results $r_1$ and $r_2$, the program would print exactly \"[r_1,r_2]\". For this problem, print \"[r_1,r_2,r_3,r_4]\" where $r_k$ is the spectral radius for case $k$, each formatted to eight digits after the decimal point.",
            "solution": "The problem requires the construction and analysis of an Incomplete Cholesky factorization with zero fill-in, denoted as IC($0$), for a matrix representing the two-dimensional discrete Laplacian. The goal is to compute the spectral radius of the error-propagation operator associated with the IC($0$) preconditioner for several grid configurations.\n\nThe solution proceeds in four main steps:\n1.  Construction of the discrete Laplacian matrix $A$.\n2.  Optional reordering of $A$ using the Reverse Cuthill–McKee (RCM) algorithm.\n3.  Computation of the IC($0$) factor $L$.\n4.  Calculation of the spectral radius of the iteration matrix $E = I - M^{-1}A$, where $M = LL^\\top$.\n\n### 1. Matrix Construction\nThe problem is set on an $N \\times N$ grid of interior nodes. We use a zero-based lexicographic ordering, where the node at grid coordinates $(i, j)$ for $0 \\le i, j < N$ is mapped to a single index $k = iN + j$. This results in a square matrix $A$ of size $n \\times n$, where $n = N^2$.\n\nThe matrix $A$ is derived from the canonical five-point stencil for the Laplacian operator, $-\\Delta u$. At each interior node $k$ (corresponding to grid point $(i, j)$), the discrete equation is:\n$$4u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1} = f_{i,j}$$\nHomogeneous Dirichlet boundary conditions imply that any term $u_{x,y}$ where $(x,y)$ is on the boundary is zero. This structure translates into the matrix $A$:\n-   The diagonal entries are $A_{kk} = 4$.\n-   The off-diagonal entries $A_{kl}$ are $-1$ if node $l$ is an immediate North, South, East, or West neighbor of node $k$ on the grid. Otherwise, $A_{kl} = 0$ for $k \\neq l$.\n\nThe resulting matrix $A$ is sparse, symmetric, and positive definite (SPD). It has a block tridiagonal structure.\n\n### 2. Reverse Cuthill–McKee (RCM) Reordering\nFor certain test cases, the matrix $A$ is reordered to reduce its bandwidth and profile. The Reverse Cuthill–McKee algorithm finds a permutation vector $p$ that, when applied to the matrix, tends to concentrate the non-zero elements closer to the diagonal. The permuted matrix is given by $A_p = P^\\top A P$, where $P$ is the permutation matrix corresponding to $p$. All subsequent computations, including IC($0$) and the spectral radius calculation, are then performed on the permuted matrix $A_p$.\n\n### 3. Incomplete Cholesky Factorization (IC(0))\nFor any SPD matrix $A$, the standard Cholesky factorization finds a unique lower triangular matrix $L$ with positive diagonal entries such that $A = LL^\\top$. The entries of $L$ are computed via the following recurrence relations:\n$$ L_{jj} = \\sqrt{A_{jj} - \\sum_{k=0}^{j-1} L_{jk}^2} $$\n$$ L_{ij} = \\frac{1}{L_{jj}} \\left( A_{ij} - \\sum_{k=0}^{j-1} L_{ik}L_{jk} \\right) \\quad \\text{for } i > j $$\nThis process typically introduces \"fill-in,\" where $L_{ij}$ can be non-zero even if $A_{ij}$ was zero.\n\nThe IC($0$) factorization prevents this by enforcing a sparsity constraint: the sparsity pattern of $L$ must be a subset of the sparsity pattern of $A$. Specifically, we compute an approximate factor $\\tilde{L}$ such that $\\tilde{L}_{ij} = 0$ whenever $A_{ij} = 0$ for $i > j$. The algorithm is modified as follows:\nLet $S = \\{(i,j) \\mid i>j, A_{ij} \\neq 0\\}$ be the set of index pairs corresponding to the strictly lower triangular non-zero entries of $A$.\n$$ \\tilde{L}_{jj} = \\sqrt{A_{jj} - \\sum_{k=0, (j,k) \\in S}^{j-1} \\tilde{L}_{jk}^2} $$\n$$ \\tilde{L}_{ij} = \\frac{1}{\\tilde{L}_{jj}} \\left( A_{ij} - \\sum_{k=0, (i,k) \\in S, (j,k) \\in S}^{j-1} \\tilde{L}_{ik}\\tilde{L}_{jk} \\right) \\quad \\text{for } (i,j) \\in S $$\nThis procedure is implemented by iterating through the rows and columns of the matrix and applying these formulas only for the indices present in the sparsity pattern of $A$. For an SPD matrix $A$, the IC($0$) factorization via this algorithm is guaranteed to exist and produce a lower triangular matrix $\\tilde{L}$ with positive diagonal entries. Henceforth, we denote $\\tilde{L}$ as $L$.\n\n### 4. Spectral Radius Calculation\nThe IC($0$) factorization provides a preconditioner $M = LL^\\top$. We are interested in the spectral properties of the stationary iteration matrix $E = I - M^{-1}A$. The spectral radius $\\rho(E)$ governs the convergence rate of the preconditioned Richardson iteration.\n$$ \\rho(E) = \\max_i |\\lambda_i(E)| $$\nwhere $\\lambda_i(E)$ are the eigenvalues of $E$.\n\nTo compute $\\rho(E)$, we first construct the dense matrix $E$ and then find its eigenvalues. The $j$-th column of $E$, denoted $e_j - (M^{-1}A)_j$, is computed by first calculating the vector $y_j = M^{-1}a_j$, where $a_j$ is the $j$-th column of $A$. The computation of $y_j$ is performed by solving the system $My_j = a_j$, or $LL^\\top y_j = a_j$. This is done efficiently without inverting $L$ by using forward and backward substitution:\n1.  **Forward solve:** Solve $Lz_j = a_j$ for $z_j$.\n2.  **Backward solve:** Solve $L^\\top y_j = z_j$ for $y_j$.\n\nThis process is repeated for each column $j = 0, \\dots, n-1$ to construct all columns of the matrix $M^{-1}A$. The matrix $E$ is then formed, and its eigenvalues are computed using a standard numerical library routine. The spectral radius is the maximum of the absolute values of these eigenvalues.\n\nSpecial Case: For $N=1$, the grid has a single interior point. The matrix $A$ is the $1 \\times 1$ matrix $[4]$. The IC($0$) factor is $L = [\\sqrt{4}] = [2]$. The preconditioner is $M=LL^\\top = [4]$. Thus, $M=A$, and the error propagation matrix is $E = I - M^{-1}A = I - A^{-1}A = I - I = [0]$. Its only eigenvalue is $0$, so the spectral radius is $\\rho(E) = 0$.",
            "answer": "```python\nimport numpy as np\nimport scipy.sparse\nfrom scipy.sparse.csgraph import reverse_cuthill_mckee\nfrom scipy.sparse.linalg import spsolve_triangular\n\ndef construct_laplacian(N):\n    \"\"\"\n    Constructs the 2D discrete Laplacian matrix A for an N x N grid.\n    Uses a 5-point stencil with homogeneous Dirichlet boundary conditions.\n    \"\"\"\n    if N == 0:\n        return scipy.sparse.csr_matrix((0, 0))\n    n = N * N\n    \n    # Use DOK format for easy construction of the sparse matrix\n    A = scipy.sparse.dok_matrix((n, n), dtype=np.float64)\n\n    for i in range(N):\n        for j in range(N):\n            k = i * N + j\n            A[k, k] = 4.0\n            # West neighbor\n            if j > 0:\n                A[k, k - 1] = -1.0\n            # East neighbor\n            if j  N - 1:\n                A[k, k + 1] = -1.0\n            # North neighbor\n            if i > 0:\n                A[k, k - N] = -1.0\n            # South neighbor\n            if i  N - 1:\n                A[k, k + N] = -1.0\n                \n    return A.tocsr()\n\ndef ic0(A_csr):\n    \"\"\"\n    Computes the Incomplete Cholesky factorization with zero fill-in (IC(0)).\n    Input A must be a symmetric positive definite sparse matrix in CSR format.\n    Returns the lower triangular factor L in CSR format.\n    \"\"\"\n    n = A_csr.shape[0]\n    # LIL format is efficient for incremental construction\n    L = scipy.sparse.lil_matrix((n, n), dtype=np.float64)\n    A_dok = A_csr.todok()\n\n    for i in range(n):\n        # Compute off-diagonal entries L[i,j] for j  i\n        # The sparsity pattern of L is the same as the lower triangle of A\n        # We can iterate through relevant column indices from A\n        row_indices = sorted([c for r, c in A_dok.keys() if r == i and c  i])\n\n        for j in row_indices:\n            s = 0.0\n            # Sum over common predecessors: sum(L[i,k] * L[j,k] for k  j)\n            # L.rows[i] gives a list of column indices of non-zero elements in row i\n            L_i_cols = L.rows[i]\n            L_j_cols = L.rows[j]\n            \n            # This can be slow, but is fine for the small matrices in this problem\n            common_cols = set(L_i_cols).intersection(L_j_cols)\n            for k in common_cols:\n                if k  j:\n                    s += L[i,k] * L[j,k]\n            \n            L[i,j] = (A_dok[i,j] - s) / L[j,j]\n\n        # Compute diagonal entry L[i,i]\n        s_diag = 0.0\n        # Sum squares of L[i,j] for j  i which are now computed\n        for j in L.rows[i]:\n             if j  i:\n                s_diag += L[i,j]**2\n\n        diag_val = A_dok[i,i] - s_diag\n        if diag_val = 0:\n            raise ValueError(f\"Matrix is not positive-definite enough for IC(0) at index {i}.\")\n        L[i,i] = np.sqrt(diag_val)\n\n    return L.tocsr()\n\ndef calculate_spectral_radius(A, L):\n    \"\"\"\n    Calculates the spectral radius of the error propagation matrix E = I - (LL^T)^-1 * A.\n    \"\"\"\n    n = A.shape[0]\n    if n == 0:\n        return 0.0\n\n    # The case N=1 is trivial and could be handled separately, but the general\n    # code works as well. A=[[4]], L=[[2]], M=A, M_inv_A=I, E=0, rho=0.\n    \n    L_T = L.transpose().tocsr()\n    \n    # Build the matrix E = I - M_inv_A\n    M_inv_A = np.zeros((n, n), dtype=np.float64)\n    \n    for j in range(n):\n        a_j = A[:, j].toarray()\n        \n        # Solve M * y_j = a_j => LL^T * y_j = a_j\n        # 1. Forward solve: L * z_j = a_j\n        z_j = spsolve_triangular(L, a_j, lower=True)\n        # 2. Backward solve: L^T * y_j = z_j\n        y_j = spsolve_triangular(L_T, z_j, lower=False) # upper triangular system\n        \n        M_inv_A[:, j] = y_j.flatten()\n\n    E = np.identity(n) - M_inv_A\n    eigenvalues = np.linalg.eigvals(E)\n    spectral_radius = np.max(np.abs(eigenvalues))\n    \n    return spectral_radius\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (5, False),  # Case 1\n        (3, False),  # Case 2\n        (5, True),   # Case 3\n        (1, False),  # Case 4\n    ]\n\n    results = []\n    for N, use_rcm in test_cases:\n        A = construct_laplacian(N)\n        \n        # Apply RCM permutation if specified\n        if use_rcm:\n            # RCM for a 1x1 matrix is trivial, but scipy handles it\n            if A.shape[0] > 0:\n                perm = reverse_cuthill_mckee(A, symmetric_mode=False)\n                A = A[perm, :][:, perm]\n        \n        L = ic0(A)\n        \n        rho = calculate_spectral_radius(A, L)\n        \n        results.append(f\"{rho:.8f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While powerful, incomplete factorization preconditioners are not universally stable; their performance can be highly sensitive to the ordering of the matrix equations. A small pivot element encountered during factorization can lead to numerical instability and a poor-quality preconditioner. This practice presents a scenario where a naive application of ILU(0) fails and challenges you to identify a reordering strategy based on numerical values, not just the sparsity pattern, to ensure a robust factorization . Understanding this principle is crucial for developing reliable iterative solvers for challenging problems.",
            "id": "3434357",
            "problem": "You are given a small, intentionally badly scaled example arising from a discretized partial differential equation after an unfortunate choice of variables and scaling. Consider the sparse matrix\n$$\nA \\;=\\;\n\\begin{bmatrix}\n\\varepsilon  1  0 \\\\\n1  \\varepsilon  1 \\\\\n0  1  \\varepsilon\n\\end{bmatrix},\n$$\nwith $0\\varepsilon\\ll 1$, for instance $\\varepsilon = 10^{-8}$. You intend to build an Incomplete Lower-Upper (ILU) factorization with level $0$ fill, denoted ILU($0$), to use as a preconditioner in a Krylov iteration for a linear system $A x = b$. The ILU($0$) factorization performs the same algebraic steps as Gaussian elimination but drops any fill outside the original sparsity pattern.\n\nBase your reasoning strictly on the following fundamental definitions and facts:\n- Gaussian elimination constructs a factorization $A = L U$ by successively forming pivots $u_{ii}$ via $u_{ii} = a_{ii}^{(i)}$, where $a_{ii}^{(i)}$ is the $i$-th diagonal entry of the Schur complement after eliminating rows and columns $1$ through $i-1$. In ILU($0$), the same update formulas apply, but entries that would occur outside the original sparsity pattern are dropped at each step.\n- A permutation $P A Q$ of rows and columns corresponds to reordering the elimination sequence without changing the set of numerical values, only their positions.\n- In the bipartite graph model of a sparse matrix, rows form a vertex set $R$, columns form a vertex set $C$, and each nonzero $a_{ij}$ is an edge $(i,j)$ with weight $|a_{ij}|$. A perfect matching that maximizes the product (or sum of logs) of edge weights selects one nonzero per row and per column and is used to construct permutations that place large entries on the diagonal.\n\nTask:\n- Consider ILU($0$) in the natural ordering for $A$ and assess the magnitude of the first pivot relative to $\\varepsilon$ and $1$.\n- Now suppose you are allowed to apply a permutation before forming ILU($0$). Identify a reordering strategy that transforms the first pivot from being nearly singular to being $\\mathcal{O}(1)$ and explain, in graph-theoretic terms, the mechanism by which this reordering achieves stability.\n- Choose the option that correctly specifies a permutation strategy that accomplishes this and gives a correct graph-theoretic explanation consistent with the above definitions.\n\nOptions:\nA. Apply Reverse Cuthill-McKee (RCM) to reduce bandwidth. Bandwidth reduction redistributes nonzeros closer to the diagonal and, by Gershgorin’s theorem, ensures that the ILU($0$) pivots are large, so the first pivot becomes $\\mathcal{O}(1)$.\n\nB. Apply Approximate Minimum Degree (AMD) to reduce fill-in. Reducing fill-in also increases diagonal dominance, which guarantees the first ILU($0$) pivot is $\\mathcal{O}(1)$ regardless of $\\varepsilon$.\n\nC. Apply a maximum weight bipartite matching to obtain a column permutation $Q$ that aligns the largest nonzeros in each row to the diagonal, for example mapping columns so that row $1$ uses its entry of magnitude $1$ on the diagonal. In graph terms, the matching finds a perfect matching in the bipartite row-column graph that maximizes the product of edge weights $|a_{ij}|$, producing a strong transversal; permuting to place these matched edges on the diagonal yields initial pivots of size comparable to those weights, so the first ILU($0$) pivot becomes $\\mathcal{O}(1)$.\n\nD. Scale each row of $A$ to have unit $2$-norm and keep the natural ordering. Row scaling reduces anisotropy and therefore removes near-singularity of the first ILU($0$) pivot, making it $\\mathcal{O}(1)$ without any reordering.",
            "solution": "The validity of the problem statement is first assessed.\n\n**Step 1: Extract Givens**\n- The matrix is given by `$A = \\begin{bmatrix} \\varepsilon  1  0 \\\\ 1  \\varepsilon  1 \\\\ 0  1  \\varepsilon \\end{bmatrix}`.\n- A constraint is provided on the parameter `$\\varepsilon$`: `$0  \\varepsilon \\ll 1$`, with an example of `$\\varepsilon = 10^{-8}$`.\n- The task is to build an Incomplete Lower-Upper factorization with level `$0$` fill, denoted ILU($0$), as a preconditioner.\n- The definition of ILU($0$) is provided: It uses Gaussian elimination formulas but drops any fill outside the original sparsity pattern.\n- The effect of a permutation `$PAQ$` is described as reordering the elimination sequence.\n- The bipartite graph model for a sparse matrix is defined: rows and columns are vertex sets, and each nonzero `$a_{ij}$` is an edge with weight `$|a_{ij}|`$.\n- The concept of a maximum weight bipartite matching is introduced as a method to select permutations that place large entries on the diagonal.\n- The task requires analyzing the first ILU($0$) pivot for the natural ordering, identifying a reordering strategy to make this pivot `$\\mathcal{O}(1)$`, explaining the mechanism in graph-theoretic terms, and selecting the option that correctly describes this.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly rooted in the field of numerical linear algebra, specifically the preconditioning of sparse linear systems arising from PDEs. The matrix `$A$` is a classic example used to illustrate the need for certain pivoting strategies in ILU factorizations. All concepts (ILU, permutations, bipartite matching) are standard and correctly defined.\n- **Well-Posed:** The problem provides a specific matrix and asks for an analysis of a standard numerical procedure (ILU($0$)) and potential improvements through reordering. The questions are clear and lead to a unique, verifiable answer.\n- **Objective:** The language is technical, precise, and free of any subjective or ambiguous terminology.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is a well-posed, scientifically sound problem in numerical analysis. The solution process may now proceed.\n\n**Derivation of the Solution**\n\nFirst, we analyze the ILU($0$) factorization of the matrix `$A$` in its natural ordering.\nThe matrix is `$A = \\begin{bmatrix} \\varepsilon  1  0 \\\\ 1  \\varepsilon  1 \\\\ 0  1  \\varepsilon \\end{bmatrix}`.\nThe sparsity pattern of `$A$` is `$\\mathcal{S} = \\{(1,1), (1,2), (2,1), (2,2), (2,3), (3,2), (3,3)\\}`.\nThe ILU($0$) factorization `$M = \\tilde{L}\\tilde{U}$` must have its non-zero entries within this same pattern `$\\mathcal{S}$`.\nWe seek `$\\tilde{L} = \\begin{bmatrix} 1  0  0 \\\\ \\tilde{l}_{21}  1  0 \\\\ 0  \\tilde{l}_{32}  1 \\end{bmatrix}` and `$\\tilde{U} = \\begin{bmatrix} \\tilde{u}_{11}  \\tilde{u}_{12}  0 \\\\ 0  \\tilde{u}_{22}  \\tilde{u}_{23} \\\\ 0  0  \\tilde{u}_{33} \\end{bmatrix}` such that `$\\tilde{L}\\tilde{U}$` matches `$A$` on the pattern `$\\mathcal{S}$`.\n\nThe factorization proceeds as follows (mimicking Gaussian elimination):\n1.  **First pivot and row:**\n    The first row of `$\\tilde{U}$` is identical to the first row of `$A$`.\n    `$\\tilde{u}_{11} = a_{11} = \\varepsilon$`.\n    `$\\tilde{u}_{12} = a_{12} = 1$`.\n    The first pivot is `$\\tilde{u}_{11} = \\varepsilon$`. Given `$0  \\varepsilon \\ll 1$`, this pivot is very small, close to zero, which indicates numerical instability.\n\n2.  **Elimination using the first pivot:**\n    The multiplier for row `$2$` is `$\\tilde{l}_{21} = a_{21} / \\tilde{u}_{11} = 1 / \\varepsilon$`. This multiplier is very large.\n    The update for the diagonal element `$a_{22}$` is `$a_{22}^{(2)} = a_{22} - \\tilde{l}_{21} \\tilde{u}_{12} = \\varepsilon - (1/\\varepsilon) \\cdot 1 = \\varepsilon - 1/\\varepsilon$`.\n    So, the second pivot is `$\\tilde{u}_{22} = \\varepsilon - 1/\\varepsilon \\approx -1/\\varepsilon$`.\n\nThe analysis confirms that in the natural ordering, the first ILU($0$) pivot is `$\\tilde{u}_{11} = \\varepsilon$`, which is nearly singular. This can lead to a very poor preconditioner and numerical issues due to the large multiplier `$\\tilde{l}_{21}$`.\n\nNext, we consider reordering strategies to improve stability. The goal is to modify the matrix such that the first pivot is `$\\mathcal{O}(1)$`. This can be achieved by placing one of the large `$1$` entries at the `$(1,1)`` position. The entries `$a_{12}=1$`, `$a_{21}=1$`, `$a_{23}=1$`, `$a_{32}=1$` are candidates.\n\nA systematic way to do this is to find a permutation that places large-magnitude entries on the diagonal. This is precisely the purpose of finding a maximum weight bipartite matching as described in the problem statement. The bipartite graph of `$A$` has row vertices `$\\{r_1, r_2, r_3\\}$` and column vertices `$\\{c_1, c_2, c_3\\}$`. Edges `$(r_i, c_j)$` exist for each `$a_{ij} \\neq 0$`, with weight `$|a_{ij}|$`.\n\nA perfect matching corresponds to a permutation `$\\sigma$` such that `$a_{i, \\sigma(i)} \\neq 0$` for all `$i$`. We seek the matching (permutation) that maximizes `$\\prod_{i} |a_{i, \\sigma(i)}|$`.\n- Identity permutation `$\\sigma=(1,2,3)`: product is `$|a_{11}a_{22}a_{33}| = \\varepsilon^3$`.\n- Permutation `$\\sigma=(2,1,3)`: product is `$|a_{12}a_{21}a_{33}| = 1 \\cdot 1 \\cdot \\varepsilon = \\varepsilon$`.\n- Permutation `$\\sigma=(1,3,2)`: product is `$|a_{11}a_{23}a_{32}| = \\varepsilon \\cdot 1 \\cdot 1 = \\varepsilon$`.\nOther permutations result in a product of `$0$` because they include a zero entry of `$A$`. The maximum product is `$\\varepsilon$`, achieved by two matchings.\n\nLet's choose the matching corresponding to `$\\sigma=(2,1,3)$`. This identifies the entries `$a_{12}=1$`, `$a_{21}=1$`, and `$a_{33}=\\varepsilon$` as the \"strong transversal\". We can apply a column permutation `$Q$` to place these elements on the diagonal of a new matrix `$A' = AQ$`. Let `$Q$` be the permutation matrix that swaps columns `$1$` and `$2$`.\n`$Q = \\begin{bmatrix} 0  1  0 \\\\ 1  0  0 \\\\ 0  0  1 \\end{bmatrix}$`.\nThe permuted matrix is `$A' = AQ = \\begin{bmatrix} \\varepsilon  1  0 \\\\ 1  \\varepsilon  1 \\\\ 0  1  \\varepsilon \\end{bmatrix} \\begin{bmatrix} 0  1  0 \\\\ 1  0  0 \\\\ 0  0  1 \\end{bmatrix} = \\begin{bmatrix} 1  \\varepsilon  0 \\\\ \\varepsilon  1  1 \\\\ 1  0  \\varepsilon \\end{bmatrix}$`.\nThe new diagonal entries are `$a'_{11}=1$`, `$a'_{22}=1$`, `$a'_{33}=\\varepsilon$`.\n\nNow, we perform ILU($0$) on `$A'$`. The first pivot is immediately seen to be `$\\tilde{u}_{11} = a'_{11} = 1$`. This is `$\\mathcal{O}(1)` and no longer nearly singular.\nThe graph-theoretic mechanism is that the maximum weight matching identifies large off-diagonal entries, and the permutation places them on the diagonal, ensuring that the initial pivots in the factorization are large, which is crucial for numerical stability.\n\n**Evaluation of Options**\n\n**A. Apply Reverse Cuthill-McKee (RCM) to reduce bandwidth. Bandwidth reduction redistributes nonzeros closer to the diagonal and, by Gershgorin’s theorem, ensures that the ILU($0$) pivots are large, so the first pivot becomes `$\\mathcal{O}(1)`$.**\nRCM is a symmetric reordering ($PAP^\\top$) based on the graph structure, not on the matrix values. For the given matrix, whose graph is a simple path `1-2-3`, RCM ordering is either `(1,2,3)` or `(3,2,1)`. In either case, the permuted matrix is identical to the original matrix `$A$`. Thus, RCM has no effect, and the first pivot remains `$\\varepsilon$`. Furthermore, the link between bandwidth reduction, Gershgorin's theorem, and ILU pivot magnitudes is tenuous and not a guarantee of stability. Gershgorin's theorem bounds eigenvalues, not LU pivots.\n**Verdict: Incorrect.**\n\n**B. Apply Approximate Minimum Degree (AMD) to reduce fill-in. Reducing fill-in also increases diagonal dominance, which guarantees the first ILU($0)$ pivot is `$\\mathcal{O}(1)` regardless of `$\\varepsilon$`.**\nAMD, like RCM, is a symmetric reordering based only on the sparsity pattern to minimize fill-in. For this small `$3 \\times 3$` matrix, the minimum degree nodes are `$1$` and `$3$`. An AMD ordering would be `(1,2,3)` or `(3,2,1)`, leading to the same outcome as RCM: the permuted matrix is unchanged, and the first pivot remains `$\\varepsilon$`. AMD is oblivious to the numerical values and cannot ensure large diagonal entries. The claim that reducing fill-in increases diagonal dominance is false in general.\n**Verdict: Incorrect.**\n\n**C. Apply a maximum weight bipartite matching to obtain a column permutation $Q$ that aligns the largest nonzeros in each row to the diagonal, for example mapping columns so that row $1$ uses its entry of magnitude $1$ on the diagonal. In graph terms, the matching finds a perfect matching in the bipartite row-column graph that maximizes the product of edge weights $|a_{ij}|$, producing a strong transversal; permuting to place these matched edges on the diagonal yields initial pivots of size comparable to those weights, so the first ILU($0)$ pivot becomes `$\\mathcal{O}(1)$`.**\nThis option correctly identifies the appropriate technique. As demonstrated in the derivation, finding the maximum weight bipartite matching is a standard and effective method for identifying large off-diagonal entries (a \"strong transversal\"). Permuting the matrix (e.g., via a column permutation `$Q$`) to place these entries on the diagonal directly addresses the problem of small pivots. The first entry of the new diagonal becomes `$a'_{11} = a_{12} = 1$`, which is `$\\mathcal{O}(1)$`. The explanation provided is precise and correct according to the principles of numerical linear algebra.\n**Verdict: Correct.**\n\n**D. Scale each row of $A$ to have unit $2$-norm and keep the natural ordering. Row scaling reduces anisotropy and therefore removes near-singularity of the first ILU($0)$ pivot, making it `$\\mathcal{O}(1)` without any reordering.**\nLet's perform the scaling. The norm of the first row is `$\\sqrt{\\varepsilon^2 + 1^2} \\approx 1$`. Dividing the first row by this norm results in a new first row `$[\\varepsilon', 1', 0]$` where `$\\varepsilon' = \\varepsilon/\\sqrt{1+\\varepsilon^2} \\approx \\varepsilon$`. When performing ILU($0$) on this scaled matrix in natural order, the first pivot is the `$(1,1)$` entry, which is `$\\varepsilon' \\approx \\varepsilon$`. The pivot remains small. Scaling alone does not move the large off-diagonal entry into the pivot position.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "Moving beyond simple positive-definite systems, many critical applications in science and engineering, such as computational fluid dynamics, involve indefinite saddle-point problems. This exercise explores the design of block triangular preconditioners for the incompressible Stokes equations, a canonical example of such a system. You will construct and analyze preconditioners by forming approximations for the main operator blocks and the associated Schur complement, providing a practical introduction to the sophisticated strategies required for these complex, multi-physics systems .",
            "id": "3434325",
            "problem": "Consider the steady two-dimensional incompressible Stokes equations discretized on the unit square using a Marker-And-Cell (MAC) grid with homogeneous Dirichlet boundary conditions for velocity and mean-zero pressure. Let the resulting discrete linear algebraic system be the saddle-point matrix $$A = \\begin{pmatrix} K  B^{\\top} \\\\ B  0 \\end{pmatrix},$$ where $K \\in \\mathbb{R}^{n_u \\times n_u}$ is the symmetric positive definite discrete vector Laplacian for velocity, $B \\in \\mathbb{R}^{n_p \\times n_u}$ is the discrete divergence operator mapping velocity degrees of freedom to pressure degrees of freedom, and the $0$ block corresponds to the incompressibility constraint. The Schur complement for the pressure block is $$S = B K^{-1} B^{\\top}.$$\n\nA block triangular preconditioner is defined by $$M = \\begin{pmatrix} \\hat{K}  0 \\\\ B  -\\hat{S} \\end{pmatrix},$$ where $\\hat{K}$ and $\\hat{S}$ are computationally cheaper approximations of $K$ and $S$, respectively. The preconditioned matrix is then $$M^{-1} A.$$\n\nYour task is to implement a self-contained program that:\n- Constructs the discrete MAC-grid Stokes matrices $K$ and $B$ for a given number of pressure cells $N_x$ by $N_y$, with uniform grid spacings $h_x = 1/N_x$ and $h_y = 1/N_y$, viscosity $\\mu  0$, homogeneous Dirichlet boundary conditions for velocity, and mean-zero pressure enforced by removing a single pressure degree of freedom.\n- Forms the exact Schur complement $S = B K^{-1} B^{\\top}$ using direct solves with $K$.\n- Constructs the block triangular preconditioner $M$ with specified choices of $\\hat{K}$ and $\\hat{S}$, where the options to compare are:\n  - For $\\hat{K}$: use the exact $K$ or the Jacobi approximation $\\operatorname{diag}(K)$.\n  - For $\\hat{S}$: use the exact $S$, the diagonal $\\operatorname{diag}(S)$, or an isotropic scaling $\\alpha I$ with $\\alpha = \\frac{1}{n_p} \\sum_{i=1}^{n_p} S_{ii}$.\n- Forms the dense matrix representation of $M^{-1} A$ by applying $M^{-1}$ to the columns of $A$ or by using block algebra in terms of $K$, $B$, $\\hat{K}^{-1}$, and $\\hat{S}^{-1}$.\n- Computes the full spectrum (all eigenvalues) of $M^{-1} A$ and reports, for each test case, two quantities:\n  1. The maximum absolute deviation of the eigenvalues from $1$, defined as $\\max_i \\lvert \\lambda_i - 1 \\rvert$, where $\\{\\lambda_i\\}$ are the eigenvalues of $M^{-1} A$.\n  2. A boolean indicating whether all eigenvalues satisfy $\\lvert \\lambda_i - 1 \\rvert \\leq 10^{-8}$.\n\nStart from fundamental principles: define the MAC-grid unknowns, the discrete vector Laplacian with homogeneous Dirichlet boundary conditions, and the discrete divergence. Explain how the Schur complement arises from block elimination and why the block triangular preconditioner is a natural choice. Derive the structure of $M^{-1} A$ when the exact Schur complement and exact velocity block are used, and use this to reason about the expected spectral clustering near $1$ with approximate choices.\n\nImplementation details to follow:\n- Let the pressure degrees of freedom be the $N_x N_y$ cell centers with one degree of freedom removed to enforce mean-zero pressure.\n- Let the horizontal velocity degrees of freedom be the internal vertical faces (indices $i=1,\\dots,N_x-1$, $j=1,\\dots,N_y$), and the vertical velocity degrees of freedom be the internal horizontal faces (indices $i=1,\\dots,N_x$, $j=1,\\dots,N_y-1$). The discrete vector Laplacian $K$ is block diagonal with five-point stencils on these two velocity grids, scaled by viscosity $\\mu$.\n- The discrete divergence $B$ acts on velocity by finite differences: for each pressure cell $(i,j)$, $$\\left(B \\begin{bmatrix} \\mathbf{u} \\\\ \\mathbf{v} \\end{bmatrix}\\right)_{ij} = \\frac{u_{i,j} - u_{i-1,j}}{h_x} + \\frac{v_{i,j} - v_{i,j-1}}{h_y},$$ where indices that fall on physical boundaries correspond to zero Dirichlet velocity values and are omitted from the unknowns.\n\nTest suite and output specification:\n- Use the following five test cases to compare $\\hat{K}$ and $\\hat{S}$ choices via the spectra of $M^{-1} A$:\n  1. $N_x = 3$, $N_y = 3$, $\\mu = 1.0$, $\\hat{K} = K$, $\\hat{S} = S$.\n  2. $N_x = 3$, $N_y = 3$, $\\mu = 1.0$, $\\hat{K} = \\operatorname{diag}(K)$, $\\hat{S} = S$.\n  3. $N_x = 5$, $N_y = 4$, $\\mu = 1.0$, $\\hat{K} = K$, $\\hat{S} = \\operatorname{diag}(S)$.\n  4. $N_x = 5$, $N_y = 4$, $\\mu = 1.0$, $\\hat{K} = \\operatorname{diag}(K)$, $\\hat{S} = \\alpha I$ with $\\alpha$ equal to the mean of the diagonal entries of $S$.\n  5. $N_x = 4$, $N_y = 4$, $\\mu = 0.1$, $\\hat{K} = K$, $\\hat{S} = \\operatorname{diag}(S)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list of the form $[\\text{max\\_dev}, \\text{all\\_close}]$. For example, the output should look like $[[0.0,\\text{True}],[0.1234,\\text{False}],\\dots]$. No physical units are required because the outputs are dimensionless spectral quantities. The angle unit is not applicable. Percentages are not applicable.",
            "solution": "The problem requires an analysis of a block triangular preconditioner for the discrete incompressible Stokes equations. We begin by validating the problem statement and then proceed to a detailed theoretical derivation and implementation strategy.\n\n### Problem Validation\n\nThe problem is assessed against the specified validation criteria.\n\n**1. Extracted Givens:**\n- **Equation:** Steady two-dimensional incompressible Stokes equations on the unit square.\n- **Discretization:** Marker-And-Cell (MAC) grid with uniform spacings $h_x = 1/N_x$ and $h_y = 1/N_y$.\n- **Boundary Conditions:** Homogeneous Dirichlet for velocity.\n- **Pressure Constraint:** Mean-zero pressure, enforced by removing one pressure degree of freedom.\n- **Algebraic System:** A saddle-point matrix $A = \\begin{pmatrix} K  B^{\\top} \\\\ B  0 \\end{pmatrix}$, where $K$ is the symmetric positive definite discrete vector Laplacian, and $B$ is the discrete divergence operator.\n- **Schur Complement:** $S = B K^{-1} B^{\\top}$.\n- **Preconditioner:** A block triangular matrix $M = \\begin{pmatrix} \\hat{K}  0 \\\\ B  -\\hat{S} \\end{pmatrix}$, with $\\hat{K}$ and $\\hat{S}$ being approximations to $K$ and $S$.\n- **Unknowns Configuration:**\n    - Pressure ($p$): $N_x N_y$ cell centers, with one degree of freedom (DOF) removed ($n_p = N_x N_y - 1$).\n    - Horizontal velocity ($u$): Internal vertical faces, $n_{u,x} = (N_x-1)N_y$ DOFs.\n    - Vertical velocity ($v$): Internal horizontal faces, $n_{u,y} = N_x(N_y-1)$ DOFs.\n    - Total velocity DOFs: $n_u = n_{u,x} + n_{u,y}$.\n- **Operator Definitions:**\n    - $K$: Block diagonal matrix composed of five-point stencils for the $u$ and $v$ grids, scaled by viscosity $\\mu  0$.\n    - $B$: Defined by the finite difference formula $\\left(B [\\mathbf{u};\\mathbf{v}]\\right)_{ij} = \\frac{u_{i,j} - u_{i-1,j}}{h_x} + \\frac{v_{i,j} - v_{i,j-1}}{h_y}$.\n- **Approximation Choices:**\n    - $\\hat{K}$: The exact block $K$ or its diagonal, $\\operatorname{diag}(K)$.\n    - $\\hat{S}$: The exact Schur complement $S$, its diagonal $\\operatorname{diag}(S)$, or an isotropic scaling $\\alpha I$ where $\\alpha$ is the mean of the diagonal entries of $S$.\n- **Task:** For several test cases, construct the matrices, form the preconditioned matrix $M^{-1} A$, compute its spectrum $\\{\\lambda_i\\}$, and report $\\max_i \\lvert \\lambda_i - 1 \\rvert$ and whether all eigenvalues satisfy $\\lvert \\lambda_i - 1 \\rvert \\leq 10^{-8}$.\n\n**2. Validation Verdict:**\n- The problem is **scientifically grounded**, dealing with standard, well-established methods in numerical analysis and computational fluid dynamics.\n- It is **well-posed**, providing a clear and complete specification for constructing the matrices and performing the analysis. The sizes of the test cases are computationally feasible. The enforcement of the mean-zero pressure constraint by removing one DOF is a standard technique to ensure the invertibility of the pressure-related operator.\n- It is **objective**, stated in precise mathematical and algorithmic language.\n- The setup is internally **consistent and complete**. All necessary parameters and definitions are provided.\n\nThe problem is deemed **VALID**. We proceed with the solution.\n\n### Principle-Based Solution\n\nThe steady, incompressible Stokes equations model slow, viscous fluid flow and are given by:\n$$ -\\mu \\Delta \\mathbf{u} + \\nabla p = \\mathbf{f} $$\n$$ \\nabla \\cdot \\mathbf{u} = 0 $$\nwhere $\\mathbf{u}$ is the velocity, $p$ is the pressure, $\\mu$ is the dynamic viscosity, and $\\mathbf{f}$ is a body force (assumed to be zero here).\n\n**1. Discretization on a MAC Grid**\nA staggered grid (MAC grid) is used for discretization. On a grid of $N_x \\times N_y$ cells over the unit square $[0,1] \\times [0,1]$:\n- Pressure $p_{i,j}$ is defined at cell centers $( (i-1/2)h_x, (j-1/2)h_y )$ for $i=1,\\dots,N_x, j=1,\\dots,N_y$.\n- Horizontal velocity $u_{i,j}$ is at vertical faces $( ih_x, (j-1/2)h_y )$ for $i=1,\\dots,N_x-1, j=1,\\dots,N_y$.\n- Vertical velocity $v_{i,j}$ is at horizontal faces $( (i-1/2)h_x, jh_y )$ for $i=1,\\dots,N_x, j=1,\\dots,N_y-1$.\n\nHomogeneous Dirichlet boundary conditions $\\mathbf{u}|_{\\partial\\Omega} = \\mathbf{0}$ are imposed. The selection of DOFs on *internal* faces naturally imposes $u=0$ on vertical boundaries and $v=0$ on horizontal boundaries. The conditions $u=0$ on horizontal boundaries and $v=0$ on vertical boundaries are incorporated into the discrete operators.\n\nThe resulting discrete system is a saddle-point linear system $A \\begin{pmatrix} \\mathbf{x_u} \\\\ \\mathbf{x_p} \\end{pmatrix} = \\begin{pmatrix} \\mathbf{f_d} \\\\ \\mathbf{0} \\end{pmatrix}$, where $\\mathbf{x_u}$ and $\\mathbf{x_p}$ are vectors of the velocity and pressure DOFs, respectively. The system matrix is $A = \\begin{pmatrix} K  B^{\\top} \\\\ B  0 \\end{pmatrix}$.\n\n**2. Construction of Discrete Operators**\n\n- **Discrete Vector Laplacian $K$**: The matrix $K$ represents the action of the negative vector Laplacian operator, $-\\mu\\Delta$, on the velocity vector. It is block diagonal: $K = \\mu \\begin{pmatrix} K_u  0 \\\\ 0  K_v \\end{pmatrix}$.\n  - $K_u$ is the discrete Laplacian for the $u$-velocity grid of size $(N_x-1) \\times N_y$.\n  - $K_v$ is the discrete Laplacian for the $v$-velocity grid of size $N_x \\times (N_y-1)$.\n  Using a standard centered finite difference five-point stencil for the negative Laplacian on a Cartesian grid of size $M \\times N$ with homogeneous Dirichlet boundary conditions, the discrete operator can be constructed using Kronecker products: $L_{M \\times N} = I_N \\otimes \\frac{1}{h_x^2} L_M + \\frac{1}{h_y^2} L_N \\otimes I_M$, where $L_k$ is the $k \\times k$ 1D Laplacian matrix $[2, -1, \\dots, -1]$.\n  - $K_u = \\mu(I_{N_y} \\otimes \\frac{1}{h_x^2}L_{N_x-1} + \\frac{1}{h_y^2} L_{N_y} \\otimes I_{N_x-1})$\n  - $K_v = \\mu(\\frac{1}{h_x^2}L_{N_x} \\otimes I_{N_y-1} + I_{N_x} \\otimes \\frac{1}{h_y^2}L_{N_y-1})$\n  This construction ensures $K$ is symmetric and positive definite (SPD).\n\n- **Discrete Divergence $B$**: The matrix $B$ represents the divergence operator, mapping the velocity vector to the cell-centered pressure grid. A row of $B$ corresponding to pressure cell $(i,j)$ is constructed from the formula:\n  $$ (\\nabla \\cdot \\mathbf{u})_{i,j} \\approx \\frac{u_{i,j} - u_{i-1,j}}{h_x} + \\frac{v_{i,j} - v_{i,j-1}}{h_y} $$\n  Velocity terms with indices falling on the physical boundary $\\partial\\Omega$ are zero and omitted.\n  The pressure is only determined up to a constant, which manifests as a null vector (the constant vector) for $B^\\top$. To obtain a unique pressure solution, a constraint such as mean-zero pressure is required. This is achieved by removing one pressure DOF (e.g., the last one), which corresponds to removing one row from $B$ and one column from $B^\\top$. The resulting $B$ has $n_p = N_x N_y - 1$ rows.\n\n**3. Block Preconditioning**\n\nSolving the system with $A$ directly is challenging due to its saddle-point structure. Block elimination gives rise to the Schur complement $S = B K^{-1} B^\\top$, which is SPD and dense. Solving the system can be recast as:\n1. Solve $S \\mathbf{x_p} = B K^{-1} \\mathbf{f_d}$.\n2. Solve $K \\mathbf{x_u} = \\mathbf{f_d} - B^\\top \\mathbf{x_p}$.\n\nThe main computational cost lies in dealing with $K^{-1}$. Preconditioning aims to replace this expensive operation with a cheaper one. The block triangular preconditioner $M = \\begin{pmatrix} \\hat{K}  0 \\\\ B  -\\hat{S} \\end{pmatrix}$ is motivated by an approximate block-LU decomposition of $A$:\n$$ A = \\begin{pmatrix} K  B^\\top \\\\ B  0 \\end{pmatrix} = \\begin{pmatrix} I  0 \\\\ B K^{-1}  I \\end{pmatrix} \\begin{pmatrix} K  B^\\top \\\\ 0  -S \\end{pmatrix} $$\n$M$ approximates this factorization by replacing $K$ with $\\hat{K}$ and $S$ with $\\hat{S}$. The inverse of $M$ is readily computable:\n$$ M^{-1} = \\begin{pmatrix} \\hat{K}^{-1}  0 \\\\ \\hat{S}^{-1} B \\hat{K}^{-1}  -\\hat{S}^{-1} \\end{pmatrix} $$\nThe preconditioned matrix is $M^{-1} A$:\n$$ M^{-1}A = \\begin{pmatrix} \\hat{K}^{-1} K  \\hat{K}^{-1} B^\\top \\\\ \\hat{S}^{-1} B (\\hat{K}^{-1} K - I)  \\hat{S}^{-1} B \\hat{K}^{-1} B^\\top \\end{pmatrix} $$\n\n**4. Spectral Analysis of the Preconditioned Matrix**\n\nThe effectiveness of a preconditioner is determined by the spectral properties of the preconditioned matrix $M^{-1}A$. An ideal preconditioner would result in $M^{-1}A = I$, whose eigenvalues are all $1$.\n\n- **Ideal Case:** If we choose $\\hat{K} = K$ and $\\hat{S} = S = B K^{-1} B^\\top$, the preconditioned matrix simplifies:\n$$ M^{-1}A = \\begin{pmatrix} I  K^{-1} B^\\top \\\\ S^{-1} B (I - I)  S^{-1} (B K^{-1} B^\\top) \\end{pmatrix} = \\begin{pmatrix} I  K^{-1} B^\\top \\\\ 0  I \\end{pmatrix} $$\nThis is an upper triangular matrix with all diagonal entries equal to $1$. Therefore, all its eigenvalues are $1$. Iterative solvers like GMRES would converge in one iteration (in exact arithmetic).\n\n- **Approximate Cases:** When $\\hat{K}$ and $\\hat{S}$ are approximations, the eigenvalues of $M^{-1}A$ will deviate from $1$.\n  - If $\\hat{K} = K$ but $\\hat{S} \\approx S$ (e.g., $\\hat{S} = \\operatorname{diag}(S)$), the matrix becomes $\\begin{pmatrix} I  K^{-1} B^\\top \\\\ 0  \\hat{S}^{-1} S \\end{pmatrix}$. The eigenvalues are $1$ (from the velocity block) and the eigenvalues of $\\hat{S}^{-1}S$ (from the pressure block). The clustering around $1$ depends on how well $\\hat{S}$ approximates $S$.\n  - If $\\hat{K} \\approx K$ (e.g., $\\hat{K} = \\operatorname{diag}(K)$), the $(2,1)$ block $\\hat{S}^{-1} B (\\hat{K}^{-1} K - I)$ becomes non-zero, and the eigenvalues are more complex. The deviation from $1$ will depend on the errors from both approximations, i.e., how close $\\hat{K}^{-1}K$ and $\\hat{S}^{-1}(B \\hat{K}^{-1} B^\\top)$ are to identity matrices.\n\nThe task is to compute these spectra for the given test cases and quantify the deviation from the ideal value of $1$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import block_diag\n\ndef laplacian_1d(n):\n    \"\"\"Constructs a 1D Laplacian matrix of size n x n with Dirichlet boundary conditions.\"\"\"\n    L = 2 * np.eye(n)\n    if n > 1:\n        L += -1 * np.eye(n, k=1)\n        L += -1 * np.eye(n, k=-1)\n    return L\n\ndef build_stokes_matrices(Nx, Ny, mu):\n    \"\"\"\n    Constructs the discrete Stokes matrices K and B for a given MAC grid.\n    \"\"\"\n    hx = 1.0 / Nx\n    hy = 1.0 / Ny\n\n    # --- Velocity DOFs ---\n    n_ux = (Nx - 1) * Ny\n    n_uy = Nx * (Ny - 1)\n    n_u = n_ux + n_uy\n\n    # --- Construct K matrix (discrete vector Laplacian) ---\n    # K_u for horizontal velocities\n    Lx_u = laplacian_1d(Nx - 1) / (hx * hx)\n    Ly_u = laplacian_1d(Ny) / (hy * hy)\n    Ku = mu * (np.kron(np.eye(Ny), Lx_u) + np.kron(Ly_u, np.eye(Nx - 1)))\n    \n    # K_v for vertical velocities\n    Lx_v = laplacian_1d(Nx) / (hx * hx)\n    Ly_v = laplacian_1d(Ny - 1) / (hy * hy)\n    Kv = mu * (np.kron(np.eye(Ny - 1), Lx_v) + np.kron(Ly_v, np.eye(Nx)))\n\n    K = block_diag(Ku, Kv)\n\n    # --- Construct B matrix (discrete divergence) ---\n    n_p_full = Nx * Ny\n    B_full = np.zeros((n_p_full, n_u))\n\n    # Mapping from 2D grid indices to 1D vector indices\n    u_idx = lambda i, j: (j - 1) * (Nx - 1) + (i - 1)\n    v_idx = lambda i, j: (j - 1) * Nx + (i - 1)\n    \n    for j_p in range(1, Ny + 1):\n        for i_p in range(1, Nx + 1):\n            p_row_idx = (j_p - 1) * Nx + (i_p - 1)\n\n            # Contribution from (u_{i,j} - u_{i-1,j}) / hx\n            if i_p = Nx - 1:  # u(i_p, j_p) is a DOF\n                col_idx = u_idx(i_p, j_p)\n                B_full[p_row_idx, col_idx] = 1.0 / hx\n            if i_p >= 2:  # u(i_p-1, j_p) is a DOF\n                col_idx = u_idx(i_p - 1, j_p)\n                B_full[p_row_idx, col_idx] = -1.0 / hx\n\n            # Contribution from (v_{i,j} - v_{i,j-1}) / hy\n            v_col_offset = n_ux\n            if j_p = Ny - 1:  # v(i_p, j_p) is a DOF\n                col_idx = v_col_offset + v_idx(i_p, j_p)\n                B_full[p_row_idx, col_idx] = 1.0 / hy\n            if j_p >= 2:  # v(i_p, j_p-1) is a DOF\n                col_idx = v_col_offset + v_idx(i_p, j_p - 1)\n                B_full[p_row_idx, col_idx] = -1.0 / hy\n    \n    # Enforce mean-zero pressure by removing one pressure DOF\n    B = B_full[:-1, :]\n    n_p = n_p_full - 1\n    \n    return K, B, n_u, n_p\n\ndef run_case(Nx, Ny, mu, Khat_choice, Shat_choice):\n    \"\"\"\n    Runs a single test case for the block preconditioner analysis.\n    \"\"\"\n    # 1. Build discrete operators\n    K, B, n_u, n_p = build_stokes_matrices(Nx, Ny, mu)\n    \n    # 2. Form the full saddle-point matrix A\n    A = np.block([\n        [K, B.T],\n        [B, np.zeros((n_p, n_p))]\n    ])\n\n    # 3. Form exact Schur complement S\n    K_inv = np.linalg.inv(K)\n    S = B @ K_inv @ B.T\n    \n    # 4. Construct approximation Khat\n    if Khat_choice == 'exact':\n        Khat = K\n    elif Khat_choice == 'diag':\n        Khat = np.diag(np.diag(K))\n    else:\n        raise ValueError(f\"Unknown Khat_choice: {Khat_choice}\")\n\n    # 5. Construct approximation Shat\n    if Shat_choice == 'exact':\n        Shat = S\n    elif Shat_choice == 'diag':\n        Shat = np.diag(np.diag(S))\n    elif Shat_choice == 'alpha_I':\n        # Need to handle case where S is 1x1 or empty\n        if S.shape[0] > 0:\n            alpha = np.mean(np.diag(S))\n        else:\n            alpha = 1.0\n        Shat = alpha * np.eye(n_p)\n    else:\n        raise ValueError(f\"Unknown Shat_choice: {Shat_choice}\")\n\n    # 6. Construct inverse of preconditioner M\n    Khat_inv = np.linalg.inv(Khat)\n    Shat_inv = np.linalg.inv(Shat) if n_p > 0 else np.array([[]])\n    \n    M_inv_top = np.hstack([Khat_inv, np.zeros((n_u, n_p))])\n    if n_p > 0:\n        M_inv_bot = np.hstack([Shat_inv @ B @ Khat_inv, -Shat_inv])\n        M_inv = np.vstack([M_inv_top, M_inv_bot])\n    else:\n        M_inv = M_inv_top\n\n    # 7. Form preconditioned matrix and find eigenvalues\n    preconditioned_A = M_inv @ A\n    eigvals = np.linalg.eigvals(preconditioned_A)\n    \n    # 8. Compute metrics\n    max_dev = np.max(np.abs(eigvals - 1.0))\n    all_close = bool(max_dev = 1e-8)\n    \n    return [max_dev, all_close]\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite, then prints the formatted results.\n    \"\"\"\n    test_cases = [\n        (3, 3, 1.0, 'exact', 'exact'),\n        (3, 3, 1.0, 'diag', 'exact'),\n        (5, 4, 1.0, 'exact', 'diag'),\n        (5, 4, 1.0, 'diag', 'alpha_I'),\n        (4, 4, 0.1, 'exact', 'diag'),\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        Nx, Ny, mu, Khat_choice, Shat_choice = case\n        result = run_case(Nx, Ny, mu, Khat_choice, Shat_choice)\n        # Python's bool __str__ method correctly produces 'True' or 'False'\n        results.append(f\"[{result[0]:.8e},{str(result[1])}]\")\n\n    print(f\"[{','.join(results)}]\")\n\n# Execute the solver\nsolve()\n\n```"
        }
    ]
}