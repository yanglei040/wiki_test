{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp the mechanics of the Biconjugate Gradient Stabilized method, there is no substitute for performing an iteration by hand. This exercise strips away the abstraction of code and reveals the concrete arithmetic and vector operations at the heart of the algorithm. By manually calculating each scalar and vector for a simple $2 \\times 2$ system, you will build a foundational intuition for how the BiCG and stabilization steps work together to refine the solution .",
            "id": "3585874",
            "problem": "Consider the linear system $A x = b$ with $A \\in \\mathbb{R}^{n \\times n}$ nonsingular. Krylov subspace methods construct approximations $x_{k}$ by projecting the residuals $r_{k} = b - A x_{k}$ according to principles such as the Petrov–Galerkin condition and residual minimization in low-dimensional subspaces. The Biconjugate Gradient Stabilized method (BiCGSTAB) is obtained by blending the biorthogonal projection characteristic of the Biconjugate Gradient (BiCG) method with a local minimal residual stabilization step.\n\nStart from the following well-tested principles:\n- The Euclidean inner product is $\\langle u, v \\rangle = u^{\\top} v$ and the Euclidean (two-)norm is $\\|u\\|_{2} = \\sqrt{\\langle u, u \\rangle}$.\n- A Petrov–Galerkin projection with a fixed nonzero “shadow residual” $\\tilde r$ enforces $\\langle \\tilde r, r_{k+1} \\rangle = 0$ for the intermediate residual obtained by a search direction update.\n- Given a nonzero intermediate residual $s_{k}$, the scalar $\\omega_{k}$ that minimizes $\\|s_{k} - \\omega A s_{k}\\|_{2}$ over $\\omega \\in \\mathbb{R}$ is determined by the normal equation arising from the least-squares problem.\n\nUse these principles to carry out one full iteration of the Biconjugate Gradient Stabilized (BiCGSTAB) method for the specific data\n$$\nA = \\begin{bmatrix}2  -1 \\\\ 1  3\\end{bmatrix}, \\quad\nb = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}, \\quad\nx_{0} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}, \\quad\n\\tilde r = r_{0},\n$$\nwith the canonical initialization $r_{0} = b - A x_{0}$ and $p_{0} = r_{0}$. Compute numerically the scalars and vectors\n$$\n\\rho_{0}, \\ \\alpha_{0}, \\ s_{0}, \\ \\omega_{0}, \\ x_{1}, \\ r_{1}, \\ p_{1}.\n$$\n\nAnswer specification:\n- Use the standard Euclidean inner product $\\langle u, v \\rangle = u^{\\top} v$.\n- As your final reported quantity, provide the single scalar value $\\|r_{1}\\|_{2}^{2}$ as an exact rational number in simplest terms.\n- Do not round; no significant-figure instruction is needed because an exact rational answer is expected.",
            "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the established principles of numerical linear algebra, specifically the Biconjugate Gradient Stabilized (BiCGSTAB) method. The problem is well-posed, providing all necessary data for one complete iteration of the algorithm. The data and objectives are stated objectively and are free from contradiction or ambiguity. Therefore, I will proceed with the solution.\n\nThe task is to perform one full iteration of the BiCGSTAB method for the given linear system $A x = b$. The algorithm consists of a series of well-defined steps. We begin with the index $k=0$.\n\n**Step 0: Initialization**\n\nThe initial guess for the solution is given as $x_{0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\nThe initial residual $r_{0}$ is computed as $r_{0} = b - A x_{0}$.\nGiven $A = \\begin{bmatrix} 2  -1 \\\\ 1  3 \\end{bmatrix}$ and $b = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$:\n$A x_{0} = \\begin{bmatrix} 2  -1 \\\\ 1  3 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\nThus, $r_{0} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} - \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\n\nThe problem specifies the initial \"shadow residual\" as $\\tilde{r}_{0} = r_{0}$, so $\\tilde{r}_{0} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\nThe initial search direction is $p_{0} = r_{0}$, so $p_{0} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\n\n**Iteration $k=0$**\n\n1.  **Compute $\\rho_{0}$:**\n    The scalar $\\rho_{0}$ is defined by the inner product $\\rho_{0} = \\langle \\tilde{r}_{0}, r_{0} \\rangle$.\n    Using the Euclidean inner product $\\langle u, v \\rangle = u^{\\top} v$:\n    $\\rho_{0} = r_{0}^{\\top} r_{0} = \\begin{bmatrix} 1  0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = (1)(1) + (0)(0) = 1$.\n\n2.  **Compute $v_{0} = A p_{0}$:**\n    $v_{0} = \\begin{bmatrix} 2  -1 \\\\ 1  3 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} (2)(1) + (-1)(0) \\\\ (1)(1) + (3)(0) \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$.\n\n3.  **Compute $\\alpha_{0}$:**\n    The scalar $\\alpha_{0}$ is derived from the Petrov-Galerkin condition on the intermediate update, which gives $\\alpha_{0} = \\frac{\\rho_{0}}{\\langle \\tilde{r}_{0}, v_{0} \\rangle}$.\n    The inner product in the denominator is:\n    $\\langle \\tilde{r}_{0}, v_{0} \\rangle = \\tilde{r}_{0}^{\\top} v_{0} = \\begin{bmatrix} 1  0 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} = (1)(2) + (0)(1) = 2$.\n    Therefore, $\\alpha_{0} = \\frac{1}{2}$.\n\n4.  **Compute the intermediate residual $s_{0}$:**\n    $s_{0} = r_{0} - \\alpha_{0} v_{0}$.\n    $s_{0} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} - \\frac{1}{2} \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1/2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -1/2 \\end{bmatrix}$.\n\n5.  **Compute $t_{0} = A s_{0}$:**\n    $t_{0} = \\begin{bmatrix} 2  -1 \\\\ 1  3 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ -1/2 \\end{bmatrix} = \\begin{bmatrix} (2)(0) + (-1)(-1/2) \\\\ (1)(0) + (3)(-1/2) \\end{bmatrix} = \\begin{bmatrix} 1/2 \\\\ -3/2 \\end{bmatrix}$.\n\n6.  **Compute $\\omega_{0}$:**\n    The scalar $\\omega_{0}$ is chosen to minimize $\\|s_{0} - \\omega A s_{0}\\|_{2}$, which is equivalent to $\\|s_{0} - \\omega t_{0}\\|_{2}$. The solution to this least-squares problem is given by the normal equation, leading to $\\omega_{0} = \\frac{\\langle s_{0}, t_{0} \\rangle}{\\langle t_{0}, t_{0} \\rangle}$.\n    First, we compute the necessary inner products:\n    $\\langle s_{0}, t_{0} \\rangle = s_{0}^{\\top} t_{0} = \\begin{bmatrix} 0  -1/2 \\end{bmatrix} \\begin{bmatrix} 1/2 \\\\ -3/2 \\end{bmatrix} = (0)(1/2) + (-1/2)(-3/2) = \\frac{3}{4}$.\n    $\\langle t_{0}, t_{0} \\rangle = t_{0}^{\\top} t_{0} = \\begin{bmatrix} 1/2  -3/2 \\end{bmatrix} \\begin{bmatrix} 1/2 \\\\ -3/2 \\end{bmatrix} = (\\frac{1}{2})^{2} + (-\\frac{3}{2})^{2} = \\frac{1}{4} + \\frac{9}{4} = \\frac{10}{4} = \\frac{5}{2}$.\n    Thus, $\\omega_{0} = \\frac{3/4}{5/2} = \\frac{3}{4} \\cdot \\frac{2}{5} = \\frac{6}{20} = \\frac{3}{10}$.\n\n7.  **Update the solution to get $x_{1}$:**\n    $x_{1} = x_{0} + \\alpha_{0} p_{0} + \\omega_{0} s_{0}$.\n    $x_{1} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\frac{1}{2} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + \\frac{3}{10} \\begin{bmatrix} 0 \\\\ -1/2 \\end{bmatrix} = \\begin{bmatrix} 1/2 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ -3/20 \\end{bmatrix} = \\begin{bmatrix} 1/2 \\\\ -3/20 \\end{bmatrix}$.\n\n8.  **Update the residual to get $r_{1}$:**\n    $r_{1} = s_{0} - \\omega_{0} t_{0}$.\n    $r_{1} = \\begin{bmatrix} 0 \\\\ -1/2 \\end{bmatrix} - \\frac{3}{10} \\begin{bmatrix} 1/2 \\\\ -3/2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -5/10 \\end{bmatrix} - \\begin{bmatrix} 3/20 \\\\ -9/20 \\end{bmatrix} = \\begin{bmatrix} 0 - 3/20 \\\\ -10/20 - (-9/20) \\end{bmatrix} = \\begin{bmatrix} -3/20 \\\\ -1/20 \\end{bmatrix}$.\n\n9.  **Update the search direction to get $p_{1}$:**\n    This requires computing $\\rho_{1}$ and $\\beta_{0}$.\n    $\\rho_{1} = \\langle \\tilde{r}_{0}, r_{1} \\rangle = \\begin{bmatrix} 1  0 \\end{bmatrix} \\begin{bmatrix} -3/20 \\\\ -1/20 \\end{bmatrix} = -\\frac{3}{20}$.\n    $\\beta_{0} = \\frac{\\rho_{1}}{\\rho_{0}} \\frac{\\alpha_{0}}{\\omega_{0}} = \\frac{-3/20}{1} \\frac{1/2}{3/10} = -\\frac{3}{20} \\cdot \\frac{1}{2} \\cdot \\frac{10}{3} = -\\frac{30}{120} = -\\frac{1}{4}$.\n    The search direction is updated as $p_{1} = r_{1} + \\beta_{0} (p_{0} - \\omega_{0} v_{0})$.\n    First, calculate the term in parentheses:\n    $p_{0} - \\omega_{0} v_{0} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} - \\frac{3}{10} \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} - \\begin{bmatrix} 6/10 \\\\ 3/10 \\end{bmatrix} = \\begin{bmatrix} 4/10 \\\\ -3/10 \\end{bmatrix} = \\begin{bmatrix} 2/5 \\\\ -3/10 \\end{bmatrix}$.\n    Now, compute $p_{1}$:\n    $p_{1} = \\begin{bmatrix} -3/20 \\\\ -1/20 \\end{bmatrix} + (-\\frac{1}{4}) \\begin{bmatrix} 2/5 \\\\ -3/10 \\end{bmatrix} = \\begin{bmatrix} -3/20 \\\\ -1/20 \\end{bmatrix} - \\begin{bmatrix} 2/20 \\\\ -3/40 \\end{bmatrix} = \\begin{bmatrix} -5/20 \\\\ -2/40 + 3/40 \\end{bmatrix} = \\begin{bmatrix} -1/4 \\\\ 1/40 \\end{bmatrix}$.\n\nThe quantities computed for the first full iteration are:\n$\\rho_{0} = 1$, $\\alpha_{0} = \\frac{1}{2}$, $s_{0} = \\begin{bmatrix} 0 \\\\ -1/2 \\end{bmatrix}$, $\\omega_{0} = \\frac{3}{10}$, $x_{1} = \\begin{bmatrix} 1/2 \\\\ -3/20 \\end{bmatrix}$, $r_{1} = \\begin{bmatrix} -3/20 \\\\ -1/20 \\end{bmatrix}$, $p_{1} = \\begin{bmatrix} -1/4 \\\\ 1/40 \\end{bmatrix}$.\n\n**Final Calculation**\n\nThe problem requires the calculation of $\\|r_{1}\\|_{2}^{2}$.\n$\\|r_{1}\\|_{2}^{2} = \\langle r_{1}, r_{1} \\rangle = r_{1}^{\\top} r_{1} = \\begin{bmatrix} -3/20  -1/20 \\end{bmatrix} \\begin{bmatrix} -3/20 \\\\ -1/20 \\end{bmatrix}$.\n$\\|r_{1}\\|_{2}^{2} = (-\\frac{3}{20})^{2} + (-\\frac{1}{20})^{2} = \\frac{9}{400} + \\frac{1}{400} = \\frac{10}{400} = \\frac{1}{40}$.\nThe result is an exact rational number as required.",
            "answer": "$$\\boxed{\\frac{1}{40}}$$"
        },
        {
            "introduction": "The BiCGSTAB method blends a Biconjugate Gradient step with a stabilizing move that locally minimizes the residual. But what happens when this stabilization step offers no improvement? This thought experiment invites you to explore such a scenario, where the intermediate vector $s_{1}$ is orthogonal to its image under $A$, $As_{1}$ . By deriving the consequences, you will gain a deeper appreciation for the role of the stabilization parameter $\\omega_{1}$ and see how the algorithm behaves when it momentarily reverts to a simpler, non-stabilized form.",
            "id": "3616023",
            "problem": "Consider the linear system $A x = b$ arising in frequency-domain seismic wavefield modeling for a viscoacoustic medium, where $A \\in \\mathbb{C}^{n \\times n}$ is large, sparse, and non-Hermitian due to absorbing boundary conditions and attenuation. An iterative solver based on the Biconjugate Gradient Stabilized (BiCGStab) method is applied to approximate the solution. Let the initial guess be $x_{0}$, the initial residual be $r_{0} = b - A x_{0}$, and the shadow residual be $r^{\\sharp}$ satisfying $\\langle r^{\\sharp}, r_{0}\\rangle \\neq 0$, where $\\langle \\cdot, \\cdot \\rangle$ denotes the standard Euclidean inner product on $\\mathbb{C}^{n}$ with the convention $\\langle u, v \\rangle = v^{*} u$. Define the initial search direction $p_{0} = r_{0}$ and the scalar $\\alpha_{1}$ via the usual non-symmetric Krylov projection, and then form the intermediate residual\n$$\ns_{1} = r_{0} - \\alpha_{1} A p_{0}.\n$$\nThe method next seeks a stabilizing scalar $\\omega_{1}$ by minimizing the squared $2$-norm of the residual $r_{1}(\\omega) = s_{1} - \\omega t_{1}$, where $t_{1} = A s_{1}$. The minimizer $\\omega_{1}$ is determined by the condition that $r_{1}(\\omega)$ has minimal $2$-norm in the one-dimensional affine subspace $\\{s_{1} - \\omega t_{1} : \\omega \\in \\mathbb{C}\\}$.\n\nAssume a scenario relevant to strongly heterogeneous attenuation where the action of $A$ on $s_{1}$ produces a vector $t_{1}$ that is orthogonal to $s_{1}$ in the Euclidean inner product, i.e.,\n$$\n\\langle t_{1}, s_{1} \\rangle = 0 \\quad \\text{and} \\quad \\langle t_{1}, t_{1} \\rangle \\neq 0.\n$$\nUnder these assumptions, determine the minimizing $\\omega_{1}$ and deduce the resulting updates to the iterate and residual,\n$$\nx_{1} = x_{0} + \\alpha_{1} p_{0} + \\omega_{1} s_{1}, \\qquad r_{1} = s_{1} - \\omega_{1} t_{1}.\n$$\nProvide the final answer as a single row matrix containing the three entries $\\omega_{1}$, $x_{1}$, and $r_{1}$, expressed symbolically in terms of $x_{0}$, $p_{0}$, $\\alpha_{1}$, and $s_{1}$. No numerical evaluation is required, and no rounding is necessary.",
            "solution": "The user has presented a problem from the field of computational geophysics, specifically concerning the Biconjugate Gradient Stabilized (BiCGStab) method for solving linear systems. As per protocol, the problem must first be rigorously validated.\n\n### Step 1: Extract Givens\nThe problem provides the following data and definitions:\n-   A linear system $A x = b$, where $A \\in \\mathbb{C}^{n \\times n}$ is large, sparse, and non-Hermitian.\n-   The initial guess is $x_{0}$.\n-   The initial residual is $r_{0} = b - A x_{0}$.\n-   A shadow residual $r^{\\sharp}$ exists such that $\\langle r^{\\sharp}, r_{0}\\rangle \\neq 0$.\n-   The inner product on $\\mathbb{C}^{n}$ is defined as $\\langle u, v \\rangle = v^{*} u$.\n-   The initial search direction is $p_{0} = r_{0}$.\n-   An intermediate residual is defined as $s_{1} = r_{0} - \\alpha_{1} A p_{0}$ for some scalar $\\alpha_{1}$.\n-   A vector $t_{1}$ is defined as $t_{1} = A s_{1}$.\n-   A stabilizing scalar $\\omega_{1}$ is sought to minimize the squared $2$-norm of $r_{1}(\\omega) = s_{1} - \\omega t_{1}$.\n-   A specific condition is assumed: $\\langle t_{1}, s_{1} \\rangle = 0$ and $\\langle t_{1}, t_{1} \\rangle \\neq 0$.\n-   The updated iterate is $x_{1} = x_{0} + \\alpha_{1} p_{0} + \\omega_{1} s_{1}$.\n-   The updated residual is $r_{1} = s_{1} - \\omega_{1} t_{1}$.\n-   The task is to find $\\omega_{1}$, $x_{1}$, and $r_{1}$ under the given condition.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria:\n-   **Scientifically Grounded**: The problem is well-grounded in numerical linear algebra and its application to seismic modeling. The BiCGStab method is a standard iterative solver. The description of the matrix $A$ as non-Hermitian due to physical phenomena (attenuation) and numerical artifacts (absorbing boundaries) is accurate. The steps outlined for the BiCGStab algorithm are standard. The specified inner product definition is one of the two common conventions in complex vector spaces. The condition $\\langle t_1, s_1 \\rangle = 0$ is a specific mathematical scenario posed for analysis, not a claim of its universal occurrence. Thus, the problem is scientifically sound.\n-   **Well-Posed**: The problem is well-posed. It asks for the determination of specific quantities ($\\omega_{1}$, $x_{1}$, $r_{1}$) based on a clear set of definitions and a specific, mathematically consistent assumption. The objective of minimizing a quadratic function ensures a unique minimizer exists.\n-   **Objective**: The language is formal, precise, and devoid of subjectivity. All terms are standard within the relevant field.\n-   The problem does not exhibit any of the flaws listed:\n    1.  No scientific or factual unsoundness.\n    2.  The problem is formally stated and directly relevant to the topic.\n    3.  The setup is complete and consistent; all necessary quantities are defined.\n    4.  The conditions are not physically impossible, even if the central assumption represents a specific, perhaps infrequent, case.\n    5.  The problem is not ill-posed; a unique, stable solution can be derived.\n    6.  The problem is not trivial; it requires a correct application of vector calculus and linear algebra principles.\n    7.  The problem is a mathematical derivation and is therefore verifiable.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A reasoned solution will now be provided.\n\n### Solution Derivation\nThe objective is to determine the scalar $\\omega_{1} \\in \\mathbb{C}$ that minimizes the squared $2$-norm of the residual $r_{1}(\\omega) = s_{1} - \\omega t_{1}$. The squared $2$-norm is defined as $\\|v\\|_{2}^{2} = \\langle v, v \\rangle$ for any vector $v \\in \\mathbb{C}^{n}$.\n\nLet $f(\\omega)$ be the function to minimize:\n$$\nf(\\omega) = \\|s_{1} - \\omega t_{1}\\|_{2}^{2} = \\langle s_{1} - \\omega t_{1}, s_{1} - \\omega t_{1} \\rangle\n$$\nUsing the given inner product definition $\\langle u, v \\rangle = v^{*} u$, we can expand this expression. The conjugate transpose of a scalar-vector product is $(\\omega t_1)^* = \\bar{\\omega} t_1^*$.\n$$\nf(\\omega) = (s_{1} - \\omega t_{1})^{*} (s_{1} - \\omega t_{1}) = (s_{1}^{*} - \\bar{\\omega} t_{1}^{*}) (s_{1} - \\omega t_{1})\n$$\nExpanding the product gives:\n$$\nf(\\omega) = s_{1}^{*}s_{1} - \\omega s_{1}^{*}t_{1} - \\bar{\\omega} t_{1}^{*}s_{1} + \\bar{\\omega}\\omega t_{1}^{*}t_{1}\n$$\nWe can express the terms using the inner product notation:\n-   $s_{1}^{*}s_{1} = \\langle s_{1}, s_{1} \\rangle = \\|s_{1}\\|_{2}^{2}$\n-   $t_{1}^{*}s_{1} = \\langle s_{1}, t_{1} \\rangle$\n-   $s_{1}^{*}t_{1} = (t_{1}^{*}s_{1})^{*} = \\langle s_{1}, t_{1} \\rangle^{*}$\n-   $t_{1}^{*}t_{1} = \\langle t_{1}, t_{1} \\rangle = \\|t_{1}\\|_{2}^{2}$\n\nSubstituting these into the expression for $f(\\omega)$:\n$$\nf(\\omega) = \\|s_{1}\\|_{2}^{2} - \\omega \\langle s_{1}, t_{1} \\rangle^{*} - \\bar{\\omega} \\langle s_{1}, t_{1} \\rangle + |\\omega|^{2} \\|t_{1}\\|_{2}^{2}\n$$\nTo find the minimum, we can set the complex derivative with respect to $\\bar{\\omega}$ to zero:\n$$\n\\frac{\\partial f}{\\partial \\bar{\\omega}} = - \\langle s_{1}, t_{1} \\rangle + \\omega \\|t_{1}\\|_{2}^{2} = 0\n$$\nSolving for $\\omega$ gives the general expression for the minimizer, which we denote $\\omega_1$:\n$$\n\\omega_{1} = \\frac{\\langle s_{1}, t_{1} \\rangle}{\\|t_{1}\\|_{2}^{2}} = \\frac{\\langle s_{1}, t_{1} \\rangle}{\\langle t_{1}, t_{1} \\rangle}\n$$\nThis is the standard formula for $\\omega_1$ in the BiCGStab algorithm.\n\nNow, we apply the specific condition given in the problem statement: $\\langle t_{1}, s_{1} \\rangle = 0$.\nThe complex inner product has the property $\\langle u, v \\rangle = \\overline{\\langle v, u \\rangle}$. Therefore:\n$$\n\\langle s_{1}, t_{1} \\rangle = \\overline{\\langle t_{1}, s_{1} \\rangle}\n$$\nGiven $\\langle t_{1}, s_{1} \\rangle = 0$, we have:\n$$\n\\langle s_{1}, t_{1} \\rangle = \\overline{0} = 0\n$$\nThe numerator of the expression for $\\omega_{1}$ is zero. The denominator is $\\langle t_{1}, t_{1} \\rangle$, which is explicitly stated to be non-zero.\nThus, the value of $\\omega_{1}$ is:\n$$\n\\omega_{1} = \\frac{0}{\\langle t_{1}, t_{1} \\rangle} = 0\n$$\nAlternatively, substituting the condition $\\langle t_1, s_1 \\rangle = 0$ (which implies $\\langle s_1, t_1 \\rangle = 0$) directly into the expanded function $f(\\omega)$:\n$$\nf(\\omega) = \\|s_{1}\\|_{2}^{2} - \\omega(0) - \\bar{\\omega}(0) + |\\omega|^{2} \\|t_{1}\\|_{2}^{2} = \\|s_{1}\\|_{2}^{2} + |\\omega|^{2} \\|t_{1}\\|_{2}^{2}\n$$\nSince $\\|s_{1}\\|_{2}^{2}$ is a non-negative constant and $\\|t_{1}\\|_{2}^{2}$ is a positive constant (as $\\langle t_{1}, t_{1} \\rangle \\neq 0$), the function $f(\\omega)$ is minimized when the term $|\\omega|^{2} \\|t_{1}\\|_{2}^{2}$ is minimized. This occurs when $|\\omega|^{2}=0$, which implies $\\omega = 0$. The minimizing scalar is therefore $\\omega_1 = 0$.\n\nWith $\\omega_{1} = 0$ determined, we can find the updated iterate $x_{1}$ and residual $r_{1}$.\nThe update for the iterate is given by:\n$$\nx_{1} = x_{0} + \\alpha_{1} p_{0} + \\omega_{1} s_{1}\n$$\nSubstituting $\\omega_{1}=0$:\n$$\nx_{1} = x_{0} + \\alpha_{1} p_{0} + (0) s_{1} = x_{0} + \\alpha_{1} p_{0}\n$$\nThe update for the residual is given by:\n$$\nr_{1} = s_{1} - \\omega_{1} t_{1}\n$$\nSubstituting $\\omega_{1}=0$:\n$$\nr_{1} = s_{1} - (0) t_{1} = s_{1}\n$$\nThe condition $\\langle t_1, s_1 \\rangle = \\langle As_1, s_1 \\rangle = 0$ signifies that the stabilization step of BiCGStab provides no improvement to the residual, as the optimal stabilizing parameter $\\omega_1$ is zero. The method effectively reduces to a single step of the Biconjugate Gradient method for this iteration, with the new residual $r_1$ simply being the intermediate residual $s_1$.\n\nThe three requested quantities expressed in terms of the specified variables are $\\omega_1=0$, $x_1=x_0 + \\alpha_1 p_0$, and $r_1=s_1$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0  x_{0} + \\alpha_{1} p_{0}  s_{1}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Theory and practice merge in this coding challenge, where you will implement the BiCGSTAB algorithm to investigate a phenomenon known as a \"lucky breakdown.\" This occurs when the method finds the exact solution far earlier than expected. Your task is to not only write the solver but to engineer specific linear systems where the initial residual is an eigenvector of the matrix, a condition that guarantees this early convergence . This practice provides a powerful lesson in how the underlying algebraic structure of a problem dictates the behavior of an iterative method.",
            "id": "3102131",
            "problem": "You are asked to implement a complete, runnable program that constructs a small test harness to demonstrate a specific phenomenon in iterative Krylov subspace methods, namely a “lucky breakdown” in the Biconjugate Gradient Stabilized method (BiCGSTAB). Work in exact arithmetic as modeled by double-precision floating point, and treat vectors and matrices as arrays over the real numbers.\n\nYour program must do the following.\n\n1. Core definitions to be used as the fundamental base:\n   - Let a linear system be given by $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$, $x \\in \\mathbb{R}^{n}$, and $b \\in \\mathbb{R}^{n}$.\n   - Given an initial guess $x_0 \\in \\mathbb{R}^{n}$, define the initial residual $r_0 = b - A x_0$.\n   - Use the standard Euclidean inner product $(u, v) = \\sum_{i=1}^{n} u_i v_i$ and the induced $2$-norm $\\lVert u \\rVert_2 = \\sqrt{(u,u)}$.\n   - Implement the Biconjugate Gradient Stabilized method (BiCGSTAB), using $r_0$ also as the fixed shadow residual. Use a standard stopping criterion based on the residual norm falling below a tolerance.\n\n2. Lucky breakdown event to detect:\n   - In each BiCGSTAB iteration, after computing the scalar step size $\\alpha_k$ and the intermediate vector $s_k = r_{k-1} - \\alpha_k A p_{k-1}$, a “lucky breakdown” is said to occur if $s_k = 0$ (in floating point, treat $\\lVert s_k \\rVert_2 \\le \\varepsilon$ as zero) before computing the next stabilization step. In this case, the exact solution has been found early within that iteration via $x_k = x_{k-1} + \\alpha_k p_{k-1}$.\n   - Your program must declare a lucky breakdown only in this sense. Do not count the trivial case $r_0 = 0$ as lucky; and do not count convergence that occurs only after completing the $\\omega_k$ stabilization step as lucky.\n\n3. Numerical tolerances:\n   - Use a convergence and zero-detection tolerance of $\\varepsilon = 10^{-12}$ for checking $\\lVert s_k \\rVert_2$ and $\\lVert r_k \\rVert_2$.\n   - Use a maximum iteration cap of $k_{\\max} = 1000$ per test case to avoid infinite loops in pathological inputs.\n\n4. Test suite design and construction:\n   - Construct four test cases by explicitly specifying $A$, $b$, and $x_0$ so that the harness covers the following behaviors. All entries and sizes must be stated explicitly, and all matrices must be nonsingular.\n     - Case L1 (symmetric lucky breakdown): Let $A \\in \\mathbb{R}^{3 \\times 3}$ be diagonal with entries $A = \\mathrm{diag}(2, 3, 4)$, let $x_0 = (0, 0, 0)^{\\top}$, and let $b = (1, 0, 0)^{\\top}$. This configuration should be constructed so that $r_0$ is an eigenvector of $A$, which algebraically enables early exact solution via $s_0 = 0$.\n     - Case L2 (nonsymmetric lucky breakdown): Let $A \\in \\mathbb{R}^{2 \\times 2}$ be the upper triangular Jordan-like block $A = \\begin{bmatrix} 5  1 \\\\ 0  5 \\end{bmatrix}$, let $x_0 = (0, 0)^{\\top}$, and let $b = (1, 0)^{\\top}$. This configuration should again ensure that $r_0$ is an eigenvector of $A$, so the same algebraic mechanism can trigger $s_0 = 0$ even though $A$ is not symmetric.\n     - Case B1 (boundary, no-lucky): Let $A \\in \\mathbb{R}^{2 \\times 2}$ be diagonal $A = \\mathrm{diag}(7, 8)$, choose $x_{\\star} = (1, 2)^{\\top}$, set $b = A x_{\\star}$, and set the initial guess equal to the exact solution $x_0 = x_{\\star}$. This yields $r_0 = 0$. This case must not be counted as lucky because no iteration is needed.\n     - Case G1 (general, no-lucky): Let $A \\in \\mathbb{R}^{2 \\times 2}$ be diagonal $A = \\mathrm{diag}(2, 3)$, let $x_0 = (0, 0)^{\\top}$, and let $b = (1, 1)^{\\top}$. This setup should produce a normal iteration sequence in which $s_0 \\ne 0$ and convergence, if reached, occurs after the stabilization step or subsequent iterations.\n\n5. Output specification:\n   - For each test case, your program must output the integer $1$ if a lucky breakdown (as defined above) occurred at some iteration’s $\\alpha$-step, and the integer $0$ otherwise (including the trivial $r_0 = 0$ case and any convergence after the $\\omega$-step).\n   - Aggregate the four results into a single line printed as a comma-separated list enclosed in square brackets, with no spaces, in the order $[\\text{L1},\\text{L2},\\text{B1},\\text{G1}]$. For example, a valid output might look like $[1,1,0,0]$.\n\n6. Angle units, physical units, and percentages:\n   - No physical quantities, angles, or percentages are involved in this task.\n\nYour program must be fully self-contained, require no user input, and must obey the exact output format described above.",
            "solution": "The problem requires the implementation of the Biconjugate Gradient Stabilized (BiCGSTAB) iterative method to solve a linear system $A x = b$ and to construct a test suite to demonstrate a specific phenomenon known as a \"lucky breakdown.\"\n\nA linear system is given by $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is a nonsingular matrix, and $x, b \\in \\mathbb{R}^{n}$ are vectors. Given an initial guess $x_0$, the initial residual is $r_0 = b - A x_0$. The BiCGSTAB method iteratively refines the solution guess $x_k$ to minimize the residual $r_k = b - A x_k$.\n\nThe BiCGSTAB algorithm, using a fixed shadow residual $\\hat{r}_0$ (set to $r_0$ as per the problem), can be formulated as follows:\n\n1.  Initialize:\n    $x_0$ is the initial guess.\n    $r_0 = b - A x_0$.\n    If $\\lVert r_0 \\rVert_2$ is close to zero, the initial guess is already the solution.\n    Set the fixed shadow residual $\\hat{r}_0 = r_0$.\n    Set the initial search direction $p_0 = r_0$.\n    Compute the initial squared norm of the residual projection $\\rho_0 = (\\hat{r}_0, r_0)$.\n\n2.  Iterate for $k = 1, 2, \\dots$ until convergence or maximum iterations:\n    a. Compute the matrix-vector product with the search direction: $v_{k-1} = A p_{k-1}$.\n    b. Compute the step size $\\alpha_k$:\n       $$ \\alpha_k = \\frac{\\rho_{k-1}}{(\\hat{r}_0, v_{k-1})} = \\frac{(\\hat{r}_0, r_{k-1})}{(\\hat{r}_0, A p_{k-1})} $$\n    c. Update the solution along the search direction $p_{k-1}$ and compute an intermediate residual $s_k$:\n       $$ s_k = r_{k-1} - \\alpha_k v_{k-1} = r_{k-1} - \\alpha_k A p_{k-1} $$\n    d. **Lucky Breakdown Check**: If $\\lVert s_k \\rVert_2 \\le \\varepsilon$ (where $\\varepsilon=10^{-12}$ is a small tolerance), a \"lucky breakdown\" occurs. The exact solution has been found prematurely. The final solution is $x_k = x_{k-1} + \\alpha_k p_{k-1}$. We report this event and terminate.\n    e. If no lucky breakdown, proceed with the stabilization step. Compute the matrix-vector product $t_k = A s_k$.\n    f. Compute the stabilization parameter $\\omega_k$:\n       $$ \\omega_k = \\frac{(t_k, s_k)}{(t_k, t_k)} $$\n    g. Perform the full update for the solution and the final residual for this iteration:\n       $$ x_k = x_{k-1} + \\alpha_k p_{k-1} + \\omega_k s_k $$\n       $$ r_k = s_k - \\omega_k t_k $$\n    h. Check for normal convergence: if $\\lVert r_k \\rVert_2 \\le \\varepsilon$, terminate.\n    i. Prepare for the next iteration. Update the projection norm and the search direction:\n       $$ \\rho_k = (\\hat{r}_0, r_k) $$\n       $$ \\beta_k = \\frac{\\rho_k}{\\rho_{k-1}} \\frac{\\alpha_k}{\\omega_k} $$\n       $$ p_k = r_k + \\beta_k (p_{k-1} - \\omega_k v_{k-1}) $$\n\nThe core of the problem is to detect the lucky breakdown. This happens if $s_k=0$. Let's analyze this condition for the first iteration ($k=1$). The initial search direction is $p_0 = r_0$. The condition $s_1 = 0$ implies:\n$$ s_1 = r_0 - \\alpha_1 A p_0 = r_0 - \\alpha_1 A r_0 = 0 $$\n$$ \\implies A r_0 = \\frac{1}{\\alpha_1} r_0 $$\nThis equation shows that a lucky breakdown occurs in the first iteration if and only if the initial residual $r_0$ is an eigenvector of the matrix $A$. The corresponding step length $\\alpha_1$ will be the reciprocal of the eigenvalue. The test cases are designed to exploit this property.\n\n- **Case L1 (symmetric lucky breakdown)**: $A = \\mathrm{diag}(2, 3, 4)$, $x_0 = (0, 0, 0)^{\\top}$, $b = (1, 0, 0)^{\\top}$.\n  The initial residual is $r_0 = b - A x_0 = (1, 0, 0)^{\\top}$. This is an eigenvector of the diagonal matrix $A$ with eigenvalue $\\lambda=2$. Thus, a lucky breakdown is expected, and the algorithm should report $1$.\n\n- **Case L2 (nonsymmetric lucky breakdown)**: $A = \\begin{bmatrix} 5  1 \\\\ 0  5 \\end{bmatrix}$, $x_0 = (0, 0)^{\\top}$, $b = (1, 0)^{\\top}$.\n  The initial residual is $r_0 = b - A x_0 = (1, 0)^{\\top}$. We check if it is an eigenvector: $A r_0 = \\begin{pmatrix} 5 \\\\ 0 \\end{pmatrix} = 5 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 5 r_0$. It is an eigenvector with eigenvalue $\\lambda=5$. Therefore, a lucky breakdown is also expected here, and the result is $1$.\n\n- **Case B1 (boundary, no-lucky)**: $A = \\mathrm{diag}(7, 8)$, $x_0 = x_{\\star} = (1, 2)^{\\top}$, $b = A x_{\\star}$.\n  The initial residual is $r_0 = b - A x_0 = A x_{\\star} - A x_{\\star} = 0$. The check $\\lVert r_0 \\rVert_2 \\le \\varepsilon$ is met before any iterations begin. According to the problem statement, this trivial case is not considered a lucky breakdown. The program should report $0$.\n\n- **Case G1 (general, no-lucky)**: $A = \\mathrm{diag}(2, 3)$, $x_0 = (0, 0)^{\\top}$, $b = (1, 1)^{\\top}$.\n  The initial residual is $r_0 = b - A x_0 = (1, 1)^{\\top}$. We check if it is an eigenvector: $A r_0 = \\mathrm{diag}(2, 3) (1, 1)^{\\top} = (2, 3)^{\\top}$. Since $(2, 3)^{\\top}$ is not a scalar multiple of $(1, 1)^{\\top}$, $r_0$ is not an eigenvector of $A$. Therefore, $s_1 \\ne 0$ and no lucky breakdown will occur in the first iteration. The algorithm will proceed normally. The expected output is $0$.\n\nThe provided program implements this logic, correctly instantiates each test case, runs the BiCGSTAB algorithm, and reports a $1$ or $0$ based on the precise definition of a lucky breakdown.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef bicgstab(A, b, x0, tol, max_iter):\n    \"\"\"\n    Implements the Biconjugate Gradient Stabilized (BiCGSTAB) method and detects \"lucky breakdowns\".\n\n    A \"lucky breakdown\" is detected if the intermediate residual 's' becomes zero\n    before the stabilization step.\n\n    Args:\n        A (np.ndarray): The matrix of the linear system.\n        b (np.ndarray): The right-hand side vector of the linear system.\n        x0 (np.ndarray): The initial guess for the solution.\n        tol (float): The tolerance for convergence and zero-detection.\n        max_iter (int): The maximum number of iterations.\n\n    Returns:\n        int: Returns 1 if a lucky breakdown occurred, 0 otherwise.\n    \"\"\"\n    x = np.copy(x0)\n    r = b - A @ x\n\n    # Per problem: do not count trivial convergence as a lucky breakdown.\n    if np.linalg.norm(r)  tol:\n        return 0\n\n    # Fixed shadow residual as per problem specification.\n    r_hat = np.copy(r)\n\n    rho = r_hat @ r\n    # Handle potential breakdown if rho is zero initially.\n    if abs(rho)  1e-50:\n        return 0 # Breakdown, not lucky.\n        \n    p = np.copy(r)\n    \n    is_lucky = 0\n\n    for _ in range(max_iter):\n        v = A @ p\n        \n        denom = r_hat @ v\n        # Handle potential breakdown.\n        if abs(denom)  1e-50:\n            return 0 # Breakdown, not lucky.\n\n        alpha = rho / denom\n        \n        s = r - alpha * v\n\n        # The specific \"lucky breakdown\" check as defined in the problem.\n        if np.linalg.norm(s)  tol:\n            x = x + alpha * p\n            is_lucky = 1\n            break\n\n        t = A @ s\n        \n        denom = t @ t\n        # If s is not zero, but As is, it's a breakdown. A must be singular, but problem guarantees nonsingular A.\n        # This check is for numerical stability with floating point arithmetic.\n        if abs(denom)  1e-50:\n            return 0 # Breakdown, not lucky.\n        \n        omega = (t @ s) / denom\n        \n        # Full update for x and r.\n        x = x + alpha * p + omega * s\n        r_new = s - omega * t\n\n        # Normal convergence check after the full stabilization step.\n        if np.linalg.norm(r_new)  tol:\n            break\n\n        rho_new = r_hat @ r_new\n\n        # Handle potential breakdown from rho or omega being zero.\n        if abs(rho)  1e-50 or abs(omega)  1e-50:\n            return 0 # Breakdown, not lucky.\n\n        beta = (rho_new / rho) * (alpha / omega)\n\n        # Update search direction p.\n        p = r_new + beta * (p - omega * v)\n\n        # Update r and rho for the next iteration.\n        r = r_new\n        rho = rho_new\n\n    return is_lucky\n\ndef solve():\n    # Define the parameters and test cases from the problem statement.\n    tol = 1e-12\n    max_iter = 1000\n\n    # Case L1 (symmetric lucky breakdown)\n    A1 = np.diag([2.0, 3.0, 4.0])\n    x0_1 = np.zeros(3)\n    b1 = np.array([1.0, 0.0, 0.0])\n\n    # Case L2 (nonsymmetric lucky breakdown)\n    A2 = np.array([[5.0, 1.0], [0.0, 5.0]])\n    x0_2 = np.zeros(2)\n    b2 = np.array([1.0, 0.0])\n\n    # Case B1 (boundary, no-lucky)\n    A3 = np.diag([7.0, 8.0])\n    x_star3 = np.array([1.0, 2.0])\n    b3 = A3 @ x_star3\n    x0_3 = np.copy(x_star3)\n\n    # Case G1 (general, no-lucky)\n    A4 = np.diag([2.0, 3.0])\n    x0_4 = np.zeros(2)\n    b4 = np.array([1.0, 1.0])\n\n    test_cases = [\n        (A1, b1, x0_1),  # L1\n        (A2, b2, x0_2),  # L2\n        (A3, b3, x0_3),  # B1\n        (A4, b4, x0_4),  # G1\n    ]\n\n    results = []\n    for A, b, x0 in test_cases:\n        is_lucky = bicgstab(A, b, x0, tol, max_iter)\n        results.append(is_lucky)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}