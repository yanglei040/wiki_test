{
    "hands_on_practices": [
        {
            "introduction": "In computational mechanics, theoretical correctness must translate into robust code. This practice grounds your numerical toolkit in fundamental linear algebra by using core tensor identities as a form of unit testing. By implementing and verifying relationships such as Cramer's rule and the inverse-transpose identity, you will build confidence in your determinant calculations and learn to diagnose issues arising from pivoting and rounding errors .",
            "id": "3605426",
            "problem": "Consider the deformation gradient tensor $F \\in \\mathbb{R}^{3 \\times 3}$ in computational solid mechanics, where the Jacobian $J = \\det(F)$ quantifies local volume change. Robust numerical evaluation of $\\det(F)$ must account for pivoting in Lower-Upper (LU) factorization to avoid sign errors caused by row permutations. The cofactor (adjugate) tensor $\\mathrm{cof}(F)$ and transpose/inverse identities provide principled unit tests to detect implementation defects, particularly those related to erroneous transposition.\n\nStarting from fundamental linear algebra definitions and facts applicable to tensors:\n- The transpose $F^T$ is defined by $(F^T)_{ij} = F_{ji}$.\n- The inverse $F^{-1}$ of an invertible $F$ satisfies $F F^{-1} = F^{-1} F = I$, where $I$ is the identity tensor.\n- The inverse-transpose identity holds for all invertible $F$: $F^{-T} = (F^{-1})^T$.\n- The cofactor (adjugate) tensor $\\mathrm{cof}(F)$ is the matrix of signed minors, and for invertible $F$ satisfies $\\mathrm{cof}(F) = \\det(F)\\, F^{-T}$.\n- Cramer's rule implies $F\\, \\mathrm{cof}(F)^T = \\det(F)\\, I$ for all $F \\in \\mathbb{R}^{n \\times n}$ (including singular $F$).\n- LU factorization with partial pivoting yields $P F = L U$, where $P$ is a permutation matrix, $L$ is unit lower triangular, and $U$ is upper triangular, implying $\\det(F) = \\det(P)\\, \\det(U)$ because $\\det(L) = 1$.\n\nYour task is to write a program that, for each given $3 \\times 3$ test tensor $F$, does the following:\n1. Compute $\\det(F)$ using a general-purpose backend call (library determinant).\n2. Compute $\\det(F)$ using LU factorization with partial pivoting via the identity $\\det(F) = \\det(P)\\, \\prod_i U_{ii}$, where $P F = L U$ and $U_{ii}$ are diagonal entries of $U$.\n3. Compute $\\mathrm{cof}(F)$ by its definition as the matrix of signed minors, then compute $\\det(F)$ again using the trace identity derived from Cramer's rule: $\\det(F) = \\frac{1}{3}\\, \\mathrm{tr}\\left(F\\, \\mathrm{cof}(F)^T\\right)$.\n4. Verify the inverse-transpose identity $F^{-T} = (F^{-1})^T$ for invertible $F$ only (skip or mark as false for singular $F$).\n5. Verify the cofactor-adjugate identity $\\mathrm{cof}(F) = \\det(F)\\, F^{-T}$ for invertible $F$ only.\n6. Verify the Cramer's rule identity $F\\, \\mathrm{cof}(F)^T = \\det(F)\\, I$ for all $F$.\n7. Compare the three determinant computations to examine any differences that could arise from pivoting and rounding.\n\nUse a tolerance of $\\varepsilon = 10^{-11}$ for relative comparisons and $\\delta = 10^{-12}$ for absolute comparisons in matrix equality checks (i.e., two matrices $A$ and $B$ are considered equal if $\\|A-B\\|_{\\infty} \\le \\delta + \\varepsilon \\|B\\|_{\\infty}$).\n\nTest Suite (five $3 \\times 3$ tensors):\n- General non-singular tensor (happy path):\n$$\nF_1 = \\begin{bmatrix}\n1.2 & -0.3 & 0.5 \\\\\n0.4 & 2.1 & -1.0 \\\\\n-0.8 & 0.7 & 1.5\n\\end{bmatrix}\n$$\n- Symmetric positive definite tensor:\n$$\nF_2 = \\begin{bmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{bmatrix}\n$$\n- Negative determinant tensor constructed by left-multiplying $F_2$ with $D = \\mathrm{diag}(-1,1,1)$:\n$$\nF_3 = D F_2 = \\begin{bmatrix}\n-2 & 1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{bmatrix}\n$$\n- Nearly singular tensor (boundary condition):\n$$\nF_4 = \\begin{bmatrix}\n1 & 1 & 1 \\\\\n1 + 10^{-8} & 1 + 10^{-8} & 1 + 10^{-8} \\\\\n1 & 2 & 3\n\\end{bmatrix}\n$$\n- Singular tensor (exact rank deficiency):\n$$\nF_5 = \\begin{bmatrix}\n1 & 2 & 3 \\\\\n2 & 4 & 6 \\\\\n0 & 0 & 1\n\\end{bmatrix}\n$$\n\nFinal Output Format:\nFor each test case $F_k$, produce a list containing six entries in the exact order:\n[`det_lib`, `det_pivot`, `det_trace`, `invT_ok`, `cof_adj_ok`, `cramer_ok`], where:\n- `det_lib` is the determinant from the backend call,\n- `det_pivot` is the determinant from LU with pivoting,\n- `det_trace` is the determinant from the trace identity,\n- `invT_ok` is a boolean for $F^{-T} = (F^{-1})^T$ (only for invertible $F$, otherwise false),\n- `cof_adj_ok` is a boolean for $\\mathrm{cof}(F) = \\det(F)\\, F^{-T}$ (only for invertible $F$, otherwise false),\n- `cramer_ok` is a boolean for $F\\, \\mathrm{cof}(F)^T = \\det(F)\\, I$ (for all $F$).\n\nYour program should produce a single line of output containing a list of the five case results as a comma-separated list enclosed in square brackets, with each case itself represented as a list in the order above. For example:\n`[ [...,...,...,...,...,...], [...,...,...,...,...,...], ... ]`.\nNo physical units enter this computation. Angles are not involved.\n\nThe goal is to use these identities as principled unit tests to catch transpose bugs and to examine differences attributable to pivoting and numerical rounding in computing $\\det(F)$.",
            "solution": "We begin from linear algebra foundations and identities that apply to deformation gradient tensors. For $F \\in \\mathbb{R}^{3 \\times 3}$, $\\det(F)$ can be obtained in several consistent ways; mismatches reveal numerical or implementation defects.\n\nDeterminant with pivoting via factorization: In Lower-Upper (LU) factorization with partial pivoting, we obtain $P F = L U$ where $P$ is a permutation matrix, $L$ is unit lower triangular (diagonal entries equal to $1$), and $U$ is upper triangular. Taking determinants,\n$$\n\\det(P)\\, \\det(F) = \\det(L)\\, \\det(U) \\quad \\Rightarrow \\quad \\det(F) = \\det(P)\\, \\det(U),\n$$\nbecause $\\det(L) = 1$. This expression makes explicit the effect of pivoting: row permutations contribute a sign factor $\\det(P) \\in \\{-1, +1\\}$ that must be accounted for to avoid sign errors in $\\det(F)$.\n\nCofactor and Cramer's rule identities: The cofactor (adjugate) matrix $\\mathrm{cof}(F)$ is defined as the matrix of signed minors:\n$$\n\\mathrm{cof}(F)_{ij} = (-1)^{i+j} \\det(M_{ij}),\n$$\nwhere $M_{ij}$ is the minor obtained by deleting row $i$ and column $j$. Cramer's rule yields the identity valid for all square matrices (including singular):\n$$\nF\\, \\mathrm{cof}(F)^T = \\det(F)\\, I.\n$$\nTaking the trace on both sides for $3 \\times 3$ matrices gives\n$$\n\\mathrm{tr}\\!\\left(F\\, \\mathrm{cof}(F)^T\\right) = \\det(F)\\, \\mathrm{tr}(I) = 3\\, \\det(F),\n$$\nwhich implies the trace-based determinant formula\n$$\n\\det(F) = \\frac{1}{3}\\, \\mathrm{tr}\\!\\left(F\\, \\mathrm{cof}(F)^T\\right).\n$$\nThis formula depends only on the cofactor definition and avoids inversion, making it robust even when $F$ is singular.\n\nInverse-transpose identity and cofactor-adjugate identity: For invertible $F$, the inverse-transpose identity\n$$\nF^{-T} = (F^{-1})^T\n$$\nfollows immediately from $F F^{-1} = I$ and properties of matrix transposition. Moreover, the classical adjugate identity\n$$\n\\mathrm{cof}(F) = \\det(F)\\, F^{-T}\n$$\nholds for invertible $F$ and is equivalent to $F\\, \\mathrm{cof}(F)^T = \\det(F)\\, I$.\n\nNumerical verification strategy and tolerances: We adopt a relative tolerance $\\varepsilon = 10^{-11}$ and an absolute tolerance $\\delta = 10^{-12}$ for equality checks. For matrix equalities $A = B$, we test\n$$\n\\|A - B\\|_{\\infty} \\le \\delta + \\varepsilon \\, \\|B\\|_{\\infty},\n$$\nwhere $\\|\\cdot\\|_{\\infty}$ is the maximum absolute entry norm. In code, we use $\\mathrm{allclose}$-style checks with these tolerances.\n\nAlgorithmic plan per test case $F$:\n1. Compute $\\det_{\\mathrm{lib}}$ via a standard backend call to the determinant.\n2. Compute $\\det_{\\mathrm{pivot}}$ from LU factorization with pivoting by evaluating $\\det(P)$ and $\\prod_i U_{ii}$.\n3. Compute $\\mathrm{cof}(F)$ using explicit $3 \\times 3$ signed minors, then obtain $\\det_{\\mathrm{trace}} = \\frac{1}{3}\\, \\mathrm{tr}(F\\, \\mathrm{cof}(F)^T)$.\n4. Determine invertibility by rank: if $\\mathrm{rank}(F) = 3$, proceed; otherwise treat as non-invertible.\n5. If invertible, verify $F^{-T} = (F^{-1})^T$ by separately computing $(F^{-1})^T$ and $(F^T)^{-1}$ and checking equality with tolerances.\n6. If invertible, verify $\\mathrm{cof}(F) = \\det_{\\mathrm{lib}}\\, F^{-T}$.\n7. For all $F$, verify $F\\, \\mathrm{cof}(F)^T = \\det_{\\mathrm{lib}}\\, I$.\n8. Record the tuple $[\\det_{\\mathrm{lib}}, \\det_{\\mathrm{pivot}}, \\det_{\\mathrm{trace}}, \\text{invT\\_ok}, \\text{cof\\_adj\\_ok}, \\text{cramer\\_ok}]$.\n\nCoverage of edge cases:\n- The general non-singular case tests consistency across methods.\n- The symmetric positive definite case checks behavior when $F = F^T$ (transpose symmetry).\n- The negative determinant case highlights the effect of orientation reversal, ensuring sign handling in pivoting is correct.\n- The nearly singular case examines numerical stability (rounding and conditioning).\n- The singular case avoids inversion and tests the Cramer's rule identity, which remains valid.\n\nThe final output aggregates the five case results into one top-level list and prints it as a single line. Differences between determinant computations, if any, stem from pivoting sign handling and numerical rounding; the unit tests based on $F^{-T} = (F^{-1})^T$ and $\\mathrm{cof}(F) = \\det(F)\\, F^{-T}$ directly expose transpose-related implementation bugs and adjugate misuse, while $F\\, \\mathrm{cof}(F)^T = \\det(F)\\, I$ provides a robust check even when $F$ is singular.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import lu\nimport json\n\ndef det_pivoting(A: np.ndarray) -> float:\n    \"\"\"\n    Compute det(A) using LU factorization with partial pivoting.\n    Given P @ A = L @ U, with L unit lower triangular, det(A) = det(P) * det(U).\n    \"\"\"\n    P, L, U = lu(A)\n    # det(P) is +/-1 for a permutation matrix; compute robustly\n    detP = np.linalg.det(P)\n    # Round to nearest integer to avoid tiny floating error\n    signP = int(np.round(detP))\n    detU = float(np.prod(np.diag(U)))\n    return signP * detU\n\ndef cofactor_3x3(A: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Compute the cofactor (adjugate) matrix of a 3x3 matrix A via signed minors.\n    \"\"\"\n    a = A\n    cof = np.empty((3, 3), dtype=float)\n    # Row 0\n    cof[0,0] = a[1,1]*a[2,2] - a[1,2]*a[2,1]\n    cof[0,1] = -(a[1,0]*a[2,2] - a[1,2]*a[2,0])\n    cof[0,2] = a[1,0]*a[2,1] - a[1,1]*a[2,0]\n    # Row 1\n    cof[1,0] = -(a[0,1]*a[2,2] - a[0,2]*a[2,1])\n    cof[1,1] = a[0,0]*a[2,2] - a[0,2]*a[2,0]\n    cof[1,2] = -(a[0,0]*a[2,1] - a[0,1]*a[2,0])\n    # Row 2\n    cof[2,0] = a[0,1]*a[1,2] - a[0,2]*a[1,1]\n    cof[2,1] = -(a[0,0]*a[1,2] - a[0,2]*a[1,0])\n    cof[2,2] = a[0,0]*a[1,1] - a[0,1]*a[1,0]\n    return cof\n\ndef det_trace_from_cof(A: np.ndarray, cofA: np.ndarray) -> float:\n    \"\"\"\n    Compute det(A) via the trace identity det(A) = (1/3) * tr(A @ cof(A).T) for 3x3.\n    \"\"\"\n    return float(np.trace(A @ cofA.T) / 3.0)\n\ndef is_invertible(A: np.ndarray) -> bool:\n    \"\"\"\n    Determine invertibility by rank for 3x3: rank == 3 => invertible.\n    \"\"\"\n    r = np.linalg.matrix_rank(A)\n    return r == 3\n\ndef allclose_inf_norm(A: np.ndarray, B: np.ndarray, rtol: float, atol: float) -> bool:\n    \"\"\"\n    Check max-norm closeness: ||A-B||_inf <= atol + rtol * ||B||_inf\n    \"\"\"\n    diff = np.max(np.abs(A - B))\n    normB = np.max(np.abs(B))\n    return diff <= (atol + rtol * normB)\n\ndef make_test_cases():\n    # F1: general non-singular\n    F1 = np.array([\n        [1.2, -0.3, 0.5],\n        [0.4,  2.1, -1.0],\n        [-0.8, 0.7, 1.5]\n    ], dtype=float)\n\n    # F2: symmetric positive definite\n    F2 = np.array([\n        [ 2.0, -1.0,  0.0],\n        [-1.0,  2.0, -1.0],\n        [ 0.0, -1.0,  2.0]\n    ], dtype=float)\n\n    # F3: negative determinant via left-multiplication by diag(-1,1,1)\n    D = np.diag([-1.0, 1.0, 1.0])\n    F3 = D @ F2\n\n    # F4: nearly singular\n    eps = 1e-8\n    F4 = np.array([\n        [1.0, 1.0, 1.0],\n        [1.0 + eps, 1.0 + eps, 1.0 + eps],\n        [1.0, 2.0, 3.0]\n    ], dtype=float)\n\n    # F5: singular (second row is 2x first row)\n    F5 = np.array([\n        [1.0, 2.0, 3.0],\n        [2.0, 4.0, 6.0],\n        [0.0, 0.0, 1.0]\n    ], dtype=float)\n\n    return [F1, F2, F3, F4, F5]\n\ndef solve():\n    rtol = 1e-11\n    atol = 1e-12\n\n    test_cases = make_test_cases()\n    results = []\n\n    for F in test_cases:\n        # Determinants\n        det_lib = float(np.linalg.det(F))\n        det_piv = det_pivoting(F)\n        cofF = cofactor_3x3(F)\n        det_trace = det_trace_from_cof(F, cofF)\n\n        # Invertibility check\n        invertible = is_invertible(F)\n\n        # Identity checks\n        if invertible:\n            invF = np.linalg.inv(F)\n            invT_from_inv = invF.T\n            invT_from_transpose = np.linalg.inv(F.T)\n            invT_ok = allclose_inf_norm(invT_from_inv, invT_from_transpose, rtol, atol)\n\n            # cof(F) = det(F) * F^{-T}\n            cof_adj_rhs = det_lib * invT_from_inv\n            cof_adj_ok = allclose_inf_norm(cofF, cof_adj_rhs, rtol, atol)\n        else:\n            invT_ok = False\n            cof_adj_ok = False\n\n        # Cramer's rule: F @ cof(F).T = det(F) * I for all F\n        lhs = F @ cofF.T\n        rhs = det_lib * np.eye(3)\n        cramer_ok = allclose_inf_norm(lhs, rhs, rtol, atol)\n\n        case_result = [det_lib, det_piv, det_trace, bool(invT_ok), bool(cof_adj_ok), bool(cramer_ok)]\n        results.append(case_result)\n\n    # Final print statement in the exact required format: single line, JSON-like brackets, no spaces.\n    print(json.dumps(results, separators=(',',':')))\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "A determinant approaching zero, $J \\approx 0$, often signals impending numerical failure or physically degenerate behavior like element inversion. This exercise moves beyond simple calculation to the crucial task of robustly *detecting* this condition, a key requirement for error control and adaptive meshing. You will leverage the Singular Value Decomposition (SVD) to engineer a scale-invariant test for near-singularity and analyze its sensitivity to perturbations compared to standard LU-based approaches .",
            "id": "3605400",
            "problem": "You are given square matrices representing the deformation gradient in three dimensions, denoted by $F \\in \\mathbb{R}^{3 \\times 3}$, in a computational solid mechanics context. The deformation gradient maps differential line elements and its Jacobian $J = \\det(F)$ measures local volume change. In physically admissible deformations, $J$ is dimensionless and strictly positive, but robust numerical detection of values near zero is important for error control and mesh adaptivity.\n\nFoundational facts:\n- The Singular Value Decomposition (SVD) factorizes any real matrix as $F = U \\Sigma V^{T}$, where $U,V \\in \\mathbb{R}^{3 \\times 3}$ are orthogonal matrices (i.e., $U^{T} U = I$ and $V^{T} V = I$) and $\\Sigma = \\operatorname{diag}(\\sigma_1,\\sigma_2,\\sigma_3)$ with singular values $\\sigma_i \\ge 0$. The orthogonal factors can encode rotations or reflections; the transpose in $V^{T}$ aligns principal directions.\n- The determinant satisfies $\\det(F) = \\det(U)\\,\\det(\\Sigma)\\,\\det(V^{T})$. Since $\\det(U), \\det(V^{T}) \\in \\{-1,+1\\}$ and $\\det(\\Sigma) = \\prod_{i=1}^{3} \\sigma_i$, the sign of $\\det(F)$ is given by $\\operatorname{sign}(\\det(U)\\det(V^{T}))$ while $|\\det(F)| = \\prod_{i=1}^{3} \\sigma_i$.\n- The Lower-Upper decomposition with pivoting (LU) factorizes $F$ as $P F = L U$, where $P$ is a permutation matrix (orthogonal with entries $0$ and $1$), $L$ is unit lower-triangular, and $U$ is upper-triangular. The determinant is $\\det(F) = \\det(P) \\det(U)$ because $\\det(L)=1$. The matrix $P$ embodies row permutations; its determinant $\\det(P) \\in \\{-1,+1\\}$ equals the parity of the permutation.\n\nYour tasks:\n1. Derive and implement a robust near-zero test for $J$ based on the SVD that is scale-aware and numerically stable. The test shall use only quantities from the SVD of $F$, and must return a boolean indicating whether $|\\det(F)|$ is near zero. The test must be invariant to orthogonal factors and robust against underflow. You must base the test on:\n   - The singular value ratio $r = \\sigma_{\\min}/\\sigma_{\\max}$, which detects near-singularity through conditioning, and\n   - A normalized determinant magnitude $D_{\\mathrm{norm}} = \\prod_{i=1}^{3} (\\sigma_i/\\sigma_{\\max})$ to detect small volume changes relative to the dominant stretch.\n   Use machine precision $\\varepsilon$ for double precision ($\\varepsilon = \\text{machine epsilon}$). A matrix $F$ shall be classified as near-zero if $(r < \\sqrt{\\varepsilon}) \\lor (D_{\\mathrm{norm}} < \\varepsilon)$.\n2. Compute a signed $J$ via SVD using $\\det(F) = \\det(U)\\,\\det(\\Sigma)\\,\\det(V^{T})$, emphasizing the role of the transpose in $V^{T}$ to recover the correct sign, and compute $J$ via LU using $\\det(F) = \\det(P)\\det(U)$.\n3. Compare the sensitivity of the two approaches (SVD product of singular values versus LU-based determinant) under small perturbations. For each test case matrix $F$, generate $K$ random perturbations $\\delta F$ with Frobenius norm scaled to a specified noise level $\\eta$ times $\\|F\\|_F$, and compute the sample standard deviation of the resulting $J$ using both methods. Use a fixed random seed to ensure reproducibility.\n\nImportant mathematical and physical requirements:\n- Use only the principles and definitions stated above. The Jacobian $J$ is dimensionless. No physical units are required in the output.\n- The angle unit is not applicable in this problem.\n- The final computed metrics must be purely numeric: booleans, integers, floats, or lists of these types.\n\nTest suite:\nFor each case, $K=200$ perturbations, with noise levels $\\eta$ as indicated:\n- Case 1 (well-conditioned stretch): $F = \\operatorname{diag}(1.2,\\,0.9,\\,1.1)$, $\\eta = 10^{-9}$.\n- Case 2 (near-singular with positive orientation): $F = \\operatorname{diag}(1,\\,1,\\,10^{-8})$, $\\eta = 10^{-12}$.\n- Case 3 (near-singular with orientation reversal): $F = R\\,\\operatorname{diag}(1, 1, 10^{-8})$ where $R = \\operatorname{diag}(-1, 1, 1)$ (orthogonal with $\\det(R)=-1$), $\\eta = 10^{-12}$.\n- Case 4 (ill-conditioned shear): \n$$\nF = \\begin{bmatrix}\n1 & 1000 & 0 \\\\\n0 & 10^{-6} & 0 \\\\\n0 & 0 & 1\n\\end{bmatrix},\\quad \\eta = 10^{-12}.\n$$\n- Case 5 (extreme underflow boundary): $F = \\operatorname{diag}(10^{-300},\\,10^{-300},\\,10^{-300})$, $\\eta = 10^{-310}$.\n\nPerturbation model:\n- For each case, draw $\\delta F$ with independent standard normal entries, and scale it so that $\\|\\delta F\\|_F = \\eta \\|F\\|_F$. Then evaluate $J(F+\\delta F)$.\n\nRequired outputs for each test case:\n- A sub-list of the form [`near_zero`, `sign_match`, `sigma_J_SVD`, `sigma_J_LU`, `Delta_rel`, `J_SVD`, `J_LU`] where:\n  - `near_zero` is a boolean from the robust test in item 1,\n  - `sign_match` is a boolean indicating whether $\\operatorname{sign}(J_{\\mathrm{SVD}}) = \\operatorname{sign}(J_{\\mathrm{LU}})$ (treat both zero as matching),\n  - `sigma_J_SVD` and `sigma_J_LU` are the sample standard deviations of $J$ under perturbation for the SVD and LU approaches, respectively,\n  - `Delta_rel` = $\\frac{|J_{\\mathrm{SVD}} - J_{\\mathrm{LU}}|}{\\max(|J_{\\mathrm{SVD}}|,|J_{\\mathrm{LU}}|,10^{-300})}$ is the relative absolute difference between the baseline determinants from the two methods,\n  - `J_SVD` is the signed determinant computed from $U$, $\\Sigma$, and $V^{T}$,\n  - `J_LU` is the determinant from LU via $\\det(P)\\det(U)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element corresponds to one test case and is itself a list as specified above. For example: \n\"[[result_case1],[result_case2],[result_case3],[result_case4],[result_case5]]\".",
            "solution": "The problem requires a thorough numerical analysis of the determinant of the deformation gradient tensor $F$, a fundamental quantity in computational solid mechanics. The analysis involves comparing two standard methods for computing the determinant, one based on Singular Value Decomposition (SVD) and the other on Lower-Upper (LU) decomposition with pivoting. The core tasks are to implement a robust test for near-singularity, compute the determinant using both methods, and evaluate the numerical stability of each method under small perturbations.\n\nThe logical flow for solving this problem is as follows:\n1.  For each test matrix $F$, calculate its determinant using both SVD-based and LU-based methods, denoted as $J_{\\mathrm{SVD}}$ and $J_{\\mathrm{LU}}$, respectively.\n2.  Implement the specified robust near-zero test for the determinant's magnitude based on the singular values of $F$.\n3.  For each $F$, conduct a Monte Carlo simulation by generating $K=200$ perturbed matrices $F' = F + \\delta F$, where the perturbation $\\delta F$ is scaled to a specified noise level $\\eta$.\n4.  For each perturbed matrix $F'$, compute its determinant using both the SVD and LU methods.\n5.  Calculate the sample standard deviation of the determinants obtained from the perturbed matrices for both methods, denoted $\\sigma_{J}^{\\mathrm{SVD}}$ and $\\sigma_{J}^{\\mathrm{LU}}$.\n6.  Finally, aggregate all computed quantities for each test case into the specified output format.\n\nAll mathematical operations will be performed using double-precision floating-point arithmetic, with machine epsilon $\\varepsilon$ taken as `numpy.finfo(float).eps`.\n\n**1. Robust Near-Zero Test**\n\nThe problem defines a specific test to classify if a matrix $F$ has a determinant whose magnitude is \"near-zero\" in a numerically robust sense. This test is based exclusively on the singular values of $F$, obtained from its SVD, $F = U \\Sigma V^{T}$, where $\\Sigma = \\operatorname{diag}(\\sigma_1, \\sigma_2, \\sigma_3)$ with $\\sigma_1 \\ge \\sigma_2 \\ge \\sigma_3 \\ge 0$. Let $\\sigma_{\\max} = \\sigma_1$ and $\\sigma_{\\min} = \\sigma_3$.\n\nThe test relies on two dimensionless quantities:\n- The singular value ratio: $r = \\sigma_{\\min} / \\sigma_{\\max}$. This is the reciprocal of the matrix condition number $\\kappa(F)$. A small value of $r$ indicates that the matrix is ill-conditioned and close to being singular (i.e., its rank is deficient).\n- The normalized determinant magnitude: $D_{\\mathrm{norm}} = \\prod_{i=1}^{3} (\\sigma_i / \\sigma_{\\max})$. This quantity measures the volume change relative to the cube defined by the maximum stretch. It is designed to be robust against uniform scaling and underflow. For instance, if $F$ is scaled by a very small number $\\alpha$, i.e., $F \\to \\alpha F$, then $\\sigma_i \\to \\alpha \\sigma_i$, but $D_{\\mathrm{norm}}$ remains unchanged.\n\nA matrix $F$ is classified as having a near-zero determinant if it is either ill-conditioned or exhibits a significant relative volume collapse. The condition is formally stated as:\n$$\n(\\text{near\\_zero}) \\equiv (r < \\sqrt{\\varepsilon}) \\lor (D_{\\mathrm{norm}} < \\varepsilon)\n$$\nwhere $\\varepsilon$ is the machine epsilon for double precision floating-point numbers. The choice of $\\sqrt{\\varepsilon}$ as a threshold for the condition number is a common heuristic in numerical analysis.\n\n**2. Determinant Calculation Methods**\n\nTwo distinct methods are used to calculate the determinant $J = \\det(F)$.\n\n**a) SVD-based Determinant ($J_{\\mathrm{SVD}}$)**\nFrom the SVD factorization $F = U \\Sigma V^{T}$, the determinant is given by the product of the determinants of the factors:\n$$\nJ_{\\mathrm{SVD}} = \\det(F) = \\det(U) \\det(\\Sigma) \\det(V^{T})\n$$\nSince $U$ and $V$ are orthogonal matrices, their determinants are either $+1$ or $-1$. The determinant of the diagonal matrix $\\Sigma$ is the product of its diagonal entries, which are the singular values $\\sigma_i$.\n$$\n\\det(\\Sigma) = \\prod_{i=1}^{3} \\sigma_i\n$$\nThe product of singular values gives the magnitude of the determinant, $|\\det(F)|$, while the product $\\det(U)\\det(V^T)$ provides its sign. The calculation is performed as $J_{\\mathrm{SVD}} = \\det(U) \\cdot (\\prod_{i} \\sigma_i) \\cdot \\det(V^T)$. A direct computation of $\\prod \\sigma_i$ can lead to underflow if the singular values are very small, even if the normalized test quantities are well-behaved.\n\n**b) LU-based Determinant ($J_{\\mathrm{LU}}$)**\nThe LU decomposition with partial pivoting, as defined in the problem, factorizes $F$ such that $P F = L U$. Here, $P$ is a permutation matrix, $L$ is a unit lower-triangular matrix, and $U$ is an upper-triangular matrix.\nTaking the determinant of both sides:\n$$\n\\det(P F) = \\det(L U) \\implies \\det(P) \\det(F) = \\det(L) \\det(U)\n$$\nSince $L$ is unit-triangular, $\\det(L) = 1$. The determinant of the permutation matrix, $\\det(P)$, is the sign of the permutation, equal to $(-1)^k$ where $k$ is the number of row swaps. This gives:\n$$\n\\det(F) = \\frac{\\det(U)}{\\det(P)}\n$$\nAs $\\det(P) \\in \\{-1, +1\\}$, we have $1/\\det(P) = \\det(P)$. The determinant of the upper-triangular matrix $U$ is the product of its diagonal elements, $\\prod_{i} U_{ii}$. Therefore,\n$$\nJ_{\\mathrm{LU}} = \\det(F) = \\det(P) \\prod_{i=1}^{3} U_{ii}\n$$\nThe `scipy.linalg.lu` function produces a factorization $F=PLU$. To align with the problem's $PF=LU$ convention, we can premultiply by $P^T$: $P^T F = P^T(PLU) = (P^T P)LU = ILU = LU$. This means the permutation matrix from the problem is $P_{prob} = P_{scipy}^T$, and the $L$ and $U$ factors are the same. The determinant is then $\\det(F) = \\det(P_{prob})\\det(U) = \\det(P_{scipy}^T)\\det(U) = \\det(P_{scipy})\\det(U)$. Thus, the formula remains the same regardless of the convention, as long as the factors are used consistently.\n\n**3. Perturbation Analysis**\n\nTo assess the numerical sensitivity of each method, a perturbation analysis is performed. For each test case $F$, we generate $K=200$ random perturbation matrices $\\delta F$. Each $\\delta F$ is constructed by first drawing entries from an independent standard normal distribution, $\\mathcal{N}(0, 1)$, to form a raw matrix $\\delta F_{\\text{raw}}$. This matrix is then scaled to match a target noise level relative to the Frobenius norm of $F$, $\\|F\\|_F = \\sqrt{\\sum_{i,j} F_{ij}^2}$.\n$$\n\\delta F = \\delta F_{\\text{raw}} \\cdot \\frac{\\eta \\|F\\|_F}{\\|\\delta F_{\\text{raw}}\\|_F}\n$$\nFor each of the $K$ perturbed matrices $F' = F+\\delta F$, we compute the determinant $J(F')$ using both the SVD and LU methods. The sample standard deviation ($\\sigma_J$) of the resulting set of $K$ determinant values is then calculated for each method. A smaller standard deviation implies greater numerical stability with respect to small input perturbations. A fixed random seed is used to ensure reproducibility.\n\n**4. Output Metrics**\n\nFor each test case, the following seven metrics are computed and reported:\n1.  $\\text{near\\_zero}$: A boolean result from the robust test.\n2.  $\\text{sign\\_match}$: A boolean, `True` if $\\operatorname{sign}(J_{\\mathrm{SVD}}) = \\operatorname{sign}(J_{\\mathrm{LU}})$.\n3.  $\\sigma_{J}^{\\mathrm{SVD}}$: The sample standard deviation of SVD-based determinants under perturbation.\n4.  $\\sigma_{J}^{\\mathrm{LU}}$: The sample standard deviation of LU-based determinants under perturbation.\n5.  $\\Delta_{\\mathrm{rel}}$: The relative difference between the baseline (unperturbed) determinants, defined as $\\frac{|J_{\\mathrm{SVD}} - J_{\\mathrm{LU}}|}{\\max(|J_{\\mathrm{SVD}}|,|J_{\\mathrm{LU}}|,10^{-300})}$. The floor value $10^{-300}$ prevents division by zero.\n6.  $J_{\\mathrm{SVD}}$: The baseline determinant from the SVD method.\n7.  $J_{\\mathrm{LU}}$: The baseline determinant from the LU method.\n\nThis comprehensive analysis will highlight the strengths and weaknesses of each determinant calculation method across a range of scenarios, including well-conditioned, ill-conditioned, and near-underflow regimes.",
            "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Solves the computational solid mechanics problem by analyzing the determinant\n    of various deformation gradient matrices.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    K = 200  # Number of perturbations\n    RANDOM_SEED = 42 # For reproducibility\n    EPSILON = np.finfo(float).eps # Machine epsilon for double precision\n\n    # --- Test Cases ---\n    F1 = np.diag([1.2, 0.9, 1.1])\n    F2 = np.diag([1.0, 1.0, 1e-8])\n    R3 = np.diag([-1.0, 1.0, 1.0])\n    F3 = R3 @ np.diag([1.0, 1.0, 1e-8])\n    F4 = np.array([[1.0, 1000.0, 0.0], [0.0, 1e-6, 0.0], [0.0, 0.0, 1.0]])\n    F5 = np.diag([1e-300, 1e-300, 1e-300])\n\n    test_cases = [\n        (F1, 1e-9),\n        (F2, 1e-12),\n        (F3, 1e-12),\n        (F4, 1e-12),\n        (F5, 1e-310),\n    ]\n\n    all_results = []\n\n    def get_det_from_svd(F_matrix):\n        \"\"\"Computes determinant from SVD factors.\"\"\"\n        try:\n            u, s, vh = np.linalg.svd(F_matrix)\n            det_u = np.linalg.det(u)\n            det_vh = np.linalg.det(vh)\n            # Direct product of singular values can underflow, which is part of the test.\n            det_s = np.prod(s)\n            return det_u * det_s * det_vh\n        except np.linalg.LinAlgError:\n            return 0.0\n\n    def get_det_from_lu(F_matrix):\n        \"\"\"Computes determinant from LU factors.\"\"\"\n        try:\n            # The scipy.linalg.lu function returns P, L, U such that F = P @ L @ U.\n            # The problem defines PF = LU. This implies P_prob = P_scipy.T.\n            # det(F) = det(P_prob)*det(U) = det(P_scipy.T)*det(U) = det(P_scipy)*det(U).\n            # So the determinant is det(P)*product(diag(U)) from scipy's output.\n            p, l, u = scipy.linalg.lu(F_matrix)\n            det_p = np.linalg.det(p)\n            det_u = np.prod(np.diag(u))\n            return det_p * det_u\n        except (np.linalg.LinAlgError, ValueError):\n            # ValueError can be raised if matrix is singular for LU\n            return 0.0\n\n    for F, eta in test_cases:\n        # --- Task 1: Robust Near-Zero Test & Task 2: Baseline Determinants ---\n        u_base, s_base, vh_base = np.linalg.svd(F)\n        \n        # Near-zero test\n        s_max = np.max(s_base) if s_base.size > 0 else 0.0\n        s_min = np.min(s_base) if s_base.size > 0 else 0.0\n\n        if s_max > 0:\n            r = s_min / s_max\n            #  D_norm should be computed carefully to avoid intermediate underflow before s_max division\n            d_norm = np.prod(s_base / s_max)\n        else: # Zero matrix case\n            r = 0.0\n            d_norm = 0.0\n\n        near_zero = (r < np.sqrt(EPSILON)) or (d_norm < EPSILON)\n\n        # Baseline determinants\n        J_svd = get_det_from_svd(F)\n        J_lu = get_det_from_lu(F)\n\n        # Sign match\n        sign_match = np.sign(J_svd) == np.sign(J_lu)\n\n        # Relative difference\n        denominator = max(abs(J_svd), abs(J_lu), 1e-300)\n        delta_rel = abs(J_svd - J_lu) / denominator\n\n        # --- Task 3: Perturbation Analysis ---\n        np.random.seed(RANDOM_SEED)\n        dets_svd = []\n        dets_lu = []\n\n        norm_F = np.linalg.norm(F, 'fro')\n        target_norm_delta = eta * norm_F\n\n        for _ in range(K):\n            delta_F_raw = np.random.randn(3, 3)\n            norm_delta_F_raw = np.linalg.norm(delta_F_raw, 'fro')\n            \n            if norm_delta_F_raw > 0:\n                delta_F = delta_F_raw * (target_norm_delta / norm_delta_F_raw)\n            else: # Statistically unlikely case\n                delta_F = np.zeros((3, 3))\n            \n            F_perturbed = F + delta_F\n            \n            dets_svd.append(get_det_from_svd(F_perturbed))\n            dets_lu.append(get_det_from_lu(F_perturbed))\n\n        # Sample standard deviation (ddof=1)\n        sigma_J_svd = np.std(dets_svd, ddof=1)\n        sigma_J_lu = np.std(dets_lu, ddof=1)\n        \n        # --- Assemble Results ---\n        case_results = [\n            near_zero,\n            sign_match,\n            sigma_J_svd,\n            sigma_J_lu,\n            delta_rel,\n            J_svd,\n            J_lu\n        ]\n        all_results.append(case_results)\n\n    # --- Final Output Formatting ---\n    # Convert list of lists to the required string format \"[[...],[...],...]\"\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Many advanced constitutive models for large deformations require the natural logarithm of the Jacobian, $\\ln(J)$. However, a naive computation of $\\ln(\\det(F))$ is dangerously susceptible to numerical overflow or underflow when the deformation gradient $F$ involves extreme scales. This capstone practice challenges you to derive and implement a professionally robust algorithm from first principles, using the SVD and dynamic scaling to compute $\\ln(J)$ accurately even for severely ill-conditioned tensors .",
            "id": "3605427",
            "problem": "Design a numerically stable algorithm to compute the natural logarithm of the Jacobian determinant, $\\,\\ln J\\,$, where $\\,J = \\det F\\,$ and $\\,F \\in \\mathbb{R}^{n \\times n}\\,$ is a real square matrix representing a deformation gradient in computational solid mechanics. Assume that $\\,F\\,$ may be ill-conditioned, and that in physically meaningful cases $\\,J>0\\,$, but allow that $\\,J<0\\,$ can arise in artificial tests. Your solution must proceed from first principles and core definitions. Begin only from the following foundational facts: (i) the existence of the Singular Value Decomposition (SVD) stating that any real matrix $\\,F\\,$ admits $\\,F = U \\Sigma V^{T}\\,$ with $\\,U, V\\,$ orthogonal and $\\,\\Sigma\\,$ diagonal with nonnegative entries called singular values, (ii) the determinant of an orthogonal matrix has magnitude $\\,1\\,$, (iii) the definition of the determinant as a multilinear, alternating form equal to the product of eigenvalues for normal matrices, and (iv) the Fréchet differential of the determinant satisfies $\\,\\mathrm{d}(\\det F)[\\delta F] = \\det(F)\\,\\mathrm{tr}(F^{-1}\\delta F)\\,$ for invertible $\\,F\\,$, implying $\\,\\mathrm{d}(\\ln \\det F)[\\delta F] = \\mathrm{tr}(F^{-1}\\delta F)\\,$. Do not assume or state any closed-form shortcut for $\\,\\ln \\det F\\,$ in terms of the singular values; instead, derive any such expression you use from these bases.\n\nYour task has three parts:\n\n1) Derivation and conditioning. Using only the foundational facts above, derive an expression for $\\,\\ln | \\det F |\\,$ in terms of quantities from the Singular Value Decomposition (SVD) and justify, using perturbation analysis and the Fréchet differential, the conditioning of this computation. Your justification must identify which aspects of $\\,F\\,$ control sensitivity and must connect to the smallest singular value. Provide a bound for the absolute perturbation $\\,|\\delta(\\ln |\\det F|)|\\,$ in terms of a matrix norm of $\\,F^{-1}\\,$ and a norm of $\\,\\delta F\\,$.\n\n2) Numerically stable algorithm. Propose a numerically stable algorithm to compute $\\,\\ln |\\det F|\\,$ for ill-conditioned $\\,F\\,$ by using only operations that do not suffer from catastrophic underflow or overflow. Your algorithm must:\n- Use the Singular Value Decomposition (SVD) $\\,F = U \\Sigma V^{T}\\,$ explicitly in its design.\n- Employ dynamic scaling by a power of two to prevent intermediate overflow or underflow in floating-point arithmetic. Let $\\,k = \\lfloor \\log_{2}\\|F\\|_{F}\\rfloor\\,$, scale by $\\,a = 2^{-k}\\,$, and relate $\\,\\ln |\\det F|\\,$ to the singular values of $\\,aF\\,$ without multiplying singular values together.\n- Also compute the orientation sign $\\,s = \\operatorname{sign}(\\det F)\\in\\{-1,+1\\}\\,$ from orthogonal factors.\n\n3) Implementation and test suite. Implement your algorithm as a program that takes no input and evaluates a fixed set of test matrices $\\,F_i\\,$, producing the required outputs. Angles are to be interpreted in radians, and logarithms are natural logarithms. The program shall evaluate the following five square matrices in $\\,\\mathbb{R}^{3\\times 3}\\,$:\n- Case $\\,1\\,$ (well-conditioned, orientation-preserving): $\\,F_1 = R_z(\\theta_1)\\,\\mathrm{diag}(2.0,\\,0.5,\\,1.5)\\,R_y(\\phi_1)^{T}\\,$ with $\\,\\theta_1=0.7\\,$ and $\\,\\phi_1=-0.4\\,$.\n- Case $\\,2\\,$ (highly ill-conditioned but with unit volume change): $\\,F_2 = R_x(\\alpha_2)\\,\\mathrm{diag}(10^{-12},\\,1.0,\\,10^{12})\\,R_z(\\beta_2)^{T}\\,$ with $\\,\\alpha_2=0.3\\,$ and $\\,\\beta_2=1.2\\,$.\n- Case $\\,3\\,$ (very large scales, risk of overflow if naively multiplied): $\\,F_3 = R_y(\\alpha_3)\\,\\mathrm{diag}(10^{100},\\,2\\cdot 10^{100},\\,5\\cdot 10^{99})\\,R_x(\\beta_3)^{T}\\,$ with $\\,\\alpha_3=0.9\\,$ and $\\,\\beta_3=-1.1\\,$.\n- Case $\\,4\\,$ (extreme scale separation, risk of underflow if naively multiplied): $\\,F_4 = R_z(\\theta_4)\\,\\mathrm{diag}(10^{-300},\\,10^{-10},\\,10^{20})\\,R_y(\\phi_4)^{T}\\,$ with $\\,\\theta_4=-0.8\\,$ and $\\,\\phi_4=0.6\\,$.\n- Case $\\,5\\,$ (orientation-reversing): $\\,F_5 = R_x(\\alpha_5)\\,\\mathrm{diag}(-2.0,\\,3.0,\\,5.0)\\,R_y(\\beta_5)^{T}\\,$ with $\\,\\alpha_5=0.5\\,$ and $\\,\\beta_5=-0.7\\,$.\n\nHere, for $\\,\\gamma \\in \\mathbb{R}\\,$,\n$$\nR_x(\\gamma)=\\begin{bmatrix}\n1 & 0 & 0\\\\\n0 & \\cos\\gamma & -\\sin\\gamma\\\\\n0 & \\sin\\gamma & \\cos\\gamma\n\\end{bmatrix},\\quad\nR_y(\\gamma)=\\begin{bmatrix}\n\\cos\\gamma & 0 & \\sin\\gamma\\\\\n0 & 1 & 0\\\\\n-\\sin\\gamma & 0 & \\cos\\gamma\n\\end{bmatrix},\\quad\nR_z(\\gamma)=\\begin{bmatrix}\n\\cos\\gamma & -\\sin\\gamma & 0\\\\\n\\sin\\gamma & \\cos\\gamma & 0\\\\\n0 & 0 & 1\n\\end{bmatrix}.\n$$\n\nFor each case, your program must output a list $\\,[\\ln|\\det F_i|,\\,s_i]\\,$, where $\\,s_i \\in\\{-1,+1\\}\\,$ is the orientation sign computed from orthogonal SVD factors. Aggregate all five results into one list. The final output format must be a single line containing a comma-separated list of the five inner lists enclosed in square brackets, for example `[[l_1,s_1],[l_2,s_2],...,[l_5,s_5]]`, with no spaces.\n\nYour implementation must use the natural logarithm and radians for angles and must not read any input. The outputs are floating-point numbers for $\\,\\ln|\\det F_i|\\,$ and integers for $\\,s_i\\,$. This constitutes the test suite and output specification.",
            "solution": "The task is to design, justify, and implement a numerically stable algorithm for computing the natural logarithm of the Jacobian determinant, $\\ln J = \\ln(\\det F)$, for a real square matrix $F \\in \\mathbb{R}^{n \\times n}$ which may be ill-conditioned. The solution is presented in three parts as requested: derivation and conditioning analysis, algorithm design, and implementation.\n\n**Part 1: Derivation and Conditioning Analysis**\n\nThis part derives an expression for $\\ln|\\det F|$ based on the Singular Value Decomposition (SVD) and analyzes the conditioning of the problem.\n\n**Derivation of $\\ln|\\det F|$**\n\nWe are given the following foundational facts:\n(i) Any real matrix $F \\in \\mathbb{R}^{n \\times n}$ admits a Singular Value Decomposition (SVD), $F = U \\Sigma V^{T}$, where $U, V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix with non-negative entries $\\sigma_i \\ge 0$, known as the singular values of $F$.\n(ii) The determinant of any orthogonal matrix has a magnitude of $1$.\n(iii) The determinant is a multilinear, alternating form, and for a normal matrix, it is the product of its eigenvalues.\n\nUsing the multiplicative property of determinants, we can write the determinant of $F$ as:\n$$\n\\det F = \\det(U \\Sigma V^{T}) = (\\det U) (\\det \\Sigma) (\\det V^{T})\n$$\nFrom fact (ii), as $U$ and $V$ are orthogonal, their determinants are $\\det U = \\pm 1$ and $\\det V = \\pm 1$. Since the determinant of a transpose of a matrix is equal to the determinant of the matrix itself, $\\det V^T = \\det V = \\pm 1$.\n\nThe matrix $\\Sigma$ is a diagonal matrix, which is a special case of a normal matrix. From fact (iii), its determinant is the product of its eigenvalues. The eigenvalues of a diagonal matrix are its diagonal entries, which in this case are the singular values $\\sigma_i$ of $F$. Therefore:\n$$\n\\det \\Sigma = \\prod_{i=1}^{n} \\sigma_i\n$$\nCombining these results, we obtain an expression for the determinant of $F$ in terms of its SVD components:\n$$\n\\det F = (\\det U) (\\det V^T) \\prod_{i=1}^{n} \\sigma_i\n$$\nTo find an expression for $\\ln|\\det F|$, we first take the absolute value of the determinant:\n$$\n|\\det F| = |(\\det U) (\\det V^T)| \\left| \\prod_{i=1}^{n} \\sigma_i \\right|\n$$\nSince $|\\det U| = 1$, $|\\det V^T| = 1$, and singular values $\\sigma_i$ are non-negative, the product $\\prod_{i=1}^{n} \\sigma_i$ is also non-negative. Thus, we have:\n$$\n|\\det F| = \\prod_{i=1}^{n} \\sigma_i\n$$\nThis expression demonstrates that the volume-change factor $|\\det F|$ is determined solely by the product of the singular values. Applying the natural logarithm to both sides, and using the property that the logarithm of a product is the sum of the logarithms, we arrive at the desired expression:\n$$\n\\ln|\\det F| = \\ln\\left(\\prod_{i=1}^{n} \\sigma_i\\right) = \\sum_{i=1}^{n} \\ln \\sigma_i\n$$\nThis derivation relies only on the foundational facts provided. This formula is the basis for a numerically stable computation, as it avoids the explicit multiplication of singular values, which could lead to numerical overflow or underflow.\n\n**Conditioning Analysis**\n\nWe are given fact (iv), the Fréchet differential of the determinant, which for an invertible matrix $F$ leads to the differential of its natural logarithm:\n$$\n\\mathrm{d}(\\ln \\det F)[\\delta F] = \\mathrm{tr}(F^{-1} \\delta F)\n$$\nwhere $\\delta F$ is a perturbation to the matrix $F$. For a small perturbation, this implies that the change in $\\ln(\\det F)$, denoted $\\delta(\\ln \\det F)$, can be approximated by:\n$$\n\\delta(\\ln \\det F) \\approx \\mathrm{tr}(F^{-1} \\delta F)\n$$\nThe problem concerns $\\ln|\\det F|$. If we assume the perturbation $\\delta F$ is small enough such that $\\mathrm{sign}(\\det(F+\\delta F)) = \\mathrm{sign}(\\det F)$, then $\\delta(\\ln|\\det F|) = \\delta(\\ln(\\det F))$ if $\\det F > 0$, and $\\delta(\\ln(-\\det F))$ if $\\det F < 0$. In both cases, the differential is $\\mathrm{tr}(F^{-1}\\delta F)$. So, we can analyze the perturbation of $\\ln|\\det F|$ as:\n$$\n|\\delta(\\ln|\\det F|)| \\approx |\\mathrm{tr}(F^{-1} \\delta F)|\n$$\nTo bound this expression, we can use the Cauchy-Schwarz inequality for the Frobenius inner product of matrices, which is defined for real matrices $A, B \\in \\mathbb{R}^{n \\times n}$ as $\\langle A, B \\rangle_F = \\mathrm{tr}(A^T B)$. The inequality states $|\\langle A, B \\rangle_F| \\le \\|A\\|_F \\|B\\|_F$. Letting $A = (F^{-1})^T$ and $B = \\delta F$, we get:\n$$\n|\\mathrm{tr}(((F^{-1})^T)^T \\delta F)| = |\\mathrm{tr}(F^{-1} \\delta F)| \\le \\|(F^{-1})^T\\|_F \\|\\delta F\\|_F\n$$\nSince the Frobenius norm is invariant under transposition, $\\|(F^{-1})^T\\|_F = \\|F^{-1}\\|_F$, we obtain the bound:\n$$\n|\\delta(\\ln|\\det F|)| \\lesssim \\|F^{-1}\\|_F \\|\\delta F\\|_F\n$$\nThis inequality relates the absolute perturbation in $\\ln|\\det F|$ to the norms of the inverse of $F$ and the perturbation $\\delta F$.\n\nTo connect this sensitivity to the singular values of $F$, we express $\\|F^{-1}\\|_F$ in terms of the singular values of $F$. The SVD of $F^{-1}$ is $F^{-1} = (U \\Sigma V^T)^{-1} = V \\Sigma^{-1} U^T$. The singular values of $F^{-1}$ are the reciprocals of the singular values of $F$, i.e., $1/\\sigma_i$. The Frobenius norm of $F^{-1}$ is the square root of the sum of squares of its singular values:\n$$\n\\|F^{-1}\\|_F = \\sqrt{\\sum_{i=1}^{n} \\left(\\frac{1}{\\sigma_i}\\right)^2}\n$$\nLet $\\sigma_{\\min} = \\min_{i} \\{\\sigma_i\\}$ be the smallest singular value of $F$. The norm of the inverse is then bounded below by $\\|F^{-1}\\|_F \\ge \\sqrt{(1/\\sigma_{\\min})^2} = 1/\\sigma_{\\min}$. If $\\sigma_{\\min}$ is very small, $F$ is ill-conditioned (close to singular), and $\\|F^{-1}\\|_F$ becomes very large. Consequently, the term $\\|F^{-1}\\|_F \\|\\delta F\\|_F$ can be large even for a small perturbation $\\|\\delta F\\|_F$, indicating that the computation of $\\ln|\\det F|$ is sensitive to perturbations in $F$. The conditioning of the problem is thus critically controlled by the smallest singular value of $F$.\n\n**Part 2: Numerically Stable Algorithm**\n\nA naive computation of $\\ln|\\det F|$ by first calculating $J = \\det F$ and then taking the logarithm is prone to numerical issues. If $F$ has singular values that are very large or very small, their product $J$ can easily overflow or underflow standard floating-point representations before the logarithm can be taken. The derived expression $\\ln|\\det F| = \\sum_i \\ln \\sigma_i$ circumvents this by converting the product into a sum, which is a much more stable operation.\n\nThe problem, however, specifies that the SVD computation itself might be affected by extreme scaling of $F$. To preempt this, a dynamic scaling procedure is required.\n\nThe proposed algorithm is as follows:\n\n1.  **Scaling**: Given the matrix $F \\in \\mathbb{R}^{n \\times n}$, compute its Frobenius norm, $\\|F\\|_F = \\sqrt{\\sum_{i,j=1}^{n} F_{ij}^2}$. To bring the matrix entries into a numerically favorable range (of order $1$), we scale $F$ by a power of $2$.\n    - Calculate the scaling exponent $k = \\lfloor \\log_2 \\|F\\|_F \\rfloor$. This can be computed using `floor(log2(norm))`.\n    - Define the scaling factor $a = 2^{-k}$.\n    - Form the scaled matrix $F' = aF$. The norm of this matrix will be in the range $1 \\le \\|F'\\|_F < 2$.\n\n2.  **SVD of Scaled Matrix**: Compute the SVD of the well-scaled matrix $F' = U' \\Sigma' (V')^T$. This yields the orthogonal matrices $U'$ and $(V')^T$, and the diagonal matrix $\\Sigma'$ containing the singular values $\\sigma'_i$ of $F'$.\n\n3.  **Compute $\\ln|\\det F|$**: We relate $\\det F$ to $\\det F'$. From $F' = aF$, we have $\\det(F') = \\det(aF) = a^n \\det F$. Thus, $\\det F = a^{-n} \\det F'$.\n    Taking the natural logarithm of the absolute value:\n    $$\n    \\ln|\\det F| = \\ln|a^{-n} \\det F'| = \\ln(a^{-n}) + \\ln|\\det F'| = -n \\ln(a) + \\sum_{i=1}^{n} \\ln \\sigma'_i\n    $$\n    Substituting $a = 2^{-k}$, we have $\\ln a = \\ln(2^{-k}) = -k \\ln 2$. The final expression for the computation is:\n    $$\n    \\ln|\\det F| = -n(-k \\ln 2) + \\sum_{i=1}^{n} \\ln \\sigma'_i = nk \\ln 2 + \\sum_{i=1}^{n} \\ln \\sigma'_i\n    $$\n    This computation is numerically stable, as it involves summing logarithms of the well-behaved singular values $\\sigma'_i$ and adding a scaled term. It completely avoids multiplying the original singular values $\\sigma_i = \\sigma'_i/a$.\n\n4.  **Compute Orientation Sign**: The orientation sign is $s = \\mathrm{sign}(\\det F)$. Since the scaling factor $a = 2^{-k}$ is positive, $\\mathrm{sign}(\\det F) = \\mathrm{sign}(\\det F')$.\n    From the SVD of $F'$, we have $\\det F' = (\\det U') (\\det \\Sigma') (\\det (V')^T)$. For a non-singular matrix, all singular values $\\sigma'_i$ are positive, so $\\det \\Sigma' = \\prod_i \\sigma'_i > 0$.\n    The sign is therefore determined by the determinants of the orthogonal factors:\n    $$\n    s = \\mathrm{sign}(\\det F) = \\mathrm{sign}((\\det U')(\\det(V')^T)) = (\\det U')(\\det(V')^T)\n    $$\n    The determinants of $U'$ and $(V')^T$ are computed numerically. Since they are known to be $\\pm 1$, the product can be rounded to the nearest integer to yield a robust sign of $\\pm 1$.\n\n**Summary of the Algorithm**\nFor a given matrix $F \\in \\mathbb{R}^{n \\times n}$:\na. If $F$ is the zero matrix, $|\\det F| = 0$ and $\\ln|\\det F| = -\\infty$. The sign is undefined, but can be taken as $0$ or $1$. We assume $F$ is non-zero.\nb. Compute the Frobenius norm $\\|F\\|_F$.\nc. Calculate the scaling exponent $k = \\lfloor \\log_2 \\|F\\|_F \\rfloor$ and the factor $a = 2^{-k}$.\nd. Construct the scaled matrix $F' = aF$.\ne. Perform SVD on $F'$ to obtain $U'$, singular values $\\sigma'_i$, and $V'^T$.\nf. Calculate $\\ln|\\det F| = n k \\ln(2) + \\sum_i \\ln(\\sigma'_i)$.\ng. Calculate the sign $s = \\mathrm{round}(\\det(U') \\cdot \\det(V'^T))$.\nh. Return the pair $[\\ln|\\det F|, s]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Designs and executes a numerically stable algorithm to compute ln|det(F)| and sign(det(F))\n    for a given set of test matrices F.\n    \"\"\"\n\n    def Rx(gamma):\n        \"\"\"Rotation matrix about x-axis.\"\"\"\n        c = np.cos(gamma)\n        s = np.sin(gamma)\n        return np.array([[1, 0, 0], [0, c, -s], [0, s, c]], dtype=np.float64)\n\n    def Ry(gamma):\n        \"\"\"Rotation matrix about y-axis.\"\"\"\n        c = np.cos(gamma)\n        s = np.sin(gamma)\n        return np.array([[c, 0, s], [0, 1, 0], [-s, 0, c]], dtype=np.float64)\n\n    def Rz(gamma):\n        \"\"\"Rotation matrix about z-axis.\"\"\"\n        c = np.cos(gamma)\n        s = np.sin(gamma)\n        return np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]], dtype=np.float64)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (well-conditioned, orientation-preserving)\n        Rz(0.7) @ np.diag([2.0, 0.5, 1.5]) @ Ry(-0.4).T,\n        # Case 2 (highly ill-conditioned but with unit volume change)\n        Rx(0.3) @ np.diag([1e-12, 1.0, 1e12]) @ Rz(1.2).T,\n        # Case 3 (very large scales, risk of overflow)\n        Ry(0.9) @ np.diag([1e100, 2e100, 5e99]) @ Rx(-1.1).T,\n        # Case 4 (extreme scale separation, risk of underflow)\n        Rz(-0.8) @ np.diag([1e-300, 1e-10, 1e20]) @ Ry(0.6).T,\n        # Case 5 (orientation-reversing)\n        Rx(0.5) @ np.diag([-2.0, 3.0, 5.0]) @ Ry(-0.7).T\n    ]\n\n    results = []\n    \n    # Process each test case using the derived numerically stable algorithm\n    for F in test_cases:\n        n = F.shape[0]\n\n        # Step 1: Scaling\n        norm_F = np.linalg.norm(F, 'fro')\n        \n        # Handle the case of a zero matrix, though not in test suite.\n        if norm_F == 0:\n            log_det_abs = -np.inf\n            sign_det = 0 # Or undefined. Problem asks for +/-1. Assume non-singular.\n            results.append([log_det_abs, sign_det])\n            continue\n            \n        k = np.floor(np.log2(norm_F))\n        a = 2.0**(-k)\n        \n        # Step 2: SVD of Scaled Matrix\n        F_scaled = a * F\n        U, s_scaled, Vt = np.linalg.svd(F_scaled)\n\n        # Step 3: Compute ln|det F|\n        # ln|det F| = nk*ln(2) + sum(ln(sigma_i_scaled))\n        log_det_abs = n * k * np.log(2) + np.sum(np.log(s_scaled))\n\n        # Step 4: Compute Orientation Sign\n        # sign(det F) = det(U) * det(Vt)\n        det_U = np.linalg.det(U)\n        det_Vt = np.linalg.det(Vt)\n        sign_det = int(np.round(det_U * det_Vt))\n        \n        results.append([log_det_abs, sign_det])\n\n    # Final print statement in the exact required format.\n    # Convert list of lists to string and remove spaces.\n    output_str = str(results).replace(' ', '')\n    print(output_str)\n\nsolve()\n```"
        }
    ]
}