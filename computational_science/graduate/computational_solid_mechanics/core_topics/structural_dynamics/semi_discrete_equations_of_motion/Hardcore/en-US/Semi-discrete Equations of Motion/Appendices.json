{
    "hands_on_practices": [
        {
            "introduction": "Explicit time integration schemes, such as the central difference method, are computationally efficient but are only conditionally stable, meaning the time step size must be smaller than a critical value. This practice guides you through the fundamental stability analysis to derive this critical time step, linking it directly to the system's highest natural frequency. You will then apply this principle to design and implement a practical adaptive time-stepping algorithm that ensures stability even for systems with time-varying stiffness .",
            "id": "3599566",
            "problem": "Consider the semi-discrete equations of motion for a linear elastic system obtained by finite element spatial discretization, given by $ \\mathbf{M} \\ddot{\\mathbf{q}}(t) + \\mathbf{K}(t)\\, \\mathbf{q}(t) = \\mathbf{f}(t) $, where $ \\mathbf{M} \\in \\mathbb{R}^{n \\times n} $ is a symmetric positive-definite mass matrix, $ \\mathbf{K}(t) \\in \\mathbb{R}^{n \\times n} $ is a symmetric positive-definite stiffness matrix that may vary with time, and $ \\mathbf{f}(t) $ is an external force vector. Assume free vibration for stability analysis, i.e., $ \\mathbf{f}(t) = \\mathbf{0} $. The explicit central difference time integration scheme advances $ \\mathbf{q}(t) $ at discrete time levels with uniform time step $ \\Delta t $ through the recurrence relation derived from discretizing $ \\ddot{\\mathbf{q}}(t) $ and substituting the semi-discrete equations of motion.\n\nStarting strictly from Newton's Second Law and the finite element semi-discretization, derive from first principles the spectral stability condition for the explicit central difference method applied to $ \\mathbf{M} \\ddot{\\mathbf{q}}(t) + \\mathbf{K}(t)\\, \\mathbf{q}(t) = \\mathbf{0} $. Show that stability requires $ \\Delta t \\leq \\Delta t_{\\mathrm{crit}} $ with $ \\Delta t_{\\mathrm{crit}} = \\dfrac{2}{\\omega_{\\max}} $ where $ \\omega_{\\max} = \\sqrt{\\lambda_{\\max}} $ and $ \\lambda_{\\max} $ is the largest generalized eigenvalue of the symmetric generalized eigenproblem $ \\mathbf{K}\\, \\boldsymbol{\\phi} = \\lambda\\, \\mathbf{M}\\, \\boldsymbol{\\phi} $. Equivalently, justify that $ \\lambda_{\\max} $ equals the spectral radius of the similarity-transformed matrix $ \\mathbf{M}^{-1/2}\\mathbf{K}\\mathbf{M}^{-1/2} $, which has the same eigenvalues as $ \\mathbf{M}^{-1}\\mathbf{K} $.\n\nNext, design a safe adaptive time stepping rule for the scenario where $ \\mathbf{K}(t) $ changes with time, ensuring that the explicit central difference scheme remains stable. The rule must satisfy the following constraints:\n\n- It must employ a safety factor $ s \\in (0,1) $ applied to the instantaneous stability limit, i.e., $ \\Delta t_{\\mathrm{raw}}(t) = s \\dfrac{2}{\\sqrt{\\lambda_{\\max}(t)}} $.\n- It must limit the rate of change of the time step. Let $ r_{\\mathrm{up}} > 1 $ be the maximum allowed multiplicative increase per step and $ r_{\\mathrm{down}} > 1 $ be the maximum allowed multiplicative decrease per step. The update from $ \\Delta t_n $ to $ \\Delta t_{n+1} $ at times $ t_n $ and $ t_{n+1} $ must satisfy\n  $$ \\Delta t_{n+1} \\leq \\min\\!\\big( \\Delta t_{\\mathrm{raw}}(t_{n+1}),\\, r_{\\mathrm{up}}\\, \\Delta t_n \\big), $$\n  and when $ \\Delta t_{\\mathrm{raw}}(t_{n+1}) $ permits, avoid decreasing faster than the bound\n  $$ \\Delta t_{n+1} \\geq \\frac{\\Delta t_n}{r_{\\mathrm{down}}}. $$\n  If the stability requirement enforces a sharper drop, then $ \\Delta t_{n+1} $ must be set to $ \\Delta t_{\\mathrm{raw}}(t_{n+1}) $ to ensure stability.\n- It must enforce $ \\Delta t_{\\min} \\leq \\Delta t_n \\leq \\Delta t_{\\max} $ for all time levels $ n $, where $ \\Delta t_{\\min} $ and $ \\Delta t_{\\max} $ are prescribed bounds.\n\nImplement this for the following test suite. For each case, compute the required quantities and return the specified outputs. All time step values must be expressed in seconds. All floating-point outputs must be rounded to eight decimal places.\n\n- Case 1 (constant stiffness, verification of $ \\Delta t_{\\mathrm{crit}} $ and constancy under adaptation):\n  - $ n = 2 $, $ \\mathbf{M} = \\mathrm{diag}(2.0,\\, 1.0)\\, \\mathrm{kg} $,\n  - Springs to walls and between masses define $ \\mathbf{K} $ by $ k_1 = 1200\\, \\mathrm{N/m} $, $ k_2 = 700\\, \\mathrm{N/m} $, $ k_3 = 900\\, \\mathrm{N/m} $, with\n    $$ \\mathbf{K} = \\begin{bmatrix} k_1 + k_2 & -k_2 \\\\ -k_2 & k_2 + k_3 \\end{bmatrix}. $$\n  - Use $ s = 0.95 $, $ r_{\\mathrm{up}} = 1.2 $, $ r_{\\mathrm{down}} = 1.5 $, $ \\Delta t_{\\min} = 10^{-6}\\, \\mathrm{s} $, $ \\Delta t_{\\max} = 10^{-2}\\, \\mathrm{s} $. Sample $ 21 $ uniformly spaced times in $ [0,\\, 1]\\, \\mathrm{s} $ (although $ \\mathbf{K} $ is constant).\n  - Output a list $ [\\Delta t_{\\mathrm{crit}},\\, \\Delta t_{\\mathrm{start}},\\, \\Delta t_{\\mathrm{end}}] $ in seconds, where $ \\Delta t_{\\mathrm{crit}} = \\dfrac{2}{\\sqrt{\\lambda_{\\max}}} $, $ \\Delta t_{\\mathrm{start}} $ is the first adaptive time step, and $ \\Delta t_{\\mathrm{end}} $ is the last adaptive time step.\n\n- Case 2 (gradually stiffening system, coverage of adaptation under smooth change):\n  - $ n = 3 $, $ \\mathbf{M} = \\mathrm{diag}(1.0,\\, 1.5,\\, 2.0)\\, \\mathrm{kg} $,\n  - Base stiffness from a three-mass chain with springs to walls and between adjacent masses:\n    $$ k_{\\mathrm{w}1} = 1000\\, \\mathrm{N/m},\\quad k_{12} = 1500\\, \\mathrm{N/m},\\quad k_{23} = 1200\\, \\mathrm{N/m},\\quad k_{\\mathrm{w}3} = 800\\, \\mathrm{N/m}, $$\n    $$ \\mathbf{K}_0 = \\begin{bmatrix} k_{\\mathrm{w}1} + k_{12} & -k_{12} & 0 \\\\ -k_{12} & k_{12} + k_{23} & -k_{23} \\\\ 0 & -k_{23} & k_{23} + k_{\\mathrm{w}3} \\end{bmatrix}. $$\n  - Additional stiffness matrix $ \\mathbf{K}_1 = \\mathrm{diag}(500,\\, 400,\\, 300)\\, \\mathrm{N/m} $ and $ \\mathbf{K}(t) = \\mathbf{K}_0 + 0.5\\, t\\, \\mathbf{K}_1 $ for $ t \\in [0,\\, 1]\\, \\mathrm{s} $.\n  - Use $ s = 0.85 $, $ r_{\\mathrm{up}} = 1.15 $, $ r_{\\mathrm{down}} = 2.0 $, $ \\Delta t_{\\min} = 10^{-5}\\, \\mathrm{s} $, $ \\Delta t_{\\max} = 5 \\times 10^{-3}\\, \\mathrm{s} $. Sample $ 51 $ uniformly spaced times in $ [0,\\, 1]\\, \\mathrm{s} $.\n  - Output a list $ [\\Delta t_{\\min\\mathrm{series}},\\, \\Delta t_{\\max\\mathrm{series}},\\, \\mathrm{stable}] $, where $ \\Delta t_{\\min\\mathrm{series}} $ and $ \\Delta t_{\\max\\mathrm{series}} $ are the minimum and maximum adaptive time steps over the series (seconds), and $ \\mathrm{stable} $ is a boolean that is $ \\mathrm{True} $ if and only if $ \\Delta t_n \\le s \\dfrac{2}{\\sqrt{\\lambda_{\\max}(t_n)}} $ for all sampled times.\n\n- Case 3 (abrupt stiffness change, edge case to test rate limiting):\n  - $ n = 3 $, $ \\mathbf{M} = \\mathrm{diag}(1.0,\\, 2.0,\\, 1.5)\\, \\mathrm{kg} $,\n  - Base stiffness $ \\mathbf{K}_{\\mathrm{b}} $ from a three-mass chain with springs:\n    $$ k_{\\mathrm{w}1} = 900\\, \\mathrm{N/m},\\quad k_{12} = 1100\\, \\mathrm{N/m},\\quad k_{23} = 1000\\, \\mathrm{N/m},\\quad k_{\\mathrm{w}3} = 700\\, \\mathrm{N/m}, $$\n    $$ \\mathbf{K}_{\\mathrm{b}} = \\begin{bmatrix} k_{\\mathrm{w}1} + k_{12} & -k_{12} & 0 \\\\ -k_{12} & k_{12} + k_{23} & -k_{23} \\\\ 0 & -k_{23} & k_{23} + k_{\\mathrm{w}3} \\end{bmatrix}. $$\n  - Define a piecewise stiffness $ \\mathbf{K}(t) = \\mathbf{K}_{\\mathrm{b}} $ for $ t < 0.6\\, \\mathrm{s} $ and $ \\mathbf{K}(t) = 3\\, \\mathbf{K}_{\\mathrm{b}} $ for $ t \\ge 0.6\\, \\mathrm{s} $.\n  - Use $ s = 0.90 $, $ r_{\\mathrm{up}} = 1.10 $, $ r_{\\mathrm{down}} = 1.30 $, $ \\Delta t_{\\min} = 10^{-5}\\, \\mathrm{s} $, $ \\Delta t_{\\max} = 5 \\times 10^{-3}\\, \\mathrm{s} $. Sample $ 101 $ uniformly spaced times in $ [0,\\, 1]\\, \\mathrm{s} $.\n  - Output a list $ [R_{\\mathrm{inc}},\\, R_{\\mathrm{dec}},\\, \\mathrm{stable}] $, where $ R_{\\mathrm{inc}} = \\max\\limits_{n} \\dfrac{\\Delta t_{n+1}}{\\Delta t_n} $ and $ R_{\\mathrm{dec}} = \\min\\limits_{n} \\dfrac{\\Delta t_{n+1}}{\\Delta t_n} $ over the entire series, and $ \\mathrm{stable} $ is a boolean as defined above.\n\nYour program should produce a single line of output containing the results for all three cases as a comma-separated list enclosed in square brackets (e.g., $ [\\mathrm{case1}, \\mathrm{case2}, \\mathrm{case3}] $), where each $ \\mathrm{case} $ is itself a list following the specifications above. All floating-point values must be in seconds and rounded to eight decimal places, and booleans must be printed with standard Python capitalization $ \\mathrm{True} $ or $ \\mathrm{False} $.",
            "solution": "### Part 1: Derivation of the Spectral Stability Condition\n\nThe analysis begins with the semi-discrete equation of motion for a linear elastic system undergoing free vibration:\n$$ \\mathbf{M} \\ddot{\\mathbf{q}}(t) + \\mathbf{K}(t)\\, \\mathbf{q}(t) = \\mathbf{0} $$\nHere, $ \\mathbf{M} $ is the symmetric positive-definite mass matrix, and $ \\mathbf{K}(t) $ is the symmetric positive-definite stiffness matrix.\n\nThe explicit central difference method approximates the acceleration at a discrete time $ t_n $ using a second-order accurate finite difference formula:\n$$ \\ddot{\\mathbf{q}}_n \\approx \\frac{\\mathbf{q}_{n+1} - 2\\mathbf{q}_n + \\mathbf{q}_{n-1}}{(\\Delta t)^2} $$\nwhere $ \\mathbf{q}_n = \\mathbf{q}(t_n) $ and $ \\Delta t $ is the time step.\n\nSubstituting this approximation into the equation of motion at time $ t_n $, assuming the stiffness matrix is locally constant $ \\mathbf{K}_n = \\mathbf{K}(t_n) $, we have:\n$$ \\mathbf{M} \\left( \\frac{\\mathbf{q}_{n+1} - 2\\mathbf{q}_n + \\mathbf{q}_{n-1}}{(\\Delta t)^2} \\right) + \\mathbf{K}_n \\mathbf{q}_n = \\mathbf{0} $$\nRearranging to solve for the displacement at the next time step, $ \\mathbf{q}_{n+1} $, yields the recurrence relation for the central difference scheme:\n$$ \\mathbf{M} \\mathbf{q}_{n+1} = 2\\mathbf{M}\\mathbf{q}_n - \\mathbf{M}\\mathbf{q}_{n-1} - (\\Delta t)^2 \\mathbf{K}_n \\mathbf{q}_n $$\nSince $ \\mathbf{M} $ is invertible, we can write:\n$$ \\mathbf{q}_{n+1} = \\left( 2\\mathbf{I} - (\\Delta t)^2 \\mathbf{M}^{-1} \\mathbf{K}_n \\right) \\mathbf{q}_n - \\mathbf{q}_{n-1} $$\n\nFor a von Neumann stability analysis, we analyze the behavior of individual modes of the system. We project the solution onto the basis of generalized eigenvectors $ \\boldsymbol{\\phi}_i $, which are solutions to the generalized eigenvalue problem $ \\mathbf{K} \\boldsymbol{\\phi}_i = \\lambda_i \\mathbf{M} \\boldsymbol{\\phi}_i $. The eigenvalues $ \\lambda_i = \\omega_i^2 $ are the squares of the system's natural frequencies. The displacement vector can be expressed as a superposition of these modes: $ \\mathbf{q}(t) = \\sum_i q_i(t) \\boldsymbol{\\phi}_i $, where $q_i(t)$ are the modal coordinates. This decouples the system into a set of single-degree-of-freedom (SDOF) oscillators:\n$$ \\ddot{q}_i(t) + \\omega_i^2 q_i(t) = 0 $$\nThe stability of the multi-degree-of-freedom integration scheme is governed by the stability of the SDOF integrator applied to the mode with the highest frequency, $ \\omega_{\\max} $.\n\nApplying the central difference scheme to the SDOF equation for a mode with frequency $ \\omega $:\n$$ \\frac{q_{n+1} - 2q_n + q_{n-1}}{(\\Delta t)^2} + \\omega^2 q_n = 0 $$\n$$ q_{n+1} = (2 - (\\omega \\Delta t)^2) q_n - q_{n-1} $$\nTo analyze the stability of this recurrence, we formulate it as a first-order system. Let the state vector be $ \\mathbf{z}_n = \\{ q_n, q_{n-1} \\}^T $. Then the update is $ \\mathbf{z}_{n+1} = \\mathbf{A} \\mathbf{z}_n $, where $ \\mathbf{A} $ is the amplification matrix:\n$$ \\mathbf{A} = \\begin{bmatrix} 2 - (\\omega \\Delta t)^2 & -1 \\\\ 1 & 0 \\end{bmatrix} $$\nThe scheme is stable if the spectral radius of the amplification matrix, $ \\rho(\\mathbf{A}) $, is less than or equal to $ 1 $. The eigenvalues $ \\gamma $ of $ \\mathbf{A} $ are given by the characteristic equation $ \\det(\\mathbf{A} - \\gamma \\mathbf{I}) = 0 $:\n$$ \\gamma^2 - (2 - (\\omega \\Delta t)^2)\\gamma + 1 = 0 $$\nThe roots are:\n$$ \\gamma_{1,2} = \\frac{2 - (\\omega \\Delta t)^2 \\pm \\sqrt{(2 - (\\omega \\Delta t)^2)^2 - 4}}{2} $$\nFor a stable, non-dissipative response, the eigenvalues $ \\gamma $ must be complex conjugates on the unit circle, which requires the discriminant to be non-positive:\n$$ (2 - (\\omega \\Delta t)^2)^2 - 4 \\leq 0 $$\n$$ (2 - (\\omega \\Delta t)^2)^2 \\leq 4 $$\n$$ -2 \\leq 2 - (\\omega \\Delta t)^2 \\leq 2 $$\nThis simplifies to $ (\\omega \\Delta t)^2 \\leq 4 $, or $ \\omega \\Delta t \\leq 2 $. At the limit $ \\omega \\Delta t = 2 $, the eigenvalues coalesce at $ \\gamma = -1 $. This leads to a weak instability (resonance) because the amplification matrix is not diagonalizable in this case. Beyond this limit, one eigenvalue has a magnitude greater than $1$, leading to exponential growth of numerical error.\n\nThus, for stability, the condition $ \\omega \\Delta t \\leq 2 $ must hold for all frequencies in the system. The most stringent constraint is imposed by the highest frequency, $ \\omega_{\\max} $:\n$$ \\omega_{\\max} \\Delta t \\leq 2 \\implies \\Delta t \\leq \\frac{2}{\\omega_{\\max}} $$\nThis defines the critical time step for stability, $ \\Delta t_{\\mathrm{crit}} = \\frac{2}{\\omega_{\\max}} $.\nThe highest frequency is related to the largest generalized eigenvalue $ \\lambda_{\\max} $ of $ \\mathbf{K} \\boldsymbol{\\phi} = \\lambda \\mathbf{M} \\boldsymbol{\\phi} $ by $ \\omega_{\\max} = \\sqrt{\\lambda_{\\max}} $. Therefore, the stability condition is:\n$$ \\Delta t \\leq \\frac{2}{\\sqrt{\\lambda_{\\max}}} $$\n\nTo justify the relation between the generalized eigenproblem and standard eigenproblems, we can transform the generalized problem. Since $ \\mathbf{M} $ is symmetric positive-definite, its Cholesky factor or spectral decomposition exists, allowing us to define an invertible matrix $ \\mathbf{M}^{-1/2} $. Pre-multiplying the generalized eigenproblem by $ \\mathbf{M}^{-1/2} $ gives:\n$$ \\mathbf{M}^{-1/2} \\mathbf{K} \\boldsymbol{\\phi} = \\lambda \\mathbf{M}^{1/2} \\boldsymbol{\\phi} $$\nBy inserting $ \\mathbf{I} = \\mathbf{M}^{-1/2} \\mathbf{M}^{1/2} $ and defining a new vector $ \\boldsymbol{\\psi} = \\mathbf{M}^{1/2} \\boldsymbol{\\phi} $, we obtain:\n$$ (\\mathbf{M}^{-1/2} \\mathbf{K} \\mathbf{M}^{-1/2}) \\boldsymbol{\\psi} = \\lambda \\boldsymbol{\\psi} $$\nThis is a standard eigenvalue problem for the matrix $ \\tilde{\\mathbf{K}} = \\mathbf{M}^{-1/2} \\mathbf{K} \\mathbf{M}^{-1/2} $. Since $ \\mathbf{M} $ and $ \\mathbf{K} $ are symmetric, $ \\tilde{\\mathbf{K}} $ is also symmetric. Its eigenvalues $ \\lambda $ are identical to the generalized eigenvalues of the pair $ (\\mathbf{K}, \\mathbf{M}) $.\nFurthermore, the matrix $ \\mathbf{M}^{-1}\\mathbf{K} $ is similar to $ \\tilde{\\mathbf{K}} $ (since $ \\mathbf{B}\\mathbf{A} $ is similar to $ \\mathbf{A}\\mathbf{B} $), so it has the same eigenvalues. Thus, $ \\lambda_{\\max} $ is equivalently the largest eigenvalue (spectral radius) of the symmetric matrix $ \\mathbf{M}^{-1/2}\\mathbf{K}\\mathbf{M}^{-1/2} $ or the non-symmetric matrix $ \\mathbf{M}^{-1}\\mathbf{K} $. The symmetric form is preferred for numerical computation.\n\n### Part 2: Implementation of the Adaptive Time-Stepping Algorithm\n\nThe adaptive time-stepping rule is implemented according to the logic specified in the problem statement. For each case, we compute a series of time steps.\nLet $ \\Delta t_n $ be the time step for step $ n $, and let $ t_{n+1} $ be the time at which the stiffness matrix is evaluated for the next step $ n+1 $.\n\n1.  Calculate the instantaneous maximum angular frequency $ \\omega_{\\max}(t_{n+1}) = \\sqrt{\\lambda_{\\max}(\\mathbf{K}(t_{n+1}), \\mathbf{M})} $.\n2.  Calculate the raw stability-limited time step: $ \\Delta t_{\\mathrm{raw}}(t_{n+1}) = s \\frac{2}{\\omega_{\\max}(t_{n+1})} $.\n3.  The logic for updating the time step from $ \\Delta t_n $ to $ \\Delta t_{n+1} $ is as follows:\n    a. Define the lower bound from the maximum decrease rate: $ \\Delta t_{\\mathrm{dec\\_lim}} = \\Delta t_n / r_{\\mathrm{down}} $.\n    b. If $ \\Delta t_{\\mathrm{raw}}(t_{n+1}) < \\Delta t_{\\mathrm{dec\\_lim}} $, a sharp decrease is required for stability. The rule specifies setting the new time step to the raw limit: $ \\Delta t_{\\mathrm{cand}} = \\Delta t_{\\mathrm{raw}}(t_{n+1}) $.\n    c. Otherwise, the new step is limited by the maximum increase rate and the raw stability limit: $ \\Delta t_{\\mathrm{inc\\_lim}} = r_{\\mathrm{up}} \\Delta t_n $, and $ \\Delta t_{\\mathrm{cand}} = \\min(\\Delta t_{\\mathrm{raw}}(t_{n+1}), \\Delta t_{\\mathrm{inc\\_lim}}) $.\n4.  Finally, this candidate step is clamped by the global minimum and maximum bounds: $ \\Delta t_{n+1} = \\min(\\Delta t_{\\max}, \\max(\\Delta t_{\\min}, \\Delta t_{\\mathrm{cand}})) $.\n5.  The first time step of the series is initialized based on the raw stability limit at $ t_0 $, clamped by $ \\Delta t_{\\min} $ and $ \\Delta t_{\\max} $.\n6.  The `stable` boolean flag is determined by checking if $ \\Delta t_n \\le \\Delta t_{\\mathrm{raw}}(t_n) / s $ for all steps $n$. Note that the rule is $ \\Delta t \\le 2/\\omega_{\\max} $, so we check against $\\Delta t_{\\mathrm{raw}}/s$. However, the problem states the check as $ \\Delta t_n \\le s \\dfrac{2}{\\sqrt{\\lambda_{\\max}(t_n)}} $, which is $ \\Delta t_n \\le \\Delta t_{\\mathrm{raw}}(t_n) $. This is the check implemented.\n\nThe following code implements this logic and computes the required outputs for the three test cases.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef adaptive_time_stepping(times, M, K_func, s, r_up, r_down, dt_min, dt_max):\n    \"\"\"\n    Implements the adaptive time stepping algorithm.\n    \n    Returns:\n        dt_series: List of adaptive time steps.\n        dt_raw_series: List of raw stability-limited time steps.\n        is_stable: Boolean indicating if all steps were stable.\n    \"\"\"\n    num_samples = len(times)\n    dt_series = []\n    dt_raw_series = []\n    \n    # Initial step\n    t0 = times[0]\n    K0 = K_func(t0)\n    try:\n        eigvals = eigh(K0, M, eigvals_only=True)\n        lambda_max_0 = eigvals[-1]\n        w_max_0 = np.sqrt(lambda_max_0)\n        dt_raw_0 = s * 2.0 / w_max_0\n    except np.linalg.LinAlgError:\n        # Handle singular matrix cases if they were to occur, not expected here\n        dt_raw_0 = dt_min\n        \n    dt_raw_series.append(dt_raw_0)\n    \n    dt_current = min(dt_max, max(dt_min, dt_raw_0))\n    dt_series.append(dt_current)\n    \n    is_stable = (dt_current <= dt_raw_0)\n\n    # Subsequent steps\n    for i in range(1, num_samples):\n        dt_prev = dt_series[-1]\n        t_i = times[i]\n        K_i = K_func(t_i)\n        \n        try:\n            eigvals = eigh(K_i, M, eigvals_only=True)\n            lambda_max_i = eigvals[-1]\n            if lambda_max_i <= 0: # Physically unexpected but for robustness\n                 w_max_i = np.inf\n                 dt_raw_i = dt_min\n            else:\n                 w_max_i = np.sqrt(lambda_max_i)\n                 dt_raw_i = s * 2.0 / w_max_i\n        except np.linalg.LinAlgError:\n            dt_raw_i = dt_min\n            \n        dt_raw_series.append(dt_raw_i)\n        \n        # Adaptive rule\n        dt_decrease_limit = dt_prev / r_down\n        \n        if dt_raw_i < dt_decrease_limit:\n            dt_cand = dt_raw_i\n        else:\n            dt_increase_limit = dt_prev * r_up\n            dt_cand = min(dt_raw_i, dt_increase_limit)\n            \n        # Global bounds\n        dt_next = min(dt_max, max(dt_min, dt_cand))\n        dt_series.append(dt_next)\n        \n        if not (dt_next <= dt_raw_i):\n            is_stable = False\n            \n    return dt_series, dt_raw_series, is_stable\n\ndef solve():\n    \"\"\"\n    Solves the three test cases and prints the results.\n    \"\"\"\n    all_results = []\n    ROUND_DIGITS = 8\n\n    # --- Case 1 ---\n    M1 = np.diag([2.0, 1.0])\n    k1, k2, k3 = 1200.0, 700.0, 900.0\n    K1_const = np.array([[k1 + k2, -k2], [-k2, k2 + k3]])\n    \n    def K_func1(t):\n        return K1_const\n        \n    params1 = {\n        's': 0.95, 'r_up': 1.2, 'r_down': 1.5,\n        'dt_min': 1e-6, 'dt_max': 1e-2\n    }\n    times1 = np.linspace(0, 1, 21)\n    \n    lambda_max_1 = eigh(K1_const, M1, eigvals_only=True)[-1]\n    dt_crit_1 = 2.0 / np.sqrt(lambda_max_1)\n    \n    dt_series1, _, _ = adaptive_time_stepping(times1, M1, K_func1, **params1)\n    dt_start_1 = dt_series1[0]\n    dt_end_1 = dt_series1[-1]\n    \n    all_results.append([\n        round(dt_crit_1, ROUND_DIGITS),\n        round(dt_start_1, ROUND_DIGITS),\n        round(dt_end_1, ROUND_DIGITS)\n    ])\n\n    # --- Case 2 ---\n    M2 = np.diag([1.0, 1.5, 2.0])\n    k_w1, k_12, k_23, k_w3 = 1000.0, 1500.0, 1200.0, 800.0\n    K0_2 = np.array([\n        [k_w1 + k_12, -k_12, 0],\n        [-k_12, k_12 + k_23, -k_23],\n        [0, -k_23, k_23 + k_w3]\n    ])\n    K1_2 = np.diag([500.0, 400.0, 300.0])\n    \n    def K_func2(t):\n        return K0_2 + 0.5 * t * K1_2\n\n    params2 = {\n        's': 0.85, 'r_up': 1.15, 'r_down': 2.0,\n        'dt_min': 1e-5, 'dt_max': 5e-3\n    }\n    times2 = np.linspace(0, 1, 51)\n    \n    dt_series2, _, is_stable2 = adaptive_time_stepping(times2, M2, K_func2, **params2)\n    dt_min_series2 = min(dt_series2)\n    dt_max_series2 = max(dt_series2)\n    \n    all_results.append([\n        round(dt_min_series2, ROUND_DIGITS),\n        round(dt_max_series2, ROUND_DIGITS),\n        is_stable2\n    ])\n    \n    # --- Case 3 ---\n    M3 = np.diag([1.0, 2.0, 1.5])\n    k_w1_3, k_12_3, k_23_3, k_w3_3 = 900.0, 1100.0, 1000.0, 700.0\n    Kb_3 = np.array([\n        [k_w1_3 + k_12_3, -k_12_3, 0],\n        [-k_12_3, k_12_3 + k_23_3, -k_23_3],\n        [0, -k_23_3, k_23_3 + k_w3_3]\n    ])\n    \n    def K_func3(t):\n        if t < 0.6:\n            return Kb_3\n        else:\n            return 3.0 * Kb_3\n            \n    params3 = {\n        's': 0.90, 'r_up': 1.10, 'r_down': 1.30,\n        'dt_min': 1e-5, 'dt_max': 5e-3\n    }\n    times3 = np.linspace(0, 1, 101)\n    \n    dt_series3, _, is_stable3 = adaptive_time_stepping(times3, M3, K_func3, **params3)\n    \n    ratios = [dt_series3[i+1] / dt_series3[i] for i in range(len(dt_series3) - 1)]\n    R_inc = max(ratios)\n    R_dec = min(ratios)\n    \n    all_results.append([\n        round(R_inc, ROUND_DIGITS),\n        round(R_dec, ROUND_DIGITS),\n        is_stable3\n    ])\n\n    # --- Format and Print Output ---\n    def format_item(item):\n        if isinstance(item, float):\n            return f\"{item:.{ROUND_DIGITS}f}\"\n        return repr(item)\n\n    results_str_parts = []\n    for res_list in all_results:\n        res_list_str = [format_item(item) for item in res_list]\n        results_str_parts.append(f\"[{','.join(res_list_str)}]\")\n    \n    print(f\"[{','.join(results_str_parts)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "While stability ensures a simulation does not diverge, accuracy determines if its results are physically meaningful, especially in wave propagation problems. This exercise delves into numerical dispersion, an artifact of spatial discretization where numerical waves travel at incorrect speeds depending on their wavelength. By implementing a Bloch-wave analysis on a periodic domain, you will quantify this phase velocity error and investigate how it is influenced by fundamental modeling choices such as element order and mass matrix formulation (consistent versus lumped) .",
            "id": "3599584",
            "problem": "You are to derive, implement, and evaluate a numerical dispersion analysis for the semi-discrete equations of motion arising from a two-dimensional scalar wave model discretized by tensor-product quadrilateral finite elements. The starting point must be the fundamental balance of linear momentum and the definition of the weak form, leading to the semi-discrete system. You must then quantify the phase velocity error introduced by mass lumping for quadratic elements, and compare it against the consistent mass formulation. Your implementation must be a runnable program that constructs the required matrices, enforces Bloch-periodic boundary conditions, and computes dispersion relations numerically.\n\nThe fundamental base is as follows. Begin from Newton’s second law in a continuum: for a scalar displacement field $u(\\boldsymbol{x},t)$, the governing equation is $ \\rho \\, \\ddot{u} - \\nabla \\cdot (\\mathbb{C} \\nabla u) = 0 $, where $\\rho$ is mass density, $\\mathbb{C}$ is a symmetric positive constant that sets the wave speed via $ c = \\sqrt{\\mathbb{C} / \\rho} $, and the overdot denotes a time derivative. The corresponding semi-discrete equations of motion obtained from the Galerkin finite element method on a uniform square element of side $h$ are\n$$\n\\boldsymbol{M} \\, \\ddot{\\boldsymbol{q}}(t) + \\boldsymbol{K} \\, \\boldsymbol{q}(t) = \\boldsymbol{0},\n$$\nwhere $ \\boldsymbol{M} $ and $ \\boldsymbol{K} $ are the consistent mass and stiffness matrices assembled from element contributions derived from the weak form. Mass lumping replaces the consistent mass matrix by a diagonal approximation obtained by the row-sum of the element-level mass matrix.\n\nTo quantify dispersion, consider Bloch-periodic waves in an infinite lattice formed by tiling identical square elements. Let the lattice vectors be $h \\boldsymbol{e}_x$ and $h \\boldsymbol{e}_y$. For a prescribed real wavenumber vector $\\boldsymbol{k} = (k_x,k_y)$ with magnitude $ |\\boldsymbol{k}| = \\sqrt{k_x^2 + k_y^2} $ in $\\mathrm{rad/m}$, the Bloch boundary conditions impose\n$$\nu(\\boldsymbol{x} + h \\, \\boldsymbol{e}_x, t) = u(\\boldsymbol{x}, t) \\, e^{\\mathrm{i} k_x h}, \\quad\nu(\\boldsymbol{x} + h \\, \\boldsymbol{e}_y, t) = u(\\boldsymbol{x}, t) \\, e^{\\mathrm{i} k_y h}.\n$$\nWhen applied to the semi-discrete system on a single square reference cell, these relations induce a Hermitian, reduced generalized eigenvalue problem\n$$\n\\left( \\boldsymbol{K}_{\\mathrm{red}}(\\boldsymbol{k}) - \\omega^2 \\, \\boldsymbol{M}_{\\mathrm{red}}(\\boldsymbol{k}) \\right) \\boldsymbol{\\varphi} = \\boldsymbol{0},\n$$\nwhose smallest positive eigenvalue $ \\omega^2 $ yields the numerically predicted angular frequency $ \\omega $ for that $\\boldsymbol{k}$. The numerical phase velocity is then $ v_p^{\\mathrm{num}} = \\omega / |\\boldsymbol{k}| $, and the exact phase velocity is $ v_p^{\\mathrm{ex}} = c $. The relative phase velocity error is\n$$\n\\varepsilon(\\boldsymbol{k}; h, p) = \\frac{v_p^{\\mathrm{num}}(\\boldsymbol{k}; h, p)}{c} - 1,\n$$\nwhere $ p $ is the polynomial order of the element basis in each parametric direction. You must compute $ |\\varepsilon| $ for both the consistent mass and the lumped mass cases.\n\nImplementation details you must follow:\n- Use tensor-product quadrilateral elements of polynomial order $p \\in \\{1,2\\}$ on a single square element of side $h$ as the periodic unit cell. For $p=1$ use bilinear shape functions defined on the reference square $[-1,1]^2$ with nodes at the corners; for $p=2$ use biquadratic Lagrange shape functions with $1\\mathrm{D}$ nodes at $\\{-1,0,1\\}$ in each parametric direction.\n- Construct element matrices by numerical quadrature exact for polynomials of the appropriate degree. For $p=1$ use $2 \\times 2$ Gauss points; for $p=2$ use $3 \\times 3$ Gauss points. The element stiffness is\n$$\nK_{ij}^{(e)} = \\mathbb{C} \\int_{\\Omega_e} \\nabla N_i \\cdot \\nabla N_j \\, \\mathrm{d}\\Omega,\n$$\nand the element mass is\n$$\nM_{ij}^{(e)} = \\rho \\int_{\\Omega_e} N_i \\, N_j \\, \\mathrm{d}\\Omega.\n$$\n- For mass lumping, replace each element mass matrix $ \\boldsymbol{M}^{(e)} $ by its row-sum diagonal before assembly into the cell’s global mass. Assemble the single-cell global matrices and enforce Bloch periodicity by eliminating slave degrees of freedom with complex phase ties. This yields the reduced Hermitian pair $ (\\boldsymbol{K}_{\\mathrm{red}}(\\boldsymbol{k}), \\boldsymbol{M}_{\\mathrm{red}}(\\boldsymbol{k})) $.\n- For each $\\boldsymbol{k}$, solve the smallest positive eigenvalue of the reduced generalized eigenproblem and compute $ |\\varepsilon| $.\n\nPhysical and numerical units:\n- Use $ \\rho = 1.0 \\, \\mathrm{kg/m^3} $, $ \\mathbb{C} = 1.0 \\, \\mathrm{Pa} $, hence $ c = \\sqrt{\\mathbb{C}/\\rho} = 1.0 \\, \\mathrm{m/s} $.\n- All lengths $h$ must be in $ \\mathrm{m} $, all wavenumbers $k_x, k_y$ must be in $ \\mathrm{rad/m} $, all angular frequencies $ \\omega $ must be in $ \\mathrm{rad/s} $.\n- Report phase velocity errors as dimensionless floats (no unit).\n\nTest suite and required outputs:\n- Use the following three test cases, each specified by element size $h$ (in $ \\mathrm{m} $) and polynomial order $p$:\n  1. Case $1$: $h = 1.0$, $p = 1$.\n  2. Case $2$: $h = 1.0$, $p = 2$.\n  3. Case $3$: $h = 0.5$, $p = 2$.\n- For each case, evaluate the dispersion over the set of wavenumber pairs (in $ \\mathrm{rad/m} $):\n$$\n\\mathcal{K} = \\left\\{ (k,0), (0,k), \\left( \\frac{k}{\\sqrt{2}}, \\frac{k}{\\sqrt{2}} \\right) \\, : \\, k \\in \\{ 0.5, 1.0, 1.5, 2.0 \\} \\right\\}.\n$$\n- For each case, compute:\n  - $E_{\\mathrm{cons}}^{\\max} = \\max_{(k_x,k_y) \\in \\mathcal{K}} \\left| \\varepsilon_{\\mathrm{cons}}( (k_x,k_y); h, p ) \\right|$ using the consistent mass.\n  - $E_{\\mathrm{lump}}^{\\max} = \\max_{(k_x,k_y) \\in \\mathcal{K}} \\left| \\varepsilon_{\\mathrm{lump}}( (k_x,k_y); h, p ) \\right|$ using the lumped mass.\n  - $\\Delta E^{\\max} = E_{\\mathrm{lump}}^{\\max} - E_{\\mathrm{cons}}^{\\max}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a three-entry list $[E_{\\mathrm{cons}}^{\\max}, E_{\\mathrm{lump}}^{\\max}, \\Delta E^{\\max}]$, in the same order as above. For example, the output format must be:\n\"[ [E1_cons,E1_lump,E1_delta], [E2_cons,E2_lump,E2_delta], [E3_cons,E3_lump,E3_delta] ]\"\nUse plain decimal floats.\n\nThe numerical algorithm must be general and must not hard-code any analytical dispersion formulas. It must compute the element matrices by quadrature, enforce Bloch periodic boundary conditions via complex-valued constraints, and solve the reduced Hermitian generalized eigenproblems to extract numerical angular frequencies. All results must be computed within the program for the specified test suite, and no user input is permitted.",
            "solution": "### Principle-Based Design of the Solution\nThe core of the solution lies in implementing the finite element method (FEM) for the scalar wave equation and then applying Bloch-Floquet theory to analyze the dispersion properties of the resulting semi-discrete system.\n\n**1. Finite Element Formulation:**\nThe process begins with the weak form of the governing equation, which, after applying the Galerkin method with shape functions $N_i$, leads to element-level mass $\\boldsymbol{M}^{(e)}$ and stiffness $\\boldsymbol{K}^{(e)}$ matrices. These matrices are computed over a single square reference element $[-1,1]^2$ and then scaled to the physical element size $h$.\n- **Shape Functions**: For an element of polynomial order $p$, we use a tensor product of 1D Lagrange polynomials of degree $p$. For $p=1$, this gives 4 bilinear functions. For $p=2$, this gives 9 biquadratic functions.\n- **Numerical Quadrature**: The integrals for $\\boldsymbol{M}^{(e)}$ and $\\boldsymbol{K}^{(e)}$ are evaluated numerically using Gauss-Legendre quadrature. The rule must be of sufficient order to integrate the polynomial products exactly; a $(p+1) \\times (p+1)$ point rule is sufficient.\n- **Mapping**: A standard isoparametric mapping transforms derivatives from the reference space $(\\xi, \\eta)$ to the physical space $(x, y)$, involving the Jacobian of the transformation. For a square element, this scaling is uniform.\n\n**2. Mass Lumping:**\nMass lumping is an approximation that simplifies the time integration of the semi-discrete equations by diagonalizing the mass matrix. This avoids solving a linear system at each time step. The most common lumping scheme, row-sum lumping, is employed here. Each diagonal entry of the lumped mass matrix is the sum of all entries in the corresponding row of the consistent mass matrix. This operation is performed at the element level before any other assembly.\n\n**3. Bloch-Floquet Boundary Conditions for Dispersion Analysis:**\nTo analyze wave propagation in an infinite periodic lattice of elements, we consider a single element as the repeating unit cell. The Bloch-Floquet theorem states that for a wave with wavenumber $\\boldsymbol{k}$, the solution at corresponding points in adjacent cells is related by a complex phase factor. This allows us to reduce the problem from an infinite number of degrees of freedom (DOFs) to just those associated with one unit cell.\n- **Master and Slave DOFs**: The DOFs within a single cell are partitioned. Those on the interior of the cell and on its \"primary\" boundaries (e.g., bottom and left) are designated as \"master\" DOFs. The DOFs on the opposing \"secondary\" boundaries (top and right) are \"slaves,\" as their values are constrained to master DOFs by the Bloch phase relations. For a $p$-th order element, there are $p^2$ master DOFs.\n- **System Reduction**: A transformation matrix, $\\boldsymbol{L}(\\boldsymbol{k})$, is constructed to express all DOFs in the cell in terms of only the master DOFs: $\\boldsymbol{q} = \\boldsymbol{L}(\\boldsymbol{k}) \\boldsymbol{q}_{\\mathrm{master}}$. Substituting this into the full-cell energy expressions, $\\frac{1}{2}\\boldsymbol{q}^H \\boldsymbol{K} \\boldsymbol{q}$ and $\\frac{1}{2}\\boldsymbol{q}^H \\boldsymbol{M} \\boldsymbol{q}$, yields the reduced matrices $\\boldsymbol{K}_{\\mathrm{red}} = \\boldsymbol{L}^H \\boldsymbol{K} \\boldsymbol{L}$ and $\\boldsymbol{M}_{\\mathrm{red}} = \\boldsymbol{L}^H \\boldsymbol{M} \\boldsymbol{L}$. These matrices are Hermitian and have dimensions $p^2 \\times p^2$.\n\n**4. Eigenvalue Problem and Error Calculation:**\nAssuming a time-harmonic solution $\\boldsymbol{q}(t) = \\boldsymbol{\\varphi} e^{-\\mathrm{i}\\omega t}$, the semi-discrete equations become a generalized eigenvalue problem: $\\boldsymbol{K}_{\\mathrm{red}}\\boldsymbol{\\varphi} = \\omega^2 \\boldsymbol{M}_{\\mathrm{red}}\\boldsymbol{\\varphi}$.\n- **Solving**: We solve this problem for each wavenumber $\\boldsymbol{k}$ to find the corresponding numerical frequencies $\\omega$. Since $\\boldsymbol{K}_{\\mathrm{red}}$ and $\\boldsymbol{M}_{\\mathrm{red}}$ are Hermitian and positive (semi-)definite, the eigenvalues $\\omega^2$ are real and non-negative. We seek the smallest positive eigenvalue, which corresponds to the lowest-frequency acoustic wave mode.\n- **Error Quantification**: The numerical phase velocity is computed as $v_p^{\\mathrm{num}} = \\omega / |\\boldsymbol{k}|$. This is compared to the exact continuum phase velocity $c=1.0$ to find the relative error $\\varepsilon$. The final reported value is the maximum of the absolute error $|\\varepsilon|$ over all specified wavenumbers for a given test case.\nThe code implements this entire pipeline, from generating element matrices to solving the final eigenproblems and collating the results.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef solve():\n    \"\"\"\n    Derives, implements, and evaluates a numerical dispersion analysis for a 2D scalar wave model\n    discretized by quadrilateral finite elements, as specified in the problem description.\n    \"\"\"\n\n    def get_lagrange_1d(p, xi_eval):\n        \"\"\"\n        Computes 1D Lagrange polynomials and their derivatives at a point xi_eval.\n        Args:\n            p (int): Polynomial order (1 or 2).\n            xi_eval (float): Evaluation point in [-1, 1].\n        Returns:\n            tuple: (N, dN) where N are shape function values and dN are their derivatives.\n        \"\"\"\n        if p == 1:\n            N = np.array([0.5 * (1 - xi_eval), 0.5 * (1 + xi_eval)])\n            dN = np.array([-0.5, 0.5])\n        elif p == 2:\n            N = np.array([\n                0.5 * xi_eval * (xi_eval - 1.0),\n                1.0 - xi_eval**2,\n                0.5 * xi_eval * (xi_eval + 1.0)\n            ])\n            dN = np.array([\n                xi_eval - 0.5,\n                -2.0 * xi_eval,\n                xi_eval + 0.5\n            ])\n        else:\n            raise ValueError(\"Polynomial order p must be 1 or 2.\")\n        return N, dN\n\n    def get_element_matrices(h, p, C_const, rho):\n        \"\"\"\n        Constructs the element stiffness (K_e) and mass (M_e) matrices.\n        Args:\n            h (float): Element side length.\n            p (int): Polynomial order.\n            C_const (float): Stiffness-like material constant.\n            rho (float): Mass density.\n        Returns:\n            tuple: (K_e, M_e) numpy arrays.\n        \"\"\"\n        n_nodes_1d = p + 1\n        n_nodes_total = n_nodes_1d**2\n        \n        K_e = np.zeros((n_nodes_total, n_nodes_total))\n        M_e = np.zeros((n_nodes_total, n_nodes_total))\n\n        n_quad_pts = p + 1\n        xi_quad, w_quad = np.polynomial.legendre.leggauss(n_quad_pts)\n\n        detJ = (h / 2.0)**2\n        \n        for iq, xi in enumerate(xi_quad):\n            for jq, eta in enumerate(xi_quad):\n                wq = w_quad[iq]\n                wj = w_quad[jq]\n\n                N_vals = np.zeros(n_nodes_total)\n                grad_N_vals_ref = np.zeros((n_nodes_total, 2))\n                \n                N_1d_xi, dN_1d_xi = get_lagrange_1d(p, xi)\n                N_1d_eta, dN_1d_eta = get_lagrange_1d(p, eta)\n\n                for i in range(n_nodes_total):\n                    ix = i % n_nodes_1d\n                    iy = i // n_nodes_1d\n                    N_vals[i] = N_1d_xi[ix] * N_1d_eta[iy]\n                    grad_N_vals_ref[i, 0] = dN_1d_xi[ix] * N_1d_eta[iy]\n                    grad_N_vals_ref[i, 1] = N_1d_xi[ix] * dN_1d_eta[iy]\n\n                grad_N_vals_phys = grad_N_vals_ref * (2.0 / h)\n\n                weight = wq * wj * detJ\n                N_outer = np.outer(N_vals, N_vals)\n                grad_outer = grad_N_vals_phys @ grad_N_vals_phys.T\n                \n                M_e += rho * N_outer * weight\n                K_e += C_const * grad_outer * weight\n                        \n        return K_e, M_e\n\n    def build_L_matrix(p, h, kx, ky):\n        \"\"\"\n        Builds the transformation matrix L(k) for Bloch periodicity.\n        This matrix maps the master DOFs to all DOFs in the unit cell.\n        Args:\n            p (int): Polynomial order.\n            h (float): Element side length.\n            kx (float): Wavenumber in x-direction.\n            ky (float): Wavenumber in y-direction.\n        Returns:\n            numpy.ndarray: The complex-valued L matrix.\n        \"\"\"\n        n_nodes_1d = p + 1\n        n_nodes_total = n_nodes_1d**2\n        n_master_nodes = p**2\n        \n        L = np.zeros((n_nodes_total, n_master_nodes), dtype=np.complex128)\n        \n        master_map = {}\n        master_idx_counter = 0\n        for i in range(n_nodes_total):\n            ix = i % n_nodes_1d\n            iy = i // n_nodes_1d\n            if ix < p and iy < p:\n                master_map[i] = master_idx_counter\n                master_idx_counter += 1\n\n        for i in range(n_nodes_total):\n            ix = i % n_nodes_1d\n            iy = i // n_nodes_1d\n            \n            master_ix = ix % p\n            master_iy = iy % p\n            master_node_idx_full = master_ix + master_iy * n_nodes_1d\n            master_node_idx_reduced = master_map[master_node_idx_full]\n            \n            nx = ix // p\n            ny = iy // p\n            phase = np.exp(1j * (nx * kx * h + ny * ky * h))\n            \n            L[i, master_node_idx_reduced] = phase\n            \n        return L\n    \n    def calculate_phase_error(h, p, kx, ky, rho, C_const, c_exact, mass_lumping):\n        \"\"\"\n        Computes the relative phase velocity error for a given configuration.\n        \"\"\"\n        K_e, M_e = get_element_matrices(h, p, C_const, rho)\n        \n        if mass_lumping:\n            M_e = np.diag(np.sum(M_e, axis=1))\n            \n        L = build_L_matrix(p, h, kx, ky)\n        \n        K_red = L.conj().T @ K_e @ L\n        M_red = L.conj().T @ M_e @ L\n        \n        # Solve generalized eigenvalue problem: K_red * v = omega^2 * M_red * v\n        eigvals = eigh(K_red, M_red, eigvals_only=True)\n        \n        # Eigenvalues omega^2 are real and sorted. Smallest positive one is for the acoustic mode.\n        # For k != 0, the smallest eigenvalue should be positive.\n        omega_sq = eigvals[0]\n        if omega_sq < 0: omega_sq = 0.0\n\n        omega_num = np.sqrt(omega_sq)\n        k_mag = np.sqrt(kx**2 + ky**2)\n        \n        if k_mag < 1e-12:\n            return 0.0\n            \n        v_num = omega_num / k_mag\n        error = (v_num / c_exact) - 1.0\n        \n        return error\n\n    # Main execution logic starts here\n    test_cases = [\n        (1.0, 1),\n        (1.0, 2),\n        (0.5, 2)\n    ]\n    \n    k_magnitudes = [0.5, 1.0, 1.5, 2.0]\n    k_vectors = []\n    for k in k_magnitudes:\n        k_vectors.append((k, 0.0))\n        k_vectors.append((0.0, k))\n        k_vectors.append((k / np.sqrt(2.0), k / np.sqrt(2.0)))\n\n    rho = 1.0\n    C_const = 1.0\n    c_exact = np.sqrt(C_const / rho)\n\n    all_results = []\n    for h, p in test_cases:\n        max_err_cons = 0.0\n        max_err_lump = 0.0\n\n        for kx, ky in k_vectors:\n            err_cons = calculate_phase_error(h, p, kx, ky, rho, C_const, c_exact, mass_lumping=False)\n            max_err_cons = max(max_err_cons, abs(err_cons))\n\n            err_lump = calculate_phase_error(h, p, kx, ky, rho, C_const, c_exact, mass_lumping=True)\n            max_err_lump = max(max_err_lump, abs(err_lump))\n        \n        delta_max_err = max_err_lump - max_err_cons\n        all_results.append([max_err_cons, max_err_lump, delta_max_err])\n\n    # Format output string exactly as required\n    result_strings = []\n    for res_triplet in all_results:\n        result_strings.append(f\"[{res_triplet[0]},{res_triplet[1]},{res_triplet[2]}]\")\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world mechanical systems are often governed by complex phenomena like contact, which are mathematically expressed as inequality constraints. This advanced practice demonstrates how to solve the semi-discrete equations of motion under such constraints using an implicit time-stepping scheme. You will formulate the time-step problem as a convex quadratic program and implement the Alternating Direction Method of Multipliers (ADMM), a powerful modern optimization algorithm, to enforce a unilateral contact condition .",
            "id": "3599599",
            "problem": "Design and implement a solver that uses the Alternating Direction Method of Multipliers (ADMM) or equivalently Alternating Least Squares (ALS) splitting to enforce unilateral contact constraints in the semi-discrete equations of motion of a single-degree-of-freedom linear elastic oscillator subject to a rigid ceiling at zero displacement. The semi-discrete equation at a single implicit backward-Euler time step is obtained from Newton's second law and Hooke's law and reads: find the next-step displacement $u_{n+1}$ and the contact multiplier $\\lambda$ such that\n$$\nM \\ddot{u}_{n+1} + K u_{n+1} + B^{\\mathsf{T}} \\lambda = f_{n+1},\n$$\nwith the kinematic update $u_{n+1} = u_n + h v_{n+1}$, where $M$ is the mass, $K$ is the linear stiffness, $f_{n+1}$ is the external load at time $t_{n+1}$, $h$ is the time step, and $B$ is the linearized contact operator. For a scalar contact gap function defined by $g(u) = u$ that enforces a rigid ceiling at $u \\le 0$, the unilateral constraint is $g(u_{n+1}) \\le 0$. Use backward Euler to eliminate the acceleration and velocity to obtain a linear-quadratic objective in $u_{n+1}$ and a single linear inequality constraint $g(u_{n+1}) \\le 0$, without introducing any damping. Specifically, derive the effective system\n$$\nH u_{n+1} + B^{\\mathsf{T}} \\lambda = r,\n$$\nwhere $H$ and $r$ are functions of $M$, $K$, $h$, $u_n$, $v_n$, and $f_{n+1}$, and $B = 1$ for $g(u) = u$.\n\nYour task is to:\n- Starting solely from Newton's second law and the backward Euler time discretization, derive the convex quadratic program in $u_{n+1}$ with the unilateral inequality $g(u_{n+1}) \\le 0$.\n- Construct the scaled ADMM updates to solve the problem in the consensus form with an auxiliary variable $z$ that enforces the constraint set $\\mathcal{C} = (-\\infty, 0]$ via a projection. Use the standard scaled form with a penalty parameter $\\rho > 0$. The update structure must alternate between a least-squares step in $u$ and a projection step in $z$.\n- Implement the ADMM iteration and monitor:\n  1. The primal residual $r_p^k = \\lVert g(u^k) - z^k \\rVert_2$.\n  2. The augmented objective sequence\n     $$\n     \\Phi_\\rho^k = \\tfrac{1}{2} H (u^k)^2 - r u^k + I_{\\mathcal{C}}(z^k) + \\tfrac{\\rho}{2} \\lVert B u^k - z^k \\rVert_2^2,\n     $$\n     where $I_{\\mathcal{C}}(z)$ is $0$ if $z \\in \\mathcal{C}$ and $+\\infty$ otherwise. Because the $z$-update is an exact projection onto $\\mathcal{C}$, $I_{\\mathcal{C}}(z^k) = 0$ along the iterates.\n- Stop the iteration when both the primal residual and the dual residual are below a tolerance, or when a maximum number of iterations is reached. The dual residual for the scaled ADMM in this setting is $s^k = \\rho \\lVert z^k - z^{k-1} \\rVert_2$.\n\nPhysical and numerical units and conventions:\n- Treat $u$, $v$, and $h$ in meters and seconds, $M$ in kilograms, $K$ in newtons per meter, $f$ in newtons, and $\\lambda$ in newtons. Although the ADMM residuals and the augmented objective are in algorithmic units, ensure all physical inputs respect these units.\n- Angles are not used; no angle unit specification is needed.\n\nTest suite and required outputs:\n- Use the following three test cases with the same ADMM parameters in each case. For all cases set the scaled ADMM penalty parameter to $\\rho = 1000$, the maximum number of iterations to $N_{\\max} = 200$, and the absolute tolerances to $\\varepsilon_p = 10^{-10}$ for the primal residual and $\\varepsilon_d = 10^{-10}$ for the dual residual.\n- For each case, construct $H$ and $r$ from the given data using backward Euler, then run ADMM starting from the unconstrained solution $u^{0} = H^{-1} r$ and $z^{0} = \\min(0, u^{0})$, $y^{0} = 0$.\n- Test cases (all quantities must be used exactly as listed):\n  1. Happy-path active contact: $M = 1$, $K = 1000$, $h = 0.05$, $u_n = -0.01$, $v_n = 0$, $f_{n+1} = 30$.\n  2. Strictly inactive contact: $M = 1$, $K = 1000$, $h = 0.05$, $u_n = -0.01$, $v_n = 0$, $f_{n+1} = 0$.\n  3. Boundary (just active) contact: $M = 1$, $K = 1000$, $h = 0.05$, $u_n = -0.01$, $v_n = 0$, $f_{n+1} = 4$.\n- For each test case, your program must compute and return:\n  1. The final primal residual $r_p$ as a floating-point number rounded to six decimal places.\n  2. A boolean flag indicating whether the augmented objective sequence $\\{\\Phi_\\rho^k\\}_{k=0}^{k_{\\text{final}}}$ is nonincreasing, i.e., $\\Phi_\\rho^{k+1} \\le \\Phi_\\rho^{k} + \\delta$ for all $k$, where $\\delta = 10^{-12}$ is a numerical slack.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the three test cases as a comma-separated list of three items, each item being a two-element list $[r_p, \\text{flag}]$. For example, an output must look like $[[0.0,True],[0.0,True],[0.0,True]]$, but with the actual computed values and booleans for the test suite above.",
            "solution": "### 1. Derivation of the Effective System\n\nThe equation of motion at time $t_{n+1}$ is given by Newton's second law, including stiffness and contact forces:\n$$\nM \\ddot{u}_{n+1} + K u_{n+1} + B^{\\mathsf{T}} \\lambda = f_{n+1}\n$$\nwhere $M$ is the mass, $K$ is the stiffness, $u_{n+1}$ is the displacement, $\\ddot{u}_{n+1}$ is the acceleration, $f_{n+1}$ is the external force, $\\lambda$ is the contact force multiplier, and $B^{\\mathsf{T}}$ is the transpose of the linearized contact operator.\n\nWe use the implicit backward-Euler method to discretize the time derivatives. The velocity $v_{n+1}$ and acceleration $\\ddot{u}_{n+1}$ are approximated as:\n$$\nv_{n+1} = \\frac{u_{n+1} - u_n}{h}\n$$\n$$\n\\ddot{u}_{n+1} = \\frac{v_{n+1} - v_n}{h} = \\frac{1}{h} \\left( \\frac{u_{n+1} - u_n}{h} - v_n \\right) = \\frac{1}{h^2} (u_{n+1} - u_n - h v_n)\n$$\nwhere $u_n$ and $v_n$ are the displacement and velocity at the previous time step $t_n$, and $h$ is the time step size.\n\nSubstituting the expression for $\\ddot{u}_{n+1}$ into the equation of motion yields:\n$$\nM \\left( \\frac{u_{n+1} - u_n - h v_n}{h^2} \\right) + K u_{n+1} + B^{\\mathsf{T}} \\lambda = f_{n+1}\n$$\nTo obtain the effective system, we rearrange the terms, grouping those involving the unknown displacement $u_{n+1}$ on the left-hand side and all known quantities on the right-hand side:\n$$\n\\frac{M}{h^2} u_{n+1} + K u_{n+1} + B^{\\mathsf{T}} \\lambda = f_{n+1} + \\frac{M}{h^2} (u_n + h v_n)\n$$\nFactoring out $u_{n+1}$, we arrive at the specified form $H u_{n+1} + B^{\\mathsf{T}} \\lambda = r$:\n$$\n\\left( \\frac{M}{h^2} + K \\right) u_{n+1} + B^{\\mathsf{T}} \\lambda = f_{n+1} + \\frac{M}{h^2} (u_n + h v_n)\n$$\nFrom this, we identify the effective stiffness matrix (a scalar in this case) $H$ and the effective force vector (a scalar) $r$:\n$$\nH = \\frac{M}{h^2} + K\n$$\n$$\nr = f_{n+1} + \\frac{M}{h^2} (u_n + h v_n)\n$$\nThe contact gap function is $g(u) = u$, which implies the linearized operator $B = \\frac{\\partial g}{\\partial u} = 1$.\n\n### 2. Quadratic Programming Formulation\n\nThe equation $H u_{n+1} - r = -B^{\\mathsf{T}}\\lambda$ is the stationarity condition for an energy functional subject to a constraint. The problem can be formulated as a constrained optimization problem. The unconstrained dynamics corresponds to minimizing the quadratic objective function $\\mathcal{E}(u) = \\frac{1}{2} u H u - r u$. The unilateral contact constraint enforces $g(u_{n+1}) \\le 0$, which for $g(u)=u$ is simply $u_{n+1} \\le 0$.\n\nThe problem is thus to find the displacement $u_{n+1}$ that solves the following convex quadratic program (QP):\n$$\n\\begin{aligned}\n& \\underset{u_{n+1}}{\\text{minimize}}\n& & \\frac{1}{2} u_{n+1} H u_{n+1} - r u_{n+1} \\\\\n& \\text{subject to}\n& & u_{n+1} \\le 0\n\\end{aligned}\n$$\n\n### 3. ADMM Formulation and Updates\n\nTo solve this QP using ADMM, we introduce an auxiliary variable $z$ and transform the problem into a consensus form. The constraint $u_{n+1} \\le 0$ is enforced by requiring $z \\in \\mathcal{C} = (-\\infty, 0]$ and setting $u_{n+1} = z$.\nThe optimization problem becomes:\n$$\n\\begin{aligned}\n& \\underset{u, z}{\\text{minimize}}\n& & \\left(\\frac{1}{2} u H u - r u\\right) + I_{\\mathcal{C}}(z) \\\\\n& \\text{subject to}\n& & u - z = 0\n\\end{aligned}\n$$\nwhere $u$ is our unknown $u_{n+1}$, and $I_{\\mathcal{C}}(z)$ is the indicator function for the set $\\mathcal{C}$, which is $0$ if $z \\in \\mathcal{C}$ and $+\\infty$ otherwise.\n\nThe scaled augmented Lagrangian for this problem is:\n$$\n\\mathcal{L}_{\\rho}(u, z, y) = \\frac{1}{2} u H u - r u + I_{\\mathcal{C}}(z) + \\frac{\\rho}{2} \\lVert u - z + y \\rVert_2^2 - \\frac{\\rho}{2}\\lVert y \\rVert_2^2\n$$\nwhere $y$ is the scaled dual variable and $\\rho > 0$ is the penalty parameter. The ADMM algorithm iteratively minimizes $\\mathcal{L}_{\\rho}$ with respect to $u$ and $z$, followed by an update of the dual variable $y$. For iteration $k$, the updates are:\n\n1.  **$u$-minimization:** We update $u^{k+1}$ by minimizing $\\mathcal{L}_{\\rho}(u, z^k, y^k)$ with respect to $u$. This involves solving $\\nabla_u \\mathcal{L}_{\\rho} = 0$:\n    $$\n    H u - r + \\rho (u - z^k + y^k) = 0 \\implies (H + \\rho) u = r + \\rho(z^k - y^k)\n    $$\n    Since $H$ is a scalar, the update for $u$ is:\n    $$\n    u^{k+1} = \\frac{r + \\rho(z^k - y^k)}{H + \\rho}\n    $$\n\n2.  **$z$-minimization:** We update $z^{k+1}$ by minimizing $\\mathcal{L}_{\\rho}(u^{k+1}, z, y^k)$ with respect to $z$:\n    $$\n    z^{k+1} = \\underset{z}{\\arg\\min} \\left( I_{\\mathcal{C}}(z) + \\frac{\\rho}{2} \\lVert u^{k+1} - z + y^k \\rVert_2^2 \\right)\n    $$\n    This is a projection problem. The solution is the projection of the point $(u^{k+1} + y^k)$ onto the set $\\mathcal{C} = (-\\infty, 0]$.\n    $$\n    z^{k+1} = \\Pi_{\\mathcal{C}}(u^{k+1} + y^k) = \\min(0, u^{k+1} + y^k)\n    $$\n\n3.  **Dual variable update:** The scaled dual variable $y$ is updated as:\n    $$\n    y^{k+1} = y^k + u^{k+1} - z^{k+1}\n    $$\nThe algorithm is initialized with $u^0 = H^{-1}r$, $z^0 = \\min(0, u^0)$, and $y^0 = 0$.\n\n### 4. Monitoring and Stopping Criteria\n\nThe convergence of the algorithm is monitored using primal and dual residuals.\n-   **Primal residual:** $r_p^{k+1} = \\lVert u^{k+1} - z^{k+1} \\rVert_2 = |u^{k+1} - z^{k+1}|$. This measures the violation of the consensus constraint $u=z$.\n-   **Dual residual:** $s_d^{k+1} = \\rho \\lVert z^{k+1} - z^k \\rVert_2 = \\rho |z^{k+1} - z^k|$. This measures the change in the auxiliary variable, which relates to the stationarity condition.\n\nThe iteration terminates when both residuals are below their respective tolerances, $\\varepsilon_p$ and $\\varepsilon_d$, or when the maximum number of iterations, $N_{\\max}$, is reached.\n$$\nr_p^{k+1} < \\varepsilon_p \\quad \\text{and} \\quad s_d^{k+1} < \\varepsilon_d\n$$\n\nAdditionally, we monitor the augmented objective sequence $\\Phi_\\rho^k$:\n$$\n\\Phi_\\rho^k = \\frac{1}{2} (u^k)^2 H - r u^k + \\frac{\\rho}{2} (u^k - z^k)^2\n$$\nThe problem requires checking if this sequence is non-increasing, i.e., $\\Phi_\\rho^{k+1} \\le \\Phi_\\rho^k + \\delta$ for a small numerical slack $\\delta$. This property is a useful diagnostic for the correct implementation and convergence behavior of the algorithm.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_admm_case(case_data, admm_params):\n    \"\"\"\n    Runs the ADMM solver for a single test case.\n\n    Args:\n        case_data (tuple): Contains physical parameters (M, K, h, u_n, v_n, f_np1).\n        admm_params (dict): Contains ADMM parameters (rho, N_max, eps_p, eps_d, delta).\n\n    Returns:\n        list: A list containing the final primal residual (rounded) and a boolean\n              indicating if the objective function was non-increasing.\n    \"\"\"\n    M, K, h, u_n, v_n, f_np1 = case_data\n    rho = admm_params['rho']\n    N_max = admm_params['N_max']\n    eps_p = admm_params['eps_p']\n    eps_d = admm_params['eps_d']\n    delta = admm_params['delta']\n\n    # 1. Construct the effective system: H u + B^T lambda = r\n    # For this scalar problem, B = 1.\n    H = M / h**2 + K\n    r = f_np1 + (M / h**2) * (u_n + h * v_n)\n\n    # 2. ADMM Initialization, as per problem statement\n    # u^0 = H^{-1} r\n    u_k = r / H\n    # z^0 = min(0, u^0)\n    z_k = min(0.0, u_k)\n    # y^0 = 0\n    y_k = 0.0\n\n    obj_values = []\n    \n    # Calculate initial augmented objective Phi^0\n    # The term I_C(z^k) = 0 because z^k is always projected onto C.\n    # The formula in the problem description includes I_C(z^k), but for computation:\n    # Phi_rho^k = 0.5 * H * (u^k)^2 - r * u^k + 0.5 * rho * ||u^k - z^k||^2\n    obj_k = 0.5 * H * u_k**2 - r * u_k + 0.5 * rho * (u_k - z_k)**2\n    obj_values.append(obj_k)\n    \n    final_rp = np.abs(u_k - z_k)\n    \n    # 3. ADMM Iteration Loop\n    for _ in range(N_max):\n        z_prev = z_k\n\n        # u-update (solve u-minimization problem)\n        u_k = (r + rho * (z_k - y_k)) / (H + rho)\n\n        # z-update (projection onto C = (-inf, 0])\n        z_k = min(0.0, u_k + y_k)\n\n        # y-update (dual variable update)\n        y_k = y_k + u_k - z_k\n\n        # Calculate current augmented objective Phi^{k+1}\n        obj_k = 0.5 * H * u_k**2 - r * u_k + 0.5 * rho * (u_k - z_k)**2\n        obj_values.append(obj_k)\n\n        # Calculate residuals for stopping criteria\n        # Primal residual r_p = ||u^k - z^k||_2\n        # Dual residual s_d = rho * ||z^k - z^{k-1}||_2\n        r_p = np.abs(u_k - z_k)\n        s_d = rho * np.abs(z_k - z_prev)\n\n        final_rp = r_p\n        \n        # Check for convergence\n        if r_p < eps_p and s_d < eps_d:\n            break\n\n    # 4. Check if the augmented objective sequence is non-increasing\n    is_non_increasing = True\n    for i in range(len(obj_values) - 1):\n        if obj_values[i+1] > obj_values[i] + delta:\n            is_non_increasing = False\n            break\n            \n    # 5. Return the required results\n    return [round(final_rp, 6), is_non_increasing]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the solver, and print results.\n    \"\"\"\n    # Define common ADMM parameters for all test cases\n    ADMM_PARAMS = {\n        'rho': 1000.0,\n        'N_max': 200,\n        'eps_p': 1e-10,\n        'eps_d': 1e-10,\n        'delta': 1e-12, # Slack for non-increasing objective check\n    }\n\n    # Define the test cases from the problem statement\n    # Each tuple is (M, K, h, u_n, v_n, f_{n+1})\n    test_cases = [\n        # 1. Happy-path active contact\n        (1.0, 1000.0, 0.05, -0.01, 0.0, 30.0),\n        # 2. Strictly inactive contact\n        (1.0, 1000.0, 0.05, -0.01, 0.0, 0.0),\n        # 3. Boundary (just active) contact\n        (1.0, 1000.0, 0.05, -0.01, 0.0, 4.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_admm_case(case, ADMM_PARAMS)\n        results.append(result)\n\n    # Format the final output string exactly as required\n    results_str = [f\"[{r[0]},{r[1]}]\" for r in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        }
    ]
}