{
    "hands_on_practices": [
        {
            "introduction": "At its core, hyperreduction seeks to efficiently approximate the integrals that arise in the weak form of nonlinear mechanics. This exercise explores an idealized scenario where this approximation can be made exact, revealing a deep connection between modern model reduction and classical numerical analysis. By deriving a hyperreduction rule for a polynomial energy function , you will discover how the principles of Gaussian quadrature provide a powerful theoretical foundation for constructing efficient and accurate reduced-order models.",
            "id": "3572656",
            "problem": "Consider a one-dimensional nonlinear reduced-order model of a bar with reference coordinate $x \\in [-1,1]$, in which the internal energy over the element is approximated by a hyperreduction rule. The exact internal energy is given by the integral $\\int_{-1}^{1} \\psi(\\varepsilon(x)) \\,\\mathrm{d}x$, where the strain field is affine, $\\varepsilon(x) = \\varepsilon_{0} + \\varepsilon_{1} x$, and the energy density $\\psi(\\varepsilon)$ is a polynomial in $\\varepsilon$ of degree at most $k$. The hyperreduced internal energy is constructed as $\\sum_{i=1}^{n} w_{i} \\,\\psi(\\varepsilon(x_{i}))$, where the sampling points $x_{i} \\in [-1,1]$ and weights $w_{i} > 0$ are to be chosen to guarantee exactness for any polynomial energy density of degree at most $k$ under the given affine strain.\n\nStarting from first principles in computational solid mechanics and the definition of quadrature exactness, design such a hyperreduction for the case $k = 3$ that is exact for all monomials in $x$ up to degree $3$. Determine the smallest $n$ and compute the corresponding sampling points $x_{i}$ and weights $w_{i}$ that achieve this exactness. Express your final answer as a single row matrix containing the weights followed by the points in the order $(w_{1}, w_{2}, x_{1}, x_{2})$. No physical units are associated with the final answer.",
            "solution": "The problem requires the design of a hyperreduction scheme for a one-dimensional nonlinear mechanical model. This scheme must exactly compute the internal energy, $E = \\int_{-1}^{1} \\psi(\\varepsilon(x)) \\,\\mathrm{d}x$, using a weighted sum at a finite number of sampling points, $E_{hr} = \\sum_{i=1}^{n} w_{i} \\,\\psi(\\varepsilon(x_{i}))$. The condition is that this approximation must be exact ($E = E_{hr}$) for any energy density function $\\psi(\\varepsilon)$ that is a polynomial in strain $\\varepsilon$ of degree at most $k=3$, and for any affine strain field $\\varepsilon(x) = \\varepsilon_{0} + \\varepsilon_{1} x$. We are tasked with finding the minimum number of sampling points $n$ and the corresponding points $x_i$ and positive weights $w_i$.\n\nFirst, we analyze the integrand, $\\psi(\\varepsilon(x))$. Since $\\psi$ is a polynomial in $\\varepsilon$ of degree at most $3$, it can be written as $\\psi(\\varepsilon) = c_3 \\varepsilon^3 + c_2 \\varepsilon^2 + c_1 \\varepsilon + c_0$ for some coefficients $c_j$. Substituting the affine strain field $\\varepsilon(x) = \\varepsilon_{0} + \\varepsilon_{1} x$, the integrand becomes:\n$$\n\\psi(\\varepsilon(x)) = c_3 (\\varepsilon_{0} + \\varepsilon_{1} x)^3 + c_2 (\\varepsilon_{0} + \\varepsilon_{1} x)^2 + c_1 (\\varepsilon_{0} + \\varepsilon_{1} x) + c_0\n$$\nExpanding this expression shows that $\\psi(\\varepsilon(x))$ is a polynomial in the spatial coordinate $x$ of degree at most $3$. The requirement that the hyperreduction scheme is exact for any choice of polynomial $\\psi$ (i.e., any $c_j$) and any affine strain (i.e., any $\\varepsilon_0, \\varepsilon_1$) is equivalent to requiring that the quadrature rule $\\int_{-1}^{1} f(x) \\,\\mathrm{d}x = \\sum_{i=1}^{n} w_i f(x_i)$ is exact for any function $f(x)$ that is a polynomial in $x$ of degree at most $3$. This is because the basis functions for polynomials in $x$ of degree $0, 1, 2, 3$ (i.e., $\\{1, x, x^2, x^3\\}$) can be generated by appropriate choices of $\\psi$, $\\varepsilon_0$, and $\\varepsilon_1$.\n\nThe problem thus reduces to finding the numerical quadrature rule with the minimum number of points, $n$, that is exact for all polynomials of degree up to $3$ on the interval $[-1, 1]$.\n\nFrom the theory of numerical integration, a quadrature rule with $n$ points and $n$ weights has $2n$ free parameters ($x_i$ and $w_i$). It can be made exact for polynomials of degree up to $2n-1$. This optimal rule is known as Gaussian quadrature. The problem requires exactness for polynomials of degree up to $3$. Therefore, we must satisfy the condition:\n$$\n2n - 1 \\ge 3\n$$\nThis implies $2n \\ge 4$, so $n \\ge 2$. The smallest integer $n$ that satisfies this condition is $n=2$. We therefore seek a $2$-point quadrature rule.\n\nWe need to find two points, $x_1, x_2$, and two weights, $w_1, w_2$, such that the following equality holds for any polynomial $P(x)$ of degree at most $3$:\n$$\n\\int_{-1}^{1} P(x) \\,\\mathrm{d}x = w_1 P(x_1) + w_2 P(x_2)\n$$\nBy linearity, it suffices to enforce this condition for a basis of the polynomial space, which we choose as the monomials $\\{1, x, x^2, x^3\\}$. This yields a system of four equations:\n\n$1$. For $P(x) = 1$:\n$$\n\\int_{-1}^{1} 1 \\,\\mathrm{d}x = [x]_{-1}^{1} = 2 \\quad \\implies \\quad w_1 \\cdot 1 + w_2 \\cdot 1 = 2 \\quad \\implies \\quad w_1 + w_2 = 2\n$$\n\n$2$. For $P(x) = x$:\n$$\n\\int_{-1}^{1} x \\,\\mathrm{d}x = \\left[\\frac{x^2}{2}\\right]_{-1}^{1} = 0 \\quad \\implies \\quad w_1 x_1 + w_2 x_2 = 0\n$$\n\n$3$. For $P(x) = x^2$:\n$$\n\\int_{-1}^{1} x^2 \\,\\mathrm{d}x = \\left[\\frac{x^3}{3}\\right]_{-1}^{1} = \\frac{2}{3} \\quad \\implies \\quad w_1 x_1^2 + w_2 x_2^2 = \\frac{2}{3}\n$$\n\n$4$. For $P(x) = x^3$:\n$$\n\\int_{-1}^{1} x^3 \\,\\mathrm{d}x = \\left[\\frac{x^4}{4}\\right]_{-1}^{1} = 0 \\quad \\implies \\quad w_1 x_1^3 + w_2 x_2^3 = 0\n$$\n\nWe now solve this system of four nonlinear equations for the four unknowns $w_1, w_2, x_1, x_2$.\nDue to the symmetry of the integration interval $[-1, 1]$, a common Ansatz for the points and weights is symmetry: $x_2 = -x_1$ and $w_2 = w_1$. Let's test this assumption.\nIf $w_1 = w_2$, the first equation gives $w_1 + w_1 = 2 \\implies 2w_1 = 2 \\implies w_1=1$. Thus, $w_1 = w_2 = 1$. These weights are positive as required.\n\nSubstituting $w_1=w_2=1$ into the second equation:\n$$\n1 \\cdot x_1 + 1 \\cdot x_2 = 0 \\quad \\implies \\quad x_1 + x_2 = 0 \\quad \\implies \\quad x_2 = -x_1\n$$\nThis confirms the symmetry of the points. The second and fourth equations are automatically satisfied with this symmetry:\nEquation 2: $x_1 + (-x_1) = 0$.\nEquation 4: $x_1^3 + (-x_1)^3 = x_1^3 - x_1^3 = 0$.\n\nWe are left with the third equation, which will determine the value of the points. Substituting $w_1=w_2=1$ and $x_2=-x_1$ into the third equation:\n$$\n1 \\cdot x_1^2 + 1 \\cdot (-x_1)^2 = \\frac{2}{3} \\quad \\implies \\quad x_1^2 + x_1^2 = \\frac{2}{3} \\quad \\implies \\quad 2x_1^2 = \\frac{2}{3}\n$$\nThis simplifies to $x_1^2 = \\frac{1}{3}$. The solutions are $x_1 = \\pm \\frac{1}{\\sqrt{3}}$.\nIf we set $x_1 = -\\frac{1}{\\sqrt{3}}$, then $x_2 = -x_1 = \\frac{1}{\\sqrt{3}}$.\nThe sampling points are $x_1 = -\\frac{1}{\\sqrt{3}}$ and $x_2 = \\frac{1}{\\sqrt{3}}$, and the corresponding weights are $w_1 = 1$ and $w_2 = 1$.\nRationalizing the denominator, the points are $x_1 = -\\frac{\\sqrt{3}}{3}$ and $x_2 = \\frac{\\sqrt{3}}{3}$.\n\nThis result corresponds to the well-known $2$-point Gauss-Legendre quadrature rule, where the sampling points are the roots of the Legendre polynomial of degree two, $P_2(x) = \\frac{1}{2}(3x^2-1)$, and the weights are found to be unity.\n\nThe minimal number of points is $n=2$. The sampling points are $\\pm \\frac{\\sqrt{3}}{3}$ and the weights are both $1$. Arranging these in the requested order $(w_1, w_2, x_1, x_2)$, where we order the points as $x_1 < x_2$:\n$w_1 = 1$, $w_2 = 1$, $x_1 = -\\frac{\\sqrt{3}}{3}$, $x_2 = \\frac{\\sqrt{3}}{3}$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1 & 1 & -\\frac{\\sqrt{3}}{3} & \\frac{\\sqrt{3}}{3} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "In most real-world applications, nonlinearities are too complex for exact integration, so we must resort to approximation. This practice  delves into the mechanics of two foundational hyperreduction techniques: the least-squares-based Gappy Proper Orthogonal Decomposition (POD) and the interpolation-based Discrete Empirical Interpolation Method (DEIM). By performing the explicit calculations for a small, illustrative system, you will gain concrete, hands-on insight into how these methods reconstruct a nonlinear state from only a small subset of its entries.",
            "id": "3572677",
            "problem": "Consider a nonlinear finite element internal force vector $f(u) \\in \\mathbb{R}^{8}$ arising from the spatial discretization of a one-dimensional hyperelastic bar in computational solid mechanics. In a reduced-order modeling context, approximate the nonlinear internal force $f(u)$ in the span of a nonlinear term basis $U \\in \\mathbb{R}^{8 \\times 3}$ constructed from snapshots. Hyperreduction techniques seek to recover the coefficients using only a subset of entries of $f(u)$.\n\nStart from the principle of virtual work and the finite element residual, which yields a discrete internal force $f(u)$ satisfying equilibrium $f(u) - F_{\\mathrm{ext}} = 0$, where $F_{\\mathrm{ext}}$ is the external force vector. A nonlinear reduced model represents the internal force as $f(u) \\approx U c(u)$ for some coefficient vector $c(u) \\in \\mathbb{R}^{3}$. Let a row-sampling operator be given by a selection matrix $P \\in \\mathbb{R}^{8 \\times 4}$ formed from standard basis vectors so that $P^{\\top} f(u)$ extracts $4$ entries of $f(u)$. In gappy Proper Orthogonal Decomposition (gappy POD), the coefficients are found by a least-squares fit that minimizes the sampled misfit $\\|P^{\\top}(Uc - f)\\|_{2}$. In the Discrete Empirical Interpolation Method (DEIM), the coefficients are instead determined by an interpolation at $r_{f}$ selected entries such that $P_{\\mathrm{DEIM}}^{\\top} U c = P_{\\mathrm{DEIM}}^{\\top} f$.\n\nUsing the following specific data that are consistent with the above framework, compute the gappy POD reconstruction $\\hat f$ and the DEIM reconstruction $\\tilde f$, and then compute the ratio of the Euclidean norms of their reconstruction errors:\n- Dimension $N = 8$, reduced nonlinear basis size $r_{f} = 3$, and $4$ sampled entries.\n- Nonlinear term basis $U = [u_{1}\\ u_{2}\\ u_{3}] \\in \\mathbb{R}^{8 \\times 3}$ with columns\n  $u_{1} = [1, 1, 1, 1, 1, 1, 1, 1]^{\\top}$,\n  $u_{2} = [1, 2, 3, 4, 5, 6, 7, 8]^{\\top}$,\n  $u_{3} = [1, 0, 1, 0, 1, 0, 1, 0]^{\\top}$.\n- Sampling selection $\\mathcal{S} = \\{2, 4, 6, 7\\}$, so $P = [e_{2}\\ e_{4}\\ e_{6}\\ e_{7}] \\in \\mathbb{R}^{8 \\times 4}$ where $e_{i}$ is the $i$-th standard basis vector in $\\mathbb{R}^{8}$.\n- Internal force vector $f \\in \\mathbb{R}^{8}$ given by\n  $f = U c^{\\star} + w$ with $c^{\\star} = [1, -\\tfrac{1}{2}, 2]^{\\top}$ and $w = \\tfrac{1}{2}[0, 1, 0, -1, 0, 1, 0, -1]^{\\top}$.\n- For the Discrete Empirical Interpolation Method (DEIM), use interpolation indices $\\{2, 6, 7\\}$, i.e., $P_{\\mathrm{DEIM}} = [e_{2}\\ e_{6}\\ e_{7}] \\in \\mathbb{R}^{8 \\times 3}$.\n\nDerive the coefficient recovery formulas from the least-squares principle for gappy Proper Orthogonal Decomposition (gappy POD) and the interpolation condition for the Discrete Empirical Interpolation Method (DEIM). Then compute explicitly:\n- The gappy POD reconstruction $\\hat f = U \\hat c$, where $\\hat c$ minimizes $\\|P^{\\top}(Uc - f)\\|_{2}$.\n- The DEIM reconstruction $\\tilde f = U \\tilde c$, where $\\tilde c$ satisfies $P_{\\mathrm{DEIM}}^{\\top} U \\tilde c = P_{\\mathrm{DEIM}}^{\\top} f$.\n\nFinally, evaluate the errors $e_{g} = \\|f - \\hat f\\|_{2}$ and $e_{d} = \\|f - \\tilde f\\|_{2}$, and report the ratio $e_{g}/e_{d}$ as a single closed-form analytic expression. No rounding is required. Express the final answer without units.",
            "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is based on established principles of reduced-order modeling and hyperreduction, specifically gappy Proper Orthogonal Decomposition (gappy POD) and the Discrete Empirical Interpolation Method (DEIM). All data and conditions required for a unique solution are provided and are self-consistent.\n\nThe problem asks for the computation of two approximations to a vector $f \\in \\mathbb{R}^{8}$, denoted $\\hat{f}$ (from gappy POD) and $\\tilde{f}$ (from DEIM), and the ratio of their reconstruction errors. Both approximations lie in the subspace spanned by the columns of a matrix $U \\in \\mathbb{R}^{8 \\times 3}$.\n\nFirst, we define the given quantities.\nThe nonlinear term basis is $U = [u_{1}\\ u_{2}\\ u_{3}]$, where\n$$\nu_{1} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}, \\quad\nu_{2} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\\\ 6 \\\\ 7 \\\\ 8 \\end{pmatrix}, \\quad\nu_{3} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n\\implies\nU = \\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 2 & 0 \\\\\n1 & 3 & 1 \\\\\n1 & 4 & 0 \\\\\n1 & 5 & 1 \\\\\n1 & 6 & 0 \\\\\n1 & 7 & 1 \\\\\n1 & 8 & 0\n\\end{pmatrix}\n$$\nThe target internal force vector is $f = U c^{\\star} + w$, with $c^{\\star} = \\begin{pmatrix} 1 \\\\ -1/2 \\\\ 2 \\end{pmatrix}$ and $w = \\frac{1}{2} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$. Let's compute $f$:\n$$\nU c^{\\star} = \\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & 2 & 0 \\\\\n1 & 3 & 1 \\\\\n1 & 4 & 0 \\\\\n1 & 5 & 1 \\\\\n1 & 6 & 0 \\\\\n1 & 7 & 1 \\\\\n1 & 8 & 0\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ -1/2 \\\\ 2 \\end{pmatrix}\n= \\begin{pmatrix}\n1 - 1/2 + 2 \\\\\n1 - 1 + 0 \\\\\n1 - 3/2 + 2 \\\\\n1 - 2 + 0 \\\\\n1 - 5/2 + 2 \\\\\n1 - 3 + 0 \\\\\n1 - 7/2 + 2 \\\\\n1 - 4 + 0\n\\end{pmatrix}\n= \\begin{pmatrix} 5/2 \\\\ 0 \\\\ 3/2 \\\\ -1 \\\\ 1/2 \\\\ -2 \\\\ -1/2 \\\\ -3 \\end{pmatrix}\n$$\n$$\nf = U c^{\\star} + w = \\begin{pmatrix} 5/2 \\\\ 0 \\\\ 3/2 \\\\ -1 \\\\ 1/2 \\\\ -2 \\\\ -1/2 \\\\ -3 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\\\ -1/2 \\\\ 0 \\\\ 1/2 \\\\ 0 \\\\ -1/2 \\end{pmatrix} = \\begin{pmatrix} 5/2 \\\\ 1/2 \\\\ 3/2 \\\\ -3/2 \\\\ 1/2 \\\\ -3/2 \\\\ -1/2 \\\\ -7/2 \\end{pmatrix}\n$$\n\n**1. Gappy Proper Orthogonal Decomposition (Gappy POD)**\n\nThe gappy POD approximation is $\\hat f = U \\hat c$, where the coefficient vector $\\hat c \\in \\mathbb{R}^{3}$ is the solution to the least-squares problem:\n$$\n\\hat c = \\underset{c \\in \\mathbb{R}^{3}}{\\operatorname{argmin}} \\|P^{\\top}(Uc - f)\\|_{2}^{2}\n$$\nThe sampling matrix $P = [e_{2}\\ e_{4}\\ e_{6}\\ e_{7}]$ implies that the operator $P^{\\top}$ selects rows $2, 4, 6, 7$. Let's define the sampled matrix $U_{S} = P^{\\top}U$ and sampled vector $f_{S} = P^{\\top}f$.\n$$\nU_{S} = \\begin{pmatrix}\n1 & 2 & 0 \\\\\n1 & 4 & 0 \\\\\n1 & 6 & 0 \\\\\n1 & 7 & 1\n\\end{pmatrix}, \\quad\nf_{S} = \\begin{pmatrix} 1/2 \\\\ -3/2 \\\\ -3/2 \\\\ -1/2 \\end{pmatrix}\n$$\nThe least-squares solution is given by the normal equations:\n$$\n(U_{S}^{\\top} U_{S}) \\hat c = U_{S}^{\\top} f_{S}\n$$\nWe compute the components:\n$$\nU_{S}^{\\top} U_{S} = \\begin{pmatrix}\n1 & 1 & 1 & 1 \\\\\n2 & 4 & 6 & 7 \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 2 & 0 \\\\\n1 & 4 & 0 \\\\\n1 & 6 & 0 \\\\\n1 & 7 & 1\n\\end{pmatrix}\n= \\begin{pmatrix}\n4 & 19 & 1 \\\\\n19 & 105 & 7 \\\\\n1 & 7 & 1\n\\end{pmatrix}\n$$\n$$\nU_{S}^{\\top} f_{S} = \\begin{pmatrix}\n1 & 1 & 1 & 1 \\\\\n2 & 4 & 6 & 7 \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix} 1/2 \\\\ -3/2 \\\\ -3/2 \\\\ -1/2 \\end{pmatrix}\n= \\begin{pmatrix}\n1/2 - 3/2 - 3/2 - 1/2 \\\\\n1 - 6 - 9 - 7/2 \\\\\n-1/2\n\\end{pmatrix}\n= \\begin{pmatrix} -3 \\\\ -35/2 \\\\ -1/2 \\end{pmatrix}\n$$\nThe determinant of $U_{S}^{\\top} U_{S}$ is $4(105-49) - 19(19-7) + 1(133-105) = 224 - 228 + 28 = 24$. The matrix is invertible. The inverse is:\n$$\n(U_{S}^{\\top} U_{S})^{-1} = \\frac{1}{24} \\begin{pmatrix}\n56 & -12 & 28 \\\\\n-12 & 3 & -9 \\\\\n28 & -9 & 59\n\\end{pmatrix}\n$$\nNow we solve for $\\hat c = (U_{S}^{\\top} U_{S})^{-1} (U_{S}^{\\top} f_{S})$:\n$$\n\\hat c = \\frac{1}{24} \\begin{pmatrix}\n56 & -12 & 28 \\\\\n-12 & 3 & -9 \\\\\n28 & -9 & 59\n\\end{pmatrix}\n\\begin{pmatrix} -3 \\\\ -35/2 \\\\ -1/2 \\end{pmatrix}\n= \\frac{1}{24} \\begin{pmatrix}\n56(-3) - 12(-35/2) + 28(-1/2) \\\\\n-12(-3) + 3(-35/2) - 9(-1/2) \\\\\n28(-3) - 9(-35/2) + 59(-1/2)\n\\end{pmatrix}\n= \\frac{1}{24} \\begin{pmatrix}\n-168 + 210 - 14 \\\\\n36 - 105/2 + 9/2 \\\\\n-84 + 315/2 - 59/2\n\\end{pmatrix}\n$$\n$$\n\\hat c = \\frac{1}{24} \\begin{pmatrix}\n28 \\\\\n36 - 48 \\\\\n-84 + 128\n\\end{pmatrix}\n= \\frac{1}{24} \\begin{pmatrix}\n28 \\\\\n-12 \\\\\n44\n\\end{pmatrix}\n= \\begin{pmatrix} 7/6 \\\\ -1/2 \\\\ 11/6 \\end{pmatrix}\n$$\n\n**2. Discrete Empirical Interpolation Method (DEIM)**\n\nThe DEIM approximation is $\\tilde f = U \\tilde c$, where the coefficient vector $\\tilde c \\in \\mathbb{R}^{3}$ is the solution to the linear system:\n$$\nP_{\\mathrm{DEIM}}^{\\top} U \\tilde c = P_{\\mathrm{DEIM}}^{\\top} f\n$$\nThe DEIM selection matrix is $P_{\\mathrm{DEIM}} = [e_{2}\\ e_{6}\\ e_{7}]$, so $P_{\\mathrm{DEIM}}^{\\top}$ selects rows $2, 6, 7$. Let's define the interpolation matrix $U_{I} = P_{\\mathrm{DEIM}}^{\\top}U$ and interpolated vector $f_{I} = P_{\\mathrm{DEIM}}^{\\top}f$.\n$$\nU_{I} = \\begin{pmatrix}\n1 & 2 & 0 \\\\\n1 & 6 & 0 \\\\\n1 & 7 & 1\n\\end{pmatrix}, \\quad\nf_{I} = \\begin{pmatrix} 1/2 \\\\ -3/2 \\\\ -1/2 \\end{pmatrix}\n$$\nWe solve the system $U_{I} \\tilde c = f_{I}$ for $\\tilde c$. The determinant of $U_{I}$ is $1(6-0) - 2(1-0) + 0 = 4$. The matrix is invertible.\n$$\nU_{I}^{-1} = \\frac{1}{4} \\operatorname{adj}(U_{I}) = \\frac{1}{4} \\begin{pmatrix}\n6 & -2 & 0 \\\\\n-1 & 1 & 0 \\\\\n1 & -5 & 4\n\\end{pmatrix}\n$$\nNow we solve for $\\tilde c = U_{I}^{-1} f_{I}$:\n$$\n\\tilde c = \\frac{1}{4} \\begin{pmatrix}\n6 & -2 & 0 \\\\\n-1 & 1 & 0 \\\\\n1 & -5 & 4\n\\end{pmatrix}\n\\begin{pmatrix} 1/2 \\\\ -3/2 \\\\ -1/2 \\end{pmatrix}\n= \\frac{1}{4} \\begin{pmatrix}\n6(1/2) - 2(-3/2) \\\\\n-1(1/2) + 1(-3/2) \\\\\n1(1/2) - 5(-3/2) + 4(-1/2)\n\\end{pmatrix}\n= \\frac{1}{4} \\begin{pmatrix}\n3 + 3 \\\\\n-1/2 - 3/2 \\\\\n1/2 + 15/2 - 2\n\\end{pmatrix}\n$$\n$$\n\\tilde c = \\frac{1}{4} \\begin{pmatrix}\n6 \\\\\n-2 \\\\\n6\n\\end{pmatrix}\n= \\begin{pmatrix} 3/2 \\\\ -1/2 \\\\ 3/2 \\end{pmatrix}\n$$\n\n**3. Reconstruction Errors**\n\nThe reconstruction error is the difference between the original vector $f$ and its approximation.\nThe error can be expressed as $f - U c = (U c^{\\star} + w) - U c = U(c^{\\star} - c) + w$.\n\nFor gappy POD, the error is $e_{g, \\text{vec}} = f - \\hat{f} = U(c^{\\star} - \\hat c) + w$.\n$$\nc^{\\star} - \\hat c = \\begin{pmatrix} 1 \\\\ -1/2 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 7/6 \\\\ -1/2 \\\\ 11/6 \\end{pmatrix} = \\begin{pmatrix} -1/6 \\\\ 0 \\\\ 1/6 \\end{pmatrix}\n$$\n$$\nU(c^{\\star} - \\hat c) = \\frac{1}{6} U \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} -1+0+1 \\\\ -1+0+0 \\\\ -1+0+1 \\\\ -1+0+0 \\\\ -1+0+1 \\\\ -1+0+0 \\\\ -1+0+1 \\\\ -1+0+0 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 0 & -1 & 0 & -1 & 0 & -1 & 0 & -1 \\end{pmatrix}^{\\top}\n$$\n$$\ne_{g, \\text{vec}} = \\frac{1}{6} \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ -1 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ -1 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ -1 \\end{pmatrix} + \\frac{1}{6} \\begin{pmatrix} 0 \\\\ 3 \\\\ 0 \\\\ -3 \\\\ 0 \\\\ 3 \\\\ 0 \\\\ -3 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 0 \\\\ 2 \\\\ 0 \\\\ -4 \\\\ 0 \\\\ 2 \\\\ 0 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1/3 \\\\ 0 \\\\ -2/3 \\\\ 0 \\\\ 1/3 \\\\ 0 \\\\ -2/3 \\end{pmatrix}\n$$\nThe squared Euclidean norm of the gappy POD error is:\n$$\ne_{g}^{2} = \\|f - \\hat f\\|_{2}^{2} = 0^{2} + (\\frac{1}{3})^{2} + 0^{2} + (-\\frac{2}{3})^{2} + 0^{2} + (\\frac{1}{3})^{2} + 0^{2} + (-\\frac{2}{3})^{2} = \\frac{1}{9} + \\frac{4}{9} + \\frac{1}{9} + \\frac{4}{9} = \\frac{10}{9}\n$$\nSo, $e_{g} = \\sqrt{\\frac{10}{9}} = \\frac{\\sqrt{10}}{3}$.\n\nFor DEIM, the error is $e_{d, \\text{vec}} = f - \\tilde{f} = U(c^{\\star} - \\tilde c) + w$.\n$$\nc^{\\star} - \\tilde c = \\begin{pmatrix} 1 \\\\ -1/2 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 3/2 \\\\ -1/2 \\\\ 3/2 \\end{pmatrix} = \\begin{pmatrix} -1/2 \\\\ 0 \\\\ 1/2 \\end{pmatrix}\n$$\n$$\nU(c^{\\star} - \\tilde c) = \\frac{1}{2} U \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ -1 \\end{pmatrix}\n$$\n$$\ne_{d, \\text{vec}} = \\frac{1}{2} \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ -1 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ -1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ -2 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ -1 \\end{pmatrix}\n$$\nThe squared Euclidean norm of the DEIM error is:\n$$\ne_{d}^{2} = \\|f - \\tilde f\\|_{2}^{2} = 0^{2} + 0^{2} + 0^{2} + (-1)^{2} + 0^{2} + 0^{2} + 0^{2} + (-1)^{2} = 1 + 1 = 2\n$$\nSo, $e_{d} = \\sqrt{2}$.\n\n**4. Ratio of Errors**\n\nFinally, the ratio of the error norms is:\n$$\n\\frac{e_{g}}{e_{d}} = \\frac{\\frac{\\sqrt{10}}{3}}{\\sqrt{2}} = \\frac{\\sqrt{10}}{3\\sqrt{2}} = \\frac{\\sqrt{5 \\cdot 2}}{3\\sqrt{2}} = \\frac{\\sqrt{5}\\sqrt{2}}{3\\sqrt{2}} = \\frac{\\sqrt{5}}{3}\n$$",
            "answer": "$$\n\\boxed{\\frac{\\sqrt{5}}{3}}\n$$"
        },
        {
            "introduction": "The true power of hyperreduction is realized when it is embedded within a nonlinear solver to accelerate simulations. This hands-on coding exercise  bridges the gap between theory and application by asking you to implement a Newton-Raphson step using a hyperreduced Jacobian and residual. You will investigate how different sampling and weighting strategies affect the convergence of the solver, providing a practical understanding of the trade-offs involved in designing a stable and efficient hyperreduced model.",
            "id": "3572702",
            "problem": "Consider a discrete nonlinear equilibrium system in computational solid mechanics defined by the residual vector $r(u) \\in \\mathbb{R}^n$ and its Jacobian matrix $J(u) \\in \\mathbb{R}^{n \\times n}$, where $u \\in \\mathbb{R}^n$ is the nodal unknown vector. The residual is given by\n$$\nr(u) = K u + \\alpha\\, u^{\\circ 3} - f,\n$$\nwhere $K \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite stiffness-like matrix, $f \\in \\mathbb{R}^n$ is a load vector, $\\alpha \\in \\mathbb{R}$ is a scalar nonlinearity parameter, and $u^{\\circ 3}$ denotes the element-wise cubic of $u$. The Jacobian of the residual is\n$$\nJ(u) = K + \\operatorname{diag}\\!\\left(3\\alpha\\, u^{\\circ 2}\\right),\n$$\nwith $u^{\\circ 2}$ the element-wise square of $u$.\n\nA reduced model is constructed with a two-dimensional reduced basis $\\Phi \\in \\mathbb{R}^{n \\times r}$, $r = 2$, so that the state is approximated as $u \\approx \\Phi a$, with $a \\in \\mathbb{R}^r$ the reduced coordinates. To perform one Newton step in reduced coordinates, a hyperreduced Jacobian-action is used. Specifically, define a sampling set of row indices $S \\subset \\{0,1,\\dots,n-1\\}$ and positive weights $w \\in \\mathbb{R}^{|S|}$; let $W = \\operatorname{diag}(w) \\in \\mathbb{R}^{|S| \\times |S|}$. Denote by $X_S$ the matrix obtained by selecting the rows of a matrix $X$ indexed by $S$, and by $v_S$ the vector obtained by selecting the entries of a vector $v$ indexed by $S$. The hyperreduced Jacobian-action constructs the reduced linear system\n$$\nA_{\\mathrm{HR}}(u) \\,\\Delta a = b_{\\mathrm{HR}}(u),\n$$\nwhere\n$$\nA_{\\mathrm{HR}}(u) = \\Phi_S^\\top W \\left(J(u)\\Phi\\right)_S \\in \\mathbb{R}^{r \\times r}, \\quad \nb_{\\mathrm{HR}}(u) = -\\Phi_S^\\top W \\, r(u)_S \\in \\mathbb{R}^{r}.\n$$\nThe Newton update is $a^{+} = a + \\Delta a$, and the updated full state is $u^{+} = \\Phi a^{+}$. The residual decrease is measured by the Euclidean norm ratio $\\|r(u^{+})\\|_2 / \\|r(u)\\|_2$ and a boolean indicating whether the norm strictly decreases.\n\nUse the following data, with $n = 5$ and $r = 2$ fixed for all test cases. The stiffness matrix $K$ is the tridiagonal matrix\n$$\nK = \\begin{bmatrix}\n2 & -1 & 0 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 & 0 \\\\\n0 & -1 & 2 & -1 & 0 \\\\\n0 & 0 & -1 & 2 & -1 \\\\\n0 & 0 & 0 & -1 & 2\n\\end{bmatrix}.\n$$\nThe reduced basis $\\Phi$ has columns\n$$\n\\phi_1 = \\frac{1}{\\sqrt{5}}\\begin{bmatrix}1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1\\end{bmatrix}, \\quad\n\\phi_2 = \\frac{1}{\\sqrt{3}}\\begin{bmatrix}1 \\\\ 0 \\\\ -1 \\\\ 0 \\\\ 1\\end{bmatrix}, \\quad\n\\Phi = \\begin{bmatrix}\\phi_1 & \\phi_2\\end{bmatrix}.\n$$\n\nThere are three test cases, each specifying $(\\alpha, f, a, S, w)$, with indices in $S$ given in $0$-based convention:\n\n- Test case $1$ (moderate nonlinearity, well-chosen sampling):\n  $$\n  \\alpha = 0.5,\\quad \n  f = \\begin{bmatrix}1 \\\\ 0 \\\\ 0.5 \\\\ 0 \\\\ -1\\end{bmatrix},\\quad\n  a = \\begin{bmatrix}0.1 \\\\ -0.2\\end{bmatrix},\\quad\n  S = \\{0, 2, 4\\},\\quad\n  w = \\begin{bmatrix}1.6 \\\\ 1.2 \\\\ 1.2\\end{bmatrix}.\n  $$\n\n- Test case $2$ (linear case, full sampling):\n  $\n  \\alpha = 0.0,\\quad \n  f = \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0.5\\end{bmatrix},\\quad\n  a = \\begin{bmatrix}0.0 \\\\ 0.0\\end{bmatrix},\\quad\n  S = \\{0, 1, 2, 3, 4\\},\\quad\n  w = \\begin{bmatrix}1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1\\end{bmatrix}.\n  $\n\n- Test case $3$ (strong nonlinearity, poor sampling):\n  $\n  \\alpha = 1.5,\\quad \n  f = \\begin{bmatrix}-0.5 \\\\ 0.2 \\\\ 0 \\\\ 0.1 \\\\ -0.1\\end{bmatrix},\\quad\n  a = \\begin{bmatrix}-0.3 \\\\ 0.15\\end{bmatrix},\\quad\n  S = \\{3, 4\\},\\quad\n  w = \\begin{bmatrix}2.5 \\\\ 2.5\\end{bmatrix}.\n  $\n\nProgram requirements:\n- For each test case, compute $u = \\Phi a$, form $r(u)$ and $J(u)$, assemble $A_{\\mathrm{HR}}(u)$ and $b_{\\mathrm{HR}}(u)$, solve for $\\Delta a$, update $a^{+}$ and $u^{+}$, and evaluate $\\|r(u)\\|_2$, $\\|r(u^{+})\\|_2$, the ratio $\\|r(u^{+})\\|_2 / \\|r(u)\\|_2$, and a boolean indicating whether $\\|r(u^{+})\\|_2 < \\|r(u)\\|_2$.\n- If $\\|r(u)\\|_2 = 0$, define the ratio as a Not-a-Number indicator; otherwise compute the ratio as specified. In all cases, report the boolean based on strict inequality.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is an inner list of the form $[$norm\\_before, norm\\_after, ratio, improved$]$. Angles are not used and no physical units apply; all outputs are dimensionless real numbers. Use floating-point numbers for norms and ratios and a boolean for the improvement indicator. For example, the output format must look like $[[x_1,y_1,z_1,\\mathrm{True}],[x_2,y_2,z_2,\\mathrm{False}],\\dots]$.",
            "solution": "The problem is subjected to validation against the stipulated criteria.\n\n### Step 1: Extract Givens\nThe problem statement provides the following definitions and data:\n- **System Equations**:\n  - Residual vector: $r(u) = K u + \\alpha\\, u^{\\circ 3} - f$, where $u \\in \\mathbb{R}^n$, $K \\in \\mathbb{R}^{n \\times n}$, $\\alpha \\in \\mathbb{R}$, $f \\in \\mathbb{R}^n$, and $u^{\\circ 3}$ is the element-wise cubic power.\n  - Jacobian matrix: $J(u) = K + \\operatorname{diag}\\!\\left(3\\alpha\\, u^{\\circ 2}\\right)$, where $u^{\\circ 2}$ is the element-wise square.\n- **Reduced-Order Model**:\n  - State approximation: $u \\approx \\Phi a$, with a reduced basis $\\Phi \\in \\mathbb{R}^{n \\times r}$ and reduced coordinates $a \\in \\mathbb{R}^r$.\n- **Hyperreduction for Newton Step**:\n  - The linear system for the update $\\Delta a$ is $A_{\\mathrm{HR}}(u) \\,\\Delta a = b_{\\mathrm{HR}}(u)$.\n  - Hyperreduced matrix: $A_{\\mathrm{HR}}(u) = \\Phi_S^\\top W \\left(J(u)\\Phi\\right)_S$.\n  - Hyperreduced right-hand side: $b_{\\mathrm{HR}}(u) = -\\Phi_S^\\top W \\, r(u)_S$.\n  - Here, $S$ is a set of sample row indices, $W = \\operatorname{diag}(w)$ is a diagonal matrix of weights $w$, and $X_S$ denotes row-sampling of a matrix/vector $X$.\n- **Update and Evaluation**:\n  - Update rule: $a^{+} = a + \\Delta a$, followed by $u^{+} = \\Phi a^{+}$.\n  - Performance metric: The ratio of Euclidean norms $\\|r(u^{+})\\|_2 / \\|r(u)\\|_2$ and a boolean flag for strict decrease, $\\|r(u^{+})\\|_2 < \\|r(u)\\|_2$. If $\\|r(u)\\|_2 = 0$, the ratio is defined as Not-a-Number (NaN).\n- **Fixed Parameters**:\n  - System dimension: $n=5$.\n  - Reduced dimension: $r=2$.\n  - Stiffness matrix: $K = \\operatorname{tridiag}(-1, 2, -1) \\in \\mathbb{R}^{5 \\times 5}$.\n  - Reduced basis: $\\Phi = [\\phi_1, \\phi_2]$, with $\\phi_1 = \\frac{1}{\\sqrt{5}}[1, 1, 1, 1, 1]^\\top$ and $\\phi_2 = \\frac{1}{\\sqrt{3}}[1, 0, -1, 0, 1]^\\top$.\n- **Test Cases**: Three cases are provided, each with a specific set of $(\\alpha, f, a, S, w)$.\n  - **Case 1**: $\\alpha = 0.5$, $f = [1, 0, 0.5, 0, -1]^\\top$, $a = [0.1, -0.2]^\\top$, $S = \\{0, 2, 4\\}$, $w = [1.6, 1.2, 1.2]^\\top$.\n  - **Case 2**: $\\alpha = 0.0$, $f = [0, 0, 0, 0, 0.5]^\\top$, $a = [0.0, 0.0]^\\top$, $S = \\{0, 1, 2, 3, 4\\}$, $w = [1, 1, 1, 1, 1]^\\top$.\n  - **Case 3**: $\\alpha = 1.5$, $f = [-0.5, 0.2, 0, 0.1, -0.1]^\\top$, $a = [-0.3, 0.15]^\\top$, $S = \\{3, 4\\}$, $w = [2.5, 2.5]^\\top$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is validated against the required criteria:\n- **Scientifically Grounded**: The problem is situated in the established field of computational solid mechanics, specifically addressing reduced-order modeling and hyperreduction. The governing equations represent a standard discrete nonlinear mechanical system, and the hyperreduced Newton method is a well-documented technique (related to GNAT or ECSW methods). All concepts are scientifically sound.\n- **Well-Posed**: For each test case, all necessary inputs are provided. The task is a direct computation of a single Newton step. The existence of a solution for $\\Delta a$ depends on the invertibility of the $2 \\times 2$ matrix $A_{\\mathrm{HR}}(u)$, which is part of the calculation itself and not a flaw in the problem statement. The problem is well-posed.\n- **Objective**: The problem is formulated with precise mathematical definitions and numerical data. It is free of ambiguity, subjectivity, or opinion.\n- **Completeness and Consistency**: The problem is self-contained. All dimensions of matrices and vectors are consistent. For example, for $A_{\\mathrm{HR}} \\in \\mathbb{R}^{2 \\times 2}$, the product is $\\Phi_S^\\top W (J\\Phi)_S$, with dimensions $(2 \\times |S|) \\times (|S| \\times |S|) \\times (|S| \\times 2)$, which correctly results in a $2 \\times 2$ matrix. All data required for the computation are explicitly given.\n- **No Other Flaws**: The problem is not unrealistic, ill-posed, trivial, or unverifiable. It is a concrete computational exercise.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be provided by executing the specified computational procedure for each test case.\n\n### Solution Procedure\n\nThe solution involves a systematic application of the given formulas for each test case. The steps are as follows:\n\n1.  **Initialization**: For a given test case $(\\alpha, f, a, S, w)$, define the constant matrices $K$ and $\\Phi$.\n2.  **Compute Initial State**: Calculate the full-order state vector $u = \\Phi a$ and the corresponding residual vector $r(u) = K u + \\alpha u^{\\circ 3} - f$. Compute its Euclidean norm, $N_{\\text{before}} = \\|r(u)\\|_2$.\n3.  **Assemble Hyperreduced System**:\n    a. Form the Jacobian matrix $J(u) = K + \\operatorname{diag}(3\\alpha u^{\\circ 2})$.\n    b. Extract the sampled rows of the basis, $\\Phi_S$, and the sampled rows of the Jacobian-basis product, $(J(u)\\Phi)_S$.\n    c. Extract the sampled entries of the residual, $r(u)_S$.\n    d. Form the diagonal weight matrix $W = \\operatorname{diag}(w)$.\n    e. Assemble the hyperreduced matrix $A_{\\mathrm{HR}}(u) = \\Phi_S^\\top W (J(u)\\Phi)_S$ and the hyperreduced right-hand side vector $b_{\\mathrm{HR}}(u) = -\\Phi_S^\\top W r(u)_S$.\n4.  **Solve for Update**: Solve the $r \\times r$ linear system $A_{\\mathrm{HR}}(u) \\Delta a = b_{\\mathrm{HR}}(u)$ for the reduced coordinate update $\\Delta a$.\n5.  **Compute Updated State**:\n    a. Update the reduced coordinates: $a^{+} = a + \\Delta a$.\n    b. Compute the new full-order state vector: $u^{+} = \\Phi a^{+}$.\n6.  **Evaluate Performance**:\n    a. Compute the new residual vector $r(u^{+}) = K u^{+} + \\alpha (u^{+})^{\\circ 3} - f$ and its Euclidean norm, $N_{\\text{after}} = \\|r(u^{+})\\|_2$.\n    b. Determine if the residual norm has strictly decreased: $\\textit{improved} = (N_{\\text{after}} < N_{\\text{before}})$.\n    c. Calculate the ratio. If $N_{\\text{before}} = 0$, the ratio is NaN. Otherwise, $\\textit{ratio} = N_{\\text{after}} / N_{\\text{before}}$.\n7.  **Store Results**: The results for the test case are compiled into the list $[N_{\\text{before}}, N_{\\text{after}}, \\textit{ratio}, \\textit{improved}]$.\n\nThis procedure is repeated for all three test cases to generate the final output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes one hyperreduced Newton step for a nonlinear system for three test cases.\n    \"\"\"\n    \n    # Define fixed problem data\n    n = 5\n    r = 2\n    \n    # Stiffness matrix K\n    K = np.diag(np.full(n, 2.0)) + np.diag(np.full(n - 1, -1.0), k=1) + np.diag(np.full(n - 1, -1.0), k=-1)\n    \n    # Reduced basis Phi\n    phi1 = np.ones(n) / np.sqrt(n)\n    phi2 = np.array([1.0, 0.0, -1.0, 0.0, 1.0]) / np.sqrt(3.0)\n    Phi = np.vstack((phi1, phi2)).T\n\n    # Define the three test cases\n    test_cases = [\n        # Case 1: Moderate nonlinearity, well-chosen sampling\n        {\n            \"alpha\": 0.5,\n            \"f\": np.array([1.0, 0.0, 0.5, 0.0, -1.0]),\n            \"a\": np.array([0.1, -0.2]),\n            \"S\": np.array([0, 2, 4]),\n            \"w\": np.array([1.6, 1.2, 1.2])\n        },\n        # Case 2: Linear case, full sampling\n        {\n            \"alpha\": 0.0,\n            \"f\": np.array([0.0, 0.0, 0.0, 0.0, 0.5]),\n            \"a\": np.array([0.0, 0.0]),\n            \"S\": np.array([0, 1, 2, 3, 4]),\n            \"w\": np.array([1.0, 1.0, 1.0, 1.0, 1.0])\n        },\n        # Case 3: Strong nonlinearity, poor sampling\n        {\n            \"alpha\": 1.5,\n            \"f\": np.array([-0.5, 0.2, 0.0, 0.1, -0.1]),\n            \"a\": np.array([-0.3, 0.15]),\n            \"S\": np.array([3, 4]),\n            \"w\": np.array([2.5, 2.5])\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        alpha = case[\"alpha\"]\n        f = case[\"f\"]\n        a = case[\"a\"]\n        S = case[\"S\"]\n        w = case[\"w\"]\n\n        # 1. Compute initial state and residual norm\n        u = Phi @ a\n        r_u = K @ u + alpha * (u**3) - f\n        norm_before = np.linalg.norm(r_u)\n\n        # 2. Assemble hyperreduced Jacobian system\n        # Jacobian J(u)\n        J_u = K + np.diag(3 * alpha * (u**2))\n        \n        # Sampled components\n        Phi_S = Phi[S, :]\n        J_Phi = J_u @ Phi\n        J_Phi_S = J_Phi[S, :]\n        r_u_S = r_u[S]\n        \n        # Weight matrix W\n        W = np.diag(w)\n        \n        # Hyperreduced system matrix A_HR and right-hand side b_HR\n        A_HR = Phi_S.T @ W @ J_Phi_S\n        b_HR = -Phi_S.T @ W @ r_u_S\n        \n        # 3. Solve for the update in reduced coordinates\n        delta_a = np.linalg.solve(A_HR, b_HR)\n        \n        # 4. Update the state\n        a_plus = a + delta_a\n        u_plus = Phi @ a_plus\n        \n        # 5. Compute the new residual norm\n        r_u_plus = K @ u_plus + alpha * (u_plus**3) - f\n        norm_after = np.linalg.norm(r_u_plus)\n        \n        # 6. Calculate performance metrics\n        if norm_before == 0.0:\n            ratio = np.nan\n        else:\n            ratio = norm_after / norm_before\n        \n        improved = bool(norm_after < norm_before)\n        \n        # Store results for this case\n        all_results.append([norm_before, norm_after, ratio, improved])\n\n    # Format the final output string exactly as required\n    # str() on a list containing booleans and nan will produce the correct text representation\n    output_str = f\"[{','.join(str(item) for item in all_results)}]\"\n    print(output_str.replace(\"'\", \"\"))\n\nsolve()\n```"
        }
    ]
}