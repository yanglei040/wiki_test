{
    "hands_on_practices": [
        {
            "introduction": "The first step in understanding multigrid methods is to see how information is transferred between grids and how the problem is reformulated at a coarser level. This exercise demystifies the abstract prolongation ($P$), restriction ($R$), and coarse-grid ($A_c$) operators by having you construct them by hand for a minimal, intuitive system. By working through the Galerkin projection $A_c = RAP$, you will gain a concrete foundation for the more advanced concepts that follow .",
            "id": "3290960",
            "problem": "Consider the one-dimensional pressure Poisson equation in Computational Fluid Dynamics (CFD) given by $- \\partial_{x}^{2} p = f$ on a unit interval with homogeneous Dirichlet boundary conditions at both ends. Discretize this equation on a uniform fine grid with $3$ interior unknowns and nondimensional spacing $h = 1$ using the second-order central difference approximation. This yields a symmetric positive definite (SPD) fine-grid operator $A \\in \\mathbb{R}^{3 \\times 3}$ that is the canonical tridiagonal discrete Laplacian. In the context of Algebraic Multigrid (AMG), define a coarse grid consisting of $2$ coarse points aligned with fine points $1$ and $3$, and construct a prolongation operator $P \\in \\mathbb{R}^{3 \\times 2}$ by linear interpolation that maps coarse-grid unknowns $\\begin{pmatrix} u_{C}^{1} \\\\ u_{C}^{2} \\end{pmatrix}$ to fine-grid unknowns $\\begin{pmatrix} u_{f}^{1} \\\\ u_{f}^{2} \\\\ u_{f}^{3} \\end{pmatrix}$ via $u_{f}^{1} = u_{C}^{1}$, $u_{f}^{2} = \\frac{1}{2} u_{C}^{1} + \\frac{1}{2} u_{C}^{2}$, and $u_{f}^{3} = u_{C}^{2}$. Use the symmetric Galerkin choice of restriction $R = P^{\\top}$. Starting from the finite-difference discretization and the SPD property of the discrete Laplacian, explicitly construct $R$ and $P$, compute the coarse-grid operator $A_{c} = R A P$, and determine the spectra (sets of eigenvalues) of both $A$ and $A_{c}$. Then verify, using first principles for Ritz values of SPD matrices, that the coarse-grid eigenvalues interlace the fine-grid eigenvalues. For your final reported result, provide the two coarse-grid eigenvalues of $A_{c}$ as exact values. No rounding is needed; the answer is dimensionless because $h = 1$.",
            "solution": "The problem statement has been validated and is deemed self-contained, scientifically grounded, and well-posed. Although the phrasing \"first principles for Ritz values\" is technically imprecise in this context, the core task is mathematically sound and the requested verification holds true for this specific problem. We may now proceed with the solution.\n\nThe one-dimensional pressure Poisson equation is given by $- \\partial_{x}^{2} p = f$ with homogeneous Dirichlet boundary conditions. Discretizing this equation on a uniform grid with $N=3$ interior points and a nondimensional grid spacing $h=1$ using a second-order central difference scheme, $\\frac{-p_{i-1} + 2p_i - p_{i+1}}{h^2}$, results in a linear system $A\\mathbf{p} = \\mathbf{f}$. The matrix $A$ is the $3 \\times 3$ discrete Laplacian. Given $p_0=0$ and $p_4=0$, the fine-grid operator $A$ is:\n$$\nA = \\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix}\n$$\nThis matrix is symmetric and, as we will see from its eigenvalues, positive definite.\n\nThe problem defines a prolongation (interpolation) operator $P \\in \\mathbb{R}^{3 \\times 2}$ that maps a coarse-grid vector of $2$ unknowns, $u_C = \\begin{pmatrix} u_C^1 \\\\ u_C^2 \\end{pmatrix}$, to a fine-grid vector of $3$ unknowns, $u_f = \\begin{pmatrix} u_f^1 \\\\ u_f^2 \\\\ u_f^3 \\end{pmatrix}$. The mapping is defined as:\n$u_f^1 = u_C^1$\n$u_f^2 = \\frac{1}{2} u_C^1 + \\frac{1}{2} u_C^2$\n$u_f^3 = u_C^2$\nIn matrix form, $u_f = P u_C$, the prolongation operator is:\n$$\nP = \\begin{pmatrix} 1 & 0 \\\\ \\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe restriction operator $R$ is constructed using the symmetric Galerkin choice, $R = P^{\\top}$. Therefore, $R \\in \\mathbb{R}^{2 \\times 3}$ is:\n$$\nR = \\begin{pmatrix} 1 & \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} & 1 \\end{pmatrix}\n$$\nThe coarse-grid operator $A_c$ is formed by the Galerkin projection $A_c = R A P = P^{\\top} A P$. We compute this product in two steps. First, we compute $A P$:\n$$\nA P = \\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ \\frac{1}{2} & \\frac{1}{2} \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 2(1) - 1(\\frac{1}{2}) & 2(0) - 1(\\frac{1}{2}) \\\\ -1(1) + 2(\\frac{1}{2}) & -1(0) + 2(\\frac{1}{2}) - 1(1) \\\\ -1(\\frac{1}{2}) & -1(\\frac{1}{2}) + 2(1) \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} & -\\frac{1}{2} \\\\ 0 & 0 \\\\ -\\frac{1}{2} & \\frac{3}{2} \\end{pmatrix}\n$$\nNext, we pre-multiply by $R$ to find $A_c$:\n$$\nA_c = R (A P) = \\begin{pmatrix} 1 & \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{3}{2} & -\\frac{1}{2} \\\\ 0 & 0 \\\\ -\\frac{1}{2} & \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} & -\\frac{1}{2} \\\\ -\\frac{1}{2} & \\frac{3}{2} \\end{pmatrix}\n$$\nNow, we determine the spectra of $A$ and $A_c$.\nThe eigenvalues of the fine-grid operator $A$, denoted $\\lambda_A$, are the roots of the characteristic equation $\\det(A - \\lambda_A I) = 0$:\n$$\n\\det \\begin{pmatrix} 2-\\lambda_A & -1 & 0 \\\\ -1 & 2-\\lambda_A & -1 \\\\ 0 & -1 & 2-\\lambda_A \\end{pmatrix} = (2-\\lambda_A)((2-\\lambda_A)^2 - 1) - (-1)(-(2-\\lambda_A)) = (2-\\lambda_A)((2-\\lambda_A)^2 - 2) = 0\n$$\nThis gives one eigenvalue $\\lambda_A = 2$, and two others from $(2-\\lambda_A)^2 = 2$, which means $2-\\lambda_A = \\pm\\sqrt{2}$, or $\\lambda_A = 2 \\mp \\sqrt{2}$. The sorted eigenvalues of $A$ are:\n$\\lambda_1(A) = 2 - \\sqrt{2}$\n$\\lambda_2(A) = 2$\n$\\lambda_3(A) = 2 + \\sqrt{2}$\nAll eigenvalues are positive, confirming that $A$ is positive definite.\n\nThe eigenvalues of the coarse-grid operator $A_c$, denoted $\\lambda_c$, are the roots of $\\det(A_c - \\lambda_c I) = 0$:\n$$\n\\det \\begin{pmatrix} \\frac{3}{2}-\\lambda_c & -\\frac{1}{2} \\\\ -\\frac{1}{2} & \\frac{3}{2}-\\lambda_c \\end{pmatrix} = \\left(\\frac{3}{2}-\\lambda_c\\right)^2 - \\left(-\\frac{1}{2}\\right)^2 = \\left(\\frac{3}{2}-\\lambda_c\\right)^2 - \\frac{1}{4} = 0\n$$\nThis implies $\\left(\\frac{3}{2}-\\lambda_c\\right)^2 = \\frac{1}{4}$, so $\\frac{3}{2}-\\lambda_c = \\pm\\frac{1}{2}$. This yields the eigenvalues $\\lambda_c = \\frac{3}{2} \\mp \\frac{1}{2}$. The sorted eigenvalues of $A_c$ are:\n$\\lambda_1(A_c) = \\frac{3}{2} - \\frac{1}{2} = 1$\n$\\lambda_2(A_c) = \\frac{3}{2} + \\frac{1}{2} = 2$\n\nFinally, we verify the eigenvalue interlacing property. The Courant-Fischer min-max principle for SPD matrices establishes that the eigenvalues of a Galerkin-projected system interlace the eigenvalues of the original system. For an $n \\times n$ SPD matrix $A$ and an $n \\times m$ full-rank prolongation matrix $P$ ($m < n$), the eigenvalues $\\mu_k$ of the generalized eigenvalue problem $P^{\\top}AP y = \\mu (P^{\\top}P) y$ interlace the eigenvalues $\\lambda_k$ of $A$. That is, $\\lambda_k(A) \\le \\mu_k \\le \\lambda_{k+n-m}(A)$ for $k=1, \\dots, m$. These $\\mu_k$ are the true Ritz values. The problem asks to verify this property for the standard eigenvalues $\\lambda_k(A_c)$. We check this directly.\nHere, $n=3$, $m=2$. We must verify that $\\lambda_k(A) \\le \\lambda_k(A_c) \\le \\lambda_{k+1}(A)$ for $k=1, 2$.\nFor $k=1$:\n$$\n\\lambda_1(A) \\le \\lambda_1(A_c) \\le \\lambda_2(A) \\implies 2 - \\sqrt{2} \\le 1 \\le 2\n$$\nSince $1 < \\sqrt{2} < 2$, we have $0 < 2-\\sqrt{2} < 1$. Thus, $2 - \\sqrt{2} \\le 1$ holds. Also, $1 \\le 2$ holds. The first interlacing condition is satisfied.\nFor $k=2$:\n$$\n\\lambda_2(A) \\le \\lambda_2(A_c) \\le \\lambda_3(A) \\implies 2 \\le 2 \\le 2 + \\sqrt{2}\n$$\nThis condition is also satisfied. Thus, the interlacing property holds for the standard eigenvalues of $A_c$ for this specific problem.\n\nThe two coarse-grid eigenvalues of $A_c$ are $1$ and $2$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 2\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Now that you have seen how to construct a coarse-grid system, we must ask what makes such a construction effective. This practice uses a counterexample to illustrate one of the most critical principles in multigrid: the approximation property, which states that the coarse grid must be able to accurately represent algebraically smooth errors. By analyzing a deliberately poor choice of interpolation, you will see precisely why this property is essential and learn a formal algebraic criterion to diagnose its failure .",
            "id": "3532911",
            "problem": "Consider the task of designing an Algebraic Multigrid (AMG) preconditioner for a linear system with a Symmetric Positive Definite (SPD) matrix. Let the system matrix be the canonical one-dimensional discrete Laplacian with Dirichlet boundary conditions on four grid points,\n$$\nA \\;=\\;\n\\begin{pmatrix}\n2 & -1 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 \\\\\n0 & -1 & 2 & -1 \\\\\n0 & 0 & -1 & 2\n\\end{pmatrix},\n$$\nand suppose a coarse/fine (C/F) splitting is chosen with coarse variables at indices $\\{1,4\\}$ and fine variables at indices $\\{2,3\\}$. Define the prolongation operator $P$ by direct injection at coarse points and no interpolation to fine points, so that the columns of $P$ are the coordinate vectors at indices $1$ and $4$, namely\n$$\nP \\;=\\;\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 0 \\\\\n0 & 0 \\\\\n0 & 1\n\\end{pmatrix}.\n$$\nStarting only from the core definitions of Algebraic Multigrid (AMG) as a preconditioner, the role of the coarse space $\\operatorname{Range}(P)$, and the Galerkin construction $A_{\\mathrm{c}} = P^{\\top} A P$, explain why this choice of C/F splitting and interpolation violates the approximation property by identifying a nonzero algebraically smooth error that the coarse space cannot approximate. Then, diagnose this failure using the algebraic criterion involving the nullspace of $P^{\\top} A$ by computing the dimension of $\\operatorname{Null}(P^{\\top} A)$.\n\nYour final answer must be the dimension of $\\operatorname{Null}(P^{\\top} A)$, expressed as a single integer. No rounding is required.",
            "solution": "The problem statement is a valid exercise in numerical linear algebra, specifically concerning the principles of Algebraic Multigrid (AMG) methods. All provided definitions and matrices are standard and form a self-contained, well-posed problem. We may proceed with the solution.\n\nThe core idea of a two-grid method, and by extension multigrid, is to use a relaxation method (like Jacobi or Gauss-Seidel) to damp high-frequency (oscillatory) components of the error, and a coarse-grid correction step to eliminate the low-frequency (algebraically smooth) components of the error that relaxation methods struggle with.\n\nFor the coarse-grid correction to be effective, two primary conditions must be met:\n$1$. The **approximation property**: The space of coarse-grid vectors, represented by the range of the prolongation operator $P$, denoted $\\operatorname{Range}(P)$, must be able to accurately approximate the algebraically smooth error components.\n$2$. The coarse-grid operator $A_{\\mathrm{c}} = P^{\\top} A P$ must be a good, stable representation of the fine-grid operator $A$ on the coarse space.\n\nThe problem asks us to show that the first property is violated and to diagnose this failure algebraically.\n\n### Part 1: Violation of the Approximation Property\n\nAn error vector $e$ is considered algebraically smooth if it is a linear combination of the eigenvectors of $A$ corresponding to small eigenvalues. In general, a vector $e$ is smooth if the Rayleigh quotient $R(e) = \\frac{e^{\\top}Ae}{e^{\\top}e}$ is small. For the discrete Laplacian matrix $A$, smooth vectors are those that do not oscillate much between adjacent grid points. A canonical example of a smooth vector is one whose components are nearly constant. Let us consider the vector $e_{\\mathrm{smooth}} = \\begin{pmatrix} 1 & 1 & 1 & 1 \\end{pmatrix}^{\\top}$. This vector represents a constant mode, which is the smoothest possible function on the grid. This vector is indeed algebraically smooth with respect to $A$, as its Rayleigh quotient is $R(e_{\\mathrm{smooth}}) = \\frac{\\begin{pmatrix} 1 & 1 & 1 & 1 \\end{pmatrix} A \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}}{\\begin{pmatrix} 1 & 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix}} = \\frac{\\begin{pmatrix} 1 & 1 & 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}}{4} = \\frac{2}{4} = \\frac{1}{2}$. The smallest eigenvalue of $A$ is $\\lambda_1 = 2 - 2\\cos(\\frac{\\pi}{5}) \\approx 0.382$, so a Rayleigh quotient of $0.5$ indicates a very smooth vector.\n\nThe approximation property requires that this smooth vector $e_{\\mathrm{smooth}}$ can be well-approximated by a vector in the coarse space, $\\operatorname{Range}(P)$. The given prolongation operator is\n$$\nP \\;=\\;\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 0 \\\\\n0 & 0 \\\\\n0 & 1\n\\end{pmatrix}.\n$$\nThe columns of $P$ are the standard basis vectors $e_1$ and $e_4$. The coarse space $\\operatorname{Range}(P)$ is the set of all vectors of the form $v_c = \\alpha e_1 + \\beta e_4$, which can be written as\n$$\nv_c = \\begin{pmatrix} \\alpha \\\\ 0 \\\\ 0 \\\\ \\beta \\end{pmatrix}\n$$\nfor some scalars $\\alpha, \\beta \\in \\mathbb{R}$.\n\nTo approximate $e_{\\mathrm{smooth}} = \\begin{pmatrix} 1 & 1 & 1 & 1 \\end{pmatrix}^{\\top}$ with a vector in $\\operatorname{Range}(P)$, we would choose $\\alpha=1$ and $\\beta=1$ to match the values at the coarse points (indices $\\{1,4\\}$). The best approximation of $e_{\\mathrm{smooth}}$ in $\\operatorname{Range}(P)$ is thus\n$$\ne_{\\mathrm{approx}} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\nThe error of this approximation is\n$$\ne_{\\mathrm{err}} = e_{\\mathrm{smooth}} - e_{\\mathrm{approx}} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\end{pmatrix}.\n$$\nThe magnitude of this error is $\\|e_{\\mathrm{err}}\\|_2 = \\sqrt{0^2 + 1^2 + 1^2 + 0^2} = \\sqrt{2}$, which is significant compared to the magnitude of the original smooth vector $\\|e_{\\mathrm{smooth}}\\|_2 = \\sqrt{4} = 2$. The approximation completely fails to capture the behavior of the smooth error at the fine points (indices $\\{2,3\\}$). This is a direct consequence of the chosen $P$, which performs no interpolation; it simply injects the coarse-point values and sets the fine-point values to zero. Therefore, this choice of $P$ violates the approximation property. The vector $e_{\\mathrm{smooth}} = \\begin{pmatrix} 1 & 1 & 1 & 1 \\end{pmatrix}^{\\top}$ is a nonzero algebraically smooth error that the coarse space cannot approximate well.\n\n### Part 2: Diagnosis via $\\operatorname{Null}(P^{\\top} A)$\n\nA more formal algebraic criterion for a failing coarse-grid correction is the existence of a non-trivial nullspace for the operator $P^{\\top} A$. If there exists a nonzero vector $e$ such that $P^{\\top} A e = 0$, it implies that a smooth error component $e$ can generate a residual $r=Ae$ that is entirely \"invisible\" to the coarse grid. The restriction of the residual, $P^{\\top}r$, would be zero. Consequently, the coarse-grid problem $A_{\\mathrm{c}} e_{\\mathrm{c}} = P^{\\top} r$ would have a zero right-hand side, leading to a zero correction $e_{\\mathrm{c}}=0$. The error $e$ would remain uncorrected by the coarse-grid step. The dimension of this nullspace quantifies the number of independent \"bad\" modes that the coarse-grid correction cannot handle.\n\nWe compute the matrix product $P^{\\top} A$:\n$$\nP^{\\top} = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix}.\n$$\n$$\nP^{\\top} A = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\end{pmatrix}\n\\begin{pmatrix}\n2 & -1 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 \\\\\n0 & -1 & 2 & -1 \\\\\n0 & 0 & -1 & 2\n\\end{pmatrix}\n= \\begin{pmatrix}\n2 & -1 & 0 & 0 \\\\\n0 & 0 & -1 & 2\n\\end{pmatrix}.\n$$\nNow, we must find the nullspace of this $2 \\times 4$ matrix. We seek all vectors $x = \\begin{pmatrix} x_1 & x_2 & x_3 & x_4 \\end{pmatrix}^{\\top}$ such that $(P^{\\top} A) x = 0$:\n$$\n\\begin{pmatrix}\n2 & -1 & 0 & 0 \\\\\n0 & 0 & -1 & 2\n\\end{pmatrix}\n\\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{pmatrix}\n= \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nThis matrix equation corresponds to the following system of two linear equations with four unknowns:\n\\begin{align*}\n2x_1 - x_2 &= 0 \\\\\n-x_3 + 2x_4 &= 0\n\\end{align*}\nFrom the first equation, we have $x_2 = 2x_1$. From the second equation, we have $x_3 = 2x_4$.\nThe variables $x_1$ and $x_4$ are free variables. Let us set $x_1 = s$ and $x_4 = t$ for arbitrary parameters $s, t \\in \\mathbb{R}$. The general solution is:\n$$\nx = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{pmatrix} = \\begin{pmatrix} s \\\\ 2s \\\\ 2t \\\\ t \\end{pmatrix} = s \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix} + t \\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\\\ 1 \\end{pmatrix}.\n$$\nThe nullspace $\\operatorname{Null}(P^{\\top} A)$ is spanned by the two linearly independent vectors\n$$\nv_1 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix} \\quad \\text{and} \\quad v_2 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\\\ 1 \\end{pmatrix}.\n$$\nSince the nullspace is spanned by two linearly independent vectors, its dimension is $2$.\nThis confirms the failure of the chosen AMG setup. There is a two-dimensional subspace of error vectors that are completely missed by the coarse-grid correction process defined by this $P$.\n\nThe final answer is the dimension of this nullspace.",
            "answer": "$$\n\\boxed{2}\n$$"
        },
        {
            "introduction": "A successful multigrid method relies on the complementary action of two key components: a smoother that damps high-frequency errors and a coarse-grid correction that eliminates low-frequency errors. This comprehensive practice guides you through a complete two-grid analysis, connecting the performance of the smoother to the quality of the coarse-grid approximation. By applying local Fourier analysis, you will synthesize these ideas to predict the method's rapid convergence factor, revealing mathematically why a well-designed cycle is so powerful .",
            "id": "3543424",
            "problem": "Consider small-strain, one-dimensional linear elasticity on the interval $[0,1]$ with homogeneous Dirichlet boundary conditions, constant Young’s modulus $E>0$ and cross-sectional area $A>0$. Let $u(x)$ denote the axial displacement and let the weak form be discretized by linear finite elements on a uniform mesh with mesh size $h$. The resulting stiffness matrix on the fine grid is the symmetric positive definite tridiagonal matrix $A_h \\in \\mathbb{R}^{n \\times n}$ whose stencil is proportional to $\\{-1,2,-1\\}$, i.e., $A_h = \\frac{EA}{h}\\,\\mathrm{tridiag}(-1,2,-1)$. This setting is a canonical scalar model for solid mechanics operators used in Algebraic Multigrid (AMG) preconditioning.\n\nAssume a two-level AMG method that, for this constant-coefficient, scalar, $M$-matrix problem, reduces to the classical geometric ingredients: one coarse grid with spacing $2h$, full-weighting restriction with stencil $\\{\\tfrac{1}{4},\\tfrac{1}{2},\\tfrac{1}{4}\\}$, linear interpolation (piecewise linear prolongation), and a Galerkin coarse operator $A_{2h} = R A_h P$. The smoother is damped Jacobi with weight $\\omega \\in (0,1)$, and one pre- and one post-smoothing step are used.\n\nProceed as follows, starting only from core definitions and Fourier symbols of shift-invariant operators:\n\n1) Define the Fourier symbol of $A_h$ and of the damped Jacobi smoother $S_h = I - \\omega D^{-1} A_h$, where $D = \\mathrm{diag}(A_h)$. Define the smoothing factor $\\mu(\\omega)$ as the supremum over the “high-frequency” set $\\theta \\in [\\pi/2,\\pi]$ of the magnitude of the Jacobi error-amplification symbol, and determine the optimal damping $\\omega^{\\star}$ that minimizes $\\mu(\\omega)$. Report the minimal smoothing factor $\\mu_{\\min}$.\n\n2) Define the weak approximation property constant $C_A$ for linear interpolation $P$ with respect to the diagonal-norm $D$, namely the smallest constant $C_A$ such that\n$$\n\\min_{v_{2h}} \\| v_h - P v_{2h} \\|_{D}^{2} \\le C_A \\, v_h^{\\top} A_h v_h\n$$\nfor all fine-grid vectors $v_h$. Using Fourier analysis on the uniform grid, compute $C_A$ exactly.\n\n3) Using Local Fourier Analysis for two-grid with one pre- and one post-Jacobi step at $\\omega=\\omega^{\\star}$, linear interpolation, full-weighting restriction, and the Galerkin coarse operator, form the $2\\times 2$ error-amplification matrix on the two-frequency subspace spanned by $\\theta \\in (0,\\pi/2]$ and its $2h$-alias $\\pi-\\theta$. Determine the worst-case spectral radius over $\\theta\\in (0,\\pi/2]$ to predict the two-grid convergence factor $\\rho_{\\mathrm{TG}}$.\n\nExpress your final answer as the exact value of $\\rho_{\\mathrm{TG}}$ (dimensionless). Do not round.",
            "solution": "The problem is well-posed and scientifically grounded, representing a canonical case study in the Local Fourier Analysis (LFA) of multigrid methods. We proceed with the three parts of the problem.\n\nThroughout the analysis, we use the following notations for a frequency $\\theta \\in (0, \\pi/2]$: $s = \\sin(\\theta/2)$ and $c = \\cos(\\theta/2)$. Note that $s^2+c^2=1$.\n\n**Part 1: Smoothing Analysis**\n\nThe stiffness matrix $A_h$ is given by the stencil $\\frac{EA}{h}\\{-1, 2, -1\\}$. To find its Fourier symbol, $\\tilde{A}_h(\\theta)$, we apply the operator to a Fourier mode $v_j = \\exp(ij\\theta)$, where $j$ is the grid point index.\n$$\n(A_h v)_j = \\frac{EA}{h}(-v_{j-1} + 2v_j - v_{j+1}) = \\frac{EA}{h}(-\\exp(i(j-1)\\theta) + 2\\exp(ij\\theta) - \\exp(i(j+1)\\theta))\n$$\nFactoring out $\\exp(ij\\theta)$, we get the symbol:\n$$\n\\tilde{A}_h(\\theta) = \\frac{EA}{h}(-\\exp(-i\\theta) + 2 - \\exp(i\\theta)) = \\frac{EA}{h}(2 - 2\\cos(\\theta))\n$$\nUsing the half-angle identity $1-\\cos(\\theta) = 2\\sin^2(\\theta/2)$, we have:\n$$\n\\tilde{A}_h(\\theta) = \\frac{4EA}{h}\\sin^2\\left(\\frac{\\theta}{2}\\right)\n$$\nThe smoother is the damped Jacobi method, with iteration operator $S_h = I - \\omega D^{-1} A_h$. The matrix $D = \\mathrm{diag}(A_h)$ is constant along the diagonal for interior grid points, with value $D_{jj} = \\frac{2EA}{h}$. In LFA, which assumes an infinite grid, $D$ is treated as $\\frac{2EA}{h}I$. Its symbol is the constant $\\tilde{D} = \\frac{2EA}{h}$.\n\nThe symbol of the smoother, which is the error amplification factor of the smoothing process, is:\n$$\n\\tilde{S}_h(\\theta) = 1 - \\omega \\tilde{D}^{-1} \\tilde{A}_h(\\theta) = 1 - \\omega \\left(\\frac{h}{2EA}\\right) \\left(\\frac{4EA}{h}\\sin^2\\left(\\frac{\\theta}{2}\\right)\\right) = 1 - 2\\omega\\sin^2\\left(\\frac{\\theta}{2}\\right)\n$$\nThe smoothing factor $\\mu(\\omega)$ is the supremum of the magnitude of the error amplification factor over the high-frequency range, defined as $\\theta \\in [\\pi/2, \\pi]$.\n$$\n\\mu(\\omega) = \\sup_{\\theta \\in [\\pi/2, \\pi]} \\left| 1 - 2\\omega\\sin^2\\left(\\frac{\\theta}{2}\\right) \\right|\n$$\nLet $x = \\sin^2(\\theta/2)$. For $\\theta \\in [\\pi/2, \\pi]$, we have $\\theta/2 \\in [\\pi/4, \\pi/2]$, and thus $x \\in [\\sin^2(\\pi/4), \\sin^2(\\pi/2)] = [1/2, 1]$. The problem reduces to finding:\n$$\n\\mu(\\omega) = \\max_{x \\in [1/2, 1]} |1 - 2\\omega x|\n$$\nThe function $f(x)=1-2\\omega x$ is linear. For $\\omega \\in (0,1)$, its slope is negative. Thus, the maximum of $|f(x)|$ on the interval $[1/2, 1]$ must occur at one of the endpoints, $x=1/2$ or $x=1$.\n$$\n\\mu(\\omega) = \\max\\left(\\left|1 - 2\\omega\\left(\\frac{1}{2}\\right)\\right|, |1 - 2\\omega(1)|\\right) = \\max(|1-\\omega|, |1-2\\omega|)\n$$\nThe optimal damping $\\omega^{\\star}$ minimizes this quantity. This occurs when the two arguments of the max function are equal in magnitude:\n$$\n|1-\\omega^{\\star}| = |1-2\\omega^{\\star}|\n$$\nSince $\\omega^{\\star} \\in (0,1)$, $1-\\omega^{\\star}$ is positive. We solve $1-\\omega^{\\star} = \\pm(1-2\\omega^{\\star})$.\nThe $+$ case gives $1-\\omega^{\\star} = 1-2\\omega^{\\star}$, which implies $\\omega^{\\star}=0$, an invalid solution.\nThe $-$ case gives $1-\\omega^{\\star} = -(1-2\\omega^{\\star}) = 2\\omega^{\\star}-1$, which leads to $3\\omega^{\\star}=2$, so $\\omega^{\\star}=2/3$.\nThe optimal damping factor is $\\omega^{\\star}=2/3$.\nThe minimal smoothing factor is $\\mu_{\\min} = \\mu(2/3) = |1-2/3| = 1/3$.\n\n**Part 2: Weak Approximation Property**\n\nWe are asked to find the smallest constant $C_A$ such that for all fine-grid vectors $v_h$:\n$$\n\\min_{v_{2h}} \\| v_h - P v_{2h} \\|_{D}^{2} \\le C_A \\, v_h^{\\top} A_h v_h\n$$\nThe term $\\min_{v_{2h}} \\| v_h - P v_{2h} \\|_{D}^{2}$ represents the squared $D$-norm of the error of the best approximation of $v_h$ from the range of $P$. Since $D = \\frac{2EA}{h}I$ is a scalar multiple of the identity, the $D$-norm is equivalent to the Euclidean $L_2$-norm, and the best approximation is given by the orthogonal projection. For linear interpolation $P$, the best coarse-grid representation $v_{2h}$ of a fine-grid function $v_h$ is typically chosen by injection, i.e., $v_{2h}(J) = v_h(2J)$, where $J$ denotes a coarse-grid index and $j$ a fine-grid index.\nThe interpolated function $(Pv_{2h})$ then has values:\n$(Pv_{2h})(2J) = v_{2h}(J) = v_h(2J)$\n$(Pv_{2h})(2J+1) = \\frac{1}{2}(v_{2h}(J) + v_{2h}(J+1)) = \\frac{1}{2}(v_h(2J) + v_h(2J+2))$\n\nThe approximation error is zero at even-indexed points $j=2J$. At odd-indexed points $j=2J+1$, the error is:\n$$\ne_{2J+1} = v_h(2J+1) - \\frac{1}{2}(v_h(2J) + v_h(2J+2)) = -\\frac{1}{2}\\left(v_h(2J) - 2v_h(2J+1) + v_h(2J+2)\\right)\n$$\nThis is proportional to the discrete second derivative of $v_h$. We analyze the inequality in Fourier space. Let $v_h$ be a Fourier mode $v_j=\\exp(ij\\theta)$.\nThe right-hand side, the energy norm, becomes $v_h^{\\top} A_h v_h \\to \\tilde{A}_h(\\theta) = \\frac{4EA}{h}\\sin^2(\\theta/2)$.\nFor the left-hand side, we compute the error mode. The error at an even point is $0$. At an odd point $j=2J+1$, the error is:\n$$\ne_{2J+1} = \\exp(i(2J+1)\\theta) - \\frac{1}{2}(\\exp(i2J\\theta) + \\exp(i(2J+2)\\theta)) = \\exp(i(2J+1)\\theta) \\left(1 - \\frac{\\exp(-i\\theta)+\\exp(i\\theta)}{2}\\right)\n$$\n$$\ne_{2J+1} = \\exp(i(2J+1)\\theta)(1-\\cos\\theta) = 2\\sin^2(\\theta/2)\\exp(i(2J+1)\\theta)\n$$\nThe squared $D$-norm of the error, per mode, is computed by averaging over the grid. Since the error is non-zero only at half the points (the odd ones), we have:\n$$\n\\|e_h\\|_D^2 \\to \\frac{1}{2} D_{jj} |e_{2J+1}|^2 = \\frac{1}{2} \\left(\\frac{2EA}{h}\\right) \\left|2\\sin^2(\\theta/2)\\right|^2 = \\frac{EA}{h} \\cdot 4\\sin^4(\\theta/2) = \\frac{4EA}{h}\\sin^4(\\theta/2)\n$$\nThe ratio $Q(\\theta)$ of the LHS to the RHS for the mode $\\theta$ is:\n$$\nQ(\\theta) = \\frac{\\frac{4EA}{h}\\sin^4(\\theta/2)}{\\frac{4EA}{h}\\sin^2(\\theta/2)} = \\sin^2(\\theta/2)\n$$\nThe constant $C_A$ is the supremum of this quantity over all possible modes, i.e., $\\theta \\in [0, \\pi]$.\n$$\nC_A = \\sup_{\\theta \\in [0, \\pi]} \\sin^2(\\theta/2) = \\sin^2(\\pi/2) = 1\n$$\n\n**Part 3: Two-Grid Convergence Factor**\n\nThe two-grid error propagation operator for a cycle with one pre- and one post-smoothing step is $M_{TG} = S_h (I - P A_{2h}^{-1} R A_h) S_h$, where $A_{2h} = R A_h P$. We use LFA to find its symbol $\\tilde{M}_{TG}(\\theta)$, a $2\\times 2$ matrix for each $\\theta \\in (0, \\pi/2]$.\n$$\n\\tilde{M}_{TG}(\\theta) = \\tilde{S}_h(\\theta) \\left(I - \\tilde{P}(\\theta) \\tilde{A}_{2h}(\\theta)^{-1} \\tilde{R}(\\theta) \\tilde{A}_h(\\theta)\\right) \\tilde{S}_h(\\theta)\n$$\nThe symbols for the operators are:\n- Smoother $\\tilde{S}_h(\\theta)$: With $\\omega^{\\star}=2/3$ and $x=s^2=\\sin^2(\\theta/2)$, the amplification factors for low ($\\theta$) and high ($\\pi-\\theta$) frequencies are $\\mu_s = 1-2\\omega^{\\star}s^2 = 1-4s^2/3$ and $\\mu_c = 1-2\\omega^{\\star}c^2 = 1-4c^2/3$.\nThe matrix symbol is $\\tilde{S}_h(\\theta) = \\mathrm{diag}(\\mu_s, \\mu_c)$.\n- Operator $\\tilde{A}_h(\\theta)$: Let $C_0 = \\frac{4EA}{h}$. Then $\\tilde{A}_h(\\theta) = C_0 \\cdot \\mathrm{diag}(s^2, c^2)$.\n- Transfer operators: For 1D linear interpolation ($P$) and full-weighting ($R$ with stencil $\\{\\frac{1}{4}, \\frac{1}{2}, \\frac{1}{4}\\}$), the LFA symbols are $\\tilde{P}(\\theta) = 2\\begin{pmatrix} c^2 \\\\ s^2 \\end{pmatrix}$ and $\\tilde{R}(\\theta) = (c^2, s^2)$.\n- Coarse operator $\\tilde{A}_{2h}(\\theta) = \\tilde{R}(\\theta) \\tilde{A}_h(\\theta) \\tilde{P}(\\theta)$:\n$$\n\\tilde{A}_{2h}(\\theta) = (c^2, s^2) \\left( C_0 \\begin{pmatrix} s^2 & 0 \\\\ 0 & c^2 \\end{pmatrix} \\right) \\left( 2 \\begin{pmatrix} c^2 \\\\ s^2 \\end{pmatrix} \\right) = 2C_0 (c^2s^2, s^2c^2) \\begin{pmatrix} c^2 \\\\ s^2 \\end{pmatrix} = 2C_0(c^4s^2 + s^4c^2) = 2C_0 s^2c^2(c^2+s^2) = 2C_0s^2c^2\n$$\n- Coarse-grid correction operator $\\tilde{CGC}(\\theta) = I - \\tilde{P}(\\theta)\\tilde{A}_{2h}(\\theta)^{-1}\\tilde{R}(\\theta)\\tilde{A}_h(\\theta)$:\nThe matrix for the second term is $\\tilde{T}(\\theta) = \\tilde{P}(\\theta)\\tilde{A}_{2h}(\\theta)^{-1}\\tilde{R}(\\theta)\\tilde{A}_h(\\theta) = \\begin{pmatrix} c^2 & c^2 \\\\ s^2 & s^2 \\end{pmatrix}$.\nThus,\n$$\n\\tilde{CGC}(\\theta) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\begin{pmatrix} c^2 & c^2 \\\\ s^2 & s^2 \\end{pmatrix} = \\begin{pmatrix} s^2 & -c^2 \\\\ -s^2 & c^2 \\end{pmatrix}\n$$\nThe symbol for the full two-grid cycle is $\\tilde{M}_{TG}(\\theta) = \\tilde{S}_h \\tilde{CGC} \\tilde{S}_h$:\n$$\n\\tilde{M}_{TG}(\\theta) = \\begin{pmatrix} \\mu_s & 0 \\\\ 0 & \\mu_c \\end{pmatrix} \\begin{pmatrix} s^2 & -c^2 \\\\ -s^2 & c^2 \\end{pmatrix} \\begin{pmatrix} \\mu_s & 0 \\\\ 0 & \\mu_c \\end{pmatrix} = \\begin{pmatrix} \\mu_s^2 s^2 & -\\mu_s \\mu_c c^2 \\\\ -\\mu_s \\mu_c s^2 & \\mu_c^2 c^2 \\end{pmatrix}\n$$\nTo find the spectral radius $\\rho(\\tilde{M}_{TG}(\\theta))$, we find its eigenvalues.\n$\\det(\\tilde{M}_{TG}(\\theta)) = (\\mu_s^2 s^2)(\\mu_c^2 c^2) - (-\\mu_s \\mu_c c^2)(-\\mu_s \\mu_c s^2) = \\mu_s^2 \\mu_c^2 s^2 c^2 - \\mu_s^2 \\mu_c^2 s^2 c^2 = 0$.\nSince the determinant is zero, one eigenvalue is $0$. The other is the trace of the matrix.\n$\\lambda_1=0$, $\\lambda_2 = \\mathrm{tr}(\\tilde{M}_{TG}(\\theta)) = \\mu_s^2 s^2 + \\mu_c^2 c^2$.\nThe spectral radius is $\\rho(\\theta) = |\\mu_s^2 s^2 + \\mu_c^2 c^2|$. Let $x=s^2=\\sin^2(\\theta/2)$.\n$$\n\\rho(x) = \\left(1-\\frac{4}{3}x\\right)^2 x + \\left(1-\\frac{4}{3}(1-x)\\right)^2(1-x)\n$$\n$$\n\\rho(x) = \\left(\\frac{3-4x}{3}\\right)^2 x + \\left(\\frac{4x-1}{3}\\right)^2(1-x) = \\frac{1}{9} \\left[ (9-24x+16x^2)x + (16x^2-8x+1)(1-x) \\right]\n$$\n$$\n\\rho(x) = \\frac{1}{9} \\left[ (9x - 24x^2 + 16x^3) + (16x^2 - 8x + 1 - 16x^3 + 8x^2 - x) \\right]\n$$\n$$\n\\rho(x) = \\frac{1}{9} \\left[ (9x - 24x^2 + 16x^3) + (-9x + 24x^2 - 16x^3 + 1) \\right] = \\frac{1}{9}\n$$\nThe spectral radius is constant for all $\\theta \\in (0, \\pi/2]$. The worst-case two-grid convergence factor is therefore this constant value.\n$$\n\\rho_{\\mathrm{TG}} = \\sup_{\\theta \\in (0, \\pi/2]} \\rho(\\theta) = \\frac{1}{9}\n$$",
            "answer": "$$\\boxed{\\frac{1}{9}}$$"
        }
    ]
}