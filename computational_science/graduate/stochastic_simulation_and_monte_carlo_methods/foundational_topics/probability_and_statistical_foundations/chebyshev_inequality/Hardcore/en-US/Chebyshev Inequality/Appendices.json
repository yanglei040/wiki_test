{
    "hands_on_practices": [
        {
            "introduction": "Chebyshev's inequality is celebrated for its universality, providing a tail probability bound for any random variable with a known variance. However, this generality comes at the cost of precision. This exercise asks you to quantify this trade-off by comparing the Chebyshev bound to the exact tail probability of a Gaussian distribution, which serves as a benchmark for many estimators due to the Central Limit Theorem. By calculating the 'conservatism index', you will gain a concrete understanding of how loose the bound can be and why distribution-specific information leads to much tighter results .",
            "id": "3294065",
            "problem": "Consider a real-valued random variable $X$ distributed as a Gaussian (normal) law with mean $\\mu$ and variance $\\sigma^{2}$, denoted $X \\sim \\mathcal{N}(\\mu,\\sigma^{2})$. In stochastic simulation and Monte Carlo (MC) methods, tail bounds control the probability of large deviations of estimators. A foundational inequality for variance-bounded random variables is Chebyshev's inequality, which states that for any $k0$,\n$$\n\\mathbb{P}\\!\\left(|X-\\mu|\\geq k\\,\\sigma\\right)\\leq \\frac{1}{k^{2}}.\n$$\nWork from first principles to compute the actual two-sided tail probability for the Gaussian, and then compare it against the Chebyshev bound. Specifically:\n- Starting from the probability density function of the normal distribution,\n$$\nf_{X}(x)=\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right),\n$$\nand the definition of the cumulative distribution function (CDF), derive the exact expression for $\\mathbb{P}(|X-\\mu|\\geq k\\,\\sigma)$ for a general $k0$.\n- Evaluate this exact tail probability for the set of thresholds $\\{k=1,\\,k=2,\\,k=3\\}$.\n- Quantify the conservatism of Chebyshev's inequality by defining the geometric-mean conservatism index over the set $\\{1,2,3\\}$ as\n$$\nC:=\\left(\\prod_{k\\in\\{1,2,3\\}}\\frac{\\frac{1}{k^{2}}}{\\mathbb{P}\\!\\left(|X-\\mu|\\geq k\\,\\sigma\\right)}\\right)^{\\frac{1}{3}}.\n$$\nCompute $C$ exactly from your derived tail probabilities.\n\nExplain why this index captures the typical multiplicative overestimation of the Chebyshev bound for sub-Gaussian tails in MC error analysis, and relate the calculation to the structure of Gaussian tails without invoking any shortcut formulas beyond core definitions and well-tested facts about Gaussian integrals.\n\nRound your final numerical value of $C$ to four significant figures. No units are required for the answer.",
            "solution": "The problem is valid as it is scientifically grounded in probability theory, well-posed with a clear objective and sufficient data, and free of any subjective or ambiguous statements.\n\nThe problem asks for a derivation of the exact tail probability for a Gaussian random variable, a comparison of this probability with the bound provided by Chebyshev's inequality, and the calculation of a \"conservatism index\" that quantifies this difference.\n\nLet $X$ be a random variable following a Gaussian (normal) distribution with mean $\\mu$ and variance $\\sigma^2$, denoted as $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The probability density function (PDF) is given by\n$$f_{X}(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$$\n\nFirst, we derive the exact expression for the two-sided tail probability $\\mathbb{P}(|X-\\mu| \\geq k\\sigma)$ for some constant $k  0$. The inequality $|X - \\mu| \\geq k\\sigma$ is equivalent to the union of two disjoint events: $(X - \\mu) \\geq k\\sigma$ or $(X - \\mu) \\leq -k\\sigma$.\nTherefore, the probability is the sum of the probabilities of these two events:\n$$\\mathbb{P}(|X - \\mu| \\geq k\\sigma) = \\mathbb{P}(X \\geq \\mu + k\\sigma) + \\mathbb{P}(X \\leq \\mu - k\\sigma)$$\nTo simplify the calculation, we standardize the random variable $X$. Let $Z = \\frac{X - \\mu}{\\sigma}$. The random variable $Z$ follows the standard normal distribution, $Z \\sim \\mathcal{N}(0, 1)$, with PDF $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$. The inequality $|X - \\mu| \\geq k\\sigma$ can be rewritten in terms of $Z$:\n$$\\left|\\frac{X - \\mu}{\\sigma}\\right| \\geq k \\implies |Z| \\geq k$$\nThe probability becomes\n$$\\mathbb{P}(|Z| \\geq k) = \\mathbb{P}(Z \\geq k) + \\mathbb{P}(Z \\leq -k)$$\nDue to the symmetry of the standard normal distribution about $z=0$, we have $\\mathbb{P}(Z \\leq -k) = \\mathbb{P}(Z \\geq k)$. Thus,\n$$\\mathbb{P}(|Z| \\geq k) = 2 \\, \\mathbb{P}(Z \\geq k)$$\nThis probability can be expressed as an integral of the standard normal PDF:\n$$\\mathbb{P}(|Z| \\geq k) = 2 \\int_{k}^{\\infty} \\phi(z) \\, dz = 2 \\int_{k}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) dz$$\nThis integral defines the tail probability of the standard normal distribution. It is commonly expressed in terms of the complementary error function, $\\text{erfc}(x)$, which is defined as\n$$\\text{erfc}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{x}^{\\infty} \\exp(-t^2) dt$$\nTo relate our integral to this definition, we perform a change of variables in the integral. Let $t = z/\\sqrt{2}$. Then $z = \\sqrt{2}t$ and $dz = \\sqrt{2}dt$. The lower limit of integration $z=k$ becomes $t=k/\\sqrt{2}$. The upper limit remains infinity.\n\\begin{align*} \\mathbb{P}(|X-\\mu| \\geq k\\sigma) = 2 \\int_{k}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right) dz \\\\ = \\frac{2}{\\sqrt{2\\pi}} \\int_{k/\\sqrt{2}}^{\\infty} \\exp(-t^2) (\\sqrt{2}dt) \\\\ = \\frac{2\\sqrt{2}}{\\sqrt{2\\pi}} \\int_{k/\\sqrt{2}}^{\\infty} \\exp(-t^2) dt \\\\ = \\frac{2}{\\sqrt{\\pi}} \\int_{k/\\sqrt{2}}^{\\infty} \\exp(-t^2) dt \\end{align*}\nBy the definition of the complementary error function, this is exactly $\\text{erfc}(k/\\sqrt{2})$.\nThus, the exact expression for the tail probability is\n$$\\mathbb{P}(|X-\\mu| \\geq k\\sigma) = \\text{erfc}\\left(\\frac{k}{\\sqrt{2}}\\right)$$\n\nNext, we evaluate this exact probability for $k \\in \\{1, 2, 3\\}$. Let's denote these probabilities as $P_k = \\mathbb{P}(|X-\\mu| \\geq k\\sigma)$.\nFor $k=1$:\n$$P_1 = \\text{erfc}\\left(\\frac{1}{\\sqrt{2}}\\right) \\approx 0.3173105$$\nFor $k=2$:\n$$P_2 = \\text{erfc}\\left(\\frac{2}{\\sqrt{2}}\\right) = \\text{erfc}(\\sqrt{2}) \\approx 0.0455003$$\nFor $k=3$:\n$$P_3 = \\text{erfc}\\left(\\frac{3}{\\sqrt{2}}\\right) \\approx 0.0026998$$\nThe corresponding Chebyshev bounds are $1/k^2$: for $k=1$, the bound is $1$; for $k=2$, it is $1/4 = 0.25$; for $k=3$, it is $1/9 \\approx 0.1111$.\n\nNow, we compute the geometric-mean conservatism index, $C$.\n$$C = \\left(\\prod_{k\\in\\{1,2,3\\}} \\frac{1/k^2}{P_k}\\right)^{1/3} = \\left( \\frac{1/1^2}{P_1} \\cdot \\frac{1/2^2}{P_2} \\cdot \\frac{1/3^2}{P_3} \\right)^{1/3}$$\nLet's calculate the individual ratios, which represent the factor by which Chebyshev's inequality overestimates the true probability:\nRatio for $k=1$: $R_1 = \\frac{1}{P_1} = \\frac{1}{0.3173105} \\approx 3.15147$\nRatio for $k=2$: $R_2 = \\frac{1/4}{P_2} = \\frac{0.25}{0.0455003} \\approx 5.49448$\nRatio for $k=3$: $R_3 = \\frac{1/9}{P_3} = \\frac{1/9}{0.0026998} \\approx 41.1565$\nThe product of these ratios is\n$$R_1 \\cdot R_2 \\cdot R_3 \\approx 3.15147 \\times 5.49448 \\times 41.1565 \\approx 712.56$$\nThe index $C$ is the cube root of this product:\n$$C = (712.56)^{1/3} \\approx 8.93208$$\nRounding to four significant figures, we get $C \\approx 8.932$.\n\nThe conservatism index $C$ quantifies the degree to which Chebyshev's inequality provides a loose bound for a Gaussian distribution. Each term in the product, $\\frac{1/k^2}{P_k}$, is the multiplicative factor by which the Chebyshev bound exceeds the actual tail probability. The geometric mean is an appropriate average for these factors, as it gives a sense of the typical overestimation across the chosen scales $k \\in \\{1, 2, 3\\}$. A value of $C \\approx 8.932$ indicates that, on average for these small $k$ values, the Chebyshev bound is almost an order of magnitude larger than the true Gaussian tail probability.\n\nThis significant discrepancy arises because Chebyshev's inequality is universal. It applies to any random variable with a finite variance, making no further assumptions about the shape of its distribution. The bound is \"tight\" only for \"worst-case\" distributions that concentrate probability mass far from the mean, right at the $\\pm k\\sigma$ boundaries. The Gaussian distribution is fundamentally different; it is a \"sub-Gaussian\" distribution, meaning its tails decay exponentially fast (specifically as $\\exp(-z^2/2)$). This is a much faster decay than the algebraic $1/k^2$ decay of the Chebyshev bound. As $k$ increases, the overestimation factor grows rapidly, as seen by the jump from $R_1 \\approx 3.15$ to $R_3 \\approx 41.16$. In the context of Monte Carlo (MC) error analysis, estimators are often sums of many random variables and, by the Central Limit Theorem, their distributions are approximately Gaussian. Using Chebyshev's inequality to bound the probability of large estimation errors would be extremely pessimistic. This calculation demonstrates that employing knowledge of the (approximate) distribution shape allows for much tighter and more realistic error bounds, which is crucial for assessing the reliability and efficiency of MC simulations.",
            "answer": "$$\n\\boxed{8.932}\n$$"
        },
        {
            "introduction": "Despite its conservatism, Chebyshev's inequality provides a rock-solid foundation for designing robust Monte Carlo experiments. A crucial practical question is determining the number of samples required to achieve a desired level of accuracy with a certain confidence. This practice guides you through the process of creating a sequential stopping rule, which translates to finding a minimum sample size, using a known upper bound $B$ on the estimator's variance to guarantee performance . This exercise demonstrates how to turn the abstract inequality into a concrete, actionable plan for simulation.",
            "id": "3294130",
            "problem": "Consider a Monte Carlo (MC) estimation scenario where independent and identically distributed random variables $\\{X_{i}\\}_{i \\geq 1}$ are sampled from an unknown distribution with finite mean $\\mu$ and finite variance $\\sigma^{2}$. Suppose an a priori upper bound $B$ satisfying $\\sigma^{2} \\leq B$ is available. Let the MC estimator of $\\mu$ after $n$ samples be the sample mean $\\bar{X}_{n} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}$. You are tasked with designing a conservative sequential stopping rule that guarantees the coverage requirement $\\mathbb{P}(|\\bar{X}_{T} - \\mu| \\leq \\epsilon) \\geq 1 - \\delta$ for prespecified tolerances $\\epsilon  0$ and $\\delta \\in (0,1)$, where $T$ is the (possibly random) stopping time determined by your rule.\n\nStarting only from fundamental facts about expectation and variance of sums of independent random variables, and general tail bounds derived from them, derive a stopping rule that is expressed solely in terms of $B$, $\\epsilon$, and $\\delta$ and does not depend on unknown distributional features beyond the bound $B$. Your rule must be sequential in the sense that it can be applied as samples arrive, but it may reduce to a deterministic threshold.\n\nFormally, specify the rule in the form “stop at the first $n$ such that $n \\geq N^{\\ast}(B,\\epsilon,\\delta)$,” and determine the exact analytic expression for the smallest integer threshold $N^{\\ast}(B,\\epsilon,\\delta)$ that guarantees the stated coverage requirement for all distributions with variance bounded by $B$. Provide the final answer as a single closed-form expression for $N^{\\ast}(B,\\epsilon,\\delta)$.",
            "solution": "The problem requires the derivation of a stopping rule for a Monte Carlo estimation of a mean $\\mu$. The rule must guarantee a specified coverage probability for the estimate. This will be accomplished by determining the minimum number of samples, $N^{\\ast}$, required to satisfy the condition $\\mathbb{P}(|\\bar{X}_{T} - \\mu| \\leq \\epsilon) \\geq 1 - \\delta$ for given tolerances $\\epsilon  0$ and $\\delta \\in (0,1)$, where the variance $\\sigma^2$ of the underlying distribution is unknown but bounded by $\\sigma^2 \\leq B$. The rule must be derived from first principles concerning expectation and variance, and a general probability inequality.\n\nLet $\\{X_{i}\\}_{i \\geq 1}$ be a sequence of independent and identically distributed (i.i.d.) random variables with finite mean $\\mathbb{E}[X_i] = \\mu$ and finite variance $\\text{Var}(X_i) = \\sigma^2$. The Monte Carlo estimator for $\\mu$ based on $n$ samples is the sample mean, $\\bar{X}_{n} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}$.\n\nFirst, we determine the expectation and variance of the estimator $\\bar{X}_n$.\nThe expectation of $\\bar{X}_n$ is:\n$$\n\\mathbb{E}[\\bar{X}_{n}] = \\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n} X_{i}\\right] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}[X_{i}] = \\frac{1}{n} \\sum_{i=1}^{n} \\mu = \\frac{n\\mu}{n} = \\mu\n$$\nThis shows that $\\bar{X}_n$ is an unbiased estimator of $\\mu$.\n\nThe variance of $\\bar{X}_n$ is calculated using the property that the variance of a sum of independent random variables is the sum of their variances:\n$$\n\\text{Var}(\\bar{X}_{n}) = \\text{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i}\\right) = \\frac{1}{n^2} \\text{Var}\\left(\\sum_{i=1}^{n} X_{i}\\right) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\text{Var}(X_i) = \\frac{1}{n^2} \\sum_{i=1}^{n} \\sigma^2 = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}\n$$\n\nThe problem specifies a coverage requirement on the stopping time $T=n$:\n$$\n\\mathbb{P}(|\\bar{X}_{n} - \\mu| \\leq \\epsilon) \\geq 1 - \\delta\n$$\nThis is equivalent to bounding the probability of the complementary event, which is that the estimation error exceeds $\\epsilon$:\n$$\n\\mathbb{P}(|\\bar{X}_{n} - \\mu|  \\epsilon) \\leq \\delta\n$$\n\nSince the underlying distribution of the $X_i$ is unknown, we cannot make strong distributional assumptions (like normality, which would lead to the Central Limit Theorem). The problem directs us to use general tail bounds derived from expectation and variance. The appropriate tool for this is Chebyshev's inequality. For any random variable $Y$ with finite mean $\\mathbb{E}[Y]$ and finite variance $\\text{Var}(Y)$, Chebyshev's inequality states that for any constant $k  0$:\n$$\n\\mathbb{P}(|Y - \\mathbb{E}[Y]| \\geq k) \\leq \\frac{\\text{Var}(Y)}{k^2}\n$$\nWe apply this inequality to our estimator $Y = \\bar{X}_n$, with $\\mathbb{E}[Y] = \\mu$, $\\text{Var}(Y) = \\sigma^2/n$, and the threshold $k = \\epsilon$ (since $\\epsilon  0$). This yields:\n$$\n\\mathbb{P}(|\\bar{X}_{n} - \\mu| \\geq \\epsilon) \\leq \\frac{\\text{Var}(\\bar{X}_{n})}{\\epsilon^2}\n$$\nThe events $|\\bar{X}_{n} - \\mu|  \\epsilon$ and $|\\bar{X}_{n} - \\mu| \\geq \\epsilon$ are equivalent for continuous random variables. For discrete variables, the inequality still holds: $\\mathbb{P}(|\\bar{X}_{n} - \\mu|  \\epsilon) \\leq \\mathbb{P}(|\\bar{X}_{n} - \\mu| \\geq \\epsilon)$. Thus, we have:\n$$\n\\mathbb{P}(|\\bar{X}_{n} - \\mu|  \\epsilon) \\leq \\frac{\\sigma^2/n}{\\epsilon^2} = \\frac{\\sigma^2}{n\\epsilon^2}\n$$\n\nTo satisfy the required condition $\\mathbb{P}(|\\bar{X}_{n} - \\mu|  \\epsilon) \\leq \\delta$, it is sufficient to enforce that the upper bound from Chebyshev's inequality is less than or equal to $\\delta$:\n$$\n\\frac{\\sigma^2}{n\\epsilon^2} \\leq \\delta\n$$\nThis provides a conservative guarantee. Now, we must solve for the number of samples, $n$:\n$$\nn \\geq \\frac{\\sigma^2}{\\delta \\epsilon^2}\n$$\nThe true variance $\\sigma^2$ is unknown. However, we are given an a priori upper bound $\\sigma^2 \\leq B$. To ensure the inequality holds for any possible value of $\\sigma^2$ under this constraint, we must use the worst-case value for $\\sigma^2$, which is $B$. By substituting $B$ for $\\sigma^2$, we obtain a condition on $n$ that is independent of any unknown distributional parameters:\n$$\nn \\geq \\frac{B}{\\delta \\epsilon^2}\n$$\nAny integer $n$ satisfying this inequality will guarantee the coverage requirement. The problem asks for the smallest integer threshold, $N^{\\ast}(B,\\epsilon,\\delta)$, that satisfies this condition. This corresponds to the smallest integer $n$ that is greater than or equal to the quantity $\\frac{B}{\\delta \\epsilon^2}$. By definition, this is the ceiling of that quantity.\n$$\nN^{\\ast}(B,\\epsilon,\\delta) = \\left\\lceil \\frac{B}{\\delta \\epsilon^2} \\right\\rceil\n$$\nThe stopping rule is to stop sampling at the first integer $n$ such that $n \\geq N^{\\ast}$. This simplifies to a deterministic rule: set the number of samples to $n = T = N^{\\ast}$. This is the smallest sample size that provides the desired guarantee for all distributions with variance bounded by $B$, based on the information provided and the use of Chebyshev's inequality.",
            "answer": "$$\n\\boxed{\\left\\lceil \\frac{B}{\\delta \\epsilon^2} \\right\\rceil}\n$$"
        },
        {
            "introduction": "The previous practice demonstrated how to determine sample size using a variance bound $B$, but it left open the question of where such a bound comes from. This exercise tackles that question directly for a common scenario where a random variable is known to be confined to a finite interval $[a, b]$. You will first derive a famous result, Popoviciu's inequality, which provides the tightest possible variance bound based only on the support, and then use it to construct a guaranteed confidence interval for the mean . This practice equips you with a powerful, distribution-free technique for error certification in Monte Carlo methods.",
            "id": "3294078",
            "problem": "In a Monte Carlo estimation scenario, let $X$ be a scalar random variable with unknown distribution supported on a known interval $[a,b]$, where $ab$ are finite constants. Denote the mean by $\\mu = \\mathbb{E}[X]$ and the variance by $\\sigma^{2} = \\operatorname{Var}(X)$. You observe $n$ independent and identically distributed (IID) samples $X_{1},\\dots,X_{n}$ and form the sample mean $\\overline{X}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$. Starting only from the definitions of variance and the statement of Chebyshev’s inequality, derive an upper bound on $\\sigma^{2}$ that depends only on $a$ and $b$. Then use this bound to construct a two-sided symmetric Chebyshev-based confidence interval for $\\mu$ with coverage at least $1-\\delta$, where $\\delta \\in (0,1)$ is a user-chosen risk level. Provide the smallest deterministic half-width $h(a,b,n,\\delta)$ such that the interval $[\\overline{X}_{n} - h,\\ \\overline{X}_{n} + h]$ has coverage at least $1-\\delta$ without using any unknown quantities. Your final answer must be a single closed-form expression for $h(a,b,n,\\delta)$ in terms of $a$, $b$, $n$, and $\\delta$.",
            "solution": "The user wants me to find the smallest deterministic half-width $h(a,b,n,\\delta)$ for a two-sided symmetric Chebyshev-based confidence interval for the mean $\\mu$ of a random variable $X$. The interval is of the form $[\\overline{X}_{n} - h, \\overline{X}_{n} + h]$ and must have a coverage probability of at least $1-\\delta$. The random variable $X$ has an unknown distribution but is known to be supported on a finite interval $[a, b]$.\n\nThe process involves two main steps:\n1.  Derive an upper bound on the variance $\\sigma^2 = \\operatorname{Var}(X)$ that depends only on the known interval endpoints $a$ and $b$.\n2.  Use this variance bound within Chebyshev's inequality applied to the sample mean $\\overline{X}_{n}$ to determine the half-width $h$.\n\nFirst, let's derive the upper bound for $\\sigma^2$. The variance of $X$ is defined as $\\sigma^2 = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = \\mathbb{E}[X^2] - \\mu^2$. Since the support of $X$ is $[a, b]$, it is guaranteed that $a \\le X \\le b$. This implies that $(X-a) \\ge 0$ and $(X-b) \\le 0$. Consequently, the product of these two terms must be non-positive:\n$$ (X-a)(X-b) \\le 0 $$\nExpanding the left side of the inequality gives:\n$$ X^2 - (a+b)X + ab \\le 0 $$\nBy the linearity of expectation, we can take the expectation of both sides of this inequality:\n$$ \\mathbb{E}[X^2 - (a+b)X + ab] \\le 0 $$\n$$ \\mathbb{E}[X^2] - (a+b)\\mathbb{E}[X] + ab \\le 0 $$\nSubstituting $\\mu = \\mathbb{E}[X]$, we obtain an upper bound on $\\mathbb{E}[X^2]$:\n$$ \\mathbb{E}[X^2] \\le (a+b)\\mu - ab $$\nNow, we can substitute this bound into the expression for the variance:\n$$ \\sigma^2 = \\mathbb{E}[X^2] - \\mu^2 \\le (a+b)\\mu - ab - \\mu^2 $$\nLet's define a function $f(\\mu) = -\\mu^2 + (a+b)\\mu - ab$. To find the tightest possible upper bound on $\\sigma^2$, we need to find the maximum value of $f(\\mu)$. The function $f(\\mu)$ is a quadratic in $\\mu$ representing a downward-opening parabola. Its maximum value occurs at its vertex. The $\\mu$-coordinate of the vertex is:\n$$ \\mu_{\\text{vertex}} = -\\frac{(a+b)}{2(-1)} = \\frac{a+b}{2} $$\nSince $X$ is supported on $[a, b]$, its mean $\\mu$ must also lie within this interval, i.e., $a \\le \\mu \\le b$. The vertex $\\mu = \\frac{a+b}{2}$ is the midpoint of $[a, b]$ and is thus a valid potential value for the mean. The maximum value of $f(\\mu)$ is obtained at this vertex:\n$$ \\max_{\\mu} f(\\mu) = f\\left(\\frac{a+b}{2}\\right) = -\\left(\\frac{a+b}{2}\\right)^2 + (a+b)\\left(\\frac{a+b}{2}\\right) - ab $$\n$$ = -\\frac{(a+b)^2}{4} + \\frac{2(a+b)^2}{4} - \\frac{4ab}{4} = \\frac{(a+b)^2 - 4ab}{4} = \\frac{a^2 + 2ab + b^2 - 4ab}{4} = \\frac{a^2 - 2ab + b^2}{4} = \\frac{(b-a)^2}{4} $$\nThus, we have established the upper bound for the variance, known as Popoviciu's inequality on variances:\n$$ \\sigma^2 \\le \\frac{(b-a)^2}{4} $$\n\nNext, we construct the confidence interval. We are given $n$ IID samples $X_1, \\dots, X_n$ and the sample mean $\\overline{X}_n = \\frac{1}{n}\\sum_{i=1}^{n} X_i$. The mean of the sample mean is $\\mathbb{E}[\\overline{X}_n] = \\mu$, and its variance is $\\operatorname{Var}(\\overline{X}_n) = \\frac{\\sigma^2}{n}$.\n\nChebyshev's inequality states that for a random variable $Y$ with mean $\\mathbb{E}[Y]$ and variance $\\operatorname{Var}(Y)$, and for any constant $k  0$:\n$$ P(|Y - \\mathbb{E}[Y]| \\ge k) \\le \\frac{\\operatorname{Var}(Y)}{k^2} $$\nWe apply this inequality to the random variable $Y = \\overline{X}_n$ with the constant $k$ set to the desired half-width $h$:\n$$ P(|\\overline{X}_n - \\mu| \\ge h) \\le \\frac{\\operatorname{Var}(\\overline{X}_n)}{h^2} = \\frac{\\sigma^2}{nh^2} $$\nThe confidence interval is $[\\overline{X}_n - h, \\overline{X}_n + h]$. The requirement is that the coverage probability, which is the probability that this interval contains the true mean $\\mu$, is at least $1-\\delta$. This condition is expressed as:\n$$ P(\\mu \\in [\\overline{X}_n - h, \\overline{X}_n + h]) \\ge 1-\\delta $$\nThis is equivalent to $P(|\\overline{X}_n - \\mu| \\le h) \\ge 1-\\delta$. The complementary event, or the failure event, is that $\\mu$ is not in the interval, which means $|\\overline{X}_n - \\mu|  h$. The probability of this failure event must be at most $\\delta$:\n$$ P(|\\overline{X}_n - \\mu|  h) \\le \\delta $$\nFrom the properties of probability, $P(|\\overline{X}_n - \\mu|  h) \\le P(|\\overline{X}_n - \\mu| \\ge h)$. Therefore, to guarantee that the failure probability is at most $\\delta$, it is sufficient to enforce the stronger condition $P(|\\overline{X}_n - \\mu| \\ge h) \\le \\delta$.\nUsing the bound from Chebyshev's inequality, we require:\n$$ \\frac{\\sigma^2}{nh^2} \\le \\delta $$\nThis inequality still contains the unknown variance $\\sigma^2$. To ensure the condition holds for any valid distribution on $[a, b]$, we must use the worst-case scenario for $\\sigma^2$, which corresponds to its maximum possible value. We substitute our previously derived bound $\\sigma^2 \\le \\frac{(b-a)^2}{4}$:\n$$ \\frac{(b-a)^2 / 4}{nh^2} \\le \\delta $$\nWe now solve for $h$. Since $h$ is a width, it must be positive. Rearranging the inequality gives:\n$$ \\frac{(b-a)^2}{4n\\delta} \\le h^2 $$\nTaking the positive square root of both sides, we get:\n$$ h \\ge \\sqrt{\\frac{(b-a)^2}{4n\\delta}} = \\frac{b-a}{2\\sqrt{n\\delta}} $$\nThe problem asks for the smallest deterministic half-width $h$ that guarantees the coverage. This corresponds to choosing the minimum possible value for $h$, which is the lower bound itself. Therefore, the expression for $h(a,b,n,\\delta)$ is:\n$$ h(a,b,n,\\delta) = \\frac{b-a}{2\\sqrt{n\\delta}} $$\nThis expression depends only on the given quantities $a$, $b$, $n$, and $\\delta$, as required.",
            "answer": "$$\\boxed{\\frac{b-a}{2\\sqrt{n\\delta}}}$$"
        }
    ]
}