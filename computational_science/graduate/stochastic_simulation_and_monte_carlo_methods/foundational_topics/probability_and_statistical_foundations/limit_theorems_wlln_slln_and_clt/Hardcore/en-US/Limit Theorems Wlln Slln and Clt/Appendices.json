{
    "hands_on_practices": [
        {
            "introduction": "Beyond estimating the mean $\\mu$ of a distribution, we often need to estimate a function of the mean, $g(\\mu)$. This practice introduces the delta method, an essential tool that combines the Central Limit Theorem with Taylor series approximations to analyze such transformed estimators. By working through this exercise , you will learn how to propagate the uncertainty from a sample mean $\\bar{X}_n$ to a function of the mean $g(\\bar{X}_n)$, allowing you to derive the asymptotic variance for a wide range of non-linear estimators. This is a foundational skill for understanding the behavior of many sophisticated statistical models and Monte Carlo outputs.",
            "id": "3317830",
            "problem": "Consider a Monte Carlo setting in which one simulates $n$ independent and identically distributed positive random variables $\\{X_{i}\\}_{i=1}^{n}$ with common mean $\\mu = \\mathbb{E}[X_{1}]$ satisfying $\\mu \\in (0,\\infty)$ and variance $\\sigma^{2} = \\operatorname{Var}(X_{1})$ satisfying $\\sigma^{2} \\in (0,\\infty)$. Let $T_{n} = \\bar{X}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$ and define the transformed estimator $Y_{n} = \\log(T_{n})$. Starting from the Weak Law of Large Numbers (WLLN) and the Central Limit Theorem (CLT), and using only first principles such as differentiability and Taylor expansion of a smooth function, apply the delta method to the function $g(x) = \\log x$ at the point $x = \\mu$ to derive the leading-order asymptotic variance of $Y_{n}$ as $n \\to \\infty$. Express your final answer in closed form as a function of $\\mu$, $\\sigma^{2}$, and $n$. No numerical evaluation is required, and no rounding is needed. The final answer must be a single analytic expression.",
            "solution": "The problem is first validated against the required criteria.\n\n### Step 1: Extract Givens\n- A set of $n$ independent and identically distributed (i.i.d.) positive random variables $\\{X_{i}\\}_{i=1}^{n}$.\n- The common mean is $\\mu = \\mathbb{E}[X_{1}]$, with $\\mu \\in (0,\\infty)$.\n- The common variance is $\\sigma^{2} = \\operatorname{Var}(X_{1})$, with $\\sigma^{2} \\in (0,\\infty)$.\n- The sample mean is $T_{n} = \\bar{X}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$.\n- A transformed estimator is defined as $Y_{n} = \\log(T_{n})$.\n- The task is to apply the delta method to the function $g(x) = \\log x$ at the point $x = \\mu$.\n- The derivation must start from the Weak Law of Large Numbers (WLLN) and the Central Limit Theorem (CLT) and use first principles like Taylor expansion.\n- The objective is to find the leading-order asymptotic variance of $Y_{n}$ as $n \\to \\infty$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It is a standard application of limit theorems in statistics.\n1.  **Scientific Soundness**: The problem rests on fundamental principles of probability theory (WLLN, CLT) and calculus (Taylor expansion). The assumptions ($\\mu > 0$, $\\sigma^2 > 0$, $X_i > 0$) are consistent and standard for this type of analysis. The function $g(x) = \\log(x)$ is well-defined and differentiable for the domain of interest, since $X_i > 0$ implies $T_n > 0$ and it is given that $\\mu > 0$.\n2.  **Well-Posedness**: The problem is clearly stated and has a unique, meaningful solution obtainable via the specified method (delta method). All necessary information is provided.\n3.  **Objectivity**: The problem is stated in precise mathematical language, free of any subjectivity or ambiguity.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation\nThe problem asks for the leading-order asymptotic variance of $Y_n = \\log(T_n)$, where $T_n = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$ is the sample mean of $n$ i.i.d. positive random variables with mean $\\mu$ and variance $\\sigma^2$.\n\nThe foundation of our analysis rests on two key limit theorems for the sample mean $T_n$:\n1.  The Weak Law of Large Numbers (WLLN) states that the sample mean converges in probability to the true mean $\\mu$. We denote this as:\n    $$T_n \\xrightarrow{p} \\mu \\quad \\text{as } n \\to \\infty$$\n2.  The Central Limit Theorem (CLT) describes the distribution of the standardized sample mean. It states that:\n    $$\\sqrt{n}(T_n - \\mu) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2) \\quad \\text{as } n \\to \\infty$$\n    where `d` denotes convergence in distribution, and $\\mathcal{N}(0, \\sigma^2)$ is a normal distribution with mean $0$ and variance $\\sigma^2$.\n\nWe are interested in the behavior of the transformed random variable $Y_n = g(T_n)$, where the function is $g(x) = \\log(x)$. The problem directs us to use the delta method, which we will derive from a first-order Taylor expansion of $g(T_n)$ around the point $x=\\mu$.\n\nSince we are given $\\mu \\in (0, \\infty)$, and the WLLN tells us $T_n$ will be close to $\\mu$ for large $n$, we can expand $g(T_n)$ around $\\mu$. The function $g(x) = \\log(x)$ is continuously differentiable for $x > 0$. Its first derivative is $g'(x) = \\frac{1}{x}$.\n\nThe first-order Taylor expansion of $g(T_n)$ around $\\mu$ is:\n$$g(T_n) = g(\\mu) + g'(\\mu)(T_n - \\mu) + R_n$$\nwhere $R_n$ is the remainder term, which satisfies $\\frac{R_n}{T_n - \\mu} \\to 0$ as $T_n \\to \\mu$.\n\nSubstituting $g(x) = \\log(x)$ and its derivative $g'(x) = \\frac{1}{x}$:\n$$Y_n = \\log(T_n) = \\log(\\mu) + \\frac{1}{\\mu}(T_n - \\mu) + R_n$$\nNote that $g'(\\mu) = \\frac{1}{\\mu}$ is well-defined and finite since $\\mu > 0$.\n\nTo analyze the asymptotic distribution, we rearrange the equation and scale by $\\sqrt{n}$:\n$$\\sqrt{n}(Y_n - \\log(\\mu)) = \\sqrt{n}(\\log(T_n) - \\log(\\mu)) = \\frac{1}{\\mu} \\sqrt{n}(T_n - \\mu) + \\sqrt{n} R_n$$\n\nAccording to the properties of the Taylor remainder and the fact that $T_n \\xrightarrow{p} \\mu$, the term $\\sqrt{n}R_n$ converges in probability to $0$. Therefore, by Slutsky's theorem, the asymptotic distribution of the left-hand side is the same as the asymptotic distribution of the first term on the right-hand side.\n$$\\sqrt{n}(Y_n - \\log(\\mu)) \\stackrel{d}{\\longrightarrow} \\frac{1}{\\mu} \\left( \\sqrt{n}(T_n - \\mu) \\right)$$\n\nFrom the CLT, we know that the term $\\sqrt{n}(T_n - \\mu)$ converges in distribution to a random variable $Z \\sim \\mathcal{N}(0, \\sigma^2)$. Consequently, the term $\\frac{1}{\\mu}\\sqrt{n}(T_n - \\mu)$ converges in distribution to $\\frac{1}{\\mu}Z$.\n\nLet's find the distribution of $\\frac{1}{\\mu}Z$. Since $Z$ is a normal random variable, any linear transformation of it is also normal.\nThe mean is $\\mathbb{E}\\left[\\frac{1}{\\mu}Z\\right] = \\frac{1}{\\mu}\\mathbb{E}[Z] = \\frac{1}{\\mu} \\cdot 0 = 0$.\nThe variance is $\\operatorname{Var}\\left(\\frac{1}{\\mu}Z\\right) = \\left(\\frac{1}{\\mu}\\right)^2 \\operatorname{Var}(Z) = \\frac{1}{\\mu^2} \\sigma^2 = \\frac{\\sigma^2}{\\mu^2}$.\n\nThus, we have established the asymptotic distribution for the transformed estimator:\n$$\\sqrt{n}(Y_n - \\log(\\mu)) \\xrightarrow{d} \\mathcal{N}\\left(0, \\frac{\\sigma^2}{\\mu^2}\\right)$$\n\nThis result implies that for large $n$, the distribution of $Y_n = \\log(T_n)$ can be approximated by a normal distribution:\n$$Y_n \\approx \\mathcal{N}\\left(\\log(\\mu), \\frac{1}{n} \\frac{\\sigma^2}{\\mu^2}\\right)$$\n\nThe variance of this approximating normal distribution is what is referred to as the leading-order asymptotic variance of $Y_n$. It is an approximation to $\\operatorname{Var}(Y_n)$ for large $n$.\n$$\\operatorname{Var}(Y_n) \\approx \\frac{\\sigma^2}{n\\mu^2}$$\nThis expression is the required leading-order asymptotic variance of $Y_n$, expressed as a function of $\\mu$, $\\sigma^2$, and $n$.",
            "answer": "$$\\boxed{\\frac{\\sigma^2}{n\\mu^2}}$$"
        },
        {
            "introduction": "The power of Monte Carlo methods is greatly enhanced by variance reduction techniques, which provide more precise estimates for the same computational cost. This practice explores stratified sampling, a cornerstone of efficient simulation design where a population is divided into disjoint strata and sampled independently from each. This exercise  guides you through deriving a Central Limit Theorem for the stratified estimator and then using optimization principles to determine the optimal allocation of samples across strata. This demonstrates how limit theorems are not just analytical tools, but also provide a powerful framework for designing maximally efficient simulation experiments.",
            "id": "3317803",
            "problem": "Consider a stratified Monte Carlo estimator built from $H$ disjoint strata indexed by $h \\in \\{1,\\dots,H\\}$. In stratum $h$, let $\\{X_{h,i}\\}_{i \\geq 1}$ be an independent and identically distributed sequence with finite mean $E[X_{h,1}]=\\mu_h$, finite variance $\\operatorname{Var}(X_{h,1})=\\sigma_h^{2}$, and finite $(2+\\delta)$-th absolute moment $E[|X_{h,1}-\\mu_h|^{2+\\delta}]\\infty$ for some fixed $\\delta0$. Assume independence across strata. Let $w_h0$ be fixed strata weights satisfying $\\sum_{h=1}^{H} w_h = 1$, and let $c_h0$ be the per-sample cost in stratum $h$. For a total budget $C0$, choose sample sizes $n_h(C)\\in\\mathbb{N}$ satisfying the exact budget constraint $\\sum_{h=1}^{H} c_h\\,n_h(C)=C$, and define the stratified estimator\n$$\n\\hat{\\mu}_{C} \\;=\\; \\sum_{h=1}^{H} w_h\\,\\bar{X}_{n_h(C),h},\n$$\nwhere $\\bar{X}_{n_h(C),h} = \\frac{1}{n_h(C)}\\sum_{i=1}^{n_h(C)} X_{h,i}$.\n\nStarting from core definitions of expectation, variance, independence, and the conditions of the Lindeberg–Feller Central Limit Theorem (CLT), derive the asymptotic normality of $\\sqrt{C}\\big(\\hat{\\mu}_{C}-\\sum_{h=1}^{H} w_h \\mu_h\\big)$ as $C\\to\\infty$ under any allocation sequence for which the budget fractions $b_h(C):=\\frac{c_h\\,n_h(C)}{C}$ converge to strictly positive limits $b_h\\in(0,1)$ with $\\sum_{h=1}^{H} b_h=1$. Express the limiting variance explicitly in terms of $\\{w_h,\\sigma_h^{2},c_h,b_h\\}_{h=1}^{H}$.\n\nThen, using first principles of constrained optimization and without invoking pre-packaged allocation rules, identify the limiting budget fractions $b_h$ that minimize the asymptotic variance of $\\sqrt{C}\\big(\\hat{\\mu}_{C}-\\sum_{h=1}^{H} w_h \\mu_h\\big)$ subject to $\\sum_{h=1}^{H} b_h=1$ and $b_h0$, and give the resulting minimal asymptotic variance in closed form in terms of $\\{w_h,\\sigma_h,c_h\\}_{h=1}^{H}$.\n\nProvide only the final closed-form expression for this minimal asymptotic variance as your answer. No numerical evaluation is required; do not round. The final answer must be a single analytic expression.",
            "solution": "The problem is valid as it is a well-posed, scientifically grounded question in the field of stochastic simulation and Monte Carlo methods. All necessary conditions and definitions are provided, and there are no internal contradictions or ambiguities.\n\nWe are tasked with finding the asymptotic distribution of the stratified estimator $\\hat{\\mu}_{C}$ and then minimizing its asymptotic variance. Let $\\mu_{S} = E[\\hat{\\mu}_{C}]$. First, we compute the expectation of the estimator:\n$$\nE[\\hat{\\mu}_{C}] = E\\left[\\sum_{h=1}^{H} w_h \\bar{X}_{n_h(C),h}\\right] = \\sum_{h=1}^{H} w_h E[\\bar{X}_{n_h(C),h}] = \\sum_{h=1}^{H} w_h \\mu_h\n$$\nWe have identified the true mean of the estimator as $\\mu_{S} = \\sum_{h=1}^{H} w_h \\mu_h$. We are interested in the asymptotic behavior of $\\sqrt{C}(\\hat{\\mu}_{C} - \\mu_{S})$ as the total budget $C \\to \\infty$.\n\nLet us analyze the variance of $\\hat{\\mu}_{C}$. Due to the independence of samples across strata, the variance of the sum is the sum of the variances:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{C}) = \\operatorname{Var}\\left(\\sum_{h=1}^{H} w_h \\bar{X}_{n_h(C),h}\\right) = \\sum_{h=1}^{H} w_h^2 \\operatorname{Var}(\\bar{X}_{n_h(C),h})\n$$\nThe variance of the sample mean in stratum $h$ is $\\operatorname{Var}(\\bar{X}_{n_h(C),h}) = \\frac{\\sigma_h^2}{n_h(C)}$. Thus,\n$$\n\\operatorname{Var}(\\hat{\\mu}_{C}) = \\sum_{h=1}^{H} \\frac{w_h^2 \\sigma_h^2}{n_h(C)}\n$$\nWe are interested in the normalized random variable $\\sqrt{C}(\\hat{\\mu}_{C} - \\mu_{S})$. Its variance is:\n$$\n\\operatorname{Var}\\left(\\sqrt{C}(\\hat{\\mu}_{C} - \\mu_{S})\\right) = C \\operatorname{Var}(\\hat{\\mu}_{C}) = C \\sum_{h=1}^{H} \\frac{w_h^2 \\sigma_h^2}{n_h(C)}\n$$\nWe are given that the budget fractions $b_h(C) = \\frac{c_h n_h(C)}{C}$ converge to a limit $b_h \\in (0,1)$ as $C \\to \\infty$. This implies that $\\frac{n_h(C)}{C} \\to \\frac{b_h}{c_h}$.\nLet us now find the limit of the variance as $C \\to \\infty$:\n$$\n\\lim_{C\\to\\infty} \\operatorname{Var}\\left(\\sqrt{C}(\\hat{\\mu}_{C} - \\mu_{S})\\right) = \\lim_{C\\to\\infty} \\sum_{h=1}^{H} \\frac{w_h^2 \\sigma_h^2}{n_h(C)/C} = \\sum_{h=1}^{H} \\frac{w_h^2 \\sigma_h^2}{b_h/c_h} = \\sum_{h=1}^{H} \\frac{c_h w_h^2 \\sigma_h^2}{b_h}\n$$\nLet this asymptotic variance be denoted by $\\sigma_{\\text{asy}}^2(b_1, \\dots, b_H)$.\n\nTo establish asymptotic normality, we represent the variable of interest as a sum of a large number of independent random variables and verify the Lindeberg condition. Let $Z_{h,i} = X_{h,i} - \\mu_h$.\n$$\n\\sqrt{C}(\\hat{\\mu}_{C} - \\mu_{S}) = \\sqrt{C} \\sum_{h=1}^{H} w_h \\left( \\frac{1}{n_h(C)} \\sum_{i=1}^{n_h(C)} Z_{h,i} \\right) = \\sum_{h=1}^{H} \\sum_{i=1}^{n_h(C)} \\frac{\\sqrt{C} w_h}{n_h(C)} Z_{h,i}\n$$\nLet $Y_{C,h,i} = \\frac{\\sqrt{C} w_h}{n_h(C)} Z_{h,i}$. These are independent random variables with $E[Y_{C,h,i}]=0$. The total variance is $s_C^2 = \\sum_{h,i} \\operatorname{Var}(Y_{C,h,i}) = \\sum_{h=1}^{H} n_h(C) \\frac{C w_h^2}{n_h(C)^2} \\sigma_h^2 = C \\sum_{h=1}^{H} \\frac{w_h^2 \\sigma_h^2}{n_h(C)}$, which converges to $\\sigma_{\\text{asy}}^2$. The Lindeberg condition requires that for any $\\epsilon  0$,\n$$\n\\frac{1}{s_C^2} \\sum_{h=1}^{H} \\sum_{i=1}^{n_h(C)} E[Y_{C,h,i}^2 \\mathbb{I}(|Y_{C,h,i}|  \\epsilon s_C) ] \\to 0\n$$\nas $C \\to \\infty$. The condition $|Y_{C,h,i}|  \\epsilon s_C$ is equivalent to $|Z_{h,i}|  \\epsilon s_C \\frac{n_h(C)}{\\sqrt{C}w_h}$. Since $s_C$ converges to a constant and $\\frac{n_h(C)}{\\sqrt{C}} \\sim \\frac{b_h \\sqrt{C}}{c_h} \\to \\infty$, the threshold for $|Z_{h,i}|$ grows to infinity. The provided condition $E[|X_{h,1}-\\mu_h|^{2+\\delta}]  \\infty$ for some $\\delta0$ is a Lyapunov condition. This condition is stronger than the Lindeberg condition and is sufficient to guarantee it holds. Therefore, by the Lindeberg-Feller Central Limit Theorem,\n$$\n\\sqrt{C}(\\hat{\\mu}_{C} - \\mu_{S}) \\xrightarrow{d} N\\left(0, \\sum_{h=1}^{H} \\frac{c_h w_h^2 \\sigma_h^2}{b_h}\\right)\n$$\nNow, we proceed to the second part of the problem: minimizing the asymptotic variance subject to constraints on the limiting budget fractions $b_h$. We want to minimize the objective function $V(b_1, \\dots, b_H) = \\sum_{h=1}^{H} \\frac{c_h w_h^2 \\sigma_h^2}{b_h}$ subject to the constraints $\\sum_{h=1}^{H} b_h = 1$ and $b_h  0$ for all $h \\in \\{1,\\dots,H\\}$. We use the method of Lagrange multipliers. The Lagrangian is:\n$$\n\\mathcal{L}(b_1, \\dots, b_H, \\lambda) = \\sum_{h=1}^{H} \\frac{c_h w_h^2 \\sigma_h^2}{b_h} + \\lambda \\left( \\left(\\sum_{h=1}^{H} b_h\\right) - 1 \\right)\n$$\nTaking the partial derivative with respect to $b_k$ for any $k \\in \\{1, \\dots, H\\}$ and setting it to zero gives:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial b_k} = -\\frac{c_k w_k^2 \\sigma_k^2}{b_k^2} + \\lambda = 0\n$$\nThis implies $\\lambda = \\frac{c_k w_k^2 \\sigma_k^2}{b_k^2}$. From this, we solve for $b_k$:\n$$\nb_k^2 = \\frac{c_k w_k^2 \\sigma_k^2}{\\lambda} \\implies b_k = \\frac{\\sqrt{c_k} w_k \\sigma_k}{\\sqrt{\\lambda}}\n$$\nWe choose the positive root since we require $b_k  0$. We substitute this into the constraint $\\sum_{h=1}^{H} b_h = 1$:\n$$\n\\sum_{k=1}^{H} \\frac{\\sqrt{c_k} w_k \\sigma_k}{\\sqrt{\\lambda}} = 1\n$$\nSolving for $\\sqrt{\\lambda}$:\n$$\n\\frac{1}{\\sqrt{\\lambda}} \\sum_{k=1}^{H} \\sqrt{c_k} w_k \\sigma_k = 1 \\implies \\sqrt{\\lambda} = \\sum_{k=1}^{H} w_k \\sigma_k \\sqrt{c_k}\n$$\nThe optimal budget fractions $b_k^*$ are thus:\n$$\nb_k^* = \\frac{\\sqrt{c_k} w_k \\sigma_k}{\\sum_{j=1}^{H} w_j \\sigma_j \\sqrt{c_j}}\n$$\nFinally, we substitute these optimal fractions back into the expression for the asymptotic variance to find the minimum value:\n$$\nV_{\\min} = \\sum_{k=1}^{H} \\frac{c_k w_k^2 \\sigma_k^2}{b_k^*} = \\sum_{k=1}^{H} \\frac{c_k w_k^2 \\sigma_k^2}{\\frac{\\sqrt{c_k} w_k \\sigma_k}{\\sum_{j=1}^{H} w_j \\sigma_j \\sqrt{c_j}}}\n$$\nSimplifying the term inside the summation:\n$$\n\\frac{c_k w_k^2 \\sigma_k^2}{\\frac{\\sqrt{c_k} w_k \\sigma_k}{\\sum_{j=1}^{H} w_j \\sigma_j \\sqrt{c_j}}} = (c_k w_k^2 \\sigma_k^2) \\left(\\frac{1}{\\sqrt{c_k} w_k \\sigma_k}\\right) \\left(\\sum_{j=1}^{H} w_j \\sigma_j \\sqrt{c_j}\\right) = (\\sqrt{c_k} w_k \\sigma_k) \\left(\\sum_{j=1}^{H} w_j \\sigma_j \\sqrt{c_j}\\right)\n$$\nSubstituting this back into the sum for $V_{\\min}$:\n$$\nV_{\\min} = \\sum_{k=1}^{H} \\left( (\\sqrt{c_k} w_k \\sigma_k) \\left(\\sum_{j=1}^{H} w_j \\sigma_j \\sqrt{c_j}\\right) \\right)\n$$\nThe term $\\sum_{j=1}^{H} w_j \\sigma_j \\sqrt{c_j}$ is a constant with respect to the summation index $k$, so we can factor it out:\n$$\nV_{\\min} = \\left(\\sum_{j=1}^{H} w_j \\sigma_j \\sqrt{c_j}\\right) \\left(\\sum_{k=1}^{H} \\sqrt{c_k} w_k \\sigma_k\\right)\n$$\nThe two sums are identical, so we obtain the final expression for the minimal asymptotic variance:\n$$\nV_{\\min} = \\left(\\sum_{h=1}^{H} w_h \\sigma_h \\sqrt{c_h}\\right)^2\n$$\nThe convexity of the objective function $V(b_1, \\dots, b_H)$ on the domain where $b_h  0$ for all $h$ ensures that this stationary point is indeed a global minimum.",
            "answer": "$$\n\\boxed{\\left(\\sum_{h=1}^{H} w_h \\sigma_h \\sqrt{c_h}\\right)^{2}}\n$$"
        },
        {
            "introduction": "Applying limit theorems effectively requires understanding not only their power but also their limitations. This practice  delves into the challenging domain of rare-event simulation, where the probability $p$ of interest is extremely small. In such regimes, the distribution of the standard estimator $\\hat{p}_n$ can be highly skewed, causing standard Central Limit Theorem-based confidence intervals to fail dramatically. This exercise challenges you to critically compare different theoretical tools—from conservative Chebyshev bounds to variance-stabilizing transformations—to diagnose these failures and construct more reliable and robust interval estimates.",
            "id": "3317804",
            "problem": "Consider a Monte Carlo simulation for estimating a rare-event probability $p = \\mathbb{P}\\{X \\in A\\}$, where $(X_i)_{i=1}^n$ are independent and identically distributed replicates of $X$ and $\\hat p_n = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{1}\\{X_i \\in A\\}$. The estimator $\\hat p_n$ is the empirical proportion of the event $\\{X_i \\in A\\}$, taking values in $[0,1]$. You are interested in interval estimation and approximate normality properties for $\\hat p_n$ and transformations thereof in the regime where $p$ is very small (rare-event simulation). Your analysis must be grounded in core probabilistic facts and limit theorems, including the Weak Law of Large Numbers (WLLN), the Strong Law of Large Numbers (SLLN), the Central Limit Theorem (CLT), Chebyshev’s inequality, and the Delta method. The Berry-Esseen theorem may be invoked to quantify normal approximation error.\n\nSelect all statements that are correct:\n\nA. By Chebyshev’s inequality and the bound $\\operatorname{Var}(\\hat p_n) \\le \\frac{1}{4n}$ for any $p \\in (0,1)$, the interval $\\left[\\hat p_n - \\sqrt{\\frac{1}{4\\alpha n}}\\,,\\, \\hat p_n + \\sqrt{\\frac{1}{4\\alpha n}}\\right]$ has coverage at least $1-\\alpha$ for all $p$, although it can be extremely conservative in the rare-event regime.\n\nB. The Central Limit Theorem-based “Wald” interval $\\left[\\hat p_n \\pm z_{1-\\alpha/2}\\sqrt{\\frac{\\hat p_n(1-\\hat p_n)}{n}}\\right]$ always attains at least nominal coverage when $p$ is small, because the CLT guarantees asymptotic normality regardless of $p$.\n\nC. For Bernoulli sampling, the normal approximation error for the standardized empirical proportion is governed by the Berry-Esseen bound; in the rare-event regime, the bound scales like $\\mathcal{O}\\!\\left(\\frac{1}{\\sqrt{np}}\\right)$, so achieving a fixed approximation error requires $np \\to \\infty$.\n\nD. A variance-stabilizing transformation for binomial proportions is $g(p) = 2\\arcsin\\sqrt{p}$. Under independent Bernoulli sampling, $\\sqrt{n}\\,\\big(g(\\hat p_n) - g(p)\\big) \\Rightarrow \\mathcal{N}(0,1)$, yielding an asymptotic variance that is constant in $p$.\n\nE. The logit transformation $g(p) = \\log\\!\\left(\\frac{p}{1-p}\\right)$ is exactly variance-stabilizing for binomial proportions, so intervals on the logit scale have constant asymptotic variance independent of $p$.\n\nF. To address boundary issues and skewness in the rare-event regime for logit-based intervals, replacing $\\hat p_n$ by the smoothed estimator $\\tilde p_n = \\frac{S + 1/2}{n + 1}$, where $S = \\sum_{i=1}^n \\mathbb{1}\\{X_i \\in A\\}$, ensures the logit exists and, via the Delta method, yields an asymptotically normal estimator with approximate variance $\\frac{1}{n\\,\\tilde p_n(1-\\tilde p_n)}$, improving robustness for small $p$ compared to the unsmoothed logit of $\\hat p_n$.",
            "solution": "The problem statement is a valid exercise in the application of probabilistic limit theorems to the statistical analysis of Monte Carlo estimators for rare-event probabilities. The setup is standard, well-defined, and scientifically grounded in the fields of probability, statistics, and simulation. I will proceed to derive the correct answer by analyzing each statement.\n\nThe problem considers a sequence of independent and identically distributed (i.i.d.) Bernoulli random variables $Y_i = \\mathbb{1}\\{X_i \\in A\\}$, for $i=1, \\dots, n$. These variables have a common mean $\\mathbb{E}[Y_i] = p$ and variance $\\operatorname{Var}(Y_i) = p(1-p)$. The estimator for $p$ is the sample mean $\\hat p_n = \\frac{1}{n}\\sum_{i=1}^n Y_i$. This estimator is unbiased, with $\\mathbb{E}[\\hat p_n] = p$, and its variance is $\\operatorname{Var}(\\hat p_n) = \\frac{p(1-p)}{n}$.\n\n_Analysis of Option A:_\nThis statement concerns a confidence interval for $p$ constructed using Chebyshev’s inequality. Chebyshev's inequality for the random variable $\\hat p_n$ states that for any $\\epsilon > 0$:\n$$ \\mathbb{P}\\{|\\hat p_n - \\mathbb{E}[\\hat p_n]| \\ge \\epsilon\\} \\le \\frac{\\operatorname{Var}(\\hat p_n)}{\\epsilon^2} $$\nSubstituting $\\mathbb{E}[\\hat p_n] = p$ and $\\operatorname{Var}(\\hat p_n) = \\frac{p(1-p)}{n}$, we have:\n$$ \\mathbb{P}\\{|\\hat p_n - p| \\ge \\epsilon\\} \\le \\frac{p(1-p)}{n\\epsilon^2} $$\nWe want to find an interval of the form $[\\hat p_n - w, \\hat p_n + w]$ such that the coverage probability, $\\mathbb{P}\\{p \\in [\\hat p_n - w, \\hat p_n + w]\\} = \\mathbb{P}\\{|\\hat p_n - p| \\le w\\}$, is at least $1-\\alpha$. This is equivalent to ensuring the probability of the complement is at most $\\alpha$: $\\mathbb{P}\\{|\\hat p_n - p| > w\\} \\le \\alpha$.\nFrom Chebyshev's inequality, we can guarantee this by setting $\\frac{p(1-p)}{nw^2} \\le \\alpha$. The variance term $p(1-p)$ is unknown. We can use its maximum value, which occurs at $p = 1/2$, yielding $p(1-p) \\le 1/4$. Using this worst-case bound, the inequality becomes:\n$$ \\frac{1/(4n)}{w^2} \\le \\alpha \\implies w^2 \\ge \\frac{1}{4n\\alpha} \\implies w \\ge \\sqrt{\\frac{1}{4n\\alpha}} $$\nThe statement proposes an interval with a half-width of $w = \\sqrt{\\frac{1}{4\\alpha n}}$. This matches the requirement derived from the worst-case Chebyshev bound. Therefore, the interval $\\left[\\hat p_n - \\sqrt{\\frac{1}{4\\alpha n}}, \\hat p_n + \\sqrt{\\frac{1}{4\\alpha n}}\\right]$ is guaranteed to have a coverage probability of at least $1-\\alpha$ for any $p \\in (0,1)$.\nIn the rare-event regime, $p$ is very small. The true variance $\\frac{p(1-p)}{n} \\approx \\frac{p}{n}$ is much smaller than the worst-case variance $\\frac{1}{4n}$. Since the interval width is based on this worst-case variance, it will be excessively wide, making it \"extremely conservative,\" as the statement correctly notes. The statement is entirely correct.\nVerdict: **Correct**.\n\n_Analysis of Option B:_\nThe Wald interval is based on the Central Limit Theorem (CLT), which states $\\frac{\\hat p_n - p}{\\sqrt{p(1-p)/n}} \\Rightarrow \\mathcal{N}(0,1)$ as $n \\to \\infty$. The interval is constructed by replacing the unknown $p$ in the standard deviation with its consistent estimator $\\hat p_n$, yielding the pivotal quantity $\\frac{\\hat p_n - p}{\\sqrt{\\hat p_n(1-\\hat p_n)/n}}$, which also converges to a standard normal distribution. The statement claims this interval \"always attains at least nominal coverage when $p$ is small\". This is demonstrably false. The CLT is an asymptotic result. For any finite $n$, the quality of the normal approximation deteriorates as $p$ approaches $0$ or $1$. When $p$ is small, the distribution of the number of successes $n\\hat p_n$ is a Binomial($n, p$) distribution, which is highly skewed to the right and poorly approximated by a symmetric normal distribution. A catastrophic failure occurs if no events are observed, i.e., $\\hat p_n = 0$. This happens with probability $(1-p)^n$. If $p$ is small, this probability can be substantial. In such a case, the Wald interval becomes $[0 \\pm z_{1-\\alpha/2}\\sqrt{0}] = [0, 0]$, a zero-width interval. This interval fails to cover the true $p > 0$, so its coverage is $0$. The claim that it \"always attains at least nominal coverage\" is false.\nVerdict: **Incorrect**.\n\n_Analysis of Option C:_\nThe Berry-Esseen theorem provides a non-asymptotic bound on the rate of convergence in the CLT. For a sum of i.i.d. random variables, the bound on the maximum error in the CDF approximation is proportional to $\\frac{\\rho_3}{\\sigma^3\\sqrt{n}}$, where $\\sigma^2$ is the variance and $\\rho_3$ is the third absolute central moment. For a Bernoulli$(p)$ random variable, $\\sigma^2 = p(1-p)$ and $\\rho_3 = \\mathbb{E}[|Y-p|^3] = p(1-p)(p^2 + (1-p)^2) = p(1-p)(1-2p+2p^2)$.\nThe Berry-Esseen bound is thus:\n$$ \\sup_{x} \\left| \\mathbb{P}\\left(\\frac{\\sqrt{n}(\\hat p_n - p)}{\\sqrt{p(1-p)}} \\le x\\right) - \\Phi(x) \\right| \\le \\frac{C \\cdot p(1-p)(1-2p+2p^2)}{(p(1-p))^{3/2}\\sqrt{n}} = \\frac{C(1-2p+2p^2)}{\\sqrt{np(1-p)}} $$\nwhere $C$ is a constant and $\\Phi$ is the standard normal CDF.\nIn the rare-event regime, $p \\to 0$. The numerator $(1-2p+2p^2)$ approaches $1$, and the denominator $\\sqrt{np(1-p)}$ approaches $\\sqrt{np}$. Thus, the bound scales as $\\mathcal{O}\\left(\\frac{1}{\\sqrt{np}}\\right)$. For this bound to be small, ensuring a good normal approximation, the term $\\sqrt{np}$ must be large, which implies $np \\to \\infty$. This is a well-known rule of thumb for the normal approximation to the binomial distribution. The statement is a correct application and interpretation of the Berry-Esseen theorem in this context.\nVerdict: **Correct**.\n\n_Analysis of Option D:_\nA variance-stabilizing transformation $g(p)$ is one for which the asymptotic variance of $\\sqrt{n}(g(\\hat p_n) - g(p))$ is constant. By the Delta method, this asymptotic variance is given by $(g'(p))^2 \\operatorname{Var}(\\sqrt{n}(\\hat p_n - p)) = (g'(p))^2 p(1-p)$. We require this quantity to be constant.\nThe proposed transformation is $g(p) = 2\\arcsin\\sqrt{p}$. Its derivative is:\n$$ g'(p) = \\frac{d}{dp}(2\\arcsin\\sqrt{p}) = 2 \\cdot \\frac{1}{\\sqrt{1-(\\sqrt{p})^2}} \\cdot \\frac{d}{dp}(\\sqrt{p}) = 2 \\cdot \\frac{1}{\\sqrt{1-p}} \\cdot \\frac{1}{2\\sqrt{p}} = \\frac{1}{\\sqrt{p(1-p)}} $$\nThe asymptotic variance of the transformed estimator is:\n$$ (g'(p))^2 p(1-p) = \\left(\\frac{1}{\\sqrt{p(1-p)}}\\right)^2 \\cdot p(1-p) = \\frac{1}{p(1-p)} \\cdot p(1-p) = 1 $$\nThe asymptotic variance is indeed the constant $1$. Thus, by the Delta method, $\\sqrt{n}\\,(g(\\hat p_n) - g(p)) \\Rightarrow \\mathcal{N}(0,1)$. The statement is entirely accurate.\nVerdict: **Correct**.\n\n_Analysis of Option E:_\nThis statement claims the logit transformation, $g(p) = \\log\\left(\\frac{p}{1-p}\\right)$, is variance-stabilizing. We use the same procedure as in D. The derivative of the logit function is:\n$$ g'(p) = \\frac{d}{dp}(\\log p - \\log(1-p)) = \\frac{1}{p} - \\frac{1}{1-p}(-1) = \\frac{1-p+p}{p(1-p)} = \\frac{1}{p(1-p)} $$\nThe asymptotic variance of the logit-transformed estimator is:\n$$ (g'(p))^2 p(1-p) = \\left(\\frac{1}{p(1-p)}\\right)^2 \\cdot p(1-p) = \\frac{1}{p(1-p)} $$\nThis variance is clearly a function of $p$ and is not constant. In the rare-event setting ($p \\to 0$), this variance explodes, as $\\frac{1}{p(1-p)} \\approx \\frac{1}{p} \\to \\infty$. Therefore, the logit transformation is not variance-stabilizing.\nVerdict: **Incorrect**.\n\n_Analysis of Option F:_\nThis statement proposes a remedy for the deficiencies of logit-based intervals, particularly the boundary issue where $\\hat p_n=0$ or $\\hat p_n=1$, making the logit undefined. The proposed smoothed estimator is $\\tilde p_n = \\frac{S+1/2}{n+1}$, where $S = n\\hat p_n$.\n1.  **Boundary Issue**: Since $0 \\le S \\le n$, we have $1/2 \\le S+1/2 \\le n+1/2$. Thus $0  \\frac{1/2}{n+1} \\le \\tilde p_n \\le \\frac{n+1/2}{n+1}  1$. $\\tilde p_n$ is always in the open interval $(0,1)$, so its logit is always well-defined. This part of the statement is correct.\n2.  **Asymptotics**: Let us examine the asymptotic behavior of $\\tilde p_n$.\n    $$ \\sqrt{n}(\\tilde p_n - p) = \\sqrt{n}\\left( \\frac{n\\hat p_n + 1/2}{n+1} - p \\right) = \\sqrt{n}\\left( \\frac{n\\hat p_n - (n+1)p + 1/2}{n+1} \\right) = \\frac{n}{n+1}\\sqrt{n}(\\hat p_n - p) + \\frac{\\sqrt{n}(-p+1/2)}{n+1} $$\n    As $n \\to \\infty$, $\\frac{n}{n+1} \\to 1$ and the second term converges to $0$. We know $\\sqrt{n}(\\hat p_n - p) \\Rightarrow \\mathcal{N}(0, p(1-p))$. By Slutsky's Theorem, $\\sqrt{n}(\\tilde p_n - p)$ has the same limiting distribution.\n3.  **Delta Method**: Applying the Delta method with $g(p)=\\log(p/(1-p))$ to $\\tilde p_n$, we find that $\\sqrt{n}(g(\\tilde p_n) - g(p)) \\Rightarrow \\mathcal{N}(0, (g'(p))^2 p(1-p))$. As calculated for option E, this limiting variance is $\\frac{1}{p(1-p)}$.\n4.  **Approximate Variance**: The variance of $g(\\tilde p_n)$ for large $n$ is thus approximately $\\frac{1}{n p(1-p)}$. To construct a confidence interval, this variance must be estimated. Since $\\tilde p_n$ is a consistent estimator of $p$, a consistent estimator for the variance is $\\frac{1}{n \\tilde p_n(1-\\tilde p_n)}$. The phrase \"approximate variance\" is a common, though slightly imprecise, way of referring to this estimated variance.\n5.  **Robustness**: This smoothing procedure is a standard technique (related to adding \"pseudo-counts\") to improve the finite-sample performance of proportion-based CIs, especially when $p$ is near the boundaries. It provides robustness against observing $S=0$ or $S=n$.\nThe statement correctly identifies a problem (boundary issues), proposes a valid solution (smoothing), and correctly describes the consequences (asymptotic normality and the form of the estimated variance) and benefits (improved robustness).\nVerdict: **Correct**.",
            "answer": "$$\\boxed{ACDF}$$"
        }
    ]
}