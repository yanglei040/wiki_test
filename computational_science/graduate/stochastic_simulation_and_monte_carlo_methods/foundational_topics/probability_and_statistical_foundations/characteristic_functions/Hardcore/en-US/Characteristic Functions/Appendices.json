{
    "hands_on_practices": [
        {
            "introduction": "A characteristic function, $\\varphi_X(t)$, provides a complete description of a random variable's distribution in the frequency domain. This first practice explores the fundamental task of recovering a discrete probability mass function (PMF) from its characteristic function using the discrete Fourier inversion formula. By implementing this inversion with the Fast Fourier Transform (FFT), you will gain a hands-on understanding of aliasing—a critical concept in numerical analysis that appears when a continuous function is sampled on a finite grid .",
            "id": "3293797",
            "problem": "Consider a real-valued random variable $X$ that is supported on the integers, with probability mass function $p_k = \\mathbb{P}\\{X = k\\}$ for $k \\in \\mathbb{Z}$ and characteristic function $\\varphi_X(t) = \\mathbb{E}\\left[e^{\\mathrm{i} t X}\\right]$. Starting from the fundamental definitions of the characteristic function and the orthogonality of complex exponentials on a single period, derive an explicit integral representation of $p_k$ in terms of $\\varphi_X(t)$ over one period of length $2\\pi$ (angles measured in radians). The derivation must proceed from first principles: linearity of expectation, absolute convergence of $\\sum_{k \\in \\mathbb{Z}} p_k e^{\\mathrm{i} t k}$ for all real $t$, and the orthogonality of the complex exponential basis on a finite interval.\n\nThen, analyze the impact of approximating the inversion integral by the trapezoidal rule on a uniform grid of $N$ points. Show, by algebraic manipulation using only discrete sums and the periodicity of $\\varphi_X(t)$, that the resulting numerical inversion coincides with an inverse discrete Fourier transform and produces a periodization of the probability mass function modulo $N$. Explain why this leads to aliasing, and characterize the aliasing explicitly in terms of which values of $p_k$ are superposed in the numerical inversion when $X$ has unbounded support.\n\nImplementation task. Write a complete, runnable program that:\n- Samples $\\varphi_X(t)$ on an equispaced grid $t_j = \\frac{2\\pi j}{N}$, for $j \\in \\{0,1,\\dots,N-1\\}$ (angles in radians).\n- Computes the numerical inversion by the inverse discrete Fourier transform induced by the trapezoidal rule, yielding estimates $\\widehat{p}_k$ for $k \\in \\{0,1,\\dots,N-1\\}$, and for symmetric integer ranges by centering via a standard shift of the discrete spectrum.\n- Computes the true $p_k$ for each distribution under test on the same index range as the numerical inversion, using closed-form formulas and well-tested special functions.\n- Reports an aliasing error metric defined as the maximum absolute difference $\\max_k \\left|\\widehat{p}_k - p_k\\right|$ over the index range compared for each test case.\n\nUse the following test suite of parameter values to ensure coverage of compact support, light tails, heavy tails, and bidirectional infinite support:\n- Test $1$ (happy path, compact support): Binomial distribution with parameters $n = 20$ and $p = 0.3$, and grid size $N = 64$. Here, $p_k = \\binom{n}{k} p^k (1-p)^{n-k}$ for $k \\in \\{0,1,\\dots,n\\}$ and $p_k = 0$ otherwise; $\\varphi_X(t)$ corresponds to the characteristic function of the Binomial distribution.\n- Test $2$ (light-tailed infinite support): Poisson distribution with parameter $\\lambda = 8$ and grid size $N = 64$. Here, $p_k = e^{-\\lambda} \\frac{\\lambda^k}{k!}$ for $k \\in \\mathbb{Z}_{\\ge 0}$ and $p_k = 0$ otherwise; $\\varphi_X(t)$ corresponds to the characteristic function of the Poisson distribution.\n- Test $3$ (heavier-tailed infinite support leading to aliasing): Poisson distribution with parameter $\\lambda = 30$ and grid size $N = 64$.\n- Test $4$ (bidirectional infinite support with aliasing on a small grid): Skellam distribution with parameters $\\lambda_1 = 20$ and $\\lambda_2 = 10$ on grid size $N = 64$. Here, $p_k = e^{-(\\lambda_1+\\lambda_2)} \\left(\\frac{\\lambda_1}{\\lambda_2}\\right)^{k/2} I_{|k|}\\!\\left(2\\sqrt{\\lambda_1 \\lambda_2}\\right)$ for $k \\in \\mathbb{Z}$, where $I_\\nu(\\cdot)$ is the modified Bessel function of the first kind; $\\varphi_X(t)$ corresponds to the characteristic function of the Skellam distribution.\n- Test $5$ (same bidirectional infinite support, larger grid to reduce aliasing): Skellam distribution with parameters $\\lambda_1 = 20$ and $\\lambda_2 = 10$ on grid size $N = 512$.\n\nFor Tests $1$–$3$, compare on the index range $k \\in \\{0,1,\\dots,N-1\\}$; for Tests $4$–$5$, compare on the centered index range $k \\in \\{-\\frac{N}{2}, -\\frac{N}{2}+1, \\dots, \\frac{N}{2}-1\\}$.\n\nYour program should produce a single line of output containing the aliasing error metrics for Tests $1$–$5$ as a comma-separated list of floating-point numbers enclosed in square brackets, for example, $[a_1,a_2,a_3,a_4,a_5]$ where each $a_i$ is the maximum absolute difference defined above. No other output is permitted.",
            "solution": "The user-provided problem is a well-posed and scientifically sound exercise in probability theory and numerical methods. It asks for a theoretical derivation of the inversion formula for the characteristic function of a discrete random variable, an analysis of its numerical approximation, and an implementation to quantify the resulting aliasing error for several standard distributions. The problem is self-contained and free of contradictions or ambiguities.\n\n### Part 1: Derivation of the Inversion Formula\n\nLet $X$ be a random variable that takes values in the set of integers $\\mathbb{Z}$, with probability mass function (PMF) $p_k = \\mathbb{P}\\{X=k\\}$ for $k \\in \\mathbb{Z}$. The characteristic function (CF) of $X$, denoted by $\\varphi_X(t)$, is defined as the expectation of $e^{\\mathrm{i}tX}$:\n$$\n\\varphi_X(t) = \\mathbb{E}\\left[e^{\\mathrm{i}tX}\\right]\n$$\nBy the law of the unconscious statistician, for a discrete random variable this becomes a sum over all possible values weighted by their probabilities:\n$$\n\\varphi_X(t) = \\sum_{m \\in \\mathbb{Z}} p_m e^{\\mathrm{i}tm}\n$$\nThe problem states that this series converges absolutely for all real $t$, which is guaranteed since $|p_m e^{\\mathrm{i}tm}| = p_m$ and $\\sum_{m \\in \\mathbb{Z}} p_m = 1$. The function $\\varphi_X(t)$ is periodic with period $2\\pi$ because $e^{\\mathrm{i}(t+2\\pi)m} = e^{\\mathrm{i}tm}e^{\\mathrm{i}2\\pi m} = e^{\\mathrm{i}tm}$, as $m$ is an integer.\n\nOur goal is to derive an explicit representation for a specific probability $p_k$ by inverting this relationship. This is accomplished by exploiting the orthogonality of the complex exponential basis functions $\\{e^{\\mathrm{i}nt}\\}_{n \\in \\mathbb{Z}}$ on any interval of length $2\\pi$, such as $[-\\pi, \\pi]$. Let's consider the integral of $\\varphi_X(t)e^{-\\mathrm{i}tk}$ over this interval, scaled by $\\frac{1}{2\\pi}$:\n$$\n\\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} \\varphi_X(t) e^{-\\mathrm{i}tk} dt\n$$\nSubstituting the series definition of $\\varphi_X(t)$ into the integral gives:\n$$\n\\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} \\left( \\sum_{m \\in \\mathbb{Z}} p_m e^{\\mathrm{i}tm} \\right) e^{-\\mathrm{i}tk} dt\n$$\nSince the series $\\sum p_m e^{\\mathrm{i}tm}$ converges absolutely and uniformly, we can interchange the order of integration and summation (by the Fubini-Tonelli theorem or dominated convergence):\n$$\n\\sum_{m \\in \\mathbb{Z}} p_m \\left( \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} e^{\\mathrm{i}t(m-k)} dt \\right)\n$$\nThe integral inside the summation is the crucial part. Let $n = m-k$, which is an integer. We evaluate the integral $\\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} e^{\\mathrm{i}tn} dt$.\n- If $n = 0$ (i.e., $m=k$), the integrand is $e^0 = 1$. The integral becomes $\\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} 1 \\, dt = \\frac{1}{2\\pi} (2\\pi) = 1$.\n- If $n \\neq 0$, the integral is $\\frac{1}{2\\pi} \\left[ \\frac{e^{\\mathrm{i}tn}}{\\mathrm{i}n} \\right]_{-\\pi}^{\\pi} = \\frac{1}{2\\pi\\mathrm{i}n} (e^{\\mathrm{i}\\pi n} - e^{-\\mathrm{i}\\pi n})$. Using Euler's formula $e^{\\mathrm{i}\\theta} - e^{-\\mathrm{i}\\theta} = 2\\mathrm{i}\\sin(\\theta)$, this becomes $\\frac{2\\mathrm{i}\\sin(\\pi n)}{2\\pi\\mathrm{i}n} = \\frac{\\sin(\\pi n)}{\\pi n}$. Since $n$ is a non-zero integer, $\\sin(\\pi n) = 0$, so the integral is $0$.\n\nThe term inside the parentheses is the Kronecker delta, $\\delta_{mk}$. The sum thus collapses, as only the term where $m=k$ survives:\n$$\n\\sum_{m \\in \\mathbb{Z}} p_m \\delta_{mk} = p_k \\cdot 1 + \\sum_{m \\neq k} p_m \\cdot 0 = p_k\n$$\nThis completes the derivation. The inversion formula for the probability mass function is:\n$$\np_k = \\frac{1}{2\\pi} \\int_{-\\pi}^{\\pi} \\varphi_X(t) e^{-\\mathrm{i}tk} dt\n$$\nDue to the $2\\pi$-periodicity of the integrand, the integration can be performed over any interval of length $2\\pi$, such as $[0, 2\\pi]$.\n\n### Part 2: Numerical Inversion and Aliasing\n\nWe now analyze the approximation of the inversion integral using the trapezoidal rule. We use the interval $[0, 2\\pi]$ and a uniform grid of $N$ points $t_j = \\frac{2\\pi j}{N}$ for $j \\in \\{0, 1, \\dots, N-1\\}$. The step size is $\\Delta t = \\frac{2\\pi}{N}$.\n\nFor a periodic function integrated over one period, the trapezoidal rule simplifies to a simple rectangular rule sum:\n$$\n\\int_{0}^{2\\pi} f(t) dt \\approx \\sum_{j=0}^{N-1} f(t_j) \\Delta t\n$$\nApplying this to the inversion integral for $p_k$:\n$$\np_k = \\frac{1}{2\\pi} \\int_{0}^{2\\pi} \\varphi_X(t) e^{-\\mathrm{i}tk} dt \\approx \\frac{1}{2\\pi} \\sum_{j=0}^{N-1} \\varphi_X(t_j) e^{-\\mathrm{i}t_j k} \\Delta t\n$$\nLet's denote the numerical estimate by $\\widehat{p}_k$. Substituting $\\Delta t = \\frac{2\\pi}{N}$ and $t_j = \\frac{2\\pi j}{N}$:\n$$\n\\widehat{p}_k = \\frac{1}{2\\pi} \\sum_{j=0}^{N-1} \\varphi_X\\left(\\frac{2\\pi j}{N}\\right) e^{-\\mathrm{i}k \\frac{2\\pi j}{N}} \\left(\\frac{2\\pi}{N}\\right) = \\frac{1}{N} \\sum_{j=0}^{N-1} \\varphi_X\\left(t_j\\right) e^{-\\mathrm{i}\\frac{2\\pi jk}{N}}\n$$\nLet $\\Phi_j = \\varphi_X(t_j)$ be the samples of the characteristic function. The formula for the numerical estimate $\\widehat{p}_k$ is:\n$$\n\\widehat{p}_k = \\frac{1}{N} \\sum_{j=0}^{N-1} \\Phi_j e^{-\\mathrm{i}\\frac{2\\pi jk}{N}}\n$$\nThis expression is proportional to the Discrete Fourier Transform (DFT) of the sequence $\\{\\Phi_j\\}$. Specifically, if the DFT is defined as $Y_k = \\sum_{j=0}^{N-1} y_j e^{-\\mathrm{i}2\\pi jk / N}$, then $\\widehat{p}_k = \\frac{1}{N} \\text{DFT}(\\{\\Phi_j\\})_k$. This structure is commonly implemented in fast Fourier transform (FFT) algorithms.\n\nTo understand the error in this approximation, we substitute the definition of $\\varphi_X(t_j)$ back into the formula for $\\widehat{p}_k$:\n$$\n\\widehat{p}_k = \\frac{1}{N} \\sum_{j=0}^{N-1} \\left( \\sum_{m \\in \\mathbb{Z}} p_m e^{\\mathrm{i}t_j m} \\right) e^{-\\mathrm{i}t_j k} = \\frac{1}{N} \\sum_{j=0}^{N-1} \\sum_{m \\in \\mathbb{Z}} p_m e^{\\mathrm{i}\\frac{2\\pi j(m-k)}{N}}\n$$\nSwapping the finite and infinite sums:\n$$\n\\widehat{p}_k = \\sum_{m \\in \\mathbb{Z}} p_m \\left( \\frac{1}{N} \\sum_{j=0}^{N-1} e^{\\mathrm{i}\\frac{2\\pi j(m-k)}{N}} \\right)\n$$\nThe inner sum is a geometric series of roots of unity. This sum is equal to $N$ if $m-k$ is an integer multiple of $N$ (i.e., $m-k = qN$ for some integer $q$, or $m \\equiv k \\pmod N$), and $0$ otherwise. Therefore, the term in parentheses is $1$ if $m \\equiv k \\pmod N$ and $0$ otherwise. The outer sum over all integers $m$ only receives contributions from those $m$ that are congruent to $k$ modulo $N$:\n$$\n\\widehat{p}_k = \\sum_{m \\in \\mathbb{Z} : m \\equiv k \\pmod N} p_m = \\sum_{q \\in \\mathbb{Z}} p_{k+qN}\n$$\nThis is the characterization of the aliasing error. The numerical estimate $\\widehat{p}_k$ is not equal to $p_k$, but is the sum of $p_k$ and the probabilities of all its \"aliases\" at indices $k \\pm N$, $k \\pm 2N$, and so on. This effect is known as periodization.\n$$\n\\widehat{p}_k = \\dots + p_{k-2N} + p_{k-N} + p_k + p_{k+N} + p_{k+2N} + \\dots\n$$\nThe aliasing error is precisely the sum of the aliased terms: $\\widehat{p}_k - p_k = \\sum_{q \\in \\mathbb{Z}, q\\neq 0} p_{k+qN}$. For a distribution with compact support on $\\{0, 1, \\dots, n\\}$, if we choose $N > n$, then for any $k \\in \\{0, \\dots, n\\}$, all terms $p_{k+qN}$ with $q \\neq 0$ are zero, so there is no aliasing error and $\\widehat{p}_k=p_k$. For distributions with infinite support (e.g., Poisson, Skellam), there will always be some aliasing. The error's magnitude depends on how fast the PMF's tails decay. Increasing $N$ makes the aliased copies $p_{k \\pm N}, \\dots$ further away, and since probabilities in the tails are smaller, the aliasing error diminishes.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import binom, gammaln, iv\nfrom scipy.stats import binom as binom_dist, poisson as poisson_dist\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating aliasing errors for five test cases.\n    \"\"\"\n\n    # Define characteristic function implementations\n    def cf_binomial(t, n, p):\n        return (1 - p + p * np.exp(1j * t))**n\n\n    def cf_poisson(t, lam):\n        return np.exp(lam * (np.exp(1j * t) - 1))\n\n    def cf_skellam(t, lam1, lam2):\n        return np.exp(lam1 * (np.exp(1j * t) - 1) + lam2 * (np.exp(-1j * t) - 1))\n\n    # Define probability mass function implementations\n    def pmf_binomial(k, n, p):\n        # Use scipy.stats for a robust implementation handling k out of support\n        return binom_dist.pmf(k, n, p)\n\n    def pmf_poisson(k, lam):\n        # Use scipy.stats for a robust implementation\n        # For k  0, pmf is 0. poisson_dist.pmf handles this.\n        return poisson_dist.pmf(k, lam)\n    \n    def pmf_skellam(k, lam1, lam2):\n        # Use log-domain calculations for numerical stability, though direct\n        # computation is fine for these parameters.\n        # I_v(z) can grow very large, but the exponential term keeps it in check.\n        term1 = -(lam1 + lam2)\n        term2 = (k / 2.0) * (np.log(lam1) - np.log(lam2))\n        # Use scipy.special.iv for the modified Bessel function of the first kind\n        bessel_term = iv(np.abs(k), 2 * np.sqrt(lam1 * lam2))\n        \n        # Combine terms, handling potential log(0) for bessel_term\n        # Since bessel_term is always non-negative, we can safely take log\n        # if we handle the case where it is zero. \n        # For integer order, iv(v,z) is zero only if z=0, which is not the case here.\n        log_bessel = np.log(bessel_term)\n        return np.exp(term1 + term2 + log_bessel)\n\n    def calculate_aliasing_error(dist_params):\n        \"\"\"\n        Calculates the max absolute aliasing error for a given distribution and parameters.\n        \"\"\"\n        dist_name = dist_params['name']\n        params = dist_params['params']\n        N = dist_params['N']\n        k_range_type = dist_params['k_range_type']\n\n        # Step 1: Set up grids\n        t_grid = 2 * np.pi * np.arange(N) / N\n        \n        if k_range_type == 'non-negative':\n            k_indices = np.arange(N)\n        elif k_range_type == 'centered':\n            k_indices = np.arange(-N // 2, N // 2)\n        else:\n            raise ValueError(\"Invalid k_range_type\")\n\n        # Step 2: Sample the characteristic function\n        if dist_name == 'binomial':\n            cf_samples = cf_binomial(t_grid, n=params['n'], p=params['p'])\n        elif dist_name == 'poisson':\n            cf_samples = cf_poisson(t_grid, lam=params['lambda'])\n        elif dist_name == 'skellam':\n            cf_samples = cf_skellam(t_grid, lam1=params['lambda1'], lam2=params['lambda2'])\n        \n        # Step 3: Compute numerical inversion via FFT\n        # The derived formula is p_hat_k = (1/N) * DFT(phi), where DFT has e^(-i...).\n        # This matches (1/N) * numpy.fft.fft.\n        p_hat_raw = (1 / N) * np.fft.fft(cf_samples)\n        \n        # Adjust indices for centered range\n        if k_range_type == 'centered':\n            p_hat_k = np.fft.fftshift(p_hat_raw)\n        else:\n            p_hat_k = p_hat_raw\n            \n        # The result should be real; take the real part to discard numerical noise.\n        p_hat_k = np.real(p_hat_k)\n        \n        # Step 4: Compute the true PMF\n        if dist_name == 'binomial':\n            p_k = pmf_binomial(k_indices, n=params['n'], p=params['p'])\n        elif dist_name == 'poisson':\n            p_k = pmf_poisson(k_indices, lam=params['lambda'])\n        elif dist_name == 'skellam':\n            p_k = pmf_skellam(k_indices, lam1=params['lambda1'], lam2=params['lambda2'])\n\n        # Step 5: Compute the aliasing error metric\n        max_abs_diff = np.max(np.abs(p_hat_k - p_k))\n        return max_abs_diff\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test 1: Binomial (compact support, no aliasing)\n        {'name': 'binomial', 'params': {'n': 20, 'p': 0.3}, 'N': 64, 'k_range_type': 'non-negative'},\n        # Test 2: Poisson (light tails, low aliasing)\n        {'name': 'poisson', 'params': {'lambda': 8}, 'N': 64, 'k_range_type': 'non-negative'},\n        # Test 3: Poisson (heavier tails, more aliasing)\n        {'name': 'poisson', 'params': {'lambda': 30}, 'N': 64, 'k_range_type': 'non-negative'},\n        # Test 4: Skellam (bidirectional, aliasing on small grid)\n        {'name': 'skellam', 'params': {'lambda1': 20, 'lambda2': 10}, 'N': 64, 'k_range_type': 'centered'},\n        # Test 5: Skellam (bidirectional, reduced aliasing on large grid)\n        {'name': 'skellam', 'params': {'lambda1': 20, 'lambda2': 10}, 'N': 512, 'k_range_type': 'centered'},\n    ]\n\n    results = []\n    for case in test_cases:\n        error = calculate_aliasing_error(case)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Building on the discrete case, this exercise tackles the more general problem of inverting characteristic functions for continuous distributions to recover the probability density function (PDF) . You will discover that the numerical approximation of the continuous Fourier inversion integral introduces practical challenges like integral truncation and spectral leakage. This practice guides you through implementing a robust inversion procedure and demonstrates how frequency-domain windowing techniques, such as the Blackman window, can significantly improve the accuracy of the result.",
            "id": "3293843",
            "problem": "You are tasked with designing a computational procedure for numerically inverting a characteristic function using the Fast Fourier Transform (FFT), analyzing aliasing due to periodic extension, and proposing and implementing windowing to reduce spectral leakage. Your program must be a complete, runnable implementation that, for a given set of test cases, constructs an FFT grid for inversion, computes a probability density function approximation from a characteristic function, and reports quantitative error metrics.\n\nThe fundamental base to be used is the following:\n- The characteristic function of a real-valued random variable is defined by $\\varphi_{X}(t) = \\mathbb{E}[e^{i t X}]$.\n- When a distribution admits a probability density function $f_{X}(x)$, the inversion relation (Fourier inversion) holds in the form $f_{X}(x) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} e^{-i t x} \\varphi_{X}(t) \\, dt$, with the understanding that the integral is taken in a suitable sense when necessary.\n- The Discrete Fourier Transform (DFT) paired with a regular grid obeys the sampling relation that links spatial spacing $\\Delta x$ and frequency spacing $\\Delta t$ via $\\Delta x \\, \\Delta t = \\frac{2\\pi}{N}$ for an $N$-point FFT when using a consistent discretization of the inversion integral.\n- Truncating the Fourier integral to a finite frequency band and sampling at a finite rate implies a periodic extension of the recovered density in the spatial domain, with period $L = N \\Delta x$, causing aliasing of out-of-interval mass back into the fundamental cell. Multiplying the sampled characteristic function by a smooth window that tapers at the ends reduces spectral leakage into the spatial domain but introduces smoothing bias.\n\nYour algorithmic design must:\n1. Construct an evenly spaced spatial grid $\\{x_{j}\\}_{j=0}^{N-1}$ of length $L = N \\Delta x$, centered so that $x_{0} = -L/2$ and $x_{j} = x_{0} + j \\Delta x$; and a dual frequency grid $\\{t_{k}\\}_{k=0}^{N-1}$ with spacing $\\Delta t = \\frac{2\\pi}{L}$ and $t_{k} = (k - N/2)\\Delta t$.\n2. Approximate the inversion integral by a Riemann sum matched to the DFT, so that you can use an $N$-point FFT to compute \n$$\nf_{X}(x_{j}) \\approx \\frac{\\Delta t}{2\\pi} \\sum_{k=0}^{N-1} \\varphi_{X}(t_{k}) \\, e^{-i t_{k} x_{j}}.\n$$\n3. Analyze and reflect the aliasing due to $L$-periodic extension induced by the discrete transform, i.e., the recovered $f_{X}$ is effectively convolved with a Dirichlet kernel and periodized with period $L$. Control aliasing by selecting $L$ sufficiently large for the target distribution and by applying a taper (window) $w_{k}$ in the frequency domain to reduce leakage from the finite band.\n4. Implement at least two windows in the frequency domain: a rectangular window $w_{k} \\equiv 1$ (no taper) and a smooth taper such as the Blackman window. The window multiplies the sampled characteristic function before the FFT.\n5. Compare the recovered density against the exact analytic density on a specified evaluation interval strictly inside the fundamental cell to quantify the maximum absolute error.\n\nDistributions and their characteristic functions and densities to be used for test cases:\n- Normal distribution with mean $\\mu$ and standard deviation $\\sigma$: $\\varphi(t) = \\exp\\!\\left(i \\mu t - \\frac{1}{2}\\sigma^{2} t^{2}\\right)$ and $f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}\\right)$.\n- Laplace (double exponential) distribution with location $\\mu$ and scale $b$: $\\varphi(t) = \\frac{e^{i \\mu t}}{1 + b^{2} t^{2}}$ and $f(x) = \\frac{1}{2b} \\exp\\!\\left(-\\frac{|x - \\mu|}{b}\\right)$.\n- Cauchy distribution with location $\\mu$ and scale $\\gamma$: $\\varphi(t) = \\exp\\!\\left(i \\mu t - \\gamma |t|\\right)$ and $f(x) = \\frac{1}{\\pi} \\frac{\\gamma}{\\gamma^{2} + (x - \\mu)^{2}}$.\n\nTest suite:\nFor each test case, build the FFT inversion with the given parameters, recover the approximate density on the grid, and report the maximum absolute error between the approximate density and the exact density over the evaluation set $\\{x \\in [-x_{\\mathrm{eval}}, x_{\\mathrm{eval}}]\\}$, where $x_{\\mathrm{eval}}$ is given per case.\n\nProvide results for each of the following parameter sets:\n- Case 1 (happy path, light tails, large grid): Normal with $\\mu = 0$, $\\sigma = 1$, $N = 4096$, $L = 50$, rectangular window, evaluate on $[-6, 6]$.\n- Case 2 (coarse grid, aliasing/leakage more visible): Normal with $\\mu = 0$, $\\sigma = 1$, $N = 512$, $L = 20$, rectangular window, evaluate on $[-6, 6]$.\n- Case 3 (same as Case 2 but with windowing): Normal with $\\mu = 0$, $\\sigma = 1$, $N = 512$, $L = 20$, Blackman window, evaluate on $[-6, 6]$.\n- Case 4 (heavier tails than normal, moderate grid): Laplace with $\\mu = 0$, $b = 1$, $N = 1024$, $L = 20$, rectangular window, evaluate on $[-6, 6]$.\n- Case 5 (Laplace with windowing): Laplace with $\\mu = 0$, $b = 1$, $N = 1024$, $L = 20$, Blackman window, evaluate on $[-6, 6]$.\n- Case 6 (heavy tails, aliasing challenge): Cauchy with $\\mu = 0$, $\\gamma = 1$, $N = 2048$, $L = 40$, rectangular window, evaluate on $[-6, 6]$.\n- Case 7 (Cauchy with windowing): Cauchy with $\\mu = 0$, $\\gamma = 1$, $N = 2048$, $L = 40$, Blackman window, evaluate on $[-6, 6]$.\n- Case 8 (boundary sensitivity check, very small domain): Normal with $\\mu = 0$, $\\sigma = 1$, $N = 256$, $L = 8$, rectangular window, evaluate on $[-3, 3]$.\n\nAll angles are in radians. There are no physical units involved. For each case, the program must compute and return a single real number: the maximum absolute error between the numerically recovered density and the exact density on the specified evaluation interval.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,...,result8]\"). Each result must be a floating-point number computed by your program for the corresponding case in the order given above. No additional text should be printed.",
            "solution": "The problem R-0391 is scientifically grounded, well-posed, and objective. All necessary parameters, formulas, and evaluation criteria are provided. The problem is a standard exercise in numerical Fourier analysis applied to probability theory and is free of any scientific or logical flaws. The task is to numerically invert a characteristic function using the Fast Fourier Transform (FFT), a well-established technique. The problem is therefore deemed **valid**.\n\nThe solution to this problem involves the numerical approximation of the Fourier inversion integral that recovers a probability density function (PDF), $f_X(x)$, from its characteristic function (CF), $\\varphi_X(t)$. The core principle is to discretize the integral and map the resulting sum to a computationally efficient Discrete Fourier Transform (DFT), implemented via the Fast Fourier Transform (FFT) algorithm.\n\nThe relationship between the PDF and the CF is given by the Fourier inversion theorem:\n$$\nf_X(x) = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} e^{-i t x} \\varphi_X(t) \\, dt\n$$\nTo evaluate this numerically, we truncate the integral to a finite domain $[t_{\\min}, t_{\\max}]$ and approximate it with a Riemann sum over a discrete grid of frequencies. The problem specifies a frequency grid $\\{t_k\\}_{k=0}^{N-1}$ and a corresponding spatial grid $\\{x_j\\}_{j=0}^{N-1}$.\n\nThe grids are defined as follows:\n- The spatial domain has length $L$. The grid spacing is $\\Delta x = L/N$. The grid points are $x_j = -L/2 + j \\Delta x$ for $j \\in \\{0, 1, \\dots, N-1\\}$.\n- The frequency domain spacing is linked by the DFT sampling relation, $\\Delta t = 2\\pi / L = 2\\pi / (N \\Delta x)$. The grid points are $t_k = (k - N/2) \\Delta t$ for $k \\in \\{0, 1, \\dots, N-1\\}$.\n\nUsing these grids, the inversion integral is approximated by the sum:\n$$\nf_X(x_j) \\approx \\frac{\\Delta t}{2\\pi} \\sum_{k=0}^{N-1} \\varphi_X(t_k) \\, e^{-i t_k x_j}\n$$\nThe primary challenge is to compute this sum efficiently for all $j=0, \\dots, N-1$. A direct summation would have a complexity of $O(N^2)$, which is inefficient for large $N$. The structure of the sum, however, is amenable to the FFT algorithm, which has a complexity of $O(N \\log N)$.\n\nTo map the sum to the standard definition of a DFT, which is $G_j = \\sum_{k=0}^{N-1} g_k e^{-i 2\\pi jk/N}$, we must analyze the phase term $e^{-i t_k x_j}$. Substituting the grid definitions:\n$$\nt_k x_j = \\left(k - \\frac{N}{2}\\right)\\Delta t \\left(j \\Delta x - \\frac{L}{2}\\right)\n$$\nUsing $\\Delta x = L/N$ and $\\Delta t = 2\\pi/L$:\n$$\n\\begin{align*}\nt_k x_j = \\left(k - \\frac{N}{2}\\right)\\frac{2\\pi}{L} \\left(j \\frac{L}{N} - \\frac{L}{2}\\right) \\\\\n= \\left(k - \\frac{N}{2}\\right)\\frac{2\\pi}{N} \\left(j - \\frac{N}{2}\\right) \\\\\n= \\frac{2\\pi}{N} \\left(kj - \\frac{kN}{2} - \\frac{jN}{2} + \\frac{N^2}{4}\\right) \\\\\n= \\frac{2\\pi kj}{N} - k\\pi - j\\pi + \\frac{N\\pi}{2}\n\\end{align*}\n$$\nThe exponential of the phase term is therefore:\n$$\ne^{-i t_k x_j} = e^{-i (2\\pi kj/N - k\\pi - j\\pi + N\\pi/2)} = e^{-i 2\\pi kj/N} e^{ik\\pi} e^{ij\\pi} e^{-iN\\pi/2}\n$$\nSince $e^{i\\pi} = -1$, this simplifies to:\n$$\ne^{-i t_k x_j} = (-1)^k (-1)^j e^{-iN\\pi/2} e^{-i 2\\pi kj/N}\n$$\nThe test cases use values of $N$ that are powers of two and thus multiples of 4 (e.g., $256, 512, \\dots, 4096$). For such $N$, $N/2$ is an even integer, so $e^{-iN\\pi/2} = \\cos(-N\\pi/2) + i\\sin(-N\\pi/2) = 1$. The phase term simplifies further to $e^{-i t_k x_j} = (-1)^k (-1)^j e^{-i 2\\pi kj/N}$.\n\nSubstituting this back into the sum, we get:\n$$\nf_X(x_j) \\approx \\frac{\\Delta t}{2\\pi} \\sum_{k=0}^{N-1} \\varphi_X(t_k) \\, (-1)^k (-1)^j e^{-i 2\\pi kj/N}\n$$\nThe term $(-1)^j$ can be moved outside the summation. Defining a new sequence $Y_k = (-1)^k \\varphi_X(t_k)$, the sum becomes:\n$$\nf_X(x_j) \\approx \\frac{\\Delta t}{2\\pi} (-1)^j \\sum_{k=0}^{N-1} Y_k \\, e^{-i 2\\pi kj/N} = \\frac{1}{L} (-1)^j \\cdot \\text{FFT}(Y)_j\n$$\nThis provides an efficient algorithm:\n1. Construct the frequency grid $\\{t_k\\}$.\n2. Evaluate the characteristic function $\\varphi_X(t_k)$ on this grid.\n3. Apply a window function $w_k$ if required: $\\varphi_{X,w}(t_k) = w_k \\varphi_X(t_k)$.\n4. Create the sequence for the FFT: $Y_k = (-1)^k \\varphi_{X,w}(t_k)$.\n5. Compute the FFT of $Y_k$ to get $\\hat{Y}_j = \\text{FFT}(Y)_j$.\n6. Compute the final approximation $f_X(x_j) \\approx \\frac{1}{L} \\text{Re}[(-1)^j \\hat{Y}_j]$. We take the real part as the PDF is real-valued, and any residual imaginary component is due to numerical approximation errors.\n\nThe discretization process introduces two main types of errors. First, truncating the integral to a finite frequency domain $[-t_{\\max}, t_{\\max}]$ (where $t_{\\max} \\approx N/2 \\cdot \\Delta t$) is equivalent to convolving the true PDF with a sinc-like kernel, causing ringing artifacts (Gibbs phenomenon or spectral leakage). Applying a smooth window function, such as the Blackman window, tapers the integrand $\\varphi_X(t)$ smoothly to zero at the truncation boundaries. This significantly reduces leakage at the cost of slightly broadening the main features of the recovered PDF (a bias-smoothing trade-off).\n\nSecond, sampling the frequency domain at a finite rate $\\Delta t$ causes the recovered spatial function to be periodic with period $L=2\\pi/\\Delta t$. If the true PDF $f_X(x)$ has significant mass outside the interval $[-L/2, L/2]$, this out-of-interval mass \"aliases\" back into the interval, distorting the approximation. This aliasing error is controlled by choosing a sufficiently large domain length $L$.\n\nThe implementation will execute this algorithm for each test case, comparing the resulting approximate PDF on the specified evaluation interval against the known analytical PDF to compute the maximum absolute error.",
            "answer": "```python\nimport numpy as np\n\ndef char_func_normal(t, mu, sigma):\n    \"\"\"Characteristic function of a Normal distribution.\"\"\"\n    return np.exp(1j * mu * t - 0.5 * (sigma**2) * (t**2))\n\ndef pdf_normal(x, mu, sigma):\n    \"\"\"Probability density function of a Normal distribution.\"\"\"\n    return (1 / (sigma * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mu) / sigma)**2)\n\ndef char_func_laplace(t, mu, b):\n    \"\"\"Characteristic function of a Laplace distribution.\"\"\"\n    return np.exp(1j * mu * t) / (1 + (b**2) * (t**2))\n\ndef pdf_laplace(x, mu, b):\n    \"\"\"Probability density function of a Laplace distribution.\"\"\"\n    return (1 / (2 * b)) * np.exp(-np.abs(x - mu) / b)\n\ndef char_func_cauchy(t, mu, gamma):\n    \"\"\"Characteristic function of a Cauchy distribution.\"\"\"\n    return np.exp(1j * mu * t - gamma * np.abs(t))\n\ndef pdf_cauchy(x, mu, gamma):\n    \"\"\"Probability density function of a Cauchy distribution.\"\"\"\n    return (1 / np.pi) * (gamma / (gamma**2 + (x - mu)**2))\n\ndef compute_pdf_error(dist_name, dist_params, N, L, window_type, x_eval):\n    \"\"\"\n    Computes the max absolute error of the FFT-recovered PDF for a given case.\n    \"\"\"\n    # 1. Select the appropriate characteristic and density functions\n    if dist_name == 'normal':\n        char_func = char_func_normal\n        pdf_exact = pdf_normal\n    elif dist_name == 'laplace':\n        char_func = char_func_laplace\n        pdf_exact = pdf_laplace\n    elif dist_name == 'cauchy':\n        char_func = char_func_cauchy\n        pdf_exact = pdf_cauchy\n    else:\n        raise ValueError(f\"Unknown distribution: {dist_name}\")\n\n    # 2. Construct spatial and frequency grids\n    dx = L / N\n    x_grid = -L / 2 + np.arange(N) * dx\n    \n    dt = 2 * np.pi / L\n    t_grid = (np.arange(N) - N / 2) * dt\n\n    # 3. Sample the characteristic function\n    phi_t = char_func(t_grid, **dist_params)\n\n    # 4. Apply windowing in the frequency domain\n    if window_type == 'rectangular':\n        window = np.ones(N)\n    elif window_type == 'blackman':\n        # The Blackman window is naturally centered at N/2, which corresponds\n        # to t=0 in our `t_grid` arrangement.\n        window = np.blackman(N)\n    else:\n        raise ValueError(f\"Unknown window type: {window_type}\")\n\n    phi_windowed = phi_t * window\n    \n    # 5. Perform FFT-based inversion\n    # The derivation f(x_j) approx (1/L) * (-1)^j * FFT[(-1)^k * phi(t_k)]\n    # is implemented below.\n    arange_N = np.arange(N)\n    phase_shift = (-1)**arange_N\n\n    Y_k = phase_shift * phi_windowed\n    Y_hat_j = np.fft.fft(Y_k)\n    f_approx_complex = (1 / L) * phase_shift * Y_hat_j\n    \n    # The resulting PDF must be real; imaginary part is numerical error.\n    f_approx = np.real(f_approx_complex)\n\n    # 6. Calculate maximum absolute error on the evaluation interval\n    eval_mask = (x_grid >= -x_eval)  (x_grid = x_eval)\n    x_compare = x_grid[eval_mask]\n    \n    if len(x_compare) == 0:\n        # This case would indicate the evaluation interval is outside the grid,\n        # which shouldn't happen with the given test cases.\n        return np.inf\n\n    f_approx_compare = f_approx[eval_mask]\n    f_exact_compare = pdf_exact(x_compare, **dist_params)\n    \n    max_abs_error = np.max(np.abs(f_approx_compare - f_exact_compare))\n    \n    return max_abs_error\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        ('normal', {'mu': 0, 'sigma': 1}, 4096, 50, 'rectangular', 6),\n        # Case 2\n        ('normal', {'mu': 0, 'sigma': 1}, 512, 20, 'rectangular', 6),\n        # Case 3\n        ('normal', {'mu': 0, 'sigma': 1}, 512, 20, 'blackman', 6),\n        # Case 4\n        ('laplace', {'mu': 0, 'b': 1}, 1024, 20, 'rectangular', 6),\n        # Case 5\n        ('laplace', {'mu': 0, 'b': 1}, 1024, 20, 'blackman', 6),\n        # Case 6\n        ('cauchy', {'mu': 0, 'gamma': 1}, 2048, 40, 'rectangular', 6),\n        # Case 7\n        ('cauchy', {'mu': 0, 'gamma': 1}, 2048, 40, 'blackman', 6),\n        # Case 8\n        ('normal', {'mu': 0, 'sigma': 1}, 256, 8, 'rectangular', 3),\n    ]\n\n    results = []\n    for case in test_cases:\n        dist_name, dist_params, N, L, window_type, x_eval = case\n        error = compute_pdf_error(dist_name, dist_params, N, L, window_type, x_eval)\n        results.append(error)\n\n    # Print results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "This final practice showcases the versatility of characteristic functions beyond simple inversion to find a density. Here, you will use the CF in two powerful ways: first, to derive the cumulants of a distribution needed to construct a Gram-Charlier series, which provides an analytical approximation to the true distribution; and second, to implement the Gil-Pelaez inversion formula for calculating the cumulative distribution function directly and with high precision . This exercise highlights the role of CFs as a foundational tool for both analytical approximation and accurate numerical computation in probability theory.",
            "id": "3293821",
            "problem": "Construct a complete, runnable program that evaluates the accuracy of a Gram–Charlier correction for a normal baseline using cumulants up to order four and compares it against a tail probability computed via a characteristic function (CF) inversion. The program must implement the following.\n\nConsider a family of standardized random variables defined as follows. Let $Z$ be a standard normal random variable with CF $\\varphi_Z(t) = \\exp(-t^2/2)$. Let $E_1$ be an exponential random variable with rate $1$ and CF $\\varphi_{E_1}(t) = \\frac{1}{1 - i t}$. Define $U = E_1 - 1$, so that $\\mathbb{E}[U] = 0$ and $\\varphi_U(t) = \\exp(-i t) \\varphi_{E_1}(t) = \\frac{\\exp(-i t)}{1 - i t}$. For a given real parameter $\\alpha \\ge 0$, define $Y = Z + \\alpha U$, and the standardized variable $X = Y / s$ where $s = \\sqrt{1 + \\alpha^2}$. This construction ensures $\\mathbb{E}[X] = 0$ and $\\mathrm{Var}(X) = 1$. \n\nThe CF of $X$ is given by $\\varphi_X(t) = \\varphi_Y(t/s)$, where $\\varphi_Y(u) = \\varphi_Z(u)\\,\\varphi_{\\alpha U}(u)$ and $\\varphi_{\\alpha U}(u) = \\varphi_U(\\alpha u)$. The cumulants of independent sums add, and cumulants scale with powers under scaling: if $W = c V$ then $\\kappa_r(W) = c^r \\kappa_r(V)$. The exponential distribution with rate $1$ has cumulants $\\kappa_r(E_1) = (r-1)!$ for all integers $r \\ge 1$; hence $U = E_1 - 1$ satisfies $\\kappa_1(U) = 0$ and $\\kappa_r(U) = (r-1)!$ for all integers $r \\ge 2$. Use only these foundational facts, together with the definition of a CF $\\varphi_X(t) = \\mathbb{E}[e^{i t X}]$ and the definition of cumulants via the logarithm of the CF, to derive the standardized cumulants of $X$ up to order four and use them to build a Gram–Charlier correction to a normal baseline.\n\nTasks to implement in the program:\n\n1. Using only the fundamental facts stated above, derive and implement the CF $\\varphi_X(t)$ for the given $X$. Then, compute the survival probability $\\mathbb{P}(X > x)$ using a valid inversion of the CF for the cumulative distribution function. The inversion must be implemented numerically by quadrature with sufficiently tight tolerances to resolve errors at the scale of $10^{-8}$ or smaller.\n\n2. Using the cumulants up to order four for $X$, construct the Gram–Charlier A series correction to a standard normal baseline, truncated to include terms that depend on the third and fourth cumulants only. From this truncated series, derive the corresponding approximation to the cumulative distribution function $F_{\\mathrm{GC}}(x)$, and from it compute the approximate survival probability $1 - F_{\\mathrm{GC}}(x)$.\n\n3. For each test case specified below, compute the absolute error between the Gram–Charlier survival probability and the CF-inversion survival probability, that is, compute $\\left| \\left(1 - F_{\\mathrm{GC}}(x)\\right) - \\mathbb{P}(X > x) \\right|$.\n\nTest suite to evaluate:\n- $(\\alpha, x) = (0.0, 1.0)$\n- $(\\alpha, x) = (0.5, 1.0)$\n- $(\\alpha, x) = (0.5, 2.0)$\n- $(\\alpha, x) = (1.0, 1.0)$\n- $(\\alpha, x) = (1.5, 0.0)$\n- $(\\alpha, x) = (1.5, 2.0)$\n\nAll computations are purely mathematical and unitless. Angles used in complex exponentials are in radians.\n\nYour program should produce a single line of output containing the absolute errors for the above test cases as a comma-separated list enclosed in square brackets, with each floating-point number formatted in scientific notation with eight digits after the decimal point. For example, the output format must be like \"[1.23456789e-04,2.34567891e-06,...]\" but with no spaces, i.e., \"[1.23456789e-04,2.34567891e-06,...]\".",
            "solution": "The user-provided problem is a well-defined exercise in computational probability and is deemed valid after a thorough review. All provided information is scientifically sound, self-contained, and mathematically consistent. The problem asks for the implementation and comparison of two methods for approximating a survival probability: one based on the numerical inversion of a characteristic function (CF) and another using a Gram-Charlier A series expansion.\n\nWe begin by systematically deriving the necessary mathematical formulas as outlined in the problem statement.\n\n### 1. Derivation of the Characteristic Function of $X$.\n\nThe random variable $X$ is constructed in several steps. We derive its characteristic function, $\\varphi_X(t) = \\mathbb{E}[e^{itX}]$, by following this construction.\n\n-   The variable $U$ is defined as $U = E_1 - 1$, where $E_1$ is an exponential random variable with rate $1$. Its characteristic function is $\\varphi_{E_1}(t) = \\frac{1}{1 - it}$. The CF of $U$ is:\n    $$ \\varphi_U(t) = \\mathbb{E}[e^{it(E_1-1)}] = e^{-it}\\mathbb{E}[e^{itE_1}] = e^{-it}\\varphi_{E_1}(t) = \\frac{e^{-it}}{1 - it} $$\n-   The variable $Y$ is the sum of a standard normal variable $Z$ and a scaled version of $U$, i.e., $Y = Z + \\alpha U$. As $Z$ and $U$ are constructed from independent sources (implied by the problem's setup), the CF of their sum is the product of their individual CFs. The CF of $\\alpha U$ is $\\varphi_{\\alpha U}(t) = \\varphi_U(\\alpha t)$.\n    $$ \\varphi_Y(t) = \\varphi_Z(t) \\cdot \\varphi_{\\alpha U}(t) = \\varphi_Z(t) \\cdot \\varphi_U(\\alpha t) $$\n    Substituting the known CFs:\n    $$ \\varphi_Y(t) = \\exp\\left(-\\frac{t^2}{2}\\right) \\cdot \\frac{e^{-i\\alpha t}}{1 - i\\alpha t} $$\n-   Finally, $X$ is the standardized version of $Y$, given by $X = Y/s$, where $s = \\sqrt{1 + \\alpha^2}$. The CF of a scaled variable $aW$ is $\\varphi_W(at)$. Thus, the CF of $X$ is:\n    $$ \\varphi_X(t) = \\varphi_Y(t/s) = \\exp\\left(-\\frac{(t/s)^2}{2}\\right) \\frac{\\exp(-i\\alpha (t/s))}{1 - i\\alpha (t/s)} $$\n    Substituting $s = \\sqrt{1+\\alpha^2}$, we obtain the final expression for the characteristic function of $X$:\n    $$ \\varphi_X(t) = \\exp\\left(-\\frac{t^2}{2(1+\\alpha^2)}\\right) \\frac{\\exp\\left(-\\frac{i\\alpha t}{\\sqrt{1+\\alpha^2}}\\right)}{1 - \\frac{i\\alpha t}{\\sqrt{1+\\alpha^2}}} $$\n\n### 2. Survival Probability via CF Inversion\n\nThe cumulative distribution function (CDF), $F_X(x) = \\mathbb{P}(X \\le x)$, can be recovered from its CF, $\\varphi_X(t)$, using the Gil-Pelaez inversion formula:\n$$ F_X(x) = \\frac{1}{2} - \\frac{1}{\\pi} \\int_0^\\infty \\text{Im}\\left( e^{-itx} \\varphi_X(t) \\right) \\frac{dt}{t} $$\nThe survival probability is $\\mathbb{P}(X > x) = 1 - F_X(x)$. Substituting the formula for $F_X(x)$:\n$$ \\mathbb{P}(X > x) = 1 - \\left( \\frac{1}{2} - \\frac{1}{\\pi} \\int_0^\\infty \\text{Im}\\left( e^{-itx} \\varphi_X(t) \\right) \\frac{dt}{t} \\right) = \\frac{1}{2} + \\frac{1}{\\pi} \\int_0^\\infty \\text{Im}\\left( e^{-itx} \\varphi_X(t) \\right) \\frac{dt}{t} $$\nThe integrand, let's call it $I(t,x,\\alpha)$, must be computed numerically. The expression inside the imaginary part operator is:\n$$ e^{-itx} \\varphi_X(t) = \\exp(-itx) \\exp\\left(-\\frac{t^2}{2s^2}\\right) \\frac{\\exp(-i \\alpha t/s)}{1 - i \\alpha t/s} = \\exp\\left(-\\frac{t^2}{2s^2} - it\\left(x + \\frac{\\alpha}{s}\\right)\\right) \\frac{1}{1 - i \\alpha t/s} $$\nThis expression is computed as a complex number for each $t$, its imaginary part is taken, divided by $t$, and then integrated from $t=0$ to $t=\\infty$. The integral will be evaluated using numerical quadrature, for which the `scipy.integrate.quad` function is suitable. The Gaussian decay term $\\exp(-t^2/(2s^2))$ ensures the integrand converges rapidly, making the numerical integration stable and efficient.\n\n### 3. Derivation of the Cumulants of $X$\n\nThe problem provides the necessary properties of cumulants, $\\kappa_r$.\n-   $\\kappa_r(E_1) = (r-1)!$ for $r \\ge 1$.\n-   For $U = E_1 - 1$, we have $\\kappa_1(U) = \\kappa_1(E_1) - 1 = (1-1)! - 1 = 0$, and for $r \\ge 2$, $\\kappa_r(U) = \\kappa_r(E_1) = (r-1)!$.\n-   For $Y = Z + \\alpha U$, using cumulant additivity for independent variables and the scaling property:\n    $$ \\kappa_r(Y) = \\kappa_r(Z) + \\kappa_r(\\alpha U) = \\kappa_r(Z) + \\alpha^r \\kappa_r(U) $$\n    The cumulants of a standard normal variable $Z$ are $\\kappa_1(Z) = 0$, $\\kappa_2(Z) = 1$, and $\\kappa_r(Z) = 0$ for $r \\ge 3$.\n    -   $\\kappa_1(Y) = 0 + \\alpha^1 \\cdot 0 = 0$.\n    -   $\\kappa_2(Y) = 1 + \\alpha^2 \\kappa_2(U) = 1 + \\alpha^2 (2-1)! = 1 + \\alpha^2$.\n    -   $\\kappa_3(Y) = 0 + \\alpha^3 \\kappa_3(U) = \\alpha^3 (3-1)! = 2\\alpha^3$.\n    -   $\\kappa_4(Y) = 0 + \\alpha^4 \\kappa_4(U) = \\alpha^4 (4-1)! = 6\\alpha^4$.\n-   For $X = Y/s$ with $s = \\sqrt{\\kappa_2(Y)} = \\sqrt{1+\\alpha^2}$, the cumulants are scaled by $s^{-r}$:\n    -   $\\kappa_1(X) = \\kappa_1(Y)/s = 0$.\n    -   $\\kappa_2(X) = \\kappa_2(Y)/s^2 = (1+\\alpha^2)/(1+\\alpha^2) = 1$.\n    -   $\\kappa_3(X) = \\kappa_3(Y)/s^3 = \\frac{2\\alpha^3}{(1+\\alpha^2)^{3/2}}$. (Standardized skewness)\n    -   $\\kappa_4(X) = \\kappa_4(Y)/s^4 = \\frac{6\\alpha^4}{(1+\\alpha^2)^2}$. (Standardized excess kurtosis)\n\n### 4. Gram-Charlier Survival Probability\n\nThe Gram-Charlier A series provides an approximation for the probability density function (PDF) of a random variable in terms of the standard normal PDF, $\\phi(x)$, and its derivatives (related to Hermite polynomials). The truncated series for the PDF $f_X(x)$ using cumulants up to order four is:\n$$ f_{\\mathrm{GC}}(x) = \\phi(x) \\left[ 1 + \\frac{\\kappa_3(X)}{3!} H_3(x) + \\frac{\\kappa_4(X)}{4!} H_4(x) \\right] $$\nwhere $H_k(x)$ are the probabilist's Hermite polynomials, e.g., $H_2(x) = x^2-1$, $H_3(x) = x^3 - 3x$.\n\nTo find the CDF approximation $F_{\\mathrm{GC}}(x)$, we integrate $f_{\\mathrm{GC}}(u)$ from $-\\infty$ to $x$. Using the property $\\int_{-\\infty}^x \\phi(u) H_k(u) du = -\\phi(x) H_{k-1}(x)$, we get:\n$$ F_{\\mathrm{GC}}(x) = \\Phi(x) - \\phi(x) \\left[ \\frac{\\kappa_3(X)}{6} H_2(x) + \\frac{\\kappa_4(X)}{24} H_3(x) \\right] $$\nwhere $\\Phi(x)$ is the standard normal CDF.\nThe approximate survival probability, $1 - F_{\\mathrm{GC}}(x)$, is therefore:\n$$ 1 - F_{\\mathrm{GC}}(x) = (1 - \\Phi(x)) + \\phi(x) \\left[ \\frac{\\kappa_3(X)}{6} (x^2 - 1) + \\frac{\\kappa_4(X)}{24} (x^3 - 3x) \\right] $$\nwhere $1 - \\Phi(x)$ is the standard normal survival function.\n\n### 5. Algorithmic Implementation\n\nThe program will implement these derived formulas for each test case $(\\alpha, x)$.\n1.  For a given $(\\alpha, x)$, the value of $s = \\sqrt{1+\\alpha^2}$ is computed.\n2.  An integrand function based on the CF inversion formula is defined. `scipy.integrate.quad` is used to compute the integral, yielding the \"true\" survival probability $\\mathbb{P}(X > x)$.\n3.  The cumulants $\\kappa_3(X)$ and $\\kappa_4(X)$ are calculated.\n4.  The Gram-Charlier survival probability approximation is computed using the formula from step 4, with $\\phi(x)$ and $1-\\Phi(x)$ provided by `scipy.stats.norm`.\n5.  The absolute error between the two probabilities is calculated and stored.\n6.  A special case is handled for $\\alpha=0$, where $X$ is exactly standard normal. In this case, the Gram-Charlier correction terms are zero, and both methods should yield the standard normal survival function. The error is thus theoretically zero.\n7.  The final list of errors is formatted and printed as specified.",
            "answer": "```python\nimport numpy as np\nfrom scipy import integrate\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the absolute error between Gram-Charlier and CF-inversion\n    approximations for the survival probability of a defined random variable.\n    \"\"\"\n    \n    test_cases = [\n        (0.0, 1.0),\n        (0.5, 1.0),\n        (0.5, 2.0),\n        (1.0, 1.0),\n        (1.5, 0.0),\n        (1.5, 2.0)\n    ]\n\n    results = []\n\n    # Set a tight tolerance for numerical integration to meet problem requirements.\n    QUAD_TOL = 1e-12\n\n    for alpha, x in test_cases:\n        # Case 1: alpha = 0. The variable X is standard normal.\n        if alpha == 0.0:\n            # k3 and k4 are 0, so GC approx is just norm.sf(x).\n            # CF inversion also yields norm.sf(x).\n            # The theoretical error is 0.\n            results.append(0.0)\n            continue\n            \n        # For alpha > 0, proceed with the full calculation.\n        s = np.sqrt(1.0 + alpha**2)\n\n        # ---- Task 1: Survival probability from CF inversion ----\n        \n        def char_func_x(t, local_alpha, local_s):\n            \"\"\"Characteristic function of the standardized variable X.\"\"\"\n            if t == 0:\n                return complex(1.0, 0.0)\n            \n            term1 = np.exp(- (t**2) / (2.0 * local_s**2))\n            \n            common_arg = local_alpha * t / local_s\n            numerator = np.exp(-1j * common_arg)\n            denominator = 1.0 - 1j * common_arg\n            \n            term2 = numerator / denominator\n            \n            return term1 * term2\n\n        def inversion_integrand(t, local_alpha, local_s, local_x):\n            \"\"\"Integrand for the Gil-Pelaez inversion formula.\"\"\"\n            if t == 0:\n                # The limit of Im(exp(-itx)phi(t))/t as t->0 is finite.\n                # However, for numerical stability, we can return 0 here\n                # because quad handles singularities at the boundary.\n                return 0.0\n            \n            phi_t = char_func_x(t, local_alpha, local_s)\n            val = np.exp(-1j * t * local_x) * phi_t\n            return val.imag / t\n        \n        # Integrate from 0 to infinity\n        integral_val, _ = integrate.quad(\n            inversion_integrand, 0, np.inf, args=(alpha, s, x),\n            epsabs=QUAD_TOL, epsrel=QUAD_TOL\n        )\n        \n        prob_cf = 0.5 + integral_val / np.pi\n\n        # ---- Task 2: Survival probability from Gram-Charlier series ----\n\n        # Calculate standardized cumulants k3 and k4\n        s_cubed = s**3\n        s_fourth = s**4\n        alpha_cubed = alpha**3\n        alpha_fourth = alpha**4\n\n        k3 = (2.0 * alpha_cubed) / s_cubed\n        k4 = (6.0 * alpha_fourth) / s_fourth\n        \n        # Hermite polynomials related terms\n        h2_term = x**2 - 1.0\n        h3_term = x**3 - 3.0 * x\n\n        # Gram-Charlier correction term\n        gc_correction = norm.pdf(x) * (k3 / 6.0 * h2_term + k4 / 24.0 * h3_term)\n        \n        prob_gc = norm.sf(x) + gc_correction\n        \n        # ---- Task 3: Compute absolute error ----\n        \n        absolute_error = abs(prob_gc - prob_cf)\n        results.append(absolute_error)\n\n    # Format and print the final output as specified.\n    output_str = \",\".join([f\"{res:.8e}\" for res in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n\n```"
        }
    ]
}