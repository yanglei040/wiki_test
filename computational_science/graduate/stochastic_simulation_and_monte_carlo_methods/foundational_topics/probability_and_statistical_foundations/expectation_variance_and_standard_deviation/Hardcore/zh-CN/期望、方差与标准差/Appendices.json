{
    "hands_on_practices": [
        {
            "introduction": "在蒙特卡洛模拟中，我们常常可以得到对同一数量的多个估计量。本练习将探讨如何最优地组合这些估计量以提高精度。你将从第一性原理出发，推导加权平均估计量的方差如何依赖于各个估计量的方差及其协方差，并利用这一关系找到最小化组合估计量方差的最优权重。",
            "id": "3307448",
            "problem": "考虑一个蒙特卡洛 (MC) 随机模拟情景，其中使用共同随机数 (CRN) 和控制变量为同一目标量 $\\theta \\in \\mathbb{R}$ 生成了 $3$ 个相关的无偏估计量 $X_{1}$、$X_{2}$ 和 $X_{3}$。每个 $X_{i}$ 都具有有限二阶矩，且它们的联合协方差矩阵已知，由下式给出\n$$\n\\Sigma \\;=\\;\n\\begin{pmatrix}\n2   1  0 \\\\\n1  2  1 \\\\\n0  1  2\n\\end{pmatrix}.\n$$\n定义加权平均估计量\n$$\n\\widehat{\\theta}(w) \\;=\\; w_{1} X_{1} \\;+\\; w_{2} X_{2} \\;+\\; w_{3} X_{3},\n$$\n其中权重 $w = (w_{1}, w_{2}, w_{3})^{\\top}$ 满足无偏性约束 $\\sum_{i=1}^{3} w_{i} = 1$。\n\n严格从期望、方差和协方差的基本定义出发，不使用任何关于线性组合的现成公式，执行以下操作：\n\n- 推导 $\\operatorname{Var}(\\widehat{\\theta}(w))$ 关于权重和 $(X_{1}, X_{2}, X_{3})$ 的协方差的一般表达式。\n- 施加无偏性约束，并确定使 $\\operatorname{Var}(\\widehat{\\theta}(w))$ 最小化的权重 $w$。\n- 使用给定的 $\\Sigma$ 计算 $\\operatorname{Var}(\\widehat{\\theta}(w))$ 的最小值。\n\n将最终答案表示为一个精确的实数；无需四舍五入。不要报告诸如最优权重之类的中间量；最终答案只需要最小方差值。",
            "solution": "所述问题具有科学依据、适定且客观。它提出了统计估计理论中的一个标准任务，即优化估计量的线性组合以最小化方差。所有必要的数据和约束都已提供，不存在内部矛盾或违反数学原理的情况。因此，我们可以进行完整求解。\n\n问题要求完成三项任务：首先，推导加权估计量方差的一般表达式；其次，在无偏性约束下找到使该方差最小化的权重；第三，针对一个特定的协方差矩阵计算此最小方差。\n\n设量 $\\theta$ 的 $3$ 个相关无偏估计量为 $X_1$、$X_2$ 和 $X_3$。由于它们是无偏的，其期望为 $E[X_i] = \\theta$，其中 $i \\in \\{1, 2, 3\\}$。加权平均估计量定义为 $\\widehat{\\theta}(w) = w_1 X_1 + w_2 X_2 + w_3 X_3 = \\sum_{i=1}^3 w_i X_i$。权重 $w = (w_1, w_2, w_3)^\\top$ 是实数，必须满足约束 $\\sum_{i=1}^3 w_i = 1$，以确保 $\\widehat{\\theta}(w)$ 也是 $\\theta$ 的一个无偏估计量。我们可以验证如下：\n$$\nE[\\widehat{\\theta}(w)] = E\\left[\\sum_{i=1}^3 w_i X_i\\right] = \\sum_{i=1}^3 w_i E[X_i] = \\sum_{i=1}^3 w_i \\theta = \\theta \\sum_{i=1}^3 w_i = \\theta \\cdot 1 = \\theta.\n$$\n\n首先，我们推导 $\\widehat{\\theta}(w)$ 的方差表达式，记为 $\\operatorname{Var}(\\widehat{\\theta}(w))$。根据方差的基本定义：\n$$\n\\operatorname{Var}(\\widehat{\\theta}(w)) = E\\left[ (\\widehat{\\theta}(w) - E[\\widehat{\\theta}(w)])^2 \\right] = E\\left[ \\left(\\sum_{i=1}^3 w_i X_i - \\theta \\right)^2 \\right].\n$$\n使用约束 $\\sum_{i=1}^3 w_i = 1$，我们可以写出 $\\theta = \\sum_{i=1}^3 w_i \\theta$。将此代入表达式可得：\n$$\n\\operatorname{Var}(\\widehat{\\theta}(w)) = E\\left[ \\left(\\sum_{i=1}^3 w_i X_i - \\sum_{i=1}^3 w_i \\theta \\right)^2 \\right] = E\\left[ \\left(\\sum_{i=1}^3 w_i (X_i - \\theta) \\right)^2 \\right].\n$$\n让我们展开平方和：\n$$\n\\left(\\sum_{i=1}^3 w_i (X_i - \\theta) \\right)^2 = \\left(\\sum_{i=1}^3 w_i (X_i - \\theta) \\right) \\left(\\sum_{j=1}^3 w_j (X_j - \\theta) \\right) = \\sum_{i=1}^3 \\sum_{j=1}^3 w_i w_j (X_i - \\theta)(X_j - \\theta).\n$$\n应用期望算子并利用其线性性：\n$$\n\\operatorname{Var}(\\widehat{\\theta}(w)) = E\\left[\\sum_{i=1}^3 \\sum_{j=1}^3 w_i w_j (X_i - \\theta)(X_j - \\theta)\\right] = \\sum_{i=1}^3 \\sum_{j=1}^3 w_i w_j E[(X_i - \\theta)(X_j - \\theta)].\n$$\n根据定义，项 $E[(X_i - \\theta)(X_j - \\theta)]$ 是 $X_i$ 和 $X_j$ 之间的协方差，记作 $\\operatorname{Cov}(X_i, X_j)$。问题提供了协方差矩阵 $\\Sigma$，其中 $\\Sigma_{ij} = \\operatorname{Cov}(X_i, X_j)$。因此，该估计量的方差为：\n$$\n\\operatorname{Var}(\\widehat{\\theta}(w)) = \\sum_{i=1}^3 \\sum_{j=1}^3 w_i w_j \\Sigma_{ij}.\n$$\n在矩阵表示法中，这可以紧凑地表示为 $\\operatorname{Var}(\\widehat{\\theta}(w)) = w^\\top \\Sigma w$。\n\n其次，我们必须找到在约束 $\\sum_{i=1}^3 w_i = 1$ 下使该方差最小化的权重 $w$。这是一个约束优化问题。我们可以使用拉格朗日乘子法。要最小化的目标函数是 $f(w) = w^\\top \\Sigma w$，约束条件是 $g(w) = w^\\top \\mathbf{1} - 1 = 0$，其中 $\\mathbf{1}$ 是一个全为 1 的列向量。拉格朗日函数为：\n$$\n\\mathcal{L}(w, \\lambda) = w^\\top \\Sigma w - \\lambda(w^\\top \\mathbf{1} - 1).\n$$\n为求最小值，我们计算 $\\mathcal{L}$ 关于 $w$ 的梯度并将其设为零。二次型 $w^\\top \\Sigma w$ 的梯度是 $2\\Sigma w$（因为 $\\Sigma$ 是对称的），而 $w^\\top \\mathbf{1}$ 的梯度是 $\\mathbf{1}$。\n$$\n\\nabla_w \\mathcal{L} = 2\\Sigma w - \\lambda\\mathbf{1} = \\mathbf{0}.\n$$\n这给出 $2\\Sigma w = \\lambda\\mathbf{1}$。假设 $\\Sigma$ 是可逆的，我们可以写出 $w = \\frac{\\lambda}{2} \\Sigma^{-1} \\mathbf{1}$。\n为了求解拉格朗日乘子 $\\lambda$，我们将这个 $w$ 的表达式代回约束方程 $w^\\top \\mathbf{1} = 1$：\n$$\n\\left(\\frac{\\lambda}{2} \\Sigma^{-1} \\mathbf{1}\\right)^\\top \\mathbf{1} = 1 \\implies \\frac{\\lambda}{2} (\\Sigma^{-1} \\mathbf{1})^\\top \\mathbf{1} = 1.\n$$\n由于 $\\Sigma$ 是对称的，$\\Sigma^{-1}$ 也是对称的，所以 $(\\Sigma^{-1})^\\top = \\Sigma^{-1}$。\n$$\n\\frac{\\lambda}{2} \\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1} = 1 \\implies \\lambda = \\frac{2}{\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}}.\n$$\n因此，最优权重向量 $w_{opt}$ 是：\n$$\nw_{opt} = \\frac{1}{\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}} \\Sigma^{-1} \\mathbf{1}.\n$$\n将 $w_{opt}$ 代入方差表达式，即可得到最小方差：\n$$\n\\operatorname{Var}_{min} = w_{opt}^\\top \\Sigma w_{opt} = \\left(\\frac{\\Sigma^{-1} \\mathbf{1}}{\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}}\\right)^\\top \\Sigma \\left(\\frac{\\Sigma^{-1} \\mathbf{1}}{\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}}\\right).\n$$\n这可以简化为：\n$$\n\\operatorname{Var}_{min} = \\frac{\\mathbf{1}^\\top (\\Sigma^{-1})^\\top \\Sigma \\Sigma^{-1} \\mathbf{1}}{(\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1})^2} = \\frac{\\mathbf{1}^\\top \\Sigma^{-1} \\Sigma \\Sigma^{-1} \\mathbf{1}}{(\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1})^2} = \\frac{\\mathbf{1}^\\top I \\Sigma^{-1} \\mathbf{1}}{(\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1})^2} = \\frac{\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}}{(\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1})^2} = \\frac{1}{\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}}.\n$$\n\n第三，我们使用给定的协方差矩阵计算这个最小方差：\n$$\n\\Sigma = \\begin{pmatrix} 2   1  0 \\\\ 1  2  1 \\\\ 0  1  2 \\end{pmatrix}.\n$$\n为了找到最小方差，我们需要计算 $\\Sigma^{-1}$。$\\Sigma$ 的行列式是：\n$$\n\\det(\\Sigma) = 2(2 \\cdot 2 - 1 \\cdot 1) - 1(1 \\cdot 2 - 1 \\cdot 0) + 0(1 \\cdot 1 - 2 \\cdot 0) = 2(3) - 1(2) = 4.\n$$\n伴随矩阵 $\\operatorname{adj}(\\Sigma)$ 是代数余子式矩阵的转置：\n$$\n\\operatorname{adj}(\\Sigma) = \\begin{pmatrix}\n\\begin{vmatrix} 2   1 \\\\ 1   2 \\end{vmatrix}   -\\begin{vmatrix} 1   1 \\\\ 0   2 \\end{vmatrix}   \\begin{vmatrix} 1   2 \\\\ 0   1 \\end{vmatrix} \\\\\n-\\begin{vmatrix} 1   0 \\\\ 1   2 \\end{vmatrix}   \\begin{vmatrix} 2   0 \\\\ 0   2 \\end{vmatrix}   -\\begin{vmatrix} 2   1 \\\\ 0   1 \\end{vmatrix} \\\\\n\\begin{vmatrix} 1   0 \\\\ 2   1 \\end{vmatrix}   -\\begin{vmatrix} 2   0 \\\\ 1   1 \\end{vmatrix}   \\begin{vmatrix} 2   1 \\\\ 1   2 \\end{vmatrix}\n\\end{pmatrix}^\\top\n= \\begin{pmatrix} 3   -2  1 \\\\ -2  4  -2 \\\\ 1  -2  3 \\end{pmatrix}^\\top\n= \\begin{pmatrix} 3   -2  1 \\\\ -2  4  -2 \\\\ 1  -2  3 \\end{pmatrix}.\n$$\n逆矩阵是 $\\Sigma^{-1} = \\frac{1}{\\det(\\Sigma)}\\operatorname{adj}(\\Sigma)$：\n$$\n\\Sigma^{-1} = \\frac{1}{4} \\begin{pmatrix} 3   -2  1 \\\\ -2  4  -2 \\\\ 1  -2  3 \\end{pmatrix}.\n$$\n现在，我们计算数量 $\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}$，即 $\\Sigma^{-1}$ 的所有元素之和：\n$$\n\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1} = \\frac{1}{4} (3 - 2 + 1 - 2 + 4 - 2 + 1 - 2 + 3) = \\frac{1}{4}(4) = 1.\n$$\n最后，最小方差为：\n$$\n\\operatorname{Var}_{min} = \\frac{1}{\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}} = \\frac{1}{1} = 1.\n$$\n组合估计量的最小方差是 $1$。",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "模拟中的总误差通常比单纯的统计噪声更为复杂。本练习旨在阐明一个关键概念：将均方误差（MSE）分解为两个截然不同的部分，即源于模型近似的系统性偏差（bias）和源于蒙特卡洛过程的抽样方差（sampling variance）。通过对这两个误差来源进行量化分析，你将更深刻地理解如何有效地提升模拟的准确性。",
            "id": "3307371",
            "problem": "考虑使用一个两阶段过程来估计一个确定性被积函数在单位区间上的积分，该过程首先对被积函数进行离散化，然后应用蒙特卡洛（MC）采样。设目标量为单位区间上均匀分布下有界连续函数的期望，定义为\n$$\n\\theta \\equiv \\mathbb{E}[f(U)] = \\int_{0}^{1} f(x)\\,dx,\n$$\n其中 $U \\sim \\mathrm{Uniform}(0,1)$ 且 $f:[0,1]\\to\\mathbb{R}$ 是被积函数。在本问题中，将被积函数固定为\n$$\nf(x)=\\exp(-x)\\,\\sin(2\\pi x).\n$$\n目标量 $\\theta$ 的精确值可通过初等微积分解析计算。您需要仅从期望、方差、独立性和均方误差的基本定义出发，推导出一个精确的误差分解，以区分离散化和采样的影响。在推导过程中不允许使用任何快捷公式。\n\n通过将区间 $[0,1]$ 划分为 $M$ 个等宽的区间（每个区间的宽度为 $h=1/M$）并使用区间中点，来定义 $f$ 的一个离散化近似。具体来说，对于每个整数 $i\\in\\{1,2,\\dots,M\\}$，定义中点\n$$\nm_i=\\frac{i-\\tfrac{1}{2}}{M},\n$$\n和分段常数函数 $g_M:[0,1]\\to\\mathbb{R}$，使得\n$$\ng_M(x)=f(m_i)\\quad\\text{对于}\\quad x\\in\\left[\\frac{i-1}{M},\\frac{i}{M}\\right).\n$$\n对于一个给定的整数 $K\\ge 1$，通过下式定义离散化期望的 MC 估计量\n$$\n\\widehat{\\theta}_{M,K}=\\frac{1}{K}\\sum_{j=1}^{K} g_M(U_j),\n$$\n其中 $U_1,\\dots,U_K$ 是从 $\\mathrm{Uniform}(0,1)$ 中抽取的独立同分布（i.i.d.）随机变量。\n\n您的任务是：\n- 从第一性原理和定义出发，推导出由离散化引起的偏差、由采样引起的方差以及 $\\widehat{\\theta}_{M,K}$ 的均方误差（MSE），并用 $M$、$K$ 以及 $f$ 在中点 $\\{m_i\\}_{i=1}^M$ 处的值来表示。\n- 清晰地展示离散化如何定义期望的一个确定性近似，以及 MC 采样如何产生一个其变异性取决于 $K$ 的随机估计量。\n- 实现一个程序，对于测试套件中的每个测试用例 $(M,K)$，计算：\n  1. 偏差平方，定义为精确值 $\\theta$ 与 $g_M(U)$ 在 $\\mathrm{Uniform}(0,1)$ 下的离散化期望之差的平方。\n  2. 按 $1/K$ 缩放的采样方差项，使用 $g_M(U)$ 在 $\\mathrm{Uniform}(0,1)$ 下的方差和样本的独立性来定义。\n  3. 总均方误差，定义为偏差平方和采样方差项之和。\n- 使用与固定被积函数 $f$ 对应的 $\\theta$ 的闭式解析值。\n- 将所有输出四舍五入到 $10$ 位小数。\n\n测试套件：\n- 案例 1：$(M,K)=(1,1)$\n- 案例 2：$(M,K)=(4,1)$\n- 案例 3：$(M,K)=(16,10)$\n- 案例 4：$(M,K)=(16,1000)$\n- 案例 5：$(M,K)=(64,10)$\n- 案例 6：$(M,K)=(64,10000)$\n\n最终输出格式：\n- 您的程序应生成单行文本，其中包含一个用方括号括起来的逗号分隔列表，每个测试用例的结果是一个浮点数三元组 $[\\text{bias}^2,\\text{variance\\_term},\\text{mse}]$，并且总输出是与测试套件顺序相同的这些三元组的列表。例如，\n$$\n\\text{print}\\left(\\left[\\left[\\text{b}_1,\\text{v}_1,\\text{m}_1\\right],\\dots,\\left[\\text{b}_6,\\text{v}_6,\\text{m}_6\\right]\\right]\\right).\n$$",
            "solution": "我们从基本定义开始。设 $U\\sim\\mathrm{Uniform}(0,1)$ 且 $f:[0,1]\\to\\mathbb{R}$ 为有界连续函数。目标量是期望\n$$\n\\theta=\\mathbb{E}[f(U)]=\\int_{0}^{1} f(x)\\,dx.\n$$\n对于固定的被积函数 $f(x)=\\exp(-x)\\sin(2\\pi x)$，我们精确计算 $\\theta$。使用标准积分恒等式\n$$\n\\int e^{ax}\\sin(bx)\\,dx=\\frac{e^{ax}}{a^2+b^2}\\left(a\\sin(bx)-b\\cos(bx)\\right),\n$$\n并设 $a=-1$ 和 $b=2\\pi$，我们得到\n$$\n\\theta=\\int_{0}^{1} e^{-x}\\sin(2\\pi x)\\,dx=\\left.\\frac{e^{-x}}{1+(2\\pi)^2}\\left(-\\sin(2\\pi x)-2\\pi\\cos(2\\pi x)\\right)\\right|_{0}^{1}.\n$$\n因为 $\\sin(2\\pi)=\\sin(0)=0$ 且 $\\cos(2\\pi)=\\cos(0)=1$，这可以简化为\n$$\n\\theta=\\frac{2\\pi\\left(1-e^{-1}\\right)}{1+4\\pi^2}.\n$$\n\n接下来，我们如下定义离散化的分段常数近似 $g_M$：将 $[0,1]$ 划分为 $M$ 个相等的区间，并令 $m_i=(i-\\tfrac{1}{2})/M$ 为第 $i$ 个区间的中点。定义\n$$\ng_M(x)=f(m_i),\\quad \\text{对于 } x\\in\\left[\\frac{i-1}{M},\\frac{i}{M}\\right),\\quad i=1,\\dots,M.\n$$\n根据构造，在 $U\\sim\\mathrm{Uniform}(0,1)$ 条件下，随机变量 $g_M(U)$ 是一个离散随机变量，以 $1/M$ 的概率取值 $f(m_i)$。因此，\n$$\n\\mathbb{E}[g_M(U)]=\\frac{1}{M}\\sum_{i=1}^{M} f(m_i),\n$$\n并且\n$$\n\\mathbb{E}[g_M(U)^2]=\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)^2.\n$$\n使用方差的定义，\n$$\n\\mathrm{Var}(g_M(U))=\\mathbb{E}[g_M(U)^2]-\\left(\\mathbb{E}[g_M(U)]\\right)^2=\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)^2-\\left(\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)\\right)^2.\n$$\n\n使用 $K$ 个来自 $\\mathrm{Uniform}(0,1)$ 的独立同分布样本 $U_1,\\dots,U_K$ 的离散化期望的蒙特卡洛估计量是\n$$\n\\widehat{\\theta}_{M,K}=\\frac{1}{K}\\sum_{j=1}^{K} g_M(U_j).\n$$\n根据独立性和期望的线性性，\n$$\n\\mathbb{E}\\left[\\widehat{\\theta}_{M,K}\\right]=\\mathbb{E}[g_M(U)]=\\frac{1}{M}\\sum_{i=1}^{M} f(m_i).\n$$\n使用独立随机变量的方差可加性和齐次性，\n$$\n\\mathrm{Var}\\left(\\widehat{\\theta}_{M,K}\\right)=\\mathrm{Var}\\left(\\frac{1}{K}\\sum_{j=1}^{K} g_M(U_j)\\right)=\\frac{1}{K^2}\\sum_{j=1}^{K}\\mathrm{Var}\\left(g_M(U_j)\\right)=\\frac{1}{K}\\,\\mathrm{Var}(g_M(U)).\n$$\n\n定义估计量相对于真实目标 $\\theta$ 的偏差为\n$$\n\\mathrm{Bias}(\\widehat{\\theta}_{M,K};\\theta)=\\mathbb{E}\\left[\\widehat{\\theta}_{M,K}\\right]-\\theta=\\left(\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)\\right)-\\theta.\n$$\n根据定义，均方误差（MSE）为\n$$\n\\mathrm{MSE}(\\widehat{\\theta}_{M,K})=\\mathbb{E}\\left[\\left(\\widehat{\\theta}_{M,K}-\\theta\\right)^2\\right].\n$$\n通过偏差-方差分解（直接从定义和配方法得出）展开，\n$$\n\\mathrm{MSE}(\\widehat{\\theta}_{M,K})=\\left(\\mathrm{Bias}(\\widehat{\\theta}_{M,K};\\theta)\\right)^2+\\mathrm{Var}\\left(\\widehat{\\theta}_{M,K}\\right).\n$$\n代入上面推导的表达式，\n$$\n\\left(\\mathrm{Bias}(\\widehat{\\theta}_{M,K};\\theta)\\right)^2=\\left(\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)-\\theta\\right)^2,\n$$\n$$\n\\mathrm{Var}\\left(\\widehat{\\theta}_{M,K}\\right)=\\frac{1}{K}\\left[\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)^2-\\left(\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)\\right)^2\\right],\n$$\n因此\n$$\n\\mathrm{MSE}(\\widehat{\\theta}_{M,K})=\\left(\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)-\\theta\\right)^2+\\frac{1}{K}\\left[\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)^2-\\left(\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)\\right)^2\\right].\n$$\n\n计算每个 $(M,K)$ 的所需量的算法设计：\n- 使用解析表达式精确计算 $\\theta$：\n$$\n\\theta=\\frac{2\\pi\\left(1-e^{-1}\\right)}{1+4\\pi^2}.\n$$\n- 对于给定的 $M$，构造中点 $m_i=(i-\\tfrac{1}{2})/M$（其中 $i=1,\\dots,M$）。\n- 计算 $f(m_i)=\\exp(-m_i)\\sin(2\\pi m_i)$ 并计算：\n  - 离散化期望 $\\mu_M=\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)$。\n  - 偏差平方 $\\left(\\mu_M-\\theta\\right)^2$。\n  - $g_M(U)$ 在 $\\mathrm{Uniform}(0,1)$ 下的方差，通过 $\\frac{1}{M}\\sum f(m_i)^2-\\mu_M^2$ 计算。\n  - 按 $1/K$ 缩放的采样方差项，即 $\\mathrm{Var}(g_M(U))/K$。\n  - 总 MSE，即偏差平方与采样方差项之和。\n- 对每个测试用例的所有三个输出四舍五入到 $10$ 位小数。\n\n误差分解解释：\n- 离散化部分产生的原因是 $g_M$ 通过分段常数函数近似 $f$；这使得期望从 $\\theta$ 变为 $\\mu_M$，引入了一个与 $K$ 无关的确定性偏差。\n- 采样部分产生的原因是 $\\widehat{\\theta}_{M,K}$ 是 $g_M(U)$ 的独立同分布评估的样本均值；其方差按 $1/K$ 衰减，这是由独立平均的性质所保证的。\n- 均方误差将这些效应相加：一个来自离散化的确定性偏差平方和一个取决于 $K$ 的随机方差项。\n\n该程序精确实现了这一推导，并为每个测试用例 $(M,K)$ 输出三元组 $[\\mathrm{bias}^2,\\mathrm{variance\\_term},\\mathrm{MSE}]$，四舍五入到 $10$ 位小数，并聚合为指定格式的单个列表。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f(x):\n    # f(x) = exp(-x) * sin(2*pi*x)\n    return np.exp(-x) * np.sin(2 * np.pi * x)\n\ndef exact_theta():\n    # theta = 2*pi*(1 - e^{-1}) / (1 + 4*pi^2)\n    pi = np.pi\n    return (2 * pi * (1 - np.exp(-1.0))) / (1 + 4 * pi * pi)\n\ndef compute_components(M, K, theta):\n    # Midpoints of M bins on [0,1]\n    i = np.arange(1, M + 1, dtype=float)\n    midpoints = (i - 0.5) / M\n\n    f_vals = f(midpoints)\n\n    mu_M = f_vals.mean()\n    bias_sq = (mu_M - theta) ** 2\n\n    var_gM = (f_vals ** 2).mean() - mu_M ** 2\n    var_term = var_gM / K\n\n    mse = bias_sq + var_term\n\n    # Round to 10 decimal places\n    def r10(x):\n        return float(f\"{x:.10f}\")\n\n    return [r10(bias_sq), r10(var_term), r10(mse)]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Cases: (M,K)\n    test_cases = [\n        (1, 1),\n        (4, 1),\n        (16, 10),\n        (16, 1000),\n        (64, 10),\n        (64, 10000),\n    ]\n\n    theta = exact_theta()\n\n    results = []\n    for M, K in test_cases:\n        results.append(compute_components(M, K, theta))\n\n    # Final print statement in the exact required format.\n    # Single line: list of triples with comma-separated values.\n    # Ensure no extra whitespace beyond commas and brackets.\n    # Convert nested list to required string format.\n    inner = \",\".join(\"[\" + \",\".join(f\"{v:.10f}\" for v in triple) + \"]\" for triple in results)\n    print(f\"[{inner}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "无偏估计量一定是最佳选择吗？本练习通过探索偏差-方差权衡（bias-variance trade-off）来挑战这一观念。你将比较标准样本均值与两种高级估计方法——控制变量法和收缩估计法——的性能，并发现某些情况下，有意引入少量偏差反而能显著降低总估计误差。",
            "id": "3307435",
            "problem": "考虑一个服从标准正态分布 $\\mathcal{N}(0,1)$ 的随机变量 $X$，以及一个由实参数 $\\alpha$ 定义的可测函数 $g(x) = \\exp(\\alpha x)$。我们关注的目标是期望 $\\mu = \\mathbb{E}[g(X)]$。您将研究基于来自 $X$ 的 $n$ 个独立同分布样本 $\\{X_i\\}_{i=1}^n$ 的三个 $\\mu$ 的蒙特卡洛估计量，重点关注偏差和方差如何通过方差缩减方法相互作用以减小均方误差。\n\n估计量 1 (样本均值)：定义 $\\bar{g}_n = \\frac{1}{n}\\sum_{i=1}^n g(X_i)$ 和估计量 $\\hat{\\theta}_{\\mathrm{mean}} = \\bar{g}_n$。\n\n估计量 2 (已知均值的控制变量)：令 $Y = X$，其期望已知为 $\\mathbb{E}[Y] = 0$。对于任意实系数 $c$，定义控制变量估计量\n$$\n\\hat{\\theta}_{\\mathrm{cv}}(c) = \\bar{g}_n - c \\,\\bar{Y}_n,\n$$\n其中 $\\bar{Y}_n = \\frac{1}{n}\\sum_{i=1}^n Y_i$。\n\n估计量 3 (朝向目标的收缩)：对于给定的目标 $t \\in \\mathbb{R}$ 和收缩参数 $\\lambda \\in [0,1]$，定义估计量\n$$\n\\hat{\\theta}_{\\lambda} = (1 - \\lambda)\\,\\bar{g}_n + \\lambda\\, t.\n$$\n\n任务：\n$1.$ 从期望和方差的定义以及标准正态分布的矩生成函数的性质出发，推导 $\\mu = \\mathbb{E}[g(X)]$、$\\operatorname{Var}\\big(g(X)\\big)$ 和 $\\operatorname{Cov}\\big(g(X), Y\\big)$ 的闭式表达式。\n\n$2.$ 使用偏差、方差和均方误差的定义，确定 $\\operatorname{Bias}\\big(\\hat{\\theta}_{\\mathrm{mean}}\\big)$、$\\operatorname{Var}\\big(\\hat{\\theta}_{\\mathrm{mean}}\\big)$ 和 $\\operatorname{MSE}\\big(\\hat{\\theta}_{\\mathrm{mean}}\\big)$。对于控制变量估计量，计算使 $\\operatorname{Var}\\big(\\hat{\\theta}_{\\mathrm{cv}}(c)\\big)$ 最小化的值 $c^*$，并给出相应的最小化方差和均方误差。对于收缩估计量，以闭式形式计算 $\\operatorname{Bias}\\big(\\hat{\\theta}_{\\lambda}\\big)$、$\\operatorname{Var}\\big(\\hat{\\theta}_{\\lambda}\\big)$ 和 $\\operatorname{MSE}\\big(\\hat{\\theta}_{\\lambda}\\big)$。\n\n$3.$ 证明均方误差 $\\operatorname{MSE}\\big(\\hat{\\theta}_{\\lambda}\\big)$ 在唯一的 $\\lambda^* \\in [0,1]$ 处最小化，并推导 $\\lambda^*$ 和最小化均方误差 $\\operatorname{MSE}\\big(\\hat{\\theta}_{\\lambda^*}\\big)$ 的闭式表达式。\n\n程序要求：\n- 实现一个程序，对于下面指定的每个测试用例，使用您推导出的公式计算以下 $5$ 个量：\n  $1.$ $\\operatorname{MSE}\\big(\\hat{\\theta}_{\\mathrm{mean}}\\big)$，\n  $2.$ $\\operatorname{MSE}\\big(\\hat{\\theta}_{\\mathrm{cv}}(c^*)\\big)$，\n  $3.$ 在给定 $\\lambda$ 下计算的 $\\operatorname{MSE}\\big(\\hat{\\theta}_{\\lambda}\\big)$，\n  $4.$ 在 $[0,1]$ 上使 $\\operatorname{MSE}\\big(\\hat{\\theta}_{\\lambda}\\big)$ 最小化的 $\\lambda^*$，\n  $5.$ $\\operatorname{MSE}\\big(\\hat{\\theta}_{\\lambda^*}\\big)$。\n- 程序不能依赖随机模拟；它应根据推导出的解析表达式精确计算这些量。\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表。每个测试用例的 $5$ 个结果应显示为用方括号括起来的逗号分隔列表，并且所有测试用例列表应聚合到一个外部列表中（例如，`[[r_{1,1},r_{1,2},r_{1,3},r_{1,4},r_{1,5}],[r_{2,1},r_{2,2},r_{2,3},r_{2,4},r_{2,5}],\\dots]`）。\n\n测试套件：\n- 用例 A (理想情况)：$(\\alpha, n, t, \\lambda) = (0.5, 10000, 1.0, 0.3)$。\n- 用例 B (目标等于真实均值的边界情况)：$(\\alpha, n, t, \\lambda) = (0.5, 10000, \\exp(\\alpha^2/2), 0.7)$。\n- 用例 C (小样本量和远离的目标)：$(\\alpha, n, t, \\lambda) = (1.0, 100, 0.0, 0.5)$。\n- 用例 D (产生零方差的退化函数)：$(\\alpha, n, t, \\lambda) = (0.0, 1000, 2.0, 0.9)$。\n- 用例 E (大样本量和非常远离的目标)：$(\\alpha, n, t, \\lambda) = (1.5, 50000, \\exp(\\alpha^2/2) + 5.0, 0.2)$。\n\n所有数值结果必须以实值浮点数报告。此问题不适用任何物理单位或角度单位，也不应使用百分比；请将任何比率表示为小数。",
            "solution": "该问题是有效的，因为它具有坚实的概率论和统计学基础，问题设定良好，信息充分，可得出唯一解，并且陈述客观。它涉及期望、方差、偏差、均方误差和蒙特卡洛估计量等标准概念，没有任何事实或逻辑上的不一致。\n\n求解过程首先按任务 $1$、$2$ 和 $3$ 的要求推导必要的理论量。然后，将在程序中实现这些闭式表达式，以计算给定测试用例所需的数值结果。\n\n**任务 1：基本量的推导**\n\n给定一个服从标准正态分布 $X \\sim \\mathcal{N}(0,1)$ 的随机变量和一个函数 $g(x) = \\exp(\\alpha x)$。控制变量为 $Y=X$。\n\n首先，我们回顾正态随机变量 $Z \\sim \\mathcal{N}(\\mu_N, \\sigma_N^2)$ 的矩生成函数 (MGF)，即 $M_Z(s) = \\mathbb{E}[e^{sZ}] = \\exp(\\mu_N s + \\frac{1}{2}\\sigma_N^2 s^2)$。对于我们的标准正态变量 $X$，有 $\\mu_N=0$ 和 $\\sigma_N^2=1$，所以其 MGF 为 $M_X(s) = \\exp(\\frac{1}{2}s^2)$。\n\n1.  **$\\mu = \\mathbb{E}[g(X)]$ 的推导**\n    期望 $\\mu$ 定义为 $\\mathbb{E}[g(X)] = \\mathbb{E}[\\exp(\\alpha X)]$。这恰好是 $X$ 的 MGF 在 $s=\\alpha$ 处的值。\n    $$\n    \\mu = M_X(\\alpha) = \\exp\\left(\\frac{\\alpha^2}{2}\\right)\n    $$\n\n2.  **$\\operatorname{Var}(g(X))$ 的推导**\n    方差由 $\\operatorname{Var}(g(X)) = \\mathbb{E}[g(X)^2] - (\\mathbb{E}[g(X)])^2$ 给出。\n    第一项是 $\\mathbb{E}[g(X)^2] = \\mathbb{E}[(\\exp(\\alpha X))^2] = \\mathbb{E}[\\exp(2\\alpha X)]$。这是 $X$ 的 MGF 在 $s=2\\alpha$ 处的值。\n    $$\n    \\mathbb{E}[g(X)^2] = M_X(2\\alpha) = \\exp\\left(\\frac{1}{2}(2\\alpha)^2\\right) = \\exp(2\\alpha^2)\n    $$\n    将此结果和 $\\mu$ 的表达式代入方差公式：\n    $$\n    \\operatorname{Var}(g(X)) = \\exp(2\\alpha^2) - \\left(\\exp\\left(\\frac{\\alpha^2}{2}\\right)\\right)^2 = \\exp(2\\alpha^2) - \\exp(\\alpha^2)\n    $$\n\n3.  **$\\operatorname{Cov}(g(X), Y)$ 的推导**\n    由于 $Y=X$，我们需要计算 $\\operatorname{Cov}(g(X), X)$。协方差定义为 $\\operatorname{Cov}(g(X), X) = \\mathbb{E}[X \\cdot g(X)] - \\mathbb{E}[X]\\mathbb{E}[g(X)]$。\n    由于 $X \\sim \\mathcal{N}(0,1)$，其期望 $\\mathbb{E}[X]=0$，这使协方差简化为 $\\operatorname{Cov}(g(X), X) = \\mathbb{E}[X \\exp(\\alpha X)]$。\n    这个期望可以通过对 MGF $M_X(s)$ 对 $s$ 求导并在 $s=\\alpha$ 处求值得到。\n    $$\n    \\frac{d}{ds}M_X(s) = \\frac{d}{ds}\\mathbb{E}[e^{sX}] = \\mathbb{E}\\left[\\frac{d}{ds}e^{sX}\\right] = \\mathbb{E}[X e^{sX}]\n    $$\n    $M_X(s) = \\exp(\\frac{1}{2}s^2)$ 的导数是 $\\frac{d}{ds}\\exp(\\frac{1}{2}s^2) = s \\exp(\\frac{1}{2}s^2)$。\n    在 $s=\\alpha$ 处求值：\n    $$\n    \\mathbb{E}[X \\exp(\\alpha X)] = \\alpha \\exp\\left(\\frac{\\alpha^2}{2}\\right)\n    $$\n    因此，协方差为：\n    $$\n    \\operatorname{Cov}(g(X), X) = \\alpha \\exp\\left(\\frac{\\alpha^2}{2}\\right)\n    $$\n\n**任务 2：估计量分析**\n\n设 $\\{X_i\\}_{i=1}^n$ 是来自 $X \\sim \\mathcal{N}(0,1)$ 的独立同分布样本。并设 $\\bar{g}_n = \\frac{1}{n}\\sum_{i=1}^n g(X_i)$ 和 $\\bar{Y}_n = \\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$。\n\n1.  **估计量 1: $\\hat{\\theta}_{\\mathrm{mean}}$**\n    -   **偏差**: $\\operatorname{Bias}(\\hat{\\theta}_{\\mathrm{mean}}) = \\mathbb{E}[\\bar{g}_n] - \\mu = \\mathbb{E}[\\frac{1}{n}\\sum_i g(X_i)] - \\mu = \\frac{1}{n}\\sum_i \\mathbb{E}[g(X_i)] - \\mu = \\frac{1}{n}(n\\mu) - \\mu = 0$。该估计量是无偏的。\n    -   **方差**: $\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{mean}}) = \\operatorname{Var}(\\bar{g}_n) = \\operatorname{Var}(\\frac{1}{n}\\sum_i g(X_i)) = \\frac{1}{n^2}\\sum_i \\operatorname{Var}(g(X_i)) = \\frac{n}{n^2}\\operatorname{Var}(g(X)) = \\frac{1}{n}\\operatorname{Var}(g(X))$。\n        $$\n        \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{mean}}) = \\frac{1}{n}(\\exp(2\\alpha^2) - \\exp(\\alpha^2))\n        $$\n    -   **均方误差**: $\\operatorname{MSE}(\\hat{\\theta}_{\\mathrm{mean}}) = \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{mean}}) + (\\operatorname{Bias}(\\hat{\\theta}_{\\mathrm{mean}}))^2 = \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{mean}})$。\n        $$\n        \\operatorname{MSE}(\\hat{\\theta}_{\\mathrm{mean}}) = \\frac{1}{n}(\\exp(2\\alpha^2) - \\exp(\\alpha^2))\n        $$\n\n2.  **估计量 2: $\\hat{\\theta}_{\\mathrm{cv}}(c)$**\n    -   **偏差**: $\\mathbb{E}[\\hat{\\theta}_{\\mathrm{cv}}(c)] = \\mathbb{E}[\\bar{g}_n - c\\bar{Y}_n] = \\mathbb{E}[\\bar{g}_n] - c\\mathbb{E}[\\bar{Y}_n] = \\mu - c \\cdot 0 = \\mu$。对于任何 $c$，该估计量都是无偏的。因此，$\\operatorname{Bias}(\\hat{\\theta}_{\\mathrm{cv}}(c))=0$。\n    -   **方差**: $\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{cv}}(c)) = \\operatorname{Var}(\\bar{g}_n - c\\bar{Y}_n) = \\operatorname{Var}(\\bar{g}_n) + c^2\\operatorname{Var}(\\bar{Y}_n) - 2c\\operatorname{Cov}(\\bar{g}_n, \\bar{Y}_n)$。\n        我们有 $\\operatorname{Var}(\\bar{g}_n) = \\frac{1}{n}\\operatorname{Var}(g(X))$，$\\operatorname{Var}(\\bar{Y}_n) = \\frac{1}{n}\\operatorname{Var}(X) = \\frac{1}{n}$，以及 $\\operatorname{Cov}(\\bar{g}_n, \\bar{Y}_n) = \\frac{1}{n}\\operatorname{Cov}(g(X),X)$。\n        $$\n        \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{cv}}(c)) = \\frac{1}{n}[\\operatorname{Var}(g(X)) + c^2\\operatorname{Var}(X) - 2c\\operatorname{Cov}(g(X),X)]\n        $$\n        为了找到使该方差最小化的最优系数 $c^*$，我们对 $c$ 求导并令其为 $0$：\n        $$\n        \\frac{d}{dc}\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{cv}}(c)) = \\frac{1}{n}[2c\\operatorname{Var}(X) - 2\\operatorname{Cov}(g(X),X)] = 0\n        $$\n        这给出了众所周知的结果 $c^* = \\frac{\\operatorname{Cov}(g(X),X)}{\\operatorname{Var}(X)}$。使用我们推导的量和 $\\operatorname{Var}(X)=1$：\n        $$\n        c^* = \\frac{\\alpha \\exp(\\alpha^2/2)}{1} = \\alpha \\exp\\left(\\frac{\\alpha^2}{2}\\right)\n        $$\n    -   **最小化方差和均方误差**: 最小化方差为 $\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{cv}}(c^*)) = \\frac{1}{n}(\\operatorname{Var}(g(X)) - \\frac{\\operatorname{Cov}(g(X),X)^2}{\\operatorname{Var}(X)})$。\n        $$\n        \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{cv}}(c^*)) = \\frac{1}{n}\\left[ (\\exp(2\\alpha^2) - \\exp(\\alpha^2)) - \\frac{(\\alpha \\exp(\\alpha^2/2))^2}{1} \\right] \\\\\n        = \\frac{1}{n}\\left[ \\exp(2\\alpha^2) - \\exp(\\alpha^2) - \\alpha^2 \\exp(\\alpha^2) \\right] \\\\\n        = \\frac{1}{n}\\left[ \\exp(2\\alpha^2) - (1+\\alpha^2)\\exp(\\alpha^2) \\right]\n        $$\n        由于估计量是无偏的，$\\operatorname{MSE}(\\hat{\\theta}_{\\mathrm{cv}}(c^*)) = \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{cv}}(c^*))$。\n\n3.  **估计量 3: $\\hat{\\theta}_{\\lambda}$**\n    -   **偏差**: $\\mathbb{E}[\\hat{\\theta}_{\\lambda}] = \\mathbb{E}[(1-\\lambda)\\bar{g}_n + \\lambda t] = (1-\\lambda)\\mathbb{E}[\\bar{g}_n] + \\lambda t = (1-\\lambda)\\mu + \\lambda t$。\n        $\\operatorname{Bias}(\\hat{\\theta}_{\\lambda}) = \\mathbb{E}[\\hat{\\theta}_{\\lambda}] - \\mu = (1-\\lambda)\\mu + \\lambda t - \\mu = \\lambda(t-\\mu)$。\n    -   **方差**: $\\operatorname{Var}(\\hat{\\theta}_{\\lambda}) = \\operatorname{Var}((1-\\lambda)\\bar{g}_n + \\lambda t) = (1-\\lambda)^2\\operatorname{Var}(\\bar{g}_n) = \\frac{(1-\\lambda)^2}{n}\\operatorname{Var}(g(X))$。\n    -   **均方误差**: $\\operatorname{MSE}(\\hat{\\theta}_{\\lambda}) = \\operatorname{Var}(\\hat{\\theta}_{\\lambda}) + (\\operatorname{Bias}(\\hat{\\theta}_{\\lambda}))^2$。\n        $$\n        \\operatorname{MSE}(\\hat{\\theta}_{\\lambda}) = \\frac{(1-\\lambda)^2}{n}\\operatorname{Var}(g(X)) + \\lambda^2(t-\\mu)^2\n        $$\n\n**任务 3：优化收缩估计量**\n\n我们希望关于 $\\lambda \\in [0,1]$ 最小化 $J(\\lambda) = \\operatorname{MSE}(\\hat{\\theta}_{\\lambda})$。\n$$\nJ(\\lambda) = \\frac{(1-\\lambda)^2}{n}\\operatorname{Var}(g(X)) + \\lambda^2(t-\\mu)^2\n$$\n这是一个关于 $\\lambda$ 的二次函数。我们通过将其导数设为零来找到最小值：\n$$\n\\frac{dJ}{d\\lambda} = -2\\frac{(1-\\lambda)}{n}\\operatorname{Var}(g(X)) + 2\\lambda(t-\\mu)^2 = 0\n$$\n$$\n\\lambda((t-\\mu)^2 + \\frac{\\operatorname{Var}(g(X))}{n}) = \\frac{\\operatorname{Var}(g(X))}{n}\n$$\n无约束最小化子是：\n$$\n\\lambda_{unc} = \\frac{\\frac{\\operatorname{Var}(g(X))}{n}}{\\frac{\\operatorname{Var}(g(X))}{n} + (t-\\mu)^2}\n$$\n二阶导数 $\\frac{d^2J}{d\\lambda^2} = 2(\\frac{\\operatorname{Var}(g(X))}{n} + (t-\\mu)^2)$ 是非负的，这证实了是一个最小值。由于 $\\operatorname{Var}(g(X)) \\ge 0$ 和 $(t-\\mu)^2 \\ge 0$，$\\lambda_{unc}$ 的分子和分母都是非负的。分母大于或等于分子，所以 $0 \\le \\lambda_{unc} \\le 1$。因此，无约束最小值位于区间 $[0,1]$ 内，并且它是该区间内唯一的最小化子 $\\lambda^*$ (假设分母不为零，这对所有测试用例都成立)。\n$$\n\\lambda^* = \\frac{\\frac{\\operatorname{Var}(g(X))}{n}}{\\frac{\\operatorname{Var}(g(X))}{n} + (t-\\mu)^2}\n$$\n为了找到最小化的均方误差，我们可以将 $\\lambda^*$ 代回均方误差公式。设 $A = \\frac{\\operatorname{Var}(g(X))}{n}$ 和 $B = (t-\\mu)^2$。那么 $\\lambda^* = \\frac{A}{A+B}$ 且 $1-\\lambda^* = \\frac{B}{A+B}$。\n$$\n\\operatorname{MSE}(\\hat{\\theta}_{\\lambda^*}) = (1-\\lambda^*)^2 A + (\\lambda^*)^2 B = \\left(\\frac{B}{A+B}\\right)^2 A + \\left(\\frac{A}{A+B}\\right)^2 B \\\\\n= \\frac{AB^2+A^2B}{(A+B)^2} = \\frac{AB(A+B)}{(A+B)^2} = \\frac{AB}{A+B}\n$$\n$$\n\\operatorname{MSE}(\\hat{\\theta}_{\\lambda^*}) = \\frac{\\left(\\frac{\\operatorname{Var}(g(X))}{n}\\right)(t-\\mu)^2}{\\frac{\\operatorname{Var}(g(X))}{n} + (t-\\mu)^2}\n$$\n如果分母为零，则意味着 $\\operatorname{Var}(g(X))=0$ 和 $t-\\mu=0$。在这种情况下，对于任何 $\\lambda$，均方误差都为零，而公式给出 $0/0$。我们将通过返回 $0$ 来处理这种情况。\n\n**用于实现的公式总结：**\n对于给定的参数 $\\alpha, n, t, \\lambda$：\n1.  $\\mu = \\exp\\left(\\frac{\\alpha^2}{2}\\right)$\n2.  $\\operatorname{Var}_g = \\exp(2\\alpha^2) - \\exp(\\alpha^2)$\n3.  $\\operatorname{MSE}_{\\mathrm{mean}} = \\frac{1}{n} \\operatorname{Var}_g$\n4.  $\\operatorname{MSE}_{\\mathrm{cv}} = \\frac{1}{n}(\\operatorname{Var}_g - \\alpha^2 \\exp(\\alpha^2)) = \\frac{1}{n}(\\exp(2\\alpha^2) - (1+\\alpha^2)\\exp(\\alpha^2))$\n5.  $\\operatorname{MSE}_{\\lambda} = (1-\\lambda)^2 \\frac{\\operatorname{Var}_g}{n} + \\lambda^2(t-\\mu)^2$\n6.  $A = \\frac{\\operatorname{Var}_g}{n}$， $B = (t-\\mu)^2$。若 $A+B > 0$，则 $\\lambda^* = A / (A+B)$，否则为 $0$。\n7.  若 $A+B > 0$，则 $\\operatorname{MSE}_{\\lambda^*} = (A \\cdot B) / (A+B)$，否则为 $0$。\n\n现在可以实现这些公式了。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing the derived analytical formulas for the MSE\n    of three different Monte Carlo estimators.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # The parameters are (alpha, n, t, lambda).\n    # For cases B and E, 't' is defined based on alpha, so we compute it here.\n    alpha_B = 0.5\n    t_B = np.exp(alpha_B**2 / 2.0)\n    \n    alpha_E = 1.5\n    t_E = np.exp(alpha_E**2 / 2.0) + 5.0\n    \n    test_cases = [\n        # Case A: happy path\n        (0.5, 10000, 1.0, 0.3),\n        # Case B: target equals the true mean\n        (alpha_B, 10000, t_B, 0.7),\n        # Case C: small sample size and distant target\n        (1.0, 100, 0.0, 0.5),\n        # Case D: degenerate function yielding zero variance\n        (0.0, 1000, 2.0, 0.9),\n        # Case E: large sample size and very distant target\n        (alpha_E, 50000, t_E, 0.2),\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        alpha, n, t, lambda_in = case\n        \n        # Derived formulas start here.\n        # Ensure all variables are floats for calculations.\n        alpha, n, t, lambda_in = float(alpha), float(n), float(t), float(lambda_in)\n\n        # Task 1: Fundamental Quantities\n        # mu = E[g(X)]\n        mu = np.exp(alpha**2 / 2.0)\n        # var_g = Var(g(X))\n        var_g = np.exp(2.0 * alpha**2) - np.exp(alpha**2)\n        \n        # Task 2: Estimator Analysis  Task 3: Shrinkage Optimization\n        \n        # 1. MSE of the sample mean estimator\n        mse_mean = var_g / n\n        \n        # 2. MSE of the optimal control variate estimator\n        # Check for alpha=0 case to avoid issues, though formula works.\n        # The variance reduction term is alpha^2 * exp(alpha^2)\n        variance_reduction = alpha**2 * np.exp(alpha**2)\n        mse_cv_star = (var_g - variance_reduction) / n\n\n        # 3. MSE of the shrinkage estimator for the given lambda\n        var_term_shrink = ((1.0 - lambda_in)**2) * (var_g / n)\n        bias_term_shrink = (lambda_in**2) * ((t - mu)**2)\n        mse_lambda = var_term_shrink + bias_term_shrink\n\n        # 4. Optimal lambda* for the shrinkage estimator\n        A = var_g / n\n        B = (t - mu)**2\n        \n        denominator_lambda_star = A + B\n        \n        if np.isclose(denominator_lambda_star, 0.0):\n             # This occurs if var_g=0 (e.g., alpha=0) AND t=mu.\n             # MSE is 0 for any lambda. Any lambda is optimal.\n             # We can choose a convention, e.g., 0, since it doesn't matter.\n            lambda_star = 0.0\n        else:\n            lambda_star = A / denominator_lambda_star\n            \n        # 5. Minimized MSE of the shrinkage estimator at lambda*\n        if np.isclose(denominator_lambda_star, 0.0):\n            # If A+B is 0, then A and B must both be 0. MSE is 0.\n            mse_lambda_star = 0.0\n        else:\n            mse_lambda_star = (A * B) / denominator_lambda_star\n            \n        case_results = [\n            mse_mean,\n            mse_cv_star,\n            mse_lambda,\n            lambda_star,\n            mse_lambda_star,\n        ]\n        all_results.append(case_results)\n\n    # Format the final output list of lists into a string\n    # e.g., [[r1,r2,...],[r'1,r'2,...]]\n    def format_list(lst):\n        # Format numbers to a high precision, then let Python's default string conversion handle it\n        # This avoids fixed-point notation for very small or large numbers.\n        return f\"[{','.join(map(str, lst))}]\"\n\n    final_output_string = f\"[{','.join(format_list(res) for res in all_results)}]\"\n    # Python's default float-to-string conversion is sufficient for the required precision.\n\n    # Final print statement in the exact required format.\n    print(final_output_string)\n\nsolve()\n```"
        }
    ]
}