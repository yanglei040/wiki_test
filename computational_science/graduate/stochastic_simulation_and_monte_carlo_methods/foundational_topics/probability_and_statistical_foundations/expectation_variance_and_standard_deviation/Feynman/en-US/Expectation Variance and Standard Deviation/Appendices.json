{
    "hands_on_practices": [
        {
            "introduction": "In Monte Carlo simulations, we can often generate multiple estimators for the same quantity, for example by using different control variates. A natural question arises: how can we combine these estimators to produce a single, improved estimate? This practice guides you through the foundational process of finding the optimal linear combination of correlated, unbiased estimators that minimizes the overall variance, a cornerstone technique in variance reduction .",
            "id": "3307448",
            "problem": "Consider a stochastic simulation context in Monte Carlo (MC) where $3$ correlated unbiased estimators $X_{1}$, $X_{2}$, and $X_{3}$ are produced using common random numbers (CRN) and control variates for the same target quantity $\\theta \\in \\mathbb{R}$. Each $X_{i}$ has finite second moments, and their joint covariance matrix is known and given by\n$$\n\\Sigma \\;=\\;\n\\begin{pmatrix}\n2  1  0 \\\\\n1  2  1 \\\\\n0  1  2\n\\end{pmatrix}.\n$$\nDefine the weighted average estimator\n$$\n\\widehat{\\theta}(w) \\;=\\; w_{1} X_{1} \\;+\\; w_{2} X_{2} \\;+\\; w_{3} X_{3},\n$$\nwhere the weights $w = (w_{1}, w_{2}, w_{3})^{\\top}$ satisfy the unbiasedness constraint $\\sum_{i=1}^{3} w_{i} = 1$.\n\nStarting strictly from the foundational definitions of expectation, variance, and covariance, and without invoking any prepackaged formulas for linear combinations, perform the following:\n\n- Derive the general expression for $\\operatorname{Var}(\\widehat{\\theta}(w))$ in terms of the weights and the covariances of $(X_{1}, X_{2}, X_{3})$.\n- Enforce the unbiasedness constraint and determine the weights $w$ that minimize $\\operatorname{Var}(\\widehat{\\theta}(w))$.\n- Using the given $\\Sigma$, compute the minimum value of $\\operatorname{Var}(\\widehat{\\theta}(w))$.\n\nExpress the final answer as an exact real number; no rounding is required. Do not report intermediate quantities such as the optimal weights; only the minimum variance value is required as the final answer.",
            "solution": "The problem as stated is scientifically grounded, well-posed, and objective. It presents a standard task in statistical estimation theory, specifically the optimization of a linear combination of estimators to minimize variance. All necessary data and constraints are provided, and there are no internal contradictions or violations of mathematical principles. We may therefore proceed with a full solution.\n\nThe problem asks for three tasks: first, to derive the general expression for the variance of a weighted estimator; second, to find the weights that minimize this variance under an unbiasedness constraint; and third, to compute this minimum variance for a specific covariance matrix.\n\nLet the $3$ correlated, unbiased estimators for a quantity $\\theta$ be $X_1$, $X_2$, and $X_3$. Since they are unbiased, their expectation is $E[X_i] = \\theta$ for $i \\in \\{1, 2, 3\\}$. The weighted average estimator is defined as $\\widehat{\\theta}(w) = w_1 X_1 + w_2 X_2 + w_3 X_3 = \\sum_{i=1}^3 w_i X_i$. The weights $w = (w_1, w_2, w_3)^\\top$ are real numbers that must satisfy the constraint $\\sum_{i=1}^3 w_i = 1$ to ensure that $\\widehat{\\theta}(w)$ is also an unbiased estimator of $\\theta$. We can verify this:\n$$\nE[\\widehat{\\theta}(w)] = E\\left[\\sum_{i=1}^3 w_i X_i\\right] = \\sum_{i=1}^3 w_i E[X_i] = \\sum_{i=1}^3 w_i \\theta = \\theta \\sum_{i=1}^3 w_i = \\theta \\cdot 1 = \\theta.\n$$\n\nFirst, we derive the expression for the variance of $\\widehat{\\theta}(w)$, denoted as $\\operatorname{Var}(\\widehat{\\theta}(w))$. By the fundamental definition of variance:\n$$\n\\operatorname{Var}(\\widehat{\\theta}(w)) = E\\left[ (\\widehat{\\theta}(w) - E[\\widehat{\\theta}(w)])^2 \\right] = E\\left[ \\left(\\sum_{i=1}^3 w_i X_i - \\theta \\right)^2 \\right].\n$$\nUsing the constraint $\\sum_{i=1}^3 w_i = 1$, we can write $\\theta = \\sum_{i=1}^3 w_i \\theta$. Substituting this into the expression gives:\n$$\n\\operatorname{Var}(\\widehat{\\theta}(w)) = E\\left[ \\left(\\sum_{i=1}^3 w_i X_i - \\sum_{i=1}^3 w_i \\theta \\right)^2 \\right] = E\\left[ \\left(\\sum_{i=1}^3 w_i (X_i - \\theta) \\right)^2 \\right].\n$$\nLet's expand the squared sum:\n$$\n\\left(\\sum_{i=1}^3 w_i (X_i - \\theta) \\right)^2 = \\left(\\sum_{i=1}^3 w_i (X_i - \\theta) \\right) \\left(\\sum_{j=1}^3 w_j (X_j - \\theta) \\right) = \\sum_{i=1}^3 \\sum_{j=1}^3 w_i w_j (X_i - \\theta)(X_j - \\theta).\n$$\nApplying the expectation operator and using its linearity:\n$$\n\\operatorname{Var}(\\widehat{\\theta}(w)) = E\\left[\\sum_{i=1}^3 \\sum_{j=1}^3 w_i w_j (X_i - \\theta)(X_j - \\theta)\\right] = \\sum_{i=1}^3 \\sum_{j=1}^3 w_i w_j E[(X_i - \\theta)(X_j - \\theta)].\n$$\nThe term $E[(X_i - \\theta)(X_j - \\theta)]$ is, by definition, the covariance between $X_i$ and $X_j$, denoted $\\operatorname{Cov}(X_i, X_j)$. The problem provides the covariance matrix $\\Sigma$, where $\\Sigma_{ij} = \\operatorname{Cov}(X_i, X_j)$. Thus, the variance of the estimator is:\n$$\n\\operatorname{Var}(\\widehat{\\theta}(w)) = \\sum_{i=1}^3 \\sum_{j=1}^3 w_i w_j \\Sigma_{ij}.\n$$\nIn matrix notation, this is expressed compactly as $\\operatorname{Var}(\\widehat{\\theta}(w)) = w^\\top \\Sigma w$.\n\nSecond, we must find the weights $w$ that minimize this variance subject to the constraint $\\sum_{i=1}^3 w_i = 1$. This is a constrained optimization problem. We can use the method of Lagrange multipliers. The objective function to minimize is $f(w) = w^\\top \\Sigma w$, and the constraint is $g(w) = w^\\top \\mathbf{1} - 1 = 0$, where $\\mathbf{1}$ is a column vector of ones. The Lagrangian is:\n$$\n\\mathcal{L}(w, \\lambda) = w^\\top \\Sigma w - \\lambda(w^\\top \\mathbf{1} - 1).\n$$\nTo find the minimum, we compute the gradient of $\\mathcal{L}$ with respect to $w$ and set it to zero. The gradient of the quadratic form $w^\\top \\Sigma w$ is $2\\Sigma w$ (since $\\Sigma$ is symmetric), and the gradient of $w^\\top \\mathbf{1}$ is $\\mathbf{1}$.\n$$\n\\nabla_w \\mathcal{L} = 2\\Sigma w - \\lambda\\mathbf{1} = \\mathbf{0}.\n$$\nThis gives us $2\\Sigma w = \\lambda\\mathbf{1}$. Assuming $\\Sigma$ is invertible, we can write $w = \\frac{\\lambda}{2} \\Sigma^{-1} \\mathbf{1}$.\nTo solve for the Lagrange multiplier $\\lambda$, we substitute this expression for $w$ back into the constraint equation $w^\\top \\mathbf{1} = 1$:\n$$\n\\left(\\frac{\\lambda}{2} \\Sigma^{-1} \\mathbf{1}\\right)^\\top \\mathbf{1} = 1 \\implies \\frac{\\lambda}{2} (\\Sigma^{-1} \\mathbf{1})^\\top \\mathbf{1} = 1.\n$$\nSince $\\Sigma$ is symmetric, $\\Sigma^{-1}$ is also symmetric, so $(\\Sigma^{-1})^\\top = \\Sigma^{-1}$.\n$$\n\\frac{\\lambda}{2} \\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1} = 1 \\implies \\lambda = \\frac{2}{\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}}.\n$$\nThe optimal weight vector $w_{opt}$ is therefore:\n$$\nw_{opt} = \\frac{1}{\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}} \\Sigma^{-1} \\mathbf{1}.\n$$\nThe minimum variance is obtained by substituting $w_{opt}$ into the variance expression:\n$$\n\\operatorname{Var}_{min} = w_{opt}^\\top \\Sigma w_{opt} = \\left(\\frac{\\Sigma^{-1} \\mathbf{1}}{\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}}\\right)^\\top \\Sigma \\left(\\frac{\\Sigma^{-1} \\mathbf{1}}{\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}}\\right).\n$$\nThis simplifies to:\n$$\n\\operatorname{Var}_{min} = \\frac{\\mathbf{1}^\\top (\\Sigma^{-1})^\\top \\Sigma \\Sigma^{-1} \\mathbf{1}}{(\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1})^2} = \\frac{\\mathbf{1}^\\top \\Sigma^{-1} \\Sigma \\Sigma^{-1} \\mathbf{1}}{(\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1})^2} = \\frac{\\mathbf{1}^\\top I \\Sigma^{-1} \\mathbf{1}}{(\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1})^2} = \\frac{\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}}{(\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1})^2} = \\frac{1}{\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}}.\n$$\n\nThird, we compute this minimum variance using the given covariance matrix:\n$$\n\\Sigma = \\begin{pmatrix} 2  1  0 \\\\ 1  2  1 \\\\ 0  1  2 \\end{pmatrix}.\n$$\nTo find the minimum variance, we need to calculate $\\Sigma^{-1}$. The determinant of $\\Sigma$ is:\n$$\n\\det(\\Sigma) = 2(2 \\cdot 2 - 1 \\cdot 1) - 1(1 \\cdot 2 - 1 \\cdot 0) + 0(1 \\cdot 1 - 2 \\cdot 0) = 2(3) - 1(2) = 4.\n$$\nThe adjugate matrix, $\\operatorname{adj}(\\Sigma)$, is the transpose of the cofactor matrix:\n$$\n\\operatorname{adj}(\\Sigma) = \\begin{pmatrix}\n\\begin{vmatrix} 2  1 \\\\ 1  2 \\end{vmatrix}  -\\begin{vmatrix} 1  1 \\\\ 0  2 \\end{vmatrix}  \\begin{vmatrix} 1  2 \\\\ 0  1 \\end{vmatrix} \\\\\n-\\begin{vmatrix} 1  0 \\\\ 1  2 \\end{vmatrix}  \\begin{vmatrix} 2  0 \\\\ 0  2 \\end{vmatrix}  -\\begin{vmatrix} 2  1 \\\\ 0  1 \\end{vmatrix} \\\\\n\\begin{vmatrix} 1  0 \\\\ 2  1 \\end{vmatrix}  -\\begin{vmatrix} 2  0 \\\\ 1  1 \\end{vmatrix}  \\begin{vmatrix} 2  1 \\\\ 1  2 \\end{vmatrix}\n\\end{pmatrix}^\\top\n= \\begin{pmatrix} 3  -2  1 \\\\ -2  4  -2 \\\\ 1  -2  3 \\end{pmatrix}^\\top\n= \\begin{pmatrix} 3  -2  1 \\\\ -2  4  -2 \\\\ 1  -2  3 \\end{pmatrix}.\n$$\nThe inverse matrix is $\\Sigma^{-1} = \\frac{1}{\\det(\\Sigma)}\\operatorname{adj}(\\Sigma)$:\n$$\n\\Sigma^{-1} = \\frac{1}{4} \\begin{pmatrix} 3  -2  1 \\\\ -2  4  -2 \\\\ 1  -2  3 \\end{pmatrix}.\n$$\nNow, we compute the quantity $\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}$, which is the sum of all elements of $\\Sigma^{-1}$:\n$$\n\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1} = \\frac{1}{4} (3 - 2 + 1 - 2 + 4 - 2 + 1 - 2 + 3) = \\frac{1}{4}(4) = 1.\n$$\nFinally, the minimum variance is:\n$$\n\\operatorname{Var}_{min} = \\frac{1}{\\mathbf{1}^\\top \\Sigma^{-1} \\mathbf{1}} = \\frac{1}{1} = 1.\n$$\nThe minimum variance of the combined estimator is $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "While minimizing variance is a primary goal, the ultimate objective is often to minimize the total Mean Squared Error ($MSE$). This exercise delves into the crucial bias-variance trade-off, a central concept in statistics and machine learning. By comparing an unbiased control variate estimator with a biased shrinkage estimator, you will see firsthand how intentionally introducing a small amount of bias can sometimes lead to a significant reduction in variance, resulting in a more accurate estimator overall .",
            "id": "3307435",
            "problem": "Consider a random variable $X$ distributed as a standard normal distribution $\\mathcal{N}(0,1)$, and a measurable function $g$ defined by $g(x) = \\exp(\\alpha x)$ for a real parameter $\\alpha$. The target of interest is the expectation $\\mu = \\mathbb{E}[g(X)]$. You will investigate three Monte Carlo estimators of $\\mu$ based on $n$ independent and identically distributed samples $\\{X_i\\}_{i=1}^n$ from $X$, focusing on how bias and variance interact in reducing mean squared error through variance reduction methods.\n\nEstimator $1$ (sample mean): Define $\\bar{g}_n = \\frac{1}{n}\\sum_{i=1}^n g(X_i)$ and the estimator $\\hat{\\theta}_{\\mathrm{mean}} = \\bar{g}_n$.\n\nEstimator $2$ (control variate with known mean): Let $Y = X$ with known expectation $\\mathbb{E}[Y] = 0$. For any real coefficient $c$, define the control variate estimator\n$$\n\\hat{\\theta}_{\\mathrm{cv}}(c) = \\bar{g}_n - c \\,\\bar{Y}_n,\n$$\nwhere $\\bar{Y}_n = \\frac{1}{n}\\sum_{i=1}^n Y_i$.\n\nEstimator $3$ (shrinkage toward a target): For a given target $t \\in \\mathbb{R}$ and a shrinkage parameter $\\lambda \\in [0,1]$, define the estimator\n$$\n\\hat{\\theta}_{\\lambda} = (1 - \\lambda)\\,\\bar{g}_n + \\lambda\\, t.\n$$\n\nTasks:\n$1.$ Starting from the definitions of expectation and variance and the properties of the moment generating function of the standard normal distribution, derive closed-form expressions for $\\mu = \\mathbb{E}[g(X)]$, $\\operatorname{Var}\\big(g(X)\\big)$, and $\\operatorname{Cov}\\big(g(X), Y\\big)$.\n\n$2.$ Using the definitions of bias, variance, and mean squared error, determine $\\operatorname{Bias}\\big(\\hat{\\theta}_{\\mathrm{mean}}\\big)$, $\\operatorname{Var}\\big(\\hat{\\theta}_{\\mathrm{mean}}\\big)$, and $\\operatorname{MSE}\\big(\\hat{\\theta}_{\\mathrm{mean}}\\big)$. For the control variate estimator, compute the value $c^*$ that minimizes $\\operatorname{Var}\\big(\\hat{\\theta}_{\\mathrm{cv}}(c)\\big)$ and give the corresponding minimized variance and mean squared error. For the shrinkage estimator, compute $\\operatorname{Bias}\\big(\\hat{\\theta}_{\\lambda}\\big)$, $\\operatorname{Var}\\big(\\hat{\\theta}_{\\lambda}\\big)$, and $\\operatorname{MSE}\\big(\\hat{\\theta}_{\\lambda}\\big)$ in closed form.\n\n$3.$ Show that the mean squared error $\\operatorname{MSE}\\big(\\hat{\\theta}_{\\lambda}\\big)$ is minimized at a unique $\\lambda^* \\in [0,1]$, and derive a closed-form expression for $\\lambda^*$ and the minimized mean squared error $\\operatorname{MSE}\\big(\\hat{\\theta}_{\\lambda^*}\\big)$.\n\nProgram requirements:\n- Implement a program that, for each test case specified below, computes the following $5$ quantities using your derived formulas:\n  $1.$ $\\operatorname{MSE}\\big(\\hat{\\theta}_{\\mathrm{mean}}\\big)$,\n  $2.$ $\\operatorname{MSE}\\big(\\hat{\\theta}_{\\mathrm{cv}}(c^*)\\big)$,\n  $3.$ $\\operatorname{MSE}\\big(\\hat{\\theta}_{\\lambda}\\big)$ evaluated at the given $\\lambda$,\n  $4.$ $\\lambda^*$ that minimizes $\\operatorname{MSE}\\big(\\hat{\\theta}_{\\lambda}\\big)$ over $[0,1]$,\n  $5.$ $\\operatorname{MSE}\\big(\\hat{\\theta}_{\\lambda^*}\\big)$.\n- The program must not rely on random simulation; it should compute the quantities exactly from the derived analytic expressions.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s $5$ results should appear as a comma-separated list enclosed in square brackets, and all test case lists should be aggregated into a single outer list (for example, $[[r_{1,1},r_{1,2},r_{1,3},r_{1,4},r_{1,5}],[r_{2,1},r_{2,2},r_{2,3},r_{2,4},r_{2,5}],\\dots]$).\n\nTest suite:\n- Case A (happy path): $(\\alpha, n, t, \\lambda) = (0.5, 10000, 1.0, 0.3)$.\n- Case B (boundary where the target equals the true mean): $(\\alpha, n, t, \\lambda) = (0.5, 10000, \\exp(\\alpha^2/2), 0.7)$.\n- Case C (small sample size and distant target): $(\\alpha, n, t, \\lambda) = (1.0, 100, 0.0, 0.5)$.\n- Case D (degenerate function yielding zero variance): $(\\alpha, n, t, \\lambda) = (0.0, 1000, 2.0, 0.9)$.\n- Case E (large sample size and very distant target): $(\\alpha, n, t, \\lambda) = (1.5, 50000, \\exp(\\alpha^2/2) + 5.0, 0.2)$.\n\nAll numerical results must be reported as real-valued floats. No physical units or angle units apply in this problem, and no percentages should be used; express any ratios as decimals.",
            "solution": "The problem is valid as it is scientifically grounded in probability theory and statistics, well-posed with sufficient information for a unique solution, and objectively stated. It involves standard concepts such as expectation, variance, bias, mean squared error, and Monte Carlo estimators, without any factual or logical inconsistencies.\n\nThe solution proceeds by first deriving the necessary theoretical quantities as requested in Tasks $1$, $2$, and $3$. These closed-form expressions will then be implemented in a program to compute the required numerical results for the given test cases.\n\n**Task 1: Derivations of Fundamental Quantities**\n\nWe are given a random variable $X$ following a standard normal distribution, $X \\sim \\mathcal{N}(0,1)$, and a function $g(x) = \\exp(\\alpha x)$. The control variate is $Y=X$.\n\nFirst, we recall the moment-generating function (MGF) of a normal random variable $Z \\sim \\mathcal{N}(\\mu_N, \\sigma_N^2)$, which is $M_Z(s) = \\mathbb{E}[e^{sZ}] = \\exp(\\mu_N s + \\frac{1}{2}\\sigma_N^2 s^2)$. For our standard normal variable $X$, we have $\\mu_N=0$ and $\\sigma_N^2=1$, so its MGF is $M_X(s) = \\exp(\\frac{1}{2}s^2)$.\n\n1.  **Derivation of $\\mu = \\mathbb{E}[g(X)]$**\n    The expectation $\\mu$ is defined as $\\mathbb{E}[g(X)] = \\mathbb{E}[\\exp(\\alpha X)]$. This is precisely the MGF of $X$ evaluated at $s=\\alpha$.\n    $$\n    \\mu = M_X(\\alpha) = \\exp\\left(\\frac{\\alpha^2}{2}\\right)\n    $$\n\n2.  **Derivation of $\\operatorname{Var}(g(X))$**\n    The variance is given by $\\operatorname{Var}(g(X)) = \\mathbb{E}[g(X)^2] - (\\mathbb{E}[g(X)])^2$.\n    The first term is $\\mathbb{E}[g(X)^2] = \\mathbb{E}[(\\exp(\\alpha X))^2] = \\mathbb{E}[\\exp(2\\alpha X)]$. This is the MGF of $X$ evaluated at $s=2\\alpha$.\n    $$\n    \\mathbb{E}[g(X)^2] = M_X(2\\alpha) = \\exp\\left(\\frac{1}{2}(2\\alpha)^2\\right) = \\exp(2\\alpha^2)\n    $$\n    Substituting this and the expression for $\\mu$ into the variance formula:\n    $$\n    \\operatorname{Var}(g(X)) = \\exp(2\\alpha^2) - \\left(\\exp\\left(\\frac{\\alpha^2}{2}\\right)\\right)^2 = \\exp(2\\alpha^2) - \\exp(\\alpha^2)\n    $$\n\n3.  **Derivation of $\\operatorname{Cov}(g(X), Y)$**\n    Since $Y=X$, we need to compute $\\operatorname{Cov}(g(X), X)$. The covariance is defined as $\\operatorname{Cov}(g(X), X) = \\mathbb{E}[X \\cdot g(X)] - \\mathbb{E}[X]\\mathbb{E}[g(X)]$.\n    Since $X \\sim \\mathcal{N}(0,1)$, its expectation is $\\mathbb{E}[X]=0$, which simplifies the covariance to $\\operatorname{Cov}(g(X), X) = \\mathbb{E}[X \\exp(\\alpha X)]$.\n    This expectation can be found by differentiating the MGF, $M_X(s)$, with respect to $s$ and evaluating at $s=\\alpha$.\n    $$\n    \\frac{d}{ds}M_X(s) = \\frac{d}{ds}\\mathbb{E}[e^{sX}] = \\mathbb{E}\\left[\\frac{d}{ds}e^{sX}\\right] = \\mathbb{E}[X e^{sX}]\n    $$\n    The derivative of $M_X(s) = \\exp(\\frac{1}{2}s^2)$ is $\\frac{d}{ds}\\exp(\\frac{1}{2}s^2) = s \\exp(\\frac{1}{2}s^2)$.\n    Evaluating at $s=\\alpha$:\n    $$\n    \\mathbb{E}[X \\exp(\\alpha X)] = \\alpha \\exp\\left(\\frac{\\alpha^2}{2}\\right)\n    $$\n    Thus, the covariance is:\n    $$\n    \\operatorname{Cov}(g(X), X) = \\alpha \\exp\\left(\\frac{\\alpha^2}{2}\\right)\n    $$\n\n**Task 2: Analysis of Estimators**\n\nLet $\\{X_i\\}_{i=1}^n$ be i.i.d. samples from $X \\sim \\mathcal{N}(0,1)$. Also, let $\\bar{g}_n = \\frac{1}{n}\\sum_{i=1}^n g(X_i)$ and $\\bar{Y}_n = \\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i$.\n\n1.  **Estimator 1: $\\hat{\\theta}_{\\mathrm{mean}}$**\n    -   **Bias**: $\\operatorname{Bias}(\\hat{\\theta}_{\\mathrm{mean}}) = \\mathbb{E}[\\bar{g}_n] - \\mu = \\mathbb{E}[\\frac{1}{n}\\sum_i g(X_i)] - \\mu = \\frac{1}{n}\\sum_i \\mathbb{E}[g(X_i)] - \\mu = \\frac{1}{n}(n\\mu) - \\mu = 0$. The estimator is unbiased.\n    -   **Variance**: $\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{mean}}) = \\operatorname{Var}(\\bar{g}_n) = \\operatorname{Var}(\\frac{1}{n}\\sum_i g(X_i)) = \\frac{1}{n^2}\\sum_i \\operatorname{Var}(g(X_i)) = \\frac{n}{n^2}\\operatorname{Var}(g(X)) = \\frac{1}{n}\\operatorname{Var}(g(X))$.\n        $$\n        \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{mean}}) = \\frac{1}{n}(\\exp(2\\alpha^2) - \\exp(\\alpha^2))\n        $$\n    -   **MSE**: $\\operatorname{MSE}(\\hat{\\theta}_{\\mathrm{mean}}) = \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{mean}}) + (\\operatorname{Bias}(\\hat{\\theta}_{\\mathrm{mean}}))^2 = \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{mean}})$.\n        $$\n        \\operatorname{MSE}(\\hat{\\theta}_{\\mathrm{mean}}) = \\frac{1}{n}(\\exp(2\\alpha^2) - \\exp(\\alpha^2))\n        $$\n\n2.  **Estimator 2: $\\hat{\\theta}_{\\mathrm{cv}}(c)$**\n    -   **Bias**: $\\mathbb{E}[\\hat{\\theta}_{\\mathrm{cv}}(c)] = \\mathbb{E}[\\bar{g}_n - c\\bar{Y}_n] = \\mathbb{E}[\\bar{g}_n] - c\\mathbb{E}[\\bar{Y}_n] = \\mu - c \\cdot 0 = \\mu$. The estimator is unbiased for any $c$. Thus, $\\operatorname{Bias}(\\hat{\\theta}_{\\mathrm{cv}}(c))=0$.\n    -   **Variance**: $\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{cv}}(c)) = \\operatorname{Var}(\\bar{g}_n - c\\bar{Y}_n) = \\operatorname{Var}(\\bar{g}_n) + c^2\\operatorname{Var}(\\bar{Y}_n) - 2c\\operatorname{Cov}(\\bar{g}_n, \\bar{Y}_n)$.\n        We have $\\operatorname{Var}(\\bar{g}_n) = \\frac{1}{n}\\operatorname{Var}(g(X))$, $\\operatorname{Var}(\\bar{Y}_n) = \\frac{1}{n}\\operatorname{Var}(X) = \\frac{1}{n}$, and $\\operatorname{Cov}(\\bar{g}_n, \\bar{Y}_n) = \\frac{1}{n}\\operatorname{Cov}(g(X),X)$.\n        $$\n        \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{cv}}(c)) = \\frac{1}{n}[\\operatorname{Var}(g(X)) + c^2\\operatorname{Var}(X) - 2c\\operatorname{Cov}(g(X),X)]\n        $$\n        To find the optimal coefficient $c^*$ that minimizes this variance, we differentiate with respect to $c$ and set to $0$:\n        $$\n        \\frac{d}{dc}\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{cv}}(c)) = \\frac{1}{n}[2c\\operatorname{Var}(X) - 2\\operatorname{Cov}(g(X),X)] = 0\n        $$\n        This gives the well-known result $c^* = \\frac{\\operatorname{Cov}(g(X),X)}{\\operatorname{Var}(X)}$. Using our derived quantities and $\\operatorname{Var}(X)=1$:\n        $$\n        c^* = \\frac{\\alpha \\exp(\\alpha^2/2)}{1} = \\alpha \\exp\\left(\\frac{\\alpha^2}{2}\\right)\n        $$\n    -   **Minimized Variance and MSE**: The minimized variance is $\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{cv}}(c^*)) = \\frac{1}{n}(\\operatorname{Var}(g(X)) - \\frac{\\operatorname{Cov}(g(X),X)^2}{\\operatorname{Var}(X)})$.\n        $$\n        \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{cv}}(c^*)) = \\frac{1}{n}\\left[ (\\exp(2\\alpha^2) - \\exp(\\alpha^2)) - \\frac{(\\alpha \\exp(\\alpha^2/2))^2}{1} \\right] \\\\\n        = \\frac{1}{n}\\left[ \\exp(2\\alpha^2) - \\exp(\\alpha^2) - \\alpha^2 \\exp(\\alpha^2) \\right] \\\\\n        = \\frac{1}{n}\\left[ \\exp(2\\alpha^2) - (1+\\alpha^2)\\exp(\\alpha^2) \\right]\n        $$\n        Since the estimator is unbiased, $\\operatorname{MSE}(\\hat{\\theta}_{\\mathrm{cv}}(c^*)) = \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{cv}}(c^*))$.\n\n3.  **Estimator 3: $\\hat{\\theta}_{\\lambda}$**\n    -   **Bias**: $\\mathbb{E}[\\hat{\\theta}_{\\lambda}] = \\mathbb{E}[(1-\\lambda)\\bar{g}_n + \\lambda t] = (1-\\lambda)\\mathbb{E}[\\bar{g}_n] + \\lambda t = (1-\\lambda)\\mu + \\lambda t$.\n        $\\operatorname{Bias}(\\hat{\\theta}_{\\lambda}) = \\mathbb{E}[\\hat{\\theta}_{\\lambda}] - \\mu = (1-\\lambda)\\mu + \\lambda t - \\mu = \\lambda(t-\\mu)$.\n    -   **Variance**: $\\operatorname{Var}(\\hat{\\theta}_{\\lambda}) = \\operatorname{Var}((1-\\lambda)\\bar{g}_n + \\lambda t) = (1-\\lambda)^2\\operatorname{Var}(\\bar{g}_n) = \\frac{(1-\\lambda)^2}{n}\\operatorname{Var}(g(X))$.\n    -   **MSE**: $\\operatorname{MSE}(\\hat{\\theta}_{\\lambda}) = \\operatorname{Var}(\\hat{\\theta}_{\\lambda}) + (\\operatorname{Bias}(\\hat{\\theta}_{\\lambda}))^2$.\n        $$\n        \\operatorname{MSE}(\\hat{\\theta}_{\\lambda}) = \\frac{(1-\\lambda)^2}{n}\\operatorname{Var}(g(X)) + \\lambda^2(t-\\mu)^2\n        $$\n\n**Task 3: Optimizing the Shrinkage Estimator**\n\nWe wish to minimize $J(\\lambda) = \\operatorname{MSE}(\\hat{\\theta}_{\\lambda})$ with respect to $\\lambda \\in [0,1]$.\n$$\nJ(\\lambda) = \\frac{(1-\\lambda)^2}{n}\\operatorname{Var}(g(X)) + \\lambda^2(t-\\mu)^2\n$$\nThis is a quadratic function of $\\lambda$. We find the minimum by setting its derivative to zero:\n$$\n\\frac{dJ}{d\\lambda} = -2\\frac{(1-\\lambda)}{n}\\operatorname{Var}(g(X)) + 2\\lambda(t-\\mu)^2 = 0\n$$\n$$\n\\lambda((t-\\mu)^2 + \\frac{\\operatorname{Var}(g(X))}{n}) = \\frac{\\operatorname{Var}(g(X))}{n}\n$$\nThe unconstrained minimizer is:\n$$\n\\lambda_{unc} = \\frac{\\frac{\\operatorname{Var}(g(X))}{n}}{\\frac{\\operatorname{Var}(g(X))}{n} + (t-\\mu)^2}\n$$\nThe second derivative $\\frac{d^2J}{d\\lambda^2} = 2(\\frac{\\operatorname{Var}(g(X))}{n} + (t-\\mu)^2)$ is non-negative, confirming a minimum. Since $\\operatorname{Var}(g(X)) \\ge 0$ and $(t-\\mu)^2 \\ge 0$, both the numerator and the denominator of $\\lambda_{unc}$ are non-negative. The denominator is greater than or equal to the numerator, so $0 \\le \\lambda_{unc} \\le 1$. Therefore, the unconstrained minimum lies within the interval $[0,1]$, and it is the unique minimizer $\\lambda^*$ in that interval (assuming the denominator is non-zero, which holds for all test cases).\n$$\n\\lambda^* = \\frac{\\frac{\\operatorname{Var}(g(X))}{n}}{\\frac{\\operatorname{Var}(g(X))}{n} + (t-\\mu)^2}\n$$\nTo find the minimized MSE, we can substitute $\\lambda^*$ back into the MSE formula. Let $A = \\frac{\\operatorname{Var}(g(X))}{n}$ and $B = (t-\\mu)^2$. Then $\\lambda^* = \\frac{A}{A+B}$ and $1-\\lambda^* = \\frac{B}{A+B}$.\n$$\n\\operatorname{MSE}(\\hat{\\theta}_{\\lambda^*}) = (1-\\lambda^*)^2 A + (\\lambda^*)^2 B = \\left(\\frac{B}{A+B}\\right)^2 A + \\left(\\frac{A}{A+B}\\right)^2 B \\\\\n= \\frac{AB^2+A^2B}{(A+B)^2} = \\frac{AB(A+B)}{(A+B)^2} = \\frac{AB}{A+B}\n$$\n$$\n\\operatorname{MSE}(\\hat{\\theta}_{\\lambda^*}) = \\frac{\\left(\\frac{\\operatorname{Var}(g(X))}{n}\\right)(t-\\mu)^2}{\\frac{\\operatorname{Var}(g(X))}{n} + (t-\\mu)^2}\n$$\nIf the denominator is zero, it implies both $\\operatorname{Var}(g(X))=0$ and $t-\\mu=0$. In this case, the MSE is zero for any $\\lambda$, and the formula gives $0/0$. We will handle this by returning $0$ for the MSE.\n\n**Summary of Formulas for Implementation:**\nFor given parameters $\\alpha, n, t, \\lambda$:\n1.  $\\mu = \\exp\\left(\\frac{\\alpha^2}{2}\\right)$\n2.  $\\operatorname{Var}_g = \\exp(2\\alpha^2) - \\exp(\\alpha^2)$\n3.  $\\operatorname{MSE}_{\\mathrm{mean}} = \\frac{1}{n} \\operatorname{Var}_g$\n4.  $\\operatorname{MSE}_{\\mathrm{cv}} = \\frac{1}{n}(\\operatorname{Var}_g - \\alpha^2 \\exp(\\alpha^2)) = \\frac{1}{n}(\\exp(2\\alpha^2) - (1+\\alpha^2)\\exp(\\alpha^2))$\n5.  $\\operatorname{MSE}_{\\lambda} = (1-\\lambda)^2 \\frac{\\operatorname{Var}_g}{n} + \\lambda^2(t-\\mu)^2$\n6.  $A = \\frac{\\operatorname{Var}_g}{n}$, $B = (t-\\mu)^2$. $\\lambda^* = A / (A+B)$ if $A+B  0$, else $0$.\n7.  $\\operatorname{MSE}_{\\lambda^*} = (A \\cdot B) / (A+B)$ if $A+B  0$, else $0$.\n\nThese formulas are now ready to be implemented.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing the derived analytical formulas for the MSE\n    of three different Monte Carlo estimators.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # The parameters are (alpha, n, t, lambda).\n    # For cases B and E, 't' is defined based on alpha, so we compute it here.\n    alpha_B = 0.5\n    t_B = np.exp(alpha_B**2 / 2.0)\n    \n    alpha_E = 1.5\n    t_E = np.exp(alpha_E**2 / 2.0) + 5.0\n    \n    test_cases = [\n        # Case A: happy path\n        (0.5, 10000, 1.0, 0.3),\n        # Case B: target equals the true mean\n        (alpha_B, 10000, t_B, 0.7),\n        # Case C: small sample size and distant target\n        (1.0, 100, 0.0, 0.5),\n        # Case D: degenerate function yielding zero variance\n        (0.0, 1000, 2.0, 0.9),\n        # Case E: large sample size and very distant target\n        (alpha_E, 50000, t_E, 0.2),\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        alpha, n, t, lambda_in = case\n        \n        # Derived formulas start here.\n        # Ensure all variables are floats for calculations.\n        alpha, n, t, lambda_in = float(alpha), float(n), float(t), float(lambda_in)\n\n        # Task 1: Fundamental Quantities\n        # mu = E[g(X)]\n        mu = np.exp(alpha**2 / 2.0)\n        # var_g = Var(g(X))\n        var_g = np.exp(2.0 * alpha**2) - np.exp(alpha**2)\n        \n        # Task 2: Estimator Analysis  Task 3: Shrinkage Optimization\n        \n        # 1. MSE of the sample mean estimator\n        mse_mean = var_g / n\n        \n        # 2. MSE of the optimal control variate estimator\n        # Check for alpha=0 case to avoid issues, though formula works.\n        # The variance reduction term is alpha^2 * exp(alpha^2)\n        variance_reduction = alpha**2 * np.exp(alpha**2)\n        mse_cv_star = (var_g - variance_reduction) / n\n\n        # 3. MSE of the shrinkage estimator for the given lambda\n        var_term_shrink = ((1.0 - lambda_in)**2) * (var_g / n)\n        bias_term_shrink = (lambda_in**2) * ((t - mu)**2)\n        mse_lambda = var_term_shrink + bias_term_shrink\n\n        # 4. Optimal lambda* for the shrinkage estimator\n        A = var_g / n\n        B = (t - mu)**2\n        \n        denominator_lambda_star = A + B\n        \n        if np.isclose(denominator_lambda_star, 0.0):\n             # This occurs if var_g=0 (e.g., alpha=0) AND t=mu.\n             # MSE is 0 for any lambda. Any lambda is optimal.\n             # We can choose a convention, e.g., 0, since it doesn't matter.\n            lambda_star = 0.0\n        else:\n            lambda_star = A / denominator_lambda_star\n            \n        # 5. Minimized MSE of the shrinkage estimator at lambda*\n        if np.isclose(denominator_lambda_star, 0.0):\n            # If A+B is 0, then A and B must both be 0. MSE is 0.\n            mse_lambda_star = 0.0\n        else:\n            mse_lambda_star = (A * B) / denominator_lambda_star\n            \n        case_results = [\n            mse_mean,\n            mse_cv_star,\n            mse_lambda,\n            lambda_star,\n            mse_lambda_star,\n        ]\n        all_results.append(case_results)\n\n    # Format the final output list of lists into a string\n    # e.g., [[r1,r2,...],[r'1,r'2,...]]\n    def format_list(lst):\n        return f\"[{','.join(f'{x:.10f}' for x in lst)}]\"\n        \n    final_output_string = f\"[{','.join(format_list(res) for res in all_results)}]\"\n\n    # Final print statement in the exact required format.\n    print(final_output_string)\n\nsolve()\n```"
        },
        {
            "introduction": "The error in a numerical simulation is rarely from a single source. This practice illustrates a fundamental decomposition of error into two distinct components: a systematic error from model approximation (discretization bias) and a statistical error from random sampling (sampling variance). By analyzing a simple numerical integration problem, you will learn to separate and quantify how each source contributes to the total Mean Squared Error, providing critical insight into how to allocate computational effort effectively .",
            "id": "3307371",
            "problem": "Consider estimating the integral of a deterministic integrand over a unit interval using a two-stage procedure that first discretizes the integrand and then applies Monte Carlo (MC) sampling. Let the target quantity be the expectation of a bounded, continuous function under the uniform distribution on the unit interval, defined by\n$$\n\\theta \\equiv \\mathbb{E}[f(U)] = \\int_{0}^{1} f(x)\\,dx,\n$$\nwhere $U \\sim \\mathrm{Uniform}(0,1)$ and $f:[0,1]\\to\\mathbb{R}$ is the integrand. In this problem, fix the integrand to be\n$$\nf(x)=\\exp(-x)\\,\\sin(2\\pi x).\n$$\nThe exact value of the target quantity $\\theta$ is analytically computable via elementary calculus. You are required to develop a precise error decomposition that separates the effects of discretization and sampling, starting only from the fundamental definitions of expectation, variance, independence, and mean squared error. No shortcut formulas are permitted in the derivation.\n\nDefine a discretized approximation to $f$ by partitioning the interval $[0,1]$ into $M$ equal-width bins, each of width $h=1/M$, and using bin midpoints. Specifically, for each integer $i\\in\\{1,2,\\dots,M\\}$, define the midpoint\n$$\nm_i=\\frac{i-\\tfrac{1}{2}}{M},\n$$\nand the piecewise constant function $g_M:[0,1]\\to\\mathbb{R}$ such that\n$$\ng_M(x)=f(m_i)\\quad\\text{for}\\quad x\\in\\left[\\frac{i-1}{M},\\frac{i}{M}\\right).\n$$\nFor a given integer $K\\ge 1$, define the MC estimator of the discretized expectation by\n$$\n\\widehat{\\theta}_{M,K}=\\frac{1}{K}\\sum_{j=1}^{K} g_M(U_j),\n$$\nwhere $U_1,\\dots,U_K$ are independent and identically distributed (i.i.d.) random variables drawn from $\\mathrm{Uniform}(0,1)$.\n\nYour task is to:\n- Derive, from first principles and definitions, the bias induced by discretization, the variance induced by sampling, and the mean squared error (MSE) of $\\widehat{\\theta}_{M,K}$, expressed in terms of $M$, $K$, and the values of $f$ at the midpoints $\\{m_i\\}_{i=1}^M$.\n- Show clearly how the discretization defines a deterministic approximation to the expectation, and how MC sampling produces a random estimator whose variability depends on $K$.\n- Implement a program that, for each test case $(M,K)$ in the test suite, computes:\n  1. The squared bias, defined as the square of the difference between the exact $\\theta$ and the discretized expectation of $g_M(U)$ under $\\mathrm{Uniform}(0,1)$.\n  2. The sampling variance term scaled by $1/K$, defined using the variance of $g_M(U)$ under $\\mathrm{Uniform}(0,1)$ and the independence of samples.\n  3. The total mean squared error, defined as the sum of the squared bias and the sampling variance term.\n- Use the closed-form analytical value for $\\theta$ corresponding to the fixed integrand $f$.\n- Round all outputs to $10$ decimal places.\n\nTest suite:\n- Case $1$: $(M,K)=(1,1)$\n- Case $2$: $(M,K)=(4,1)$\n- Case $3$: $(M,K)=(16,10)$\n- Case $4$: $(M,K)=(16,1000)$\n- Case $5$: $(M,K)=(64,10)$\n- Case $6$: $(M,K)=(64,10000)$\n\nFinal output format:\n- Your program should produce a single line containing the results as a comma-separated list enclosed in square brackets, where each test case’s result is a triple of floats $[\\text{bias}^2,\\text{variance\\_term},\\text{mse}]$, and the overall output is a list of these triples in the same order as the test suite. For example,\n$$\n\\text{print}\\left(\\left[\\left[\\text{b}_1,\\text{v}_1,\\text{m}_1\\right],\\dots,\\left[\\text{b}_6,\\text{v}_6,\\text{m}_6\\right]\\right]\\right).\n$$",
            "solution": "We begin from the foundational definitions. Let $U\\sim\\mathrm{Uniform}(0,1)$ and $f:[0,1]\\to\\mathbb{R}$ be bounded and continuous. The target quantity is the expectation\n$$\n\\theta=\\mathbb{E}[f(U)]=\\int_{0}^{1} f(x)\\,dx.\n$$\nWith the fixed integrand $f(x)=\\exp(-x)\\sin(2\\pi x)$, we compute $\\theta$ exactly. Using the standard integral identity\n$$\n\\int e^{ax}\\sin(bx)\\,dx=\\frac{e^{ax}}{a^2+b^2}\\left(a\\sin(bx)-b\\cos(bx)\\right),\n$$\nand setting $a=-1$ and $b=2\\pi$, we obtain\n$$\n\\theta=\\int_{0}^{1} e^{-x}\\sin(2\\pi x)\\,dx=\\left.\\frac{e^{-x}}{1+(2\\pi)^2}\\left(-\\sin(2\\pi x)-2\\pi\\cos(2\\pi x)\\right)\\right|_{0}^{1}.\n$$\nSince $\\sin(2\\pi)=\\sin(0)=0$ and $\\cos(2\\pi)=\\cos(0)=1$, this simplifies to\n$$\n\\theta=\\frac{2\\pi\\left(1-e^{-1}\\right)}{1+4\\pi^2}.\n$$\n\nNext, we define the discretized piecewise-constant approximation $g_M$ as follows: partition $[0,1]$ into $M$ equal bins, and let $m_i=(i-\\tfrac{1}{2})/M$ be the midpoint of the $i$-th bin. Define\n$$\ng_M(x)=f(m_i),\\quad \\text{for } x\\in\\left[\\frac{i-1}{M},\\frac{i}{M}\\right),\\quad i=1,\\dots,M.\n$$\nBy construction, under $U\\sim\\mathrm{Uniform}(0,1)$, the random variable $g_M(U)$ is discrete with probabilities $1/M$ at values $f(m_i)$. Therefore,\n$$\n\\mathbb{E}[g_M(U)]=\\frac{1}{M}\\sum_{i=1}^{M} f(m_i),\n$$\nand\n$$\n\\mathbb{E}[g_M(U)^2]=\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)^2.\n$$\nUsing the definition of variance,\n$$\n\\mathrm{Var}(g_M(U))=\\mathbb{E}[g_M(U)^2]-\\left(\\mathbb{E}[g_M(U)]\\right)^2=\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)^2-\\left(\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)\\right)^2.\n$$\n\nThe Monte Carlo estimator of the discretized expectation with $K$ i.i.d. samples $U_1,\\dots,U_K$ from $\\mathrm{Uniform}(0,1)$ is\n$$\n\\widehat{\\theta}_{M,K}=\\frac{1}{K}\\sum_{j=1}^{K} g_M(U_j).\n$$\nBy independence and the linearity of expectation,\n$$\n\\mathbb{E}\\left[\\widehat{\\theta}_{M,K}\\right]=\\mathbb{E}[g_M(U)]=\\frac{1}{M}\\sum_{i=1}^{M} f(m_i).\n$$\nUsing the variance additivity for independent random variables and homogeneity,\n$$\n\\mathrm{Var}\\left(\\widehat{\\theta}_{M,K}\\right)=\\mathrm{Var}\\left(\\frac{1}{K}\\sum_{j=1}^{K} g_M(U_j)\\right)=\\frac{1}{K^2}\\sum_{j=1}^{K}\\mathrm{Var}\\left(g_M(U_j)\\right)=\\frac{1}{K}\\,\\mathrm{Var}(g_M(U)).\n$$\n\nDefine the bias of the estimator with respect to the true target $\\theta$ by\n$$\n\\mathrm{Bias}(\\widehat{\\theta}_{M,K};\\theta)=\\mathbb{E}\\left[\\widehat{\\theta}_{M,K}\\right]-\\theta=\\left(\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)\\right)-\\theta.\n$$\nThe mean squared error (MSE), by definition, is\n$$\n\\mathrm{MSE}(\\widehat{\\theta}_{M,K})=\\mathbb{E}\\left[\\left(\\widehat{\\theta}_{M,K}-\\theta\\right)^2\\right].\n$$\nExpanding via the bias-variance decomposition, which follows directly from the definitions and completing the square,\n$$\n\\mathrm{MSE}(\\widehat{\\theta}_{M,K})=\\left(\\mathrm{Bias}(\\widehat{\\theta}_{M,K};\\theta)\\right)^2+\\mathrm{Var}\\left(\\widehat{\\theta}_{M,K}\\right).\n$$\nSubstituting the expressions derived above,\n$$\n\\left(\\mathrm{Bias}(\\widehat{\\theta}_{M,K};\\theta)\\right)^2=\\left(\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)-\\theta\\right)^2,\n$$\n$$\n\\mathrm{Var}\\left(\\widehat{\\theta}_{M,K}\\right)=\\frac{1}{K}\\left[\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)^2-\\left(\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)\\right)^2\\right],\n$$\nand thus\n$$\n\\mathrm{MSE}(\\widehat{\\theta}_{M,K})=\\left(\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)-\\theta\\right)^2+\\frac{1}{K}\\left[\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)^2-\\left(\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)\\right)^2\\right].\n$$\n\nAlgorithmic design to compute the requested quantities for each $(M,K)$:\n- Compute $\\theta$ exactly using the analytical expression:\n$$\n\\theta=\\frac{2\\pi\\left(1-e^{-1}\\right)}{1+4\\pi^2}.\n$$\n- For a given $M$, form midpoints $m_i=(i-\\tfrac{1}{2})/M$ for $i=1,\\dots,M$.\n- Evaluate $f(m_i)=\\exp(-m_i)\\sin(2\\pi m_i)$ and compute:\n  - The discretized expectation $\\mu_M=\\frac{1}{M}\\sum_{i=1}^{M} f(m_i)$.\n  - The squared bias $\\left(\\mu_M-\\theta\\right)^2$.\n  - The variance of $g_M(U)$ under $\\mathrm{Uniform}(0,1)$ via $\\frac{1}{M}\\sum f(m_i)^2-\\mu_M^2$.\n  - The sampling variance term scaled by $1/K$, i.e., $\\mathrm{Var}(g_M(U))/K$.\n  - The total MSE as the sum of the squared bias and the sampling variance term.\n- Round all three outputs to $10$ decimal places for each test case.\n\nError decomposition interpretation:\n- The discretization component arises because $g_M$ approximates $f$ by a piecewise constant function over bins; this alters the expectation from $\\theta$ to $\\mu_M$, introducing a deterministic bias independent of $K$.\n- The sampling component arises because $\\widehat{\\theta}_{M,K}$ is a sample mean of i.i.d. evaluations of $g_M(U)$; its variance decays as $1/K$, as guaranteed by the properties of independent averages.\n- The mean squared error combines these effects additively: a deterministic squared bias from discretization and a stochastic variance term that depends on $K$.\n\nThe program implements this derivation precisely and outputs, for each test case $(M,K)$, the triple $[\\mathrm{bias}^2,\\mathrm{variance\\_term},\\mathrm{MSE}]$, rounded to $10$ decimal places, aggregated into a single list in the specified format.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f(x):\n    # f(x) = exp(-x) * sin(2*pi*x)\n    return np.exp(-x) * np.sin(2 * np.pi * x)\n\ndef exact_theta():\n    # theta = 2*pi*(1 - e^{-1}) / (1 + 4*pi^2)\n    pi = np.pi\n    return (2 * pi * (1 - np.exp(-1.0))) / (1 + 4 * pi * pi)\n\ndef compute_components(M, K, theta):\n    # Midpoints of M bins on [0,1]\n    i = np.arange(1, M + 1, dtype=float)\n    midpoints = (i - 0.5) / M\n\n    f_vals = f(midpoints)\n\n    mu_M = f_vals.mean()\n    bias_sq = (mu_M - theta) ** 2\n\n    var_gM = (f_vals ** 2).mean() - mu_M ** 2\n    var_term = var_gM / K\n\n    mse = bias_sq + var_term\n\n    # Round to 10 decimal places\n    def r10(x):\n        return float(f\"{x:.10f}\")\n\n    return [r10(bias_sq), r10(var_term), r10(mse)]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Cases: (M,K)\n    test_cases = [\n        (1, 1),\n        (4, 1),\n        (16, 10),\n        (16, 1000),\n        (64, 10),\n        (64, 10000),\n    ]\n\n    theta = exact_theta()\n\n    results = []\n    for M, K in test_cases:\n        results.append(compute_components(M, K, theta))\n\n    # Final print statement in the exact required format.\n    # Single line: list of triples with comma-separated values.\n    # Ensure no extra whitespace beyond commas and brackets.\n    # Convert nested list to required string format.\n    inner = \",\".join(\"[\" + \",\".join(f\"{v:.10f}\" for v in triple) + \"]\" for triple in results)\n    print(f\"[{inner}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}