{
    "hands_on_practices": [
        {
            "introduction": "Before applying an abstract principle more broadly, it is often invaluable to verify it in a concrete setting. This first exercise provides a direct, hands-on numerical test of the law of total variance. By calculating the total variance through two different pathways—first by direct computation and then by summing the \"within\" and \"between\" components of the decomposition—you will build a solid, intuitive understanding of how these pieces fit together .",
            "id": "3354738",
            "problem": "Consider the following two-point mixture model that is commonly used to formalize stratified sampling in Monte Carlo (MC) simulation. Let $Y \\in \\{0,1\\}$ be a Bernoulli random variable with $\\mathbb{P}(Y=1)=p$, where $p=\\frac{2}{5}$. Conditional on $Y$, let $X \\mid (Y=0) \\sim \\mathcal{N}(\\mu_{0},\\sigma_{0}^{2})$ and $X \\mid (Y=1) \\sim \\mathcal{N}(\\mu_{1},\\sigma_{1}^{2})$, with parameters $\\mu_{0}=0$, $\\sigma_{0}^{2}=1$, $\\mu_{1}=3$, and $\\sigma_{1}^{2}=4$. \n\nUsing only the definitions of expectation and variance and the tower property of conditional expectation (i.e., $\\mathbb{E}[\\mathbb{E}[Z \\mid Y]]=\\mathbb{E}[Z]$ for any integrable random variable $Z$), do the following:\n\n1. Compute $\\mathbb{E}[X]$ and $\\operatorname{Var}(X)$ directly by expanding $\\operatorname{Var}(X)=\\mathbb{E}[X^{2}] - (\\mathbb{E}[X])^{2}$ and conditioning on $Y$ where appropriate.\n2. Compute $\\mathbb{E}[\\operatorname{Var}(X \\mid Y)]$ and $\\operatorname{Var}(\\mathbb{E}[X \\mid Y])$ separately, using only conditional means and variances.\n3. Verify by explicit computation that the two routes to $\\operatorname{Var}(X)$ agree, and report the common value of $\\operatorname{Var}(X)$ as a single number. \n\nRound your final reported value to four significant figures. No units are required.",
            "solution": "The problem provides a two-point mixture model for a random variable $X$. The mixing variable is $Y \\sim \\text{Bernoulli}(p)$, with $\\mathbb{P}(Y=1)=p=\\frac{2}{5}$ and thus $\\mathbb{P}(Y=0)=1-p=\\frac{3}{5}$. The conditional distributions of $X$ given $Y$ are specified as $X \\mid (Y=0) \\sim \\mathcal{N}(\\mu_{0},\\sigma_{0}^{2})$ and $X \\mid (Y=1) \\sim \\mathcal{N}(\\mu_{1},\\sigma_{1}^{2})$. The parameters are given as $\\mu_{0}=0$, $\\sigma_{0}^{2}=1$, $\\mu_{1}=3$, and $\\sigma_{1}^{2}=4$. We are tasked with computing the variance of $X$, $\\operatorname{Var}(X)$, through two different methods and verifying their agreement.\n\nFirst, we establish the necessary conditional moments. For a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$, the mean is $\\mu$ and the variance is $\\sigma^2$. The second moment is $\\mathbb{E}[X^2] = \\operatorname{Var}(X) + (\\mathbb{E}[X])^2 = \\sigma^2 + \\mu^2$.\nTherefore, we have the following conditional moments:\n$\\mathbb{E}[X \\mid Y=0] = \\mu_{0} = 0$.\n$\\operatorname{Var}(X \\mid Y=0) = \\sigma_{0}^{2} = 1$.\n$\\mathbb{E}[X^2 \\mid Y=0] = \\sigma_{0}^{2} + \\mu_{0}^2 = 1 + 0^2 = 1$.\n\n$\\mathbb{E}[X \\mid Y=1] = \\mu_{1} = 3$.\n$\\operatorname{Var}(X \\mid Y=1) = \\sigma_{1}^{2} = 4$.\n$\\mathbb{E}[X^2 \\mid Y=1] = \\sigma_{1}^{2} + \\mu_{1}^2 = 4 + 3^2 = 4 + 9 = 13$.\n\nWe address the three parts of the problem in sequence.\n\n1. Direct computation of $\\mathbb{E}[X]$ and $\\operatorname{Var}(X)$.\n\nTo compute the unconditional expectation $\\mathbb{E}[X]$, we use the law of total expectation (or tower property): $\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X \\mid Y]]$. The inner expectation, $\\mathbb{E}[X \\mid Y]$, is a random variable that depends on $Y$.\n$$\n\\mathbb{E}[X \\mid Y] = \\begin{cases} \\mu_0  \\text{if } Y=0 \\\\ \\mu_1  \\text{if } Y=1 \\end{cases}\n$$\nTaking the expectation over $Y$:\n$$\n\\mathbb{E}[X] = \\mathbb{E}[\\mathbb{E}[X \\mid Y]] = \\mathbb{E}[X \\mid Y=0] \\cdot \\mathbb{P}(Y=0) + \\mathbb{E}[X \\mid Y=1] \\cdot \\mathbb{P}(Y=1)\n$$\n$$\n\\mathbb{E}[X] = \\mu_{0}(1-p) + \\mu_{1}p = (0)\\left(\\frac{3}{5}\\right) + (3)\\left(\\frac{2}{5}\\right) = 0 + \\frac{6}{5} = \\frac{6}{5}\n$$\nTo compute $\\operatorname{Var}(X)$, we use the formula $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. We first need $\\mathbb{E}[X^2]$, which we find again using the law of total expectation: $\\mathbb{E}[X^2] = \\mathbb{E}[\\mathbb{E}[X^2 \\mid Y]]$.\n$$\n\\mathbb{E}[X^2 \\mid Y] = \\begin{cases} \\sigma_0^2 + \\mu_0^2  \\text{if } Y=0 \\\\ \\sigma_1^2 + \\mu_1^2  \\text{if } Y=1 \\end{cases}\n$$\nTaking the expectation over $Y$:\n$$\n\\mathbb{E}[X^2] = \\mathbb{E}[\\mathbb{E}[X^2 \\mid Y]] = (\\sigma_{0}^{2} + \\mu_{0}^2)(1-p) + (\\sigma_{1}^{2} + \\mu_{1}^2)p\n$$\n$$\n\\mathbb{E}[X^2] = (1)\\left(\\frac{3}{5}\\right) + (13)\\left(\\frac{2}{5}\\right) = \\frac{3}{5} + \\frac{26}{5} = \\frac{29}{5}\n$$\nNow we can compute the variance:\n$$\n\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = \\frac{29}{5} - \\left(\\frac{6}{5}\\right)^2 = \\frac{29}{5} - \\frac{36}{25} = \\frac{145}{25} - \\frac{36}{25} = \\frac{109}{25}\n$$\n\n2. Computation of the components of the law of total variance.\n\nThe law of total variance states that $\\operatorname{Var}(X) = \\mathbb{E}[\\operatorname{Var}(X \\mid Y)] + \\operatorname{Var}(\\mathbb{E}[X \\mid Y])$. We compute each term separately.\n\nFirst, we compute the expected conditional variance, $\\mathbb{E}[\\operatorname{Var}(X \\mid Y)]$. The term $\\operatorname{Var}(X \\mid Y)$ is a random variable whose value depends on $Y$:\n$$\n\\operatorname{Var}(X \\mid Y) = \\begin{cases} \\sigma_0^2  \\text{if } Y=0 \\\\ \\sigma_1^2  \\text{if } Y=1 \\end{cases}\n$$\nTaking the expectation over $Y$:\n$$\n\\mathbb{E}[\\operatorname{Var}(X \\mid Y)] = \\operatorname{Var}(X \\mid Y=0) \\cdot \\mathbb{P}(Y=0) + \\operatorname{Var}(X \\mid Y=1) \\cdot \\mathbb{P}(Y=1)\n$$\n$$\n\\mathbb{E}[\\operatorname{Var}(X \\mid Y)] = \\sigma_{0}^{2}(1-p) + \\sigma_{1}^{2}p = (1)\\left(\\frac{3}{5}\\right) + (4)\\left(\\frac{2}{5}\\right) = \\frac{3}{5} + \\frac{8}{5} = \\frac{11}{5}\n$$\nSecond, we compute the variance of the conditional expectation, $\\operatorname{Var}(\\mathbb{E}[X \\mid Y])$. Let $Z = \\mathbb{E}[X \\mid Y]$. From Part 1, we know that $Z$ is a random variable that takes the value $\\mu_0=0$ with probability $1-p=\\frac{3}{5}$ and $\\mu_1=3$ with probability $p=\\frac{2}{5}$. We also found its expectation, $\\mathbb{E}[Z] = \\mathbb{E}[\\mathbb{E}[X \\mid Y]] = \\mathbb{E}[X] = \\frac{6}{5}$. To find $\\operatorname{Var}(Z)$, we use $\\operatorname{Var}(Z) = \\mathbb{E}[Z^2] - (\\mathbb{E}[Z])^2$. We must compute $\\mathbb{E}[Z^2] = \\mathbb{E}[(\\mathbb{E}[X \\mid Y])^2]$.\n$$\n\\mathbb{E}[(\\mathbb{E}[X \\mid Y])^2] = (\\mathbb{E}[X \\mid Y=0])^2 \\cdot \\mathbb{P}(Y=0) + (\\mathbb{E}[X \\mid Y=1])^2 \\cdot \\mathbb{P}(Y=1)\n$$\n$$\n\\mathbb{E}[(\\mathbb{E}[X \\mid Y])^2] = \\mu_{0}^2(1-p) + \\mu_{1}^2 p = (0)^2\\left(\\frac{3}{5}\\right) + (3)^2\\left(\\frac{2}{5}\\right) = 0 + \\frac{18}{5} = \\frac{18}{5}\n$$\nNow, we compute the variance of the conditional expectation:\n$$\n\\operatorname{Var}(\\mathbb{E}[X \\mid Y]) = \\mathbb{E}[(\\mathbb{E}[X \\mid Y])^2] - (\\mathbb{E}[\\mathbb{E}[X \\mid Y]])^2 = \\frac{18}{5} - \\left(\\frac{6}{5}\\right)^2 = \\frac{18}{5} - \\frac{36}{25} = \\frac{90}{25} - \\frac{36}{25} = \\frac{54}{25}\n$$\n\n3. Verification and final result.\n\nWe now verify that the sum of the two components calculated in Part 2 equals the direct calculation of $\\operatorname{Var}(X)$ from Part 1.\n$$\n\\mathbb{E}[\\operatorname{Var}(X \\mid Y)] + \\operatorname{Var}(\\mathbb{E}[X \\mid Y]) = \\frac{11}{5} + \\frac{54}{25} = \\frac{55}{25} + \\frac{54}{25} = \\frac{109}{25}\n$$\nThis result matches the value of $\\operatorname{Var}(X) = \\frac{109}{25}$ computed in Part 1. The agreement is verified.\n\nThe common value of the variance is $\\operatorname{Var}(X) = \\frac{109}{25} = 4.36$. The problem requires this value to be reported to four significant figures. Thus, the value is $4.360$.",
            "answer": "$$\n\\boxed{4.360}\n$$"
        },
        {
            "introduction": "Moving from numerical verification to symbolic derivation, this practice demonstrates the law's analytical power in the context of a hierarchical Bayesian model. You are tasked with deriving a fundamental result for the posterior predictive variance, which is a key quantity in Bayesian forecasting and model checking. This exercise illustrates how the law of total variance provides a formal framework for decomposing predictive uncertainty into inherent process randomness and the uncertainty in our parameter estimates .",
            "id": "3354749",
            "problem": "Consider a hierarchical Bayesian data-generating mechanism used in stochastic simulation and Monte Carlo methods. A latent parameter $\\,\\Theta\\,$ has a posterior distribution given observed data $\\,D\\,$, namely $\\,\\Theta\\,|\\,D \\sim \\mathcal{N}(\\mu_{D}, \\tau_{D}^{2})\\,$, where $\\,\\mu_{D}\\,$ and $\\,\\tau_{D}^{2}\\,$ are data-dependent posterior hyperparameters. Conditional on $\\,\\Theta\\,$, the predictive quantity $\\,X\\,$ is generated as $\\,X\\,|\\,\\Theta \\sim \\mathcal{N}(\\Theta, \\sigma^{2})\\,$ with known $\\,\\sigma^{2}  0\\,$, and with $\\,X\\,|\\,\\Theta\\,$ independent of $\\,D\\,$ given $\\,\\Theta\\,$. In a Monte Carlo simulation, one could sample $\\,\\Theta\\,|\\,D\\,$ first and then $\\,X\\,|\\,\\Theta\\,$, but here your goal is to analytically derive the marginal predictive variance $\\,\\operatorname{Var}(X\\,|\\,D)\\,$.\n\nStarting only from foundational definitions of conditional expectation and variance, and using standard properties such as the tower property of conditional expectation, derive a closed-form expression for $\\,\\operatorname{Var}(X\\,|\\,D)\\,$. Do not quote any pre-packaged result; instead, show how the relationship emerges from first principles. Express your final answer as a symbolic expression in terms of $\\,\\sigma^{2}\\,$ and $\\,\\tau_{D}^{2}\\,$. No numerical rounding is required.",
            "solution": "The goal is to derive the marginal predictive variance, $\\operatorname{Var}(X\\,|\\,D)$, for a quantity $\\,X\\,$ given observed data $\\,D\\,$. The derivation must start from first principles.\n\nThe law of total variance states that for two random variables, $\\operatorname{Var}(A) = \\mathbb{E}[\\operatorname{Var}(A|B)] + \\operatorname{Var}(\\mathbb{E}[A|B])$. In our context, we are interested in variances and expectations conditional on the data $\\,D\\,$. The equivalent law is $\\operatorname{Var}(X\\,|\\,D) = \\mathbb{E}[\\operatorname{Var}(X\\,|\\,\\Theta, D)\\,|\\,D] + \\operatorname{Var}(\\mathbb{E}[X\\,|\\,\\Theta, D]\\,|\\,D)$. The problem states that $\\,X\\,|\\,\\Theta\\,$ is independent of $\\,D\\,$ given $\\,\\Theta\\,$. This implies that conditioning on $\\,D\\,$ provides no additional information about $\\,X\\,$ once $\\,\\Theta\\,$ is known. Therefore, $\\operatorname{Var}(X\\,|\\,\\Theta, D) = \\operatorname{Var}(X\\,|\\,\\Theta)\\,$ and $\\,\\mathbb{E}[X\\,|\\,\\Theta, D] = \\mathbb{E}[X\\,|\\,\\Theta]\\,$. The relationship thus simplifies to:\n$$\n\\operatorname{Var}(X\\,|\\,D) = \\mathbb{E}[\\operatorname{Var}(X\\,|\\,\\Theta)\\,|\\,D] + \\operatorname{Var}(\\mathbb{E}[X\\,|\\,\\Theta]\\,|\\,D)\n$$\nWe can derive this from foundational definitions. The definition of variance of $\\,X\\,$ conditional on $\\,D\\,$ is:\n$$\n\\operatorname{Var}(X\\,|\\,D) = \\mathbb{E}[(X - \\mathbb{E}[X\\,|\\,D])^{2}\\,|\\,D]\n$$\nFirst, we express the inner expectation, $\\,\\mathbb{E}[X\\,|\\,D]\\,$, using the law of total expectation (or tower property):\n$$\n\\mathbb{E}[X\\,|\\,D] = \\mathbb{E}[\\mathbb{E}[X\\,|\\,\\Theta, D]\\,|\\,D] = \\mathbb{E}[\\mathbb{E}[X\\,|\\,\\Theta]\\,|\\,D]\n$$\nNow, we substitute this into the variance definition, adding and subtracting $\\,\\mathbb{E}[X\\,|\\,\\Theta]\\,$ inside the squared term:\n$$\n\\operatorname{Var}(X\\,|\\,D) = \\mathbb{E}[( (X - \\mathbb{E}[X\\,|\\,\\Theta]) + (\\mathbb{E}[X\\,|\\,\\Theta] - \\mathbb{E}[X\\,|\\,D]) )^{2}\\,|\\,D]\n$$\nExpanding the square gives three terms. The cross-term is zero because, using the law of total expectation again, we find that $\\mathbb{E}[(X - \\mathbb{E}[X\\,|\\,\\Theta])(E[X\\,|\\,\\Theta] - \\mathbb{E}[X\\,|\\,D])\\,|\\,D] = \\mathbb{E}[(E[X\\,|\\,\\Theta] - \\mathbb{E}[X\\,|\\,D]) \\mathbb{E}[X - \\mathbb{E}[X\\,|\\,\\Theta]\\,|\\,\\Theta, D]\\,|\\,D] = \\mathbb{E}[(\\dots) \\cdot 0 \\,|\\,D] = 0$.\nThe remaining two terms are:\n$$\n\\operatorname{Var}(X\\,|\\,D) = \\mathbb{E}[(X - \\mathbb{E}[X\\,|\\,\\Theta])^{2}\\,|\\,D] + \\mathbb{E}[(\\mathbb{E}[X\\,|\\,\\Theta] - \\mathbb{E}[X\\,|\\,D])^{2}\\,|\\,D]\n$$\nThe first term is $\\mathbb{E}[\\mathbb{E}[(X - \\mathbb{E}[X|\\Theta])^2 | \\Theta, D] | D] = \\mathbb{E}[\\operatorname{Var}(X|\\Theta)|D]$.\nThe second term is the definition of $\\operatorname{Var}(\\mathbb{E}[X|\\Theta]|D)$. This confirms the law of total variance.\n\nNow we apply this formula to the specific distributions given in the problem.\n\nStep 1: Calculate the first term, $\\,\\mathbb{E}[\\operatorname{Var}(X\\,|\\,\\Theta)\\,|\\,D]\\,$.\nWe are given that $\\,X\\,|\\,\\Theta \\sim \\mathcal{N}(\\Theta, \\sigma^{2})\\,$.\nThe variance of $\\,X\\,$ conditional on $\\,\\Theta\\,$ is simply the variance parameter of this normal distribution:\n$$\n\\operatorname{Var}(X\\,|\\,\\Theta) = \\sigma^{2}\n$$\nSince $\\,\\sigma^{2}\\,$ is a known constant, it does not depend on $\\,\\Theta\\,$. The expectation of a constant is the constant itself.\n$$\n\\mathbb{E}[\\operatorname{Var}(X\\,|\\,\\Theta)\\,|\\,D] = \\mathbb{E}[\\sigma^{2}\\,|\\,D] = \\sigma^{2}\n$$\n\nStep 2: Calculate the second term, $\\,\\operatorname{Var}(\\mathbb{E}[X\\,|\\,\\Theta]\\,|\\,D)\\,$.\nFirst, we find the expectation of $\\,X\\,$ conditional on $\\,\\Theta\\,$. Since $\\,X\\,|\\,\\Theta \\sim \\mathcal{N}(\\Theta, \\sigma^{2})\\,$, the expectation is the mean parameter of this normal distribution:\n$$\n\\mathbb{E}[X\\,|\\,\\Theta] = \\Theta\n$$\nSubstituting this into the expression for the second term, we need to find:\n$$\n\\operatorname{Var}(\\Theta\\,|\\,D)\n$$\nThe problem states that the posterior distribution of $\\,\\Theta\\,$ given the data $\\,D\\,$ is $\\,\\Theta\\,|\\,D \\sim \\mathcal{N}(\\mu_{D}, \\tau_{D}^{2})\\,$.\nBy definition, the variance of this distribution is its variance parameter:\n$$\n\\operatorname{Var}(\\Theta\\,|\\,D) = \\tau_{D}^{2}\n$$\n\nStep 3: Combine the results.\nThe marginal predictive variance $\\,\\operatorname{Var}(X\\,|\\,D)\\,$ is the sum of the two terms calculated above:\n$$\n\\operatorname{Var}(X\\,|\\,D) = \\sigma^{2} + \\tau_{D}^{2}\n$$\nThis expression represents the total predictive uncertainty. It is composed of two parts: the inherent variability of the process $\\,(\\sigma^{2})\\,$ and the uncertainty in the parameter $\\,\\Theta\\,$ after observing the data $\\,D\\,(\\tau_{D}^{2})\\,$.",
            "answer": "$$\n\\boxed{\\sigma^{2} + \\tau_{D}^{2}}\n$$"
        },
        {
            "introduction": "The true power of a theoretical tool is revealed when it is used to solve practical problems. This final exercise applies the law of total variance as a design principle for optimizing Monte Carlo simulations. Your goal is to derive an optimality criterion for stratified sampling, a powerful variance reduction technique, by minimizing the expected within-stratum variance component . This practice directly connects the abstract decomposition of variance to the concrete goal of designing more efficient and precise estimators.",
            "id": "3354809",
            "problem": "Consider a stratified simulation design for a univariate random input where $U$ is uniformly distributed on the unit interval, $U \\sim \\mathrm{Uniform}(0,1)$. Let $f:[0,1]\\to\\mathbb{R}$ be a continuously differentiable, strictly monotone function representing the scalar output of interest in a Monte Carlo (MC) computation. Define a stratification by choosing boundaries $0=b_{0}b_{1}\\dotsb_{K}=1$ and a stratum index $Y\\in\\{1,\\dots,K\\}$ via the deterministic rule $Y=k$ if and only if $U\\in[b_{k-1},b_{k})$. Let $m_{k}=\\mathbb{E}[f(U)\\mid Y=k]$ denote the conditional mean of $f(U)$ in stratum $k$ and $\\sigma_{k}^{2}=\\operatorname{Var}(f(U)\\mid Y=k)$ its conditional variance; let $w_{k}=\\mathbb{P}(Y=k)=b_{k}-b_{k-1}$ be the stratum mass.\n\nStarting from the fundamental definitions of conditional expectation and variance together with the law of total variance, derive a necessary optimality criterion for the stratification boundaries $\\{b_{k}\\}$ that minimize the objective $\\mathbb{E}\\big[\\operatorname{Var}(f(U)\\mid Y)\\big]$ over all partitions of $[0,1]$ into $K$ intervals. Your derivation must proceed from first principles and logically justify any equivalence or transformation you use; do not assume unproven results from quantization or clustering theory.\n\nThen, specialize to the case $K=2$ and $f(u)=u^{2}$. In this case there is a single interior boundary $b\\in(0,1)$, free to vary. Compute the optimal boundary $b$ implied by your necessary criterion as an exact closed-form expression. Express your final answer as a single exact algebraic expression. No rounding is required, and no physical units apply.",
            "solution": "The objective is to find a necessary optimality criterion for the stratification boundaries $\\{b_k\\}_{k=1}^{K-1}$ that minimize the objective function $\\mathbb{E}\\big[\\operatorname{Var}(f(U)\\mid Y)\\big]$. This objective represents the expected conditional variance, which is the \"within-stratum variance\" component in the law of total variance, $\\operatorname{Var}(f(U)) = \\mathbb{E}[\\operatorname{Var}(f(U)\\mid Y)] + \\operatorname{Var}(\\mathbb{E}[f(U)\\mid Y)]$.\n\nLet the objective function be denoted by $L$. By the definition of expectation for a discrete random variable $Y$, the objective function can be written as a sum over the $K$ strata:\n$$ L = \\mathbb{E}\\big[\\operatorname{Var}(f(U)\\mid Y)\\big] = \\sum_{k=1}^{K} \\operatorname{Var}(f(U)\\mid Y=k) \\cdot \\mathbb{P}(Y=k) $$\nUsing the notation provided in the problem statement, this is:\n$$ L(b_1, \\dots, b_{K-1}) = \\sum_{k=1}^{K} \\sigma_k^2 w_k $$\nThe problem defines $w_k = \\mathbb{P}(Y=k) = \\mathbb{P}(U \\in [b_{k-1}, b_k)) = b_k - b_{k-1}$, since $U \\sim \\mathrm{Uniform}(0,1)$. The conditional variance is $\\sigma_k^2 = \\mathbb{E}[f(U)^2 \\mid Y=k] - (\\mathbb{E}[f(U) \\mid Y=k])^2$.\n\nTo express these conditional expectations as integrals, we need the conditional probability density of $U$ given $Y=k$. This is a uniform distribution over the interval $[b_{k-1}, b_k)$:\n$$ p(u \\mid Y=k) = \\begin{cases} \\frac{1}{b_k - b_{k-1}}  \\text{if } u \\in [b_{k-1}, b_k) \\\\ 0  \\text{otherwise} \\end{cases} $$\nThe conditional moments are integrals with respect to this density. Specifically, the conditional mean $m_k$ and the conditional expectation of $f(U)^2$ are:\n$$ m_k = \\mathbb{E}[f(U) \\mid Y=k] = \\int_{b_{k-1}}^{b_k} f(u) \\frac{1}{b_k - b_{k-1}} du = \\frac{1}{w_k} \\int_{b_{k-1}}^{b_k} f(u) du $$\n$$ \\mathbb{E}[f(U)^2 \\mid Y=k] = \\int_{b_{k-1}}^{b_k} f(u)^2 \\frac{1}{b_k - b_{k-1}} du = \\frac{1}{w_k} \\int_{b_{k-1}}^{b_k} f(u)^2 du $$\nSubstituting these into the expression for $\\sigma_k^2$:\n$$ \\sigma_k^2 = \\frac{1}{w_k} \\int_{b_{k-1}}^{b_k} f(u)^2 du - m_k^2 = \\frac{1}{w_k} \\int_{b_{k-1}}^{b_k} f(u)^2 du - \\left(\\frac{1}{w_k} \\int_{b_{k-1}}^{b_k} f(u) du\\right)^2 $$\nThe objective function $L = \\sum w_k \\sigma_k^2$ becomes:\n$$ L = \\sum_{k=1}^{K} w_k \\left[ \\frac{1}{w_k} \\int_{b_{k-1}}^{b_k} f(u)^2 du - \\frac{1}{w_k^2} \\left(\\int_{b_{k-1}}^{b_k} f(u) du\\right)^2 \\right] $$\n$$ L = \\sum_{k=1}^{K} \\left[ \\int_{b_{k-1}}^{b_k} f(u)^2 du - \\frac{1}{b_k - b_{k-1}} \\left(\\int_{b_{k-1}}^{b_k} f(u) du\\right)^2 \\right] $$\nTo find the necessary optimality criterion, we differentiate $L$ with respect to an arbitrary interior boundary $b_j$ for $j \\in \\{1, \\dots, K-1\\}$ and set the derivative to zero. The boundary $b_j$ appears only in the terms for stratum $j$ (as the upper boundary) and stratum $j+1$ (as the lower boundary). Let $T_k$ denote the $k$-th term in the sum for $L$.\n$$ \\frac{\\partial L}{\\partial b_j} = \\frac{\\partial T_j}{\\partial b_j} + \\frac{\\partial T_{j+1}}{\\partial b_j} $$\nWe compute each partial derivative using the Leibniz integral rule.\nFor the term $T_j(b_{j-1}, b_j) = \\int_{b_{j-1}}^{b_j} f(u)^2 du - \\frac{1}{b_j - b_{j-1}} (\\int_{b_{j-1}}^{b_j} f(u) du)^2$:\n$$ \\frac{\\partial T_j}{\\partial b_j} = f(b_j)^2 - \\frac{2\\left(\\int_{b_{j-1}}^{b_j} f(u) du\\right)f(b_j)(b_j - b_{j-1}) - \\left(\\int_{b_{j-1}}^{b_j} f(u) du\\right)^2(1)}{(b_j - b_{j-1})^2} $$\nDividing the numerator and denominator by $(b_j - b_{j-1})^2$ and recognizing $m_j = \\frac{1}{b_j-b_{j-1}}\\int_{b_{j-1}}^{b_j} f(u) du$, we get:\n$$ \\frac{\\partial T_j}{\\partial b_j} = f(b_j)^2 - \\left[2 m_j f(b_j) - m_j^2\\right] = (f(b_j) - m_j)^2 $$\nFor the term $T_{j+1}(b_j, b_{j+1}) = \\int_{b_j}^{b_{j+1}} f(u)^2 du - \\frac{1}{b_{j+1} - b_j} (\\int_{b_j}^{b_{j+1}} f(u) du)^2$:\n$$ \\frac{\\partial T_{j+1}}{\\partial b_j} = -f(b_j)^2 - \\frac{2\\left(\\int_{b_j}^{b_{j+1}} f(u) du\\right)(-f(b_j))(b_{j+1} - b_j) - \\left(\\int_{b_j}^{b_{j+1}} f(u) du\\right)^2(-1)}{(b_{j+1} - b_j)^2} $$\nSimilarly, recognizing $m_{j+1} = \\frac{1}{b_{j+1}-b_j}\\int_{b_j}^{b_{j+1}} f(u) du$, we have:\n$$ \\frac{\\partial T_{j+1}}{\\partial b_j} = -f(b_j)^2 - \\left[-2 m_{j+1} f(b_j) + m_{j+1}^2\\right] = - (f(b_j)^2 - 2m_{j+1}f(b_j) + m_{j+1}^2) = -(f(b_j) - m_{j+1})^2 $$\nSetting the total derivative $\\frac{\\partial L}{\\partial b_j}$ to zero for an optimal configuration yields:\n$$ (f(b_j) - m_j)^2 - (f(b_j) - m_{j+1})^2 = 0 $$\nThis implies $|f(b_j) - m_j| = |f(b_j) - m_{j+1}|$, which leads to two possibilities:\n$1$. $f(b_j) - m_j = f(b_j) - m_{j+1}$, which simplifies to $m_j = m_{j+1}$.\n$2$. $f(b_j) - m_j = -(f(b_j) - m_{j+1}) = m_{j+1} - f(b_j)$.\n\nThe function $f$ is given as strictly monotone. Let's assume it is strictly increasing without loss of generality. For any $u_1 \\in [b_{j-1}, b_j)$ and $u_2 \\in [b_j, b_{j+1})$, we have $u_1  b_j \\leq u_2$, so $f(u_1)  f(u_2)$. The conditional mean $m_j$ is the average of $f(u)$ values over the first interval, and $m_{j+1}$ is the average over the second. Therefore, $m_j  m_{j+1}$. The equality $m_j = m_{j+1}$ is impossible for non-degenerate intervals, so the first case is rejected.\n\nThe necessary optimality criterion must therefore be the second case:\n$$ 2f(b_j) = m_j + m_{j+1} \\implies f(b_j) = \\frac{m_j + m_{j+1}}{2} $$\nThis criterion states that at an optimal boundary $b_j$, the function value $f(b_j)$ must be the arithmetic mean of the conditional expectations of $f(U)$ in the two adjacent strata.\n\nNow, we specialize to the case where $K=2$ and $f(u)=u^2$. There is a single interior boundary, $b \\in (0,1)$. The strata are $[0, b)$ and $[b, 1)$. The optimality criterion becomes:\n$$ f(b) = \\frac{m_1 + m_2}{2} $$\nwhere $m_1 = \\mathbb{E}[f(U) \\mid U \\in [0, b)]$ and $m_2 = \\mathbb{E}[f(U) \\mid U \\in [b, 1)]$. With $f(u)=u^2$, we have $f(b)=b^2$. We compute the conditional means:\n$$ m_1 = \\mathbb{E}[U^2 \\mid U \\in [0, b)] = \\int_0^b u^2 \\frac{1}{b} du = \\frac{1}{b} \\left[ \\frac{u^3}{3} \\right]_0^b = \\frac{b^3}{3b} = \\frac{b^2}{3} $$\n$$ m_2 = \\mathbb{E}[U^2 \\mid U \\in [b, 1)] = \\int_b^1 u^2 \\frac{1}{1-b} du = \\frac{1}{1-b} \\left[ \\frac{u^3}{3} \\right]_b^1 = \\frac{1-b^3}{3(1-b)} $$\nUsing the identity $1-b^3 = (1-b)(1+b+b^2)$, we simplify $m_2$:\n$$ m_2 = \\frac{(1-b)(1+b+b^2)}{3(1-b)} = \\frac{1+b+b^2}{3} $$\nSubstituting these expressions into the optimality criterion:\n$$ b^2 = \\frac{1}{2} \\left(\\frac{b^2}{3} + \\frac{1+b+b^2}{3}\\right) $$\n$$ b^2 = \\frac{1}{6} (b^2 + 1 + b + b^2) $$\n$$ 6b^2 = 2b^2 + b + 1 $$\n$$ 4b^2 - b - 1 = 0 $$\nThis is a quadratic equation for the optimal boundary $b$. We use the quadratic formula to solve for $b$:\n$$ b = \\frac{-(-1) \\pm \\sqrt{(-1)^2 - 4(4)(-1)}}{2(4)} = \\frac{1 \\pm \\sqrt{1 + 16}}{8} = \\frac{1 \\pm \\sqrt{17}}{8} $$\nWe have two solutions: $b_1 = \\frac{1+\\sqrt{17}}{8}$ and $b_2 = \\frac{1-\\sqrt{17}}{8}$.\nThe boundary $b$ must lie in the interval $(0,1)$. We know that $4  \\sqrt{17}  5$.\nFor the first root: $\\frac{1+4}{8}  b_1  \\frac{1+5}{8}$, so $0.625  b_1  0.75$. This root is within the valid interval $(0,1)$.\nFor the second root: $\\frac{1-5}{8}  b_2  \\frac{1-4}{8}$, so $-0.5  b_2  -0.375$. This root is negative and thus is not a valid boundary.\nThe only valid solution for the optimal boundary is $b = \\frac{1+\\sqrt{17}}{8}$.",
            "answer": "$$\\boxed{\\frac{1+\\sqrt{17}}{8}}$$"
        }
    ]
}