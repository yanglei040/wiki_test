{
    "hands_on_practices": [
        {
            "introduction": "The axioms of probability form the bedrock of the entire theory, providing the fundamental rules that every valid probability measure must obey. This first exercise provides a direct application of these rules, asking you to act as a quality control engineer for proposed probability models. By testing different assignments against the core principles of non-negativity, normalization, and additivity, you will build a concrete understanding of what makes a probability measure valid .",
            "id": "1295797",
            "problem": "In the design of a simple error-detection system for digital communication, the outcome of any single bit transmission is classified into one of three mutually exclusive categories. The sample space of outcomes is $\\Omega = \\{S, E_1, E_2\\}$, where $S$ represents a successful transmission, $E_1$ represents a Type 1 error, and $E_2$ represents a Type 2 error. The event space, $\\mathcal{F}$, is the power set of $\\Omega$, meaning it contains all possible subsets of $\\Omega$.\n\nA function $P: \\mathcal{F} \\to \\mathbb{R}$ is proposed to assign probabilities to these events. For a function to be a valid probability measure on the measurable space $(\\Omega, \\mathcal{F})$, it must satisfy the axioms of probability.\n\nConsider the following five proposals for assigning probabilities. Which one of these proposals can be extended to define a valid probability measure on the entire event space $\\mathcal{F}$?\n\nA. $P(\\{S\\}) = 0.9$, $P(\\{E_1\\}) = 0.1$, $P(\\{E_2\\}) = 0.1$\n\nB. $P(\\{S\\}) = 1.2$, $P(\\{E_1\\}) = -0.1$, $P(\\{E_2\\}) = -0.1$\n\nC. $P(\\{S\\}) = 0.95$, $P(\\{E_1\\}) = 0.03$, $P(\\{E_2\\}) = 0.02$\n\nD. $P(\\{S\\}) = 0.8$, $P(\\{E_1\\}) = 0.1$, $P(\\{E_2\\}) = 0.05$\n\nE. $P(\\{S\\}) = 0.9$, $P(\\{E_1\\}) = 0.05$, and for the event $\\{S, E_1\\}$, the assignment is $P(\\{S, E_1\\}) = 1.0$.",
            "solution": "We work on the finite measurable space $(\\Omega,\\mathcal{F})$ with $\\Omega=\\{S,E_{1},E_{2}\\}$ and $\\mathcal{F}$ the power set. For a probability measure $P$ on this space, the axioms require:\n1) Nonnegativity: for all $A\\in\\mathcal{F}$, $P(A)\\geq 0$.\n2) Normalization: $P(\\Omega)=1$.\n3) Countable additivity: if $(A_{i})$ are pairwise disjoint, then $P\\big(\\bigcup_{i}A_{i}\\big)=\\sum_{i}P(A_{i})$.\n\nSince $\\Omega$ is finite and $\\mathcal{F}$ is the power set, any probability measure is determined by the singleton probabilities $p_{S}=P(\\{S\\})$, $p_{E_{1}}=P(\\{E_{1}\\})$, $p_{E_{2}}=P(\\{E_{2}\\})$ via\n$$\nP(A)=\\sum_{\\omega\\in A}P(\\{\\omega\\})\\quad\\text{for all }A\\subseteq\\Omega,\n$$\nand the axioms are equivalent to\n$$\np_{S}\\geq 0,\\quad p_{E_{1}}\\geq 0,\\quad p_{E_{2}}\\geq 0,\\quad p_{S}+p_{E_{1}}+p_{E_{2}}=1.\n$$\nIf probabilities for composite events are also specified, they must be consistent with finite additivity; for instance, because $\\{S\\}$ and $\\{E_{1}\\}$ are disjoint and their union is $\\{S,E_{1}\\}$, we must have\n$$\nP(\\{S,E_{1}\\})=P(\\{S\\})+P(\\{E_{1}\\}).\n$$\n\nWe now test each proposal:\n\nA. $P(\\{S\\})=0.9$, $P(\\{E_{1}\\})=0.1$, $P(\\{E_{2}\\})=0.1$. Then\n$$\nP(\\Omega)=P(\\{S\\}\\cup\\{E_{1}\\}\\cup\\{E_{2}\\})=0.9+0.1+0.1=1.1\\neq 1,\n$$\nviolating normalization. Not valid.\n\nB. $P(\\{S\\})=1.2$, $P(\\{E_{1}\\})=-0.1$, $P(\\{E_{2}\\})=-0.1$. This violates nonnegativity since $P(\\{E_{1}\\})0$ and $P(\\{E_{2}\\})0$. Not valid.\n\nC. $P(\\{S\\})=0.95$, $P(\\{E_{1}\\})=0.03$, $P(\\{E_{2}\\})=0.02$. These satisfy\n$$\n0.95\\geq 0,\\quad 0.03\\geq 0,\\quad 0.02\\geq 0,\\quad 0.95+0.03+0.02=1,\n$$\nso by the construction above, there is a unique extension to all $A\\subseteq\\Omega$ given by $P(A)=\\sum_{\\omega\\in A}P(\\{\\omega\\})$. Valid.\n\nD. $P(\\{S\\})=0.8$, $P(\\{E_{1}\\})=0.1$, $P(\\{E_{2}\\})=0.05$. Then\n$$\nP(\\Omega)=0.8+0.1+0.05=0.95\\neq 1,\n$$\nviolating normalization. Not valid.\n\nE. $P(\\{S\\})=0.9$, $P(\\{E_{1}\\})=0.05$, and $P(\\{S,E_{1}\\})=1.0$. Since $\\{S\\}$ and $\\{E_{1}\\}$ are disjoint and their union is $\\{S,E_{1}\\}$, finite additivity requires\n$$\nP(\\{S,E_{1}\\})=P(\\{S\\})+P(\\{E_{1}\\})=0.9+0.05=0.95,\n$$\nwhich contradicts the proposed value $1.0$. Hence this assignment is inconsistent and cannot be extended.\n\nTherefore, only proposal C can be extended to a valid probability measure on $\\mathcal{F}$.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "A probability space is more than just a probability measure; it also includes a $\\sigma$-algebra, $\\mathcal{F}$, which defines the set of all \"askable questions\" or measurable events. This practice moves our focus to this structural component, guiding you to construct different $\\sigma$-algebras on the same set of outcomes and see how they can represent distinct information structures . Understanding how to build and combine these event spaces is a key skill for modeling information flow and filtration in stochastic processes.",
            "id": "3331638",
            "problem": "In the context of modeling information structures in stochastic simulation and Monte Carlo methods, distinct collections of measurable events on the same underlying outcome space can be incomparable by inclusion. Let $\\Omega$ be a finite outcome space with $|\\Omega|=4$. \n\nUsing only the definitions of a $\\sigma$-algebra (closed under complements and countable unions, and containing $\\emptyset$ and $\\Omega$) and the notion of atoms induced by a partition, do the following:\n\n- Explicitly construct $2$ distinct $\\sigma$-algebras $\\mathcal{F}$ and $\\mathcal{G}$ on the same $\\Omega$ such that neither $\\mathcal{F} \\subset \\mathcal{G}$ nor $\\mathcal{G} \\subset \\mathcal{F}$ holds. Justify from first principles why each is a $\\sigma$-algebra and why neither contains the other.\n\n- Then, without appealing to any unproven theorems beyond these definitions, determine the cardinality of the $\\sigma$-algebra $\\sigma(\\mathcal{F} \\cup \\mathcal{G})$ generated by the union $\\mathcal{F} \\cup \\mathcal{G}$.\n\nYour final response must be a single integer equal to the number of sets in $\\sigma(\\mathcal{F} \\cup \\mathcal{G})$. No rounding is required. Do not include units.",
            "solution": "Let the finite outcome space be $\\Omega = \\{1, 2, 3, 4\\}$, which satisfies the condition $|\\Omega|=4$.\n\nFirst, we construct two distinct $\\sigma$-algebras, $\\mathcal{F}$ and $\\mathcal{G}$, on $\\Omega$ that are incomparable under set inclusion. A $\\sigma$-algebra on a finite set is generated by a partition of that set. The elements of the $\\sigma$-algebra are all possible unions of the disjoint sets (atoms) in the partition.\n\nLet us define a partition of $\\Omega$ as $P_{\\mathcal{F}} = \\{\\{1, 2\\}, \\{3, 4\\}\\}$. The atoms of the corresponding $\\sigma$-algebra $\\mathcal{F}$ are $A_1 = \\{1, 2\\}$ and $A_2 = \\{3, 4\\}$. The $\\sigma$-algebra $\\mathcal{F}$ consists of all possible unions of these atoms, which includes the empty union (the empty set) and the union of all atoms ($\\Omega$).\nThus, $\\mathcal{F} = \\{\\emptyset, \\{1, 2\\}, \\{3, 4\\}, \\{1, 2, 3, 4\\}\\}$.\n\nTo verify from first principles that $\\mathcal{F}$ is a $\\sigma$-algebra:\n1.  $\\Omega \\in \\mathcal{F}$: By construction, $\\Omega = \\{1, 2, 3, 4\\} = \\{1, 2\\} \\cup \\{3, 4\\}$ is in $\\mathcal{F}$.\n2.  Closure under complementation:\n    - $\\emptyset^c = \\Omega \\in \\mathcal{F}$\n    - $\\Omega^c = \\emptyset \\in \\mathcal{F}$\n    - $(\\{1, 2\\})^c = \\{3, 4\\} \\in \\mathcal{F}$\n    - $(\\{3, 4\\})^c = \\{1, 2\\} \\in \\mathcal{F}$\n    The set is closed under complements.\n3.  Closure under countable (and thus finite) unions: All possible unions of elements of $\\mathcal{F}$ are already in $\\mathcal{F}$. For example, $\\{1, 2\\} \\cup \\{3, 4\\} = \\{1, 2, 3, 4\\} \\in \\mathcal{F}$. Other unions are trivial (e.g., $A \\cup \\emptyset = A$). The set is closed under unions.\nTherefore, $\\mathcal{F}$ is a valid $\\sigma$-algebra.\n\nNext, we construct a second $\\sigma$-algebra, $\\mathcal{G}$. To ensure it is incomparable with $\\mathcal{F}$, we must choose a partition whose atoms are not unions of the atoms of $\\mathcal{F}$, nor are the atoms of $\\mathcal{F}$ unions of atoms of the new partition.\nLet us define the partition $P_{\\mathcal{G}} = \\{\\{1, 3\\}, \\{2, 4\\}\\}$. The atoms are $B_1 = \\{1, 3\\}$ and $B_2 = \\{2, 4\\}$.\nThe resulting $\\sigma$-algebra is $\\mathcal{G} = \\{\\emptyset, \\{1, 3\\}, \\{2, 4\\}, \\{1, 2, 3, 4\\}\\}$.\nThe verification that $\\mathcal{G}$ is a $\\sigma$-algebra is analogous to the verification for $\\mathcal{F}$.\n\nNow, we justify why neither contains the other:\n- To show $\\mathcal{F} \\not\\subset \\mathcal{G}$, we must find an element in $\\mathcal{F}$ that is not in $\\mathcal{G}$. The set $\\{1, 2\\} \\in \\mathcal{F}$. However, the elements of $\\mathcal{G}$ are $\\emptyset, \\{1, 3\\}, \\{2, 4\\}, \\Omega$. Clearly, $\\{1, 2\\} \\notin \\mathcal{G}$. Thus, $\\mathcal{F} \\not\\subset \\mathcal{G}$.\n- To show $\\mathcal{G} \\not\\subset \\mathcal{F}$, we must find an element in $\\mathcal{G}$ that is not in $\\mathcal{F}$. The set $\\{1, 3\\} \\in \\mathcal{G}$. The elements of $\\mathcal{F}$ are $\\emptyset, \\{1, 2\\}, \\{3, 4\\}, \\Omega$. Clearly, $\\{1, 3\\} \\notin \\mathcal{F}$. Thus, $\\mathcal{G} \\not\\subset \\mathcal{F}$.\nThe two $\\sigma$-algebras are therefore incomparable by inclusion.\n\nThe second part of the problem is to determine the cardinality of $\\sigma(\\mathcal{F} \\cup \\mathcal{G})$, the smallest $\\sigma$-algebra containing both $\\mathcal{F}$ and $\\mathcal{G}$. Let us denote this a $\\sigma$-algebra by $\\mathcal{H}$.\n$\\mathcal{H}$ must contain all elements of $\\mathcal{F}$ and all elements of $\\mathcal{G}$. Since a $\\sigma$-algebra must be closed under intersections (as $A \\cap B = (A^c \\cup B^c)^c$), $\\mathcal{H}$ must contain all intersections of sets from $\\mathcal{F} \\cup \\mathcal{G}$.\n\nThe atoms of $\\mathcal{H}$ are formed by the non-empty intersections of the atoms of $\\mathcal{F}$ and $\\mathcal{G}$.\nThe atoms of $\\mathcal{F}$ are $P_{\\mathcal{F}} = \\{\\{1, 2\\}, \\{3, 4\\}\\}$.\nThe atoms of $\\mathcal{G}$ are $P_{\\mathcal{G}} = \\{\\{1, 3\\}, \\{2, 4\\}\\}$.\n\nLet's compute the intersections:\n- Intersection of the first atom of $\\mathcal{F}$ with the atoms of $\\mathcal{G}$:\n  - $\\{1, 2\\} \\cap \\{1, 3\\} = \\{1\\}$\n  - $\\{1, 2\\} \\cap \\{2, 4\\} = \\{2\\}$\n- Intersection of the second atom of $\\mathcal{F}$ with the atoms of $\\mathcal{G}$:\n  - $\\{3, 4\\} \\cap \\{1, 3\\} = \\{3\\}$\n  - $\\{3, 4\\} \\cap \\{2, 4\\} = \\{4\\}$\n\nThese intersection operations show that any $\\sigma$-algebra containing $\\mathcal{F}$ and $\\mathcal{G}$ must contain the singleton sets $\\{1\\}, \\{2\\}, \\{3\\}, \\{4\\}$. These singletons form a partition of $\\Omega$, $P_{\\mathcal{H}} = \\{\\{1\\}, \\{2\\}, \\{3\\}, \\{4\\}\\}$. These are the atoms of the generated $\\sigma$-algebra $\\mathcal{H} = \\sigma(\\mathcal{F} \\cup \\mathcal{G})$.\n\nSince $\\mathcal{H}$ is a $\\sigma$-algebra and it contains all the singleton sets $\\{i\\}$ for $i \\in \\Omega$, it must be closed under countable unions. Therefore, any subset of $\\Omega$ can be formed as a finite union of these singletons, and thus must belong to $\\mathcal{H}$. For any set $A \\subseteq \\Omega$, we have $A = \\bigcup_{i \\in A} \\{i\\}$. Since each $\\{i\\} \\in \\mathcal{H}$, their union $A$ must also be in $\\mathcal{H}$.\nThis means that every subset of $\\Omega$ must be in $\\mathcal{H}$. The collection of all subsets of $\\Omega$ is the power set of $\\Omega$, denoted $2^\\Omega$.\nTherefore, $\\mathcal{H} = \\sigma(\\mathcal{F} \\cup \\mathcal{G}) = 2^\\Omega$.\n\nThe problem asks for the cardinality of this generated $\\sigma$-algebra. The cardinality of the power set of a finite set with $n$ elements is $2^n$.\nIn our case, $|\\Omega| = 4$.\nSo, the cardinality of $\\mathcal{H}$ is $|2^\\Omega| = 2^{|\\Omega|} = 2^4$.\n\nCalculating the value:\n$2^4 = 2 \\times 2 \\times 2 \\times 2 = 16$.\n\nThe number of sets in $\\sigma(\\mathcal{F} \\cup \\mathcal{G})$ is $16$. This result relies only on the axioms of a $\\sigma$-algebra and the concept of it being generated by a partition of atoms.",
            "answer": "$$\n\\boxed{16}\n$$"
        },
        {
            "introduction": "With a probability space fully defined, we can investigate relationships between events, with independence being the most critical concept for simplifying complex models. However, the notion of independence is more nuanced than it may first appear. This exercise explores the crucial distinction between pairwise and mutual independence using a classic counterexample , demonstrating a potential pitfall in probabilistic modeling and how to quantify the resulting modeling error.",
            "id": "3331642",
            "problem": "Consider the following construction motivated by diagnostic counterexamples in stochastic simulation and Monte Carlo methods. Let the probability space be $(\\Omega,\\mathcal{F},\\mathbb{P})$ with $\\Omega=\\{(0,0),(0,1),(1,0),(1,1)\\}$, $\\mathcal{F}=2^{\\Omega}$, and $\\mathbb{P}$ the uniform probability measure on $\\Omega$ (each atom has probability $1/4$). Let $X:\\Omega\\to\\{0,1\\}$ and $Y:\\Omega\\to\\{0,1\\}$ be the coordinate projection random variables, and define the events\n$$A=\\{X=1\\},\\quad B=\\{Y=1\\},\\quad C=\\{X=Y\\}.$$\nThese events are used to test independence assumptions in algorithms that rely on sampling from Bernoulli variables.\n\nUsing only the axioms of probability and the definitions of independence, do the following:\n\n- Show that $A$, $B$, and $C$ are pairwise independent, but not mutually independent.\n- Compute the probabilities $\\mathbb{P}(A)$, $\\mathbb{P}(B)$, $\\mathbb{P}(C)$, $\\mathbb{P}(A\\cap B)$, $\\mathbb{P}(A\\cap C)$, $\\mathbb{P}(B\\cap C)$, and $\\mathbb{P}(A\\cap B\\cap C)$.\n- Let $I_A$, $I_B$, and $I_C$ denote the indicator random variables of $A$, $B$, and $C$, respectively. Determine the joint distribution of $(I_A,I_B,I_C)$ and the product measure $\\mathbb{P}_{I_A}\\otimes\\mathbb{P}_{I_B}\\otimes\\mathbb{P}_{I_C}$ on $\\{0,1\\}^3$.\n\nFinally, compute the Kullback–Leibler divergence (KLD) between the joint distribution of $(I_A,I_B,I_C)$ and the product of the marginals, that is\n$$D_{\\mathrm{KL}}\\!\\left(\\mathbb{P}_{(I_A,I_B,I_C)}\\,\\middle\\|\\,\\mathbb{P}_{I_A}\\otimes\\mathbb{P}_{I_B}\\otimes\\mathbb{P}_{I_C}\\right),$$\nand express your final answer as a closed-form analytic expression. No rounding is required.",
            "solution": "We begin by formally defining the events and computing their probabilities based on the provided probability space $(\\Omega,\\mathcal{F},\\mathbb{P})$, where $\\Omega=\\{(0,0),(0,1),(1,0),(1,1)\\}$ and the measure $\\mathbb{P}$ is uniform, assigning a probability of $\\frac{1}{4}$ to each elementary outcome. The random variables $X$ and $Y$ are the coordinate projections, so for an outcome $\\omega=(\\omega_1, \\omega_2) \\in \\Omega$, we have $X(\\omega)=\\omega_1$ and $Y(\\omega)=\\omega_2$.\n\nThe events $A$, $B$, and $C$ are defined as subsets of $\\Omega$:\n$A = \\{ \\omega \\in \\Omega \\mid X(\\omega)=1 \\} = \\{(1,0), (1,1)\\}$\n$B = \\{ \\omega \\in \\Omega \\mid Y(\\omega)=1 \\} = \\{(0,1), (1,1)\\}$\n$C = \\{ \\omega \\in \\Omega \\mid X(\\omega)=Y(\\omega) \\} = \\{(0,0), (1,1)\\}$\n\nWe compute the probabilities of these events:\n$\\mathbb{P}(A) = \\frac{|A|}{|\\Omega|} = \\frac{2}{4} = \\frac{1}{2}$\n$\\mathbb{P}(B) = \\frac{|B|}{|\\Omega|} = \\frac{2}{4} = \\frac{1}{2}$\n$\\mathbb{P}(C) = \\frac{|C|}{|\\Omega|} = \\frac{2}{4} = \\frac{1}{2}$\n\nNext, we identify the pairwise and three-way intersections of these events:\n$A \\cap B = \\{(1,1)\\}$\n$A \\cap C = \\{(1,1)\\}$\n$B \\cap C = \\{(1,1)\\}$\n$A \\cap B \\cap C = (A \\cap B) \\cap C = \\{(1,1)\\} \\cap \\{(0,0), (1,1)\\} = \\{(1,1)\\}$\n\nWe compute the probabilities of these intersections:\n$\\mathbb{P}(A \\cap B) = \\frac{|A \\cap B|}{|\\Omega|} = \\frac{1}{4}$\n$\\mathbb{P}(A \\cap C) = \\frac{|A \\cap C|}{|\\Omega|} = \\frac{1}{4}$\n$\\mathbb{P}(B \\cap C) = \\frac{|B \\cap C|}{|\\Omega|} = \\frac{1}{4}$\n$\\mathbb{P}(A \\cap B \\cap C) = \\frac{|A \\cap B \\cap C|}{|\\Omega|} = \\frac{1}{4}$\n\nWith these probabilities, we can demonstrate the independence properties.\nFor pairwise independence, we check if $\\mathbb{P}(E_1 \\cap E_2) = \\mathbb{P}(E_1)\\mathbb{P}(E_2)$ for each pair of events.\n$\\mathbb{P}(A)\\mathbb{P}(B) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}$. Since this equals $\\mathbb{P}(A \\cap B)$, events $A$ and $B$ are independent.\n$\\mathbb{P}(A)\\mathbb{P}(C) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}$. Since this equals $\\mathbb{P}(A \\cap C)$, events $A$ and $C$ are independent.\n$\\mathbb{P}(B)\\mathbb{P}(C) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}$. Since this equals $\\mathbb{P}(B \\cap C)$, events $B$ and $C$ are independent.\nThus, the events $A$, $B$, and $C$ are pairwise independent.\n\nFor mutual independence, the condition $\\mathbb{P}(A \\cap B \\cap C) = \\mathbb{P}(A)\\mathbb{P}(B)\\mathbb{P}(C)$ must also hold.\n$\\mathbb{P}(A)\\mathbb{P}(B)\\mathbb{P}(C) = \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{8}$.\nWe have $\\mathbb{P}(A \\cap B \\cap C) = \\frac{1}{4}$.\nSince $\\frac{1}{4} \\neq \\frac{1}{8}$, the events $A$, $B$, and $C$ are not mutually independent.\n\nNext, we determine the joint distribution of the indicator random variables $(I_A, I_B, I_C)$, which we denote by $P(i,j,k) = \\mathbb{P}(I_A=i, I_B=j, I_C=k)$. We evaluate the vector $(I_A(\\omega), I_B(\\omega), I_C(\\omega))$ for each $\\omega \\in \\Omega$:\nFor $\\omega=(0,0)$: $X=0, Y=0$. Events $A,B$ are false, $C$ is true. The outcome is $(0,0,1)$.\nFor $\\omega=(0,1)$: $X=0, Y=1$. Events $A,C$ are false, $B$ is true. The outcome is $(0,1,0)$.\nFor $\\omega=(1,0)$: $X=1, Y=0$. Events $B,C$ are false, $A$ is true. The outcome is $(1,0,0)$.\nFor $\\omega=(1,1)$: $X=1, Y=1$. Events $A,B,C$ are all true. The outcome is $(1,1,1)$.\nSince each $\\omega \\in \\Omega$ has probability $\\frac{1}{4}$, the joint distribution $\\mathbb{P}_{(I_A,I_B,I_C)}$ is given by the probability mass function $P(i,j,k)$:\n$P(0,0,1) = \\frac{1}{4}$, $P(0,1,0) = \\frac{1}{4}$, $P(1,0,0) = \\frac{1}{4}$, $P(1,1,1) = \\frac{1}{4}$.\nThe probability is $0$ for the other four outcomes in $\\{0,1\\}^3$: $(0,0,0)$, $(0,1,1)$, $(1,0,1)$, $(1,1,0)$.\n\nNow, we determine the product measure $\\mathbb{P}_{I_A}\\otimes\\mathbb{P}_{I_B}\\otimes\\mathbb{P}_{I_C}$. Let us denote its probability mass function by $Q(i,j,k)$. This measure corresponds to the distribution of three mutually independent Bernoulli variables.\nThe marginal distributions are:\n$\\mathbb{P}_{I_A}(1) = \\mathbb{P}(A) = \\frac{1}{2}$, so $\\mathbb{P}_{I_A}(0) = 1-\\frac{1}{2} = \\frac{1}{2}$.\n$\\mathbb{P}_{I_B}(1) = \\mathbb{P}(B) = \\frac{1}{2}$, so $\\mathbb{P}_{I_B}(0) = 1-\\frac{1}{2} = \\frac{1}{2}$.\n$\\mathbb{P}_{I_C}(1) = \\mathbb{P}(C) = \\frac{1}{2}$, so $\\mathbb{P}_{I_C}(0) = 1-\\frac{1}{2} = \\frac{1}{2}$.\nThe product measure is defined by $Q(i,j,k) = \\mathbb{P}_{I_A}(i) \\mathbb{P}_{I_B}(j) \\mathbb{P}_{I_C}(k)$.\nFor any $(i,j,k) \\in \\{0,1\\}^3$, $Q(i,j,k) = \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{8}$. This is the uniform distribution on the $8$ vertices of the cube $\\{0,1\\}^3$.\n\nFinally, we compute the Kullback–Leibler divergence from the product-of-marginals distribution $Q$ to the true joint distribution $P$. The problem asks for $D_{\\mathrm{KL}}(P \\| Q)$.\nThe formula for the KLD for discrete distributions is:\n$$D_{\\mathrm{KL}}(P \\| Q) = \\sum_{(i,j,k) \\in \\{0,1\\}^3} P(i,j,k) \\ln\\left(\\frac{P(i,j,k)}{Q(i,j,k)}\\right)$$\nThe summation is non-zero only over the support of $P$, which is the set $S = \\{(0,0,1), (0,1,0), (1,0,0), (1,1,1)\\}$.\nFor each $(i,j,k) \\in S$, we have $P(i,j,k) = \\frac{1}{4}$ and $Q(i,j,k) = \\frac{1}{8}$.\nThe ratio $\\frac{P(i,j,k)}{Q(i,j,k)}$ is constant for all points in the support of $P$:\n$$\\frac{P(i,j,k)}{Q(i,j,k)} = \\frac{1/4}{1/8} = 2$$\nSubstituting this into the KLD formula:\n$$D_{\\mathrm{KL}}(P \\| Q) = \\sum_{(i,j,k) \\in S} \\frac{1}{4} \\ln(2)$$\nSince the size of the support set $S$ is $|S|=4$, the sum becomes:\n$$D_{\\mathrm{KL}}(P \\| Q) = \\frac{1}{4}\\ln(2) + \\frac{1}{4}\\ln(2) + \\frac{1}{4}\\ln(2) + \\frac{1}{4}\\ln(2) = 4 \\left( \\frac{1}{4} \\ln(2) \\right) = \\ln(2)$$\nThe Kullback–Leibler divergence is $\\ln(2)$.",
            "answer": "$$\\boxed{\\ln(2)}$$"
        }
    ]
}