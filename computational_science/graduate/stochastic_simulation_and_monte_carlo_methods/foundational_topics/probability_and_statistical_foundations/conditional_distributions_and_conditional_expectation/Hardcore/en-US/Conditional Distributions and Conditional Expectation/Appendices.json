{
    "hands_on_practices": [
        {
            "introduction": "Before applying conditional expectation in complex simulations, one must master its calculation in fundamental settings. The bivariate normal distribution is arguably the most important continuous case, forming the bedrock for linear models, Kalman filters, and Gaussian processes. This foundational exercise guides you through the first-principles derivation of the conditional density and expectation, moving from the joint distribution to the conditional by using basic matrix algebra and completing the square—a technique central to working with Gaussian forms .",
            "id": "3297685",
            "problem": "Consider a jointly Gaussian pair $(X,Y)$ with mean vector $(\\mu_X,\\mu_Y)$ and covariance matrix\n$$\n\\Sigma = \\begin{pmatrix}\n\\sigma_X^2  \\rho \\sigma_X \\sigma_Y \\\\\n\\rho \\sigma_X \\sigma_Y  \\sigma_Y^2\n\\end{pmatrix},\n$$\nwhere $\\sigma_X0$, $\\sigma_Y0$, and $|\\rho|1$. In the context of a Gibbs step within a Markov Chain Monte Carlo (MCMC) algorithm for simulating from a multivariate normal distribution, one needs the conditional distribution of one component given the other. Starting only from the definition of the joint Gaussian density and the definition of a conditional density via the ratio $f_{X|Y}(x|y) = f_{X,Y}(x,y)/f_Y(y)$, and using standard linear algebra identities for inverting a covariance matrix and completing the square, derive the conditional density $f_{X|Y}(x \\mid y)$ for fixed $y$. Then deduce the conditional expectation $E[X \\mid Y=y]$.\n\nYour reasoning must start from core definitions and well-tested facts and proceed by explicit algebraic derivations, without invoking any shortcut formulas for conditional normals. The final answer must be a single closed-form symbolic expression for $E[X \\mid Y=y]$. Do not provide any numerical approximations.",
            "solution": "The problem is valid as it is scientifically grounded in probability theory, well-posed with all necessary information provided, and objective in its formulation. The task is a standard derivation in multivariate statistics.\n\nLet the random vector $\\mathbf{Z} = \\begin{pmatrix} X \\\\ Y \\end{pmatrix}$ follow a bivariate normal distribution with mean vector $\\mathbf{\\mu} = \\begin{pmatrix} \\mu_X \\\\ \\mu_Y \\end{pmatrix}$ and covariance matrix $\\Sigma = \\begin{pmatrix} \\sigma_X^2  \\rho \\sigma_X \\sigma_Y \\\\ \\rho \\sigma_X \\sigma_Y  \\sigma_Y^2 \\end{pmatrix}$. The joint probability density function (PDF) is given by\n$$ f_{\\mathbf{Z}}(\\mathbf{z}) = \\frac{1}{\\sqrt{(2\\pi)^2 \\det(\\Sigma)}} \\exp\\left(-\\frac{1}{2} (\\mathbf{z}-\\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{z}-\\mathbf{\\mu})\\right) $$\nwhere $\\mathbf{z} = \\begin{pmatrix} x \\\\ y \\end{pmatrix}$.\n\nFirst, we compute the determinant and inverse of the covariance matrix $\\Sigma$. The determinant is\n$$ \\det(\\Sigma) = (\\sigma_X^2)(\\sigma_Y^2) - (\\rho \\sigma_X \\sigma_Y)^2 = \\sigma_X^2 \\sigma_Y^2 (1-\\rho^2) $$\nThe condition $|\\rho|  1$ ensures that $\\det(\\Sigma)  0$ and the matrix is invertible. The inverse is\n$$ \\Sigma^{-1} = \\frac{1}{\\det(\\Sigma)} \\begin{pmatrix} \\sigma_Y^2  -\\rho \\sigma_X \\sigma_Y \\\\ -\\rho \\sigma_X \\sigma_Y  \\sigma_X^2 \\end{pmatrix} = \\frac{1}{\\sigma_X^2 \\sigma_Y^2 (1-\\rho^2)} \\begin{pmatrix} \\sigma_Y^2  -\\rho \\sigma_X \\sigma_Y \\\\ -\\rho \\sigma_X \\sigma_Y  \\sigma_X^2 \\end{pmatrix} = \\frac{1}{1-\\rho^2} \\begin{pmatrix} 1/\\sigma_X^2  -\\rho/(\\sigma_X \\sigma_Y) \\\\ -\\rho/(\\sigma_X \\sigma_Y)  1/\\sigma_Y^2 \\end{pmatrix} $$\nThe quadratic form in the exponent is $Q(x,y) = (\\mathbf{z}-\\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{z}-\\mathbf{\\mu})$, which expands to:\n$$ Q(x,y) = \\begin{pmatrix} x-\\mu_X  y-\\mu_Y \\end{pmatrix} \\frac{1}{1-\\rho^2} \\begin{pmatrix} 1/\\sigma_X^2  -\\rho/(\\sigma_X \\sigma_Y) \\\\ -\\rho/(\\sigma_X \\sigma_Y)  1/\\sigma_Y^2 \\end{pmatrix} \\begin{pmatrix} x-\\mu_X \\\\ y-\\mu_Y \\end{pmatrix} $$\n$$ Q(x,y) = \\frac{1}{1-\\rho^2} \\left[ \\frac{(x-\\mu_X)^2}{\\sigma_X^2} - \\frac{2\\rho(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X\\sigma_Y} + \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} \\right] $$\nSo, the joint PDF $f_{X,Y}(x,y)$ is:\n$$ f_{X,Y}(x,y) = \\frac{1}{2\\pi \\sigma_X \\sigma_Y \\sqrt{1-\\rho^2}} \\exp\\left(-\\frac{1}{2(1-\\rho^2)} \\left[ \\frac{(x-\\mu_X)^2}{\\sigma_X^2} - \\frac{2\\rho(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X\\sigma_Y} + \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} \\right]\\right) $$\nThe marginal distribution of $Y$ is a normal distribution with mean $\\mu_Y$ and variance $\\sigma_Y^2$. Its PDF is:\n$$ f_Y(y) = \\frac{1}{\\sqrt{2\\pi \\sigma_Y^2}} \\exp\\left(-\\frac{(y-\\mu_Y)^2}{2\\sigma_Y^2}\\right) = \\frac{1}{\\sigma_Y \\sqrt{2\\pi}} \\exp\\left(-\\frac{(y-\\mu_Y)^2}{2\\sigma_Y^2}\\right) $$\nBy definition, the conditional density $f_{X|Y}(x|y)$ is the ratio of the joint density to the marginal density:\n$$ f_{X \\mid Y}(x \\mid y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)} $$\nThe constant part of the conditional PDF is:\n$$ \\frac{1 / (2\\pi \\sigma_X \\sigma_Y \\sqrt{1-\\rho^2})}{1 / (\\sigma_Y \\sqrt{2\\pi})} = \\frac{\\sigma_Y \\sqrt{2\\pi}}{2\\pi \\sigma_X \\sigma_Y \\sqrt{1-\\rho^2}} = \\frac{1}{\\sigma_X \\sqrt{2\\pi} \\sqrt{1-\\rho^2}} $$\nThe exponent of the conditional PDF is the difference between the exponent of the joint PDF and the exponent of the marginal PDF:\n$$ \\text{Exp}_{X|Y} = -\\frac{1}{2(1-\\rho^2)} \\left[ \\frac{(x-\\mu_X)^2}{\\sigma_X^2} - \\frac{2\\rho(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X\\sigma_Y} + \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} \\right] - \\left(-\\frac{(y-\\mu_Y)^2}{2\\sigma_Y^2}\\right) $$\n$$ \\text{Exp}_{X|Y} = -\\frac{1}{2} \\left\\{ \\frac{1}{1-\\rho^2} \\left[ \\frac{(x-\\mu_X)^2}{\\sigma_X^2} - \\frac{2\\rho(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X\\sigma_Y} + \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} \\right] - \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} \\right\\} $$\nWe combine the terms involving $(y-\\mu_Y)^2$:\n$$ \\text{Exp}_{X|Y} = -\\frac{1}{2} \\left\\{ \\frac{1}{1-\\rho^2}\\left[ \\frac{(x-\\mu_X)^2}{\\sigma_X^2} - \\frac{2\\rho(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X\\sigma_Y} \\right] + \\left(\\frac{1}{1-\\rho^2} - 1\\right)\\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} \\right\\} $$\nSince $\\frac{1}{1-\\rho^2} - 1 = \\frac{1 - (1-\\rho^2)}{1-\\rho^2} = \\frac{\\rho^2}{1-\\rho^2}$, the expression becomes:\n$$ \\text{Exp}_{X|Y} = -\\frac{1}{2} \\left\\{ \\frac{1}{1-\\rho^2}\\left[ \\frac{(x-\\mu_X)^2}{\\sigma_X^2} - \\frac{2\\rho(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X\\sigma_Y} + \\rho^2\\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} \\right] \\right\\} $$\n$$ \\text{Exp}_{X|Y} = -\\frac{1}{2(1-\\rho^2)} \\left[ \\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)^2 - 2\\rho\\left(\\frac{x-\\mu_X}{\\sigma_X}\\right)\\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right) + \\rho^2\\left(\\frac{y-\\mu_Y}{\\sigma_Y}\\right)^2 \\right] $$\nThe term in the square brackets is a perfect square. We complete the square for the variable $x$:\n$$ \\text{Exp}_{X|Y} = -\\frac{1}{2(1-\\rho^2)} \\left( \\frac{x-\\mu_X}{\\sigma_X} - \\rho\\frac{y-\\mu_Y}{\\sigma_Y} \\right)^2 $$\nTo identify the mean and variance, we rearrange this expression into the standard form $-\\frac{(x-\\mu)^2}{2\\sigma^2}$:\n$$ \\text{Exp}_{X|Y} = -\\frac{1}{2(1-\\rho^2)\\sigma_X^2} \\left( (x-\\mu_X) - \\rho\\frac{\\sigma_X}{\\sigma_Y}(y-\\mu_Y) \\right)^2 $$\n$$ \\text{Exp}_{X|Y} = -\\frac{\\left( x - \\left[\\mu_X + \\rho\\frac{\\sigma_X}{\\sigma_Y}(y-\\mu_Y)\\right] \\right)^2}{2\\sigma_X^2(1-\\rho^2)} $$\nThe conditional density $f_{X|Y}(x|y)$ is therefore:\n$$ f_{X \\mid Y}(x \\mid y) = \\frac{1}{\\sigma_X \\sqrt{2\\pi(1-\\rho^2)}} \\exp\\left( -\\frac{\\left( x - \\left[\\mu_X + \\rho\\frac{\\sigma_X}{\\sigma_Y}(y-\\mu_Y)\\right] \\right)^2}{2\\sigma_X^2(1-\\rho^2)} \\right) $$\nThis is the PDF of a normal distribution with respect to $x$. By comparing this to the general form of a normal PDF, $f(z) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z-\\mu)^2}{2\\sigma^2}\\right)$, we can identify the conditional mean and conditional variance.\nThe conditional mean is the expectation of $X$ given $Y=y$:\n$$ E[X \\mid Y=y] = \\mu_{X \\mid Y} = \\mu_X + \\rho\\frac{\\sigma_X}{\\sigma_Y}(y-\\mu_Y) $$\nThe conditional variance is:\n$$ \\operatorname{Var}(X \\mid Y=y) = \\sigma_{X \\mid Y}^2 = \\sigma_X^2(1-\\rho^2) $$\nThe problem asks specifically for the conditional expectation $E[X \\mid Y=y]$. Based on the derivation, this is the mean of the conditional normal distribution derived above.",
            "answer": "$$ \\boxed{\\mu_X + \\rho \\frac{\\sigma_X}{\\sigma_Y}(y - \\mu_Y)} $$"
        },
        {
            "introduction": "One of the primary motivations for studying conditional expectation in Monte Carlo methods is its power to reduce variance. The Rao-Blackwell theorem provides the theoretical guarantee that replacing an estimator with its conditional expectation with respect to a sufficient statistic yields a new estimator that is at least as good, and often much better. This practice provides a hands-on demonstration of this principle by having you first derive a classic Rao-Blackwellized estimator and then implement a simulation to empirically confirm its superior efficiency .",
            "id": "3297654",
            "problem": "Consider the following Monte Carlo (MC) study of the Rao–Blackwell (RB) improvement via conditional expectation. Let $\\{Y_i\\}_{i=1}^n$ be independent and identically distributed with $Y_i \\sim \\mathrm{Poisson}(\\lambda)$ for a fixed but unknown parameter $\\lambda  0$. You are interested in estimating the functional $\\theta(\\lambda) = \\mathbb{E}_\\lambda[X] = e^{-\\lambda}$ using an unbiased estimator $X$ and its Rao–Blackwellized version $\\mathbb{E}[X \\mid \\sigma(T)]$, where $T$ is a sufficient statistic for $\\lambda$. The task is to derive $\\mathbb{E}[X \\mid \\sigma(T)]$ from first principles, and then to compare the MC variance of $X$ against the MC variance of $\\mathbb{E}[X \\mid \\sigma(T)]$ across several sample sizes.\n\nDefinitions and setup:\n- Let $X = \\mathbf{1}\\{Y_1 = 0\\}$, where $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function. Note that $\\mathbb{E}_\\lambda[X] = \\mathbb{P}_\\lambda(Y_1 = 0) = e^{-\\lambda}$.\n- Let $T = \\sum_{i=1}^n Y_i$. Use the factorization theorem to justify that $T$ is sufficient for $\\lambda$.\n- Use the definition of conditional expectation to construct the RB estimator by projecting $X$ onto $\\sigma(T)$, i.e., derive $\\mathbb{E}[X \\mid \\sigma(T)]$ explicitly as a function of $T$ and $n$.\n\nYour tasks:\n1. Starting from the joint probability mass function (PMF) of $(Y_1,\\dots,Y_n)$ and the definition of sufficiency, establish that $T$ is sufficient for $\\lambda$. Then, using the conditional distribution of $Y_1$ given $T=t$, derive a closed form for $\\mathbb{E}[X \\mid T=t]$ and hence for $\\mathbb{E}[X \\mid \\sigma(T)]$.\n2. Explain why $\\mathbb{E}[X \\mid \\sigma(T)]$ is still unbiased for $\\theta(\\lambda)$, and justify (using the Rao–Blackwell theorem) why $\\operatorname{Var}(\\mathbb{E}[X \\mid \\sigma(T)]) \\le \\operatorname{Var}(X)$ for any $n \\in \\mathbb{N}$ and $\\lambda  0$.\n3. Implement an MC experiment to estimate and compare $\\operatorname{Var}(X)$ and $\\operatorname{Var}(\\mathbb{E}[X \\mid \\sigma(T)])$ via simulation. For a given pair $(\\lambda,n)$, generate $M$ independent and identically distributed replicates of $(Y_1,\\dots,Y_n)$, compute the corresponding $X$ and $\\mathbb{E}[X \\mid \\sigma(T)]$ for each replicate, and then compute unbiased MC sample variances (with Bessel’s correction) of $X$ and of $\\mathbb{E}[X \\mid \\sigma(T)]$. Report the ratio\n$$\nR(\\lambda,n) = \\frac{\\widehat{\\operatorname{Var}}_{\\mathrm{MC}}(\\mathbb{E}[X \\mid \\sigma(T)])}{\\widehat{\\operatorname{Var}}_{\\mathrm{MC}}(X)}.\n$$\n4. Use the following MC settings for all test cases: $M = 100000$ and a fixed random seed of $0$ to ensure reproducibility.\n5. Test suite: compute $R(\\lambda,n)$ for the parameter pairs $(\\lambda,n)$ in the following list (in this exact order):\n   - $(\\lambda,n) = (0.3,1)$,\n   - $(\\lambda,n) = (0.3,2)$,\n   - $(\\lambda,n) = (1.0,5)$,\n   - $(\\lambda,n) = (3.0,10)$,\n   - $(\\lambda,n) = (5.0,25)$.\n6. Final output format: your program must print a single line containing a Python-style list with the five ratios $[R(\\lambda,n),\\dots]$ in the order given above, where each ratio is rounded to $6$ decimal places. There are no physical units, no angles, and no percentages in this problem; numerical answers must be real numbers.\n\nAdditional requirements:\n- Your program must be a complete, runnable script that performs the MC simulation, with no user input.\n- Use only the specified runtime environment and libraries as detailed in the final answer section.",
            "solution": "The problem is deemed valid. It is a well-posed and scientifically grounded exercise in computational statistics, specifically demonstrating the application of the Rao-Blackwell theorem through Monte Carlo simulation. All necessary information, including the model, estimators, parameters, and simulation settings, is provided in a clear and consistent manner.\n\nThe solution proceeds in three parts:\n1. Derivation of the Rao–Blackwell estimator.\n2. Theoretical justification for its properties (unbiasedness and variance reduction).\n3. Description of the Monte Carlo simulation to compare variances.\n\n### 1. Derivation of the Rao–Blackwell Estimator\n\nLet $\\{Y_i\\}_{i=1}^n$ be independent and identically distributed random variables with $Y_i \\sim \\mathrm{Poisson}(\\lambda)$. The initial estimator for $\\theta(\\lambda) = e^{-\\lambda}$ is $X = \\mathbf{1}\\{Y_1 = 0\\}$. The Rao–Blackwell theorem states that if $T$ is a sufficient statistic for $\\lambda$, then the estimator $\\tilde{X} = \\mathbb{E}[X \\mid \\sigma(T)]$ is also unbiased for $\\theta(\\lambda)$ and has a variance no larger than that of $X$.\n\n**Sufficiency of $T = \\sum_{i=1}^n Y_i$:**\n\nThe joint probability mass function (PMF) of the sample $\\mathbf{Y} = (Y_1, \\dots, Y_n)$ is given by the product of the individual Poisson PMFs, due to independence:\n$$ f(\\mathbf{y}; \\lambda) = \\prod_{i=1}^n \\mathbb{P}(Y_i = y_i) = \\prod_{i=1}^n \\frac{e^{-\\lambda} \\lambda^{y_i}}{y_i!} $$\nThis can be rewritten as:\n$$ f(\\mathbf{y}; \\lambda) = \\frac{e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n y_i}}{\\prod_{i=1}^n y_i!} $$\nLet $t = \\sum_{i=1}^n y_i$. The PMF can be factored as:\n$$ f(\\mathbf{y}; \\lambda) = \\left( e^{-n\\lambda} \\lambda^t \\right) \\cdot \\left( \\frac{1}{\\prod_{i=1}^n y_i!} \\right) $$\nThis is of the form $g(t; \\lambda)h(\\mathbf{y})$, where $g(t; \\lambda) = e^{-n\\lambda} \\lambda^t$ depends on the data only through the statistic $T$, and $h(\\mathbf{y}) = (\\prod_{i=1}^n y_i!)^{-1}$ does not depend on $\\lambda$. By the Fisher-Neyman Factorization Theorem, this proves that $T = \\sum_{i=1}^n Y_i$ is a sufficient statistic for $\\lambda$.\n\n**Derivation of $\\mathbb{E}[X \\mid T=t]$:**\n\nThe Rao-Blackwellized estimator $\\tilde{X}$ is the conditional expectation of $X$ given the sufficient statistic $T$. For a given observation $T=t$, we have:\n$$ \\mathbb{E}[X \\mid T=t] = \\mathbb{E}[\\mathbf{1}\\{Y_1=0\\} \\mid T=t] = \\mathbb{P}(Y_1=0 \\mid T=t) $$\nUsing the definition of conditional probability:\n$$ \\mathbb{P}(Y_1=0 \\mid \\sum_{i=1}^n Y_i = t) = \\frac{\\mathbb{P}(Y_1=0, \\sum_{i=1}^n Y_i = t)}{\\mathbb{P}(\\sum_{i=1}^n Y_i = t)} $$\nThe event in the numerator is equivalent to $\\{Y_1=0 \\text{ and } \\sum_{i=2}^n Y_i = t\\}$.\nThe sum of independent Poisson random variables is itself a Poisson random variable. Specifically, $S_k = \\sum_{i=1}^k Y_i \\sim \\mathrm{Poisson}(k\\lambda)$.\nThus, $T = \\sum_{i=1}^n Y_i \\sim \\mathrm{Poisson}(n\\lambda)$, and its PMF is $\\mathbb{P}(T=t) = \\frac{e^{-n\\lambda}(n\\lambda)^t}{t!}$.\nThe term in the numerator can be evaluated using independence:\n$$ \\mathbb{P}(Y_1=0, \\sum_{i=2}^n Y_i = t) = \\mathbb{P}(Y_1=0) \\cdot \\mathbb{P}\\left(\\sum_{i=2}^n Y_i = t\\right) $$\nwhere $\\mathbb{P}(Y_1=0) = e^{-\\lambda}$ and $\\sum_{i=2}^n Y_i \\sim \\mathrm{Poisson}((n-1)\\lambda)$. This holds for $n  1$.\nThe numerator is:\n$$ (e^{-\\lambda}) \\cdot \\left( \\frac{e^{-(n-1)\\lambda}((n-1)\\lambda)^t}{t!} \\right) = \\frac{e^{-n\\lambda}((n-1)\\lambda)^t}{t!} $$\nSubstituting the numerator and denominator back into the conditional probability formula:\n$$ \\mathbb{P}(Y_1=0 \\mid T=t) = \\frac{\\frac{e^{-n\\lambda}((n-1)\\lambda)^t}{t!}}{\\frac{e^{-n\\lambda}(n\\lambda)^t}{t!}} = \\frac{((n-1)\\lambda)^t}{(n\\lambda)^t} = \\left(\\frac{n-1}{n}\\right)^t $$\nThis derivation is for $n  1$. For the case $n=1$, we have $T=Y_1$. The estimator is $\\mathbb{E}[\\mathbf{1}\\{Y_1=0\\} \\mid Y_1=t]$. This is $1$ if $t=0$ and $0$ if $t \\ne 0$, which can be written as $\\mathbf{1}\\{t=0\\}$. Applying the formula $\\left(\\frac{n-1}{n}\\right)^t$ for $n=1$ yields $0^t$, which by convention is $1$ for $t=0$ and $0$ for $t0$. Thus, the expression holds for $n=1$ as well.\n\nThe Rao-Blackwell estimator is therefore $\\tilde{X} = \\mathbb{E}[X \\mid \\sigma(T)] = \\left(\\frac{n-1}{n}\\right)^T$.\n\n### 2. Properties of the Estimator\n\n**Unbiasedness:**\nBy the Law of Total Expectation (or tower property), the expectation of the conditional expectation is the unconditional expectation:\n$$ \\mathbb{E}[\\tilde{X}] = \\mathbb{E}\\left[\\mathbb{E}[X \\mid \\sigma(T)]\\right] = \\mathbb{E}[X] $$\nSince the original estimator $X = \\mathbf{1}\\{Y_1=0\\}$ is unbiased for $\\theta(\\lambda) = e^{-\\lambda}$ (as $\\mathbb{E}[X] = \\mathbb{P}(Y_1=0) = e^{-\\lambda}$), the Rao-Blackwellized estimator $\\tilde{X}$ is also unbiased for $\\theta(\\lambda)$.\n\n**Variance Reduction:**\nThe Law of Total Variance states that:\n$$ \\operatorname{Var}(X) = \\mathbb{E}[\\operatorname{Var}(X \\mid T)] + \\operatorname{Var}(\\mathbb{E}[X \\mid T]) $$\nThe variance of our improved estimator is $\\operatorname{Var}(\\tilde{X}) = \\operatorname{Var}(\\mathbb{E}[X \\mid T])$. The term $\\mathbb{E}[\\operatorname{Var}(X \\mid T)]$ is the expectation of a conditional variance. Since variance is always non-negative, $\\operatorname{Var}(X \\mid T) \\ge 0$, and therefore its expectation is also non-negative, i.e., $\\mathbb{E}[\\operatorname{Var}(X \\mid T)] \\ge 0$.\nThis implies:\n$$ \\operatorname{Var}(X) \\ge \\operatorname{Var}(\\mathbb{E}[X \\mid T]) \\quad \\text{or} \\quad \\operatorname{Var}(X) \\ge \\operatorname{Var}(\\tilde{X}) $$\nEquality holds if and only if $\\operatorname{Var}(X \\mid T) = 0$ almost surely, which means $X$ is a function of $T$. In our case, for $n=1$, $T=Y_1$, so $X=\\mathbf{1}\\{Y_1=0\\}$ is a function of $T$. Thus for $n=1$, $\\operatorname{Var}(X) = \\operatorname{Var}(\\tilde{X})$ and the variance ratio will be $1$. For $n1$, $X$ is not a function of $T$, so we expect a strict inequality, $\\operatorname{Var}(X)  \\operatorname{Var}(\\tilde{X})$, and a variance ratio less than $1$.\n\n### 3. Monte Carlo Simulation Design\n\nThe simulation will estimate and compare the variances of $X$ and $\\tilde{X}$ for specified parameter pairs $(\\lambda, n)$.\nThe procedure for each pair $(\\lambda, n)$ is as follows:\n1.  Set the random number generator seed to $0$ for reproducibility.\n2.  Generate $M=100000$ independent replicates of the sample $(Y_1, \\dots, Y_n)$, where each $Y_i \\sim \\mathrm{Poisson}(\\lambda)$. This yields $M$ vectors of length $n$.\n3.  For each of the $M$ replicates:\n    a.  Calculate the original estimator $X = \\mathbf{1}\\{Y_1=0\\}$.\n    b.  Calculate the sufficient statistic $T = \\sum_{i=1}^n Y_i$.\n    c.  Calculate the Rao-Blackwellized estimator $\\tilde{X} = \\left(\\frac{n-1}{n}\\right)^T$.\n4.  This produces two sets of $M$ samples: $\\{X_j\\}_{j=1}^M$ and $\\{\\tilde{X}_j\\}_{j=1}^M$.\n5.  Compute the sample variances using an unbiased estimator (with Bessel's correction, $ddof=1$):\n    $$ \\widehat{\\operatorname{Var}}_{\\mathrm{MC}}(X) = \\frac{1}{M-1}\\sum_{j=1}^M (X_j - \\bar{X})^2 \\quad \\text{and} \\quad \\widehat{\\operatorname{Var}}_{\\mathrm{MC}}(\\tilde{X}) = \\frac{1}{M-1}\\sum_{j=1}^M (\\tilde{X}_j - \\bar{\\tilde{X}})^2 $$\n6.  Calculate the ratio $R(\\lambda,n) = \\frac{\\widehat{\\operatorname{Var}}_{\\mathrm{MC}}(\\tilde{X})}{\\widehat{\\operatorname{Var}}_{\\mathrm{MC}}(X)}$.\n7.  This process will be repeated for the following test cases:\n    - $(\\lambda,n) = (0.3, 1)$\n    - $(\\lambda,n) = (0.3, 2)$\n    - $(\\lambda,n) = (1.0, 5)$\n    - $(\\lambda,n) = (3.0, 10)$\n    - $(\\lambda,n) = (5.0, 25)$\nThe final output will be a list of these ratios, rounded to $6$ decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the Monte Carlo study to compare the variance of an estimator\n    with its Rao-Blackwellized version.\n    \"\"\"\n    # Define simulation parameters as specified in the problem statement.\n    M = 100000\n    SEED = 0\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.3, 1),\n        (0.3, 2),\n        (1.0, 5),\n        (3.0, 10),\n        (5.0, 25),\n    ]\n\n    results = []\n\n    # Initialize the random number generator for reproducibility.\n    rng = np.random.default_rng(SEED)\n\n    for lam, n in test_cases:\n        # Generate M independent replicates of n i.i.d. Poisson samples.\n        # The shape of Y_samples will be (M, n).\n        Y_samples = rng.poisson(lam=lam, size=(M, n))\n\n        # Calculate the original estimator X = 1{Y_1 = 0} for each of the M replicates.\n        # This results in a 1D array of length M.\n        x_values = (Y_samples[:, 0] == 0).astype(np.float64)\n\n        # Calculate the sufficient statistic T = sum(Y_i) for each of the M replicates.\n        # This results in a 1D array of length M.\n        t_values = np.sum(Y_samples, axis=1)\n\n        # Calculate the Rao-Blackwellized estimator E[X|T] = ((n-1)/n)^T.\n        # For n=1, the base is 0. np.power(0, 0) correctly evaluates to 1,\n        # which is the value of E[X|T] when T=0. For T0, np.power(0, T) is 0.\n        # This handles the n=1 case correctly where E[X|T] = 1{T=0}.\n        if n == 1:\n            base = 0.0\n        else:\n            base = (n - 1) / n\n        \n        x_tilde_values = np.power(base, t_values)\n\n        # Calculate the sample variances of the M estimates for both estimators.\n        # ddof=1 provides Bessel's correction for an unbiased estimate of the variance.\n        var_x = np.var(x_values, ddof=1)\n        var_x_tilde = np.var(x_tilde_values, ddof=1)\n\n        # Calculate the ratio of the variances.\n        # For n=1, X and E[X|T] are identical, so their variances are identical\n        # and the ratio is 1.0. For other cases, Var(X) will be non-zero for\n        # the given lambda values.\n        if var_x == 0:\n            # This case occurs if all x_values are identical.\n            # If Var(X) = 0, then X is constant, so Var(E[X|T]) also must be 0.\n            # The ratio should be 1, as E[X|T] = X in this case.\n            ratio = 1.0\n        else:\n            ratio = var_x_tilde / var_x\n\n        results.append(round(ratio, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "The Gibbs sampler is a cornerstone of modern MCMC, built on the elegant idea of iteratively sampling from a set of full conditional distributions. A subtle but crucial theoretical question is whether a given family of conditionals is \"coherent\"—that is, whether they derive from a valid joint distribution. This advanced computational practice challenges you to implement a sophisticated diagnostic tool based on the property that for a coherent system, the stationary distribution of the Gibbs sampler is invariant to the scan order of the variables .",
            "id": "3297655",
            "problem": "You are given a family of full conditional distributions for a bivariate random vector $X=(X_1,X_2)$ of the following linear-Gaussian form:\n$$\nX_1 \\mid X_2=x_2 \\sim \\mathcal{N}\\big(\\mu_1 + a\\,x_2,\\, s_1^2\\big),\\qquad\nX_2 \\mid X_1=x_1 \\sim \\mathcal{N}\\big(\\mu_2 + b\\,x_1,\\, s_2^2\\big),\n$$\nwhere $a,b\\in\\mathbb{R}$, $s_1^20$, $s_2^20$, and $\\mu_1,\\mu_2\\in\\mathbb{R}$ are fixed parameters. The central question is whether these full conditionals are coherent, that is, whether there exists any joint distribution on $\\mathbb{R}^2$ whose conditionals coincide with the family above. Because exact analytical compatibility conditions can be delicate, you will empirically test a necessary manifestation of coherence based on invariance of iterated conditionals under different Gibbs-scan orders using Monte Carlo simulation.\n\nFoundational base and definitions to be used:\n- For any integrable function $f:\\mathbb{R}^2\\to\\mathbb{R}$, the conditional expectation $E[f \\mid X_i]$ is the orthogonal projection of $f$ onto the $\\sigma$-field generated by $X_i$, $i\\in\\{1,2\\}$.\n- A one-scan two-step Gibbs update from state $(x_1,x_2)$ in order $1\\to 2$ consists of sampling $X_1'\\sim p(\\cdot \\mid X_2=x_2)$ and then $X_2'\\sim p(\\cdot \\mid X_1=X_1')$. The corresponding Markov operator applied to $f$ is the iterated conditional expectation $K_{12}f(x_1,x_2)=E[E[f \\mid X_1] \\mid X_2]$ evaluated at $(x_1,x_2)$. Similarly, for order $2\\to 1$, $K_{21}f(x_1,x_2)=E[E[f \\mid X_2] \\mid X_1]$.\n- If the full conditionals are coherent (i.e., come from some joint distribution $\\Pi$ on $\\mathbb{R}^2$), then both scan orders define Markov chains that share the same stationary distribution $\\Pi$, and for any integrable $f$ and either scan order, the stationary expectation satisfies $E_\\Pi[f]=E_\\Pi[K_{12}f]=E_\\Pi[K_{21}f]$. Thus, under coherence, stationary expectations of a set of test functions are invariant to scan order.\n\nDesign a Monte Carlo coherence diagnostic that, for a given parameter set $(a,b,s_1,s_2,\\mu_1,\\mu_2)$, performs the following:\n- Construct the two Gibbs-scan orders $1\\to 2$ and $2\\to 1$ using the given full conditionals.\n- Simulate each chain for a burn-in of $N_{\\mathrm{burn}}$ steps, then collect $N$ post burn-in iterates.\n- For the five test functions\n$$\nf_1(x_1,x_2)=x_1,\\quad\nf_2(x_1,x_2)=x_2,\\quad\nf_3(x_1,x_2)=x_1x_2,\\quad\nf_4(x_1,x_2)=x_1^2,\\quad\nf_5(x_1,x_2)=x_2^2,\n$$\ncompute Monte Carlo estimates of their stationary expectations under each scan order, together with Monte Carlo standard errors via non-overlapping batch means. Use a batch size $B$ and $M=\\lfloor N/B\\rfloor$ batches.\n- For each $f_k$, compute the standardized discrepancy\n$$\nZ_k=\\frac{\\left|\\widehat{E}_{12}[f_k]-\\widehat{E}_{21}[f_k]\\right|}{\\sqrt{\\widehat{\\operatorname{Var}}_{12}(\\bar{f}_k)+\\widehat{\\operatorname{Var}}_{21}(\\bar{f}_k)}},\n$$\nwhere $\\widehat{E}_{12}[f_k]$ and $\\widehat{E}_{21}[f_k]$ are the sample means from the $1\\to 2$ and $2\\to 1$ scans respectively, and $\\widehat{\\operatorname{Var}}(\\bar{f}_k)$ are their batch-means variance estimators of the sample mean. Declare the parameter set “empirically coherent” if $\\max_{k\\in\\{1,\\dots,5\\}} Z_k \\le \\tau$ for a fixed threshold $\\tau$.\n\nUse fixed simulation settings:\n- Use $N_{\\mathrm{burn}}=20000$, $N=120000$, $B=600$, and $\\tau=5$.\n- Initialize both chains at $(0,0)$.\n- Use a fixed pseudorandom generator seed so that results are exactly reproducible.\n- Angles are not used; there are no physical units to report.\n\nTest suite:\nProvide results for the following four parameter sets $(a,b,s_1,s_2,\\mu_1,\\mu_2)$:\n- Case $1$ (coherent, derived from a genuine bivariate normal with mean $(m_1,m_2)=(0.5,-0.3)$, variances $(v_1,v_2)=(1.5,0.7)$, covariance $c=0.6$): \n$$\na=\\frac{0.6}{0.7},\\quad b=\\frac{0.6}{1.5},\\quad s_1=\\sqrt{1.5-\\frac{0.6^2}{0.7}},\\quad s_2=\\sqrt{0.7-\\frac{0.6^2}{1.5}},\n$$\n$$\n\\mu_1=0.5-a(-0.3),\\quad \\mu_2=-0.3-b(0.5).\n$$\n- Case $2$ (intentionally incoherent via slope/variance mismatch):\n$$\na=0.8,\\quad b=0.2,\\quad s_1=1.0,\\quad s_2=\\sqrt{0.9},\\quad \\mu_1=0.0,\\quad \\mu_2=0.0.\n$$\n- Case $3$ (intentionally incoherent via mean mismatch while slopes/variances are compatible with some zero-mean joint):\n$$\na=0.25,\\quad b=0.5,\\quad s_1=\\sqrt{0.875},\\quad s_2=\\sqrt{1.75},\\quad \\mu_1=0.6,\\quad \\mu_2=1.5.\n$$\n- Case $4$ (coherent, near the boundary $ab$ close to $1$):\n$$\na=b=\\sqrt{0.95},\\quad s_1=s_2=\\sqrt{1-0.95},\\quad \\mu_1=0.0,\\quad \\mu_2=0.0.\n$$\n\nYour program must implement the diagnostic above for all four cases and output a single line containing a list of four Boolean values, each indicating whether the corresponding parameter set is empirically coherent according to the criterion $\\max_k Z_k \\le \\tau$. The format must be exactly a comma-separated list enclosed in square brackets, for example `[True,False,False,True]`.",
            "solution": "The supplied problem is valid. It is scientifically grounded in the theory of Markov chain Monte Carlo (MCMC) methods and conditional probability distributions. The problem is well-posed, providing all necessary parameters, constants, and a complete specification of the algorithm to be implemented. The terminology is precise and the objectives are clear.\n\nThe core of the problem is to devise an empirical diagnostic for the coherence of a set of full conditional distributions. A set of full conditionals $\\{p(x_i | x_{-i})\\}_{i=1}^d$ is coherent if there exists a joint probability distribution $p(x_1, \\dots, x_d)$ that has these conditionals. For the given linear-Gaussian case,\n$$\nX_1 \\mid X_2=x_2 \\sim \\mathcal{N}(\\mu_1 + a\\,x_2, s_1^2)\n$$\n$$\nX_2 \\mid X_1=x_1 \\sim \\mathcal{N}(\\mu_2 + b\\,x_1, s_2^2)\n$$\na joint bivariate normal distribution exists if and only if $ab \\ge 0$, $ab  1$, and $as_2^2 = bs_1^2$. If these conditions hold, then a Gibbs sampler constructed from these conditionals will have the corresponding joint normal distribution as its unique stationary distribution, $\\Pi$. A critical property of such a sampler is that its stationary distribution is invariant to the scan order of the variables. That is, updating variables in the order $X_1 \\to X_2$ or $X_2 \\to X_1$ results in two distinct Markov chains that nevertheless share the same stationary distribution $\\Pi$.\n\nThis invariance provides a powerful diagnostic tool. If the conditionals are coherent, the long-run average of any integrable test function $f(X_1, X_2)$ must be the same regardless of scan order. Let $E_{12}[f]$ and $E_{21}[f]$ denote the stationary expectations of $f$ under the $1 \\to 2$ and $2 \\to 1$ scan orders, respectively. Coherence implies $E_{12}[f] = E_{21}[f] = E_\\Pi[f]$. The proposed diagnostic tests this implication using Monte Carlo simulation.\n\nThe algorithm proceeds as follows:\n\n1.  **Gibbs Sampler Simulation**: Two parallel Gibbs sampling chains are run, one for each scan order.\n    -   **Chain 1 (Order $1 \\to 2$)**: From a state $(x_1^{(t)}, x_2^{(t)})$, the next state $(x_1^{(t+1)}, x_2^{(t+1)})$ is generated by sampling $x_{1, \\text{new}} \\sim \\mathcal{N}(\\mu_1 + a\\,x_2^{(t)}, s_1^2)$ and then $x_2^{(t+1)} \\sim \\mathcal{N}(\\mu_2 + b\\,x_{1, \\text{new}}, s_2^2)$, setting $x_1^{(t+1)} = x_{1, \\text{new}}$.\n    -   **Chain 2 (Order $2 \\to 1$)**: The update involves first sampling $x_{2, \\text{new}} \\sim \\mathcal{N}(\\mu_2 + b\\,x_1^{(t)}, s_2^2)$ and then $x_1^{(t+1)} \\sim \\mathcal{N}(\\mu_1 + a\\,x_{2, \\text{new}}, s_1^2)$, setting $x_2^{(t+1)} = x_{2, \\text{new}}$.\n\n    Both chains are initialized at $(x_1^{(0)}, x_2^{(0)}) = (0,0)$. They are run for $N_{\\mathrm{burn}} = 20000$ iterations to discard initial transient samples (burn-in). Subsequently, $N = 120000$ samples are collected from each chain.\n\n2.  **Monte Carlo Estimation of Expectations**: For each of the five test functions $f_k$ and for each chain (denoted by scan order $j \\in \\{12, 21\\}$), the stationary expectation $E_j[f_k]$ is estimated using the sample mean of the function evaluated over the $N$ post-burn-in samples $\\{(x_{1,j}^{(i)}, x_{2,j}^{(i)})\\}_{i=1}^N$:\n    $$\n    \\widehat{E}_j[f_k] = \\frac{1}{N} \\sum_{i=1}^{N} f_k(x_{1,j}^{(i)}, x_{2,j}^{(i)})\n    $$\n\n3.  **Standard Error Estimation via Batch Means**: The samples from an MCMC chain are inherently correlated, invalidating the simple variance formula for an i.i.d. sample mean. To obtain a valid estimate of the variance of $\\widehat{E}_j[f_k]$, the method of non-overlapping batch means (NOBM) is employed. The $N$ post-burn-in samples of $f_k$ are partitioned into $M = \\lfloor N/B \\rfloor = \\lfloor 120000/600 \\rfloor = 200$ non-overlapping batches, each of size $B=600$. For each batch $m \\in \\{1, \\dots, M\\}$, the batch mean $\\bar{f}_{k,j}^{(m)}$ is computed. The variance of the overall sample mean $\\widehat{E}_j[f_k]$ is then estimated as:\n    $$\n    \\widehat{\\operatorname{Var}}_j(\\bar{f}_k) = \\frac{1}{M} \\cdot \\frac{1}{M-1} \\sum_{m=1}^{M} \\left( \\bar{f}_{k,j}^{(m)} - \\widehat{E}_j[f_k] \\right)^2\n    $$\n    This formula computes the sample variance of the batch means and scales it by $1/M$. For large enough batch size $B$, the batch means are approximately uncorrelated and, by the Central Limit Theorem, normally distributed.\n\n4.  **Standardized Discrepancy Test**: For each test function $f_k$, the standardized discrepancy $Z_k$ between the estimates from the two chains is calculated. This is analogous to a two-sample Z-test:\n    $$\n    Z_k = \\frac{\\left|\\widehat{E}_{12}[f_k] - \\widehat{E}_{21}[f_k]\\right|}{\\sqrt{\\widehat{\\operatorname{Var}}_{12}(\\bar{f}_k) + \\widehat{\\operatorname{Var}}_{21}(\\bar{f}_k)}}\n    $$\n    Under the null hypothesis of coherence, $\\widehat{E}_{12}[f_k]$ and $\\widehat{E}_{21}[f_k]$ are estimates of the same quantity. Their difference should be small, and the $Z_k$ statistics should be realizations of approximately standard normal random variables. A large value of $Z_k$ provides evidence against coherence. The parameter set is declared \"empirically coherent\" if the maximum observed discrepancy across all test functions is statistically insignificant, i.e., $\\max_{k\\in\\{1,\\dots,5\\}} Z_k \\le \\tau$, for a given threshold $\\tau=5$.\n\nThis entire procedure is implemented in Python using the `numpy` library for numerical computations and random number generation, with a fixed seed to ensure reproducibility. The diagnostic is applied to each of the four specified parameter sets to determine its empirical coherence status.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_gibbs_sampler(params, settings, scan_order, rng):\n    \"\"\"\n    Runs a Gibbs sampler for a given scan order and returns the collected samples.\n    \"\"\"\n    a, b, s1, s2, mu1, mu2 = params\n    N_burn, N = settings['N_burn'], settings['N']\n    x1, x2 = 0.0, 0.0\n\n    # Burn-in phase\n    for _ in range(N_burn):\n        if scan_order == '12':\n            x1 = rng.normal(loc=mu1 + a * x2, scale=s1)\n            x2 = rng.normal(loc=mu2 + b * x1, scale=s2)\n        elif scan_order == '21':\n            x2 = rng.normal(loc=mu2 + b * x1, scale=s2)\n            x1 = rng.normal(loc=mu1 + a * x2, scale=s1)\n\n    # Sample collection phase\n    samples = np.zeros((N, 2))\n    for i in range(N):\n        if scan_order == '12':\n            x1 = rng.normal(loc=mu1 + a * x2, scale=s1)\n            x2 = rng.normal(loc=mu2 + b * x1, scale=s2)\n        elif scan_order == '21':\n            x2 = rng.normal(loc=mu2 + b * x1, scale=s2)\n            x1 = rng.normal(loc=mu1 + a * x2, scale=s1)\n        samples[i, :] = [x1, x2]\n\n    return samples\n\ndef calculate_stats(samples, settings):\n    \"\"\"\n    Computes Monte Carlo estimates and their batch-means variances.\n    \"\"\"\n    N, B = settings['N'], settings['B']\n    M = N // B\n\n    x1_s = samples[:, 0]\n    x2_s = samples[:, 1]\n\n    # Evaluate the five test functions over the samples\n    f_values = [\n        x1_s,\n        x2_s,\n        x1_s * x2_s,\n        x1_s**2,\n        x2_s**2\n    ]\n\n    means = []\n    vars_of_mean = []\n\n    for f_vec in f_values:\n        # Overall sample mean\n        overall_mean = np.mean(f_vec)\n        means.append(overall_mean)\n\n        # Reshape for batching\n        reshaped_f = f_vec.reshape((M, B))\n        batch_means = np.mean(reshaped_f, axis=1)\n\n        # Variance of the sample mean using batch means\n        # This is Var(batch_means) / M\n        var_of_batch_means = np.var(batch_means, ddof=1)\n        var_of_mean = var_of_batch_means / M\n        vars_of_mean.append(var_of_mean)\n\n    return means, vars_of_mean\n\ndef run_coherence_diagnostic(params, settings):\n    \"\"\"\n    Performs the full coherence diagnostic for a single parameter set.\n    \"\"\"\n    rng = np.random.default_rng(settings['seed'])\n\n    # Run sampler for scan order 1-2\n    samples_12 = run_gibbs_sampler(params, settings, '12', rng)\n    means_12, vars_12 = calculate_stats(samples_12, settings)\n\n    # Run sampler for scan order 2-1\n    samples_21 = run_gibbs_sampler(params, settings, '21', rng)\n    means_21, vars_21 = calculate_stats(samples_21, settings)\n\n    Z_scores = []\n    for k in range(5):  # For each of the five test functions\n        numerator = np.abs(means_12[k] - means_21[k])\n        denominator = np.sqrt(vars_12[k] + vars_21[k])\n        \n        if denominator == 0.0:\n            Z_k = 0.0 if numerator == 0.0 else np.inf\n        else:\n            Z_k = numerator / denominator\n        Z_scores.append(Z_k)\n    \n    max_Z = np.max(Z_scores)\n    return max_Z = settings['tau']\n\ndef solve():\n    # Define the fixed simulation settings\n    simulation_settings = {\n        'N_burn': 20000,\n        'N': 120000,\n        'B': 600,\n        'tau': 5.0,\n        'seed': 42 # Fixed seed for reproducibility\n    }\n\n    # Define the test cases from the problem statement.\n    # Case 1 (coherent)\n    c1_v1, c1_v2, c1_c = 1.5, 0.7, 0.6\n    c1_m1, c1_m2 = 0.5, -0.3\n    c1_a = c1_c / c1_v2\n    c1_b = c1_c / c1_v1\n    c1_s1 = np.sqrt(c1_v1 - c1_c**2 / c1_v2)\n    c1_s2 = np.sqrt(c1_v2 - c1_c**2 / c1_v1)\n    c1_mu1 = c1_m1 - c1_a * c1_m2\n    c1_mu2 = c1_m2 - c1_b * c1_m1\n    case1 = (c1_a, c1_b, c1_s1, c1_s2, c1_mu1, c1_mu2)\n\n    # Case 2 (incoherent)\n    case2 = (0.8, 0.2, 1.0, np.sqrt(0.9), 0.0, 0.0)\n\n    # Case 3 (incoherent label, but params compatible with non-zero mean)\n    case3 = (0.25, 0.5, np.sqrt(0.875), np.sqrt(1.75), 0.6, 1.5)\n\n    # Case 4 (coherent, near boundary)\n    a_b_4 = np.sqrt(0.95)\n    s_4 = np.sqrt(1.0 - 0.95)\n    case4 = (a_b_4, a_b_4, s_4, s_4, 0.0, 0.0)\n\n    test_cases = [case1, case2, case3, case4]\n\n    results = []\n    for case in test_cases:\n        is_coherent = run_coherence_diagnostic(case, simulation_settings)\n        results.append(is_coherent)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}