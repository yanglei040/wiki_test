{
    "hands_on_practices": [
        {
            "introduction": "A crucial aspect of evaluating any simulation algorithm is understanding its computational cost. This first exercise guides you through a formal analysis of the composition method's efficiency by calculating the expected number of random numbers it consumes . By applying the law of total expectation, you will derive a general formula that links the overall cost to the costs of the individual component samplers and their mixing weights.",
            "id": "3351329",
            "problem": "Consider a mixture distribution with $k$ components whose probability density function is defined by $f(x) = \\sum_{i=1}^{k} \\alpha_{i} f_{i}(x)$, where $\\alpha_{i} > 0$ for all $i$ and $\\sum_{i=1}^{k} \\alpha_{i} = 1$. A single variate from $f(x)$ is to be generated using the composition method. In this method, one first samples a discrete component index $J \\in \\{1, 2, \\dots, k\\}$ according to the categorical distribution with probabilities $(\\alpha_{1}, \\dots, \\alpha_{k})$, and then generates a draw $X$ from the conditional distribution with density $f_{J}(x)$. Assume that the component index $J$ is generated using the inverse transform method applied to the categorical distribution, which consumes one draw from the $\\text{Uniform}(0,1)$ distribution. For each component $i$, denote by $N_{i}$ the random number of $\\text{Uniform}(0,1)$ draws consumed by the sampler that generates $X \\sim f_{i}(x)$, and suppose that the expected value $\\mathbb{E}[N_{i}] = c_{i}$ is finite and known for all $i=1,\\dots,k$.\n\nStarting from the definition of a mixture distribution and the law of total expectation, derive an analytic expression for the expected total number of $\\text{Uniform}(0,1)$ random draws consumed by a single iteration of the composition method. Express your final answer in terms of $k$, $\\alpha_{i}$, and $c_{i}$ for $i=1,\\dots,k$. The final answer must be a single closed-form expression with no numerical evaluation required.",
            "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique solution based on established principles of probability theory and stochastic simulation.\n\nThe objective is to find the expected total number of $\\text{Uniform}(0,1)$ random draws required to generate a single variate from a mixture distribution using the composition method. Let $N_{total}$ be the random variable representing this total number of draws.\n\nThe composition method consists of two sequential steps:\n1.  A component index $J$ is sampled from the discrete categorical distribution over the set $\\{1, 2, \\dots, k\\}$, where the probability of selecting index $i$ is $P(J=i) = \\alpha_i$.\n2.  A random variate $X$ is generated from the probability density function $f_J(x)$ corresponding to the selected index $J$.\n\nLet $N_{index}$ be the number of $\\text{Uniform}(0,1)$ draws consumed in the first step (sampling the index $J$), and let $N_{sample}$ be the number of $\\text{Uniform}(0,1)$ draws consumed in the second step (sampling the variate $X$). The total number of draws is the sum of the draws from these two steps:\n$$N_{total} = N_{index} + N_{sample}$$\nBy the linearity of expectation, the expected total number of draws is:\n$$\\mathbb{E}[N_{total}] = \\mathbb{E}[N_{index} + N_{sample}] = \\mathbb{E}[N_{index}] + \\mathbb{E}[N_{sample}]$$\nWe will now evaluate each term on the right-hand side.\n\nFirst, consider $\\mathbb{E}[N_{index}]$. The problem states that the component index $J$ is generated using the inverse transform method, which consumes exactly one draw from the $\\text{Uniform}(0,1)$ distribution. Therefore, $N_{index}$ is not a random variable in this context but a constant value of $1$. Its expectation is:\n$$\\mathbb{E}[N_{index}] = 1$$\n\nNext, we evaluate $\\mathbb{E}[N_{sample}]$. The number of draws $N_{sample}$ required to generate the variate $X$ depends on which component index $J$ was selected in the first step. To find the expectation of $N_{sample}$, we must apply the law of total expectation, also known as the law of iterated expectations. This law states that for two random variables $A$ and $B$, $\\mathbb{E}[A] = \\mathbb{E}[\\mathbb{E}[A|B]]$. Applying this to our problem, we condition on the random variable $J$:\n$$\\mathbb{E}[N_{sample}] = \\mathbb{E}[\\mathbb{E}[N_{sample} | J]]$$\nThe inner term, $\\mathbb{E}[N_{sample} | J=i]$, represents the expected number of draws required to generate a variate, given that component $i$ has been selected. The problem defines $N_i$ as the random number of $\\text{Uniform}(0,1)$ draws consumed by the sampler for $X \\sim f_i(x)$ and provides its expectation as $\\mathbb{E}[N_i] = c_i$. Thus, the conditional expectation is:\n$$\\mathbb{E}[N_{sample} | J=i] = \\mathbb{E}[N_i] = c_i$$\nThe outer expectation is taken over the distribution of the discrete random variable $J$. The probability mass function for $J$ is given by $P(J=i) = \\alpha_i$ for $i \\in \\{1, 2, \\dots, k\\}$. The expectation of the function $g(J) = \\mathbb{E}[N_{sample} | J]$ is therefore:\n$$\\mathbb{E}[\\mathbb{E}[N_{sample} | J]] = \\sum_{i=1}^{k} \\mathbb{E}[N_{sample} | J=i] \\cdot P(J=i)$$\nSubstituting the expressions for the conditional expectation and the probability, we get:\n$$\\mathbb{E}[N_{sample}] = \\sum_{i=1}^{k} c_i \\alpha_i$$\nFinally, we combine the expectations of the two stages to find the total expected number of draws:\n$$\\mathbb{E}[N_{total}] = \\mathbb{E}[N_{index}] + \\mathbb{E}[N_{sample}] = 1 + \\sum_{i=1}^{k} \\alpha_i c_i$$\nThis is the final analytic expression for the expected total number of $\\text{Uniform}(0,1)$ random draws consumed by a single iteration of the composition method.",
            "answer": "$$\\boxed{1 + \\sum_{i=1}^{k} \\alpha_{i} c_{i}}$$"
        },
        {
            "introduction": "Translating a theoretically sound algorithm into robust code requires careful attention to the limitations of floating-point arithmetic. This practice explores a challenging but realistic scenario: sampling from a truncated mixture where parameter scales are extreme, leading to potential numerical underflow and loss of precision . You will evaluate several implementation strategies, learning to distinguish naive approaches from numerically stable techniques like log-domain computation and rescaling.",
            "id": "3351355",
            "problem": "You are tasked with implementing the composition method to draw samples from a truncated mixture distribution in a Monte Carlo simulation (Monte Carlo defined as repeated random sampling to obtain numerical results). Consider a two-component mixture of exponential distributions with density\n$$\nf(x) \\,=\\, \\pi_1 \\,\\lambda_1 \\,e^{-\\lambda_1 x} \\;+\\; \\pi_2 \\,\\lambda_2 \\,e^{-\\lambda_2 x}, \\quad x \\ge 0,\n$$\nwhere $0  \\pi_1  1$, $\\pi_2 = 1-\\pi_1$, and the rates satisfy $\\lambda_1 \\ll \\lambda_2$. You need to sample from the conditional (truncated) distribution of $X$ given $X>t$ for a large threshold $t>0$, using the composition method: first select a component index according to the conditional mixing probabilities given the truncation, and then sample from the corresponding truncated component distribution.\n\nIn finite-precision arithmetic, when $\\lambda_1 \\ll \\lambda_2$ and $t$ is large, direct implementations may suffer from severe underflow or loss of significance in both the component selection step and in the subsequent sampling of the truncated component. Which of the following implementation choices are numerically stable and unbiased in floating-point arithmetic when $\\lambda_1 \\ll \\lambda_2$ and $t$ is large? Select all that apply.\n\nA. Compute the post-truncation component weights as $w_k=\\pi_k \\, e^{-\\lambda_k t}$ in floating point, normalize by dividing by $w_1+w_2$, draw a uniform $U \\sim \\text{Unif}(0,1)$ to select the component with these normalized weights, and then sample from the truncated exponential by drawing $U' \\sim \\text{Unif}(0,1)$ and setting $X=t - \\frac{1}{\\lambda_K}\\log U'$. This direct implementation is numerically stable and unbiased.\n\nB. Work in the logarithmic domain for component selection: compute $\\ell_k=\\log \\pi_k - \\lambda_k t$, and sample the component index $K$ by the $\\mathrm{Gumbel}$-max method, i.e., draw $G_k \\stackrel{\\mathrm{iid}}{\\sim} \\text{Gumbel}(0,1)$ and take $K=\\operatorname{argmax}_{k\\in\\{1,2\\}} \\{\\ell_k+G_k\\}$. Then sample from the truncated exponential as in option A. This avoids underflow in the selection step and is unbiased.\n\nC. Rescale the post-truncation weights by factoring out the smallest rate: let $\\lambda_\\star=\\min\\{\\lambda_1,\\lambda_2\\}$ and compute unnormalized weights $w_k'=\\pi_k \\exp\\{-(\\lambda_k-\\lambda_\\star)t\\}$; normalize $w_k'$ to form selection probabilities. Then sample from the truncated exponential as in option A. This rescaling prevents underflow in the selection step and is unbiased.\n\nD. If $\\lambda_2 t$ is so large that $e^{-\\lambda_2 t}$ would underflow to $0$ in double precision, set the corresponding weight to $0$, renormalize the remaining weight(s), and proceed. The resulting bias is negligible and can be ignored.\n\nE. In the truncated sampling step, to preserve small increments when $t \\gg \\frac{1}{\\lambda_K}$, compute the increment as $Y = -\\frac{1}{\\lambda_K}\\log\\!\\big(1-U\\big)$ but evaluate the logarithm with the compensated function `log1p(-U)` to avoid subtractive cancellation, i.e., set $Y = -\\frac{1}{\\lambda_K}\\,\\text{log1p}(-U)$ with $U \\sim \\text{Unif}(0,1)$. If high relative accuracy near $t$ is needed, return the sample as the two-part representation $(t,Y)$ rather than the rounded floating-point sum $t+Y$. This maintains the law and improves numerical robustness for large $t$.\n\nSelect all that apply.",
            "solution": "The problem statement is critically evaluated for validity prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n- The probability density function of a two-component mixture of exponential distributions is $f(x) \\,=\\, \\pi_1 \\,\\lambda_1 \\,e^{-\\lambda_1 x} \\;+\\; \\pi_2 \\,\\lambda_2 \\,e^{-\\lambda_2 x}$ for $x \\ge 0$.\n- The mixing probabilities satisfy $0  \\pi_1  1$ and $\\pi_2 = 1-\\pi_1$.\n- The exponential rates satisfy $\\lambda_1 \\ll \\lambda_2$.\n- The task is to generate samples from the conditional distribution of the random variable $X$ given the event $X > t$, where $t > 0$ is a large threshold.\n- The sampling procedure is the composition method:\n    1. Select a component index $K$ based on the conditional mixing probabilities given $X>t$.\n    2. Sample from the truncated distribution of the selected component $K$.\n- The context is finite-precision floating-point arithmetic, where underflow and loss of significance are potential issues, particularly for large $t$ under the condition $\\lambda_1 \\ll \\lambda_2$.\n- The question is to identify which of the provided implementation choices are numerically stable and unbiased.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. It addresses a standard topic in computational statistics and Monte Carlo methods: generating random variates from a truncated mixture distribution. The numerical challenges described (underflow, loss of significance) are genuine concerns in floating-point arithmetic and are central to the field of numerical analysis. The setup is self-contained, with all necessary definitions provided. The problem is well-posed, asking for an evaluation of specific numerical strategies against the criteria of stability and unbiasedness. The terminology is precise and objective. The problem does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. A full solution and evaluation of options will be performed.\n\n### Principle-Based Derivation\nLet $X$ be a random variable with the given mixture density $f(x)$. The survival function is $S(x) = P(X>x) = \\pi_1 P(X_1>x) + \\pi_2 P(X_2>x)$, where $X_k \\sim \\text{Exp}(\\lambda_k)$. The survival function for an exponential distribution is $S_k(x) = P(X_k>x) = e^{-\\lambda_k x}$. Thus, the mixture survival function is $S(t) = \\pi_1 e^{-\\lambda_1 t} + \\pi_2 e^{-\\lambda_2 t}$.\n\nThe composition method for sampling from the truncated distribution $f(x|X>t)$ requires two steps:\n\n1.  **Component Selection**: Select a component index $K \\in \\{1, 2\\}$. The probability of selecting component $k$ is the conditional probability that the sample originated from component $k$, given that it is greater than $t$. This is given by Bayes' theorem:\n    $$\n    \\pi'_k = P(K=k | X>t) = \\frac{P(X>t|K=k)P(K=k)}{P(X>t)} = \\frac{S_k(t) \\pi_k}{S(t)} = \\frac{\\pi_k e^{-\\lambda_k t}}{\\pi_1 e^{-\\lambda_1 t} + \\pi_2 e^{-\\lambda_2 t}}\n    $$\n    Let $w_k = \\pi_k e^{-\\lambda_k t}$ be the unnormalized posterior weights. Then $\\pi'_k = w_k / (w_1 + w_2)$. Given $\\lambda_1 \\ll \\lambda_2$ and large $t$, the term $e^{-\\lambda_1 t}$ is much larger than $e^{-\\lambda_2 t}$. However, for sufficiently large $t$, both $e^{-\\lambda_1 t}$ and $e^{-\\lambda_2 t}$ can underflow to $0$ in floating-point arithmetic, causing the sum $w_1+w_2$ to become $0$ and leading to a division-by-zero error. This is a primary numerical stability challenge.\n\n2.  **Truncated Sampling**: After selecting a component $K$, we must sample from the distribution of $X_K$ conditioned on $X_K > t$. Due to the memoryless property of the exponential distribution, if $X_K \\sim \\text{Exp}(\\lambda_K)$, then the distribution of $X_K - t$ conditioned on $X_K > t$ is also $\\text{Exp}(\\lambda_K)$. Therefore, one can generate a sample $Y \\sim \\text{Exp}(\\lambda_K)$ and set the final sample to $X = t+Y$. The standard way to generate $Y$ is via inverse transform sampling: $Y = -\\frac{1}{\\lambda_K} \\log(U)$, where $U \\sim \\text{Unif}(0,1)$. A numerical stability challenge arises here when $t$ is large and $Y$ is small, as the sum $t+Y$ may be rounded to $t$, losing the information in the increment $Y$.\n\n### Option-by-Option Analysis\n\n**A. Compute the post-truncation component weights as $w_k=\\pi_k \\, e^{-\\lambda_k t}$ in floating point, normalize by dividing by $w_1+w_2$, draw a uniform $U \\sim \\text{Unif}(0,1)$ to select the component with these normalized weights, and then sample from the truncated exponential by drawing $U' \\sim \\text{Unif}(0,1)$ and setting $X=t - \\frac{1}{\\lambda_K}\\log U'$. This direct implementation is numerically stable and unbiased.**\n\nThis option describes the most direct, naive implementation. As established in the derivation, the computation of $w_k = \\pi_k e^{-\\lambda_k t}$ is susceptible to underflow. If $\\lambda_1 t$ is sufficiently large (e.g., $\\lambda_1 t > 709.8$ for IEEE 754 double precision), $e^{-\\lambda_1 t}$ will evaluate to $0$. Since $\\lambda_2 > \\lambda_1$, $e^{-\\lambda_2 t}$ will also be $0$. The denominator $w_1+w_2$ will then be $0$, causing a failure. Therefore, this implementation is fundamentally not numerically stable. The sampling formula $X=t - \\frac{1}{\\lambda_K}\\log U'$ is mathematically correct, as $1-U'$ for $U' \\sim \\text{Unif}(0,1)$ is equivalent to a fresh uniform variate, and $X = t - \\frac{1}{\\lambda_K} \\log(1-U')$ is the correct inverse transform sampling formula for the truncated exponential.\nVerdict: **Incorrect**. The component selection step is not numerically stable.\n\n**B. Work in the logarithmic domain for component selection: compute $\\ell_k=\\log \\pi_k - \\lambda_k t$, and sample the component index $K$ by the $\\mathrm{Gumbel}$-max method, i.e., draw $G_k \\stackrel{\\mathrm{iid}}{\\sim} \\text{Gumbel}(0,1)$ and take $K=\\operatorname{argmax}_{k\\in\\{1,2\\}} \\{\\ell_k+G_k\\}$. Then sample from the truncated exponential as in option A. This avoids underflow in the selection step and is unbiased.**\n\nThis option proposes a sophisticated and stable method for the component selection step. The selection probabilities are $\\pi'_k \\propto \\exp(\\log \\pi_k - \\lambda_k t) = \\exp(\\ell_k)$. The Gumbel-max trick is a standard and exactly correct (unbiased) method for sampling from a categorical distribution whose probabilities are proportional to $\\exp(\\ell_k)$ for given log-weights $\\ell_k$. By working in the logarithmic domain, this method avoids computing the small exponential terms directly, thus preventing the underflow that plagues the naive method in option A. The values of $\\ell_k$ are large negative numbers, which are well-represented in floating-point systems. This method is therefore numerically stable and unbiased for the selection step.\nVerdict: **Correct**.\n\n**C. Rescale the post-truncation weights by factoring out the smallest rate: let $\\lambda_\\star=\\min\\{\\lambda_1,\\lambda_2\\}$ and compute unnormalized weights $w_k'=\\pi_k \\exp\\{-(\\lambda_k-\\lambda_\\star)t\\}$; normalize $w_k'$ to form selection probabilities. Then sample from the truncated exponential as in option A. This rescaling prevents underflow in the selection step and is unbiased.**\n\nThis option proposes a rescaling of weights. The new weights $w'_k$ are related to the original weights $w_k$ by $w'_k = w_k \\cdot e^{\\lambda_\\star t}$. Since we normalize by the sum, this common factor $e^{\\lambda_\\star t}$ cancels out, meaning the resulting probabilities are identical to $\\pi'_k$. The method is thus unbiased.\nNumerically, since $\\lambda_1 \\ll \\lambda_2$, we have $\\lambda_\\star=\\lambda_1$. The new weights are $w'_1 = \\pi_1 \\exp\\{-(\\lambda_1-\\lambda_1)t\\} = \\pi_1$ and $w'_2 = \\pi_2 \\exp\\{-(\\lambda_2-\\lambda_1)t\\}$. The term for the dominant component becomes a stable constant. The exponential term for component $2$ can still underflow to $0$ if $(\\lambda_2-\\lambda_1)t$ is large, but this is not a catastrophic failure; it correctly reflects that the probability of selecting component $2$ is negligible. This a version of the \"log-sum-exp\" trick implemented in the linear domain by factoring out the largest term. The method is numerically stable and unbiased.\nVerdict: **Correct**.\n\n**D. If $\\lambda_2 t$ is so large that $e^{-\\lambda_2 t}$ would underflow to $0$ in double precision, set the corresponding weight to $0$, renormalize the remaining weight(s), and proceed. The resulting bias is negligible and can be ignored.**\n\nThis option describes a practical but ultimately biased approximation. The statement \"The resulting bias is negligible\" is an admission that there is, in fact, a bias. A method is unbiased if its expectation is exactly the target quantity. By intentionally setting a non-zero (albeit tiny) weight to zero, the sampling distribution is altered, and the method becomes biased. A meticulous analysis must distinguish between an algorithm that is mathematically exact (like B and C) versus one that is an explicit approximation (like D). While the bias is indeed extremely small and insignificant for most practical purposes, the procedure is not strictly \"unbiased\" as required by the question. Furthermore, this prescription is incomplete; it does not specify what to do if the weights for *all* components underflow, a scenario that methods B and C handle gracefully.\nVerdict: **Incorrect**. The method is explicitly, if negligibly, biased.\n\n**E. In the truncated sampling step, to preserve small increments when $t \\gg \\frac{1}{\\lambda_K}$, compute the increment as $Y = -\\frac{1}{\\lambda_K}\\log\\!\\big(1-U\\big)$ but evaluate the logarithm with the compensated function `log1p(-U)` to avoid subtractive cancellation, i.e., set $Y = -\\frac{1}{\\lambda_K}\\,\\text{log1p}(-U)$ with $U \\sim \\text{Unif}(0,1)$. If high relative accuracy near $t$ is needed, return the sample as the two-part representation $(t,Y)$ rather than the rounded floating-point sum $t+Y$. This maintains the law and improves numerical robustness for large $t$.**\n\nThis option addresses numerical issues in the second step of the composition method: generating the sample $X = t+Y$.\nFirst, it proposes generating the exponential increment $Y$ via the formula $Y = -\\frac{1}{\\lambda_K} \\log(1-U)$. This is mathematically equivalent to the standard $Y = -\\frac{1}{\\lambda_K} \\log(U)$, as $U$ and $1-U$ are identically distributed. To generate a small increment $Y$, one needs $1-U$ to be close to $1$, meaning $U$ must be close to $0$. The option correctly suggests computing $\\log(1-U)$ via the function `log1p(-U)`. The `log1p(x)` function computes $\\log(1+x)$ accurately for small $|x|$, thus `log1p(-U)` accurately computes $\\log(1-U)$ for small $U$. This is a numerically robust technique.\nSecond, it correctly identifies that for large $t$, the floating-point addition $t+Y$ can suffer from \"swamping\", where the small increment $Y$ is lost due to rounding. Proposing to return the result as an unevaluated sum $(t,Y)$ is a standard and effective technique to preserve full precision. This option describes two valid and robust numerical improvements that \"maintain the law\" (i.e., are unbiased) and enhance stability.\nVerdict: **Correct**.",
            "answer": "$$\\boxed{BCE}$$"
        },
        {
            "introduction": "After implementing a stochastic sampler, how can you be sure it is correct? This final, comprehensive practice challenges you to build a unit-testing framework to empirically validate a composition sampler . You will use standard statistical tools, such as the chi-square and Kolmogorov-Smirnov tests, to verify that your sampler's output matches the theoretical target distribution, both at the component level and for the overall mixture.",
            "id": "3351387",
            "problem": "You are tasked with designing and coding a unit-testing strategy for validating a composition sampler for mixture distributions using statistical tests on simulated data. The composition method for variate generation constructs a random variable $X$ by first selecting a discrete component index $I$ according to a categorical distribution with weights $\\{w_i\\}_{i=1}^k$ (where $\\sum_{i=1}^k w_i = 1$ and $w_i \\ge 0$), and then drawing $X \\mid (I=i)$ from the component distribution with probability density function $f_i(x)$ and cumulative distribution function $F_i(x)$. The resulting mixture distribution has density $f(x) = \\sum_{i=1}^k w_i f_i(x)$ and cumulative distribution function $F(x) = \\sum_{i=1}^k w_i F_i(x)$.\n\nStarting from the fundamental definitions of a categorical distribution, multinomial counting for independent trials, and cumulative distribution function properties, you must implement unit tests that validate both parts of the composition method:\n- The component selection mechanism, which should match the specified weights in frequency over many samples.\n- The conditional draws given the selected component, which should match the specified component distributions.\n\nAdditionally, validate the overall mixture distribution against its theoretical cumulative distribution function.\n\nYour program must implement the following statistical tests to provide objective acceptance criteria:\n1. A chi-square goodness-of-fit test for the component selection counts. Let $N$ be the total number of simulated samples, let $C_i$ be the observed count of selections of component $i$, and let $E_i = N w_i$ be the expected count. For all indices with $E_i > 0$, compute the chi-square test statistic $\\chi^2 = \\sum_{i: E_i>0} \\frac{(C_i - E_i)^2}{E_i}$ and the corresponding $p$-value against the chi-square distribution with $k' - 1$ degrees of freedom, where $k'$ is the number of indices with $E_i>0$. Accept the selection mechanism if the $p$-value exceeds a significance level $\\alpha$.\n2. One-sample Kolmogorov–Smirnov tests for the conditional distributions. For each component $i$ with $w_i > 0$, let $\\{X_j : I_j=i\\}$ be the subsample, and test that this subsample follows $F_i(x)$ using the Kolmogorov–Smirnov statistic $D_i = \\sup_x \\left| \\hat{F}_i(x) - F_i(x) \\right|$, where $\\hat{F}_i$ is the empirical cumulative distribution function of the subsample. Accept each conditional distribution if its $p$-value exceeds $\\alpha$; accept the entire conditional suite for a test case if all components with $w_i > 0$ are accepted.\n3. A one-sample Kolmogorov–Smirnov test against the full mixture cumulative distribution $F(x) = \\sum_{i=1}^k w_i F_i(x)$. Accept if the $p$-value exceeds $\\alpha$.\n4. A zero-weight constraint check. For any component with $w_i = 0$, assert that $C_i = 0$ exactly. Accept if this holds for all such components.\n\nYou must implement a reusable composition sampler that supports the following component families:\n- Normal (Gaussian): $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ with parameters $\\mu$ and $\\sigma$.\n- Exponential: $X \\sim \\text{Exponential}(\\lambda)$ with rate parameter $\\lambda$, where $F(x) = 1 - e^{-\\lambda x}$ for $x \\ge 0$ and $F(x) = 0$ for $x  0$.\n\nUse a fixed random seed to ensure reproducibility, and use a single significance level $\\alpha = 10^{-6}$ for all tests.\n\nImplement the following test suite of parameter sets. For each test case, simulate $N$ samples and perform the four validations listed above:\n\n- Test Case A (Normal mixture, two components):\n  - $k = 2$.\n  - Weights: $w = [0.3, 0.7]$.\n  - Components: $\\mathcal{N}(\\mu_1=0, \\sigma_1=1)$ and $\\mathcal{N}(\\mu_2=3, \\sigma_2=0.5)$.\n  - Sample size: $N = 100000$.\n\n- Test Case B (Exponential mixture, three components):\n  - $k = 3$.\n  - Weights: $w = [0.2, 0.5, 0.3]$.\n  - Components: $\\text{Exponential}(\\lambda_1=1.0)$, $\\text{Exponential}(\\lambda_2=2.0)$, and $\\text{Exponential}(\\lambda_3=0.5)$.\n  - Sample size: $N = 100000$.\n\n- Test Case C (Normal mixture, near-zero weight component):\n  - $k = 2$.\n  - Weights: $w = [0.999, 0.001]$.\n  - Components: $\\mathcal{N}(\\mu_1=-2, \\sigma_1=0.8)$ and $\\mathcal{N}(\\mu_2=10, \\sigma_2=1.2)$.\n  - Sample size: $N = 200000$.\n\n- Test Case D (Normal mixture, zero-weight component):\n  - $k = 2$.\n  - Weights: $w = [1.0, 0.0]$.\n  - Components: $\\mathcal{N}(\\mu_1=0, \\sigma_1=1)$ and $\\mathcal{N}(\\mu_2=5, \\sigma_2=1)$.\n  - Sample size: $N = 100000$.\n\nYour program must produce a single line of output containing the aggregated boolean pass/fail results of the four validations for each of the four test cases, in order A, B, C, D. For each test case, output four booleans in the order: selection frequency test, conditional distribution tests, mixture distribution test, zero-weight constraint check. Aggregate these across all test cases into a single comma-separated list enclosed in square brackets. For example, the output format must be exactly:\n\"[$b_{A,1},$ $b_{A,2},$ $b_{A,3},$ $b_{A,4},$ $b_{B,1},$ $b_{B,2},$ $b_{B,3},$ $b_{B,4},$ $b_{C,1},$ $b_{C,2},$ $b_{C,3},$ $b_{C,4},$ $b_{D,1},$ $b_{D,2},$ $b_{D,3},$ $b_{D,4}]$\", where each $b_{\\cdot,\\cdot}$ is either the boolean value True or False.",
            "solution": "The exercise requires the design and implementation of a comprehensive unit-testing framework for a composition sampler, a fundamental algorithm in stochastic simulation. The scientific validity of any simulation study rests upon the correctness of its random number generation, and this problem formalizes the validation process using established statistical hypothesis tests. The solution is structured around two main parts: first, a robust implementation of the composition sampler itself, and second, a suite of four distinct validation procedures that test different aspects of the sampler's output.\n\nA random variable $X$ is said to follow a mixture distribution if its probability density function (PDF) $f(x)$ is a weighted sum of other density functions $f_i(x)$:\n$$f(x) = \\sum_{i=1}^{k} w_i f_i(x)$$\nwhere $k$ is the number of components, $\\{w_i\\}_{i=1}^k$ are the non-negative weights that sum to unity ($\\sum w_i = 1, w_i \\ge 0$), and each $f_i(x)$ is a component PDF. The cumulative distribution function (CDF) is similarly a weighted sum:\n$$F(x) = \\sum_{i=1}^{k} w_i F_i(x)$$\nThe composition method generates a variate from this distribution in two stages:\n1.  A component index $I$ is drawn from a categorical distribution with probabilities $P(I=i) = w_i$.\n2.  A random variate $X$ is drawn from the selected component distribution with CDF $F_I(x)$.\n\nOur implementation encapsulates this logic in a reusable sampler. To generate a dataset of size $N$, we first generate $N$ independent draws of the index $I$ using the specified weights $w_i$. This is efficiently accomplished using `numpy.random.choice`. Then, for each component index $i \\in \\{1, \\dots, k\\}$, we identify the subset of trials where $I=i$ and generate the required number of variates from the corresponding distribution, $f_i(x)$, using `numpy.random` functions for the Normal and Exponential families. The parameterization must be handled carefully: for an Exponential distribution with rate $\\lambda$, the `scale` parameter in both `numpy` and `scipy` is $1/\\lambda$.\n\nThe validation of this sampler is multifaceted, addressing four distinct properties of the generated data. A fixed random seed is employed to ensure the entire process is deterministic and reproducible. A stringent significance level of $\\alpha = 10^{-6}$ is used for all hypothesis tests to minimize the probability of Type I errors (i.e., falsely rejecting a correct sampler).\n\n1.  **Component Selection Frequencies (Chi-Square Test)**: The first test validates the component selection mechanism. Over a large number of trials $N$, the observed count $C_i$ for each component $i$ should be close to its expected count, $E_i = N w_i$. This is a goodness-of-fit problem, for which the Pearson's chi-square test is appropriate. The test statistic is calculated as:\n    $$\\chi^2 = \\sum_{i: E_i > 0} \\frac{(C_i - E_i)^2}{E_i}$$\n    This statistic is compared against a chi-square distribution with $k' - 1$ degrees of freedom, where $k'$ is the number of components with a non-zero expected count ($E_i > 0$). The null hypothesis, stating that the observed counts follow the expected distribution, is accepted if the resulting $p$-value is greater than $\\alpha$. The `scipy.stats.chi2.sf` function is used to compute this $p$-value.\n\n2.  **Conditional Distributions (Kolmogorov-Smirnov Tests)**: The second test verifies that the variates for each component are drawn from the correct conditional distribution. For each component $i$ with $w_i > 0$, we isolate the subsample of variates $\\{X_j \\mid I_j=i\\}$ and perform a one-sample Kolmogorov-Smirnov (KS) test. The KS test compares the empirical CDF of this subsample, $\\hat{F}_i(x)$, against the theoretical component CDF, $F_i(x)$. The test statistic $D_i = \\sup_x |\\hat{F}_i(x) - F_i(x)|$ gives rise to a $p$-value. The null hypothesis is that the subsample is drawn from $F_i(x)$. The entire suite of conditional distributions is deemed correctly implemented only if the $p$-value for *every* component with $w_i > 0$ exceeds $\\alpha$. This is implemented using `scipy.stats.kstest`, providing it with the appropriate theoretical CDF callable.\n\n3.  **Overall Mixture Distribution (Kolmogorov-Smirnov Test)**: The third test validates the final, aggregated output of the sampler. The entire sample of $N$ variates is tested against the theoretical mixture CDF, $F(x) = \\sum_{i=1}^k w_i F_i(x)$. A single one-sample KS test is performed. A callable function representing $F(x)$ is constructed, which computes the weighted sum of the component CDFs. The null hypothesis, that the generated sample follows the target mixture distribution, is accepted if the $p$-value from `scipy.stats.kstest` is greater than $\\alpha$.\n\n4.  **Zero-Weight Constraint**: The final validation is a deterministic check, not a statistical test. For any component $i$ with a specified weight of $w_i = 0$, it is a logical necessity that it is never selected. Therefore, the observed count $C_i$ must be exactly $0$. The test passes if this condition holds for all components with zero weight. If no components have zero weight, the test passes vacuously.\n\nThe program systematically applies these four tests to each of the specified test cases (A, B, C, D), which are designed to probe different aspects of the sampler's behavior, including balanced and skewed weights, different distribution families, and the crucial zero-weight scenario. The boolean result (pass/fail) of each of the four tests for each of the four cases is aggregated into a final list.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2, norm, expon, kstest\n\ndef solve():\n    \"\"\"\n    Implements and validates a composition sampler for mixture distributions.\n    \"\"\"\n    \n    # Global parameters for the test suite\n    SEED = 42\n    ALPHA = 1e-6\n    rng = np.random.default_rng(SEED)\n\n    # Test case definitions\n    test_cases = [\n        {\n            \"name\": \"A\",\n            \"N\": 100000,\n            \"weights\": np.array([0.3, 0.7]),\n            \"components\": [\n                {\"dist\": \"norm\", \"params\": {\"loc\": 0.0, \"scale\": 1.0}},\n                {\"dist\": \"norm\", \"params\": {\"loc\": 3.0, \"scale\": 0.5}},\n            ]\n        },\n        {\n            \"name\": \"B\",\n            \"N\": 100000,\n            \"weights\": np.array([0.2, 0.5, 0.3]),\n            \"components\": [\n                {\"dist\": \"expon\", \"params\": {\"rate\": 1.0}},\n                {\"dist\": \"expon\", \"params\": {\"rate\": 2.0}},\n                {\"dist\": \"expon\", \"params\": {\"rate\": 0.5}},\n            ]\n        },\n        {\n            \"name\": \"C\",\n            \"N\": 200000,\n            \"weights\": np.array([0.999, 0.001]),\n            \"components\": [\n                {\"dist\": \"norm\", \"params\": {\"loc\": -2.0, \"scale\": 0.8}},\n                {\"dist\": \"norm\", \"params\": {\"loc\": 10.0, \"scale\": 1.2}},\n            ]\n        },\n        {\n            \"name\": \"D\",\n            \"N\": 100000,\n            \"weights\": np.array([1.0, 0.0]),\n            \"components\": [\n                {\"dist\": \"norm\", \"params\": {\"loc\": 0.0, \"scale\": 1.0}},\n                {\"dist\": \"norm\", \"params\": {\"loc\": 5.0, \"scale\": 1.0}},\n            ]\n        },\n    ]\n\n    def composition_sampler(n_samples, weights, components, specific_rng):\n        \"\"\"\n        Generates samples from a mixture distribution using the composition method.\n        \"\"\"\n        k = len(weights)\n        indices = specific_rng.choice(np.arange(k), size=n_samples, p=weights)\n        samples = np.zeros(n_samples, dtype=float)\n\n        for i in range(k):\n            mask = (indices == i)\n            count = np.sum(mask)\n            if count  0:\n                comp_def = components[i]\n                if comp_def[\"dist\"] == \"norm\":\n                    samples[mask] = specific_rng.normal(size=count, **comp_def[\"params\"])\n                elif comp_def[\"dist\"] == \"expon\":\n                    rate = comp_def[\"params\"][\"rate\"]\n                    samples[mask] = specific_rng.exponential(size=count, scale=1.0 / rate)\n        return samples, indices\n\n    all_results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        weights = case[\"weights\"]\n        components = case[\"components\"]\n        k = len(weights)\n\n        samples, indices = composition_sampler(N, weights, components, rng)\n        observed_counts = np.bincount(indices, minlength=k)\n        \n        # Test 1: Chi-square test for component selection frequency\n        expected_counts = N * weights\n        positive_mask = expected_counts  0\n        chisq_passed = True\n        if np.any(positive_mask):\n            df = int(np.sum(positive_mask)) - 1\n            if df  0:\n                chisq_stat = np.sum(\n                    (observed_counts[positive_mask] - expected_counts[positive_mask])**2\n                    / expected_counts[positive_mask]\n                )\n                p_val_chisq = chi2.sf(chisq_stat, df)\n                chisq_passed = p_val_chisq  ALPHA\n\n        # Test 2: KS tests for conditional distributions\n        cond_ks_passed = True\n        for i in range(k):\n            if weights[i]  0:\n                sub_samples = samples[indices == i]\n                if len(sub_samples)  0:\n                    comp_def = components[i]\n                    if comp_def[\"dist\"] == \"norm\":\n                        scipy_cdf = lambda x, mu=comp_def[\"params\"][\"loc\"], s=comp_def[\"params\"][\"scale\"]: norm.cdf(x, loc=mu, scale=s)\n                    elif comp_def[\"dist\"] == \"expon\":\n                        rate = comp_def[\"params\"][\"rate\"]\n                        scipy_cdf = lambda x, r=rate: expon.cdf(x, scale=1.0/r)\n                    \n                    _, p_val = kstest(sub_samples, scipy_cdf)\n                    if p_val = ALPHA:\n                        cond_ks_passed = False\n                        break\n        \n        # Test 3: KS test for the full mixture distribution\n        def mixture_cdf(x):\n            total_cdf = np.zeros_like(np.asarray(x), dtype=float)\n            for i in range(k):\n                if weights[i]  0:\n                    comp_def = components[i]\n                    if comp_def[\"dist\"] == \"norm\":\n                        total_cdf += weights[i] * norm.cdf(x, **comp_def[\"params\"])\n                    elif comp_def[\"dist\"] == \"expon\":\n                        rate = comp_def[\"params\"][\"rate\"]\n                        total_cdf += weights[i] * expon.cdf(x, scale=1.0/rate)\n            return total_cdf\n        \n        _, p_val_mix = kstest(samples, mixture_cdf)\n        mix_ks_passed = p_val_mix  ALPHA\n\n        # Test 4: Zero-weight constraint check\n        zero_weight_passed = True\n        zero_weight_indices = np.where(weights == 0)[0]\n        if len(zero_weight_indices)  0:\n            if np.any(observed_counts[zero_weight_indices] != 0):\n                zero_weight_passed = False\n\n        all_results.extend([chisq_passed, cond_ks_passed, mix_ks_passed, zero_weight_passed])\n    \n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}