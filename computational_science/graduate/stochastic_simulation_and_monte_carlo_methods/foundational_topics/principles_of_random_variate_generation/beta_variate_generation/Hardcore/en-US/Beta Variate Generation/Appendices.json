{
    "hands_on_practices": [
        {
            "introduction": "The first step in any simulation task is to build a reliable tool. This practice guides you through implementing the fundamental ratio-of-Gammas method for generating Beta variates, a cornerstone technique. This exercise  emphasizes numerical robustness by requiring the use of the log-sum-exp trick, a vital method in scientific computing for preventing overflow and underflow when dealing with extreme parameters.",
            "id": "3292073",
            "problem": "You are asked to design, analyze, and implement a robust random variate generator for the Beta distribution based on the ratio of independent Gamma variates. The target is a generator for a random variable $X$ with distribution $X \\sim \\mathrm{Beta}(\\alpha,\\beta)$, built from two independent Gamma variates $G_\\alpha \\sim \\Gamma(\\alpha,1)$ and $G_\\beta \\sim \\Gamma(\\beta,1)$ via the transformation $X = \\dfrac{G_\\alpha}{G_\\alpha + G_\\beta}$. The implementation must be numerically stable for extreme shape parameters, specifically when $\\alpha$ or $\\beta$ are very large or very small. Your program must implement the stabilized computation of $X$ using the log-sum-exp identity to avoid catastrophic cancellation:\n$$\n\\log\\left(e^{u} + e^{v}\\right) = \\mathrm{LSE}(u,v) \\equiv \\max(u,v) + \\log\\!\\left( 1 + e^{-|u-v|} \\right),\n$$\nso that, when $u = \\log G_\\alpha$ and $v = \\log G_\\beta$, the ratio can be computed as\n$$\nX = \\exp\\!\\left( \\log G_\\alpha - \\mathrm{LSE}(\\log G_\\alpha, \\log G_\\beta) \\right).\n$$\n\nStart from the fundamental definitions of the Gamma distribution with shape-scale parameterization and the Beta distribution. Assume that $G_\\alpha$ and $G_\\beta$ are independent, and recall that numerical stability must be considered a first-class design constraint. You must reason from first principles to justify the correctness of the ratio-of-Gammas construction and the stability of the log-sum-exp technique.\n\nYour program must:\n- Implement a function that, for given real shapes $\\alpha0$, $\\beta0$ and integer sample size $n \\ge 1$, returns $n$ independent samples from $\\mathrm{Beta}(\\alpha,\\beta)$ using the ratio $X=\\dfrac{G_\\alpha}{G_\\alpha+G_\\beta}$ with $G_\\alpha \\sim \\Gamma(\\alpha,1)$, $G_\\beta \\sim \\Gamma(\\beta,1)$, and compute the ratio via the log-sum-exp identity as described above.\n- Use a fixed pseudorandom number generator seed to ensure reproducibility.\n- For validation, compute Monte Carlo diagnostics against known analytic properties of the Beta distribution. Specifically, if $\\mu=\\dfrac{\\alpha}{\\alpha+\\beta}$ and $\\sigma^2=\\dfrac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$ denote the exact mean and variance, then the Monte Carlo standard error of the sample mean $\\bar{X}_n$ is $\\mathrm{SE}=\\sqrt{\\sigma^2/n}$. For selected test cases, declare a pass if $|\\bar{X}_n-\\mu| \\le c \\cdot \\mathrm{SE}$ for a specified constant $c$.\n\nTest suite and required outputs:\n- Use the following five test cases, each specified by $(\\alpha,\\beta,n,c)$:\n  - Case A (happy path): $(\\alpha,\\beta,n,c)=(2.5,5.0,100000,5)$.\n  - Case B (both shapes less than one): $(\\alpha,\\beta,n,c)=(0.3,0.7,200000,5)$.\n  - Case C (very large, balanced shapes): $(\\alpha,\\beta,n,c)=(10^6,10^6,20000,10)$.\n  - Case D (highly imbalanced shapes): $(\\alpha,\\beta,n,c)=(10^{-6},10^{6},200000,6)$.\n  - Case E (quantile accuracy in the interior): $(\\alpha,\\beta,n)=(15.2,9.7,150000)$. For this case, assess the empirical quantiles at probabilities $\\tau \\in \\{0.1,0.5,0.9\\}$ against the theoretical quantiles $q_\\tau$ of $\\mathrm{Beta}(\\alpha,\\beta)$. Using the asymptotic variance formula for the sample quantile $\\hat{q}_\\tau$, \n    $$\n    \\mathrm{Var}(\\hat{q}_\\tau) \\approx \\frac{\\tau(1-\\tau)}{n\\,f(q_\\tau)^2},\n    $$\n    where $f$ is the $\\mathrm{Beta}(\\alpha,\\beta)$ probability density function, declare a pass if \n    $$\n    \\max_{\\tau \\in \\{0.1,0.5,0.9\\}} \\left|\\hat{q}_\\tau - q_\\tau\\right| \\le 4 \\cdot \\sqrt{\\frac{\\tau(1-\\tau)}{n\\,f(q_\\tau)^2}}.\n    $$\n    For this case, report a single boolean indicating whether all three quantiles meet the bound simultaneously.\n- Your program must produce a single line of output containing the results for Cases A–E as a comma-separated list enclosed in square brackets, for example, $[\\text{True},\\text{False},\\text{True},\\text{True},\\text{True}]$.\n\nAll numerical answers must be dimensionless real numbers. Angles are not involved. Do not express any answer as a percentage; if any proportion appears, it must be a real number in $[0,1]$.\n\nScientific realism and coverage:\n- The Gamma and Beta distributions must be used exactly as defined in probability theory, with shapes $\\alpha,\\beta0$ and unit scale for the Gamma variates.\n- The numeric stabilization via the log-sum-exp identity must be explicitly used in computing $X$ to guard against underflow or overflow when $\\alpha$ or $\\beta$ are extreme.\n- The test suite covers typical, small-shape, large-shape, imbalanced-shape, and quantile-accuracy scenarios.",
            "solution": "The problem of generating random variates from a Beta distribution is a fundamental task in stochastic simulation. The posed problem requires the design and implementation of a generator for $X \\sim \\mathrm{Beta}(\\alpha,\\beta)$ based on the ratio of independent Gamma variates, with a strict requirement for numerical stability, particularly for extreme values of the shape parameters $\\alpha$ and $\\beta$.\n\n**1. Theoretical Foundation: The Ratio-of-Gammas Method**\n\nThe foundation of the method lies in a well-established theorem in probability theory that connects the Beta and Gamma distributions.\n\nFirst, let us define the relevant distributions. A random variable $Y$ follows a Gamma distribution with shape parameter $k0$ and unit scale parameter ($\\theta=1$), denoted $Y \\sim \\Gamma(k,1)$, if its probability density function (PDF) is given by:\n$$f_Y(y; k, 1) = \\frac{1}{\\Gamma(k)} y^{k-1} e^{-y} \\quad \\text{for } y  0$$\nwhere $\\Gamma(k) = \\int_0^\\infty t^{k-1} e^{-t} dt$ is the Gamma function.\n\nA random variable $X$ follows a Beta distribution with shape parameters $\\alpha0$ and $\\beta0$, denoted $X \\sim \\mathrm{Beta}(\\alpha, \\beta)$, if its PDF is:\n$$f_X(x; \\alpha, \\beta) = \\frac{1}{B(\\alpha, \\beta)} x^{\\alpha-1} (1-x)^{\\beta-1} \\quad \\text{for } x \\in (0, 1)$$\nwhere $B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$ is the Beta function.\n\nThe theorem states that if $G_\\alpha \\sim \\Gamma(\\alpha, 1)$ and $G_\\beta \\sim \\Gamma(\\beta, 1)$ are two independent random variables, then the variable $X$ defined by the ratio\n$$X = \\frac{G_\\alpha}{G_\\alpha + G_\\beta}$$\nis distributed as $\\mathrm{Beta}(\\alpha, \\beta)$.\n\nWe can prove this result via a change of variables. Let the joint PDF of the independent variables $(G_\\alpha, G_\\beta)$ be:\n$$f(g_\\alpha, g_\\beta) = f_{G_\\alpha}(g_\\alpha) f_{G_\\beta}(g_\\beta) = \\left(\\frac{1}{\\Gamma(\\alpha)} g_\\alpha^{\\alpha-1} e^{-g_\\alpha}\\right) \\left(\\frac{1}{\\Gamma(\\beta)} g_\\beta^{\\beta-1} e^{-g_\\beta}\\right)$$\nfor $g_\\alpha, g_\\beta  0$. We introduce the transformation:\n$$X = \\frac{G_\\alpha}{G_\\alpha + G_\\beta} \\quad \\text{and} \\quad Y = G_\\alpha + G_\\beta$$\nThe inverse transformation is $G_\\alpha = XY$ and $G_\\beta = Y(1-X)$. The domain $(g_\\alpha, g_\\beta) \\in (0, \\infty) \\times (0, \\infty)$ maps to $(x, y) \\in (0, 1) \\times (0, \\infty)$. The Jacobian of this inverse transformation is:\n$$J = \\det \\begin{pmatrix} \\frac{\\partial g_\\alpha}{\\partial x}  \\frac{\\partial g_\\alpha}{\\partial y} \\\\ \\frac{\\partial g_\\beta}{\\partial x}  \\frac{\\partial g_\\beta}{\\partial y} \\end{pmatrix} = \\det \\begin{pmatrix} y  x \\\\ -y  1-x \\end{pmatrix} = y(1-x) - (-y)x = y$$\nThe joint PDF of $(X, Y)$ is $f_{X,Y}(x, y) = f(xy, y(1-x))|J|$:\n$$f_{X,Y}(x, y) = \\frac{1}{\\Gamma(\\alpha)\\Gamma(\\beta)} (xy)^{\\alpha-1} e^{-xy} (y(1-x))^{\\beta-1} e^{-y(1-x)} y$$\n$$f_{X,Y}(x, y) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{\\Gamma(\\alpha)\\Gamma(\\beta)} y^{\\alpha-1} y^{\\beta-1} y \\, e^{-xy - y(1-x)}$$\n$$f_{X,Y}(x, y) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{\\Gamma(\\alpha)\\Gamma(\\beta)} y^{\\alpha+\\beta-1} e^{-y}$$\nTo find the marginal PDF of $X$, we integrate over all possible values of $Y$:\n$$f_X(x) = \\int_0^\\infty f_{X,Y}(x, y) dy = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\int_0^\\infty y^{\\alpha+\\beta-1} e^{-y} dy$$\nThe integral is the definition of $\\Gamma(\\alpha+\\beta)$. Therefore:\n$$f_X(x) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha-1}(1-x)^{\\beta-1} = \\frac{1}{B(\\alpha, \\beta)} x^{\\alpha-1}(1-x)^{\\beta-1}$$\nThis is precisely the PDF of a $\\mathrm{Beta}(\\alpha, \\beta)$ distribution, which validates the method.\n\n**2. Numerical Stability and the Log-Sum-Exp Technique**\n\nDirectly computing the ratio $X = G_\\alpha / (G_\\alpha + G_\\beta)$ is numerically unstable when the shape parameters are extreme.\n- If $\\alpha$ and $\\beta$ are very large (e.g., $10^6$), the Gamma variates $G_\\alpha$ and $G_\\beta$ will likely be very large, since $\\mathbb{E}[\\Gamma(k,1)] = k$. Their sum $G_\\alpha + G_\\beta$ could overflow standard floating-point representations.\n- If $\\alpha$ is very small and $\\beta$ is very large (e.g., $\\alpha=10^{-6}, \\beta=10^6$), then $G_\\alpha$ will be extremely close to zero and may underflow to $0.0$. This would incorrectly yield $X=0$. Similarly, if $\\alpha$ is large and $\\beta$ is small, $G_\\beta$ may underflow, yielding $X=1$.\n\nTo circumvent these issues, we perform the computation in the logarithmic domain. Let $u = \\log G_\\alpha$ and $v = \\log G_\\beta$. Then $X$ can be written as:\n$$X = \\frac{e^u}{e^u + e^v}$$\nTaking the logarithm of $X$:\n$$\\log X = \\log(e^u) - \\log(e^u + e^v) = u - \\log(e^u + e^v)$$\nThe term $\\log(e^u + e^v)$, known as the log-sum-exp (LSE) function, can still cause overflow if $u$ or $v$ is large. We use the following stable identity for its computation:\n$$\\mathrm{LSE}(u, v) = \\log(e^u + e^v) = \\max(u, v) + \\log(1 + e^{-|u - v|})$$\nThis form is stable because the argument of the exponential, $-|u-v|$, is always non-positive, preventing overflow. The value of $e^{-|u-v|}$ is always in $[0, 1]$, so its sum with $1$ and the subsequent logarithm are well-behaved.\n\nThe stable algorithm for computing $X$ is therefore:\n$$X = \\exp\\left( \\log G_\\alpha - \\mathrm{LSE}(\\log G_\\alpha, \\log G_\\beta) \\right)$$\n\n**3. Algorithmic Implementation Steps**\n\nTo generate a vector of $n$ samples from $\\mathrm{Beta}(\\alpha, \\beta)$:\n1. Generate a vector $\\mathbf{g}_\\alpha$ of $n$ samples from $\\Gamma(\\alpha, 1)$.\n2. Generate a vector $\\mathbf{g}_\\beta$ of $n$ samples from $\\Gamma(\\beta, 1)$.\n3. Compute the element-wise natural logarithms: $\\mathbf{u} = \\log \\mathbf{g}_\\alpha$ and $\\mathbf{v} = \\log \\mathbf{g}_\\beta$.\n4. Compute the element-wise maximum: $\\mathbf{m} = \\max(\\mathbf{u}, \\mathbf{v})$.\n5. Compute the LSE term: $\\mathbf{lse} = \\mathbf{m} + \\log(1 + \\exp(-|\\mathbf{u} - \\mathbf{v}|))$.\n6. Compute the final samples: $\\mathbf{x} = \\exp(\\mathbf{u} - \\mathbf{lse})$. The vector $\\mathbf{x}$ contains the desired Beta variates.\n\n**4. Validation Methodology**\n\nThe generated samples are validated against known theoretical properties of the Beta distribution.\n\n**Mean Validation (Cases A-D):** The Central Limit Theorem states that for a large sample size $n$, the sample mean $\\bar{X}_n$ is approximately normally distributed with mean $\\mu$ and variance $\\sigma^2/n$, where $\\mu$ and $\\sigma^2$ are the true mean and variance of the distribution.\n- True Mean: $\\mu = \\frac{\\alpha}{\\alpha+\\beta}$\n- True Variance: $\\sigma^2 = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$\nThe Monte Carlo standard error of the sample mean is $\\mathrm{SE} = \\sqrt{\\sigma^2/n}$. We declare a pass if the sample mean is within $c$ standard errors of the true mean: $|\\bar{X}_n - \\mu| \\le c \\cdot \\mathrm{SE}$.\n\n**Quantile Validation (Case E):** For a stronger test of the distribution's shape, we compare empirical quantiles with theoretical quantiles. The asymptotic theory of order statistics provides the approximate variance of the sample quantile $\\hat{q}_\\tau$ for a probability level $\\tau$:\n$$\\mathrm{Var}(\\hat{q}_\\tau) \\approx \\frac{\\tau(1-\\tau)}{n f(q_\\tau)^2}$$\nwhere $q_\\tau$ is the true quantile and $f$ is the PDF of the distribution. We declare a pass if the maximum absolute deviation over a set of quantiles is bounded by $4$ times the asymptotic standard error:\n$$\\max_{\\tau \\in \\{0.1, 0.5, 0.9\\}} |\\hat{q}_\\tau - q_\\tau| \\le 4 \\cdot \\sqrt{\\frac{\\tau(1-\\tau)}{n f(q_\\tau)^2}}$$\nThis comprehensive validation framework ensures that the implemented generator is not only numerically stable but also statistically accurate.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the Beta variate generator.\n    \"\"\"\n    \n    # Use a fixed seed for the pseudorandom number generator for reproducibility.\n    SEED = 42\n    RNG = np.random.default_rng(SEED)\n\n    def generate_beta_stable(alpha, beta, n, rng_instance):\n        \"\"\"\n        Generates n samples from Beta(alpha, beta) using the stable\n        ratio-of-gammas method with the log-sum-exp trick.\n\n        Args:\n            alpha (float): Shape parameter  0.\n            beta (float): Shape parameter  0.\n            n (int): Number of samples to generate = 1.\n            rng_instance (numpy.random.Generator): A numpy random generator instance.\n\n        Returns:\n            numpy.ndarray: An array of n samples from the Beta distribution.\n        \"\"\"\n        # Generate n samples from two independent Gamma distributions.\n        # Scale parameter is 1 by default in numpy.random.gamma.\n        g_alpha = rng_instance.gamma(alpha, size=n)\n        g_beta = rng_instance.gamma(beta, size=n)\n\n        # Compute in the log domain to prevent overflow/underflow.\n        log_g_alpha = np.log(g_alpha)\n        log_g_beta = np.log(g_beta)\n\n        # Compute log(g_alpha + g_beta) robustly using log-sum-exp.\n        u = log_g_alpha\n        v = log_g_beta\n        m = np.maximum(u, v)\n        # LSE(u,v) = log(e^u + e^v)\n        log_sum_exp = m + np.log(1 + np.exp(-np.abs(u - v)))\n\n        # Compute log(X) = log(g_alpha / (g_alpha + g_beta))\n        # log(X) = log(g_alpha) - log(g_alpha + g_beta)\n        log_x = log_g_alpha - log_sum_exp\n        \n        # Exponentiate to get the final samples.\n        x = np.exp(log_x)\n        \n        return x\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (happy path)\n        {'type': 'mean', 'params': (2.5, 5.0, 100000, 5)},\n        # Case B (both shapes less than one)\n        {'type': 'mean', 'params': (0.3, 0.7, 200000, 5)},\n        # Case C (very large, balanced shapes)\n        {'type': 'mean', 'params': (10**6, 10**6, 20000, 10)},\n        # Case D (highly imbalanced shapes)\n        {'type': 'mean', 'params': (10**-6, 10**6, 200000, 6)},\n        # Case E (quantile accuracy)\n        {'type': 'quantile', 'params': (15.2, 9.7, 150000, 4)},\n    ]\n\n    results = []\n    for case in test_cases:\n        if case['type'] == 'mean':\n            alpha, beta, n, c = case['params']\n            \n            samples = generate_beta_stable(alpha, beta, n, RNG)\n            \n            sample_mean = np.mean(samples)\n            \n            # Theoretical properties\n            # Using np.longdouble for precision with extreme parameters\n            a_ld, b_ld = np.longdouble(alpha), np.longdouble(beta)\n            \n            mu = a_ld / (a_ld + b_ld)\n            \n            # Handle potential overflow in (alpha+beta+1) for large alpha/beta\n            if (a_ld + b_ld + 1) == np.inf:\n                 var = 0.0\n            else:\n                 var = (a_ld * b_ld) / ((a_ld + b_ld)**2 * (a_ld + b_ld + 1))\n            \n            se = np.sqrt(var / n)\n            \n            # Check pass condition\n            passed = np.abs(sample_mean - mu) = c * se\n            results.append(bool(passed))\n\n        elif case['type'] == 'quantile':\n            alpha, beta, n, c_q = case['params']\n            \n            samples = generate_beta_stable(alpha, beta, n, RNG)\n\n            taus = np.array([0.1, 0.5, 0.9])\n            \n            # Empirical quantiles\n            q_hats = np.quantile(samples, taus)\n            \n            # Theoretical properties\n            q_s = stats.beta.ppf(taus, a=alpha, b=beta)\n            f_q_s = stats.beta.pdf(q_s, a=alpha, b=beta)\n            \n            # Asymptotic standard error of the sample quantile\n            asymptotic_var = (taus * (1 - taus)) / (n * f_q_s**2)\n            bound = c_q * np.sqrt(asymptotic_var)\n\n            # Check if all quantiles pass the test simultaneously.\n            passed = np.all(np.abs(q_hats - q_s) = bound)\n            results.append(bool(passed))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "After building a generator, how can we be confident it is statistically correct? This practice introduces a rigorous framework for validation through moment matching. You will test whether large samples from your generator exhibit a mean and variance consistent with the theoretical Beta distribution, whose moments you will derive from first principles . This exercise provides essential experience in connecting probability theory with empirical verification using the Central Limit Theorem.",
            "id": "3292115",
            "problem": "Construct a complete, runnable program that stress-tests a Beta variate generator across a grid of parameter pairs by verifying that the empirical mean and variance conform to their theoretical values derived from first principles. The program must be self-contained and use only standard numerical libraries. Your implementation must adhere to the following requirements.\n\n1. Fundamental base and generator specification:\n   - Use the definition that if $X \\sim \\mathrm{Gamma}(\\alpha,1)$ and $Y \\sim \\mathrm{Gamma}(\\beta,1)$ are independent, then $T = \\dfrac{X}{X+Y}$ has a $\\mathrm{Beta}(\\alpha,\\beta)$ distribution. Here $\\mathrm{Gamma}(\\alpha,1)$ denotes the Gamma distribution with shape parameter $\\alpha$ and unit scale.\n   - Implement a Beta variate generator by sampling $X$ and $Y$ as independent Gamma variates with shape parameters $\\alpha$ and $\\beta$ and computing $T = \\dfrac{X}{X+Y}$ elementwise.\n   - Do not call any built-in Beta random variate generator.\n\n2. Theoretical targets from core definitions:\n   - Starting from the Beta probability density and the Beta function $B(\\alpha,\\beta)$, derive and use the raw moment identity\n     $$\\mathbb{E}[X^k] = \\prod_{j=0}^{k-1} \\frac{\\alpha + j}{\\alpha+\\beta + j}, \\quad k \\in \\{1,2,3,4\\},$$\n     to compute the theoretical mean $\\mu = \\mathbb{E}[X]$, the theoretical variance $\\sigma^2 = \\mathbb{E}[X^2] - \\mu^2$, and the fourth central moment\n     $$\\mu_4 = \\mathbb{E}[(X-\\mu)^4] = \\mathbb{E}[X^4] - 4 \\mu \\mathbb{E}[X^3] + 6 \\mu^2 \\mathbb{E}[X^2] - 3 \\mu^4.$$\n   - Use the general identity for the variance of the unbiased sample variance $s^2$ from an independent and identically distributed sample of size $n$,\n     $$\\mathrm{Var}(s^2) = \\frac{1}{n}\\left(\\mu_4 - \\frac{n-3}{n-1}\\sigma^4\\right),$$\n     where $s^2$ uses denominator $n-1$.\n\n3. Statistical verification criteria:\n   - For each parameter pair $(\\alpha,\\beta)$, generate $n$ independent $\\mathrm{Beta}(\\alpha,\\beta)$ samples, compute the sample mean $\\hat{\\mu}$, and the unbiased sample variance $s^2$ (using denominator $n-1$).\n   - Define the standardized discrepancies\n     $$Z_{\\mathrm{mean}} = \\frac{|\\hat{\\mu} - \\mu|}{\\sqrt{\\sigma^2/n}}, \\qquad Z_{\\mathrm{var}} = \\frac{|s^2 - \\sigma^2|}{\\sqrt{\\mathrm{Var}(s^2)}},$$\n     where $\\mu$, $\\sigma^2$, and $\\mathrm{Var}(s^2)$ are as defined above.\n   - Using the Central Limit Theorem (CLT) and large-sample approximations, declare a test case as passing if both $Z_{\\mathrm{mean}} \\leq \\tau$ and $Z_{\\mathrm{var}} \\leq \\tau$, with threshold $\\tau$ fixed across all cases. Use $\\tau = 4.5$.\n\n4. Test suite and coverage:\n   - Use a common sample size $n = 50000$ for every case.\n   - Use the following grid of $(\\alpha,\\beta)$ pairs to probe balanced, imbalanced, small-shape, and large-shape regimes:\n     - $(\\alpha,\\beta) \\in \\{(0.1,0.1),(0.1,5.0),(5.0,0.1),(0.5,0.5),(1.0,1.0),(2.0,5.0),(5.0,2.0),(50.0,50.0),(100.0,1.0),(1.0,100.0)\\}$.\n   - To ensure reproducibility, fix a deterministic seed for the random number generator.\n\n5. Program input and output:\n   - The program must be fully self-contained and must not require any user input.\n   - The final output must be a single line containing a comma-separated list of booleans enclosed in square brackets. Each boolean corresponds to one test case in the order given above and is true if and only if both standardized criteria pass for that case. For example, an output like\n     $$[\\mathrm{True},\\mathrm{False},\\dots]$$\n     indicates which $(\\alpha,\\beta)$ pairs passed or failed.\n\nNo physical units or angle units are involved. All numerical values must be represented in fixed decimal form. The final program must run as is in a standard environment and must use only the specified libraries. The answer must be the code only, producing exactly one line of output in the specified format.",
            "solution": "The problem requires the construction and statistical validation of a Beta random variate generator. The generator's implementation is based on the fundamental principle that the ratio of two independent Gamma-distributed random variables follows a Beta distribution. The validation process involves comparing the empirical mean and variance of a large sample generated for various parameter sets against their exact theoretical values.\n\nThe process for each parameter pair $(\\alpha, \\beta)$ involves several steps: derivation of theoretical moments, generation of a sample, computation of empirical statistics, and a statistical test based on standardized discrepancies.\n\nFirst, we establish the theoretical foundation for the validation. A random variable $T$ is said to follow a Beta distribution with shape parameters $\\alpha  0$ and $\\beta  0$, denoted $T \\sim \\mathrm{Beta}(\\alpha, \\beta)$, if its probability density function is given by\n$$f(t; \\alpha, \\beta) = \\frac{t^{\\alpha-1}(1-t)^{\\beta-1}}{B(\\alpha, \\beta)}, \\quad t \\in [0, 1]$$\nwhere $B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}$ is the Beta function.\n\nThe generator will be constructed based on the property that if $X \\sim \\mathrm{Gamma}(\\alpha, 1)$ and $Y \\sim \\mathrm{Gamma}(\\beta, 1)$ are independent random variables, their ratio $T = \\frac{X}{X+Y}$ is distributed as $\\mathrm{Beta}(\\alpha, \\beta)$.\n\nFor the validation, we need the first four raw moments of the $\\mathrm{Beta}(\\alpha, \\beta)$ distribution. The $k$-th raw moment, $\\mathbb{E}[T^k]$, is given by:\n$$\\mathbb{E}[T^k] = \\int_0^1 t^k f(t; \\alpha, \\beta) dt = \\frac{1}{B(\\alpha, \\beta)} \\int_0^1 t^{\\alpha+k-1} (1-t)^{\\beta-1} dt = \\frac{B(\\alpha+k, \\beta)}{B(\\alpha, \\beta)}$$\nUsing the identity $B(a, b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}$ and the property $\\Gamma(z+1)=z\\Gamma(z)$, we can express this as a product:\n$$\\mathbb{E}[T^k] = \\frac{\\Gamma(\\alpha+k)\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\alpha+\\beta+k)} = \\frac{\\alpha(\\alpha+1)\\dots(\\alpha+k-1)}{(\\alpha+\\beta)(\\alpha+\\beta+1)\\dots(\\alpha+\\beta+k-1)} = \\prod_{j=0}^{k-1} \\frac{\\alpha + j}{\\alpha+\\beta + j}$$\nThis product form is computationally convenient as it avoids direct computation of Gamma functions.\n\nUsing this identity for $k \\in \\{1, 2, 3, 4\\}$, we compute the necessary theoretical quantities:\n$1$. The theoretical mean $\\mu$:\n$$\\mu = \\mathbb{E}[T^1] = \\frac{\\alpha}{\\alpha+\\beta}$$\n$2$. The theoretical variance $\\sigma^2$:\n$$\\sigma^2 = \\mathbb{E}[T^2] - \\mu^2, \\quad \\text{where} \\quad \\mathbb{E}[T^2] = \\frac{\\alpha(\\alpha+1)}{(\\alpha+\\beta)(\\alpha+\\beta+1)}$$\n$3$. The fourth central moment $\\mu_4$:\n$$\\mu_4 = \\mathbb{E}[(T-\\mu)^4] = \\mathbb{E}[T^4] - 4\\mu\\mathbb{E}[T^3] + 6\\mu^2\\mathbb{E}[T^2] - 3\\mu^4$$\nwhere $\\mathbb{E}[T^3]$ and $\\mathbb{E}[T^4]$ are calculated from the product formula.\n$4$. The variance of the unbiased sample variance $s^2$. For an i.i.d. sample of size $n$, the variance of $s^2 = \\frac{1}{n-1}\\sum_{i=1}^n(T_i - \\hat{\\mu})^2$ is given by:\n$$\\mathrm{Var}(s^2) = \\frac{1}{n}\\left(\\mu_4 - \\frac{n-3}{n-1}\\sigma^4\\right)$$\n\nThe validation procedure for a given $(\\alpha, \\beta)$ pair is as follows:\n$1$. Generate a sample of size $n = 50000$. This is done by first drawing $n$ samples from $\\mathrm{Gamma}(\\alpha, 1)$ to form a vector $X$ and $n$ samples from $\\mathrm{Gamma}(\\beta, 1)$ to form a vector $Y$. The Beta variates are then computed element-wise as $T = X / (X+Y)$.\n$2$. From this sample, compute the sample mean $\\hat{\\mu}$ and the unbiased sample variance $s^2$ (with denominator $n-1$).\n$3$. The Central Limit Theorem suggests that for large $n$, the sample mean $\\hat{\\mu}$ is approximately normally distributed with mean $\\mu$ and variance $\\sigma^2/n$. Similarly, the sample variance $s^2$ is approximately normal with mean $\\sigma^2$ and variance $\\mathrm{Var}(s^2)$. We use these results to define two standardized discrepancies:\n$$Z_{\\mathrm{mean}} = \\frac{|\\hat{\\mu} - \\mu|}{\\sqrt{\\sigma^2/n}}, \\qquad Z_{\\mathrm{var}} = \\frac{|s^2 - \\sigma^2|}{\\sqrt{\\mathrm{Var}(s^2)}}$$\n$4$. A test case is declared as passing if both discrepancies are below a specified threshold $\\tau = 4.5$. This threshold corresponds to a very strict confidence level, as the probability of a standard normal variable exceeding $4.5$ in magnitude is extremely small ($P(|Z|  4.5) \\approx 6.8 \\times 10^{-6}$).\n\nThe implementation will consist of a main loop that iterates through the specified grid of $(\\alpha, \\beta)$ pairs. For each pair, it will compute the theoretical moments, generate the random sample, compute the empirical statistics, and evaluate the pass/fail criteria. A fixed seed for the random number generator ensures the reproducibility of the test results. The final output will be a list of boolean values, one for each test case, indicating whether it passed the validation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs and stress-tests a Beta variate generator.\n\n    The generator is based on the ratio of two Gamma variates. Its output\n    is validated by comparing the empirical mean and variance of large samples\n    against their theoretical values derived from first principles.\n    The test is performed across a grid of shape parameter pairs (alpha, beta).\n    \"\"\"\n\n    # 4. Test suite and coverage:\n    # Use a common sample size n = 50000 for every case.\n    N_SAMPLES = 50000\n    # Use the following grid of (α,β) pairs.\n    TEST_CASES = [\n        (0.1, 0.1), (0.1, 5.0), (5.0, 0.1), (0.5, 0.5), (1.0, 1.0),\n        (2.0, 5.0), (5.0, 2.0), (50.0, 50.0), (100.0, 1.0), (1.0, 100.0)\n    ]\n    # To ensure reproducibility, fix a deterministic seed.\n    RNG_SEED = 12345\n    # Use threshold τ = 4.5.\n    TAU = 4.5\n\n    rng = np.random.default_rng(RNG_SEED)\n    results = []\n\n    def _compute_theoretical_moments(alpha, beta, n):\n        \"\"\"\n        Computes theoretical moments for the Beta(alpha, beta) distribution.\n        \n        Args:\n            alpha (float): The alpha shape parameter.\n            beta (float): The beta shape parameter.\n            n (int): The sample size.\n\n        Returns:\n            A tuple containing:\n            - mu (float): Theoretical mean.\n            - sigma_sq (float): Theoretical variance.\n            - var_s_sq (float): Theoretical variance of the sample variance.\n        \"\"\"\n        # 2. Theoretical targets from core definitions:\n        # Use the raw moment identity.\n        e1 = (alpha) / (alpha + beta)\n        e2 = e1 * (alpha + 1) / (alpha + beta + 1)\n        e3 = e2 * (alpha + 2) / (alpha + beta + 2)\n        e4 = e3 * (alpha + 3) / (alpha + beta + 3)\n\n        mu = e1\n        sigma_sq = e2 - mu**2\n\n        # 2 ... and the fourth central moment\n        mu4 = e4 - 4 * mu * e3 + 6 * mu**2 * e2 - 3 * mu**4\n        \n        # 2 ... use the general identity for the variance of the unbiased sample variance s^2\n        if n  1:\n            var_s_sq = (1 / n) * (mu4 - ((n - 3) / (n - 1)) * sigma_sq**2)\n        else:\n            var_s_sq = np.nan # Undefined for n=1\n            \n        return mu, sigma_sq, var_s_sq\n\n    def _generate_beta_samples(alpha, beta, n, generator):\n        \"\"\"\n        Generates beta variates using the Gamma ratio method.\n\n        Args:\n            alpha (float): The alpha shape parameter.\n            beta (float): The beta shape parameter.\n            n (int): The number of samples to generate.\n            generator (np.random.Generator): The random number generator instance.\n\n        Returns:\n            np.ndarray: An array of n beta variates.\n        \"\"\"\n        # 1. Fundamental base and generator specification:\n        # Use the definition that if X ~ Gamma(α,1) and Y ~ Gamma(β,1) are\n        # independent, then T = X/(X+Y) has a Beta(α,β) distribution.\n        x_samples = generator.gamma(shape=alpha, scale=1.0, size=n)\n        y_samples = generator.gamma(shape=beta, scale=1.0, size=n)\n        \n        # Avoid division by zero if both samples are zero, though highly unlikely.\n        denominator = x_samples + y_samples\n        # Set to a very small positive number where denominator is zero\n        denominator[denominator == 0] = 1e-300\n        \n        return x_samples / denominator\n\n    for alpha, beta in TEST_CASES:\n        # Compute theoretical targets\n        mu, sigma_sq, var_s_sq = _compute_theoretical_moments(alpha, beta, N_SAMPLES)\n        \n        # Generate samples\n        samples = _generate_beta_samples(alpha, beta, N_SAMPLES, rng)\n        \n        # Compute empirical statistics\n        mu_hat = np.mean(samples)\n        s_sq = np.var(samples, ddof=1) # Unbiased sample variance with n-1\n\n        # 3. Statistical verification criteria:\n        # Define the standardized discrepancies\n        std_err_mean = np.sqrt(sigma_sq / N_SAMPLES)\n        std_err_var = np.sqrt(var_s_sq)\n        \n        z_mean = np.abs(mu_hat - mu) / std_err_mean if std_err_mean  0 else 0.0\n        z_var = np.abs(s_sq - sigma_sq) / std_err_var if std_err_var  0 else 0.0\n\n        # Declare a test case as passing if both criteria are met\n        passed = (z_mean = TAU) and (z_var = TAU)\n        results.append(passed)\n\n    # 5. Program input and output:\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "In practice, multiple algorithms can solve the same problem, but their performance can vary dramatically. This exercise shifts our focus from implementation to analysis by asking you to build a performance model that predicts an algorithm's computational cost. By modeling the expected number of trials in rejection sampling, you will learn to estimate the CPU cost of different Beta variate generators and make principled decisions about algorithmic efficiency without exhaustive benchmarking .",
            "id": "3292114",
            "problem": "You are asked to construct a principled performance model for generating a Beta random variate by combining algorithmic operation counts, acceptance rates, and hardware-specific costs for primitive operations. The objective is to predict the expected Central Processing Unit (CPU) cycles per Beta variate for multiple algorithmic families under different hardware and acceptance regimes. The model must be based on first principles of probability and expectation rather than heuristics.\n\nBegin from the following foundational base:\n- Consider an algorithm that attempts to produce a sample through a sequence of independent trials, where each trial succeeds with probability $p \\in (0,1]$. The number of trials required until the first success is a geometrically distributed random variable. The expected number of trials equals the reciprocal of the success probability.\n- The expected total cost of a procedure equals the expectation of the sum of its constituent costs, and, under independence and fixed costs per trial, equals the expected number of trials multiplied by the cost per trial.\n- When an algorithm composes independent subprocedures, the expected overall cost is the sum of the expected costs of the components by linearity of expectation.\n\nYou will build a computational model around three families of Beta variate generation algorithms:\n1. A rejection-based power-transform method using uniform and power primitives with logarithm and exponential evaluations (denoted Algorithm J).\n2. A normal-based acceptance method using one normal proposal, one logarithm, and one uniform per trial (denoted Algorithm C).\n3. A ratio-of-Gamma method using two independent Gamma variates produced by a normal-based acceptance method (denoted Algorithm G). The Beta variate is obtained from the ratio of the two Gamma variates and requires no additional primitives beyond those used to obtain the two Gamma variates.\n\nFor each algorithm, a trial uses a fixed count of primitive operations. There is no additional post-acceptance primitive cost other than the primitives enumerated per trial. The primitive operations and their hardware-specific costs (in cycles) are:\n- Uniform random draw: hardware cost $C_{\\mathrm{u}}$ cycles.\n- Normal random draw: hardware cost $C_{\\mathrm{n}}$ cycles.\n- Natural logarithm: hardware cost $C_{\\log}$ cycles.\n- Exponential: hardware cost $C_{\\exp}$ cycles.\n\nAlgorithmic per-trial primitive counts are:\n- Algorithm J: $2$ uniforms, $2$ logarithms, $2$ exponentials, $0$ normals per trial. The acceptance probability per trial is $p_{\\mathrm{J}}$.\n- Algorithm C: $1$ normal, $1$ logarithm, $1$ uniform, $0$ exponentials per trial. The acceptance probability per trial is $p_{\\mathrm{C}}$.\n- Algorithm G: Two independent Gamma variates are generated. Each Gamma variate requires per trial $1$ normal, $1$ logarithm, $1$ uniform, $0$ exponentials, with acceptance probability $p_{\\mathrm{G}}(\\cdot)$ depending on the shape parameter used in that Gamma variate. Let $p_{\\mathrm{G}}(a)$ and $p_{\\mathrm{G}}(b)$ denote the acceptance probabilities for the two Gamma variates. The Beta variate is formed by the ratio of these two Gamma samples, and this ratio incurs no additional primitive operations.\n\nYour task is to write a program that, given the hardware primitive costs and acceptance probabilities for each algorithm and subcomponent, computes the expected CPU cycles per Beta variate for each algorithm and returns, for each test case, the minimum expected cycles among the three algorithms. All outputs must be expressed in cycles.\n\nUse the following test suite. Each test case provides the hardware costs $\\{C_{\\mathrm{u}}, C_{\\mathrm{n}}, C_{\\log}, C_{\\exp}\\}$, the Beta shape parameters $(a,b)$, and acceptance probabilities $\\{p_{\\mathrm{J}}, p_{\\mathrm{C}}, p_{\\mathrm{G}}(a), p_{\\mathrm{G}}(b)\\}$:\n\n- Test Case $1$ (general happy path): $C_{\\mathrm{u}}=40$, $C_{\\mathrm{n}}=200$, $C_{\\log}=80$, $C_{\\exp}=90$, $(a,b)=(2.0,5.0)$, $p_{\\mathrm{J}}=0.35$, $p_{\\mathrm{C}}=0.80$, $p_{\\mathrm{G}}(a)=0.95$, $p_{\\mathrm{G}}(b)=0.90$.\n- Test Case $2$ (near-unity acceptance): $C_{\\mathrm{u}}=40$, $C_{\\mathrm{n}}=200$, $C_{\\log}=80$, $C_{\\exp}=90$, $(a,b)=(1.5,3.0)$, $p_{\\mathrm{J}}=0.95$, $p_{\\mathrm{C}}=0.99$, $p_{\\mathrm{G}}(a)=0.99$, $p_{\\mathrm{G}}(b)=0.99$.\n- Test Case $3$ (expensive transcendental hardware, very low acceptance in one method): $C_{\\mathrm{u}}=40$, $C_{\\mathrm{n}}=150$, $C_{\\log}=500$, $C_{\\exp}=700$, $(a,b)=(0.5,0.5)$, $p_{\\mathrm{J}}=0.02$, $p_{\\mathrm{C}}=0.50$, $p_{\\mathrm{G}}(a)=0.60$, $p_{\\mathrm{G}}(b)=0.60$.\n- Test Case $4$ (expensive normal hardware): $C_{\\mathrm{u}}=40$, $C_{\\mathrm{n}}=1000$, $C_{\\log}=80$, $C_{\\exp}=90$, $(a,b)=(3.0,3.0)$, $p_{\\mathrm{J}}=0.40$, $p_{\\mathrm{C}}=0.60$, $p_{\\mathrm{G}}(a)=0.85$, $p_{\\mathrm{G}}(b)=0.85$.\n\nYour program must:\n- For each test case, compute the expected CPU cycles per Beta variate for Algorithms J, C, and G using only the operation counts, acceptance probabilities, and primitive costs provided.\n- For Algorithm J, base the expected cycles on repeated independent trials with success probability $p_{\\mathrm{J}}$.\n- For Algorithm C, base the expected cycles on repeated independent trials with success probability $p_{\\mathrm{C}}$.\n- For Algorithm G, compute the expected cycles as the sum of the expected cycles for generating the two Gamma variates with acceptance probabilities $p_{\\mathrm{G}}(a)$ and $p_{\\mathrm{G}}(b)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the minimum expected CPU cycles across the three algorithms for each test case, in order, as a comma-separated list enclosed in square brackets, for example $[x_{1},x_{2},x_{3},x_{4}]$. All values must be floats measured in cycles. No units should be printed, only the numeric values.\n\nEnsure scientific realism and internal consistency. Do not perform any random simulation; compute using the principled model described. The program must be self-contained and require no user input or external files.",
            "solution": "The problem statement is evaluated to be scientifically grounded, well-posed, objective, and self-contained. It presents a standard performance modeling task based on fundamental principles of probability theory and algorithm analysis. All necessary data are provided, and the task is to compute expected values based on a clearly defined model. Therefore, I will proceed with the solution.\n\nThe core of this problem rests on two fundamental principles of probability theory: the expected number of trials for a geometrically distributed process and the linearity of expectation for the total cost.\n\nLet a random process consist of a sequence of independent Bernoulli trials, each succeeding with probability $p \\in (0,1]$. Let $N$ be the random variable representing the number of trials required to achieve the first success. $N$ follows a geometric distribution on $\\{1, 2, 3, \\ldots\\}$ with probability mass function $P(N=k) = (1-p)^{k-1}p$. The expected value of $N$ is given by:\n$$\nE[N] = \\frac{1}{p}\n$$\nThis represents the expected number of trials required for any of the rejection-based algorithms.\n\nIf each trial incurs a fixed cost, say $C_{\\text{trial}}$, then the total cost of the procedure is the random variable $C_{\\text{total}} = N \\times C_{\\text{trial}}$. By the properties of expectation, the expected total cost is:\n$$\nE[C_{\\text{total}}] = E[N \\times C_{\\text{trial}}] = C_{\\text{trial}} \\times E[N] = \\frac{C_{\\text{trial}}}{p}\n$$\nFurthermore, if an algorithm's total cost is the sum of costs from independent subprocedures, the linearity of expectation states that the expected total cost is the sum of the expected costs of the subprocedures.\n\nWe will now apply these principles to derive the expected CPU cycle cost for each of the three specified algorithms. Let the hardware costs for primitive operations be denoted by $C_{\\mathrm{u}}$ (uniform), $C_{\\mathrm{n}}$ (normal), $C_{\\log}$ (logarithm), and $C_{\\exp}$ (exponential).\n\n**1. Algorithm J (Rejection-based power-transform method)**\n\nThis algorithm performs independent trials until one is accepted.\nThe primitives used per trial are specified as $2$ uniforms, $2$ logarithms, and $2$ exponentials.\nThe cost per trial, $C_{\\text{trial,J}}$, is the sum of the costs of these primitives:\n$$\nC_{\\text{trial,J}} = 2 C_{\\mathrm{u}} + 2 C_{\\log} + 2 C_{\\exp}\n$$\nThe probability of a single trial being successful (acceptance) is given as $p_{\\mathrm{J}}$.\nApplying the formula for expected total cost, the expected number of CPU cycles per generated Beta variate, $E_{\\mathrm{J}}$, is:\n$$\nE_{\\mathrm{J}} = \\frac{C_{\\text{trial,J}}}{p_{\\mathrm{J}}} = \\frac{2 C_{\\mathrm{u}} + 2 C_{\\log} + 2 C_{\\exp}}{p_{\\mathrm{J}}}\n$$\n\n**2. Algorithm C (Normal-based acceptance method)**\n\nThis algorithm also performs independent trials until acceptance.\nThe primitives used per trial are $1$ normal, $1$ logarithm, and $1$ uniform.\nThe cost per trial, $C_{\\text{trial,C}}$, is:\n$$\nC_{\\text{trial,C}} = C_{\\mathrm{n}} + C_{\\log} + C_{\\mathrm{u}}\n$$\nThe acceptance probability per trial is given as $p_{\\mathrm{C}}$.\nThe expected number of CPU cycles per generated Beta variate, $E_{\\mathrm{C}}$, is:\n$$\nE_{\\mathrm{C}} = \\frac{C_{\\text{trial,C}}}{p_{\\mathrm{C}}} = \\frac{C_{\\mathrm{n}} + C_{\\log} + C_{\\mathrm{u}}}{p_{\\mathrm{C}}}\n$$\n\n**3. Algorithm G (Ratio-of-Gamma method)**\n\nThis algorithm generates a Beta variate by first generating two independent Gamma variates and then taking their ratio. The problem states that the total expected cost is the sum of the expected costs for generating the two Gamma variates, which is a direct application of the linearity of expectation.\n\nLet the two Gamma variates correspond to the Beta shape parameters $a$ and $b$. Let their respective generation procedures be $G_a$ and $G_b$.\nThe total expected cost is $E_{\\mathrm{G}} = E[G_a] + E[G_b]$.\n\nFor each Gamma variate, the generation method is a normal-based acceptance method. The primitives per trial for generating one Gamma variate are $1$ normal, $1$ logarithm, and $1$ uniform.\nThe cost per trial for generating a single Gamma variate, $C_{\\text{trial,G}}$, is therefore:\n$$\nC_{\\text{trial,G}} = C_{\\mathrm{n}} + C_{\\log} + C_{\\mathrm{u}}\n$$\nNote that this is identical to $C_{\\text{trial,C}}$.\n\nFor the first Gamma variate (associated with parameter $a$), the acceptance probability is given as $p_{\\mathrm{G}}(a)$. The expected cost, $E[G_a]$, is:\n$$\nE[G_a] = \\frac{C_{\\text{trial,G}}}{p_{\\mathrm{G}}(a)} = \\frac{C_{\\mathrm{n}} + C_{\\log} + C_{\\mathrm{u}}}{p_{\\mathrm{G}}(a)}\n$$\nFor the second Gamma variate (associated with parameter $b$), the acceptance probability is given as $p_{\\mathrm{G}}(b)$. The expected cost, $E[G_b]$, is:\n$$\nE[G_b] = \\frac{C_{\\text{trial,G}}}{p_{\\mathrm{G}}(b)} = \\frac{C_{\\mathrm{n}} + C_{\\log} + C_{\\mathrm{u}}}{p_{\\mathrm{G}}(b)}\n$$\nThe total expected cost for Algorithm G is the sum of these two costs:\n$$\nE_{\\mathrm{G}} = E[G_a] + E[G_b] = \\frac{C_{\\mathrm{n}} + C_{\\log} + C_{\\mathrm{u}}}{p_{\\mathrm{G}}(a)} + \\frac{C_{\\mathrm{n}} + C_{\\log} + C_{\\mathrm{u}}}{p_{\\mathrm{G}}(b)}\n$$\nThis can be factored as:\n$$\nE_{\\mathrm{G}} = (C_{\\mathrm{n}} + C_{\\log} + C_{\\mathrm{u}}) \\left( \\frac{1}{p_{\\mathrm{G}}(a)} + \\frac{1}{p_{\\mathrm{G}}(b)} \\right)\n$$\n\nThe final task is to compute $E_{\\mathrm{J}}$, $E_{\\mathrm{C}}$, and $E_{\\mathrm{G}}$ for each test case using the provided parameters and determine the minimum value, $\\min(E_{\\mathrm{J}}, E_{\\mathrm{C}}, E_{\\mathrm{G}})$. This will be implemented programmatically.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the minimum expected CPU cycles for generating a Beta variate\n    across three different algorithmic families based on a provided performance model.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a dictionary containing hardware costs and acceptance probabilities.\n    test_cases = [\n        {\n            \"costs\": {\"Cu\": 40, \"Cn\": 200, \"Clog\": 80, \"Cexp\": 90},\n            \"params\": {\"a\": 2.0, \"b\": 5.0},\n            \"probs\": {\"pJ\": 0.35, \"pC\": 0.80, \"pG_a\": 0.95, \"pG_b\": 0.90}\n        },\n        {\n            \"costs\": {\"Cu\": 40, \"Cn\": 200, \"Clog\": 80, \"Cexp\": 90},\n            \"params\": {\"a\": 1.5, \"b\": 3.0},\n            \"probs\": {\"pJ\": 0.95, \"pC\": 0.99, \"pG_a\": 0.99, \"pG_b\": 0.99}\n        },\n        {\n            \"costs\": {\"Cu\": 40, \"Cn\": 150, \"Clog\": 500, \"Cexp\": 700},\n            \"params\": {\"a\": 0.5, \"b\": 0.5},\n            \"probs\": {\"pJ\": 0.02, \"pC\": 0.50, \"pG_a\": 0.60, \"pG_b\": 0.60}\n        },\n        {\n            \"costs\": {\"Cu\": 40, \"Cn\": 1000, \"Clog\": 80, \"Cexp\": 90},\n            \"params\": {\"a\": 3.0, \"b\": 3.0},\n            \"probs\": {\"pJ\": 0.40, \"pC\": 0.60, \"pG_a\": 0.85, \"pG_b\": 0.85}\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        costs = case[\"costs\"]\n        probs = case[\"probs\"]\n        \n        c_u, c_n, c_log, c_exp = costs[\"Cu\"], costs[\"Cn\"], costs[\"Clog\"], costs[\"Cexp\"]\n        p_j, p_c, p_g_a, p_g_b = probs[\"pJ\"], probs[\"pC\"], probs[\"pG_a\"], probs[\"pG_b\"]\n\n        # 1. Calculate Expected Cost for Algorithm J\n        # Cost per trial = 2*Cu + 2*Clog + 2*Cexp\n        # Expected cost = Cost per trial / pJ\n        cost_per_trial_j = 2 * c_u + 2 * c_log + 2 * c_exp\n        expected_cost_j = cost_per_trial_j / p_j\n\n        # 2. Calculate Expected Cost for Algorithm C\n        # Cost per trial = Cn + Clog + Cu\n        # Expected cost = Cost per trial / pC\n        cost_per_trial_c = c_n + c_log + c_u\n        expected_cost_c = cost_per_trial_c / p_c\n\n        # 3. Calculate Expected Cost for Algorithm G\n        # Cost is the sum of costs for two independent Gamma variates.\n        # Cost per trial for one Gamma = Cn + Clog + Cu\n        # Total Expected Cost = (Cost per trial / pG_a) + (Cost per trial / pG_b)\n        cost_per_trial_g = c_n + c_log + c_u\n        expected_cost_g = cost_per_trial_g * (1 / p_g_a + 1 / p_g_b)\n\n        # Find the minimum expected cost among the three algorithms\n        min_cost = np.min([expected_cost_j, expected_cost_c, expected_cost_g])\n        results.append(min_cost)\n\n    # Format the output as a comma-separated list of floating-point numbers\n    # enclosed in square brackets.\n    # The map to str is important to handle the floating point representation.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}