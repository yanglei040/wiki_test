{
    "hands_on_practices": [
        {
            "introduction": "To truly understand the alias method, it is essential to work through its construction process manually. This exercise guides you through the partitioning and pairing steps of the Walker algorithm for a specific, highly skewed distribution. By building the probability and alias arrays from first principles, you will gain a concrete intuition for how the method redistributes probability mass to achieve constant-time sampling. ",
            "id": "3350550",
            "problem": "Consider a discrete distribution on $n$ categories with probabilities $p_{1}, p_{2}, \\dots, p_{n}$, where $\\sum_{i=1}^{n} p_{i} = 1$. The alias method of Walker constructs two arrays for constant-time sampling: a probability array $q$ with entries in $[0,1]$ and an alias array $a$ consisting of integer indices in $\\{1,2,\\dots,n\\}$. The construction is based on partitioning categories via scaled probabilities $s_{i} = n p_{i}$ into “small” (those with $s_{i}  1$) and “large” (those with $s_{i} \\ge 1$), and then pairing small with large to conserve mass until all categories are resolved. The resulting arrays $(q,a)$ must, when used in the standard alias sampling scheme that selects a column uniformly from $\\{1,\\dots,n\\}$ and then chooses either the column’s primary index with probability $q$ or its alias otherwise, reproduce the original distribution $p$.\n\nFor the explicit case $n=5$ and $p=(0.01, 0.01, 0.01, 0.01, 0.96)$, carry out the Walker partition and pairing steps from first principles, beginning with the scaled probabilities $s_{i} = n p_{i}$. Determine the final probability array $q$ and alias array $a$ that exactly encode this distribution under the alias method and briefly justify that the constructed $(q,a)$ reproduces the original probabilities. Report the final arrays by listing their entries in the natural index order $1$ through $5$. The final answer must be a single mathematical object; present both arrays in one row in the order $(q_{1}, q_{2}, q_{3}, q_{4}, q_{5}, a_{1}, a_{2}, a_{3}, a_{4}, a_{5})$. No rounding is required.",
            "solution": "The user wants me to solve a problem that involves building lookup tables for the alias method. I have validated the problem and it is sound. Now I will solve it.\n\nThe problem requires the construction of the probability array $q$ and the alias array $a$ for a discrete distribution on $n=5$ outcomes with probabilities $p = (0.01, 0.01, 0.01, 0.01, 0.96)$. The construction must follow the principles of the Walker alias method.\n\nFirst, we define the scaled probabilities $s_i = n p_i$ for $i \\in \\{1, 2, 3, 4, 5\\}$. With $n=5$, we have:\n$s_1 = 5 \\times 0.01 = 0.05$\n$s_2 = 5 \\times 0.01 = 0.05$\n$s_3 = 5 \\times 0.01 = 0.05$\n$s_4 = 5 \\times 0.01 = 0.05$\n$s_5 = 5 \\times 0.96 = 4.80$\n\nThe sum of these scaled probabilities is $\\sum_{i=1}^5 s_i = 4 \\times 0.05 + 4.80 = 0.20 + 4.80 = 5 = n$, as expected.\n\nThe core of the algorithm involves partitioning the categories into two sets: “small” categories with $s_i  1$ and “large” categories with $s_i \\ge 1$. We maintain two worklists, $S$ for small and $L$ for large.\nInitially:\n$S = \\{1, 2, 3, 4\\}$ since $s_{1,2,3,4} = 0.05  1$.\n$L = \\{5\\}$ since $s_5 = 4.80 \\ge 1$.\n\nThe algorithm iteratively processes pairs of small and large categories. In each step, we pick one small category $i \\in S$ and one large category $j \\in L$. The probability slot for column $i$ is partially filled by its own scaled probability $s_i$. The remaining space, $1 - s_i$, is filled by \"donating\" probability mass from the large category $j$.\n\nLet's trace the iterative construction of the arrays $q$ and $a$. We will use the scaled probabilities $s_i$ as a work array that tracks remaining probability mass.\n\n**Initial State:**\nWorklists: $S = \\{1, 2, 3, 4\\}$, $L = \\{5\\}$\nScaled probabilities: $s = (0.05, 0.05, 0.05, 0.05, 4.80)$\nArrays to be built: $q, a$\n\n**Iteration 1:**\n1. Select a small category $i=1$ from $S$ and a large category $j=5$ from $L$.\n2. The probability for the primary outcome in column $1$ is set to its scaled probability: $q_1 = s_1 = 0.05$.\n3. The alias for column $1$ is the large category that donates mass: $a_1 = 5$.\n4. The mass required to fill column $1$ is $1 - s_1 = 1 - 0.05 = 0.95$. This mass is subtracted from the large category's mass: $s_5 \\leftarrow s_5 - (1-s_1) = 4.80 - 0.95 = 3.85$.\n5. Category $1$ is now fully processed. We remove it from $S$.\nState after Iteration 1:\nWorklists: $S = \\{2, 3, 4\\}$, $L = \\{5\\}$ (since $s_5 = 3.85 \\ge 1$)\nWorking probabilities: $s = (\\text{resolved}, 0.05, 0.05, 0.05, 3.85)$\nArrays: $q_1 = 0.05$, $a_1 = 5$.\n\n**Iteration 2:**\n1. Select small category $i=2$ from $S$ and large category $j=5$ from $L$.\n2. Set $q_2 = s_2 = 0.05$.\n3. Set $a_2 = 5$.\n4. Update $s_5 \\leftarrow s_5 - (1-s_2) = 3.85 - 0.95 = 2.90$.\n5. Remove $i=2$ from $S$.\nState after Iteration 2:\nWorklists: $S = \\{3, 4\\}$, $L = \\{5\\}$ (since $s_5 = 2.90 \\ge 1$)\nWorking probabilities: $s = (\\dots, \\text{resolved}, 0.05, 0.05, 2.90)$\nArrays: $q_1 = 0.05, q_2 = 0.05$, $a_1 = 5, a_2 = 5$.\n\n**Iteration 3:**\n1. Select small category $i=3$ from $S$ and large category $j=5$ from $L$.\n2. Set $q_3 = s_3 = 0.05$.\n3. Set $a_3 = 5$.\n4. Update $s_5 \\leftarrow s_5 - (1-s_3) = 2.90 - 0.95 = 1.95$.\n5. Remove $i=3$ from $S$.\nState after Iteration 3:\nWorklists: $S = \\{4\\}$, $L = \\{5\\}$ (since $s_5 = 1.95 \\ge 1$)\nWorking probabilities: $s = (\\dots, \\text{resolved}, 0.05, 1.95)$\nArrays: $q_{1,2,3} = 0.05$, $a_{1,2,3} = 5$.\n\n**Iteration 4:**\n1. Select small category $i=4$ from $S$ and large category $j=5$ from $L$.\n2. Set $q_4 = s_4 = 0.05$.\n3. Set $a_4 = 5$.\n4. Update $s_5 \\leftarrow s_5 - (1-s_4) = 1.95 - 0.95 = 1.00$.\n5. Remove $i=4$ from $S$.\nState after Iteration 4:\nWorklists: $S = \\emptyset$, $L = \\{5\\}$\nWorking probabilities: $s = (\\dots, \\text{resolved}, 1.00)$\nArrays: $q_{1,2,3,4} = 0.05$, $a_{1,2,3,4} = 5$.\n\n**Termination:**\nThe loop terminates as the small-category worklist $S$ is empty. At this point, any remaining categories (which must all be in $L$) are guaranteed to have a remaining scaled probability of exactly $1$. In our case, category $5$ remains with $s_5 = 1.00$. For such a category $k$, we set its probability slot to be filled entirely by itself, so $q_k = 1$. The alias $a_k$ will never be used, so it is conventionally set to $k$.\n\nFor $k=5$:\nSet $q_5 = 1.0$.\nSet $a_5 = 5$.\n\n**Final Arrays:**\nThe constructed arrays are:\nProbability array: $q = (0.05, 0.05, 0.05, 0.05, 1.0)$\nAlias array: $a = (5, 5, 5, 5, 5)$\n\n**Justification:**\nTo verify that these arrays reproduce the original probabilities $p_i$, we analyze the sampling process. When sampling, we first select a column $i \\in \\{1, \\dots, n\\}$ uniformly at random (with probability $1/n$), then sample outcome $i$ with probability $q_i$ or outcome $a_i$ with probability $1-q_i$.\n\nThe total probability $p'_k$ of sampling outcome $k$ is the sum of probabilities of all events leading to this outcome. An outcome $k$ can be generated from column $k$ (as the primary outcome) or from any column $i$ where $a_i=k$ (as the alias outcome).\nThe probability of getting $k$ from column $k$ is $\\frac{1}{n}q_k$.\nThe probability of getting $k$ from another column $i \\neq k$ (where $a_i=k$) is $\\frac{1}{n}(1-q_i)$.\nThe reconstructed probability for outcome $k$, $p'_k$, is:\n$$p'_k = \\frac{q_k}{n} + \\sum_{i: a_i=k, i \\neq k} \\frac{1-q_i}{n}$$\n\nLet's check this for our results ($n=5$):\n\nFor categories $k \\in \\{1, 2, 3, 4\\}$:\nIn our construction, none of these categories appear in the alias array $a$. The summation term is therefore zero.\n$p'_k = \\frac{q_k}{5} = \\frac{0.05}{5} = 0.01$. This matches the original $p_k$.\n\nFor category $k=5$:\nThe alias for columns $i=1, 2, 3, 4$ is $a_i=5$. Also, $a_5=5$. In this case, our formula is applied as:\n$p'_5 = \\frac{q_5}{5} + \\sum_{i \\in \\{1,2,3,4\\}} \\frac{1-q_i}{5}$\n$p'_5 = \\frac{1.0}{5} + \\frac{(1-0.05) + (1-0.05) + (1-0.05) + (1-0.05)}{5}$\n$p'_5 = \\frac{1.0}{5} + \\frac{4 \\times 0.95}{5} = \\frac{1.0 + 3.8}{5} = \\frac{4.8}{5} = 0.96$\nThis matches the original $p_5$.\n\nThe justification is complete. The constructed arrays $(q,a)$ correctly reproduce the original probability distribution. The final result is the combined list of elements from $q$ and $a$.",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 0.05  0.05  0.05  0.05  1.0  5  5  5  5  5 \\end{pmatrix} } $$"
        },
        {
            "introduction": "A robust algorithm must handle imperfect inputs gracefully, and a common source of error is the use of unnormalized probability weights. This practice explores the consequences of bypassing the crucial normalization step before constructing an alias table. You will analyze how this mistake can lead to an entirely different sampling distribution and quantify the resulting bias using the total variation distance. ",
            "id": "3350552",
            "problem": "Consider a categorical random variable with three outcomes indexed by $i \\in \\{1,2,3\\}$. Due to floating-point accumulation error in an upstream computation, the input \"probabilities\" arrive as nonnegative weights $w_{i}$ that do not sum to $1$. Specifically, suppose the weights are\n$$\nw_{1} = \\frac{1}{3} + \\epsilon,\\quad w_{2} = \\frac{1}{3},\\quad w_{3} = \\frac{1}{3},\n$$\nwith $\\epsilon  0$ and the intended distribution is the normalized categorical distribution $p_{i} = w_{i} / \\sum_{j=1}^{3} w_{j}$. You aim to sample from $p$ using the alias method, which represents a categorical distribution by partitioning a unit-area rectangle into $3$ columns of equal width; each column is split into a primary index and an alias index so that the total area assigned to each outcome equals its probability.\n\nAssume an implementer erroneously constructs the alias table by skipping the normalization step and uses the unnormalized weights $w_{i}$ directly. Under this mistake, and using the standard column-area interpretation of the alias method, demonstrate why the resulting sampler becomes uniform over $\\{1,2,3\\}$ in this specific scenario, and then quantify the resulting bias by computing the total variation distance between the intended normalized distribution $p$ and the produced uniform distribution. The total variation distance between two discrete distributions $P$ and $Q$ on the same finite set is defined as\n$$\n\\operatorname{TV}(P,Q) = \\frac{1}{2}\\sum_{i} \\left| P(i) - Q(i) \\right|.\n$$\nProvide the exact closed-form expression for $\\operatorname{TV}(p, U)$ as a function of $\\epsilon$, where $U$ denotes the uniform distribution on $\\{1,2,3\\}$. Your final answer must be a single closed-form expression; do not approximate and do not round.",
            "solution": "The problem presents a scenario where the alias method for sampling from a discrete distribution is implemented incorrectly. Specifically, unnormalized weights are used instead of probabilities that sum to $1$. We must first demonstrate why this error leads to sampling from a uniform distribution in the given case, and second, quantify the resulting bias using the total variation distance.\n\nLet the number of outcomes be $n=3$. The unnormalized, non-negative weights are given as\n$$\nw_{1} = \\frac{1}{3} + \\epsilon, \\quad w_{2} = \\frac{1}{3}, \\quad w_{3} = \\frac{1}{3}\n$$\nwhere $\\epsilon  0$.\n\nThe standard alias method algorithm begins by scaling the input probabilities $p_i$ by the number of outcomes $n$. In this erroneous implementation, the weights $w_i$ are used in place of probabilities. The scaled weights, which we denote by $w'_i$, are:\n$$\nw'_{i} = n \\cdot w_{i} = 3 \\cdot w_{i}\n$$\nSubstituting the given values, we find:\n$$\nw'_{1} = 3 \\left(\\frac{1}{3} + \\epsilon\\right) = 1 + 3\\epsilon\n$$\n$$\nw'_{2} = 3 \\left(\\frac{1}{3}\\right) = 1\n$$\n$$\nw'_{3} = 3 \\left(\\frac{1}{3}\\right) = 1\n$$\nThe next step in the algorithm (e.g., Vose's algorithm) is to partition the indices $\\{1, 2, 3\\}$ into two worklists: a `Small` list for indices $i$ where the scaled value is less than $1$, and a `Large` list for indices where the scaled value is greater than or equal to $1$. Based on our calculated $w'_i$:\n- Since $w'_{1} = 1 + 3\\epsilon  1$ (as $\\epsilon  0$), index $1$ goes into the `Large` list.\n- Since $w'_{2} = 1$, index $2$ goes into the `Large` list.\n- Since $w'_{3} = 1$, index $3$ goes into the `Large` list.\n\nThe `Small` worklist is empty, and the `Large` worklist contains all indices, i.e., $\\{1, 2, 3\\}$.\n\nThe main loop of the alias table construction algorithm proceeds by taking one item from the `Small` list and one from the `Large` list to balance them out. Since the `Small` list is empty, this main loop is never executed. The algorithm then finalizes the construction. A standard implementation handles any remaining items in the `Large` list by setting their corresponding probability table entries to $1$. Thus, the `Prob` table is populated as:\n$$\n\\text{Prob}[1] = 1, \\quad \\text{Prob}[2] = 1, \\quad \\text{Prob}[3] = 1\n$$\nThe `Alias` table is never written to, so its contents are irrelevant.\n\nNow, we analyze the sampling procedure with this erroneously constructed table. To generate a sample, the alias method performs two steps:\n1. Select a column index $i \\in \\{1, 2, 3\\}$ uniformly at random. The probability of choosing any specific $i$ is $\\frac{1}{n} = \\frac{1}{3}$.\n2. Generate a uniform random number $U \\sim \\text{Uniform}(0, 1)$. If $U  \\text{Prob}[i]$, the sampled outcome is $i$. Otherwise, the outcome is $\\text{Alias}[i]$.\n\nIn our case, $\\text{Prob}[i] = 1$ for all $i \\in \\{1, 2, 3\\}$. Since a random variable $U$ from a continuous distribution on $[0,1]$ satisfies $U  1$ with probability $1$, the condition $U  \\text{Prob}[i]$ is always met. Consequently, the sampler will always return the index $i$ that was chosen in the first step. Because the index $i$ is chosen uniformly at random from $\\{1, 2, 3\\}$, the resulting sampled outcomes follow a uniform distribution on $\\{1, 2, 3\\}$. We denote this distribution by $U$, where $U(i) = \\frac{1}{3}$ for $i \\in \\{1, 2, 3\\}$.\n\nNext, we quantify the bias by computing the total variation distance between the intended distribution $p$ and the produced uniform distribution $U$. The intended distribution $p$ is obtained by normalizing the weights $w_i$. The sum of the weights is:\n$$\n\\sum_{j=1}^{3} w_{j} = \\left(\\frac{1}{3} + \\epsilon\\right) + \\frac{1}{3} + \\frac{1}{3} = 1 + \\epsilon\n$$\nThe probabilities of the intended distribution $p$ are therefore:\n$$\np_{1} = \\frac{w_{1}}{\\sum w_{j}} = \\frac{\\frac{1}{3} + \\epsilon}{1 + \\epsilon}\n$$\n$$\np_{2} = \\frac{w_{2}}{\\sum w_{j}} = \\frac{\\frac{1}{3}}{1 + \\epsilon}\n$$\n$$\np_{3} = \\frac{w_{3}}{\\sum w_{j}} = \\frac{\\frac{1}{3}}{1 + \\epsilon}\n$$\nThe uniform distribution $U$ has probabilities $U(i) = \\frac{1}{3}$ for each outcome $i \\in \\{1, 2, 3\\}$. The total variation distance is defined as:\n$$\n\\operatorname{TV}(p, U) = \\frac{1}{2}\\sum_{i=1}^{3} |p_{i} - U(i)|\n$$\nWe compute the differences $p_i - U(i)$:\n$$\np_{1} - U(1) = \\frac{\\frac{1}{3} + \\epsilon}{1 + \\epsilon} - \\frac{1}{3} = \\frac{3(\\frac{1}{3} + \\epsilon) - (1 + \\epsilon)}{3(1 + \\epsilon)} = \\frac{1 + 3\\epsilon - 1 - \\epsilon}{3(1 + \\epsilon)} = \\frac{2\\epsilon}{3(1+\\epsilon)}\n$$\n$$\np_{2} - U(2) = \\frac{\\frac{1}{3}}{1 + \\epsilon} - \\frac{1}{3} = \\frac{1 - (1 + \\epsilon)}{3(1 + \\epsilon)} = \\frac{-\\epsilon}{3(1+\\epsilon)}\n$$\n$$\np_{3} - U(3) = \\frac{\\frac{1}{3}}{1 + \\epsilon} - \\frac{1}{3} = \\frac{-\\epsilon}{3(1+\\epsilon)}\n$$\nSince $\\epsilon  0$, the first difference is positive, and the other two are negative. Now we sum the absolute values of these differences:\n$$\n\\sum_{i=1}^{3} |p_{i} - U(i)| = \\left|\\frac{2\\epsilon}{3(1+\\epsilon)}\\right| + \\left|\\frac{-\\epsilon}{3(1+\\epsilon)}\\right| + \\left|\\frac{-\\epsilon}{3(1+\\epsilon)}\\right|\n$$\n$$\n= \\frac{2\\epsilon}{3(1+\\epsilon)} + \\frac{\\epsilon}{3(1+\\epsilon)} + \\frac{\\epsilon}{3(1+\\epsilon)} = \\frac{4\\epsilon}{3(1+\\epsilon)}\n$$\nFinally, we substitute this sum into the formula for total variation distance:\n$$\n\\operatorname{TV}(p, U) = \\frac{1}{2} \\left( \\frac{4\\epsilon}{3(1+\\epsilon)} \\right) = \\frac{2\\epsilon}{3(1+\\epsilon)}\n$$\nThis is the exact closed-form expression for the total variation distance between the intended distribution and the uniform distribution produced by the faulty sampler.",
            "answer": "$$\n\\boxed{\\frac{2\\epsilon}{3(1+\\epsilon)}}\n$$"
        },
        {
            "introduction": "Beyond gross input errors, the accuracy of the alias method can also be affected by subtle floating-point perturbations in the stored probability table. This exercise applies the concept of influence functions to analyze the sensitivity of a Monte Carlo estimator to such small numerical errors. By deriving the first-order bias, you will learn how to quantify the impact of implementation-level inaccuracies on downstream statistical results. ",
            "id": "3350521",
            "problem": "Consider a discrete target distribution supported on the finite set $\\{1,2,\\dots,n\\}$ with true probabilities $p_{1},\\dots,p_{n}$, and a real-valued integrand $f:\\{1,\\dots,n\\}\\to\\mathbb{R}$ such that the true Monte Carlo (MC) estimand $\\mu$ satisfies $\\mu=\\sum_{i=1}^{n}p_{i}f(i)$. Sampling is performed via the alias method: a preprocessing step produces an alias table consisting of an alias index map $A:\\{1,\\dots,n\\}\\to\\{1,\\dots,n\\}$ and per-column acceptance probabilities $q_{1},\\dots,q_{n}\\in[0,1]$. A single draw is generated by first choosing a column $J$ uniformly from $\\{1,\\dots,n\\}$ and then outputting $X=J$ with probability $q_{J}$, or $X=A_{J}$ with probability $1-q_{J}$. Assume the preprocessing perfectly represents $p$ in exact arithmetic.\n\nSuppose the runtime environment evaluates the acceptance probabilities in floating-point arithmetic so that, at sampling time, the acceptance probabilities are perturbed to $\\tilde{q}_{j}=q_{j}+\\delta_{j}$ with small perturbations $\\delta_{j}$ satisfying $|\\delta_{j}|\\ll 1$ for all $j\\in\\{1,\\dots,n\\}$. Assume the alias indices $A_{j}$ remain as constructed in exact arithmetic. Let $\\tilde{p}$ denote the resulting sampling distribution induced by the perturbed table, and consider the MC estimator\n$$\n\\hat{\\mu}=\\frac{1}{m}\\sum_{i=1}^{m}f(X_{i}),\n$$\nwhere $X_{1},\\dots,X_{m}$ are independent and identically distributed draws from $\\tilde{p}$ via the perturbed alias procedure described above. Using the influence function formalism for statistical functionals, derive the first-order bias of $\\hat{\\mu}$ relative to $\\mu$, to order $O\\!\\left(\\max_{j}|\\delta_{j}|^{2}\\right)$, expressed in closed form in terms of $n$, $f$, the alias table $(A_{j})_{j=1}^{n}$, and the perturbations $(\\delta_{j})_{j=1}^{n}$. Your final answer must be a single analytic expression. No rounding is required, and no units are involved. Ignore all higher-order terms beyond first order in the perturbations.",
            "solution": "The problem asks for the first-order bias of the Monte Carlo estimator $\\hat{\\mu}$ for the true estimand $\\mu = \\sum_{i=1}^{n}p_{i}f(i)$. The bias is defined as $B = \\mathbb{E}[\\hat{\\mu}] - \\mu$.\n\nThe estimator is given by $\\hat{\\mu}=\\frac{1}{m}\\sum_{i=1}^{m}f(X_{i})$, where the samples $X_{i}$ are independent and identically distributed draws from the perturbed probability distribution $\\tilde{p}$. The expectation of the estimator is:\n$$\n\\mathbb{E}[\\hat{\\mu}] = \\mathbb{E}\\left[\\frac{1}{m}\\sum_{i=1}^{m}f(X_{i})\\right] = \\frac{1}{m}\\sum_{i=1}^{m}\\mathbb{E}[f(X_{i})]\n$$\nSince each $X_{i}$ is drawn from the same distribution $\\tilde{p}$, $\\mathbb{E}[f(X_{i})]$ is constant for all $i$. Let's denote this expected value as $\\tilde{\\mu}$.\n$$\n\\tilde{\\mu} = \\mathbb{E}[f(X_1)] = \\sum_{k=1}^{n} \\tilde{p}_{k} f(k)\n$$\nTherefore, $\\mathbb{E}[\\hat{\\mu}] = \\tilde{\\mu}$. The bias is $B = \\tilde{\\mu} - \\mu$.\n\nThe problem states that the alias method sampling is perturbed. The acceptance probabilities $q_j$ are replaced by $\\tilde{q}_j = q_j + \\delta_j$. The true distribution $p$ and the true estimand $\\mu$ correspond to the exact parameters $(q_j)$, while the perturbed distribution $\\tilde{p}$ and the resulting expectation $\\tilde{\\mu}$ correspond to the perturbed parameters $(\\tilde{q}_j)$. We can thus view the expected value as a function of the vector of acceptance probabilities, $\\mathbf{q} = (q_1, \\dots, q_n)$. Let this function be $\\mu(\\mathbf{q})$.\n\nThe bias is $B = \\mu(\\mathbf{q}+\\mathbf{\\delta}) - \\mu(\\mathbf{q})$, where $\\mathbf{\\delta} = (\\delta_1, \\dots, \\delta_n)$. Since the perturbations $\\delta_j$ are small, we can approximate the bias to first order using a multivariate Taylor expansion of $\\mu(\\mathbf{q})$ around $\\mathbf{q}$:\n$$\nB \\approx \\sum_{j=1}^{n} \\frac{\\partial \\mu}{\\partial q_j} \\bigg|_{\\mathbf{q}} \\delta_j\n$$\nThis approach is consistent with the influence function formalism, where $\\frac{\\partial \\mu}{\\partial q_j}$ represents the influence of the parameter $q_j$ on the estimand $\\mu$.\n\nOur task is to compute these partial derivatives. First, we must express the probability $p_k$ of sampling outcome $k$ in terms of the alias table parameters. According to the specified sampling procedure, an outcome $X=k$ is generated if:\n1. A column $J=k$ is chosen (with probability $1/n$), and the primary outcome is selected (with probability $q_k$).\n2. A column $J=j$ is chosen (with probability $1/n$), the alias for that column is $A_j=k$, and the alias outcome is selected (with probability $1-q_j$).\nSumming over all possibilities, the probability $p_k$ is:\n$$\np_k = P(X=k) = \\sum_{j=1}^{n} P(X=k|J=j)P(J=j) = \\frac{1}{n}\\sum_{j=1}^{n}P(X=k|J=j)\n$$\nThe conditional probability $P(X=k|J=j)$ is $q_j$ if $k=j$ and $1-q_j$ if $k=A_j$. If $j=A_j$, the outcome is always $j$, so $P(X=j|J=j) = q_j+(1-q_j)=1$. Using the Kronecker delta, $\\delta_{ab}$, we can write this precisely as $P(X=k|J=j) = q_j \\delta_{kj} + (1-q_j) \\delta_{k,A_j}$.\nThus, the probability $p_k$ is:\n$$\np_k(\\mathbf{q}) = \\frac{1}{n} \\sum_{j=1}^{n} (q_j \\delta_{kj} + (1-q_j) \\delta_{k,A_j})\n$$\nThe summation over $j$ for the term $q_j \\delta_{kj}$ results in a single term $q_k$. So we can write:\n$$\np_k(\\mathbf{q}) = \\frac{1}{n} \\left( q_k + \\sum_{j=1}^{n} (1-q_j) \\delta_{k,A_j} \\right)\n$$\nNow we can compute the partial derivative of $p_k$ with respect to an arbitrary $q_j$:\n$$\n\\frac{\\partial p_k}{\\partial q_j} = \\frac{\\partial}{\\partial q_j} \\left[ \\frac{1}{n} \\left( q_k + \\sum_{l=1}^{n} (1-q_l) \\delta_{k,A_l} \\right) \\right]\n$$\nUsing a different index $l$ for the sum to avoid confusion:\n$$\n\\frac{\\partial p_k}{\\partial q_j} = \\frac{1}{n} \\left( \\frac{\\partial q_k}{\\partial q_j} + \\sum_{l=1}^{n} \\frac{\\partial (1-q_l)}{\\partial q_j} \\delta_{k,A_l} \\right)\n$$\nThe derivatives are $\\frac{\\partial q_k}{\\partial q_j} = \\delta_{kj}$ and $\\frac{\\partial (1-q_l)}{\\partial q_j} = -\\delta_{lj}$. Substituting these in:\n$$\n\\frac{\\partial p_k}{\\partial q_j} = \\frac{1}{n} \\left( \\delta_{kj} - \\sum_{l=1}^{n} \\delta_{lj} \\delta_{k,A_l} \\right)\n$$\nThe sum over $l$ has at most one non-zero term, which occurs when $l=j$.\n$$\n\\frac{\\partial p_k}{\\partial q_j} = \\frac{1}{n} (\\delta_{kj} - \\delta_{k,A_j})\n$$\nThis derivative quantifies how a change in $q_j$ affects the probability of outcome $k$. An increase in $q_j$ increases $p_j$ and decreases $p_{A_j}$ (unless $A_j=j$, in which case there is no change).\n\nNext, we calculate the influence of $q_j$ on the estimand $\\mu = \\sum_{k=1}^n p_k f(k)$:\n$$\n\\frac{\\partial \\mu}{\\partial q_j} = \\sum_{k=1}^{n} \\frac{\\partial p_k}{\\partial q_j} f(k) = \\sum_{k=1}^{n} \\frac{1}{n} (\\delta_{kj} - \\delta_{k,A_j}) f(k)\n$$\nDistributing the sum:\n$$\n\\frac{\\partial \\mu}{\\partial q_j} = \\frac{1}{n} \\left( \\sum_{k=1}^{n} \\delta_{kj}f(k) - \\sum_{k=1}^{n} \\delta_{k,A_j}f(k) \\right)\n$$\nThe first sum evaluates to $f(j)$, and the second sum evaluates to $f(A_j)$.\n$$\n\\frac{\\partial \\mu}{\\partial q_j} = \\frac{1}{n} (f(j) - f(A_j))\n$$\nFinally, we substitute this result back into the first-order approximation for the bias $B$:\n$$\nB \\approx \\sum_{j=1}^{n} \\frac{\\partial \\mu}{\\partial q_j} \\delta_j = \\sum_{j=1}^{n} \\frac{1}{n} (f(j) - f(A_j)) \\delta_j\n$$\nRearranging the terms gives the closed-form expression for the first-order bias:\n$$\nB = \\frac{1}{n} \\sum_{j=1}^{n} \\delta_j (f(j) - f(A_j))\n$$\nThis expression is linear in the perturbations $\\delta_j$ and is thus the exact first-order term, with all higher-order terms in $\\delta_j$ being zero in this derivation path (or negligible as per the problem statement). The expression is in terms of the given quantities $n$, $f$, $A$, and $\\delta$.",
            "answer": "$$\n\\boxed{\\frac{1}{n} \\sum_{j=1}^{n} \\delta_j (f(j) - f(A_j))}\n$$"
        }
    ]
}