{
    "hands_on_practices": [
        {
            "introduction": "Before we can optimize or validate a generator, we must understand its fundamental mechanics. This first practice requires you to manually trace the steps of the inversion method, the most direct way to generate a Poisson variate. By iteratively applying the recurrence relation for the probability mass function, you will gain a concrete understanding of how a uniform random number is transformed into a discrete Poisson-distributed sample .",
            "id": "3329682",
            "problem": "Consider generating a Poisson-distributed random variate with rate parameter $\\lambda$ by inversion using a single realization $U$ from the continuous uniform distribution on $(0,1)$. Begin from first principles: the probability mass function (PMF) of a Poisson distribution is $p(k)=\\mathbb{P}(X=k)$ and the cumulative distribution function (CDF) is $F(k)=\\mathbb{P}(X\\le k)=\\sum_{j=0}^{k}p(j)$. The inversion principle defines a random variate $X$ via $X=\\min\\{k\\in\\mathbb{Z}_{\\ge 0}:F(k)\\ge U\\}$ when $U\\sim\\text{Uniform}(0,1)$.\n\nTask: Derive a minimal-operation inversion sequence based on the above fundamental definitions that:\n- Starts from the base probability $p(0)$, which equals the probability of zero events, and\n- Uses only multiplicative updates between adjacent PMF values, obtained from the ratio of consecutive PMF terms, to update $p(k)$ and the running CDF $F(k)$,\n- Employs a stopping criterion formulated in terms of $U$ and the running CDF that halts at the first $k$ that satisfies the inversion definition.\n\nThen, apply your derived procedure to the specific case $\\lambda=7.3$ with the given uniform variate $U=0.513$, and execute the sequence until the stopping criterion is met. Report the returned sample $X$ as a single integer. No rounding is necessary, and no units are required.",
            "solution": "The derivation proceeds from the definitions of the probability mass function (PMF), the cumulative distribution function (CDF), and the inversion principle. For a Poisson random variable with rate $\\lambda$, the PMF is\n$$\np(k)=\\mathbb{P}(X=k)=\\exp(-\\lambda)\\frac{\\lambda^{k}}{k!},\\quad k\\in\\mathbb{Z}_{\\ge 0}.\n$$\nThe CDF is\n$$\nF(k)=\\sum_{j=0}^{k}p(j).\n$$\nBy the inversion principle, for $U\\sim\\text{Uniform}(0,1)$, the variate\n$$\nX=\\min\\{k\\in\\mathbb{Z}_{\\ge 0}:\\,F(k)\\ge U\\}\n$$\nhas the desired distribution, since\n$$\n\\mathbb{P}(X\\le k)=\\mathbb{P}\\big(U\\le F(k)\\big)=F(k),\n$$\nusing the fact that for $U\\sim\\text{Uniform}(0,1)$, $\\mathbb{P}(U\\le t)=t$ for $t\\in[0,1]$.\n\nTo obtain a minimal-operation inversion sequence, avoid recomputing factorials or powers at each step by exploiting the ratio of consecutive PMF values:\n$$\n\\frac{p(k)}{p(k-1)}=\\frac{\\exp(-\\lambda)\\lambda^{k}/k!}{\\exp(-\\lambda)\\lambda^{k-1}/(k-1)!}=\\frac{\\lambda}{k},\\quad k\\ge 1.\n$$\nThis yields the multiplicative update\n$$\np(k)=p(k-1)\\frac{\\lambda}{k},\\quad k\\ge 1,\n$$\nwith initialization at the base probability\n$$\np(0)=\\exp(-\\lambda).\n$$\nDefine a running partial sum for the CDF,\n$$\nF(k)=F(k-1)+p(k),\\quad F(-1)=0,\n$$\nso the procedural steps are:\n- Initialize $k\\leftarrow 0$, $p\\leftarrow \\exp(-\\lambda)$, $F\\leftarrow p$.\n- While $UF$, increment $k\\leftarrow k+1$, update $p\\leftarrow p\\cdot \\lambda/k$, and update $F\\leftarrow F+p$.\n- Stop at the first $k$ such that $U\\le F$, and return $X\\leftarrow k$.\n\nThis procedure is minimal in operations because each step uses only one multiplication and one division to obtain $p(k)$ from $p(k-1)$, plus one addition to update $F(k)$.\n\nNow apply the sequence to $\\lambda=7.3$ and $U=0.513$. Initialize\n$$\np(0)=\\exp(-7.3)\\approx 0.0006754629,\\quad F(0)=0.0006754629.\n$$\nSince $U=0.513F(0)$, continue. Use $p(k)=p(k-1)\\cdot \\lambda/k$ and $F(k)=F(k-1)+p(k)$:\n- $k=1$: $p(1)=p(0)\\cdot 7.3/1\\approx 0.004930876$, $F(1)\\approx 0.0006754629+0.004930876=0.0056063389$.\n- $k=2$: $p(2)=p(1)\\cdot 7.3/2\\approx 0.017997697$, $F(2)\\approx 0.0236040359$.\n- $k=3$: $p(3)=p(2)\\cdot 7.3/3\\approx 0.043794397$, $F(3)\\approx 0.067398433$.\n- $k=4$: $p(4)=p(3)\\cdot 7.3/4\\approx 0.079924775$, $F(4)\\approx 0.147323208$.\n- $k=5$: $p(5)=p(4)\\cdot 7.3/5\\approx 0.116690171$, $F(5)\\approx 0.264013379$.\n- $k=6$: $p(6)=p(5)\\cdot 7.3/6\\approx 0.141973041$, $F(6)\\approx 0.405986420$.\n- $k=7$: $p(7)=p(6)\\cdot 7.3/7\\approx 0.148057600$, $F(7)\\approx 0.554044020$.\n\nAt $k=6$, $F(6)\\approx 0.405986420U=0.513$, so continue. At $k=7$, $F(7)\\approx 0.554044020\\ge U=0.513$. Therefore, by the stopping criterion $U\\le F(k)$ at the smallest such $k$, the algorithm returns\n$$\nX=7.\n$$\nThis is the required sample.",
            "answer": "$$\\boxed{7}$$"
        },
        {
            "introduction": "A correct algorithm is only the beginning; in practice, performance matters. This exercise challenges you to scale up the inversion method by parallelizing it in a Single Instruction, Multiple Data (SIMD) fashion, a common paradigm in modern computing. You will explore how to manage multiple generation \"lanes\" that terminate at different times and quantify the resulting load-balancing efficiency .",
            "id": "3329675",
            "problem": "Design and implement a complete, vectorized inversion algorithm for generating Poisson random variates across multiple parallel lanes using the recurrence update. Your task is to derive the algorithm from first principles and then implement it as a Single Instruction Multiple Data (SIMD) style masked vector loop that updates the probability recurrence simultaneously across lanes and handles lane termination when individual lanes complete at different iteration counts.\n\nMathematical foundation and goals:\n- The Poisson distribution with rate parameter $\\lambda \\gt 0$ has probability mass function\n$$\n\\mathbb{P}(K = k) \\;=\\; e^{-\\lambda}\\,\\frac{\\lambda^k}{k!}, \\quad k \\in \\{0,1,2,\\dots\\}.\n$$\n- The inversion method draws $U \\sim \\mathrm{Uniform}(0,1)$ and returns the smallest integer $k$ such that the cumulative distribution function satisfies\n$$\n\\sum_{j=0}^{k} e^{-\\lambda}\\,\\frac{\\lambda^j}{j!} \\;\\ge\\; U.\n$$\n- The cumulative distribution can be accumulated iteratively using the well-tested probability recurrence\n$$\np(k+1) \\;=\\; p(k)\\,\\frac{\\lambda}{k+1}, \\quad \\text{with } p(0) \\;=\\; e^{-\\lambda}.\n$$\n- You must vectorize the inversion across $L$ lanes by maintaining lane-wise arrays for $p(k)$ and the cumulative sum and by advancing a single iteration index $k$ for all lanes, using a boolean activity mask to update only those lanes that have not yet terminated.\n\nAlgorithmic requirements:\n1. Begin from the recurrence definition above and derive a masked vector loop that:\n   - Initializes $p(0) = e^{-\\lambda}$ identically across $L$ lanes.\n   - Maintains a lane-wise cumulative sum $S(k) = \\sum_{j=0}^k p(j)$.\n   - At $k=0$, compares $S(0)$ against each lane’s uniform $U_\\ell$ to determine immediate terminations with $K_\\ell=0$.\n   - For $k \\ge 1$, updates only the active (non-terminated) lanes via the recurrence $p(k) = p(k-1)\\,\\lambda/k$, increments their cumulative sums, and marks those lanes that newly satisfy $S(k) \\ge U_\\ell$, storing $K_\\ell = k$.\n   - Stops when all lanes have terminated. Define the total number of iterations processed as\n     $$\n     T \\;=\\; 1 + \\max_{\\ell \\in \\{1,\\dots,L\\}} K_\\ell,\n     $$\n     where the $+\\,1$ accounts for the initial $k=0$ step.\n2. Define the load-balancing efficiency as the average fraction of active lanes over iterations:\n   $$\n   \\eta \\;=\\; \\frac{\\sum_{\\ell=1}^{L} \\left(K_\\ell + 1\\right)}{T \\cdot L}.\n   $$\n   Here, each lane $\\ell$ contributes $K_\\ell + 1$ active iterations (from $k=0$ through $k=K_\\ell$ inclusive). Also compute the total number of idle lane-steps,\n   $$\n   I \\;=\\; T\\cdot L \\;-\\; \\sum_{\\ell=1}^{L} \\left(K_\\ell + 1\\right).\n   $$\n   These definitions quantify the load imbalance caused by different termination depths across lanes.\n\nImplementation constraints:\n- Implement the algorithm in a vectorized manner using masked updates, without per-lane scalar loops.\n- Do not assume any special hardware; use standard array operations to emulate SIMD masking semantics.\n- All answers are unitless and should be provided as integers or decimals as specified.\n\nTest suite:\nUse $L = 6$ lanes. For each test case, you are given a rate $\\lambda$ and a fixed vector of uniforms $U = \\{u_1,u_2,u_3,u_4,u_5,u_6\\}$. For each test case, compute:\n- The lane-wise Poisson outputs $[K_1,K_2,K_3,K_4,K_5,K_6]$ as integers.\n- The total iteration count $T$ as an integer.\n- The efficiency $\\eta$ rounded to exactly six decimal places.\n- The idle lane-steps $I$ as an integer.\n\nProvide results for the following four test cases:\n- Case A: $\\lambda = 0.1$, $U = \\{0.001, 0.1, 0.5, 0.9, 0.999, 0.37\\}$.\n- Case B: $\\lambda = 5.0$, $U = \\{0.001, 0.1, 0.5, 0.9, 0.999, 0.37\\}$.\n- Case C: $\\lambda = 30.0$, $U = \\{0.001, 0.1, 0.5, 0.9, 0.999, 0.37\\}$.\n- Case D: $\\lambda = 0.000001$, $U = \\{0.001, 0.1, 0.5, 0.9, 0.999, 0.37\\}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Specifically, the output should be a top-level list with four elements (one per test case). Each element is a sublist of the form\n$$\n[\\,[K_1,K_2,K_3,K_4,K_5,K_6],\\; T,\\; \\eta,\\; I\\,],\n$$\nwith $\\eta$ rounded to six decimal places. For example, the printed structure should look like\n$$\n[[[\\dots],T,\\eta,I],[[\\dots],T,\\eta,I],[[\\dots],T,\\eta,I],[[\\dots],T,\\eta,I]].\n$$",
            "solution": "### Algorithm Derivation from First Principles\n\nThe generation of random variates from a specified distribution is a fundamental task in Monte Carlo methods. The problem requires implementing the inversion method for the Poisson distribution in a vectorized, SIMD-like fashion.\n\n**1. The Inversion Method for Discrete Distributions**\n\nThe core principle is the inversion method (also known as inverse transform sampling). For a discrete random variable $K$ with cumulative distribution function (CDF) $F(k) = \\mathbb{P}(K \\le k)$, we can generate a variate by drawing a random number $U$ from a uniform distribution on $(0,1)$, denoted $U \\sim \\mathrm{Uniform}(0,1)$, and finding the smallest integer $k$ such that $F(k) \\ge U$.\n\n**2. Application to the Poisson Distribution**\n\nThe Poisson distribution with rate parameter $\\lambda  0$ has a probability mass function (PMF) given by:\n$$\np(k) = \\mathbb{P}(K=k) = e^{-\\lambda} \\frac{\\lambda^k}{k!} \\quad \\text{for } k \\in \\{0, 1, 2, \\dots\\}\n$$\nThe CDF is the sum of these probabilities:\n$$\nF(k) = \\sum_{j=0}^{k} p(j) = \\sum_{j=0}^{k} e^{-\\lambda} \\frac{\\lambda^j}{j!}\n$$\nThe inversion method requires us to find the smallest integer $k$ such that $\\sum_{j=0}^{k} p(j) \\ge U$.\n\n**3. An Iterative Scalar Algorithm**\n\nDirectly calculating $F(k)$ for each $k$ is computationally expensive due to the factorial and power terms. A more efficient approach is to build the sum iteratively. The problem provides the following recurrence relation for the PMF terms:\n$$\np(k+1) = p(k) \\frac{\\lambda}{k+1}\n$$\nThis relation is derived by observing the ratio $p(k+1)/p(k)$. With the base case $p(0) = e^{-\\lambda}$, we can sequentially generate all $p(k)$. This leads to a simple scalar algorithm:\n\n1.  Initialize $k=0$, the probability term $p = e^{-\\lambda}$, and the cumulative sum $S = p$.\n2.  Draw $U \\sim \\mathrm{Uniform}(0,1)$.\n3.  While $S  U$:\n    a. Increment $k \\leftarrow k+1$.\n    b. Update the probability term: $p \\leftarrow p \\cdot \\frac{\\lambda}{k}$.\n    c. Update the cumulative sum: $S \\leftarrow S + p$.\n4.  The result is the final value of $k$.\n\n**4. Vectorized (SIMD) Algorithm Design**\n\nThe task is to perform this generation for $L$ independent lanes in parallel, each with its own uniform random number $U_\\ell$. A naive approach would be to run the scalar algorithm $L$ times. However, the requirement is for a vectorized algorithm that processes all lanes in a single loop, emulating Single Instruction, Multiple Data (SIMD) execution.\n\nThis is achieved by representing the state of each lane in a vector (an array) and using a mask to control which lanes are updated at each step. The key insight is that the loop counter $k$ is common to all lanes. Lanes simply become \"inactive\" once their termination condition is met.\n\n**State Variables (as vectors of size $L$):**\n- $U$: The vector of input uniform random numbers for each lane.\n- $K$: The vector to store the resulting Poisson variates. Initialized to a sentinel value (e.g., $-1$).\n- $p$: The vector of current PMF terms $p(k)$ for each lane.\n- $S$: The vector of current CDF values $S(k) = \\sum_{j=0}^k p(j)$ for each lane.\n- `active_mask`: A boolean vector indicating which lanes are still seeking their $k$ (i.e., for which $S_\\ell  U_\\ell$).\n\n**Vectorized Algorithm Steps:**\n\n1.  **Initialization ($k=0$):**\n    a. Set the loop counter $k=0$.\n    b. Initialize the result vector $K$ with sentinels.\n    c. Initialize the activity mask `active_mask` to `True` for all $L$ lanes.\n    d. Calculate the initial probability $p_0 = e^{-\\lambda}$.\n    e. Initialize the probability vector $p$ with $p_0$ for all lanes.\n    f. Initialize the cumulative sum vector $S$ with $p_0$ for all lanes (since $S(0) = p(0)$).\n\n2.  **Initial Termination Check (at $k=0$):**\n    a. Compare the initial cumulative sum vector $S$ with the uniform vector $U$.\n    b. Identify lanes where $S_\\ell \\ge U_\\ell$. For these lanes, the result is $K_\\ell = 0$.\n    c. Update the result vector $K$ for these newly terminated lanes.\n    d. Update the `active_mask` to `False` for these lanes.\n\n3.  **Main Iteration Loop (for $k \\ge 1$):**\n    The loop continues as long as any lane remains active (`any(active_mask)` is `True`).\n    a. Increment the global counter: $k \\leftarrow k+1$.\n    b. **Masked Recurrence Update:** Update the probability vector $p$ using the recurrence, but only for the active lanes:\n       $$ p_\\ell \\leftarrow p_\\ell \\cdot \\frac{\\lambda}{k} \\quad \\text{for all } \\ell \\text{ where } \\texttt{active\\_mask}_\\ell \\text{ is True} $$\n    c. **Masked Cumulative Sum Update:** Add the new probability term to the cumulative sum, again only for active lanes:\n       $$ S_\\ell \\leftarrow S_\\ell + p_\\ell \\quad \\text{for all } \\ell \\text{ where } \\texttt{active\\_mask}_\\ell \\text{ is True} $$\n    d. **Masked Termination Check:** Identify which of the currently active lanes have now met their termination condition ($S_\\ell \\ge U_\\ell$).\n    e. Store the result $k$ in the corresponding elements of the $K$ vector for any newly terminated lanes.\n    f. Set the `active_mask` to `False` for these lanes.\n\n4.  **Termination and Metrics Calculation:**\n    a. The loop terminates when `active_mask` is all `False`. The vector $K$ now holds the generated Poisson variates for all lanes.\n    b. The total number of iterations processed, $T$, is $1$ plus the maximum value found in $K$, accounting for the initial $k=0$ step: $T = 1 + \\max_{\\ell} K_\\ell$.\n    c. The total number of active lane-steps is the sum of iterations each lane ran for. A lane with result $K_\\ell$ was active for iterations $k=0, 1, \\dots, K_\\ell$, which is $K_\\ell+1$ steps. The sum is $\\sum_{\\ell=1}^{L} (K_\\ell + 1)$.\n    d. The total possible compute slots is $T \\cdot L$.\n    e. The load-balancing efficiency $\\eta$ is the ratio of active steps to total slots: $\\eta = \\frac{\\sum_{\\ell=1}^{L} (K_\\ell + 1)}{T \\cdot L}$.\n    f. The total number of idle lane-steps $I$ is the difference: $I = (T \\cdot L) - \\sum_{\\ell=1}^{L} (K_\\ell + 1)$.\n\nThis vectorized design strictly adheres to the problem's requirements, using array-wide operations controlled by a boolean mask to emulate SIMD processing and correctly handle asynchronous lane completion.",
            "answer": "[[[0, 0, 0, 0, 2, 0], 3, 0.444444, 10],[[0, 2, 5, 8, 13, 4], 14, 0.452381, 46],[[15, 22, 30, 39, 54, 28], 55, 0.612121, 128],[[0, 0, 0, 0, 0, 0], 1, 1.000000, 0]]"
        },
        {
            "introduction": "How can we be certain that our implemented generator is correct? This final practice guides you through designing and implementing a robust validation protocol to test a sampler's output against the theoretical properties of the Poisson distribution. You will use statistical tests to check for moment consistency (mean and variance) and overall distributional fidelity, learning to distinguish a correct generator from several common but flawed alternatives .",
            "id": "3329708",
            "problem": "You are tasked to design and implement a validation protocol for a sampler of the Poisson distribution, grounded in first principles of stochastic simulation and Monte Carlo methods. The protocol must simultaneously evaluate moment consistency and distributional fidelity. You must implement the following requirements in a complete, runnable program.\n\nThe sampler under test generates independent samples purportedly following the Poisson distribution with parameter $ \\lambda  0 $. The Poisson distribution is defined by the probability mass function $ p(k) = \\mathbb{P}(X = k) = e^{-\\lambda} \\lambda^{k} / k! $ for integer $ k \\ge 0 $. The mathematical facts forming the foundational base are: $ \\mathbb{E}[X] = \\lambda $ and $ \\operatorname{Var}(X) = \\lambda $, and that the normalized sample mean satisfies a limit theorem under the Central Limit Theorem (CLT). You must derive your tests using these foundations and the asymptotic behavior of aggregate statistics for independent and identically distributed (i.i.d.) samples.\n\nMoment validation:\n- Construct a two-sided test for the sample mean $ \\bar{X} $ to assess whether it is consistent with $ \\lambda $ under the CLT, using a significance threshold $ \\alpha_{\\text{mean}} $ expressed as a decimal. Your final test must return a boolean result indicating acceptance or rejection.\n- Construct a two-sided test for the sample variance $ S^{2} $ to assess equidispersion (variance equal to mean) using an asymptotic approximation derived from the fourth central moment and the variance of the sample variance for i.i.d. observations. Use a significance threshold $ \\alpha_{\\text{var}} $ expressed as a decimal. Your final test must return a boolean result indicating acceptance or rejection.\n\nGoodness-of-fit validation:\n- Implement a discrete goodness-of-fit test using the Pearson chi-square statistic. Binning is mandatory. Define bins over nonnegative integers such that each bin’s expected count under the Poisson model is at least $ c_{\\min} $, where $ c_{\\min} $ is provided as an integer and must be at least $ 5 $. Let the number of bins be $ B $. When computing the goodness-of-fit statistic, treat $ \\lambda $ as known and do not estimate it from the data, so that the degrees of freedom are $ B - 1 $.\n- Your bin specification must be algorithmic: starting from $ k = 0 $, aggregate consecutive integer categories until each bin meets the expected count threshold $ c_{\\min} $. You must also include a final tail bin that aggregates all values beyond the largest explicitly enumerated category. To make enumeration finite, truncate the explicit category list at the smallest integer $ K $ such that the cumulative Poisson probability up to $ K $ is at least $ 1 - \\varepsilon $, where $ \\varepsilon $ is a provided tail probability threshold expressed as a decimal (e.g., $ 10^{-12} $). Merge the tail bin into the previous bin if its expected count is below $ c_{\\min} $.\n- Compute the Pearson chi-square statistic over the bins and its associated $ p $-value using the chi-square distribution. Use a significance threshold $ \\alpha_{\\text{gof}} $ expressed as a decimal. Your final test must return a boolean result indicating acceptance or rejection.\n\nProtocol decision:\n- The overall protocol must accept a sampler if and only if all three validations accept: mean test, variance test, and goodness-of-fit test.\n\nYou must also include the Discrete Kolmogorov–Smirnov (KS) test notionally in your design rationale by referencing its role as an alternative discrete goodness-of-fit metric, but the implemented test in the program must be the chi-square test as specified above.\n\nImplementation constraints:\n- Implement a Poisson sampler using the inversion method: draw $ U \\sim \\text{Uniform}(0,1) $, enumerate the cumulative distribution function (CDF) $ F(k) = \\sum_{j=0}^{k} p(j) $ via recursion $ p(0) = e^{-\\lambda} $ and $ p(k+1) = p(k) \\cdot \\lambda / (k+1) $, and return the smallest $ k $ such that $ F(k) \\ge U $. This sampler must be used to generate the \"correct\" samples in the test suite.\n- Additionally, implement three faulty samplers to challenge the protocol:\n    1. A mean-shifted sampler that returns samples from a Poisson distribution with parameter $ (1+\\delta)\\lambda $ for a specified $ \\delta  0 $.\n    2. A variance-inflated mixture sampler that draws half the samples from $ \\text{Poisson}(\\lambda) $ and half from $ \\text{Poisson}(\\lambda+\\Delta) $ for a specified $ \\Delta  0 $.\n    3. A truncated sampler that draws from $ \\text{Poisson}(\\lambda) $ and clips values above a specified cap $ k_{\\max} $.\n\nYour program must implement the validation protocol and apply it to the following test suite. Use a fixed random seed for reproducibility.\n\nTest suite parameter sets:\n- Case $ 1 $: $ \\lambda = 3.0 $, sample size $ n = 8000 $, sampler type \"correct\", $ \\alpha_{\\text{mean}} = 0.01 $, $ \\alpha_{\\text{var}} = 0.01 $, $ \\alpha_{\\text{gof}} = 0.05 $, $ c_{\\min} = 5 $, $ \\varepsilon = 10^{-12} $.\n- Case $ 2 $: $ \\lambda = 20.0 $, sample size $ n = 15000 $, sampler type \"correct\", $ \\alpha_{\\text{mean}} = 0.01 $, $ \\alpha_{\\text{var}} = 0.01 $, $ \\alpha_{\\text{gof}} = 0.05 $, $ c_{\\min} = 5 $, $ \\varepsilon = 10^{-12} $.\n- Case $ 3 $: $ \\lambda = 0.3 $, sample size $ n = 10000 $, sampler type \"correct\", $ \\alpha_{\\text{mean}} = 0.01 $, $ \\alpha_{\\text{var}} = 0.01 $, $ \\alpha_{\\text{gof}} = 0.05 $, $ c_{\\min} = 5 $, $ \\varepsilon = 10^{-12} $.\n- Case $ 4 $: $ \\lambda = 5.0 $, sample size $ n = 8000 $, sampler type \"mean\\_shift\" with $ \\delta = 0.1 $, $ \\alpha_{\\text{mean}} = 0.01 $, $ \\alpha_{\\text{var}} = 0.01 $, $ \\alpha_{\\text{gof}} = 0.05 $, $ c_{\\min} = 5 $, $ \\varepsilon = 10^{-12} $.\n- Case $ 5 $: $ \\lambda = 10.0 $, sample size $ n = 10000 $, sampler type \"variance\\_mixture\" with $ \\Delta = 4.0 $, $ \\alpha_{\\text{mean}} = 0.01 $, $ \\alpha_{\\text{var}} = 0.01 $, $ \\alpha_{\\text{gof}} = 0.05 $, $ c_{\\min} = 5 $, $ \\varepsilon = 10^{-12} $.\n- Case $ 6 $: $ \\lambda = 30.0 $, sample size $ n = 8000 $, sampler type \"truncated\" with cap $ k_{\\max} = 22 $, $ \\alpha_{\\text{mean}} = 0.01 $, $ \\alpha_{\\text{var}} = 0.01 $, $ \\alpha_{\\text{gof}} = 0.05 $, $ c_{\\min} = 5 $, $ \\varepsilon = 10^{-12} $.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of booleans enclosed in square brackets, for the six cases in order (e.g., $ [\\text{True},\\text{False},\\ldots] $). No other text should be printed.\n\nAngle units are not applicable. There are no physical units involved. All significance thresholds must be treated as decimals, not percentages. Ensure that all algorithmic choices are consistent with the foundational facts and with statistically sound practice for binning and asymptotic approximations.",
            "solution": "### Foundational Principles of the Poisson Distribution\n\nThe Poisson distribution, parameterized by its rate $\\lambda  0$, describes the probability of a given number of events occurring in a fixed interval of time or space. Its probability mass function (PMF) is given by:\n$$\np(k; \\lambda) = \\mathbb{P}(X = k) = \\frac{e^{-\\lambda} \\lambda^k}{k!} \\quad \\text{for } k \\in \\{0, 1, 2, \\dots\\}\n$$\nA defining characteristic of the Poisson distribution is equidispersion, where the mean and variance are equal:\n$$\n\\mathbb{E}[X] = \\lambda\n$$\n$$\n\\operatorname{Var}(X) = \\sigma^2 = \\lambda\n$$\nThese fundamental properties form the basis for our validation tests. We are given a set of supposedly independent and identically distributed (i.i.d.) samples $\\{X_1, X_2, \\dots, X_n\\}$ and must test the null hypothesis $H_0$ that these samples originate from a $\\text{Poisson}(\\lambda)$ distribution.\n\n### 1. Moment Validation\n\nMoment-based tests verify whether the sample moments (mean, variance) are statistically consistent with their theoretical counterparts under the null hypothesis. We use asymptotic results derived from the Central Limit Theorem (CLT) for large sample sizes $n$.\n\n#### 1.1. Test for the Sample Mean\n\n**Principle:** The Central Limit Theorem states that for a large i.i.d. sample, the sample mean $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i$ is approximately normally distributed. Under $H_0$, the population mean is $\\mu = \\lambda$ and the variance is $\\sigma^2 = \\lambda$. Therefore, the distribution of the sample mean is:\n$$\n\\bar{X} \\dot{\\sim} \\mathcal{N}\\left(\\lambda, \\frac{\\lambda}{n}\\right)\n$$\nwhere $\\dot{\\sim}$ denotes \"is approximately distributed as\".\n\n**Test Construction:** We can construct a standardized test statistic $Z_{\\text{mean}}$ that follows a standard normal distribution $\\mathcal{N}(0, 1)$:\n$$\nZ_{\\text{mean}} = \\frac{\\bar{X} - \\mathbb{E}[\\bar{X}]}{\\sqrt{\\operatorname{Var}(\\bar{X})}} = \\frac{\\bar{X} - \\lambda}{\\sqrt{\\lambda/n}}\n$$\nFor a two-sided test with a significance level $\\alpha_{\\text{mean}}$, we reject $H_0$ if the observed statistic falls in the tails of the standard normal distribution. The acceptance region is defined by $|Z_{\\text{mean}}| \\le z_{1-\\alpha_{\\text{mean}}/2}$, where $z_{q}$ is the $q$-th quantile of the $\\mathcal{N}(0, 1)$ distribution. The test accepts if this condition holds, returning `True`.\n\n#### 1.2. Test for the Sample Variance\n\n**Principle:** Similar to the sample mean, the sample variance $S^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})^2$ also has an asymptotically normal distribution for large $n$. The expected value of $S^2$ is $\\sigma^2$. Under $H_0$, this is $\\lambda$. The variance of the sample variance, for large $n$, is given by:\n$$\n\\operatorname{Var}(S^2) \\approx \\frac{1}{n} (\\mu_4 - \\sigma^4)\n$$\nwhere $\\mu_4 = \\mathbb{E}[(X-\\mu)^4]$ is the fourth central moment of the underlying distribution.\n\n**Test Construction:** For the Poisson($\\lambda$) distribution, the fourth central moment is $\\mu_4 = \\lambda + 3\\lambda^2$. Since $\\sigma^2 = \\lambda$, we have $\\sigma^4 = \\lambda^2$. The variance of the sample variance is therefore:\n$$\n\\operatorname{Var}(S^2) \\approx \\frac{1}{n} ((\\lambda + 3\\lambda^2) - \\lambda^2) = \\frac{2\\lambda^2 + \\lambda}{n}\n$$\nWe construct a standardized test statistic $Z_{\\text{var}}$ for the sample variance:\n$$\nZ_{\\text{var}} = \\frac{S^2 - \\mathbb{E}[S^2]}{\\sqrt{\\operatorname{Var}(S^2)}} = \\frac{S^2 - \\lambda}{\\sqrt{(2\\lambda^2 + \\lambda)/n}}\n$$\nFor a two-sided test with significance level $\\alpha_{\\text{var}}$, the acceptance region is $|Z_{\\text{var}}| \\le z_{1-\\alpha_{\\text{var}}/2}$. The test accepts if this condition holds, returning `True`. This test directly assesses the equidispersion property.\n\n### 2. Distributional Fidelity Validation (Goodness-of-Fit)\n\nWhile moment tests are necessary, they are not sufficient. A distribution can have the correct mean and variance but still deviate from the target distribution shape. A goodness-of-fit (GOF) test compares the empirical distribution of the sample to the theoretical distribution.\n\n#### 2.1. Pearson's Chi-Square Test\n\n**Principle:** This test partitions the sample space into a finite number of bins, $B$, and compares the observed count of samples in each bin, $O_i$, to the expected count, $E_i$, under the null hypothesis. The test statistic is:\n$$\n\\chi^2 = \\sum_{i=1}^{B} \\frac{(O_i - E_i)^2}{E_i}\n$$\nUnder $H_0$, and provided that expected counts are sufficiently large (e.g., $E_i \\ge c_{\\min}=5$), this statistic follows a chi-square distribution, $\\chi^2_{df}$, with $df$ degrees of freedom.\n\n**Test Construction:**\n*   **Degrees of Freedom:** Since the parameter $\\lambda$ is specified by the null hypothesis and not estimated from the data, the degrees of freedom are $df = B - 1$.\n*   **Binning a Discrete Distribution:** The problem requires a specific algorithmic binning strategy to ensure each bin has a sufficient expected count, $E_i = n \\cdot P(\\text{bin}_i) \\ge c_{\\min}$.\n    1.  The range of values $k$ is infinite. We first truncate the explicit enumeration at a point $K$ such that the tail probability is negligible: $F(K) = \\sum_{j=0}^{K} p(j; \\lambda) \\ge 1 - \\varepsilon$.\n    2.  Starting from $k=0$, we form bins by grouping consecutive integer outcomes. A new bin is started once the cumulative expected count for the current bin reaches or exceeds $c_{\\min}$.\n    3.  The final bin includes all remaining outcomes up to $K$ and the tail probability mass beyond $K$. If this final bin has an expected count below $c_{\\min}$, it is merged with the preceding bin.\n*   **Decision Rule:** We calculate the $p$-value, which is the probability of observing a $\\chi^2$ statistic as extreme or more extreme than the one computed, assuming $H_0$ is true: $p\\text{-value} = \\mathbb{P}(\\chi^2_{df} \\ge \\chi^2_{\\text{observed}})$. We accept $H_0$ if the $p$-value is greater than or equal to the significance level $\\alpha_{\\text{gof}}$.\n\n**Alternative GOF Test:** The problem statement references the discrete Kolmogorov-Smirnov (KS) test as a notional alternative. The KS test evaluates the maximum absolute difference between the empirical cumulative distribution function (ECDF) and the theoretical CDF. While powerful for continuous distributions, its application to discrete distributions is more complex, as the standard test becomes conservative (i.e., less likely to reject a false null hypothesis). The chi-square test, with its mandated binning, is a more standard and direct approach for this problem's context.\n\n### 3. Sampler Implementation\n\nThe validation protocol will be tested against several samplers.\n\n*   **Correct Sampler (Inversion Method):** This method is based on the probability integral transform. For a discrete distribution with CDF $F(k)$, we generate a uniform random variate $U \\sim \\text{Uniform}(0,1)$ and find the smallest integer $k$ such that $F(k) \\ge U$. The CDF is computed iteratively: $p(0) = e^{-\\lambda}$, $p(k+1) = p(k)\\lambda/(k+1)$, and $F(k) = F(k-1) + p(k)$.\n*   **Faulty Samplers:**\n    1.  **Mean-Shifted:** Samples from $\\text{Poisson}((1+\\delta)\\lambda)$, which should be detected by the mean test.\n    2.  **Variance-Inflated:** A $50/50$ mixture of $\\text{Poisson}(\\lambda)$ and $\\text{Poisson}(\\lambda+\\Delta)$. This distribution will be overdispersed (variance greater than mean), a property the variance test is designed to detect.\n    3.  **Truncated:** Samples from $\\text{Poisson}(\\lambda)$ are capped at a maximum value $k_{\\max}$. This distorts the entire distribution, affecting mean, variance, and shape, and should be detected by all three tests.\n\n### 4. Overall Protocol Decision\n\nThe validation protocol is uncompromising. A sampler is deemed valid if and only if it passes all three independent checks: the mean consistency test, the variance consistency (equidispersion) test, and the distributional goodness-of-fit test. The final output is the logical AND of the boolean results from these three components.\n$$\n\\text{Accept Sampler} \\iff (\\text{Accept}_{\\text{mean}} \\land \\text{Accept}_{\\text{var}} \\land \\text{Accept}_{\\text{gof}})\n$$\nThis composite structure ensures a rigorous assessment of the sampler's correctness, checking both its low-order moments and its overall distributional shape.",
            "answer": "[True,True,True,False,False,False]"
        }
    ]
}