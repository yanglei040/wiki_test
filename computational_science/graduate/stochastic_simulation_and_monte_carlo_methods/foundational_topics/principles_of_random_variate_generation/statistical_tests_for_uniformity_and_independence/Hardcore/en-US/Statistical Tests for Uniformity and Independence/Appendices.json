{
    "hands_on_practices": [
        {
            "introduction": "The first step in validating any source of random numbers is to verify that its output conforms to the intended probability distribution. For most applications in stochastic simulation, this means testing for uniformity on the interval $[0,1]$. This practice guides you through the implementation of the Kolmogorov-Smirnov (KS) test, a cornerstone of goodness-of-fit testing. You will develop an efficient algorithm to compute the KS statistic, which measures the maximum deviation between the sample's empirical distribution and the theoretical uniform distribution, and use it to perform a formal hypothesis test .",
            "id": "3347511",
            "problem": "You are given a finite sample $u_1,\\dots,u_n \\in [0,1]$ and tasked with assessing the null hypothesis that the data are independent and identically distributed from the continuous Uniform distribution on $[0,1]$. Your program must implement an $O(n\\log n)$ algorithm to compute the exact Kolmogorov–Smirnov statistic $D_n$ and then compute a two-sided $p$-value for the one-sample test of uniformity using either an exact finite-sample method or an asymptotic method, as specified per test case.\n\nFundamental definitions to be used:\n- The empirical cumulative distribution function (empirical CDF) is $F_n(u) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{u_i \\le u\\}$.\n- The Kolmogorov–Smirnov statistic is $D_n = \\sup_{u \\in [0,1]} \\lvert F_n(u) - u \\rvert$.\n\nRequirements:\n1. Compute $D_n$ exactly by an algorithm with $O(n\\log n)$ time complexity. The intended approach should be derived from first principles of the empirical CDF and should not rely on numerical approximation of the supremum over a grid. The algorithm must handle ties in the sample.\n2. Compute the two-sided $p$-value $p = \\mathbb{P}(D_n \\ge d_{\\text{obs}})$ under the null hypothesis that $u_1,\\dots,u_n$ are independent and identically distributed Uniform on $[0,1]$. For this, use either:\n   - An exact finite-sample computation based on recursive counting of admissible paths or an equivalent numerically stable implementation, or\n   - An asymptotic approximation based on the weak convergence of $\\sqrt{n} D_n$ to the Kolmogorov distribution.\n3. Your program must implement both options and select which one to use based on a per-test-case directive.\n\nTest suite:\nImplement the following four test cases. For each case, compute and report both $D_n$ and the two-sided $p$-value according to the specified method. For clarity, define $\\operatorname{frac}(x)$ as the fractional part of $x$.\n\n- Case A (small $n$, exact):\n  - $n = 5$\n  - Sample: $[0.01, 0.20, 0.24, 0.90, 0.95]$\n  - Method: exact finite-sample\n\n- Case B (ties and endpoints, exact):\n  - $n = 10$\n  - Sample: $[0.00, 1.00, 0.12, 0.12, 0.30, 0.31, 0.55, 0.56, 0.78, 0.79]$\n  - Method: exact finite-sample\n\n- Case C (deterministic grid, asymptotic):\n  - $n = 2000$\n  - Sample: $u_i = \\frac{i}{n}$ for $i \\in \\{1,2,\\dots,n\\}$\n  - Method: asymptotic\n\n- Case D (deterministic quasi-random, asymptotic):\n  - $n = 100$\n  - Sample: $u_i = \\operatorname{frac}(i\\sqrt{2})$ for $i \\in \\{1,2,\\dots,n\\}$\n  - Method: asymptotic\n\nOutput specification:\n- For each test case, your program must produce a pair $[d, p]$ where $d$ is the computed $D_n$ and $p$ is the two-sided $p$-value for the specified method.\n- Round both $d$ and $p$ to $10$ decimal places.\n- Your program should produce a single line of output containing a list of these pairs, in order of the cases A, B, C, D, formatted as a comma-separated list enclosed in square brackets, with each pair itself enclosed in square brackets. For example, the output must look like:\n  - $[[d_A,p_A],[d_B,p_B],[d_C,p_C],[d_D,p_D]]$\n- All reported values are dimensionless real numbers and must be given as decimals.\n\nConstraints and notes:\n- The algorithm for $D_n$ must be $O(n\\log n)$ in time, which is achieved by sorting followed by a single linear pass.\n- The exact finite-sample method must target the true null distribution of $D_n$ for finite $n$. The asymptotic method must rely on the convergence in distribution of $\\sqrt{n} D_n$ to the Kolmogorov distribution.\n- No randomness is allowed in constructing the test samples; all samples are deterministic as specified.",
            "solution": "The user requests the implementation of the one-sample Kolmogorov-Smirnov test for uniformity. The problem is valid as it is scientifically grounded in well-established statistical theory, is well-posed with all necessary information provided, and is expressed in objective, formal language. We will proceed with a solution.\n\nThe solution requires two main components:\n1.  An algorithm to compute the Kolmogorov-Smirnov (KS) statistic, $D_n$.\n2.  Algorithms to compute the two-sided p-value using both an exact finite-sample method and an asymptotic approximation.\n\n### 1. Computation of the KS Statistic $D_n$\n\nThe KS statistic $D_n$ is defined as the supremum of the absolute difference between the empirical cumulative distribution function (ECDF) $F_n(u)$ and the CDF of the uniform distribution, $F(u)=u$.\n$$\nD_n = \\sup_{u \\in [0,1]} |F_n(u) - u|\n$$\nThe ECDF for a sample $u_1, u_2, \\dots, u_n$ is given by:\n$$\nF_n(u) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{u_i \\le u\\}\n$$\n$F_n(u)$ is a step function that increases by $1/n$ at each sample point. The function $g(u) = F_n(u) - u$ is piecewise linear with a slope of $-1$, and it has jumps at the sample points. Therefore, the supremum of $|g(u)|$ must be attained immediately before or at one of these jump points.\n\nLet the sample be sorted in non-decreasing order: $u_{(1)} \\le u_{(2)} \\le \\dots \\le u_{(n)}$.\nThe ECDF has the value $F_n(u) = (i-1)/n$ for $u \\in [u_{(i-1)}, u_{(i)})$ and jumps to $F_n(u_{(i)}) = i/n$ if there are no ties at $u_{(i)}$. If there are ties, the jump is larger.\n\nA general and robust algorithm that correctly handles ties is to check the deviation at each sorted sample point $u_{(i)}$. At each point $u_{(i)}$, we evaluate the difference just before the jump and at the jump.\n- The value of the ECDF as $u$ approaches $u_{(i)}$ from the left is $F_n(u_{(i)}^{-}) = (i-1)/n$. The deviation is $|(i-1)/n - u_{(i)}| = u_{(i)} - (i-1)/n$ (since $u_{(i-1)}  u_{(i)}$ and $(i-1)/n \\approx u_{(i-1)}$).\n- The value of the ECDF at $u_{(i)}$ is $F_n(u_{(i)})$. If there are no ties, this is $i/n$. The deviation is $|i/n - u_{(i)}|$.\n\nThis suggests that the supremum can be found by calculating two quantities for each $i \\in \\{1, \\dots, n\\}$:\n$$\nd_i^+ = \\frac{i}{n} - u_{(i)} \\quad \\text{and} \\quad d_i^- = u_{(i)} - \\frac{i-1}{n}\n$$\nThe statistic $D_n$ is the maximum over all these values.\n$$\nD_n = \\max_{i=1,\\dots,n} \\left( d_i^+, d_i^- \\right)\n$$\nThis is equivalent to:\n$$\nD_n = \\max \\left( \\max_{i=1,\\dots,n} \\left( \\frac{i}{n} - u_{(i)} \\right), \\max_{i=1,\\dots,n} \\left( u_{(i)} - \\frac{i-1}{n} \\right) \\right)\n$$\nThe algorithm is as follows:\n1.  Sort the sample $u_1, \\dots, u_n$. This step has a time complexity of $O(n \\log n)$.\n2.  Perform a single linear pass over the sorted sample, calculating $d_i^+$ and $d_i^-$ for each $i$ and tracking the maximum value. This step is $O(n)$.\n\nThe total time complexity is dominated by the sorting step, resulting in an $O(n \\log n)$ algorithm as required. This procedure correctly handles ties in the data, as the indices $i$ will correctly reflect the cumulative counts.\n\n### 2. Computation of the p-value\n\nThe p-value is the probability of observing a test statistic as extreme as or more extreme than the one computed ($d_{obs}$), assuming the null hypothesis is true: $p = \\mathbb{P}(D_n \\ge d_{obs})$.\n\n#### 2.1. Exact Finite-Sample Method\nFor small sample sizes, the exact distribution of $D_n$ can be computed. This is a complex combinatorial problem, often solved with recursive algorithms based on counting admissible paths or using matrix-based methods. These algorithms compute the probability $\\mathbb{P}(D_n  d)$ by integrating the joint density of the order statistics of a uniform sample over the region defined by the constraints $|F_n(u) - u|  d$.\n\nThe problem is equivalent to computing the volume of the polytope defined by:\n$0  u_{(1)}  \\dots  u_{(n)}  1$\nand\n$\\frac{i}{n} - d  u_{(i)}  \\frac{i-1}{n} + d \\quad \\text{for } i=1, \\dots, n$.\n\nDirect implementation of the associated recursive integral is highly non-trivial. Standard statistical libraries like `scipy` contain numerically stable, well-vetted implementations of these exact methods (e.g., based on the work of Marsaglia, Tsang, and Wang, or Abas and Veraverbeke). Given the complexity and need for robustness, we will leverage the `scipy.stats.ks_1samp` function with the `mode='exact'` option. This is a sound scientific practice, as it uses a peer-reviewed algorithm, satisfying the problem's requirement for an \"exact finite-sample computation\". The function takes the sample as input and returns the statistic and the exact p-value.\n\n#### 2.2. Asymptotic Method\nFor large sample sizes ($n$), the distribution of the random variable $\\sqrt{n}D_n$ converges to the Kolmogorov distribution. Let $K$ be a random variable following this distribution.\n$$\n\\sqrt{n}D_n \\xrightarrow{d} K\n$$\nThe p-value can be approximated by:\n$$\np = \\mathbb{P}(D_n \\ge d_{obs}) \\approx \\mathbb{P}(K \\ge \\sqrt{n} d_{obs})\n$$\nThe survival function of the Kolmogorov distribution, $\\mathbb{P}(K  x)$, is given by the well-known series:\n$$\n\\mathbb{P}(K  x) = 1 - \\left( 1 - 2\\sum_{j=1}^{\\infty} (-1)^{j-1} e^{-2j^2x^2} \\right) = 2\\sum_{j=1}^{\\infty} (-1)^{j-1} e^{-2j^2x^2}\n$$\nThis series converges rapidly for moderate to large values of $x$. For very small $x$, an alternative series based on Jacobi theta functions is used for better numerical stability. The function `scipy.special.kolmogorov` provides an accurate implementation for computing this probability.\n\nThe algorithm is:\n1.  Compute $d_{obs}$ from the sample.\n2.  Calculate the scaled statistic $x = \\sqrt{n} d_{obs}$.\n3.  Compute the p-value using the asymptotic formula, for instance via `scipy.special.kolmogorov(x)`.\n\nWe will now implement these methods to solve the four test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import ks_1samp\nfrom scipy.special import kolmogorov\n\ndef solve():\n    \"\"\"\n    Implements the Kolmogorov-Smirnov test for uniformity for a set of predefined test cases.\n    \"\"\"\n\n    def calculate_dn(sample: list[float]) - float:\n        \"\"\"\n        Calculates the Kolomogorov-Smirnov statistic D_n in O(n log n) time.\n        \n        The statistic is D_n = sup_u |F_n(u) - u|, where F_n is the ECDF.\n        The calculation is based on the formula:\n        D_n = max(max_{i=1..n}(i/n - u_{(i)}), max_{i=1..n}(u_{(i)} - (i-1)/n))\n        where u_{(i)} are the sorted sample values.\n        \"\"\"\n        n = len(sample)\n        if n == 0:\n            return 0.0\n        \n        u_sorted = np.sort(np.asarray(sample, dtype=np.float64))\n        \n        d_max = 0.0\n        for i in range(n):\n            # i is 0-indexed (0 to n-1), so we use (i+1) for the formula's 1-indexed i.\n            # u_{(i+1)} corresponds to u_sorted[i].\n            # Deviation from above: F_n(u_{(i+1)}) - u_{(i+1)} = (i+1)/n - u_sorted[i]\n            d_plus = (i + 1) / n - u_sorted[i]\n            # Deviation from below: u_{(i+1)} - F_n(u_{(i+1)}^{-}) = u_sorted[i] - i/n\n            d_minus = u_sorted[i] - i / n\n            \n            d_max = max(d_max, d_plus, d_minus)\n            \n        return d_max\n\n    def p_value_asymptotic(d: float, n: int) - float:\n        \"\"\"\n        Calculates the two-sided p-value using the asymptotic Kolmogorov distribution.\n        p = P(D_n = d) approx P(K = sqrt(n)*d).\n        \"\"\"\n        if n == 0:\n            return 1.0\n        x = np.sqrt(n) * d\n        # scipy.special.kolmogorov(x) computes P(K  x), which is what we need.\n        return kolmogorov(x)\n\n    def solve_case(sample: list[float], method: str) - list[float, float]:\n        \"\"\"\n        Solves one test case by computing D_n and the p-value.\n        \"\"\"\n        n = len(sample)\n        d = calculate_dn(sample)\n        \n        p = 0.0\n        if method == 'exact':\n            # We use scipy's vetted implementation for the exact p-value.\n            # It takes the sample and computes both statistic and p-value.\n            # While it recomputes d, it guarantees correctness for this complex calculation.\n            # The CDF is F(x)=x for the uniform distribution on [0,1].\n            _, p = ks_1samp(sample, lambda x: x, mode='exact')\n        elif method == 'asymptotic':\n            p = p_value_asymptotic(d, n)\n        else:\n            raise ValueError(f\"Unknown method: {method}\")\n            \n        return [round(d, 10), round(p, 10)]\n\n    # Define the test cases from the problem statement.\n    test_cases_spec = [\n        {\n            'name': 'A',\n            'n': 5,\n            'sample_gen': lambda n: [0.01, 0.20, 0.24, 0.90, 0.95],\n            'method': 'exact'\n        },\n        {\n            'name': 'B',\n            'n': 10,\n            'sample_gen': lambda n: [0.00, 1.00, 0.12, 0.12, 0.30, 0.31, 0.55, 0.56, 0.78, 0.79],\n            'method': 'exact'\n        },\n        {\n            'name': 'C',\n            'n': 2000,\n            'sample_gen': lambda n: [(i + 1) / n for i in range(n)],\n            'method': 'asymptotic'\n        },\n        {\n            'name': 'D',\n            'n': 100,\n            'sample_gen': lambda n: [(i * np.sqrt(2)) % 1.0 for i in range(1, n + 1)],\n            'method': 'asymptotic'\n        }\n    ]\n\n    results = []\n    for case in test_cases_spec:\n        sample = case['sample_gen'](case['n'])\n        result = solve_case(sample, case['method'])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # e.g., [[d_A,p_A],[d_B,p_B],[d_C,p_C],[d_D,p_D]]\n    result_str = \",\".join([f\"[{d},{p}]\" for d, p in results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A sequence of numbers can be uniformly distributed yet fail to be random if there are patterns in its ordering. The property of independence—that each value is unpredictable from the preceding ones—is just as critical as uniformity. The runs test is a classical method for detecting such serial dependence. This exercise  challenges you to implement this test from first principles, focusing on the number of \"ascending runs\" in a sequence. You will delve into the combinatorial nature of permutation-based statistics by deriving the exact null distribution of the test statistic using Eulerian numbers, providing a deep understanding of how distribution-free tests work.",
            "id": "3347502",
            "problem": "You must design and implement a complete, runnable program that constructs and applies a test for serial independence in a sequence of realizations $U_1,U_2,\\dots,U_n$ from the distribution $\\mathrm{Unif}(0,1)$ by using the distribution of ascending runs. The program must compute the exact null distribution for small $n$ and use it to produce an exact two-sided $p$-value. The test must be based only on the relative order structure implied by independence and continuity, and not on any shortcut formulas. The foundational base you must adhere to is as follows.\n\nFundamental base:\n- If $U_1,U_2,\\dots,U_n$ are independent and identically distributed from a continuous distribution, then the vector of ranks of $(U_1,\\dots,U_n)$ is uniformly distributed over all $n!$ permutations. This follows from the Probability Integral Transform: if $X$ has continuous cumulative distribution function $F$, then $F(X)\\sim\\mathrm{Unif}(0,1)$, and independence is preserved under measurable transformations.\n- For any finite sequence of distinct real numbers $(x_1,\\dots,x_n)$, an ascending run is defined as a maximal contiguous block of indices on which the sequence is strictly increasing. The number of ascending runs $R$ equals $1$ plus the number of descents, where a descent is an index $i\\in\\{1,\\dots,n-1\\}$ such that $x_ix_{i+1}$.\n\nTask requirements:\n1. Define precisely the count of ascending runs $R$ in the observed order of $U_1,\\dots,U_n$ as the number of maximal contiguous strictly increasing segments. Formally, for $n\\geq 1$, start with $R=1$, and for each $i\\in\\{1,\\dots,n-1\\}$ add $1$ to $R$ whenever $U_i\\geq U_{i+1}$. Because $\\mathrm{Unif}(0,1)$ is continuous, equalities occur with probability $0$, but your algorithm must treat $U_i\\geq U_{i+1}$ as a run break.\n2. Under the null hypothesis $\\mathsf{H}_0$ that the sample is independent and identically distributed $\\mathrm{Unif}(0,1)$, derive the exact null distribution of $R$ for small $n$ from first principles. Specifically, use the fact that the ranks of $(U_1,\\dots,U_n)$ form a uniformly random permutation, and the number of ascending runs $R$ depends only on the permutation’s descents. Let $\\langle n,k\\rangle$ denote the Eulerian number that counts permutations of $\\{1,\\dots,n\\}$ with exactly $k$ ascents. Compute $\\langle n,k\\rangle$ for $n\\leq 10$ using the recurrence derived from insertion arguments on permutations:\n$$\n\\langle 1,0\\rangle=1,\\quad \\text{and for } n\\geq 2,\\; 0\\leq k\\leq n-1,\\quad\n\\langle n,k\\rangle=(k+1)\\langle n-1,k\\rangle + (n-k)\\langle n-1,k-1\\rangle,\n$$\nwith the convention that $\\langle n-1,-1\\rangle=\\langle n-1,n-1\\rangle=0$.\nRelate runs to ascents via descents: if a permutation has $d$ descents, then it has $R=d+1$ runs and $k=n-1-d$ ascents. Therefore, the null distribution of $R$ is:\n$$\n\\mathbb{P}_{\\mathsf{H}_0}(R=r)=\\frac{\\langle n,\\,n-r\\rangle}{n!},\\quad r\\in\\{1,2,\\dots,n\\}.\n$$\n3. Define the exact two-sided $p$-value for an observed $r$ using probability ordering on the discrete null distribution of $R$:\n$$\np\\text{-value}=\\sum_{r':\\,\\mathbb{P}_{\\mathsf{H}_0}(R=r')\\leq \\mathbb{P}_{\\mathsf{H}_0}(R=r)} \\mathbb{P}_{\\mathsf{H}_0}(R=r').\n$$\nReject $\\mathsf{H}_0$ at significance level $\\alpha$ if and only if the $p$-value is less than or equal to $\\alpha$. Use $\\alpha$ supplied in each test case.\n4. Implement the algorithm to compute $R$, compute $\\langle n,k\\rangle$ for all $k\\in\\{0,1,\\dots,n-1\\}$ using the recurrence, construct the null distribution $\\mathbb{P}_{\\mathsf{H}_0}(R=r)$ for $r\\in\\{1,\\dots,n\\}$, compute the exact two-sided $p$-value, and return the decision to reject or not.\n5. The program must be fully deterministic and use no randomization. All input is internal to the program.\n\nTest suite:\n- Case $1$: $n=1$, sequence $[0.42]$, $\\alpha=0.05$.\n- Case $2$: $n=2$, sequence $[0.7,0.6]$, $\\alpha=0.50$.\n- Case $3$: $n=5$, sequence $[0.1,0.8,0.2,0.7,0.3]$, $\\alpha=0.05$.\n- Case $4$: $n=8$, sequence $[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8]$, $\\alpha=0.05$.\n- Case $5$: $n=8$, sequence $[0.9,0.1,0.8,0.2,0.7,0.3,0.6,0.4]$, $\\alpha=0.05$.\n\nAnswer specification:\n- For each test case, compute and return a list $[n,R,p,\\text{reject}]$, where $n$ is the integer length, $R$ is the integer number of ascending runs, $p$ is the exact two-sided $p$-value as a decimal, and $\\text{reject}$ is a boolean indicating whether to reject $\\mathsf{H}_0$ at the given $\\alpha$.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, with each test case’s result in its own bracket. For example, the format must be like $[[n_1,R_1,p_1,\\text{reject}_1],[n_2,R_2,p_2,\\text{reject}_2],\\dots]$.",
            "solution": "The problem requires the design and implementation of a statistical test for the serial independence of a sequence of random variables, purportedly drawn from a $\\mathrm{Unif}(0,1)$ distribution. The test is to be based on the number of ascending runs in the sequence. The solution involves deriving and using the exact null distribution of the test statistic for small sample sizes.\n\nThe fundamental principle underlying this test is that if $U_1, U_2, \\dots, U_n$ are independent and identically distributed (i.i.d.) from any continuous distribution (such as $\\mathrm{Unif}(0,1)$), then any permutation of their values is equally likely. Consequently, the vector of their ranks is uniformly distributed over the $n!$ possible permutations of $\\{1, 2, \\dots, n\\}$. The number of ascending runs, $R$, depends only on the relative ordering of the $U_i$, i.e., on their rank permutation, and is thus a distribution-free statistic under the null hypothesis of independence.\n\nThe solution proceeds in five principal steps:\n1.  **Count the observed number of ascending runs, $R_{obs}$**: Define and compute the test statistic from the given sequence.\n2.  **Compute Eulerian Numbers**: Programmatically compute the table of Eulerian numbers, $\\langle n, k \\rangle$, which are essential for defining the null distribution.\n3.  **Construct the Null Distribution**: Use the Eulerian numbers to calculate the exact probability mass function (PMF) of $R$ under the null hypothesis, $\\mathbb{P}_{\\mathsf{H}_0}(R=r)$.\n4.  **Calculate the Two-Sided $p$-value**: Based on the observed statistic $R_{obs}$ and the null distribution, compute the exact two-sided $p$-value.\n5.  **Make a Decision**: Compare the $p$-value to the given significance level $\\alpha$ to decide whether to reject the null hypothesis.\n\n### Step 1: Counting Ascending Runs ($R$)\nAn ascending run is a maximal contiguous subsequence of increasing values. The number of such runs, $R$, is determined by the number of \"descents\" in the sequence. A descent occurs at index $i \\in \\{1, \\dots, n-1\\}$ if $U_i  U_{i+1}$. The problem statement specifies that a run break occurs when $U_i \\geq U_{i+1}$. For a continuous distribution, the probability of an equality $U_i = U_{i+1}$ is zero, so this distinction is typically negligible. The number of runs is one more than the number of descents.\n\nThe algorithm to count runs in a sequence $(U_1, \\dots, U_n)$ is:\n1. Initialize the number of runs, $R$, to $1$ (for $n \\geq 1$; for $n=0$, $R=0$).\n2. Iterate from $i=1$ to $n-1$.\n3. If $U_i \\geq U_{i+1}$, increment $R$ by $1$.\n\nFor example, in the sequence $[0.1, 0.8, 0.2, 0.7, 0.3]$, there are descents at $U_2=0.8  U_3=0.2$ and $U_4=0.7  U_5=0.3$. Starting with $R=1$, we add $2$ for the two descents, yielding an observed run count of $R_{obs}=3$.\n\n### Step 2: Computing Eulerian Numbers\nThe Eulerian number $\\langle n, k \\rangle$ counts the number of permutations of $\\{1, \\dots, n\\}$ with exactly $k$ ascents. An ascent is an index $i \\in \\{1, \\dots, n-1\\}$ where the permutation value at $i$ is less than the value at $i+1$. The number of descents, $d$, and ascents, $k$, in a permutation of length $n$ are related by $d+k = n-1$.\n\nThe problem provides the recurrence relation for Eulerian numbers:\n$$\n\\langle n, k \\rangle = (k+1) \\langle n-1, k \\rangle + (n-k) \\langle n-1, k-1 \\rangle\n$$\nfor $n \\geq 2$ and $0 \\leq k \\leq n-1$. The base case is $\\langle 1, 0 \\rangle = 1$. The boundary conditions are $\\langle n-1, -1 \\rangle = 0$ and $\\langle n-1, n-1 \\rangle = 0$. These can be computed and stored in a two-dimensional array for efficient lookup. For instance, to compute the row for $n$, we only need the values from the row for $n-1$.\n\n### Step 3: Constructing the Null Distribution of $R$\nThe number of runs $R$ is related to the number of descents $d$ by $R = d+1$. The number of ascents $k$ is $k = n-1-d$. Substituting $d = R-1$, we find the relationship between runs and ascents:\n$$\nk = n-1 - (R-1) = n-R\n$$\nA permutation with $R$ runs has $k = n-R$ ascents. Under the null hypothesis $\\mathsf{H}_0$, each of the $n!$ permutations is equally likely. The number of permutations with $R=r$ runs is therefore equal to the number of permutations with $k=n-r$ ascents, which is $\\langle n, n-r \\rangle$.\n\nThe probability of observing exactly $r$ runs is the ratio of favorable outcomes to the total number of outcomes:\n$$\n\\mathbb{P}_{\\mathsf{H}_0}(R=r) = \\frac{\\langle n, n-r \\rangle}{n!}, \\quad \\text{for } r \\in \\{1, 2, \\dots, n\\}.\n$$\nWe can pre-compute factorials and use the table of Eulerian numbers from Step 2 to build the PMF of $R$ for a given $n$.\n\n### Step 4: Calculating the Two-Sided $p$-value\nFor a discrete test statistic, the two-sided $p$-value is defined as the sum of probabilities of all outcomes that are as extreme or more extreme than the one observed. \"More extreme\" is interpreted as having a probability under $\\mathsf{H}_0$ that is less than or equal to the probability of the observed outcome.\n\nGiven an observed run count $R_{obs} = r$, its probability under the null is $P_{obs} = \\mathbb{P}_{\\mathsf{H}_0}(R=r)$. The $p$-value is then calculated as:\n$$\np\\text{-value} = \\sum_{r' \\in \\{1, \\dots, n\\} \\text{ s.t. } \\mathbb{P}_{\\mathsf{H}_0}(R=r') \\leq P_{obs}} \\mathbb{P}_{\\mathsf{H}_0}(R=r')\n$$\nThis involves iterating through all possible values of $R$ from $1$ to $n$, calculating their probabilities, and summing them up if they meet the criterion.\n\nAs an example, consider $n=5$. The null distribution, derived from the Eulerian numbers, is:\n$\\mathbb{P}(R=1) = 1/120$, $\\mathbb{P}(R=2) = 26/120$, $\\mathbb{P}(R=3) = 66/120$, $\\mathbb{P}(R=4) = 26/120$, $\\mathbb{P}(R=5) = 1/120$.\nIf we observe $R_{obs}=2$, then $P_{obs} = 26/120$. The outcomes $r'$ with $\\mathbb{P}(R=r') \\le 26/120$ are $r'=1$, $r'=2$, $r'=4$, and $r'=5$.\nThe $p$-value would be $\\mathbb{P}(R=1) + \\mathbb{P}(R=2) + \\mathbb{P}(R=4) + \\mathbb{P}(R=5) = (1+26+26+1)/120 = 54/120 = 0.45$.\n\n### Step 5: Making a Decision\nThe final step is to apply the standard decision rule. Given a significance level $\\alpha$, we compare the computed $p$-value to $\\alpha$:\n- If $p\\text{-value} \\leq \\alpha$, we reject the null hypothesis $\\mathsf{H}_0$. This suggests there is statistically significant evidence against the independence of the sequence.\n- If $p\\text{-value}  \\alpha$, we fail to reject $\\mathsf{H}_0$. This implies there is insufficient evidence to conclude that the sequence is not independent.\n\nThe entire procedure is deterministic and relies on established combinatorial and statistical principles, as stipulated by the problem. The implementation will combine these steps into a single program to process the specified test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the runs test for all specified test cases.\n    \"\"\"\n    test_cases = [\n        {'n': 1, 'sequence': [0.42], 'alpha': 0.05},\n        {'n': 2, 'sequence': [0.7, 0.6], 'alpha': 0.50},\n        {'n': 5, 'sequence': [0.1, 0.8, 0.2, 0.7, 0.3], 'alpha': 0.05},\n        {'n': 8, 'sequence': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8], 'alpha': 0.05},\n        {'n': 8, 'sequence': [0.9, 0.1, 0.8, 0.2, 0.7, 0.3, 0.6, 0.4], 'alpha': 0.05},\n    ]\n\n    max_n = max(case['n'] for case in test_cases)\n    \n    # Pre-compute Eulerian numbers and factorials to avoid redundant calculations.\n    eulerian_table = _compute_eulerian_table(max_n)\n    factorials = {i: math.factorial(i) for i in range(max_n + 1)}\n\n    results = []\n    for case in test_cases:\n        result = _perform_runs_test(\n            case['n'],\n            case['sequence'],\n            case['alpha'],\n            eulerian_table,\n            factorials\n        )\n        results.append(result)\n    \n    # Format the final output precisely as specified.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef _compute_eulerian_table(max_n):\n    \"\"\"\n    Computes Eulerian numbers n, k for 1 = n = max_n and 0 = k  n.\n    The table stores n, k at index [n][k].\n    \"\"\"\n    # max_n+1 rows for n=0..max_n, max_n columns for k=0..max_n-1\n    table = np.zeros((max_n + 1, max_n), dtype=np.double)\n    \n    if max_n = 1:\n        table[1, 0] = 1.0\n\n    for n in range(2, max_n + 1):\n        for k in range(n):\n            # Term 1: (k+1) * n-1, k\n            term1 = 0.0\n            if k  n - 1: # k for n-1 goes up to n-2\n                term1 = (k + 1) * table[n - 1, k]\n            \n            # Term 2: (n-k) * n-1, k-1\n            term2 = 0.0\n            if k  0:\n                term2 = (n - k) * table[n - 1, k - 1]\n            \n            table[n, k] = term1 + term2\n            \n    return table\n\ndef _count_runs(sequence):\n    \"\"\"\n    Counts the number of ascending runs in a sequence.\n    \"\"\"\n    n = len(sequence)\n    if n = 1:\n        return n\n    \n    runs = 1\n    for i in range(n - 1):\n        if sequence[i] = sequence[i+1]:\n            runs += 1\n    return runs\n\ndef _perform_runs_test(n, sequence, alpha, eulerian_table, factorials):\n    \"\"\"\n    Performs the full runs test for a single case.\n    \"\"\"\n    if n == 0:\n        return [0, 0, 1.0, False]\n    \n    # Step 1: Count observed runs\n    r_obs = _count_runs(sequence)\n    \n    # Step 2  3: Get the null distribution from pre-computed tables\n    null_dist = {}\n    n_factorial = float(factorials[n])\n    if n_factorial == 0:  # Handles n=0 case, though guarded earlier\n        n_factorial = 1.0\n\n    for r in range(1, n + 1):\n        # k = n - r (number of ascents)\n        k = n - r\n        # P(R=r) = n, n-r / n! = n, k / n!\n        num_permutations = eulerian_table[n, k]\n        prob = num_permutations / n_factorial\n        null_dist[r] = prob\n\n    if not (1 = r_obs = n):\n        # This case should not be possible for n  0\n        p_value = 1.0\n    else:\n        # Step 4: Compute the two-sided p-value\n        prob_obs = null_dist[r_obs]\n        p_value = 0.0\n        for r_val, prob in null_dist.items():\n            if prob = prob_obs:\n                p_value += prob\n\n    # Step 5: Make decision\n    reject_h0 = p_value = alpha\n    \n    return [n, r_obs, p_value, reject_h0]\n\nsolve()\n```"
        },
        {
            "introduction": "While simple tests can detect obvious flaws, some pseudo-random number generators, particularly Linear Congruential Generators (LCGs), hide their deficiencies in higher-dimensional correlations. The spectral test is a powerful and sophisticated tool designed to uncover these hidden structural patterns. It analyzes the geometric lattice formed by successive points from the generator, where a \"good\" generator produces a fine, dense lattice. This hands-on practice  will demystify the spectral test by having you compute the key quality metric—the spectral index—and demonstrate its profound practical implication: how a poor lattice structure can cause catastrophic errors, or aliasing, in Monte Carlo integration.",
            "id": "3347551",
            "problem": "Consider a multiplicative linear congruential generator defined by the recurrence $X_{n+1} \\equiv a X_n \\pmod m$ with integers $m \\ge 2$, $1 \\le a \\le m-1$, and an initial seed $X_0 \\in \\{1,2,\\dots,m-1\\}$. Define $U_n = X_n / m$ and, for a fixed dimension $s \\ge 1$, consider the $s$-dimensional point set formed by successive tuples $u_n = (U_n, U_{n+1}, \\dots, U_{n+s-1})$ for $n \\ge 0$.\n\nThe spectral test evaluates the geometric regularity of the points $\\{u_n\\}$ via the dual lattice in dimension $s$. Using the fundamental definitions of the linear congruential generator and integer linear congruences, define the set of dual vectors\n$$\n\\mathcal{H}_s = \\left\\{ h \\in \\mathbb{Z}^s \\setminus \\{0\\} \\ \\bigg| \\ \\sum_{i=1}^s h_i a^{i-1} \\equiv 0 \\ (\\mathrm{mod}\\ m) \\right\\}.\n$$\nFor $h \\in \\mathcal{H}_s$, the family of hyperplanes $H_{h,z} = \\{ u \\in \\mathbb{R}^s : h^\\top u = z \\}$ partitions the point set $\\{u_n\\}$ into at most $m$ layers with spacing determined by $h$. The spectral index in dimension $s$ is defined as the Euclidean length of the shortest nonzero vector in the dual lattice,\n$$\nb_s = \\min_{h \\in \\mathcal{H}_s} \\|h\\|_2,\n$$\nand the maximal spacing between consecutive parallel hyperplanes containing the points is $1 / b_s$. A larger $b_s$ indicates finer stratification and better uniformity.\n\nIn Monte Carlo integration over the unit cube $[0,1]^s$, consider the oscillatory integrands aligned and misaligned with the dual lattice, defined by\n$$\nf_h(u) = \\cos\\!\\left(2\\pi\\, h^\\top u\\right), \\quad f_k(u) = \\cos\\!\\left(2\\pi\\, k^\\top u\\right),\n$$\nwhere $h \\in \\mathcal{H}_s$ and $k \\in \\mathbb{Z}^s \\setminus \\{0\\}$ is chosen such that $\\sum_{i=1}^s k_i a^{i-1} \\not\\equiv 0 \\ (\\mathrm{mod}\\ m)$. The exact integrals over $[0,1]^s$ are $0$ for both $f_h$ and $f_k$ whenever at least one component of the frequency vector is nonzero. However, the sample averages $\\widehat{I}_h$ and $\\widehat{I}_k$ over the $N$ successive tuples $u_n$ can exhibit lattice aliasing for $f_h$ that is absent for $f_k$. Use angles in radians for the cosine functions. Express all numerical outputs as decimals.\n\nYour task is to write a complete program that, for each parameter set in the test suite, performs the following steps from first principles:\n\n- Compute the shortest nonzero vector $h^\\star \\in \\mathcal{H}_s$ and its spectral index $b_s = \\|h^\\star\\|_2$ by enumerating integer vectors in a bounded search region. Use the search radius\n$$\nR = \\left\\lceil 2\\, m^{1/s} \\right\\rceil\n$$\nand scan all $h \\in \\{-R,-R+1,\\dots,R\\}^s \\setminus \\{0\\}$ satisfying $\\sum_{i=1}^s h_i a^{i-1} \\equiv 0 \\ (\\mathrm{mod}\\ m)$; select the $h$ that minimizes $\\|h\\|_2$.\n- Generate the sequence $\\{X_n\\}$ and the normalized values $\\{U_n\\}$ over one full period $N$ of the recurrence starting at $X_0 = 1$, where $N$ is the smallest positive integer such that $X_N = X_0$.\n- Form the $N$ successive $s$-tuples $u_n = (U_n, U_{n+1}, \\dots, U_{n+s-1})$.\n- Compute the sample mean $\\widehat{I}_h$ of $f_{h^\\star}$ over $\\{u_n\\}$ and the sample mean $\\widehat{I}_k$ of $f_k$ for a misaligned integer vector $k$ with small components (choose $k = (1,1,\\dots,1)$ unless it is aligned, in which case choose $k = (1,2,\\dots,s)$); report the absolute errors $|\\widehat{I}_h - 0|$ and $|\\widehat{I}_k - 0|$.\n- Output, for each test case, the triplet $[b_s, |\\widehat{I}_h|, |\\widehat{I}_k|]$.\n\nUse the following test suite of parameter values to assess different behaviors:\n\n- Case $1$: $m = 31$, $a = 3$, $s = 2$.\n- Case $2$: $m = 127$, $a = 3$, $s = 3$.\n- Case $3$: $m = 256$, $a = 5$, $s = 2$.\n- Case $4$ (edge case): $m = 31$, $a = 3$, $s = 1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets, in the exact format\n$$\n\\text{[}[b_{s_1},|\\widehat{I}_h^{(1)}|,|\\widehat{I}_k^{(1)}|],[b_{s_2},|\\widehat{I}_h^{(2)}|,|\\widehat{I}_k^{(2)}|],\\dots\\text{]}\n$$\nwhere each numeric value should be printed in decimal form. Angles must be in radians. No external input is permitted.",
            "solution": "The user-provided problem is assessed as valid. It is scientifically grounded in the theory of pseudo-random number generation and Monte Carlo methods, specifically the spectral test for linear congruential generators (LCGs). The problem is well-posed, with all parameters, definitions, and procedures specified unambiguously, leading to a unique and computable result for each test case. All provided information is self-consistent and objective.\n\nThe solution proceeds by implementing the specified steps for each parameter set $(m, a, s)$. The overall process for each case is partitioned into three main stages: determining the generator's period, finding the shortest vector in the dual lattice, and calculating the Monte Carlo integration errors.\n\n### 1. Sequence Generation and Period Determination\nA multiplicative linear congruential generator is defined by the recurrence $X_{n+1} \\equiv a X_n \\pmod m$. Given the initial seed $X_0 = 1$, the sequence of states $\\{X_n\\}$ is generated until the state returns to $X_0$ for the first time. The number of steps taken, $N$, is the period of the generator for this seed. The sequence of $N$ states is $\\{X_0, X_1, \\dots, X_{N-1}\\}$. This sequence is then normalized to produce values in $[0, 1)$ by defining $U_n = X_n / m$. For subsequent calculations involving $s$-tuples $u_n = (U_n, \\dots, U_{n+s-1})$, indices are handled cyclically modulo the period $N$, i.e., $U_{N+j} = U_j$.\n\n### 2. Spectral Test: Shortest Dual Vector Search\nThe spectral test assesses the quality of an LCG by examining the lattice structure formed by the generated points. The set of dual vectors, which characterizes this structure, is given by\n$$\n\\mathcal{H}_s = \\left\\{ h \\in \\mathbb{Z}^s \\setminus \\{0\\} \\ \\bigg| \\ \\sum_{i=1}^s h_i a^{i-1} \\equiv 0 \\pmod m \\right\\}.\n$$\nThe spectral index $b_s$ is the Euclidean length of the shortest non-zero vector $h^\\star$ in this set, $b_s = \\|h^\\star\\|_2$. A larger $b_s$ implies a better distribution of points.\n\nTo find $h^\\star$, a systematic search is conducted over a bounded region of $\\mathbb{Z}^s$. The problem specifies a search hypercube defined by the radius $R = \\lceil 2\\, m^{1/s} \\rceil$. Every non-zero integer vector $h = (h_1, \\dots, h_s)$ with components $h_i \\in [-R, R]$ is tested. For each such vector, we compute the sum $C(h) = \\sum_{i=1}^s h_i a^{i-1}$ and check if $C(h) \\equiv 0 \\pmod m$. If the congruence holds, $h$ is a dual vector. We then calculate its squared Euclidean norm, $\\|h\\|_2^2 = \\sum_{i=1}^s h_i^2$. The vector that yields the minimum non-zero squared norm is identified as $h^\\star$, and its norm $b_s = \\sqrt{\\|h^\\star\\|_2^2}$ is the spectral index.\n\n### 3. Monte Carlo Integration Error\nThe problem investigates the error of a Monte Carlo estimate for two specific oscillatory integrands, $f_h(u) = \\cos(2\\pi h^\\top u)$ and $f_k(u) = \\cos(2\\pi k^\\top u)$, where $h \\in \\mathcal{H}_s$ and $k \\notin \\mathcal{H}_s$. The true integral of both functions over $[0,1]^s$ is $0$. The sample means over the $N$ points generated by the LCG are:\n$$\n\\widehat{I}_h = \\frac{1}{N} \\sum_{n=0}^{N-1} f_h(u_n), \\quad \\widehat{I}_k = \\frac{1}{N} \\sum_{n=0}^{N-1} f_k(u_n).\n$$\nThe absolute errors are $|\\widehat{I}_h|$ and $|\\widehat{I}_k|$.\n\n#### Error for the Aligned Vector $h^\\star$\nA crucial property of the dual lattice provides a theoretical shortcut for calculating $\\widehat{I}_h$. For any $h \\in \\mathcal{H}_s$, the dot product $h^\\top u_n$ is always an integer. This can be shown as follows:\nThe point $u_n$ is $(X_n/m, \\dots, X_{n+s-1}/m)$. The dot product is $h^\\top u_n = \\frac{1}{m} \\sum_{i=1}^s h_i X_{n+i-1}$.\nFrom the LCG definition, $X_{n+j} \\equiv a^j X_n \\pmod m$. This implies that the vector of states $(X_n, \\dots, X_{n+s-1})^\\top$ is congruent to $X_n (1, \\dots, a^{s-1})^\\top$ modulo $m$.\nThe dot product with $h$ in $\\mathbb{Z}$, $\\sum_{i=1}^s h_i X_{n+i-1}$, is therefore congruent to $X_n \\sum_{i=1}^s h_i a^{i-1}$ modulo $m$.\nSince $h \\in \\mathcal{H}_s$, we have $\\sum_{i=1}^s h_i a^{i-1} \\equiv 0 \\pmod m$.\nThus, for every $n$, $\\sum_{i=1}^s h_i X_{n+i-1}$ must be an integer multiple of $m$.\nThis means $h^\\top u_n = \\frac{1}{m} (\\text{multiple of } m) = \\text{integer}$.\nConsequently, $f_{h^\\star}(u_n) = \\cos(2\\pi \\cdot (\\text{integer})) = 1$ for all $n$. The sample mean $\\widehat{I}_h$ is therefore exactly $1$. The absolute error $|\\widehat{I}_h - 0|$ is $1$, a phenomenon known as aliasing, where the discrete sampling completely misrepresents the continuous function.\n\n#### Error for the Misaligned Vector $k$\nThe vector $k$ is chosen to be misaligned, meaning $\\sum_{i=1}^s k_i a^{i-1} \\not\\equiv 0 \\pmod m$. The default choice is $k=(1,1,\\dots,1)$, unless this vector is aligned, in which case $k=(1,2,\\dots,s)$ is used. For such a vector, the dot product $k^\\top u_n$ is not generally an integer. The sample mean $\\widehat{I}_k$ must be computed by explicitly summing the terms $\\cos(2\\pi k^\\top u_n)$ over all $N$ points $u_n$ and dividing by $N$. The absolute error is then $|\\widehat{I}_k|$. This value is expected to be small, reflecting the cancellation of positive and negative values of the cosine function.\n\nThe program implements these three stages for each test case and reports the triplet $[b_s, |\\widehat{I}_h|, |\\widehat{I}_k|]$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\nimport itertools\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the specified test suite.\n    \"\"\"\n    test_cases = [\n        # (m, a, s)\n        (31, 3, 2),\n        (127, 3, 3),\n        (256, 5, 2),\n        (31, 3, 1),\n    ]\n\n    results = []\n    for m, a, s in test_cases:\n        result = _solve_case(m, a, s)\n        results.append(result)\n\n    # Format the final output string exactly as required.\n    formatted_results = [str(r).replace(' ', '') for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef _solve_case(m, a, s):\n    \"\"\"\n    Solves for a single case (m, a, s) and returns the result triplet.\n    \"\"\"\n    # 1. Generate sequence and find period N\n    x0 = 1\n    x_seq = [x0]\n    current_x = x0\n    while True:\n        next_x = (a * current_x) % m\n        if next_x == x0:\n            break\n        x_seq.append(next_x)\n        current_x = next_x\n    N = len(x_seq)\n\n    # 2. Find the shortest dual vector and spectral index b_s\n    R = math.ceil(2 * m**(1/s))\n    min_norm_sq = float('inf')\n    a_powers = [pow(a, i, m) for i in range(s)]\n\n    # Per the problem, \"scan all h in {-R,...,R}^s\"\n    for h in itertools.product(range(-R, R + 1), repeat=s):\n        if all(c == 0 for c in h):\n            continue\n        \n        # Check the congruence condition for h to be in the dual lattice\n        congruence_sum = sum(h[i] * a_powers[i] for i in range(s)) % m\n        \n        if congruence_sum == 0:\n            norm_sq = sum(c**2 for c in h)\n            if norm_sq  min_norm_sq:\n                min_norm_sq = norm_sq\n\n    b_s = math.sqrt(min_norm_sq)\n\n    # 3. Compute Monte Carlo integration errors\n    \n    # As derived in the solution, for any h in the dual lattice, the sample mean\n    # for cos(2*pi*h.T*u) is exactly 1 due to aliasing.\n    abs_I_h = 1.0\n\n    # Choose misaligned vector k\n    k = tuple([1] * s)\n    k_sum = sum(k[i] * a_powers[i] for i in range(s)) % m\n    if k_sum == 0:\n        k = tuple(range(1, s + 1))\n    \n    # Compute the sample mean for the misaligned vector k\n    u_seq = np.array(x_seq, dtype=float) / m\n    k_vec = np.array(k, dtype=float)\n\n    cos_sum_k = 0.0\n    for n in range(N):\n        # Construct the s-dimensional point u_n\n        u_n_tuple = np.array([u_seq[(n + i) % N] for i in range(s)])\n        dot_product_k = np.dot(k_vec, u_n_tuple)\n        cos_sum_k += np.cos(2 * np.pi * dot_product_k)\n    \n    I_hat_k = cos_sum_k / N\n    abs_I_k = abs(I_hat_k)\n\n    return [b_s, abs_I_h, abs_I_k]\n\nsolve()\n```"
        }
    ]
}