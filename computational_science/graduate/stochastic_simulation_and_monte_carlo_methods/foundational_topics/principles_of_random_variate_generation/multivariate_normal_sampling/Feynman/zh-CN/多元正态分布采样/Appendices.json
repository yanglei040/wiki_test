{
    "hands_on_practices": [
        {
            "introduction": "蒙特卡洛模拟的效率不仅取决于样本数量，更取决于样本生成的巧妙程度。对偶变量是一种经典的方差缩减技术，它利用高斯分布的对称性来降低估计量的方差，从而用更少的样本获得更精确的结果。这项练习将引导你通过数学推导，从根本上理解其工作原理以及为何能达到如此效果，特别是对于关于均值对称的函数，其方差缩减效果将是惊人的 。",
            "id": "3322640",
            "problem": "考虑一个维度 $d \\in \\mathbb{N}$ 和一个随机向量 $Z \\in \\mathbb{R}^{d}$，其服从 $d$ 维多元正态分布 $\\mathcal{N}_{d}(0, I_{d})$，其中 $I_{d}$ 是 $d \\times d$ 的单位矩阵。设 $\\mu \\in \\mathbb{R}^{d}$ 是一个固定的向量，且 $A \\in \\mathbb{R}^{d \\times d}$ 是一个固定的满秩矩阵，因此协方差矩阵 $\\Sigma := A A^{\\top}$ 是正定的。定义线性变换 $X := \\mu + A Z$，这是在随机模拟中从多元正态分布 $\\mathcal{N}_{d}(\\mu, \\Sigma)$ 中抽样的标准方法。\n\n在蒙特卡洛 (MC) 方法的对偶变量 (AV) 技术中，通过使用 $Z$ 及其对偶变量 $-Z$ 来构成对，相应地，$X := \\mu + A Z$ 和 $X^{-} := \\mu + A(-Z) = \\mu - A Z$。假设我们希望为一个可测函数 $f : \\mathbb{R}^{d} \\to \\mathbb{R}$（满足 $\\operatorname{Var}(f(X))  \\infty$）估计 $\\theta := \\mathbb{E}[f(X)]$。考虑两种每次重复使用两次函数评估的估计量：\n(i) 独立样本均值 $\\hat{\\theta}_{\\mathrm{ind}} := \\frac{1}{2}\\big(f(X^{(1)}) + f(X^{(2)})\\big)$，其中 $X^{(1)}$ 和 $X^{(2)}$ 独立同分布于 $\\mathcal{N}_{d}(\\mu, \\Sigma)$，\n以及 (ii) 对偶对均值 $\\hat{\\theta}_{\\mathrm{ant}} := \\frac{1}{2}\\big(f(X) + f(X^{-})\\big)$，由单个 $Z \\sim \\mathcal{N}_{d}(0, I_{d})$ 及其对偶变量 $-Z$ 构建。\n\n仅从多元正态分布的定义性质、高斯向量的线性变换，以及方差、协方差和相关性的定义出发，完成以下任务：\n1. 证明对偶对中的每个元素 $X$ 和 $X^{-}$ 的边缘抽样分布保持不变，仍然是 $\\mathcal{N}_{d}(\\mu, \\Sigma)$。\n2. 以 $\\operatorname{Var}(f(X))$ 和 $\\operatorname{Cov}(f(X), f(X^{-}))$. 的形式，推导出 $\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}})$ 的精确表达式。\n3. 推导方差比 $R := \\frac{\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}})}{\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ind}})}$，并用相关性 $\\rho := \\operatorname{Corr}(f(X), f(X^{-}))$. 表示。\n4. 假设 $f$ 是关于 $\\mu$ 的奇函数，即对于所有 $y \\in \\mathbb{R}^{d}$，都有 $f(\\mu + y) = - f(\\mu - y)$。利用多元正态分布的对称性和 $A$ 的线性性，计算此情况下的 $\\rho$ 并推导出 $R$ 的值。\n\n以 $\\rho$ 表示的 $R$ 的单个闭式解析表达式，以及上述奇函数 $f$ 情况下的 $R$ 值，作为你的最终答案。无需四舍五入，不涉及物理单位。",
            "solution": "该问题经验证是自洽的、有科学依据且定义明确的。我们进行分步推导。\n\n1. $X$ 和 $X^{-}$ 的边缘分布证明。\n\n给定随机向量 $Z \\in \\mathbb{R}^{d}$ 服从标准多元正态分布 $Z \\sim \\mathcal{N}_{d}(0, I_{d})$，其中 $I_d$ 是 $d \\times d$ 的单位矩阵。$Z$ 的概率密度函数 (PDF) 关于原点对称，即对所有 $z \\in \\mathbb{R}^{d}$ 都有 $p_{Z}(z) = p_{Z}(-z)$。这意味着随机向量 $-Z$ 与 $Z$ 具有相同的分布。\n为了正式地证明这一点，我们来求 $-Z$ 的均值和协方差。\n均值为 $\\mathbb{E}[-Z] = -\\mathbb{E}[Z] = -0 = 0$。\n协方差矩阵为 $\\operatorname{Cov}(-Z) = \\mathbb{E}[(-Z)(-Z)^{\\top}] - \\mathbb{E}[-Z]\\mathbb{E}[-Z]^{\\top} = \\mathbb{E}[ZZ^{\\top}] - 0 = \\operatorname{Cov}(Z) = I_{d}$。\n由于 $-Z$ 是多元正态向量 $Z$ 的线性变换，所以它也是多元正态的。因为它与 $Z$ 有相同的均值 ($0$) 和协方差矩阵 ($I_d$)，我们得出结论 $-Z \\sim \\mathcal{N}_{d}(0, I_{d})$。\n\n随机向量 $X$ 由仿射变换 $X := \\mu + A Z$ 定义。由于 $Z$ 是多元正态的， $X$ 也是多元正态的。其均值为：\n$$ \\mathbb{E}[X] = \\mathbb{E}[\\mu + A Z] = \\mu + A \\mathbb{E}[Z] = \\mu + A \\cdot 0 = \\mu $$\n其协方差矩阵为：\n$$ \\operatorname{Cov}(X) = \\operatorname{Cov}(\\mu + A Z) = \\operatorname{Cov}(A Z) = A \\operatorname{Cov}(Z) A^{\\top} = A I_{d} A^{\\top} = A A^{\\top} = \\Sigma $$\n因此，如问题所述，$X \\sim \\mathcal{N}_{d}(\\mu, \\Sigma)$。\n\n对偶向量 $X^{-}$ 定义为 $X^{-} := \\mu - A Z = \\mu + A(-Z)$。由于我们已经确定 $-Z$ 与 $Z$ 具有相同的分布，即 $\\mathcal{N}_{d}(0, I_{d})$，所以随机向量 $X^{-}$ 必须与 $X$ 具有相同的分布。遵循相同的推导过程：\n$$ \\mathbb{E}[X^{-}] = \\mathbb{E}[\\mu + A(-Z)] = \\mu + A \\mathbb{E}[-Z] = \\mu + A \\cdot 0 = \\mu $$\n$$ \\operatorname{Cov}(X^{-}) = \\operatorname{Cov}(\\mu + A(-Z)) = \\operatorname{Cov}(A(-Z)) = A \\operatorname{Cov}(-Z) A^{\\top} = A I_{d} A^{\\top} = A A^{\\top} = \\Sigma $$\n因此，$X^{-}$ 的边缘分布也是 $\\mathcal{N}_{d}(\\mu, \\Sigma)$，与 $X$ 的分布相同。这完成了第一部分的证明。\n\n2. $\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}})$ 的推导。\n\n对偶对估计量为 $\\hat{\\theta}_{\\mathrm{ant}} = \\frac{1}{2}\\big(f(X) + f(X^{-})\\big)$。我们使用两个随机变量之和的方差通用公式，$\\operatorname{Var}(aY_1 + bY_2) = a^2\\operatorname{Var}(Y_1) + b^2\\operatorname{Var}(Y_2) + 2ab\\operatorname{Cov}(Y_1, Y_2)$。\n此处，$Y_1 = f(X)$，$Y_2 = f(X^{-})$，且 $a = b = \\frac{1}{2}$。\n$$ \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}}) = \\operatorname{Var}\\left(\\frac{1}{2}f(X) + \\frac{1}{2}f(X^{-})\\right) = \\left(\\frac{1}{2}\\right)^2 \\operatorname{Var}(f(X)) + \\left(\\frac{1}{2}\\right)^2 \\operatorname{Var}(f(X^{-})) + 2\\left(\\frac{1}{2}\\right)\\left(\\frac{1}{2}\\right)\\operatorname{Cov}(f(X), f(X^{-})) $$\n$$ \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}}) = \\frac{1}{4}\\operatorname{Var}(f(X)) + \\frac{1}{4}\\operatorname{Var}(f(X^{-})) + \\frac{1}{2}\\operatorname{Cov}(f(X), f(X^{-})) $$\n从第 1 部分可知，$X$ 和 $X^{-}$ 是同分布的。这意味着随机变量 $f(X)$ 和 $f(X^{-})$ 也是同分布的，因此具有相同的方差：$\\operatorname{Var}(f(X)) = \\operatorname{Var}(f(X^{-}))$. \n将此代入方程，我们得到：\n$$ \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}}) = \\frac{1}{4}\\operatorname{Var}(f(X)) + \\frac{1}{4}\\operatorname{Var}(f(X)) + \\frac{1}{2}\\operatorname{Cov}(f(X), f(X^{-})) $$\n$$ \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}}) = \\frac{1}{2}\\operatorname{Var}(f(X)) + \\frac{1}{2}\\operatorname{Cov}(f(X), f(X^{-})) $$\n这就是所求的表达式。\n\n3. 方差比 $R$ 的推导。\n\n首先，我们计算独立样本估计量 $\\hat{\\theta}_{\\mathrm{ind}} = \\frac{1}{2}\\big(f(X^{(1)}) + f(X^{(2)})\\big)$ 的方差。向量 $X^{(1)}$ 和 $X^{(2)}$ 是独立同分布 (i.i.d.) 于 $\\mathcal{N}_{d}(\\mu, \\Sigma)$ 的。因此，随机变量 $f(X^{(1)})$ 和 $f(X^{(2)})$ 是独立同分布的。它们的独立性意味着它们的协方差为零：$\\operatorname{Cov}(f(X^{(1)}), f(X^{(2)})) = 0$。\n方差为：\n$$ \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ind}}) = \\operatorname{Var}\\left(\\frac{1}{2}f(X^{(1)}) + \\frac{1}{2}f(X^{(2)})\\right) = \\left(\\frac{1}{2}\\right)^2\\operatorname{Var}(f(X^{(1)})) + \\left(\\frac{1}{2}\\right)^2\\operatorname{Var}(f(X^{(2)})) + 0 $$\n由于它们同分布，$\\operatorname{Var}(f(X^{(1)})) = \\operatorname{Var}(f(X^{(2)})) = \\operatorname{Var}(f(X))$。\n$$ \\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ind}}) = \\frac{1}{4}\\operatorname{Var}(f(X)) + \\frac{1}{4}\\operatorname{Var}(f(X)) = \\frac{1}{2}\\operatorname{Var}(f(X)) $$\n方差比 $R$ 定义为 $R := \\frac{\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ant}})}{\\operatorname{Var}(\\hat{\\theta}_{\\mathrm{ind}})}$。代入我们推导出的表达式：\n$$ R = \\frac{\\frac{1}{2}\\operatorname{Var}(f(X)) + \\frac{1}{2}\\operatorname{Cov}(f(X), f(X^{-}))}{\\frac{1}{2}\\operatorname{Var}(f(X))} = 1 + \\frac{\\operatorname{Cov}(f(X), f(X^{-}))}{\\operatorname{Var}(f(X))} $$\n相关系数 $\\rho$ 定义为 $\\rho := \\operatorname{Corr}(f(X), f(X^{-})) = \\frac{\\operatorname{Cov}(f(X), f(X^{-}))}{\\sqrt{\\operatorname{Var}(f(X)) \\operatorname{Var}(f(X^{-}))}}$。\n由于 $\\operatorname{Var}(f(X)) = \\operatorname{Var}(f(X^{-}))$, 这可以简化为 $\\rho = \\frac{\\operatorname{Cov}(f(X), f(X^{-}))}{\\operatorname{Var}(f(X))}$。\n将此代入我们关于 $R$ 的表达式，我们得到：\n$$ R = 1 + \\rho $$\n\n4. 对于奇函数 $f$ 计算 $\\rho$ 和 $R$。\n\n给定 $f$ 是关于 $\\mu$ 的奇函数，即对所有 $y \\in \\mathbb{R}^{d}$ 都有 $f(\\mu + y) = -f(\\mu - y)$。\n我们利用这个性质来表示 $f(X)$ 和 $f(X^{-})$。我们有 $X = \\mu + AZ$ 和 $X^{-} = \\mu - AZ$。令 $y = AZ$。然后，我们可以写出：\n$f(X) = f(\\mu + AZ)$\n$f(X^{-}) = f(\\mu - AZ)$\n使用 $y = AZ$ 的奇函数性质，我们有 $f(\\mu + AZ) = -f(\\mu - AZ)$。\n这意味着随机变量 $f(X)$ 和 $f(X^{-})$ 之间存在一个确定性关系：\n$$ f(X) = -f(X^{-}) $$\n这个关系对 $Z$ 的每一次实现都成立。现在我们计算相关性 $\\rho = \\operatorname{Corr}(f(X), f(X^{-}))$. \n使用关系 $f(X^{-}) = -f(X)$，我们有：\n$$ \\rho = \\operatorname{Corr}(f(X), -f(X)) $$\n使用协方差和方差的性质，$\\operatorname{Cov}(U, -V) = -\\operatorname{Cov}(U, V)$ 和 $\\operatorname{Var}(-U) = \\operatorname{Var}(U)$：\n$$ \\rho = \\frac{\\operatorname{Cov}(f(X), -f(X))}{\\sqrt{\\operatorname{Var}(f(X))\\operatorname{Var}(-f(X))}} = \\frac{-\\operatorname{Cov}(f(X), f(X))}{\\sqrt{\\operatorname{Var}(f(X))\\operatorname{Var}(f(X))}} = \\frac{-\\operatorname{Var}(f(X))}{\\operatorname{Var}(f(X))} $$\n假设 $\\operatorname{Var}(f(X))  0$（否则，$f(X)$ 是一个常数，问题变得微不足道），我们得到：\n$$ \\rho = -1 $$\n实现了完全负相关。\n最后，我们可以推导出方差比 $R$ 的值：\n$$ R = 1 + \\rho = 1 + (-1) = 0 $$\n这个结果表明，对于一个关于 $\\mu$ 的奇函数，对偶变量技术产生一个零方差估计量，这是可能的最大方差缩减。估计量 $\\hat{\\theta}_{\\mathrm{ant}} = \\frac{1}{2}(f(X)+f(X^-)) = \\frac{1}{2}(f(X)-f(X)) = 0$，所以其方差确实为 $0$。\n\n最终答案结合了以 $\\rho$ 表示的 $R$ 的表达式和在 $f$ 为奇函数情况下 $R$ 的值。",
            "answer": "$$ \\boxed{ \\begin{pmatrix} 1 + \\rho  0 \\end{pmatrix} } $$"
        },
        {
            "introduction": "理论上的增益最好通过在实际问题中的应用来理解。这项练习将理论付诸实践，要求你为一个高维贝叶斯线性回归模型构建一个完整的模拟流程，这是机器学习和统计学中的一项常见任务。你将亲手实现并比较朴素的独立同分布抽样、前一个练习中探讨的对偶变量法，以及更高级的拟蒙特卡洛方法，直观地观察它们在估计后验矩时的收敛性质差异 。",
            "id": "3322642",
            "problem": "构建一个完整的程序，针对多个使用高斯先验的高维贝叶斯线性回归实例，比较三种用于在多变量正态后验分布下近似预测矩的蒙特卡洛策略：独立同分布采样、对偶配对采样，以及通过Sobol序列转换为多变量正态分布的拟蒙特卡洛采样。目标是展示在拟多变量正态采样下预测矩的收敛性有所改善，并量化对偶配对对于线性泛函的方差缩减特性。\n\n您必须基于以下基本原理进行工作：\n- 线性高斯模型由 $y = X w + \\varepsilon$ 给出，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ 且先验为 $w \\sim \\mathcal{N}(0, \\tau^2 I_d)$。\n- 对于高斯似然和高斯先验，后验分布 $p(w \\mid y, X)$ 是高斯分布。其精度矩阵是先验精度和数据精度之和。避免任何显式的矩阵求逆；仅依赖线性代数恒等式和分解（例如，正定精度矩阵的Cholesky分解）以及三角求解。\n- 如果 $A$ 是一个对称正定矩阵，且 $A = R^\\top R$ 是其Cholesky分解，其中 $R$ 是上三角矩阵，那么采样 $z \\sim \\mathcal{N}(0, I_d)$ 并设置 $u = R^{-1} z$ 会得到 $\\operatorname{Cov}(u) = A^{-1}$。对于协方差为 $A^{-1}$、均值为 $\\mu$ 的多变量正态采样，可以使用 $w = \\mu + u$。\n- 对偶配对利用成对的参考抽样 $z$ 和 $-z$ 来处理对称分布，以减少奇函数的被积函数的估计量方差。\n- 拟蒙特卡洛采样将一个低差异序列 $u \\in (0,1)^d$ 通过逆累积分布函数映射到一个标准正态向量 $z$，然后通过线性变换映射到目标多变量正态分布。\n\n程序要求：\n1) 对于下面的每个测试用例，按如下方式生成数据，使用指定的随机数生成器种子（所有种子均为非负整数，可用于任何语言的伪随机数生成器）。所有向量和矩阵都应为实值。\n   - 抽取设计矩阵 $X \\in \\mathbb{R}^{n \\times d}$，其行独立同分布于 $\\mathcal{N}(0, \\Sigma_X)$，其中对于所有 $i, j \\in \\{1,\\dots,d\\}$ 和 $|\\rho|  1$，有 $(\\Sigma_X)_{ij} = \\rho^{|i-j|}$。使用 $\\Sigma_X$ 的Cholesky分解来生成这些行，而不依赖于通用的多变量正态例程。生成 $X$ 时，使用种子 $s_X$。\n   - 使用种子 $s_w$ 抽取真实系数向量 $w_{\\text{true}} \\sim \\mathcal{N}(0, \\tau^2 I_d)$，并使用种子 $s_\\varepsilon$ 抽取噪声 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$。然后设置 $y = X w_{\\text{true}} + \\varepsilon$。\n   - 使用相同的 $\\Sigma_X$ 和种子 $s_*$ 抽取一个预测协变量 $x_* \\sim \\mathcal{N}(0, \\Sigma_X)$。\n2) 使用高斯模型的线性代数恒等式计算后验精度矩阵 $A$ 和后验均值 $\\mu$。通过Cholesky分解将 $A$ 分解为 $A = R^\\top R$，其中 $R$ 是上三角矩阵。不要显式地对任何矩阵求逆。\n3) 计算精确的预测均值 $\\mu_* = \\mathbb{E}[x_*^\\top w \\mid y, X]$ 和精确的预测方差 $v_* = \\operatorname{Var}(x_*^\\top w \\mid y, X) + \\sigma^2$，通过用 $A$、$\\mu$ 和 $x_*$ 来表示它们，再次使用线性求解而无需显式矩阵求逆。\n4) 实现三种采样估计器来近似 $y_*$ 的预测均值和方差：\n   - 独立同分布 (IID) 采样器：抽取 $N$ 个独立的 $z \\sim \\mathcal{N}(0, I_d)$ 并映射到 $w = \\mu + R^{-1} z$。对于IID采样器，使用种子 $s_{\\text{iid}}$。\n   - 对偶采样器：抽取 $N/2$ 个独立的 $z \\sim \\mathcal{N}(0, I_d)$ 并同时包含 $z$ 和 $-z$，将每个映射到 $w = \\mu + R^{-1} z$。使用种子 $s_{\\text{anti}}$。假设所有考虑的样本大小 $N$ 均为偶数。\n   - 拟蒙特卡洛 (QMC) 采样器：使用种子 $s_{\\text{qmc}}$ 通过Owen加扰在 $[0,1)^d$ 中创建一个Sobol序列，通过逆标准正态累积分布函数逐分量变换以获得 $z \\in \\mathbb{R}^d$，然后映射到 $w = \\mu + R^{-1} z$。使用2的幂次方的样本大小，并对每个 $N$ 使用前 $N$ 个Sobol点。\n5) 对于每个采样器和样本大小集合中的每个 $N$，近似：\n   - 预测均值，通过 $x_*^\\top w$ 的样本均值来近似。\n   - 预测方差，通过 $x_*^\\top w$ 的样本方差（使用总体归一化）加上 $\\sigma^2$ 来近似。\n6) 对于每个采样器，累积在样本大小 $N \\in \\{128, 512, 2048, 8192\\}$ 上的积分绝对误差：\n   - 对于预测均值：$E_{\\text{mean}} = \\sum_{N} \\left| \\hat{\\mu}_*(N) - \\mu_* \\right|$。\n   - 对于预测方差：$E_{\\text{var}} = \\sum_{N} \\left| \\hat{v}_*(N) - v_* \\right|$。\n7) 对于每个测试用例，报告三个布尔值结果：\n   - 对偶配对是否相对于IID改善了预测均值的收敛性：$E_{\\text{mean}}^{\\text{anti}}  E_{\\text{mean}}^{\\text{iid}}$。\n   - 拟蒙特卡洛是否相对于IID改善了预测均值的收敛性：$E_{\\text{mean}}^{\\text{qmc}}  E_{\\text{mean}}^{\\text{iid}}$。\n   - 拟蒙特卡洛是否相对于IID改善了预测方差的收敛性：$E_{\\text{var}}^{\\text{qmc}}  E_{\\text{var}}^{\\text{iid}}$。\n\n测试套件：\n- 用例 A (高维，中度病态)：\n  - $n = 60$, $d = 50$, $\\tau = 1.0$, $\\sigma = 0.5$, $\\rho = 0.7$,\n  - $s_X = 1729$, $s_w = 2718$, $s_\\varepsilon = 31415$, $s_* = 4242$,\n  - $s_{\\text{iid}} = 7771$, $s_{\\text{anti}} = 7772$, $s_{\\text{qmc}} = 12345$。\n- 用例 B (欠定，$d \\gg n$)：\n  - $n = 30$, $d = 100$, $\\tau = 1.5$, $\\sigma = 1.0$, $\\rho = 0.5$,\n  - $s_X = 2021$, $s_w = 1618$, $s_\\varepsilon = 1414$, $s_* = 1732$,\n  - $s_{\\text{iid}} = 8881$, $s_{\\text{anti}} = 8882$, $s_{\\text{qmc}} = 23456$。\n- 用例 C (近共線性)：\n  - $n = 80$, $d = 80$, $\\tau = 0.8$, $\\sigma = 0.3$, $\\rho = 0.95$,\n  - $s_X = 271828$, $s_w = 161803$, $s_\\varepsilon = 141421$, $s_* = 173205$,\n  - $s_{\\text{iid}} = 9991$, $s_{\\text{anti}} = 9992$, $s_{\\text{qmc}} = 34567$。\n\n实现说明：\n- 所有线性代数必须避免显式矩阵求逆。仅使用分解和三角求解。\n- 基于Sobol的拟蒙特卡洛必须使用带有所提供种子的加扰Sobol生成器，并使用2的幂次方的样本大小 $N \\in \\{128, 512, 2048, 8192\\}$；对每个 $N$ 使用前 $N$ 个点。通过逆累积分布函数逐分量映射到标准正态分布。\n- 将用于逆累积分布函数映射的任何均匀值裁剪到闭区间 $[10^{-12}, 1-10^{-12}]$ 以避免无穷大。\n- 不会出现角度；物理单位不适用。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个用例的结果，格式为逗号分隔的方括号三元组列表，无空格，例如：\"[[True,True,True],[True,True,True],[True,True,True]]\"。每个内部三元组对应一个用例，并按 $[\\text{对偶改善均值}, \\text{拟蒙特卡洛改善均值}, \\text{拟蒙特卡洛改善方差}]$ 的顺序排列。",
            "solution": "该问题要求对三种蒙特卡洛方法在高维贝叶斯线性回归模型中近似预测矩的性能进行比较分析。这些方法是独立同分布（IID）采样、对偶采样和拟蒙特卡洛（QMC）采样。解决方案涉及推导后验分布、计算精确的预测矩、实现三种采样器，并基于积分绝对误差评估它们的性能。\n\n### 1. 贝叶斯线性回归模型和后验分布\n\n指定的模型是设计矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 与响应向量 $y \\in \\mathbb{R}^n$ 之间的线性关系，并带有高斯噪声。\n数据的似然由 $p(y \\mid w, \\sigma^2) = \\mathcal{N}(y \\mid Xw, \\sigma^2 I_n)$ 给出，其中 $w \\in \\mathbb{R}^d$ 是回归系数向量，$\\sigma^2$ 是噪声方差。似然函数正比于：\n$$\np(y \\mid w) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2} (y - Xw)^\\top (y - Xw)\\right)\n$$\n对权重施加高斯先验，$p(w \\mid \\tau^2) = \\mathcal{N}(w \\mid 0, \\tau^2 I_d)$，其中 $\\tau^2$ 是先验方差。先验正比于：\n$$\np(w) \\propto \\exp\\left(-\\frac{1}{2\\tau^2} w^\\top w\\right)\n$$\n根据贝叶斯定理，后验分布 $p(w \\mid y, X)$ 正比于似然和先验的乘积，$p(w \\mid y, X) \\propto p(y \\mid w) p(w)$。后验的指数部分是似然和先验的指数部分之和：\n$$\n-\\frac{1}{2\\sigma^2} (y^\\top y - 2y^\\top Xw + w^\\top X^\\top X w) - \\frac{1}{2\\tau^2} w^\\top w\n$$\n对 $w$ 合并同类项，我们得到：\n$$\n-\\frac{1}{2} \\left( w^\\top \\left(\\frac{1}{\\sigma^2} X^\\top X + \\frac{1}{\\tau^2} I_d\\right) w - 2 \\left(\\frac{1}{\\sigma^2} y^\\top X\\right) w \\right) + \\text{const}\n$$\n这是一个多变量高斯分布的指数部分，证实了后验是高斯分布，$p(w \\mid y, X) = \\mathcal{N}(w \\mid \\mu, A^{-1})$。通过“配方法”或与一般高斯形式 $\\exp(-\\frac{1}{2}(w-\\mu)^\\top A(w-\\mu))$ 比较，我们确定后验精度矩阵 $A$ 和后验均值 $\\mu$：\n后验精度矩阵为：\n$$\nA = \\frac{1}{\\sigma^2} X^\\top X + \\frac{1}{\\tau^2} I_d\n$$\n后验均值 $\\mu$ 是以下线性系统的解：\n$$\nA\\mu = \\frac{1}{\\sigma^2} X^\\top y\n$$\n为了在不显式求逆矩阵的情况下计算 $\\mu$，我们首先计算对称正定矩阵 $A$ 的Cholesky分解，使得 $A = R^\\top R$，其中 $R$ 是上三角矩阵。然后，我们通过两次三角求解来解 $R^\\top R \\mu = \\frac{1}{\\sigma^2} X^\\top y$：首先解 $R^\\top v = \\frac{1}{\\sigma^2} X^\\top y$ 得到 $v$（前向替换），然后解 $R\\mu = v$ 得到 $\\mu$（后向替换）。\n\n### 2. 精确预测矩\n\n对于一个新的协变量向量 $x_* \\in \\mathbb{R}^d$，预测响应为 $y_* = x_*^\\top w + \\varepsilon_*$，其中 $w$ 从后验分布中抽取。我们感兴趣的预测量是 $x_*^\\top w$。由于 $w$ 是高斯分布的， $x_*^\\top w$ 是一个一元高斯分布。\n\n$x_*^\\top w$ 的精确预测均值为：\n$$\n\\mu_* = \\mathbb{E}[x_*^\\top w \\mid y, X] = x_*^\\top \\mathbb{E}[w \\mid y, X] = x_*^\\top \\mu\n$$\n$y_*$ 的精确预测方差包括 $w$ 中的不确定性和观测噪声 $\\sigma^2$。项 $x_*^\\top w$ 的方差由下式给出：\n$$\n\\operatorname{Var}(x_*^\\top w \\mid y, X) = x_*^\\top \\operatorname{Cov}(w) x_* = x_*^\\top A^{-1} x_*\n$$\n总预测方差 $v_*$ 为：\n$$\nv_* = \\operatorname{Var}(x_*^\\top w \\mid y, X) + \\sigma^2 = x_*^\\top A^{-1} x_* + \\sigma^2\n$$\n为了在不求逆 $A$ 的情况下计算 $x_*^\\top A^{-1} x_*$，我们求解线性系统 $Av = x_*$ 得到 $v$。然后，方差项为 $x_*^\\top v$。这个线性系统像之前一样使用Cholesky因子 $R$ 求解：解 $R^\\top u = x_*$ 得到 $u$，然后解 $Rv = u$ 得到 $v$。\n\n### 3. 蒙特卡洛估计策略\n\n问题的核心是使用来自后验 $p(w \\mid y, X) = \\mathcal{N}(\\mu, A^{-1})$ 的样本来近似 $\\mu_*$ 和 $v_*$。为了从这个分布中抽取一个样本 $w$，我们首先抽取一个标准正态向量 $z \\sim \\mathcal{N}(0, I_d)$ 然后应用一个仿射变换：\n$$\nw = \\mu + R^{-1} z\n$$\n这是有效的，因为如果 $z \\sim \\mathcal{N}(0, I_d)$，那么 $u = R^{-1} z$ 的协方差为 $\\operatorname{Cov}(u) = R^{-1} \\operatorname{Cov}(z) (R^{-1})^\\top = R^{-1} (R^\\top)^{-1} = (R^\\top R)^{-1} = A^{-1}$。项 $R^{-1}z$ 是通过后向替换求解 $R u = z$ 得到 $u$ 来计算的。三种采样方法的不同之处在于它们如何生成 $z$ 向量序列。\n\n**独立同分布（IID）采样：** 这是基准方法。从 $\\mathcal{N}(0, I_d)$ 中独立抽取一组 $N$ 个向量 $\\{z_i\\}_{i=1}^N$，从而得到 $N$ 个后验样本 $\\{w_i\\}_{i=1}^N$。蒙特卡洛积分的近似误差通常以 $O(N^{-1/2})$ 的速率减小。\n\n**对偶采样：** 这种方差缩减技术利用了被积函数的对称性。对于像 $\\mathcal{N}(0, I_d)$ 这样的对称分布，如果我们抽取一个样本 $z$，我们也会包含它的对偶对 $-z$。对于一个奇函数的被积函数 $f(z)$，这对样本的平均值 $[f(z) + f(-z)]/2 = 0$，完美地消除了变异。预测均值的目标是 $x_*^\\top w = x_*^\\top \\mu + x_*^\\top R^{-1} z$。项 $x_*^\\top R^{-1} z$ 是 $z$ 的一个奇函数。此项的每个对偶对的平均值为 $0$，因此 $\\mathbb{E}[x_*^\\top R^{-1} z]$ 的估计量恰好是 $0$，而 $\\mathbb{E}[x_*^\\top w]$ 的估计量恰好变成 $x_*^\\top \\mu$。这为均值估计提供了巨大的方差缩减。对于偶函数的被积函数，比如用于方差的那个，对偶采样没有任何好处。\n\n**拟蒙特卡洛（QMC）采样：** QMC方法用确定性的、低差异度的序列（如Sobol序列）取代随机样本。这些点比伪随机点更均匀地填充样本空间。Sobol序列在单位超立方体 $[0,1)^d$ 中生成点 $u_i$。为了获得标准正态样本 $z_i$， $u_i$ 的每个分量都通过标准正态累积分布函数（CDF）（也称为probit函数）的逆函数进行变换。对于行为良好的被积函数，QMC方法可以实现更快的收敛速度，接近 $O(N^{-1})$，显著优于IID采样。对Sobol序列进行加扰通过打破确定性模式同时保持低差异度，进一步改善了其属性。\n\n### 4. 算法与实现\n\n对于每个测试用例，程序将执行以下步骤：\n1.  **数据生成：**\n    -   构建 $d \\times d$ 协方差矩阵 $\\Sigma_X$，其元素为 $(\\Sigma_X)_{ij} = \\rho^{|i-j|}$。\n    -   计算 $\\Sigma_X$ 的下Cholesky因子 $L_X$，使得 $\\Sigma_X = L_X L_X^\\top$。\n    -   使用指定的种子 $s_X$，生成 $n$ 个向量 $z_{X,i} \\sim \\mathcal{N}(0, I_d)$，并将 $X$ 的行构造成 $x_i^\\top = (L_X z_{X,i})^\\top$。\n    -   使用各自的种子和参数类似地生成 $w_{\\text{true}}$、$\\varepsilon$ 和 $x_*$。\n    -   计算观测数据 $y = X w_{\\text{true}} + \\varepsilon$。\n\n2.  **后验和精确矩的计算：**\n    -   计算后验精度 $A = (1/\\sigma^2) X^\\top X + (1/\\tau^2) I_d$。\n    -   计算 $A$ 的上Cholesky因子 $R$，使得 $A = R^\\top R$。\n    -   计算右侧项 $b = (1/\\sigma^2) X^\\top y$。\n    -   通过两次三角求解从 $R^\\top R \\mu = b$ 解出后验均值 $\\mu$。\n    -   计算精确的预测均值 $\\mu_* = x_*^\\top \\mu$。\n    -   通过两次三角求解从 $R^\\top R v = x_*$ 解出 $v$。\n    -   计算精确的预测方差 $v_* = x_*^\\top v + \\sigma^2$。\n\n3.  **蒙特卡洛估计和误差累积：**\n    -   对于每个采样器（IID、对偶、QMC）和每个样本大小 $N \\in \\{128, 512, 2048, 8192\\}$：\n        -   根据采样器的逻辑，使用其指定的种子生成 $N$ 个标准正态向量 $\\{z_i\\}$。对于QMC，这涉及生成一个加扰的Sobol序列并应用逆正态CDF。\n        -   对于每个 $z_i$，解 $R u_i = z_i$ 得到 $u_i$，并构成后验样本 $w_i = \\mu + u_i$。\n        -   计算样本预测值 $p_i = x_*^\\top w_i$。\n        -   将均值 $\\hat{\\mu}_*(N)$ 估计为 $\\{p_i\\}$ 的样本均值。\n        -   将方差 $\\hat{v}_*(N)$ 估计为 $\\{p_i\\}$ 的样本方差（使用 $1/N$ 归一化）加上 $\\sigma^2$。\n        -   计算绝对误差 $|\\hat{\\mu}_*(N) - \\mu_*|$ 和 $|\\hat{v}_*(N) - v_*|$。\n    -   将这些绝对误差在所有 $N$ 上求和，得到每个采样器的积分误差 $E_{\\text{mean}}$ 和 $E_{\\text{var}}$。\n\n4.  **最终比较：**\n    -   对于每个测试用例，执行问题描述中指定的三个布尔比较，并存储结果三元组。\n\n5.  **输出：**\n    -   收集所有测试用例的三元组，并以要求的格式打印它们：`[[bool,bool,bool],[bool,bool,bool],[...]]`。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\nfrom scipy.stats import norm\nfrom scipy.stats.qmc import Sobol\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases = [\n        # Case A: high-dimensional, moderately ill-conditioned\n        {'n': 60, 'd': 50, 'tau': 1.0, 'sigma': 0.5, 'rho': 0.7,\n         's_X': 1729, 's_w': 2718, 's_epsilon': 31415, 's_star': 4242,\n         's_iid': 7771, 's_anti': 7772, 's_qmc': 12345},\n        # Case B: underdetermined, d > n\n        {'n': 30, 'd': 100, 'tau': 1.5, 'sigma': 1.0, 'rho': 0.5,\n         's_X': 2021, 's_w': 1618, 's_epsilon': 1414, 's_star': 1732,\n         's_iid': 8881, 's_anti': 8882, 's_qmc': 23456},\n        # Case C: near-collinearity\n        {'n': 80, 'd': 80, 'tau': 0.8, 'sigma': 0.3, 'rho': 0.95,\n         's_X': 271828, 's_w': 161803, 's_epsilon': 141421, 's_star': 173205,\n         's_iid': 9991, 's_anti': 9992, 's_qmc': 34567},\n    ]\n\n    sample_sizes = [128, 512, 2048, 8192]\n    all_results = []\n    for case_params in test_cases:\n        results = run_case(case_params, sample_sizes)\n        all_results.append(results)\n\n    print(str(all_results).replace(\" \", \"\"))\n\ndef run_case(params, sample_sizes):\n    \"\"\"\n    Executes the entire simulation for a single test case.\n    \"\"\"\n    n, d, tau, sigma, rho = params['n'], params['d'], params['tau'], params['sigma'], params['rho']\n    s_X, s_w, s_epsilon, s_star = params['s_X'], params['s_w'], params['s_epsilon'], params['s_star']\n    s_iid, s_anti, s_qmc = params['s_iid'], params['s_anti'], params['s_qmc']\n\n    # 1. Data Generation\n    # Generate covariance matrix Sigma_X and its Cholesky factor\n    indices = np.arange(d)\n    Sigma_X = rho ** np.abs(indices[:, np.newaxis] - indices)\n    L_X = np.linalg.cholesky(Sigma_X)\n\n    # Generate X\n    rng_X = np.random.default_rng(s_X)\n    Z_X = rng_X.standard_normal((n, d))\n    X = Z_X @ L_X.T\n\n    # Generate w_true\n    rng_w = np.random.default_rng(s_w)\n    w_true = tau * rng_w.standard_normal(d)\n\n    # Generate epsilon\n    rng_eps = np.random.default_rng(s_epsilon)\n    epsilon = sigma * rng_eps.standard_normal(n)\n\n    # Generate y\n    y = X @ w_true + epsilon\n\n    # Generate x_star\n    rng_star = np.random.default_rng(s_star)\n    x_star = L_X @ rng_star.standard_normal(d)\n\n    # 2. Posterior and Exact Moments Calculation\n    # Posterior precision matrix A\n    A = (1 / sigma**2) * (X.T @ X) + (1 / tau**2) * np.identity(d)\n    \n    # Cholesky factorization of A (upper triangular)\n    R = cholesky(A, lower=False)\n\n    # Posterior mean mu\n    b = (1 / sigma**2) * (X.T @ y)\n    v = solve_triangular(R, b, trans='T', lower=False)\n    mu = solve_triangular(R, v, trans='N', lower=False)\n\n    # Exact predictive mean mu_star\n    mu_star = x_star @ mu\n\n    # Exact predictive variance v_star\n    v_solve = solve_triangular(R, x_star, trans='T', lower=False)\n    u_solve = solve_triangular(R, v_solve, trans='N', lower=False)\n    var_w_term = x_star @ u_solve\n    v_star = var_w_term + sigma**2\n\n    # 3. Monte Carlo Estimation\n    errors = {}\n    samplers = {\n        'iid': lambda N, seed: np.random.default_rng(seed).standard_normal((N, d)),\n        'anti': lambda N, seed: _generate_antithetic(N, d, seed),\n        'qmc': lambda N, seed: _generate_qmc(N, d, seed)\n    }\n    sampler_seeds = {'iid': s_iid, 'anti': s_anti, 'qmc': s_qmc}\n\n    for name, sampler_func in samplers.items():\n        E_mean, E_var = 0.0, 0.0\n        for N in sample_sizes:\n            z_samples = sampler_func(N, sampler_seeds[name])\n\n            # Transform standard normal samples to posterior samples\n            # u = R^-1 * z\n            u_samples = solve_triangular(R, z_samples.T, lower=False).T\n            w_samples = mu + u_samples\n            \n            # Predictions\n            p_samples = w_samples @ x_star\n\n            # Estimated moments\n            mu_hat = np.mean(p_samples)\n            v_hat = np.var(p_samples, ddof=0) + sigma**2\n\n            # Accumulate errors\n            E_mean += np.abs(mu_hat - mu_star)\n            E_var += np.abs(v_hat - v_star)\n        \n        errors[name] = {'mean': E_mean, 'var': E_var}\n\n    # 4. Final Comparison\n    anti_improves_mean = errors['anti']['mean']  errors['iid']['mean']\n    qmc_improves_mean = errors['qmc']['mean']  errors['iid']['mean']\n    qmc_improves_var = errors['qmc']['var']  errors['iid']['var']\n\n    return [anti_improves_mean, qmc_improves_mean, qmc_improves_var]\n\ndef _generate_antithetic(N, d, seed):\n    \"\"\"Generates N antithetic standard normal samples.\"\"\"\n    rng = np.random.default_rng(seed)\n    half_N = N // 2\n    z_half = rng.standard_normal((half_N, d))\n    return np.vstack((z_half, -z_half))\n\ndef _generate_qmc(N, d, seed):\n    \"\"\"Generates N QMC standard normal samples.\"\"\"\n    sobol_engine = Sobol(d=d, scramble=True, seed=seed)\n    u_samples = sobol_engine.random(n=N)\n    # Clip to avoid infinity from ppf\n    u_samples = np.clip(u_samples, 1e-12, 1 - 1e-12)\n    return norm.ppf(u_samples)\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "在掌握了高效的抽样方法后，我们必须关注另一个核心问题：正确性。随着模拟变得复杂，尤其是在并行计算环境中，对随机数流的不当处理可能引入难以察觉的错误，从而使整个模拟结果失效。这最后一个练习为你提供了一套至关重要的诊断工具，通过一系列统计检验来识别这些隐藏的依赖关系，确保你的抽样器真正符合设计预期，这是任何严谨的模拟实践者都必须具备的技能 。",
            "id": "3322614",
            "problem": "您的任务是在随机模拟和蒙特卡洛方法领域，诊断因并行实现中复用或共享随机数流而导致的多元正态分布样本坐标间的非预期依赖性。目标是提出、论证并实现一组能够检测不属于预期目标协方差结构中的坐标间依赖性的统计检验。您的最终程序必须实现这些检验，将它们应用于指定的测试场景套件，并输出包含结果的单行文本。\n\n从基本定义和广为接受的事实出发：\n- 一个随机向量 $\\mathbf{X} \\in \\mathbb{R}^p$ 若其分量的任意线性组合都服从单变量正态分布，则称其服从均值为 $\\boldsymbol{\\mu} \\in \\mathbb{R}^p$、协方差矩阵为 $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{p \\times p}$ 的多元正态分布，记为 $\\mathbf{X} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$。\n- 如果 $\\boldsymbol{\\Sigma}$ 是正定矩阵，则存在一个下三角Cholesky因子 $\\mathbf{L}$，使得 $\\boldsymbol{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top$。\n- 如果 $\\mathbf{Z} \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$，则 $\\boldsymbol{\\mu} + \\mathbf{L}\\mathbf{Z} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$。\n- 如果样本 $\\{\\mathbf{X}_k\\}_{k=1}^n$ 是从已知目标 $\\boldsymbol{\\Sigma}_0$ 的 $\\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}_0)$ 分布中生成的，那么白化后的样本 $\\mathbf{Y}_k = \\mathbf{L}_0^{-1}(\\mathbf{X}_k - \\boldsymbol{\\mu})$（其中 $\\boldsymbol{\\Sigma}_0 = \\mathbf{L}_0\\mathbf{L}_0^\\top$），在不存在超出 $\\boldsymbol{\\Sigma}_0$ 的非预期依赖性的情况下，应为独立同分布且 $\\mathbf{Y}_k \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$。\n- 在多元正态性假设下，样本协方差矩阵服从Wishart分布，其相关性结构可以使用已有的检验方法来评估是否满足球形性（单位相关性）。\n\n设计并论证一组检验，当应用于白化样本 $\\{\\mathbf{Y}_k\\}$ 时，能够诊断坐标间的非预期依赖性：\n1. 一项关于 $\\{\\mathbf{Y}_k\\}$ 相关矩阵球形性的全局检验，其原假设为真实相关性是 $\\mathbf{I}_p$。具体来说，使用Bartlett球形性检验，评估样本相关矩阵与 $\\mathbf{I}_p$ 的偏差是否超过了在 $\\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$ 假设下的预期。\n2. 一系列关于 $\\{\\mathbf{Y}_k\\}$ 坐标对之间零相关的成对检验，通过Bonferroni校正来控制族系误差率并进行汇总。每个成对检验应评估样本相关系数是否为零。\n\n您的程序必须：\n- 使用指定的采样器为每个测试用例生成样本。\n- 使用已知目标协方差 $\\boldsymbol{\\Sigma}_0$ 对样本进行白化，将问题转化为在 $\\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$ 下检验独立性。\n- 计算Bartlett球形性检验统计量及其近似的卡方 $p$ 值，自由度为 $p(p-1)/2$。\n- 使用学生t分布计算所有成对相关性检验，检验原假设为相关系数为零，并对所有 $p(p-1)/2$ 个配对应用Bonferroni校正来控制族系误差率。\n- 如果Bartlett球形性检验在显著性水平 $\\alpha$ 下拒绝原假设，或者任何经过Bonferroni校正的成对检验在显著性水平 $\\alpha$ 下拒绝原假设，则声明检测到非预期依赖性。\n\n使用显著性水平 $\\alpha = 0.01$。\n\n测试套件：\n为保证可复现性，请使用指定的种子。每个测试用例是一个元组，指定了采样器类型、样本量 $n$、维度 $p$、目标协方差 $\\boldsymbol{\\Sigma}_0$、共享流参数 $\\rho$（如果适用）以及随机种子。采样器定义如下：\n- \"correct\"：对每个样本，独立抽取 $\\mathbf{Z}_k \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$，并设置 $\\mathbf{X}_k = \\boldsymbol{\\mu} + \\mathbf{L}_0 \\mathbf{Z}_k$。\n- \"shared\"：对每个样本，抽取 $U_k \\sim \\mathcal{N}(0,1)$ 并设置 $\\mathbf{Z}_k = U_k \\mathbf{1}_p$，因此所有坐标共享相同的基础标准正态随机数，然后设置 $\\mathbf{X}_k = \\boldsymbol{\\mu} + \\mathbf{L}_0 \\mathbf{Z}_k$。\n- \"partial\"：对每个样本，独立抽取 $U_k \\sim \\mathcal{N}(0,1)$ 和 $\\mathbf{V}_k \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$，并设置 $\\mathbf{Z}_k = \\sqrt{\\rho}\\,U_k \\mathbf{1}_p + \\sqrt{1-\\rho}\\,\\mathbf{V}_k$，然后设置 $\\mathbf{X}_k = \\boldsymbol{\\mu} + \\mathbf{L}_0 \\mathbf{Z}_k$。\n\n所有情况下均设 $\\boldsymbol{\\mu} = \\mathbf{0}$。将目标协方差 $\\boldsymbol{\\Sigma}_0$ 定义为：\n- 用例1 (\"correct\")：$\\boldsymbol{\\Sigma}_0$ 是参数为 $\\phi = 0.6$ 的1阶自回归结构，即 $(\\boldsymbol{\\Sigma}_0)_{ij} = \\phi^{|i-j|}$，维度 $p = 6$，样本量 $n = 1000$，种子 $12345$。\n- 用例2 (\"shared\")：$\\boldsymbol{\\Sigma}_0$ 是与用例1相同的自回归协方差，$\\phi = 0.6$，维度 $p = 6$，样本量 $n = 300$，种子 $54321$。\n- 用例3 (\"partial\")：$\\boldsymbol{\\Sigma}_0$ 是对角矩阵，对角线元素为 $(1.0, 1.5, 0.5, 2.0, 1.2, 0.8, 1.7, 1.1)$，维度 $p = 8$，共享流参数 $\\rho = 0.3$，样本量 $n = 100$，种子 $2024$。\n\n答案规格：\n- 对每个测试用例，您的程序必须输出一个布尔值，指示是否检测到非预期依赖性（若检测到则为 $\\text{True}$，否则为 $\\text{False}$），使用上述在显著性水平 $\\alpha = 0.01$ 下的决策规则。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的、按用例1、用例2、用例3顺序排列的、逗号分隔的结果列表。例如，输出形式可能为 $[\\text{False},\\text{True},\\text{True}]$。\n\n此问题不涉及物理单位或角度单位。显著性水平应表示为小数（例如，$0.01$）。程序的实现应完全确定性，基于提供的种子，且不需要用户输入。",
            "solution": "我们从多元正态分布及其性质的基本定义开始。一个随机向量 $\\mathbf{X} \\in \\mathbb{R}^p$ 若其分量的任意线性组合都服从单变量正态分布，则称其服从均值 $\\boldsymbol{\\mu}$、协方差 $\\boldsymbol{\\Sigma}$ 的多元正态分布，记为 $\\mathbf{X} \\sim \\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$。如果 $\\boldsymbol{\\Sigma}$ 是正定矩阵，我们可以写成 $\\boldsymbol{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top$，其中 $\\mathbf{L}$ 是一个下三角Cholesky因子。如果 $\\mathbf{Z} \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$，那么 $\\boldsymbol{\\mu} + \\mathbf{L}\\mathbf{Z}$ 的分布为 $\\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$。\n\n当从 $\\mathcal{N}_p(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}_0)$ 采样时，如果我们应用白化变换 $\\mathbf{Y}_k = \\mathbf{L}_0^{-1}(\\mathbf{X}_k - \\boldsymbol{\\mu})$（其中 $\\boldsymbol{\\Sigma}_0 = \\mathbf{L}_0\\mathbf{L}_0^\\top$），那么在正确实现的情况下，我们应该得到独立同分布的 $\\mathbf{Y}_k \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$。在并行蒙特卡洛实现中，跨坐标的非预期随机数流共享或复用可能产生其分量依赖性超出 $\\boldsymbol{\\Sigma}_0$ 所隐含的预期结构的 $\\mathbf{Z}_k$ 向量。白化后，这种非预期依赖性表现为 $\\mathbf{Y}_k$ 坐标间独立性的偏离。\n\n现在我们设计并论证用于检测此类偏离的检验，这些检验基于正态理论和协方差性质。\n\n步骤1：白化变换。\n给定样本 $\\{\\mathbf{X}_k\\}_{k=1}^n$、已知目标协方差 $\\boldsymbol{\\Sigma}_0$ 和均值 $\\boldsymbol{\\mu}$，计算Cholesky因子 $\\mathbf{L}_0$ 使得 $\\boldsymbol{\\Sigma}_0 = \\mathbf{L}_0\\mathbf{L}_0^\\top$，并设置 $\\mathbf{Y}_k = \\mathbf{L}_0^{-1}(\\mathbf{X}_k - \\boldsymbol{\\mu})$。在正确采样的原假设下，$\\mathbf{Y}_k \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$ 独立同分布。\n\n步骤2：全局球形性检验（Bartlett）。\n在 $\\mathbf{Y}_k \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$ 的假设下，真实相关矩阵为 $\\mathbf{I}_p$。令 $\\hat{\\mathbf{R}}$ 表示 $\\{\\mathbf{Y}_k\\}$ 的样本相关矩阵。Bartlett球形性检验评估\n$$\nH_0: \\ \\mathbf{R} = \\mathbf{I}_p \\quad \\text{versus} \\quad H_1: \\ \\mathbf{R} \\neq \\mathbf{I}_p.\n$$\n检验统计量为\n$$\n\\chi^2_{\\text{Bart}} = -\\left(n - 1 - \\frac{2p + 5}{6}\\right)\\ln\\det(\\hat{\\mathbf{R}}),\n$$\n在 $H_0$ 和多元正态性假设下，该统计量近似服从自由度为 $p(p-1)/2$ 的卡方分布。较大的 $\\chi^2_{\\text{Bart}}$ 值表示偏离了球形性。如果 $\\det(\\hat{\\mathbf{R}}) \\leq 0$（这可能在 $\\hat{\\mathbf{R}}$ 近似奇异时发生），我们将 $\\chi^2_{\\text{Bart}}$ 解释为 $+\\infty$ 并以 $p$ 值为 $0$ 拒绝 $H_0$。\n\n论证：在 $\\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$ 假设下，样本相关矩阵的期望为 $\\mathbf{I}_p$。跨坐标共享随机数流会在 $\\mathbf{Y}_k$ 中引入线性依赖，这会减小 $\\det(\\hat{\\mathbf{R}})$ 并增大 $\\chi^2_{\\text{Bart}}$ 统计量，从而对全局性的独立性偏离具有敏感性。\n\n步骤3：使用Bonferroni控制的成对相关性检验。\n对于每个满足 $1 \\leq i  j \\leq p$ 的对 $(i,j)$，计算样本相关系数 $\\hat{r}_{ij}$。在独立性的 $H_0$ 假设下，统计量\n$$\nt_{ij} = \\hat{r}_{ij}\\sqrt{\\frac{n - 2}{1 - \\hat{r}_{ij}^2}}\n$$\n服从自由度为 $n - 2$ 的学生t分布。计算双边 $p$ 值 $p_{ij} = 2 \\cdot \\Pr\\left(T_{n-2} \\geq |t_{ij}|\\right)$。共有 $m = p(p-1)/2$ 个配对；应用Bonferroni校正以控制族系误差率，方法是在 $\\alpha/m$ 水平上检验每个配对，或者等价地，将调整后的 $p$ 值 $p_{ij}^{\\text{adj}} = \\min(1, m \\cdot p_{ij})$ 与 $\\alpha$ 进行比较。如果 $p_{ij}^{\\text{adj}}  \\alpha$，则声明配对 $(i,j)$ 存在依赖关系。如果任何配对被拒绝，则声明检测到非预期依赖性。\n\n论证：共享随机数流通常会在坐标间引入线性相关。成对相关性检验可以捕捉这种线性依赖。Bonferroni校正为至少出现一次错误拒绝的概率提供了一种保守但简单的控制方法。\n\n决策规则：\n在显著性水平 $\\alpha = 0.01$ 下，如果Bartlett球形性检验的 $p$ 值小于 $\\alpha$，或者所有配对中最小的经Bonferroni校正的成对 $p$ 值小于 $\\alpha$，则声明检测到非预期依赖性。\n\n测试套件与采样器：\n我们考虑三个使用指定种子的用例以确保可复现性。\n\n用例1 (\"correct\")：$p = 6$, $n = 1000$, $\\boldsymbol{\\mu} = \\mathbf{0}$, $\\boldsymbol{\\Sigma}_0$ 为参数 $\\phi = 0.6$ 的1阶自回归结构，即 $(\\boldsymbol{\\Sigma}_0)_{ij} = \\phi^{|i - j|}$，种子 $12345$。采样器生成独立的 $\\mathbf{Z}_k \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$，因此白化后 $\\mathbf{Y}_k \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$，检验应不会检测到依赖性，结果为 $\\text{False}$。\n\n用例2 (\"shared\")：$p = 6$, $n = 300$, $\\boldsymbol{\\mu} = \\mathbf{0}$, $\\boldsymbol{\\Sigma}_0$ 是与用例1相同的自回归协方差，$\\phi = 0.6$，种子 $54321$。采样器使用 $\\mathbf{Z}_k = U_k \\mathbf{1}_p$（其中 $U_k \\sim \\mathcal{N}(0,1)$），因此所有坐标共享相同的标准正态随机数。白化后，$\\mathbf{Y}_k = \\mathbf{Z}_k$，其坐标间具有完全相关性。Bartlett检验和成对检验都应检测到依赖性，结果为 $\\text{True}$。\n\n用例3 (\"partial\")：$p = 8$, $n = 100$, $\\boldsymbol{\\mu} = \\mathbf{0}$, $\\boldsymbol{\\Sigma}_0 = \\operatorname{diag}(1.0, 1.5, 0.5, 2.0, 1.2, 0.8, 1.7, 1.1)$，种子 $2024$，共享流参数 $\\rho = 0.3$。采样器使用 $\\mathbf{Z}_k = \\sqrt{\\rho}\\, U_k \\mathbf{1}_p + \\sqrt{1 - \\rho}\\,\\mathbf{V}_k$（其中独立的 $U_k \\sim \\mathcal{N}(0,1)$ 和 $\\mathbf{V}_k \\sim \\mathcal{N}_p(\\mathbf{0}, \\mathbf{I}_p)$）；共享分量在 $\\mathbf{Y}_k$ 的坐标间引入了近似为 $\\rho$ 的等相关性。对全局相关结构敏感的Bartlett球形性检验应在 $\\alpha = 0.01$ 水平上检测到依赖性，结果为 $\\text{True}$。\n\n算法设计：\n- 实现函数以构建1阶自回归和对角情况下的 $\\boldsymbol{\\Sigma}_0$。\n- 实现 \"correct\"、\"shared\" 和 \"partial\" 场景的采样器。\n- 通过对每个样本求解 $\\mathbf{L}_0 \\mathbf{y} = \\mathbf{x} - \\boldsymbol{\\mu}$（等价于 $\\mathbf{y} = \\mathbf{L}_0^{-1}(\\mathbf{x} - \\boldsymbol{\\mu})$）来实现白化。\n- 计算样本相关矩阵、Bartlett球形性统计量和 $p$ 值，以及带有Bonferroni校正 $p$ 值的成对相关性检验族。\n- 应用决策规则，为每个测试用例生成布尔结果。\n\n最终输出格式：\n生成单行输出，其中包含按顺序排列的三个布尔结果，格式为用方括号括起来的逗号分隔列表，例如 $[\\text{False},\\text{True},\\text{True}]$。\n\n该方法整合了多元正态理论、协方差结构、白化变换和经典假设检验，以诊断并行实现中因共享随机数流而产生的非预期依赖性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef make_cov_ar1(p: int, phi: float) - np.ndarray:\n    \"\"\"Construct an AR(1) covariance matrix with parameter phi.\"\"\"\n    idx = np.arange(p)\n    return phi ** np.abs(idx[:, None] - idx[None, :])\n\ndef sample_correct(n: int, mu: np.ndarray, Sigma: np.ndarray, rng: np.random.Generator) - np.ndarray:\n    \"\"\"Generate n samples from N(mu, Sigma) using independent standard normals.\"\"\"\n    p = Sigma.shape[0]\n    L = np.linalg.cholesky(Sigma)\n    Z = rng.standard_normal(size=(n, p))\n    return mu + Z @ L.T\n\ndef sample_shared_stream(n: int, mu: np.ndarray, Sigma: np.ndarray, rng: np.random.Generator) - np.ndarray:\n    \"\"\"Generate n samples where all coordinates share the same standard normal per sample.\"\"\"\n    p = Sigma.shape[0]\n    L = np.linalg.cholesky(Sigma)\n    U = rng.standard_normal(size=(n, 1))\n    Z = np.repeat(U, p, axis=1)  # each row has identical entries\n    return mu + Z @ L.T\n\ndef sample_partial_shared(n: int, mu: np.ndarray, Sigma: np.ndarray, rho: float, rng: np.random.Generator) - np.ndarray:\n    \"\"\"Generate n samples with a shared component sqrt(rho)*U*1_p plus independent component.\"\"\"\n    p = Sigma.shape[0]\n    L = np.linalg.cholesky(Sigma)\n    U = rng.standard_normal(size=(n, 1))\n    V = rng.standard_normal(size=(n, p))\n    Z = np.sqrt(rho) * np.repeat(U, p, axis=1) + np.sqrt(1.0 - rho) * V\n    return mu + Z @ L.T\n\ndef whiten_samples(X: np.ndarray, mu: np.ndarray, Sigma0: np.ndarray) - np.ndarray:\n    \"\"\"Whiten samples with respect to the known target covariance Sigma0.\"\"\"\n    L0 = np.linalg.cholesky(Sigma0)\n    # Solve L0 Y^T = (X - mu)^T - Y = L0^{-1} (X - mu)\n    centered = X - mu\n    # Use solve for stability\n    YT = np.linalg.solve(L0, centered.T)\n    Y = YT.T\n    return Y\n\ndef bartlett_sphericity_test(Y: np.ndarray) - float:\n    \"\"\"Compute Bartlett's sphericity test p-value for the correlation matrix of Y.\"\"\"\n    n, p = Y.shape\n    R = np.corrcoef(Y, rowvar=False)\n    sign, logdet = np.linalg.slogdet(R)\n    if sign = 0:\n        # Singular or non-positive definite correlation matrix: infinite statistic, p-value 0\n        return 0.0\n    # Bartlett's test statistic\n    c = n - 1 - (2 * p + 5) / 6.0\n    stat = -c * logdet\n    df = p * (p - 1) // 2\n    pval = 1.0 - stats.chi2.cdf(stat, df)\n    return float(pval)\n\ndef pairwise_bonferroni_test(Y: np.ndarray, alpha: float) - (bool, float):\n    \"\"\"Perform all pairwise correlation tests with Bonferroni adjustment. Return (reject_any, min_adj_p).\"\"\"\n    n, p = Y.shape\n    m = p * (p - 1) // 2\n    # Compute correlation matrix\n    R = np.corrcoef(Y, rowvar=False)\n    df = n - 2\n    min_adj_p = 1.0\n    reject_any = False\n    for i in range(p):\n        for j in range(i + 1, p):\n            r = R[i, j]\n            # Guard against edge cases r ~ +/-1 leading to division by zero; clamp slightly\n            r = np.clip(r, -0.999999, 0.999999)\n            t_stat = r * np.sqrt(df / (1.0 - r ** 2))\n            pval = 2.0 * stats.t.sf(np.abs(t_stat), df)\n            adj_p = min(1.0, m * pval)\n            if adj_p  min_adj_p:\n                min_adj_p = adj_p\n            if adj_p  alpha:\n                reject_any = True\n    return reject_any, float(min_adj_p)\n\ndef detect_unintended_dependence(X: np.ndarray, mu: np.ndarray, Sigma0: np.ndarray, alpha: float) - bool:\n    \"\"\"Apply whitening, then Bartlett sphericity and pairwise Bonferroni tests; return boolean detection.\"\"\"\n    Y = whiten_samples(X, mu, Sigma0)\n    pval_bartlett = bartlett_sphericity_test(Y)\n    reject_bartlett = pval_bartlett  alpha\n    reject_pairs, _ = pairwise_bonferroni_test(Y, alpha)\n    return bool(reject_bartlett or reject_pairs)\n\ndef solve():\n    # Define significance level\n    alpha = 0.01\n\n    # Define the test cases from the problem statement.\n    # Each case: dict with keys: sampler, n, p, Sigma0, rho (optional), seed\n    # Case 1: Correct sampler, AR(1) covariance\n    p1 = 6\n    Sigma1 = make_cov_ar1(p1, phi=0.6)\n    case1 = {\n        \"sampler\": \"correct\",\n        \"n\": 1000,\n        \"p\": p1,\n        \"Sigma0\": Sigma1,\n        \"seed\": 12345\n    }\n\n    # Case 2: Shared stream sampler, same AR(1) covariance\n    p2 = 6\n    Sigma2 = make_cov_ar1(p2, phi=0.6)\n    case2 = {\n        \"sampler\": \"shared\",\n        \"n\": 300,\n        \"p\": p2,\n        \"Sigma0\": Sigma2,\n        \"seed\": 54321\n    }\n\n    # Case 3: Partial shared stream sampler, diagonal covariance\n    Sigma3 = np.diag([1.0, 1.5, 0.5, 2.0, 1.2, 0.8, 1.7, 1.1])\n    case3 = {\n        \"sampler\": \"partial\",\n        \"n\": 100,\n        \"p\": 8,\n        \"Sigma0\": Sigma3,\n        \"rho\": 0.3,\n        \"seed\": 2024\n    }\n\n    test_cases = [case1, case2, case3]\n\n    results = []\n    for case in test_cases:\n        sampler = case[\"sampler\"]\n        n = case[\"n\"]\n        Sigma0 = case[\"Sigma0\"]\n        p = case[\"p\"]\n        mu = np.zeros(p)\n        rng = np.random.default_rng(case[\"seed\"])\n\n        if sampler == \"correct\":\n            X = sample_correct(n, mu, Sigma0, rng)\n        elif sampler == \"shared\":\n            X = sample_shared_stream(n, mu, Sigma0, rng)\n        elif sampler == \"partial\":\n            rho = case[\"rho\"]\n            X = sample_partial_shared(n, mu, Sigma0, rho, rng)\n        else:\n            # Unknown sampler type; treat as failure to detect\n            X = sample_correct(n, mu, Sigma0, rng)\n\n        detected = detect_unintended_dependence(X, mu, Sigma0, alpha)\n        results.append(detected)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}