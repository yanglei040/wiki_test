{
    "hands_on_practices": [
        {
            "introduction": "贝叶斯优化的核心在于其采集函数，它指导着对未知函数下一个评估点的选择。汤普森采样 (Thompson Sampling, TS) 是一种概念上优雅且实践中非常有效的采集策略。这个练习将指导您从头开始实现一个基于高斯过程的汤普森采样器，这需要您首先计算后验分布，然后从该分布中抽取函数样本以找到其最大值，从而选择下一个评估点 。通过这个实践，您将掌握构建贝叶斯优化循环的核心编程技能。",
            "id": "3291534",
            "problem": "实现一个用于连续域贝叶斯优化的单步汤普森采样策略，该策略使用具有平方指数协方差的高斯过程先验。您的程序必须为几个测试用例计算选择的下一个评估位置，并按规定格式输出结果。\n\n从以下基本概念开始：\n- 在一个紧凑区间上，一个关于未知函数 $f$ 的高斯过程 (GP) 先验，其均值为零，协方差核为正定核，其定义属性为：任何有限集合的评估点都服从联合多元正态分布。\n- 对于方差参数为 $\\alpha^2$、长度尺度为 $\\ell$ 的平方指数协方差核，其核函数由下式给出\n$$\nk(x,x') \\;=\\; \\alpha^2 \\exp\\!\\Big(-\\,\\frac{(x-x')^2}{2\\,\\ell^2}\\Big).\n$$\n- 观测值受到独立同分布的高斯噪声的干扰，该噪声的已知方差为 $\\tau^2$。因此，对于观测到的输入 $\\{x_i\\}_{i=1}^n$ 和输出 $\\{y_i\\}_{i=1}^n$，我们有 $y_i \\,=\\, f(x_i) + \\varepsilon_i$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0,\\tau^2)$ 且在 $i$ 之间相互独立。\n\n待实现的定义和要求：\n- 考虑一个闭区间 $[a,b] \\subset \\mathbb{R}$ 和一个由 $M$ 个等距点组成的离散化网格 $X_\\star = \\{x_\\star^{(j)}\\}_{j=1}^M$，包括端点 $a$ 和 $b$。\n- 给定观测数据 $(X,y)$，其中 $X = (x_1,\\dots,x_n)$ 且 $y=(y_1,\\dots,y_n)$，在高斯过程网格点 $X_\\star$ 处的后验是一个多元正态分布。您必须将高斯条件恒等式应用于 $(f(X), f(X_\\star))$ 的联合先验（$y$ 上有加性高斯噪声），以推导出在 $X_\\star$ 处的后验均值向量 $m_\\star$ 和后验协方差矩阵 $\\Sigma_\\star$。\n- 在网格 $X_\\star$ 上的单步汤普森采样策略是从后验分布中抽取一个样本 $g_\\star \\sim \\mathcal{N}(m_\\star,\\Sigma_\\star)$，并选择网格上的最大化点作为下一个要评估的输入：\n$$\nx_{\\text{TS}} \\;=\\; \\arg\\max_{x \\in X_\\star} g_\\star(x).\n$$\n- 如果在数值容差范围内出现并列最大值，则在最大化点中选择索引最小的（即最左边的网格点）。\n\n数值要求：\n- 使用 Cholesky 分解进行线性求解和从多元正态分布中采样。\n- 在您分解的任何协方差矩阵中加入一个小的正对角“抖动”$\\epsilon$，以确保数值上的正定性。使用 $\\epsilon = 10^{-9}$。\n- 所有算术运算都是实数值，且不带物理单位。\n- 对于每个测试用例，在抽取后验样本之前，使用指定的种子初始化伪随机数生成器。该种子必须独立地应用于每个测试用例，并且仅用于采样步骤。\n\n离散化：\n- 对于所有测试用例，使用相同的网格大小 $M = 501$ 和区间 $[a,b]=[0,1]$。因此，$x_\\star^{(j)} = a + (j-1)\\,\\frac{b-a}{M-1}$，其中 $j=1,\\dots,M$。\n\n测试套件：\n- 案例 1 (仅先验)：\n  - 输入：无观测数据 ($n=0$)。\n  - 超参数：$\\alpha = 1.2$, $\\ell = 0.18$, $\\tau = 0.05$ (当 n=0 时未使用)。\n  - 种子：$1234$。\n- 案例 2 (单个带噪观测)：\n  - 输入：$X=(0.3)$, $y=(0.4)$，因此 $n=1$。\n  - 超参数：$\\alpha = 0.9$, $\\ell = 0.2$, $\\tau = 0.01$。\n  - 种子：$2025$。\n- 案例 3 (中等数据量，中等噪声)：\n  - 输入：$X=(0.15,\\,0.5,\\,0.85)$, $y=(1.0,\\,-0.2,\\,0.7)$，因此 $n=3$。\n  - 超参数：$\\alpha = 1.0$, $\\ell = 0.25$, $\\tau = 0.05$。\n  - 种子：$7$。\n- 案例 4 (近似共线设计，极低噪声)：\n  - 输入：$X=(0.499,\\,0.5,\\,0.501,\\,0.9)$, $y=(0.0,\\,0.0,\\,0.0,\\,0.2)$，因此 $n=4$。\n  - 超参数：$\\alpha = 0.8$, $\\ell = 0.05$, $\\tau = 10^{-4}$。\n  - 种子：$42$。\n\n您的程序必须为每个案例计算网格 $X_\\star$ 上的 $x_{\\text{TS}}$，并将这四个值作为一行输出，该行包含一个 Python 风格的列表，其中按案例顺序列出四个实数，例如 $[r_1,r_2,r_3,r_4]$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，列表内无空格，并严格按照案例顺序排列结果：$[x_{\\text{TS,1}},x_{\\text{TS,2}},x_{\\text{TS,3}},x_{\\text{TS,4}}]$。",
            "solution": "我们从高斯过程先验和高斯似然模型开始。设 $f \\sim \\mathcal{GP}(0,k)$，其中 $k(x,x') = \\alpha^2 \\exp\\!\\big(-\\frac{(x-x')^2}{2\\ell^2}\\big)$。观测值满足 $y_i = f(x_i) + \\varepsilon_i$，其中 $\\varepsilon_i \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,\\tau^2)$。对于有限的输入集，其联合分布是多元正态的，我们将利用这一点来推导后验预测分布。\n\n定义观测输入 $X = (x_1,\\dots,x_n)$ 和网格输入 $X_\\star = (x_\\star^{(1)},\\dots,x_\\star^{(M)})$。构建以下协方差矩阵和向量：\n- $n \\times n$ 的 Gram 矩阵 $K_{XX}$，其元素为 $[K_{XX}]_{ij} = k(x_i,x_j)$。\n- $M \\times n$ 的互协方差矩阵 $K_{\\star X}$，其元素为 $[K_{\\star X}]_{j i} = k(x_\\star^{(j)}, x_i)$。\n- $M \\times M$ 的网格 Gram 矩阵 $K_{\\star \\star}$，其元素为 $[K_{\\star \\star}]_{j j'} = k(x_\\star^{(j)}, x_\\star^{(j')})$。\n\n令 $\\sigma^2_n = \\tau^2$ 表示噪声方差。$(f(X), f(X_\\star))$ 的联合先验是均值为零的高斯分布，其协方差为\n$$\n\\begin{bmatrix}\nK_{XX} & K_{X \\star} \\\\\nK_{\\star X} & K_{\\star \\star}\n\\end{bmatrix}.\n$$\n对于高斯观测噪声，似然函数意味着 $y \\mid f(X) \\sim \\mathcal{N}(f(X), \\sigma_n^2 I_n)$。通过对 $f(X)$进行积分，得到 $y$ 的边际分布为 $y \\sim \\mathcal{N}(0, K_{XX} + \\sigma_n^2 I_n)$。然后，多元正态分布的高斯条件恒等式给出了网格点上的后验预测分布：\n$$\nf_\\star \\mid X, y, X_\\star \\sim \\mathcal{N}\\big(m_\\star, \\Sigma_\\star \\big),\n$$\n其中\n$$\nm_\\star \\;=\\; K_{\\star X}\\,(K_{XX} + \\sigma_n^2 I_n)^{-1} y,\n$$\n以及\n$$\n\\Sigma_\\star \\;=\\; K_{\\star \\star} \\;-\\; K_{\\star X}\\,(K_{XX} + \\sigma_n^2 I_n)^{-1} K_{X \\star}.\n$$\n\n这些公式直接源于对联合高斯向量进行条件化的标准结果。具体来说，如果\n$$\n\\begin{bmatrix} u \\\\ v \\end{bmatrix} \\sim \\mathcal{N}\\!\\left( \\begin{bmatrix} \\mu_u \\\\ \\mu_v \\end{bmatrix}, \\begin{bmatrix} \\Sigma_{uu} & \\Sigma_{uv} \\\\ \\Sigma_{vu} & \\Sigma_{vv} \\end{bmatrix} \\right),\n$$\n那么\n$$\nu \\mid v \\sim \\mathcal{N}\\!\\big(\\mu_u + \\Sigma_{uv}\\Sigma_{vv}^{-1}(v - \\mu_v),\\; \\Sigma_{uu} - \\Sigma_{uv}\\Sigma_{vv}^{-1}\\Sigma_{vu}\\big).\n$$\n通过识别 $u = f_\\star$, $v = y$, $\\mu_u = 0$, $\\mu_v=0$, $\\Sigma_{uu} = K_{\\star \\star}$, $\\Sigma_{uv} = K_{\\star X}$ 以及 $\\Sigma_{vv} = K_{XX} + \\sigma_n^2 I_n$，即可得到所述的后验分布。\n\n汤普森采样需要从后验分布中抽取一个函数样本，在网格 $X_\\star$ 上，这是一个样本 $g_\\star \\sim \\mathcal{N}(m_\\star, \\Sigma_\\star)$。为了高效且稳定地计算，需要：\n- 对正定矩阵 $K_{XX} + \\sigma_n^2 I_n + \\epsilon I_n$ 使用 Cholesky 分解以避免显式矩阵求逆。定义 $L = \\operatorname{chol}(K_{XX} + \\sigma_n^2 I_n + \\epsilon I_n)$，其中 $\\epsilon = 10^{-9}$。通过求解两个三角系统来解算 $(K_{XX} + \\sigma_n^2 I_n)^{-1} y$：首先解 $L z = y$ 得到 $z$，然后解 $L^\\top \\alpha = z$ 得到 $\\alpha$，从而有 $\\alpha = (K_{XX} + \\sigma_n^2 I_n)^{-1} y$。类似地，通过解 $L W = K_{X \\star}$ 得到 $W$ 来计算 $V = L^{-1} K_{X \\star}$，这样 $K_{\\star X} (K_{XX} + \\sigma_n^2 I_n)^{-1} K_{X \\star} = (K_{\\star X} L^{-T})(L^{-1} K_{X \\star}) = V^\\top V$。\n- 因此，后验均值计算为 $m_\\star = K_{\\star X}\\,\\alpha$，后验协方差计算为 $\\Sigma_\\star = K_{\\star \\star} - V^\\top V$。为了采样时的数值稳定性，如果需要，在分解前向 $\\Sigma_\\star$ 加入 $\\epsilon I_M$。\n- 为了采样，计算 Cholesky 因子 $L_\\star = \\operatorname{chol}(\\Sigma_\\star + \\epsilon I_M)$ 并设置 $g_\\star = m_\\star + L_\\star z$，其中 $z \\sim \\mathcal{N}(0, I_M)$。在抽取 $z$ 之前，立即为每个测试用例按规定设置伪随机数生成器的种子。\n\n当 $n=0$ 时，没有观测数据。在这种情况下，$m_\\star = 0$ 且 $\\Sigma_\\star = K_{\\star \\star}$，采样简化为从网格上的先验分布中抽取。\n\n最后，计算汤普森采样决策 $x_{\\text{TS}} = \\arg\\max_{x \\in X_\\star} g_\\star(x)$，通过选择最小索引来打破并列。由于 $X_\\star$ 在 $[0,1]$ 上等距分布且 $M=501$，我们有 $x_\\star^{(j)} = \\frac{j-1}{500}$，其中 $j=1,\\dots,501$。\n\n我们现在概述每个测试用例的算法：\n- 在 $[0,1]$ 上构建 $X_\\star$，其中 $M=501$。\n- 如果 $n>0$，构建 $K_{XX}$、$K_{\\star X}$ 和 $K_{\\star \\star}$。向 $K_{XX}$ 中加入 $\\sigma_n^2 I_n$ 和抖动 $\\epsilon I_n$，然后进行 Cholesky 分解。通过三角求解和 Schur 补公式计算 $m_\\star$ 和 $\\Sigma_\\star$。如果需要，向 $\\Sigma_\\star$ 中加入抖动。\n- 如果 $n=0$，设置 $m_\\star = 0$ 和 $\\Sigma_\\star = K_{\\star \\star}$ (带抖动)。\n- 设置伪随机数生成器的种子，并通过 $\\Sigma_\\star + \\epsilon I_M$ 的 Cholesky 因子抽取 $g_\\star \\sim \\mathcal{N}(m_\\star,\\Sigma_\\star)$。\n- 返回使 $g_\\star$ 最大化的网格点。\n\n此过程基于高斯过程和多元正态条件化的基本属性，确保了在离散化为精细网格的连续域上单步汤普森采样的正确和数值稳定的实现。\n\n程序将对四个指定的案例执行这些步骤，并按要求格式用单行打印出四个选定的网格位置。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef se_kernel(x, xprime, alpha, ell):\n    # Squared Exponential (RBF) kernel for 1D arrays x, xprime\n    # Returns Gram matrix of shape (len(x), len(xprime))\n    x = np.atleast_2d(x).T  # (n,1)\n    xprime = np.atleast_2d(xprime).T  # (m,1)\n    d2 = (x - xprime.T) ** 2  # (n,m)\n    return (alpha ** 2) * np.exp(-0.5 * d2 / (ell ** 2))\n\ndef cholesky_psd(A, jitter=1e-9, max_tries=5):\n    # Attempt a Cholesky factorization, increasing jitter if needed\n    jitter_scale = 1.0\n    for _ in range(max_tries):\n        try:\n            L = np.linalg.cholesky(A + (jitter * jitter_scale) * np.eye(A.shape[0]))\n            return L\n        except np.linalg.LinAlgError:\n            jitter_scale *= 10.0\n    # Final attempt may still fail; raise the error\n    L = np.linalg.cholesky(A + (jitter * jitter_scale) * np.eye(A.shape[0]))\n    return L\n\ndef gp_posterior_params(X_obs, y_obs, X_star, alpha, ell, tau, jitter=1e-9):\n    # Compute posterior mean m_star and covariance Sigma_star at X_star\n    n = len(X_obs)\n    M = len(X_star)\n    K_ss = se_kernel(X_star, X_star, alpha, ell)\n    if n == 0:\n        m_star = np.zeros(M)\n        # Add jitter for numerical stability\n        Sigma_star = K_ss.copy()\n        # Ensure symmetry\n        Sigma_star = (Sigma_star + Sigma_star.T) * 0.5\n        return m_star, Sigma_star\n    # Build K_xx and cross-covariances\n    K_xx = se_kernel(X_obs, X_obs, alpha, ell)\n    # Add noise variance and jitter to K_xx\n    K_xx_noisy = K_xx + (tau**2) * np.eye(n) + jitter * np.eye(n)\n    L = cholesky_psd(K_xx_noisy, jitter=jitter)\n    # Solve for alpha_vec = (K_xx + tau^2 I)^{-1} y\n    # First solve L z = y, then L^T alpha_vec = z\n    z = np.linalg.solve(L, y_obs)\n    alpha_vec = np.linalg.solve(L.T, z)\n    K_sx = se_kernel(X_star, X_obs, alpha, ell)  # (M,n)\n    # Posterior mean\n    m_star = K_sx @ alpha_vec  # (M,)\n    # Compute V = L^{-1} K_xs where K_xs = K(X_obs, X_star)\n    K_xs = K_sx.T  # (n,M)\n    V = np.linalg.solve(L, K_xs)  # (n,M)\n    # Posterior covariance: K_ss - V^T V\n    Sigma_star = K_ss - V.T @ V\n    # Symmetrize to remove numerical asymmetry\n    Sigma_star = (Sigma_star + Sigma_star.T) * 0.5\n    return m_star, Sigma_star\n\ndef thompson_sample_argmax(X_star, m_star, Sigma_star, seed, jitter=1e-9):\n    # Draw one sample from N(m_star, Sigma_star) using Cholesky and return argmax location\n    # Add jitter to ensure PSD\n    Ls = cholesky_psd(Sigma_star, jitter=jitter)\n    rng = np.random.RandomState(seed)\n    z = rng.randn(len(X_star))\n    g_star = m_star + Ls @ z\n    # Argmax with tie-break to smallest index\n    idx = int(np.argmax(g_star))\n    return X_star[idx]\n\ndef solve():\n    # Define grid [0,1] with M=501\n    M = 501\n    a, b = 0.0, 1.0\n    X_star = np.linspace(a, b, M)\n\n    # Define the test cases from the problem statement.\n    # Each case: dict with X_obs, y_obs, alpha, ell, tau, seed\n    test_cases = [\n        # Case 1: prior only\n        {\n            \"X_obs\": np.array([]),\n            \"y_obs\": np.array([]),\n            \"alpha\": 1.2,\n            \"ell\": 0.18,\n            \"tau\": 0.05,  # unused\n            \"seed\": 1234,\n        },\n        # Case 2: single observation\n        {\n            \"X_obs\": np.array([0.3]),\n            \"y_obs\": np.array([0.4]),\n            \"alpha\": 0.9,\n            \"ell\": 0.2,\n            \"tau\": 0.01,\n            \"seed\": 2025,\n        },\n        # Case 3: three observations\n        {\n            \"X_obs\": np.array([0.15, 0.5, 0.85]),\n            \"y_obs\": np.array([1.0, -0.2, 0.7]),\n            \"alpha\": 1.0,\n            \"ell\": 0.25,\n            \"tau\": 0.05,\n            \"seed\": 7,\n        },\n        # Case 4: nearly collinear design, very low noise\n        {\n            \"X_obs\": np.array([0.499, 0.5, 0.501, 0.9]),\n            \"y_obs\": np.array([0.0, 0.0, 0.0, 0.2]),\n            \"alpha\": 0.8,\n            \"ell\": 0.05,\n            \"tau\": 1e-4,\n            \"seed\": 42,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        X_obs = case[\"X_obs\"]\n        y_obs = case[\"y_obs\"]\n        alpha = case[\"alpha\"]\n        ell = case[\"ell\"]\n        tau = case[\"tau\"]\n        seed = case[\"seed\"]\n\n        m_star, Sigma_star = gp_posterior_params(X_obs, y_obs, X_star, alpha, ell, tau, jitter=1e-9)\n        x_ts = thompson_sample_argmax(X_star, m_star, Sigma_star, seed=seed, jitter=1e-9)\n        # Ensure Python float printing consistency\n        results.append(x_ts)\n\n    # Final print statement in the exact required format (no spaces).\n    print(f\"[{','.join(map(lambda v: repr(float(v)), results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "许多现实世界中的优化问题不仅要求我们最小化某个目标函数，还必须满足一系列约束条件，例如成本、安全或物理限制。这就需要我们将贝叶斯优化从无约束设置扩展到有约束设置。本练习将引导您推导约束期望增益 (Constrained Expected Improvement, CEI) 采集函数，它是在满足约束的概率下，对期望改进进行的加权 。从第一性原理出发进行推导，将加深您对如何构建采集函数以及如何将约束等额外信息整合到优化决策过程中的理解。",
            "id": "3291581",
            "problem": "一个研究团队正在使用约束贝叶斯优化（Constrained Bayesian Optimization）来为一个黑箱模拟器调整一个标量设计变量 $x \\in \\mathbb{R}$。其目标是在满足单个不等式约束 $c(x) \\le 0$ 的条件下，最小化一个未知的目标函数 $f(x)$。该模拟器计算成本高昂，因此团队采用高斯过程（GP）回归（Gaussian Process (GP) regression）来对潜在的、无噪声的函数 $f$ 和 $c$ 进行建模。在候选点 $x$ 处，潜在值当前的高斯过程后验分布是独立的，且服从正态分布：\n- $f(x) \\sim \\mathcal{N}(\\mu_{f}, \\sigma_{f}^{2})$,\n- $c(x) \\sim \\mathcal{N}(\\mu_{c}, \\sigma_{c}^{2})$,\n$f(x)$ 和 $c(x)$ 之间的预测独立性，其合理性来自于独立的先验和条件独立的数据似然。\n\n将用于最小化问题的、在 $x$ 点的约束期望提升（constrained expected improvement, CEI）定义为期望\n$$\n\\operatorname{CEI}(x) \\equiv \\mathbb{E}\\big[(f_{\\star}-f(x))_{+} \\,\\mathbf{1}\\{c(x)\\le 0\\}\\big],\n$$\n其中 $(a)_{+} \\equiv \\max\\{a,0\\}$，$\\mathbf{1}\\{\\cdot\\}$ 是指示函数，$f_{\\star}$ 是当前观测到的最佳可行目标值。\n\n仅从期望、指示函数和正态分布性质的定义出发，完成以下任务：\n1. 在所述的高斯过程独立性假设下，推导出 $\\operatorname{CEI}(x)$ 的闭式表达式，并用 $\\mu_{f}$、$\\sigma_{f}$、$f_{\\star}$、$\\mu_{c}$ 和 $\\sigma_{c}$ 表示你的答案。\n2. 在一个候选点，当参数为 $\\mu_{f} = 0.4$，$\\sigma_{f} = 0.3$，$f_{\\star} = 0.4$，$\\mu_{c} = 0$ 和 $\\sigma_{c} = 1$ 时，对你的表达式进行数值计算。\n\n将你的最终数值答案四舍五入至四位有效数字。最终结果以无单位的纯数字表示。",
            "solution": "该问题是有效的，因为它在科学上以贝叶斯优化理论为基础，问题提法明确，是客观的，并且包含了获得唯一解所需的所有信息。\n\n约束期望提升 $\\operatorname{CEI}(x)$ 定义为两个项乘积的期望：目标函数中的潜在提升 $(f_{\\star}-f(x))_{+}$ 和可行性指示函数 $\\mathbf{1}\\{c(x)\\le 0\\}$。\n$$\n\\operatorname{CEI}(x) = \\mathbb{E}\\big[(f_{\\star}-f(x))_{+} \\,\\mathbf{1}\\{c(x)\\le 0\\}\\big]\n$$\n该期望是关于随机变量 $f(x)$ 和 $c(x)$ 的联合概率分布计算的。问题陈述 $f(x)$ 和 $c(x)$ 的高斯过程后验是独立的。令 $Y_f = f(x)$ 和 $Y_c = c(x)$。它们的分布为：\n- $Y_f \\sim \\mathcal{N}(\\mu_{f}, \\sigma_{f}^{2})$\n- $Y_c \\sim \\mathcal{N}(\\mu_{c}, \\sigma_{c}^{2})$\n\n由于它们的独立性，联合概率密度函数是它们各自概率密度函数的乘积：$p(y_f, y_c) = p_f(y_f) p_c(y_c)$。\n此外，对于两个独立的随机变量 $A$ 和 $B$，它们乘积的期望等于它们各自期望的乘积，即 $\\mathbb{E}[AB] = \\mathbb{E}[A]\\mathbb{E}[B]$。在我们的情况中，项 $(f_{\\star}-Y_f)_{+}$ 仅是 $Y_f$ 的函数，而 $\\mathbf{1}\\{Y_c\\le 0\\}$ 仅是 $Y_c$ 的函数。因此，这两个派生出的随机变量也是独立的。我们可以将期望分离：\n$$\n\\operatorname{CEI}(x) = \\mathbb{E}\\big[(f_{\\star}-Y_f)_{+}\\big] \\cdot \\mathbb{E}\\big[\\mathbf{1}\\{Y_c\\le 0\\}\\big]\n$$\n我们现在将分别计算每个期望。\n\n**第1部分：闭式表达式的推导**\n\n我们首先计算目标提升的期望，记为 $I_f$：\n$$\nI_f = \\mathbb{E}\\big[(f_{\\star}-Y_f)_{+}\\big] = \\int_{-\\infty}^{\\infty} (f_{\\star}-y_f)_{+} \\, p_f(y_f) \\, dy_f\n$$\n其中 $p_f(y_f)$ 是均值为 $\\mu_f$、方差为 $\\sigma_f^2$ 的正态分布的概率密度函数 (PDF)。项 $(f_{\\star}-y_f)_{+} = \\max\\{f_{\\star}-y_f, 0\\}$ 仅在 $y_f  f_{\\star}$ 时非零。因此，积分的上限变为 $f_{\\star}$：\n$$\nI_f = \\int_{-\\infty}^{f_{\\star}} (f_{\\star}-y_f) \\, p_f(y_f) \\, dy_f\n$$\n我们可以将这个积分分成两部分：\n$$\nI_f = f_{\\star} \\int_{-\\infty}^{f_{\\star}} p_f(y_f) \\, dy_f - \\int_{-\\infty}^{f_{\\star}} y_f \\, p_f(y_f) \\, dy_f\n$$\n为了计算这些积分，我们进行变量替换以标准化 $Y_f$。令 $Z_f = \\frac{Y_f - \\mu_f}{\\sigma_f}$。变量 $Z_f$ 服从标准正态分布 $Z_f \\sim \\mathcal{N}(0, 1)$，其概率密度函数 (PDF) 为 $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^2/2)$，累积分布函数 (CDF) 为 $\\Phi(z)$。微分元变换为 $p_f(y_f) \\, dy_f = \\phi(z_f) \\, dz_f$。积分上限 $y_f = f_{\\star}$ 变为 $z_f = \\frac{f_{\\star}-\\mu_f}{\\sigma_f}$。我们定义 $\\gamma_f = \\frac{f_{\\star}-\\mu_f}{\\sigma_f}$。\n\n第一个积分是概率 $P(Y_f \\le f_{\\star})$：\n$$\n\\int_{-\\infty}^{f_{\\star}} p_f(y_f) \\, dy_f = P(Y_f \\le f_{\\star}) = P\\left(Z_f \\le \\frac{f_{\\star}-\\mu_f}{\\sigma_f}\\right) = \\Phi(\\gamma_f)\n$$\n第二个积分是：\n$$\n\\int_{-\\infty}^{f_{\\star}} y_f \\, p_f(y_f) \\, dy_f = \\int_{-\\infty}^{\\gamma_f} (\\mu_f + \\sigma_f z_f) \\, \\phi(z_f) \\, dz_f\n$$\n$$\n= \\mu_f \\int_{-\\infty}^{\\gamma_f} \\phi(z_f) \\, dz_f + \\sigma_f \\int_{-\\infty}^{\\gamma_f} z_f \\, \\phi(z_f) \\, dz_f\n$$\n根据定义，积分 $\\int_{-\\infty}^{\\gamma_f} \\phi(z_f) \\, dz_f$ 等于 $\\Phi(\\gamma_f)$。对于第二个积分，我们使用性质 $\\frac{d}{dz}(-\\phi(z)) = z\\phi(z)$。\n$$\n\\int_{-\\infty}^{\\gamma_f} z_f \\, \\phi(z_f) \\, dz_f = [-\\phi(z_f)]_{-\\infty}^{\\gamma_f} = -\\phi(\\gamma_f) - \\lim_{z_f \\to -\\infty} (-\\phi(z_f)) = -\\phi(\\gamma_f)\n$$\n所以，第二个积分的计算结果为 $\\mu_f \\Phi(\\gamma_f) - \\sigma_f \\phi(\\gamma_f)$。\n综合这些结果得到 $I_f$：\n$$\nI_f = f_{\\star} \\Phi(\\gamma_f) - (\\mu_f \\Phi(\\gamma_f) - \\sigma_f \\phi(\\gamma_f))\n$$\n$$\nI_f = (f_{\\star} - \\mu_f) \\Phi(\\gamma_f) + \\sigma_f \\phi(\\gamma_f)\n$$\n\n接下来，我们计算可行性指示函数的期望，记为 $I_c$：\n$$\nI_c = \\mathbb{E}\\big[\\mathbf{1}\\{Y_c\\le 0\\}\\big] = \\int_{-\\infty}^{\\infty} \\mathbf{1}\\{y_c\\le 0\\} \\, p_c(y_c) \\, dy_c\n$$\n这其实就是 $Y_c$ 小于或等于 $0$ 的概率：\n$$\nI_c = P(Y_c \\le 0) = \\int_{-\\infty}^{0} p_c(y_c) \\, dy_c\n$$\n用 $Z_c = \\frac{Y_c - \\mu_c}{\\sigma_c} \\sim \\mathcal{N}(0, 1)$ 标准化变量 $Y_c$，条件 $Y_c \\le 0$ 变为 $Z_c \\le \\frac{0-\\mu_c}{\\sigma_c} = -\\frac{\\mu_c}{\\sigma_c}$。\n因此，该概率为：\n$$\nI_c = P\\left(Z_c \\le -\\frac{\\mu_c}{\\sigma_c}\\right) = \\Phi\\left(-\\frac{\\mu_c}{\\sigma_c}\\right)\n$$\n最后，$\\operatorname{CEI}(x)$ 的完整表达式是 $I_f$ 和 $I_c$ 的乘积：\n$$\n\\operatorname{CEI}(x) = \\left[ (f_{\\star} - \\mu_f) \\Phi\\left(\\frac{f_{\\star} - \\mu_f}{\\sigma_f}\\right) + \\sigma_f \\phi\\left(\\frac{f_{\\star} - \\mu_f}{\\sigma_f}\\right) \\right] \\Phi\\left(-\\frac{\\mu_c}{\\sigma_c}\\right)\n$$\n这就是所要求的闭式表达式。\n\n**第2部分：数值计算**\n\n对于一个候选点 $x$，给定以下参数值：\n- $\\mu_{f} = 0.4$\n- $\\sigma_{f} = 0.3$\n- $f_{\\star} = 0.4$\n- $\\mu_{c} = 0$\n- $\\sigma_{c} = 1$\n\n首先，我们计算标准正态 CDF 和 PDF 的自变量。\n对于目标函数部分：\n$$\n\\gamma_f = \\frac{f_{\\star} - \\mu_f}{\\sigma_f} = \\frac{0.4 - 0.4}{0.3} = 0\n$$\n对于约束部分：\n$$\n-\\frac{\\mu_c}{\\sigma_c} = -\\frac{0}{1} = 0\n$$\n现在，我们计算这些特殊函数在这些自变量处的值：\n- $\\Phi(0) = 0.5$\n- $\\phi(0) = \\frac{1}{\\sqrt{2\\pi}}$\n\n将这些值代入 $\\operatorname{CEI}(x)$ 的表达式中：\n$$\n\\operatorname{CEI}(x) = \\left[ (0.4 - 0.4) \\Phi(0) + 0.3 \\cdot \\phi(0) \\right] \\cdot \\Phi(0)\n$$\n$$\n\\operatorname{CEI}(x) = \\left[ 0 \\cdot 0.5 + 0.3 \\cdot \\frac{1}{\\sqrt{2\\pi}} \\right] \\cdot 0.5\n$$\n$$\n\\operatorname{CEI}(x) = \\left( 0.3 \\cdot \\frac{1}{\\sqrt{2\\pi}} \\right) \\cdot 0.5\n$$\n$$\n\\operatorname{CEI}(x) = \\frac{0.15}{\\sqrt{2\\pi}}\n$$\n现在我们计算数值：\n$$\n\\operatorname{CEI}(x) \\approx \\frac{0.15}{2.50662827} \\approx 0.059841342\n$$\n四舍五入到四位有效数字，我们得到 $0.05984$。",
            "answer": "$$\\boxed{0.05984}$$"
        },
        {
            "introduction": "在科学和工程领域，我们优化的“黑箱”函数通常是一个随机计算机模拟。这意味着函数的评估本身带有噪声，并且我们可以控制模拟的运行方式，例如分配的计算预算。本练习探讨了如何将贝叶斯优化与随机模拟中的经典方差缩减技术——公共随机数 (Common Random Numbers, CRN)——相结合 。您将推导在固定计算预算下，如何最优地分配模拟次数以最小化两个备选点之间性能差异的方差，从而提高决策的可靠性。这项实践将贝叶斯优化与随机模拟理论深度结合，展示了如何通过智能地管理评估过程的噪声来增强优化框架的效率。",
            "id": "3291579",
            "problem": "考虑贝叶斯优化（BO）中的单次迭代，其目标是决定如何在两个候选输入点之间分配随机模拟器的复制次数，以提高指导 BO 决策的成对比较的可靠性。对于输入 $x_i$，模拟器产生独立同分布的蒙特卡洛（MC）输出 $Y_i^{(r)}$，其均值 $f(x_i)$ 未知，方差参数 $\\sigma_i^2$ 已知，其中 $i \\in \\{1,2\\}$，复制索引 $r \\in \\{1,2,\\dots\\}$。当使用公共随机数（CRN）时，$x_1$ 的第 $r$ 次复制与 $x_2$ 的第 $r$ 次复制共享相同的底层随机种子，从而产生相关性。假设当复制通过相同种子配对时，CRN 会在两个输入的单次复制噪声之间产生一个成对相关系数 $\\rho \\in [0,1)$。可用的总复制预算是一个固定的整数 $N \\ge 2$，需分配为 $n_1$ 和 $n_2$，其中 $n_1 + n_2 = N$ 且每个 $n_i$ 都必须是正整数。\n\n您将分析 CRN 对样本均值差方差的影响，并推导最优的复制分配策略。该分析的基本依据是：\n- 样本均值的定义 $\\bar{Y}_i = \\frac{1}{n_i} \\sum_{r=1}^{n_i} Y_i^{(r)}$。\n- 随机变量线性组合的方差和协方差性质，包括 $\\operatorname{Var}(aX + bY) = a^2 \\operatorname{Var}(X) + b^2 \\operatorname{Var}(Y) + 2ab \\operatorname{Cov}(X,Y)$ 以及复制间的独立性。\n- 设计原则是，在固定预算约束下，通过最小化样本均值差 $\\bar{Y}_1 - \\bar{Y}_2$ 的方差可以提高成对比较的可靠性，并且当真实均值差距 $\\Delta = f(x_1) - f(x_2)$ 固定时，减小该方差会直接降低错误选择较差点的近似概率。\n\n任务：\n1. 从基本原理出发，推导在 CRN 配对下样本均值差 $\\bar{Y}_1 - \\bar{Y}_2$ 的方差表达式，并认识到只有前 $\\min(n_1,n_2)$ 次复制可以使用公共种子进行配对，而其余的复制（若有）是未配对且独立的。仅使用上述基本定义，将此方差完全用 $n_1$, $n_2$, $\\sigma_1^2$, $\\sigma_2^2$ 和 $\\rho$ 表示。不要使用任何简便公式。\n2. 建立并求解连续优化问题，在约束条件 $n_1 + n_2 = N$ 下最小化此方差（其中 $n_1 > 0$ 和 $n_2 > 0$ 为实数），并考虑到配对结构引入的分段性质。描述最优比率以及每种情况适用的条件。然后解释当 $\\rho = 0$（即无 CRN）时，该解如何简化为经典的 Neyman 分配，以及增大 $\\rho$ 如何驱动分配趋向于均衡。\n3. 为 $(n_1,n_2)$ 提供一个有原则的整数取整规则，该规则需满足 $n_1 + n_2 = N$ 和 $n_i \\in \\{1,2,\\dots\\}$，并使用该整数对计算精确方差。\n4. 对于给定的真实均值差距 $\\Delta$，在高斯近似下，将错误选择更优点的概率近似为 $\\Phi\\!\\left(-\\frac{\\Delta}{\\sqrt{\\operatorname{Var}(\\bar{Y}_1 - \\bar{Y}_2)}}\\right)$，其中 $\\Phi(\\cdot)$ 是标准正态累积分布函数。使用此公式来量化 CRN 带来的好处。\n\n实现一个完整的程序，对于下面指定的每个测试用例，计算：\n- 通过应用您推导的连续解和取整规则，计算整数最优的 CRN 分配 $(n_1^{\\mathrm{crn}}, n_2^{\\mathrm{crn}})$。\n- 对于给定的 $\\Delta$，计算产生的样本均值差方差 $V^{\\mathrm{crn}}$ 和错选概率 $p^{\\mathrm{crn}}$。\n- 当 $\\rho=0$ 时，从您推导的连续解和相同的取整规则得到的整数最优非 CRN 分配 $(n_1^{\\mathrm{noc}}, n_2^{\\mathrm{noc}})$。\n- 产生的方差 $V^{\\mathrm{noc}}$ 和错选概率 $p^{\\mathrm{noc}}$。\n\n测试套件：\n- 情况1：$(N,\\sigma_1,\\sigma_2,\\rho,\\Delta) = (40, 1.0, 2.0, 0.5, 0.4)$。\n- 情况2：$(N,\\sigma_1,\\sigma_2,\\rho,\\Delta) = (40, 1.5, 0.8, 0.0, 0.3)$。\n- 情况3：$(N,\\sigma_1,\\sigma_2,\\rho,\\Delta) = (40, 1.0, 1.5, 0.9, 0.1)$。\n- 情况4：$(N,\\sigma_1,\\sigma_2,\\rho,\\Delta) = (100, 0.5, 3.0, 0.6, 0.7)$。\n- 情况5：$(N,\\sigma_1,\\sigma_2,\\rho,\\Delta) = (2, 1.0, 1.0, 0.8, 0.2)$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例的结果本身必须是一个包含八个条目的列表，顺序如下：\n$[n_1^{\\mathrm{crn}}, n_2^{\\mathrm{crn}}, V^{\\mathrm{crn}}, p^{\\mathrm{crn}}, n_1^{\\mathrm{noc}}, n_2^{\\mathrm{noc}}, V^{\\mathrm{noc}}, p^{\\mathrm{noc}}]$。\n所有条目必须是数字。例如，一个有效的总输出形式为 $[[\\dots],[\\dots],[\\dots],[\\dots],[\\dots]]$。",
            "solution": "该问题经评估是有效的，因为它在科学上基于随机模拟和统计优化的原理，提法恰当、客观且内部一致。我们进行逐步推导和求解。\n\n### 任务1：带CRN的样本均值差方差的推导\n\n我们的目标是找到 $V(n_1, n_2) = \\operatorname{Var}(\\bar{Y}_1 - \\bar{Y}_2)$ 的表达式，其中 $\\bar{Y}_i = \\frac{1}{n_i} \\sum_{r=1}^{n_i} Y_i^{(r)}$。\n\n根据方差的性质，我们有：\n$$\n\\operatorname{Var}(\\bar{Y}_1 - \\bar{Y}_2) = \\operatorname{Var}(\\bar{Y}_1) + \\operatorname{Var}(\\bar{Y}_2) - 2 \\operatorname{Cov}(\\bar{Y}_1, \\bar{Y}_2)\n$$\n\n首先，我们求每个样本均值的方差。由于对于固定输入 $x_i$ 的复制 $Y_i^{(r)}$ 是独立同分布的，方差为 $\\sigma_i^2$，我们有：\n$$\n\\operatorname{Var}(\\bar{Y}_i) = \\operatorname{Var}\\left(\\frac{1}{n_i} \\sum_{r=1}^{n_i} Y_i^{(r)}\\right) = \\frac{1}{n_i^2} \\sum_{r=1}^{n_i} \\operatorname{Var}(Y_i^{(r)}) = \\frac{1}{n_i^2} (n_i \\sigma_i^2) = \\frac{\\sigma_i^2}{n_i}\n$$\n\n接下来，我们计算协方差项 $\\operatorname{Cov}(\\bar{Y}_1, \\bar{Y}_2)$。\n$$\n\\operatorname{Cov}(\\bar{Y}_1, \\bar{Y}_2) = \\operatorname{Cov}\\left(\\frac{1}{n_1} \\sum_{r=1}^{n_1} Y_1^{(r)}, \\frac{1}{n_2} \\sum_{s=1}^{n_2} Y_2^{(s)}\\right) = \\frac{1}{n_1 n_2} \\sum_{r=1}^{n_1} \\sum_{s=1}^{n_2} \\operatorname{Cov}(Y_1^{(r)}, Y_2^{(s)})\n$$\n根据问题描述，除非在 CRN 下通过相同的随机种子配对，否则复制是独立的。这意味着当 $r \\neq s$ 时，$\\operatorname{Cov}(Y_1^{(r)}, Y_2^{(s)}) = 0$。协方差仅在复制索引匹配时非零，最多可配对的次数为 $m = \\min(n_1, n_2)$。对于这些配对的复制（$r=s \\le m$），协方差由相关系数 $\\rho$ 给出：\n$$\n\\operatorname{Cov}(Y_1^{(r)}, Y_2^{(r)}) = \\rho \\sqrt{\\operatorname{Var}(Y_1^{(r)})\\operatorname{Var}(Y_2^{(r)})} = \\rho \\sigma_1 \\sigma_2 \\quad \\text{for } r = 1, \\dots, m\n$$\n因此，协方差的双重求和简化为：\n$$\n\\sum_{r=1}^{n_1} \\sum_{s=1}^{n_2} \\operatorname{Cov}(Y_1^{(r)}, Y_2^{(s)}) = \\sum_{r=1}^{\\min(n_1, n_2)} \\operatorname{Cov}(Y_1^{(r)}, Y_2^{(r)}) = \\min(n_1, n_2) \\cdot \\rho \\sigma_1 \\sigma_2\n$$\n将此代入 $\\operatorname{Cov}(\\bar{Y}_1, \\bar{Y}_2)$ 的表达式中：\n$$\n\\operatorname{Cov}(\\bar{Y}_1, \\bar{Y}_2) = \\frac{\\min(n_1, n_2) \\rho \\sigma_1 \\sigma_2}{n_1 n_2}\n$$\n最后，我们将所有项合并，得到差值的方差：\n$$\nV(n_1, n_2) = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2} - 2 \\frac{\\min(n_1, n_2) \\rho \\sigma_1 \\sigma_2}{n_1 n_2}\n$$\n\n### 任务2：复制分配的连续优化\n\n我们旨在最小化 $V(n_1, n_2)$，约束条件为 $n_1 + n_2 = N$，其中 $n_1, n_2 > 0$ 为实数。$\\min(n_1, n_2)$ 项的存在使得目标函数是分段的。我们使用 $n_2 = N-n_1$，在两种情况下进行分析。\n\n**情况1：$n_1 \\le n_2 \\implies n_1 \\le N/2$**\n在此情况下，$\\min(n_1, n_2) = n_1$。方差为：\n$$\nV_1(n_1) = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{N-n_1} - \\frac{2 n_1 \\rho \\sigma_1 \\sigma_2}{n_1(N-n_1)} = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2 - 2 \\rho \\sigma_1 \\sigma_2}{N-n_1}\n$$\n为求最小值，我们对 $n_1$ 求导并令其为零，假设 $\\sigma_2^2 - 2 \\rho \\sigma_1 \\sigma_2 > 0$：\n$$\n\\frac{dV_1}{dn_1} = -\\frac{\\sigma_1^2}{n_1^2} + \\frac{\\sigma_2^2 - 2 \\rho \\sigma_1 \\sigma_2}{(N-n_1)^2} = 0 \\implies \\frac{n_1}{ N-n_1} = \\frac{\\sigma_1}{\\sqrt{\\sigma_2^2 - 2 \\rho \\sigma_1 \\sigma_2}}\n$$\n此解在该情况下有效，即 $n_1/n_2 \\le 1$，这要求 $\\sigma_1 \\le \\sqrt{\\sigma_2^2 - 2 \\rho \\sigma_1 \\sigma_2}$，或 $\\sigma_1^2 + 2 \\rho \\sigma_1 \\sigma_2 \\le \\sigma_2^2$。\n\n**情况2：$n_1 > n_2 \\implies n_1 > N/2$**\n在此情况下，$\\min(n_1, n_2) = n_2 = N-n_1$。方差为：\n$$\nV_2(n_1) = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{N-n_1} - \\frac{2 (N-n_1) \\rho \\sigma_1 \\sigma_2}{n_1(N-n_1)} = \\frac{\\sigma_1^2 - 2 \\rho \\sigma_1 \\sigma_2}{n_1} + \\frac{\\sigma_2^2}{N-n_1}\n$$\n根据对称性，通过交换索引 1 和 2 可以找到最优比率：\n$$\n\\frac{n_2}{N-n_2} = \\frac{\\sigma_2}{\\sqrt{\\sigma_1^2 - 2 \\rho \\sigma_1 \\sigma_2}} \\implies \\frac{N-n_1}{n_1} = \\frac{\\sigma_2}{\\sqrt{\\sigma_1^2 - 2 \\rho \\sigma_1 \\sigma_2}}\n$$\n此解在该情况下有效，即 $n_2/n_1  1$，这要求 $\\sigma_2^2 + 2 \\rho \\sigma_1 \\sigma_2  \\sigma_1^2$。\n\n**情况3：均衡分配**\n如果上述两个条件都不成立，则凸函数 $V(n_1)$ 的最小值出现在边界 $n_1 = N/2$ 处。这种情况发生在情况1的无约束最小值位于 $n_1 > N/2$ 而情况2的无约束最小值位于 $n_1  N/2$ 时。这种均衡分配的条件是 $|\\sigma_1^2 - \\sigma_2^2| \\le 2\\rho\\sigma_1\\sigma_2$。如果平方根下的项变为非正数（例如 $\\sigma_2^2 - 2 \\rho \\sigma_1 \\sigma_2 \\le 0$），这种情况也适用，因为相应的导数符号不会改变，从而将最优解推向边界 $n_1 = N/2$。\n\n因此，最优连续分配 $n_1^*$ 为：\n$$\nn_1^* =\n\\begin{cases}\nN \\frac{\\sigma_1}{\\sigma_1 + \\sqrt{\\sigma_2^2 - 2\\rho\\sigma_1\\sigma_2}}  \\text{if } \\sigma_1^2 + 2\\rho\\sigma_1\\sigma_2  \\sigma_2^2 \\\\\nN \\frac{\\sqrt{\\sigma_1^2 - 2\\rho\\sigma_1\\sigma_2}}{\\sqrt{\\sigma_1^2 - 2\\rho\\sigma_1\\sigma_2} + \\sigma_2}  \\text{if } \\sigma_2^2 + 2\\rho\\sigma_1\\sigma_2  \\sigma_1^2 \\\\\nN/2  \\text{otherwise}\n\\end{cases}\n$$\n\n**与 Neyman 分配的关系及 $\\rho$ 的影响：**\n如果 $\\rho=0$（无 CRN），方差表达式简化为 $V=\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}$。分段解的条件合并，最优分配变为 $n_1^* = N \\frac{\\sigma_1}{\\sigma_1+\\sigma_2}$ 和 $n_2^* = N \\frac{\\sigma_2}{\\sigma_1+\\sigma_2}$。这正是经典的 Neyman 分配。\n随着 $\\rho$ 增加并趋近于 1，项 $2\\rho\\sigma_1\\sigma_2$ 会增大。这使得均衡分配的条件 $|\\sigma_1^2 - \\sigma_2^2| \\le 2\\rho\\sigma_1\\sigma_2$ 更容易满足。因此，更强的 CRN 有利于更均衡的分配（$n_1 \\approx n_2$），以最大化配对复制的数量，这正是 CRN 产生方差缩减效益的地方。\n\n### 任务3：整数取整规则\n\n给定连续最优解 $n_1^*$，我们必须找到一个整数解 $(n_1, n_2)$，使得 $n_1, n_2 \\ge 1$ 且 $n_1+n_2=N$。由于方差函数 $V(n_1)$ 是凸函数，整数最小值必定位于包围连续最小值 $n_1^*$ 的两个整数之一。因此，$n_1$ 的候选整数值为 $\\lfloor n_1^* \\rfloor$ 和 $\\lceil n_1^* \\rceil$。为满足正值约束，我们必须从候选集合 $C = \\{\\max(1, \\lfloor n_1^* \\rfloor), \\min(N-1, \\lceil n_1^* \\rceil)\\}$ 中选择。我们为每个唯一的 $n_1 \\in C$ 计算方差 $V(n_1, N-n_1)$，并选择产生最小方差的那个。\n\n### 任务4：错选概率\n\n根据中心极限定理，样本均值差 $\\bar{Y}_1 - \\bar{Y}_2$ 的分布近似为高斯分布，其均值为 $\\mathbb{E}[\\bar{Y}_1 - \\bar{Y}_2] = f(x_1) - f(x_2) = \\Delta$，方差为 $V = \\operatorname{Var}(\\bar{Y}_1 - \\bar{Y}_2)$。如果我们不失一般性地假设 $\\Delta > 0$（即 $x_1$ 是更优的点），那么当我们观测到 $\\bar{Y}_1 - \\bar{Y}_2  0$ 时，就发生了错选。这一事件的概率是：\n$$\nP(\\bar{Y}_1 - \\bar{Y}_2  0) = P\\left(\\frac{(\\bar{Y}_1 - \\bar{Y}_2) - \\Delta}{\\sqrt{V}}  \\frac{0 - \\Delta}{\\sqrt{V}}\\right) \\approx \\Phi\\left(-\\frac{\\Delta}{\\sqrt{V}}\\right)\n$$\n其中 $\\Phi(\\cdot)$ 是标准正态分布的累积分布函数（CDF）。为了处理正负 $\\Delta$ 的情况，我们使用其绝对值 $|\\Delta|$。因此，选择较差点的概率近似为：\n$$\np = \\Phi\\left(-\\frac{|\\Delta|}{\\sqrt{V}}\\right)\n$$\n这个公式量化了通过 CRN 和最优分配实现的方差缩减如何转化为成对比较可靠性的提高。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef calculate_variance(n1, n2, s1, s2, rho):\n    \"\"\"Calculates the variance of the sample mean difference.\"\"\"\n    if n1 == 0 or n2 == 0:\n        return np.inf\n    var = (s1**2 / n1) + (s2**2 / n2)\n    if rho > 0:\n        cov_term = 2 * rho * s1 * s2 * min(n1, n2) / (n1 * n2)\n        var -= cov_term\n    return var\n\ndef get_continuous_allocation(N, s1, s2, rho):\n    \"\"\"Calculates the optimal continuous allocation n1_star.\"\"\"\n    if rho == 0:\n        if s1 + s2 == 0:\n            return N / 2.0\n        return N * s1 / (s1 + s2)\n\n    s1_sq = s1**2\n    s2_sq = s2**2\n    rho_term = 2 * rho * s1 * s2\n\n    # Balanced allocation regime\n    if abs(s1_sq - s2_sq) = rho_term:\n        return N / 2.0\n    \n    # Unbalanced allocation regimes\n    if s1_sq > s2_sq: # Regime where n1 > n2 likely\n        term_sqrt = s1_sq - rho_term\n        if term_sqrt = 0: # If variance term becomes non-positive, defaults to balanced\n            return N / 2.0\n        s1_eff = np.sqrt(term_sqrt)\n        return N * s1_eff / (s1_eff + s2)\n    else: # Regime where n2 > n1 likely (s2_sq > s1_sq)\n        term_sqrt = s2_sq - rho_term\n        if term_sqrt = 0:\n            return N / 2.0\n        s2_eff = np.sqrt(term_sqrt)\n        return N * s1 / (s1 + s2_eff)\n\ndef get_integer_allocation(n1_star, N, s1, s2, rho):\n    \"\"\"Finds the best integer allocation by checking neighbors of the continuous solution.\"\"\"\n    # Candidate n1 values are the floor and ceiling of n1_star, clipped to [1, N-1]\n    n1_candidates = {\n        max(1, int(np.floor(n1_star))),\n        min(N - 1, int(np.ceil(n1_star)))\n    }\n\n    best_n1 = -1\n    min_variance = np.inf\n\n    for n1_cand in n1_candidates:\n        n2_cand = N - n1_cand\n        if n1_cand  1 or n2_cand  1:\n            continue\n        \n        current_variance = calculate_variance(n1_cand, n2_cand, s1, s2, rho)\n        if current_variance  min_variance:\n            min_variance = current_variance\n            best_n1 = n1_cand\n            \n    return best_n1, N - best_n1\n\ndef solve_case(N, s1, s2, rho, delta):\n    \"\"\"Computes all required metrics for a single test case.\"\"\"\n    \n    # 1. Non-CRN case (rho = 0)\n    n1_noc_star = get_continuous_allocation(N, s1, s2, 0.0)\n    n1_noc, n2_noc = get_integer_allocation(n1_noc_star, N, s1, s2, 0.0)\n    v_noc = calculate_variance(n1_noc, n2_noc, s1, s2, 0.0)\n    p_noc = norm.cdf(-abs(delta) / np.sqrt(v_noc))\n    \n    # 2. CRN case\n    n1_crn_star = get_continuous_allocation(N, s1, s2, rho)\n    n1_crn, n2_crn = get_integer_allocation(n1_crn_star, N, s1, s2, rho)\n    v_crn = calculate_variance(n1_crn, n2_crn, s1, s2, rho)\n    p_crn = norm.cdf(-abs(delta) / np.sqrt(v_crn))\n    \n    return [n1_crn, n2_crn, v_crn, p_crn, n1_noc, n2_noc, v_noc, p_noc]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (N, sigma1, sigma2, rho, delta)\n        (40, 1.0, 2.0, 0.5, 0.4),\n        (40, 1.5, 0.8, 0.0, 0.3),\n        (40, 1.0, 1.5, 0.9, 0.1),\n        (100, 0.5, 3.0, 0.6, 0.7),\n        (2, 1.0, 1.0, 0.8, 0.2),\n    ]\n\n    results = []\n    for case in test_cases:\n        N, s1, s2, rho, delta = case\n        result = solve_case(N, s1, s2, rho, delta)\n        # Format each number to have a consistent representation\n        results.append(f\"[{','.join(f'{x:.10f}' for x in result)}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}