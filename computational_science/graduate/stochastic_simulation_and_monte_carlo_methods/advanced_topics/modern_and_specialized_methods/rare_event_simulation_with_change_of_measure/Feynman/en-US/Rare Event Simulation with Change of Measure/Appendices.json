{
    "hands_on_practices": [
        {
            "introduction": "The most direct way to understand the mechanics of importance sampling via exponential tilting is to apply it to a familiar, analytically tractable case. This exercise uses the sum of Gaussian random variables—a cornerstone of probability theory—to provide a step-by-step derivation of the key components. By working through this problem, you will see precisely how the cumulant generating function is used to find the optimal tilting parameter and how this, in turn, defines both the new sampling measure and the corresponding likelihood ratio needed for an unbiased estimate. ",
            "id": "3335110",
            "problem": "Consider independent and identically distributed random variables $X_1,\\dots,X_n$ with common distribution $\\mathcal{N}(m,\\sigma^2)$, where $m \\in \\mathbb{R}$ and $\\sigma^2 \\in (0,\\infty)$. Let $S_n \\doteq \\sum_{i=1}^{n} X_i$ and fix a constant $a \\in \\mathbb{R}$ with $a>m$. The rare-event probability of interest is $p_n \\doteq \\mathbb{P}(S_n \\ge a n)$. For importance sampling of $p_n$, consider exponential tilting (Esscher transform) based on the cumulant generating function $K(t) \\doteq \\ln \\mathbb{E}[\\exp(t X_1)]$, which induces, for each $t$ in the effective domain of $K$, a tilted law $\\mathbb{P}_t$ under which the one-step density is modified by $f_t(x) \\propto \\exp(t x) f(x)$, and the pathwise likelihood ratio (Radon–Nikodym derivative) for a trajectory $(X_1,\\dots,X_n)$ is $L_n(t) \\doteq \\frac{\\mathrm{d}\\mathbb{P}}{\\mathrm{d}\\mathbb{P}_t}(X_1,\\dots,X_n)$. The large-deviation motivated choice $t=\\theta^\\star$ is the unique solution to $K'(\\theta^\\star)=a$, which enforces that the tilted mean equals the rare-event threshold level.\n\nStarting from these definitions and without assuming any Gaussian-specific shortcut beyond standard properties of the moment generating function, explicitly compute:\n- the optimal tilting parameter $\\theta^\\star$,\n- the tilted distribution of $X_i$ under $\\mathbb{P}_{\\theta^\\star}$,\n- and the corresponding likelihood ratio $L_n^\\star \\doteq L_n(\\theta^\\star)$ to be used with the indicator $\\mathbf{1}\\{S_n \\ge a n\\}$.\n\nExpress all results symbolically in terms of $m$, $\\sigma^2$, $a$, $n$, and $S_n$. Provide your final answer as a row matrix containing, in order, $\\theta^\\star$, the tilted one-step law, and $L_n^\\star$. No numerical approximation is required.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It is a standard problem in the field of rare event simulation and large deviations theory. We proceed to solve it by computing the requested quantities in three steps.\n\nThe random variables $X_i$ are independent and identically distributed (i.i.d.) following a normal distribution $X_i \\sim \\mathcal{N}(m, \\sigma^2)$. The probability density function (PDF) of $X_i$ is given by\n$$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-m)^2}{2\\sigma^2}\\right)$$\nThe problem requires us to find the optimal tilting parameter $\\theta^\\star$, the tilted distribution under this parameter, and the corresponding likelihood ratio $L_n^\\star$.\n\n### 1. Computation of the Optimal Tilting Parameter $\\theta^\\star$\n\nThe optimal tilting parameter $\\theta^\\star$ is defined as the unique solution to the equation $K'(\\theta^\\star) = a$, where $K(t)$ is the cumulant generating function (CGF) of $X_1$.\n\nThe CGF is the natural logarithm of the moment generating function (MGF), $K(t) = \\ln \\mathbb{E}[\\exp(t X_1)]$. First, we compute the MGF of $X_1 \\sim \\mathcal{N}(m, \\sigma^2)$.\n$$\n\\mathbb{E}[\\exp(t X_1)] = \\int_{-\\infty}^{\\infty} \\exp(tx) \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-m)^2}{2\\sigma^2}\\right) \\mathrm{d}x\n$$\nThe argument of the exponent is\n$$\ntx - \\frac{(x-m)^2}{2\\sigma^2} = tx - \\frac{x^2 - 2mx + m^2}{2\\sigma^2} = -\\frac{1}{2\\sigma^2} [x^2 - 2mx - 2\\sigma^2tx + m^2]\n$$\nWe complete the square for the terms involving $x$:\n$$\nx^2 - 2(m + \\sigma^2 t)x + m^2 = [x - (m + \\sigma^2 t)]^2 - (m + \\sigma^2 t)^2 + m^2\n$$\n$$\n= [x - (m + \\sigma^2 t)]^2 - (m^2 + 2m\\sigma^2 t + \\sigma^4 t^2) + m^2 = [x - (m + \\sigma^2 t)]^2 - 2m\\sigma^2 t - \\sigma^4 t^2\n$$\nSubstituting this back into the exponent argument:\n$$\n-\\frac{1}{2\\sigma^2} \\left( [x - (m + \\sigma^2 t)]^2 - 2m\\sigma^2 t - \\sigma^4 t^2 \\right) = -\\frac{[x - (m + \\sigma^2 t)]^2}{2\\sigma^2} + mt + \\frac{1}{2}\\sigma^2 t^2\n$$\nThe MGF becomes:\n$$\n\\mathbb{E}[\\exp(t X_1)] = \\exp\\left(mt + \\frac{1}{2}\\sigma^2 t^2\\right) \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{[x - (m + \\sigma^2 t)]^2}{2\\sigma^2}\\right) \\mathrm{d}x\n$$\nThe integral is over the PDF of a normal distribution with mean $m + \\sigma^2 t$ and variance $\\sigma^2$, so it integrates to $1$. Thus, the MGF is:\n$$\nM_{X_1}(t) = \\exp\\left(mt + \\frac{1}{2}\\sigma^2 t^2\\right)\n$$\nThe CGF is the natural logarithm of the MGF:\n$$\nK(t) = \\ln(M_{X_1}(t)) = mt + \\frac{1}{2}\\sigma^2 t^2\n$$\nThe effective domain for $t$ is $\\mathbb{R}$. Now, we find its derivative with respect to $t$:\n$$\nK'(t) = \\frac{\\mathrm{d}}{\\mathrm{d}t} \\left(mt + \\frac{1}{2}\\sigma^2 t^2\\right) = m + \\sigma^2 t\n$$\nThe optimal parameter $\\theta^\\star$ solves $K'(\\theta^\\star) = a$:\n$$\nm + \\sigma^2 \\theta^\\star = a\n$$\nSolving for $\\theta^\\star$, we obtain:\n$$\n\\theta^\\star = \\frac{a - m}{\\sigma^2}\n$$\nGiven that $a > m$ and $\\sigma^2 > 0$, we have $\\theta^\\star > 0$.\n\n### 2. Computation of the Tilted Distribution\n\nThe tilted density $f_t(x)$ is defined by $f_t(x) \\propto \\exp(tx)f(x)$. The normalized density is:\n$$\nf_t(x) = \\frac{\\exp(tx)f(x)}{\\mathbb{E}[\\exp(tX_1)]} = \\exp(tx - K(t)) f(x)\n$$\nWe substitute the expressions for $f(x)$ and $K(t)$ at $t=\\theta^\\star$:\n$$\nf_{\\theta^\\star}(x) = \\exp\\left(\\theta^\\star x - K(\\theta^\\star)\\right) \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-m)^2}{2\\sigma^2}\\right)\n$$\n$$\nf_{\\theta^\\star}(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(\\theta^\\star x - K(\\theta^\\star) - \\frac{(x-m)^2}{2\\sigma^2}\\right)\n$$\nLet's analyze the argument of the exponent. We use the expressions for $\\theta^\\star$ and $K(\\theta^\\star)$ found previously.\nThe exponent's argument is:\n$$\n\\theta^\\star x - (m\\theta^\\star + \\frac{1}{2}\\sigma^2(\\theta^\\star)^2) - \\frac{x^2 - 2mx + m^2}{2\\sigma^2}\n$$\n$$\n= -\\frac{x^2}{2\\sigma^2} + \\left(\\frac{m}{\\sigma^2} + \\theta^\\star\\right)x - \\left(\\frac{m^2}{2\\sigma^2} + m\\theta^\\star + \\frac{1}{2}\\sigma^2(\\theta^\\star)^2\\right)\n$$\nWe identify the new mean $m_{\\theta^\\star}$ from the term linear in $x$. For a normal density with variance $\\sigma^2$, the exponent is of the form $-\\frac{(x-m_{\\theta^\\star})^2}{2\\sigma^2} = -\\frac{x^2}{2\\sigma^2} + \\frac{m_{\\theta^\\star}}{\\sigma^2}x - \\frac{m_{\\theta^\\star}^2}{2\\sigma^2}$.\nComparing coefficients of $x$, we have:\n$$\n\\frac{m_{\\theta^\\star}}{\\sigma^2} = \\frac{m}{\\sigma^2} + \\theta^\\star \\implies m_{\\theta^\\star} = m + \\sigma^2 \\theta^\\star\n$$\nSubstituting $\\theta^\\star = \\frac{a-m}{\\sigma^2}$:\n$$\nm_{\\theta^\\star} = m + \\sigma^2 \\left(\\frac{a-m}{\\sigma^2}\\right) = m + a - m = a\n$$\nThis is consistent with the property that the mean of the tilted distribution is $K'(\\theta^\\star)=a$. The variance term, determined by the coefficient of $x^2$, remains $\\sigma^2$. The resulting tilted density is:\n$$\nf_{\\theta^\\star}(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-a)^2}{2\\sigma^2}\\right)\n$$\nThis is the PDF of a normal distribution with mean $a$ and variance $\\sigma^2$. Therefore, the tilted one-step law is $\\mathcal{N}(a, \\sigma^2)$.\n\n### 3. Computation of the Likelihood Ratio $L_n^\\star$\n\nThe pathwise likelihood ratio is the Radon-Nikodym derivative $L_n(t) = \\frac{\\mathrm{d}\\mathbb{P}}{\\mathrm{d}\\mathbb{P}_t}$. Since the variables $X_1, \\dots, X_n$ are i.i.d. under both measures, the ratio of joint densities is the product of the marginal ratios:\n$$\nL_n(t) = \\frac{\\prod_{i=1}^n f(X_i)}{\\prod_{i=1}^n f_t(X_i)} = \\prod_{i=1}^n \\frac{f(X_i)}{f_t(X_i)}\n$$\nFrom the definition of $f_t(x)$, we have $\\frac{f(x)}{f_t(x)} = \\exp(-tx + K(t))$. Therefore:\n$$\nL_n(t) = \\prod_{i=1}^n \\exp(-tX_i + K(t)) = \\exp\\left(\\sum_{i=1}^n (-tX_i + K(t))\\right)\n$$\n$$\nL_n(t) = \\exp\\left(-t \\sum_{i=1}^n X_i + n K(t)\\right) = \\exp(-t S_n + nK(t))\n$$\nWe need to evaluate this at $t=\\theta^\\star$. Let $L_n^\\star = L_n(\\theta^\\star)$:\n$$\nL_n^\\star = \\exp(-\\theta^\\star S_n + nK(\\theta^\\star))\n$$\nWe substitute $\\theta^\\star = \\frac{a-m}{\\sigma^2}$ and compute $K(\\theta^\\star)$:\n$$\nK(\\theta^\\star) = m\\theta^\\star + \\frac{1}{2}\\sigma^2 (\\theta^\\star)^2 = m\\left(\\frac{a-m}{\\sigma^2}\\right) + \\frac{1}{2}\\sigma^2 \\left(\\frac{a-m}{\\sigma^2}\\right)^2\n$$\n$$\nK(\\theta^\\star) = \\frac{m(a-m)}{\\sigma^2} + \\frac{(a-m)^2}{2\\sigma^2} = \\frac{2m(a-m) + (a-m)^2}{2\\sigma^2} = \\frac{(a-m)(2m + a - m)}{2\\sigma^2}\n$$\n$$\nK(\\theta^\\star) = \\frac{(a-m)(a+m)}{2\\sigma^2} = \\frac{a^2 - m^2}{2\\sigma^2}\n$$\nFinally, we substitute $\\theta^\\star$ and $K(\\theta^\\star)$ into the expression for $L_n^\\star$:\n$$\nL_n^\\star = \\exp\\left( - \\left(\\frac{a-m}{\\sigma^2}\\right) S_n + n \\left(\\frac{a^2-m^2}{2\\sigma^2}\\right) \\right)\n$$\nThis expression depends on the specified variables $m, \\sigma^2, a, n$, and the sample sum $S_n$.\n\nThe three requested quantities are:\n1.  $\\theta^\\star = \\frac{a-m}{\\sigma^2}$\n2.  The tilted one-step law is $\\mathcal{N}(a, \\sigma^2)$\n3.  $L_n^\\star = \\exp\\left( - \\frac{a-m}{\\sigma^2} S_n + n\\frac{a^2-m^2}{2\\sigma^2} \\right)$\nThese results are collected into a single row matrix for the final answer.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{a-m}{\\sigma^2} & \\mathcal{N}(a, \\sigma^2) & \\exp\\left( - \\frac{a-m}{\\sigma^2} S_n + n\\frac{a^2-m^2}{2\\sigma^2} \\right) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "In real-world applications, the optimal change of measure is often unknown and must be estimated using pilot simulations. This raises a crucial question: can we reuse the data from these pilot runs in our final estimate? This exercise exposes a subtle but critical pitfall of such adaptive schemes, demonstrating how reusing data for both parameter selection and estimation introduces a systematic bias. By calculating this bias explicitly in a simple Bernoulli model, you will gain a deeper appreciation for the care required when designing and implementing adaptive importance sampling algorithms. ",
            "id": "3335116",
            "problem": "Consider estimating a rare-event probability under a change of measure. Let $X$ be a Bernoulli random variable under the nominal measure $\\mathbb{P}_{p}$ with $\\mathbb{P}_{p}(X=1)=p$, where $p \\in (0,1)$ is small. For importance sampling (IS), consider proposal measures $\\mathbb{Q}_{q}$ under which $X$ is Bernoulli with parameter $q \\in (0,1)$. The Radon–Nikodym derivative is $L_{q}(X)=\\frac{\\mathrm{d}\\mathbb{P}_{p}}{\\mathrm{d}\\mathbb{Q}_{q}}(X)$, so that $L_{q}(1)=\\frac{p}{q}$ and $L_{q}(0)=\\frac{1-p}{1-q}$. The ordinary IS estimator of $p$ using $n$ independent and identically distributed samples $X_{1},\\dots,X_{n}$ from $\\mathbb{Q}_{q}$ is\n$$\n\\widehat{p}(q)=\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\{X_{i}=1\\}\\,\\frac{p}{q},\n$$\nwhich is unbiased for $p$ when $q$ is fixed and independent of the data used in the estimator.\n\nNow construct the following $2$-stage adaptive scheme with two candidate proposals $q_{1},q_{2}\\in (0,1)$ and an integer threshold $\\tau\\in\\{1,2,\\dots,n\\}$.\n\nStage $1$ (pilot, change-of-measure selection):\n- Generate $n$ independent and identically distributed samples $X^{(1)}_{1},\\dots,X^{(1)}_{n}$ from $\\mathbb{Q}_{q_{1}}$ and independently generate $n$ independent and identically distributed samples $X^{(2)}_{1},\\dots,X^{(2)}_{n}$ from $\\mathbb{Q}_{q_{2}}$.\n- Let $S=\\sum_{i=1}^{n}X^{(1)}_{i}$. If $S\\ge \\tau$, select proposal $q=q_{1}$; otherwise, select proposal $q=q_{2}$.\n\nStage $2$ (estimation with data reuse):\n- Produce the adaptive estimator\n$$\n\\widehat{p}_{\\mathrm{adapt}}=\\mathbf{1}\\{S\\ge \\tau\\}\\,\\widehat{p}(q_{1})+\\mathbf{1}\\{S<\\tau\\}\\,\\widehat{p}(q_{2}),\n$$\nwhere $\\widehat{p}(q_{1})$ is computed from $X^{(1)}_{1},\\dots,X^{(1)}_{n}$ and $\\widehat{p}(q_{2})$ is computed from $X^{(2)}_{1},\\dots,X^{(2)}_{n}$, each using the ordinary importance sampling formula above.\n\nBecause the selection event $\\{S\\ge \\tau\\}$ depends on the same data used to compute $\\widehat{p}(q_{1})$, the estimator $\\widehat{p}_{\\mathrm{adapt}}$ is biased in general.\n\nDerive a closed-form expression for the bias $\\mathbb{E}[\\widehat{p}_{\\mathrm{adapt}}]-p$ in terms of $p$, $q_{1}$, $n$, and $\\tau$, expressing your answer in a single analytical expression. Your final answer must be a single closed-form expression. Do not report an inequality or an equation. No rounding is required.",
            "solution": "The problem asks for the bias of the adaptive importance sampling estimator $\\widehat{p}_{\\mathrm{adapt}}$, which is defined as $\\mathbb{E}[\\widehat{p}_{\\mathrm{adapt}}] - p$. The expectation is taken with respect to the sampling distributions. The samples $X^{(1)}_{1},\\dots,X^{(1)}_{n}$ are drawn independently from a Bernoulli distribution with parameter $q_{1}$ (denoted by the measure $\\mathbb{Q}_{q_{1}}$), and the samples $X^{(2)}_{1},\\dots,X^{(2)}_{n}$ are drawn independently from a Bernoulli distribution with parameter $q_{2}$ (denoted by $\\mathbb{Q}_{q_{2}}$). Let $\\mathbb{Q}$ be the joint product measure.\n\nThe adaptive estimator is given by\n$$\n\\widehat{p}_{\\mathrm{adapt}}=\\mathbf{1}\\{S\\ge \\tau\\}\\,\\widehat{p}(q_{1})+\\mathbf{1}\\{S<\\tau\\}\\,\\widehat{p}(q_{2})\n$$\nwhere $S=\\sum_{i=1}^{n}X^{(1)}_{i}$. The expectation of $\\widehat{p}_{\\mathrm{adapt}}$ under the sampling measure is\n$$\n\\mathbb{E}_{\\mathbb{Q}}[\\widehat{p}_{\\mathrm{adapt}}] = \\mathbb{E}_{\\mathbb{Q}}[\\mathbf{1}\\{S\\ge \\tau\\}\\,\\widehat{p}(q_{1})] + \\mathbb{E}_{\\mathbb{Q}}[\\mathbf{1}\\{S<\\tau\\}\\,\\widehat{p}(q_{2})]\n$$\nWe analyze each term separately.\n\nFor the second term, the event $\\{S<\\tau\\}$ depends only on the samples $X^{(1)}_{1},\\dots,X^{(1)}_{n}$, while the estimator $\\widehat{p}(q_{2})$ depends only on the independent samples $X^{(2)}_{1},\\dots,X^{(2)}_{n}$. Therefore, we can separate the expectation:\n$$\n\\mathbb{E}_{\\mathbb{Q}}[\\mathbf{1}\\{S<\\tau\\}\\,\\widehat{p}(q_{2})] = \\mathbb{E}_{\\mathbb{Q}_{q_1}}[\\mathbf{1}\\{S<\\tau\\}] \\cdot \\mathbb{E}_{\\mathbb{Q}_{q_2}}[\\widehat{p}(q_{2})]\n$$\nThe problem states that $\\widehat{p}(q)$ is an unbiased estimator for $p$. This means its expectation under the sampling measure is $p$. Thus, $\\mathbb{E}_{\\mathbb{Q}_{q_2}}[\\widehat{p}(q_{2})] = p$. Let $\\mathbb{Q}_{q_1}(S<\\tau)$ denote the probability of the event $\\{S<\\tau\\}$ under the measure $\\mathbb{Q}_{q_1}$. The second term simplifies to $p \\cdot \\mathbb{Q}_{q_1}(S<\\tau)$.\n\nFor the first term, both the indicator $\\mathbf{1}\\{S\\ge \\tau\\}$ and the estimator $\\widehat{p}(q_{1})$ depend on the same set of samples $X^{(1)}_{1},\\dots,X^{(1)}_{n}$. The estimator $\\widehat{p}(q_{1})$ is defined as\n$$\n\\widehat{p}(q_{1}) = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\{X^{(1)}_{i}=1\\}\\,\\frac{p}{q_{1}}\n$$\nSince $\\mathbf{1}\\{X^{(1)}_{i}=1\\}$ is simply $X^{(1)}_{i}$ for a Bernoulli variable, and $S = \\sum_{i=1}^{n} X^{(1)}_{i}$, we can write $\\widehat{p}(q_{1}) = \\frac{1}{n} S \\frac{p}{q_{1}}$.\nThe first term is then\n$$\n\\mathbb{E}_{\\mathbb{Q}}[\\mathbf{1}\\{S\\ge \\tau\\}\\,\\widehat{p}(q_{1})] = \\mathbb{E}_{\\mathbb{Q}_{q_1}}\\left[\\mathbf{1}\\{S\\ge \\tau\\} \\frac{S}{n} \\frac{p}{q_{1}}\\right] = \\frac{p}{n q_{1}} \\mathbb{E}_{\\mathbb{Q}_{q_1}}[S \\cdot \\mathbf{1}\\{S\\ge \\tau\\}]\n$$\nCombining both terms, the expected value of the adaptive estimator is\n$$\n\\mathbb{E}_{\\mathbb{Q}}[\\widehat{p}_{\\mathrm{adapt}}] = \\frac{p}{n q_{1}} \\mathbb{E}_{\\mathbb{Q}_{q_1}}[S \\cdot \\mathbf{1}\\{S\\ge \\tau\\}] + p \\cdot \\mathbb{Q}_{q_1}(S<\\tau)\n$$\nThe bias is $\\mathbb{E}_{\\mathbb{Q}}[\\widehat{p}_{\\mathrm{adapt}}] - p$. We can write $p$ as $p=p \\cdot (\\mathbb{Q}_{q_1}(S\\ge \\tau) + \\mathbb{Q}_{q_1}(S<\\tau))$, since the probabilities sum to $1$.\n$$\n\\text{Bias} = \\left( \\frac{p}{n q_{1}} \\mathbb{E}_{\\mathbb{Q}_{q_1}}[S \\cdot \\mathbf{1}\\{S\\ge \\tau\\}] + p \\cdot \\mathbb{Q}_{q_1}(S<\\tau) \\right) - p \\cdot (\\mathbb{Q}_{q_1}(S\\ge \\tau) + \\mathbb{Q}_{q_1}(S<\\tau))\n$$\n$$\n\\text{Bias} = \\frac{p}{n q_{1}} \\mathbb{E}_{\\mathbb{Q}_{q_1}}[S \\cdot \\mathbf{1}\\{S\\ge \\tau\\}] - p \\cdot \\mathbb{Q}_{q_1}(S\\ge \\tau)\n$$\nUnder the measure $\\mathbb{Q}_{q_1}$, the variables $X^{(1)}_{i}$ are i.i.d. Bernoulli with parameter $q_{1}$. Their sum $S$ follows a binomial distribution, $S \\sim \\text{Binomial}(n, q_{1})$. The probability mass function (PMF) is $\\mathbb{Q}_{q_1}(S=k) = \\binom{n}{k} q_{1}^k (1-q_{1})^{n-k}$.\n\nThe partial expectation $\\mathbb{E}_{\\mathbb{Q}_{q_1}}[S \\cdot \\mathbf{1}\\{S\\ge \\tau\\}]$ is\n$$\n\\mathbb{E}_{\\mathbb{Q}_{q_1}}[S \\cdot \\mathbf{1}\\{S\\ge \\tau\\}] = \\sum_{k=\\tau}^{n} k \\cdot \\mathbb{Q}_{q_1}(S=k) = \\sum_{k=\\tau}^{n} k \\binom{n}{k} q_{1}^k (1-q_{1})^{n-k}\n$$\nUsing the identity $k \\binom{n}{k} = n \\binom{n-1}{k-1}$ for $k\\geq 1$ (and noting $\\tau \\geq 1$):\n$$\n\\sum_{k=\\tau}^{n} n \\binom{n-1}{k-1} q_{1}^k (1-q_{1})^{n-k} = n q_{1} \\sum_{k=\\tau}^{n} \\binom{n-1}{k-1} q_{1}^{k-1} (1-q_{1})^{n-k}\n$$\nLet $j=k-1$. The sum becomes\n$$\nn q_{1} \\sum_{j=\\tau-1}^{n-1} \\binom{n-1}{j} q_{1}^{j} (1-q_{1})^{(n-1)-j}\n$$\nThis sum is the tail probability $\\mathbb{Q}_{q_1}(Y \\ge \\tau-1)$, where $Y$ is a random variable with distribution $Y \\sim \\text{Binomial}(n-1, q_{1})$.\nSo, $\\mathbb{E}_{\\mathbb{Q}_{q_1}}[S \\cdot \\mathbf{1}\\{S\\ge \\tau\\}] = n q_{1} \\mathbb{Q}_{q_1}(Y \\ge \\tau-1)$.\n\nSubstituting this back into the expression for the bias:\n$$\n\\text{Bias} = \\frac{p}{n q_{1}} (n q_{1} \\mathbb{Q}_{q_1}(Y \\ge \\tau-1)) - p \\cdot \\mathbb{Q}_{q_1}(S\\ge \\tau) = p (\\mathbb{Q}_{q_1}(Y \\ge \\tau-1) - \\mathbb{Q}_{q_1}(S\\ge \\tau))\n$$\nWe can find a simpler form for this difference of probabilities using the relationship between the distributions of $S$ and $Y$. Let $S = Y + X^{(1)}_{n}$, where $Y \\sim \\text{Binomial}(n-1, q_{1})$ and $X^{(1)}_n \\sim \\text{Bernoulli}(q_{1})$ are independent.\nThe law of total probability gives:\n$$\n\\mathbb{Q}_{q_1}(S \\ge \\tau) = \\mathbb{Q}_{q_1}(Y+X^{(1)}_{n} \\ge \\tau) = \\mathbb{Q}_{q_1}(Y+1 \\ge \\tau)\\mathbb{Q}_{q_1}(X^{(1)}_{n}=1) + \\mathbb{Q}_{q_1}(Y \\ge \\tau)\\mathbb{Q}_{q_1}(X^{(1)}_{n}=0)\n$$\n$$\n\\mathbb{Q}_{q_1}(S \\ge \\tau) = \\mathbb{Q}_{q_1}(Y \\ge \\tau-1) \\cdot q_{1} + \\mathbb{Q}_{q_1}(Y \\ge \\tau) \\cdot (1-q_{1})\n$$\nThis is a standard recurrence relation for binomial tail probabilities. We can now simplify the term in the bias expression:\n$$\n\\mathbb{Q}_{q_1}(Y \\ge \\tau-1) - \\mathbb{Q}_{q_1}(S \\ge \\tau) = \\mathbb{Q}_{q_1}(Y \\ge \\tau-1) - [q_{1}\\mathbb{Q}_{q_1}(Y \\ge \\tau-1) + (1-q_{1})\\mathbb{Q}_{q_1}(Y \\ge \\tau)]\n$$\n$$\n= (1-q_{1})\\mathbb{Q}_{q_1}(Y \\ge \\tau-1) - (1-q_{1})\\mathbb{Q}_{q_1}(Y \\ge \\tau)\n$$\n$$\n= (1-q_{1})[\\mathbb{Q}_{q_1}(Y \\ge \\tau-1) - \\mathbb{Q}_{q_1}(Y \\ge \\tau)]\n$$\nThe difference in the brackets is precisely the probability that $Y = \\tau-1$.\n$$\n= (1-q_{1})\\mathbb{Q}_{q_1}(Y = \\tau-1)\n$$\nThe PMF for $Y \\sim \\text{Binomial}(n-1, q_{1})$ at $\\tau-1$ is:\n$$\n\\mathbb{Q}_{q_1}(Y=\\tau-1) = \\binom{n-1}{\\tau-1} q_{1}^{\\tau-1} (1-q_{1})^{(n-1)-(\\tau-1)} = \\binom{n-1}{\\tau-1} q_{1}^{\\tau-1} (1-q_{1})^{n-\\tau}\n$$\nThe bias is therefore:\n$$\n\\text{Bias} = p \\cdot (1-q_{1}) \\cdot \\left( \\binom{n-1}{\\tau-1} q_{1}^{\\tau-1} (1-q_{1})^{n-\\tau} \\right)\n$$\n$$\n\\text{Bias} = p \\binom{n-1}{\\tau-1} q_{1}^{\\tau-1} (1-q_{1})^{n-\\tau+1}\n$$\nThis is the final closed-form expression for the bias, in terms of $p$, $n$, $q_{1}$, and $\\tau$. As expected, the parameter $q_{2}$ does not appear due to the independence of the second-stage sample set when the second proposal is chosen.",
            "answer": "$$\\boxed{p \\binom{n-1}{\\tau-1} q_{1}^{\\tau-1} (1-q_{1})^{n-\\tau+1}}$$"
        },
        {
            "introduction": "This final practice bridges the gap between analytical exercises and applied rare event simulation by tackling a realistic network reliability problem. You are tasked with implementing a complete importance sampling estimator, from deriving importance weights using graph-theoretic concepts like effective resistance to running the Monte Carlo simulation itself. This hands-on coding challenge requires you to synthesize the principles of change of measure, likelihood ratios, and statistical estimation into a functional program, providing a powerful demonstration of how these theoretical tools solve complex, practical problems. ",
            "id": "3335056",
            "problem": "Consider an undirected, simple, connected graph with nodes indexed as integers starting at $0$. Each edge $e$ fails independently according to a Bernoulli random variable $E_e \\sim \\mathrm{Bern}(p_e)$ under a baseline probability measure $\\mathbb{P}$. Define the rare event as the graph becoming disconnected after failures. The task is to design and implement a change of measure for importance sampling to estimate the disconnection probability $\\theta = \\mathbb{P}(\\text{disconnected})$.\n\nStart from the following foundational elements:\n- Independent Bernoulli failures $E_e \\sim \\mathrm{Bern}(p_e)$.\n- Importance sampling with a change of measure $\\mathbb{Q}$ that preserves independence but replaces $p_e$ with tilted probabilities $p_e'$.\n- Effective resistance between nodes, derived from the graph Laplacian and its Moore–Penrose pseudoinverse, as an importance metric for edges.\n\nYou must implement a tilt that is edge-specific and proportional to edge importance, using effective resistance. Specifically, for each edge $e = (i,j)$, compute its effective resistance $R_{ij}$ using the Laplacian $L$ of the original graph (with unit edge weights), the Moore–Penrose pseudoinverse $L^{+}$, and the formula\n$$\nR_{ij} = (e_i - e_j)^\\top L^{+} (e_i - e_j),\n$$\nwhere $e_k$ is the standard basis vector in $\\mathbb{R}^n$ with a $1$ at position $k$ and $0$ elsewhere, and $n$ is the number of nodes.\n\nNormalize the edge importance weights by\n$$\nw_e = \\frac{R_{ij}}{\\max_{f} R_f},\n$$\nso that $w_e \\in [0,1]$ and all edges retain a valid importance score. Using these weights, implement a logistic tilt with intensity parameter $\\alpha > 0$:\n$$\n\\mathrm{logit}(p_e') = \\mathrm{logit}(p_e) + \\alpha w_e,\n$$\nwhere $\\mathrm{logit}(x) = \\log\\left(\\frac{x}{1-x}\\right)$ and the inverse-logit $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ gives $p_e' = \\sigma(\\mathrm{logit}(p_e) + \\alpha w_e)$.\n\nUnder the tilted measure $\\mathbb{Q}$ in which $E_e \\sim \\mathrm{Bern}(p_e')$ independently across edges, estimate $\\theta$ via importance sampling using the unbiased estimator\n$$\n\\hat{\\theta}_N = \\frac{1}{N} \\sum_{k=1}^N \\mathbf{1}\\{\\text{disconnected after failures } \\mathbf{E}^{(k)}\\} \\cdot L(\\mathbf{E}^{(k)}),\n$$\nwhere $\\mathbf{E}^{(k)} = (E_e^{(k)})_e$ is the $k$-th sample under $\\mathbb{Q}$, and the likelihood ratio $L(\\mathbf{E})$ is\n$$\nL(\\mathbf{E}) = \\prod_{e} \\frac{p_e^{E_e} (1-p_e)^{1-E_e}}{(p_e')^{E_e} (1-p_e')^{1-E_e}}.\n$$\n\nConnectivity must be tested on the post-failure graph that retains all nodes and only the surviving edges (edges with $E_e = 0$). Use a breadth-first search from node $0$ and verify that the number of visited nodes equals $n$. If angles or physical units were to appear, you would be required to specify radians or a physical unit; however, this problem involves only unitless probabilities and graph-theoretic quantities, so no physical units are needed.\n\nImplement the above estimator and produce numerical outputs for the following test suite. For reproducibility, use a fixed pseudorandom generator seed for each case as specified.\n\nTest suite:\n- Case $1$ (moderate probability, path graph):\n  - Nodes $n = 5$.\n  - Edge list $[(0,1),(1,2),(2,3),(3,4)]$.\n  - Baseline probabilities $p_e = 0.02$ for all $e$.\n  - Tilt intensity $\\alpha = 2.0$.\n  - Samples $N = 30000$.\n  - Seed $s = 12345$.\n- Case $2$ (rare event, complete graph):\n  - Nodes $n = 5$.\n  - Edge list is all unordered pairs $(i,j)$ with $0 \\le i < j \\le 4$.\n  - Baseline probabilities $p_e = 0.005$ for all $e$.\n  - Tilt intensity $\\alpha = 4.0$.\n  - Samples $N = 40000$.\n  - Seed $s = 23456$.\n- Case $3$ (heterogeneous importance, ladder graph):\n  - Nodes $n = 6$.\n  - Edge list $[(0,1),(2,3),(4,5),(0,2),(2,4),(1,3),(3,5)]$.\n  - Baseline probabilities $p_e = [0.01,0.01,0.01,0.02,0.02,0.02,0.02]$ in the order of the given edges.\n  - Tilt intensity $\\alpha = 3.0$.\n  - Samples $N = 40000$.\n  - Seed $s = 34567$.\n- Case $4$ (boundary check, no tilt, star graph):\n  - Nodes $n = 6$.\n  - Edge list $[(0,1),(0,2),(0,3),(0,4),(0,5)]$.\n  - Baseline probabilities $p_e = 0.01$ for all $e$.\n  - Tilt intensity $\\alpha = 0.0$.\n  - Samples $N = 30000$.\n  - Seed $s = 45678$.\n\nYour program must:\n- Compute effective resistances $R_{ij}$ from the original graph Laplacian $L$ and its pseudoinverse $L^{+}$.\n- Construct tilted probabilities $p_e'$ from $p_e$ via the logistic tilt with weights $w_e$ and intensity $\\alpha$.\n- Simulate $N$ independent samples of $\\mathbf{E}$ under $\\mathbb{Q}$.\n- For each sample, compute the connectivity indicator and the likelihood ratio $L(\\mathbf{E})$.\n- Output the final estimates $\\hat{\\theta}_N$ for all cases.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$). Each entry must be a float.",
            "solution": "The problem requires the estimation of the probability of graph disconnection, a rare event, using the importance sampling Monte Carlo method. The core of the task is to design and implement a specific change of measure, where the simulation probabilities are \"tilted\" towards configurations that cause disconnection. This tilt is guided by the graph-theoretic concept of effective resistance, which serves as a measure of edge criticality.\n\nThe solution is structured as a two-stage process. First, a pre-computation stage where we analyze the graph structure to determine edge importance and derive the tilted probabilities. Second, a simulation stage where we run a Monte Carlo experiment under the new probabilities and compute the unbiased estimator for the disconnection probability $\\theta$.\n\n**1. Graph Laplacian and Effective Resistance**\n\nThe foundation of the edge importance metric is the graph Laplacian. For a given undirected, simple graph $G=(V, E)$ with $n = |V|$ nodes, we first consider a version with unit weights on all edges. Its structure is captured by the adjacency matrix $A$, where $A_{ij} = 1$ if an edge exists between nodes $i$ and $j$, and $A_{ij} = 0$ otherwise. The degree matrix $D$ is a diagonal matrix with $D_{ii}$ being the degree of node $i$. The graph Laplacian $L$ is defined as:\n$$\nL = D - A\n$$\nThe Laplacian matrix is singular; its smallest eigenvalue is $0$ with a corresponding eigenvector of all ones, $(1, 1, \\dots, 1)^\\top$. This singularity prevents direct inversion. To proceed, we use the Moore-Penrose pseudoinverse, denoted $L^{+}$.\n\nEffective resistance, $R_{ij}$, between two nodes $i$ and $j$ is a concept derived from interpreting the graph as an electrical network where each edge is a unit resistor. It quantifies the voltage difference that would appear between $i$ and $j$ if a unit of current were injected at $i$ and withdrawn at $j$. A high effective resistance for an edge $(i,j)$ implies it is part of a bottleneck in the graph's connectivity. It is calculated using the pseudoinverse of the Laplacian:\n$$\nR_{ij} = (e_i - e_j)^\\top L^{+} (e_i - e_j)\n$$\nwhere $e_k$ is the standard basis vector in $\\mathbb{R}^n$ with a $1$ at position $k$. This is equivalent to $R_{ij} = L^{+}_{ii} + L^{+}_{jj} - 2L^{+}_{ij}$.\n\nThese resistances are then normalized to create dimensionless importance weights $w_e \\in [0, 1]$ for each edge $e$:\n$$\nw_e = \\frac{R_e}{\\max_{f} R_f}\n$$\nThis normalization ensures that the tilting mechanism is consistently applied across different graph structures.\n\n**2. Importance Sampling with a Logistic Tilt**\n\nThe objective is to estimate $\\theta = \\mathbb{P}(\\mathcal{D})$, where $\\mathcal{D}$ is the event of graph disconnection. Under the baseline measure $\\mathbb{P}$, each edge $e$ fails (is removed) independently with probability $p_e$. A naive Monte Carlo simulation would be inefficient because $\\mathcal{D}$ is a rare event.\n\nImportance sampling addresses this by introducing a new probability measure $\\mathbb{Q}$ under which the event $\\mathcal{D}$ is more frequent. We then correct for this change of measure using a likelihood ratio. The importance sampling estimator for $\\theta$ is:\n$$\n\\hat{\\theta}_N = \\frac{1}{N} \\sum_{k=1}^N \\mathbf{1}\\{\\mathbf{E}^{(k)} \\in \\mathcal{D}\\} \\cdot L(\\mathbf{E}^{(k)})\n$$\nHere, $\\mathbf{E}^{(k)}$ is the $k$-th sample of edge failures drawn from the tilted distribution $\\mathbb{Q}$, $\\mathbf{1}\\{\\cdot\\}$ is the indicator function, and $L(\\mathbf{E}^{(k)})$ is the likelihood ratio.\n\nThe problem specifies a logistic tilt. The new failure probability $p_e'$ for edge $e$ is defined via the logit function, $\\mathrm{logit}(x) = \\log(x/(1-x))$:\n$$\n\\mathrm{logit}(p_e') = \\mathrm{logit}(p_e) + \\alpha w_e\n$$\nwhere $\\alpha > 0$ is a parameter controlling the intensity of the tilt. A positive $\\alpha$ increases the failure probability for edges with high importance ($w_e > 0$), thus making disconnection more likely. The new probability $p_e'$ is recovered using the inverse-logit (sigmoid) function, $\\sigma(x) = (1 + e^{-x})^{-1}$:\n$$\np_e' = \\sigma(\\mathrm{logit}(p_e) + \\alpha w_e)\n$$\nUnder $\\mathbb{Q}$, edge failures remain independent, but with probabilities $p_e'$. The likelihood ratio $L(\\mathbf{E})$ for a given failure configuration $\\mathbf{E} = (E_e)_e$, where $E_e=1$ for failure and $E_e=0$ for survival, is the product of the individual edge likelihood ratios:\n$$\nL(\\mathbf{E}) = \\frac{d\\mathbb{P}}{d\\mathbb{Q}}(\\mathbf{E}) = \\prod_e \\frac{p_e^{E_e} (1-p_e)^{1-E_e}}{(p_e')^{E_e} (1-p_e')^{1-E_e}}\n$$\nFor numerical stability, it is preferable to compute the log-likelihood ratio first:\n$$\n\\log L(\\mathbf{E}) = \\sum_e \\left[ E_e (\\log p_e - \\log p_e') + (1-E_e) (\\log(1-p_e) - \\log(1-p_e')) \\right]\n$$\nand then exponentiate the result.\n\n**3. Algorithmic Implementation**\n\nThe overall algorithm for each test case proceeds as follows:\n\n1.  **Initialization**: Given the number of nodes $n$, the edge list, the baseline failure probabilities $p_e$, tilt intensity $\\alpha$, sample size $N$, and random seed $s$. Set the seed for reproducibility.\n\n2.  **Pre-computation**:\n    a. Construct the $n \\times n$ graph Laplacian $L$ assuming unit edge weights.\n    b. Compute the Moore-Penrose pseudoinverse $L^{+}$ using a standard linear algebra library function.\n    c. For each edge $e=(i,j)$ in the graph, calculate its effective resistance $R_{ij}$.\n    d. Find the maximum effective resistance and compute the normalized weights $w_e$.\n    e. Calculate the tilted failure probabilities $p_e'$ for all edges using the logistic tilt formula.\n    f. Pre-calculate the components of the log-likelihood ratio, i.e., $\\log p_e, \\log(1-p_e), \\log p_e', \\log(1-p_e')$, for all edges to use inside the main loop.\n\n3.  **Monte Carlo Simulation**:\n    a. Initialize a total estimator value, $\\Sigma = 0$.\n    b. Loop $N$ times from $k=1$ to $N$:\n        i.  Generate a vector of edge failures $\\mathbf{E}^{(k)}$ by drawing independent Bernoulli random variables $E_e^{(k)} \\sim \\mathrm{Bern}(p_e')$.\n        ii. Construct the post-failure graph, which includes all nodes but only the surviving edges (where $E_e^{(k)} = 0$).\n        iii. Check for connectivity. A breadth-first search (BFS) or depth-first search (DFS) is initiated from a starting node (e.g., node $0$). If the number of nodes visited is less than $n$, the graph is disconnected.\n        iv. If the graph is disconnected:\n            - Calculate the log-likelihood ratio $\\log L(\\mathbf{E}^{(k)})$ using the pre-computed values and the sampled failure vector $\\mathbf{E}^{(k)}$.\n            - Compute the likelihood ratio $L(\\mathbf{E}^{(k)}) = \\exp(\\log L(\\mathbf{E}^{(k)}))$.\n            - Add this value to the total: $\\Sigma = \\Sigma + L(\\mathbf{E}^{(k)})$.\n\n4.  **Final Estimate**:\n    a. The final estimated probability of disconnection is $\\hat{\\theta}_N = \\Sigma / N$.\n\nThis procedure is repeated for each of the four test cases specified in the problem statement. The case with $\\alpha=0$ serves as a baseline, corresponding to standard Monte Carlo simulation since $p_e' = p_e$ and the likelihood ratio is always $1$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\nimport collections\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1: (n, edges, p_e, alpha, N, seed)\n        (\n            5,\n            [(0, 1), (1, 2), (2, 3), (3, 4)],\n            0.02,\n            2.0,\n            30000,\n            12345\n        ),\n        # Case 2:\n        (\n            5,\n            [(i, j) for i in range(5) for j in range(i + 1, 5)],\n            0.005,\n            4.0,\n            40000,\n            23456\n        ),\n        # Case 3:\n        (\n            6,\n            [(0, 1), (2, 3), (4, 5), (0, 2), (2, 4), (1, 3), (3, 5)],\n            np.array([0.01, 0.01, 0.01, 0.02, 0.02, 0.02, 0.02]),\n            3.0,\n            40000,\n            34567\n        ),\n        # Case 4:\n        (\n            6,\n            [(0, 1), (0, 2), (0, 3), (0, 4), (0, 5)],\n            0.01,\n            0.0,\n            30000,\n            45678\n        )\n    ]\n\n    results = []\n    for case in test_cases:\n        n, edges, p_e, alpha, N, seed = case\n        estimate = estimate_disconnection_prob(n, edges, p_e, alpha, N, seed)\n        results.append(estimate)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef estimate_disconnection_prob(n, edges, p_e, alpha, N, seed):\n    \"\"\"\n    Estimates graph disconnection probability using importance sampling.\n\n    Args:\n        n (int): Number of nodes.\n        edges (list): List of tuples representing edges.\n        p_e (float or np.ndarray): Baseline edge failure probabilities.\n        alpha (float): Tilt intensity parameter.\n        N (int): Number of simulation samples.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        float: The estimated probability of disconnection.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    num_edges = len(edges)\n\n    # 1. Pre-computation Stage\n    \n    # Ensure p_e is a numpy array\n    if isinstance(p_e, (float, int)):\n        p_e = np.full(num_edges, float(p_e))\n\n    # a. Construct Graph Laplacian (L) for unit-weight graph\n    A = np.zeros((n, n))\n    for u, v in edges:\n        A[u, v] = A[v, u] = 1\n    D = np.diag(np.sum(A, axis=1))\n    L = D - A\n\n    # b. Compute Moore-Penrose Pseudoinverse (L+)\n    L_plus = linalg.pinv(L)\n\n    # c. Calculate Effective Resistances (R_ij)\n    R_e = np.zeros(num_edges)\n    for i, (u, v) in enumerate(edges):\n        R_e[i] = L_plus[u, u] + L_plus[v, v] - 2 * L_plus[u, v]\n    \n    # d. Normalize resistances to get weights (w_e)\n    max_R = np.max(R_e)\n    w_e = R_e / max_R if max_R > 0 else np.zeros_like(R_e)\n    \n    # e. Compute tilted probabilities (p_e')\n    # Use small epsilon to avoid log(0) for p=0 or p=1\n    epsilon = 1e-12\n    p_e_clipped = np.clip(p_e, epsilon, 1 - epsilon)\n    \n    logit_p = np.log(p_e_clipped / (1 - p_e_clipped))\n    logit_p_prime = logit_p + alpha * w_e\n    p_prime_e = 1 / (1 + np.exp(-logit_p_prime))\n    p_prime_e_clipped = np.clip(p_prime_e, epsilon, 1 - epsilon)\n\n    # f. Pre-compute log-probabilities for the likelihood ratio\n    log_p = np.log(p_e_clipped)\n    log_1_minus_p = np.log(1 - p_e_clipped)\n    log_p_prime = np.log(p_prime_e_clipped)\n    log_1_minus_p_prime = np.log(1 - p_prime_e_clipped)\n\n    log_ratio_fail = log_p - log_p_prime\n    log_ratio_survive = log_1_minus_p - log_1_minus_p_prime\n\n    # 2. Monte Carlo Simulation Stage\n    total_estimator_val = 0.0\n\n    for _ in range(N):\n        # a. Generate edge failures from tilted distribution Q\n        failures = rng.random(num_edges) < p_prime_e\n        \n        # b. Check for connectivity using Breadth-First Search (BFS)\n        adj = [[] for _ in range(n)]\n        surviving_edges_indices = np.where(failures == 0)[0]\n        for edge_idx in surviving_edges_indices:\n            u, v = edges[edge_idx]\n            adj[u].append(v)\n            adj[v].append(u)\n\n        queue = collections.deque([0])\n        visited = {0}\n        while queue:\n            u_curr = queue.popleft()\n            for v_neighbor in adj[u_curr]:\n                if v_neighbor not in visited:\n                    visited.add(v_neighbor)\n                    queue.append(v_neighbor)\n\n        # c. If disconnected, compute likelihood and add to total\n        if len(visited) < n:\n            log_L = np.sum(failures * log_ratio_fail) + np.sum((1 - failures) * log_ratio_survive)\n            likelihood_ratio = np.exp(log_L)\n            total_estimator_val += likelihood_ratio\n\n    # 3. Final Estimate\n    return total_estimator_val / N\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}