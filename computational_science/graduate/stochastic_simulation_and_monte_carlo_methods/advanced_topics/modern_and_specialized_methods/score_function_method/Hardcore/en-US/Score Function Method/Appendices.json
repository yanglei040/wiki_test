{
    "hands_on_practices": [
        {
            "introduction": "Mastering a new method begins with understanding its mechanics in the simplest possible setting. This first practice grounds you in the fundamentals of the score function method by applying it to a Bernoulli random variable . By deriving the score function and the core likelihood ratio identity from scratch, you will build a solid intuition for how this powerful technique transforms a derivative of an expectation into an expectation of a score-weighted function.",
            "id": "3337816",
            "problem": "Consider a single random variable $X$ distributed as a Bernoulli random variable with parameter $p \\in (0,1)$, with probability mass function $f_{p}(x) = p^{x} (1-p)^{1-x}$ for $x \\in \\{0,1\\}$. Let the score function be defined as $S_{p}(X) = \\partial_{p} \\ln f_{p}(X)$. Starting from the definition of the log-likelihood and without using any pre-stated formulas specific to the likelihood ratio method, first derive an explicit expression for $S_{p}(X)$ in terms of $X$ and $p$. Next, using only the definition of expectation $\\mathbb{E}_{p}[h(X)] = \\sum_{x \\in \\{0,1\\}} h(x) f_{p}(x)$ and the validity of interchanging differentiation and summation under appropriate regularity, derive the identity that relates $\\partial_{p} \\mathbb{E}_{p}[h(X)]$ to an expectation involving $S_{p}(X)$ for a generic measurable function $h$. Finally, specialize to $h(X) = \\mathbf{1}\\{X=1\\}$ and compute $\\partial_{p} \\mathbb{E}_{p}[\\mathbf{1}\\{X=1\\}]$ using the likelihood ratio (LR) identity you derived. Provide your final answer as a single exact real number with no units.",
            "solution": "The user-provided problem is evaluated as valid. It is scientifically grounded in probability theory and statistics, well-posed with all necessary information, and stated in objective, formal language. The problem requires the derivation of the score function and the likelihood ratio identity for a Bernoulli random variable from first principles, followed by an application of this identity. This is a standard and verifiable exercise in the field of stochastic simulation. We will now proceed with the solution.\n\nThe problem is structured in three parts. First, we must derive an explicit expression for the score function $S_{p}(X)$. Second, we derive the likelihood ratio (LR) identity relating the derivative of an expectation to an expectation involving the score function. Third, we apply this identity to a specific function $h(X) = \\mathbf{1}\\{X=1\\}$ to compute the derivative.\n\n**Part 1: Derivation of the Score Function $S_{p}(X)$**\n\nThe random variable $X$ follows a Bernoulli distribution with parameter $p \\in (0,1)$. Its probability mass function (PMF) is given by:\n$$f_{p}(x) = p^{x} (1-p)^{1-x} \\quad \\text{for } x \\in \\{0, 1\\}$$\nThe score function $S_{p}(X)$ is defined as the partial derivative of the log-likelihood function with respect to the parameter $p$. First, we find the log-likelihood function, $\\ln f_{p}(x)$:\n$$ \\ln f_{p}(x) = \\ln(p^{x} (1-p)^{1-x}) $$\nUsing the properties of logarithms, this simplifies to:\n$$ \\ln f_{p}(x) = x \\ln(p) + (1-x) \\ln(1-p) $$\nNow, we differentiate this expression with respect to $p$:\n$$ \\partial_{p} \\ln f_{p}(x) = \\partial_{p} [x \\ln(p) + (1-x) \\ln(1-p)] $$\n$$ \\partial_{p} \\ln f_{p}(x) = x \\cdot \\frac{1}{p} + (1-x) \\cdot \\frac{1}{1-p} \\cdot (-1) $$\n$$ \\partial_{p} \\ln f_{p}(x) = \\frac{x}{p} - \\frac{1-x}{1-p} $$\nBy definition, the score function is $S_{p}(X) = \\partial_{p} \\ln f_{p}(X)$. Therefore, substituting the random variable $X$ for the specific outcome $x$, we obtain the expression for the score function:\n$$ S_{p}(X) = \\frac{X}{p} - \\frac{1-X}{1-p} $$\n\n**Part 2: Derivation of the Likelihood Ratio (LR) Identity**\n\nWe are asked to derive the identity that relates $\\partial_{p} \\mathbb{E}_{p}[h(X)]$ to an expectation involving $S_{p}(X)$ for a generic measurable function $h$. We start with the definition of the expectation of $h(X)$:\n$$ \\mathbb{E}_{p}[h(X)] = \\sum_{x \\in \\{0,1\\}} h(x) f_{p}(x) $$\nWe then differentiate this expression with respect to $p$:\n$$ \\partial_{p} \\mathbb{E}_{p}[h(X)] = \\partial_{p} \\left[ \\sum_{x \\in \\{0,1\\}} h(x) f_{p}(x) \\right] $$\nThe problem states that we can interchange the operations of differentiation and summation. This is a crucial step known as the \"log-derivative trick\" or \"likelihood ratio method\":\n$$ \\partial_{p} \\mathbb{E}_{p}[h(X)] = \\sum_{x \\in \\{0,1\\}} h(x) \\left( \\partial_{p} f_{p}(x) \\right) $$\nWe can rewrite the derivative of the PMF, $\\partial_{p} f_{p}(x)$, using the chain rule for derivatives applied to the logarithm: $\\partial_z \\ln(y) = \\frac{1}{y} \\partial_z y$, which implies $\\partial_z y = y \\cdot (\\partial_z \\ln y)$. Applying this, we get:\n$$ \\partial_{p} f_{p}(x) = f_{p}(x) \\cdot (\\partial_{p} \\ln f_{p}(x)) $$\nThe term $\\partial_{p} \\ln f_{p}(x)$ is precisely the score function evaluated at $x$, which we denote $S_p(x)$. Substituting this back into the summation gives:\n$$ \\partial_{p} \\mathbb{E}_{p}[h(X)] = \\sum_{x \\in \\{0,1\\}} h(x) f_{p}(x) S_{p}(x) $$\nThe expression on the right-hand side is, by definition, the expectation of the random variable $h(X)S_p(X)$. Thus, we have derived the general likelihood ratio identity:\n$$ \\partial_{p} \\mathbb{E}_{p}[h(X)] = \\mathbb{E}_{p}[h(X) S_{p}(X)] $$\n\n**Part 3: Specialization and Final Computation**\n\nFinally, we must compute $\\partial_{p} \\mathbb{E}_{p}[\\mathbf{1}\\{X=1\\}]$ using the LR identity derived in Part 2. Let $h(X) = \\mathbf{1}\\{X=1\\}$, where $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\nApplying the LR identity, we have:\n$$ \\partial_{p} \\mathbb{E}_{p}[\\mathbf{1}\\{X=1\\}] = \\mathbb{E}_{p}[\\mathbf{1}\\{X=1\\} S_{p}(X)] $$\nWe compute the expectation on the right-hand side using its definition:\n$$ \\mathbb{E}_{p}[\\mathbf{1}\\{X=1\\} S_{p}(X)] = \\sum_{x \\in \\{0,1\\}} \\mathbf{1}\\{x=1\\} S_{p}(x) f_{p}(x) $$\nThe term $\\mathbf{1}\\{x=1\\}$ is zero for $x=0$ and one for $x=1$. Therefore, the summation reduces to a single term corresponding to $x=1$:\n$$ \\mathbb{E}_{p}[\\mathbf{1}\\{X=1\\} S_{p}(X)] = \\mathbf{1}\\{0=1\\} S_{p}(0) f_{p}(0) + \\mathbf{1}\\{1=1\\} S_{p}(1) f_{p}(1) $$\n$$ = (0) \\cdot S_{p}(0) f_{p}(0) + (1) \\cdot S_{p}(1) f_{p}(1) = S_{p}(1) f_{p}(1) $$\nNow we need the specific values for $S_{p}(1)$ and $f_{p}(1)$.\nFrom Part 1, the score function is $S_{p}(x) = \\frac{x}{p} - \\frac{1-x}{1-p}$. For $x=1$, this gives:\n$$ S_{p}(1) = \\frac{1}{p} - \\frac{1-1}{1-p} = \\frac{1}{p} - 0 = \\frac{1}{p} $$\nThe PMF for $x=1$ is:\n$$ f_{p}(1) = p^{1} (1-p)^{1-1} = p \\cdot (1-p)^{0} = p $$\nSubstituting these results back into our expression:\n$$ \\partial_{p} \\mathbb{E}_{p}[\\mathbf{1}\\{X=1\\}] = S_{p}(1) f_{p}(1) = \\left(\\frac{1}{p}\\right) \\cdot (p) = 1 $$\nThe result is a constant, independent of $p$. This can be verified by direct calculation: $\\mathbb{E}_{p}[\\mathbf{1}\\{X=1\\}] = P(X=1) = p$, and $\\partial_{p}(p) = 1$. Our derivation using the score function method yields the correct result. The final numerical answer is $1$.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "Having established the method for a discrete variable, the next step is to ensure it applies equally well to continuous distributions. This exercise extends your skills to the ubiquitous Normal distribution, a cornerstone of statistics and modeling . Working through this problem will demonstrate the score function method's generality and provide a template for applying it to other continuous parametric families.",
            "id": "3337798",
            "problem": "Consider a one-parameter family of probability densities for a real-valued random variable $X$ given by the normal distribution with mean parameter $\\mu \\in \\mathbb{R}$ and known variance $\\sigma^{2} \\in (0,\\infty)$, with density\n$$\nf_{\\mu}(x) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right), \\quad x \\in \\mathbb{R}.\n$$\nThe score function (likelihood ratio) for the parameter $\\mu$ is defined by $S_{\\mu}(x) \\equiv \\partial_{\\mu}\\ln f_{\\mu}(x)$. Using only the definition of the score function and the representation $\\mathbb{E}_{\\mu}[h(X)] = \\int_{\\mathbb{R}} h(x) f_{\\mu}(x)\\,dx$ together with justified interchange of differentiation and integration under suitable regularity conditions, do the following:\n\n- Compute $S_{\\mu}(X)$ explicitly.\n- Apply the score-function method to derive an exact expression for $\\partial_{\\mu}\\,\\mathbb{E}_{\\mu}[X^{2}]$ in terms of $\\mu$ and $\\sigma$ only.\n\nAssume any regularity conditions needed for the exchange of differentiation and integration are satisfied for the normal family and the integrand under consideration. Report only the value of $\\partial_{\\mu}\\,\\mathbb{E}_{\\mu}[X^{2}]$ as your final answer. Express your answer in exact closed form. No rounding is required.",
            "solution": "The problem is first validated against the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   A one-parameter family of probability densities for a real-valued random variable $X$ is the normal distribution $N(\\mu, \\sigma^2)$ with density:\n    $$f_{\\mu}(x) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right), \\quad x \\in \\mathbb{R}$$\n-   The mean parameter is $\\mu \\in \\mathbb{R}$.\n-   The variance is a known constant $\\sigma^{2} \\in (0,\\infty)$.\n-   The score function is defined as $S_{\\mu}(x) \\equiv \\partial_{\\mu}\\ln f_{\\mu}(x)$.\n-   The expectation is defined as $\\mathbb{E}_{\\mu}[h(X)] = \\int_{\\mathbb{R}} h(x) f_{\\mu}(x)\\,dx$.\n-   An assumption is made that any regularity conditions needed for the exchange of differentiation and integration are satisfied.\n-   The tasks are to compute $S_{\\mu}(X)$ and then use the score-function method to derive an expression for $\\partial_{\\mu}\\,\\mathbb{E}_{\\mu}[X^{2}]$.\n-   The final reported answer should be only the value of $\\partial_{\\mu}\\,\\mathbb{E}_{\\mu}[X^{2}]$.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded**: The problem is based on fundamental and well-established concepts in probability theory and statistics, namely the normal distribution, expectation, and the score function (likelihood ratio) method. These are standard topics in mathematical statistics and stochastic simulation.\n-   **Well-Posed**: The problem is clearly stated. All necessary definitions are provided, and the quantity to be calculated is unambiguous. The assumption of regularity conditions simplifies the analysis and ensures that a standard method can be applied without needing to prove the underlying measure-theoretic conditions, making the problem self-contained and solvable. A unique solution exists.\n-   **Objective**: The problem is stated in precise, formal mathematical language, free from any subjective or ambiguous terminology.\n\nThe problem does not exhibit any of the flaws listed in the validation checklist. It is scientifically sound, well-posed, objective, and directly relevant to the topic of the score-function method.\n\n**Step 3: Verdict and Action**\n\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Solution\n\nThe solution proceeds in two parts as requested by the problem statement. First, we compute the score function $S_{\\mu}(X)$. Second, we apply the score-function method to find an expression for $\\partial_{\\mu}\\,\\mathbb{E}_{\\mu}[X^{2}]$.\n\n**Part 1: Computation of the Score Function**\n\nThe score function is defined as $S_{\\mu}(x) = \\partial_{\\mu}\\ln f_{\\mu}(x)$. We begin by taking the natural logarithm of the probability density function $f_{\\mu}(x)$:\n$$ \\ln f_{\\mu}(x) = \\ln\\left( \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{(x-\\mu)^{2}}{2\\sigma^{2}}\\right) \\right) $$\nUsing the properties of the logarithm, we can simplify this expression:\n$$ \\ln f_{\\mu}(x) = \\ln\\left((2\\pi\\sigma^2)^{-1/2}\\right) - \\frac{(x-\\mu)^2}{2\\sigma^2} = -\\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2} $$\nNow, we differentiate $\\ln f_{\\mu}(x)$ with respect to the parameter $\\mu$. The term $-\\frac{1}{2}\\ln(2\\pi\\sigma^2)$ is constant with respect to $\\mu$ since $\\sigma^2$ is known.\n$$ S_{\\mu}(x) = \\partial_{\\mu} \\left( -\\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{(x-\\mu)^2}{2\\sigma^2} \\right) = 0 - \\frac{1}{2\\sigma^2} \\partial_{\\mu}\\left((x-\\mu)^2\\right) $$\nUsing the chain rule for differentiation, we have $\\partial_{\\mu}(x-\\mu)^2 = 2(x-\\mu) \\cdot (-1) = -2(x-\\mu)$.\nSubstituting this back, we obtain the score function:\n$$ S_{\\mu}(x) = -\\frac{1}{2\\sigma^2}(-2(x-\\mu)) = \\frac{x-\\mu}{\\sigma^2} $$\nWhen evaluated at the random variable $X$, the score function is $S_{\\mu}(X) = \\frac{X-\\mu}{\\sigma^2}$.\n\n**Part 2: Application of the Score-Function Method**\n\nThe problem requires using the score-function method to derive $\\partial_{\\mu}\\,\\mathbb{E}_{\\mu}[X^{2}]$. This method, also known as the likelihood ratio method, expresses the derivative of an expectation as an expectation. We start from the definition of $\\mathbb{E}_{\\mu}[h(X)]$ and differentiate with respect to $\\mu$:\n$$ \\partial_{\\mu}\\,\\mathbb{E}_{\\mu}[h(X)] = \\partial_{\\mu} \\int_{\\mathbb{R}} h(x) f_{\\mu}(x)\\,dx $$\nThe problem states we can assume regularity conditions are met to interchange the derivative and the integral (Leibniz integral rule):\n$$ \\partial_{\\mu}\\,\\mathbb{E}_{\\mu}[h(X)] = \\int_{\\mathbb{R}} h(x) \\left(\\partial_{\\mu}f_{\\mu}(x)\\right)\\,dx $$\nUsing the identity $\\partial_{\\mu}f_{\\mu}(x) = f_{\\mu}(x) \\cdot \\partial_{\\mu}\\ln f_{\\mu}(x) = f_{\\mu}(x)S_{\\mu}(x)$, we can rewrite the integral:\n$$ \\partial_{\\mu}\\,\\mathbb{E}_{\\mu}[h(X)] = \\int_{\\mathbb{R}} h(x) S_{\\mu}(x) f_{\\mu}(x)\\,dx $$\nThis integral is, by definition, the expectation of the product $h(X)S_{\\mu}(X)$ with respect to the density $f_{\\mu}(x)$. Thus, we arrive at the score-function identity:\n$$ \\partial_{\\mu}\\,\\mathbb{E}_{\\mu}[h(X)] = \\mathbb{E}_{\\mu}[h(X)S_{\\mu}(X)] $$\nFor this problem, we are interested in $h(X) = X^2$. Applying the identity:\n$$ \\partial_{\\mu}\\,\\mathbb{E}_{\\mu}[X^{2}] = \\mathbb{E}_{\\mu}[X^2 S_{\\mu}(X)] $$\nSubstituting the expression for $S_{\\mu}(X) = \\frac{X-\\mu}{\\sigma^2}$:\n$$ \\partial_{\\mu}\\,\\mathbb{E}_{\\mu}[X^{2}] = \\mathbb{E}_{\\mu}\\left[X^2 \\left(\\frac{X-\\mu}{\\sigma^2}\\right)\\right] = \\frac{1}{\\sigma^2}\\mathbb{E}_{\\mu}[X^2(X-\\mu)] $$\nUsing the linearity of expectation:\n$$ \\partial_{\\mu}\\,\\mathbb{E}_{\\mu}[X^{2}] = \\frac{1}{\\sigma^2}\\left(\\mathbb{E}_{\\mu}[X^3] - \\mu\\mathbb{E}_{\\mu}[X^2]\\right) $$\nTo proceed, we need the second and third moments of the normal distribution $X \\sim N(\\mu, \\sigma^2)$.\nThe second moment, $\\mathbb{E}_{\\mu}[X^2]$, can be found from the definition of variance, $\\text{Var}_{\\mu}(X) = \\mathbb{E}_{\\mu}[X^2] - (\\mathbb{E}_{\\mu}[X])^2$. With $\\mathbb{E}_{\\mu}[X] = \\mu$ and $\\text{Var}_{\\mu}(X) = \\sigma^2$, we have:\n$$ \\mathbb{E}_{\\mu}[X^2] = \\text{Var}_{\\mu}(X) + (\\mathbb{E}_{\\mu}[X])^2 = \\sigma^2 + \\mu^2 $$\nThe third moment, $\\mathbb{E}_{\\mu}[X^3]$, can be computed by considering the central moments. Let us expand $X^3 = ((X-\\mu)+\\mu)^3$:\n$$ \\mathbb{E}_{\\mu}[X^3] = \\mathbb{E}_{\\mu}[((X-\\mu)+\\mu)^3] = \\mathbb{E}_{\\mu}[(X-\\mu)^3 + 3\\mu(X-\\mu)^2 + 3\\mu^2(X-\\mu) + \\mu^3] $$\nBy linearity of expectation:\n$$ \\mathbb{E}_{\\mu}[X^3] = \\mathbb{E}_{\\mu}[(X-\\mu)^3] + 3\\mu\\mathbb{E}_{\\mu}[(X-\\mu)^2] + 3\\mu^2\\mathbb{E}_{\\mu}[X-\\mu] + \\mathbb{E}_{\\mu}[\\mu^3] $$\nWe use the following standard properties of the normal distribution:\n-   The first central moment is $\\mathbb{E}_{\\mu}[X-\\mu] = \\mathbb{E}_{\\mu}[X] - \\mu = \\mu - \\mu = 0$.\n-   The second central moment is the variance, $\\mathbb{E}_{\\mu}[(X-\\mu)^2] = \\sigma^2$.\n-   The third central moment, $\\mathbb{E}_{\\mu}[(X-\\mu)^3]$, is zero because the normal distribution is symmetric about its mean. All odd central moments of a symmetric distribution are zero.\nSubstituting these values:\n$$ \\mathbb{E}_{\\mu}[X^3] = 0 + 3\\mu(\\sigma^2) + 3\\mu^2(0) + \\mu^3 = 3\\mu\\sigma^2 + \\mu^3 $$\nNow we substitute the expressions for the second and third moments back into our equation for $\\partial_{\\mu}\\,\\mathbb{E}_{\\mu}[X^{2}]$:\n$$ \\partial_{\\mu}\\,\\mathbb{E}_{\\mu}[X^{2}] = \\frac{1}{\\sigma^2}\\left( (3\\mu\\sigma^2 + \\mu^3) - \\mu(\\sigma^2 + \\mu^2) \\right) $$\nExpanding the terms inside the parentheses:\n$$ \\partial_{\\mu}\\,\\mathbb{E}_{\\mu}[X^{2}] = \\frac{1}{\\sigma^2}\\left( 3\\mu\\sigma^2 + \\mu^3 - \\mu\\sigma^2 - \\mu^3 \\right) $$\nThe $\\mu^3$ terms cancel out, and we combine the $\\mu\\sigma^2$ terms:\n$$ \\partial_{\\mu}\\,\\mathbb{E}_{\\mu}[X^{2}] = \\frac{1}{\\sigma^2}\\left( 2\\mu\\sigma^2 \\right) $$\nFinally, simplifying the expression gives the result:\n$$ \\partial_{\\mu}\\,\\mathbb{E}_{\\mu}[X^{2}] = 2\\mu $$\nThis result can be verified by directly differentiating the expression for the second moment, $\\mathbb{E}_{\\mu}[X^2] = \\sigma^2 + \\mu^2$, with respect to $\\mu$, which yields $\\partial_{\\mu}(\\sigma^2 + \\mu^2) = 2\\mu$. This confirms our derivation using the score-function method.",
            "answer": "$$\\boxed{2\\mu}$$"
        },
        {
            "introduction": "While the score function method is remarkably versatile, its practical application can be hampered by high variance in Monte Carlo settings, leading to slow convergence. This advanced practice addresses this critical issue by introducing a variance reduction technique known as a baseline, a type of control variate . Deriving the optimal constant baseline that minimizes the estimator's variance will equip you with a crucial tool for making score function estimators not just theoretically sound, but practically efficient.",
            "id": "3337807",
            "problem": "Consider the one-parameter family of probability densities for a real-valued random variable $X$ given by the normal distribution $X \\sim \\mathcal{N}(\\mu,\\sigma^{2})$, where $\\mu \\in \\mathbb{R}$ is the mean and $\\sigma^{2}  0$ is the variance. Let $h(X) = X^{3}$ and define the parametric expectation $\\mathbb{E}_{\\mu}[h(X)] = \\int h(x) f_{\\mu}(x) \\,\\mathrm{d}x$, where $f_{\\mu}(x)$ is the normal density with mean $\\mu$ and variance $\\sigma^{2}$.\n\nUsing the score function (likelihood ratio) method, the gradient $\\partial_{\\mu} \\mathbb{E}_{\\mu}[h(X)]$ can be estimated via Monte Carlo by forming products of the integrand $h(X)$ with the score function $s_{\\mu}(X) = \\partial_{\\mu} \\ln f_{\\mu}(X)$. A common variance reduction technique is to subtract a constant baseline $b$ from $h(X)$ and use the estimator $(h(X) - b) \\, s_{\\mu}(X)$; the baseline $b$ does not depend on the sample $X$.\n\nStarting from first principles—namely, the definition of the score function, the likelihood ratio identity for parametric differentiation of expectations, and properties of moments of the normal distribution—derive the expression for the constant baseline $b^{\\star}(\\mu,\\sigma)$ that minimizes the variance of the likelihood ratio estimator for $\\partial_{\\mu} \\mathbb{E}_{\\mu}[h(X)]$ in this setting. Then, compute this optimal baseline explicitly for $h(X) = X^{3}$ and $X \\sim \\mathcal{N}(\\mu,\\sigma^{2})$.\n\nYour final answer must be a single closed-form analytic expression in terms of $\\mu$ and $\\sigma$, with no numerical approximation.",
            "solution": "The problem asks for the derivation of the optimal constant baseline $b^{\\star}$ that minimizes the variance of the score function estimator for the gradient of a parametric expectation, and then to compute this baseline for a specific case.\n\nThe problem is valid as it is scientifically grounded in the theory of Monte Carlo methods, is well-posed, and all necessary information is provided. We can proceed with the solution.\n\nFirst, we derive the general expression for the optimal baseline $b^{\\star}$. Let $X$ be a random variable with a probability density function $f_{\\theta}(x)$ depending on a parameter $\\theta$. Here, the parameter is the mean $\\mu$, so the density is $f_{\\mu}(x)$. The score function is defined as $s_{\\mu}(X) = \\partial_{\\mu} \\ln f_{\\mu}(X)$. The likelihood ratio method is used to estimate the gradient of an expectation $\\partial_{\\mu} \\mathbb{E}_{\\mu}[h(X)]$. The identity is $\\partial_{\\mu} \\mathbb{E}_{\\mu}[h(X)] = \\mathbb{E}_{\\mu}[h(X)s_{\\mu}(X)]$.\n\nA variance reduction technique introduces a constant baseline $b$, leading to a new estimator with random variable $Y(b) = (h(X) - b) s_{\\mu}(X)$. We seek to find the value of $b$, denoted $b^{\\star}$, that minimizes the variance $\\mathrm{Var}_{\\mu}[Y(b)]$.\n\nFirst, let's compute the expectation of the estimator.\n$$\n\\mathbb{E}_{\\mu}[Y(b)] = \\mathbb{E}_{\\mu}[(h(X) - b) s_{\\mu}(X)] = \\mathbb{E}_{\\mu}[h(X) s_{\\mu}(X)] - b \\mathbb{E}_{\\mu}[s_{\\mu}(X)]\n$$\nA fundamental property of the score function is that its expectation is zero. We demonstrate this from first principles. Under regularity conditions that allow swapping differentiation and integration (which hold for the normal distribution family with respect to its mean), we have:\n$$\n\\mathbb{E}_{\\mu}[s_{\\mu}(X)] = \\int_{-\\infty}^{\\infty} s_{\\mu}(x) f_{\\mu}(x) \\,\\mathrm{d}x = \\int_{-\\infty}^{\\infty} \\left(\\frac{\\partial_{\\mu} f_{\\mu}(x)}{f_{\\mu}(x)}\\right) f_{\\mu}(x) \\,\\mathrm{d}x = \\int_{-\\infty}^{\\infty} \\partial_{\\mu} f_{\\mu}(x) \\,\\mathrm{d}x\n$$\n$$\n= \\partial_{\\mu} \\int_{-\\infty}^{\\infty} f_{\\mu}(x) \\,\\mathrm{d}x = \\partial_{\\mu} (1) = 0\n$$\nTherefore, the expectation of our estimator is:\n$$\n\\mathbb{E}_{\\mu}[Y(b)] = \\mathbb{E}_{\\mu}[h(X) s_{\\mu}(X)] - b \\cdot 0 = \\mathbb{E}_{\\mu}[h(X) s_{\\mu}(X)] = \\partial_{\\mu} \\mathbb{E}_{\\mu}[h(X)]\n$$\nThis confirms that the estimator is unbiased for any constant baseline $b$.\n\nThe variance of the estimator is given by $\\mathrm{Var}_{\\mu}[Y(b)] = \\mathbb{E}_{\\mu}[Y(b)^2] - (\\mathbb{E}_{\\mu}[Y(b)])^2$. Since the mean $\\mathbb{E}_{\\mu}[Y(b)]$ does not depend on $b$, minimizing the variance is equivalent to minimizing the second moment, $\\mathbb{E}_{\\mu}[Y(b)^2]$. Let's define this second moment as a function of $b$:\n$$\nM_2(b) = \\mathbb{E}_{\\mu}[Y(b)^2] = \\mathbb{E}_{\\mu}[((h(X) - b)s_{\\mu}(X))^2] = \\mathbb{E}_{\\mu}[(h(X)^2 - 2bh(X) + b^2) s_{\\mu}(X)^2]\n$$\nBy linearity of expectation, we can expand this expression:\n$$\nM_2(b) = \\mathbb{E}_{\\mu}[h(X)^2 s_{\\mu}(X)^2] - 2b \\mathbb{E}_{\\mu}[h(X) s_{\\mu}(X)^2] + b^2 \\mathbb{E}_{\\mu}[s_{\\mu}(X)^2]\n$$\nThis is a quadratic function of $b$. To find the minimum, we differentiate with respect to $b$ and set the derivative to zero:\n$$\n\\frac{\\mathrm{d} M_2(b)}{\\mathrm{d}b} = -2 \\mathbb{E}_{\\mu}[h(X) s_{\\mu}(X)^2] + 2b \\mathbb{E}_{\\mu}[s_{\\mu}(X)^2] = 0\n$$\nSolving for $b$ gives the optimal baseline $b^{\\star}$:\n$$\nb^{\\star} = \\frac{\\mathbb{E}_{\\mu}[h(X) s_{\\mu}(X)^2]}{\\mathbb{E}_{\\mu}[s_{\\mu}(X)^2]}\n$$\nThis is the general expression for the optimal constant baseline.\n\nNow, we apply this formula to the specific case where $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, $h(X) = X^3$, and the parameter of interest is $\\mu$.\n\n1.  **Calculate the score function $s_{\\mu}(X)$**.\n    The probability density function for $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is:\n    $$\n    f_{\\mu}(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n    $$\n    The log-likelihood is:\n    $$\n    \\ln f_{\\mu}(x) = -\\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{(x - \\mu)^2}{2\\sigma^2}\n    $$\n    The score function is the partial derivative with respect to $\\mu$:\n    $$\n    s_{\\mu}(x) = \\partial_{\\mu} \\ln f_{\\mu}(x) = -\\frac{1}{2\\sigma^2} \\partial_{\\mu}[ (x - \\mu)^2 ] = -\\frac{1}{2\\sigma^2} [2(x-\\mu)(-1)] = \\frac{x - \\mu}{\\sigma^2}\n    $$\n    So, for the random variable $X$, we have $s_{\\mu}(X) = \\frac{X - \\mu}{\\sigma^2}$.\n\n2.  **Calculate the denominator $\\mathbb{E}_{\\mu}[s_{\\mu}(X)^2]$**.\n    $$\n    \\mathbb{E}_{\\mu}[s_{\\mu}(X)^2] = \\mathbb{E}_{\\mu}\\left[\\left(\\frac{X - \\mu}{\\sigma^2}\\right)^2\\right] = \\frac{1}{\\sigma^4} \\mathbb{E}_{\\mu}[(X - \\mu)^2]\n    $$\n    The term $\\mathbb{E}_{\\mu}[(X - \\mu)^2]$ is the definition of the variance of $X$, which is $\\sigma^2$.\n    $$\n    \\mathbb{E}_{\\mu}[s_{\\mu}(X)^2] = \\frac{\\sigma^2}{\\sigma^4} = \\frac{1}{\\sigma^2}\n    $$\n\n3.  **Calculate the numerator $\\mathbb{E}_{\\mu}[h(X) s_{\\mu}(X)^2]$**.\n    With $h(X) = X^3$, the expression is:\n    $$\n    \\mathbb{E}_{\\mu}[X^3 s_{\\mu}(X)^2] = \\mathbb{E}_{\\mu}\\left[X^3 \\left(\\frac{X - \\mu}{\\sigma^2}\\right)^2\\right] = \\frac{1}{\\sigma^4} \\mathbb{E}_{\\mu}[X^3 (X - \\mu)^2]\n    $$\n    Expanding the term inside the expectation:\n    $$\n    \\mathbb{E}_{\\mu}[X^3 (X^2 - 2\\mu X + \\mu^2)] = \\mathbb{E}_{\\mu}[X^5 - 2\\mu X^4 + \\mu^2 X^3]\n    $$\n    Using the linearity of expectation, we need to compute the 3rd, 4th, and 5th moments of $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$. Let $Z \\sim \\mathcal{N}(0, 1)$ be a standard normal variable. Then $X = \\mu + \\sigma Z$. The raw moments of $Z$ are $\\mathbb{E}[Z]=0$, $\\mathbb{E}[Z^2]=1$, $\\mathbb{E}[Z^3]=0$, $\\mathbb{E}[Z^4]=3$, $\\mathbb{E}[Z^5]=0$.\n\n    -   **3rd moment of X**:\n        $\\mathbb{E}_{\\mu}[X^3] = \\mathbb{E}[(\\mu + \\sigma Z)^3] = \\mathbb{E}[\\mu^3 + 3\\mu^2\\sigma Z + 3\\mu\\sigma^2 Z^2 + \\sigma^3 Z^3]$\n        $= \\mu^3 + 3\\mu^2\\sigma \\mathbb{E}[Z] + 3\\mu\\sigma^2 \\mathbb{E}[Z^2] + \\sigma^3 \\mathbb{E}[Z^3] = \\mu^3 + 0 + 3\\mu\\sigma^2(1) + 0 = \\mu^3 + 3\\mu\\sigma^2$.\n\n    -   **4th moment of X**:\n        $\\mathbb{E}_{\\mu}[X^4] = \\mathbb{E}[(\\mu + \\sigma Z)^4] = \\mathbb{E}[\\mu^4 + 4\\mu^3\\sigma Z + 6\\mu^2\\sigma^2 Z^2 + 4\\mu\\sigma^3 Z^3 + \\sigma^4 Z^4]$\n        $= \\mu^4 + 6\\mu^2\\sigma^2 \\mathbb{E}[Z^2] + \\sigma^4 \\mathbb{E}[Z^4] = \\mu^4 + 6\\mu^2\\sigma^2(1) + \\sigma^4(3) = \\mu^4 + 6\\mu^2\\sigma^2 + 3\\sigma^4$.\n\n    -   **5th moment of X**:\n        $\\mathbb{E}_{\\mu}[X^5] = \\mathbb{E}[(\\mu + \\sigma Z)^5] = \\mathbb{E}[\\binom{5}{0}\\mu^5 Z^0 + \\binom{5}{1}\\mu^4\\sigma Z^1 + \\binom{5}{2}\\mu^3\\sigma^2 Z^2 + \\binom{5}{3}\\mu^2\\sigma^3 Z^3 + \\binom{5}{4}\\mu\\sigma^4 Z^4 + \\binom{5}{5}\\sigma^5 Z^5]$\n        $= \\mu^5 + 10\\mu^3\\sigma^2 \\mathbb{E}[Z^2] + 5\\mu\\sigma^4 \\mathbb{E}[Z^4] = \\mu^5 + 10\\mu^3\\sigma^2(1) + 5\\mu\\sigma^4(3) = \\mu^5 + 10\\mu^3\\sigma^2 + 15\\mu\\sigma^4$.\n\n    Now, substitute these moments back:\n    $$\n    \\mathbb{E}_{\\mu}[X^5 - 2\\mu X^4 + \\mu^2 X^3] = (\\mu^5 + 10\\mu^3\\sigma^2 + 15\\mu\\sigma^4) - 2\\mu(\\mu^4 + 6\\mu^2\\sigma^2 + 3\\sigma^4) + \\mu^2(\\mu^3 + 3\\mu\\sigma^2)\n    $$\n    $$\n    = \\mu^5 + 10\\mu^3\\sigma^2 + 15\\mu\\sigma^4 - 2\\mu^5 - 12\\mu^3\\sigma^2 - 6\\mu\\sigma^4 + \\mu^5 + 3\\mu^3\\sigma^2\n    $$\n    Group terms by powers of $\\mu$ and $\\sigma$:\n    $$\n    = (\\mu^5 - 2\\mu^5 + \\mu^5) + (10\\mu^3\\sigma^2 - 12\\mu^3\\sigma^2 + 3\\mu^3\\sigma^2) + (15\\mu\\sigma^4 - 6\\mu\\sigma^4)\n    $$\n    $$\n    = 0 + \\mu^3\\sigma^2 + 9\\mu\\sigma^4 = \\mu^3\\sigma^2 + 9\\mu\\sigma^4\n    $$\n    The numerator is then:\n    $$\n    \\mathbb{E}_{\\mu}[h(X) s_{\\mu}(X)^2] = \\frac{1}{\\sigma^4}(\\mu^3\\sigma^2 + 9\\mu\\sigma^4) = \\frac{\\mu^3}{\\sigma^2} + 9\\mu\n    $$\n\n4.  **Combine numerator and denominator to find $b^{\\star}$**.\n    $$\n    b^{\\star} = \\frac{\\mathbb{E}_{\\mu}[h(X) s_{\\mu}(X)^2]}{\\mathbb{E}_{\\mu}[s_{\\mu}(X)^2]} = \\frac{\\frac{\\mu^3}{\\sigma^2} + 9\\mu}{\\frac{1}{\\sigma^2}}\n    $$\n    Multiplying the numerator and denominator by $\\sigma^2$ gives the final expression for the optimal baseline:\n    $$\n    b^{\\star} = \\mu^3 + 9\\mu\\sigma^2\n    $$",
            "answer": "$$\\boxed{\\mu^{3} + 9\\mu\\sigma^{2}}$$"
        }
    ]
}