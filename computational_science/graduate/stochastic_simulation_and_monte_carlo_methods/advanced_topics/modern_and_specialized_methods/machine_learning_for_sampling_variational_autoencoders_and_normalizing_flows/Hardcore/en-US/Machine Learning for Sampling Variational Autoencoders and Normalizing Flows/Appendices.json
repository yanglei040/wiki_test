{
    "hands_on_practices": [
        {
            "introduction": "This first exercise explores a critical phenomenon in training Variational Autoencoders known as posterior collapse. By analyzing a simplified VAE where the decoder ignores the latent code, you will derive the optimal behavior of the encoder and quantitatively demonstrate how the model can learn to disregard the latent variable entirely, resulting in zero mutual information between the data and the latent representation . This practice illuminates a key challenge that motivates the development of more sophisticated VAE architectures and training strategies.",
            "id": "3318904",
            "problem": "Consider a one-dimensional Variational Autoencoder (VAE), that is, a Variational Autoencoder (VAE) with scalar latent variable $z \\in \\mathbb{R}$ and scalar observation $x \\in \\mathbb{R}$. Let the data-generating distribution be $p_{\\mathrm{data}}(x)$, which is Gaussian with zero mean and variance $\\sigma_{d}^{2}$, that is, $p_{\\mathrm{data}}(x) = \\mathcal{N}(0,\\sigma_{d}^{2})$. The prior over the latent variable is $p(z) = \\mathcal{N}(0,1)$. The decoder is specified to ignore $z$ and output $p_{\\theta}(x \\mid z) = \\mathcal{N}(0,\\sigma_{x}^{2})$, independent of $z$. The encoder is restricted to the Gaussian family $q_{\\phi}(z \\mid x) = \\mathcal{N}(\\alpha x,\\sigma_{q}^{2})$ with parameters $\\phi=(\\alpha,\\sigma_{q}^{2})$.\n\nStarting from first principles, use the definition of the Evidence Lower Bound (ELBO) as a lower bound to the marginal log-likelihood and the definition of Kullback–Leibler divergence to derive the expected ELBO under $p_{\\mathrm{data}}(x)$ as a function of $\\alpha$ and $\\sigma_{q}^{2}$, and then determine the encoder parameters $(\\alpha^{\\star},\\sigma_{q}^{2\\star})$ that maximize this expected ELBO. Next, starting from the definition of mutual information, compute the mutual information $\\mathbb{I}_{q}(x;z)$ under the joint $q(x,z) = p_{\\mathrm{data}}(x)\\, q_{\\phi}(z \\mid x)$, evaluated at the optimizing parameters $(\\alpha^{\\star},\\sigma_{q}^{2\\star})$. Report the pair consisting of the maximized expected ELBO and the mutual information at the optimizer as a row matrix. Express your final answer in closed form in terms of $\\sigma_{x}^{2}$ and $\\sigma_{d}^{2}$. No rounding is required.",
            "solution": "The user wants to solve a problem about a one-dimensional Variational Autoencoder (VAE).\n\n### Step 1: Extract Givens\nThe givens are:\n- Data-generating distribution: $p_{\\mathrm{data}}(x) = \\mathcal{N}(x | 0, \\sigma_{d}^{2})$.\n- Latent prior distribution: $p(z) = \\mathcal{N}(z | 0, 1)$.\n- Decoder (likelihood) distribution: $p_{\\theta}(x \\mid z) = \\mathcal{N}(x \\mid 0, \\sigma_{x}^{2})$.\n- Encoder (variational posterior) distribution: $q_{\\phi}(z \\mid x) = \\mathcal{N}(z \\mid \\alpha x, \\sigma_{q}^{2})$.\n- The parameters to be optimized are $\\phi = (\\alpha, \\sigma_{q}^{2})$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, using standard definitions and principles from machine learning, probability theory, and information theory. It is a well-posed theoretical exercise with a clear objective. All terms are precisely defined, and the premises are self-contained and consistent. The setup, while simplified (e.g., a decoder that ignores the latent variable), is mathematically valid and serves to test fundamental concepts. The problem is not trivial, requiring multi-step derivation.\n\n### Step 3: Verdict and Action\nThe problem is valid. We will now proceed with the solution.\n\n### Derivation of the Expected Evidence Lower Bound (ELBO)\n\nThe Evidence Lower Bound (ELBO) for a single data point $x$ is defined as:\n$$\n\\mathrm{ELBO}(x) = \\mathbb{E}_{z \\sim q_{\\phi}(z \\mid x)}[\\ln p_{\\theta}(x \\mid z)] - \\mathrm{KL}(q_{\\phi}(z \\mid x) \\| p(z))\n$$\nWe will compute each of the two terms separately.\n\nThe first term is the expected log-likelihood, or the reconstruction term. The decoder is $p_{\\theta}(x \\mid z) = \\mathcal{N}(x \\mid 0, \\sigma_{x}^{2})$. The probability density function is:\n$$\np_{\\theta}(x \\mid z) = \\frac{1}{\\sqrt{2\\pi\\sigma_{x}^{2}}} \\exp\\left(-\\frac{x^2}{2\\sigma_{x}^{2}}\\right)\n$$\nThe natural logarithm of this is:\n$$\n\\ln p_{\\theta}(x \\mid z) = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{x^2}{2\\sigma_{x}^{2}}\n$$\nSince this expression does not depend on the latent variable $z$, the expectation with respect to $q_{\\phi}(z \\mid x)$ is simply the expression itself:\n$$\n\\mathbb{E}_{z \\sim q_{\\phi}(z \\mid x)}[\\ln p_{\\theta}(x \\mid z)] = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{x^2}{2\\sigma_{x}^{2}}\n$$\n\nThe second term is the Kullback–Leibler (KL) divergence between the encoder distribution $q_{\\phi}(z \\mid x)$ and the prior $p(z)$. We are given $q_{\\phi}(z \\mid x) = \\mathcal{N}(z \\mid \\alpha x, \\sigma_{q}^{2})$ and $p(z) = \\mathcal{N}(z \\mid 0, 1)$. The KL divergence between two univariate normal distributions $\\mathcal{N}(\\mu_1, \\sigma_1^2)$ and $\\mathcal{N}(\\mu_2, \\sigma_2^2)$ is:\n$$\n\\mathrm{KL}(\\mathcal{N}(\\mu_1, \\sigma_1^2) \\| \\mathcal{N}(\\mu_2, \\sigma_2^2)) = \\ln\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2 + (\\mu_1-\\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n$$\nIn our case, $\\mu_1 = \\alpha x$, $\\sigma_1^2 = \\sigma_q^2$, $\\mu_2 = 0$, and $\\sigma_2^2 = 1$. Substituting these into the formula:\n$$\n\\mathrm{KL}(q_{\\phi}(z \\mid x) \\| p(z)) = \\ln\\frac{1}{\\sigma_q} + \\frac{\\sigma_q^2 + (\\alpha x - 0)^2}{2 \\cdot 1^2} - \\frac{1}{2}\n$$\n$$\n= -\\frac{1}{2}\\ln(\\sigma_q^2) + \\frac{\\sigma_q^2 + \\alpha^2 x^2}{2} - \\frac{1}{2}\n$$\n\nCombining the two terms, the ELBO for a single point $x$ is:\n$$\n\\mathrm{ELBO}(x) = \\left(-\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{x^2}{2\\sigma_{x}^{2}}\\right) - \\left(-\\frac{1}{2}\\ln(\\sigma_q^2) + \\frac{\\sigma_q^2 + \\alpha^2 x^2}{2} - \\frac{1}{2}\\right)\n$$\n$$\n\\mathrm{ELBO}(x) = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{x^2}{2\\sigma_{x}^{2}} + \\frac{1}{2}\\ln(\\sigma_q^2) - \\frac{\\sigma_q^2}{2} - \\frac{\\alpha^2 x^2}{2} + \\frac{1}{2}\n$$\n\nThe problem asks for the expected ELBO under the data distribution $p_{\\mathrm{data}}(x) = \\mathcal{N}(x \\mid 0, \\sigma_d^2)$. Let's denote this as $\\mathcal{L}(\\alpha, \\sigma_q^2) = \\mathbb{E}_{x \\sim p_{\\mathrm{data}}(x)}[\\mathrm{ELBO}(x)]$. We take the expectation of the terms that depend on $x$:\n$$\n\\mathcal{L}(\\alpha, \\sigma_q^2) = \\mathbb{E}_{x \\sim \\mathcal{N}(0,\\sigma_d^2)}\\left[ -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{x^2}{2\\sigma_{x}^{2}} + \\frac{1}{2}\\ln(\\sigma_q^2) - \\frac{\\sigma_q^2}{2} - \\frac{\\alpha^2 x^2}{2} + \\frac{1}{2} \\right]\n$$\nUsing the linearity of expectation, and noting that for $x \\sim \\mathcal{N}(0, \\sigma_d^2)$, we have $\\mathbb{E}[x^2] = \\mathrm{Var}(x) + (\\mathbb{E}[x])^2 = \\sigma_d^2 + 0^2 = \\sigma_d^2$:\n$$\n\\mathcal{L}(\\alpha, \\sigma_q^2) = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\mathbb{E}[x^2]}{2\\sigma_{x}^{2}} + \\frac{1}{2}\\ln(\\sigma_q^2) - \\frac{\\sigma_q^2}{2} - \\frac{\\alpha^2 \\mathbb{E}[x^2]}{2} + \\frac{1}{2}\n$$\n$$\n\\mathcal{L}(\\alpha, \\sigma_q^2) = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\sigma_d^2}{2\\sigma_{x}^{2}} - \\frac{\\alpha^2 \\sigma_d^2}{2} + \\frac{1}{2}\\ln(\\sigma_q^2) - \\frac{\\sigma_q^2}{2} + \\frac{1}{2}\n$$\n\n### Maximization of the Expected ELBO\n\nTo find the optimal parameters $(\\alpha^{\\star}, \\sigma_q^{2\\star})$, we maximize $\\mathcal{L}(\\alpha, \\sigma_q^2)$ by taking partial derivatives with respect to $\\alpha$ and $\\sigma_q^2$ and setting them to zero. The terms not involving $\\alpha$ or $\\sigma_q^2$ are constant with respect to the optimization.\n\nThe function can be separated for $\\alpha$ and $\\sigma_q^2$:\n$$\n\\mathcal{L}(\\alpha, \\sigma_q^2) = \\left( -\\frac{\\alpha^2 \\sigma_d^2}{2} \\right) + \\left( \\frac{1}{2}\\ln(\\sigma_q^2) - \\frac{\\sigma_q^2}{2} \\right) + \\text{Constant}\n$$\nFor $\\alpha$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\alpha} = - \\alpha \\sigma_d^2\n$$\nSetting this to zero, $- \\alpha \\sigma_d^2 = 0$. Since $\\sigma_d^2 > 0$, this implies $\\alpha^{\\star} = 0$. The second derivative $\\frac{\\partial^2 \\mathcal{L}}{\\partial \\alpha^2} = -\\sigma_d^2  0$, confirming this is a maximum.\n\nFor $\\sigma_q^2$: Let $v = \\sigma_q^2$. We optimize the term $f(v) = \\frac{1}{2}\\ln(v) - \\frac{v}{2}$.\n$$\n\\frac{\\partial f}{\\partial v} = \\frac{1}{2v} - \\frac{1}{2}\n$$\nSetting this to zero, $\\frac{1}{2v} - \\frac{1}{2} = 0 \\implies v = 1$. So, $\\sigma_q^{2\\star} = 1$. The second derivative $\\frac{\\partial^2 f}{\\partial v^2} = -\\frac{1}{2v^2}  0$ for $v>0$, confirming a maximum.\n\nThus, the optimal encoder parameters are $(\\alpha^{\\star}, \\sigma_q^{2\\star}) = (0, 1)$.\n\n### Maximized Expected ELBO Value\n\nWe substitute these optimal parameters back into the expression for $\\mathcal{L}(\\alpha, \\sigma_q^2)$:\n$$\n\\mathcal{L}^{\\star} = \\mathcal{L}(0, 1) = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\sigma_d^2}{2\\sigma_{x}^{2}} - \\frac{0^2 \\sigma_d^2}{2} + \\frac{1}{2}\\ln(1) - \\frac{1}{2} + \\frac{1}{2}\n$$\n$$\n\\mathcal{L}^{\\star} = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\sigma_d^2}{2\\sigma_{x}^{2}} - 0 + 0 - \\frac{1}{2} + \\frac{1}{2}\n$$\n$$\n\\mathcal{L}^{\\star} = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\sigma_d^2}{2\\sigma_{x}^{2}}\n$$\n\n### Mutual Information\n\nThe mutual information between $x$ and $z$ under the joint distribution $q(x,z) = p_{\\mathrm{data}}(x)q_{\\phi}(z \\mid x)$ is given by:\n$$\n\\mathbb{I}_q(x;z) = \\mathbb{E}_{x \\sim p_{\\mathrm{data}}(x)}[\\mathrm{KL}(q_\\phi(z \\mid x) \\| p(z))]\n$$\nThis is precisely the expectation of the KL-divergence term that we calculated for the ELBO. We need to evaluate this at the optimizing parameters $(\\alpha^{\\star}, \\sigma_q^{2\\star}) = (0, 1)$.\n\nAt these parameters, the encoder becomes:\n$$\nq_{\\phi^{\\star}}(z \\mid x) = \\mathcal{N}(z \\mid 0 \\cdot x, 1) = \\mathcal{N}(z \\mid 0, 1)\n$$\nThis is identical to the prior distribution $p(z)$. Therefore, for any value of $x$, the KL divergence is:\n$$\n\\mathrm{KL}(q_{\\phi^{\\star}}(z \\mid x) \\| p(z)) = \\mathrm{KL}(\\mathcal{N}(z \\mid 0, 1) \\| \\mathcal{N}(z \\mid 0, 1)) = 0\n$$\nSince the KL divergence is $0$ for all $x$, its expectation over $p_{\\mathrm{data}}(x)$ is also $0$:\n$$\n\\mathbb{I}_q(x;z)|_{\\phi=\\phi^{\\star}} = \\mathbb{E}_{x \\sim p_{\\mathrm{data}}(x)}[0] = 0\n$$\nThis result is expected. The optimal encoder $q_{\\phi^{\\star}}(z \\mid x)$ ignores the input $x$ entirely and outputs a distribution identical to the prior. This means that under the model $q(x,z)$, the latent variable $z$ is independent of the observation $x$, and thus their mutual information is zero. This phenomenon is known as posterior collapse.\n\n### Final Result\n\nThe pair consists of the maximized expected ELBO and the mutual information at the optimizer.\n- Maximized expected ELBO: $\\mathcal{L}^{\\star} = -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\sigma_{d}^{2}}{2\\sigma_{x}^{2}}$\n- Mutual information: $\\mathbb{I}_q(x;z)|_{\\phi=\\phi^{\\star}} = 0",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{1}{2}\\ln(2\\pi\\sigma_{x}^{2}) - \\frac{\\sigma_{d}^{2}}{2\\sigma_{x}^{2}}  0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Normalizing flows offer a powerful way to construct complex, learnable probability distributions by transforming a simple base density through a series of invertible functions. This exercise focuses on the mechanics of a single Real NVP (Non-Volume Preserving) coupling layer, a fundamental building block for many flow architectures . You will practice computing the forward and inverse transformations and, crucially, the log-determinant of the Jacobian, which is essential for applying the change of variables formula and can be calculated very efficiently in these models.",
            "id": "3318886",
            "problem": "Consider a two-dimensional Real Non-Volume Preserving (RNVP) coupling transformation from normalizing flows, defined on $\\mathbb{R}^{2}$ as follows. Let the input be $z = (z_{1}, z_{2})$ and let the output be $x = (x_{1}, x_{2})$ under the coupling rule\n$$\nx_{1} = z_{1}, \\quad x_{2} = z_{2} \\cdot \\exp\\big(s(z_{1})\\big) + t(z_{1}),\n$$\nwhere the scale function $s(\\cdot)$ and the shift function $t(\\cdot)$ are specified by\n$$\ns(u) = \\ln(1 + u^{2}), \\qquad t(u) = \\sin(u) + u^{3}.\n$$\nAssume the change-of-variables principle for probability densities and the standard Jacobian determinant rule under differentiable bijections. For the specific input\n$$\nz = \\left(1, \\frac{1}{2}\\right),\n$$\nperform the following tasks:\n- Compute the forward transformation $x = f(z)$.\n- Compute the inverse transformation $z = f^{-1}(x)$ evaluated at the forward image $x$ obtained above.\n- Compute the log absolute determinant of the Jacobian matrix of the forward transformation evaluated at $z$, which quantifies the local log-density change under the mapping.\n\nExpress your final results as a single row matrix containing, in order, the five quantities $(x_{1}, x_{2}, z_{1}^{\\mathrm{inv}}, z_{2}^{\\mathrm{inv}}, \\Delta)$, where $\\Delta$ denotes the log absolute determinant of the Jacobian of the forward transformation at $z$. No rounding is required, and your final answer must be exact.",
            "solution": "The user has provided a problem statement regarding a Real Non-Volume Preserving (RNVP) coupling transformation from the field of normalizing flows. I will first validate the problem statement.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Transformation**: A two-dimensional Real Non-Volume Preserving (RNVP) coupling transformation $f: \\mathbb{R}^{2} \\to \\mathbb{R}^{2}$, mapping $z = (z_{1}, z_{2})$ to $x = (x_{1}, x_{2})$.\n-   **Transformation Rule**:\n    $$\n    x_{1} = z_{1}\n    $$\n    $$\n    x_{2} = z_{2} \\cdot \\exp\\big(s(z_{1})\\big) + t(z_{1})\n    $$\n-   **Scale Function**: $s(u) = \\ln\\big(1 + u^{2}\\big)$.\n-   **Shift Function**: $t(u) = \\sin(u) + u^{3}$.\n-   **Input Point**: $z = \\left(1, \\frac{1}{2}\\right)$.\n-   **Required Computations**:\n    1.  The forward transformation $x = f(z)$ at the given input point.\n    2.  The inverse transformation $z^{\\mathrm{inv}} = f^{-1}(x)$ evaluated at the computed image $x$.\n    3.  The log absolute determinant of the Jacobian of the forward transformation, $\\Delta = \\ln|\\det(J_{f}(z))|$, evaluated at the given input point.\n-   **Output Format**: A single row matrix containing the five quantities $(x_{1}, x_{2}, z_{1}^{\\mathrm{inv}}, z_{2}^{\\mathrm{inv}}, \\Delta)$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Grounding**: The problem is based on established mathematical principles of normalizing flows, specifically the RNVP architecture. The transformation, functions, and Jacobian calculation are standard components in this area of machine learning and are mathematically sound.\n-   **Well-Posedness**: The transformation is a diffeomorphism. The scale function $s(u)$ and shift function $t(u)$ are infinitely differentiable for all $u \\in \\mathbb{R}$. The transformation is invertible since the term $\\exp(s(z_{1})) = \\exp(\\ln(1+z_1^2)) = 1+z_1^2$ is strictly positive, allowing for a unique solution for $z_{2}$. The problem is clearly defined and has a unique, stable solution.\n-   **Objectivity**: The problem statement is purely mathematical and objective, with no subjective or ambiguous language.\n\n**Step 3: Verdict and Action**\nThe problem is mathematically consistent, well-posed, and scientifically grounded. It is therefore **valid**. I will proceed with the solution.\n\n### Solution\n\nThe solution requires performing three distinct computations based on the provided RNVP transformation.\n\n**1. Forward Transformation $x = f(z)$**\n\nWe are given the input point $z = (z_{1}, z_{2}) = (1, \\frac{1}{2})$.\nThe transformation is defined as:\n$$\nx_{1} = z_{1}\n$$\n$$\nx_{2} = z_{2} \\cdot \\exp\\big(s(z_{1})\\big) + t(z_{1})\n$$\nFirst, we evaluate the scale function $s(z_{1})$ and the shift function $t(z_{1})$ at $z_{1} = 1$.\n$$\ns(1) = \\ln(1 + 1^{2}) = \\ln(2)\n$$\n$$\nt(1) = \\sin(1) + 1^{3} = \\sin(1) + 1\n$$\nNow, we can compute $x_{1}$ and $x_{2}$.\n$$\nx_{1} = z_{1} = 1\n$$\n$$\nx_{2} = z_{2} \\cdot \\exp(s(1)) + t(1) = \\frac{1}{2} \\cdot \\exp(\\ln(2)) + (\\sin(1) + 1)\n$$\nSince $\\exp(\\ln(2)) = 2$, this simplifies to:\n$$\nx_{2} = \\frac{1}{2} \\cdot 2 + \\sin(1) + 1 = 1 + \\sin(1) + 1 = 2 + \\sin(1)\n$$\nThus, the forward transformation yields the point $x = (x_{1}, x_{2}) = (1, 2 + \\sin(1))$.\n\n**2. Inverse Transformation $z^{\\mathrm{inv}} = f^{-1}(x)$**\n\nTo find the inverse transformation $f^{-1}$, we must solve the forward transformation equations for $z_{1}$ and $z_{2}$ in terms of $x_{1}$ and $x_{2}$.\nFrom $x_{1} = z_{1}$, we immediately have:\n$$\nz_{1} = x_{1}\n$$\nWe substitute this into the second equation:\n$$\nx_{2} = z_{2} \\cdot \\exp(s(x_{1})) + t(x_{1})\n$$\nNow, we solve for $z_{2}$:\n$$\nx_{2} - t(x_{1}) = z_{2} \\cdot \\exp(s(x_{1}))\n$$\n$$\nz_{2} = \\frac{x_{2} - t(x_{1})}{\\exp(s(x_{1}))} = \\big(x_{2} - t(x_{1})\\big) \\exp(-s(x_{1}))\n$$\nSo the inverse transformation $f^{-1}$ is given by:\n$$\nz_{1} = x_{1}\n$$\n$$\nz_{2} = \\big(x_{2} - t(x_{1})\\big) \\exp(-s(x_{1}))\n$$\nWe evaluate this inverse transformation at the point $x = (1, 2 + \\sin(1))$ computed in the previous step. Let the result be $z^{\\mathrm{inv}} = (z_{1}^{\\mathrm{inv}}, z_{2}^{\\mathrm{inv}})$.\n$$\nz_{1}^{\\mathrm{inv}} = x_{1} = 1\n$$\nFor $z_{2}^{\\mathrm{inv}}$, we first evaluate $s(x_{1})$ and $t(x_{1})$ at $x_{1} = 1$:\n$$\ns(1) = \\ln(1+1^{2}) = \\ln(2)\n$$\n$$\nt(1) = \\sin(1) + 1^{3} = \\sin(1) + 1\n$$\nNow substitute these into the expression for $z_{2}$:\n$$\nz_{2}^{\\mathrm{inv}} = \\big((2 + \\sin(1)) - (\\sin(1) + 1)\\big) \\exp(-\\ln(2))\n$$\n$$\nz_{2}^{\\mathrm{inv}} = (2 + \\sin(1) - \\sin(1) - 1) \\cdot \\frac{1}{\\exp(\\ln(2))}\n$$\n$$\nz_{2}^{\\mathrm{inv}} = (1) \\cdot \\frac{1}{2} = \\frac{1}{2}\n$$\nAs expected, the inverse transformation applied to the image of $z$ returns the original point. So, $z^{\\mathrm{inv}} = (z_{1}^{\\mathrm{inv}}, z_{2}^{\\mathrm{inv}}) = (1, \\frac{1}{2})$.\n\n**3. Log Absolute Determinant of the Jacobian, $\\Delta$**\n\nThe forward transformation is $x = f(z)$, where $x_{1} = z_{1}$ and $x_{2} = z_{2} \\exp(s(z_{1})) + t(z_{1})$. The Jacobian matrix $J_{f}(z)$ is given by:\n$$\nJ_{f}(z) = \\begin{pmatrix} \\frac{\\partial x_{1}}{\\partial z_{1}}  \\frac{\\partial x_{1}}{\\partial z_{2}} \\\\ \\frac{\\partial x_{2}}{\\partial z_{1}}  \\frac{\\partial x_{2}}{\\partial z_{2}} \\end{pmatrix}\n$$\nWe calculate the partial derivatives:\n$$\n\\frac{\\partial x_{1}}{\\partial z_{1}} = 1\n$$\n$$\n\\frac{\\partial x_{1}}{\\partial z_{2}} = 0\n$$\n$$\n\\frac{\\partial x_{2}}{\\partial z_{1}} = \\frac{\\partial}{\\partial z_{1}} \\left( z_{2} \\exp(s(z_{1})) + t(z_{1}) \\right) = z_{2} \\exp(s(z_{1})) s'(z_{1}) + t'(z_{1})\n$$\n$$\n\\frac{\\partial x_{2}}{\\partial z_{2}} = \\frac{\\partial}{\\partial z_{2}} \\left( z_{2} \\exp(s(z_{1})) + t(z_{1}) \\right) = \\exp(s(z_{1}))\n$$\nThe Jacobian matrix is:\n$$\nJ_{f}(z) = \\begin{pmatrix} 1  0 \\\\ z_{2} \\exp(s(z_{1})) s'(z_{1}) + t'(z_{1})  \\exp(s(z_{1})) \\end{pmatrix}\n$$\nThis is a lower triangular matrix. The determinant of a triangular matrix is the product of its diagonal elements.\n$$\n\\det(J_{f}(z)) = 1 \\cdot \\exp(s(z_{1})) = \\exp(s(z_{1}))\n$$\nThe log absolute determinant $\\Delta$ is:\n$$\n\\Delta = \\ln |\\det(J_{f}(z))| = \\ln |\\exp(s(z_{1}))|\n$$\nSince $\\exp(\\cdot)$ is always positive, the absolute value is redundant.\n$$\n\\Delta = \\ln(\\exp(s(z_{1}))) = s(z_{1})\n$$\nThis is a characteristic property of RNVP coupling layers which makes the Jacobian determinant computationally efficient. We evaluate $\\Delta$ at the input point $z = (1, \\frac{1}{2})$, which only depends on $z_{1}=1$.\n$$\n\\Delta = s(1) = \\ln(1 + 1^{2}) = \\ln(2)\n$$\n\n**Final Assembly**\n\nWe now assemble the five computed quantities $(x_{1}, x_{2}, z_{1}^{\\mathrm{inv}}, z_{2}^{\\mathrm{inv}}, \\Delta)$ into a single row matrix:\n-   $x_{1} = 1$\n-   $x_{2} = 2 + \\sin(1)$\n-   $z_{1}^{\\mathrm{inv}} = 1$\n-   $z_{2}^{\\mathrm{inv}} = \\frac{1}{2}$\n-   $\\Delta = \\ln(2)$\n\nThe resulting vector is $(1, 2 + \\sin(1), 1, \\frac{1}{2}, \\ln(2))$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  2 + \\sin(1)  1  \\frac{1}{2}  \\ln(2)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Building on the concept of invertible transformations, this practice guides you through constructing a complete, albeit simple, two-dimensional density model using an autoregressive normalizing flow. You will apply the change of variables principle to derive the exact log-probability density of the transformed data, starting from a standard Gaussian base distribution . This exercise solidifies the end-to-end process of defining a target density with a normalizing flow, which is central to their use in both density estimation and as proposal distributions for advanced Monte Carlo methods.",
            "id": "3318881",
            "problem": "Consider a two-dimensional Normalizing Flow (NF) from a base latent variable $z = (z_{1}, z_{2})$ to an observed variable $x = (x_{1}, x_{2})$. The base distribution $p_{Z}(z)$ is the standard bivariate Gaussian with independent components, and the NF is constructed as a triangular autoregressive affine transformation. Specifically, define the forward mapping $f: \\mathbb{R}^{2} \\to \\mathbb{R}^{2}$ by\n$$\nx_{1} = \\sigma_{1} z_{1} + \\mu_{1}, \\quad x_{2} = \\sigma_{2}(x_{1}) z_{2} + \\mu_{2}(x_{1}),\n$$\nwith explicit choices:\n$$\n\\mu_{1} = 1, \\quad \\sigma_{1} = 2, \\quad \\mu_{2}(x_{1}) = x_{1}, \\quad \\sigma_{2}(x_{1}) = \\exp\\!\\big(\\ln 3 + (\\ln 2)\\, x_{1}\\big) = 3 \\cdot 2^{x_{1}}.\n$$\nThe base density is\n$$\np_{Z}(z) = \\frac{1}{2\\pi} \\exp\\!\\left( -\\frac{1}{2}\\left( z_{1}^{2} + z_{2}^{2} \\right) \\right).\n$$\n\nTasks:\n1. Starting from the definitions above, derive the explicit forward and inverse mappings between $z$ and $x$.\n2. Using only fundamental principles of density transformation under invertible mappings and the definition of the multivariate Gaussian density, derive a general analytic expression for $\\log p_{X}(x)$ for this NF, expressed in terms of $x_{1}$ and $x_{2}$.\n3. Evaluate $\\log p_{X}(x)$ at the point $x = (1, 1)$ and give the final answer as a single closed-form expression involving natural logarithms. Do not approximate; express your final answer exactly with $\\ln$ and $\\pi$.\n\nYour final answer must be a single closed-form analytic expression. No rounding is required.",
            "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a unique solution. The concepts of Normalizing Flows and the change of variables formula for probability densities are standard in the field of machine learning and statistics. Therefore, the problem is valid. We proceed with the solution.\n\nThe tasks are to find the forward and inverse mappings, derive the log-probability density $\\log p_{X}(x)$, and evaluate it at a specific point.\n\n**Task 1: Forward and Inverse Mappings**\n\nThe forward mapping $f: \\mathbb{R}^{2} \\to \\mathbb{R}^{2}$ transforms a latent variable $z = (z_1, z_2)$ to an observed variable $x = (x_1, x_2)$. The general form is given by:\n$$x_{1} = \\sigma_{1} z_{1} + \\mu_{1}$$\n$$x_{2} = \\sigma_{2}(x_{1}) z_{2} + \\mu_{2}(x_{1})$$\nSubstituting the provided parameters $\\mu_{1} = 1$, $\\sigma_{1} = 2$, $\\mu_{2}(x_{1}) = x_{1}$, and $\\sigma_{2}(x_{1}) = 3 \\cdot 2^{x_{1}}$, we obtain the explicit forward mapping $x = f(z)$:\n$$x_{1} = 2 z_{1} + 1$$\n$$x_{2} = (3 \\cdot 2^{x_{1}}) z_{2} + x_{1}$$\n\nTo find the inverse mapping $z = f^{-1}(x)$, we solve the system of equations for $z_1$ and $z_2$ in terms of $x_1$ and $x_2$. From the first equation:\n$$2 z_{1} = x_{1} - 1 \\implies z_{1} = \\frac{x_{1} - 1}{2}$$\nSubstituting this into the second equation:\n$$(3 \\cdot 2^{x_{1}}) z_{2} = x_{2} - x_{1} \\implies z_{2} = \\frac{x_{2} - x_{1}}{3 \\cdot 2^{x_{1}}}$$\nThus, the explicit inverse mapping $z=f^{-1}(x)$ is:\n$$z_{1}(x) = \\frac{x_{1} - 1}{2}$$\n$$z_{2}(x) = \\frac{x_{2} - x_{1}}{3 \\cdot 2^{x_{1}}}$$\n\n**Task 2: General Analytic Expression for $\\log p_{X}(x)$**\n\nThe probability density of the transformed variable $x$ is given by the change of variables formula:\n$$p_{X}(x) = p_{Z}(f^{-1}(x)) \\left| \\det\\left( J_{f^{-1}}(x) \\right) \\right|$$\nwhere $J_{f^{-1}}(x)$ is the Jacobian of the inverse transformation. An equivalent and often more convenient form uses the Jacobian of the forward transformation, $J_{f}(z)$:\n$$p_{X}(x) = p_{Z}(f^{-1}(x)) \\left| \\det\\left( J_{f}(z) \\right) \\right|^{-1}$$\nwhere $z=f^{-1}(x)$. Taking the natural logarithm of this expression yields:\n$$\\log p_{X}(x) = \\log p_{Z}(f^{-1}(x)) - \\log \\left| \\det\\left( J_{f}(z) \\right) \\right|$$\n\nFirst, we compute the Jacobian of the forward map, $J_{f}(z) = \\frac{\\partial x}{\\partial z}$.\n$$J_{f}(z) = \\begin{pmatrix} \\frac{\\partial x_{1}}{\\partial z_{1}}  \\frac{\\partial x_{1}}{\\partial z_{2}} \\\\ \\frac{\\partial x_{2}}{\\partial z_{1}}  \\frac{\\partial x_{2}}{\\partial z_{2}} \\end{pmatrix}$$\nThe components are:\n$\\frac{\\partial x_{1}}{\\partial z_{1}} = \\sigma_{1} = 2$\n$\\frac{\\partial x_{1}}{\\partial z_{2}} = 0$\n$\\frac{\\partial x_{2}}{\\partial z_{2}} = \\sigma_{2}(x_{1}) = 3 \\cdot 2^{x_{1}}$\nSince the transformation is triangular, the Jacobian matrix is lower triangular:\n$$J_{f}(z) = \\begin{pmatrix} \\sigma_{1}  0 \\\\ \\frac{\\partial x_{2}}{\\partial z_{1}}  \\sigma_{2}(x_{1}) \\end{pmatrix}$$\nThe determinant of a triangular matrix is the product of its diagonal elements:\n$$\\det(J_{f}(z)) = \\sigma_{1} \\cdot \\sigma_{2}(x_{1})$$\nGiven that $\\sigma_{1} = 2 > 0$ and $\\sigma_{2}(x_{1}) = 3 \\cdot 2^{x_{1}} > 0$ for all $x_1 \\in \\mathbb{R}$, the absolute value is redundant. The log-determinant is:\n$$\\log(\\det(J_{f}(z))) = \\log(\\sigma_{1}) + \\log(\\sigma_{2}(x_{1}))$$\nSubstituting the given functions:\n$$\\log(\\det(J_{f}(z))) = \\ln(2) + \\ln(3 \\cdot 2^{x_{1}}) = \\ln(2) + \\ln(3) + \\ln(2^{x_{1}}) = \\ln(2) + \\ln(3) + x_{1}\\ln(2) = (1+x_{1})\\ln(2) + \\ln(3)$$\n\nNext, we evaluate the log-density of the base distribution at $z=f^{-1}(x)$. The base density is $p_{Z}(z) = \\frac{1}{2\\pi} \\exp\\left( -\\frac{1}{2}(z_{1}^{2} + z_{2}^{2}) \\right)$. Its logarithm is:\n$$\\log p_{Z}(z) = -\\ln(2\\pi) - \\frac{1}{2}(z_{1}^{2} + z_{2}^{2})$$\nSubstituting the inverse mappings $z_1(x)$ and $z_2(x)$:\n$$\\log p_{Z}(f^{-1}(x)) = -\\ln(2\\pi) - \\frac{1}{2} \\left[ \\left(\\frac{x_{1} - 1}{2}\\right)^{2} + \\left(\\frac{x_{2} - x_{1}}{3 \\cdot 2^{x_{1}}}\\right)^{2} \\right]$$\n\nFinally, we combine the terms to obtain $\\log p_{X}(x)$:\n$$\\log p_{X}(x) = \\left(-\\ln(2\\pi) - \\frac{1}{2} \\left[ \\left(\\frac{x_{1} - 1}{2}\\right)^{2} + \\left(\\frac{x_{2} - x_{1}}{3 \\cdot 2^{x_{1}}}\\right)^{2} \\right]\\right) - \\left((1+x_{1})\\ln(2) + \\ln(3)\\right)$$\nExpanding this expression gives the general formula for the log-density:\n$$\\log p_{X}(x) = -\\ln(2) - \\ln(\\pi) - \\frac{(x_{1}-1)^{2}}{8} - \\frac{(x_{2}-x_{1})^{2}}{18 \\cdot 4^{x_{1}}} - (1+x_{1})\\ln(2) - \\ln(3)$$\n$$\\log p_{X}(x) = -\\frac{(x_{1}-1)^{2}}{8} - \\frac{(x_{2}-x_{1})^{2}}{18 \\cdot 4^{x_{1}}} - (x_{1}+2)\\ln(2) - \\ln(3) - \\ln(\\pi)$$\n\n**Task 3: Evaluation at $x=(1, 1)$**\n\nWe evaluate the expression for $\\log p_{X}(x)$ at the point $x=(x_{1}, x_{2}) = (1, 1)$.\n$$\\log p_{X}(1, 1) = -\\frac{(1-1)^{2}}{8} - \\frac{(1-1)^{2}}{18 \\cdot 4^{1}} - (1+2)\\ln(2) - \\ln(3) - \\ln(\\pi)$$\nThe first two terms are zero:\n$$\\log p_{X}(1, 1) = -0 - 0 - 3\\ln(2) - \\ln(3) - \\ln(\\pi)$$\nSimplifying this, we get the final result:\n$$\\log p_{X}(1, 1) = -3\\ln(2) - \\ln(3) - \\ln(\\pi)$$\nThis is the required single closed-form analytic expression.",
            "answer": "$$\n\\boxed{-3\\ln(2) - \\ln(3) - \\ln(\\pi)}\n$$"
        }
    ]
}