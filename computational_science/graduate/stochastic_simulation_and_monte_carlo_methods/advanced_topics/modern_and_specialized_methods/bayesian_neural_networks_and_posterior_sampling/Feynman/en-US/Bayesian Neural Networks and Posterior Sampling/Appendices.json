{
    "hands_on_practices": [
        {
            "introduction": "Effective Bayesian modeling often involves designing priors that encode specific structural assumptions, such as sparsity or feature relevance. This exercise explores Automatic Relevance Determination (ARD), a classic hierarchical Bayesian technique where each weight is given its own precision, controlled by a hyperprior. By deriving the conditional posteriors for this model, you will practice using conjugate priors and lay the groundwork for understanding Gibbs sampling, a fundamental MCMC algorithm. ",
            "id": "3291240",
            "problem": "Consider a Bayesian neural network regression model in which the final linear readout has weights $w \\in \\mathbb{R}^{d}$ acting on a fixed feature matrix $X \\in \\mathbb{R}^{n \\times d}$, with observed targets $y \\in \\mathbb{R}^{n}$. The observation model is the Gaussian likelihood $y \\mid X, w \\sim \\mathcal{N}(Xw, \\sigma_{y}^{2} I_{n})$, where $\\sigma_{y}^{2} > 0$ is known and $I_{n}$ is the $n \\times n$ identity matrix. To enable Automatic Relevance Determination (ARD), place independent Gaussian priors on the weights conditional on precisions $\\alpha = (\\alpha_{1}, \\dots, \\alpha_{d})$:\n$$\np(w \\mid \\alpha) = \\prod_{j=1}^{d} \\mathcal{N}(w_{j}; 0, \\alpha_{j}^{-1}) ,\n$$\nand independent Gamma hyperpriors on the precisions $\\alpha_{j}$:\n$$\np(\\alpha_{j}) = \\mathrm{Gamma}(\\alpha_{j}; a, b) ,\n$$\nwhere $a > 0$ and $b > 0$ are hyperparameters, and the Gamma density is parameterized in shape–rate form as\n$$\n\\mathrm{Gamma}(\\alpha_{j}; a, b) = \\frac{b^{a}}{\\Gamma(a)} \\, \\alpha_{j}^{a-1} \\exp(- b \\alpha_{j}) .\n$$\nUsing only Bayes’ rule, conjugacy properties of the Gaussian and Gamma families, and linear algebra identities for quadratic forms, derive the full conditional posterior distributions $p(w \\mid \\alpha, D)$ and $p(\\alpha \\mid w)$, where $D = \\{X, y\\}$. Your derivation must start from the definitions of the likelihood and prior and proceed to identify the normalized forms of the conditionals, explicitly specifying the mean vector and covariance matrix for the Gaussian $p(w \\mid \\alpha, D)$ and the shape and rate parameters for each Gamma component of $p(\\alpha \\mid w)$. Express your final answer as a single closed-form analytic expression. No numerical evaluation or rounding is required.",
            "solution": "The problem requires the derivation of the full conditional posterior distributions for the weights $w$ and the precision hyperparameters $\\alpha$ in a Bayesian linear regression model. The model is specified by a Gaussian likelihood, a Gaussian prior on the weights, and Gamma hyperpriors on the precisions of the weight priors. We will derive each conditional posterior in turn using Bayes' rule and properties of conjugate distributions. The dataset is denoted as $D = \\{X, y\\}$.\n\nFirst, we derive the conditional posterior for the weights, $p(w \\mid \\alpha, D)$.\nBy Bayes' rule, the posterior distribution of $w$ given $\\alpha$ and the data $D$ is proportional to the product of the likelihood and the prior for $w$:\n$$\np(w \\mid \\alpha, D) \\propto p(y \\mid X, w) p(w \\mid \\alpha)\n$$\nThe likelihood is given as $p(y \\mid X, w) = \\mathcal{N}(y; Xw, \\sigma_{y}^{2} I_{n})$. Its probability density function is proportional to:\n$$\np(y \\mid X, w) \\propto \\exp\\left( -\\frac{1}{2\\sigma_{y}^{2}} (y - Xw)^T (y - Xw) \\right)\n$$\nThe prior on the weights is a multivariate Gaussian, $p(w \\mid \\alpha) = \\mathcal{N}(w; 0, A^{-1})$, where $A = \\mathrm{diag}(\\alpha_{1}, \\dots, \\alpha_{d})$. Its probability density function is proportional to:\n$$\np(w \\mid \\alpha) \\propto \\exp\\left( -\\frac{1}{2} w^T A w \\right)\n$$\nMultiplying the likelihood and the prior, we get the posterior for $w$:\n$$\np(w \\mid \\alpha, D) \\propto \\exp\\left( -\\frac{1}{2\\sigma_{y}^{2}} (y - Xw)^T (y - Xw) - \\frac{1}{2} w^T A w \\right)\n$$\nTo identify the form of this distribution, we analyze the exponent, focusing on terms involving $w$. Let the exponent be $L(w)$.\n$$\nL(w) = -\\frac{1}{2\\sigma_{y}^{2}} (y^T y - 2y^T Xw + w^T X^T X w) - \\frac{1}{2} w^T A w\n$$\nIgnoring terms not dependent on $w$, we have:\n$$\nL(w) = -\\frac{1}{2} \\left( \\frac{1}{\\sigma_{y}^{2}} w^T X^T X w - 2 \\frac{1}{\\sigma_{y}^{2}} y^T X w + w^T A w \\right) + \\text{const.}\n$$\n$$\nL(w) = -\\frac{1}{2} \\left( w^T \\left(\\frac{1}{\\sigma_{y}^{2}} X^T X + A\\right) w - 2 \\frac{1}{\\sigma_{y}^{2}} y^T X w \\right) + \\text{const.}\n$$\nThis is a quadratic form in $w$. The kernel of a multivariate Gaussian distribution $\\mathcal{N}(w; \\mu, \\Sigma)$ is proportional to $\\exp\\left(-\\frac{1}{2} (w - \\mu)^T \\Sigma^{-1} (w - \\mu)\\right)$, which expands to $\\exp\\left(-\\frac{1}{2} (w^T \\Sigma^{-1} w - 2\\mu^T \\Sigma^{-1} w + \\mu^T \\Sigma^{-1} \\mu)\\right)$.\nBy comparing the terms, we can identify the inverse covariance matrix (precision matrix) $\\Sigma_w^{-1}$ and the mean $\\mu_w$ of the posterior distribution.\nThe quadratic term in $w$ gives the precision matrix:\n$$\n\\Sigma_{w}^{-1} = \\frac{1}{\\sigma_{y}^{2}} X^T X + A\n$$\nThe linear term in $w$ gives the mean. We have $2\\mu_w^T \\Sigma_w^{-1} w = 2 \\frac{1}{\\sigma_y^2} y^T X w$. So, $\\mu_w^T \\Sigma_w^{-1} = \\frac{1}{\\sigma_y^2} y^T X$. Taking the transpose yields $\\Sigma_w^{-1} \\mu_w = \\frac{1}{\\sigma_y^2} X^T y$.\nSolving for the mean $\\mu_w$:\n$$\n\\mu_w = \\left(\\Sigma_{w}^{-1}\\right)^{-1} \\left(\\frac{1}{\\sigma_{y}^{2}} X^T y\\right) = \\left(\\frac{1}{\\sigma_{y}^{2}} X^T X + A\\right)^{-1} \\left(\\frac{1}{\\sigma_{y}^{2}} X^T y\\right)\n$$\nThus, the conditional posterior for $w$ is a multivariate Gaussian distribution $p(w \\mid \\alpha, D) = \\mathcal{N}(w; \\mu_w, \\Sigma_w)$ with covariance matrix $\\Sigma_w = \\left(\\frac{1}{\\sigma_{y}^{2}} X^T X + A\\right)^{-1}$ and mean vector $\\mu_w = \\frac{1}{\\sigma_{y}^{2}} \\Sigma_w X^T y$.\n\nNext, we derive the conditional posterior for the precisions, $p(\\alpha \\mid w)$. Note that from the model structure, $\\alpha$ is conditionally independent of the data $D=\\{X, y\\}$ given $w$. Therefore, $p(\\alpha \\mid w, D) = p(\\alpha \\mid w)$.\nBy Bayes' rule:\n$$\np(\\alpha \\mid w) \\propto p(w \\mid \\alpha) p(\\alpha)\n$$\nThe priors $p(w \\mid \\alpha)$ and $p(\\alpha)$ are defined to be factorizable over their components:\n$$\np(w \\mid \\alpha) = \\prod_{j=1}^{d} p(w_{j} \\mid \\alpha_{j}) \\quad \\text{and} \\quad p(\\alpha) = \\prod_{j=1}^{d} p(\\alpha_{j})\n$$\nThis implies that the posterior for $\\alpha$ also factorizes:\n$$\np(\\alpha \\mid w) = \\prod_{j=1}^{d} p(\\alpha_{j} \\mid w_{j})\n$$\nWe can derive the posterior for each $\\alpha_{j}$ independently. For a single component $j$:\n$$\np(\\alpha_{j} \\mid w_{j}) \\propto p(w_{j} \\mid \\alpha_{j}) p(\\alpha_{j})\n$$\nThe distributions are given as:\n$$\np(w_{j} \\mid \\alpha_{j}) = \\mathcal{N}(w_{j}; 0, \\alpha_{j}^{-1}) = \\left(\\frac{\\alpha_{j}}{2\\pi}\\right)^{1/2} \\exp\\left(-\\frac{\\alpha_{j} w_{j}^2}{2}\\right)\n$$\n$$\np(\\alpha_{j}) = \\mathrm{Gamma}(\\alpha_{j}; a, b) = \\frac{b^{a}}{\\Gamma(a)} \\alpha_{j}^{a-1} \\exp(-b \\alpha_{j})\n$$\nMultiplying these and ignoring constants with respect to $\\alpha_j$:\n$$\np(\\alpha_{j} \\mid w_{j}) \\propto \\left[ \\alpha_{j}^{1/2} \\exp\\left(-\\frac{w_{j}^2}{2} \\alpha_{j}\\right) \\right] \\left[ \\alpha_{j}^{a-1} \\exp(-b \\alpha_{j}) \\right]\n$$\nCombining terms in the exponent and the base $\\alpha_j$:\n$$\np(\\alpha_{j} \\mid w_{j}) \\propto \\alpha_{j}^{a - 1 + 1/2} \\exp\\left(-\\left(b + \\frac{w_{j}^2}{2}\\right)\\alpha_{j}\\right)\n$$\n$$\np(\\alpha_{j} \\mid w_{j}) \\propto \\alpha_{j}^{\\left(a + \\frac{1}{2}\\right) - 1} \\exp\\left(-\\left(b + \\frac{w_{j}^2}{2}\\right)\\alpha_{j}\\right)\n$$\nThis is the kernel of a Gamma distribution $\\mathrm{Gamma}(\\alpha_j; a', b')$, which is proportional to $\\alpha_j^{a'-1}\\exp(-b'\\alpha_j)$. By comparison, we identify the parameters of the posterior distribution for $\\alpha_j$:\nThe posterior shape parameter is $a' = a + \\frac{1}{2}$.\nThe posterior rate parameter is $b' = b + \\frac{w_{j}^2}{2}$.\nThis holds for each $j \\in \\{1, \\dots, d\\}$. Therefore, the full conditional posterior for $\\alpha$ is a product of independent Gamma distributions:\n$$\np(\\alpha \\mid w) = \\prod_{j=1}^{d} \\mathrm{Gamma}\\left(\\alpha_{j}; a + \\frac{1}{2}, b + \\frac{w_j^2}{2}\\right)\n$$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\np(w \\mid \\alpha, D) = \\mathcal{N}\\left(w; \\frac{1}{\\sigma_y^2} \\left(\\frac{1}{\\sigma_y^2} X^T X + A\\right)^{-1} X^T y, \\left(\\frac{1}{\\sigma_y^2} X^T X + A\\right)^{-1}\\right) & p(\\alpha \\mid w) = \\prod_{j=1}^{d} \\mathrm{Gamma}\\left(\\alpha_j; a + \\frac{1}{2}, b + \\frac{w_j^2}{2}\\right)\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While Gibbs sampling is powerful, the complex, non-conjugate posteriors of deep neural networks require more general methods. This practice introduces Stochastic Gradient Langevin Dynamics (SGLD), a scalable MCMC algorithm that injects noise into the standard stochastic gradient descent process to sample from the target posterior. You will derive the SGLD update rule from first principles, connecting the theory of stochastic differential equations to a practical algorithm for training BNNs on large datasets. ",
            "id": "3291187",
            "problem": "Consider a Bayesian neural network with weights $w \\in \\mathbb{R}^{d}$, prior $p(w)=\\mathcal{N}(0,\\sigma_{p}^{2} I_{d})$, and data $\\mathcal{D}=\\{(x_{i},y_{i})\\}_{i=1}^{N}$ modeled with additive Gaussian observation noise of known variance $\\sigma_{y}^{2}$ around the network output $f_{w}(x)$, so that $p(y_{i}\\mid x_{i},w)=\\mathcal{N}(f_{w}(x_{i}),\\sigma_{y}^{2})$ independently for $i=1,\\dots,N$. The posterior satisfies $p(w\\mid \\mathcal{D}) \\propto p(w)\\prod_{i=1}^{N}p(y_{i}\\mid x_{i},w)$. Let $B_{t}\\subset\\{1,\\dots,N\\}$ be a mini-batch of size $m=|B_{t}|$ sampled uniformly without replacement at iteration $t$, and let $\\xi_{t}\\sim\\mathcal{N}(0,I_{d})$ be independent across $t$. Starting from the overdamped Langevin diffusion targeting the posterior and its Euler–Maruyama discretization, derive the stochastic gradient Langevin dynamics update using the unbiased mini-batch estimator of the posterior score based on $B_{t}$. Express your final update at iteration $t$ explicitly in terms of $w_{t}$, $\\sigma_{p}^{2}$, $\\sigma_{y}^{2}$, $N$, $m$, $f_{w_{t}}(x)$, and $\\nabla_{w} f_{w_{t}}(x)$.\n\nThen, state the necessary step-size schedule conditions on the sequence $\\{\\eta_{t}\\}_{t\\geq 1}$ under which the resulting Markov chain asymptotically samples from the exact posterior in the limit $t\\to\\infty$ (assume standard regularity conditions ensuring stability, including measurability, Lipschitz continuity of the score, and bounded second moments of the mini-batch gradient noise, hold). Your final answer must contain only the derived update and the step-size conditions. Do not include any explanatory text. If you provide multiple items, present them as a single row vector. The answer does not require rounding and has no physical units.",
            "solution": "The problem requires the derivation of the Stochastic Gradient Langevin Dynamics (SGLD) update rule for a Bayesian neural network and the statement of the step-size conditions for its convergence to the posterior distribution.\n\nFirst, we formalize the target distribution. The posterior distribution over the weights $w \\in \\mathbb{R}^{d}$, given the data $\\mathcal{D}=\\{(x_{i},y_{i})\\}_{i=1}^{N}$, is given by Bayes' theorem:\n$$\np(w \\mid \\mathcal{D}) \\propto p(\\mathcal{D} \\mid w) p(w)\n$$\nGiven the independence of the data points, the likelihood is $p(\\mathcal{D} \\mid w) = \\prod_{i=1}^{N} p(y_{i} \\mid x_{i}, w)$. Thus, the posterior is:\n$$\np(w \\mid \\mathcal{D}) \\propto p(w) \\prod_{i=1}^{N} p(y_{i} \\mid x_{i}, w)\n$$\nIt is more convenient to work with the log-posterior, which is given by:\n$$\n\\ln p(w \\mid \\mathcal{D}) = \\ln p(w) + \\sum_{i=1}^{N} \\ln p(y_{i} \\mid x_{i}, w) + C\n$$\nwhere $C$ is a normalization constant independent of $w$.\n\nThe problem specifies a Gaussian prior $p(w) = \\mathcal{N}(0, \\sigma_{p}^{2} I_{d})$ and a Gaussian likelihood $p(y_{i} \\mid x_{i}, w) = \\mathcal{N}(f_{w}(x_{i}), \\sigma_{y}^{2})$. The corresponding log-densities, ignoring constants, are:\n$$\n\\ln p(w) = -\\frac{1}{2\\sigma_{p}^{2}} w^{T}w + C_{p} = -\\frac{1}{2\\sigma_{p}^{2}} \\|w\\|_{2}^{2} + C_{p}\n$$\n$$\n\\ln p(y_{i} \\mid x_{i}, w) = -\\frac{1}{2\\sigma_{y}^{2}} (y_{i} - f_{w}(x_{i}))^{2} + C_{y}\n$$\nSubstituting these into the log-posterior expression, we get:\n$$\n\\ln p(w \\mid \\mathcal{D}) = -\\frac{1}{2\\sigma_{p}^{2}} \\|w\\|_{2}^{2} - \\frac{1}{2\\sigma_{y}^{2}} \\sum_{i=1}^{N} (y_{i} - f_{w}(x_{i}))^{2} + C'\n$$\nThe SGLD algorithm is a discretization of an overdamped Langevin diffusion process. The continuous-time Langevin diffusion that has $p(w \\mid \\mathcal{D})$ as its invariant distribution is described by the stochastic differential equation (SDE):\n$$\ndw_{t} = \\frac{1}{2} \\nabla_{w} \\ln p(w_{t} \\mid \\mathcal{D}) dt + d\\mathcal{W}_{t}\n$$\nwhere $\\mathcal{W}_{t}$ is a standard $d$-dimensional Wiener process.\nThe Euler-Maruyama discretization of this SDE with a step-size $\\eta_{t}$ leads to the Langevin Dynamics (LD) update:\n$$\nw_{t+1} = w_{t} + \\frac{\\eta_{t}}{2} \\nabla_{w} \\ln p(w_{t} \\mid \\mathcal{D}) + \\sqrt{\\eta_{t}} \\xi_{t}\n$$\nwhere $\\xi_{t} \\sim \\mathcal{N}(0, I_{d})$ is a vector of independent standard Gaussian noise.\n\nThe next step is to compute the gradient of the log-posterior, also known as the score.\n$$\n\\nabla_{w} \\ln p(w \\mid \\mathcal{D}) = \\nabla_{w} \\left( -\\frac{1}{2\\sigma_{p}^{2}} w^{T}w - \\frac{1}{2\\sigma_{y}^{2}} \\sum_{i=1}^{N} (y_{i} - f_{w}(x_{i}))^{2} \\right)\n$$\nUsing the chain rule, we obtain:\n$$\n\\nabla_{w} \\ln p(w \\mid \\mathcal{D}) = -\\frac{1}{\\sigma_{p}^{2}} w - \\frac{1}{2\\sigma_{y}^{2}} \\sum_{i=1}^{N} 2(y_{i} - f_{w}(x_{i}))(-\\nabla_{w} f_{w}(x_{i}))\n$$\n$$\n\\nabla_{w} \\ln p(w \\mid \\mathcal{D}) = -\\frac{1}{\\sigma_{p}^{2}} w + \\frac{1}{\\sigma_{y}^{2}} \\sum_{i=1}^{N} (y_{i} - f_{w}(x_{i})) \\nabla_{w} f_{w}(x_{i})\n$$\nFor large datasets where $N$ is large, computing this full gradient at each iteration is computationally prohibitive. SGLD approximates this gradient using a mini-batch of data. Let $B_{t} \\subset \\{1, \\dots, N\\}$ be a mini-batch of size $m$ sampled uniformly at random. An unbiased estimator of the full gradient is constructed by scaling the sum over the mini-batch:\n$$\n\\widehat{\\nabla_{w} \\ln p(w \\mid \\mathcal{D})} = -\\frac{1}{\\sigma_{p}^{2}} w + \\frac{N}{m} \\sum_{i \\in B_{t}} \\frac{1}{\\sigma_{y}^{2}}(y_{i} - f_{w}(x_{i})) \\nabla_{w} f_{w}(x_{i})\n$$\nSubstituting this stochastic gradient estimator into the LD update equation yields the SGLD update rule:\n$$\nw_{t+1} = w_{t} + \\frac{\\eta_{t}}{2} \\left[ -\\frac{1}{\\sigma_{p}^{2}} w_{t} + \\frac{N}{m\\sigma_{y}^{2}} \\sum_{i \\in B_{t}} (y_{i} - f_{w_{t}}(x_{i})) \\nabla_{w} f_{w_{t}}(x_{i}) \\right] + \\sqrt{\\eta_{t}}\\xi_{t}\n$$\nThis expression can be rewritten to clearly show the components:\n$$\nw_{t+1} = w_{t} - \\frac{\\eta_{t}}{2\\sigma_{p}^{2}} w_{t} + \\frac{\\eta_{t}N}{2m\\sigma_{y}^{2}} \\sum_{i \\in B_{t}} (y_{i} - f_{w_{t}}(x_{i})) \\nabla_{w} f_{w_{t}}(x_{i}) + \\sqrt{\\eta_{t}}\\xi_{t}\n$$\n\nFor the SGLD iterates to converge in distribution to the true posterior $p(w \\mid \\mathcal{D})$, the step-size schedule $\\{\\eta_{t}\\}_{t\\geq 1}$ must satisfy specific conditions. These conditions are required to balance the exploration of the state space (driven by the noise term and the gradient) with the reduction of the discretization error and the variance from the stochastic gradient estimation. The standard conditions, analogous to those for stochastic approximation (Robbins-Monro conditions), are:\n1. The step sizes must not decay too quickly, to allow the process to explore the entire state space. This is ensured by the condition that the sum of step sizes must diverge:\n$$\n\\sum_{t=1}^{\\infty} \\eta_{t} = \\infty\n$$\n2. The step sizes must decay quickly enough to ensure that the variance of the injected noise decreases over time and that the discretization error of the Euler-Maruyama scheme becomes negligible in the limit. This is ensured by the condition that the sum of the squares of the step sizes must converge:\n$$\n\\sum_{t=1}^{\\infty} \\eta_{t}^{2} < \\infty\n$$\nUnder these conditions, and assuming standard regularity of the target distribution and the network function, the distribution of $w_{t}$ generated by the SGLD algorithm converges weakly to the posterior distribution $p(w \\mid \\mathcal{D})$ as $t \\to \\infty$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} w_{t+1} = w_{t} + \\frac{\\eta_{t}}{2} \\left[ -\\frac{1}{\\sigma_{p}^{2}} w_{t} + \\frac{N}{m\\sigma_{y}^{2}} \\sum_{i \\in B_{t}} (y_{i} - f_{w_{t}}(x_{i})) \\nabla_{w} f_{w_{t}}(x_{i}) \\right] + \\sqrt{\\eta_{t}}\\xi_{t} & \\sum_{t=1}^{\\infty} \\eta_{t} = \\infty & \\sum_{t=1}^{\\infty} \\eta_{t}^{2} < \\infty \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A primary motivation for using Bayesian neural networks is their ability to quantify uncertainty. This practice focuses on estimating epistemic uncertainty—the model's uncertainty about its own parameters—which is crucial for applications like active learning and out-of-distribution detection. You will derive a Monte Carlo estimator for the mutual information between model weights and predictions, translating a key concept from information theory into a practical computational tool. ",
            "id": "3291210",
            "problem": "Consider a classification Bayesian Neural Network (BNN), defined as a probabilistic model over weights $w$ with posterior distribution $p(w \\mid D)$ after observing dataset $D$. For an input $x^\\star$, the BNN produces class probabilities via a softmax output $q_k(w, x^\\star)$ for classes $k \\in \\{1, \\dots, K\\}$, which form a valid discrete distribution for each $w$. The predictive distribution over labels $y^\\star$ given $x^\\star$ and $D$ is determined by averaging these class probabilities with respect to the posterior over $w$. The mutual information between $y^\\star$ and $w$ conditioned on $x^\\star$ and $D$ quantifies epistemic uncertainty and is a central quantity in Bayesian Active Learning by Disagreement (BALD). Your task is to derive, from first principles in probability and information theory, an implementable Monte Carlo (MC) estimator for this mutual information using independent samples $w^{(s)} \\sim p(w \\mid D)$.\n\nUse the following foundational base:\n- The definition of the posterior predictive distribution for a discrete label: the probability of $y^\\star = k$ given $x^\\star$ and $D$ is the expectation of $q_k(w, x^\\star)$ under $p(w \\mid D)$.\n- Shannon entropy for a discrete distribution in natural units (nats): for probabilities $\\{p_k\\}_{k=1}^K$, $H = -\\sum_{k=1}^K p_k \\log p_k$, where $\\log$ denotes the natural logarithm.\n- The chain rule definition of conditional mutual information: for random variables $A$, $B$, and $C$, $I(A; B \\mid C) = H(A \\mid C) - H(A \\mid B, C)$.\n\nStarting from these definitions, derive an estimator where expectations with respect to $p(w \\mid D)$ are replaced by empirical averages over $S$ independent posterior samples $w^{(s)}$, each yielding a class-probability vector $\\big(q_1(w^{(s)}, x^\\star), \\dots, q_K(w^{(s)}, x^\\star)\\big)$. Then, implement a program that computes this MC estimator of mutual information in nats for a given set of posterior class-probability samples.\n\nNumerical and implementation requirements:\n- All logarithms must be natural logarithms, and entropy and mutual information must be reported in nats.\n- For numerical stability, when computing $\\log p$ for any probability $p$, you should clip $p$ into the closed interval $[\\varepsilon, 1]$ for a small positive $\\varepsilon$ (choose a fixed $\\varepsilon$ appropriate for double-precision arithmetic).\n- The final output must be a single line containing a comma-separated list enclosed in square brackets, with each mutual information estimate rounded to $6$ decimal digits.\n\nTest suite:\nFor each test case below, treat each row as the class-probability vector produced by a single independently drawn posterior sample $w^{(s)}$. In all cases, $S$ denotes the number of posterior samples and $K$ the number of classes. Compute the MC mutual information estimate in nats for each case.\n\n- Case $1$ (uniform, identical predictions; expected mutual information close to $0$): $S = 5$, $K = 3$, rows\n  - $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$\n  - $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$\n  - $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$\n  - $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$\n  - $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$.\n- Case $2$ (deterministic disagreement across samples): $S = 2$, $K = 3$, rows\n  - $(1, 0, 0)$\n  - $(0, 1, 0)$.\n- Case $3$ (identical deterministic predictions; expected mutual information $0$): $S = 3$, $K = 4$, rows\n  - $(0, 0, 1, 0)$\n  - $(0, 0, 1, 0)$\n  - $(0, 0, 1, 0)$.\n- Case $4$ (moderate variation across samples): $S = 4$, $K = 3$, rows\n  - $(0.7, 0.2, 0.1)$\n  - $(0.6, 0.3, 0.1)$\n  - $(0.3, 0.4, 0.3)$\n  - $(0.2, 0.7, 0.1)$.\n- Case $5$ (extreme probabilities and one balanced sample; tests numerical stability): $S = 3$, $K = 2$, rows\n  - $(10^{-12}, 1 - 10^{-12})$\n  - $(1 - 10^{-12}, 10^{-12})$\n  - $(0.5, 0.5)$.\n- Case $6$ (degenerate single-class scenario; expected mutual information $0$): $S = 3$, $K = 1$, rows\n  - $(1)$\n  - $(1)$\n  - $(1)$.\n\nProgram input and output specification:\n- Your program must be self-contained and must not read any input; it should internally construct the above test cases.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each mutual information estimate rounded to $6$ decimal digits (for example, $[r_1, r_2, r_3]$).",
            "solution": "We begin with a classification Bayesian Neural Network (BNN) that outputs, for any fixed weight vector $w$ and input $x^\\star$, a valid discrete distribution over $K$ classes denoted $\\{q_k(w, x^\\star)\\}_{k=1}^K$, where $q_k(w, x^\\star) \\ge 0$ and $\\sum_{k=1}^K q_k(w, x^\\star) = 1$. The posterior over weights after observing dataset $D$ is $p(w \\mid D)$.\n\nThe posterior predictive distribution over the discrete label $y^\\star$ is given by the expectation of the softmax outputs under the posterior:\n$$\np(y^\\star = k \\mid x^\\star, D) = \\mathbb{E}_{w \\sim p(w \\mid D)} \\left[ q_k(w, x^\\star) \\right], \\quad k \\in \\{1, \\dots, K\\}.\n$$\nThis is the foundational law for Bayesian model averaging in a BNN.\n\nFor any discrete distribution $\\{p_k\\}_{k=1}^K$, Shannon entropy measured in natural units (nats) is defined as\n$$\nH(\\{p_k\\}) = -\\sum_{k=1}^K p_k \\log p_k,\n$$\nwhere $\\log$ denotes the natural logarithm. Conditional mutual information between random variables $A$ and $B$ given $C$ is defined by the chain rule identity\n$$\nI(A; B \\mid C) = H(A \\mid C) - H(A \\mid B, C).\n$$\n\nWe apply these definitions to $A = y^\\star$, $B = w$, and $C = (x^\\star, D)$. The first term, $H(y^\\star \\mid x^\\star, D)$, is the Shannon entropy of the posterior predictive distribution over $y^\\star$. The second term, $H(y^\\star \\mid w, x^\\star, D)$, is the conditional entropy of $y^\\star$ given a fixed weight $w$ and $x^\\star$, which depends only on the softmax output $\\{q_k(w, x^\\star)\\}_{k=1}^K$ because $D$ is incorporated through $p(w \\mid D)$. Therefore, by conditioning on $w$ as a random variable distributed according to $p(w \\mid D)$, we obtain\n$$\nI(y^\\star; w \\mid x^\\star, D) = H(y^\\star \\mid x^\\star, D) - \\mathbb{E}_{w \\sim p(w \\mid D)} \\big[ H(y^\\star \\mid w, x^\\star) \\big].\n$$\n\nTo construct a Monte Carlo (MC) estimator, draw $S$ independent samples $w^{(s)} \\sim p(w \\mid D)$ for $s \\in \\{1, \\dots, S\\}$. Each sample yields a class-probability vector\n$$\n\\mathbf{q}^{(s)}(x^\\star) = \\big( q_1(w^{(s)}, x^\\star), \\dots, q_K(w^{(s)}, x^\\star) \\big),\n$$\nwith $\\sum_{k=1}^K q_k(w^{(s)}, x^\\star) = 1$. The posterior predictive probabilities are approximated by the empirical average\n$$\n\\bar{p}_k(x^\\star, D) \\approx \\frac{1}{S} \\sum_{s=1}^S q_k(w^{(s)}, x^\\star).\n$$\nThe predictive entropy is then approximated by the plug-in estimator\n$$\n\\widehat{H}_{\\text{pred}} = -\\sum_{k=1}^K \\bar{p}_k(x^\\star, D) \\log \\bar{p}_k(x^\\star, D).\n$$\nThe conditional entropy given a single sampled weight $w^{(s)}$ is\n$$\nH^{(s)} = -\\sum_{k=1}^K q_k(w^{(s)}, x^\\star) \\log q_k(w^{(s)}, x^\\star),\n$$\nand its posterior expectation is approximated by the empirical mean\n$$\n\\widehat{\\mathbb{E}}[H] = \\frac{1}{S} \\sum_{s=1}^S H^{(s)}.\n$$\nTherefore, the MC estimator of mutual information in nats is\n$$\n\\widehat{I}(y^\\star; w \\mid x^\\star, D) = \\widehat{H}_{\\text{pred}} - \\widehat{\\mathbb{E}}[H].\n$$\n\nNumerical stability considerations: when evaluating $\\log p$ for probabilities $p$ that may be extremely close to $0$, we clip $p$ into $[\\varepsilon, 1]$ for a small $\\varepsilon > 0$, for example $\\varepsilon = 10^{-12}$. This prevents undefined logarithms at $p = 0$ and suppresses numerical overflow in $-\\log p$.\n\nProperties of the estimator:\n- The estimator $\\widehat{\\mathbb{E}}[H]$ is an unbiased estimator of $\\mathbb{E}_{w \\sim p(w \\mid D)}[H(y^\\star \\mid w, x^\\star)]$ because it is a sample mean of independent and identically distributed (i.i.d.) terms.\n- The plug-in estimator $\\widehat{H}_{\\text{pred}}$ is generally biased for finite $S$ due to the nonlinearity of entropy, but it is consistent as $S \\to \\infty$ because $\\bar{p}_k(x^\\star, D)$ converges almost surely to $\\mathbb{E}[q_k(w, x^\\star)]$.\n- The mutual information estimator $\\widehat{I}$ is consistent as $S \\to \\infty$.\n\nAlgorithmic steps for each test case:\n1. Let $P$ be the $S \\times K$ matrix whose $s$-th row is $\\mathbf{q}^{(s)}(x^\\star)$.\n2. Clip entries of $P$ into $[\\varepsilon, 1]$ and renormalize rows if necessary to maintain row sums at $1$ (for the provided test cases, rows already sum to $1$).\n3. Compute $\\bar{\\mathbf{p}} = \\frac{1}{S} \\sum_{s=1}^S \\mathbf{q}^{(s)}(x^\\star)$.\n4. Compute $\\widehat{H}_{\\text{pred}} = -\\sum_{k=1}^K \\bar{p}_k \\log \\bar{p}_k$.\n5. For each $s$, compute $H^{(s)} = -\\sum_{k=1}^K q^{(s)}_k \\log q^{(s)}_k$ and then $\\widehat{\\mathbb{E}}[H] = \\frac{1}{S} \\sum_{s=1}^S H^{(s)}$.\n6. Output $\\widehat{I} = \\widehat{H}_{\\text{pred}} - \\widehat{\\mathbb{E}}[H]$ in nats.\n\nThe program implements this estimator for the specified test suite and prints a single line containing the results as a comma-separated list enclosed in square brackets, each rounded to $6$ decimal digits, as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef mutual_information_mc(prob_matrix: np.ndarray, eps: float = 1e-12) -> float:\n    \"\"\"\n    Estimate I(y*, w | x*, D) for a classification BNN using Monte Carlo samples.\n    prob_matrix: shape (S, K), rows are class-probability vectors from posterior samples.\n    eps: small positive constant for numerical stability when taking logs.\n    Returns MI in nats as a float.\n    \"\"\"\n    # Ensure input is float64 for numerical stability\n    P = np.array(prob_matrix, dtype=np.float64)\n\n    # Clip probabilities to avoid log(0); rows should already sum to 1 in test cases.\n    P = np.clip(P, eps, 1.0)\n    # Renormalize rows in case clipping perturbed sums slightly\n    row_sums = P.sum(axis=1, keepdims=True)\n    # Avoid division by zero, though with eps clipping this shouldn't happen\n    row_sums = np.maximum(row_sums, eps)\n    P = P / row_sums\n\n    # Number of samples S and classes K\n    S, K = P.shape\n\n    # Predictive probabilities: average over samples\n    p_bar = P.mean(axis=0)\n\n    # Entropy of predictive distribution (in nats)\n    H_pred = -np.sum(p_bar * np.log(p_bar))\n\n    # Conditional entropy for each sample\n    H_cond_samples = -np.sum(P * np.log(P), axis=1)\n    H_cond_avg = H_cond_samples.mean()\n\n    # Mutual information estimate\n    MI = H_pred - H_cond_avg\n    return float(MI)\n\n\ndef solve():\n    # Define epsilon for numerical stability\n    eps = 1e-12\n\n    # Construct test cases as per the problem statement.\n    # Case 1: Uniform, identical predictions (S=5, K=3)\n    case1 = np.array([[1/3, 1/3, 1/3]] * 5, dtype=np.float64)\n\n    # Case 2: Deterministic disagreement across samples (S=2, K=3)\n    case2 = np.array([[1.0, 0.0, 0.0],\n                      [0.0, 1.0, 0.0]], dtype=np.float64)\n\n    # Case 3: Identical deterministic predictions (S=3, K=4)\n    case3 = np.array([[0.0, 0.0, 1.0, 0.0]] * 3, dtype=np.float64)\n\n    # Case 4: Moderate variation across samples (S=4, K=3)\n    case4 = np.array([[0.7, 0.2, 0.1],\n                      [0.6, 0.3, 0.1],\n                      [0.3, 0.4, 0.3],\n                      [0.2, 0.7, 0.1]], dtype=np.float64)\n\n    # Case 5: Extreme probabilities and one balanced sample (S=3, K=2)\n    tiny = 1e-12\n    case5 = np.array([[tiny, 1.0 - tiny],\n                      [1.0 - tiny, tiny],\n                      [0.5, 0.5]], dtype=np.float64)\n\n    # Case 6: Degenerate single-class scenario (S=3, K=1)\n    case6 = np.array([[1.0], [1.0], [1.0]], dtype=np.float64)\n\n    test_cases = [case1, case2, case3, case4, case5, case6]\n\n    results = []\n    for P in test_cases:\n        result = mutual_information_mc(P, eps=eps)\n        results.append(result)\n\n    # Final print statement in the exact required format with 6 decimal digits.\n    print(\"[\" + \",\".join(f\"{r:.6f}\" for r in results) + \"]\")\n\nsolve()\n```"
        }
    ]
}