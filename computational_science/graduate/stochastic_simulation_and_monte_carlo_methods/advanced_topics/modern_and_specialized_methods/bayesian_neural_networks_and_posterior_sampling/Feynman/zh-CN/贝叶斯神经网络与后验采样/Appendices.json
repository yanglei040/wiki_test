{
    "hands_on_practices": [
        {
            "introduction": "贝叶斯神经网络（BNN）的核心挑战在于如何从其高维复杂的权重后验分布中进行采样。随机梯度朗之万动力学（SGLD）是一种强大且可扩展的蒙特卡洛方法，它巧妙地将随机梯度下降的效率与朗之万动力学的物理直觉相结合，通过注入高斯噪声来探索整个后验景观。这项练习  将指导你从第一性原理出发，推导 SGLD 的核心更新方程，这是在实践中训练 BNN 的基石。",
            "id": "3291187",
            "problem": "考虑一个贝叶斯神经网络，其权重为 $\\mathbf{w} \\in \\mathbb{R}^{d}$，先验分布为 $p(\\mathbf{w})=\\mathcal{N}(0,\\sigma_{p}^{2} I_{d})$，数据为 $\\mathcal{D}=\\{(x_{i},y_{i})\\}_{i=1}^{N}$。该模型假设在网络输出 $f_{\\mathbf{w}}(x)$ 周围存在加性高斯观测噪声，其已知方差为 $\\sigma_{y}^{2}$，因此对于 $i=1,\\dots,N$，$p(y_{i}\\mid x_{i},\\mathbf{w})=\\mathcal{N}(f_{\\mathbf{w}}(x_{i}),\\sigma_{y}^{2})$ 独立成立。后验分布满足 $p(\\mathbf{w}\\mid \\mathcal{D}) \\propto p(\\mathbf{w})\\prod_{i=1}^{N}p(y_{i}\\mid x_{i},\\mathbf{w})$。设 $B_{t}\\subset\\{1,\\dots,N\\}$ 是在迭代 $t$ 时无放回均匀采样的大小为 $m=|B_{t}|$ 的小批量（mini-batch），并设 $\\xi_{t}\\sim\\mathcal{N}(0,I_{d})$ 在 $t$ 上独立。从目标为后验分布的过阻尼朗之万扩散及其欧拉-丸山（Euler–Maruyama）离散化出发，使用基于 $B_{t}$ 的后验分数（score）的无偏小批量估计量，推导出随机梯度朗之万动力学（stochastic gradient Langevin dynamics）的更新规则。将您在迭代 $t$ 时的最终更新规则明确地用 $\\mathbf{w}_{t}$、$\\sigma_{p}^{2}$、$\\sigma_{y}^{2}$、$N$、$m$、$f_{\\mathbf{w}_{t}}(x)$ 和 $\\nabla_{\\mathbf{w}} f_{\\mathbf{w}_{t}}(x)$ 表示。\n\n然后，陈述在序列 $\\{\\eta_{t}\\}_{t\\geq 1}$ 上所需的步长调度条件，在此条件下，当 $t\\to\\infty$ 时，所产生的马尔可夫链能渐近地从精确后验分布中采样（假设确保稳定性的标准正则性条件成立，包括可测性、分数的 Lipschitz 连续性以及小批量梯度噪声的二阶矩有界）。你的最终答案必须只包含推导出的更新规则和步长条件。不要包含任何解释性文本。如果提供多个项目，请将它们表示为单个行向量。答案不需要四舍五入，也没有物理单位。",
            "solution": "问题要求推导贝叶斯神经网络的随机梯度朗之万动力学（SGLD）更新规则，并陈述其收敛到后验分布的步长条件。\n\n首先，我们形式化目标分布。给定数据 $\\mathcal{D}=\\{(x_{i},y_{i})\\}_{i=1}^{N}$，权重 $\\mathbf{w} \\in \\mathbb{R}^{d}$ 的后验分布由贝叶斯定理给出：\n$$\np(\\mathbf{w} \\mid \\mathcal{D}) \\propto p(\\mathcal{D} \\mid \\mathbf{w}) p(\\mathbf{w})\n$$\n鉴于数据点的独立性，似然函数为 $p(\\mathcal{D} \\mid \\mathbf{w}) = \\prod_{i=1}^{N} p(y_{i} \\mid x_{i}, \\mathbf{w})$。因此，后验分布为：\n$$\np(\\mathbf{w} \\mid \\mathcal{D}) \\propto p(\\mathbf{w}) \\prod_{i=1}^{N} p(y_{i} \\mid x_{i}, \\mathbf{w})\n$$\n处理对数后验分布更为方便，其表达式为：\n$$\n\\ln p(\\mathbf{w} \\mid \\mathcal{D}) = \\ln p(\\mathbf{w}) + \\sum_{i=1}^{N} \\ln p(y_{i} \\mid x_{i}, \\mathbf{w}) + C\n$$\n其中 $C$ 是一个与 $\\mathbf{w}$ 无关的归一化常数。\n\n问题指定了高斯先验 $p(\\mathbf{w}) = \\mathcal{N}(0, \\sigma_{p}^{2} I_{d})$ 和高斯似然 $p(y_{i} \\mid x_{i}, \\mathbf{w}) = \\mathcal{N}(f_{\\mathbf{w}}(x_{i}), \\sigma_{y}^{2})$。忽略常数项，相应的对数密度为：\n$$\n\\ln p(\\mathbf{w}) = -\\frac{1}{2\\sigma_{p}^{2}} \\mathbf{w}^{T}\\mathbf{w} + C_{p} = -\\frac{1}{2\\sigma_{p}^{2}} \\|\\mathbf{w}\\|_{2}^{2} + C_{p}\n$$\n$$\n\\ln p(y_{i} \\mid x_{i}, \\mathbf{w}) = -\\frac{1}{2\\sigma_{y}^{2}} (y_{i} - f_{\\mathbf{w}}(x_{i}))^{2} + C_{y}\n$$\n将这些代入对数后验表达式，我们得到：\n$$\n\\ln p(\\mathbf{w} \\mid \\mathcal{D}) = -\\frac{1}{2\\sigma_{p}^{2}} \\|\\mathbf{w}\\|_{2}^{2} - \\frac{1}{2\\sigma_{y}^{2}} \\sum_{i=1}^{N} (y_{i} - f_{\\mathbf{w}}(x_{i}))^{2} + C'\n$$\nSGLD 算法是一个过阻尼朗之万扩散过程的离散化。以 $p(\\mathbf{w} \\mid \\mathcal{D})$ 为其不变分布的连续时间朗之万扩散由以下随机微分方程（SDE）描述：\n$$\nd\\mathbf{w}_{t} = \\frac{1}{2} \\nabla_{\\mathbf{w}} \\ln p(\\mathbf{w}_{t} \\mid \\mathcal{D}) dt + d\\mathcal{W}_{t}\n$$\n其中 $\\mathcal{W}_{t}$ 是一个标准的 $d$ 维维纳过程。\n使用步长 $\\eta_{t}$ 对此 SDE 进行欧拉-丸山（Euler-Maruyama）离散化，得到朗之万动力学（LD）更新规则：\n$$\n\\mathbf{w}_{t+1} = \\mathbf{w}_{t} + \\frac{\\eta_{t}}{2} \\nabla_{\\mathbf{w}} \\ln p(\\mathbf{w}_{t} \\mid \\mathcal{D}) + \\sqrt{\\eta_{t}} \\xi_{t}\n$$\n其中 $\\xi_{t} \\sim \\mathcal{N}(0, I_{d})$ 是一个独立标准高斯噪声向量。\n\n下一步是计算对数后验的梯度，也称为分数（score）。\n$$\n\\nabla_{\\mathbf{w}} \\ln p(\\mathbf{w} \\mid \\mathcal{D}) = \\nabla_{\\mathbf{w}} \\left( -\\frac{1}{2\\sigma_{p}^{2}} \\mathbf{w}^{T}\\mathbf{w} - \\frac{1}{2\\sigma_{y}^{2}} \\sum_{i=1}^{N} (y_{i} - f_{\\mathbf{w}}(x_{i}))^{2} \\right)\n$$\n使用链式法则，我们得到：\n$$\n\\nabla_{\\mathbf{w}} \\ln p(\\mathbf{w} \\mid \\mathcal{D}) = -\\frac{1}{\\sigma_{p}^{2}} \\mathbf{w} - \\frac{1}{2\\sigma_{y}^{2}} \\sum_{i=1}^{N} 2(y_{i} - f_{\\mathbf{w}}(x_{i}))(-\\nabla_{\\mathbf{w}} f_{\\mathbf{w}}(x_{i}))\n$$\n$$\n\\nabla_{\\mathbf{w}} \\ln p(\\mathbf{w} \\mid \\mathcal{D}) = -\\frac{1}{\\sigma_{p}^{2}} \\mathbf{w} + \\frac{1}{\\sigma_{y}^{2}} \\sum_{i=1}^{N} (y_{i} - f_{\\mathbf{w}}(x_{i})) \\nabla_{\\mathbf{w}} f_{\\mathbf{w}}(x_{i})\n$$\n对于 $N$ 很大的大型数据集，在每次迭代中计算这个完整梯度在计算上是不可行的。SGLD 使用一小批数据来近似这个梯度。设 $B_{t} \\subset \\{1, \\dots, N\\}$ 是一个大小为 $m$ 的随机均匀采样的小批量。完整梯度的一个无偏估计量可以通过对小批量上的和进行缩放来构造：\n$$\n\\widehat{\\nabla_{\\mathbf{w}} \\ln p(\\mathbf{w} \\mid \\mathcal{D})} = -\\frac{1}{\\sigma_{p}^{2}} \\mathbf{w} + \\frac{N}{m} \\sum_{i \\in B_{t}} \\frac{1}{\\sigma_{y}^{2}}(y_{i} - f_{\\mathbf{w}}(x_{i})) \\nabla_{\\mathbf{w}} f_{\\mathbf{w}}(x_{i})\n$$\n将此随机梯度估计量代入 LD 更新方程，得到 SGLD 更新规则：\n$$\n\\mathbf{w}_{t+1} = \\mathbf{w}_{t} + \\frac{\\eta_{t}}{2} \\left[ -\\frac{1}{\\sigma_{p}^{2}} \\mathbf{w}_{t} + \\frac{N}{m\\sigma_{y}^{2}} \\sum_{i \\in B_{t}} (y_{i} - f_{\\mathbf{w}_{t}}(x_{i})) \\nabla_{\\mathbf{w}} f_{\\mathbf{w}_{t}}(x_{i}) \\right] + \\sqrt{\\eta_{t}}\\xi_{t}\n$$\n这个表达式可以重写以清晰地显示各个组成部分：\n$$\n\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\frac{\\eta_{t}}{2\\sigma_{p}^{2}} \\mathbf{w}_{t} + \\frac{\\eta_{t}N}{2m\\sigma_{y}^{2}} \\sum_{i \\in B_{t}} (y_{i} - f_{\\mathbf{w}_{t}}(x_{i})) \\nabla_{\\mathbf{w}} f_{\\mathbf{w}_{t}}(x_{i}) + \\sqrt{\\eta_{t}}\\xi_{t}\n$$\n\n为使 SGLD 迭代在分布上收敛到真实的后验分布 $p(\\mathbf{w} \\mid \\mathcal{D})$，步长调度 $\\{\\eta_{t}\\}_{t\\geq 1}$ 必须满足特定条件。这些条件要求在探索状态空间（由噪声项和梯度驱动）与减少离散化误差和随机梯度估计带来的方差之间取得平衡。标准条件，类似于随机近似的条件（Robbins-Monro 条件），是：\n1. 步长不能衰减得太快，以允许过程探索整个状态空间。这通过步长之和必须发散的条件来保证：\n$$\n\\sum_{t=1}^{\\infty} \\eta_{t} = \\infty\n$$\n2. 步长必须衰减得足够快，以确保注入噪声的方差随时间减小，并且欧拉-丸山方案的离散化误差在极限情况下可以忽略不计。这通过步长的平方和必须收敛的条件来保证：\n$$\n\\sum_{t=1}^{\\infty} \\eta_{t}^{2}  \\infty\n$$\n在这些条件下，并假设目标分布和网络函数具有标准的正则性，由 SGLD 算法生成的 $\\mathbf{w}_{t}$ 的分布在 $t \\to \\infty$ 时弱收敛于后验分布 $p(\\mathbf{w} \\mid \\mathcal{D})$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\mathbf{w}_{t+1} = \\mathbf{w}_{t} + \\frac{\\eta_{t}}{2} \\left[ -\\frac{1}{\\sigma_{p}^{2}} \\mathbf{w}_{t} + \\frac{N}{m\\sigma_{y}^{2}} \\sum_{i \\in B_{t}} (y_{i} - f_{\\mathbf{w}_{t}}(x_{i})) \\nabla_{\\mathbf{w}} f_{\\mathbf{w}_{t}}(x_{i}) \\right] + \\sqrt{\\eta_{t}}\\xi_{t}  \\sum_{t=1}^{\\infty} \\eta_{t} = \\infty  \\sum_{t=1}^{\\infty} \\eta_{t}^{2}  \\infty \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "BNN 的真正威力在于其超越单一预测值的能力，能够捕捉和量化数据中复杂的不确定性。一个重要的例子是异方差回归，其中模型不仅预测输出的均值 $\\mu_\\mathbf{w}(x)$，还预测随输入变化的方差 $\\sigma_\\mathbf{w}^2(x)$。这项练习  将带你深入模型构建的细节，推导这类高级模型的对数似然梯度，这是使用 SGLD 等基于梯度的方法进行后验推断的关键一步。",
            "id": "3291222",
            "problem": "考虑一个用于一维异方差回归的贝叶斯神经网络（BNN）。对于每个输入 $x_i$，权重为 $\\mathbf{w}$ 的网络输出两个标量函数 $(\\mu_{\\mathbf{w}}(x_i), s_{\\mathbf{w}}(x_i))$，其中 $s_{\\mathbf{w}}(x_i) = \\log \\sigma_{\\mathbf{w}}^2(x_i)$ 通过 $\\sigma_{\\mathbf{w}}^2(x_i) = \\exp(s_{\\mathbf{w}}(x_i))$ 编码预测方差。假设单个观测值 $(x_i, y_i)$ 的数据似然是高斯密度 $p(y_i \\mid x_i, \\mathbf{w}) = \\mathcal{N}(y_i; \\mu_{\\mathbf{w}}(x_i), \\sigma_{\\mathbf{w}}^2(x_i))$。从高斯概率密度函数的定义出发，使用标准的微分法则（包括链式法则），推导单位样本对数似然 $\\ell_i(\\mathbf{w}) = \\ln p(y_i \\mid x_i, \\mathbf{w})$ 及其关于 $\\mathbf{w}$ 的梯度，并用 $y_i$、$\\mu_{\\mathbf{w}}(x_i)$、$s_{\\mathbf{w}}(x_i)$ 以及权重空间雅可比矩阵 $\\nabla_{\\mathbf{w}} \\mu_{\\mathbf{w}}(x_i)$ 和 $\\nabla_{\\mathbf{w}} s_{\\mathbf{w}}(x_i)$ 显式表示。您的推导应仅假设高斯密度的性质以及网络输出关于 $\\mathbf{w}$ 的可微性。将您的最终答案表示为一个包含标量 $\\ell_i(\\mathbf{w})$ 和向量 $\\nabla_{\\mathbf{w}} \\ell_i(\\mathbf{w})$ 的数对。最终答案必须是单一的闭式解析表达式。不要包含任何单位。无需四舍五入。",
            "solution": "问题陈述已经过验证，被认为是合理的。这是一个在随机模拟和机器学习领域中定义明确的问题，具体涉及贝叶斯神经网络对数似然梯度的推导，这是通过变分推断或基于梯度的MCMC方法训练此类模型的一项基本计算。所有必要的组成部分均已提供。\n\n我们首先陈述问题中指定的高斯似然的概率密度函数（PDF）。对于单个数据点 $(x_i, y_i)$，似然由下式给出：\n$$\np(y_i \\mid x_i, \\mathbf{w}) = \\mathcal{N}(y_i; \\mu_{\\mathbf{w}}(x_i), \\sigma_{\\mathbf{w}}^2(x_i))\n$$\n对于均值为 $\\mu$、方差为 $\\sigma^2$ 的变量 $y$，高斯 PDF 的显式形式为：\n$$\n\\mathcal{N}(y; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right)\n$$\n在我们的例子中，均值为 $\\mu_{\\mathbf{w}}(x_i)$，方差为 $\\sigma_{\\mathbf{w}}^2(x_i)$。问题定义了方差通过网络输出 $s_{\\mathbf{w}}(x_i)$ 的特定参数化：\n$$\n\\sigma_{\\mathbf{w}}^2(x_i) = \\exp(s_{\\mathbf{w}}(x_i))\n$$\n将此代入 PDF，我们得到特定模型的似然：\n$$\np(y_i \\mid x_i, \\mathbf{w}) = \\frac{1}{\\sqrt{2\\pi\\exp(s_{\\mathbf{w}}(x_i))}} \\exp\\left(-\\frac{(y_i - \\mu_{\\mathbf{w}}(x_i))^2}{2\\exp(s_{\\mathbf{w}}(x_i))}\\right)\n$$\n任务的第一部分是推导单位样本对数似然 $\\ell_i(\\mathbf{w}) = \\ln p(y_i \\mid x_i, \\mathbf{w})$。我们对上述表达式取自然对数，利用属性 $\\ln(ab) = \\ln(a) + \\ln(b)$、$\\ln(a^c) = c \\ln(a)$ 和 $\\ln(\\exp(c)) = c$。\n$$\n\\ell_i(\\mathbf{w}) = \\ln\\left( \\left(2\\pi\\exp(s_{\\mathbf{w}}(x_i))\\right)^{-1/2} \\right) + \\ln\\left( \\exp\\left(-\\frac{(y_i - \\mu_{\\mathbf{w}}(x_i))^2}{2\\exp(s_{\\mathbf{w}}(x_i))}\\right) \\right)\n$$\n$$\n\\ell_i(\\mathbf{w}) = -\\frac{1}{2} \\ln(2\\pi\\exp(s_{\\mathbf{w}}(x_i))) - \\frac{(y_i - \\mu_{\\mathbf{w}}(x_i))^2}{2\\exp(s_{\\mathbf{w}}(x_i))}\n$$\n$$\n\\ell_i(\\mathbf{w}) = -\\frac{1}{2} \\left( \\ln(2\\pi) + \\ln(\\exp(s_{\\mathbf{w}}(x_i))) \\right) - \\frac{1}{2}(y_i - \\mu_{\\mathbf{w}}(x_i))^2 \\exp(-s_{\\mathbf{w}}(x_i))\n$$\n$$\n\\ell_i(\\mathbf{w}) = -\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}s_{\\mathbf{w}}(x_i) - \\frac{1}{2}(y_i - \\mu_{\\mathbf{w}}(x_i))^2 \\exp(-s_{\\mathbf{w}}(x_i))\n$$\n这就是单位样本对数似然的表达式，是我们答案的第一部分。\n\n任务的第二部分是推导对数似然关于网络权重 $\\mathbf{w}$ 的梯度 $\\nabla_{\\mathbf{w}} \\ell_i(\\mathbf{w})$。我们必须对 $\\ell_i(\\mathbf{w})$ 的表达式关于向量 $\\mathbf{w}$ 进行微分。量 $\\mu_{\\mathbf{w}}(x_i)$ 和 $s_{\\mathbf{w}}(x_i)$ 是 $\\mathbf{w}$ 的函数。我们将应用多元链式法则。\n$$\n\\nabla_{\\mathbf{w}} \\ell_i(\\mathbf{w}) = \\nabla_{\\mathbf{w}} \\left( -\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}s_{\\mathbf{w}}(x_i) - \\frac{1}{2}(y_i - \\mu_{\\mathbf{w}}(x_i))^2 \\exp(-s_{\\mathbf{w}}(x_i)) \\right)\n$$\n第一项 $-\\frac{1}{2}\\ln(2\\pi)$ 是关于 $\\mathbf{w}$ 的常数，因此其梯度为零。\n$$\n\\nabla_{\\mathbf{w}} \\ell_i(\\mathbf{w}) = -\\frac{1}{2}\\nabla_{\\mathbf{w}}(s_{\\mathbf{w}}(x_i)) - \\frac{1}{2}\\nabla_{\\mathbf{w}} \\left( (y_i - \\mu_{\\mathbf{w}}(x_i))^2 \\exp(-s_{\\mathbf{w}}(x_i)) \\right)\n$$\n对于第二项，我们使用梯度乘法法则 $\\nabla(uv) = (\\nabla u)v + u(\\nabla v)$。令 $u = (y_i - \\mu_{\\mathbf{w}}(x_i))^2$ 和 $v = \\exp(-s_{\\mathbf{w}}(x_i))$。我们使用链式法则求它们关于 $\\mathbf{w}$ 的梯度。\n\n$u$ 关于 $\\mathbf{w}$ 的梯度是：\n$$\n\\nabla_{\\mathbf{w}} u = \\frac{\\partial u}{\\partial \\mu_{\\mathbf{w}}(x_i)} \\nabla_{\\mathbf{w}} \\mu_{\\mathbf{w}}(x_i) = 2(y_i - \\mu_{\\mathbf{w}}(x_i)) \\cdot (-1) \\cdot \\nabla_{\\mathbf{w}} \\mu_{\\mathbf{w}}(x_i) = -2(y_i - \\mu_{\\mathbf{w}}(x_i)) \\nabla_{\\mathbf{w}} \\mu_{\\mathbf{w}}(x_i)\n$$\n$v$ 关于 $\\mathbf{w}$ 的梯度是：\n$$\n\\nabla_{\\mathbf{w}} v = \\frac{\\partial v}{\\partial s_{\\mathbf{w}}(x_i)} \\nabla_{\\mathbf{w}} s_{\\mathbf{w}}(x_i) = \\exp(-s_{\\mathbf{w}}(x_i)) \\cdot (-1) \\cdot \\nabla_{\\mathbf{w}} s_{\\mathbf{w}}(x_i) = -\\exp(-s_{\\mathbf{w}}(x_i)) \\nabla_{\\mathbf{w}} s_{\\mathbf{w}}(x_i)\n$$\n现在，应用乘法法则：\n$$\n\\nabla_{\\mathbf{w}} (uv) = \\left( -2(y_i - \\mu_{\\mathbf{w}}(x_i)) \\nabla_{\\mathbf{w}} \\mu_{\\mathbf{w}}(x_i) \\right) \\exp(-s_{\\mathbf{w}}(x_i)) + (y_i - \\mu_{\\mathbf{w}}(x_i))^2 \\left( -\\exp(-s_{\\mathbf{w}}(x_i)) \\nabla_{\\mathbf{w}} s_{\\mathbf{w}}(x_i) \\right)\n$$\n$$\n\\nabla_{\\mathbf{w}} (uv) = -2(y_i - \\mu_{\\mathbf{w}}(x_i)) \\exp(-s_{\\mathbf{w}}(x_i)) \\nabla_{\\mathbf{w}} \\mu_{\\mathbf{w}}(x_i) - (y_i - \\mu_{\\mathbf{w}}(x_i))^2 \\exp(-s_{\\mathbf{w}}(x_i)) \\nabla_{\\mathbf{w}} s_{\\mathbf{w}}(x_i)\n$$\n将此代回 $\\nabla_{\\mathbf{w}} \\ell_i(\\mathbf{w})$ 的表达式中：\n$$\n\\nabla_{\\mathbf{w}} \\ell_i(\\mathbf{w}) = -\\frac{1}{2}\\nabla_{\\mathbf{w}} s_{\\mathbf{w}}(x_i) - \\frac{1}{2} \\left[ -2(y_i - \\mu_{\\mathbf{w}}(x_i)) \\exp(-s_{\\mathbf{w}}(x_i)) \\nabla_{\\mathbf{w}} \\mu_{\\mathbf{w}}(x_i) - (y_i - \\mu_{\\mathbf{w}}(x_i))^2 \\exp(-s_{\\mathbf{w}}(x_i)) \\nabla_{\\mathbf{w}} s_{\\mathbf{w}}(x_i) \\right]\n$$\n分配 $-\\frac{1}{2}$ 项：\n$$\n\\nabla_{\\mathbf{w}} \\ell_i(\\mathbf{w}) = -\\frac{1}{2}\\nabla_{\\mathbf{w}} s_{\\mathbf{w}}(x_i) + (y_i - \\mu_{\\mathbf{w}}(x_i)) \\exp(-s_{\\mathbf{w}}(x_i)) \\nabla_{\\mathbf{w}} \\mu_{\\mathbf{w}}(x_i) + \\frac{1}{2}(y_i - \\mu_{\\mathbf{w}}(x_i))^2 \\exp(-s_{\\mathbf{w}}(x_i)) \\nabla_{\\mathbf{w}} s_{\\mathbf{w}}(x_i)\n$$\n最后，我们按雅可比矩阵 $\\nabla_{\\mathbf{w}} \\mu_{\\mathbf{w}}(x_i)$ 和 $\\nabla_{\\mathbf{w}} s_{\\mathbf{w}}(x_i)$ 对各项进行分组：\n$$\n\\nabla_{\\mathbf{w}} \\ell_i(\\mathbf{w}) = \\left( (y_i - \\mu_{\\mathbf{w}}(x_i)) \\exp(-s_{\\mathbf{w}}(x_i)) \\right) \\nabla_{\\mathbf{w}} \\mu_{\\mathbf{w}}(x_i) + \\left( \\frac{1}{2}(y_i - \\mu_{\\mathbf{w}}(x_i))^2 \\exp(-s_{\\mathbf{w}}(x_i)) - \\frac{1}{2} \\right) \\nabla_{\\mathbf{w}} s_{\\mathbf{w}}(x_i)\n$$\n为了清晰起见，可以稍微分解一下：\n$$\n\\nabla_{\\mathbf{w}} \\ell_i(\\mathbf{w}) = (y_i - \\mu_{\\mathbf{w}}(x_i))\\exp(-s_{\\mathbf{w}}(x_i)) \\nabla_{\\mathbf{w}} \\mu_{\\mathbf{w}}(x_i) + \\frac{1}{2} \\left[ (y_i - \\mu_{\\mathbf{w}}(x_i))^2 \\exp(-s_{\\mathbf{w}}(x_i)) - 1 \\right] \\nabla_{\\mathbf{w}} s_{\\mathbf{w}}(x_i)\n$$\n这是单位样本对数似然梯度的最终表达式。要求的答案是由 $\\ell_i(\\mathbf{w})$ 和 $\\nabla_{\\mathbf{w}} \\ell_i(\\mathbf{w})$ 组成的数对。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{1}{2}s_{\\mathbf{w}}(x_i) - \\frac{1}{2}(y_i - \\mu_{\\mathbf{w}}(x_i))^2 \\exp(-s_{\\mathbf{w}}(x_i)) - \\frac{1}{2}\\ln(2\\pi)  (y_i - \\mu_{\\mathbf{w}}(x_i))\\exp(-s_{\\mathbf{w}}(x_i)) \\nabla_{\\mathbf{w}} \\mu_{\\mathbf{w}}(x_i) + \\frac{1}{2} \\left[ (y_i - \\mu_{\\mathbf{w}}(x_i))^2 \\exp(-s_{\\mathbf{w}}(x_i)) - 1 \\right] \\nabla_{\\mathbf{w}} s_{\\mathbf{w}}(x_i) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "获得后验样本本身并非最终目的；真正的价值在于利用这些样本来量化和理解模型的不确定性。认知不确定性（epistemic uncertainty），即模型由于数据不足而产生的不确定性，是 BNN 的一个关键输出。这项练习  提供了一个从理论到实践的完整指南，教你如何利用后验样本，通过计算互信息来具体地量化这种不确定性，这在主动学习等前沿领域至关重要。",
            "id": "3291210",
            "problem": "考虑一个分类贝叶斯神经网络 (BNN)，它被定义为一个关于权重 $\\mathbf{w}$ 的概率模型，在观测到数据集 $D$ 后的后验分布为 $p(\\mathbf{w} \\mid D)$。对于一个输入 $x^\\star$，BNN 通过 softmax 输出 $q_k(\\mathbf{w}, x^\\star)$ 为类别 $k \\in \\{1, \\dots, K\\}$ 生成类别概率，这些概率对每个 $\\mathbf{w}$ 都构成一个有效的离散分布。在给定 $x^\\star$ 和 $D$ 的条件下，关于标签 $y^\\star$ 的预测分布是通过对这些类别概率关于 $\\mathbf{w}$ 的后验分布求平均来确定的。在 $x^\\star$ 和 $D$ 的条件下，$y^\\star$ 和 $\\mathbf{w}$ 之间的互信息量化了认知不确定性，并且是贝叶斯分歧主动学习 (BALD) 中的一个核心量。你的任务是，从概率论和信息论的第一性原理出发，使用从 $p(\\mathbf{w} \\mid D)$ 中抽取的独立样本 $\\mathbf{w}^{(s)}$，为该互信息推导出一个可实现的蒙特卡洛 (MC) 估计器。\n\n使用以下基础定义：\n- 离散标签的后验预测分布的定义：在给定 $x^\\star$ 和 $D$ 的条件下 $y^\\star = k$ 的概率是在 $p(\\mathbf{w} \\mid D)$ 分布下 $q_k(\\mathbf{w}, x^\\star)$ 的期望。\n- 离散分布的香农熵，以自然单位 (nats) 计量：对于概率 $\\{p_k\\}_{k=1}^K$，$H = -\\sum_{k=1}^K p_k \\log p_k$，其中 $\\log$ 表示自然对数。\n- 条件互信息的链式法则定义：对于随机变量 $A$、$B$ 和 $C$，$I(A; B \\mid C) = H(A \\mid C) - H(A \\mid B, C)$。\n\n从这些定义出发，推导一个估计器，其中关于 $p(\\mathbf{w} \\mid D)$ 的期望被替换为对 $S$ 个独立后验样本 $\\mathbf{w}^{(s)}$ 的经验平均值，每个样本都产生一个类别概率向量 $\\big(q_1(\\mathbf{w}^{(s)}, x^\\star), \\dots, q_K(\\mathbf{w}^{(s)}, x^\\star)\\big)$。然后，实现一个程序，为一组给定的后验类别概率样本计算此互信息的 MC 估计器（以 nats 为单位）。\n\n数值和实现要求：\n- 所有对数必须是自然对数，熵和互信息必须以 nats 为单位报告。\n- 为了数值稳定性，在计算任何概率 $p$ 的 $\\log p$ 时，你应该将 $p$ 裁剪到闭区间 $[\\varepsilon, 1]$ 内，其中 $\\varepsilon$ 是一个小的正数（选择一个适合双精度算术的固定 $\\varepsilon$）。\n- 最终输出必须是单行，包含一个用方括号括起来的逗号分隔列表，其中每个互信息估计值四舍五入到 $6$ 位小数。\n\n测试套件：\n对于下面的每个测试用例，将每一行视为由单个独立抽取的后验样本 $\\mathbf{w}^{(s)}$ 产生的类别概率向量。在所有情况下，$S$ 表示后验样本的数量，$K$ 表示类别的数量。计算每种情况下的 MC 互信息估计值（以 nats 为单位）。\n\n- 用例 1（均匀、相同的预测；预期互信息接近 0）：$S = 5$，$K = 3$，行\n  - $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$\n  - $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$\n  - $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$\n  - $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$\n  - $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$。\n- 用例 2（样本间的确定性分歧）：$S = 2$，$K = 3$，行\n  - $(1, 0, 0)$\n  - $(0, 1, 0)$。\n- 用例 3（相同的确定性预测；预期互信息为 0）：$S = 3$，$K = 4$，行\n  - $(0, 0, 1, 0)$\n  - $(0, 0, 1, 0)$\n  - $(0, 0, 1, 0)$。\n- 用例 4（样本间的适度变化）：$S = 4$，$K = 3$，行\n  - $(0.7, 0.2, 0.1)$\n  - $(0.6, 0.3, 0.1)$\n  - $(0.3, 0.4, 0.3)$\n  - $(0.2, 0.7, 0.1)$。\n- 用例 5（极端概率和一个平衡样本；测试数值稳定性）：$S = 3$，$K = 2$，行\n  - $(10^{-12}, 1 - 10^{-12})$\n  - $(1 - 10^{-12}, 10^{-12})$\n  - $(0.5, 0.5)$。\n- 用例 6（退化的单类场景；预期互信息为 0）：$S = 3$，$K = 1$，行\n  - $(1)$\n  - $(1)$\n  - $(1)$。\n\n程序输入和输出规范：\n- 你的程序必须是自包含的，并且不得读取任何输入；它应该在内部构建上述测试用例。\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个互信息估计值四舍五入到 $6$ 位小数（例如，$[r_1, r_2, r_3]$）。",
            "solution": "我们从一个分类贝叶斯神经网络 (BNN) 开始，对于任何固定的权重向量 $\\mathbf{w}$ 和输入 $x^\\star$，它会输出一个关于 $K$ 个类别的有效离散分布，记为 $\\{q_k(\\mathbf{w}, x^\\star)\\}_{k=1}^K$，其中 $q_k(\\mathbf{w}, x^\\star) \\ge 0$ 且 $\\sum_{k=1}^K q_k(\\mathbf{w}, x^\\star) = 1$。在观测到数据集 $D$ 后，权重的后验分布是 $p(\\mathbf{w} \\mid D)$。\n\n离散标签 $y^\\star$ 的后验预测分布由 softmax 输出在后验分布下的期望给出：\n$$\np(y^\\star = k \\mid x^\\star, D) = \\mathbb{E}_{\\mathbf{w} \\sim p(\\mathbf{w} \\mid D)} \\left[ q_k(\\mathbf{w}, x^\\star) \\right], \\quad k \\in \\{1, \\dots, K\\}.\n$$\n这是 BNN 中贝叶斯模型平均的基本法则。\n\n对于任何离散分布 $\\{p_k\\}_{k=1}^K$，以自然单位 (nats) 计量的香农熵定义为\n$$\nH(\\{p_k\\}) = -\\sum_{k=1}^K p_k \\log p_k,\n$$\n其中 $\\log$ 表示自然对数。给定 $C$ 时，随机变量 $A$ 和 $B$ 之间的条件互信息由链式法则恒等式定义\n$$\nI(A; B \\mid C) = H(A \\mid C) - H(A \\mid B, C).\n$$\n\n我们将这些定义应用于 $A = y^\\star$，$B = \\mathbf{w}$ 和 $C = (x^\\star, D)$。第一项 $H(y^\\star \\mid x^\\star, D)$ 是 $y^\\star$ 的后验预测分布的香农熵。第二项 $H(y^\\star \\mid \\mathbf{w}, x^\\star, D)$ 是在给定固定权重 $\\mathbf{w}$ 和 $x^\\star$ 的条件下 $y^\\star$ 的条件熵，它仅依赖于 softmax 输出 $\\{q_k(\\mathbf{w}, x^\\star)\\}_{k=1}^K$，因为 $D$ 是通过 $p(\\mathbf{w} \\mid D)$ 融入的。因此，通过将 $\\mathbf{w}$ 视为一个根据 $p(\\mathbf{w} \\mid D)$ 分布的随机变量并对其取条件，我们得到\n$$\nI(y^\\star; \\mathbf{w} \\mid x^\\star, D) = H(y^\\star \\mid x^\\star, D) - \\mathbb{E}_{\\mathbf{w} \\sim p(\\mathbf{w} \\mid D)} \\big[ H(y^\\star \\mid \\mathbf{w}, x^\\star) \\big].\n$$\n\n为了构建一个蒙特卡洛 (MC) 估计器，我们为 $s \\in \\{1, \\dots, S\\}$ 抽取 $S$ 个独立样本 $\\mathbf{w}^{(s)} \\sim p(\\mathbf{w} \\mid D)$。每个样本产生一个类别概率向量\n$$\n\\mathbf{q}^{(s)}(x^\\star) = \\big( q_1(\\mathbf{w}^{(s)}, x^\\star), \\dots, q_K(\\mathbf{w}^{(s)}, x^\\star) \\big),\n$$\n满足 $\\sum_{k=1}^K q_k(\\mathbf{w}^{(s)}, x^\\star) = 1$。后验预测概率通过经验平均值来近似\n$$\n\\bar{p}_k(x^\\star, D) \\approx \\frac{1}{S} \\sum_{s=1}^S q_k(\\mathbf{w}^{(s)}, x^\\star).\n$$\n预测熵则通过插件估计器来近似\n$$\n\\widehat{H}_{\\text{pred}} = -\\sum_{k=1}^K \\bar{p}_k(x^\\star, D) \\log \\bar{p}_k(x^\\star, D).\n$$\n给定单个采样权重 $\\mathbf{w}^{(s)}$ 的条件熵是\n$$\nH^{(s)} = -\\sum_{k=1}^K q_k(\\mathbf{w}^{(s)}, x^\\star) \\log q_k(\\mathbf{w}^{(s)}, x^\\star),\n$$\n其后验期望通过经验均值来近似\n$$\n\\widehat{\\mathbb{E}}[H] = \\frac{1}{S} \\sum_{s=1}^S H^{(s)}.\n$$\n因此，互信息的 MC 估计器（以 nats 为单位）是\n$$\n\\widehat{I}(y^\\star; \\mathbf{w} \\mid x^\\star, D) = \\widehat{H}_{\\text{pred}} - \\widehat{\\mathbb{E}}[H].\n$$\n\n数值稳定性考虑：当评估可能极度接近 0 的概率 $p$ 的 $\\log p$ 时，我们将 $p$ 裁剪到 $[\\varepsilon, 1]$ 范围内，其中 $\\varepsilon > 0$ 是一个小数，例如 $\\varepsilon = 10^{-12}$。这可以防止在 $p = 0$ 时出现未定义的对数，并抑制 $-\\log p$ 中的数值溢出。\n\n估计器的性质：\n- 估计器 $\\widehat{\\mathbb{E}}[H]$ 是 $\\mathbb{E}_{\\mathbf{w} \\sim p(\\mathbf{w} \\mid D)}[H(y^\\star \\mid \\mathbf{w}, x^\\star)]$ 的无偏估计器，因为它是独立同分布 (i.i.d.) 项的样本均值。\n- 插件估计器 $\\widehat{H}_{\\text{pred}}$ 由于熵的非线性，对于有限的 $S$ 通常是有偏的，但当 $S \\to \\infty$ 时它是一致的，因为 $\\bar{p}_k(x^\\star, D)$ 几乎必然收敛于 $\\mathbb{E}[q_k(\\mathbf{w}, x^\\star)]$。\n- 当 $S \\to \\infty$ 时，互信息估计器 $\\widehat{I}$ 是一致的。\n\n每个测试用例的算法步骤：\n1. 设 $P$ 为 $S \\times K$ 矩阵，其第 $s$ 行为 $\\mathbf{q}^{(s)}(x^\\star)$。\n2. 将 $P$ 的元素裁剪到 $[\\varepsilon, 1]$ 内，并在必要时重新归一化行以保持行和为 1（对于所提供的测试用例，行和已经为 1）。\n3. 计算 $\\bar{\\mathbf{p}} = \\frac{1}{S} \\sum_{s=1}^S \\mathbf{q}^{(s)}(x^\\star)$。\n4. 计算 $\\widehat{H}_{\\text{pred}} = -\\sum_{k=1}^K \\bar{p}_k \\log \\bar{p}_k$。\n5. 对于每个 $s$，计算 $H^{(s)} = -\\sum_{k=1}^K q^{(s)}_k \\log q^{(s)}_k$，然后计算 $\\widehat{\\mathbb{E}}[H] = \\frac{1}{S} \\sum_{s=1}^S H^{(s)}$。\n6. 输出以 nats 为单位的 $\\widehat{I} = \\widehat{H}_{\\text{pred}} - \\widehat{\\mathbb{E}}[H]$。\n\n该程序为指定的测试套件实现了这个估计器，并按要求打印单行结果，格式为用方括号括起来的逗号分隔列表，每个值四舍五入到 6 位小数。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef mutual_information_mc(prob_matrix: np.ndarray, eps: float = 1e-12) - float:\n    \"\"\"\n    Estimate I(y*, w | x*, D) for a classification BNN using Monte Carlo samples.\n    prob_matrix: shape (S, K), rows are class-probability vectors from posterior samples.\n    eps: small positive constant for numerical stability when taking logs.\n    Returns MI in nats as a float.\n    \"\"\"\n    # Ensure input is float64 for numerical stability\n    P = np.array(prob_matrix, dtype=np.float64)\n\n    # Clip probabilities to avoid log(0); rows should already sum to 1 in test cases.\n    P = np.clip(P, eps, 1.0)\n    # Renormalize rows in case clipping perturbed sums slightly\n    row_sums = P.sum(axis=1, keepdims=True)\n    # Avoid division by zero, though with eps clipping this shouldn't happen\n    row_sums = np.maximum(row_sums, eps)\n    P = P / row_sums\n\n    # Number of samples S and classes K\n    S, K = P.shape\n\n    # Predictive probabilities: average over samples\n    p_bar = P.mean(axis=0)\n\n    # Entropy of predictive distribution (in nats)\n    H_pred = -np.sum(p_bar * np.log(p_bar))\n\n    # Conditional entropy for each sample\n    H_cond_samples = -np.sum(P * np.log(P), axis=1)\n    H_cond_avg = H_cond_samples.mean()\n\n    # Mutual information estimate\n    MI = H_pred - H_cond_avg\n    return float(MI)\n\n\ndef solve():\n    # Define epsilon for numerical stability\n    eps = 1e-12\n\n    # Construct test cases as per the problem statement.\n    # Case 1: Uniform, identical predictions (S=5, K=3)\n    case1 = np.array([[1/3, 1/3, 1/3]] * 5, dtype=np.float64)\n\n    # Case 2: Deterministic disagreement across samples (S=2, K=3)\n    case2 = np.array([[1.0, 0.0, 0.0],\n                      [0.0, 1.0, 0.0]], dtype=np.float64)\n\n    # Case 3: Identical deterministic predictions (S=3, K=4)\n    case3 = np.array([[0.0, 0.0, 1.0, 0.0]] * 3, dtype=np.float64)\n\n    # Case 4: Moderate variation across samples (S=4, K=3)\n    case4 = np.array([[0.7, 0.2, 0.1],\n                      [0.6, 0.3, 0.1],\n                      [0.3, 0.4, 0.3],\n                      [0.2, 0.7, 0.1]], dtype=np.float64)\n\n    # Case 5: Extreme probabilities and one balanced sample (S=3, K=2)\n    tiny = 1e-12\n    case5 = np.array([[tiny, 1.0 - tiny],\n                      [1.0 - tiny, tiny],\n                      [0.5, 0.5]], dtype=np.float64)\n\n    # Case 6: Degenerate single-class scenario (S=3, K=1)\n    case6 = np.array([[1.0], [1.0], [1.0]], dtype=np.float64)\n\n    test_cases = [case1, case2, case3, case4, case5, case6]\n\n    results = []\n    for P in test_cases:\n        result = mutual_information_mc(P, eps=eps)\n        results.append(result)\n\n    # Final print statement in the exact required format with 6 decimal digits.\n    print(\"[\" + \",\".join(f\"{r:.6f}\" for r in results) + \"]\")\n\nsolve()\n```"
        }
    ]
}