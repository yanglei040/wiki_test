{
    "hands_on_practices": [
        {
            "introduction": "A central challenge in Bayesian deep learning is performing posterior inference over a high-dimensional weight space with massive datasets. Stochastic Gradient Langevin Dynamics (SGLD) provides a scalable solution by combining the gradient-based updates of stochastic optimization with injected noise to explore the posterior distribution. This practice requires you to derive the SGLD update rule from its foundations in Langevin diffusion, providing a deep understanding of the mechanics behind this powerful MCMC algorithm .",
            "id": "3291187",
            "problem": "Consider a Bayesian neural network with weights $w \\in \\mathbb{R}^{d}$, prior $p(w)=\\mathcal{N}(0,\\sigma_{p}^{2} I_{d})$, and data $\\mathcal{D}=\\{(x_{i},y_{i})\\}_{i=1}^{N}$ modeled with additive Gaussian observation noise of known variance $\\sigma_{y}^{2}$ around the network output $f_{w}(x)$, so that $p(y_{i}\\mid x_{i},w)=\\mathcal{N}(f_{w}(x_{i}),\\sigma_{y}^{2})$ independently for $i=1,\\dots,N$. The posterior satisfies $p(w\\mid \\mathcal{D}) \\propto p(w)\\prod_{i=1}^{N}p(y_{i}\\mid x_{i},w)$. Let $B_{t}\\subset\\{1,\\dots,N\\}$ be a mini-batch of size $m=|B_{t}|$ sampled uniformly without replacement at iteration $t$, and let $\\xi_{t}\\sim\\mathcal{N}(0,I_{d})$ be independent across $t$. Starting from the overdamped Langevin diffusion targeting the posterior and its Eulerâ€“Maruyama discretization, derive the stochastic gradient Langevin dynamics update using the unbiased mini-batch estimator of the posterior score based on $B_{t}$. Express your final update at iteration $t$ explicitly in terms of $w_{t}$, $\\sigma_{p}^{2}$, $\\sigma_{y}^{2}$, $N$, $m$, $f_{w_{t}}(x)$, and $\\nabla_{w} f_{w_{t}}(x)$.\n\nThen, state the necessary step-size schedule conditions on the sequence $\\{\\eta_{t}\\}_{t\\geq 1}$ under which the resulting Markov chain asymptotically samples from the exact posterior in the limit $t\\to\\infty$ (assume standard regularity conditions ensuring stability, including measurability, Lipschitz continuity of the score, and bounded second moments of the mini-batch gradient noise, hold). Your final answer must contain only the derived update and the step-size conditions. Do not include any explanatory text. If you provide multiple items, present them as a single row vector. The answer does not require rounding and has no physical units.",
            "solution": "The problem requires the derivation of the Stochastic Gradient Langevin Dynamics (SGLD) update rule for a Bayesian neural network and the statement of the step-size conditions for its convergence to the posterior distribution.\n\nFirst, we formalize the target distribution. The posterior distribution over the weights $w \\in \\mathbb{R}^{d}$, given the data $\\mathcal{D}=\\{(x_{i},y_{i})\\}_{i=1}^{N}$, is given by Bayes' theorem:\n$$\np(w \\mid \\mathcal{D}) \\propto p(\\mathcal{D} \\mid w) p(w)\n$$\nGiven the independence of the data points, the likelihood is $p(\\mathcal{D} \\mid w) = \\prod_{i=1}^{N} p(y_{i} \\mid x_{i}, w)$. Thus, the posterior is:\n$$\np(w \\mid \\mathcal{D}) \\propto p(w) \\prod_{i=1}^{N} p(y_{i} \\mid x_{i}, w)\n$$\nIt is more convenient to work with the log-posterior, which is given by:\n$$\n\\ln p(w \\mid \\mathcal{D}) = \\ln p(w) + \\sum_{i=1}^{N} \\ln p(y_{i} \\mid x_{i}, w) + C\n$$\nwhere $C$ is a normalization constant independent of $w$.\n\nThe problem specifies a Gaussian prior $p(w) = \\mathcal{N}(0, \\sigma_{p}^{2} I_{d})$ and a Gaussian likelihood $p(y_{i} \\mid x_{i}, w) = \\mathcal{N}(f_{w}(x_{i}), \\sigma_{y}^{2})$. The corresponding log-densities, ignoring constants, are:\n$$\n\\ln p(w) = -\\frac{1}{2\\sigma_{p}^{2}} w^{T}w + C_{p} = -\\frac{1}{2\\sigma_{p}^{2}} \\|w\\|_{2}^{2} + C_{p}\n$$\n$$\n\\ln p(y_{i} \\mid x_{i}, w) = -\\frac{1}{2\\sigma_{y}^{2}} (y_{i} - f_{w}(x_{i}))^{2} + C_{y}\n$$\nSubstituting these into the log-posterior expression, we get:\n$$\n\\ln p(w \\mid \\mathcal{D}) = -\\frac{1}{2\\sigma_{p}^{2}} \\|w\\|_{2}^{2} - \\frac{1}{2\\sigma_{y}^{2}} \\sum_{i=1}^{N} (y_{i} - f_{w}(x_{i}))^{2} + C'\n$$\nThe SGLD algorithm is a discretization of an overdamped Langevin diffusion process. The continuous-time Langevin diffusion that has $p(w \\mid \\mathcal{D})$ as its invariant distribution is described by the stochastic differential equation (SDE):\n$$\ndw_{t} = \\frac{1}{2} \\nabla_{w} \\ln p(w_{t} \\mid \\mathcal{D}) dt + d\\mathcal{W}_{t}\n$$\nwhere $\\mathcal{W}_{t}$ is a standard $d$-dimensional Wiener process.\nThe Euler-Maruyama discretization of this SDE with a step-size $\\eta_{t}$ leads to the Langevin Dynamics (LD) update:\n$$\nw_{t+1} = w_{t} + \\frac{\\eta_{t}}{2} \\nabla_{w} \\ln p(w_{t} \\mid \\mathcal{D}) + \\sqrt{\\eta_{t}} \\xi_{t}\n$$\nwhere $\\xi_{t} \\sim \\mathcal{N}(0, I_{d})$ is a vector of independent standard Gaussian noise.\n\nThe next step is to compute the gradient of the log-posterior, also known as the score.\n$$\n\\nabla_{w} \\ln p(w \\mid \\mathcal{D}) = \\nabla_{w} \\left( -\\frac{1}{2\\sigma_{p}^{2}} w^{T}w - \\frac{1}{2\\sigma_{y}^{2}} \\sum_{i=1}^{N} (y_{i} - f_{w}(x_{i}))^{2} \\right)\n$$\nUsing the chain rule, we obtain:\n$$\n\\nabla_{w} \\ln p(w \\mid \\mathcal{D}) = -\\frac{1}{\\sigma_{p}^{2}} w - \\frac{1}{2\\sigma_{y}^{2}} \\sum_{i=1}^{N} 2(y_{i} - f_{w}(x_{i}))(-\\nabla_{w} f_{w}(x_{i}))\n$$\n$$\n\\nabla_{w} \\ln p(w \\mid \\mathcal{D}) = -\\frac{1}{\\sigma_{p}^{2}} w + \\frac{1}{\\sigma_{y}^{2}} \\sum_{i=1}^{N} (y_{i} - f_{w}(x_{i})) \\nabla_{w} f_{w}(x_{i})\n$$\nFor large datasets where $N$ is large, computing this full gradient at each iteration is computationally prohibitive. SGLD approximates this gradient using a mini-batch of data. Let $B_{t} \\subset \\{1, \\dots, N\\}$ be a mini-batch of size $m$ sampled uniformly at random. An unbiased estimator of the full gradient is constructed by scaling the sum over the mini-batch:\n$$\n\\widehat{\\nabla_{w} \\ln p(w \\mid \\mathcal{D})} = -\\frac{1}{\\sigma_{p}^{2}} w + \\frac{N}{m} \\sum_{i \\in B_{t}} \\frac{1}{\\sigma_{y}^{2}}(y_{i} - f_{w}(x_{i})) \\nabla_{w} f_{w}(x_{i})\n$$\nSubstituting this stochastic gradient estimator into the LD update equation yields the SGLD update rule:\n$$\nw_{t+1} = w_{t} + \\frac{\\eta_{t}}{2} \\left[ -\\frac{1}{\\sigma_{p}^{2}} w_{t} + \\frac{N}{m\\sigma_{y}^{2}} \\sum_{i \\in B_{t}} (y_{i} - f_{w_{t}}(x_{i})) \\nabla_{w} f_{w_{t}}(x_{i}) \\right] + \\sqrt{\\eta_{t}}\\xi_{t}\n$$\nThis expression can be rewritten to clearly show the components:\n$$\nw_{t+1} = w_{t} - \\frac{\\eta_{t}}{2\\sigma_{p}^{2}} w_{t} + \\frac{\\eta_{t}N}{2m\\sigma_{y}^{2}} \\sum_{i \\in B_{t}} (y_{i} - f_{w_{t}}(x_{i})) \\nabla_{w} f_{w_{t}}(x_{i}) + \\sqrt{\\eta_{t}}\\xi_{t}\n$$\n\nFor the SGLD iterates to converge in distribution to the true posterior $p(w \\mid \\mathcal{D})$, the step-size schedule $\\{\\eta_{t}\\}_{t\\geq 1}$ must satisfy specific conditions. These conditions are required to balance the exploration of the state space (driven by the noise term and the gradient) with the reduction of the discretization error and the variance from the stochastic gradient estimation. The standard conditions, analogous to those for stochastic approximation (Robbins-Monro conditions), are:\n1. The step sizes must not decay too quickly, to allow the process to explore the entire state space. This is ensured by the condition that the sum of step sizes must diverge:\n$$\n\\sum_{t=1}^{\\infty} \\eta_{t} = \\infty\n$$\n2. The step sizes must decay quickly enough to ensure that the variance of the injected noise decreases over time and that the discretization error of the Euler-Maruyama scheme becomes negligible in the limit. This is ensured by the condition that the sum of the squares of the step sizes must converge:\n$$\n\\sum_{t=1}^{\\infty} \\eta_{t}^{2}  \\infty\n$$\nUnder these conditions, and assuming standard regularity of the target distribution and the network function, the distribution of $w_{t}$ generated by the SGLD algorithm converges weakly to the posterior distribution $p(w \\mid \\mathcal{D})$ as $t \\to \\infty$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} w_{t+1} = w_{t} + \\frac{\\eta_{t}}{2} \\left[ -\\frac{1}{\\sigma_{p}^{2}} w_{t} + \\frac{N}{m\\sigma_{y}^{2}} \\sum_{i \\in B_{t}} (y_{i} - f_{w_{t}}(x_{i})) \\nabla_{w} f_{w_{t}}(x_{i}) \\right] + \\sqrt{\\eta_{t}}\\xi_{t}  \\sum_{t=1}^{\\infty} \\eta_{t} = \\infty  \\sum_{t=1}^{\\infty} \\eta_{t}^{2}  \\infty \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A key advantage of Bayesian neural networks is their ability to quantify uncertainty. For regression tasks, this can be extended to heteroscedastic models where the predictive uncertainty is input-dependent. To train such a model or sample from its posterior using a gradient-based method like SGLD, we must first compute the gradient of the log-likelihood with respect to the model weights. This exercise walks you through the essential calculus of deriving these gradients for a BNN that parameterizes both the mean and variance of its output .",
            "id": "3291222",
            "problem": "Consider a Bayesian neural network (BNN) for one-dimensional heteroscedastic regression. For each input $x_i$, the network with weights $w$ outputs two scalar functions $(\\mu_w(x_i), s_w(x_i))$, where $s_w(x_i) = \\log \\sigma_w^2(x_i)$ encodes the predictive variance via $\\sigma_w^2(x_i) = \\exp(s_w(x_i))$. Assume the data likelihood for a single observation $(x_i, y_i)$ is the Gaussian density $p(y_i \\mid x_i, w) = \\mathcal{N}(y_i; \\mu_w(x_i), \\sigma_w^2(x_i))$. Starting from the definition of the Gaussian probability density function and using standard rules of differentiation (including the chain rule), derive the per-sample log-likelihood $\\ell_i(w) = \\ln p(y_i \\mid x_i, w)$ and its gradient with respect to $w$, expressed explicitly in terms of $y_i$, $\\mu_w(x_i)$, $s_w(x_i)$, and the weight-space Jacobians $\\nabla_w \\mu_w(x_i)$ and $\\nabla_w s_w(x_i)$. Your derivation should assume only the properties of tweaking the Gaussian density and differentiability of the network outputs with respect to $w$. Express your final answer as a pair containing the scalar $\\ell_i(w)$ and the vector $\\nabla_w \\ell_i(w)$. The final answer must be a single closed-form analytic expression. Do not include any units. No rounding is required.",
            "solution": "We begin by stating the probability density function (PDF) for the Gaussian likelihood, as specified in the problem statement. For a single data point $(x_i, y_i)$, the likelihood is given by:\n$$\np(y_i \\mid x_i, w) = \\mathcal{N}(y_i; \\mu_w(x_i), \\sigma_w^2(x_i))\n$$\nThe explicit form of the Gaussian PDF for a variable $y$ with mean $\\mu$ and variance $\\sigma^2$ is:\n$$\n\\mathcal{N}(y; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right)\n$$\nIn our case, the mean is $\\mu_w(x_i)$ and the variance is $\\sigma_w^2(x_i)$. The problem defines a specific parameterization for the variance in terms of the network output $s_w(x_i)$:\n$$\n\\sigma_w^2(x_i) = \\exp(s_w(x_i))\n$$\nSubstituting this into the PDF, we get the likelihood for our specific model:\n$$\np(y_i \\mid x_i, w) = \\frac{1}{\\sqrt{2\\pi\\exp(s_w(x_i))}} \\exp\\left(-\\frac{(y_i - \\mu_w(x_i))^2}{2\\exp(s_w(x_i))}\\right)\n$$\nThe first part of the task is to derive the per-sample log-likelihood, $\\ell_i(w) = \\ln p(y_i \\mid x_i, w)$. We take the natural logarithm of the expression above, using the properties $\\ln(ab) = \\ln(a) + \\ln(b)$, $\\ln(a^c) = c \\ln(a)$, and $\\ln(\\exp(c)) = c$.\n$$\n\\ell_i(w) = \\ln\\left( \\left(2\\pi\\exp(s_w(x_i))\\right)^{-1/2} \\right) + \\ln\\left( \\exp\\left(-\\frac{(y_i - \\mu_w(x_i))^2}{2\\exp(s_w(x_i))}\\right) \\right)\n$$\n$$\n\\ell_i(w) = -\\frac{1}{2} \\ln(2\\pi\\exp(s_w(x_i))) - \\frac{(y_i - \\mu_w(x_i))^2}{2\\exp(s_w(x_i))}\n$$\n$$\n\\ell_i(w) = -\\frac{1}{2} \\left( \\ln(2\\pi) + \\ln(\\exp(s_w(x_i))) \\right) - \\frac{1}{2}(y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i))\n$$\n$$\n\\ell_i(w) = -\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}s_w(x_i) - \\frac{1}{2}(y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i))\n$$\nThis is the expression for the per-sample log-likelihood, the first part of our answer.\n\nThe second part of the task is to derive the gradient of the log-likelihood with respect to the network weights, $\\nabla_w \\ell_i(w)$. We must differentiate the expression for $\\ell_i(w)$ with respect to the vector $w$. The quantities $\\mu_w(x_i)$ and $s_w(x_i)$ are functions of $w$. We will apply the multivariate chain rule.\n$$\n\\nabla_w \\ell_i(w) = \\nabla_w \\left( -\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}s_w(x_i) - \\frac{1}{2}(y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i)) \\right)\n$$\nThe first term, $-\\frac{1}{2}\\ln(2\\pi)$, is a constant with respect to $w$, so its gradient is zero.\n$$\n\\nabla_w \\ell_i(w) = -\\frac{1}{2}\\nabla_w(s_w(x_i)) - \\frac{1}{2}\\nabla_w \\left( (y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i)) \\right)\n$$\nFor the second term, we use the product rule for gradients, $\\nabla(uv) = (\\nabla u)v + u(\\nabla v)$. Let $u = (y_i - \\mu_w(x_i))^2$ and $v = \\exp(-s_w(x_i))$. We find their gradients with respect to $w$ using the chain rule.\n\nThe gradient of $u$ with respect to $w$ is:\n$$\n\\nabla_w u = \\frac{\\partial u}{\\partial \\mu_w(x_i)} \\nabla_w \\mu_w(x_i) = 2(y_i - \\mu_w(x_i)) \\cdot (-1) \\cdot \\nabla_w \\mu_w(x_i) = -2(y_i - \\mu_w(x_i)) \\nabla_w \\mu_w(x_i)\n$$\nThe gradient of $v$ with respect to $w$ is:\n$$\n\\nabla_w v = \\frac{\\partial v}{\\partial s_w(x_i)} \\nabla_w s_w(x_i) = \\exp(-s_w(x_i)) \\cdot (-1) \\cdot \\nabla_w s_w(x_i) = -\\exp(-s_w(x_i)) \\nabla_w s_w(x_i)\n$$\nNow, applying the product rule:\n$$\n\\nabla_w (uv) = \\left( -2(y_i - \\mu_w(x_i)) \\nabla_w \\mu_w(x_i) \\right) \\exp(-s_w(x_i)) + (y_i - \\mu_w(x_i))^2 \\left( -\\exp(-s_w(x_i)) \\nabla_w s_w(x_i) \\right)\n$$\n$$\n\\nabla_w (uv) = -2(y_i - \\mu_w(x_i)) \\exp(-s_w(x_i)) \\nabla_w \\mu_w(x_i) - (y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i)) \\nabla_w s_w(x_i)\n$$\nSubstituting this back into the expression for $\\nabla_w \\ell_i(w)$:\n$$\n\\nabla_w \\ell_i(w) = -\\frac{1}{2}\\nabla_w s_w(x_i) - \\frac{1}{2} \\left[ -2(y_i - \\mu_w(x_i)) \\exp(-s_w(x_i)) \\nabla_w \\mu_w(x_i) - (y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i)) \\nabla_w s_w(x_i) \\right]\n$$\nDistributing the $-\\frac{1}{2}$ term:\n$$\n\\nabla_w \\ell_i(w) = -\\frac{1}{2}\\nabla_w s_w(x_i) + (y_i - \\mu_w(x_i)) \\exp(-s_w(x_i)) \\nabla_w \\mu_w(x_i) + \\frac{1}{2}(y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i)) \\nabla_w s_w(x_i)\n$$\nFinally, we group terms by the Jacobians $\\nabla_w \\mu_w(x_i)$ and $\\nabla_w s_w(x_i)$:\n$$\n\\nabla_w \\ell_i(w) = \\left( (y_i - \\mu_w(x_i)) \\exp(-s_w(x_i)) \\right) \\nabla_w \\mu_w(x_i) + \\left( \\frac{1}{2}(y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i)) - \\frac{1}{2} \\right) \\nabla_w s_w(x_i)\n$$\nThis can be factored slightly for clarity:\n$$\n\\nabla_w \\ell_i(w) = (y_i - \\mu_w(x_i))\\exp(-s_w(x_i)) \\nabla_w \\mu_w(x_i) + \\frac{1}{2} \\left[ (y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i)) - 1 \\right] \\nabla_w s_w(x_i)\n$$\nThis is the final expression for the gradient of the per-sample log-likelihood. The required answer is the pair consisting of $\\ell_i(w)$ and $\\nabla_w \\ell_i(w)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{1}{2}s_w(x_i) - \\frac{1}{2}(y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i)) - \\frac{1}{2}\\ln(2\\pi)  (y_i - \\mu_w(x_i))\\exp(-s_w(x_i)) \\nabla_w \\mu_w(x_i) + \\frac{1}{2} \\left[ (y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i)) - 1 \\right] \\nabla_w s_w(x_i) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "With a set of samples from the posterior distribution of a BNN, we can do more than just make predictions; we can analyze the model's uncertainty. Epistemic uncertainty, or the model's uncertainty about its own parameters, can be quantified using information theory. This hands-on exercise guides you through deriving and implementing a Monte Carlo estimator for the mutual information between predictions and model weights, a quantity central to applications like Bayesian Active Learning by Disagreement (BALD) .",
            "id": "3291210",
            "problem": "Consider a classification Bayesian Neural Network (BNN), defined as a probabilistic model over weights $w$ with posterior distribution $p(w \\mid D)$ after observing dataset $D$. For an input $x^\\star$, the BNN produces class probabilities via a softmax output $q_k(w, x^\\star)$ for classes $k \\in \\{1, \\dots, K\\}$, which form a valid discrete distribution for each $w$. The predictive distribution over labels $y^\\star$ given $x^\\star$ and $D$ is determined by averaging these class probabilities with respect to the posterior over $w$. The mutual information between $y^\\star$ and $w$ conditioned on $x^\\star$ and $D$ quantifies epistemic uncertainty and is a central quantity in Bayesian Active Learning by Disagreement (BALD). Your task is to derive, from first principles in probability and information theory, an implementable Monte Carlo (MC) estimator for this mutual information using independent samples $w^{(s)} \\sim p(w \\mid D)$.\n\nUse the following foundational base:\n- The definition of the posterior predictive distribution for a discrete label: the probability of $y^\\star = k$ given $x^\\star$ and $D$ is the expectation of $q_k(w, x^\\star)$ under $p(w \\mid D)$.\n- Shannon entropy for a discrete distribution in natural units (nats): for probabilities $\\{p_k\\}_{k=1}^K$, $H = -\\sum_{k=1}^K p_k \\log p_k$, where $\\log$ denotes the natural logarithm.\n- The chain rule definition of conditional mutual information: for random variables $A$, $B$, and $C$, $I(A; B \\mid C) = H(A \\mid C) - H(A \\mid B, C)$.\n\nStarting from these definitions, derive an estimator where expectations with respect to $p(w \\mid D)$ are replaced by empirical averages over $S$ independent posterior samples $w^{(s)}$, each yielding a class-probability vector $\\big(q_1(w^{(s)}, x^\\star), \\dots, q_K(w^{(s)}, x^\\star)\\big)$. Then, implement a program that computes this MC estimator of mutual information in nats for a given set of posterior class-probability samples.\n\nNumerical and implementation requirements:\n- All logarithms must be natural logarithms, and entropy and mutual information must be reported in nats.\n- For numerical stability, when computing $\\log p$ for any probability $p$, you should clip $p$ into the closed interval $[\\varepsilon, 1]$ for a small positive $\\varepsilon$ (choose a fixed $\\varepsilon$ appropriate for double-precision arithmetic).\n- The final output must be a single line containing a comma-separated list enclosed in square brackets, with each mutual information estimate rounded to $6$ decimal digits.\n\nTest suite:\nFor each test case below, treat each row as the class-probability vector produced by a single independently drawn posterior sample $w^{(s)}$. In all cases, $S$ denotes the number of posterior samples and $K$ the number of classes. Compute the MC mutual information estimate in nats for each case.\n\n- Case $1$ (uniform, identical predictions; expected mutual information close to $0$): $S = 5$, $K = 3$, rows\n  - $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$\n  - $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$\n  - $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$\n  - $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$\n  - $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$.\n- Case $2$ (deterministic disagreement across samples): $S = 2$, $K = 3$, rows\n  - $(1, 0, 0)$\n  - $(0, 1, 0)$.\n- Case $3$ (identical deterministic predictions; expected mutual information $0$): $S = 3$, $K = 4$, rows\n  - $(0, 0, 1, 0)$\n  - $(0, 0, 1, 0)$\n  - $(0, 0, 1, 0)$.\n- Case $4$ (moderate variation across samples): $S = 4$, $K = 3$, rows\n  - $(0.7, 0.2, 0.1)$\n  - $(0.6, 0.3, 0.1)$\n  - $(0.3, 0.4, 0.3)$\n  - $(0.2, 0.7, 0.1)$.\n- Case $5$ (extreme probabilities and one balanced sample; tests numerical stability): $S = 3$, $K = 2$, rows\n  - $(10^{-12}, 1 - 10^{-12})$\n  - $(1 - 10^{-12}, 10^{-12})$\n  - $(0.5, 0.5)$.\n- Case $6$ (degenerate single-class scenario; expected mutual information $0$): $S = 3$, $K = 1$, rows\n  - $(1)$\n  - $(1)$\n  - $(1)$.\n\nProgram input and output specification:\n- Your program must be self-contained and must not read any input; it should internally construct the above test cases.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each mutual information estimate rounded to $6$ decimal digits (for example, $[r_1, r_2, r_3]$).",
            "solution": "We begin with a classification Bayesian Neural Network (BNN) that outputs, for any fixed weight vector $w$ and input $x^\\star$, a valid discrete distribution over $K$ classes denoted $\\{q_k(w, x^\\star)\\}_{k=1}^K$, where $q_k(w, x^\\star) \\ge 0$ and $\\sum_{k=1}^K q_k(w, x^\\star) = 1$. The posterior over weights after observing dataset $D$ is $p(w \\mid D)$.\n\nThe posterior predictive distribution over the discrete label $y^\\star$ is given by the expectation of the softmax outputs under the posterior:\n$$\np(y^\\star = k \\mid x^\\star, D) = \\mathbb{E}_{w \\sim p(w \\mid D)} \\left[ q_k(w, x^\\star) \\right], \\quad k \\in \\{1, \\dots, K\\}.\n$$\nThis is the foundational law for Bayesian model averaging in a BNN.\n\nFor any discrete distribution $\\{p_k\\}_{k=1}^K$, Shannon entropy measured in natural units (nats) is defined as\n$$\nH(\\{p_k\\}) = -\\sum_{k=1}^K p_k \\log p_k,\n$$\nwhere $\\log$ denotes the natural logarithm. Conditional mutual information between random variables $A$ and $B$ given $C$ is defined by the chain rule identity\n$$\nI(A; B \\mid C) = H(A \\mid C) - H(A \\mid B, C).\n$$\n\nWe apply these definitions to $A = y^\\star$, $B = w$, and $C = (x^\\star, D)$. The first term, $H(y^\\star \\mid x^\\star, D)$, is the Shannon entropy of the posterior predictive distribution over $y^\\star$. The second term, $H(y^\\star \\mid w, x^\\star, D)$, is the conditional entropy of $y^\\star$ given a fixed weight $w$ and $x^\\star$, which depends only on the softmax output $\\{q_k(w, x^\\star)\\}_{k=1}^K$ because $D$ is incorporated through $p(w \\mid D)$. Therefore, by conditioning on $w$ as a random variable distributed according to $p(w \\mid D)$, we obtain\n$$\nI(y^\\star; w \\mid x^\\star, D) = H(y^\\star \\mid x^\\star, D) - \\mathbb{E}_{w \\sim p(w \\mid D)} \\big[ H(y^\\star \\mid w, x^\\star) \\big].\n$$\n\nTo construct a Monte Carlo (MC) estimator, draw $S$ independent samples $w^{(s)} \\sim p(w \\mid D)$ for $s \\in \\{1, \\dots, S\\}$. Each sample yields a class-probability vector\n$$\n\\mathbf{q}^{(s)}(x^\\star) = \\big( q_1(w^{(s)}, x^\\star), \\dots, q_K(w^{(s)}, x^\\star) \\big),\n$$\nwith $\\sum_{k=1}^K q_k(w^{(s)}, x^\\star) = 1$. The posterior predictive probabilities are approximated by the empirical average\n$$\n\\bar{p}_k(x^\\star, D) \\approx \\frac{1}{S} \\sum_{s=1}^S q_k(w^{(s)}, x^\\star).\n$$\nThe predictive entropy is then approximated by the plug-in estimator\n$$\n\\widehat{H}_{\\text{pred}} = -\\sum_{k=1}^K \\bar{p}_k(x^\\star, D) \\log \\bar{p}_k(x^\\star, D).\n$$\nThe conditional entropy given a single sampled weight $w^{(s)}$ is\n$$\nH^{(s)} = -\\sum_{k=1}^K q_k(w^{(s)}, x^\\star) \\log q_k(w^{(s)}, x^\\star),\n$$\nand its posterior expectation is approximated by the empirical mean\n$$\n\\widehat{\\mathbb{E}}[H] = \\frac{1}{S} \\sum_{s=1}^S H^{(s)}.\n$$\nTherefore, the MC estimator of mutual information in nats is\n$$\n\\widehat{I}(y^\\star; w \\mid x^\\star, D) = \\widehat{H}_{\\text{pred}} - \\widehat{\\mathbb{E}}[H].\n$$\n\nNumerical stability considerations: when evaluating $\\log p$ for probabilities $p$ that may be extremely close to $0$, we clip $p$ into $[\\varepsilon, 1]$ for a small $\\varepsilon > 0$, for example $\\varepsilon = 10^{-12}$. This prevents undefined logarithms at $p = 0$ and suppresses numerical overflow in $-\\log p$.\n\nProperties of the estimator:\n- The estimator $\\widehat{\\mathbb{E}}[H]$ is an unbiased estimator of $\\mathbb{E}_{w \\sim p(w \\mid D)}[H(y^\\star \\mid w, x^\\star)]$ because it is a sample mean of independent and identically distributed (i.i.d.) terms.\n- The plug-in estimator $\\widehat{H}_{\\text{pred}}$ is generally biased for finite $S$ due to the nonlinearity of entropy, but it is consistent as $S \\to \\infty$ because $\\bar{p}_k(x^\\star, D)$ converges almost surely to $\\mathbb{E}[q_k(w, x^\\star)]$.\n- The mutual information estimator $\\widehat{I}$ is consistent as $S \\to \\infty$.\n\nAlgorithmic steps for each test case:\n1. Let $P$ be the $S \\times K$ matrix whose $s$-th row is $\\mathbf{q}^{(s)}(x^\\star)$.\n2. Clip entries of $P$ into $[\\varepsilon, 1]$ and renormalize rows if necessary to maintain row sums at $1$ (for the provided test cases, rows already sum to $1$).\n3. Compute $\\bar{\\mathbf{p}} = \\frac{1}{S} \\sum_{s=1}^S \\mathbf{q}^{(s)}(x^\\star)$.\n4. Compute $\\widehat{H}_{\\text{pred}} = -\\sum_{k=1}^K \\bar{p}_k \\log \\bar{p}_k$.\n5. For each $s$, compute $H^{(s)} = -\\sum_{k=1}^K q^{(s)}_k \\log q^{(s)}_k$ and then $\\widehat{\\mathbb{E}}[H] = \\frac{1}{S} \\sum_{s=1}^S H^{(s)}$.\n6. Output $\\widehat{I} = \\widehat{H}_{\\text{pred}} - \\widehat{\\mathbb{E}}[H]$ in nats.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef mutual_information_mc(prob_matrix: np.ndarray, eps: float = 1e-12) - float:\n    \"\"\"\n    Estimate I(y*, w | x*, D) for a classification BNN using Monte Carlo samples.\n    prob_matrix: shape (S, K), rows are class-probability vectors from posterior samples.\n    eps: small positive constant for numerical stability when taking logs.\n    Returns MI in nats as a float.\n    \"\"\"\n    # Ensure input is float64 for numerical stability\n    P = np.array(prob_matrix, dtype=np.float64)\n\n    # Clip probabilities to avoid log(0); rows should already sum to 1 in test cases.\n    P = np.clip(P, eps, 1.0)\n    # Renormalize rows in case clipping perturbed sums slightly\n    row_sums = P.sum(axis=1, keepdims=True)\n    # Avoid division by zero, though with eps clipping this shouldn't happen\n    row_sums = np.maximum(row_sums, eps)\n    P = P / row_sums\n\n    # Number of samples S and classes K\n    S, K = P.shape\n\n    # Predictive probabilities: average over samples\n    p_bar = P.mean(axis=0)\n\n    # Entropy of predictive distribution (in nats)\n    H_pred = -np.sum(p_bar * np.log(p_bar))\n\n    # Conditional entropy for each sample\n    H_cond_samples = -np.sum(P * np.log(P), axis=1)\n    H_cond_avg = H_cond_samples.mean()\n\n    # Mutual information estimate\n    MI = H_pred - H_cond_avg\n    return float(MI)\n\n\ndef solve():\n    # Define epsilon for numerical stability\n    eps = 1e-12\n\n    # Construct test cases as per the problem statement.\n    # Case 1: Uniform, identical predictions (S=5, K=3)\n    case1 = np.array([[1/3, 1/3, 1/3]] * 5, dtype=np.float64)\n\n    # Case 2: Deterministic disagreement across samples (S=2, K=3)\n    case2 = np.array([[1.0, 0.0, 0.0],\n                      [0.0, 1.0, 0.0]], dtype=np.float64)\n\n    # Case 3: Identical deterministic predictions (S=3, K=4)\n    case3 = np.array([[0.0, 0.0, 1.0, 0.0]] * 3, dtype=np.float64)\n\n    # Case 4: Moderate variation across samples (S=4, K=3)\n    case4 = np.array([[0.7, 0.2, 0.1],\n                      [0.6, 0.3, 0.1],\n                      [0.3, 0.4, 0.3],\n                      [0.2, 0.7, 0.1]], dtype=np.float64)\n\n    # Case 5: Extreme probabilities and one balanced sample (S=3, K=2)\n    tiny = 1e-12\n    case5 = np.array([[tiny, 1.0 - tiny],\n                      [1.0 - tiny, tiny],\n                      [0.5, 0.5]], dtype=np.float64)\n\n    # Case 6: Degenerate single-class scenario (S=3, K=1)\n    case6 = np.array([[1.0], [1.0], [1.0]], dtype=np.float64)\n\n    test_cases = [case1, case2, case3, case4, case5, case6]\n\n    results = []\n    for P in test_cases:\n        result = mutual_information_mc(P, eps=eps)\n        results.append(result)\n\n    # Final print statement in the exact required format with 6 decimal digits.\n    print(\"[\" + \",\".join(f\"{r:.6f}\" for r in results) + \"]\")\n\nsolve()\n```"
        }
    ]
}