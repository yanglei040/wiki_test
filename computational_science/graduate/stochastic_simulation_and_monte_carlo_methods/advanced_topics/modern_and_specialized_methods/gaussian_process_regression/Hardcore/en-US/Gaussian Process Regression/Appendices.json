{
    "hands_on_practices": [
        {
            "introduction": "To truly grasp Gaussian Processes, it's helpful to move beyond the abstract definition and see how a function can be constructed from its statistical properties. This exercise guides you through the Karhunen–Loève expansion, which is analogous to a Fourier series but for a stochastic process. By deriving the spectral components for a canonical example, you will gain a concrete understanding of how a GP can be represented as a weighted sum of basis functions, reinforcing the connection between the covariance kernel and the shape of the resulting functions .",
            "id": "3309595",
            "problem": "Consider a zero-mean Gaussian process (GP) $f$ on the compact domain $[0,1]$ equipped with the Lebesgue measure, with a continuous, symmetric, positive semi-definite covariance kernel $k(x,x')$. Define the associated Hilbert–Schmidt integral operator $\\mathcal{T}:L^{2}([0,1])\\to L^{2}([0,1])$ by\n$$\n(\\mathcal{T}g)(x) \\equiv \\int_{0}^{1} k(x,t)\\, g(t)\\, dt,\n$$\nwhich is self-adjoint, positive, and compact. Let $\\{(\\lambda_{j},\\phi_{j})\\}_{j\\geq 1}$ be the eigenpairs of $\\mathcal{T}$, where the eigenfunctions $\\{\\phi_{j}\\}_{j\\geq 1}$ form an orthonormal basis of $L^{2}([0,1])$ and the eigenvalues satisfy $\\lambda_{j}\\geq 0$ with $\\sum_{j\\geq 1}\\lambda_{j}\\infty$. The Mercer decomposition then states that $k(x,x')$ admits the uniformly convergent series $k(x,x')=\\sum_{j\\geq 1}\\lambda_{j}\\phi_{j}(x)\\phi_{j}(x')$.\n\nDefine the truncated Karhunen–Loève (KL) expansion for $f$ by\n$$\nf_{M}(x) \\equiv \\sum_{j=1}^{M} \\sqrt{\\lambda_{j}}\\, \\xi_{j}\\, \\phi_{j}(x),\n$$\nwhere $\\{\\xi_{j}\\}_{j\\geq 1}$ are independent and identically distributed standard normal random variables, $\\xi_{j}\\sim \\mathcal{N}(0,1)$. Let the truncation error be the mean-square $L^{2}([0,1])$ error $\\mathbb{E}\\big[\\|f-f_{M}\\|_{L^{2}}^{2}\\big]$, where $\\|g\\|_{L^{2}}^{2}=\\int_{0}^{1} g(x)^{2}\\, dx$.\n\nSpecialize to the kernel $k(x,x')=\\min(x,x')$ on $[0,1]$. Starting from the spectral definition above and without assuming any shortcut formulas, derive the eigenpairs $(\\lambda_{j},\\phi_{j})$, construct $f_{M}(x)$, and then compute the exact analytic expression for the truncation error $\\mathbb{E}\\big[\\|f-f_{M}\\|_{L^{2}}^{2}\\big]$ as a function of $M$. You may express your final result using classical special functions, provided they are explicitly defined in your derivation.\n\nNo numerical approximation is required; provide the final answer as a single closed-form analytic expression. If you introduce any acronyms (for example, Monte Carlo (MC)), spell out their full names on first use.",
            "solution": "The primary goal is to compute the truncation error $\\mathbb{E}\\big[\\|f-f_{M}\\|_{L^{2}}^{2}\\big]$. This quantity depends on the eigenvalues of the integral operator $\\mathcal{T}$. Thus, the first step is to solve the eigenvalue problem for the specified kernel.\n\n**Part 1: Derivation of the Eigenpairs $(\\lambda_j, \\phi_j)$**\n\nThe eigenvalue problem for the integral operator $\\mathcal{T}$ is given by the Fredholm integral equation of the second kind:\n$$ (\\mathcal{T}\\phi)(x) = \\lambda \\phi(x) $$\nSubstituting the kernel $k(x,t) = \\min(x,t)$, we have:\n$$ \\int_{0}^{1} \\min(x,t) \\phi(t) dt = \\lambda \\phi(x) $$\nWe can split the integral based on the definition of the minimum function:\n$$ \\int_{0}^{x} t \\phi(t) dt + \\int_{x}^{1} x \\phi(t) dt = \\lambda \\phi(x) $$\nAssuming $\\phi$ is sufficiently smooth, we can differentiate this equation with respect to $x$ using the Leibniz integral rule. The first differentiation yields:\n$$ \\frac{d}{dx}\\left(\\int_{0}^{x} t \\phi(t) dt\\right) + \\frac{d}{dx}\\left(x \\int_{x}^{1} \\phi(t) dt\\right) = \\lambda \\phi'(x) $$\n$$ x\\phi(x) + \\left(1 \\cdot \\int_{x}^{1} \\phi(t) dt + x \\cdot (-\\phi(x))\\right) = \\lambda \\phi'(x) $$\n$$ x\\phi(x) + \\int_{x}^{1} \\phi(t) dt - x\\phi(x) = \\lambda \\phi'(x) $$\n$$ \\int_{x}^{1} \\phi(t) dt = \\lambda \\phi'(x) $$\nDifferentiating a second time with respect to $x$:\n$$ -\\phi(x) = \\lambda \\phi''(x) $$\nThis gives the second-order ordinary differential equation (ODE):\n$$ \\phi''(x) + \\frac{1}{\\lambda} \\phi(x) = 0 $$\nThe general solution to this ODE is $\\phi(x) = A \\sin(\\frac{x}{\\sqrt{\\lambda}}) + B \\cos(\\frac{x}{\\sqrt{\\lambda}})$.\n\nTo find the constants $A$ and $B$ and the eigenvalues $\\lambda$, we must establish boundary conditions.\n1.  From the original integral equation, setting $x=0$:\n    $$ \\int_{0}^{1} \\min(0,t)\\phi(t)dt = \\lambda \\phi(0) \\implies \\int_{0}^{1} 0 \\cdot \\phi(t) dt = 0 = \\lambda \\phi(0) $$\n    For a non-trivial eigensystem, we seek $\\lambda \\neq 0$, which implies $\\phi(0) = 0$.\n2.  From the once-differentiated equation, setting $x=1$:\n    $$ \\int_{1}^{1} \\phi(t) dt = \\lambda \\phi'(1) \\implies 0 = \\lambda \\phi'(1) $$\n    Again, for $\\lambda \\neq 0$, this implies $\\phi'(1) = 0$.\n\nNow, we apply these boundary conditions to the general solution:\n-   At $x=0$, $\\phi(0) = A \\sin(0) + B \\cos(0) = B$. Since $\\phi(0)=0$, we have $B=0$. The solution simplifies to $\\phi(x) = A \\sin(\\frac{x}{\\sqrt{\\lambda}})$.\n-   The derivative is $\\phi'(x) = \\frac{A}{\\sqrt{\\lambda}} \\cos(\\frac{x}{\\sqrt{\\lambda}})$. At $x=1$, we must have $\\phi'(1) = 0$:\n    $$ \\frac{A}{\\sqrt{\\lambda}} \\cos\\left(\\frac{1}{\\sqrt{\\lambda}}\\right) = 0 $$\n    For a non-trivial eigenfunction, $A \\neq 0$. Thus, we require $\\cos(\\frac{1}{\\sqrt{\\lambda}}) = 0$. This condition is met when the argument is an odd multiple of $\\frac{\\pi}{2}$:\n    $$ \\frac{1}{\\sqrt{\\lambda_j}} = \\frac{(2j-1)\\pi}{2}, \\quad \\text{for } j = 1, 2, 3, \\dots $$\n    Solving for the eigenvalues $\\lambda_j$:\n    $$ \\lambda_j = \\frac{4}{(2j-1)^2 \\pi^2} $$\nThe corresponding eigenfunctions are of the form $\\phi_j(x) = A_j \\sin\\left(\\frac{(2j-1)\\pi}{2} x\\right)$. We determine the constant $A_j$ by the normalization condition $\\|\\phi_j\\|_{L^2}^2 = 1$:\n$$ \\int_{0}^{1} \\phi_j(x)^2 dx = A_j^2 \\int_{0}^{1} \\sin^2\\left(\\frac{(2j-1)\\pi}{2} x\\right) dx = 1 $$\nUsing the identity $\\sin^2(\\theta) = \\frac{1 - \\cos(2\\theta)}{2}$:\n$$ \\int_{0}^{1} \\frac{1}{2}\\left(1 - \\cos((2j-1)\\pi x)\\right) dx = \\frac{1}{2} \\left[ x - \\frac{\\sin((2j-1)\\pi x)}{(2j-1)\\pi} \\right]_{0}^{1} $$\n$$ = \\frac{1}{2} \\left( (1 - \\frac{\\sin((2j-1)\\pi)}{(2j-1)\\pi}) - (0-0) \\right) = \\frac{1}{2}(1-0) = \\frac{1}{2} $$\nSo, $A_j^2 \\cdot \\frac{1}{2} = 1$, which gives $A_j = \\sqrt{2}$. The orthonormal eigenfunctions are:\n$$ \\phi_j(x) = \\sqrt{2} \\sin\\left(\\frac{(2j-1)\\pi}{2} x\\right) $$\n\n**Part 2: Calculation of the Truncation Error**\n\nThe Karhunen-Loève (KL) expansion of the process $f(x)$ is given by $f(x) = \\sum_{j=1}^{\\infty} \\sqrt{\\lambda_j} \\xi_j \\phi_j(x)$. The truncated expansion is $f_M(x) = \\sum_{j=1}^{M} \\sqrt{\\lambda_j} \\xi_j \\phi_j(x)$.\nThe difference is the residual process:\n$$ f(x) - f_M(x) = \\sum_{j=M+1}^{\\infty} \\sqrt{\\lambda_j} \\xi_j \\phi_j(x) $$\nThe squared $L^2$-norm of this difference is:\n$$ \\|f - f_M\\|_{L^2}^2 = \\int_0^1 \\left( \\sum_{j=M+1}^{\\infty} \\sqrt{\\lambda_j} \\xi_j \\phi_j(x) \\right) \\left( \\sum_{k=M+1}^{\\infty} \\sqrt{\\lambda_k} \\xi_k \\phi_k(x) \\right) dx $$\nBy swapping the summation and integration (justified by the uniform convergence of the expansion):\n$$ \\|f - f_M\\|_{L^2}^2 = \\sum_{j=M+1}^{\\infty} \\sum_{k=M+1}^{\\infty} \\sqrt{\\lambda_j \\lambda_k} \\xi_j \\xi_k \\int_0^1 \\phi_j(x) \\phi_k(x) dx $$\nSince the eigenfunctions $\\{\\phi_j\\}$ are orthonormal, we have $\\int_0^1 \\phi_j(x) \\phi_k(x) dx = \\delta_{jk}$, where $\\delta_{jk}$ is the Kronecker delta. The double sum collapses to a single sum:\n$$ \\|f - f_M\\|_{L^2}^2 = \\sum_{j=M+1}^{\\infty} \\lambda_j \\xi_j^2 $$\nThe truncation error is the expectation of this quantity. Using the linearity of expectation:\n$$ \\mathbb{E}\\big[\\|f - f_M\\|_{L^2}^2\\big] = \\mathbb{E}\\left[\\sum_{j=M+1}^{\\infty} \\lambda_j \\xi_j^2\\right] = \\sum_{j=M+1}^{\\infty} \\lambda_j \\mathbb{E}[\\xi_j^2] $$\nThe variables $\\xi_j$ are independent and identically distributed standard normal, $\\xi_j \\sim \\mathcal{N}(0,1)$. For such a variable, $\\mathbb{E}[\\xi_j] = 0$ and $\\text{Var}(\\xi_j) = 1$. The variance is $\\text{Var}(\\xi_j) = \\mathbb{E}[\\xi_j^2] - (\\mathbb{E}[\\xi_j])^2$, so $1 = \\mathbb{E}[\\xi_j^2] - 0^2$, which gives $\\mathbb{E}[\\xi_j^2] = 1$.\nTherefore, the truncation error is simply the sum of the eigenvalues of the truncated modes:\n$$ \\mathbb{E}\\big[\\|f - f_M\\|_{L^2}^2\\big] = \\sum_{j=M+1}^{\\infty} \\lambda_j $$\n\n**Part 3: Evaluation of the Eigenvalue Sum**\n\nWe now substitute the derived eigenvalues $\\lambda_j = \\frac{4}{(2j-1)^2 \\pi^2}$ into the sum:\n$$ \\mathbb{E}\\big[\\|f - f_M\\|_{L^2}^2\\big] = \\sum_{j=M+1}^{\\infty} \\frac{4}{(2j-1)^2 \\pi^2} = \\frac{4}{\\pi^2} \\sum_{j=M+1}^{\\infty} \\frac{1}{(2j-1)^2} $$\nTo evaluate this sum, we use the trigamma function, which is a special function defined as the second derivative of the logarithm of the gamma function, $\\psi_1(z) = \\frac{d^2}{dz^2} \\ln \\Gamma(z)$. It has a series representation:\n$$ \\psi_1(z) = \\sum_{n=0}^{\\infty} \\frac{1}{(z+n)^2} $$\nLet's rewrite our summation to match this form.\n$$ \\sum_{j=M+1}^{\\infty} \\frac{1}{(2j-1)^2} = \\frac{1}{4} \\sum_{j=M+1}^{\\infty} \\frac{1}{(j - 1/2)^2} $$\nLet the index of summation be $n=j-(M+1)$, so $j = n+M+1$. When $j=M+1$, $n=0$. The sum becomes:\n$$ \\frac{1}{4} \\sum_{n=0}^{\\infty} \\frac{1}{((n+M+1) - 1/2)^2} = \\frac{1}{4} \\sum_{n=0}^{\\infty} \\frac{1}{(n + (M+1/2))^2} $$\nThis perfectly matches the series definition of the trigamma function with argument $z = M+\\frac{1}{2}$. Thus:\n$$ \\sum_{j=M+1}^{\\infty} \\frac{1}{(2j-1)^2} = \\frac{1}{4} \\psi_1\\left(M+\\frac{1}{2}\\right) $$\nSubstituting this back into the expression for the truncation error:\n$$ \\mathbb{E}\\big[\\|f - f_M\\|_{L^2}^2\\big] = \\frac{4}{\\pi^2} \\left( \\frac{1}{4} \\psi_1\\left(M+\\frac{1}{2}\\right) \\right) = \\frac{1}{\\pi^2} \\psi_1\\left(M+\\frac{1}{2}\\right) $$\nThis is the final analytic expression for the truncation error.",
            "answer": "$$ \\boxed{ \\frac{1}{\\pi^2} \\psi_1\\left(M+\\frac{1}{2}\\right) } $$"
        },
        {
            "introduction": "While elegant in theory, applying Gaussian Processes in practice often reveals a critical challenge: numerical instability. When input points are too close, the resulting covariance matrix can become nearly singular, making standard computations like Cholesky factorization fail. This practice places you in a realistic scenario of ill-conditioning and asks you to diagnose the issue quantitatively and evaluate common remedies, honing essential skills for building robust and reliable GP models .",
            "id": "3309568",
            "problem": "Consider Gaussian process regression with a zero-mean prior and a positive definite covariance function $k(x,x')$ on inputs $\\{x_i\\}_{i=1}^n \\subset \\mathbb{R}^d$. Observations are modeled as $y_i = f(x_i) + \\epsilon_i$ with $f \\sim \\mathcal{GP}(0,k)$ and independent Gaussian noise $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_n^2)$. The Gram matrix is $K \\in \\mathbb{R}^{n \\times n}$ with entries $K_{ij} = k(x_i,x_j)$, and the data Gram matrix is $K_y = K + \\sigma_n^2 I_n$. For symmetric positive definite matrices, the spectral condition number in the $2$-norm is defined by $\\kappa_2(A) = \\lambda_{\\max}(A)/\\lambda_{\\min}(A)$, where $\\lambda_{\\max}(A)$ and $\\lambda_{\\min}(A)$ denote the largest and smallest eigenvalues of $A$, respectively. Assume double-precision machine epsilon satisfies $\\varepsilon_{\\mathrm{mach}} \\approx 2.22 \\times 10^{-16}$.\n\nSuppose $n = 300$ inputs are heavily clustered (e.g., several tight clusters separated by moderate gaps), and the covariance function $k$ is the squared exponential $k(x,x') = \\sigma_f^2 \\exp\\!\\big(-\\|x-x'\\|^2/(2\\ell^2)\\big)$ with $\\sigma_f^2 = 1$ and a short length-scale $\\ell$. A practitioner computes the extremal eigenvalues of $K$ and finds $\\lambda_{\\max}(K) \\approx 1.2$ and $\\lambda_{\\min}(K) \\approx 10^{-12}$, while the current noise level is $\\sigma_n^2 \\approx 10^{-14}$. The practitioner wishes to diagnose and remedy numerical ill-conditioning issues that affect downstream stochastic simulation, such as sampling from the posterior $\\mathcal{N}(m, K_y)$ via Cholesky factorization or Markov chain Monte Carlo (MCMC) exploration of hyperparameters.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. Given the stated eigenvalues and $\\sigma_n^2 \\approx 10^{-14}$, the data Gram matrix $K_y$ has a spectral condition number $\\kappa_2(K_y)$ of order $10^{12}$, which is large enough in double precision to cause loss of accuracy and instability in factorizations used by stochastic simulation.\n\nB. Increasing the noise level by adding a jitter $\\delta I_n$ (equivalently replacing $\\sigma_n^2$ by $\\sigma_n^2 + \\delta$) strictly decreases $\\kappa_2(K_y)$ as a function of $\\delta$. Under the given eigenvalues, the smallest $\\delta$ that achieves $\\kappa_2(K_y) \\leq 10^8$ is approximately $1.2 \\times 10^{-8}$.\n\nC. Tapering the kernel by multiplying $k(x,x')$ with a compactly supported function that equals $1$ at $x=x'$ but is $1$ otherwise always decreases $\\kappa_2(K_y)$ and leaves the predictive mean at training inputs unchanged.\n\nD. Replacing $K$ by a low-rank approximation $U U^\\top$ with $U \\in \\mathbb{R}^{n \\times m}$ and $m \\ll n$ (e.g., via the Nyström method or pivoted Cholesky), and then using $U U^\\top + \\sigma_n^2 I_n$, can improve conditioning; a principled truncation rule is to monitor the discarded spectrum and choose $m$ so that the sum of omitted eigenvalues is below a tolerance proportional to $\\varepsilon_{\\mathrm{mach}}$ times $\\operatorname{trace}(K)$.\n\nE. For the squared exponential kernel, rescaling inputs by $x_i \\mapsto a x_i$ while keeping $\\ell$ fixed preserves the spectral condition number of $K_y$ for any $a  0$.\n\nF. In MCMC for hyperparameter inference, an ill-conditioned $K_y$ typically improves mixing because the log marginal likelihood becomes flatter near near-singular configurations, reducing autocorrelation in the chain.",
            "solution": "The core of the problem lies in the properties of the data Gram matrix $K_y = K + \\sigma_n^2 I_n$. The eigenvalues of $K_y$ are $\\lambda_i(K_y) = \\lambda_i(K) + \\sigma_n^2$, since adding a multiple of the identity matrix simply shifts the spectrum. The spectral condition number is therefore:\n$$ \\kappa_2(K_y) = \\frac{\\lambda_{\\max}(K_y)}{\\lambda_{\\min}(K_y)} = \\frac{\\lambda_{\\max}(K) + \\sigma_n^2}{\\lambda_{\\min}(K) + \\sigma_n^2} $$\nWe are given $\\lambda_{\\max}(K) \\approx 1.2$, $\\lambda_{\\min}(K) \\approx 10^{-12}$, and $\\sigma_n^2 \\approx 10^{-14}$.\n\n### Option-by-Option Analysis\n\n**A. Given the stated eigenvalues and $\\sigma_n^2 \\approx 10^{-14}$, the data Gram matrix $K_y$ has a spectral condition number $\\kappa_2(K_y)$ of order $10^{12}$, which is large enough in double precision to cause loss of accuracy and instability in factorizations used by stochastic simulation.**\n\nLet's compute the condition number with the given values:\n$$ \\lambda_{\\max}(K_y) = \\lambda_{\\max}(K) + \\sigma_n^2 \\approx 1.2 + 10^{-14} \\approx 1.2 $$\n$$ \\lambda_{\\min}(K_y) = \\lambda_{\\min}(K) + \\sigma_n^2 \\approx 10^{-12} + 10^{-14} = 10^{-12} + 0.01 \\times 10^{-12} = 1.01 \\times 10^{-12} $$\nThe condition number is:\n$$ \\kappa_2(K_y) = \\frac{\\lambda_{\\max}(K_y)}{\\lambda_{\\min}(K_y)} \\approx \\frac{1.2}{1.01 \\times 10^{-12}} \\approx 1.19 \\times 10^{12} $$\nThis value is of the order $10^{12}$. In double-precision arithmetic, where $\\varepsilon_{\\mathrm{mach}} \\approx 10^{-16}$, a general rule of thumb is that one can expect to lose about $\\log_{10}(\\kappa_2)$ digits of accuracy when solving a linear system. Here, $\\log_{10}(1.19 \\times 10^{12}) \\approx 12.08$. Given about $16$ available digits of precision, this leaves only about $16 - 12 = 4$ significant digits, which constitutes a severe loss of accuracy. This level of ill-conditioning makes standard algorithms like Cholesky factorization numerically unstable. The statement is accurate.\n\n**Verdict: Correct**\n\n**B. Increasing the noise level by adding a jitter $\\delta I_n$ (equivalently replacing $\\sigma_n^2$ by $\\sigma_n^2 + \\delta$) strictly decreases $\\kappa_2(K_y)$ as a function of $\\delta$. Under the given eigenvalues, the smallest $\\delta$ that achieves $\\kappa_2(K_y) \\leq 10^8$ is approximately $1.2 \\times 10^{-8}$.**\n\nLet the new condition number be a function of the added jitter $\\delta  0$:\n$$ \\kappa(\\delta) = \\frac{\\lambda_{\\max}(K) + \\sigma_n^2 + \\delta}{\\lambda_{\\min}(K) + \\sigma_n^2 + \\delta} $$\nTo check if this function is strictly decreasing, we examine its derivative with respect to $\\delta$. Let $a = \\lambda_{\\max}(K) + \\sigma_n^2$ and $b = \\lambda_{\\min}(K) + \\sigma_n^2$.\n$$ \\frac{d\\kappa}{d\\delta} = \\frac{d}{d\\delta}\\left(\\frac{a + \\delta}{b + \\delta}\\right) = \\frac{1(b+\\delta) - 1(a+\\delta)}{(b+\\delta)^2} = \\frac{b-a}{(b+\\delta)^2} $$\nSince $K$ is positive definite and not a scalar multiple of the identity, $\\lambda_{\\max}(K)  \\lambda_{\\min}(K)$, which implies $a  b$. Thus, $b-a  0$, and the derivative is strictly negative for all $\\delta \\ge 0$. So, $\\kappa(\\delta)$ is a strictly decreasing function of $\\delta$.\n\nNow, we find the smallest $\\delta$ that achieves $\\kappa_2(K_y) \\le 10^8$. Since the function is decreasing, this occurs at the boundary:\n$$ \\frac{\\lambda_{\\max}(K) + \\sigma_n^2 + \\delta}{\\lambda_{\\min}(K) + \\sigma_n^2 + \\delta} = 10^8 $$\nUsing the given values:\n$$ \\frac{1.2 + 10^{-14} + \\delta}{10^{-12} + 10^{-14} + \\delta} = 10^8 $$\n$$ 1.2 + 10^{-14} + \\delta = 10^8 (1.01 \\times 10^{-12} + \\delta) = 1.01 \\times 10^{-4} + 10^8 \\delta $$\n$$ 1.2 - 1.01 \\times 10^{-4} \\approx (10^8 - 1) \\delta $$\n$$ \\delta \\approx \\frac{1.2}{10^8} = 1.2 \\times 10^{-8} $$\nThe calculation confirms the approximate value. Both parts of the statement are accurate.\n\n**Verdict: Correct**\n\n**C. Tapering the kernel by multiplying $k(x,x')$ with a compactly supported function that equals $1$ at $x=x'$ but is $1$ otherwise always decreases $\\kappa_2(K_y)$ and leaves the predictive mean at training inputs unchanged.**\n\nThis statement makes two claims. Let's analyze the second one first: that tapering \"leaves the predictive mean at training inputs unchanged\". The predictive mean at the training inputs $\\{x_i\\}_{i=1}^n$ is given by the vector $\\mathbf{m}_{\\text{train}} = K (K_y)^{-1} \\mathbf{y} = K (K + \\sigma_n^2 I_n)^{-1} \\mathbf{y}$. This can be rewritten as $\\mathbf{m}_{\\text{train}} = (I_n - \\sigma_n^2 (K + \\sigma_n^2 I_n)^{-1})\\mathbf{y}$. Tapering replaces the kernel matrix $K$ with a new matrix $K'$, where $K'_{ij} = K_{ij} T_{ij}$ for a tapering matrix $T$ with $T_{ii}=1$ and $|T_{ij}| \\le 1$ for $i \\ne j$. Since $T$ is not the matrix of all ones, $K' \\neq K$. The predictive mean after tapering is $\\mathbf{m}'_{\\text{train}} = K' (K' + \\sigma_n^2 I_n)^{-1} \\mathbf{y}$. As the entire expression for the mean depends on the full matrix $K$, changing it to $K'$ will, in general, change the predictive mean. The predictive mean is not unchanged. Since a part of the statement is false, the entire statement is incorrect.\n\n**Verdict: Incorrect**\n\n**D. Replacing $K$ by a low-rank approximation $U U^\\top$ with $U \\in \\mathbb{R}^{n \\times m}$ and $m \\ll n$ (e.g., via the Nyström method or pivoted Cholesky), and then using $U U^\\top + \\sigma_n^2 I_n$, can improve conditioning; a principled truncation rule is to monitor the discarded spectrum and choose $m$ so that the sum of omitted eigenvalues is below a tolerance proportional to $\\varepsilon_{\\mathrm{mach}}$ times $\\operatorname{trace}(K)$.**\n\nGaussian process approximations based on low-rank methods are a standard technique to improve scalability and numerical stability. The inversion of the $n \\times n$ matrix $K_y = UU^\\top + \\sigma_n^2 I_n$ is performed efficiently using the Woodbury matrix identity:\n$$ (UU^\\top + \\sigma_n^2 I_n)^{-1} = \\frac{1}{\\sigma_n^2}I_n - \\frac{1}{\\sigma_n^2} U \\left( \\sigma_n^2 I_m + U^\\top U \\right)^{-1} U^\\top $$\nThis requires the inversion of an $m \\times m$ matrix, $\\sigma_n^2 I_m + U^\\top U$. In methods like Nyström, the eigenvalues of $U^\\top U$ correspond to the largest eigenvalues of $K$. The smallest eigenvalue involved in the $m \\times m$ inversion will be much larger than $\\lambda_{\\min}(K)$, leading to a much better-conditioned matrix inversion problem. Therefore, these methods improve conditioning by replacing the ill-conditioned $n \\times n$ problem with a well-conditioned $m \\times m$ one.\n\nThe proposed truncation rule is to choose the rank $m$ such that the sum of the discarded eigenvalues, $\\sum_{i=m+1}^n \\lambda_i(K)$, is small relative to the total sum of eigenvalues, $\\operatorname{trace}(K) = \\sum_{i=1}^n \\lambda_i(K)$. Setting the tolerance for this relative error to be on the order of machine epsilon, i.e., error $\\le C \\cdot \\varepsilon_{\\mathrm{mach}} \\operatorname{trace}(K)$, is a standard and principled heuristic in numerical linear algebra for creating approximations that are accurate up to machine precision. The statement is accurate.\n\n**Verdict: Correct**\n\n**E. For the squared exponential kernel, rescaling inputs by $x_i \\mapsto a x_i$ while keeping $\\ell$ fixed preserves the spectral condition number of $K_y$ for any $a  0$.**\n\nLet the original kernel be $k(x,x') = \\sigma_f^2 \\exp(-\\|x-x'\\|^2 / (2\\ell^2))$. After rescaling the inputs to $z_i = a x_i$, the new kernel matrix $K'$ has entries:\n$$ K'_{ij} = k(z_i, z_j) = \\sigma_f^2 \\exp(-\\|z_i-z_j\\|^2 / (2\\ell^2)) = \\sigma_f^2 \\exp(-\\|a(x_i-x_j)\\|^2 / (2\\ell^2)) = \\sigma_f^2 \\exp(-a^2\\|x_i-x_j\\|^2 / (2\\ell^2)) $$\nThis is equivalent to using the original inputs with a new length-scale $\\ell' = \\ell/a$. The Gram matrix $K$ is highly sensitive to the length-scale parameter. For example, as $\\ell \\to \\infty$, $K \\to \\sigma_f^2 \\mathbf{1}\\mathbf{1}^\\top$ (a rank-1 matrix), which is very ill-conditioned. As $\\ell \\to 0$, $K \\to \\sigma_f^2 I_n$, which is perfectly conditioned ($\\kappa_2(K)=1$). Since rescaling the inputs is equivalent to changing the length-scale, and the spectrum of $K$ (and thus $\\kappa_2(K)$ and $\\kappa_2(K_y)$) depends strongly on the length-scale, the condition number is not preserved.\n\n**Verdict: Incorrect**\n\n**F. In MCMC for hyperparameter inference, an ill-conditioned $K_y$ typically improves mixing because the log marginal likelihood becomes flatter near near-singular configurations, reducing autocorrelation in the chain.**\n\nThe log marginal likelihood (LML) for GP regression is given by $\\log p(\\mathbf{y}|\\theta) = -\\frac{1}{2}\\mathbf{y}^\\top K_y^{-1} \\mathbf{y} - \\frac{1}{2}\\log|K_y| - \\frac{n}{2}\\log(2\\pi)$. When $K_y$ becomes ill-conditioned, its smallest eigenvalue approaches zero, causing its determinant $|K_y| = \\prod_i \\lambda_i(K_y)$ to approach zero. Consequently, $\\log|K_y|$ approaches $-\\infty$. This creates extremely steep gradients and sharp, narrow valleys or ridges in the LML surface in the space of hyperparameters $\\theta$. Such topologies are notoriously difficult for MCMC samplers to explore. Samplers tend to get stuck in these regions, leading to very slow mixing and high autocorrelation in the chain. The LML surface becomes more rugged and \"spiky,\" not flatter. Therefore, ill-conditioning severely hinders MCMC performance, it does not improve it.\n\n**Verdict: Incorrect**",
            "answer": "$$\\boxed{ABD}$$"
        },
        {
            "introduction": "The infamous cubic complexity of standard GP regression is its greatest practical limitation, but this bottleneck is not always unavoidable. This exercise explores a powerful class of methods for scaling up GPs by exploiting specific data structures. You will investigate how stationary kernels on regular grids lead to Toeplitz covariance matrices, unlocking massive computational speedups via the Fast Fourier Transform (FFT) and iterative solvers, a cornerstone of scalable GP modeling .",
            "id": "3309545",
            "problem": "Consider a one-dimensional grid of input locations $x_i = i\\Delta$ for $i = 0, 1, \\ldots, n-1$ where $\\Delta  0$ is fixed. Let $k(x, x') = \\kappa(|x - x'|)$ be a stationary covariance function, and define the covariance matrix $K \\in \\mathbb{R}^{n \\times n}$ by $K_{ij} = k(x_i, x_j)$. Suppose one wishes to compute the Gaussian process regression quantities involving the linear system $(K + \\sigma^2 I)\\alpha = y$ for a given vector $y \\in \\mathbb{R}^n$ and noise level $\\sigma^2  0$, as well as approximate the quantity $\\log\\det(K + \\sigma^2 I)$, without forming dense matrix factorizations.\n\nFrom first principles, such as the definition of translation invariance for stationary kernels, the definition of Toeplitz and circulant matrices, properties of discrete convolution, and the fact that the Discrete Fourier Transform (DFT) diagonalizes circulant matrices, reason about which of the following statements are correct.\n\nA. Because $k(x, x')$ depends only on $|x - x'|$ and the inputs lie on an evenly spaced grid, the matrix $K$ has a Toeplitz structure, meaning $K_{ij}$ depends only on $|i-j|$ so that all entries on the same diagonal are equal.\n\nB. For any $n$, the Discrete Fourier Transform (DFT) matrix $F \\in \\mathbb{C}^{n \\times n}$ diagonalizes the Toeplitz matrix $K$ exactly, i.e., $F^* K F$ is diagonal, so $K$ can be inverted by a single forward and inverse Fast Fourier Transform (FFT).\n\nC. A matrix-vector product $Kv$ can be implemented in $O(n\\log n)$ time using the Fast Fourier Transform (FFT) by embedding $K$ into a larger circulant matrix $C \\in \\mathbb{R}^{m \\times m}$, zero-padding $v$ to length $m$, and computing the associated circular convolution; truncating to the first $n$ entries yields the desired product with $K$.\n\nD. The linear system $(K + \\sigma^2 I)\\alpha = y$ can be solved exactly in $O(n\\log n)$ time by using FFT-based diagonalization of $K$, independently of $n$ and the choice of $\\kappa$.\n\nE. Using Conjugate Gradients (CG) with FFT-accelerated matrix-vector products with $K$ yields $O(n\\log n)$ cost per iteration for solving $(K + \\sigma^2 I)\\alpha = y$, and with an effective preconditioner the overall cost can scale nearly as $O(n\\log n)$ up to a factor depending on the condition number.\n\nF. If $C$ is a circulant embedding of $K$ constructed from a periodized version of the covariance, then the eigenvalues of $C$ are given by the DFT of the first column of $C$, and $\\log\\det(K + \\sigma^2 I)$ can be approximated by $\\sum_{j=0}^{m-1} \\log\\big(\\lambda_j(C) + \\sigma^2\\big)$; this approximation is accurate when the embedding reflects a valid periodic covariance and the embedding size $m$ is sufficiently large, but is generally biased for nonperiodic covariances.\n\nG. A stochastic trace estimator such as Hutchinson’s method with random Rademacher vectors $z$ satisfies $\\mathbb{E}\\left[z^{\\top}\\log(K + \\sigma^2 I)z\\right] = \\operatorname{trace}\\big(\\log(K + \\sigma^2 I)\\big)$, and combined with Lanczos quadrature and FFT-based matrix-vector products with $K$, it provides an $O(n\\log n)$ per-iteration Monte Carlo method (up to factors for the number of probe vectors and Lanczos steps) for approximating $\\log\\det(K + \\sigma^2 I)$.",
            "solution": "The problem asks for an evaluation of several statements concerning computational methods for Gaussian process regression on a one-dimensional, evenly-spaced grid. The core of the problem lies in the structure of the covariance matrix $K$ derived from a stationary kernel, and how this structure can be exploited for efficient computation.\n\nThe given input locations are $x_i = i\\Delta$ for $i = 0, 1, \\ldots, n-1$. The covariance function is stationary, $k(x, x') = \\kappa(|x - x'|)$. The covariance matrix entries are $K_{ij} = k(x_i, x_j)$. We must analyze the solution of $(K + \\sigma^2 I)\\alpha = y$ and the approximation of $\\log\\det(K + \\sigma^2 I)$.\n\nLet's evaluate each statement from first principles.\n\n**A. Because $k(x, x')$ depends only on $|x - x'|$ and the inputs lie on an evenly spaced grid, the matrix $K$ has a Toeplitz structure, meaning $K_{ij}$ depends only on $|i-j|$ so that all entries on the same diagonal are equal.**\n\nThe entries of the matrix $K$ are given by $K_{ij} = k(x_i, x_j)$. Substituting the grid definition, we have:\n$$K_{ij} = \\kappa(|x_i - x_j|) = \\kappa(|i\\Delta - j\\Delta|) = \\kappa(|i-j|\\Delta)$$\nA matrix $T$ is a Toeplitz matrix if its entries satisfy $T_{ij} = t_{i-j}$ for some sequence $\\{t_k\\}$. This means that the entries are constant along any diagonal. A diagonal is defined by a constant value of the difference in indices, $j-i = d$.\nFor our matrix $K$, consider a diagonal defined by $j-i = d$. An entry on this diagonal is $K_{i, i+d}$. Its value is:\n$$K_{i, i+d} = \\kappa(|i - (i+d)|\\Delta) = \\kappa(|-d|\\Delta) = \\kappa(|d|\\Delta)$$\nSince this value depends only on the diagonal index $d$ and not on the specific row index $i$, all entries on the same diagonal are indeed equal. Therefore, $K$ is a Toeplitz matrix. Furthermore, since $K_{ij} = \\kappa(|i-j|\\Delta)$ and $K_{ji} = \\kappa(|j-i|\\Delta) = \\kappa(|i-j|\\Delta)$, we have $K_{ij} = K_{ji}$, so $K$ is a symmetric Toeplitz matrix. The statement that $K_{ij}$ depends only on $|i-j|$ is correct from our derivation, and this directly implies the Toeplitz structure described.\n\n**Verdict:** Correct.\n\n**B. For any $n$, the Discrete Fourier Transform (DFT) matrix $F \\in \\mathbb{C}^{n \\times n}$ diagonalizes the Toeplitz matrix $K$ exactly, i.e., $F^* K F$ is diagonal, so $K$ can be inverted by a single forward and inverse Fast Fourier Transform (FFT).**\n\nThe Discrete Fourier Transform (DFT) matrix $F$ is known to diagonalize all circulant matrices. A matrix $C \\in \\mathbb{R}^{n \\times n}$ is circulant if its rows are cyclic shifts of the first row, i.e., $C_{ij} = c_{(j-i) \\pmod n}$. In general, a Toeplitz matrix is not circulant.\nFor our Toeplitz matrix $K$ to be circulant, we would require $K_{ij}$ to be a function of $(j-i) \\pmod n$. Let's check this. We have $K_{ij} = \\kappa(|i-j|\\Delta)$. Consider the entry $K_{0, n-1}$. For a circulant matrix, this should be equal to $K_{1,0}$.\n$$K_{0, n-1} = \\kappa(|0 - (n-1)|\\Delta) = \\kappa((n-1)\\Delta)$$\n$$K_{1,0} = \\kappa(|1-0|\\Delta) = \\kappa(\\Delta)$$\nFor $K$ to be circulant, we would need $\\kappa((n-1)\\Delta) = \\kappa(\\Delta)$. This is not true for general stationary covariance functions, for example, the squared exponential kernel $\\kappa(r) = \\exp(-r^2)$.\nSince $K$ is a general Toeplitz matrix and not necessarily a circulant one, the DFT matrix $F$ does not diagonalize it. The claim that $F^* K F$ is diagonal is false.\n\n**Verdict:** Incorrect.\n\n**C. A matrix-vector product $Kv$ can be implemented in $O(n\\log n)$ time using the Fast Fourier Transform (FFT) by embedding $K$ into a larger circulant matrix $C \\in \\mathbb{R}^{m \\times m}$, zero-padding $v$ to length $m$, and computing the associated circular convolution; truncating to the first $n$ entries yields the desired product with $K$.**\n\nThe $i$-th component of the matrix-vector product $Kv$ is given by $(Kv)_i = \\sum_{j=0}^{n-1} K_{ij} v_j$. Since $K$ is a Toeplitz matrix, we can write $K_{ij} = t_{i-j}$ where $t_k = \\kappa(|k|\\Delta)$. Thus, $(Kv)_i = \\sum_{j=0}^{n-1} t_{i-j} v_j$. This is the mathematical form of a linear convolution.\nThe convolution theorem states that a convolution of two sequences can be computed by element-wise multiplication in the Fourier domain. Specifically, a *circular* convolution $a *_{circ} b$ can be computed as $\\text{IDFT}(\\text{DFT}(a) \\odot \\text{DFT}(b))$, where $\\odot$ is element-wise multiplication.\nTo compute a *linear* convolution using this theorem, one must prevent the \"wrap-around\" effect of the circular convolution. This is achieved by zero-padding the input vectors. If we want to convolve a sequence of length $n$ with a sequence of length $n$, the result has length $2n-1$. Thus, we must pad both vectors to a length $m \\ge 2n-1$.\nThe procedure described in the statement is the standard algorithm:\n1.  Define the circulant matrix $C$ of size $m \\times m$ (where $m \\ge 2n-1$) from the first column of the Toeplitz matrix $K$. Let $c$ be the first column of $C$.\n2.  Zero-pad the vector $v$ to length $m$, creating $v_{pad}$.\n3.  Compute the circular convolution via FFT: $w = \\text{iFFT}(\\text{FFT}(c) \\odot \\text{FFT}(v_{pad}))$.\n4.  The first $n$ entries of $w$ are equal to the desired product $Kv$.\nThe Fast Fourier Transform (FFT) algorithm computes the DFT in $O(m \\log m)$ time. Since $m$ is chosen on the order of $n$ (e.g., $m \\approx 2n$), the overall complexity is $O(n \\log n)$.\n\n**Verdict:** Correct.\n\n**D. The linear system $(K + \\sigma^2 I)\\alpha = y$ can be solved exactly in $O(n\\log n)$ time by using FFT-based diagonalization of $K$, independently of $n$ and the choice of $\\kappa$.**\n\nThis statement claims an exact solution. An exact solution using FFT-based diagonalization would require the matrix $A = K + \\sigma^2 I$ to be diagonalized by the DFT matrix $F$.\nThe matrix $K$ is Toeplitz, and $I$ is also Toeplitz (and circulant). The sum of two Toeplitz matrices is a Toeplitz matrix. Thus, $A = K + \\sigma^2 I$ is a Toeplitz matrix.\nAs established in the analysis of statement B, a general Toeplitz matrix is not diagonalized by the DFT. Therefore, the premise of the proposed solution method is false.\nOne can *approximate* the solution by replacing the Toeplitz matrix $K + \\sigma^2 I$ with a \"close\" circulant matrix $C$, solving $(C + \\sigma^2 I)\\tilde{\\alpha} = y$ in $O(n \\log n)$ time. However, $\\tilde{\\alpha}$ will not be the *exact* solution $\\alpha$, and the quality of this approximation depends on the kernel $\\kappa$ and the size $n$. The claim of an exact solution is incorrect.\n\n**Verdict:** Incorrect.\n\n**E. Using Conjugate Gradients (CG) with FFT-accelerated matrix-vector products with $K$ yields $O(n\\log n)$ cost per iteration for solving $(K + \\sigma^2 I)\\alpha = y$, and with an effective preconditioner the overall cost can scale nearly as $O(n\\log n)$ up to a factor depending on the condition number.**\n\nThe Conjugate Gradient (CG) algorithm is an iterative method for solving linear systems $A\\alpha = y$ where $A$ is symmetric and positive-definite. The matrix $K$ is a covariance matrix, hence symmetric and positive semi-definite. With $\\sigma^2  0$, the matrix $A = K + \\sigma^2 I$ is symmetric and positive-definite, so CG is applicable.\nThe main computational work in each iteration of CG is a single matrix-vector product of the form $Ap$. In our case, this is $(K + \\sigma^2 I)p = Kp + \\sigma^2 p$.\nAs established in the analysis of statement C, the matrix-vector product $Kp$ with the Toeplitz matrix $K$ can be computed in $O(n \\log n)$ time using FFTs. The term $\\sigma^2 p$ is a vector scaling operation, costing $O(n)$. Therefore, the total cost per iteration is dominated by the FFT and is indeed $O(n \\log n)$.\nThe total cost of CG is the cost per iteration multiplied by the number of iterations required for convergence. The number of iterations depends on the condition number of the matrix $A$. Preconditioning is a technique to transform the system into an equivalent one with a better condition number, thus reducing the number of iterations. For Toeplitz systems, circulant preconditioners are a common and effective choice. Applying such a preconditioner also costs $O(n \\log n)$. If the preconditioner is effective, the number of iterations can be made small and nearly independent of $n$, leading to a total cost that scales close to $O(n \\log n)$. The statement is a precise and correct description of this state-of-the-art approach.\n\n**Verdict:** Correct.\n\n**F. If $C$ is a circulant embedding of $K$ constructed from a periodized version of the covariance, then the eigenvalues of $C$ are given by the DFT of the first column of $C$, and $\\log\\det(K + \\sigma^2 I)$ can be approximated by $\\sum_{j=0}^{m-1} \\log\\big(\\lambda_j(C) + \\sigma^2\\big)$; this approximation is accurate when the embedding reflects a valid periodic covariance and the embedding size $m$ is sufficiently large, but is generally biased for nonperiodic covariances.**\n\nThis statement describes a method for approximating the log-determinant. The method approximates the Toeplitz matrix $K$ with a circulant matrix $C$ of a larger size $m \\times m$.\nThe approximation is $\\log\\det(K + \\sigma^2 I) \\approx \\log\\det(C_{nn} + \\sigma^2 I_n)$, where $C_{nn}$ is the $n \\times n$ top-left block of $C$. A simpler but more common approximation is $\\log\\det(K+\\sigma^2 I) \\approx \\frac{n}{m} \\log\\det(C+\\sigma^2 I)$. The statement suggests a formula that looks like $\\log\\det(C+\\sigma^2 I)$, which is an approximation to the determinant of the embedded matrix, not the original. However, let's analyze the components.\n1.  Eigenvalues of $C$: It is a fundamental property that the eigenvalues of a circulant matrix $C$ are the components of the DFT of its first column. This is correct.\n2.  Log-determinant of the circulant approximation: For the matrix $C + \\sigma^2 I$, the eigenvalues are $\\lambda_j(C) + \\sigma^2$. The determinant is the product of eigenvalues, and the log-determinant is the sum of the log-eigenvalues. So, $\\log\\det(C + \\sigma^2 I) = \\sum_{j=0}^{m-1} \\log(\\lambda_j(C) + \\sigma^2)$. This formula is correct for the circulant matrix $C+\\sigma^2 I$.\n3.  Approximation quality: The expression $\\log\\det(C+\\sigma^2 I)$ serves as an approximation to $\\log\\det(K+\\sigma^2 I)$ (possibly with a scaling factor like $n/m$). This approximation arises from replacing the non-periodic covariance structure (Toeplitz) with a periodic one (circulant). This replacement introduces a systematic error, or bias, unless the original kernel was already periodic in a way that matches the circulant structure. The quality of the approximation improves as the embedding size $m$ increases relative to $n$, reducing the \"wrap-around\" effects. The statement accurately describes these aspects.\n\n**Verdict:** Correct.\n\n**G. A stochastic trace estimator such as Hutchinson’s method with random Rademacher vectors $z$ satisfies $\\mathbb{E}\\left[z^{\\top}\\log(K + \\sigma^2 I)z\\right] = \\operatorname{trace}\\big(\\log(K + \\sigma^2 I)\\big)$, and combined with Lanczos quadrature and FFT-based matrix-vector products with $K$, it provides an $O(n\\log n)$ per-iteration Monte Carlo method (up to factors for the number of probe vectors and Lanczos steps) for approximating $\\log\\det(K + \\sigma^2 I)$.**\n\nThis statement describes the Stochastic Lanczos Quadrature (SLQ) method for approximating a log-determinant.\nFirst, the log-determinant of any positive-definite matrix $A$ can be expressed as the trace of its matrix logarithm: $\\log\\det(A) = \\operatorname{tr}(\\log A)$.\nHutchinson's method estimates the trace of a matrix $B$ by the expectation $\\mathbb{E}[z^T B z]$, where $z$ is a random vector whose entries are i.i.d. with mean $0$ and variance $1$. This is because $\\mathbb{E}[z^T B z] = \\mathbb{E}[\\sum_{i,j} z_i B_{ij} z_j] = \\sum_{i,j} B_{ij} \\mathbb{E}[z_i z_j] = \\sum_i B_{ii} = \\operatorname{tr}(B)$, since $\\mathbb{E}[z_i z_j] = \\delta_{ij}$. The statement correctly applies this to $B = \\log(K + \\sigma^2 I)$.\nThe main difficulty is computing the quadratic form $z^T \\log(A) z$ for $A = K+\\sigma^2 I$. This is not a simple computation. Lanczos quadrature is a method for approximating quantities of the form $v^T f(A) v$. By running $k_{L}$ steps of the Lanczos algorithm on $A$ with starting vector $z$, one obtains a tridiagonal matrix $T_{k_L}$. The approximation is $z^T f(A) z \\approx z^T z [f(T_{k_L})]_{11}$.\nThe Lanczos algorithm's primary operation is matrix-vector multiplication with $A$. As per statement E, one such product costs $O(n \\log n)$ time.\nThus, one \"iteration\" of the Monte Carlo method (computing the estimate for one random probe vector $z$) requires $k_{L}$ matrix-vector products, costing $O(k_L n \\log n)$. If $k_L$, the number of Lanczos steps, is considered a small constant, the cost per probe vector is $O(n \\log n)$. The statement is a correct and accurate description of this advanced numerical method.\n\n**Verdict:** Correct.",
            "answer": "$$\\boxed{ACEFG}$$"
        }
    ]
}