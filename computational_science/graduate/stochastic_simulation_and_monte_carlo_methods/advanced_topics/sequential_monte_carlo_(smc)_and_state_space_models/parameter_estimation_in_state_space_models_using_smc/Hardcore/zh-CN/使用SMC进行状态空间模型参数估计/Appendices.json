{
    "hands_on_practices": [
        {
            "introduction": "将序贯蒙特卡洛（SMC）方法应用于参数估计的一个直接方法是，将其作为评估似然函数的“黑箱”。这个练习将指导你实现一个基于网格搜索的参数估计流程：你将为未知参数设定一个候选值网格，并为每个候选值运行一个粒子滤波器来估计其对数似然函数值，最终选择似然函数值最大的参数作为估计结果。通过这个过程，你不仅能掌握参数估计的基本流程，还能亲手实现并比较不同重采样方案（如多项式、分层、系统和残差重采样）对估计精度的影响，从而深入理解粒子滤波器的核心机制。",
            "id": "3326835",
            "problem": "考虑一个用于潜状态 $x_{t}$ 和观测值 $y_{t}$ 的线性高斯状态空间模型，该模型具有已知的自回归系数 $\\phi$ 和过程噪声标准差 $q$，以及未知的观测噪声标准差 $r$。该模型由以下两个方程给出：\n- 状态转移：$x_{t} = \\phi x_{t-1} + q \\,\\eta_{t}$，其中 $\\eta_{t} \\sim \\mathcal{N}(0,1)$。\n- 观测：$y_{t} = x_{t} + r \\,\\epsilon_{t}$，其中 $\\epsilon_{t} \\sim \\mathcal{N}(0,1)$。\n\n您的任务是使用序贯蒙特卡洛（SMC）方法和自举粒子滤波器，为 $r$ 实现一个参数估计程序，并比较不同重采样方案及其在参数估计精度方面的特性。\n\n您的工作必须基于以下基本原理和定义：\n- 马尔可夫性质：$p(x_{t} \\mid x_{1:t-1}) = p(x_{t} \\mid x_{t-1})$ 和观测值的条件独立性：$p(y_{t} \\mid x_{1:t}) = p(y_{t} \\mid x_{t})$。\n- 用于序贯推断的贝叶斯定理：滤波分布 $p(x_{t} \\mid y_{1:t})$ 可以通过似然 $p(y_{t} \\mid x_{t})$ 和转移 $p(x_{t} \\mid x_{t-1})$ 从 $p(x_{t-1} \\mid y_{1:t-1})$ 更新。\n- 自举粒子滤波器从转移分布 $p(x_{t} \\mid x_{t-1})$ 中抽取粒子，并根据似然 $p(y_{t} \\mid x_{t})$ 按比例更新权重。边缘似然分解 $p(y_{1:T}) = \\prod_{t=1}^{T} p(y_{t} \\mid y_{1:t-1})$ 源于概率的链式法则。\n- 有效样本量（ESS）定义为 $\\mathrm{ESS} = \\left(\\sum_{i=1}^{N} w_{t}^{(i)2}\\right)^{-1}$，其中 $w_{t}^{(i)}$ 是归一化权重，重采样触发条件由 $\\mathrm{ESS}/N \\le \\tau$ 决定，其中 $\\tau \\in [0,1]$ 是一个阈值。\n\n实现要求：\n- 使用固定的随机种子和指定的模型参数，模拟一个长度为 $T$ 的数据集。使用平稳先验 $x_{0} \\sim \\mathcal{N}\\!\\left(0, \\frac{q^{2}}{1-\\phi^{2}}\\right)$ 并根据模型模拟 $x_{t}$ 和 $y_{t}$。\n- 实现一个带有 $N$ 个粒子的自举粒子滤波器，该滤波器为任何提供的候选值 $r$ 估计对数边缘似然 $\\log p(y_{1:T} \\mid r)$。\n- 实现四种重采样方案：多项式重采样、分层重采样、系统重采样和残差重采样。当发生重采样时，用重采样后的祖先粒子替换粒子，并将权重重置为均匀分布。\n- 使用基于ESS的规则 $\\mathrm{ESS}/N \\le \\tau$ 实现重采样触发器。\n- 对于参数估计，评估一个候选观测噪声标准差的网格，并选择估计的对数边缘似然的最大化者作为点估计 $\\hat{r}$。\n\n数据集生成协议：\n- 使用 $\\phi = 0.9$，$q = 1.0$，$r_{\\text{true}} = 0.5$，$T = 200$。\n- 使用固定的数据集生成种子 $s_{\\text{data}} = 20240517$。\n- 生成 $x_{0} \\sim \\mathcal{N}\\!\\left(0, \\frac{q^{2}}{1-\\phi^{2}}\\right)$，然后相应地模拟 $\\{x_{t}\\}_{t=1}^{T}$ 和 $\\{y_{t}\\}_{t=1}^{T}$。\n\n粒子滤波和重采样细节：\n- 使用 $N = 1000$ 个粒子。\n- 在时间 $t$，通过转移密度传播每个粒子，并计算与观测值 $y_{t}$ 的观测似然成比例的权重。\n- 在每个时间步计算增量归一化常数，作为未归一化权重的平均值，并累积其对数以形成对数边缘似然估计。\n- 使用归一化权重计算ESS，并应用重采样触发器 $\\mathrm{ESS}/N \\le \\tau$。\n- 实现以下重采样方案：\n  - 多项式重采样。\n  - 分层重采样。\n  - 系统重采样。\n  - 残差重采样。\n\n参数网格和可复现性：\n- 使用候选网格 $\\mathcal{R} = \\{0.25, 0.5, 0.75, 1.0\\}$。\n- 为了在测试用例和候选参数之间具有可复现性，使用基础种子 $s_{\\text{base}} = 987654321$，并将测试用例 $j \\in \\{0,1,2,3,4,5\\}$ 中索引为 $k \\in \\{0,1,2,3\\}$ 的候选参数的粒子滤波器随机种子设置为 $s_{\\text{base}} + 1000\\,j + k$。该种子控制该评估中粒子滤波器的所有内部随机性，包括初始粒子和所有重采样随机数。\n\n测试套件：\n- 共有 $6$ 个测试用例，每个都是一对 $(\\text{scheme}, \\tau)$：\n  1. $(\\text{multinomial}, 0.5)$\n  2. $(\\text{stratified}, 0.5)$\n  3. $(\\text{systematic}, 0.5)$\n  4. $(\\text{residual}, 0.5)$\n  5. $(\\text{multinomial}, 1.0)$\n  6. $(\\text{stratified}, 0.0)$\n\n对于每个测试用例，通过在 $r \\in \\mathcal{R}$ 上最大化估计的 $\\log p(y_{1:T} \\mid r)$ 来计算 $\\hat{r}$，并报告绝对误差 $|\\hat{r} - r_{\\text{true}}|$。\n\n要求的最终输出格式：\n- 您的程序应生成一行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，第 $j$ 个条目是第 $j$ 个测试用例的绝对误差，顺序如上所列。例如，一个有效的输出形式为 $[\\text{err}_{1},\\text{err}_{2},\\ldots,\\text{err}_{6}]$。",
            "solution": "所提出的问题是计算统计学中一个有效且定义明确的练习，特别关注在线性高斯状态空间模型中使用序贯蒙特卡洛（SMC）方法进行参数估计。所有必要的参数、模型和程序细节都已提供，并且该问题在科学上基于贝叶斯推断和随机模拟的原理。我们将进行完整的解答。\n\n问题的核心是估计状态空间模型的一个静态参数，即观测噪声标准差 $r$。该模型由一个状态转移方程和一个观测方程定义。\n\n状态转移方程描述了潜（未观测）状态 $x_t$ 随时间 $t$ 的演变：\n$$x_{t} = \\phi x_{t-1} + q \\,\\eta_{t}, \\quad \\eta_{t} \\sim \\mathcal{N}(0,1)$$\n这是一个1阶自回归过程（AR($1$)），具有已知的系数 $\\phi$ 和标准差为 $q$ 的过程噪声。\n\n观测方程将潜状态 $x_t$ 与观测数据 $y_t$ 联系起来：\n$$y_{t} = x_{t} + r \\,\\epsilon_{t}, \\quad \\epsilon_{t} \\sim \\mathcal{N}(0,1)$$\n观测值是状态的带噪版本，其中观测噪声具有未知的标准差 $r$。\n\n我们的目标是给定一个观测序列 $y_{1:T} = \\{y_1, y_2, \\dots, y_T\\}$，找到 $r$ 的一个估计值 $\\hat{r}$。一种标准而强大的方法是最大似然估计。我们旨在找到使观测值的边缘似然 $p(y_{1:T} \\mid r)$ 最大化的 $r$ 值。\n$$\\hat{r} = \\underset{r}{\\arg\\max} \\, p(y_{1:T} \\mid r) = \\underset{r}{\\arg\\max} \\, \\log p(y_{1:T} \\mid r)$$\n对于状态空间模型，边缘似然通常难以直接计算，因为它需要对潜状态进行积分：\n$$p(y_{1:T} \\mid r) = \\int p(y_{1:T}, x_{0:T} \\mid r) \\,dx_{0:T}$$\nSMC方法，也称为粒子滤波器，提供了一种近似该量的方法。粒子滤波器的关键思想是用一组 $N$ 个加权样本或“粒子” $\\{x_t^{(i)}, W_t^{(i)}\\}_{i=1}^N$ 来表示滤波分布 $p(x_t \\mid y_{1:t})$。然后，该分布由一个经验测度近似：\n$$p(x_t \\mid y_{1:t}) \\approx \\sum_{i=1}^N W_t^{(i)} \\delta(x_t - x_t^{(i)})$$\n其中 $\\delta(\\cdot)$ 是狄拉克δ函数。\n\n自举粒子滤波器是此处采用的一种特定类型的SMC算法。它按时间顺序操作，在每个新观测 $y_t$ 到达时更新粒子集。它利用了边缘似然的分解：\n$$\\log p(y_{1:T} \\mid r) = \\sum_{t=1}^T \\log p(y_t \\mid y_{1:t-1}, r)$$\n粒子滤波器为每一项 $p(y_t \\mid y_{1:t-1}, r)$ 提供一个蒙特卡洛估计。\n\n算法流程如下：\n1.  **初始化 ($t=0$)：**\n    我们首先从先验分布 $p(x_0)$ 中抽样 $N$ 个粒子 $\\{x_0^{(i)}\\}_{i=1}^N$。问题指定使用AR($1$)过程的平稳分布，即 $x_0 \\sim \\mathcal{N}(0, q^2/(1-\\phi^2))$。初始权重是均匀的，$W_0^{(i)} = 1/N$ 对所有 $i \\in \\{1, \\dots, N\\}$。\n\n2.  **序贯更新 (对于 $t=1, \\dots, T$)：**\n    对于每个时间步，执行以下操作：\n    a. **传播：** 根据状态转移模型演化每个粒子。由于自举滤波器使用转移动态作为其提议分布，我们抽样：\n    $$x_t^{(i)} \\sim p(x_t \\mid x_{t-1}^{(i)}) = \\mathcal{N}(x_t; \\phi x_{t-1}^{(i)}, q^2)$$\n    b. **加权：** 基于新粒子 $x_t^{(i)}$ 对新观测 $y_t$ 的解释程度，计算其重要性权重。未归一化的权重 $\\tilde{w}_t^{(i)}$ 由给定粒子状态的观测似然给出：\n    $$\\tilde{w}_t^{(i)} = p(y_t \\mid x_t^{(i)}, r) = \\frac{1}{\\sqrt{2\\pi}r} \\exp\\left(-\\frac{(y_t - x_t^{(i)})^2}{2r^2}\\right)$$\n    c. **似然估计：** 预测似然 $p(y_t \\mid y_{1:t-1}, r)$ 通过未归一化权重的平均值来近似：\n    $$\\hat{p}(y_t \\mid y_{1:t-1}, r) \\approx \\frac{1}{N} \\sum_{i=1}^N \\tilde{w}_t^{(i)}$$\n    此值的对数被加到总的对数边缘似然估计中。\n    d. **归一化：** 将权重归一化，使其总和为1：\n    $$W_t^{(i)} = \\frac{\\tilde{w}_t^{(i)}}{\\sum_{j=1}^N \\tilde{w}_t^{(j)}}$$\n    e. **重采样：** 粒子滤波中的一个常见问题是权重退化，即经过几个步骤后，一个粒子的权重接近1，而所有其他粒子的权重接近0。这通过有效样本量（ESS）来监控，估计为：\n    $$\\mathrm{ESS} = \\left(\\sum_{i=1}^{N} (W_{t}^{(i)})^2\\right)^{-1}$$\n    如果ESS下降到粒子总数的某个分数 $\\tau$ 以下，即 $\\mathrm{ESS}/N \\le \\tau$，则执行重采样步骤。重采样涉及从当前集合 $\\{x_t^{(i)}\\}_{i=1}^N$ 中有放回地抽取 $N$ 个新粒子，其中抽取粒子 $i$ 的概率是其权重 $W_t^{(i)}$。这会消除低权重粒子并复制高权重粒子。重采样后，权重被重置为均匀分布，$W_t^{(i)} = 1/N$。\n\n问题要求比较四种重采样方案：\n*   **多项式重采样：** 这是最直接的方法，相当于从一个由权重定义的分类分布中抽取 $N$ 个样本。\n*   **系统重采样：** 该方案减少了重采样步骤的蒙特卡洛方差。它生成一个单一的随机数 $u \\in [0, 1/N)$，然后创建 $N$ 个有序指针序列 $u, u+1/N, \\dots, u+(N-1)/N$。这些指针用于从权重的累积分布中选择粒子。\n*   **分层重采样：** 这是一种相关的方差缩减技术。它将区间 $[0, 1)$ 分为 $N$ 个层，并从每个层中抽取一个随机样本。然后，这 $N$ 个样本像系统情况一样充当指针。\n*   **残差重采样：** 这是一种混合方法。它首先确定性地复制权重 $W_t^{(i)} > 1/N$ 的粒子。具体来说，每个粒子 $i$ 获得 $\\lfloor N W_t^{(i)} \\rfloor$ 个副本。剩余数量的粒子然后使用权重的残差部分通过多项式重采样来抽取。\n\n为了实现解决方案，首先使用真实参数 $\\phi=0.9$、$q=1.0$ 和 $r_{\\text{true}}=0.5$ 以及指定的随机种子生成一个长度为 $T=200$ 的数据集。然后，对于由一对（重采样方案，阈值 $\\tau$）定义的每个测试用例，为网格 $\\mathcal{R}=\\{0.25, 0.5, 0.75, 1.0\\}$ 中的每个候选 $r$ 估计对数边缘似然 $\\log \\hat{p}(y_{1:T} \\mid r)$。选择产生最高对数似然的候选值 $\\hat{r}$ 作为估计值。然后计算并报告绝对误差 $|\\hat{r} - r_{\\text{true}}|$。种子协议确保粒子滤波器的每次评估都是确定性的。测试用例探索了不同重采样方案和触发阈值的行为：$\\tau=0.5$ 是一个常见的选择，$\\tau=1.0$ 强制在每一步都进行重采样，而 $\\tau=0.0$ 则完全阻止重采样。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\n#\n# --- Problem Constants and Definitions ---\n#\n\n# Dataset parameters\nPHI = 0.9\nQ = 1.0\nR_TRUE = 0.5\nT = 200\nDATA_SEED = 20240517\n\n# Particle filter parameters\nN_PARTICLES = 1000\nR_GRID = [0.25, 0.5, 0.75, 1.0]\n\n# Reproducibility parameters\nBASE_SEED = 987654321\n\n# Test cases: (resampling_scheme_name, resampling_threshold_tau)\nTEST_CASES = [\n    ('multinomial', 0.5),\n    ('stratified', 0.5),\n    ('systematic', 0.5),\n    ('residual', 0.5),\n    ('multinomial', 1.0),\n    ('stratified', 0.0),\n]\n\n\n#\n# --- Data Generation ---\n#\n\ndef generate_data(phi, q, r_true, T, seed):\n    \"\"\"\n    Generates a single time series dataset from the linear Gaussian state-space model.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Initial state from stationary distribution\n    std_x0 = q / np.sqrt(1 - phi**2)\n    x = np.zeros(T + 1)\n    y = np.zeros(T)\n    x[0] = rng.normal(0, std_x0)\n    \n    # Simulate states and observations for t = 1 to T\n    for t in range(1, T + 1):\n        x[t] = phi * x[t-1] + q * rng.normal()\n        y[t-1] = x[t] + r_true * rng.normal()\n        \n    return y\n\n\n#\n# --- Resampling Schemes ---\n#\n\ndef multinomial_resample(weights, num_samples, rng):\n    \"\"\"Performs multinomial resampling.\"\"\"\n    return rng.choice(len(weights), size=num_samples, p=weights, replace=True)\n\ndef systematic_resample(weights, num_samples, rng):\n    \"\"\"Performs systematic resampling.\"\"\"\n    positions = (rng.random() + np.arange(num_samples)) / num_samples\n    cumulative_weights = np.cumsum(weights)\n    return np.searchsorted(cumulative_weights, positions)\n\ndef stratified_resample(weights, num_samples, rng):\n    \"\"\"Performs stratified resampling.\"\"\"\n    positions = (rng.random(size=num_samples) + np.arange(num_samples)) / num_samples\n    cumulative_weights = np.cumsum(weights)\n    return np.searchsorted(cumulative_weights, positions)\n\ndef residual_resample(weights, num_samples, rng):\n    \"\"\"Performs residual resampling.\"\"\"\n    num_copies = np.floor(num_samples * weights).astype(int)\n    num_residual = num_samples - np.sum(num_copies)\n    \n    indices = np.repeat(np.arange(num_samples), num_copies)\n    \n    if num_residual > 0:\n        residual_weights = (num_samples * weights) - num_copies\n        # Normalize residual weights to form a probability distribution\n        residual_sum = np.sum(residual_weights)\n        if residual_sum > 1e-12:\n            residual_weights /= residual_sum\n            residual_indices = rng.choice(num_samples, size=num_residual, p=residual_weights, replace=True)\n            indices = np.concatenate([indices, residual_indices])\n\n    return indices\n\nRESAMPLING_SCHEMES = {\n    'multinomial': multinomial_resample,\n    'stratified': stratified_resample,\n    'systematic': systematic_resample,\n    'residual': residual_resample,\n}\n\n\n#\n# --- Particle Filter Implementation ---\n#\n\ndef run_particle_filter(obs, phi, q, r_candidate, N, scheme, tau, seed):\n    \"\"\"\n    Runs a bootstrap particle filter to estimate the log marginal likelihood.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    T_obs = len(obs)\n    \n    # 1. Initialization\n    std_x0 = q / np.sqrt(1 - phi**2)\n    particles = rng.normal(0, std_x0, N)\n    log_likelihood = 0.0\n    \n    # 2. Sequential Importance Sampling and Resampling\n    for t in range(T_obs):\n        # a. Propagate particles\n        particles = phi * particles + q * rng.normal(size=N)\n\n        # b. Compute unnormalized weights\n        unnormalized_weights = norm.pdf(obs[t], loc=particles, scale=r_candidate)\n\n        # c. Update log-likelihood estimate\n        mean_weight = np.mean(unnormalized_weights)\n        if mean_weight > 1e-100: # Avoid log(0)\n            log_likelihood += np.log(mean_weight)\n        else:\n            return -np.inf # Log-likelihood is effectively -infinity\n\n        # d. Normalize weights\n        sum_weights = np.sum(unnormalized_weights)\n        if sum_weights  1e-100: # All particles have zero weight\n            # This can happen if r_candidate is very wrong. Re-initialize to avoid crash.\n            particles = rng.normal(0, std_x0, N)\n            normalized_weights = np.full(N, 1.0 / N)\n        else:\n            normalized_weights = unnormalized_weights / sum_weights\n\n        # e. Resampling\n        ess = 1.0 / np.sum(normalized_weights**2)\n        if (ess / N) = tau:\n            resampling_func = RESAMPLING_SCHEMES[scheme]\n            indices = resampling_func(normalized_weights, N, rng)\n            particles = particles[indices]\n            # Weights are reset to uniform implicitly for the next propagation step\n            # because the next unnormalized weights will be calculated from a\n            # uniformly weighted particle set. We only need to store the normalized\n            # weights for the ESS check for the current step.\n\n    return log_likelihood\n\n\n#\n# --- Main Solver ---\n#\n\ndef solve():\n    \"\"\"\n    Main function to execute the problem specification.\n    \"\"\"\n    y_obs = generate_data(PHI, Q, R_TRUE, T, DATA_SEED)\n    \n    results = []\n\n    for j, (scheme, tau) in enumerate(TEST_CASES):\n        log_likelihoods = []\n        for k, r_candidate in enumerate(R_GRID):\n            \n            # Set seed for this specific run for reproducibility\n            run_seed = BASE_SEED + 1000 * j + k\n            \n            log_lik = run_particle_filter(\n                obs=y_obs,\n                phi=PHI,\n                q=Q,\n                r_candidate=r_candidate,\n                N=N_PARTICLES,\n                scheme=scheme,\n                tau=tau,\n                seed=run_seed\n            )\n            log_likelihoods.append(log_lik)\n        \n        # Find the r that maximizes the log-likelihood\n        best_r_index = np.argmax(log_likelihoods)\n        r_hat = R_GRID[best_r_index]\n        \n        # Calculate absolute error\n        error = abs(r_hat - R_TRUE)\n        results.append(error)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\n# Execute the solver\nsolve()\n\n```"
        },
        {
            "introduction": "在实践中实现并比较了不同的重采样方案之后，从理论上理解它们的性质至关重要。本练习聚焦于一个衡量粒子滤波器健康状况的关键指标：重采样后的唯一祖先粒子数量，它直接关系到粒子多样性及滤波器退化的风险。通过为多项式重采样和分层重采样这两种基本方案推导该指标的期望值，你将对为何分层重采样等高级方法在实践中通常更优获得定量的认识，因为它能有效降低粒子系统的蒙特卡洛方差。",
            "id": "3326877",
            "problem": "考虑一种用于状态空间模型中联合状态和静态参数推断的序贯蒙特卡罗（SMC）方法，在给定的时间指数 $t$，您维护 $N \\geq 2$ 个带权粒子，其归一化权重为 $W^{(1)},\\dots,W^{(N)}$，满足对所有 $i$ 都有 $W^{(i)}  0$ 且 $\\sum_{i=1}^{N} W^{(i)} = 1$。在重采样步骤中，您从由权重引导的离散分布中抽取 $N$ 个后代索引，然后根据模型将每个选中的祖先向前传播。\n\n将重采样后不同祖先的数量定义为接收到至少一个后代的索引 $i \\in \\{1,\\dots,N\\}$ 集合的基数。令 $K_i$ 表示分配给祖先 $i$ 的后代数量。那么，不同祖先的数量等于 $\\sum_{i=1}^{N} \\mathbf{1}\\{K_i  0\\}$，其期望为 $\\sum_{i=1}^{N} \\mathbb{P}(K_i  0)$。\n\n考虑两种重采样方案：\n\n- 多项式重采样：从 $(W^{(1)},\\dots,W^{(N)})$ 中抽取 $N$ 个独立的分类样本，使得 $(K_1,\\dots,K_N)$ 服从参数为 $N$ 和 $(W^{(1)},\\dots,W^{(N)})$ 的多项式分布。\n\n- 分层重采样：对 $j=1,\\dots,N$，抽取 $U_j = \\frac{j-1 + V_j}{N}$，其中 $V_j \\sim \\text{Uniform}(0,1)$ 是独立的，如果 $U_j \\in [C_{i-1}, C_i)$，则选择祖先 $i$，其中 $C_0 = 0$ 且对 $i \\in \\{1,\\dots,N\\}$ 有 $C_i = \\sum_{k=1}^{i} W^{(k)}$。令 $S_j = \\left[\\frac{j-1}{N}, \\frac{j}{N}\\right)$ 表示第 $j$ 层。\n\n仅从这些重采样机制的核心定义出发，不使用任何专门的公式，推导在多项式重采样和分层重采样下，不同祖先期望数量 $\\mathbb{E}\\left[\\sum_{i=1}^{N} \\mathbf{1}\\{K_i  0\\}\\right]$ 的闭式解析表达式。表达式应完全用 $N$ 和权重 $W^{(1)},\\dots,W^{(N)}$（及其累积和 $C_i$）表示。无需四舍五入；请以精确的符号形式表示您的最终答案。",
            "solution": "问题陈述定义明确、科学依据充分且自成体系。它提出了计算统计学和序贯蒙特卡罗方法领域中一个标准的、非平凡的问题。所有术语都得到了形式化和精确的定义，目标清晰。该问题是有效的。\n\n令 $D$ 为重采样后不同祖先的数量，定义为 $D = \\sum_{i=1}^{N} \\mathbf{1}\\{K_i  0\\}$，其中 $K_i$ 是祖先 $i$ 的后代数量。根据期望的线性性质，不同祖先的期望数量为\n$$\n\\mathbb{E}[D] = \\mathbb{E}\\left[\\sum_{i=1}^{N} \\mathbf{1}\\{K_i  0\\}\\right] = \\sum_{i=1}^{N} \\mathbb{E}[\\mathbf{1}\\{K_i  0\\}] = \\sum_{i=1}^{N} \\mathbb{P}(K_i  0)\n$$\n我们需要为这两种重采样方案分别推导 $\\mathbb{P}(K_i  0)$。更方便的做法是计算其互补概率 $\\mathbb{P}(K_i = 0)$，并使用关系式 $\\mathbb{P}(K_i  0) = 1 - \\mathbb{P}(K_i = 0)$。\n\n**1. 多项式重采样**\n\n在多项式重采样中，我们从一个概率为 $(W^{(1)}, W^{(2)}, \\dots, W^{(N)})$ 的分类分布中抽取 $N$ 个独立样本。对于特定的祖先 $i$，其后代数量 $K_i$ 对应于在这 $N$ 次试验中抽中索引 $i$ 的次数。\n\n每次抽取都是一次独立的伯努利试验，其中“成功”是抽中祖先 $i$，其概率为 $W^{(i)}$。因此，祖先 $i$ 的总后代数量 $K_i$ 服从参数为 $N$（试验次数）和 $p=W^{(i)}$（成功概率）的二项分布：\n$$\nK_i \\sim \\text{Binomial}(N, W^{(i)})\n$$\n其概率质量函数由 $\\mathbb{P}(K_i = k) = \\binom{N}{k} (W^{(i)})^k (1 - W^{(i)})^{N-k}$ 给出。\n\n我们关心事件 $\\{K_i=0\\}$，这意味着在 $N$ 次独立抽取中，祖先 $i$ 一次也未被选中。该事件的概率为：\n$$\n\\mathbb{P}(K_i = 0) = \\binom{N}{0} (W^{(i)})^0 (1 - W^{(i)})^{N-0} = (1 - W^{(i)})^N\n$$\n因此，祖先 $i$ 至少有一个后代的概率为：\n$$\n\\mathbb{P}(K_i  0) = 1 - \\mathbb{P}(K_i = 0) = 1 - (1 - W^{(i)})^N\n$$\n对所有祖先求和，多项式重采样下不同祖先的期望数量为：\n$$\n\\mathbb{E}[D]_{\\text{multinomial}} = \\sum_{i=1}^{N} \\mathbb{P}(K_i  0) = \\sum_{i=1}^{N} \\left(1 - (1 - W^{(i)})^N\\right)\n$$\n\n**2. 分层重采样**\n\n在分层重采样中，从每个层 $S_j = \\left[\\frac{j-1}{N}, \\frac{j}{N}\\right)$（$j=1, \\dots, N$）中抽取一个有序的均匀随机数。第 $j$ 次抽取为 $U_j = \\frac{j-1 + V_j}{N}$，其中 $V_j \\sim \\text{Uniform}(0,1)$ 是独立的。如果 $U_j$ 落在累积权重区间 $I_i = [C_{i-1}, C_i)$ 内，则第 $j$ 次抽取选中祖先 $i$，其中 $C_i = \\sum_{k=1}^i W^{(k)}$。\n\n祖先 $i$ 的总后代数量为 $K_i = \\sum_{j=1}^N \\mathbf{1}\\{U_j \\in I_i\\}$。我们希望求出 $\\mathbb{P}(K_i  0)$。\n事件 $\\{K_i = 0\\}$ 意味着对于所有 $j \\in \\{1, \\dots, N\\}$，抽取的 $U_j$ 都不在区间 $I_i$ 内。由于随机变量 $V_j$ 是独立的，抽取的 $U_j$ 也是独立的。因此，$\\{K_i=0\\}$ 的概率是各个概率的乘积：\n$$\n\\mathbb{P}(K_i = 0) = \\mathbb{P}\\left(\\bigcap_{j=1}^{N} \\{U_j \\notin I_i\\}\\right) = \\prod_{j=1}^{N} \\mathbb{P}(U_j \\notin I_i) = \\prod_{j=1}^{N} \\left(1 - \\mathbb{P}(U_j \\in I_i)\\right)\n$$\n随机变量 $U_j$ 在其层 $S_j$ 上均匀分布，该层长度为 $\\frac{1}{N}$。$U_j$ 落在区间 $I_i$ 内的概率是 $I_i$ 和 $S_j$ 交集的长度除以 $S_j$ 的长度：\n$$\n\\mathbb{P}(U_j \\in I_i) = \\frac{\\text{length}(I_i \\cap S_j)}{\\text{length}(S_j)} = \\frac{\\text{length}\\left([C_{i-1}, C_i) \\cap \\left[\\frac{j-1}{N}, \\frac{j}{N}\\right)\\right)}{1/N} = N \\cdot \\text{length}\\left(I_i \\cap S_j\\right)\n$$\n两个区间 $[a, b)$ 和 $[c, d)$ 交集的长度由 $\\max(0, \\min(b, d) - \\max(a, c))$ 给出。应用此公式：\n$$\n\\text{length}\\left(I_i \\cap S_j\\right) = \\max\\left(0, \\min\\left(C_i, \\frac{j}{N}\\right) - \\max\\left(C_{i-1}, \\frac{j-1}{N}\\right)\\right)\n$$\n将此代回 $\\mathbb{P}(K_i=0)$ 的表达式中：\n$$\n\\mathbb{P}(K_i = 0) = \\prod_{j=1}^{N} \\left(1 - N \\cdot \\max\\left(0, \\min\\left(C_i, \\frac{j}{N}\\right) - \\max\\left(C_{i-1}, \\frac{j-1}{N}\\right)\\right)\\right)\n$$\n因此，祖先 $i$ 至少有一个后代的概率为：\n$$\n\\mathbb{P}(K_i  0) = 1 - \\prod_{j=1}^{N} \\left(1 - N \\cdot \\max\\left(0, \\min\\left(C_i, \\frac{j}{N}\\right) - \\max\\left(C_{i-1}, \\frac{j-1}{N}\\right)\\right)\\right)\n$$\n分层重采样下不同祖先的期望数量是这些概率对所有祖先求和的结果：\n$$\n\\mathbb{E}[D]_{\\text{stratified}} = \\sum_{i=1}^{N} \\left[1 - \\prod_{j=1}^{N} \\left(1 - N \\cdot \\max\\left(0, \\min\\left(C_i, \\frac{j}{N}\\right) - \\max\\left(C_{i-1}, \\frac{j-1}{N}\\right)\\right)\\right)\\right]\n$$\n该表达式是所需的闭式形式，完全用 $N$ 和权重 $W^{(1)}, \\dots, W^{(N)}$（通过其累积和 $C_i = \\sum_{k=1}^i W^{(k)}$ 且 $C_0=0$）来表示。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\sum_{i=1}^{N} \\left(1 - \\left(1 - W^{(i)}\\right)^N\\right) \\\\ \\sum_{i=1}^{N} \\left[1 - \\prod_{j=1}^{N} \\left(1 - N \\cdot \\max\\left(0, \\min\\left(C_i, \\frac{j}{N}\\right) - \\max\\left(C_{i-1}, \\frac{j-1}{N}\\right)\\right)\\right)\\right] \\end{pmatrix}}\n$$"
        }
    ]
}