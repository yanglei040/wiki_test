{
    "hands_on_practices": [
        {
            "introduction": "自适应Metropolis (AM)算法的效率关键在于其提议机制。本练习将推导该算法提议设计的基石性结论：最优提议协方差应与目标协方差成正比，且在高维下具有特定的缩放因子。通过理解这个推导过程，您将掌握自适应MCMC方法背后的核心理论依据 。",
            "id": "3353620",
            "problem": "考虑针对协方差矩阵为 $\\Sigma_{\\star}$ 的 $d$ 维零均值高斯分布的 Haario–Saksman–Tamminen 自适应 Metropolis (AM) 算法，其目标密度 $\\pi(x)$ 正比于 $\\exp\\!\\big(-\\tfrac{1}{2} x^{\\top}\\Sigma_{\\star}^{-1}x\\big)$，其中 $x\\in\\mathbb{R}^{d}$。在每次迭代中，AM 算法使用形式为 $q(x,\\cdot)=\\mathcal{N}\\!\\big(x,\\;S\\big)$ 的对称高斯随机游走提议，其中 $S$ 是一个随时间自适应调整的正定提议协方差矩阵。\n\n从 Metropolis–Hastings 算法的基本性质和高斯目标在线性变换下的仿射不变性出发，论证——在不引用任何特定最优尺度公式的情况下——$S$ 的渐近最优结构是 $\\Sigma_{\\star}$ 的一个标量倍数，即 $S=s_{d}^{2}\\Sigma_{\\star}$，其中标量 $s_{d}>0$ 取决于维度 $d$。然后，利用高维随机游走 Metropolis (RWM) 的扩散极限启发式方法，推导接受概率作为重标度的步长参数的函数的极限形式，并通过最大化期望平方跳跃距离，确定最优标量 $s_{d}$ 作为 $d$ 的显式函数。\n\n你的最终答案必须是关于 $d$ 的 $s_d$ 的单个闭式解析表达式。将数值前置因子四舍五入到三位有效数字。本问题不涉及物理单位。",
            "solution": "该问题要求关于高斯目标的自适应 Metropolis (AM) 算法的两个主要结果。首先，利用仿射不变性论证来推断提议协方差矩阵 $S$ 的最优结构。其次，推导该协方差矩阵在高维极限下的最优尺度因子。\n\n### 第1部分：提议协方差的最优结构\n\n目标密度为 $\\pi(x) \\propto \\exp(-\\frac{1}{2} x^{\\top}\\Sigma_{\\star}^{-1}x)$，其中 $x \\in \\mathbb{R}^d$。提议是一个高斯随机游走，$x' = x + \\xi$，其中 $\\xi \\sim \\mathcal{N}(0, S)$。Metropolis-Hastings 接受概率为 $\\alpha(x, x') = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x)}\\right)$。\n\n我们援引仿射不变性原理。采样算法的效率不应依赖于状态空间 $\\mathbb{R}^d$ 的基的选择。考虑一个任意的可逆线性变换 $y = Ax$，其中 $A$ 是一个 $d \\times d$ 矩阵。\n如果 $x \\sim \\mathcal{N}(0, \\Sigma_{\\star})$，那么变换后的变量 $y$ 服从高斯分布 $y \\sim \\mathcal{N}(0, A\\Sigma_{\\star}A^{\\top})$。对于 $y$ 的目标密度为 $\\pi_y(y) \\propto \\exp(-\\frac{1}{2} y^{\\top}(A\\Sigma_{\\star}A^{\\top})^{-1}y)$。\n\n在 $y$ 空间中，与 $x$ 空间中对应的提议机制是 $y' = Ax' = A(x+\\xi) = y + A\\xi$。$y$ 空间中的提议增量是 $\\xi_y = A\\xi$。由于 $\\xi \\sim \\mathcal{N}(0, S)$，变换后的增量服从 $\\xi_y \\sim \\mathcal{N}(0, ASA^{\\top})$。因此，$y$ 空间中的提议协方差是 $S_y = ASA^{\\top}$。\n\n原始链的接受概率是目标密度对数比的函数：\n$$ \\ln\\left(\\frac{\\pi(x')}{\\pi(x)}\\right) = -\\frac{1}{2}\\left( (x+\\xi)^{\\top}\\Sigma_{\\star}^{-1}(x+\\xi) - x^{\\top}\\Sigma_{\\star}^{-1}x \\right) = -x^{\\top}\\Sigma_{\\star}^{-1}\\xi - \\frac{1}{2}\\xi^{\\top}\\Sigma_{\\star}^{-1}\\xi $$\n变换后链的接受概率取决于类似量：\n$$ \\ln\\left(\\frac{\\pi_y(y')}{\\pi_y(y)}\\right) = -y^{\\top}(A\\Sigma_{\\star}A^{\\top})^{-1}\\xi_y - \\frac{1}{2}\\xi_y^{\\top}(A\\Sigma_{\\star}A^{\\top})^{-1}\\xi_y $$\n代入 $y = Ax$，$\\xi_y = A\\xi$，以及 $(A\\Sigma_{\\star}A^{\\top})^{-1} = (A^{\\top})^{-1}\\Sigma_{\\star}^{-1}A^{-1}$：\n$$ \\ln\\left(\\frac{\\pi_y(y')}{\\pi_y(y)}\\right) = -(Ax)^{\\top}(A^{\\top})^{-1}\\Sigma_{\\star}^{-1}A^{-1}(A\\xi) - \\frac{1}{2}(A\\xi)^{\\top}(A^{\\top})^{-1}\\Sigma_{\\star}^{-1}A^{-1}(A\\xi) $$\n$$ = -x^{\\top}A^{\\top}(A^{\\top})^{-1}\\Sigma_{\\star}^{-1}A^{-1}A\\xi - \\frac{1}{2}\\xi^{\\top}A^{\\top}(A^{\\top})^{-1}\\Sigma_{\\star}^{-1}A^{-1}A\\xi $$\n$$ = -x^{\\top}\\Sigma_{\\star}^{-1}\\xi - \\frac{1}{2}\\xi^{\\top}\\Sigma_{\\star}^{-1}\\xi = \\ln\\left(\\frac{\\pi(x')}{\\pi(x)}\\right) $$\n接受概率是相同的。这意味着算法的性能指标（如接受率和自相关时间）在仿射变换下是不变的，只要提议协方差相应地变换为 $S \\to ASA^{\\top}$。\n\n对于一个学习提议协方差 $S$ 的自适应算法，很自然地要求最优的学习协方差能尊重这种不变性。在目标分布不相关且在所有方向上具有单位方差（即各向同性）的坐标系中，问题变得最简单。我们可以通过选择一个变换矩阵 $A$ 使得 $A\\Sigma_{\\star}A^{\\top} = I_d$（$d \\times d$ 单位矩阵）来实现这一点。例如，我们可以使用白化变换 $A = \\Sigma_{\\star}^{-1/2}$。在这个白化空间中，目标是 $\\mathcal{N}(0, I_d)$。\n\n对于一个各向同性的目标，没有理由偏好任何一个方向。任何对状态空间的有效探索也应该是各向同性的。因此，白化空间中的最优提议协方差必须与单位矩阵成比例：$S_y = c I_d$，对于某个标量 $c > 0$。\n\n将这个最优提议变换回原始的 $x$ 空间，就得到了最优 $S$ 的结构：\n$S_y = ASA^{\\top} \\implies S = A^{-1} S_y (A^{\\top})^{-1}$\n使用 $A = \\Sigma_{\\star}^{-1/2}$（所以 $A^{-1} = \\Sigma_{\\star}^{1/2}$）和 $S_y = c I_d$：\n$$ S = \\Sigma_{\\star}^{1/2} (c I_d) (\\Sigma_{\\star}^{-1/2})^{\\top} = c \\Sigma_{\\star}^{1/2} (\\Sigma_{\\star}^{1/2})^{\\top} = c \\Sigma_{\\star} $$\n因此，渐近最优提议协方差矩阵 $S$ 必须是目标协方差矩阵 $\\Sigma_{\\star}$ 的一个标量倍数。我们将这个正常数比例常数记为 $s_d^2$，所以 $S = s_d^2 \\Sigma_{\\star}$。\n\n### 第2部分：最优尺度因子 $s_d$\n\n基于以上论证，我们可以在标准化空间中分析算法的性能，其中目标是 $\\pi(y) \\propto \\exp(-\\frac{1}{2}y^{\\top}y)$，提议是 $\\mathcal{N}(y, s_d^2 I_d)$，其结果将适用于一般情况。为简便起见，本节我们用 $x$ 代替 $y$。提议是 $x' = x + \\xi$，其中 $\\xi \\sim \\mathcal{N}(0, s_d^2 I_d)$。\n\n我们使用高维 ($d \\to \\infty$) 扩散极限启发式方法。为使接受率保持非零，步长必须趋于零。我们使用尺度拟设 $s_d = \\lambda / \\sqrt{d}$，其中 $\\lambda > 0$ 是某个常数。因此提议为 $\\xi \\sim \\mathcal{N}(0, (\\lambda^2/d)I_d)$。\n\n对数目标的变化是 $\\Delta E(x, \\xi) = \\frac{1}{2}\\|x\\|^2 - \\frac{1}{2}\\|x'\\|^2 = -x^{\\top}\\xi - \\frac{1}{2}\\|\\xi\\|^2$。接受概率是 $\\alpha(x, x') = \\min(1, \\exp(\\Delta E))$。我们来分析 $\\Delta E$ 中的两项：\n1.  根据大数定律，当 $d \\to \\infty$ 时，$\\|\\xi\\|^2 = \\sum_{i=1}^d \\xi_i^2$ 收敛到其期望值：\n    $$ E[\\|\\xi\\|^2] = E\\left[\\sum_{i=1}^d \\xi_i^2\\right] = d \\cdot E[\\xi_1^2] = d \\cdot \\text{Var}(\\xi_1) = d \\cdot \\frac{\\lambda^2}{d} = \\lambda^2 $$\n    所以，$\\|\\xi\\|^2 \\to \\lambda^2$。\n2.  项 $x^{\\top}\\xi = \\sum_{i=1}^d x_i \\xi_i$ 是独立同分布项的和，其中 $x_i \\sim \\mathcal{N}(0, 1)$（因为 $x$ 来自平稳分布）且 $\\xi_i \\sim \\mathcal{N}(0, \\lambda^2/d)$。每一项的均值为 $E[x_i\\xi_i] = E[x_i]E[\\xi_i] = 0$，方差为 $\\text{Var}(x_i\\xi_i) = E[x_i^2]E[\\xi_i^2] = 1 \\cdot (\\lambda^2/d) = \\lambda^2/d$。\n    该和的方差为 $\\text{Var}(x^{\\top}\\xi) = d \\cdot (\\lambda^2/d) = \\lambda^2$。根据中心极限定理，$x^{\\top}\\xi$ 依分布收敛到一个正态随机变量 $Z \\sim \\mathcal{N}(0, \\lambda^2)$。\n\n在极限 $d\\to\\infty$ 下，接受概率变成 $\\lambda$ 的一个确定性函数，通过对随机分量的极限分布求平均得到：\n$$ \\alpha(\\lambda) = E_Z[\\min(1, \\exp(-Z - \\frac{1}{2}\\lambda^2))] $$\n令 $Y = -Z - \\frac{1}{2}\\lambda^2$。由于 $Z \\sim \\mathcal{N}(0, \\lambda^2)$，所以 $Y \\sim \\mathcal{N}(-\\frac{1}{2}\\lambda^2, \\lambda^2)$。期望为\n$$ \\alpha(\\lambda) = \\int_{-\\infty}^{\\infty} \\min(1, e^y) p(y) dy = \\int_{-\\infty}^{0} e^y p(y) dy + \\int_{0}^{\\infty} p(y) dy $$\n其中 $p(y)$ 是 $Y$ 的密度。设 $\\Phi$ 为标准正态累积分布函数(CDF)。第二项是 $P(Y > 0) = P\\left(\\mathcal{N}(-\\frac{1}{2}\\lambda^2, \\lambda^2) > 0\\right) = P\\left(\\mathcal{N}(0, 1) > \\frac{\\lambda}{2}\\right) = 1 - \\Phi(\\lambda/2) = \\Phi(-\\lambda/2)$。\n可以证明，第一个积分的计算结果也是 $\\Phi(-\\lambda/2)$。因此，极限接受率为\n$$ \\alpha(\\lambda) = 2\\Phi(-\\lambda/2) $$\n\n为了找到最优的 $\\lambda$，我们最大化采样器的效率，它正比于每次迭代的期望平方跳跃距离 (ESJD)。跳跃是 $x_{n+1}-x_n$，接受时为 $\\xi$，拒绝时为 $0$。\n$$ \\text{ESJD} = E[\\|x_{n+1}-x_n\\|^2] = E[\\alpha(x_n, x_n') \\|\\xi\\|^2] $$\n在高维极限下，接受概率和提议步长的范数渐近独立。\n$$ \\text{ESJD}(\\lambda) \\approx E[\\alpha(x_n, x_n')] E[\\|\\xi\\|^2] = \\alpha(\\lambda) E[\\|\\xi\\|^2] $$\n如前所示，$E[\\|\\xi\\|^2] = \\lambda^2$。所以我们要最大化函数 $f(\\lambda) = \\lambda^2 \\alpha(\\lambda) = 2\\lambda^2 \\Phi(-\\lambda/2)$。我们通过将其关于 $\\lambda$ 的导数设为零来找到最大值：\n$$ \\frac{df}{d\\lambda} = \\frac{d}{d\\lambda} \\left[2\\lambda^2 \\Phi(-\\lambda/2)\\right] = 4\\lambda\\Phi(-\\lambda/2) + 2\\lambda^2 \\frac{d}{d\\lambda}\\Phi(-\\lambda/2) = 0 $$\n使用链式法则，并令 $\\phi$ 为标准正态概率密度函数(PDF)，$\\frac{d}{d\\lambda}\\Phi(-\\lambda/2) = \\phi(-\\lambda/2) \\cdot (-\\frac{1}{2}) = -\\frac{1}{2}\\phi(\\lambda/2)$，因为 $\\phi$ 是偶函数。\n$$ 4\\lambda\\Phi(-\\lambda/2) - \\lambda^2\\phi(\\lambda/2) = 0 $$\n对于 $\\lambda > 0$，我们可以除以 $\\lambda$：\n$$ 4\\Phi(-\\lambda/2) = \\lambda\\phi(\\lambda/2) $$\n这是一个关于最优 $\\lambda$（我们记为 $\\lambda_{opt}$）的超越方程。该方程无法用初等函数求解。数值求解得出 $\\lambda_{opt} \\approx 2.379$。问题要求四舍五入到三位有效数字，所以 $\\lambda_{opt} \\approx 2.38$。\n\n最优标量 $s_d$ 则由我们的尺度拟设给出：\n$$ s_d = \\frac{\\lambda_{opt}}{\\sqrt{d}} \\approx \\frac{2.38}{\\sqrt{d}} $$\n这给出了 $s_d$ 对维度 $d$ 的显式函数依赖关系。",
            "answer": "$$ \\boxed{\\frac{2.38}{\\sqrt{d}}} $$"
        },
        {
            "introduction": "本练习探讨了标准高斯目标下的最优缩放规则的普适性及其局限。通过分析一类具有不同尾部行为的分布，您将运用拉普拉斯方法来理解目标分布的几何特性如何影响最优步长。这项练习强调了最优缩放并非一成不变，而是关键地依赖于目标分布的特性 。",
            "id": "3353663",
            "problem": "考虑一个形式为 $\\pi_{d}(\\mathbf{x}) \\propto \\exp\\!\\big(-\\|\\mathbf{x}\\|^{\\beta}\\big)$（其中 $\\beta>2$）的 $d$ 维目标密度，以及一个随机游走Metropolis提议 $\\mathbf{y}=\\mathbf{x}+\\delta\\,\\mathbf{Z}$，其中 $\\mathbf{Z}\\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I}_{d})$ 且 $\\delta>0$ 是一个标量步长。在 Haario–Saksman–Tamminen 自适应Metropolis算法 (AM) 中，在运行过程中会调整提议协方差以实现有效的缩放。假设已达到平稳性且 $d$ 很大。使用拉普拉斯方法来刻画目标质量在半径上的集中位置，并利用对数密度差的二阶展开，当步长按 $\\delta=\\ell\\,d^{-(\\beta-1)/\\beta}$（其中 $\\ell>0$ 为常数）进行缩放时，推导期望接受概率 $E[\\alpha]$ 的一个渐近近似。将你的最终答案表示为关于 $\\ell$ 和 $\\beta$ 的闭式解析表达式。然后，基于此近似，定性地解释指数 $\\beta$ 如何影响 AM 算法中的最优缩放规则（你可以在推导过程中定性地讨论这一点，但最终要求的数值表达式必须仅针对 $E[\\alpha]$）。",
            "solution": "问题要求为随机游走Metropolis算法推导期望接受概率 $E[\\alpha]$ 的渐近近似，该算法的目标是一个 $d$ 维密度 $\\pi_d(\\mathbf{x}) \\propto \\exp(-\\|\\mathbf{x}\\|^{\\beta})$，其中 $\\beta > 2$。提议为 $\\mathbf{y}=\\mathbf{x}+\\delta\\,\\mathbf{Z}$，其中 $\\mathbf{Z}\\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I}_{d})$，步长按 $\\delta=\\ell\\,d^{-(\\beta-1)/\\beta}$ 进行缩放。我们假设链处于平稳状态且 $d$ 很大。\n\n首先，我们确定目标概率质量集中的区域。密度 $\\pi_d(\\mathbf{x})$ 是球对称的。令 $r = \\|\\mathbf{x}\\|$。概率元与密度值乘以体积元成正比。半径为 $r$、厚度为 $dr$ 的薄球壳的体积与其表面积成正比，表面积与 $r^{d-1}$ 成比例。因此，径向概率密度 $p(r)$ 由下式给出：\n$$p(r) \\propto r^{d-1} \\exp(-r^{\\beta})$$\n为了找到该密度最大时的半径 $r_d$，我们通过最大化 $p(r)$ 的对数来使用拉普拉斯方法：\n$$L(r) = \\ln(p(r)) = (d-1)\\ln(r) - r^{\\beta} + C$$\n其中 $C$ 是一个常数。对 $r$ 求导并令结果为零，得到众数半径：\n$$\\frac{dL}{dr} = \\frac{d-1}{r} - \\beta r^{\\beta-1} = 0$$\n$$d-1 = \\beta r^{\\beta}$$\n$$r = \\left(\\frac{d-1}{\\beta}\\right)^{1/\\beta}$$\n对于大的 $d$，我们有 $d-1 \\approx d$，因此链所在的典型半径近似为：\n$$r_d \\approx \\left(\\frac{d}{\\beta}\\right)^{1/\\beta}$$\n\n接下来，我们分析接受概率 $\\alpha(\\mathbf{x}, \\mathbf{y})$。由于提议核是对称的，接受概率为\n$$\\alpha(\\mathbf{x}, \\mathbf{y}) = \\min\\left(1, \\frac{\\pi_d(\\mathbf{y})}{\\pi_d(\\mathbf{x})}\\right)$$\n密度之比为\n$$\\frac{\\pi_d(\\mathbf{y})}{\\pi_d(\\mathbf{x})} = \\frac{\\exp(-\\|\\mathbf{y}\\|^{\\beta})}{\\exp(-\\|\\mathbf{x}\\|^{\\beta})} = \\exp\\left(-\\|\\mathbf{y}\\|^{\\beta} + \\|\\mathbf{x}\\|^{\\beta}\\right)$$\n令 $f(\\mathbf{x}) = -\\|\\mathbf{x}\\|^{\\beta}$。对数比率为 $U = f(\\mathbf{y}) - f(\\mathbf{x})$。对于提议 $\\mathbf{y} = \\mathbf{x} + \\delta\\mathbf{Z}$，我们可以使用 $f(\\mathbf{y})$ 在 $\\mathbf{x}$ 附近的二阶泰勒展开：\n$$U = f(\\mathbf{x} + \\delta\\mathbf{Z}) - f(\\mathbf{x}) \\approx \\delta \\nabla f(\\mathbf{x})^T \\mathbf{Z} + \\frac{1}{2}\\delta^2 \\mathbf{Z}^T H_f(\\mathbf{x}) \\mathbf{Z}$$\n其中 $\\nabla f(\\mathbf{x})$ 是梯度， $H_f(\\mathbf{x})$ 是 $f$ 的海森矩阵。\n\n$f(\\mathbf{x})$ 的梯度为：\n$$\\nabla f(\\mathbf{x}) = -\\nabla(\\|\\mathbf{x}\\|^{\\beta}) = -\\beta \\|\\mathbf{x}\\|^{\\beta-2} \\mathbf{x}$$\n海森矩阵为：\n$$H_f(\\mathbf{x})_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} = -\\beta \\left[ (\\beta-2) \\|\\mathbf{x}\\|^{\\beta-4} x_i x_j + \\|\\mathbf{x}\\|^{\\beta-2} \\delta_{ij} \\right]$$\n$$H_f(\\mathbf{x}) = -\\beta \\left[ (\\beta-2) \\|\\mathbf{x}\\|^{\\beta-4} \\mathbf{x}\\mathbf{x}^T + \\|\\mathbf{x}\\|^{\\beta-2} \\mathbf{I}_d \\right]$$\n我们对一个典型点 $\\mathbf{x}$（满足 $\\|\\mathbf{x}\\| = r_d$）分析展开式的两项。\n\n第一项是 $T_1 = \\delta \\nabla f(\\mathbf{x})^T \\mathbf{Z}$。由于 $\\mathbf{Z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_d)$，其期望值为 $E[T_1] = 0$。其方差为：\n$$Var(T_1) = \\delta^2 Var(\\nabla f(\\mathbf{x})^T \\mathbf{Z}) = \\delta^2 E[(\\nabla f(\\mathbf{x})^T \\mathbf{Z})^2] = \\delta^2 \\nabla f(\\mathbf{x})^T E[\\mathbf{Z}\\mathbf{Z}^T] \\nabla f(\\mathbf{x}) = \\delta^2 \\|\\nabla f(\\mathbf{x})\\|^2$$\n$$\\|\\nabla f(\\mathbf{x})\\|^2 = \\|-\\beta \\|\\mathbf{x}\\|^{\\beta-2} \\mathbf{x}\\|^2 = \\beta^2 (r_d^{\\beta-2})^2 \\|\\mathbf{x}\\|^2 = \\beta^2 r_d^{2\\beta-4} r_d^2 = \\beta^2 r_d^{2\\beta-2}$$\n代入 $r_d = (d/\\beta)^{1/\\beta}$：\n$$Var(T_1) = \\delta^2 \\beta^2 \\left(\\left(\\frac{d}{\\beta}\\right)^{1/\\beta}\\right)^{2\\beta-2} = \\delta^2 \\beta^2 \\left(\\frac{d}{\\beta}\\right)^{2-2/\\beta} = \\delta^2 \\beta^2 \\beta^{-2+2/\\beta} d^{2-2/\\beta} = \\delta^2 \\beta^{2/\\beta} d^{2-2/\\beta}$$\n使用给定的缩放 $\\delta = \\ell d^{-(\\beta-1)/\\beta} = \\ell d^{-1+1/\\beta}$，我们有 $\\delta^2 = \\ell^2 d^{-2+2/\\beta}$。\n$$Var(T_1) = (\\ell^2 d^{-2+2/\\beta}) (\\beta^{2/\\beta} d^{2-2/\\beta}) = \\ell^2 \\beta^{2/\\beta}$$\n这个方差在 $d$ 上是 $O(1)$ 的。我们记 $\\sigma^2 = \\ell^2 \\beta^{2/\\beta}$。\n\n第二项是 $T_2 = \\frac{1}{2}\\delta^2 \\mathbf{Z}^T H_f(\\mathbf{x}) \\mathbf{Z}$。在高维极限下，这一项依概率收敛于其均值。\n$$E[T_2] = \\frac{1}{2}\\delta^2 E[\\mathbf{Z}^T H_f(\\mathbf{x}) \\mathbf{Z}] = \\frac{1}{2}\\delta^2 E[\\text{Tr}(\\mathbf{Z}^T H_f(\\mathbf{x}) \\mathbf{Z})] = \\frac{1}{2}\\delta^2 \\text{Tr}(H_f(\\mathbf{x}) E[\\mathbf{Z}\\mathbf{Z}^T]) = \\frac{1}{2}\\delta^2 \\text{Tr}(H_f(\\mathbf{x}))$$\n海森矩阵的迹为：\n$$\\text{Tr}(H_f(\\mathbf{x})) = -\\beta \\left[ (\\beta-2) \\|\\mathbf{x}\\|^{\\beta-4} \\text{Tr}(\\mathbf{x}\\mathbf{x}^T) + \\|\\mathbf{x}\\|^{\\beta-2} \\text{Tr}(\\mathbf{I}_d) \\right]$$\n由于 $\\text{Tr}(\\mathbf{x}\\mathbf{x}^T) = \\|\\mathbf{x}\\|^2 = r_d^2$ 且 $\\text{Tr}(\\mathbf{I}_d) = d$：\n$$\\text{Tr}(H_f(\\mathbf{x})) = -\\beta \\left[ (\\beta-2) r_d^{\\beta-4} r_d^2 + d r_d^{\\beta-2} \\right] = -\\beta r_d^{\\beta-2} (d + \\beta - 2)$$\n对于大的 $d$，$d+\\beta-2 \\approx d$。因此，$\\text{Tr}(H_f(\\mathbf{x})) \\approx -\\beta d r_d^{\\beta-2}$。\n代入 $r_d^{\\beta-2} = (d/\\beta)^{(\\beta-2)/\\beta} = d^{1-2/\\beta} \\beta^{-1+2/\\beta}$：\n$$\\text{Tr}(H_f(\\mathbf{x})) \\approx -\\beta d (d^{1-2/\\beta} \\beta^{-1+2/\\beta}) = -\\beta^{2/\\beta} d^{2-2/\\beta}$$\n现在我们计算第二项的期望值：\n$$E[T_2] \\approx \\frac{1}{2} (\\ell^2 d^{-2+2/\\beta}) (-\\beta^{2/\\beta} d^{2-2/\\beta}) = -\\frac{1}{2} \\ell^2 \\beta^{2/\\beta} = -\\frac{1}{2}\\sigma^2$$\n当 $d \\to \\infty$ 时，对数比率 $U$ 依分布收敛到一个正态随机变量，其均值为 $\\mu = E[T_2] = -\\frac{1}{2}\\sigma^2$，方差为 $Var(T_1) = \\sigma^2$。\n$$U \\xrightarrow{d} \\mathcal{N}\\left(-\\frac{1}{2}\\sigma^2, \\sigma^2\\right)$$\n\n最后，我们计算期望接受概率 $E[\\alpha] = E[\\min(1, \\exp(U))]$。令 $\\Phi(z)$ 表示标准正态分布 $\\mathcal{N}(0, 1)$ 的累积分布函数。\n$$E[\\alpha] = \\int_{-\\infty}^{\\infty} \\min(1, e^u) \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(u - \\mu)^2}{2\\sigma^2}\\right) du$$\n$$E[\\alpha] = \\int_{-\\infty}^{0} e^u p(u) du + \\int_{0}^{\\infty} 1 \\cdot p(u) du$$\n其中 $p(u)$ 是 $U$ 的密度。\n第二个积分是 $P(U>0) = 1 - \\Phi((0-\\mu)/\\sigma) = \\Phi(\\mu/\\sigma)$。\n第一个积分是 $E[e^U \\mathbf{1}_{U0}]$。这可以使用正态分布的矩生成函数来计算。对于 $U \\sim \\mathcal{N}(\\mu, \\sigma^2)$，结果是 $E[\\min(1, \\exp(U))] = \\exp(\\mu + \\sigma^2/2) \\Phi(-\\sigma - \\mu/\\sigma) + \\Phi(\\mu/\\sigma)$。\n代入我们的渐近均值 $\\mu = -\\frac{1}{2}\\sigma^2$：\n$$E[\\alpha] = \\exp\\left(-\\frac{1}{2}\\sigma^2 + \\frac{\\sigma^2}{2}\\right) \\Phi\\left(-\\sigma - \\frac{-\\sigma^2/2}{\\sigma}\\right) + \\Phi\\left(\\frac{-\\sigma^2/2}{\\sigma}\\right)$$\n$$E[\\alpha] = \\exp(0) \\Phi\\left(-\\sigma + \\frac{\\sigma}{2}\\right) + \\Phi\\left(-\\frac{\\sigma}{2}\\right)$$\n$$E[\\alpha] = \\Phi\\left(-\\frac{\\sigma}{2}\\right) + \\Phi\\left(-\\frac{\\sigma}{2}\\right) = 2\\Phi\\left(-\\frac{\\sigma}{2}\\right)$$\n最后一步是代入 $\\sigma = \\sqrt{\\ell^2 \\beta^{2/\\beta}} = \\ell \\beta^{1/\\beta}$。\n$$E[\\alpha] \\approx 2\\Phi\\left(-\\frac{\\ell \\beta^{1/\\beta}}{2}\\right)$$\n\n定性地看，缩放规则为 $\\delta \\propto d^{-p(\\beta)}$，其中指数为 $p(\\beta) = (\\beta-1)/\\beta = 1 - 1/\\beta$。参数 $\\beta$ 控制目标分布的尾部行为。较大的 $\\beta$ 对应于更轻的尾部（一种下降更陡峭的“更硬”的密度）。对于 $\\beta > 0$，函数 $p(\\beta)$ 是一个增函数。例如，对于 $\\beta=2$（在此情境下为类高斯目标），指数为 $1/2$。当 $\\beta \\rightarrow \\infty$（接近球上的均匀分布）时，指数 $p(\\beta) \\rightarrow 1$。这意味着，对于具有更轻尾部（更大的 $\\beta$）的目标，提议步长 $\\delta$ 必须随着维度 $d$ 的增加而更快地收缩，以维持非零的接受率。这是因为“更硬”的密度剖面对移动更敏感，使得提议的步长更有可能落入概率极低的区域，从而导致拒绝。因此，对于这个族中尾部较轻的分布，“维度灾难”更为严重，需要在 $d$ 增长时更大幅度地减小步长。最优缩放是通过调整 $\\ell$ 来平衡接受率和步长来实现的，通常目标接受率约为 $0.234$。",
            "answer": "$$ \\boxed{2\\Phi\\left(-\\frac{\\ell\\beta^{1/\\beta}}{2}\\right)} $$"
        },
        {
            "introduction": "在理解了统计理论之后，本练习将重点转向在高维情况下实现AM算法的实际挑战。您将分析算法核心部分的计算复杂度（浮点运算次数），这对于理解并设计如周期性更新和低秩近似等计算捷径至关重要。这些优化策略使得AM算法在解决大规模问题时变得切实可行 。",
            "id": "3353639",
            "problem": "考虑由 Haario、Saksman 和 Tamminen 提出的自适应 Metropolis (AM) 算法，该算法用于马尔可夫链蒙特卡洛 (MCMC) 方法，其目标分布在 $\\mathbb{R}^{d}$ 上，在第 $n$ 次迭代时的状态为 $X_{n} \\in \\mathbb{R}^{d}$。AM 提议的形式为 $Y_{n} = X_{n} + s L_{n} Z_{n}$，其中 $s > 0$ 是一个标量，$Z_{n} \\sim \\mathcal{N}(0, I_{d})$，$L_{n}$ 是一个 Cholesky 因子，使得 $C_{n} = L_{n} L_{n}^{\\top}$ 近似于目标协方差。协方差的更新基于链的经验协方差和一个 Robbins-Monro 步长 $\\eta_{n} \\in (0,1)$，并在对角线上施加一个小的正则化。设均值更新为 $ \\mu_{n+1} = \\mu_{n} + \\eta_{n} (X_{n} - \\mu_{n})$，协方差更新为\n$$\nC_{n+1} = (1 - \\eta_{n}) C_{n} + \\eta_{n} \\left( (X_{n} - \\mu_{n})(X_{n} - \\mu_{n})^{\\top} \\right) + \\eta_{n} \\epsilon I_{d},\n$$\n其中 $\\epsilon > 0$ 是一个很小的常数。假设一个标准的浮点成本模型，其中一个稠密的 $d \\times d$ 矩阵与向量的乘法成本为 $2d^2$ 次浮点运算（flops），一个下三角 $d \\times d$ 矩阵的乘法成本恰好为 $d^2$ 次 flops，两个 $d$ 维向量的稠密外积成本为 $d^2$ 次 flops，一个 $d \\times d$ 矩阵的加法或缩放成本为 $d^2$ 次 flops，一个 $d$ 维向量的加法或缩放成本为 $d$ 次 flops，一个稠密对称正定 $d \\times d$ 矩阵的 Cholesky 分解成本为 $\\frac{1}{3}d^3$ 次 flops。忽略随机数生成成本和目标密度评估成本；只关注提议生成和自适应调整的线性代数工作。\n\n提出了两种降低成本的策略：\n\n- 周期性协方差分解：不​​在每次迭代中重新计算 $L_{n}$，而是每 $m \\in \\mathbb{N}$ 次迭代重新计算一次 $C_{n}$ 的完整 Cholesky 分解，并在其间重用最新的 $L_{n}$。将 Cholesky 成本摊销到 $m$ 次迭代中，视为每次迭代 $\\frac{d^3}{3m}$ 次 flops。\n\n- 低秩协方差近似：维持一个近似 $C_{n} \\approx D_{n} + U_{n} U_{n}^{\\top}$，其中 $D_{n} \\in \\mathbb{R}^{d \\times d}$ 是对角矩阵，$U_{n} \\in \\mathbb{R}^{d \\times r}$ 是秩为 $r$ 的矩阵，且 $1 \\le r \\ll d$。对于提议生成，抽取 $Z_{r} \\sim \\mathcal{N}(0, I_{r})$ 和 $Z_{d} \\sim \\mathcal{N}(0, I_{d})$，然后计算 $Y_{n} = X_{n} + s \\left( U_{n} Z_{r} + \\sqrt{D_{n}} \\odot Z_{d} \\right)$，其中 $\\odot$ 表示逐元素乘法，$\\sqrt{D_{n}}$ 表示对角线项的逐元素平方根。对于此低秩提议，使用以下运算计数：计算 $U_{n} Z_{r}$ 的成本为 $2dr$ 次 flops，计算 $\\sqrt{D_{n}} \\odot Z_{d}$ 的成本为 $d$ 次 flops，向量加法和缩放增加 $3d$ 次 flops，因此总提议生成成本为 $2dr + 4d$ 次 flops。对于低秩协方差更新，假设每次迭代有以下步骤和成本：以 $d$ 次 flops 计算 $v_{n} = X_{n} - \\mu_{n}$，以 $2d$ 次 flops 更新 $\\mu_{n+1} = \\mu_{n} + \\eta_{n} v_{n}$，从秩一项的对角线以 $2d$ 次 flops 更新 $D_{n+1}$，并通过秩为 $r$ 的增量投影和增广以 $2dr + r^2$ 次 flops 更新 $U_{n+1}$。因此，每次迭代的总低秩自适应调整成本为 $2dr + r^2 + 5d$ 次 flops。继续每 $m$ 次迭代周期性地对 $C_{n}$ 执行完整的 Cholesky 分解以防止长期漂移，因此摊销的周期性成本 $\\frac{d^3}{3m}$ 如上所述同样适用。\n\n从这些基本定义和成本模型出发，推导出当两种策略（每次迭代进行低秩提议和协方差更新，以及每 $m$ 次迭代进行完整的 Cholesky 重新计算）联合使用时，每次迭代的摊销浮点运算次数（记为 $C_{\\mathrm{lr}}(d, m, r)$）的闭式解。将最终答案表示为关于 $d$、$m$ 和 $r$ 的单个解析表达式。不要简化为大$\\mathcal{O}$表示法；给出在所述浮点运算模型下的精确表达式。不需要数值舍入，答案中也不报告单位。",
            "solution": "自适应 Metropolis 算法通过从一个协方差与当前估计 $C_{n}$ 成正比的高斯分布中抽样来构造提议。每次迭代的计算负担来自三个来源：使用协方差因子的提议生成、均值和协方差的自适应更新，以及偶尔为获得新的 Cholesky 因子而对协方差进行的重新分解。为了推导同时使用周期性分解和低秩协方差近似时每次迭代摊销浮点运算次数的闭式表达式，我们将每个部分的贡献相加。\n\n在全秩设置下，基本的 AM 提议是 $Y_{n} = X_{n} + s L_{n} Z_{n}$，其中 $C_{n} = L_{n} L_{n}^{\\top}$ 且 $Z_{n} \\sim \\mathcal{N}(0, I_{d})$。一个下三角 $d \\times d$ 矩阵与一个 $d$ 维向量相乘的成本恰好是 $d^2$ 次 flops（有 $d(d+1)/2$ 次乘法和 $d(d-1)/2$ 次加法），而缩放和相加 $d$ 维向量的成本是 $2d$ 次 flops，因此全秩提议的采样成本是 $d^2 + 2d$ 次 flops。均值更新 $\\mu_{n+1} = \\mu_{n} + \\eta_{n} (X_{n} - \\mu_{n})$ 是一个向量运算，需要以 $d$ 次 flops 计算 $v_{n} = X_{n} - \\mu_{n}$，以及以 $2d$ 次 flops 进行缩放和加法，总共 $3d$ 次 flops。协方差更新\n$$\nC_{n+1} = (1 - \\eta_{n}) C_{n} + \\eta_{n} \\left( v_{n} v_{n}^{\\top} \\right) + \\eta_{n} \\epsilon I_{d}\n$$\n需要以下操作：以 $d^2$ 次 flops 计算外积 $v_{n} v_{n}^{\\top}$，以 $d^2$ 次 flops 将 $C_{n}$ 乘以 $(1 - \\eta_{n})$，以 $d^2$ 次 flops 相加两个 $d \\times d$ 矩阵，以及以 $d$ 次 flops 添加对角线正则化项。加上均值更新的 $3d$ 次 flops，全秩情况下的总自适应调整成本为每次迭代 $3d^2 + 3d$ 次 flops。如果每次迭代都重新计算 Cholesky 分解 $L_{n}$，则每次迭代的成本还将包括 Cholesky 分解的 $\\frac{1}{3}d^3$ 次 flops；然而，通过每 $m$ 次迭代进行周期性分解，每次迭代的摊销 Cholesky 成本为 $\\frac{d^3}{3m}$ 次 flops。\n\n现在我们分析低秩策略。协方差近似为 $C_{n} \\approx D_{n} + U_{n} U_{n}^{\\top}$，其中 $D_{n}$ 是对角矩阵，$U_{n}$ 有 $r$ 列。提议使用两个独立的标准正态分布，$Z_{r} \\sim \\mathcal{N}(0, I_{r})$ 和 $Z_{d} \\sim \\mathcal{N}(0, I_{d})$，并计算\n$$\nY_{n} = X_{n} + s \\left( U_{n} Z_{r} + \\sqrt{D_{n}} \\odot Z_{d} \\right).\n$$\n$U_{n} Z_{r}$（一个稠密的 $d \\times r$ 矩阵乘以一个 $r$ 维向量）的成本是 $2dr$ 次 flops；$\\sqrt{D_{n}} \\odot Z_{d}$ 是一个对角缩放，成本为 $d$ 次 flops；三个 $d$ 维向量操作（两个分量的相加、乘以 $s$ 进行缩放、以及与 $X_{n}$ 相加）的成本是 $3d$ 次 flops。因此，在低秩近似下，提议生成的成本为每次迭代 $2dr + 4d$ 次 flops。\n\n对于低秩协方差更新，我们保留之前的均值更新成本 $3d$ 次 flops，其中以 $d$ 次 flops 计算 $v_{n} = X_{n} - \\mu_{n}$，以 $2d$ 次 flops 进行缩放和加法。对角矩阵 $D_{n}$ 捕获了低秩部分未表示的方差；用秩一项 $\\eta_{n} v_{n} v_{n}^{\\top}$ 的对角线来更新 $D_{n}$ 需要对 $d$ 个条目进行逐元素操作，模型化为 $2d$ 次 flops（一次用于计算 $v_{n} \\odot v_{n}$，一次用于缩放和加法）。更新因子 $U_{n}$ 以反映协方差的非对角结构可以通过一个增量秩-$r$ 过程来完成（例如，将 $v_{n}$ 投影到 $U_{n}$ 的当前生成空间上，然后进行校正和截断），其主要成本是一个 $d \\times r$ 矩阵与一个 $r$ 维向量的乘法以及相关的小型 $r \\times r$ 运算。我们将其模型化为每次迭代 $2dr + r^2$ 次 flops。将这些部分相加，每次迭代的低秩自适应调整成本为\n$$\n\\text{adapt}_{\\mathrm{lr}}(d, r) = d + 2d + 2d + (2dr + r^2) = 2dr + r^2 + 5d.\n$$\n\n为防止漂移在低秩近似中累积，我们仍然每 $m$ 次迭代对当前的 $C_{n}$ 执行周期性的完整 Cholesky 分解。这种周期性分解的摊销成本是每次迭代 $\\frac{d^3}{3m}$ 次 flops。\n\n将联合策略的三个组成部分——低秩提议生成、低秩自适应调整和摊销的周期性完整 Cholesky 分解——结合起来，总的每次迭代摊销浮点运算次数为\n$$\nC_{\\mathrm{lr}}(d, m, r) = \\underbrace{(2dr + 4d)}_{\\text{提议}} + \\underbrace{(2dr + r^2 + 5d)}_{\\text{自适应调整}} + \\underbrace{\\frac{d^3}{3m}}_{\\text{摊销的 Cholesky}}.\n$$\n化简得，\n$$\nC_{\\mathrm{lr}}(d, m, r) = 4dr + r^2 + 9d + \\frac{d^3}{3m}.\n$$\n\n这是在所述的浮点运算模型下，当使用秩为 $r$ 的低秩提议和协方差更新，并结合每 $m$ 次迭代的周期性完整 Cholesky 重新计算时，每次迭代的闭式摊销浮点运算次数。它明确了其中的权衡：降低 $r$ 会减小 $4dr + r^2$ 项，增加 $m$ 会减小 $\\frac{d^3}{3m}$ 项，而线性项 $9d$ 则反映了每次迭代不可避免的 $d$ 维向量运算。",
            "answer": "$$\\boxed{4 d r + r^{2} + 9 d + \\frac{d^{3}}{3 m}}$$"
        }
    ]
}