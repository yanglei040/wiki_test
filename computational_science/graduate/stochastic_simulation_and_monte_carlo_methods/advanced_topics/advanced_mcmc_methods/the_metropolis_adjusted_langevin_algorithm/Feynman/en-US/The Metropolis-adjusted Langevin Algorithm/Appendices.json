{
    "hands_on_practices": [
        {
            "introduction": "The best way to truly understand an algorithm is to build it from first principles. This practice guides you through the complete process of creating a Metropolis-Adjusted Langevin Algorithm (MALA) sampler, from theoretical derivation to implementation and testing. By working through the derivation of the acceptance probability and coding the algorithm for a standard Gaussian target, you will solidify your understanding of how the gradient-based proposal and the asymmetric Metropolis-Hastings correction work together.",
            "id": "3355276",
            "problem": "You are to work with the Metropolis-adjusted Langevin algorithm (MALA) at an advanced graduate level. Your tasks combine derivation from first principles, algorithm design, and implementation with a deterministic test harness.\n\nStarting from the overdamped Langevin diffusion defined by the stochastic differential equation $dX_t = \\frac{1}{2}\\nabla \\log \\pi(X_t)\\,dt + dW_t$, where $W_t$ is a standard Wiener process and $\\pi$ is a target density known up to normalization, the Unadjusted Langevin Algorithm arises by applying Euler–Maruyama with step size $h>0$. To correct the discretization bias introduced by Euler–Maruyama and recover $\\pi$ as the invariant distribution, one can use a Metropolis–Hastings accept/reject step with an asymmetric proposal given by a Gaussian distribution centered at a drifted point that involves $\\nabla \\log \\pi$. This results in the Metropolis-adjusted Langevin algorithm (MALA). The proposal distribution is $q_h(\\cdot \\mid x) = \\mathcal{N}(x + \\tfrac{h}{2}\\nabla \\log \\pi(x),\\, h I_d)$, where $I_d$ is the $d\\times d$ identity matrix.\n\nTask A (derivation and pseudocode):\n- Derive the MALA acceptance probability by combining the Metropolis–Hastings rule with the proposal kernel $q_h(\\cdot \\mid x)$ described above, starting only from:\n  - The definition of the overdamped Langevin diffusion,\n  - The Euler–Maruyama discretization,\n  - The Metropolis–Hastings acceptance probability $\\alpha(x,y) = \\min\\{1, \\frac{\\pi(y) q_h(x\\mid y)}{\\pi(x) q_h(y\\mid x)}\\}$.\n- Provide clear pseudocode for one iteration of MALA that emphasizes:\n  - The explicit computation of the gradient $\\nabla \\log \\pi(x)$,\n  - The sampling of a Gaussian vector,\n  - The Metropolis–Hastings acceptance calculation.\n- Annotate the per-iteration computational complexity in terms of the dimension $d$, counting:\n  - How many gradient evaluations are performed per iteration,\n  - How many Gaussian random vectors of dimension $d$ are sampled per iteration,\n  - The dominant complexity class in $d$ (e.g., $O(d)$, $O(d^2)$), assuming that evaluating $\\nabla \\log \\pi(x)$ for the test target below costs $O(d)$ operations.\n\nTask B (implementation and testing):\n- Implement a program that executes MALA for the $d$-dimensional standard normal target, i.e., $\\pi(x) \\propto \\exp(-\\tfrac{1}{2}\\|x\\|_2^2)$, so that $\\log \\pi(x)$ can be used up to an additive constant, with gradient $\\nabla \\log \\pi(x) = -x$.\n- Use the proposal kernel with step size $h>0$:\n  - Proposal mean $\\mu(x) = x + \\tfrac{h}{2}\\nabla \\log \\pi(x)$,\n  - Proposal covariance $h I_d$.\n- Use the Metropolis–Hastings acceptance probability constructed from $q_h(y\\mid x)$ and $q_h(x\\mid y)$, and accept or reject accordingly. If rejected, the state remains unchanged.\n- Initialize the random number generator with seed $12345$ for reproducibility. Compute the empirical mean $\\bar{x}_T = \\frac{1}{T}\\sum_{t=1}^T X_t$, where $X_t$ is the chain state after the $t$-th accept/reject step. Report the Euclidean norm $\\|\\bar{x}_T\\|_2$.\n- For each iteration, count the number of gradient evaluations and the number of sampled Gaussian $d$-vectors. For this implementation, do not cache gradients across iterations; compute what is needed for the proposal and acceptance within the same iteration.\n\nTest suite:\nRun your program on the following three cases. For each case, the initial state is $x_0$ as specified, the dimension is $d$, the step size is $h$, and the number of iterations is $T$.\n1) Case $1$: $d=1$, $h=0.5$, $T=6000$, $x_0 = [3.0]$.\n2) Case $2$: $d=10$, $h=0.3$, $T=8000$, $x_0 = [2.0,2.0,\\dots,2.0]$ (ten entries).\n3) Case $3$: $d=1$, $h=5.0$, $T=4000$, $x_0 = [0.0]$.\n\nRequired outputs per test case:\n- The acceptance rate as a float,\n- The Euclidean norm $\\|\\bar{x}_T\\|_2$ of the empirical mean as a float,\n- The number of gradient evaluations per iteration as an integer,\n- The number of sampled Gaussian $d$-vectors per iteration as an integer,\n- The dominant complexity exponent for this target as an integer equal to $1$ for $O(d)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all three cases as a comma-separated list enclosed in square brackets, in the order of the test cases, and flattened. That is, the output should be a list of $15$ numbers: for each case, print the five outputs in order, then concatenate across cases.\n- Floats must be rounded to exactly six decimal places. Integers must be printed without decimal places.\n- Example of formatting (illustrative only): $[\\text{acc}_1,\\|\\bar{x}\\|_{1},g_1,z_1,e_1,\\text{acc}_2,\\|\\bar{x}\\|_{2},g_2,z_2,e_2,\\text{acc}_3,\\|\\bar{x}\\|_{3},g_3,z_3,e_3]$.\n\nAngles and physical units:\n- No angles or physical units are involved.\n\nYour program must be self-contained, accept no input, use the seed $12345$, and adhere to the output format described above. The only allowed libraries are the Python standard library, NumPy, and SciPy; you may choose not to use SciPy.",
            "solution": "The problem requires a derivation of the Metropolis-Adjusted Langevin Algorithm (MALA) acceptance probability, the creation of its pseudocode, an analysis of its computational complexity, and finally, its implementation and testing on a specific target distribution.\n\n### Task A: Derivation, Pseudocode, and Complexity Analysis\n\n#### Derivation of the MALA Acceptance Probability\n\nThe Metropolis-Adjusted Langevin Algorithm is a Metropolis-Hastings (MH) algorithm designed to sample from a target probability distribution with density $\\pi(x)$, known up to a normalization constant. MALA uses a proposal mechanism inspired by the discretization of the overdamped Langevin diffusion Stochastic Differential Equation (SDE):\n$$\ndX_t = \\frac{1}{2}\\nabla \\log \\pi(X_t)\\,dt + dW_t\n$$\nwhere $W_t$ is a standard $d$-dimensional Wiener process. Applying the Euler-Maruyama discretization with a step size $h>0$ yields a proposal $y$ from the current state $x$:\n$$\ny = x + \\frac{h}{2}\\nabla \\log \\pi(x) + \\sqrt{h}Z\n$$\nwhere $Z \\sim \\mathcal{N}(0, I_d)$ is a standard $d$-dimensional normal random vector. This defines an asymmetric proposal kernel $q_h(y \\mid x)$, which is the probability density of a normal distribution, $y \\sim \\mathcal{N}(\\mu(x), \\Sigma)$, with mean $\\mu(x) = x + \\frac{h}{2}\\nabla \\log \\pi(x)$ and covariance $\\Sigma = hI_d$. The probability density function is:\n$$\nq_h(y \\mid x) = \\frac{1}{(2\\pi h)^{d/2}} \\exp\\left(-\\frac{1}{2h} \\|y - \\mu(x)\\|_2^2\\right)\n$$\n\nThe Metropolis-Hastings acceptance probability $\\alpha(x, y)$ for a move from state $x$ to a proposed state $y$ is given by:\n$$\n\\alpha(x,y) = \\min\\left\\{1, \\frac{\\pi(y) q_h(x\\mid y)}{\\pi(x) q_h(y\\mid x)}\\right\\}\n$$\nTo make this expression practical, we work with the logarithm of the ratio, often called the log-acceptance ratio, $\\log R$:\n$$\n\\log R = \\log\\left(\\frac{\\pi(y)}{\\pi(x)}\\right) + \\log\\left(\\frac{q_h(x\\mid y)}{q_h(y\\mid x)}\\right)\n$$\nThe first term is the difference in the log-target densities: $\\log\\pi(y) - \\log\\pi(x)$.\n\nThe second term involves the ratio of proposal densities. The log-proposal densities are:\n$$\n\\log q_h(y \\mid x) = -\\frac{d}{2}\\log(2\\pi h) - \\frac{1}{2h} \\left\\|y - \\left(x + \\frac{h}{2}\\nabla \\log \\pi(x)\\right)\\right\\|_2^2 \\\\\n\\log q_h(x \\mid y) = -\\frac{d}{2}\\log(2\\pi h) - \\frac{1}{2h} \\left\\|x - \\left(y + \\frac{h}{2}\\nabla \\log \\pi(y)\\right)\\right\\|_2^2\n$$\nThe term $-\\frac{d}{2}\\log(2\\pi h)$ is a constant that cancels in the difference $\\log q_h(x \\mid y) - \\log q_h(y \\mid x)$:\n$$\n\\log\\left(\\frac{q_h(x\\mid y)}{q_h(y\\mid x)}\\right) = -\\frac{1}{2h} \\left( \\left\\|x - \\left(y + \\frac{h}{2}\\nabla \\log \\pi(y)\\right)\\right\\|_2^2 - \\left\\|y - \\left(x + \\frac{h}{2}\\nabla \\log \\pi(x)\\right)\\right\\|_2^2 \\right)\n$$\nCombining all parts, the log-acceptance ratio is:\n$$\n\\log R(x,y) = \\log\\pi(y) - \\log\\pi(x) - \\frac{1}{2h} \\left( \\left\\|x - y - \\frac{h}{2}\\nabla \\log \\pi(y)\\right\\|_2^2 - \\left\\|y - x - \\frac{h}{2}\\nabla \\log \\pi(x)\\right\\|_2^2 \\right)\n$$\nThe final acceptance probability is $\\alpha(x,y) = \\min\\{1, \\exp(\\log R(x,y))\\}$. For numerical implementation, it is better to compare $\\log u$ with $\\log R(x,y)$, where $u \\sim \\text{Uniform}(0,1)$.\n\nA more efficient way to compute the log-proposal ratio term arises from observing that $y - (x + \\frac{h}{2}\\nabla \\log \\pi(x)) = \\sqrt{h}Z$, where $Z$ is the standard normal variate used to generate the proposal. Thus, the squared norm is $h\\|Z\\|_2^2$, and $\\log q_h(y \\mid x) = C - \\frac{1}{2}\\|Z\\|_2^2$. This simplifies the log-acceptance ratio to:\n$$\n\\log R(x,y) = \\log\\pi(y) - \\log\\pi(x) - \\frac{1}{2h} \\left\\|x - y - \\frac{h}{2}\\nabla \\log \\pi(y)\\right\\|_2^2 + \\frac{1}{2}\\|Z\\|_2^2\n$$\nThis form avoids one norm calculation and can be more numerically stable.\n\n#### Pseudocode for One MALA Iteration\n\nGiven the current state $x^{(t)}$, step size $h$, and log-target density function $\\log\\pi$:\n\n1.  **Compute Gradient at Current State**: $g^{(t)} \\leftarrow \\nabla \\log \\pi(x^{(t)})$.\n2.  **Form Proposal Mean**: $\\mu^{(t)} \\leftarrow x^{(t)} + \\frac{h}{2} g^{(t)}$.\n3.  **Sample Proposal**:\n    a. Sample a random vector $Z^{(t)} \\sim \\mathcal{N}(0, I_d)$.\n    b. Generate the proposal state $y \\leftarrow \\mu^{(t)} + \\sqrt{h} Z^{(t)}$.\n4.  **Compute Gradient at Proposal State**: $g_y \\leftarrow \\nabla \\log \\pi(y)$.\n5.  **Calculate Log-Acceptance Ratio**:\n    a. Compute forward log-proposal term: $\\log q_{fwd} \\leftarrow -\\frac{1}{2} \\|Z^{(t)}\\|_2^2$.\n    b. Compute reverse log-proposal term: $\\log q_{rev} \\leftarrow -\\frac{1}{2h} \\|x^{(t)} - y - \\frac{h}{2} g_y\\|_2^2$.\n    c. Compute log-ratio: $\\log R \\leftarrow (\\log\\pi(y) - \\log\\pi(x^{(t)})) + (\\log q_{rev} - \\log q_{fwd})$.\n6.  **Accept or Reject**:\n    a. Sample a uniform random number $u \\sim \\text{Uniform}(0,1)$.\n    b. If $\\log u < \\log R$:\n        Set $x^{(t+1)} \\leftarrow y$ (Accept).\n    c. Else:\n        Set $x^{(t+1)} \\leftarrow x^{(t)}$ (Reject).\n\n#### Computational Complexity Analysis\n\nWe analyze the complexity per iteration in terms of the state dimension $d$.\n\n-   **Gradient Evaluations per Iteration**: The algorithm requires the gradient at the current state, $\\nabla \\log \\pi(x^{(t)})$, to form the proposal. It then requires the gradient at the proposed state, $\\nabla \\log \\pi(y)$, to calculate the acceptance probability. As per the problem's explicit instruction not to cache gradients across iterations, these two evaluations are performed in every single iteration, regardless of whether the proposal is accepted or rejected.\n    - **Count**: $2$ gradient evaluations.\n\n-   **Gaussian Random Vectors Sampled per Iteration**: One $d$-dimensional standard normal vector $Z^{(t)}$ is sampled to generate the proposal $y$.\n    - **Count**: $1$ Gaussian vector sample.\n\n-   **Dominant Complexity Class**: We assume the cost of evaluating $\\nabla \\log \\pi(x)$ is $O(d)$, as specified for the test target.\n    1.  Step 1 (Gradient at $x^{(t)}$): $O(d)$.\n    2.  Step 2 (Proposal mean): Vector addition and scalar-vector multiplication are $O(d)$.\n    3.  Step 3 (Sample proposal): Sampling a $d$-dimensional normal vector is $O(d)$, followed by $O(d)$ vector operations.\n    4.  Step 4 (Gradient at $y$): $O(d)$.\n    5.  Step 5 (Log-acceptance ratio): This involves evaluating $\\log\\pi$ (typically $O(d)$ for separable densities like the target), vector norms ($\\|v\\|_2^2$ is $O(d)$), and vector arithmetic ($O(d)$). The total cost is $O(d)$.\n    6.  Step 6 (Accept/Reject): This is an $O(d)$ operation (vector copy).\n\nSince all steps have a complexity of at most $O(d)$, the dominant complexity class for one iteration of MALA is $O(d)$.\n- **Dominant Complexity Exponent**: For $O(d) = O(d^1)$, the exponent is $1$.",
            "answer": "```python\nimport numpy as np\n# No other libraries are needed for this problem.\n\ndef solve():\n    \"\"\"\n    Implements and tests the Metropolis-Adjusted Langevin Algorithm (MALA)\n    for a d-dimensional standard normal target distribution.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (d, h, T, x0)\n        (1, 0.5, 6000, np.array([3.0])),\n        (10, 0.3, 8000, np.full(10, 2.0)),\n        (1, 5.0, 4000, np.array([0.0])),\n    ]\n\n    results = []\n    \n    # Initialize the random number generator with a fixed seed for reproducibility.\n    rng = np.random.default_rng(12345)\n\n    for d, h, T, x0 in test_cases:\n        x_current = np.copy(x0)\n        x_sum = np.zeros(d)\n        accepted_count = 0\n\n        # The number of gradient evaluations and Gaussian samples are constant per iteration.\n        grad_evals_per_iter = 2\n        gauss_samples_per_iter = 1\n        complexity_exponent = 1 # O(d^1)\n\n        for _ in range(T):\n            # 1. Compute gradient at the current state.\n            # For pi(x) propto exp(-0.5*||x||^2), grad(log(pi(x))) = -x.\n            grad_x = -x_current\n\n            # 2. Form proposal by sampling from the proposal distribution.\n            # Proposal mean: mu = x + (h/2) * grad(log(pi(x)))\n            # Proposal distribution: y ~ N(mu, h*I)\n            mu_proposal = x_current + (h / 2.0) * grad_x\n            \n            # Sample a standard normal vector Z\n            z = rng.standard_normal(size=d)\n            y_proposal = mu_proposal + np.sqrt(h) * z\n\n            # 3. Compute gradient at the proposed state.\n            grad_y = -y_proposal\n\n            # 4. Calculate the log of the Metropolis-Hastings acceptance ratio.\n            # log R = log(pi(y)/pi(x)) + log(q(x|y)/q(y|x))\n\n            # Log-target densities (ignoring constants)\n            log_pi_x = -0.5 * np.dot(x_current, x_current)\n            log_pi_y = -0.5 * np.dot(y_proposal, y_proposal)\n\n            # Log-proposal densities (ignoring constants)\n            # Forward: q(y|x)\n            # We can use the generated z to simplify: y-mu_proposal = sqrt(h)*z\n            # The exponent term is -1/(2h) * ||y-mu_proposal||^2 = -1/(2h) * h*||z||^2 = -0.5*||z||^2\n            log_q_y_given_x = -0.5 * np.dot(z, z)\n            \n            # Reverse: q(x|y)\n            # Mean of reverse proposal: mu_rev = y + (h/2)*grad_y\n            mu_reverse = y_proposal + (h / 2.0) * grad_y\n            log_q_x_given_y = -1.0 / (2.0 * h) * np.sum((x_current - mu_reverse)**2)\n\n            # Total log acceptance ratio\n            log_alpha = (log_pi_y - log_pi_x) + (log_q_x_given_y - log_q_y_given_x)\n\n            # 5. Accept or reject the proposal.\n            if np.log(rng.uniform())  log_alpha:\n                x_current = y_proposal\n                accepted_count += 1\n            # If rejected, x_current remains the same.\n            \n            # Add the state of the chain (after accept/reject) to the sum.\n            x_sum += x_current\n\n        # After all iterations, compute the final statistics.\n        acceptance_rate = accepted_count / T\n        empirical_mean = x_sum / T\n        norm_of_mean = np.linalg.norm(empirical_mean)\n\n        results.extend([\n            round(acceptance_rate, 6),\n            round(norm_of_mean, 6),\n            grad_evals_per_iter,\n            gauss_samples_per_iter,\n            complexity_exponent\n        ])\n\n    # Format the final output string as specified.\n    final_output = f\"[{','.join(map(str, results))}]\"\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "The efficiency and stability of MALA are closely tied to the numerical integrator used to discretize the underlying Langevin SDE. This advanced practice challenges you to move beyond the standard Euler-Maruyama scheme and explore an implicit midpoint integrator, which often provides better stability for \"stiff\" potentials. You will derive the modified acceptance probability, which now includes a Jacobian term from the implicit change of variables, and then implement and compare the performance of the two schemes. This exercise highlights the deep connection between numerical analysis and MCMC algorithm design, showing how more sophisticated integrators can lead to significant performance gains.",
            "id": "3355207",
            "problem": "Consider a one-dimensional target probability density $\\pi(x)$ on $\\mathbb{R}$, defined up to normalization by $\\pi(x) \\propto \\exp(-U(x))$, where the potential is $U(x) = \\frac{k}{2} x^2 + \\frac{\\lambda}{4} x^4$ with stiffness parameters $k  0$ and $\\lambda \\ge 0$. The overdamped Langevin diffusion that preserves $\\pi$ has the form $dX_t = \\nabla \\log \\pi(X_t)\\, dt + \\sqrt{2}\\, dW_t$, where $W_t$ is standard Brownian motion. Two proposal mechanisms for Metropolis–Hastings (MH) are to be compared:\n\n1. Euler–Maruyama proposal: $Y = X + h \\nabla \\log \\pi(X) + \\sqrt{2h}\\, \\xi$ with $\\xi \\sim \\mathcal{N}(0,1)$ and step size $h  0$.\n\n2. Implicit midpoint (Crank–Nicolson) proposal: $Y$ is defined implicitly by $Y = X + \\frac{h}{2} \\left[\\nabla \\log \\pi(X) + \\nabla \\log \\pi(Y)\\right] + \\sqrt{2h}\\, \\xi$, with the same $\\xi \\sim \\mathcal{N}(0,1)$.\n\nStarting from the fundamentals of Metropolis–Hastings detailed balance and change-of-variables for probability densities, derive the acceptance probability for each proposal. For the implicit midpoint method, the derivation must include the Jacobian factor arising from the transformation from the standard normal variable to the proposed state. Use the MH acceptance probability formula $\\alpha(x,y) = \\min\\left(1, \\frac{\\pi(y) q(y \\to x)}{\\pi(x) q(x \\to y)}\\right)$, where $q(\\cdot\\, \\to\\, \\cdot)$ is the proposal density, and express all terms explicitly in terms of $U$, its derivatives, and $h$.\n\nImplement both algorithms as Markov chain Monte Carlo (MCMC) samplers that:\n- Generate proposals using the specified discretizations.\n- Compute the correct acceptance probabilities derived from first principles.\n- Accept or reject proposals according to the MH rule.\n\nUse the following test suite of parameter sets to evaluate whether the higher-order implicit midpoint scheme reduces the rejection rate compared to Euler–Maruyama for stiff potentials. For each case, run a single chain of $N$ steps starting from $x_0 = 0$ with the given random number generator seed. All quantities are unitless.\n\n- Test Case 1 (happy path, moderately stiff quadratic): $(k, \\lambda, h, N, \\text{seed}) = (100, 0, 0.01, 10000, 123)$.\n- Test Case 2 (very stiff quadratic): $(k, \\lambda, h, N, \\text{seed}) = (1000, 0, 0.005, 10000, 456)$.\n- Test Case 3 (moderately stiff quartic): $(k, \\lambda, h, N, \\text{seed}) = (100, 10, 0.004, 10000, 789)$.\n- Test Case 4 (very stiff mixed quadratic–quartic with small step, boundary case): $(k, \\lambda, h, N, \\text{seed}) = (1000, 50, 0.001, 10000, 42)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true,true,true,true]\"), where each entry is a boolean indicating whether, for that test case, the implicit midpoint scheme produced a lower rejection fraction than Euler–Maruyama (true if the implicit midpoint had fewer rejections, false otherwise).",
            "solution": "The user wants to derive the Metropolis-Hastings acceptance probability for the preconditioned Metropolis-Adjusted Langevin Algorithm (MALA). The derivation must start from the underlying stochastic differential equation (SDE), use the Euler-Maruyama discretization to find the proposal density, and then apply the Metropolis-Hastings rule for asymmetric proposals.\n\n### Step 1: Problem Validation\n\n**1.1. Extract Givens**\n-   Target probability density: $\\pi(x)$ on $\\mathbb{R}^{d}$, known up to a normalizing constant.\n-   Log-density $\\ln\\pi(x)$ is continuously differentiable.\n-   Preconditioning matrix: $M \\in \\mathbb{R}^{d \\times d}$, symmetric and positive definite.\n-   Method: Metropolis-Hastings (MH) with a proposal based on preconditioned overdamped Langevin dynamics.\n-   Discretization: Euler–Maruyama scheme.\n-   Step size: $h0$, fixed.\n-   Task: Derive the proposal density $q(x' \\mid x)$ and the MH acceptance probability $\\alpha(x,x')$.\n\n**1.2. Validate Using Extracted Givens**\n-   **Scientific Grounding**: The problem is based on fundamental principles of stochastic calculus (Langevin SDE), numerical methods for SDEs (Euler-Maruyama), and statistical computation (Metropolis-Hastings algorithm). These are standard and well-established concepts in computational statistics and physics. The problem is scientifically sound.\n-   **Well-Posedness**: The problem is clearly stated and provides all necessary information to derive the requested quantity. The assumptions on $\\pi(x)$ and $M$ are standard and ensure that all mathematical operations (gradients, matrix inversions, etc.) are well-defined. A unique analytical solution exists.\n-   **Objectivity**: The problem is phrased in precise, objective mathematical language, free from ambiguity or subjective claims.\n\n**1.3. Verdict and Action**\nThe problem is valid as it is scientifically grounded, well-posed, and objective. I will proceed with a full derivation.\n\n### Step 2: Derivation of the Acceptance Probability\n\n**2.1. The Preconditioned Langevin SDE**\nThe overdamped Langevin SDE is a stochastic process whose stationary distribution is a given target density $\\pi(x)$. For a target density $\\pi(x) \\propto \\exp(-E(x))$, where $E(x)$ is the potential energy, the SDE describes the motion of a particle in this potential landscape under the influence of random thermal fluctuations.\n\nThe general form for a preconditioned Langevin SDE with a constant preconditioner $M$ that has $\\pi(x)$ as its invariant density is\n$$\ndX_t = \\frac{1}{2} M \\nabla \\ln \\pi(X_t) dt + \\sqrt{M} dW_t\n$$\nwhere $X_t \\in \\mathbb{R}^d$ is the state of the system at time $t$, $\\nabla \\ln \\pi(X_t)$ is the gradient of the log-target-density, $M$ is the symmetric positive definite preconditioning matrix, and $dW_t$ is a standard $d$-dimensional Wiener process (i.e., its increments are independent Gaussian variables with mean $0$ and covariance $I_d dt$, where $I_d$ is the $d \\times d$ identity matrix).\n\nThe diffusion term is $\\sqrt{M} dW_t$, where $\\sqrt{M}$ is the unique symmetric positive definite square root of $M$. The covariance of this noise term is $E[(\\sqrt{M} dW_t)(\\sqrt{M} dW_t)^T] = \\sqrt{M} E[dW_t dW_t^T] \\sqrt{M}^T = \\sqrt{M} (I_d dt) \\sqrt{M} = M dt$. The choice of the drift term, $\\frac{1}{2} M \\nabla \\ln \\pi(X_t)$, ensures that the detailed balance condition is satisfied at the SDE level, making $\\pi(x)$ the invariant density.\n\n**2.2. Discretization and Proposal Kernel**\nTo generate proposals for a Metropolis-Hastings algorithm, we discretize the SDE using the Euler-Maruyama scheme with a finite time step $h  0$. Let $x$ be the current state at step $n$, and $x'$ be the proposed state at step $n+1$. The discretization is given by:\n$$\nx' = x + h \\left(\\frac{1}{2} M \\nabla \\ln \\pi(x)\\right) + \\sqrt{h} \\sqrt{M} Z\n$$\nwhere $Z \\sim \\mathcal{N}(0, I_d)$ is a vector of $d$ independent standard normal random variables.\n\nThis equation defines the proposal mechanism. We can see that $x'$ is generated by adding a deterministic drift term and a random Gaussian fluctuation to the current state $x$.\n\n**2.3. The Gaussian Proposal Density $q(x' \\mid x)$**\nThe proposal rule can be rewritten to show that $x'$ is drawn from a multivariate Gaussian distribution. Let the mean of this distribution be $\\mu(x)$:\n$$\n\\mu(x) = x + \\frac{h}{2} M \\nabla \\ln \\pi(x)\n$$\nThe proposal $x'$ is then given by $x' = \\mu(x) + \\sqrt{hM}Z$. This shows that, conditioned on $x$, the proposed state $x'$ follows a Gaussian distribution with mean $\\mu(x)$ and covariance $\\Sigma = E[(\\sqrt{hM}Z)(\\sqrt{hM}Z)^T] = h M E[ZZ^T] = hM$.\n\nTherefore, the proposal density $q(x' \\mid x)$ is the probability density function of the multivariate normal distribution $\\mathcal{N}(\\mu(x), hM)$:\n$$\nq(x' \\mid x) = \\frac{1}{\\sqrt{(2\\pi)^d \\det(hM)}} \\exp\\left( -\\frac{1}{2} (x' - \\mu(x))^T (hM)^{-1} (x' - \\mu(x)) \\right)\n$$\nSubstituting the expression for $\\mu(x)$, we get:\n$$\nq(x' \\mid x) = \\frac{1}{\\sqrt{(2\\pi h)^d \\det(M)}} \\exp\\left( -\\frac{1}{2h} \\left(x' - x - \\frac{h}{2} M \\nabla \\ln\\pi(x)\\right)^T M^{-1} \\left(x' - x - \\frac{h}{2} M \\nabla \\ln\\pi(x)\\right) \\right)\n$$\nThis proposal density is generally not symmetric, i.e., $q(x' \\mid x) \\neq q(x \\mid x')$, because the drift term $\\frac{h}{2} M \\nabla \\ln \\pi(x)$ depends on the starting state $x$.\n\n**2.4. Metropolis-Hastings Acceptance Probability $\\alpha(x, x'$)**\nThe Metropolis-Hastings algorithm ensures that the resulting Markov chain has $\\pi(x)$ as its stationary distribution by introducing an acceptance-rejection step. The acceptance probability $\\alpha(x, x')$ for a proposed move from $x$ to $x'$ is given by:\n$$\n\\alpha(x, x') = \\min \\left( 1, \\frac{\\pi(x') q(x \\mid x')}{\\pi(x) q(x' \\mid x)} \\right)\n$$\nTo evaluate this, we need the reverse proposal density $q(x \\mid x')$, which is the probability of proposing a move from $x'$ to $x$. By symmetry of the derivation, it is given by:\n$$\nq(x \\mid x') = \\frac{1}{\\sqrt{(2\\pi h)^d \\det(M)}} \\exp\\left( -\\frac{1}{2h} \\left(x - x' - \\frac{h}{2} M \\nabla \\ln\\pi(x')\\right)^T M^{-1} \\left(x - x' - \\frac{h}{2} M \\nabla \\ln\\pi(x')\\right) \\right)\n$$\nThe ratio of the proposal densities is:\n$$\n\\frac{q(x \\mid x')}{q(x' \\mid x)} = \\frac{\\exp\\left( -\\frac{1}{2h} \\left(x - x' - \\frac{h}{2} M \\nabla \\ln\\pi(x')\\right)^T M^{-1} \\left(x - x' - \\frac{h}{2} M \\nabla \\ln\\pi(x')\\right) \\right)}{\\exp\\left( -\\frac{1}{2h} \\left(x' - x - \\frac{h}{2} M \\nabla \\ln\\pi(x)\\right)^T M^{-1} \\left(x' - x - \\frac{h}{2} M \\nabla \\ln\\pi(x)\\right) \\right)}\n$$\nNote that the normalization constants $\\frac{1}{\\sqrt{(2\\pi h)^d \\det(M)}}$ cancel out.\n\nSubstituting this ratio into the acceptance probability formula gives the final expression for $\\alpha(x, x')$. Since $\\pi(x)$ is known only up to a constant, we use the ratio $\\pi(x')/\\pi(x)$, which is computable. The final acceptance probability is:\n$$\n\\alpha(x, x') = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x)} \\frac{\\exp\\left( -\\frac{1}{2h} \\left(x - x' - \\frac{h}{2} M \\nabla \\ln\\pi(x')\\right)^T M^{-1} \\left(x - x' - \\frac{h}{2} M \\nabla \\ln\\pi(x')\\right) \\right)}{\\exp\\left( -\\frac{1}{2h} \\left(x' - x - \\frac{h}{2} M \\nabla \\ln\\pi(x)\\right)^T M^{-1} \\left(x' - x - \\frac{h}{2} M \\nabla \\ln\\pi(x)\\right) \\right)}\\right)\n$$\nThis expression can also be written more compactly by combining the exponents:\n$$\n\\alpha(x, x') = \\min\\left(1, \\exp\\left( \\ln\\left(\\frac{\\pi(x')}{\\pi(x)}\\right) + \\ln\\left(\\frac{q(x \\mid x')}{q(x' \\mid x)}\\right) \\right)\\right)\n$$\nwhere the log-ratio of proposals is given by:\n$$\n\\ln\\left(\\frac{q(x \\mid x')}{q(x' \\mid x)}\\right) = -\\frac{1}{2h} \\left[ \\left\\|x - x' - \\frac{h}{2}M\\nabla\\ln\\pi(x')\\right\\|_{M^{-1}}^2 - \\left\\|x' - x - \\frac{h}{2}M\\nabla\\ln\\pi(x)\\right\\|_{M^{-1}}^2 \\right]\n$$\nwith $\\|v\\|_{A}^2 = v^T A v$. The expression requested in the prompt is the explicit form first derived.",
            "answer": "$$\n\\boxed{\\min\\left(1, \\frac{\\pi(x')}{\\pi(x)} \\frac{\\exp\\left( -\\frac{1}{2h} \\left(x - x' - \\frac{h}{2} M \\nabla \\ln\\pi(x')\\right)^T M^{-1} \\left(x - x' - \\frac{h}{2} M \\nabla \\ln\\pi(x')\\right) \\right)}{\\exp\\left( -\\frac{1}{2h} \\left(x' - x - \\frac{h}{2} M \\nabla \\ln\\pi(x)\\right)^T M^{-1} \\left(x' - x - \\frac{h}{2} M \\nabla \\ln\\pi(x)\\right) \\right)}\\right)}\n$$"
        },
        {
            "introduction": "The efficiency and stability of MALA are closely tied to the numerical integrator used to discretize the underlying Langevin SDE. This advanced practice challenges you to move beyond the standard Euler-Maruyama scheme and explore an implicit midpoint integrator, which often provides better stability for \"stiff\" potentials. You will derive the modified acceptance probability, which now includes a Jacobian term from the implicit change of variables, and then implement and compare the performance of the two schemes. This exercise highlights the deep connection between numerical analysis and MCMC algorithm design, showing how more sophisticated integrators can lead to significant performance gains.",
            "id": "3355234",
            "problem": "Consider a one-dimensional target probability density $\\pi(x)$ on $\\mathbb{R}$, defined up to normalization by $\\pi(x) \\propto \\exp(-U(x))$, where the potential is $U(x) = \\frac{k}{2} x^2 + \\frac{\\lambda}{4} x^4$ with stiffness parameters $k  0$ and $\\lambda \\ge 0$. The overdamped Langevin diffusion that preserves $\\pi$ has the form $dX_t = \\nabla \\log \\pi(X_t)\\, dt + \\sqrt{2}\\, dW_t$, where $W_t$ is standard Brownian motion. Two proposal mechanisms for Metropolis–Hastings (MH) are to be compared:\n\n1. Euler–Maruyama proposal: $Y = X + h \\nabla \\log \\pi(X) + \\sqrt{2h}\\, \\xi$ with $\\xi \\sim \\mathcal{N}(0,1)$ and step size $h  0$.\n\n2. Implicit midpoint (Crank–Nicolson) proposal: $Y$ is defined implicitly by $Y = X + \\frac{h}{2} \\left[\\nabla \\log \\pi(X) + \\nabla \\log \\pi(Y)\\right] + \\sqrt{2h}\\, \\xi$, with the same $\\xi \\sim \\mathcal{N}(0,1)$.\n\nStarting from the fundamentals of Metropolis–Hastings detailed balance and change-of-variables for probability densities, derive the acceptance probability for each proposal. For the implicit midpoint method, the derivation must include the Jacobian factor arising from the transformation from the standard normal variable to the proposed state. Use the MH acceptance probability formula $\\alpha(x,y) = \\min\\left(1, \\frac{\\pi(y) q(y \\to x)}{\\pi(x) q(x \\to y)}\\right)$, where $q(\\cdot\\, \\to\\, \\cdot)$ is the proposal density, and express all terms explicitly in terms of $U$, its derivatives, and $h$.\n\nImplement both algorithms as Markov chain Monte Carlo (MCMC) samplers that:\n- Generate proposals using the specified discretizations.\n- Compute the correct acceptance probabilities derived from first principles.\n- Accept or reject proposals according to the MH rule.\n\nUse the following test suite of parameter sets to evaluate whether the higher-order implicit midpoint scheme reduces the rejection rate compared to Euler–Maruyama for stiff potentials. For each case, run a single chain of $N$ steps starting from $x_0 = 0$ with the given random number generator seed. All quantities are unitless.\n\n- Test Case 1 (happy path, moderately stiff quadratic): $(k, \\lambda, h, N, \\text{seed}) = (100, 0, 0.01, 10000, 123)$.\n- Test Case 2 (very stiff quadratic): $(k, \\lambda, h, N, \\text{seed}) = (1000, 0, 0.005, 10000, 456)$.\n- Test Case 3 (moderately stiff quartic): $(k, \\lambda, h, N, \\text{seed}) = (100, 10, 0.004, 10000, 789)$.\n- Test Case 4 (very stiff mixed quadratic–quartic with small step, boundary case): $(k, \\lambda, h, N, \\text{seed}) = (1000, 50, 0.001, 10000, 42)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true,true,true,true]\"), where each entry is a boolean indicating whether, for that test case, the implicit midpoint scheme produced a lower rejection fraction than Euler–Maruyama (true if the implicit midpoint had fewer rejections, false otherwise).",
            "solution": "We begin with the overdamped Langevin diffusion $dX_t = \\nabla \\log \\pi(X_t)\\, dt + \\sqrt{2}\\, dW_t$, whose invariant density is $\\pi(x) \\propto \\exp(-U(x))$ when $\\nabla \\log \\pi(x) = -\\nabla U(x)$. In one dimension, we have $\\nabla \\log \\pi(x) = -U'(x)$ and $\\nabla^2 \\log \\pi(x) = -U''(x)$, where $U'(x)$ and $U''(x)$ denote the first and second derivatives of the potential with respect to $x$. For $U(x) = \\frac{k}{2} x^2 + \\frac{\\lambda}{4} x^4$, it follows that $U'(x) = k x + \\lambda x^3$ and $U''(x) = k + 3 \\lambda x^2$.\n\nThe Metropolis–Hastings algorithm accepts a proposed move from $x$ to $y$ with probability\n$$\n\\alpha(x, y) = \\min\\left(1, \\frac{\\pi(y) q(y \\to x)}{\\pi(x) q(x \\to y)}\\right),\n$$\nwhere $q(u \\to v)$ is the proposal density for going from $u$ to $v$. Since $\\pi(x)$ is only known up to a proportionality constant, it is sufficient to use $\\log \\pi(x) = -U(x) + C$ with constant $C$ that cancels in ratios.\n\nFor the Euler–Maruyama scheme, the proposal is\n$$\nY = X + h \\nabla \\log \\pi(X) + \\sqrt{2h}\\, \\xi = X - h U'(X) + \\sqrt{2h}\\, \\xi,\\quad \\xi \\sim \\mathcal{N}(0,1).\n$$\nConditioned on $X = x$, the proposal density $q_{\\mathrm{EM}}(x \\to y)$ is Gaussian with mean $m_x = x + h \\nabla \\log \\pi(x) = x - h U'(x)$ and variance $2h$, i.e.,\n$$\nq_{\\mathrm{EM}}(x \\to y) = \\frac{1}{\\sqrt{4\\pi h}} \\exp\\left(-\\frac{(y - m_x)^2}{4h}\\right).\n$$\nTherefore, the acceptance probability is\n$$\n\\alpha_{\\mathrm{EM}}(x, y) = \\min\\left(1, \\exp\\left[(\\log \\pi(y) - \\log \\pi(x)) + (\\log q_{\\mathrm{EM}}(y \\to x) - \\log q_{\\mathrm{EM}}(x \\to y))\\right]\\right),\n$$\nwhere\n$$\n\\log q_{\\mathrm{EM}}(x \\to y) = -\\frac{1}{2}\\log(4\\pi h) - \\frac{(y - x - h \\nabla \\log \\pi(x))^2}{4h}.\n$$\nIn one dimension, the reverse proposal $q_{\\mathrm{EM}}(y \\to x)$ simply substitutes $y$ for $x$ in the above expression.\n\nFor the implicit midpoint (Crank–Nicolson) scheme, the proposal is defined implicitly by\n$$\nY = X + \\frac{h}{2} [\\nabla \\log \\pi(X) + \\nabla \\log \\pi(Y)] + \\sqrt{2h}\\, \\xi = X - \\frac{h}{2}[U'(X) + U'(Y)] + \\sqrt{2h}\\, \\xi,\\quad \\xi \\sim \\mathcal{N}(0,1).\n$$\nTo compute the proposal density $q_{\\mathrm{IM}}(x \\to y)$, observe that the proposal $y$ is obtained from a transformation of the standard normal variable $\\xi$. Define the mapping\n$$\ng(y; x) := \\frac{y - x - \\frac{h}{2}[\\nabla \\log \\pi(x) + \\nabla \\log \\pi(y)]}{\\sqrt{2h}} = \\frac{y - x + \\frac{h}{2}[U'(x) + U'(y)]}{\\sqrt{2h}},\n$$\nso that $g(y; x) = \\xi$ when $y$ satisfies the implicit midpoint equation for a given $x$ and $\\xi$. The change-of-variables formula gives, in one dimension,\n$$\nq_{\\mathrm{IM}}(x \\to y) = \\phi(g(y; x))\\, \\left|\\frac{\\partial g(y; x)}{\\partial y}\\right|,\n$$\nwhere $\\phi(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{u^2}{2}\\right)$ is the standard normal density and\n$$\n\\frac{\\partial g(y; x)}{\\partial y} = \\frac{1 - \\frac{h}{2} \\nabla^2 \\log \\pi(y)}{\\sqrt{2h}} = \\frac{1 + \\frac{h}{2} U''(y)}{\\sqrt{2h}}.\n$$\nThe reverse mapping $g(x; y)$ is\n$$\ng(x; y) = \\frac{x - y - \\frac{h}{2}[\\nabla \\log \\pi(y) + \\nabla \\log \\pi(x)]}{\\sqrt{2h}} = -g(y; x),\n$$\nand its derivative with respect to $x$ is\n$$\n\\frac{\\partial g(x; y)}{\\partial x} = \\frac{1 - \\frac{h}{2} \\nabla^2 \\log \\pi(x)}{\\sqrt{2h}} = \\frac{1 + \\frac{h}{2} U''(x)}{\\sqrt{2h}}.\n$$\nThus,\n$$\nq_{\\mathrm{IM}}(y \\to x) = \\phi(g(x; y))\\, \\left|\\frac{\\partial g(x; y)}{\\partial x}\\right| = \\phi(-g(y; x))\\, \\frac{\\left|1 - \\frac{h}{2} \\nabla^2 \\log \\pi(x)\\right|}{\\sqrt{2h}}.\n$$\nBecause $\\phi(-u) = \\phi(u)$, the standard normal density factors cancel in the Metropolis–Hastings acceptance ratio, leaving\n$$\n\\alpha_{\\mathrm{IM}}(x, y) = \\min\\left(1, \\frac{\\pi(y)}{\\pi(x)} \\cdot \\frac{\\left|1 - \\frac{h}{2} \\nabla^2 \\log \\pi(x)\\right|}{\\left|1 - \\frac{h}{2} \\nabla^2 \\log \\pi(y)\\right|}\\right) = \\min\\left(1, \\exp\\left[-U(y) + U(x)\\right] \\cdot \\frac{1 + \\frac{h}{2} U''(x)}{1 + \\frac{h}{2} U''(y)}\\right).\n$$\nFor the quartic potentials considered with $k  0$ and $\\lambda \\ge 0$, we have $U''(x) = k + 3 \\lambda x^2 \\ge k  0$, ensuring $1 + \\frac{h}{2} U''(x)  1$ and positivity of the Jacobian factors. The implicit midpoint proposal $y$ can be obtained by solving the scalar nonlinear equation\n$$\nf(y) := y - x + \\frac{h}{2}[U'(x) + U'(y)] - \\sqrt{2h}\\, \\xi = 0\n$$\nfor each draw of $\\xi$, using Newton's method with derivative $f'(y) = 1 + \\frac{h}{2} U''(y)$. Since $f'(y)  1$ for all $y$, the mapping is strictly monotone and Newton's method converges rapidly for moderate $h$.\n\nAlgorithmic design:\n- For Euler–Maruyama: from state $x$, draw $\\xi \\sim \\mathcal{N}(0,1)$, set $y = x - h U'(x) + \\sqrt{2h}\\, \\xi$, compute\n$$\n\\log \\alpha_{\\mathrm{EM}}(x, y) = [-U(y) + U(x)] + \\left[-\\frac{1}{2}\\log(4\\pi h) - \\frac{(x - y + h U'(y))^2}{4h}\\right] - \\left[-\\frac{1}{2}\\log(4\\pi h) - \\frac{(y - x + h U'(x))^2}{4h}\\right],\n$$\nand accept with probability $\\min(1, \\exp(\\log \\alpha_{\\mathrm{EM}}(x, y)))$.\n- For implicit midpoint: from state $x$, draw $\\xi \\sim \\mathcal{N}(0,1)$, solve $f(y) = 0$ for $y$, and compute\n$$\n\\alpha_{\\mathrm{IM}}(x, y) = \\min\\left(1, \\exp\\left[-U(y) + U(x)\\right] \\cdot \\frac{1 + \\frac{h}{2} U''(x)}{1 + \\frac{h}{2} U''(y)}\\right).\n$$\n\nTesting methodology:\n- For each given $(k, \\lambda, h, N, \\text{seed})$, run two chains of length $N$ starting from $x_0 = 0$: one using the Euler–Maruyama proposal with its acceptance probability, and one using the implicit midpoint proposal with the Jacobian-adjusted acceptance probability derived above.\n- Record the fraction of proposed moves that are rejected for each method.\n- Output a boolean per test case indicating whether the implicit midpoint scheme yielded a lower rejection fraction than Euler–Maruyama.\n\nEdge cases and coverage:\n- Test Case 1 evaluates a moderately stiff quadratic potential with a relatively larger $h$, a standard use-case.\n- Test Case 2 is a very stiff quadratic potential, where explicit Euler proposals are prone to large drift and potentially higher rejection, revealing robustness of midpoint.\n- Test Case 3 introduces quartic stiffness, examining effects of nonlinearity on Jacobian factors.\n- Test Case 4 uses very stiff mixed quadratic–quartic with a small step size, probing the boundary where both methods can have high acceptance and Jacobian effects are subtle.\n\nThe final program implements these algorithms, ensures reproducibility via seeds, and prints a single line list of booleans indicating lower rejection under the implicit midpoint scheme for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef potential_U(x, k, lam):\n    # U(x) = 0.5*k*x^2 + 0.25*lam*x^4\n    return 0.5 * k * x**2 + 0.25 * lam * x**4\n\ndef grad_U(x, k, lam):\n    # U'(x) = k*x + lam*x^3\n    return k * x + lam * x**3\n\ndef hess_U(x, k, lam):\n    # U''(x) = k + 3*lam*x^2\n    return k + 3.0 * lam * x**2\n\ndef log_pi(x, k, lam):\n    # log pi(x) = -U(x) + const; constant cancels in ratios\n    return -potential_U(x, k, lam)\n\ndef grad_log_pi(x, k, lam):\n    # grad log pi = -U'(x)\n    return -grad_U(x, k, lam)\n\ndef hess_log_pi(x, k, lam):\n    # hess log pi = -U''(x)\n    return -hess_U(x, k, lam)\n\ndef em_propose_accept(x, h, k, lam, rng):\n    # Euler–Maruyama proposal and MH acceptance\n    xi = rng.normal()\n    y = x + h * grad_log_pi(x, k, lam) + np.sqrt(2.0 * h) * xi\n    # log q(x-y)\n    m_x = x + h * grad_log_pi(x, k, lam)\n    log_q_xy = -0.5 * np.log(4.0 * np.pi * h) - ((y - m_x)**2) / (4.0 * h)\n    # log q(y-x)\n    m_y = y + h * grad_log_pi(y, k, lam)\n    log_q_yx = -0.5 * np.log(4.0 * np.pi * h) - ((x - m_y)**2) / (4.0 * h)\n    log_a = (log_pi(y, k, lam) - log_pi(x, k, lam)) + (log_q_yx - log_q_xy)\n    accept = (rng.random()  np.exp(min(0.0, log_a)))\n    return y if accept else x, not accept\n\ndef im_solve_y(x, h, k, lam, xi, max_iter=50, tol=1e-12):\n    # Solve y from implicit midpoint equation:\n    # f(y) = y - x + (h/2)*(U'(x) + U'(y)) - sqrt(2h)*xi = 0\n    # Using Newton's method.\n    # Good initial guess: EM proposal\n    y = x - h * grad_U(x, k, lam) + np.sqrt(2.0 * h) * xi\n    for _ in range(max_iter):\n        f = y - x + 0.5 * h * (grad_U(x, k, lam) + grad_U(y, k, lam)) - np.sqrt(2.0 * h) * xi\n        if abs(f)  tol:\n            break\n        fp = 1.0 + 0.5 * h * hess_U(y, k, lam)\n        # Newton update\n        y_new = y - f / fp\n        # Simple damping if needed\n        if np.isfinite(y_new):\n            y = y_new\n        else:\n            # fallback to smaller step towards x\n            y = 0.5 * (y + x)\n    return y\n\ndef im_propose_accept(x, h, k, lam, rng):\n    # Implicit midpoint proposal and MH acceptance using Jacobian-adjusted ratio\n    xi = rng.normal()\n    y = im_solve_y(x, h, k, lam, xi)\n    # Acceptance ratio:\n    # alpha = min(1, exp(-U(y)+U(x)) * (1 + h/2 U''(x)) / (1 + h/2 U''(y)))\n    jac_x = 1.0 + 0.5 * h * hess_U(x, k, lam)\n    jac_y = 1.0 + 0.5 * h * hess_U(y, k, lam)\n    ratio = np.exp(-potential_U(y, k, lam) + potential_U(x, k, lam)) * (jac_x / jac_y)\n    accept = (rng.random()  min(1.0, ratio))\n    return y if accept else x, not accept\n\ndef run_chain(method, k, lam, h, N, seed):\n    rng = np.random.default_rng(seed)\n    x = 0.0\n    rejects = 0\n    if method == 'EM':\n        step_fn = em_propose_accept\n    elif method == 'IM':\n        step_fn = im_propose_accept\n    else:\n        raise ValueError(\"Unknown method\")\n    for _ in range(N):\n        x, rejected = step_fn(x, h, k, lam, rng)\n        rejects += int(rejected)\n    return rejects / N\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each test case: (k, lambda, h, N, seed)\n    test_cases = [\n        (100, 0, 0.01, 10000, 123),   # Test Case 1\n        (1000, 0, 0.005, 10000, 456), # Test Case 2\n        (100, 10, 0.004, 10000, 789), # Test Case 3\n        (1000, 50, 0.001, 10000, 42), # Test Case 4\n    ]\n\n    results = []\n    for k, lam, h, N, seed in test_cases:\n        rej_em = run_chain('EM', k, lam, h, N, seed)\n        rej_im = run_chain('IM', k, lam, h, N, seed)\n        results.append(rej_im  rej_em)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(r).lower() for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}