{
    "hands_on_practices": [
        {
            "introduction": "To build robust adaptive MCMC algorithms, we must first understand their theoretical underpinnings. This exercise guides you through a foundational analysis of a one-dimensional adaptive Metropolis-Hastings algorithm using the framework of stochastic approximation . By deriving the mean-field dynamics and applying a central limit theorem, you will gain insight into how these algorithms self-tune and why averaging the adapted parameters can lead to optimal statistical inference.",
            "id": "3287318",
            "problem": "Consider an Adaptive Metropolis–Hastings (AMH) algorithm in one dimension targeting the standard normal density $\\pi(x) \\propto \\exp(-x^{2}/2)$. Proposals are Gaussian random-walks with scale $s_{t} = \\exp(\\theta_{t})$, that is, $X_{t+1} \\sim \\mathcal{N}(X_{t}, s_{t}^{2})$. Let $A_{t+1} \\in \\{0,1\\}$ denote the Metropolis–Hastings acceptance indicator at iteration $t+1$. The adaptation of the log-scale parameter is governed by the stochastic approximation recursion\n$$\n\\theta_{t+1} \\;=\\; \\theta_{t} \\;+\\; \\gamma_{t}\\,\\big(A_{t+1} - \\alpha^{\\star}\\big), \n\\qquad \\gamma_{t} \\;=\\; \\frac{c}{t},\n$$\nwith fixed $c \\in (0,\\infty)$ and target acceptance fraction $\\alpha^{\\star} \\in (0,1)$. Define the running average $\\bar{\\theta}_{t} \\;=\\; t^{-1} \\sum_{k=1}^{t} \\theta_{k}$. Assume the standard time-scale separation used in adaptive Markov chain Monte Carlo (MCMC): the adaptation is applied every $b_{t}$ MCMC steps with $b_{t} \\to \\infty$ sufficiently fast, and the chain mixes fast enough, so that the acceptance indicators used in the stochastic approximation are asymptotically independent Bernoulli random variables with mean $\\alpha(s)$ when the proposal scale equals $s$.\n\nStarting from first principles (Metropolis–Hastings acceptance rule, properties of the standard normal distribution, and the stochastic approximation Ordinary Differential Equation method), perform the following:\n\n1) Show that the mean-field drift of the stochastic approximation is\n$$\nh(\\theta) \\;=\\; \\mathbb{E}\\big[A_{t+1} \\,\\big|\\, \\theta_{t}=\\theta\\big] \\;-\\; \\alpha^{\\star} \\;=\\; \\alpha\\big(\\exp(\\theta)\\big) \\;-\\; \\alpha^{\\star},\n$$\nwhere $\\alpha(s)$ is the stationary expected acceptance probability of the random-walk Metropolis algorithm with proposal scale $s$. Derive from first principles that for the standard normal target and Gaussian random-walk proposal,\n$$\n\\alpha(s) \\;=\\; \\frac{2}{\\pi}\\,\\arctan\\!\\Big(\\frac{2}{s}\\Big), \\qquad s \\in (0,\\infty).\n$$\n\n2) Let $\\theta^{\\star}$ denote the unique root of $h(\\theta)$, and write $s^{\\star} = \\exp(\\theta^{\\star})$. Show that $h'(\\theta^{\\star}) < 0$, and express $h'(\\theta^{\\star})$ explicitly in terms of $s^{\\star}$.\n\n3) Using the Polyak–Ruppert averaging Central Limit Theorem for stochastic approximation, establish the asymptotic normality of the averaged iterates,\n$$\n\\sqrt{t}\\,\\big(\\bar{\\theta}_{t} - \\theta^{\\star}\\big) \\;\\Rightarrow\\; \\mathcal{N}\\big(0,\\, V\\big),\n$$\nand derive the asymptotic variance $V$ in closed form as a function of $\\alpha^{\\star}$ only.\n\n4) Evaluate the resulting $V$ at the canonical one-dimensional target acceptance $\\alpha^{\\star} = 0.44$. Round your answer to four significant figures. Your final answer must be a single real number with no units.",
            "solution": "The problem asks for a multi-part analysis of a one-dimensional Adaptive Metropolis-Hastings (AMH) algorithm. We will address each part in sequence, starting from first principles as requested.\n\n### Part 1: Mean-Field Drift and Acceptance Probability\n\nFirst, we establish the form of the mean-field drift function $h(\\theta)$. The stochastic approximation recursion for the log-scale parameter $\\theta_t$ is given by\n$$\n\\theta_{t+1} = \\theta_{t} + \\gamma_{t}(A_{t+1} - \\alpha^{\\star}).\n$$\nThe mean-field drift, denoted $h(\\theta_t)$, is the expected change in $\\theta_t$ per unit of step-size $\\gamma_t$, conditional on the current state $\\theta_t$. Formally,\n$$\nh(\\theta_t) = \\mathbb{E}\\left[\\frac{\\theta_{t+1} - \\theta_t}{\\gamma_t} \\bigg| \\theta_t\\right] = \\mathbb{E}[A_{t+1} - \\alpha^{\\star} | \\theta_t].\n$$\nUsing the law of iterated expectations and the assumption that the chain mixes fast enough, the acceptance probability $A_{t+1}$ at step $t+1$ depends only on the proposal scale $s_t = \\exp(\\theta_t)$ used to generate the proposal. Let $\\theta_t = \\theta$. Then $s_t=s=\\exp(\\theta)$. We have\n$$\n\\mathbb{E}[A_{t+1} | \\theta_t=\\theta] = \\alpha(s) = \\alpha(\\exp(\\theta)),\n$$\nwhere $\\alpha(s)$ is the stationary expected acceptance probability of the Metropolis-Hastings algorithm with proposal scale $s$. Thus, the mean-field drift is\n$$\nh(\\theta) = \\alpha(\\exp(\\theta)) - \\alpha^{\\star}.\n$$\n\nNext, we derive the expression for $\\alpha(s)$ for a standard normal target density $\\pi(x) \\propto \\exp(-x^2/2)$ and a Gaussian random-walk proposal $Y \\sim \\mathcal{N}(x, s^2)$. The proposal density $q(y|x)$ is symmetric, i.e., $q(y|x) = q(x|y)$. The Metropolis-Hastings acceptance probability for a move from $x$ to $y$ is\n$$\na(x,y) = \\min\\left(1, \\frac{\\pi(y)}{\\pi(x)}\\right) = \\min\\left(1, \\frac{\\exp(-y^2/2)}{\\exp(-x^2/2)}\\right) = \\min\\left(1, \\exp\\left(-\\frac{y^2-x^2}{2}\\right)\\right).\n$$\nThe Stationary expected acceptance probability $\\alpha(s)$ is the expectation of $a(X,Y)$ where $X \\sim \\pi$ and $Y|X=x \\sim q(\\cdot|x)$. So $X \\sim \\mathcal{N}(0,1)$, and we can write $Y = X + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, s^2)$ is independent of $X$.\nThe acceptance probability becomes a function of $X$ and $\\epsilon$:\n$$\na(X, \\epsilon) = \\min\\left(1, \\exp\\left(-\\frac{(X+\\epsilon)^2-X^2}{2}\\right)\\right) = \\min\\left(1, \\exp\\left(-\\frac{2X\\epsilon+\\epsilon^2}{2}\\right)\\right).\n$$\nWe compute $\\alpha(s) = \\mathbb{E}_{X,\\epsilon}[a(X, \\epsilon)]$ by first conditioning on $\\epsilon$:\n$$\n\\alpha(s) = \\mathbb{E}_{\\epsilon}\\left[ \\mathbb{E}_{X| \\epsilon} \\left[ \\min\\left(1, \\exp\\left(-X\\epsilon - \\frac{\\epsilon^2}{2}\\right)\\right) \\right] \\right].\n$$\nLet's analyze the inner expectation. The ratio is greater than $1$ when $-X\\epsilon - \\epsilon^2/2 > 0$, which is equivalent to $X\\epsilon < -\\epsilon^2/2$.\nCase 1: $\\epsilon > 0$. The condition is $X < -\\epsilon/2$.\nCase 2: $\\epsilon < 0$. The condition is $X > -\\epsilon/2$.\nLet $\\Phi(\\cdot)$ be the CDF of the standard normal distribution. Let's compute the conditional expectation $f(\\epsilon) = \\mathbb{E}_{X}[a(X,\\epsilon)]$.\nFor $\\epsilon > 0$:\n$$\nf(\\epsilon) = \\int_{-\\infty}^{-\\epsilon/2} 1 \\cdot \\phi(x) \\, dx + \\int_{-\\epsilon/2}^{\\infty} \\exp\\left(-x\\epsilon - \\frac{\\epsilon^2}{2}\\right) \\phi(x) \\, dx,\n$$\nwhere $\\phi(x) = (2\\pi)^{-1/2}\\exp(-x^2/2)$. The first integral is $\\Phi(-\\epsilon/2)$. The second integral is\n$$\n\\frac{1}{\\sqrt{2\\pi}} \\int_{-\\epsilon/2}^{\\infty} \\exp\\left(-x\\epsilon - \\frac{\\epsilon^2}{2} - \\frac{x^2}{2}\\right) \\, dx = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\epsilon/2}^{\\infty} \\exp\\left(-\\frac{(x+\\epsilon)^2}{2}\\right) \\, dx.\n$$\nWith substitution $u = x+\\epsilon$, the lower limit becomes $-\\epsilon/2+\\epsilon = \\epsilon/2$. The integral is $\\int_{\\epsilon/2}^{\\infty} \\phi(u) \\, du = 1-\\Phi(\\epsilon/2) = \\Phi(-\\epsilon/2)$.\nSo, for $\\epsilon > 0$, $f(\\epsilon) = \\Phi(-\\epsilon/2) + \\Phi(-\\epsilon/2) = 2\\Phi(-|\\epsilon|/2)$.\nA similar calculation for $\\epsilon < 0$ shows that $f(\\epsilon) = 2\\Phi(\\epsilon/2) = 2\\Phi(-|\\epsilon|/2)$ as well.\nThus, for any $\\epsilon \\neq 0$, we have $\\mathbb{E}_{X}[a(X,\\epsilon)] = 2\\Phi(-|\\epsilon|/2)$.\nNow we take the expectation over $\\epsilon \\sim \\mathcal{N}(0,s^2)$. Let $\\epsilon=sZ$ where $Z \\sim \\mathcal{N}(0,1)$.\n$$\n\\alpha(s) = \\mathbb{E}_{Z}\\left[ 2\\Phi\\left(-\\frac{s|Z|}{2}\\right) \\right] = 2 \\int_{-\\infty}^{\\infty} \\Phi\\left(-\\frac{s|z|}{2}\\right)\\phi(z) \\, dz = 4 \\int_{0}^{\\infty} \\Phi\\left(-\\frac{sz}{2}\\right)\\phi(z) \\, dz.\n$$\nThe integral can be computed as follows:\n$$\nI = \\int_{0}^{\\infty} \\Phi\\left(-\\frac{sz}{2}\\right)\\phi(z) \\, dz = \\int_0^\\infty \\left( \\int_{-\\infty}^{-sz/2} \\phi(u) \\, du \\right) \\phi(z) \\, dz = \\frac{1}{2\\pi} \\iint_D \\exp\\left(-\\frac{u^2+z^2}{2}\\right) \\, du \\, dz,\n$$\nwhere the domain of integration is $D = \\{(u,z) : z>0, u<-sz/2\\}$. This is a sector in the $(u,z)$-plane. We convert to polar coordinates $(r, \\theta)$ with $z=r\\sin\\theta, u=r\\cos\\theta$.\nThe condition $z>0$ implies $\\theta \\in (0, \\pi)$. The condition $u<0$ implies $\\theta \\in (\\pi/2, 3\\pi/2)$. Together, $\\theta \\in (\\pi/2, \\pi)$. The condition $u<-sz/2$ becomes $r\\cos\\theta < -s/2 \\cdot r\\sin\\theta$, which simplifies to $\\cot\\theta < -s/2$ (since $\\sin\\theta>0$ in this range). The arccotangent function maps $(-\\infty,0)$ to $(\\pi/2, \\pi)$, so the condition on $\\theta$ is $\\text{arccot}(-s/2) < \\theta < \\pi$.\nUsing the identity $\\text{arccot}(-x) = \\pi - \\text{arccot}(x) = \\pi - (\\pi/2 - \\arctan(x)) = \\pi/2 + \\arctan(x)$ for $x>0$, the range for $\\theta$ is $(\\pi/2 + \\arctan(s/2), \\pi)$.\nThe integral in polar coordinates is:\n$$\nI = \\frac{1}{2\\pi} \\int_{\\pi/2 + \\arctan(s/2)}^{\\pi} \\int_0^\\infty r\\exp(-r^2/2) \\, dr \\, d\\theta.\n$$\nThe inner integral is $\\int_0^\\infty r e^{-r^2/2} \\, dr = [-e^{-r^2/2}]_0^\\infty = 1$.\n$$\nI = \\frac{1}{2\\pi} \\left[ \\pi - \\left(\\frac{\\pi}{2} + \\arctan\\left(\\frac{s}{2}\\right)\\right) \\right] = \\frac{1}{2\\pi} \\left( \\frac{\\pi}{2} - \\arctan\\left(\\frac{s}{2}\\right) \\right).\n$$\nUsing the identity $\\arctan(x) + \\arctan(1/x) = \\pi/2$ for $x>0$, this simplifies to $I = \\frac{1}{2\\pi} \\arctan(2/s)$.\nFinally, $\\alpha(s) = 4I = 4 \\left( \\frac{1}{2\\pi} \\arctan(2/s) \\right) = \\frac{2}{\\pi} \\arctan(2/s)$.\n\n### Part 2: Stability Analysis\n\nThe root $\\theta^\\star$ of $h(\\theta)$ satisfies $h(\\theta^\\star) = \\alpha(\\exp(\\theta^\\star)) - \\alpha^\\star = 0$. Let $s^\\star = \\exp(\\theta^\\star)$. Then $\\alpha(s^\\star) = \\alpha^\\star$.\nWe need to determine the sign of $h'(\\theta^\\star)$. Using the chain rule:\n$$\nh'(\\theta) = \\frac{d}{d\\theta} \\alpha(\\exp(\\theta)) = \\alpha'(\\exp(\\theta)) \\cdot \\exp(\\theta).\n$$\nAt the root $\\theta^\\star$, we have $h'(\\theta^\\star) = \\alpha'(s^\\star) \\cdot s^\\star$.\nWe first compute the derivative of $\\alpha(s)$:\n$$\n\\alpha'(s) = \\frac{d}{ds} \\left( \\frac{2}{\\pi} \\arctan\\left(\\frac{2}{s}\\right) \\right) = \\frac{2}{\\pi} \\cdot \\frac{1}{1+(2/s)^2} \\cdot \\left(-\\frac{2}{s^2}\\right) = \\frac{2}{\\pi} \\cdot \\frac{s^2}{s^2+4} \\cdot \\left(-\\frac{2}{s^2}\\right) = -\\frac{4}{\\pi(s^2+4)}.\n$$\nSince $s \\in (0, \\infty)$, we have $s^2+4>0$, so $\\alpha'(s) < 0$ for all $s$.\nThis implies that $h'(\\theta^\\star) = \\alpha'(s^\\star) s^\\star < 0$, which confirms that the equilibrium is stable.\nThe explicit expression for $h'(\\theta^\\star)$ in terms of $s^\\star$ is:\n$$\nh'(\\theta^\\star) = s^\\star \\cdot \\left( -\\frac{4}{\\pi((s^\\star)^2+4)} \\right) = -\\frac{4s^\\star}{\\pi((s^\\star)^2+4)}.\n$$\n\n### Part 3: Asymptotic Variance\n\nThe Polyak-Ruppert averaging Central Limit Theorem states that, under certain conditions (including the stability shown in Part 2 and a condition on the step-size constant $c$), the averaged iterates $\\bar{\\theta}_t = t^{-1}\\sum_{k=1}^t \\theta_k$ are asymptotically normal:\n$$\n\\sqrt{t}(\\bar{\\theta}_t - \\theta^\\star) \\Rightarrow \\mathcal{N}(0, V).\n$$\nThe asymptotic variance $V$ is given by the formula $V = \\frac{\\Sigma}{(h'(\\theta^\\star))^2}$, where $\\Sigma$ is the variance of the noise process driving the stochastic approximation at equilibrium.\nThe recursion can be written as $\\theta_{t+1} = \\theta_t + \\gamma_t(h(\\theta_t) + M_{t+1})$, where $M_{t+1} = A_{t+1} - \\mathbb{E}[A_{t+1}|\\theta_t]$ is the noise term. At equilibrium ($\\theta_t = \\theta^\\star$), the acceptance indicator $A_{t+1}$ follows a Bernoulli distribution with success probability $\\mathbb{E}[A_{t+1}|\\theta^\\star] = \\alpha(s^\\star) = \\alpha^\\star$.\nThe variance of a Bernoulli($p$) random variable is $p(1-p)$. Therefore, the noise variance at equilibrium is\n$$\n\\Sigma = \\text{Var}(A_{t+1}|\\theta_t=\\theta^\\star) = \\alpha^\\star(1-\\alpha^\\star).\n$$\nTo express $V$ as a function of $\\alpha^\\star$ only, we must express $h'(\\theta^\\star)$ in terms of $\\alpha^\\star$. From $\\alpha^\\star = \\frac{2}{\\pi}\\arctan(2/s^\\star)$, we have $\\frac{\\pi\\alpha^\\star}{2} = \\arctan(2/s^\\star)$. Taking the tangent of both sides gives $\\tan(\\frac{\\pi\\alpha^\\star}{2}) = \\frac{2}{s^\\star}$, which implies $s^\\star = \\frac{2}{\\tan(\\frac{\\pi\\alpha^\\star}{2})}$.\nSubstituting this into the expression for $h'(\\theta^\\star)$:\n$$\nh'(\\theta^\\star) = -\\frac{4s^\\star}{\\pi((s^\\star)^2+4)} = -\\frac{4 \\left(\\frac{2}{\\tan(\\frac{\\pi\\alpha^\\star}{2})}\\right)}{\\pi \\left( \\left(\\frac{2}{\\tan(\\frac{\\pi\\alpha^\\star}{2})}\\right)^2 + 4 \\right)} = -\\frac{\\frac{8}{\\tan(\\frac{\\pi\\alpha^\\star}{2})}}{\\pi \\left( \\frac{4}{\\tan^2(\\frac{\\pi\\alpha^\\star}{2})} + 4 \\right)} = -\\frac{8}{\\pi \\tan(\\frac{\\pi\\alpha^\\star}{2})} \\frac{\\tan^2(\\frac{\\pi\\alpha^\\star}{2})}{4(1+\\tan^2(\\frac{\\pi\\alpha^\\star}{2}))}.\n$$\nUsing the identity $1+\\tan^2(x) = \\sec^2(x)$, this simplifies to:\n$$\nh'(\\theta^\\star) = -\\frac{2\\tan(\\frac{\\pi\\alpha^\\star}{2})}{\\pi \\sec^2(\\frac{\\pi\\alpha^\\star}{2})} = -\\frac{2}{\\pi} \\frac{\\sin(\\frac{\\pi\\alpha^\\star}{2})/\\cos(\\frac{\\pi\\alpha^\\star}{2})}{1/\\cos^2(\\frac{\\pi\\alpha^\\star}{2})} = -\\frac{2}{\\pi}\\sin\\left(\\frac{\\pi\\alpha^\\star}{2}\\right)\\cos\\left(\\frac{\\pi\\alpha^\\star}{2}\\right).\n$$\nUsing the double-angle identity $2\\sin(x)\\cos(x) = \\sin(2x)$, we get a remarkably simple expression:\n$$\nh'(\\theta^\\star) = -\\frac{1}{\\pi}\\sin\\left(2 \\cdot \\frac{\\pi\\alpha^\\star}{2}\\right) = -\\frac{\\sin(\\pi\\alpha^\\star)}{\\pi}.\n$$\nNow we assemble the asymptotic variance $V$:\n$$\nV = \\frac{\\Sigma}{(h'(\\theta^\\star))^2} = \\frac{\\alpha^\\star(1-\\alpha^\\star)}{\\left(-\\frac{\\sin(\\pi\\alpha^\\star)}{\\pi}\\right)^2} = \\frac{\\pi^2 \\alpha^\\star(1-\\alpha^\\star)}{\\sin^2(\\pi\\alpha^\\star)}.\n$$\n\n### Part 4: Numerical Evaluation\n\nWe are asked to evaluate $V$ for the target acceptance rate $\\alpha^\\star = 0.44$.\nPlugging this value into our derived formula for $V$:\n$$\nV = \\frac{\\pi^2 (0.44)(1-0.44)}{\\sin^2(0.44\\pi)} = \\frac{\\pi^2 (0.44)(0.56)}{\\sin^2(0.44\\pi)} = \\frac{0.2464 \\pi^2}{\\sin^2(0.44\\pi)}.\n$$\nWe perform the numerical calculation:\nThe argument of the sine function is $0.44\\pi$ radians.\n$0.44\\pi \\approx 1.382300767$\n$\\sin(0.44\\pi) \\approx 0.98223604$\n$\\sin^2(0.44\\pi) \\approx 0.96478761$\nThe numerator is $0.2464 \\pi^2 \\approx 0.2464 \\times 9.8696044 \\approx 2.4318705$.\nThus, the variance is\n$$\nV \\approx \\frac{2.4318705}{0.96478761} \\approx 2.520653.\n$$\nRounding to four significant figures, we get $2.521$.",
            "answer": "$$\\boxed{2.521}$$"
        },
        {
            "introduction": "Theoretical guarantees are necessary, but not sufficient; a practical implementation must also be numerically robust. This exercise confronts a critical challenge in multidimensional adaptive MCMC: the potential for the estimated covariance matrix $\\Sigma_n$ to lose positive definiteness due to floating-point errors . You will evaluate several strategies for detecting and correcting this issue, learning to design a fail-safe procedure that is essential for the stability of algorithms like the Haario-Saksman-Tamminen (HST) method.",
            "id": "3353669",
            "problem": "Consider the Adaptive Metropolis algorithm of Haario–Saksman–Tamminen (HST), which updates a proposal covariance matrix $\\Sigma_n$ along the iterations $n$ of a Markov chain in order to adapt to the target distribution. In exact arithmetic, one desires $\\Sigma_n$ to be symmetric positive definite so that a Gaussian proposal with covariance $\\Sigma_n$ is well-defined and so that a Cholesky factorization exists. In floating-point arithmetic, roundoff, accumulation of small updates, and ill-conditioning can lead to loss of positive definiteness or numerical failure in forming the proposal. Design a numerically robust test for loss of positive definiteness of $\\Sigma_n$ and specify corrective actions that make $\\Sigma_n$ suitable for use, such as increasing a diagonal regularization $\\epsilon$ or reinitializing the factorization when needed.\n\nUse the following fundamental facts as your starting point and justify your choice(s) using them:\n- A real symmetric matrix $A$ is positive definite if and only if all its eigenvalues are strictly positive, which is equivalent to the existence of a Cholesky factorization $A = LL^\\top$ with positive diagonal entries.\n- Adding a multiple of the identity matrix, i.e., replacing $A$ by $A + \\epsilon I$ with $\\epsilon > 0$, shifts all eigenvalues upward by $\\epsilon$ and preserves symmetry.\n- The determinant alone is not a reliable numerical test of positive definiteness in high dimensions, and Gaussian elimination without pivoting is numerically unstable for general matrices.\n- Gershgorin’s circle theorem provides bounds on eigenvalues from row sums but may be conservative and must be applied carefully to symmetric matrices.\n- In adaptive algorithms, repeatedly failing numerical checks should trigger a safe reinitialization of the proposal, such as resetting to a scaled identity covariance, to ensure stability.\n\nWhich of the following procedures is a numerically robust approach to detect and correct loss of positive definiteness in $\\Sigma_n$ during HST adaptation?\n\nA. At each update, form the symmetric part $S_n = (\\Sigma_n + \\Sigma_n^\\top)/2$ and attempt a Cholesky factorization of $S_n$. If the factorization fails due to a nonpositive pivot or returns non-numeric values, increase a diagonal regularization by replacing $S_n$ with $S_n + \\epsilon I$ and retry, using an adaptive schedule $\\epsilon \\leftarrow \\max\\{\\epsilon_{\\min}, \\gamma \\epsilon\\}$ with $\\gamma > 1$ until a factorization succeeds. If failures persist for $k$ consecutive attempts, reinitialize the covariance to $c I$ for some scale $c > 0$ and rebuild the factorization. Use the resulting factor to define the proposal. \n\nB. Compute the determinant $\\det(\\Sigma_n)$ via an LU decomposition without pivoting. If $\\det(\\Sigma_n) > 0$, conclude that $\\Sigma_n$ is positive definite and proceed. If $\\det(\\Sigma_n) \\le 0$, reduce $\\epsilon$ toward $0$ and retry the LU decomposition until $\\det(\\Sigma_n) > 0$, then continue without reinitializing. \n\nC. Estimate the smallest eigenvalue $\\lambda_{\\min}$ of the symmetric part $S_n = (\\Sigma_n + \\Sigma_n^\\top)/2$ using a symmetric eigensolver (e.g., Householder tridiagonalization followed by the QR algorithm or a Lanczos iteration). If $\\lambda_{\\min} < \\tau$ for a small threshold $\\tau > 0$, correct by either (i) setting $\\Sigma_n \\leftarrow S_n + \\epsilon I$ with $\\epsilon \\ge \\tau - \\lambda_{\\min}$, or (ii) projecting the spectrum by replacing negative or too-small eigenvalues by $\\tau$ and reconstructing $S_n$ from its eigendecomposition, then compute a Cholesky factor to define the proposal. If repeated corrections exceed a budget, reinitialize to $c I$. \n\nD. Apply Gershgorin’s circle theorem to the rows of $\\Sigma_n$. If any Gershgorin disc intersects the negative real axis, declare loss of positive definiteness and correct by subtracting a diagonal term, i.e., set $\\Sigma_n \\leftarrow \\Sigma_n - \\epsilon I$ with $\\epsilon > 0$, to move discs away from the negative axis. Continue without reinitialization. \n\nE. When a loss of positive definiteness is suspected, add a random rank-one term $uu^\\top$ with $u$ drawn uniformly from a hypercube to $\\Sigma_n$ and attempt a Cholesky downdate of the existing factor. Repeat with new $u$ until the downdate succeeds, then use the updated factorization for the proposal without further checks.\n\nSelect all correct options.",
            "solution": "The problem statement asks to identify numerically robust procedures for detecting and correcting the loss of positive definiteness in the adaptive covariance matrix $\\Sigma_n$ used in the Haario-Saksman-Tamminen (HST) Metropolis algorithm.\n\n**Problem Validation**\n\nFirst, I will validate the problem statement itself.\n\n**Step 1: Extract Givens**\n-   **Algorithm**: Haario–Saksman–Tamminen (HST) Adaptive Metropolis.\n-   **Matrix**: Proposal covariance matrix $\\Sigma_n$ at iteration $n$.\n-   **Desired Property**: $\\Sigma_n$ should be symmetric positive definite (SPD).\n-   **Numerical Issue**: Floating-point arithmetic can lead to loss of positive definiteness.\n-   **Task**: Identify a numerically robust procedure to test for and correct this loss.\n-   **Provided Facts**:\n    1.  A real symmetric matrix $A$ is SPD $\\iff$ all eigenvalues are strictly positive $\\iff$ a Cholesky factorization $A = LL^\\top$ with positive diagonal entries exists.\n    2.  For a symmetric matrix $A$, the matrix $A + \\epsilon I$ (with $\\epsilon > 0$) has eigenvalues shifted up by $\\epsilon$ and remains symmetric.\n    3.  The determinant is not a reliable numerical test for positive definiteness. Gaussian elimination without pivoting is numerically unstable.\n    4.  Gershgorin’s circle theorem gives eigenvalue bounds that may be conservative.\n    5.  Persistent numerical failures in adaptive algorithms warrant a safe reinitialization of the proposal mechanism.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is well-grounded in the field of computational statistics and numerical linear algebra. The HST algorithm is a canonical method in adaptive MCMC, and the loss of positive definiteness of a numerically updated covariance matrix is a real and practical issue. The provided facts are all standard, correct theorems and principles.\n-   **Well-Posed**: The problem is well-posed. It asks to evaluate a set of proposed computational procedures against the criteria of numerical robustness, based on a given set of fundamental principles. A definite answer can be reached through logical deduction.\n-   **Objective**: The problem is stated in objective, technical language, free from ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically sound, well-posed, and objective. I will now proceed to analyze each option.\n\n**Derivation and Option-by-Option Analysis**\n\nThe core task is to maintain the symmetric positive definiteness of the covariance matrix $\\Sigma_n$. A robust procedure must have three components: a reliable test for positive definiteness, an effective correction mechanism, and a fail-safe for stability. Due to floating-point errors, $\\Sigma_n$ might become slightly non-symmetric, so enforcing symmetry by taking $S_n = (\\Sigma_n + \\Sigma_n^\\top)/2$ is a standard and sound preliminary step.\n\n**Option A Analysis**\nThis option proposes a procedure with the following steps:\n1.  **Symmetrization**: Form $S_n = (\\Sigma_n + \\Sigma_n^\\top)/2$. This correctly handles any minor asymmetries from floating-point updates.\n2.  **Detection**: Attempt a Cholesky factorization of $S_n$. As stated in Fact 1, the existence of a Cholesky factorization is equivalent to the matrix being symmetric positive definite. Computationally, attempting the factorization is the standard, most efficient, and numerically reliable method to test for this property. If an algorithm for Cholesky factorization (e.g., involving computing $\\sqrt{a_{kk} - \\sum_{j=1}^{k-1} L_{kj}^2}$) encounters a non-positive term inside the square root, the matrix is not positive definite. This is a direct and conclusive test.\n3.  **Correction**: If the factorization fails, replace $S_n$ with $S_n + \\epsilon I$ for some $\\epsilon > 0$. Fact 2 confirms that this operation shifts all eigenvalues of $S_n$ up by $\\epsilon$. For a sufficiently large $\\epsilon$, all eigenvalues will become positive, thus making the matrix positive definite. This is a standard technique known as diagonal loading or Tikhonov regularization.\n4.  **Adaptation & Stability**: The procedure suggests an adaptive schedule for $\\epsilon$ ($\\epsilon \\leftarrow \\max\\{\\epsilon_{\\min}, \\gamma \\epsilon\\}$ with $\\gamma > 1$) and a reinitialization to $cI$ if correction fails repeatedly. This ensures that the correction is minimally invasive but becomes stronger if needed, and the overall algorithm stability is guaranteed by the fallback mechanism, in full agreement with Fact 5.\n\nThis procedure combines a reliable test, a standard correction, and a stability mechanism, all consistent with the provided facts and best practices of numerical linear algebra.\n**Verdict: Correct**\n\n**Option B Analysis**\nThis option proposes:\n1.  **Detection**: Compute $\\det(\\Sigma_n)$ and check if it's positive. Fact 3 explicitly and correctly states that the determinant is not a reliable test. A symmetric matrix can have a positive determinant while not being positive definite (e.g., a diagonal matrix with an even number of negative entries). For example, if $\\Sigma_n = \\text{diag}(-1, -1, 4)$, its determinant is $4 > 0$, but it is not positive definite.\n2.  **Implementation Detail**: It suggests using LU decomposition without pivoting. Fact 3 warns that this is numerically unstable for general matrices. Since we suspect $\\Sigma_n$ may not be SPD, we cannot assume it belongs to the class of matrices for which unpivoted LU is stable.\n3.  **Correction**: It suggests reducing $\\epsilon$ towards $0$ if $\\det(\\Sigma_n) \\le 0$. This is the opposite of the correct action. To make a matrix positive definite via diagonal loading, one must *add* a positive value to the diagonal, which corresponds to *increasing* $\\epsilon$ in the expression $S_n + \\epsilon I$.\n4.  **Stability**: It proposes continuing without reinitialization, which contradicts the principle of ensuring stability outlined in Fact 5.\n\nThis procedure is flawed in every aspect: the test is wrong, the correction is counterproductive, and it lacks a stability guarantee.\n**Verdict: Incorrect**\n\n**Option C Analysis**\nThis option proposes:\n1.  **Symmetrization**: Form $S_n = (\\Sigma_n + \\Sigma_n^\\top)/2$. Correct.\n2.  **Detection**: Estimate the smallest eigenvalue $\\lambda_{\\min}$ of $S_n$. According to Fact 1, the matrix is positive definite if and only if all its eigenvalues are positive. Therefore, checking if $\\lambda_{\\min}$ is positive (or greater than a small tolerance $\\tau > 0$ for numerical safety) is a direct and theoretically perfect test. While computationally more expensive than a Cholesky factorization, it is a completely valid and robust detection method.\n3.  **Correction**: It offers two valid correction strategies.\n    (i) Set $\\Sigma_n \\leftarrow S_n + \\epsilon I$ with $\\epsilon \\ge \\tau - \\lambda_{\\min}$. This is a targeted application of Fact 2. The smallest eigenvalue of the new matrix will be $\\lambda_{\\min} + \\epsilon$. Choosing $\\epsilon \\ge \\tau - \\lambda_{\\min}$ guarantees that the new smallest eigenvalue is at least $\\tau$, making the matrix robustly positive definite.\n    (ii) Project the spectrum by replacing eigenvalues smaller than $\\tau$ with $\\tau$ and reconstructing the matrix. This is a well-known method for finding a nearby positive semidefinite (or definite) matrix and is a principled way to perform the correction.\n4.  **Stability**: It includes a reinitialization step ($cI$) if corrections are needed too frequently, which adheres to Fact 5.\n\nThis procedure is theoretically sound and numerically robust. Its higher computational cost relative to Option A is a practical trade-off, not a flaw in its robustness.\n**Verdict: Correct**\n\n**Option D Analysis**\nThis option proposes:\n1.  **Detection**: Use Gershgorin’s circle theorem. Fact 4 correctly points out that this theorem provides bounds that may be conservative. A Gershgorin disc intersecting the negative real axis does not prove that an eigenvalue is negative; it only indicates that it is a possibility. This can lead to unnecessary corrections (false positives), making the test suboptimal.\n2.  **Correction**: It suggests setting $\\Sigma_n \\leftarrow \\Sigma_n - \\epsilon I$. This operation *subtracts* a positive multiple of the identity, which shifts all eigenvalues *down* by $\\epsilon$. This would make a matrix that is not positive definite even less so, and could even make a positive definite matrix non-positive definite. This is fundamentally incorrect.\n3.  **Stability**: It lacks a reinitialization mechanism, violating Fact 5.\n\nThe proposed correction is catastrophically wrong.\n**Verdict: Incorrect**\n\n**Option E Analysis**\nThis option proposes:\n1.  **Correction**: Add a random rank-one matrix $uu^\\top$. While adding a positive semidefinite matrix like $uu^\\top$ does not decrease any eigenvalues, it is not a targeted or guaranteed way to fix negative eigenvalues. It's a heuristic, not a robust, controlled procedure. It is not clear how many random attempts would be required, or if it would even succeed for a matrix with multiple or large-magnitude negative eigenvalues.\n2.  **Mechanism**: It mentions a \"Cholesky downdate\". A downdate corresponds to subtracting a rank-one matrix, i.e., finding the Cholesky factor of $A - uu^\\top$ from the factor of $A$. The procedure describes *adding* $uu^\\top$, for which one would use a Cholesky *update*. Cholesky downdating is known to be numerically less stable than updating. This suggests either a terminological error or a misunderstanding of the numerical methods.\n3.  **Stability**: The procedure relies on repeated random trials with no ultimate fail-safe like reinitialization to a known good state, violating Fact 5.\n\nThis procedure is not robust; it is based on a random heuristic and uses questionable terminology regarding the numerical implementation.\n**Verdict: Incorrect**\n\n**Conclusion**\nOptions A and C both describe valid, numerically robust procedures for ensuring the proposal covariance matrix remains positive definite. Option A is based on the computationally efficient Cholesky factorization test. Option C is based on the more computationally expensive but diagnostically powerful eigenvalue analysis. Both employ correct and standard principles for correction and stability. Therefore, both are correct answers.",
            "answer": "$$\\boxed{AC}$$"
        },
        {
            "introduction": "The ultimate test of an algorithm is its performance in practice. This hands-on coding exercise tasks you with implementing and evaluating an adaptive Random-Walk Metropolis sampler with a power-law scaling schedule, $\\sigma_n = c n^{-\\alpha}$ . By measuring the Effective Sample Size (ESS) against targets with varying smoothness and tail behavior, you will empirically investigate the trade-offs in choosing the adaptation rate $\\alpha$ and see firsthand how it impacts sampler efficiency.",
            "id": "3287303",
            "problem": "Consider the Random-Walk Metropolis (RWM) algorithm within Markov Chain Monte Carlo (MCMC). Let the target density on the real line be a log-integrable probability density function $p(x)$, and consider proposals of the form $x^{\\prime} = x + \\eta$, where $\\eta \\sim \\mathcal{N}(0,\\sigma_n^2)$ with a time-dependent scale $\\sigma_n$. The RWM acceptance probability at step $n$ is given by the Metropolis–Hastings rule $a(x,x^{\\prime}) = \\min\\{1, p(x^{\\prime})/p(x)\\}$. We examine adaptive schedules of the form $\\sigma_n = c \\, n^{-\\alpha}$ with $c \\gt 0$ and $\\alpha \\gt 0$. The Diminishing Adaptation condition in adaptive MCMC requires that the difference between successive transition kernels converges to zero, which for RWM can be analyzed via the convergence of $|\\sigma_{n+1} - \\sigma_n|$ to zero.\n\nDefine near-optimal efficiency over a finite computational budget as the maximization of an estimator’s Effective Sample Size (ESS), where the ESS is inversely proportional to the Integrated Autocorrelation Time (IACT). For a scalar function $f(x)$ of the state, with samples $\\{f(X_i)\\}_{i=1}^N$, define the IACT by $\\tau = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_k$, where $\\rho_k$ is the lag-$k$ autocorrelation of the stationary sequence $\\{f(X_i)\\}$. The ESS is $N / \\tau$. Over a fixed budget of iterations, higher ESS indicates better near-optimal efficiency.\n\nYour task is to determine which $\\alpha$ in a given finite set approximately maximizes efficiency while satisfying a numerical check of Diminishing Adaptation, and to test sensitivity to target smoothness. Work from the following foundational base:\n- The Metropolis–Hastings framework for constructing reversible Markov chains with stationary density $p(x)$.\n- The definition of diminishing adaptation for adaptive MCMC, as the vanishing of the adaptation magnitude, here approximated by $|\\sigma_{n+1} - \\sigma_n| \\to 0$.\n- The relationship between IACT and ESS, with ESS $= N/\\tau$, where $\\tau$ is estimated from sample autocorrelations.\n- The standard law of large numbers for MCMC averages under appropriate ergodicity conditions, justifying the ESS-based comparison over a fixed finite horizon.\n\nYou must implement the following experiment entirely in code:\n\n1) Targets capturing different smoothness regimes:\n- Smooth, strongly log-concave: standard normal target with density $p_{\\mathrm{G}}(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\{-x^2/2\\}$.\n- Non-smooth log-density at the mode: Laplace target with density $p_{\\mathrm{L}}(x) = \\frac{1}{2}\\exp\\{-|x|\\}$.\n- Smooth but heavy-tailed: Student-$t$ with $\\nu = 3$ degrees of freedom, $p_{\\mathrm{T}}(x) = \\frac{\\Gamma((\\nu+1)/2)}{\\sqrt{\\nu\\pi}\\,\\Gamma(\\nu/2)}\\left(1 + \\frac{x^2}{\\nu}\\right)^{-(\\nu+1)/2}$ with $\\nu = 3$.\n\n2) Algorithmic setup shared across targets:\n- Proposal scale schedule $\\sigma_n = c \\, n^{-\\alpha}$ with $c = 1.5$ and $\\alpha$ selected from the finite set $\\{0.2, 0.5, 0.8\\}$.\n- Total iterations $T = 20000$ and burn-in $B = 2000$.\n- Initial state $x_0 = 0$.\n- Use the RWM algorithm with symmetric Gaussian proposals as given.\n- For efficiency, evaluate the function $f(x) = x$.\n- Estimate the Integrated Autocorrelation Time $\\tau$ using a consistent lag-window method grounded in the initial positive sequence rule on the empirical autocorrelation sequence $\\{\\rho_k\\}_{k \\ge 1}$.\n- Compute the ESS as $N/\\tau$ where $N = T - B$.\n\n3) Diminishing Adaptation numerical verification:\n- For a given $\\alpha$, compute the terminal adaptation magnitude $\\Delta_T = |\\sigma_{T+1} - \\sigma_T|$ and require $\\Delta_T \\lt \\epsilon$ with $\\epsilon = 10^{-5}$. Only $\\alpha$ that satisfy this check are eligible for selection for that target.\n\n4) Selection criterion:\n- For each target, among eligible $\\alpha$, choose the $\\alpha$ that maximizes the ESS. In the event of a numerical tie within a relative tolerance of $10^{-3}$ in ESS, choose the smallest $\\alpha$ among the tied values.\n\n5) Test suite:\n- Target set: $\\{p_{\\mathrm{G}}, p_{\\mathrm{L}}, p_{\\mathrm{T}}\\}$ as specified above, in that order.\n- Candidate $\\alpha$ set: $\\{0.2, 0.5, 0.8\\}$.\n- Hyperparameters: $c = 1.5$, $T = 20000$, $B = 2000$, $\\epsilon = 10^{-5}$, $x_0 = 0$.\n\n6) Final output format:\n- Your program should produce a single line of output containing the selected $\\alpha$ values, one per target in the order $\\{p_{\\mathrm{G}}, p_{\\mathrm{L}}, p_{\\mathrm{T}}\\}$, as a comma-separated list enclosed in square brackets, with each $\\alpha$ rounded to one decimal place, for example $[0.2,0.5,0.2]$.\n- There are no physical units involved in this problem, and angles are not used.\n\nYour implementation must be fully deterministic by setting an explicit random number generator seed of your choice. The solution should be principled and reproducible.\n\nYour deliverables are:\n- A complete, runnable program that performs the described RWM simulations, computes the ESS for each candidate $\\alpha$, verifies the Diminishing Adaptation check, selects $\\alpha$ per target according to the criterion, and prints the final list in the exact required format.\n- The single line of program output as specified.",
            "solution": "The problem requires conducting a numerical experiment to determine the optimal adaptation rate parameter, $\\alpha$, for a Random-Walk Metropolis (RWM) algorithm. The optimization is performed for a finite number of iterations and is evaluated across three different target probability distributions, each representing a distinct challenge for the sampler.\n\n### Principles and Methodology\n\nThe core of the problem lies in the intersection of three key concepts in computational statistics: the Metropolis-Hastings algorithm, adaptive Markov Chain Monte Carlo (MCMC), and the evaluation of sampler efficiency.\n\n**1. Random-Walk Metropolis (RWM) Algorithm**\n\nThe RWM is a specific instance of the Metropolis-Hastings algorithm used to generate a sequence of samples $\\{X_0, X_1, \\dots, X_T\\}$ from a target probability distribution with density $p(x)$. At each step $n$, a candidate state $x^{\\prime}$ is proposed from a symmetric distribution centered at the current state $x_n$. The problem specifies a Gaussian proposal:\n$$\nx^{\\prime} = x_n + \\eta, \\quad \\text{where } \\eta \\sim \\mathcal{N}(0, \\sigma_n^2)\n$$\nThe candidate is accepted with probability $a(x_n, x^{\\prime})$ given by:\n$$\na(x_n, x^{\\prime}) = \\min\\left\\{1, \\frac{p(x^{\\prime})}{p(x_n)}\\right\\} = \\min\\left\\{1, \\exp(\\log p(x^{\\prime}) - \\log p(x_n))\\right\\}\n$$\nIf the proposal is accepted, $X_{n+1} = x^{\\prime}$; otherwise, it is rejected, and $X_{n+1} = x_n$. The performance of the RWM algorithm is critically dependent on the proposal scale $\\sigma_n$. A small $\\sigma_n$ leads to high acceptance rates but slow exploration of the state space, while a large $\\sigma_n$ leads to frequent rejections and a chain that rarely moves.\n\n**2. Adaptive MCMC and Diminishing Adaptation**\n\nFinding an optimal, fixed $\\sigma$ can be difficult. Adaptive MCMC addresses this by allowing $\\sigma_n$ to change during the simulation, learning a suitable value as it runs. The problem specifies a power-law decay schedule for the proposal scale:\n$$\n\\sigma_n = c \\, n^{-\\alpha}\n$$\nwhere $c > 0$ is an initial scale factor and $\\alpha > 0$ controls the rate of decay. For an adaptive MCMC algorithm to be theoretically valid (i.e., for its empirical averages to converge to the correct expectations), the adaptation must diminish over time. This is known as the \"Diminishing Adaptation\" condition. Formally, it requires the total variation distance between successive transition kernels to converge to zero. For the given RWM scheme, a necessary (though not always sufficient) condition is that the magnitude of change in the adaptive parameter vanishes, i.e., $|\\sigma_{n+1} - \\sigma_n| \\to 0$.\n\nFor the schedule $\\sigma_n = c n^{-\\alpha}$, we have:\n$$\n|\\sigma_{n+1} - \\sigma_n| = |c(n+1)^{-\\alpha} - c n^{-\\alpha}| = c n^{-\\alpha} |(1+1/n)^{-\\alpha} - 1|\n$$\nUsing a Taylor expansion for large $n$, $(1+1/n)^{-\\alpha} \\approx 1 - \\alpha/n$, so we have:\n$$\n|\\sigma_{n+1} - \\sigma_n| \\approx c n^{-\\alpha} \\left|-\\frac{\\alpha}{n}\\right| = c\\alpha n^{-(\\alpha+1)}\n$$\nThis quantity converges to zero for any $\\alpha > 0$. The problem imposes a numerical check on this condition over a finite horizon $T$, requiring that the terminal adaptation magnitude $\\Delta_T = |\\sigma_{T+1} - \\sigma_T|$ be smaller than a threshold $\\epsilon = 10^{-5}$. An $\\alpha$ value is only considered eligible if it satisfies this check. A larger $\\alpha$ causes the adaptation to diminish more quickly, satisfying the condition more strongly but potentially \"freezing\" the scale $\\sigma_n$ at a suboptimal value too early in the run.\n\n**3. Sampler Efficiency: IACT and ESS**\n\nThe efficiency of an MCMC sampler is measured by how quickly it produces independent samples from the target distribution. Since MCMC samples are correlated, we use the Effective Sample Size (ESS) as a metric. Given $N$ post-burn-in samples, the ESS is defined as:\n$$\n\\text{ESS} = \\frac{N}{\\tau}\n$$\nwhere $\\tau$ is the Integrated Autocorrelation Time (IACT), defined as:\n$$\n\\tau = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_k\n$$\nHere, $\\rho_k$ is the lag-$k$ autocorrelation of the stationary sequence of a function of the states, $\\{f(X_i)\\}$. A smaller IACT implies less correlation between samples and thus a higher ESS and better efficiency. The task is to maximize ESS for the function $f(x) = x$. The IACT must be estimated from the finite sample chain. We use the \"initial positive sequence\" method, which estimates $\\tau$ by summing the empirical autocorrelations as long as they remain positive, a common heuristic to avoid noise from higher-lag estimates.\n\n### Experimental Design and Implementation\n\nThe experiment proceeds as follows for each of the three target distributions (Normal, Laplace, Student-t):\n\n1.  **Eligibility Check**: For each candidate $\\alpha \\in \\{0.2, 0.5, 0.8\\}$, the terminal adaptation magnitude $\\Delta_T = |\\sigma_{T+1} - \\sigma_T|$ is calculated with $c=1.5$ and $T=20000$. If $\\Delta_T \\ge 10^{-5}$, the $\\alpha$ is discarded.\n\n2.  **Simulation**: For each eligible $\\alpha$, an RWM simulation is run for $T=20000$ iterations starting from $x_0=0$. The proposal scale at step $n$ is $\\sigma_n = 1.5 \\times n^{-\\alpha}$. The first $B=2000$ samples are discarded as burn-in. To ensure a fair comparison, the random number generator is re-seeded for each simulation.\n\n3.  **Efficiency Calculation**: The post-burn-in chain of $N = T-B = 18000$ samples is used to estimate the IACT for $f(x)=x$. This involves computing the empirical autocorrelation function (ACF) and summing the initial positive terms. The ESS is then computed as $N/\\text{IACT}$.\n\n4.  **Selection**: After computing the ESS for all eligible $\\alpha$ values, the optimal $\\alpha$ is chosen. The primary criterion is to select the $\\alpha$ that yields the maximum ESS. A tie-breaking rule is specified: if multiple $\\alpha$ values produce ESS values that are within a relative tolerance of $10^{-3}$ of the maximum ESS, the smallest $\\alpha$ among them is chosen.\n\nThis procedure is repeated for each of the three target densities, which are chosen to test the algorithm's performance on targets with different smoothness and tail-weight properties:\n-   **Gaussian ($p_G$)**: Smooth and strongly log-concave, the ideal case for many samplers.\n-   **Laplace ($p_L$)**: Has a non-differentiable peak at the mode, challenging for algorithms that rely on gradients.\n-   **Student-t ($p_T$ with $\\nu=3$)**: Smooth but with heavier tails than the Gaussian, which can make exploration difficult.\n\nThe final output is a list of the selected $\\alpha$ values for the Gaussian, Laplace, and Student-t targets, in that order.\n\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ... # No other libraries needed\n\ndef solve():\n    \"\"\"\n    Performs an MCMC experiment to find the optimal adaptation rate alpha\n    for a Random-Walk Metropolis sampler on different target distributions.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = {\n        # Hyperparameters\n        \"SEED\": 42,\n        \"C\": 1.5,\n        \"ALPHA_CANDIDATES\": [0.2, 0.5, 0.8],\n        \"T\": 20000,\n        \"B\": 2000,\n        \"X0\": 0.0,\n        \"EPSILON\": 1e-5,\n        \"REL_TOL\": 1e-3,\n        \"NU_T\": 3.0\n    }\n\n    # Unnormalized log-probability density functions for the targets.\n    # Constant terms are omitted as they cancel in the M-H ratio.\n    def logp_normal(x):\n        return -0.5 * x**2\n\n    def logp_laplace(x):\n        return -np.abs(x)\n\n    def logp_student_t(x, nu=test_cases[\"NU_T\"]):\n        return -((nu + 1) / 2.0) * np.log(1.0 + x**2 / nu)\n\n    targets = [\n        {'name': 'Gaussian', 'log_p': logp_normal},\n        {'name': 'Laplace', 'log_p': logp_laplace},\n        {'name': 'Student-t', 'log_p': logp_student_t}\n    ]\n\n    def calculate_ess(samples):\n        \"\"\"\n        Calculates the Effective Sample Size (ESS) from a sequence of samples.\n        IACT is estimated using the initial positive sequence rule.\n        \"\"\"\n        n_samples = len(samples)\n        if n_samples  2:\n            return 0.0\n\n        demeaned_samples = samples - np.mean(samples)\n        \n        # Calculate autocovariance using FFT for efficiency\n        n_fft = 1  (2 * n_samples - 1).bit_length() # Next power of 2\n        fft_samples = np.fft.fft(demeaned_samples, n_fft)\n        autocov = np.fft.ifft(fft_samples * np.conj(fft_samples)).real\n        autocov = autocov[:n_samples] / n_samples\n        \n        if autocov[0] == 0:\n            return 0.0\n\n        acf = autocov / autocov[0]\n        \n        # Estimate IACT using the initial positive sequence rule\n        iact = 1.0\n        # Sum positive correlations up to a max lag for robustness\n        max_lag = n_samples // 5 \n        for k in range(1, min(len(acf), max_lag)):\n            if acf[k] > 0:\n                iact += 2.0 * acf[k]\n            else:\n                break\n        \n        return n_samples / iact if iact > 0 else 0.0\n\n    def rwm_sampler(log_p, x0, t_total, t_burn, c_adapt, alpha_adapt, rng):\n        \"\"\"\n        Implements the adaptive Random-Walk Metropolis sampler.\n        \"\"\"\n        chain = np.zeros(t_total)\n        x_current = x0\n        log_p_current = log_p(x_current)\n        \n        for n in range(1, t_total + 1):\n            sigma_n = c_adapt * n**(-alpha_adapt)\n            proposal = x_current + rng.normal(loc=0.0, scale=sigma_n)\n            \n            log_p_proposal = log_p(proposal)\n            log_acceptance_ratio = log_p_proposal - log_p_current\n            \n            if np.log(rng.uniform(0.0, 1.0))  log_acceptance_ratio:\n                x_current = proposal\n                log_p_current = log_p_proposal\n            \n            chain[n-1] = x_current\n            \n        return chain[t_burn:]\n\n    results = []\n    \n    C = test_cases[\"C\"]\n    T = test_cases[\"T\"]\n    B = test_cases[\"B\"]\n    X0 = test_cases[\"X0\"]\n    EPSILON = test_cases[\"EPSILON\"]\n    REL_TOL = test_cases[\"REL_TOL\"]\n    ALPHA_CANDIDATES = test_cases[\"ALPHA_CANDIDATES\"]\n    SEED = test_cases[\"SEED\"]\n\n    for target_spec in targets:\n        log_p_func = target_spec['log_p']\n        ess_results = []\n        \n        for alpha in ALPHA_CANDIDATES:\n            # 1. Diminishing Adaptation numerical verification\n            sigma_T = C * T**(-alpha)\n            sigma_T_plus_1 = C * (T + 1)**(-alpha)\n            delta_T = np.abs(sigma_T_plus_1 - sigma_T)\n            \n            if delta_T >= EPSILON:\n                continue\n\n            # 2. Run MCMC simulation. Reset seed for each run for fair comparison.\n            rng = np.random.default_rng(seed=SEED)\n            samples = rwm_sampler(log_p_func, X0, T, B, C, alpha, rng)\n            \n            # 3. Calculate ESS\n            ess = calculate_ess(samples)\n            ess_results.append((alpha, ess))\n            \n        # 4. Selection criterion: find best alpha\n        if not ess_results:\n            # This should not happen since all alphas pass the DA check.\n            # Handle defensively by appending a placeholder.\n            results.append(np.nan) \n            continue\n\n        max_ess = -1.0\n        for _, ess in ess_results:\n            if ess > max_ess:\n                max_ess = ess\n        \n        # Identify alphas within the tolerance of the max ESS\n        tied_alphas = []\n        for alpha, ess in ess_results:\n            if ess >= max_ess * (1.0 - REL_TOL):\n                tied_alphas.append(alpha)\n        \n        # Choose the smallest alpha among the tied values\n        best_alpha = min(tied_alphas)\n        results.append(best_alpha)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.1f}' for r in results)}]\")\n\nsolve()\n```",
            "answer": "[0.2,0.5,0.2]"
        }
    ]
}