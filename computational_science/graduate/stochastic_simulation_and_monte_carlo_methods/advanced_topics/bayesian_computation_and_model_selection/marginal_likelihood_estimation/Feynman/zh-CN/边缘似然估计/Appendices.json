{
    "hands_on_practices": [
        {
            "introduction": "理论学习之后，最好的检验方式便是动手实践。我们从一个理想化的场景开始：在共轭模型中，边际似然有时可以被精确地解析计算。这项练习  将指导您在一个简单的正态-正态模型中推导贝叶斯因子 (Bayes Factor) 的精确表达式，它为您提供了一个宝贵的“黄金标准”，并让您深入理解模型证据如何对先验信念的确定性（即先验方差）做出反应，直观地揭示贝叶斯奥卡姆剃刀原则。",
            "id": "3319160",
            "problem": "考虑一个具有已知方差的单参数正态似然。您从抽样模型 $y_{i} \\mid \\mu \\sim \\mathcal{N}(\\mu,\\sigma^{2})$ 中观测到 $n$ 个独立同分布 (i.i.d.) 的样本 $y_{1},\\dots,y_{n}$，其中 $\\sigma^{2} > 0$ 已知。为未知均值 $\\mu$ 设定了两个贝叶斯模型：\n- 模型 $\\mathcal{M}_{1}$: $\\mu \\sim \\mathcal{N}(m_{0},\\tau_{1}^{2})$，其中 $\\tau_{1}^{2} > 0$，\n- 模型 $\\mathcal{M}_{2}$: $\\mu \\sim \\mathcal{N}(m_{0},\\tau_{2}^{2})$，其中 $\\tau_{2}^{2} > 0$，\n\n其中 $m_{0} \\in \\mathbb{R}$ 是一个共同的先验均值，$\\tau_{1}^{2} \\neq \\tau_{2}^{2}$ 是不同的先验方差。令 $\\bar{y} \\equiv \\frac{1}{n}\\sum_{i=1}^{n} y_{i}$ 表示样本均值，令 $R \\equiv \\sum_{i=1}^{n} (y_{i}-\\bar{y})^{2}$ 表示样本内平方和。支持 $\\mathcal{M}_{1}$ 而非 $\\mathcal{M}_{2}$ 的贝叶斯因子 (BF) 定义为边际似然的比率，\n$$\nBF_{12} \\equiv \\frac{p(y_{1},\\dots,y_{n} \\mid \\mathcal{M}_{1})}{p(y_{1},\\dots,y_{n} \\mid \\mathcal{M}_{2})},\n$$\n其中，对于每个模型 $\\mathcal{M}_{j}$（$j \\in \\{1,2\\}$），边际似然 $p(y_{1},\\dots,y_{n} \\mid \\mathcal{M}_{j})$ 由似然函数对先验积分得到。\n\n仅从边际似然作为积分的定义和高斯密度的性质出发，完成以下任务：\n- 通过在每个模型下对 $\\mu$ 进行解析积分，推导 $p(y_{1},\\dots,y_{n} \\mid \\mathcal{M}_{j})$ 的闭式表达式，并用 $n$、$\\sigma^{2}$、$\\tau_{j}^{2}$、$m_{0}$、$\\bar{y}$ 和 $R$ 来表示结果。\n- 使用您的结果，计算 $BF_{12}$ 的闭式解并尽可能简化。\n\n然后，通过分析 $\\ln BF_{12}$ 关于 $\\tau_{1}^{2}$ 和 $\\tau_{2}^{2}$ 的偏导数的符号，评估当先验方差 $\\tau_{1}^{2}$ 和 $\\tau_{2}^{2}$ 变化时，$BF_{12}$ 如何变化。简要解释样本量 $n$、已知方差 $\\sigma^{2}$ 以及差异 $|\\bar{y}-m_{0}|$ 在此灵敏度分析中的作用。\n\n答案格式要求：仅报告 $BF_{12}$ 的最终简化解析表达式。不要近似。您最终的方框答案必须是单个闭式解析表达式，不含单位，也无额外评注。",
            "solution": "该问题要求推导贝叶斯因子 $BF_{12}$，用于比较两个仅在参数 $\\mu$ 的先验方差上有所不同的模型 $\\mathcal{M}_{1}$ 和 $\\mathcal{M}_{2}$。这首先需要为通用模型 $\\mathcal{M}_{j}$ 找到边际似然 $p(y_{1},\\dots,y_{n} \\mid \\mathcal{M}_{j})$ 的闭式表达式。\n\n### 第 1 步：推导边际似然 $p(y_{1},\\dots,y_{n} \\mid \\mathcal{M}_{j})$\n\n模型 $\\mathcal{M}_{j}$ 的边际似然定义为似然函数乘以参数 $\\mu$ 的先验概率密度的积分：\n$$ p(y_{1},\\dots,y_{n} \\mid \\mathcal{M}_{j}) = \\int_{-\\infty}^{\\infty} p(y_{1},\\dots,y_{n} \\mid \\mu, \\sigma^{2}) p(\\mu \\mid \\mathcal{M}_{j}) d\\mu $$\n令数据由向量 $\\mathbf{y} = (y_{1},\\dots,y_{n})$ 表示。\n\n似然函数 $p(\\mathbf{y} \\mid \\mu, \\sigma^{2})$ 由 $n$ 个独立高斯密度的乘积给出，因为样本是来自 $\\mathcal{N}(\\mu, \\sigma^{2})$ 的独立同分布样本：\n$$ p(\\mathbf{y} \\mid \\mu, \\sigma^{2}) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left(-\\frac{(y_{i} - \\mu)^{2}}{2\\sigma^{2}}\\right) = (2\\pi\\sigma^{2})^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{n} (y_{i} - \\mu)^{2}\\right) $$\n指数中的平方和可以使用样本均值 $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^{n} y_{i}$ 和样本内平方和 $R = \\sum_{i=1}^{n} (y_{i}-\\bar{y})^{2}$ 进行分解：\n$$ \\sum_{i=1}^{n} (y_{i} - \\mu)^{2} = \\sum_{i=1}^{n} ((y_{i} - \\bar{y}) + (\\bar{y} - \\mu))^{2} = \\sum_{i=1}^{n} (y_{i} - \\bar{y})^{2} + n(\\bar{y} - \\mu)^{2} = R + n(\\bar{y} - \\mu)^{2} $$\n将此代回似然函数可得：\n$$ p(\\mathbf{y} \\mid \\mu, \\sigma^{2}) = (2\\pi\\sigma^{2})^{-n/2} \\exp\\left(-\\frac{R}{2\\sigma^{2}}\\right) \\exp\\left(-\\frac{n(\\mu - \\bar{y})^{2}}{2\\sigma^{2}}\\right) $$\n模型 $\\mathcal{M}_{j}$ 中 $\\mu$ 的先验是 $p(\\mu \\mid \\mathcal{M}_{j}) \\sim \\mathcal{N}(m_{0}, \\tau_{j}^{2})$：\n$$ p(\\mu \\mid \\mathcal{M}_{j}) = \\frac{1}{\\sqrt{2\\pi\\tau_{j}^{2}}} \\exp\\left(-\\frac{(\\mu - m_{0})^{2}}{2\\tau_{j}^{2}}\\right) $$\n现在，我们可以写出边际似然的积分。我们可以将不依赖于 $\\mu$ 的项提取出来：\n$$ p(\\mathbf{y} \\mid \\mathcal{M}_{j}) = (2\\pi\\sigma^{2})^{-n/2} (2\\pi\\tau_{j}^{2})^{-1/2} \\exp\\left(-\\frac{R}{2\\sigma^{2}}\\right) \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{n(\\mu - \\bar{y})^{2}}{2\\sigma^{2}} - \\frac{(\\mu - m_{0})^{2}}{2\\tau_{j}^{2}}\\right) d\\mu $$\n为求解该积分，我们对指数中的 $\\mu$ 进行配方。令指数的参数为 $-\\frac{1}{2}Q(\\mu)$：\n$$ Q(\\mu) = \\frac{n(\\mu - \\bar{y})^{2}}{\\sigma^{2}} + \\frac{(\\mu - m_{0})^{2}}{\\tau_{j}^{2}} $$\n展开平方项：\n$$ Q(\\mu) = \\mu^{2}\\left(\\frac{n}{\\sigma^{2}} + \\frac{1}{\\tau_{j}^{2}}\\right) - 2\\mu\\left(\\frac{n\\bar{y}}{\\sigma^{2}} + \\frac{m_{0}}{\\tau_{j}^{2}}\\right) + \\left(\\frac{n\\bar{y}^{2}}{\\sigma^{2}} + \\frac{m_{0}^{2}}{\\tau_{j}^{2}}\\right) $$\n这是一个关于 $\\mu$ 的二次式，形式为 $A\\mu^{2} - 2B\\mu + C$。我们可以将其写为 $A(\\mu - B/A)^{2} - B^{2}/A + C$。\n$\\mu$ 的后验精度为 $A = \\frac{1}{\\tau_{jn}^{2}} = \\frac{n}{\\sigma^{2}} + \\frac{1}{\\tau_{j}^{2}} = \\frac{n\\tau_{j}^{2} + \\sigma^{2}}{\\sigma^{2}\\tau_{j}^{2}}$。\n后验均值为 $m_{jn} = B/A = \\left(\\frac{n\\bar{y}}{\\sigma^{2}} + \\frac{m_{0}}{\\tau_{j}^{2}}\\right) \\tau_{jn}^{2} = \\frac{n\\tau_{j}^{2}\\bar{y} + \\sigma^{2}m_{0}}{n\\tau_{j}^{2} + \\sigma^{2}}$。\n该积分为高斯核的形式：\n$$ \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{(\\mu - m_{jn})^{2}}{2\\tau_{jn}^{2}}\\right) \\exp\\left(-\\frac{1}{2}(C - B^{2}/A)\\right) d\\mu $$\n$C - B^{2}/A$ 项简化为 $\\frac{n(\\bar{y}-m_{0})^{2}}{n\\tau_{j}^{2} + \\sigma^{2}}$。\n积分的计算结果为 $\\sqrt{2\\pi\\tau_{jn}^{2}}$。\n因此完整的积分为：\n$$ I = \\sqrt{2\\pi\\tau_{jn}^{2}} \\exp\\left(-\\frac{n(\\bar{y}-m_{0})^{2}}{2(n\\tau_{j}^{2} + \\sigma^{2})}\\right) = \\sqrt{2\\pi \\frac{\\sigma^{2}\\tau_{j}^{2}}{n\\tau_{j}^{2} + \\sigma^{2}}} \\exp\\left(-\\frac{n(\\bar{y}-m_{0})^{2}}{2(n\\tau_{j}^{2} + \\sigma^{2})}\\right) $$\n将此代回 $p(\\mathbf{y} \\mid \\mathcal{M}_{j})$ 的表达式中：\n$$ p(\\mathbf{y} \\mid \\mathcal{M}_{j}) = (2\\pi\\sigma^{2})^{-n/2} (2\\pi\\tau_{j}^{2})^{-1/2} \\exp\\left(-\\frac{R}{2\\sigma^{2}}\\right) \\sqrt{2\\pi \\frac{\\sigma^{2}\\tau_{j}^{2}}{n\\tau_{j}^{2} + \\sigma^{2}}} \\exp\\left(-\\frac{n(\\bar{y}-m_{0})^{2}}{2(n\\tau_{j}^{2} + \\sigma^{2})}\\right) $$\n简化常数前置因子：\n$$ (2\\pi)^{-n/2}(\\sigma^{2})^{-n/2}(2\\pi)^{-1/2}(\\tau_{j}^{2})^{-1/2}(2\\pi)^{1/2}\\frac{\\sigma\\tau_{j}}{\\sqrt{n\\tau_{j}^{2} + \\sigma^{2}}} = (2\\pi)^{-n/2}(\\sigma^{2})^{-(n-1)/2}(n\\tau_{j}^{2} + \\sigma^{2})^{-1/2} $$\n因此，边际似然的最终表达式为：\n$$ p(\\mathbf{y} \\mid \\mathcal{M}_{j}) = (2\\pi)^{-n/2} (\\sigma^{2})^{-(n-1)/2} (n\\tau_{j}^{2} + \\sigma^{2})^{-1/2} \\exp\\left(-\\frac{R}{2\\sigma^{2}}\\right) \\exp\\left(-\\frac{n(\\bar{y}-m_{0})^{2}}{2(n\\tau_{j}^{2} + \\sigma^{2})}\\right) $$\n\n### 第 2 步：计算贝叶斯因子 $BF_{12}$\n\n贝叶斯因子 $BF_{12}$ 是模型 $\\mathcal{M}_{1}$ 和 $\\mathcal{M}_{2}$ 的边际似然之比：\n$$ BF_{12} = \\frac{p(\\mathbf{y} \\mid \\mathcal{M}_{1})}{p(\\mathbf{y} \\mid \\mathcal{M}_{2})} $$\n使用上面推导的表达式，我们构建这个比率。所有不依赖于模型索引 $j$ 的项都将消去。这些项是 $(2\\pi)^{-n/2}$、$(\\sigma^{2})^{-(n-1)/2}$ 和 $\\exp\\left(-R/(2\\sigma^{2})\\right)$。\n$$ BF_{12} = \\frac{(n\\tau_{1}^{2} + \\sigma^{2})^{-1/2} \\exp\\left(-\\frac{n(\\bar{y}-m_{0})^{2}}{2(n\\tau_{1}^{2} + \\sigma^{2})}\\right)}{(n\\tau_{2}^{2} + \\sigma^{2})^{-1/2} \\exp\\left(-\\frac{n(\\bar{y}-m_{0})^{2}}{2(n\\tau_{2}^{2} + \\sigma^{2})}\\right)} $$\n这可以简化为：\n$$ BF_{12} = \\left(\\frac{n\\tau_{2}^{2} + \\sigma^{2}}{n\\tau_{1}^{2} + \\sigma^{2}}\\right)^{1/2} \\exp\\left( \\frac{n(\\bar{y}-m_{0})^{2}}{2} \\left[ \\frac{1}{n\\tau_{2}^{2} + \\sigma^{2}} - \\frac{1}{n\\tau_{1}^{2} + \\sigma^{2}} \\right] \\right) $$\n为指数括号中的项找到公分母：\n$$ \\frac{1}{n\\tau_{2}^{2} + \\sigma^{2}} - \\frac{1}{n\\tau_{1}^{2} + \\sigma^{2}} = \\frac{(n\\tau_{1}^{2} + \\sigma^{2}) - (n\\tau_{2}^{2} + \\sigma^{2})}{(n\\tau_{1}^{2} + \\sigma^{2})(n\\tau_{2}^{2} + \\sigma^{2})} = \\frac{n(\\tau_{1}^{2} - \\tau_{2}^{2})}{(n\\tau_{1}^{2} + \\sigma^{2})(n\\tau_{2}^{2} + \\sigma^{2})} $$\n将此代回，得到贝叶斯因子的最终简化表达式：\n$$ BF_{12} = \\left(\\frac{n\\tau_{2}^{2} + \\sigma^{2}}{n\\tau_{1}^{2} + \\sigma^{2}}\\right)^{1/2} \\exp\\left( \\frac{n^{2}(\\bar{y}-m_{0})^{2}(\\tau_{1}^{2} - \\tau_{2}^{2})}{2(n\\tau_{1}^{2} + \\sigma^{2})(n\\tau_{2}^{2} + \\sigma^{2})} \\right) $$\n\n### 第 3 步：灵敏度分析\n\n为了分析 $BF_{12}$ 如何随先验方差变化，我们考察其自然对数 $\\ln(BF_{12})$ 的偏导数。\n$$ \\ln(BF_{12}) = \\frac{1}{2}\\ln(n\\tau_{2}^{2}+\\sigma^{2}) - \\frac{1}{2}\\ln(n\\tau_{1}^{2}+\\sigma^{2}) + \\frac{n(\\bar{y}-m_{0})^{2}}{2}\\left(\\frac{1}{n\\tau_{2}^{2}+\\sigma^{2}} - \\frac{1}{n\\tau_{1}^{2}+\\sigma^{2}}\\right) $$\n关于 $\\tau_{1}^{2}$ 的偏导数为：\n$$ \\frac{\\partial \\ln(BF_{12})}{\\partial \\tau_{1}^{2}} = -\\frac{n}{2(n\\tau_{1}^{2}+\\sigma^{2})} + \\frac{n(\\bar{y}-m_{0})^{2}}{2} \\left(\\frac{n}{(n\\tau_{1}^{2}+\\sigma^{2})^{2}}\\right) = \\frac{n}{2(n\\tau_{1}^{2}+\\sigma^{2})^{2}} \\left[ n(\\bar{y}-m_{0})^{2} - (n\\tau_{1}^{2}+\\sigma^{2}) \\right] $$\n该导数的符号由方括号中的项决定。$\\frac{\\partial \\ln(BF_{12})}{\\partial \\tau_{1}^{2}} > 0$ 当且仅当 $n(\\bar{y}-m_{0})^{2} > n\\tau_{1}^{2}+\\sigma^{2}$，或 $(\\bar{y}-m_{0})^{2} > \\tau_{1}^{2} + \\frac{\\sigma^{2}}{n}$。这意味着当数据与先验的差异的平方 $|\\bar{y}-m_{0}|^{2}$ 大于模型 $\\mathcal{M}_{1}$ 下 $\\bar{y}$ 的预测方差时，增加先验方差 $\\tau_{1}^{2}$ 会增加支持 $\\mathcal{M}_{1}$ 的证据。\n\n关于 $\\tau_{2}^{2}$ 的偏导数为：\n$$ \\frac{\\partial \\ln(BF_{12})}{\\partial \\tau_{2}^{2}} = \\frac{n}{2(n\\tau_{2}^{2}+\\sigma^{2})} - \\frac{n(\\bar{y}-m_{0})^{2}}{2} \\left(\\frac{n}{(n\\tau_{2}^{2}+\\sigma^{2})^{2}}\\right) = \\frac{n}{2(n\\tau_{2}^{2}+\\sigma^{2})^{2}} \\left[ (n\\tau_{2}^{2}+\\sigma^{2}) - n(\\bar{y}-m_{0})^{2} \\right] $$\n该符号为正当且仅当 $n\\tau_{2}^{2}+\\sigma^{2} > n(\\bar{y}-m_{0})^{2}$，或 $\\tau_{2}^{2} + \\frac{\\sigma^{2}}{n} > (\\bar{y}-m_{0})^{2}$。这意味着当模型 $\\mathcal{M}_{2}$ 下 $\\bar{y}$ 的预测方差大于数据与先验的差异的平方时，增加先验方差 $\\tau_{2}^{2}$ 会增加支持 $\\mathcal{M}_{1}$ 的证据（通过减少支持 $\\mathcal{M}_{2}$ 的证据）。\n\n**参数的作用：** 贝叶斯因子对先验方差的灵敏度由项 $\\tau_{j}^{2} + \\sigma^{2}/n$（即模型 $\\mathcal{M}_{j}$ 下 $\\bar{y}$ 的预测方差）介导。\n- **$|\\bar{y}-m_{0}|$**：数据与先验均值之间的较大差异有利于具有较**大**先验方差的模型，因为这些模型会为更极端的观测值赋予更高的似然。\n- **$n$**：随着样本量 $n$ 的增加，项 $\\sigma^{2}/n$ 减小。数据变得更具信息量，预测方差趋近于先验方差 $\\tau_{j}^{2}$。贝叶斯因子对 $\\tau_{j}^{2}$ 的选择变得更加敏感。\n- **$\\sigma^{2}$**：较大的数据方差 $\\sigma^{2}$ 会增加两个模型的预测方差，使它们更难区分，并降低贝叶斯因子对 $\\tau_{j}^{2}$ 具体选择的灵敏度。",
            "answer": "$$ \\boxed{ \\left(\\frac{n\\tau_{2}^{2} + \\sigma^{2}}{n\\tau_{1}^{2} + \\sigma^{2}}\\right)^{1/2} \\exp\\left( \\frac{n^{2}(\\bar{y}-m_{0})^{2}(\\tau_{1}^{2} - \\tau_{2}^{2})}{2(n\\tau_{1}^{2} + \\sigma^{2})(n\\tau_{2}^{2} + \\sigma^{2})} \\right) } $$"
        },
        {
            "introduction": "在绝大多数现实模型中，解析积分是不可行的，我们必须依赖蒙特卡洛方法来估计边际似然。然而，仅仅计算出一个估计值是不够的，理解其统计性质对于确保结果的可靠性至关重要。这项练习  聚焦于重要性采样 (Importance Sampling, IS) 估计量，通过运用Delta方法推导其对数形式的方差。这不仅是一次理论推导，更揭示了一个深刻的实践准则：当估计量方差较大时，在对数尺度上进行聚合通常比在原始尺度上更为稳健，这对于处理高度倾斜的重要性权重分布尤为关键。",
            "id": "3319185",
            "problem": "考虑一个贝叶斯模型，其观测数据为 $y$，潜变量为 $x$，先验密度为 $p(x)$，似然为 $p(y \\mid x)$。边缘似然是归一化常数\n$$\nZ \\equiv \\int p(y \\mid x)\\,p(x)\\,dx.\n$$\n令 $q(x)$ 为一个提议密度，其支撑集覆盖 $p(y \\mid x)\\,p(x)$ 的支撑集。从 $q$ 中独立同分布地抽取样本 $X_{1},\\dots,X_{n} \\stackrel{\\text{i.i.d.}}{\\sim} q$，并定义重要性抽样 (IS) 权重为 $w(X_{i}) \\equiv \\frac{p(y \\mid X_{i})\\,p(X_{i})}{q(X_{i})}$，以及 $Z$ 的 IS 估计量为\n$$\n\\widehat{Z}_{n} \\equiv \\frac{1}{n}\\sum_{i=1}^{n} w(X_{i}).\n$$\n假设 $Z \\in (0,\\infty)$，权重 $w(X)$ 的二阶矩 $m_{2} \\equiv \\mathbb{E}_{q}\\!\\left[w(X)^{2}\\right]$ 是有限的，并且其方差 $\\sigma^{2} \\equiv \\mathrm{Var}_{q}\\!\\left(w(X)\\right)= m_{2} - Z^{2}$ 是正且有限的。考虑对数-IS 估计量 $\\ell_{n} \\equiv \\ln\\!\\left(\\widehat{Z}_{n}\\right)$。\n\n仅使用大数定律、中心极限定理和一阶Delta方法等基础性结论，推导 $\\ell_{n}$ 的大样本方差，其闭式解应表示为 $n$、$Z$ 和 $m_{2}$ 的表达式。请清楚地陈述您在推导过程中所引用的正则性条件。\n\n然后，基于您的推导，且不借助未经证实的启发式方法，提供一个有原则的建议，说明在何种情况下，在对数尺度上聚合独立运行的估计值（即对 $\\ell_{n}$ 求平均后取指数）优于在线性尺度上聚合（即对 $\\widehat{Z}_{n}$ 求平均）。\n\n您的最终答案必须是 $\\ell_{n}$ 的渐近方差的单个闭式表达式，用 $n$、$Z$ 和 $m_{2}$ 表示。无需四舍五入。",
            "solution": "目标是推导对数重要性采样（IS）估计量 $\\ell_{n} = \\ln(\\widehat{Z}_{n})$ 的大样本方差。\nIS估计量 $\\widehat{Z}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} w(X_{i})$ 是 $n$ 个独立同分布 (i.i.d.) 随机变量 $w(X_i)$ 的样本均值。这些变量的分布是由从提议密度 $q(x)$ 中抽取 $X_i$ 所引致的。\n\n首先，我们确定随机变量 $w(X)$ 的性质。它在提议分布 $q(x)$ 下的期望是：\n$$\n\\mathbb{E}_{q}[w(X)] = \\int w(x) q(x) dx = \\int \\frac{p(y \\mid x) p(x)}{q(x)} q(x) dx = \\int p(y \\mid x) p(x) dx = Z.\n$$\n这表明 IS 估计量 $\\widehat{Z}_n$ 是 $Z$ 的一个无偏估计量，因为 $\\mathbb{E}_{q}[\\widehat{Z}_n] = \\frac{1}{n}\\sum_{i=1}^{n}\\mathbb{E}_{q}[w(X_i)] = Z$。\n\n$w(X)$ 的方差在问题陈述中已给出，为 $\\sigma^2 = \\mathrm{Var}_{q}(w(X))$，其中 $\\sigma^2 = m_2 - Z^2$。由于权重 $w(X_i)$ 是独立同分布的，IS 估计量的方差为：\n$$\n\\mathrm{Var}_{q}(\\widehat{Z}_{n}) = \\mathrm{Var}_{q}\\left(\\frac{1}{n}\\sum_{i=1}^{n} w(X_i)\\right) = \\frac{1}{n^2} \\sum_{i=1}^n \\mathrm{Var}_q(w(X_i)) = \\frac{n \\sigma^2}{n^2} = \\frac{\\sigma^2}{n} = \\frac{m_{2} - Z^{2}}{n}.\n$$\n问题陈述指出 $Z$ 是有限的且 $\\sigma^2$ 是有限的，这是将中心极限定理 (CLT) 应用于样本均值 $\\widehat{Z}_n$ 的充分条件。CLT 表明：\n$$\n\\sqrt{n}(\\widehat{Z}_n - Z) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2),\n$$\n其中 $\\xrightarrow{d}$ 表示依分布收敛，$\\mathcal{N}(0, \\sigma^2)$ 是一个均值为 $0$、方差为 $\\sigma^2 = m_2 - Z^2$ 的正态分布。\n\n我们关心 $\\ell_n = \\ln(\\widehat{Z}_n)$ 的渐近分布。这可以使用一阶Delta方法找到。令 $g(z) = \\ln(z)$。Delta方法指出，对于一个随机变量序列 $T_n$ 满足 $\\sqrt{n}(T_n - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\tau^2)$，以及一个在 $\\theta$ 处可微且 $g'(\\theta) \\neq 0$ 的函数 $g$，以下结论成立：\n$$\n\\sqrt{n}(g(T_n) - g(\\theta)) \\xrightarrow{d} \\mathcal{N}(0, [g'(\\theta)]^2 \\tau^2).\n$$\n在我们的情况下，$T_n = \\widehat{Z}_n$，$\\theta = Z$，且 $\\tau^2 = \\sigma^2 = m_2 - Z^2$。函数为 $g(z) = \\ln(z)$，其导数为 $g'(z) = 1/z$。\nDelta方法的正则性条件得到满足：\n1.  问题陈述指出 $Z \\in (0, \\infty)$，因此 $g(z) = \\ln(z)$ 在 $z=Z$ 处是良定义的且连续可微的。\n2.  在 $Z$ 处的导数为 $g'(Z) = 1/Z$，由于 $Z \\in (0, \\infty)$，该导数非零。\n\n应用Delta方法：\n$$\n\\sqrt{n}(\\ln(\\widehat{Z}_n) - \\ln(Z)) \\xrightarrow{d} \\mathcal{N}\\left(0, [g'(Z)]^2 \\sigma^2\\right).\n$$\n代入 $g'(Z)$ 和 $\\sigma^2$ 的表达式：\n$$\n\\sqrt{n}(\\ell_n - \\ln(Z)) \\xrightarrow{d} \\mathcal{N}\\left(0, \\left(\\frac{1}{Z}\\right)^2 (m_2 - Z^2)\\right).\n$$\n$\\sqrt{n}(\\ell_n - \\ln(Z))$ 的渐近方差是 $\\frac{m_2 - Z^2}{Z^2}$。这意味着对于大的 $n$，$\\ell_n$ 的分布可以近似为一个正态分布：\n$$\n\\ell_n \\approx \\mathcal{N}\\left(\\ln(Z), \\frac{1}{n}\\frac{m_2 - Z^2}{Z^2}\\right).\n$$\n因此，$\\ell_n$ 的大样本方差是：\n$$\n\\mathrm{Var}(\\ell_n) \\approx \\frac{m_2 - Z^2}{nZ^2}.\n$$\n\n对于问题的第二部分，我们提供关于聚合估计值的建议。在线性尺度上聚合 $K$ 个独立估计值意味着计算算术平均值 $\\frac{1}{K}\\sum_{k=1}^K \\widehat{Z}_{n,k}$。在对数尺度上聚合然后变换回来意味着计算 $\\exp\\left(\\frac{1}{K}\\sum_{k=1}^K \\ell_{n,k}\\right) = \\left(\\prod_{k=1}^K \\widehat{Z}_{n,k}\\right)^{1/K}$，这是各个估计值的几何平均值。\n\n推导出的 $\\ell_n$ 的大样本方差可以重写为：\n$$\n\\mathrm{Var}(\\ell_n) \\approx \\frac{1}{n} \\left(\\frac{m_2}{Z^2} - 1\\right) = \\frac{1}{n} \\left(\\frac{\\mathbb{E}_q[w(X)^2]}{(\\mathbb{E}_q[w(X)])^2} - 1\\right) = \\frac{1}{n} \\frac{\\mathrm{Var}_q(w(X))}{(\\mathbb{E}_q[w(X)])^2} = \\frac{1}{n} [\\mathrm{CV}_q(w(X))]^2,\n$$\n其中 $\\mathrm{CV}_q(w(X))$ 是重要性权重的变异系数 (CV)。\n\n一个有原则的建议是基于估计量的性质。实践中的重要性抽样通常以高度右偏的权重分布为特征，这意味着少数权重比其余的大几个数量级。高度的偏斜对应于大的变异系数。当权重的CV很大时，对于有限的 $n$，估计量 $\\widehat{Z}_n$ 的分布也是偏斜的，并且根据中心极限定理其向正态性的收敛速度很慢。这类估计值的算术平均值对任何因离群权重而异常大的单个估计值都高度敏感。\n\n对数函数 $g(z)=\\ln(z)$ 是一个压缩大值的凹函数。将对数应用于 $\\widehat{Z}_n$ 产生统计量 $\\ell_n$，其分布通常比 $\\widehat{Z}_n$ 的分布更对称、更接近正态，特别是当底层权重具有高方差时。对数尺度上的中心极限定理（由Delta方法建立）对于有限的 $n$ 往往比原始尺度上的中心极限定理提供更好的近似。在对数尺度上求平均（即对 $\\ell_n$ 求平均）更稳健，因为人们平均的是性质更好且分布更接近对称的量。这对应于取几何平均值，众所周知，几何平均值比算术平均值对极端离群值不那么敏感。\n\n因此，建议如下：当重要性权重的变异系数很大时（即当 $m_2 \\gg Z^2$ 时），在对数尺度上进行聚合是更可取的。这一条件可以通过大的变异系数平方值 $\\frac{m_2 - Z^2}{Z^2}$ 来量化，这正是（按 $1/n$ 缩放后）控制我们上面推导的对数估计量 $\\ell_n$ 方差的项。在这种高方差情景中，对数变换提供了稳定和对称化的效应，从而得到更稳健和可靠的聚合估计值。",
            "answer": "$$\n\\boxed{\\frac{m_2 - Z^2}{n Z^2}}\n$$"
        },
        {
            "introduction": "当我们面对更具挑战性的场景，即似然函数本身难以计算或处理时，就需要更前沿的近似方法。近似贝叶斯计算 (Approximate Bayesian Computation, ABC) 作为一种“免似然” (likelihood-free) 技术应运而生。这项编程练习  将引导您亲手构建一个基于ABC的伪边际估计量，用于一个扩散过程模型。通过在一个实际的计算任务中调整容忍度 $\\epsilon$ 并比较不同核函数的效果，您将直接体验和量化在近似计算中普遍存在的“计算成本”与“统计精度”（偏差）之间的基本权衡。",
            "id": "3319175",
            "problem": "考虑由随机微分方程 $dX_t = \\sigma\\, dW_t$ 定义的一维扩散模型，其中 $X_0 = 0$，$W_t$ 是标准布朗运动。您在等间隔时间点 $t_k = k \\Delta$（其中 $k = 1, \\dots, n$，步长 $\\Delta$ 已知）观测离散增量，得到数据 $y = (y_1,\\dots,y_n)$，其中 $y_k = X_{t_k} - X_{t_{k-1}}$。在此模型下，各增量独立地满足 $y_k \\sim \\mathcal{N}(0, \\sigma^2 \\Delta)$。设参数为 $\\theta = \\sigma^2$，其先验为逆伽马分布 $\\theta \\sim \\text{Inverse-Gamma}(\\alpha, \\beta)$，其中形状参数为 $\\alpha$，尺度参数为 $\\beta$。\n\n定义充分概括统计量 $S(y) = \\sum_{k=1}^n y_k^2$。边际似然（也称为证据）为 $p(y) = \\int p(y \\mid \\theta)\\, p(\\theta)\\, d\\theta$。\n\n您将使用基于概括统计量和核加权的近似贝叶斯计算 (ABC) 方法，构建 $p(y)$ 的伪边际估计量。该估计量应通过从 $p(\\theta)$ 进行蒙特卡洛采样来近似该积分，并对每个采样的 $\\theta$，从模型中模拟合成的概括统计量 $S(x)$，然后使用一个以 $S(y)$ 为中心、容差为 $\\epsilon$ 的ABC核进行加权。用 $K_\\epsilon(u)$ 表示一维的ABC核，它可以是高斯核或均匀核。利用对于此高斯模型的基本恒等式，即似然 $p(y \\mid \\theta)$ 仅通过 $S(y)$ 依赖于 $y$，并将 $p(y \\mid \\theta)$ 用在 $\\theta$ 下 $S$ 的密度以及与 $\\theta$ 无关的常数来表示。利用此关系，基于对 $\\theta \\sim p(\\theta)$ 和 $x \\sim p(x \\mid \\theta)$ 的 $K_\\epsilon(S(x) - S(y))$ 的联合蒙特卡洛平均，构建一个伪边际估计量 $\\widehat{p}_{\\text{ABC}}(y;\\epsilon,K)$，并通过适当缩放以 $p(y)$ 为目标。\n\n然后，将偏差 $\\text{bias}(\\epsilon, K) = \\widehat{p}_{\\text{ABC}}(y;\\epsilon,K) - p(y)$ 作为容差 $\\epsilon$ 和核 $K$ 的函数进行量化。您的程序必须：\n\n- 从基本定义和事实出发：给定 $\\theta$ 时 $y_k$ 的分布，给定 $\\theta$ 时 $S(y)$ 的分布，以及在先验 $p(\\theta)$ 下 $p(y)$ 作为对 $\\theta$ 的积分的定义。\n- 通过用概括统计量上的ABC卷积替换难解的似然来构建伪边际估计量，并证明将概括统计量密度与 $p(y \\mid \\theta)$ 联系起来的缩放因子的合理性。\n- 实现两种核：\n  - 高斯核 $K_\\epsilon(u) = \\frac{1}{\\sqrt{2\\pi}\\, \\epsilon} \\exp\\left(-\\frac{u^2}{2 \\epsilon^2}\\right)$。\n  - 均匀核 $K_\\epsilon(u) = \\frac{1}{2\\epsilon}\\, \\mathbb{I}\\{|u| \\le \\epsilon\\}$。\n- 使用带有固定随机种子的蒙特卡洛采样以确保可复现性。\n\n对数据生成和先验使用以下固定设置：\n- 增量数量 $n = 20$。\n- 步长 $\\Delta = 1$。\n- 用于数据生成的真实扩散方差 $\\theta^\\star = 1.5$。\n- 先验超参数 $\\alpha = 3$ 和 $\\beta = 2$。\n- 通过模拟 $y_k \\sim \\mathcal{N}(0, \\theta^\\star \\Delta)$ 生成观测数据 $y$，并计算 $S(y)$。\n\n对于伪边际估计量，使用：\n- 先验样本数量 $N = 800$。\n- 每个先验样本的合成概括统计量复制品数量 $R = 400$。\n\n对于每个采样的 $\\theta$，您可以使用在 $\\theta$ 下已知的概括统计量分布来高效地模拟 $S(x)$。确保估计量对联合 $(\\theta, x)$ 样本进行平均，以近似ABC卷积，并重新缩放以 $p(y)$ 为目标。\n\n设计一个容差-核对的测试套件，以研究不同情景下的偏差：\n- 情况1：$\\epsilon = 0.5$，高斯核。\n- 情况2：$\\epsilon = 2.0$，高斯核。\n- 情况3：$\\epsilon = 8.0$，高斯核。\n- 情况4：$\\epsilon = 0.5$，均匀核。\n- 情况5：$\\epsilon = 2.0$，均匀核。\n- 情况6：$\\epsilon = 8.0$，均匀核。\n- 情况7：$\\epsilon = 12.0$，均匀核。\n\n对于每种情况，将偏差 $\\text{bias}(\\epsilon, K)$ 计算为单个实数。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，\"[result1,result2,...]\"）。\n\n所有答案必须表示为不带任何单位的实数，且输出中不应出现百分号。最终输出行应精确包含与上述测试套件顺序对应的 $7$ 个浮点数。",
            "solution": "核心任务是计算偏差 $\\text{bias} = \\widehat{p}_{\\text{ABC}}(y) - p(y)$。这需要得到真实边际似然 $p(y)$ 及其 ABC 估计量 $\\widehat{p}_{\\text{ABC}}(y)$ 的表达式。\n\n#### 真实边际似然 $p(y)$\n边际似然是似然乘以先验在参数空间上的积分。\n- 对于观测数据 $y=(y_1, ..., y_n)$，给定 $\\theta = \\sigma^2$ 的似然为：\n$$ p(y \\mid \\theta) = \\prod_{k=1}^n \\frac{1}{\\sqrt{2\\pi \\theta \\Delta}} \\exp\\left(-\\frac{y_k^2}{2\\theta\\Delta}\\right) = (2\\pi \\theta \\Delta)^{-n/2} \\exp\\left(-\\frac{S(y)}{2\\theta\\Delta}\\right) $$\n- $\\theta$ 的先验分布是逆伽马 $(\\alpha, \\beta)$：\n$$ p(\\theta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\theta^{-\\alpha-1} \\exp\\left(-\\frac{\\beta}{\\theta}\\right) $$\n- 边际似然 $p(y)$ 是积分：\n$$ p(y) = \\int_0^\\infty p(y \\mid \\theta) p(\\theta) \\,d\\theta $$\n$$ p(y) = \\frac{(2\\pi \\Delta)^{-n/2} \\beta^\\alpha}{\\Gamma(\\alpha)} \\int_0^\\infty \\theta^{-(n/2 + \\alpha) - 1} \\exp\\left(-\\frac{1}{\\theta} \\left(\\beta + \\frac{S(y)}{2\\Delta}\\right)\\right) \\,d\\theta $$\n该积分是一个形状参数为 $\\alpha' = \\alpha + n/2$、尺度参数为 $\\beta' = \\beta + S(y)/(2\\Delta)$ 的逆伽马分布的未归一化概率密度。积分的值是 $\\Gamma(\\alpha')/(\\beta')^{\\alpha'}$。\n- 将此代回，边际似然的解析表达式为：\n$$ p(y) = \\frac{(2\\pi \\Delta)^{-n/2} \\beta^\\alpha}{\\Gamma(\\alpha)} \\frac{\\Gamma(\\alpha + n/2)}{(\\beta + S(y)/(2\\Delta))^{\\alpha + n/2}} $$\n此公式将使用对数尺度变换来实现，以保持数值稳定性。\n\n#### ABC 估计量 $\\widehat{p}_{\\text{ABC}}(y)$\n该问题要求基于概括统计量似然的 ABC 近似，为 $p(y)$ 构建一个伪边际估计量。\n- 首先，我们建立完整数据似然 $p(y \\mid \\theta)$ 和概括统计量似然 $p_S(s \\mid \\theta)$ 之间的关系。给定 $y_k \\sim \\mathcal{N}(0, \\theta\\Delta)$，标准化变量 $z_k = y_k/\\sqrt{\\theta\\Delta} \\sim \\mathcal{N}(0, 1)$。因此，$\\sum z_k^2 = S(y)/(\\theta\\Delta) \\sim \\chi^2_n$。\n- 这意味着概括统计量 $S(y)$ 服从伽马分布：$S(y) \\sim \\text{Gamma}(\\text{shape}=n/2, \\text{scale}=2\\theta\\Delta)$。其密度为：\n$$ p_S(s \\mid \\theta) = \\frac{1}{\\Gamma(n/2) (2\\theta\\Delta)^{n/2}} s^{n/2 - 1} \\exp\\left(-\\frac{s}{2\\theta\\Delta}\\right) $$\n- 通过比较 $p_S(S(y) \\mid \\theta)$ 和 $p(y \\mid \\theta)$，我们得到因式分解：\n$$ p(y \\mid \\theta) = C(S(y)) \\cdot p_S(S(y) \\mid \\theta), \\quad \\text{其中} \\quad C(s) = \\frac{\\Gamma(n/2)}{\\pi^{n/2} s^{n/2-1}} $$\n缩放因子 $C(s)$ 仅依赖于数据概括 $s$，而不依赖于参数 $\\theta$。\n- 边际似然可以表示为：\n$$ p(y) = C(S(y)) \\int p_S(S(y) \\mid \\theta) p(\\theta) \\,d\\theta $$\n- ABC 方法用基于核的近似替换了真实的概括统计量似然 $p_S(S(y) \\mid \\theta)$。该近似在合成数据 $x \\sim p(x \\mid \\theta)$ 上的期望是一个卷积：\n$$ \\mathbb{E}_{x \\sim p(\\cdot \\mid \\theta)}[K_\\epsilon(S(x) - S(y))] = \\int p_S(s' \\mid \\theta) K_\\epsilon(s' - S(y)) \\,ds' = (p_S(\\cdot \\mid \\theta) * K_\\epsilon)(S(y)) $$\n- 结合这些思想，ABC 估计量旨在估计有偏的边际似然：\n$$ p_{\\text{ABC}}(y) = C(S(y)) \\int (p_S(\\cdot \\mid \\theta) * K_\\epsilon)(S(y)) \\, p(\\theta) \\,d\\theta $$\n这可以通过联合蒙特卡洛平均来估计。\n- ABC 估计量构造如下：\n$$ \\widehat{p}_{\\text{ABC}}(y) = C(S(y)) \\cdot \\frac{1}{NR} \\sum_{i=1}^N \\sum_{j=1}^R K_\\epsilon(S(x_{ij}) - S(y)) $$\n其中 $\\theta_i \\sim p(\\theta)$ 且 $x_{ij} \\sim p(x \\mid \\theta_i)$。在实践中，我们直接从其已知的伽马分布中模拟合成的概括统计量 $S(x_{ij})$。\n\n#### 算法\n1.  **初始化**：设置常数 $n, \\Delta, \\theta^\\star, \\alpha, \\beta, N, R$，并使用固定种子初始化一个随机数生成器。\n2.  **生成数据**：模拟观测数据向量 $y$ 并计算其概括统计量 $S_{obs} = S(y)$。\n3.  **计算真实边际似然**：使用上述推导的解析公式计算 $p(y)$。\n4.  **计算缩放因子**：计算 $C(S_{obs})$。\n5.  **蒙特卡洛模拟**：\n    a. 从先验分布 $\\text{Inverse-Gamma}(\\alpha, \\beta)$ 中抽取 $N$ 个样本 $\\theta_i$。\n    b. 对每个 $\\theta_i$，从其对应的概括统计量分布 $\\text{Gamma}(n/2, 2\\theta_i\\Delta)$ 中抽取 $R$ 个样本 $s'_{ij}$。\n6.  **遍历测试用例**：对每个指定的 $(\\epsilon, \\text{Kernel})$ 对：\n    a. 计算所有 $N \\times R$ 个合成样本的核函数值 $K_\\epsilon(s'_{ij} - S_{obs})$。\n    b. 计算这些核函数值的均值 $\\bar{K}$。\n    c. 计算 ABC 估计量：$\\widehat{p}_{\\text{ABC}}(y) = C(S_{obs}) \\cdot \\bar{K}$。\n    d. 计算偏差：$\\text{bias} = \\widehat{p}_{\\text{ABC}}(y) - p(y)$，并存储。\n7.  **输出**：按要求格式打印包含所有偏差值的列表。",
            "answer": "```python\nimport numpy as np\nfrom scipy import special, stats\n\ndef solve():\n    \"\"\"\n    Computes the bias of an ABC estimator for the marginal likelihood of a diffusion model.\n    \"\"\"\n    # --- Problem Parameters ---\n    n = 20          # Number of increments\n    delta = 1.0     # Step size\n    theta_star = 1.5 # True diffusion variance for data generation\n    alpha = 3.0     # Prior shape hyperparameter\n    beta = 2.0      # Prior scale hyperparameter\n    \n    # --- Estimator Parameters ---\n    N = 800         # Number of prior samples\n    R = 400         # Number of synthetic summary replicates per prior sample\n    \n    # --- Test Cases for Bias Quantification ---\n    test_cases = [\n        # (epsilon, kernel_type)\n        (0.5, 'gaussian'),\n        (2.0, 'gaussian'),\n        (8.0, 'gaussian'),\n        (0.5, 'uniform'),\n        (2.0, 'uniform'),\n        (8.0, 'uniform'),\n        (12.0, 'uniform'),\n    ]\n\n    # --- Reproducibility ---\n    seed = 42\n    rng = np.random.default_rng(seed)\n\n    # --- Step 1: Generate Observed Data and Summary Statistic ---\n    # y_k ~ N(0, theta_star * delta)\n    y_obs = rng.normal(loc=0.0, scale=np.sqrt(theta_star * delta), size=n)\n    # S(y) = sum(y_k^2)\n    S_obs = np.sum(y_obs**2)\n\n    # --- Step 2: Calculate the True Marginal Likelihood p(y) ---\n    # Based on the analytical solution for the conjugate Inverse-Gamma-Normal model.\n    # We use log-probabilities for numerical stability.\n    log_p_y = (\n        -(n / 2.0) * np.log(2.0 * np.pi * delta) +\n        alpha * np.log(beta) -\n        special.gammaln(alpha) +\n        special.gammaln(alpha + n / 2.0) -\n        (alpha + n / 2.0) * np.log(beta + S_obs / (2.0 * delta))\n    )\n    p_y = np.exp(log_p_y)\n\n    # --- Step 3: Compute the Scaling Factor C(S_obs) for the ABC Estimator ---\n    # C(s) = Gamma(n/2) / (pi^(n/2) * s^((n/2)-1))\n    log_C_S = (\n        special.gammaln(n / 2.0) -\n        (n / 2.0) * np.log(np.pi) -\n        (n / 2.0 - 1.0) * np.log(S_obs)\n    )\n    C_S = np.exp(log_C_S)\n\n    # --- Step 4: Perform Monte Carlo Simulation for ABC ---\n    # Sample thetas from the prior: theta ~ Inverse-Gamma(alpha, beta)\n    theta_samples = stats.invgamma.rvs(a=alpha, scale=beta, size=N, random_state=rng)\n    \n    # For each theta, sample synthetic summaries S(x) from p(S|theta):\n    # S(x) ~ Gamma(shape=n/2, scale=2*theta*delta)\n    s_prime_samples = np.zeros((N, R))\n    for i in range(N):\n        s_prime_samples[i, :] = rng.gamma(\n            shape=n / 2.0,\n            scale=2.0 * theta_samples[i] * delta,\n            size=R\n        )\n    \n    # Pre-calculate the difference u = s' - S_obs for all samples\n    u_samples = s_prime_samples - S_obs\n\n    # --- Step 5: Calculate Bias for Each Test Case ---\n    results = []\n    \n    def gaussian_kernel(u, epsilon):\n        return (1.0 / (np.sqrt(2 * np.pi) * epsilon)) * np.exp(-u**2 / (2 * epsilon**2))\n\n    def uniform_kernel(u, epsilon):\n        return (1.0 / (2 * epsilon)) * (np.abs(u) = epsilon)\n\n    for epsilon, kernel_type in test_cases:\n        if kernel_type == 'gaussian':\n            kernel_values = gaussian_kernel(u_samples, epsilon)\n        else: # 'uniform'\n            kernel_values = uniform_kernel(u_samples, epsilon)\n            \n        # Compute the average kernel value over all N*R samples\n        avg_K = np.mean(kernel_values)\n        \n        # Compute the ABC estimate of the marginal likelihood\n        p_abc = C_S * avg_K\n        \n        # Compute the bias\n        bias = p_abc - p_y\n        results.append(bias)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}