{
    "hands_on_practices": [
        {
            "introduction": "近似贝叶斯计算（ABC）虽然常被视为一种“无似然”方法，但它与传统的基于似然的推断有着深刻的联系。本练习旨在揭示ABC框架与一种高效的参数化近似——“合成似然”（synthetic likelihood）——之间的关系 。通过推导，您将理解在特定假设下，ABC的核密度近似如何收敛于基于高斯假设的合成似然，从而深化对不同ABC变体背后理论基础的认识。",
            "id": "3288741",
            "problem": "令 $X$ 表示在由参数 $\\theta \\in \\Theta \\subset \\mathbb{R}^{p}$ 索引的参数模型下生成的数据，并令 $s(X) \\in \\mathbb{R}^{d}$ 为一个固定的低维汇总统计量。假设对于每个 $\\theta$，$s(X)\\mid \\theta$ 的分布可以由一个均值为 $\\mu_{\\theta} \\in \\mathbb{R}^{d}$、正定协方差矩阵为 $\\Sigma_{\\theta} \\in \\mathbb{R}^{d \\times d}$ 的多元正态分布很好地近似。您观测到一个单一数据集 $X_{\\mathrm{obs}}$，其观测到的汇总统计量为 $s_{\\mathrm{obs}} = s(X_{\\mathrm{obs}})$。考虑基于汇总统计量的近似贝叶斯计算 (ABC)，其使用平滑核 $K_{\\epsilon}(\\cdot)$、容差 $\\epsilon > 0$ 和先验密度 $\\pi(\\theta)$。基于 ABC 汇总统计量的似然是核卷积\n$$\nL_{\\mathrm{ABC},\\epsilon}(\\theta) \\propto \\int_{\\mathbb{R}^{d}} K_{\\epsilon}\\!\\left(s - s_{\\mathrm{obs}}\\right)\\, p\\!\\left(s \\mid \\theta\\right)\\, ds,\n$$\n其中 $p\\!\\left(s \\mid \\theta\\right)$ 是 $s(X)\\mid \\theta$ 的抽样密度。令 $\\phi(\\cdot;\\mu,\\Sigma)$ 表示均值为 $\\mu$、协方差为 $\\Sigma$ 的 $d$ 元高斯密度。\n\n任务：\n1) 从 $L_{\\mathrm{ABC},\\epsilon}(\\theta)$ 的定义以及高斯核在 $\\epsilon \\to 0$ 时构成一个近似单位的性质出发，证明如果 $s(X)\\mid \\theta$ 近似为均值为 $\\mu_{\\theta}$、协方差为 $\\Sigma_{\\theta}$ 的高斯分布，那么合成似然\n$$\nL_{\\mathrm{SL}}(\\theta) \\equiv \\phi\\!\\left(s_{\\mathrm{obs}};\\mu_{\\theta},\\Sigma_{\\theta}\\right)\n$$\n在极限 $\\epsilon \\to 0$ 时可作为参数化的近似贝叶斯计算 (ABC) 似然。\n2) 现在假设对于一个固定的 $\\theta$，您可以进行独立模拟以从 $s(X)\\mid \\theta$ 的分布中获得 $m \\in \\mathbb{N}$ 个独立重复样本 $s^{(1)},\\dots,s^{(m)}$。仅使用期望和协方差的定义，基于 $\\{s^{(i)}\\}_{i=1}^{m}$ 推导在 $s(X)\\mid \\theta$ 精确高斯性的条件下对 $\\mu_{\\theta}$ 和 $\\Sigma_{\\theta}$ 无偏的蒙特卡洛估计量 $\\widehat{\\mu}_{\\theta}$ 和 $\\widehat{\\Sigma}_{\\theta}$；用 $\\{s^{(i)}\\}_{i=1}^{m}$ 和 $m$ 以闭式形式表示您的答案。您可以视 $d$ 为固定且有限的，并且可以假设通常的正则性条件成立，以确保近似单位的积分与极限可以互换。\n\n您的最终答案必须包含一个单行矩阵，按顺序包含：$L_{\\mathrm{SL}}(\\theta)$ 作为 $s_{\\mathrm{obs}},\\mu_{\\theta},\\Sigma_{\\theta}$ 函数的显式闭式表达式，其后是 $\\widehat{\\mu}_{\\theta}$，再其后是 $\\widehat{\\Sigma}_{\\theta}$。无需进行数值计算，也无需四舍五入。所有符号必须在您的推导中定义。最终答案中不要包含任何单位。",
            "solution": "该问题包含两部分。第一部分要求证明合成似然 $L_{\\mathrm{SL}}(\\theta)$ 可以被看作是近似贝叶斯计算 (ABC) 似然 $L_{\\mathrm{ABC},\\epsilon}(\\theta)$ 的一个极限情况。第二部分要求基于一组模拟重复样本，推导高斯分布的汇总统计量的均值和协方差的无偏估计量。\n\n**第1部分：作为极限ABC似然的合成似然**\n\n问题给出了基于ABC汇总统计量的似然的定义，它是一个核卷积：\n$$\nL_{\\mathrm{ABC},\\epsilon}(\\theta) \\propto \\int_{\\mathbb{R}^{d}} K_{\\epsilon}\\!\\left(s - s_{\\mathrm{obs}}\\right)\\, p\\!\\left(s \\mid \\theta\\right)\\, ds\n$$\n在这里，$p(s \\mid \\theta)$ 是在给定参数 $\\theta$ 时汇总统计量 $s(X)$ 的抽样密度，$s_{\\mathrm{obs}}$ 是观测到的汇总统计量，$K_{\\epsilon}$ 是一个容差为 $\\epsilon > 0$ 的平滑核。\n\n问题指出我们应该使用一个高斯核，它在 $\\epsilon \\to 0$ 时构成一个近似单位。近似单位是一个函数序列 $\\{K_{\\epsilon}\\}_{\\epsilon > 0}$，在极限 $\\epsilon \\to 0$ 时收敛于狄拉克δ函数 $\\delta$。对于任何行为良好的函数 $f$，与近似单位的卷积具有以下性质：\n$$\n\\lim_{\\epsilon \\to 0} \\int_{\\mathbb{R}^{d}} K_{\\epsilon}(x-y) f(y) dy = f(x)\n$$\n在我们的例子中，函数是汇总统计量的似然，$f(s) = p(s \\mid \\theta)$，我们正在 $s_{\\mathrm{obs}}$ 处计算该卷积。假设核是对称的，即 $K_{\\epsilon}(u) = K_{\\epsilon}(-u)$，则该积分可以写为：\n$$\n\\int_{\\mathbb{R}^{d}} K_{\\epsilon}\\!\\left(s - s_{\\mathrm{obs}}\\right)\\, p\\!\\left(s \\mid \\theta\\right)\\, ds = \\int_{\\mathbb{R}^{d}} K_{\\epsilon}\\!\\left(s_{\\mathrm{obs}} - s\\right)\\, p\\!\\left(s \\mid \\theta\\right)\\, ds\n$$\n这是卷积 $(K_{\\epsilon} * p(\\cdot \\mid \\theta))(s_{\\mathrm{obs}})$。当 $\\epsilon \\to 0$ 时，近似单位的性质意味着该卷积收敛到在 $s_{\\mathrm{obs}}$ 处求值的函数 $p(\\cdot \\mid \\theta)$。\n$$\n\\lim_{\\epsilon \\to 0} L_{\\mathrm{ABC},\\epsilon}(\\theta) \\propto \\lim_{\\epsilon \\to 0} \\int_{\\mathbb{R}^{d}} K_{\\epsilon}\\!\\left(s - s_{\\mathrm{obs}}\\right)\\, p\\!\\left(s \\mid \\theta\\right)\\, ds = p(s_{\\mathrm{obs}} \\mid \\theta)\n$$\n这表明在容差为零的极限下，ABC似然与汇总统计量的真实似然成正比。\n\n问题接着引入了假设，即 $s(X) \\mid \\theta$ 的分布可以由一个均值为 $\\mu_{\\theta}$、协方差为 $\\Sigma_{\\theta}$ 的 $d$ 元正态分布很好地近似。这意味着我们可以用高斯概率密度函数 $\\phi(s; \\mu_{\\theta}, \\Sigma_{\\theta})$ 来近似抽样密度 $p(s \\mid \\theta)$：\n$$\np(s \\mid \\theta) \\approx \\phi(s; \\mu_{\\theta}, \\Sigma_{\\theta})\n$$\n将此近似代入我们的极限结果，得到：\n$$\n\\lim_{\\epsilon \\to 0} L_{\\mathrm{ABC},\\epsilon}(\\theta) \\propto \\phi(s_{\\mathrm{obs}}; \\mu_{\\theta}, \\Sigma_{\\theta})\n$$\n右侧的表达式正是合成似然 $L_{\\mathrm{SL}}(\\theta)$ 的定义。\n$$\nL_{\\mathrm{SL}}(\\theta) \\equiv \\phi(s_{\\mathrm{obs}}; \\mu_{\\theta}, \\Sigma_{\\theta})\n$$\n因此，在 $\\epsilon \\to 0$ 的极限下，合成似然可作为ABC似然的参数化近似。\n\n$d$ 元高斯密度 $\\phi(x; \\mu, \\Sigma)$ 的显式闭式表达式为：\n$$\n\\phi(x; \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{d/2} \\det(\\Sigma)^{1/2}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)\\right)\n$$\n因此，合成似然为：\n$$\nL_{\\mathrm{SL}}(\\theta) = \\frac{1}{(2\\pi)^{d/2} \\det(\\Sigma_{\\theta})^{1/2}} \\exp\\left(-\\frac{1}{2}(s_{\\mathrm{obs}}-\\mu_{\\theta})^T \\Sigma_{\\theta}^{-1} (s_{\\mathrm{obs}}-\\mu_{\\theta})\\right)\n$$\n\n**第2部分：无偏估计量的推导**\n\n我们被给予从 $s(X)\\mid \\theta$ 分布中抽取的 $m$ 个独立同分布 (i.i.d.) 的重复样本 $s^{(1)}, \\dots, s^{(m)}$。在这一部分，我们假设该分布是精确高斯的，$s^{(i)} \\sim \\mathcal{N}(\\mu_{\\theta}, \\Sigma_{\\theta})$。我们需要为 $\\mu_{\\theta}$ 和 $\\Sigma_{\\theta}$ 推导无偏估计量。\n\n**均值 $\\mu_{\\theta}$ 的无偏估计量**\n总体均值的标准估计量是样本均值。我们定义估计量 $\\widehat{\\mu}_{\\theta}$ 为：\n$$\n\\widehat{\\mu}_{\\theta} = \\frac{1}{m} \\sum_{i=1}^{m} s^{(i)}\n$$\n为了证明它是无偏的，我们必须证明它的期望值是 $\\mu_{\\theta}$。利用期望的线性性：\n$$\nE[\\widehat{\\mu}_{\\theta}] = E\\left[\\frac{1}{m} \\sum_{i=1}^{m} s^{(i)}\\right] = \\frac{1}{m} \\sum_{i=1}^{m} E[s^{(i)}]\n$$\n由于每个 $s^{(i)}$ 都从均值为 $\\mu_{\\theta}$ 的分布中抽取，我们有对于所有 $i \\in \\{1, \\dots, m\\}$，$E[s^{(i)}] = \\mu_{\\theta}$。\n$$\nE[\\widehat{\\mu}_{\\theta}] = \\frac{1}{m} \\sum_{i=1}^{m} \\mu_{\\theta} = \\frac{1}{m} (m \\mu_{\\theta}) = \\mu_{\\theta}\n$$\n这证实了 $\\widehat{\\mu}_{\\theta}$ 是 $\\mu_{\\theta}$ 的一个无偏估计量。\n\n**协方差 $\\Sigma_{\\theta}$ 的无偏估计量**\n总体协方差的标准无偏估计量是样本协方差矩阵，它包含了贝塞尔校正。我们定义估计量 $\\widehat{\\Sigma}_{\\theta}$ 为：\n$$\n\\widehat{\\Sigma}_{\\theta} = \\frac{1}{m-1} \\sum_{i=1}^{m} (s^{(i)} - \\widehat{\\mu}_{\\theta})(s^{(i)} - \\widehat{\\mu}_{\\theta})^T\n$$\n为了证明其无偏性，我们计算它的期望。让我们首先分析求和项 $\\sum_{i=1}^{m} (s^{(i)} - \\widehat{\\mu}_{\\theta})(s^{(i)} - \\widehat{\\mu}_{\\theta})^T$。我们可以通过加上和减去真实均值 $\\mu_{\\theta}$ 来重写求和内的项：\n$$\ns^{(i)} - \\widehat{\\mu}_{\\theta} = (s^{(i)} - \\mu_{\\theta}) - (\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})\n$$\n求和项变为：\n\\begin{align*}\n\\sum_{i=1}^{m} (s^{(i)} - \\widehat{\\mu}_{\\theta})(s^{(i)} - \\widehat{\\mu}_{\\theta})^T = \\sum_{i=1}^{m} \\left[(s^{(i)} - \\mu_{\\theta}) - (\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})\\right]\\left[(s^{(i)} - \\mu_{\\theta}) - (\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})\\right]^T \\\\\n= \\sum_{i=1}^{m} (s^{(i)} - \\mu_{\\theta})(s^{(i)} - \\mu_{\\theta})^T - \\sum_{i=1}^m (s^{(i)} - \\mu_{\\theta})(\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})^T \\\\\n \\quad - \\sum_{i=1}^m (\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})(s^{(i)} - \\mu_{\\theta})^T + \\sum_{i=1}^m (\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})(\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})^T\n\\end{align*}\n我们注意到 $\\sum_{i=1}^{m} (s^{(i)} - \\mu_{\\theta}) = m(\\frac{1}{m}\\sum s^{(i)}) - m\\mu_{\\theta} = m(\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})$。利用这一点，表达式简化为：\n$$\n\\sum_{i=1}^{m} (s^{(i)} - \\mu_{\\theta})(s^{(i)} - \\mu_{\\theta})^T - m(\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})(\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})^T\n$$\n现在，我们取该表达式的期望：\n$$\nE\\left[ \\sum_{i=1}^{m} (s^{(i)} - \\widehat{\\mu}_{\\theta})(s^{(i)} - \\widehat{\\mu}_{\\theta})^T \\right] = E\\left[\\sum_{i=1}^{m} (s^{(i)} - \\mu_{\\theta})(s^{(i)} - \\mu_{\\theta})^T\\right] - m E\\left[(\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})(\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})^T\\right]\n$$\n右边的第一项是：\n$$\nE\\left[\\sum_{i=1}^{m} (s^{(i)} - \\mu_{\\theta})(s^{(i)} - \\mu_{\\theta})^T\\right] = \\sum_{i=1}^{m} E\\left[(s^{(i)} - \\mu_{\\theta})(s^{(i)} - \\mu_{\\theta})^T\\right] = \\sum_{i=1}^{m} \\text{Cov}(s^{(i)}) = \\sum_{i=1}^{m} \\Sigma_{\\theta} = m\\Sigma_{\\theta}\n$$\n第二项涉及样本均值的协方差矩阵，$\\text{Cov}(\\widehat{\\mu}_{\\theta}) = E\\left[(\\widehat{\\mu}_{\\theta} - E[\\widehat{\\mu}_{\\theta}])(\\widehat{\\mu}_{\\theta} - E[\\widehat{\\mu}_{\\theta}])^T\\right] = E\\left[(\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})(\\widehat{\\mu}_{\\theta} - \\mu_{\\theta})^T\\right]$。\n$$\n\\text{Cov}(\\widehat{\\mu}_{\\theta}) = \\text{Cov}\\left(\\frac{1}{m}\\sum_{i=1}^{m} s^{(i)}\\right) = \\frac{1}{m^2} \\text{Cov}\\left(\\sum_{i=1}^{m} s^{(i)}\\right)\n$$\n由于样本 $s^{(i)}$ 是独立的，和的协方差是协方差的和：\n$$\n\\text{Cov}(\\widehat{\\mu}_{\\theta}) = \\frac{1}{m^2} \\sum_{i=1}^{m} \\text{Cov}(s^{(i)}) = \\frac{1}{m^2} \\sum_{i=1}^{m} \\Sigma_{\\theta} = \\frac{1}{m^2}(m\\Sigma_{\\theta}) = \\frac{1}{m}\\Sigma_{\\theta}\n$$\n将这些结果代回，我们得到：\n$$\nE\\left[ \\sum_{i=1}^{m} (s^{(i)} - \\widehat{\\mu}_{\\theta})(s^{(i)} - \\widehat{\\mu}_{\\theta})^T \\right] = m\\Sigma_{\\theta} - m\\left(\\frac{1}{m}\\Sigma_{\\theta}\\right) = (m-1)\\Sigma_{\\theta}\n$$\n因此，我们提出的估计量 $\\widehat{\\Sigma}_{\\theta}$ 的期望是：\n$$\nE[\\widehat{\\Sigma}_{\\theta}] = E\\left[ \\frac{1}{m-1} \\sum_{i=1}^{m} (s^{(i)} - \\widehat{\\mu}_{\\theta})(s^{(i)} - \\widehat{\\mu}_{\\theta})^T \\right] = \\frac{1}{m-1} E\\left[ \\sum_{i=1}^{m} (s^{(i)} - \\widehat{\\mu}_{\\theta})(s^{(i)} - \\widehat{\\mu}_{\\theta})^T \\right] = \\frac{1}{m-1} (m-1)\\Sigma_{\\theta} = \\Sigma_{\\theta}\n$$\n这证实了 $\\widehat{\\Sigma}_{\\theta}$ 是 $\\Sigma_{\\theta}$ 的一个无偏估计量。\n\n最终答案由 $L_{\\mathrm{SL}}(\\theta)$ 的闭式表达式、估计量 $\\widehat{\\mu}_{\\theta}$ 和估计量 $\\widehat{\\Sigma}_{\\theta}$ 组成。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{(2\\pi)^{d/2} \\det(\\Sigma_{\\theta})^{1/2}} \\exp\\left(-\\frac{1}{2}(s_{\\mathrm{obs}}-\\mu_{\\theta})^T \\Sigma_{\\theta}^{-1} (s_{\\mathrm{obs}}-\\mu_{\\theta})\\right) & \\frac{1}{m}\\sum_{i=1}^{m} s^{(i)} & \\frac{1}{m-1}\\sum_{i=1}^{m} (s^{(i)} - \\widehat{\\mu}_{\\theta})(s^{(i)} - \\widehat{\\mu}_{\\theta})^T \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "理论的价值最终体现在实践中。本练习将引导您动手编写一个ABC校准测试，从而将理论知识应用于实际代码中 。您将为一个泊松-伽马共轭模型实现一个ABC拒绝采样器，并将其生成的后验预测分布与精确的解析解进行比较。通过使用Cramer-von Mises统计量来量化近似误差，您将掌握评估和验证ABC方法准确性的关键技能，这是在科学研究中负责任地使用近似计算的基石。",
            "id": "3288776",
            "problem": "你需要为近似贝叶斯计算（Approximate Bayesian Computation, ABC）的后验预测检验构建一个校准测试。目标是比较从ABC后验预测过程中抽取的重复数据的经验分布与从精确后验预测过程中抽取的重复数据的经验分布，并使用Cramer–von Mises统计量来量化偏差。\n\n本任务的基础包括以下经过充分检验的公式和定义：\n\n- 贝叶斯法则以及泊松模型的共轭先验-后验关系：设数据为 $y_1, y_2, \\dots, y_n$，其中 $y_i \\sim \\mathrm{Poisson}(\\theta)$，先验分布为形状-速率参数化的 $\\theta \\sim \\mathrm{Gamma}(\\alpha, \\beta)$。后验分布为 $\\theta \\mid y \\sim \\mathrm{Gamma}(\\alpha', \\beta')$，其中 $\\alpha' = \\alpha + \\sum_{i=1}^n y_i$ 且 $\\beta' = \\beta + n$。\n- 单个未来观测值 $y^{\\mathrm{rep}}$ 的后验预测分布是泊松-伽马混合分布：等价地，从 $\\mathrm{Gamma}(\\alpha', \\beta')$ 中抽取样本 $\\lambda$，然后令 $y^{\\mathrm{rep}} \\sim \\mathrm{Poisson}(\\lambda)$ 以获得精确的后验预测样本。\n- 使用拒绝法的近似贝叶斯计算（ABC）：提议 $\\theta^{(j)} \\sim \\mathrm{Gamma}(\\alpha, \\beta)$，从模型中模拟一个数据集 $y^{(j)} = (y_1^{(j)}, \\dots, y_n^{(j)})$，计算一个摘要统计量 $s(y^{(j)})$，如果差异 $d(s(y^{(j)}), s(y))$ 小于阈值 $\\varepsilon$，则接受 $\\theta^{(j)}$。使用接受的 $\\theta$ 抽样，模拟 $y^{\\mathrm{rep}} \\sim \\mathrm{Poisson}(\\theta)$ 来近似后验预测分布。\n- Cramer–von Mises (CvM) 双样本统计量：给定两个样本 $x_1, \\dots, x_m$ 和 $z_1, \\dots, z_n$，令 $N = m + n$，并令 $\\{w_{(i)}\\}_{i=1}^N$ 为 $\\{x_j\\}$ 和 $\\{z_k\\}$ 的混合排序值。定义经验累积分布函数 $F_m$ 和 $G_n$。双样本Cramer–von Mises统计量为\n$$\nT_{\\mathrm{CvM}} = \\frac{mn}{N^2} \\sum_{i=1}^N \\left( F_m(w_{(i)}) - G_n(w_{(i)}) \\right)^2,\n$$\n其中 $F_m(w)$ 是 $\\{x_j\\}$ 中小于或等于 $w$ 的比例，而 $G_n(w)$ 是 $\\{z_k\\}$ 中小于或等于 $w$ 的比例。\n\n待实现的模型和算法规范：\n\n1. 数据生成过程：对于每个测试用例，使用固定的随机种子 $123$ 生成观测数据 $y_1, \\dots, y_n$，作为独立的 $\\mathrm{Poisson}(\\theta_{\\mathrm{true}})$ 计数，以确保可复现性。\n2. 先验：使用形状-速率参数 $(\\alpha, \\beta)$ 的先验分布 $\\theta \\sim \\mathrm{Gamma}(\\alpha, \\beta)$。\n3. 通过拒绝法得到ABC后验：\n   - 摘要统计量：$s(y) = \\frac{1}{n} \\sum_{i=1}^n y_i$。\n   - 差异：$d(s_{\\mathrm{sim}}, s_{\\mathrm{obs}}) = \\left| s_{\\mathrm{sim}} - s_{\\mathrm{obs}} \\right|$。\n   - 阈值：如果 $d(s_{\\mathrm{sim}}, s_{\\mathrm{obs}}) \\le \\varepsilon$ 则接受。\n   - 提议和模拟：抽取 $N_{\\mathrm{prior}}$ 个独立的提议 $\\theta^{(j)} \\sim \\mathrm{Gamma}(\\alpha, \\beta)$；对每个提议，模拟 $n$ 个独立的 $\\mathrm{Poisson}(\\theta^{(j)})$ 观测值并计算 $s_{\\mathrm{sim}}$。\n   - 接受的参数集：$\\{\\theta^{(j)} : d \\le \\varepsilon\\}$。\n4. ABC后验预测样本：对于每个接受的 $\\theta^{(j)}$，抽取一个 $y^{\\mathrm{rep}}_j \\sim \\mathrm{Poisson}(\\theta^{(j)})$；这将产生 $m$ 个ABC后验预测重复样本，其中 $m$ 是接受的提议数量。\n5. 精确后验预测样本：根据观测数据计算 $(\\alpha', \\beta')$，抽取 $m$ 个独立的 $\\lambda_j \\sim \\mathrm{Gamma}(\\alpha', \\beta')$，然后抽取 $z_j \\sim \\mathrm{Poisson}(\\lambda_j)$ 以获得m个重复的精确后验预测样本。使用与接受的ABC重复样本数量相同的精确重复样本数量 $m$，以便于双样本比较。\n6. 对于每个测试用例，计算ABC预测样本和精确预测样本之间的双样本Cramer–von Mises统计量 $T_{\\mathrm{CvM}}$。\n\n测试套件：\n\n使用以下三个测试用例来评估不同情况下的校准情况。对于所有测试用例，均使用固定的随机种子 $123$。\n\n- 情况1（理想路径）：$(n, \\alpha, \\beta, \\theta_{\\mathrm{true}}, \\varepsilon, N_{\\mathrm{prior}}) = (50, 2.0, 1.0, 3.0, 0.12, 20000)$。\n- 情况2（数据更分散）：$(n, \\alpha, \\beta, \\theta_{\\mathrm{true}}, \\varepsilon, N_{\\mathrm{prior}}) = (30, 2.0, 1.0, 1.5, 0.20, 20000)$。\n- 情况3（样本量更大，容忍度更紧）：$(n, \\alpha, \\beta, \\theta_{\\mathrm{true}}, \\varepsilon, N_{\\mathrm{prior}}) = (100, 2.0, 1.0, 4.0, 0.10, 25000)$。\n\n答案规范和要求的输出格式：\n\n- 对于每个测试用例，计算一个实数，即比较ABC后验预测样本与精确后验预测样本的Cramer–von Mises双样本统计量 $T_{\\mathrm{CvM}}$。\n- 你的程序应生成单行输出，包含一个用方括号括起来的逗号分隔列表（例如，$[t_1,t_2,t_3]$）。不应打印任何其他文本。\n\n所有量均为计数或无量纲；不涉及物理单位。不使用角度。所有数值输出均表示为十进制浮点数。\n\n实现约束：\n\n- 对所有随机数生成使用等于 $123$ 的固定随机种子，以确保可复现性。\n- 程序必须是自包含的，无需输入，并在指定环境下运行至完成。",
            "solution": "该问题要求构建并执行一个针对近似贝叶斯计算（ABC）程序的校准测试。任务的核心是量化由ABC近似的后验预测分布与相应的精确后验预测分布之间的差异。这通过计算Cramer-von Mises（CvM）双样本统计量来实现。模型是具有共轭伽马先验的泊松似然，这允许对精确后验进行解析推导，并因此推导出精确后验预测分布，为比较提供了一个黄金标准。\n\n首先，让我们正式定义所涉及的统计模型和分布。\n观测数据是一组$n$个独立同分布的计数 $y = \\{y_1, y_2, \\dots, y_n\\}$，其中每个观测值都从一个未知速率参数 $\\theta$ 的泊松分布中抽取：\n$$y_i \\mid \\theta \\sim \\mathrm{Poisson}(\\theta)$$\n参数 $\\theta$ 本身被假定服从作为先验的伽马分布。伽马先验在其形状-速率参数化下由下式给出：\n$$\\theta \\sim \\mathrm{Gamma}(\\alpha, \\beta)$$\n其中 $\\alpha$ 是形状参数，$\\beta$ 是速率参数。其概率密度函数为 $f(\\theta; \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\theta^{\\alpha-1} e^{-\\beta\\theta}$。\n\n由于伽马先验对于泊松似然是共轭的，给定数据 $y$ 后 $\\theta$ 的后验分布也是一个伽馬分布。后验参数（记为 $\\alpha'$ 和 $\\beta'$）更新如下：\n$$\\alpha' = \\alpha + \\sum_{i=1}^n y_i$$\n$$\\beta' = \\beta + n$$\n因此，精确的后验分布是 $\\theta \\mid y \\sim \\mathrm{Gamma}(\\alpha', \\beta')$。\n\n后验预测分布是在参数 $\\theta$ 的不确定性（由其后验分布描述）上积分后得到的一个新的、未观测到的数据点 $y^{\\mathrm{rep}}$ 的分布。精确后验预测分布的正式定义为：\n$$p(y^{\\mathrm{rep}} \\mid y) = \\int p(y^{\\mathrm{rep}} \\mid \\theta) p(\\theta \\mid y) \\, d\\theta$$\n在本例中，这对应于一个泊松-伽马混合模型，也称为负二项分布。可以通过一个两步分层过程从此分布中生成样本：\n1. 从后验分布中抽取一个参数值 $\\lambda$：$\\lambda \\sim \\mathrm{Gamma}(\\alpha', \\beta')$。\n2. 使用该参数从泊松分布中抽取一个重复数据点 $y^{\\mathrm{rep}}$：$y^{\\mathrm{rep}} \\sim \\mathrm{Poisson}(\\lambda)$。\n重复此过程即可得到一个来自精确后验预测分布的样本。\n\n然后，问题指定了一个ABC程序来近似后验分布。这是一种基于模拟的方法，它绕过了似然函数的显式计算。ABC拒绝算法的步骤如下：\n1. 从先验分布中提议一个参数 $\\theta^{(j)}$：$\\theta^{(j)} \\sim \\mathrm{Gamma}(\\alpha, \\beta)$。\n2. 使用提议的参数从似然中模拟一个数据集 $y^{(j)} = \\{y_1^{(j)}, \\dots, y_n^{(j)}\\}$：$y_i^{(j)} \\sim \\mathrm{Poisson}(\\theta^{(j)})$。\n3. 为观测数据 $s_{\\mathrm{obs}} = s(y)$ 和模拟数据 $s_{\\mathrm{sim}} = s(y^{(j)})$ 计算摘要统计量。指定的摘要统计量是样本均值 $s(y) = \\frac{1}{n} \\sum_{i=1}^n y_i$。\n4. 计算摘要统计量之间的差异度量 $d(s_{\\mathrm{sim}}, s_{\\mathrm{obs}}) = |s_{\\mathrm{sim}} - s_{\\mathrm{obs}}|$。\n5. 如果差异 $d$ 小于或等于容忍度阈值 $\\varepsilon$（即 $d \\le \\varepsilon$），则接受提议的参数 $\\theta^{(j)}$ 作为后验分布的一个近似抽样。否则，拒绝它。\n对大量提议（$N_{\\mathrm{prior}}$个）重复此过程，以形成一个接受的参数集 $\\{\\theta_k^{\\mathrm{acc}}\\}_{k=1}^m$，其中 $m$ 是接受的样本总数。该集合构成了后验分布 $\\theta \\mid y$ 的一个经验近似。\n\n然后，通过对每个接受的参数 $\\theta_k^{\\mathrm{acc}}$ 抽取一个新的数据点 $y_k^{\\mathrm{rep}} \\sim \\mathrm{Poisson}(\\theta_k^{\\mathrm{acc}})$ 来生成ABC后验预测样本。这将产生样本 $\\{y_k^{\\mathrm{rep}}\\}_{k=1}^m$。\n\n最后一步是比较ABC后验预测样本和精确后验预测样本。设大小为 $m$ 的ABC预测样本为 $X = \\{x_1, \\dots, x_m\\}$，大小同样为 $m$ 的精确预测样本为 $Z = \\{z_1, \\dots, z_m\\}$。问题要求使用双样本Cramer-von Mises (CvM) 统计量来量化它们经验分布之间的差异。\n设 $F_m$ 和 $G_m$ 分别是样本 $X$ 和 $Z$ 的经验累积分布函数（ECDF）。设 $\\{w_{(i)}\\}_{i=1}^N$ 是合并样本的混合排序值，其中 $N = 2m$。CvM统计量由以下公式给出：\n$$\nT_{\\mathrm{CvM}} = \\frac{m \\cdot m}{(m+m)^2} \\sum_{i=1}^{N} \\left( F_m(w_{(i)}) - G_m(w_{(i)}) \\right)^2 = \\frac{1}{4} \\sum_{i=1}^{2m} \\left( F_m(w_{(i)}) - G_m(w_{(i)}) \\right)^2\n$$\n此处，$F_m(w)$ 是 $X$ 中小于或等于 $w$ 的元素比例，$G_m(w)$ 是 $Z$ 中小于或等于 $w$ 的元素比例。$T_{\\mathrm{CvM}}$ 的值越小，表示ABC近似与精确分布之间的匹配度越高。\n\n对于每个测试用例，计算过程如下，使用固定的随机种子 $123$ 以实现完全的可复现性：\n1. 为特定情况设置参数 $(n, \\alpha, \\beta, \\theta_{\\mathrm{true}}, \\varepsilon, N_{\\mathrm{prior}})$。\n2. 通过从 $\\mathrm{Poisson}(\\theta_{\\mathrm{true}})$ 中抽样，生成大小为 $n$ 的观测数据集 $y_{\\mathrm{obs}}$。\n3. 对 $N_{\\mathrm{prior}}$ 个提议执行ABC拒绝算法，以获得一组包含 $m$ 个已接受参数的集合 $\\{\\theta_k^{\\mathrm{acc}}\\}$。\n4. 为每个接受的参数抽取一个 $\\mathrm{Poisson}(\\theta_k^{\\mathrm{acc}})$ 样本，生成大小为 $m$ 的ABC后验预测样本。\n5. 计算精确后验分布的参数 $\\alpha' = \\alpha + \\sum y_i$ 和 $\\beta' = \\beta + n$。\n6. 通过对 $k=1, \\dots, m$ 抽取 $\\lambda_k \\sim \\mathrm{Gamma}(\\alpha', \\beta')$，然后抽取 $z_k \\sim \\mathrm{Poisson}(\\lambda_k)$，生成大小为 $m$ 的精确后验预测样本。\n7. 使用指定的公式计算两个预测样本之间的 $T_{\\mathrm{CvM}}$ 统计量。\n对提供的三个测试用例重复这整个过程。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Runs the ABC calibration test for the three specified cases and prints the results.\n    \"\"\"\n    # Use a fixed random seed for reproducibility, as required.\n    seed = 123\n    rng = np.random.default_rng(seed)\n\n    # Test suite parameters:\n    # (n, alpha, beta, theta_true, epsilon, N_prior)\n    test_cases = [\n        (50, 2.0, 1.0, 3.0, 0.12, 20000),\n        (30, 2.0, 1.0, 1.5, 0.20, 20000),\n        (100, 2.0, 1.0, 4.0, 0.10, 25000),\n    ]\n\n    results = []\n    for case_params in test_cases:\n        # The rng object is passed to ensure the random number stream is\n        # consumed sequentially across test cases, maintaining reproducibility.\n        cvm_statistic = run_simulation_case(case_params, rng)\n        results.append(cvm_statistic)\n\n    # Print the final results in the specified format: [t_1,t_2,t_3]\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation_case(case_params, rng):\n    \"\"\"\n    Executes the full simulation and comparison for a single test case.\n\n    Args:\n        case_params (tuple): A tuple containing the parameters\n                             (n, alpha, beta, theta_true, epsilon, N_prior).\n        rng (numpy.random.Generator): The random number generator instance.\n\n    Returns:\n        float: The calculated Cramer-von Mises statistic.\n    \"\"\"\n    n, alpha, beta, theta_true, epsilon, N_prior = case_params\n\n    # Step 1: Generate observed data\n    # Generate n i.i.d. Poisson samples for the \"observed\" data.\n    y_obs = rng.poisson(theta_true, size=n)\n    s_obs = np.mean(y_obs)\n\n    # Step 2: ABC posterior via rejection\n    # Propose N_prior parameters from the prior distribution.\n    theta_proposals = rng.gamma(shape=alpha, scale=1.0/beta, size=N_prior)\n\n    # Vectorized simulation for efficiency:\n    # For each proposed theta, simulate a dataset of size n.\n    # The shape of y_sim will be (N_prior, n).\n    y_sim = rng.poisson(theta_proposals[:, np.newaxis], size=(N_prior, n))\n    \n    # Compute the summary statistic (mean) for each simulated dataset.\n    s_sim = np.mean(y_sim, axis=1)\n\n    # Apply the ABC acceptance criterion.\n    discrepancy = np.abs(s_sim - s_obs)\n    accepted_mask = discrepancy = epsilon\n    accepted_thetas = theta_proposals[accepted_mask]\n\n    m_acc = len(accepted_thetas)\n    # If no samples are accepted, the discrepancy cannot be measured.\n    # According to the problem's well-posed nature, this is not expected.\n    # We return 0.0, indicating no deviation between empty sets.\n    if m_acc == 0:\n        return 0.0\n\n    # Step 3: ABC posterior predictive sample\n    # For each accepted theta, draw one posterior predictive replicate.\n    abc_pred_sample = rng.poisson(accepted_thetas)\n\n    # Step 4: Exact posterior predictive sample\n    # Compute the parameters of the exact Gamma posterior.\n    alpha_prime = alpha + np.sum(y_obs)\n    beta_prime = beta + n\n\n    # Draw m_acc parameters from the exact posterior.\n    exact_posterior_thetas = rng.gamma(shape=alpha_prime, scale=1.0/beta_prime, size=m_acc)\n    # For each, draw one exact posterior predictive replicate.\n    exact_pred_sample = rng.poisson(exact_posterior_thetas)\n\n    # Step 5: Compute the Cramer-von Mises statistic\n    sample_1 = abc_pred_sample\n    sample_2 = exact_pred_sample\n    n_1 = len(sample_1)\n    n_2 = len(sample_2)\n    N = n_1 + n_2\n\n    # Pool and sort the observations from both samples.\n    w_pooled_sorted = np.sort(np.concatenate((sample_1, sample_2)))\n\n    # Sort each sample individually to compute ECDFs efficiently.\n    sample_1_sorted = np.sort(sample_1)\n    sample_2_sorted = np.sort(sample_2)\n\n    # Compute the ECDF values for each sample at each point in the pooled data.\n    # np.searchsorted gives the number of elements = value, which is what we need for the ECDF.\n    ecdf_1 = np.searchsorted(sample_1_sorted, w_pooled_sorted, side='right') / n_1\n    ecdf_2 = np.searchsorted(sample_2_sorted, w_pooled_sorted, side='right') / n_2\n\n    # Calculate the sum of squared differences of the ECDFs.\n    sum_sq_diff = np.sum((ecdf_1 - ecdf_2)**2)\n\n    # Apply the full formula for the CvM statistic.\n    T_cvm = (n_1 * n_2 / N**2) * sum_sq_diff\n    \n    return T_cvm\n\nif __name__ == '__main__':\n    solve()\n```"
        },
        {
            "introduction": "基础的ABC算法在处理复杂模型时可能面临巨大的计算挑战。为了解决这一问题，研究者们开发了更高级的算法，本练习将带您探索其中一种强大的效率提升技术——多层蒙特卡洛（MLMC）方法 。您将通过构建一个多层ABC估计器，并推导在固定计算成本下最小化估计方差的最优样本分配策略，来深入理解如何通过结合不同精度水平的模拟来优化计算资源。这项练习将使您掌握设计和分析高级ABC算法的核心思想，从而能够更有效地解决现实世界中的复杂推断问题。",
            "id": "3288798",
            "problem": "考虑一个贝叶斯模型，其先验密度为 $\\,\\pi(\\theta)\\,$，似然为 $\\,p(x\\mid\\theta)\\,$，观测数据为 $\\,y\\,$。在近似贝叶斯计算 (ABC) 中，在容差 $\\,\\epsilon0\\,$ 下的 ABC 后验由以下密度定义\n$$\n\\pi_{\\epsilon}(\\theta\\mid y)\\;\\propto\\;\\pi(\\theta)\\int K\\!\\left(\\frac{d\\!\\left(s(x),s(y)\\right)}{\\epsilon}\\right)p(x\\mid\\theta)\\,dx,\n$$\n其中 $\\,s(\\cdot)\\,$ 是一个固定的总结统计量，$\\,d(\\cdot,\\cdot)\\,$ 是一个固定的非负差异度量，$\\,K:\\mathbb{R}_{+}\\to\\mathbb{R}_{+}\\,$ 是一个固定的核函数，该函数非增且其支撑集为有界区间。对于一个有界 Borel 可测的检验函数 $\\,\\varphi:\\Theta\\to\\mathbb{R}\\,$，考虑多层级容差 $\\,\\epsilon_{\\ell}=2^{-\\ell}\\epsilon_{0}\\,$，其中 $\\,\\ell\\in\\{0,1,\\dots,L\\}\\,$。定义多层级伸缩恒等式\n$$\n\\mathbb{E}_{\\pi_{\\epsilon_{L}}(\\cdot\\mid y)}[\\varphi(\\theta)]\n\\;=\\;\\mathbb{E}_{\\pi_{\\epsilon_{0}}(\\cdot\\mid y)}[\\varphi(\\theta)]\n\\;+\\;\\sum_{\\ell=1}^{L}\\left(\\mathbb{E}_{\\pi_{\\epsilon_{\\ell}}(\\cdot\\mid y)}[\\varphi(\\theta)]-\\mathbb{E}_{\\pi_{\\epsilon_{\\ell-1}}(\\cdot\\mid y)}[\\varphi(\\theta)]\\right).\n$$\n假设对于每个层级 $\\,\\ell\\in\\{1,\\dots,L\\}\\,$，你可以模拟耦合对 $\\,(\\theta^{(\\ell,\\mathrm{f})},\\theta^{(\\ell,\\mathrm{c})})\\,$，使得其边际分布为 $\\,\\theta^{(\\ell,\\mathrm{f})}\\sim\\pi_{\\epsilon_{\\ell}}(\\cdot\\mid y)\\,$ 和 $\\,\\theta^{(\\ell,\\mathrm{c})}\\sim\\pi_{\\epsilon_{\\ell-1}}(\\cdot\\mid y)\\,$，并且你可以在层级 $\\,\\ell=0\\,$ 模拟 $\\,\\theta^{(0)}\\sim\\pi_{\\epsilon_{0}}(\\cdot\\mid y)\\,$。考虑多层级估计量\n$$\n\\widehat{\\mu}_{L}\n\\;=\\;\\frac{1}{n_{0}}\\sum_{i=1}^{n_{0}}\\varphi\\!\\left(\\theta^{(0,i)}\\right)\n\\;+\\;\\sum_{\\ell=1}^{L}\\frac{1}{n_{\\ell}}\\sum_{i=1}^{n_{\\ell}}\\Big(\\varphi\\!\\left(\\theta^{(\\ell,\\mathrm{f},i)}\\right)-\\varphi\\!\\left(\\theta^{(\\ell,\\mathrm{c},i)}\\right)\\Big),\n$$\n其中所有层级和所有重复实验的样本都是独立的，且 $\\,n_{\\ell}\\in\\mathbb{N}\\,$ 是在层级 $\\,\\ell\\,$ 使用的样本数量。假设以下逐层级的性能模型：\n- 对于每个 $\\,\\ell\\in\\{0,1,\\dots,L\\}\\,$，在层级 $\\,\\ell\\,$ 的每个样本的期望计算成本是一个有限常数 $\\,C_{\\ell}\\in(0,\\infty)\\,$，总成本为 $\\,\\sum_{\\ell=0}^{L}n_{\\ell}C_{\\ell}\\,$。\n- 对于每个 $\\,\\ell\\in\\{1,\\dots,L\\}\\,$，层级差异的方差为 $\\,\\mathrm{Var}\\!\\left(\\varphi(\\theta^{(\\ell,\\mathrm{f})})-\\varphi(\\theta^{(\\ell,\\mathrm{c})})\\right)=V_{\\ell}\\in(0,\\infty)\\,$，在层级 $\\,\\ell=0\\,$ 我们设置 $\\,V_{0}=\\mathrm{Var}\\!\\left(\\varphi(\\theta^{(0)})\\right)\\in(0,\\infty)\\,$。\n\n仅使用关于期望的线性性、层级间的独立性以及独立加数的方差可加性的基本事实，完成以下任务：\n- 从第一性原理构建用于估计 $\\,\\mathbb{E}_{\\pi_{\\epsilon_{L}}(\\cdot\\mid y)}[\\varphi(\\theta)]\\,$ 的基于伸缩和的多层级 ABC 估计量 $\\,\\widehat{\\mu}_{L}\\,$，并用 $\\,\\{n_{\\ell}\\}_{\\ell=0}^{L}\\,$、$\\,\\{V_{\\ell}\\}_{\\ell=0}^{L}\\,$ 和 $\\,\\{C_{\\ell}\\}_{\\ell=0}^{L}\\,$ 表示其方差和期望成本。\n- 然后，对于固定的总计算预算 $\\,C_{\\mathrm{tot}}\\in(0,\\infty)\\,$，推导最优分配 $\\,\\{n_{\\ell}^{\\star}\\}_{\\ell=0}^{L}\\,$，使其在约束条件 $\\,\\sum_{\\ell=0}^{L}n_{\\ell}C_{\\ell}=C_{\\mathrm{tot}}\\,$ 下能最小化 $\\,\\widehat{\\mu}_{L}\\,$ 的方差。\n\n你的最终答案必须是关于 $\\,\\{V_{\\ell}\\}_{\\ell=0}^{L}\\,$、$\\,\\{C_{\\ell}\\}_{\\ell=0}^{L}\\,$ 和 $\\,C_{\\mathrm{tot}}\\,$ 的函数，表示最优 $\\,n_{\\ell}^{\\star}\\,$ 的单个闭式解析表达式。不要提供任何数值近似。确保所有数学符号都以 LaTeX 格式呈现。最终答案中不需要单位。",
            "solution": "该问题旨在为近似贝叶斯计算 (ABC) 的多层级蒙特卡洛 (MLMC) 估计量推导最优样本分配。这包括首先表示出估计量的方差和计算成本，然后在固定成本预算的约束下最小化方差。推导将按要求从第一性原理出发。\n\n设我们感兴趣的量为 $\\mu_{L} = \\mathbb{E}_{\\pi_{\\epsilon_{L}}(\\cdot\\mid y)}[\\varphi(\\theta)]$，其中 $\\pi_{\\epsilon_{L}}(\\cdot\\mid y)$ 是在最精细容差水平 $\\epsilon_{L}$ 下的 ABC 后验。所提供的多层级伸缩恒等式允许我们将 $\\mu_{L}$ 表示为一个和式：\n$$\n\\mu_{L} = \\mathbb{E}_{\\pi_{\\epsilon_{0}}(\\cdot\\mid y)}[\\varphi(\\theta)] + \\sum_{\\ell=1}^{L}\\left(\\mathbb{E}_{\\pi_{\\epsilon_{\\ell}}(\\cdot\\mid y)}[\\varphi(\\theta)]-\\mathbb{E}_{\\pi_{\\epsilon_{\\ell-1}}(\\cdot\\mid y)}[\\varphi(\\theta)]\\right)\n$$\n设 $\\mu_0 = \\mathbb{E}_{\\pi_{\\epsilon_0}(\\cdot \\mid y)}[\\varphi(\\theta)]$，并且对于 $\\ell \\in \\{1,\\dots,L\\}$，设 $\\Delta_{\\ell} = \\mathbb{E}_{\\pi_{\\epsilon_\\ell}(\\cdot \\mid y)}[\\varphi(\\theta)] - \\mathbb{E}_{\\pi_{\\epsilon_{\\ell-1}}(\\cdot \\mid y)}[\\varphi(\\theta)]$。那么 $\\mu_L = \\mu_0 + \\sum_{\\ell=1}^{L} \\Delta_{\\ell}$。\n\n$\\mu_L$ 的 MLMC 估计量由下式给出\n$$\n\\widehat{\\mu}_{L} = \\frac{1}{n_{0}}\\sum_{i=1}^{n_{0}}\\varphi\\!\\left(\\theta^{(0,i)}\\right) + \\sum_{\\ell=1}^{L}\\frac{1}{n_{\\ell}}\\sum_{i=1}^{n_{\\ell}}\\Big(\\varphi\\!\\left(\\theta^{(\\ell,\\mathrm{f},i)}\\right)-\\varphi\\!\\left(\\theta^{(\\ell,\\mathrm{c},i)}\\right)\\Big)\n$$\n其中 $\\theta^{(0,i)} \\sim \\pi_{\\epsilon_0}(\\cdot|y)$ 是独立同分布 (i.i.d.) 样本，而 $(\\theta^{(\\ell,\\mathrm{f},i)}, \\theta^{(\\ell,\\mathrm{c},i)})$ 是 i.i.d. 耦合对，满足 $\\theta^{(\\ell,\\mathrm{f},i)} \\sim \\pi_{\\epsilon_\\ell}(\\cdot|y)$ 和 $\\theta^{(\\ell,\\mathrm{c},i)} \\sim \\pi_{\\epsilon_{\\ell-1}}(\\cdot|y)$。\n\n我们可以将该估计量写作 $\\widehat{\\mu}_{L} = \\sum_{\\ell=0}^{L} \\widehat{Y}_{\\ell}$，其中\n$$\n\\widehat{Y}_{0} = \\frac{1}{n_0}\\sum_{i=1}^{n_0}\\varphi(\\theta^{(0,i)})\n$$\n并且对于 $\\ell \\in \\{1, \\dots, L\\}$，\n$$\n\\widehat{Y}_{\\ell} = \\frac{1}{n_\\ell}\\sum_{i=1}^{n_\\ell}\\Big(\\varphi(\\theta^{(\\ell,\\mathrm{f},i)}) - \\varphi(\\theta^{(\\ell,\\mathrm{c},i)})\\Big)\n$$\n\n首先，我们证明 $\\widehat{\\mu}_{L}$ 是 $\\mu_{L}$ 的一个无偏估计量。根据期望的线性性，\n$$\n\\mathbb{E}[\\widehat{Y}_{0}] = \\mathbb{E}\\left[\\frac{1}{n_0}\\sum_{i=1}^{n_0}\\varphi(\\theta^{(0,i)})\\right] = \\frac{1}{n_0}\\sum_{i=1}^{n_0}\\mathbb{E}[\\varphi(\\theta^{(0,i)})] = \\mathbb{E}_{\\pi_{\\epsilon_0}(\\cdot|y)}[\\varphi(\\theta)] = \\mu_0\n$$\n对于 $\\ell \\in \\{1, \\dots, L\\}$，\n$$\n\\mathbb{E}[\\widehat{Y}_{\\ell}] = \\frac{1}{n_\\ell}\\sum_{i=1}^{n_\\ell}\\mathbb{E}[\\varphi(\\theta^{(\\ell,\\mathrm{f},i)}) - \\varphi(\\theta^{(\\ell,\\mathrm{c},i)})] = \\mathbb{E}[\\varphi(\\theta^{(\\ell,\\mathrm{f})})] - \\mathbb{E}[\\varphi(\\theta^{(\\ell,\\mathrm{c})})] = \\mathbb{E}_{\\pi_{\\epsilon_\\ell}(\\cdot|y)}[\\varphi(\\theta)] - \\mathbb{E}_{\\pi_{\\epsilon_{\\ell-1}}(\\cdot|y)}[\\varphi(\\theta)] = \\Delta_\\ell\n$$\n因此，总估计量的期望为\n$$\n\\mathbb{E}[\\widehat{\\mu}_{L}] = \\mathbb{E}\\left[\\sum_{\\ell=0}^{L} \\widehat{Y}_{\\ell}\\right] = \\sum_{\\ell=0}^{L} \\mathbb{E}[\\widehat{Y}_{\\ell}] = \\mu_0 + \\sum_{\\ell=1}^{L} \\Delta_\\ell = \\mu_L\n$$\n这证实了该估计量是无偏的。\n\n接下来，我们推导 $\\widehat{\\mu}_{L}$ 的方差。问题陈述中说明，所有层级和所有重复实验的样本都是独立的。这意味着每个层级的估计量 $\\widehat{Y}_0, \\widehat{Y}_1, \\dots, \\widehat{Y}_L$ 是相互独立的随机变量。根据独立变量的方差可加性，\n$$\n\\mathrm{Var}(\\widehat{\\mu}_{L}) = \\mathrm{Var}\\left(\\sum_{\\ell=0}^{L} \\widehat{Y}_{\\ell}\\right) = \\sum_{\\ell=0}^{L} \\mathrm{Var}(\\widehat{Y}_{\\ell})\n$$\n对于层级 $\\ell=0$，由于 $\\varphi(\\theta^{(0,i)})$ 是 i.i.d. 随机变量，\n$$\n\\mathrm{Var}(\\widehat{Y}_{0}) = \\mathrm{Var}\\left(\\frac{1}{n_0}\\sum_{i=1}^{n_0}\\varphi(\\theta^{(0,i)})\\right) = \\frac{1}{n_0^2}\\sum_{i=1}^{n_0}\\mathrm{Var}(\\varphi(\\theta^{(0,i)})) = \\frac{n_0}{n_0^2}\\mathrm{Var}(\\varphi(\\theta^{(0)})) = \\frac{V_0}{n_0}\n$$\n其中给定了 $V_0 = \\mathrm{Var}(\\varphi(\\theta^{(0)}))$。对于层级 $\\ell \\in \\{1, \\dots, L\\}$，差异项 $\\varphi(\\theta^{(\\ell,\\mathrm{f},i)}) - \\varphi(\\theta^{(\\ell,\\mathrm{c},i)})$ 对于 $i=1,\\dots,n_{\\ell}$ 是 i.i.d. 的。因此，\n$$\n\\mathrm{Var}(\\widehat{Y}_{\\ell}) = \\mathrm{Var}\\left(\\frac{1}{n_\\ell}\\sum_{i=1}^{n_\\ell}\\Big(\\varphi(\\theta^{(\\ell,\\mathrm{f},i)}) - \\varphi(\\theta^{(\\ell,\\mathrm{c},i)})\\Big)\\right) = \\frac{1}{n_\\ell^2}\\sum_{i=1}^{n_\\ell}\\mathrm{Var}\\big(\\varphi(\\theta^{(\\ell,\\mathrm{f},i)}) - \\varphi(\\theta^{(\\ell,\\mathrm{c},i)})\\big) = \\frac{V_\\ell}{n_\\ell}\n$$\n其中给定了 $V_\\ell = \\mathrm{Var}(\\varphi(\\theta^{(\\ell,\\mathrm{f})})-\\varphi(\\theta^{(\\ell,\\mathrm{c})}))$。\n因此，总方差为：\n$$\n\\mathrm{Var}(\\widehat{\\mu}_{L}) = \\sum_{\\ell=0}^{L}\\frac{V_{\\ell}}{n_{\\ell}}\n$$\n期望总计算成本为各层级成本之和：\n$$\n\\mathrm{Cost} = \\sum_{\\ell=0}^{L} n_{\\ell} C_{\\ell}\n$$\n\n优化问题是在固定总成本 $C_{\\mathrm{tot}}$ 的约束下，最小化方差 $\\mathrm{Var}(\\widehat{\\mu}_{L})$。\n$$\n\\text{最小化} \\quad \\mathcal{V}(\\{n_{\\ell}\\}) = \\sum_{\\ell=0}^{L}\\frac{V_{\\ell}}{n_{\\ell}} \\quad \\text{约束于} \\quad \\sum_{\\ell=0}^{L} n_{\\ell} C_{\\ell} = C_{\\mathrm{tot}}\n$$\n我们将 $n_{\\ell}$ 视为连续正实数变量，并使用拉格朗日乘数法来解决这个约束优化问题。拉格朗日函数为：\n$$\n\\mathcal{L}(\\{n_{\\ell}\\}_{\\ell=0}^{L}, \\lambda) = \\sum_{\\ell=0}^{L}\\frac{V_{\\ell}}{n_{\\ell}} + \\lambda\\left(\\sum_{\\ell=0}^{L}n_{\\ell}C_{\\ell} - C_{\\mathrm{tot}}\\right)\n$$\n为求最小值，我们将关于每个 $n_k$（其中 $k \\in \\{0, 1, \\dots, L\\}$）的偏导数设为零：\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial n_k} = -\\frac{V_k}{n_k^2} + \\lambda C_k = 0\n$$\n由于给定的 $V_k  0$ 和 $C_k  0$，拉格朗日乘子 $\\lambda$ 必须为正。解出 $n_k$：\n$$\n\\lambda C_k = \\frac{V_k}{n_k^2} \\implies n_k^2 = \\frac{V_k}{\\lambda C_k} \\implies n_k = \\frac{1}{\\sqrt{\\lambda}}\\sqrt{\\frac{V_k}{C_k}}\n$$\n现在，我们使用预算约束来确定 $\\lambda$。将 $n_k$ 的表达式代入约束方程：\n$$\n\\sum_{k=0}^{L} n_k C_k = \\sum_{k=0}^{L} \\left(\\frac{1}{\\sqrt{\\lambda}}\\sqrt{\\frac{V_k}{C_k}}\\right) C_k = C_{\\mathrm{tot}}\n$$\n$$\n\\frac{1}{\\sqrt{\\lambda}}\\sum_{k=0}^{L} \\sqrt{V_k C_k} = C_{\\mathrm{tot}}\n$$\n解出项 $1/\\sqrt{\\lambda}$：\n$$\n\\frac{1}{\\sqrt{\\lambda}} = \\frac{C_{\\mathrm{tot}}}{\\sum_{j=0}^{L}\\sqrt{V_j C_j}}\n$$\n此处我们使用 $j$ 作为求和索引以避免混淆。最后，我们将此结果代回 $n_k$ 的方程中，以获得最优分配，我们将其表示为 $n_k^{\\star}$。换回问题陈述中使用的索引 $\\ell$：\n$$\nn_{\\ell}^{\\star} = \\left(\\frac{C_{\\mathrm{tot}}}{\\sum_{j=0}^{L}\\sqrt{V_{j}C_{j}}}\\right) \\sqrt{\\frac{V_{\\ell}}{C_{\\ell}}}\n$$\n该表达式可以重写为：\n$$\nn_{\\ell}^{\\star} = C_{\\mathrm{tot}} \\frac{\\sqrt{V_{\\ell}/C_{\\ell}}}{\\sum_{j=0}^{L}\\sqrt{V_{j}C_{j}}}\n$$\n这就是在固定总计算预算 $C_{\\mathrm{tot}}$ 下，最小化估计量方差的每个层级 $\\ell \\in \\{0, \\dots, L\\}$ 的最优样本数 $n_{\\ell}^{\\star}$。",
            "answer": "$$\n\\boxed{\nC_{\\mathrm{tot}} \\frac{\\sqrt{\\frac{V_{\\ell}}{C_{\\ell}}}}{\\sum_{j=0}^{L} \\sqrt{V_{j} C_{j}}}\n}\n$$"
        }
    ]
}