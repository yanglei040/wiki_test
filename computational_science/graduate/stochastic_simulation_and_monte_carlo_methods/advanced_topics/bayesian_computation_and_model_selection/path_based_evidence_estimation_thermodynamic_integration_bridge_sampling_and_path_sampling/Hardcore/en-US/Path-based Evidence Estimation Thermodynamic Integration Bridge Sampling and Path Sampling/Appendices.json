{
    "hands_on_practices": [
        {
            "introduction": "This first practice provides a foundational, analytical look at what drives the variance in path-based estimators. By working through a simplified case with two Gaussian distributions, you will derive a closed-form expression for a path stability functional, $S(\\sigma_{0},\\sigma_{1})$, directly linking the statistical overlap between distributions to the variance of the thermodynamic integration estimator. This exercise  is crucial for building intuition about why some integration paths are more numerically stable than others.",
            "id": "3328156",
            "problem": "Consider two unnormalized densities on the real line, $q_{0}(x) = \\exp\\!\\big(-x^{2}/(2\\sigma_{0}^{2})\\big)$ and $q_{1}(x) = \\exp\\!\\big(-x^{2}/(2\\sigma_{1}^{2})\\big)$, where $\\sigma_{0} > 0$ and $\\sigma_{1} > 0$. Define the geometric path $\\{p_{t}(x) : t \\in [0,1]\\}$ by\n$$\np_{t}(x) \\propto q_{0}(x)^{1-t}\\,q_{1}(x)^{t},\n$$\nand the log-density increment along the path by\n$$\nU(x) \\equiv \\ln q_{1}(x) - \\ln q_{0}(x).\n$$\nA path-stability functional that quantifies overlap for thermodynamic integration (TI) and geometric-bridge path sampling is\n$$\nS(\\sigma_{0},\\sigma_{1}) \\equiv \\int_{0}^{1} \\mathrm{Var}_{p_{t}}\\!\\big[U(X)\\big] \\,\\mathrm{d}t,\n$$\nwhere the variance is taken with respect to $p_{t}$. Starting only from the definitions above and standard properties of the Gaussian distribution, derive a closed-form analytic expression for $S(\\sigma_{0},\\sigma_{1})$ in terms of $\\sigma_{0}$ and $\\sigma_{1}$ only. Your final answer must be a single simplified analytic expression. Do not include any units. No numerical rounding is required.",
            "solution": "The problem is valid as it is scientifically grounded in statistical mechanics, well-posed with all necessary information, and stated objectively. We can proceed to derive the closed-form expression for the path-stability functional $S(\\sigma_{0},\\sigma_{1})$.\n\nThe derivation proceeds in three main steps:\n1.  Determine the normalized probability distribution $p_t(x)$ for any given $t \\in [0,1]$.\n2.  Calculate the variance of the log-density increment, $\\mathrm{Var}_{p_{t}}[U(X)]$, as a function of $t$.\n3.  Integrate this variance over $t$ from $0$ to $1$ to find $S(\\sigma_{0},\\sigma_{1})$.\n\nStep 1: Characterize the probability distribution $p_t(x)$.\nThe unnormalized density path is given by $p_{t}(x) \\propto q_{0}(x)^{1-t}\\,q_{1}(x)^{t}$. Substituting the expressions for $q_{0}(x)$ and $q_{1}(x)$:\n$$\np_t(x) \\propto \\left(\\exp\\left(-\\frac{x^2}{2\\sigma_0^2}\\right)\\right)^{1-t} \\left(\\exp\\left(-\\frac{x^2}{2\\sigma_1^2}\\right)\\right)^{t}\n$$\nUsing the property $(\\exp(a))^b = \\exp(ab)$, we combine the exponents:\n$$\np_t(x) \\propto \\exp\\left(-\\frac{(1-t)x^2}{2\\sigma_0^2} - \\frac{tx^2}{2\\sigma_1^2}\\right) = \\exp\\left(-\\frac{x^2}{2}\\left(\\frac{1-t}{\\sigma_0^2} + \\frac{t}{\\sigma_1^2}\\right)\\right)\n$$\nThis functional form is that of a zero-mean Gaussian distribution, $\\mathcal{N}(0, \\sigma_t^2)$, with an unnormalized density kernel $\\exp(-x^2/(2\\sigma_t^2))$. By comparing the exponents, we can define the variance $\\sigma_t^2$ of the distribution $p_t(x)$ for each $t$:\n$$\n\\frac{1}{2\\sigma_t^2} = \\frac{1}{2}\\left(\\frac{1-t}{\\sigma_0^2} + \\frac{t}{\\sigma_1^2}\\right) \\implies \\frac{1}{\\sigma_t^2} = \\frac{1-t}{\\sigma_0^2} + \\frac{t}{\\sigma_1^2}\n$$\nThe normalized probability density function is therefore:\n$$\np_t(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma_t} \\exp\\left(-\\frac{x^2}{2\\sigma_t^2}\\right)\n$$\n\nStep 2: Calculate the variance $\\mathrm{Var}_{p_{t}}[U(X)]$.\nFirst, we find an explicit expression for the log-density increment $U(x)$:\n$$\nU(x) = \\ln q_{1}(x) - \\ln q_{0}(x) = \\ln\\left(\\exp\\left(-\\frac{x^2}{2\\sigma_1^2}\\right)\\right) - \\ln\\left(\\exp\\left(-\\frac{x^2}{2\\sigma_0^2}\\right)\\right)\n$$\n$$\nU(x) = -\\frac{x^2}{2\\sigma_1^2} + \\frac{x^2}{2\\sigma_0^2} = x^2 \\left(\\frac{1}{2\\sigma_0^2} - \\frac{1}{2\\sigma_1^2}\\right)\n$$\nLet's define a constant $C$ that is independent of $x$ and $t$:\n$$\nC \\equiv \\frac{1}{2\\sigma_0^2} - \\frac{1}{2\\sigma_1^2} = \\frac{\\sigma_1^2 - \\sigma_0^2}{2\\sigma_0^2\\sigma_1^2}\n$$\nSo, $U(x) = C x^2$. We need to compute the variance of $U(X)$ where $X$ is a random variable drawn from $p_t(x)$.\n$$\n\\mathrm{Var}_{p_{t}}[U(X)] = \\mathrm{Var}_{p_{t}}[C X^2] = C^2 \\mathrm{Var}_{p_{t}}[X^2]\n$$\nThe variance of $X^2$ is given by $\\mathrm{Var}_{p_{t}}[X^2] = \\mathbb{E}_{p_t}[(X^2)^2] - (\\mathbb{E}_{p_t}[X^2])^2 = \\mathbb{E}_{p_t}[X^4] - (\\mathbb{E}_{p_t}[X^2])^2$.\nFor a random variable $X \\sim \\mathcal{N}(0, \\sigma_t^2)$, the second and fourth moments are known to be:\n$$\n\\mathbb{E}_{p_t}[X^2] = \\sigma_t^2\n$$\n$$\n\\mathbb{E}_{p_t}[X^4] = 3(\\sigma_t^2)^2 = 3\\sigma_t^4\n$$\nSubstituting these moments into the expression for $\\mathrm{Var}_{p_{t}}[X^2]$:\n$$\n\\mathrm{Var}_{p_{t}}[X^2] = 3\\sigma_t^4 - (\\sigma_t^2)^2 = 2\\sigma_t^4\n$$\nNow we can express $\\mathrm{Var}_{p_{t}}[U(X)]$ in terms of $C$ and $\\sigma_t$:\n$$\n\\mathrm{Var}_{p_{t}}[U(X)] = C^2 (2\\sigma_t^4) = 2 \\left(\\frac{\\sigma_1^2 - \\sigma_0^2}{2\\sigma_0^2\\sigma_1^2}\\right)^2 \\sigma_t^4 = \\frac{(\\sigma_1^2 - \\sigma_0^2)^2}{2(\\sigma_0^2\\sigma_1^2)^2}\\sigma_t^4\n$$\nWe express $\\sigma_t^4$ using the relationship for $1/\\sigma_t^2$:\n$$\n\\sigma_t^2 = \\left(\\frac{1-t}{\\sigma_0^2} + \\frac{t}{\\sigma_1^2}\\right)^{-1} = \\left(\\frac{(1-t)\\sigma_1^2 + t\\sigma_0^2}{\\sigma_0^2\\sigma_1^2}\\right)^{-1} = \\frac{\\sigma_0^2\\sigma_1^2}{(1-t)\\sigma_1^2 + t\\sigma_0^2}\n$$\nTherefore,\n$$\n\\sigma_t^4 = \\frac{(\\sigma_0^2\\sigma_1^2)^2}{((1-t)\\sigma_1^2 + t\\sigma_0^2)^2}\n$$\nSubstituting this into the expression for the variance:\n$$\n\\mathrm{Var}_{p_{t}}[U(X)] = \\frac{(\\sigma_1^2 - \\sigma_0^2)^2}{2(\\sigma_0^2\\sigma_1^2)^2} \\frac{(\\sigma_0^2\\sigma_1^2)^2}{((1-t)\\sigma_1^2 + t\\sigma_0^2)^2} = \\frac{(\\sigma_1^2 - \\sigma_0^2)^2}{2((1-t)\\sigma_1^2 + t\\sigma_0^2)^2}\n$$\n\nStep 3: Integrate $\\mathrm{Var}_{p_{t}}[U(X)]$ over $t \\in [0,1]$.\nThe path-stability functional is $S(\\sigma_{0},\\sigma_{1}) = \\int_{0}^{1} \\mathrm{Var}_{p_{t}}[U(X)] \\mathrm{d}t$.\n$$\nS(\\sigma_{0},\\sigma_{1}) = \\int_{0}^{1} \\frac{(\\sigma_1^2 - \\sigma_0^2)^2}{2((1-t)\\sigma_1^2 + t\\sigma_0^2)^2} \\mathrm{d}t\n$$\nThe numerator is constant with respect to $t$, so we can pull it out of the integral:\n$$\nS(\\sigma_{0},\\sigma_{1}) = \\frac{(\\sigma_1^2 - \\sigma_0^2)^2}{2} \\int_{0}^{1} \\frac{1}{(\\sigma_1^2 + t(\\sigma_0^2 - \\sigma_1^2))^2} \\mathrm{d}t\n$$\nLet's evaluate the integral. Let $u = \\sigma_1^2 + t(\\sigma_0^2 - \\sigma_1^2)$. Then $\\mathrm{d}u = (\\sigma_0^2 - \\sigma_1^2) \\mathrm{d}t$. We change the integration variable from $t$ to $u$.\nAt $t=0$, $u = \\sigma_1^2$. At $t=1$, $u = \\sigma_1^2 + (\\sigma_0^2 - \\sigma_1^2) = \\sigma_0^2$.\nThe integral becomes:\n$$\n\\int_{\\sigma_1^2}^{\\sigma_0^2} \\frac{1}{u^2} \\frac{\\mathrm{d}u}{\\sigma_0^2 - \\sigma_1^2} = \\frac{1}{\\sigma_0^2 - \\sigma_1^2} \\int_{\\sigma_1^2}^{\\sigma_0^2} u^{-2} \\mathrm{d}u\n$$\nAssuming $\\sigma_0^2 \\neq \\sigma_1^2$:\n$$\n= \\frac{1}{\\sigma_0^2 - \\sigma_1^2} \\left[-\\frac{1}{u}\\right]_{\\sigma_1^2}^{\\sigma_0^2} = \\frac{1}{\\sigma_0^2 - \\sigma_1^2} \\left(-\\frac{1}{\\sigma_0^2} - \\left(-\\frac{1}{\\sigma_1^2}\\right)\\right)\n$$\n$$\n= \\frac{1}{\\sigma_0^2 - \\sigma_1^2} \\left(\\frac{1}{\\sigma_1^2} - \\frac{1}{\\sigma_0^2}\\right) = \\frac{1}{\\sigma_0^2 - \\sigma_1^2} \\left(\\frac{\\sigma_0^2 - \\sigma_1^2}{\\sigma_0^2\\sigma_1^2}\\right) = \\frac{1}{\\sigma_0^2\\sigma_1^2}\n$$\nNow, substitute this result back into the expression for $S(\\sigma_{0},\\sigma_{1})$:\n$$\nS(\\sigma_{0},\\sigma_{1}) = \\frac{(\\sigma_1^2 - \\sigma_0^2)^2}{2} \\left(\\frac{1}{\\sigma_0^2\\sigma_1^2}\\right) = \\frac{(\\sigma_1^2 - \\sigma_0^2)^2}{2\\sigma_0^2\\sigma_1^2}\n$$\nThis expression can be further simplified. We can rewrite it as:\n$$\nS(\\sigma_{0},\\sigma_{1}) = \\frac{1}{2} \\left(\\frac{\\sigma_1^2 - \\sigma_0^2}{\\sigma_0\\sigma_1}\\right)^2 = \\frac{1}{2} \\left(\\frac{\\sigma_1^2}{\\sigma_0\\sigma_1} - \\frac{\\sigma_0^2}{\\sigma_0\\sigma_1}\\right)^2 = \\frac{1}{2} \\left(\\frac{\\sigma_1}{\\sigma_0} - \\frac{\\sigma_0}{\\sigma_1}\\right)^2\n$$\nThis is the final closed-form analytic expression. If $\\sigma_0 = \\sigma_1$, then $U(x)=0$, so $\\mathrm{Var}_{p_t}[U(X)]=0$ and $S(\\sigma_0, \\sigma_0)=0$, which is consistent with the derived formula.",
            "answer": "$$\n\\boxed{\\frac{1}{2}\\left(\\frac{\\sigma_{1}}{\\sigma_{0}} - \\frac{\\sigma_{0}}{\\sigma_{1}}\\right)^{2}}\n$$"
        },
        {
            "introduction": "Building on the insight that path choice is critical, this exercise moves from theory to practice by comparing two fundamental path construction strategies. You will implement and analyze thermodynamic integration for a simple Bayesian model using both the standard power posterior path (tempering the likelihood) and an alternative path that tempers the prior. This hands-on comparison  will reveal the conditions under which one path is superior, a vital skill for optimizing evidence calculations.",
            "id": "3328093",
            "problem": "You are to implement and analyze two path-based estimators of the log-evidence (log marginal likelihood) in a Bayesian conjugate Gaussian model, and determine conditions under which tempering the prior within a path defined by $\\pi_\\beta \\propto p(y \\mid x) \\, p(x)^\\beta$ can reduce the Monte Carlo variance of thermodynamic integration relative to the standard power posterior path $\\pi_\\beta \\propto p(y \\mid x)^\\beta \\, p(x)$.\n\nConsider the model with prior $p(x) = \\mathcal{N}(x; 0, \\tau^2)$ and likelihood $p(y \\mid x) = \\mathcal{N}(y; x, \\sigma^2)$, where $\\mathcal{N}(a; b, c)$ denotes a Gaussian density in $a$ with mean $b$ and variance $c$. The goal is to estimate $\\log p(y)$ using thermodynamic integration along two different paths and to compute the asymptotic Monte Carlo variance of the corresponding numerical quadrature estimators when the expectation at each inverse-temperature parameter $\\beta$ is estimated by an independent Monte Carlo average of size $n$.\n\nBase definitions to use:\n- Bayes’ rule and normalizing constants: for any unnormalized density $q_\\beta(x)$ with normalizing constant $Z(\\beta) = \\int q_\\beta(x) \\, dx$, the normalized density is $\\pi_\\beta(x) = q_\\beta(x) / Z(\\beta)$.\n- The thermodynamic integration identity: if $q_\\beta(x)$ is differentiable in $\\beta$ and $\\partial_\\beta \\log q_\\beta(x)$ exists with sufficient integrability, then $\\frac{d}{d\\beta} \\log Z(\\beta) = \\mathbb{E}_{\\pi_\\beta} \\left[ \\partial_\\beta \\log q_\\beta(X) \\right]$ and hence $\\log Z(1) - \\log Z(0) = \\int_0^1 \\mathbb{E}_{\\pi_\\beta} \\left[ \\partial_\\beta \\log q_\\beta(X) \\right] \\, d\\beta$.\n- For a numerical quadrature with nodes $\\{\\beta_j\\}_{j=0}^{K-1}$ and weights $\\{w_j\\}_{j=0}^{K-1}$ approximating $\\int_0^1 g(\\beta) \\, d\\beta \\approx \\sum_{j=0}^{K-1} w_j g(\\beta_j)$, and independent Monte Carlo estimates $\\widehat{g}(\\beta_j)$ with $\\operatorname{Var}(\\widehat{g}(\\beta_j)) = \\operatorname{Var}_{\\pi_{\\beta_j}}(f(X))/n$, the asymptotic variance of the quadrature estimator is $\\sum_{j=0}^{K-1} w_j^2 \\, \\operatorname{Var}_{\\pi_{\\beta_j}}(f(X))/n$.\n\nPaths to compare:\n- Power posterior path: $q_\\beta^{\\mathrm{PP}}(x) = p(y \\mid x)^\\beta \\, p(x)$, so that $\\partial_\\beta \\log q_\\beta^{\\mathrm{PP}}(x) = \\log p(y \\mid x)$ and $\\pi_\\beta^{\\mathrm{PP}}(x) \\propto p(y \\mid x)^\\beta \\, p(x)$.\n- Prior-tempering path: $q_\\beta^{\\mathrm{PT}}(x) = p(y \\mid x) \\, p(x)^\\beta$, so that $\\partial_\\beta \\log q_\\beta^{\\mathrm{PT}}(x) = \\log p(x)$ and $\\pi_\\beta^{\\mathrm{PT}}(x) \\propto p(y \\mid x) \\, p(x)^\\beta$.\n\nIn the conjugate Gaussian setting, both $\\pi_\\beta^{\\mathrm{PP}}$ and $\\pi_\\beta^{\\mathrm{PT}}$ are Gaussian for each $\\beta \\in [0,1]$. You will exploit this to derive closed-form expressions for the variance, under $\\pi_\\beta$, of the relevant integrands $f_{\\mathrm{PP}}(x) = \\log p(y \\mid x)$ and $f_{\\mathrm{PT}}(x) = \\log p(x)$ at each $\\beta$.\n\nTask:\n- Derive, from first principles and the Gaussian identities, explicit formulas for the mean and variance of $\\pi_\\beta^{\\mathrm{PP}}$ and $\\pi_\\beta^{\\mathrm{PT}}$ as functions of $\\beta$, $y$, $\\tau^2$, and $\\sigma^2$.\n- From these, derive $\\operatorname{Var}_{\\pi_\\beta^{\\mathrm{PP}}}(\\log p(y \\mid X))$ and $\\operatorname{Var}_{\\pi_\\beta^{\\mathrm{PT}}}(\\log p(X))$ as functions of $\\beta$, $y$, $\\tau^2$, and $\\sigma^2$.\n- Using the uniform trapezoidal rule with $K$ grid points $\\beta_j = j/(K-1)$ for $j \\in \\{0, \\dots, K-1\\}$ and weights $w_0 = w_{K-1} = \\Delta \\beta / 2$, $w_j = \\Delta \\beta$ for $j \\in \\{1, \\dots, K-2\\}$, where $\\Delta \\beta = 1/(K-1)$, compute the asymptotic Monte Carlo variance of the thermodynamic integration estimator for each path, assuming $n$ independent samples are used at each grid point and independence across grid points.\n- State conditions on $\\sigma^2$, $\\tau^2$, and $y$ under which the prior-tempering path reduces the variance relative to the power posterior path. Your program must implement the variance computations for a finite set of parameter values and report which path is better in each case. The numerical value of the asymptotic variance for each path must be reported.\n\nTest suite:\nProvide results for the following parameter sets $(y, \\tau^2, \\sigma^2, K, n)$:\n- Case $1$: $(y, \\tau^2, \\sigma^2, K, n) = (0.5, 1.0, 1.0, 41, 1000)$.\n- Case $2$: $(y, \\tau^2, \\sigma^2, K, n) = (3.0, 10.0, 1.0, 41, 1000)$.\n- Case $3$: $(y, \\tau^2, \\sigma^2, K, n) = (3.0, 0.2, 5.0, 41, 1000)$.\n- Case $4$ (boundary discretization check): $(y, \\tau^2, \\sigma^2, K, n) = (1.0, 1.0, 0.1, 2, 1000)$.\n\nRequired final output format:\n- Your program should produce a single line of output containing a list of per-case results. Each per-case result must be a list of three entries: the asymptotic variance for the power posterior path (a float), the asymptotic variance for the prior-tempering path (a float), and a boolean indicating whether the prior-tempering path has strictly smaller variance than the power posterior path. The overall output must be a single list of these per-case lists, printed without extra whitespace beyond what Python’s default list-to-string conversion includes.\n- For example, the output must look like $[[v_{1,\\mathrm{PP}}, v_{1,\\mathrm{PT}}, b_1], [v_{2,\\mathrm{PP}}, v_{2,\\mathrm{PT}}, b_2], [v_{3,\\mathrm{PP}}, v_{3,\\mathrm{PT}}, b_3], [v_{4,\\mathrm{PP}}, v_{4,\\mathrm{PT}}, b_4]]$, where $v_{i,\\cdot}$ are floats and $b_i$ booleans. You must print the floats with at most $6$ decimal places.",
            "solution": "The problem requires the derivation and comparison of the asymptotic Monte Carlo variance for two thermodynamic integration (TI) paths used to estimate the log marginal likelihood, $\\log p(y)$, for a conjugate Gaussian model. The two paths are the standard power posterior (PP) path and a prior-tempering (PT) path.\n\nThe model is defined by a Gaussian prior and likelihood:\n- Prior: $p(x) = \\mathcal{N}(x; 0, \\tau^2) = \\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left(-\\frac{x^2}{2\\tau^2}\\right)$\n- Likelihood: $p(y \\mid x) = \\mathcal{N}(y; x, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-x)^2}{2\\sigma^2}\\right)$\n\nThe core of thermodynamic integration is the identity $\\log Z(1) - \\log Z(0) = \\int_0^1 \\mathbb{E}_{\\pi_\\beta} \\left[ \\partial_\\beta \\log q_\\beta(X) \\right] \\, d\\beta$, where $\\pi_\\beta(x) = q_\\beta(x)/Z(\\beta)$ is a family of distributions indexed by $\\beta \\in [0,1]$ that connects a tractable initial distribution ($\\beta=0$) to the target distribution ($\\beta=1$). The asymptotic variance of a numerical quadrature estimator of this integral is $\\mathcal{V} = \\frac{1}{n} \\sum_{j=0}^{K-1} w_j^2 \\operatorname{Var}_{\\pi_{\\beta_j}}(f(X))$, where $f(X) = \\partial_\\beta \\log q_\\beta(X)$ and $n$ is the number of Monte Carlo samples per grid point $\\beta_j$.\n\nWe will first derive the parameters of the intermediate distributions $\\pi_\\beta(x)$ and the variance of the integrand for each path. A key mathematical tool is the property that the product of Gaussian densities results in another (unnormalized) Gaussian density. A generic unnormalized log-Gaussian density of the form $-\\frac{1}{2v}x^2 + \\frac{\\mu}{v}x + C$ corresponds to a Gaussian distribution $\\mathcal{N}(x; \\mu, v)$.\n\n### 1. Power Posterior (PP) Path Analysis\nFor the PP path, the unnormalized density is $q_\\beta^{\\mathrm{PP}}(x) = p(y \\mid x)^\\beta p(x)$. The corresponding normalized density is $\\pi_\\beta^{\\mathrm{PP}}(x)$. The log-density is:\n$$ \\log q_\\beta^{\\mathrm{PP}}(x) = \\beta \\log p(y \\mid x) + \\log p(x) + C_1 = -\\beta \\frac{(x-y)^2}{2\\sigma^2} - \\frac{x^2}{2\\tau^2} + C_2 $$\nExpanding and collecting terms in $x$:\n$$ \\log q_\\beta^{\\mathrm{PP}}(x) = -\\frac{1}{2} \\left[ \\left(\\frac{\\beta}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)x^2 - \\frac{2\\beta y}{\\sigma^2} x \\right] + C_3 $$\nThis shows that $\\pi_\\beta^{\\mathrm{PP}}(x)$ is a Gaussian distribution $\\mathcal{N}(x; \\mu_\\beta^{\\mathrm{PP}}, v_\\beta^{\\mathrm{PP}})$. By completing the square, we identify the inverse variance and the mean-variance product:\n$$ \\frac{1}{v_\\beta^{\\mathrm{PP}}} = \\frac{\\beta}{\\sigma^2} + \\frac{1}{\\tau^2} \\implies v_\\beta^{\\mathrm{PP}} = \\frac{\\sigma^2\\tau^2}{\\beta\\tau^2 + \\sigma^2} $$\n$$ \\frac{\\mu_\\beta^{\\mathrm{PP}}}{v_\\beta^{\\mathrm{PP}}} = \\frac{\\beta y}{\\sigma^2} \\implies \\mu_\\beta^{\\mathrm{PP}} = v_\\beta^{\\mathrm{PP}} \\frac{\\beta y}{\\sigma^2} = \\frac{\\beta y \\tau^2}{\\beta\\tau^2 + \\sigma^2} $$\nThe TI integrand for this path is $f_{\\mathrm{PP}}(x) = \\partial_\\beta \\log q_\\beta^{\\mathrm{PP}}(x) = \\log p(y \\mid x) = -\\frac{(x-y)^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi\\sigma^2)$. We need its variance, $\\operatorname{Var}_{\\pi_\\beta^{\\mathrm{PP}}}(f_{\\mathrm{PP}}(X))$. The constant term does not affect variance.\n$$ \\operatorname{Var}_{\\pi_\\beta^{\\mathrm{PP}}}(\\log p(y \\mid X)) = \\operatorname{Var}_{\\pi_\\beta^{\\mathrm{PP}}}\\left(-\\frac{(X-y)^2}{2\\sigma^2}\\right) = \\frac{1}{4\\sigma^4} \\operatorname{Var}((X-y)^2) $$\nFor $X \\sim \\mathcal{N}(\\mu, v)$, the random variable $Y = X-y$ follows $\\mathcal{N}(\\mu-y, v)$. The variance of $Y^2$ is $\\operatorname{Var}(Y^2) = 2v(2(\\mu-y)^2 + v)$. Substituting $\\mu = \\mu_\\beta^{\\mathrm{PP}}$ and $v = v_\\beta^{\\mathrm{PP}}$:\n$$ \\operatorname{Var}_{\\pi_\\beta^{\\mathrm{PP}}} = \\frac{1}{4\\sigma^4} \\left[ 2v_\\beta^{\\mathrm{PP}} (2(\\mu_\\beta^{\\mathrm{PP}}-y)^2 + v_\\beta^{\\mathrm{PP}}) \\right] = \\frac{v_\\beta^{\\mathrm{PP}}}{2\\sigma^4} (2(\\mu_\\beta^{\\mathrm{PP}}-y)^2 + v_\\beta^{\\mathrm{PP}}) $$\nSubstituting the expressions for $\\mu_\\beta^{\\mathrm{PP}}$ and $v_\\beta^{\\mathrm{PP}}$ and simplifying yields the per-stratum variance for the PP path:\n$$ V_{\\mathrm{PP}}(\\beta) = \\frac{y^2 \\sigma^2 \\tau^2}{(\\beta\\tau^2 + \\sigma^2)^3} + \\frac{\\tau^4}{2(\\beta\\tau^2 + \\sigma^2)^2} $$\n\n### 2. Prior-Tempering (PT) Path Analysis\nFor the PT path, $q_\\beta^{\\mathrm{PT}}(x) = p(y \\mid x) p(x)^\\beta$. The log-density is:\n$$ \\log q_\\beta^{\\mathrm{PT}}(x) = \\log p(y \\mid x) + \\beta \\log p(x) + C_4 = -\\frac{(x-y)^2}{2\\sigma^2} - \\beta\\frac{x^2}{2\\tau^2} + C_5 $$\n$$ \\log q_\\beta^{\\mathrm{PT}}(x) = -\\frac{1}{2} \\left[ \\left(\\frac{1}{\\sigma^2} + \\frac{\\beta}{\\tau^2}\\right)x^2 - \\frac{2y}{\\sigma^2} x \\right] + C_6 $$\nThis corresponds to a Gaussian distribution $\\pi_\\beta^{\\mathrm{PT}}(x) = \\mathcal{N}(x; \\mu_\\beta^{\\mathrm{PT}}, v_\\beta^{\\mathrm{PT}})$ with parameters:\n$$ \\frac{1}{v_\\beta^{\\mathrm{PT}}} = \\frac{1}{\\sigma^2} + \\frac{\\beta}{\\tau^2} \\implies v_\\beta^{\\mathrm{PT}} = \\frac{\\sigma^2\\tau^2}{\\tau^2 + \\beta\\sigma^2} $$\n$$ \\frac{\\mu_\\beta^{\\mathrm{PT}}}{v_\\beta^{\\mathrm{PT}}} = \\frac{y}{\\sigma^2} \\implies \\mu_\\beta^{\\mathrm{PT}} = v_\\beta^{\\mathrm{PT}} \\frac{y}{\\sigma^2} = \\frac{y \\tau^2}{\\tau^2 + \\beta\\sigma^2} $$\nThe TI integrand is $f_{\\mathrm{PT}}(x) = \\partial_\\beta \\log q_\\beta^{\\mathrm{PT}}(x) = \\log p(x) = -\\frac{x^2}{2\\tau^2} - \\frac{1}{2}\\log(2\\pi\\tau^2)$. We require its variance under $\\pi_\\beta^{\\mathrm{PT}}$:\n$$ \\operatorname{Var}_{\\pi_\\beta^{\\mathrm{PT}}}(\\log p(X)) = \\operatorname{Var}_{\\pi_\\beta^{\\mathrm{PT}}}\\left(-\\frac{X^2}{2\\tau^2}\\right) = \\frac{1}{4\\tau^4} \\operatorname{Var}(X^2) $$\nUsing the identity $\\operatorname{Var}(X^2) = 2v(2\\mu^2+v)$ for $X \\sim \\mathcal{N}(\\mu,v)$, with $\\mu = \\mu_\\beta^{\\mathrm{PT}}$ and $v = v_\\beta^{\\mathrm{PT}}$:\n$$ \\operatorname{Var}_{\\pi_\\beta^{\\mathrm{PT}}} = \\frac{1}{4\\tau^4} \\left[ 2v_\\beta^{\\mathrm{PT}} (2(\\mu_\\beta^{\\mathrm{PT}})^2 + v_\\beta^{\\mathrm{PT}}) \\right] = \\frac{v_\\beta^{\\mathrm{PT}}}{2\\tau^4} (2(\\mu_\\beta^{\\mathrm{PT}})^2 + v_\\beta^{\\mathrm{PT}}) $$\nSubstituting and simplifying leads to the per-stratum variance for the PT path:\n$$ V_{\\mathrm{PT}}(\\beta) = \\frac{y^2 \\sigma^2 \\tau^2}{(\\tau^2 + \\beta\\sigma^2)^3} + \\frac{\\sigma^4}{2(\\tau^2 + \\beta\\sigma^2)^2} $$\n\n### 3. Asymptotic Variance and Path Comparison\nThe total asymptotic variance for an estimator using the trapezoidal rule with $K$ points is:\n$$ \\mathcal{V} = \\frac{1}{n} \\sum_{j=0}^{K-1} w_j^2 V(\\beta_j) = \\frac{(\\Delta\\beta)^2}{n} \\left[ \\frac{V(0)}{4} + \\sum_{j=1}^{K-2} V(\\beta_j) + \\frac{V(1)}{4} \\right] $$\nwhere $\\beta_j = j/(K-1)$, $\\Delta\\beta = 1/(K-1)$, and $V(\\beta)$ is either $V_{\\mathrm{PP}}(\\beta)$ or $V_{\\mathrm{PT}}(\\beta)$.\n\nTo determine when the PT path is superior (i.e., has lower variance), we compare $V_{\\mathrm{PP}}(\\beta)$ and $V_{\\mathrm{PT}}(\\beta)$. The variance expressions are complex, but analysis at the endpoints $\\beta=0$ and $\\beta=1$ is revealing.\nAt $\\beta=0$:\n$$ V_{\\mathrm{PP}}(0) = \\frac{2y^2\\tau^2 + \\tau^4}{2\\sigma^4}, \\quad V_{\\mathrm{PT}}(0) = \\frac{2y^2\\sigma^2 + \\sigma^4}{2\\tau^4} $$\n$V_{\\mathrm{PT}}(0) < V_{\\mathrm{PP}}(0)$ if and only if $\\tau^4(2y^2\\sigma^2 + \\sigma^4) < \\sigma^4(2y^2\\tau^2 + \\tau^4)$, which simplifies to $2y^2\\sigma^6+\\sigma^8 < 2y^2\\tau^6+\\tau^8$. Since the function $g(v) = 2y^2v^3 + v^4$ is monotonically increasing for $v > 0$, this inequality holds if and only if $\\sigma^2 < \\tau^2$.\n\nAt $\\beta=1$:\n$$ V_{\\mathrm{PP}}(1) = \\frac{y^2 \\sigma^2 \\tau^2}{(\\tau^2 + \\sigma^2)^3} + \\frac{\\tau^4}{2(\\tau^2 + \\sigma^2)^2}, \\quad V_{\\mathrm{PT}}(1) = \\frac{y^2 \\sigma^2 \\tau^2}{(\\tau^2 + \\sigma^2)^3} + \\frac{\\sigma^4}{2(\\tau^2 + \\sigma^2)^2} $$\n$V_{\\mathrm{PT}}(1) < V_{\\mathrm{PP}}(1)$ if and only if $\\sigma^4 < \\tau^4$, which is equivalent to $\\sigma^2 < \\tau^2$.\n\nSince the variance for the PT path is lower at both endpoints if and only if $\\sigma^2 < \\tau^2$, and given that the variance is typically dominated by the behavior near $\\beta=0$ (especially when there is a mismatch between prior and likelihood), the general condition for the prior-tempering path to have lower variance than the power posterior path is $\\sigma^2 < \\tau^2$. This corresponds to the case where the likelihood is more concentrated (informative) than the prior. The PT path is advantageous because it avoids evaluating a narrow likelihood using samples from a broad prior, which is the high-variance scenario for the PP path near $\\beta=0$.\n\nThe implementation will compute the total asymptotic variance for each path and each test case based on the derived formulas and the trapezoidal rule sum.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares the asymptotic variance of thermodynamic integration\n    for a conjugate Gaussian model using two different paths: power posterior and\n    prior tempering.\n    \"\"\"\n    test_cases = [\n        # (y, tau^2, sigma^2, K, n)\n        (0.5, 1.0, 1.0, 41, 1000),\n        (3.0, 10.0, 1.0, 41, 1000),\n        (3.0, 0.2, 5.0, 41, 1000),\n        (1.0, 1.0, 0.1, 2, 1000),\n    ]\n\n    def v_pp_func(beta, y, tau2, sigma2):\n        \"\"\"Calculates Var_pi_beta^PP(log p(y|X)) for a given beta.\"\"\"\n        denominator = beta * tau2 + sigma2\n        term1 = (y**2 * sigma2 * tau2) / (denominator**3)\n        term2 = (tau2**2) / (2 * denominator**2)\n        return term1 + term2\n\n    def v_pt_func(beta, y, tau2, sigma2):\n        \"\"\"Calculates Var_pi_beta^PT(log p(X)) for a given beta.\"\"\"\n        denominator = tau2 + beta * sigma2\n        term1 = (y**2 * sigma2 * tau2) / (denominator**3)\n        term2 = (sigma2**2) / (2 * denominator**2)\n        return term1 + term2\n\n    final_results = []\n\n    for case in test_cases:\n        y, tau2, sigma2, K, n = case\n\n        if K  2:\n            # The trapezoidal rule as defined requires at least 2 points (K>=2).\n            # A result of NaN indicates this invalid parameter.\n            final_results.append([np.nan, np.nan, False])\n            continue\n        \n        # Grid points for numerical integration\n        betas = np.linspace(0.0, 1.0, K)\n        \n        # Calculate integrand variances at each grid point\n        V_pp_values = v_pp_func(betas, y, tau2, sigma2)\n        V_pt_values = v_pt_func(betas, y, tau2, sigma2)\n\n        # Calculate total asymptotic variance using trapezoidal rule weights\n        # V_total = (1/n) * sum(w_j^2 * V(beta_j))\n        # w_j = delta_beta for interior, delta_beta/2 for endpoints\n        # So w_j^2 = (delta_beta)^2 for interior, (delta_beta/2)^2 for endpoints\n        delta_beta = 1.0 / (K - 1)\n        \n        if K == 2:\n            # sum over j=1...K-2 is empty\n            sum_V_pp_interior = 0\n            sum_V_pt_interior = 0\n        else:\n            sum_V_pp_interior = np.sum(V_pp_values[1:-1])\n            sum_V_pt_interior = np.sum(V_pt_values[1:-1])\n\n        total_var_pp = (delta_beta**2 / n) * (\n            V_pp_values[0] / 4.0 + sum_V_pp_interior + V_pp_values[-1] / 4.0\n        )\n        total_var_pt = (delta_beta**2 / n) * (\n            V_pt_values[0] / 4.0 + sum_V_pt_interior + V_pt_values[-1] / 4.0\n        )\n\n        is_pt_better = total_var_pt  total_var_pp\n        \n        final_results.append([total_var_pp, total_var_pt, is_pt_better])\n\n    # Format the final output string as specified\n    results_to_print = []\n    for res in final_results:\n        v_pp, v_pt, b = res\n        # Round to 6 decimal places for printing as per instruction \"at most 6\"\n        # str(round(val, 6)) achieves this.\n        results_to_print.append([round(v_pp, 6), round(v_pt, 6), b])\n\n    # Convert list of lists to string and remove spaces\n    print(str(results_to_print).replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world modeling often involves some degree of misspecification, which can cause severe instability in evidence estimators. This final practice tackles this advanced challenge by guiding you to design and implement a \"robust\" integration path that actively counteracts the effects of a misspecified observation variance. By simultaneously tempering the likelihood and adapting the prior, you will learn how to engineer a path that maintains stability even when the model and data are in conflict .",
            "id": "3328132",
            "problem": "Consider a hierarchical Gaussian location model analyzed under a potentially misspecified observation noise variance. Let $y \\in \\mathbb{R}^n$ denote the observed data vector, modeled as $y_i \\mid \\theta \\sim \\mathcal{N}(\\theta, \\sigma_{\\mathrm{mis}}^2)$ for $i \\in \\{1,\\dots,n\\}$ with a Gaussian prior $\\theta \\sim \\mathcal{N}(0, \\tau^2)$. The symbol $\\sigma_{\\mathrm{mis}}^2$ denotes the variance used in the analysis model (which may be misspecified relative to the data-generating process), and $\\tau^2$ is the prior variance. The marginal likelihood (also called the evidence) is $Z = \\int L(y \\mid \\theta; \\sigma_{\\mathrm{mis}}^2)\\, p(\\theta; \\tau^2)\\, d\\theta$, where $L$ is the likelihood and $p$ is the prior.\n\nThermodynamic integration (TI) and more generally path sampling estimate $\\log Z$ by integrating expectations along a path of distributions $\\{\\pi_t: t \\in [0,1]\\}$ with unnormalized density $q_t(\\theta)$. The fundamental identity is that for any smooth path with $q_t(\\theta)$ and normalized $\\pi_t(\\theta) = q_t(\\theta)/Z_t$, one has\n$$\n\\log Z_1 - \\log Z_0 \\;=\\; \\int_0^1 \\mathbb{E}_{\\pi_t}\\!\\left[ \\frac{\\partial}{\\partial t} \\log q_t(\\theta) \\right] dt,\n$$\nwhere $Z_t = \\int q_t(\\theta)\\, d\\theta$ and $Z_1 = Z$ by appropriate endpoint choice. To reduce integrand instability under likelihood–prior conflict (e.g., due to misspecified $\\sigma_{\\mathrm{mis}}^2$), consider a robust path that tempers both the likelihood scale and the prior scale. Let the unnormalized path be\n$$\nq_t(\\theta) \\;\\propto\\; L(y \\mid \\theta; \\sigma_{\\mathrm{mis}}^2)^{t}\\; p_t(\\theta),\n$$\nwhere $p_t(\\theta)$ is a Gaussian $\\mathcal{N}(0, \\tau^2(t))$ with a $t$-dependent variance chosen to vary linearly in $t$ between an inflated prior variance at $t=0$ and the target prior at $t=1$, namely\n$$\n\\tau^2(t) \\;=\\; \\tau^2 \\left( c - (c-1) t \\right),\n$$\nwith $c  1$ fixed. Note that $p_t$ is normalized for all $t \\in [0,1]$, and hence $Z_0 = \\int L(y \\mid \\theta; \\sigma_{\\mathrm{mis}}^2)^{0} p_0(\\theta)\\, d\\theta = 1$.\n\nTasks:\n\n- Starting only from the definitions of marginal likelihood, Gaussian densities, and the path sampling identity above, derive the exact form of the TI integrand for:\n  - the standard likelihood-tempering path with constant prior variance, i.e., $q_t^{\\mathrm{std}}(\\theta) \\propto L(y \\mid \\theta; \\sigma_{\\mathrm{mis}}^2)^{t} \\, \\mathcal{N}(0,\\tau^2)$,\n  - the robust path with $\\tau^2(t)$ as specified, i.e., $q_t^{\\mathrm{rob}}(\\theta) \\propto L(y \\mid \\theta; \\sigma_{\\mathrm{mis}}^2)^{t} \\, \\mathcal{N}(0, \\tau^2(t))$.\n- Using only Gaussian conjugacy, derive the closed-form Gaussian posterior $\\pi_t(\\theta)$ for each path, along with its mean and variance as functions of $t$, $n$, $\\sigma_{\\mathrm{mis}}^2$, $\\tau^2$ (or $\\tau^2(t)$), and the sample mean $\\bar{y}$.\n- Express the TI integrand entirely in terms of expectations under $\\pi_t(\\theta)$ of quadratic forms in $\\theta$, reducing these to functions of the posterior mean and variance. For the robust path, include the derivative contribution from the changing prior scale.\n- Provide a closed-form expression for $\\log Z$ under the analysis model $\\mathcal{N}(\\theta, \\sigma_{\\mathrm{mis}}^2)$ with prior $\\mathcal{N}(0,\\tau^2)$, as a function of $y$, $n$, $\\sigma_{\\mathrm{mis}}^2$, and $\\tau^2$. This will serve as a ground truth for error assessment.\n\nImplementation requirements:\n\n- Data generation for sensitivity study: For each test case, generate $y$ by simulating $n$ independent draws $y_i \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{true}}^2)$ using a fixed random seed. The analysis model uses $\\sigma_{\\mathrm{mis}}^2$ as the observation variance and the given $\\tau^2$ as the prior variance. The parameter $\\sigma_{\\mathrm{true}}^2$ is used only for data generation.\n- Numerical integration: Approximate the $t$-integral using the trapezoidal rule on an equally spaced grid of $K$ points $t_j = j/(K-1)$ for $j \\in \\{0,1,\\dots,K-1\\}$ with $K = 9$.\n- For each test case, compute two TI estimates of $\\log Z$: one for the standard path and one for the robust path with the specified inflation factor $c$. Then compute the absolute errors of both estimates relative to the closed-form $\\log Z$.\n- Final output format: Your program should produce a single line containing a comma-separated list representation of a list of per-case triplets, each triplet being $[\\mathrm{err\\_std}, \\mathrm{err\\_rob}, \\mathrm{robust\\_better}]$, where $\\mathrm{err\\_std}$ and $\\mathrm{err\\_rob}$ are the absolute errors of the standard and robust path TI estimates, respectively, each rounded to $6$ decimal places, and $\\mathrm{robust\\_better}$ is a boolean indicating whether the robust path absolute error is strictly smaller than the standard path absolute error. The line must have no spaces. For example, a valid output with two cases would look like $[[0.123456,0.012345,True],[0.010000,0.020000,False]]$.\n\nTest suite:\n\nUse the following test cases, each specified by $(n, \\sigma_{\\mathrm{true}}, \\sigma_{\\mathrm{mis}}, \\tau, c, \\mathrm{seed})$.\n\n- Case $1$: $(20, 1.0, 1.0, 1.5, 5.0, 1)$.\n- Case $2$: $(50, 2.0, 0.5, 1.0, 20.0, 2)$.\n- Case $3$: $(10, 0.5, 2.0, 1.0, 5.0, 3)$.\n- Case $4$: $(2, 1.0, 0.2, 0.5, 10.0, 4)$.\n\nAll angles, if any, must be interpreted in radians. There are no physical units in this problem. Your program must implement the full pipeline described above and print exactly one line in the specified format.",
            "solution": "The problem is assessed as valid as it is scientifically grounded, well-posed, and objective, presenting a standard problem in computational statistics without any contradictions or ambiguities.\n\nThe derivation of the required quantities proceeds in four steps. First, we determine the general form of the thermodynamic integration (TI) integrand. Second, we derive the path-dependent posterior distributions. Third, we combine these results to obtain explicit expressions for the integrands for both the standard and robust paths. Fourth, we derive the closed-form marginal likelihood, which serves as the ground truth for error evaluation.\n\nThe likelihood for the data $y = (y_1, \\dots, y_n)$ and the prior for $\\theta$ are given by Gaussian densities:\n$$\nL(y \\mid \\theta; \\sigma_{\\mathrm{mis}}^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma_{\\mathrm{mis}}^2}} \\exp\\left(-\\frac{(y_i - \\theta)^2}{2\\sigma_{\\mathrm{mis}}^2}\\right) = (2\\pi\\sigma_{\\mathrm{mis}}^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma_{\\mathrm{mis}}^2}\\sum_{i=1}^n (y_i - \\theta)^2\\right)\n$$\n$$\np(\\theta; \\tau^2) = \\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left(-\\frac{\\theta^2}{2\\tau^2}\\right)\n$$\nFor the robust path, the prior $p_t(\\theta)$ is $\\mathcal{N}(0, \\tau^2(t))$, where $\\tau^2(t) = \\tau^2(c - (c-1)t)$.\n\nThe unnormalized density along the path is $q_t(\\theta) \\propto L(y \\mid \\theta; \\sigma_{\\mathrm{mis}}^2)^t p_t(\\theta)$. Taking the logarithm, we have:\n$$\n\\log q_t(\\theta) = t \\log L(y \\mid \\theta; \\sigma_{\\mathrm{mis}}^2) + \\log p_t(\\theta) + C\n$$\nwhere $C$ is a constant independent of $\\theta$ and $t$. The TI integrand is the expectation of the derivative of $\\log q_t(\\theta)$ with respect to $t$:\n$$\n\\frac{\\partial}{\\partial t}\\log q_t(\\theta) = \\log L(y \\mid \\theta; \\sigma_{\\mathrm{mis}}^2) + \\frac{\\partial}{\\partial t}\\log p_t(\\theta)\n$$\nThe path-dependent prior is $p_t(\\theta) = (2\\pi\\tau^2(t))^{-1/2} \\exp(-\\frac{\\theta^2}{2\\tau^2(t)})$. Its log-derivative is:\n$$\n\\frac{\\partial}{\\partial t}\\log p_t(\\theta) = \\frac{\\partial}{\\partial t}\\left(-\\frac{1}{2}\\log(2\\pi\\tau^2(t)) - \\frac{\\theta^2}{2\\tau^2(t)}\\right) = -\\frac{1}{2\\tau^2(t)}\\frac{d\\tau^2(t)}{dt} + \\frac{\\theta^2}{2(\\tau^2(t))^2}\\frac{d\\tau^2(t)}{dt} = \\left(\\frac{\\theta^2}{2\\tau^2(t)} - \\frac{1}{2}\\right)\\frac{1}{\\tau^2(t)}\\frac{d\\tau^2(t)}{dt}\n$$\nThe full TI integrand, $I(t) = \\mathbb{E}_{\\pi_t}[\\frac{\\partial}{\\partial t}\\log q_t(\\theta)]$, is therefore:\n$$\nI(t) = \\mathbb{E}_{\\pi_t}[\\log L(y \\mid \\theta; \\sigma_{\\mathrm{mis}}^2)] + \\left(\\frac{\\mathbb{E}_{\\pi_t}[\\theta^2]}{2\\tau^2(t)} - \\frac{1}{2}\\right)\\frac{1}{\\tau^2(t)}\\frac{d\\tau^2(t)}{dt}\n$$\nwhere $\\pi_t(\\theta) = q_t(\\theta) / \\int q_t(\\theta') d\\theta'$.\n\nFor the **standard path**, $\\tau^2(t) = \\tau^2$ is constant, so $\\frac{d\\tau^2(t)}{dt} = 0$. The second term vanishes, and the integrand simplifies to:\n$$\nI_{\\mathrm{std}}(t) = \\mathbb{E}_{\\pi_t^{\\mathrm{std}}}[\\log L(y \\mid \\theta; \\sigma_{\\mathrm{mis}}^2)]\n$$\nFor the **robust path**, $\\tau^2(t) = \\tau^2(c - (c-1)t)$, so $\\frac{d\\tau^2(t)}{dt} = -\\tau^2(c-1)$. The integrand is:\n$$\nI_{\\mathrm{rob}}(t) = \\mathbb{E}_{\\pi_t^{\\mathrm{rob}}}[\\log L(y \\mid \\theta; \\sigma_{\\mathrm{mis}}^2)] - \\frac{\\tau^2(c-1)}{\\tau^2(t)}\\left(\\frac{\\mathbb{E}_{\\pi_t^{\\mathrm{rob}}}[\\theta^2]}{2\\tau^2(t)} - \\frac{1}{2}\\right)\n$$\n\nNext, we find the posterior distribution $\\pi_t(\\theta)$. Since the likelihood and prior are Gaussian, the posterior is also Gaussian. The log-posterior is quadratic in $\\theta$:\n$$\n\\log \\pi_t(\\theta) \\propto t \\log L(y \\mid \\theta) + \\log p_t(\\theta) \\propto -\\frac{t}{2\\sigma_{\\mathrm{mis}}^2}\\sum_{i=1}^n(y_i - \\theta)^2 - \\frac{\\theta^2}{2\\tau^2(t)}\n$$\nExpanding the sum and collecting terms in $\\theta$:\n$$\n\\sum(y_i - \\theta)^2 = n\\theta^2 - 2n\\bar{y}\\theta + \\sum y_i^2\n$$\n$$\n\\log \\pi_t(\\theta) \\propto -\\frac{1}{2}\\left(\\frac{nt}{\\sigma_{\\mathrm{mis}}^2} + \\frac{1}{\\tau^2(t)}\\right)\\theta^2 + \\left(\\frac{nt\\bar{y}}{\\sigma_{\\mathrm{mis}}^2}\\right)\\theta + \\mathrm{const}\n$$\nBy completing the square, we identify $\\pi_t(\\theta)$ as $\\mathcal{N}(\\theta \\mid \\mu_t, V_t)$ with posterior precision (inverse variance) $\\frac{1}{V_t} = \\frac{nt}{\\sigma_{\\mathrm{mis}}^2} + \\frac{1}{\\tau^2(t)}$ and mean $\\mu_t = V_t \\left(\\frac{nt\\bar{y}}{\\sigma_{\\mathrm{mis}}^2}\\right)$. The posterior variance is thus $V_t = \\left(\\frac{nt}{\\sigma_{\\mathrm{mis}}^2} + \\frac{1}{\\tau^2(t)}\\right)^{-1}$. The required expectation $\\mathbb{E}_{\\pi_t}[\\theta^2]$ is given by $V_t + \\mu_t^2$.\nFor the standard path, $\\tau^2(t)$ is replaced by the constant $\\tau^2$. For the robust path, the full expression for $\\tau^2(t)$ is used.\n\nThe expectation of the log-likelihood term can be expressed using the posterior moments:\n$$\n\\mathbb{E}_{\\pi_t}[\\log L] = -\\frac{n}{2}\\log(2\\pi\\sigma_{\\mathrm{mis}}^2) - \\frac{1}{2\\sigma_{\\mathrm{mis}}^2}\\mathbb{E}_{\\pi_t}\\left[\\sum(y_i-\\theta)^2\\right]\n$$\n$$\n\\mathbb{E}_{\\pi_t}\\left[\\sum(y_i-\\theta)^2\\right] = \\mathbb{E}_{\\pi_t}\\left[ \\sum y_i^2 - 2n\\bar{y}\\theta + n\\theta^2 \\right] = \\sum y_i^2 - 2n\\bar{y}\\mu_t + n(V_t + \\mu_t^2)\n$$\nThese expressions fully specify the integrands $I_{\\mathrm{std}}(t)$ and $I_{\\mathrm{rob}}(t)$ in terms of known quantities and the path parameter $t$.\n\nFinally, the exact marginal likelihood $Z$ is derived. The model specifies $y_i|\\theta \\sim \\mathcal{N}(\\theta, \\sigma_{\\mathrm{mis}}^2)$ and $\\theta \\sim \\mathcal{N}(0, \\tau^2)$. This implies a marginal distribution for the data vector $y$ that is a multivariate normal, $y \\sim \\mathcal{N}(0, \\Sigma_y)$. The mean is $\\mathbb{E}[y] = \\mathbb{E}[\\mathbb{E}[y|\\theta]] = \\mathbb{E}[\\mathbf{1}\\theta]=0$. The covariance is $\\mathrm{Cov}(y) = \\mathbb{E}[\\mathrm{Cov}(y|\\theta)]+\\mathrm{Cov}(\\mathbb{E}[y|\\theta]) = \\sigma_{\\mathrm{mis}}^2 I_n + \\tau^2 J_n$, where $I_n$ is the identity matrix and $J_n$ is the matrix of all ones. The log marginal likelihood is $\\log Z = \\log p(y)$:\n$$\n\\log Z = -\\frac{n}{2}\\log(2\\pi) - \\frac{1}{2}\\log(\\det(\\Sigma_y)) - \\frac{1}{2}y^T\\Sigma_y^{-1}y\n$$\nThe determinant is $\\det(\\Sigma_y) = (\\sigma_{\\mathrm{mis}}^2)^{n-1}(\\sigma_{\\mathrm{mis}}^2 + n\\tau^2)$. The quadratic form is $y^T\\Sigma_y^{-1}y = \\frac{\\sum(y_i-\\bar{y})^2}{\\sigma_{\\mathrm{mis}}^2} + \\frac{n\\bar{y}^2}{\\sigma_{\\mathrm{mis}}^2 + n\\tau^2}$. Combining these yields the closed-form expression for the ground truth:\n$$\n\\log Z = -\\frac{n}{2}\\log(2\\pi\\sigma_{\\mathrm{mis}}^2) - \\frac{1}{2}\\log\\left(1 + \\frac{n\\tau^2}{\\sigma_{\\mathrm{mis}}^2}\\right) - \\frac{1}{2}\\left(\\frac{\\sum(y_i-\\bar{y})^2}{\\sigma_{\\mathrm{mis}}^2} + \\frac{n\\bar{y}^2}{\\sigma_{\\mathrm{mis}}^2 + n\\tau^2}\\right)\n$$\nThis formula can be simplified from the general expression by factoring out $\\sigma_{\\mathrm{mis}}^2$ from the log-determinant term. With these analytical results, we can proceed to numerical implementation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares thermodynamic integration estimates of marginal likelihood\n    for a hierarchical Gaussian model using standard and robust paths.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, sigma_true, sigma_mis, tau, c, seed)\n        (20, 1.0, 1.0, 1.5, 5.0, 1),\n        (50, 2.0, 0.5, 1.0, 20.0, 2),\n        (10, 0.5, 2.0, 1.0, 5.0, 3),\n        (2, 1.0, 0.2, 0.5, 10.0, 4),\n    ]\n\n    results_as_strings = []\n    K = 9  # Number of grid points for trapezoidal rule\n    t_grid = np.linspace(0, 1, K)\n\n    for n, sigma_true, sigma_mis, tau, c, seed in test_cases:\n        # Generate data\n        rng = np.random.default_rng(seed)\n        y = rng.normal(loc=0.0, scale=sigma_true, size=n)\n        \n        # Pre-compute statistics and parameters\n        y_bar = np.mean(y)\n        sum_sq_y = np.sum(y**2)\n        sum_sq_dev_y = np.sum((y - y_bar)**2)\n        s2_mis = sigma_mis**2\n        tau2 = tau**2\n\n        # 1. Ground Truth (Closed-Form) Log Marginal Likelihood\n        log_det_term = np.log(s2_mis + n * tau2) + (n - 1) * np.log(s2_mis)\n        quad_term = sum_sq_dev_y / s2_mis + (n * y_bar**2) / (s2_mis + n * tau2)\n        logZ_true = -n / 2.0 * np.log(2.0 * np.pi) - 0.5 * log_det_term - 0.5 * quad_term\n\n        # 2. Thermodynamic Integration\n        integrand_std = np.zeros(K)\n        integrand_rob = np.zeros(K)\n\n        for i, t in enumerate(t_grid):\n            # --- Standard Path Calculation ---\n            prec_prior_std = 1.0 / tau2\n            prec_lik_std = n * t / s2_mis\n            V_t_std = 1.0 / (prec_lik_std + prec_prior_std)\n            mu_t_std = V_t_std * (n * t * y_bar / s2_mis)\n            \n            # Integrand: E[log L]\n            E_logL_std = -n/2.0 * np.log(2.0 * np.pi * s2_mis) - \\\n                         1.0/(2.0*s2_mis) * (sum_sq_y - 2.0*n*y_bar*mu_t_std + n*(V_t_std + mu_t_std**2))\n            integrand_std[i] = E_logL_std\n\n            # --- Robust Path Calculation ---\n            tau2_t = tau2 * (c - (c - 1) * t)\n            prec_prior_rob = 1.0 / tau2_t\n            prec_lik_rob = n * t / s2_mis\n            V_t_rob = 1.0 / (prec_lik_rob + prec_prior_rob)\n            mu_t_rob = V_t_rob * (n * t * y_bar / s2_mis)\n            \n            # Integrand Part 1: E[log L]\n            E_logL_rob = -n/2.0 * np.log(2.0 * np.pi * s2_mis) - \\\n                         1.0/(2.0*s2_mis) * (sum_sq_y - 2.0*n*y_bar*mu_t_rob + n*(V_t_rob + mu_t_rob**2))\n\n            # Integrand Part 2: Derivative of log prior\n            E_theta2_rob = V_t_rob + mu_t_rob**2\n            prior_deriv_term = -((c - 1) / (c - (c - 1) * t)) * (E_theta2_rob / (2.0 * tau2_t) - 0.5)\n            \n            integrand_rob[i] = E_logL_rob + prior_deriv_term\n            \n        # 3. Numerical Integration (Trapezoidal Rule)\n        logZ_std = np.trapz(integrand_std, t_grid)\n        logZ_rob = np.trapz(integrand_rob, t_grid)\n\n        # 4. Compute Absolute Errors and Compare\n        err_std = abs(logZ_std - logZ_true)\n        err_rob = abs(logZ_rob - logZ_true)\n        robust_better = err_rob  err_std\n\n        # Format results as specified\n        results_as_strings.append(f\"[{err_std:.6f},{err_rob:.6f},{robust_better}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```"
        }
    ]
}