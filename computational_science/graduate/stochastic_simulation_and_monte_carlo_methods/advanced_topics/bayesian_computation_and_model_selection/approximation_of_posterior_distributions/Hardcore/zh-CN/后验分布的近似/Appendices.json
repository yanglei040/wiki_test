{
    "hands_on_practices": [
        {
            "introduction": "重要性抽样是当无法直接从后验分布中抽样时，用于近似后验期望的一种基本技巧。然而，常见的自归一化重要性抽样器 (Self-Normalized Importance Sampler) 在样本量有限时是有偏的，理解这一特性至关重要。本练习提供了一个可控的环境：通过使用一个共轭模型（泊松-伽马），我们可以计算出精确的后验期望，从而通过模拟来精确地量化和感受这种偏差。",
            "id": "3289041",
            "problem": "给定一个单一观测值，其模型为泊松似然，未知率参数服从Gamma先验。您的任务是从第一性原理出发，推导精确的后验分布，计算指定泛函的精确后验期望，然后使用对数正态提议分布，量化自归一化重要性抽样（SNIS）近似的有限样本偏差。您的程序必须实现完整的流程，并生成单行输出，该输出汇总了所提供测试套件的偏差估计值。\n\n您可以使用的基本原理包括：\n- 贝叶斯定理：对于先验密度 $p(\\lambda)$、似然 $p(y \\mid \\lambda)$ 和后验 $p(\\lambda \\mid y)$，有恒等式 $p(\\lambda \\mid y) \\propto p(y \\mid \\lambda) p(\\lambda)$。\n- 泊松概率质量函数：$p(y \\mid \\lambda) = \\exp(-\\lambda) \\lambda^{y} / y!$，其中 $y \\in \\{0,1,2,\\dots\\}$ 且 $\\lambda > 0$。\n- 形状-率参数化的Gamma概率密度函数：$p(\\lambda \\mid \\alpha, \\beta) = \\beta^{\\alpha} \\lambda^{\\alpha - 1} \\exp(-\\beta \\lambda) / \\Gamma(\\alpha)$，其中 $\\alpha > 0$，$\\beta > 0$ 且 $\\lambda > 0$。\n- 参数为 $(\\mu, \\sigma)$ 的对数正态概率密度函数：$q(\\lambda \\mid \\mu, \\sigma) = \\left[ \\lambda \\sigma \\sqrt{2 \\pi} \\right]^{-1} \\exp\\left( - \\frac{(\\log \\lambda - \\mu)^2}{2 \\sigma^2} \\right)$，其中 $\\sigma > 0$ 且 $\\lambda > 0$。\n\n需要完成的任务：\n1) 仅从贝叶斯定理以及泊松和Gamma分布的定义出发，推导模型的精确后验分布 $p(\\lambda \\mid y, \\alpha, \\beta)$。该模型包含单一观测值 $y \\in \\{0,1,2,\\dots\\}$，先验 $\\lambda \\sim \\mathrm{Gamma}(\\alpha, \\beta)$（形状参数为 $\\alpha$，率参数为 $\\beta$），以及似然 $y \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$。实现一个函数，以相同的形状-率形式返回后验参数。\n2) 对于步骤1中得到的后验分布，考虑泛函 $h_1(\\lambda) = \\lambda$ 和 $h_2(\\lambda) = \\log \\lambda$。利用Gamma分布的公认性质，计算在精确后验分布下的精确期望 $\\mathbb{E}[h_1(\\lambda) \\mid y, \\alpha, \\beta]$ 和 $\\mathbb{E}[h_2(\\lambda) \\mid y, \\alpha, \\beta]$。\n3) 使用对数正态提议分布 $q(\\lambda \\mid \\mu, \\sigma)$ 实现自归一化重要性抽样（SNIS），以近似计算 $h \\in \\{h_1, h_2\\}$ 的 $\\mathbb{E}[h(\\lambda)]$。给定从 $q$ 中抽取的 $N$ 个独立同分布（IID）样本 $\\{\\lambda_i\\}_{i=1}^N$，使用精确到乘法常数的后验密度，构成未归一化的重要性权重 $w_i \\propto \\frac{p(\\lambda_i \\mid y, \\alpha, \\beta)}{q(\\lambda_i \\mid \\mu, \\sigma)}$，然后归一化权重 $\\tilde{w}_i = \\frac{w_i}{\\sum_{j=1}^N w_j}$，并计算SNIS估计值 $\\hat{I}_N(h) = \\sum_{i=1}^N \\tilde{w}_i h(\\lambda_i)$。在对数域中对权重进行数值稳定的计算。\n4) 通过蒙特卡洛复制方法，量化SNIS估计器对 $h_1$ 和 $h_2$ 的有限样本偏差。具体而言，通过对SNIS过程进行 $R$ 次独立同分布的复制，计算经验偏差估计 $\\widehat{\\mathrm{Bias}}_R(h) = \\left( \\frac{1}{R} \\sum_{r=1}^R \\hat{I}_{N,r}(h) \\right) - \\mathbb{E}[h(\\lambda) \\mid y, \\alpha, \\beta]$，其中 $\\hat{I}_{N,r}(h)$ 是第 $r$ 次复制中的SNIS估计值。使用基础随机种子 $s_0$，并对于测试用例索引 $k \\in \\{0,1,2\\}$，使用种子 $s_0 + k$ 来初始化该测试用例的随机数生成器。所有随机性必须从用于提议分布 $q$ 的、由 $(\\mu, \\sigma)$ 参数化的对数正态分布中生成。\n5) 实现以下测试套件。对于每个测试用例，按顺序 $[ \\widehat{\\mathrm{Bias}}_R(h_1), \\widehat{\\mathrm{Bias}}_R(h_2) ]$ 计算两个经验偏差，并按用例1、用例2、用例3的顺序，将所有测试用例的结果汇总成一个单一的扁平化列表。\n- 使用 $N = 2000$ 和 $R = 400$。\n- 使用基础种子 $s_0 = 20251010$。\n- 测试用例1：$y = 12$, $\\alpha = 2.5$, $\\beta = 1.3$, $\\mu = \\log\\left( \\frac{\\alpha + y}{\\beta + 1} \\right)$, $\\sigma = 0.7$。\n- 测试用例2：$y = 1$, $\\alpha = 0.6$, $\\beta = 0.2$, $\\mu = -0.5$, $\\sigma = 1.1$。\n- 测试用例3：$y = 100$, $\\alpha = 10.0$, $\\beta = 5.0$, $\\mu = \\log\\left( \\frac{\\alpha + y}{\\beta + 1} \\right) - 0.5$, $\\sigma = 0.5$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按 $[\\widehat{\\mathrm{Bias}}_R(h_1)\\_\\mathrm{case1}, \\widehat{\\mathrm{Bias}}_R(h_2)\\_\\mathrm{case1}, \\widehat{\\mathrm{Bias}}_R(h_1)\\_\\mathrm{case2}, \\widehat{\\mathrm{Bias}}_R(h_2)\\_\\mathrm{case2}, \\widehat{\\mathrm{Bias}}_R(h_1)\\_\\mathrm{case3}, \\widehat{\\mathrm{Bias}}_R(h_2)\\_\\mathrm{case3}]$ 的顺序包含6个浮点数。",
            "solution": "问题陈述已经过严格审查，并被确定为有效。它具有科学依据，问题设定良好、客观、自洽且一致。所有提供的数据和定义都清晰且足以推导出唯一且有意义的解。该问题是贝叶斯推断和蒙特卡洛模拟中一个标准而全面的练习。我们现在开始解答。\n\n### 步骤1：后验分布的推导\n\n该模型由单一观测值 $y$ 的泊松似然和率参数 $\\lambda$ 的Gamma先验定义。\n先验分布为 $\\lambda \\sim \\mathrm{Gamma}(\\alpha, \\beta)$，其概率密度函数（PDF）为：\n$$\np(\\lambda \\mid \\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} e^{-\\beta \\lambda}\n$$\n观测值 $y$ 的似然为 $y \\mid \\lambda \\sim \\mathrm{Poisson}(\\lambda)$，其概率质量函数（PMF）为：\n$$\np(y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{y}}{y!}\n$$\n根据贝叶斯定理，后验分布 $p(\\lambda \\mid y, \\alpha, \\beta)$ 与似然和先验的乘积成正比：\n$$\np(\\lambda \\mid y, \\alpha, \\beta) \\propto p(y \\mid \\lambda) p(\\lambda \\mid \\alpha, \\beta)\n$$\n代入似然和先验的函数形式，我们得到：\n$$\np(\\lambda \\mid y, \\alpha, \\beta) \\propto \\left( \\frac{e^{-\\lambda} \\lambda^{y}}{y!} \\right) \\left( \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} e^{-\\beta \\lambda} \\right)\n$$\n我们可以将所有不依赖于 $\\lambda$ 的项归入比例常数中。这些项是 $\\frac{1}{y!}$、$\\beta^{\\alpha}$ 和 $\\frac{1}{\\Gamma(\\alpha)}$。合并作为 $\\lambda$ 函数的项：\n$$\np(\\lambda \\mid y, \\alpha, \\beta) \\propto \\lambda^{y} e^{-\\lambda} \\cdot \\lambda^{\\alpha - 1} e^{-\\beta \\lambda}\n$$\n利用指数的性质，我们合并 $\\lambda$ 的幂和指数函数的参数：\n$$\np(\\lambda \\mid y, \\alpha, \\beta) \\propto \\lambda^{y + \\alpha - 1} e^{-(\\lambda + \\beta\\lambda)}\n$$\n$$\np(\\lambda \\mid y, \\alpha, \\beta) \\propto \\lambda^{(\\alpha + y) - 1} e^{-(\\beta + 1)\\lambda}\n$$\n所得表达式是Gamma分布的核。通过观察，我们可以确定这个后验Gamma分布的新参数。设后验参数为 $\\alpha'$ 和 $\\beta'$。我们可以看到：\n$$\n\\alpha' = \\alpha + y\n$$\n$$\n\\beta' = \\beta + 1\n$$\n因此，给定观测值 $y$ 和先验参数 $\\alpha$、$\\beta$ 时，$\\lambda$ 的后验分布是具有更新参数的Gamma分布：\n$$\n\\lambda \\mid y, \\alpha, \\beta \\sim \\mathrm{Gamma}(\\alpha' = \\alpha + y, \\beta' = \\beta + 1)\n$$\n这证明了Gamma先验与泊松似然的共轭性。\n\n### 步骤2：精确后验期望的计算\n\n在确定后验分布为 $\\lambda \\mid y \\sim \\mathrm{Gamma}(\\alpha', \\beta')$ 后，我们可以计算指定泛函 $h_1(\\lambda) = \\lambda$ 和 $h_2(\\lambda) = \\log\\lambda$ 的精确期望。\n\n对于形状为 $k$、尺度为 $\\theta$ 的随机变量 $X \\sim \\mathrm{Gamma}(k, \\theta)$，其均值为 $\\mathbb{E}[X] = k\\theta$。在我们的形状-率参数化中，率参数为 $\\beta_p = 1/\\theta$，均值为 $\\mathbb{E}[X] = k/\\beta_p$。\n对于我们的后验分布 $\\mathrm{Gamma}(\\alpha', \\beta')$， $h_1(\\lambda) = \\lambda$ 的期望是：\n$$\n\\mathbb{E}[h_1(\\lambda) \\mid y, \\alpha, \\beta] = \\mathbb{E}[\\lambda \\mid y] = \\frac{\\alpha'}{\\beta'} = \\frac{\\alpha + y}{\\beta + 1}\n$$\n对于泛函 $h_2(\\lambda) = \\log\\lambda$，服从Gamma分布的变量 $X \\sim \\mathrm{Gamma}(\\alpha_p, \\beta_p)$ 的期望由 $\\mathbb{E}[\\log X] = \\psi(\\alpha_p) - \\log(\\beta_p)$ 给出，其中 $\\psi(\\cdot)$ 是双伽玛函数，定义为伽玛函数的对数导数，即 $\\psi(z) = \\frac{d}{dz}\\log\\Gamma(z)$。\n将此应用于我们的后验分布：\n$$\n\\mathbb{E}[h_2(\\lambda) \\mid y, \\alpha, \\beta] = \\mathbb{E}[\\log \\lambda \\mid y] = \\psi(\\alpha') - \\log(\\beta') = \\psi(\\alpha + y) - \\log(\\beta + 1)\n$$\n\n### 步骤3：自归一化重要性抽样（SNIS）\n\n目标是使用提议分布 $q(\\lambda)$ 来估计后验期望 $\\mathbb{E}[h(\\lambda) \\mid y] = \\frac{\\int h(\\lambda) p(y|\\lambda)p(\\lambda) d\\lambda}{\\int p(y|\\lambda)p(\\lambda) d\\lambda}$。令 $\\tilde{p}(\\lambda) = p(y|\\lambda)p(\\lambda)$ 为未归一化的后验。\n基于从提议分布 $q(\\lambda)$ 中抽取的 $N$ 个样本 $\\{\\lambda_i\\}_{i=1}^N$，泛函 $h(\\lambda)$ 的SNIS估计器为：\n$$\n\\hat{I}_N(h) = \\sum_{i=1}^{N} \\tilde{w}_i h(\\lambda_i)\n$$\n其中归一化权重 $\\tilde{w}_i$ 由下式给出：\n$$\n\\tilde{w}_i = \\frac{w_i}{\\sum_{j=1}^{N} w_j} \\quad \\text{其中原始权重为} \\quad w_i = \\frac{\\tilde{p}(\\lambda_i)}{q(\\lambda_i)}\n$$\n我们的目标分布（未归一化后验）是 $\\tilde{p}(\\lambda) \\propto \\lambda^{\\alpha' - 1} e^{-\\beta'\\lambda}$，而我们的提议分布是对数正态分布 $q(\\lambda \\mid \\mu, \\sigma)$。\n为防止数值下溢或上溢，计算在对数域中进行。原始权重的对数为：\n$$\n\\log w_i = \\log \\tilde{p}(\\lambda_i) - \\log q(\\lambda_i)\n$$\n我们只需要目标密度的核，所以我们使用 $\\log \\tilde{p}_{\\text{kernel}}(\\lambda) = (\\alpha' - 1)\\log\\lambda - \\beta'\\lambda$。提议分布PDF的对数为 $\\log q(\\lambda \\mid \\mu, \\sigma) = -\\log\\lambda - \\log\\sigma - \\frac{1}{2}\\log(2\\pi) - \\frac{(\\log\\lambda - \\mu)^2}{2\\sigma^2}$。所以，\n$$\n\\log w_i = \\left( (\\alpha' - 1)\\log\\lambda_i - \\beta'\\lambda_i \\right) - \\left( -\\log\\lambda_i - \\log\\sigma - \\frac{1}{2}\\log(2\\pi) - \\frac{(\\log\\lambda_i - \\mu)^2}{2\\sigma^2} \\right)\n$$\n为了归一化，我们使用log-sum-exp技巧。令 $L_i = \\log w_i$ 且 $L_{\\max} = \\max_i \\{L_i\\}$。归一化权重为：\n$$\n\\tilde{w}_i = \\frac{e^{L_i}}{\\sum_{j=1}^N e^{L_j}} = \\frac{e^{L_i - L_{\\max}}}{\\sum_{j=1}^N e^{L_j - L_{\\max}}}\n$$\n这种稳定的计算方法可以避免浮点误差。\n\n### 步骤4：量化有限样本偏差\n\n对于有限的 $N$，SNIS估计器通常是有偏的。偏差定义为 $\\mathrm{Bias}(\\hat{I}_N(h)) = \\mathbb{E}[\\hat{I}_N(h)] - \\mathbb{E}[h(\\lambda) \\mid y]$。我们使用蒙特卡洛模拟来估计此偏差。通过生成 $R$ 个SNIS估计的独立复制 $\\{\\hat{I}_{N,r}(h)\\}_{r=1}^R$，我们可以用样本均值 $\\frac{1}{R}\\sum_{r=1}^R \\hat{I}_{N,r}(h)$ 来近似期望 $\\mathbb{E}[\\hat{I}_N(h)]$。于是，经验偏差为：\n$$\n\\widehat{\\mathrm{Bias}}_R(h) = \\left( \\frac{1}{R} \\sum_{r=1}^R \\hat{I}_{N,r}(h) \\right) - \\mathbb{E}[h(\\lambda) \\mid y]\n$$\n此过程将针对问题中指定的每个测试用例，为泛函 $h_1(\\lambda)$ 和 $h_2(\\lambda)$ 实现。随机数生成器使用种子 $s_0 + k$（其中 $k$ 为测试用例索引）以确保可复现性。",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import digamma\n\ndef solve():\n    \"\"\"\n    Implements the full pipeline to derive posterior, compute exact expectations,\n    and quantify the finite-sample bias of a Self-Normalized Importance Sampling\n    (SNIS) approximation for a Poisson-Gamma model.\n    \"\"\"\n\n    # Global parameters from the problem statement\n    N = 2000\n    R = 400\n    s0 = 20251010\n\n    def calculate_biases(y, alpha, beta, mu, sigma, seed):\n        \"\"\"\n        Calculates the SNIS bias for a single test case.\n\n        Args:\n            y (int): The observed Poisson count.\n            alpha (float): The shape parameter of the Gamma prior.\n            beta (float): The rate parameter of the Gamma prior.\n            mu (float): The mean parameter of the Lognormal proposal (on the log scale).\n            sigma (float): The standard deviation of the Lognormal proposal (on the log scale).\n            seed (int): The random seed for this test case.\n\n        Returns:\n            A tuple (bias_h1, bias_h2) containing the empirical biases for\n            h1(lambda) = lambda and h2(lambda) = log(lambda).\n        \"\"\"\n        # Set the seed for this specific test case to ensure reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Step 1: Derive posterior parameters\n        # Posterior is Gamma(alpha', beta')\n        alpha_post = alpha + y\n        beta_post = beta + 1\n\n        # Step 2: Compute exact posterior expectations\n        exact_exp_h1 = alpha_post / beta_post\n        exact_exp_h2 = digamma(alpha_post) - np.log(beta_post)\n\n        # Accumulators for the mean of the SNIS estimates over R replicates\n        total_snis_h1 = 0.0\n        total_snis_h2 = 0.0\n\n        # Step 4: Quantify bias via Monte Carlo replication\n        for _ in range(R):\n            # Step 3: Implement one replicate of the SNIS estimator\n\n            # Draw N samples from the Lognormal proposal distribution q(lambda | mu, sigma)\n            # numpy.random.lognormal uses mu and sigma of the underlying normal distribution.\n            lambda_samples = rng.lognormal(mean=mu, sigma=sigma, size=N)\n            log_lambda_samples = np.log(lambda_samples)\n\n            # Calculate log of the unnormalized posterior (target) density kernel\n            # log p(lambda|y) \\propto (alpha_post - 1) * log(lambda) - beta_post * lambda\n            log_target_unnorm = (alpha_post - 1) * log_lambda_samples - beta_post * lambda_samples\n\n            # Calculate log of the Lognormal proposal density\n            # log q(lambda) = -log(lambda) - log(sigma) - 0.5*log(2*pi) - (log(lambda)-mu)^2 / (2*sigma^2)\n            log_proposal = -log_lambda_samples - np.log(sigma) - 0.5 * np.log(2 * np.pi) - \\\n                           (log_lambda_samples - mu)**2 / (2 * sigma**2)\n\n            # Calculate log of the unnormalized importance weights\n            log_weights = log_target_unnorm - log_proposal\n\n            # Normalize weights in a numerically stable way (log-sum-exp trick)\n            # This prevents underflow/overflow when exponentiating.\n            log_weights_max = np.max(log_weights)\n            weights = np.exp(log_weights - log_weights_max)\n            normalized_weights = weights / np.sum(weights)\n\n            # Compute the SNIS estimates for this replicate for h1 and h2\n            snis_h1 = np.sum(normalized_weights * lambda_samples)\n            snis_h2 = np.sum(normalized_weights * log_lambda_samples)\n\n            # Accumulate the estimates\n            total_snis_h1 += snis_h1\n            total_snis_h2 += snis_h2\n\n        # Calculate the average SNIS estimates over all R replicates\n        avg_snis_h1 = total_snis_h1 / R\n        avg_snis_h2 = total_snis_h2 / R\n\n        # Compute the final empirical bias estimates\n        bias_h1 = avg_snis_h1 - exact_exp_h1\n        bias_h2 = avg_snis_h2 - exact_exp_h2\n\n        return bias_h1, bias_h2\n\n    # Step 5: Implement the test suite\n    test_cases_params = [\n        # Test Case 1\n        {'y': 12, 'alpha': 2.5, 'beta': 1.3, 'mu_func': lambda a, y, b: np.log((a + y) / (b + 1)), 'sigma': 0.7},\n        # Test Case 2\n        {'y': 1, 'alpha': 0.6, 'beta': 0.2, 'mu_func': lambda a, y, b: -0.5, 'sigma': 1.1},\n        # Test Case 3\n        {'y': 100, 'alpha': 10.0, 'beta': 5.0, 'mu_func': lambda a, y, b: np.log((a + y) / (b + 1)) - 0.5, 'sigma': 0.5},\n    ]\n\n    all_biases = []\n\n    for i, params in enumerate(test_cases_params):\n        y_val = params['y']\n        alpha_val = params['alpha']\n        beta_val = params['beta']\n        mu_val = params['mu_func'](alpha_val, y_val, beta_val)\n        sigma_val = params['sigma']\n        seed_val = s0 + i\n\n        bias_h1, bias_h2 = calculate_biases(y_val, alpha_val, beta_val, mu_val, sigma_val, seed_val)\n        all_biases.extend([bias_h1, bias_h2])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_biases))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Bernstein–von Mises 定理指出，在有足够数据的情况下，许多后验分布会趋近于简单的高斯形态，这是许多近似方法（如拉普拉斯近似）的理论基石。然而，这个强大的结果依赖于特定的“正则性”条件。本练习将通过一个经典的反例——数据范围依赖于参数——来让你亲手验证，当关键条件被违反时，即使在大量数据下，后验分布也不会收敛到高斯分布。",
            "id": "3289088",
            "problem": "考虑诊断伯恩斯坦–冯·米塞斯定理（后验的渐近正态性）在非正则模型与正则基线模型中失效的任务。请从第一性原理出发：用于构建后验的贝叶斯法则、似然函数的定义以及基本的概率变换。不要先验地假设任何渐近正态性结果。\n\n您的目标是实现一个程序，该程序针对一小部分测试用例，构建后验分布，从中生成独立同分布的样本，并计算定量诊断指标，以揭示即使在样本量很大时，后验形状是接近高斯分布还是明显非高斯分布。\n\n模型和任务：\n\n1) 非正则模型（支撑集依赖于参数）：\n- 数据模型：对于参数 $\\theta \\in (0, b)$，其中 $b > 0$ 是固定且已知的，观测值 $X_1, \\dots, X_n$ 是独立同分布的，且 $X_i \\mid \\theta \\sim \\mathrm{Uniform}(0,\\theta)$。\n- 先验：$\\pi(\\theta)$ 在 $(0, b)$ 上为常数，在此之外为零。\n- 使用的基本原理：贝叶斯法则以及独立同分布观测值的似然函数的基本形式。\n- 任务：\n  - 使用贝叶斯法则推导后验密度 $\\pi(\\theta \\mid x_{1:n})$。\n  - 证明 $\\pi(\\theta \\mid x_{1:n})$ 仅通过样本最大值 $X_{(n)} = \\max_i X_i$ 依赖于数据。\n  - 设计并论证一种从 $\\pi(\\theta \\mid x_{1:n})$ 生成独立同分布样本的方法，该方法通过一个解析推导的逆累积分布函数来变换独立的 $\\mathrm{Uniform}(0,1)$ 变量。您的方法必须是精确的（无马尔可夫链蒙特卡洛）。\n  - 使用您的采样器，生成 $S$ 个独立同分布的后验抽样，使用经验后验均值 $\\hat{\\mu}$ 和经验后验标准差 $\\hat{\\sigma}$ 将它们标准化为零均值和单位方差，即计算 $Z_j = (\\Theta_j - \\hat{\\mu})/\\hat{\\sigma}$，并计算以下两者：\n    - 标准化三阶中心矩（偏度），定义为 $Z_j^3$ 的经验平均值；\n    - $\\{Z_j\\}$ 的经验累积分布函数与标准正态累积分布函数之间的柯尔莫哥洛夫–斯米尔诺夫上确界范数距离，即 $\\sup_z \\lvert \\hat{F}_S(z) - \\Phi(z) \\rvert$。\n  - 在您的解决方案中解释为什么该模型是后验渐近正态性的一个反例，以及为什么即使对于较大的 $n$，上述两个诊断指标仍然远离其高斯理想值。\n\n2) 正则基线模型：\n- 数据模型：对于参数 $\\theta \\in \\mathbb{R}$，观测值 $Y_1, \\dots, Y_n$ 是独立同分布的，且 $Y_i \\mid \\theta \\sim \\mathcal{N}(\\theta, 1)$。\n- 先验：$\\theta \\sim \\mathcal{N}(0, \\tau^2)$，其中 $\\tau^2$ 是一个很大但有限的值。\n- 使用的基本原理：用于高斯似然的贝叶斯法则与共轭高斯先验。\n- 任务：\n  - 推导后验 $\\pi(\\theta \\mid y_{1:n})$。\n  - 以闭合形式从该后验生成 $S$ 个独立同分布的抽样。\n  - 使用经验后验均值和经验后验标准差将样本标准化为零均值和单位方差，并计算与上文相同的两个诊断指标。\n\n需计算并返回的诊断指标：\n- 对于下述每个测试用例，返回两个数字：\n  - 标准化后验抽样的经验偏度（标准化三阶中心矩）；\n  - 标准化后与标准正态累积分布函数的柯尔莫哥洛夫–斯米尔诺夫上确界范数距离。\n\n实现细节：\n- 仅使用下面提供的测试套件。对于所有随机模拟，将伪随机数生成器种子设置为 $20231011$ 以确保可复现性。\n- 对于所有后验模拟，使用 $S = 120000$ 个独立同分布的抽样。\n- 对于柯尔莫哥洛夫–斯米尔诺夫距离，通过在排序后的样本点上比较经验累积分布函数与标准正态累积分布函数，来计算标准化抽样上的 $\\sup_z \\lvert \\hat{F}_S(z) - \\Phi(z) \\rvert$。\n\n测试套件：\n- 案例 1（非正则，大 $n$）：$\\theta^\\star = 1.0$, $n = 1000$, $b = 10.0$。\n- 案例 2（非正则，更大 $n$）：$\\theta^\\star = 1.0$, $n = 10000$, $b = 10.0$。\n- 案例 3（正则基线）：$\\theta^\\star = 1.0$, $n = 1000$, $\\tau^2 = 10^6$。\n\n数据生成：\n- 对于案例 1 和 2，独立同分布地生成数据 $X_i \\sim \\mathrm{Uniform}(0, \\theta^\\star)$；基于实现的 $X_{(n)}$ 形成后验。\n- 对于案例 3，独立同分布地生成数据 $Y_i \\sim \\mathcal{N}(\\theta^\\star, 1)$；基于实现的数据形成后验。\n\n最终输出格式：\n- 您的程序应生成一行输出，其中包含三个测试用例的结果，形式为一个由三个数对组成的逗号分隔列表，每个数对的数字格式化为小数点后六位，并用方括号括起来。具体来说，打印一行形式如下：\n  - [[skew1,ks1],[skew2,ks2],[skew3,ks3]]\n- 不应打印任何额外文本。\n\n角度和物理单位：\n- 本问题中没有物理单位或角度。\n\n答案类型：\n- 每个报告的数字都是一个实数（浮点数）。最终输出是一个列表，其中包含三个各含两个浮点数的列表，如上所述。\n\n您的程序必须是自包含的，且不得要求任何用户输入。它必须遵守最终答案部分中指定的执行环境。",
            "solution": "目标是分析两种不同模型的后验分布的渐近行为：一个是非正则模型，其数据的支撑集依赖于参数；另一个是标准的正则模型。我们将推导后验分布，实现精确采样方法，并计算诊断指标（偏度和柯尔莫哥洛夫-斯米尔诺夫距离），以检验正则模型的伯恩斯坦–冯·米塞斯（BvM）定理所预测的渐近正态性。\n\n### 模型 1：非正则均匀模型\n\n该模型是 BvM 定理的一个典型反例。\n\n**1. 数据设定与先验**\n- 数据模型：$X_1, \\dots, X_n$ 是从 $\\mathrm{Uniform}(0,\\theta)$ 分布中抽取的独立同分布（i.i.d.）样本，即 $X_i \\mid \\theta \\sim \\mathcal{U}(0,\\theta)$。参数 $\\theta$ 未知，但被限制在区间 $(0, b)$ 内，其中 $b > 0$ 是一个已知常数。\n- 先验分布：为 $\\theta$ 选择一个无信息先验，它在其支撑集上是常数：$\\pi(\\theta) \\propto \\mathbb{I}(0  \\theta  b)$。这是一个 $\\mathrm{Uniform}(0,b)$ 分布，因此先验密度为 $\\pi(\\theta) = \\frac{1}{b} \\mathbb{I}(0  \\theta  b)$，其中 $\\mathbb{I}(\\cdot)$ 是指示函数。\n\n**2. 后验分布的推导**\n我们使用贝叶斯法则，即后验密度正比于似然与先验的乘积：\n$$\n\\pi(\\theta \\mid x_{1:n}) \\propto p(x_{1:n} \\mid \\theta) \\pi(\\theta)\n$$\n对于 i.i.d. 观测值，似然函数 $p(x_{1:n} \\mid \\theta)$ 是各个密度的乘积：\n$$\np(x_{1:n} \\mid \\theta) = \\prod_{i=1}^n p(x_i \\mid \\theta) = \\prod_{i=1}^n \\left( \\frac{1}{\\theta} \\mathbb{I}(0  x_i  \\theta) \\right)\n$$\n指示函数之积 $\\prod_{i=1}^n \\mathbb{I}(0  x_i  \\theta)$ 等于 $1$ 当且仅当所有 $x_i$ 都小于 $\\theta$。这个条件可以用样本最大值 $X_{(n)} = \\max\\{X_1, \\dots, X_n\\}$ 来简洁地表示。该条件成立当且仅当 $X_{(n)}  \\theta$。我们假设所有 $x_i  0$。因此，似然函数为：\n$$\np(x_{1:n} \\mid \\theta) = \\left(\\frac{1}{\\theta}\\right)^n \\mathbb{I}(X_{(n)}  \\theta) = \\theta^{-n} \\mathbb{I}(\\theta > X_{(n)})\n$$\n如上所示，似然函数，并因此后验分布，仅通过单一的充分统计量 $X_{(n)}$ 依赖于数据 $x_{1:n}$。\n\n结合似然和先验，未归一化的后验是：\n$$\n\\pi(\\theta \\mid x_{1:n}) \\propto \\left( \\theta^{-n} \\mathbb{I}(\\theta > X_{(n)}) \\right) \\times \\left( \\frac{1}{b} \\mathbb{I}(0  \\theta  b) \\right)\n$$\n这可以简化为：\n$$\n\\pi(\\theta \\mid x_{1:n}) \\propto \\theta^{-n} \\mathbb{I}(X_{(n)}  \\theta  b)\n$$\n为了找到归一化的后验密度，我们计算归一化常数 $C$：\n$$\nC = \\int_{X_{(n)}}^b \\theta^{-n} d\\theta = \\left[ \\frac{\\theta^{-n+1}}{-n+1} \\right]_{X_{(n)}}^b = \\frac{1}{n-1} \\left( X_{(n)}^{-n+1} - b^{-n+1} \\right) \\quad (\\text{对于 } n1)\n$$\n确切的后验密度是一个截断帕累托分布：\n$$\n\\pi(\\theta \\mid x_{1:n}) = \\frac{\\theta^{-n}}{C} = \\frac{(n-1)\\theta^{-n}}{X_{(n)}^{-n+1} - b^{-n+1}} \\mathbb{I}(X_{(n)}  \\theta  b)\n$$\n\n**3. 逆 CDF 采样法**\n为了从该后验生成精确的 i.i.d. 抽样，我们使用逆变换采样法。首先，我们推导后验累积分布函数（CDF），$F(\\theta_0) = P(\\theta \\le \\theta_0 \\mid x_{1:n})$：\n$$\nF(\\theta_0) = \\int_{X_{(n)}}^{\\theta_0} \\pi(\\theta \\mid x_{1:n}) d\\theta = \\frac{1}{C} \\int_{X_{(n)}}^{\\theta_0} \\theta^{-n} d\\theta = \\frac{\\frac{1}{n-1}(X_{(n)}^{-n+1} - \\theta_0^{-n+1})}{\\frac{1}{n-1}(X_{(n)}^{-n+1} - b^{-n+1})} = \\frac{X_{(n)}^{-n+1} - \\theta_0^{-n+1}}{X_{(n)}^{-n+1} - b^{-n+1}}\n$$\n对于 $\\theta_0 \\in [X_{(n)}, b]$。我们令 $F(\\theta_0) = u$，其中 $u \\sim \\mathcal{U}(0,1)$，然后解出 $\\theta_0$：\n$$\nu = \\frac{X_{(n)}^{-n+1} - \\theta_0^{-n+1}}{X_{(n)}^{-n+1} - b^{-n+1}} \\implies u(X_{(n)}^{-n+1} - b^{-n+1}) = X_{(n)}^{-n+1} - \\theta_0^{-n+1}\n$$\n$$\n\\theta_0^{-n+1} = X_{(n)}^{-n+1} - u(X_{(n)}^{-n+1} - b^{-n+1}) = (1-u)X_{(n)}^{-n+1} + u b^{-n+1}\n$$\n$$\n\\theta_0 = \\left( (1-u)X_{(n)}^{-n+1} + u b^{-n+1} \\right)^{\\frac{1}{1-n}}\n$$\n这就是逆 CDF，$F^{-1}(u)$。一种用于计算的数值稳定形式是通过提出 $X_{(n)}$ 因子得到的：\n$$\n\\theta_0 = X_{(n)} \\left( 1 - u \\left(1 - \\left(\\frac{X_{(n)}}{b}\\right)^{n-1}\\right) \\right)^{\\frac{-1}{n-1}}\n$$\n生成一个均匀随机变量 $u$ 并应用此变换，可以得到一个来自后验的精确抽样。\n\n**4. 伯恩斯坦–冯·米塞斯定理的失效**\nBvM 定理指出，在某些“正则性条件”下，随着样本量 $n \\to \\infty$，后验分布在形状上收敛于高斯分布。一个关键的正则性条件是数据分布 $p(x|\\theta)$ 的支撑集不能依赖于参数 $\\theta$。在我们的模型中，支撑集是 $(0, \\theta)$，这违反了此条件。\n\n因此，后验不会变成高斯分布。当 $n \\to \\infty$ 时，$X_{(n)}$ 收敛到真实参数值 $\\theta^{\\star}$，而项 $b^{-n+1}$（其中 $b > X_{(n)}$）比 $X_{(n)}^{-n+1}$ 快得多地趋向于零。后验分布越来越集中在 $X_{(n)}$ 的正上方。密度 $\\pi(\\theta | x_{1:n}) \\propto \\theta^{-n}$ 在其支撑集的下边界 $\\theta=X_{(n)}$ 处达到最大值，并急剧下降。这种形状是高度不对称的，类似于一个反向（且截断的）幂律分布。随着 $n$ 的增加，这种不对称性不会消失。对后验进行适当的重新缩放，例如 $n( \\theta - X_{(n)} )$，可以证明它会收敛于一个指数分布，而非高斯分布。因此，旨在测量高斯性的诊断指标，如偏度（对于高斯分布为 $0$）和与正态 CDF 的 K-S 距离，将不会收敛到它们的理想值。它们反而会收敛到这个非高斯极限形状所特有的值，即使对于非常大的 $n$ 也仍然远离高斯理想值。\n\n### 模型 2：正则高斯模型\n\n该模型满足 BvM 正则性条件，可作为比较的基线。\n\n**1. 数据设定与先验**\n- 数据模型：$Y_1, \\dots, Y_n$ 是从 $\\mathcal{N}(\\theta, 1)$ 分布中抽取的 i.i.d. 样本。\n- 先验分布：高斯分布均值的共轭先验是高斯分布。我们使用 $\\theta \\sim \\mathcal{N}(0, \\tau^2)$，其中 $\\tau^2$ 有较大的方差，使其成为弱信息性先验。\n\n**2. 后验分布的推导**\n我们再次使用贝叶斯法则：$\\pi(\\theta \\mid y_{1:n}) \\propto p(y_{1:n} \\mid \\theta) \\pi(\\theta)$。\n- 似然是：$p(y_{1:n} \\mid \\theta) \\propto \\exp\\left(-\\frac{1}{2} \\sum_{i=1}^n (y_i - \\theta)^2 \\right)$。\n- 先验是：$\\pi(\\theta) \\propto \\exp\\left(-\\frac{\\theta^2}{2\\tau^2}\\right)$。\n后验正比于二者之积：\n$$\n\\pi(\\theta \\mid y_{1:n}) \\propto \\exp\\left(-\\frac{1}{2} \\sum_{i=1}^n (y_i - \\theta)^2 - \\frac{\\theta^2}{2\\tau^2}\\right)\n$$\n指数中的项是关于 $\\theta$ 的二次型：$\\sum(y_i - \\theta)^2 + \\frac{\\theta^2}{\\tau^2} = \\sum y_i^2 - 2\\theta \\sum y_i + n\\theta^2 + \\frac{\\theta^2}{\\tau^2}$。忽略不含 $\\theta$ 的项，我们关注于：\n$$\n-\\frac{1}{2} \\left[ \\left(n + \\frac{1}{\\tau^2}\\right)\\theta^2 - 2(n\\bar{y})\\theta \\right]\n$$\n其中 $\\bar{y} = \\frac{1}{n}\\sum y_i$。对 $\\theta$ 进行配方，表明这是高斯密度的核。$\\theta$ 的后验分布也是一个高斯分布，即 $\\mathcal{N}(\\mu_n, \\sigma_n^2)$，其参数为：\n$$\n\\sigma_n^2 = \\left(n + \\frac{1}{\\tau^2}\\right)^{-1} \\qquad \\mu_n = \\sigma_n^2 (n\\bar{y}) = \\frac{n\\bar{y}}{n + 1/\\tau^2}\n$$\n从这个后验进行采样很简单：只需从具有计算出的均值 $\\mu_n$ 和方差 $\\sigma_n^2$ 的正态分布中生成抽样即可。由于这个后验本身就是完美的高斯分布，生成样本，将其标准化，并与标准正态分布进行比较，应该会得到一个非常接近 $0$ 的偏度和一个非常小的 K-S 距离，其误差仅受限于有限后验抽样数量 $S$ 带来的蒙特卡洛误差。\n\n### 诊断计算\n\n对于两个模型，在生成 $S$ 个后验抽样 $\\{\\Theta_j\\}_{j=1}^S$ 后，我们计算：\n1.  经验均值 $\\hat{\\mu} = \\frac{1}{S} \\sum_{j=1}^S \\Theta_j$ 和标准差 $\\hat{\\sigma} = \\sqrt{\\frac{1}{S-1} \\sum_{j=1}^S (\\Theta_j - \\hat{\\mu})^2}$。\n2.  标准化抽样 $Z_j = (\\Theta_j - \\hat{\\mu})/\\hat{\\sigma}$。\n3.  **偏度**：标准化三阶中心矩，$\\mathrm{Skew} = \\frac{1}{S} \\sum_{j=1}^S Z_j^3$。\n4.  **柯尔莫哥洛夫-斯米尔诺夫距离**：$D_S = \\sup_z \\lvert \\hat{F}_S(z) - \\Phi(z) \\rvert$，其中 $\\hat{F}_S$ 是 $\\{Z_j\\}$ 的经验 CDF，$\\Phi$ 是标准正态 CDF。这通过在排序后的样本点上找到最大偏差来计算。",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the problem of diagnosing the failure of the Bernstein–von Mises theorem\n    by comparing a nonregular Uniform model with a regular Gaussian model.\n    \"\"\"\n    RNG_SEED = 20231011\n    S = 120000  # Number of posterior draws\n\n    rng = np.random.default_rng(RNG_SEED)\n\n    test_cases = [\n        {'model': 'nonregular', 'theta_star': 1.0, 'n': 1000, 'b': 10.0},\n        {'model': 'nonregular', 'theta_star': 1.0, 'n': 10000, 'b': 10.0},\n        {'model': 'regular', 'theta_star': 1.0, 'n': 1000, 'tau_sq': 1e6}\n    ]\n\n    results = []\n\n    for case in test_cases:\n        if case['model'] == 'nonregular':\n            # Parameters for the nonregular case\n            theta_star = case['theta_star']\n            n = case['n']\n            b = case['b']\n\n            # 1. Generate data\n            # X_i ~ Uniform(0, theta_star)\n            data = rng.uniform(0, theta_star, n)\n            x_n_max = np.max(data)\n\n            # 2. Generate samples from the posterior using inverse CDF sampling\n            u = rng.uniform(0, 1, S)\n            \n            # Numerically stable inverse CDF formula:\n            # theta = x_n_max * (1 - u * (1 - (x_n_max/b)**(n-1)))**(-1/(n-1))\n            power_term = (x_n_max / b)**(n - 1)\n            base = 1 - u * (1 - power_term)\n            exponent = -1 / (n - 1)\n            theta_samples = x_n_max * (base**exponent)\n\n        elif case['model'] == 'regular':\n            # Parameters for the regular case\n            theta_star = case['theta_star']\n            n = case['n']\n            tau_sq = case['tau_sq']\n\n            # 1. Generate data\n            # Y_i ~ Normal(theta_star, 1)\n            data = rng.normal(theta_star, 1, n)\n            y_bar = np.mean(data)\n\n            # 2. Calculate posterior parameters (Normal-Normal conjugate model)\n            post_var = 1 / (n + 1 / tau_sq)\n            post_mean = post_var * (n * y_bar)\n            post_std = np.sqrt(post_var)\n\n            # 3. Generate samples from the posterior\n            theta_samples = rng.normal(post_mean, post_std, S)\n\n        # 4. Compute diagnostics for all cases\n        # Standardize samples to zero mean and unit variance\n        mu_hat = np.mean(theta_samples)\n        # Using ddof=0 for population standard deviation of the sample\n        sigma_hat = np.std(theta_samples) \n        z_samples = (theta_samples - mu_hat) / sigma_hat\n\n        # Compute empirical skewness (standardized third central moment)\n        skewness = np.mean(z_samples**3)\n\n        # Compute Kolmogorov-Smirnov distance\n        z_sorted = np.sort(z_samples)\n        \n        # Empirical CDF values at each sorted sample point\n        ecdf = np.arange(1, S + 1) / S\n        \n        # Standard normal CDF values at the same points\n        norm_cdf_vals = norm.cdf(z_sorted)\n\n        # The KS statistic is the max difference between ECDF and the true CDF.\n        # The difference can be maximal just before or at the ECDF jump points.\n        dist1 = np.abs(ecdf - norm_cdf_vals)\n        dist2 = np.abs((ecdf - 1/S) - norm_cdf_vals)\n        ks_dist = np.max(np.maximum(dist1, dist2))\n\n        results.append([skewness, ks_dist])\n\n    # Format the final output as specified\n    formatted_results = [f\"[{s:.6f},{k:.6f}]\" for s, k in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "具有多个独立峰值的多峰后验分布对标准的 MCMC 采样器构成了巨大挑战，因为采样器很容易被困在单个模式中。本练习引入了“能量景观”的概念，用以量化模式之间的“能量壁垒”。你将通过分析推导，探索一种广受欢迎的 MCMC 改进技术——温度调整（tempering）——是如何有效降低这些壁垒，从而促进采样器在不同模式之间移动的。",
            "id": "3289096",
            "problem": "考虑一个由非高斯先验和高斯似然引起具有双阱结构的一维贝叶斯后验。设先验由密度 $p(\\theta) \\propto \\exp\\left(-\\lambda(\\theta^2 - b^2)^2\\right)$ 定义，其中参数 $\\lambda  0$ 且 $b  0$。设观测数据 $y$ 的似然为 $p(y \\mid \\theta) = \\mathcal{N}(y; \\theta, s^2)$，其中方差 $s^2  0$ 已知。对于本问题，固定观测数据为 $y = 0$。则未归一化的后验为 $p(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)$，相关的能量函数为 $U(\\theta; y) = -\\log p(\\theta \\mid y)$，忽略不依赖于 $\\theta$ 的加性常数。\n\n定义温度为 $T \\geq 1$ 时的退火后验为 $p_T(\\theta \\mid y) \\propto p(\\theta \\mid y)^{1/T}$，这对应于将能量按因子 $1/T$ 进行缩放，即 $U_T(\\theta; y) = U(\\theta; y)/T$。在马尔可夫链蒙特卡洛（MCMC）中，退火是一种改善跨模态混合的方法。在本问题中，你必须使用 $-\\log p(\\theta \\mid y)$ 通过能量势垒来量化多模态性，并计算退火如何减少双阱后验中模态间的期望首达时间。\n\n你的任务是：\n- 从后验 $p(\\theta \\mid y)$、能量 $U(\\theta; y)$ 和退火后验 $p_T(\\theta \\mid y)$ 的定义出发，当双阱结构存在时，识别出 $U(\\theta; y)$ 的两个对称极小值点和中心鞍点（局部极大值点）的位置。建立关于 $\\lambda$、$b$ 和 $s$ 的条件，使得双阱结构存在，并用 $\\lambda$、$b$ 和 $s$ 确定极小值点的确切位置。\n- 将能量势垒高度 $\\Delta E$ 定义为 $U(\\theta_{\\text{saddle}}; y) - U(\\theta_{\\text{min}}; y)$ 的差值，其中 $\\theta_{\\text{saddle}}$ 是中心鞍点，$\\theta_{\\text{min}}$ 是两个对称极小值点中的任意一个。为给定的模型参数推导 $\\Delta E$ 的闭式表达式。\n- 使用基于原理的一维过阻尼动力学中稀有事件跃迁的近似（例如 Eyring-Kramers 原理），论证在温度 $T$ 下两个模态之间的期望首达时间 $\\tau(T)$ 主要由一个依赖于势垒高度和温度的指数因子决定。推导由 $R(T) = \\tau(T)/\\tau(1)$ 定义的缩减因子 $R(T)$，并将其完全用 $\\Delta E$ 和 $T$ 表示。\n- 所有能量必须以与自然对数一致的无量纲单位报告（即与 $-\\log$ 密度相关的能量单位），所有时间也必须以无量纲单位报告。你必须为势垒高度 $\\Delta E$ 和缩减因子 $R(T)$ 生成数值输出。\n\n实现一个程序，对下面测试套件中的每个参数元组 $(\\lambda, b, s, T)$ 计算：\n- 势垒高度 $\\Delta E$。\n- 缩减因子 $R(T) = \\tau(T)/\\tau(1)$。\n\n如果给定的参数元组不满足双阱条件，则为该元组返回对 $[\\text{nan}, \\text{nan}]$。最终输出必须是单行，包含一个逗号分隔的浮点数列表的列表，每个内部列表对应一个测试用例，按下面给出的顺序排列，并用方括号括起来。例如，输出形式应为 $[[\\Delta E_1, R(T)_1],[\\Delta E_2, R(T)_2],\\ldots]$。\n\n使用以下参数值测试套件来测试解的不同方面：\n- 分离良好的常规模态（正常路径）：$(\\lambda, b, s, T) = (1.0, 2.0, 1.0, 1.0)$。\n- 中度分离模态与非平凡退火：$(\\lambda, b, s, T) = (1.5, 1.5, 0.8, 2.0)$。\n- 宽似然与浅阱，强退火：$(\\lambda, b, s, T) = (0.8, 1.2, 1.5, 4.0)$。\n- 接近双阱存在边界：$(\\lambda, b, s, T) = (1.0, 0.6, 1.0, 3.0)$。\n- 深阱与极高温度退火：$(\\lambda, b, s, T) = (2.0, 1.8, 0.7, 10.0)$。\n\n覆盖性设计：\n- 第一个案例检查 $T = 1$ 时的基线情况，此时退火不改变首达时间。\n- 第二和第三个案例探讨了对于不同的势垒高度，增加 $T$ 如何减少首达时间。\n- 第四个案例检验了在多模态性消失附近的表现，以测试近似的稳定性。\n- 第五个案例探测了极端退火机制。\n\n你的程序应生成单行输出，其中包含一个逗号分隔的列表，该列表用方括号括起，作为结果（例如 $[[\\Delta E_1,R_1],[\\Delta E_2,R_2],[\\Delta E_3,R_3],[\\Delta E_4,R_4],[\\Delta E_5,R_5]]$）。",
            "solution": "该问题已经过验证，被确定为计算统计学中一个适定、有科学依据的问题。它内容自洽，其前提和目标清晰、一致且可形式化。因此，我们可以着手提供完整解答。\n\n解答分为四个步骤：首先，我们从后验分布推导能量函数 $U(\\theta)$；其次，我们确定能量函数的临界点，并建立双阱势的条件；第三，我们推导能量势垒 $\\Delta E$ 的闭式表达式；第四，我们基于一个已有的稀有事件动力学近似，推导首达时间缩减因子 $R(T)$。\n\n### 步骤 1：能量函数的推导\n\n未归一化的后验分布 $p(\\theta \\mid y)$ 与先验 $p(\\theta)$ 和似然 $p(y \\mid \\theta)$ 的乘积成正比。\n先验密度由 $p(\\theta) \\propto \\exp\\left(-\\lambda(\\theta^2 - b^2)^2\\right)$ 给出，其中 $\\lambda  0$ 且 $b  0$。\n似然是一个高斯分布，$p(y \\mid \\theta) = \\mathcal{N}(y; \\theta, s^2)$，其方差 $s^2  0$ 已知。对于给定的数据 $y=0$，似然变为：\n$$\np(y=0 \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi s^2}} \\exp\\left(-\\frac{(0 - \\theta)^2}{2s^2}\\right) \\propto \\exp\\left(-\\frac{\\theta^2}{2s^2}\\right)\n$$\n那么，未归一化的后验为：\n$$\np(\\theta \\mid y=0) \\propto p(y=0 \\mid \\theta) p(\\theta) \\propto \\exp\\left(-\\frac{\\theta^2}{2s^2}\\right) \\exp\\left(-\\lambda(\\theta^2 - b^2)^2\\right)\n$$\n$$\np(\\theta \\mid y=0) \\propto \\exp\\left( -\\left[ \\frac{\\theta^2}{2s^2} + \\lambda(\\theta^2 - b^2)^2 \\right] \\right)\n$$\n能量函数 $U(\\theta; y)$ 定义为 $U(\\theta; y) = -\\log p(\\theta \\mid y)$，忽略任何不依赖于 $\\theta$ 的加性常数。因此，设 $y=0$ 并省略显式依赖关系符号，我们得到：\n$$\nU(\\theta) = \\frac{\\theta^2}{2s^2} + \\lambda(\\theta^2 - b^2)^2\n$$\n展开四次项，我们可以将能量函数重写为：\n$$\nU(\\theta) = \\frac{\\theta^2}{2s^2} + \\lambda(\\theta^4 - 2b^2\\theta^2 + b^4) = \\lambda\\theta^4 + \\left(\\frac{1}{2s^2} - 2\\lambda b^2\\right)\\theta^2 + \\lambda b^4\n$$\n\n### 步骤 2：临界点与双阱条件\n\n能量函数 $U(\\theta)$ 的临界点（极小值、极大值和鞍点）在其关于 $\\theta$ 的一阶导数为零处找到。\n$$\n\\frac{dU}{d\\theta} = \\frac{d}{d\\theta} \\left[ \\frac{\\theta^2}{2s^2} + \\lambda(\\theta^2 - b^2)^2 \\right] = \\frac{2\\theta}{2s^2} + \\lambda \\cdot 2(\\theta^2 - b^2) \\cdot (2\\theta)\n$$\n$$\n\\frac{dU}{d\\theta} = \\frac{\\theta}{s^2} + 4\\lambda\\theta(\\theta^2 - b^2) = \\theta \\left[ \\frac{1}{s^2} + 4\\lambda(\\theta^2 - b^2) \\right] = 0\n$$\n$$\n\\theta \\left[ 4\\lambda\\theta^2 + \\frac{1}{s^2} - 4\\lambda b^2 \\right] = 0\n$$\n此方程产生三个临界点：\n1.  一个临界点在 $\\theta = 0$。\n2.  另外两个通过求解 $4\\lambda\\theta^2 + \\frac{1}{s^2} - 4\\lambda b^2 = 0$ 得到，这给出 $\\theta^2 = b^2 - \\frac{1}{4\\lambda s^2}$。这些解是实数，当且仅当 $b^2  \\frac{1}{4\\lambda s^2}$。\n\n为了对这些点进行分类，我们考察二阶导数 $U''(\\theta)$：\n$$\nU''(\\theta) = \\frac{d}{d\\theta} \\left[ 4\\lambda\\theta^3 + \\left(\\frac{1}{s^2} - 4\\lambda b^2\\right)\\theta \\right] = 12\\lambda\\theta^2 + \\frac{1}{s^2} - 4\\lambda b^2\n$$\n在 $\\theta = 0$ 处，二阶导数为 $U''(0) = \\frac{1}{s^2} - 4\\lambda b^2$。为了使 $\\theta=0$ 成为一个局部极大值点（代表阱间势垒的鞍点），我们必须有 $U''(0)  0$。这给出了条件：\n$$\n\\frac{1}{s^2}  4\\lambda b^2 \\implies \\boldsymbol{4\\lambda b^2 s^2 > 1}\n$$\n这就是存在双阱势的条件。它与另外两个临界点为实数的条件相同。当此条件成立时，$\\theta_{\\text{saddle}} = 0$。\n\n现在我们对另外两个临界点 $\\theta^2 = b^2 - \\frac{1}{4\\lambda s^2}$ 进行分类。\n$$\nU''\\left(\\theta^2 = b^2 - \\frac{1}{4\\lambda s^2}\\right) = 12\\lambda\\left(b^2 - \\frac{1}{4\\lambda s^2}\\right) + \\frac{1}{s^2} - 4\\lambda b^2 = 12\\lambda b^2 - \\frac{3}{s^2} + \\frac{1}{s^2} - 4\\lambda b^2\n$$\n$$\n= 8\\lambda b^2 - \\frac{2}{s^2} = 2\\left(4\\lambda b^2 - \\frac{1}{s^2}\\right)\n$$\n根据双阱条件 $4\\lambda b^2 s^2 > 1$，我们有 $4\\lambda b^2 > 1/s^2$，这意味着 $4\\lambda b^2 - 1/s^2 > 0$。因此，在这些点上二阶导数为正，证实了它们是局部极小值点。这些极小值点的位置是：\n$$\n\\theta_{\\text{min}} = \\pm \\sqrt{b^2 - \\frac{1}{4\\lambda s^2}}\n$$\n\n### 步骤 3：能量势垒 $\\Delta E$ 的推导\n\n能量势垒 $\\Delta E$ 定义为鞍点处的能量与极小值点处的能量之差：$\\Delta E = U(\\theta_{\\text{saddle}}) - U(\\theta_{\\text{min}})$。\n鞍点 $\\theta_{\\text{saddle}} = 0$ 处的能量为：\n$$\nU(0) = \\frac{0^2}{2s^2} + \\lambda(0^2 - b^2)^2 = \\lambda b^4\n$$\n极小值点处的能量在 $\\theta_{\\text{min}}^2 = b^2 - \\frac{1}{4\\lambda s^2}$ 处计算：\n$$\nU(\\theta_{\\text{min}}) = \\frac{\\theta_{\\text{min}}^2}{2s^2} + \\lambda(\\theta_{\\text{min}}^2 - b^2)^2\n$$\n$$\nU(\\theta_{\\text{min}}) = \\frac{1}{2s^2}\\left(b^2 - \\frac{1}{4\\lambda s^2}\\right) + \\lambda\\left(\\left(b^2 - \\frac{1}{4\\lambda s^2}\\right) - b^2\\right)^2\n$$\n$$\nU(\\theta_{\\text{min}}) = \\frac{b^2}{2s^2} - \\frac{1}{8\\lambda s^4} + \\lambda\\left(-\\frac{1}{4\\lambda s^2}\\right)^2 = \\frac{b^2}{2s^2} - \\frac{1}{8\\lambda s^4} + \\frac{1}{16\\lambda s^4} = \\frac{b^2}{2s^2} - \\frac{1}{16\\lambda s^4}\n$$\n现在，我们计算能量势垒 $\\Delta E$：\n$$\n\\Delta E = U(0) - U(\\theta_{\\text{min}}) = \\lambda b^4 - \\left(\\frac{b^2}{2s^2} - \\frac{1}{16\\lambda s^4}\\right) = \\lambda b^4 - \\frac{b^2}{2s^2} + \\frac{1}{16\\lambda s^4}\n$$\n这个表达式是一个完全平方：\n$$\n\\Delta E = \\lambda \\left(b^4 - \\frac{b^2}{2\\lambda s^2} + \\frac{1}{16\\lambda^2 s^4}\\right) = \\lambda \\left( b^2 - \\frac{1}{4\\lambda s^2} \\right)^2\n$$\n这就是能量势垒的最终闭式表达式。\n\n### 步骤 4：首达时间缩减因子 $R(T)$ 的推导\n\n在 MCMC 和统计物理学的背景下，从退火后验 $p_T(\\theta) \\propto p(\\theta)^{1/T} \\propto \\exp(-U(\\theta)/T)$ 进行采样等效于在有效温度 $T$ 下（单位中玻尔兹曼常数为 1）模拟一个具有能量势 $U(\\theta)$ 的物理系统。\n根据过阻尼动力学的 Eyring-Kramers 原理，两个能量极小值点之间的平均跃迁时间（期望首达时间 $\\tau$）主要由一个与能量势垒 $\\Delta E$ 和温度相关的指数项决定。对于一个能量为 $U(\\theta)$、温度为 $T$ 的系统，这表示为：\n$$\n\\tau(T) \\propto \\exp\\left(\\frac{\\Delta E}{T}\\right)\n$$\n指数前因子取决于势在极小值点和鞍点处的曲率，但对于高势垒，指数项占绝对主导地位。本问题指导我们关注这个主导因子。\n缩减因子 $R(T)$ 定义为温度 $T$ 时的首达时间与温度 $T=1$ 时的首达时间之比：\n$$\nR(T) = \\frac{\\tau(T)}{\\tau(1)}\n$$\n假设指数前因子对 $T$ 的依赖性与指数项相比很弱或可忽略不计，我们得到：\n$$\nR(T) \\approx \\frac{\\exp(\\Delta E / T)}{\\exp(\\Delta E / 1)} = \\exp\\left(\\frac{\\Delta E}{T} - \\Delta E\\right) = \\exp\\left(\\Delta E \\left(\\frac{1}{T} - 1\\right)\\right)\n$$\n这个 $R(T)$ 的表达式仅依赖于势垒高度 $\\Delta E$ 和温度 $T$，符合要求。\n因此，计算流程如下：对于每个参数集 $(\\lambda, b, s, T)$，首先检查是否 $4\\lambda b^2 s^2 > 1$。如果不满足，则系统不是双模态的，结果为 $[\\text{nan}, \\text{nan}]$。如果条件成立，则使用推导出的公式计算 $\\Delta E$ 和 $R(T)$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the energy barrier and hitting time reduction factor for a double-well posterior.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (lambda, b, s, T)\n        (1.0, 2.0, 1.0, 1.0),   # General well-separated modes\n        (1.5, 1.5, 0.8, 2.0),   # Moderately separated modes with nontrivial tempering\n        (0.8, 1.2, 1.5, 4.0),   # Broad likelihood and mild wells, strong tempering\n        (1.0, 0.6, 1.0, 3.0),   # Near the boundary of double-well existence\n        (2.0, 1.8, 0.7, 10.0),  # Strong wells and very high tempering\n    ]\n\n    results = []\n    for case in test_cases:\n        lam, b, s, T = case\n\n        # Step 1: Check the double-well condition: 4 * lambda * b^2 * s^2 > 1\n        # If this condition is not met, the potential has a single minimum,\n        # and the concept of an energy barrier between modes is not applicable.\n        condition = 4.0 * lam * b**2 * s**2\n        if condition = 1.0:\n            results.append([np.nan, np.nan])\n            continue\n\n        # Step 2: Compute the energy barrier height Delta E.\n        # The derived formula is: Delta_E = lambda * (b^2 - 1 / (4 * lambda * s^2))^2\n        term_in_paren = b**2 - 1.0 / (4.0 * lam * s**2)\n        delta_E = lam * (term_in_paren**2)\n\n        # Step 3: Compute the hitting time reduction factor R(T).\n        # The derived formula is: R(T) = exp(Delta_E * (1/T - 1))\n        # This approximates the ratio of expected hitting times tau(T)/tau(1).\n        if T == 1.0:\n            R_T = 1.0\n        else:\n            R_T = np.exp(delta_E * (1.0 / T - 1.0))\n        \n        results.append([delta_E, R_T])\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"[{de:.8f},{rt:.8f}]\".replace(\"nan\",\"nan\") for de, rt in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```"
        }
    ]
}