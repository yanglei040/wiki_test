{
    "hands_on_practices": [
        {
            "introduction": "Before applying the Chib method in complex simulation-based settings, it is essential to solidify our understanding of its theoretical foundation. This exercise  provides a 'pen-and-paper' practice by asking you to derive the marginal likelihood for a simple conjugate Gaussian model directly from first principles. By confirming that the analytically derived marginal likelihood matches the value obtained from Chib's identity using the closed-form prior, likelihood, and posterior, you will gain a concrete intuition for why the method is valid.",
            "id": "3294504",
            "problem": "Consider a single-observation Gaussian location model with known variance: the likelihood is $y \\mid \\theta \\sim \\mathcal{N}(\\theta, \\sigma^{2})$, and the prior is $\\theta \\sim \\mathcal{N}(\\mu_{0}, \\tau_{0}^{2})$, where $y \\in \\mathbb{R}$ is observed, and $\\mu_{0} \\in \\mathbb{R}$, $\\tau_{0}^{2} \\in (0,\\infty)$, and $\\sigma^{2} \\in (0,\\infty)$ are known constants. \n\nStarting only from first principles, derive the marginal data density $p(y)$ in closed form by integrating out the parameter $\\theta$. Your derivation must proceed from the foundational definition of the marginal data density as the prior predictive distribution and arrive at a simplified analytic density. Then, explain briefly, at the level of principles, how this conjugate calculation illuminates the key identity exploited by Chib’s method for marginal likelihood estimation in nonconjugate models, specifically the role of a fixed evaluation point in the parameter space and the posterior ordinate.\n\nExpress your final answer as a single closed-form analytic expression for $p(y)$ in terms of $y$, $\\mu_{0}$, $\\tau_{0}^{2}$, and $\\sigma^{2}$. No numerical evaluation is required, and no rounding is needed.",
            "solution": "The problem requires the derivation of the marginal data density, $p(y)$, for a Gaussian location model with a conjugate Gaussian prior, and an explanation of how this derivation illuminates Chib's method for marginal likelihood estimation.\n\n**Part 1: Derivation of the Marginal Data Density $p(y)$**\n\nWe are given the following:\n- Likelihood: $p(y \\mid \\theta) = \\mathcal{N}(\\theta, \\sigma^2)$, which is the normal distribution for the data $y$ given the parameter $\\theta$, with known variance $\\sigma^2$.\n- Prior: $p(\\theta) = \\mathcal{N}(\\mu_0, \\tau_0^2)$, which is the normal distribution for the parameter $\\theta$, with known mean $\\mu_0$ and variance $\\tau_0^2$.\n\nThe probability density functions (PDFs) are:\n$$p(y \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - \\theta)^2}{2\\sigma^2}\\right)$$\n$$p(\\theta) = \\frac{1}{\\sqrt{2\\pi\\tau_0^2}} \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right)$$\n\nThe marginal data density, $p(y)$, also known as the prior predictive distribution or evidence, is obtained by integrating the joint distribution $p(y, \\theta) = p(y \\mid \\theta) p(\\theta)$ over all possible values of the parameter $\\theta$:\n$$p(y) = \\int_{-\\infty}^{\\infty} p(y \\mid \\theta) p(\\theta) \\,d\\theta$$\n\nSubstituting the PDFs into the integral:\n$$p(y) = \\int_{-\\infty}^{\\infty} \\left[\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - \\theta)^2}{2\\sigma^2}\\right)\\right] \\left[\\frac{1}{\\sqrt{2\\pi\\tau_0^2}} \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right)\\right] \\,d\\theta$$\n\nWe can combine the constant terms and the exponential functions:\n$$p(y) = \\frac{1}{2\\pi\\sigma\\tau_0} \\int_{-\\infty}^{\\infty} \\exp\\left( -\\frac{1}{2}\\left[ \\frac{(y - \\theta)^2}{\\sigma^2} + \\frac{(\\theta - \\mu_0)^2}{\\tau_0^2} \\right] \\right) \\,d\\theta$$\n\nTo solve the integral, we focus on the argument of the exponential, which is a quadratic function of $\\theta$. We will complete the square with respect to $\\theta$. Let the term in the brackets be $Q(\\theta)$:\n$$Q(\\theta) = \\frac{y^2 - 2y\\theta + \\theta^2}{\\sigma^2} + \\frac{\\theta^2 - 2\\mu_0\\theta + \\mu_0^2}{\\tau_0^2}$$\nGrouping terms by powers of $\\theta$:\n$$Q(\\theta) = \\theta^2 \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2}\\right) - 2\\theta \\left(\\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2}\\right) + \\left(\\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2}\\right)$$\n\nLet us define two new quantities that correspond to the posterior variance, $\\tau_1^2$, and posterior mean, $\\mu_1$, of $\\theta$:\nThe precision of the posterior is the sum of the data precision and prior precision:\n$$\\frac{1}{\\tau_1^2} = \\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2} = \\frac{\\sigma^2 + \\tau_0^2}{\\sigma^2\\tau_0^2} \\implies \\tau_1^2 = \\frac{\\sigma^2\\tau_0^2}{\\sigma^2 + \\tau_0^2}$$\nThe posterior mean is a precision-weighted average of the data and prior mean:\n$$\\mu_1 = \\left(\\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2}\\right) \\bigg/ \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2}\\right) = \\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 + \\tau_0^2}$$\n\nUsing these definitions, we can rewrite $Q(\\theta)$ as:\n$$Q(\\theta) = \\frac{1}{\\tau_1^2}\\theta^2 - \\frac{2\\mu_1}{\\tau_1^2}\\theta + \\left(\\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2}\\right)$$\nCompleting the square for the terms involving $\\theta$:\n$$Q(\\theta) = \\frac{1}{\\tau_1^2}(\\theta^2 - 2\\mu_1\\theta) + \\dots = \\frac{1}{\\tau_1^2}(\\theta - \\mu_1)^2 - \\frac{\\mu_1^2}{\\tau_1^2} + \\left(\\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2}\\right)$$\n$$Q(\\theta) = \\frac{(\\theta - \\mu_1)^2}{\\tau_1^2} + \\left(\\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2} - \\frac{\\mu_1^2}{\\tau_1^2}\\right)$$\nThe integral for $p(y)$ becomes:\n$$p(y) = \\frac{1}{2\\pi\\sigma\\tau_0} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{1}{2}\\left[\\frac{(\\theta - \\mu_1)^2}{\\tau_1^2} + C\\right]\\right) \\,d\\theta$$\nwhere $C = \\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2} - \\frac{\\mu_1^2}{\\tau_1^2}$ is a constant with respect to $\\theta$.\n$$p(y) = \\frac{1}{2\\pi\\sigma\\tau_0} \\exp\\left(-\\frac{C}{2}\\right) \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{(\\theta - \\mu_1)^2}{2\\tau_1^2}\\right) \\,d\\theta$$\nThe integral is the integral of the kernel of a Gaussian PDF, $\\mathcal{N}(\\mu_1, \\tau_1^2)$. Its value is $\\sqrt{2\\pi\\tau_1^2}$.\n$$p(y) = \\frac{\\sqrt{2\\pi\\tau_1^2}}{2\\pi\\sigma\\tau_0} \\exp\\left(-\\frac{C}{2}\\right) = \\frac{\\tau_1}{\\sqrt{2\\pi}\\sigma\\tau_0} \\exp\\left(-\\frac{C}{2}\\right)$$\n\nNow, we simplify the constant factor and the exponent term $C$.\nThe constant factor is:\n$$\\frac{\\tau_1}{\\sqrt{2\\pi}\\sigma\\tau_0} = \\frac{1}{\\sqrt{2\\pi}\\sigma\\tau_0} \\sqrt{\\frac{\\sigma^2\\tau_0^2}{\\sigma^2 + \\tau_0^2}} = \\frac{1}{\\sqrt{2\\pi}\\sigma\\tau_0} \\frac{\\sigma\\tau_0}{\\sqrt{\\sigma^2 + \\tau_0^2}} = \\frac{1}{\\sqrt{2\\pi(\\sigma^2 + \\tau_0^2)}}$$\nThe exponent term $C$ simplifies as:\n$$C = \\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2} - \\frac{1}{\\tau_1^2} \\mu_1^2 = \\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2} - \\left(\\frac{\\sigma^2+\\tau_0^2}{\\sigma^2\\tau_0^2}\\right) \\left(\\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 + \\tau_0^2}\\right)^2$$\n$$C = \\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2} - \\frac{(y\\tau_0^2 + \\mu_0\\sigma^2)^2}{\\sigma^2\\tau_0^2(\\sigma^2 + \\tau_0^2)}$$\nBringing to a common denominator $\\sigma^2\\tau_0^2(\\sigma^2 + \\tau_0^2)$:\n$$C = \\frac{y^2\\tau_0^2(\\sigma^2 + \\tau_0^2) + \\mu_0^2\\sigma^2(\\sigma^2 + \\tau_0^2) - (y^2\\tau_0^4 + 2y\\mu_0\\sigma^2\\tau_0^2 + \\mu_0^2\\sigma^4)}{\\sigma^2\\tau_0^2(\\sigma^2 + \\tau_0^2)}$$\nExpanding and simplifying the numerator:\n$$(y^2\\sigma^2\\tau_0^2 + y^2\\tau_0^4) + (\\mu_0^2\\sigma^4 + \\mu_0^2\\sigma^2\\tau_0^2) - y^2\\tau_0^4 - 2y\\mu_0\\sigma^2\\tau_0^2 - \\mu_0^2\\sigma^4$$\n$$= y^2\\sigma^2\\tau_0^2 - 2y\\mu_0\\sigma^2\\tau_0^2 + \\mu_0^2\\sigma^2\\tau_0^2 = \\sigma^2\\tau_0^2(y^2 - 2y\\mu_0 + \\mu_0^2) = \\sigma^2\\tau_0^2(y - \\mu_0)^2$$\nThus,\n$$C = \\frac{\\sigma^2\\tau_0^2(y - \\mu_0)^2}{\\sigma^2\\tau_0^2(\\sigma^2 + \\tau_0^2)} = \\frac{(y - \\mu_0)^2}{\\sigma^2 + \\tau_0^2}$$\nSubstituting the simplified constant factor and exponent term back into the expression for $p(y)$:\n$$p(y) = \\frac{1}{\\sqrt{2\\pi(\\sigma^2 + \\tau_0^2)}} \\exp\\left(-\\frac{(y - \\mu_0)^2}{2(\\sigma^2 + \\tau_0^2)}\\right)$$\nThis is the PDF of a normal distribution $\\mathcal{N}(\\mu_0, \\sigma^2 + \\tau_0^2)$.\n\n**Part 2: Connection to Chib's Method**\n\nThe derivation above illuminates the principle behind Chib's method for marginal likelihood estimation. The fundamental identity of Bayesian inference relates the posterior, prior, likelihood, and marginal likelihood (evidence):\n$$p(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)}$$\nThis equation can be rearranged to express the marginal likelihood:\n$$p(y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(\\theta \\mid y)}$$\nCrucially, this identity must hold for *any* value of the parameter $\\theta$ in its support. Chib's method exploits this fact by evaluating the identity at a single, fixed point $\\theta^*$:\n$$p(y) = \\frac{p(y \\mid \\theta^*) p(\\theta^*)}{p(\\theta^* \\mid y)}$$\nIn log space, this is $\\ln p(y) = \\ln p(y \\mid \\theta^*) + \\ln p(\\theta^*) - \\ln p(\\theta^* \\mid y)$.\n\nOur conjugate calculation illuminates this principle in several ways:\n1.  **Analytical Verification**: For the conjugate Gaussian model, we have closed-form expressions for all three terms on the right-hand side.\n    - $p(y \\mid \\theta^*)$ is the given likelihood PDF evaluated at $\\theta^*$.\n    - $p(\\theta^*)$ is the given prior PDF evaluated at $\\theta^*$.\n    - $p(\\theta^* \\mid y)$ is the posterior PDF evaluated at $\\theta^*$. Our derivation revealed the posterior is $p(\\theta \\mid y) = \\mathcal{N}(\\mu_1, \\tau_1^2)$, which is fully specified.\n    The analytical calculation of $p(y)$ via integration is equivalent to computing the ratio on the right-hand side. The fact that the result is independent of the choice of $\\theta^*$ demonstrates the validity of the identity. The terms depending on $\\theta^*$ in the numerator (from the joint density) and the denominator (from the posterior ordinate) must cancel perfectly.\n\n2.  **Role of the Fixed Point $\\theta^*$**: The fixed point $\\theta^*$ is an arbitrary point chosen for computational convenience. In a non-conjugate model where the integral for $p(y)$ is intractable, one typically has MCMC samples that approximate the posterior distribution $p(\\theta \\mid y)$, but not its normalizing constant $p(y)$. Chib's method uses these samples to construct an estimate of the denominator, $p(\\theta^* \\mid y)$ (the posterior ordinate). The numerator terms, $p(y \\mid \\theta^*)$ and $p(\\theta^*)$, are usually trivial to compute directly from their functional forms.\n\n3.  **Role of the Posterior Ordinate**: The term $p(\\theta^* \\mid y)$ is the key quantity to be estimated. In our conjugate example, its value is known analytically. In non-conjugate MCMC settings, estimating this density ordinate from a set of samples is the main challenge that Chib's method and its variants address, often using techniques like kernel density estimation or Rao-Blackwellization.\n\nIn summary, the conjugate case provides a proof of principle. It demonstrates that the marginal likelihood $p(y)$ is precisely the normalization constant that relates the joint density $p(y, \\theta)$ to the posterior density $p(\\theta \\mid y)$. Chib's method is a numerical strategy to compute this constant by rearranging the definition of the posterior and estimating the only unknown term, the posterior ordinate, via simulation.",
            "answer": "$$\\boxed{\\frac{1}{\\sqrt{2\\pi(\\sigma^{2} + \\tau_{0}^{2})}} \\exp\\left(-\\frac{(y - \\mu_{0})^{2}}{2(\\sigma^{2} + \\tau_{0}^{2})}\\right)}$$"
        },
        {
            "introduction": "Moving from theory to computation, this practice  guides you through the canonical application of the Chib method for models amenable to Gibbs sampling. You will implement a complete estimation pipeline for a Bayesian linear regression model, from deriving the full conditional posteriors to running the sampler and estimating the posterior ordinate. This hands-on problem is designed to build your core skills in applying the Rao-Blackwellized decomposition of the posterior ordinate, a cornerstone of the method for multi-parameter models.",
            "id": "3294515",
            "problem": "You are given a Bayesian linear regression model with conditionally conjugate priors that admit a two-block Gibbs sampler with full conditional densities available in closed form. Your task is to derive, implement, and compute the log marginal likelihood of the observed data using Chib’s method, specifically evaluating the posterior ordinate at a chosen point $\\theta^{\\star}$ via the product of averaged full conditionals. Your final program must be fully self-contained, produce the requested outputs for a specified test suite, and adhere to the final output format.\n\nModel and prior:\n- Likelihood: For observed data matrix $X \\in \\mathbb{R}^{n \\times p}$ and response vector $y \\in \\mathbb{R}^{n}$, assume\n$$\ny \\mid \\beta, \\sigma^2 \\sim \\mathcal{N}\\!\\left(X \\beta, \\sigma^2 I_n\\right).\n$$\n- Prior: Let the prior be conditionally conjugate of Normal-Inverse-Gamma form,\n$$\n\\beta \\mid \\sigma^2 \\sim \\mathcal{N}\\!\\left(m_0, \\sigma^2 S_0\\right), \\quad \\sigma^2 \\sim \\text{Inverse-Gamma}\\!\\left(a_0, b_0\\right),\n$$\nwith positive-definite $S_0 \\in \\mathbb{R}^{p \\times p}$ and positive hyperparameters $a_0, b_0 > 0$. Use the Inverse-Gamma density given by\n$$\nf_{\\text{IG}}(x \\mid a, b) = \\frac{b^a}{\\Gamma(a)} x^{-(a+1)} \\exp\\!\\left(-\\frac{b}{x}\\right), \\quad x > 0.\n$$\n\nFoundational base:\n- Use Bayes’ theorem and the factorization principle for joint densities.\n- Use the full conditional densities obtained from conjugacy for the Gibbs sampler.\n- Use Chib’s identity for the marginal likelihood and the decomposition of the posterior ordinate into a product of conditional posterior ordinates.\n- Use the law of large numbers to approximate expectations by averages over Gibbs samples.\n- Use standard multivariate normal and Inverse-Gamma density formulas.\n\nWhat you must derive and implement:\n- Derive the full conditional densities for $\\beta \\mid \\sigma^2, y$ and $\\sigma^2 \\mid \\beta, y$.\n- Construct a two-block Gibbs sampler based on these full conditionals.\n- Specify a consistent procedure to choose a high-posterior-density point $\\theta^{\\star} = (\\beta^{\\star}, \\sigma^{2\\star})$ from the Gibbs output, such as the posterior mean.\n- Starting from Bayes’ theorem and factorization of the posterior, derive the decomposition required to evaluate the posterior ordinate $p(\\theta^{\\star} \\mid y)$ as a product of averaged full conditionals at $\\theta^{\\star}$.\n- Express the log marginal likelihood as a sum/difference of terms that can be evaluated given the model, prior, and the estimated posterior ordinate. Avoid any shortcut formulas not derived from the above core principles.\n\nComputational plan to implement:\n- Implement a Gibbs sampler for the model.\n- Run the Gibbs sampler, discard burn-in, and compute $\\theta^{\\star}$ as the posterior mean of $(\\beta, \\sigma^2)$.\n- Compute the log likelihood $\\log p(y \\mid \\theta^{\\star})$ using the Gaussian likelihood.\n- Compute the log prior $\\log p(\\theta^{\\star})$ using the Normal-Inverse-Gamma prior.\n- Compute the posterior ordinate $\\log p(\\theta^{\\star} \\mid y)$ using the product of averaged full conditionals at $\\theta^{\\star}$, where the first factor $p(\\beta^{\\star} \\mid y)$ is approximated by averaging the full conditional density $p(\\beta^{\\star} \\mid \\sigma^{2(m)}, y)$ over posterior draws $\\{\\sigma^{2(m)}\\}$ from the Gibbs sampler, and the second factor $p(\\sigma^{2\\star} \\mid \\beta^{\\star}, y)$ is evaluated in closed form from the corresponding full conditional density at $(\\beta^{\\star}, \\sigma^{2\\star})$.\n- Combine the terms to obtain $\\log p(y)$ via Chib’s identity.\n\nTest suite and required outputs:\nImplement your program to compute the log marginal likelihood estimates for the following three test cases. For all tests, you must use the same hyperparameters:\n- Prior hyperparameters: $m_0 = \\mathbf{0}_p$, $S_0 = c_0 I_p$ with $c_0 = 100$, $a_0 = 2$, $b_0 = 1$.\n- Gibbs sampling settings for each test: total iterations $N_{\\text{iter}} = 9000$, burn-in $N_{\\text{burn}} = 4000$. Use the specified random seeds for data generation and for the Gibbs sampler to ensure reproducibility.\n\nTest case 1 (intercept-only model):\n- Data: $n = 6$, $p = 1$; $X = \\mathbf{1}_n$ and $y = [0.8, 1.2, 1.1, 0.7, 1.3, 0.9]^{\\top}$.\n- Gibbs sampler seed: $202$.\n\nTest case 2 (moderate dimension with correlated predictors):\n- Data generation seed: $123$.\n- Data: $n = 30$, $p = 3$; construct $x_1$ as $x_{1,i} = -2 + 4 (i-1)/(n-1)$ for $i = 1,\\dots,n$, draw $\\epsilon^{(x)}_i \\sim \\mathcal{N}(0, 0.1^2)$ and set $x_{2,i} = 0.8 x_{1,i} + \\epsilon^{(x)}_i$. Let $X = [\\mathbf{1}_n, x_1, x_2]$. Draw $\\epsilon^{(y)}_i \\sim \\mathcal{N}(0, 0.5^2)$ and set $y_i = 1 + 2 x_{1,i} - 1 x_{2,i} + \\epsilon^{(y)}_i$.\n- Gibbs sampler seed: $203$.\n\nTest case 3 (near-collinearity):\n- Data generation seed: $456$.\n- Data: $n = 20$, $p = 3$; construct $x_1$ as $x_{1,i} = -1 + 2 (i-1)/(n-1)$ for $i = 1,\\dots,n$, draw $\\delta_i \\sim \\mathcal{N}(0, 1)$, set $x_{2,i} = x_{1,i} + 10^{-4} \\delta_i$. Let $X = [\\mathbf{1}_n, x_1, x_2]$. Draw $\\epsilon^{(y)}_i \\sim \\mathcal{N}(0, 0.1^2)$ and set $y_i = 0.5 + 1.0 x_{1,i} + 1.0 x_{2,i} + \\epsilon^{(y)}_i$.\n- Gibbs sampler seed: $204$.\n\nAngle and physical units: Not applicable. No physical units and no angles appear in this problem.\n\nFinal output format:\n- Your program should produce a single line of output containing the three estimated log marginal likelihoods for the test cases, rounded to six decimal places, as a comma-separated list enclosed in square brackets, for example, \"[x1,x2,x3]\".\n\nYour implementation must be a single, complete, runnable program that generates the data (for tests 2 and 3), runs the Gibbs sampler, computes the Chib estimator using the product of averaged full conditionals at $\\theta^{\\star}$, and prints the results in the exact required format. No user input is permitted.",
            "solution": "The task is to compute the log marginal likelihood, $\\log p(y)$, for a Bayesian linear regression model using Chib's method. This requires deriving the full conditional posteriors for a Gibbs sampler and then using the Gibbs output to estimate the posterior ordinate at a specific high-density point $\\theta^{\\star} = (\\beta^{\\star}, \\sigma^{2\\star})$.\n\nThe model is defined by:\n- Likelihood: $y \\mid \\beta, \\sigma^2 \\sim \\mathcal{N}(X \\beta, \\sigma^2 I_n)$\n- Prior: $\\beta \\mid \\sigma^2 \\sim \\mathcal{N}(m_0, \\sigma^2 S_0)$ and $\\sigma^2 \\sim \\text{Inverse-Gamma}(a_0, b_0)$\n\nThe density for the Inverse-Gamma distribution is $f_{\\text{IG}}(x \\mid a, b) = \\frac{b^a}{\\Gamma(a)} x^{-(a+1)} \\exp(-b/x)$ for $x > 0$.\n\n**1. Derivation of Full Conditional Posterior Densities**\n\nThe joint posterior distribution is proportional to the product of the likelihood and the priors:\n$$p(\\beta, \\sigma^2 \\mid y) \\propto p(y \\mid \\beta, \\sigma^2) p(\\beta \\mid \\sigma^2) p(\\sigma^2)$$\n\n**Full Conditional for $\\beta$:**\nTo find the full conditional $p(\\beta \\mid \\sigma^2, y)$, we isolate terms in the joint posterior that involve $\\beta$:\n$$p(\\beta \\mid \\sigma^2, y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}(y - X\\beta)^T(y - X\\beta)\\right) \\exp\\left(-\\frac{1}{2\\sigma^2}(\\beta - m_0)^T S_0^{-1}(\\beta - m_0)\\right)$$\nExpanding the quadratic forms in the exponent:\n$$-\\frac{1}{2\\sigma^2} \\left[ (y^T y - 2\\beta^T X^T y + \\beta^T X^T X \\beta) + (\\beta^T S_0^{-1} \\beta - 2\\beta^T S_0^{-1} m_0 + m_0^T S_0^{-1} m_0) \\right]$$\nCollecting terms involving $\\beta$:\n$$-\\frac{1}{2\\sigma^2} \\left[ \\beta^T(X^T X + S_0^{-1})\\beta - 2\\beta^T(X^T y + S_0^{-1} m_0) \\right] + \\text{const.}$$\nThis is the kernel of a multivariate Normal density for $\\beta$. By completing the square, we identify the posterior precision matrix $S_n^{-1} = (X^T X + S_0^{-1})$ and posterior mean $m_n = S_n(X^T y + S_0^{-1} m_0)$. The full conditional for $\\beta$ is therefore:\n$$\\beta \\mid \\sigma^2, y \\sim \\mathcal{N}(m_n, \\sigma^2 S_n)$$\nwhere $S_n = (X^T X + S_0^{-1})^{-1}$ and $m_n = S_n(X^T y + S_0^{-1} m_0)$.\n\n**Full Conditional for $\\sigma^2$:**\nTo find the full conditional $p(\\sigma^2 \\mid \\beta, y)$, we isolate terms involving $\\sigma^2$:\n$$p(\\sigma^2 \\mid \\beta, y) \\propto p(y \\mid \\beta, \\sigma^2) p(\\beta \\mid \\sigma^2) p(\\sigma^2)$$\n$$p(\\sigma^2 \\mid \\beta, y) \\propto \\left((\\sigma^2)^{-n/2} \\exp\\left(-\\frac{(y-X\\beta)^T(y-X\\beta)}{2\\sigma^2}\\right)\\right) \\times \\left((\\sigma^2)^{-p/2} \\exp\\left(-\\frac{(\\beta-m_0)^T S_0^{-1}(\\beta-m_0)}{2\\sigma^2}\\right)\\right) \\times \\left((\\sigma^2)^{-(a_0+1)} \\exp\\left(-\\frac{b_0}{\\sigma^2}\\right)\\right)$$\nCombining the terms:\n$$p(\\sigma^2 \\mid \\beta, y) \\propto (\\sigma^2)^{-(a_0 + \\frac{n+p}{2} + 1)} \\exp\\left(-\\frac{1}{\\sigma^2}\\left[b_0 + \\frac{1}{2}(y-X\\beta)^T(y-X\\beta) + \\frac{1}{2}(\\beta-m_0)^T S_0^{-1}(\\beta-m_0)\\right]\\right)$$\nThis is the kernel of an Inverse-Gamma density. The posterior shape and rate parameters are:\n$$a_n = a_0 + \\frac{n+p}{2}$$\n$$b_n = b_0 + \\frac{1}{2}(y-X\\beta)^T(y-X\\beta) + \\frac{1}{2}(\\beta-m_0)^T S_0^{-1}(\\beta-m_0)$$\nThus, the full conditional for $\\sigma^2$ is:\n$$\\sigma^2 \\mid \\beta, y \\sim \\text{Inverse-Gamma}(a_n, b_n)$$\n\n**2. Derivation of Chib's Method for Marginal Likelihood**\n\nThe marginal likelihood $p(y)$ can be expressed using the identity derived from Bayes' theorem, which holds for any parameter value $\\theta = (\\beta, \\sigma^2)$:\n$$p(y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(\\theta \\mid y)}$$\nIn log space, this is:\n$$\\log p(y) = \\log p(y \\mid \\theta) + \\log p(\\theta) - \\log p(\\theta \\mid y)$$\nFor numerical stability, we evaluate this identity at a high-density point $\\theta^{\\star} = (\\beta^{\\star}, \\sigma^{2\\star})$, which we choose as the posterior mean calculated from the Gibbs sampler's output. The three terms are:\n1.  $\\log p(y \\mid \\theta^{\\star})$: The log-likelihood evaluated at $\\theta^{\\star}$. This is the log-pdf of $\\mathcal{N}(y \\mid X\\beta^{\\star}, \\sigma^{2\\star}I_n)$ at the data $y$.\n2.  $\\log p(\\theta^{\\star})$: The log-prior evaluated at $\\theta^{\\star}$. Due to the prior structure $p(\\theta) = p(\\beta \\mid \\sigma^2) p(\\sigma^2)$, this is $\\log p(\\beta^{\\star} \\mid \\sigma^{2\\star}) + \\log p(\\sigma^{2\\star})$, where the densities are those of the priors $\\mathcal{N}(m_0, \\sigma^{2\\star}S_0)$ and $\\text{IG}(a_0, b_0)$.\n3.  $\\log p(\\theta^{\\star} \\mid y)$: The log-posterior ordinate evaluated at $\\theta^{\\star}$. This term requires careful estimation.\n\n**Estimating the Posterior Ordinate $\\log p(\\theta^{\\star} \\mid y)$**\n\nAs specified, we decompose the posterior ordinate using the chain rule:\n$$p(\\theta^{\\star} \\mid y) = p(\\beta^{\\star}, \\sigma^{2\\star} \\mid y) = p(\\beta^{\\star} \\mid y) p(\\sigma^{2\\star} \\mid \\beta^{\\star}, y)$$\nThe two factors are handled as follows:\n\n- **Second Factor $p(\\sigma^{2\\star} \\mid \\beta^{\\star}, y)$**: This is the full conditional density of $\\sigma^2$ evaluated at $\\sigma^{2\\star}$, given $\\beta = \\beta^{\\star}$. We have derived this density as $\\text{IG}(a_n, b_n)$. We can directly calculate its value by plugging $\\beta^{\\star}$ and $\\sigma^{2\\star}$ into the IG pdf with parameters $a_n^{\\star} = a_0 + \\frac{n+p}{2}$ and $b_n^{\\star} = b_0 + \\frac{1}{2}(y-X\\beta^{\\star})^T(y-X\\beta^{\\star}) + \\frac{1}{2}(\\beta^{\\star}-m_0)^T S_0^{-1}(\\beta^{\\star}-m_0)$.\n\n- **First Factor $p(\\beta^{\\star} \\mid y)$**: This is the marginal posterior density of $\\beta$ evaluated at $\\beta^{\\star}$. It can be expressed as an integral over $\\sigma^2$:\n$$p(\\beta^{\\star} \\mid y) = \\int p(\\beta^{\\star}, \\sigma^2 \\mid y) d\\sigma^2 = \\int p(\\beta^{\\star} \\mid \\sigma^2, y) p(\\sigma^2 \\mid y) d\\sigma^2 = E_{\\sigma^2 \\mid y}[p(\\beta^{\\star} \\mid \\sigma^2, y)]$$\nWe can estimate this expectation by averaging over the post-burn-in samples of $\\sigma^2$ from our Gibbs sampler. Let $\\{\\sigma^{2(m)}\\}_{m=1}^M$ be the $M$ posterior samples. The Monte Carlo estimate is:\n$$\\hat{p}(\\beta^{\\star} \\mid y) = \\frac{1}{M} \\sum_{m=1}^{M} p(\\beta^{\\star} \\mid \\sigma^{2(m)}, y)$$\nEach term $p(\\beta^{\\star} \\mid \\sigma^{2(m)}, y)$ is the PDF of the full conditional for $\\beta$, $\\mathcal{N}(m_n, \\sigma^{2(m)} S_n)$, evaluated at $\\beta^{\\star}$.\n\nCombining these, the log posterior ordinate is estimated as:\n$$\\log p(\\theta^{\\star} \\mid y) \\approx \\log \\left(\\frac{1}{M} \\sum_{m=1}^M p(\\beta^{\\star} \\mid \\sigma^{2(m)}, y)\\right) + \\log p(\\sigma^{2\\star} \\mid \\beta^{\\star}, y)$$\n\n**3. Computational Algorithm**\n\nThe complete algorithm is as follows:\n1.  **Gibbs Sampling**:\n    a. Initialize $\\beta^{(0)}$ and $\\sigma^{2(0)}$.\n    b. Pre-compute constant quantities: $S_0^{-1}$, $S_n=(X^TX+S_0^{-1})^{-1}$, $m_n=S_n(X^Ty+S_0^{-1}m_0)$, and $a_n=a_0+(n+p)/2$.\n    c. For $t=1, \\dots, N_{\\text{iter}}$:\n        i.  Draw $\\beta^{(t)} \\sim \\mathcal{N}(m_n, \\sigma^{2(t-1)}S_n)$.\n        ii. Calculate $b_n^{(t)} = b_0 + \\frac{1}{2}(y-X\\beta^{(t)})^T(y-X\\beta^{(t)}) + \\frac{1}{2}(\\beta^{(t)}-m_0)^T S_0^{-1}(\\beta^{(t)}-m_0)$.\n        iii. Draw $\\sigma^{2(t)} \\sim \\text{IG}(a_n, b_n^{(t)})$.\n    d. Discard the first $N_{\\text{burn}}$ samples to get $M = N_{\\text{iter}} - N_{\\text{burn}}$ posterior samples.\n\n2.  **Compute High-Density Point $\\theta^{\\star}$**:\n    a. Calculate the posterior means: $\\beta^{\\star} = \\frac{1}{M} \\sum_{m=1}^M \\beta^{(m)}$ and $\\sigma^{2\\star} = \\frac{1}{M} \\sum_{m=1}^M \\sigma^{2(m)}$.\n\n3.  **Evaluate Chib's Identity Terms**:\n    a. **Log-Likelihood**: Compute $\\log p(y \\mid \\theta^{\\star}) = \\log \\mathcal{N}(y \\mid X\\beta^{\\star}, \\sigma^{2\\star}I_n)$.\n    b. **Log-Prior**: Compute $\\log p(\\theta^{\\star}) = \\log \\mathcal{N}(\\beta^{\\star} \\mid m_0, \\sigma^{2\\star}S_0) + \\log \\text{IG}(\\sigma^{2\\star} \\mid a_0, b_0)$.\n    c. **Log-Posterior Ordinate**:\n        i.  Estimate $\\hat{p}(\\beta^{\\star} \\mid y) = \\frac{1}{M} \\sum_{m=1}^M \\mathcal{N}(\\beta^{\\star} \\mid m_n, \\sigma^{2(m)} S_n)$.\n        ii. Calculate $b_n^{\\star}$ using $\\beta^{\\star}$. Then calculate $p(\\sigma^{2\\star} \\mid \\beta^{\\star}, y) = \\text{IG}(\\sigma^{2\\star} \\mid a_n, b_n^{\\star})$.\n        iii. Compute $\\log p(\\theta^{\\star} \\mid y) = \\log(\\hat{p}(\\beta^{\\star} \\mid y)) + \\log(p(\\sigma^{2\\star} \\mid \\beta^{\\star}, y))$.\n\n4.  **Final Calculation**:\n    a. Compute $\\log p(y) = \\log p(y \\mid \\theta^{\\star}) + \\log p(\\theta^{\\star}) - \\log p(\\theta^{\\star} \\mid y)$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import multivariate_normal, invgamma\nfrom scipy.special import gammaln\n\ndef compute_log_marginal_likelihood(X, y, m0, S0, a0, b0, N_iter, N_burn, gibbs_seed):\n    \"\"\"\n    Computes the log marginal likelihood for a Bayesian linear regression model\n    using Chib's method.\n    \"\"\"\n    n, p = X.shape\n    M = N_iter - N_burn\n    \n    # --- 1. Gibbs Sampler ---\n    rng = np.random.default_rng(gibbs_seed)\n\n    # Pre-compute fixed quantities for the full conditionals\n    S0_inv = np.linalg.inv(S0)\n    XTX = X.T @ X\n    S_n_prec = XTX + S0_inv\n    S_n_inv = np.linalg.inv(S_n_prec)\n    XTy = X.T @ y\n    S0_inv_m0 = S0_inv @ m0\n    m_n = S_n_inv @ (XTy + S0_inv_m0)\n    \n    a_n = a0 + (n + p) / 2.0\n\n    # Initialize Gibbs sampler\n    beta_curr = np.zeros(p)\n    sigma2_curr = 1.0\n\n    # Store posterior samples\n    beta_samples = np.zeros((M, p))\n    sigma2_samples = np.zeros(M)\n    \n    for i in range(N_iter):\n        # Draw beta from its full conditional\n        beta_cov = sigma2_curr * S_n_inv\n        beta_curr = rng.multivariate_normal(m_n, beta_cov)\n\n        # Draw sigma^2 from its full conditional\n        resid = y - X @ beta_curr\n        beta_prior_resid = beta_curr - m0\n        \n        b_n = b0 + 0.5 * (resid @ resid) + 0.5 * (beta_prior_resid.T @ S0_inv @ beta_prior_resid)\n        \n        # Sample from IG(a_n, b_n) by sampling from Gamma and inverting\n        sigma2_curr = 1.0 / rng.gamma(shape=a_n, scale=1.0 / b_n)\n        \n        if i >= N_burn:\n            beta_samples[i - N_burn] = beta_curr\n            sigma2_samples[i - N_burn] = sigma2_curr\n\n    # --- 2. Choose High-Density Point (theta_star) ---\n    beta_star = np.mean(beta_samples, axis=0)\n    sigma2_star = np.mean(sigma2_samples)\n\n    # --- 3. Evaluate Terms of Chib's Identity ---\n    \n    # 3a. Log-Likelihood at theta_star\n    log_likelihood_star = multivariate_normal.logpdf(y, mean=X @ beta_star, cov=sigma2_star * np.identity(n))\n\n    # 3b. Log-Prior at theta_star\n    log_prior_beta_star = multivariate_normal.logpdf(beta_star, mean=m0, cov=sigma2_star * S0)\n    log_prior_sigma2_star = invgamma.logpdf(sigma2_star, a=a0, scale=b0)\n    log_prior_star = log_prior_beta_star + log_prior_sigma2_star\n\n    # 3c. Log-Posterior Ordinate at theta_star\n    \n    # First term: log p(beta* | y) estimated via averaging\n    p_beta_star_terms = np.zeros(M)\n    for i in range(M):\n        sigma2_m = sigma2_samples[i]\n        cov_m = sigma2_m * S_n_inv\n        # We need the PDF value, not the log-PDF, for averaging\n        p_beta_star_terms[i] = multivariate_normal.pdf(beta_star, mean=m_n, cov=cov_m)\n    \n    p_beta_star_hat = np.mean(p_beta_star_terms)\n    log_p_beta_star_hat = np.log(p_beta_star_hat)\n\n    # Second term: log p(sigma2* | beta*, y) computed directly\n    resid_star = y - X @ beta_star\n    beta_prior_resid_star = beta_star - m0\n    b_n_star = b0 + 0.5 * (resid_star @ resid_star) + 0.5 * (beta_prior_resid_star.T @ S0_inv @ beta_prior_resid_star)\n    log_p_sigma2_star = invgamma.logpdf(sigma2_star, a=a_n, scale=b_n_star)\n\n    log_posterior_ordinate_star = log_p_beta_star_hat + log_p_sigma2_star\n\n    # --- 4. Final Calculation ---\n    log_marginal_likelihood = log_likelihood_star + log_prior_star - log_posterior_ordinate_star\n    \n    return log_marginal_likelihood\n\n\ndef solve():\n    # --- Global settings ---\n    c0 = 100.0\n    a0 = 2.0\n    b0 = 1.0\n    N_iter = 9000\n    N_burn = 4000\n    \n    results = []\n    \n    # --- Test Case 1 ---\n    n1, p1 = 6, 1\n    X1 = np.ones((n1, p1))\n    y1 = np.array([0.8, 1.2, 1.1, 0.7, 1.3, 0.9])\n    m0_1 = np.zeros(p1)\n    S0_1 = c0 * np.identity(p1)\n    gibbs_seed_1 = 202\n    \n    log_ml_1 = compute_log_marginal_likelihood(X1, y1, m0_1, S0_1, a0, b0, N_iter, N_burn, gibbs_seed_1)\n    results.append(log_ml_1)\n\n    # --- Test Case 2 ---\n    data_gen_seed_2 = 123\n    gibbs_seed_2 = 203\n    n2, p2 = 30, 3\n    \n    rng_data2 = np.random.default_rng(data_gen_seed_2)\n    x1_2 = np.linspace(-2, 2, n2)\n    eps_x2 = rng_data2.normal(0, 0.1, size=n2)\n    x2_2 = 0.8 * x1_2 + eps_x2\n    X2 = np.c_[np.ones(n2), x1_2, x2_2]\n    \n    eps_y2 = rng_data2.normal(0, 0.5, size=n2)\n    y2 = 1.0 + 2.0 * x1_2 - 1.0 * x2_2 + eps_y2\n    \n    m0_2 = np.zeros(p2)\n    S0_2 = c0 * np.identity(p2)\n    \n    log_ml_2 = compute_log_marginal_likelihood(X2, y2, m0_2, S0_2, a0, b0, N_iter, N_burn, gibbs_seed_2)\n    results.append(log_ml_2)\n    \n    # --- Test Case 3 ---\n    data_gen_seed_3 = 456\n    gibbs_seed_3 = 204\n    n3, p3 = 20, 3\n    \n    rng_data3 = np.random.default_rng(data_gen_seed_3)\n    x1_3 = np.linspace(-1, 1, n3)\n    delta3 = rng_data3.normal(0, 1, size=n3)\n    x2_3 = x1_3 + 1e-4 * delta3\n    X3 = np.c_[np.ones(n3), x1_3, x2_3]\n    \n    eps_y3 = rng_data3.normal(0, 0.1, size=n3)\n    y3 = 0.5 + 1.0 * x1_3 + 1.0 * x2_3 + eps_y3\n    \n    m0_3 = np.zeros(p3)\n    S0_3 = c0 * np.identity(p3)\n\n    log_ml_3 = compute_log_marginal_likelihood(X3, y3, m0_3, S0_3, a0, b0, N_iter, N_burn, gibbs_seed_3)\n    results.append(log_ml_3)\n\n    # Final print statement\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A key part of a statistician's toolkit is not just knowing how to implement a method, but also understanding its advantages and limitations compared to alternatives. This exercise  facilitates a critical comparison between the Chib method and the notoriously unstable Harmonic Mean Estimator (HME). By analyzing a scenario specifically designed to trigger the HME's infinite variance, you will directly observe its failure and verify the stability of the posterior ordinate approach, thereby appreciating the robustness that the Chib method offers.",
            "id": "3294514",
            "problem": "Consider Bayesian model evidence (marginal likelihood) computation for a univariate normal sampling model with unknown mean and unknown variance. Let the observations be $y_1,\\dots,y_n \\in \\mathbb{R}$ with likelihood\n$$\np(y \\mid \\mu,\\sigma^2) \\;=\\; \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\!\\left(-\\frac{(y_i-\\mu)^2}{2\\sigma^2}\\right),\n$$\nand let the prior be the conjugate Normal-Inverse-Gamma distribution,\n$$\n\\sigma^2 \\sim \\text{Inverse-Gamma}(\\alpha_0,\\beta_0), \\quad \\mu \\mid \\sigma^2 \\sim \\mathcal{N}\\!\\left(\\mu_0,\\frac{\\sigma^2}{\\kappa_0}\\right),\n$$\nwith hyperparameters $\\mu_0 \\in \\mathbb{R}$, $\\kappa_0 > 0$, $\\alpha_0 > 0$, $\\beta_0 > 0$. The posterior is Normal-Inverse-Gamma with updated parameters\n$$\n\\kappa_n \\;=\\; \\kappa_0 + n,\\quad\n\\mu_n \\;=\\; \\frac{\\kappa_0 \\mu_0 + n \\bar{y}}{\\kappa_0 + n},\\quad\n\\alpha_n \\;=\\; \\alpha_0 + \\frac{n}{2},\\quad\n\\beta_n \\;=\\; \\beta_0 + \\frac{1}{2}\\left(\\sum_{i=1}^n (y_i-\\bar{y})^2 + \\frac{\\kappa_0 n}{\\kappa_0+n}(\\bar{y}-\\mu_0)^2\\right),\n$$\nwhere $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$.\n\nYou must implement and compare two estimators for the log marginal likelihood $\\log p(y)$:\n\n- The harmonic mean estimator based on Monte Carlo draws $(\\mu^{(s)},\\sigma^{2\\,(s)})$ from the exact posterior $p(\\mu,\\sigma^2 \\mid y)$, defined by\n$$\n\\widehat{p}_{\\text{HM}}(y) \\;=\\; \\left[\\frac{1}{S}\\sum_{s=1}^S \\frac{1}{p(y \\mid \\mu^{(s)},\\sigma^{2\\,(s)})}\\right]^{-1}.\n$$\n\n- Chib’s posterior ordinate method, which uses the identity\n$$\n\\log p(y) \\;=\\; \\log p(y \\mid \\mu^\\star,\\sigma^{2\\,\\star}) \\;+\\; \\log p(\\mu^\\star,\\sigma^{2\\,\\star}) \\;-\\; \\log p(\\mu^\\star,\\sigma^{2\\,\\star} \\mid y),\n$$\nfor any fixed point $(\\mu^\\star,\\sigma^{2\\,\\star})$. In this problem, you must take $\\mu^\\star=\\mu_n$ and $\\sigma^{2\\,\\star}=\\frac{\\beta_n}{\\alpha_n+1}$ (the Normal-Inverse-Gamma posterior mode for $\\sigma^2$ and the posterior mean/mode for $\\mu$ conditional on $\\sigma^2$), and evaluate all three densities in closed form.\n\nYour program must also compute the exact log marginal likelihood $\\log p(y)$ by analytically integrating out $(\\mu,\\sigma^2)$ under the conjugate prior. Use only the foundational definitions of the Normal-Inverse-Gamma prior-likelihood system and properties of the gamma function as the starting point for any derivations you need.\n\nThen, using a reproducible Monte Carlo design with multiple independent replications, you must demonstrate a setting where the harmonic mean estimator exhibits catastrophic instability (infinite or near-infinite variance) and how Chib’s posterior ordinate approach avoids this pathology. The catastrophic instability arises because the harmonic mean estimator’s variance diverges when the Monte Carlo average involves tail contributions that decay too slowly; in this Normal-Inverse-Gamma setting, this occurs whenever the posterior for $\\sigma^2$ has sufficiently heavy tails so that the expectation of the squared harmonic weight under the posterior does not exist. Concretely, by analyzing the tail behavior of $\\sigma^2$ under the posterior and $p(y \\mid \\mu,\\sigma^2)$ for large $\\sigma^2$, a sufficient condition for divergence of the harmonic mean estimator’s variance is $\\alpha_0 \\le \\frac{n}{2}$.\n\nTest Suite and required computations:\n\nFor each test case below, you must:\n- Compute the exact log marginal likelihood $\\log p(y)$ in closed form.\n- Compute Chib’s log marginal likelihood estimate $\\log \\widehat{p}_{\\text{Chib}}(y)$ using the posterior ordinate at $(\\mu^\\star,\\sigma^{2\\,\\star})=(\\mu_n,\\beta_n/(\\alpha_n+1))$.\n- Compute $R$ independent harmonic mean estimates, each using $S$ posterior draws from $p(\\mu,\\sigma^2 \\mid y)$, and report:\n  - The median absolute error of harmonic mean estimates: $\\operatorname{median}_{r=1,\\dots,R} \\left| \\log \\widehat{p}_{\\text{HM}}^{(r)}(y) - \\log p(y)\\right|$.\n  - The empirical standard deviation of the $R$ values $\\log \\widehat{p}_{\\text{HM}}^{(r)}(y)$.\n\nUse $R=20$ independent replications and $S=3000$ posterior draws per replication. For each replication $r \\in \\{1,\\dots,R\\}$ in test case index $c \\in \\{0,1,2\\}$, initialize the random number generator with seed $12345 + 1000c + r$ to ensure determinism.\n\nTest cases:\n\n- Case A (well-behaved prior, \"happy path\"):\n  - Data $y$ of length $n=\\;10$: $[0.10,-0.35,0.47,1.26,-0.27,0.02,-1.06,0.53,0.79,-0.15]$.\n  - Hyperparameters: $\\mu_0=\\;0.0$, $\\kappa_0=\\;1.0$, $\\alpha_0=\\;10.0$, $\\beta_0=\\;10.0$.\n\n- Case B (boundary for divergence of harmonic mean variance):\n  - Data $y$ of length $n=\\;8$: $[0.48,0.51,-0.06,0.83,-1.15,0.27,0.12,-0.44]$.\n  - Hyperparameters: $\\mu_0=\\;0.0$, $\\kappa_0=\\;1.0$, $\\alpha_0=\\;\\frac{n}{2}=\\;4.0$, $\\beta_0=\\;4.0$.\n\n- Case C (catastrophic instability with extremely heavy-tailed posterior in $\\sigma^2$ and an outlier):\n  - Data $y$ of length $n=\\;8$: $[0.10,-0.20,0.30,0.00,0.20,-0.10,0.00,10.00]$.\n  - Hyperparameters: $\\mu_0=\\;0.0$, $\\kappa_0=\\;1.0$, $\\alpha_0=\\;0.01$, $\\beta_0=\\;0.01$.\n\nRequired final outputs:\n\nFor each case in the order A, B, C, produce three floating-point numbers:\n- $e_{\\text{Chib}}$: the absolute error $\\left|\\log \\widehat{p}_{\\text{Chib}}(y)-\\log p(y)\\right|$.\n- $e_{\\text{HM,med}}$: the median absolute error of the $R$ harmonic mean estimates relative to $\\log p(y)$.\n- $s_{\\text{HM}}$: the empirical standard deviation across the $R$ harmonic mean log-evidence estimates.\n\nYour program should produce a single line of output containing the $9$ results (three per case in order A, then B, then C) as a comma-separated list enclosed in square brackets with each value rounded to exactly $6$ decimal places, for example, $[0.000001,0.123456,9.876543, \\dots]$. No other text should be printed.",
            "solution": "The task is to compare the Harmonic Mean Estimator (HME) and Chib's method against an exact analytical solution for the marginal likelihood of a Normal-Inverse-Gamma model.\n\n**1. Exact Log Marginal Likelihood**\nThe marginal likelihood $p(y)$ is the integral of the likelihood times the prior. For this conjugate system, it can be calculated as the ratio of the normalizing constants of the prior and posterior, adjusted for the likelihood's constant terms.\n$$p(y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(\\theta \\mid y)}$$\nBy matching the non-parameter-dependent terms, we get:\n$$p(y) = (2\\pi)^{-n/2} \\frac{\\text{Prior Normalizer}}{\\text{Posterior Normalizer}} = (2\\pi)^{-n/2} \\frac{\\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)}\\sqrt{\\frac{\\kappa_0}{2\\pi}}}{\\frac{\\beta_n^{\\alpha_n}}{\\Gamma(\\alpha_n)}\\sqrt{\\frac{\\kappa_n}{2\\pi}}}$$\nSimplifying this gives the analytical formula:\n$$p(y) = (2\\pi)^{-n/2} \\sqrt{\\frac{\\kappa_0}{\\kappa_n}} \\frac{\\Gamma(\\alpha_n)}{\\Gamma(\\alpha_0)} \\frac{\\beta_0^{\\alpha_0}}{\\beta_n^{\\alpha_n}}$$\nIn log-space, which is more numerically stable:\n$$\\log p(y) = -\\frac{n}{2}\\log(2\\pi) + \\frac{1}{2}(\\log\\kappa_0 - \\log\\kappa_n) + \\gammaln(\\alpha_n) - \\gammaln(\\alpha_0) + \\alpha_0\\log\\beta_0 - \\alpha_n\\log\\beta_n$$\nThis serves as the ground truth for our comparison.\n\n**2. Harmonic Mean Estimator (HME)**\nThe HME is based on samples $\\{\\mu^{(s)}, \\sigma^{2(s)}\\}_{s=1}^S$ drawn from the posterior distribution, $p(\\mu, \\sigma^2 \\mid y) = \\text{Normal-Inverse-Gamma}(\\mu_n, \\kappa_n, \\alpha_n, \\beta_n)$.\nThe estimator is:\n$$\\widehat{p}_{\\text{HM}}(y) = \\left[\\frac{1}{S}\\sum_{s=1}^S \\frac{1}{p(y \\mid \\mu^{(s)},\\sigma^{2\\,(s)})}\\right]^{-1}$$\nComputationally, to avoid numerical underflow/overflow, the log of the HME is calculated using the log-sum-exp trick. Let $l^{(s)} = \\log p(y \\mid \\mu^{(s)}, \\sigma^{2(s)})$. Then:\n$$\\log \\widehat{p}_{\\text{HM}}(y) = -\\log\\left(\\sum_{s=1}^S e^{-l^{(s)}}\\right) + \\log S$$\nThe instability of this estimator arises because the expectation of the inverse likelihood, $E_{post}[1/p(y \\mid \\theta)]$, may be dominated by rare draws from the tail of the posterior that have very low likelihoods. This can lead to infinite variance of the estimator, as explored in the test cases.\n\n**3. Chib's Method**\nChib's method uses the identity:\n$$\\log p(y) = \\log p(y \\mid \\theta^\\star) + \\log p(\\theta^\\star) - \\log p(\\theta^\\star \\mid y)$$\nWe evaluate this at the specified high-density point $\\theta^\\star = (\\mu^\\star, \\sigma^{2\\star}) = (\\mu_n, \\frac{\\beta_n}{\\alpha_n+1})$. All densities on the right-hand side have analytical forms.\n- **Log-Likelihood $\\log p(y \\mid \\theta^\\star)$**: Calculated using the normal density PDF with parameters $\\mu^\\star$ and $\\sigma^{2\\star}$.\n- **Log-Prior $\\log p(\\theta^\\star)$**: Decomposed as $\\log p(\\theta^\\star) = \\log p(\\mu^\\star \\mid \\sigma^{2\\star}) + \\log p(\\sigma^{2\\star})$. The densities are the prior $\\mathcal{N}(\\mu_0, \\sigma^{2\\star}/\\kappa_0)$ and $\\text{Inverse-Gamma}(\\alpha_0, \\beta_0)$.\n- **Log-Posterior $\\log p(\\theta^\\star \\mid y)$**: Decomposed as $\\log p(\\theta^\\star \\mid y) = \\log p(\\mu^\\star \\mid \\sigma^{2\\star}, y) + \\log p(\\sigma^{2\\star} \\mid y)$. The densities are the posterior conditional $\\mathcal{N}(\\mu_n, \\sigma^{2\\star}/\\kappa_n)$ and marginal $\\text{Inverse-Gamma}(\\alpha_n, \\beta_n)$.\nSince all components are calculated from closed-form PDFs, this estimate is deterministic and should be highly accurate, matching the exact analytical value up to floating-point precision.",
            "answer": "```python\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n\n    test_cases = []\n    # Case A: Well-behaved prior\n    y_A = np.array([0.10, -0.35, 0.47, 1.26, -0.27, 0.02, -1.06, 0.53, 0.79, -0.15])\n    test_cases.append({'y': y_A, 'mu0': 0.0, 'kappa0': 1.0, 'alpha0': 10.0, 'beta0': 10.0})\n\n    # Case B: Boundary for HM divergence\n    y_B = np.array([0.48, 0.51, -0.06, 0.83, -1.15, 0.27, 0.12, -0.44])\n    n_B = len(y_B)\n    test_cases.append({'y': y_B, 'mu0': 0.0, 'kappa0': 1.0, 'alpha0': n_B / 2.0, 'beta0': n_B / 2.0})\n\n    # Case C: Catastrophic instability\n    y_C = np.array([0.10, -0.20, 0.30, 0.00, 0.20, -0.10, 0.00, 10.00])\n    test_cases.append({'y': y_C, 'mu0': 0.0, 'kappa0': 1.0, 'alpha0': 0.01, 'beta0': 0.01})\n    \n    R = 20\n    S = 3000\n\n    all_results = []\n    for i, case_params in enumerate(test_cases):\n        results = _solve_case(case_params, i, R, S)\n        all_results.extend(results)\n\n    print(f\"[{','.join([f'{val:.6f}' for val in all_results])}]\")\n\ndef _solve_case(params, case_idx, R, S):\n    \"\"\"\n    Computes the required metrics for a single test case.\n    \"\"\"\n    y = params['y']\n    mu0 = params['mu0']\n    kappa0 = params['kappa0']\n    alpha0 = params['alpha0']\n    beta0 = params['beta0']\n    \n    n = len(y)\n    y_bar = np.mean(y)\n    sum_sq_dev = np.sum((y - y_bar)**2)\n\n    # Calculate posterior parameters\n    kappa_n = kappa0 + n\n    mu_n = (kappa0 * mu0 + n * y_bar) / kappa_n\n    alpha_n = alpha0 + n / 2.0\n    beta_n = beta0 + 0.5 * (sum_sq_dev + (kappa0 * n / kappa_n) * (y_bar - mu0)**2)\n    \n    # 1. Exact Log Marginal Likelihood\n    log_p_exact = (-0.5 * n * np.log(2 * np.pi) +\n                   0.5 * (np.log(kappa0) - np.log(kappa_n)) +\n                   gammaln(alpha_n) - gammaln(alpha0) +\n                   alpha0 * np.log(beta0) - alpha_n * np.log(beta_n))\n\n    # 2. Chib's Method\n    sigma2_star = beta_n / (alpha_n + 1.0)\n    \n    # Term 1: log likelihood at the point\n    log_lik_star = -0.5 * n * np.log(2 * np.pi * sigma2_star) - np.sum((y - mu_n)**2) / (2 * sigma2_star)\n    \n    # Term 2: log prior at the point\n    log_prior_sigma2 = alpha0 * np.log(beta0) - gammaln(alpha0) - (alpha0 + 1.0) * np.log(sigma2_star) - beta0 / sigma2_star\n    log_prior_mu_cond = -0.5 * np.log(2 * np.pi * sigma2_star / kappa0) - (kappa0 * (mu_n - mu0)**2) / (2 * sigma2_star)\n    log_prior_star = log_prior_sigma2 + log_prior_mu_cond\n    \n    # Term 3: log posterior at the point\n    log_post_sigma2 = alpha_n * np.log(beta_n) - gammaln(alpha_n) - (alpha_n + 1.0) * np.log(sigma2_star) - beta_n / sigma2_star\n    log_post_mu_cond = -0.5 * np.log(2 * np.pi * sigma2_star / kappa_n)\n    log_post_star = log_post_sigma2 + log_post_mu_cond\n    \n    log_p_chib = log_lik_star + log_prior_star - log_post_star\n    e_chib = np.abs(log_p_chib - log_p_exact)\n\n    # 3. Harmonic Mean Estimator\n    log_phm_reps = np.zeros(R)\n    for r in range(1, R + 1):\n        seed = 12345 + 1000 * case_idx + r\n        rng = np.random.default_rng(seed)\n        \n        # Sample from posterior\n        sigma2_samples = 1.0 / rng.gamma(shape=alpha_n, scale=1.0/beta_n, size=S)\n        mu_samples = rng.normal(loc=mu_n, scale=np.sqrt(sigma2_samples / kappa_n), size=S)\n        \n        # Calculate log-likelihoods for each sample\n        # Vectorized implementation for speed\n        sum_sq_errs = np.sum((y[:, np.newaxis] - mu_samples)**2, axis=0)\n        log_liks = -0.5 * n * np.log(2 * np.pi * sigma2_samples) - sum_sq_errs / (2 * sigma2_samples)\n\n        # Compute log harmonic mean estimate using log-sum-exp trick\n        m = -log_liks\n        M = np.max(m)\n        log_sum_exp = M + np.log(np.sum(np.exp(m - M)))\n        log_hm_avg = log_sum_exp - np.log(S)\n        log_phm_reps[r-1] = -log_hm_avg\n    \n    abs_errors_hm = np.abs(log_phm_reps - log_p_exact)\n    e_hm_med = np.median(abs_errors_hm)\n    s_hm = np.std(log_phm_reps, ddof=1)\n    \n    return [e_chib, e_hm_med, s_hm]\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}