{
    "hands_on_practices": [
        {
            "introduction": "The theory of variance reduction is best understood through direct application. This first practice explores antithetic variates, a technique that exploits the structure of the input randomness and the target function. For a monotonic function $f(u)$, pairing a random number $U$ with its \"opposite\" $1-U$ induces a negative correlation between $f(U)$ and $f(1-U)$, which in turn reduces the variance of their average. This exercise  makes this principle tangible by guiding you through a first-principles derivation to quantify the exact variance reduction achieved when estimating the integral of an exponential function, a classic case where this method proves highly effective.",
            "id": "3360544",
            "problem": "Consider estimating the integral $I=\\mathbb{E}[f(U)]$ for $f(u)=\\exp(u)$ when $U\\sim \\text{Uniform}(0,1)$. Work from first principles: definitions of expectation as an integral, variance as $\\operatorname{Var}(X)=\\mathbb{E}[X^{2}]-(\\mathbb{E}[X])^{2}$, and linearity of expectation. Two estimators using a fixed computational budget of $N$ function evaluations (with $N$ even) are considered.\n\n- Plain Monte Carlo (MC): Draw independent and identically distributed (i.i.d.) $U_{1},\\ldots,U_{N}\\sim \\text{Uniform}(0,1)$ and compute $\\widehat{I}_{N}=\\frac{1}{N}\\sum_{i=1}^{N} f(U_{i})$.\n\n- Antithetic variates: Draw i.i.d. $U_{1},\\ldots,U_{N/2}\\sim \\text{Uniform}(0,1)$, form antithetic pairs $(U_{i},1-U_{i})$, and compute $\\widehat{I}^{\\mathrm{anti}}_{N}=\\frac{2}{N}\\sum_{i=1}^{N/2}\\frac{f(U_{i})+f(1-U_{i})}{2}$.\n\nUsing only the integral definitions and properties of expectation and variance stated above, do the following:\n\n1. Derive the exact expression for $\\operatorname{Var}(f(U))$.\n2. Derive the exact expression for $\\operatorname{Var}\\!\\left(\\frac{f(U)+f(1-U)}{2}\\right)$ for a single antithetic pair.\n3. Hence, for equal computational cost $N$, derive the exact variance of $\\widehat{I}_{N}$ and of $\\widehat{I}^{\\mathrm{anti}}_{N}$.\n4. Define the variance reduction factor $R$ for equal cost as the ratio $R=\\operatorname{Var}(\\widehat{I}^{\\mathrm{anti}}_{N})/\\operatorname{Var}(\\widehat{I}_{N})$, and simplify it to a closed-form expression that does not depend on $N$.\n\nProvide $R$ as your final answer in exact symbolic form. Do not approximate and do not include units. If any intermediate numerical value is used for verification, do not use it in the final result.",
            "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a unique solution. We proceed with the derivation.\n\nLet $U$ be a random variable following a Uniform($0,1$) distribution. Its probability density function is $p(u) = 1$ for $u \\in [0, 1]$ and $0$ otherwise. The function of interest is $f(u) = \\exp(u)$. The integral to be estimated is $I = \\mathbb{E}[f(U)]$.\n\nFirst, we compute the true value of the integral $I$:\n$$I = \\mathbb{E}[f(U)] = \\int_{0}^{1} f(u) p(u) \\, du = \\int_{0}^{1} \\exp(u) \\cdot 1 \\, du = [\\exp(u)]_{0}^{1} = \\exp(1) - \\exp(0) = e-1$$\n\nThis confirms the expectation of our estimators should be $e-1$.\n\n**1. Derivation of $\\operatorname{Var}(f(U))$**\n\nTo compute the variance, we use the formula $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. We first need the second moment of $f(U)$.\n$$\\mathbb{E}[(f(U))^2] = \\mathbb{E}[\\exp(2U)] = \\int_{0}^{1} \\exp(2u) \\, du = \\left[\\frac{1}{2}\\exp(2u)\\right]_{0}^{1} = \\frac{1}{2}(\\exp(2) - \\exp(0)) = \\frac{e^2 - 1}{2}$$\nNow, we can compute the variance of $f(U)$:\n$$\\operatorname{Var}(f(U)) = \\mathbb{E}[(f(U))^2] - (\\mathbb{E}[f(U)])^2 = \\frac{e^2 - 1}{2} - (e-1)^2$$\nExpanding the squared term:\n$$(e-1)^2 = e^2 - 2e + 1$$\nSubstituting this back into the variance expression:\n$$\\operatorname{Var}(f(U)) = \\frac{e^2 - 1}{2} - (e^2 - 2e + 1) = \\frac{e^2 - 1 - 2(e^2 - 2e + 1)}{2} = \\frac{e^2 - 1 - 2e^2 + 4e - 2}{2}$$\n$$\\operatorname{Var}(f(U)) = \\frac{-e^2 + 4e - 3}{2}$$\n\n**2. Derivation of $\\operatorname{Var}\\!\\left(\\frac{f(U)+f(1-U)}{2}\\right)$**\n\nLet $Y = \\frac{f(U)+f(1-U)}{2} = \\frac{\\exp(U)+\\exp(1-U)}{2}$. We need to compute $\\operatorname{Var}(Y)$.\nFirst, we find the expectation of $Y$:\n$$\\mathbb{E}[Y] = \\mathbb{E}\\left[\\frac{\\exp(U)+\\exp(1-U)}{2}\\right] = \\frac{1}{2}(\\mathbb{E}[\\exp(U)] + \\mathbb{E}[\\exp(1-U)])$$\nIf $U \\sim \\text{Uniform}(0,1)$, then the random variable $V = 1-U$ is also distributed as $\\text{Uniform}(0,1)$. Therefore, $\\mathbb{E}[\\exp(1-U)] = \\mathbb{E}[\\exp(V)] = \\mathbb{E}[\\exp(U)] = e-1$.\n$$\\mathbb{E}[Y] = \\frac{1}{2}((e-1) + (e-1)) = e-1$$\nThis shows that the antithetic variate sampling provides an unbiased estimate.\n\nNext, we find the second moment of $Y$:\n$$\\mathbb{E}[Y^2] = \\mathbb{E}\\left[\\left(\\frac{\\exp(U)+\\exp(1-U)}{2}\\right)^2\\right] = \\frac{1}{4}\\mathbb{E}[\\exp(2U) + 2\\exp(U)\\exp(1-U) + \\exp(2(1-U))]$$\nThe middle term simplifies: $2\\exp(U)\\exp(1-U) = 2\\exp(U+1-U) = 2e$.\n$$\\mathbb{E}[Y^2] = \\frac{1}{4}\\mathbb{E}[\\exp(2U) + 2e + \\exp(2-2U)]$$\nUsing linearity of expectation:\n$$\\mathbb{E}[Y^2] = \\frac{1}{4}(\\mathbb{E}[\\exp(2U)] + \\mathbb{E}[2e] + \\mathbb{E}[\\exp(2-2U)])$$\nWe already know $\\mathbb{E}[\\exp(2U)] = \\frac{e^2-1}{2}$. For the last term, we compute the integral:\n$$\\mathbb{E}[\\exp(2-2U)] = \\int_{0}^{1} \\exp(2-2u) \\, du = \\left[-\\frac{1}{2}\\exp(2-2u)\\right]_{0}^{1} = -\\frac{1}{2}(\\exp(0) - \\exp(2)) = \\frac{e^2-1}{2}$$\nSubstituting these results back:\n$$\\mathbb{E}[Y^2] = \\frac{1}{4}\\left(\\frac{e^2-1}{2} + 2e + \\frac{e^2-1}{2}\\right) = \\frac{1}{4}(e^2-1+2e) = \\frac{e^2+2e-1}{4}$$\nNow we compute the variance of $Y$:\n$$\\operatorname{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2 = \\frac{e^2+2e-1}{4} - (e-1)^2 = \\frac{e^2+2e-1 - 4(e^2-2e+1)}{4}$$\n$$\\operatorname{Var}(Y) = \\frac{e^2+2e-1 - 4e^2+8e-4}{4} = \\frac{-3e^2+10e-5}{4}$$\n\n**3. Derivation of the Variances of the Estimators**\n\nThe plain Monte Carlo estimator is $\\widehat{I}_{N} = \\frac{1}{N}\\sum_{i=1}^{N} f(U_{i})$, where $U_i$ are i.i.d. The variance of the mean of $N$ i.i.d. random variables is the variance of a single variable divided by $N$.\n$$\\operatorname{Var}(\\widehat{I}_{N}) = \\frac{1}{N}\\operatorname{Var}(f(U)) = \\frac{1}{N} \\left(\\frac{-e^2 + 4e - 3}{2}\\right) = \\frac{-e^2 + 4e - 3}{2N}$$\nThe antithetic variates estimator is $\\widehat{I}^{\\mathrm{anti}}_{N} = \\frac{1}{N/2}\\sum_{i=1}^{N/2} Y_i$, where $Y_i = \\frac{f(U_i)+f(1-U_i)}{2}$. The random variables $Y_i$ are i.i.d. because the $U_i$ are i.i.d. for $i=1, \\dots, N/2$. The estimator is the mean of $N/2$ i.i.d. samples.\n$$\\operatorname{Var}(\\widehat{I}^{\\mathrm{anti}}_{N}) = \\frac{1}{N/2}\\operatorname{Var}(Y) = \\frac{2}{N}\\left(\\frac{-3e^2+10e-5}{4}\\right) = \\frac{-3e^2+10e-5}{2N}$$\n\n**4. Derivation of the Variance Reduction Factor $R$**\n\nThe variance reduction factor $R$ for equal computational cost $N$ is the ratio of the two variances.\n$$R = \\frac{\\operatorname{Var}(\\widehat{I}^{\\mathrm{anti}}_{N})}{\\operatorname{Var}(\\widehat{I}_{N})} = \\frac{\\frac{-3e^2+10e-5}{2N}}{\\frac{-e^2+4e-3}{2N}}$$\nThe term $2N$ in the denominators cancels out, yielding an expression for $R$ that is independent of $N$:\n$$R = \\frac{-3e^2+10e-5}{-e^2+4e-3}$$\nThis can be rewritten by multiplying the numerator and denominator by $-1$ to have positive leading coefficients:\n$$R = \\frac{3e^2-10e+5}{e^2-4e+3}$$\nBoth forms are equivalent. We present the form derived directly from the positive variance terms.",
            "answer": "$$\\boxed{\\frac{-3e^2+10e-5}{-e^2+4e-3}}$$"
        },
        {
            "introduction": "Moving beyond exploiting input symmetry, control variates reduce variance by leveraging auxiliary information. This powerful method introduces a \"control\" random variable $g(X)$ that is highly correlated with the primary integrand $f(X)$ and whose expectation is known. By subtracting a scaled version of the control's deviation from its mean, we can cancel out a significant portion of the variance in $f(X)$. This practice  first walks you through the general derivation of the optimal scaling coefficient $\\beta^{*}$ and then challenges you to apply it in a concrete setting, revealing how to harness statistical correlation to significantly improve an estimator's precision.",
            "id": "3360574",
            "problem": "Let $X$ be a real-valued random variable with distribution $X \\sim \\mathcal{N}(0,1)$. Consider estimating the expectation $\\mu = \\mathbb{E}[f(X)]$ using a control variate based on a function $g(X)$ whose expectation is known. Let $f(X) = X^{4}$ and $g(X) = X^{2}$. For a scalar coefficient $\\beta \\in \\mathbb{R}$, define the control variate estimator $Y_{\\beta} = f(X) - \\beta\\big(g(X) - \\mathbb{E}[g(X)]\\big)$. Using only the definitions of variance, covariance, and the properties of linearity of expectation, derive from first principles the coefficient $\\beta^{*}$ that minimizes $\\operatorname{Var}(Y_{\\beta})$ over $\\beta$. Then, specializing to the given $f$ and $g$, compute:\n- the optimal coefficient $\\beta^{*}$,\n- the resulting variance $\\operatorname{Var}\\big(X^{4} - \\beta^{*} X^{2}\\big)$,\n- the reduction factor defined as $\\operatorname{Var}\\big(X^{4} - \\beta^{*} X^{2}\\big)\\big/\\operatorname{Var}(X^{4})$.\n\nProvide exact values; no rounding is required. For clarity, note that subtracting a constant does not change variance, so $\\operatorname{Var}\\big(X^{4} - \\beta^{*} X^{2}\\big) = \\operatorname{Var}\\big(X^{4} - \\beta^{*}(X^{2} - \\mathbb{E}[X^{2}])\\big)$ for any $\\beta \\in \\mathbb{R}$. Report your final results in the order listed above.",
            "solution": "The problem is valid as it is scientifically grounded in probability theory, well-posed, objective, and self-contained. We shall proceed with the solution.\n\nThe problem asks for three items: the derivation of the optimal control variate coefficient $\\beta^{*}$, its specific value for the given functions, the resulting variance, and the variance reduction factor.\n\nFirst, we derive the optimal coefficient $\\beta^{*}$ that minimizes the variance of the control variate estimator $Y_{\\beta}$. The estimator is defined as:\n$$\nY_{\\beta} = f(X) - \\beta\\big(g(X) - \\mathbb{E}[g(X)]\\big)\n$$\nwhere $f(X)$ and $g(X)$ are functions of a random variable $X$, and $\\mathbb{E}[g(X)]$ is the known expectation of $g(X)$.\n\nWe seek to minimize $\\operatorname{Var}(Y_{\\beta})$ with respect to the coefficient $\\beta \\in \\mathbb{R}$. We begin by expressing the variance of $Y_{\\beta}$ using the properties of variance. Let $C = \\beta \\mathbb{E}[g(X)]$ be a constant. The variance of a random variable is not affected by adding or subtracting a constant, so:\n$$\n\\operatorname{Var}(Y_{\\beta}) = \\operatorname{Var}\\big(f(X) - \\beta g(X) + \\beta \\mathbb{E}[g(X)]\\big) = \\operatorname{Var}\\big(f(X) - \\beta g(X)\\big)\n$$\nUsing the formula for the variance of a linear combination of random variables, $\\operatorname{Var}(A - B) = \\operatorname{Var}(A) + \\operatorname{Var}(B) - 2\\operatorname{Cov}(A, B)$, we have:\n$$\n\\operatorname{Var}(Y_{\\beta}) = \\operatorname{Var}(f(X)) + \\operatorname{Var}(\\beta g(X)) - 2\\operatorname{Cov}(f(X), \\beta g(X))\n$$\nUsing the properties $\\operatorname{Var}(cZ) = c^2\\operatorname{Var}(Z)$ and $\\operatorname{Cov}(Z_1, cZ_2) = c\\operatorname{Cov}(Z_1, Z_2)$ for a constant $c$, we obtain:\n$$\n\\operatorname{Var}(Y_{\\beta}) = \\operatorname{Var}(f(X)) + \\beta^2 \\operatorname{Var}(g(X)) - 2\\beta \\operatorname{Cov}(f(X), g(X))\n$$\nTo find the value of $\\beta$ that minimizes this variance, we treat $\\operatorname{Var}(Y_{\\beta})$ as a function of $\\beta$ and find its minimum by taking the derivative with respect to $\\beta$ and setting it to zero.\n$$\n\\frac{d}{d\\beta} \\operatorname{Var}(Y_{\\beta}) = \\frac{d}{d\\beta} \\left( \\operatorname{Var}(f(X)) + \\beta^2 \\operatorname{Var}(g(X)) - 2\\beta \\operatorname{Cov}(f(X), g(X)) \\right)\n$$\n$$\n\\frac{d}{d\\beta} \\operatorname{Var}(Y_{\\beta}) = 2\\beta \\operatorname{Var}(g(X)) - 2\\operatorname{Cov}(f(X), g(X))\n$$\nSetting the derivative to zero to find the optimal coefficient $\\beta^{*}$:\n$$\n2\\beta^{*} \\operatorname{Var}(g(X)) - 2\\operatorname{Cov}(f(X), g(X)) = 0\n$$\n$$\n\\beta^{*} \\operatorname{Var}(g(X)) = \\operatorname{Cov}(f(X), g(X))\n$$\nAssuming $\\operatorname{Var}(g(X))  0$ (i.e., $g(X)$ is not a constant), we can solve for $\\beta^{*}$:\n$$\n\\beta^{*} = \\frac{\\operatorname{Cov}(f(X), g(X))}{\\operatorname{Var}(g(X))}\n$$\nThe second derivative, $\\frac{d^2}{d\\beta^2}\\operatorname{Var}(Y_{\\beta}) = 2\\operatorname{Var}(g(X))$, is positive, confirming that this value of $\\beta$ corresponds to a minimum.\n\nNext, we specialize to the given problem where $X \\sim \\mathcal{N}(0,1)$, $f(X) = X^4$, and $g(X) = X^2$. To compute $\\beta^*$, we must find the moments of the standard normal distribution. The $n$-th moment of a standard normal random variable $X$ is $\\mathbb{E}[X^n]$. For $n$ odd, $\\mathbb{E}[X^n] = 0$. For $n$ even, say $n=2k$, the moments are given by $\\mathbb{E}[X^{2k}] = (2k-1)!! = (2k-1)(2k-3)\\cdots 1$.\nWe need the following expectations:\n- $\\mathbb{E}[X^2] = (2-1)!! = 1$\n- $\\mathbb{E}[X^4] = (4-1)!! = 3 \\times 1 = 3$\n- $\\mathbb{E}[X^6] = (6-1)!! = 5 \\times 3 \\times 1 = 15$\n- $\\mathbb{E}[X^8] = (8-1)!! = 7 \\times 5 \\times 3 \\times 1 = 105$\n\nNow we compute the terms in the expression for $\\beta^{*}$:\nThe covariance term is:\n$$\n\\operatorname{Cov}(f(X), g(X)) = \\operatorname{Cov}(X^4, X^2) = \\mathbb{E}[X^4 \\cdot X^2] - \\mathbb{E}[X^4]\\mathbb{E}[X^2] = \\mathbb{E}[X^6] - \\mathbb{E}[X^4]\\mathbb{E}[X^2]\n$$\n$$\n\\operatorname{Cov}(X^4, X^2) = 15 - (3)(1) = 12\n$$\nThe variance term is:\n$$\n\\operatorname{Var}(g(X)) = \\operatorname{Var}(X^2) = \\mathbb{E}[(X^2)^2] - (\\mathbb{E}[X^2])^2 = \\mathbb{E}[X^4] - (\\mathbb{E}[X^2])^2\n$$\n$$\n\\operatorname{Var(X^2)} = 3 - (1)^2 = 2\n$$\nThe optimal coefficient is therefore:\n$$\n\\beta^{*} = \\frac{12}{2} = 6\n$$\nNow, we compute the resulting variance $\\operatorname{Var}\\big(X^4 - \\beta^{*} X^2\\big) = \\operatorname{Var}\\big(X^4 - 6X^2\\big)$.\nLet $W = X^4 - 6X^2$.\n$$\n\\operatorname{Var}(W) = \\mathbb{E}[W^2] - (\\mathbb{E}[W])^2\n$$\nFirst, the expectation of $W$:\n$$\n\\mathbb{E}[W] = \\mathbb{E}[X^4 - 6X^2] = \\mathbb{E}[X^4] - 6\\mathbb{E}[X^2] = 3 - 6(1) = -3\n$$\nNext, the expectation of $W^2$:\n$$\n\\mathbb{E}[W^2] = \\mathbb{E}[(X^4 - 6X^2)^2] = \\mathbb{E}[X^8 - 12X^6 + 36X^4]\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}[W^2] = \\mathbb{E}[X^8] - 12\\mathbb{E}[X^6] + 36\\mathbb{E}[X^4] = 105 - 12(15) + 36(3) = 105 - 180 + 108 = 33\n$$\nThus, the variance is:\n$$\n\\operatorname{Var}(X^4 - 6X^2) = 33 - (-3)^2 = 33 - 9 = 24\n$$\nFinally, we compute the reduction factor, which is the ratio of the controlled variance to the original variance. The original variance is $\\operatorname{Var}(f(X)) = \\operatorname{Var}(X^4)$.\n$$\n\\operatorname{Var}(X^4) = \\mathbb{E}[(X^4)^2] - (\\mathbb{E}[X^4])^2 = \\mathbb{E}[X^8] - (\\mathbb{E}[X^4])^2 = 105 - (3)^2 = 105 - 9 = 96\n$$\nThe reduction factor is:\n$$\n\\frac{\\operatorname{Var}(X^4 - \\beta^{*} X^2)}{\\operatorname{Var}(X^4)} = \\frac{24}{96} = \\frac{1}{4}\n$$\nThe three required values are the optimal coefficient $\\beta^{*} = 6$, the resulting variance $\\operatorname{Var}(X^4 - 6X^2) = 24$, and the reduction factor $1/4$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 6  24  \\frac{1}{4} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Conditional Monte Carlo, an application of the Rao-Blackwell theorem, offers one of the most powerful strategies for variance reduction by analytically integrating out sources of randomness. Instead of simulating a random variable $Y$, we replace it with its conditional expectation given other simulated variables $X$, effectively smoothing the estimator. This comprehensive exercise  provides a capstone experience by bridging theory and computation. You will first derive the conditional estimator and the resulting analytical variance reduction, then implement a simulation to empirically verify that your theoretical findings hold in practice.",
            "id": "3360535",
            "problem": "You are given two independent exponential random variables $X$ and $Y$ with rates $\\lambda_X$ and $\\lambda_Y$, respectively. Consider the indicator function $f(X,Y) = \\mathbf{1}\\{X+Yc\\}$ for a fixed threshold $c0$. The goal is to analyze conditional Monte Carlo as a variance reduction technique, using the conditional expectation $g(X) = \\mathbb{E}[f(X,Y)\\mid X]$ as the integrand, and to quantify the variance reduction compared to crude Monte Carlo.\n\nStarting from the fundamental definition of conditional expectation and the survival function of an exponential random variable, derive an explicit expression for $g(x)=\\mathbb{E}[\\mathbf{1}\\{x+Yc\\}\\mid X=x]$ as a function of $x$, $\\lambda_Y$, and $c$, without assuming any closed-form for $\\mathbb{P}(X+Yc)$ in advance. Using well-tested principles such as the law of total variance and the independence of $X$ and $Y$, express the variance of the crude Monte Carlo integrand $f(X,Y)$ and the variance of the conditional Monte Carlo integrand $g(X)$ in closed form in terms of $\\lambda_X$, $\\lambda_Y$, and $c$. From these, derive a closed-form expression for the variance reduction factor defined by\n$$\n\\mathrm{VRF} = \\frac{\\mathrm{Var}(f(X,Y))}{\\mathrm{Var}(g(X))}.\n$$\nYou must start from the definition of the exponential survival function and conditional expectation, and from the law of total variance, without assuming or quoting any pre-derived formula for $\\mathbb{P}(X+Yc)$ or for any intermediate quantity.\n\nThen, numerically evaluate the variance reduction for the following test suite of parameter values:\n- Test case $1$: $\\lambda_X = 1.5$, $\\lambda_Y = 2.0$, $c = 1.0$.\n- Test case $2$: $\\lambda_X = 1.0$, $\\lambda_Y = 1.0$, $c = 1.0$.\n- Test case $3$: $\\lambda_X = 1.0$, $\\lambda_Y = 0.5$, $c = 1.2$.\n- Test case $4$: $\\lambda_X = 0.1$, $\\lambda_Y = 2.0$, $c = 2.0$.\n\nYour program must:\n- Compute the exact probability $p = \\mathbb{P}(X+Yc)$ for each test case using first principles and analytic integration results consistent with your derivations.\n- Compute the analytic variance reduction factor $\\mathrm{VRF}$ using your derived closed forms for $\\mathrm{Var}(f(X,Y))$ and $\\mathrm{Var}(g(X))$.\n- Estimate the empirical variance reduction factor by Monte Carlo simulation with $N = 500000$ independent draws for each test case, using a fixed random seed of $20241010$ for reproducibility. The empirical variance reduction factor is defined as the ratio of the empirical variance of the crude indicator samples to the empirical variance of the conditional samples. Use population variance (normalization by $N$) for both to avoid estimator bias cancellations.\n- Report, for each test case, a triplet of three floating-point numbers: the exact $p$, the analytic $\\mathrm{VRF}$, and the empirical $\\mathrm{VRF}$. Round each reported float to six decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of lists, where each inner list corresponds to one test case in the order given above and has the form $[p,\\mathrm{VRF}_{\\mathrm{analytic}},\\mathrm{VRF}_{\\mathrm{empirical}}]$. For example:\n\"[[p1,vrfA1,vrfE1],[p2,vrfA2,vrfE2],[p3,vrfA3,vrfE3],[p4,vrfA4,vrfE4]]\".\n\nNo physical units or angle units are involved in this problem. All probabilities must be expressed as decimal numbers. Ensure scientific realism by using only mathematically consistent definitions and derivations grounded in the properties of exponential distributions and conditional expectation.",
            "solution": "The user-provided problem is assessed to be valid. It is self-contained, scientifically grounded in probability theory and stochastic simulation, and well-posed. The problem requests the derivation of analytical expressions for variance reduction in a conditional Monte Carlo setup and their numerical evaluation, which is a standard and meaningful task in the specified field.\n\nThe problem is to analyze the variance reduction achieved by using conditional Monte Carlo to estimate $p = \\mathbb{P}(X+Yc)$, where $X \\sim \\text{Exp}(\\lambda_X)$ and $Y \\sim \\text{Exp}(\\lambda_Y)$ are independent exponential random variables. The crude Monte Carlo estimator uses the integrand $f(X,Y) = \\mathbf{1}\\{X+Yc\\}$, while the conditional Monte Carlo estimator uses the integrand $g(X) = \\mathbb{E}[f(X,Y)|X]$.\n\n**Step 1: Derivation of the Conditional Expectation $g(x)$**\nThe conditional expectation $g(x)$ is defined as $g(x) = \\mathbb{E}[\\mathbf{1}\\{X+Yc\\} \\mid X=x]$. Since $X$ and $Y$ are independent, conditioning on $X=x$ does not alter the distribution of $Y$.\n$$\ng(x) = \\mathbb{E}[\\mathbf{1}\\{x+Yc\\}] = \\mathbb{P}(x+Yc) = \\mathbb{P}(Yc-x)\n$$\nThe survival function of an exponential random variable $Y$ with rate $\\lambda_Y$ is $\\mathbb{P}(Yt) = e^{-\\lambda_Y t}$ for $t \\geq 0$ and $\\mathbb{P}(Yt)=1$ for $t  0$. We must consider two cases for the argument $c-x$:\n1. If $x \\leq c$, then $c-x \\geq 0$, so $\\mathbb{P}(Yc-x) = e^{-\\lambda_Y (c-x)}$.\n2. If $x  c$, then $c-x  0$, so $\\mathbb{P}(Yc-x) = 1$.\nSince $X$ is an exponential random variable, its support is $x \\ge 0$. Thus, we can write $g(x)$ as:\n$$\ng(x) = \\begin{cases} e^{-\\lambda_Y(c-x)}  \\text{if } 0 \\leq x \\leq c \\\\ 1  \\text{if } x  c \\end{cases}\n$$\n\n**Step 2: Derivation of the Exact Probability $p = \\mathbb{P}(X+Yc)$**\nBy the law of total expectation, $p = \\mathbb{E}[f(X,Y)] = \\mathbb{E}[\\mathbb{E}[f(X,Y)|X]] = \\mathbb{E}[g(X)]$. The expectation is computed by integrating $g(x)$ against the probability density function (PDF) of $X$, which is $f_X(x) = \\lambda_X e^{-\\lambda_X x}$ for $x \\ge 0$.\n$$\np = \\int_{0}^{\\infty} g(x) f_X(x) dx = \\int_{0}^{c} g(x) f_X(x) dx + \\int_{c}^{\\infty} g(x) f_X(x) dx\n$$\nSubstituting the expressions for $g(x)$:\n$$\np = \\int_{0}^{c} e^{-\\lambda_Y(c-x)} (\\lambda_X e^{-\\lambda_X x}) dx + \\int_{c}^{\\infty} 1 \\cdot (\\lambda_X e^{-\\lambda_X x}) dx\n$$\nThe second integral is the survival function of $X$ at $c$: $\\int_{c}^{\\infty} \\lambda_X e^{-\\lambda_X x} dx = [-e^{-\\lambda_X x}]_c^\\infty = e^{-\\lambda_X c}$.\nThe first integral is:\n$$\n\\int_{0}^{c} \\lambda_X e^{-\\lambda_Y c} e^{\\lambda_Y x} e^{-\\lambda_X x} dx = \\lambda_X e^{-\\lambda_Y c} \\int_{0}^{c} e^{(\\lambda_Y - \\lambda_X)x} dx\n$$\nWe consider two cases for this integral:\n1. If $\\lambda_X \\neq \\lambda_Y$:\n$$\n\\lambda_X e^{-\\lambda_Y c} \\left[ \\frac{e^{(\\lambda_Y - \\lambda_X)x}}{\\lambda_Y - \\lambda_X} \\right]_0^c = \\frac{\\lambda_X e^{-\\lambda_Y c}}{\\lambda_Y - \\lambda_X} (e^{(\\lambda_Y - \\lambda_X)c} - 1) = \\frac{\\lambda_X}{\\lambda_Y - \\lambda_X}(e^{-\\lambda_X c} - e^{-\\lambda_Y c})\n$$\n2. If $\\lambda_X = \\lambda_Y = \\lambda$:\n$$\n\\lambda e^{-\\lambda c} \\int_{0}^{c} e^{0 \\cdot x} dx = \\lambda e^{-\\lambda c} \\int_{0}^{c} 1 dx = \\lambda c e^{-\\lambda c}\n$$\nCombining the results, we obtain the expression for $p$:\n- If $\\lambda_X \\neq \\lambda_Y$:\n$p = \\frac{\\lambda_X}{\\lambda_Y - \\lambda_X}(e^{-\\lambda_X c} - e^{-\\lambda_Y c}) + e^{-\\lambda_X c} = \\frac{\\lambda_Y e^{-\\lambda_X c} - \\lambda_X e^{-\\lambda_Y c}}{\\lambda_Y - \\lambda_X}$\n- If $\\lambda_X = \\lambda_Y = \\lambda$: $p = \\lambda c e^{-\\lambda c} + e^{-\\lambda c} = (1+\\lambda c)e^{-\\lambda c}$\n\n**Step 3: Derivation of Variances and the Variance Reduction Factor (VRF)**\nThe crude Monte Carlo integrand is $f(X,Y)$, which is a Bernoulli random variable with success probability $p$. Its variance is:\n$$\n\\mathrm{Var}(f(X,Y)) = p(1-p)\n$$\nThe conditional Monte Carlo integrand is $g(X)$. Its variance is $\\mathrm{Var}(g(X)) = \\mathbb{E}[(g(X))^2] - (\\mathbb{E}[g(X)])^2 = \\mathbb{E}[(g(X))^2] - p^2$. We need to compute $\\mathbb{E}[(g(X))^2]$:\n$$\n\\mathbb{E}[(g(X))^2] = \\int_{0}^{\\infty} (g(x))^2 f_X(x) dx = \\int_{0}^{c} (e^{-\\lambda_Y(c-x)})^2 (\\lambda_X e^{-\\lambda_X x}) dx + \\int_{c}^{\\infty} 1^2 \\cdot (\\lambda_X e^{-\\lambda_X x}) dx\n$$\nThe second integral is again $e^{-\\lambda_X c}$. The first integral is:\n$$\n\\int_{0}^{c} \\lambda_X e^{-2\\lambda_Y c} e^{2\\lambda_Y x} e^{-\\lambda_X x} dx = \\lambda_X e^{-2\\lambda_Y c} \\int_{0}^{c} e^{(2\\lambda_Y - \\lambda_X)x} dx\n$$\nWe consider two cases depending on whether $2\\lambda_Y - \\lambda_X = 0$:\n1. If $2\\lambda_Y \\neq \\lambda_X$:\n$$\n\\lambda_X e^{-2\\lambda_Y c} \\left[\\frac{e^{(2\\lambda_Y-\\lambda_X)x}}{2\\lambda_Y - \\lambda_X}\\right]_0^c = \\frac{\\lambda_X e^{-2\\lambda_Y c}}{2\\lambda_Y - \\lambda_X}(e^{(2\\lambda_Y - \\lambda_X)c} - 1) = \\frac{\\lambda_X}{2\\lambda_Y - \\lambda_X}(e^{-\\lambda_X c} - e^{-2\\lambda_Y c})\n$$\n2. If $2\\lambda_Y = \\lambda_X$:\n$$\n\\lambda_X e^{-2\\lambda_Y c} \\int_0^c 1 dx = \\lambda_X c e^{-2\\lambda_Y c}\n$$\nCombining results for $\\mathbb{E}[(g(X))^2]$:\n- If $2\\lambda_Y \\neq \\lambda_X$:\n$\\mathbb{E}[(g(X))^2] = \\frac{\\lambda_X}{2\\lambda_Y - \\lambda_X}(e^{-\\lambda_X c} - e^{-2\\lambda_Y c}) + e^{-\\lambda_X c} = \\frac{2\\lambda_Y e^{-\\lambda_X c} - \\lambda_X e^{-2\\lambda_Y c}}{2\\lambda_Y - \\lambda_X}$\n- If $2\\lambda_Y = \\lambda_X$:\n$\\mathbb{E}[(g(X))^2] = \\lambda_X c e^{-2\\lambda_Y c} + e^{-\\lambda_X c} = \\lambda_X c e^{-\\lambda_X c} + e^{-\\lambda_X c} = (1 + \\lambda_X c) e^{-\\lambda_X c}$\n\nFinally, the variance of the conditional estimator is $\\mathrm{Var}(g(X)) = \\mathbb{E}[(g(X))^2] - p^2$. The variance reduction factor (VRF) is the ratio of the two variances:\n$$\n\\mathrm{VRF} = \\frac{\\mathrm{Var}(f(X,Y))}{\\mathrm{Var}(g(X))} = \\frac{p(1-p)}{\\mathbb{E}[(g(X))^2] - p^2}\n$$\nThe law of total variance states $\\mathrm{Var}(f(X,Y)) = \\mathrm{Var}(g(X)) + \\mathbb{E}[\\mathrm{Var}(f(X,Y)|X)]$. Since $\\mathrm{Var}(f(X,Y)|X) = g(X)(1-g(X)) \\geq 0$, its expectation is also non-negative. This guarantees that $\\mathrm{Var}(f(X,Y)) \\geq \\mathrm{Var}(g(X))$, so $\\mathrm{VRF} \\geq 1$, confirming variance reduction.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by deriving analytical expressions for variance reduction\n    and validating them with Monte Carlo simulation for the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.5, 2.0, 1.0),\n        (1.0, 1.0, 1.0),\n        (1.0, 0.5, 1.2),\n        (0.1, 2.0, 2.0),\n    ]\n\n    results = []\n    N = 500000\n    seed = 20241010\n    rng = np.random.default_rng(seed)\n    \n    TOLERANCE = 1e-9\n\n    for lx, ly, c in test_cases:\n        # --- Analytical Calculations ---\n        \n        # Calculate p = E[g(X)]\n        if abs(lx - ly)  TOLERANCE:\n            # Case lambda_X = lambda_Y\n            p_analytic = (1 + lx * c) * np.exp(-lx * c)\n        else:\n            # Case lambda_X != lambda_Y\n            term1 = ly * np.exp(-lx * c)\n            term2 = lx * np.exp(-ly * c)\n            p_analytic = (term1 - term2) / (ly - lx)\n            \n        # Calculate E[g(X)^2]\n        if abs(lx - 2 * ly)  TOLERANCE:\n            # Case lambda_X = 2 * lambda_Y\n            e_g_sq_analytic = (1 + lx * c) * np.exp(-lx * c)\n        else:\n            # Case lambda_X != 2 * lambda_Y\n            term1 = 2 * ly * np.exp(-lx * c)\n            term2 = lx * np.exp(-2 * ly * c)\n            e_g_sq_analytic = (term1 - term2) / (2 * ly - lx)\n\n        # Calculate analytical variances\n        var_f_analytic = p_analytic * (1 - p_analytic)\n        var_g_analytic = e_g_sq_analytic - p_analytic**2\n        \n        # Ensure variance is non-negative before division\n        if var_g_analytic > 0:\n            vrf_analytic = var_f_analytic / var_g_analytic\n        else:\n            vrf_analytic = np.inf\n\n        # --- Monte Carlo Simulation ---\n        \n        # Generate random samples\n        x_samples = rng.exponential(scale=1/lx, size=N)\n        y_samples = rng.exponential(scale=1/ly, size=N)\n        \n        # Crude Monte Carlo estimator samples\n        f_samples = (x_samples + y_samples > c).astype(float)\n        \n        # Conditional Monte Carlo estimator samples\n        g_samples = np.where(x_samples > c, 1.0, np.exp(-ly * (c - x_samples)))\n        \n        # Calculate empirical population variances (ddof=0)\n        var_f_empirical = np.var(f_samples, ddof=0)\n        var_g_empirical = np.var(g_samples, ddof=0)\n        \n        # Calculate empirical variance reduction factor\n        if var_g_empirical > 0:\n            vrf_empirical = var_f_empirical / var_g_empirical\n        else:\n            vrf_empirical = np.inf\n            \n        # Store results rounded to six decimal places\n        results.append([\n            round(p_analytic, 6),\n            round(vrf_analytic, 6),\n            round(vrf_empirical, 6)\n        ])\n\n    # Final print statement in the exact required format.\n    formatted_results = [f\"[{p},{vrfA},{vrfE}]\" for p, vrfA, vrfE in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}