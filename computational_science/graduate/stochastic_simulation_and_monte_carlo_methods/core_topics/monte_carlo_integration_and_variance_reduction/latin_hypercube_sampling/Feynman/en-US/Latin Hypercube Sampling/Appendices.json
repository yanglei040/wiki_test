{
    "hands_on_practices": [
        {
            "introduction": "Mastering a new technique begins with building it from the ground up. This first practice guides you through the implementation of a Latin Hypercube Sampling (LHS) generator, focusing on the crucial concept of reproducibility in stochastic simulations. By separating the random seed for stratum permutation from the seed for within-stratum jitter, you will see how to create controlled, repeatable experiments and quantify the variance reduction achieved by fixing the stratification across multiple runs .",
            "id": "3317057",
            "problem": "You are asked to implement a self-contained program that constructs a reproducible Latin Hypercube Sampling (LHS) generator, with the ability to store and reuse the underlying permutations to enable reruns and variance estimation. The problem must be solved from first principles in stochastic simulation and Monte Carlo methods, and the program must execute deterministically. The final output must be a single line containing the results of all test cases as a comma-separated list enclosed in square brackets.\n\nFundamental base: You must rely only on core definitions and well-tested properties:\n- The unit hypercube in $d$ dimensions is $[0,1]^d$.\n- A uniform random variable $U$ on $[0,1]$ has distribution function $F_U(u) = u$ for $u \\in [0,1]$.\n- A permutation on $\\{0,1,\\dots,n-1\\}$ is a bijection that reorders indices.\n- Stratification: Partition $[0,1]$ into $n$ disjoint intervals of equal measure, namely $I_k = [k/n,(k+1)/n)$ for $k \\in \\{0,1,\\dots,n-1\\}$.\n- Pseudorandom number generation with a fixed seed is reproducible.\n\nTarget construction: A Latin Hypercube Sampling (LHS) design of size $n$ in $d$ dimensions uses one distinct stratum per dimension for each sample. Concretely:\n- For each dimension $j \\in \\{1,\\dots,d\\}$, choose a random permutation $\\pi_j$ of $\\{0,1,\\dots,n-1\\}$.\n- For each sample index $i \\in \\{1,\\dots,n\\}$ and dimension $j$, define a jitter $U_{i,j} \\sim \\text{Uniform}(0,1)$ independent across $i$ and $j$ and independent of the permutations.\n- The point $X_{i,j} \\in [0,1]$ is then constructed as $X_{i,j} = \\left(\\pi_j(i-1)+U_{i,j}\\right)/n$, yielding the sample matrix $X \\in [0,1]^{n \\times d}$ with the property that in each column $j$ exactly one point lies in each stratum $I_k$.\n\nReproducibility and reruns: The program must separate two random inputs:\n- A base seed that determines and stores the permutations $\\{\\pi_j\\}_{j=1}^d$.\n- A jitter seed that determines the uniform jitter matrix $\\{U_{i,j}\\}$.\nBy storing and reusing $\\{\\pi_j\\}$ while changing only the jitter seed, one obtains reruns that preserve stratum occupancy but produce different points within each stratum. This enables variance estimation of Monte Carlo estimators across replicated LHS runs with common stratification.\n\nProgram requirements:\n- Implement an LHS generator that returns both the sample and the stored permutations when given $n$, $d$, and a base seed.\n- Implement a reconstruction function that takes stored permutations and a jitter seed and generates a new LHS sample reusing the exact permutations and only changing the jitter.\n- Implement a function to compute the Monte Carlo estimator of an integral of a function $f:[0,1]^d \\to \\mathbb{R}$. Use the simple sample mean $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n f(X_i)$.\n- Implement a function to produce multiple replicated LHS estimators with either fixed stored permutations or independently regenerated permutations for each replicate. For variance estimation across $R$ replicates with estimates $\\{\\hat{\\mu}_r\\}_{r=1}^R$, compute the unbiased sample variance $s^2 = \\frac{1}{R-1}\\sum_{r=1}^R (\\hat{\\mu}_r - \\bar{\\mu})^2$, where $\\bar{\\mu} = \\frac{1}{R}\\sum_{r=1}^R \\hat{\\mu}_r$.\n\nFunctions to be used in variance estimation:\n- Define $f_1(x) = \\sum_{j=1}^d x_j^2$ for $x \\in [0,1]^d$.\n- For the edge case, use $f_0(x) = x_1$ for $x \\in [0,1]^1$ only.\n\nTest suite and required outputs:\nYour program must implement the following four test cases and output the corresponding results:\n\n- Test $1$ (Happy path coverage check):\n  - Parameters: $n=5$, $d=3$, base seed $=314159$, jitter seed $=271828$.\n  - Task: Generate one LHS sample and verify that the per-dimension stratum coverage is exact, i.e., for each dimension $j$, the multiset $\\{\\lfloor n X_{i,j}\\rfloor : i=1,\\dots,n\\}$ equals $\\{0,1,2,3,4\\}$.\n  - Output: A boolean equal to $true$ if and only if all three dimensions pass.\n\n- Test $2$ (Variance estimation with fixed permutations across replicates):\n  - Parameters: $n=50$, $d=4$, base seed $=12345$, jitter seeds $\\in \\{100,101,102,103,104\\}$ (five replicates using the same stored permutations), and function $f_1$.\n  - Task: Generate five replicate LHS samples reusing the stored permutations and compute the unbiased sample variance $s^2$ of the five Monte Carlo estimates of $\\int_{[0,1]^d} f_1(x)\\,dx$.\n  - Output: A single float $s^2$.\n\n- Test $3$ (Variance comparison with independent permutations across replicates):\n  - Parameters: $n=50$, $d=4$, base seeds $\\in \\{200,201,202,203,204\\}$ (five replicates using independently regenerated permutations for each replicate), jitter seeds identical to Test $2$, and function $f_1$.\n  - Task: Compute the unbiased sample variance $s^2_{\\text{indep}}$ of the five Monte Carlo estimates with independently regenerated permutations and output the ratio $r = s^2_{\\text{fixed}}/s^2_{\\text{indep}}$, where $s^2_{\\text{fixed}}$ is from Test $2$.\n  - Output: A single float $r$.\n\n- Test $4$ (Edge case $n=1$, $d=1$):\n  - Parameters: $n=1$, $d=1$, base seed $=777$, jitter seed $=888$, and function $f_0$.\n  - Task: Generate the single-point LHS sample and report the sample value $X_{1,1}$.\n  - Output: A single float equal to $X_{1,1}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results of Tests $1$ through $4$ as a comma-separated list enclosed in square brackets. The list items must appear in the order of the tests; for example, a list with a boolean followed by three floats must look like $[true,0.12345,0.6789,0.2468]$. No additional text should be printed.\n\nAngle units: Not applicable. Physical units: Not applicable. Percentages: Not applicable.\n\nYour solution must be language-agnostic in description but rendered in an actual runnable program in the Final Answer section. Ensure scientific realism, internal consistency, and that all floating-point outputs are derived deterministically from the specified seeds and are therefore reproducible.",
            "solution": "The problem requires the implementation of a reproducible Latin Hypercube Sampling (LHS) generator from first principles, emphasizing the ability to separate the randomization of stratum permutations from the randomization of jitter within strata. This separation is crucial for variance reduction techniques in Monte Carlo methods, where replicates with common random numbers (in this case, common stratification) are used.\n\nA Latin hypercube sample of size $n$ in $d$ dimensions is a set of $n$ points $\\{X_1, X_2, \\dots, X_n\\}$ in the unit hypercube $[0,1]^d$. The core property of LHS is that it stratifies the marginal distributions. For each dimension $j \\in \\{1, \\dots, d\\}$, the domain $[0,1]$ is divided into $n$ disjoint intervals of equal length, $I_k = [k/n, (k+1)/n)$ for $k \\in \\{0, \\dots, n-1\\}$. The sampling design ensures that for each dimension, exactly one sample point falls into each of these $n$ intervals.\n\nThe construction of the sample matrix $X \\in \\mathbb{R}^{n \\times d}$ is as follows.\nFirst, for each dimension $j \\in \\{1, \\dots, d\\}$, we generate a random permutation, $\\pi_j$, of the set of indices $\\{0, 1, \\dots, n-1\\}$. These $d$ permutations define the stratification structure of the sample.\nSecond, we generate an $n \\times d$ matrix of jitter values, $U$, where each element $U_{i,j}$ is an independent random variable drawn from a uniform distribution on $[0,1]$, i.e., $U_{i,j} \\sim \\text{Uniform}(0,1)$.\n\nThe $j$-th coordinate of the $i$-th sample point, $X_{i,j}$, is then constructed by combining the permutation and jitter:\n$$X_{i,j} = \\frac{\\pi_j(i-1) + U_{i,j}}{n}$$\nHere, we interpret $\\pi_j(i-1)$ as the $i$-th element (using $1$-based indexing for $i$, so $(i-1)$-th for $0$-based array indexing) of the array representing the $j$-th permutation. This ensures that for any fixed dimension $j$, the set of values $\\{\\lfloor n X_{i,j} \\rfloor\\}_{i=1}^n$ is precisely the set $\\{0, 1, \\dots, n-1\\}$, satisfying the stratification property.\n\nReproducibility and variance analysis rely on controlling the sources of randomness. The problem specifies two seeds:\n1.  A `base_seed` to control the generation of the set of $d$ permutations, $\\{\\pi_j\\}_{j=1}^d$.\n2.  A `jitter_seed` to control the generation of the jitter matrix, $U = \\{U_{i,j}\\}$.\n\nBy fixing the `base_seed` (and thus the permutations) while using different `jitter_seed`s for multiple runs, we generate replicated samples that share the same stratification structure. This often introduces positive correlation between the outputs of an estimator, leading to a variance reduction in the estimator of the mean performance. The problem tests this by comparing the variance of Monte Carlo estimates under two scenarios:\n- **Fixed Permutations**: One set of permutations is generated and reused across all replicates, with only the jitter being resampled. This is expected to yield a lower variance for many integrands.\n- **Independent Permutations**: Each replicate is generated with a new, independent set of permutations and jitters. This corresponds to standard independent replications of the entire LHS experiment.\n\nThe Monte Carlo estimator for the integral $I = \\int_{[0,1]^d} f(x) \\, dx$ is the sample mean of the function evaluated at the LHS points:\n$$\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n f(X_i)$$\nGiven $R$ such estimates from $R$ replicates, $\\{\\hat{\\mu}_r\\}_{r=1}^R$, the unbiased sample variance is computed as:\n$$s^2 = \\frac{1}{R-1} \\sum_{r=1}^R (\\hat{\\mu}_r - \\bar{\\mu})^2$$\nwhere $\\bar{\\mu} = \\frac{1}{R} \\sum_{r=1}^R \\hat{\\mu}_r$ is the mean of the estimates.\n\nThe implementation will consist of functions to perform these steps deterministically using `numpy`'s pseudorandom number generator, seeded as specified.\n\n**Test Case 1** directly verifies the fundamental stratification property of the generated LHS sample. For a sample of size $n=5$ in $d=3$, it checks that for each of the $3$ dimensions, the sample contains exactly one point in each of the $5$ strata $[0/5, 1/5), [1/5, 2/5), \\dots, [4/5, 5/5)$.\n\n**Test Case 2** computes the variance $s^2_{\\text{fixed}}$ across $R=5$ replicates using a single `base_seed` (fixed permutations) and five distinct `jitter_seed`s. This measures the variability of the estimator when only within-stratum randomness is changed.\n\n**Test Case 3** computes the variance $s^2_{\\text{indep}}$ across $R=5$ replicates where each replicate uses an independent `base_seed` and `jitter_seed`. This represents the total variability of the estimator. The ratio $r = s^2_{\\text{fixed}} / s^2_{\\text{indep}}$ quantifies the variance reduction achieved by fixing the stratification. For additive functions like $f_1(x) = \\sum_{j=1}^d x_j^2$, LHS is known to be particularly effective, so we expect this ratio to be less than $1$.\n\n**Test Case 4** examines the edge case of $n=1$ and $d=1$. For $n=1$, the only stratum is $[0,1)$, and the only permutation of $\\{0\\}$ is $(0)$. The formula becomes $X_{1,1} = (0 + U_{1,1})/1 = U_{1,1}$. The output is simply a single draw from a uniform distribution determined by the `jitter_seed`.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# from scipy import ... is not used.\n\ndef generate_lhs_permutations(n, d, base_seed):\n    \"\"\"\n    Generates d random permutations of {0, 1, ..., n-1} using a base seed.\n    These permutations define the stratification for the LHS design.\n    \"\"\"\n    base_rng = np.random.default_rng(base_seed)\n    # A list of d arrays, where each array is a permutation of arange(n).\n    permutations = [base_rng.permutation(n) for _ in range(d)]\n    return permutations\n\ndef construct_lhs_sample(permutations, n, d, jitter_seed):\n    \"\"\"\n    Constructs an LHS sample using pre-computed permutations and a jitter seed.\n    \"\"\"\n    jitter_rng = np.random.default_rng(jitter_seed)\n    # Generate an n x d matrix of uniform random numbers for jitter.\n    jitters = jitter_rng.uniform(size=(n, d))\n    \n    # Initialize the sample matrix.\n    sample = np.empty((n, d))\n    \n    # Construct the sample column by column.\n    for j in range(d):\n        perm_j = permutations[j]\n        jitter_j = jitters[:, j]\n        sample[:, j] = (perm_j + jitter_j) / n\n        \n    return sample\n\ndef f1(x):\n    \"\"\"Integrand for Tests 2 and 3: f(x) = sum(x_j^2).\"\"\"\n    # x is an n x d matrix. sum over the second axis (d dimensions).\n    return np.sum(x**2, axis=1)\n\ndef f0(x):\n    \"\"\"Integrand for Test 4: f(x) = x_1.\"\"\"\n    # x is an n x 1 matrix.\n    return x[:, 0]\n\ndef monte_carlo_estimator(func, sample):\n    \"\"\"Computes the Monte Carlo estimate for a given function and sample.\"\"\"\n    f_values = func(sample)\n    return np.mean(f_values)\n\ndef solve():\n    \"\"\"\n    Executes the four test cases and prints the results in the required format.\n    \"\"\"\n    results = []\n\n    # --- Test 1: Happy path coverage check ---\n    n1, d1, base_seed1, jitter_seed1 = 5, 3, 314159, 271828\n    perms1 = generate_lhs_permutations(n1, d1, base_seed1)\n    sample1 = construct_lhs_sample(perms1, n1, d1, jitter_seed1)\n    \n    is_valid_lhs = True\n    expected_strata = np.arange(n1)\n    for j in range(d1):\n        strata_indices = np.floor(n1 * sample1[:, j])\n        if not np.array_equal(np.sort(strata_indices), expected_strata):\n            is_valid_lhs = False\n            break\n    results.append(str(is_valid_lhs).lower())\n\n    # --- Test 2: Variance with fixed permutations ---\n    n2, d2, base_seed2 = 50, 4, 12345\n    jitter_seeds2 = [100, 101, 102, 103, 104]\n    \n    perms2 = generate_lhs_permutations(n2, d2, base_seed2)\n    estimates_fixed = []\n    for js in jitter_seeds2:\n        sample = construct_lhs_sample(perms2, n2, d2, js)\n        estimate = monte_carlo_estimator(f1, sample)\n        estimates_fixed.append(estimate)\n    \n    s2_fixed = np.var(estimates_fixed, ddof=1)\n    results.append(s2_fixed)\n\n    # --- Test 3: Variance with independent permutations and ratio ---\n    n3, d3 = 50, 4\n    base_seeds3 = [200, 201, 202, 203, 204]\n    jitter_seeds3 = [100, 101, 102, 103, 104]\n    \n    estimates_indep = []\n    for bs, js in zip(base_seeds3, jitter_seeds3):\n        perms = generate_lhs_permutations(n3, d3, bs)\n        sample = construct_lhs_sample(perms, n3, d3, js)\n        estimate = monte_carlo_estimator(f1, sample)\n        estimates_indep.append(estimate)\n        \n    s2_indep = np.var(estimates_indep, ddof=1)\n    ratio = s2_fixed / s2_indep\n    results.append(ratio)\n\n    # --- Test 4: Edge case n=1, d=1 ---\n    n4, d4, base_seed4, jitter_seed4 = 1, 1, 777, 888\n    \n    perms4 = generate_lhs_permutations(n4, d4, base_seed4)\n    # perms4 will be [array([0])]\n    sample4 = construct_lhs_sample(perms4, n4, d4, jitter_seed4)\n    # For n=1, sample value X_11 is just the jitter U_11.\n    result4 = sample4[0, 0]\n    results.append(result4)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Now that you can build an LHS, the next step is to understand why it is so effective. This exercise provides a sharp, analytical demonstration of the variance reduction power of LHS by comparing it against a standard two-dimensional stratified sampling scheme for the purely additive function $f(x,y) = x+y$. By deriving the variance of both estimators, you will quantify the significant advantage that LHS's superior projection properties provide in this ideal scenario .",
            "id": "3285740",
            "problem": "Consider estimating the integral $\\mu$ of the function $f(x,y)=x+y$ over the unit square with respect to the product measure of two independent Uniform$(0,1)$ random variables. That is, let $(X,Y)$ be independent with $X \\sim \\text{Uniform}(0,1)$ and $Y \\sim \\text{Uniform}(0,1)$, and define $\\mu = \\mathbb{E}[f(X,Y)]$. You will compare two sampling strategies under a fixed computational budget quantified by the number of function evaluations.\n\nStrategy A: Latin Hypercube Sampling (LHS). Draw $N$ samples $\\{(X_{i},Y_{\\pi(i)})\\}_{i=1}^{N}$ where $\\{X_{i}\\}_{i=1}^{N}$ are independent with $X_{i} \\sim \\text{Uniform}\\!\\left(\\frac{i-1}{N},\\frac{i}{N}\\right)$, $\\{Y_{j}\\}_{j=1}^{N}$ are independent with $Y_{j} \\sim \\text{Uniform}\\!\\left(\\frac{j-1}{N},\\frac{j}{N}\\right)$, and $\\pi$ is a random permutation of $\\{1,2,\\dots,N\\}$ independent of the draws. The estimator is the sample average $\\hat{\\mu}_{\\text{LHS}}=\\frac{1}{N}\\sum_{i=1}^{N}f\\!\\left(X_{i},Y_{\\pi(i)}\\right)$.\n\nStrategy B: Two-dimensional stratified sampling on an $m \\times m$ grid with one sample per stratum, where $N$ is a perfect square and $m=\\sqrt{N}$. Partition $[0,1]^{2}$ into $m \\times m$ equal square cells, and in each cell sample one point uniformly at random within the cell. The estimator is the sample average $\\hat{\\mu}_{\\text{STRAT}}=\\frac{1}{N}\\sum_{k=1}^{N}f\\!\\left(X^{(k)},Y^{(k)}\\right)$, where $(X^{(k)},Y^{(k)})$ is uniform on the $k$-th cell and the $N$ cellwise draws are independent.\n\nUsing only foundational definitions of expectation and variance, independence of coordinates under uniform sampling, and properties of the Uniform distribution on an interval, derive the variance of each estimator and then quantify the improvement of Strategy A over Strategy B by the ratio\n$$\\mathcal{R}(N) = \\frac{\\operatorname{Var}\\!\\left(\\hat{\\mu}_{\\text{STRAT}}\\right)}{\\operatorname{Var}\\!\\left(\\hat{\\mu}_{\\text{LHS}}\\right)}.$$\nExpress $\\mathcal{R}(N)$ as a closed-form analytic expression in terms of $N$. Provide a brief explanation of the result grounded in the structure of the function and the sampling designs. Your final answer must be the single expression for $\\mathcal{R}(N)$; no rounding is required and no units are involved.",
            "solution": "The problem is well-posed and scientifically grounded, so we proceed with the solution.\n\nThe integral to be estimated is $\\mu = \\mathbb{E}[f(X,Y)]$ where $f(x,y)=x+y$ and $X, Y$ are independent random variables distributed as $\\text{Uniform}(0,1)$. The linearity of expectation yields $\\mu = \\mathbb{E}[X] + \\mathbb{E}[Y] = \\frac{1}{2} + \\frac{1}{2} = 1$.\n\nWe will derive the variance of each estimator, $\\hat{\\mu}_{\\text{STRAT}}$ and $\\hat{\\mu}_{\\text{LHS}}$, and then compute their ratio.\n\n### Variance of the Stratified Sampling Estimator ($\\operatorname{Var}(\\hat{\\mu}_{\\text{STRAT}})$)\n\nStrategy B uses stratified sampling on an $m \\times m$ grid, where $N=m^2$ is the total number of samples. One sample is drawn uniformly from each of the $N$ square cells. The estimator is given by:\n$$ \\hat{\\mu}_{\\text{STRAT}} = \\frac{1}{N} \\sum_{k=1}^{N} f(X^{(k)}, Y^{(k)}) $$\nwhere $(X^{(k)}, Y^{(k)})$ is a sample drawn uniformly from the $k$-th cell. The draws from different cells are independent. Due to this independence, the variance of the estimator is:\n$$ \\operatorname{Var}(\\hat{\\mu}_{\\text{STRAT}}) = \\operatorname{Var}\\left(\\frac{1}{N} \\sum_{k=1}^{N} f(X^{(k)}, Y^{(k)})\\right) = \\frac{1}{N^2} \\sum_{k=1}^{N} \\operatorname{Var}(f(X^{(k)}, Y^{(k)})) $$\nLet's denote the cells by $S_{ij} = [\\frac{i-1}{m}, \\frac{i}{m}] \\times [\\frac{j-1}{m}, \\frac{j}{m}]$ for $i,j \\in \\{1, 2, \\dots, m\\}$. A sample $(X_{ij}, Y_{ij})$ drawn from cell $S_{ij}$ consists of two independent random variables: $X_{ij} \\sim \\text{Uniform}(\\frac{i-1}{m}, \\frac{i}{m})$ and $Y_{ij} \\sim \\text{Uniform}(\\frac{j-1}{m}, \\frac{j}{m})$.\nFor the function $f(x,y)=x+y$, the variance within cell $S_{ij}$ is:\n$$ \\operatorname{Var}(f(X_{ij}, Y_{ij})) = \\operatorname{Var}(X_{ij} + Y_{ij}) $$\nSince $X_{ij}$ and $Y_{ij}$ are independent, this becomes:\n$$ \\operatorname{Var}(X_{ij} + Y_{ij}) = \\operatorname{Var}(X_{ij}) + \\operatorname{Var}(Y_{ij}) $$\nThe variance of a uniform distribution on an interval of length $L$ is $\\frac{L^2}{12}$. Both intervals for $X_{ij}$ and $Y_{ij}$ have length $L = \\frac{1}{m}$.\n$$ \\operatorname{Var}(X_{ij}) = \\frac{(1/m)^2}{12} = \\frac{1}{12m^2} $$\n$$ \\operatorname{Var}(Y_{ij}) = \\frac{(1/m)^2}{12} = \\frac{1}{12m^2} $$\nTherefore, the variance within any cell is:\n$$ \\operatorname{Var}(f(X_{ij}, Y_{ij})) = \\frac{1}{12m^2} + \\frac{1}{12m^2} = \\frac{2}{12m^2} = \\frac{1}{6m^2} $$\nThis variance is the same for all $N=m^2$ cells. Let's call it $\\sigma_{\\text{cell}}^2$.\nThe total variance of the estimator is:\n$$ \\operatorname{Var}(\\hat{\\mu}_{\\text{STRAT}}) = \\frac{1}{N^2} \\sum_{k=1}^{N} \\sigma_{\\text{cell}}^2 = \\frac{1}{N^2} (N \\cdot \\frac{1}{6m^2}) = \\frac{1}{6Nm^2} $$\nSubstituting $m^2=N$, we get:\n$$ \\operatorname{Var}(\\hat{\\mu}_{\\text{STRAT}}) = \\frac{1}{6N \\cdot N} = \\frac{1}{6N^2} $$\n\n### Variance of the Latin Hypercube Sampling Estimator ($\\operatorname{Var}(\\hat{\\mu}_{\\text{LHS}})$)\n\nStrategy A uses Latin Hypercube Sampling with $N$ samples. The estimator is:\n$$ \\hat{\\mu}_{\\text{LHS}} = \\frac{1}{N} \\sum_{i=1}^{N} f(X_i, Y_{\\pi(i)}) = \\frac{1}{N} \\sum_{i=1}^{N} (X_i + Y_{\\pi(i)}) $$\nwhere $X_i \\sim \\text{Uniform}(\\frac{i-1}{N}, \\frac{i}{N})$, $Y_j \\sim \\text{Uniform}(\\frac{j-1}{N}, \\frac{j}{N})$, and $\\pi$ is a random permutation of $\\{1, 2, \\dots, N\\}$. The collections of variables $\\{X_i\\}_{i=1}^N$ and $\\{Y_j\\}_{j=1}^N$ are independent of each other, and of the permutation $\\pi$.\n\nDue to the additive nature of $f(x,y)$, the estimator can be split:\n$$ \\hat{\\mu}_{\\text{LHS}} = \\frac{1}{N}\\sum_{i=1}^{N} X_i + \\frac{1}{N}\\sum_{i=1}^{N} Y_{\\pi(i)} $$\nThe second sum is a permutation of the variables $\\{Y_1, \\dots, Y_N\\}$, so $\\sum_{i=1}^{N} Y_{\\pi(i)} = \\sum_{j=1}^{N} Y_j$. Thus, the estimator is:\n$$ \\hat{\\mu}_{\\text{LHS}} = \\left(\\frac{1}{N}\\sum_{i=1}^{N} X_i\\right) + \\left(\\frac{1}{N}\\sum_{j=1}^{N} Y_j\\right) $$\nThe set of variables $\\{X_i\\}$ is independent of the set $\\{Y_j\\}$. Therefore, the variance of the sum is the sum of the variances:\n$$ \\operatorname{Var}(\\hat{\\mu}_{\\text{LHS}}) = \\operatorname{Var}\\left(\\frac{1}{N}\\sum_{i=1}^{N} X_i\\right) + \\operatorname{Var}\\left(\\frac{1}{N}\\sum_{j=1}^{N} Y_j\\right) $$\nThe variables $X_1, \\dots, X_N$ are independent. So are $Y_1, \\dots, Y_N$.\n$$ \\operatorname{Var}\\left(\\frac{1}{N}\\sum_{i=1}^{N} X_i\\right) = \\frac{1}{N^2} \\sum_{i=1}^{N} \\operatorname{Var}(X_i) $$\nFor each $i$, $X_i$ is uniform on an interval of length $\\frac{1}{N}$. Its variance is $\\operatorname{Var}(X_i) = \\frac{(1/N)^2}{12} = \\frac{1}{12N^2}$.\n$$ \\operatorname{Var}\\left(\\frac{1}{N}\\sum_{i=1}^{N} X_i\\right) = \\frac{1}{N^2} \\sum_{i=1}^{N} \\frac{1}{12N^2} = \\frac{1}{N^2} \\left(N \\cdot \\frac{1}{12N^2}\\right) = \\frac{1}{12N^3} $$\nSimilarly, for the $Y$ component:\n$$ \\operatorname{Var}\\left(\\frac{1}{N}\\sum_{j=1}^{N} Y_j\\right) = \\frac{1}{12N^3} $$\nAdding the two variances gives the total variance for the LHS estimator:\n$$ \\operatorname{Var}(\\hat{\\mu}_{\\text{LHS}}) = \\frac{1}{12N^3} + \\frac{1}{12N^3} = \\frac{2}{12N^3} = \\frac{1}{6N^3} $$\nNote that for this additive function, the variance is independent of the random permutation $\\pi$.\n\n### Ratio of Variances\n\nFinally, we compute the ratio $\\mathcal{R}(N) = \\frac{\\operatorname{Var}(\\hat{\\mu}_{\\text{STRAT}})}{\\operatorname{Var}(\\hat{\\mu}_{\\text{LHS}})}$:\n$$ \\mathcal{R}(N) = \\frac{1/(6N^2)}{1/(6N^3)} = \\frac{6N^3}{6N^2} = N $$\n\nThe improvement of LHS over stratified sampling for this specific problem is a factor of $N$. This significant improvement is due to the additive structure of the integrand $f(x,y)=x+y$. LHS effectively ensures that the sample projections onto each axis are perfectly stratified into $N$ bins. This is equivalent to performing two independent one-dimensional stratified sampling estimations, each with $N$ strata, for $\\mathbb{E}[X]$ and $\\mathbb{E}[Y]$. The variance of such a 1D estimator for a linear function scales as $O(N^{-3})$. In contrast, the $m \\times m$ stratified sampling projects to only $m=\\sqrt{N}$ strata on each axis, leading to a variance that scales as $O(m^{-4}) = O(N^{-2})$. The ratio of the two variances is therefore $O(N)$.",
            "answer": "$$\\boxed{N}$$"
        },
        {
            "introduction": "While standard LHS guarantees excellent stratification on each input dimension individually, its random nature can introduce spurious correlations between dimensions, which is undesirable in many applications. This final practice moves into the realm of design optimization, challenging you to implement an algorithm that improves upon a basic LHS design by actively minimizing these correlations. You will use a greedy, iterative swap-based approach to enhance the space-filling properties of the sample, a key technique in the field of design of computer experiments .",
            "id": "3317028",
            "problem": "You are given the task of formalizing and implementing a correlation-minimizing scheme for Latin Hypercube Sampling (LHS), suitable for high-dimensional stochastic simulation and Monte Carlo methods. A Latin Hypercube Sample (LHS) of size $N$ in $m$ dimensions is a matrix $X \\in \\mathbb{R}^{N \\times m}$ such that, for each column $j \\in \\{1,\\dots,m\\}$, the $N$ entries $X_{1j},\\dots,X_{Nj}$ occupy $N$ disjoint strata formed by partitioning the unit interval $[0,1]$ into $N$ equal subintervals. A standard randomized construction assigns to each row $i \\in \\{1,\\dots,N\\}$ and each column $j \\in \\{1,\\dots,m\\}$ a value of the form $(\\pi_j(i) + U_{ij}) / N$, where $\\pi_j$ is a permutation of $\\{0,1,\\dots,N-1\\}$ and $U_{ij}$ are independent and identically distributed random variables uniform on $[0,1)$. This ensures that each stratum is occupied exactly once per column.\n\nFor any column $j$, let $\\mu_j$ denote the sample mean and let $\\sigma_j$ denote the sample standard deviation (with divisor $N$). Define the standardized matrix $Z \\in \\mathbb{R}^{N \\times m}$ by $Z_{ij} = (X_{ij} - \\mu_j)/\\sigma_j$. The Pearson correlation between columns $j$ and $k$ is given by\n$$\nr_{jk} \\;=\\; \\frac{1}{N}\\sum_{i=1}^{N} Z_{ij} Z_{ik}.\n$$\nDefine the minimum column-correlation criterion (objective) as\n$$\n\\Phi(X) \\;=\\; \\sum_{1 \\le j < k \\le m} \\left| r_{jk} \\right|.\n$$\nThe goal is to decrease $\\Phi(X)$ via swaps that preserve the LHS property.\n\nYou must implement the following targeted-swap algorithmic scheme to reduce column correlations:\n\n- Initialization:\n  - Construct an initial randomized Latin Hypercube Sample $X \\in [0,1]^{N \\times m}$ using stratified uniform jitter as described above.\n  - Standardize the columns to obtain $Z$.\n  - Compute the correlation matrix entries $r_{jk}$ and $\\Phi(X)$.\n\n- Iterative targeted-swap procedure:\n  - At each iteration, compute for each column $j$ its correlation burden $s_j = \\sum_{k \\ne j} |r_{jk}|$.\n  - Select the column $c$ with the largest $s_c$.\n  - Consider swaps that exchange the two entries in column $c$ at two distinct rows $i$ and $k$. Such a swap preserves the LHS property because it permutes entries within a column and does not alter the multiset of strata occupied by that column.\n  - Among all possible row pairs $(i,k)$ with $1 \\le i < k \\le N$, choose the single swap in column $c$ that yields the largest strict decrease in $\\Phi(X)$. If no swap yields a strict decrease, terminate the procedure. Otherwise, perform that swap, update the correlation quantities, and repeat until either termination or a maximum number of accepted swaps $S_{\\max}$ is reached.\n\nYour program must implement the above and return the final value of $\\Phi(X)$ after the algorithm terminates for each parameter set in the provided test suite. All randomization must be made reproducible by the specified seeds.\n\nTest suite. For each case, the inputs are the LHS size $N$, the dimension $m$, the random seed for the generator used to construct the LHS, and the maximum number of accepted swaps $S_{\\max}$:\n- Case A (general): $N = 20$, $m = 5$, seed $= 314159$, $S_{\\max} = 2000$.\n- Case B (boundary, one dimension): $N = 8$, $m = 1$, seed $= 271828$, $S_{\\max} = 2000$.\n- Case C (higher dimension with capped iterations): $N = 30$, $m = 10$, seed $= 424242$, $S_{\\max} = 300$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order [Case A result, Case B result, Case C result], where each result is the final value of $\\Phi(X)$ after the algorithm terminates, expressed as a decimal rounded to six digits after the decimal point (e.g., $[0.123456,0.000000,0.654321]$). No additional text should be printed.",
            "solution": "The problem requires the implementation of a specific iterative algorithm to reduce spurious correlations in a Latin Hypercube Sample (LHS). The algorithm is a greedy local search procedure that seeks to minimize an objective function, $\\Phi(X)$, defined as the sum of the absolute values of the pairwise Pearson correlations between the columns of the sample matrix. The search proceeds by iteratively swapping pairs of elements within the column that contributes most to the total correlation.\n\nThe solution is developed by following the algorithmic steps outlined in the problem statement: initialization, and an iterative targeted-swap procedure.\n\n### 1. Initialization\n\nFirst, an initial Latin Hypercube Sample matrix $X \\in \\mathbb{R}^{N \\times m}$ must be constructed. According to the problem, for a sample of size $N$ in $m$ dimensions, each column $j \\in \\{1, \\dots, m\\}$ must contain one value from each of the $N$ disjoint strata $[k/N, (k+1)/N)$ for $k \\in \\{0, \\dots, N-1\\}$. This is achieved through a randomized stratified sampling procedure.\n\nFor each column $j$:\n1.  A random permutation $\\pi_j$ of the set of integers $\\{0, 1, \\dots, N-1\\}$ is generated. This assigns a unique stratum index to each of the $N$ rows.\n2.  A vector of $N$ independent random numbers $U_{ij}$ is drawn from the uniform distribution on $[0, 1)$, for $i \\in \\{1, \\dots, N\\}$.\n3.  The entries of the $j$-th column of the matrix $X$ are then calculated as:\n    $$\n    X_{ij} = \\frac{\\pi_j(i) + U_{ij}}{N}\n    $$\nThis construction guarantees the LHS property. All randomization is seeded to ensure reproducibility.\n\nNext, the sample matrix $X$ is standardized. For each column $j$, we compute the sample mean $\\mu_j$ and the sample standard deviation $\\sigma_j$ (using a divisor of $N$ as specified).\n$$\n\\mu_j = \\frac{1}{N}\\sum_{i=1}^{N} X_{ij} \\quad \\text{and} \\quad \\sigma_j = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N} (X_{ij} - \\mu_j)^2}\n$$\nThe standardized matrix $Z \\in \\mathbb{R}^{N \\times m}$ is then obtained by:\n$$\nZ_{ij} = \\frac{X_{ij} - \\mu_j}{\\sigma_j}\n$$\nBy construction, each column of $Z$ has a mean of $0$ and a standard deviation of $1$.\n\nThe Pearson correlation coefficient $r_{jk}$ between any two columns $j$ and $k$ is computed from the standardized matrix. The matrix of these coefficients, $R$, can be calculated efficiently using a matrix product:\n$$\nr_{jk} = \\frac{1}{N}\\sum_{i=1}^{N} Z_{ij} Z_{ik} \\quad \\implies \\quad R = \\frac{1}{N} Z^T Z\n$$\nNote that the diagonal elements $r_{jj}$ are always equal to $1$.\n\nFinally, the initial value of the objective function $\\Phi(X)$ is computed:\n$$\n\\Phi(X) = \\sum_{1 \\le j < k \\le m} |r_{jk}|\n$$\nThis represents the total magnitude of off-diagonal correlations, which the algorithm aims to minimize. For the edge case where $m=1$, this sum is empty and its value is $0$.\n\n### 2. Iterative Targeted-Swap Procedure\n\nThe core of the algorithm is an iterative loop that performs a greedy search for correlation-reducing swaps. The loop continues until no further improvement can be found or a maximum number of accepted swaps, $S_{\\max}$, is reached.\n\nAt each iteration, the following steps are performed:\n\n1.  **Select Target Column**: The column that contributes most to the objective function is identified. This \"correlation burden\" $s_j$ for each column $j$ is defined as the sum of the absolute correlations with all other columns:\n    $$\n    s_j = \\sum_{k \\neq j} |r_{jk}|\n    $$\n    The column $c$ with the largest burden, $c = \\arg\\max_j s_j$, is selected as the target for optimization.\n\n2.  **Find Best Swap**: The algorithm searches for a pair of rows $(i, k)$ such that swapping the entries $X_{ic}$ and $X_{kc}$ in the target column $c$ produces the largest possible strict decrease in $\\Phi(X)$. Swapping two elements within a single column preserves the set of values in that column, thus leaving its mean $\\mu_c$ and standard deviation $\\sigma_c$ unchanged. Consequently, the standardized values $Z_{ic}$ and $Z_{kc}$ are simply swapped. This swap only affects correlations involving column $c$, i.e., $r_{cl}$ for $l \\neq c$.\n\n    Let's derive the change in a correlation coefficient $r_{cl}$ when entries $Z_{ic}$ and $Z_{kc}$ are swapped. Let $Z'$ be the matrix after the swap. The new correlation $r'_{cl}$ is:\n    $$\n    r'_{cl} = \\frac{1}{N}\\sum_{p=1}^{N} Z'_{pc} Z_{pl} = \\frac{1}{N} \\left( \\sum_{p \\notin \\{i,k\\}} Z_{pc}Z_{pl} + Z_{kc}Z_{il} + Z_{ic}Z_{kl} \\right)\n    $$\n    The change $\\Delta r_{cl} = r'_{cl} - r_{cl}$ is therefore:\n    $$\n    \\Delta r_{cl} = \\frac{1}{N} \\left( (Z_{kc}Z_{il} + Z_{ic}Z_{kl}) - (Z_{ic}Z_{il} + Z_{kc}Z_{kl}) \\right) = \\frac{1}{N} (Z_{kc} - Z_{ic})(Z_{il} - Z_{kl})\n    $$\n    The correlations $r_{jk}$ where neither $j$ nor $k$ is equal to $c$ are unaffected. The total change in the objective function, $\\Delta \\Phi$, for a swap in column $c$ between rows $i$ and $k$ is:\n    $$\n    \\Delta \\Phi_{i,k,c} = \\sum_{l \\neq c} \\left( |r_{cl} + \\Delta r_{cl}| - |r_{cl}| \\right)\n    $$\n    The algorithm computes $\\Delta \\Phi_{i,k,c}$ for all possible pairs $(i, k)$ with $1 \\le i < k \\le N$ and identifies the one yielding the most negative value (the largest decrease).\n\n3.  **Update State or Terminate**:\n    *   If the best swap found results in a strict decrease ($\\Delta \\Phi < 0$), the swap is accepted. The matrices $Z$ and $R$ are updated, as is the objective function $\\Phi$. Instead of recomputing the entire matrix $R$, we apply the calculated changes: for all $l \\neq c$, $r_{cl}$ is updated by adding $\\Delta r_{cl}$ corresponding to the best swap. The value of $\\Phi$ is updated by adding the best $\\Delta \\Phi$. The swap counter is incremented.\n    *   If no swap can produce a strict decrease ($\\Delta \\Phi \\ge 0$ for all pairs), no improvement is possible for the current target column. The greedy nature of the algorithm dictates termination at this point. The main loop is broken.\n\nThe procedure repeats until one of the termination conditions is met. The final value of $\\Phi(X)$ is then reported.\n\n### 3. Implementation and Vectorization\n\nTo implement this efficiently, particularly the search for the best swap, we use vectorization. For the chosen column $c$, we can compute the changes $\\Delta r_{cl}$ for all pairs $(i, k)$ and all other columns $l$ simultaneously using `numpy` array operations. This avoids slow nested loops in Python. Specifically, for all $N(N-1)/2$ row pairs, a matrix of correlation changes is computed. From this, a vector of $\\Delta \\Phi$ values is derived, and the minimum value identifies the best swap. The updates to the matrices $Z$ and $R$ and the scalar $\\Phi$ are then performed efficiently based on this single best swap.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the correlation-minimizing LHS algorithm for the given test suite.\n    \"\"\"\n    test_cases = [\n        # (N, m, seed, S_max)\n        (20, 5, 314159, 2000),  # Case A\n        (8, 1, 271828, 2000),   # Case B\n        (30, 10, 424242, 300),  # Case C\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N, m, seed, S_max = case\n        rng = np.random.default_rng(seed)\n\n        # Handle the trivial 1D case. The objective function sum is empty.\n        if m <= 1:\n            results.append(0.0)\n            continue\n\n        # --- 1. Initialization ---\n        \n        # Create the initial Latin Hypercube Sample matrix X\n        X = np.empty((N, m))\n        for j in range(m):\n            perm = rng.permutation(N)\n            u = rng.random(size=N)\n            X[:, j] = (perm + u) / N\n\n        # Standardize the matrix X to get Z\n        mu = np.mean(X, axis=0, keepdims=True)\n        # Use ddof=0 for divisor N as specified\n        sigma = np.std(X, axis=0, ddof=0, keepdims=True)\n        Z = (X - mu) / sigma\n        \n        # Compute the initial correlation matrix R and objective phi\n        R = (Z.T @ Z) / N\n        phi = np.sum(np.abs(np.triu(R, k=1)))\n        \n        # --- 2. Iterative Targeted-Swap Procedure ---\n        \n        num_swaps_accepted = 0\n        for _ in range(S_max):\n            # Select target column 'c' with the highest correlation burden\n            R_abs = np.abs(R)\n            np.fill_diagonal(R_abs, 0)\n            s = np.sum(R_abs, axis=1)\n            c = np.argmax(s)\n            \n            # Find the best swap in column 'c'\n            best_delta_phi = 0.0\n            best_swap_indices = None\n            best_delta_r = None\n\n            # Get all unique row pairs (i, k) where i < k\n            i_indices, k_indices = np.triu_indices(N, k=1)\n            \n            # Vectorized computation of potential changes\n            # For each potential swap (i,k), calculate the change in correlations r_cl\n            Z_col_c = Z[:, c]\n            Z_col_c_diffs = Z_col_c[k_indices] - Z_col_c[i_indices] # Shape: (num_pairs,)\n            \n            Z_row_diffs = Z[i_indices, :] - Z[k_indices, :] # Shape: (num_pairs, m)\n            \n            # Matrix of correlation changes. Shape: (num_pairs, m)\n            # Each row corresponds to a swap (i,k) and contains delta_r_cl for all l.\n            delta_r_matrix = (1 / N) * Z_col_c_diffs[:, np.newaxis] * Z_row_diffs\n            delta_r_matrix[:, c] = 0.0 # Change in r_cc is 0\n            \n            # Calculate the change in phi for each potential swap\n            mask = np.ones(m, dtype=bool)\n            mask[c] = False\n            \n            r_c_vec = R[c, mask]\n            r_c_new_matrix = r_c_vec[np.newaxis, :] + delta_r_matrix[:, mask]\n            \n            delta_phi_vec = np.sum(np.abs(r_c_new_matrix), axis=1) - np.sum(np.abs(r_c_vec))\n            \n            # Find the best swap among all pairs\n            min_idx = np.argmin(delta_phi_vec)\n            current_best_delta_phi = delta_phi_vec[min_idx]\n\n            if current_best_delta_phi < 0:\n                best_delta_phi = current_best_delta_phi\n                best_swap_indices = (i_indices[min_idx], k_indices[min_idx])\n                best_delta_r = delta_r_matrix[min_idx, :]\n\n            # If no improvement found, terminate the algorithm\n            if best_swap_indices is None:\n                break\n            \n            # --- 3. Update State ---\n            \n            # Perform the best swap and update all relevant quantities\n            i, k = best_swap_indices\n            \n            # Update phi\n            phi += best_delta_phi\n            \n            # Update standardized matrix Z\n            Z[i, c], Z[k, c] = Z[k, c], Z[i, c]\n            \n            # Update correlation matrix R\n            R[c, :] += best_delta_r\n            R[:, c] = R[c, :]\n            \n            num_swaps_accepted += 1\n\n        results.append(phi)\n\n    # Format output as specified\n    print(f\"[{','.join(f'{res:.6f}' for res in results)}]\")\n\nsolve()\n```"
        }
    ]
}