{
    "hands_on_practices": [
        {
            "introduction": "The practical efficiency of any MCMC algorithm is paramount, and it often depends on tunable parameters. This first exercise provides a deep dive into the computational cost of a standard slice sampler by analyzing the expected number of function evaluations per iteration . By dissecting the \"stepping-out\" and \"shrinking\" phases, you will derive a closed-form expression for this cost, revealing how it depends critically on the relationship between the true slice width $W$ and the algorithm's step-size parameter $w$. This practice builds intuition for parameter tuning and highlights the trade-offs inherent in the algorithm's design.",
            "id": "3344638",
            "problem": "Consider one-dimensional slice sampling within the framework of Markov chain Monte Carlo (MCMC). Let the target density be unimodal and continuous so that, for a level $u$ drawn uniformly from $[0, f(x_0)]$, the slice $\\{x : f(x) \\geq u\\}$ is a single interval $[a,b]$ of width $W = b - a$. Assume $x_0 \\in [a,b]$ and, conditional on $u$, that $x_0$ is uniformly distributed over $[a,b]$. Implement one iteration of the stepping-out and shrinking slice sampler as follows.\n\nStepping-out:\n- Fix a bracket width parameter $w  0$.\n- Draw an offset $U \\sim \\mathrm{Uniform}(0,w)$ and set the initial bracket endpoints to $L_0 = x_0 - U$ and $R_0 = x_0 + (w - U)$.\n- On the left, while $L  a$, move left by $w$: $L \\leftarrow L - w$. On the right, while $R  b$, move right by $w$: $R \\leftarrow R + w$. At each endpoint check, evaluate $f$ once to determine membership in the slice at that endpoint. Count one function evaluation at the initial endpoints $L_0$ and $R_0$, and one for each subsequent $w$-step endpoint check on both sides.\n\nShrinking:\n- With the bracket $[L,R]$ now containing $[a,b]$ and having both endpoints outside the slice, repeatedly draw $z \\sim \\mathrm{Uniform}(L,R)$ and evaluate $f(z)$. If $z \\in [a,b]$, accept and stop. If $z  a$, set $L \\leftarrow z$; if $z  b$, set $R \\leftarrow z$; then repeat. Count one function evaluation for each proposal $z$ (including the accepted one).\n\nAssume that each membership test of the form “is $x$ in the slice at level $u$?” requires exactly one evaluation of $f$. Do not count any evaluation of $f(x_0)$ used to generate $u$. Under these assumptions, derive from first principles a closed-form expression, as a function of $W$ and $w$, for the expected total number of function evaluations in one iteration (including both stepping-out and shrinking). Provide the final answer as a single analytic expression. No rounding is required, and no physical units are involved.",
            "solution": "The problem asks for the expected total number of function evaluations, $E[N]$, in one iteration of a specific slice sampling algorithm. The total number of evaluations $N$ is the sum of the evaluations in the stepping-out phase, $N_{step}$, and the shrinking phase, $N_{shrink}$. By the linearity of expectation, the total expected number of evaluations is the sum of the expectations for each phase:\n$$E[N] = E[N_{step}] + E[N_{shrink}]$$\nWe will calculate each term separately.\n\nFirst, we analyze the stepping-out phase.\nThe number of function evaluations in this phase is given by $N_{step} = 2 + N_L + N_R$, where $N_L$ and $N_R$ are the number of subsequent expansion steps to the left and right, respectively. The initial $2$ evaluations correspond to the initial bracket endpoints $L_0 = x_0 - U$ and $R_0 = x_0 + (w - U)$.\n\nLet's determine the expected number of leftward expansion steps, $E[N_L]$. The expansion to the left proceeds with steps of size $w$ as long as the endpoint $L$ is greater than $a$. The process starts at $L_0 = x_0 - U$ and generates endpoints $L_k = x_0 - U - k w$. The stepping stops at the first integer $k \\geq 0$ for which $L_k \\leq a$. Let $N_L$ be this value of $k$.\nIf $L_0 \\leq a$, no steps are taken, so $N_L=0$. If $L_0  a$, steps are taken. $N_L$ is the smallest non-negative integer such that $x_0 - U - N_L w \\leq a$. This can be rewritten as $N_L w \\geq x_0 - U - a$.\nThis implies that $N_L = \\max(0, \\lceil \\frac{x_0 - a - U}{w} \\rceil)$, where $\\lceil \\cdot \\rceil$ is the ceiling function.\n\nTo find the expectation $E[N_L]$, we average over the distributions of $x_0$ and $U$. The problem states that $x_0$ is uniformly distributed on the slice $[a,b]$, and $U$ is uniform on $[0,w]$. Let $Y = x_0 - a$. Since $x_0 \\sim \\mathrm{Uniform}(a,b)$, then $Y \\sim \\mathrm{Uniform}(0, W)$, where $W=b-a$ is the width of the slice. The variables $Y$ and $U$ are independent.\n$$E[N_L] = E_{Y,U}\\left[\\max\\left(0, \\left\\lceil \\frac{Y-U}{w} \\right\\rceil\\right)\\right]$$\nWe can compute this using the law of total expectation: $E[N_L] = E_Y[E_U[N_L | Y]]$. Let's compute the inner expectation for a fixed value $Y=y$:\n$$E_U[N_L|Y=y] = \\int_0^w \\max\\left(0, \\left\\lceil \\frac{y-u}{w} \\right\\rceil\\right) \\frac{1}{w} du$$\nLet's analyze the integral $I(y) = \\int_0^w \\max(0, \\lceil \\frac{y-u}{w} \\rceil) du$. Let $v = \\frac{y-u}{w}$, so $u = y - wv$ and $du = -w dv$. When $u=0$, $v=y/w$. When $u=w$, $v=(y-w)/w = y/w-1$.\n$$I(y) = \\int_{y/w}^{y/w-1} \\max(0, \\lceil v \\rceil) (-w) dv = w \\int_{y/w-1}^{y/w} \\max(0, \\lceil v \\rceil) dv$$\nLet $A = y/w$. We need to evaluate $\\int_{A-1}^A \\max(0, \\lceil v \\rceil) dv$.\nIf $A-1 \\ge 0$ (i.e., $y \\ge w$), the integral is $\\int_{A-1}^A \\lceil v \\rceil dv$. For any interval of length $1$, say from $z$ to $z+1$, the integral of the ceiling function is $\\int_z^{z+1} \\lceil v \\rceil dv = z+1$. Here, the integral is over an interval of length $1$ ending at $A$, so $\\int_{A-1}^A \\lceil v \\rceil dv = A$.\nIf $A-1  0$ (i.e., $y  w$), the integral becomes $\\int_{A-1}^0 0 \\cdot dv + \\int_0^A 1 \\cdot dv = A$.\nIn both cases, the integral is $A = y/w$.\nSo, $I(y) = w \\cdot (y/w) = y$.\nThis gives a remarkably simple result for the conditional expectation:\n$$E_U[N_L|Y=y] = \\frac{1}{w} I(y) = \\frac{y}{w}$$\nNow, we take the expectation over $Y \\sim \\mathrm{Uniform}(0, W)$:\n$$E[N_L] = E_Y\\left[\\frac{Y}{w}\\right] = \\frac{1}{w} E[Y]$$\nThe expected value of a $\\mathrm{Uniform}(0,W)$ random variable is $W/2$.\n$$E[N_L] = \\frac{1}{w} \\frac{W}{2} = \\frac{W}{2w}$$\nBy symmetry, we can calculate the expected number of rightward steps, $E[N_R]$. Here, $N_R = \\max(0, \\lceil \\frac{b-x_0-(w-U)}{w} \\rceil)$. Let $Y' = b-x_0$ and $U' = w-U$. Since $x_0 \\sim \\mathrm{Uniform}(a,b)$, $Y' \\sim \\mathrm{Uniform}(0,W)$. Since $U \\sim \\mathrm{Uniform}(0,w)$, $U' \\sim \\mathrm{Uniform}(0,w)$. The expression for $N_R$ has the same form as $N_L$, so the expectation is the same:\n$$E[N_R] = \\frac{W}{2w}$$\nThe total expected number of evaluations for the stepping-out phase is:\n$$E[N_{step}] = E[2 + N_L + N_R] = 2 + E[N_L] + E[N_R] = 2 + \\frac{W}{2w} + \\frac{W}{2w} = 2 + \\frac{W}{w}$$\n\nNext, we analyze the shrinking phase.\nThis phase begins with a bracket $[L,R]$ which contains the slice $[a,b]$. The width of this bracket is $S = R-L$. The final endpoints are $L = x_0-U-N_L w$ and $R=x_0+w-U+N_R w$.\n$$S = (x_0+w-U+N_R w) - (x_0-U-N_L w) = w + (N_L + N_R)w = w(1 + N_L + N_R)$$\nIn each step of the shrinking phase, a point $z$ is drawn uniformly from $[L,R]$. The proposal is accepted if $z \\in [a,b]$. The probability of acceptance is:\n$$p_{acc} = \\frac{\\text{length}([a,b])}{\\text{length}([L,R])} = \\frac{W}{S}$$\nThe number of proposals required until an acceptance occurs, $N_{shrink}$, follows a geometric distribution with success probability $p_{acc}$. The expected number of trials is $E[N_{shrink}] = 1/p_{acc}$.\nConditional on $x_0$ and $U$, the expected number of shrinking evaluations is:\n$$E[N_{shrink} | x_0, U] = \\frac{1}{p_{acc}} = \\frac{S}{W} = \\frac{w(1 + N_L + N_R)}{W}$$\nTo find the total unconditional expectation $E[N_{shrink}]$, we take the expectation of this expression over $x_0$ and $U$:\n$$E[N_{shrink}] = E\\left[\\frac{w(1 + N_L + N_R)}{W}\\right] = \\frac{w}{W} E[1 + N_L + N_R]$$\nUsing the linearity of expectation again:\n$$E[N_{shrink}] = \\frac{w}{W} (1 + E[N_L] + E[N_R])$$\nSubstituting the previously derived values for $E[N_L]$ and $E[N_R]$:\n$$E[N_{shrink}] = \\frac{w}{W} \\left(1 + \\frac{W}{2w} + \\frac{W}{2w}\\right) = \\frac{w}{W} \\left(1 + \\frac{W}{w}\\right) = \\frac{w}{W} + 1$$\n\nFinally, we sum the expected number of evaluations from both phases to find the total expected number of evaluations:\n$$E[N] = E[N_{step}] + E[N_{shrink}] = \\left(2 + \\frac{W}{w}\\right) + \\left(1 + \\frac{w}{W}\\right) = 3 + \\frac{W}{w} + \\frac{w}{W}$$\nThis is the closed-form expression for the expected total number of function evaluations in one iteration.",
            "answer": "$$\\boxed{3 + \\frac{W}{w} + \\frac{w}{W}}$$"
        },
        {
            "introduction": "Correctly implementing an MCMC algorithm requires careful attention to its underlying theoretical guarantees. Even a small deviation from the correct procedure can introduce bias and invalidate the results. This problem presents a cautionary tale, asking you to analyze a \"naive\" implementation of the shrinking step in a slice sampler that violates a core principle of the algorithm . By calculating the probability of sampling from a specific region, you will quantify the bias introduced by this flaw and, more importantly, reason from first principles—namely, the detailed balance condition—to identify the necessary correction that ensures the sampler produces uniform draws from the slice.",
            "id": "3344650",
            "problem": "Consider one-dimensional slice sampling for a target on the unit interval with unnormalized density defined by\n$$\nf(x) \\;=\\; \\begin{cases}\n1,  x \\in [0,\\,0.2] \\cup [0.7,\\,1],\\\\\n0.4,  x \\in (0.2,\\,0.7).\n\\end{cases}\n$$\nLet the current state be $x_{0} \\in [0.7,\\,1]$, and suppose the slice level is fixed at $y^{\\star} = 0.8$. The horizontal slice is therefore\n$$\nS(y^{\\star}) \\;=\\; \\{ x \\in [0,\\,1] : f(x) \\ge y^{\\star} \\} \\;=\\; [0,\\,0.2] \\cup [0.7,\\,1].\n$$\nA naive implementer initializes the sampling bracket to $[L,R] = [0,\\,1]$, and then performs the following “biased interval update” shrink procedure:\n- Propose $x' \\sim \\mathrm{Uniform}(L,R)$.\n- If $x' \\in S(y^{\\star})$, accept and set $x_{1} = x'$.\n- If $x' \\notin S(y^{\\star})$, update the bracket by deterministically keeping the left side, i.e., set $[L,R] \\leftarrow [L,\\,x']$, and repeat.\n\nThis update rule does not enforce the standard slice-sampling requirement that the bracket always contain the current state $x_{0}$. In fact, when $x' \\in (0.2,\\,0.7)$, this update discards the right side and hence discards any possibility of sampling from the right slice component $[0.7,\\,1]$ thereafter.\n\nWorking conditional on the fixed slice level $y^{\\star}$ and the fixed initialization $[L,R]=[0,\\,1]$, compute the probability that the finally accepted sample $x_{1}$ produced by the above naive procedure lies in the left slice component $[0,\\,0.2]$. Express your answer as a single exact fraction. No rounding is required.\n\nThen, briefly identify the correction needed in the shrink step to restore uniform sampling over $S(y^{\\star})$, starting from the core definitions of the slice sampler’s invariance condition and the requirement that the bracket updates preserve detailed balance. Your explanation must derive from first principles and must not rely on shortcut formulas.",
            "solution": "This problem consists of two parts. The first part requires the calculation of a probability for a flawed slice sampling procedure. The second part asks for an explanation of the flaw and the necessary correction based on first principles.\n\n### Part 1: Probability Calculation\n\nLet $S(y^{\\star}) = [0, 0.2] \\cup [0.7, 1]$ be the horizontal slice at level $y^{\\star} = 0.8$. We can denote the left component as $I_L = [0, 0.2]$ and the right component as $I_R = [0.7, 1]$. The region between them is $I_M = (0.2, 0.7)$.\n\nThe naive sampling procedure begins with an initial bracket $[L, R] = [0, 1]$. A proposal $x'$ is drawn from a uniform distribution on this bracket, $x' \\sim \\mathrm{Uniform}(0, 1)$.\n\nThere are three mutually exclusive outcomes for the first proposal $x'_1$:\n1.  $x'_1 \\in I_L = [0, 0.2]$. The probability of this event is $P(x'_1 \\in I_L) = 0.2 - 0 = 0.2$. In this case, the sample is accepted, $x_1 = x'_1$, and the procedure terminates. The final sample lies in the left component.\n2.  $x'_1 \\in I_R = [0.7, 1]$. The probability of this event is $P(x'_1 \\in I_R) = 1 - 0.7 = 0.3$. The sample is accepted, $x_1 = x'_1$, and the procedure terminates. The final sample lies in the right component.\n3.  $x'_1 \\in I_M = (0.2, 0.7)$. The probability of this event is $P(x'_1 \\in I_M) = 0.7 - 0.2 = 0.5$. The sample is rejected, and the sampling bracket is updated according to the rule $[L, R] \\leftarrow [L, x'_1]$. Since the initial left boundary is $L=0$, the new bracket becomes $[0, x'_1]$.\n\nLet's analyze the consequence of the shrinking step. If the first proposal $x'_1$ falls in $I_M$, the new sampling bracket for all subsequent proposals becomes $[0, x'_1]$. Since $x'_1  0.7$, this new bracket has no overlap with the right component of the slice, $I_R = [0.7, 1]$. Therefore, if the algorithm enters the shrinking phase, it is impossible for it to ever generate a sample from $I_R$. Any sample that is eventually accepted must come from the only part of the slice available in the new bracket, which is $I_L = [0, 0.2]$.\n\nLet $E_L$ be the event that the finally accepted sample $x_1$ lies in the left component $I_L$. Let $E_R$ be the event that $x_1$ lies in the right component $I_R$. The procedure for generating a sample must terminate to be a valid part of an MCMC algorithm. Assuming termination, the final sample must be in either $I_L$ or $I_R$, so $P(E_L) + P(E_R) = 1$.\n\nThe event $E_R$ can occur only if the accepted sample is from $I_R$. Based on the logic above, a sample can be drawn from $I_R$ only on the very first proposal. If the first proposal is not in $I_R$, the opportunity to sample from $I_R$ is permanently lost.\nThus, the event $E_R$ is equivalent to the event that the first proposal $x'_1$ lies in $I_R$.\n\nThe probability of this is:\n$$ P(E_R) = P(x'_1 \\in I_R) = P(x'_1 \\in [0.7, 1]) $$\nSince $x'_1 \\sim \\mathrm{Uniform}(0, 1)$, this probability is simply the length of the interval:\n$$ P(E_R) = 1 - 0.7 = 0.3 = \\frac{3}{10} $$\n\nThe question asks for the probability of the complementary event, $E_L$. Since the process must terminate in either $E_L$ or $E_R$, we have:\n$$ P(E_L) = 1 - P(E_R) = 1 - 0.3 = 0.7 $$\nAs an exact fraction, the probability is $\\frac{7}{10}$.\n\n### Part 2: Correction to the Shrink Step\n\nThe goal of this stage of the slice sampler, given the slice level $y^{\\star}$, is to draw a sample uniformly from the set $S(y^{\\star})$. The described procedure fails to do this, as shown by the first part: a correct uniform sampler would pick a point from $I_L$ with probability $\\frac{|I_L|}{|I_L|+|I_R|} = \\frac{0.2}{0.2+0.3} = \\frac{0.2}{0.5} = 0.4$, not $0.7$.\n\nThe flaw in the naive procedure stems from its violation of the principles required for a valid Markov Chain Monte Carlo (MCMC) transition. A slice sampler is an MCMC method whose stationary distribution must be the target distribution $\\pi(x) \\propto f(x)$. A sufficient condition for this is that the transition kernel $K(x_1 | x_0)$ satisfies the detailed balance equation:\n$$ \\pi(x_0) K(x_1 | x_0) = \\pi(x_1) K(x_0 | x_1) $$\nThis equation ensures the reversibility of the Markov chain.\n\nThe transition $x_0 \\to x_1$ involves two steps: drawing a vertical level $y \\sim \\mathrm{Uniform}(0, f(x_0))$, and then drawing a new horizontal position $x_1$ from a procedure designed to sample from the slice $S_y = \\{x : f(x) \\ge y\\}$. The entire transition must be reversible.\n\nThe flawed \"biased interval update\" is given by $[L,R] \\leftarrow [L, x']$ when a proposal $x'$ is rejected. This rule is independent of the current state $x_0$. As noted in the problem, this can lead to a new bracket that no longer contains $x_0$. For example, with $x_0 \\in [0.7, 1]$ and a rejected proposal $x' \\in (0.2, 0.7)$, the new bracket becomes $[0, x']$, which excludes $x_0$.\n\nThis has a critical consequence for detailed balance. If the sampling procedure can eliminate the starting point $x_0$ from the set of possible outcomes, then the transition from $x_0$ to itself (or back to $x_0$ from another state $x_1$) might become impossible. Specifically, the kernel for the reverse move $K(x_0|x_1)$ might be zero while the kernel for the forward move $K(x_1|x_0)$ is non-zero. This would violate detailed balance. For the reversibility proofs of the slice sampler to hold, the procedure for sampling $x_1$ given $x_0$ must not preclude sampling $x_0$ itself.\n\nThe necessary correction is to make the shrinking procedure dependent on the current state $x_0$, ensuring that $x_0$ always remains within the sampling bracket. This preserves the possibility of sampling $x_0$ and is a necessary condition for the detailed balance arguments to apply.\n\nThe corrected shrink step is as follows:\nWhen a proposal $x'$ is rejected (i.e., $x' \\notin S(y^{\\star})$), the bracket $[L,R]$ is updated based on the position of $x'$ relative to $x_0$:\n- If $x'  x_0$, the proposal fell to the left of the current state. The region $[L, x')$ can be discarded. The update is $L \\leftarrow x'$.\n- If $x'  x_0$, the proposal fell to the right of the current state. The region $(x', R]$ can be discarded. The update is $R \\leftarrow x'$.\n\nThis rule ensures that the interval $[L, R]$ always contains $x_0$. It removes the directional bias of the naive implementation by treating the regions to the left and right of $x_0$ symmetrically. This correction restores a key property required for the slice sampling algorithm to satisfy detailed balance and thus correctly sample from the target distribution.",
            "answer": "$$\\boxed{\\frac{7}{10}}$$"
        },
        {
            "introduction": "Beyond understanding a single algorithm, a crucial skill is to compare the performance of different methods. This advanced practice places the slice sampler in a broader context by comparing it to the workhorse Random-Walk Metropolis-Hastings (RWMH) algorithm using the formal framework of Peskun ordering . You will see that the slice sampler's construction as a Gibbs sampler on an augmented space fundamentally avoids the self-transitions that can hinder RWMH, leading to superior performance. Calculating the lag-one autocorrelation for a Gaussian target provides a concrete demonstration of the slice sampler's remarkable mixing properties, showing how it can generate nearly independent samples in a single step for certain observables.",
            "id": "3344725",
            "problem": "Consider a one-dimensional target distribution with quadratic log-density, defined by the probability density function $$\\pi(x) \\propto \\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right), \\quad x \\in \\mathbb{R},$$ where $\\sigma^{2}  0$. Two Markov Chain Monte Carlo (MCMC) algorithms are used to sample from $\\pi$: (i) a random-walk Metropolis-Hastings (MH) algorithm with Gaussian proposal $$q(y \\mid x) = \\frac{1}{\\sqrt{2\\pi \\tau^{2}}} \\exp\\!\\left(-\\frac{(y-x)^{2}}{2\\tau^{2}}\\right),$$ with proposal variance $\\tau^{2}  0$ and acceptance probability $$\\alpha(x,y) = \\min\\!\\left\\{1, \\frac{\\pi(y)}{\\pi(x)}\\right\\} = \\min\\!\\left\\{1, \\exp\\!\\left(-\\frac{y^{2}-x^{2}}{2\\sigma^{2}}\\right)\\right\\},$$ and (ii) a simple slice sampler that proceeds by introducing an auxiliary variable $u$ and updating as follows: given the current state $x$, sample $$u \\sim \\mathrm{Uniform}(0, \\pi(x)),$$ then sample the next state $$x' \\sim \\mathrm{Uniform}(S(u)),$$ where the slice is $$S(u) = \\{x \\in \\mathbb{R} : \\pi(x) \\ge u\\}.$$\n\nUsing only fundamental definitions and properties of reversible Markov kernels, Peskun ordering, and the Gibbs heatbath construction for slice sampling on the augmented space, do the following:\n\n1. Derive the joint augmented target density on $(x,u)$ for the slice sampler and the full conditional distributions needed to implement the simple slice sampler update described above. Then, using these conditionals, express the one-step transition kernel of the $x$-marginal chain in terms of integrals over the auxiliary variable $u$.\n\n2. State and apply the definition of Peskun ordering for reversible Markov kernels with common invariant distribution $\\pi$ to compare the simple slice sampler kernel with the random-walk Metropolis-Hastings kernel. Your comparison must start from the structural properties of the two kernels (e.g., presence or absence of a self-transition component and how off-diagonal probability mass is distributed) and conclude which kernel Peskun-dominates the other for the quadratic log-density target.\n\n3. Consider the observable $f(x) = x$. Under stationarity of the slice sampler, use the augmented representation to compute the lag-one autocorrelation coefficient $$\\rho_{1} = \\frac{\\mathrm{Cov}_{\\mathrm{stat}}(X_{n}, X_{n+1})}{\\mathrm{Var}_{\\pi}(X)}$$ of $f$ for the $x$-marginal chain produced by the simple slice sampler. Express your final answer as an exact real number. No rounding is required.\n\nFinally, based on the Peskun ordering established in part 2, infer which algorithm (the simple slice sampler or the random-walk Metropolis-Hastings) yields a lower asymptotic variance for estimators of $\\mathbb{E}_{\\pi}[f(X)]$ for square-integrable $f$, and explain why this inference holds from first principles. Your final submitted answer must be only the value of $\\rho_{1}$ specified in part 3.",
            "solution": "The problem requires a three-part analysis comparing a simple slice sampler and a Random-Walk Metropolis-Hastings (RWMH) algorithm for a Gaussian target, followed by a conceptual inference. The final output must be the lag-one autocorrelation coefficient for the slice sampler.\n\nThe target probability density function is given as $\\pi(x) \\propto \\exp(-\\frac{x^{2}}{2\\sigma^{2}})$. For all sampling steps and acceptance ratios, we can use the unnormalized density, which we denote as $\\pi(x) = \\exp(-\\frac{x^{2}}{2\\sigma^{2}})$. For calculations of expectations and variances under the stationary distribution, we consider the corresponding normalized density, implicitly dividing by the normalization constant $Z = \\int_{-\\infty}^{\\infty} \\exp(-\\frac{t^{2}}{2\\sigma^{2}}) dt = \\sqrt{2\\pi\\sigma^2}$.\n\n### Part 1: Slice Sampler Kernel Derivation\n\nThe simple slice sampler is an auxiliary variable method. It samples from a joint distribution on an augmented space $(x,u)$ whose marginal distribution for $x$ is the target $\\pi(x)$.\n\nThe joint density $\\pi(x,u)$ is constructed to be uniform over the region under the curve of the target density. It is given by:\n$$ \\pi(x,u) = \\begin{cases} K  \\text{if } 0  u  \\pi(x) \\\\ 0  \\text{otherwise} \\end{cases} $$\nwhere $K$ is a normalization constant. The marginal distribution for $x$ is $\\int_{0}^{\\infty} \\pi(x,u) du = \\int_{0}^{\\pi(x)} K du = K \\pi(x)$. For this to be proportional to $\\pi(x)$, this construction is valid. We can set $K=1$ and work with unnormalized densities throughout. So, $\\pi(x,u) = \\mathbf{1}_{\\{0  u  \\pi(x)\\}}$.\n\nThe simple slice sampler algorithm described is a Gibbs sampler on this augmented space, which updates $u$ and $x$ iteratively from their full conditional distributions.\n1.  **Full conditional for $u$ given $x$**: $p(u \\mid x) \\propto \\pi(x,u) = \\mathbf{1}_{\\{0  u  \\pi(x)\\}}$. To normalize, we integrate over $u$: $\\int_{0}^{\\pi(x)} 1 \\, du = \\pi(x)$. Thus, the normalized conditional density is\n    $$ p(u \\mid x) = \\frac{1}{\\pi(x)} \\mathbf{1}_{\\{0  u  \\pi(x)\\}} $$\n    This corresponds to drawing $u \\sim \\mathrm{Uniform}(0, \\pi(x))$, as stated in the problem.\n\n2.  **Full conditional for $x$ given $u$**: $p(x \\mid u) \\propto \\pi(x,u) = \\mathbf{1}_{\\{0  u  \\pi(x)\\}}$. The support of this distribution is the set $S(u) = \\{x \\in \\mathbb{R} : \\pi(x) \\ge u\\}$, which is the \"slice\". The conditional density is uniform over this set.\n    $$ p(x \\mid u) = \\frac{1}{|S(u)|} \\mathbf{1}_{\\{x \\in S(u)\\}} $$\n    where $|S(u)| = \\int_{S(u)} 1 \\, dx$ is the Lebesgue measure of the slice. This corresponds to drawing $x' \\sim \\mathrm{Uniform}(S(u))$.\n\nThe one-step transition kernel of the $x$-marginal chain, $P_{SS}(x' \\mid x)$, which gives the probability density of moving from state $x$ to $x'$, is obtained by integrating out the auxiliary variable $u$. A transition from $x$ to $x'$ involves first drawing $u \\sim p(u \\mid x)$ and then drawing $x' \\sim p(x' \\mid u)$.\n$$ P_{SS}(x' \\mid x) = \\int_{0}^{\\infty} p(x' \\mid u) p(u \\mid x) \\, du $$\nSubstituting the conditional densities:\n$$ P_{SS}(x' \\mid x) = \\int_{0}^{\\pi(x)} \\left( \\frac{1}{|S(u)|} \\mathbf{1}_{\\{x' \\in S(u)\\}} \\right) \\frac{1}{\\pi(x)} \\, du $$\nThe condition $x' \\in S(u)$ is equivalent to $\\pi(x') \\ge u$. The indicator $\\mathbf{1}_{\\{\\pi(x') \\ge u\\}}$ restricts the integration domain of $u$. The integration is over $u \\in [0, \\pi(x)]$, so the combined condition is $0 \\le u \\le \\min(\\pi(x), \\pi(x'))$.\n$$ P_{SS}(x' \\mid x) = \\frac{1}{\\pi(x)} \\int_{0}^{\\min(\\pi(x), \\pi(x'))} \\frac{1}{|S(u)|} \\, du $$\nThis is the expression for the one-step transition kernel of the $x$-marginal chain in terms of an integral over the auxiliary variable $u$.\n\n### Part 2: Peskun Ordering\n\nPeskun ordering provides a way to compare the efficiency of two reversible Markov kernels, $P_1$ and $P_2$, that share the same invariant distribution $\\pi$. The kernel $P_2$ is said to be Peskun-superior to $P_1$ (denoted $P_1 \\preceq_P P_2$) if its off-diagonal transition probabilities are uniformly larger, i.e., $P_2(x,A) \\ge P_1(x,A)$ for all $x$ and all measurable sets $A$ not containing $x$. A direct consequence and a more practical condition for comparison, especially for kernels with densities, is to compare their self-transition probabilities. If $P_1(x, \\{x\\}) \\ge P_2(x, \\{x\\})$ for all $x$, then $P_2$ is at least as good as $P_1$ in terms of the asymptotic variance of estimators.\n\nLet's analyze the structure of the two kernels in question.\n\n1.  **Random-Walk Metropolis-Hastings (RWMH) Kernel**: The transition from $x$ to $y$ is proposed from a density $q(y \\mid x)$ and accepted with probability $\\alpha(x,y)$. If the proposal is rejected, the chain stays at $x$. The kernel can be written as:\n    $$ P_{MH}(x, dy) = q(y \\mid x)\\alpha(x,y) \\, dy + \\left( 1 - \\int_{-\\infty}^{\\infty} q(z \\mid x)\\alpha(x,z) \\, dz \\right) \\delta_x(dy) $$\n    where $\\delta_x$ is the Dirac delta measure at $x$. The second term represents the probability mass of self-transition (rejection). Since the acceptance probability $\\alpha(x,z)$ is not always $1$, there is a non-zero probability of rejection, $r(x) = P_{MH}(x, \\{x\\})  0$.\n\n2.  **Simple Slice Sampler Kernel**: As derived in Part 1, the transition density is\n    $$ P_{SS}(x' \\mid x) = \\frac{1}{\\pi(x)} \\int_{0}^{\\min(\\pi(x), \\pi(x'))} \\frac{1}{|S(u)|} \\, du $$\n    For our specific target $\\pi(x) \\propto \\exp(-\\frac{x^{2}}{2\\sigma^{2}})$, the slice $S(u)$ is an interval $[-c(u), c(u)]$, a continuous set. The next state $x'$ is drawn from a continuous distribution, $\\mathrm{Uniform}(S(u))$. Therefore, the probability of drawing the exact same point $x'$ as the starting point $x$ is zero. The kernel $P_{SS}$ is purely absolutely continuous with respect to the Lebesgue measure and has no atomic part. That is, $P_{SS}(x, \\{x\\}) = 0$.\n\n**Comparison**:\nWe compare the self-transition probabilities:\n-   $P_{MH}(x, \\{x\\}) = r(x)  0$ for almost all $x$.\n-   $P_{SS}(x, \\{x\\}) = 0$ for all $x$.\n\nClearly, $P_{SS}(x, \\{x\\}) \\le P_{MH}(x, \\{x\\})$ for all $x$. This inequality implies that the slice sampler is never \"stuck\" at the current state, whereas the RWMH algorithm can be. An algorithm that always moves to a new state explores the state space more efficiently than one that might remain in place. According to the theory of Peskun ordering and its extensions, a kernel with smaller self-transition probabilities leads to smaller asymptotic variances for estimators. Therefore, the simple slice sampler Peskun-dominates (or is superior in the related ordering based on asymptotic variance) the random-walk Metropolis-Hastings algorithm for this target distribution. $P_{MH} \\preceq_P P_{SS}$.\n\n### Part 3: Lag-one Autocorrelation for Slice Sampler\n\nWe need to compute the lag-one autocorrelation coefficient for the observable $f(x)=x$:\n$$ \\rho_{1} = \\frac{\\mathrm{Cov}_{\\mathrm{stat}}(X_{n}, X_{n+1})}{\\mathrm{Var}_{\\pi}(X)} $$\nThe chain is in stationarity, so $X_n \\sim \\pi$. The target density is a Gaussian $N(0, \\sigma^2)$. Therefore:\n-   $\\mathbb{E}_{\\pi}[X] = 0$\n-   $\\mathrm{Var}_{\\pi}(X) = \\mathbb{E}_{\\pi}[X^2] - (\\mathbb{E}_{\\pi}[X])^2 = \\sigma^2 - 0 = \\sigma^2$.\n\nThe covariance is $\\mathrm{Cov}_{\\mathrm{stat}}(X_{n}, X_{n+1}) = \\mathbb{E}[X_{n}X_{n+1}] - \\mathbb{E}[X_{n}]\\mathbb{E}[X_{n+1}]$. Since the chain is stationary, $\\mathbb{E}[X_n] = \\mathbb{E}[X_{n+1}] = \\mathbb{E}_\\pi[X] = 0$.\nThus, $\\mathrm{Cov}_{\\mathrm{stat}}(X_{n}, X_{n+1}) = \\mathbb{E}[X_{n}X_{n+1}]$.\n\nWe compute this expectation using the law of total expectation and the augmented representation of the slice sampler. Let $(X_n, U_{n+1}, X_{n+1})$ represent one full transition.\n$$ \\mathbb{E}[X_n X_{n+1}] = \\mathbb{E}_{X_n} \\left[ X_n \\mathbb{E}[X_{n+1} \\mid X_n] \\right] $$\nThe inner expectation is the conditional expectation of the next state, given the current state $x_n=x$. This is computed by averaging over the intermediate variable $u$:\n$$ \\mathbb{E}[X_{n+1} \\mid X_n=x] = \\mathbb{E}_{u \\sim p(u|x)} \\left[ \\mathbb{E}[X_{n+1} \\mid U_{n+1}=u, X_n=x] \\right] $$\nThe draw of $X_{n+1}$ only depends on $U_{n+1}$, not on $X_n$. So, $\\mathbb{E}[X_{n+1} \\mid U_{n+1}=u, X_n=x] = \\mathbb{E}[X_{n+1} \\mid U_{n+1}=u]$.\nThe distribution of $X_{n+1}$ given $U_{n+1}=u$ is $\\mathrm{Uniform}(S(u))$. Let's characterize the slice $S(u)$:\n$$ S(u) = \\{x \\in \\mathbb{R} : \\exp(-\\frac{x^{2}}{2\\sigma^{2}}) \\ge u\\} $$\nThis is equivalent to $-\\frac{x^2}{2\\sigma^2} \\ge \\ln(u)$, or $x^2 \\le -2\\sigma^2 \\ln(u)$. This defines an interval $[-c(u), c(u)]$, where $c(u) = \\sqrt{-2\\sigma^2\\ln(u)}$. This interval is symmetric about $0$.\nThe expectation of a random variable drawn from $\\mathrm{Uniform}([-c(u), c(u)])$ is:\n$$ \\mathbb{E}[X_{n+1} \\mid U_{n+1}=u] = \\int_{-c(u)}^{c(u)} y \\frac{1}{2c(u)} \\, dy = \\frac{1}{2c(u)} \\left[ \\frac{y^2}{2} \\right]_{-c(u)}^{c(u)} = 0 $$\nSince this conditional expectation is $0$ for any value of $u$, the expectation over $u$ is also $0$:\n$$ \\mathbb{E}[X_{n+1} \\mid X_n=x] = \\mathbb{E}_{u \\sim p(u|x)} [0] = 0 $$\nThis holds for any starting state $x$. Now we can compute the covariance:\n$$ \\mathrm{Cov}_{\\mathrm{stat}}(X_{n}, X_{n+1}) = \\mathbb{E}[X_n X_{n+1}] = \\mathbb{E}_{X_n}[X_n \\cdot 0] = 0 $$\nFinally, the lag-one autocorrelation coefficient is:\n$$ \\rho_{1} = \\frac{0}{\\sigma^2} = 0 $$\nThis result is a special consequence of the symmetry of both the target distribution and the observable $f(x)=x$.\n\n### Conceptual Inference on Asymptotic Variance\n\nBased on the Peskun ordering established in Part 2 ($P_{MH} \\preceq_P P_{SS}$), we infer that the **simple slice sampler yields a lower (or equal) asymptotic variance** for estimators of $\\mathbb{E}_{\\pi}[f(X)]$ for any square-integrable function $f$.\n\nThis inference holds from first principles because the asymptotic variance of an MCMC estimator is proportional to the integrated autocorrelation time (IACT), $\\tau_f = 1 + 2\\sum_{k=1}^\\infty \\rho_k(f)$, where $\\rho_k(f)$ is the lag-$k$ autocorrelation for the process $f(X_n)$. Peskun ordering is intrinsically linked to the speed of convergence of the Markov chain. A Peskun-superior kernel (like the slice sampler here) corresponds to a smaller second-largest eigenvalue modulus of the transition operator, which implies a larger spectral gap and faster convergence to the stationary distribution. This faster mixing reduces the correlation between successive samples, leading to lower autocorrelation values $\\rho_k(f)$ for $k \\ge 1$. A smaller IACT directly translates to a smaller asymptotic variance, meaning that for a fixed number of samples $N$, the estimator produced by the slice sampler will be more precise. The structural reason for this superiority is the slice sampler's avoidance of self-transitions, which allows for more efficient exploration of the state space compared to the RWMH algorithm, which may remain in the same state due to rejections.",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}