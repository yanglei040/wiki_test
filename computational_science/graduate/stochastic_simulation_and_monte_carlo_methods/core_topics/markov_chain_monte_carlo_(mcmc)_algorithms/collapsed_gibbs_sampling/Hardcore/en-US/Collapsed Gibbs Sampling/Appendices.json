{
    "hands_on_practices": [
        {
            "introduction": "Latent Dirichlet Allocation (LDA) is a cornerstone topic model where collapsed Gibbs sampling is the canonical inference algorithm. This first practice challenges you to derive the celebrated update equation from first principles, integrating out the continuous model parameters . By working through a concrete numerical example, you will solidify your understanding of how corpus-wide and document-specific statistics govern the assignment of a word to a topic.",
            "id": "3296126",
            "problem": "Consider a single-token update step in collapsed Gibbs sampling for Latent Dirichlet Allocation (LDA). You are given one document index $d$ and one held-out token index $i$ whose word type is $v^{\\star}$. The current state excludes token $i$ from all counts (denoted by superscript $-i$). There are $K=3$ topics and a vocabulary of size $V=4$. The Dirichlet prior over document-topic proportions is $\\operatorname{Dirichlet}(\\boldsymbol{\\alpha})$ with $\\boldsymbol{\\alpha}=(\\alpha_{1},\\alpha_{2},\\alpha_{3})=(0.7,0.5,0.3)$. The Dirichlet prior over each topic-word distribution is symmetric $\\operatorname{Dirichlet}(\\beta \\boldsymbol{1}_{V})$ with $\\beta=0.2$. The held-out word token is $v^{\\star}=3$. The observed counts excluding token $i$ are:\n- Document-topic counts for document $d$: $(n_{d,1}^{-i},n_{d,2}^{-i},n_{d,3}^{-i})=(3,1,0)$.\n- Topic-word counts aggregated over the entire corpus:\n  - For topic $k=1$: total $n_{1,\\cdot}^{-i}=10$ with $n_{1,3}^{-i}=2$ for word $v=3$.\n  - For topic $k=2$: total $n_{2,\\cdot}^{-i}=5$ with $n_{2,3}^{-i}=1$ for word $v=3$.\n  - For topic $k=3$: total $n_{3,\\cdot}^{-i}=8$ with $n_{3,3}^{-i}=4$ for word $v=3$.\n\nStarting only from the fundamental definitions of the Latent Dirichlet Allocation generative model, the Dirichlet density, Dirichlet-multinomial conjugacy, and Bayes’ rule, perform the following:\n\n1) Derive from first principles the collapsed conditional assignment distribution for $z_{i}$ given $(\\boldsymbol{z}_{-i},\\boldsymbol{w})$ by integrating out all document-topic proportions and all topic-word distributions. Your derivation must explicitly show how the Dirichlet-multinomial integrals enter and simplify.\n\n2) Using your derived expression, compute the three unnormalized assignment weights for topics $k=1,2,3$ for token $i$ and then normalize them to obtain the assignment probabilities. Keep all intermediate computations exact whenever possible.\n\n3) Independently verify numerically that your collapsed conditional is consistent with the collapsed joint by evaluating the ratio of the collapsed joint density $p(\\boldsymbol{w},\\boldsymbol{z}\\,|\\,\\boldsymbol{\\alpha},\\beta)$ under two configurations that differ only by the setting of $z_{i}$, and confirming that this ratio equals the ratio of the corresponding unnormalized assignment weights you computed. Show this equality holds for at least one nontrivial pair of topics.\n\nReport as your final answer only the normalized assignment probability for topic $k=2$ for token $i$, expressed as a single reduced fraction. Do not include any units. Do not round.",
            "solution": "The problem is assessed to be valid as it is scientifically grounded in the established theory of Latent Dirichlet Allocation (LDA), is well-posed with a complete and consistent set of parameters and data, and is expressed in objective, formal language. It is a standard problem in Bayesian machine learning that admits a unique, verifiable solution. We proceed with the solution.\n\nThe problem asks for three tasks: derivation of the collapsed Gibbs sampling update rule for LDA, calculation of assignment probabilities for a specific token, and numerical verification of the result.\n\n### 1. Derivation of the Collapsed Conditional Distribution\n\nThe goal of collapsed Gibbs sampling for LDA is to sample the topic assignment $z_i$ for a single token $i$ from its conditional distribution given all other topic assignments $\\boldsymbol{z}_{-i}$ and all observed words $\\boldsymbol{w}$. The model parameters, document-topic proportions $\\boldsymbol{\\theta} = \\{\\boldsymbol{\\theta}_d\\}_{d=1}^D$ and topic-word distributions $\\boldsymbol{\\phi} = \\{\\boldsymbol{\\phi}_k\\}_{k=1}^K$, are integrated out (collapsed).\n\nBy Bayes' rule, the desired conditional probability is:\n$$p(z_i = k | \\boldsymbol{z}_{-i}, \\boldsymbol{w}, \\boldsymbol{\\alpha}, \\beta) \\propto p(z_i = k, \\boldsymbol{z}_{-i}, \\boldsymbol{w} | \\boldsymbol{\\alpha}, \\beta) = p(\\boldsymbol{z}, \\boldsymbol{w} | \\boldsymbol{\\alpha}, \\beta)$$\nwhere the full vector of assignments is $\\boldsymbol{z} = (z_i=k, \\boldsymbol{z}_{-i})$. The term $p(\\boldsymbol{z}, \\boldsymbol{w} | \\boldsymbol{\\alpha}, \\beta)$ is the marginal likelihood of the assignments and words after integrating out the parameters $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\phi}$.\n\nThe complete generative model is defined by:\n$p(\\boldsymbol{w}, \\boldsymbol{z}, \\boldsymbol{\\theta}, \\boldsymbol{\\phi} | \\boldsymbol{\\alpha}, \\beta) = p(\\boldsymbol{w} | \\boldsymbol{z}, \\boldsymbol{\\phi}) p(\\boldsymbol{z} | \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta} | \\boldsymbol{\\alpha}) p(\\boldsymbol{\\phi} | \\beta)$.\nDue to conditional independencies, this can be written as:\n$$p(\\boldsymbol{w}, \\boldsymbol{z}, \\boldsymbol{\\theta}, \\boldsymbol{\\phi} | \\boldsymbol{\\alpha}, \\beta) = \\left( \\prod_{d=1}^{D} p(\\boldsymbol{z}_d|\\boldsymbol{\\theta}_d)p(\\boldsymbol{\\theta}_d|\\boldsymbol{\\alpha}) \\right) \\left( \\prod_{k=1}^{K} p(\\boldsymbol{w}^{(k)}|\\boldsymbol{\\phi}_k)p(\\boldsymbol{\\phi}_k|\\beta) \\right)$$\nwhere $\\boldsymbol{w}^{(k)}$ are the words assigned to topic $k$.\n\nWe integrate out $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\phi}$:\n$$p(\\boldsymbol{z}, \\boldsymbol{w} | \\boldsymbol{\\alpha}, \\beta) = \\int p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha})p(\\boldsymbol{z}|\\boldsymbol{\\theta})\\,d\\boldsymbol{\\theta} \\int p(\\boldsymbol{\\phi}|\\beta)p(\\boldsymbol{w}|\\boldsymbol{z},\\boldsymbol{\\phi})\\,d\\boldsymbol{\\phi}$$\nLet's evaluate each integral. The likelihoods $p(\\boldsymbol{z}|\\boldsymbol{\\theta})$ and $p(\\boldsymbol{w}|\\boldsymbol{z},\\boldsymbol{\\phi})$ are multinomial, and the priors $p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha})$ and $p(\\boldsymbol{\\phi}|\\beta)$ are Dirichlet. This is a conjugate pairing.\n\nThe integral over $\\boldsymbol{\\theta}$ is a product of Dirichlet-multinomial marginal likelihoods over documents:\n$$\\int \\prod_{d=1}^{D} p(\\boldsymbol{z}_d|\\boldsymbol{\\theta}_d)p(\\boldsymbol{\\theta}_d|\\boldsymbol{\\alpha})\\,d\\boldsymbol{\\theta}_d = \\prod_{d=1}^{D} \\frac{\\Gamma(\\sum_k \\alpha_k)}{\\prod_k \\Gamma(\\alpha_k)} \\frac{\\prod_k \\Gamma(n_{d,k} + \\alpha_k)}{\\Gamma(\\sum_k(n_{d,k} + \\alpha_k))}$$\nwhere $n_{d,k}$ is the number of tokens in document $d$ assigned to topic $k$.\n\nSimilarly, the integral over $\\boldsymbol{\\phi}$ is a product over topics:\n$$\\int \\prod_{k=1}^{K} p(\\boldsymbol{w}^{(k)}|\\boldsymbol{\\phi}_k)p(\\boldsymbol{\\phi}_k|\\beta)\\,d\\boldsymbol{\\phi}_k = \\prod_{k=1}^{K} \\frac{\\Gamma(V\\beta)}{\\Gamma(\\beta)^V} \\frac{\\prod_v \\Gamma(n_{k,v} + \\beta)}{\\Gamma(\\sum_v(n_{k,v} + \\beta))}$$\nwhere $n_{k,v}$ is the number of times word type $v$ is assigned to topic $k$ across the corpus.\n\nThe full collapsed joint probability is the product of these terms. To find the conditional $p(z_i = k | \\boldsymbol{z}_{-i}, \\boldsymbol{w})$, we analyze how the collapsed joint $p(\\boldsymbol{z}, \\boldsymbol{w})$ changes as we vary the assignment $z_i$ of a single token $i$. Let this token be in document $d$ and have word type $v^{\\star}$. The counts are related by $n_{d,k} = n_{d,k}^{-i} + \\delta(z_i, k)$ and $n_{k,v^{\\star}} = n_{k,v^{\\star}}^{-i} + \\delta(z_i, k)$, where $\\delta$ is the Kronecker delta and counts with superscript $-i$ exclude token $i$. All terms in the joint probability not depending on $k$ are constant with respect to the sampling decision and can be absorbed into the proportionality constant.\n\nThe terms that vary with the choice of $k$ for $z_i$ are:\n1. From the document-topic part, for document $d$: $\\Gamma(n_{d,k} + \\alpha_k)$.\n2. From the topic-word part, for topic $k$: $\\prod_v \\Gamma(n_{k,v} + \\beta)$ and $\\Gamma(\\sum_v(n_{k,v} + \\beta))$.\n\nLet's look at the ratio of the joint probability for $z_i=k$ to a baseline state with token $i$ removed.\nThe change in the document-topic term is:\n$$\\frac{\\Gamma(n_{d,k}^{-i}+1+\\alpha_k)}{\\Gamma(n_{d,k}^{-i}+\\alpha_k)} \\frac{\\Gamma(n_{d,\\cdot}^{-i}+\\sum_j \\alpha_j)}{\\Gamma(n_{d,\\cdot}^{-i}+1+\\sum_j \\alpha_j)} = \\frac{n_{d,k}^{-i}+\\alpha_k}{n_{d,\\cdot}^{-i}+\\sum_j \\alpha_j}$$\nusing the property $\\Gamma(x+1)=x\\Gamma(x)$.\n\nThe change in the topic-word term for topic $k$ (with word $w_i=v^{\\star}$) is:\n$$\\frac{\\Gamma(n_{k,v^{\\star}}^{-i}+1+\\beta)}{\\Gamma(n_{k,v^{\\star}}^{-i}+\\beta)} \\frac{\\Gamma(n_{k,\\cdot}^{-i}+V\\beta)}{\\Gamma(n_{k,\\cdot}^{-i}+1+V\\beta)} = \\frac{n_{k,v^{\\star}}^{-i}+\\beta}{n_{k,\\cdot}^{-i}+V\\beta}$$\n\nCombining these terms, the conditional probability is proportional to the product of these factors:\n$$p(z_i = k | \\boldsymbol{z}_{-i}, \\boldsymbol{w}) \\propto \\frac{n_{d,k}^{-i}+\\alpha_k}{n_{d,\\cdot}^{-i}+\\sum_j \\alpha_j} \\times \\frac{n_{k,v^{\\star}}^{-i}+\\beta}{n_{k,\\cdot}^{-i}+V\\beta}$$\nThe term $n_{d,\\cdot}^{-i}+\\sum_j \\alpha_j$ is constant with respect to $k$, so it can be dropped from the unnormalized weight. Thus, the unnormalized assignment weight $W_k$ is:\n$$W_k \\propto (n_{d,k}^{-i} + \\alpha_k) \\times \\frac{n_{k,v^{\\star}}^{-i} + \\beta}{n_{k,\\cdot}^{-i} + V\\beta}$$\nThis completes the derivation.\n\n### 2. Calculation of Assignment Probabilities\n\nWe are given the following data:\n- Number of topics $K=3$.\n- Vocabulary size $V=4$.\n- Document-topic prior $\\boldsymbol{\\alpha}=(\\alpha_{1},\\alpha_{2},\\alpha_{3})=(0.7,0.5,0.3)$.\n- Topic-word prior parameter $\\beta=0.2$.\n- Held-out word type $v^{\\star}=3$.\n- Document-topic counts for document $d$: $(n_{d,1}^{-i},n_{d,2}^{-i},n_{d,3}^{-i})=(3,1,0)$.\n- Topic-word counts:\n  - $k=1$: $n_{1,\\cdot}^{-i}=10$, $n_{1,3}^{-i}=2$.\n  - $k=2$: $n_{2,\\cdot}^{-i}=5$, $n_{2,3}^{-i}=1$.\n  - $k=3$: $n_{3,\\cdot}^{-i}=8$, $n_{3,3}^{-i}=4$.\n\nThe constant term is $V\\beta = 4 \\times 0.2 = 0.8$. We compute the unnormalized weight $W_k$ for each topic $k \\in \\{1,2,3\\}$.\n\nFor topic $k=1$:\n$$W_1 = (n_{d,1}^{-i} + \\alpha_1) \\times \\frac{n_{1,3}^{-i} + \\beta}{n_{1,\\cdot}^{-i} + V\\beta} = (3 + 0.7) \\times \\frac{2 + 0.2}{10 + 0.8} = 3.7 \\times \\frac{2.2}{10.8} = \\frac{37}{10} \\times \\frac{22}{108} = \\frac{37 \\times 11}{10 \\times 54} = \\frac{407}{540}$$\n\nFor topic $k=2$:\n$$W_2 = (n_{d,2}^{-i} + \\alpha_2) \\times \\frac{n_{2,3}^{-i} + \\beta}{n_{2,\\cdot}^{-i} + V\\beta} = (1 + 0.5) \\times \\frac{1 + 0.2}{5 + 0.8} = 1.5 \\times \\frac{1.2}{5.8} = \\frac{3}{2} \\times \\frac{12}{58} = \\frac{18}{58} = \\frac{9}{29}$$\n\nFor topic $k=3$:\n$$W_3 = (n_{d,3}^{-i} + \\alpha_3) \\times \\frac{n_{3,3}^{-i} + \\beta}{n_{3,\\cdot}^{-i} + V\\beta} = (0 + 0.3) \\times \\frac{4 + 0.2}{8 + 0.8} = 0.3 \\times \\frac{4.2}{8.8} = \\frac{3}{10} \\times \\frac{42}{88} = \\frac{3 \\times 21}{10 \\times 44} = \\frac{63}{440}$$\n\nThe total unnormalized weight is $W_{total} = W_1 + W_2 + W_3$:\n$$W_{total} = \\frac{407}{540} + \\frac{9}{29} + \\frac{63}{440}$$\nThe least common denominator of $540=2^2 \\cdot 3^3 \\cdot 5$, $29$, and $440=2^3 \\cdot 5 \\cdot 11$ is $2^3 \\cdot 3^3 \\cdot 5 \\cdot 11 \\cdot 29 = 344520$.\n$$W_{total} = \\frac{407 \\cdot 638}{344520} + \\frac{9 \\cdot 11880}{344520} + \\frac{63 \\cdot 783}{344520} = \\frac{259666 + 106920 + 49329}{344520} = \\frac{415915}{344520}$$\nSimplifying by dividing by $5$:\n$$W_{total} = \\frac{83183}{68904}$$\nThe normalized probability for topic $k=2$, $P_2$, is:\n$$P_2 = \\frac{W_2}{W_{total}} = \\frac{9/29}{415915/344520} = \\frac{9 \\times (344520/29)}{415915} = \\frac{9 \\times 11880}{415915} = \\frac{106920}{415915}$$\nSimplifying the fraction by dividing numerator and denominator by 5:\n$$P_2 = \\frac{106920 \\div 5}{415915 \\div 5} = \\frac{21384}{83183}$$\nThe prime factors of the numerator $21384$ are $2^3$, $3^5$, and $11$. The denominator $83183$ is not divisible by $2$, $3$, or $11$. Thus, the fraction is fully reduced.\n\n### 3. Numerical Verification\n\nWe verify that the ratio of conditional probabilities equals the ratio of the full collapsed joint probabilities for two different assignments of $z_i$. Let $\\boldsymbol{z}$ be the configuration where $z_i=1$ and $\\boldsymbol{z}'$ be the configuration where $z_i=2$. We must show that $\\frac{W_2}{W_1} = \\frac{p(\\boldsymbol{w}, \\boldsymbol{z}')}{p(\\boldsymbol{w}, \\boldsymbol{z})}$.\n\nRatio of unnormalized weights:\n$$\\frac{W_2}{W_1} = \\frac{9/29}{407/540} = \\frac{9}{29} \\times \\frac{540}{407} = \\frac{4860}{11803}$$\n\nRatio of joint probabilities:\nThe joint probability is given by the product of Gamma function ratios. Most terms cancel out when taking the ratio $\\frac{p(\\boldsymbol{w}, \\boldsymbol{z}')}{p(\\boldsymbol{w}, \\boldsymbol{z})}$. The only terms that do not cancel are those related to counts that differ between $\\boldsymbol{z}$ and $\\boldsymbol{z}'$.\n- For $\\boldsymbol{z}$: $z_i=1$. Counts updated are $n_{d,1}$, $n_{d,\\cdot}$, $n_{1,v^{\\star}}$, $n_{1,\\cdot}$.\n- For $\\boldsymbol{z}'$: $z_i=2$. Counts updated are $n_{d,2}$, $n_{d,\\cdot}$, $n_{2,v^{\\star}}$, $n_{2,\\cdot}$.\nThe ratio of joint probabilities is:\n$$\\frac{p(\\boldsymbol{w}, \\boldsymbol{z}')}{p(\\boldsymbol{w}, \\boldsymbol{z})} = \\frac{\\Gamma(n_{d,1}^{-i}+\\alpha_1) \\Gamma(n_{d,2}^{-i}+1+\\alpha_2)}{\\Gamma(n_{d,1}^{-i}+1+\\alpha_1) \\Gamma(n_{d,2}^{-i}+\\alpha_2)} \\times \\frac{\\frac{\\Gamma(n_{1,v^{\\star}}^{-i}+\\beta) \\Gamma(n_{2,v^{\\star}}^{-i}+1+\\beta)}{\\Gamma(n_{1,\\cdot}^{-i}+V\\beta)\\Gamma(n_{2,\\cdot}^{-i}+1+V\\beta)}}{\\frac{\\Gamma(n_{1,v^{\\star}}^{-i}+1+\\beta) \\Gamma(n_{2,v^{\\star}}^{-i}+\\beta)}{\\Gamma(n_{1,\\cdot}^{-i}+1+V\\beta)\\Gamma(n_{2,\\cdot}^{-i}+V\\beta)}}$$\nSimplifying using $\\Gamma(x+1)=x\\Gamma(x)$:\n$$ \\frac{p(\\boldsymbol{w}, \\boldsymbol{z}')}{p(\\boldsymbol{w}, \\boldsymbol{z})} = \\frac{n_{d,2}^{-i}+\\alpha_2}{n_{d,1}^{-i}+\\alpha_1} \\times \\frac{(n_{2,v^{\\star}}^{-i}+\\beta)/(n_{1,v^{\\star}}^{-i}+\\beta)}{(n_{2,\\cdot}^{-i}+V\\beta)/(n_{1,\\cdot}^{-i}+V\\beta)} = \\frac{n_{d,2}^{-i}+\\alpha_2}{n_{d,1}^{-i}+\\alpha_1} \\times \\frac{n_{2,v^{\\star}}^{-i}+\\beta}{n_{2,\\cdot}^{-i}+V\\beta} \\times \\frac{n_{1,\\cdot}^{-i}+V\\beta}{n_{1,v^{\\star}}^{-i}+\\beta} $$\nThis is precisely $\\frac{W_2}{W_1}$ before simplifying constants. Plugging in numbers:\n$$ \\frac{p(\\boldsymbol{w}, \\boldsymbol{z}')}{p(\\boldsymbol{w}, \\boldsymbol{z})} = \\frac{1+0.5}{3+0.7} \\times \\frac{1+0.2}{5+0.8} \\times \\frac{10+0.8}{2+0.2} = \\frac{1.5}{3.7} \\times \\frac{1.2}{5.8} \\times \\frac{10.8}{2.2} = \\frac{15}{37} \\times \\frac{12}{58} \\times \\frac{108}{22} $$\n$$ = \\frac{15}{37} \\times \\frac{6}{29} \\times \\frac{54}{11} = \\frac{15 \\times 324}{37 \\times 319} = \\frac{4860}{11803} $$\nThis matches the ratio of the weights $\\frac{W_2}{W_1}$ exactly, confirming the consistency of the derivation and calculation.\n\nThe final answer required is the normalized assignment probability for topic $k=2$.\n$$ P_2 = \\frac{21384}{83183} $$",
            "answer": "$$\\boxed{\\frac{21384}{83183}}$$"
        },
        {
            "introduction": "Building on the mechanics of the LDA sampler, we now explore the conceptual role of the Dirichlet hyperparameters, $\\alpha$ and $\\beta$. This exercise develops crucial modeling intuition by examining how these parameters, which encode our prior beliefs, steer the sampler's behavior between sparse and diffuse topic structures . Analyzing the sampler's limiting behavior reveals the fundamental tension between relying on observed counts and smoothing with prior knowledge.",
            "id": "3296154",
            "problem": "Consider Latent Dirichlet Allocation (LDA) with symmetric Dirichlet priors, where for each topic $k \\in \\{1,\\dots,K\\}$ the topic-word distribution $\\phi_{k}$ is drawn as $\\phi_{k} \\sim \\mathrm{Dirichlet}_{V}(\\beta)$ and for each document $d$ the document-topic proportions $\\theta_{d}$ are drawn as $\\theta_{d} \\sim \\mathrm{Dirichlet}_{K}(\\alpha)$, with $\\alpha > 0$ and $\\beta > 0$. For each token $i$ in document $d$, a topic assignment $z_{di}$ is drawn as $z_{di} \\sim \\mathrm{Categorical}(\\theta_{d})$ and a word $w_{di}$ is drawn as $w_{di} \\sim \\mathrm{Categorical}(\\phi_{z_{di}})$. In collapsed Gibbs sampling, the document-topic proportions and topic-word distributions are analytically integrated out. Let $w_{di}$ denote the observed word type of token $i$ in document $d$, and denote by $z_{-i}$ and $w_{-i}$ the assignments and word tokens for all positions except token $i$. Define the usual count notation, excluding token $i$: $n_{dk}^{-i}$ as the number of tokens in document $d$ assigned to topic $k$, $n_{kv}^{-i}$ as the number of tokens assigned to topic $k$ with word type $v$, and $n_{k\\cdot}^{-i} = \\sum_{v=1}^{V} n_{kv}^{-i}$.\n\nYour tasks are:\n- Using only Bayes’ rule, the definition of the Dirichlet distribution, and Dirichlet–Multinomial conjugacy, derive from first principles a collapsed Gibbs transition for a single token, i.e., the conditional distribution of $z_{di}$ given $z_{-i}$ and $w$ up to a proportionality constant. Do not assume any pre-stated collapsed formula.\n- Using the expression you derived, explain qualitatively how the hyperparameters $\\alpha$ and $\\beta$ influence the collapsed Gibbs transition probabilities for $z_{di}$, focusing on the relative weighting of document-level topic frequency and topic-level word frequency.\n- Derive the limiting, normalized conditional probabilities $\\lim_{\\alpha,\\beta \\to 0^{+}} \\Pr\\!\\big(z_{di} = k \\mid z_{-i}, w, \\alpha, \\beta\\big)$ and $\\lim_{\\alpha,\\beta \\to \\infty} \\Pr\\!\\big(z_{di} = k \\mid z_{-i}, w, \\alpha, \\beta\\big)$, and state the conditions under which these limits place nonzero mass on a topic.\n\nNow apply these results to a concrete instance with $K = 3$ topics and vocabulary size $V = 6$. Consider a particular document $d$ and token position $i$ at which the observed word type is $v^{\\star}$. The corpus counts excluding token $i$ are:\n- Document counts: $n_{d1}^{-i} = 2$, $n_{d2}^{-i} = 0$, $n_{d3}^{-i} = 5$.\n- Topic totals: $n_{1\\cdot}^{-i} = 20$, $n_{2\\cdot}^{-i} = 15$, $n_{3\\cdot}^{-i} = 1$.\n- Word-specific counts for $v^{\\star}$: $n_{1 v^{\\star}}^{-i} = 3$, $n_{2 v^{\\star}}^{-i} = 0$, $n_{3 v^{\\star}}^{-i} = 1$.\n\nCompute the exact limiting, normalized conditional probabilities for assigning $z_{di} = 3$ under the two regimes $\\alpha,\\beta \\to 0^{+}$ (sparse priors) and $\\alpha,\\beta \\to \\infty$ (diffuse priors). Provide your final answer as a row matrix containing the two exact probabilities in the order $\\big(\\lim_{\\alpha,\\beta \\to 0^{+}} \\Pr(z_{di}=3 \\mid \\cdot),\\ \\lim_{\\alpha,\\beta \\to \\infty} \\Pr(z_{di}=3 \\mid \\cdot)\\big)$. No rounding is required and no units are involved.",
            "solution": "The posed problem is valid as it is scientifically grounded in the established theory of Latent Dirichlet Allocation (LDA) and collapsed Gibbs sampling, is well-posed with sufficient information for a unique solution, and is formulated objectively. We proceed with the solution.\n\nThe core task is to derive the conditional probability for a single topic assignment $z_{di}$ given all other assignments $z_{-i}$ and all observed words $w$. This is the collapsed Gibbs sampling update rule. The term \"collapsed\" refers to the fact that the continuous parameters $\\theta_d$ (document-topic proportions) and $\\phi_k$ (topic-word distributions) are integrated out.\n\nFirst, we derive the general collapsed Gibbs sampling transition probability for $z_{di}=k$. By Bayes' rule, the desired conditional probability is:\n$$\n\\Pr(z_{di}=k \\mid z_{-i}, w, \\alpha, \\beta) = \\frac{\\Pr(w, z)}{\\Pr(w, z_{-i})} = \\frac{\\Pr(w, z)}{\\sum_{j=1}^K \\Pr(w, z_{di}=j, z_{-i})}\n$$\nWe only need to find an expression proportional to the numerator, as the denominator is just a normalization constant.\n$$\n\\Pr(z_{di}=k \\mid z_{-i}, w, \\alpha, \\beta) \\propto \\Pr(w, z \\mid \\alpha, \\beta)\n$$\nwhere $z$ denotes the full set of topic assignments $\\{z_{di}\\}$. We can factor the joint probability $\\Pr(w, z \\mid \\alpha, \\beta)$ as $\\Pr(w \\mid z, \\beta) \\Pr(z \\mid \\alpha)$. The parameters $\\theta$ and $\\phi$ have been integrated out.\n\nThe term $\\Pr(z \\mid \\alpha)$ concerns the topic assignments. Within each document $d$, the assignments $z_d = \\{z_{d1}, z_{d2}, \\dots \\}$ are generated from a shared $\\theta_d \\sim \\mathrm{Dirichlet}_K(\\alpha)$. Due to the properties of Dirichlet-Multinomial conjugacy, we can integrate out $\\theta_d$:\n$$\n\\Pr(z_d \\mid \\alpha) = \\int \\Pr(z_d \\mid \\theta_d) \\Pr(\\theta_d \\mid \\alpha) d\\theta_d = \\int \\left( \\prod_{k=1}^K \\theta_{dk}^{n_{dk}} \\right) \\frac{\\Gamma(K\\alpha)}{\\Gamma(\\alpha)^K} \\left( \\prod_{k=1}^K \\theta_{dk}^{\\alpha-1} \\right) d\\theta_d\n$$\nwhere $n_{dk}$ is the count of tokens in document $d$ assigned to topic $k$. This integrates to the Dirichlet-Multinomial distribution:\n$$\n\\Pr(z_d \\mid \\alpha) = \\frac{\\Gamma(K\\alpha)}{\\Gamma(\\alpha)^K} \\frac{\\prod_{k=1}^K \\Gamma(n_{dk}+\\alpha)}{\\Gamma(n_{d\\cdot}+K\\alpha)}\n$$\nwhere $n_{d\\cdot} = \\sum_k n_{dk}$. Since topic assignments are independent across documents, $\\Pr(z \\mid \\alpha) = \\prod_d \\Pr(z_d \\mid \\alpha)$.\n\nSimilarly, the term $\\Pr(w \\mid z, \\beta)$ concerns the words. Given the topic assignments $z$, the words are generated from the topic-word distributions $\\phi_k \\sim \\mathrm{Dirichlet}_V(\\beta)$. For each topic $k$, we can integrate out $\\phi_k$:\n$$\n\\Pr(w \\mid z, \\beta) = \\int \\Pr(w \\mid z, \\{\\phi_k\\}) \\Pr(\\{\\phi_k\\} \\mid \\beta) d\\{\\phi_k\\} = \\prod_{k=1}^K \\int \\Pr(w_k \\mid \\phi_k) \\Pr(\\phi_k \\mid \\beta) d\\phi_k\n$$\nwhere $w_k$ are the words assigned to topic $k$. This yields another product of Dirichlet-Multinomial probabilities:\n$$\n\\Pr(w \\mid z, \\beta) = \\prod_{k=1}^K \\frac{\\Gamma(V\\beta)}{\\Gamma(\\beta)^V} \\frac{\\prod_{v=1}^V \\Gamma(n_{kv}+\\beta)}{\\Gamma(n_{k\\cdot}+V\\beta)}\n$$\nwhere $n_{kv}$ is the count of word type $v$ in topic $k$, and $n_{k\\cdot} = \\sum_v n_{kv}$.\n\nTo find the Gibbs sampling update for $z_{di}=k$, we look at the ratio of joint probabilities $\\Pr(w,z)$ before and after changing the assignment of only token $i$. All terms in the products for $\\Pr(z \\mid \\alpha)$ and $\\Pr(w \\mid z, \\beta)$ not related to document $d$ or topic $k$ will cancel out. The counts are related by $n_{dk} = n_{dk}^{-i} + 1$ and $n_{kv} = n_{kv}^{-i} + 1$ (for $w_{di}=v$). Using the property $\\Gamma(x+1) = x\\Gamma(x)$:\n$$\n\\Pr(z_{di}=k \\mid z_{-i}, w) \\propto \\frac{\\Gamma(n_{dk}^{-i} + 1 + \\alpha)}{\\Gamma(n_{dk}^{-i} + \\alpha)} \\frac{\\Gamma(n_{d\\cdot}^{-i} + K\\alpha)}{\\Gamma(n_{d\\cdot}^{-i} + 1 + K\\alpha)} \\times \\frac{\\Gamma(n_{kv}^{-i} + 1 + \\beta)}{\\Gamma(n_{kv}^{-i} + \\beta)} \\frac{\\Gamma(n_{k\\cdot}^{-i} + V\\beta)}{\\Gamma(n_{k\\cdot}^{-i} + 1 + V\\beta)}\n$$\nSimplifying using $\\Gamma(x+1) = x\\Gamma(x)$:\n$$\n\\Pr(z_{di}=k \\mid z_{-i}, w) \\propto (n_{dk}^{-i} + \\alpha) \\frac{1}{n_{d\\cdot}^{-i}+K\\alpha} \\times (n_{kv}^{-i}+\\beta) \\frac{1}{n_{k\\cdot}^{-i}+V\\beta}\n$$\nThe term $n_{d\\cdot}^{-i}+K\\alpha$ is the same for all topics $k$ and can be dropped from the proportionality. Thus, the final collapsed Gibbs sampling update rule is:\n$$\n\\Pr(z_{di}=k \\mid z_{-i}, w, \\alpha, \\beta) \\propto (n_{dk}^{-i} + \\alpha) \\frac{n_{kv}^{-i} + \\beta}{n_{k\\cdot}^{-i} + V\\beta}\n$$\nThis derivation uses first principles as requested.\n\nThe expression has two main parts:\n1.  $(n_{dk}^{-i} + \\alpha)$: This term reflects how prevalent topic $k$ is in document $d$.\n2.  $\\frac{n_{kv}^{-i} + \\beta}{n_{k\\cdot}^{-i} + V\\beta}$: This term reflects how likely word $v=w_{di}$ is to be generated by topic $k$.\n\nThe hyperparameters $\\alpha$ and $\\beta$ act as smoothing pseudocounts.\n-   $\\alpha$ influences the document-topic distributions. A small $\\alpha$ (e.g., $\\alpha \\ll 1$) implies that the prior favors documents being generated from a small number of topics (sparsity). The update probability is dominated by existing counts $n_{dk}^{-i}$. A large $\\alpha$ implies that the prior favors documents having a mixture of many topics (density). The term $(n_{dk}^{-i} + \\alpha)$ becomes less sensitive to the specific counts $n_{dk}^{-i}$, making the document-level distribution flatter and giving unobserved topics a higher chance.\n-   $\\beta$ influences the topic-word distributions. A small $\\beta$ (e.g., $\\beta \\ll 1$) implies that priors favor topics being composed of a small number of words. The update probability is dominated by the ratio of observed counts $\\frac{n_{kv}^{-i}}{n_{k\\cdot}^{-i}}$. A large $\\beta$ implies priors favor topics having a mixture of many different words from the vocabulary. The term $\\frac{n_{kv}^{-i} + \\beta}{n_{k\\cdot}^{-i} + V\\beta}$ approaches $\\frac{1}{V}$, making the word-topic association less informative and pushing topic-word distributions toward uniform.\n\nNext, we derive the limiting normalized conditional probabilities. Let the unnormalized probability be $q_k(\\alpha, \\beta) = (n_{dk}^{-i} + \\alpha) \\frac{n_{kv}^{-i} + \\beta}{n_{k\\cdot}^{-i} + V\\beta}$. The normalized probability is $P_k = \\frac{q_k}{\\sum_{j=1}^K q_j}$.\n\n-   **Limit as $\\alpha, \\beta \\to 0^{+}$ (sparse priors):**\n    We analyze the behavior of $q_k(\\alpha, \\beta)$ as the hyperparameters approach zero.\n    If $n_{dk}^{-i}=0$, then $q_k(\\alpha, \\beta) \\propto \\alpha$, which vanishes. So, for a topic to have non-zero probability in the limit, it must have already been observed in the document ($n_{dk}^{-i} > 0$).\n    If $n_{dk}^{-i} > 0$, then $\\lim_{\\alpha \\to 0^+} q_k(\\alpha, \\beta) = n_{dk}^{-i} \\lim_{\\beta \\to 0^+} \\frac{n_{kv}^{-i} + \\beta}{n_{k\\cdot}^{-i} + V\\beta}$.\n    - If $n_{k\\cdot}^{-i} > 0$, the limit is $n_{dk}^{-i} \\frac{n_{kv}^{-i}}{n_{k\\cdot}^{-i}}$.\n    - If $n_{k\\cdot}^{-i} = 0$, then $n_{kv}^{-i}=0$ for all $v$. The limit is $n_{dk}^{-i} \\lim_{\\beta \\to 0^+} \\frac{\\beta}{V\\beta} = \\frac{n_{dk}^{-i}}{V}$.\n    Let $L_k = \\lim_{\\alpha,\\beta \\to 0^{+}} q_k(\\alpha, \\beta)$ up to a common scaling factor. So, for $k \\in \\{1,\\dots,K\\}$:\n    $L_k = n_{dk}^{-i} \\frac{n_{kv}^{-i}}{n_{k\\cdot}^{-i}}$ if $n_{dk}^{-i}>0$ and $n_{k\\cdot}^{-i}>0$.\n    $L_k = \\frac{n_{dk}^{-i}}{V}$ if $n_{dk}^{-i}>0$ and $n_{k\\cdot}^{-i}=0$.\n    $L_k = 0$ if $n_{dk}^{-i}=0$.\n    The limiting normalized probability is $\\Pr(z_{di}=k|\\cdot) = \\frac{L_k}{\\sum_{j=1}^K L_j}$. A topic $k$ can have nonzero mass only if $n_{dk}^{-i} > 0$.\n\n-   **Limit as $\\alpha, \\beta \\to \\infty$ (diffuse priors):**\n    As $\\alpha,\\beta \\to \\infty$, the counts $n_{dk}^{-i}, n_{kv}^{-i}, n_{k\\cdot}^{-i}$ become negligible.\n    $q_k(\\alpha,\\beta) \\approx \\alpha \\frac{\\beta}{V\\beta} = \\frac{\\alpha}{V}$.\n    Since this asymptotic behavior is the same for all topics $k$, the normalized probability becomes uniform.\n    $$\n    \\lim_{\\alpha,\\beta \\to \\infty} \\Pr(z_{di}=k|\\cdot) = \\lim_{\\alpha,\\beta \\to \\infty} \\frac{q_k(\\alpha, \\beta)}{\\sum_{j=1}^K q_j(\\alpha, \\beta)} = \\frac{\\lim \\frac{\\alpha}{V}}{\\sum_{j=1}^K \\lim \\frac{\\alpha}{V}} = \\frac{\\frac{\\alpha}{V}}{K \\frac{\\alpha}{V}} = \\frac{1}{K}\n    $$\n    In this limit, nonzero mass is placed on every topic, $P_k = 1/K > 0$.\n\nNow we apply these results to the given instance:\n$K=3$, $V=6$, $w_{di}=v^\\star$.\nCounts: $n_{d1}^{-i} = 2$, $n_{d2}^{-i} = 0$, $n_{d3}^{-i} = 5$.\n$n_{1\\cdot}^{-i} = 20$, $n_{2\\cdot}^{-i} = 15$, $n_{3\\cdot}^{-i} = 1$.\n$n_{1v^\\star}^{-i} = 3$, $n_{2v^\\star}^{-i} = 0$, $n_{3v^\\star}^{-i} = 1$.\n\n-   **Calculation for $\\lim_{\\alpha,\\beta \\to 0^{+}} \\Pr(z_{di}=3|\\cdot)$:**\n    We compute the unnormalized scores $L_k$.\n    For $k=1$: $n_{d1}^{-i}=2>0$ and $n_{1\\cdot}^{-i}=20>0$.\n    $L_1 = n_{d1}^{-i} \\frac{n_{1v^\\star}^{-i}}{n_{1\\cdot}^{-i}} = 2 \\times \\frac{3}{20} = \\frac{6}{20} = \\frac{3}{10}$.\n    For $k=2$: $n_{d2}^{-i}=0$.\n    $L_2 = 0$.\n    For $k=3$: $n_{d3}^{-i}=5>0$ and $n_{3\\cdot}^{-i}=1>0$.\n    $L_3 = n_{d3}^{-i} \\frac{n_{3v^\\star}^{-i}}{n_{3\\cdot}^{-i}} = 5 \\times \\frac{1}{1} = 5$.\n    The sum is $\\sum_{j=1}^3 L_j = L_1 + L_2 + L_3 = \\frac{3}{10} + 0 + 5 = \\frac{3+50}{10} = \\frac{53}{10}$.\n    The normalized probability for $z_{di}=3$ is:\n    $$\n    \\lim_{\\alpha,\\beta \\to 0^{+}} \\Pr(z_{di}=3|\\cdot) = \\frac{L_3}{\\sum_{j=1}^3 L_j} = \\frac{5}{\\frac{53}{10}} = \\frac{50}{53}\n    $$\n\n-   **Calculation for $\\lim_{\\alpha,\\beta \\to \\infty} \\Pr(z_{di}=3|\\cdot)$:**\n    As derived, this limit is $\\frac{1}{K}$. With $K=3$:\n    $$\n    \\lim_{\\alpha,\\beta \\to \\infty} \\Pr(z_{di}=3|\\cdot) = \\frac{1}{3}\n    $$\n\nThe two exact probabilities are $\\frac{50}{53}$ and $\\frac{1}{3}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{50}{53} & \\frac{1}{3}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While collapsing can improve a sampler's statistical efficiency by reducing autocorrelation, it often comes at the cost of more complex computations per iteration. This exercise formalizes this trade-off, asking you to model the costs and benefits to determine when collapsing is computationally advantageous . This practice encourages a practical, performance-aware perspective on algorithm design beyond just theoretical elegance.",
            "id": "3296131",
            "problem": "Consider a latent-variable finite mixture model with $K$ components in which Gibbs sampling updates a single latent assignment per iteration. In an uncollapsed Gibbs sampler, the per-iteration computational cost is $c>0$. In a collapsed Gibbs sampler, exact marginalization of the mixture weights is performed, which adds an additional per-iteration cost that scales linearly in $K$, so the collapsed per-iteration cost is $c+\\alpha K$ for some $\\alpha>0$. Assume stationarity and ergodicity of the Markov chain under both schemes.\n\nTo quantify the sampling efficiency, define the Effective Sample Size (ESS) as $\\mathrm{ESS}=N/\\tau_{\\mathrm{int}}$, where $N$ is the number of iterations and $\\tau_{\\mathrm{int}}$ is the integrated autocorrelation time. The ESS per unit wall-clock time is then proportional to $1/\\left(\\text{cost per iteration} \\times \\tau_{\\mathrm{int}}\\right)$. For the uncollapsed chain, assume a geometric autocorrelation function with lag parameter $r\\in(0,1)$, i.e., $\\rho_{k}=r^{k}$ for lag $k\\geq 0$. For the collapsed chain, assume that exact marginalization improves mixing by reducing the lag parameter to $r^{\\delta}$, with $\\delta>1$ fixed and independent of $K$.\n\nStarting from the definition of the integrated autocorrelation time,\n$$\n\\tau_{\\mathrm{int}} \\equiv 1 + 2\\sum_{k=1}^{\\infty} \\rho_{k},\n$$\nderive a closed-form expression for the critical component count $K^{\\star}$ at which the collapsed sampler and the uncollapsed sampler have equal ESS per unit wall-clock time. Express $K^{\\star}$ symbolically as a function of $c$, $\\alpha$, $r$, and $\\delta$. Your final answer must be a single closed-form analytic expression and should not be rounded.",
            "solution": "The user has asked for a critical validation and, if valid, a solution to the problem.\n\n### Step 1: Extract Givens\n- **Model:** Latent-variable finite mixture model with $K$ components.\n- **Samplers:**\n    1.  Uncollapsed Gibbs sampler (U)\n    2.  Collapsed Gibbs sampler (C)\n- **Costs per iteration:**\n    -   Uncollapsed: $C^{(U)} = c$, where $c > 0$.\n    -   Collapsed: $C^{(C)} = c + \\alpha K$, where $\\alpha > 0$.\n- **Efficiency Metric:** Effective Sample Size (ESS) per unit wall-clock time, $\\eta$.\n    -   $\\mathrm{ESS} = N/\\tau_{\\mathrm{int}}$, where $N$ is the number of iterations and $\\tau_{\\mathrm{int}}$ is the integrated autocorrelation time.\n    -   $\\eta \\propto 1 / (\\text{cost per iteration} \\times \\tau_{\\mathrm{int}})$.\n- **Integrated Autocorrelation Time Definition:**\n    -   $\\tau_{\\mathrm{int}} = 1 + 2\\sum_{k=1}^{\\infty} \\rho_{k}$, where $\\rho_k$ is the autocorrelation at lag $k$.\n- **Autocorrelation Models:**\n    -   Uncollapsed: $\\rho_{k}^{(U)} = r^{k}$ for $k \\geq 0$, with $r \\in (0,1)$.\n    -   Collapsed: The lag parameter is reduced to $r^{\\delta}$, so $\\rho_{k}^{(C)} = (r^{\\delta})^{k} = r^{\\delta k}$, with $\\delta > 1$.\n- **Objective:** Find the critical component count $K^{\\star}$ at which the collapsed and uncollapsed samplers have equal ESS per unit wall-clock time.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding (Critical):** The problem is well-grounded in the field of computational statistics and machine learning, specifically concerning MCMC methods. The concepts of Gibbs sampling, collapsed samplers, ESS, and autocorrelation time are standard tools for analyzing the efficiency of stochastic algorithms. The comparison between a standard and a collapsed sampler by modeling their respective costs and mixing rates is a classic and fundamentally sound approach.\n- **Well-Posed:** The problem provides all necessary definitions, parameters ($c, \\alpha, r, \\delta$), and functional forms to establish a solvable equation. The objective is to find a specific value, $K^{\\star}$, that satisfies a clear equality, leading to a unique solution.\n- **Objective (Critical):** The problem is stated using formal, precise, and unbiased language. There are no subjective or opinion-based elements.\n- **Incomplete or Contradictory Setup:** The problem is self-contained. All variables and assumptions are explicitly defined. There are no contradictions. The assumptions ($c>0$, $\\alpha>0$, $r \\in (0,1)$, $\\delta>1$) are consistent and ensure the problem is physically and mathematically meaningful (e.g., positive costs, decaying correlations).\n- **Other Flaws:** The problem does not violate any of the other criteria for invalidity. It is a formal, verifiable, and non-trivial problem squarely within the specified topic.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe efficiency, $\\eta$, is defined as being proportional to the inverse of the product of the cost per iteration and the integrated autocorrelation time. We are looking for the value of $K$, denoted $K^{\\star}$, where the efficiencies of the uncollapsed and collapsed samplers are equal.\n$$\n\\eta^{(U)} = \\eta^{(C)}\n$$\nSince the proportionality constant is the same for both, this implies:\n$$\n\\frac{1}{C^{(U)} \\tau_{\\mathrm{int}}^{(U)}} = \\frac{1}{C^{(C)} \\tau_{\\mathrm{int}}^{(C)}}\n$$\nThis simplifies to the condition that the total computational cost per effective sample is equal for both methods:\n$$\nC^{(U)} \\tau_{\\mathrm{int}}^{(U)} = C^{(C)} \\tau_{\\mathrm{int}}^{(C)}\n$$\nFirst, we must derive the closed-form expressions for the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, for each sampler. The definition is given as $\\tau_{\\mathrm{int}} = 1 + 2\\sum_{k=1}^{\\infty} \\rho_{k}$. The sum is a geometric series. For a generic lag parameter $p \\in (0,1)$ where $\\rho_k = p^k$, the sum is $\\sum_{k=1}^{\\infty} p^k = \\frac{p}{1-p}$.\nTherefore, the general form for $\\tau_{\\mathrm{int}}$ is:\n$$\n\\tau_{\\mathrm{int}} = 1 + 2 \\left( \\frac{p}{1-p} \\right) = \\frac{1-p+2p}{1-p} = \\frac{1+p}{1-p}\n$$\n\nFor the uncollapsed sampler, the lag parameter is $p = r$. The integrated autocorrelation time is:\n$$\n\\tau_{\\mathrm{int}}^{(U)} = \\frac{1+r}{1-r}\n$$\n\nFor the collapsed sampler, the lag parameter is $p = r^{\\delta}$. The integrated autocorrelation time is:\n$$\n\\tau_{\\mathrm{int}}^{(C)} = \\frac{1+r^{\\delta}}{1-r^{\\delta}}\n$$\n\nNow, we can substitute these expressions, along with the given costs, into our equality condition. The cost for the uncollapsed sampler is $C^{(U)} = c$, and for the collapsed sampler at the critical point $K^{\\star}$ it is $C^{(C)} = c + \\alpha K^{\\star}$.\n$$\nc \\left( \\frac{1+r}{1-r} \\right) = (c + \\alpha K^{\\star}) \\left( \\frac{1+r^{\\delta}}{1-r^{\\delta}} \\right)\n$$\nOur goal is to solve for $K^{\\star}$. We begin by isolating the term $(c + \\alpha K^{\\star})$:\n$$\nc + \\alpha K^{\\star} = c \\left( \\frac{1+r}{1-r} \\right) \\left( \\frac{1-r^{\\delta}}{1+r^{\\delta}} \\right)\n$$\nNext, we subtract $c$ from both sides to isolate the term with $K^{\\star}$:\n$$\n\\alpha K^{\\star} = c \\left[ \\left( \\frac{1+r}{1-r} \\right) \\left( \\frac{1-r^{\\delta}}{1+r^{\\delta}} \\right) - 1 \\right]\n$$\nTo simplify the expression in the brackets, we find a common denominator:\n$$\n\\alpha K^{\\star} = c \\left[ \\frac{(1+r)(1-r^{\\delta}) - (1-r)(1+r^{\\delta})}{(1-r)(1+r^{\\delta})} \\right]\n$$\nNow, we expand the terms in the numerator:\n$$\n\\begin{align*}\n(1+r)(1-r^{\\delta}) &= 1 - r^{\\delta} + r - r^{1+\\delta} \\\\\n(1-r)(1+r^{\\delta}) &= 1 + r^{\\delta} - r - r^{1+\\delta}\n\\end{align*}\n$$\nSubtracting the second expansion from the first:\n$$\n(1 - r^{\\delta} + r - r^{1+\\delta}) - (1 + r^{\\delta} - r - r^{1+\\delta}) = 2r - 2r^{\\delta} = 2(r - r^{\\delta})\n$$\nSubstituting this simplified numerator back into the equation for $\\alpha K^{\\star}$:\n$$\n\\alpha K^{\\star} = c \\left[ \\frac{2(r-r^{\\delta})}{(1-r)(1+r^{\\delta})} \\right]\n$$\nFinally, we solve for $K^{\\star}$ by dividing by $\\alpha$:\n$$\nK^{\\star} = \\frac{2c(r-r^{\\delta})}{\\alpha(1-r)(1+r^{\\delta})}\n$$\nThis is the closed-form expression for the critical component count $K^{\\star}$ at which the two samplers have equal efficiency. Given the constraints $c>0$, $\\alpha>0$, $r \\in (0,1)$, and $\\delta > 1$, we have $r > r^{\\delta}$, so $(r-r^{\\delta})>0$. All other terms are also positive, ensuring $K^{\\star} > 0$, which is physically meaningful for a component count.",
            "answer": "$$\n\\boxed{\\frac{2c(r-r^{\\delta})}{\\alpha(1-r)(1+r^{\\delta})}}\n$$"
        }
    ]
}