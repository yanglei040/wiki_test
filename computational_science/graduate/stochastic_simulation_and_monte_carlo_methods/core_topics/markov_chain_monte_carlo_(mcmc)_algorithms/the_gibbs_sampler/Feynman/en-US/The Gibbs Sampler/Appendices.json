{
    "hands_on_practices": [
        {
            "introduction": "The multivariate Gaussian distribution is a cornerstone of statistical modeling, and sampling from it is a fundamental task. This exercise builds foundational skills by having you derive and implement the Gibbs sampler for this core distribution . It reveals a surprising and elegant connection between a statistical sampling algorithm and the classic Gauss-Seidel method from numerical linear algebra, deepening your understanding of both fields.",
            "id": "3352959",
            "problem": "Consider a zero-mean multivariate Gaussian target distribution with density specified in terms of its precision matrix. Let $x \\in \\mathbb{R}^d$ be a random vector with distribution $x \\sim \\mathcal{N}(0, \\Sigma)$, and let the precision matrix be $Q = \\Sigma^{-1}$, where $Q$ is symmetric positive definite. The Gibbs sampler, a method within Markov Chain Monte Carlo (MCMC), updates each component $x_i$ conditional on the current values of all other components $x_{-i}$. \n\nTask 1 (Derivation): Starting from the joint density in canonical (precision) form and the definition of conditional distributions for a multivariate Gaussian, derive the univariate full conditional distribution of $x_i$ given $x_{-i}$, expressed solely in terms of $Q$ and $x_{-i}$. Your derivation must start from the joint density of $x$ in terms of $Q$ and proceed by partitioning $Q$ and completing the square in $x_i$; you must not invoke pre-formed conditioning formulas. \n\nTask 2 (Algorithmic equivalence): Consider a systematic scan Gibbs sweep that updates coordinates $i=1,2,\\ldots,d$ in order. Show that if each $x_i$ is set to the conditional mean (without stochastic sampling), the resulting mapping $x^{\\text{new}} \\leftarrow T(x^{\\text{old}})$ is equivalent to one Gauss–Seidel iteration for solving the linear system $Q x = 0$. Express this equivalence by decomposing $Q$ into $Q = D + L + U$, where $D$ is diagonal, $L$ is strictly lower triangular, and $U$ is strictly upper triangular, and derive the Gauss–Seidel update $x^{\\text{new}} = -(D+L)^{-1} U x^{\\text{old}}$ for the zero right-hand side.\n\nTask 3 (Implementation and verification): Implement a single deterministic Gibbs sweep that sets each component $x_i$ to its conditional mean, using updated values for indices already swept and old values for those not yet swept. Independently implement a single Gauss–Seidel step for $Q x = 0$ using the standard splitting $Q = D + L + U$ and the formula \n$$\nx_i^{\\text{new}} = \\frac{0 - \\sum_{j<i} Q_{ij} x_j^{\\text{new}} - \\sum_{j>i} Q_{ij} x_j^{\\text{old}}}{Q_{ii}}.\n$$\nVerify that the two updates coincide to within a specified numerical tolerance. \n\nYou must implement the program using the following test suite of symmetric positive definite matrices and initial vectors. For each case, the program must compute one deterministic Gibbs sweep and one Gauss–Seidel step and compare their outputs with absolute tolerance $10^{-10}$.\n\n- Case $1$: \n  $$\n  Q_1 = \\begin{bmatrix}\n  4 & -1 & 0 \\\\\n  -1 & 3 & -1 \\\\\n  0 & -1 & 2\n  \\end{bmatrix}, \\quad\n  x_1^{(0)} = \\begin{bmatrix}\n  1.0 \\\\ -2.0 \\\\ 0.5\n  \\end{bmatrix}.\n  $$\n\n- Case $2$: \n  $$\n  Q_2 = \\operatorname{diag}(1.5, 2.0, 3.0), \\quad\n  x_2^{(0)} = \\begin{bmatrix}\n  3.0 \\\\ -1.0 \\\\ 2.0\n  \\end{bmatrix}.\n  $$\n\n- Case $3$:\n  $$\n  Q_3 = \\begin{bmatrix}\n  2 & -1 & 0 & 0 \\\\\n  -1 & 2 & -1 & 0 \\\\\n  0 & -1 & 2 & -1 \\\\\n  0 & 0 & -1 & 2\n  \\end{bmatrix}, \\quad\n  x_3^{(0)} = \\begin{bmatrix}\n  0.0 \\\\ 5.0 \\\\ -3.0 \\\\ 1.0\n  \\end{bmatrix}.\n  $$\n\n- Case $4$: Let \n  $$\n  A = \\begin{bmatrix}\n  1.0 & 2.0 & 0.5 & -1.0 \\\\\n  -2.0 & 3.0 & 1.0 & 0.0 \\\\\n  0.0 & -1.0 & 2.5 & 1.5 \\\\\n  1.0 & 0.0 & -1.0 & 2.0\n  \\end{bmatrix},\n  $$\n  and define \n  $$\n  Q_4 = A^\\top A + 0.5 I_4, \\quad\n  x_4^{(0)} = \\begin{bmatrix}\n  -10.0 \\\\ 0.1 \\\\ 4.0 \\\\ -2.0\n  \\end{bmatrix}.\n  $$\n\nFor each of the four cases, the required output is a boolean indicating whether the deterministic Gibbs sweep output equals the Gauss–Seidel step output within the tolerance $10^{-10}$. \n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[{\\text{result}_1},{\\text{result}_2},{\\text{result}_3},{\\text{result}_4}]$, where each ${\\text{result}_k}$ is either $\\text{True}$ or $\\text{False}$.",
            "solution": "The user has provided a multi-part problem that requires derivation, thematic connection, and implementation concerning the Gibbs sampler for multivariate Gaussian distributions and its relationship to the Gauss-Seidel method for solving linear systems.\n\n### Problem Validation\n\n*   **Step 1: Extract Givens**\n    *   **Target Distribution**: A zero-mean multivariate Gaussian, $x \\in \\mathbb{R}^d$, with distribution $x \\sim \\mathcal{N}(0, \\Sigma)$.\n    *   **Precision Matrix**: $Q = \\Sigma^{-1}$, symmetric positive definite.\n    *   **Task 1**: Derive the univariate full conditional distribution of $x_i$ given $x_{-i}$, $p(x_i | x_{-i})$, in terms of $Q$ and $x_{-i}$, by starting from the joint density $p(x) \\propto \\exp(-\\frac{1}{2} x^\\top Q x)$ and completing the square.\n    *   **Task 2**: Show that a systematic scan Gibbs sweep where each $x_i$ is set to its conditional mean is equivalent to one Gauss–Seidel iteration for solving $Q x = 0$. This involves using the decomposition $Q = D + L + U$ and showing the equivalence of the update rules.\n    *   **Task 3**: Implement and verify the equivalence.\n        *   Implement a deterministic Gibbs sweep setting each component to its conditional mean.\n        *   Implement a Gauss–Seidel step using the provided component-wise formula.\n        *   Verify the outputs of both implementations are equal within an absolute tolerance of $10^{-10}$.\n    *   **Test Cases**: Four specific pairs of precision matrices $(Q_k)$ and initial vectors $(x_k^{(0)})$ are provided.\n    *   **Output Format**: A single-line string representing a list of booleans, e.g., `[True,True,True,True]`.\n\n*   **Step 2: Validate Using Extracted Givens**\n    *   **Scientific Grounding**: The problem is deeply grounded in standard, well-established principles of multivariate statistics (Gaussian distributions, conditional probability) and numerical linear algebra (iterative methods like Gauss-Seidel). The equivalence between deterministic Gibbs sampling on a Gaussian graphical model and Gauss-Seidel relaxation is a classic result.\n    *   **Well-Posed**: The tasks are clearly defined. The derivations are standard textbook exercises. The implementation task is specific, with all necessary data (matrices, vectors, tolerance) provided. The given matrices are constructed to be symmetric positive definite, ensuring the validity of the underlying statistical model and the convergence properties (for a sufficient number of iterations) of the Gauss-Seidel method.\n    *   **Objective**: The problem is stated in precise mathematical and algorithmic language, free of ambiguity or subjective claims.\n\n*   **Step 3: Verdict and Action**\n    *   The problem is **valid**. It is scientifically sound, well-posed, and objective. I will proceed with a full solution.\n\n---\n\n### Solution\n\nThe solution is presented in three parts, corresponding to the three tasks in the problem statement. All mathematical entities are typeset in LaTeX as required.\n\n#### Task 1: Derivation of the Full Conditional Distribution\n\nThe probability density function (PDF) of a zero-mean multivariate Gaussian random vector $x \\in \\mathbb{R}^d$ with precision matrix $Q$ is given by:\n$$\np(x) \\propto \\exp\\left(-\\frac{1}{2} x^\\top Q x\\right)\n$$\nwhere the term $x^\\top Q x$ is a quadratic form that can be written as $\\sum_{j=1}^d \\sum_{k=1}^d Q_{jk} x_j x_k$.\n\nTo derive the full conditional distribution $p(x_i | x_{-i})$, where $x_{-i}$ denotes the vector $x$ with component $x_i$ removed, we treat all components $x_j$ for $j \\neq i$ as constants. The conditional density is proportional to the joint density:\n$$\np(x_i | x_{-i}) \\propto p(x) = p(x_i, x_{-i})\n$$\nWe can, therefore, analyze the exponent of the joint PDF, $-\\frac{1}{2} x^\\top Q x$, and discard any terms that do not depend on $x_i$, as they will be absorbed into the normalization constant of $p(x_i | x_{-i})$.\n\nLet's expand the quadratic form and isolate all terms containing $x_i$:\n$$\nx^\\top Q x = \\sum_{j=1}^d \\sum_{k=1}^d Q_{jk} x_j x_k = Q_{ii} x_i^2 + \\sum_{j \\neq i} Q_{ij} x_i x_j + \\sum_{k \\neq i} Q_{ki} x_k x_i + \\sum_{j \\neq i} \\sum_{k \\neq i} Q_{jk} x_j x_k\n$$\nSince $Q$ is symmetric ($Q_{ij} = Q_{ji}$), the two cross-terms are identical:\n$$\nx^\\top Q x = Q_{ii} x_i^2 + 2x_i \\sum_{j \\neq i} Q_{ij} x_j + \\sum_{j \\neq i} \\sum_{k \\neq i} Q_{jk} x_j x_k\n$$\nThe last term, $\\sum_{j \\neq i} \\sum_{k \\neq i} Q_{jk} x_j x_k$, does not depend on $x_i$ and can be treated as a constant. Thus, the conditional density depends on $x_i$ as follows:\n$$\np(x_i | x_{-i}) \\propto \\exp\\left(-\\frac{1}{2} \\left[ Q_{ii} x_i^2 + 2x_i \\sum_{j \\neq i} Q_{ij} x_j \\right]\\right)\n$$\nTo identify the form of this distribution, we complete the square with respect to $x_i$. The general form of the exponent for a univariate Gaussian distribution $\\mathcal{N}(\\mu, \\sigma^2)$ is:\n$$\n-\\frac{1}{2\\sigma^2} (x_i - \\mu)^2 = -\\frac{1}{2\\sigma^2} (x_i^2 - 2\\mu x_i + \\mu^2) = -\\frac{1}{2} \\left( \\frac{1}{\\sigma^2} x_i^2 - \\frac{2\\mu}{\\sigma^2} x_i \\right) + \\text{const}\n$$\nComparing the coefficients of $x_i^2$ and $x_i$ from our derived exponent, we have:\n$$\n\\frac{1}{\\sigma^2} = Q_{ii} \\implies \\sigma^2 = \\frac{1}{Q_{ii}}\n$$\nAnd for the linear term:\n$$\n\\frac{2\\mu}{\\sigma^2} = -2\\sum_{j \\neq i} Q_{ij} x_j \\implies \\mu Q_{ii} = -\\sum_{j \\neq i} Q_{ij} x_j \\implies \\mu = -\\frac{1}{Q_{ii}} \\sum_{j \\neq i} Q_{ij} x_j\n$$\nTherefore, the full conditional distribution of $x_i$ is a univariate Gaussian:\n$$\np(x_i | x_{-i}) = \\mathcal{N}\\left(x_i \\bigg| \\mu_i = -\\frac{1}{Q_{ii}} \\sum_{j \\neq i} Q_{ij} x_j, \\quad \\sigma_i^2 = \\frac{1}{Q_{ii}}\\right)\n$$\n\n#### Task 2: Algorithmic Equivalence with Gauss-Seidel\n\nA deterministic Gibbs sweep updates each component $x_i$ to its conditional expectation, ignoring the stochastic sampling step. The conditional mean of $x_i$ is $\\mathbb{E}[x_i | x_{-i}] = \\mu_i$, as derived in Task $1$.\n\nIn a systematic scan Gibbs sweep, the components are updated in a fixed order, for instance, $i=1, 2, \\ldots, d$. When updating $x_i$, the values for components $x_j$ with $j < i$ have already been updated in the current sweep, while for $j > i$ they are still from the previous state. Let $x^{\\text{old}}$ be the vector before the sweep and $x^{\\text{new}}$ be the vector after the sweep. The update for the $i$-th component is:\n$$\nx_i^{\\text{new}} = \\mathbb{E}[x_i | x_1^{\\text{new}}, \\ldots, x_{i-1}^{\\text{new}}, x_{i+1}^{\\text{old}}, \\ldots, x_d^{\\text{old}}]\n$$\nUsing the formula for the conditional mean:\n$$\nx_i^{\\text{new}} = -\\frac{1}{Q_{ii}} \\left( \\sum_{j=1}^{i-1} Q_{ij} x_j^{\\text{new}} + \\sum_{j=i+1}^{d} Q_{ij} x_j^{\\text{old}} \\right)\n$$\nThis can be rearranged:\n$$\nQ_{ii} x_i^{\\text{new}} = - \\sum_{j=1}^{i-1} Q_{ij} x_j^{\\text{new}} - \\sum_{j=i+1}^{d} Q_{ij} x_j^{\\text{old}}\n$$\n$$\nQ_{ii} x_i^{\\text{new}} + \\sum_{j=1}^{i-1} Q_{ij} x_j^{\\text{new}} = - \\sum_{j=i+1}^{d} Q_{ij} x_j^{\\text{old}}\n$$\n$$\n\\sum_{j=1}^{i} Q_{ij} x_j^{\\text{new}} = - \\sum_{j=i+1}^{d} Q_{ij} x_j^{\\text{old}}\n$$\nNow, consider the Gauss-Seidel method for solving the linear system $Ax=b$. The iterative update for the $i$-th component is:\n$$\nx_i^{(k+1)} = \\frac{1}{A_{ii}} \\left(b_i - \\sum_{j=1}^{i-1} A_{ij} x_j^{(k+1)} - \\sum_{j=i+1}^{d} A_{ij} x_j^{(k)} \\right)\n$$\nFor our problem, the linear system is $Qx=0$, so we have $A=Q$ and $b=0$. Substituting these into the Gauss-Seidel formula and using $x^{\\text{new}}$ for $x^{(k+1)}$ and $x^{\\text{old}}$ for $x^{(k)}$:\n$$\nx_i^{\\text{new}} = \\frac{1}{Q_{ii}} \\left(0 - \\sum_{j=1}^{i-1} Q_{ij} x_j^{\\text{new}} - \\sum_{j=i+1}^{d} Q_{ij} x_j^{\\text{old}} \\right)\n$$\nThis is precisely the update rule derived for the deterministic Gibbs sweep.\n\nTo express this in matrix form, we decompose the matrix $Q$ into its diagonal ($D$), strictly lower triangular ($L$), and strictly upper triangular ($U$) parts, such that $Q = D + L + U$. The equation $\\sum_{j=1}^{i} Q_{ij} x_j^{\\text{new}} = - \\sum_{j=i+1}^{d} Q_{ij} x_j^{\\text{old}}$ holds for each $i=1, \\ldots, d$. In matrix notation, this set of equations is:\n$$\n(D+L) x^{\\text{new}} = -U x^{\\text{old}}\n$$\nThe matrix $D+L$ is lower triangular with non-zero diagonal entries (since $Q$ is positive definite, $Q_{ii} > 0$), so it is invertible. Solving for $x^{\\text{new}}$ gives the matrix form of the Gauss-Seidel iteration:\n$$\nx^{\\text{new}} = -(D+L)^{-1} U x^{\\text{old}}\n$$\nThis establishes the formal equivalence between one deterministic Gibbs sweep and one Gauss-Seidel iteration for the system $Qx=0$.\n\n#### Task 3: Implementation and Verification\n\nThe final task is to implement the two procedures described in Task $2$ and verify that they produce identical results for the given test cases. The first implementation, `deterministic_gibbs_sweep`, directly models the systematic scan by updating a vector in-place. The second, `gauss_seidel_step`, explicitly uses the problem's formula, separating sums over \"new\" and \"old\" values. Their numerical equivalence, as proven above, is confirmed by the code in the final answer.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing and comparing a deterministic Gibbs sweep\n    with a Gauss-Seidel step for the specified test cases.\n    \"\"\"\n\n    def deterministic_gibbs_sweep(Q: np.ndarray, x_old: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Performs a single deterministic Gibbs sweep.\n\n        This implementation directly models the systematic scan by updating a\n        single vector in-place. For each component x_i, the update uses the\n        most current values of all other components x_j.\n        \"\"\"\n        d = Q.shape[0]\n        x = np.copy(x_old)  # x will be updated in-place\n        for i in range(d):\n            # The conditional mean is E[x_i | x_{-i}] = - (1/Q_ii) * sum_{j!=i} Q_ij * x_j.\n            # In a systematic scan, this sum uses already-updated values for j<i\n            # and old values for j>i. An in-place update achieves this.\n            # We can calculate the sum efficiently: sum_{j!=i} Q_ij * x_j = (Q[i,:] @ x) - Q_ii * x_i\n            off_diagonal_dot_product = np.dot(Q[i, :], x) - Q[i, i] * x[i]\n            x[i] = -off_diagonal_dot_product / Q[i, i]\n        return x\n\n    def gauss_seidel_step(Q: np.ndarray, x_old: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Performs a single Gauss-Seidel step for the system Qx = 0.\n\n        This implementation explicitly follows the formula provided in the problem,\n        separating sums over components with j < i and j > i.\n        \"\"\"\n        d = Q.shape[0]\n        x_new = np.copy(x_old)\n        for i in range(d):\n            # Formula: x_i^new = (- sum_{j<i} Q_ij x_j^new - sum_{j>i} Q_ij x_j^old) / Q_ii\n            \n            # Sum over j < i, using values of x_new updated in this sweep\n            sum_lower = np.dot(Q[i, :i], x_new[:i])\n            \n            # Sum over j > i, using values from the previous iteration vector, x_old\n            sum_upper = np.dot(Q[i, i+1:], x_old[i+1:])\n            \n            x_new[i] = -(sum_lower + sum_upper) / Q[i, i]\n        return x_new\n\n    # --- Test Cases ---\n\n    # Case 1\n    Q1 = np.array([\n        [4., -1., 0.],\n        [-1., 3., -1.],\n        [0., -1., 2.]\n    ])\n    x1_0 = np.array([1.0, -2.0, 0.5])\n\n    # Case 2\n    Q2 = np.diag([1.5, 2.0, 3.0])\n    x2_0 = np.array([3.0, -1.0, 2.0])\n\n    # Case 3\n    Q3 = np.array([\n        [2., -1., 0., 0.],\n        [-1., 2., -1., 0.],\n        [0., -1., 2., -1.],\n        [0., 0., -1., 2.]\n    ])\n    x3_0 = np.array([0.0, 5.0, -3.0, 1.0])\n\n    # Case 4\n    A4 = np.array([\n        [1.0, 2.0, 0.5, -1.0],\n        [-2.0, 3.0, 1.0, 0.0],\n        [0.0, -1.0, 2.5, 1.5],\n        [1.0, 0.0, -1.0, 2.0]\n    ])\n    Q4 = A4.T @ A4 + 0.5 * np.identity(4)\n    x4_0 = np.array([-10.0, 0.1, 4.0, -2.0])\n\n    test_cases = [\n        (Q1, x1_0),\n        (Q2, x2_0),\n        (Q3, x3_0),\n        (Q4, x4_0),\n    ]\n\n    results = []\n    tolerance = 1e-10\n\n    for Q, x_old in test_cases:\n        # Compute the result from both implementations\n        res_gibbs = deterministic_gibbs_sweep(Q, x_old)\n        res_gs = gauss_seidel_step(Q, x_old)\n        \n        # Verify that the two results are equal within the given tolerance\n        are_equal = np.allclose(res_gibbs, res_gs, atol=tolerance, rtol=0)\n        results.append(are_equal)\n\n    # Final print statement in the exact required format.\n    # str(True) -> 'True', str(False) -> 'False'\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While the power of the Gibbs sampler lies in its simplicity, this can be deceptive. Naively applying the Gibbs recipe without considering the geometry of the state space can lead to a sampler that fails to explore the target distribution. This practice problem illustrates a critical failure mode—the loss of irreducibility—on a constrained manifold, which causes the sampler to get stuck, and introduces variable blocking as the necessary solution .",
            "id": "3352921",
            "problem": "Consider a target probability distribution on a constrained support defined by the one-dimensional manifold \n$$\n\\mathcal{S} \\equiv \\{(x_{1},x_{2}) \\in [0,1]^{2} : x_{1}+x_{2}=1\\}.\n$$\nLet the unnormalized density on $\\mathcal{S}$ be given by \n$$\n\\tilde{\\pi}(x_{1},x_{2}) = x_{1}^{\\alpha_{1}-1} x_{2}^{\\alpha_{2}-1} \\mathbf{1}\\{(x_{1},x_{2}) \\in \\mathcal{S}\\},\n$$\nwith fixed shape parameters $\\alpha_{1} > 0$ and $\\alpha_{2} > 0$. This induces a proper probability distribution $\\pi$ on $\\mathcal{S}$ with respect to the one-dimensional Lebesgue measure on the line segment $\\mathcal{S}$. You are asked to analyze the irreducibility of a Gibbs sampling scheme under this constraint and to construct a viable alternative.\n\nTasks:\n1) Using the definition of conditional distributions derived from joint densities, and the definition of irreducibility for Markov chains on general state spaces, justify whether a naive single-coordinate Gibbs sampler that alternates updating $x_{1}$ given $x_{2}$ and then $x_{2}$ given $x_{1}$ (each from the exact full conditional) is irreducible on $\\mathcal{S}$. Your analysis must start from first principles and may not assume any special-case results beyond those definitions.\n\n2) Propose a block Gibbs update that restores irreducibility on $\\mathcal{S}$. Argue from the definition of irreducibility (and the support of the block conditional) why your block Gibbs kernel is irreducible with respect to the one-dimensional Lebesgue measure on $\\mathcal{S}$.\n\n3) Under your valid block Gibbs scheme, the stationary distribution is $\\pi$. Let $\\alpha_{1}=2$ and $\\alpha_{2}=3$. Compute the stationary expectation \n$$\n\\mathbb{E}_{\\pi}\\!\\left[x_{1}^{2} x_{2}\\right].\n$$\nYour final answer must be a single exact number or a single closed-form expression. No rounding is required.",
            "solution": "This problem requires an analysis of a Gibbs sampling scheme on a constrained state space, a proposal for a valid alternative, and a calculation of an expectation under the resulting stationary distribution. The validation and solution proceed as follows.\n\nThe problem is deemed valid. It is scientifically grounded in the theory of Markov chain Monte Carlo methods, specifically Gibbs sampling. It is well-posed, with a clearly defined target distribution and state space, and tasks that admit unique, derivable solutions. The language is objective and formal. The problem, while illustrating a fundamental issue with naive Gibbs sampling on manifolds, is not trivial and requires a firm grasp of the definitions of conditional probability and Markov chain irreducibility.\n\nThe solution is divided into three parts corresponding to the tasks.\n\n1) Analysis of the naive single-coordinate Gibbs sampler.\n\nA Gibbs sampler is a Markov chain Monte Carlo algorithm for obtaining a sequence of observations which are approximated as being from a specified multivariate probability distribution, when direct sampling is difficult. The state of the chain is updated by sampling each variable (or coordinate) from its full conditional distribution given the current values of all other variables.\n\nThe target distribution $\\pi$ has support on the set $\\mathcal{S} \\equiv \\{(x_{1},x_{2}) \\in [0,1]^{2} : x_{1}+x_{2}=1\\}$. The unnormalized density is given by $\\tilde{\\pi}(x_{1},x_{2}) = x_{1}^{\\alpha_{1}-1} x_{2}^{\\alpha_{2}-1}$ for $(x_{1},x_{2}) \\in \\mathcal{S}$, and $\\tilde{\\pi}(x_{1},x_{2})=0$ otherwise.\n\nLet us analyze the full conditional distributions. The full conditional distribution for $x_{1}$ given $x_{2}$ has a density $p(x_{1}|x_{2})$ that is proportional to the joint density $\\pi(x_{1},x_{2})$.\n$$\np(x_{1}|x_{2}) \\propto \\tilde{\\pi}(x_{1},x_{2}) = x_{1}^{\\alpha_{1}-1} x_{2}^{\\alpha_{2}-1} \\mathbf{1}\\{(x_{1},x_{2}) \\in \\mathcal{S}\\}\n$$\nThe indicator function $\\mathbf{1}\\{(x_{1},x_{2}) \\in \\mathcal{S}\\}$ is crucial. Given a value for $x_{2} \\in [0,1]$, this indicator is non-zero only if $x_{1} \\in [0,1]$ and $x_{1}+x_{2}=1$. This deterministically fixes the value of $x_{1}$ to be $x_{1}=1-x_{2}$. Therefore, the support of the conditional distribution $p(x_{1}|x_{2})$ is the single point $\\{1-x_{2}\\}$. The conditional distribution is a point mass (or Dirac delta measure) at $x_{1}=1-x_{2}$. There is no \"sampling\" in the usual sense; the value is fully determined.\n\nSimilarly, the full conditional distribution for $x_{2}$ given $x_{1}$, $p(x_{2}|x_{1})$, is a point mass at $x_{2}=1-x_{1}$.\n\nA single-coordinate Gibbs sampler proceeds as follows. Starting from a state $(x_{1}^{(t)}, x_{2}^{(t)}) \\in \\mathcal{S}$ at iteration $t$:\ni) Sample $x_{1}^{(t+1)}$ from $p(x_{1}|x_{2}^{(t)})$. As derived, this means we set $x_{1}^{(t+1)} = 1 - x_{2}^{(t)}$.\nii) Sample $x_{2}^{(t+1)}$ from $p(x_{2}|x_{1}^{(t+1)})$. This means we set $x_{2}^{(t+1)} = 1 - x_{1}^{(t+1)}$.\n\nSince the starting point $(x_{1}^{(t)}, x_{2}^{(t)})$ is on the support $\\mathcal{S}$, we know $x_{1}^{(t)}+x_{2}^{(t)}=1$, which implies $x_{1}^{(t)} = 1-x_{2}^{(t)}$.\nSubstituting this into step (i), we find $x_{1}^{(t+1)} = x_{1}^{(t)}$.\nThen, for step (ii), we have $x_{2}^{(t+1)} = 1 - x_{1}^{(t+1)} = 1 - x_{1}^{(t)} = x_{2}^{(t)}$.\nThe resulting state is $(x_{1}^{(t+1)}, x_{2}^{(t+1)}) = (x_{1}^{(t)}, x_{2}^{(t)})$. The chain never moves from its initial state.\n\nA Markov chain with transition kernel $K$ is said to be irreducible with respect to its stationary distribution $\\pi$ (or $\\pi$-irreducible) if for any starting state $x \\in \\mathcal{S}$ and any set $A \\subseteq \\mathcal{S}$ such that $\\pi(A)>0$, there exists an integer $n \\ge 1$ for which the $n$-step transition probability $K^n(x,A) > 0$.\nIn our case, the sampler, if started at a point $y \\in \\mathcal{S}$, remains at $y$ for all subsequent iterations. Let $z \\in \\mathcal{S}$ be any other point such that $z \\neq y$. Consider a small open neighborhood $A$ around $z$ that does not contain $y$. Since $\\pi$ has positive density over the interior of $\\mathcal{S}$, $\\pi(A)>0$. However, the probability of the chain ever reaching $A$ from $y$ is $K^n(y, A) = 0$ for all $n \\ge 1$.\nThis violates the definition of irreducibility. The naive single-coordinate Gibbs sampler is not irreducible on $\\mathcal{S}$.\n\n2) Block Gibbs update to restore irreducibility.\n\nThe failure of the single-coordinate sampler is due to the perfect deterministic dependency between $x_{1}$ and $x_{2}$ imposed by the constraint. The correct way to handle such a situation is to group the dependent variables into a \"block\" and update them jointly. Here, the block must be $(x_{1}, x_{2})$.\n\nA block Gibbs sampler for the block $(x_{1}, x_{2})$ would update the state by sampling a new pair $(x_{1}', x_{2}')$ from its joint distribution conditional on all other variables. Since there are no other variables in this model, this simply means drawing a new state directly from the target distribution $\\pi(x_{1}, x_{2})$. This type of MCMC where the new state is drawn from the target distribution, independently of the current state, is known as an independent sampler.\n\nThe transition kernel for this sampler is $K((x_{1}',x_{2}') | (x_{1},x_{2})) = \\pi(x_{1}',x_{2}')$.\n\nTo demonstrate irreducibility, we use the same definition as before. Let $\\lambda_1$ be the one-dimensional Lebesgue measure on the line segment $\\mathcal{S}$. The target distribution $\\pi$ has a density with respect to $\\lambda_1$. For $\\alpha_{1}>0$ and $\\alpha_{2}>0$, this density is positive on the interior of $\\mathcal{S}$.\nLet $x = (x_1, x_2)$ be any starting point in $\\mathcal{S}$. Let $A$ be any subset of $\\mathcal{S}$ with positive measure under $\\pi$, which implies $\\lambda_1(A)>0$. The probability of transitioning from $x$ to the set $A$ in one step is:\n$$\nK(x, A) = \\int_{A} K(y | x) d\\lambda_1(y) = \\int_{A} \\pi(y) d\\lambda_1(y) = \\pi(A)\n$$\nSince $\\pi(A) > 0$ by our assumption, the transition probability $K(x, A)$ is positive for $n=1$. This holds for any starting point $x \\in \\mathcal{S}$ and any set $A$ with $\\pi(A)>0$. Thus, the block Gibbs sampler (which is an independent sampler in this case) is irreducible with respect to $\\pi$.\n\n3) Computation of the expectation.\n\nWe are asked to compute $\\mathbb{E}_{\\pi}[x_{1}^{2} x_{2}]$ with $\\alpha_{1}=2$ and $\\alpha_{2}=3$.\nThe distribution is on the one-dimensional manifold $\\mathcal{S}$. We can parameterize this manifold with a single variable. Let $x_{1} = u$. Due to the constraint $x_1+x_2=1$, we have $x_{2}=1-u$. Since $x_1, x_2 \\in [0,1]$, we must have $u \\in [0,1]$.\nSubstituting this into the unnormalized density, we get a density for $u$:\n$$\n\\tilde{p}(u) \\propto u^{\\alpha_{1}-1} (1-u)^{\\alpha_{2}-1} \\quad \\text{for } u \\in [0,1]\n$$\nThis is the kernel of a Beta distribution. Therefore, the random variable $u$ corresponding to $x_1$ follows a Beta distribution, $u \\sim \\text{Beta}(\\alpha_{1}, \\alpha_{2})$.\nWith the given parameters $\\alpha_{1}=2$ and $\\alpha_{2}=3$, we have $u \\sim \\text{Beta}(2, 3)$.\nThe expectation we need to compute is $\\mathbb{E}_{\\pi}[x_{1}^{2} x_{2}]$, which in terms of $u$ becomes $\\mathbb{E}[u^2(1-u)]$.\n$$\n\\mathbb{E}[u^{2}(1-u)] = \\mathbb{E}[u^{2} - u^{3}] = \\mathbb{E}[u^{2}] - \\mathbb{E}[u^{3}]\n$$\nWe use the formula for the $k$-th raw moment of a $\\text{Beta}(\\alpha,\\beta)$ random variable:\n$$\n\\mathbb{E}[u^{k}] = \\frac{\\Gamma(\\alpha+k)\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\alpha+\\beta+k)}\n$$\nFor $\\alpha=2$ and $\\beta=3$, we have $\\alpha+\\beta=5$. The formula simplifies to:\n$$\n\\mathbb{E}[u^{k}] = \\frac{\\Gamma(2+k)\\Gamma(5)}{\\Gamma(2)\\Gamma(5+k)}\n$$\nFor $k=2$:\n$$\n\\mathbb{E}[u^{2}] = \\frac{\\Gamma(4)\\Gamma(5)}{\\Gamma(2)\\Gamma(7)} = \\frac{3! \\cdot 4!}{1! \\cdot 6!} = \\frac{6 \\cdot 24}{720} = \\frac{144}{720} = \\frac{1}{5}\n$$\nFor $k=3$:\n$$\n\\mathbb{E}[u^{3}] = \\frac{\\Gamma(5)\\Gamma(5)}{\\Gamma(2)\\Gamma(8)} = \\frac{4! \\cdot 4!}{1! \\cdot 7!} = \\frac{24 \\cdot 24}{5040} = \\frac{576}{5040} = \\frac{4}{35}\n$$\nSubtracting the moments, we obtain the final result:\n$$\n\\mathbb{E}_{\\pi}[x_{1}^{2} x_{2}] = \\mathbb{E}[u^{2}] - \\mathbb{E}[u^{3}] = \\frac{1}{5} - \\frac{4}{35} = \\frac{7}{35} - \\frac{4}{35} = \\frac{3}{35}\n$$",
            "answer": "$$\\boxed{\\frac{3}{35}}$$"
        },
        {
            "introduction": "A valid Gibbs sampler is only a starting point; its practical utility depends on its efficiency, or how quickly it explores the posterior distribution. This exercise demonstrates how to diagnose and solve the common problem of slow mixing due to high parameter correlation using reparameterization and blocking . By analytically comparing the spectral gaps of a naive sampler and an improved one, you will gain a rigorous understanding of what makes a sampler efficient and how to design better ones.",
            "id": "3352929",
            "problem": "Consider a bivariate target distribution with density proportional to $p(x,y)\\propto \\exp\\left\\{-\\tfrac{1}{2}\\left(x^2 + y^2 - 2\\rho xy\\right)\\right\\}$, where $\\rho\\in(-1,1)$ and $(x,y)\\in\\mathbb{R}^2$. This defines a Gaussian Markov random field with precision matrix $Q=\\begin{pmatrix}1 & -\\rho \\\\ -\\rho & 1\\end{pmatrix}$ and hence a unique normalizing constant. Beginning from fundamental definitions of the Gibbs sampler, the multivariate normal distribution, and the Markov chain spectral gap, complete the following tasks:\n\n$1.$ Derive the full conditional distributions $p(x\\mid y)$ and $p(y\\mid x)$ by manipulating the joint quadratic form directly, without invoking pre-derived shortcut formulas. You may use standard facts about completing the square and properties of the multivariate normal distribution as foundational tools.\n\n$2.$ Introduce the reparameterization $(u,v)$ defined by $u=x+y$ and $v=x-y$. Use linear algebraic properties of covariance transformations to characterize the joint distribution of $(u,v)$ and determine whether $u$ and $v$ are independent under the target. Design a blocked Gibbs sampler that updates $(u,v)$ in a single block and maps back to $(x,y)$ via the inverse linear transformation $x=(u+v)/2$, $y=(u-v)/2$.\n\n$3.$ For the coordinate-wise Gibbs sampler that performs a full sweep $(x\\to y\\to x)$, formalize its Markov transition operator acting on square-integrable functions with respect to the stationary distribution. Using first principles, determine the second-largest eigenvalue modulus of this operator and hence its spectral gap. Define the spectral gap as $1-\\lambda_{\\star}$, where $\\lambda_{\\star}$ is the largest eigenvalue modulus of the operator restricted to mean-zero functions in $L^2(\\pi)$, with $\\pi$ the stationary distribution.\n\n$4.$ For the reparameterized blocked Gibbs sampler in $(u,v)$ (updated jointly), similarly determine the spectral gap by analyzing the transition operator in the transformed coordinates and then translating the result back to $(x,y)$.\n\n$5.$ Implement a self-contained program that, for a given set of correlation values $\\rho$, computes for each $\\rho$ a pair of floats: the spectral gap of the coordinate-wise Gibbs sampler in $(x,y)$ and the spectral gap of the blocked Gibbs sampler in $(u,v)$, using your derived expressions. No simulation is required; compute the spectral gaps exactly from your derivations.\n\nTest Suite:\nEvaluate your program on the following set of $\\rho$ values to exercise a variety of regimes:\n- $\\rho = 0$\n- $\\rho = 0.3$\n- $\\rho = 0.9$\n- $\\rho = 0.99$\n- $\\rho = -0.9$\n- $\\rho = 0.999$\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one entry per test case. Each entry must be a two-element list containing the spectral gap of the coordinate-wise Gibbs sampler in $(x,y)$ followed by the spectral gap of the blocked Gibbs sampler in $(u,v)$. For example, the output should look like $[[g_{xy,1},g_{uv,1}],[g_{xy,2},g_{uv,2}],\\dots]$, where $g_{xy,i}$ and $g_{uv,i}$ are floats in standard decimal notation for the $i$-th $\\rho$ value.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the theory of Markov Chain Monte Carlo methods, specifically the analysis of the Gibbs sampler for a bivariate normal distribution. The problem is well-posed, with all necessary definitions and constraints provided, and is free of ambiguity or contradiction. The parameter restriction $\\rho \\in (-1, 1)$ ensures the precision matrix $Q$ is positive definite, defining a proper probability distribution. The tasks are sequential and lead to a unique, verifiable set of analytical results.\n\n### Part 1: Derivation of Full Conditional Distributions\n\nThe target distribution has a probability density function $p(x, y)$ proportional to the given expression. Let $\\mathbf{z} = (x, y)^T$. The density is of the form $p(\\mathbf{z}) \\propto \\exp\\left(-\\frac{1}{2}\\mathbf{z}^T Q \\mathbf{z}\\right)$, where $Q = \\begin{pmatrix} 1 & -\\rho \\\\ -\\rho & 1 \\end{pmatrix}$ is the precision matrix. The un-normalized density is:\n$$\np(x,y) \\propto \\exp\\left\\{-\\frac{1}{2}\\left(x^2 + y^2 - 2\\rho xy\\right)\\right\\}\n$$\nTo derive the full conditional distribution $p(x \\mid y)$, we treat $y$ as a constant and view the expression as a function of $x$. Any terms not involving $x$ are absorbed into the normalization constant.\n$$\np(x \\mid y) \\propto \\exp\\left\\{-\\frac{1}{2}\\left(x^2 - 2\\rho yx + y^2\\right)\\right\\} \\propto \\exp\\left\\{-\\frac{1}{2}\\left(x^2 - 2(\\rho y)x\\right)\\right\\}\n$$\nThe term in the exponent is a quadratic in $x$. We complete the square for $x$:\n$$\nx^2 - 2(\\rho y)x = (x - \\rho y)^2 - (\\rho y)^2 = (x - \\rho y)^2 - \\rho^2 y^2\n$$\nSubstituting this back into the expression for $p(x \\mid y)$:\n$$\np(x \\mid y) \\propto \\exp\\left\\{-\\frac{1}{2}\\left((x - \\rho y)^2 - \\rho^2 y^2\\right)\\right\\} = \\exp\\left\\{-\\frac{1}{2}(x - \\rho y)^2\\right\\} \\exp\\left\\{\\frac{1}{2}\\rho^2 y^2\\right\\}\n$$\nThe term $\\exp\\left\\{\\frac{1}{2}\\rho^2 y^2\\right\\}$ is constant with respect to $x$ and can be absorbed into the normalization constant. We are left with:\n$$\np(x \\mid y) \\propto \\exp\\left\\{-\\frac{(x - \\rho y)^2}{2}\\right\\}\n$$\nThis is the kernel of a normal distribution with mean $\\mu = \\rho y$ and variance $\\sigma^2 = 1$. Thus, the full conditional distribution for $x$ is:\n$$\nx \\mid y \\sim \\mathcal{N}(\\rho y, 1)\n$$\nBy symmetry of the joint distribution's quadratic form with respect to $x$ and $y$, the full conditional distribution for $y$ given $x$ is obtained by swapping their roles:\n$$\ny \\mid x \\sim \\mathcal{N}(\\rho x, 1)\n$$\n\n### Part 2: Reparameterization and Blocked Gibbs Sampler\n\nThe target distribution is a zero-mean bivariate normal, $\\mathbf{z} = (x, y)^T \\sim \\mathcal{N}(\\mathbf{0}, \\Sigma)$, where the covariance matrix $\\Sigma$ is the inverse of the precision matrix $Q$:\n$$\n\\Sigma = Q^{-1} = \\begin{pmatrix}1 & -\\rho \\\\ -\\rho & 1\\end{pmatrix}^{-1} = \\frac{1}{\\det(Q)}\\begin{pmatrix}1 & \\rho \\\\ \\rho & 1\\end{pmatrix} = \\frac{1}{1-\\rho^2}\\begin{pmatrix}1 & \\rho \\\\ \\rho & 1\\end{pmatrix}\n$$\nWe introduce the reparameterization $\\mathbf{w} = (u,v)^T$ via the linear transformation $\\mathbf{w} = A\\mathbf{z}$, where $A = \\begin{pmatrix}1 & 1 \\\\ 1 & -1\\end{pmatrix}$.\nSince $\\mathbf{z}$ is multivariate normal, the transformed vector $\\mathbf{w}$ is also multivariate normal. Its mean is $E[\\mathbf{w}] = A E[\\mathbf{z}] = A\\mathbf{0} = \\mathbf{0}$. Its covariance matrix, $\\Sigma_{\\mathbf{w}}$, is given by the transformation rule $\\Sigma_{\\mathbf{w}} = A \\Sigma A^T$.\n$$\n\\Sigma_{\\mathbf{w}} = \\begin{pmatrix}1 & 1 \\\\ 1 & -1\\end{pmatrix} \\left(\\frac{1}{1-\\rho^2}\\begin{pmatrix}1 & \\rho \\\\ \\rho & 1\\end{pmatrix}\\right) \\begin{pmatrix}1 & 1 \\\\ 1 & -1\\end{pmatrix}^T\n$$\n$$\n\\Sigma_{\\mathbf{w}} = \\frac{1}{1-\\rho^2} \\begin{pmatrix}1+\\rho & 1+\\rho \\\\ 1-\\rho & \\rho-1\\end{pmatrix} \\begin{pmatrix}1 & 1 \\\\ 1 & -1\\end{pmatrix}\n$$\n$$\n\\Sigma_{\\mathbf{w}} = \\frac{1}{1-\\rho^2} \\begin{pmatrix}(1+\\rho)+(1+\\rho) & (1+\\rho)-(1+\\rho) \\\\ (1-\\rho)+(\\rho-1) & (1-\\rho)-(\\rho-1)\\end{pmatrix} = \\frac{1}{(1-\\rho)(1+\\rho)} \\begin{pmatrix}2(1+\\rho) & 0 \\\\ 0 & 2(1-\\rho)\\end{pmatrix}\n$$\n$$\n\\Sigma_{\\mathbf{w}} = \\begin{pmatrix}\\frac{2}{1-\\rho} & 0 \\\\ 0 & \\frac{2}{1+\\rho}\\end{pmatrix}\n$$\nThe covariance matrix $\\Sigma_{\\mathbf{w}}$ is diagonal, which implies that the random variables $u$ and $v$ are uncorrelated. Because they are jointly Gaussian, this also implies they are independent. Their marginal distributions are:\n$$\nu \\sim \\mathcal{N}\\left(0, \\frac{2}{1-\\rho}\\right) \\quad \\text{and} \\quad v \\sim \\mathcal{N}\\left(0, \\frac{2}{1+\\rho}\\right)\n$$\nA blocked Gibbs sampler that updates $(u,v)$ jointly would draw a new pair $(u', v')$ from their joint distribution $p(u,v)$. Since $u$ and $v$ are independent, this is equivalent to drawing $u'$ and $v'$ independently from their respective marginal distributions. This procedure is, in fact, an independent sampler from the target distribution expressed in $(u,v)$ coordinates. After drawing $(u', v')$, the sampler maps back to the original coordinates using the inverse transformation: $x' = (u'+v')/2$ and $y' = (u'-v')/2$.\n\n### Part 3: Spectral Gap of the Coordinate-Wise Gibbs Sampler\n\nThe coordinate-wise Gibbs sampler performs a full sweep by sequentially updating components. For a state $(x_k, y_k)$, the next state $(x_{k+1}, y_{k+1})$ is generated via:\n1. Draw $y' \\sim p(y \\mid x_k) = \\mathcal{N}(\\rho x_k, 1)$.\n2. Draw $x_{k+1} \\sim p(x \\mid y') = \\mathcal{N}(\\rho y', 1)$.\n3. Set $y_{k+1} = y'$.\n\nSo, the update equations are:\n$y_{k+1} = \\rho x_k + \\epsilon_{y,k}$, where $\\epsilon_{y,k} \\sim \\mathcal{N}(0, 1)$.\n$x_{k+1} = \\rho y_{k+1} + \\epsilon_{x,k} = \\rho(\\rho x_k + \\epsilon_{y,k}) + \\epsilon_{x,k} = \\rho^2 x_k + \\rho\\epsilon_{y,k} + \\epsilon_{x,k}$, where $\\epsilon_{x,k} \\sim \\mathcal{N}(0, 1)$ is independent of $\\epsilon_{y,k}$ and past states.\n\nThe spectral gap is $1 - \\lambda_\\star$, where $\\lambda_\\star$ is the second-largest eigenvalue modulus of the Markov transition operator $K$. For a Gaussian system, the slowest-mixing modes (corresponding to $\\lambda_\\star$) are typically the linear functions of the state variables. The convergence rate of the chain is determined by the autocorrelation of the states. Let's compute the autocorrelation of the $x$-component at lag $1$. At stationarity, $\\text{Var}(x_k) = \\text{Var}(x_{k+1}) = \\sigma_x^2 = 1/(1-\\rho^2)$. The covariance is:\n$$\n\\text{Cov}(x_k, x_{k+1}) = \\text{Cov}(x_k, \\rho^2 x_k + \\rho\\epsilon_{y,k} + \\epsilon_{x,k})\n$$\nSince the noise terms are independent of $x_k$, this simplifies to:\n$$\n\\text{Cov}(x_k, x_{k+1}) = \\rho^2 \\text{Cov}(x_k, x_k) = \\rho^2 \\text{Var}(x_k)\n$$\nThe autocorrelation is therefore:\n$$\n\\text{Corr}(x_k, x_{k+1}) = \\frac{\\text{Cov}(x_k, x_{k+1})}{\\sqrt{\\text{Var}(x_k)\\text{Var}(x_{k+1})}} = \\frac{\\rho^2 \\text{Var}(x_k)}{\\text{Var}(x_k)} = \\rho^2\n$$\nThis autocorrelation is the second-largest eigenvalue modulus, $\\lambda_\\star$, of the transition operator for the full sweep.\n$$\n\\lambda_\\star = \\rho^2\n$$\nThe spectral gap for the coordinate-wise sampler is then:\n$$\ng_{xy} = 1 - \\lambda_\\star = 1 - \\rho^2\n$$\n\n### Part 4: Spectral Gap of the Blocked Gibbs Sampler\n\nThe blocked Gibbs sampler in $(u,v)$ generates a new state $(u', v')$ by drawing independently from the marginals $u' \\sim \\mathcal{N}(0, 2/(1-\\rho))$ and $v' \\sim \\mathcal{N}(0, 2/(1+\\rho))$. The resulting state $(x', y')$ is completely independent of the previous state $(x, y)$.\n\nLet $K_{uv}$ be the transition operator for this sampler, acting on a square-integrable function $f \\in L^2(\\pi)$, where $\\pi$ is the stationary distribution. By definition, $(K_{uv}f)(\\mathbf{z}) = E[f(\\mathbf{z}') \\mid \\mathbf{z}]$, where $\\mathbf{z}=(x,y)^T$. Since the next state $\\mathbf{z}'$ is drawn from $\\pi$ independently of $\\mathbf{z}$, this expectation is:\n$$\n(K_{uv}f)(\\mathbf{z}) = \\int f(\\mathbf{z}') \\pi(\\mathbf{z}') d\\mathbf{z}' = E_{\\pi}[f]\n$$\nNow, suppose $f$ is an eigenfunction of $K_{uv}$ with eigenvalue $\\lambda$, so $K_{uv}f = \\lambda f$. This implies:\n$$\n\\lambda f(\\mathbf{z}) = E_{\\pi}[f]\n$$\nThe right side is a constant, let's call it $C$. So, $\\lambda f(\\mathbf{z}) = C$.\nIf $\\lambda=1$ (the largest eigenvalue), then $f(\\mathbf{z}) = C$, which means $f$ is a constant function. This corresponds to the trivial eigenfunction associated with the stationary distribution.\nIf $f$ is any non-constant function (e.g., any function in the subspace $L_0^2(\\pi)$ of mean-zero functions), then $f(\\mathbf{z})$ is not constant. For the equation $\\lambda f(\\mathbf{z}) = C$ to hold for all $\\mathbf{z}$, we must have $\\lambda=0$ (which implies $C=0$, consistent with $f$ having zero mean).\nTherefore, any eigenvalue $\\lambda$ corresponding to a non-constant eigenfunction must be $0$. The second-largest eigenvalue modulus is the largest modulus among all eigenvalues for functions in $L_0^2(\\pi)$, which is:\n$$\n\\lambda_\\star = 0\n$$\nThe spectral gap for the blocked sampler is:\n$$\ng_{uv} = 1 - \\lambda_\\star = 1 - 0 = 1\n$$\nA spectral gap of $1$ signifies that the chain mixes perfectly in a single step, which is expected for an independent sampler.\n\n### Part 5: Implementation\nThe derived formulas for the spectral gaps are:\n- Coordinate-wise sampler: $g_{xy}(\\rho) = 1 - \\rho^2$\n- Blocked sampler: $g_{uv}(\\rho) = 1$\n\nThese formulas will be implemented to compute the required values for the given test suite of $\\rho$ values.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the spectral gaps for a coordinate-wise Gibbs sampler and a\n    blocked Gibbs sampler for a bivariate normal distribution.\n    \"\"\"\n    # Define the test cases for the correlation parameter rho from the problem statement.\n    test_cases = [\n        0.0,\n        0.3,\n        0.9,\n        0.99,\n        -0.9,\n        0.999,\n    ]\n\n    results = []\n    for rho in test_cases:\n        # The target distribution is p(x,y) proportional to\n        # exp{-0.5 * (x^2 + y^2 - 2*rho*x*y)}.\n\n        # 1. Spectral gap for the coordinate-wise Gibbs sampler in (x, y).\n        # The sampler performs a full sweep, e.g., sampling y | x then x | y.\n        # The second-largest eigenvalue modulus of the transition operator\n        # for this sampler on a bivariate normal is rho^2.\n        # The spectral gap is defined as 1 - lambda_star, where lambda_star\n        # is the second-largest eigenvalue modulus.\n        lambda_star_xy = rho**2\n        g_xy = 1.0 - lambda_star_xy\n\n        # 2. Spectral gap for the blocked Gibbs sampler in (u, v).\n        # The reparameterization u = x+y, v = x-y diagonalizes the covariance\n        # matrix, making u and v independent.\n        # A blocked sampler on (u,v) draws from their joint (and in this case,\n        # product of marginals) distribution. This is equivalent to drawing\n        # directly from the target distribution, an independent sampler.\n        # An independent sampler has no memory, so all non-trivial eigenvalues\n        # of its transition operator are 0.\n        # Thus, the second-largest eigenvalue modulus is 0.\n        lambda_star_uv = 0.0\n        g_uv = 1.0 - lambda_star_uv\n\n        results.append([g_xy, g_uv])\n\n    # Final print statement in the exact required format.\n    # e.g., [[g_xy_1, g_uv_1],[g_xy_2, g_uv_2],...]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}