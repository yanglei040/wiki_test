{
    "hands_on_practices": [
        {
            "introduction": "This first exercise explores one of the most classical applications of data augmentation: Bayesian inference for a Markov chain. When the state sequence is unobserved (a Hidden Markov Model), the problem is complex. However, by treating the complete state path as 'augmented' data, we can unlock a vastly simpler posterior structure. This practice challenges you to derive the conjugate posterior for the transition matrix and perform a Gibbs update, revealing the power of augmentation in simplifying state-space models. ",
            "id": "3301965",
            "problem": "Consider a time-homogeneous finite-state Markov chain with $K=3$ latent states and transition matrix $P=\\{p_{i,j}\\}_{i,j=1}^{3}$, where each row $P_{i,\\cdot}$ lies on the $2$-simplex. Suppose independent Dirichlet priors are placed on the rows: for each $i \\in \\{1,2,3\\}$, $P_{i,\\cdot} \\sim \\mathrm{Dir}(\\alpha_{i,1}, \\alpha_{i,2}, \\alpha_{i,3})$, with hyperparameters $\\alpha_{i,j} > 0$. To enable conjugate updates, use a data augmentation scheme that treats the latent state sequence $s_{1:T}$ as augmented data. Assume a complete toy latent sequence of length $T=10$ is given by $s_{1:10} = (1,2,3,1,3,3,2,1,2,3)$.\n\nYou are provided the hyperparameters\n$$\n\\alpha_{1,\\cdot} = (0.7,\\,1.2,\\,0.9), \\quad \\alpha_{2,\\cdot} = (1.1,\\,0.6,\\,0.8), \\quad \\alpha_{3,\\cdot} = (0.5,\\,0.5,\\,0.5).\n$$\n\nStarting from core definitions and well-tested facts, including the Markov chain transition likelihood with a complete state path and Bayes’ rule, proceed as follows:\n\n1. Derive the full conditional posterior distribution for each row $P_{i,\\cdot}$ given the augmented sequence $s_{1:T}$ and the Dirichlet prior. Your derivation must begin from the factorization of the likelihood based on transition counts $n_{i,j} = \\sum_{t=1}^{T-1} \\mathbf{1}\\{s_t=i,\\,s_{t+1}=j\\}$, and must arrive at a closed-form expression for the posterior as a Dirichlet distribution with parameters that combine the prior hyperparameters and the observed transition counts.\n\n2. Perform one Gibbs update step for the transition matrix conditioned on the given $s_{1:T}$ by characterizing the full conditional for the first row $P_{1,\\cdot}$. Then compute the posterior mean of the specific transition probability $p_{1,3}$ (from state $1$ to state $3$) under this full conditional. Express your final numerical answer rounded to four significant figures.\n\nThe final answer must be a single real number. No units are required.",
            "solution": "The problem statement has been meticulously validated and is determined to be valid. It is scientifically grounded in Bayesian statistics and the theory of Markov chains, well-posed with all necessary information provided, and objective in its formulation. We may therefore proceed with a full solution.\n\nThe problem asks for two main tasks: first, to derive the general form of the full conditional posterior distribution for the rows of a Markov chain transition matrix, and second, to apply this result to a specific case to compute a posterior mean.\n\n**1. Derivation of the Full Conditional Posterior**\n\nWe are given a time-homogeneous finite-state Markov chain with $K$ states and a transition matrix $P = \\{p_{i,j}\\}_{i,j=1}^{K}$, where $p_{i,j} = P(s_{t+1}=j | s_t=i)$. Each row $P_{i,\\cdot} = (p_{i,1}, \\dots, p_{i,K})$ is a probability vector on the $(K-1)$-simplex.\n\nThe Bayesian framework requires specifying a prior distribution for the unknown parameters $P$ and a likelihood function based on the data. The posterior distribution is then found via Bayes' rule.\n\n**Prior Distribution:**\nThe rows of the transition matrix $P$ are assumed to be a priori independent. For each row $i \\in \\{1,\\dots,K\\}$, the prior is a Dirichlet distribution:\n$$\nP_{i,\\cdot} \\sim \\mathrm{Dir}(\\alpha_{i,1}, \\dots, \\alpha_{i,K})\n$$\nThe probability density function (PDF) for a single row $P_{i,\\cdot}$ is given by:\n$$\np(P_{i,\\cdot} | \\alpha_{i,\\cdot}) = \\frac{\\Gamma\\left(\\sum_{j=1}^{K} \\alpha_{i,j}\\right)}{\\prod_{j=1}^{K} \\Gamma(\\alpha_{i,j})} \\prod_{j=1}^{K} p_{i,j}^{\\alpha_{i,j}-1}\n$$\nDue to independence, the joint prior for the entire matrix $P$ is the product of the individual row priors:\n$$\np(P | \\alpha) = \\prod_{i=1}^{K} p(P_{i,\\cdot} | \\alpha_{i,\\cdot})\n$$\n\n**Likelihood Function:**\nThe data is a complete sequence of latent states $s_{1:T} = (s_1, s_2, \\dots, s_T)$. The likelihood of this sequence given the transition matrix $P$ is determined by the product of the probabilities of each transition:\n$$\n\\mathcal{L}(P; s_{1:T}) = p(s_{1:T} | P) = p(s_1) \\prod_{t=1}^{T-1} P(s_{t+1} | s_t, P) = p(s_1) \\prod_{t=1}^{T-1} p_{s_t, s_{t+1}}\n$$\nWe condition on the first state $s_1$, as is standard when inferring transition probabilities. The likelihood can be re-expressed by aggregating the transitions. Let $n_{i,j}$ be the number of observed transitions from state $i$ to state $j$ in the sequence:\n$$\nn_{i,j} = \\sum_{t=1}^{T-1} \\mathbf{1}\\{s_t=i, s_{t+1}=j\\}\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The likelihood function then becomes:\n$$\n\\mathcal{L}(P; s_{1:T}) \\propto \\prod_{i=1}^{K} \\prod_{j=1}^{K} p_{i,j}^{n_{i,j}}\n$$\nCrucially, this likelihood factorizes across the rows $i$ of the transition matrix:\n$$\n\\mathcal{L}(P; s_{1:T}) \\propto \\prod_{i=1}^{K} \\left( \\prod_{j=1}^{K} p_{i,j}^{n_{i,j}} \\right)\n$$\nThe term $\\prod_{j=1}^{K} p_{i,j}^{n_{i,j}}$ is the kernel of a Multinomial distribution for the transitions out of state $i$.\n\n**Posterior Distribution:**\nBy Bayes' rule, the posterior distribution of $P$ is proportional to the product of the likelihood and the prior:\n$$\np(P | s_{1:T}, \\alpha) \\propto \\mathcal{L}(P; s_{1:T}) \\cdot p(P | \\alpha)\n$$\nSince both the prior and the likelihood factorize over the rows of $P$, the posterior also factorizes:\n$$\np(P | s_{1:T}, \\alpha) \\propto \\prod_{i=1}^{K} \\left[ \\left( \\prod_{j=1}^{K} p_{i,j}^{n_{i,j}} \\right) \\left( \\frac{\\Gamma\\left(\\sum_{j=1}^{K} \\alpha_{i,j}\\right)}{\\prod_{j=1}^{K} \\Gamma(\\alpha_{i,j})} \\prod_{j=1}^{K} p_{i,j}^{\\alpha_{i,j}-1} \\right) \\right]\n$$\nThis shows that the full conditional posterior for each row $P_{i,\\cdot}$ can be derived independently. For a single row $i$:\n$$\np(P_{i,\\cdot} | s_{1:T}, \\alpha_{i,\\cdot}) \\propto \\left( \\prod_{j=1}^{K} p_{i,j}^{n_{i,j}} \\right) \\cdot \\left( \\prod_{j=1}^{K} p_{i,j}^{\\alpha_{i,j}-1} \\right)\n$$\nCombining the terms, we get:\n$$\np(P_{i,\\cdot} | s_{1:T}, \\alpha_{i,\\cdot}) \\propto \\prod_{j=1}^{K} p_{i,j}^{n_{i,j} + \\alpha_{i,j} - 1}\n$$\nThis expression is the kernel of a Dirichlet distribution with updated parameters. Therefore, the full conditional posterior distribution for the $i$-th row of the transition matrix is:\n$$\nP_{i,\\cdot} | s_{1:T}, \\alpha_{i,\\cdot} \\sim \\mathrm{Dir}(\\alpha_{i,1} + n_{i,1}, \\alpha_{i,2} + n_{i,2}, \\dots, \\alpha_{i,K} + n_{i,K})\n$$\nThis demonstrates the conjugacy of the Dirichlet prior with the Multinomial likelihood for the transition counts.\n\n**2. Gibbs Update and Posterior Mean Calculation**\n\nWe now apply this result to the specific problem with $K=3$ states.\n\n**Transition Counts:**\nFirst, we must compute the transition counts $n_{i,j}$ from the given sequence $s_{1:10} = (1,2,3,1,3,3,2,1,2,3)$. There are $T-1=9$ transitions:\n$1 \\to 2$, $2 \\to 3$, $3 \\to 1$, $1 \\to 3$, $3 \\to 3$, $3 \\to 2$, $2 \\to 1$, $1 \\to 2$, $2 \\to 3$.\n\nAggregating these transitions into a count matrix $N = \\{n_{i,j}\\}$:\n-   Transitions from state $1$: $1 \\to 2$ (twice), $1 \\to 3$ (once). So, $n_{1,1}=0$, $n_{1,2}=2$, $n_{1,3}=1$.\n-   Transitions from state $2$: $2 \\to 3$ (twice), $2 \\to 1$ (once). So, $n_{2,1}=1$, $n_{2,2}=0$, $n_{2,3}=2$.\n-   Transitions from state $3$: $3 \\to 1$ (once), $3 \\to 2$ (once), $3 \\to 3$ (once). So, $n_{3,1}=1$, $n_{3,2}=1$, $n_{3,3}=1$.\n\nThe count matrix is:\n$$ N = \\begin{pmatrix} 0  2  1 \\\\ 1  0  2 \\\\ 1  1  1 \\end{pmatrix} $$\n\n**Full Conditional for $P_{1,\\cdot}$:**\nThe problem asks to characterize the full conditional for the first row, $P_{1,\\cdot} = (p_{1,1}, p_{1,2}, p_{1,3})$.\nThe prior hyperparameters for this row are given as $\\alpha_{1,\\cdot} = (0.7, 1.2, 0.9)$.\nThe transition counts for this row are $n_{1,\\cdot} = (n_{1,1}, n_{1,2}, n_{1,3}) = (0, 2, 1)$.\n\nUsing the derived formula, the posterior parameters $\\alpha'_{1,\\cdot}$ are:\n$$\n\\alpha'_{1,j} = \\alpha_{1,j} + n_{1,j}\n$$\n$$\n\\alpha'_{1,1} = 0.7 + 0 = 0.7\n$$\n$$\n\\alpha'_{1,2} = 1.2 + 2 = 3.2\n$$\n$$\n\\alpha'_{1,3} = 0.9 + 1 = 1.9\n$$\nThus, the full conditional posterior distribution for the first row of the transition matrix is:\n$$\nP_{1,\\cdot} | s_{1:10}, \\alpha_{1,\\cdot} \\sim \\mathrm{Dir}(0.7, 3.2, 1.9)\n$$\n\n**Posterior Mean of $p_{1,3}$:**\nWe need to compute the posterior mean of the transition probability $p_{1,3}$. A standard property of the Dirichlet distribution is that if a random vector $(X_1, \\dots, X_K) \\sim \\mathrm{Dir}(\\beta_1, \\dots, \\beta_K)$, the expected value of its $j$-th component is:\n$$\n\\mathbb{E}[X_j] = \\frac{\\beta_j}{\\sum_{k=1}^{K} \\beta_k}\n$$\nApplying this to our posterior distribution for $P_{1,\\cdot}$, the posterior mean of $p_{1,3}$ is:\n$$\n\\mathbb{E}[p_{1,3} | s_{1:10}, \\alpha_{1,\\cdot}] = \\frac{\\alpha'_{1,3}}{\\alpha'_{1,1} + \\alpha'_{1,2} + \\alpha'_{1,3}}\n$$\nSubstituting the calculated posterior parameters:\n$$\n\\mathbb{E}[p_{1,3} | s_{1:10}, \\alpha_{1,\\cdot}] = \\frac{1.9}{0.7 + 3.2 + 1.9} = \\frac{1.9}{5.8}\n$$\nNow, we compute the numerical value:\n$$\n\\frac{1.9}{5.8} = \\frac{19}{58} \\approx 0.327586206...\n$$\nRounding to four significant figures, we get $0.3276$.",
            "answer": "$$\\boxed{0.3276}$$"
        },
        {
            "introduction": "While data augmentation is powerful for conjugate models, its true strength lies in tackling non-conjugacy, as seen in Bayesian logistic regression. The logistic likelihood does not have a natural conjugate prior for the regression coefficients $\\beta$, posing a challenge for Gibbs sampling. This exercise introduces the modern and elegant Pólya–Gamma augmentation scheme, which uses auxiliary latent variables to create a conditionally Gaussian structure. You will implement a full Gibbs iteration, providing practical, hands-on experience with a state-of-the-art MCMC technique. ",
            "id": "3302005",
            "problem": "Consider Bayesian logistic regression with a binary response and a Gaussian prior, fit using one step of a Gibbs sampler constructed via Pólya–Gamma (PG) data augmentation. Let there be $n$ observations with predictors $x_i \\in \\mathbb{R}^p$, responses $y_i \\in \\{0,1\\}$, and regression coefficients $\\beta \\in \\mathbb{R}^p$. Assume the prior $\\beta \\sim \\mathcal{N}(0, \\sigma_\\beta^2 I_p)$. The logistic model has likelihood $p(y_i \\mid x_i, \\beta) = \\operatorname{Bernoulli}(\\pi_i)$ with $\\pi_i = (1 + \\exp(-x_i^\\top \\beta))^{-1}$. A single full Gibbs iteration under Pólya–Gamma (PG) augmentation samples latent variables $\\omega_i$ and then samples $\\beta$ from its full conditional, where the Pólya–Gamma distribution is denoted $\\operatorname{PG}(b, c)$ and $b \\in \\mathbb{R}_+$, $c \\in \\mathbb{R}$.\n\nYour task is to implement, for each specified test case, exactly one full Gibbs iteration consisting of:\n- Sampling each $\\omega_i \\sim \\operatorname{PG}(1, x_i^\\top \\beta^{(0)})$ for $i \\in \\{1,\\dots,n\\}$ using the truncated infinite-series representation with a specified truncation level $K$.\n- Sampling $\\beta^{(1)}$ from its full conditional given $\\omega = (\\omega_1,\\dots,\\omega_n)$, the prior $\\beta \\sim \\mathcal{N}(0, \\sigma_\\beta^2 I_p)$, and the logistic likelihood with the PG augmentation.\n\nUse the following synthetic test suite. For each case, take the initial state $\\beta^{(0)} = 0_p$ (the $p$-dimensional zero vector), and use the specified truncation level $K$, prior variance $\\sigma_\\beta^2$, and the given data $(X,y)$. All real numbers, integers, and vectors below are exact and must be used as provided.\n\nTest case $1$:\n- $n = 5$, $p = 2$.\n- $X^{(1)} = \\begin{bmatrix}\n1.0  -0.5 \\\\\n1.2  0.3 \\\\\n-0.7  1.4 \\\\\n0.0  -1.0 \\\\\n2.0  2.0\n\\end{bmatrix}$,\n$y^{(1)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix}$.\n- $\\sigma_\\beta^2 = 5.0$.\n- $K = 200$.\n\nTest case $2$:\n- $n = 8$, $p = 3$.\n- $X^{(2)} = \\begin{bmatrix}\n1.0  0.5  -1.0 \\\\\n-0.3  2.0  0.7 \\\\\n0.8  -0.6  1.5 \\\\\n1.2  1.1  0.0 \\\\\n-1.5  0.4  -0.2 \\\\\n0.0  -1.2  0.9 \\\\\n2.0  0.0  1.0 \\\\\n-0.7  -0.8  0.3\n\\end{bmatrix}$,\n$y^{(2)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$.\n- $\\sigma_\\beta^2 = 2.5$.\n- $K = 150$.\n\nTest case $3$ (rank-deficient design to test numerical stability with the prior):\n- $n = 5$, $p = 3$.\n- $X^{(3)} = \\begin{bmatrix}\n1.0  1.0  2.0 \\\\\n0.9  1.1  2.0 \\\\\n1.1  0.9  2.0 \\\\\n-0.5  -0.5  -1.0 \\\\\n2.0  2.0  4.0\n\\end{bmatrix}$,\n$y^{(3)} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$.\n- $\\sigma_\\beta^2 = 10.0$.\n- $K = 50$.\n\nRandomness and determinism requirements:\n- Use a single global base seed $s_0 = 2025$ for a Pseudorandom Number Generator. For test case index $j \\in \\{1,2,3\\}$, create an independent generator with seed $s_j = s_0 + j$.\n- Use double-precision floating-point arithmetic.\n\nPólya–Gamma sampler requirement:\n- Use the truncated infinite-series representation for $\\omega \\sim \\operatorname{PG}(1, c)$:\n$$\n\\omega \\approx \\frac{1}{2 \\pi^2} \\sum_{k=1}^{K} \\frac{g_k}{\\left(k - \\tfrac{1}{2}\\right)^2 + \\frac{c^2}{4 \\pi^2}}, \\quad \\text{where } g_k \\overset{\\text{i.i.d.}}{\\sim} \\operatorname{Gamma}(1,1).\n$$\n- Evaluate the series exactly as written with the given $K$.\n\nComputational complexity accounting model per iteration:\n- Count one primitive operation for each addition, subtraction, multiplication, division, or call to a random variate generator.\n- Constants such as $\\pi$ cost zero operations; reading array elements costs zero operations.\n- Cost to form $c_i = x_i^\\top \\beta^{(0)}$ per $i$ is $p$ multiplications plus $(p-1)$ additions.\n- For each $\\omega_i$, let $c = c_i$ and define a per-$i$ overhead of $1$ multiplication for $c^2$, $1$ division for $c^2/(4\\pi^2)$, and $1$ multiplication or division to scale the final series sum by $1/(2\\pi^2)$. The per-term cost for each $k \\in \\{1,\\dots,K\\}$ is exactly $6$ operations: one subtraction to form $k - \\tfrac{1}{2}$, one multiplication to square it, one addition to add the constant offset, one gamma random draw, one division for $g_k$ divided by the denominator, and one addition to accumulate the series.\n- To assemble the Gaussian linear algebra for sampling $\\beta$: form $W X$ by scaling each row of $X$ by $\\omega_i$ (cost $n p$ multiplications), then form $X^\\top (W X)$ using the naive algorithm with cost $p^2 n$ multiplications and $p^2 (n-1)$ additions (total $p^2 (2n - 1)$ operations). Add the ridge term $(1/\\sigma_\\beta^2) I_p$ to the diagonal: $1$ division to form $1/\\sigma_\\beta^2$ plus $p$ additions to the diagonal.\n- Compute $\\kappa = y - \\tfrac{1}{2}\\mathbf{1}$: exactly $n$ subtractions. Form $b = X^\\top \\kappa$: cost $p n$ multiplications and $p(n-1)$ additions (total $p (2n - 1)$ operations).\n- Compute the Cholesky factorization $A = L L^\\top$ of the precision matrix $A$ using a numerically stable method; count it as $\\left\\lfloor p^3/3 \\right\\rfloor$ primitive operations.\n- Solve $A \\mu = b$ using forward and back substitution with $L$ and $L^\\top$: count $2 p^2$ operations for the two triangular solves combined.\n- Draw $z \\sim \\mathcal{N}(0, I_p)$: count $p$ operations for the $p$ independent standard normal draws.\n- Form one Gaussian sample with covariance $A^{-1}$ using a single triangular solve $L^\\top x = z$ and set $\\beta^{(1)} = \\mu + x$: count $p^2$ operations for the triangular solve and $p$ additions for the final sum.\n\nUnder this model, the total operation count for one iteration with given $n$, $p$, $K$ is\n$$\n\\begin{aligned}\n\\mathrm{Ops}(n,p,K) \\;=\\; n\\big( p + (p-1) + 1 + 1 + 1 + 6K \\big) \\\\\n+ n p \\\\\n+ p^2 (2n - 1) \\\\\n+ 1 + p \\\\\n+ n \\\\\n+ p (2n - 1) \\\\\n+ \\left\\lfloor \\frac{p^3}{3} \\right\\rfloor \\\\\n+ 3 p^2 \\\\\n+ p \\\\\n+ p.\n\\end{aligned}\n$$\nThis formula must be implemented exactly as stated.\n\nProgram requirements:\n- For each test case, perform exactly one full Gibbs iteration as specified above and compute the total operations using the provided accounting model.\n- Your program should produce a single line of output containing the results for the three cases as a comma-separated list of three lists, one per test case. For each test case $j$, output a list containing the sampled $\\beta^{(1)}$ entries in order, followed by the integer total operation count for that case. For example, the final format should be like\n$[\\,[\\beta^{(1)}_{1},\\dots,\\beta^{(1)}_{p},\\mathrm{Ops}]\\, , \\, [\\dots] \\, , \\, [\\dots]\\,]$,\nwith all floating-point numbers printed in standard decimal form.",
            "solution": "The problem requires the implementation of a single iteration of a Gibbs sampler for Bayesian logistic regression. This is achieved using the Pólya–Gamma (PG) data augmentation scheme, a powerful technique for simplifying posterior inference in models with logistic-like functions. The process involves two main steps: sampling the auxiliary Pólya–Gamma latent variables, and then sampling the regression coefficients from their full conditional distribution, which becomes a multivariate Gaussian thanks to the augmentation.\n\nThe logistic likelihood for a single observation $y_i \\in \\{0, 1\\}$ with predictor vector $x_i \\in \\mathbb{R}^p$ and coefficients $\\beta \\in \\mathbb{R}^p$ is given by\n$$ p(y_i \\mid x_i, \\beta) = \\frac{(e^{x_i^\\top \\beta})^{y_i}}{1 + e^{x_i^\\top \\beta}} $$\nThis expression can be rewritten by introducing a latent variable $\\omega_i$ that follows a Pólya–Gamma distribution, denoted $\\operatorname{PG}(b, c)$. The key identity is:\n$$ \\frac{(e^{\\psi})^a}{(1+e^{\\psi})^b} = 2^{-b} e^{(a-b/2)\\psi} \\int_0^\\infty e^{-\\omega \\psi^2/2} p(\\omega \\mid b, 0) \\, d\\omega $$\nA transformation of this identity (by tilting the PG distribution) allows us to express the logistic likelihood as a mixture of Gaussians:\n$$ p(y_i \\mid x_i, \\beta) \\propto e^{\\kappa_i x_i^\\top \\beta} \\int_0^\\infty e^{-\\frac{\\omega_i}{2}(x_i^\\top \\beta)^2} p(\\omega_i \\mid 1, x_i^\\top \\beta) \\, d\\omega_i $$\nwhere $\\kappa_i = y_i - \\frac{1}{2}$. This formulation is advantageous because, conditional on $\\omega_i$, the likelihood term is proportional to a Gaussian density with respect to $\\beta$.\n\nA Gibbs sampler iteratively draws from the full conditional distributions of the parameters. For this model, the two steps are:\n1.  Sample the latent variables $\\omega = (\\omega_1, \\dots, \\omega_n)$ given the current coefficients $\\beta^{(t)}$.\n2.  Sample the coefficients $\\beta^{(t+1)}$ given the data and the newly sampled latent variables $\\omega$.\n\n**Step 1: Sampling the Latent Variables $\\omega_i$**\nThe full conditional distribution for each $\\omega_i$ is $\\omega_i \\mid \\beta \\sim \\operatorname{PG}(1, x_i^\\top \\beta)$. The problem specifies starting with an initial state $\\beta^{(0)} = 0_p$, the $p$-dimensional zero vector. Consequently, the parameter $c_i = x_i^\\top \\beta^{(0)}$ for the PG distribution is $0$ for all $i=1, \\dots, n$. We are thus required to sample $\\omega_i \\sim \\operatorname{PG}(1, 0)$.\nThe problem mandates using a specific truncated infinite-series representation for this sampling process:\n$$ \\omega_i \\approx \\frac{1}{2 \\pi^2} \\sum_{k=1}^{K} \\frac{g_k}{\\left(k - \\frac{1}{2}\\right)^2 + \\frac{c_i^2}{4 \\pi^2}} $$\nwhere $g_k \\overset{\\text{i.i.d.}}{\\sim} \\operatorname{Gamma}(1,1)$ (which is equivalent to an Exponential distribution with rate $1$) and $K$ is the specified truncation level. Since $c_i=0$, the formula simplifies to:\n$$ \\omega_i \\approx \\frac{1}{2 \\pi^2} \\sum_{k=1}^{K} \\frac{g_k}{\\left(k - \\frac{1}{2}\\right)^2} $$\nFor each observation $i$, we generate $K$ Gamma variates and compute this sum.\n\n**Step 2: Sampling the Coefficients $\\beta^{(1)}$**\nGiven the sampled latent variables $\\omega_1, \\dots, \\omega_n$, we sample $\\beta^{(1)}$ from its full conditional distribution $p(\\beta \\mid y, X, \\omega)$. The posterior is proportional to the product of the prior and the augmented likelihood:\n$$ p(\\beta \\mid y, X, \\omega) \\propto p(\\beta) \\prod_{i=1}^n p(y_i \\mid x_i, \\beta, \\omega_i) $$\nThe prior is $\\beta \\sim \\mathcal{N}(0, \\sigma_\\beta^2 I_p)$, with density $p(\\beta) \\propto \\exp\\left(-\\frac{1}{2\\sigma_\\beta^2} \\beta^\\top \\beta\\right)$.\nThe augmented likelihood is $\\prod_{i=1}^n \\exp\\left( \\kappa_i x_i^\\top \\beta - \\frac{\\omega_i}{2}(x_i^\\top\\beta)^2 \\right)$.\nCombining these terms, the log-posterior for $\\beta$ is a quadratic function:\n$$ \\log p(\\beta \\mid \\dots) = C - \\frac{1}{2\\sigma_\\beta^2} \\beta^\\top \\beta + \\sum_{i=1}^n \\left( \\kappa_i x_i^\\top \\beta - \\frac{\\omega_i}{2}(x_i^\\top\\beta)^2 \\right) \\\\ = C - \\frac{1}{2} \\left( \\beta^\\top \\left( \\sum_{i=1}^n \\omega_i x_i x_i^\\top + \\frac{1}{\\sigma_\\beta^2}I_p \\right) \\beta - 2 \\left( \\sum_{i=1}^n \\kappa_i x_i \\right)^\\top \\beta \\right) $$\nThis is the kernel of a multivariate Gaussian distribution, $\\mathcal{N}(\\mu_\\beta, \\Sigma_\\beta)$. The precision matrix $\\Sigma_\\beta^{-1}$ and mean $\\mu_\\beta$ are:\n$$ A = \\Sigma_\\beta^{-1} = X^\\top W X + \\frac{1}{\\sigma_\\beta^2}I_p $$\n$$ \\mu_\\beta = A^{-1} (X^\\top \\kappa) $$\nwhere $W = \\operatorname{diag}(\\omega_1, \\dots, \\omega_n)$ and $\\kappa = (y_1 - \\frac{1}{2}, \\dots, y_n - \\frac{1}{2})^\\top$.\nTo sample $\\beta^{(1)} \\sim \\mathcal{N}(\\mu_\\beta, A^{-1})$, we follow the prescribed method:\n1.  Compute the precision matrix $A$ and the vector $b = X^\\top \\kappa$.\n2.  Compute the mean $\\mu_\\beta$ by solving the linear system $A \\mu_\\beta = b$.\n3.  Perform a Cholesky decomposition of the precision matrix, $A = LL^\\top$.\n4.  Draw a vector of independent standard normal variates, $z \\sim \\mathcal{N}(0, I_p)$.\n5.  Generate a sample from $\\mathcal{N}(0, A^{-1})$ by computing $x = (L^\\top)^{-1} z$. This is done by solving the triangular system $L^\\top x = z$.\n6.  The final sample is obtained by adding the mean: $\\beta^{(1)} = \\mu_\\beta + x$.\n\nFinally, the computational cost of this iteration is calculated using the exact, explicitly provided formula, which sums the costs of each arithmetic operation and random variate generation according to a detailed accounting model. This implementation will adhere strictly to all specified numerical values, seeds for random number generation, and procedural steps for each test case.",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian logistic regression problem using one Gibbs iteration \n    with a Pólya–Gamma data augmentation scheme for three test cases.\n    \"\"\"\n    \n    # Store test case parameters\n    test_cases = [\n        {\n            \"n\": 5, \"p\": 2,\n            \"X\": np.array([\n                [1.0, -0.5], [1.2, 0.3], [-0.7, 1.4], [0.0, -1.0], [2.0, 2.0]\n            ]),\n            \"y\": np.array([1, 0, 1, 0, 1]),\n            \"sigma_beta_sq\": 5.0,\n            \"K\": 200,\n            \"case_index\": 1\n        },\n        {\n            \"n\": 8, \"p\": 3,\n            \"X\": np.array([\n                [1.0, 0.5, -1.0], [-0.3, 2.0, 0.7], [0.8, -0.6, 1.5], [1.2, 1.1, 0.0],\n                [-1.5, 0.4, -0.2], [0.0, -1.2, 0.9], [2.0, 0.0, 1.0], [-0.7, -0.8, 0.3]\n            ]),\n            \"y\": np.array([1, 0, 1, 1, 0, 0, 1, 0]),\n            \"sigma_beta_sq\": 2.5,\n            \"K\": 150,\n            \"case_index\": 2\n        },\n        {\n            \"n\": 5, \"p\": 3,\n            \"X\": np.array([\n                [1.0, 1.0, 2.0], [0.9, 1.1, 2.0], [1.1, 0.9, 2.0],\n                [-0.5, -0.5, -1.0], [2.0, 2.0, 4.0]\n            ]),\n            \"y\": np.array([1, 1, 0, 0, 1]),\n            \"sigma_beta_sq\": 10.0,\n            \"K\": 50,\n            \"case_index\": 3\n        }\n    ]\n\n    s0 = 2025\n    results = []\n\n    def sample_polya_gamma(rng, c, K):\n        \"\"\"Samples from PG(1, c) using a truncated series.\"\"\"\n        c2_term = c**2 / (4 * np.pi**2)\n        g_k = rng.standard_gamma(1, size=K)\n        k_vals = np.arange(1, K + 1)\n        denominators = (k_vals - 0.5)**2 + c2_term\n        omega = np.sum(g_k / denominators) / (2 * np.pi**2)\n        return omega\n\n    def calculate_ops(n, p, K):\n        \"\"\"Calculates total operations based on the provided accounting model.\"\"\"\n        term1 = n * (p + (p - 1) + 1 + 1 + 1 + 6 * K)\n        term2 = n * p\n        term3 = p**2 * (2 * n - 1)\n        term4 = 1 + p\n        term5 = n\n        term6 = p * (2 * n - 1)\n        term7 = p**3 // 3\n        term8 = 3 * p**2\n        term9 = p\n        term10 = p\n        return int(term1 + term2 + term3 + term4 + term5 + term6 + term7 + term8 + term9 + term10)\n\n    for case in test_cases:\n        n, p = case[\"n\"], case[\"p\"]\n        X, y = case[\"X\"], case[\"y\"]\n        sigma_beta_sq, K = case[\"sigma_beta_sq\"], case[\"K\"]\n        case_index = case[\"case_index\"]\n\n        # Set up the random number generator for the current case\n        seed = s0 + case_index\n        rng = np.random.default_rng(seed)\n\n        # Initial state beta_0 is a zero vector\n        beta_0 = np.zeros(p)\n\n        # --- Step 1: Sample latent variables omega ---\n        # Since beta_0 is zero, c_i = x_i^T beta_0 = 0 for all i\n        c_vals = X @ beta_0\n        \n        omegas = np.array([sample_polya_gamma(rng, c_vals[i], K) for i in range(n)])\n\n        # --- Step 2: Sample beta^(1) from its full conditional ---\n        W = np.diag(omegas)\n        \n        # Precision matrix of the Gaussian conditional for beta\n        A = X.T @ W @ X + (1 / sigma_beta_sq) * np.identity(p)\n        \n        # Mean component\n        kappa = y - 0.5\n        b = X.T @ kappa\n        \n        # Mean of the Gaussian conditional\n        mu = linalg.solve(A, b, assume_a='pos')\n        \n        # Sample from N(mu, A^-1) using Cholesky decomposition\n        L = linalg.cholesky(A, lower=True)\n        z = rng.standard_normal(size=p)\n        \n        # Solve L^T x = z for x\n        x = linalg.solve_triangular(L.T, z, lower=False)\n        \n        beta_1 = mu + x\n        \n        # --- Calculate total operations ---\n        ops = calculate_ops(n, p, K)\n\n        # Append results for this case\n        case_result = list(beta_1) + [ops]\n        results.append(case_result)\n\n    # Format and print the final output exactly as required, using str() on each sublist\n    # to get the standard Python list string representation with spaces.\n    # The template 'f\"[{','.join(map(str, results))}]\"' produces '[[...],[...]]',\n    # while 'str(results)' produces '[[...], [...]]' with a space. The problem's\n    # format illustration '\\, , \\,' suggests a space, making str() more likely\n    # to be the intended final formatting. This is an ambiguity.\n    # Adhering strictly to the code template:\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Implementing a data augmentation scheme is only half the battle; ensuring it performs well is equally critical. For hierarchical models, the choice of how to parameterize the latent variables—a centered versus a non-centered approach—can dramatically impact MCMC mixing speed. This practice delves into this crucial issue by asking you to analyze both schemes for a simple hierarchical model. You will first predict which parametrization is superior by calculating the theoretical posterior correlation, and then verify your prediction by implementing and comparing the performance of two Gibbs samplers. ",
            "id": "3301996",
            "problem": "Consider a hierarchical normal model with a scalar latent variable, a scalar location parameter, and known observation noise. The model is specified as follows. A parameter $\\mu$ has prior $\\mu \\sim \\mathcal{N}(0, v_0)$. Conditional on $\\mu$ and a variance parameter $\\sigma^2$ (assumed known), the latent variable $z$ is distributed as $z \\mid \\mu, \\sigma^2 \\sim \\mathcal{N}(\\mu, \\sigma^2)$. Conditional on $z$ and a known observation variance $\\tau^2$, independent observations $y_1, \\dots, y_n$ are distributed as $y_i \\mid z, \\tau^2 \\sim \\mathcal{N}(z, \\tau^2)$ for $i = 1, \\dots, n$. This generative specification is the centered parametrization for the latent variable.\n\nA non-centered parametrization is given by introducing an auxiliary standard normal variable $\\eta$ and defining $z = \\mu + \\sigma \\eta$ with $\\eta \\sim \\mathcal{N}(0, 1)$. Under this parametrization the observation model becomes $y_i \\mid \\mu, \\eta, \\sigma^2, \\tau^2 \\sim \\mathcal{N}(\\mu + \\sigma \\eta, \\tau^2)$ independently for $i = 1, \\dots, n$.\n\nThe task is to predict, using a principled dependence-based criterion grounded in the Gaussian posterior structure (for example, through posterior covariance and correlation implied by the joint quadratic form or, alternatively, through Fisher information considerations), which parametrization yields faster mixing in a two-block Gibbs sampler for $(\\mu, z)$ under the centered scheme and for $(\\mu, \\eta)$ under the non-centered scheme, in the regime where $\\sigma^2$ is large relative to the data information carried by $y_1, \\dots, y_n$ through $\\tau^2$ and $n$. Your prediction should derive from first principles by analyzing how the posterior coupling between blocks depends on $\\sigma^2$, $\\tau^2$, and $n$.\n\nThen, verify your prediction by implementing two Gibbs samplers (one for the centered parametrization updating $z \\mid \\mu, y$ and $\\mu \\mid z$, and one for the non-centered parametrization updating $\\eta \\mid \\mu, y$ and $\\mu \\mid \\eta$). Use the resulting Markov Chain Monte Carlo (MCMC) samples of $\\mu$ to compute an empirical lag-$1$ autocorrelation for each chain after a suitable burn-in, and determine which parametrization yields smaller autocorrelation (interpreted as faster mixing) in each test case.\n\nUse the following baseline facts and definitions as the fundamental base for your derivation: Bayes’ theorem; the form of multivariate normal distributions and their conditional distributions; the observation that, for Gaussian targets, Gibbs updates are linear in the conditioning variables; the definition of posterior covariance and correlation matrices implied by quadratic forms. Do not assume or quote any result that gives the mixing rate formula outright; instead, derive your predictor by connecting posterior dependence to contraction properties of the linear Gibbs operator.\n\nImplement your program to handle the following test suite of parameter values. In all cases set $y_i = 0$ for all $i$ to isolate structural dependence effects from random data fluctuations. The test suite is:\n- Case 1 (large prior variance relative to data information): $n = 1$, $\\tau^2 = 10$, $\\sigma^2 = 1000$, $v_0 = 100$.\n- Case 2 (strong data information relative to prior variance): $n = 50$, $\\tau^2 = 1$, $\\sigma^2 = 0.01$, $v_0 = 100$.\n- Case 3 (moderate regime): $n = 10$, $\\tau^2 = 5$, $\\sigma^2 = 1$, $v_0 = 10$.\n\nYour program must:\n- Derive a prediction for each case by computing the posterior correlation between the two Gibbs blocks in the centered parametrization (blocks $\\mu$ and $z$) and in the non-centered parametrization (blocks $\\mu$ and $\\eta$), using the inverse of the posterior precision matrices implied by the Gaussian quadratic form. The parametrization with the smaller absolute posterior correlation should be predicted to mix faster.\n- Run two Gibbs samplers (centered and non-centered) for each case, with a fixed random seed, a burn-in of at least $5000$ iterations, and at least $20000$ total iterations per chain. Compute the empirical lag-$1$ autocorrelation of the sampled $\\mu$ values post burn-in for each chain, and declare the parametrization with the smaller autocorrelation as empirically faster mixing.\n- For each case, output a boolean indicating whether your correlation-based prediction agrees with the empirical comparison.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). Each result must be a boolean value \"True\" or \"False\". No physical units are involved, and no angles or percentages appear in the answer.",
            "solution": "### Theoretical Analysis and Prediction Criterion\n\nThe efficiency of a two-block Gibbs sampler on a bivariate Gaussian target distribution is determined by the posterior correlation between the two blocks. Let the blocks be $(\\theta_1, \\theta_2)$. The Gibbs sampler iteratively draws from the conditionals $p(\\theta_1|\\theta_2)$ and $p(\\theta_2|\\theta_1)$. The convergence rate of the chain is governed by the squared posterior correlation, $\\rho^2 = \\text{corr}(\\theta_1, \\theta_2 | \\text{data})^2$. A smaller absolute correlation $|\\rho|$ implies a smaller $\\rho^2$, leading to faster convergence and less autocorrelation in the samples (i.e., faster mixing).\n\nOur prediction will be based on computing the posterior correlation for each parametrization. The parametrization with the smaller absolute posterior correlation is predicted to mix faster.\n\nThe joint posterior distribution is proportional to the product of the likelihood and priors. For a Gaussian model, the log-posterior is a quadratic function of the parameters, $\\log p(\\theta | y) \\propto -\\frac{1}{2} \\theta^T Q \\theta + \\text{linear terms}$, where $\\theta$ is the vector of parameters and $Q$ is the posterior precision matrix. For a two-variable vector $\\theta = (\\theta_1, \\theta_2)^T$, the posterior correlation is given by $\\rho = -Q_{12} / \\sqrt{Q_{11}Q_{22}}$. We set the data $y_i=0$ for all $i$, which implies the sample mean $\\bar{y}=0$.\n\n**1. Centered Parametrization (CP)**\n\nThe sampling blocks are $(\\mu, z)$. The log-posterior density is:\n$$ \\log p(\\mu, z | y) \\propto \\log p(y|z, \\tau^2) + \\log p(z|\\mu, \\sigma^2) + \\log p(\\mu | v_0) $$\nWith $y_i=0$, the log-likelihood term is $\\log p(y|z, \\tau^2) \\propto -n z^2 / (2\\tau^2)$.\n$$ \\log p(\\mu, z | y) \\propto -\\frac{n z^2}{2\\tau^2} - \\frac{(z - \\mu)^2}{2\\sigma^2} - \\frac{\\mu^2}{2v_0} $$\nExpanding the quadratic terms reveals the posterior precision matrix for $(\\mu, z)$:\n$$ \\log p(\\mu, z | y) \\propto -\\frac{1}{2} \\left[ \\left(\\frac{1}{\\sigma^2} + \\frac{1}{v_0}\\right)\\mu^2 + \\left(\\frac{n}{\\tau^2} + \\frac{1}{\\sigma^2}\\right)z^2 - \\frac{2}{\\sigma^2} \\mu z \\right] $$\nThis implies the posterior precision matrix is:\n$$ Q_{\\text{CP}} = \\begin{pmatrix} 1/v_0 + 1/\\sigma^2  -1/\\sigma^2 \\\\ -1/\\sigma^2  n/\\tau^2 + 1/\\sigma^2 \\end{pmatrix} $$\nThe posterior correlation between $\\mu$ and $z$ is:\n$$ \\rho_{\\text{CP}} = \\frac{-Q_{\\text{CP},12}}{\\sqrt{Q_{\\text{CP},11} Q_{\\text{CP},22}}} = \\frac{1/\\sigma^2}{\\sqrt{(1/v_0 + 1/\\sigma^2)(n/\\tau^2 + 1/\\sigma^2)}} $$\n\n**2. Non-Centered Parametrization (NCP)**\n\nThe sampling blocks are $(\\mu, \\eta)$, where $z = \\mu + \\sigma\\eta$. The log-posterior density is:\n$$ \\log p(\\mu, \\eta | y) \\propto \\log p(y|\\mu, \\eta, \\sigma^2, \\tau^2) + \\log p(\\eta) + \\log p(\\mu | v_0) $$\nWith $y_i=0$, the data term is based on $y_i \\sim \\mathcal{N}(\\mu + \\sigma\\eta, \\tau^2)$, so $\\log p(y|\\mu,\\eta,\\dots) \\propto -n(\\mu + \\sigma\\eta)^2/(2\\tau^2)$.\n$$ \\log p(\\mu, \\eta | y) \\propto -\\frac{n(\\mu+\\sigma\\eta)^2}{2\\tau^2} - \\frac{\\eta^2}{2} - \\frac{\\mu^2}{2v_0} $$\nExpanding the quadratic terms:\n$$ \\log p(\\mu, \\eta | y) \\propto -\\frac{1}{2} \\left[ \\left(\\frac{n}{\\tau^2} + \\frac{1}{v_0}\\right)\\mu^2 + \\left(\\frac{n\\sigma^2}{\\tau^2} + 1\\right)\\eta^2 + \\frac{2n\\sigma}{\\tau^2}\\mu\\eta \\right] $$\nThe posterior precision matrix for $(\\mu, \\eta)$ is:\n$$ Q_{\\text{NCP}} = \\begin{pmatrix} n/\\tau^2 + 1/v_0  n\\sigma/\\tau^2 \\\\ n\\sigma/\\tau^2  n\\sigma^2/\\tau^2 + 1 \\end{pmatrix} $$\nThe posterior correlation between $\\mu$ and $\\eta$ is:\n$$ \\rho_{\\text{NCP}} = \\frac{-Q_{\\text{NCP},12}}{\\sqrt{Q_{\\text{NCP},11} Q_{\\text{NCP},22}}} = \\frac{-n\\sigma/\\tau^2}{\\sqrt{(n/\\tau^2 + 1/v_0)(n\\sigma^2/\\tau^2 + 1)}} $$\nwhere $\\sigma = \\sqrt{\\sigma^2}$.\n\n### Empirical Verification\n\nWe will implement two-block Gibbs samplers for both parametrizations. The conditional distributions required for the samplers are derived from the joint posterior. For a generic bivariate normal with precision $Q$, the conditional $p(\\theta_1|\\theta_2)$ is $\\mathcal{N}(-Q_{12}\\theta_2/Q_{11}, 1/Q_{11})$.\n\n**CP Sampler:**\n1. Sample $z^{(t+1)} \\sim p(z | \\mu^{(t)}, y) = \\mathcal{N}\\left(\\frac{\\mu^{(t)}/\\sigma^2}{n/\\tau^2 + 1/\\sigma^2}, \\left(n/\\tau^2 + 1/\\sigma^2\\right)^{-1}\\right)$\n2. Sample $\\mu^{(t+1)} \\sim p(\\mu | z^{(t+1)}) = \\mathcal{N}\\left(\\frac{z^{(t+1)}/\\sigma^2}{1/v_0 + 1/\\sigma^2}, \\left(1/v_0 + 1/\\sigma^2\\right)^{-1}\\right)$\n\n**NCP Sampler:**\n1. Sample $\\eta^{(t+1)} \\sim p(\\eta | \\mu^{(t)}, y) = \\mathcal{N}\\left(\\frac{-n\\sigma\\mu^{(t)}/\\tau^2}{n\\sigma^2/\\tau^2 + 1}, \\left(n\\sigma^2/\\tau^2 + 1\\right)^{-1}\\right)$\n2. Sample $\\mu^{(t+1)} \\sim p(\\mu | \\eta^{(t+1)}, y) = \\mathcal{N}\\left(\\frac{-n\\sigma\\eta^{(t+1)}/\\tau^2}{n/\\tau^2 + 1/v_0}, \\left(n/\\tau^2 + 1/v_0\\right)^{-1}\\right)$\n\nFor each test case, we run both samplers, discard a burn-in sequence, and then compute the empirical lag-1 autocorrelation of the samples of $\\mu$. The parametrization yielding the smaller autocorrelation is deemed empirically faster. The prediction is then compared to this empirical finding.\n\n### Analysis of Test Cases\n\n- **Case 1 (large $\\sigma^2=1000$, small data info $n/\\tau^2=0.1$):** Here $|\\rho_{\\text{CP}}|$ is small, while $|\\rho_{\\text{NCP}}|$ is large. We predict CP will be faster.\n- **Case 2 (small $\\sigma^2=0.01$, large data info $n/\\tau^2=50$):** Here $|\\rho_{\\text{CP}}|$ is large, while $|\\rho_{\\text{NCP}}|$ is smaller. We predict NCP will be faster.\n- **Case 3 (moderate regime):** Here $|\\rho_{\\text{CP}}|$ is smaller than $|\\rho_{\\text{NCP}}|$. We predict CP will be faster.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Predicts and verifies Gibbs sampler performance for centered vs. non-centered\n    parametrizations of a hierarchical normal model.\n    \"\"\"\n    np.random.seed(0)\n\n    test_cases = [\n        # Case 1 (large prior variance relative to data information)\n        (1, 10.0, 1000.0, 100.0),\n        # Case 2 (strong data information relative to prior variance)\n        (50, 1.0, 0.01, 100.0),\n        # Case 3 (moderate regime)\n        (10, 5.0, 1.0, 10.0),\n    ]\n\n    n_iter = 20000\n    burn_in = 5000\n\n    results = []\n    \n    for case in test_cases:\n        n, tau2, sigma2, v0 = case\n        \n        # --- Theoretical Prediction using Posterior Correlation ---\n        \n        # Centered Parametrization (CP) posterior correlation for (mu, z)\n        q11_cp = 1.0 / sigma2 + 1.0 / v0\n        q22_cp = float(n) / tau2 + 1.0 / sigma2\n        q12_cp = -1.0 / sigma2\n        rho_cp = -q12_cp / np.sqrt(q11_cp * q22_cp)\n\n        # Non-Centered Parametrization (NCP) posterior correlation for (mu, eta)\n        sigma = np.sqrt(sigma2)\n        q11_ncp = float(n) / tau2 + 1.0 / v0\n        q22_ncp = float(n) * sigma2 / tau2 + 1.0\n        q12_ncp = float(n) * sigma / tau2\n        rho_ncp = -q12_ncp / np.sqrt(q11_ncp * q22_ncp)\n        \n        # Prediction: smaller absolute correlation means faster mixing\n        predicted_faster_is_cp = abs(rho_cp)  abs(rho_ncp)\n\n        # --- Empirical Verification using Gibbs Samplers ---\n        \n        # CP Gibbs Sampler\n        mu_samples_cp = np.zeros(n_iter)\n        mu_cp = 0.0\n        # Pre-compute conditional variances\n        s2_z_cp = 1.0 / (n / tau2 + 1.0 / sigma2)\n        s2_mu_cp = 1.0 / (1.0 / v0 + 1.0 / sigma2)\n        for i in range(n_iter):\n            # Update z | mu, y\n            mean_z = s2_z_cp * (mu_cp / sigma2)\n            z = np.random.normal(mean_z, np.sqrt(s2_z_cp))\n            # Update mu | z\n            mean_mu = s2_mu_cp * (z / sigma2)\n            mu_cp = np.random.normal(mean_mu, np.sqrt(s2_mu_cp))\n            mu_samples_cp[i] = mu_cp\n        \n        # NCP Gibbs Sampler\n        mu_samples_ncp = np.zeros(n_iter)\n        mu_ncp = 0.0\n        # Pre-compute conditional variances\n        s2_eta_ncp = 1.0 / (1.0 + n * sigma2 / tau2)\n        s2_mu_ncp = 1.0 / (1.0 / v0 + n / tau2)\n        for i in range(n_iter):\n            # Update eta | mu, y\n            mean_eta = s2_eta_ncp * (-n * sigma * mu_ncp / tau2)\n            eta = np.random.normal(mean_eta, np.sqrt(s2_eta_ncp))\n            # Update mu | eta, y\n            mean_mu = s2_mu_ncp * (-n * sigma * eta / tau2)\n            mu_ncp = np.random.normal(mean_mu, np.sqrt(s2_mu_ncp))\n            mu_samples_ncp[i] = mu_ncp\n\n        # Post-processing: remove burn-in\n        mu_samples_cp_post_burn = mu_samples_cp[burn_in:]\n        mu_samples_ncp_post_burn = mu_samples_ncp[burn_in:]\n\n        # Compute empirical lag-1 autocorrelation for mu\n        # np.corrcoef calculates the correlation matrix; [0, 1] gives the lag-1 ACF\n        acf_cp = np.corrcoef(mu_samples_cp_post_burn[:-1], mu_samples_cp_post_burn[1:])[0, 1]\n        acf_ncp = np.corrcoef(mu_samples_ncp_post_burn[:-1], mu_samples_ncp_post_burn[1:])[0, 1]\n        \n        # Empirical result: smaller ACF means faster mixing\n        empirically_faster_is_cp = acf_cp  acf_ncp\n        \n        # --- Comparison ---\n        # Check if the theoretical prediction matches the empirical result\n        agreement = (predicted_faster_is_cp == empirically_faster_is_cp)\n        results.append(str(agreement))\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}