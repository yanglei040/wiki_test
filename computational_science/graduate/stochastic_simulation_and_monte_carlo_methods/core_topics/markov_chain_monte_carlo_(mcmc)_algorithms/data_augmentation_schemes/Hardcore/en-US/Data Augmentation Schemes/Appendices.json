{
    "hands_on_practices": [
        {
            "introduction": "Inferring the parameters of models with latent structures, such as the transition matrix of a Markov chain, can be challenging due to the complex dependencies in the likelihood. Data augmentation provides a powerful solution by treating the unobserved latent state sequence as \"missing data.\" By augmenting the parameter space with these states, we can simplify the posterior distribution, often revealing a conjugate structure that allows for direct and efficient Gibbs sampling. This practice walks through the foundational application of this idea, where you will derive the conjugate posterior for a Markov chain's transition probabilities and perform a concrete Gibbs update .",
            "id": "3301965",
            "problem": "Consider a time-homogeneous finite-state Markov chain with $K=3$ latent states and transition matrix $P=\\{p_{i,j}\\}_{i,j=1}^{3}$, where each row $P_{i,\\cdot}$ lies on the $2$-simplex. Suppose independent Dirichlet priors are placed on the rows: for each $i \\in \\{1,2,3\\}$, $P_{i,\\cdot} \\sim \\mathrm{Dir}(\\alpha_{i,1}, \\alpha_{i,2}, \\alpha_{i,3})$, with hyperparameters $\\alpha_{i,j}  0$. To enable conjugate updates, use a data augmentation scheme that treats the latent state sequence $s_{1:T}$ as augmented data. Assume a complete toy latent sequence of length $T=10$ is given by $s_{1:10} = (1,2,3,1,3,3,2,1,2,3)$.\n\nYou are provided the hyperparameters\n$$\n\\alpha_{1,\\cdot} = (0.7,\\,1.2,\\,0.9), \\quad \\alpha_{2,\\cdot} = (1.1,\\,0.6,\\,0.8), \\quad \\alpha_{3,\\cdot} = (0.5,\\,0.5,\\,0.5).\n$$\n\nStarting from core definitions and well-tested facts, including the Markov chain transition likelihood with a complete state path and Bayes’ rule, proceed as follows:\n\n1. Derive the full conditional posterior distribution for each row $P_{i,\\cdot}$ given the augmented sequence $s_{1:T}$ and the Dirichlet prior. Your derivation must begin from the factorization of the likelihood based on transition counts $n_{i,j} = \\sum_{t=1}^{T-1} \\mathbf{1}\\{s_t=i,\\,s_{t+1}=j\\}$, and must arrive at a closed-form expression for the posterior as a Dirichlet distribution with parameters that combine the prior hyperparameters and the observed transition counts.\n\n2. Perform one Gibbs update step for the transition matrix conditioned on the given $s_{1:T}$ by characterizing the full conditional for the first row $P_{1,\\cdot}$. Then compute the posterior mean of the specific transition probability $p_{1,3}$ (from state $1$ to state $3$) under this full conditional. Express your final numerical answer rounded to four significant figures.\n\nThe final answer must be a single real number. No units are required.",
            "solution": "The problem statement has been meticulously validated and is determined to be valid. It is scientifically grounded in Bayesian statistics and the theory of Markov chains, well-posed with all necessary information provided, and objective in its formulation. We may therefore proceed with a full solution.\n\nThe problem asks for two main tasks: first, to derive the general form of the full conditional posterior distribution for the rows of a Markov chain transition matrix, and second, to apply this result to a specific case to compute a posterior mean.\n\n**1. Derivation of the Full Conditional Posterior**\n\nWe are given a time-homogeneous finite-state Markov chain with $K$ states and a transition matrix $P = \\{p_{i,j}\\}_{i,j=1}^{K}$, where $p_{i,j} = P(s_{t+1}=j | s_t=i)$. Each row $P_{i,\\cdot} = (p_{i,1}, \\dots, p_{i,K})$ is a probability vector on the $(K-1)$-simplex.\n\nThe Bayesian framework requires specifying a prior distribution for the unknown parameters $P$ and a likelihood function based on the data. The posterior distribution is then found via Bayes' rule.\n\n**Prior Distribution:**\nThe rows of the transition matrix $P$ are assumed to be a priori independent. For each row $i \\in \\{1,\\dots,K\\}$, the prior is a Dirichlet distribution:\n$$\nP_{i,\\cdot} \\sim \\mathrm{Dir}(\\alpha_{i,1}, \\dots, \\alpha_{i,K})\n$$\nThe probability density function (PDF) for a single row $P_{i,\\cdot}$ is given by:\n$$\np(P_{i,\\cdot} | \\alpha_{i,\\cdot}) = \\frac{\\Gamma\\left(\\sum_{j=1}^{K} \\alpha_{i,j}\\right)}{\\prod_{j=1}^{K} \\Gamma(\\alpha_{i,j})} \\prod_{j=1}^{K} p_{i,j}^{\\alpha_{i,j}-1}\n$$\nDue to independence, the joint prior for the entire matrix $P$ is the product of the individual row priors:\n$$\np(P | \\alpha) = \\prod_{i=1}^{K} p(P_{i,\\cdot} | \\alpha_{i,\\cdot})\n$$\n\n**Likelihood Function:**\nThe data is a complete sequence of latent states $s_{1:T} = (s_1, s_2, \\dots, s_T)$. The likelihood of this sequence given the transition matrix $P$ is determined by the product of the probabilities of each transition:\n$$\n\\mathcal{L}(P; s_{1:T}) = p(s_{1:T} | P) = p(s_1) \\prod_{t=1}^{T-1} P(s_{t+1} | s_t, P) = p(s_1) \\prod_{t=1}^{T-1} p_{s_t, s_{t+1}}\n$$\nWe condition on the first state $s_1$, as is standard when inferring transition probabilities. The likelihood can be re-expressed by aggregating the transitions. Let $n_{i,j}$ be the number of observed transitions from state $i$ to state $j$ in the sequence:\n$$\nn_{i,j} = \\sum_{t=1}^{T-1} \\mathbf{1}\\{s_t=i, s_{t+1}=j\\}\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. The likelihood function then becomes:\n$$\n\\mathcal{L}(P; s_{1:T}) \\propto \\prod_{i=1}^{K} \\prod_{j=1}^{K} p_{i,j}^{n_{i,j}}\n$$\nCrucially, this likelihood factorizes across the rows $i$ of the transition matrix:\n$$\n\\mathcal{L}(P; s_{1:T}) \\propto \\prod_{i=1}^{K} \\left( \\prod_{j=1}^{K} p_{i,j}^{n_{i,j}} \\right)\n$$\nThe term $\\prod_{j=1}^{K} p_{i,j}^{n_{i,j}}$ is the kernel of a Multinomial distribution for the transitions out of state $i$.\n\n**Posterior Distribution:**\nBy Bayes' rule, the posterior distribution of $P$ is proportional to the product of the likelihood and the prior:\n$$\np(P | s_{1:T}, \\alpha) \\propto \\mathcal{L}(P; s_{1:T}) \\cdot p(P | \\alpha)\n$$\nSince both the prior and the likelihood factorize over the rows of $P$, the posterior also factorizes:\n$$\np(P | s_{1:T}, \\alpha) \\propto \\prod_{i=1}^{K} \\left[ \\left( \\prod_{j=1}^{K} p_{i,j}^{n_{i,j}} \\right) \\left( \\frac{\\Gamma\\left(\\sum_{j=1}^{K} \\alpha_{i,j}\\right)}{\\prod_{j=1}^{K} \\Gamma(\\alpha_{i,j})} \\prod_{j=1}^{K} p_{i,j}^{\\alpha_{i,j}-1} \\right) \\right]\n$$\nThis shows that the full conditional posterior for each row $P_{i,\\cdot}$ can be derived independently. For a single row $i$:\n$$\np(P_{i,\\cdot} | s_{1:T}, \\alpha_{i,\\cdot}) \\propto \\left( \\prod_{j=1}^{K} p_{i,j}^{n_{i,j}} \\right) \\cdot \\left( \\prod_{j=1}^{K} p_{i,j}^{\\alpha_{i,j}-1} \\right)\n$$\nCombining the terms, we get:\n$$\np(P_{i,\\cdot} | s_{1:T}, \\alpha_{i,\\cdot}) \\propto \\prod_{j=1}^{K} p_{i,j}^{n_{i,j} + \\alpha_{i,j} - 1}\n$$\nThis expression is the kernel of a Dirichlet distribution with updated parameters. Therefore, the full conditional posterior distribution for the $i$-th row of the transition matrix is:\n$$\nP_{i,\\cdot} | s_{1:T}, \\alpha_{i,\\cdot} \\sim \\mathrm{Dir}(\\alpha_{i,1} + n_{i,1}, \\alpha_{i,2} + n_{i,2}, \\dots, \\alpha_{i,K} + n_{i,K})\n$$\nThis demonstrates the conjugacy of the Dirichlet prior with the Multinomial likelihood for the transition counts.\n\n**2. Gibbs Update and Posterior Mean Calculation**\n\nWe now apply this result to the specific problem with $K=3$ states.\n\n**Transition Counts:**\nFirst, we must compute the transition counts $n_{i,j}$ from the given sequence $s_{1:10} = (1,2,3,1,3,3,2,1,2,3)$. There are $T-1=9$ transitions:\n$1 \\to 2$, $2 \\to 3$, $3 \\to 1$, $1 \\to 3$, $3 \\to 3$, $3 \\to 2$, $2 \\to 1$, $1 \\to 2$, $2 \\to 3$.\n\nAggregating these transitions into a count matrix $N = \\{n_{i,j}\\}$:\n-   Transitions from state $1$: $1 \\to 2$ (twice), $1 \\to 3$ (once). So, $n_{1,1}=0$, $n_{1,2}=2$, $n_{1,3}=1$.\n-   Transitions from state $2$: $2 \\to 3$ (twice), $2 \\to 1$ (once). So, $n_{2,1}=1$, $n_{2,2}=0$, $n_{2,3}=2$.\n-   Transitions from state $3$: $3 \\to 1$ (once), $3 \\to 2$ (once), $3 \\to 3$ (once). So, $n_{3,1}=1$, $n_{3,2}=1$, $n_{3,3}=1$.\n\nThe count matrix is:\n$$ N = \\begin{pmatrix} 0  2  1 \\\\ 1  0  2 \\\\ 1  1  1 \\end{pmatrix} $$\n\n**Full Conditional for $P_{1,\\cdot}$:**\nThe problem asks to characterize the full conditional for the first row, $P_{1,\\cdot} = (p_{1,1}, p_{1,2}, p_{1,3})$.\nThe prior hyperparameters for this row are given as $\\alpha_{1,\\cdot} = (0.7, 1.2, 0.9)$.\nThe transition counts for this row are $n_{1,\\cdot} = (n_{1,1}, n_{1,2}, n_{1,3}) = (0, 2, 1)$.\n\nUsing the derived formula, the posterior parameters $\\alpha'_{1,\\cdot}$ are:\n$$\n\\alpha'_{1,j} = \\alpha_{1,j} + n_{1,j}\n$$\n$$\n\\alpha'_{1,1} = 0.7 + 0 = 0.7\n$$\n$$\n\\alpha'_{1,2} = 1.2 + 2 = 3.2\n$$\n$$\n\\alpha'_{1,3} = 0.9 + 1 = 1.9\n$$\nThus, the full conditional posterior distribution for the first row of the transition matrix is:\n$$\nP_{1,\\cdot} | s_{1:10}, \\alpha_{1,\\cdot} \\sim \\mathrm{Dir}(0.7, 3.2, 1.9)\n$$\n\n**Posterior Mean of $p_{1,3}$:**\nWe need to compute the posterior mean of the transition probability $p_{1,3}$. A standard property of the Dirichlet distribution is that if a random vector $(X_1, \\dots, X_K) \\sim \\mathrm{Dir}(\\beta_1, \\dots, \\beta_K)$, the expected value of its $j$-th component is:\n$$\n\\mathbb{E}[X_j] = \\frac{\\beta_j}{\\sum_{k=1}^{K} \\beta_k}\n$$\nApplying this to our posterior distribution for $P_{1,\\cdot}$, the posterior mean of $p_{1,3}$ is:\n$$\n\\mathbb{E}[p_{1,3} | s_{1:10}, \\alpha_{1,\\cdot}] = \\frac{\\alpha'_{1,3}}{\\alpha'_{1,1} + \\alpha'_{1,2} + \\alpha'_{1,3}}\n$$\nSubstituting the calculated posterior parameters:\n$$\n\\mathbb{E}[p_{1,3} | s_{1:10}, \\alpha_{1,\\cdot}] = \\frac{1.9}{0.7 + 3.2 + 1.9} = \\frac{1.9}{5.8}\n$$\nNow, we compute the numerical value:\n$$\n\\frac{1.9}{5.8} = \\frac{19}{58} \\approx 0.327586206...\n$$\nRounding to four significant figures, we get $0.3276$.",
            "answer": "$$\\boxed{0.3276}$$"
        },
        {
            "introduction": "While data augmentation can make posterior inference tractable, the specific way a model is augmented can have profound consequences for the efficiency of an MCMC sampler. This exercise delves into the critical distinction between centered and non-centered parameterizations, two different augmentation strategies for the same hierarchical model. You will analyze how these choices alter the posterior dependence structure between parameters, which can lead to challenging geometries, such as the infamous \"funnel,\" that severely hinder sampler mixing .",
            "id": "3301998",
            "problem": "Consider the following heavy-tailed regression model framed as a Gaussian scale mixture representation of the Student’s t-distribution. For observations indexed by $i \\in \\{1,\\dots,n\\}$, covariates $x_i \\in \\mathbb{R}^p$, and regression coefficients $\\beta \\in \\mathbb{R}^p$, suppose the data-generating hierarchy is\n$$\ny_i \\mid \\beta,\\sigma^2,\\lambda_i \\sim \\mathcal{N}\\!\\left(x_i^{\\top}\\beta,\\, \\frac{\\sigma^2}{\\lambda_i}\\right), \\quad \\lambda_i \\stackrel{\\text{i.i.d.}}{\\sim} \\mathrm{Gamma}\\!\\left(\\frac{\\nu}{2},\\,\\frac{\\nu}{2}\\right), \\quad i=1,\\dots,n,\n$$\nwith $\\sigma^20$ and degrees of freedom $\\nu0$. Assume a conjugate Gaussian prior $\\beta \\sim \\mathcal{N}(b_0, V_0)$ with positive-definite $V_0$, and an independent prior on $\\sigma^2$ with density proportional to $(\\sigma^2)^{-a_0-1}\\exp(-b_0^{\\sigma}/\\sigma^2)$ for fixed $a_00$ and $b_0^{\\sigma}0$. Here $\\mathrm{Gamma}(a,b)$ denotes the Gamma distribution with shape $a$ and rate $b$. Let $X \\in \\mathbb{R}^{n \\times p}$ with rows $x_i^{\\top}$ be full column rank.\n\nTwo data augmentation schemes are under consideration:\n\n- Centered augmentation (CA): treat $\\lambda = (\\lambda_1,\\dots,\\lambda_n)$ as the latent variables and work directly with the likelihood $y_i \\mid \\beta,\\sigma^2,\\lambda_i \\sim \\mathcal{N}(x_i^{\\top}\\beta, \\sigma^2/\\lambda_i)$.\n\n- Non-centered augmentation (NCA): introduce latent standard normals by reparameterizing the observation equation as\n$$\ny_i \\;=\\; x_i^{\\top}\\beta \\;+\\; \\frac{\\sigma}{\\sqrt{\\lambda_i}}\\,\\epsilon_i, \\qquad \\epsilon_i \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,1),\n$$\nand regard $\\epsilon=(\\epsilon_1,\\dots,\\epsilon_n)$ as the augmented variables.\n\nStarting only from Bayes’ theorem and the definitions of the Normal and Gamma densities, analyze the posterior dependence structure induced by each augmentation. In particular:\n\n- Derive the full conditional distribution of $\\lambda_i$ under CA and show explicitly how it depends on the residual $r_i = y_i - x_i^{\\top}\\beta$ and the scale $\\sigma^2$.\n\n- Express the conditional Gaussian form for $\\beta \\mid \\sigma^2,\\lambda,y$ under CA, and identify where the dependence on $\\lambda$ enters the precision.\n\n- Show that the joint posterior density under CA can be written, up to a normalizing constant, as\n$$\np(\\beta,\\sigma^2,\\lambda \\mid y) \\;\\propto\\; p(\\beta)\\,p(\\sigma^2)\\,\\prod_{i=1}^n \\left\\{\\frac{\\sqrt{\\lambda_i}}{\\sigma}\\right\\}\\exp\\!\\left(-\\frac{1}{2}\\sum_{i=1}^n \\lambda_i \\frac{(y_i - x_i^{\\top}\\beta)^2}{\\sigma^2}\\right)\\,\\prod_{i=1}^n \\lambda_i^{\\frac{\\nu}{2}-1} e^{-\\frac{\\nu}{2}\\lambda_i}.\n$$\nRelate this to potential “funnel” geometries as $\\sigma^2$ varies with $\\lambda$.\n\n- Under NCA, consider the change of variables $z_i = \\sqrt{\\lambda_i}\\,(y_i - x_i^{\\top}\\beta)/\\sigma$. Using the independence structure of $\\epsilon_i$, show that $z=(z_1,\\dots,z_n)$ has a standard multivariate normal density, independent of $\\sigma^2$, and explain how this alters posterior coupling relative to CA.\n\nBased on your derivations, when the degrees of freedom $\\nu$ are small (heavy tails), which statement best characterizes the expected Markov chain Monte Carlo (MCMC) mixing behavior?\n\nA. When $\\nu$ is small, centered augmentation typically mixes poorly because $\\lambda_i \\mid \\beta,\\sigma^2,y$ has a low shape parameter and high variability, creating strong posterior dependence between $\\beta$, $\\sigma^2$, and $\\lambda$. The non-centered augmentation renders $\\epsilon$ ancillary to $(\\beta,\\sigma^2)$ and attenuates this dependence, improving mixing.\n\nB. When $\\nu$ is small, centered and non-centered parameterizations mix identically because they are related by a one-to-one reparameterization; any observed difference is purely an implementation artifact.\n\nC. When $\\nu$ is small, centered augmentation mixes better because the small values of $\\lambda_i$ downweight outliers in the conditional for $\\beta$, breaking dependence and stabilizing the chain.\n\nD. When $\\nu$ is small, non-centered augmentation becomes invalid because $\\epsilon_i = \\sqrt{\\lambda_i}(y_i - x_i^{\\top}\\beta)/\\sigma$ is deterministic given $(\\beta,\\sigma^2,\\lambda)$, so $\\epsilon$ cannot be used in a valid MCMC algorithm.",
            "solution": "This problem requires an analysis of two data augmentation schemes, Centered Augmentation (CA) and Non-Centered Augmentation (NCA), for a Bayesian heavy-tailed regression model. The model for an observation $y_i$ is specified as a Gaussian scale mixture, which is equivalent to a Student's t-distribution for the error term.\n\nFirst, I shall validate the problem statement.\n**Step 1: Extract Givens**\n-   **Likelihood:** $y_i \\mid \\beta,\\sigma^2,\\lambda_i \\sim \\mathcal{N}\\!\\left(x_i^{\\top}\\beta,\\, \\frac{\\sigma^2}{\\lambda_i}\\right)$ for $i=1,\\dots,n$.\n-   **Latent Variable Prior:** $\\lambda_i \\stackrel{\\text{i.i.d.}}{\\sim} \\mathrm{Gamma}\\!\\left(\\frac{\\nu}{2},\\,\\frac{\\nu}{2}\\right)$.\n-   **Model Parameters:** Regression coefficients $\\beta \\in \\mathbb{R}^p$, scale $\\sigma^20$, degrees of freedom $\\nu0$.\n-   **Priors:** $\\beta \\sim \\mathcal{N}(b_0, V_0)$ with $V_0$ positive-definite; $p(\\sigma^2) \\propto (\\sigma^2)^{-a_0-1}\\exp(-b_0^{\\sigma}/\\sigma^2)$ for constants $a_00, b_0^{\\sigma}0$. The prior on $\\sigma^2$ is an Inverse-Gamma distribution, $\\sigma^2 \\sim \\mathrm{IG}(a_0, b_0^{\\sigma})$.\n-   **Data:** Covariate vectors $x_i \\in \\mathbb{R}^p$. The matrix $X \\in \\mathbb{R}^{n \\times p}$ with rows $x_i^{\\top}$ is full column rank.\n-   **Centered Augmentation (CA):** Latent variables are $\\lambda = (\\lambda_1,\\dots,\\lambda_n)$.\n-   **Non-Centered Augmentation (NCA):** Reparameterization $y_i = x_i^{\\top}\\beta + \\frac{\\sigma}{\\sqrt{\\lambda_i}}\\,\\epsilon_i$ with $\\epsilon_i \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,1)$, and $\\epsilon=(\\epsilon_1,\\dots,\\epsilon_n)$ are the augmented variables.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, employing a standard and well-established statistical model (Bayesian robust regression via Student's t-likelihood). The scale-mixture representation of the Student's t-distribution is correctly formulated. The concepts of centered and non-centered parameterizations are fundamental topics in MCMC methods for hierarchical models. The problem is well-posed, with a fully specified model hierarchy and proper priors, ensuring a proper posterior distribution. All mathematical terms are standard and clearly defined. The language is objective and precise. The problem setup is internally consistent and complete. It poses a meaningful question about the computational properties of two valid MCMC strategies.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will now proceed with the derivations and analysis.\n\n### Analysis of Centered Augmentation (CA)\n\nUnder CA, a Gibbs sampler would iterate through the full conditional distributions of $\\beta$, $\\sigma^2$, and $\\lambda$.\n\n**1. Full conditional distribution of $\\lambda_i$ under CA:**\nThe full conditional for $\\lambda_i$ is $p(\\lambda_i \\mid \\beta, \\sigma^2, y, \\lambda_{j \\neq i})$. Due to conditional independence, this simplifies to $p(\\lambda_i \\mid \\beta, \\sigma^2, y_i)$. Using Bayes' theorem, $p(\\lambda_i \\mid \\beta, \\sigma^2, y_i) \\propto p(y_i \\mid \\beta, \\sigma^2, \\lambda_i) p(\\lambda_i)$.\n\nThe likelihood term is the density of $\\mathcal{N}(x_i^{\\top}\\beta, \\sigma^2/\\lambda_i)$:\n$$p(y_i \\mid \\beta, \\sigma^2, \\lambda_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2/\\lambda_i}}\\exp\\left(-\\frac{(y_i-x_i^{\\top}\\beta)^2}{2\\sigma^2/\\lambda_i}\\right) \\propto \\lambda_i^{1/2} \\exp\\left(-\\frac{\\lambda_i (y_i-x_i^{\\top}\\beta)^2}{2\\sigma^2}\\right)$$\nThe prior is $p(\\lambda_i) \\sim \\mathrm{Gamma}(\\nu/2, \\nu/2)$:\n$$p(\\lambda_i) \\propto \\lambda_i^{\\frac{\\nu}{2}-1} \\exp\\left(-\\frac{\\nu}{2}\\lambda_i\\right)$$\nMultiplying these gives the kernel of the conditional posterior for $\\lambda_i$:\n$$p(\\lambda_i \\mid \\beta, \\sigma^2, y_i) \\propto \\lambda_i^{1/2} \\exp\\left(-\\frac{\\lambda_i (y_i-x_i^{\\top}\\beta)^2}{2\\sigma^2}\\right) \\lambda_i^{\\frac{\\nu}{2}-1} \\exp\\left(-\\frac{\\nu}{2}\\lambda_i\\right)$$\n$$p(\\lambda_i \\mid \\beta, \\sigma^2, y_i) \\propto \\lambda_i^{\\left(\\frac{\\nu+1}{2}\\right)-1} \\exp\\left(-\\lambda_i \\left[\\frac{(y_i-x_i^{\\top}\\beta)^2}{2\\sigma^2} + \\frac{\\nu}{2}\\right]\\right)$$\nThis is the kernel of a Gamma distribution. Defining the residual as $r_i = y_i - x_i^{\\top}\\beta$, the full conditional is:\n$$\\lambda_i \\mid \\beta, \\sigma^2, y_i \\sim \\mathrm{Gamma}\\left(\\frac{\\nu+1}{2}, \\frac{r_i^2}{2\\sigma^2} + \\frac{\\nu}{2}\\right)$$\nThis explicitly shows that the conditional for $\\lambda_i$ depends on both the residual $r_i$ (which involves $\\beta$) and the scale $\\sigma^2$.\n\n**2. Conditional Gaussian form for $\\beta \\mid \\sigma^2, \\lambda, y$ under CA:**\nThe full conditional for $\\beta$ is $p(\\beta \\mid \\sigma^2, \\lambda, y) \\propto p(y \\mid \\beta, \\sigma^2, \\lambda) p(\\beta)$.\nThe likelihood can be written in vector form as $y \\sim \\mathcal{N}(X\\beta, \\sigma^2\\Lambda^{-1})$, where $\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$. The log-likelihood is:\n$$\\log p(y \\mid \\beta, \\sigma^2, \\lambda) = -\\frac{1}{2\\sigma^2}(y-X\\beta)^{\\top}\\Lambda(y-X\\beta) + C_1$$\nThe prior is $\\beta \\sim \\mathcal{N}(b_0, V_0)$, with log-prior:\n$$\\log p(\\beta) = -\\frac{1}{2}(\\beta-b_0)^{\\top}V_0^{-1}(\\beta-b_0) + C_2$$\nThe log-posterior for $\\beta$ is the sum of these. Expanding the quadratic forms in $\\beta$ reveals the posterior is Gaussian. The terms involving $\\beta$ are:\n$$-\\frac{1}{2\\sigma^2}(\\beta^{\\top}X^{\\top}\\Lambda X \\beta - 2\\beta^{\\top}X^{\\top}\\Lambda y) - \\frac{1}{2}(\\beta^{\\top}V_0^{-1}\\beta - 2\\beta^{\\top}V_0^{-1}b_0)$$\nThis corresponds to a Gaussian distribution $\\mathcal{N}(b_n, V_n)$ with precision $V_n^{-1}$ and mean $b_n$ given by:\n$$V_n^{-1} = \\frac{1}{\\sigma^2}X^{\\top}\\Lambda X + V_0^{-1} = \\frac{1}{\\sigma^2}\\sum_{i=1}^n \\lambda_i x_i x_i^{\\top} + V_0^{-1}$$\n$$b_n = V_n\\left(\\frac{1}{\\sigma^2}X^{\\top}\\Lambda y + V_0^{-1}b_0\\right)$$\nThe dependence on $\\lambda$ enters the precision matrix through the weighted sum-of-squares term $X^{\\top}\\Lambda X$, where each $\\lambda_i$ acts as a weight for the $i$-th observation's contribution.\n\n**3. Joint posterior and funnel geometry under CA:**\nThe joint posterior is $p(\\beta, \\sigma^2, \\lambda \\mid y) \\propto p(y \\mid \\beta, \\sigma^2, \\lambda) p(\\beta) p(\\sigma^2) p(\\lambda)$. The problem asks to verify a specific form:\n$$p(y \\mid \\beta, \\sigma^2, \\lambda) = \\prod_{i=1}^n p(y_i \\mid \\beta, \\sigma^2, \\lambda_i) = \\prod_{i=1}^n \\frac{\\sqrt{\\lambda_i}}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{\\lambda_i(y_i-x_i^{\\top}\\beta)^2}{2\\sigma^2}\\right)$$\n$$p(\\lambda) = \\prod_{i=1}^n p(\\lambda_i) \\propto \\prod_{i=1}^n \\lambda_i^{\\frac{\\nu}{2}-1}e^{-\\frac{\\nu}{2}\\lambda_i}$$\nMultiplying by $p(\\beta)p(\\sigma^2)$ and grouping terms confirms the expression in the problem statement. The expression reveals a strong coupling between $\\sigma^2$ and $\\lambda_i$ through the term $\\exp(-\\frac{\\lambda_i r_i^2}{2\\sigma^2})$. This coupling leads to a challenging posterior geometry.\nFrom the conditional for $\\lambda_i$, its rate is $\\frac{r_i^2}{2\\sigma^2} + \\frac{\\nu}{2}$. As $\\sigma^2 \\to 0$, this rate approaches infinity, forcing the conditional distribution of $\\lambda_i$ to concentrate sharply at $0$. Conversely, the conditional for $\\sigma^2$ is $\\mathrm{IG}(a_0 + n/2, b_0^{\\sigma} + \\frac{1}{2}\\sum_i \\lambda_i r_i^2)$, whose scale parameter depends on the $\\lambda_i$'s. This interdependence creates a \"funnel\" shape in the joint posterior of $(\\sigma^2, \\lambda)$: the allowable region for $\\lambda$ dramatically shrinks as $\\sigma^2$ becomes small. A Gibbs sampler can mix very slowly as it struggles to move between the narrow \"neck\" and the wide \"mouth\" of this funnel.\n\n### Analysis of Non-Centered Augmentation (NCA)\n\n**4. NCA characterization:**\nThe NCA reparameterizes the model as $y_i = x_i^{\\top}\\beta + \\frac{\\sigma}{\\sqrt{\\lambda_i}}\\epsilon_i$, where $\\epsilon_i \\sim \\mathcal{N}(0,1)$.\nThe question introduces the variable $z_i = \\sqrt{\\lambda_i}\\,(y_i - x_i^{\\top}\\beta)/\\sigma$. By rearranging the NCA equation, we get $\\epsilon_i = \\sqrt{\\lambda_i}\\,(y_i - x_i^{\\top}\\beta)/\\sigma$. Thus, $z_i = \\epsilon_i$.\nSince $\\epsilon_i$ are i.i.d. standard normal variables in the generative model, the vector $z=(z_1,\\dots,z_n)$ by definition has a standard multivariate normal distribution, $z \\sim \\mathcal{N}(0, I_n)$. The distribution of $z$, being fixed as $\\mathcal{N}(0, I_n)$, is independent of all model parameters, including $\\sigma^2$ and $\\beta$.\n\nThe CA Gibbs sampler suffers because the conditional distributions create a tight feedback loop: $\\beta \\to r_i \\to \\lambda_i \\to \\beta$ and $\\sigma^2 \\leftrightarrow \\lambda_i$. The NCA approach is designed to mitigate this coupling. It is based on reparameterizing the model in terms of variables that are, a priori, independent. In the generative model, $\\epsilon$, $\\lambda$, $\\beta$, and $\\sigma^2$ are all drawn from independent priors.\nAn MCMC algorithm built on the NCA formulation can sample from a state space (e.g., involving $\\epsilon$) where the components are less correlated in the posterior. For example, by sampling $\\sigma$ and $\\epsilon$ separately, the direct multiplicative link $\\sigma_i^* = \\sigma/\\sqrt{\\lambda_i}$ that causes the funnel in CA is broken at the algorithmic level. This change in the geometry of the sampling space leads to more efficient exploration and better MCMC mixing, especially when the posterior dependence in the CA is strong. Such strong dependence occurs when $\\nu$ is small, as the prior for $\\lambda_i$, $\\mathrm{Gamma}(\\nu/2, \\nu/2)$, has high variance ($2/\\nu$), allowing for extreme values of $\\lambda_i$ and thus amplifying the coupling.\n\n### Evaluation of Options\n\n**A. When $\\nu$ is small, centered augmentation typically mixes poorly because $\\lambda_i \\mid \\beta,\\sigma^2,y$ has a low shape parameter and high variability, creating strong posterior dependence between $\\beta$, $\\sigma^2$, and $\\lambda$. The non-centered augmentation renders $\\epsilon$ ancillary to $(\\beta,\\sigma^2)$ and attenuates this dependence, improving mixing.**\nThis statement is a correct and precise summary of the situation. For small $\\nu$, the prior variance of $\\lambda_i$ is high, which leads to strong posterior coupling (the funnel geometry) in the CA scheme, causing poor MCMC mixing. The NCA approach leverages the fact that $\\epsilon_i$ has a parameter-free prior distribution (ancillarity in the generative model) to construct an MCMC algorithm with reduced posterior dependence between parameters, thereby improving mixing.\n\n**B. When $\\nu$ is small, centered and non-centered parameterizations mix identically because they are related by a one-to-one reparameterization; any observed difference is purely an implementation artifact.**\nThis is incorrect. While they are reparameterizations of the same model and lead to the same posterior distribution, their MCMC performance is fundamentally different. The choice of parameterization alters the geometry of the space the sampler explores, which directly impacts mixing efficiency. The differences are a well-documented and fundamental aspect of MCMC algorithm design, not an artifact.\n\n**C. When $\\nu$ is small, centered augmentation mixes better because the small values of $\\lambda_i$ downweight outliers in the conditional for $\\beta$, breaking dependence and stabilizing the chain.**\nThis is incorrect. While it is true that small $\\lambda_i$ downweight the influence of corresponding observations (which is the mechanism of robustness), this very mechanism is the cause of the strong posterior dependence. The feedback between large residuals, small $\\lambda_i$, and the conditionals for $\\beta$ and $\\sigma^2$ is what creates the problematic funnel geometry and *destabilizes* MCMC mixing, rather than breaking dependence.\n\n**D. When $\\nu$ is small, non-centered augmentation becomes invalid because $\\epsilon_i = \\sqrt{\\lambda_i}(y_i - x_i^{\\top}\\beta)/\\sigma$ is deterministic given $(\\beta,\\sigma^2,\\lambda)$, so $\\epsilon$ cannot be used in a valid MCMC algorithm.**\nThis is incorrect. The deterministic relationship simply reflects the change of variables. It does not invalidate the MCMC scheme. Non-centered parameterization is a standard and valid technique. The deterministic link means that one cannot treat, for instance, $\\lambda$ and $\\epsilon$ as independent parameters in a Gibbs sampler state. However, it allows for a valid reparameterization of the state space (e.g., from $(\\beta, \\sigma^2, \\lambda)$ to $(\\beta, \\sigma^2, \\epsilon)$ with appropriate Jacobian adjustments) or for designing more sophisticated MCMC updates that exploit this structure.\n\nBased on the derivations and analysis, statement A provides the best characterization of the MCMC mixing behavior.",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "The principles of data augmentation extend beyond models with natural latent variables to create convenient structure in seemingly unrelated contexts, like logistic regression. This practice explores the powerful and modern Pólya-Gamma data augmentation scheme, which transforms the intractable logistic likelihood into a conditionally Gaussian form by introducing auxiliary latent variables. You will bridge theory and practice by implementing a complete Gibbs sampling iteration for Bayesian logistic regression, gaining hands-on experience with a state-of-the-art MCMC technique .",
            "id": "3302005",
            "problem": "Consider Bayesian logistic regression with a binary response and a Gaussian prior, fit using one-step of a Gibbs sampler constructed via Pólya–Gamma (PG) data augmentation. Let there be $n$ observations with predictors $x_i \\in \\mathbb{R}^p$, responses $y_i \\in \\{0,1\\}$, and regression coefficients $\\beta \\in \\mathbb{R}^p$. Assume the prior $\\beta \\sim \\mathcal{N}(0, \\sigma_\\beta^2 I_p)$. The logistic model has likelihood $p(y_i \\mid x_i, \\beta) = \\operatorname{Bernoulli}(\\pi_i)$ with $\\pi_i = (1 + \\exp(-x_i^\\top \\beta))^{-1}$. A single full Gibbs iteration under Pólya–Gamma (PG) augmentation samples latent variables $\\omega_i$ and then samples $\\beta$ from its full conditional, where the Pólya–Gamma distribution is denoted $\\operatorname{PG}(b, c)$ and $b \\in \\mathbb{R}_+$, $c \\in \\mathbb{R}$.\n\nYour task is to implement, for each specified test case, exactly one full Gibbs iteration consisting of:\n- Sampling each $\\omega_i \\sim \\operatorname{PG}(1, x_i^\\top \\beta^{(0)})$ for $i \\in \\{1,\\dots,n\\}$ using the truncated infinite-series representation with a specified truncation level $K$.\n- Sampling $\\beta^{(1)}$ from its full conditional given $\\omega = (\\omega_1,\\dots,\\omega_n)$, the prior $\\beta \\sim \\mathcal{N}(0, \\sigma_\\beta^2 I_p)$, and the logistic likelihood with the PG augmentation.\n\nUse the following synthetic test suite. For each case, take the initial state $\\beta^{(0)} = 0_p$ (the $p$-dimensional zero vector), and use the specified truncation level $K$, prior variance $\\sigma_\\beta^2$, and the given data $(X,y)$. All real numbers, integers, and vectors below are exact and must be used as provided.\n\nTest case $1$:\n- $n = 5$, $p = 2$.\n- $X^{(1)} = \\begin{bmatrix}\n1.0  -0.5 \\\\\n1.2  0.3 \\\\\n-0.7  1.4 \\\\\n0.0  -1.0 \\\\\n2.0  2.0\n\\end{bmatrix}$,\n$y^{(1)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix}$.\n- $\\sigma_\\beta^2 = 5.0$.\n- $K = 200$.\n\nTest case $2$:\n- $n = 8$, $p = 3$.\n- $X^{(2)} = \\begin{bmatrix}\n1.0  0.5  -1.0 \\\\\n-0.3  2.0  0.7 \\\\\n0.8  -0.6  1.5 \\\\\n1.2  1.1  0.0 \\\\\n-1.5  0.4  -0.2 \\\\\n0.0  -1.2  0.9 \\\\\n2.0  0.0  1.0 \\\\\n-0.7  -0.8  0.3\n\\end{bmatrix}$,\n$y^{(2)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$.\n- $\\sigma_\\beta^2 = 2.5$.\n- $K = 150$.\n\nTest case $3$ (rank-deficient design to test numerical stability with the prior):\n- $n = 5$, $p = 3$.\n- $X^{(3)} = \\begin{bmatrix}\n1.0  1.0  2.0 \\\\\n0.9  1.1  2.0 \\\\\n1.1  0.9  2.0 \\\\\n-0.5  -0.5  -1.0 \\\\\n2.0  2.0  4.0\n\\end{bmatrix}$,\n$y^{(3)} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$.\n- $\\sigma_\\beta^2 = 10.0$.\n- $K = 50$.\n\nRandomness and determinism requirements:\n- Use a single global base seed $s_0 = 2025$ for a Pseudorandom Number Generator. For test case index $j \\in \\{1,2,3\\}$, create an independent generator with seed $s_j = s_0 + j$.\n- Use double-precision floating-point arithmetic.\n\nPólya–Gamma sampler requirement:\n- Use the truncated infinite-series representation for $\\omega \\sim \\operatorname{PG}(1, c)$:\n$$\n\\omega \\approx \\frac{1}{2 \\pi^2} \\sum_{k=1}^{K} \\frac{g_k}{\\left(k - \\tfrac{1}{2}\\right)^2 + \\frac{c^2}{4 \\pi^2}}, \\quad g_k \\overset{\\text{i.i.d.}}{\\sim} \\operatorname{Gamma}(1,1).\n$$\n- Evaluate the series exactly as written with the given $K$.\n\nComputational complexity accounting model per iteration:\n- Count one primitive operation for each addition, subtraction, multiplication, division, or call to a random variate generator.\n- Constants such as $\\pi$ cost zero operations; reading array elements costs zero operations.\n- Cost to form $c_i = x_i^\\top \\beta^{(0)}$ per $i$ is $p$ multiplications plus $(p-1)$ additions.\n- For each $\\omega_i$, let $c = c_i$ and define a per-$i$ overhead of $1$ multiplication for $c^2$, $1$ division for $c^2/(4\\pi^2)$, and $1$ multiplication or division to scale the final series sum by $1/(2\\pi^2)$. The per-term cost for each $k \\in \\{1,\\dots,K\\}$ is exactly $6$ operations: one subtraction to form $k - \\tfrac{1}{2}$, one multiplication to square it, one addition to add the constant offset, one gamma random draw, one division for $g_k$ divided by the denominator, and one addition to accumulate the series.\n- To assemble the Gaussian linear algebra for sampling $\\beta$: form $W X$ by scaling each row of $X$ by $\\omega_i$ (cost $n p$ multiplications), then form $X^\\top (W X)$ using the naive algorithm with cost $p^2 n$ multiplications and $p^2 (n-1)$ additions (total $p^2 (2n - 1)$ operations). Add the ridge term $(1/\\sigma_\\beta^2) I_p$ to the diagonal: $1$ division to form $1/\\sigma_\\beta^2$ plus $p$ additions to the diagonal.\n- Compute $\\kappa = y - \\tfrac{1}{2}\\mathbf{1}$: exactly $n$ subtractions. Form $b = X^\\top \\kappa$: cost $p n$ multiplications and $p(n-1)$ additions (total $p (2n - 1)$ operations).\n- Compute the Cholesky factorization $A = L L^\\top$ of the precision matrix $A$ using a numerically stable method; count it as $\\left\\lfloor p^3/3 \\right\\rfloor$ primitive operations.\n- Solve $A \\mu = b$ using forward and back substitution with $L$ and $L^\\top$: count $2 p^2$ operations for the two triangular solves combined.\n- Draw $z \\sim \\mathcal{N}(0, I_p)$: count $p$ operations for the $p$ independent standard normal draws.\n- Form one Gaussian sample with covariance $A^{-1}$ using a single triangular solve $L^\\top x = z$ and set $\\beta^{(1)} = \\mu + x$: count $p^2$ operations for the triangular solve and $p$ additions for the final sum.\n\nUnder this model, the total operation count for one iteration with given $n$, $p$, $K$ is\n$$\n\\begin{aligned}\n\\mathrm{Ops}(n,p,K) \\;=\\; n\\big( p + (p-1) + 1 + 1 + 1 + 6K \\big) \\\\\n+ n p \\\\\n+ p^2 (2n - 1) \\\\\n+ 1 + p \\\\\n+ n \\\\\n+ p (2n - 1) \\\\\n+ \\left\\lfloor \\frac{p^3}{3} \\right\\rfloor \\\\\n+ 3 p^2 \\\\\n+ p \\\\\n+ p.\n\\end{aligned}\n$$\nThis formula must be implemented exactly as stated.\n\nProgram requirements:\n- For each test case, perform exactly one full Gibbs iteration as specified above and compute the total operations using the provided accounting model.\n- Your program should produce a single line of output containing the results for the three cases as a comma-separated list of three lists, one per test case. For each test case $j$, output a list containing the sampled $\\beta^{(1)}$ entries in order, followed by the integer total operation count for that case. For example, the final format should be like\n$[\\,[\\beta^{(1)}_{1},\\dots,\\beta^{(1)}_{p},\\mathrm{Ops}]\\, , \\, [\\dots] \\, , \\, [\\dots]\\,]$,\nwith all floating-point numbers printed in standard decimal form.",
            "solution": "The problem requires the implementation of a single iteration of a Gibbs sampler for Bayesian logistic regression. This is achieved using the Pólya–Gamma (PG) data augmentation scheme, a powerful technique for simplifying posterior inference in models with logistic-like functions. The process involves two main steps: sampling the auxiliary Pólya–Gamma latent variables, and then sampling the regression coefficients from their full conditional distribution, which becomes a multivariate Gaussian thanks to the augmentation.\n\nThe logistic likelihood for a single observation $y_i \\in \\{0, 1\\}$ with predictor vector $x_i \\in \\mathbb{R}^p$ and coefficients $\\beta \\in \\mathbb{R}^p$ is given by\n$$ p(y_i \\mid x_i, \\beta) = \\frac{(e^{x_i^\\top \\beta})^{y_i}}{1 + e^{x_i^\\top \\beta}} $$\nThis expression can be rewritten by introducing a latent variable $\\omega_i$ that follows a Pólya–Gamma distribution, denoted $\\operatorname{PG}(b, c)$. The key identity is:\n$$ \\frac{(e^{\\psi})^a}{(1+e^{\\psi})^b} = 2^{-b} e^{(a-b/2)\\psi} \\int_0^\\infty e^{-\\omega \\psi^2/2} p(\\omega \\mid b, 0) \\, d\\omega $$\nA transformation of this identity (by tilting the PG distribution) allows us to express the logistic likelihood as a mixture of Gaussians:\n$$ p(y_i \\mid x_i, \\beta) \\propto e^{\\kappa_i x_i^\\top \\beta} \\int_0^\\infty e^{-\\frac{\\omega_i}{2}(x_i^\\top \\beta)^2} p(\\omega_i \\mid 1, x_i^\\top \\beta) \\, d\\omega_i $$\nwhere $\\kappa_i = y_i - \\frac{1}{2}$. This formulation is advantageous because, conditional on $\\omega_i$, the likelihood term is proportional to a Gaussian density with respect to $\\beta$.\n\nA Gibbs sampler iteratively draws from the full conditional distributions of the parameters. For this model, the two steps are:\n1.  Sample the latent variables $\\omega = (\\omega_1, \\dots, \\omega_n)$ given the current coefficients $\\beta^{(t)}$.\n2.  Sample the coefficients $\\beta^{(t+1)}$ given the data and the newly sampled latent variables $\\omega$.\n\n**Step 1: Sampling the Latent Variables $\\omega_i$**\nThe full conditional distribution for each $\\omega_i$ is $\\omega_i \\mid \\beta \\sim \\operatorname{PG}(1, x_i^\\top \\beta)$. The problem specifies starting with an initial state $\\beta^{(0)} = 0_p$, the $p$-dimensional zero vector. Consequently, the parameter $c_i = x_i^\\top \\beta^{(0)}$ for the PG distribution is $0$ for all $i=1, \\dots, n$. We are thus required to sample $\\omega_i \\sim \\operatorname{PG}(1, 0)$.\nThe problem mandates using a specific truncated infinite-series representation for this sampling process:\n$$ \\omega_i \\approx \\frac{1}{2 \\pi^2} \\sum_{k=1}^{K} \\frac{g_k}{\\left(k - \\frac{1}{2}\\right)^2 + \\frac{c_i^2}{4 \\pi^2}} $$\nwhere $g_k \\overset{\\text{i.i.d.}}{\\sim} \\operatorname{Gamma}(1,1)$ (which is equivalent to an Exponential distribution with rate $1$) and $K$ is the specified truncation level. Since $c_i=0$, the formula simplifies to:\n$$ \\omega_i \\approx \\frac{1}{2 \\pi^2} \\sum_{k=1}^{K} \\frac{g_k}{\\left(k - \\frac{1}{2}\\right)^2} $$\nFor each observation $i$, we generate $K$ Gamma variates and compute this sum.\n\n**Step 2: Sampling the Coefficients $\\beta^{(1)}$**\nGiven the sampled latent variables $\\omega_1, \\dots, \\omega_n$, we sample $\\beta^{(1)}$ from its full conditional distribution $p(\\beta \\mid y, X, \\omega)$. The posterior is proportional to the product of the prior and the augmented likelihood:\n$$ p(\\beta \\mid y, X, \\omega) \\propto p(\\beta) \\prod_{i=1}^n p(y_i \\mid x_i, \\beta, \\omega_i) $$\nThe prior is $\\beta \\sim \\mathcal{N}(0, \\sigma_\\beta^2 I_p)$, with density $p(\\beta) \\propto \\exp\\left(-\\frac{1}{2\\sigma_\\beta^2} \\beta^\\top \\beta\\right)$.\nThe augmented likelihood is $\\prod_{i=1}^n \\exp\\left( \\kappa_i x_i^\\top \\beta - \\frac{\\omega_i}{2}(x_i^\\top\\beta)^2 \\right)$.\nCombining these terms, the log-posterior for $\\beta$ is a quadratic function:\n$$ \\log p(\\beta \\mid \\dots) = C - \\frac{1}{2\\sigma_\\beta^2} \\beta^\\top \\beta + \\sum_{i=1}^n \\left( \\kappa_i x_i^\\top \\beta - \\frac{\\omega_i}{2}(x_i^\\top\\beta)^2 \\right) \\\\ = C - \\frac{1}{2} \\left( \\beta^\\top \\left( \\sum_{i=1}^n \\omega_i x_i x_i^\\top + \\frac{1}{\\sigma_\\beta^2}I_p \\right) \\beta - 2 \\left( \\sum_{i=1}^n \\kappa_i x_i \\right)^\\top \\beta \\right) $$\nThis is the kernel of a multivariate Gaussian distribution, $\\mathcal{N}(\\mu_\\beta, \\Sigma_\\beta)$. The precision matrix $\\Sigma_\\beta^{-1}$ and mean $\\mu_\\beta$ are:\n$$ A = \\Sigma_\\beta^{-1} = X^\\top W X + \\frac{1}{\\sigma_\\beta^2}I_p $$\n$$ \\mu_\\beta = A^{-1} (X^\\top \\kappa) $$\nwhere $W = \\operatorname{diag}(\\omega_1, \\dots, \\omega_n)$ and $\\kappa = (y_1 - \\frac{1}{2}, \\dots, y_n - \\frac{1}{2})^\\top$.\nTo sample $\\beta^{(1)} \\sim \\mathcal{N}(\\mu_\\beta, A^{-1})$, we follow the prescribed method:\n1.  Compute the precision matrix $A$ and the vector $b = X^\\top \\kappa$.\n2.  Compute the mean $\\mu_\\beta$ by solving the linear system $A \\mu_\\beta = b$.\n3.  Perform a Cholesky decomposition of the precision matrix, $A = LL^\\top$.\n4.  Draw a vector of independent standard normal variates, $z \\sim \\mathcal{N}(0, I_p)$.\n5.  Generate a sample from $\\mathcal{N}(0, A^{-1})$ by computing $x = (L^\\top)^{-1} z$. This is done by solving the triangular system $L^\\top x = z$.\n6.  The final sample is obtained by adding the mean: $\\beta^{(1)} = \\mu_\\beta + x$.\n\nFinally, the computational cost of this iteration is calculated using the exact, explicitly provided formula, which sums the costs of each arithmetic operation and random variate generation according to a detailed accounting model. This implementation will adhere strictly to all specified numerical values, seeds for random number generation, and procedural steps for each test case.",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian logistic regression problem using one Gibbs iteration \n    with a Pólya–Gamma data augmentation scheme for three test cases.\n    \"\"\"\n    \n    # Store test case parameters\n    test_cases = [\n        {\n            \"n\": 5, \"p\": 2,\n            \"X\": np.array([\n                [1.0, -0.5], [1.2, 0.3], [-0.7, 1.4], [0.0, -1.0], [2.0, 2.0]\n            ]),\n            \"y\": np.array([1, 0, 1, 0, 1]),\n            \"sigma_beta_sq\": 5.0,\n            \"K\": 200,\n            \"case_index\": 1\n        },\n        {\n            \"n\": 8, \"p\": 3,\n            \"X\": np.array([\n                [1.0, 0.5, -1.0], [-0.3, 2.0, 0.7], [0.8, -0.6, 1.5], [1.2, 1.1, 0.0],\n                [-1.5, 0.4, -0.2], [0.0, -1.2, 0.9], [2.0, 0.0, 1.0], [-0.7, -0.8, 0.3]\n            ]),\n            \"y\": np.array([1, 0, 1, 1, 0, 0, 1, 0]),\n            \"sigma_beta_sq\": 2.5,\n            \"K\": 150,\n            \"case_index\": 2\n        },\n        {\n            \"n\": 5, \"p\": 3,\n            \"X\": np.array([\n                [1.0, 1.0, 2.0], [0.9, 1.1, 2.0], [1.1, 0.9, 2.0],\n                [-0.5, -0.5, -1.0], [2.0, 2.0, 4.0]\n            ]),\n            \"y\": np.array([1, 1, 0, 0, 1]),\n            \"sigma_beta_sq\": 10.0,\n            \"K\": 50,\n            \"case_index\": 3\n        }\n    ]\n\n    s0 = 2025\n    results = []\n\n    def sample_polya_gamma(rng, c, K):\n        \"\"\"Samples from PG(1, c) using a truncated series.\"\"\"\n        c2_term = c**2 / (4 * np.pi**2)\n        g_k = rng.standard_gamma(1, size=K)\n        k_vals = np.arange(1, K + 1)\n        denominators = (k_vals - 0.5)**2 + c2_term\n        omega = np.sum(g_k / denominators) / (2 * np.pi**2)\n        return omega\n\n    def calculate_ops(n, p, K):\n        \"\"\"Calculates total operations based on the provided accounting model.\"\"\"\n        term1 = n * (p + (p - 1) + 1 + 1 + 1 + 6 * K)\n        term2 = n * p\n        term3 = p**2 * (2 * n - 1)\n        term4 = 1 + p\n        term5 = n\n        term6 = p * (2 * n - 1)\n        term7 = p**3 // 3\n        term8 = 3 * p**2\n        term9 = p\n        term10 = p\n        return int(term1 + term2 + term3 + term4 + term5 + term6 + term7 + term8 + term9 + term10)\n\n    for case in test_cases:\n        n, p = case[\"n\"], case[\"p\"]\n        X, y = case[\"X\"], case[\"y\"]\n        sigma_beta_sq, K = case[\"sigma_beta_sq\"], case[\"K\"]\n        case_index = case[\"case_index\"]\n\n        # Set up the random number generator for the current case\n        seed = s0 + case_index\n        rng = np.random.default_rng(seed)\n\n        # Initial state beta_0 is a zero vector\n        beta_0 = np.zeros(p)\n\n        # --- Step 1: Sample latent variables omega ---\n        # Since beta_0 is zero, c_i = x_i^T beta_0 = 0 for all i\n        c_vals = X @ beta_0\n        \n        omegas = np.array([sample_polya_gamma(rng, c_vals[i], K) for i in range(n)])\n\n        # --- Step 2: Sample beta^(1) from its full conditional ---\n        W = np.diag(omegas)\n        \n        # Precision matrix of the Gaussian conditional for beta\n        A = X.T @ W @ X + (1 / sigma_beta_sq) * np.identity(p)\n        \n        # Mean component\n        kappa = y - 0.5\n        b = X.T @ kappa\n        \n        # Mean of the Gaussian conditional\n        mu = linalg.solve(A, b, assume_a='pos')\n        \n        # Sample from N(mu, A^-1) using Cholesky decomposition\n        L = linalg.cholesky(A, lower=True)\n        z = rng.standard_normal(size=p)\n        \n        # Solve L^T x = z for x\n        x = linalg.solve_triangular(L.T, z, lower=False)\n        \n        beta_1 = mu + x\n        \n        # --- Calculate total operations ---\n        ops = calculate_ops(n, p, K)\n\n        # Append results for this case\n        case_result = list(beta_1) + [ops]\n        results.append(case_result)\n\n    # Format and print the final output exactly as required\n    output_str = str(results).replace(\"], [\", \"], \\n [\")\n    final_output = f\"[{','.join(str(r) for r in results)}]\"\n    \n    # Based on the prompt's example format: \"[\\,[\\beta^{(1)}_{1},\\dots,\\beta^{(1)}_{p},\\mathrm{Ops}]\\, , \\, [\\dots] \\, , \\, [\\dots]\\,]\"\n    # This implies a comma and space between the lists.\n    print(str(results).replace(\"], [\", \"], [\"))\n\nsolve()\n```"
        }
    ]
}