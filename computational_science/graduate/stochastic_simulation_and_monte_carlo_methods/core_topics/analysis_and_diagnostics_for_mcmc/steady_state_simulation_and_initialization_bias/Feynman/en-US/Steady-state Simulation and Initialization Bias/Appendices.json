{
    "hands_on_practices": [
        {
            "introduction": "Understanding the origin and structure of initialization bias is the first step toward mastering steady-state simulation. This exercise provides a foundational, first-principles look at how bias arises in a simple two-state Markov chain. By solving the Poisson equation, you will derive the exact leading-order term for the bias, revealing a deep connection between the system's dynamics ($p$ and $q$), its long-run behavior, and the transient error caused by a specific starting state .",
            "id": "3347939",
            "problem": "Consider a discrete-time, time-homogeneous, irreducible and aperiodic Markov chain on the finite state space $\\{0,1\\}$ with transition matrix\n$$\nP \\;=\\; \\begin{pmatrix} 1-p & p \\\\ q & 1-q \\end{pmatrix},\n$$\nwhere $p \\in (0,1)$ and $q \\in (0,1)$. Let $X_0$ denote the initial state and let $\\{X_k\\}_{k \\geq 0}$ denote the trajectory of the chain. Define the performance function $f:\\{0,1\\}\\to\\mathbb{R}$ by $f(0)=0$ and $f(1)=1$, and define the time average\n$$\n\\bar{f}_n \\;=\\; \\frac{1}{n} \\sum_{k=0}^{n-1} f(X_k).\n$$\nLet $\\pi$ denote the unique stationary distribution of the Markov chain and write $\\pi(f) = \\sum_{x \\in \\{0,1\\}} \\pi(x) f(x)$. The initialization bias in steady-state simulation is the deviation $\\mathbb{E}[\\bar{f}_n] - \\pi(f)$ induced by the initial condition $X_0=0$.\n\nStarting from core definitions of stationary distributions and the Markov property, and using the characterization that a solution $h:\\{0,1\\}\\to\\mathbb{R}$ to the Poisson equation satisfies $h - P h = f - \\pi(f)$ together with the normalization $\\sum_{x \\in \\{0,1\\}} \\pi(x) h(x) = 0$, carry out the following steps:\n\n- Derive the stationary distribution $\\pi$ and compute $\\pi(f)$.\n- Solve the Poisson equation for $h$ with the stated normalization.\n- From first principles, derive an explicit expression for the leading constant $C(p,q)$ in the asymptotic expansion of the initialization bias for $X_0=0$,\n$$\n\\mathbb{E}[\\bar{f}_n] - \\pi(f) \\;=\\; \\frac{C(p,q)}{n} + o\\!\\left(\\frac{1}{n}\\right) \\quad \\text{as } n \\to \\infty.\n$$\n\nProvide your final answer as a single closed-form analytic expression for $C(p,q)$. No rounding is required and no physical units apply.",
            "solution": "The problem asks for the leading constant $C(p,q)$ in the asymptotic expansion of the initialization bias for a two-state Markov chain. The analysis will proceed in three steps as outlined in the problem: first, we derive the stationary distribution $\\pi$ and the steady-state performance $\\pi(f)$; second, we solve the Poisson equation for the potential function $h$; and third, we use this solution to find the asymptotic bias.\n\n**Step 1: Stationary Distribution and Steady-State Performance**\n\nLet the stationary distribution be denoted by the row vector $\\pi = (\\pi_0, \\pi_1)$, where $\\pi_x = \\pi(x)$ is the stationary probability of being in state $x \\in \\{0, 1\\}$. By definition, the stationary distribution satisfies the equation $\\pi P = \\pi$, along with the normalization condition $\\sum_{x \\in \\{0,1\\}} \\pi_x = 1$.\n\nThe equation $\\pi P = \\pi$ is:\n$$\n(\\pi_0, \\pi_1) \\begin{pmatrix} 1-p & p \\\\ q & 1-q \\end{pmatrix} = (\\pi_0, \\pi_1)\n$$\nThis gives a system of two linear equations:\n$$\n\\pi_0 (1-p) + \\pi_1 q = \\pi_0 \\\\\n\\pi_0 p + \\pi_1 (1-q) = \\pi_1\n$$\nFrom the first equation, we get $-\\pi_0 p + \\pi_1 q = 0$, which implies $\\pi_0 p = \\pi_1 q$. The second equation gives the same relationship. Since $p,q \\in (0,1)$, this implies a non-trivial relationship between $\\pi_0$ and $\\pi_1$.\n\nUsing the normalization condition $\\pi_0 + \\pi_1 = 1$, we can substitute $\\pi_1 = 1 - \\pi_0$ into the relation $\\pi_0 p = \\pi_1 q$:\n$$\n\\pi_0 p = (1-\\pi_0) q = q - \\pi_0 q\n$$\n$$\n\\pi_0 p + \\pi_0 q = q\n$$\n$$\n\\pi_0 (p+q) = q\n$$\nSince $p,q \\in (0,1)$, $p+q > 0$, so we can solve for $\\pi_0$:\n$$\n\\pi_0 = \\frac{q}{p+q}\n$$\nAnd for $\\pi_1$:\n$$\n\\pi_1 = 1 - \\pi_0 = 1 - \\frac{q}{p+q} = \\frac{p+q-q}{p+q} = \\frac{p}{p+q}\n$$\nSo the stationary distribution is $\\pi = \\left(\\frac{q}{p+q}, \\frac{p}{p+q}\\right)$.\n\nThe steady-state performance $\\pi(f)$ is the expected value of the function $f$ under the stationary distribution:\n$$\n\\pi(f) = \\sum_{x \\in \\{0,1\\}} \\pi(x) f(x) = \\pi_0 f(0) + \\pi_1 f(1)\n$$\nUsing the given values $f(0)=0$ and $f(1)=1$:\n$$\n\\pi(f) = \\left(\\frac{q}{p+q}\\right) \\cdot 0 + \\left(\\frac{p}{p+q}\\right) \\cdot 1 = \\frac{p}{p+q}\n$$\n\n**Step 2: Solving the Poisson Equation**\n\nThe Poisson equation is given as $h - P h = f - \\pi(f)$, where $h$ and $f$ are understood as column vectors indexed by the state space $\\{0,1\\}$. We write this as $(I-P)h = f - \\pi(f)\\mathbf{1}$, where $I$ is the identity matrix and $\\mathbf{1}$ is a column vector of ones.\n\nThe vectors are:\n$h = \\begin{pmatrix} h(0) \\\\ h(1) \\end{pmatrix}$, $f = \\begin{pmatrix} f(0) \\\\ f(1) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nThe right-hand side is:\n$$\nf - \\pi(f)\\mathbf{1} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} - \\frac{p}{p+q} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -\\frac{p}{p+q} \\\\ 1 - \\frac{p}{p+q} \\end{pmatrix} = \\begin{pmatrix} -\\frac{p}{p+q} \\\\ \\frac{q}{p+q} \\end{pmatrix}\n$$\nThe matrix $(I-P)$ is:\n$$\nI-P = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\begin{pmatrix} 1-p & p \\\\ q & 1-q \\end{pmatrix} = \\begin{pmatrix} p & -p \\\\ -q & q \\end{pmatrix}\n$$\nThe Poisson equation becomes:\n$$\n\\begin{pmatrix} p & -p \\\\ -q & q \\end{pmatrix} \\begin{pmatrix} h(0) \\\\ h(1) \\end{pmatrix} = \\begin{pmatrix} -\\frac{p}{p+q} \\\\ \\frac{q}{p+q} \\end{pmatrix}\n$$\nThis matrix equation yields two dependent linear equations:\n1. $p \\cdot h(0) - p \\cdot h(1) = -\\frac{p}{p+q}$\n2. $-q \\cdot h(0) + q \\cdot h(1) = \\frac{q}{p+q}$\n\nSince $p, q \\in (0,1)$, we can divide the first equation by $p$ and the second by $q$, both yielding the same relation:\n$$\nh(0) - h(1) = -\\frac{1}{p+q}\n$$\nThis system is underdetermined, as expected. We use the specified normalization condition $\\sum_{x \\in \\{0,1\\}} \\pi(x) h(x) = 0$:\n$$\n\\pi_0 h(0) + \\pi_1 h(1) = 0\n$$\n$$\n\\frac{q}{p+q} h(0) + \\frac{p}{p+q} h(1) = 0\n$$\n$$\nq h(0) + p h(1) = 0\n$$\nWe now solve the system of two independent linear equations for $h(0)$ and $h(1)$:\n(a) $h(0) - h(1) = -\\frac{1}{p+q}$\n(b) $q h(0) + p h(1) = 0$\n\nFrom (b), we express $h(0)$ in terms of $h(1)$: $h(0) = -\\frac{p}{q}h(1)$. Substituting this into (a):\n$$\n-\\frac{p}{q}h(1) - h(1) = -\\frac{1}{p+q}\n$$\n$$\n-h(1)\\left(\\frac{p}{q} + 1\\right) = -\\frac{1}{p+q}\n$$\n$$\n-h(1)\\left(\\frac{p+q}{q}\\right) = -\\frac{1}{p+q}\n$$\n$$\nh(1) = \\frac{q}{(p+q)^2}\n$$\nNow we find $h(0)$:\n$$\nh(0) = -\\frac{p}{q} h(1) = -\\frac{p}{q} \\left(\\frac{q}{(p+q)^2}\\right) = -\\frac{p}{(p+q)^2}\n$$\nThus, the solution to the Poisson equation with the given normalization is $h(0) = -\\frac{p}{(p+q)^2}$ and $h(1) = \\frac{q}{(p+q)^2}$.\n\n**Step 3: Derivation of the Initialization Bias Constant**\n\nLet $\\tilde{f}(x) = f(x) - \\pi(f)$. The Poisson equation is $h(x) - \\mathbb{E}[h(X_{k+1})|X_k=x] = \\tilde{f}(x)$.\nWe sum this relation over $k$ from $0$ to $n-1$:\n$$\n\\sum_{k=0}^{n-1} \\tilde{f}(X_k) = \\sum_{k=0}^{n-1} \\left( h(X_k) - \\mathbb{E}[h(X_{k+1})|X_k] \\right)\n$$\nTaking the expectation conditional on the initial state $X_0=x_0$:\n$$\n\\mathbb{E}_{x_0}\\left[\\sum_{k=0}^{n-1} \\tilde{f}(X_k)\\right] = \\sum_{k=0}^{n-1} \\left(\\mathbb{E}_{x_0}[h(X_k)] - \\mathbb{E}_{x_0}[\\mathbb{E}[h(X_{k+1})|X_k]]\\right)\n$$\nBy the law of total expectation, $\\mathbb{E}_{x_0}[\\mathbb{E}[h(X_{k+1})|X_k]] = \\mathbb{E}_{x_0}[h(X_{k+1})]$. The sum becomes a telescoping series:\n$$\n\\sum_{k=0}^{n-1} (\\mathbb{E}_{x_0}[h(X_k)] - \\mathbb{E}_{x_0}[h(X_{k+1})]) = \\mathbb{E}_{x_0}[h(X_0)] - \\mathbb{E}_{x_0}[h(X_n)]\n$$\nSince $X_0=x_0$ is fixed, $\\mathbb{E}_{x_0}[h(X_0)] = h(x_0)$. The left side of the equation is $\\mathbb{E}_{x_0}\\left[n \\bar{f}_n - n\\pi(f)\\right] = n(\\mathbb{E}_{x_0}[\\bar{f}_n] - \\pi(f))$. Therefore,\n$$\nn\\left(\\mathbb{E}_{x_0}[\\bar{f}_n] - \\pi(f)\\right) = h(x_0) - \\mathbb{E}_{x_0}[h(X_n)]\n$$\nDividing by $n$:\n$$\n\\mathbb{E}_{x_0}[\\bar{f}_n] - \\pi(f) = \\frac{h(x_0)}{n} - \\frac{\\mathbb{E}_{x_0}[h(X_n)]}{n}\n$$\nFor an irreducible and aperiodic Markov chain on a finite state space, the distribution of $X_n$ converges exponentially fast to the stationary distribution $\\pi$. Thus,\n$$\n\\mathbb{E}_{x_0}[h(X_n)] = \\sum_{y \\in \\{0,1\\}} \\mathbb{P}(X_n = y | X_0=x_0)h(y) \\to \\sum_{y \\in \\{0,1\\}} \\pi(y)h(y) \\quad \\text{as } n \\to \\infty\n$$\nBy our normalization of $h$, this limit is $\\sum_y \\pi(y)h(y) = 0$. The convergence is exponential, meaning $\\mathbb{E}_{x_0}[h(X_n)] = O(\\lambda^n)$ for some $|\\lambda|<1$. Therefore, the term $\\frac{1}{n}\\mathbb{E}_{x_0}[h(X_n)]$ decays faster than $\\frac{1}{n}$ and is of order $o(1/n)$.\nThe asymptotic expansion is:\n$$\n\\mathbb{E}_{x_0}[\\bar{f}_n] - \\pi(f) = \\frac{h(x_0)}{n} + o\\left(\\frac{1}{n}\\right)\n$$\nThe problem specifies the initial condition $X_0=0$. The constant $C(p,q)$ is therefore $h(0)$.\nUsing our result from Step 2:\n$$\nC(p,q) = h(0) = -\\frac{p}{(p+q)^2}\n$$\nThis is the final expression for the leading constant in the initialization bias.",
            "answer": "$$\\boxed{-\\frac{p}{(p+q)^{2}}}$$"
        },
        {
            "introduction": "After mitigating bias with warm-up deletion, a practitioner's next critical task is to quantify the uncertainty of the steady-state estimate. This is challenging because the remaining data points are still correlated. This exercise guides you through the non-overlapping batch means method, a standard and robust technique for constructing a valid confidence interval from a dependent time series . It forces you to critically evaluate the assumptions that justify using a Student's $t$-distribution, a crucial detail for producing reliable finite-sample results.",
            "id": "3347878",
            "problem": "Consider a Harris ergodic and aperiodic Markov chain $\\{X_t\\}_{t \\ge 0}$ on a general state space with invariant distribution $\\pi$. Let $f$ be a measurable function such that $E_{\\pi}[|f(X)|] < \\infty$ and define the steady-state mean $\\pi f = \\int f(x)\\,\\pi(dx)$. Suppose the chain is initialized out of stationarity at $X_0 \\sim \\mu \\neq \\pi$ and is simulated for $n$ steps, producing the time series $Y_t = f(X_t)$ for $t = 1, \\dots, n$. To mitigate initialization bias, you discard a warm-up of $w$ observations and retain $n' = n - w$ observations $\\{Y_{w+1}, \\dots, Y_{n}\\}$. Denote the post-warm-up average by $\\bar{f}_{n'} = \\frac{1}{n'} \\sum_{t=w+1}^{n} Y_t$. Under standard strong-mixing conditions, the Central Limit Theorem (CLT) for stationary sequences asserts that $\\sqrt{n'}(\\bar{f}_{n'} - \\pi f) \\Rightarrow \\mathcal{N}(0, \\tau^2)$, where the time-average variance constant $\\tau^2$ is finite and equals the spectral density at frequency zero or, equivalently, the sum of covariances $\\tau^2 = \\sum_{h=-\\infty}^{\\infty} \\operatorname{Cov}_{\\pi}(Y_0, Y_h)$ when the series is absolutely summable.\n\nTo estimate $\\tau^2$, you apply the non-overlapping batch means method: choose a positive integer batch size $k$ such that $n' = b k$ for an integer $b \\ge 2$, form batch means $M_j = \\frac{1}{k} \\sum_{t=(j-1)k + w + 1}^{j k + w} Y_t$ for $j = 1, \\dots, b$, and compute their empirical average $\\bar{M} = \\frac{1}{b} \\sum_{j=1}^{b} M_j$ and sample variance $s_b^2 = \\frac{1}{b-1} \\sum_{j=1}^{b} (M_j - \\bar{M})^2$. Under standard conditions ensuring weak dependence and sufficiently large $k$, the batch-means estimator $\\hat{\\tau}_{\\mathrm{BM}}^2 = k s_b^2$ is consistent for $\\tau^2$, the batch means $M_j$ are approximately independent and approximately normal, and $\\bar{M} = \\bar{f}_{n'}$ by construction.\n\nWhich option below correctly constructs a $(1-\\alpha)$ confidence interval for $\\pi f$ based on $\\bar{f}_{n'}$ and the batch-means estimator of $\\tau^2$, and provides the most principled justification for using either Student’s $t$-distribution or standard normal quantiles in finite samples?\n\nA. Use the full series without warm-up deletion and invoke the CLT directly: the interval is $\\bar{f}_{n} \\pm z_{1-\\alpha/2}\\,\\frac{\\hat{\\tau}_{\\mathrm{BM}}}{\\sqrt{n}}$, where $z_{1-\\alpha/2}$ is the $(1-\\alpha/2)$ standard normal quantile and $\\hat{\\tau}_{\\mathrm{BM}}^2 = k s_b^2$ is computed from $b$ batches over all $n$ observations. Justification: the CLT provides asymptotic normality, so normal quantiles are appropriate even in finite samples.\n\nB. Delete a warm-up of $w$ observations and partition the remaining $n' = b k$ observations into $b$ non-overlapping batches of size $k$. Form $\\hat{\\tau}_{\\mathrm{BM}}^2 = k s_b^2$ from the $b$ batch means and use the interval $\\bar{f}_{n'} \\pm t_{1-\\alpha/2,\\, b-1}\\,\\frac{\\hat{\\tau}_{\\mathrm{BM}}}{\\sqrt{n'}}$, where $t_{1-\\alpha/2,\\, b-1}$ is the $(1-\\alpha/2)$ quantile of Student’s $t$-distribution with $b-1$ degrees of freedom. Justification: after warm-up deletion, the batches are approximately independent and approximately normal; since $\\tau^2$ is estimated from $b$ batch means, using Studentization with $b-1$ degrees of freedom yields a finite-sample correction relative to the normal limit, becoming asymptotically equivalent to normal quantiles as $b \\to \\infty$.\n\nC. After warm-up deletion, treat the raw post-warm-up observations as independent and use the usual $t$-interval with $n'-1$ degrees of freedom: the interval is $\\bar{f}_{n'} \\pm t_{1-\\alpha/2,\\, n'-1}\\,\\frac{s_{n'}}{\\sqrt{n'}}$, where $s_{n'}^2 = \\frac{1}{n'-1} \\sum_{t=w+1}^{n} (Y_t - \\bar{f}_{n'})^2$. Justification: estimating the variance from all $n'$ dependent observations induces a Student’s $t$ pivot with $n'-1$ degrees of freedom.\n\nD. Delete warm-up and use the asymptotic CLT with the consistent batch-means estimator, but retain normal quantiles: the interval is $\\bar{f}_{n'} \\pm z_{1-\\alpha/2}\\,\\frac{\\hat{\\tau}_{\\mathrm{BM}}}{\\sqrt{n'}}$ with $\\hat{\\tau}_{\\mathrm{BM}}^2 = k s_b^2$. Justification: the CLT implies asymptotic normality regardless of variance estimation, so normal quantiles remain valid in finite samples; using Student’s $t$ is unnecessary without exact normality and independence.\n\nE. Delete warm-up and use Student’s $t$ with $b-1$ degrees of freedom, but estimate $\\tau^2$ by $\\hat{\\tau}_{\\mathrm{BM}}^2 = \\frac{s_b^2}{k}$. Justification: the batch means are approximately independent, so dividing by the batch size $k$ corrects for inflation in $s_b^2$, and Student’s $t$ accounts for finite-sample uncertainty.",
            "solution": "We begin from the fundamental definitions and well-tested results. The steady-state quantity of interest is $\\pi f = E_{\\pi}[f(X)]$. For steady-state simulation of a dependent sequence $\\{Y_t\\}$ derived from a Harris ergodic Markov chain, standard strong-mixing conditions and the Central Limit Theorem (CLT) for stationary sequences yield\n$$\n\\sqrt{n'}\\left(\\bar{f}_{n'} - \\pi f\\right) \\Rightarrow \\mathcal{N}(0, \\tau^2),\n$$\nwhere $\\bar{f}_{n'} = \\frac{1}{n'} \\sum_{t=w+1}^{n} Y_t$, $n' = n - w$ reflects warm-up deletion, and $\\tau^2$ equals the spectral density at zero (equivalently $\\tau^2 = \\sum_{h=-\\infty}^{\\infty}\\operatorname{Cov}_{\\pi}(Y_0,Y_h)$ when the auto-covariances are absolutely summable). The CLT is the fundamental base: it governs the asymptotic behavior of the sample mean and motivates confidence interval construction.\n\nInitialization bias arises because $X_0 \\sim \\mu \\neq \\pi$. Under geometric ergodicity, $E_{\\mu}[f(X_t)] \\to E_{\\pi}[f(X)]$ at a geometric rate, and the bias of $\\bar{f}_{n}$ decays on the order of $O(1/n)$; however, in finite samples, especially moderate $n$, this bias can be non-negligible. Therefore, warm-up deletion of $w$ observations is a principled approach to ensure the retained sequence is approximately stationary, thereby aligning the finite-sample distribution of $\\bar{f}_{n'}$ more closely with the CLT’s stationary premise.\n\nTo estimate the time-average variance constant $\\tau^2$, the non-overlapping batch means method partitions the post-warm-up sequence into $b$ contiguous batches of size $k$ with $n' = b k$. For batch $j$, define\n$$\nM_j = \\frac{1}{k} \\sum_{t=(j-1)k+w+1}^{jk+w} Y_t, \\quad j = 1, \\dots, b.\n$$\nLet $\\bar{M} = \\frac{1}{b} \\sum_{j=1}^{b} M_j$ and\n$$\ns_b^2 = \\frac{1}{b-1}\\sum_{j=1}^{b} \\left(M_j - \\bar{M}\\right)^2.\n$$\nUnder mixing and for sufficiently large $k$, each $M_j$ is approximately normal with\n$$\n\\operatorname{Var}(M_j) \\approx \\frac{\\tau^2}{k}.\n$$\nConsequently, $s_b^2$ estimates $\\operatorname{Var}(M_j)$, and the batch-means estimator\n$$\n\\hat{\\tau}_{\\mathrm{BM}}^2 = k\\, s_b^2\n$$\nis consistent for $\\tau^2$. Moreover, $\\bar{M} = \\bar{f}_{n'}$ by construction.\n\nA confidence interval for $\\pi f$ follows from the CLT and a consistent estimator of $\\tau^2$. As $n' \\to \\infty$ (equivalently $b \\to \\infty$ with $k \\to \\infty$), the standardized statistic\n$$\nZ_{n'} = \\frac{\\bar{f}_{n'} - \\pi f}{\\hat{\\tau}_{\\mathrm{BM}}/\\sqrt{n'}}\n$$\nconverges in distribution to $\\mathcal{N}(0,1)$, yielding the asymptotically valid interval\n$$\n\\bar{f}_{n'} \\pm z_{1-\\alpha/2}\\,\\frac{\\hat{\\tau}_{\\mathrm{BM}}}{\\sqrt{n'}}.\n$$\nHowever, in finite samples, there is additional uncertainty introduced by estimating $\\tau^2$ from a finite number $b$ of batch means. When the $M_j$ are approximately independent and approximately normal (a premise strengthened by larger $k$ and adequate mixing), Studentization with the sample variance $s_b^2$ across $b$ nearly independent normals leads to an approximate Student’s $t$ distribution with $b-1$ degrees of freedom:\n$$\nT_{b} = \\frac{\\bar{M} - \\pi f}{\\sqrt{s_b^2/b}} \\approx t_{b-1}.\n$$\nEquivalently,\n$$\n\\frac{\\bar{f}_{n'} - \\pi f}{\\hat{\\tau}_{\\mathrm{BM}}/\\sqrt{n'}} = \\frac{\\bar{M} - \\pi f}{\\sqrt{s_b^2/b}} \\approx t_{b-1}.\n$$\nThis finite-sample correction inflates the quantiles relative to $z_{1-\\alpha/2}$ when $b$ is not large, providing more reliable coverage by accounting for the variability in the variance estimator. As $b \\to \\infty$, $t_{b-1} \\to z$, recovering the asymptotic normal interval.\n\nWe now analyze each option:\n\nA. The proposed interval is\n$$\n\\bar{f}_{n} \\pm z_{1-\\alpha/2}\\,\\frac{\\hat{\\tau}_{\\mathrm{BM}}}{\\sqrt{n}}.\n$$\nThis ignores initialization bias by using all $n$ observations without warm-up deletion, violating the stationary premise needed for the CLT to be applicable in finite samples. Furthermore, it relies on standard normal quantiles without addressing the finite-sample uncertainty arising from estimating $\\tau^2$ from a finite number of batches. In small to moderate $b$, this can under-cover. Verdict: Incorrect.\n\nB. The proposed interval is\n$$\n\\bar{f}_{n'} \\pm t_{1-\\alpha/2,\\, b-1}\\,\\frac{\\hat{\\tau}_{\\mathrm{BM}}}{\\sqrt{n'}}, \\quad \\hat{\\tau}_{\\mathrm{BM}}^2 = k s_b^2,\n$$\nwith $n' = b k$ post-warm-up observations. This construction accounts for initialization bias via warm-up deletion, uses a consistent estimator $\\hat{\\tau}_{\\mathrm{BM}}^2$ derived from the variance of batch means, and employs Student’s $t$ quantiles with $b-1$ degrees of freedom to address finite-sample uncertainty due to estimating the variance from $b$ approximately independent and approximately normal batch means. As $b$ grows, the $t$ quantiles approach normal quantiles, ensuring asymptotic validity. Verdict: Correct.\n\nC. The proposed interval is\n$$\n\\bar{f}_{n'} \\pm t_{1-\\alpha/2,\\, n'-1}\\,\\frac{s_{n'}}{\\sqrt{n'}}, \\quad s_{n'}^2 = \\frac{1}{n'-1}\\sum_{t=w+1}^{n} \\left(Y_t - \\bar{f}_{n'}\\right)^2.\n$$\nThis treats the autocorrelated sequence $\\{Y_t\\}$ as independent and bases variance estimation on the sample variance of the raw dependent data, which estimates $\\operatorname{Var}(Y_t)$, not the time-average variance constant $\\tau^2$ governing the CLT for $\\bar{f}_{n'}$. The exact $t$-pivot with $n'-1$ degrees of freedom requires independent and normally distributed observations, which does not hold here. Verdict: Incorrect.\n\nD. The proposed interval is\n$$\n\\bar{f}_{n'} \\pm z_{1-\\alpha/2}\\,\\frac{\\hat{\\tau}_{\\mathrm{BM}}}{\\sqrt{n'}}, \\quad \\hat{\\tau}_{\\mathrm{BM}}^2 = k s_b^2.\n$$\nThis deletes warm-up and uses the correct batch-means estimator, yielding an asymptotically valid interval by the CLT. However, the justification explicitly claims normal quantiles are valid in finite samples without accounting for the additional uncertainty in estimating $\\tau^2$ from $b$ batch means. In practice, with moderate $b$, using $t_{b-1}$ is more principled and yields better finite-sample coverage. The prompt specifically asks to justify the choice between Student’s $t$ and normal quantiles in finite samples; this option fails to provide the finite-sample correction. Verdict: Incorrect for the stated objective.\n\nE. The proposed interval uses $t$ quantiles but sets\n$$\n\\hat{\\tau}_{\\mathrm{BM}}^2 = \\frac{s_b^2}{k},\n$$\nso the interval is\n$$\n\\bar{f}_{n'} \\pm t_{1-\\alpha/2,\\, b-1}\\,\\frac{\\hat{\\tau}_{\\mathrm{BM}}}{\\sqrt{n'}} = \\bar{f}_{n'} \\pm t_{1-\\alpha/2,\\, b-1}\\,\\frac{\\sqrt{s_b^2/k}}{\\sqrt{n'}}.\n$$\nThis inverts the correct scaling: since $\\operatorname{Var}(M_j) \\approx \\tau^2/k$, we have $s_b^2 \\approx \\tau^2/k$, so the consistent estimator is $\\hat{\\tau}_{\\mathrm{BM}}^2 = k s_b^2$, not $s_b^2/k$. The proposed scaling underestimates the variance and leads to undercoverage. Verdict: Incorrect.\n\nIn summary, the principled finite-sample construction is to delete an appropriate warm-up, estimate $\\tau^2$ via batch means with $\\hat{\\tau}_{\\mathrm{BM}}^2 = k s_b^2$, and use Student’s $t$ quantiles with $b-1$ degrees of freedom:\n$$\n\\text{CI}_{1-\\alpha}: \\quad \\bar{f}_{n'} \\pm t_{1-\\alpha/2,\\, b-1}\\,\\frac{\\hat{\\tau}_{\\mathrm{BM}}}{\\sqrt{n'}}.\n$$\nThis yields conservative coverage for moderate $b$ and recovers the asymptotic normal interval as $b \\to \\infty$.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "While truncation methods are common, the regenerative method offers a more elegant and powerful framework for steady-state analysis by leveraging a process's underlying cyclic structure. This practice uses a simplified model with deterministic cycle lengths to provide a crystal-clear illustration of how arbitrary deletion can introduce bias by cutting through cycles . By calculating this bias and then proposing a correction that aligns with cycle boundaries, you will grasp the fundamental principle that makes regenerative simulation a uniquely powerful tool for obtaining unbiased estimators.",
            "id": "3347886",
            "problem": "Consider a discrete-time regenerative process with regeneration (cycle start) epochs at times $S_{1} = 1$, $S_{k} = 1 + \\sum_{i=1}^{k-1} L_{i}$ for $k \\geq 2$, where $\\{L_{i}\\}$ are independent and identically distributed (IID) cycle lengths. Within each cycle, the observed process $\\{X_{t}\\}$ takes the value $X_{t} = x_{0}$ at the first observation of the cycle (i.e., when $t = S_{k}$), and $X_{t} = x_{1}$ at all subsequent observations of that same cycle. A replication of length $T$ begins at the regeneration epoch $t = 1$.\n\nThe steady-state time-average mean of $\\{X_{t}\\}$, denoted by $\\mu$, is defined by the renewal-reward framework: over a cycle of length $L$, the cycle reward is $R = x_{0} + (L - 1)x_{1}$, and under standard regularity conditions for regenerative processes, $\\mu$ equals the ratio of expectations $\\mu = \\mathbb{E}[R] / \\mathbb{E}[L]$.\n\nMany practitioners delete a fixed number $m$ of initial observations to mitigate initialization bias. Define the delete-$m$ sample mean\n$$\n\\hat{\\mu}_{m} \\triangleq \\frac{1}{T - m} \\sum_{t = m + 1}^{T} X_{t}.\n$$\nIf the initial deletion cuts through a regeneration cycle (i.e., $m$ is not aligned with a cycle boundary), the composition of the retained window includes a tail of the first cycle, which can introduce bias relative to $\\mu$.\n\nWork with the specific, scientifically realistic scenario:\n- Deterministic cycle length $L \\equiv 5$ (so cycles start at times $1, 6, 11, 16, \\dots$).\n- Within-cycle values $x_{0} = 3$ and $x_{1} = 1$.\n- Replication length $T = 103$ and deletion size $m = 12$.\n\nTasks:\n1. Starting from the renewal-reward definition of the steady-state mean, derive $\\mu$ in terms of $x_{0}$, $x_{1}$, and $L$.\n2. Express $\\hat{\\mu}_{m}$ in terms of the number of cycle starts within $\\{m + 1, \\dots, T\\}$. Then compute $\\mathbb{E}[\\hat{\\mu}_{m}]$ by exactly counting the cycle starts in the given interval for the deterministic $L = 5$ case, and derive the bias $\\mathbb{E}[\\hat{\\mu}_{m}] - \\mu$.\n3. Propose a correction that avoids cutting through cycles by deleting whole cycles: specifically, delete up to the next regeneration epoch after $m$ and truncate the end to the last complete cycle before $T$, and explain why this removes the bias in this deterministic setting.\n\nAnswer specification:\n- Provide the bias $\\mathbb{E}[\\hat{\\mu}_{m}] - \\mu$ from Task 2 as a single reduced fraction. No units are required.",
            "solution": "The problem as stated is scientifically sound, self-contained, and well-posed. All parameters and definitions are provided, and they conform to the standard framework of regenerative processes in stochastic simulation. The use of a deterministic cycle length is a valid simplification for pedagogical purposes, allowing for an exact analysis of initialization bias. We may therefore proceed with the solution.\n\nThe problem asks for three tasks related to a discrete-time regenerative process. We will address them in order.\n\n**Task 1: Derivation of the steady-state mean $\\mu$**\n\nThe steady-state time-average mean, $\\mu$, of a regenerative process $\\{X_t\\}$ is defined by the ratio of the expected reward per cycle, $\\mathbb{E}[R]$, to the expected length of a cycle, $\\mathbb{E}[L]$.\n$$\n\\mu = \\frac{\\mathbb{E}[R]}{\\mathbb{E}[L]}\n$$\nThe problem defines the cycle reward as $R = x_{0} + (L - 1)x_{1}$, where $L$ is the cycle length.\nIn the specific scenario given, the cycle length is deterministic, $L \\equiv 5$. For a deterministic quantity, the expectation is the quantity itself.\nTherefore, the expected cycle length is $\\mathbb{E}[L] = 5$.\nSimilarly, the cycle reward $R$ becomes a deterministic value:\n$$\nR = x_{0} + (L - 1)x_{1}\n$$\nThe expected reward is thus:\n$$\n\\mathbb{E}[R] = x_{0} + (L - 1)x_{1}\n$$\nSubstituting these into the definition of $\\mu$, we obtain the general expression for this process structure:\n$$\n\\mu = \\frac{x_{0} + (L - 1)x_{1}}{L}\n$$\nNow, we substitute the specified numerical values: $x_{0} = 3$, $x_{1} = 1$, and $L = 5$.\n$$\n\\mu = \\frac{3 + (5 - 1) \\times 1}{5} = \\frac{3 + 4}{5} = \\frac{7}{5}\n$$\n\n**Task 2: Calculation of the bias $\\mathbb{E}[\\hat{\\mu}_{m}] - \\mu$**\n\nThe delete-$m$ sample mean is defined as:\n$$\n\\hat{\\mu}_{m} = \\frac{1}{T - m} \\sum_{t = m + 1}^{T} X_{t}\n$$\nWe are given the parameters $T = 103$ and $m = 12$. The number of observations in the sample mean is $T - m = 103 - 12 = 91$. The summation runs from $t = 13$ to $t = 103$.\n$$\n\\hat{\\mu}_{12} = \\frac{1}{91} \\sum_{t = 13}^{103} X_{t}\n$$\nSince the process is entirely deterministic, the expected value of the estimator is the estimator itself: $\\mathbb{E}[\\hat{\\mu}_{12}] = \\hat{\\mu}_{12}$. To compute this value, we must determine the sequence of values $X_t$ and sum them over the specified interval.\n\nThe regeneration epochs are $S_{k} = 1 + 5(k-1)$, for $k = 1, 2, 3, \\dots$.\nThe process values are $X_{S_k} = x_0 = 3$ and $X_t = x_1 = 1$ for other times $t$ within the cycle starting at $S_k$.\nLet's list the first few cycles:\n- Cycle 1: $S_1=1$. Runs from $t=1$ to $t=5$. $X_1=3$, $X_{2,3,4,5}=1$.\n- Cycle 2: $S_2=6$. Runs from $t=6$ to $t=10$. $X_6=3$, $X_{7,8,9,10}=1$.\n- Cycle 3: $S_3=11$. Runs from $t=11$ to $t=15$. $X_{11}=3$, $X_{12,13,14,15}=1$.\n\nThe summation window is from $t = 13$ to $t = 103$. We can partition the sum into three parts: the tail of the cycle active at $t=13$, the sum over complete cycles within the window, and the head of the cycle active at $t=103$.\n\n1.  **Tail of the first partial cycle:** The summation starts at $t=13$. This falls into Cycle 3, which started at $S_3=11$. For this cycle, $X_{11}=3$ and $X_{12}=X_{13}=X_{14}=X_{15}=1$. The relevant part of the sum is for $t=13, 14, 15$.\n    The contribution is $X_{13} + X_{14} + X_{15} = 1 + 1 + 1 = 3$.\n\n2.  **Sum over complete cycles:** The first full cycle within the window is Cycle 4, which starts at $S_4 = 1 + 5(3) = 16$. The last full cycle must end at or before $T=103$. A cycle starting at $S_k$ ends at $S_k+L-1 = S_k+4$. We need $S_k+4 \\le 103$, which means $1+5(k-1)+4 \\le 103 \\implies 5(k-1) \\le 98 \\implies k-1 \\le 19.6 \\implies k \\le 20.6$. The last such integer is $k=20$.\n    So, the complete cycles are from $k=4$ to $k=20$. The number of complete cycles is $20 - 4 + 1 = 17$.\n    The sum of observations in one complete cycle is $x_0 + (L-1)x_1 = 3 + (5-1)(1) = 7$.\n    The total contribution from these $17$ cycles is $17 \\times 7 = 119$.\n\n3.  **Head of the last partial cycle:** The observation window ends at $T=103$. The cycles after $k=20$ are:\n    - Cycle 21: $S_{21} = 1 + 5(20) = 101$. This cycle runs from $t=101$ to $t=105$.\n    The summation window includes observations at $t=101, 102, 103$.\n    The values are $X_{101} = x_0 = 3$ (since $t=101$ is a regeneration epoch) and $X_{102} = X_{103} = x_1 = 1$.\n    The contribution from this partial cycle is $3 + 1 + 1 = 5$.\n\nThe total sum is the sum of these three parts:\n$$\n\\sum_{t=13}^{103} X_{t} = 3 + 119 + 5 = 127\n$$\nNow we can compute $\\mathbb{E}[\\hat{\\mu}_{12}]$:\n$$\n\\mathbb{E}[\\hat{\\mu}_{12}] = \\hat{\\mu}_{12} = \\frac{127}{91}\n$$\nThe bias is the difference between this value and the true mean $\\mu$:\n$$\n\\text{Bias} = \\mathbb{E}[\\hat{\\mu}_{12}] - \\mu = \\frac{127}{91} - \\frac{7}{5}\n$$\nTo subtract, we find a common denominator, which is $91 \\times 5 = 455$:\n$$\n\\text{Bias} = \\frac{127 \\times 5}{91 \\times 5} - \\frac{7 \\times 91}{5 \\times 91} = \\frac{635}{455} - \\frac{637}{455} = \\frac{635 - 637}{455} = -\\frac{2}{455}\n$$\nThe fraction is irreducible since $2$ is prime and $455$ is odd.\n\n**Task 3: Proposed correction and explanation**\n\nThe bias calculated above arises from the fact that the simple truncation rule creates partial cycles at the start and end of the observation window. The composition of these partial cycles is not representative of a full cycle's average. The initial partial cycle is a \"tail\" (missing its high-value start), and the final partial cycle is a \"head\".\n\nA superior method, as suggested, is to align the observation window with cycle boundaries. This entails:\n1.  Deleting data until the start of the next complete cycle after the initial warmup period $m$.\n2.  Truncating the data collection at the end of the last complete cycle before the replication ends at $T$.\n\nGiven $m=12$, the current time is within Cycle 3 ($t \\in [11, 15]$). The next regeneration epoch is $S_4=16$. So, the corrected observation window should begin at $t=16$.\nThe replication ends at $T=103$. The last complete cycle before this time is Cycle 20, which runs from $t=96$ to $t=100$. So, the corrected observation window should end at $t=100$.\n\nThe corrected sample mean, $\\hat{\\mu}_{corr}$, is calculated over the interval $[16, 100]$. This interval contains exactly the $17$ full cycles from Cycle 4 to Cycle 20.\nThe total number of observations is $17 \\times L = 17 \\times 5 = 85$.\nThe total sum of values is $17 \\times (x_0 + (L-1)x_1) = 17 \\times 7 = 119$.\nThe corrected mean is:\n$$\n\\hat{\\mu}_{corr} = \\frac{119}{85} = \\frac{17 \\times 7}{17 \\times 5} = \\frac{7}{5}\n$$\nWe can see that $\\hat{\\mu}_{corr} = \\mu$. The bias is zero.\n\nThis method removes bias in the deterministic setting because the estimator becomes an average over an integer number of identical, complete cycles. Each cycle contributes an average value of $\\frac{x_0+(L-1)x_1}{L} = \\mu$. Averaging these identical components naturally yields $\\mu$. This approach effectively eliminates the \"end effects\" caused by partial cycles, which were the source of the bias in $\\hat{\\mu}_m$. In the more general stochastic case, this forms the basis of the regenerative method, which provides consistent (though not strictly unbiased for finite samples) estimators of steady-state performance measures.",
            "answer": "$$\n\\boxed{-\\frac{2}{455}}\n$$"
        }
    ]
}