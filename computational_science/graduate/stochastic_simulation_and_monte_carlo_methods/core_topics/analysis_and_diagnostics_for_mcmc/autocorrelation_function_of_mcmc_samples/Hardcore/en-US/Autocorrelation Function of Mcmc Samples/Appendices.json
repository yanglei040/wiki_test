{
    "hands_on_practices": [
        {
            "introduction": "A fundamental challenge in MCMC is that samples are correlated, rendering the standard $\\sigma/\\sqrt{n}$ formula for standard error invalid. The method of non-overlapping batch means provides a simple yet powerful way to address this by grouping consecutive samples into batches and treating the means of these batches as approximately independent observations. This exercise  gives you direct practice in applying this technique and thinking critically about its limitations, such as the bias introduced by finite batch sizes.",
            "id": "3289752",
            "problem": "Consider a time-homogeneous, ergodic Markov chain $\\{X_t\\}_{t=1}^{n}$ targeting a stationary distribution, and a measurable function $f$ with finite second moment under stationarity. Let $Y_t = f(X_t)$ denote the correlated output. Assume the initial transient has been removed so that $\\{Y_t\\}$ is observed in stationarity. You are given $n = 12$ correlated observations $y_t$ defined by $y_{1:12} = (1.2,\\,0.8,\\,0.9,\\,1.1,\\,1.4,\\,1.5,\\,1.3,\\,1.2,\\,0.7,\\,0.6,\\,0.8,\\,1.0)$. Using non-overlapping batch means with batch size $b = 4$ and number of batches $m = 3$ (so $n = m b$), compute the Monte Carlo standard error estimate $\\widehat{\\mathrm{SE}}(\\bar{f}_n) = \\sqrt{\\hat{\\sigma}_f^2/n}$ from batch means, where $\\hat{\\sigma}_f^2$ is the batch-means estimator of the time-average variance constant of the Markov chain central limit theorem. Round your final numerical answer to four significant figures. In addition, explain qualitatively, using the autocorrelation function and the batch-means construction, the direction of finite-sample bias of $\\hat{\\sigma}_f^2$ as a function of $b$ and $m$ when autocorrelations are positive and decay slowly.",
            "solution": "The solution involves two parts: a numerical calculation of the standard error using batch means, and a qualitative explanation of the estimator's bias.\n\n**1. Numerical Calculation**\n\nFirst, we partition the $n=12$ observations into $m=3$ non-overlapping batches of size $b=4$.\n\n*   **Batch 1:** $(1.2, 0.8, 0.9, 1.1)$\n*   **Batch 2:** $(1.4, 1.5, 1.3, 1.2)$\n*   **Batch 3:** $(0.7, 0.6, 0.8, 1.0)$\n\nNext, we compute the mean of each batch:\n\n*   $\\bar{Y}_1 = \\frac{1.2+0.8+0.9+1.1}{4} = \\frac{4.0}{4} = 1.0$\n*   $\\bar{Y}_2 = \\frac{1.4+1.5+1.3+1.2}{4} = \\frac{5.4}{4} = 1.35$\n*   $\\bar{Y}_3 = \\frac{0.7+0.6+0.8+1.0}{4} = \\frac{3.1}{4} = 0.775$\n\nThe batch means estimator for the long-run variance, $\\hat{\\sigma}_f^2$, is given by the batch size multiplied by the sample variance of the batch means. First, we find the grand mean of the batch means, $\\bar{Y}$:\n$$\n\\bar{Y} = \\frac{\\bar{Y}_1 + \\bar{Y}_2 + \\bar{Y}_3}{3} = \\frac{1.0 + 1.35 + 0.775}{3} = \\frac{3.125}{3} \\approx 1.04167\n$$\nNow, we compute the sample variance of the batch means, $S^2_{\\bar{Y}}$:\n$$\nS^2_{\\bar{Y}} = \\frac{1}{m-1} \\sum_{j=1}^{m} (\\bar{Y}_j - \\bar{Y})^2 = \\frac{1}{2} \\left[ (1.0 - \\frac{3.125}{3})^2 + (1.35 - \\frac{3.125}{3})^2 + (0.775 - \\frac{3.125}{3})^2 \\right] \\approx 0.0839583\n$$\nThe estimator for the long-run variance is then:\n$$\n\\hat{\\sigma}_f^2 = b \\cdot S^2_{\\bar{Y}} = 4 \\times 0.0839583 \\approx 0.335833\n$$\nFinally, the standard error of the overall mean is:\n$$\n\\widehat{\\mathrm{SE}}(\\bar{f}_n) = \\sqrt{\\frac{\\hat{\\sigma}_f^2}{n}} = \\sqrt{\\frac{0.335833}{12}} \\approx \\sqrt{0.0279861} \\approx 0.16729\n$$\nRounding to four significant figures, the standard error is 0.1673.\n\n**2. Qualitative Explanation of Bias**\n\nThe batch-means estimator $\\hat{\\sigma}_f^2$ relies on the assumption that the batch means $\\bar{Y}_j$ are approximately independent. This assumption is valid only if the batch size $b$ is significantly larger than the integrated autocorrelation time ($\\tau_{\\mathrm{int}}$) of the process.\n\nWhen MCMC samples are positively correlated and the batch size $b$ is too small (i.e., not much larger than $\\tau_{\\mathrm{int}}$), the last observations in one batch are still correlated with the first observations in the next. This induces a positive correlation between consecutive batch means, $\\mathrm{Cov}(\\bar{Y}_j, \\bar{Y}_{j+1}) > 0$.\n\nThe standard sample variance formula for $S^2_{\\bar{Y}}$ treats the batch means as independent and identically distributed. For positively correlated data, this formula is known to underestimate the true variance. Because the batch means are positively correlated, their sample variance $S^2_{\\bar{Y}}$ will be a downwardly biased estimator of the variance that would be observed if the means came from truly independent blocks.\n\nSince $\\hat{\\sigma}_f^2$ is directly proportional to this underestimated sample variance, it will also be a negatively biased estimator of the true long-run variance $\\sigma_f^2$. In conclusion, for a process with positive and slowly decaying autocorrelations, choosing a batch size $b$ that is too small leads to a systematic **underestimation** of the true variance and the Monte Carlo standard error.",
            "answer": "$$\n\\boxed{0.1673}\n$$"
        },
        {
            "introduction": "To assess the efficiency of an MCMC sampler, we often compute the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, which measures the number of correlated samples equivalent to one independent sample. This computational exercise  demystifies this concept by having you calculate $\\tau_{\\mathrm{int}}$ for the well-understood first-order autoregressive model. You will use a standard Bartlett window to see how theoretical autocorrelations are combined in practice to produce a stable estimate of sampler performance.",
            "id": "3289769",
            "problem": "Consider a stationary first-order autoregressive Markov chain, which is a standard model for time series generated by Markov Chain Monte Carlo (MCMC) methods. Let the chain be given by the recursion $X_t = \\phi X_{t-1} + \\varepsilon_t$ with $|\\phi|  1$, where $\\{\\varepsilon_t\\}$ is a sequence of independent, identically distributed, zero-mean random variables with finite variance. Suppose we observe a realization $\\{X_t\\}_{t=1}^n$.\n\nDefine the sample mean $\\bar{X} = \\frac{1}{n} \\sum_{t=1}^n X_t$, the sample autocovariance at lag $k$,\n$$\n\\hat{\\gamma}_k = \\frac{1}{n} \\sum_{t=1}^{n-k} (X_t - \\bar{X})(X_{t+k} - \\bar{X}),\n$$\nand the sample autocorrelation function at lag $k$,\n$$\n\\hat{\\rho}_k = \\frac{\\hat{\\gamma}_k}{\\hat{\\gamma}_0}.\n$$\nThe integrated autocorrelation time is defined by\n$$\n\\tau_{\\mathrm{int}} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_k,\n$$\nwhere $\\rho_k$ is the true autocorrelation at lag $k$. A practical estimator uses a window to truncate and taper the sum. In particular, the Bartlett window with cutoff $m$ uses weights\n$$\nw_k = 1 - \\frac{k}{m+1}, \\quad k = 1,2,\\dots,m,\n$$\nand the windowed estimator is\n$$\n\\hat{\\tau}_{\\mathrm{int}}^{(B)} = 1 + 2 \\sum_{k=1}^{m} w_k \\hat{\\rho}_k.\n$$\n\nWorking to leading order in $1/n$ (i.e., ignoring terms of order $1/n$ and smaller), treat $\\hat{\\rho}_k$ as having expectation approximately equal to the true autocorrelation $\\rho_k$. For a first-order autoregression, the true autocorrelation satisfies $\\rho_k = \\phi^k$.\n\nYour tasks:\n- For each specified parameter set $(n,\\phi,m)$ in the test suite, compute the leading-order behavior of $\\hat{\\rho}_k$ for all integer lags $k$ with $1 \\le k \\le 100$, namely $\\hat{\\rho}_k^{(\\mathrm{lead})} \\approx \\phi^k$.\n- Using the Bartlett window with cutoff $m$, compute the corresponding leading-order windowed estimate of the integrated autocorrelation time,\n$$\n\\hat{\\tau}_{\\mathrm{int}}^{(B,\\mathrm{lead})} = 1 + 2 \\sum_{k=1}^{m} \\left(1 - \\frac{k}{m+1}\\right) \\phi^k.\n$$\n\nReturn, for each test case, a compact summary consisting of the list\n$$\n\\left[ \\hat{\\tau}_{\\mathrm{int}}^{(B,\\mathrm{lead})},\\ \\hat{\\rho}_1^{(\\mathrm{lead})},\\ \\hat{\\rho}_{50}^{(\\mathrm{lead})},\\ \\hat{\\rho}_{100}^{(\\mathrm{lead})} \\right],\n$$\nwith all floating-point values rounded to six decimal places.\n\nTest suite (each tuple is $(n,\\phi,m)$):\n- $(10^5,\\, 0.95,\\, 200)$\n- $(10^5,\\, 0.0,\\, 200)$\n- $(10^5,\\, 0.5,\\, 200)$\n- $(10^5,\\, 0.999,\\, 200)$\n- $(10^5,\\, -0.5,\\, 200)$\n\nFinal output format:\n- Your program should produce a single line of output containing a list of five elements, one per test case, preserving the order shown above. Each element must itself be a list of four floating-point numbers in the order specified. The entire output must be a single line in the form\n$[\\text{case1},\\text{case2},\\text{case3},\\text{case4},\\text{case5}]$\nwhere, for example, $\\text{case1} = [\\hat{\\tau}_{\\mathrm{int}}^{(B,\\mathrm{lead})},\\hat{\\rho}_1^{(\\mathrm{lead})},\\hat{\\rho}_{50}^{(\\mathrm{lead})},\\hat{\\rho}_{100}^{(\\mathrm{lead})}]$.",
            "solution": "The problem requires the computation of a Bartlett-windowed integrated autocorrelation time and several autocorrelation values for a first-order autoregressive process, AR(1). The key simplification is to use the true autocorrelation $\\rho_k = \\phi^k$ in place of sample autocorrelations, which is a valid leading-order approximation for large sample sizes $n$.\n\nThe required quantities for each test case $(n, \\phi, m)$ are:\n1.  **Leading-order autocorrelations:** $\\hat{\\rho}_k^{(\\mathrm{lead})} = \\phi^k$ for $k \\in \\{1, 50, 100\\}$.\n2.  **Leading-order Bartlett-windowed IAT:** $\\hat{\\tau}_{\\mathrm{int}}^{(B,\\mathrm{lead})} = 1 + 2 \\sum_{k=1}^{m} \\left(1 - \\frac{k}{m+1}\\right) \\phi^k$.\n\nThe following Python script implements the calculation for the entire test suite and formats the output as specified. The large-sample approximation means the parameter $n$ does not enter the final calculation.\n\n```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the leading-order Bartlett-windowed integrated autocorrelation time\n    and autocorrelations for a first-order autoregressive process for several\n    parameter sets.\n    \"\"\"\n    \n    # Test suite: each tuple is (n, phi, m)\n    # n is provided for context (large sample approximation) but not used in the calculation.\n    test_cases = [\n        (10**5, 0.95, 200),\n        (10**5, 0.0, 200),\n        (10**5, 0.5, 200),\n        (10**5, 0.999, 200),\n        (10**5, -0.5, 200)\n    ]\n\n    all_results_str = []\n    \n    for _, phi, m in test_cases:\n        # Calculate the leading-order approximations of rho_k = phi^k\n        rho_1_lead = np.power(phi, 1)\n        rho_50_lead = np.power(phi, 50)\n        rho_100_lead = np.power(phi, 100)\n\n        # Numerically compute the sum for the windowed integrated autocorrelation time\n        total_sum = 0.0\n        m_plus_1 = float(m + 1)\n        \n        current_phi_k = phi\n        for k in range(1, m + 1):\n            weight = 1.0 - k / m_plus_1\n            rho_k_lead = current_phi_k\n            total_sum += weight * rho_k_lead\n            current_phi_k *= phi\n\n        tau_int_b_lead = 1.0 + 2.0 * total_sum\n\n        # Assemble the results for the current case\n        case_results = [\n            tau_int_b_lead,\n            rho_1_lead,\n            rho_50_lead,\n            rho_100_lead\n        ]\n        \n        # Format the list of numbers into the specified string format with 6 decimal places.\n        case_str = f\"[{','.join(f'{x:.6f}' for x in case_results)}]\"\n        all_results_str.append(case_str)\n\n    # The entire output must be a single string representing a list of lists.\n    final_output = f\"[{','.join(all_results_str)}]\"\n    return final_output\n\n# To generate the answer for the answer tag:\n# print(solve())\n```",
            "answer": "[[34.208170,0.950000,0.076945,0.005921],[1.000000,0.000000,0.000000,0.000000],[2.940592,0.500000,0.000000,0.000000],[199.697334,0.999000,0.951229,0.904792],[0.334975,-0.500000,0.000000,0.000000]]"
        },
        {
            "introduction": "While the integrated autocorrelation time is a vital diagnostic, its estimation is fraught with potential pitfalls due to the inherent noise in sample autocorrelations. This exercise  confronts a common but flawed heuristic: truncating the sum of autocorrelations at the first negative value, which systematically underestimates the true variance. By analyzing this bias, you will develop a deeper appreciation for the principled, consistent methods based on lag-window estimators that are essential for reliable MCMC output analysis.",
            "id": "3289812",
            "problem": "Consider a stationary, ergodic output process $\\{X_t\\}_{t \\ge 0}$ produced by a Markov Chain Monte Carlo (MCMC) algorithm targeting an integrable function with mean $\\mu = \\mathbb{E}[X_0]$. Let the autocovariance function be $\\gamma(k) = \\operatorname{Cov}(X_t, X_{t+k})$ for integer lag $k$, and let the autocorrelation function (ACF) be $\\rho(k) = \\gamma(k)/\\gamma(0)$. Define the sample mean $\\bar{X}_n = \\frac{1}{n}\\sum_{t=1}^n X_t$ and the sample autocovariance $\\hat{\\gamma}(k)$ and sample autocorrelation $\\hat{\\rho}(k)$ in the usual way. Suppose the autocovariance sequence $(\\gamma(k))_{k \\in \\mathbb{Z}}$ is absolutely summable and the spectral density at frequency zero exists and is finite.\n\nAn estimator of the long-run variance relevant for $\\operatorname{Var}(\\bar{X}_n)$ often takes the form of a sum of sample autocovariances. A common heuristic truncation sets a random cutoff at the first nonpositive sample ACF value: define the stopping time $\\hat{\\ell} = \\inf\\{k \\ge 1 : \\hat{\\rho}(k) \\le 0\\}$ and construct the estimator $\\hat{\\sigma}^2_{\\mathrm{FZC}} = \\hat{\\gamma}(0) + 2\\sum_{k=1}^{\\hat{\\ell}-1} \\hat{\\gamma}(k)$, where “FZC” stands for “first zero crossing.” From first principles starting at the covariance identity for $\\operatorname{Var}(\\bar{X}_n)$ in terms of $(\\gamma(k))$, analyze the effect of this stopping rule on bias. Then, propose a principled alternative based on lag-window estimators that yields a consistent estimate of the long-run variance, explicitly stating conditions on the window and its bandwidth that ensure consistency under the assumptions given.\n\nWhich option correctly provides a principle-based explanation for why using the first zero crossing of the sample ACF as a truncation point can induce underestimation of the variance, and specifies a window-based estimator that is consistent under appropriate conditions?\n\nA. The first zero crossing rule causes underestimation because for positively correlated chains the true $\\rho(k)$ remains positive over many lags, whereas $\\hat{\\rho}(k)$ is noisy and systematically downward biased at positive lags due to finite-sample effects; the random stopping time $\\hat{\\ell}$ is therefore driven by noise rather than the true decay and truncates too early, yielding a negatively biased sum of autocovariances. A consistent alternative is the lag-window (spectral) estimator\n$$\n\\hat{\\sigma}^2_{\\mathrm{LW}} \\;=\\; \\hat{\\gamma}(0) \\;+\\; 2\\sum_{k=1}^{n-1} w\\!\\left(\\frac{k}{b_n}\\right)\\,\\hat{\\gamma}(k),\n$$\nwhere $w$ is an even, bounded window with $w(0)=1$ and $w(x)\\to 0$ as $|x|\\to\\infty$ (e.g., Bartlett or flat-top), and the bandwidth $b_n$ satisfies $b_n \\to \\infty$ and $b_n/n \\to 0$ as $n \\to \\infty$; under absolute summability of $(\\gamma(k))$, $\\hat{\\sigma}^2_{\\mathrm{LW}}$ is consistent.\n\nB. The first zero crossing rule is conservative and tends to overestimate variance, because negative correlations dominate at large lags; a principled alternative is a fixed-bandwidth window, taking $b_n=b$ constant and any nonnegative window $w$, which is consistent as $n\\to\\infty$.\n\nC. The first zero crossing only causes underestimation for reversible chains; for nonreversible chains the effect disappears. A window estimator is unnecessary if the chain is nonreversible, because $\\sum_{k\\ge 1}\\gamma(k)$ vanishes by antisymmetry.\n\nD. The first zero crossing should be replaced by the initial positive sequence rule that sums $\\hat{\\gamma}(k)$ only while the cumulative sum stays nonnegative; this guarantees unbiasedness without requiring a window or bandwidth and is consistent for any stationary output process.\n\nE. The first zero crossing is unbiased if the target $\\rho(k)$ is nonincreasing in $k$; for consistency it suffices to use a window $w$ with $w(0)=1$ and bandwidth satisfying $b_n/n \\to c \\in (0,1)$, so that the effective number of terms grows proportionally to $n$.",
            "solution": "### Analysis of the Problem\n\nThe problem asks for an analysis of a heuristic estimator for the long-run variance, $\\sigma^2 = \\sum_{k=-\\infty}^{\\infty} \\gamma(k)$, which is crucial for finding the standard error of the mean in MCMC analysis.\n\n**1. The \"First Zero Crossing\" (FZC) Estimator:**\nThe estimator is defined as $\\hat{\\sigma}^2_{\\mathrm{FZC}} = \\hat{\\gamma}(0) + 2\\sum_{k=1}^{\\hat{\\ell}-1} \\hat{\\gamma}(k)$, where the sum is truncated at a random stopping time $\\hat{\\ell} = \\inf\\{k \\ge 1 : \\hat{\\rho}(k) \\le 0\\}$.\n\n-   **Bias Analysis:** For typical MCMC samplers, samples are positively correlated, meaning the true autocorrelation function $\\rho(k)$ is positive and decays towards zero as the lag $k$ increases. The sample ACF, $\\hat{\\rho}(k)$, is a noisy estimate of $\\rho(k)$. The variance of $\\hat{\\rho}(k)$ is non-zero, especially for larger lags where the estimate is based on fewer data pairs. This noise will almost certainly cause $\\hat{\\rho}(k)$ to fluctuate and dip below zero for a relatively small lag $k$, even if the true $\\rho(k)$ is still positive. The FZC rule terminates the sum at this point. By doing so, it prematurely cuts off the summation, omitting later terms $\\hat{\\gamma}(k)$ which, on average, would be positive and contribute to the total sum. This premature, noise-driven truncation results in a systematic **underestimation** of the true long-run variance $\\sigma^2$. The estimator is therefore negatively biased.\n\n**2. A Principled Alternative: Lag-Window Estimators:**\nA consistent method for estimating $\\sigma^2$ is the lag-window (or spectral density at zero) estimator. Its general form is:\n$$\n\\hat{\\sigma}^2_{\\mathrm{LW}} = \\hat{\\gamma}(0) + 2\\sum_{k=1}^{n-1} w\\left(\\frac{k}{b_n}\\right)\\hat{\\gamma}(k)\n$$\nHere, $w(\\cdot)$ is a window function (or kernel) and $b_n$ is a bandwidth parameter.\n\n-   **Consistency Conditions:** For this estimator to be consistent (i.e., $\\hat{\\sigma}^2_{\\mathrm{LW}} \\to \\sigma^2$ as $n \\to \\infty$), the bias and variance of the estimator must both go to zero.\n    -   To control bias, the window must eventually include all significant lags. This is achieved by letting the bandwidth grow with the sample size: $b_n \\to \\infty$.\n    -   To control variance, the number of noisy $\\hat{\\gamma}(k)$ terms included must not grow too quickly relative to the total sample size. This is achieved by requiring $b_n/n \\to 0$.\n    -   The window function $w$ itself must satisfy certain properties, such as being a bounded, even function with $w(0)=1$.\n\n### Evaluation of Options\n\n-   **A:** This option correctly identifies that the FZC rule causes underestimation due to premature truncation by noise in the sample ACF. It also correctly specifies the form of a consistent lag-window estimator and provides the correct asymptotic conditions on the bandwidth ($b_n \\to \\infty$ and $b_n/n \\to 0$) and the window function. This aligns perfectly with standard time series theory.\n-   **B:** Incorrectly claims overestimation. It also incorrectly states that a fixed bandwidth leads to consistency.\n-   **C:** The bias issue is not specific to reversible chains. The claim about antisymmetry of $\\gamma(k)$ for non-reversible chains is false for a single real-valued process.\n-   **D:** The proposed \"initial positive sequence\" rule is another ad-hoc, data-dependent rule that is also prone to bias and is not generally consistent. It does not guarantee unbiasedness.\n-   **E:** Incorrectly claims the FZC estimator is unbiased under a non-increasing $\\rho(k)$. The bias is due to the noise in the *estimator* $\\hat{\\rho}(k)$. The condition on the bandwidth ($b_n/n \\to c \\in (0,1)$) is incorrect for consistency.\n\nTherefore, option A provides the correct analysis of the bias and the correct description of a consistent alternative.",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}