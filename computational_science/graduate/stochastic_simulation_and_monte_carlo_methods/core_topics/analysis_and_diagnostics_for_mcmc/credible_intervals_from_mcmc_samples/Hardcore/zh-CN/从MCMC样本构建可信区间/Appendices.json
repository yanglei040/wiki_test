{
    "hands_on_practices": [
        {
            "introduction": "在使用马尔可夫链蒙特卡洛（MCMC）样本之前，我们必须处理初始的“预烧期”（burn-in），在此期间马尔可夫链尚未收敛到其平稳分布。这个练习  提供了一个定量框架，以理解选择预烧期长度所涉及的权衡：丢弃更多的样本可以减少由初始状态引起的偏差，但同时会因样本量的减小而增加估计量的方差。通过模拟，我们将探索如何在这种相互竞争的误差来源之间找到一个有原则的平衡点。",
            "id": "3301169",
            "problem": "考虑一个一维目标分布，其密度正比于 $p(x) \\propto \\exp\\left(-\\frac{x^2}{2}\\right)$（一个标准正态密度，省略了归一化常数）。目标是使用通过随机游走 Metropolis-Hastings (MH) 算法构建的马尔可夫链样本来估计单个参数 $x$ 的对称可信区间，并量化老化期比例的选择如何影响区间端点的偏差和方差。然后，设计并实现一个用于移除老化期的偏差-方差最优停止规则。\n\n推导和算法应基于以下基本事实：\n- 当马尔可夫链是遍历的时，其遍历均值会收敛到平稳分布下的期望。对于一个转移核为 $K$ 的马尔可夫链，如果它是几何遍历的并且从任意初始分布开始，那么随着步数的增加，状态的分布会收敛到平稳分布 $\\pi$。\n- 采用对称提议密度 $q(y \\mid x) = \\mathcal{N}(x, \\sigma^2)$ 的随机游走 Metropolis-Hastings (MH) 算法，当 $\\pi$ 在不考虑归一化常数的情况下已知时，其接受概率为 $a(x,y) = \\min\\left\\{1, \\frac{\\pi(y)}{\\pi(x)}\\right\\}$。\n- 对于一个固定的可信水平 $1-\\alpha$（其中 $0 < \\alpha < 1$），对称可信区间由下分位数 $q_{\\ell} = F^{-1}\\left(\\frac{\\alpha}{2}\\right)$ 和上分位数 $q_{u} = F^{-1}\\left(1 - \\frac{\\alpha}{2}\\right)$ 定义，其中 $F$ 是后验分布（在此即为目标分布）的累积分布函数。对于标准正态目标分布， $q_{\\ell}$ 和 $q_{u}$ 的值是解析已知的。\n- 估计量 $\\hat{\\theta}$ 的均方误差可分解为 $\\mathrm{MSE}(\\hat{\\theta}) = \\left(\\mathrm{Bias}(\\hat{\\theta})\\right)^2 + \\mathrm{Var}(\\hat{\\theta})$，其中 $\\mathrm{Bias}(\\hat{\\theta}) = \\mathbb{E}[\\hat{\\theta}] - \\theta$ 且 $\\mathrm{Var}(\\hat{\\theta}) = \\mathbb{E}\\left[(\\hat{\\theta} - \\mathbb{E}[\\hat{\\theta}])^2\\right]$。\n\n定义和要求：\n- 马尔可夫链蒙特卡洛 (MCMC) 指的是一类抽样方法，其构建一个马尔可夫链，使该链的平稳分布是目标分布，从而可以从链的实现值中估计期望和分布量。\n- 老化期比例 $f \\in [0,1)$ 指定在计算估计量之前，应丢弃长度为 $N$ 的链中的前 $b = \\lfloor f N \\rfloor$ 个样本。\n- 可信区间的端点通过从老化期后的样本计算出的经验分位数来估计：对于可信水平 $1-\\alpha$（其中 $\\alpha$ 以小数表示），下端点估计值 $\\hat{q}_{\\ell}$ 是 $\\frac{\\alpha}{2}$ 处的经验分位数，上端点估计值 $\\hat{q}_{u}$ 是 $1 - \\frac{\\alpha}{2}$ 处的经验分位数。\n- 为量化给定老化期比例 $f$ 下区间端点的偏差和方差，使用 $R$ 个独立同分布的 MCMC 副本（每个副本具有独立的随机种子）来计算 $\\{\\hat{q}_{\\ell}^{(r)}(f), \\hat{q}_{u}^{(r)}(f)\\}_{r=1}^R$。设真实端点为 $q_{\\ell}$ 和 $q_{u}$。将每个端点的经验偏差和经验方差定义为：\n$$\n\\mathrm{Bias}_{\\ell}(f) = \\left(\\frac{1}{R}\\sum_{r=1}^R \\hat{q}_{\\ell}^{(r)}(f)\\right) - q_{\\ell}, \\quad\n\\mathrm{Var}_{\\ell}(f) = \\frac{1}{R}\\sum_{r=1}^R \\left(\\hat{q}_{\\ell}^{(r)}(f) - \\frac{1}{R}\\sum_{s=1}^R \\hat{q}_{\\ell}^{(s)}(f)\\right)^2,\n$$\n上端点同理，\n$$\n\\mathrm{Bias}_{u}(f) = \\left(\\frac{1}{R}\\sum_{r=1}^R \\hat{q}_{u}^{(r)}(f)\\right) - q_{u}, \\quad\n\\mathrm{Var}_{u}(f) = \\frac{1}{R}\\sum_{r=1}^R \\left(\\hat{q}_{u}^{(r)}(f) - \\frac{1}{R}\\sum_{s=1}^R \\hat{q}_{u}^{(s)}(f)\\right)^2.\n$$\n- 提出一个用于老化期选择的偏差-方差最优停止规则，该规则最小化两个端点的组合均方误差：\n$$\n\\mathrm{Objective}(f;\\lambda) = \\left(\\mathrm{Bias}_{\\ell}(f)\\right)^2 + \\lambda \\, \\mathrm{Var}_{\\ell}(f) + \\left(\\mathrm{Bias}_{u}(f)\\right)^2 + \\lambda \\, \\mathrm{Var}_{u}(f),\n$$\n其中 $\\lambda > 0$ 是一个权衡权重参数，表示方差相对于偏差平方的相对重要性。最优老化期比例即为 $f^\\star = \\arg\\min_{f \\in \\mathcal{F}} \\mathrm{Objective}(f;\\lambda)$，在分数候选集 $\\mathcal{F}$ 上进行优化。\n\n对每个副本使用以下随机游走 Metropolis-Hastings 算法：\n- 在 $x_0$ 处初始化。\n- 对于 $t = 1, \\dots, N$，提议 $y_t \\sim \\mathcal{N}(x_{t-1}, \\sigma^2)$ 并以概率 $a(x_{t-1}, y_t) = \\min\\left(1, \\exp\\left(-\\frac{y_t^2 - x_{t-1}^2}{2}\\right)\\right)$ 接受；如果接受则设 $x_t = y_t$，否则 $x_t = x_{t-1}$。将 $x_t$ 记录为第 $t$ 个样本。\n\n实现上述算法并评估三个测试用例的最优老化期比例。对于每个测试用例，从提供的候选集 $\\mathcal{F}$ 中计算 $f^\\star$，并将三个最优比例以单行列表的形式输出。所有量均为无量纲；没有物理单位。\n\n测试套件：\n- 案例1（收敛慢，起始点远）：\n    - 目标：标准正态（如上）。\n    - 链长度：$N = 4000$。\n    - 提议标准差：$\\sigma = 0.05$。\n    - 初始值：$x_0 = 5.0$。\n    - 可信水平：$1-\\alpha = 0.95$（使用小数形式 $\\alpha = 0.05$）。\n    - 候选老化期比例：$\\mathcal{F} = \\{0.0, 0.1, 0.2, 0.4, 0.6\\}$。\n    - 副本数：$R = 100$。\n    - 权衡权重：$\\lambda = 1.0$。\n    - 随机种子基数：$s = 12345$。\n- 案例2（边界高老化期选项，与案例1相同的慢收敛链）：\n    - 目标：标准正态。\n    - 链长度：$N = 4000$。\n    - 提议标准差：$\\sigma = 0.05$。\n    - 初始值：$x_0 = 5.0$。\n    - 可信水平：$1-\\alpha = 0.95$（使用 $\\alpha = 0.05$）。\n    - 候选老化期比例：$\\mathcal{F} = \\{0.7, 0.8, 0.9\\}$。\n    - 副本数：$R = 100$。\n    - 权衡权重：$\\lambda = 1.0$。\n    - 随机种子基数：$s = 54321$。\n- 案例3（混合良好的链）：\n    - 目标：标准正态。\n    - 链长度：$N = 4000$。\n    - 提议标准差：$\\sigma = 1.0$。\n    - 初始值：$x_0 = 0.0$。\n    - 可信水平：$1-\\alpha = 0.95$（使用 $\\alpha = 0.05$）。\n    - 候选老化期比例：$\\mathcal{F} = \\{0.0, 0.25, 0.5\\}$。\n    - 副本数：$R = 100$。\n    - 权衡权重：$\\lambda = 1.0$。\n    - 随机种子基数：$s = 67890$。\n\n要求的最终输出格式：\n- 你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表，每个老化期比例以保留两位小数的小数形式报告（例如，$[0.20,0.70,0.00]$）。",
            "solution": "所提出的问题要求为马尔可夫链蒙特卡洛 (MCMC) 模拟生成的样本确定一个最优的老化期比例。最优性准则定义为最小化可信区间端点估计量的偏差平方和方差的加权和。本分析探讨了 MCMC 诊断中的一个基本权衡：通过丢弃初始样本（老化期）来减少源于非平稳起始点的偏差，其代价是由于有效样本量的减少而导致估计量的方差增大。\n\n这个问题的基础在于马尔可夫链的遍历理论。对于一个以平稳分布 $\\pi(x)$ 为目标的 MCMC 模拟，当步数 $t$ 趋于无穷大时，链的状态 $x_t$ 的分布会收敛到 $\\pi(x)$。随机游走 Metropolis-Hastings (MH) 算法被用来构建这样一条链。给定当前状态 $x_{t-1}$，从一个对称提议分布 $q(y \\mid x_{t-1})$ 中提议一个新状态 $y$，在本例中，该分布是以当前状态为中心的正态分布 $\\mathcal{N}(x_{t-1}, \\sigma^2)$。目标分布的概率密度函数 (PDF) 正比于 $p(x) \\propto \\exp\\left(-\\frac{x^2}{2}\\right)$，这是一个标准正态分布 $\\mathcal{N}(0, 1)$ 的核。提议的状态 $y$ 以如下概率被接受：\n$$\na(x_{t-1}, y) = \\min\\left\\{1, \\frac{p(y)}{p(x_{t-1})}\\right\\} = \\min\\left\\{1, \\frac{\\exp(-y^2/2)}{\\exp(-x_{t-1}^2/2)}\\right\\} = \\min\\left\\{1, \\exp\\left(-\\frac{y^2 - x_{t-1}^2}{2}\\right)\\right\\}。\n$$\n如果提议被接受，则下一个状态为 $x_t = y$；否则，为 $x_t = x_{t-1}$。\n\n目标是估计参数 $x$ 的一个对称 $1-\\alpha$ 可信区间。对于目标分布 $\\mathcal{N}(0, 1)$，该区间的真实端点是分位数 $q_{\\ell} = \\Phi^{-1}(\\alpha/2)$ 和 $q_{u} = \\Phi^{-1}(1 - \\alpha/2)$，其中 $\\Phi^{-1}$ 是标准正态分布的逆累积分布函数 (CDF)。对于指定的可信水平 $1-\\alpha = 0.95$，我们有 $\\alpha=0.05$。因此，真实端点为 $q_{\\ell} = \\Phi^{-1}(0.025) \\approx -1.96$ 和 $q_{u} = \\Phi^{-1}(0.975) \\approx 1.96$。\n\n这些端点是根据 MCMC 样本估计的。生成一条总长度为 $N$ 的链。为了减轻来自初始状态 $x_0$（可能位于低概率区域）的偏差，丢弃前 $b = \\lfloor fN \\rfloor$ 个样本，其中 $f$ 是老化期比例。端点的估计量，记为 $\\hat{q}_{\\ell}(f)$ 和 $\\hat{q}_{u}(f)$，分别是从剩余的 $N-b$ 个样本计算出的 $\\alpha/2$ 和 $1-\\alpha/2$ 水平的经验分位数。\n\n这些估计量的质量通过其偏差和方差来评估。由于这些量依赖于 MCMC 链的具体实现，我们必须在许多独立副本上取平均值。我们生成 $R$ 条独立的链。对于给定的老化期比例 $f$，我们为每个端点获得 $R$ 个估计值，即 $\\{\\hat{q}_{\\ell}^{(r)}(f)\\}_{r=1}^R$ 和 $\\{\\hat{q}_{u}^{(r)}(f)\\}_{r=1}^R$。然后根据规定计算经验偏差和经验方差：\n$$\n\\mathrm{Bias}_{\\ell}(f) = \\left(\\frac{1}{R}\\sum_{r=1}^R \\hat{q}_{\\ell}^{(r)}(f)\\right) - q_{\\ell}\n$$\n$$\n\\mathrm{Var}_{\\ell}(f) = \\frac{1}{R}\\sum_{r=1}^R \\left(\\hat{q}_{\\ell}^{(r)}(f) - \\frac{1}{R}\\sum_{s=1}^R \\hat{q}_{\\ell}^{(s)}(f)\\right)^2\n$$\n上端点估计量 $\\hat{q}_{u}$ 的计算也与此类似。\n\n选择最优老化期比例 $f^\\star$ 以最小化一个复合目标函数，该函数表示端点估计量的总均方误差 (MSE)，其中方差项带有一个权重因子 $\\lambda$。该目标函数为：\n$$\n\\mathrm{Objective}(f;\\lambda) = \\left(\\mathrm{Bias}_{\\ell}(f)\\right)^2 + \\lambda \\, \\mathrm{Var}_{\\ell}(f) + \\left(\\mathrm{Bias}_{u}(f)\\right)^2 + \\lambda \\, \\mathrm{Var}_{u}(f).\n$$\n最优比例 $f^\\star$ 是通过在候选比例的离散集合 $\\mathcal{F}$ 上搜索找到的：\n$$\nf^\\star = \\arg\\min_{f \\in \\mathcal{F}} \\mathrm{Objective}(f;\\lambda).\n$$\n增加 $f$ 通常会减少偏差项，因为更多非平稳的初始样本被移除。然而，这同时减少了用于估计的样本数量，从而通常会增加方差项。对于给定的 $\\lambda$，最优的 $f^\\star$ 代表了在这种权衡中的最佳折衷。\n\n算法流程如下：\n1. 对每个测试用例，定义参数：链长度 $N$、提议标准差 $\\sigma$、初始值 $x_0$、可信水平 $\\alpha$、候选比例集 $\\mathcal{F}$、副本数 $R$、权衡权重 $\\lambda$ 和随机种子基数 $s$。\n2. 使用标准正态分布的逆累积分布函数计算真实分位数端点 $q_{\\ell}$ 和 $q_{u}$。\n3. 生成并存储 $R$ 条独立的 MCMC 链，每条链长为 $N$。链 $r \\in \\{1, \\dots, R\\}$ 使用 $s+r-1$ 进行种子设定，以确保可复现性和独立性。\n4. 对每个候选老化期比例 $f \\in \\mathcal{F}$：\n    a. 确定老化期样本数 $b = \\lfloor fN \\rfloor$。\n    b. 对 $R$ 条链中的每一条，取索引 $b$ 到 $N-1$ 的样本。\n    c. 对每个副本 $r$，从这些老化期后的样本中计算经验分位数估计值 $\\hat{q}_{\\ell}^{(r)}(f)$ 和 $\\hat{q}_{u}^{(r)}(f)$。\n    d. 使用每个端点的 $R$ 个估计值集合，根据提供的公式计算经验偏差 $\\mathrm{Bias}_{\\ell}(f)$ 和 $\\mathrm{Bias}_{u}(f)$，以及经验方差 $\\mathrm{Var}_{\\ell}(f)$ 和 $\\mathrm{Var}_{u}(f)$。\n    e. 评估目标函数 $\\mathrm{Objective}(f;\\lambda)$。\n5. 确定使目标函数值最小的比例 $f^\\star \\in \\mathcal{F}$。这是该测试用例的最优老化期比例。\n6. 对所有三个测试用例重复以上步骤，并报告最终得到的最优比例。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef run_mh_chain(N, x0, sigma, rng):\n    \"\"\"\n    Generates a single MCMC chain using the Random-Walk Metropolis-Hastings algorithm.\n\n    Args:\n        N (int): The length of the chain.\n        x0 (float): The initial value.\n        sigma (float): The proposal standard deviation.\n        rng (numpy.random.Generator): The random number generator.\n\n    Returns:\n        numpy.ndarray: The generated MCMC chain of length N.\n    \"\"\"\n    chain = np.zeros(N)\n    chain[0] = x0\n    current_x = x0\n    \n    # We use log-probabilities for numerical stability\n    log_alpha_num_term_current = -0.5 * current_x**2\n\n    for t in range(1, N):\n        proposed_y = rng.normal(loc=current_x, scale=sigma)\n        \n        log_alpha_num_term_proposed = -0.5 * proposed_y**2\n        log_acceptance_ratio = log_alpha_num_term_proposed - log_alpha_num_term_current\n\n        if np.log(rng.uniform())  log_acceptance_ratio:\n            current_x = proposed_y\n            log_alpha_num_term_current = log_alpha_num_term_proposed\n        \n        chain[t] = current_x\n        \n    return chain\n\ndef find_optimal_burn_in(N, sigma, x0, alpha, F_set, R, lambd, seed_base):\n    \"\"\"\n    Finds the optimal burn-in fraction for a given set of MCMC parameters.\n\n    Args:\n        N (int): Chain length.\n        sigma (float): Proposal standard deviation.\n        x0 (float): Initial value.\n        alpha (float): Significance level for credible interval.\n        F_set (list): Candidate burn-in fractions.\n        R (int): Number of replicates.\n        lambd (float): Bias-variance trade-off weight.\n        seed_base (int): Base for random seeds.\n\n    Returns:\n        float: The optimal burn-in fraction from F_set.\n    \"\"\"\n    # True quantiles for a standard normal distribution\n    q_lower_true = norm.ppf(alpha / 2.0)\n    q_upper_true = norm.ppf(1.0 - alpha / 2.0)\n\n    # Generate all R chains first\n    all_chains = np.zeros((R, N))\n    for r in range(R):\n        rng = np.random.default_rng(seed_base + r)\n        all_chains[r, :] = run_mh_chain(N, x0, sigma, rng)\n\n    objective_values = []\n    for f in F_set:\n        b = int(np.floor(f * N))\n        \n        # Ensure there are samples left after burn-in\n        if b = N:\n          # Assign a very large value to objective if no samples remain\n          objective_values.append(np.inf)\n          continue\n          \n        post_burn_in_samples = all_chains[:, b:]\n\n        # Compute empirical quantiles for all R replicates at once\n        q_lower_estimates = np.quantile(post_burn_in_samples, q=alpha / 2.0, axis=1)\n        q_upper_estimates = np.quantile(post_burn_in_samples, q=1.0 - alpha / 2.0, axis=1)\n\n        # Calculate empirical bias\n        mean_q_lower = np.mean(q_lower_estimates)\n        mean_q_upper = np.mean(q_upper_estimates)\n        bias_l = mean_q_lower - q_lower_true\n        bias_u = mean_q_upper - q_upper_true\n\n        # Calculate empirical variance (using population variance formula, ddof=0)\n        var_l = np.var(q_lower_estimates, ddof=0)\n        var_u = np.var(q_upper_estimates, ddof=0)\n        \n        # Calculate the objective function\n        objective = bias_l**2 + lambd * var_l + bias_u**2 + lambd * var_u\n        objective_values.append(objective)\n\n    # Find the fraction that minimizes the objective function\n    min_idx = np.argmin(objective_values)\n    f_star = F_set[min_idx]\n    \n    return f_star\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case 1: Slow convergence, far start\n        {\n            \"N\": 4000, \"sigma\": 0.05, \"x0\": 5.0, \"alpha\": 0.05,\n            \"F_set\": [0.0, 0.1, 0.2, 0.4, 0.6], \"R\": 100, \n            \"lambd\": 1.0, \"seed_base\": 12345\n        },\n        # Case 2: Boundary high burn-in options, same slow chain\n        {\n            \"N\": 4000, \"sigma\": 0.05, \"x0\": 5.0, \"alpha\": 0.05,\n            \"F_set\": [0.7, 0.8, 0.9], \"R\": 100,\n            \"lambd\": 1.0, \"seed_base\": 54321\n        },\n        # Case 3: Well-mixed chain\n        {\n            \"N\": 4000, \"sigma\": 1.0, \"x0\": 0.0, \"alpha\": 0.05,\n            \"F_set\": [0.0, 0.25, 0.5], \"R\": 100,\n            \"lambd\": 1.0, \"seed_base\": 67890\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        f_star = find_optimal_burn_in(\n            N=case[\"N\"],\n            sigma=case[\"sigma\"],\n            x0=case[\"x0\"],\n            alpha=case[\"alpha\"],\n            F_set=case[\"F_set\"],\n            R=case[\"R\"],\n            lambd=case[\"lambd\"],\n            seed_base=case[\"seed_base\"]\n        )\n        results.append(f\"{f_star:.2f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "一旦我们获得了平稳样本，构建置信区间最常用的方法是取经验分位数。这个练习  深入探讨了该过程的数学性质，揭示了一个微妙但重要的观点：从有限数量的样本构建的名义 $95\\%$ 区间，其平均后验概率质量不一定恰好为 $0.95$。通过应用顺序统计量的理论，你将计算出由有限样本构建的区间的真实期望覆盖率。",
            "id": "3301098",
            "problem": "一个标量参数 $\\theta$ 具有连续的后验累积分布函数 $F$。一个马尔可夫链蒙特卡洛（MCMC）采样器在稳态下从中生成了 $m$ 个有效的独立抽样。将这些抽样表示为 $Y_{1},\\dots,Y_{m}$，其顺序统计量表示为 $Y_{(1)} \\le \\dots \\le Y_{(m)}$。您希望使用有限样本的顺序统计量来报告一个名义上的 $95\\%$ 等尾置信区间，并遵循以下索引规则：对于尾部概率 $\\alpha = 0.025$，取下端点为 $Y_{(\\ell)}$，其中 $\\ell = \\lceil (m+1)\\alpha \\rceil$，上端点为 $Y_{(u)}$，其中 $u = \\lfloor (m+1)(1-\\alpha) \\rfloor$。\n\n假设这些抽样是来自具有连续函数 $F$ 的真实后验分布的独立同分布样本。对于 $m=2000$ 的情况，计算所报告区间内包含的期望后验概率质量，即计算 $\\mathbb{E}\\!\\left[F\\!\\left(Y_{(u)}\\right) - F\\!\\left(Y_{(\\ell)}\\right)\\right]$。将您的答案四舍五入到四位有效数字。",
            "solution": "用户想要找到由马尔可夫链蒙特卡洛（MCMC）样本构建的置信区间的期望后验概率质量。\n\n### 第1步：提取已知条件\n- 关注的参数：一个标量参数 $\\theta$。\n- 后验累积分布函数（CDF）：$F$，是连续的。\n- MCMC样本：$Y_1, \\dots, Y_m$ 是从 $\\theta$ 的后验分布中抽取的 $m$ 个有效独立样本。\n- 样本的顺序统计量：$Y_{(1)} \\le Y_{(2)} \\le \\dots \\le Y_{(m)}$。\n- 名义置信区间水平：$95\\%$，等尾。\n- 尾部概率：$\\alpha = 0.025$。\n- 区间端点：区间为 $[Y_{(\\ell)}, Y_{(u)}]$。\n- 下端点索引：$\\ell = \\lceil (m+1)\\alpha \\rceil$。\n- 上端点索引：$u = \\lfloor (m+1)(1-\\alpha) \\rfloor$。\n- 样本大小：$m=2000$。\n- 假设：抽样 $Y_1, \\dots, Y_m$ 是来自连续CDF $F$ 的独立同分布（i.i.d.）样本。\n- 目标：计算期望后验概率质量 $\\mathbb{E}\\!\\left[F\\!\\left(Y_{(u)}\\right) - F\\!\\left(Y_{(\\ell)}\\right)\\right]$。\n- 要求精度：将最终答案四舍五入到四位有效数字。\n\n### 第2步：使用提取的已知条件进行验证\n该问题在贝叶斯统计和顺序统计量理论的框架内定义明确。所使用的概念，如后验分布、MCMC、置信区间和顺序统计量，都是标准的。假设MCMC抽样是有效独立样本是理论分析中常见的理想化情况。该问题具有科学依据，是客观且自洽的。它需要基于既定统计理论进行一项特定的、不平凡的计算。问题没有矛盾、歧义或事实错误。该问题是有效的。\n\n### 第3步：推导解答\n我们感兴趣的量是随机区间 $[Y_{(\\ell)}, Y_{(u)}]$ 内包含的后验概率质量的期望值。这个区间的后验概率质量由 $F(Y_{(u)}) - F(Y_{(\\ell)})$ 给出。我们需要计算其期望值：\n$$\n\\mathbb{E}\\!\\left[F\\!\\left(Y_{(u)}\\right) - F\\!\\left(Y_{(\\ell)}\\right)\\right]\n$$\n根据期望的线性性质，这等于：\n$$\n\\mathbb{E}\\!\\left[F\\!\\left(Y_{(u)}\\right)\\right] - \\mathbb{E}\\!\\left[F\\!\\left(Y_{(\\ell)}\\right)\\right]\n$$\n问题陈述，抽样 $Y_1, \\dots, Y_m$ 是来自具有连续CDF $F$ 的分布的独立同分布样本。我们可以应用概率积分变换。令 $U_i = F(Y_i)$，其中 $i=1, \\dots, m$。概率论中的一个基本结果指出，如果 $Y$ 具有连续的CDF $F$，那么随机变量 $U = F(Y)$ 服从标准均匀分布，$U \\sim \\text{Uniform}(0,1)$。\n\n因此，$U_1, \\dots, U_m$ 是来自 $\\text{Uniform}(0,1)$ 分布的 $m$ 个独立同分布的随机变量。由于CDF $F$ 是一个非递减函数，它会保留随机变量的顺序。这意味着如果 $Y_{(j)}$ 是样本 $\\{Y_i\\}$ 的第 $j$ 个顺序统计量，那么 $F(Y_{(j)})$ 就是变换后样本 $\\{U_i\\}$ 的第 $j$ 个顺序统计量。我们将其表示为 $U_{(j)} = F(Y_{(j)})$。\n\n问题现在简化为计算 $\\mathbb{E}[U_{(u)}] - \\mathbb{E}[U_{(\\ell)}]$，其中 $U_{(j)}$ 是来自 $\\text{Uniform}(0,1)$ 分布、大小为 $m$ 的样本的第 $j$ 个顺序统计量。\n\n对于来自 $\\text{Uniform}(0,1)$ 分布、大小为 $m$ 的样本，第 $j$ 个顺序统计量 $U_{(j)}$ 已知服从贝塔分布。具体来说，$U_{(j)} \\sim \\text{Beta}(j, m-j+1)$。\n服从 $\\text{Beta}(a,b)$ 分布的随机变量 $X$ 的期望值由 $\\mathbb{E}[X] = \\frac{a}{a+b}$ 给出。\n\n将此应用于我们的顺序统计量 $U_{(j)}$，我们有 $a=j$ 和 $b=m-j+1$。期望值为：\n$$\n\\mathbb{E}[U_{(j)}] = \\frac{j}{j + (m-j+1)} = \\frac{j}{m+1}\n$$\n使用这个结果，我们可以计算我们特定索引 $\\ell$ 和 $u$ 的期望值：\n$$\n\\mathbb{E}[F(Y_{(\\ell)})] = \\mathbb{E}[U_{(\\ell)}] = \\frac{\\ell}{m+1}\n$$\n$$\n\\mathbb{E}[F(Y_{(u)})] = \\mathbb{E}[U_{(u)}] = \\frac{u}{m+1}\n$$\n因此，期望的后验概率质量为：\n$$\n\\mathbb{E}\\!\\left[F\\!\\left(Y_{(u)}\\right) - F\\!\\left(Y_{(\\ell)}\\right)\\right] = \\frac{u}{m+1} - \\frac{\\ell}{m+1} = \\frac{u - \\ell}{m+1}\n$$\n现在，我们必须使用给定的公式和值 $m=2000$ 和 $\\alpha=0.025$ 来计算 $\\ell$ 和 $u$ 的值。\n\n首先，计算下索引 $\\ell$：\n$$\n\\ell = \\lceil (m+1)\\alpha \\rceil = \\lceil (2000+1) \\times 0.025 \\rceil = \\lceil 2001 \\times 0.025 \\rceil = \\lceil 50.025 \\rceil = 51\n$$\n接下来，计算上索引 $u$：\n$$\nu = \\lfloor (m+1)(1-\\alpha) \\rfloor = \\lfloor (2000+1)(1 - 0.025) \\rfloor = \\lfloor 2001 \\times 0.975 \\rfloor = \\lfloor 1950.975 \\rfloor = 1950\n$$\n现在，我们将这些索引代入我们的期望概率质量表达式中：\n$$\n\\frac{u - \\ell}{m+1} = \\frac{1950 - 51}{2001} = \\frac{1899}{2001}\n$$\n为了得到最终的数值答案，我们进行除法运算并四舍五入到四位有效数字：\n$$\n\\frac{1899}{2001} \\approx 0.949025487...\n$$\n将此值四舍五入到四位有效数字，得到 $0.9490$。\n\n这个结果表明，在有限样本上使用此特定索引规则构建的名义 $95\\%$ 置信区间的期望覆盖率略低于 $95\\%$。这种差异是由于顺序统计量的离散性以及向上取整和向下取整函数的使用造成的。",
            "answer": "$$\n\\boxed{0.9490}\n$$"
        },
        {
            "introduction": "我们通过 MCMC 方法得到的区间端点本身就是估计值，因此会受到蒙特卡洛误差的影响。最后一个练习  展示了如何使用蒙特卡洛标准误差（MCSE）来量化这种不确定性，并正确地处理 MCMC 样本中固有的自相关性。你将实现一个谱方差估计器，并用它来研究常见但经常被误解的“稀疏化”（thinning）做法，从而发现它通常会损害而非提高统计精度。",
            "id": "3301155",
            "problem": "考虑一个平稳高斯一阶自回归马尔可夫链，定义为 $X_t = \\varphi X_{t-1} + \\sqrt{1 - \\varphi^2}\\,Z_t$，其中 $Z_t \\sim \\mathcal{N}(0,1)$ 是独立同分布的，且 $X_0 \\sim \\mathcal{N}(0,1)$ 独立于 $\\{Z_t\\}$。对于每个 $t \\ge 0$，此链具有平稳边际分布 $\\mathcal{N}(0,1)$。令 $F$ 表示平稳累积分布函数，$f$ 表示相应的平稳概率密度函数。对于一个置信水平 $p \\in (0,1)$，定义 $p$-分位数 $x_p$ 满足 $F(x_p) = p$。单变量后验的 $(1-\\alpha)$ 可信区间的端点是位于 $p = \\alpha/2$ 和 $p = 1 - \\alpha/2$ 的分位数，这里取 $\\alpha = 0.05$，即 $p \\in \\{0.025, 0.975\\}$。\n\n目标是量化在固定计算预算下，从马尔可夫链蒙特卡洛（MCMC）样本中估计的可信区间端点的蒙特卡洛标准误差，并评估稀疏化对精度的影响。使用以下基本依据：对于一个几何遍历马尔可夫链和一个平方可积函数 $h$，马尔可夫链中心极限定理给出\n$$\n\\sqrt{n}\\left(\\frac{1}{n}\\sum_{t=1}^{n} h(X_t) - \\mathbb{E}[h(X)]\\right) \\xrightarrow{d} \\mathcal{N}\\left(0, \\sigma_h^2\\right),\n$$\n其中长期方差为 $\\sigma_h^2 = \\gamma_0 + 2\\sum_{\\ell=1}^{\\infty} \\gamma_\\ell$，且 $\\gamma_\\ell = \\mathrm{Cov}(h(X_t), h(X_{t+\\ell}))$。对于分位数，使用delta方法：当 $h_p(x) = \\mathbf{1}\\{x \\le x_p\\}$ 时，我们有 $\\mathbb{E}[h_p(X)] = p$ 且\n$$\n\\mathrm{MCSE}\\left(\\hat{x}_p\\right) \\approx \\sqrt{\\frac{\\sigma_{h_p}^2}{n\\, f(x_p)^2}},\n$$\n其中 $n$ 是保留的抽样数量，$f(x_p)$ 是在 $x_p$ 处的平稳密度。\n\n在实践中，$\\sigma_{h_p}^2$ 是未知的，必须通过谱（长期方差）估计器从抽样中估计。实现Bartlett谱方差估计器，截断值为 $m = \\lfloor n^{1/3} \\rfloor$，并应用于中心化指示序列 $Y_t = h_p(X_t) - p$：\n$$\n\\widehat{\\sigma}_{h_p}^2 = \\widehat{\\gamma}_0 + 2 \\sum_{\\ell=1}^{m} w_\\ell\\, \\widehat{\\gamma}_\\ell,\n\\quad\nw_\\ell = 1 - \\frac{\\ell}{m+1},\n\\quad\n\\widehat{\\gamma}_\\ell = \\frac{1}{n}\\sum_{t=1}^{n-\\ell} Y_t Y_{t+\\ell}.\n$$\n\n将稀疏化因子 $k \\in \\mathbb{N}$ 定义为仅保留马尔可夫链中每第 $k$ 个抽样，同时消耗同样的总转移次数；在固定计算预算 $B$ 下，保留的样本大小为 $n = B/k$。对于稀疏化后的序列，同样的谱方差估计器适用，但现在应用于稀疏化后的指示序列，并使用其自身的截断参数 $m = \\lfloor n^{1/3} \\rfloor$。\n\n任务：\n- 对于每个指定的参数集 $(\\varphi, B, k)$，模拟 $\\mathcal{N}(0,1)$ 平稳自回归链的 $B$ 步。使用未稀疏化的抽样（保留所有 $B$ 个抽样，即 $k=1$）和稀疏化的抽样（保留每第 $k$ 个抽样）来计算每个可信区间端点 $p \\in \\{0.025, 0.975\\}$ 的近似蒙特卡洛标准误差。在每种情况下，对保留的样本大小 $n$ 使用带有 $m = \\lfloor n^{1/3} \\rfloor$ 的Bartlett谱方差估计器。使用平稳密度 $f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2}$ 在真实的 $x_p$ 处评估 $f(x_p)$。\n- 对于每个参数集，计算比率\n$$\nR_p(\\varphi, B, k) = \\frac{\\mathrm{MCSE}_{\\text{thin}}(p)}{\\mathrm{MCSE}_{\\text{all}}(p)},\n$$\n其中 $\\mathrm{MCSE}_{\\text{thin}}(p)$ 使用大小为 $n = B/k$ 的稀疏化保留样本，而 $\\mathrm{MCSE}_{\\text{all}}(p)$ 使用全部 $B$ 个抽样。\n- 通过为每个参数集返回一个布尔值来汇总证据。如果两个端点在稀疏化下都没有显示出精度提升，则该布尔值为真，否则为假。此处形式化为 $R_p(\\varphi, B, k) \\ge 1 - \\epsilon$（对于 $p \\in \\{0.025, 0.975\\}$），容差 $\\epsilon = 0.02$ 用以考虑谱方差的有限样本估计变异性。\n\n定义和约束：\n- 马尔可夫链蒙特卡洛（MCMC）指的是构建一个马尔可夫链，其平稳分布等于目标后验分布；在这里，目标是 $\\mathcal{N}(0,1)$，链是参数为 $\\varphi$ 的自回归链。\n- 计算预算 $B$ 是执行的转移次数；除了整数步长之外没有其他单位。\n- 稀疏化因子 $k$ 是一个正整数；$k=1$ 意味着不进行稀疏化。\n- 可信区间端点是水平为 $p \\in \\{0.025, 0.975\\}$ 的分位数，以小数表示。\n\n实现程序以使用以下参数值的测试套件：\n1. $(\\varphi, B, k) = (0.0, 200000, 10)$: 独立抽样，中度稀疏化。\n2. $(\\varphi, B, k) = (0.5, 200000, 10)$: 中度自相关，中度稀疏化。\n3. $(\\varphi, B, k) = (0.95, 200000, 10)$: 高度自相关，中度稀疏化。\n4. $(\\varphi, B, k) = (0.95, 200000, 50)$: 高度自相关，重度稀疏化。\n5. $(\\varphi, B, k) = (0.0, 200000, 1)$: 独立抽样，无稀疏化（基准）。\n\n你的程序应该产生一行输出，包含用逗号分隔并括在方括号中的结果列表（例如，“[result1,result2,...]”），其中每个条目是与上述顺序中的参数集相对应的布尔值。",
            "solution": "该问题被评估为有效，它在马尔可夫链蒙特卡洛（MCMC）方法的理论中有科学依据，问题提法得当，包含了所有必要的定义和参数，且陈述客观。因此，我们可以着手解决。\n\n目标是实证评估稀疏化对从平稳马尔可夫链估计的可信区间端点统计精度的影响。一个常见的误解是，稀疏化通过减少自相关来提高精度。然而，对于固定的计算预算（即固定的马尔可夫链总转移次数），稀疏化会丢弃信息，这通常会导致精度下降（即蒙特卡洛标准误差，或MCSE，的增加）。\n\n此评估的方法如下：\n\n1.  **平稳过程模拟**：对于每个参数集 $(\\varphi, B, k)$，我们模拟总共 $B$ 步的平稳高斯一阶自回归过程，该过程定义为 $X_t = \\varphi X_{t-1} + \\sqrt{1 - \\varphi^2}\\,Z_t$。该过程从其平稳分布 $X_0 \\sim \\mathcal{N}(0,1)$ 初始化，确保整个生成的序列 $\\{X_t\\}_{t=1}^B$ 来自平稳分布 $\\mathcal{N}(0,1)$。\n\n2.  **样本生成**：从模拟序列中派生出两个样本：\n    *   一个“未稀疏化”或“全部”样本，包含全部 $B$ 个抽样，样本大小为 $n_{\\text{all}} = B$。\n    *   一个“稀疏化”样本，通过从原始序列中选取每第 $k$ 个抽样创建。其大小为 $n_{\\text{thin}} = B/k$。\n\n3.  **MCSE 估计**：分析的核心是计算分位数估计值 $\\hat{x}_p$ 的蒙特卡洛标准误差（MCSE）。问题提供了基于delta方法应用于马尔可夫链中心极限定理结果的公式：\n    $$\n    \\mathrm{MCSE}\\left(\\hat{x}_p\\right) \\approx \\sqrt{\\frac{\\sigma_{h_p}^2}{n\\, f(x_p)^2}}\n    $$\n    该公式的组成部分是：\n    *   $p \\in \\{0.025, 0.975\\}$：对应于 $95\\%$ 可信区间（$\\alpha=0.05$）的分位数水平。\n    *   $x_p$：真实分位数，$x_p = \\Phi^{-1}(p)$，其中 $\\Phi$ 是标准正态分布的累积分布函数（CDF）。\n    *   $f(x_p)$：标准正态概率密度函数（PDF）在真实分位数 $x_p$ 处的值。\n    *   $n$：样本中的抽样数量（$n_{\\text{all}}$ 或 $n_{\\text{thin}}$）。\n    *   $\\sigma_{h_p}^2$：分位数指示过程的长期方差，其中指示函数为 $h_p(x) = \\mathbf{1}\\{x \\le x_p\\}$。\n\n4.  **长期方差计算**：长期方差 $\\sigma_{h_p}^2$ 必须从马尔可夫链输出中估计。问题指定使用Bartlett谱方差估计器。这涉及到创建一个中心化指示序列 $Y_t = h_p(X_t) - p$，然后计算：\n    $$\n    \\widehat{\\sigma}_{h_p}^2 = \\widehat{\\gamma}_0 + 2 \\sum_{\\ell=1}^{m} w_\\ell\\, \\widehat{\\gamma}_\\ell\n    $$\n    其中滞后 $\\ell$ 阶的样本自协方差为 $\\widehat{\\gamma}_\\ell = \\frac{1}{n}\\sum_{t=1}^{n-\\ell} Y_t Y_{t+\\ell}$，Bartlett权重为 $w_\\ell = 1 - \\frac{\\ell}{m+1}$，且截断滞后为 $m = \\lfloor n^{1/3} \\rfloor$。此计算对每个样本（全部样本和稀疏化样本）独立执行。\n\n5.  **性能比較**：稀疏化的效果通过估计的MCSE的比率来量化：\n    $$\n    R_p(\\varphi, B, k) = \\frac{\\mathrm{MCSE}_{\\text{thin}}(p)}{\\mathrm{MCSE}_{\\text{all}}(p)}\n    $$\n    大于 $1$ 的比率表示稀疏化增加了估计误差，从而降低了精度。每个参数集的最终评估是一个布尔值，如果稀疏化对两个端点均未提供精度改进，则为 `True`，定义为 $R_p \\geq 1 - \\epsilon$（对于 $p=0.025$ 和 $p=0.975$），容差为 $\\epsilon=0.02$。\n\n提供的Python代码实现了这整个过程。它遍历指定的测试用例，执行AR(1)模拟，使用Bartlett估计器计算完整样本和稀疏化样本的MCSE，计算比率，并应用决策规则生成最终的布尔结果列表。使用固定的随机种子以确保模拟结果是可复现的。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef calculate_mcse(samples, p, x_p, f_at_xp):\n    \"\"\"\n    Calculates the Monte Carlo Standard Error for a quantile estimate using the\n    Bartlett spectral variance estimator.\n    \n    Args:\n        samples (np.ndarray): The MCMC sample draws.\n        p (float): The quantile level.\n        x_p (float): The true value of the quantile.\n        f_at_xp (float): The value of the stationary PDF at the true quantile.\n        \n    Returns:\n        float: The estimated MCSE of the quantile.\n    \"\"\"\n    n = len(samples)\n    if n == 0:\n        return np.inf\n\n    # Form the centered indicator series Y_t = 1{X_t = x_p} - p\n    Y = (samples = x_p).astype(np.float64) - p\n    \n    # Define truncation lag for Bartlett estimator\n    m = int(n**(1/3))\n    \n    # Calculate sample autocovariance at lag 0 (proportional to sample variance)\n    # The problem defines gamma_hat_l with normalization by n.\n    gamma_0_hat = np.sum(Y**2) / n\n    \n    auto_cov_sum = 0.0\n    if m > 0:\n        for l in range(1, m + 1):\n            # Bartlett kernel weight\n            w_l = 1.0 - l / (m + 1.0)\n            \n            # Sample autocovariance at lag l, normalized by n\n            gamma_l_hat = np.sum(Y[:-l] * Y[l:]) / n\n            auto_cov_sum += w_l * gamma_l_hat\n            \n    # Estimate the long-run variance (spectral density at frequency 0)\n    sigma_h_sq_hat = gamma_0_hat + 2 * auto_cov_sum\n    \n    # Calculate MCSE using the formula from the delta method\n    denominator = n * f_at_xp**2\n    if denominator == 0:\n        return np.inf\n        \n    mcse = np.sqrt(sigma_h_sq_hat / denominator)\n    \n    return mcse\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation and comparison for all test cases.\n    \"\"\"\n    # Set a seed for reproducibility of the simulation.\n    np.random.seed(42)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 200000, 10),\n        (0.5, 200000, 10),\n        (0.95, 200000, 10),\n        (0.95, 200000, 50),\n        (0.0, 200000, 1),\n    ]\n\n    # Define quantile levels and tolerance for the decision rule.\n    p_levels = [0.025, 0.975]\n    epsilon = 0.02\n\n    # Pre-compute true quantiles and density values for the N(0,1) distribution.\n    x_p_values = stats.norm.ppf(p_levels)\n    f_at_x_p_values = stats.norm.pdf(x_p_values)\n\n    results = []\n    \n    for phi, B, k in test_cases:\n        # 1. Simulate the stationary AR(1) chain of length B.\n        X_all = np.zeros(B)\n        x_current = np.random.randn() # Start from stationary distribution X_0 ~ N(0,1)\n        sqrt_term = np.sqrt(1 - phi**2)\n        \n        for i in range(B):\n            Z = np.random.randn()\n            x_current = phi * x_current + sqrt_term * Z\n            X_all[i] = x_current\n            \n        # 2. Prepare the thinned sample by taking every k-th draw.\n        X_thin = X_all[::k]\n\n        # 3. Calculate MCSE for both samples and for both quantile endpoints.\n        mcse_all_list = []\n        mcse_thin_list = []\n\n        for p, x_p, f_at_xp in zip(p_levels, x_p_values, f_at_x_p_values):\n            # MCSE for the full sample (all draws)\n            mcse_all = calculate_mcse(X_all, p, x_p, f_at_xp)\n            mcse_all_list.append(mcse_all)\n            \n            # MCSE for the thinned sample\n            mcse_thin = calculate_mcse(X_thin, p, x_p, f_at_xp)\n            mcse_thin_list.append(mcse_thin)\n\n        # 4. Compute ratios of MCSEs.\n        ratios = np.array(mcse_thin_list) / np.array(mcse_all_list)\n        \n        # 5. Apply the decision rule: True if thinning does not improve precision\n        # for both endpoints, within the given tolerance.\n        decision = np.all(ratios >= 1.0 - epsilon)\n        results.append(decision.item())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}