{
    "hands_on_practices": [
        {
            "introduction": "A common step in processing Markov Chain Monte Carlo (MCMC) output is thinning, which involves keeping only every $k$-th sample to reduce autocorrelation. While this is often presented as a standard procedure, its impact on the precision of your final estimates is not always beneficial. This exercise guides you through a computational experiment to explore the subtle trade-offs involved, demonstrating how thinning can increase the Monte Carlo Standard Error ($MCSE$) for a fixed computational budget, even as it successfully reduces autocorrelation. By simulating an autoregressive process and applying diagnostics, you will gain critical, hands-on experience in evaluating the practical consequences of MCMC post-processing decisions. ",
            "id": "3299623",
            "problem": "Construct a fully specified, self-contained computational experiment to analyze thinning in a stationary first-order autoregressive Markov chain. Consider a scalar quantity of interest with stationary distribution modeled by a standard normal random variable with mean $0$ and variance $1$. For each test case, you will simulate multiple independent chains using an autoregressive sampler of order one with autoregressive parameter $\\phi \\in (-1, 1)$ and independent Gaussian innovations. You will adopt the following base definitions and widely accepted facts:\n\n- A Markov chain Monte Carlo estimator of the mean uses the sample average of the retained draws.\n- The autocorrelation function at lag $t$, denoted $\\rho_t$, governs the variance inflation via the integrated autocorrelation time.\n- The Central Limit Theorem for Markov chains implies that the normalized sample mean of retained draws is approximately Gaussian for large samples, with a variance inflation controlled by the integrated autocorrelation time.\n- The Gelman–Rubin Potential Scale Reduction Factor (PSRF) (also called $\\hat{R}$) is a convergence diagnostic that compares between-chain and within-chain variances to assess whether multiple chains have converged to the same stationary distribution.\n- The Geweke convergence diagnostic compares early and late segments of a single chain via a standardized difference of means to assess consistency with stationarity.\n- The Heidelberger–Welch diagnostic conducts a stationarity test followed by a half-width decision for the estimated mean; here you will implement the half-width decision under the assumption of stationarity after burn-in.\n- The Raftery–Lewis diagnostic estimates the number of iterations required to estimate a specified quantile to a given accuracy and probability, accounting for dependence.\n\nYour simulation protocol must follow these principles:\n\n1. Generate $M$ independent chains of length $C$ transitions, each starting from a dispersed initial value (for example, each initial state drawn from a normal distribution with a larger variance than the target), and then apply a burn-in by discarding a fraction $b$ of the initial transitions. A retained series is formed by thinning with stride $k$, i.e., keeping every $k$-th post-burn-in state. The unthinned case corresponds to $k=1$.\n\n2. For a fixed candidate thinning stride $k$ and the unthinned case $k=1$, compute:\n   - The empirical lag-$1$ autocorrelation of the retained series for $k=1$ and $k$, denoted $\\hat{\\rho}_{1}^{(1)}$ and $\\hat{\\rho}_{1}^{(k)}$. Also report the theoretical ratio $\\rho_{1}^{(k)} / \\rho_{1}^{(1)}$ under the autoregressive model, where $\\rho_{t}$ denotes the true autocorrelation function of the process. Use this ratio to assess reduction in autocorrelation under thinning.\n   - The Monte Carlo Standard Error (MCSE) of the sample mean of the retained series for $k=1$ and $k$, defined via the integrated autocorrelation time and the variance of the stationary distribution. Report the ratio of the MCSEs, $\\mathrm{MCSE}^{(k)} / \\mathrm{MCSE}^{(1)}$, under a fixed total compute budget $C$. The integrated autocorrelation time must be derived from the autocorrelation function of the autoregressive model and the effect of thinning on the autocorrelation sequence.\n   - The Heidelberger–Welch half-width decision for an absolute half-width threshold $h_{\\text{target}}$ at significance level $\\alpha$, using a two-sided normal critical value. A pass occurs when the half-width is less than or equal to $h_{\\text{target}}$ for the estimated mean.\n   - The Gelman–Rubin PSRF for unthinned and thinned retained series, computed from the post-burn-in samples across $M$ chains. Determine whether thinning leaves $\\hat{R}$ unchanged within a tolerance $t_{\\hat{R}}$, i.e., whether the absolute difference between the two $\\hat{R}$ values is less than or equal to $t_{\\hat{R}}$.\n\n3. Under the fixed compute budget $C$, search integer thinning strides $k \\in \\{1, 2, \\dots, k_{\\max}\\}$ subject to a minimum retained sample size constraint $n_{\\min}$ after burn-in, and identify an optimal $k$ that minimizes the MCSE of the sample mean. In cases of ties, select the smallest $k$.\n\nUse a test suite with three scientifically plausible test cases. For each test case, specify parameters $(\\phi,C,M,b,k_{\\text{cand}},\\alpha,h_{\\text{target}},t_{\\hat{R}},k_{\\max},n_{\\min})$:\n\n- Test Case A (happy path): $(\\phi=0.95, C=100000, M=4, b=0.2, k_{\\text{cand}}=5, \\alpha=0.05, h_{\\text{target}}=0.01, t_{\\hat{R}}=0.01, k_{\\max}=20, n_{\\min}=500)$.\n- Test Case B (low dependence boundary): $(\\phi=0.2, C=100000, M=4, b=0.2, k_{\\text{cand}}=5, \\alpha=0.05, h_{\\text{target}}=0.005, t_{\\hat{R}}=0.01, k_{\\max}=20, n_{\\min}=500)$.\n- Test Case C (high dependence edge with tighter budget): $(\\phi=0.99, C=30000, M=4, b=0.5, k_{\\text{cand}}=10, \\alpha=0.05, h_{\\text{target}}=0.02, t_{\\hat{R}}=0.02, k_{\\max}=30, n_{\\min}=200)$.\n\nFor each test case, produce the following outputs:\n\n- A float equal to the theoretical ratio $\\rho_{1}^{(k_{\\text{cand}})} / \\rho_{1}^{(1)}$.\n- A float equal to the MCSE ratio $\\mathrm{MCSE}^{(k_{\\text{cand}})} / \\mathrm{MCSE}^{(1)}$.\n- A boolean indicating the Heidelberger–Welch half-width decision on the unthinned retained series.\n- A boolean indicating the Heidelberger–Welch half-width decision on the thinned retained series with stride $k_{\\text{cand}}$.\n- A boolean indicating whether the Gelman–Rubin PSRF remains unchanged under thinning within tolerance $t_{\\hat{R}}$.\n- An integer equal to the optimal thinning stride $k$ under the fixed compute budget and the minimum retained sample size constraint.\n\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list of three lists, each list corresponding to one test case and having the six results in the exact order specified, enclosed in square brackets and with floating-point values rounded to six decimal places. For example, the program must print a line of the form:\n\"[[r11,r12,b11,b12,b13,k1],[r21,r22,b21,b22,b23,k2],[r31,r32,b31,b32,b33,k3]]\"\nwhere $ r\\cdot\\cdot $ are floats, $ b\\cdot\\cdot $ are booleans, and $ k\\cdot $ are integers.",
            "solution": "The user has provided a valid, well-posed, and scientifically grounded problem in the domain of Markov Chain Monte Carlo (MCMC) methods. The task is to construct a computational experiment to analyze the effects of thinning on an MCMC sampler for a first-order autoregressive (AR(1)) process. The solution proceeds by first detailing the theoretical underpinnings and then implementing the simulation and analysis as specified.\n\n### Theoretical Framework\n\nThe problem centers on a stationary, first-order autoregressive process, $X_t$, defined by:\n$$\nX_t = \\phi X_{t-1} + \\epsilon_t\n$$\nwhere $\\phi \\in (-1, 1)$ is the autoregressive parameter and $\\epsilon_t$ are independent and identically distributed Gaussian innovations. The stationary distribution is specified as a standard normal, $X \\sim N(0, 1)$, which implies that $\\mathrm{Var}(X_t) = 1$. For an AR(1) process, the stationary variance is $\\frac{\\mathrm{Var}(\\epsilon_t)}{1-\\phi^2}$. Thus, we must have $\\mathrm{Var}(\\epsilon_t) = 1 - \\phi^2$, so $\\epsilon_t \\sim N(0, 1-\\phi^2)$.\n\nThe autocorrelation function (ACF) of this process at lag $t$ is given by $\\rho_t = \\phi^{|t|}$.\n\nThe quantities to be computed are derived as follows:\n\n**1. Theoretical Autocorrelation Ratio:**\nIf we thin the original series by keeping every $k$-th sample, the resulting series $Y_i = X_{ki}$ has an ACF given by $\\rho'_j = \\mathrm{Corr}(Y_i, Y_{i+j}) = \\mathrm{Corr}(X_{ki}, X_{k(i+j)}) = \\rho_{kj} = \\phi^{kj}$.\nThe lag-$1$ autocorrelation of the unthinned series ($k=1$) is $\\rho_1^{(1)} = \\rho_1 = \\phi$.\nThe lag-$1$ autocorrelation of the thinned series (stride $k$) is $\\rho_1^{(k)} = \\phi^k$.\nThe required theoretical ratio is therefore:\n$$\n\\frac{\\rho_1^{(k)}}{\\rho_1^{(1)}} = \\frac{\\phi^k}{\\phi} = \\phi^{k-1}\n$$\n\n**2. Theoretical Monte Carlo Standard Error (MCSE) Ratio:**\nThe MCSE of the sample mean from a stationary chain of length $n$ is $\\mathrm{MCSE} = \\sqrt{\\frac{\\sigma^2 \\tau}{n}}$, where $\\sigma^2=1$ is the process variance and $\\tau$ is the integrated autocorrelation time (IACT).\nThe IACT of the originalAR(1) process is:\n$$\n\\tau_1 = 1 + 2 \\sum_{t=1}^{\\infty} \\rho_t = 1 + 2 \\sum_{t=1}^{\\infty} \\phi^t = \\frac{1+\\phi}{1-\\phi}\n$$\nFor a series thinned by stride $k$, the IACT is:\n$$\n\\tau_k = 1 + 2 \\sum_{j=1}^{\\infty} \\rho'_j = 1 + 2 \\sum_{j=1}^{\\infty} \\phi^{kj} = \\frac{1+\\phi^k}{1-\\phi^k}\n$$\nUnder a fixed computational budget of $C$ total transitions and a burn-in fraction $b$, the number of unthinned post-burn-in samples is $n_1 = C(1-b)$. The number of thinned samples is $n_k \\approx \\frac{C(1-b)}{k}$.\nThe MCSE ratio is then:\n$$\n\\frac{\\mathrm{MCSE}^{(k)}}{\\mathrm{MCSE}^{(1)}} = \\frac{\\sqrt{\\sigma^2 \\tau_k / n_k}}{\\sqrt{\\sigma^2 \\tau_1 / n_1}} = \\sqrt{\\frac{\\tau_k n_1}{\\tau_1 n_k}} \\approx \\sqrt{\\frac{\\tau_k (k n_k)}{\\tau_1 n_k}} = \\sqrt{k \\frac{\\tau_k}{\\tau_1}}\n$$\nSubstituting the expressions for $\\tau_1$ and $\\tau_k$:\n$$\n\\frac{\\mathrm{MCSE}^{(k)}}{\\mathrm{MCSE}^{(1)}} \\approx \\sqrt{k \\frac{(1+\\phi^k)/(1-\\phi^k)}{(1+\\phi)/(1-\\phi)}}\n$$\n\n**3. Heidelberger–Welch Half-Width Decision:**\nThis diagnostic assesses if the sample mean is estimated with sufficient precision. Assuming the MCMC sample $\\{X_i\\}_{i=1}^N$ is stationary, the half-width of a $(1-\\alpha)$ confidence interval for the population mean is $H = z_{\\alpha/2} \\times \\mathrm{MCSE}(\\bar{X})$, where $z_{\\alpha/2}$ is the critical value from the standard normal distribution. The test passes if $H \\le h_{\\text{target}}$. The MCSE is estimated empirically from the simulated chain data. A robust method for this is the batch means estimator. The full sample of size $N$ is divided into $B$ batches of length $L$. The MCSE of the grand mean is then estimated as $\\sqrt{\\frac{\\hat{\\sigma}^2_{\\text{BM}}}{B}}$, where $\\hat{\\sigma}^2_{\\text{BM}}$ is the sample variance of the batch means.\n\n**4. Gelman–Rubin Potential Scale Reduction Factor (PSRF or $\\hat{R}$):**\nThis diagnostic compares the between-chain variance ($B$) to the within-chain variance ($W$) across $M$ parallel chains, each of post-burn-in length $n$.\n$$\n\\hat{R} = \\sqrt{\\frac{\\widehat{\\mathrm{Var}}^+}{W}} \\quad \\text{where} \\quad \\widehat{\\mathrm{Var}}^+ = \\frac{n-1}{n}W + \\frac{1}{n}B\n$$\nAn $\\hat{R}$ value close to $1$ suggests that the chains have converged to the same distribution. The analysis checks if the $\\hat{R}$ value remains stable (within tolerance $t_{\\hat{R}}$) when the chains are thinned.\n\n**5. Optimal Thinning Stride ($k$):**\nThe optimal stride $k$ is the one that minimizes the MCSE of the sample mean, under the constraints on $k$ ($k \\le k_{\\max}$) and retained sample size ($n'_k \\ge n_{\\min}$). Minimizing $\\mathrm{MCSE}(k) \\propto \\sqrt{\\frac{\\tau_k}{n'_k}}$ is equivalent to minimizing the objective function $f(k) = \\frac{\\tau_k}{n'_k}$, where $n'_k = \\lfloor (C(1-b)-1)/k \\rfloor + 1$. The search is performed over the valid range of $k$. For an AR(1) process, theory predicts that the minimum MCSE is always achieved at $k=1$, meaning no thinning is optimal.\n\n### Simulation Protocol\n\nFor each test case, the following steps are executed:\n1.  **Generation**: $M$ independent AR(1) chains of length $C$ are simulated. Initial values are drawn from a dispersed distribution $N(0, 100)$ to assess convergence from overdispersed starting points.\n2.  **Burn-in and Thinning**: The first $\\lfloor b \\times C \\rfloor$ samples of each chain are discarded. Unthinned ($k=1$) and thinned ($k=k_{\\text{cand}}$) series are created from the remaining samples.\n3.  **Analysis**: The theoretical ratios are computed directly from the formulas. The empirical diagnostics (Heidelberger-Welch, Gelman-Rubin) are computed from the simulated chain data. The optimal $k$ is found by minimizing the theoretical MCSE objective function over the allowed search space.\n4.  **Output Formatting**: The six results for each test case are collected and formatted into a single string as specified, with floating-point numbers rounded to six decimal places and booleans represented as lowercase `true` or `false`.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef calculate_rhat(chains):\n    \"\"\"\n    Calculates the Gelman-Rubin Potential Scale Reduction Factor (PSRF), or R-hat.\n    \"\"\"\n    M, n = chains.shape\n    if M <= 1 or n <= 1:\n        return 1.0\n\n    # Calculate within-chain means\n    psi_i_dot = np.mean(chains, axis=1)\n    \n    # Calculate overall mean\n    psi_dot_dot = np.mean(psi_i_dot)\n    \n    # Calculate between-chain variance\n    B = (n / (M - 1)) * np.sum((psi_i_dot - psi_dot_dot)**2)\n    \n    # Calculate average within-chain variance\n    s_i_sq = np.var(chains, axis=1, ddof=1)\n    W = np.mean(s_i_sq)\n\n    if np.isclose(W, 0):\n        return 1.0\n\n    # Estimate marginal posterior variance\n    var_hat_plus = ((n - 1) / n) * W + (1 / n) * B\n    \n    # Calculate PSRF\n    r_hat = np.sqrt(var_hat_plus / W)\n    return r_hat\n\ndef hw_halfwidth_test(series, alpha, h_target):\n    \"\"\"\n    Performs the Heidelberger-Welch half-width test using the batch means method.\n    \"\"\"\n    N = len(series)\n    # Require at least 4 samples for 2 batches of size 2\n    if N < 4:\n        return False\n        \n    B = int(np.floor(np.sqrt(N)))\n    if B < 2:\n        return False\n        \n    L = int(np.floor(N / B))\n    if L < 1:\n        return False\n    \n    N_eff = B * L\n    series_trimmed = series[:N_eff]\n    \n    batches = series_trimmed.reshape((B, L))\n    batch_means = np.mean(batches, axis=1)\n    \n    if batch_means.size < 2:\n        return False\n\n    var_bm = np.var(batch_means, ddof=1)\n    mcse = np.sqrt(var_bm / B)\n    \n    z_alpha_2 = norm.ppf(1 - alpha / 2)\n    half_width = z_alpha_2 * mcse\n    \n    return half_width <= h_target\n\ndef find_optimal_k(phi, C, b, k_max, n_min):\n    \"\"\"\n    Finds the optimal thinning stride k that minimizes theoretical MCSE.\n    \"\"\"\n    N_post = C - int(np.floor(C * b))\n    \n    best_k = -1\n    min_mcse_metric = float('inf')\n    \n    for k in range(1, k_max + 1):\n        n_k_per_chain = int(np.floor((N_post - 1) / k)) + 1\n        \n        if n_k_per_chain < n_min:\n            continue\n            \n        if abs(1 - phi**k) < 1e-12:\n           continue\n        \n        tau_k = (1 + phi**k) / (1 - phi**k)\n        mcse_metric = tau_k / n_k_per_chain\n        \n        if mcse_metric < min_mcse_metric:\n            min_mcse_metric = mcse_metric\n            best_k = k\n            \n    return best_k if best_k != -1 else 1\n\ndef run_analysis(params, seed=42):\n    \"\"\"\n    Runs the full simulation and analysis for a single test case.\n    \"\"\"\n    phi, C, M, b, k_cand, alpha, h_target, t_R, k_max, n_min = params\n    \n    rng = np.random.RandomState(seed)\n\n    # 1. Simulate M chains of length C for an AR(1) process\n    chains = np.zeros((M, C))\n    chains[:, 0] = rng.normal(0, 10.0, size=M) # Dispersed initial values\n    sigma_eps = np.sqrt(max(0, 1 - phi**2))\n    \n    for i in range(M):\n        innovations = rng.normal(0, sigma_eps, size=C-1)\n        for t in range(1, C):\n            chains[i, t] = phi * chains[i, t-1] + innovations[t-1]\n            \n    # 2. Extract post-burn-in samples\n    N_burn = int(np.floor(C * b))\n    post_burn_chains = chains[:, N_burn:]\n    \n    # 3. Perform calculations for the six required outputs\n\n    # Output 1: Theoretical lag-1 autocorrelation ratio\n    theo_acf_ratio = phi**(k_cand - 1)\n    \n    # Output 2: Theoretical MCSE ratio\n    if abs(1 - phi) < 1e-9:\n        theo_mcse_ratio = 1.0 # The limit as phi -> 1\n    else:\n        tau_1 = (1 + phi) / (1 - phi)\n        tau_k = (1 + phi**k_cand) / (1 - phi**k_cand)\n        theo_mcse_ratio = np.sqrt(k_cand * (tau_k / tau_1))\n\n    # Output 3 & 4: Heidelberger–Welch half-width decisions\n    pooled_unthinned = post_burn_chains.flatten()\n    hw_unthinned_pass = hw_halfwidth_test(pooled_unthinned, alpha, h_target)\n    \n    thinned_chains = post_burn_chains[:, ::k_cand]\n    pooled_thinned = thinned_chains.flatten()\n    hw_thinned_pass = hw_halfwidth_test(pooled_thinned, alpha, h_target)\n    \n    # Output 5: Gelman–Rubin PSRF stability\n    rhat_unthinned = calculate_rhat(post_burn_chains)\n    rhat_thinned = calculate_rhat(thinned_chains)\n    gr_stable = abs(rhat_thinned - rhat_unthinned) <= t_R\n\n    # Output 6: Optimal thinning stride k\n    optimal_k = find_optimal_k(phi, C, b, k_max, n_min)\n    \n    return [\n        theo_acf_ratio, theo_mcse_ratio, hw_unthinned_pass,\n        hw_thinned_pass, gr_stable, optimal_k\n    ]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (phi, C, M, b, k_cand, alpha, h_target, t_R, k_max, n_min)\n        (0.95, 100000, 4, 0.2, 5, 0.05, 0.01, 0.01, 20, 500), # Case A\n        (0.2, 100000, 4, 0.2, 5, 0.05, 0.005, 0.01, 20, 500), # Case B\n        (0.99, 30000, 4, 0.5, 10, 0.05, 0.02, 0.02, 30, 200), # Case C\n    ]\n\n    # Process all test cases and collect results\n    all_results_str = []\n    for case in test_cases:\n        result = run_analysis(case)\n        # Format each result item into a string\n        r1_str = f\"{result[0]:.6f}\"\n        r2_str = f\"{result[1]:.6f}\"\n        b1_str = str(result[2]).lower()\n        b2_str = str(result[3]).lower()\n        b3_str = str(result[4]).lower()\n        k_str = str(result[5])\n        \n        # Create a string representation of the list for this case\n        case_str = f\"[{','.join([r1_str, r2_str, b1_str, b2_str, b3_str, k_str])}]\"\n        all_results_str.append(case_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "MCMC convergence diagnostics are essential for ensuring the reliability of simulation results, but their performance can degrade when underlying assumptions are violated. This practice moves from applying diagnostics to critically analyzing their theoretical foundations. You will investigate the behavior of the Geweke diagnostic test under the challenging scenario of heavy-tailed innovations, where the variance may not exist, causing classical mean-based statistics to fail. By deriving the asymptotic properties of both the standard test and a robust alternative based on medians, you will develop a deeper understanding of how statistical tests are constructed and why robustness is a crucial consideration in modern data analysis. ",
            "id": "3299577",
            "problem": "Consider a Markov Chain Monte Carlo (MCMC) convergence diagnostic based on comparing two non-overlapping windows from an independent and identically distributed (i.i.d.) sequence of innovations $\\{X_t\\}$ having a Student $t_{\\nu}$ distribution with degrees-of-freedom $\\nu \\in \\{2, 3, 5\\}$, centered to have mean $0$. Assume there is no autocorrelation so that the spectral density at frequency zero equals the variance whenever the variance exists. Let the early window consist of $L$ observations and the late window consist of another $L$ observations, and let $\\alpha = 0.05$ denote the two-sided significance level with corresponding standard normal critical value $z_{1-\\alpha/2}$.\n\nThe classical Geweke diagnostic computes\n$$\nZ_{\\mathrm{GR}} \\equiv \\frac{\\bar{X}_{A}-\\bar{X}_{B}}{\\sqrt{\\hat{V}_{A}/L+\\hat{V}_{B}/L}},\n$$\nwhere $\\bar{X}_{A}$ and $\\bar{X}_{B}$ are sample means in the early and late windows, respectively, and $\\hat{V}_{A}$ and $\\hat{V}_{B}$ are consistent estimators of the spectral variance (which, under independence, reduces to the variance of $X_{t}$ when it exists). A rejection occurs if $|Z_{\\mathrm{GR}}| > z_{1-\\alpha/2}$.\n\nFor heavy-tailed innovations, the classical mean-based statistic may be unreliable when the variance does not exist. To mitigate heavy-tail effects, consider the following robust median-of-means (MoM) variant constructed at the window level: partition the early window into $L$ blocks of size $1$ (so that each block mean is just one observation), compute the median of these $L$ block means to obtain $M_{A}$, and do the same for the late window to obtain $M_{B}$. Under i.i.d. sampling from a continuous distribution with probability density function $f_{\\nu}(x)$ at the median (here equal to $0$ by symmetry), the sample median $M$ of $L$ observations has the asymptotic distribution\n$$\n\\sqrt{L}\\,\\big(M - 0\\big) \\;\\xrightarrow{d}\\; \\mathcal{N}\\!\\left(0,\\;\\frac{1}{4\\,f_{\\nu}(0)^{2}}\\right),\n$$\nprovided $f_{\\nu}(0)\\in(0,\\infty)$. Define the robust test statistic\n$$\nZ_{\\mathrm{MoM}} \\equiv \\frac{M_{A}-M_{B}}{\\sqrt{\\mathrm{Var}(M_{A})+\\mathrm{Var}(M_{B})}} \\approx \\sqrt{2L}\\,f_{\\nu}(0)\\,\\big(M_{A}-M_{B}\\big),\n$$\nand reject if $|Z_{\\mathrm{MoM}}| > z_{1-\\alpha/2}$.\n\nThe Student $t_{\\nu}$ probability density function at $x=0$ (unit scale) is\n$$\nf_{\\nu}(0) \\;=\\; \\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\,\\sqrt{\\nu\\pi}}.\n$$\n\nStarting from the Central Limit Theorem (CLT) for the sample mean when the variance exists and the asymptotic normality of the sample median described above, derive the asymptotic two-sided false positive rates (type I error probabilities) of the classical $Z_{\\mathrm{GR}}$ test for $\\nu \\in \\{3, 5\\}$ and of the robust $Z_{\\mathrm{MoM}}$ test for $\\nu \\in \\{2, 3, 5\\}$. Express your final answer as a single row matrix containing, in order,\n$$\np_{\\mathrm{GR}}(3),\\; p_{\\mathrm{GR}}(5),\\; p_{\\mathrm{MoM}}(2;L),\\; p_{\\mathrm{MoM}}(3;L),\\; p_{\\mathrm{MoM}}(5;L),\n$$\nwhere each entry is the asymptotic false positive rate under the corresponding test and $\\nu$. Use $\\alpha=0.05$, and treat $L$ as fixed but large enough to justify the asymptotic approximations. No rounding is needed; provide exact decimal values. If needed, you may assume the consistency of $\\hat{V}_{A}$ and $\\hat{V}_{B}$ under $\\nu\\in\\{3,5\\}$ and the regularity conditions ensuring $f_{\\nu}(0)\\in(0,\\infty)$ for the median-of-means construction under $\\nu\\in\\{2,3,5\\}$.",
            "solution": "The problem requires the derivation of the asymptotic two-sided false positive rates, also known as Type I error probabilities, for two different convergence diagnostic test statistics, $Z_{\\mathrm{GR}}$ and $Z_{\\mathrm{MoM}}$. The false positive rate is the probability of rejecting the null hypothesis when it is true.\n\nThe setup of the problem assumes an underlying sequence of independent and identically distributed (i.i.d.) innovations $\\{X_t\\}$. A test for convergence compares an early window of observations (A) with a late window (B). The null hypothesis, $H_0$, is that the stochastic process has converged to its stationary distribution, meaning that observations in both windows are drawn from the same underlying distribution. The problem statement itself posits that the entire sequence is i.i.d., which is the ideal realization of the null hypothesis. Therefore, we are asked to calculate $P(\\text{reject } H_0 | H_0 \\text{ is true})$.\n\nFor both tests, the rejection rule is of the form $|Z| > z_{1-\\alpha/2}$, where $Z$ is the test statistic, $\\alpha = 0.05$ is the significance level, and $z_{1-\\alpha/2} = z_{0.975}$ is the corresponding critical value from the standard normal distribution, $\\mathcal{N}(0,1)$. By definition of the critical value for a two-sided test, if a random variable $W$ follows a standard normal distribution, then $P(|W| > z_{1-\\alpha/2}) = \\alpha$.\n\nConsequently, the core of the problem is to determine the asymptotic distribution of each test statistic under the given conditions. If a statistic $Z$ converges in distribution to $\\mathcal{N}(0,1)$ as the sample size $L \\to \\infty$, its asymptotic Type I error rate will be exactly $\\alpha$.\n\nFirst, we analyze the classical Geweke-style statistic, $Z_{\\mathrm{GR}}$, for $\\nu \\in \\{3,5\\}$.\n$$\nZ_{\\mathrm{GR}} \\equiv \\frac{\\bar{X}_{A}-\\bar{X}_{B}}{\\sqrt{\\hat{V}_{A}/L+\\hat{V}_{B}/L}}\n$$\nThe innovations $\\{X_t\\}$ are drawn from a Student's $t$-distribution with $\\nu$ degrees of freedom, centered to have mean $\\mu=0$. The variance of a $t_{\\nu}$ distribution is $\\sigma^2 = \\frac{\\nu}{\\nu-2}$, which is finite only for $\\nu > 2$.\nFor $\\nu=3$, the variance is $\\sigma^2 = \\frac{3}{3-2} = 3$.\nFor $\\nu=5$, the variance is $\\sigma^2 = \\frac{5}{5-2} = \\frac{5}{3}$.\nIn both of these cases, the variance is finite.\n\nSince the observations are i.i.d. and have finite mean and variance, the Central Limit Theorem (CLT) applies to the sample means $\\bar{X}_{A}$ and $\\bar{X}_{B}$. As the window size $L \\to \\infty$:\n$$\n\\sqrt{L}(\\bar{X}_{A} - \\mu) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2) \\quad \\text{and} \\quad \\sqrt{L}(\\bar{X}_{B} - \\mu) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2)\n$$\nThis implies that for large $L$, $\\bar{X}_{A}$ is approximately distributed as $\\mathcal{N}(0, \\sigma^2/L)$ and $\\bar{X}_{B}$ is approximately distributed as $\\mathcal{N}(0, \\sigma^2/L)$. Since the two windows are non-overlapping and the data are i.i.d., $\\bar{X}_{A}$ and $\\bar{X}_{B}$ are independent. The distribution of their difference is therefore:\n$$\n\\bar{X}_{A} - \\bar{X}_{B} \\sim \\mathcal{N}\\left(0, \\frac{\\sigma^2}{L} + \\frac{\\sigma^2}{L}\\right) = \\mathcal{N}\\left(0, \\frac{2\\sigma^2}{L}\\right)\n$$\nThe denominator of $Z_{\\mathrm{GR}}$ involves $\\hat{V}_{A}$ and $\\hat{V}_{B}$, which are given as consistent estimators of the spectral variance. For an i.i.d. sequence, the spectral variance is simply the process variance, $\\sigma^2$. Thus, as $L \\to \\infty$, $\\hat{V}_{A} \\to \\sigma^2$ and $\\hat{V}_{B} \\to \\sigma^2$ in probability. By Slutsky's theorem, we can replace the estimated variances in the denominator with their true values for the asymptotic analysis:\n$$\nZ_{\\mathrm{GR}} = \\frac{\\bar{X}_{A}-\\bar{X}_{B}}{\\sqrt{\\hat{V}_{A}/L+\\hat{V}_{B}/L}} \\xrightarrow{d} \\frac{\\mathcal{N}(0, 2\\sigma^2/L)}{\\sqrt{\\sigma^2/L+\\sigma^2/L}} = \\frac{\\mathcal{N}(0, 2\\sigma^2/L)}{\\sqrt{2\\sigma^2/L}} = \\mathcal{N}(0,1)\n$$\nSince the statistic $Z_{\\mathrm{GR}}$ is asymptotically standard normal for both $\\nu=3$ and $\\nu=5$, the probability of rejecting $H_0$ is, by definition, the significance level $\\alpha$.\n$$\np_{\\mathrm{GR}}(3) = \\lim_{L\\to\\infty} P(|Z_{\\mathrm{GR}}| > z_{1-\\alpha/2}) = \\alpha = 0.05\n$$\n$$\np_{\\mathrm{GR}}(5) = \\lim_{L\\to\\infty} P(|Z_{\\mathrm{GR}}| > z_{1-\\alpha/2}) = \\alpha = 0.05\n$$\n\nNext, we analyze the robust median-of-means statistic, $Z_{\\mathrm{MoM}}$, for $\\nu \\in \\{2,3,5\\}$.\nThe statistic is defined as:\n$$\nZ_{\\mathrm{MoM}} \\approx \\sqrt{2L}\\,f_{\\nu}(0)\\,\\big(M_{A}-M_{B}\\big)\n$$\nHere, $M_A$ and $M_B$ are the sample medians of the observations in the early and late windows, respectively. The problem provides the asymptotic distribution for a sample median $M$ of $L$ i.i.d. observations from a continuous distribution with median $0$ and density $f_{\\nu}(x)$:\n$$\n\\sqrt{L}\\,M \\xrightarrow{d} \\mathcal{N}\\left(0, \\frac{1}{4\\,f_{\\nu}(0)^{2}}\\right)\n$$\nThis holds provided $f_{\\nu}(0) \\in (0, \\infty)$. The problem states this condition holds for $\\nu \\in \\{2,3,5\\}$.\nThis asymptotic distribution implies that for large $L$, $M_A$ and $M_B$ are approximately distributed as:\n$$\nM_A \\sim \\mathcal{N}\\left(0, \\frac{1}{4L\\,f_{\\nu}(0)^{2}}\\right) \\quad \\text{and} \\quad M_B \\sim \\mathcal{N}\\left(0, \\frac{1}{4L\\,f_{\\nu}(0)^{2}}\\right)\n$$\nAs before, $M_A$ and $M_B$ are independent. The distribution of their difference is:\n$$\nM_{A} - M_{B} \\sim \\mathcal{N}\\left(0, \\frac{1}{4L\\,f_{\\nu}(0)^{2}} + \\frac{1}{4L\\,f_{\\nu}(0)^{2}}\\right) = \\mathcal{N}\\left(0, \\frac{2}{4L\\,f_{\\nu}(0)^{2}}\\right) = \\mathcal{N}\\left(0, \\frac{1}{2L\\,f_{\\nu}(0)^{2}}\\right)\n$$\nThe test statistic $Z_{\\mathrm{MoM}}$ is a scaled version of this difference. Let $Y = M_A - M_B$. Then $Z_{\\mathrm{MoM}} = cY$, where the scaling constant is $c = \\sqrt{2L}\\,f_{\\nu}(0)$. The asymptotic distribution of $Z_{\\mathrm{MoM}}$ is normal with mean $c \\cdot E[Y] = 0$ and variance $c^2 \\cdot \\mathrm{Var}(Y)$.\n$$\n\\mathrm{Var}(Z_{\\mathrm{MoM}}) = \\left(\\sqrt{2L}\\,f_{\\nu}(0)\\right)^2 \\mathrm{Var}(M_A - M_B) = \\left(2L\\,f_{\\nu}(0)^2\\right) \\left(\\frac{1}{2L\\,f_{\\nu}(0)^2}\\right) = 1\n$$\nThus, the statistic $Z_{\\mathrm{MoM}}$ is constructed to be asymptotically standard normal, regardless of the specific value of $\\nu$, as long as the conditions for the asymptotic normality of the median hold.\n$$\nZ_{\\mathrm{MoM}} \\xrightarrow{d} \\mathcal{N}(0,1)\n$$\nThis result is valid for $\\nu=2$ (where the variance is infinite), as well as for $\\nu=3$ and $\\nu=5$. The robustness of the median ensures a well-behaved asymptotic distribution even when the second moment of the parent distribution does not exist.\nSince $Z_{\\mathrm{MoM}}$ is asymptotically standard normal for all three cases considered, the asymptotic Type I error rate is $\\alpha$.\n$$\np_{\\mathrm{MoM}}(2; L) = \\lim_{L\\to\\infty} P(|Z_{\\mathrm{MoM}}| > z_{1-\\alpha/2}) = \\alpha = 0.05\n$$\n$$\np_{\\mathrm{MoM}}(3; L) = \\lim_{L\\to\\infty} P(|Z_{\\mathrm{MoM}}| > z_{1-\\alpha/2}) = \\alpha = 0.05\n$$\n$$\np_{\\mathrm{MoM}}(5; L) = \\lim_{L\\to\\infty} P(|Z_{\\mathrm{MoM}}| > z_{1-\\alpha/2}) = \\alpha = 0.05\n$$\nBoth tests are constructed to have a nominal asymptotic Type I error rate of $\\alpha$. The derivation confirms that they achieve this rate under the specified conditions.\n\nThe final answer is the ordered collection of these five rates.\n$p_{\\mathrm{GR}}(3) = 0.05$\n$p_{\\mathrm{GR}}(5) = 0.05$\n$p_{\\mathrm{MoM}}(2;L) = 0.05$\n$p_{\\mathrm{MoM}}(3;L) = 0.05$\n$p_{\\mathrm{MoM}}(5;L) = 0.05$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.05 & 0.05 & 0.05 & 0.05 & 0.05\n\\end{pmatrix}\n}\n$$"
        }
    ]
}