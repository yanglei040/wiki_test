{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation in Markov chain theory, we begin with the most fundamental question: how does the distribution of a chain's state evolve over time? This exercise  invites you to analyze a Gaussian random walk on $\\mathbb{R}$, a cornerstone model in stochastic processes. By deriving the $n$-step distribution from first principles, you will gain hands-on experience with the mechanics of transition kernels and see the Chapman–Kolmogorov equations in action through the convolution of probability distributions.",
            "id": "3319838",
            "problem": "Consider a time-homogeneous Markov chain $\\{X_{n}\\}_{n \\geq 0}$ on $\\mathbb{R}$ with transition kernel $P(x, \\cdot) = \\mathcal{N}(x, \\sigma^{2})$, where $\\sigma^{2} \\in (0, \\infty)$ is fixed, and with deterministic initialization $X_{0} = x_{0} \\in \\mathbb{R}$. That is, for each $n \\in \\mathbb{N}$ and each Borel set $A \\subset \\mathbb{R}$, the one-step transition probability satisfies $P(x, A) = \\int_{A} \\varphi_{\\sigma}(y - x) \\, dy$ with $\\varphi_{\\sigma}(u) = (2 \\pi \\sigma^{2})^{-1/2} \\exp\\!\\big(-u^{2}/(2 \\sigma^{2})\\big)$. Starting only from the fundamental definitions of a time-homogeneous Markov chain, the Chapman–Kolmogorov equations, and basic facts about the normal distribution and convolution of independent random variables, derive the $n$-step law of $X_{n}$ under $P^{n}(x_{0}, \\cdot)$, identify its distribution in closed form, and obtain its mean and variance in terms of $x_{0}$, $n$, and $\\sigma^{2}$. \n\nYour final reported answer must be the ordered pair $\\big(\\mathbb{E}[X_{n}], \\operatorname{Var}(X_{n})\\big)$ written as a single row matrix using the $\\mathrm{pmatrix}$ environment. Do not include the distributional form in the final answer; provide that within your derivation. No rounding is required. Express the final answer using exact symbolic quantities $x_{0}$, $n$, and $\\sigma^{2}$.",
            "solution": "We begin from the definition of a time-homogeneous Markov chain on $\\mathbb{R}$ with transition kernel $P(x, \\cdot)$. The chain satisfies, for all $n \\in \\mathbb{N}$, all bounded measurable $f$, and all $x \\in \\mathbb{R}$,\n$$\n\\mathbb{E}\\!\\left[f(X_{n+1}) \\mid X_{n} = x \\right] \\equiv \\int_{\\mathbb{R}} f(y) \\, P(x, dy).\n$$\nHere the kernel is $P(x, \\cdot) = \\mathcal{N}(x, \\sigma^{2})$, meaning that, conditional on $X_{n} = x$, the next state $X_{n+1}$ has the normal distribution with mean $x$ and variance $\\sigma^{2}$. Equivalently, one may realize the dynamics by introducing an independent and identically distributed sequence $\\{Z_{k}\\}_{k \\geq 1}$ with $Z_{k} \\sim \\mathcal{N}(0, \\sigma^{2})$, independent of $X_{0}$, and writing\n$$\nX_{n+1} = X_{n} + Z_{n+1}, \\quad n \\geq 0.\n$$\nThis representation is consistent with $P(x, \\cdot)$ because for any Borel set $A \\subset \\mathbb{R}$,\n$$\n\\mathbb{P}\\!\\left(X_{n+1} \\in A \\mid X_{n} = x \\right) = \\mathbb{P}\\!\\left(x + Z_{n+1} \\in A \\right) = \\int_{A} \\varphi_{\\sigma}(y - x) \\, dy = P(x, A),\n$$\nwhere $\\varphi_{\\sigma}(u) = (2 \\pi \\sigma^{2})^{-1/2} \\exp\\!\\big(-u^{2}/(2 \\sigma^{2})\\big)$ is the normal density with mean $0$ and variance $\\sigma^{2}$.\n\nIterating the update shows that for any $n \\in \\mathbb{N}$,\n$$\nX_{n} = X_{0} + \\sum_{k=1}^{n} Z_{k} = x_{0} + \\sum_{k=1}^{n} Z_{k}.\n$$\nSince $\\{Z_{k}\\}_{k \\geq 1}$ are independent and each $Z_{k} \\sim \\mathcal{N}(0, \\sigma^{2})$, their sum $\\sum_{k=1}^{n} Z_{k}$ is normal with mean $0$ and variance $n \\sigma^{2}$. There are several equivalent fundamental ways to establish this:\n\n- Convolution of independent normal distributions: the convolution of $\\mathcal{N}(0, \\sigma^{2})$ with itself $n$ times is $\\mathcal{N}(0, n \\sigma^{2})$.\n\n- Characteristic functions: the characteristic function of $Z_{k}$ is $\\phi_{Z}(t) = \\mathbb{E}[\\exp(i t Z_{k})] = \\exp\\!\\big(-\\tfrac{1}{2} \\sigma^{2} t^{2}\\big)$ for $t \\in \\mathbb{R}$. Independence gives\n$$\n\\phi_{\\sum_{k=1}^{n} Z_{k}}(t) = \\prod_{k=1}^{n} \\phi_{Z}(t) = \\left(\\exp\\!\\big(-\\tfrac{1}{2} \\sigma^{2} t^{2}\\big)\\right)^{n} = \\exp\\!\\big(-\\tfrac{1}{2} n \\sigma^{2} t^{2}\\big),\n$$\nwhich is the characteristic function of $\\mathcal{N}(0, n \\sigma^{2})$.\n\nAdding the constant shift $x_{0}$ translates the distribution by $x_{0}$. Hence the law of $X_{n}$ is\n$$\nX_{n} \\sim \\mathcal{N}\\!\\big(x_{0}, \\, n \\sigma^{2}\\big).\n$$\nThis identifies the $n$-step transition kernel as\n$$\nP^{n}(x_{0}, \\cdot) = \\mathcal{N}\\!\\big(x_{0}, \\, n \\sigma^{2}\\big),\n$$\nwhich also follows from the Chapman–Kolmogorov equations by $n$-fold convolution of the one-step kernel with itself.\n\nFrom the identified distribution, the mean and variance follow immediately:\n$$\n\\mathbb{E}[X_{n}] = x_{0}, \\qquad \\operatorname{Var}(X_{n}) = n \\sigma^{2}.\n$$\nThese formulas are consistent with the base case $n = 0$, where $X_{0} = x_{0}$ is degenerate with variance $0$, and they satisfy the additive variance growth expected from the sum of $n$ independent increments each of variance $\\sigma^{2}$.\n\nTherefore, the requested ordered pair $\\big(\\mathbb{E}[X_{n}], \\operatorname{Var}(X_{n})\\big)$ is $\\big(x_{0}, \\, n \\sigma^{2}\\big)$.",
            "answer": "$$\\boxed{\\begin{pmatrix} x_{0}  n \\sigma^{2} \\end{pmatrix}}$$"
        },
        {
            "introduction": "After exploring the chain's transient dynamics, we focus on its long-term behavior, which is the essence of MCMC. A critical task is to quantify the uncertainty of our estimates. This practice  demystifies the concept of asymptotic variance by connecting it directly to the spectral properties of the transition operator. By explicitly calculating this variance for a simple two-state reversible chain, you will see how the chain's eigenvalues dictate the performance of MCMC estimators.",
            "id": "3319839",
            "problem": "Consider a time-homogeneous, irreducible, aperiodic Markov chain with finite state space $\\mathcal{X}$, transition kernel $P(x,y)$, and stationary distribution $\\pi$. Assume $P$ is reversible with respect to $\\pi$, meaning $P$ satisfies detailed balance: $\\pi(x) P(x,y) = \\pi(y) P(y,x)$ for all $x,y \\in \\mathcal{X}$. Let $P$ act on $L^{2}(\\pi)$ via $(P g)(x) = \\sum_{y \\in \\mathcal{X}} P(x,y) g(y)$, and define the $L^{2}(\\pi)$ inner product by $\\langle g, h \\rangle_{\\pi} = \\sum_{x \\in \\mathcal{X}} \\pi(x) g(x) h(x)$. For a measurable function $f : \\mathcal{X} \\to \\mathbb{R}$ with $\\mathbb{E}_{\\pi}[f] = \\sum_{x \\in \\mathcal{X}} \\pi(x) f(x)$ finite, consider the ergodic averages $\\bar{f}_{n} = \\frac{1}{n} \\sum_{t=1}^{n} f(X_{t})$, where $(X_{t})$ is the Markov chain with transition kernel $P$ started in stationarity.\n\nAssume the central limit theorem for Markov chains applies to $f$ so that $\\sqrt{n}(\\bar{f}_{n} - \\mathbb{E}_{\\pi}[f])$ converges in distribution to a centered normal random variable with asymptotic variance $\\sigma^{2}_{f}$. Starting from the definitions above and using only foundational facts about self-adjoint linear operators on Hilbert spaces and spectral measures, derive a representation of $\\sigma^{2}_{f}$ in terms of the spectral measure associated with $P$ and the centered function $g = f - \\mathbb{E}_{\\pi}[f]$.\n\nThen, specialize to a two-state reversible chain $\\mathcal{X} = \\{0,1\\}$ with\n$$\nP = \\begin{pmatrix}\n1 - \\alpha  \\alpha \\\\\n\\beta  1 - \\beta\n\\end{pmatrix},\n$$\nwhere $\\alpha \\in (0,1)$ and $\\beta \\in (0,1)$, so the stationary distribution is $\\pi(0) = \\frac{\\beta}{\\alpha + \\beta}$ and $\\pi(1) = \\frac{\\alpha}{\\alpha + \\beta}$. Let $f(0) = u$ and $f(1) = v$ with $u, v \\in \\mathbb{R}$. Compute a closed-form expression for the asymptotic variance $\\sigma^{2}_{f}$ of $\\bar{f}_{n}$ as a function of $\\alpha$, $\\beta$, $u$, and $v$. Express your final answer as a single analytic expression. If you carry out any intermediate simplifications, ensure all steps are valid for $\\alpha \\in (0,1)$ and $\\beta \\in (0,1)$.",
            "solution": "### Part 1: Derivation of the Asymptotic Variance in Spectral Form\n\nThe asymptotic variance of the ergodic average $\\bar{f}_n$ is given by the sum of autocovariances:\n$$ \\sigma_f^2 = \\sum_{k=-\\infty}^{\\infty} \\text{Cov}_{\\pi}(f(X_0), f(X_k)) $$\nSince the chain is stationary, $\\text{Cov}_{\\pi}(f(X_0), f(X_k)) = \\text{Cov}_{\\pi}(f(X_0), f(X_{-k}))$, so we can write:\n$$ \\sigma_f^2 = \\text{Var}_{\\pi}(f(X_0)) + 2 \\sum_{k=1}^{\\infty} \\text{Cov}_{\\pi}(f(X_0), f(X_k)) $$\nLet $g = f - \\mathbb{E}_{\\pi}[f]$ be the centered function. We have $\\mathbb{E}_{\\pi}[g] = 0$, so $\\sigma_f^2 = \\sigma_g^2$. The variance and covariance terms can be expressed using the $L^2(\\pi)$ inner product.\n$\\text{Var}_{\\pi}(f(X_0)) = \\mathbb{E}_{\\pi}[g(X_0)^2] = \\sum_x \\pi(x) g(x)^2 = \\langle g, g \\rangle_{\\pi}$.\nThe covariance is $\\text{Cov}_{\\pi}(f(X_0), f(X_k)) = \\mathbb{E}_{\\pi}[g(X_0) g(X_k)]$. By the law of total expectation and the definition of the operator $P$:\n$$ \\mathbb{E}_{\\pi}[g(X_0) g(X_k)] = \\mathbb{E}_{\\pi} \\left[ g(X_0) \\mathbb{E}[g(X_k) | X_0] \\right] = \\mathbb{E}_{\\pi} [g(X_0) (P^k g)(X_0)] = \\langle g, P^k g \\rangle_{\\pi} $$\nThus, the asymptotic variance can be written in operator notation as:\n$$ \\sigma_f^2 = \\langle g, g \\rangle_{\\pi} + 2 \\sum_{k=1}^{\\infty} \\langle g, P^k g \\rangle_{\\pi} = \\left\\langle g, \\left(I + 2 \\sum_{k=1}^{\\infty} P^k \\right) g \\right\\rangle_{\\pi} $$\nThe reversibility condition implies that the operator $P$ is self-adjoint on the Hilbert space $L^2(\\pi)$. Since the state space $\\mathcal{X}$ is finite, $L^2(\\pi)$ is finite-dimensional. By the spectral theorem, there exists an orthonormal basis of eigenfunctions $\\{\\psi_i\\}_{i=1}^{|\\mathcal{X}|}$ such that $P\\psi_i = \\lambda_i \\psi_i$ and $\\langle \\psi_i, \\psi_j \\rangle_{\\pi} = \\delta_{ij}$. The eigenvalues $\\lambda_i$ are real.\nFor an irreducible, aperiodic chain, the eigenvalue $\\lambda_1=1$ is simple, and the corresponding eigenfunction is a constant, which we can take to be $\\psi_1(x) = 1$. All other eigenvalues satisfy $|\\lambda_i|  1$.\nThe centered function $g$ is orthogonal to the space of constant functions:\n$$ \\langle g, \\psi_1 \\rangle_{\\pi} = \\sum_x \\pi(x) g(x) \\psi_1(x) = \\sum_x \\pi(x) g(x) = \\mathbb{E}_{\\pi}[g] = 0 $$\nWe can expand $g$ in the eigenbasis: $g = \\sum_{i=1}^{|\\mathcal{X}|} \\langle g, \\psi_i \\rangle_{\\pi} \\psi_i$. Since $\\langle g, \\psi_1 \\rangle_{\\pi} = 0$, this sum is effectively over $i=2, \\dots, |\\mathcal{X}|$.\nUsing this expansion, we compute $\\langle g, P^k g \\rangle_{\\pi}$:\n$$ \\langle g, P^k g \\rangle_{\\pi} = \\left\\langle \\sum_i \\langle g, \\psi_i \\rangle_{\\pi} \\psi_i, \\sum_j \\langle g, \\psi_j \\rangle_{\\pi} \\lambda_j^k \\psi_j \\right\\rangle_{\\pi} = \\sum_{i,j} \\langle g, \\psi_i \\rangle_{\\pi} \\langle g, \\psi_j \\rangle_{\\pi} \\lambda_j^k \\langle \\psi_i, \\psi_j \\rangle_{\\pi} $$\nUsing orthonormality, $\\langle \\psi_i, \\psi_j \\rangle_{\\pi} = \\delta_{ij}$, this simplifies to:\n$$ \\langle g, P^k g \\rangle_{\\pi} = \\sum_{i=2}^{|\\mathcal{X}|} (\\langle g, \\psi_i \\rangle_{\\pi})^2 \\lambda_i^k $$\nSubstituting this back into the expression for $\\sigma_f^2$:\n$$ \\sigma_f^2 = \\sum_{i=2}^{|\\mathcal{X}|} (\\langle g, \\psi_i \\rangle_{\\pi})^2 + 2 \\sum_{k=1}^{\\infty} \\sum_{i=2}^{|\\mathcal{X}|} (\\langle g, \\psi_i \\rangle_{\\pi})^2 \\lambda_i^k = \\sum_{i=2}^{|\\mathcal{X}|} (\\langle g, \\psi_i \\rangle_{\\pi})^2 \\left( 1 + 2 \\sum_{k=1}^{\\infty} \\lambda_i^k \\right) $$\nFor each $i \\ge 2$, $|\\lambda_i|  1$, so the geometric series converges: $\\sum_{k=1}^{\\infty} \\lambda_i^k = \\frac{\\lambda_i}{1 - \\lambda_i}$. The term in parentheses becomes:\n$$ 1 + 2 \\frac{\\lambda_i}{1 - \\lambda_i} = \\frac{1 - \\lambda_i + 2\\lambda_i}{1 - \\lambda_i} = \\frac{1 + \\lambda_i}{1 - \\lambda_i} $$\nSo the spectral representation of the asymptotic variance is:\n$$ \\sigma_f^2 = \\sum_{i=2}^{|\\mathcal{X}|} \\frac{1 + \\lambda_i}{1 - \\lambda_i} (\\langle g, \\psi_i \\rangle_{\\pi})^2 $$\nThis can be expressed using the projection-valued spectral measure $E$ of $P$. $P = \\int \\lambda dE(\\lambda)$. The measure $\\mu_g(A) = \\langle g, E(A)g \\rangle_{\\pi} = \\|E(A)g\\|_{\\pi}^2$ captures the distribution of $g$ across the spectrum. Since $E(\\{\\lambda_i\\})$ is the projector onto the $\\lambda_i$-eigenspace $\\Pi_i$, we have $\\|\\Pi_i g\\|_{\\pi}^2 = (\\langle g, \\psi_i \\rangle_{\\pi})^2$ for simple eigenvalues. The sum becomes an integral over the spectrum of $P$, excluding the eigenvalue $1$ because $g$ has no component in that eigenspace:\n$$ \\sigma_f^2 = \\int_{\\sigma(P) \\setminus \\{1\\}} \\frac{1+\\lambda}{1-\\lambda} \\, d\\mu_g(\\lambda) $$\n\n### Part 2: Calculation for the Two-State Chain\n\nWe specialize the result to the given two-state chain. The state space is $\\mathcal{X}=\\{0,1\\}$.\nFirst, we find the eigenvalues of the transition matrix $P = \\begin{pmatrix} 1-\\alpha  \\alpha \\\\ \\beta  1-\\beta \\end{pmatrix}$. The characteristic equation is $\\det(P - \\lambda I)=0$:\n$$ (1-\\alpha-\\lambda)(1-\\beta-\\lambda) - \\alpha\\beta = \\lambda^2 - (2-\\alpha-\\beta)\\lambda + (1-\\alpha-\\beta) = 0 $$\nThis factors as $(\\lambda-1)(\\lambda - (1-\\alpha-\\beta)) = 0$. The eigenvalues are $\\lambda_1 = 1$ and $\\lambda_2 = 1-\\alpha-\\beta$. Since $\\alpha, \\beta \\in (0,1)$, we have $-1  \\lambda_2  1$.\n\nFor a two-state space, $|\\mathcal{X}|=2$, the sum for $\\sigma_f^2$ has only one term, for $i=2$:\n$$ \\sigma_f^2 = \\frac{1+\\lambda_2}{1-\\lambda_2} (\\langle g, \\psi_2 \\rangle_{\\pi})^2 $$\nwhere $\\psi_2$ is the normalized eigenfunction for $\\lambda_2$.\nSince $g$ is orthogonal to the constant eigenfunction $\\psi_1$, it must lie entirely in the eigenspace of $\\lambda_2$. Therefore, the coefficient squared $(\\langle g, \\psi_2 \\rangle_{\\pi})^2$ is simply the total squared norm of $g$, $\\|g\\|_{\\pi}^2 = \\langle g, g \\rangle_{\\pi}$.\nSo, we have the simplified formula:\n$$ \\sigma_f^2 = \\frac{1+\\lambda_2}{1-\\lambda_2} \\|g\\|_{\\pi}^2 $$\nNext, we compute the two components of this formula. The term involving eigenvalues is:\n$$ \\frac{1+\\lambda_2}{1-\\lambda_2} = \\frac{1+(1-\\alpha-\\beta)}{1-(1-\\alpha-\\beta)} = \\frac{2-\\alpha-\\beta}{\\alpha+\\beta} $$\nNext, we compute $\\|g\\|_{\\pi}^2$. First, we find $g = f - \\mathbb{E}_{\\pi}[f]$. The expectation is:\n$$ \\mathbb{E}_{\\pi}[f] = \\pi(0)f(0) + \\pi(1)f(1) = \\frac{\\beta}{\\alpha+\\beta} u + \\frac{\\alpha}{\\alpha+\\beta} v = \\frac{\\beta u + \\alpha v}{\\alpha+\\beta} $$\nThe components of the centered function $g$ are:\n$g(0) = f(0) - \\mathbb{E}_{\\pi}[f] = u - \\frac{\\beta u + \\alpha v}{\\alpha+\\beta} = \\frac{(\\alpha+\\beta)u - \\beta u - \\alpha v}{\\alpha+\\beta} = \\frac{\\alpha(u-v)}{\\alpha+\\beta}$.\n$g(1) = f(1) - \\mathbb{E}_{\\pi}[f] = v - \\frac{\\beta u + \\alpha v}{\\alpha+\\beta} = \\frac{(\\alpha+\\beta)v - \\beta u - \\alpha v}{\\alpha+\\beta} = \\frac{-\\beta(u-v)}{\\alpha+\\beta}$.\nThe squared norm $\\|g\\|_{\\pi}^2 = \\pi(0)g(0)^2 + \\pi(1)g(1)^2$ is:\n$$ \\|g\\|_{\\pi}^2 = \\frac{\\beta}{\\alpha+\\beta} \\left(\\frac{\\alpha(u-v)}{\\alpha+\\beta}\\right)^2 + \\frac{\\alpha}{\\alpha+\\beta} \\left(\\frac{-\\beta(u-v)}{\\alpha+\\beta}\\right)^2 $$\n$$ = \\frac{(u-v)^2}{(\\alpha+\\beta)^3} \\left( \\beta\\alpha^2 + \\alpha\\beta^2 \\right) = \\frac{(u-v)^2}{(\\alpha+\\beta)^3} \\alpha\\beta(\\alpha+\\beta) = \\frac{\\alpha\\beta(u-v)^2}{(\\alpha+\\beta)^2} $$\nFinally, we combine the two components to find $\\sigma_f^2$:\n$$ \\sigma_f^2 = \\left(\\frac{2-\\alpha-\\beta}{\\alpha+\\beta}\\right) \\left(\\frac{\\alpha\\beta(u-v)^2}{(\\alpha+\\beta)^2}\\right) = \\frac{\\alpha\\beta(2-\\alpha-\\beta)(u-v)^2}{(\\alpha+\\beta)^3} $$\nThis is the closed-form expression for the asymptotic variance.",
            "answer": "$$\\boxed{\\frac{\\alpha\\beta(2 - \\alpha - \\beta)(u-v)^2}{(\\alpha+\\beta)^3}}$$"
        },
        {
            "introduction": "Building on the spectral analysis of variance, we now ask a more general question: can we characterize a chain's efficiency with a single, powerful metric? This exercise  introduces the spectral gap, a fundamental quantity that measures the mixing rate of a reversible Markov chain. You will derive a classic bound that relates the asymptotic variance of an MCMC estimator to the spectral gap, revealing how this value directly controls the statistical efficiency of the sampler.",
            "id": "3319845",
            "problem": "Consider a time-homogeneous, ergodic, aperiodic, reversible Markov chain with transition kernel $P$ on a measurable state space $(\\mathcal{X}, \\mathcal{F})$ admitting stationary distribution $\\pi$. Let $\\mathsf{L}_{2}(\\pi)$ denote the Hilbert space of square-integrable functions with respect to $\\pi$, with inner product $\\langle g, h \\rangle_{\\pi} = \\int_{\\mathcal{X}} g(x) h(x) \\,\\pi(\\mathrm{d}x)$ and norm $\\|g\\|_{2,\\pi}^{2} = \\langle g, g \\rangle_{\\pi}$. Let $\\mathsf{L}_{2}^{0}(\\pi)$ denote the mean-zero subspace $\\{g \\in \\mathsf{L}_{2}(\\pi) : \\int g \\,\\mathrm{d}\\pi = 0\\}$. Assume $P$ is self-adjoint on $\\mathsf{L}_{2}(\\pi)$ and define the $\\mathsf{L}_{2}$ spectral gap by\n$$\n\\gamma \\;=\\; 1 \\;-\\; \\sup\\Big\\{\\frac{\\langle g, P g \\rangle_{\\pi}}{\\langle g, g \\rangle_{\\pi}} : g \\in \\mathsf{L}_{2}^{0}(\\pi),\\, g \\neq 0\\Big\\} \\;\\; 0.\n$$\nFor an observable $f \\in \\mathsf{L}_{2}(\\pi)$, consider the Markov chain Monte Carlo (MCMC) estimator $\\bar{f}_{n} = \\frac{1}{n}\\sum_{t=1}^{n} f(X_{t})$, where $(X_{t})$ evolves according to $P$ from stationarity. The asymptotic variance $\\sigma_{f}^{2}$ is defined by the Central Limit Theorem for Markov chains through\n$$\n\\sqrt{n}\\,\\big(\\bar{f}_{n} - \\mathbb{E}_{\\pi}[f]\\big) \\;\\xrightarrow{d}\\; \\mathcal{N}\\big(0,\\,\\sigma_{f}^{2}\\big),\n$$\nunder standard regularity conditions that you may assume to hold.\n\nStarting from the foundational definitions of reversibility, the spectral theorem for self-adjoint operators on $\\mathsf{L}_{2}(\\pi)$, and the representation of autocovariances via the spectral measure of $P$ associated with $f$, derive a nontrivial closed-form upper bound on the asymptotic variance $\\sigma_{f}^{2}$ that depends only on the stationary variance $\\mathrm{Var}_{\\pi}(f)$ and the spectral gap $\\gamma$. Your derivation should proceed from first principles and justify each inequality step.\n\nThen, interpret the implication of this bound for the magnitude of the Monte Carlo error of $\\bar{f}_{n}$ for large $n$, in particular how the spectral gap $\\gamma$ controls the inflation of the error due to temporal dependence.\n\nProvide only the bound expression as your final answer. No numerical evaluation is required and no rounding is needed.",
            "solution": "The user wants to derive an upper bound on the asymptotic variance $\\sigma_{f}^{2}$ of a Markov chain Monte Carlo estimator in terms of the stationary variance $\\mathrm{Var}_{\\pi}(f)$ and the $\\mathsf{L}_{2}$ spectral gap $\\gamma$.\n\nFirst, we validate the problem statement.\n\n**Step 1: Extract Givens**\n- The Markov chain is time-homogeneous, ergodic, aperiodic, and reversible.\n- The transition kernel is $P$ on a measurable state space $(\\mathcal{X}, \\mathcal{F})$.\n- The stationary distribution is $\\pi$.\n- $\\mathsf{L}_{2}(\\pi)$ is the Hilbert space of square-integrable functions with respect to $\\pi$, with inner product $\\langle g, h \\rangle_{\\pi} = \\int_{\\mathcal{X}} g(x) h(x) \\,\\pi(\\mathrm{d}x)$ and norm $\\|g\\|_{2,\\pi}^{2} = \\langle g, g \\rangle_{\\pi}$.\n- $\\mathsf{L}_{2}^{0}(\\pi)$ is the mean-zero subspace $\\{g \\in \\mathsf{L}_{2}(\\pi) : \\int g \\,\\mathrm{d}\\pi = 0\\}$.\n- $P$ is a self-adjoint operator on $\\mathsf{L}_{2}(\\pi)$.\n- The $\\mathsf{L}_{2}$ spectral gap is defined as $\\gamma = 1 - \\sup\\{\\frac{\\langle g, P g \\rangle_{\\pi}}{\\langle g, g \\rangle_{\\pi}} : g \\in \\mathsf{L}_{2}^{0}(\\pi),\\, g \\neq 0\\}  0$.\n- The observable is $f \\in \\mathsf{L}_{2}(\\pi)$.\n- The MCMC estimator is $\\bar{f}_{n} = \\frac{1}{n}\\sum_{t=1}^{n} f(X_{t})$, where the chain $(X_{t})$ starts from stationarity ($X_{0} \\sim \\pi$).\n- The asymptotic variance $\\sigma_{f}^{2}$ is defined by the Central Limit Theorem: $\\sqrt{n}\\,(\\bar{f}_{n} - \\mathbb{E}_{\\pi}[f]) \\xrightarrow{d} \\mathcal{N}(0,\\,\\sigma_{f}^{2})$.\n- Standard regularity conditions for the CLT are assumed to hold.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is firmly rooted in the mathematical theory of Markov chains and MCMC methods. All definitions (reversibility, self-adjointness, spectral gap, asymptotic variance) are standard and correct.\n- **Well-Posed:** The problem provides a sufficient set of conditions to derive the requested bound. The goal is clearly defined.\n- **Objective:** The problem is stated in precise, formal mathematical language, devoid of ambiguity or subjectivity.\n- **Completeness and Consistency:** The assumptions are consistent and form the standard framework for analyzing the convergence of reversible Markov chains. The condition $\\gamma  0$ is crucial and is explicitly given.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed with the derivation.\n\nThe asymptotic variance $\\sigma_{f}^{2}$ for a stationary Markov chain is given by the sum of autocovariances:\n$$\n\\sigma_{f}^{2} \\;=\\; \\mathrm{Var}_{\\pi}(f(X_{0})) \\;+\\; 2 \\sum_{k=1}^{\\infty} \\mathrm{Cov}_{\\pi}(f(X_{0}), f(X_{k}))\n$$\nLet's define a mean-zero function $g(x) = f(x) - \\mathbb{E}_{\\pi}[f]$. By construction, $g \\in \\mathsf{L}_{2}^{0}(\\pi)$. The asymptotic variance is invariant to shifting by a constant, so $\\sigma_{f}^{2} = \\sigma_{g}^{2}$. The stationary variance of $f$ is given by $\\mathrm{Var}_{\\pi}(f) = \\mathbb{E}_{\\pi}[(f - \\mathbb{E}_{\\pi}[f])^{2}] = \\mathbb{E}_{\\pi}[g^{2}]$. In the $\\mathsf{L}_{2}(\\pi)$ inner product notation, this is $\\mathrm{Var}_{\\pi}(f) = \\langle g, g \\rangle_{\\pi}$.\n\nThe autocovariance at lag $k \\ge 1$ for the centered function $g$ is:\n$$\n\\mathrm{Cov}_{\\pi}(g(X_{0}), g(X_{k})) = \\mathbb{E}_{\\pi}[g(X_{0})g(X_{k})]\n$$\nSince the chain is stationary, we can condition on $X_{0}$:\n$$\n\\mathbb{E}_{\\pi}[g(X_{0})g(X_{k})] \\;=\\; \\int_{\\mathcal{X}} g(x) \\, \\mathbb{E}[g(X_{k})|X_{0}=x] \\, \\pi(\\mathrm{d}x)\n$$\nThe conditional expectation is the action of the $k$-step transition operator $P^{k}$ on the function $g$, i.e., $(P^{k}g)(x) = \\mathbb{E}[g(X_{k})|X_{0}=x]$. Thus, the autocovariance becomes:\n$$\n\\mathrm{Cov}_{\\pi}(g(X_{0}), g(X_{k})) \\;=\\; \\int_{\\mathcal{X}} g(x) (P^{k}g)(x) \\, \\pi(\\mathrm{d}x) \\;=\\; \\langle g, P^{k}g \\rangle_{\\pi}\n$$\nSubstituting these expressions back into the formula for $\\sigma_{f}^{2}$:\n$$\n\\sigma_{f}^{2} \\;=\\; \\langle g, g \\rangle_{\\pi} \\;+\\; 2 \\sum_{k=1}^{\\infty} \\langle g, P^{k}g \\rangle_{\\pi} \\;=\\; \\left\\langle g, \\left(I + 2\\sum_{k=1}^{\\infty} P^{k}\\right)g \\right\\rangle_{\\pi}\n$$\nThe condition $\\gamma  0$ implies that the spectral radius of $P$ restricted to $\\mathsf{L}_{2}^{0}(\\pi)$ is strictly less than $1$, which guarantees the convergence of the geometric operator series $\\sum_{k=0}^{\\infty} P^{k} = (I-P)^{-1}$. The sum in our expression is $\\sum_{k=1}^{\\infty} P^{k} = P(I-P)^{-1}$.\nSubstituting this into the expression for $\\sigma_{f}^{2}$:\n$$\n\\sigma_{f}^{2} \\;=\\; \\left\\langle g, \\left(I + 2P(I-P)^{-1}\\right)g \\right\\rangle_{\\pi}\n$$\nWe can simplify the operator inside the inner product:\n$$\nI + 2P(I-P)^{-1} \\;=\\; (I-P)(I-P)^{-1} + 2P(I-P)^{-1} \\;=\\; (I-P+2P)(I-P)^{-1} \\;=\\; (I+P)(I-P)^{-1}\n$$\nSo, the asymptotic variance can be expressed as:\n$$\n\\sigma_{f}^{2} \\;=\\; \\left\\langle g, (I+P)(I-P)^{-1}g \\right\\rangle_{\\pi}\n$$\nLet $A = (I+P)(I-P)^{-1}$. Since $P$ is self-adjoint on $\\mathsf{L}_{2}(\\pi)$ (and thus on its invariant subspace $\\mathsf{L}_{2}^{0}(\\pi)$), the operator $A$ is also self-adjoint on $\\mathsf{L}_{2}^{0}(\\pi)$. By the spectral theorem for self-adjoint operators, the value of the quadratic form $\\langle g, Ag \\rangle_{\\pi}$ is bounded by the extremal eigenvalues of $A$:\n$$\n\\sigma_{f}^{2} \\;=\\; \\langle g, Ag \\rangle_{\\pi} \\;\\le\\; \\left( \\sup_{\\lambda \\in \\mathrm{spec}(A)} \\lambda \\right) \\langle g, g \\rangle_{\\pi}\n$$\nThe spectrum of $A$ is determined by the spectrum of $P$. If $\\lambda$ is an eigenvalue of $P$ on $\\mathsf{L}_{2}^{0}(\\pi)$, then the corresponding eigenvalue of $A$ is $h(\\lambda) = \\frac{1+\\lambda}{1-\\lambda}$. The function $h(\\lambda)$ is monotonically increasing for $\\lambda  1$.\nThe supremum of the spectrum of $A$ is therefore determined by the supremum of the spectrum of $P$ on $\\mathsf{L}_{2}^{0}(\\pi)$. Let $\\lambda_{\\max}^{0}(P) = \\sup \\mathrm{spec}(P|_{\\mathsf{L}_{2}^{0}(\\pi)})$.\nBy the definition of the spectral gap,\n$$\n1 - \\gamma \\;=\\; \\sup\\left\\{\\frac{\\langle h, P h \\rangle_{\\pi}}{\\langle h, h \\rangle_{\\pi}} : h \\in \\mathsf{L}_{2}^{0}(\\pi),\\, h \\neq 0\\right\\}\n$$\nThis supremum, the Rayleigh quotient, corresponds to the largest eigenvalue of the self-adjoint operator $P$ on the subspace $\\mathsf{L}_{2}^{0}(\\pi)$. Thus, $\\lambda_{\\max}^{0}(P) = 1-\\gamma$.\nBecause $h(\\lambda)$ is increasing, the largest eigenvalue of $A$ is:\n$$\n\\sup_{\\lambda \\in \\mathrm{spec}(A)} \\lambda \\;=\\; h(\\lambda_{\\max}^{0}(P)) \\;=\\; h(1-\\gamma) \\;=\\; \\frac{1+(1-\\gamma)}{1-(1-\\gamma)} \\;=\\; \\frac{2-\\gamma}{\\gamma}\n$$\nWe can now establish the upper bound on the asymptotic variance:\n$$\n\\sigma_{f}^{2} \\;\\le\\; \\frac{2-\\gamma}{\\gamma} \\langle g, g \\rangle_{\\pi}\n$$\nRecalling that $\\langle g, g \\rangle_{\\pi} = \\mathrm{Var}_{\\pi}(f)$, we arrive at the final inequality:\n$$\n\\sigma_{f}^{2} \\;\\le\\; \\frac{2-\\gamma}{\\gamma} \\mathrm{Var}_{\\pi}(f)\n$$\nThis is a non-trivial closed-form upper bound depending only on $\\mathrm{Var}_{\\pi}(f)$ and $\\gamma$.\n\nFor the interpretation, the variance of the MCMC estimator $\\bar{f}_{n}$ for large $n$ is approximately $\\frac{\\sigma_{f}^{2}}{n}$. In the ideal case of independent and identically distributed samples from $\\pi$, the variance would be $\\frac{\\mathrm{Var}_{\\pi}(f)}{n}$. The ratio $\\frac{\\sigma_{f}^{2}}{\\mathrm{Var}_{\\pi}(f)}$ represents the factor by which the variance is inflated due to temporal correlations in the Markov chain. This ratio is related to the integrated autocorrelation time.\nOur derived bound shows that this variance inflation factor is bounded above by $\\frac{2-\\gamma}{\\gamma}$:\n$$\n\\frac{\\sigma_{f}^{2}}{\\mathrm{Var}_{\\pi}(f)} \\;\\le\\; \\frac{2-\\gamma}{\\gamma}\n$$\nThe spectral gap $\\gamma \\in (0, 1]$ directly controls this bound. A small spectral gap ($\\gamma \\to 0$) implies slow mixing of the chain. This corresponds to a very large upper bound on variance inflation, scaling as $\\frac{2}{\\gamma}$. In this scenario, a much larger number of samples $n$ is required to achieve a desired Monte Carlo error, as the samples are highly correlated. Conversely, a large spectral gap ($\\gamma \\to 1$) implies rapid mixing. The bound $\\frac{2-\\gamma}{\\gamma}$ approaches $1$, indicating that the asymptotic variance is close to the i.i.d. variance. This means the chain produces nearly independent samples efficiently, and the Monte Carlo error decreases rapidly with $n$. Therefore, the spectral gap $\\gamma$ serves as a crucial measure of the statistical efficiency of the MCMC sampler, with its inverse governing the worst-case inflation of statistical error due to sample dependence.",
            "answer": "$$\\boxed{\\frac{2-\\gamma}{\\gamma} \\mathrm{Var}_{\\pi}(f)}$$"
        }
    ]
}