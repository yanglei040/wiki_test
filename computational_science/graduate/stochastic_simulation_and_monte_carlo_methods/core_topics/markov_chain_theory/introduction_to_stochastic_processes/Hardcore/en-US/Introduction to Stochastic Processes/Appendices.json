{
    "hands_on_practices": [
        {
            "introduction": "Many fundamental stochastic processes, such as the Poisson process, are built upon the concept of waiting times between events. This exercise  focuses on the exponential distribution, the canonical model for memoryless waiting times. By starting from first principles, you will verify that a given function is a valid probability density function and then compute its essential properties—mean and variance—which are crucial for understanding the behavior of processes built from it.",
            "id": "3314043",
            "problem": "Consider a continuous-time inter-arrival time model commonly used in stochastic simulation, where a nonnegative random variable $X$ represents the waiting time between events. Suppose $X$ has a probability density function (PDF) $f(x)$ supported on $[0,\\infty)$ of the form $f(x)=c\\exp(-x)$ for $x\\geq 0$. Determine the normalization constant $c$ that makes $f$ a valid PDF, and then compute the expectation $\\mathbb{E}[X]$ and the variance $\\operatorname{Var}(X)$ using first-principles definitions appropriate for an introduction to stochastic processes within the context of stochastic simulation and Monte Carlo methods. Express your final results exactly with no rounding.",
            "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- A random variable $X$ represents the waiting time between events in a continuous-time inter-arrival model.\n- $X$ is a nonnegative random variable.\n- The support of the probability density function (PDF) for $X$ is $[0,\\infty)$.\n- The PDF has the form $f(x)=c\\exp(-x)$ for $x \\geq 0$, where $c$ is a constant.\n- The tasks are to determine the normalization constant $c$, the expectation $\\mathbb{E}[X]$, and the variance $\\operatorname{Var}(X)$.\n- The context is an introduction to stochastic processes.\n- The final results must be exact.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It describes the exponential distribution with an unspecified rate parameter (which will be determined), a cornerstone of stochastic processes, particularly in modeling arrival times in Poisson processes. The formulation is a standard, fundamental exercise in probability theory. All necessary information is provided, there are no internal contradictions, the language is precise, and the problem admits a unique, verifiable mathematical solution.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\nThe analysis proceeds by first determining the normalization constant $c$, then using the resulting valid PDF to compute the expectation and variance from their first-principles definitions.\n\n**1. Determination of the Normalization Constant $c$**\n\nFor $f(x)$ to be a valid probability density function, its integral over the entire support must equal $1$. The support of $X$ is given as $[0, \\infty)$. Therefore, we must satisfy the condition:\n$$ \\int_{0}^{\\infty} f(x) \\, dx = 1 $$\nSubstituting the given form of $f(x)=c\\exp(-x)$:\n$$ \\int_{0}^{\\infty} c\\exp(-x) \\, dx = 1 $$\nThe constant $c$ can be factored out of the integral:\n$$ c \\int_{0}^{\\infty} \\exp(-x) \\, dx = 1 $$\nWe evaluate the definite integral:\n$$ \\int_{0}^{\\infty} \\exp(-x) \\, dx = \\left[-\\exp(-x)\\right]_{0}^{\\infty} = \\left(-\\lim_{b \\to \\infty} \\exp(-b)\\right) - (-\\exp(-0)) = (0) - (-1) = 1 $$\nSubstituting this result back into the normalization equation:\n$$ c \\cdot 1 = 1 $$\nThis yields the value of the normalization constant:\n$$ c = 1 $$\nThus, the valid PDF for the random variable $X$ is $f(x) = \\exp(-x)$ for $x \\ge 0$. This is the PDF of an exponential distribution with a rate parameter $\\lambda=1$.\n\n**2. Computation of the Expectation $\\mathbb{E}[X]$**\n\nThe expectation (or mean) of a continuous random variable $X$ with PDF $f(x)$ supported on $[0, \\infty)$ is defined by:\n$$ \\mathbb{E}[X] = \\int_{0}^{\\infty} x f(x) \\, dx $$\nSubstituting the now-validated PDF, $f(x) = \\exp(-x)$:\n$$ \\mathbb{E}[X] = \\int_{0}^{\\infty} x \\exp(-x) \\, dx $$\nThis integral can be solved using integration by parts, where $\\int u \\, dv = uv - \\int v \\, du$. Let $u = x$ and $dv = \\exp(-x) \\, dx$. This implies $du = dx$ and $v = -\\exp(-x)$.\n$$ \\mathbb{E}[X] = \\left[x(-\\exp(-x))\\right]_{0}^{\\infty} - \\int_{0}^{\\infty} (-\\exp(-x)) \\, dx $$\nThe first term is evaluated at the limits:\n$$ \\left[-x\\exp(-x)\\right]_{0}^{\\infty} = \\left(-\\lim_{b \\to \\infty} b\\exp(-b)\\right) - (-0 \\cdot \\exp(-0)) $$\nThe limit $\\lim_{b \\to \\infty} b\\exp(-b)$ is an indeterminate form $\\infty \\cdot 0$. Rewriting it as $\\lim_{b \\to \\infty} \\frac{b}{\\exp(b)}$ gives the form $\\frac{\\infty}{\\infty}$, to which L'Hôpital's rule can be applied: $\\lim_{b \\to \\infty} \\frac{1}{\\exp(b)} = 0$. Therefore, the first term evaluates to $0 - 0 = 0$.\nThe second term becomes:\n$$ \\int_{0}^{\\infty} \\exp(-x) \\, dx $$\nAs established during normalization, this integral equals $1$.\nCombining the terms, the expectation is:\n$$ \\mathbb{E}[X] = 0 - (-1) = 1 $$\n\n**3. Computation of the Variance $\\operatorname{Var}(X)$**\n\nThe variance of a random variable $X$ is defined as $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$. We have already found $\\mathbb{E}[X]=1$. We now need to compute the second moment, $\\mathbb{E}[X^2]$.\n$$ \\mathbb{E}[X^2] = \\int_{0}^{\\infty} x^2 f(x) \\, dx = \\int_{0}^{\\infty} x^2 \\exp(-x) \\, dx $$\nWe again use integration by parts. Let $u = x^2$ and $dv = \\exp(-x) \\, dx$. This implies $du = 2x \\, dx$ and $v = -\\exp(-x)$.\n$$ \\mathbb{E}[X^2] = \\left[x^2(-\\exp(-x))\\right]_{0}^{\\infty} - \\int_{0}^{\\infty} (-\\exp(-x))(2x) \\, dx $$\nThe first term, $\\left[-x^2\\exp(-x)\\right]_{0}^{\\infty}$, evaluates to $0$ by a similar application of L'Hôpital's rule twice.\nThe integral term simplifies to:\n$$ 2 \\int_{0}^{\\infty} x \\exp(-x) \\, dx $$\nWe recognize the integral $\\int_{0}^{\\infty} x \\exp(-x) \\, dx$ as the definition of $\\mathbb{E}[X]$, which we have already calculated to be $1$.\nThus, the second moment is:\n$$ \\mathbb{E}[X^2] = 0 - (-2 \\cdot 1) = 2 $$\nFinally, we compute the variance using its definition:\n$$ \\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 = 2 - (1)^2 = 2 - 1 = 1 $$\n\nThe required quantities are the normalization constant $c$, the expectation $\\mathbb{E}[X]$, and the variance $\\operatorname{Var}(X)$. The final answer will be presented as a row matrix with these three values in that order.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 1  1  1 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Discrete-time Markov chains provide a powerful framework for modeling systems that transition between a finite number of states. This practice  delves into the long-term dynamics of such a system. You will apply fundamental concepts from linear algebra, particularly the spectral decomposition of the transition matrix, to derive a closed-form expression for the system's state distribution after $n$ steps, offering a clear view of its convergence towards equilibrium.",
            "id": "3314063",
            "problem": "Consider a time-homogeneous discrete-time Markov chain on the finite state space $\\{1,2,3\\}$ used as a mixing kernel within a Monte Carlo (MC) procedure. The transition mechanism is defined by the $3 \\times 3$ transition matrix $P$ whose entries satisfy $P_{ii} = \\frac{5}{9}$ for all $i \\in \\{1,2,3\\}$ and $P_{ij} = \\frac{2}{9}$ for all $i \\neq j$. Let the initial distribution be the row vector $\\mu = \\left(\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{4}\\right)$.\n\nStarting only from the core definition that the $n$-step law is $\\mu P^{n}$ and from fundamental linear-algebraic properties of stochastic matrices, derive an exact closed-form expression for $P^{n}$ and for the $n$-step distribution $\\mu P^{n}$, justifying each step in terms of those principles. Then, determine explicitly the probability that the chain is in state $2$ after $n$ steps, starting from the given $\\mu$.\n\nProvide the final probability for state $2$ after $n$ steps as a single closed-form analytic expression in terms of $n$. Do not approximate; no rounding is required.",
            "solution": "The problem is well-posed and scientifically grounded in the theory of stochastic processes. It provides a complete and consistent set of givens, allowing for a unique and meaningful solution. We may therefore proceed with the derivation.\n\nThe state space of the discrete-time Markov chain is $S = \\{1, 2, 3\\}$. The transition matrix $P$ is given by its entries $P_{ii} = \\frac{5}{9}$ and $P_{ij} = \\frac{2}{9}$ for $i \\neq j$. Explicitly, the matrix is:\n$$ P = \\begin{pmatrix} 5/9  2/9  2/9 \\\\ 2/9  5/9  2/9 \\\\ 2/9  2/9  5/9 \\end{pmatrix} $$\nThe initial probability distribution is the row vector $\\mu = \\left(\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{4}\\right)$. We are tasked with finding an exact closed-form expression for $P^n$ and the $n$-step distribution $\\mu P^n$, from which we will find the probability of being in state $2$ after $n$ steps. The derivation will be based on fundamental linear-algebraic properties.\n\nThe matrix $P$ is a real symmetric matrix. A fundamental result from linear algebra, the Spectral Theorem, states that any real symmetric matrix is diagonalizable by an orthonormal basis of eigenvectors. This property allows for a straightforward calculation of the matrix power $P^n$.\n\nFirst, we find the eigenvalues and eigenvectors of $P$.\nThe matrix $P$ can be written as a linear combination of the identity matrix $I$ and the matrix $J$ of all ones:\n$$ P = \\frac{1}{3}I + \\frac{2}{9}J $$\nwhere $I = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}$ and $J = \\begin{pmatrix} 1  1  1 \\\\ 1  1  1 \\\\ 1  1  1 \\end{pmatrix}$.\nThe eigenvalues of $J$ are well-known. Since $J$ has rank $1$, its null space has dimension $3-1 = 2$. Any vector $\\mathbf{v}$ in the null space satisfies $J\\mathbf{v} = 0\\mathbf{v}$, so $\\lambda=0$ is an eigenvalue with geometric multiplicity $2$. The vector $\\mathbf{1} = (1, 1, 1)^T$ is an eigenvector with eigenvalue $3$, as $J\\mathbf{1} = (3, 3, 3)^T = 3 \\cdot \\mathbf{1}$.\nThe eigenvalues of $P = aI + bJ$ are $a + b\\lambda_J$, where $\\lambda_J$ are the eigenvalues of $J$.\nThus, the eigenvalues of $P$ are:\n$\\lambda_1 = \\frac{1}{3} + \\frac{2}{9}(3) = \\frac{1}{3} + \\frac{2}{3} = 1$.\n$\\lambda_{2,3} = \\frac{1}{3} + \\frac{2}{9}(0) = \\frac{1}{3}$.\nSo, the eigenvalues of $P$ are $1$ (with multiplicity $1$) and $\\frac{1}{3}$ (with multiplicity $2$).\n\nThe eigenspace for $\\lambda_1=1$ is spanned by the eigenvector $\\mathbf{v}_1 = (1, 1, 1)^T$. The corresponding normalized left eigenvector is the stationary distribution $\\pi$. Since $P$ is symmetric, it is doubly stochastic, and the stationary distribution is uniform: $\\pi = (\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$.\nThe eigenspace for $\\lambda_2 = \\frac{1}{3}$ is the null space of $P - \\frac{1}{3}I = \\frac{2}{9}J$. This is the set of all vectors $\\mathbf{x} = (x_1, x_2, x_3)^T$ such that $x_1+x_2+x_3=0$. This two-dimensional space is the orthogonal complement of the eigenspace for $\\lambda_1=1$.\n\nUsing the spectral decomposition, $P$ can be expressed as $P = \\sum_{k=1}^3 \\lambda_k \\mathbf{u}_k \\mathbf{u}_k^T$, where $\\{\\mathbf{u}_k\\}$ is an orthonormal basis of eigenvectors. We can then find $P^n$ as:\n$$ P^n = \\lambda_1^n \\mathbf{u}_1 \\mathbf{u}_1^T + \\lambda_2^n \\mathbf{u}_2 \\mathbf{u}_2^T + \\lambda_3^n \\mathbf{u}_3 \\mathbf{u}_3^T $$\nLet $\\Pi = \\mathbf{u}_1 \\mathbf{u}_1^T$ be the projection matrix onto the eigenspace for $\\lambda_1=1$. Since $\\mathbf{u}_1 = \\frac{1}{\\sqrt{3}}(1, 1, 1)^T$, we have $\\Pi = \\frac{1}{3}J$. The sum of projectors onto the orthogonal eigenspaces for $\\lambda_2 = \\lambda_3 = \\frac{1}{3}$ is $I - \\Pi = I - \\frac{1}{3}J$.\n$$ P^n = 1^n \\Pi + (\\frac{1}{3})^n (I - \\Pi) = \\Pi + (\\frac{1}{3})^n (I - \\Pi) $$\nSubstituting $\\Pi = \\frac{1}{3}J$:\n$$ P^n = \\frac{1}{3}J + (\\frac{1}{3})^n (I - \\frac{1}{3}J) = (\\frac{1}{3})^n I + \\left(\\frac{1}{3} - \\frac{1}{3}(\\frac{1}{3})^n\\right)J = (\\frac{1}{3})^n I + \\frac{1}{3}(1 - (\\frac{1}{3})^n)J $$\nThis is the closed-form expression for $P^n$.\n\nNext, we find the $n$-step distribution $\\mu^{(n)} = \\mu P^n$. We can leverage the same eigen-decomposition. The initial distribution $\\mu$ can be decomposed into a component along the stationary distribution $\\pi$ and a component in the orthogonal space (the eigenspace for $\\lambda=1/3$).\n$$ \\mu = \\pi + (\\mu - \\pi) $$\nThe vector $\\pi$ is the left eigenvector for $\\lambda_1=1$, so $\\pi P^n = \\pi$.\nThe vector $\\delta = \\mu - \\pi$ has components that sum to zero: $\\sum_i (\\mu_i - \\pi_i) = \\sum_i \\mu_i - \\sum_i \\pi_i = 1 - 1 = 0$. Thus, $\\delta$ is a linear combination of left eigenvectors for the eigenvalue $\\frac{1}{3}$. Consequently, $\\delta P^n = (\\frac{1}{3})^n \\delta$.\nApplying $P^n$ to the decomposition of $\\mu$:\n$$ \\mu P^n = (\\pi + \\delta)P^n = \\pi P^n + \\delta P^n = \\pi + (\\frac{1}{3})^n \\delta $$\nSubstituting $\\delta = \\mu - \\pi$, we get the closed-form expression for the $n$-step distribution:\n$$ \\mu P^n = \\pi + (\\frac{1}{3})^n (\\mu - \\pi) $$\nThis result shows that the distribution converges to the stationary distribution $\\pi$ as $n \\to \\infty$.\n\nNow, we substitute the given values:\n$\\mu = \\left(\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{4}\\right)$\n$\\pi = \\left(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}\\right)$\n$\\mu - \\pi = \\left(\\frac{1}{2} - \\frac{1}{3}, \\frac{1}{4} - \\frac{1}{3}, \\frac{1}{4} - \\frac{1}{3}\\right) = \\left(\\frac{1}{6}, -\\frac{1}{12}, -\\frac{1}{12}\\right)$\n\nThe $n$-step distribution vector is:\n$$ \\mu P^n = \\left(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3}\\right) + (\\frac{1}{3})^n \\left(\\frac{1}{6}, -\\frac{1}{12}, -\\frac{1}{12}\\right) $$\nThe probability that the chain is in state $2$ after $n$ steps is the second component of this vector, denoted $(\\mu P^n)_2$:\n$$ (\\mu P^n)_2 = \\frac{1}{3} + (\\frac{1}{3})^n \\left(-\\frac{1}{12}\\right) = \\frac{1}{3} - \\frac{1}{12}(\\frac{1}{3})^n $$\nThis expression can be simplified:\n$$ \\frac{1}{3} - \\frac{1}{12 \\cdot 3^n} = \\frac{1}{3} - \\frac{1}{4 \\cdot 3 \\cdot 3^n} = \\frac{1}{3} - \\frac{1}{4 \\cdot 3^{n+1}} $$\nThis is the final, exact, closed-form expression for the desired probability.",
            "answer": "$$\n\\boxed{\\frac{1}{3} - \\frac{1}{4 \\cdot 3^{n+1}}}\n$$"
        },
        {
            "introduction": "Standard Brownian motion is a cornerstone of modern probability theory and a key model in fields ranging from finance to physics. It can be intuitively understood as the limit of a simple random walk as the step size and time interval shrink to zero. This computational exercise  invites you to make this profound theoretical connection concrete by simulating a scaled random walk and numerically quantifying its convergence in distribution to a true Brownian motion, providing a tangible verification of the Central Limit Theorem's power.",
            "id": "3314093",
            "problem": "Let $\\{W_t\\}_{t \\ge 0}$ denote a standard Brownian motion, defined as a Gaussian process with stationary independent increments, with $W_0 = 0$ and $W_t \\sim \\mathcal{N}(0,t)$ for each fixed $t \\ge 0$. Consider the scaled simple random walk approximation $\\{X^\\Delta(t)\\}_{t \\ge 0}$ constructed as follows: fix a time step $\\Delta t  0$, let $n = t / \\Delta t \\in \\mathbb{N}$, and define $X^\\Delta(t) = \\sum_{k=1}^{n} \\xi_k \\sqrt{\\Delta t}$ where $\\{\\xi_k\\}_{k=1}^n$ are independent and identically distributed Rademacher random variables taking values $+1$ and $-1$ with equal probability. The classical Central Limit Theorem provides a route to understanding why, for each fixed $t$, the distribution of $X^\\Delta(t)$ approximates the distribution of $W_t$ as $\\Delta t \\to 0$ (equivalently $n \\to \\infty$ with $n \\Delta t = t$ fixed). In this problem, you will quantify the convergence in distribution at fixed times by computing a numerical estimate of the Kolmogorov distance between the empirical distribution of $X^\\Delta(t)$ and the exact distribution of $W_t$.\n\nStarting only from the core definitions of standard Brownian motion and the scaled simple random walk above, derive an algorithm to compute the Kolmogorov–Smirnov distance $D_{\\mathrm{KS}}(t,\\Delta t,M)$ between:\n- the empirical cumulative distribution function $F_M(x)$ of $M$ independent samples of $X^\\Delta(t)$, and\n- the exact cumulative distribution function $\\Phi_t(x)$ of the normal distribution $\\mathcal{N}(0,t)$.\n\nFormally, for fixed $t$ and $\\Delta t$ with $n = t/\\Delta t \\in \\mathbb{N}$, and given $M \\in \\mathbb{N}$, define\n$$\nD_{\\mathrm{KS}}(t,\\Delta t,M) = \\sup_{x \\in \\mathbb{R}} \\left| F_M(x) - \\Phi_t(x) \\right|.\n$$\nYour task is to implement a complete, runnable program that:\n- Generates $M$ independent samples of $X^\\Delta(t)$ by simulating $n$ independent Rademacher increments scaled by $\\sqrt{\\Delta t}$ and summing them.\n- Computes $D_{\\mathrm{KS}}(t,\\Delta t,M)$ exactly from these samples using the fact that for an empirical cumulative distribution function, the supremum in the Kolmogorov distance is attained at sample points and their left limits. Concretely, if $x_{(1)} \\le x_{(2)} \\le \\cdots \\le x_{(M)}$ are the sorted samples, then\n$$\nD_{\\mathrm{KS}}(t,\\Delta t,M) = \\max\\left\\{ \\max_{1 \\le i \\le M} \\left(\\frac{i}{M} - \\Phi_t(x_{(i)})\\right), \\ \\max_{1 \\le i \\le M} \\left(\\Phi_t(x_{(i)}) - \\frac{i-1}{M}\\right) \\right\\}.\n$$\n- Uses a fixed random number generator seed $123456$ to ensure reproducibility.\n- Computes the exact normal cumulative distribution function $\\Phi_t(x)$ of $\\mathcal{N}(0,t)$ for $t  0$ using the error function: $\\Phi_t(x) = \\frac{1}{2}\\left(1 + \\operatorname{erf}\\left(\\frac{x}{\\sqrt{2t}}\\right)\\right)$.\n- Assumes $t/\\Delta t \\in \\mathbb{N}$ for all test cases. No physical units or angle units are involved in this problem.\n\nTest Suite:\nCompute $D_{\\mathrm{KS}}(t,\\Delta t,M)$ for the following parameter sets, which cover a range of behaviors from coarse to fine time discretizations, including boundary-like coarse cases:\n- $(t,\\Delta t,M) = (1.0, 1.0, 40000)$\n- $(t,\\Delta t,M) = (1.0, 0.25, 40000)$\n- $(t,\\Delta t,M) = (1.0, 0.10, 40000)$\n- $(t,\\Delta t,M) = (1.0, 0.01, 40000)$\n- $(t,\\Delta t,M) = (2.0, 0.02, 40000)$\n\nFinal Output Format:\nYour program should produce a single line of output containing the five computed Kolmogorov distances as a comma-separated list enclosed in square brackets, in the same order as the test suite above. For example, the output should look like $[d_1,d_2,d_3,d_4,d_5]$, where each $d_i$ is a floating-point number.",
            "solution": "The problem requires the computation of the Kolmogorov-Smirnov distance between the empirical distribution of a scaled simple random walk and the theoretical distribution of a standard Brownian motion at a fixed time. This serves as a numerical verification of the convergence in distribution of the random walk to Brownian motion, a result underpinned by the functional central limit theorem.\n\nFirst, we formalize the two stochastic processes involved.\nThe target process is the standard Brownian motion, denoted $\\{W_t\\}_{t \\ge 0}$. For any fixed time $t  0$, the random variable $W_t$ follows a normal distribution with mean $0$ and variance $t$, written as $W_t \\sim \\mathcal{N}(0,t)$. Its cumulative distribution function (CDF), $\\Phi_t(x) = P(W_t \\le x)$, is given by the centered and scaled standard normal CDF. This can be expressed using the error function, $\\operatorname{erf}(z) = \\frac{2}{\\sqrt{\\pi}} \\int_0^z e^{-u^2} du$, as:\n$$\n\\Phi_t(x) = \\frac{1}{2}\\left(1 + \\operatorname{erf}\\left(\\frac{x}{\\sqrt{2t}}\\right)\\right)\n$$\n\nThe approximating process is the scaled simple random walk, $\\{X^\\Delta(t)\\}_{t \\ge 0}$. For a fixed time $t$ and a time step $\\Delta t  0$, we define the number of steps as $n = t/\\Delta t$, which is an integer for all cases considered. The position of the walk at time $t$ is given by the sum of $n$ scaled, independent steps:\n$$\nX^\\Delta(t) = \\sum_{k=1}^{n} \\xi_k \\sqrt{\\Delta t}\n$$\nHere, $\\{\\xi_k\\}_{k=1}^n$ are independent and identically distributed (i.i.d.) Rademacher random variables, which take the values $+1$ and $-1$ with equal probability: $P(\\xi_k = 1) = P(\\xi_k = -1) = 1/2$. The mean and variance of each $\\xi_k$ are $\\mathbb{E}[\\xi_k] = 0$ and $\\operatorname{Var}(\\xi_k) = \\mathbb{E}[\\xi_k^2] - (\\mathbb{E}[\\xi_k])^2 = 1^2 - 0^2 = 1$. Consequently, the mean and variance of $X^\\Delta(t)$ are:\n$$\n\\mathbb{E}[X^\\Delta(t)] = \\mathbb{E}\\left[\\sum_{k=1}^{n} \\xi_k \\sqrt{\\Delta t}\\right] = \\sqrt{\\Delta t} \\sum_{k=1}^{n} \\mathbb{E}[\\xi_k] = 0\n$$\n$$\n\\operatorname{Var}(X^\\Delta(t)) = \\operatorname{Var}\\left(\\sum_{k=1}^{n} \\xi_k \\sqrt{\\Delta t}\\right) = (\\sqrt{\\Delta t})^2 \\sum_{k=1}^{n} \\operatorname{Var}(\\xi_k) = \\Delta t \\cdot n \\cdot 1 = \\Delta t \\cdot \\frac{t}{\\Delta t} = t\n$$\nThe mean and variance of $X^\\Delta(t)$ match those of $W_t$. The Central Limit Theorem suggests that as $n \\to \\infty$ (or equivalently, $\\Delta t \\to 0$ with $t$ fixed), the distribution of the sum $X^\\Delta(t)$ converges to a normal distribution with the same mean and variance, i.e., $\\mathcal{N}(0,t)$.\n\nTo quantify this convergence, we use the Kolmogorov-Smirnov (KS) distance. Given $M$ independent samples of $X^\\Delta(t)$, we first construct the empirical cumulative distribution function (ECDF), denoted $F_M(x)$. If $x_{(1)} \\le x_{(2)} \\le \\cdots \\le x_{(M)}$ are the samples sorted in non-decreasing order, the ECDF is a step function defined as $F_M(x) = \\frac{1}{M} \\times (\\text{number of samples } \\le x)$. The KS distance is the supremum of the absolute difference between the ECDF and the theoretical CDF:\n$$\nD_{\\mathrm{KS}}(t,\\Delta t,M) = \\sup_{x \\in \\mathbb{R}} \\left| F_M(x) - \\Phi_t(x) \\right|\n$$\nSince $\\Phi_t(x)$ is continuous and $F_M(x)$ is a step function, the supremum must be attained at one of the sample points $x_{(i)}$. Specifically, the difference must be evaluated just before and at each jump in the ECDF. This leads to the well-established computational formula:\n$$\nD_{\\mathrm{KS}}(t,\\Delta t,M) = \\max\\left\\{ \\max_{1 \\le i \\le M} \\left(\\frac{i}{M} - \\Phi_t(x_{(i)})\\right), \\ \\max_{1 \\le i \\le M} \\left(\\Phi_t(x_{(i)}) - \\frac{i-1}{M}\\right) \\right\\}\n$$\nThe first term inside the outer $\\max$ corresponds to the largest positive deviation of the ECDF above the theoretical CDF, evaluated at the sample points $x_{(i)}$. The second term corresponds to the largest positive deviation of the theoretical CDF above the ECDF, evaluated just to the left of the sample points, where the ECDF has value $\\frac{i-1}{M}$.\n\nThe algorithm to compute $D_{\\mathrm{KS}}(t,\\Delta t,M)$ for a given set of parameters $(t, \\Delta t, M)$ is as follows:\n1.  **Set Seed for Reproducibility**: Initialize the pseudo-random number generator with a fixed seed ($123456$) to ensure that the simulation results are identical upon every execution.\n2.  **Generate Samples**:\n    a. Calculate the number of steps $n = t/\\Delta t$.\n    b. Generate an $M \\times n$ matrix of random integers, where each element is either $1$ or $-1$ with equal probability. This represents the outcomes of the $\\xi_k$ variables for all $M$ simulations.\n    c. Sum the elements along each of the $M$ rows to obtain $M$ realizations of the unscaled sum $\\sum_{k=1}^n \\xi_k$.\n    d. Multiply each sum by the scaling factor $\\sqrt{\\Delta t}$ to produce the final $M$ samples of $X^\\Delta(t)$.\n3.  **Compute KS Distance**:\n    a. Sort the $M$ generated samples in ascending order to obtain the ordered statistics $x_{(1)}, x_{(2)}, \\dots, x_{(M)}$.\n    b. For each $x_{(i)}$, calculate the corresponding theoretical CDF value $\\Phi_t(x_{(i)})$ using the formula involving the error function.\n    c. Create two arrays representing the ECDF values: one for the upper bound at each step, $v_i = i/M$ for $i=1, \\dots, M$, and one for the lower bound, $u_i = (i-1)/M$.\n    d. Calculate the two maximum differences required by the KS formula:\n       $D^+ = \\max_{i} (v_i - \\Phi_t(x_{(i)}))$\n       $D^- = \\max_{i} (\\Phi_t(x_{(i)}) - u_i)$\n    e. The result is the maximum of these two values: $D_{\\mathrm{KS}} = \\max(D^+, D^-)$.\n\nThis procedure is repeated for each parameter set in the test suite. We expect to observe that for a fixed $t$ and $M$, the computed distance $D_{\\mathrm{KS}}$ decreases as the time step $\\Delta t$ gets smaller, illustrating the convergence of the discrete random walk model to the continuous Brownian motion model.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erf\n\ndef solve():\n    \"\"\"\n    Computes the Kolmogorov-Smirnov distance between the empirical CDF of a\n    scaled random walk and the exact CDF of a Brownian motion for several\n    parameter sets.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (t, delta_t, M)\n        (1.0, 1.0, 40000),\n        (1.0, 0.25, 40000),\n        (1.0, 0.10, 40000),\n        (1.0, 0.01, 40000),\n        (2.0, 0.02, 40000)\n    ]\n\n    # Fixed seed for reproducibility.\n    seed = 123456\n    rng = np.random.default_rng(seed)\n\n    results = []\n    for t, delta_t, M in test_cases:\n        # Step 1: Parameter calculation\n        # n is the number of steps in the random walk.\n        # The problem guarantees t/delta_t is an integer.\n        n = int(t / delta_t)\n\n        # Step 2: Sample Generation\n        # Generate M x n matrix of Rademacher random variables (+1 or -1)\n        # 2 * rng.integers(0, 2, size=(M, n)) - 1 is an efficient way to get -1, 1\n        xi_k_matrix = 2 * rng.integers(0, 2, size=(M, n), dtype=np.int8) - 1\n        \n        # Sum the increments for each of the M paths\n        sum_of_increments = np.sum(xi_k_matrix, axis=1)\n\n        # Scale the sums to get M samples of X_delta(t)\n        samples = sum_of_increments * np.sqrt(delta_t)\n        \n        # Step 3: Kolmogorov-Smirnov distance calculation\n        # Sort the samples in ascending order\n        sorted_samples = np.sort(samples)\n\n        # Calculate the theoretical CDF Phi_t(x) for each sorted sample\n        # Phi_t(x) = 0.5 * (1 + erf(x / sqrt(2*t)))\n        arg_erf = sorted_samples / np.sqrt(2 * t)\n        phi_t_values = 0.5 * (1.0 + erf(arg_erf))\n        \n        # ECDF values: (i-1)/M and i/M for i=1,...,M\n        i_values = np.arange(1, M + 1)\n        ecdf_upper = i_values / M\n        ecdf_lower = (i_values - 1) / M\n        \n        # Calculate the two components of the KS distance\n        d_plus = np.max(ecdf_upper - phi_t_values)\n        d_minus = np.max(phi_t_values - ecdf_lower)\n        \n        # The KS distance is the max of the two\n        ks_dist = np.max([d_plus, d_minus])\n        \n        results.append(ks_dist)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}