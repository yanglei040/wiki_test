{
    "hands_on_practices": [
        {
            "introduction": "The first step towards proving geometric ergodicity is often establishing that some part of the state space allows the chain to \"reset\" itself. This property is formalized by the concept of a \"small set\" and its associated minorization condition, which guarantees that transitions from the set are uniformly bounded below by a fixed probability measure. This exercise  provides direct, from-the-ground-up practice in constructing the minorization measure $\\nu$ and constant $\\varepsilon$ for a given kernel, a fundamental skill for analyzing Markov chains.",
            "id": "3310306",
            "problem": "Consider a time-homogeneous Markov transition kernel $P$ on $\\mathbb{R}$ with density $p(x,y)$ with respect to the Lebesgue measure, defined for a fixed $\\lambda \\in (0,1)$ by\n$$\np(x,y) \\;=\\; \\lambda \\cdot \\frac{1}{2}\\,\\mathbf{1}_{[-1,1]}(y) \\;+\\; (1-\\lambda)\\cdot \\frac{1}{2}\\,\\exp\\!\\big(-|y-x|\\big),\n$$\nwhere $\\mathbf{1}_{[-1,1]}$ is the indicator function of the interval $[-1,1]$. Let $C = [-2,2]$. The notions of Markov transition kernel and small set are understood in the standard way: a measurable set $C$ is called $1$-small if there exist a constant $\\varepsilon \\in (0,1]$ and a probability measure $\\nu$ on $(\\mathbb{R},\\mathcal{B}(\\mathbb{R}))$ such that, for all $x \\in C$ and all measurable $A \\subset \\mathbb{R}$, one has $P(x,A) \\ge \\varepsilon\\,\\nu(A)$.\n\nStarting from first principles and using only the definitions of a Markov transition kernel, the indicator function, and the small set (minorization) condition, construct an explicit probability measure $\\nu$ and a constant $\\varepsilon$ that verify the small set condition on $C$. Among all such pairs $(\\varepsilon,\\nu)$ with $\\nu$ a probability measure, determine the largest possible value of $\\varepsilon$ as a closed-form analytic expression in terms of $\\lambda$.\n\nYour final answer must be this maximal $\\varepsilon$ expressed in simplest exact form. No rounding is required. Do not include units.",
            "solution": "We begin with the definition of a Markov transition kernel $P$ on $\\mathbb{R}$ with density $p(x,y)$ with respect to the Lebesgue measure, so that for each $x \\in \\mathbb{R}$ and measurable $A \\subset \\mathbb{R}$,\n$$\nP(x,A) \\;=\\; \\int_{A} p(x,y)\\,\\mathrm{d}y.\n$$\nA set $C \\subset \\mathbb{R}$ is called $1$-small if there exist $\\varepsilon \\in (0,1]$ and a probability measure $\\nu$ on $\\mathbb{R}$ such that, for all $x \\in C$ and all measurable $A \\subset \\mathbb{R}$,\n$$\nP(x,A) \\;\\ge\\; \\varepsilon\\,\\nu(A).\n$$\nEquivalently, writing $\\nu$ in terms of its density $v(y)$ with respect to the Lebesgue measure (when it exists), the minorization condition is verified if one can find a nonnegative function $g(y)$ and a constant $\\varepsilon$ such that, for all $x \\in C$ and almost every $y \\in \\mathbb{R}$,\n$$\np(x,y) \\;\\ge\\; g(y), \\qquad \\text{and} \\qquad \\varepsilon \\;=\\; \\int_{\\mathbb{R}} g(y)\\,\\mathrm{d}y, \\qquad v(y) \\;=\\; \\frac{g(y)}{\\varepsilon}.\n$$\nIndeed, if $p(x,y) \\ge g(y)$ for all $x \\in C$, then for all measurable $A$,\n$$\nP(x,A) \\;=\\; \\int_{A} p(x,y)\\,\\mathrm{d}y \\;\\ge\\; \\int_{A} g(y)\\,\\mathrm{d}y \\;=\\; \\varepsilon \\int_{A} \\frac{g(y)}{\\varepsilon}\\,\\mathrm{d}y \\;=\\; \\varepsilon\\,\\nu(A),\n$$\nwith $\\nu(\\mathrm{d}y) = v(y)\\,\\mathrm{d}y = \\frac{g(y)}{\\varepsilon}\\,\\mathrm{d}y$. By construction, $\\nu$ is a probability measure because\n$$\n\\int_{\\mathbb{R}} \\nu(\\mathrm{d}y) \\;=\\; \\int_{\\mathbb{R}} \\frac{g(y)}{\\varepsilon}\\,\\mathrm{d}y \\;=\\; \\frac{1}{\\varepsilon} \\int_{\\mathbb{R}} g(y)\\,\\mathrm{d}y \\;=\\; 1.\n$$\n\nWe now apply this principle to the given density\n$$\np(x,y) \\;=\\; \\lambda \\cdot \\frac{1}{2}\\,\\mathbf{1}_{[-1,1]}(y) \\;+\\; (1-\\lambda)\\cdot \\frac{1}{2}\\,\\exp\\!\\big(-|y-x|\\big),\n$$\nwith $C = [-2,2]$. For fixed $y \\in \\mathbb{R}$, we compute the pointwise lower envelope over $x \\in C$:\n$$\ng(y) \\;:=\\; \\inf_{x \\in C} p(x,y).\n$$\nWe handle the two mixture components separately. The first component does not depend on $x$ and contributes\n$$\n\\lambda \\cdot \\frac{1}{2}\\,\\mathbf{1}_{[-1,1]}(y)\n$$\nto $g(y)$ directly. For the second component, we minimize $\\frac{1}{2}\\exp(-|y-x|)$ over $x \\in [-2,2]$. Since $x \\mapsto \\exp(-|y-x|)$ is strictly decreasing in $|y-x|$, the infimum over $x \\in [-2,2]$ is attained at the point in $[-2,2]$ that maximizes $|y-x|$. The maximum distance from $y$ to the interval $[-2,2]$ is\n$$\n\\max_{x \\in [-2,2]} |y-x| \\;=\\; 2 + |y|.\n$$\nTherefore,\n$$\n\\inf_{x \\in C} \\frac{1}{2}\\exp\\!\\big(-|y-x|\\big) \\;=\\; \\frac{1}{2}\\,\\exp\\!\\big(-(2+|y|)\\big).\n$$\nCombining the two components, we obtain the common lower bound\n$$\ng(y) \\;=\\; \\lambda \\cdot \\frac{1}{2}\\,\\mathbf{1}_{[-1,1]}(y) \\;+\\; (1-\\lambda)\\cdot \\frac{1}{2}\\,\\exp\\!\\big(-(2+|y|)\\big),\n$$\nwhich satisfies $p(x,y) \\ge g(y)$ for all $x \\in C$ and all $y \\in \\mathbb{R}$.\n\nWe now compute\n$$\n\\varepsilon \\;=\\; \\int_{\\mathbb{R}} g(y)\\,\\mathrm{d}y \\;=\\; \\lambda \\int_{\\mathbb{R}} \\frac{1}{2}\\,\\mathbf{1}_{[-1,1]}(y)\\,\\mathrm{d}y \\;+\\; (1-\\lambda) \\int_{\\mathbb{R}} \\frac{1}{2}\\,\\exp\\!\\big(-(2+|y|)\\big)\\,\\mathrm{d}y.\n$$\nThe first integral equals\n$$\n\\int_{\\mathbb{R}} \\frac{1}{2}\\,\\mathbf{1}_{[-1,1]}(y)\\,\\mathrm{d}y \\;=\\; \\frac{1}{2} \\cdot 2 \\;=\\; 1.\n$$\nFor the second integral, factor out $\\exp(-2)$ and use the symmetry of the Laplace density:\n$$\n\\int_{\\mathbb{R}} \\frac{1}{2}\\,\\exp\\!\\big(-(2+|y|)\\big)\\,\\mathrm{d}y \\;=\\; \\exp(-2)\\int_{\\mathbb{R}} \\frac{1}{2}\\,\\exp(-|y|)\\,\\mathrm{d}y \\;=\\; \\exp(-2)\\cdot 1 \\;=\\; \\exp(-2).\n$$\nTherefore,\n$$\n\\varepsilon \\;=\\; \\lambda \\cdot 1 \\;+\\; (1-\\lambda)\\cdot \\exp(-2) \\;=\\; \\lambda \\;+\\; (1-\\lambda)\\,\\exp(-2).\n$$\n\nDefine the probability measure $\\nu$ by its density\n$$\nv(y) \\;=\\; \\frac{g(y)}{\\varepsilon} \\;=\\; \\frac{\\lambda \\cdot \\frac{1}{2}\\,\\mathbf{1}_{[-1,1]}(y) \\;+\\; (1-\\lambda)\\cdot \\frac{1}{2}\\,\\exp\\!\\big(-(2+|y|)\\big)}{\\lambda \\;+\\; (1-\\lambda)\\,\\exp(-2)}.\n$$\nThen, for all $x \\in C$ and all measurable $A \\subset \\mathbb{R}$,\n$$\nP(x,A) \\;=\\; \\int_{A} p(x,y)\\,\\mathrm{d}y \\;\\ge\\; \\int_{A} g(y)\\,\\mathrm{d}y \\;=\\; \\varepsilon \\int_{A} v(y)\\,\\mathrm{d}y \\;=\\; \\varepsilon\\,\\nu(A),\n$$\nverifying that $C$ is $1$-small with this explicit $\\nu$ and $\\varepsilon$.\n\nIt remains to show that this $\\varepsilon$ is maximal over all probability measures $\\nu$. Suppose there exists a probability measure $\\tilde{\\nu}$ and a constant $\\tilde{\\varepsilon}$ such that, for all $x \\in C$ and all measurable $A$,\n$$\nP(x,A) \\;\\ge\\; \\tilde{\\varepsilon}\\,\\tilde{\\nu}(A).\n$$\nIf $\\tilde{\\nu}$ has density $\\tilde{v}(y)$ with respect to the Lebesgue measure, then $p(x,y) \\ge \\tilde{\\varepsilon}\\,\\tilde{v}(y)$ for all $x \\in C$ and almost every $y$. Taking the infimum over $x \\in C$ yields $g(y) \\ge \\tilde{\\varepsilon}\\,\\tilde{v}(y)$ almost everywhere. Integrating both sides over $\\mathbb{R}$ gives\n$$\n\\varepsilon \\;=\\; \\int_{\\mathbb{R}} g(y)\\,\\mathrm{d}y \\;\\ge\\; \\tilde{\\varepsilon} \\int_{\\mathbb{R}} \\tilde{v}(y)\\,\\mathrm{d}y \\;=\\; \\tilde{\\varepsilon},\n$$\nbecause $\\tilde{\\nu}$ is a probability measure. Hence no $\\tilde{\\varepsilon}$ larger than $\\varepsilon$ is possible, and the value\n$$\n\\varepsilon \\;=\\; \\lambda \\;+\\; (1-\\lambda)\\,\\exp(-2)\n$$\nis the largest constant for which the small set minorization on $C$ holds.\n\nThus an explicit verifying pair is\n$$\n\\nu(\\mathrm{d}y) \\;=\\; \\frac{\\lambda \\cdot \\frac{1}{2}\\,\\mathbf{1}_{[-1,1]}(y) \\;+\\; (1-\\lambda)\\cdot \\frac{1}{2}\\,\\exp\\!\\big(-(2+|y|)\\big)}{\\lambda \\;+\\; (1-\\lambda)\\,\\exp(-2)}\\,\\mathrm{d}y,\n$$\nand\n$$\n\\varepsilon \\;=\\; \\lambda \\;+\\; (1-\\lambda)\\,\\exp(-2),\n$$\nwith this $\\varepsilon$ being maximal.",
            "answer": "$$\\boxed{\\lambda+(1-\\lambda)\\exp(-2)}$$"
        },
        {
            "introduction": "While a minorization condition ensures local mixing, a drift condition provides the complementary global stability, guaranteeing that the chain is recurrent and does not escape to infinity. This is achieved by identifying a Lyapunov function $V(x)$ whose expected value contracts towards the center of the state space. This exercise  guides you through the essential process of postulating and verifying a suitable Lyapunov function for a common stochastic process, a cornerstone technique for proving geometric ergodicity.",
            "id": "3347148",
            "problem": "Consider a time-homogeneous Markov chain on the real line with transition kernel defined as follows. Given the current state $x \\in \\mathbb{R}$, the next state $X'$ is generated by a mixture mechanism:\n- with probability $\\delta \\in (0,1)$, $X'$ is drawn independently from a Gaussian distribution with mean $0$ and variance $\\sigma_{0}^{2} > 0$, i.e., $X' \\sim \\mathcal{N}(0,\\sigma_{0}^{2})$;\n- with probability $1-\\delta$, $X'$ is generated by the autoregressive update $X' = \\rho x + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2})$ with $\\sigma^{2} > 0$, and $\\rho \\in \\mathbb{R}$ satisfies $|\\rho|  1$.\n\nThis kernel commonly arises in Stochastic Simulation and Markov Chain Monte Carlo (MCMC), for example when periodically refreshing from a tractable reference distribution while otherwise following a stable linear Gaussian dynamic.\n\nTasks:\n1. Starting from the foundational definitions of Markov chains and invariant distributions, state the Foster–Lyapunov drift condition that ensures the existence of a stationary (invariant) distribution and geometric ergodicity for a $\\psi$-irreducible, aperiodic Markov chain.\n2. For the given kernel, construct a Lyapunov function $V:\\mathbb{R}\\to[1,\\infty)$ and verify the drift inequality by explicitly computing $\\mathbb{E}[V(X')\\mid X=x]$ and showing it can be bounded in the Foster–Lyapunov form with appropriate constants and a suitable small set. You should justify the choice of $V$ from first principles by analyzing the one-step expectation under the transition mechanism and demonstrating contraction away from a petite set.\n\nYour final answer must be a single closed-form analytic expression for a Lyapunov function $V(x)$ that satisfies the drift condition for the specified kernel. No numerical rounding is required. Express your final answer solely as a function of $x$.",
            "solution": "This solution is presented in two parts as requested by the problem statement.\n\nPart 1: The Foster-Lyapunov Drift Condition\n\nA time-homogeneous Markov chain is a sequence of random variables $\\{X_n\\}_{n \\ge 0}$ taking values in a state space $\\mathcal{X}$, whose evolution is governed by a fixed transition kernel $P(x, A) = \\mathbb{P}(X_{n+1} \\in A \\mid X_n = x)$ for any measurable set $A \\subseteq \\mathcal{X}$. An invariant (or stationary) distribution is a probability measure $\\pi$ on $\\mathcal{X}$ such that if $X_n \\sim \\pi$, then $X_{n+1} \\sim \\pi$. This is equivalent to the condition $\\pi(A) = \\int_{\\mathcal{X}} P(x, A) \\pi(dx)$ for all measurable sets $A$.\n\nFor a Markov chain on a general state space, geometric ergodicity guarantees the existence of a unique stationary distribution $\\pi$ and that the distribution of $X_n$ converges to $\\pi$ at a geometric rate in total variation norm, irrespective of the initial state $X_0=x$. A sufficient condition for geometric ergodicity is provided by the Foster-Lyapunov drift criterion.\n\nFirst, some preliminary definitions are required. A Markov chain is $\\psi$-irreducible if for some measure $\\psi$ on $\\mathcal{X}$, for every $x \\in \\mathcal{X}$ and any set $A$ with $\\psi(A) > 0$, there exists an integer $n \\ge 1$ such that $P^n(x, A) > 0$. An irreducible chain is aperiodic if it does not get trapped in cycles. A measurable set $C \\subseteq \\mathcal{X}$ is called a small set (or $m$-small set) if there exists an integer $m \\ge 1$, a constant $\\varepsilon > 0$, and a probability measure $\\nu$ such that for all $x \\in C$, the $m$-step transition kernel satisfies $P^m(x, \\cdot) \\ge \\varepsilon \\nu(\\cdot)$. If $m=1$, the set is called one-step small. A set $C$ is petite if for some probability distribution $a$ on the non-negative integers, the kernel $K_a(x, \\cdot) = \\sum_{n=0}^{\\infty} a(n) P^n(x, \\cdot)$ satisfies $K_a(x, \\cdot) \\ge \\varepsilon \\nu(\\cdot)$ for all $x \\in C$.\n\nThe Foster-Lyapunov drift condition for geometric ergodicity states that a $\\psi$-irreducible and aperiodic Markov chain is geometrically ergodic if there exists a test function (or Lyapunov function) $V: \\mathcal{X} \\to [1, \\infty)$, a petite set $C \\subset \\mathcal{X}$, and constants $\\lambda \\in [0, 1)$ and $L  \\infty$ such that the following drift inequality holds:\n$$ \\mathbb{E}[V(X_{n+1}) \\mid X_n = x] \\le \\lambda V(x) + L \\mathbb{I}_C(x) \\quad \\text{for all } x \\in \\mathcal{X} $$\nwhere $\\mathbb{I}_C(x)$ is the indicator function, which is $1$ if $x \\in C$ and $0$ otherwise. The expectation is taken with respect to the transition kernel $P(x, \\cdot)$. An equivalent, and often more practical, formulation is that for some $\\lambda' \\in [0, 1)$ and $L'  \\infty$, the condition holds for all $x$ outside the petite set $C$:\n$$ \\mathbb{E}[V(X_{n+1}) \\mid X_n = x] \\le \\lambda' V(x) + L' \\quad \\text{for all } x \\in \\mathcal{X} \\setminus C $$\n\nPart 2: Construction and Verification of a Lyapunov Function\n\nWe are given a Markov chain on $\\mathcal{X} = \\mathbb{R}$. The transition mechanism for generating the next state $X'$ from the current state $x$ is a mixture: with probability $\\delta$, $X' \\sim \\mathcal{N}(0, \\sigma_0^2)$, and with probability $1-\\delta$, $X' = \\rho x + \\varepsilon$ where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$. The parameters satisfy $\\delta \\in (0,1)$, $|\\rho|  1$, $\\sigma_0^2 > 0$, and $\\sigma^2 > 0$.\n\nFirst, we establish the irreducibility and aperiodicity of the chain. The transition kernel $P(x, dy)$ has a component $\\delta \\phi(y; 0, \\sigma_0^2) dy$, where $\\phi(y; \\mu, s^2)$ is the Gaussian probability density function. Since $\\delta > 0$ and $\\phi(y; 0, \\sigma_0^2) > 0$ for all $y \\in \\mathbb{R}$, the transition probability from any $x$ to any open set is positive. This implies the chain is Lebesgue-irreducible (i.e., $\\psi$-irreducible with $\\psi$ being the Lebesgue measure) and aperiodic. Furthermore, this implies that any compact set in $\\mathbb{R}$ is a one-step small set, and therefore also petite.\n\nNext, we construct a Lyapunov function $V(x)$. The dynamics involve a linear autoregressive component, $X' = \\rho x + \\varepsilon$. The squaring operation inherent in calculating the second moment suggests that a quadratic function would be a natural candidate for a Lyapunov function, as the expectation of a quadratic function of the next state might be a quadratic function of the current state. We seek a function $V: \\mathbb{R} \\to [1, \\infty)$. The simplest quadratic function satisfying this condition is $V(x) = x^2 + 1$. We will use this as our candidate Lyapunov function.\n\nWe now compute the one-step expected value of $V(X')$ conditioned on $X=x$:\n$$ \\mathbb{E}[V(X') \\mid X=x] = \\mathbb{E}[(X')^2 + 1 \\mid X=x] = \\mathbb{E}[(X')^2 \\mid X=x] + 1 $$\nUsing the law of total expectation over the mixture components:\n$$ \\mathbb{E}[(X')^2 \\mid X=x] = \\delta \\cdot \\mathbb{E}[Y_0^2] + (1-\\delta) \\cdot \\mathbb{E}[Y_1^2 \\mid X=x] $$\nwhere $Y_0 \\sim \\mathcal{N}(0, \\sigma_0^2)$ and $Y_1 \\sim \\mathcal{N}(\\rho x, \\sigma^2)$.\nFor a random variable $Y \\sim \\mathcal{N}(\\mu, s^2)$, its second moment is $\\mathbb{E}[Y^2] = \\text{Var}(Y) + (\\mathbb{E}[Y])^2 = s^2 + \\mu^2$.\nApplying this, we get:\n- For the first component, $\\mathbb{E}[Y_0^2] = \\sigma_0^2 + 0^2 = \\sigma_0^2$.\n- For the second component, $\\mathbb{E}[Y_1^2 \\mid X=x] = \\sigma^2 + (\\rho x)^2 = \\sigma^2 + \\rho^2 x^2$.\n\nSubstituting these back into the expression for the conditional expectation of $(X')^2$:\n$$ \\mathbb{E}[(X')^2 \\mid X=x] = \\delta \\sigma_0^2 + (1-\\delta)(\\sigma^2 + \\rho^2 x^2) = (1-\\delta)\\rho^2 x^2 + \\delta\\sigma_0^2 + (1-\\delta)\\sigma^2 $$\nTherefore, the expected value of our Lyapunov function is:\n$$ \\mathbb{E}[V(X') \\mid X=x] = (1-\\delta)\\rho^2 x^2 + \\delta\\sigma_0^2 + (1-\\delta)\\sigma^2 + 1 $$\nTo verify the drift condition, we express the right-hand side in terms of $V(x) = x^2+1$. We have $x^2 = V(x) - 1$.\n$$ \\mathbb{E}[V(X') \\mid X=x] = (1-\\delta)\\rho^2 (V(x) - 1) + \\delta\\sigma_0^2 + (1-\\delta)\\sigma^2 + 1 $$\n$$ \\mathbb{E}[V(X') \\mid X=x] = (1-\\delta)\\rho^2 V(x) + [1 - (1-\\delta)\\rho^2 + \\delta\\sigma_0^2 + (1-\\delta)\\sigma^2] $$\nThis equation is of the form $\\mathbb{E}[V(X') \\mid X=x] = \\lambda_V V(x) + b_V$, where the drift coefficient is $\\lambda_V = (1-\\delta)\\rho^2$ and the additive constant is $b_V = 1 - (1-\\delta)\\rho^2 + \\delta\\sigma_0^2 + (1-\\delta)\\sigma^2$.\n\nThe conditions on the parameters are $\\delta \\in (0,1)$ and $|\\rho|1$. This implies $0  1-\\delta  1$ and $0 \\le \\rho^2  1$. Thus, the drift coefficient satisfies $0 \\le \\lambda_V  1$. The constant $b_V$ is finite, as all parameters are finite.\nThe derived relationship\n$$ \\mathbb{E}[V(X') \\mid X=x] = \\lambda_V V(x) + b_V $$\nholds for all $x \\in \\mathbb{R}$. This is a global drift condition, which is stronger than what is required by the Foster-Lyapunov theorem. It directly satisfies the drift inequality $\\mathbb{E}[V(X') \\mid X=x] \\le \\lambda V(x) + L$ for any choice of petite set $C$, with $\\lambda = \\lambda_V$ and $L=b_V$. The contraction towards the center of the state space is guaranteed by $\\lambda_V  1$. This confirms that $V(x) = x^2+1$ is a valid Lyapunov function for the given Markov chain, ensuring its geometric ergodicity and the existence of a unique stationary distribution.",
            "answer": "$$\n\\boxed{x^2 + 1}\n$$"
        },
        {
            "introduction": "The true power of drift analysis lies not just in proving convergence, but in quantifying and comparing the performance of different algorithms. The drift constant $\\lambda$ in the inequality $PV(x) \\le \\lambda V(x) + b\\mathbf{1}_C(x)$ directly relates to the rate of convergence, with smaller values indicating faster mixing. This advanced exercise  applies the drift framework to compare two Gibbs sampling strategies, revealing a deep connection between an algorithm's efficiency and the target distribution's correlation structure.",
            "id": "3310291",
            "problem": "Consider a zero-mean bivariate Gaussian target distribution used in Markov Chain Monte Carlo (MCMC), with covariance matrix\n$$\n\\Sigma \\;=\\; \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}, \\qquad |\\rho|1,\n$$\nand let $\\kappa(\\Sigma)$ denote the matrix condition number of $\\Sigma$, defined by $\\kappa(\\Sigma) \\equiv \\lambda_{\\max}(\\Sigma)/\\lambda_{\\min}(\\Sigma)$. Two Gibbs sampling schemes are considered:\n\n1. A systematic coordinate-wise Gibbs sampler that performs one full sweep by first updating $X$ from the conditional distribution of $X$ given $Y$, and then updating $Y$ from the conditional distribution of $Y$ given the newly updated $X$.\n\n2. A block Gibbs sampler that updates the entire vector $(X,Y)$ jointly by drawing a fresh sample from the target distribution $\\mathcal{N}\\!\\left(0,\\Sigma\\right)$.\n\nLet the drift (Lyapunov) function be\n$$\nV(x,y) \\;=\\; y^{2}.\n$$\nFor each sampler, let $P$ denote its Markov transition kernel acting on functions, and consider the drift inequality\n$$\n(PV)(x,y) \\;\\le\\; \\lambda\\, V(x,y) + b\\,\\mathbf{1}_{C}(x,y),\n$$\nwith a measurable set $C \\subseteq \\mathbb{R}^{2}$, constant $b \\ge 0$, and multiplicative drift constant $\\lambda \\in [0,1)$. For this problem, take $C=\\mathbb{R}^{2}$ so that $\\mathbf{1}_{C}(x,y)=1$ for all $(x,y)$, and focus on identifying the exact multiplicative drift constants $\\lambda$ (the smallest possible value making the inequality true with some finite $b$) for both samplers.\n\nStarting only from the definitions of Gibbs sampling for multivariate Gaussians and the drift inequality, derive the multiplicative drift constants for the coordinate-wise and block Gibbs samplers in terms of the condition number $\\kappa(\\Sigma)$. Report your final answer as the ordered pair $(\\lambda_{\\text{coord}},\\lambda_{\\text{block}})$ expressed analytically in $\\kappa(\\Sigma)$.",
            "solution": "The problem requires the derivation of the exact multiplicative drift constants, $\\lambda$, for two different Gibbs samplers targeting a zero-mean bivariate Gaussian distribution with covariance matrix $\\Sigma = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$. The drift function is given by $V(x,y) = y^2$, and the drift inequality is $(PV)(x,y) \\le \\lambda V(x,y) + b$ for some constant $b \\ge 0$, where the small set $C$ is taken to be the entire state space $\\mathbb{R}^2$.\n\nFirst, we establish the relationship between the matrix condition number $\\kappa(\\Sigma)$ and the correlation coefficient $\\rho$. The condition number is defined as the ratio of the largest to the smallest eigenvalue of $\\Sigma$. The eigenvalues $\\lambda_e$ are found by solving the characteristic equation $\\det(\\Sigma - \\lambda_e I) = 0$.\n$$\n\\det\\begin{pmatrix} 1-\\lambda_e  \\rho \\\\ \\rho  1-\\lambda_e \\end{pmatrix} = (1-\\lambda_e)^2 - \\rho^2 = 0\n$$\nThis gives $1-\\lambda_e = \\pm \\rho$, so the eigenvalues are $\\lambda_e = 1 \\mp \\rho$. Given the constraint $|\\rho|  1$, both eigenvalues are positive, and the matrix $\\Sigma$ is positive definite. The largest and smallest eigenvalues are $\\lambda_{\\max}(\\Sigma) = 1 + |\\rho|$ and $\\lambda_{\\min}(\\Sigma) = 1 - |\\rho|$, respectively.\n\nThe condition number is therefore:\n$$\n\\kappa(\\Sigma) = \\frac{\\lambda_{\\max}(\\Sigma)}{\\lambda_{\\min}(\\Sigma)} = \\frac{1 + |\\rho|}{1 - |\\rho|}\n$$\nWe can express $|\\rho|$ in terms of $\\kappa(\\Sigma)$, which we denote as $\\kappa$ for brevity:\n$$\n\\kappa(1 - |\\rho|) = 1 + |\\rho| \\implies \\kappa - \\kappa|\\rho| = 1 + |\\rho| \\implies \\kappa - 1 = |\\rho|(\\kappa + 1)\n$$\n$$\n|\\rho| = \\frac{\\kappa - 1}{\\kappa + 1}\n$$\nThis relationship will be used to express the final answer.\n\nNext, we analyze each sampler to find its drift constant.\n\n**Sampler 2: Block Gibbs Sampler**\n\nThe block Gibbs sampler updates the entire vector $(X,Y)$ by drawing a new sample $(X',Y')$ directly from the target distribution $\\pi = \\mathcal{N}(0, \\Sigma)$. The new state is thereby independent of the current state $(x,y)$. The operator $(P_{\\text{block}}V)(x,y)$ is the expectation of $V(X',Y')$ where $(X',Y') \\sim \\pi$.\n$$\n(P_{\\text{block}}V)(x,y) = \\mathbb{E}_{(X',Y')\\sim\\pi}[V(X',Y')] = \\mathbb{E}_{\\pi}[(Y')^2]\n$$\nSince $(X',Y')$ is drawn from a zero-mean Gaussian, the expectation of the square of a component is its variance. The marginal distribution of $Y'$ is $\\mathcal{N}(0, \\Sigma_{22})$, where $\\Sigma_{22} = 1$. Thus, $\\mathbb{E}_{\\pi}[(Y')^2] = \\text{Var}(Y') = 1$.\nThe value of $(P_{\\text{block}}V)(x,y)$ is constant and equal to $1$.\n\nThe drift inequality becomes:\n$$\n1 \\le \\lambda_{\\text{block}} V(x,y) + b \\implies 1 \\le \\lambda_{\\text{block}} y^2 + b\n$$\nWe seek the smallest constant $\\lambda_{\\text{block}} \\in [0,1)$ for which this inequality holds for all $y \\in \\mathbb{R}$ for some finite $b \\ge 0$. If we choose $\\lambda_{\\text{block}} = 0$, the inequality becomes $1 \\le b$. We can satisfy this by choosing any $b \\ge 1$, for instance $b=1$. Since a valid solution exists for $\\lambda_{\\text{block}} = 0$, and $\\lambda$ must be non-negative, the smallest possible value is $\\lambda_{\\text{block}} = 0$. This aligns with the understanding that an i.i.d. sampler from the target distribution exhibits the fastest possible convergence.\n\n**Sampler 1: Systematic Coordinate-Wise Gibbs Sampler**\n\nThis sampler performs a full sweep, first updating $X$ and then updating $Y$. Let the state at the beginning of the sweep be $(x,y)$.\n1.  Draw $X' \\sim p(X|Y=y)$.\n2.  Draw $Y' \\sim p(Y|X=X')$.\nThe resulting state is $(X',Y')$. We need to compute $(P_{\\text{coord}}V)(x,y) = \\mathbb{E}[V(X',Y') | (x,y)] = \\mathbb{E}[ (Y')^2 | (x,y) ]$.\n\nFor the given bivariate Gaussian, the conditional distributions are:\n$p(X|Y=y) \\sim \\mathcal{N}(\\rho y, 1-\\rho^2)$\n$p(Y|X=x) \\sim \\mathcal{N}(\\rho x, 1-\\rho^2)$\n\nLet us model the sampling steps. Let $\\epsilon_X$ and $\\epsilon_Y$ be independent standard normal random variables, $\\epsilon_X, \\epsilon_Y \\sim \\mathcal{N}(0,1)$.\nThe new state components can be written as:\nStep 1: $X' = \\rho y + \\sqrt{1-\\rho^2} \\epsilon_X$\nStep 2: $Y' = \\rho X' + \\sqrt{1-\\rho^2} \\epsilon_Y$\n\nSubstitute the expression for $X'$ into the expression for $Y'$:\n$$\nY' = \\rho \\left( \\rho y + \\sqrt{1-\\rho^2} \\epsilon_X \\right) + \\sqrt{1-\\rho^2} \\epsilon_Y = \\rho^2 y + \\rho\\sqrt{1-\\rho^2} \\epsilon_X + \\sqrt{1-\\rho^2} \\epsilon_Y\n$$\nNow we compute the expectation of $(Y')^2$, conditional on the starting state $(x,y)$. Note that the result will only depend on $y$.\n$$\n\\mathbb{E}[(Y')^2 | y] = \\mathbb{E}\\left[ \\left( \\rho^2 y + \\rho\\sqrt{1-\\rho^2} \\epsilon_X + \\sqrt{1-\\rho^2} \\epsilon_Y \\right)^2 \\right]\n$$\nWhen expanding the square, the cross-terms involving a single $\\epsilon_X$ or $\\epsilon_Y$ will have an expectation of zero, since $\\mathbb{E}[\\epsilon_X]=\\mathbb{E}[\\epsilon_Y]=0$ and $\\epsilon_X, \\epsilon_Y$ are independent. We are left with the expectations of the squared terms:\n$$\n\\mathbb{E}[(Y')^2 | y] = \\mathbb{E}\\left[ (\\rho^2 y)^2 + (\\rho\\sqrt{1-\\rho^2} \\epsilon_X)^2 + (\\sqrt{1-\\rho^2} \\epsilon_Y)^2 \\right]\n$$\n$$\n= \\rho^4 y^2 + \\rho^2(1-\\rho^2)\\mathbb{E}[\\epsilon_X^2] + (1-\\rho^2)\\mathbb{E}[\\epsilon_Y^2]\n$$\nSince $\\mathbb{E}[\\epsilon_X^2] = \\text{Var}(\\epsilon_X)=1$ and $\\mathbb{E}[\\epsilon_Y^2] = \\text{Var}(\\epsilon_Y)=1$, we have:\n$$\n(P_{\\text{coord}}V)(x,y) = \\rho^4 y^2 + \\rho^2(1-\\rho^2) + (1-\\rho^2) = \\rho^4 y^2 + (1-\\rho^2)(1+\\rho^2) = \\rho^4 y^2 + 1 - \\rho^4\n$$\nThe drift inequality is:\n$$\n\\rho^4 y^2 + 1 - \\rho^4 \\le \\lambda_{\\text{coord}} y^2 + b\n$$\nRearranging the terms, we get:\n$$\n(\\rho^4 - \\lambda_{\\text{coord}}) y^2 + (1 - \\rho^4 - b) \\le 0\n$$\nFor this inequality to hold for all $y \\in \\mathbb{R}$, the coefficient of the quadratic term $y^2$ must be non-positive, as otherwise the left-hand side would grow to $+\\infty$ as $|y| \\to \\infty$.\n$$\n\\rho^4 - \\lambda_{\\text{coord}} \\le 0 \\implies \\lambda_{\\text{coord}} \\ge \\rho^4\n$$\nThe smallest possible value for the drift constant is $\\lambda_{\\text{coord}} = \\rho^4$. With this choice, the inequality becomes $1 - \\rho^4 - b \\le 0$, which can be satisfied by choosing any $b \\ge 1 - \\rho^4$. Since $|\\rho|1$, $1-\\rho^4 > 0$, so a valid non-negative $b$ exists.\n\nThe drift constant is $\\lambda_{\\text{coord}} = \\rho^4 = (|\\rho|)^4$. We now substitute the expression for $|\\rho|$ in terms of the condition number $\\kappa = \\kappa(\\Sigma)$:\n$$\n\\lambda_{\\text{coord}} = \\left( \\frac{\\kappa - 1}{\\kappa + 1} \\right)^4\n$$\nThe problem asks for the ordered pair $(\\lambda_{\\text{coord}}, \\lambda_{\\text{block}})$.\n\nThus, the drift constant for the coordinate-wise sampler is $\\left(\\frac{\\kappa(\\Sigma) - 1}{\\kappa(\\Sigma) + 1}\\right)^4$, and the drift constant for the block sampler is $0$.",
            "answer": "$$\n\\boxed{\\left( \\left(\\frac{\\kappa(\\Sigma) - 1}{\\kappa(\\Sigma) + 1}\\right)^{4}, 0 \\right)}\n$$"
        }
    ]
}