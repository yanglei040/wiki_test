{
    "hands_on_practices": [
        {
            "introduction": "马尔可夫链收敛理论在谷歌的PageRank算法中得到了强有力的实际应用。这项练习  提供了一个动手实践的机会，让你通过将PageRank向量视为一个精心构造的马尔可夫链的平稳分布，来亲手实现这一算法。通过模拟幂迭代法，你将从数值上探究收敛速度如何受到关键参数（如“瞬移”概率 $\\alpha$）和底层网络图结构的影响。",
            "id": "3300074",
            "problem": "将谷歌PageRank公式视为有限有向图上带有瞬移的马尔可夫链的平稳分布。令 $n \\in \\mathbb{N}$ 表示节点数，令 $W \\in \\{0,1\\}^{n \\times n}$ 为邻接矩阵，按列定义如下：若存在从节点 $j$ 到节点 $i$ 的有向边，则 $W_{ij} = 1$，否则 $W_{ij} = 0$。定义瞬移分布 $v \\in \\mathbb{R}^n$ 为均匀概率向量，$v_i = 1/n$ 对所有 $i \\in \\{1,\\dots,n\\}$ 成立。通过对 $W$ 的每一列进行归一化来定义列随机矩阵 $S \\in \\mathbb{R}^{n \\times n}$；如果某列 $j$ 的和为零（一个悬挂节点），则设 $S_{\\cdot j} = v$。对于瞬移概率 $\\alpha \\in [0,1)$，考虑PageRank迭代\n$$\nx_{k+1} \\;=\\; \\alpha S x_k \\;+\\; (1 - \\alpha)\\, v,\\quad k = 0,1,2,\\dots,\n$$\n初始分布为 $x_0 = v$。平稳分布 $x^\\star \\in \\mathbb{R}^n$ 是以下方程组的唯一解\n$$\nx^\\star \\;=\\; \\alpha S x^\\star \\;+\\; (1-\\alpha)\\, v, \\qquad x^\\star \\ge 0, \\;\\; \\sum_{i=1}^n x^\\star_i = 1,\n$$\n等价于\n$$\n(I - \\alpha S)\\, x^\\star \\;=\\; (1-\\alpha)\\, v.\n$$\n使用全变差距离 $d_{\\mathrm{TV}}(p,q) = \\frac{1}{2}\\lVert p - q\\rVert_1$ 来量化收敛性。目标是通过幂迭代模拟向 $x^\\star$ 的收敛，并量化收敛速度对 $\\alpha$ 及悬挂节点存在的依赖性。\n\n基于以下基本事实：(i) 根据构造，$S$ 是列随机的；(ii) 对任意 $\\alpha \\in [0,1)$，$x^\\star$ 存在且唯一；(iii) 迭代是线性的。请设计并实现一个程序，对下述每个测试用例执行以下任务：\n1. 根据上述定义，从 $W$ 和 $v$ 构造 $S$。\n2. 通过求解 $(I - \\alpha S)\\,x^\\star = (1-\\alpha) v$ 并归一化为概率向量，计算精确的平稳分布 $x^\\star$。\n3. 运行幂迭代 $x_{k+1} = \\alpha S x_k + (1 - \\alpha) v$，从 $x_0 = v$ 开始，直到满足停止准则 $d_{\\mathrm{TV}}(x_k, x^\\star) \\le \\varepsilon$，或达到最大迭代次数 $K_{\\max} = 100000$。对所有情况使用 $\\varepsilon = 10^{-10}$。\n4. 记录首次满足停止准则时执行的迭代次数 $K$（如果初始的 $x_0$ 已满足准则，则 $K=0$）。如果在 $K_{\\max}$ 次迭代内未满足停止准则，则设 $K=K_{\\max}$。\n5. 通过计算运行过程中 $k \\ge 1$ 的比率 $e_k/e_{k-1}$，从 $\\ell_1$ 误差序列 $e_k = \\lVert x_k - x^\\star \\rVert_1$ 中估计经验线性收敛率 $r_{\\mathrm{hat}}$，并报告最后 $\\min\\{50, \\text{比率数量}\\}$ 个比率的中位数。如果可用迭代次数少于两次，则设 $r_{\\mathrm{hat}}=0.0$。此 $r_{\\mathrm{hat}}$ 近似了零和子空间上的渐进收缩因子，并反映了对 $\\alpha$ 和悬挂节点的敏感性。\n\n您必须为以下图和参数的测试套件实现上述过程。每个图由其邻接矩阵 $W$（使用上述基于列的约定）指定，每个测试用例使用均匀的 $v$ 和 $\\varepsilon = 10^{-10}$：\n\n- 测试用例1（无悬挂节点，中等瞬移概率）：$n=4$，$\\alpha=0.85$，\n$$\nW = \\begin{bmatrix}\n0  0  0  1 \\\\\n1  0  0  0 \\\\\n0  1  0  0 \\\\\n0  0  1  0\n\\end{bmatrix}.\n$$\n- 测试用例2（无悬挂节点，慢混合）：$n=4$，$\\alpha=0.99$，使用与测试用例1相同的 $W$。\n- 测试用例3（一个悬挂节点，中等瞬移概率）：$n=3$，$\\alpha=0.85$，\n$$\nW = \\begin{bmatrix}\n0  1  0 \\\\\n1  0  0 \\\\\n0  0  0\n\\end{bmatrix}.\n$$\n- 测试用例4（一个悬挂节点，慢混合）：$n=3$，$\\alpha=0.99$，使用与测试用例3相同的 $W$。\n- 测试用例5（无悬挂节点的可约图，中等瞬移概率）：$n=4$，$\\alpha=0.90$，\n$$\nW = \\begin{bmatrix}\n0  1  0  0 \\\\\n1  0  0  0 \\\\\n0  0  0  1 \\\\\n0  0  1  0\n\\end{bmatrix}.\n$$\n- 测试用例6（瞬移占主导）：$n=4$，$\\alpha=0.00$，使用与测试用例1相同的 $W$。\n\n您的程序必须生成单行输出，其中包含六个测试用例的结果，形式为方括号括起来的逗号分隔列表。每个测试用例的结果必须是按顺序排列的双元素列表 $[K, r_{\\mathrm{hat}}]$。因此，输出格式必须严格为\n\"[ [K1,rhat1],[K2,rhat2],[K3,rhat3],[K4,rhat4],[K5,rhat5],[K6,rhat6] ]\"\n除了所示空格外，不需要其他空格，浮点数以十进制或科学记数法表示。不涉及任何物理单位或角度，输出中也不出现百分比。所有计算均为纯数值且无量纲。",
            "solution": "问题陈述已经过严格验证，被认为是合理的。它在科学上基于马尔可夫链理论和数值线性代数，特别是PageRank算法。该问题是适定的，提供了所有必要的数据、定义和约束，以确保一个唯一、稳定且有意义的解。目标明确，并以精确的数学语言表达。因此，我们可以着手求解。\n\n该问题要求针对一组指定的图和参数，实现并分析PageRank算法。对于每个测试用例，解决方案涉及五个主要任务：\n1.  构造谷歌矩阵的分量 $S$。\n2.  计算精确的平稳PageRank向量 $x^\\star$。\n3.  使用幂迭代模拟向 $x^\\star$ 的收敛。\n4.  确定达到指定容差所需的迭代次数 $K$。\n5.  估计经验收敛率 $r_{\\mathrm{hat}}$。\n\n我们现在将详细说明每个任务的方法。\n\n**1. 随机矩阵 $S$ 的构造**\n\n矩阵 $S \\in \\mathbb{R}^{n \\times n}$ 是从图的邻接矩阵 $W \\in \\{0,1\\}^{n \\times n}$ 导出的。问题为 $W$ 指定了基于列的约定，其中 $W_{ij} = 1$ 表示从节点 $j$ 到节点 $i$ 的一条有向边。$S$ 是一个按如下方式构造的列随机矩阵：\n\n对于 $W$ 的每一列 $j \\in \\{1, \\dots, n\\}$：\n- 令 $c_j = \\sum_{i=1}^n W_{ij}$ 为第 $j$ 列元素的总和，这对应于节点 $j$ 的出度。\n- 如果 $c_j  0$，则节点 $j$ 有出链。$S$ 的相应列 $S_{\\cdot j}$ 通过对列 $W_{\\cdot j}$ 进行归一化得到：\n$$S_{ij} = \\frac{W_{ij}}{c_j} \\quad \\text{for } i=1, \\dots, n.$$\n- 如果 $c_j = 0$，节点 $j$ 是一个没有出链的“悬挂节点”。为确保所得矩阵是随机的（即其各列之和为1），列 $S_{\\cdot j}$ 被设置为均匀瞬移向量 $v \\in \\mathbb{R}^n$，其中对所有 $i$ 都有 $v_i = 1/n$。\n$$S_{\\cdot j} = v = \\begin{bmatrix} 1/n  1/n  \\dots  1/n \\end{bmatrix}^T.$$\n通过这种构造， $S$ 的每一列都是非负的且和为1，这使得 $S$ 成为一个列随机矩阵。\n\n**2. 精确平稳分布 $x^\\star$ 的计算**\n\n平稳分布，或称PageRank向量，$x^\\star$，是PageRank迭代的唯一不动点概率向量。它满足方程：\n$$x^\\star = \\alpha S x^\\star + (1-\\alpha)v$$\n其中 $\\alpha \\in [0,1)$ 是瞬移概率。该方程可以重排为一个标准的线性系统：\n$$I x^\\star - \\alpha S x^\\star = (1-\\alpha)v$$\n$$(I - \\alpha S) x^\\star = (1-\\alpha)v$$\n其中 $I$ 是 $n \\times n$ 的单位矩阵。\n\n对于 $\\alpha \\in [0,1)$，解 $x^\\star$ 的存在性和唯一性是有保证的。矩阵 $S$ 是随机的，因此其谱半径（其特征值的最大模）为 $\\rho(S)=1$。$\\alpha S$ 的谱半径为 $\\rho(\\alpha S) = \\alpha \\rho(S) = \\alpha$。由于 $\\alpha  1$，$\\alpha S$ 的所有特征值都严格位于复平面的单位圆内。这确保了 $1$ 不是 $\\alpha S$ 的特征值，因此矩阵 $(I - \\alpha S)$ 是可逆的。\n\n唯一解 $x^\\star$ 可以通过求解这个线性系统来计算。虽然问题陈述建议对结果进行归一化，但该系统的解保证是一个概率向量（其分量之和为1），因此归一化是多余但无害的。\n\n**3. 幂迭代模拟**\n\nPageRank向量 $x^\\star$ 是使用幂法迭代计算的。迭代定义为：\n$$x_{k+1} = \\alpha S x_k + (1-\\alpha)v$$\n从初始分布 $x_0 = v$ 开始。这个过程持续进行，直到迭代向量 $x_k$ 足够接近真实的平稳分布 $x^\\star$。收敛性使用全变差距离 $d_{\\mathrm{TV}}(p,q) = \\frac{1}{2} \\lVert p - q \\rVert_1$ 来衡量。当满足条件 $d_{\\mathrm{TV}}(x_k, x^\\star) \\le \\varepsilon$ 时，对于给定的容差 $\\varepsilon = 10^{-10}$，迭代停止。记录满足此准则所需的迭代次数 $K$。如果在最大迭代次数 $K_{\\max} = 100000$ 内未满足该准则，则将 $K$ 设为 $K_{\\max}$。如果初始向量 $x_0$ 已满足该准则，则 $K=0$。\n\n**4. 经验收敛率 $r_{\\mathrm{hat}}$ 的估计**\n\n幂迭代的收敛是线性的。第 $k$ 步的误差向量是 $e_k = x_k - x^\\star$。误差传播如下：\n$$e_{k+1} = x_{k+1} - x^\\star = (\\alpha S x_k + (1-\\alpha)v) - (\\alpha S x^\\star + (1-\\alpha)v) = \\alpha S (x_k - x^\\star) = \\alpha S e_k.$$\n取 $\\ell_1$ 范数，我们得到 $\\lVert e_{k+1} \\rVert_1 = \\alpha \\lVert S e_k \\rVert_1$。$\\ell_1$ 误差 $\\lVert e_k \\rVert_1$ 的渐进收敛率由算子 $\\alpha S$ 在零和向量子空间上的作用决定（因为 $x_k$ 和 $x^\\star$ 都是概率向量，它们的差 $e_k$ 的和为零）。理论收敛率与 $\\alpha$ 以及 $S$ 的谱性质有关。\n\n问题要求对此速率进行 *经验* 估计，即 $r_{\\mathrm{hat}}$。这是根据幂迭代期间生成的 $\\ell_1$ 误差序列 $e_k = \\lVert x_k - x^\\star \\rVert_1$ 计算的。对于 $k \\ge 1$，比率 $e_k/e_{k-1}$ 近似了每一步的误差减少量。为了获得渐进速率的稳定估计，我们计算这些比率中最后 $\\min\\{50, K\\}$ 个的中位数，其中 $K$ 是执行的总迭代次数。如果生成的迭代次数少于两次（即 $K=0$），意味着无法计算出比率，则将 $r_{\\mathrm{hat}}$ 设为 $0.0$。\n\n以下程序为六个指定的测试用例实现了这整个过程，并严格遵守所描述的方法。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the PageRank convergence problem for all test cases.\n    \"\"\"\n\n    def solve_one_case(n, alpha, W, epsilon, k_max):\n        \"\"\"\n        Solves a single test case of the PageRank problem.\n\n        Args:\n            n (int): Number of nodes.\n            alpha (float): Teleportation probability.\n            W (np.ndarray): Adjacency matrix (column-based).\n            epsilon (float): Convergence tolerance for total variation distance.\n            k_max (int): Maximum number of iterations.\n\n        Returns:\n            list: A list containing [K, r_hat], the number of iterations and the\n                  empirical convergence rate.\n        \"\"\"\n        # 1. Construct the stochastic matrix S\n        v = np.ones((n, 1)) / n\n        S = np.zeros((n, n), dtype=np.float64)\n        col_sums = np.sum(W, axis=0)\n        for j in range(n):\n            if col_sums[j] == 0:\n                S[:, j] = v.flatten()\n            else:\n                S[:, j] = W[:, j] / col_sums[j]\n\n        # 2. Compute the exact stationary distribution x_star\n        I = np.identity(n, dtype=np.float64)\n        A = I - alpha * S\n        b = (1 - alpha) * v\n        x_star = np.linalg.solve(A, b)\n        # Normalization is generally redundant but specified as a safeguard\n        x_star /= np.sum(x_star)\n\n        # 3.  4. Run power iteration and find K\n        x_k = v.copy()\n        errors_l1 = []\n        K = 0\n\n        # Loop from k=0 to k_max\n        for k_iter in range(k_max + 1):\n            e_k = np.linalg.norm(x_k - x_star, ord=1)\n            errors_l1.append(e_k)\n            d_tv = 0.5 * e_k\n\n            if d_tv = epsilon:\n                K = k_iter\n                break\n            \n            if k_iter == k_max:\n                K = k_max\n                # The loop will naturally break here, K is set correctly.\n                # No need for special handling if limit is reached.\n                break\n\n            # Update for next iteration\n            x_k = alpha * S @ x_k + (1 - alpha) * v\n        else:\n            # This part of loop is only executed if the loop completes without a break.\n            # In our case, range(k_max + 1) will always terminate.\n            # We set K=k_max if the loop finishes without meeting the criterion.\n            # The break statement at k_iter == k_max handles this.\n            K = k_max\n\n        # 5. Estimate empirical convergence rate r_hat\n        if len(errors_l1)  2:  # K=0 or K=1 case, not enough data for a ratio\n            r_hat = 0.0\n        else:\n            # Calculate ratios e_k / e_{k-1}\n            ratios = [errors_l1[i] / errors_l1[i-1] \n                      for i in range(1, len(errors_l1)) if errors_l1[i-1] > 0]\n\n            if not ratios:\n                r_hat = 0.0\n            else:\n                num_ratios_to_consider = min(50, len(ratios))\n                last_ratios = ratios[-num_ratios_to_consider:]\n                r_hat = np.median(last_ratios)\n\n        return [K, r_hat]\n\n    # --- Test Case Definitions ---\n    # Epsilon and K_max are common for all cases\n    epsilon = 1e-10\n    k_max = 100000\n\n    test_cases = [\n        # Case 1: n=4, alpha=0.85, W for a 4-cycle\n        (4, 0.85, np.array([[0, 0, 0, 1], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]], dtype=np.float64)),\n        # Case 2: n=4, alpha=0.99, W for a 4-cycle (slower mixing)\n        (4, 0.99, np.array([[0, 0, 0, 1], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]], dtype=np.float64)),\n        # Case 3: n=3, alpha=0.85, W with one dangling node\n        (3, 0.85, np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]], dtype=np.float64)),\n        # Case 4: n=3, alpha=0.99, W with one dangling node (slower mixing)\n        (3, 0.99, np.array([[0, 1, 0], [1, 0, 0], [0, 0, 0]], dtype=np.float64)),\n        # Case 5: n=4, alpha=0.90, reducible graph (two 2-cycles)\n        (4, 0.90, np.array([[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]], dtype=np.float64)),\n        # Case 6: n=4, alpha=0.00, teleportation only\n        (4, 0.00, np.array([[0, 0, 0, 1], [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]], dtype=np.float64)),\n    ]\n\n    results = []\n    for n, alpha, W in test_cases:\n        result = solve_one_case(n, alpha, W, epsilon, k_max)\n        results.append(result)\n\n    # Format the final output string exactly as required.\n    # The default string representation of a list of lists matches the format needs.\n    # E.g., str([[1, 2.0], [3, 4.0]]) -> '[[1, 2.0], [3, 4.0]]'\n    # The request is for \"[ [K1,rhat1],[K2,rhat2]... ]\" which str() does not produce.\n    # The template `print(f\"[{','.join(map(str, results))}]\")` will produce a string like\n    # '[[71, 0.85],[498, 0.99]]' which is a standard compact representation.\n    print(f\"[{','.join(map(str, results))}]\".replace(\" \", \"\"))\n\nsolve()\n```"
        },
        {
            "introduction": "一个马尔可夫链即使对于某个目标分布满足细致平衡条件，也并不能自动保证其会收敛到该分布。这个问题  展示了一个Metropolis-Hastings采样器，其提议机制中一个看似无害的设计，在仔细检查后会发现它违反了至关重要的不可约性。这项练习旨在磨练你的诊断技能，让你识别出确定性约束如何将状态空间分割，从而导致多个平稳分布，这是MCMC算法设计中的一个常见陷阱。",
            "id": "3300040",
            "problem": "考虑一个马尔可夫链蒙特卡洛（MCMC）方案，其旨在从整数格点 $\\mathbb{Z}$ 上的目标分布 $\\pi$ 中采样，其中对于 $k \\in \\mathbb{Z}$，$\\pi(k) \\propto \\exp(-|k|)$。该算法是一个 Metropolis-Hastings (MH) 方案，其提议机制中有一个确定性约束：从当前状态 $x \\in \\mathbb{Z}$ 开始，它以给定的 $q(y \\mid x)$ 提议 $y \\in \\{x-2, x, x+2\\}$，其中 $q(x-2 \\mid x) = p$，$q(x \\mid x) = 1 - 2p$，$q(x+2 \\mid x) = p$ (对于某个固定的 $p \\in (0, 1/2)$)，并使用标准的 MH 接受规则。根据构造，该提议保留了状态的奇偶性；也就是说，如果 $x$ 是偶数（或奇数），那么任何提议的 $y$ 也将是偶数（或奇数）。假设对于所有 $k \\in \\mathbb{Z}$，都有 $\\pi(k)  0$。\n\n供参考的基础定义：\n- 一个在可数状态空间上具有转移核 $P$ 的马尔可夫链是不可约的，如果对于所有状态 $x, y$，存在一个 $n \\in \\mathbb{N}$ 使得 $P^n(x, y)  0$。\n- 状态 $x$ 的周期 $d(x)$ 定义为 $d(x) = \\gcd\\{n \\geq 1 : P^n(x, x)  0\\}$。如果对于关心的互通类中的所有 $x$，都有 $d(x) = 1$，则该链是非周期的。\n- 如果一个概率测度 $\\mu$ 满足 $\\mu P = \\mu$，则它对于 $P$ 是平稳的（不变的）。\n\n在此设定下，请识别关于所述链的遍历性、不变分布和收敛行为的正确陈述。\n\nA. 该链在 $\\mathbb{Z}$ 上是不可约的，并且无论初始化如何，都收敛到唯一的平稳分布 $\\pi$。\n\nB. 该链是非周期的但可约，具有两个不相交的闭互通类，即偶数集和奇数集；该链从不在这些类之间转移。\n\nC. 存在无穷多个形式为 $\\mu_\\alpha = \\alpha \\,\\pi_{\\mathrm{even}} + (1-\\alpha)\\,\\pi_{\\mathrm{odd}}$（其中 $\\alpha \\in [0,1]$）的不变分布，其中 $\\pi_{\\mathrm{even}}$ 和 $\\pi_{\\mathrm{odd}}$ 分别是 $\\pi$ 限制在偶数集和奇数集上的归一化。如果从一个偶数状态初始化，该链收敛到 $\\pi_{\\mathrm{even}}$。\n\nD. 因为 MH 接受规则强制了关于 $\\pi$ 的细致平衡，所以不可约性不是必需的；该链从任何初始状态都收敛到 $\\pi$。\n\nE. 对于任何可积函数 $f$，经验平均 $\\frac{1}{T} \\sum_{t=1}^T f(X_t)$ 几乎必然收敛到 $\\mathbb{E}_\\pi[f(X)]$，且与初始化无关，因为该链是非周期的。",
            "solution": "该问题涉及一个 Metropolis-Hastings (MH) 算法，旨在从状态空间 $\\mathcal{S} = \\mathbb{Z}$ 上的目标分布 $\\pi(k) \\propto \\exp(-|k|)$ 中采样。所得马尔可夫链的转移核 $P(x,y)$ 决定了其性质。\n\n**1. 提议分布 $q(y \\mid x)$ 的分析**\n\n提议分布由 $q(x-2 \\mid x) = p$，$q(x+2 \\mid x) = p$ 和 $q(x \\mid x) = 1-2p$ 给出。这个提议分布是对称的：$q(y \\mid x) = q(x \\mid y)$。例如，如果 $y = x+2$，那么 $x=y-2$。此提议的概率为 $q(x+2 \\mid x) = p$，而反向提议的概率为 $q(x \\mid y) = q(y-2 \\mid y) = p$。\n\n**2. MH 接受概率 $\\alpha(x,y)$**\n\n标准的 MH 接受概率是 $\\alpha(x, y) = \\min\\left(1, \\frac{\\pi(y)q(x \\mid y)}{\\pi(x)q(y \\mid x)}\\right)$。由于提议分布的对称性，接受概率简化为 Metropolis 算法的形式：\n$$ \\alpha(x, y) = \\min\\left(1, \\frac{\\pi(y)}{\\pi(x)}\\right) = \\min\\left(1, \\exp(|x| - |y|)\\right) $$\n由于对于所有 $k \\in \\mathbb{Z}$，$\\pi(k)  0$，因此对于任何可能进行提议的配对 $(x, y)$（即 $y \\in \\{x-2, x+2\\}$），接受概率 $\\alpha(x, y)$ 都是严格为正的。\n\n**3. 不可约性分析**\n\n一个马尔可夫链在 $\\mathbb{Z}$ 上是不可约的，如果对于任何 $x, y \\in \\mathbb{Z}$，存在一个整数 $n \\geq 1$ 使得 $P^n(x, y)  0$。提议机制只允许在相同奇偶性的状态之间跳转。如果 $x$ 是偶数，所有提议的状态 $y \\in \\{x-2, x, x+2\\}$ 也都是偶数。如果 $x$ 是奇数，所有提议的状态也都是奇数。\n\n因此，该链永远无法从偶数状态转移到奇数状态，反之亦然。状态空间 $\\mathbb{Z}$ 被划分为两个不相交的、闭合的互通状态集：\n*   $\\mathcal{S}_E = \\{ k \\in \\mathbb{Z} : k \\text{ 是偶数} \\}$\n*   $\\mathcal{S}_O = \\{ k \\in \\mathbb{Z} : k \\text{ 是奇数} \\}$\n\n由于无法从 $\\mathcal{S}_E$ 中的状态转移到 $\\mathcal{S}_O$ 中的状态，该链在 $\\mathbb{Z}$ 上是**可约的**。在每个子类（$\\mathcal{S}_E$ 和 $\\mathcal{S}_O$）内部，链是不可约的，因为可以通过一系列步长为 $\\pm 2$ 的步骤从任何状态到达任何其他状态。\n\n**4. 非周期性分析**\n\n状态 $x$ 的周期为 $d(x) = \\gcd\\{n \\geq 1 : P^n(x, x)  0\\}$。因为提议分布允许停留在原地（$q(x \\mid x) = 1-2p  0$），所以对于所有 $x \\in \\mathbb{Z}$，都有 $P(x,x)  0$。这意味着对于所有 $x$，$d(x) = \\gcd(\\{1, \\dots\\}) = 1$。因此，该链是**非周期的**。\n\n**5. 不变分布与收敛性分析**\n\n根据构造，MH 算法满足关于 $\\pi$ 的细致平衡条件，这意味着 $\\pi$ 是该链的一个平稳分布。然而，由于链是可约的，平稳分布不是唯一的。所有平稳分布的集合是不可约分量的唯一平稳分布的凸包。\n\n*   令 $\\pi_{\\mathrm{even}}$ 为 $\\pi$ 限制在 $\\mathcal{S}_E$ 上的归一化分布。\n*   令 $\\pi_{\\mathrm{odd}}$ 为 $\\pi$ 限制在 $\\mathcal{S}_O$ 上的归一化分布。\n\n任何形式为 $\\mu_\\alpha = \\alpha \\, \\pi_{\\mathrm{even}} + (1-\\alpha) \\, \\pi_{\\mathrm{odd}}$（其中 $\\alpha \\in [0, 1]$）的分布都是整个 $\\mathbb{Z}$ 上链的一个有效的平稳分布。\n\n**选项评估：**\n*   A: 错误。链是可约的。\n*   B: 正确。链是非周期的（因为 $P(x,x) > 0$）且可约，具有偶数集和奇数集两个闭互通类。\n*   C: 正确。存在由 $\\alpha$ 参数化的无限多个平稳分布。如果从一个偶数状态初始化，链被限制在偶数集内，并且由于在该子空间上是遍历的，它将收敛到该子空间上的唯一平稳分布 $\\pi_{\\mathrm{even}}$。\n*   D: 错误。细致平衡不保证全局收敛。不可约性是必需的。\n*   E: 错误。强大数定律需要遍历性（在整个空间上）。由于链是可约的，经验平均的极限取决于初始状态所在的互通类。\n\n因此，选项 B 和 C 都是对该链的正确描述。",
            "answer": "$$\\boxed{BC}$$"
        },
        {
            "introduction": "一个概率分布序列的“收敛”究竟意味着什么？这最后一个练习  通过考察一个简单的迭代函数系统，深入探讨了这个微妙但关键的问题。你将分析一个链，其分布在Wasserstein距离（$W_1$）下收敛到平稳分布，但在全变差距离（TV）下却不收敛。这种鲜明的对比揭示了不同收敛模式对于估计期望值和近似集合概率的实际意义。",
            "id": "3300048",
            "problem": "考虑紧度量空间 $([0,1], d)$ 上的时间齐次马尔可夫链 $(X_n)_{n \\ge 0}$，其中 $d(x,y) = |x - y|$，该链由随机迭代函数系统定义\n$$\nX_{n+1} \\in \\left\\{ \\tfrac{1}{3} X_n,\\ \\tfrac{2 + X_n}{3} \\right\\} \\quad \\text{以相等的概率 } \\tfrac{1}{2}, \\text{ 且随时间独立。}\n$$\n令 $P$ 表示其马尔可夫转移核，令 $\\pi$ 表示一个平稳分布（如果存在）。令 $\\|\\cdot\\|_{\\mathrm{TV}}$ 表示全变差距离， $W_1$ 表示在 $([0,1], d)$ 上的概率测度上的 1-Wasserstein 距离。\n\n选择所有正确的陈述。\n\nA. 马尔可夫算子在 $W_1$ 中是模严格小于 $1$ 的压缩映射，因此存在唯一的平稳分布 $\\pi$，并且对于任何初始分布 $\\mu$，有\n$$\nW_1(\\mu P^n, \\pi) \\le \\left(\\tfrac{1}{3}\\right)^n W_1(\\mu, \\pi) \\quad \\text{对于所有 } n \\ge 0.\n$$\n\nB. 对于任意 $x \\in [0,1]$，法则序列 $P^n(x, \\cdot)$ 不在全变差意义下收敛到 $\\pi$；事实上，\n$$\n\\|P^n(x, \\cdot) - \\pi\\|_{\\mathrm{TV}} = 1 \\quad \\text{对于所有 } n \\ge 1.\n$$\n\nC. 对于任何有界 Lipschitz 函数 $f \\colon [0,1] \\to \\mathbb{R}$，其 Lipschitz 常数为 $\\mathrm{Lip}(f)$，偏差以 $W_1$ 速率衰减：\n$$\n\\big|\\mathbb{E}_x[f(X_n)] - \\pi(f)\\big| \\le \\mathrm{Lip}(f)\\, W_1\\!\\big(P^n(x, \\cdot), \\pi\\big) \\quad \\text{对于所有 } n \\ge 0,\n$$\n因此，特别地，当 $n \\to \\infty$ 时，$\\mathbb{E}_x[f(X_n)] \\to \\pi(f)$。\n\nD. 对于任何有界可测函数 $f$，\n$$\n\\big|\\mathbb{E}_x[f(X_n)] - \\pi(f)\\big| \\le \\|f\\|_\\infty\\, W_1\\!\\big(P^n(x, \\cdot), \\pi\\big) \\quad \\text{对于所有 } n \\ge 0,\n$$\n因此 $W_1$ 收敛意味着对所有有界可测函数 $f$ 的偏差有一致的控制。\n\nE. 如果模拟该链并形成时间平均 $n^{-1}\\sum_{k=1}^n f(X_k)$，那么对于任何满足 $\\pi(\\partial A) = 0$ 的指示函数 $f = \\mathbf{1}_A$，对于任何起始点 $X_0 = x \\in [0,1]$，都有几乎必然收敛\n$$\n\\frac{1}{n}\\sum_{k=1}^n \\mathbf{1}_A(X_k) \\longrightarrow \\pi(A) \\quad \\text{当 } n \\to \\infty,\n$$\n\n选择所有适用的选项。",
            "solution": "首先，我们分析马尔可夫链的结构。定义迭代函数系统（IFS）的两个函数是 $f_1(x) = \\frac{1}{3}x$ 和 $f_2(x) = \\frac{2+x}{3}$。两者都是在 $[0,1]$ 上的压缩映射，具有相同的 Lipschitz 常数 $1/3$。根据 IFS 理论，存在一个唯一的平稳分布 $\\pi$，其支撑集是标准的三分康托尔集。此分布 $\\pi$ 是连续的（即非原子的），但关于勒贝格测度是奇异的。\n\n**A. 马尔可夫算子在 $W_1$ 中是模严格小于 $1$ 的压缩映射...**\n\n令 $\\mu_1$ 和 $\\mu_2$ 为 $[0,1]$ 上的两个概率测度。我们可以通过同步耦合（即对两个从不同点开始的粒子应用相同的随机选择函数）来分析 $W_1$ 距离的演化。在一步之后，两个粒子之间的期望距离为：\n$$ \\mathbb{E}[|X_1 - Y_1|] = \\mathbb{E}\\left[\\frac{1}{2}|f_1(X_0)-f_1(Y_0)| + \\frac{1}{2}|f_2(X_0)-f_2(Y_0)|\\right] \\le \\frac{1}{3}\\mathbb{E}[|X_0-Y_0|] $$\n根据 Kantorovich-Rubinstein 对偶性，这意味着 $W_1(\\mu_1 P, \\mu_2 P) \\le \\frac{1}{3} W_1(\\mu_1, \\mu_2)$。因此，马尔可夫算子 $P$ 是一个在（具有 $W_1$ 度量的）概率测度空间上的压缩映射，其模为 $1/3$。根据巴拿赫不动点定理，存在唯一的不动点 $\\pi$。通过归纳法，我们得到 $W_1(\\mu P^n, \\pi) \\le (\\frac{1}{3})^n W_1(\\mu, \\pi)$。因此，该陈述正确。\n\n**B. ...法则序列 $P^n(x, \\cdot)$ 不在全变差意义下收敛到 $\\pi$...**\n\n从初始状态 $x$ 开始，经过 $n$ 步后，链的状态 $X_n$ 只能取 $2^n$ 个可能的值。因此，$X_n$ 的法则（或分布）$P^n(x, \\cdot)$ 是一个支撑在有限集 $S_n$ 上的离散测度。另一方面，平稳分布 $\\pi$ 是康托尔分布，这是一个连续的（非原子的）测度，即对任何单点集 $\\{y\\}$，都有 $\\pi(\\{y\\})=0$。\n两个测度 $\\mu$ 和 $\\nu$ 之间的全变差距离为 $\\|\\mu - \\nu\\|_{\\mathrm{TV}} = \\sup_{A} |\\mu(A) - \\nu(A)|$。选择集合 $A = S_n$，我们有 $P^n(x, \\cdot)(S_n) = 1$，而 $\\pi(S_n) = 0$。因此，\n$$ \\|P^n(x, \\cdot) - \\pi\\|_{\\mathrm{TV}} \\ge |P^n(x, \\cdot)(S_n) - \\pi(S_n)| = |1 - 0| = 1 $$\n由于全变差距离最大为 $1$，所以它恰好为 $1$。这对所有 $n \\ge 1$ 都成立，所以不存在全变差意义下的收敛。该陈述正确。\n\n**C. 对于任何有界 Lipschitz 函数 $f$...**\n\n该陈述的第一部分是 Kantorovich-Rubinstein 对偶定理的直接应用：\n$$ |\\mathbb{E}_\\mu[f] - \\mathbb{E}_\\nu[f]| \\le \\mathrm{Lip}(f) W_1(\\mu, \\nu) $$\n将 $\\mu = P^n(x, \\cdot)$ 和 $\\nu = \\pi$ 代入即可得到该不等式。对于第二部分，我们已经从 A 中得知 $W_1(P^n(x, \\cdot), \\pi) \\to 0$（以几何速率）。由于 $f$ 是有界的 Lipschitz 函数，$\\mathrm{Lip}(f)$ 是有限的，因此当 $n \\to \\infty$ 时，偏差 $|\\mathbb{E}_x[f(X_n)] - \\pi(f)|$ 收敛到 $0$。该陈述正确。\n\n**D. 对于任何有界可测函数 $f$...**\n\n该陈述不正确。$W_1$ 收敛（弱收敛加上一阶矩收敛）并不保证对所有有界可测函数的期望都收敛。这种更强的收敛模式由全变差收敛所蕴含。正如 B 中所示，$\\|P^n(x, \\cdot) - \\pi\\|_{\\mathrm{TV}} = 1$。这意味着我们可以找到一个有界可测函数（例如，一个集合的指示函数），其期望不会收敛。例如，令 $f = \\mathbf{1}_{S_n}$（$P^n(x, \\cdot)$ 的支撑集），那么 $|\\mathbb{E}_x[f(X_n)] - \\pi(f)| = |1 - 0| = 1$，它不趋于零。所给的不等式是错误的。\n\n**E. 如果模拟该链并形成时间平均...**\n\n该陈述是关于此马尔可夫链的强大数定律（遍历定理）。由于该链存在唯一的平稳分布 $\\pi$，并且根据 A 的分析，该链具有很强的遍历性（从任意两个点开始的耦合路径的距离期望会收敛到零），因此遍历定理对任何初始状态 $x \\in [0,1]$ 和任何 $f \\in L^1(\\pi)$ 的函数都成立。函数 $f = \\mathbf{1}_A$ 是有界的，因此属于 $L^1(\\pi)$。所以，几乎必然有 $\\frac{1}{n}\\sum_{k=1}^n f(X_k) \\to \\pi(A)$。该陈述正确。",
            "answer": "$$\\boxed{ABCE}$$"
        }
    ]
}