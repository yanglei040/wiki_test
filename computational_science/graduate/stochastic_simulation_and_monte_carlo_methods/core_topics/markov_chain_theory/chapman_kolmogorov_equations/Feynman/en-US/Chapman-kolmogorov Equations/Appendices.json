{
    "hands_on_practices": [
        {
            "introduction": "The Chapman-Kolmogorov equations are most readily understood in the context of discrete-time Markov chains. This exercise provides a concrete scenario to practice the fundamental calculation of multi-step transition probabilities. By determining the likelihood of a student's attention state after two time intervals, you will apply the core logic of summing over all possible intermediate pathways to see the equation in action .",
            "id": "1347960",
            "problem": "A cognitive scientist models a student's attention level during a lecture using a discrete-time stochastic process. The student can be in one of three states at any given moment: State 1 (Focused), State 2 (Distracted), or State 3 (Daydreaming). The transitions between these states are observed over fixed 5-minute intervals. The one-step transition probabilities, $P_{ij}$, which denote the probability of moving from state $i$ to state $j$ in one 5-minute interval, are given as follows:\n\n-   A student who is Focused (State 1) has a 0.6 probability of remaining Focused, a 0.3 probability of becoming Distracted, and a 0.1 probability of starting to Daydream.\n-   A student who is Distracted (State 2) has a 0.4 probability of becoming Focused, a 0.4 probability of remaining Distracted, and a 0.2 probability of starting to Daydream.\n-   A student who is Daydreaming (State 3) has a 0.1 probability of becoming Focused, a 0.5 probability of becoming Distracted, and a 0.4 probability of remaining in a Daydreaming state.\n\nSuppose a student is Focused at the beginning of an observation (time $t=0$). What is the probability that this student will be in the Daydreaming state after two 5-minute intervals (i.e., at time $t=10$ minutes)?\n\nRound your final answer to three significant figures.",
            "solution": "We model the student's attention as a time-homogeneous discrete-time Markov chain with state space $\\{1,2,3\\}$ corresponding to Focused, Distracted, and Daydreaming. The one-step transition matrix $P=\\left(P_{ij}\\right)$, with rows indexed by the current state and columns by the next state, is\n$$\nP=\\begin{pmatrix}\n0.6  0.3  0.1\\\\\n0.4  0.4  0.2\\\\\n0.1  0.5  0.4\n\\end{pmatrix}.\n$$\nBy the Chapman–Kolmogorov equations, the two-step transition probabilities are given by $P^{(2)}=P^{2}$, and specifically\n$$\nP^{(2)}_{13}=\\sum_{k=1}^{3}P_{1k}P_{k3}.\n$$\nSince the student starts in state $1$ at $t=0$, the probability of being in state $3$ after two steps (at $t=10$ minutes) is $P^{(2)}_{13}$. Computing term by term,\n$$\nP^{(2)}_{13}=P_{11}P_{13}+P_{12}P_{23}+P_{13}P_{33}\n=0.6\\cdot 0.1+0.3\\cdot 0.2+0.1\\cdot 0.4\n=0.06+0.06+0.04\n=0.16.\n$$\nRounding to three significant figures gives $0.160$.",
            "answer": "$$\\boxed{0.160}$$"
        },
        {
            "introduction": "Beyond simple calculation, the Chapman-Kolmogorov equation serves as a powerful consistency condition that dictates the functional form of valid stochastic processes. This problem challenges you to use the equation as a theoretical tool to derive a fundamental property of diffusion processes . By requiring a Gaussian transition kernel to satisfy the equation, you will uncover the necessary relationship between time and variance, a cornerstone result that underpins the physics of Brownian motion.",
            "id": "731491",
            "problem": "A one-dimensional, time-homogeneous, centered (driftless) Markovian stochastic process is characterized by its transition probability density $P(x_f, t_f | x_i, t_i) = P(x_f | x_i; \\tau)$, where $\\tau = t_f - t_i  0$. This density must satisfy the Chapman-Kolmogorov equation for any intermediate time $t_m$ such that $t_i  t_m  t_f$:\n$$ P(x_f | x_i; t_f - t_i) = \\int_{-\\infty}^{\\infty} P(x_f | x_m; t_f - t_m) P(x_m | x_i; t_m - t_i) \\, dx_m $$\nConsider a family of such processes where the transition kernel is Gaussian:\n$$ P(x_f | x_i; \\tau) = \\frac{1}{\\sqrt{2\\pi V(\\tau)}} \\exp\\left(-\\frac{(x_f - x_i)^2}{2V(\\tau)}\\right) $$\nHere, $V(\\tau)$ is the variance of the displacement over a time interval $\\tau$. It is an unknown positive, continuous function for $\\tau  0$, and we know that $V(\\tau) \\to 0$ as $\\tau \\to 0$. By requiring that this Gaussian form consistently satisfies the Chapman-Kolmogorov equation for all time intervals, derive the general functional form of $V(t)$ for $t  0$. Express your answer in terms of an arbitrary positive constant $D$.",
            "solution": "1. Chapman–Kolmogorov equation for a driftless Markov process:\n$$P(x_f|x_i;\\tau_1+\\tau_2)\n=\\int_{-\\infty}^\\infty P(x_f|x_m;\\tau_2)\\,P(x_m|x_i;\\tau_1)\\,dx_m.$$\n2. Plug in the Gaussian kernels \n$$P(\\Delta x;\\tau)\n=\\frac{1}{\\sqrt{2\\pi V(\\tau)}}\\exp\\!\\Big(-\\frac{(\\Delta x)^2}{2V(\\tau)}\\Big).$$\nThe convolution of two Gaussians yields a Gaussian with variance\n$$V(\\tau_1+\\tau_2)=V(\\tau_1)+V(\\tau_2).$$\n3. The functional equation $V(t+s)=V(t)+V(s)$ with continuity and $V(0)=0$ implies \n$$V(t)=a\\,t,\\quad a0.$$\nRenaming $a=D$ gives\n$$V(t)=D\\,t.$$",
            "answer": "$$\\boxed{Dt}$$"
        },
        {
            "introduction": "The Chapman-Kolmogorov equation is not merely a theoretical identity; it is a constructive principle for designing powerful simulation algorithms. This advanced exercise demonstrates how the equation's decomposition of time provides a blueprint for \"ancestral sampling\" in a continuous-time process, the Ornstein-Uhlenbeck model . You will bridge theory and practice by deriving the process dynamics, designing a conditional Monte Carlo scheme, and analyzing the statistical properties of the resulting estimator.",
            "id": "3293504",
            "problem": "Consider the one-dimensional Ornstein–Uhlenbeck (OU) diffusion defined by the stochastic differential equation (SDE) $dX_{u}=-\\theta\\,(X_{u}-\\mu)\\,du+\\sigma\\,dW_{u}$ for $u\\geq 0$, where $X_{0}=x_{0}$, $\\theta0$, $\\sigma0$, $\\mu\\in\\mathbb{R}$, and $W_{u}$ is a standard Brownian motion (BM). Let $0st$ be two fixed times. Denote by $p(x,t\\,|\\,y,s)$ the transition density of the OU process from time $s$ to time $t$, and by $p(y,s\\,|\\,x_{0},0)$ the transition density from time $0$ to time $s$. Define the target integral at a fixed spatial location $x\\in\\mathbb{R}$ by $I(x)=\\int_{\\mathbb{R}}p(x,t\\,|\\,y,s)\\,p(y,s\\,|\\,x_{0},0)\\,dy$.\n\nYour tasks are:\n\n1) Starting from the SDE and the Markov property, derive the explicit Gaussian forms of the transition densities $p(y,s\\,|\\,x_{0},0)$ and $p(x,t\\,|\\,y,s)$.\n\n2) Design a conditional sampling scheme that draws $Y_{s}\\sim p(y,s\\,|\\,x_{0},0)$ and then $X_{t}\\sim p(x,t\\,|\\,Y_{s},s)$ to approximate $I(x)$ by Monte Carlo (MC). Explain why the resulting marginal law of $X_{t}$ equals the law at time $t$ given $X_{0}=x_{0}$.\n\n3) Using the independent and identically distributed sample $\\{X_{t}^{(i)}\\}_{i=1}^{N}$ constructed in part $2$, define the Gaussian-kernel density estimator $\\widehat{p}_{N}(x)=\\frac{1}{N h_{N}}\\sum_{i=1}^{N}K\\!\\left(\\frac{x-X_{t}^{(i)}}{h_{N}}\\right)$ of $I(x)$, where $K(u)=\\frac{1}{\\sqrt{2\\pi}}\\exp(-u^{2}/2)$ and the bandwidth is $h_{N}=c\\,N^{-1/5}$ with $c0$ fixed. Starting from first principles of kernel smoothing and Taylor expansion, derive the leading-order asymptotic bias $\\,\\mathbb{E}[\\widehat{p}_{N}(x)]-I(x)\\,$ as $N\\to\\infty$, expressed in closed form in terms of $x$, $x_{0}$, $\\mu$, $\\theta$, $\\sigma$, $t$, $c$, and $N$. Your final expression must not depend on $s$.\n\nProvide your final answer as a single closed-form analytic expression. Do not include any inequality or equation in the final answer. No rounding is required.",
            "solution": "The problem is valid and can be solved by applying principles from stochastic calculus, Monte Carlo methods, and non-parametric statistics. We address each of the three tasks in sequence.\n\n### Part 1: Derivation of Transition Densities\n\nThe Ornstein–Uhlenbeck (OU) process is described by the stochastic differential equation (SDE):\n$$dX_{u} = -\\theta(X_{u} - \\mu)du + \\sigma dW_{u}, \\quad X_0=x_0$$\nwhere $\\theta  0$, $\\sigma  0$, $\\mu \\in \\mathbb{R}$, and $W_u$ is a standard Brownian motion. This is a linear SDE. To solve it, we introduce the transformation $Y_{u} = X_{u} - \\mu$. The SDE for $Y_u$ becomes:\n$$dY_{u} = dX_{u} = -\\theta Y_{u} du + \\sigma dW_{u}$$\nWe solve this SDE using an integrating factor, $e^{\\theta u}$. Applying Itô's product rule to $e^{\\theta u}Y_u$:\n$$d(e^{\\theta u}Y_{u}) = (\\theta e^{\\theta u} du) Y_{u} + e^{\\theta u} dY_{u} = \\theta e^{\\theta u} Y_{u} du + e^{\\theta u}(-\\theta Y_{u} du + \\sigma dW_{u}) = \\sigma e^{\\theta u} dW_{u}$$\nIntegrating from a general time $s$ to $t$ (with $st$):\n$$\\int_{s}^{t} d(e^{\\theta u}Y_{u}) = \\int_{s}^{t} \\sigma e^{\\theta u} dW_{u}$$\n$$e^{\\theta t}Y_{t} - e^{\\theta s}Y_{s} = \\sigma \\int_{s}^{t} e^{\\theta u} dW_{u}$$\nSolving for $Y_t$:\n$$Y_{t} = Y_{s}e^{-\\theta(t-s)} + \\sigma \\int_{s}^{t} e^{-\\theta(t-u)} dW_{u}$$\nSubstituting back $X_u = Y_u + \\mu$, we find the solution for $X_t$ given $X_s$:\n$$X_{t} - \\mu = (X_{s} - \\mu)e^{-\\theta(t-s)} + \\sigma \\int_{s}^{t} e^{-\\theta(t-u)} dW_{u}$$\n$$X_{t} = \\mu + (X_{s} - \\mu)e^{-\\theta(t-s)} + \\sigma \\int_{s}^{t} e^{-\\theta(t-u)} dW_{u}$$\nGiven $X_{s} = y$, $X_t$ is a Gaussian random variable because it is a constant plus an Itô integral with a deterministic integrand. We find its conditional mean and variance. The expectation of the Itô integral is zero:\n$$\\mathbb{E}[X_t | X_s=y] = \\mu + (y - \\mu)e^{-\\theta(t-s)} + \\sigma \\mathbb{E}\\left[\\int_{s}^{t} e^{-\\theta(t-u)} dW_{u}\\right] = \\mu + (y - \\mu)e^{-\\theta(t-s)}$$\nThe variance is calculated using Itô isometry:\n$$\\text{Var}(X_t | X_s=y) = \\text{Var}\\left(\\sigma \\int_{s}^{t} e^{-\\theta(t-u)} dW_{u}\\right) = \\sigma^2 \\int_{s}^{t} (e^{-\\theta(t-u)})^2 du$$\n$$\\text{Var}(X_t | X_s=y) = \\sigma^2 \\int_{s}^{t} e^{-2\\theta(t-u)} du = \\sigma^2 \\left[ \\frac{e^{-2\\theta(t-u)}}{2\\theta} \\right]_{u=s}^{u=t} = \\frac{\\sigma^2}{2\\theta} (1 - e^{-2\\theta(t-s)})$$\nThus, the conditional distribution of $X_t$ given $X_s=y$ is Gaussian:\n$$X_t | X_s=y \\sim \\mathcal{N}\\left(\\mu + (y-\\mu)e^{-\\theta(t-s)}, \\frac{\\sigma^2}{2\\theta}(1-e^{-2\\theta(t-s)})\\right)$$\nThe transition density $p(x,t|y,s)$ is the probability density function (PDF) of this distribution:\n$$p(x,t|y,s) = \\frac{1}{\\sqrt{2\\pi V_{t-s}}} \\exp\\left(-\\frac{(x - M_{t|s}(y))^2}{2V_{t-s}}\\right)$$\nwhere $M_{t|s}(y) = \\mu + (y-\\mu)e^{-\\theta(t-s)}$ and $V_{t-s} = \\frac{\\sigma^2}{2\\theta}(1-e^{-2\\theta(t-s)})$.\n\nTo find $p(y,s|x_0,0)$, we substitute $(t,s,x,y)$ with $(s,0,y,x_0)$:\n$$M_{s|0}(x_0) = \\mu + (x_0-\\mu)e^{-\\theta s}$$\n$$V_s = \\frac{\\sigma^2}{2\\theta}(1-e^{-2\\theta s})$$\nSo, the transition density from time $0$ to $s$ is:\n$$p(y,s|x_0,0) = \\frac{1}{\\sqrt{2\\pi V_s}} \\exp\\left(-\\frac{(y - M_{s|0}(x_0))^2}{2V_s}\\right)$$\n\n### Part 2: Conditional Sampling Scheme\n\nThe target integral is $I(x) = \\int_{\\mathbb{R}} p(x,t|y,s) p(y,s|x_0,0) dy$.\nThe OU process $X_u$ is a Markov process. The law of the process at time $t$, given its state at time $0$, can be found by integrating over all possible intermediate states at time $s$ ($0st$). This is expressed by the Chapman-Kolmogorov equation:\n$$p(x,t|x_0,0) = \\int_{\\mathbb{R}} p(x,t|y,s) p(y,s|x_0,0) dy$$\nComparing this with the definition of the target integral, we see that $I(x) = p(x,t|x_{0},0)$. This is the density of the process $X_t$ given the initial condition $X_{0}=x_{0}$.\n\nThe proposed conditional sampling scheme is as follows for each sample $i=1, \\dots, N$:\n1. Draw a sample $Y_s^{(i)}$ from the distribution with density $p(y,s|x_0,0)$. Based on Part $1$, this means sampling from $\\mathcal{N}(M_{s|0}(x_0), V_s)$.\n2. Conditional on the value $Y_s^{(i)}$, draw a sample $X_t^{(i)}$ from the distribution with density $p(x,t|Y_s^{(i)},s)$. This means sampling from $\\mathcal{N}(M_{t|s}(Y_s^{(i)}), V_{t-s})$.\n\nThis procedure is known as ancestral sampling. The distribution of the resulting samples $\\{X_{t}^{(i)}\\}_{i=1}^N$ is the marginal distribution of $X_t$. Its density is obtained by integrating the joint density $p(x,t,y,s|x_0,0)$ over the intermediate variable $y$:\n$$f_{X_t}(x) = \\int_{\\mathbb{R}} p(x,t,y,s|x_0,0) dy$$\nBy the definition of conditional probability, $p(x,t,y,s|x_0,0) = p(x,t|y,s,x_0,0) p(y,s|x_0,0)$. Due to the Markov property, $p(x,t|y,s,x_0,0) = p(x,t|y,s)$. Therefore:\n$$f_{X_t}(x) = \\int_{\\mathbb{R}} p(x,t|y,s) p(y,s|x_0,0) dy = I(x)$$\nThis confirms that the samples $\\{X_t^{(i)}\\}$ are independent and identically distributed draws from the target distribution with density $I(x)$, which is the law of the OU process at time $t$ given $X_0=x_0$.\n\n### Part 3: Asymptotic Bias of the KDE\n\nThe Gaussian-kernel density estimator for $I(x) = p(x,t|x_0,0)$ is\n$$\\widehat{p}_{N}(x)=\\frac{1}{N h_{N}}\\sum_{i=1}^{N}K\\!\\left(\\frac{x-X_{t}^{(i)}}{h_{N}}\\right)$$\nwhere $\\{X_{t}^{(i)}\\}$ are i.i.d. samples from the density $I(x)$, $K(u) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-u^2/2)$, and $h_N = cN^{-1/5}$.\nThe expectation of the estimator is:\n$$\\mathbb{E}[\\widehat{p}_{N}(x)] = \\mathbb{E}\\left[\\frac{1}{h_{N}}K\\left(\\frac{x-X_{t}}{h_{N}}\\right)\\right] = \\frac{1}{h_{N}} \\int_{\\mathbb{R}} K\\left(\\frac{x-z}{h_{N}}\\right)I(z)dz$$\nLet $u = (x-z)/h_N$, so $z = x - uh_N$ and $dz = -h_N du$. The integral becomes:\n$$\\mathbb{E}[\\widehat{p}_{N}(x)] = \\frac{1}{h_N} \\int_{\\mathbb{R}} K(u) I(x-uh_N) (h_N du) = \\int_{\\mathbb{R}} K(u)I(x-uh_N)du$$\nFor large $N$, $h_N \\to 0$. We perform a Taylor expansion of $I(x-uh_N)$ around $x$:\n$$I(x-uh_N) = I(x) - uh_N I'(x) + \\frac{1}{2}(uh_N)^2 I''(x) + \\mathcal{O}(h_N^3)$$\nSubstituting into the integral:\n$$\\mathbb{E}[\\widehat{p}_{N}(x)] = \\int_{\\mathbb{R}} K(u)\\left[I(x) - uh_N I'(x) + \\frac{h_N^2 u^2}{2}I''(x) + \\mathcal{O}(h_N^3)\\right]du$$\n$$\\mathbb{E}[\\widehat{p}_{N}(x)] = I(x)\\int K(u)du - h_N I'(x)\\int uK(u)du + \\frac{h_N^2}{2}I''(x)\\int u^2K(u)du + \\mathcal{O}(h_N^3)$$\nThe kernel $K(u)$ is the standard normal PDF, so its moments are $\\int K(u)du=1$, $\\int uK(u)du=0$, and $\\int u^2K(u)du=1$.\n$$\\mathbb{E}[\\widehat{p}_{N}(x)] = I(x) + \\frac{h_N^2}{2}I''(x) + \\mathcal{O}(h_N^3)$$\nThe asymptotic bias is $\\mathbb{E}[\\widehat{p}_{N}(x)]-I(x)$, and its leading-order term is $\\frac{1}{2}h_N^2 I''(x) = \\frac{1}{2}c^2 N^{-2/5} I''(x)$.\n\nWe need to compute $I''(x)$. From Part $2$, $I(x)=p(x,t|x_0,0)$, which is the PDF of $\\mathcal{N}(M_t, V_t)$ where $M_t = \\mu + (x_0-\\mu)e^{-\\theta t}$ and $V_t = \\frac{\\sigma^2}{2\\theta}(1-e^{-2\\theta t})$.\n$$I(x) = \\frac{1}{\\sqrt{2\\pi V_t}} \\exp\\left(-\\frac{(x-M_t)^2}{2V_t}\\right)$$\nThe first derivative with respect to $x$ is:\n$$I'(x) = I(x) \\cdot \\frac{d}{dx}\\left(-\\frac{(x-M_t)^2}{2V_t}\\right) = -\\frac{x-M_t}{V_t}I(x)$$\nThe second derivative is obtained using the product rule:\n$$I''(x) = \\frac{d}{dx}\\left(-\\frac{x-M_t}{V_t}I(x)\\right) = -\\frac{1}{V_t}I(x) - \\frac{x-M_t}{V_t}I'(x)$$\nSubstituting $I'(x)$:\n$$I''(x) = -\\frac{1}{V_t}I(x) - \\frac{x-M_t}{V_t}\\left(-\\frac{x-M_t}{V_t}I(x)\\right) = \\left(\\frac{(x-M_t)^2}{V_t^2} - \\frac{1}{V_t}\\right) I(x)$$\nThus, the leading-order asymptotic bias is:\n$$\\text{Bias} \\approx \\frac{1}{2}c^2 N^{-2/5} \\left(\\frac{(x-M_t)^2}{V_t^2} - \\frac{1}{V_t}\\right) \\frac{1}{\\sqrt{2\\pi V_t}} \\exp\\left(-\\frac{(x-M_t)^2}{2V_t}\\right)$$\nSubstituting the expressions for $M_t$ and $V_t$ gives the final closed-form expression, which is independent of $s$ as required. The expression can be written as:\n$$\\text{Bias} \\approx \\frac{c^2 N^{-2/5}}{2} \\frac{(x-M_t)^2 - V_t}{V_t^2 \\sqrt{2\\pi V_t}} \\exp\\left(-\\frac{(x-M_t)^2}{2V_t}\\right)$$\nSubstituting the full expressions for $M_t$ and $V_t$ yields the final result.",
            "answer": "$$ \\boxed{ \\frac{c^2 N^{-2/5}}{2} \\frac{(x - (\\mu + (x_0-\\mu)\\exp(-\\theta t)))^2 - \\frac{\\sigma^2}{2\\theta}(1-\\exp(-2\\theta t))}{\\left(\\frac{\\sigma^2}{2\\theta}(1-\\exp(-2\\theta t))\\right)^2 \\sqrt{2\\pi \\frac{\\sigma^2}{2\\theta}(1-\\exp(-2\\theta t))}} \\exp\\left(-\\frac{(x-(\\mu + (x_0-\\mu)\\exp(-\\theta t)))^2}{2 \\frac{\\sigma^2}{2\\theta}(1-\\exp(-2\\theta t))}\\right) } $$"
        }
    ]
}