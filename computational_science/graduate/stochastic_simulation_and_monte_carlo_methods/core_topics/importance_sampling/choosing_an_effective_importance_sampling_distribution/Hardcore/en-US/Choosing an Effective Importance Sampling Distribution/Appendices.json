{
    "hands_on_practices": [
        {
            "introduction": "In many real-world applications, the target distribution is only known up to a normalizing constant, necessitating the use of self-normalized importance sampling (SNIS). The first step toward designing an effective proposal is establishing a clear, computable objective. This practice guides you through the derivation of a key criterion for minimizing estimator variance, which is fundamentally linked to minimizing the variability of the normalized importance weights.",
            "id": "3295474",
            "problem": "Consider self-normalized importance sampling for estimating the expectation $I = \\mathbb{E}_{p}[f(X)]$, where $p(x) = \\tilde{p}(x)/Z$ is a target probability density function known only up to a positive normalization constant $Z = \\int \\tilde{p}(x) \\, dx$. Let $\\{X_{i}\\}_{i=1}^{n}$ be independent and identically distributed draws from a proposal family $\\{q_{\\theta}\\}_{\\theta \\in \\Theta}$, with $q_{\\theta}$ supported wherever $\\tilde{p}$ is positive. Define the unnormalized importance weights $w_{i} = \\tilde{p}(X_{i}) / q_{\\theta}(X_{i})$, and the normalized weights\n$$\n\\tilde{w}_{i} = \\frac{w_{i}}{\\sum_{j=1}^{n} w_{j}} \\, .\n$$\nThe self-normalized importance sampling estimator is\n$$\n\\hat{I}_{\\text{SNIS}} = \\sum_{i=1}^{n} \\tilde{w}_{i} \\, f(X_{i}) \\, .\n$$\nAssume $\\mathbb{E}_{p}[f(X)^{2}]  \\infty$. Using only foundational definitions of self-normalized importance sampling, the ratio-of-means structure of $\\hat{I}_{\\text{SNIS}}$, and the definition of the coefficient of variation (CV), derive a principled criterion for choosing $q_{\\theta}$ to minimize the asymptotic relative mean squared error, defined as\n$$\n\\text{rMSE}_{\\infty}(\\theta) = \\lim_{n \\to \\infty} \\frac{\\mathbb{E}\\!\\left[ \\left( \\hat{I}_{\\text{SNIS}} - I \\right)^{2} \\right]}{I^{2}} \\, .\n$$\nIn particular, argue directly by controlling the coefficient of variation of the normalized importance weights that, in the large-sample limit, minimizing $\\text{rMSE}_{\\infty}(\\theta)$ over $\\theta$ reduces to minimizing a single functional of $q_{\\theta}$ that depends only on $\\tilde{p}$ and $q_{\\theta}$ but not on $Z$ or on $f$. Provide that functional explicitly in closed form as your final answer. No numerical evaluation is required.",
            "solution": "The problem asks for a principled criterion to select a proposal distribution $q_{\\theta}$ from a family $\\{q_{\\theta}\\}_{\\theta \\in \\Theta}$ for self-normalized importance sampling (SNIS). The criterion should aim to minimize the asymptotic relative mean squared error of the SNIS estimator, $\\hat{I}_{\\text{SNIS}}$, for the true expectation $I = \\mathbb{E}_{p}[f(X)]$. The desired criterion must be a functional of $q_{\\theta}$ that is independent of the function $f$ and the unknown normalization constant $Z$ of the target density $p(x) = \\tilde{p}(x)/Z$. The derivation is to be based on the structure of the estimator and the coefficient of variation of the normalized importance weights.\n\nFirst, we validate the problem.\n\n### Step 1: Extract Givens\n- Target expectation: $I = \\mathbb{E}_{p}[f(X)]$.\n- Target density: $p(x) = \\tilde{p}(x)/Z$, with $Z = \\int \\tilde{p}(x) \\, dx$ unknown.\n- Samples: $\\{X_{i}\\}_{i=1}^{n}$ are independent and identically distributed (i.i.d.) draws from a proposal density $q_{\\theta}(x)$.\n- Support condition: $q_{\\theta}(x)  0$ for all $x$ where $\\tilde{p}(x)  0$.\n- Unnormalized importance weights: $w_{i} = \\tilde{p}(X_{i}) / q_{\\theta}(X_{i})$.\n- Normalized importance weights: $\\tilde{w}_{i} = \\frac{w_{i}}{\\sum_{j=1}^{n} w_{j}}$.\n- SNIS estimator: $\\hat{I}_{\\text{SNIS}} = \\sum_{i=1}^{n} \\tilde{w}_{i} \\, f(X_{i})$.\n- Assumption: $\\mathbb{E}_{p}[f(X)^{2}]  \\infty$.\n- Objective: Minimize the asymptotic relative mean squared error, $\\text{rMSE}_{\\infty}(\\theta) = \\lim_{n \\to \\infty} \\frac{\\mathbb{E}\\!\\left[ \\left( \\hat{I}_{\\text{SNIS}} - I \\right)^{2} \\right]}{I^{2}}$.\n- Derivation constraint: Argue by controlling the coefficient of variation (CV) of the normalized importance weights in the large-sample limit.\n- Result constraint: The minimization criterion must be a single functional of $q_{\\theta}$ that depends only on $\\tilde{p}$ and $q_{\\theta}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, rooted in the well-established statistical theory of Monte Carlo methods, specifically importance sampling. It is well-posed, with a clear objective and sufficient information to derive the requested criterion. The language used is precise and objective. All definitions are standard in the field. The problem does not violate any of the invalidity criteria. For example, it is not scientifically unsound, as it describes a standard statistical estimation problem. It is not incomplete, as all necessary definitions for the derivation are provided. It is a non-trivial but standard theoretical exercise in computational statistics.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the derivation.\n\nThe self-normalized importance sampling estimator for $I = \\mathbb{E}_p[f(X)]$ is given by\n$$\n\\hat{I}_{\\text{SNIS}} = \\sum_{i=1}^{n} \\tilde{w}_{i} \\, f(X_{i}) = \\frac{\\sum_{i=1}^{n} w_{i} f(X_{i})}{\\sum_{j=1}^{n} w_{j}}\n$$\nwhere $w_{i} = \\tilde{p}(X_{i})/q_{\\theta}(X_{i})$ are the unnormalized weights for samples $X_i \\sim q_{\\theta}$. This estimator is a weighted average of the function values $\\{f(X_i)\\}_{i=1}^n$, with the random weights being the normalized importance weights $\\{\\tilde{w}_i\\}_{i=1}^n$.\n\nThe accuracy and precision of this estimator depend critically on the properties of these normalized weights. An ideal set of weights would be perfectly uniform, i.e., $\\tilde{w}_i = 1/n$ for all $i=1, \\dots, n$. In this case, the estimator simplifies to the standard Monte Carlo average $\\frac{1}{n} \\sum_{i=1}^n f(X_i)$. This uniformity occurs if we can sample directly from the target distribution $p(x)$, since then $q_{\\theta}(x)=p(x)$ implies $w_i = \\tilde{p}(X_i)/p(X_i) = Z$ for all $i$, and thus $\\tilde{w}_i = Z / (nZ) = 1/n$.\n\nSince we cannot sample from $p(x)$ (as $Z$ is unknown), we must use a proposal $q_{\\theta}(x)$. The weights will no longer be constant. A proposal $q_{\\theta}$ is considered effective if it produces a set of normalized weights $\\{\\tilde{w}_i\\}$ that is as close to uniform as possible. High variability in the weights implies that the estimate is dominated by a few samples with large weights, leading to high variance and an unstable estimator. This intuition suggests that a principled criterion for choosing $q_{\\theta}$, which is robust to the choice of the function $f$, is to select $\\theta$ such that the resulting normalized weights exhibit minimal variation.\n\nWe can quantify the non-uniformity of the normalized weights for a given sample $\\{X_i\\}_{i=1}^n$ by their sample coefficient of variation (CV). The set of weights $\\{\\tilde{w}_i\\}$ sums to $1$, so their sample mean is $\\bar{\\tilde{w}} = \\frac{1}{n}\\sum_{i=1}^n \\tilde{w}_i = 1/n$. The sample variance is $S^2_{\\tilde{w}} = \\frac{1}{n-1}\\sum_{i=1}^n (\\tilde{w}_i - 1/n)^2$. The sample CV is $S_{\\tilde{w}}/\\bar{\\tilde{w}} = n S_{\\tilde{w}}$. Minimizing this sample CV is equivalent to minimizing the sample variance $S^2_{\\tilde{w}}$. This, in turn, is equivalent to minimizing the sum of squared deviations $\\sum_{i=1}^n (\\tilde{w}_i - 1/n)^2$. Expanding this sum gives:\n$$\n\\sum_{i=1}^{n} \\left(\\tilde{w}_i - \\frac{1}{n}\\right)^2 = \\sum_{i=1}^{n} \\tilde{w}_i^2 - \\frac{2}{n}\\sum_{i=1}^{n} \\tilde{w}_i + \\sum_{i=1}^{n}\\frac{1}{n^2} = \\sum_{i=1}^{n} \\tilde{w}_i^2 - \\frac{2}{n}(1) + \\frac{n}{n^2} = \\sum_{i=1}^{n} \\tilde{w}_i^2 - \\frac{1}{n}\n$$\nThus, minimizing the sample CV of the normalized weights for a given sample size $n$ is equivalent to minimizing the sum of the squared normalized weights, $\\sum_{i=1}^{n} \\tilde{w}_i^2$. This quantity is inversely related to the popular diagnostic known as the effective sample size, $ESS = 1/\\sum_{i=1}^{n} \\tilde{w}_i^2$.\n\nTo obtain a criterion for choosing $\\theta$ that is independent of a particular sample, we analyze the behavior of $\\sum_{i=1}^{n} \\tilde{w}_i^2$ in the large-sample limit ($n \\to \\infty$). Let us express this sum in terms of the unnormalized weights $w_i$:\n$$\n\\sum_{i=1}^{n} \\tilde{w}_i^2 = \\sum_{i=1}^{n} \\left( \\frac{w_{i}}{\\sum_{j=1}^{n} w_{j}} \\right)^2 = \\frac{\\sum_{i=1}^{n} w_i^2}{\\left( \\sum_{j=1}^{n} w_j \\right)^2}\n$$\nTo analyze the asymptotic behavior, we can rewrite this as:\n$$\n\\sum_{i=1}^{n} \\tilde{w}_i^2 = \\frac{\\frac{1}{n} \\sum_{i=1}^{n} w_i^2}{n \\left( \\frac{1}{n} \\sum_{j=1}^{n} w_j \\right)^2}\n$$\nThe samples $\\{X_i\\}_{i=1}^n$ are i.i.d. draws from $q_{\\theta}(x)$. Therefore, the quantities $\\{w_i\\}_{i=1}^n$ and $\\{w_i^2\\}_{i=1}^n$ are also sets of i.i.d. random variables. By the Law of Large Numbers, as $n \\to \\infty$, the sample means converge in probability to their true expectations under $q_{\\theta}$:\n$$\n\\frac{1}{n} \\sum_{j=1}^{n} w_j \\xrightarrow{p} \\mathbb{E}_{q_{\\theta}}[w(X)] = \\int \\frac{\\tilde{p}(x)}{q_{\\theta}(x)} q_{\\theta}(x) \\, dx = \\int \\tilde{p}(x) \\, dx = Z\n$$\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} w_i^2 \\xrightarrow{p} \\mathbb{E}_{q_{\\theta}}[w(X)^2] = \\int \\left(\\frac{\\tilde{p}(x)}{q_{\\theta}(x)}\\right)^2 q_{\\theta}(x) \\, dx = \\int \\frac{\\tilde{p}(x)^2}{q_{\\theta}(x)} \\, dx\n$$\nApplying the continuous mapping theorem to the expression for $\\sum_{i=1}^{n} \\tilde{w}_i^2$, we find the asymptotic behavior of the scaled sum $n \\sum_{i=1}^{n} \\tilde{w}_i^2$:\n$$\nn \\sum_{i=1}^{n} \\tilde{w}_i^2 = \\frac{\\frac{1}{n} \\sum_{i=1}^{n} w_i^2}{\\left( \\frac{1}{n} \\sum_{j=1}^{n} w_j \\right)^2} \\xrightarrow{p} \\frac{\\mathbb{E}_{q_{\\theta}}[w(X)^2]}{(\\mathbb{E}_{q_{\\theta}}[w(X)])^2} = \\frac{\\int \\frac{\\tilde{p}(x)^2}{q_{\\theta}(x)} \\, dx}{Z^2}\n$$\nThis limiting value represents the asymptotic measure of weight non-uniformity. To make the weights as uniform as possible for large $n$, we must choose $\\theta$ to minimize this quantity. Since $Z^2$ is a positive constant that does not depend on the choice of $q_{\\theta}$, minimizing this ratio is equivalent to minimizing its numerator.\n\nTherefore, the principled criterion for choosing $q_{\\theta}$ is to minimize the functional:\n$$\nJ(\\theta) = \\int \\frac{\\tilde{p}(x)^2}{q_{\\theta}(x)} \\, dx\n$$\nThis functional depends only on the unnormalized target density $\\tilde{p}(x)$ and the proposal family $\\{q_{\\theta}(x)\\}$, and is independent of the normalization constant $Z$ and the specific function $f(x)$, as required by the problem statement. This criterion is equivalent to minimizing the $\\chi^2$-divergence from $q_{\\theta}$ to $p$, since $\\int \\frac{\\tilde{p}(x)^2}{q_{\\theta}(x)} \\, dx = Z^2 \\int \\frac{p(x)^2}{q_{\\theta}(x)} \\, dx = Z^2(\\chi^2(p || q_{\\theta}) + 1)$. Minimizing this functional leads to a proposal distribution that is robustly effective across a wide range of well-behaved functions $f$.",
            "answer": "$$\n\\boxed{\\int \\frac{\\tilde{p}(x)^{2}}{q_{\\theta}(x)} \\, dx}\n$$"
        },
        {
            "introduction": "A common and catastrophic failure in importance sampling occurs when the proposal distribution has lighter tails than the target, leading to exploding weights and infinite variance. This exercise provides a crucial thought experiment to diagnose this \"tail mismatch\" . By analyzing the asymptotic behavior of the variance criterion, you will learn to select a proposal family with appropriate tail heaviness, a critical skill for building robust estimators.",
            "id": "3295486",
            "problem": "Consider importance sampling for estimating the expectation $I = \\mathbb{E}_{p}[g(X)]$ of a bounded measurable function $g$ under a target density $p$ on $\\mathbb{R}$. Assume there exists $x_{0}  0$ and a constant $c_{p}  0$ such that $p$ has symmetric Pareto-type tails:\n$$\np(x) \\sim c_{p} \\lvert x \\rvert^{-(\\alpha+1)} \\quad \\text{as } \\lvert x \\rvert \\to \\infty, \\quad \\alpha  0.\n$$\nSuppose $g$ is bounded, i.e., there exists $M  \\infty$ such that $\\lvert g(x) \\rvert \\le M$ for all $x$. You are tasked with choosing an effective importance sampling proposal $q$ to avoid infinite variance of the importance sampling estimator based on the importance weight $w(x) = p(x)/q(x)$.\n\nTwo proposal families are under consideration:\n\n$1.$ A Gaussian proposal $q_{G}$ with density $q_{G}(x) \\propto \\exp(-x^{2}/2)$.\n\n$2.$ A Student $t_{\\nu}$ proposal $q_{T,\\nu}$ with degrees of freedom $\\nu  0$ and continuous density $q_{T,\\nu}(x)$ that is symmetric and satisfies\n$$\nq_{T,\\nu}(x) \\sim c_{q} \\lvert x \\rvert^{-(\\nu+1)} \\quad \\text{as } \\lvert x \\rvert \\to \\infty,\n$$\nfor some constant $c_{q}  0$ (i.e., the usual Student $t$ tail behavior; you may assume a fixed scale parameter whose value does not affect the tail index).\n\nYou also have access to a consistent estimator $\\widehat{\\alpha}_{n}$ of the tail index $\\alpha$ constructed from external tail data (for example, via a Hill estimator on a separate heavy-tail sample), and you can choose $\\nu$ as a function of $\\widehat{\\alpha}_{n}$ and a small safety margin $\\epsilon \\in (0,1)$.\n\nUsing only fundamental definitions of importance sampling and variance, and standard asymptotic tail comparisons, decide which of the following statements gives a correct diagnosis of the tail mismatch and a correct rule for choosing $\\nu$ so that the variance of the importance sampling estimator is finite:\n\nA. A Gaussian proposal $q_{G}$ suffices because the Central Limit Theorem ensures finite variance when $g$ is bounded.\n\nB. Use a Student $t_{\\nu}$ proposal and choose any $\\nu \\in (0, 2\\widehat{\\alpha}_{n} - \\epsilon)$ for a fixed small $\\epsilon  0$; this makes $q$ heavier-tailed than required and guarantees $\\mathbb{E}_{q}[(p(X)/q(X))^{2}]  \\infty$, hence finite variance for the estimator when $g$ is bounded.\n\nC. Use a Student $t_{\\nu}$ proposal with $\\nu = \\widehat{\\alpha}_{n}$ to match the target’s tail index; this choice is both sufficient and necessary to ensure $\\mathbb{E}_{q}[(p(X)/q(X))^{2}]  \\infty$.\n\nD. Use a Student $t_{\\nu}$ proposal with $\\nu  2\\widehat{\\alpha}_{n}$; making $q$ lighter-tailed than $p$ avoids overly large weights and guarantees finite variance.\n\nE. Use a Student $t_{\\nu}$ proposal with $\\nu \\le 2\\widehat{\\alpha}_{n}$; equality $\\nu = 2\\widehat{\\alpha}_{n}$ is acceptable and still yields finite variance of the importance sampling estimator.\n\nSelect the correct option(s) and be precise about the underlying variance finiteness condition in terms of the tail indices $\\alpha$ and $\\nu$.",
            "solution": "We begin from the definition of the importance sampling estimator. If $X \\sim q$, the basic estimator for $I = \\mathbb{E}_{p}[g(X)]$ is $\\widehat{I} = \\frac{1}{m} \\sum_{i=1}^{m} w(X_{i}) g(X_{i})$ with $w(x) = p(x)/q(x)$. Its variance under $q$ is\n$$\n\\operatorname{Var}_{q}(\\widehat{I}) = \\frac{1}{m} \\left\\{ \\mathbb{E}_{q}\\big[(w(X) g(X))^{2}\\big] - I^{2} \\right\\}.\n$$\nSince $g$ is bounded by $M$, we have\n$$\n\\mathbb{E}_{q}\\big[(w(X) g(X))^{2}\\big] \\le M^{2} \\, \\mathbb{E}_{q}\\big[ w(X)^{2} \\big].\n$$\nTherefore, a sufficient condition for finite variance is\n$$\n\\mathbb{E}_{q}\\big[ w(X)^{2} \\big] = \\int_{\\mathbb{R}} \\frac{p(x)^{2}}{q(x)} \\, \\mathrm{d}x  \\infty.\n$$\nThus, the core integrability question is whether $\\int p(x)^{2}/q(x) \\, \\mathrm{d}x$ is finite.\n\nWe now diagnose tail mismatch by asymptotic comparison. In the tails, we have $p(x) \\sim c_{p} \\lvert x \\rvert^{-(\\alpha+1)}$. For the Student $t_{\\nu}$ proposal, $q_{T,\\nu}(x) \\sim c_{q} \\lvert x \\rvert^{-(\\nu+1)}$. In the far tails, the integrand behaves as\n$$\n\\frac{p(x)^{2}}{q_{T,\\nu}(x)} \\sim C \\, \\lvert x \\rvert^{-\\left(2(\\alpha+1)\\right)} \\, \\lvert x \\rvert^{\\nu+1} = C \\, \\lvert x \\rvert^{-\\left(2\\alpha + 2 - \\nu - 1\\right)} = C \\, \\lvert x \\rvert^{-\\left(2\\alpha + 1 - \\nu\\right)},\n$$\nfor some constant $C  0$. In one dimension, the integral $\\int_{R}^{\\infty} x^{-k} \\, \\mathrm{d}x$ converges if and only if $k  1$. Hence we require the tail exponent to satisfy\n$$\n2\\alpha + 1 - \\nu  1 \\quad \\Longleftrightarrow \\quad 2\\alpha - \\nu  0 \\quad \\Longleftrightarrow \\quad \\nu  2\\alpha.\n$$\nTherefore, for finite $\\mathbb{E}_{q}[w^{2}]$ (and hence finite variance of the estimator when $g$ is bounded), the Student $t$ degrees of freedom must satisfy the strict inequality $\\nu  2\\alpha$. The boundary case $\\nu = 2\\alpha$ yields a tail exponent equal to $1$, which leads to a logarithmically divergent integral; thus it does not yield finite variance.\n\nNext, consider a Gaussian proposal with density $q_{G}(x) \\propto \\exp(-x^{2}/2)$. In the tails,\n$$\n\\frac{p(x)^{2}}{q_{G}(x)} \\asymp \\lvert x \\rvert^{-2(\\alpha+1)} \\, \\exp\\!\\left(\\frac{x^{2}}{2}\\right),\n$$\nwhose integral clearly diverges because the Gaussian denominator decays exponentially, making $1/q_{G}(x)$ grow super-polynomially, and the polynomial decay of $p(x)^{2}$ cannot offset the exponential growth. Thus, $\\mathbb{E}_{q_{G}}[w^{2}] = \\infty$ and the importance sampling variance is infinite under $q_{G}$.\n\nWe now evaluate the options:\n\nA. This claims that a Gaussian proposal suffices due to the Central Limit Theorem. This is incorrect. The Central Limit Theorem does not guarantee finite variance of the importance sampling estimator; indeed, we explicitly computed that $\\int p^{2}/q_{G}$ diverges, so $\\mathbb{E}_{q_{G}}[w^{2}] = \\infty$, even with bounded $g$. Verdict: Incorrect.\n\nB. This proposes to use a Student $t_{\\nu}$ with any $\\nu \\in (0, 2\\widehat{\\alpha}_{n} - \\epsilon)$ for a small $\\epsilon  0$. Since $\\widehat{\\alpha}_{n}$ is consistent for $\\alpha$ and $\\epsilon  0$ is fixed, for large $n$ this ensures $\\nu  2\\alpha$ with high probability, which is exactly the strict condition required for $\\mathbb{E}_{q}[w^{2}]  \\infty$. Moreover, choosing $\\nu$ smaller (heavier tails) than the threshold is conservative and helps avoid infinite variance due to estimation error. The additional suggestion to tune a scale parameter to match the body does not affect the tail index and hence does not alter the variance finiteness conclusion. Verdict: Correct.\n\nC. This sets $\\nu = \\widehat{\\alpha}_{n}$ and asserts that this is both sufficient and necessary. The condition $\\nu  2\\alpha$ does not require equality $\\nu = \\alpha$; many values satisfy it. While $\\nu = \\alpha$ (if indeed equal to the true $\\alpha$) is sufficient, it is not necessary. The necessity claim is therefore false. Verdict: Incorrect.\n\nD. This recommends $\\nu  2\\widehat{\\alpha}_{n}$, which makes the proposal lighter-tailed than the threshold and, asymptotically, lighter-tailed than required. This violates the strict condition $\\nu  2\\alpha$ and leads to $\\mathbb{E}_{q}[w^{2}] = \\infty$. Verdict: Incorrect.\n\nE. This allows $\\nu \\le 2\\widehat{\\alpha}_{n}$ and claims equality is acceptable. The boundary $\\nu = 2\\alpha$ yields a tail exponent equal to $1$, which causes logarithmic divergence of $\\int p^{2}/q$. Thus, strict inequality is required; equality is not acceptable. Verdict: Incorrect.\n\nTherefore, the correct choice is to use a Student $t_{\\nu}$ proposal with $\\nu$ strictly less than $2\\alpha$. When using an estimator $\\widehat{\\alpha}_{n}$, a practical and principled rule is to pick $\\nu$ below $2\\widehat{\\alpha}_{n}$ by a fixed safety margin $\\epsilon  0$ to preserve the strict inequality with high probability and avoid infinite variance of the importance sampling estimator.",
            "answer": "$$\\boxed{B}$$"
        },
        {
            "introduction": "Complex integration problems can often be simplified by partitioning the state space into more manageable regions, or \"strata.\" This hands-on exercise  puts theory into practice by developing a complete stratified importance sampling scheme, from derivation to implementation. You will first derive the optimal proposal within each stratum and then determine the optimal allocation of computational budget across strata—a principle that directs sampling effort to where it is needed most.",
            "id": "3295480",
            "problem": "You are given a target expectation under a known target density and a measurable integrand. Let $X \\in \\mathcal{X}$ have density $p(x)$ with respect to Lebesgue measure, and let the goal be to estimate the integral $\\mu = \\mathbb{E}_p[f(X)] = \\int_{\\mathcal{X}} f(x) p(x) \\, dx$. Suppose the state space is partitioned into disjoint measurable strata $\\{S_k\\}_{k=1}^K$ such that $\\bigcup_{k=1}^K S_k = \\mathcal{X}$. Consider a stratified importance sampling estimator that, for each stratum $S_k$, draws $n_k$ independent samples from a proposal density $q_k(x)$ supported on $S_k$ and forms the unbiased estimator\n$$\\hat{\\mu} = \\sum_{k=1}^K \\frac{1}{n_k} \\sum_{i=1}^{n_k} f(X_{k,i}) \\frac{p(X_{k,i})}{q_k(X_{k,i})} \\mathbf{1}_{S_k}(X_{k,i}), \\quad X_{k,i} \\sim q_k(\\cdot).$$\nAssume all integrals are finite and the proposals $q_k$ are absolutely continuous with respect to Lebesgue measure on $S_k$.\n\nTask A (Derivation): Starting only from the definitions of importance sampling, unbiasedness, variance, and the Cauchy–Schwarz inequality, derive the following design principles for a fixed total sample budget $N = \\sum_{k=1}^K n_k$:\n- For each fixed stratum $S_k$, characterize the choice of proposal density $q_k$ (supported on $S_k$) that minimizes the per-sample variance contribution $\\mathrm{Var}_{q_k}\\big(f(X) \\tfrac{p(X)}{q_k(X)} \\mathbf{1}_{S_k}(X)\\big)$. Express the answer up to a normalizing constant and prove that it achieves the stated minimum.\n- Show that, under these per-stratum optimal proposals, the minimal per-sample variance in stratum $k$ can be written in terms of stratum-specific integrals of $f$ and $|f|$ against $p$, and that it is nonnegative.\n- Using a Lagrange multiplier argument, derive the allocation rule for $\\{n_k\\}$ that minimizes the total variance $\\sum_{k=1}^K \\mathrm{Var}_{q_k}\\big(f(X)\\tfrac{p}{q_k}\\mathbf{1}_{S_k}\\big)/n_k$ subject to $\\sum_{k=1}^K n_k = N$, and show how this rule equalizes the marginal variance contributions across strata. Give the expression for the resulting minimal total variance as a function of $N$ and the per-stratum difficulty measures.\n\nTask B (Computation): Implement a program that, for each of the following test cases, computes the per-stratum difficulty measures implied by Task A, the optimal allocation fractions $r_k = n_k/N$ that minimize variance for a fixed total budget $N$, and the corresponding minimal total variance. Because the required integrals do not generally admit closed forms, your program must evaluate them numerically by one-dimensional quadrature. All angles must be interpreted in radians.\n\nFor each test case, the program must:\n- Define $p(x)$ and $f(x)$ as specified below.\n- Define the strata $\\{S_k\\}_{k=1}^K$ as intervals; use $(-\\infty)$ and $(+\\infty)$ for unbounded endpoints.\n- Compute, for each $k$, the stratum integrals\n  $$\\mu_k = \\int_{S_k} f(x) p(x) \\, dx, \\qquad A_k = \\int_{S_k} |f(x)| p(x) \\, dx.$$\n- Compute the minimal per-sample variance in stratum $k$ as\n  $$V_k^\\star = \\max\\{A_k^2 - \\mu_k^2, 0\\},$$\n  where the maximum protects against small negative values due to numerical quadrature error.\n- Compute the allocation fractions\n  $$r_k = \\begin{cases}\n    \\dfrac{\\sqrt{V_k^\\star}}{\\sum_{j=1}^K \\sqrt{V_j^\\star}},  \\text{if } \\sum_{j=1}^K \\sqrt{V_j^\\star}  0,\\\\[1em]\n    \\dfrac{1}{K},  \\text{otherwise,}\n  \\end{cases}$$\n  and the minimal total variance\n  $$\\mathrm{Var}_{\\min} = \\frac{\\left(\\sum_{k=1}^K \\sqrt{V_k^\\star}\\right)^2}{N}.$$\n\nTest Suite:\n- Case 1:\n  - Target density $p(x)$ is $\\mathrm{Beta}(a,b)$ on $[0,1]$ with parameters $a=2.0$, $b=5.0$; i.e., $p(x) = \\dfrac{x^{a-1}(1-x)^{b-1}}{B(a,b)}$ for $x \\in [0,1]$ and $p(x)=0$ otherwise, where $B(a,b)$ is the beta function.\n  - Integrand $f(x) = \\sin(6 \\pi x)$.\n  - Strata: $S_1 = [0,0.5]$, $S_2 = (0.5,1]$.\n  - Total budget $N = 1000$.\n- Case 2:\n  - Target density $p(x)$ is $\\mathrm{Beta}(a,b)$ on $[0,1]$ with $a=0.7$, $b=3.2$.\n  - Integrand $f(x) = \\sin(10 \\pi x) + 0.5 \\cos(4 \\pi x)$.\n  - Strata: $S_1 = [0,0.25]$, $S_2 = (0.25,0.75]$, $S_3 = (0.75,1]$.\n  - Total budget $N = 1500$.\n- Case 3:\n  - Target density $p(x)$ is $\\mathcal{N}(0,1)$ on $\\mathbb{R}$, i.e., $p(x) = \\dfrac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\dfrac{x^2}{2}\\right)$ for all $x \\in \\mathbb{R}$.\n  - Integrand $f(x) = x^2 \\sin(3 x)$.\n  - Strata: $S_1 = (-\\infty,-1]$, $S_2 = (-1,1]$, $S_3 = (1,\\infty)$.\n  - Total budget $N = 5000$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists.\n- For each test case, output a list of $K+1$ floating-point numbers: the $K$ allocation fractions $[r_1,\\dots,r_K]$ followed by the minimal total variance $\\mathrm{Var}_{\\min}$, in that order.\n- All floating-point numbers must be rounded to six decimal places.\n- The final output must be a single line in the exact format: for example, for two test cases with two strata each, it should look like $[[r_{1,1},r_{1,2},V_{\\min,1}],[r_{2,1},r_{2,2},V_{\\min,2}]]$ with no spaces.\n\nYour program must be self-contained, must not read any input, must not access external files or networks, and must run deterministically.",
            "solution": "The problem asks for two tasks. Task A is the derivation of the optimal design for a stratified importance sampling estimator. Task B is the computation of this optimal design for three specific test cases.\n\n### Task A: Derivation of Design Principles\n\nThe estimator for the integral $\\mu = \\mathbb{E}_p[f(X)]$ is given by\n$$ \\hat{\\mu} = \\sum_{k=1}^K \\hat{\\mu}_k = \\sum_{k=1}^K \\frac{1}{n_k} \\sum_{i=1}^{n_k} f(X_{k,i}) \\frac{p(X_{k,i})}{q_k(X_{k,i})} \\mathbf{1}_{S_k}(X_{k,i}), \\quad X_{k,i} \\sim q_k(\\cdot) $$\nwhere the strata $\\{S_k\\}_{k=1}^K$ partition the state space $\\mathcal{X}$. Since samples are drawn independently for each stratum and across strata, the variance of the total estimator is the sum of the variances of the stratum estimators:\n$$ \\mathrm{Var}(\\hat{\\mu}) = \\sum_{k=1}^K \\mathrm{Var}(\\hat{\\mu}_k) $$\nThe estimator for a single stratum $S_k$ is an average of $n_k$ independent and identically distributed random variables. Let $Y_{k,i} = f(X_{k,i}) \\frac{p(X_{k,i})}{q_k(X_{k,i})}$, where $X_{k,i} \\sim q_k$. The estimator for stratum $k$ is $\\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i=1}^{n_k} Y_{k,i}$. Its variance is\n$$ \\mathrm{Var}(\\hat{\\mu}_k) = \\mathrm{Var}\\left(\\frac{1}{n_k} \\sum_{i=1}^{n_k} Y_{k,i}\\right) = \\frac{1}{n_k} \\mathrm{Var}_{q_k}(Y_k) $$\nwhere $Y_k$ is a generic random variable with the same distribution as any of the $Y_{k,i}$. The total variance is therefore\n$$ \\mathrm{Var}(\\hat{\\mu}) = \\sum_{k=1}^K \\frac{\\mathrm{Var}_{q_k}(Y_k)}{n_k} $$\nMinimizing this total variance involves two steps: first, for each stratum $S_k$, choosing the proposal $q_k$ to minimize the per-sample variance $\\mathrm{Var}_{q_k}(Y_k)$; second, allocating the total sample budget $N$ among the strata (i.e., choosing $\\{n_k\\}$) to minimize the resulting total variance.\n\n**1. Optimal Proposal Density $q_k(x)$**\n\nFor a fixed stratum $S_k$, we want to minimize the per-sample variance:\n$$ \\mathrm{Var}_{q_k}(Y_k) = \\mathbb{E}_{q_k}[Y_k^2] - (\\mathbb{E}_{q_k}[Y_k])^2 $$\nFirst, let's analyze the expectation term. By definition of importance sampling, the estimator is unbiased for the stratum integral $\\mu_k = \\int_{S_k} f(x) p(x) \\, dx$.\n$$ \\mathbb{E}_{q_k}[Y_k] = \\int_{S_k} f(x) \\frac{p(x)}{q_k(x)} q_k(x) \\, dx = \\int_{S_k} f(x) p(x) \\, dx =: \\mu_k $$\nThis expectation $\\mu_k$ is a constant that depends only on $f$, $p$, and $S_k$, not on the choice of the proposal density $q_k(x)$. Therefore, minimizing $\\mathrm{Var}_{q_k}(Y_k)$ is equivalent to minimizing the second moment, $\\mathbb{E}_{q_k}[Y_k^2]$.\n\nThe second moment is:\n$$ \\mathbb{E}_{q_k}[Y_k^2] = \\int_{S_k} \\left(f(x) \\frac{p(x)}{q_k(x)}\\right)^2 q_k(x) \\, dx = \\int_{S_k} \\frac{f(x)^2 p(x)^2}{q_k(x)} \\, dx $$\nWe seek to minimize this integral with respect to the function $q_k(x)$, subject to the constraint that $q_k(x)$ is a probability density function on $S_k$, i.e., $\\int_{S_k} q_k(x) \\, dx = 1$ and $q_k(x) \\ge 0$.\n\nWe can find a lower bound for this integral using the Cauchy-Schwarz inequality for integrals, which states that for real-valued functions $g$ and $h$, $(\\int g(x)h(x) dx)^2 \\le (\\int g(x)^2 dx)(\\int h(x)^2 dx)$. Let's define $g(x) = \\sqrt{q_k(x)}$ and $h(x) = \\frac{|f(x)|p(x)}{\\sqrt{q_k(x)}}$. Applying the inequality over the domain $S_k$:\n$$ \\left( \\int_{S_k} \\sqrt{q_k(x)} \\cdot \\frac{|f(x)|p(x)}{\\sqrt{q_k(x)}} \\, dx \\right)^2 \\le \\left( \\int_{S_k} (\\sqrt{q_k(x)})^2 \\, dx \\right) \\left( \\int_{S_k} \\left(\\frac{|f(x)|p(x)}{\\sqrt{q_k(x)}}\\right)^2 \\, dx \\right) $$\nThis simplifies to:\n$$ \\left( \\int_{S_k} |f(x)|p(x) \\, dx \\right)^2 \\le \\left( \\int_{S_k} q_k(x) \\, dx \\right) \\left( \\int_{S_k} \\frac{f(x)^2 p(x)^2}{q_k(x)} \\, dx \\right) $$\nSince $\\int_{S_k} q_k(x) \\, dx = 1$, we have:\n$$ \\mathbb{E}_{q_k}[Y_k^2] = \\int_{S_k} \\frac{f(x)^2 p(x)^2}{q_k(x)} \\, dx \\ge \\left( \\int_{S_k} |f(x)|p(x) \\, dx \\right)^2 $$\nThe minimum value of $\\mathbb{E}_{q_k}[Y_k^2]$ is achieved when the equality in the Cauchy-Schwarz inequality holds. This occurs if and only if $g(x)$ is proportional to $h(x)$, i.e., $\\sqrt{q_k(x)} \\propto \\frac{|f(x)|p(x)}{\\sqrt{q_k(x)}}$. This implies $q_k(x) \\propto |f(x)|p(x)$.\n\nThus, the optimal proposal density $q_k^*(x)$ that minimizes the per-sample variance is proportional to $|f(x)|p(x)$ on the stratum $S_k$:\n$$ q_k^*(x) = \\frac{|f(x)|p(x)}{\\int_{S_k} |f(y)|p(y) \\, dy} $$\n\n**2. Minimal Per-Sample Variance**\n\nLet's compute the minimal variance using the optimal proposal $q_k^*(x)$. Let $A_k = \\int_{S_k} |f(x)|p(x) \\, dx$.\nThe minimum value of the second moment is the lower bound we found:\n$$ \\min_{q_k} \\mathbb{E}_{q_k}[Y_k^2] = \\left( \\int_{S_k} |f(x)|p(x) \\, dx \\right)^2 = A_k^2 $$\nThe minimal per-sample variance for stratum $k$, denoted $V_k^\\star$, is then:\n$$ V_k^\\star = \\min_{q_k} \\mathrm{Var}_{q_k}(Y_k) = A_k^2 - \\mu_k^2 = \\left(\\int_{S_k} |f(x)|p(x) \\, dx\\right)^2 - \\left(\\int_{S_k} f(x)p(x) \\, dx\\right)^2 $$\nTo show this is non-negative, we use the property of integrals that $|\\int g(x) dx| \\le \\int |g(x)| dx$. Let $g(x) = f(x)p(x)$. Since $p(x) \\ge 0$, $|g(x)| = |f(x)|p(x)$.\n$$ |\\mu_k| = \\left|\\int_{S_k} f(x)p(x) \\, dx\\right| \\le \\int_{S_k} |f(x)p(x)| \\, dx = \\int_{S_k} |f(x)|p(x) \\, dx = A_k $$\nSquaring both sides gives $\\mu_k^2 \\le A_k^2$, which implies $V_k^\\star = A_k^2 - \\mu_k^2 \\ge 0$.\n\n**3. Optimal Sample Allocation $\\{n_k\\}$**\n\nWith the optimal proposal densities $q_k^*(x)$ for each stratum, the total variance becomes:\n$$ V_{tot}(\\{n_k\\}) = \\sum_{k=1}^K \\frac{V_k^\\star}{n_k} $$\nWe need to minimize this quantity subject to the constraint on the total sample size, $\\sum_{k=1}^K n_k = N$. We use the method of Lagrange multipliers. The Lagrangian is:\n$$ \\mathcal{L}(\\{n_k\\}, \\lambda) = \\sum_{k=1}^K \\frac{V_k^\\star}{n_k} + \\lambda \\left( \\left(\\sum_{k=1}^K n_k\\right) - N \\right) $$\nTaking the partial derivative with respect to each $n_k$ and setting it to zero gives the optimality condition:\n$$ \\frac{\\partial\\mathcal{L}}{\\partial n_k} = -\\frac{V_k^\\star}{n_k^2} + \\lambda = 0 \\implies n_k^2 = \\frac{V_k^\\star}{\\lambda} \\implies n_k = \\frac{\\sqrt{V_k^\\star}}{\\sqrt{\\lambda}} $$\nThis result shows that the optimal sample size $n_k$ is proportional to $\\sqrt{V_k^\\star}$, which can be interpreted as the standard deviation of the importance-weighted samples in stratum $k$ under the optimal proposal. The condition $\\lambda = V_k^\\star / n_k^2$ being constant for all $k$ means that at the optimum, the marginal reduction in variance from allocating one more sample point is equal across all strata.\n\nTo find the value of the multiplier, we use the budget constraint:\n$$ \\sum_{k=1}^K n_k = \\sum_{k=1}^K \\frac{\\sqrt{V_k^\\star}}{\\sqrt{\\lambda}} = \\frac{1}{\\sqrt{\\lambda}} \\sum_{k=1}^K \\sqrt{V_k^\\star} = N $$\nSolving for $\\sqrt{\\lambda}$:\n$$ \\sqrt{\\lambda} = \\frac{\\sum_{j=1}^K \\sqrt{V_j^\\star}}{N} $$\nSubstituting this back into the expression for $n_k$:\n$$ n_k = \\sqrt{V_k^\\star} \\cdot \\frac{N}{\\sum_{j=1}^K \\sqrt{V_j^\\star}} = N \\frac{\\sqrt{V_k^\\star}}{\\sum_{j=1}^K \\sqrt{V_j^\\star}} $$\nThis is the optimal allocation rule, also known as Neyman allocation.\n\nFinally, we find the resulting minimum total variance, $\\mathrm{Var}_{\\min}$, by substituting the optimal $n_k$ values back into the expression for $V_{tot}$:\n$$ \\mathrm{Var}_{\\min} = \\sum_{k=1}^K \\frac{V_k^\\star}{n_k} = \\sum_{k=1}^K V_k^\\star \\left( N \\frac{\\sqrt{V_k^\\star}}{\\sum_{j=1}^K \\sqrt{V_j^\\star}} \\right)^{-1} = \\sum_{k=1}^K \\frac{\\sqrt{V_k^\\star} \\left(\\sum_{j=1}^K \\sqrt{V_j^\\star}\\right)}{N} $$\n$$ \\mathrm{Var}_{\\min} = \\frac{1}{N} \\left(\\sum_{j=1}^K \\sqrt{V_j^\\star}\\right) \\left(\\sum_{k=1}^K \\sqrt{V_k^\\star}\\right) = \\frac{\\left(\\sum_{k=1}^K \\sqrt{V_k^\\star}\\right)^2}{N} $$\nThe terms $\\sqrt{V_k^\\star}$ are the per-stratum difficulty measures mentioned in the problem.",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.stats import beta, norm\n\ndef solve():\n    \"\"\"\n    Computes optimal allocation and minimal variance for stratified importance sampling\n    for three specified test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"p_dist\": (\"beta\", 2.0, 5.0),\n            \"f\": lambda x: np.sin(6 * np.pi * x),\n            \"strata\": [[0, 0.5], [0.5, 1.0]],\n            \"N\": 1000,\n        },\n        {\n            \"p_dist\": (\"beta\", 0.7, 3.2),\n            \"f\": lambda x: np.sin(10 * np.pi * x) + 0.5 * np.cos(4 * np.pi * x),\n            \"strata\": [[0, 0.25], [0.25, 0.75], [0.75, 1.0]],\n            \"N\": 1500,\n        },\n        {\n            \"p_dist\": (\"norm\", 0.0, 1.0),\n            \"f\": lambda x: x**2 * np.sin(3 * x),\n            \"strata\": [[-np.inf, -1.0], [-1.0, 1.0], [1.0, np.inf]],\n            \"N\": 5000,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        p_dist_info = case[\"p_dist\"]\n        f = case[\"f\"]\n        strata = case[\"strata\"]\n        N = case[\"N\"]\n        K = len(strata)\n\n        if p_dist_info[0] == \"beta\":\n            a, b = p_dist_info[1], p_dist_info[2]\n            p = lambda x: beta.pdf(x, a, b)\n        elif p_dist_info[0] == \"norm\":\n            mu, sigma = p_dist_info[1], p_dist_info[2]\n            p = lambda x: norm.pdf(x, mu, sigma)\n        \n        V_star_list = []\n        for k in range(K):\n            lower, upper = strata[k]\n            \n            # Integrand for mu_k: f(x) * p(x)\n            integrand_mu = lambda x: f(x) * p(x)\n            mu_k, _ = quad(integrand_mu, lower, upper)\n            \n            # Integrand for A_k: |f(x)| * p(x)\n            integrand_A = lambda x: np.abs(f(x)) * p(x)\n            A_k, _ = quad(integrand_A, lower, upper)\n            \n            # Minimal per-sample variance for stratum k\n            V_star_k = max(A_k**2 - mu_k**2, 0)\n            V_star_list.append(V_star_k)\n\n        # Compute allocation fractions and minimal total variance\n        sqrt_V_star = np.sqrt(V_star_list)\n        sum_sqrt_V_star = np.sum(sqrt_V_star)\n\n        if sum_sqrt_V_star > 0:\n            ratios = sqrt_V_star / sum_sqrt_V_star\n        else:\n            ratios = np.full(K, 1.0 / K)\n            \n        var_min = (sum_sqrt_V_star**2) / N\n\n        case_result = list(ratios)\n        case_result.append(var_min)\n        results.append(case_result)\n\n    # Format output string\n    case_strings = []\n    for res_list in results:\n        formatted_nums = [f\"{x:.6f}\" for x in res_list]\n        case_strings.append(f\"[{','.join(formatted_nums)}]\")\n    \n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```"
        }
    ]
}