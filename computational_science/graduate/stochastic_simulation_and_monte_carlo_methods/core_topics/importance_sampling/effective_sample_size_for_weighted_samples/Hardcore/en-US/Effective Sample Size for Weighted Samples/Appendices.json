{
    "hands_on_practices": [
        {
            "introduction": "The most common formula for the Effective Sample Size (ESS) is often presented without justification. This exercise rectifies that by guiding you through its derivation from first principles. By equating the variance of a self-normalized importance sampling estimator with that of an ideal, unweighted estimator, you will develop a deeper intuition for what ESS truly represents: the number of independent samples that would provide an equivalent amount of information. This fundamental understanding is crucial for interpreting and applying ESS correctly in practice .",
            "id": "3304977",
            "problem": "Consider a target distribution with density $\\pi(x)$ on $\\mathbb{R}^{d}$ and an integrand $h:\\mathbb{R}^{d}\\to\\mathbb{R}$ with finite variance under $\\pi$. Let $x_{1},\\dots,x_{N}$ be independent draws from a proposal density $q(x)$, and define the unnormalized importance weights $w_{i}=\\pi(x_{i})/q(x_{i}) > 0$. The Self-Normalized Importance Sampling (SNIS) estimator of $I=\\mathbb{E}_{\\pi}[h(X)]$ is $\\hat{I}_{\\mathrm{SN}}=\\sum_{i=1}^{N}\\tilde{w}_{i}h(x_{i})$ with normalized weights $\\tilde{w}_{i}=w_{i}/\\sum_{j=1}^{N}w_{j}$. Define the Effective Sample Size (ESS) as the number $N_{\\mathrm{eff}}$ such that the variance of $\\hat{I}_{\\mathrm{SN}}$ is matched to that of an equal-weight average of $N_{\\mathrm{eff}}$ independent target draws, in the following sense: use the variance property of weighted averages of independent and identically distributed random variables to express $\\operatorname{Var}(\\hat{I}_{\\mathrm{SN}})$ in terms of the $\\tilde{w}_{i}$ and equate it to the variance of an equal-weight average of $N_{\\mathrm{eff}}$ independent copies of $h(X)$ under $\\pi$. Derive an explicit expression for $N_{\\mathrm{eff}}$ in terms of the unnormalized weights $\\{w_{i}\\}_{i=1}^{N}$.\n\nNext, assess the sensitivity of $N_{\\mathrm{eff}}$ to scaling of the unnormalized weights: for a constant $c>0$, consider the scaled weights $w_{i}\\mapsto c\\,w_{i}$ and compute the derivative $\\frac{\\mathrm{d}}{\\mathrm{d}c}N_{\\mathrm{eff}}(c\\,w_{1},\\dots,c\\,w_{N})$ at an arbitrary $c>0$.\n\nExpress your final answer as a two-entry row vector containing, in order: the derived expression for $N_{\\mathrm{eff}}$ in terms of $\\{w_{i}\\}$ and the simplified analytic expression for $\\frac{\\mathrm{d}}{\\mathrm{d}c}N_{\\mathrm{eff}}(c\\,w_{1},\\dots,c\\,w_{N})$. No rounding is required.",
            "solution": "The problem presents two tasks: first, to derive an expression for the effective sample size ($N_{\\mathrm{eff}}$) for a Self-Normalized Importance Sampling (SNIS) estimator based on a specified variance-matching condition; second, to evaluate the sensitivity of this $N_{\\mathrm{eff}}$ to a global scaling of the unnormalized weights.\n\nWe begin with the first task: the derivation of $N_{\\mathrm{eff}}$.\n\nThe problem defines the effective sample size $N_{\\mathrm{eff}}$ as the number of independent samples from the target distribution $\\pi(x)$ that would yield an estimator with the same variance as the SNIS estimator $\\hat{I}_{\\mathrm{SN}}$.\n\nFirst, let us establish the variance of the benchmark estimator. An ideal Monte Carlo estimator for $I = \\mathbb{E}_{\\pi}[h(X)]$ based on $N_{\\mathrm{eff}}$ independent and identically distributed (i.i.d.) samples $\\{X'_j\\}_{j=1}^{N_{\\mathrm{eff}}}$ drawn directly from the target distribution $\\pi(x)$ would be the simple mean $\\hat{I}_{\\mathrm{MC}} = \\frac{1}{N_{\\mathrm{eff}}} \\sum_{j=1}^{N_{\\mathrm{eff}}} h(X'_j)$. The variance of this estimator is given by:\n$$\n\\operatorname{Var}(\\hat{I}_{\\mathrm{MC}}) = \\operatorname{Var}\\left(\\frac{1}{N_{\\mathrm{eff}}} \\sum_{j=1}^{N_{\\mathrm{eff}}} h(X'_j)\\right) = \\frac{1}{N_{\\mathrm{eff}}^2} \\sum_{j=1}^{N_{\\mathrm{eff}}} \\operatorname{Var}_{\\pi}(h(X)) = \\frac{N_{\\mathrm{eff}}}{N_{\\mathrm{eff}}^2} \\operatorname{Var}_{\\pi}(h(X)) = \\frac{\\sigma_{\\pi}^2}{N_{\\mathrm{eff}}}\n$$\nwhere $\\sigma_{\\pi}^2 = \\operatorname{Var}_{\\pi}(h(X))$ is the variance of the integrand $h(X)$ under the target distribution $\\pi$.\n\nNext, we consider the SNIS estimator, $\\hat{I}_{\\mathrm{SN}} = \\sum_{i=1}^{N}\\tilde{w}_{i}h(x_{i})$, where $x_i$ are i.i.d. draws from the proposal distribution $q(x)$ and $\\tilde{w}_i$ are the normalized importance weights. A rigorous derivation of $\\operatorname{Var}(\\hat{I}_{\\mathrm{SN}})$ is complex because the weights $\\tilde{w}_i$ are themselves random variables and are correlated. However, the problem statement provides a specific instruction: \"use the variance property of weighted averages of independent and identically distributed random variables to express $\\operatorname{Var}(\\hat{I}_{\\mathrm{SN}})$ in terms of the $\\tilde{w}_{i}$\".\n\nThis directs us to adopt a common heuristic model where the estimator's variance is approximated by treating the normalized weights $\\tilde{w}_i$ as fixed, pre-computed constants and pretending the random variables being averaged, let's call them $Z_i$, are i.i.d. draws from the *target* distribution $\\pi$. Under this simplifying model, the variance of the weighted sum $\\sum_{i=1}^{N} \\tilde{w}_i Z_i$, where $Z_i$ are i.i.d. with variance $\\sigma_{\\pi}^2$, is:\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{N} \\tilde{w}_i Z_i\\right) = \\sum_{i=1}^{N} \\operatorname{Var}(\\tilde{w}_i Z_i) = \\sum_{i=1}^{N} \\tilde{w}_i^2 \\operatorname{Var}(Z_i) = \\left(\\sum_{i=1}^{N} \\tilde{w}_i^2\\right) \\sigma_{\\pi}^2\n$$\nThis is the expression for the variance of $\\hat{I}_{\\mathrm{SN}}$ that we are instructed to use.\n\nAccording to the problem's definition of $N_{\\mathrm{eff}}$, we must equate the two variance expressions:\n$$\n\\frac{\\sigma_{\\pi}^2}{N_{\\mathrm{eff}}} = \\left(\\sum_{i=1}^{N} \\tilde{w}_i^2\\right) \\sigma_{\\pi}^2\n$$\nAssuming that $h(x)$ is not constant almost everywhere under $\\pi$, we have $\\sigma_{\\pi}^2 > 0$, and we can divide both sides by $\\sigma_{\\pi}^2$ to solve for $N_{\\mathrm{eff}}$:\n$$\nN_{\\mathrm{eff}} = \\frac{1}{\\sum_{i=1}^{N} \\tilde{w}_i^2}\n$$\nThe problem requires this expression in terms of the unnormalized weights $w_i = \\pi(x_i)/q(x_i)$. The normalized weights are $\\tilde{w}_i = w_i / \\sum_{j=1}^{N} w_j$. Substituting this into our expression for $N_{\\mathrm{eff}}$:\n$$\nN_{\\mathrm{eff}} = \\frac{1}{\\sum_{i=1}^{N} \\left(\\frac{w_i}{\\sum_{j=1}^{N} w_j}\\right)^2} = \\frac{1}{\\frac{\\sum_{i=1}^{N} w_i^2}{\\left(\\sum_{j=1}^{N} w_j\\right)^2}}\n$$\nSimplifying this yields the first part of our answer:\n$$\nN_{\\mathrm{eff}} = \\frac{\\left(\\sum_{i=1}^{N} w_i\\right)^2}{\\sum_{i=1}^{N} w_i^2}\n$$\n\nNow, for the second task, we must assess the sensitivity of $N_{\\mathrm{eff}}$ to a scaling of the unnormalized weights. Let's consider a set of scaled weights $w'_i = c w_i$ for some constant $c > 0$. We define $N_{\\mathrm{eff}}(c)$ as the effective sample size computed using these scaled weights:\n$$\nN_{\\mathrm{eff}}(c) = N_{\\mathrm{eff}}(c w_1, \\dots, c w_N) = \\frac{\\left(\\sum_{i=1}^{N} (c w_i)\\right)^2}{\\sum_{i=1}^{N} (c w_i)^2}\n$$\nWe can factor the constant $c$ out of the sums:\n$$\nN_{\\mathrm{eff}}(c) = \\frac{\\left(c \\sum_{i=1}^{N} w_i\\right)^2}{\\sum_{i=1}^{N} c^2 w_i^2} = \\frac{c^2 \\left(\\sum_{i=1}^{N} w_i\\right)^2}{c^2 \\sum_{i=1}^{N} w_i^2}\n$$\nSince $c > 0$, we have $c^2 \\neq 0$, so we can cancel the $c^2$ terms in the numerator and denominator:\n$$\nN_{\\mathrm{eff}}(c) = \\frac{\\left(\\sum_{i=1}^{N} w_i\\right)^2}{\\sum_{i=1}^{N} w_i^2}\n$$\nThis expression is identical to the original expression for $N_{\\mathrm{eff}}$ and is independent of the scaling factor $c$. This is expected, as the process of normalizing weights, $\\tilde{w}_i = w_i / \\sum_j w_j$, inherently removes any global scaling factor. The scaled normalized weights are $\\tilde{w}'_i = \\frac{c w_i}{\\sum_j (c w_j)} = \\frac{c w_i}{c \\sum_j w_j} = \\tilde{w}_i$, so the quantity $N_{\\mathrm{eff}} = 1/\\sum_i \\tilde{w}_i^2$ is manifestly invariant to scaling.\n\nSince $N_{\\mathrm{eff}}(c)$ is a constant function with respect to $c$, its derivative with respect to $c$ must be zero for any $c > 0$.\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}c}N_{\\mathrm{eff}}(c\\,w_{1},\\dots,c\\,w_{N}) = \\frac{\\mathrm{d}}{\\mathrm{d}c} \\left( \\frac{\\left(\\sum_{i=1}^{N} w_i\\right)^2}{\\sum_{i=1}^{N} w_i^2} \\right) = 0\n$$\nThis completes the second part of the problem. We assemble the two results into a row vector as requested.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\left(\\sum_{i=1}^{N} w_i\\right)^2}{\\sum_{i=1}^{N} w_i^2}  0\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "While the mathematical formula for ESS is elegant, its direct implementation can be a numerical minefield, especially when working with the large range of values encountered in modern statistical models. This practice confronts the real-world challenge of computing ESS from log-weights, a common scenario in algorithms where likelihoods are handled in log-space to prevent underflow or overflow. You will design and implement a numerically stable algorithm using the log-sum-exp trick, a vital technique in any computational statistician's toolkit .",
            "id": "3304971",
            "problem": "You are given a collection of weighted samples with unnormalized weights known only through their natural logarithms. Let there be $n$ samples indexed by $i \\in \\{1,\\dots,n\\}$, with unnormalized positive weights $W_i \\in (0,\\infty)$, and define the log-weights $\\ell_i = \\log W_i \\in \\mathbb{R} \\cup \\{-\\infty\\}$. The weighted estimator is formed with normalized weights $w_i = W_i / \\sum_{j=1}^{n} W_j$, and the Effective Sample Size (ESS) is the quantity that equates the variance of this weighted estimator to the variance of an unweighted estimator with the same underlying samples but equal weights. Your task is to start from the variance properties of independent random variables and derive the ESS in terms of the weights, and then design a numerically stable algorithm that operates only on $\\ell_i$ to compute the ESS without explicit formation of $W_i$ and without sacrificing numerical accuracy when $\\ell_i$ are very large (positive or negative) or equal to $-\\infty$.\n\nUse the following fundamental base in your derivation:\n- Variance of a linear combination of independent, mean-zero random variables $Z_i$ with common variance $\\sigma^2$ is $\\mathrm{Var}\\left(\\sum_{i=1}^{n} a_i Z_i\\right) = \\sigma^2 \\sum_{i=1}^{n} a_i^2$.\n- For unweighted averaging with $n$ samples, equal weights are $a_i = 1/n$, yielding variance $\\sigma^2 / n$.\n\nUsing these foundations, do all of the following:\n1. Derive, from first principles, a closed-form expression for the Effective Sample Size (ESS) in terms of the weights $W_i$ and their normalized versions $w_i$. Do not assume any shortcut formula; show how ESS arises by equating the variance of the weighted estimator to that of an unweighted estimator.\n2. Starting from the derived expression, show how ESS can be computed using only the log-weights $\\ell_i$ by manipulating sums and products in the logarithmic domain. Your derivation must explicitly exhibit a transformation that avoids direct exponentiation of very large or very small numbers.\n3. Explain the numerical instability that arises if one naively exponentiates $\\ell_i$ to form $W_i$, including cases of overflow when $\\ell_i$ are large and underflow when $\\ell_i$ are very negative. Provide the precise numerically stable transformation that mitigates these issues using the logarithmic domain.\n4. Specify a robust convention for the degenerate case where all $\\ell_i = -\\infty$ (which implies all $W_i = 0$). Under this convention, define the ESS value your program must return.\n\nThen, implement a complete and runnable program that:\n- Consumes no input and uses only the specified test suite below.\n- For each test case, computes the ESS from the given list of log-weights using your numerically stable method that does not form $W_i$ explicitly and does not require normalization of $w_i$ in floating-point arithmetic.\n- Produces as output a single line containing a comma-separated list enclosed in square brackets with the ESS values for all test cases, rounded to $10$ decimal places. The output must be exactly in the format $[r_1,r_2,\\dots,r_m]$, where each $r_k$ is a decimal string with $10$ digits after the decimal point.\n\nNo physical units are involved; no angles are involved. All answers must be expressed as floating-point decimal numbers as specified above.\n\nTest suite (each case is a vector of log-weights):\n- Case 1 (uniform weights): $\\ell = [0.0, 0.0, 0.0, 0.0, 0.0]$.\n- Case 2 (extremely large positive log-weights with hierarchy): $\\ell = [1000.0, 999.0, 990.0, 900.0, 800.0]$.\n- Case 3 (extremely negative but equal log-weights): $\\ell = [-1000.0, -1000.0, -1000.0]$.\n- Case 4 (one dominant weight, others negligible): $\\ell = [0.0, -1000.0, -1000.0, -1000.0]$.\n- Case 5 (mixture of finite and negative infinity): $\\ell = [0.0, -\\infty, -\\infty, -10.0]$.\n- Case 6 (all negative infinity; degenerate): $\\ell = [-\\infty, -\\infty, -\\infty]$.\n- Case 7 (single sample): $\\ell = [123.456]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,\\dots,r_7]$), where each $r_k$ is the ESS for the corresponding case, rounded to $10$ decimal places.",
            "solution": "The problem of computing the Effective Sample Size (ESS) for a set of weighted samples from their log-weights is a standard task in computational statistics, particularly in the context of importance sampling and sequential Monte Carlo methods. The problem as stated is scientifically grounded, well-posed, objective, and complete. We proceed with the solution by first deriving the required expressions from fundamental principles and then designing a numerically stable algorithm.\n\n### Step 1: Derivation of the Effective Sample Size (ESS)\n\nWe begin from the foundational principles laid out in the problem statement. Consider a set of $n$ independent, identically distributed, mean-zero random variables $Z_i$, each with a common variance $\\mathrm{Var}(Z_i) = \\sigma^2$.\n\nA weighted estimator for the mean of the underlying distribution from which the $Z_i$ are sampled is given by $\\hat{\\mu} = \\sum_{i=1}^{n} w_i Z_i$, where $w_i$ are normalized weights such that $\\sum_{i=1}^{n} w_i = 1$. The variance of this estimator is derived using the provided formula for the variance of a linear combination of independent random variables, $\\mathrm{Var}\\left(\\sum_{i=1}^{n} a_i Z_i\\right) = \\sigma^2 \\sum_{i=1}^{n} a_i^2$. Setting the coefficients $a_i = w_i$, we get:\n$$ \\mathrm{Var}(\\hat{\\mu}) = \\sigma^2 \\sum_{i=1}^{n} w_i^2 $$\n\nFor a standard unweighted estimator with $N$ samples, the weights are uniform, $a_i = 1/N$, and its variance is $\\mathrm{Var}(\\hat{\\mu}_{\\text{unweighted}}) = \\sigma^2 \\sum_{i=1}^{N} (1/N)^2 = \\sigma^2 \\cdot N \\cdot (1/N^2) = \\sigma^2 / N$.\n\nThe Effective Sample Size (ESS) is defined as the number of samples $n_{eff}$ that an unweighted estimator would need to have the same variance as our weighted estimator. We denote $\\mathrm{ESS} = n_{eff}$. Therefore, we equate the two variances:\n$$ \\mathrm{Var}(\\hat{\\mu}) = \\frac{\\sigma^2}{\\mathrm{ESS}} $$\n$$ \\sigma^2 \\sum_{i=1}^{n} w_i^2 = \\frac{\\sigma^2}{\\mathrm{ESS}} $$\nSolving for ESS, we arrive at the first required expression in terms of the normalized weights $w_i$:\n$$ \\mathrm{ESS} = \\frac{1}{\\sum_{i=1}^{n} w_i^2} $$\n\nThe normalized weights $w_i$ are related to the unnormalized positive weights $W_i$ by the relation $w_i = W_i / S_W$, where $S_W = \\sum_{j=1}^{n} W_j$ is the sum of unnormalized weights. Substituting this into the denominator of the ESS expression gives:\n$$ \\sum_{i=1}^{n} w_i^2 = \\sum_{i=1}^{n} \\left( \\frac{W_i}{\\sum_{j=1}^{n} W_j} \\right)^2 = \\frac{\\sum_{i=1}^{n} W_i^2}{\\left(\\sum_{j=1}^{n} W_j\\right)^2} $$\nSubstituting this back into the expression for ESS yields the closed-form expression in terms of the unnormalized weights $W_i$:\n$$ \\mathrm{ESS} = \\frac{\\left(\\sum_{j=1}^{n} W_j\\right)^2}{\\sum_{i=1}^{n} W_i^2} $$\nThis completes the first part of the derivation.\n\n### Step 2  3: Numerically Stable Computation from Log-Weights\n\nWe are given the natural logarithms of the unnormalized weights, $\\ell_i = \\log W_i$, which implies $W_i = e^{\\ell_i}$. Substituting this into the derived ESS formula gives:\n$$ \\mathrm{ESS} = \\frac{\\left(\\sum_{i=1}^{n} e^{\\ell_i}\\right)^2}{\\sum_{i=1}^{n} e^{2\\ell_i}} $$\n\n**Numerical Instability:**\nA naive implementation would first compute the weights $W_i = e^{\\ell_i}$ and then use the formula. This approach is numerically unstable.\n1.  **Overflow:** If any $\\ell_i$ is large and positive (e.g., $\\ell_i  709.78$ for IEEE 754 double precision), the computation of $e^{\\ell_i}$ will result in overflow (infinity). This contaminates the sums, leading to an incorrect or undefined result like $\\infty/\\infty$.\n2.  **Underflow:** If any $\\ell_i$ is large and negative (e.g., $\\ell_i  -745$), $e^{\\ell_i}$ will underflow to $0$. While this may be acceptable for a single weight, if all weights are small, their sum might also underflow to $0$, leading to a division by zero. Furthermore, this leads to a catastrophic loss of relative precision if the weights have a large dynamic range.\n\n**Numerically Stable Transformation:**\nTo mitigate these issues, we employ a standard technique often called the \"log-sum-exp\" trick. Let $\\ell_{\\max} = \\max_{i \\in \\{1,\\dots,n\\}} \\ell_i$. We can factor out $e^{\\ell_{\\max}}$ from the numerator sum and $e^{2\\ell_{\\max}}$ from the denominator sum.\n\nLet $S_1 = \\sum_{i=1}^{n} W_i = \\sum_{i=1}^{n} e^{\\ell_i}$ and $S_2 = \\sum_{i=1}^{n} W_i^2 = \\sum_{i=1}^{n} e^{2\\ell_i}$.\nWe rewrite $S_1$ as:\n$$ S_1 = \\sum_{i=1}^{n} e^{\\ell_i - \\ell_{\\max}} e^{\\ell_{\\max}} = e^{\\ell_{\\max}} \\sum_{i=1}^{n} e^{\\ell_i - \\ell_{\\max}} $$\nAnd we rewrite $S_2$ as:\n$$ S_2 = \\sum_{i=1}^{n} e^{2\\ell_i - 2\\ell_{\\max}} e^{2\\ell_{\\max}} = e^{2\\ell_{\\max}} \\sum_{i=1}^{n} e^{2(\\ell_i - \\ell_{\\max})} $$\n\nNow, substituting these factored forms back into the ESS expression $\\mathrm{ESS} = S_1^2 / S_2$:\n$$ \\mathrm{ESS} = \\frac{\\left(e^{\\ell_{\\max}} \\sum_{i=1}^{n} e^{\\ell_i - \\ell_{\\max}}\\right)^2}{e^{2\\ell_{\\max}} \\sum_{i=1}^{n} e^{2(\\ell_i - \\ell_{\\max})}} = \\frac{e^{2\\ell_{\\max}} \\left(\\sum_{i=1}^{n} e^{\\ell_i - \\ell_{\\max}}\\right)^2}{e^{2\\ell_{\\max}} \\sum_{i=1}^{n} e^{2(\\ell_i - \\ell_{\\max})}} $$\nThe term $e^{2\\ell_{\\max}}$ cancels out algebraically, yielding the final, numerically stable formula:\n$$ \\mathrm{ESS} = \\frac{\\left(\\sum_{i=1}^{n} e^{\\ell_i - \\ell_{\\max}}\\right)^2}{\\sum_{i=1}^{n} e^{2(\\ell_i - \\ell_{\\max})}} $$\nThis expression is stable because the arguments of the exponentiation, $\\ell_i - \\ell_{\\max}$, are always less than or equal to $0$. This prevents overflow. The largest term in the sum is $e^0 = 1$, which preserves numerical precision for other terms that would have otherwise underflowed relative to a very large number. This transformation allows us to compute the ESS using only operations on the log-weights, without ever forming the potentially problematic $W_i$ or normalized $w_i$ values directly.\n\n### Step 4: Convention for the Degenerate Case\n\nThe degenerate case occurs when all log-weights are negative infinity, i.e., $\\ell_i = -\\infty$ for all $i \\in \\{1,\\dots,n\\}$. This corresponds to all unnormalized weights being zero, $W_i = 0$.\nIn this scenario, the sum of weights $\\sum W_i = 0$ and the sum of squared weights $\\sum W_i^2 = 0$. The ESS formula becomes $\\mathrm{ESS} = 0^2 / 0$, which is an indeterminate form.\n\nFrom a statistical perspective, a set of all-zero weights provides no information, as no sample contributes to the estimator. It is equivalent to having zero effective samples. Therefore, the most sensible and robust convention is to define the ESS as $0$ for this case.\n\nIn our numerically stable algorithm, this case corresponds to $\\ell_{\\max} = -\\infty$. We must explicitly check for this condition before proceeding with the main computation to avoid the indeterminate form $\\ell_i - \\ell_{\\max} = -\\infty - (-\\infty)$. If $\\ell_{\\max} = -\\infty$, the algorithm should return $0.0$. For cases where only some, but not all, $\\ell_i$ are $-\\infty$, the stable formula handles them correctly: if $\\ell_{\\max}$ is finite, then for an $\\ell_i = -\\infty$, the term $\\ell_i - \\ell_{\\max} = -\\infty$, and $e^{-\\infty}$ evaluates to $0$, correctly excluding that sample from the calculation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the effective sample size problem for a predefined test suite.\n    \"\"\"\n\n    def compute_ess(log_weights: np.ndarray) - float:\n        \"\"\"\n        Computes the Effective Sample Size (ESS) from a vector of log-weights\n        using a numerically stable algorithm.\n\n        Args:\n            log_weights: A 1D numpy array of unnormalized log-weights.\n\n        Returns:\n            The calculated ESS as a float.\n        \"\"\"\n        # Ensure input is a numpy array for vectorized operations\n        log_weights = np.asarray(log_weights, dtype=np.float64)\n\n        # Handle the trivial case of a single sample\n        if log_weights.size == 1:\n            return 1.0\n        \n        # Handle the case of an empty set of weights\n        if log_weights.size == 0:\n            return 0.0\n\n        # Find the maximum log-weight\n        max_log_weight = np.max(log_weights)\n\n        # Handle the degenerate case where all log-weights are -inf\n        # This implies all weights are 0, so ESS is 0.\n        if max_log_weight == -np.inf:\n            return 0.0\n\n        # Shift log-weights to prevent overflow/underflow\n        # This is the core of the log-sum-exp trick\n        shifted_log_weights = log_weights - max_log_weight\n        \n        # Calculate the terms for the sums in the stable ESS formula.\n        # Numerator sum's terms: exp(ell_i - ell_max)\n        # Denominator sum's terms: exp(2 * (ell_i - ell_max))\n        # This avoids direct calculation of W_i = exp(ell_i)\n        \n        # Sum of exp(l_i - l_max)\n        s1 = np.sum(np.exp(shifted_log_weights))\n        \n        # Sum of exp(2 * (l_i - l_max)) = sum of (exp(l_i - l_max))^2\n        s2 = np.sum(np.exp(2 * shifted_log_weights))\n        \n        # The terms exp(2 * l_max) from numerator and denominator cancel out\n        # ESS = (sum(W_i))^2 / sum(W_i^2)\n        #     = (exp(l_max) * sum(exp(l_i-l_max)))^2 / (exp(2*l_max) * sum(exp(2*(l_i-l_max))))\n        #     = (sum(exp(l_i-l_max)))^2 / sum(exp(2*(l_i-l_max)))\n        \n        # The denominator s2 cannot be zero if there is at least one finite log-weight,\n        # because the maximum shifted log-weight is 0, making its term exp(0)=1.\n        ess = s1**2 / s2\n        \n        return ess\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n        np.array([1000.0, 999.0, 990.0, 900.0, 800.0]),\n        np.array([-1000.0, -1000.0, -1000.0]),\n        np.array([0.0, -1000.0, -1000.0, -1000.0]),\n        np.array([0.0, -np.inf, -np.inf, -10.0]),\n        np.array([-np.inf, -np.inf, -np.inf]),\n        np.array([123.456])\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_ess(case)\n        # Format to 10 decimal places as specified\n        results.append(f\"{result:.10f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The true power of the Effective Sample Size lies not just in diagnosing the quality of a completed simulation, but in proactively designing more efficient algorithms. This practice demonstrates this principle in the context of Annealed Importance Sampling (AIS), a powerful technique for exploring complex distributions. You will derive a criterion based on the per-step ESS to determine the optimal number of stages in a tempering ladder, balancing computational cost against the statistical quality of the path from a simple distribution to a complex target .",
            "id": "3304985",
            "problem": "Consider two $d$-dimensional Gaussian distributions: a base distribution $p_0(x) = \\mathcal{N}(0, I_d)$ and a target distribution $p_1(x) = \\mathcal{N}(\\mu, I_d)$, where $I_d$ is the $d \\times d$ identity matrix and $\\mu \\in \\mathbb{R}^d$ is a fixed mean vector. Define the geometric path of intermediate distributions $\\{\\pi_\\beta\\}_{\\beta \\in [0,1]}$ by $\\pi_\\beta(x) \\propto p_0(x)^{1-\\beta} p_1(x)^\\beta$. In Annealed Importance Sampling (AIS), one constructs a tempering ladder $0 = \\beta_0  \\beta_1  \\cdots  \\beta_K = 1$ and, at each step $k$, computes incremental importance weights for particles $x_i \\sim \\pi_{\\beta_k}$ given by\n$$\nw_i^{(k)} = \\exp\\big((\\beta_{k+1} - \\beta_k) \\, \\Delta U(x_i)\\big),\n$$\nwhere $\\Delta U(x) = \\log p_1(x) - \\log p_0(x)$. For a weighted sample with unnormalized weights $\\{w_i\\}_{i=1}^N$, define the normalized weights $\\tilde{w}_i = w_i / \\sum_{j=1}^N w_j$ and the effective sample size (ESS) by\n$$\nN_{\\mathrm{eff}} = \\frac{1}{\\sum_{i=1}^N \\tilde{w}_i^2}.\n$$\nIn the large-sample regime for independent and identically distributed weights, a fundamental surrogate for the per-step effective sample size fraction is the ratio\n$$\nR = \\frac{\\big(\\mathbb{E}[w]\\big)^2}{\\mathbb{E}[w^2]},\n$$\nwhich is the asymptotic limit of $N_{\\mathrm{eff}}/N$ as $N \\to \\infty$ for fixed weight distribution.\n\nYour tasks are as follows.\n\n1) Starting from the definitions above and without assuming any result that directly gives the answer, derive the distribution of $\\Delta U(x)$ under $\\pi_\\beta$ for the Gaussian pair $(p_0, p_1)$ specified, and from it derive the distribution of the incremental importance weight $w = \\exp((\\beta' - \\beta)\\Delta U(x))$ for a single step from $\\beta$ to $\\beta'$. Then, using only first principles and properties of Gaussian and log-normal random variables, derive the exact expression of the asymptotic per-step effective sample size fraction $R(\\Delta \\beta)$ as a function of $\\Delta \\beta = \\beta' - \\beta$ and $\\|\\mu\\|_2$, where $\\|\\mu\\|_2$ denotes the Euclidean norm of $\\mu$.\n\n2) Fix a threshold $\\theta \\in (0,1)$ and a ladder $0 = \\beta_0  \\beta_1  \\cdots  \\beta_K = 1$. Impose the constraint that, for every adjacent pair $(\\beta_k, \\beta_{k+1})$, the asymptotic per-step effective sample size fraction satisfies $R(\\beta_{k+1}-\\beta_k) \\ge \\theta$. Show that, under these constraints and for the model specified, it is optimal to choose an equally spaced ladder, and determine the minimal integer number of increments $K_{\\min}$ that satisfies the constraint in terms of $\\|\\mu\\|_2$ and $\\theta$. Provide a clear justification from first principles, including any convexity or inequality arguments employed.\n\n3) Define the computational cost of an AIS pass with $N$ particles as $C = N K$, where $K$ is the number of incremental steps. For each parameter set in the test suite below, compute the pair $(K_{\\min}, C_{\\min})$ using your derived formula. All numeric answers must be integers.\n\nTest suite:\n- Case $1$: $d = 10$, $\\|\\mu\\|_2 = 3.0$, $\\theta = 0.5$, $N = 1000$.\n- Case $2$: $d = 5$, $\\|\\mu\\|_2 = 0.0$, $\\theta = 0.9$, $N = 500$.\n- Case $3$: $d = 50$, $\\|\\mu\\|_2 = 5.0$, $\\theta = 0.95$, $N = 100$.\n- Case $4$: $d = 100$, $\\|\\mu\\|_2 = 20.0$, $\\theta = 0.2$, $N = 200$.\n- Case $5$: $d = 3$, $\\|\\mu\\|_2 = 1.0$, $\\theta = 0.999$, $N = 1000$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a two-element list $[K_{\\min}, C_{\\min}]$ corresponding to a test case in the same order as above. For example, an output with two hypothetical cases would look like $[[1,100],[4,400]]$. No physical units are involved in this problem, and all numeric outputs must be integers.",
            "solution": "The problem requires a multi-step derivation concerning Annealed Importance Sampling (AIS) for a specific pair of Gaussian distributions, followed by an analysis of the optimal number of annealing steps and application to a test suite. The solution is presented in three corresponding parts.\n\nPart $1$: Derivation of the asymptotic per-step effective sample size fraction $R(\\Delta \\beta)$.\n\nThe derivation begins by characterizing the intermediate distribution $\\pi_\\beta(x) \\propto p_0(x)^{1-\\beta} p_1(x)^\\beta$. The base distribution is $p_0(x) = \\mathcal{N}(x | 0, I_d)$ and the target distribution is $p_1(x) = \\mathcal{N}(x | \\mu, I_d)$. Their log-probability densities are, up to a normalization constant,\n$$\n\\log p_0(x) = -\\frac{1}{2} x^T x + \\text{const.}\n$$\n$$\n\\log p_1(x) = -\\frac{1}{2} (x-\\mu)^T (x-\\mu) + \\text{const.}\n$$\nThe log-density of the intermediate distribution $\\pi_\\beta(x)$ is therefore:\n$$\n\\log \\pi_\\beta(x) = (1-\\beta) \\log p_0(x) + \\beta \\log p_1(x) + \\text{const.}\n$$\n$$\n= -\\frac{1-\\beta}{2} x^T x - \\frac{\\beta}{2} (x-\\mu)^T (x-\\mu) + \\text{const.}\n$$\n$$\n= -\\frac{1-\\beta}{2} x^T x - \\frac{\\beta}{2} (x^T x - 2x^T\\mu + \\mu^T\\mu) + \\text{const.}\n$$\n$$\n= -\\frac{1}{2} x^T x + \\beta x^T\\mu - \\frac{\\beta}{2} \\mu^T\\mu + \\text{const.}\n$$\nTo identify this as a Gaussian distribution, we complete the square for the terms involving $x$:\n$$\n-\\frac{1}{2} (x^T x - 2\\beta x^T\\mu) - \\frac{\\beta}{2} \\mu^T\\mu = -\\frac{1}{2} (x - \\beta\\mu)^T (x - \\beta\\mu) + \\frac{1}{2} (\\beta\\mu)^T(\\beta\\mu) - \\frac{\\beta}{2} \\mu^T\\mu\n$$\nThe terms not involving $x$ are absorbed into the normalization constant. The quadratic form in the exponent, $-\\frac{1}{2} (x - \\beta\\mu)^T (x - \\beta\\mu)$, reveals that $\\pi_\\beta(x)$ is a Gaussian distribution with mean $\\beta\\mu$ and covariance matrix $I_d$. Thus, $\\pi_\\beta(x) = \\mathcal{N}(x | \\beta\\mu, I_d)$.\n\nNext, we determine the distribution of the log-likelihood difference, $\\Delta U(x) = \\log p_1(x) - \\log p_0(x)$, for a sample $x \\sim \\pi_\\beta(x)$.\n$$\n\\Delta U(x) = \\left(-\\frac{1}{2} (x-\\mu)^T (x-\\mu)\\right) - \\left(-\\frac{1}{2} x^T x\\right) = -\\frac{1}{2} (x^T x - 2x^T\\mu + \\mu^T\\mu) + \\frac{1}{2} x^T x = x^T\\mu - \\frac{1}{2} \\mu^T\\mu\n$$\nSince $x \\sim \\mathcal{N}(\\beta\\mu, I_d)$, $\\Delta U(x)$ is an affine transformation of a Gaussian random vector, which means it is a scalar Gaussian random variable. Its mean is:\n$$\n\\mathbb{E}_{\\pi_\\beta}[\\Delta U(x)] = \\mathbb{E}[x^T\\mu] - \\frac{1}{2} \\|\\mu\\|_2^2 = (\\mathbb{E}[x])^T\\mu - \\frac{1}{2} \\|\\mu\\|_2^2 = (\\beta\\mu)^T\\mu - \\frac{1}{2} \\|\\mu\\|_2^2 = \\left(\\beta - \\frac{1}{2}\\right) \\|\\mu\\|_2^2\n$$\nIts variance is:\n$$\n\\text{Var}_{\\pi_\\beta}(\\Delta U(x)) = \\text{Var}(x^T\\mu) = \\mu^T \\text{Cov}(x) \\mu = \\mu^T I_d \\mu = \\|\\mu\\|_2^2\n$$\nSo, for $x \\sim \\pi_\\beta(x)$, we have established that $\\Delta U(x) \\sim \\mathcal{N}\\left( (\\beta - \\frac{1}{2})\\|\\mu\\|_2^2, \\|\\mu\\|_2^2 \\right)$.\n\nThe incremental weight is $w = \\exp(\\Delta\\beta \\cdot \\Delta U(x))$, where $\\Delta\\beta = \\beta_{k+1} - \\beta_k$. The logarithm of the weight, $\\log w = \\Delta\\beta \\cdot \\Delta U(x)$, is also normally distributed:\n$$\n\\log w \\sim \\mathcal{N}\\left( \\Delta\\beta\\left(\\beta - \\frac{1}{2}\\right)\\|\\mu\\|_2^2, (\\Delta\\beta)^2\\|\\mu\\|_2^2 \\right)\n$$\nThus, $w$ follows a log-normal distribution. If a random variable $Y$ follows a log-normal distribution such that $\\log Y \\sim \\mathcal{N}(m, s^2)$, its moments are given by $\\mathbb{E}[Y^k] = \\exp(km + \\frac{1}{2}k^2s^2)$. In our case, $m = \\Delta\\beta(\\beta - \\frac{1}{2})\\|\\mu\\|_2^2$ and $s^2 = (\\Delta\\beta)^2\\|\\mu\\|_2^2$.\n\nFinally, we compute the asymptotic ESS fraction $R(\\Delta\\beta) = (\\mathbb{E}[w])^2 / \\mathbb{E}[w^2]$.\nThe first raw moment ($k=1$) is $\\mathbb{E}[w] = \\exp(m + \\frac{1}{2}s^2)$.\nThe second raw moment ($k=2$) is $\\mathbb{E}[w^2] = \\exp(2m + \\frac{1}{2}(2^2)s^2) = \\exp(2m + 2s^2)$.\nThe ratio $R$ is therefore:\n$$\nR(\\Delta\\beta) = \\frac{\\left(\\exp(m + \\frac{1}{2}s^2)\\right)^2}{\\exp(2m + 2s^2)} = \\frac{\\exp(2m + s^2)}{\\exp(2m + 2s^2)} = \\exp(s^2 - 2s^2) = \\exp(-s^2)\n$$\nSubstituting the expression for $s^2$, we arrive at the exact expression:\n$$\nR(\\Delta\\beta) = \\exp\\left(-(\\Delta\\beta)^2 \\|\\mu\\|_2^2\\right)\n$$\nThis expression notably depends only on the step size $\\Delta\\beta$ and the squared norm of the mean shift vector $\\mu$, but not on the sampling temperature $\\beta$ or the dimensionality $d$.\n\nPart $2$: Determination of the minimal number of increments $K_{\\min}$.\n\nThe problem imposes the constraint $R(\\beta_{k+1}-\\beta_k) \\ge \\theta$ for all adjacent steps $(\\beta_k, \\beta_{k+1})$ in the ladder, where $\\theta \\in (0,1)$. Let $\\Delta\\beta_k = \\beta_{k+1}-\\beta_k$. The constraint becomes:\n$$\n\\exp(-(\\Delta\\beta_k)^2 \\|\\mu\\|_2^2) \\ge \\theta\n$$\nTaking the natural logarithm of both sides, which is a strictly increasing function:\n$$\n-(\\Delta\\beta_k)^2 \\|\\mu\\|_2^2 \\ge \\log \\theta \\implies (\\Delta\\beta_k)^2 \\le -\\frac{\\log \\theta}{\\|\\mu\\|_2^2}\n$$\nSince $\\theta \\in (0,1)$, $\\log\\theta  0$, so the right-hand side is positive. This implies an upper bound on the step size:\n$$\n\\Delta\\beta_k \\le \\frac{\\sqrt{-\\log \\theta}}{\\|\\mu\\|_2} \\equiv \\Delta\\beta_{\\max}\n$$\nThis derivation assumes $\\|\\mu\\|_2  0$. We seek to minimize the total number of steps $K$ required to traverse the interval $[0,1]$, subject to $\\sum_{k=0}^{K-1} \\Delta\\beta_k = 1$ and $\\Delta\\beta_k \\le \\Delta\\beta_{\\max}$ for all $k$. To minimize $K$, the average step size $1/K$ must be maximized, which implies that each individual step $\\Delta\\beta_k$ should be as large as possible. The condition $\\sum_{k=0}^{K-1} \\Delta\\beta_k = 1$ combined with the uniform upper bound $\\Delta\\beta_k \\le \\Delta\\beta_{\\max}$ gives:\n$$\n1 = \\sum_{k=0}^{K-1} \\Delta\\beta_k \\le \\sum_{k=0}^{K-1} \\Delta\\beta_{\\max} = K \\cdot \\Delta\\beta_{\\max}\n$$\nThis yields a lower bound on $K$: $K \\ge 1 / \\Delta\\beta_{\\max}$. Since $K$ must be an integer, the minimal number of steps is $K_{\\min} = \\lceil 1 / \\Delta\\beta_{\\max} \\rceil$. This minimal value is achieved with an equally spaced ladder, where $\\Delta\\beta_k = 1/K_{\\min}$ for all $k$. This choice satisfies the constraint because $1/K_{\\min} \\le \\Delta\\beta_{\\max}$. Thus, an equally spaced ladder is optimal.\nThe explicit formula for $K_{\\min}$ is:\n$$\nK_{\\min} = \\left\\lceil \\frac{\\|\\mu\\|_2}{\\sqrt{-\\log \\theta}} \\right\\rceil\n$$\nIn the special case $\\|\\mu\\|_2 = 0$, $p_0$ and $p_1$ are identical. Thus $\\Delta U(x) = 0$, the weight $w=1$, and $R=1$. The constraint $1 \\ge \\theta$ is always satisfied, so a single step from $\\beta=0$ to $\\beta=1$ is sufficient, yielding $K_{\\min} = 1$. The formula above would give $\\lceil 0 \\rceil = 0$, which is not a valid number of steps. Hence, we must handle this as a special case.\n\nPart $3$: Calculation for the test suite.\n\nUsing the derived formula, the minimal number of steps is $K_{\\min} = 1$ if $\\|\\mu\\|_2=0$, and $K_{\\min} = \\lceil \\|\\mu\\|_2 / \\sqrt{-\\log \\theta} \\rceil$ if $\\|\\mu\\|_20$. The corresponding minimal computational cost is $C_{\\min} = N \\cdot K_{\\min}$. The provided Python code implements these calculations for the given test cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Calculates the minimal number of AIS steps and the corresponding computational cost\n    for a series of test cases based on a derived formula.\n    \"\"\"\n    # Test suite: each element is a tuple of (d, ||mu||_2, theta, N)\n    # The dimension d is not used in the final formula but is kept for consistency.\n    test_cases = [\n        # Case 1: d = 10, ||mu||_2 = 3.0, theta = 0.5, N = 1000\n        (10, 3.0, 0.5, 1000),\n        # Case 2: d = 5, ||mu||_2 = 0.0, theta = 0.9, N = 500\n        (5, 0.0, 0.9, 500),\n        # Case 3: d = 50, ||mu||_2 = 5.0, theta = 0.95, N = 100\n        (50, 5.0, 0.95, 100),\n        # Case 4: d = 100, ||mu||_2 = 20.0, theta = 0.2, N = 200\n        (100, 20.0, 0.2, 200),\n        # Case 5: d = 3, ||mu||_2 = 1.0, theta = 0.999, N = 1000\n        (3, 1.0, 0.999, 1000),\n    ]\n\n    results = []\n    for case in test_cases:\n        _d, mu_norm, theta, N = case\n\n        if mu_norm == 0.0:\n            # If ||mu||_2 is 0, p0 and p1 are identical. The ESS fraction R is always 1.\n            # Thus, a single step is sufficient.\n            k_min = 1\n        else:\n            # For ||mu||_2  0, the minimal number of steps K_min is given by\n            # ceil(||mu||_2 / sqrt(-log(theta))).\n            # We use numpy for log and sqrt, and math.ceil to get an integer result.\n            argument = mu_norm / np.sqrt(-np.log(theta))\n            k_min = math.ceil(argument)\n\n        # The minimal cost C_min is N * K_min.\n        c_min = N * k_min\n        \n        results.append([k_min, c_min])\n\n    # Format the output as a string representation of a list of lists.\n    # e.g., [[K1, C1], [K2, C2]]\n    # Each inner list is converted to a string, then they are joined by commas,\n    # and finally enclosed in square brackets.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    \n    # Python's default string representation of a list includes spaces.\n    # The example output [[1,100],[4,400]] has no spaces.\n    # We remove spaces to match the dense format.\n    print(output_str.replace(\" \", \"\"))\n\nsolve()\n```"
        }
    ]
}