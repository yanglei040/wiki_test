{
    "hands_on_practices": [
        {
            "introduction": "A primary challenge in importance sampling is the choice of the proposal distribution $q$. A poor choice can lead to an estimator with infinite variance, rendering it practically useless even if it is formally unbiased. This exercise demonstrates this critical failure mode by constructing a scenario where the proposal distribution has \"lighter tails\" than the target, a common pitfall in practice . By working through this foundational example, you will gain a concrete understanding of why the condition for finite variance, namely that $\\mathbb{E}_{q}[w(X)^2 f(X)^2] \\lt \\infty$, is a vital practical requirement and not just a theoretical footnote.",
            "id": "3241960",
            "problem": "Consider the task of estimating the target expectation $I = \\mathbb{E}_{\\pi}[f(X)]$ using importance sampling (IS), where the weight function is $w(x) = \\frac{\\pi(x)}{q(x)}$ and samples are drawn from a proposal distribution $q(x)$. Start from the fundamental definitions of expectation and variance under a probability density function (PDF): for any measurable function $g$, $\\mathbb{E}_{p}[g(X)] = \\int_{\\mathbb{R}} g(x)\\,p(x)\\,dx$, and $\\operatorname{Var}_{p}(Y) = \\mathbb{E}_{p}[Y^{2}] - \\left(\\mathbb{E}_{p}[Y]\\right)^{2}$. In importance sampling, the estimator of $I$ based on samples from $q$ is the average of $w(X)f(X)$, and its variance under $q$ is $\\operatorname{Var}_{q}\\!\\left[w(X)f(X)\\right]$.\n\nConstruct a one-dimensional example in which the proposal $q$ under-covers the tails of the target $\\pi$, causing the variance $\\operatorname{Var}_{q}\\!\\left[w(X)f(X)\\right]$ to be infinite even though the target expectation $I$ is finite. Specifically, take the target PDF to be the standard Cauchy distribution\n$$\n\\pi(x) = \\frac{1}{\\pi\\,(1+x^{2})}, \\quad x \\in \\mathbb{R},\n$$\nthe proposal PDF to be the standard normal distribution\n$$\nq(x) = \\frac{1}{\\sqrt{2\\pi}}\\,\\exp\\!\\left(-\\frac{x^{2}}{2}\\right), \\quad x \\in \\mathbb{R},\n$$\nand the integrand to be the bounded function\n$$\nf(x) = 1, \\quad x \\in \\mathbb{R}.\n$$\nUsing only the core definitions above, perform the following:\n\n1. Compute the target expectation $I = \\int_{\\mathbb{R}} f(x)\\,\\pi(x)\\,dx$ exactly.\n2. Derive an explicit integral expression for the second moment $\\mathbb{E}_{q}\\!\\left[(w(X)f(X))^{2}\\right]$ and show, by a tail comparison argument grounded in the given PDFs, that this integral diverges to $+\\infty$. Conclude that $\\operatorname{Var}_{q}\\!\\left[w(X)f(X)\\right]$ is infinite while $I$ is finite.\n\nState your final numeric answer for $I$ exactly. No rounding is required and no units apply.",
            "solution": "The problem presents a well-posed question in the domain of numerical methods, specifically importance sampling. All provided functions—the target distribution $\\pi(x)$, the proposal distribution $q(x)$, and the integrand $f(x)$—are standard, well-defined mathematical objects. The problem is scientifically grounded and self-contained, allowing for a rigorous analysis. We proceed with the two required tasks.\n\nThe problem asks us to investigate an importance sampling scenario where the target expectation is finite but the variance of the estimator is infinite. The components are defined as follows:\nTarget probability density function (PDF): Standard Cauchy distribution, $\\pi(x) = \\frac{1}{\\pi(1+x^2)}$.\nProposal PDF: Standard normal distribution, $q(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-\\frac{x^2}{2})$.\nIntegrand function: $f(x) = 1$.\nThe goal is to estimate the target expectation $I = \\mathbb{E}_{\\pi}[f(X)]$. The importance sampling weight is $w(x) = \\frac{\\pi(x)}{q(x)}$.\n\n**1. Computation of the Target Expectation $I$**\n\nThe target expectation is defined as $I = \\int_{\\mathbb{R}} f(x)\\pi(x)dx$. Substituting the given expressions for $f(x)$ and $\\pi(x)$:\n$$\nI = \\int_{-\\infty}^{\\infty} 1 \\cdot \\frac{1}{\\pi(1+x^2)} dx = \\frac{1}{\\pi} \\int_{-\\infty}^{\\infty} \\frac{1}{1+x^2} dx\n$$\nThe integral of $\\pi(x)$ over its support $\\mathbb{R}$ must equal $1$ by the definition of a probability density function. We can verify this directly. The antiderivative of $\\frac{1}{1+x^2}$ is $\\arctan(x)$. Therefore, we can evaluate the definite integral:\n$$\nI = \\frac{1}{\\pi} \\left[ \\arctan(x) \\right]_{-\\infty}^{\\infty} = \\frac{1}{\\pi} \\left( \\lim_{b \\to \\infty} \\arctan(b) - \\lim_{a \\to -\\infty} \\arctan(a) \\right)\n$$\nUsing the known limits of the arctangent function, $\\lim_{b \\to \\infty} \\arctan(b) = \\frac{\\pi}{2}$ and $\\lim_{a \\to -\\infty} \\arctan(a) = -\\frac{\\pi}{2}$, we obtain:\n$$\nI = \\frac{1}{\\pi} \\left( \\frac{\\pi}{2} - \\left(-\\frac{\\pi}{2}\\right) \\right) = \\frac{1}{\\pi} \\left( \\frac{\\pi}{2} + \\frac{\\pi}{2} \\right) = \\frac{1}{\\pi}(\\pi) = 1\n$$\nThe target expectation $I$ is finite and equal to $1$.\n\n**2. Analysis of the Importance Sampling Estimator Variance**\n\nThe variance of the importance sampling estimator for $I$ is given by $\\operatorname{Var}_{q}[w(X)f(X)]$. Using the definition of variance, $\\operatorname{Var}_{p}(Y) = \\mathbb{E}_{p}[Y^2] - (\\mathbb{E}_{p}[Y])^2$, we have:\n$$\n\\operatorname{Var}_{q}[w(X)f(X)] = \\mathbb{E}_{q}\\left[(w(X)f(X))^2\\right] - \\left( \\mathbb{E}_{q}[w(X)f(X)] \\right)^2\n$$\nFirst, let's compute the expectation of the estimator, $\\mathbb{E}_{q}[w(X)f(X)]$:\n$$\n\\mathbb{E}_{q}[w(X)f(X)] = \\int_{-\\infty}^{\\infty} w(x)f(x)q(x)dx = \\int_{-\\infty}^{\\infty} \\frac{\\pi(x)}{q(x)} \\cdot 1 \\cdot q(x)dx = \\int_{-\\infty}^{\\infty} \\pi(x)dx = I = 1\n$$\nThis confirms that the importance sampling estimator is unbiased for $I$, and so $(\\mathbb{E}_{q}[w(X)f(X)])^2 = 1^2 = 1$. The variance is thus infinite if and only if the second moment, $\\mathbb{E}_{q}\\left[(w(X)f(X))^2\\right]$, is infinite.\n\nNext, we derive the explicit integral for the second moment:\n$$\n\\mathbb{E}_{q}\\left[(w(X)f(X))^2\\right] = \\int_{-\\infty}^{\\infty} (w(x)f(x))^2 q(x) dx\n$$\nSubstituting $f(x)=1$ and $w(x) = \\frac{\\pi(x)}{q(x)}$:\n$$\n\\mathbb{E}_{q}\\left[(w(X)f(X))^2\\right] = \\int_{-\\infty}^{\\infty} \\left(\\frac{\\pi(x)}{q(x)}\\right)^2 q(x) dx = \\int_{-\\infty}^{\\infty} \\frac{\\pi(x)^2}{q(x)} dx\n$$\nNow, we substitute the expressions for the PDFs $\\pi(x)$ and $q(x)$:\n$$\n\\frac{\\pi(x)^2}{q(x)} = \\frac{\\left(\\frac{1}{\\pi(1+x^2)}\\right)^2}{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right)} = \\frac{\\frac{1}{\\pi^2(1+x^2)^2}}{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right)} = \\frac{\\sqrt{2\\pi}}{\\pi^2} \\frac{\\exp\\left(\\frac{x^2}{2}\\right)}{(1+x^2)^2}\n$$\nThe second moment is the integral of this expression:\n$$\n\\mathbb{E}_{q}\\left[(w(X)f(X))^2\\right] = \\frac{\\sqrt{2\\pi}}{\\pi^2} \\int_{-\\infty}^{\\infty} \\frac{\\exp\\left(\\frac{x^2}{2}\\right)}{(1+x^2)^2} dx\n$$\nTo show that this integral diverges, we perform a tail comparison argument. The integrand is a positive function. A necessary condition for the convergence of an improper integral $\\int_{-\\infty}^{\\infty} g(x) dx$ is that $\\lim_{|x| \\to \\infty} g(x) = 0$. Let's examine this limit for our integrand:\n$$\n\\lim_{|x| \\to \\infty} \\frac{\\exp\\left(\\frac{x^2}{2}\\right)}{(1+x^2)^2}\n$$\nThe numerator, $\\exp\\left(\\frac{x^2}{2}\\right)$, exhibits exponential growth. The denominator, $(1+x^2)^2$, exhibits polynomial growth (asymptotic to $x^4$). It is a fundamental result of calculus that exponential growth dominates any polynomial growth. Therefore:\n$$\n\\lim_{|x| \\to \\infty} \\frac{\\exp\\left(\\frac{x^2}{2}\\right)}{(1+x^2)^2} = +\\infty\n$$\nSince the limit of the integrand as $|x| \\to \\infty$ is not $0$, the necessary condition for convergence is not met. Because the integrand is strictly positive, the integral must diverge to $+\\infty$.\n$$\n\\mathbb{E}_{q}\\left[(w(X)f(X))^2\\right] = +\\infty\n$$\nThis occurs because the proposal distribution $q(x)$ (standard normal) has tails that decay to zero much faster ($\\exp(-x^2/2)$) than the tails of the target distribution $\\pi(x)$ (Cauchy, which decays as $1/x^2$). This is a classic example of under-covering the tails. The weight function $w(x)$ becomes extremely large in the tails, causing the variance to explode.\n\nFinally, we conclude for the variance:\n$$\n\\operatorname{Var}_{q}[w(X)f(X)] = \\mathbb{E}_{q}\\left[(w(X)f(X))^2\\right] - \\left( \\mathbb{E}_{q}[w(X)f(X)] \\right)^2 = +\\infty - 1 = +\\infty\n$$\nWe have successfully demonstrated an example where the target expectation $I$ is finite ($I=1$), yet the variance of the importance sampling estimator is infinite.",
            "answer": "$$\\boxed{1}$$"
        },
        {
            "introduction": "Once the condition for finite variance is met, the next goal is to actively minimize this variance by selecting the best possible proposal distribution from a given family. This practice guides you through this optimization process, a common task in the practical application of importance sampling methods . By deriving the variance of the estimator as an explicit function of a proposal parameter $\\theta$ and using calculus to find the optimal value, you will see how analytical tools can inform the design of efficient estimators and build intuition for what makes a proposal distribution effective.",
            "id": "3360244",
            "problem": "Let $p(x)$ be the probability density function of the standard normal distribution $\\mathcal{N}(0,1)$ on $\\mathbb{R}$. Consider the task of estimating the target expectation $I = \\mathbb{E}_{p}[f(X)]$ for $f(x)=x$ using importance sampling (IS) based on a Gaussian proposal family $\\{q_{\\theta}: \\theta \\in \\mathbb{R}\\}$, where $q_{\\theta}(x)$ is the density of $\\mathcal{N}(\\theta,1)$. The importance weight is defined by $w_{\\theta}(x) = p(x)/q_{\\theta}(x)$, and the single-draw IS summand is $Y_{\\theta} = w_{\\theta}(X) f(X)$ with $X \\sim q_{\\theta}$. The standard Monte Carlo (MC) IS estimator with $n$ independent and identically distributed draws is $\\widehat{I}_{n}(\\theta) = \\frac{1}{n} \\sum_{i=1}^{n} Y_{\\theta,i}$.\n\nStarting from the definitions of importance sampling and variance, and using only well-tested formulas for the Gaussian exponential family (e.g., the moment generating function of a normal random variable), derive the closed-form expression for the variance\n$$\nV(\\theta) \\equiv \\operatorname{Var}_{q_{\\theta}}\\!\\left(Y_{\\theta}\\right),\n$$\nas an explicit function of the parameter $\\theta$. Then, determine all stationary points of the function $\\theta \\mapsto V(\\theta)$ and classify them.\n\nYour final answer must be given as a two-entry row vector using the $\\LaTeX$ $\\pmatrix$ environment, where the first entry is the closed-form expression for $V(\\theta)$ and the second entry is the collection of stationary points. No numerical approximation is required for any part of the answer, and no units are involved. If there are multiple stationary points, list them all in the second entry separated by commas inside the same row vector.",
            "solution": "The objective is to derive the closed-form expression for the variance $V(\\theta) = \\operatorname{Var}_{q_{\\theta}}(Y_{\\theta})$ and to find and classify the stationary points of $V(\\theta)$. The variance is defined as $V(\\theta) = \\mathbb{E}_{q_{\\theta}}[Y_{\\theta}^2] - (\\mathbb{E}_{q_{\\theta}}[Y_{\\theta}])^2$.\n\nFirst, we calculate the expected value of the importance sampling summand, $\\mathbb{E}_{q_{\\theta}}[Y_{\\theta}]$.\nThe summand is $Y_{\\theta} = w_{\\theta}(X) f(X)$, where $X \\sim q_{\\theta}$. The importance weight is $w_{\\theta}(x) = p(x)/q_{\\theta}(x)$.\nThe expectation is:\n$$\n\\mathbb{E}_{q_{\\theta}}[Y_{\\theta}] = \\mathbb{E}_{q_{\\theta}}\\left[\\frac{p(X)}{q_{\\theta}(X)} f(X)\\right] = \\int_{-\\infty}^{\\infty} \\frac{p(x)}{q_{\\theta}(x)} f(x) q_{\\theta}(x) \\,dx = \\int_{-\\infty}^{\\infty} f(x) p(x) \\,dx = \\mathbb{E}_{p}[f(X)]\n$$\nThis is the target quantity $I$. In this problem, $p(x)$ is the probability density function (PDF) of the standard normal distribution $\\mathcal{N}(0,1)$, and $f(x) = x$.\nTherefore, the expectation is the mean of the standard normal distribution:\n$$\n\\mathbb{E}_{q_{\\theta}}[Y_{\\theta}] = I = \\mathbb{E}_{p}[X] = 0\n$$\nSince the mean of $Y_{\\theta}$ is $0$, the variance simplifies to the second moment:\n$$\nV(\\theta) = \\mathbb{E}_{q_{\\theta}}[Y_{\\theta}^2] - (0)^2 = \\mathbb{E}_{q_{\\theta}}[Y_{\\theta}^2]\n$$\n\nNext, we derive the expression for this second moment. The squared summand is $Y_{\\theta}^2 = (w_{\\theta}(X) f(X))^2$.\nWe first find the explicit form of the weight $w_{\\theta}(x)$. The PDF for $p(x)$ is $\\frac{1}{\\sqrt{2\\pi}}\\exp(-x^2/2)$ and for $q_{\\theta}(x)$ is $\\frac{1}{\\sqrt{2\\pi}}\\exp(-(x-\\theta)^2/2)$.\n$$\nw_{\\theta}(x) = \\frac{p(x)}{q_{\\theta}(x)} = \\frac{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{x^2}{2}\\right)}{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{(x-\\theta)^2}{2}\\right)} = \\exp\\left(-\\frac{x^2}{2} + \\frac{(x-\\theta)^2}{2}\\right)\n$$\nThe exponent simplifies to:\n$$\n-\\frac{x^2}{2} + \\frac{x^2 - 2\\theta x + \\theta^2}{2} = \\frac{-x^2 + x^2 - 2\\theta x + \\theta^2}{2} = -\\theta x + \\frac{\\theta^2}{2}\n$$\nSo, the weight is $w_{\\theta}(x) = \\exp(-\\theta x + \\theta^2/2)$.\nThe summand is $Y_{\\theta} = f(X) w_{\\theta}(X) = X \\exp(-\\theta X + \\theta^2/2)$, where $X \\sim q_{\\theta}$, i.e., $X \\sim \\mathcal{N}(\\theta, 1)$.\nThe squared summand is $Y_{\\theta}^2 = X^2 \\exp(-2\\theta X + \\theta^2)$.\nNow we compute its expectation under $q_{\\theta}$:\n$$\nV(\\theta) = \\mathbb{E}_{q_{\\theta}}[Y_{\\theta}^2] = \\mathbb{E}_{q_{\\theta}}\\left[X^2 \\exp(-2\\theta X + \\theta^2)\\right] = \\exp(\\theta^2) \\mathbb{E}_{q_{\\theta}}\\left[X^2 \\exp(-2\\theta X)\\right]\n$$\nTo evaluate the expectation, we use the moment generating function (MGF) of $X \\sim \\mathcal{N}(\\theta, 1)$. The MGF of a general normal random variable $Z \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is $M_Z(t) = \\exp(\\mu t + \\sigma^2 t^2/2)$. For $X \\sim \\mathcal{N}(\\theta, 1)$, we have $\\mu = \\theta$ and $\\sigma^2 = 1$, so its MGF is:\n$$\nM_X(t) = \\mathbb{E}_{q_{\\theta}}[\\exp(tX)] = \\exp(\\theta t + t^2/2)\n$$\nWe use the property that $\\mathbb{E}_{q_{\\theta}}[X^2 \\exp(tX)] = \\frac{d^2}{dt^2} M_X(t)$.\nFirst derivative:\n$$\n\\frac{d}{dt} M_X(t) = \\frac{d}{dt} \\exp(\\theta t + t^2/2) = (\\theta + t) \\exp(\\theta t + t^2/2)\n$$\nSecond derivative (using the product rule):\n$$\n\\frac{d^2}{dt^2} M_X(t) = \\frac{d}{dt} \\left[(\\theta + t) \\exp(\\theta t + t^2/2)\\right] = (1)\\exp(\\theta t + t^2/2) + (\\theta + t)(\\theta + t)\\exp(\\theta t + t^2/2) = \\left[1 + (\\theta + t)^2\\right] \\exp(\\theta t + t^2/2)\n$$\nWe need to evaluate $\\mathbb{E}_{q_{\\theta}}[X^2 \\exp(-2\\theta X)]$, which corresponds to setting $t = -2\\theta$ in the expression for the second derivative of the MGF:\n\\begin{align*}\n\\mathbb{E}_{q_{\\theta}}\\left[X^2 \\exp(-2\\theta X)\\right] &= \\left[1 + (\\theta + (-2\\theta))^2\\right] \\exp\\left(\\theta(-2\\theta) + \\frac{(-2\\theta)^2}{2}\\right) \\\\\n&= \\left[1 + (-\\theta)^2\\right] \\exp\\left(-2\\theta^2 + \\frac{4\\theta^2}{2}\\right) \\\\\n&= (1 + \\theta^2) \\exp(-2\\theta^2 + 2\\theta^2) \\\\\n&= (1 + \\theta^2) \\exp(0) = 1 + \\theta^2\n\\end{align*}\nSubstituting this back into the expression for $V(\\theta)$:\n$$\nV(\\theta) = \\exp(\\theta^2) \\cdot (1 + \\theta^2)\n$$\nSo, the closed-form expression for the variance is $V(\\theta) = (1 + \\theta^2)\\exp(\\theta^2)$.\n\nNext, we find and classify the stationary points of $V(\\theta)$. Stationary points occur where the first derivative, $V'(\\theta) = \\frac{dV}{d\\theta}$, is zero. We use the product rule to differentiate $V(\\theta)$:\n$$\nV'(\\theta) = \\frac{d}{d\\theta}\\left[(1 + \\theta^2)\\exp(\\theta^2)\\right] = \\left(\\frac{d}{d\\theta}(1+\\theta^2)\\right)\\exp(\\theta^2) + (1+\\theta^2)\\left(\\frac{d}{d\\theta}\\exp(\\theta^2)\\right)\n$$\n$$\nV'(\\theta) = (2\\theta)\\exp(\\theta^2) + (1+\\theta^2)(2\\theta\\exp(\\theta^2)) = 2\\theta\\exp(\\theta^2) [1 + (1+\\theta^2)] = 2\\theta(2+\\theta^2)\\exp(\\theta^2)\n$$\nTo find the stationary points, we set $V'(\\theta) = 0$:\n$$\n2\\theta(2+\\theta^2)\\exp(\\theta^2) = 0\n$$\nFor any real $\\theta$, $\\exp(\\theta^2) > 0$ and $2+\\theta^2 \\ge 2 > 0$. Therefore, the only way for the product to be zero is if $2\\theta = 0$, which implies $\\theta = 0$.\nThere is a single stationary point at $\\theta = 0$.\n\nTo classify this stationary point, we use the second derivative test. We compute $V''(\\theta) = \\frac{d^2V}{d\\theta^2}$.\nLet's differentiate $V'(\\theta) = (4\\theta+2\\theta^3)\\exp(\\theta^2)$:\n$$\nV''(\\theta) = \\frac{d}{d\\theta}\\left[(4\\theta+2\\theta^3)\\exp(\\theta^2)\\right] = (4+6\\theta^2)\\exp(\\theta^2) + (4\\theta+2\\theta^3)(2\\theta\\exp(\\theta^2))\n$$\n$$\nV''(\\theta) = \\exp(\\theta^2) \\left[ (4+6\\theta^2) + 2\\theta(4\\theta+2\\theta^3) \\right] = \\exp(\\theta^2) \\left[ 4+6\\theta^2+8\\theta^2+4\\theta^4 \\right]\n$$\n$$\nV''(\\theta) = (4\\theta^4 + 14\\theta^2 + 4)\\exp(\\theta^2)\n$$\nNow, we evaluate the second derivative at the stationary point $\\theta = 0$:\n$$\nV''(0) = (4(0)^4 + 14(0)^2 + 4)\\exp(0) = (4)(1) = 4\n$$\nSince $V''(0) = 4 > 0$, the stationary point $\\theta=0$ corresponds to a local minimum.\nFurthermore, for any real $\\theta$, $V(\\theta) = (1+\\theta^2)\\exp(\\theta^2) \\ge (1)\\exp(0) = 1$, and $V(0) = 1$. Thus, $\\theta=0$ is a global minimum.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} (1+\\theta^2)\\exp(\\theta^2) & 0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Beyond optimizing the proposal distribution itself, the variance of an importance sampling estimator can be further reduced by integrating complementary Monte Carlo techniques. This exercise explores antithetic sampling, a powerful method that generates negative correlation between pairs of samples to reduce the estimator's overall variance . This hands-on problem allows you to explicitly quantify the variance reduction achieved and understand its deep connection to fundamental statistical concepts, including covariance and the Rao-Blackwell theorem.",
            "id": "3360249",
            "problem": "Consider importance sampling (IS) for estimating the mean of a shifted Gaussian target. Let the target distribution be $p(x)$ equal to the normal distribution $\\mathcal{N}(\\delta, 1)$ on $\\mathbb{R}$, and let the proposal distribution be $q(x)$ equal to the normal distribution $\\mathcal{N}(0, 1)$. The quantity of interest is $\\mu = \\mathbb{E}_{p}[X]$, and the importance sampling weight is $w(x) = p(x)/q(x)$. Define the estimator under importance sampling as $\\hat{\\mu}_{n} = n^{-1} \\sum_{i=1}^{n} w(X_{i}) h(X_{i})$ with $h(x) = x$ and independent samples $X_{i} \\sim q$.\n\nTo study variance reduction mechanisms, consider the following three constructions under the proposal distribution $q$:\n\n1. Antithetic coupling: construct antithetic pairs $(X^{(a)}, X^{(b)})$ such that $X^{(a)} = Z$ and $X^{(b)} = -Z$ for a single draw $Z \\sim \\mathcal{N}(0,1)$. Define the single-pair antithetic estimator\n$$\nA(Z) = \\frac{1}{2}\\big(w(X^{(a)}) h(X^{(a)}) + w(X^{(b)}) h(X^{(b)})\\big).\n$$\n\n2. Independent pairing baseline: define the two-sample baseline estimator\n$$\n\\tilde{A} = \\frac{1}{2}\\big(w(X_{1}) h(X_{1}) + w(X_{2}) h(X_{2})\\big),\n$$\nwhere $X_{1}, X_{2} \\sim q$ are independent.\n\n3. Control variates and Rao–Blackwellization (RB): introduce the control variate $c(x) = w(x) - \\mathbb{E}_{q}[w(X)]$ whose expectation under $q$ is known exactly, and consider the randomized sign-flip estimator $Y_{S} = w(SZ)\\, S Z$ with a random sign $S \\in \\{-1, +1\\}$ independent of $Z \\sim \\mathcal{N}(0,1)$. The Rao–Blackwellized estimator $\\mathbb{E}[Y_{S} \\mid Z]$ eliminates the auxiliary randomness in $S$.\n\nStarting only from fundamental definitions of importance sampling, variance, covariance, and the properties of the normal distribution, do the following:\n\n- Derive a closed-form expression for $\\operatorname{Var}_{q}(A(Z))$ and for $\\operatorname{Var}_{q}(\\tilde{A})$.\n- Use these expressions to show explicitly how antithetic coupling induces negative correlation between $w(X^{(a)}) h(X^{(a)})$ and $w(X^{(b)}) h(X^{(b)})$, and why this reduces $\\operatorname{Var}(\\hat{\\mu}_{n})$ relative to the independent pairing baseline for the same computational budget.\n- Provide the closed-form analytic expression, as a function of $\\delta$, for the variance ratio\n$$\nR(\\delta) = \\frac{\\operatorname{Var}_{q}(A(Z))}{\\operatorname{Var}_{q}(\\tilde{A})}.\n$$\n\nYour final answer must be the single closed-form expression for $R(\\delta)$. No numerical approximation is required.",
            "solution": "We begin from the definitions of importance sampling and variance. The IS estimator for $\\mu = \\mathbb{E}_{p}[X]$ with proposal $q$ and integrand $h(x) = x$ is $w(X) h(X)$ where $w(x) = p(x)/q(x)$. With $p(x) = \\mathcal{N}(\\delta,1)$ and $q(x) = \\mathcal{N}(0,1)$, the densities are\n$$\np(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\Big(-\\frac{(x - \\delta)^{2}}{2}\\Big), \n\\qquad\nq(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\Big(-\\frac{x^{2}}{2}\\Big),\n$$\nso the importance weight simplifies to\n$$\nw(x) = \\frac{p(x)}{q(x)} = \\exp\\!\\Big(\\delta x - \\frac{\\delta^{2}}{2}\\Big).\n$$\n\nLet $Z \\sim \\mathcal{N}(0,1)$ and define $Y = w(Z) Z$. This single-sample IS contribution satisfies\n$$\n\\mathbb{E}_{q}[Y] = \\mathbb{E}_{q}\\big[Z \\exp\\!\\big(\\delta Z - \\frac{\\delta^{2}}{2}\\big)\\big] \n= \\exp\\!\\Big(-\\frac{\\delta^{2}}{2}\\Big) \\mathbb{E}_{q}\\big[Z \\exp(\\delta Z)\\big].\n$$\nUsing the moment generating function of the standard normal $\\mathbb{E}[\\exp(tZ)] = \\exp(t^{2}/2)$, we obtain the identity\n$$\n\\mathbb{E}[Z \\exp(tZ)] = \\frac{\\mathrm{d}}{\\mathrm{d}t} \\mathbb{E}[\\exp(tZ)] = t \\exp\\!\\Big(\\frac{t^{2}}{2}\\Big).\n$$\nTherefore,\n$$\n\\mathbb{E}_{q}[Y] = \\exp\\!\\Big(-\\frac{\\delta^{2}}{2}\\Big) \\cdot \\delta \\exp\\!\\Big(\\frac{\\delta^{2}}{2}\\Big) = \\delta = \\mu,\n$$\nconfirming unbiasedness.\n\nWe next compute $\\operatorname{Var}_{q}(Y) = \\mathbb{E}_{q}[Y^{2}] - \\delta^{2}$. Note\n$$\nY^{2} = Z^{2} \\exp\\!\\big(2 \\delta Z - \\delta^{2}\\big),\n$$\nso\n$$\n\\mathbb{E}_{q}[Y^{2}] = \\exp(-\\delta^{2}) \\, \\mathbb{E}\\big[ Z^{2} \\exp(2\\delta Z) \\big].\n$$\nWe use the well-tested identity for the standard normal\n$$\n\\mathbb{E}\\big[Z^{2} \\exp(t Z)\\big] \n= \\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}} \\mathbb{E}[\\exp(tZ)] \n= \\frac{\\mathrm{d}^{2}}{\\mathrm{d}t^{2}} \\exp\\!\\Big(\\frac{t^{2}}{2}\\Big)\n= (t^{2} + 1) \\exp\\!\\Big(\\frac{t^{2}}{2}\\Big).\n$$\nSetting $t = 2\\delta$ yields\n$$\n\\mathbb{E}_{q}[Y^{2}] = \\exp(-\\delta^{2}) \\cdot \\big( (2\\delta)^{2} + 1 \\big) \\exp\\!\\Big( \\frac{(2\\delta)^{2}}{2} \\Big)\n= (4\\delta^{2} + 1) \\exp(\\delta^{2}).\n$$\nHence,\n$$\n\\operatorname{Var}_{q}(Y) = (4\\delta^{2} + 1) \\exp(\\delta^{2}) - \\delta^{2}.\n$$\n\nWe now consider the two-sample baseline estimator\n$$\n\\tilde{A} = \\frac{1}{2}\\big( w(X_{1}) h(X_{1}) + w(X_{2}) h(X_{2}) \\big),\n$$\nwith $X_{1}, X_{2} \\stackrel{\\text{iid}}{\\sim} q$. By independence and identical distribution, \n$$\n\\operatorname{Var}_{q}(\\tilde{A}) = \\frac{1}{4} \\big( \\operatorname{Var}_{q}(Y) + \\operatorname{Var}_{q}(Y) \\big) = \\frac{1}{2} \\operatorname{Var}_{q}(Y)\n= \\frac{(4\\delta^{2} + 1) \\exp(\\delta^{2}) - \\delta^{2}}{2}.\n$$\n\nFor the antithetic estimator, let $X^{(a)} = Z$ and $X^{(b)} = -Z$ for a single draw $Z \\sim \\mathcal{N}(0,1)$. Then\n$$\nA(Z) = \\frac{1}{2}\\big( w(Z) Z + w(-Z) (-Z) \\big).\n$$\nUsing the weight symmetry $w(-Z) = \\exp(-\\delta Z - \\delta^{2}/2)$, we can write\n$$\nA(Z) = \\frac{1}{2} \\exp\\!\\Big(-\\frac{\\delta^{2}}{2}\\Big) \\big( Z \\exp(\\delta Z) - Z \\exp(-\\delta Z) \\big)\n= Z \\exp\\!\\Big(-\\frac{\\delta^{2}}{2}\\Big) \\sinh(\\delta Z).\n$$\nWe compute its second moment,\n$$\nA(Z)^{2} = Z^{2} \\exp(-\\delta^{2}) \\sinh^{2}(\\delta Z) \n= Z^{2} \\exp(-\\delta^{2}) \\cdot \\frac{ \\exp(2\\delta Z) - 2 + \\exp(-2\\delta Z) }{4}.\n$$\nTherefore,\n$$\n\\mathbb{E}_{q}[A(Z)^{2}] = \\frac{\\exp(-\\delta^{2})}{4} \\Big( \\mathbb{E}[ Z^{2} \\exp(2\\delta Z) ] - 2 \\mathbb{E}[Z^{2}] + \\mathbb{E}[ Z^{2} \\exp(-2\\delta Z) ] \\Big).\n$$\nUsing the identity above with $t = \\pm 2\\delta$ and $\\mathbb{E}[Z^{2}] = 1$, we get\n$$\n\\mathbb{E}_{q}[A(Z)^{2}] \n= \\frac{\\exp(-\\delta^{2})}{4} \\Big( 2 (4\\delta^{2} + 1) \\exp(2\\delta^{2}) - 2 \\Big)\n= \\frac{1}{2} \\Big( (4\\delta^{2} + 1) \\exp(\\delta^{2}) - \\exp(-\\delta^{2}) \\Big).\n$$\nHence,\n$$\n\\operatorname{Var}_{q}(A(Z)) = \\mathbb{E}_{q}[A(Z)^{2}] - \\mu^{2}\n= \\frac{1}{2} \\Big( (4\\delta^{2} + 1) \\exp(\\delta^{2}) - \\exp(-\\delta^{2}) \\Big) - \\delta^{2}.\n$$\n\nTo connect antithetic coupling and covariance, let $g(Z) = w(Z) Z$ and $g(-Z) = w(-Z) (-Z)$. Then \n$$\n\\operatorname{Var}_{q}\\!\\Big( \\frac{g(Z) + g(-Z)}{2} \\Big)\n= \\frac{ \\operatorname{Var}_{q}(g(Z)) + \\operatorname{Var}_{q}(g(-Z)) + 2 \\operatorname{Cov}_{q}(g(Z), g(-Z)) }{4}\n= \\frac{ \\operatorname{Var}_{q}(Y) + \\operatorname{Cov}_{q}(g(Z), g(-Z)) }{2}.\n$$\nComparing with $\\operatorname{Var}_{q}(\\tilde{A}) = \\operatorname{Var}_{q}(Y)/2$, the variance reduction is precisely governed by $\\operatorname{Cov}_{q}(g(Z), g(-Z))$. A direct computation gives\n$$\ng(Z) g(-Z) = Z (-Z) \\exp\\!\\Big(\\delta Z - \\frac{\\delta^{2}}{2}\\Big) \\exp\\!\\Big(-\\delta Z - \\frac{\\delta^{2}}{2}\\Big) = - Z^{2} \\exp(-\\delta^{2}),\n$$\nso\n$$\n\\mathbb{E}_{q}[g(Z) g(-Z)] = - \\exp(-\\delta^{2}) \\mathbb{E}[Z^{2}] = - \\exp(-\\delta^{2}),\n$$\nand, since $\\mathbb{E}_{q}[g(Z)] = \\mathbb{E}_{q}[g(-Z)] = \\delta$,\n$$\n\\operatorname{Cov}_{q}(g(Z), g(-Z)) = \\mathbb{E}_{q}[g(Z) g(-Z)] - \\mathbb{E}_{q}[g(Z)] \\mathbb{E}_{q}[g(-Z)]\n= - \\exp(-\\delta^{2}) - \\delta^{2} < 0.\n$$\nThus, antithetic coupling induces strictly negative correlation for all $\\delta$, yielding a variance reduction relative to the independent pairing baseline.\n\nWe now form the variance ratio\n$$\nR(\\delta) = \\frac{ \\operatorname{Var}_{q}(A(Z)) }{ \\operatorname{Var}_{q}(\\tilde{A}) } \n= \\frac{ \\displaystyle \\frac{1}{2} \\Big( (4\\delta^{2} + 1) \\exp(\\delta^{2}) - \\exp(-\\delta^{2}) \\Big) - \\delta^{2} }{ \\displaystyle \\frac{ (4\\delta^{2} + 1) \\exp(\\delta^{2}) - \\delta^{2} }{2} }.\n$$\nA simplified closed form is\n$$\nR(\\delta) = \\frac{ (4\\delta^{2} + 1) \\exp(\\delta^{2}) - \\exp(-\\delta^{2}) - 2\\delta^{2} }{ (4\\delta^{2} + 1) \\exp(\\delta^{2}) - \\delta^{2} }.\n$$\n\nFinally, we note the connection to control variates and Rao–Blackwellization (RB). The control variate $c(X) = w(X) - \\mathbb{E}_{q}[w(X)]$ has zero expectation under $q$, since $\\mathbb{E}_{q}[w(X)] = \\int q(x) \\frac{p(x)}{q(x)} \\,\\mathrm{d}x = 1$. Including $c(X)$ linearly with an optimally chosen coefficient reduces variance further. For RB, consider the randomized sign-flip estimator $Y_{S} = w(SZ) S Z$ with a random sign $S \\in \\{-1, +1\\}$ independent of $Z$. Then \n$$\n\\mathbb{E}[Y_{S} \\mid Z] = \\frac{1}{2} \\big( w(Z) Z + w(-Z) (-Z) \\big) = A(Z),\n$$\nso the antithetic estimator is the Rao–Blackwellization of $Y_{S}$, guaranteeing a variance reduction by Jensen’s inequality for conditional expectations, which is consistent with our explicit calculation showing $R(\\delta) < 1$ for all $\\delta$.\n\nThe requested final answer is the closed-form analytic expression for $R(\\delta)$.",
            "answer": "$$\\boxed{\\frac{(4\\delta^{2}+1)\\,\\exp(\\delta^{2})-\\exp(-\\delta^{2})-2\\delta^{2}}{(4\\delta^{2}+1)\\,\\exp(\\delta^{2})-\\delta^{2}}}$$"
        }
    ]
}