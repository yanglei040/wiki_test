{
    "hands_on_practices": [
        {
            "introduction": "The foundation of any automated structure elucidation platform is the ability to acquire highly accurate mass measurements. This precision is essential for determining the elemental composition of an unknown compound, which drastically narrows down the field of possible structures. This first exercise  will ground your understanding in this fundamental principle by having you derive the expression for mass accuracy and apply a lock-mass correction, a standard technique used to compensate for instrument drift and improve data quality.",
            "id": "3693918",
            "problem": "An automated structure elucidation platform relies on high-resolution mass measurements to prune candidate structures generated from tandem mass spectrometry. A core scoring feature is the mass accuracy expressed in parts per million. Starting from the fundamental definition of relative error and the definition of parts per million as one part per million units, first derive an expression for the mass accuracy in parts per million in terms of an observed mass and a theoretical mass. Then, consider a high-resolution mass spectrometry acquisition in which a background calibrant ion is used for lock-mass correction. Assume measurements are taken in the positive mode with charge $z=1$, so that $m/z$ equals mass numerically. Over the relevant mass range, assume the instrument’s mass error is well-approximated by an additive constant offset.\n\nYou are given:\n- Theoretical lock-mass $m_{\\text{L,theoretical}} = 371.101237$.\n- Observed lock-mass $m_{\\text{L,observed}} = 371.101737$.\n- Theoretical analyte monoisotopic mass $m_{\\text{A,theoretical}} = 300.123456$.\n- Observed analyte monoisotopic mass $m_{\\text{A,observed}} = 300.124011$.\n\nUnder the additive-offset assumption, derive the lock-mass-corrected analyte mass and then compute the analyte’s post-correction mass accuracy in parts per million using your derived expression. Round your final answer to four significant figures. Express the final answer in ppm.",
            "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\nThe data and conditions explicitly provided are:\n-   Theoretical lock-mass: $m_{\\text{L,theoretical}} = 371.101237$\n-   Observed lock-mass: $m_{\\text{L,observed}} = 371.101737$\n-   Theoretical analyte monoisotopic mass: $m_{\\text{A,theoretical}} = 300.123456$\n-   Observed analyte monoisotopic mass: $m_{\\text{A,observed}} = 300.124011$\n-   Charge state: $z=1$\n-   Mass error model: The instrument's mass error is approximated by an additive constant offset over the relevant mass range.\n-   Task 1: Derive an expression for mass accuracy in parts per million (ppm).\n-   Task 2: Derive the lock-mass-corrected analyte mass.\n-   Task 3: Compute the analyte's post-correction mass accuracy in ppm.\n-   Rounding requirement: Round the final answer to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as lock-mass correction is a standard and fundamental technique in high-resolution mass spectrometry for improving mass accuracy by correcting for instrument drift. The given mass values are realistic for small organic molecules. The assumption of an additive constant mass error is a common and reasonable first-order approximation for instrument calibration over a limited mass range. The problem is well-posed, providing all necessary data and a clear, unambiguous objective. The language is precise and objective. The problem is therefore deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Solution Derivation\n\nThe solution proceeds in four stages as requested by the problem:\n1.  Derivation of the mass accuracy formula in parts per million (ppm).\n2.  Calculation of the mass error offset using the lock-mass calibrant.\n3.  Calculation of the lock-mass-corrected analyte mass.\n4.  Calculation of the post-correction mass accuracy for the analyte.\n\n#### 1. Derivation of Mass Accuracy in ppm\n\nMass accuracy is a measure of the error in a mass measurement. The absolute error, $\\Delta m$, is the difference between the observed mass, $m_{\\text{observed}}$, and the theoretical mass, $m_{\\text{theoretical}}$.\n$$\n\\Delta m = m_{\\text{observed}} - m_{\\text{theoretical}}\n$$\nThe relative error, $E_{\\text{rel}}$, normalizes the absolute error by the theoretical mass, making it a dimensionless quantity.\n$$\nE_{\\text{rel}} = \\frac{\\Delta m}{m_{\\text{theoretical}}} = \\frac{m_{\\text{observed}} - m_{\\text{theoretical}}}{m_{\\text{theoretical}}}\n$$\nThe unit \"parts per million\" (ppm) corresponds to a factor of $10^{-6}$. Therefore, to express the relative error in ppm, we multiply it by $10^6$. Let this be denoted as $E_{\\text{ppm}}$.\n$$\nE_{\\text{ppm}} = E_{\\text{rel}} \\times 10^6 = \\left( \\frac{m_{\\text{observed}} - m_{\\text{theoretical}}}{m_{\\text{theoretical}}} \\right) \\times 10^6\n$$\nThis is the required expression for mass accuracy in ppm.\n\n#### 2. Calculation of the Mass Error Offset\n\nThe problem states that the instrument's mass error can be modeled as an additive constant offset, which we will denote as $\\delta$. This implies that for any true mass $m_{\\text{true}}$, the instrument measures an observed mass $m_{\\text{observed}}$ given by:\n$$\nm_{\\text{observed}} = m_{\\text{true}} + \\delta\n$$\nThe lock-mass calibrant provides a reference point to estimate this offset. Using the lock-mass data, the offset $\\delta$ can be calculated as the difference between the observed and theoretical lock-mass values.\n$$\n\\delta = m_{\\text{L,observed}} - m_{\\text{L,theoretical}}\n$$\nSubstituting the given values:\n$$\n\\delta = 371.101737 - 371.101237 = 0.000500\n$$\nThis constant offset of $0.000500$ Da is assumed to apply to all measurements in the relevant mass range, including that of the analyte.\n\n#### 3. Calculation of the Lock-Mass-Corrected Analyte Mass\n\nTo find the corrected mass of the analyte, $m_{\\text{A,corrected}}$, we subtract the calculated mass error offset $\\delta$ from the observed analyte mass, $m_{\\text{A,observed}}$.\n$$\nm_{\\text{A,corrected}} = m_{\\text{A,observed}} - \\delta\n$$\nSubstituting the given value for $m_{\\text{A,observed}}$ and the calculated value for $\\delta$:\n$$\nm_{\\text{A,corrected}} = 300.124011 - 0.000500 = 300.123511\n$$\nThis is the lock-mass-corrected analyte mass.\n\n#### 4. Calculation of Post-Correction Mass Accuracy\n\nFinally, we compute the analyte's mass accuracy in ppm after the lock-mass correction. We use the derived formula for $E_{\\text{ppm}}$ with the corrected analyte mass, $m_{\\text{A,corrected}}$, now serving as the \"observed\" mass, and compare it to the theoretical analyte mass, $m_{\\text{A,theoretical}}$.\n$$\nE_{\\text{A,ppm}} = \\left( \\frac{m_{\\text{A,corrected}} - m_{\\text{A,theoretical}}}{m_{\\text{A,theoretical}}} \\right) \\times 10^6\n$$\nFirst, we calculate the new absolute error for the analyte:\n$$\n\\Delta m_{\\text{A,corrected}} = m_{\\text{A,corrected}} - m_{\\text{A,theoretical}} = 300.123511 - 300.123456 = 0.000055\n$$\nNow, we calculate the ppm error:\n$$\nE_{\\text{A,ppm}} = \\left( \\frac{0.000055}{300.123456} \\right) \\times 10^6 \\approx 0.18325609 \\text{ ppm}\n$$\nThe problem requires rounding the final answer to four significant figures. The first significant figure is $1$, followed by $8$, $3$, and $2$. The fifth significant figure is $5$, which requires rounding up the fourth digit.\n$$\nE_{\\text{A,ppm}} \\approx 0.1833 \\text{ ppm}\n$$\nThis is the final post-correction mass accuracy for the analyte.",
            "answer": "$$\n\\boxed{0.1833}\n$$"
        },
        {
            "introduction": "Once high-accuracy mass data is obtained, the next step is to interpret the rich information contained within isotopic patterns. The relative abundance of heavy isotopes, like carbon-13, provides direct clues about a molecule's elemental composition. In this practice , you will employ a powerful statistical method, Maximum Likelihood Estimation, to determine the number of carbon atoms in an analyte from its isotopic distribution, and quantify the uncertainty of your estimate.",
            "id": "3693966",
            "problem": "An automated mass spectrometry structure elucidation platform based on Fourier Transform Ion Cyclotron Resonance (FT-ICR) acquires fine isotopic structure for a molecular ion and algorithmically assigns isotopologue counts. The platform isolates carbon-13 satellite peaks by correcting for non-carbon isotopic contributions (for example, nitrogen-15, deuterium, and oxygen-17) using internal calibration and elemental ratio priors, and then reports the number of detected ions corresponding to isotopologues with exactly one carbon-13 substitution.\n\nAssume the following scientifically standard foundations:\n- Each carbon site independently carries the heavy isotope carbon-13 with probability $q$ (the natural abundance of carbon-13 under terrestrial conditions).\n- For a molecule with $n$ carbon atoms, the total number of carbon-13 substitutions follows a Binomial distribution across the $n$ sites.\n- Ion counting by the detector is well modeled by independent sampling from the isotopologue distribution across $N$ ions assigned to the monoisotopic cluster of the molecular ion.\n\nThe platform reports $N = 200000$ total assigned ions and $x_{1} = 28280$ ions assigned to the isotopologue with exactly one carbon-13 substitution. The calibrated natural abundance is $q = 0.0110$.\n\nStarting only from the foundations stated above, use maximum likelihood to estimate the number of carbon atoms $n$ in the analyte and, using a Fisher information approximation, quantify the uncertainty as a standard deviation for the estimator. Express your final numerical answers as follows:\n- Round both the maximum likelihood estimate and its standard deviation to three significant figures.\n- Report the two numbers in a single response.\n- No units are required; treat the count of carbons as dimensionless.",
            "solution": "The problem is situated in spectrometric identification of organic compounds using fine isotopic structure, where an automated structure elucidation platform estimates elemental counts by exploiting isotopic patterns. The fundamental statistical base is that each carbon atom independently carries the heavy isotope carbon-13 with probability $q$, so the number of carbon-13 substitutions across $n$ carbon sites is Binomial.\n\nLet $n$ denote the number of carbon atoms. The number of carbon-13 substitutions in a given molecule is a Binomial random variable $K \\sim \\text{Binomial}(n, q)$, with probability mass function\n$$\n\\Pr(K = k) = \\binom{n}{k} q^{k} (1 - q)^{n - k}.\n$$\nThe event \"exactly one carbon-13 substitution\" has probability\n$$\np(n) = \\Pr(K = 1) = \\binom{n}{1} q (1 - q)^{n - 1} = n q (1 - q)^{n - 1}.\n$$\nAcross $N$ independent ion assignments, the count $x_{1}$ of ions with exactly one carbon-13 substitution is Binomial with parameters $N$ and $p(n)$,\n$$\nx_{1} \\sim \\text{Binomial}(N, p(n)), \\quad p(n) = n q (1 - q)^{n - 1}.\n$$\nThe likelihood for $n$ given $x_{1}$ and $N$ (up to a multiplicative constant independent of $n$) is\n$$\nL(n) \\propto \\left[p(n)\\right]^{x_{1}} \\left[1 - p(n)\\right]^{N - x_{1}},\n$$\nand the log-likelihood is\n$$\n\\ell(n) = x_{1} \\ln p(n) + (N - x_{1}) \\ln\\!\\left(1 - p(n)\\right).\n$$\nTo find the maximum likelihood estimate, differentiate with respect to $n$ and set the derivative to zero. Write $s = 1 - q$ to simplify notation. Then $p(n) = n q s^{n - 1}$ and\n$$\n\\frac{d}{dn} p(n) = q s^{n - 1} \\left(1 + n \\ln s \\right),\n$$\nwhich follows from $\\ln p(n) = \\ln q + \\ln n + (n - 1) \\ln s$ and the chain rule. The derivative of the log-likelihood is\n$$\n\\frac{d}{dn} \\ell(n) = x_{1} \\frac{p'(n)}{p(n)} - (N - x_{1}) \\frac{p'(n)}{1 - p(n)} = p'(n) \\left[ \\frac{x_{1}}{p(n)} - \\frac{N - x_{1}}{1 - p(n)} \\right].\n$$\nSetting $\\frac{d}{dn} \\ell(n) = 0$ yields the likelihood equation\n$$\n\\frac{x_{1}}{p(n)} = \\frac{N - x_{1}}{1 - p(n)} \\quad \\Rightarrow \\quad \\frac{x_{1}}{N} = p(n).\n$$\nDefine the empirical fraction $y = x_{1} / N$. The maximum likelihood estimate $\\hat{n}$ solves\n$$\ny = n q s^{n - 1}.\n$$\nThis equation can be solved in closed form using the Lambert $W$ function. Rearranging gives\n$$\n\\frac{y}{q} = n s^{n - 1} \\quad \\Rightarrow \\quad \\frac{y s}{q} = n s^{n}.\n$$\nLet $\\alpha = \\ln s$, so $s^{n} = \\exp(\\alpha n)$. Then\n$$\nn \\exp(\\alpha n) = \\frac{y s}{q}.\n$$\nSet $z = \\alpha n$. Then the equation becomes\n$$\n\\frac{z}{\\alpha} \\exp(z) = \\frac{y s}{q} \\quad \\Rightarrow \\quad z \\exp(z) = \\alpha \\frac{y s}{q}.\n$$\nBy definition of the Lambert $W$ function, $z = W\\!\\left(\\alpha \\frac{y s}{q}\\right)$, hence\n$$\n\\hat{n} = \\frac{W\\!\\left(\\alpha \\frac{y s}{q}\\right)}{\\alpha}, \\quad \\text{with} \\quad \\alpha = \\ln(1 - q), \\quad s = 1 - q, \\quad y = \\frac{x_{1}}{N}.\n$$\n\nTo quantify uncertainty, we use the Fisher information approximation. For the Binomial likelihood in the reparameterization by $n$, the Fisher information in $n$ is\n$$\nI(n) = N \\frac{\\left[p'(n)\\right]^{2}}{p(n) \\left[1 - p(n)\\right]}.\n$$\nWe already have $p(n) = n q s^{n - 1}$ and $p'(n) = q s^{n - 1} \\left(1 + n \\ln s \\right)$. Substituting yields\n$$\nI(n) = N \\cdot \\frac{\\left[q s^{n - 1} \\left(1 + n \\ln s \\right)\\right]^{2}}{n q s^{n - 1} \\left(1 - n q s^{n - 1}\\right)} = N \\cdot \\frac{q s^{n - 1} \\left(1 + n \\ln s \\right)^{2}}{n \\left(1 - n q s^{n - 1}\\right)}.\n$$\nThe asymptotic variance of the maximum likelihood estimator is approximated by the inverse Fisher information evaluated at $\\hat{n}$,\n$$\n\\operatorname{Var}(\\hat{n}) \\approx \\frac{1}{I(\\hat{n})}, \\quad \\text{so} \\quad \\operatorname{sd}(\\hat{n}) \\approx \\sqrt{\\frac{1}{I(\\hat{n})}}.\n$$\n\nNow plug in the given numerical values. The inputs are $q = 0.0110$, $N = 200000$, $x_{1} = 28280$. Compute $y = x_{1}/N$,\n$$\ny = \\frac{28280}{200000} = 0.1414.\n$$\nThen $s = 1 - q = 0.9890$ and $\\alpha = \\ln(s) = \\ln(0.9890)$. Numerically, $\\alpha \\approx -0.011061$. The argument for the Lambert $W$ function is\n$$\n\\alpha \\frac{y s}{q} = \\alpha \\cdot \\frac{0.1414 \\times 0.9890}{0.0110}.\n$$\nCompute $c = \\frac{y s}{q} \\approx \\frac{0.1414 \\times 0.9890}{0.0110} \\approx 12.713$, hence $\\alpha c \\approx -0.011061 \\times 12.713 \\approx -0.1406$. The Lambert $W$ evaluation gives $W(-0.1406) \\approx -0.166$. Therefore\n$$\n\\hat{n} = \\frac{W(\\alpha c)}{\\alpha} \\approx \\frac{-0.166}{-0.011061} \\approx 15.01.\n$$\nRounding to three significant figures, the maximum likelihood estimate is $15.0$.\n\nNext compute the Fisher information at $\\hat{n} \\approx 15.01$. Using $p(\\hat{n}) = y = 0.1414$, $s^{\\hat{n} - 1} = s^{14} = \\exp(14 \\alpha) \\approx \\exp(14 \\times -0.011061) \\approx \\exp(-0.1549) \\approx 0.8564$, and\n$$\n1 + \\hat{n} \\ln s \\approx 1 + 15.01 \\times (-0.011061) \\approx 0.834.\n$$\nThen\n$$\nI(\\hat{n}) = N \\cdot \\frac{q s^{\\hat{n} - 1} \\left(1 + \\hat{n} \\ln s \\right)^{2}}{\\hat{n} \\left(1 - \\hat{n} q s^{\\hat{n} - 1}\\right)} \\approx 200000 \\cdot \\frac{0.0110 \\times 0.8564 \\times (0.834)^{2}}{15.01 \\times \\left(1 - 0.1414\\right)}.\n$$\nNumerically, the numerator factor is $0.0110 \\times 0.8564 \\times (0.834)^{2} \\approx 0.00655$, and the denominator factor is $15.01 \\times 0.8586 \\approx 12.88$, giving\n$$\nI(\\hat{n}) \\approx 200000 \\times \\frac{0.00655}{12.88} \\approx 1.02 \\times 10^{2}.\n$$\nHence\n$$\n\\operatorname{sd}(\\hat{n}) \\approx \\sqrt{\\frac{1}{I(\\hat{n})}} \\approx \\sqrt{\\frac{1}{1.02 \\times 10^{2}}} \\approx 0.0992.\n$$\nRounded to three significant figures, the standard deviation is $0.0992$.\n\nReport the two numbers in a single response, rounded to three significant figures, with no units.",
            "answer": "$$\\boxed{\\begin{pmatrix}15.0 & 0.0992\\end{pmatrix}}$$"
        },
        {
            "introduction": "Automated platforms truly showcase their power when interpreting complex tandem mass spectrometry (MS/MS) data, which reveals how a molecule fragments. This process involves matching a list of observed fragment ion masses to those predicted from a candidate structure. This final hands-on exercise  challenges you to formalize this matching process as a rigorous optimization problem, developing an algorithm that finds the most plausible assignment while penalizing poor matches and overfitting.",
            "id": "3693904",
            "problem": "You are tasked with formalizing and solving a peak-to-fragment assignment problem arising in Tandem Mass Spectrometry (MS/MS) for automated structure elucidation platforms. The goal is to design and implement a program that, given observed MS/MS peaks and a set of predicted fragment or neutral loss masses, assigns each observed peak to at most one predicted mass or marks it as unassigned. The assignment should minimize an objective that penalizes mass deviations according to a Gaussian instrument error model, penalizes leaving peaks unassigned, and penalizes model complexity to avoid overfitting. All quantities are defined precisely below and every symbol is used in a consistent and scientifically realistic manner.\n\nFundamental base: Assume a Gaussian measurement error model for mass deviations, which states that the mass error between an observed peak and its true fragment is modeled as a Gaussian random variable with mean $0$ and standard deviation $\\sigma$ in daltons. Let the negative log-likelihood for a deviation $\\Delta$ be\n$$\n\\ell(\\Delta;\\sigma) = \\frac{1}{2}\\left(\\frac{\\Delta}{\\sigma}\\right)^2 + \\ln\\left(\\sqrt{2\\pi}\\,\\sigma\\right).\n$$\nThis penalty increases with the square of the deviation, consistent with well-tested instrument mass accuracy models.\n\nProblem formulation: Let observed peak masses be $m_i$ for $i \\in \\{1,\\dots,n\\}$ in daltons, with corresponding normalized intensities $w_i \\in [0,1]$. Let the set of predicted fragment or neutral loss masses be $c_j$ for $j \\in \\{1,\\dots,k\\}$ in daltons. Let $\\sigma$ be the mass accuracy in daltons, $T$ be the matching tolerance in daltons, $\\lambda_u$ be the penalty coefficient for leaving a peak unassigned, and $\\lambda_c$ be the model complexity penalty coefficient per assigned predicted fragment or neutral loss. Let $M$ be a large infeasibility constant to block assignments when the absolute deviation exceeds the tolerance. Define binary assignment variables $x_{i,j} \\in \\{0,1\\}$ indicating whether peak $i$ is assigned to predicted mass $j$, and binary unassignment variables $u_i \\in \\{0,1\\}$ indicating whether peak $i$ is unassigned.\n\nThe objective is to minimize the total cost\n$$\n\\min_{x,u}\\quad \\sum_{i=1}^{n}\\sum_{j=1}^{k} x_{i,j}\\left[w_i\\,\\ell(m_i - c_j;\\sigma) + \\lambda_c\\right] + \\sum_{i=1}^{n} u_i \\left[\\lambda_u\\,w_i\\right],\n$$\nsubject to the constraints\n$$\n\\sum_{j=1}^{k} x_{i,j} + u_i = 1 \\quad \\text{for all } i \\in \\{1,\\dots,n\\},\n$$\n$$\n\\sum_{i=1}^{n} x_{i,j} \\le 1 \\quad \\text{for all } j \\in \\{1,\\dots,k\\},\n$$\n$$\nx_{i,j} \\in \\{0,1\\},\\quad u_i \\in \\{0,1\\},\n$$\nand with feasibility encoded by\n$$\nx_{i,j} = 0 \\quad \\text{whenever } |m_i - c_j| > T,\n$$\nwhich is operationalized by setting the corresponding assignment cost to a large constant $M$ to prevent choosing infeasible matches. This formulation assigns each peak to at most one predicted mass, penalizes leaving a peak unassigned, and discourages overfitting via a per-assignment complexity penalty.\n\nUnit specifications: All masses $m_i$ and $c_j$ are in daltons, $\\sigma$ and $T$ are in daltons, intensities $w_i$ are dimensionless in $[0,1]$, and the objective value is dimensionless. Angles are not used. No percentages are used; any fraction is represented as a decimal.\n\nYour task is to implement a program that solves the above optimization exactly as a linear sum assignment on a rectangular cost matrix with $n$ rows and $k+n$ columns (the last $n$ columns act as per-peak dummy \"unassigned\" slots). Use the following test suite with scientifically realistic parameters. For each test case, output the minimized objective value and the peak-to-candidate assignment vector where an assigned candidate is represented by its $0$-based index and an unassigned peak is represented by $-1$.\n\nTest suite:\n- Case $1$ (happy path): $m = [100.0,114.0,128.0]$ daltons, $w = [0.9,0.6,0.4]$, $c = [99.999,114.002,127.999]$ daltons, $\\sigma = 0.01$ daltons, $T = 0.05$ daltons, $\\lambda_u = 1.2$, $\\lambda_c = 0.3$, $M = 10^6$.\n- Case $2$ (tolerance edge): $m = [150.0,200.0]$ daltons, $w = [1.0,0.5]$, $c = [150.04,200.005]$ daltons, $\\sigma = 0.01$ daltons, $T = 0.03$ daltons, $\\lambda_u = 0.8$, $\\lambda_c = 0.2$, $M = 10^6$.\n- Case $3$ (no candidates): $m = [300.0,320.0,340.0]$ daltons, $w = [0.7,0.5,0.3]$, $c = []$, $\\sigma = 0.02$ daltons, $T = 0.05$ daltons, $\\lambda_u = 1.0$, $\\lambda_c = 0.0$, $M = 10^6$.\n- Case $4$ (overfitting pressure with collisions): $m = [500.0,500.03,499.97]$ daltons, $w = [1.0,0.8,0.7]$, $c = [500.0,500.02]$ daltons, $\\sigma = 0.01$ daltons, $T = 0.03$ daltons, $\\lambda_u = 0.5$, $\\lambda_c = 0.25$, $M = 10^6$.\n\nRequired final output format: Your program should produce a single line containing the results as a comma-separated list enclosed in square brackets, where each test case result is a two-element list of the form $[\\text{objective\\_value},\\ \\text{assignment\\_vector}]$. For example, the output structure is $[[o_1,[a_{1,1},\\dots,a_{1,n_1}]], [o_2,[a_{2,1},\\dots]], \\dots]$, where each $o_i$ is a float and each $a_{i,\\cdot}$ is an integer index into the candidate list or $-1$ if unassigned.",
            "solution": "The problem presented is a constrained optimization task aimed at assigning observed mass spectral peaks to a set of predicted fragment masses. This is a common subproblem in the field of computational mass spectrometry for automated structure elucidation. I have validated the problem statement and found it to be scientifically sound, well-posed, and algorithmically solvable.\n\nThe core of the problem is to minimize a total cost function subject to a set of constraints. The objective function is a sum of three terms:\n$1$. A goodness-of-fit cost for assigning an observed peak $m_i$ to a predicted candidate mass $c_j$, weighted by the peak's intensity $w_i$. This cost is based on the negative log-likelihood $\\ell(\\Delta;\\sigma)$ of the mass deviation $\\Delta = m_i - c_j$ under a Gaussian error model with standard deviation $\\sigma$.\n$2$. A model complexity penalty $\\lambda_c$ for each assignment made, to prevent overfitting.\n$3$. A penalty $\\lambda_u w_i$ for each peak $m_i$ that is left unassigned.\n\nThe constraints are:\n$1$. Each observed peak $m_i$ must be either assigned to exactly one predicted candidate $c_j$ or be marked as unassigned.\n$2$. Each predicted candidate $c_j$ can be assigned to at most one observed peak $m_i$.\n$3$. An assignment between $m_i$ and $c_j$ is only permissible if their absolute mass difference $|m_i - c_j|$ is within a specified tolerance $T$.\n\nThis structure is formally equivalent to the **linear sum assignment problem**, also known as the minimum weight perfect matching problem in a bipartite graph. We can solve this problem efficiently using standard algorithms, such as the Hungarian algorithm or min-cost max-flow algorithms. The `scipy.optimize.linear_sum_assignment` function implements such a solver.\n\nTo apply this solver, we must first construct a cost matrix that encapsulates the objective function and constraints. Let there be $n$ observed peaks and $k$ predicted candidates. The task is to assign each of the $n$ peaks. Each peak can be assigned to one of the $k$ candidates or left unassigned. To model the \"unassigned\" option within the assignment framework, we introduce $n$ \"dummy\" or \"unassigned\" slots, one for each peak. This ensures that every peak has an assignment option (either to a real candidate or its own unassigned slot) and that leaving multiple peaks unassigned is possible.\n\nWe therefore construct a cost matrix $C$ of size $n \\times (k+n)$. The rows $i \\in \\{0, \\dots, n-1\\}$ correspond to the observed peaks $m_i$. The first $k$ columns $j \\in \\{0, \\dots, k-1\\}$ correspond to the predicted candidates $c_j$. The last $n$ columns $j \\in \\{k, \\dots, k+n-1\\}$ correspond to the dummy unassigned slots.\n\nThe entries of the cost matrix $C_{i,j}$ are defined as follows:\n\n$1$. **Assignment Costs (for $j < k$)**: This is the cost of assigning peak $m_i$ to candidate $c_j$. According to the objective function, this cost is $w_i\\,\\ell(m_i - c_j;\\sigma) + \\lambda_c$. The feasibility constraint $|m_i - c_j| \\le T$ is enforced by setting the cost to a large infeasibility constant $M$ if the condition is not met.\nLet $\\Delta_{i,j} = m_i - c_j$. The negative log-likelihood function is $\\ell(\\Delta_{i,j};\\sigma) = \\frac{1}{2}\\left(\\frac{\\Delta_{i,j}}{\\sigma}\\right)^2 + \\ln\\left(\\sqrt{2\\pi}\\,\\sigma\\right)$.\nThus, for $i \\in \\{0, \\dots, n-1\\}$ and $j \\in \\{0, \\dots, k-1\\}$:\n$$\nC_{i,j} =\n\\begin{cases}\nw_i \\left[ \\frac{1}{2}\\left(\\frac{m_i - c_j}{\\sigma}\\right)^2 + \\ln\\left(\\sqrt{2\\pi}\\,\\sigma\\right) \\right] + \\lambda_c & \\text{if } |m_i - c_j| \\le T \\\\\nM & \\text{if } |m_i - c_j| > T\n\\end{cases}\n$$\n\n$2$. **Unassignment Costs (for $j \\ge k$)**: This is the cost of leaving peak $m_i$ unassigned. We create a dedicated unassigned slot for each peak. Let column $k+i$ be the slot for leaving peak $i$ unassigned. The cost is given by the problem as $\\lambda_u w_i$. To ensure that peak $i$ can only occupy its own unassigned slot, we set the cost of assigning peak $i$ to another peak's dummy slot (i.e., column $k+i'$ where $i' \\ne i$) to the large constant $M$.\nThus, for $i \\in \\{0, \\dots, n-1\\}$ and for the dummy column index $j' = j-k \\in \\{0, \\dots, n-1\\}$:\n$$\nC_{i,j} = C_{i,k+j'} =\n\\begin{cases}\n\\lambda_u w_i & \\text{if } j' = i \\\\\nM & \\text{if } j' \\ne i\n\\end{cases}\n$$\n\nWith the $n \\times (k+n)$ cost matrix $C$ constructed, `scipy.optimize.linear_sum_assignment(C)` finds the optimal assignment of each of the $n$ rows (peaks) to $n$ unique columns. The solver returns the row and column indices of the optimal assignments. The sum of the costs at these indices gives the minimized total objective value.\n\nFinally, we interpret the resulting column indices to build the final assignment vector. For each peak $i$, if it is assigned to a column $j < k$, it is matched with candidate $c_j$, and we record the index $j$. If it is assigned to a column $j \\ge k$, it is unassigned, and we record this as $-1$. This procedure guarantees an exact solution to the specified optimization problem.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Test suite from the problem statement\n    test_cases = [\n        {\n            \"m\": [100.0, 114.0, 128.0], \"w\": [0.9, 0.6, 0.4],\n            \"c\": [99.999, 114.002, 127.999], \"sigma\": 0.01,\n            \"T\": 0.05, \"lambda_u\": 1.2, \"lambda_c\": 0.3, \"M\": 1e6\n        },\n        {\n            \"m\": [150.0, 200.0], \"w\": [1.0, 0.5],\n            \"c\": [150.04, 200.005], \"sigma\": 0.01,\n            \"T\": 0.03, \"lambda_u\": 0.8, \"lambda_c\": 0.2, \"M\": 1e6\n        },\n        {\n            \"m\": [300.0, 320.0, 340.0], \"w\": [0.7, 0.5, 0.3],\n            \"c\": [], \"sigma\": 0.02,\n            \"T\": 0.05, \"lambda_u\": 1.0, \"lambda_c\": 0.0, \"M\": 1e6\n        },\n        {\n            \"m\": [500.0, 500.03, 499.97], \"w\": [1.0, 0.8, 0.7],\n            \"c\": [500.0, 500.02], \"sigma\": 0.01,\n            \"T\": 0.03, \"lambda_u\": 0.5, \"lambda_c\": 0.25, \"M\": 1e6\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_assignment_problem(**case)\n        results.append(result)\n\n    # Format the final output string exactly as specified.\n    # str() on a list provides the required internal spacing.\n    # The join then combines the list strings with just a comma.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef solve_assignment_problem(m, w, c, sigma, T, lambda_u, lambda_c, M):\n    \"\"\"\n    Solves the peak-to-fragment assignment problem for a single case.\n\n    Args:\n        m (list): Observed peak masses.\n        w (list): Normalized peak intensities.\n        c (list): Predicted candidate masses.\n        sigma (float): Mass accuracy standard deviation.\n        T (float): Matching tolerance.\n        lambda_u (float): Penalty coefficient for unassigned peaks.\n        lambda_c (float): Model complexity penalty coefficient.\n        M (float): Large infeasibility constant.\n\n    Returns:\n        list: A list containing the minimized objective value (float) and\n              the assignment vector (list of ints).\n    \"\"\"\n    n = len(m)\n    k = len(c)\n\n    # The cost matrix will have n rows (peaks) and k+n columns\n    # (k candidates + n unassigned slots).\n    cost_matrix = np.full((n, k + n), M)\n\n    # Part 1: Calculate assignment costs (first k columns)\n    if k > 0:\n        m_arr = np.array(m)[:, np.newaxis]\n        c_arr = np.array(c)[np.newaxis, :]\n        w_arr = np.array(w)[:, np.newaxis]\n\n        # Mass deviations for all (peak, candidate) pairs\n        delta = m_arr - c_arr\n\n        # Negative log-likelihood calculation\n        # l(d;s) = 0.5 * (d/s)^2 + ln(sqrt(2*pi)*s)\n        log_term = np.log(np.sqrt(2 * np.pi) * sigma)\n        neg_log_likelihood = 0.5 * (delta / sigma)**2 + log_term\n\n        # Full assignment cost, including intensity weight and complexity penalty\n        assignment_costs = w_arr * neg_log_likelihood + lambda_c\n\n        # Apply tolerance T to filter infeasible assignments\n        feasible_mask = np.abs(delta) <= T\n        cost_matrix[:, :k] = np.where(feasible_mask, assignment_costs, M)\n\n    # Part 2: Calculate unassignment costs (last n columns)\n    # Each peak i has a dedicated unassigned slot at column k+i.\n    unassignment_costs = lambda_u * np.array(w)\n    for i in range(n):\n        cost_matrix[i, k + i] = unassignment_costs[i]\n\n    # Solve the linear sum assignment problem\n    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n\n    # Calculate the total cost from the optimal assignment\n    min_objective_value = cost_matrix[row_ind, col_ind].sum()\n\n    # Construct the assignment vector from the column indices\n    # col_ind[i] gives the column chosen for peak (row) i.\n    # If col_ind[i] < k, it's an assignment to candidate col_ind[i].\n    # Otherwise, the peak is unassigned.\n    assignment_vector = np.full(n, -1, dtype=int)\n    assigned_mask = col_ind < k\n    assignment_vector[assigned_mask] = col_ind[assigned_mask]\n\n    return [min_objective_value, assignment_vector.tolist()]\n\nif __name__ == '__main__':\n    solve()\n```"
        }
    ]
}