{
    "hands_on_practices": [
        {
            "introduction": "Before estimating a parameter, it's crucial to understand the theoretical limits on our precision. This exercise introduces the Cramér-Rao Lower Bound, a cornerstone of statistical inference that provides a benchmark for the minimum possible variance of any unbiased estimator. By deriving this bound for a common Poisson defect model in materials science, you will gain a concrete understanding of how the quantity of data and the underlying physical process govern the fundamental limits of what can be known from an experiment .",
            "id": "3480528",
            "problem": "In a computational materials science study of thin-film reliability, suppose defects nucleate on the film surface according to a spatially uniform Poisson point process with rate parameter $\\lambda$ per unit area. A set of $m$ independent Scanning Electron Microscope (SEM) micrographs with areas $a_{1},a_{2},\\dots,a_{m}$ are collected, and on each micrograph $i$ the observed defect count is $x_{i}$. Under the Poisson point process assumption, the count $x_{i}$ is modeled as a Poisson random variable with mean $\\lambda a_{i}$.\n\nStarting from the fundamental definitions of the Poisson probability mass function, the likelihood for independent observations, the log-likelihood, and Fisher information as the expected negative second derivative of the log-likelihood with respect to the parameter, derive an analytic expression for the Fisher information $I(\\lambda)$ about $\\lambda$ contained in the $m$ observations $\\{x_{i}\\}_{i=1}^{m}$ with known areas $\\{a_{i}\\}_{i=1}^{m}$. Then, using the definition of the Cramér–Rao Lower Bound (CRLB), derive a closed-form expression for the minimum achievable variance of any unbiased estimator of $\\lambda$ under this model.\n\nExpress your final results as analytic expressions. No rounding is required, and no units should be included in your final expressions.",
            "solution": "The user wants me to solve this problem.\n\n### Step 1: Extract Givens\n- A spatially uniform Poisson point process governs defect nucleation with rate parameter $\\lambda$ per unit area.\n- A set of $m$ independent Scanning Electron Microscope (SEM) micrographs are collected.\n- The areas of the micrographs are $a_{1}, a_{2}, \\dots, a_{m}$.\n- The observed defect count on micrograph $i$ is $x_{i}$.\n- The count $x_{i}$ is a Poisson random variable with mean $\\lambda a_{i}$.\n- Derive the Fisher information $I(\\lambda)$ for the observations $\\{x_{i}\\}_{i=1}^{m}$.\n- Derive the Cramér–Rao Lower Bound (CRLB) for the minimum achievable variance of any unbiased estimator of $\\lambda$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. The use of a Poisson point process to model defects is a standard and well-established method in materials science and related fields. The statistical model is clearly defined: $m$ independent observations from Poisson distributions with means proportional to known areas. The questions asked—derivation of Fisher information and the Cramér–Rao Lower Bound—are standard tasks in statistical inference and are well-posed. The problem is self-contained, objective, and does not violate any fundamental principles. There are no contradictions, ambiguities, or missing information.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full, reasoned solution will be provided.\n\nThe derivation proceeds from the fundamental definitions as required.\n\nFirst, we define the probability mass function (PMF) for a single observation $x_i$. The problem states that $x_i$ follows a Poisson distribution with mean $\\mu_i = \\lambda a_i$. The PMF for a Poisson random variable $X$ with mean $\\mu$ is given by $P(X=k) = \\frac{\\exp(-\\mu)\\mu^k}{k!}$. Therefore, for our observation $x_i$, the PMF is:\n$$ P(X_i = x_i | \\lambda) = \\frac{\\exp(-\\lambda a_i) (\\lambda a_i)^{x_i}}{x_i!} $$\nThe parameter we are interested in is $\\lambda$, and the areas $a_i$ are known constants.\n\nNext, we construct the likelihood function $L(\\lambda)$ for the set of $m$ independent observations $\\{x_i\\}_{i=1}^{m}$. Since the observations are independent, the total likelihood is the product of the individual probabilities:\n$$ L(\\lambda; \\{x_i\\}) = \\prod_{i=1}^{m} P(X_i = x_i | \\lambda) = \\prod_{i=1}^{m} \\frac{\\exp(-\\lambda a_i) (\\lambda a_i)^{x_i}}{x_i!} $$\n\nFor mathematical convenience, we work with the log-likelihood function, $\\ell(\\lambda)$, which is the natural logarithm of the likelihood function:\n$$ \\ell(\\lambda) = \\ln(L(\\lambda)) = \\ln\\left(\\prod_{i=1}^{m} \\frac{\\exp(-\\lambda a_i) (\\lambda a_i)^{x_i}}{x_i!}\\right) $$\nUsing the properties of logarithms, the product becomes a sum:\n$$ \\ell(\\lambda) = \\sum_{i=1}^{m} \\ln\\left(\\frac{\\exp(-\\lambda a_i) (\\lambda a_i)^{x_i}}{x_i!}\\right) = \\sum_{i=1}^{m} \\left( \\ln(\\exp(-\\lambda a_i)) + \\ln((\\lambda a_i)^{x_i}) - \\ln(x_i!) \\right) $$\n$$ \\ell(\\lambda) = \\sum_{i=1}^{m} \\left( -\\lambda a_i + x_i \\ln(\\lambda a_i) - \\ln(x_i!) \\right) $$\nWe can expand the term $\\ln(\\lambda a_i)$ as $\\ln(\\lambda) + \\ln(a_i)$:\n$$ \\ell(\\lambda) = \\sum_{i=1}^{m} \\left( -\\lambda a_i + x_i (\\ln(\\lambda) + \\ln(a_i)) - \\ln(x_i!) \\right) $$\nSeparating the terms with respect to the summation:\n$$ \\ell(\\lambda) = -\\lambda \\sum_{i=1}^{m} a_i + \\ln(\\lambda) \\sum_{i=1}^{m} x_i + \\sum_{i=1}^{m} x_i \\ln(a_i) - \\sum_{i=1}^{m} \\ln(x_i!) $$\n\nTo find the Fisher information, we must compute the second derivative of the log-likelihood with respect to $\\lambda$. First, we compute the first derivative (the score function):\n$$ \\frac{\\partial \\ell(\\lambda)}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda} \\left( -\\lambda \\sum_{i=1}^{m} a_i + \\ln(\\lambda) \\sum_{i=1}^{m} x_i + \\sum_{i=1}^{m} x_i \\ln(a_i) - \\sum_{i=1}^{m} \\ln(x_i!) \\right) $$\nThe last two terms do not depend on $\\lambda$, so their derivatives are zero.\n$$ \\frac{\\partial \\ell(\\lambda)}{\\partial \\lambda} = -\\sum_{i=1}^{m} a_i + \\frac{1}{\\lambda} \\sum_{i=1}^{m} x_i $$\n\nNext, we compute the second derivative:\n$$ \\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2} = \\frac{\\partial}{\\partial \\lambda} \\left( -\\sum_{i=1}^{m} a_i + \\frac{1}{\\lambda} \\sum_{i=1}^{m} x_i \\right) $$\nThe first term is a constant with respect to $\\lambda$.\n$$ \\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2} = -\\frac{1}{\\lambda^2} \\sum_{i=1}^{m} x_i $$\n\nThe Fisher information $I(\\lambda)$ is defined as the negative of the expected value of the second derivative of the log-likelihood function:\n$$ I(\\lambda) = -E\\left[\\frac{\\partial^2 \\ell(\\lambda)}{\\partial \\lambda^2}\\right] $$\nSubstituting our expression for the second derivative:\n$$ I(\\lambda) = -E\\left[-\\frac{1}{\\lambda^2} \\sum_{i=1}^{m} x_i\\right] = \\frac{1}{\\lambda^2} E\\left[\\sum_{i=1}^{m} x_i\\right] $$\nBy the linearity of expectation, $E\\left[\\sum_{i=1}^{m} x_i\\right] = \\sum_{i=1}^{m} E[x_i]$. The problem states that $x_i$ is a Poisson random variable with mean $\\lambda a_i$, so $E[x_i] = \\lambda a_i$.\n$$ \\sum_{i=1}^{m} E[x_i] = \\sum_{i=1}^{m} \\lambda a_i = \\lambda \\sum_{i=1}^{m} a_i $$\nSubstituting this back into the expression for $I(\\lambda)$:\n$$ I(\\lambda) = \\frac{1}{\\lambda^2} \\left( \\lambda \\sum_{i=1}^{m} a_i \\right) = \\frac{1}{\\lambda} \\sum_{i=1}^{m} a_i $$\nThis is the analytic expression for the Fisher information.\n\nFinally, we derive the Cramér–Rao Lower Bound (CRLB). The CRLB states that for any unbiased estimator $\\hat{\\lambda}$ of the parameter $\\lambda$, its variance must satisfy the inequality:\n$$ \\mathrm{Var}(\\hat{\\lambda}) \\ge \\frac{1}{I(\\lambda)} $$\nThe minimum achievable variance for any unbiased estimator is therefore the inverse of the Fisher information.\n$$ \\mathrm{CRLB} = \\frac{1}{I(\\lambda)} = \\frac{1}{\\frac{1}{\\lambda} \\sum_{i=1}^{m} a_i} = \\frac{\\lambda}{\\sum_{i=1}^{m} a_i} $$\nThis is the closed-form expression for the minimum achievable variance of any unbiased estimator of $\\lambda$.\n\nThe two results are the Fisher information, $I(\\lambda) = \\frac{\\sum_{i=1}^{m} a_i}{\\lambda}$, and the Cramér–Rao Lower Bound, $\\frac{\\lambda}{\\sum_{i=1}^{m} a_i}$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sum_{i=1}^{m} a_i}{\\lambda}  \\frac{\\lambda}{\\sum_{i=1}^{m} a_i}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Moving from theoretical limits to practical application, this computational exercise explores how we can proactively design more informative experiments. You will use the Fisher Information Matrix and profile likelihood to assess how different experimental loading paths affect our ability to identify the parameters of a material's constitutive model. This practice demonstrates that statistical principles are not just for post-processing data, but are essential for designing efficient and effective computational or physical experiments that yield the most valuable information .",
            "id": "3480468",
            "problem": "You are given a one-dimensional phenomenological hardening law widely used in computational materials science to describe monotonic uniaxial plastic flow beyond initial yield. The constitutive relation is expressed as a mapping from strain to stress, where the plastic strain is approximated by the total strain under monotonic loading. The model is\n$$\n\\sigma(\\epsilon;\\theta) \\;=\\; \\sigma_s \\;-\\; \\big(\\sigma_s - \\sigma_0\\big)\\,e^{-b\\,\\epsilon},\n$$\nwhere $\\epsilon$ denotes strain (dimensionless), $\\sigma$ denotes Cauchy stress (in megapascals), and the parameter vector is $\\theta = (\\sigma_0,\\sigma_s,b)$ with $\\sigma_0$ the initial flow stress (in megapascals), $\\sigma_s$ the saturation stress (in megapascals), and $b$ (dimensionless) the saturation rate. Measurements of stress are corrupted by additive Gaussian noise so that the observed stresses satisfy\n$$\ny_i \\;=\\; \\sigma(\\epsilon_i;\\theta_{\\text{true}}) \\;+\\; \\eta_i,\\quad \\eta_i \\sim \\mathcal{N}(0,\\sigma_n^2),\n$$\nwhere $\\sigma_n$ is the standard deviation of the measurement noise (in megapascals). For identifiability assessment using probability and statistical inference basics, use the profile likelihood curvature for a single parameter and optimal experimental design criteria for the full parameter vector. Specifically:\n\n- Adopt the Gaussian likelihood and its log-likelihood, and define the Fisher Information Matrix (FIM) as the expected negative Hessian of the log-likelihood. For the above model under independent, identically distributed Gaussian noise, the FIM at $\\theta_{\\text{true}}$ reduces to\n$$\n\\mathcal{I}(\\theta_{\\text{true}}) \\;=\\; \\frac{1}{\\sigma_n^2}\\,J(\\theta_{\\text{true}})^\\top J(\\theta_{\\text{true}}),\n$$\nwhere $J(\\theta_{\\text{true}})$ is the Jacobian matrix of $\\sigma(\\epsilon;\\theta)$ with respect to $\\theta$ evaluated at $\\theta_{\\text{true}}$ and at the design strains $\\{\\epsilon_i\\}$.\n\n- To ensure dimensionless identifiability metrics, use parameter scaling. Define a diagonal scaling matrix $S=\\mathrm{diag}(1/s_{\\sigma_0},1/s_{\\sigma_s},1/s_b)$ and form the scaled Jacobian $J_s = J(\\theta_{\\text{true}})\\,S$. The scaled Fisher Information Matrix is\n$$\n\\mathcal{I}_s(\\theta_{\\text{true}}) \\;=\\; \\frac{1}{\\sigma_n^2}\\,J_s^\\top J_s.\n$$\nFrom $\\mathcal{I}_s$, compute the $D$-optimality criterion $D = \\det\\big(\\mathcal{I}_s\\big)$ and the $E$-optimality criterion $E = \\lambda_{\\min}\\big(\\mathcal{I}_s\\big)$, where $\\lambda_{\\min}$ denotes the smallest eigenvalue.\n\n- For the profile likelihood curvature, treat $b$ as the parameter of interest and $\\sigma_0,\\sigma_s$ as nuisance parameters. Given synthetic noise-free data $y_i = \\sigma(\\epsilon_i;\\theta_{\\text{true}})$, define the profile sum of squared errors for a fixed $b$ as\n$$\n\\mathrm{SSE}(b) \\;=\\; \\min_{\\sigma_0,\\sigma_s}\\;\\sum_{i}\\Big(\\sigma(\\epsilon_i;\\sigma_0,\\sigma_s,b) - y_i\\Big)^2,\n$$\nand the corresponding profile log-likelihood as\n$$\n\\ell_p(b) \\;=\\; -\\frac{1}{2\\,\\sigma_n^2}\\,\\mathrm{SSE}(b).\n$$\nCompute the curvature of the profile log-likelihood at $b=b_{\\text{true}}$ via a second-order central finite difference\n$$\n\\kappa_b \\;=\\; -\\frac{\\ell_p(b_{\\text{true}}+h) - 2\\,\\ell_p(b_{\\text{true}}) + \\ell_p(b_{\\text{true}}-h)}{h^2},\n$$\nwith a small step $h0$. This curvature is a dimensionless local identifiability measure for $b$ under the given design.\n\nUse the following fundamental base for the derivation and algorithmic design:\n- The definition of the Gaussian likelihood and its log-likelihood for independent, identically distributed observations.\n- The Fisher Information Matrix for Gaussian noise models and its relation to the Jacobian of the mean function.\n- The definition of the profile likelihood by optimizing nuisance parameters for each fixed value of the parameter of interest.\n- Ordinary Least Squares (OLS) as the solution of linear regression in the nuisance parameters when the model is linear in those parameters for fixed $b$.\n\nTest Suite and Required Numerical Values:\n- True parameters $\\theta_{\\text{true}} = (\\sigma_0,\\sigma_s,b) = (\\,300.0,\\,600.0,\\,12.0\\,)$, where the stresses are in megapascals and $b$ is dimensionless.\n- Noise standard deviation $\\sigma_n = 5.0$ (in megapascals).\n- Parameter scaling $s_{\\sigma_0} = 300.0$, $s_{\\sigma_s} = 600.0$, $s_b = 12.0$ so that the scaled Fisher Information Matrix is dimensionless.\n- Finite difference step $h=0.5$ (dimensionless) for the profile curvature in $b$.\n- Loading path designs (strain arrays, dimensionless):\n  1. Limited early hardening: $\\epsilon = [\\,0.0000,\\;0.0050,\\;0.0100,\\;0.0150,\\;0.0200\\,]$.\n  2. Mid-range hardening: $\\epsilon = [\\,0.0000,\\;0.0200,\\;0.0500,\\;0.0800,\\;0.1000\\,]$.\n  3. Wide-range saturation: $\\epsilon = [\\,0.0000,\\;0.0200,\\;0.0500,\\;0.1000,\\;0.2000\\,]$.\n\nTasks for each design in the test suite:\n- Compute the scaled Fisher Information Matrix $\\mathcal{I}_s(\\theta_{\\text{true}})$, the $D$-optimality $D$, and the $E$-optimality $E$.\n- Compute the profile log-likelihood curvature $\\kappa_b$ at $b=12.0$ using $h=0.5$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the three designs as a comma-separated list enclosed in square brackets, in the order\n$$\n[\\,D_1,\\,E_1,\\,\\kappa_{b,1},\\,D_2,\\,E_2,\\,\\kappa_{b,2},\\,D_3,\\,E_3,\\,\\kappa_{b,3}\\,],\n$$\nwhere the subscripts indicate the design number ($1$, $2$, $3$). All outputs must be floating-point numbers with no units.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in established principles of continuum mechanics and statistical inference, is mathematically well-posed with all necessary data and conditions provided, and is expressed in objective, unambiguous language. The tasks are clearly defined and lead to a unique, verifiable solution. We may therefore proceed with the derivation and computational implementation.\n\nThe solution requires the computation of two sets of identifiability metrics for three different experimental designs (strain paths). The first set, derived from the Fisher Information Matrix (FIM), assesses the overall identifiability of the parameter vector $\\theta = (\\sigma_0, \\sigma_s, b)$. The second, the profile likelihood curvature, assesses the local identifiability of a single parameter, $b$, while accounting for the effect of the nuisance parameters $\\sigma_0$ and $\\sigma_s$.\n\nThe phenomenological hardening model is given by:\n$$\n\\sigma(\\epsilon;\\theta) = \\sigma_s - (\\sigma_s - \\sigma_0)e^{-b\\epsilon}\n$$\nThe true parameters are $\\theta_{\\text{true}} = (\\sigma_{0,\\text{true}}, \\sigma_{s,\\text{true}}, b_{\\text{true}}) = (300.0, 600.0, 12.0)$. The measurement noise standard deviation is $\\sigma_n = 5.0$.\n\nFirst, we address the calculation of the scaled Fisher Information Matrix, $\\mathcal{I}_s$, and the associated optimality criteria, $D$ and $E$. The FIM is a function of the Jacobian of the model with respect to the parameters. The Jacobian matrix, $J$, has elements $J_{ij} = \\frac{\\partial \\sigma(\\epsilon_i;\\theta)}{\\partial \\theta_j}$. The partial derivatives of $\\sigma$ with respect to the parameters $\\theta = (\\sigma_0, \\sigma_s, b)$ are:\n$$\n\\frac{\\partial \\sigma}{\\partial \\sigma_0} = e^{-b\\epsilon}\n$$\n$$\n\\frac{\\partial \\sigma}{\\partial \\sigma_s} = 1 - e^{-b\\epsilon}\n$$\n$$\n\\frac{\\partial \\sigma}{\\partial b} = (\\sigma_s - \\sigma_0)\\epsilon e^{-b\\epsilon}\n$$\nFor each experimental design, an $N \\times 3$ Jacobian matrix $J(\\theta_{\\text{true}})$ is constructed, where $N$ is the number of strain points in the design. The $i$-th row of $J$ consists of the three partial derivatives evaluated at strain $\\epsilon_i$ and the true parameters $\\theta_{\\text{true}}$.\n\nTo ensure the resulting metrics are dimensionless, we use parameter scaling. The scaling matrix is $S = \\mathrm{diag}(1/s_{\\sigma_0}, 1/s_{\\sigma_s}, 1/s_b)$, with scaling factors $s_{\\sigma_0} = 300.0$, $s_{\\sigma_s} = 600.0$, and $s_b = 12.0$. The scaled Jacobian is $J_s = J(\\theta_{\\text{true}}) S$.\n\nThe scaled Fisher Information Matrix is then computed as:\n$$\n\\mathcal{I}_s(\\theta_{\\text{true}}) = \\frac{1}{\\sigma_n^2} J_s^\\top J_s\n$$\nFrom $\\mathcal{I}_s$, we calculate two optimality criteria:\n1.  $D$-optimality: $D = \\det(\\mathcal{I}_s)$, which relates to the volume of the confidence ellipsoid for the parameters. A larger determinant implies better overall parameter identifiability.\n2.  $E$-optimality: $E = \\lambda_{\\min}(\\mathcal{I}_s)$, the smallest eigenvalue of $\\mathcal{I}_s$. This criterion relates to the worst-case variance of a parameter estimate and guards against marginal non-identifiability. A larger smallest eigenvalue is preferred.\n\nSecond, we compute the profile likelihood curvature $\\kappa_b$. This requires calculating the profile log-likelihood $\\ell_p(b)$. For a fixed value of $b$, we must find the values of $\\sigma_0$ and $\\sigma_s$ that minimize the sum of squared errors (SSE) between the model prediction and the data. The data, $y$, are specified to be noise-free, i.e., $y_i = \\sigma(\\epsilon_i; \\theta_{\\text{true}})$.\nThe model can be rewritten to highlight its linearity in $\\sigma_0$ and $\\sigma_s$:\n$$\n\\sigma(\\epsilon_i; \\sigma_0, \\sigma_s, b) = \\sigma_0 e^{-b\\epsilon_i} + \\sigma_s (1 - e^{-b\\epsilon_i})\n$$\nThis is a linear regression problem of the form $\\mathbf{y} = X\\boldsymbol{\\beta}$, where $\\mathbf{y}$ is the vector of stress data, $\\boldsymbol{\\beta} = [\\sigma_0, \\sigma_s]^\\top$ is the vector of nuisance parameters, and $X$ is the design matrix with rows $[e^{-b\\epsilon_i}, 1 - e^{-b\\epsilon_i}]$. The Ordinary Least Squares (OLS) solution for $\\boldsymbol{\\beta}$ is $\\hat{\\boldsymbol{\\beta}} = (X^\\top X)^{-1} X^\\top \\mathbf{y}$.\n\nWith the optimal $\\hat{\\boldsymbol{\\beta}}(b) = [\\hat{\\sigma}_0(b), \\hat{\\sigma}_s(b)]^\\top$ found for a given $b$, the profile sum of squared errors is:\n$$\n\\mathrm{SSE}(b) = \\sum_{i} \\left( y_i - \\sigma(\\epsilon_i; \\hat{\\sigma}_0(b), \\hat{\\sigma}_s(b), b) \\right)^2\n$$\nThe profile log-likelihood is then $\\ell_p(b) = -\\frac{1}{2\\sigma_n^2} \\mathrm{SSE}(b)$. We note that for $b = b_{\\text{true}}$, the OLS solution will be $\\hat{\\sigma}_0 = \\sigma_{0,\\text{true}}$ and $\\hat{\\sigma}_s = \\sigma_{s,\\text{true}}$, since the data are noise-free. This results in $\\mathrm{SSE}(b_{\\text{true}}) = 0$ and thus $\\ell_p(b_{\\text{true}}) = 0$.\n\nThe curvature $\\kappa_b$ at $b=b_{\\text{true}}$ is calculated using a second-order central finite difference with step $h=0.5$:\n$$\n\\kappa_b = -\\frac{\\ell_p(b_{\\text{true}}+h) - 2\\ell_p(b_{\\text{true}}) + \\ell_p(b_{\\text{true}}-h)}{h^2} = -\\frac{\\ell_p(b_{\\text{true}}+h) + \\ell_p(b_{\\text{true}}-h)}{h^2}\n$$\nThis curvature provides a measure of how sharply the likelihood function is peaked around the true value of $b$, indicating how well $b$ can be identified from the data, given the specified strain path. A higher curvature implies better identifiability.\n\nThe procedure is executed for each of the three strain path designs provided.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes identifiability metrics for a phenomenological hardening law\n    based on Fisher information and profile likelihood.\n    \"\"\"\n\n    # --- Problem Constants ---\n    # True parameters: sigma_0, sigma_s, b\n    theta_true = np.array([300.0, 600.0, 12.0])\n    # Noise standard deviation\n    sigma_n = 5.0\n    # Finite difference step for profile curvature\n    h_b = 0.5\n    # Parameter scaling factors for dimensionless FIM\n    s_factors = np.array([300.0, 600.0, 12.0])\n    scaling_matrix = np.diag(1.0 / s_factors)\n\n    # --- Test Suite: Experimental Designs (Strain Paths) ---\n    test_cases = [\n        # Design 1: Limited early hardening\n        np.array([0.0000, 0.0050, 0.0100, 0.0150, 0.0200]),\n        # Design 2: Mid-range hardening\n        np.array([0.0000, 0.0200, 0.0500, 0.0800, 0.1000]),\n        # Design 3: Wide-range saturation\n        np.array([0.0000, 0.0200, 0.0500, 0.1000, 0.2000]),\n    ]\n\n    results = []\n    \n    sigma0_true, sigmas_true, b_true = theta_true\n\n    for eps_design in test_cases:\n        # --- Part 1: Fisher Information Matrix  Optimality Criteria ---\n        \n        # Jacobian matrix J = [d(sigma)/d(sigma_0), d(sigma)/d(sigma_s), d(sigma)/d(b)]\n        J = np.zeros((len(eps_design), 3))\n        exp_term = np.exp(-b_true * eps_design)\n        \n        # Column 1: d(sigma)/d(sigma_0)\n        J[:, 0] = exp_term\n        # Column 2: d(sigma)/d(sigma_s)\n        J[:, 1] = 1.0 - exp_term\n        # Column 3: d(sigma)/d(b)\n        J[:, 2] = (sigmas_true - sigma0_true) * eps_design * exp_term\n\n        # Scaled Jacobian\n        J_s = J @ scaling_matrix\n\n        # Scaled Fisher Information Matrix\n        I_s = (1.0 / sigma_n**2) * (J_s.T @ J_s)\n\n        # D-optimality: determinant of I_s\n        D_opt = np.linalg.det(I_s)\n\n        # E-optimality: minimum eigenvalue of I_s\n        # Use eigvalsh for symmetric matrices\n        eigenvalues = np.linalg.eigvalsh(I_s)\n        E_opt = np.min(eigenvalues)\n\n        # --- Part 2: Profile Likelihood Curvature for b ---\n\n        # Noise-free stress data\n        # y_true = sigma(eps; theta_true)\n        y_true = sigmas_true - (sigmas_true - sigma0_true) * np.exp(-b_true * eps_design)\n        \n        def get_profile_log_likelihood(b_val):\n            \"\"\"\n            Calculates the profile log-likelihood for a given b by\n            optimizing over sigma_0 and sigma_s via OLS.\n            \"\"\"\n            # Design matrix for linear regression in (sigma_0, sigma_s)\n            # X_ij = [exp(-b*eps_i), 1 - exp(-b*eps_i)]\n            X = np.zeros((len(eps_design), 2))\n            exp_term_b = np.exp(-b_val * eps_design)\n            X[:, 0] = exp_term_b\n            X[:, 1] = 1.0 - exp_term_b\n\n            # OLS solution for beta = [sigma_0, sigma_s]\n            # beta_hat = (X.T @ X)^-1 @ X.T @ y\n            try:\n                # Add a small regularization term for stability if needed, though unlikely here.\n                # C = X.T @ X + 1e-15 * np.eye(2)\n                C = X.T @ X\n                beta_hat = np.linalg.inv(C) @ X.T @ y_true\n            except np.linalg.LinAlgError:\n                # In case of singularity, return a large negative number\n                return -np.inf\n            \n            # Predicted stress values with optimal sigma_0, sigma_s\n            y_pred = X @ beta_hat\n            \n            # Sum of squared errors\n            sse = np.sum((y_true - y_pred)**2)\n            \n            # Profile log-likelihood\n            return -sse / (2.0 * sigma_n**2)\n            \n        # Calculate profile log-likelihood at b_true +- h\n        # Note: ell_p(b_true) is 0 because data is noise-free\n        ell_p_plus_h = get_profile_log_likelihood(b_true + h_b)\n        ell_p_minus_h = get_profile_log_likelihood(b_true - h_b)\n\n        # Curvature using central finite difference\n        kappa_b = -(ell_p_plus_h + ell_p_minus_h) / h_b**2\n        \n        results.extend([D_opt, E_opt, kappa_b])\n\n    # Format the final output string\n    # E.g., \"[D1,E1,kappa1,D2,E2,kappa2,...]\"\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Simulation data, much like experimental data, rarely conforms to the idealized assumptions of simple statistical models, often featuring outliers and time-correlations. This practice challenges you to implement robust methods to analyze data from a simulated molecular dynamics experiment, moving beyond basic regression techniques. By applying an $M$-estimator to handle outliers and a Heteroskedasticity and Autocorrelation Consistent (HAC) variance estimator to account for time dependence, you will learn to draw valid statistical conclusions from complex, non-ideal data sets .",
            "id": "3480531",
            "problem": "Consider a small-strain uniaxial loading Molecular Dynamics (MD) experiment that produces a time series of strain $\\varepsilon_t$ and Cauchy stress $\\sigma_t$ at equally spaced time steps $t=1,\\dots,n$. In the linear elastic regime, assume the constitutive relation $\\sigma_t = \\theta_0 + \\theta_1 \\varepsilon_t + u_t$, where $\\theta_1$ is the elastic modulus and $\\theta_0$ is an offset (intercept), with $u_t$ a stationary, mean-zero, time-dependent disturbance due to thermal fluctuations and finite-size effects. The goal is to estimate $\\theta=(\\theta_0,\\theta_1)$ using an $M$-estimator based on the Huber loss and to compute the asymptotic sandwich variance under time dependence.\n\nUse the following fundamental base:\n- The linear elastic constitutive law at small strain: $\\sigma_t = \\theta_0 + \\theta_1 \\varepsilon_t + u_t$ with $\\varepsilon_t$ dimensionless and $\\sigma_t$ in gigapascals (GPa).\n- The definition of $M$-estimators: $\\hat\\theta$ minimizes $\\sum_{t=1}^n \\rho(u_t)$ for a convex loss $\\rho$, which is equivalent to solving $\\sum_{t=1}^n g_t(\\theta)=0$ with $g_t(\\theta)=x_t \\psi(u_t)$, where $x_t = (1,\\varepsilon_t)^\\top$, $u_t=\\sigma_t-x_t^\\top\\theta$, and $\\psi=\\rho'$.\n- The Huber loss with threshold $c0$: $\\rho(u) = \\frac{1}{2}u^2$ if $|u|\\le c$ and $\\rho(u)=c|u|-\\frac{1}{2}c^2$ if $|u|c$, so $\\psi(u)=u$ if $|u|\\le c$ and $\\psi(u)=c\\,\\mathrm{sign}(u)$ if $|u|c$, and $\\psi'(u)=1$ if $|u|\\le c$ and $\\psi'(u)=0$ if $|u|c$.\n- Under weak dependence and regularity, $\\sqrt{n}(\\hat\\theta-\\theta)\\overset{d}{\\to}\\mathcal{N}(0,V)$ with sandwich covariance $V=H^{-1}JH^{-1}$, where $H=\\mathbb{E}[\\psi'(u_t)x_t x_t^\\top]$ and $J$ is the long-run covariance of $g_t(\\theta)$, which admits a Heteroskedasticity and Autocorrelation Consistent (HAC) estimator (Newey–West) with Bartlett kernel: $$\\hat J=\\hat\\Gamma_0 + \\sum_{\\ell=1}^{L} w_\\ell\\big(\\hat\\Gamma_\\ell + \\hat\\Gamma_\\ell^\\top\\big),\\quad w_\\ell = 1-\\frac{\\ell}{L+1},$$ where $\\hat\\Gamma_\\ell=\\frac{1}{n}\\sum_{t=\\ell+1}^{n} g_t(\\hat\\theta)g_{t-\\ell}(\\hat\\theta)^\\top$ and $\\hat H=\\frac{1}{n}\\sum_{t=1}^{n}\\psi'(u_t(\\hat\\theta))x_t x_t^\\top$.\n\nImplement an iterative reweighted least squares procedure to compute $\\hat\\theta$ for the Huber loss: starting from ordinary least squares, update weights $w_t=\\psi(u_t)/u_t$ (with $w_t=1$ when $u_t=0$), and solve the weighted normal equations until convergence with absolute parameter change less than $10^{-10}$ or a maximum number of iterations of $100$.\n\nYou must generate synthetic MD data for the following test suite. In all cases, set the stress unit to gigapascals (GPa) and the strain to be dimensionless, and report variances in $\\mathrm{GPa}^2$. The disturbance $u_t$ follows an autoregressive process of order $1$ (Autoregressive (AR(1))) with coefficient $\\phi$, $u_t=\\phi u_{t-1} + \\eta_t$, where $\\eta_t$ are independent innovations. To ensure a stationary standard deviation of $u_t$ equal to the specified $\\sigma_u$ (in GPa), set $\\mathrm{Var}(\\eta_t)=\\sigma_u^2(1-\\phi^2)$. For Gaussian innovations, use $\\eta_t\\sim\\mathcal{N}(0,\\sigma_u^2(1-\\phi^2))$. For heavy-tailed innovations, use the Student $t$ distribution with $\\nu$ degrees of freedom, $\\eta_t = s\\,Z_t$, $Z_t\\sim t_\\nu$, with $s=\\sqrt{\\sigma_u^2(1-\\phi^2)\\cdot\\frac{\\nu-2}{\\nu}}$, so that $\\mathrm{Var}(\\eta_t)=\\sigma_u^2(1-\\phi^2)$. Simulate with a burn-in of $200$ steps to approximate stationarity, discarding the burn-in. The strain path is a linear ramp from $0$ to $\\varepsilon_{\\max}$ over $n$ steps.\n\nTest suite (each case is fully specified):\n- Case $1$: $n=500$, $\\varepsilon_{\\max}=0.01$, $\\theta_0=0.0$ GPa, $\\theta_1=70.0$ GPa, $\\phi=0.0$, $\\sigma_u=0.05$ GPa, Gaussian innovations, Huber threshold $c=0.15$ GPa, HAC bandwidth $L=0$, random seed $101$.\n- Case $2$: $n=500$, $\\varepsilon_{\\max}=0.01$, $\\theta_0=0.0$ GPa, $\\theta_1=70.0$ GPa, $\\phi=0.6$, $\\sigma_u=0.05$ GPa, Gaussian innovations, Huber threshold $c=0.15$ GPa, HAC bandwidth $L=10$, random seed $202$.\n- Case $3$: $n=500$, $\\varepsilon_{\\max}=0.01$, $\\theta_0=0.0$ GPa, $\\theta_1=70.0$ GPa, $\\phi=0.0$, $\\sigma_u=0.2$ GPa, Student $t$ innovations with $\\nu=3$ degrees of freedom, Huber threshold $c=0.10$ GPa, HAC bandwidth $L=0$, random seed $303$.\n- Case $4$: $n=50$, $\\varepsilon_{\\max}=0.005$, $\\theta_0=0.0$ GPa, $\\theta_1=150.0$ GPa, $\\phi=0.8$, $\\sigma_u=0.05$ GPa, Gaussian innovations, Huber threshold $c=0.15$ GPa, HAC bandwidth $L=5$, random seed $404$.\n\nFor each case:\n1. Simulate $\\varepsilon_t$ and $\\sigma_t$ as specified.\n2. Compute $\\hat\\theta=(\\hat\\theta_0,\\hat\\theta_1)$ via the Huber $M$-estimator.\n3. Compute the HAC sandwich covariance $\\hat V=\\hat H^{-1}\\hat J\\hat H^{-1}$ using the Bartlett kernel with the given maximum lag $L$.\n4. Extract and return the variance of the elastic modulus estimator, i.e., the $(2,2)$ entry $\\hat V_{22}$, expressed in $\\mathrm{GPa}^2$ as a floating-point number.\n\nYour program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets (e.g., $[\\text{result1},\\text{result2},\\text{result3},\\text{result4}]$). The entries must be the four variances $\\hat V_{22}$ corresponding to Cases $1$ through $4$ in order. No additional text should be printed.",
            "solution": "The user has requested the solution to a problem in computational materials science involving statistical inference. The problem is to estimate the elastic modulus of a material from simulated Molecular Dynamics (MD) data and to compute the variance of this estimate. The solution involves robust regression and heteroskedasticity and autocorrelation consistent (HAC) variance estimation. The problem statement is first validated for correctness and completeness.\n\n**Problem Validation**\n\nThe problem is deemed **valid**. It is scientifically sound, well-posed, and all necessary parameters and procedures are explicitly defined.\n-   **Scientific Grounding**: The use of a linear elastic model (Hooke's Law) for small strains is fundamental in solid mechanics. Modeling the disturbance term as a stationary autoregressive process (AR(1)) is a standard and physically plausible approach for thermal noise in MD simulations. The statistical methods employed—Huber $M$-estimation for robust regression and the Newey-West HAC estimator for variance—are well-established in statistics and econometrics. All physical parameters are realistic.\n-   **Well-Posedness**: The problem provides a complete specification for four distinct test cases, including all physical parameters, statistical model parameters, simulation parameters (like random seeds for reproducibility), and computational algorithm details (like convergence criteria). This ensures that a unique and meaningful solution can be computed for each case.\n-   **Objectivity and Clarity**: The problem is stated in precise, technical language, free from ambiguity or subjective claims. All mathematical entities and procedures are formally defined.\n\nThe problem is a well-designed computational task that correctly integrates principles from materials science, statistical mechanics, and advanced statistical inference.\n\n**Methodology**\n\nThe solution is implemented in three main stages for each test case: (1) Data Simulation, (2) Parameter Estimation using Iteratively Reweighted Least Squares (IRLS), and (3) Asymptotic Variance Calculation using a HAC sandwich estimator.\n\n**1. Data Simulation**\n\nThe synthetic data are generated to mimic an MD uniaxial loading experiment.\n-   **Strain Path ($\\varepsilon_t$)**: A deterministic linear strain ramp is generated, from $\\varepsilon=0$ to $\\varepsilon=\\varepsilon_{\\max}$ over $n$ time steps. The design matrix $X$ is constructed with columns of 1s and $\\varepsilon_t$, i.e., $x_t^\\top = [1, \\varepsilon_t]$.\n-   **Disturbance Process ($u_t$)**: The noise term $u_t$ is simulated as a stationary AR(1) process: $u_t = \\phi u_{t-1} + \\eta_t$. To ensure stationarity, the process is initiated from $u_0=0$ and run for a burn-in period of $200$ steps, which are then discarded. The innovations $\\eta_t$ are independent and identically distributed random variables.\n    -   The variance of the innovations, $\\mathrm{Var}(\\eta_t)$, is set to $\\sigma_u^2(1-\\phi^2)$ to ensure the stationary variance of the AR(1) process is $\\mathrm{Var}(u_t) = \\sigma_u^2$.\n    -   For Gaussian innovations, $\\eta_t \\sim \\mathcal{N}(0, \\sigma_u^2(1-\\phi^2))$.\n    -   For heavy-tailed innovations, a scaled Student's $t$-distribution is used: $\\eta_t = s Z_t$, where $Z_t \\sim t_\\nu$ and the scaling factor $s = \\sqrt{\\sigma_u^2(1-\\phi^2) (\\nu-2)/\\nu}$ is chosen to match the required variance.\n-   **Stress Path ($\\sigma_t$)**: The final stress time series is generated according to the linear constitutive relation: $\\sigma_t = \\theta_0 + \\theta_1 \\varepsilon_t + u_t$, using the true parameter values $(\\theta_0, \\theta_1)$ specified in each case.\n\n**2. Huber M-Estimation via IRLS**\n\nThe goal is to find the parameter estimate $\\hat\\theta$ that minimizes the sum of Huber losses, $\\sum_{t=1}^n \\rho(\\sigma_t - x_t^\\top\\theta)$. This is equivalent to solving the estimating equations $\\sum_{t=1}^n x_t \\psi(\\sigma_t - x_t^\\top\\theta) = 0$, where $\\psi = \\rho'$ is the Huber score function. This non-linear system is solved using an Iteratively Reweighted Least Squares (IRLS) algorithm.\n\nThe IRLS algorithm proceeds as follows:\n1.  Obtain an initial estimate $\\hat\\theta^{(0)}$ using Ordinary Least Squares (OLS) on the data $(\\sigma_t, x_t)$.\n2.  For each iteration $k=1, 2, \\dots$:\n    a.  Calculate the residuals: $u_t^{(k-1)} = \\sigma_t - x_t^\\top\\hat\\theta^{(k-1)}$.\n    b.  Calculate the weights: $w_t^{(k-1)} = \\psi(u_t^{(k-1)}) / u_t^{(k-1)}$. The Huber score function is $\\psi(u) = \\mathrm{clip}(u, -c, c)$. The weight becomes $w_t=1$ for $|u_t| \\le c$ (giving these observations full weight as in OLS) and $w_t=c/|u_t|$ for $|u_t|  c$ (down-weighting outliers). For $u_t=0$, the limiting weight is $1$.\n    c.  Solve for the updated parameters $\\hat\\theta^{(k)}$ by solving the weighted normal equations:\n        $$ (X^\\top W^{(k-1)} X) \\hat\\theta^{(k)} = X^\\top W^{(k-1)} Y $$\n        where $Y$ is the vector of stresses $\\sigma_t$ and $W^{(k-1)}$ is a diagonal matrix of the weights $w_t^{(k-1)}$.\n3.  The iteration continues until the maximum absolute change in the parameter vector, $\\|\\hat\\theta^{(k)} - \\hat\\theta^{(k-1)}\\|_\\infty$, is less than a tolerance of $10^{-10}$, or a maximum of $100$ iterations is reached.\n\n**3. Asymptotic Variance Calculation**\n\nUnder regularity conditions, the estimator $\\hat\\theta$ is asymptotically normal, with $\\sqrt{n}(\\hat\\theta - \\theta)$ converging in distribution to $\\mathcal{N}(0, V)$. The asymptotic covariance matrix $V$ has a \"sandwich\" form, $V = H^{-1} J H^{-1}$. We compute its empirical estimate $\\hat V = \\hat H^{-1} \\hat J \\hat H^{-1}$.\n\n-   **\"Bread\" Matrix ($\\hat H$)**: The matrix $H = \\mathbb{E}[\\psi'(u_t)x_t x_t^\\top]$ is estimated by its sample analogue:\n    $$ \\hat H = \\frac{1}{n} \\sum_{t=1}^n \\psi'(\\hat u_t) x_t x_t^\\top $$\n    where $\\hat u_t = \\sigma_t - x_t^\\top\\hat\\theta$ are the final residuals. For the Huber function, $\\psi'(u)$ is an indicator function, $\\mathbb{I}(|u| \\le c)$.\n\n-   **\"Meat\" Matrix ($\\hat J$)**: The matrix $J$ is the long-run variance of the moment condition scores $g_t(\\theta) = x_t \\psi(u_t)$. It is estimated using the Heteroskedasticity and Autocorrelation Consistent (HAC) estimator of Newey and West with a Bartlett kernel:\n    $$ \\hat J = \\hat\\Gamma_0 + \\sum_{\\ell=1}^{L} w_\\ell \\left( \\hat\\Gamma_\\ell + \\hat\\Gamma_\\ell^\\top \\right) $$\n    where:\n    -   The Bartlett weights are $w_\\ell = 1 - \\frac{\\ell}{L+1}$.\n    -   The lag-$L$ is the specified bandwidth.\n    -   The sample autocovariance matrices are $\\hat\\Gamma_\\ell = \\frac{1}{n} \\sum_{t=\\ell+1}^n g_t(\\hat\\theta) g_{t-\\ell}(\\hat\\theta)^\\top$.\n\nThe final required quantity is the variance of the elastic modulus estimator, specified as the $(2,2)$ element of the computed matrix $\\hat V$. This value, $\\hat V_{22}$, has units of $(\\text{GPa})^2$ as required.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t as student_t\n\ndef huber_psi(u, c):\n    \"\"\"Huber score function psi(u).\"\"\"\n    return np.clip(u, -c, c)\n\ndef huber_psi_prime(u, c):\n    \"\"\"Derivative of Huber score function psi'(u).\"\"\"\n    return (np.abs(u) = c).astype(float)\n\ndef huber_weights(u, c):\n    \"\"\"IRLS weights psi(u)/u.\"\"\"\n    with np.errstate(divide='ignore', invalid='ignore'):\n        weights = huber_psi(u, c) / u\n    # The limit of psi(u)/u as u-0 is 1.\n    weights[u == 0] = 1.0\n    return weights\n\ndef calculate_huber_m_estimator(X, y, c, max_iter=100, tol=1e-10):\n    \"\"\"\n    Computes Huber M-estimator using Iteratively Reweighted Least Squares (IRLS).\n    \"\"\"\n    # Initial estimate using Ordinary Least Squares (OLS)\n    try:\n        theta = np.linalg.lstsq(X, y, rcond=None)[0]\n    except np.linalg.LinAlgError:\n        theta = np.linalg.pinv(X) @ y\n    \n    for _ in range(max_iter):\n        theta_old = theta.copy()\n        \n        u = y - X @ theta\n        w = huber_weights(u, c)\n        \n        # Solve weighted least squares: (X.T W X) theta = X.T W y\n        X_w = X * w[:, np.newaxis]\n        XT_X_w = X.T @ X_w\n        XT_y_w = X_w.T @ y\n        \n        try:\n            theta = np.linalg.solve(XT_X_w, XT_y_w)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudo-inverse if matrix is singular\n            theta = np.linalg.pinv(XT_X_w) @ XT_y_w\n\n        if np.max(np.abs(theta - theta_old))  tol:\n            break\n            \n    return theta\n\ndef calculate_hac_variance(X, y, theta_hat, c, L):\n    \"\"\"\n    Computes the HAC sandwich covariance matrix V = H^-1 J H^-1.\n    \"\"\"\n    n, p = X.shape\n    u_hat = y - X @ theta_hat\n    \n    # --- Compute H_hat = (1/n) * sum(psi'(u_t) * x_t * x_t.T) ---\n    psi_prime_vals = huber_psi_prime(u_hat, c)\n    H_hat = (X.T * psi_prime_vals) @ X / n\n\n    # --- Compute J_hat (Newey-West) ---\n    # g_t = x_t * psi(u_t), stored as n x p matrix g_hat where each row is g_t.T\n    psi_vals = huber_psi(u_hat, c)\n    g_hat = X * psi_vals[:, np.newaxis]\n\n    # Gamma_0\n    J_hat = (g_hat.T @ g_hat) / n\n\n    if L  0:\n        for l in range(1, L + 1):\n            w_l = 1 - l / (L + 1)\n            # Gamma_l = (1/n) * sum_{t=l+1 to n} g_t * g_{t-l}.T\n            # This is computed efficiently using vectorized operations.\n            Gamma_l = (g_hat[l:].T @ g_hat[:-l]) / n\n            J_hat += w_l * (Gamma_l + Gamma_l.T)\n            \n    # --- Compute V_hat sandwich covariance ---\n    try:\n        H_hat_inv = np.linalg.inv(H_hat)\n    except np.linalg.LinAlgError:\n        H_hat_inv = np.linalg.pinv(H_hat)\n        \n    V_hat = H_hat_inv @ J_hat @ H_hat_inv\n    \n    return V_hat\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {'n': 500, 'eps_max': 0.01, 'theta0': 0.0, 'theta1': 70.0, 'phi': 0.0, 'sigma_u': 0.05, \n         'innov': 'gaussian', 'nu': None, 'c': 0.15, 'L': 0, 'seed': 101},\n        {'n': 500, 'eps_max': 0.01, 'theta0': 0.0, 'theta1': 70.0, 'phi': 0.6, 'sigma_u': 0.05, \n         'innov': 'gaussian', 'nu': None, 'c': 0.15, 'L': 10, 'seed': 202},\n        {'n': 500, 'eps_max': 0.01, 'theta0': 0.0, 'theta1': 70.0, 'phi': 0.0, 'sigma_u': 0.2, \n         'innov': 'student_t', 'nu': 3, 'c': 0.10, 'L': 0, 'seed': 303},\n        {'n': 50, 'eps_max': 0.005, 'theta0': 0.0, 'theta1': 150.0, 'phi': 0.8, 'sigma_u': 0.05, \n         'innov': 'gaussian', 'nu': None, 'c': 0.15, 'L': 5, 'seed': 404},\n    ]\n\n    results = []\n    burn_in_steps = 200\n\n    for case in test_cases:\n        n, eps_max = case['n'], case['eps_max']\n        theta_true = np.array([case['theta0'], case['theta1']])\n        phi, sigma_u = case['phi'], case['sigma_u']\n        innov_type, nu = case['innov'], case['nu']\n        c, L, seed = case['c'], case['L'], case['seed']\n        \n        rng = np.random.default_rng(seed)\n        \n        # 1. Simulate data\n        eps = np.linspace(0, eps_max, n)\n        X = np.vstack((np.ones(n), eps)).T\n        \n        total_len = n + burn_in_steps\n        \n        if innov_type == 'gaussian':\n            var_eta = sigma_u**2 * (1 - phi**2)\n            std_eta = np.sqrt(var_eta)\n            eta = rng.normal(0, std_eta, total_len)\n        else: # student_t\n            scale = np.sqrt(sigma_u**2 * (1 - phi**2) * ((nu - 2) / nu))\n            eta = scale * student_t.rvs(df=nu, size=total_len, random_state=rng)\n        \n        u_full = np.zeros(total_len)\n        for t in range(1, total_len):\n            u_full[t] = phi * u_full[t-1] + eta[t]\n        \n        u = u_full[burn_in_steps:]\n        sigma = X @ theta_true + u\n        \n        # 2. Compute Huber M-estimator\n        theta_hat = calculate_huber_m_estimator(X, sigma, c)\n        \n        # 3. Compute HAC sandwich covariance\n        V_hat = calculate_hac_variance(X, sigma, theta_hat, c, L)\n        \n        # 4. Extract variance of elastic modulus estimator (V_hat_22)\n        var_theta1 = V_hat[1, 1]\n        results.append(var_theta1)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}