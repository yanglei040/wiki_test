{
    "hands_on_practices": [
        {
            "introduction": "A crucial first step in any reliable computational materials science study is to ensure that the results are converged with respect to numerical parameters. This practice establishes a systematic protocol for managing the trade-off between computational cost and accuracy in Density Functional Theory (DFT) calculations. By working through this exercise , you will gain a quantitative understanding of how numerical errors arising from the plane-wave basis set, Brillouin zone sampling, and electronic smearing can be controlled to meet a target precision, a foundational skill for all first-principles simulations.",
            "id": "3458680",
            "problem": "Consider the cohesive energy of crystalline silicon defined as follows. The cohesive energy per atom, denoted $E_{\\mathrm{coh}}$, is the difference between the total energy of an isolated silicon atom $E_{\\mathrm{atom}}$ and the total energy per atom of bulk silicon $E_{\\mathrm{bulk}}/N$, i.e., $E_{\\mathrm{coh}} = E_{\\mathrm{atom}} - E_{\\mathrm{bulk}}/N$. In a plane-wave Density Functional Theory (DFT) calculation, the computed cohesive energy suffers from three primary controllable numerical errors: basis-set truncation due to finite plane-wave cutoff, incomplete Brillouin zone integration due to finite $k$-point density, and smearing-induced bias due to a nonzero electronic smearing width. Your task is to design a systematic and quantitative convergence protocol for $E_{\\mathrm{coh}}$ with respect to these three parameters.\n\nUse the following fundamental physical and numerical bases as starting points:\n- The Rayleigh–Ritz variational principle implies that a larger basis set (higher cutoff energy) cannot increase the ground state energy and that the basis-set incompleteness error of a plane-wave expansion for smooth pseudopotentials typically decays as a power law with the plane-wave kinetic energy cutoff $E_{\\mathrm{cut}}$.\n- Brillouin zone integration of smooth functions with Monkhorst–Pack sampling behaves as a Riemann sum whose integration error decays with the number of $k$-points $N_k$.\n- For a semiconductor, a small finite electronic smearing width $\\sigma$ (for example, Fermi–Dirac smearing) introduces a systematic bias in the internal energy that is analytic at small $\\sigma$ and scales quadratically with $\\sigma$.\n\nAdopt a surrogate, additive error model for the cohesive energy of silicon that is consistent with these bases:\n$$\n\\Delta E_{\\mathrm{coh}}(E_{\\mathrm{cut}}, N_k, \\sigma) \\;=\\; \\Delta E_{\\mathrm{pw}}(E_{\\mathrm{cut}}) \\;+\\; \\Delta E_{k}(N_k) \\;+\\; \\Delta E_{\\mathrm{sm}}(\\sigma),\n$$\nwith the following asymptotic scalings and positive prefactors:\n$$\n\\Delta E_{\\mathrm{pw}}(E_{\\mathrm{cut}}) \\;=\\; A_{\\mathrm{pw}} \\, E_{\\mathrm{cut}}^{-\\tfrac{3}{2}}, \\qquad\n\\Delta E_{k}(N_k) \\;=\\; \\frac{A_{k}}{N_k}, \\qquad\n\\Delta E_{\\mathrm{sm}}(\\sigma) \\;=\\; A_{\\mathrm{sm}} \\, \\sigma^{2}.\n$$\nAssume the following fixed material- and method-dependent constants appropriate for a norm-conserving or projector-augmented-wave silicon calculation with a smooth pseudopotential:\n- $A_{\\mathrm{pw}} = 12.0$ (with $E_{\\mathrm{cut}}$ in electronvolts, abbreviated eV),\n- $A_{k} = 0.6$ (in eV),\n- $A_{\\mathrm{sm}} = 0.1$ (in eV$^{-1}$),\nand a reference cohesive energy $E_{\\mathrm{coh}}^{\\mathrm{true}} = 4.63$ eV per atom used only to interpret the magnitude of errors. All energies must be expressed in eV, and $\\sigma$ must be expressed in eV.\n\nDesign a protocol that, given a target absolute error budget $\\tau$ and a triplet of nonnegative sub-budgets $(b_{\\mathrm{pw}}, b_{k}, b_{\\mathrm{sm}})$ satisfying $b_{\\mathrm{pw}} + b_{k} + b_{\\mathrm{sm}} \\le \\tau$, produces the smallest parameters that separately enforce the following three inequalities:\n$$\n\\Delta E_{\\mathrm{pw}}(E_{\\mathrm{cut}}) \\le b_{\\mathrm{pw}}, \\quad\n\\Delta E_{k}(N_k) \\le b_{k}, \\quad\n\\Delta E_{\\mathrm{sm}}(\\sigma) \\le b_{\\mathrm{sm}}.\n$$\nImpose the following discretization and reporting rules to make the results practically actionable and unambiguous:\n- $E_{\\mathrm{cut}}$ must be chosen as the smallest integer (in eV) that satisfies its inequality; report $E_{\\mathrm{cut}}$ in eV rounded to three decimal places.\n- $N_k$ must be chosen as the smallest positive integer that satisfies its inequality; report $N_k$ as an integer.\n- $\\sigma$ must be chosen as the largest value that still satisfies its inequality, then rounded down to the nearest $0.001$ eV to avoid exceeding the error sub-budget after rounding; report $\\sigma$ in eV rounded to three decimal places.\n\nFinally, compute and report, for each test case, the achieved total absolute error\n$$\n\\Delta E_{\\mathrm{tot}} \\;=\\; \\Delta E_{\\mathrm{pw}}(E_{\\mathrm{cut}}) + \\Delta E_{k}(N_k) + \\Delta E_{\\mathrm{sm}}(\\sigma)\n$$\nusing the actually selected $(E_{\\mathrm{cut}}, N_k, \\sigma)$ after applying the discretization and rounding rules. Report $\\Delta E_{\\mathrm{tot}}$ in eV rounded to six decimal places.\n\nTest suite. Apply your protocol to the following four cases, which together probe a general case, a tight boundary, a loose tolerance, and an imbalanced smearing budget. Each case is specified by $(\\tau, b_{\\mathrm{pw}}, b_{k}, b_{\\mathrm{sm}})$:\n- Case A (general): $(\\tau, b_{\\mathrm{pw}}, b_{k}, b_{\\mathrm{sm}}) = (0.005, 0.0025, 0.0015, 0.001)$.\n- Case B (tight): $(\\tau, b_{\\mathrm{pw}}, b_{k}, b_{\\mathrm{sm}}) = (0.001, 0.0005, 0.00025, 0.00025)$.\n- Case C (loose): $(\\tau, b_{\\mathrm{pw}}, b_{k}, b_{\\mathrm{sm}}) = (0.020, 0.015, 0.003, 0.002)$.\n- Case D (imbalanced smearing): $(\\tau, b_{\\mathrm{pw}}, b_{k}, b_{\\mathrm{sm}}) = (0.003, 0.0015, 0.0014, 0.0001)$.\n\nYour program must compute, for each case, the recommended $(E_{\\mathrm{cut}}, N_k, \\sigma)$ and the achieved $\\Delta E_{\\mathrm{tot}}$ using the model above and the specified rounding rules.\n\nFinal output format. Your program should produce a single line of output containing the four case results aggregated into one Python-like list literal of lists, where each inner list is\n$$\n[E_{\\mathrm{cut}} \\text{ in eV (float with three decimals)},\\; N_k \\text{ (integer)},\\; \\sigma \\text{ in eV (float with three decimals)},\\; \\Delta E_{\\mathrm{tot}} \\text{ in eV (float with six decimals)}].\n$$\nFor example, the overall output must have the form\n$[[e_1, n_1, s_1, d_1],[e_2, n_2, s_2, d_2],[e_3, n_3, s_3, d_3],[e_4, n_4, s_4, d_4]]$\non a single line, with no additional text, and all energies in eV. Angles do not appear in this problem. Percentages must not be used anywhere; use decimals only.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of computational materials science, specifically Density Functional Theory ($DFT$). The error models provided for plane-wave cutoff energy, $k$-point sampling, and electronic smearing are standard, well-posed approximations used in convergence studies. The problem is self-contained, objective, and provides all necessary data and rules to arrive at a unique, verifiable solution. We may therefore proceed with the derivation of the computational protocol.\n\nThe objective is to determine the minimum computational parameters $(E_{\\mathrm{cut}}, N_k, \\sigma)$ that satisfy a set of individual error budgets $(b_{\\mathrm{pw}}, b_{k}, b_{\\mathrm{sm}})$. The error models are given as:\n$$\n\\Delta E_{\\mathrm{pw}}(E_{\\mathrm{cut}}) = A_{\\mathrm{pw}} \\, E_{\\mathrm{cut}}^{-\\tfrac{3}{2}}\n$$\n$$\n\\Delta E_{k}(N_k) = \\frac{A_{k}}{N_k}\n$$\n$$\n\\Delta E_{\\mathrm{sm}}(\\sigma) = A_{\\mathrm{sm}} \\, \\sigma^{2}\n$$\nwith constants $A_{\\mathrm{pw}} = 12.0$ (in units of $\\text{eV}^{5/2}$), $A_{k} = 0.6$ ($\\text{eV}$), and $A_{\\mathrm{sm}} = 0.1$ ($\\text{eV}^{-1}$). For each parameter, we will solve the corresponding inequality and apply the specified discretization and rounding rules.\n\nFirst, we determine the required plane-wave cutoff energy, $E_{\\mathrm{cut}}$. The condition is $\\Delta E_{\\mathrm{pw}}(E_{\\mathrm{cut}}) \\le b_{\\mathrm{pw}}$. Substituting the model and rearranging the inequality for $E_{\\mathrm{cut}}$ yields:\n$$\nA_{\\mathrm{pw}} \\, E_{\\mathrm{cut}}^{-\\tfrac{3}{2}} \\le b_{\\mathrm{pw}} \\implies E_{\\mathrm{cut}}^{\\tfrac{3}{2}} \\ge \\frac{A_{\\mathrm{pw}}}{b_{\\mathrm{pw}}} \\implies E_{\\mathrm{cut}} \\ge \\left(\\frac{A_{\\mathrm{pw}}}{b_{\\mathrm{pw}}}\\right)^{\\tfrac{2}{3}}\n$$\nSince $\\Delta E_{\\mathrm{pw}}$ is a monotonically decreasing function of $E_{\\mathrm{cut}}$, this inequality defines a lower bound for $E_{\\mathrm{cut}}$. The problem requires the smallest integer value for $E_{\\mathrm{cut}}$ that satisfies this condition. This is obtained by computing the lower bound and taking its ceiling.\n$$\nE_{\\mathrm{cut,final}} = \\left\\lceil \\left(\\frac{A_{\\mathrm{pw}}}{b_{\\mathrm{pw}}}\\right)^{\\tfrac{2}{3}} \\right\\rceil\n$$\n\nSecond, we determine the required number of $k$-points, $N_k$. The condition is $\\Delta E_{k}(N_k) \\le b_{k}$. Substituting the model for the $k$-point integration error and solving for $N_k$:\n$$\n\\frac{A_{k}}{N_k} \\le b_{k} \\implies N_k \\ge \\frac{A_{k}}{b_{k}}\n$$\nThe error $\\Delta E_{k}$ is a monotonically decreasing function of $N_k$. We require the smallest positive integer for $N_k$, which is found by taking the ceiling of the lower bound.\n$$\nN_{k,\\text{final}} = \\left\\lceil \\frac{A_{k}}{b_{k}} \\right\\rceil\n$$\n\nThird, we determine the electronic smearing width, $\\sigma$. The condition is $\\Delta E_{\\mathrm{sm}}(\\sigma) \\le b_{\\mathrm{sm}}$. The error $\\Delta E_{\\mathrm{sm}}$ increases with $\\sigma$, so this condition sets an upper bound on $\\sigma$.\n$$\nA_{\\mathrm{sm}} \\, \\sigma^{2} \\le b_{\\mathrm{sm}} \\implies \\sigma^2 \\le \\frac{b_{\\mathrm{sm}}}{A_{\\mathrm{sm}}} \\implies \\sigma \\le \\sqrt{\\frac{b_{\\mathrm{sm}}}{A_{\\mathrm{sm}}}}\n$$\nThe protocol asks for the largest value of $\\sigma$ satisfying this inequality, which is the upper bound itself, $\\sigma_{\\mathrm{max}} = \\sqrt{b_{\\mathrm{sm}}/A_{\\mathrm{sm}}}$. This value must then be rounded down to the nearest $0.001$ eV. This can be expressed mathematically as:\n$$\n\\sigma_{\\text{final}} = \\frac{\\left\\lfloor 1000 \\cdot \\sqrt{\\frac{b_{\\mathrm{sm}}}{A_{\\mathrm{sm}}}} \\right\\rfloor}{1000}\n$$\nThis rounding rule ensures the resulting error $\\Delta E_{\\mathrm{sm}}(\\sigma_{\\text{final}})$ does not exceed the sub-budget $b_{\\mathrm{sm}}$.\n\nFinally, after determining the discrete, actionable parameters $(E_{\\mathrm{cut,final}}, N_{k,\\text{final}}, \\sigma_{\\text{final}})$, we calculate the total achieved error, $\\Delta E_{\\mathrm{tot}}$, by substituting these values back into the individual error functions and summing the results.\n$$\n\\Delta E_{\\mathrm{tot}} = \\Delta E_{\\mathrm{pw}}(E_{\\mathrm{cut,final}}) + \\Delta E_{k}(N_{k,\\text{final}}) + \\Delta E_{\\mathrm{sm}}(\\sigma_{\\text{final}})\n$$\n$$\n\\Delta E_{\\mathrm{tot}} = A_{\\mathrm{pw}} \\, E_{\\mathrm{cut,final}}^{-\\tfrac{3}{2}} + \\frac{A_{k}}{N_{k,\\text{final}}} + A_{\\mathrm{sm}} \\, \\sigma_{\\text{final}}^{2}\n$$\nThe values of $(E_{\\mathrm{cut,final}}, N_{k,\\text{final}}, \\sigma_{\\text{final}})$ and the resulting $\\Delta E_{\\mathrm{tot}}$ are then reported according to the specified formatting rules for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the convergence parameters for DFT calculations based on a\n    surrogate error model and a set of test cases.\n    \"\"\"\n    \n    # Define the constants from the problem statement.\n    A_pw = 12.0  # Units: eV^(5/2)\n    A_k = 0.6    # Units: eV\n    A_sm = 0.1   # Units: eV^(-1)\n\n    # Define the test cases from the problem statement.\n    # Each case is (tau, b_pw, b_k, b_sm).\n    test_cases = [\n        # Case A (general)\n        (0.005, 0.0025, 0.0015, 0.001),\n        # Case B (tight)\n        (0.001, 0.0005, 0.00025, 0.00025),\n        # Case C (loose)\n        (0.020, 0.015, 0.003, 0.002),\n        # Case D (imbalanced smearing)\n        (0.003, 0.0015, 0.0014, 0.0001),\n    ]\n\n    all_results_formatted = []\n\n    for case in test_cases:\n        _, b_pw, b_k, b_sm = case\n\n        # 1. Determine E_cut\n        # E_cut >= (A_pw / b_pw)^(2/3)\n        # We need the smallest integer satisfying this.\n        e_cut_bound = (A_pw / b_pw)**(2.0/3.0)\n        e_cut_final = int(np.ceil(e_cut_bound))\n\n        # 2. Determine N_k\n        # N_k >= A_k / b_k\n        # We need the smallest positive integer satisfying this.\n        n_k_bound = A_k / b_k\n        n_k_final = int(np.ceil(n_k_bound))\n\n        # 3. Determine sigma\n        # sigma <= sqrt(b_sm / A_sm)\n        # We need the largest value, rounded down to the nearest 0.001 eV.\n        sigma_bound = np.sqrt(b_sm / A_sm)\n        sigma_final = np.floor(sigma_bound * 1000) / 1000.0\n\n        # 4. Calculate the total achieved error with the final parameters\n        delta_e_pw = A_pw * (e_cut_final**(-1.5))\n        delta_e_k = A_k / n_k_final\n        delta_e_sm = A_sm * (sigma_final**2)\n        delta_e_tot = delta_e_pw + delta_e_k + delta_e_sm\n\n        # Format the inner list as a string according to output rules.\n        inner_list_str = (\n            f\"[{e_cut_final:.3f},\"\n            f\"{n_k_final},\"\n            f\"{sigma_final:.3f},\"\n            f\"{delta_e_tot:.6f}]\"\n        )\n        all_results_formatted.append(inner_list_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results_formatted)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once we can confidently compute total energies, we can begin to investigate fundamental material properties, such as the energetics of defects. This exercise  guides you through the calculation of a vacancy formation energy, a key quantity that governs a material's thermodynamic and kinetic behavior. You will implement the supercell approach to model an isolated defect within a periodic crystal, learn how to approximate the chemical potential in a finite system, and observe the finite-size effects that are inherent to this powerful technique.",
            "id": "3458666",
            "problem": "You are tasked with implementing a computational workflow to estimate the vacancy formation energy in face-centered cubic (FCC) aluminum using a supercell approach and to examine finite-size convergence with respect to cell size. Work at fixed volume with periodic boundary conditions.\n\nFundamental bases to use:\n- The definition of vacancy formation as removing one atom from a perfect crystal and placing it into a reservoir with the same chemical potential, all at fixed volume and boundary conditions.\n- The total energy of a system as a sum over pairwise interactions within a physically justified cutoff and using the minimum image convention for periodic boundary conditions.\n- The relation between chemical potential and energy per atom in a large, homogeneous crystal at fixed volume.\n\nConstruct an FCC aluminum lattice as follows:\n- Use the conventional cubic lattice parameter $a_0 = 4.05\\,\\mathrm{\\AA}$.\n- The FCC basis in fractional coordinates of the conventional cubic cell is $\\{(0,0,0), (0, \\tfrac{1}{2}, \\tfrac{1}{2}), (\\tfrac{1}{2}, 0, \\tfrac{1}{2}), (\\tfrac{1}{2}, \\tfrac{1}{2}, 0)\\}$, with Cartesian positions obtained by multiplying by $a_0$.\n- Build a cubic supercell by replicating the conventional cell $n \\times n \\times n$ times, where $n$ is an integer.\n\nModel the interatomic interactions with a Morse pair potential:\n- The pair potential is $V(r) = D\\left(e^{-2 a (r - r_0)} - 2 e^{-a (r - r_0)}\\right)$.\n- Use $D = 0.35\\,\\mathrm{eV}$, $a = 1.2\\,\\mathrm{\\AA^{-1}}$, and $r_0 = a_0/\\sqrt{2}\\,\\mathrm{\\AA}$.\n- Impose a real-space cutoff $r_c = 8.0\\,\\mathrm{\\AA}$; interactions with $r \\ge r_c$ are neglected.\n- Use the minimum image convention under Born–von Karman periodic boundary conditions, with a cubic box of side length $L = n a_0$.\n\nDefine the perfect supercell energy $E_{\\mathrm{perf}}(n)$ as the total pair energy of all atoms in the $n \\times n \\times n$ FCC supercell. Define the defective supercell energy $E_{\\mathrm{def}}(n)$ by removing a single atom to create a vacancy in the same box. Remove the atom that is closest to the box center at $(L/2, L/2, L/2)$, measured in Cartesian coordinates.\n\nStarting only from the above bases and definitions, derive the expression for the vacancy formation energy $E_f(n)$ in terms of $E_{\\mathrm{perf}}(n)$ and $E_{\\mathrm{def}}(n)$ and implement it in your program. Clearly justify the role of the chemical potential and how it is approximated within this finite supercell method. Discuss how $E_f(n)$ is expected to converge as $n$ increases, based on elastic and electronic finite-size effects, and the scaling of defect–image interactions with $n$.\n\nYour program must:\n- Construct the FCC positions for each given $n$.\n- Compute $E_{\\mathrm{perf}}(n)$ and $E_{\\mathrm{def}}(n)$ as described.\n- Compute the vacancy formation energy $E_f(n)$ using your derived expression.\n- Use $\\mathrm{\\AA}$ for lengths and $\\mathrm{eV}$ for energies.\n- Express each final vacancy formation energy in electronvolts (eV) as a float rounded to three decimal places.\n\nTest suite:\n- Use the replication sizes $n \\in \\{1, 2, 3, 4\\}$ to illustrate finite-size convergence.\n- Edge case: $n = 1$ represents the minimal periodic image interaction dominated case.\n- Increasing $n$ values provide the trend towards the large-cell limit.\n\nFinal output format:\n- Your program should produce a single line of output containing the vacancy formation energies $[E_f(1), E_f(2), E_f(3), E_f(4)]$ as a comma-separated list enclosed in square brackets, with each entry rounded to three decimal places in $\\mathrm{eV}$ (for example, \"[0.123,0.456,0.789,1.234]\").",
            "solution": "The problem requires the calculation of the vacancy formation energy, $E_f$, in face-centered cubic (FCC) aluminum for different supercell sizes, $n$, to investigate finite-size convergence. This will be accomplished using a pairwise Morse potential within a fixed-volume, periodic supercell framework.\n\n**1. Theoretical Framework: Vacancy Formation Energy**\n\nThe vacancy formation energy, $E_f$, is defined as the energy cost to create a single vacancy in an otherwise perfect crystal. This process involves removing one atom from a bulk site and transferring it to a conceptual energy reservoir, which represents the state of an atom in a perfect, infinite crystal. The energy of an atom in this reservoir is the chemical potential, $\\mu$.\n\nLet the perfect supercell contain $N$ atoms and have a total energy of $E_{\\mathrm{perf}}(N)$. Upon removing one atom, the system becomes a defective supercell with $N-1$ atoms and a total energy of $E_{\\mathrm{def}}(N-1)$. The energy change, and thus the vacancy formation energy, is given by:\n$$E_f = [E_{\\mathrm{def}}(N-1) + \\mu] - E_{\\mathrm{perf}}(N)$$\n\nIn a computational model using finite supercells, the chemical potential $\\mu$ of the infinite crystal must be approximated. The most common and physically sound approximation is the average energy per atom in the \"best\" available perfect crystal, which is the perfect supercell itself. Therefore, we approximate $\\mu$ for a supercell containing $N$ atoms as:\n$$\\mu \\approx \\frac{E_{\\mathrm{perf}}(N)}{N}$$\n\nSubstituting this approximation into the definition of $E_f$ yields the formula for the vacancy formation energy within the supercell model:\n$$E_f(N) = E_{\\mathrm{def}}(N-1) + \\frac{E_{\\mathrm{perf}}(N)}{N} - E_{\\mathrm{perf}}(N)$$\n$$E_f(N) = E_{\\mathrm{def}}(N-1) - \\left(1 - \\frac{1}{N}\\right) E_{\\mathrm{perf}}(N) = E_{\\mathrm{def}}(N-1) - \\frac{N-1}{N} E_{\\mathrm{perf}}(N)$$\nFor an $n \\times n \\times n$ supercell of a conventional FCC cell (which contains $4$ atoms), the total number of atoms is $N = 4n^3$. The formula is thus applied with this value of $N$. This calculation is performed at a constant supercell volume, meaning no atomic relaxations around the vacancy or cell volume changes are considered, which is a common first-order approximation.\n\n**2. Computational Methodology**\n\nThe implementation follows a direct simulation of the physical system as defined.\n\n**FCC Supercell Construction**: An FCC supercell is constructed by replicating the conventional cubic cell, with lattice parameter $a_0 = 4.05\\,\\mathrm{\\AA}$, $n$ times along each of the three Cartesian axes. The conventional cell contains $4$ atoms at fractional coordinates $\\{(0,0,0), (0, \\tfrac{1}{2}, \\tfrac{1}{2}), (\\tfrac{1}{2}, 0, \\tfrac{1}{2}), (\\tfrac{1}{2}, \\tfrac{1}{2}, 0)\\}$. The resulting supercell is a cubic box of side length $L = n a_0$ containing $N = 4n^3$ atoms and subject to periodic boundary conditions.\n\n**Total Energy Calculation**: The total energy of a configuration of atoms is computed as the sum of all pairwise interactions, governed by the Morse potential:\n$$V(r) = D\\left(e^{-2 a (r - r_0)} - 2 e^{-a (r - r_0)}\\right)$$\nwith parameters $D = 0.35\\,\\mathrm{eV}$, $a = 1.2\\,\\mathrm{\\AA^{-1}}$, and $r_0 = a_0/\\sqrt{2}\\,\\mathrm{\\AA}$. The total energy $E_{\\mathrm{total}}$ for a system of atoms is:\n$$E_{\\mathrm{total}} = \\sum_{i<j} V(r_{ij})$$\nwhere the sum is over all unique pairs of atoms $(i, j)$. The interatomic distance $r_{ij}$ is calculated using the **minimum image convention** to account for periodic boundary conditions. For a displacement vector $\\Delta\\vec{x} = \\vec{x}_i - \\vec{x}_j$, the minimum image displacement is $\\Delta\\vec{x}' = \\Delta\\vec{x} - L \\cdot \\mathrm{round}(\\Delta\\vec{x}/L)$, where `round` is applied component-wise. Interactions are neglected for distances $r_{ij} \\ge r_c$, where the cutoff radius is $r_c = 8.0\\,\\mathrm{\\AA}$.\n\n**Workflow**:\n1.  For each replication factor $n \\in \\{1, 2, 3, 4\\}$, the positions of all $N = 4n^3$ atoms in the perfect supercell are generated.\n2.  The total energy of this perfect cell, $E_{\\mathrm{perf}}(n)$, is computed.\n3.  A vacancy is introduced by removing the single atom closest to the geometric center of the supercell, $(L/2, L/2, L/2)$. This creates a defective cell with $N-1$ atoms.\n4.  The total energy of the defective cell, $E_{\\mathrm{def}}(n)$, is computed using the same box size $L$ and parameters.\n5.  The vacancy formation energy $E_f(n)$ is calculated using the derived formula.\n\n**3. Convergence and Finite-Size Effects**\n\nThe use of a finite supercell with periodic boundary conditions introduces artificial interactions between the vacancy and its periodic images. These spurious interactions are a source of finite-size error. As the supercell size $n$ (and thus $L$) increases, the distance between a vacancy and its nearest images increases, causing the error to diminish. For a neutral defect like a vacancy in three dimensions, the leading-order elastic interaction energy is expected to decay as $1/L^3$, or equivalently, $1/n^3$. Therefore, we expect the calculated $E_f(n)$ to converge towards the infinite-cell limit, $E_f(\\infty)$, as $n$ increases. The test suite with $n = \\{1, 2, 3, 4\\}$ will illustrate this convergence trend. The case $n=1$ is expected to have the largest error due to strong interactions across the small periodic cell ($L=a_0=4.05\\,\\mathrm{\\AA}$).",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the vacancy formation energy in FCC aluminum for various supercell sizes\n    using a Morse pair potential and a fixed-volume supercell approach.\n    \"\"\"\n\n    # --- Problem Constants and Parameters ---\n    A0 = 4.05  # Lattice parameter in Angstroms\n    D_MORSE = 0.35  # Morse parameter D in eV\n    A_MORSE = 1.2   # Morse parameter a in Angstrom^-1\n    RC = 8.0     # Cutoff radius in Angstroms\n    R0_MORSE = A0 / np.sqrt(2.0)  # Morse parameter r0 in Angstroms\n\n    # --- Test Suite ---\n    N_REPLICATIONS = [1, 2, 3, 4]\n\n    # --- FCC Lattice Basis ---\n    # Basis vectors in fractional coordinates for a conventional cell\n    fcc_basis_frac = np.array([\n        [0.0, 0.0, 0.0],\n        [0.0, 0.5, 0.5],\n        [0.5, 0.0, 0.5],\n        [0.5, 0.5, 0.0]\n    ])\n    # Basis vectors in Cartesian coordinates\n    fcc_basis_cart = fcc_basis_frac * A0\n\n    def calculate_total_energy(positions, L, rc, D, a_p, r0):\n        \"\"\"\n        Calculates the total potential energy of a system of atoms using a Morse potential.\n        \n        Args:\n            positions (np.ndarray): Array of atomic positions (num_atoms, 3).\n            L (float): Side length of the cubic simulation box.\n            rc (float): Cutoff radius for the pair potential.\n            D, a_p, r0 (float): Morse potential parameters.\n        \n        Returns:\n            float: Total potential energy of the system in eV.\n        \"\"\"\n        num_atoms = positions.shape[0]\n        total_energy = 0.0\n        rc_sq = rc * rc\n\n        for i in range(num_atoms):\n            for j in range(i + 1, num_atoms):\n                # Calculate displacement vector between atom i and atom j\n                dr = positions[i] - positions[j]\n                \n                # Apply Minimum Image Convention for periodic boundary conditions\n                dr -= L * np.rint(dr / L)\n                \n                r_sq = np.dot(dr, dr)\n                \n                # Check if the pair is within the cutoff radius\n                if r_sq < rc_sq:\n                    r = np.sqrt(r_sq)\n                    # The r > 0 check is implicitly handled by the j > i loop\n                    exp_term = np.exp(-a_p * (r - r0))\n                    pair_energy = D * (exp_term**2 - 2.0 * exp_term)\n                    total_energy += pair_energy\n        return total_energy\n\n    vacancy_formation_energies = []\n\n    for n in N_REPLICATIONS:\n        # 1. Construct the perfect supercell by replicating the conventional cell\n        L = n * A0  # Supercell box length\n        \n        perfect_positions = []\n        for i in range(n):\n            for j in range(n):\n                for k in range(n):\n                    # Translation vector for this replica of the conventional cell\n                    translation = np.array([i, j, k]) * A0\n                    for basis_pos in fcc_basis_cart:\n                        perfect_positions.append(basis_pos + translation)\n        \n        perfect_positions = np.array(perfect_positions)\n        N = perfect_positions.shape[0]\n        \n        # 2. Calculate the total energy of the perfect supercell, E_perf(n)\n        E_perf = calculate_total_energy(perfect_positions, L, RC, D_MORSE, A_MORSE, R0_MORSE)\n        \n        # 3. Create a vacancy by removing the atom closest to the box center\n        box_center = np.array([L/2.0, L/2.0, L/2.0])\n        \n        # Find the index of the atom to remove\n        distances_to_center = np.linalg.norm(perfect_positions - box_center, axis=1)\n        idx_to_remove = np.argmin(distances_to_center)\n        \n        # Create the defective positions array by removing the identified atom\n        defective_positions = np.delete(perfect_positions, idx_to_remove, axis=0)\n\n        # 4. Calculate the total energy of the defective supercell, E_def(n)\n        E_def = calculate_total_energy(defective_positions, L, RC, D_MORSE, A_MORSE, R0_MORSE)\n\n        # 5. Calculate the vacancy formation energy E_f(n) using the derived formula\n        if N > 0:\n            E_f = E_def - ((N - 1) / N) * E_perf\n        else:\n            E_f = 0.0 # This case will not be reached for n >= 1\n        \n        vacancy_formation_energies.append(E_f)\n\n    # 6. Format and print the final output as specified\n    # The results are rounded to three decimal places.\n    formatted_results = [f\"{val:.3f}\" for val in vacancy_formation_energies]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the main function to solve the problem\nsolve()\n```"
        },
        {
            "introduction": "A primary goal of computational materials science is to connect quantum mechanical results with the intuitive concepts of chemistry. This practice  focuses on bridging this gap by using simulated structural data to calibrate and validate a chemical model. You will work with the Bond Valence Sum (BVS) model, a powerful tool for assigning oxidation states based on local bond lengths, and learn the practical skill of parameterizing such a model from a high-fidelity training set, demonstrating how first-principles calculations can inform and empower simpler, chemically insightful frameworks.",
            "id": "3458658",
            "problem": "You are tasked with developing and validating a computational model for bond valence in complex oxides with mixed valence, starting from first principles consistent with introductory solid state concepts and chemical bonding. The central requirement is to calibrate a two-parameter, reference-length-based mapping from distance to bond valence, and to use it to validate oxidation states from Density Functional Theory (DFT)-optimized local environments. The derivation must begin from the following foundational bases: the additivity of valence as a sum of individual bond orders, the requirement that each individual bond order is dimensionless and monotonically decreases with increasing bond length, the existence of a reference bond length, and the need for a single pair of universal parameters to describe mid-$3d$ transition metal cation to oxygen bonding within the given dataset.\n\nYour program must implement a calibration procedure that determines the parameter pair $(R_0, b)$ by minimizing the sum of squared deviations between the predicted valence sums and the provided target oxidation states over a scientifically plausible, DFT-optimized training dataset of local environments. After calibration, apply the model to a test suite to predict oxidation states and validate local bonding environments. All distances must be treated as physical quantities in ångströms (Å), and all valences are dimensionless. You must express $R_0$ and $b$ in Å and round them to $4$ decimals. Predicted oxidation states for the test suite must be rounded to $3$ decimals. Validation booleans should be computed by checking whether the absolute deviation between predicted valence and target oxidation state is less than or equal to a tolerance $\\tau$, which is set to $\\tau = 0.2$ in valence units.\n\nUse the following DFT-optimized training dataset of cation environments, each defined by a set of cation-oxygen distances in Ångström and an integer oxidation state:\n\n- Environment A (nominal $+3$ oxidation, octahedral): distances $[1.98, 2.01, 1.99, 2.00, 2.02, 2.00]$, target valence $3$.\n- Environment B (nominal $+2$ oxidation, octahedral): distances $[2.13, 2.17, 2.15, 2.14, 2.16, 2.15]$, target valence $2$.\n- Environment C (nominal $+4$ oxidation, octahedral): distances $[1.90, 1.89, 1.91, 1.92, 1.90, 1.89]$, target valence $4$.\n- Environment D (nominal $+3$ oxidation, Jahn-Teller distorted octahedral): distances $[1.92, 1.93, 1.91, 1.94, 2.17, 2.18]$, target valence $3$.\n- Environment E (nominal $+3$ oxidation, octahedral): distances $[1.91, 1.92, 1.93, 1.90, 1.91, 1.92]$, target valence $3$.\n- Environment F (nominal $+2$ oxidation, octahedral): distances $[2.05, 2.06, 2.04, 2.07, 2.05, 2.06]$, target valence $2$.\n\nYour program must calibrate $(R_0, b)$ from the above training dataset by minimizing the squared sum of deviations between predicted valence sums and the provided target valences. The calibrated $(R_0, b)$ must obey $R_0 > 0$ and $b > 0$, and remain within physically plausible bounds for mid-$3d$ transition metal cations bonded to oxygen; restrict $R_0$ to the interval $[1.40, 2.00]$ Å and $b$ to $[0.20, 0.60]$ Å.\n\nAfter calibration, validate against the following test suite (all distances in Å). For each test case, compute the predicted valence, round to $3$ decimals, and determine whether it is within $\\tau = 0.2$ of the target oxidation state (output a boolean):\n\n- Test Case $1$ (octahedral, nominal $+3$): $[2.00, 2.00, 2.00, 2.00, 2.00, 2.00]$, target valence $3$.\n- Test Case $2$ (octahedral, nominal $+2$): $[2.12, 2.14, 2.15, 2.16, 2.17, 2.18]$, target valence $2$.\n- Test Case $3$ (distorted octahedral, mixed short and long bonds, nominal $+3.5$): $[1.88, 1.90, 1.91, 1.92, 2.10, 2.12]$, target valence $3.5$.\n- Test Case $4$ (fivefold coordination, nominal $+3$): $[1.94, 1.95, 1.96, 1.97, 1.98]$, target valence $3$.\n- Test Case $5$ (tetrahedral coordination, nominal $+3$): $[1.84, 1.86, 1.87, 1.85]$, target valence $3$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order: the calibrated $R_0$ (Å, rounded to $4$ decimals), the calibrated $b$ (Å, rounded to $4$ decimals), followed by for each test case the predicted valence (rounded to $3$ decimals) and the validation boolean. Thus the output should be of the form $[R_0,b,v_1,\\mathrm{ok}_1,v_2,\\mathrm{ok}_2,v_3,\\mathrm{ok}_3,v_4,\\mathrm{ok}_4,v_5,\\mathrm{ok}_5]$. Angles are not used in this problem. No percentages appear; all quantities are dimensionless numbers or lengths in Å.",
            "solution": "The user-provided problem statement has been evaluated and is determined to be valid. The problem is scientifically grounded in the well-established Bond Valence Sum (BVS) model used in solid-state chemistry and materials science. It is well-posed, providing a complete set of data, constraints, and a clear objective for a numerical optimization task. The language is objective and the requirements are formalizable into a solvable computational problem.\n\nThe problem requires the calibration and application of a bond valence model. The foundational principle is that the formal oxidation state, or valence $V$, of a central cation can be approximated by summing the \"valences\" $s_i$ of each of its bonds to neighboring anions. The valence $s_i$ of an individual bond is a function of its length, $R_i$. The problem specifies a two-parameter model that is consistent with the standard exponential form of the BVS model:\n$$ s_i(R_i; R_0, b) = \\exp\\left(\\frac{R_0 - R_i}{b}\\right) $$\nHere, $s_i$ is the dimensionless valence of bond $i$, $R_i$ is the length of bond $i$ (in Ångström, Å), $R_0$ is a reference bond length for a bond of unit valence, and $b$ is a \"softness\" parameter, both also in units of Å.\n\nThe total valence sum, $V_j^{\\text{pred}}$, for a cation in environment $j$ with multiple bond lengths $\\{R_{ij}\\}$ is the sum over all its bonds:\n$$ V_j^{\\text{pred}}(R_0, b) = \\sum_{i} s_{ij} = \\sum_{i} \\exp\\left(\\frac{R_0 - R_{ij}}{b}\\right) $$\n\nThe first task is to calibrate the universal parameters $(R_0, b)$ for mid-$3d$ transition metal-oxygen bonds using the provided DFT-optimized training dataset. This is a parameter fitting problem. We aim to find the values of $R_0$ and $b$ that best reproduce the known target valences, $V_j^{\\text{target}}$, for the given training environments. This is achieved by minimizing an objective function, which is defined as the sum of squared errors (or deviations) between the predicted valence sums and the target valences for all $N$ environments in the training set:\n$$ \\mathcal{L}(R_0, b) = \\sum_{j=1}^{N} \\left( V_j^{\\text{pred}}(R_0, b) - V_j^{\\text{target}} \\right)^2 $$\nThe minimization must be performed subject to the specified physical bounds: $R_0 \\in [1.40, 2.00]$ Å and $b \\in [0.20, 0.60]$ Å.\n\nThis is a non-linear, bounded optimization problem in two dimensions. We will employ a numerical optimization algorithm to find the minimum of $\\mathcal{L}(R_0, b)$. Specifically, we use the `minimize` function from the `scipy.optimize` library, which provides robust algorithms for such tasks. The L-BFGS-B (Limited-memory Broyden–Fletcher–Goldfarb–Shanno with box constraints) method is well-suited for this purpose. An initial guess for $(R_0, b)$ within the defined bounds is required to start the optimization process. The result of this optimization will be the calibrated parameter pair $(R_0^*, b^*)$. These values will be rounded to $4$ decimal places for reporting.\n\nThe second task is to apply the calibrated model to a test suite of coordination environments. Using the determined parameters $(R_0^*, b^*)$, we calculate the predicted valence sum $V_k^{\\text{pred}}$ for each test case $k$:\n$$ V_k^{\\text{pred}} = \\sum_{i} \\exp\\left(\\frac{R_0^* - R_{ik}}{b^*}\\right) $$\nThe calculated $V_k^{\\text{pred}}$ is then rounded to $3$ decimal places.\n\nFinally, for each test case, we perform a validation check. The model's prediction is considered valid if the absolute difference between the predicted valence and the target valence is within a specified tolerance, $\\tau$. The problem defines $\\tau=0.2$. The validation check for test case $k$ is thus a boolean evaluation of the condition:\n$$ |\\text{round}(V_k^{\\text{pred}}, 3) - V_k^{\\text{target}}| \\le \\tau $$\nNote: to ensure maximum precision in the validation step, the comparison is made between the unrounded predicted valence and the target valence.\n\nThe final output is an aggregation of the calibrated parameters and the results from the test suite, formatted into a single comma-separated list as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Calibrates Bond Valence Sum parameters (R0, b) from a training dataset\n    and applies the calibrated model to a test suite.\n    \"\"\"\n    \n    # Define the training dataset from the problem statement.\n    # Each entry is a tuple: (list of cation-oxygen distances in Å, target integer oxidation state).\n    training_data = [\n        ([1.98, 2.01, 1.99, 2.00, 2.02, 2.00], 3),  # Environment A\n        ([2.13, 2.17, 2.15, 2.14, 2.16, 2.15], 2),  # Environment B\n        ([1.90, 1.89, 1.91, 1.92, 1.90, 1.89], 4),  # Environment C\n        ([1.92, 1.93, 1.91, 1.94, 2.17, 2.18], 3),  # Environment D\n        ([1.91, 1.92, 1.93, 1.90, 1.91, 1.92], 3),  # Environment E\n        ([2.05, 2.06, 2.04, 2.07, 2.05, 2.06], 2),  # Environment F\n    ]\n\n    def objective_function(params):\n        \"\"\"\n        Calculates the sum of squared errors between predicted valence sums\n        and target valences for the training dataset.\n        \n        Args:\n            params (list or tuple): A list [R0, b] of the parameters to optimize.\n            \n        Returns:\n            float: The sum of squared errors.\n        \"\"\"\n        R0, b = params\n        sse = 0.0\n        for distances, target_valence in training_data:\n            distances_arr = np.array(distances)\n            bond_valences = np.exp((R0 - distances_arr) / b)\n            predicted_valence_sum = np.sum(bond_valences)\n            sse += (predicted_valence_sum - target_valence)**2\n        return sse\n\n    # Define bounds and initial guess for the optimization.\n    bounds = [(1.40, 2.00), (0.20, 0.60)]\n    initial_guess = [1.70, 0.40] # A point in the middle of the search space.\n\n    # Perform the minimization.\n    result = minimize(\n        objective_function,\n        initial_guess,\n        method='L-BFGS-B',\n        bounds=bounds\n    )\n\n    # Extract and round the calibrated parameters.\n    R0_calibrated, b_calibrated = result.x\n    R0_rounded = round(R0_calibrated, 4)\n    b_rounded = round(b_calibrated, 4)\n\n    # Define the test suite and validation tolerance.\n    test_suite = [\n        # (list of distances, target valence)\n        ([2.00, 2.00, 2.00, 2.00, 2.00, 2.00], 3.0),\n        ([2.12, 2.14, 2.15, 2.16, 2.17, 2.18], 2.0),\n        ([1.88, 1.90, 1.91, 1.92, 2.10, 2.12], 3.5),\n        ([1.94, 1.95, 1.96, 1.97, 1.98], 3.0),\n        ([1.84, 1.86, 1.87, 1.85], 3.0),\n    ]\n    tolerance = 0.2\n\n    # Container for all final results.\n    final_results = [R0_rounded, b_rounded]\n\n    # Apply the calibrated model to the test suite.\n    for distances, target_valence in test_suite:\n        distances_arr = np.array(distances)\n        # Use un-rounded parameters for calculation to maintain precision.\n        bond_valences = np.exp((R0_calibrated - distances_arr) / b_calibrated)\n        predicted_valence_sum = np.sum(bond_valences)\n        \n        # Round the predicted valence for output.\n        valence_rounded = round(predicted_valence_sum, 3)\n        \n        # Perform validation check using the un-rounded predicted valence.\n        is_valid = abs(predicted_valence_sum - target_valence) <= tolerance\n        \n        # Append results for this test case.\n        final_results.extend([valence_rounded, is_valid])\n\n    # Format the final output as a single comma-separated list string.\n    # Python's str() for booleans produces \"True\" and \"False\", which is acceptable.\n    output_string = f\"[{','.join(map(str, final_results))}]\"\n\n    # Final print statement in the exact required format.\n    print(output_string)\n\nsolve()\n```"
        }
    ]
}