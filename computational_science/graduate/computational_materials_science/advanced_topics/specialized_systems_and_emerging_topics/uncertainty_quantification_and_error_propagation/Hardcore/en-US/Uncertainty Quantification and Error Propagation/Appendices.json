{
    "hands_on_practices": [
        {
            "introduction": "A ubiquitous challenge in computational materials science is extrapolating properties calculated in finite simulation cells to the thermodynamic limit. This practice guides you through applying Bayesian linear regression to a common finite-size scaling model for defect formation energies . By treating the model parameters as random variables and incorporating prior knowledge, you will learn to compute a robust estimate for the infinite-system energy, complete with a principled credible interval.",
            "id": "3499812",
            "problem": "You are given a finite-size scaling model for the defect formation energy in a crystalline material that is simulated using periodic supercells of size $N$ atoms. The model is\n$$\nE_f(N) = E_\\infty + \\frac{a}{N} + \\frac{b}{N^{3/2}},\n$$\nwhere $E_f(N)$ is the measured defect formation energy in electronvolts (eV), $E_\\infty$ is the thermodynamic-limit formation energy in eV, and $a$ and $b$ are finite-size correction coefficients. The parameters $a$ and $b$ are uncertain and must be treated as random variables. The observed energies $E_f(N)$ are subject to independent and identically distributed additive measurement noise modeled as Gaussian with zero mean and known variance $\\sigma^2$.\n\nYour task is to implement a program that performs uncertainty quantification and error propagation by computing the posterior distribution of $E_\\infty$ using Bayesian inference under the following assumptions:\n- The measurement model is linear in the parameters and has additive Gaussian noise, that is, $E_f(N_i)$ is modeled by $E_\\infty + a/N_i + b/N_i^{3/2}$ plus Gaussian noise of variance $\\sigma^2$.\n- The prior over the parameters is Gaussian. The uncertainty in $a$ and $b$ is encoded by finite prior variances. The prior over $E_\\infty$ is taken to be weakly informative or noninformative by using a large prior variance.\n\nStarting from the definitions of conditional probability and Bayes’ theorem and the assumption of Gaussian likelihood and Gaussian priors, derive an algorithm to compute the posterior mean and variance of $E_\\infty$. Then use it to compute the central $0.95$ credible interval for $E_\\infty$ for each test case below. All outputs must be expressed in electronvolts (eV) and rounded to six decimal places.\n\nThe program must implement the following test suite of parameter sets. Each test case provides:\n- A list of supercell sizes $N_i$.\n- A list of measured formation energies $E_f(N_i)$ in eV.\n- The known measurement noise variance $\\sigma^2$ in $\\text{eV}^2$.\n- The prior mean vector $\\mu_0 = [\\mu_{E_\\infty}, \\mu_a, \\mu_b]$ in eV and $\\text{eV}$-scaled units.\n- The diagonal prior covariance matrix $\\Sigma_0 = \\mathrm{diag}([\\sigma^2_{E_\\infty}, \\sigma^2_a, \\sigma^2_b])$ in $\\text{eV}^2$.\n\nTest suite:\n1. Happy path with multiple sizes and moderate noise:\n   - $N = [\\,64,\\,216,\\,512,\\,1000\\,]$\n   - $E_f(N) = [\\,4.529063,\\,3.731086,\\,3.443474,\\,3.360513\\,]$ in eV\n   - $\\sigma^2 = 0.0009$ in $\\text{eV}^2$\n   - $\\mu_0 = [\\,3.0,\\,100.0,\\,-200.0\\,]$\n   - $\\Sigma_0 = \\mathrm{diag}([\\,10000.0,\\,2500.0,\\,10000.0\\,])$\n2. Boundary condition with minimal sizes:\n   - $N = [\\,64,\\,216\\,]$\n   - $E_f(N) = [\\,4.534063,\\,3.716086\\,]$ in eV\n   - $\\sigma^2 = 0.0009$ in $\\text{eV}^2$\n   - $\\mu_0 = [\\,3.0,\\,100.0,\\,-200.0\\,]$\n   - $\\Sigma_0 = \\mathrm{diag}([\\,10000.0,\\,900.0,\\,3600.0\\,])$\n3. Ill-conditioned design with close, large sizes:\n   - $N = [\\,800,\\,900,\\,1000\\,]$\n   - $E_f(N) = [\\,4.162897,\\,4.099815,\\,4.084189\\,]$ in eV\n   - $\\sigma^2 = 0.0025$ in $\\text{eV}^2$\n   - $\\mu_0 = [\\,3.0,\\,200.0,\\,-400.0\\,]$\n   - $\\Sigma_0 = \\mathrm{diag}([\\,1000.0,\\,10000.0,\\,22500.0\\,])$\n4. Strongly biased prior scenario:\n   - $N = [\\,64,\\,216,\\,512,\\,1000\\,]$\n   - $E_f(N) = [\\,4.062500,\\,2.847531,\\,2.647191,\\,2.542649\\,]$ in eV\n   - $\\sigma^2 = 0.0004$ in $\\text{eV}^2$\n   - $\\mu_0 = [\\,2.0,\\,-100.0,\\,-300.0\\,]$\n   - $\\Sigma_0 = \\mathrm{diag}([\\,500.0,\\,100.0,\\,400.0\\,])$\n\nAlgorithmic requirements:\n- Formulate the linear design matrix based on the model $E_f(N) = E_\\infty + a/N + b/N^{3/2}$.\n- Use the Gaussian likelihood and Gaussian prior to obtain the Gaussian posterior over the parameters.\n- Extract the posterior mean and variance of $E_\\infty$.\n- Compute the central $0.95$ credible interval for $E_\\infty$ using the standard normal quantile.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of three floats $[\\,\\text{mean},\\,\\text{lower},\\,\\text{upper}\\,]$ for $E_\\infty$ in eV, all rounded to six decimal places. For example:\n$[\\, [\\,x_1,\\,\\ell_1,\\,u_1\\,],\\,[\\,x_2,\\,\\ell_2,\\,u_2\\,],\\,[\\,x_3,\\,\\ell_3,\\,u_3\\,],\\,[\\,x_4,\\,\\ell_4,\\,u_4\\,]\\,]$.",
            "solution": "The user has provided a problem in Bayesian inference applied to a finite-size scaling model in computational materials science. The problem is to derive and implement an algorithm to compute the posterior distribution of a model parameter, $E_\\infty$, and its credible interval.\n\n### Step 1: Problem Validation\n\n**Extraction of Givens:**\n- **Model Equation**: The defect formation energy $E_f(N)$ is modeled as a function of the supercell size $N$:\n$$ E_f(N) = E_\\infty + \\frac{a}{N} + \\frac{b}{N^{3/2}} $$\nwhere $\\theta = [E_\\infty, a, b]^T$ are the parameters to be inferred.\n- **Measurement Model**: The observed energies $E_f(N_i)$ are subject to independent and identically distributed (IID) additive Gaussian noise with mean $0$ and known variance $\\sigma^2$. This defines the likelihood function.\n- **Prior Distribution**: The prior over the parameters $\\theta$ is a multivariate Gaussian distribution, $p(\\theta) = \\mathcal{N}(\\theta|\\mu_0, \\Sigma_0)$, with a given prior mean vector $\\mu_0$ and a diagonal prior covariance matrix $\\Sigma_0$.\n- **Objective**: Compute the posterior mean and the central $0.95$ credible interval for the parameter $E_\\infty$.\n- **Test Cases**: Four specific test cases are provided, each with a set of supercell sizes $\\{N_i\\}$, corresponding measured energies $\\{E_f(N_i)\\}$, noise variance $\\sigma^2$, prior mean $\\mu_0$, and prior covariance $\\Sigma_0$.\n\n**Validation using Extracted Givens:**\n1.  **Scientific Grounding**: The problem is scientifically sound. The finite-size scaling model is a standard tool in condensed matter physics and materials science for extrapolating results from finite-system simulations to the thermodynamic limit. Bayesian linear regression is a fundamental and robust method in statistical inference. The chosen model and methodology are appropriate and well-established.\n2.  **Well-Posedness**: The problem is well-posed. It involves finding the posterior distribution for the parameters of a linear model with Gaussian noise and a Gaussian prior. This is a standard conjugate-prior scenario in Bayesian statistics, which guarantees that a unique, well-defined Gaussian posterior distribution exists. The provision of a full-rank prior covariance matrix $\\Sigma_0$ ensures that the posterior is well-defined even if the data alone are insufficient to identify all parameters (e.g., in Test Case 2 where the number of data points is less than the number of parameters).\n3.  **Objectivity**: The problem is formulated using precise mathematical and statistical language, free of subjective or ambiguous terminology.\n4.  **Completeness & Consistency**: All necessary data and parameters ($\\{N_i\\}$, $\\{E_f(N_i)\\}$, $\\sigma^2$, $\\mu_0$, $\\Sigma_0$) are provided for each test case. The units and dimensions are consistent throughout.\n5.  **No Other Flaws**: The problem is not trivial, metaphorical, or plagued by any of the other invalidity criteria. The test cases are well-designed to probe different aspects of the model, such as ill-conditioning and the influence of the prior.\n\n**Verdict:**\nThe problem is valid. It is a well-defined task in applied Bayesian statistics, grounded in a realistic scenario from computational materials science.\n\n### Step 2: Principled Solution\n\nThe problem can be framed as a Bayesian linear regression. The goal is to infer the parameter vector $\\theta = [E_\\infty, a, b]^T$ given a set of $M$ measurements.\n\n**1. Linear Model Formulation**\nLet the vector of observed energies be $\\mathbf{y} = [E_f(N_1), E_f(N_2), \\dots, E_f(N_M)]^T$. The model equation for each measurement $i=1, \\dots, M$ is:\n$$ E_f(N_i) = [1, N_i^{-1}, N_i^{-3/2}] \\begin{pmatrix} E_\\infty \\\\ a \\\\ b \\end{pmatrix} $$\nWe can write this in matrix form:\n$$ \\mathbf{y} = X\\theta + \\mathbf{\\epsilon} $$\nwhere $X$ is the $M \\times 3$ design matrix, and $\\mathbf{\\epsilon}$ is the vector of measurement noise. The $i$-th row of $X$ is given by $\\mathbf{x}_i^T = [1, N_i^{-1}, N_i^{-3/2}]$.\n\n**2. Likelihood Function**\nThe measurement noise $\\mathbf{\\epsilon}$ is assumed to be IID Gaussian with zero mean and variance $\\sigma^2$. This means $\\mathbf{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I)$, where $I$ is the $M \\times M$ identity matrix. The likelihood of observing the data $\\mathbf{y}$ given the parameters $\\theta$ is:\n$$ p(\\mathbf{y}|\\theta, \\sigma^2) = \\mathcal{N}(\\mathbf{y}|X\\theta, \\sigma^2 I) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2}(\\mathbf{y} - X\\theta)^T(\\mathbf{y} - X\\theta) \\right) $$\n\n**3. Prior Distribution**\nThe prior belief about the parameters $\\theta$ is modeled by a multivariate Gaussian distribution with mean $\\mu_0$ and covariance $\\Sigma_0$:\n$$ p(\\theta) = \\mathcal{N}(\\theta|\\mu_0, \\Sigma_0) \\propto \\exp\\left( -\\frac{1}{2}(\\theta - \\mu_0)^T\\Sigma_0^{-1}(\\theta - \\mu_0) \\right) $$\n\n**4. Posterior Distribution**\nUsing Bayes' theorem, the posterior distribution of the parameters is proportional to the product of the likelihood and the prior:\n$$ p(\\theta|\\mathbf{y}, \\sigma^2) \\propto p(\\mathbf{y}|\\theta, \\sigma^2) p(\\theta) $$\nSince both the likelihood (as a function of $\\theta$) and the prior are Gaussian, the posterior is also a Gaussian distribution, $p(\\theta|\\mathbf{y}, \\sigma^2) = \\mathcal{N}(\\theta|\\mu_p, \\Sigma_p)$. The logarithm of the posterior is (up to a constant):\n$$ \\ln p(\\theta|\\mathbf{y}) = -\\frac{1}{2\\sigma^2}(\\mathbf{y} - X\\theta)^T(\\mathbf{y} - X\\theta) -\\frac{1}{2}(\\theta - \\mu_0)^T\\Sigma_0^{-1}(\\theta - \\mu_0) + \\text{const.} $$\nExpanding and collecting terms in $\\theta$:\n$$ \\ln p(\\theta|\\mathbf{y}) = -\\frac{1}{2} \\left[ \\theta^T \\left(\\frac{1}{\\sigma^2}X^T X + \\Sigma_0^{-1}\\right)\\theta - 2\\theta^T \\left(\\frac{1}{\\sigma^2}X^T\\mathbf{y} + \\Sigma_0^{-1}\\mu_0\\right) \\right] + \\text{const.} $$\nBy completing the square for $\\theta$, we can identify the posterior covariance $\\Sigma_p$ and mean $\\mu_p$. The general form of a multivariate Gaussian exponent is $-\\frac{1}{2}(\\theta - \\mu_p)^T\\Sigma_p^{-1}(\\theta - \\mu_p) = -\\frac{1}{2}(\\theta^T\\Sigma_p^{-1}\\theta - 2\\theta^T\\Sigma_p^{-1}\\mu_p + \\dots)$. Comparing terms, we find the inverse posterior covariance (or precision matrix):\n$$ \\Sigma_p^{-1} = \\Sigma_0^{-1} + \\frac{1}{\\sigma^2}X^TX $$\nAnd the posterior mean:\n$$ \\mu_p = \\Sigma_p \\left( \\Sigma_0^{-1}\\mu_0 + \\frac{1}{\\sigma^2}X^T\\mathbf{y} \\right) $$\nwhere the posterior covariance is $\\Sigma_p = (\\Sigma_p^{-1})^{-1}$.\n\n**5. Marginal Posterior for $E_\\infty$ and Credible Interval**\nThe joint posterior $p(\\theta|\\mathbf{y}, \\sigma^2)$ is a multivariate Gaussian. The marginal distribution for any individual parameter is also Gaussian. We are interested in $E_\\infty$, which is the first component of the vector $\\theta$.\n- The posterior mean of $E_\\infty$ is the first element of the posterior mean vector $\\mu_p$:\n$$ \\mu_{p, E_\\infty} = (\\mu_p)_1 $$\n- The posterior variance of $E_\\infty$ is the first diagonal element of the posterior covariance matrix $\\Sigma_p$:\n$$ \\sigma^2_{p, E_\\infty} = (\\Sigma_p)_{11} $$\nThe posterior standard deviation is $\\sigma_{p, E_\\infty} = \\sqrt{\\sigma^2_{p, E_\\infty}}$.\n\nA central $0.95$ credible interval for $E_\\infty$ is constructed from its Gaussian posterior. The interval is given by:\n$$ [\\mu_{p, E_\\infty} - z \\cdot \\sigma_{p, E_\\infty}, \\quad \\mu_{p, E_\\infty} + z \\cdot \\sigma_{p, E_\\infty}] $$\nwhere $z$ is the quantile of the standard normal distribution corresponding to a cumulative probability of $1 - (1 - 0.95)/2 = 0.975$. This value is $z \\approx 1.959964$.\n\n**Algorithm Summary:**\nFor each test case:\n1.  Receive inputs: $\\{N_i\\}$, $\\{E_f(N_i)\\}$, $\\sigma^2$, $\\mu_0$, $\\Sigma_0$.\n2.  Construct the data vector $\\mathbf{y}$ and the design matrix $X$.\n3.  Compute the inverse prior covariance $\\Sigma_0^{-1}$.\n4.  Compute the matrix products $X^TX$ and $X^T\\mathbf{y}$.\n5.  Calculate the inverse posterior covariance $\\Sigma_p^{-1} = \\Sigma_0^{-1} + (1/\\sigma^2)X^TX$.\n6.  Invert to find the posterior covariance $\\Sigma_p = (\\Sigma_p^{-1})^{-1}$.\n7.  Calculate the posterior mean vector $\\mu_p = \\Sigma_p ( \\Sigma_0^{-1}\\mu_0 + (1/\\sigma^2)X^T\\mathbf{y} )$.\n8.  Extract the posterior mean $\\mu_{p, E_\\infty} = (\\mu_p)_1$ and variance $\\sigma^2_{p, E_\\infty} = (\\Sigma_p)_{11}$.\n9.  Calculate the standard deviation $\\sigma_{p, E_\\infty} = \\sqrt{\\sigma^2_{p, E_\\infty}}$.\n10. Determine the $z$-score for the $0.95$ credible interval.\n11. Compute the lower and upper bounds of the credible interval.\n12. Format the results as $[\\text{mean}, \\text{lower bound}, \\text{upper bound}]$, rounded to six decimal places.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to solve the Bayesian inference problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # 1. Happy path with multiple sizes and moderate noise\n        {\n            'N': [64., 216., 512., 1000.],\n            'Ef': [4.529063, 3.731086, 3.443474, 3.360513],\n            'sigma2': 0.0009,\n            'mu0': [3.0, 100.0, -200.0],\n            'Sigma0_diag': [10000.0, 2500.0, 10000.0]\n        },\n        # 2. Boundary condition with minimal sizes\n        {\n            'N': [64., 216.],\n            'Ef': [4.534063, 3.716086],\n            'sigma2': 0.0009,\n            'mu0': [3.0, 100.0, -200.0],\n            'Sigma0_diag': [10000.0, 900.0, 3600.0]\n        },\n        # 3. Ill-conditioned design with close, large sizes\n        {\n            'N': [800., 900., 1000.],\n            'Ef': [4.162897, 4.099815, 4.084189],\n            'sigma2': 0.0025,\n            'mu0': [3.0, 200.0, -400.0],\n            'Sigma0_diag': [1000.0, 10000.0, 22500.0]\n        },\n        # 4. Strongly biased prior scenario\n        {\n            'N': [64., 216., 512., 1000.],\n            'Ef': [4.062500, 2.847531, 2.647191, 2.542649],\n            'sigma2': 0.0004,\n            'mu0': [2.0, -100.0, -300.0],\n            'Sigma0_diag': [500.0, 100.0, 400.0]\n        }\n    ]\n\n    results = []\n    \n    # Z-score for a 95% credible interval\n    z_score = norm.ppf(1 - (1 - 0.95) / 2.0)\n\n    for case in test_cases:\n        N = np.array(case['N'], dtype=float)\n        y = np.array(case['Ef'], dtype=float)\n        sigma2 = case['sigma2']\n        mu0 = np.array(case['mu0'], dtype=float)\n        Sigma0 = np.diag(case['Sigma0_diag'])\n        \n        # 1. Construct the design matrix X\n        # The model is E_f(N) = E_inf * 1 + a * N^(-1) + b * N^(-3/2)\n        X = np.c_[np.ones_like(N), 1.0/N, 1.0/N**1.5]\n        \n        # 2. Calculate the posterior distribution parameters\n        \n        # Inverse of the prior covariance matrix\n        Sigma0_inv = np.linalg.inv(Sigma0)\n        \n        # Precision (inverse variance) from the likelihood\n        likelihood_precision = 1.0 / sigma2\n        \n        # Calculate the inverse of the posterior covariance matrix (posterior precision)\n        # Sigma_p^-1 = Sigma_0^-1 + (1/sigma^2) * X^T * X\n        XtX = X.T @ X\n        Sigma_p_inv = Sigma0_inv + likelihood_precision * XtX\n        \n        # Calculate the posterior covariance matrix by inverting the precision matrix\n        Sigma_p = np.linalg.inv(Sigma_p_inv)\n        \n        # Calculate the posterior mean vector\n        # mu_p = Sigma_p * (Sigma_0^-1 * mu_0 + (1/sigma^2) * X^T * y)\n        Xty = X.T @ y\n        term_in_paren = Sigma0_inv @ mu0 + likelihood_precision * Xty\n        mu_p = Sigma_p @ term_in_paren\n        \n        # 3. Extract marginal posterior for E_infinity\n        # E_infinity is the first parameter\n        mean_E_inf = mu_p[0]\n        var_E_inf = Sigma_p[0, 0]\n        std_E_inf = np.sqrt(var_E_inf)\n        \n        # 4. Compute the 95% credible interval\n        margin_of_error = z_score * std_E_inf\n        lower_bound = mean_E_inf - margin_of_error\n        upper_bound = mean_E_inf + margin_of_error\n        \n        results.append([mean_E_inf, lower_bound, upper_bound])\n\n    # Format the final output string as specified\n    formatted_results = []\n    for res in results:\n        # Round each value to six decimal places and format as a string\n        formatted_list = [f\"{v:.6f}\" for v in res]\n        formatted_results.append(f\"[{','.join(formatted_list)}]\")\n        \n    final_output = f\"[{','.join(formatted_results)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "Many physical phenomena are described by non-linear models, for which exact uncertainty quantification can be intractable. This exercise tackles this challenge within the context of nuclear reaction theory, using the Laplace approximation to perform a Bayesian fit of a non-linear resonance model to synthetic cross-section data . You will see firsthand how informative priors from theoretical calculations can constrain the model parameters and dramatically reduce the uncertainty of a low-energy extrapolation, a critical task in nuclear astrophysics.",
            "id": "3542512",
            "problem": "Implement an uncertainty-aware resonance matrix (R-matrix) fit for a simplified single-level, single-channel radiative capture model of the reaction $^{10}\\mathrm{B}(p,\\gamma)^{11}\\mathrm{C}$, and quantify how Gaussian prior constraints on the level energy and reduced widths sharpen the low-energy astrophysical $S$-factor extrapolation. Use the following fundamental bases and definitions as the starting point.\n\n1. Define the astrophysical $S$-factor by the identity\n$$\nS(E) \\equiv \\sigma(E)\\, E\\, e^{2\\pi \\eta(E)},\n$$\nwhere $E$ is the center-of-mass energy, $\\sigma(E)$ is the radiative capture cross section, and $\\eta(E)$ is the Sommerfeld parameter.\n\n2. For a single isolated level in resonance matrix (R-matrix) theory with orbital angular momentum $\\ell=0$ and a constant photon width, model the capture cross section by the Breit–Wigner form\n$$\n\\sigma(E) = \\frac{\\pi}{k^2(E)} \\, \\omega \\, \\frac{\\Gamma_p(E)\\, \\Gamma_\\gamma}{(E - E_\\lambda)^2 + \\left(\\frac{\\Gamma_p(E) + \\Gamma_\\gamma}{2}\\right)^2},\n$$\nwith the spin statistical factor $\\omega$ fixed to $1$ for this exercise, and the energy-dependent proton width\n$$\n\\Gamma_p(E) = 2\\, P_0(E)\\, \\gamma_p^2,\n$$\nwhere $\\gamma_p$ is the reduced width amplitude and $P_0(E)$ is the $s$-wave barrier penetrability. For this problem, approximate the penetrability as\n$$\nP_0(E) = e^{-2 \\pi \\eta(E)}.\n$$\n\n3. Use the nonrelativistic kinematics in natural nuclear units to compute each ingredient:\n- The wave number\n$$\nk(E) = \\frac{\\sqrt{2\\, \\mu\\, E}}{\\hbar c},\n$$\nwith $\\mu$ the reduced mass in energy units.\n- The Sommerfeld parameter\n$$\n\\eta(E) = \\alpha Z_1 Z_2 \\sqrt{\\frac{\\mu}{2E}},\n$$\nwhere $Z_1 = 1$ and $Z_2 = 5$ are the projectile and target charges, respectively, and $\\alpha$ is the fine-structure constant.\n\n4. Adopt the following constants and unit conversions:\n- Fine-structure constant $\\alpha = 1/137.035999084$.\n- Reduced mass for $^{10}\\mathrm{B}+p$ in energy units: $\\mu = \\mu_{\\mathrm{amu}}\\, (m_u c^2)$ with $\\mu_{\\mathrm{amu}} = \\frac{A_1 A_2}{A_1 + A_2}$, $A_1 = 1$, $A_2 = 10$, and $m_u c^2 = 931494.10242\\,\\mathrm{keV}$.\n- $\\hbar c = 197326.9804\\,\\mathrm{keV\\cdot fm}$.\n- $1\\,\\mathrm{b} = 100\\,\\mathrm{fm}^2$.\n- The $S$-factor must be reported in $\\mathrm{keV\\cdot b}$, and energy must be handled in $\\mathrm{keV}$.\n\n5. Synthetic data generation protocol (used identically by all solutions, no external files):\n- True parameters: $E_\\lambda^{(\\mathrm{true})} = 500\\,\\mathrm{keV}$, $\\gamma_p^{(\\mathrm{true})} = 0.70\\,\\mathrm{keV}^{1/2}$, $\\Gamma_\\gamma^{(\\mathrm{true})} = 0.20\\,\\mathrm{keV}$.\n- Measurement energies: $E_i$ linearly spaced from $200\\,\\mathrm{keV}$ to $900\\,\\mathrm{keV}$ with $12$ points.\n- Compute noiseless $S_i^{(\\mathrm{true})} = S(E_i \\mid E_\\lambda^{(\\mathrm{true})}, \\gamma_p^{(\\mathrm{true})}, \\Gamma_\\gamma^{(\\mathrm{true})})$.\n- Assign independent Gaussian measurement noise with standard deviation $\\sigma_i = 0.05\\, S_i^{(\\mathrm{true})}$ and draw observed data $y_i \\sim \\mathcal{N}(S_i^{(\\mathrm{true})}, \\sigma_i^2)$ using a fixed pseudorandom seed $123456$.\n\n6. Bayesian inference model:\n- Parameters are $\\theta = (E_\\lambda, \\gamma_p, \\Gamma_\\gamma)$ with constraints $E_\\lambda \\in [200,900]\\,\\mathrm{keV}$, $\\gamma_p > 0$, and $\\Gamma_\\gamma > 0$.\n- Likelihood for the data $\\{(E_i,y_i,\\sigma_i)\\}_{i=1}^{N}$ is Gaussian,\n$$\n\\mathcal{L}(\\theta) \\propto \\prod_{i=1}^{N} \\exp\\left[-\\frac{(y_i - S(E_i \\mid \\theta))^2}{2\\sigma_i^2}\\right].\n$$\n- Gaussian priors (from ab initio constraints) are imposed independently on each parameter:\n$E_\\lambda \\sim \\mathcal{N}(\\mu_E, \\sigma_E^2), \\quad \\gamma_p \\sim \\mathcal{N}(\\mu_\\gamma, \\sigma_\\gamma^2), \\quad \\Gamma_\\gamma \\sim \\mathcal{N}(\\mu_\\Gamma, \\sigma_\\Gamma^2)$.\n\n7. Posterior quantification and extrapolation:\n- Compute the maximum a posteriori (MAP) estimate by minimizing the negative log-posterior.\n- Approximate the posterior near the MAP by a Gaussian using the Laplace approximation (LA): the covariance is the inverse of the Hessian of the negative log-posterior at the MAP.\n- Predict the astrophysical $S$-factor at a low energy $E_0 = 10\\,\\mathrm{keV}$ and quantify its posterior uncertainty by linear error propagation:\n$\\mathrm{Var}[S(E_0)] \\approx \\nabla_\\theta S(E_0 \\mid \\hat{\\theta})^\\top \\, \\Sigma \\, \\nabla_\\theta S(E_0 \\mid \\hat{\\theta})$,\nwhere $\\hat{\\theta}$ is the MAP and $\\Sigma$ is the LA covariance.\n\nImplement the above, and evaluate the following three prior scenarios (test suite):\n\n- Case A (uninformative): $\\mu_E = 600\\,\\mathrm{keV}$, $\\sigma_E = 10^6\\,\\mathrm{keV}$; $\\mu_\\gamma = 1.50\\,\\mathrm{keV}^{1/2}$, $\\sigma_\\gamma = 10^3\\,\\mathrm{keV}^{1/2}$; $\\mu_\\Gamma = 0.40\\,\\mathrm{keV}$, $\\sigma_\\Gamma = 10^3\\,\\mathrm{keV}$.\n- Case B (moderate ab initio): $\\mu_E = 520\\,\\mathrm{keV}$, $\\sigma_E = 50\\,\\mathrm{keV}$; $\\mu_\\gamma = 0.90\\,\\mathrm{keV}^{1/2}$, $\\sigma_\\gamma = 0.30\\,\\mathrm{keV}^{1/2}$; $\\mu_\\Gamma = 0.25\\,\\mathrm{keV}$, $\\sigma_\\Gamma = 0.10\\,\\mathrm{keV}$.\n- Case C (tight ab initio): $\\mu_E = 500\\,\\mathrm{keV}$, $\\sigma_E = 5\\,\\mathrm{keV}$; $\\mu_\\gamma = 0.70\\,\\mathrm{keV}^{1/2}$, $\\sigma_\\gamma = 0.05\\,\\mathrm{keV}^{1/2}$; $\\mu_\\Gamma = 0.20\\,\\mathrm{keV}$, $\\sigma_\\Gamma = 0.02\\,\\mathrm{keV}$.\n\nNumerical requirements and deliverables:\n\n- Use central finite-difference formulas to compute the Hessian of the negative log-posterior at the MAP and the gradient of $S(E_0)$ with respect to $(E_\\lambda,\\gamma_p,\\Gamma_\\gamma)$, with step sizes chosen adaptively as small fractions of the parameter magnitudes.\n- Stabilize the covariance inversion if needed by adding a small positive multiple of the identity to the Hessian.\n- Report, for each case A, B, C, the posterior standard deviation $\\sqrt{\\mathrm{Var}[S(E_0)]}$ in $\\mathrm{keV\\cdot b}$, rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_A,r_B,r_C]$).\n\nAll quantities with physical units must be handled exactly as specified, and angles are not used. The final outputs are three floats corresponding to cases A, B, and C in that order, each in $\\mathrm{keV\\cdot b}$ as decimals rounded to six places.",
            "solution": "We construct a computational model for the astrophysical $S$-factor extrapolation under Gaussian prior constraints using a single-level resonance matrix (R-matrix) description of $^{10}\\mathrm{B}(p,\\gamma)^{11}\\mathrm{C}$. The procedure follows a sequence of physically grounded steps.\n\n1. Foundational definitions. The astrophysical $S$-factor is defined by $S(E) = \\sigma(E)\\, E\\, e^{2\\pi \\eta(E)}$, where $E$ is in $\\mathrm{keV}$, $\\sigma(E)$ is the capture cross section in $\\mathrm{b}$, and $\\eta(E)$ is the Sommerfeld parameter. For an isolated level with $\\ell=0$ and a constant photon width, the Breit–Wigner expression gives $\\sigma(E) = \\frac{\\pi}{k^2(E)} \\, \\omega \\, \\frac{\\Gamma_p(E)\\, \\Gamma_\\gamma}{(E - E_\\lambda)^2 + \\left(\\frac{\\Gamma_p(E) + \\Gamma_\\gamma}{2}\\right)^2}$. We choose $\\omega=1$ for clarity. The proton partial width is energy dependent via $\\Gamma_p(E) = 2\\, P_0(E)\\, \\gamma_p^2$, and we approximate the penetrability as $P_0(E) = e^{-2 \\pi \\eta(E)}$, which captures the leading tunneling dependence for charged-particle $s$-waves at low energy.\n\n2. Kinematics and units. We work in mixed nuclear units with energy in $\\mathrm{keV}$ and length in $\\mathrm{fm}$. The wave number is $k(E) = \\sqrt{2\\, \\mu\\, E}/(\\hbar c)$, where $\\mu$ is the reduced mass in $\\mathrm{keV}$ (as a mass energy), and $\\hbar c = 197326.9804\\,\\mathrm{keV\\cdot fm}$. The Sommerfeld parameter is $\\eta(E) = \\alpha Z_1 Z_2 \\sqrt{\\mu/(2E)}$, with $Z_1=1$ for the proton and $Z_2=5$ for $^{10}\\mathrm{B}$. The reduced mass is $\\mu = \\mu_{\\mathrm{amu}} (m_u c^2)$ with $\\mu_{\\mathrm{amu}} = \\frac{A_1 A_2}{A_1 + A_2} = \\frac{1 \\cdot 10}{1+10}$ and $m_u c^2 = 931494.10242\\,\\mathrm{keV}$. The cross section formula yields $\\sigma(E)$ in $\\mathrm{fm}^2$ if $k$ is in $\\mathrm{fm}^{-1}$; converting to barns uses $1\\,\\mathrm{b} = 100\\,\\mathrm{fm}^2$. The astrophysical $S$-factor then emerges in $\\mathrm{keV\\cdot b}$ via $S(E) = \\sigma(E)\\, E\\, e^{2\\pi\\eta(E)}$.\n\n3. Synthetic data. We generate mock data to emulate measurements. With fixed true parameters $E_\\lambda^{(\\mathrm{true})} = 500\\,\\mathrm{keV}$, $\\gamma_p^{(\\mathrm{true})} = 0.70\\,\\mathrm{keV}^{1/2}$, and $\\Gamma_\\gamma^{(\\mathrm{true})} = 0.20\\,\\mathrm{keV}$, we produce $N=12$ energies $E_i$ evenly spaced from $200\\,\\mathrm{keV}$ to $900\\,\\mathrm{keV}$. The noiseless values $S_i^{(\\mathrm{true})}$ are computed by the above formulas. Independent Gaussian noise with standard deviations $\\sigma_i = 0.05\\, S_i^{(\\mathrm{true})}$ is added using a pseudorandom seed of $123456$, yielding observed values $y_i$.\n\n4. Bayesian inference. We adopt a Gaussian likelihood $\\mathcal{L}(\\theta)\\propto \\exp\\left[-\\sum_i (y_i - S(E_i\\mid\\theta))^2/(2\\sigma_i^2)\\right]$ and independent Gaussian priors $E_\\lambda\\sim\\mathcal{N}(\\mu_E,\\sigma_E^2)$, $\\gamma_p\\sim\\mathcal{N}(\\mu_\\gamma,\\sigma_\\gamma^2)$, and $\\Gamma_\\gamma\\sim\\mathcal{N}(\\mu_\\Gamma,\\sigma_\\Gamma^2)$. The negative log-posterior (up to constants independent of $\\theta$) is\n$$\n\\mathcal{N}(\\theta) = \\frac{1}{2}\\sum_{i=1}^{N}\\frac{(y_i - S(E_i \\mid \\theta))^2}{\\sigma_i^2} + \\frac{1}{2}\\left(\\frac{E_\\lambda - \\mu_E}{\\sigma_E}\\right)^2 + \\frac{1}{2}\\left(\\frac{\\gamma_p - \\mu_\\gamma}{\\sigma_\\gamma}\\right)^2 + \\frac{1}{2}\\left(\\frac{\\Gamma_\\gamma - \\mu_\\Gamma}{\\sigma_\\Gamma}\\right)^2.\n$$\nWe compute the maximum a posteriori (MAP) estimate $\\hat{\\theta}$ by minimizing $\\mathcal{N}(\\theta)$ under the constraints $E_\\lambda\\in[200,900]\\,\\mathrm{keV}$, $\\gamma_p>0$, and $\\Gamma_\\gamma>0$.\n\n5. Laplace approximation and uncertainty propagation. To quantify the posterior uncertainty of the extrapolated $S$-factor at a low energy $E_0 = 10\\,\\mathrm{keV}$, we approximate the posterior near $\\hat{\\theta}$ by a Gaussian with covariance matrix $\\Sigma$ equal to the inverse of the Hessian $H$ of $\\mathcal{N}(\\theta)$ evaluated at $\\hat{\\theta}$. We compute $H$ numerically by central finite differences:\n- For diagonal elements,\n$$\nH_{ii} \\approx \\frac{\\mathcal{N}(\\hat{\\theta}+h_i e_i) - 2\\mathcal{N}(\\hat{\\theta}) + \\mathcal{N}(\\hat{\\theta}-h_i e_i)}{h_i^2},\n$$\n- For off-diagonal elements with $i\\neq j$,\n$$\nH_{ij} \\approx \\frac{\\mathcal{N}(\\hat{\\theta}+h_i e_i + h_j e_j) - \\mathcal{N}(\\hat{\\theta}+h_i e_i - h_j e_j) - \\mathcal{N}(\\hat{\\theta}-h_i e_i + h_j e_j) + \\mathcal{N}(\\hat{\\theta}-h_i e_i - h_j e_j)}{4 h_i h_j},\n$$\nwhere $e_i$ are coordinate unit vectors and $h_i$ are small step sizes chosen as a fraction of $|\\hat{\\theta}_i|$ with safety floors to avoid zero.\n\nWe similarly compute the gradient $g = \\nabla_\\theta S(E_0\\mid\\hat{\\theta})$ by central differences. The linearized posterior variance of $S(E_0)$ is\n$$\n\\mathrm{Var}[S(E_0)] \\approx g^\\top \\Sigma g,\n$$\nand the standard deviation is its square root.\n\n6. Prior scenarios and expectations. We evaluate three prior cases: A (uninformative, extremely broad), B (moderate ab initio constraints), and C (tight ab initio constraints). In general, adding informative priors increases curvature in the posterior near the MAP, shrinking $\\Sigma$ and tightening the extrapolated uncertainty. Thus we expect the posterior standard deviation at $E_0=10\\,\\mathrm{keV}$ to satisfy approximately $\\mathrm{sd}_\\mathrm{C}  \\mathrm{sd}_\\mathrm{B}  \\mathrm{sd}_\\mathrm{A}$.\n\n7. Algorithmic summary.\n- Compute constants $\\alpha$, $\\mu$, $\\hbar c$, and the conversion factor to barns.\n- Implement $k(E)$, $\\eta(E)$, $P_0(E)$, $\\Gamma_p(E)$, $\\sigma(E)$, and $S(E)$.\n- Generate synthetic data $\\{E_i,y_i,\\sigma_i\\}$ with the specified seed and noise model.\n- For each prior case, minimize $\\mathcal{N}(\\theta)$ from a reasonable initial guess subject to bounds, obtain $\\hat{\\theta}$.\n- Compute the Hessian $H$ at $\\hat{\\theta}$ by central finite differences; stabilize if necessary by adding a small multiple of the identity for positive definiteness; invert to get $\\Sigma$.\n- Compute $g$ at $E_0=10\\,\\mathrm{keV}$ and the posterior standard deviation $\\sqrt{g^\\top \\Sigma g}$.\n- Report three floats corresponding to cases A, B, C in $\\mathrm{keV\\cdot b}$, each rounded to six decimal places, as a single-line, comma-separated list enclosed in square brackets.\n\nThe final program adheres to these steps and produces the required outputs reproducibly with the specified pseudorandom seed.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import optimize\n\n# Constants\nALPHA = 1.0 / 137.035999084  # fine-structure constant\nHBAR_C_keV_fm = 197326.9804  # keV*fm\nAMU_C2_keV = 931_494.10242   # keV\nZ1 = 1\nZ2 = 5\n\n# Reduced mass mu in keV (mass-energy), using A1=1 (p) and A2=10 (B-10)\nA1 = 1.0\nA2 = 10.0\nmu_amu = (A1 * A2) / (A1 + A2)  # in atomic mass units\nMU_keV = mu_amu * AMU_C2_keV    # reduced mass energy in keV\n\n# Conversion: sigma in fm^2 - barns: 1 b = 100 fm^2\nFM2_TO_B = 1.0 / 100.0\n\ndef sommerfeld_eta(E_keV):\n    # E in keV\n    # eta = alpha Z1 Z2 sqrt(mu/(2E))\n    E = np.asarray(E_keV, dtype=float)\n    return ALPHA * Z1 * Z2 * np.sqrt(MU_keV / (2.0 * E))\n\ndef wavenumber_k_fm_inv(E_keV):\n    # k = sqrt(2 mu E) / (hbar c), with mu in keV, E in keV, k in fm^-1\n    return np.sqrt(2.0 * MU_keV * E_keV) / HBAR_C_keV_fm\n\ndef penetrability_P0(E_keV):\n    # s-wave penetrability approximation\n    eta = sommerfeld_eta(E_keV)\n    return np.exp(-2.0 * np.pi * eta)\n\ndef gamma_p_width(E_keV, gamma_p_keV_sqrt):\n    # Gamma_p(E) = 2 P0(E) gamma_p^2, returns keV\n    return 2.0 * penetrability_P0(E_keV) * (gamma_p_keV_sqrt ** 2)\n\ndef sigma_barns(E_keV, E_lambda_keV, gamma_p_keV_sqrt, Gamma_gamma_keV, omega=1.0):\n    # Breit-Wigner capture cross section in barns\n    k = wavenumber_k_fm_inv(E_keV)\n    Gamma_p = gamma_p_width(E_keV, gamma_p_keV_sqrt)\n    Gamma_tot = Gamma_p + Gamma_gamma_keV\n    numer = Gamma_p * Gamma_gamma_keV\n    denom = (E_keV - E_lambda_keV) ** 2 + (Gamma_tot / 2.0) ** 2\n    sigma_fm2 = (np.pi / (k ** 2)) * omega * (numer / denom)\n    return sigma_fm2 * FM2_TO_B\n\ndef s_factor_keV_b(E_keV, params):\n    # params: (E_lambda_keV, gamma_p_keV_sqrt, Gamma_gamma_keV)\n    E_lambda_keV, gamma_p_keV_sqrt, Gamma_gamma_keV = params\n    sigma_b = sigma_barns(E_keV, E_lambda_keV, gamma_p_keV_sqrt, Gamma_gamma_keV, omega=1.0)\n    eta = sommerfeld_eta(E_keV)\n    return sigma_b * E_keV * np.exp(2.0 * np.pi * eta)\n\ndef generate_synthetic_data(seed=123456):\n    rng = np.random.default_rng(seed)\n    # True parameters\n    E_lambda_true = 500.0  # keV\n    gamma_p_true = 0.70    # keV^0.5\n    Gamma_gamma_true = 0.20  # keV\n    # Energies\n    E_data = np.linspace(200.0, 900.0, 12)\n    S_true = s_factor_keV_b(E_data, (E_lambda_true, gamma_p_true, Gamma_gamma_true))\n    sigma = 0.05 * S_true  # 5% relative noise\n    y = rng.normal(loc=S_true, scale=sigma)\n    return E_data, y, sigma\n\ndef neg_log_posterior(theta, E_data, y, sigma, prior_mu, prior_sigma):\n    # theta = (E_lambda, gamma_p, Gamma_gamma)\n    # enforce positivity softly via bounds in optimizer; here just compute model\n    E_lambda, gamma_p, Gamma_gamma = theta\n    # Small penalty if parameters go non-physical (helps optimizer)\n    if gamma_p = 0 or Gamma_gamma = 0:\n        return 1e100\n    S_model = s_factor_keV_b(E_data, theta)\n    resid = (y - S_model) / sigma\n    nll = 0.5 * np.sum(resid ** 2)\n    # Gaussian prior penalties (independent)\n    prior_terms = 0.5 * (((E_lambda - prior_mu[0]) / prior_sigma[0]) ** 2 +\n                         ((gamma_p - prior_mu[1]) / prior_sigma[1]) ** 2 +\n                         ((Gamma_gamma - prior_mu[2]) / prior_sigma[2]) ** 2)\n    return nll + prior_terms\n\ndef finite_diff_hessian(f, x, step):\n    # Central finite difference Hessian for 3D x\n    x = np.asarray(x, dtype=float)\n    n = x.size\n    H = np.zeros((n, n), dtype=float)\n    fx = f(x)\n    # Diagonal terms\n    for i in range(n):\n        ei = np.zeros(n); ei[i] = 1.0\n        hi = step[i]\n        fp = f(x + hi * ei)\n        fm = f(x - hi * ei)\n        H[i, i] = (fp - 2.0 * fx + fm) / (hi ** 2)\n    # Off-diagonal terms\n    for i in range(n):\n        for j in range(i + 1, n):\n            ei = np.zeros(n); ei[i] = 1.0\n            ej = np.zeros(n); ej[j] = 1.0\n            hi = step[i]; hj = step[j]\n            fpp = f(x + hi * ei + hj * ej)\n            fpm = f(x + hi * ei - hj * ej)\n            fmp = f(x - hi * ei + hj * ej)\n            fmm = f(x - hi * ei - hj * ej)\n            val = (fpp - fpm - fmp + fmm) / (4.0 * hi * hj)\n            H[i, j] = val\n            H[j, i] = val\n    return H\n\ndef finite_diff_gradient_S(E0, theta, step):\n    # Central finite difference gradient of S(E0) wrt theta\n    theta = np.asarray(theta, dtype=float)\n    n = theta.size\n    g = np.zeros(n, dtype=float)\n    S0 = s_factor_keV_b(E0, theta)\n    for i in range(n):\n        ei = np.zeros(n); ei[i] = 1.0\n        hi = step[i]\n        Sp = s_factor_keV_b(E0, theta + hi * ei)\n        Sm = s_factor_keV_b(E0, theta - hi * ei)\n        g[i] = (Sp - Sm) / (2.0 * hi)\n    return g\n\ndef map_and_uncertainty(E_data, y, sigma, prior_mu, prior_sigma, E0=10.0):\n    # Bounds: E_lambda in [200,900], gamma_p in [1e-6, 5], Gamma_gamma in [1e-6, 1]\n    bounds = [(200.0, 900.0), (1e-6, 5.0), (1e-6, 1.0)]\n    # Initial guess: prior mean projected into bounds\n    x0 = np.array([\n        np.clip(prior_mu[0], bounds[0][0], bounds[0][1]),\n        np.clip(prior_mu[1], bounds[1][0], bounds[1][1]),\n        np.clip(prior_mu[2], bounds[2][0], bounds[2][1]),\n    ], dtype=float)\n\n    def obj(theta_vec):\n        return neg_log_posterior(theta_vec, E_data, y, sigma, prior_mu, prior_sigma)\n\n    res = optimize.minimize(obj, x0, method=\"L-BFGS-B\", bounds=bounds, options={\"maxiter\": 1000})\n    theta_map = res.x\n\n    # Finite-difference steps relative to parameter scales\n    steps = np.array([\n        max(1e-3 * max(1.0, abs(theta_map[0])), 1e-3),\n        max(1e-3 * max(1.0, abs(theta_map[1])), 1e-4),\n        max(1e-3 * max(1.0, abs(theta_map[2])), 1e-4),\n    ], dtype=float)\n\n    # Hessian\n    H = finite_diff_hessian(obj, theta_map, steps)\n    # Stabilize Hessian if not positive definite\n    jitter = 1e-9\n    # Ensure symmetry\n    H = 0.5 * (H + H.T)\n    # Add jitter until invertible\n    for _ in range(10):\n        try:\n            Sigma = np.linalg.inv(H + jitter * np.eye(3))\n            # Check positive definiteness\n            if np.all(np.linalg.eigvalsh(H + jitter * np.eye(3))  0):\n                break\n        except np.linalg.LinAlgError:\n            pass\n        jitter *= 10.0\n    else:\n        # Fallback: pseudo-inverse\n        Sigma = np.linalg.pinv(H + jitter * np.eye(3))\n\n    # Gradient of S at E0\n    g = finite_diff_gradient_S(E0, theta_map, steps)\n    var_S0 = float(np.dot(g, Sigma @ g))\n    var_S0 = max(var_S0, 0.0)\n    sd_S0 = np.sqrt(var_S0)\n    return sd_S0\n\ndef solve():\n    # Generate synthetic dataset\n    E_data, y_obs, y_sigma = generate_synthetic_data(seed=123456)\n\n    # Prior cases\n    cases = [\n        # Case A: uninformative\n        (np.array([600.0, 1.50, 0.40]), np.array([1e6, 1e3, 1e3])),\n        # Case B: moderate ab initio\n        (np.array([520.0, 0.90, 0.25]), np.array([50.0, 0.30, 0.10])),\n        # Case C: tight ab initio\n        (np.array([500.0, 0.70, 0.20]), np.array([5.0, 0.05, 0.02])),\n    ]\n\n    results = []\n    for mu_prior, sigma_prior in cases:\n        sd = map_and_uncertainty(E_data, y_obs, y_sigma, mu_prior, sigma_prior, E0=10.0)\n        results.append(sd)\n\n    # Round to six decimal places\n    results_str = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(results_str)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Beyond quantifying parameter uncertainty, a crucial application of UQ is to estimate the probability of rare but critical events, such as material failure. Standard Monte Carlo simulation is often too inefficient for this task. In this practice, you will implement the advanced Cross-Entropy method, an adaptive importance sampling algorithm, to efficiently estimate the probability of brittle fracture in a component with uncertain material properties and loading conditions . This exercise will equip you with a powerful tool for reliability analysis and risk assessment in materials engineering.",
            "id": "3499844",
            "problem": "Consider a brittle fracture criterion in linear elastic fracture mechanics where failure occurs when the mode-I stress intensity factor exceeds the material fracture toughness. Let the stress intensity factor be given by $$K_I(a,\\sigma) = Y\\,\\sigma\\,\\sqrt{\\pi a},$$ where $a$ is the crack length in meters, $\\sigma$ is the applied tensile stress in megapascals (MPa), and $Y$ is a dimensionless geometry factor. Let the material fracture toughness be $K_{IC}$, expressed in units of $\\text{MPa}\\sqrt{\\text{m}}$. Define the performance function $$f(\\theta) = K_I(a,\\sigma) - K_{IC},$$ with $\\theta = (a,\\sigma,K_{IC})$, and consider the rare-event probability $$p = \\mathbb{P}\\left(f(\\theta)  \\tau\\right),$$ for a given threshold $\\tau$. In this problem, set $\\tau = 0$, so that failure occurs when $K_I(a,\\sigma)  K_{IC}$.\n\nAssume that the nominal uncertainty model for the inputs is that $a$, $\\sigma$, and $K_{IC}$ are independent lognormal random variables with parameters $(\\mu_{\\ell a},\\sigma_{\\ell a})$, $(\\mu_{\\ell \\sigma},\\sigma_{\\ell \\sigma})$, and $(\\mu_{\\ell K},\\sigma_{\\ell K})$, respectively. Specifically, if $X$ is lognormal with log-space parameters $(\\mu_{\\ell},\\sigma_{\\ell})$, then its probability density function is $$\\pi_X(x) = \\frac{1}{x\\,\\sigma_{\\ell}\\sqrt{2\\pi}}\\exp\\left(-\\frac{\\left(\\ln x - \\mu_{\\ell}\\right)^2}{2\\sigma_{\\ell}^2}\\right),\\quad x0.$$ Denote the joint nominal density as $$\\pi(\\theta) = \\pi_a(a)\\,\\pi_{\\sigma}(\\sigma)\\,\\pi_K(K_{IC}).$$\n\nTask 1 (Derivation): Starting from the foundational definition of importance sampling for Monte Carlo estimation, derive the zero-variance importance sampling density $q^\\star(\\theta)$ for estimating $p = \\mathbb{P}(f(\\theta)  0)$ when sampling from a proposal $q(\\theta)$ and evaluating with the nominal density $\\pi(\\theta)$. Specifically:\n- Begin from the definitions of the indicator function, the change-of-measure identity, and the variance of the importance sampling estimator.\n- Use these principles to derive the form of $q^\\star(\\theta)$ that yields zero estimator variance.\n\nTask 2 (Cross-Entropy Method): In practice, $q^\\star(\\theta)$ is intractable. Consider the parametric proposal family $q(\\theta;v)$, where $q$ is the product of three independent lognormal densities with parameters $v = (\\mu_{\\ell a},\\sigma_{\\ell a},\\mu_{\\ell \\sigma},\\sigma_{\\ell \\sigma},\\mu_{\\ell K},\\sigma_{\\ell K})$. Derive how to select $v$ by minimizing the Kullback–Leibler divergence between $q^\\star(\\theta)$ and $q(\\theta;v)$. Show that this is equivalent to maximizing the expected log-likelihood under the truncated nominal distribution proportional to $\\mathbb{I}\\{f(\\theta)0\\}\\pi(\\theta)$, and derive the weighted maximum-likelihood update equations for the lognormal parameters when sampling from a current proposal $q(\\theta;v_t)$:\n- Let weights be defined as $$w(\\theta) = \\frac{\\pi(\\theta)}{q(\\theta;v_t)},$$ and let an elite set be defined by an adaptively chosen threshold $\\gamma_t$ so that a fixed fraction $\\rho$ of samples satisfy $f(\\theta)\\ge \\gamma_t$.\n- Derive the updates for the log-space means and standard deviations using the elite samples and weights $w(\\theta)$.\n\nImplementation Requirements:\n- Implement a program that approximates the optimal proposal $q(\\theta;v)$ using the Cross-Entropy method with smoothing. Use the following algorithmic structure:\n  1. Initialize $v_0$ to the nominal parameters.\n  2. For $t=1,\\dots,T$:\n     - Draw $N$ samples $\\theta_i \\sim q(\\cdot;v_{t-1})$.\n     - Compute $f(\\theta_i)$ and select $\\gamma_t$ as the $(1-\\rho)$-quantile of $\\{f(\\theta_i)\\}$.\n     - Form the elite set $\\mathcal{E}_t = \\{\\theta_i: f(\\theta_i)\\ge \\gamma_t\\}$.\n     - Compute weights $w_i = \\pi(\\theta_i)/q(\\theta_i;v_{t-1})$, and normalize them over $\\mathcal{E}_t$.\n     - Update the parameters $v_t$ by weighted maximum-likelihood on $\\mathcal{E}_t$, with exponential smoothing parameter $\\alpha \\in (0,1)$ applied to both means and standard deviations in log-space.\n  3. After $T$ iterations, use $q(\\cdot;v_T)$ to estimate $p$ by importance sampling with $M$ independent samples and report the estimator $$\\hat{p} = \\frac{1}{M}\\sum_{i=1}^M \\mathbb{I}\\{f(\\theta_i)0\\}\\frac{\\pi(\\theta_i)}{q(\\theta_i;v_T)}.$$ Also compute the standard error as the square root of the sample variance of the importance weights times the indicator divided by $M$ and use it internally to ensure numerical stability. You should output only the probability estimates.\n\n- Units: Use the International System of Units (SI) consistently for input variables. Specifically:\n  - $a$ in meters (m).\n  - $\\sigma$ in megapascals (MPa).\n  - $K_{IC}$ in $\\text{MPa}\\sqrt{\\text{m}}$.\n  - $Y$ is dimensionless.\n  The output probability is a dimensionless decimal number.\n\n- Angle units are not applicable in this problem.\n\nTest Suite:\nUse the following test cases, all with $\\tau=0$ and the same algorithmic hyperparameters $N=8000$, $T=6$, $\\rho=0.1$, $\\alpha=0.7$, $M=150000$, and a fixed random seed for reproducibility.\n\n1. Case A (rare-event baseline, happy path):\n   - $Y = 1.12$\n   - Nominal lognormal parameters:\n     - $\\mu_{\\ell a} = \\ln(3.0\\times 10^{-5})$, $\\sigma_{\\ell a} = 0.5$\n     - $\\mu_{\\ell \\sigma} = \\ln(120)$, $\\sigma_{\\ell \\sigma} = 0.25$\n     - $\\mu_{\\ell K} = \\ln(60)$, $\\sigma_{\\ell K} = 0.15$\n\n2. Case B (less rare, increased crack length and lower toughness):\n   - $Y = 1.12$\n   - Nominal lognormal parameters:\n     - $\\mu_{\\ell a} = \\ln(2.0\\times 10^{-4})$, $\\sigma_{\\ell a} = 0.45$\n     - $\\mu_{\\ell \\sigma} = \\ln(150)$, $\\sigma_{\\ell \\sigma} = 0.25$\n     - $\\mu_{\\ell K} = \\ln(30)$, $\\sigma_{\\ell K} = 0.20$\n\n3. Case C (extremely rare, nearly impossible failure):\n   - $Y = 1.12$\n   - Nominal lognormal parameters:\n     - $\\mu_{\\ell a} = \\ln(5.0\\times 10^{-6})$, $\\sigma_{\\ell a} = 0.4$\n     - $\\mu_{\\ell \\sigma} = \\ln(80)$, $\\sigma_{\\ell \\sigma} = 0.20$\n     - $\\mu_{\\ell K} = \\ln(120)$, $\\sigma_{\\ell K} = 0.20$\n\nYour program should produce a single line of output containing the estimated rare-event probabilities for the three cases as a comma-separated list enclosed in square brackets (e.g., \"[pA,pB,pC]\"). Each entry must be a float in decimal form. No other text should be printed.",
            "solution": "The problem is valid as it is scientifically grounded in linear elastic fracture mechanics and statistical reliability theory, is well-posed with a clear objective and sufficient data, and is free from any logical contradictions or factual unsoundness. We will proceed with the derivation and implementation.\n\nThe solution is presented in three parts. The first two parts address the theoretical derivations requested in Task 1 and Task 2. The third part describes the implementation logic which is realized in the final answer.\n\n### Task 1: Derivation of the Zero-Variance Importance Sampling Density\n\nThe goal is to estimate the probability of failure, $p$, defined as:\n$$p = \\mathbb{P}(f(\\theta)  0) = \\int_{\\Theta} \\mathbb{I}\\{f(\\theta)  0\\} \\pi(\\theta) d\\theta$$\nwhere $\\theta = (a, \\sigma, K_{IC})$, $\\pi(\\theta)$ is the joint probability density function (PDF) of the input parameters, and $\\mathbb{I}\\{\\cdot\\}$ is the indicator function, which is $1$ if its argument is true and $0$ otherwise.\n\nImportance Sampling (IS) is a variance reduction technique for Monte Carlo integration. Instead of drawing samples from the nominal density $\\pi(\\theta)$, we draw from a proposal density $q(\\theta)$, and re-weight the samples to maintain an unbiased estimate. The expectation is rewritten as:\n$$p = \\int_{\\Theta} \\mathbb{I}\\{f(\\theta)  0\\} \\frac{\\pi(\\theta)}{q(\\theta)} q(\\theta) d\\theta = \\mathbb{E}_{q}\\left[ \\mathbb{I}\\{f(\\theta)  0\\} \\frac{\\pi(\\theta)}{q(\\theta)} \\right]$$\nwhere $\\mathbb{E}_q[\\cdot]$ denotes the expectation with respect to the proposal density $q(\\theta)$. The term $w(\\theta) = \\pi(\\theta)/q(\\theta)$ is the importance weight.\n\nThe IS estimator for $p$, based on $M$ independent and identically distributed samples $\\{\\theta_i\\}_{i=1}^M$ drawn from $q(\\theta)$, is:\n$$\\hat{p} = \\frac{1}{M} \\sum_{i=1}^M \\mathbb{I}\\{f(\\theta_i)  0\\} \\frac{\\pi(\\theta_i)}{q(\\theta_i)}$$\nThe variance of this estimator is given by $\\frac{1}{M}\\text{Var}_q(Z(\\theta))$, where $Z(\\theta) = \\mathbb{I}\\{f(\\theta)  0\\} \\frac{\\pi(\\theta)}{q(\\theta)}$. The variance of a single sample $Z(\\theta)$ is:\n$$\\text{Var}_q(Z(\\theta)) = \\mathbb{E}_q[Z(\\theta)^2] - (\\mathbb{E}_q[Z(\\theta)])^2$$\nWe already know that $\\mathbb{E}_q[Z(\\theta)] = p$. The first term is:\n$$\\mathbb{E}_q[Z(\\theta)^2] = \\int_{\\Theta} \\left( \\mathbb{I}\\{f(\\theta)  0\\} \\frac{\\pi(\\theta)}{q(\\theta)} \\right)^2 q(\\theta) d\\theta = \\int_{\\Theta} \\mathbb{I}\\{f(\\theta)  0\\}^2 \\frac{\\pi(\\theta)^2}{q(\\theta)} d\\theta$$\nSince $\\mathbb{I}\\{\\cdot\\}^2 = \\mathbb{I}\\{\\cdot\\}$, this simplifies to:\n$$\\mathbb{E}_q[Z(\\theta)^2] = \\int_{f(\\theta)0} \\frac{\\pi(\\theta)^2}{q(\\theta)} d\\theta$$\nThe variance is zero if and only if the random variable $Z(\\theta)$ is a constant for all $\\theta$ where $q(\\theta)  0$. Since its expectation is $p$, this constant must be $p$.\n$$Z(\\theta) = \\mathbb{I}\\{f(\\theta)  0\\} \\frac{\\pi(\\theta)}{q(\\theta)} = p$$\nSolving for $q(\\theta)$, we obtain the optimal, zero-variance proposal density, denoted $q^\\star(\\theta)$:\n$$q^\\star(\\theta) = \\frac{\\mathbb{I}\\{f(\\theta)  0\\} \\pi(\\theta)}{p}$$\nThis is a valid PDF as it is non-negative and integrates to $1$:\n$$\\int_{\\Theta} q^\\star(\\theta) d\\theta = \\frac{1}{p} \\int_{\\Theta} \\mathbb{I}\\{f(\\theta)  0\\} \\pi(\\theta) d\\theta = \\frac{p}{p} = 1$$\nThis optimal density $q^\\star(\\theta)$ is the nominal density $\\pi(\\theta)$ conditioned on the failure event $\\{f(\\theta)  0\\}$. In practice, $q^\\star(\\theta)$ is intractable because it depends on $p$ (the very quantity we wish to estimate) and it requires sampling from a truncated distribution, which is generally a difficult problem.\n\n### Task 2: Derivation of the Cross-Entropy Method Update Equations\n\nSince $q^\\star(\\theta)$ is intractable, the Cross-Entropy (CE) method seeks the best approximation to $q^\\star(\\theta)$ from a chosen parametric family of densities, $\\{q(\\theta;v)\\}$. The \"best\" approximation is the one that minimizes the Kullback-Leibler (KL) divergence from $q^\\star(\\theta)$ to $q(\\theta;v)$:\n$$v_{\\text{opt}} = \\arg\\min_v D_{KL}(q^\\star || q(\\cdot;v))$$\nThe KL divergence is defined as:\n$$D_{KL}(q^\\star || q) = \\int_{\\Theta} q^\\star(\\theta) \\ln\\left(\\frac{q^\\star(\\theta)}{q(\\theta;v)}\\right) d\\theta = \\mathbb{E}_{q^\\star}\\left[ \\ln q^\\star(\\theta) \\right] - \\mathbb{E}_{q^\\star}\\left[ \\ln q(\\theta;v) \\right]$$\nMinimizing the KL divergence with respect to $v$ is equivalent to maximizing the second term, as the first term does not depend on $v$:\n$$v_{\\text{opt}} = \\arg\\max_v \\mathbb{E}_{q^\\star}\\left[ \\ln q(\\theta;v) \\right]$$\nSubstituting the expression for $q^\\star(\\theta)$:\n$$\\mathbb{E}_{q^\\star}\\left[ \\ln q(\\theta;v) \\right] = \\int_{\\Theta} \\frac{\\mathbb{I}\\{f(\\theta)  0\\} \\pi(\\theta)}{p} \\ln q(\\theta;v) d\\theta$$\nDropping the constant factor $1/p$, the optimization problem is equivalent to solving a stochastic program:\n$$v_{\\text{opt}} = \\arg\\max_v \\int_{\\Theta} \\mathbb{I}\\{f(\\theta)  0\\} \\pi(\\theta) \\ln q(\\theta;v) d\\theta$$\nThis integral is typically evaluated using Monte Carlo sampling. In the CE method, we use an iterative approach. At iteration $t$, we have a parameter vector $v_t$. We generate samples from $q(\\theta;v_t)$ and use importance-weighting to approximate the maximization. To handle rare events, an \"elite set\" of samples is defined based on a performance threshold $\\gamma_t$, which is chosen adaptively. This leads to the problem:\n$$v_{t+1} = \\arg\\max_v \\mathbb{E}_{q(\\cdot;v_t)}\\left[ \\mathbb{I}\\{f(\\theta) \\ge \\gamma_t\\} \\frac{\\pi(\\theta)}{q(\\theta;v_t)} \\ln q(\\theta;v) \\right]$$\nGiven $N$ samples $\\{\\theta_i\\}_{i=1}^N$ from $q(\\cdot;v_t)$, the sample-based version of this problem is:\n$$v_{t+1} = \\arg\\max_v \\sum_{i=1}^N \\mathbb{I}\\{f(\\theta_i) \\ge \\gamma_t\\} w_i \\ln q(\\theta_i;v)$$\nwhere $w_i = \\pi(\\theta_i)/q(\\theta_i;v_t)$ are the importance weights. The sum is over the elite set $\\mathcal{E}_t = \\{\\theta_i : f(\\theta_i) \\ge \\gamma_t\\}$.\n\nThe proposal family $q(\\theta;v)$ is a product of three independent lognormal densities for $a$, $\\sigma$, and $K_{IC}$. Therefore, $\\ln q(\\theta;v)$ decomposes into a sum of log-PDFs, and the maximization can be performed independently for each variable's parameters. Let's derive the update for variable $a$, with log-space parameters $v_a = (\\mu_{\\ell a}, \\sigma_{\\ell a})$. We want to maximize:\n$$L_a(\\mu_{\\ell a}, \\sigma_{\\ell a}) = \\sum_{\\theta_i \\in \\mathcal{E}_t} w_i \\ln q_a(a_i; \\mu_{\\ell a}, \\sigma_{\\ell a})$$\nThe log-PDF for a lognormal distribution of a variable $x$ with log-space parameters $(\\mu_\\ell, \\sigma_\\ell)$ is $\\ln \\pi_X(x) = -\\ln x - \\ln \\sigma_\\ell - \\ln\\sqrt{2\\pi} - \\frac{(\\ln x - \\mu_\\ell)^2}{2\\sigma_\\ell^2}$. Dropping terms not dependent on $(\\mu_\\ell, \\sigma_\\ell)$, we maximize:\n$$L_a(\\mu_{\\ell a}, \\sigma_{\\ell a}) \\propto \\sum_{\\theta_i \\in \\mathcal{E}_t} w_i \\left( -\\ln \\sigma_{\\ell a} - \\frac{(\\ln a_i - \\mu_{\\ell a})^2}{2\\sigma_{\\ell a}^2} \\right)$$\nThis is a standard weighted maximum likelihood estimation for the parameters of a normal distribution for the variable $y_a = \\ln a$. By taking derivatives with respect to $\\mu_{\\ell a}$ and $\\sigma_{\\ell a}$ and setting them to zero, we obtain the estimators for the new parameters, which we denote with a hat ($\\hat{\\cdot}$):\n$$\\hat{\\mu}_{\\ell a} = \\frac{\\sum_{i \\in \\mathcal{E}_t} w_i \\ln a_i}{\\sum_{i \\in \\mathcal{E}_t} w_i}$$\n$$\\hat{\\sigma}_{\\ell a}^2 = \\frac{\\sum_{i \\in \\mathcal{E}_t} w_i (\\ln a_i - \\hat{\\mu}_{\\ell a})^2}{\\sum_{i \\in \\mathcal{E}_t} w_i}$$\nThe weight $w_i$ can be pre-normalized over the elite set $\\mathcal{E}_t$ such that their sum is $1$, simplifying the denominators. The updates for the log-space parameters of $\\sigma$ and $K_{IC}$ are analogous.\n\nFinally, the CE algorithm uses exponential smoothing to stabilize convergence. The updated parameters $v_{t}$ are a convex combination of the previous parameters $v_{t-1}$ and the newly computed MLEs $\\hat{v}_{t}$:\n$$\\mu_{\\ell, t} = \\alpha \\hat{\\mu}_{\\ell} + (1-\\alpha)\\mu_{\\ell, t-1}$$\n$$\\sigma_{\\ell, t} = \\alpha \\hat{\\sigma}_{\\ell} + (1-\\alpha)\\sigma_{\\ell, t-1}$$\nThis is applied to all six log-space parameters, where $\\hat{\\sigma}_{\\ell} = \\sqrt{\\hat{\\sigma}_{\\ell}^2}$.\n\n### Implementation Logic\n\nThe implementation follows the derived Cross-Entropy procedure.\n1.  **Initialization**: The algorithm starts with the proposal parameters $v_0$ set to the nominal distribution parameters.\n2.  **Iteration**: For $t = 1, \\dots, T$:\n    a.  $N$ samples for $(a, \\sigma, K_{IC})$ are drawn from the current proposal $q(\\cdot;v_{t-1})$, which is a set of three independent lognormal distributions.\n    b.  The performance function $f(\\theta_i) = Y\\sigma_i\\sqrt{\\pi a_i} - K_{IC,i}$ is evaluated for each sample.\n    c.  The threshold $\\gamma_t$ is determined as the $(1-\\rho)$-quantile of the computed $f$ values.\n    d.  The elite set $\\mathcal{E}_t$ consists of all samples $\\theta_i$ for which $f(\\theta_i) \\ge \\gamma_t$.\n    e.  For each sample in $\\mathcal{E}_t$, an importance weight $w_i = \\pi(\\theta_i)/q(\\theta_i; v_{t-1})$ is calculated. This is done in log-space for numerical stability: $\\ln w_i = \\ln \\pi(\\theta_i) - \\ln q(\\theta_i; v_{t-1})$. The PDFs are sums of log-PDFs of the individual lognormal distributions. These weights are then normalized to sum to $1$.\n    f.  The new parameter estimates $\\hat{v}_t$ are computed using the weighted MLE formulas derived above on the elite set.\n    g.  The proposal parameters are updated to $v_t$ using the exponential smoothing rule with parameter $\\alpha$.\n3.  **Final Estimation**: After $T$ iterations, the optimized proposal distribution $q(\\cdot;v_T)$ is used.\n    a.  A large number of samples, $M$, are drawn from $q(\\cdot;v_T)$.\n    b.  The final probability estimate $\\hat{p}$ is computed using the standard importance sampling estimator: $\\hat{p} = \\frac{1}{M}\\sum_{i=1}^M \\mathbb{I}\\{f(\\theta_i)0\\} \\frac{\\pi(\\theta_i)}{q(\\theta_i;v_T)}$.\n\nThis procedure is applied to each test case with the specified hyperparameters and a fixed random seed to ensure reproducibility.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import lognorm\n\ndef cross_entropy_solver(case_params):\n    \"\"\"\n    Estimates the rare-event probability using the Cross-Entropy method.\n    \"\"\"\n    # Unpack case parameters\n    Y, nominal_params, hyperparams = case_params\n    N, T, rho, alpha, M = hyperparams\n\n    # Unpack nominal parameters (log-space)\n    mu_la_nom, sig_la_nom = nominal_params['a']\n    mu_ls_nom, sig_ls_nom = nominal_params['sigma']\n    mu_lk_nom, sig_lk_nom = nominal_params['k_ic']\n\n    # Initialize proposal distribution parameters with nominal values\n    v_t = {\n        'a': [mu_la_nom, sig_la_nom],\n        'sigma': [mu_ls_nom, sig_ls_nom],\n        'k_ic': [mu_lk_nom, sig_lk_nom],\n    }\n\n    # Cross-Entropy iteration loop\n    for t in range(T):\n        # 1. Draw N samples from the current proposal distribution q(.; v_t-1)\n        mu_la, sig_la = v_t['a']\n        mu_ls, sig_ls = v_t['sigma']\n        mu_lk, sig_lk = v_t['k_ic']\n\n        samples_a = np.random.lognormal(mean=mu_la, sigma=sig_la, size=N)\n        samples_sigma = np.random.lognormal(mean=mu_ls, sigma=sig_ls, size=N)\n        samples_k_ic = np.random.lognormal(mean=mu_lk, sigma=sig_lk, size=N)\n\n        # 2. Compute performance function f(theta_i)\n        f_theta = Y * samples_sigma * np.sqrt(np.pi * samples_a) - samples_k_ic\n\n        # 3. Select elite-set threshold gamma_t\n        gamma_t = np.quantile(f_theta, 1 - rho)\n\n        # 4. Form the elite set\n        elite_indices = np.where(f_theta >= gamma_t)[0]\n        if len(elite_indices) == 0:\n            # If no samples are elite, stop updating to avoid errors.\n            # This can happen in extreme cases or if rho is too small.\n            break\n\n        elite_a = samples_a[elite_indices]\n        elite_sigma = samples_sigma[elite_indices]\n        elite_k_ic = samples_k_ic[elite_indices]\n\n        # 5. Compute and normalize importance weights\n        # log_pdf(x, mu_log, sig_log)\n        def log_pdf(x, mu, sig):\n            return lognorm.logpdf(x, s=sig, scale=np.exp(mu))\n\n        # Log of nominal density PI(theta)\n        log_pi_a = log_pdf(elite_a, mu_la_nom, sig_la_nom)\n        log_pi_sigma = log_pdf(elite_sigma, mu_ls_nom, sig_ls_nom)\n        log_pi_k_ic = log_pdf(elite_k_ic, mu_lk_nom, sig_lk_nom)\n        log_pi = log_pi_a + log_pi_sigma + log_pi_k_ic\n\n        # Log of proposal density Q(theta)\n        log_q_a = log_pdf(elite_a, mu_la, sig_la)\n        log_q_sigma = log_pdf(elite_sigma, mu_ls, sig_ls)\n        log_q_k_ic = log_pdf(elite_k_ic, mu_lk, sig_lk)\n        log_q = log_q_a + log_q_sigma + log_q_k_ic\n\n        log_weights = log_pi - log_q\n        # Subtract max for numerical stability before exponentiating\n        log_weights -= np.max(log_weights)\n        weights = np.exp(log_weights)\n        normalized_weights = weights / np.sum(weights)\n\n        # 6. Update parameters via weighted MLE with smoothing\n        params_to_update = [\n            ('a', elite_a, mu_la, sig_la),\n            ('sigma', elite_sigma, mu_ls, sig_ls),\n            ('k_ic', elite_k_ic, mu_lk, sig_lk),\n        ]\n        \n        v_t_next = {}\n        for name, elite_samples, mu_current, sig_current in params_to_update:\n            log_elite_samples = np.log(elite_samples)\n            \n            # Weighted MLE for lognormal parameters\n            mu_hat = np.sum(normalized_weights * log_elite_samples)\n            var_hat = np.sum(normalized_weights * (log_elite_samples - mu_hat)**2)\n            sig_hat = np.sqrt(var_hat)\n            \n            # Exponential smoothing\n            mu_next = alpha * mu_hat + (1 - alpha) * mu_current\n            sig_next = alpha * sig_hat + (1 - alpha) * sig_current\n            v_t_next[name] = [mu_next, sig_next]\n            \n        v_t = v_t_next\n\n    # Final estimation step\n    mu_la_final, sig_la_final = v_t['a']\n    mu_ls_final, sig_ls_final = v_t['sigma']\n    mu_lk_final, sig_lk_final = v_t['k_ic']\n    \n    # Draw M samples from the final proposal distribution\n    final_samples_a = np.random.lognormal(mean=mu_la_final, sigma=sig_la_final, size=M)\n    final_samples_sigma = np.random.lognormal(mean=mu_ls_final, sigma=sig_ls_final, size=M)\n    final_samples_k_ic = np.random.lognormal(mean=mu_lk_final, sigma=sig_lk_final, size=M)\n\n    # Compute performance function\n    final_f_theta = Y * final_samples_sigma * np.sqrt(np.pi * final_samples_a) - final_samples_k_ic\n\n    # Indicator for failure events\n    indicator = (final_f_theta > 0).astype(float)\n    \n    # Final importance weights\n    log_pi_final_a = log_pdf(final_samples_a, mu_la_nom, sig_la_nom)\n    log_pi_final_sigma = log_pdf(final_samples_sigma, mu_ls_nom, sig_ls_nom)\n    log_pi_final_k_ic = log_pdf(final_samples_k_ic, mu_lk_nom, sig_lk_nom)\n    log_pi_final = log_pi_final_a + log_pi_final_sigma + log_pi_final_k_ic\n    \n    log_q_final_a = log_pdf(final_samples_a, mu_la_final, sig_la_final)\n    log_q_final_sigma = log_pdf(final_samples_sigma, mu_ls_final, sig_ls_final)\n    log_q_final_k_ic = log_pdf(final_samples_k_ic, mu_lk_final, sig_lk_final)\n    log_q_final = log_q_final_a + log_q_final_sigma + log_q_final_k_ic\n\n    final_weights = np.exp(log_pi_final - log_q_final)\n\n    # Final probability estimate\n    p_hat = np.mean(indicator * final_weights)\n\n    return p_hat\n\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and run the solver.\n    \"\"\"\n    np.random.seed(42)\n\n    hyperparams = [8000, 6, 0.1, 0.7, 150000] # N, T, rho, alpha, M\n\n    test_cases = [\n        # Case A\n        (1.12, {\n            'a': [np.log(3.0e-5), 0.5],\n            'sigma': [np.log(120), 0.25],\n            'k_ic': [np.log(60), 0.15],\n        }, hyperparams),\n        # Case B\n        (1.12, {\n            'a': [np.log(2.0e-4), 0.45],\n            'sigma': [np.log(150), 0.25],\n            'k_ic': [np.log(30), 0.20],\n        }, hyperparams),\n        # Case C\n        (1.12, {\n            'a': [np.log(5.0e-6), 0.4],\n            'sigma': [np.log(80), 0.20],\n            'k_ic': [np.log(120), 0.20],\n        }, hyperparams),\n    ]\n\n    results = []\n    for case in test_cases:\n        p_estimate = cross_entropy_solver(case)\n        results.append(f\"{p_estimate:.8e}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}