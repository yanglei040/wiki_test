{
    "hands_on_practices": [
        {
            "introduction": "The heart of the Transition Path Sampling (TPS) algorithm is the \"shooting move,\" a procedure for generating new reactive trajectories from an existing one. To ensure that our Monte Carlo simulation correctly samples the true path ensemble, the acceptance criterion for these moves must satisfy the detailed balance condition. This foundational exercise guides you through deriving the Metropolis-Hastings acceptance probability for a biased shooting move, a critical skill for understanding, implementing, and even modifying TPS algorithms.",
            "id": "1195098",
            "problem": "In Transition Path Sampling (TPS), a Monte Carlo method is used to sample the ensemble of reactive trajectories connecting two stable states, A and B. A common algorithm to generate new trial trajectories from an existing one is the \"shooting move.\"\n\nConsider a system governed by time-reversible, deterministic, energy-conserving dynamics (e.g., Hamiltonian). The ensemble of interest is the transition path ensemble of trajectories $x(t) = (\\mathbf{q}(t), \\mathbf{p}(t))$ with a fixed duration $T$. The probability of a given trajectory $x$ in this ensemble is given by\n$$\nP_{TP}[x] \\propto h_A(\\mathbf{q}(0)) h_B(\\mathbf{q}(T)) \\exp(-\\beta H[x])\n$$\nwhere $h_A$ and $h_B$ are characteristic functions that are 1 if the trajectory starts in region A and ends in region B respectively, and 0 otherwise. $H[x]$ is the total energy of the trajectory (which is constant along the path), and $\\beta$ is the inverse temperature.\n\nA biased two-way shooting move is proposed to generate a new trial path $x_{new}$ from an old path $x_{old}$:\n1.  A \"shooting point\" at time $t_s \\in [0, T]$ is selected from the old path $x_{old}$. The selection is biased by the potential energy $U(\\mathbf{q}(t))$ along the path, with the probability density for selecting $t_s$ given by:\n    $$\n    p_{sel}(t_s|x) = \\frac{\\exp(\\gamma U(\\mathbf{q}(t_s)))}{W[x]}\n    $$\n    Here, $\\gamma$ is a constant biasing parameter, and $W[x] = \\int_0^T \\exp(\\gamma U(\\mathbf{q}(t'))) dt'$ is a path-dependent normalization factor.\n\n2.  At the selected phase space point $(\\mathbf{q}_{old}(t_s), \\mathbf{p}_{old}(t_s))$, the momenta are perturbed by adding a random vector $\\delta \\mathbf{p}$, i.e., $\\mathbf{p}'(t_s) = \\mathbf{p}_{old}(t_s) + \\delta \\mathbf{p}$. The perturbation $\\delta \\mathbf{p}$ is drawn from a probability distribution $\\pi_{gen}(\\delta \\mathbf{p})$ which is symmetric, i.e., $\\pi_{gen}(\\delta \\mathbf{p}) = \\pi_{gen}(-\\delta \\mathbf{p})$.\n\n3.  A new trajectory $x_{new}$ is generated by integrating the equations of motion forward and backward in time from the new phase space point $(\\mathbf{q}_{old}(t_s), \\mathbf{p}'(t_s))$. Note that at the shooting time $t_s$, the new and old paths share the same configuration, $\\mathbf{q}_{new}(t_s) = \\mathbf{q}_{old}(t_s)$. The move is accepted only if the new path also connects A to B.\n\nTo ensure that the sampling converges to the correct distribution $P_{TP}[x]$, the acceptance probability must satisfy the detailed balance condition. The Metropolis-Hastings acceptance probability for the move $x_{old} \\to x_{new}$ is given by $P_{acc} = \\min(1, \\alpha)$.\n\nDerive the expression for the acceptance ratio $\\alpha$ for this biased shooting move. Express your answer in terms of the energies $H[x_{old}]$, $H[x_{new}]$, the normalization factors $W[x_{old}]$, $W[x_{new}]$, and the inverse temperature $\\beta$.",
            "solution": "1. The transition-path ensemble weight is\n$$P_{TP}[x] \\propto h_A(\\mathbf{q}(0)) h_B(\\mathbf{q}(T)) e^{-\\beta H[x]}.$$\n2. Detailed balance for Metropolis–Hastings requires\n$$P_{TP}[x_{old}] p_{prop}(x_{old}\\to x_{new}) P_{acc} = P_{TP}[x_{new}] p_{prop}(x_{new}\\to x_{old}) P_{acc}^{\\rm rev}.$$\nSince we choose $t_s$ with bias\n$$p_{sel}(t_s|x) = \\frac{e^{\\gamma U(\\mathbf{q}(t_s))}}{W[x]}, \\quad W[x] = \\int_0^T e^{\\gamma U(\\mathbf{q}(t'))} dt',$$\nand perturb momenta with a symmetric $\\pi_{gen}(\\delta \\mathbf{p}) = \\pi_{gen}(-\\delta \\mathbf{p})$, the forward and reverse proposal densities are\n$$p_{prop}(x_{old}\\to x_{new}) = p_{sel}(t_s|x_{old}) \\pi_{gen}(\\delta \\mathbf{p}), \\quad p_{prop}(x_{new}\\to x_{old}) = p_{sel}(t_s|x_{new}) \\pi_{gen}(-\\delta \\mathbf{p}).$$\n3. The ratio of proposals is\n$$\\frac{p_{prop}(x_{new}\\to x_{old})}{p_{prop}(x_{old}\\to x_{new})} \n= \\frac{p_{sel}(t_s|x_{new})}{p_{sel}(t_s|x_{old})}\n= \\frac{e^{\\gamma U(\\mathbf{q}(t_s))}/W[x_{new}]}{e^{\\gamma U(\\mathbf{q}(t_s))}/W[x_{old}]}\n= \\frac{W[x_{old}]}{W[x_{new}]}.$$\n4. The Metropolis ratio is then\n$$\n\\alpha\n= \\frac{P_{TP}[x_{new}] p_{prop}(x_{new}\\to x_{old})}{P_{TP}[x_{old}] p_{prop}(x_{old}\\to x_{new})}\n= \\frac{e^{-\\beta H[x_{new}]}}{e^{-\\beta H[x_{old}]}}\n\\frac{W[x_{old}]}{W[x_{new}]}\n= \\exp\\bigl(-\\beta(H[x_{new}]-H[x_{old}])\\bigr) \\frac{W[x_{old}]}{W[x_{new}]}.$$",
            "answer": "$$\\boxed{\\exp\\bigl(-\\beta\\bigl(H[x_{new}]-H[x_{old}]\\bigr)\\bigr)\\,\\frac{W[x_{old}]}{W[x_{new}]}}$$"
        },
        {
            "introduction": "Having explored the mechanics of path generation, we now turn to the object being sampled: the path ensemble itself. For systems governed by stochastic dynamics, the probability of a given trajectory can be expressed via a path action, a concept central to the path integral formulation of statistical mechanics. This practice challenges you to derive the explicit form of this action—the Onsager-Machlup functional—for a particle in a simple harmonic potential, thereby bridging the gap between the discrete steps of a Langevin simulation and the elegant continuous-time theoretical framework.",
            "id": "3498770",
            "problem": "In Transition Path Sampling (TPS) for rare-event dynamics in computational materials science, overdamped Langevin dynamics in one spatial dimension are often used as the fundamental model for atomic or collective coordinates subject to thermal fluctuations. Consider a particle with coordinate $x(t)$ evolving under a potential $V(x)$ at absolute temperature $T$ in the overdamped limit with mobility $\\mu$ and diffusion coefficient $D$. The dynamics are defined by the stochastic differential equation $dx(t) = -\\mu\\,\\partial_{x}V(x(t))\\,dt + \\sqrt{2D}\\,dW_{t}$, where $W_{t}$ is a standard Wiener process and $D$ and $\\mu$ satisfy the Einstein relation $D = \\mu k_{B} T$, with $k_{B}$ the Boltzmann constant. A discretization of these dynamics over a fixed observation time $\\tau$ with time step $\\Delta t$ and $N = \\tau/\\Delta t$ steps is given by the Euler–Maruyama scheme $x_{n+1} = x_{n} - \\mu\\,\\partial_{x}V(x_{n})\\,\\Delta t + \\sqrt{2D\\,\\Delta t}\\,\\eta_{n}$, with $\\eta_{n}$ independent standard normal random variables. The TPS path weight for a discrete path $\\{x_{0}, x_{1}, \\dots, x_{N}\\}$ with fixed endpoints $x(0) = x_{A}$ and $x(\\tau) = x_{B}$ can be written in the form of an exponential of a path functional $S[x]$ (the action), which in the continuous-time limit defines the Onsager–Machlup functional relevant for sampling and reweighting path ensembles.\n\nStarting from the discrete-time transition kernel implied by the above Euler–Maruyama scheme and the Einstein relation, derive the explicit continuous-time form of the action $S[x]$ for the harmonic potential $V(x) = \\frac{1}{2}k x^{2}$, with $k > 0$ a constant stiffness. Use only the stated fundamental dynamics and relations to construct the path probability density and identify $S[x]$ in the limit of small $\\Delta t$ (with $\\tau$ fixed). Then, by analyzing the discrete-time transition kernel, verify that the path measure is Gaussian in the increments and normalizable in the limit of small $\\Delta t$.\n\nExpress your final action $S[x]$ as a single closed-form analytic functional of $x(t)$, $\\mu$, $D$, $k$, and $\\tau$. No numerical evaluation is required.",
            "solution": "The overdamped Langevin dynamics $dx(t) = -\\mu\\,\\partial_{x}V(x(t))\\,dt + \\sqrt{2D}\\,dW_{t}$ with $D = \\mu k_{B} T$ define a Markov process whose discrete-time approximation via the Euler–Maruyama scheme is $x_{n+1} = x_{n} - \\mu\\,\\partial_{x}V(x_{n})\\,\\Delta t + \\sqrt{2D\\,\\Delta t}\\,\\eta_{n}$, where $\\eta_{n}$ are independent standard normal random variables. The conditional transition density for one step is Gaussian and given by\n$$\np(x_{n+1}\\,|\\,x_{n}) = \\frac{1}{\\sqrt{4\\pi D\\,\\Delta t}} \\exp\\!\\left(-\\frac{\\left[x_{n+1} - x_{n} + \\mu\\,\\partial_{x}V(x_{n})\\,\\Delta t\\right]^{2}}{4D\\,\\Delta t}\\right).\n$$\nThis follows from the distribution of $\\eta_{n}$ and the affine transformation linking $\\eta_{n}$ to $x_{n+1}$ for fixed $x_{n}$. For a path $\\{x_{0}, x_{1}, \\dots, x_{N}\\}$ with fixed endpoints, the (unnormalized) path probability density is the product of the stepwise conditional densities,\n$$\n\\mathcal{P}[\\{x_{n}\\}] \\propto \\prod_{n=0}^{N-1} \\exp\\!\\left(-\\frac{\\left[x_{n+1} - x_{n} + \\mu\\,\\partial_{x}V(x_{n})\\,\\Delta t\\right]^{2}}{4D\\,\\Delta t}\\right),\n$$\nup to a normalization factor $\\left(4\\pi D\\,\\Delta t\\right)^{-N/2}$ that is independent of the particular path when considered as a conditional density over successive $x_{n+1}$ given $x_{n}$. Identifying the action $S[x]$ via $\\mathcal{P}[\\{x_{n}\\}] \\propto \\exp(-S[\\{x_{n}\\}])$, we read off the discrete-time action\n$$\nS[\\{x_{n}\\}] = \\sum_{n=0}^{N-1} \\frac{\\left[x_{n+1} - x_{n} + \\mu\\,\\partial_{x}V(x_{n})\\,\\Delta t\\right]^{2}}{4D\\,\\Delta t}.\n$$\nWe now specialize to the harmonic potential $V(x) = \\frac{1}{2}k x^{2}$, for which $\\partial_{x}V(x) = k x$. The discrete action becomes\n$$\nS[\\{x_{n}\\}] = \\sum_{n=0}^{N-1} \\frac{\\left[x_{n+1} - x_{n} + \\mu k x_{n}\\,\\Delta t\\right]^{2}}{4D\\,\\Delta t}.\n$$\nTo obtain the continuous-time action, we consider the limit $\\Delta t \\to 0$ with $\\tau = N\\,\\Delta t$ fixed, such that $x_{n+1} - x_{n} \\approx \\dot{x}(t_{n})\\,\\Delta t$ with $t_{n} = n\\,\\Delta t$. Rewriting the sum as a Riemann sum and then a time integral, we find\n$$\nS[x] = \\int_{0}^{\\tau} \\frac{\\left[\\dot{x}(t) + \\mu k x(t)\\right]^{2}}{4D}\\,dt.\n$$\nThis expression is the Onsager–Machlup action for an overdamped particle in a harmonic potential with constant diffusion. It captures the quadratic penalty of deviations from the deterministic drift $\\dot{x}(t) = -\\mu k x(t)$, scaled by the noise strength $D$.\n\nWe now verify that the path measure is Gaussian and normalizable in the limit of small $\\Delta t$. The stepwise conditional density $p(x_{n+1}\\,|\\,x_{n})$ is a Gaussian in the variable $x_{n+1}$ with mean\n$$\nm_{n} = x_{n} - \\mu\\,\\partial_{x}V(x_{n})\\,\\Delta t = x_{n} - \\mu k x_{n}\\,\\Delta t,\n$$\nand variance\n$$\n\\sigma^{2} = 2D\\,\\Delta t.\n$$\nThe Gaussian form is explicit, and for each step,\n$$\n\\int_{-\\infty}^{\\infty} p(x_{n+1}\\,|\\,x_{n})\\,dx_{n+1} = 1,\n$$\nsince the normalization factor $1/\\sqrt{4\\pi D\\,\\Delta t}$ and the quadratic exponent define a properly normalized Gaussian for any $\\Delta t > 0$. In the limit $\\Delta t \\to 0$, the variance $\\sigma^{2} = 2D\\,\\Delta t$ tends to $0$, reflecting the weak-noise scaling per time step, but the sequential product structure ensures that the overall path measure, considered as a composition of normalized conditional densities, remains normalized over the space of trajectories for fixed endpoints and the Markovian evolution. The Gaussian character of the increments $x_{n+1} - x_{n}$ with mean $-\\mu k x_{n}\\,\\Delta t$ and variance $2D\\,\\Delta t$ ensures that the discrete path weight has the quadratic form used to define $S[\\{x_{n}\\}]$, and passing to the continuous limit yields the quadratic Onsager–Machlup functional $S[x]$ derived above. Any additional term associated with the divergence of the drift (for the Itō interpretation) would be $\\frac{\\mu}{2}\\int_{0}^{\\tau} \\partial_{x}^{2}V(x(t))\\,dt = \\frac{\\mu}{2}\\int_{0}^{\\tau} k\\,dt = \\frac{\\mu k \\tau}{2}$, which is path-independent and therefore contributes only to the overall normalization of the path measure; it does not change the Gaussian character nor the normalizability and can be omitted from $S[x]$ when focusing on relative path weights.\n\nThus, for the harmonic potential, the explicit continuous-time action functional is\n$$\nS[x] = \\int_{0}^{\\tau} \\frac{\\left[\\dot{x}(t) + \\mu k x(t)\\right]^{2}}{4D}\\,dt,\n$$\nand the discrete-time path measure is Gaussian in the increments with variance $2D\\,\\Delta t$, yielding a normalized conditional transition kernel at each step and a normalizable path probability density in the small $\\Delta t$ limit.",
            "answer": "$$\\boxed{S[x]=\\int_{0}^{\\tau}\\frac{\\left[\\dot{x}(t)+\\mu\\,k\\,x(t)\\right]^{2}}{4D}\\,dt}$$"
        },
        {
            "introduction": "While standard TPS is powerful, sampling extremely rare events can still be computationally prohibitive. Advanced techniques often employ biasing potentials to enhance the exploration of transition regions, a method analogous to umbrella sampling but applied to the entire path. This practice delves into this crucial technique by tasking you with deriving the reweighting factor needed to recover unbiased observables from a biased simulation. Furthermore, you will analyze the statistical efficiency of this approach, confronting the critical \"overlap problem\" that is paramount to the successful application of any advanced sampling method.",
            "id": "3498791",
            "problem": "Consider a reactive path ensemble in a molecular system evolving with overdamped Langevin dynamics at temperature $T$, with inverse temperature $\\beta = 1/(k_{B} T)$, where $k_{B}$ is the Boltzmann constant. Let a discrete-time path be denoted by $\\omega = (x_{0}, x_{1}, \\dots, x_{M})$, and let the unbiased path probability density (restricted to paths that start in basin $A$ and end in basin $B$) be $P_{0}(\\omega \\mid A \\to B) \\propto \\rho_{\\text{eq}}(x_{0}) \\prod_{t=0}^{M-1} p(x_{t+1} \\mid x_{t})$, where $\\rho_{\\text{eq}}(x) \\propto \\exp(-\\beta V(x))$ is the canonical equilibrium distribution associated with potential energy $V(x)$, and $p(x' \\mid x)$ is the single-step Markov transition kernel consistent with detailed balance with respect to $\\rho_{\\text{eq}}(x)$. Define a time-averaged order parameter $\\Lambda(\\omega) = \\frac{1}{M+1} \\sum_{t=0}^{M} \\lambda(x_{t})$.\n\nTo enhance sampling efficiency in transition path sampling, consider sampling from a biased reactive path ensemble with probability density\n$$\nP_{\\text{b}}(\\omega \\mid A \\to B) = \\frac{1}{Z_{\\text{b}}} \\, P_{0}(\\omega \\mid A \\to B) \\, \\exp\\!\\big(-\\beta \\, U_{\\text{bias}}(\\Lambda(\\omega))\\big),\n$$\nwhere $U_{\\text{bias}}(\\Lambda)$ is a prescribed bias potential acting on the path functional $\\Lambda(\\omega)$ and $Z_{\\text{b}}$ is the normalization factor ensuring that $P_{\\text{b}}$ integrates to $1$ over the reactive path space.\n\nTasks:\n- Starting from the definition of importance sampling and the above path measures, derive the reweighting factor $w(\\omega)$ that allows unbiased estimation of any path observable $O(\\omega)$ under $P_{0}(\\omega \\mid A \\to B)$ using samples from $P_{\\text{b}}(\\omega \\mid A \\to B)$. Write down the corresponding self-normalized estimator for $\\langle O \\rangle_{0} \\equiv \\int \\mathrm{d}\\omega \\, P_{0}(\\omega \\mid A \\to B) \\, O(\\omega)$ in terms of averages over the biased ensemble.\n- Now assume that, under the biased ensemble $P_{\\text{b}}$, the random variable $Y(\\omega) \\equiv U_{\\text{bias}}(\\Lambda(\\omega))$ is approximately Gaussian with mean $\\mu$ and variance $s^{2}$ (both finite and independent of the sample size), i.e., $Y \\sim \\mathcal{N}(\\mu, s^{2})$. Using this assumption and your reweighting factor, derive the squared coefficient of variation of the weights,\n$$\n\\mathrm{CV}^{2} \\equiv \\frac{\\operatorname{Var}_{\\text{b}}[w(\\omega)]}{\\big(\\mathbb{E}_{\\text{b}}[w(\\omega)]\\big)^{2}},\n$$\nas an explicit closed-form function of $\\beta$ and $s$. Interpret physically when the variance of reweighted observables becomes prohibitive in terms of overlap between $P_{0}$ and $P_{\\text{b}}$, and the effective sample size scaling with the number of independent paths $N$.\nYour final reported answer must be the explicit analytic expression for $\\mathrm{CV}^{2}$ as a function of $\\beta$ and $s$. This expression is dimensionless; report it without units.",
            "solution": "The first task is to derive the reweighting factor $w(\\omega)$ and the self-normalized estimator for an observable's average over the unbiased ensemble, $\\langle O \\rangle_{0}$, using samples from the biased ensemble.\n\nThe expectation value of an observable $O(\\omega)$ over the unbiased path probability distribution $P_{0}(\\omega \\mid A \\to B)$ is defined as:\n$$\n\\langle O \\rangle_{0} = \\int \\mathrm{d}\\omega \\, P_{0}(\\omega \\mid A \\to B) \\, O(\\omega)\n$$\nwhere the integral is over all paths $\\omega$ that start in state space region $A$ and end in region $B$. To evaluate this expectation using samples drawn from the biased distribution $P_{\\text{b}}(\\omega \\mid A \\to B)$, we employ the principle of importance sampling. We rewrite the integral by multiplying and dividing by $P_{\\text{b}}(\\omega \\mid A \\to B)$:\n$$\n\\langle O \\rangle_{0} = \\int \\mathrm{d}\\omega \\, \\frac{P_{0}(\\omega \\mid A \\to B)}{P_{\\text{b}}(\\omega \\mid A \\to B)} \\, P_{\\text{b}}(\\omega \\mid A \\to B) \\, O(\\omega)\n$$\nThis can be expressed as an expectation value over the biased ensemble:\n$$\n\\langle O \\rangle_{0} = \\mathbb{E}_{\\text{b}} \\left[ w(\\omega) O(\\omega) \\right]\n$$\nwhere the reweighting factor $w(\\omega)$ is defined as the ratio of the probability densities:\n$$\nw(\\omega) \\equiv \\frac{P_{0}(\\omega \\mid A \\to B)}{P_{\\text{b}}(\\omega \\mid A \\to B)}\n$$\nUsing the given definition for $P_{\\text{b}}(\\omega \\mid A \\to B)$:\n$$\nP_{\\text{b}}(\\omega \\mid A \\to B) = \\frac{1}{Z_{\\text{b}}} \\, P_{0}(\\omega \\mid A \\to B) \\, \\exp(-\\beta \\, U_{\\text{bias}}(\\Lambda(\\omega)))\n$$\nwe can find an explicit expression for the reweighting factor:\n$$\nw(\\omega) = \\frac{P_{0}(\\omega \\mid A \\to B)}{\\frac{1}{Z_{\\text{b}}} \\, P_{0}(\\omega \\mid A \\to B) \\, \\exp(-\\beta \\, U_{\\text{bias}}(\\Lambda(\\omega)))} = Z_{\\text{b}} \\exp(\\beta \\, U_{\\text{bias}}(\\Lambda(\\omega)))\n$$\nThe normalization constant $Z_{\\text{b}}$ is generally unknown. To address this, we use a self-normalized estimator. We can write $\\langle O \\rangle_{0}$ as a ratio of expectations:\n$$\n\\langle O \\rangle_{0} = \\frac{\\int \\mathrm{d}\\omega \\, O(\\omega) P_{0}(\\omega)}{\\int \\mathrm{d}\\omega \\, P_{0}(\\omega)} = \\frac{\\int \\mathrm{d}\\omega \\, O(\\omega) w(\\omega) P_{\\text{b}}(\\omega)}{\\int \\mathrm{d}\\omega \\, w(\\omega) P_{\\text{b}}(\\omega)} = \\frac{\\mathbb{E}_{\\text{b}}[O(\\omega)w(\\omega)]}{\\mathbb{E}_{\\text{b}}[w(\\omega)]}\n$$\nSubstituting the expression for $w(\\omega)$, the unknown constant $Z_{\\text{b}}$ cancels out:\n$$\n\\langle O \\rangle_{0} = \\frac{\\mathbb{E}_{\\text{b}}[O(\\omega) Z_{\\text{b}} \\exp(\\beta \\, U_{\\text{bias}}(\\Lambda(\\omega)))]}{\\mathbb{E}_{\\text{b}}[Z_{\\text{b}} \\exp(\\beta \\, U_{\\text{bias}}(\\Lambda(\\omega)))]} = \\frac{\\mathbb{E}_{\\text{b}}[O(\\omega) \\exp(\\beta \\, U_{\\text{bias}}(\\Lambda(\\omega)))]}{\\mathbb{E}_{\\text{b}}[\\exp(\\beta \\, U_{\\text{bias}}(\\Lambda(\\omega)))]}\n$$\nThis is the self-normalized estimator for $\\langle O \\rangle_{0}$ in terms of averages over the biased ensemble $P_{\\text{b}}$. If one has a set of $N$ paths $\\{\\omega_i\\}_{i=1}^N$ drawn from $P_{\\text{b}}$, the estimator is computed as:\n$$\n\\langle O \\rangle_{0} \\approx \\frac{\\sum_{i=1}^{N} O(\\omega_i) \\exp(\\beta \\, U_{\\text{bias}}(\\Lambda(\\omega_i)))}{\\sum_{i=1}^{N} \\exp(\\beta \\, U_{\\text{bias}}(\\Lambda(\\omega_i)))}\n$$\n\nThe second task is to derive the squared coefficient of variation of the weights, $\\mathrm{CV}^{2}$, under the assumption that $Y(\\omega) \\equiv U_{\\text{bias}}(\\Lambda(\\omega))$ follows a Gaussian distribution, $Y \\sim \\mathcal{N}(\\mu, s^{2})$, when sampled from the biased ensemble $P_{\\text{b}}$.\n\nThe squared coefficient of variation is defined as:\n$$\n\\mathrm{CV}^{2} \\equiv \\frac{\\operatorname{Var}_{\\text{b}}[w(\\omega)]}{\\left(\\mathbb{E}_{\\text{b}}[w(\\omega)]\\right)^{2}}\n$$\nUsing the identity $\\operatorname{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2$, we can write:\n$$\n\\mathrm{CV}^{2} = \\frac{\\mathbb{E}_{\\text{b}}[w(\\omega)^2] - \\left(\\mathbb{E}_{\\text{b}}[w(\\omega)]\\right)^{2}}{\\left(\\mathbb{E}_{\\text{b}}[w(\\omega)]\\right)^{2}} = \\frac{\\mathbb{E}_{\\text{b}}[w(\\omega)^2]}{\\left(\\mathbb{E}_{\\text{b}}[w(\\omega)]\\right)^{2}} - 1\n$$\nWe need to compute the first and second moments of $w(\\omega)$ with respect to the biased distribution $P_{\\text{b}}$. The weight is $w(\\omega) = Z_{\\text{b}} \\exp(\\beta Y(\\omega))$.\nThe expectations are:\n$$\n\\mathbb{E}_{\\text{b}}[w(\\omega)] = \\mathbb{E}_{\\text{b}}[Z_{\\text{b}} \\exp(\\beta Y)] = Z_{\\text{b}} \\mathbb{E}_{\\text{b}}[\\exp(\\beta Y)]\n$$\n$$\n\\mathbb{E}_{\\text{b}}[w(\\omega)^2] = \\mathbb{E}_{\\text{b}}[(Z_{\\text{b}} \\exp(\\beta Y))^2] = Z_{\\text{b}}^2 \\mathbb{E}_{\\text{b}}[\\exp(2\\beta Y)]\n$$\nThe expectations $\\mathbb{E}_{\\text{b}}[\\exp(k Y)]$ are moments of the random variable $Y$. Since $Y$ is Gaussian, $Y \\sim \\mathcal{N}(\\mu, s^{2})$, we can use the formula for its moment-generating function, $M_Y(t) = \\mathbb{E}[\\exp(tY)] = \\exp(\\mu t + \\frac{1}{2}\\sigma^2 t^2)$. In our case, the variance is $\\sigma^2 = s^2$.\n\nFor the first moment of $w$, we set $t = \\beta$:\n$$\n\\mathbb{E}_{\\text{b}}[\\exp(\\beta Y)] = M_Y(\\beta) = \\exp\\left(\\mu \\beta + \\frac{1}{2}s^2 \\beta^2\\right)\n$$\nThus,\n$$\n\\mathbb{E}_{\\text{b}}[w(\\omega)] = Z_{\\text{b}} \\exp\\left(\\mu \\beta + \\frac{1}{2}s^2 \\beta^2\\right)\n$$\nFor the second moment of $w$, we need the expectation of $\\exp(2\\beta Y)$, so we set $t = 2\\beta$:\n$$\n\\mathbb{E}_{\\text{b}}[\\exp(2\\beta Y)] = M_Y(2\\beta) = \\exp\\left(\\mu (2\\beta) + \\frac{1}{2}s^2 (2\\beta)^2\\right) = \\exp\\left(2\\mu\\beta + 2s^2\\beta^2\\right)\n$$\nThus,\n$$\n\\mathbb{E}_{\\text{b}}[w(\\omega)^2] = Z_{\\text{b}}^2 \\exp\\left(2\\mu\\beta + 2s^2\\beta^2\\right)\n$$\nNow, we substitute these moments back into the expression for $\\mathrm{CV}^{2}$:\n$$\n\\mathrm{CV}^{2} = \\frac{Z_{\\text{b}}^2 \\exp(2\\mu\\beta + 2s^2\\beta^2)}{\\left(Z_{\\text{b}} \\exp(\\mu \\beta + \\frac{1}{2}s^2 \\beta^2)\\right)^2} - 1\n$$\n$$\n\\mathrm{CV}^{2} = \\frac{Z_{\\text{b}}^2 \\exp(2\\mu\\beta + 2s^2\\beta^2)}{Z_{\\text{b}}^2 \\exp(2(\\mu \\beta + \\frac{1}{2}s^2 \\beta^2))} - 1 = \\frac{\\exp(2\\mu\\beta + 2s^2\\beta^2)}{\\exp(2\\mu\\beta + s^2\\beta^2)} - 1\n$$\n$$\n\\mathrm{CV}^{2} = \\exp(2\\mu\\beta + 2s^2\\beta^2 - (2\\mu\\beta + s^2\\beta^2)) - 1 = \\exp(s^2\\beta^2) - 1\n$$\nThis is the desired closed-form expression for the squared coefficient of variation of the weights as a function of $\\beta$ and $s$.\n\nFinally, we interpret the result. The quantity $\\mathrm{CV}^{2}$ is a measure of the statistical inefficiency of the importance sampling procedure. A large $\\mathrm{CV}^{2}$ indicates that the variance of the weights is large, meaning the estimates of observables will be dominated by a few trajectories with large weights, leading to poor convergence and high statistical error. A common metric for sampling efficiency is the effective sample size, $N_{\\text{eff}}$, which for a set of $N$ independent samples is often approximated as $N_{\\text{eff}} \\approx \\frac{N}{1 + \\mathrm{CV}^2}$. Substituting our result:\n$$\nN_{\\text{eff}} \\approx \\frac{N}{1 + (\\exp(s^2\\beta^2) - 1)} = \\frac{N}{\\exp(s^2\\beta^2)} = N \\exp(-s^2\\beta^2)\n$$\nThis expression shows that the number of effective samples decreases exponentially with the parameter $s^2\\beta^2$. The term $s^2$ is the variance of the bias potential energy $U_{\\text{bias}}$ experienced by the paths in the biased ensemble, and $\\beta^2 = (1/k_B T)^2$. Therefore, $s^2\\beta^2$ is the variance of the bias potential energy measured in units of $(k_B T)^2$.\nThe variance of reweighted observables becomes prohibitive when $s^2\\beta^2 \\gtrsim 1$. This condition implies that the standard deviation of the bias energy, $s$, is on the order of or larger than the thermal energy $k_B T$. This wide fluctuation in bias energy means that the biased sampling explores regions of path space that have vastly different probabilities under the unbiased distribution $P_0$. This lack of overlap between the sampled distribution $P_{\\text{b}}$ and the target distribution $P_0$ is the fundamental reason for the high variance of the weights and the resulting inefficiency of the reweighting scheme.",
            "answer": "$$\\boxed{\\exp(s^{2}\\beta^{2}) - 1}$$"
        }
    ]
}