{
    "hands_on_practices": [
        {
            "introduction": "The journey of data-driven materials discovery begins not with complex algorithms, but with meticulous data preparation. Materials data aggregated from diverse sources often suffers from inconsistencies in units and the presence of erroneous entries or outliers. This exercise  provides hands-on practice in building a robust data curation pipeline, a critical first step that ensures the quality and integrity of your dataset. You will implement unit standardization for common materials properties and apply a statistically sound method based on the Median Absolute Deviation (MAD) to identify and remove outliers.",
            "id": "3463877",
            "problem": "Design and implement a complete, self-contained data curation program for a small materials property database that performs two tasks on each numerical property: unit standardization and robust outlier removal. The program must also maintain provenance information for each record indicating whether a unit transformation was applied to each property. Your implementation must be based on fundamental definitions from unit analysis and robust statistics, and it must be fully deterministic.\n\nThe pipeline must operate on two scalar properties per record:\n- Formation energy per atom, denoted by $E_{\\mathrm{f}}$, reported either in electronvolt per atom ($\\mathrm{eV/atom}$) or kilojoule per mole ($\\mathrm{kJ/mol}$).\n- Synthesis temperature, denoted by $T$, reported either in Kelvin ($\\mathrm{K}$) or degrees Celsius (°C).\n\nUnit standardization must map each reported value $y$ to a standardized value $y'$ using a linear transformation of the form $y' = a\\,y + b$, with coefficients determined by physical unit relations:\n- For temperature, the mapping to Kelvin is given by the linear relation between Celsius and Kelvin:\n  - If $y$ is in $\\mathrm{K}$, use $a = 1$ and $b = 0$.\n  - If $y$ is in °C, use $a = 1$ and $b = 273.15$ so that $T_{\\mathrm{K}} = T_{{}^{\\circ}\\mathrm{C}} + 273.15$.\n- For formation energy per atom, the mapping to $\\mathrm{eV/atom}$ from $\\mathrm{kJ/mol}$ must be derived from the International System of Units (SI) definitions using the Avogadro constant $N_{\\mathrm{A}} = 6.02214076 \\times 10^{23}\\ \\mathrm{mol}^{-1}$ and the elementary charge $e = 1.602176634 \\times 10^{-19}\\ \\mathrm{C}$. One has $1\\ \\mathrm{eV} = 1.602176634 \\times 10^{-19}\\ \\mathrm{J}$, and $1\\ \\mathrm{kJ/mol} = \\dfrac{10^{3}}{N_{\\mathrm{A}}}\\ \\mathrm{J/particle}$. Therefore,\n  $$\n  a = \\frac{10^{3}}{N_{\\mathrm{A}} e},\\quad b = 0,\\quad \\text{so that}\\quad E_{\\mathrm{f}}[\\mathrm{eV/atom}] = \\frac{10^{3}}{N_{\\mathrm{A}} e} \\, E_{\\mathrm{f}}[\\mathrm{kJ/mol}].\n  $$\n  If $E_{\\mathrm{f}}$ is already in $\\mathrm{eV/atom}$, use $a = 1$ and $b = 0$.\n\nRobust outlier removal must be performed independently on the standardized arrays $\\{E_{\\mathrm{f},i}'\\}$ and $\\{T_{i}'\\}$ using the following well-tested robust-statistics definitions:\n- The median $m$ of a finite set $\\{x_{i}\\}_{i=1}^{n}$ is any value that minimizes the sum of absolute deviations and, for odd $n$, equals the central order statistic.\n- The median absolute deviation (MAD) is defined as\n  $$\n  \\mathrm{MAD} = \\operatorname{median}\\big(\\,\\lvert x_{i} - m \\rvert\\,\\big).\n  $$\n- Use the normal-consistency scale factor $c = 1.4826$ to define the robust scale $s = c \\cdot \\mathrm{MAD}$. The robust score for each $x_{i}$ is\n  $$\n  z_{i} = \\frac{\\lvert x_{i} - m \\rvert}{s}.\n  $$\n- A record is flagged as an outlier for that property if $z_{i} > \\tau$, where $\\tau$ is the property-specific threshold.\n- Boundary handling when $\\mathrm{MAD} = 0$: define $z_{i} = 0$ if $x_{i} = m$ and $z_{i} = +\\infty$ if $x_{i} \\neq m$. Thus, when $\\mathrm{MAD} = 0$, any nonidentical value is treated as an outlier for that property.\n\nA record is removed from the curated dataset if it is flagged as an outlier for either standardized $E_{\\mathrm{f}}$ or standardized $T$. Provenance must be tracked as a function $\\pi(\\text{record})$ that at least encodes two binary indicators per record: whether $E_{\\mathrm{f}}$ was unit-transformed and whether $T$ was unit-transformed. For each test case, the program must report the number of kept records that required an $E_{\\mathrm{f}}$ unit transformation and the number of kept records that required a $T$ unit transformation.\n\nYour program must hard-code and process the following test suite. In each test case, list entries are indexed from $0$.\n\n- Test case $1$ (mixed units with clear outliers):\n  - Thresholds: $\\tau_{E} = 3.5$, $\\tau_{T} = 3.5$.\n  - Records (each as $(\\text{index}, E_{\\mathrm{f}}, \\text{unit}_{E}, T, \\text{unit}_{T})$):\n    - $(0, -2.5, \\mathrm{eV/atom}, 800, °C)$\n    - $(1, -240, \\mathrm{kJ/mol}, 750, °C)$\n    - $(2, -3.0, \\mathrm{eV/atom}, 1050, \\mathrm{K})$\n    - $(3, -150, \\mathrm{kJ/mol}, 300, \\mathrm{K})$\n    - $(4, -2.6, \\mathrm{eV/atom}, 25, °C)$\n    - $(5, -900, \\mathrm{kJ/mol}, 900, °C)$\n    - $(6, 0.2, \\mathrm{eV/atom}, 100, \\mathrm{K})$\n- Test case $2$ (boundary with $\\mathrm{MAD} = 0$ for $E_{\\mathrm{f}}$):\n  - Thresholds: $\\tau_{E} = 3.5$, $\\tau_{T} = 10.0$.\n  - Records:\n    - $(0, -1.0, \\mathrm{eV/atom}, 300, \\mathrm{K})$\n    - $(1, -96.4853321233, \\mathrm{kJ/mol}, 27, °C)$\n    - $(2, -1.0, \\mathrm{eV/atom}, 300, \\mathrm{K})$\n- Test case $3$ (offset sensitivity for temperature, $\\mathrm{MAD} = 0$ for $T$):\n  - Thresholds: $\\tau_{E} = 10.0$, $\\tau_{T} = 3.5$.\n  - Records:\n    - $(0, -2.0, \\mathrm{eV/atom}, 0, °C)$\n    - $(1, -193.0, \\mathrm{kJ/mol}, 273.15, \\mathrm{K})$\n    - $(2, -2.0, \\mathrm{eV/atom}, -273.15, °C)$\n- Test case $4$ (asymmetric $E_{\\mathrm{f}}$ outlier with nonzero $\\mathrm{MAD}$):\n  - Thresholds: $\\tau_{E} = 3.0$, $\\tau_{T} = 10.0$.\n  - Records:\n    - $(0, -2.0, \\mathrm{eV/atom}, 300, \\mathrm{K})$\n    - $(1, -2.5, \\mathrm{eV/atom}, 300, \\mathrm{K})$\n    - $(2, -1.5, \\mathrm{eV/atom}, 300, \\mathrm{K})$\n    - $(3, -2.0, \\mathrm{eV/atom}, 27, °C)$\n    - $(4, 3.0, \\mathrm{eV/atom}, 300, \\mathrm{K})$\n\nImplementation requirements:\n- Use the transformation $y' = a y + b$ for both properties as above to standardize to $\\mathrm{eV/atom}$ and $\\mathrm{K}$.\n- Use the robust outlier rule with $c = 1.4826$, strict inequality $z_{i} > \\tau$ for flagging outliers, and the $\\mathrm{MAD} = 0$ boundary handling described above.\n- Track provenance $\\pi(\\text{record})$ minimally as two binary flags per record indicating whether a unit transformation was applied to $E_{\\mathrm{f}}$ and to $T$.\n- For each test case, output a list with three elements: the sorted list of indices of records kept after curation, the integer count of kept records that required an $E_{\\mathrm{f}}$ unit transformation (originally in $\\mathrm{kJ/mol}$), and the integer count of kept records that required a $T$ unit transformation (originally in °C).\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets, where each test case result is of the form $[\\text{kept\\_indices\\_list}, \\text{count\\_Ef\\_transformed}, \\text{count\\_T\\_transformed}]$. For example, a syntactically valid format is $[[[0,1],1,1],[[0],0,0],\\dots]$ with no additional text. All temperatures must be in $\\mathrm{K}$ and all formation energies must be in $\\mathrm{eV/atom}$ within the curation logic; counts are unitless integers.",
            "solution": "The problem requires the design and implementation of a data curation program for materials property data. The program must perform two sequential tasks: unit standardization and robust outlier removal, while also tracking the provenance of transformations. The solution is designed to be fully deterministic and grounded in the provided physical and statistical definitions.\n\nThe overall algorithm processes a list of records, where each record contains a formation energy ($E_{\\mathrm{f}}$) and a synthesis temperature ($T$), each with associated units.\n\nFirst, we establish the necessary physical constants and conversion factors as specified.\n- Avogadro constant: $N_{\\mathrm{A}} = 6.02214076 \\times 10^{23}\\ \\mathrm{mol}^{-1}$\n- Elementary charge: $e = 1.602176634 \\times 10^{-19}\\ \\mathrm{C}$\n- Temperature conversion offset from Celsius to Kelvin: $b = 273.15$\n- Conversion factor for energy from $\\mathrm{kJ/mol}$ to $\\mathrm{eV/atom}$:\n$$a = \\frac{10^{3}}{N_{\\mathrm{A}} e} \\approx 0.01036426965\\ \\frac{\\mathrm{eV/atom}}{\\mathrm{kJ/mol}}$$\n\nThe curation pipeline is implemented as a three-step process for each test case.\n\n**Step 1: Unit Standardization and Provenance Tracking**\nEach record is processed to convert its properties to a standard set of units: $E_{\\mathrm{f}}$ in $\\mathrm{eV/atom}$ and $T$ in Kelvin ($\\mathrm{K}$). This is achieved using the linear transformation $y' = ay + b$.\n\n- For temperature $T$:\n  - If the unit is °C, the standardized value is $T' = T + 273.15$. The parameters are $a = 1$ and $b = 273.15$. A provenance flag `T_transformed` is set to true.\n  - If the unit is already $\\mathrm{K}$, then $T' = T$ ($a = 1$, $b = 0$). The `T_transformed` flag is set to false.\n\n- For formation energy $E_{\\mathrm{f}}$:\n  - If the unit is $\\mathrm{kJ/mol}$, the standardized value is $E_{\\mathrm{f}}' = E_{\\mathrm{f}} \\cdot (10^3 / (N_{\\mathrm{A}} e))$. The parameters are $a = 10^3 / (N_{\\mathrm{A}} e)$ and $b = 0$. A provenance flag `Ef_transformed` is set to true.\n  - If the unit is already $\\mathrm{eV/atom}$, then $E_{\\mathrm{f}}' = E_{\\mathrm{f}}$ ($a=1$, $b=0$). The `Ef_transformed` flag is set to false.\n\nThe standardized values $\\{E_{\\mathrm{f},i}'\\}$ and $\\{T_{i}'\\}$ are stored in separate numerical arrays, and the provenance flags are stored in corresponding boolean arrays for later aggregation.\n\n**Step 2: Robust Outlier Removal**\nThis step is performed independently on the array of standardized formation energies $\\{E_{\\mathrm{f},i}'\\}$ and the array of standardized temperatures $\\{T_{i}'\\}$. For a generic data array $\\{x_i\\}$ of size $n$, the procedure is as follows:\n\n1.  Compute the median, $m = \\operatorname{median}(\\{x_i\\})$. The `numpy.median` function is used for this, which correctly handles both odd and even sized arrays.\n2.  Compute the absolute deviations from the median for each data point: $\\{|x_i - m|\\}$.\n3.  Compute the median absolute deviation, $\\mathrm{MAD} = \\operatorname{median}(\\{|x_i - m|\\})$.\n4.  Handle the boundary case where $\\mathrm{MAD} = 0$:\n    - The problem defines the robust score $z_i$ to be $0$ if $x_i = m$ and $+\\infty$ if $x_i \\neq m$.\n    - The outlier condition is $z_i > \\tau$. For any finite threshold $\\tau > 0$, this means any point $x_i$ that is not equal to the median is flagged as an outlier.\n5.  Handle the standard case where $\\mathrm{MAD} > 0$:\n    - Calculate the robust scale factor $s = c \\cdot \\mathrm{MAD}$, using the given constant $c = 1.4826$.\n    - Compute the robust score for each point: $z_i = |x_i - m| / s$.\n    - A point $x_i$ is flagged as an outlier if its score $z_i$ is strictly greater than the property-specific threshold $\\tau$, i.e., $z_i > \\tau$.\n\nThis procedure yields two boolean arrays of outlier flags, one for $E_{\\mathrm{f}}$ and one for $T$.\n\n**Step 3: Record Curation and Provenance Aggregation**\nThe final step integrates the results of the outlier analysis and computes the required output values.\n\n1.  **Record Filtering**: A record $i$ is identified as a final outlier and is removed from the dataset if it was flagged as an outlier for either formation energy OR temperature. This corresponds to a logical OR operation on the two outlier flag arrays.\n2.  **Kept Indices**: The indices of the records that are not final outliers are collected. This list of kept indices is then sorted in ascending order.\n3.  **Provenance Counting**: The number of kept records that required an $E_{\\mathrm{f}}$ transformation is calculated by summing the `Ef_transformed` flags for the kept records. Similarly, the count for $T$ transformations is found by summing the `T_transformed` flags for the same kept records.\n\nThe final output for each test case is a list containing three elements: the sorted list of kept indices, the integer count of $E_{\\mathrmf}$ transformations for kept records, and the integer count of $T$ transformations for kept records. The program iterates through all provided test cases and formats the collected results into a single output line as specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the data curation pipeline on all test cases.\n    \"\"\"\n    \n    # Define physical and statistical constants as per the problem description.\n    NA = 6.02214076e23  # Avogadro constant (mol^-1)\n    E = 1.602176634e-19  # Elementary charge (C)\n    KJ_PER_MOL_TO_EV_PER_ATOM = 1000.0 / (NA * E)\n    C_TO_K_OFFSET = 273.15\n    MAD_SCALE_FACTOR = 1.4826\n    \n    def get_outlier_flags(data, tau):\n        \"\"\"\n        Identifies outliers in a dataset using the MAD-based robust statistics method.\n        \n        Args:\n            data (np.ndarray): The array of numerical data.\n            tau (float): The outlier threshold for the robust score.\n        \n        Returns:\n            np.ndarray: A boolean array where True indicates an outlier.\n        \"\"\"\n        if len(data) == 0:\n            return np.array([], dtype=bool)\n\n        data_array = np.asarray(data)\n        \n        # Calculate the median of the data.\n        median_val = np.median(data_array)\n        \n        # Calculate the absolute deviations from the median.\n        abs_devs = np.abs(data_array - median_val)\n        \n        # Calculate the Median Absolute Deviation (MAD).\n        mad = np.median(abs_devs)\n        \n        # Determine outliers based on the MAD value.\n        if mad == 0.0:\n            # Boundary case: MAD is zero. Outliers are any points not equal to the median.\n            # This corresponds to an infinite robust score for those points.\n            outlier_flags = (data_array != median_val)\n        else:\n            # Standard case: MAD is non-zero.\n            robust_scale = MAD_SCALE_FACTOR * mad\n            z_scores = abs_devs / robust_scale\n            outlier_flags = z_scores > tau\n            \n        return outlier_flags\n\n    def process_case(records, tau_E, tau_T):\n        \"\"\"\n        Processes a single test case, performing standardization and outlier removal.\n        \n        Args:\n            records (list): A list of tuples, each representing a data record.\n            tau_E (float): The outlier threshold for formation energy.\n            tau_T (float): The outlier threshold for temperature.\n            \n        Returns:\n            list: A list containing [kept_indices, count_Ef_transformed, count_T_transformed].\n        \"\"\"\n        if not records:\n            return [[], 0, 0]\n\n        num_records = len(records)\n        original_indices = np.array([r[0] for r in records])\n        \n        standardized_ef = np.zeros(num_records)\n        standardized_t = np.zeros(num_records)\n        ef_transformed = np.zeros(num_records, dtype=bool)\n        t_transformed = np.zeros(num_records, dtype=bool)\n\n        # Step 1: Unit Standardization and Provenance Tracking\n        for i, record in enumerate(records):\n            _, ef_val, ef_unit, t_val, t_unit = record\n            \n            if ef_unit == 'kJ/mol':\n                standardized_ef[i] = ef_val * KJ_PER_MOL_TO_EV_PER_ATOM\n                ef_transformed[i] = True\n            else:  # 'eV/atom'\n                standardized_ef[i] = ef_val\n            \n            if t_unit == '°C':\n                standardized_t[i] = t_val + C_TO_K_OFFSET\n                t_transformed[i] = True\n            else:  # 'K'\n                standardized_t[i] = t_val\n\n        # Step 2: Robust Outlier Removal\n        ef_outlier_flags = get_outlier_flags(standardized_ef, tau_E)\n        t_outlier_flags = get_outlier_flags(standardized_t, tau_T)\n        \n        # A record is an outlier if it's an outlier for either property.\n        combined_outlier_flags = ef_outlier_flags | t_outlier_flags\n        \n        # Step 3: Record Curation and Provenance Aggregation\n        kept_mask = ~combined_outlier_flags\n        kept_indices = sorted(original_indices[kept_mask].tolist())\n        \n        count_ef_transformed = int(np.sum(ef_transformed[kept_mask]))\n        count_t_transformed = int(np.sum(t_transformed[kept_mask]))\n        \n        return [kept_indices, count_ef_transformed, count_t_transformed]\n\n    # Hard-code and process the test suite from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            \"tau_E\": 3.5, \"tau_T\": 3.5,\n            \"records\": [\n                (0, -2.5, 'eV/atom', 800, '°C'), (1, -240, 'kJ/mol', 750, '°C'),\n                (2, -3.0, 'eV/atom', 1050, 'K'), (3, -150, 'kJ/mol', 300, 'K'),\n                (4, -2.6, 'eV/atom', 25, '°C'), (5, -900, 'kJ/mol', 900, '°C'),\n                (6, 0.2, 'eV/atom', 100, 'K')\n            ]\n        },\n        # Test case 2\n        {\n            \"tau_E\": 3.5, \"tau_T\": 10.0,\n            \"records\": [\n                (0, -1.0, 'eV/atom', 300, 'K'),\n                (1, -96.4853321233, 'kJ/mol', 27, '°C'),\n                (2, -1.0, 'eV/atom', 300, 'K')\n            ]\n        },\n        # Test case 3\n        {\n            \"tau_E\": 10.0, \"tau_T\": 3.5,\n            \"records\": [\n                (0, -2.0, 'eV/atom', 0, '°C'),\n                (1, -193.0, 'kJ/mol', 273.15, 'K'),\n                (2, -2.0, 'eV/atom', -273.15, '°C')\n            ]\n        },\n        # Test case 4\n        {\n            \"tau_E\": 3.0, \"tau_T\": 10.0,\n            \"records\": [\n                (0, -2.0, 'eV/atom', 300, 'K'), (1, -2.5, 'eV/atom', 300, 'K'),\n                (2, -1.5, 'eV/atom', 300, 'K'), (3, -2.0, 'eV/atom', 27, '°C'),\n                (4, 3.0, 'eV/atom', 300, 'K')\n            ]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(case[\"records\"], case[\"tau_E\"], case[\"tau_T\"])\n        # Format the result list into its string representation and remove spaces.\n        results.append(str(result).replace(\" \", \"\"))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Crystal structures are the fundamental input for many materials informatics models, yet their representation is not unique; the same periodic lattice can be described by an infinite number of unit cells. To meaningfully compare structures or use them as machine learning features, we must first map them to a canonical form. This practice  delves into this essential task by guiding you through the implementation of the Niggli reduction algorithm, a standard method for producing a unique, conventional cell representation from first principles.",
            "id": "3463942",
            "problem": "You are given a set of cases, each specified by a real-space lattice basis (a matrix of three linearly independent lattice vectors in Cartesian coordinates) and a set of atomic positions in fractional coordinates. Your task is to write a complete and runnable program that, starting only from core definitions about Bravais lattices and integer change-of-basis operations, constructs a Niggli-reduced primitive cell and consistently transforms all atomic fractional coordinates into that reduced representation.\n\nFundamental base and definitions to be used:\n- A Bravais lattice in three dimensions is the set of vectors $\\{\\mathbf{r} = \\mathbf{L}\\mathbf{f} \\mid \\mathbf{f} \\in \\mathbb{Z}^3\\}$ generated by a basis matrix $\\mathbf{L} \\in \\mathbb{R}^{3 \\times 3}$ whose columns are the lattice vectors $\\mathbf{a}$, $\\mathbf{b}$, $\\mathbf{c}$. The metric tensor is $\\mathbf{G} = \\mathbf{L}^{\\mathsf{T}}\\mathbf{L}$, with elements $G_{ij} = \\mathbf{v}_i \\cdot \\mathbf{v}_j$, where $\\mathbf{v}_1=\\mathbf{a}$, $\\mathbf{v}_2=\\mathbf{b}$, $\\mathbf{v}_3=\\mathbf{c}$.\n- Two bases $\\mathbf{L}$ and $\\mathbf{L}'$ generate the same lattice if and only if there exists an integer matrix $\\mathbf{S} \\in \\mathrm{GL}(3, \\mathbb{Z})$ (the General Linear group over the integers (GL), with determinant $\\det(\\mathbf{S}) = \\pm 1$) such that $\\mathbf{L}' = \\mathbf{L}\\mathbf{S}$. Under this change of basis, fractional coordinates transform contravariantly as $\\mathbf{f}' = \\mathbf{S}^{-1}\\mathbf{f}$.\n- The six-dimensional metric representation (G6) of a cell is the $6$-tuple $(A,B,C,D,E,F)$ defined by $A = \\mathbf{a}\\cdot\\mathbf{a}$, $B = \\mathbf{b}\\cdot\\mathbf{b}$, $C = \\mathbf{c}\\cdot\\mathbf{c}$, $D = 2\\,\\mathbf{b}\\cdot\\mathbf{c}$, $E = 2\\,\\mathbf{a}\\cdot\\mathbf{c}$, $F = 2\\,\\mathbf{a}\\cdot\\mathbf{b}$.\n- A canonical Niggli-reduced primitive cell may be characterized, for the purposes of this problem, as any basis reachable by an integer unimodular transform $\\mathbf{S}\\in\\mathrm{GL}(3, \\mathbb{Z})$ for which: (i) the edge-length squares are nondecreasing, i.e., $A \\le B \\le C$, (ii) the interaxial angles are non-acute so that $D \\le 0$, $E \\le 0$, $F \\le 0$ (angles $\\geq 90$ degrees), and (iii) among all such transforms, the $6$-tuple $(A,B,C,D,E,F)$ is lexicographically minimal. You must accomplish this using only integer unimodular transformations. You may assume that, for the supplied test suite, searching over integer unimodular transformations with entries in $\\{-1,0,1\\}$, along with column permutations and independent sign inversions of the basis vectors, suffices to reach such a canonical representative.\n\nDerivation target:\n- Starting from the above definitions, derive an algorithm that searches a finite subset of $\\mathrm{GL}(3,\\mathbb{Z})$, applies the induced change-of-basis to candidate lattices, and chooses the canonical reduced cell by enforcing the ordering $A \\le B \\le C$, enforcing non-acute interaxial angles (equivalently, $D \\le 0$, $E \\le 0$, $F \\le 0$), and selecting the lexicographically minimal $(A,B,C,D,E,F)$ among those satisfying these conditions. Then, transform all atomic fractional coordinates via $\\mathbf{f}'=\\mathbf{S}^{-1}\\mathbf{f}$ and reduce them componentwise into the half-open interval $[0,1)$ by applying the fractional part map modulo $1$.\n\nUnits and numerical conventions:\n- Lattice vectors are given in angstroms. Use angstroms consistently in all internal computations involving lengths. Interaxial angles must be interpreted and, if needed, computed in degrees. Your program’s final reported outputs are booleans and integers only, so no unit annotations are required in the output.\n\nValidation criteria to compute for each case:\n- Sorted-edge check: return a boolean indicating whether the reduced cell satisfies $A \\le B \\le C$ within a numerical tolerance.\n- Non-acute-angles check: return a boolean indicating whether all three interaxial angles are at least $90$ degrees (equivalently, $D \\le 0$, $E \\le 0$, $F \\le 0$ within tolerance).\n- Volume preservation check: return a boolean indicating whether $|\\det(\\mathbf{L})|$ equals $|\\det(\\mathbf{L}')|$ within tolerance.\n- Atom count: return the integer number of atomic positions after transformation (it must equal the input count when only unimodular transforms are used).\n- Unit-cube containment check: return a boolean indicating whether all transformed fractional coordinates lie in $[0,1)$ componentwise within tolerance.\n\nProgram requirements:\n- Implement the above algorithm using only definitions and facts stated here.\n- Use a fixed small numerical tolerance (for example, $10^{-8}$) for comparisons.\n- For each case, produce a result list of the form $[\\text{sorted\\_edge},\\ \\text{non\\_acute\\_angles},\\ \\text{volume\\_preserved},\\ \\text{atom\\_count},\\ \\text{coords\\_in\\_unit}]$.\n- The test suite below is to be hard-coded in your program. Your program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets (e.g., $\\texttt{[[...],[...],...]}$), with each case’s result list in order.\n\nTest suite (each lattice is given by its three column vectors; atomic positions are fractional):\n- Case $1$ (simple cubic, already reduced):\n  - $\\mathbf{L}_1$ has columns $(3,0,0)$, $(0,3,0)$, $(0,0,3)$, all in angstroms.\n  - Fractional positions: $(0,0,0)$ and $(0.5,0.5,0.5)$.\n- Case $2$ (orthorhombic cell, permuted with a sign flip):\n  - Start from $(3,0,0)$, $(0,4,0)$, $(0,0,5)$ and rearrange columns to $(0,4,0)$, $(0,0,-5)$, $(3,0,0)$ to form $\\mathbf{L}_2$ in angstroms.\n  - Fractional positions: $(0.25,0.4,0.6)$ and $(0.9,0.1,0.3)$.\n- Case $3$ (acute $\\gamma$ angle that must be made non-acute):\n  - $\\mathbf{L}_3$ has columns $(3,0,0)$, $(1.5,2.598076211,0)$, $(0,0,4)$ in angstroms.\n  - Fractional positions: $(0,0,0)$ and $(1.2,-0.1,0.5)$.\n- Case $4$ (equal edges $a=b$ with right angles; includes out-of-range fractional components to test wrapping):\n  - $\\mathbf{L}_4$ has columns $(3,0,0)$, $(0,3,0)$, $(0,0,4)$ in angstroms.\n  - Fractional positions: $(-0.25,1.75,0.0)$, $(0.5,0.5,0.5)$, and $(1.000000001, -1\\times 10^{-9}, 0.999999999)$.\n\nFinal output format:\n- Your program should produce exactly one line: a single outer list containing one inner list per case, in the same order as above. Each inner list must be of the form $[\\text{boolean},\\ \\text{boolean},\\ \\text{boolean},\\ \\text{integer},\\ \\text{boolean}]$, and the outer list must be printed as standard Python list syntax with commas separating elements and no extra text.",
            "solution": "The user-provided problem is assessed as **valid**. It is scientifically grounded in the principles of crystallography and linear algebra, well-posed with clear objectives and constraints, and formulated objectively. The problem requires the implementation of an algorithm to find a Niggli-reduced primitive cell for a given crystal lattice and to transform atomic coordinates into this new basis, adhering to a specific set of simplified reduction rules.\n\nThe algorithmic approach is designed based on the principles of basis transformation in a Bravais lattice and optimization under lexicographical ordering.\n\nA Bravais lattice is defined by a set of points $\\mathbf{r} = n_1\\mathbf{a} + n_2\\mathbf{b} + n_3\\mathbf{c}$, where $n_i \\in \\mathbb{Z}$ and $\\{\\mathbf{a}, \\mathbf{b}, \\mathbf{c}\\}$ are three linearly independent lattice vectors. These vectors form the columns of a basis matrix $\\mathbf{L} = [\\mathbf{a}\\ \\mathbf{b}\\ \\mathbf{c}] \\in \\mathbb{R}^{3 \\times 3}$. Any point in the lattice is given by $\\mathbf{r} = \\mathbf{L}\\mathbf{f}$, where $\\mathbf{f} \\in \\mathbb{Z}^3$. The geometry of the cell is fully described by the metric tensor $\\mathbf{G} = \\mathbf{L}^{\\mathsf{T}}\\mathbf{L}$, whose elements are the dot products of the basis vectors, $G_{ij} = \\mathbf{v}_i \\cdot \\mathbf{v}_j$.\n\nAn infinite number of basis matrices $\\mathbf{L}$ can generate the same lattice. Two bases, $\\mathbf{L}$ and $\\mathbf{L}'$, describe the same lattice if they are related by a unimodular integer transformation matrix $\\mathbf{S} \\in \\mathrm{GL}(3,\\mathbb{Z})$, such that $\\mathbf{L}' = \\mathbf{L}\\mathbf{S}$. The matrix $\\mathbf{S}$ must have integer entries and a determinant of $\\det(\\mathbf{S}) = \\pm 1$. The volume of the unit cell, given by $|\\det(\\mathbf{L})|$, is invariant under such transformations, as $|\\det(\\mathbf{L}')| = |\\det(\\mathbf{L}\\mathbf{S})| = |\\det(\\mathbf{L})||\\det(\\mathbf{S})| = |\\det(\\mathbf{L})|$. When the basis is changed via $\\mathbf{S}$, any fractional coordinate vector $\\mathbf{f}$ must transform contravariantly as $\\mathbf{f}' = \\mathbf{S}^{-1}\\mathbf{f}$ to ensure the real-space position $\\mathbf{r} = \\mathbf{L}\\mathbf{f} = \\mathbf{L}'\\mathbf{f}'$ remains invariant.\n\nThe problem requires finding a \"canonical\" Niggli-reduced cell according to a simplified set of rules. This is achieved by searching for an optimal transformation matrix $\\mathbf{S}$ that maps a given initial basis $\\mathbf{L}$ to a reduced basis $\\mathbf{L}'$ that satisfies specific criteria. The problem provides a crucial simplification: the search for $\\mathbf{S}$ can be restricted to the finite set of $3 \\times 3$ matrices whose entries are in $\\{-1, 0, 1\\}$ and whose determinant is $\\pm 1$.\n\nThe algorithm proceeds as follows:\nFirst, a static set $\\mathcal{S}$ of all candidate transformation matrices is generated. This is done by iterating through all $3^9 = 19,683$ possible $3 \\times 3$ matrices with entries in $\\{-1, 0, 1\\}$ and retaining only those for which the determinant is close to $\\pm 1$ within a small numerical tolerance. This set $\\mathcal{S}$ is pre-computed and used for all test cases.\n\nFor each test case, specified by an initial lattice $\\mathbf{L}_{\\text{initial}}$ and a set of initial fractional coordinates $\\{\\mathbf{f}_{\\text{initial}}\\}$:\n1.  Initialize a variable `best_g6` to hold the lexicographically smallest qualifying G6 vector found so far, along with the corresponding `best_L` and `best_S`.\n2.  Iterate through every transformation matrix $\\mathbf{S} \\in \\mathcal{S}$.\n3.  For each $\\mathbf{S}$, compute the candidate transformed basis $\\mathbf{L}_{\\text{cand}} = \\mathbf{L}_{\\text{initial}}\\mathbf{S}$.\n4.  From $\\mathbf{L}_{\\text{cand}}$, calculate the metric tensor $\\mathbf{G}_{\\text{cand}} = \\mathbf{L}_{\\text{cand}}^{\\mathsf{T}}\\mathbf{L}_{\\text{cand}}$ and the corresponding G6 vector $(A,B,C,D,E,F)$, where $A=\\mathbf{a}'\\cdot\\mathbf{a}'$, $B=\\mathbf{b}'\\cdot\\mathbf{b}'$, $C=\\mathbf{c}'\\cdot\\mathbf{c}'$, $D=2\\,\\mathbf{b}'\\cdot\\mathbf{c}'$, $E=2\\,\\mathbf{a}'\\cdot\\mathbf{c}'$, and $F=2\\,\\mathbf{a}'\\cdot\\mathbf{b}'$.\n5.  Apply the reduction criteria from the problem statement to the candidate G6 vector:\n    a.  Sorted edge-length squares: $A \\le B \\le C$.\n    b.  Non-acute interaxial angles: $D \\le 0, E \\le 0, F \\le 0$.\n    Numerical comparisons are performed using a tolerance $\\epsilon = 10^{-8}$.\n6.  If a candidate satisfies these conditions, its G6 vector is compared lexicographically to `best_g6`. If `best_g6` is not yet set or the candidate G6 is strictly smaller, `best_g6`, `best_L`, and `best_S` are updated with the current candidate's properties.\n7.  After searching through all $\\mathbf{S} \\in \\mathcal{S}$, the final `best_L` and `best_S` represent the Niggli-reduced cell and the transformation that produces it.\n\nOnce the optimal transformation $\\mathbf{S}_{\\text{final}}$ is found, the validation checks are performed:\n1.  **Sorted-edge check**: Verifies if the final cell's G6 parameters satisfy $A \\le B \\le C$ within tolerance.\n2.  **Non-acute-angles check**: Verifies if $D \\le 0, E \\le 0, F \\le 0$ within tolerance.\n3.  **Volume preservation check**: Confirms that $|\\det(\\mathbf{L}_{\\text{initial}})|$ is equal to $|\\det(\\mathbf{L}_{\\text{final}})|$ within tolerance.\n4.  **Atom count**: The number of atoms is simply the count of initial fractional coordinate vectors.\n5.  **Unit-cube containment check**: The initial fractional coordinates are transformed via $\\mathbf{f}_{\\text{final}} = \\mathbf{S}_{\\text{final}}^{-1}\\mathbf{f}_{\\text{initial}}$. Each component of $\\mathbf{f}_{\\text{final}}$ is then mapped to the interval $[0, 1)$ using the modulo operator, $f'' = f' \\pmod 1$. A boolean check confirms that all components of all resulting coordinate vectors lie within $[0, 1)$ with appropriate numerical tolerance, specifically checking if each wrapped coordinate $c$ satisfies $c \\ge -\\epsilon$ and $c  1.0$.\n\nThis systematic procedure guarantees finding the canonical cell as defined by the problem and correctly transforming all associated properties for validation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import product\n\n# Define a global tolerance for floating point comparisons.\nTOL = 1e-8\n\ndef get_s_matrices():\n    \"\"\"\n    Generates and returns the set of all 3x3 matrices with entries in {-1, 0, 1}\n    and determinant of +/-1. This set is pre-calculated once.\n    \"\"\"\n    s_matrices = []\n    for p in product([-1, 0, 1], repeat=9):\n        s = np.array(p, dtype=np.int32).reshape(3, 3)\n        det_s = np.linalg.det(s)\n        if np.isclose(det_s, 1.0) or np.isclose(det_s, -1.0):\n            s_matrices.append(s)\n    return s_matrices\n\n# Pre-compute the set of valid transformation matrices.\nS_MATRICES = get_s_matrices()\n\ndef get_g6_from_L(L):\n    \"\"\"\n    Calculates the 6-dimensional metric representation (G6) from a basis matrix L.\n    G6 = (a*a, b*b, c*c, 2*b*c, 2*a*c, 2*a*b)\n    \"\"\"\n    G = L.T @ L\n    A = G[0, 0]\n    B = G[1, 1]\n    C = G[2, 2]\n    D = 2 * G[1, 2]\n    E = 2 * G[0, 2]\n    F = 2 * G[0, 1]\n    return (A, B, C, D, E, F)\n\ndef solve_case(L_initial, f_initial):\n    \"\"\"\n    Finds the Niggli-reduced cell for a given lattice and performs validation checks.\n    \"\"\"\n    best_g6 = None\n    best_L = None\n    best_S = None\n\n    for s_matrix in S_MATRICES:\n        L_candidate = L_initial @ s_matrix\n        g6_candidate = get_g6_from_L(L_candidate)\n        A, B, C, D, E, F = g6_candidate\n\n        # Condition (i): Sorted edge-length squares (A = B = C)\n        if not (A = B + TOL and B = C + TOL):\n            continue\n        \n        # Condition (ii): Non-acute interaxial angles (D, E, F = 0)\n        if not (D = TOL and E = TOL and F = TOL):\n            continue\n\n        # Condition (iii): Lexicographically minimal G6 vector\n        if best_g6 is None or g6_candidate  best_g6:\n            best_g6 = g6_candidate\n            best_L = L_candidate\n            best_S = s_matrix\n    \n    # After checking all S, we have the best reduced cell\n    L_final = best_L\n    S_final = best_S\n    g6_final = best_g6\n\n    # --- Perform Validation Checks ---\n    \n    # 1. Sorted-edge check\n    A_f, B_f, C_f, _, _, _ = g6_final\n    sorted_edge = (A_f = B_f + TOL) and (B_f = C_f + TOL)\n\n    # 2. Non-acute-angles check\n    _, _, _, D_f, E_f, F_f = g6_final\n    non_acute_angles = (D_f = TOL) and (E_f = TOL) and (F_f = TOL)\n\n    # 3. Volume preservation check\n    vol_initial = np.abs(np.linalg.det(L_initial))\n    vol_final = np.abs(np.linalg.det(L_final))\n    volume_preserved = np.isclose(vol_initial, vol_final, atol=TOL, rtol=TOL)\n\n    # 4. Atom count\n    f_initial_arr = np.array(f_initial)\n    if f_initial_arr.ndim == 1:\n        atom_count = 1 if f_initial_arr.size  0 else 0\n    else:\n        atom_count = f_initial_arr.shape[0]\n\n    # 5. Unit-cube containment check\n    coords_in_unit = True # Default to True for case with no atoms\n    if atom_count  0:\n        S_inv = np.linalg.inv(S_final)\n        # Transform coords: S_inv is 3x3, f_initial_arr.T is 3xN_atoms\n        f_transformed = (S_inv @ f_initial_arr.T).T\n        # Reduce components to [0, 1) interval\n        f_wrapped = np.mod(f_transformed, 1.0)\n        # Check if all components are in [0, 1) within tolerance\n        # Check = -TOL for lower bound robustness, and  1.0 for upper bound\n        coords_in_unit = bool(np.all((f_wrapped >= -TOL)  (f_wrapped  1.0)))\n\n    return [sorted_edge, non_acute_angles, volume_preserved, atom_count, coords_in_unit]\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"L\": np.array([[3, 0, 0], [0, 3, 0], [0, 0, 3]], dtype=float).T,\n            \"f\": [[0, 0, 0], [0.5, 0.5, 0.5]]\n        },\n        {\n            \"L\": np.array([[0, 4, 0], [0, 0, -5], [3, 0, 0]], dtype=float).T,\n            \"f\": [[0.25, 0.4, 0.6], [0.9, 0.1, 0.3]]\n        },\n        {\n            \"L\": np.array([[3, 0, 0], [1.5, 2.598076211, 0], [0, 0, 4]], dtype=float).T,\n            \"f\": [[0, 0, 0], [1.2, -0.1, 0.5]]\n        },\n        {\n            \"L\": np.array([[3, 0, 0], [0, 3, 0], [0, 0, 4]], dtype=float).T,\n            \"f\": [[-0.25, 1.75, 0.0], [0.5, 0.5, 0.5], [1.000000001, -1e-9, 0.999999999]]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        L_initial = case[\"L\"]\n        f_initial = case[\"f\"]\n        result_list = solve_case(L_initial, f_initial)\n        results.append(result_list)\n\n    # Final print statement in the exact required format.\n    print(results)\n\nsolve()\n```"
        },
        {
            "introduction": "With cleaned and standardized data, the next step is often to build predictive models. However, materials datasets present unique challenges for model validation, particularly due to the existence of polymorphs—different crystal structures with the same chemical composition. This exercise  addresses the critical issue of information leakage by implementing grouped cross-validation, ensuring that your model's performance is evaluated realistically and robustly. You will train a regularized linear model and learn to properly estimate its prediction uncertainty, a key skill for any computational materials scientist.",
            "id": "3463892",
            "problem": "You are given a synthetic materials dataset designed to mimic a materials database containing multiple polymorphs per composition, where a polymorph is a distinct crystal structure of the same chemical composition. The goal is to implement grouped cross-validation so that all polymorphs of the same composition are kept in the same fold, train a regression model for formation energy, and report the Mean Absolute Error (MAE) with a $95\\%$ confidence interval. The task is framed to prevent information leakage by ensuring that polymorphs sharing composition-level descriptors are not split across training and test folds.\n\nDataset construction and physical realism:\n- There are $12$ distinct compositions (groups), indexed by integers $0,1,\\dots,11$.\n- Each composition has exactly $6$ polymorphs, so there are $72$ samples in total.\n- Each composition $g$ has a composition-level descriptor $\\mathbf{c}_g \\in \\mathbb{R}^3$ which is fixed across its polymorphs and generated from a uniform distribution in $[0,1]^3$ using a fixed random seed for reproducibility.\n- Each polymorph within composition $g$ has a structure-level descriptor $\\mathbf{s}_{g,j} \\in \\mathbb{R}^2$ generated independently from a uniform distribution in $[0,1]^2$ using the same fixed random seed for reproducibility.\n- The ground-truth formation energy per atom is generated by a linear model with an intercept and additive noise:\n$$\nE_f(g,j) = b + \\mathbf{c}_g \\cdot \\mathbf{w}_c + \\mathbf{s}_{g,j} \\cdot \\mathbf{w}_s + \\varepsilon_{g,j},\n$$\nwhere $b=-2.0$ (in $\\mathrm{eV/atom}$), $\\mathbf{w}_c = [0.8,-0.6,0.4]$ (in $\\mathrm{eV}$ per descriptor unit), $\\mathbf{w}_s = [0.5,-0.3]$ (in $\\mathrm{eV}$ per descriptor unit), and $\\varepsilon_{g,j} \\sim \\mathcal{N}(0,\\sigma^2)$ is additive Gaussian noise with standard deviation $\\sigma$ specified per test case. All energies are in electronvolts per atom ($\\mathrm{eV/atom}$).\n\nModeling and evaluation requirements:\n- Implement grouped $K$-fold Cross-Validation (CV), where the groups are the composition indices. All polymorphs belonging to the same composition must be assigned to the same fold.\n- For each fold, train a linear regression model for formation energy that minimizes the sum of squared residuals with an $\\ell_2$ (Euclidean) penalty of strength $\\lambda$ on the weights, but does not penalize the intercept. The learned model must predict formation energy on the held-out fold. Use standard linear algebra to obtain the solution; the intercept must be handled correctly and excluded from regularization.\n- Compute the per-fold Mean Absolute Error (MAE) as the average of $|E_f^{\\mathrm{pred}} - E_f^{\\mathrm{true}}|$ over all samples in the test fold. Aggregate the per-fold MAEs into an overall MAE by averaging across folds.\n- Compute a $95\\%$ confidence interval for the overall MAE by treating the per-fold MAEs as $K$ samples and using the Student’s $t$-distribution with $K-1$ degrees of freedom. Specifically, if $m_k$ are the per-fold MAEs for $k=1,\\dots,K$, then with sample mean $\\bar{m}$ and sample standard deviation $s$, report the interval $\\left[\\bar{m} - t^\\star \\frac{s}{\\sqrt{K}}, \\bar{m} + t^\\star \\frac{s}{\\sqrt{K}}\\right]$ where $t^\\star$ is the $0.975$ quantile of the Student’s $t$ distribution with $K-1$ degrees of freedom.\n- All error metrics must be reported in $\\mathrm{eV/atom}$. No angles are used; if any trigonometric functions appear in your implementation, their angles must be in radians.\n\nImplementation details and reproducibility:\n- Use a fixed seed $0$ for generating all composition and structure descriptors so that the features are identical across test cases.\n- Use distinct noise seeds for the additive Gaussian noise in each test case to guarantee reproducible targets while varying the noise level.\n- Use a fixed seed $99$ for shuffling and partitioning the groups into $K$ folds.\n\nTest suite:\nImplement your program to run the following three test cases, each specified by the tuple $(K,\\lambda,\\sigma,\\text{noise\\_seed})$:\n1. $K=3$, $\\lambda=0.1$, $\\sigma=0.05$, $\\text{noise\\_seed}=11$.\n2. $K=2$, $\\lambda=0.0$, $\\sigma=0.10$, $\\text{noise\\_seed}=22$.\n3. $K=4$, $\\lambda=1.0$, $\\sigma=0.20$, $\\text{noise\\_seed}=33$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all test cases in order, formatted as a comma-separated list enclosed in square brackets, where each test case contributes a three-element list $[\\bar{m}, \\mathrm{lower}, \\mathrm{upper}]$ with floats in $\\mathrm{eV/atom}$. For example, the output must look like\n$$\n[[\\bar{m}_1,\\mathrm{lower}_1,\\mathrm{upper}_1],[\\bar{m}_2,\\mathrm{lower}_2,\\mathrm{upper}_2],[\\bar{m}_3,\\mathrm{lower}_3,\\mathrm{upper}_3]]\n$$\nprinted on a single line, with no additional text.",
            "solution": "The core scientific rationale is to avoid information leakage by ensuring that polymorphs sharing the same composition-level descriptors are not split between training and test sets, because they would give the model indirect access to the held-out composition’s descriptor through other polymorphs. Grouped $K$-fold Cross-Validation (CV) directly enforces this by partitioning data on group labels (here, compositions).\n\nWe begin from empirical risk minimization. Let $\\mathbf{x}_{i} \\in \\mathbb{R}^{d}$ denote the descriptor vector for sample $i$ formed by concatenating the composition descriptor $\\mathbf{c}_g \\in \\mathbb{R}^{3}$ and the structure descriptor $\\mathbf{s}_{g,j} \\in \\mathbb{R}^{2}$, so $d=5$. Let $y_{i} \\in \\mathbb{R}$ be the formation energy in $\\mathrm{eV/atom}$. The ridge regression objective with intercept $b$ and weights $\\mathbf{w} \\in \\mathbb{R}^{d}$ is\n$$\nJ(b,\\mathbf{w}) = \\sum_{i=1}^{n} \\left(y_i - b - \\mathbf{x}_i^\\top \\mathbf{w}\\right)^2 + \\lambda \\lVert \\mathbf{w} \\rVert_2^2,\n$$\nwhere $\\lambda \\ge 0$ is the regularization strength and the intercept $b$ is not penalized. This is a convex quadratic problem whose minimizer satisfies the normal equations. Defining the augmented design matrix\n$$\n\\mathbf{X}_{\\mathrm{aug}} = \\begin{bmatrix}\n\\mathbf{1}  \\mathbf{X}\n\\end{bmatrix},\n$$\nwhere $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ stacks $\\mathbf{x}_i^\\top$ and $\\mathbf{1} \\in \\mathbb{R}^{n}$ is a column of ones, and the augmented parameter vector\n$$\n\\boldsymbol{\\beta} = \\begin{bmatrix}\nb \\\\ \\mathbf{w}\n\\end{bmatrix} \\in \\mathbb{R}^{d+1},\n$$\nthe first-order optimality condition yields\n$$\n\\left(\\mathbf{X}_{\\mathrm{aug}}^\\top \\mathbf{X}_{\\mathrm{aug}} + \\mathbf{L}\\right)\\boldsymbol{\\beta} = \\mathbf{X}_{\\mathrm{aug}}^\\top \\mathbf{y},\n$$\nwith $\\mathbf{L} \\in \\mathbb{R}^{(d+1)\\times(d+1)}$ a diagonal matrix that has $0$ in the intercept entry and $\\lambda$ on the remaining $d$ diagonal entries. Solving this linear system gives the unique minimizer when $\\lambda0$, and the ordinary least squares (OLS) solution when $\\lambda=0$ and $\\mathbf{X}_{\\mathrm{aug}}^\\top \\mathbf{X}_{\\mathrm{aug}}$ is invertible. Predictions on a test set are obtained by $\\hat{\\mathbf{y}} = \\mathbf{X}_{\\mathrm{aug,test}} \\boldsymbol{\\beta}$.\n\nFor grouped $K$-fold CV, let the dataset be indexed by $i=1,\\dots,n$ and let $g(i)$ be the group label (composition index) for sample $i$. We randomly permute the $G$ unique groups using a fixed seed and partition them into $K$ disjoint folds $\\mathcal{G}_1,\\dots,\\mathcal{G}_K$ of nearly equal size. For each fold $k$, the training set is all samples with group labels not in $\\mathcal{G}_k$, and the test set is all samples with group labels in $\\mathcal{G}_k$. This ensures that for any composition $g$, either all its polymorphs are in training or all are in testing for a given fold.\n\nThe Mean Absolute Error (MAE) on fold $k$ is defined as\n$$\nm_k = \\frac{1}{|\\mathcal{I}_k|} \\sum_{i \\in \\mathcal{I}_k} \\left| \\hat{y}_i - y_i \\right|,\n$$\nwhere $\\mathcal{I}_k$ is the index set of samples in the test fold $k$. The overall MAE is the average across folds,\n$$\n\\bar{m} = \\frac{1}{K} \\sum_{k=1}^{K} m_k.\n$$\nTo obtain a $95\\%$ confidence interval for $\\bar{m}$, we treat $\\{m_k\\}_{k=1}^K$ as $K$ samples and rely on the Student’s $t$-distribution. Compute the sample standard deviation\n$$\ns = \\sqrt{\\frac{1}{K-1}\\sum_{k=1}^{K} \\left(m_k - \\bar{m}\\right)^2},\n$$\nand let $t^\\star$ be the $0.975$ quantile of the Student’s $t$ distribution with $K-1$ degrees of freedom. Then the $95\\%$ confidence interval is\n$$\n\\left[\\bar{m} - t^\\star \\frac{s}{\\sqrt{K}}, \\bar{m} + t^\\star \\frac{s}{\\sqrt{K}}\\right].\n$$\nAll error values are in $\\mathrm{eV/atom}$.\n\nAlgorithmic steps consistent with the above principles:\n1. Generate composition descriptors $\\mathbf{c}_g$ for $g=0,\\dots,11$ from a uniform distribution on $[0,1]^3$ with seed $0$. Generate structure descriptors $\\mathbf{s}_{g,j}$ for $j=1,\\dots,6$ from a uniform distribution on $[0,1]^2$ with the same seed so features are fixed. Form $\\mathbf{x}_{g,j} = [\\mathbf{c}_g; \\mathbf{s}_{g,j}] \\in \\mathbb{R}^{5}$.\n2. For each test case, generate targets using $E_f(g,j) = b + \\mathbf{c}_g \\cdot \\mathbf{w}_c + \\mathbf{s}_{g,j} \\cdot \\mathbf{w}_s + \\varepsilon_{g,j}$ with the specified $\\sigma$ and noise seed, yielding $y_{g,j}$ in $\\mathrm{eV/atom}$.\n3. Split groups into $K$ folds using seed $99$. For each fold, fit ridge regression on the training set by solving $\\left(\\mathbf{X}_{\\mathrm{aug}}^\\top \\mathbf{X}_{\\mathrm{aug}} + \\mathbf{L}\\right)\\boldsymbol{\\beta} = \\mathbf{X}_{\\mathrm{aug}}^\\top \\mathbf{y}$, with $\\mathbf{L}=\\mathrm{diag}(0,\\lambda,\\dots,\\lambda)$ to exclude the intercept from regularization.\n4. Compute per-fold MAEs $m_k$, then compute $\\bar{m}$, $s$, and the $95\\%$ confidence interval using the Student’s $t$-distribution with $K-1$ degrees of freedom.\n5. Repeat for all test cases and print the list of triples $[\\bar{m},\\mathrm{lower},\\mathrm{upper}]$ for the three cases on a single line.\n\nThis design is grounded in empirical risk minimization with $\\ell_2$ regularization, proper handling of intercepts, statistical inference via the Student’s $t$ distribution, and group-aware validation to prevent information leakage across polymorphs sharing composition descriptors. The reporting of MAE and its confidence interval in $\\mathrm{eV/atom}$ aligns with physically meaningful units in computational materials science.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import t\n\ndef generate_features(num_groups=12, polymorphs_per_group=6, comp_dim=3, struct_dim=2, seed=0):\n    \"\"\"\n    Generate fixed composition and structure descriptors for reproducibility.\n    Returns:\n        X: (N, d) feature matrix where d = comp_dim + struct_dim\n        groups: (N,) array of group labels (composition indices)\n        comp_desc: (num_groups, comp_dim) array of composition descriptors\n        struct_desc: (N, struct_dim) array of structure descriptors\n    \"\"\"\n    rng = np.random.RandomState(seed)\n    comp_desc = rng.uniform(0.0, 1.0, size=(num_groups, comp_dim))\n    N = num_groups * polymorphs_per_group\n    struct_desc = rng.uniform(0.0, 1.0, size=(N, struct_dim))\n\n    # Build full features by concatenating composition and structure descriptors\n    d = comp_dim + struct_dim\n    X = np.zeros((N, d))\n    groups = np.zeros(N, dtype=int)\n\n    idx = 0\n    for g in range(num_groups):\n        c = comp_desc[g]\n        for j in range(polymorphs_per_group):\n            s = struct_desc[idx]\n            X[idx, :] = np.concatenate([c, s])\n            groups[idx] = g\n            idx += 1\n    return X, groups, comp_desc, struct_desc\n\ndef generate_targets(comp_desc, struct_desc, num_groups=12, polymorphs_per_group=6,\n                     w_c=np.array([0.8, -0.6, 0.4]),\n                     w_s=np.array([0.5, -0.3]),\n                     b=-2.0, sigma=0.05, noise_seed=11):\n    \"\"\"\n    Generate formation energy targets based on ground-truth linear model and Gaussian noise.\n    Returns:\n        y: (N,) target vector in eV/atom\n    \"\"\"\n    N = num_groups * polymorphs_per_group\n    y = np.zeros(N)\n    rng = np.random.RandomState(noise_seed)\n    idx = 0\n    for g in range(num_groups):\n        c = comp_desc[g]\n        for j in range(polymorphs_per_group):\n            s = struct_desc[idx]\n            mean_val = b + np.dot(c, w_c) + np.dot(s, w_s)\n            noise = rng.normal(loc=0.0, scale=sigma)\n            y[idx] = mean_val + noise\n            idx += 1\n    return y\n\ndef grouped_kfold(groups, K, seed=99):\n    \"\"\"\n    Create grouped K-fold splits: returns list of (train_indices, test_indices).\n    Ensures all samples of the same group are in the same fold.\n    \"\"\"\n    unique_groups = np.unique(groups)\n    rng = np.random.RandomState(seed)\n    shuffled = unique_groups.copy()\n    rng.shuffle(shuffled)\n    n_groups = len(shuffled)\n    # Partition groups into K folds as evenly as possible\n    fold_sizes = np.full(K, n_groups // K, dtype=int)\n    fold_sizes[: (n_groups % K)] += 1\n    folds = []\n    start = 0\n    for fs in fold_sizes:\n        folds.append(shuffled[start:start+fs])\n        start += fs\n\n    # Map groups to sample indices\n    group_to_indices = {g: np.where(groups == g)[0] for g in unique_groups}\n    splits = []\n    all_indices = np.arange(len(groups))\n    for test_groups in folds:\n        test_idx_list = []\n        for g in test_groups:\n            test_idx_list.append(group_to_indices[g])\n        test_indices = np.concatenate(test_idx_list)\n        train_mask = np.ones(len(groups), dtype=bool)\n        train_mask[test_indices] = False\n        train_indices = all_indices[train_mask]\n        splits.append((train_indices, test_indices))\n    return splits\n\ndef ridge_fit(X, y, lam):\n    \"\"\"\n    Fit ridge regression with intercept unpenalized.\n    Solve (X_aug^T X_aug + L) beta = X_aug^T y, where L has 0 for intercept and lam for weights.\n    Returns:\n        beta: (d+1,) parameter vector [intercept, weights...]\n    \"\"\"\n    n, d = X.shape\n    X_aug = np.hstack([np.ones((n, 1)), X])\n    # Regularization matrix L: 0 for intercept, lam for other coefficients\n    L = np.diag(np.concatenate(([0.0], np.full(d, lam))))\n    A = X_aug.T @ X_aug + L\n    b = X_aug.T @ y\n    beta = np.linalg.solve(A, b)\n    return beta\n\ndef evaluate_case(K, lam, sigma, noise_seed,\n                  num_groups=12, polymorphs_per_group=6,\n                  comp_dim=3, struct_dim=2):\n    \"\"\"\n    Evaluate grouped K-fold CV for one case.\n    Returns:\n        (mae_mean, ci_lower, ci_upper)\n    \"\"\"\n    # Fixed features across cases\n    X, groups, comp_desc, struct_desc = generate_features(\n        num_groups=num_groups,\n        polymorphs_per_group=polymorphs_per_group,\n        comp_dim=comp_dim,\n        struct_dim=struct_dim,\n        seed=0\n    )\n    # Targets for this case\n    y = generate_targets(\n        comp_desc=comp_desc,\n        struct_desc=struct_desc,\n        num_groups=num_groups,\n        polymorphs_per_group=polymorphs_per_group,\n        b=-2.0,\n        w_c=np.array([0.8, -0.6, 0.4]),\n        w_s=np.array([0.5, -0.3]),\n        sigma=sigma,\n        noise_seed=noise_seed\n    )\n\n    splits = grouped_kfold(groups, K=K, seed=99)\n    fold_maes = []\n    for train_idx, test_idx in splits:\n        X_train, y_train = X[train_idx], y[train_idx]\n        X_test, y_test = X[test_idx], y[test_idx]\n        beta = ridge_fit(X_train, y_train, lam=lam)\n        # Predict\n        X_test_aug = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n        y_pred = X_test_aug @ beta\n        mae = float(np.mean(np.abs(y_pred - y_test)))\n        fold_maes.append(mae)\n\n    # Aggregate MAE and compute 95% CI via Student's t\n    K_float = float(K)\n    mae_mean = float(np.mean(fold_maes))\n    if K  1:\n        s = float(np.std(fold_maes, ddof=1))\n        t_star = float(t.ppf(0.975, df=K - 1))\n        margin = t_star * s / np.sqrt(K_float)\n    else:\n        # Degenerate case; no CI with a single fold, margin set to 0\n        margin = 0.0\n\n    ci_lower = mae_mean - margin\n    ci_upper = mae_mean + margin\n    return [mae_mean, ci_lower, ci_upper]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (K, lambda, sigma, noise_seed)\n    test_cases = [\n        (3, 0.1, 0.05, 11),\n        (2, 0.0, 0.10, 22),\n        (4, 1.0, 0.20, 33),\n    ]\n\n    results = []\n    for case in test_cases:\n        K, lam, sigma, noise_seed = case\n        result = evaluate_case(K=K, lam=lam, sigma=sigma, noise_seed=noise_seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}