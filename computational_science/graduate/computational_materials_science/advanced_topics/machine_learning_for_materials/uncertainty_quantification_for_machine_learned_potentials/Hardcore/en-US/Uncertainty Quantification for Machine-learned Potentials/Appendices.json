{
    "hands_on_practices": [
        {
            "introduction": "Gaussian Processes (GPs) represent a cornerstone of nonparametric Bayesian modeling, making them an ideal starting point for rigorous uncertainty quantification. Unlike models that yield only point predictions, a GP provides a full predictive distribution for any new input, characterized by a predictive mean and a crucial predictive variance. This exercise will guide you through deriving and implementing the GP posterior variance from first principles and then using it to perform a fundamental diagnostic: assessing whether the predicted uncertainty intervals are well-calibrated by checking their empirical coverage of true data points .",
            "id": "3500259",
            "problem": "You are given a supervised learning setting for interatomic energies where a Gaussian Process (GP) regression model is used as a nonparametric Machine-Learned Potential (MLP) to emulate Density Functional Theory (DFT) energies. Let the latent energy function be denoted by $f(\\mathbf{R})$ for a configuration descriptor $\\mathbf{R}$, and let training observations be $y_i = f(\\mathbf{R}_i) + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_n^2)$ are independent Gaussian measurement errors. Assume a zero-mean Gaussian Process (GP) prior $f \\sim \\mathcal{GP}(0, k(\\cdot,\\cdot))$ with a squared-exponential kernel\n$$\nk(\\mathbf{R},\\mathbf{R}') = s^2 \\exp\\left(-\\tfrac{1}{2}\\left\\|\\tfrac{\\mathbf{R}-\\mathbf{R}'}{\\ell}\\right\\|^2\\right),\n$$\nwhere $s^2$ is the signal variance and $\\ell$ is the characteristic length scale. In this problem, you will (i) derive the GP posterior variance for the latent function at test inputs, and (ii) assess calibration by computing empirical coverage of DFT energies within two-standard-deviation Bayesian credible intervals.\n\nStart from the fundamental base: the joint Gaussianity of the prior over the latent training outputs and the latent test outputs, and the additive Gaussian noise model on the observations. Use multivariate normal conditioning and linearity of Gaussians, without appealing to any unproven shortcut formulas.\n\nYou must implement a complete, runnable program that:\n- Constructs the Gram matrix from the provided kernel and hyperparameters.\n- Computes the posterior mean $\\mu(\\mathbf{R}_*)$ and the posterior variance $\\sigma^2(\\mathbf{R}_*)$ of the latent function $f(\\mathbf{R}_*)$ at test descriptors $\\mathbf{R}_*$.\n- Treats Density Functional Theory (DFT) energies as noise-free realizations of the latent function for the purpose of coverage. That is, when defining the $2$-standard-deviation interval, use the latent predictive variance only (exclude observation noise).\n- Computes the empirical coverage rate as a decimal fraction for each configuration group (bulk, surface, defect), defined as the fraction of test points in that group whose DFT energy lies within $[\\mu(\\mathbf{R}_*) - 2\\sigma(\\mathbf{R}_*), \\mu(\\mathbf{R}_*) + 2\\sigma(\\mathbf{R}_*)]$.\n- Computes the overall coverage across all groups combined.\n- Prints a single line containing a comma-separated list enclosed in square brackets, in the order $[\\text{bulk\\_coverage}, \\text{surface\\_coverage}, \\text{defect\\_coverage}, \\text{overall\\_coverage}]$, where each entry is rounded to three decimal places, with no percent signs.\n\nAll energies must be treated and reported in electronvolts, abbreviated as $\\text{eV}$ where appropriate. Angles are not used. The coverage values must be expressed as decimal fractions rounded to three decimal places.\n\nUse the following kernel hyperparameters and dataset. The descriptor is one-dimensional and dimensionless (so write $\\mathbf{R}$ as $R$). The kernel hyperparameters are:\n- Signal variance $s^2 = 0.25$ (in $\\text{eV}^2$),\n- Length scale $\\ell = 0.6$ (dimensionless),\n- Observation noise standard deviation $\\sigma_n = 0.02$ (in $\\text{eV}$), hence $\\sigma_n^2 = 0.0004$ (in $\\text{eV}^2$).\n\nTraining data $(R_i, E_i)$ with $E_i$ in $\\text{eV}$:\n- $R_{\\text{train}} = [0.00, 0.25, 0.55, 0.90, 1.20, 1.50]$,\n- $E_{\\text{train}} = [1.00000, 0.90625, 0.81025, 0.72100, 0.66400, 0.62500]$.\n\nTest suite grouped by configuration class, with $R$ dimensionless and $E$ in $\\text{eV}$:\n- Bulk: $R_{\\text{bulk}} = [0.10, 0.60, 1.00]$, $E_{\\text{bulk}} = [0.96100, 0.79600, 0.70000]$.\n- Surface: $R_{\\text{surface}} = [1.10, 1.20, 1.60]$, $E_{\\text{surface}} = [0.68100, 0.66400, 0.61600]$.\n- Defect: $R_{\\text{defect}} = [1.80, 2.20, 2.60]$, $E_{\\text{defect}} = [0.60400, 0.60400, 0.63600]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[0.667,0.667,1.000,0.778]$). The entries must correspond to the bulk, surface, defect, and overall coverage fractions, in that order, each rounded to three decimal places. No other output is allowed.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It provides a complete and consistent set of data and parameters for a standard application of Gaussian Process (GP) regression in computational materials science. The tasks are clearly defined and computationally tractable. We will proceed with a solution.\n\nThe core of the problem is to derive the posterior distribution for a latent energy function $f(R)$ modeled by a Gaussian Process, given a set of noisy training observations, and then to use this posterior to assess the model's uncertainty calibration.\n\nLet the training data be a set of $N$ pairs $\\{ (R_i, y_i) \\}_{i=1}^N$, where $R_i$ are descriptor values and $y_i$ are the corresponding noisy energy observations. We will denote the collection of training inputs as $X = \\{R_1, \\dots, R_N\\}$ and the vector of training observations as $\\mathbf{y} = [y_1, \\dots, y_N]^T$. Similarly, let the test inputs be $M$ new descriptor values, denoted $X_* = \\{R_{*,1}, \\dots, R_{*,M}\\}$, at which we wish to predict the latent function values $\\mathbf{f}_* = [f(R_{*,1}), \\dots, f(R_{*,M})]^T$.\n\nThe model is defined by two main components: the GP prior and the likelihood.\n\n1.  **GP Prior**: The latent function $f$ is assumed to have a Gaussian Process prior, $f \\sim \\mathcal{GP}(m(R), k(R, R'))$. The problem specifies a zero-mean prior, so $m(R) = 0$. The kernel function is the squared-exponential:\n    $$\n    k(R, R') = s^2 \\exp\\left(-\\frac{(R-R')^2}{2\\ell^2}\\right)\n    $$\n    where $s^2$ is the signal variance and $\\ell$ is the characteristic length-scale. By the definition of a GP, any finite collection of function values is jointly Gaussian. Let $\\mathbf{f}$ be the vector of latent function values at the training inputs $X$. The joint prior distribution over the latent values at training inputs, $\\mathbf{f}$, and test inputs, $\\mathbf{f}_*$, is given by:\n    $$\n    \\begin{pmatrix} \\mathbf{f} \\\\ \\mathbf{f}_* \\end{pmatrix}\n    \\sim \\mathcal{N} \\left(\n        \\begin{pmatrix} \\mathbf{0} \\\\ \\mathbf{0} \\end{pmatrix},\n        \\begin{pmatrix} K(X, X) & K(X, X_*) \\\\ K(X_*, X) & K(X_*, X_*) \\end{pmatrix}\n    \\right)\n    $$\n    Here, $K(A, B)$ denotes the matrix of kernel evaluations between all points in sets $A$ and $B$. For brevity, we use the notation $K = K(X, X)$, $K_* = K(X, X_*)$, and $K_{**} = K(X_*, X_*)$. Note that $K(X_*, X) = K_*^T$.\n\n2.  **Likelihood**: The training observations $y_i$ are related to the latent function values $f(R_i)$ through an additive, independent, and identically distributed Gaussian noise model:\n    $$\n    y_i = f(R_i) + \\varepsilon_i, \\quad \\text{where} \\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)\n    $$\n    In vector form, this is $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\varepsilon}$, with $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 I)$, where $I$ is the $N \\times N$ identity matrix.\n\nFrom these two components, we derive the posterior distribution $p(\\mathbf{f}_* | X, \\mathbf{y}, X_*)$. The derivation proceeds by first finding the joint distribution of the observables $\\mathbf{y}$ and the quantities to be predicted $\\mathbf{f}_*$. Since $\\mathbf{f}$ and $\\boldsymbol{\\varepsilon}$ are independent Gaussian vectors, their sum $\\mathbf{y}$ is also Gaussian. The joint distribution of $(\\mathbf{y}, \\mathbf{f}_*)$ is thus a multivariate normal distribution. Its mean is:\n$$\nE\\left[\\begin{pmatrix} \\mathbf{y} \\\\ \\mathbf{f}_* \\end{pmatrix}\\right] = \\begin{pmatrix} E[\\mathbf{f} + \\boldsymbol{\\varepsilon}] \\\\ E[\\mathbf{f}_*] \\end{pmatrix} = \\begin{pmatrix} \\mathbf{0} \\\\ \\mathbf{0} \\end{pmatrix}\n$$\nThe covariance matrix is:\n$$\n\\text{Cov}\\left(\\begin{pmatrix} \\mathbf{y} \\\\ \\mathbf{f}_* \\end{pmatrix}\\right) =\n\\begin{pmatrix}\n\\text{Cov}(\\mathbf{y}) & \\text{Cov}(\\mathbf{y}, \\mathbf{f}_*) \\\\\n\\text{Cov}(\\mathbf{f}_*, \\mathbf{y}) & \\text{Cov}(\\mathbf{f}_*)\n\\end{pmatrix}\n$$\nThe blocks of this matrix are:\n- $\\text{Cov}(\\mathbf{y}) = \\text{Cov}(\\mathbf{f} + \\boldsymbol{\\varepsilon}) = \\text{Cov}(\\mathbf{f}) + \\text{Cov}(\\boldsymbol{\\varepsilon}) = K + \\sigma_n^2 I$.\n- $\\text{Cov}(\\mathbf{f}_*) = K_{**}$.\n- $\\text{Cov}(\\mathbf{y}, \\mathbf{f}_*) = \\text{Cov}(\\mathbf{f} + \\boldsymbol{\\varepsilon}, \\mathbf{f}_*) = \\text{Cov}(\\mathbf{f}, \\mathbf{f}_*) + \\text{Cov}(\\boldsymbol{\\varepsilon}, \\mathbf{f}_*) = K_* + \\mathbf{0} = K_*$.\n- $\\text{Cov}(\\mathbf{f}_*, \\mathbf{y}) = \\text{Cov}(\\mathbf{y}, \\mathbf{f}_*)^T = K_*^T$.\nSo, the joint distribution is:\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ \\mathbf{f}_* \\end{pmatrix}\n\\sim \\mathcal{N} \\left(\n    \\begin{pmatrix} \\mathbf{0} \\\\ \\mathbf{0} \\end{pmatrix},\n    \\begin{pmatrix} K + \\sigma_n^2 I & K_* \\\\ K_*^T & K_{**} \\end{pmatrix}\n\\right)\n$$\nTo find the posterior distribution of $\\mathbf{f}_*$ given the observations $\\mathbf{y}$, we use the standard rules for conditioning multivariate Gaussians. For a joint Gaussian distribution $\\begin{pmatrix} \\mathbf{a} \\\\ \\mathbf{b} \\end{pmatrix} \\sim \\mathcal{N} \\left( \\begin{pmatrix} \\boldsymbol{\\mu}_a \\\\ \\boldsymbol{\\mu}_b \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{aa} & \\Sigma_{ab} \\\\ \\Sigma_{ba} & \\Sigma_{bb} \\end{pmatrix} \\right)$, the conditional distribution $p(\\mathbf{b}|\\mathbf{a})$ is a Gaussian with mean $\\boldsymbol{\\mu}_{b|a} = \\boldsymbol{\\mu}_b + \\Sigma_{ba} \\Sigma_{aa}^{-1} (\\mathbf{a} - \\boldsymbol{\\mu}_a)$ and covariance $\\Sigma_{b|a} = \\Sigma_{bb} - \\Sigma_{ba} \\Sigma_{aa}^{-1} \\Sigma_{ab}$.\n\nApplying this to our case (with $\\mathbf{a}=\\mathbf{y}$ and $\\mathbf{b}=\\mathbf{f}_*$, and zero means), the posterior distribution $p(\\mathbf{f}_* | X, \\mathbf{y}, X_*)$ is a Gaussian $\\mathcal{N}(\\boldsymbol{\\mu}_*, \\Sigma_*)$ with:\n- Posterior Mean: $\\boldsymbol{\\mu}_* = K_*^T (K + \\sigma_n^2 I)^{-1} \\mathbf{y}$\n- Posterior Covariance: $\\Sigma_* = K_{**} - K_*^T (K + \\sigma_n^2 I)^{-1} K_*$\n\nFor a single test point $R_*$, these expressions simplify. $K_*$ becomes a vector $\\mathbf{k}_* = [k(R_1, R_*), \\dots, k(R_N, R_*)]^T$ and $K_{**}$ becomes the scalar $k_{**} = k(R_*, R_*)$. The posterior distribution for the latent value $f(R_*)$ is $\\mathcal{N}(\\mu(R_*), \\sigma^2(R_*))$, where:\n- Posterior Mean: $\\mu(R_*) = \\mathbf{k}_*^T (K + \\sigma_n^2 I)^{-1} \\mathbf{y}$\n- Posterior Variance (of the latent function): $\\sigma^2(R_*) = k_{**} - \\mathbf{k}_*^T (K + \\sigma_n^2 I)^{-1} \\mathbf{k}_*$\n\nThis posterior variance $\\sigma^2(R_*)$ quantifies the uncertainty in the model's estimate of the *latent function value* $f(R_*)$, excluding observation noise. The problem requires a credible interval based on this latent variance. A $2$-standard-deviation credible interval for $f(R_*)$ is given by $[\\mu(R_*) - 2\\sigma(R_*), \\mu(R_*) + 2\\sigma(R_*)]$.\n\nThe empirical coverage is then computed for each group of test configurations. For a given group with $M$ test points, we count how many of the true DFT energies $E_{*,j}$ fall within their corresponding credible interval:\n$$\n\\text{Coverage} = \\frac{1}{M} \\sum_{j=1}^M \\mathbb{I} \\left( E_{*,j} \\in [\\mu(R_{*,j}) - 2\\sigma(R_{*,j}), \\mu(R_{*,j}) + 2\\sigma(R_{*,j})] \\right)\n$$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. The implementation will compute this for the 'bulk', 'surface', and 'defect' groups, as well as for all test points combined. For numerical stability, the matrix inversion $(K + \\sigma_n^2 I)^{-1}$ is performed by solving a linear system, preferably via Cholesky decomposition.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, cho_solve\n\ndef solve():\n    \"\"\"\n    Computes the empirical coverage of a Gaussian Process regression model\n    for emulating Density Functional Theory (DFT) energies.\n    \"\"\"\n    \n    # 1. Define kernel hyperparameters and dataset as per the problem statement.\n    # All energies are in eV. Descriptor R is dimensionless.\n    s2 = 0.25         # Signal variance, in eV^2\n    ell = 0.6         # Characteristic length scale, dimensionless\n    sigma_n2 = 0.0004 # Observation noise variance, in eV^2\n    \n    # Training data\n    R_train = np.array([0.00, 0.25, 0.55, 0.90, 1.20, 1.50]).reshape(-1, 1)\n    E_train = np.array([1.00000, 0.90625, 0.81025, 0.72100, 0.66400, 0.62500])\n\n    # Test suite grouped by configuration class\n    test_suite = {\n        \"bulk\": (np.array([0.10, 0.60, 1.00]), np.array([0.96100, 0.79600, 0.70000])),\n        \"surface\": (np.array([1.10, 1.20, 1.60]), np.array([0.68100, 0.66400, 0.61600])),\n        \"defect\": (np.array([1.80, 2.20, 2.60]), np.array([0.60400, 0.60400, 0.63600]))\n    }\n\n    # 2. Define the squared-exponential kernel function.\n    def squared_exp_kernel(X1, X2, s2_param, l2_param):\n        \"\"\"\n        Computes the squared-exponential kernel matrix between two sets of 1D inputs.\n        X1: (N, 1) array\n        X2: (M, 1) array\n        Returns: (N, M) kernel matrix\n        \"\"\"\n        # Broadcasting (X1 - X2.T) creates an (N, M) matrix of pairwise differences.\n        sq_dist_matrix = (X1 - X2.T)**2\n        return s2_param * np.exp(-0.5 * sq_dist_matrix / l2_param)\n\n    # 3. Perform one-time pre-computation for the GP model.\n    l2 = ell**2\n    K_train = squared_exp_kernel(R_train, R_train, s2, l2)\n    Ky = K_train + sigma_n2 * np.eye(len(R_train))\n\n    # For numerical stability, use Cholesky decomposition to solve linear systems\n    # involving the inverse of Ky. L is the lower-triangular Cholesky factor.\n    try:\n        L = cholesky(Ky, lower=True)\n    except np.linalg.LinAlgError:\n        # This case should not be reached with the given valid parameters.\n        # Fallback for robustness.\n        print(\"Error: The covariance matrix Ky is not positive definite.\")\n        return\n\n    # Pre-compute alpha = Ky^-1 * y\n    alpha = cho_solve((L, True), E_train)\n\n    # 4. Calculate coverage for each test group and overall.\n    coverages = {}\n    all_R_test = []\n    all_E_test = []\n\n    def calculate_coverage(R_test, E_test):\n        \"\"\"Helper function to compute coverage for a given test set.\"\"\"\n        if len(R_test) == 0:\n            return 0.0\n\n        M = len(R_test)\n        R_test_reshaped = R_test.reshape(-1, 1)\n        \n        # Vectorized prediction for all test points\n        k_star_matrix = squared_exp_kernel(R_train, R_test_reshaped, s2, l2)\n        \n        # Posterior mean\n        mu_star_vector = k_star_matrix.T @ alpha\n        \n        # Posterior variance (of the latent function)\n        # We need the diagonal of K_** - K_*^T Ky^-1 K_*.\n        # diag(K_**): is just s2 for all test points.\n        # diag(K_*^T Ky^-1 K_*): can be computed efficiently.\n        # Let v = Ky^-1 K_*. Then the diagonal is sum(K_* * v, axis=0).\n        v = cho_solve((L, True), k_star_matrix)\n        var_star_vector = s2 - np.sum(k_star_matrix * v, axis=0)\n        \n        # Correct for potential minor numerical precision issues\n        var_star_vector[var_star_vector < 0] = 0\n        sigma_star_vector = np.sqrt(var_star_vector)\n\n        # Define 2-sigma credible intervals\n        lower_bounds = mu_star_vector - 2 * sigma_star_vector\n        upper_bounds = mu_star_vector + 2 * sigma_star_vector\n        \n        # Check which true energies fall within the intervals\n        covered_mask = (E_test >= lower_bounds) & (E_test <= upper_bounds)\n        \n        return np.mean(covered_mask)\n\n    for group_name, (R_group, E_group) in test_suite.items():\n        coverages[group_name] = calculate_coverage(R_group, E_group)\n        all_R_test.append(R_group)\n        all_E_test.append(E_group)\n        \n    # Overall coverage calculation\n    R_overall = np.concatenate(all_R_test)\n    E_overall = np.concatenate(all_E_test)\n    coverages['overall'] = calculate_coverage(R_overall, E_overall)\n    \n    # 5. Format and print the final result.\n    output_values = [\n        round(coverages['bulk'], 3),\n        round(coverages['surface'], 3),\n        round(coverages['defect'], 3),\n        round(coverages['overall'], 3),\n    ]\n\n    # Print in the required format: [val1,val2,val3,val4]\n    print(f\"[{','.join(f'{x:.3f}' for x in output_values)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A comprehensive uncertainty analysis goes beyond simply calculating a variance; it requires understanding the sources of that uncertainty. This practice addresses the critical task of decomposing total prediction error into its aleatoric (data-related) and epistemic (model-related) components. By leveraging a clever experimental design with multiple data-labeling protocols, you will derive and implement a statistical test to quantify these distinct error sources, a skill essential for guiding efficient data acquisition in active learning and for assessing the true limits of model knowledge .",
            "id": "3500186",
            "problem": "You are given two labeling protocols from Density Functional Theory (DFT), applied to the same held-out configurations for a fixed Machine-Learned Potential (MLP) architecture. For each configuration index $i \\in \\{1,\\dots,n\\}$, the two labeling protocols produce scalar labels $y_{1,i}$ and $y_{2,i}$ for the same atomic configuration, and the MLP predicts a scalar $\\mu_i$. Define residuals $r_{1,i} = y_{1,i} - \\mu_i$ and $r_{2,i} = y_{2,i} - \\mu_i$. Assume an additive random-effects model in which the residuals decompose as $r_{k,i} = e_i + \\varepsilon_{k,i}$ for $k \\in \\{1,2\\}$, where $e_i$ is the epistemic error component associated with the MLP prediction on configuration $i$, and $\\varepsilon_{k,i}$ is the aleatoric noise component associated with the $k$-th DFT labeling protocol on configuration $i$. Assume that $e_i$ and $\\varepsilon_{k,i}$ are zero-mean across the held-out set, that $\\varepsilon_{1,i}$ and $\\varepsilon_{2,i}$ are conditionally independent given $i$, and that $e_i$ is independent of each $\\varepsilon_{k,i}$.\n\nYour task is to design and implement a one-sided hypothesis test to assess whether aleatoric noise dominates epistemic error in the held-out set. Formulate the hypothesis in terms of a parameter that cleanly compares the epistemic variance and the average aleatoric variance across the two labeling protocols, using only the definitions of variance, covariance, and independence. Derive a test statistic from first principles and a decision rule at significance level $\\alpha = 0.05$, relying only on fundamental statistical definitions and well-tested procedures. You may assume the use of a nonparametric paired bootstrap to quantify sampling uncertainty.\n\nThe program you write must implement the following procedure for each test case:\n- From the provided parameters, generate the held-out residual pairs $\\{(r_{1,i}, r_{2,i})\\}_{i=1}^n$ using the specified random seeds and distributions.\n- Using the generated residuals, construct an estimator of the epistemic variance and estimators of the aleatoric variances of the two labeling protocols based solely on the definitions of variance and covariance.\n- Construct a one-sided hypothesis test at significance $\\alpha = 0.05$ with the null hypothesis chosen so that “aleatoric noise dominates epistemic error” corresponds to the alternative hypothesis. Use a paired nonparametric bootstrap with $B$ resamples to assess uncertainty in the test statistic and implement a decision rule that returns a boolean for each test case indicating whether the data provide sufficient evidence, at level $\\alpha$, that aleatoric noise dominates epistemic error.\n- Use exactly $B = 2000$ bootstrap resamples in all cases.\n\nYou must consider the following test suite, where all random number generation must use the specified seeds to ensure determinism. Each test case specifies $n$ and the generative model for $(e_i, \\varepsilon_{1,i}, \\varepsilon_{2,i})$, and the program must generate $r_{k,i} = e_i + \\varepsilon_{k,i}$ accordingly.\n\n- Case A (happy path, clear aleatoric dominance): $n = 800$. Draw $e_i$ independently from a normal distribution with variance $\\sigma_{\\mathrm{e}}^2 = 0.05$, draw $\\varepsilon_{1,i}$ independently from a normal distribution with variance $\\sigma_{\\mathrm{a1}}^2 = 0.20$, draw $\\varepsilon_{2,i}$ independently from a normal distribution with variance $\\sigma_{\\mathrm{a2}}^2 = 0.25$. Use seed $s = 10$.\n- Case B (epistemic dominance): $n = 800$. Draw $e_i$ independently from a normal distribution with variance $\\sigma_{\\mathrm{e}}^2 = 0.50$, draw $\\varepsilon_{1,i}$ independently from a normal distribution with variance $\\sigma_{\\mathrm{a1}}^2 = 0.05$, draw $\\varepsilon_{2,i}$ independently from a normal distribution with variance $\\sigma_{\\mathrm{a2}}^2 = 0.05$. Use seed $s = 20$.\n- Case C (boundary-like, comparable magnitudes): $n = 300$. Draw $e_i$ independently from a normal distribution with variance $\\sigma_{\\mathrm{e}}^2 = 0.10$, draw $\\varepsilon_{1,i}$ independently from a normal distribution with variance $\\sigma_{\\mathrm{a1}}^2 = 0.10$, draw $\\varepsilon_{2,i}$ independently from a normal distribution with variance $\\sigma_{\\mathrm{a2}}^2 = 0.10$. Use seed $s = 30$.\n- Case D (heteroscedastic aleatoric noise, still aleatoric dominant): $n = 1000$. Draw $e_i$ independently from a normal distribution with variance $\\sigma_{\\mathrm{e}}^2 = 0.02$. For $i \\in \\{1,\\dots,n\\}$, draw $\\varepsilon_{1,i}$ independently from a normal distribution with variance $\\sigma_{\\mathrm{a1},i}^2 = 0.20 \\times \\left(1 + 0.5 \\sin\\left(2\\pi i / n\\right)\\right)$ and draw $\\varepsilon_{2,i}$ independently from a normal distribution with variance $\\sigma_{\\mathrm{a2},i}^2 = 0.18 \\times \\left(1 - 0.5 \\cos\\left(2\\pi i / n\\right)\\right)$. Use seed $s = 40$.\n- Case E (zero aleatoric noise): $n = 500$. Draw $e_i$ independently from a normal distribution with variance $\\sigma_{\\mathrm{e}}^2 = 0.10$. Set $\\varepsilon_{1,i} = 0$ and $\\varepsilon_{2,i} = 0$ deterministically for all $i$. Use seed $s = 50$.\n- Case F (heavy-tailed aleatoric noise with small epistemic variance): $n = 800$. Draw $e_i$ independently from a normal distribution with variance $\\sigma_{\\mathrm{e}}^2 = 0.01$. Draw $\\varepsilon_{1,i}$ and $\\varepsilon_{2,i}$ independently from Student’s $t$ distributions with $\\nu = 3$ degrees of freedom, scaled to achieve variances $\\sigma_{\\mathrm{a1}}^2 = 0.25$ and $\\sigma_{\\mathrm{a2}}^2 = 0.25$ respectively. Use seed $s = 60$.\n\nFor all cases, interpret “aleatoric noise dominates epistemic error” precisely as the average aleatoric variance across the two labeling protocols being greater than the epistemic variance on the held-out set, and design your one-sided hypothesis test accordingly at significance $\\alpha = 0.05$.\n\nYour program should produce a single line of output containing the results for the six cases as a comma-separated list of booleans enclosed in square brackets (for example, `[True,False,True,True,False,True]`). The $k$-th boolean must be `True` if and only if, at significance level $\\alpha = 0.05$, your test concludes that aleatoric noise dominates epistemic error for case $k$, and `False` otherwise. No other output is allowed.",
            "solution": "The problem requires the design and implementation of a one-sided hypothesis test to determine if aleatoric noise dominates epistemic error for a machine-learned potential, based on residuals from two different labeling protocols. The validation of the problem statement confirms that it is scientifically grounded, well-posed, and contains all necessary information to proceed with a solution.\n\nThe foundation of our analysis is the additive random-effects model provided for the residuals $r_{k,i}$ for labeling protocol $k \\in \\{1, 2\\}$ and configuration $i \\in \\{1, \\dots, n\\}$:\n$$r_{k,i} = e_i + \\varepsilon_{k,i}$$\nHere, $e_i$ is the epistemic error of the machine-learned potential for configuration $i$, and $\\varepsilon_{k,i}$ is the aleatoric noise from the $k$-th labeling protocol. The problem states several key assumptions about these error components:\n1.  They are zero-mean over the population: $E[e_i] = 0$ and $E[\\varepsilon_{k,i}] = 0$ for $k=1,2$.\n2.  The epistemic error $e_i$ is independent of both aleatoric noise components, $\\varepsilon_{1,i}$ and $\\varepsilon_{2,i}$. This implies $Cov(e_i, \\varepsilon_{k,i}) = 0$.\n3.  The two aleatoric noise components, $\\varepsilon_{1,i}$ and $\\varepsilon_{2,i}$, are conditionally independent given the configuration $i$. This implies $Cov(\\varepsilon_{1,i}, \\varepsilon_{2,i}) = 0$.\n\nLet $\\sigma_e^2 = \\text{Var}(e_i)$, $\\sigma_{a1}^2 = \\text{Var}(\\varepsilon_{1,i})$, and $\\sigma_{a2}^2 = \\text{Var}(\\varepsilon_{2,i})$ denote the population variances of the epistemic and aleatoric errors. Note that for cases with heteroscedastic noise (like Case D), $\\sigma_{ak}^2$ represents the average variance across all configurations. Using the independence assumptions, we can derive expressions for the variance of each residual series and their covariance.\n\nThe variance of the first residual series is:\n$$\\text{Var}(r_1) = \\text{Var}(e + \\varepsilon_1) = \\text{Var}(e) + \\text{Var}(\\varepsilon_1) + 2\\text{Cov}(e, \\varepsilon_1) = \\sigma_e^2 + \\sigma_{a1}^2$$\nSimilarly, the variance of the second residual series is:\n$$\\text{Var}(r_2) = \\text{Var}(e + \\varepsilon_2) = \\text{Var}(e) + \\text{Var}(\\varepsilon_2) + 2\\text{Cov}(e, \\varepsilon_2) = \\sigma_e^2 + \\sigma_{a2}^2$$\nThe covariance between the two residual series is:\n$$\\text{Cov}(r_1, r_2) = \\text{Cov}(e + \\varepsilon_1, e + \\varepsilon_2) = \\text{Cov}(e,e) + \\text{Cov}(e,\\varepsilon_2) + \\text{Cov}(\\varepsilon_1,e) + \\text{Cov}(\\varepsilon_1,\\varepsilon_2)$$\nGiven the independence assumptions, all cross-terms are zero, and $\\text{Cov}(e,e) = \\text{Var}(e)$. Therefore:\n$$\\text{Cov}(r_1, r_2) = \\sigma_e^2$$\nThis provides a direct way to identify the epistemic variance from the covariance of the observable residuals. We can then solve for the aleatoric variances:\n$$\\sigma_{a1}^2 = \\text{Var}(r_1) - \\sigma_e^2 = \\text{Var}(r_1) - \\text{Cov}(r_1, r_2)$$\n$$\\sigma_{a2}^2 = \\text{Var}(r_2) - \\sigma_e^2 = \\text{Var}(r_2) - \\text{Cov}(r_1, r_2)$$\nThe problem defines \"aleatoric noise dominates epistemic error\" as the average aleatoric variance being greater than the epistemic variance. Let $\\sigma_A^2 = (\\sigma_{a1}^2 + \\sigma_{a2}^2)/2$. The condition is $\\sigma_A^2 > \\sigma_e^2$.\n\nWe formulate a one-sided hypothesis test. The alternative hypothesis, $H_1$, corresponds to the claim of interest:\n$$H_1: \\sigma_A^2 > \\sigma_e^2$$\nThe null hypothesis, $H_0$, is the complement:\n$$H_0: \\sigma_A^2 \\le \\sigma_e^2$$\nTo construct a test, we define a parameter $\\theta = \\sigma_A^2 - \\sigma_e^2$. The hypotheses are then $H_0: \\theta \\le 0$ versus $H_1: \\theta > 0$. We express $\\theta$ in terms of the observable moments of the residuals:\n$$\\theta = \\frac{(\\text{Var}(r_1) - \\text{Cov}(r_1, r_2)) + (\\text{Var}(r_2) - \\text{Cov}(r_1, r_2))}{2} - \\text{Cov}(r_1, r_2)$$\n$$\\theta = \\frac{\\text{Var}(r_1) + \\text{Var}(r_2)}{2} - 2\\text{Cov}(r_1, r_2)$$\nOur test statistic, $\\hat{\\theta}$, is the method-of-moments estimator of $\\theta$ using sample statistics calculated from the data pairs $\\{(r_{1,i}, r_{2,i})\\}_{i=1}^n$. Let $\\widehat{\\text{Var}}$ and $\\widehat{\\text{Cov}}$ denote the standard unbiased sample variance and covariance estimators (with a denominator of $n-1$):\n$$\\hat{\\theta} = \\frac{\\widehat{\\text{Var}}(r_1) + \\widehat{\\text{Var}}(r_2)}{2} - 2\\widehat{\\text{Cov}}(r_1, r_2)$$\nA large positive value of $\\hat{\\theta}$ provides evidence for $H_1$. We need to determine if the observed value, $\\hat{\\theta}_{obs}$, is statistically significant. We use a nonparametric paired bootstrap to assess the sampling uncertainty of $\\hat{\\theta}$, with $B=2000$ resamples. The hypothesis test will be conducted at a significance level of $\\alpha = 0.05$.\n\nThe test for $H_0: \\theta \\le 0$ versus $H_1: \\theta > 0$ can be performed by inverting a one-sided confidence interval for $\\theta$. We will construct a $(1-\\alpha)$ lower confidence bound for $\\theta$. If this lower bound is greater than $0$, we reject $H_0$. Using a basic (pivotal) bootstrap method, the sampling distribution of $\\hat{\\theta} - \\theta$ is approximated by the bootstrap distribution of $\\hat{\\theta}^* - \\hat{\\theta}_{obs}$, where $\\hat{\\theta}^*$ is the statistic computed on a bootstrap sample. This leads to a $(1-\\alpha)$ one-sided confidence interval for $\\theta$ of $[2\\hat{\\theta}_{obs} - q_{1-\\alpha}, \\infty)$, where $q_{1-\\alpha}$ is the $(1-\\alpha)$-quantile of the bootstrap distribution $\\{\\hat{\\theta}^*_b\\}_{b=1}^B$.\n\nWe reject $H_0$ if the lower bound is greater than $0$:\n$$2\\hat{\\theta}_{obs} - q_{1-\\alpha} > 0 \\implies 2\\hat{\\theta}_{obs} > q_{1-\\alpha}$$\nFor $\\alpha=0.05$, the decision rule is to reject $H_0$ if $2\\hat{\\theta}_{obs}$ is greater than the $95$-th percentile of the bootstrap distribution of $\\hat{\\theta}^*$. If we reject $H_0$, we conclude that the data provide sufficient evidence that aleatoric noise dominates epistemic error.\n\nThe implementation will proceed as follows for each test case:\n1.  Generate the $n$ data triplets $(e_i, \\varepsilon_{1,i}, \\varepsilon_{2,i})$ according to the specified distributions and random seed.\n2.  Compute the observable residuals $r_{1,i} = e_i + \\varepsilon_{1,i}$ and $r_{2,i} = e_i + \\varepsilon_{2,i}$.\n3.  Calculate the observed test statistic $\\hat{\\theta}_{obs}$ from the pairs $(r_{1,i}, r_{2,i})$.\n4.  Generate $B = 2000$ paired bootstrap samples. For each bootstrap sample, compute the statistic $\\hat{\\theta}^*_b$.\n5.  Determine the $95$-th percentile, $q_{0.95}$, of the empirical distribution of $\\{\\hat{\\theta}^*_b\\}$.\n6.  Apply the decision rule: if $2 \\times \\hat{\\theta}_{obs} > q_{0.95}$, the result is True; otherwise, it is False.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to run hypothesis tests for all specified cases.\n    \"\"\"\n    test_cases = [\n        {\n            'case_id': 'A', 'n': 800, 'seed': 10, 'model': 'normal',\n            'params': {'var_e': 0.05, 'var_a1': 0.20, 'var_a2': 0.25}\n        },\n        {\n            'case_id': 'B', 'n': 800, 'seed': 20, 'model': 'normal',\n            'params': {'var_e': 0.50, 'var_a1': 0.05, 'var_a2': 0.05}\n        },\n        {\n            'case_id': 'C', 'n': 300, 'seed': 30, 'model': 'normal',\n            'params': {'var_e': 0.10, 'var_a1': 0.10, 'var_a2': 0.10}\n        },\n        {\n            'case_id': 'D', 'n': 1000, 'seed': 40, 'model': 'heteroscedastic',\n            'params': {'var_e': 0.02, 'var_a1_func': lambda i, n: 0.20 * (1 + 0.5 * np.sin(2 * np.pi * (i + 1) / n)),\n                       'var_a2_func': lambda i, n: 0.18 * (1 - 0.5 * np.cos(2 * np.pi * (i + 1) / n))}\n        },\n        {\n            'case_id': 'E', 'n': 500, 'seed': 50, 'model': 'zero_aleatoric',\n            'params': {'var_e': 0.10}\n        },\n        {\n            'case_id': 'F', 'n': 800, 'seed': 60, 'model': 'student_t',\n            'params': {'var_e': 0.01, 'var_a1': 0.25, 'var_a2': 0.25, 'df': 3}\n        },\n    ]\n\n    results = []\n    B = 2000  # Number of bootstrap resamples\n\n    for case in test_cases:\n        rng = np.random.default_rng(case['seed'])\n        n = case['n']\n        params = case['params']\n\n        # Generate error components based on the case model\n        if case['model'] == 'normal':\n            e = rng.normal(loc=0, scale=np.sqrt(params['var_e']), size=n)\n            eps1 = rng.normal(loc=0, scale=np.sqrt(params['var_a1']), size=n)\n            eps2 = rng.normal(loc=0, scale=np.sqrt(params['var_a2']), size=n)\n        elif case['model'] == 'heteroscedastic':\n            e = rng.normal(loc=0, scale=np.sqrt(params['var_e']), size=n)\n            indices = np.arange(n)\n            var_a1_i = params['var_a1_func'](indices, n)\n            var_a2_i = params['var_a2_func'](indices, n)\n            eps1 = rng.normal(loc=0, scale=np.sqrt(var_a1_i))\n            eps2 = rng.normal(loc=0, scale=np.sqrt(var_a2_i))\n        elif case['model'] == 'zero_aleatoric':\n            e = rng.normal(loc=0, scale=np.sqrt(params['var_e']), size=n)\n            eps1 = np.zeros(n)\n            eps2 = np.zeros(n)\n        elif case['model'] == 'student_t':\n            e = rng.normal(loc=0, scale=np.sqrt(params['var_e']), size=n)\n            df = params['df']\n            # Scale standard t-distribution to achieve target variance\n            # Var(c * T_df) = c^2 * df / (df - 2)\n            # c = sqrt(target_var * (df - 2) / df)\n            scale1 = np.sqrt(params['var_a1'] * (df - 2) / df)\n            scale2 = np.sqrt(params['var_a2'] * (df - 2) / df)\n            eps1 = scale1 * rng.standard_t(df, size=n)\n            eps2 = scale2 * rng.standard_t(df, size=n)\n\n        # Compute residuals\n        r1 = e + eps1\n        r2 = e + eps2\n\n        # Perform the hypothesis test\n        decision = perform_hypothesis_test(r1, r2, rng, B)\n        results.append(decision)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef get_theta_hat(r1, r2):\n    \"\"\"\n    Calculates the test statistic theta_hat from a sample of residuals.\n    This function is designed to handle 2D arrays of resamples.\n    \"\"\"\n    n = r1.shape[-1]\n    \n    # Calculate means for each resample\n    mean1 = np.mean(r1, axis=-1, keepdims=True)\n    mean2 = np.mean(r2, axis=-1, keepdims=True)\n\n    # Calculate variances for each resample\n    var1 = np.sum((r1 - mean1)**2, axis=-1) / (n - 1)\n    var2 = np.sum((r2 - mean2)**2, axis=-1) / (n - 1)\n    \n    # Calculate covariances for each resample\n    cov12 = np.sum((r1 - mean1) * (r2 - mean2), axis=-1) / (n - 1)\n    \n    return (var1 + var2) / 2.0 - 2.0 * cov12\n\ndef perform_hypothesis_test(r1, r2, rng, B, alpha=0.05):\n    \"\"\"\n    Performs the one-sided hypothesis test using a paired nonparametric bootstrap.\n    \"\"\"\n    n = len(r1)\n\n    # Calculate observed test statistic\n    theta_obs = get_theta_hat(r1, r2)\n\n    # Generate bootstrap indices\n    bootstrap_indices = rng.integers(0, n, size=(B, n))\n\n    # Create bootstrap resamples\n    r1_resamples = r1[bootstrap_indices]\n    r2_resamples = r2[bootstrap_indices]\n\n    # Calculate test statistic for all bootstrap resamples (vectorized)\n    bootstrap_thetas = get_theta_hat(r1_resamples, r2_resamples)\n\n    # Find the (1-alpha) quantile of the bootstrap distribution\n    q_1_minus_alpha = np.percentile(bootstrap_thetas, 100 * (1 - alpha))\n\n    # Apply the decision rule\n    return 2 * theta_obs > q_1_minus_alpha\n\nsolve()\n```"
        },
        {
            "introduction": "Machine-learned potentials intended for physical simulations must not only be accurate but also obey the fundamental laws of physics. This final practice bridges the gap between statistical modeling and physical consistency by examining the predicted force covariance matrix, $\\boldsymbol{\\Sigma}$. You will learn how to translate physical principles, such as conservation of momentum and Newton's Third Law, into concrete algebraic constraints on $\\boldsymbol{\\Sigma}$, and implement a numerical validation test . Mastering this validation is vital for building trust in machine-learned potentials and ensuring they produce stable and physically meaningful dynamics in molecular simulations.",
            "id": "3500194",
            "problem": "You are given a probabilistic machine-learned interatomic potential that, for a configuration of $N$ atoms, outputs a Gaussian predictive distribution for the concatenated force vector $\\mathbf{f} \\in \\mathbb{R}^{3N}$ with mean $\\boldsymbol{\\mu} \\in \\mathbb{R}^{3N}$ and covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{3N \\times 3N}$. The force components are ordered per atom and Cartesian axis as $(F_{1x},F_{1y},F_{1z},F_{2x},\\dots,F_{Nz})$, and all forces are expressed in electronvolt per Angstrom, so each covariance entry is in $(\\text{eV}/\\text{\\AA})^2$. A physically consistent potential derived from a scalar energy must respect Newton’s Third Law of motion and total momentum conservation. Specifically, for any deterministic configuration, the net force on the system must be zero for each Cartesian axis, and for two atoms interacting in isolation, the forces must be equal and opposite. In a Gaussian predictive model, these physical symmetries translate into linear-algebraic constraints on the covariance matrix.\n\nStarting from the fundamental definitions of covariance and the statements of Newton’s Third Law and momentum conservation, derive a numerical test that verifies whether the predicted force covariance matrix $\\boldsymbol{\\Sigma}$ respects these constraints within a given tolerance $\\varepsilon$. Your test must check the following conditions:\n\n- Symmetry consistency: $\\|\\boldsymbol{\\Sigma} - \\boldsymbol{\\Sigma}^\\top\\|_{\\infty} \\le \\varepsilon$.\n- Positive semidefiniteness within tolerance: the minimum eigenvalue $\\lambda_{\\min}(\\boldsymbol{\\Sigma}) \\ge -\\varepsilon$.\n- Momentum-conservation-induced linear constraints: for each atom index $i \\in \\{1,\\dots,N\\}$ and Cartesian axes $\\alpha,\\beta \\in \\{x,y,z\\}$, the covariance must satisfy $\\left|\\sum_{j=1}^{N} \\Sigma_{(i\\alpha),(j\\beta)}\\right| \\le \\varepsilon$.\n- Pairwise action-reaction structure for two-atom systems ($N=2$): for all $\\alpha,\\beta \\in \\{x,y,z\\}$, the $3 \\times 3$ covariance blocks must be consistent with $\\mathbf{f}_2 = -\\mathbf{f}_1$, which implies $|\\Sigma_{(1\\alpha),(2\\beta)} + \\Sigma_{(1\\alpha),(1\\beta)}| \\le \\varepsilon$, $|\\Sigma_{(2\\alpha),(1\\beta)} + \\Sigma_{(1\\alpha),(1\\beta)}| \\le \\varepsilon$, and $|\\Sigma_{(2\\alpha),(2\\beta)} - \\Sigma_{(1\\alpha),(1\\beta)}| \\le \\varepsilon$.\n\nBased on these derived checks, implement a program that takes a fixed suite of test covariance matrices and tolerances and returns a boolean for each case indicating whether all checks pass.\n\nUse the following test suite. For clarity, the component-to-index mapping is defined as $m(i,\\alpha) = 3(i-1) + \\gamma(\\alpha)$ where $\\gamma(x)=1$, $\\gamma(y)=2$, $\\gamma(z)=3$ when using one-based indexing in the formulas; in code, you may use zero-based indexing with $\\gamma(x)=0$, $\\gamma(y)=1$, $\\gamma(z)=2$.\n\n- Case 1 (two atoms, physically consistent): $N=2$, tolerance $\\varepsilon = 10^{-8}$, and\n  $$\n  \\mathbf{C} = \\begin{bmatrix}\n  0.40 & 0.05 & -0.02 \\\\\n  0.05 & 0.30 & 0.01 \\\\\n  -0.02 & 0.01 & 0.50\n  \\end{bmatrix}, \\quad\n  \\boldsymbol{\\Sigma} = \\begin{bmatrix}\n  \\mathbf{C} & -\\mathbf{C} \\\\\n  -\\mathbf{C} & \\mathbf{C}\n  \\end{bmatrix}.\n  $$\n- Case 2 (two atoms, violated action-reaction coupling): $N=2$, tolerance $\\varepsilon = 10^{-3}$. Start from Case 1 and add a symmetric perturbation $\\delta = 2\\times 10^{-2}$ to the $(F_{1x},F_{2x})$ cross-covariance entries so that $\\Sigma_{(1x),(2x)} \\leftarrow \\Sigma_{(1x),(2x)} + \\delta$ and $\\Sigma_{(2x),(1x)} \\leftarrow \\Sigma_{(2x),(1x)} + \\delta$.\n- Case 3 (three atoms, projected to satisfy momentum conservation): $N=3$, tolerance $\\varepsilon = 10^{-8}$. Construct $\\boldsymbol{\\Sigma}$ as follows. Let $\\mathbf{v}_x,\\mathbf{v}_y,\\mathbf{v}_z \\in \\mathbb{R}^{9}$ be the three vectors with ones at indices corresponding to $x$, $y$, and $z$ components across all atoms and zeros elsewhere. Form $\\mathbf{V} = [\\mathbf{v}_x, \\mathbf{v}_y, \\mathbf{v}_z] \\in \\mathbb{R}^{9 \\times 3}$ and the projector $\\mathbf{P} = \\mathbf{I}_{9} - \\mathbf{V}(\\mathbf{V}^\\top \\mathbf{V})^{-1}\\mathbf{V}^\\top$. With a fixed random seed of $123$, draw a dense matrix $\\mathbf{M} \\in \\mathbb{R}^{9 \\times 9}$ with independent standard normal entries and set $\\mathbf{G} = \\mathbf{M}\\mathbf{M}^\\top$. Define $\\boldsymbol{\\Sigma} = \\mathbf{P}\\mathbf{G}\\mathbf{P}^\\top + 10^{-6}\\,\\mathbf{P}$.\n- Case 4 (three atoms, violated momentum conservation along $x$): $N=3$, tolerance $\\varepsilon = 10^{-6}$. Start from Case 3 and set $\\boldsymbol{\\Sigma} \\leftarrow \\boldsymbol{\\Sigma} + a\\,\\mathbf{v}_x \\mathbf{v}_x^\\top$ with $a = 10^{-3}$.\n- Case 5 (two atoms, boundary tolerance): $N=2$, tolerance $\\varepsilon = 10^{-5}$. Start from Case 1 and add a symmetric perturbation $\\delta = 10^{-5}$ to the $(F_{1y},F_{2y})$ cross-covariance entries so that $\\Sigma_{(1y),(2y)} \\leftarrow \\Sigma_{(1y),(2y)} + \\delta$ and $\\Sigma_{(2y),(1y)} \\leftarrow \\Sigma_{(2y),(1y)} + \\delta$.\n\nYour program should compute, for each case, a boolean indicating whether all the above checks pass within the specified tolerance. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"). No other text should be printed. Angles are not involved. All physical quantities should be treated in $(\\text{eV}/\\text{\\AA})$ and $(\\text{eV}/\\text{\\AA})^2$ where applicable; however, the outputs are booleans and thus unitless.",
            "solution": "The validation of a predicted force covariance matrix, $\\boldsymbol{\\Sigma}$, for physical consistency is a critical step in uncertainty quantification for machine-learned interatomic potentials. A physically meaningful potential must adhere to fundamental symmetries of nature, which translate into precise algebraic constraints on the moments of its predictive distribution. The problem requires the derivation and implementation of a numerical test to verify four such constraints: symmetry, positive semidefiniteness, momentum conservation, and pairwise action-reaction for two-atom systems.\n\nWe shall first derive the rationale for each numerical test from its corresponding physical principle. The force vector is denoted $\\mathbf{f} \\in \\mathbb{R}^{3N}$, where $N$ is the number of atoms. An entry of the covariance matrix $\\boldsymbol{\\Sigma}$ is $\\Sigma_{(i\\alpha),(j\\beta)} = \\text{Cov}(F_{i\\alpha}, F_{j\\beta})$, where $F_{i\\alpha}$ is the force component on atom $i \\in \\{1, \\dots, N\\}$ along the Cartesian axis $\\alpha \\in \\{x,y,z\\}$.\n\n**1. Symmetry of the Covariance Matrix**\nBy definition, the covariance operator is symmetric: $\\text{Cov}(X, Y) = \\text{Cov}(Y, X)$ for any two random variables $X$ and $Y$. Consequently, the force covariance matrix must be symmetric, i.e., $\\Sigma_{(i\\alpha),(j\\beta)} = \\Sigma_{(j\\beta),(i\\alpha)}$. In matrix notation, this is $\\boldsymbol{\\Sigma} = \\boldsymbol{\\Sigma}^\\top$. Due to finite precision in numerical computations, we verify this property within a tolerance $\\varepsilon > 0$. The infinity norm, $\\|\\mathbf{A}\\|_{\\infty} = \\max_{i} \\sum_{j} |A_{ij}|$, is used to measure the deviation from perfect symmetry.\nThe numerical test is:\n$$ \\|\\boldsymbol{\\Sigma} - \\boldsymbol{\\Sigma}^\\top\\|_{\\infty} \\le \\varepsilon $$\n\n**2. Positive Semidefiniteness**\nA covariance matrix must be positive semidefinite. This is a direct consequence of the fact that the variance of any linear combination of the random variables must be non-negative. For an arbitrary vector of coefficients $\\mathbf{a} \\in \\mathbb{R}^{3N}$, the variance of the scalar random variable $Z = \\mathbf{a}^\\top \\mathbf{f}$ is given by $\\text{Var}(Z) = \\mathbf{a}^\\top \\boldsymbol{\\Sigma} \\mathbf{a}$. Since variance cannot be negative, we must have $\\mathbf{a}^\\top \\boldsymbol{\\Sigma} \\mathbf{a} \\ge 0$ for all $\\mathbf{a}$. This is the definition of a positive semidefinite matrix. A key property of a real symmetric positive semidefinite matrix is that all its eigenvalues, $\\lambda_k$, are non-negative.\nNumerically, small floating-point errors can lead to small negative eigenvalues. Therefore, the test is relaxed to check if the minimum eigenvalue, $\\lambda_{\\min}$, is not \"too negative\":\n$$ \\lambda_{\\min}(\\boldsymbol{\\Sigma}) \\ge -\\varepsilon $$\n\n**3. Momentum Conservation**\nThe total potential energy of an isolated system of atoms is invariant under a uniform translation of all atomic positions. This symmetry, by Noether's theorem, implies conservation of total linear momentum. For a potential that depends only on positions, this means the net force on the entire system must be zero. For each Cartesian axis $\\alpha \\in \\{x,y,z\\}$, this translates to the punctual constraint:\n$$ \\sum_{i=1}^{N} F_{i\\alpha} = 0 $$\nIn a probabilistic model, this physical law must hold for every single realization (sample) of the forces. If the scalar random variable $S_\\beta = \\sum_{j=1}^{N} F_{j\\beta}$ is identically zero, its mean and variance must be zero. Furthermore, its covariance with any other random variable must also be zero. We thus require the covariance of any individual force component, $F_{i\\alpha}$, with the total force component, $S_\\beta$, to be zero for all $i, \\alpha, \\beta$:\n$$ \\text{Cov}(F_{i\\alpha}, S_\\beta) = \\text{Cov}\\left(F_{i\\alpha}, \\sum_{j=1}^{N} F_{j\\beta}\\right) = 0 $$\nUsing the linearity of the covariance operator:\n$$ \\sum_{j=1}^{N} \\text{Cov}(F_{i\\alpha}, F_{j\\beta}) = \\sum_{j=1}^{N} \\Sigma_{(i\\alpha),(j\\beta)} = 0 $$\nThis constraint must hold for all $i \\in \\{1,\\dots,N\\}$ and all $\\alpha, \\beta \\in \\{x,y,z\\}$. The corresponding numerical test allows for a small tolerance $\\varepsilon$:\n$$ \\left|\\sum_{j=1}^{N} \\Sigma_{(i\\alpha),(j\\beta)}\\right| \\le \\varepsilon $$\n\n**4. Pairwise Action-Reaction for Two-Atom Systems**\nFor an isolated system of only two atoms ($N=2$), Newton's Third Law dictates that the force atom $1$ exerts on atom $2$ is equal and opposite to the force atom $2$ exerts on atom $1$. The total force on each atom, $\\mathbf{f}_1$ and $\\mathbf{f}_2$, is thus related by $\\mathbf{f}_2 = -\\mathbf{f}_1$. This is a stronger condition than momentum conservation ($\\mathbf{f}_1 + \\mathbf{f}_2 = \\mathbf{0}$).\nThis relation must hold for each component, $F_{2\\alpha} = -F_{1\\alpha}$, for every sample from the predictive distribution. This imposes strict relations on the $3 \\times 3$ covariance blocks of the full $6 \\times 6$ matrix $\\boldsymbol{\\Sigma}$. Let $\\mathbf{C}_{ij}$ be the block such that $(\\mathbf{C}_{ij})_{\\alpha\\beta} = \\Sigma_{(i\\alpha),(j\\beta)}$.\nUsing the properties of covariance:\n-   $\\Sigma_{(1\\alpha),(2\\beta)} = \\text{Cov}(F_{1\\alpha}, F_{2\\beta}) = \\text{Cov}(F_{1\\alpha}, -F_{1\\beta}) = -\\text{Cov}(F_{1\\alpha}, F_{1\\beta}) = -\\Sigma_{(1\\alpha),(1\\beta)}$.\n    This implies $\\mathbf{C}_{12} = -\\mathbf{C}_{11}$. The numerical test is $|\\Sigma_{(1\\alpha),(2\\beta)} + \\Sigma_{(1\\alpha),(1\\beta)}| \\le \\varepsilon$.\n-   $\\Sigma_{(2\\alpha),(1\\beta)} = \\text{Cov}(F_{2\\alpha}, F_{1\\beta}) = \\text{Cov}(-F_{1\\alpha}, F_{1\\beta}) = -\\text{Cov}(F_{1\\alpha}, F_{1\\beta}) = -\\Sigma_{(1\\alpha),(1\\beta)}$.\n    This implies $\\mathbf{C}_{21} = -\\mathbf{C}_{11}$. The numerical test is $|\\Sigma_{(2\\alpha),(1\\beta)} + \\Sigma_{(1\\alpha),(1\\beta)}| \\le \\varepsilon$.\n-   $\\Sigma_{(2\\alpha),(2\\beta)} = \\text{Cov}(F_{2\\alpha}, F_{2\\beta}) = \\text{Cov}(-F_{1\\alpha}, -F_{1\\beta}) = (-1)(-1)\\text{Cov}(F_{1\\alpha}, F_{1\\beta}) = \\Sigma_{(1\\alpha),(1\\beta)}$.\n    This implies $\\mathbf{C}_{22} = \\mathbf{C}_{11}$. The numerical test is $|\\Sigma_{(2\\alpha),(2\\beta)} - \\Sigma_{(1\\alpha),(1\\beta)}| \\le \\varepsilon$.\n\nThe algorithmic design consists of implementing a single function that sequentially performs these four checks. If any check fails, the function returns `False`; if all checks pass, it returns `True`. This function is then applied to each test case provided.",
            "answer": "```python\nimport numpy as np\n\ndef validate_covariance(N, Sigma, epsilon):\n    \"\"\"\n    Validates a force covariance matrix against physical constraints.\n\n    Args:\n        N (int): The number of atoms.\n        Sigma (np.ndarray): The 3N x 3N covariance matrix.\n        epsilon (float): The numerical tolerance for all checks.\n\n    Returns:\n        bool: True if all checks pass, False otherwise.\n    \"\"\"\n    dim = 3 * N\n    if Sigma.shape != (dim, dim):\n        raise ValueError(\"Sigma dimensions are inconsistent with N.\")\n\n    # 1. Symmetry Check\n    sym_diff = Sigma - Sigma.T\n    norm_inf_sym = np.linalg.norm(sym_diff, ord=np.inf)\n    if not (norm_inf_sym <= epsilon):\n        return False\n\n    # 2. Positive Semidefiniteness Check\n    # Use eigh for symmetric matrices; it is more stable and guarantees real eigenvalues.\n    try:\n        eigvals = np.linalg.eigh(Sigma)[0]\n        min_eig = np.min(eigvals)\n        if not (min_eig >= -epsilon):\n            return False\n    except np.linalg.LinAlgError:\n         # Matrix is not Hermitian or did not converge\n        return False\n\n\n    # 3. Momentum Conservation Check\n    # For each beta, sum of Sigma_i,j over j where j is a beta component must be zero.\n    V = np.zeros((dim, 3))\n    for axis in range(3):\n        V[axis::3, axis] = 1.0\n    \n    # S contains the sums for each row, for each total force component v_x, v_y, v_z\n    S = Sigma @ V\n    max_sum_violation = np.max(np.abs(S))\n    if not (max_sum_violation <= epsilon):\n        return False\n\n    # 4. Pairwise Action-Reaction Check (for N=2 only)\n    if N == 2:\n        C11 = Sigma[0:3, 0:3]\n        C12 = Sigma[0:3, 3:6]\n        C21 = Sigma[3:6, 0:3]\n        C22 = Sigma[3:6, 3:6]\n\n        # Check 1: C12 = -C11\n        if not (np.max(np.abs(C12 + C11)) <= epsilon):\n            return False\n        \n        # Check 2: C21 = -C11\n        if not (np.max(np.abs(C21 + C11)) <= epsilon):\n            return False\n            \n        # Check 3: C22 = C11\n        if not (np.max(np.abs(C22 - C11)) <= epsilon):\n            return False\n\n    return True\n\ndef solve():\n    \"\"\"\n    Main function to construct and validate the test suite of covariance matrices.\n    \"\"\"\n    test_cases = []\n\n    # Case 1: Two atoms, physically consistent\n    C1 = np.array([\n        [0.40, 0.05, -0.02],\n        [0.05, 0.30, 0.01],\n        [-0.02, 0.01, 0.50]\n    ])\n    Sigma1 = np.block([[C1, -C1], [-C1, C1]])\n    test_cases.append({'N': 2, 'Sigma': Sigma1, 'epsilon': 1e-8})\n\n    # Case 2: Two atoms, violated action-reaction coupling\n    Sigma2 = np.copy(Sigma1)\n    delta2 = 2e-2\n    # Indices for (F_1x, F_2x) are 0 and 3\n    Sigma2[0, 3] += delta2\n    Sigma2[3, 0] += delta2\n    test_cases.append({'N': 2, 'Sigma': Sigma2, 'epsilon': 1e-3})\n\n    # Case 3: Three atoms, projected to satisfy momentum conservation\n    N3 = 3\n    dim3 = 3 * N3\n    V3 = np.zeros((dim3, 3))\n    V3[0::3, 0] = 1.  # v_x\n    V3[1::3, 1] = 1.  # v_y\n    V3[2::3, 2] = 1.  # v_z\n    P3 = np.eye(dim3) - V3 @ np.linalg.inv(V3.T @ V3) @ V3.T\n    rng = np.random.default_rng(123)\n    M3 = rng.standard_normal((dim3, dim3))\n    G3 = M3 @ M3.T\n    # P is symmetric, so P.T = P\n    Sigma3 = P3 @ G3 @ P3 + 1e-6 * P3\n    test_cases.append({'N': N3, 'Sigma': Sigma3, 'epsilon': 1e-8})\n\n    # Case 4: Three atoms, violated momentum conservation along x\n    Sigma4 = np.copy(Sigma3)\n    a4 = 1e-3\n    v_x4 = V3[:, 0]\n    Sigma4 += a4 * np.outer(v_x4, v_x4)\n    test_cases.append({'N': N3, 'Sigma': Sigma4, 'epsilon': 1e-6})\n\n    # Case 5: Two atoms, boundary tolerance\n    Sigma5 = np.copy(Sigma1)\n    delta5 = 1e-5\n    # Indices for (F_1y, F_2y) are 1 and 4\n    Sigma5[1, 4] += delta5\n    Sigma5[4, 1] += delta5\n    test_cases.append({'N': 2, 'Sigma': Sigma5, 'epsilon': 1e-5})\n\n    results = []\n    for case in test_cases:\n        result = validate_covariance(case['N'], case['Sigma'], case['epsilon'])\n        results.append(str(result))\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        }
    ]
}