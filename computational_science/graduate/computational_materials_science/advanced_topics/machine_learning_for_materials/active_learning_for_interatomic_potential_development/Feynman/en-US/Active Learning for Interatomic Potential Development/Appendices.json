{
    "hands_on_practices": [
        {
            "introduction": "In practical active learning scenarios, we often need to select not just one, but an entire batch of configurations to label within a fixed computational budget. This transforms the selection process into a resource allocation challenge. This exercise frames this task as a classic 0-1 Knapsack problem, providing a rigorous framework for maximizing the scientific value (utility) gained from a set of expensive DFT calculations, making it an essential skill for efficient potential development .",
            "id": "3431900",
            "problem": "You are developing an interatomic potential using active learning in computational materials science. In each active learning round, you must select a subset of candidate atomic configurations (hereafter called frames) to label with first-principles calculations using Density Functional Theory (DFT). Labeling each frame consumes computational resources and is bounded by a fixed budget. Assume you are given a set of $n$ frames, indexed by $i \\in \\{0,1,\\dots,n-1\\}$. Each frame $i$ has an expected error reduction utility $u_i$ (dimensionless, representing the expected decrease in model generalization error if the frame is labeled and incorporated into training) and a labeling cost $c_i$ measured in core-hour units. You also have a total budget $B$ in core-hour units. Your objective is to choose a subset of frames that maximizes the total expected error reduction while respecting the DFT budget constraint.\n\nFormally, define binary decision variables $x_i \\in \\{0,1\\}$ indicating whether frame $i$ is selected. The selection problem is:\n$$\n\\text{maximize } \\sum_{i=0}^{n-1} u_i x_i \\quad \\text{subject to} \\quad \\sum_{i=0}^{n-1} c_i x_i \\le B, \\quad x_i \\in \\{0,1\\}.\n$$\nAssume $c_i$ and $B$ are nonnegative integers measured in core-hour units and $u_i$ are nonnegative real numbers. Note that a zero cost $c_i = 0$ is allowed and models cases where a frame is already cheaply labeled or available without additional cost.\n\nTie-breaking rules must be deterministic and follow this order:\n- Among all subsets that achieve the maximal total utility under the budget, choose the one with the smallest total cost (in core-hour units).\n- If there is still a tie, choose the lexicographically smallest index set when indices are listed in ascending order (i.e., prefer the subset that would compare smaller under standard lexicographic comparison of sorted index lists).\n\nYour program must implement an algorithm that returns the selected indices for each test case according to the specification above. Indices must be reported with zero-based indexing. All returned selections must be sorted in ascending order.\n\nTest Suite:\nProvide results for the following test cases. In each case, report the list of selected indices.\n\n- Test case $1$ (happy path, moderate budget):\n  - Utilities: $\\mathbf{u} = [0.8, 1.0, 1.2, 0.3]$.\n  - Costs in core-hour units: $\\mathbf{c} = [4, 5, 6, 2]$.\n  - Budget in core-hour units: $B = 10$.\n\n- Test case $2$ (boundary condition, zero budget):\n  - Utilities: $\\mathbf{u} = [1.1, 0.9]$.\n  - Costs in core-hour units: $\\mathbf{c} = [3, 4]$.\n  - Budget in core-hour units: $B = 0$.\n\n- Test case $3$ (all items fit within budget):\n  - Utilities: $\\mathbf{u} = [0.5, 0.7, 0.9]$.\n  - Costs in core-hour units: $\\mathbf{c} = [1, 2, 3]$.\n  - Budget in core-hour units: $B = 6$.\n\n- Test case $4$ (tie case requiring lexicographic tie-breaking):\n  - Utilities: $\\mathbf{u} = [1.0, 1.0, 0.1]$.\n  - Costs in core-hour units: $\\mathbf{c} = [3, 3, 1]$.\n  - Budget in core-hour units: $B = 4$.\n\n- Test case $5$ (zero-cost item present):\n  - Utilities: $\\mathbf{u} = [0.6, 0.4, 0.9]$.\n  - Costs in core-hour units: $\\mathbf{c} = [0, 2, 2]$.\n  - Budget in core-hour units: $B = 2$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each entry is the sorted list of selected indices for the corresponding test case. For example, an output with three test cases might look like $[\\,[0,2],[\\,],[1,3]\\ ]$ (no spaces are required). The only printed line must be this list in the exact required format.",
            "solution": "The user has provided a valid problem statement.\n\nThe problem is a variant of the classic **0-1 Knapsack Problem**. We are asked to select a subset of items (atomic configuration frames) to maximize total utility (expected error reduction), subject to a weight constraint (total computational cost not exceeding a budget $B$). The items are discrete and cannot be partially selected ($x_i \\in \\{0,1\\}$).\n\nThis problem is defined by the following integer linear program:\n$$\n\\text{maximize } \\sum_{i=0}^{n-1} u_i x_i \\\\\n\\text{subject to } \\sum_{i=0}^{n-1} c_i x_i \\le B \\\\\nx_i \\in \\{0,1\\}\n$$\nwhere $u_i$ is the utility of frame $i$, $c_i$ is its cost, and $B$ is the total budget. The problem is complicated by a three-tiered, deterministic tie-breaking rule:\n1.  Maximize total utility $\\sum u_i x_i$.\n2.  For subsets with the same maximal utility, minimize total cost $\\sum c_i x_i$.\n3.  If a tie persists, choose the subset whose sorted list of indices is lexicographically smallest.\n\nGiven the integer costs and budget, and the relatively small size of the item sets in the test cases, a **dynamic programming (DP)** approach is appropriate and efficient. To handle the complex tie-breaking rules, the state stored in our DP table must contain information about all three criteria.\n\nLet's define the DP state for a given budget capacity $w$ as a tuple: $(\\text{utility}, \\text{cost}, \\text{indices})$. The 'best' state is determined by a custom comparison function that implements the specified tie-breaking rules. A state $S_1 = (u_1, c_1, \\text{idx}_1)$ is considered better than a state $S_2 = (u_2, c_2, \\text{idx}_2)$ if:\n- $u_1 > u_2$, or\n- $u_1 = u_2$ and $c_1 < c_2$, or\n- $u_1 = u_2$, $c_1 = c_2$, and the list $\\text{idx}_1$ is lexicographically smaller than $\\text{idx}_2$.\n\nWe will use a one-dimensional DP array, `dp`, of size $B+1$. `dp[w]` will store the best state achievable with a budget of at most $w$. The array is initialized to a state representing zero utility and cost with an empty set of indices: $(0.0, 0, [])$.\n\nThe algorithm proceeds by iterating through each frame $i \\in \\{0, 1, \\dots, n-1\\}$ in increasing order of their indices. This specific order is crucial for correctly resolving the lexicographical tie-breaking rule. For each frame $i$ with utility $u_i$ and cost $c_i$, we update the `dp` table. To ensure that each frame is considered at most once (the 0-1 constraint), we iterate through the budget $w$ in reverse order, from $B$ down to $c_i$.\n\nFor each budget capacity $w$, we consider two possibilities for frame $i$:\n1.  **Frame $i$ is not selected**: The best state for budget $w$ remains `dp[w]` as determined by the preceding frames.\n2.  **Frame $i$ is selected**: This is only possible if $w \\ge c_i$. The new candidate state is formed by taking the optimal state for the remaining budget, `dp[w - c_i]`, and adding frame $i$'s utility, cost, and index.\n\nThe recurrence relation for the state update is:\n`dp[w] = better_of(dp[w], dp[w - c_i] + {u_i, c_i, i})`\n\nThe `+` operation on the state implies creating a new state where utility and cost are summed, and the index $i$ is appended to the list of indices. Since we process frames in increasing order of index $i$, and the index lists in `dp` are always sorted, appending $i$ to a list of smaller indices maintains the sorted order.\n\nAfter iterating through all frames, `dp[B]` will hold the optimal state for a budget up to $B$. The formulation of the DP ensures that `dp[w]` represents the best outcome for a budget less than or equal to $w$, as any solution for a smaller budget is a valid candidate for a larger budget. Thus, the solution for the overall problem is simply the set of indices stored in `dp[B]`.\n\nThe time complexity of this algorithm is $O(n \\cdot B \\cdot k)$, where $n$ is the number of frames, $B$ is the budget, and $k$ is the average number of items in a solution, which dictates the cost of list manipulation (copying and comparison). For the given problem constraints where $n$ is small, this is highly efficient. The space complexity is $O(B \\cdot k)$.",
            "answer": "```python\nimport numpy as np\n\ndef _is_better(s1, s2):\n    \"\"\"\n    Compares two states s1 and s2 based on the problem's tie-breaking rules.\n    A state is a tuple: (utility, cost, indices).\n    Returns True if s1 is better than s2.\n    \"\"\"\n    u1, c1, idx1 = s1\n    u2, c2, idx2 = s2\n    \n    if u1 > u2:\n        return True\n    if u1  u2:\n        return False\n    \n    # Utilities are equal, compare costs (lower is better)\n    if c1  c2:\n        return True\n    if c1 > c2:\n        return False\n        \n    # Utilities and costs are equal, compare index lists (lexicographically smaller is better)\n    # Python's default list comparison is lexicographical.\n    return idx1  idx2\n\ndef _solve_knapsack(utilities, costs, budget):\n    \"\"\"\n    Solves the 0-1 Knapsack problem with special tie-breaking rules using dynamic programming.\n    \"\"\"\n    n = len(utilities)\n    \n    # dp[w] stores the best (utility, cost, indices) tuple for a budget of at most w.\n    base_state = (0.0, 0, [])\n    dp = [base_state] * (budget + 1)\n\n    # Process items in their natural order (sorted by index 0, 1, ..., n-1)\n    for i in range(n):\n        u_i, c_i = utilities[i], costs[i]\n        \n        # Iterate backwards over the budget for 0-1 knapsack logic\n        for w in range(budget, c_i - 1, -1):\n            \n            # The state we can build upon if we add item i\n            prev_state = dp[w - c_i]\n            prev_u, prev_c, prev_indices = prev_state\n            \n            # Create candidate state by adding item i\n            candidate_state = (\n                prev_u + u_i,\n                prev_c + c_i,\n                prev_indices + [i]\n            )\n\n            # Compare candidate with current best for budget w\n            current_state = dp[w]\n            \n            if _is_better(candidate_state, current_state):\n                dp[w] = candidate_state\n    \n    # The best solution for a budget up to B is at dp[B].\n    final_state = dp[budget]\n    \n    # The indices list is already sorted due to the processing order.\n    return final_state[2]\n\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        {'u': [0.8, 1.0, 1.2, 0.3], 'c': [4, 5, 6, 2], 'B': 10},\n        {'u': [1.1, 0.9], 'c': [3, 4], 'B': 0},\n        {'u': [0.5, 0.7, 0.9], 'c': [1, 2, 3], 'B': 6},\n        {'u': [1.0, 1.0, 0.1], 'c': [3, 3, 1], 'B': 4},\n        {'u': [0.6, 0.4, 0.9], 'c': [0, 2, 2], 'B': 2},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = _solve_knapsack(case['u'], case['c'], case['B'])\n        results.append(result)\n\n    # Format the final output string to be a compact representation of a list of lists.\n    # e.g., [[0,2],[],[0,1,2]]\n    print(str(results).replace(' ', ''))\n\nsolve()\n```"
        },
        {
            "introduction": "The \"utility\" of a candidate configuration, central to the previous selection problem, must be intelligently defined. A powerful strategy is to prioritize configurations where the model violates known physical laws. This exercise explores how to create a physics-informed acquisition function by ensuring the potential respects a repulsive energy floor at short interatomic distances, a key feature for preventing unphysical atomic collapse. You will use the Lower Confidence Bound (LCB) to formalize a risk-averse policy that proactively queries configurations where the model is most likely to fail .",
            "id": "3431926",
            "problem": "A materials modeler is developing an interatomic potential $E_{\\theta}(\\mathbf{R})$ for multi-species systems using Active Learning (AL). The potential is trained to reproduce energies from Density Functional Theory (DFT) while remaining physically consistent at short interatomic distances. From quantum mechanics, the non-relativistic many-body Hamiltonian with Coulomb interactions implies that as two nuclei approach, the nuclear-nuclear repulsion term scales like $\\frac{Z_i Z_j e^2}{4\\pi\\epsilon_0 r_{ij}}$, and the Pauli exclusion principle drives the electronic kinetic energy upward under extreme compression. Together, these mechanisms enforce that the total energy should not fall below a repulsive floor when atoms are brought very close. To incorporate this physics, the modeler defines an analytic short-range repulsion floor $E_{\\text{core}}(\\mathbf{R})$, for example via a pairwise Born-Mayer form $V_{\\text{BM}}(r_{ij})=A_{ij}\\exp(-B_{ij}r_{ij})$ summed over pairs, which acts as a lower bound surrogate based on well-tested short-range physics.\n\nBecause $E_{\\theta}(\\mathbf{R})$ is learned and uncertain away from current training coverage, the modeler uses an ensemble whose predictive distribution at configuration $\\mathbf{R}$ is approximated as Gaussian with mean $\\mu(\\mathbf{R})$ and standard deviation $\\sigma(\\mathbf{R})$. The AL objective is to prioritize querying DFT labels for configurations under compression where the model may violate the physical bound $E_{\\theta}(\\mathbf{R})\\ge E_{\\text{core}}(\\mathbf{R})$, while accounting for predictive uncertainty in a way that controls the risk of unphysical predictions.\n\nWhich acquisition rule below correctly formalizes a physics-informed query policy that targets configurations whose predicted worst-case (within a chosen confidence) energy can fall below the repulsive floor, thereby flagging potential violations for correction? Assume a user-chosen confidence parameter $\\kappa0$.\n\nA. Query configurations $\\mathbf{R}$ with the most negative margin $M_{\\text{LCB}}(\\mathbf{R})=\\mu(\\mathbf{R})-\\kappa\\sigma(\\mathbf{R})-E_{\\text{core}}(\\mathbf{R})$, i.e., select those satisfying $\\mu(\\mathbf{R})-\\kappa\\sigma(\\mathbf{R})E_{\\text{core}}(\\mathbf{R})$.\n\nB. Query configurations $\\mathbf{R}$ for which $\\mu(\\mathbf{R})+\\kappa\\sigma(\\mathbf{R})E_{\\text{core}}(\\mathbf{R})$, because the upper confidence bound is below the floor.\n\nC. Query configurations $\\mathbf{R}$ for which $\\mu(\\mathbf{R})E_{\\text{core}}(\\mathbf{R})$, ignoring $\\sigma(\\mathbf{R})$ to avoid overfitting to uncertain predictions.\n\nD. Query configurations $\\mathbf{R}$ with the largest force norm $\\lVert\\nabla_{\\mathbf{R}}E_{\\theta}(\\mathbf{R})\\rVert$, because large forces under compression indicate likely unphysical behavior even without comparing to $E_{\\text{core}}(\\mathbf{R})$.\n\nE. Query configurations $\\mathbf{R}$ with $\\mu(\\mathbf{R})-\\kappa\\sigma(\\mathbf{R})0$, using a zero baseline so that the model never predicts negative energies within confidence.",
            "solution": "The user wants me to act as a STEM professor and solve a problem related to Active Learning (AL) for interatomic potential development.\n\n### Step 1: Problem Validation\n\nThe problem statement is critically evaluated for scientific validity, completeness, and clarity.\n\n#### Extracting Givens\n-   **Model**: An interatomic potential $E_{\\theta}(\\mathbf{R})$ for multi-species systems being developed via Active Learning (AL).\n-   **Reference Data**: Energies from Density Functional Theory (DFT).\n-   **Physical Constraint**: The potential must be physically consistent at short interatomic distances, meaning the total energy should not fall below a repulsive floor.\n-   **Repulsive Floor Model**: An analytic short-range repulsion floor $E_{\\text{core}}(\\mathbf{R})$ is defined as a lower bound surrogate, e.g., a sum of pairwise Born-Mayer potentials $V_{\\text{BM}}(r_{ij})=A_{ij}\\exp(-B_{ij}r_{ij})$.\n-   **Uncertainty Quantification**: The model is an ensemble whose predictive distribution for a configuration $\\mathbf{R}$ is approximated as a Gaussian with mean $\\mu(\\mathbf{R})$ and standard deviation $\\sigma(\\mathbf{R})$.\n-   **AL Objective**: To query DFT labels for configurations where the model might violate the physical bound $E_{\\theta}(\\mathbf{R}) \\ge E_{\\text{core}}(\\mathbf{R})$.\n-   **Acquisition Strategy Requirement**: The strategy must account for predictive uncertainty to control the risk of unphysical predictions.\n-   **Parameter**: A user-chosen confidence parameter $\\kappa > 0$.\n-   **Question**: Identify the acquisition rule that formalizes a query policy targeting configurations whose predicted worst-case energy (within a chosen confidence level) can fall below the repulsive floor $E_{\\text{core}}(\\mathbf{R})$.\n\n#### Validation using Extracted Givens\n-   **Scientific Grounding**: The problem is firmly grounded in computational materials science and machine learning. The concepts of AL, DFT, ensemble-based uncertainty quantification, Gaussian predictive distributions, and physics-informed constraints (specifically, short-range repulsion modeled by potentials like Born-Mayer) are all standard, well-established practices in the field. The physical reasoning regarding nuclear-nuclear repulsion and the Pauli exclusion principle is correct.\n-   **Well-Posedness**: The problem is well-posed. It provides a clear objective and all necessary components ($\\mu(\\mathbf{R})$, $\\sigma(\\mathbf{R})$, $E_{\\text{core}}(\\mathbf{R})$, $\\kappa$) to construct a unique, meaningful acquisition function as requested.\n-   **Objectivity**: The language is technical, precise, and free of subjective or ambiguous terminology.\n\n#### Verdict\nThe problem statement is valid. It describes a realistic and scientifically sound scenario in modern materials modeling. I will proceed with the solution.\n\n### Step 2: Solution Derivation\n\nThe core task is to formulate an active learning acquisition rule that identifies configurations where the learned potential $E_{\\theta}(\\mathbf{R})$ is at risk of violating a physical lower bound, $E_{\\text{core}}(\\mathbf{R})$. The formulation must be risk-averse, incorporating the model's predictive uncertainty.\n\n1.  The learned potential's prediction at a configuration $\\mathbf{R}$ is not a single value but a probability distribution, which is approximated by a Gaussian with mean $\\mu(\\mathbf{R})$ and standard deviation $\\sigma(\\mathbf{R})$.\n\n2.  The objective is to find configurations where the model might predict an energy $E_{\\theta}(\\mathbf{R})$ that is unphysically low, specifically $E_{\\theta}(\\mathbf{R})  E_{\\text{core}}(\\mathbf{R})$.\n\n3.  To be risk-averse, we should not just check if the mean prediction $\\mu(\\mathbf{R})$ violates the bound. We must consider the entire predictive distribution. A \"worst-case\" prediction, within a certain confidence level, corresponds to the lower end of the predictive distribution.\n\n4.  For a Gaussian distribution, a one-sided confidence interval is constructed using a multiple of the standard deviation. The Lower Confidence Bound (LCB) provides a statistical lower limit for the predicted value. It is defined as:\n    $$ E_{\\text{LCB}}(\\mathbf{R}) = \\mu(\\mathbf{R}) - \\kappa\\sigma(\\mathbf{R}) $$\n    Here, $\\kappa  0$ is a parameter that controls the confidence level. A larger $\\kappa$ corresponds to a more conservative lower bound (i.e., higher confidence that the true value is above $E_{\\text{LCB}}$).\n\n5.  The acquisition rule should flag a configuration for labeling if its \"worst-case\" predicted energy, the LCB, falls below the physical floor $E_{\\text{core}}(\\mathbf{R})$. This condition is expressed mathematically as:\n    $$ E_{\\text{LCB}}(\\mathbf{R})  E_{\\text{core}}(\\mathbf{R}) $$\n    Substituting the definition of the LCB, we get:\n    $$ \\mu(\\mathbf{R}) - \\kappa\\sigma(\\mathbf{R})  E_{\\text{core}}(\\mathbf{R}) $$\n\n6.  In an active learning loop, we typically have a budget to query only one or a few configurations per iteration. Therefore, we need to rank the candidate configurations that satisfy the violation condition. A logical ranking scheme is to prioritize the configuration that shows the most severe potential violation.\n\n7.  We can define a margin or score, $M(\\mathbf{R})$, representing the gap between the LCB and the physical floor:\n    $$ M(\\mathbf{R}) = (\\mu(\\mathbf{R}) - \\kappa\\sigma(\\mathbf{R})) - E_{\\text{core}}(\\mathbf{R}) $$\n    A negative value of $M(\\mathbf{R})$ indicates a potential violation. The acquisition rule would be to select the configuration $\\mathbf{R}$ that minimizes this margin, i.e., has the most negative value of $M(\\mathbf{R})$. This focuses the AL process on the regions of greatest concern.\n\n### Step 3: Option-by-Option Analysis\n\nNow, I will evaluate each provided option based on the derived formulation.\n\n**A. Query configurations $\\mathbf{R}$ with the most negative margin $M_{\\text{LCB}}(\\mathbf{R})=\\mu(\\mathbf{R})-\\kappa\\sigma(\\mathbf{R})-E_{\\text{core}}(\\mathbf{R})$, i.e., select those satisfying $\\mu(\\mathbf{R})-\\kappa\\sigma(\\mathbf{R})E_{\\text{core}}(\\mathbf{R})$.**\nThis option perfectly matches the derived logic. It correctly identifies the \"worst-case\" energy as the LCB, $\\mu(\\mathbf{R})-\\kappa\\sigma(\\mathbf{R})$. It compares this LCB to the correct physical baseline, $E_{\\text{core}}(\\mathbf{R})$. The condition for a query, $\\mu(\\mathbf{R})-\\kappa\\sigma(\\mathbf{R})  E_{\\text{core}}(\\mathbf{R})$, is precisely the condition for a potential violation. Prioritizing by the most negative margin, $M_{\\text{LCB}}(\\mathbf{R})$, is the standard way to implement this strategy.\n**Verdict: Correct.**\n\n**B. Query configurations $\\mathbf{R}$ for which $\\mu(\\mathbf{R})+\\kappa\\sigma(\\mathbf{R})E_{\\text{core}}(\\mathbf{R})$, because the upper confidence bound is below the floor.**\nThis option uses the Upper Confidence Bound (UCB), $\\mu(\\mathbf{R})+\\kappa\\sigma(\\mathbf{R})$. The condition that the UCB is below the floor implies that the entire confidence interval of the prediction is unphysically low. This is an extremely strong condition, indicating a region where the model is already severely wrong, not merely at risk of being wrong. The goal of a proactive AL strategy is to sample where the *lower* bound of the prediction is unphysical to prevent the model from developing such catastrophic failures. This rule is overly conservative and misinterprets the concept of risk in this context.\n**Verdict: Incorrect.**\n\n**C. Query configurations $\\mathbf{R}$ for which $\\mu(\\mathbf{R})E_{\\text{core}}(\\mathbf{R})$, ignoring $\\sigma(\\mathbf{R})$ to avoid overfitting to uncertain predictions.**\nThis option considers only the mean prediction $\\mu(\\mathbf{R})$ and ignores the uncertainty $\\sigma(\\mathbf{R})$. This violates the problem's explicit requirement to account for predictive uncertainty. An AL strategy should explore regions of high uncertainty where the model's behavior is unknown. A configuration might have a mean prediction $\\mu(\\mathbf{R})$ slightly above $E_{\\text{core}}(\\mathbf{R})$ but a large $\\sigma(\\mathbf{R})$, making a violation plausible. This rule would fail to identify such informative points. The provided justification is also unsound; using uncertainty for exploration is a primary mechanism to *prevent* overfitting by ensuring the model is trained on a diverse and challenging set of configurations.\n**Verdict: Incorrect.**\n\n**D. Query configurations $\\mathbf{R}$ with the largest force norm $\\lVert\\nabla_{\\mathbf{R}}E_{\\theta}(\\mathbf{R})\\rVert$, because large forces under compression indicate likely unphysical behavior even without comparing to $E_{\\text{core}}(\\mathbf{R})$.**\nQuerying based on large forces is a known heuristic in AL for potentials, as large forces often correspond to dynamically important events or regions where the potential surface is steep and possibly inaccurate. However, this strategy is not a direct formalization of the objective stated in the problem, which is to specifically enforce the energy bound $E_{\\theta}(\\mathbf{R}) \\ge E_{\\text{core}}(\\mathbf{R})$. This option does not use $E_{\\text{core}}(\\mathbf{R})$ or the LCB concept, making it an indirect and less targeted approach for the specified goal.\n**Verdict: Incorrect.**\n\n**E. Query configurations $\\mathbf{R}$ with $\\mu(\\mathbf{R})-\\kappa\\sigma(\\mathbf{R})0$, using a zero baseline so that the model never predicts negative energies within confidence.**\nThis option correctly uses the LCB, $\\mu(\\mathbf{R})-\\kappa\\sigma(\\mathbf{R})$. However, it compares it to an incorrect and physically arbitrary baseline of $0$. The absolute energy scale in quantum mechanical calculations is set by convention (e.g., zero energy for infinitely separated atoms). The physically meaningful lower bound in this problem is the configuration-dependent repulsive floor, $E_{\\text{core}}(\\mathbf{R})$, which arises from nuclear-nuclear and electron-electron repulsion at short distances and will typically be a large positive value. Comparing the LCB to $0$ does not enforce the required physical constraint.\n**Verdict: Incorrect.**",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "To implement physics-informed acquisition functions like the LCB, we need robust and reliable estimates of model uncertainty. This practice provides a complete, hands-on guide to the entire uncertainty quantification pipeline. You will learn to compute and distinguish between aleatoric and epistemic uncertainty using deep ensembles, calibrate the raw predictions using temperature scaling to ensure they are statistically meaningful, and deploy this calibrated uncertainty to create an automated trigger for on-the-fly DFT calculations during a molecular dynamics simulation .",
            "id": "3431917",
            "problem": "You are given a synthetic but scientifically consistent setup for uncertainty calibration of a Graph Neural Network (GNN) interatomic potential using deep ensembles and temperature scaling, and the subsequent use of calibrated predictive intervals to decide when to trigger on-the-fly Density Functional Theory (DFT) evaluations during Molecular Dynamics (MD). All energies are in electronvolts, and all variances are in squared electronvolts. Your task is to implement a complete program that performs the following steps purely from first principles of probability and statistics.\n\nDefinitions and assumptions:\n- A deep ensemble consists of $M$ independently trained models. For a configuration index $i$, model index $m$ provides a predicted mean energy $\\mu_{i,m}$ and a predicted aleatoric variance $s^{2}_{i,m}$. For any set of $M$ models, the ensemble predictive mean is the arithmetic average, and the ensemble predictive variance is the sum of the average predicted aleatoric variance and the variance of model means (epistemic contribution), both computed as population moments across the $M$ models.\n- On a calibration set with known true energies $y_{i}$, we assume a Gaussian likelihood with residual $r_{i} = y_{i} - \\bar{\\mu}_{i}$ and pre-calibration ensemble variance $v_{i}$. Temperature scaling multiplies each variance by a common factor $\\tau^{2}$, which is to be determined by minimizing the average negative log-likelihood under the Gaussian assumption without introducing any additional regularization.\n- For a target nominal two-sided coverage $c = 1 - \\alpha$, the predictive interval is constructed by using the quantile of the standard normal distribution. For MD frames without ground truth, a DFT trigger is decided by the Gaussian tail probability that the absolute error exceeds a user-specified tolerance $\\varepsilon$: trigger if the probability that the absolute error exceeds $\\varepsilon$ is greater than or equal to $\\alpha$.\n\nNumerical stability:\n- To avoid degeneracy, always replace any non-positive variance with a small floor $v_{\\min} > 0$ before any division or square root. The floor should be applied consistently to calibration and MD variances.\n\nUnits and format:\n- Energies are in electronvolts. Variances are in squared electronvolts. The final outputs are dimensionless or integer as specified, and you must round any floating-point outputs to exactly six digits after the decimal point.\n\nMathematical and computational requirements that must be implemented:\n1. For each test case, compute the ensemble mean $\\bar{\\mu}_{i}$ and pre-calibration variance $v_{i}$ for calibration configurations by\n   - $\\bar{\\mu}_{i} = \\frac{1}{M} \\sum_{m=1}^{M} \\mu_{i,m}$,\n   - $v_{\\text{ale},i} = \\frac{1}{M} \\sum_{m=1}^{M} s^{2}_{i,m}$,\n   - $v_{\\text{epi},i} = \\frac{1}{M} \\sum_{m=1}^{M} \\left(\\mu_{i,m} - \\bar{\\mu}_{i}\\right)^{2}$,\n   - $v_{i} = \\max\\left(v_{\\text{ale},i} + v_{\\text{epi},i}, v_{\\min}\\right)$.\n2. Determine the temperature scaling factor $\\tau > 0$ by minimizing the average negative log-likelihood under the Gaussian assumption for the calibration set with respect to $\\tau$, starting from only the above definitions. Do not use any surrogate or shortcut expression that is not derived from the likelihood definition.\n3. Using the calibrated variance $\\tilde{v}_{i} = \\tau^{2} v_{i}$, report the empirical coverage on the calibration set at nominal coverage $c = 1 - \\alpha$ by counting the fraction of $y_{i}$ that fall inside the two-sided interval $\\left[\\bar{\\mu}_{i} - z \\sqrt{\\tilde{v}_{i}}, \\bar{\\mu}_{i} + z \\sqrt{\\tilde{v}_{i}}\\right]$, where $z$ is the standard normal quantile corresponding to $(1+c)/2$.\n4. For each MD frame $j$, compute the ensemble mean $\\bar{\\mu}_{j}$ and variance $v_{j}$ in the same way, apply calibration to get $\\tilde{v}_{j} = \\tau^{2} v_{j}$, compute the tail probability $p_{j} = 2 \\left[1 - \\Phi\\left(\\varepsilon / \\sqrt{\\tilde{v}_{j}}\\right)\\right]$ where $\\Phi$ is the cumulative distribution function of the standard normal distribution, and decide a DFT trigger if $p_{j} \\ge \\alpha$. Report the total number of triggers per test case as an integer.\n\nRounding and output:\n- For each test case, output a triplet consisting of $\\tau$ rounded to six decimals, the empirical coverage (as a decimal fraction) rounded to six decimals, and the trigger count (an integer). Aggregate the triplets from all test cases into a single line printed as a list of lists with no spaces, for example $[[\\tau_{1},\\text{cov}_{1},n_{1}],[\\tau_{2},\\text{cov}_{2},n_{2}],\\dots]$ where each $\\tau_{k}$ and coverage is rounded to exactly six decimals.\n- The final output must be exactly one line containing the aggregated results in this format.\n\nTest suite:\nImplement your program to run the following three test cases. All energies are in electronvolts, all variances are in squared electronvolts, and all angles (if any) would be in radians but none are used here.\n\nTest case $1$ (general ensemble with both aleatoric and epistemic contributions):\n- Ensemble size $M = 3$.\n- Calibration true energies $y = [0.10, -0.05, 0.20, -0.10, 0.00]$.\n- Calibration per-model predicted means and variances:\n  - Model $1$: $\\mu_{:,1} = [0.12, -0.06, 0.18, -0.12, 0.02]$, $s^{2}_{:,1} = [0.0025, 0.0025, 0.0036, 0.0025, 0.0036]$.\n  - Model $2$: $\\mu_{:,2} = [0.11, -0.04, 0.22, -0.08, -0.01]$, $s^{2}_{:,2} = [0.0036, 0.0025, 0.0036, 0.0036, 0.0025]$.\n  - Model $3$: $\\mu_{:,3} = [0.09, -0.05, 0.21, -0.11, -0.02]$, $s^{2}_{:,3} = [0.0025, 0.0025, 0.0049, 0.0025, 0.0036]$.\n- MD per-model predicted means and variances:\n  - Model $1$: $\\mu^{\\text{MD}}_{:,1} = [0.05, -0.02, 0.15, -0.08]$, $(s^{2})^{\\text{MD}}_{:,1} = [0.0036, 0.0025, 0.0036, 0.0025]$.\n  - Model $2$: $\\mu^{\\text{MD}}_{:,2} = [0.04, -0.01, 0.17, -0.07]$, $(s^{2})^{\\text{MD}}_{:,2} = [0.0025, 0.0036, 0.0036, 0.0036]$.\n  - Model $3$: $\\mu^{\\text{MD}}_{:,3} = [0.06, -0.03, 0.16, -0.09]$, $(s^{2})^{\\text{MD}}_{:,3} = [0.0036, 0.0025, 0.0049, 0.0025]$.\n- Error tolerance $\\varepsilon = 0.05$.\n- Risk threshold $\\alpha = 0.10$.\n- Variance floor $v_{\\min} = 10^{-8}$.\n- Report triplet $[\\tau,\\text{coverage},\\text{triggers}]$ with $\\tau$ and coverage rounded to six decimals.\n\nTest case $2$ (boundary-trigger construction with exact temperature scaling unity):\n- Ensemble size $M = 2$.\n- Calibration true energies $y = [0.50, -0.30, 0.10]$.\n- Choose calibration residuals $r = [0.10, -0.20, 0.05]$ and define identical per-model means $\\mu_{:,1} = \\mu_{:,2} = y - r = [0.40, -0.10, 0.05]$. Choose identical per-model variances $s^{2}_{:,1} = s^{2}_{:,2} = [0.01, 0.04, 0.0025]$ so that the ratio $r_{i}^{2} / v_{i}$ equals $1$ for all $i$, which implies the optimal $\\tau$ equals $1$ when derived from the Gaussian likelihood.\n- MD construction to place one frame exactly at the DFT-trigger boundary: let the error tolerance be $\\varepsilon = 0.10$ and the risk threshold $\\alpha = 0.10$, and define $z_{\\alpha} = \\Phi^{-1}(1 - \\alpha/2)$. Let $\\sigma_{\\text{th}} = \\varepsilon / z_{\\alpha}$, and construct three MD frames with identical means across the two models (any reasonable values) and identical per-model variances across models per frame as follows:\n  - Frame A: per-model variance equals $\\sigma_{\\text{th}}^{2}$ (exact boundary).\n  - Frame B: per-model variance equals $(0.9 \\sigma_{\\text{th}})^{2}$ (below boundary).\n  - Frame C: per-model variance equals $(1.1 \\sigma_{\\text{th}})^{2}$ (above boundary).\n  Because models are identical on each frame, the ensemble predictive variance equals these values, making the trigger outcomes $(\\text{equal},\\text{no},\\text{yes})$ under the rule $p \\ge \\alpha$.\n- Use variance floor $v_{\\min} = 10^{-12}$.\n- Report triplet $[\\tau,\\text{coverage},\\text{triggers}]$ with $\\tau$ and coverage rounded to six decimals.\n\nTest case $3$ (edge case with near-zero aleatoric variance in one model and nontrivial epistemic spread):\n- Ensemble size $M = 3$.\n- Calibration true energies $y = [0.00, 0.10, -0.10, 0.05]$.\n- Calibration per-model predicted means and variances:\n  - Model $1$: $\\mu_{:,1} = [0.02, 0.08, -0.12, 0.04]$, $s^{2}_{:,1} = [0.0, 10^{-8}, 0.0, 10^{-8}]$.\n  - Model $2$: $\\mu_{:,2} = [0.00, 0.12, -0.08, 0.06]$, $s^{2}_{:,2} = [0.0025, 0.0036, 0.0025, 0.0036]$.\n  - Model $3$: $\\mu_{:,3} = [-0.01, 0.09, -0.11, 0.07]$, $s^{2}_{:,3} = [0.0025, 0.0025, 0.0036, 0.0025]$.\n- MD per-model predicted means and variances:\n  - Model $1$: $\\mu^{\\text{MD}}_{:,1} = [0.03, -0.02, 0.12, -0.05]$, $(s^{2})^{\\text{MD}}_{:,1} = [0.0, 0.0, 0.0, 0.0]$.\n  - Model $2$: $\\mu^{\\text{MD}}_{:,2} = [-0.04, -0.01, 0.08, -0.06]$, $(s^{2})^{\\text{MD}}_{:,2} = [0.0025, 0.0025, 0.0025, 0.0025]$.\n  - Model $3$: $\\mu^{\\text{MD}}_{:,3} = [0.00, -0.03, 0.10, -0.04]$, $(s^{2})^{\\text{MD}}_{:,3} = [0.0025, 0.0049, 0.0036, 0.0025]$.\n- Error tolerance $\\varepsilon = 0.05$.\n- Risk threshold $\\alpha = 0.10$.\n- Variance floor $v_{\\min} = 10^{-8}$.\n- Report triplet $[\\tau,\\text{coverage},\\text{triggers}]$ with $\\tau$ and coverage rounded to six decimals.\n\nFinal output format for the entire program:\n- Your program should produce a single line of output containing the results as a comma-separated list of the three triplets enclosed in square brackets with no spaces. The exact format is $[[\\tau_{1},\\text{cov}_{1},n_{1}],[\\tau_{2},\\text{cov}_{2},n_{2}],[\\tau_{3},\\text{cov}_{3},n_{3}]]$, where each $\\tau_{k}$ and $\\text{cov}_{k}$ is rounded to six decimals and each $n_{k}$ is an integer.",
            "solution": "The problem requires the implementation of a complete workflow for uncertainty calibration of a deep ensemble interatomic potential and its subsequent application in an active learning context for molecular dynamics (MD) simulations. The methodology is based on firm principles of probability and statistics. We will first derive the necessary formulas from first principles and then outline the computational algorithm.\n\n**1. Ensemble Predictions: Mean and Variance**\n\nA deep ensemble consists of $M$ independently trained models. For a given atomic configuration, indexed by $i$, each model $m \\in \\{1, \\dots, M\\}$ provides a prediction for the mean energy, $\\mu_{i,m}$, and an associated aleatoric variance, $s^{2}_{i,m}$. Aleatoric uncertainty is inherent to the data (e.g., due to thermal noise in the training data) and is predicted by the model itself.\n\nThe ensemble's consensus prediction for the mean energy is the arithmetic average of the individual model means:\n$$ \\bar{\\mu}_{i} = \\frac{1}{M} \\sum_{m=1}^{M} \\mu_{i,m} $$\n\nThe total predictive variance of the ensemble, $v_i$, is decomposed into two components: the average aleatoric variance and the epistemic variance. Epistemic uncertainty arises from the model's lack of knowledge, captured by the disagreement among the ensemble members.\n\nThe average aleatoric variance, $v_{\\text{ale},i}$, is the mean of the variances predicted by each model:\n$$ v_{\\text{ale},i} = \\frac{1}{M} \\sum_{m=1}^{M} s^{2}_{i,m} $$\n\nThe epistemic variance, $v_{\\text{epi},i}$, is the variance of the mean predictions from the ensemble members, calculated as the population variance:\n$$ v_{\\text{epi},i} = \\frac{1}{M} \\sum_{m=1}^{M} \\left(\\mu_{i,m} - \\bar{\\mu}_{i}\\right)^{2} $$\n\nThe total pre-calibration variance is the sum of these two contributions:\n$$ v_i^{\\text{raw}} = v_{\\text{ale},i} + v_{\\text{epi},i} $$\nFor numerical stability, especially when computing logarithms or inverse square roots, any non-positive variance is floored at a small positive value $v_{\\min}$. The final pre-calibration variance is thus:\n$$ v_{i} = \\max\\left(v_i^{\\text{raw}}, v_{\\min}\\right) $$\n\n**2. Calibration via Temperature Scaling**\n\nThe raw predictive variance from the ensemble is often miscalibrated, meaning the predicted uncertainties do not accurately reflect the true error distribution. Temperature scaling is a simple and effective post-hoc calibration method that corrects the variance by a single multiplicative factor, $\\tau^2$. The calibrated variance is $\\tilde{v}_i = \\tau^2 v_i$.\n\nThe optimal temperature $\\tau  0$ is determined by maximizing the log-likelihood of a calibration dataset under a Gaussian assumption. Let the calibration set consist of $N$ configurations with known true energies $y_i$. We model the true energy for each configuration as a draw from a normal distribution with mean $\\bar{\\mu}_i$ and variance $\\tilde{v}_i$:\n$$ p(y_i | \\bar{\\mu}_i, v_i, \\tau) = \\mathcal{N}(y_i; \\bar{\\mu}_i, \\tau^2 v_i) = \\frac{1}{\\sqrt{2\\pi \\tau^2 v_i}} \\exp\\left(-\\frac{(y_i - \\bar{\\mu}_i)^2}{2 \\tau^2 v_i}\\right) $$\nThe log-likelihood for a single data point is:\n$$ \\ln p(y_i) = -\\frac{1}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\tau^2 v_i) - \\frac{r_i^2}{2 \\tau^2 v_i} $$\nwhere $r_i = y_i - \\bar{\\mu}_i$ is the residual. Assuming the data points are independent, the total negative log-likelihood (NLL) for the set is:\n$$ \\text{NLL}(\\tau) = -\\sum_{i=1}^{N} \\ln p(y_i) = \\sum_{i=1}^{N} \\left( \\frac{1}{2} \\ln(2\\pi) + \\ln(\\tau) + \\frac{1}{2} \\ln(v_i) + \\frac{r_i^2}{2 \\tau^2 v_i} \\right) $$\nTo find the optimal $\\tau$ that minimizes this NLL, we compute the derivative with respect to $\\tau$ and set it to zero.\n$$ \\frac{\\partial(\\text{NLL})}{\\partial \\tau} = \\sum_{i=1}^{N} \\left( \\frac{1}{\\tau} - \\frac{r_i^2}{v_i \\tau^3} \\right) = 0 $$\n$$ \\frac{N}{\\tau} = \\frac{1}{\\tau^3} \\sum_{i=1}^{N} \\frac{r_i^2}{v_i} $$\nSolving for $\\tau^2$ gives:\n$$ \\tau^2 = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{r_i^2}{v_i} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{(y_i - \\bar{\\mu}_i)^2}{v_i} $$\nThe optimal temperature is the square root of this quantity:\n$$ \\tau = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} \\frac{(y_i - \\bar{\\mu}_i)^2}{v_i}} $$\nThis result provides a direct analytical method to find the calibration factor from the calibration data.\n\n**3. Empirical Coverage and Predictive Intervals**\n\nAfter determining $\\tau$, we can assess the quality of the calibration by computing the empirical coverage on the calibration set. For a target nominal coverage probability $c = 1-\\alpha$, the two-sided predictive interval for $y_i$ is constructed as:\n$$ \\left[ \\bar{\\mu}_i - z \\sqrt{\\tilde{v}_i}, \\; \\bar{\\mu}_i + z \\sqrt{\\tilde{v}_i} \\right] $$\nwhere $\\tilde{v}_i = \\tau^2 v_i$ is the calibrated variance and $z = \\Phi^{-1}\\left(\\frac{1+c}{2}\\right) = \\Phi^{-1}\\left(1-\\frac{\\alpha}{2}\\right)$ is the quantile of the standard normal distribution $\\mathcal{N}(0,1)$, with $\\Phi$ being its cumulative distribution function (CDF).\nThe empirical coverage is the fraction of calibration points for which the true energy $y_i$ falls within this interval:\n$$ \\text{coverage} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{I}\\left( |y_i - \\bar{\\mu}_i| \\le z \\sqrt{\\tilde{v}_i} \\right) $$\nwhere $\\mathbb{I}(\\cdot)$ is the indicator function. A well-calibrated model should have an empirical coverage close to the nominal coverage $c$.\n\n**4. Uncertainty-based On-the-Fly DFT Trigger**\n\nIn active learning for MD, the calibrated uncertainty is used to decide when to perform a computationally expensive quantum mechanical calculation (e.g., DFT) instead of relying on the cheaper GNN potential. A DFT calculation is triggered if the model's uncertainty for a given configuration is too high.\n\nThis decision is formalized by setting an energy tolerance $\\varepsilon$ and a risk level $\\alpha$. A trigger is activated if the predicted probability of the absolute error $|y_j - \\bar{\\mu}_j|$ exceeding $\\varepsilon$ is greater than or equal to $\\alpha$. Under the calibrated Gaussian model, this probability $p_j$ is:\n$$ p_j = P(|y_j - \\bar{\\mu}_j| > \\varepsilon) = P\\left(\\frac{|y_j - \\bar{\\mu}_j|}{\\sqrt{\\tilde{v}_j}} > \\frac{\\varepsilon}{\\sqrt{\\tilde{v}_j}}\\right) $$\nSince $(y_j - \\bar{\\mu}_j)/\\sqrt{\\tilde{v}_j}$ is assumed to follow a standard normal distribution, this tail probability is:\n$$ p_j = 2 \\left[1 - \\Phi\\left(\\frac{\\varepsilon}{\\sqrt{\\tilde{v}_j}}\\right)\\right] $$\nThe trigger condition for MD frame $j$ is thus:\n$$ 2 \\left[1 - \\Phi\\left(\\frac{\\varepsilon}{\\sqrt{\\tilde{v}_j}}\\right)\\right] \\ge \\alpha $$\nThe total number of triggers is the count of MD frames that satisfy this condition.\n\n**Computational Algorithm Summary**\n\nFor each test case, the program will execute the following sequence:\n\n1.  **Process Calibration Data**:\n    a. For each of the $N$ calibration points, compute the ensemble mean $\\bar{\\mu}_i$ and pre-calibration variance $v_i = \\max(v_{\\text{ale},i} + v_{\\text{epi},i}, v_{\\min})$.\n    b. Compute the optimal temperature $\\tau$ using the derived formula.\n    c. Compute the calibrated variances $\\tilde{v}_i = \\tau^2 v_i$.\n    d. Determine the standard normal quantile $z$ for the nominal coverage $c = 1 - \\alpha$.\n    e. Calculate the empirical coverage by checking how many true energies $y_i$ are within their respective predictive intervals.\n\n2.  **Process MD Data**:\n    a. For each MD frame $j$, compute the ensemble mean $\\bar{\\mu}_j$ and pre-calibration variance $v_j$ using the same method as for the calibration data.\n    b. Apply the temperature scaling factor to get the calibrated variance $\\tilde{v}_j = \\tau^2 v_j$.\n    c. Calculate the trigger probability $p_j$ based on the error tolerance $\\varepsilon$.\n    d. Count the number of frames for which $p_j \\ge \\alpha$.\n\n3.  **Report Results**: For each test case, collect the calculated $\\tau$, empirical coverage (both rounded to six decimal places), and the integer count of triggers into a triplet. Aggregate these triplets into a single list of lists for the final output.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases and print the results.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1\n        {\n            \"M\": 3,\n            \"calib_y\": np.array([0.10, -0.05, 0.20, -0.10, 0.00]),\n            \"calib_mu\": np.array([\n                [0.12, -0.06, 0.18, -0.12, 0.02],\n                [0.11, -0.04, 0.22, -0.08, -0.01],\n                [0.09, -0.05, 0.21, -0.11, -0.02]\n            ]).T,\n            \"calib_s2\": np.array([\n                [0.0025, 0.0025, 0.0036, 0.0025, 0.0036],\n                [0.0036, 0.0025, 0.0036, 0.0036, 0.0025],\n                [0.0025, 0.0025, 0.0049, 0.0025, 0.0036]\n            ]).T,\n            \"md_mu\": np.array([\n                [0.05, -0.02, 0.15, -0.08],\n                [0.04, -0.01, 0.17, -0.07],\n                [0.06, -0.03, 0.16, -0.09]\n            ]).T,\n            \"md_s2\": np.array([\n                [0.0036, 0.0025, 0.0036, 0.0025],\n                [0.0025, 0.0036, 0.0036, 0.0036],\n                [0.0036, 0.0025, 0.0049, 0.0025]\n            ]).T,\n            \"epsilon\": 0.05,\n            \"alpha\": 0.10,\n            \"v_min\": 1e-8,\n        },\n        # Test Case 2\n        {\n            \"M\": 2,\n            \"calib_y\": np.array([0.50, -0.30, 0.10]),\n            \"calib_mu\": np.array([\n                [0.40, -0.10, 0.05],\n                [0.40, -0.10, 0.05]\n            ]).T,\n            \"calib_s2\": np.array([\n                [0.01, 0.04, 0.0025],\n                [0.01, 0.04, 0.0025]\n            ]).T,\n            \"epsilon\": 0.10,\n            \"alpha\": 0.10,\n            \"v_min\": 1e-12,\n            # MD part needs to be constructed\n            \"md_construction\": True,\n        },\n        # Test Case 3\n        {\n            \"M\": 3,\n            \"calib_y\": np.array([0.00, 0.10, -0.10, 0.05]),\n            \"calib_mu\": np.array([\n                [0.02, 0.08, -0.12, 0.04],\n                [0.00, 0.12, -0.08, 0.06],\n                [-0.01, 0.09, -0.11, 0.07]\n            ]).T,\n            \"calib_s2\": np.array([\n                [0.0, 1e-8, 0.0, 1e-8],\n                [0.0025, 0.0036, 0.0025, 0.0036],\n                [0.0025, 0.0025, 0.0036, 0.0025]\n            ]).T,\n            \"md_mu\": np.array([\n                [0.03, -0.02, 0.12, -0.05],\n                [-0.04, -0.01, 0.08, -0.06],\n                [0.00, -0.03, 0.10, -0.04]\n            ]).T,\n            \"md_s2\": np.array([\n                [0.0, 0.0, 0.0, 0.0],\n                [0.0025, 0.0025, 0.0025, 0.0025],\n                [0.0025, 0.0049, 0.0036, 0.0025]\n            ]).T,\n            \"epsilon\": 0.05,\n            \"alpha\": 0.10,\n            \"v_min\": 1e-8,\n        }\n    ]\n\n    # Construct MD data for Test Case 2\n    tc2 = test_cases[1]\n    alpha_tc2 = tc2[\"alpha\"]\n    epsilon_tc2 = tc2[\"epsilon\"]\n    z_alpha_tc2 = norm.ppf(1 - alpha_tc2 / 2)\n    sigma_th_sq = (epsilon_tc2 / z_alpha_tc2)**2\n    # Define some reasonable identical means\n    tc2[\"md_mu\"] = np.array([[0.1, 0.1], [0.2, 0.2], [0.3, 0.3]])\n    tc2[\"md_s2\"] = np.array([\n        [sigma_th_sq, sigma_th_sq],             # Frame A\n        [(0.9**2) * sigma_th_sq, (0.9**2) * sigma_th_sq], # Frame B\n        [(1.1**2) * sigma_th_sq, (1.1**2) * sigma_th_sq]  # Frame C\n    ])\n\n    results = []\n    for case in test_cases:\n        result = process_case(case)\n        results.append(result)\n\n    # Print the final result in the exact required format\n    print(str(results).replace(\" \", \"\"))\n\ndef get_ensemble_predictions(mu_models, s2_models, v_min):\n    \"\"\"\n    Computes ensemble mean and variance.\n    \n    Args:\n        mu_models (np.ndarray): Per-model mean predictions, shape (N_points, M).\n        s2_models (np.ndarray): Per-model aleatoric variances, shape (N_points, M).\n        v_min (float): Minimum variance floor.\n        \n    Returns:\n        tuple: (ensemble_mean, ensemble_variance)\n    \"\"\"\n    mu_bar = np.mean(mu_models, axis=1)\n    v_ale = np.mean(s2_models, axis=1)\n    # ddof=0 for population variance\n    v_epi = np.var(mu_models, axis=1, ddof=0)\n    v_raw = v_ale + v_epi\n    v = np.maximum(v_raw, v_min)\n    return mu_bar, v\n\ndef process_case(case_data):\n    \"\"\"\n    Processes a single test case.\n    \"\"\"\n    # Unpack data\n    y_calib = case_data[\"calib_y\"]\n    mu_calib_models = case_data[\"calib_mu\"]\n    s2_calib_models = case_data[\"calib_s2\"]\n    mu_md_models = case_data[\"md_mu\"]\n    s2_md_models = case_data[\"md_s2\"]\n    alpha = case_data[\"alpha\"]\n    epsilon = case_data[\"epsilon\"]\n    v_min = case_data[\"v_min\"]\n    N_calib = len(y_calib)\n\n    # 1. Calibration Phase\n    mu_bar_calib, v_calib = get_ensemble_predictions(mu_calib_models, s2_calib_models, v_min)\n    \n    # 2. Determine temperature scaling factor tau\n    residuals_sq = (y_calib - mu_bar_calib)**2\n    tau_sq = np.mean(residuals_sq / v_calib)\n    tau = np.sqrt(tau_sq)\n    \n    # 3. Compute empirical coverage on calibration set\n    v_calib_calibrated = tau_sq * v_calib\n    nominal_coverage_c = 1 - alpha\n    z = norm.ppf((1 + nominal_coverage_c) / 2)\n    \n    interval_half_width = z * np.sqrt(v_calib_calibrated)\n    is_covered = np.abs(y_calib - mu_bar_calib) = interval_half_width\n    empirical_coverage = np.sum(is_covered) / N_calib\n    \n    # 4. MD Phase - Triggering\n    mu_bar_md, v_md = get_ensemble_predictions(mu_md_models, s2_md_models, v_min)\n    v_md_calibrated = tau_sq * v_md\n    \n    # To prevent division by zero for any pathological variance\n    std_dev_md_calibrated = np.sqrt(np.maximum(v_md_calibrated, v_min))\n    \n    # Probability P(|error| > epsilon)\n    # This is 2 * (1 - CDF(epsilon / std_dev))\n    arg = epsilon / std_dev_md_calibrated\n    p_values = 2 * (1 - norm.cdf(arg))\n    \n    triggers = np.sum(p_values >= alpha)\n\n    # Round and format output for the case\n    tau_rounded = round(tau, 6)\n    coverage_rounded = round(empirical_coverage, 6)\n    \n    return [tau_rounded, coverage_rounded, int(triggers)]\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}