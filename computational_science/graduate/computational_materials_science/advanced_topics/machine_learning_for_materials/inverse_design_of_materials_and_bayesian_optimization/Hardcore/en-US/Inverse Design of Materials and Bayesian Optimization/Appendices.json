{
    "hands_on_practices": [
        {
            "introduction": "Real-world materials design often involves combining imperfect physics-based simulators with sparse, noisy experimental data. This practice introduces the full end-to-end workflow for inverse design under uncertainty, starting from raw data and culminating in an optimal design recommendation. You will learn to use a Gaussian Process (GP) to formally model the discrepancy $\\delta(x)$ between a known simulator $f(x)$ and observed reality, creating a more accurate and robust hybrid model. By completing this exercise , you will bridge the gap between theoretical GP regression and its practical application in designing materials with specific target properties, a cornerstone of modern computational materials science.",
            "id": "3459016",
            "problem": "You are given a stochastic model for a materials property, where a deterministic process variable $x \\in [0,1]$ maps to a measured property $y$ through a deterministic physics-based simulator $f(x)$, an unknown model discrepancy $\\delta(x)$, and a stochastic measurement error $\\epsilon$. The data-generating relationship is $y = f(x) + \\delta(x) + \\epsilon$, where $\\epsilon$ is modeled as zero-mean Gaussian noise with known variance. The goal is to perform Bayesian calibration of the discrepancy $\\delta(x)$ using Gaussian Process (GP) regression and then perform inverse design by optimizing $x$ to achieve a target property value using the calibrated posterior.\n\nFundamental modeling assumptions to use are:\n- Bayes' theorem as the foundation for probabilistic inference and conditioning on observations.\n- A zero-mean Gaussian Process (GP) prior for the discrepancy $\\delta(x)$ with a squared-exponential kernel parameterized by an amplitude (variance) and a length-scale, and additive independent Gaussian noise with known variance.\n- The deterministic simulator $f(x)$ is known and fixed.\n\nLet the deterministic simulator be $f(x) = 0.6 \\cos(2 \\pi x) + 0.4 x$ for $x \\in [0,1]$. Treat the discrepancy $\\delta(x)$ as a zero-mean GP with squared-exponential covariance $k(x,x') = \\sigma_f^2 \\exp\\left(-\\frac{(x-x')^2}{2 \\ell^2}\\right)$, where $\\sigma_f^2$ is the process variance and $\\ell$ is the characteristic length-scale. Assume the observation noise variance $\\sigma_n^2$ is known for each case, and the observations are conditionally independent given the latent function.\n\nYour task is to:\n1. Calibrate the discrepancy $\\delta(x)$ from training data by operating on the residuals $r_i = y_i - f(x_i)$ using GP regression. Estimate the hyperparameters $\\sigma_f^2$ and $\\ell$ by maximizing the GP marginal log-likelihood under known $\\sigma_n^2$. Restrict the hyperparameters to $\\sigma_f^2 \\in [10^{-4}, 1]$ and $\\ell \\in [0.05, 1]$. Perform optimization in log-parameter space for numerical stability, and use a numerically stable linear algebra method with a small positive diagonal jitter if necessary.\n2. For any $x \\in [0,1]$, compute the posterior mean and variance of $y(x)$ under the calibrated model by appropriately combining the deterministic $f(x)$ and the GP posterior for $\\delta(x)$.\n3. For a given target property value $y^\\star$, define the inverse design objective as the expected squared deviation from $y^\\star$ under the posterior, that is, $J(x) = \\mathbb{E}[(y(x) - y^\\star)^2 \\mid \\text{data}]$. Find the design $x^\\star \\in [0,1]$ minimizing $J(x)$ over a uniform grid of $N=2001$ points in $[0,1]$ (i.e., grid spacing $0.0005$). If there are ties, select the smallest $x$.\n\nFrom first principles, use Bayesian decision theory and the normality of the GP posterior for $\\delta(x)$ to derive a computable expression for $J(x)$ in terms of the posterior mean and variance of $y(x)$, and implement it.\n\nFor each independent test case below, you must:\n- Use the given training inputs and outputs to calibrate the GP for $\\delta(x)$.\n- Compute the posterior mean $\\mu_y(x)$ and variance $\\sigma_y^2(x)$ of $y(x)$ on the grid.\n- Compute $J(x)$ on the grid and return the minimizer $x^\\star$ and the corresponding posterior mean $\\mu_y(x^\\star)$.\n\nHyperparameter estimation must use Maximum Likelihood Estimation (MLE) by maximizing the GP marginal likelihood with respect to $\\sigma_f^2$ and $\\ell$, subject to the bounds specified above. Use a numerically stable method for matrix factorizations.\n\nTest suite:\n- Case A:\n  - Training inputs $X_{\\text{train}} = [\\,0.1,\\,0.4,\\,0.6,\\,0.8\\,]$.\n  - Training outputs $Y_{\\text{train}} = [\\,0.57541019,\\,-0.3653731671,\\,-0.19004635764,\\,0.50077402636\\,]$.\n  - Known noise variance $\\sigma_n^2 = 0.0009$.\n  - Target property $y^\\star = 0.0$.\n- Case B:\n  - Training inputs $X_{\\text{train}} = [\\,0.15,\\,0.35,\\,0.55,\\,0.75,\\,0.95\\,]$.\n  - Training outputs $Y_{\\text{train}} = [\\,0.4626711514,\\,-0.2526697054,\\,-0.3690141424,\\,0.45364023493,\\,0.80121304604\\,]$.\n  - Known noise variance $\\sigma_n^2 = 0.0025$.\n  - Target property $y^\\star = 0.2$.\n- Case C:\n  - Training inputs $X_{\\text{train}} = [\\,0.2,\\,0.5,\\,0.9\\,]$.\n  - Training outputs $Y_{\\text{train}} = [\\,0.285410194,\\,-0.4345053083334,\\,0.7309048856666\\,]$.\n  - Known noise variance $\\sigma_n^2 = 10^{-12}$.\n  - Target property $y^\\star = 0.6$.\n\nAngle units are not applicable. No additional physical units are required beyond the dimensionless quantities shown.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results for the three cases as a comma-separated list of lists, with no spaces, where each inner list contains the optimal $x^\\star$ and the corresponding posterior mean $\\mu_y(x^\\star)$ rounded to six decimal places. For example, your output must look like $[[x_1^\\star,\\mu_1],[x_2^\\star,\\mu_2],[x_3^\\star,\\mu_3]]$, where each value is rounded to six decimal places and there are no spaces anywhere in the line.\n\nYour program must be self-contained, not read any external input, and implement the calibration and optimization steps described above using only the specified numerical libraries. The optimization over the hyperparameters must be performed by maximizing the marginal likelihood as specified, and the optimization over $x$ must be performed over the uniform grid of $N=2001$ points as specified.",
            "solution": "The problem requires the development of a computational framework for Bayesian calibration and inverse design of a materials property. The solution involves three primary stages: (1) formulating the probabilistic model and the objective for inverse design, (2) calibrating the model by estimating hyperparameters from data, and (3) optimizing the design variable to achieve a target property.\n\n**1. Probabilistic Model Formulation**\n\nThe relationship between the process variable $x \\in [0,1]$ and the measured property $y$ is given by the stochastic model:\n$$y = f(x) + \\delta(x) + \\epsilon$$\nHere, $f(x) = 0.6 \\cos(2 \\pi x) + 0.4 x$ is the known deterministic physics-based simulator. The term $\\delta(x)$ represents a systematic model discrepancy, and $\\epsilon$ is a random measurement error. We model $\\epsilon$ as independent and identically distributed Gaussian noise, $\\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2)$, with a known variance $\\sigma_n^2$.\n\nThe core of the calibration task is to infer the unknown discrepancy function, $\\delta(x)$, from data. We adopt a Bayesian approach by placing a Gaussian Process (GP) prior on $\\delta(x)$. A GP is a distribution over functions, and we assume a zero-mean prior:\n$$\\delta(x) \\sim \\mathcal{GP}(0, k(x, x'))$$\nThe covariance function, or kernel, $k(x, x')$, defines the properties of the functions drawn from the GP. We use the squared-exponential kernel:\n$$k(x,x') = \\sigma_f^2 \\exp\\left(-\\frac{(x-x')^2}{2 \\ell^2}\\right)$$\nThis kernel is parameterized by the amplitude (process variance) $\\sigma_f^2$ and the characteristic length-scale $\\ell$. These are the hyperparameters of our GP model.\n\n**2. Bayesian Calibration via Maximum Likelihood Estimation**\n\nGiven a set of training data $\\mathcal{D} = \\{ (x_i, y_i) \\}_{i=1}^n$, we first compute the residuals $r_i = y_i - f(x_i)$. According to our model, these residuals are samples of the discrepancy plus noise: $r_i = \\delta(x_i) + \\epsilon_i$. Let $\\mathbf{r} = [r_1, \\dots, r_n]^T$ be the vector of residuals and $\\mathbf{X}_{\\text{train}} = [x_1, \\dots, x_n]^T$ be the vector of training inputs. The vector $\\mathbf{r}$ is drawn from a multivariate Gaussian distribution:\n$$\\mathbf{r} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{K} + \\sigma_n^2 \\mathbf{I})$$\nwhere $\\mathbf{K}$ is the $n \\times n$ kernel matrix with entries $K_{ij} = k(x_i, x_j)$ and $\\mathbf{I}$ is the identity matrix.\n\nThe hyperparameters $\\theta = \\{\\sigma_f^2, \\ell\\}$ are unknown. We estimate them by maximizing the marginal log-likelihood of the observed residuals. The log-likelihood function is:\n$$\\log p(\\mathbf{r} \\mid \\mathbf{X}_{\\text{train}}, \\theta) = -\\frac{1}{2} \\mathbf{r}^T (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{r} - \\frac{1}{2} \\log |\\mathbf{K} + \\sigma_n^2 \\mathbf{I}| - \\frac{n}{2} \\log(2\\pi)$$\nTo find the optimal hyperparameters $\\theta^\\star$, we maximize this function subject to the constraints $\\sigma_f^2 \\in [10^{-4}, 1]$ and $\\ell \\in [0.05, 1]$. For numerical stability, this optimization is performed on the logarithm of the parameters, i.e., finding $\\arg\\max [\\log p]$ over $\\log(\\sigma_f^2)$ and $\\log(\\ell)$. The computation of the inverse and determinant is performed efficiently and stably using the Cholesky decomposition of the covariance matrix $\\mathbf{K}_{yy} = \\mathbf{K} + \\sigma_n^2 \\mathbf{I}$. A small jitter term is added to the diagonal of $\\mathbf{K}_{yy}$ to ensure it is positive definite.\n\n**3. Posterior Prediction and Inverse Design**\n\nWith the optimized hyperparameters $\\theta^\\star$, we can compute the posterior distribution of the discrepancy $\\delta(x_*)$ at any new test point $x_*$. The posterior $p(\\delta(x_*) \\mid \\mathcal{D})$ is also Gaussian, with mean $\\mu_\\delta(x_*)$ and variance $\\sigma_\\delta^2(x_*)$ given by:\n$$\\mu_\\delta(x_*) = \\mathbf{k}_*^T (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{r}$$\n$$\\sigma_\\delta^2(x_*) = k(x_*, x_*) - \\mathbf{k}_*^T (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*$$\nwhere $\\mathbf{k}_*$ is a vector with elements $k(x_i, x_*)$ for $i=1, \\dots, n$.\n\nThe property of interest for inverse design is the latent function $y(x) = f(x) + \\delta(x)$. Since $f(x)$ is deterministic, the posterior distribution for $y(x_*)$ is a shifted version of the posterior for $\\delta(x_*)$:\n$$p(y(x_*) \\mid \\mathcal{D}) \\sim \\mathcal{N}(\\mu_y(x_*), \\sigma_y^2(x_*))$$\nwhere the posterior mean and variance are:\n$$\\mu_y(x_*) = f(x_*) + \\mu_\\delta(x_*)$$\n$$\\sigma_y^2(x_*) = \\sigma_\\delta^2(x_*)$$\n\nThe inverse design objective is to find the design $x^\\star$ that minimizes the expected squared deviation from a target value $y^\\star$, where the expectation is taken over the posterior distribution of $y(x)$:\n$$J(x) = \\mathbb{E}[(y(x) - y^\\star)^2 \\mid \\mathcal{D}]$$\nUsing the law of total expectation, for a random variable $Z$ with mean $\\mu$ and variance $\\sigma^2$, we have $\\mathbb{E}[(Z-c)^2] = \\text{Var}(Z) + (\\mathbb{E}[Z]-c)^2$. Applying this to our posterior for $y(x)$, we obtain a computable expression for the objective function:\n$$J(x) = \\sigma_y^2(x) + (\\mu_y(x) - y^\\star)^2$$\nThis objective function judiciously balances two goals: driving the posterior mean $\\mu_y(x)$ towards the target $y^\\star$ (exploitation) and reducing the posterior variance $\\sigma_y^2(x)$ by designing in regions of high certainty (exploration).\n\n**4. Computational Procedure**\n\nThe overall algorithm proceeds as follows for each test case:\n1.  Compute the training residuals $\\mathbf{r} = \\mathbf{Y}_{\\text{train}} - f(\\mathbf{X}_{\\text{train}})$.\n2.  Define the negative marginal log-likelihood as the objective function for hyperparameter optimization.\n3.  Use a numerical optimizer (`L-BFGS-B`) to find the optimal $\\log(\\sigma_f^2)$ and $\\log(\\ell)$ that minimize this objective, within the specified bounds.\n4.  Establish a uniform grid of $N=2001$ test points $x_j$ in $[0,1]$.\n5.  Using the optimized hyperparameters, calculate the posterior mean $\\mu_y(x_j)$ and variance $\\sigma_y^2(x_j)$ for all points on the grid.\n6.  Evaluate the inverse design objective $J(x_j) = \\sigma_y^2(x_j) + (\\mu_y(x_j) - y^\\star)^2$ across the grid.\n7.  Identify the grid point $x^\\star$ that corresponds to the minimum value of $J(x)$. If multiple points yield the same minimum value, the one with the smallest $x$ is chosen as per the problem specification.\n8.  The final result for the case is the optimal design $x^\\star$ and its corresponding posterior mean property value $\\mu_y(x^\\star)$.\nThis procedure is repeated for all provided test cases.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import cho_solve, solve_triangular\n\ndef solve():\n    \"\"\"\n    Main function to solve the Bayesian calibration and inverse design problem\n    for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"X_train\": np.array([0.1, 0.4, 0.6, 0.8]),\n            \"Y_train\": np.array([0.57541019, -0.3653731671, -0.19004635764, 0.50077402636]),\n            \"sigma_n_sq\": 0.0009,\n            \"y_star\": 0.0,\n        },\n        {\n            \"X_train\": np.array([0.15, 0.35, 0.55, 0.75, 0.95]),\n            \"Y_train\": np.array([0.4626711514, -0.2526697054, -0.3690141424, 0.45364023493, 0.80121304604]),\n            \"sigma_n_sq\": 0.0025,\n            \"y_star\": 0.2,\n        },\n        {\n            \"X_train\": np.array([0.2, 0.5, 0.9]),\n            \"Y_train\": np.array([0.285410194, -0.4345053083334, 0.7309048856666]),\n            \"sigma_n_sq\": 1e-12,\n            \"y_star\": 0.6,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        x_opt, mu_y_opt = process_case(\n            case[\"X_train\"],\n            case[\"Y_train\"],\n            case[\"sigma_n_sq\"],\n            case[\"y_star\"]\n        )\n        results.append([x_opt, mu_y_opt])\n    \n    # Format the final output string\n    result_str = \",\".join([f\"[{val[0]:.6f},{val[1]:.6f}]\" for val in results])\n    print(f\"[{result_str}]\")\n\ndef f(x):\n    \"\"\"The deterministic simulator f(x)\"\"\"\n    return 0.6 * np.cos(2 * np.pi * x) + 0.4 * x\n\ndef kernel(x1, x2, sigma_f_sq, l):\n    \"\"\"Squared-exponential kernel\"\"\"\n    # Using broadcasting to compute squared Euclidean distances\n    sq_dist = (x1.reshape(-1, 1) - x2.reshape(1, -1))**2\n    return sigma_f_sq * np.exp(-sq_dist / (2 * l**2))\n\ndef neg_log_likelihood(log_params, X, r, sigma_n_sq):\n    \"\"\"Negative marginal log-likelihood of the GP.\"\"\"\n    log_sigma_f_sq, log_l = log_params\n    sigma_f_sq = np.exp(log_sigma_f_sq)\n    l = np.exp(log_l)\n    \n    n = len(X)\n    jitter = 1e-8\n\n    K = kernel(X, X, sigma_f_sq, l)\n    K_yy = K + np.eye(n) * (sigma_n_sq + jitter)\n\n    try:\n        L = np.linalg.cholesky(K_yy)\n    except np.linalg.LinAlgError:\n        return np.inf\n\n    alpha = cho_solve((L, True), r)\n    log_det_K_yy = 2 * np.sum(np.log(np.diag(L)))\n    \n    nll = 0.5 * r.T @ alpha + 0.5 * log_det_K_yy + 0.5 * n * np.log(2 * np.pi)\n    return nll\n\ndef process_case(X_train, Y_train, sigma_n_sq, y_star):\n    \"\"\"\n    Processes a single test case: calibrates GP and performs inverse design.\n    \"\"\"\n    # 1. Compute residuals\n    r_train = Y_train - f(X_train)\n\n    # 2. Hyperparameter optimization\n    log_sigma_f_sq_bounds = (np.log(1e-4), np.log(1.0))\n    log_l_bounds = (np.log(0.05), np.log(1.0))\n    bounds = [log_sigma_f_sq_bounds, log_l_bounds]\n    \n    # Initial guess: center of the log-space hyperparameter box\n    x0 = [np.mean(b) for b in bounds]\n    \n    opt_result = minimize(\n        neg_log_likelihood,\n        x0=x0,\n        args=(X_train, r_train, sigma_n_sq),\n        method='L-BFGS-B',\n        bounds=bounds\n    )\n    \n    opt_log_sigma_f_sq, opt_log_l = opt_result.x\n    opt_sigma_f_sq = np.exp(opt_log_sigma_f_sq)\n    opt_l = np.exp(opt_log_l)\n\n    # 3. Posterior Prediction on the grid\n    N_grid = 2001\n    x_grid = np.linspace(0, 1, N_grid)\n\n    # Pre-compute matrices for prediction\n    jitter = 1e-8\n    K_train = kernel(X_train, X_train, opt_sigma_f_sq, opt_l)\n    K_yy = K_train + np.eye(len(X_train)) * (sigma_n_sq + jitter)\n    L = np.linalg.cholesky(K_yy)\n    alpha = cho_solve((L, True), r_train)\n\n    # Predict at grid points\n    k_star = kernel(X_train, x_grid, opt_sigma_f_sq, opt_l)\n    \n    # Posterior mean for delta(x)\n    mu_delta_grid = k_star.T @ alpha\n    \n    # Posterior variance for delta(x)\n    v = solve_triangular(L, k_star, lower=True)\n    var_delta_grid = opt_sigma_f_sq - np.sum(v**2, axis=0)\n    \n    # Posterior for y(x) = f(x) + delta(x)\n    mu_y_grid = f(x_grid) + mu_delta_grid\n    var_y_grid = var_delta_grid\n\n    # 4. Inverse Design Optimization\n    # Objective function J(x) = E[(y(x) - y_star)^2]\n    J_grid = var_y_grid + (mu_y_grid - y_star)**2\n    \n    # Find minimizer\n    idx_min = np.argmin(J_grid)\n    x_star = x_grid[idx_min]\n    mu_y_at_x_star = mu_y_grid[idx_min]\n    \n    return x_star, mu_y_at_x_star\n\nif __name__ == '__main__':\n    solve()\n\n```"
        },
        {
            "introduction": "Once a surrogate model is in place, the next challenge is to improve it efficiently, as acquiring new data from simulations or experiments is often the primary bottleneck. This practice zooms in on this critical active learning step, focusing on how to intelligently select the next experiment when you have multiple options with different costs and fidelities (e.g., a fast, noisy Molecular Dynamics simulation versus a slow, precise DFT calculation). You will implement the knowledge gradient (KG), a powerful acquisition function that quantifies the expected value of information from a potential measurement, to make a principled, cost-aware decision . Mastering this concept is vital for navigating complex design spaces where computational or experimental budgets are a major constraint.",
            "id": "3459019",
            "problem": "You are designing an information-efficient one-step decision rule for inverse design under a Gaussian posterior model on a finite set of candidate compositions. You have two information sources: Molecular Dynamics (MD) and Density Functional Theory (DFT), which differ by their observational noise and computational cost. At a single specified candidate index, you must compute the knowledge gradient, defined as the expected single-step increase in the maximum posterior mean objective value across all candidates, and then normalize by the cost of the selected source to obtain the expected value per unit cost. You must then choose the source that maximizes expected value per unit cost.\n\nStart from the following fundamental base in Bayesian linear-Gaussian inference and finite-horizon decision making:\n- The posterior of a Gaussian process on a finite set of candidates is described by a mean vector $\\,\\boldsymbol{\\mu} \\in \\mathbb{R}^n\\,$ and a covariance matrix $\\,\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{n \\times n}\\,$.\n- A noisy observation at candidate index $\\,q\\,$ from source $\\,s \\in \\{\\text{MD}, \\text{DFT}\\}\\,$ is modeled as $\\,y = f_q + \\varepsilon_s\\,$ with $\\,\\varepsilon_s \\sim \\mathcal{N}(0,\\tau_s^2)\\,$, where $\\,\\tau_s^2\\,$ is the observation noise variance for source $\\,s\\,$.\n- Gaussian conditioning gives an updated posterior mean after observing $\\,y\\,$ at index $\\,q\\,$:\n$$\n\\boldsymbol{\\mu}^+ = \\boldsymbol{\\mu} + \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2}\\,\\bigl(y - \\mu_q\\bigr),\n$$\nand the updated covariance is not required for the knowledge gradient calculation below.\n- The knowledge gradient for a one-step measurement at index $\\,q\\,$ using source $\\,s\\,$ is the expected increase in the value of the best design under the updated posterior mean, minus the current best value:\n$$\n\\mathrm{KG}(q,s) \\equiv \\mathbb{E}\\Bigl[\\max_{i \\in \\{1,\\dots,n\\}} \\mu_i^+ \\Bigr] - \\max_{i \\in \\{1,\\dots,n\\}} \\mu_i.\n$$\n- The expected value per unit cost is then $\\,\\mathrm{KG}(q,s) / c_s\\,$, where $\\,c_s\\,$ is the cost of source $\\,s\\,$.\n\nYour task is to implement a deterministic algorithm that, for each test case provided below, computes $\\,\\mathrm{KG}(q,\\text{MD})/c_{\\text{MD}}\\,$ and $\\,\\mathrm{KG}(q,\\text{DFT})/c_{\\text{DFT}}\\,$ and then selects the source with the larger expected value per unit cost. If the two values are equal within a tolerance of $\\,10^{-12}\\,$, you must choose Molecular Dynamics (MD).\n\nYou must implement the knowledge gradient using first principles. Namely, use the facts above to express the updated mean $\\,\\boldsymbol{\\mu}^+\\,$ as an affine function of the scalar observation, and then compute the expectation of the maximum of affine functions of a standard normal random variable in closed form via integration against the standard normal density over the breakpoints where the pointwise maximum changes identity. Your implementation must be fully deterministic and must not use random sampling.\n\nTest suite:\n- Case A:\n  - Number of candidates $\\,n = 3\\,$.\n  - Posterior mean\n    $$\n    \\boldsymbol{\\mu}^{(A)} = \\begin{bmatrix} 0.5 & 0.6 & 0.4 \\end{bmatrix}.\n    $$\n  - Posterior covariance\n    $$\n    \\boldsymbol{\\Sigma}^{(A)} = \\begin{bmatrix}\n    0.16 & 0.04 & 0.08 \\\\\n    0.04 & 0.26 & 0.07 \\\\\n    0.08 & 0.07 & 0.41\n    \\end{bmatrix}.\n    $$\n  - Measurement index $\\,q^{(A)} = 2\\,$ (using one-based human indexing, this corresponds to the second entry; your code should use zero-based indexing internally).\n  - MD noise variance $\\,\\tau_{\\text{MD}}^{2\\,(A)} = 0.2^2\\,$, DFT noise variance $\\,\\tau_{\\text{DFT}}^{2\\,(A)} = 0.05^2\\,$.\n  - MD cost $\\,c_{\\text{MD}}^{(A)} = 1.0\\,$, DFT cost $\\,c_{\\text{DFT}}^{(A)} = 12.0\\,$.\n\n- Case B:\n  - Number of candidates $\\,n = 3\\,$.\n  - Posterior mean\n    $$\n    \\boldsymbol{\\mu}^{(B)} = \\begin{bmatrix} 0.7 & 0.65 & 0.66 \\end{bmatrix}.\n    $$\n  - Posterior covariance\n    $$\n    \\boldsymbol{\\Sigma}^{(B)} = \\begin{bmatrix}\n    0.0 & 0.0 & 0.0 \\\\\n    0.0 & 0.2 & 0.0 \\\\\n    0.0 & 0.0 & 0.1\n    \\end{bmatrix}.\n    $$\n  - Measurement index $\\,q^{(B)} = 1\\,$ (first entry in one-based indexing).\n  - MD noise variance $\\,\\tau_{\\text{MD}}^{2\\,(B)} = 0.3^2\\,$, DFT noise variance $\\,\\tau_{\\text{DFT}}^{2\\,(B)} = 0.05^2\\,$.\n  - MD cost $\\,c_{\\text{MD}}^{(B)} = 1.0\\,$, DFT cost $\\,c_{\\text{DFT}}^{(B)} = 10.0\\,$.\n\n- Case C:\n  - Number of candidates $\\,n = 4\\,$.\n  - Posterior mean\n    $$\n    \\boldsymbol{\\mu}^{(C)} = \\begin{bmatrix} 1.0 & 0.9 & 0.95 & 0.8 \\end{bmatrix}.\n    $$\n  - Posterior covariance (symmetric positive definite)\n    $$\n    \\boldsymbol{\\Sigma}^{(C)} = \\begin{bmatrix}\n    0.36 & -0.12 & 0.06 & 0.00 \\\\\n    -0.12 & 0.53 & -0.23 & -0.14 \\\\\n    0.06 & -0.23 & 0.35 & 0.11 \\\\\n    0.00 & -0.14 & 0.11 & 0.21\n    \\end{bmatrix}.\n    $$\n  - Measurement index $\\,q^{(C)} = 3\\,$ (third entry in one-based indexing).\n  - MD noise variance $\\,\\tau_{\\text{MD}}^{2\\,(C)} = 0.3^2\\,$, DFT noise variance $\\,\\tau_{\\text{DFT}}^{2\\,(C)} = 0.1^2\\,$.\n  - MD cost $\\,c_{\\text{MD}}^{(C)} = 1.0\\,$, DFT cost $\\,c_{\\text{DFT}}^{(C)} = 3.0\\,$.\n\nNumerical and output requirements:\n- For each test case, compute $\\,\\mathrm{KG}(q,\\text{MD})/c_{\\text{MD}}\\,$ and $\\,\\mathrm{KG}(q,\\text{DFT})/c_{\\text{DFT}}\\,$ exactly as defined above.\n- All intermediate calculations are unitless and use real numbers.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, append three values in order: the MD expected value per unit cost, the DFT expected value per unit cost, and the chosen source as an integer ($0$ for MD and $1$ for DFT). All floating-point values must be rounded to six decimal places. For example, the overall output format must be like $[\\text{md}_A,\\text{dft}_A,\\text{choice}_A,\\text{md}_B,\\text{dft}_B,\\text{choice}_B,\\text{md}_C,\\text{dft}_C,\\text{choice}_C]$ with the specified rounding.",
            "solution": "The problem requires selecting the most cost-effective measurement source (MD or DFT) for a single candidate material in a Bayesian optimization context. The decision is based on maximizing the **Knowledge Gradient (KG) per unit cost**. This involves a closed-form, deterministic calculation of the KG.\n\n### 1. Formulating the Updated Posterior Mean\n\nThe knowledge gradient quantifies the expected improvement in the maximum value of our posterior belief after making a new observation. The first step is to model how the posterior mean vector $\\boldsymbol{\\mu}$ updates after an observation $y$ is made at candidate index $q$ using source $s$. The update rule is given as:\n$$\n\\boldsymbol{\\mu}^+ = \\boldsymbol{\\mu} + \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2}\\,\\bigl(y - \\mu_q\\bigr)\n$$\nThe observation $y$ itself is a random variable, drawn from our current predictive distribution at point $q$: $y \\sim \\mathcal{N}(\\mu_q, \\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2)$. We can express the random deviation from the mean using a standard normal variable $Z \\sim \\mathcal{N}(0,1)$:\n$$\ny - \\mu_q = Z \\cdot \\sqrt{\\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2}\n$$\nSubstituting this into the update rule gives the updated mean vector $\\boldsymbol{\\mu}^+$ as a function of the standard normal variable $Z$:\n$$\n\\boldsymbol{\\mu}^+(Z) = \\boldsymbol{\\mu} + \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\sqrt{\\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2}} Z\n$$\nFor each candidate $i$, the updated mean $\\mu_i^+$ is an affine function of $Z$:\n$$\n\\mu_i^+(Z) = a_i + b_i Z\n$$\nwhere the intercepts are the current means, $a_i = \\mu_i$, and the slopes are given by the vector $\\boldsymbol{b} = \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\sqrt{\\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2}}$.\n\n### 2. Calculating the Knowledge Gradient\n\nThe Knowledge Gradient is defined as the expected increase in the maximum posterior mean:\n$$\n\\mathrm{KG}(q,s) = \\mathbb{E}_Z\\left[\\max_{i} \\mu_i^+(Z) \\right] - \\max_{i} \\mu_i\n$$\nThe core challenge is computing the expectation term, $\\mathbb{E}_Z[\\max_i(a_i + b_i Z)]$. The function $g(Z) = \\max_i(a_i + b_i Z)$ is the upper envelope of a set of lines, making it a piecewise linear and convex function. The points where the identity of the maximizing line changes are called breakpoints. For any two lines $i$ and $j$ with different slopes ($b_i \\neq b_j$), the breakpoint $z_{ij}$ occurs where $a_i + b_i z = a_j + b_j z$, which gives:\n$$\nz_{ij} = \\frac{a_j - a_i}{b_i - b_j}\n$$\nThe set of all unique, sorted breakpoints $\\{\\zeta_k\\}$ partitions the real line into intervals. Within each interval $(\\zeta_k, \\zeta_{k+1})$, a single line $a_{i^*} + b_{i^*}Z$ is maximal. The expectation can thus be computed by summing the integrals over these intervals:\n$$\n\\mathbb{E}_Z[g(Z)] = \\sum_{k} \\int_{\\zeta_k}^{\\zeta_{k+1}} (a_{i_k^*} + b_{i_k^*} Z) \\phi(Z) \\, dZ\n$$\nwhere $\\phi(Z)$ is the standard normal PDF.\n\n### 3. Analytical Integration\n\nThe integral over each segment can be solved analytically using the properties of the standard normal PDF $\\phi(Z)$ and CDF $\\Phi(Z)$:\n$$\n\\int_{L}^{U} (a + bZ) \\phi(Z) \\, dZ = a \\int_{L}^{U} \\phi(Z) \\, dZ + b \\int_{L}^{U} Z \\phi(Z) \\, dZ\n$$\n$$\n= a[\\Phi(U) - \\Phi(L)] + b[\\phi(L) - \\phi(U)]\n$$\nBy summing these contributions over all intervals defined by the breakpoints (from $-\\infty$ to $+\\infty$), we obtain the exact value for $\\mathbb{E}_Z[\\max_i \\mu_i^+(Z)]$.\n\n### 4. Decision Rule\n\nThe full algorithm for a given source $s$ is:\n1.  Calculate the slope vector $\\boldsymbol{b}$ and intercept vector $\\boldsymbol{a} = \\boldsymbol{\\mu}$.\n2.  If the measurement is uninformative (e.g., the corresponding column in $\\boldsymbol{\\Sigma}$ is zero), the slopes are all zero, KG is zero, and the value is zero.\n3.  Compute all unique breakpoints $z_{ij}$ and sort them to define the integration intervals.\n4.  For each interval, identify the maximizing line and compute the analytical integral's contribution to the total expectation.\n5.  Sum the contributions to get $\\mathbb{E}_Z[\\max_i \\mu_i^+]$.\n6.  Calculate $\\mathrm{KG}(q,s) = \\mathbb{E}_Z[\\max_i \\mu_i^+] - \\max_i \\mu_i$.\n7.  Calculate the final metric: $\\mathrm{KG}(q,s) / c_s$.\n\nThis procedure is performed for both MD and DFT. The source with the higher metric is chosen, with MD being the tie-breaker as specified. This deterministic approach provides an exact evaluation of the one-step value of information.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef calculate_kg_per_cost(mu, Sigma, q_one_based, tau2, cost):\n    \"\"\"\n    Computes the knowledge gradient per unit cost for a measurement.\n    \"\"\"\n    if cost = 0:\n        return -np.inf\n\n    n = len(mu)\n    q_idx = q_one_based - 1\n\n    # Define intercepts `a` and slopes `b` for the affine functions of Z.\n    a = mu\n    \n    # The effective variance of the observation's predictive distribution.\n    var_predictive = Sigma[q_idx, q_idx] + tau2\n\n    # If the measurement is completely uninformative or the variance is zero.\n    sigma_q_col = Sigma[:, q_idx]\n    if var_predictive = 1e-15 or np.all(np.abs(sigma_q_col)  1e-15):\n        return 0.0\n\n    b = sigma_q_col / np.sqrt(var_predictive)\n\n    # Compute all unique pairwise breakpoints.\n    breakpoints = set()\n    for i in range(n):\n        for j in range(i + 1, n):\n            if np.abs(b[i] - b[j]) > 1e-15:\n                z = (a[j] - a[i]) / (b[i] - b[j])\n                breakpoints.add(z)\n    \n    sorted_breakpoints = sorted(list(breakpoints))\n    \n    # Define the integration intervals using the breakpoints.\n    zeta_points = [-np.inf] + sorted_breakpoints + [np.inf]\n    \n    # Calculate the expected maximum of the updated mean by integrating.\n    expected_max_mu_plus = 0.0\n    for k in range(len(zeta_points) - 1):\n        z_low = zeta_points[k]\n        z_high = zeta_points[k+1]\n        \n        # Select a test point to find the maximizing line in the interval.\n        if np.isneginf(z_low):\n            test_z = z_high - 1.0\n        elif np.isposinf(z_high):\n            test_z = z_low + 1.0\n        else:\n            test_z = (z_low + z_high) / 2.0\n            \n        # Determine a_star and b_star for the maximizing line.\n        line_values = a + b * test_z\n        i_star = np.argmax(line_values)\n        a_star, b_star = a[i_star], b[i_star]\n        \n        # Analytically compute the integral of (a* + b*Z)phi(Z) over [z_low, z_high].\n        cdf_high = norm.cdf(z_high)\n        cdf_low = norm.cdf(z_low)\n        pdf_high = norm.pdf(z_high)\n        pdf_low = norm.pdf(z_low)\n        \n        # Handle -inf and +inf boundaries.\n        if np.isneginf(z_low): cdf_low, pdf_low = 0.0, 0.0\n        if np.isposinf(z_high): cdf_high, pdf_high = 1.0, 0.0\n            \n        term_a = a_star * (cdf_high - cdf_low)\n        term_b = b_star * (pdf_low - pdf_high)\n        \n        expected_max_mu_plus += term_a + term_b\n\n    # Compute the knowledge gradient and normalize by cost.\n    kg = expected_max_mu_plus - np.max(mu)\n    \n    return kg / cost\n\ndef solve():\n    \"\"\"\n    Main solver function that processes all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            # Case A\n            \"mu\": np.array([0.5, 0.6, 0.4]),\n            \"Sigma\": np.array([\n                [0.16, 0.04, 0.08],\n                [0.04, 0.26, 0.07],\n                [0.08, 0.07, 0.41]\n            ]),\n            \"q\": 2,\n            \"md\": {\"tau2\": 0.2**2, \"cost\": 1.0},\n            \"dft\": {\"tau2\": 0.05**2, \"cost\": 12.0}\n        },\n        {\n            # Case B\n            \"mu\": np.array([0.7, 0.65, 0.66]),\n            \"Sigma\": np.array([\n                [0.0, 0.0, 0.0],\n                [0.0, 0.2, 0.0],\n                [0.0, 0.0, 0.1]\n            ]),\n            \"q\": 1,\n            \"md\": {\"tau2\": 0.3**2, \"cost\": 1.0},\n            \"dft\": {\"tau2\": 0.05**2, \"cost\": 10.0}\n        },\n        {\n            # Case C\n            \"mu\": np.array([1.0, 0.9, 0.95, 0.8]),\n            \"Sigma\": np.array([\n                [0.36, -0.12, 0.06, 0.00],\n                [-0.12, 0.53, -0.23, -0.14],\n                [0.06, -0.23, 0.35, 0.11],\n                [0.00, -0.14, 0.11, 0.21]\n            ]),\n            \"q\": 3,\n            \"md\": {\"tau2\": 0.3**2, \"cost\": 1.0},\n            \"dft\": {\"tau2\": 0.1**2, \"cost\": 3.0}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        mu = case[\"mu\"]\n        Sigma = case[\"Sigma\"]\n        q = case[\"q\"]\n\n        # Calculate KG per unit cost for MD\n        val_md = calculate_kg_per_cost(mu, Sigma, q, case[\"md\"][\"tau2\"], case[\"md\"][\"cost\"])\n        \n        # Calculate KG per unit cost for DFT\n        val_dft = calculate_kg_per_cost(mu, Sigma, q, case[\"dft\"][\"tau2\"], case[\"dft\"][\"cost\"])\n\n        # Decide which source to use based on the specified tolerance and tie-breaking rule\n        # If val_md >= val_dft - 1e-12, choose MD.\n        if val_dft - val_md = 1e-12:\n            choice = 0  # MD\n        else:\n            choice = 1  # DFT\n        \n        results.append(f\"{val_md:.6f}\")\n        results.append(f\"{val_dft:.6f}\")\n        results.append(str(choice))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}