{
    "hands_on_practices": [
        {
            "introduction": "A foundational requirement for a Markov chain Monte Carlo simulation to be valid is ergodicityâ€”the principle that the sampler must, in principle, be able to reach any relevant configuration from any other. When this property, known as irreducibility, is violated, the simulation becomes confined to a subset of the state space and converges to an incorrect, conditional distribution. This exercise provides a concrete, cautionary tale by analyzing a Metropolis-Hastings sampler with a flawed proposal mechanism on a simple Ising model, demonstrating exactly how and why such a chain fails to sample the intended target distribution. ",
            "id": "3463572",
            "problem": "Consider a one-dimensional periodic Ising lattice with $N=4$ sites modeling a binary alloy on a ring, with spin variables $s_i \\in \\{-1,+1\\}$ representing local species states at lattice site $i$. The energy of a configuration $\\mathbf{s} = (s_1,s_2,s_3,s_4)$ is given by the nearest-neighbor Ising Hamiltonian with an external field,\n$$\nE(\\mathbf{s}) = - J \\left( s_1 s_2 + s_2 s_3 + s_3 s_4 + s_4 s_1 \\right) - h \\left( s_1 + s_2 + s_3 + s_4 \\right),\n$$\nwhere $J$ and $h$ are material-dependent interaction and field parameters, respectively, and $\\beta$ is the inverse thermal energy with $\\beta = 1/(k_B T)$ for Boltzmann constant $k_B$ and temperature $T$. The target distribution for sampling equilibrium configurations is the Boltzmann distribution\n$$\n\\pi(\\mathbf{s}) \\propto \\exp\\!\\left( - \\beta E(\\mathbf{s}) \\right).\n$$\nA Markov chain Monte Carlo (MCMC) method with the Metropolis-Hastings (MH) algorithm is used to sample from $\\pi(\\mathbf{s})$. The proposal kernel is defined as follows: at each step, one of the even-indexed sites from the set $\\{2,4\\}$ is chosen uniformly at random and its spin is flipped, while odd-indexed spins are never proposed to change. This proposal respects detailed balance locally but violates irreducibility on the full configuration space.\n\nStarting from the initial configuration with odd spins fixed as $s_1 = +1$ and $s_3 = -1$, perform the following tasks:\n\n- Using the definitions of irreducibility and detailed balance for Markov chains, explain why this proposal induces closed communicating classes of states and identify the closed class determined by the odd-spin constraint $s_1 = +1$ and $s_3 = -1$.\n- Derive the stationary distribution supported on this closed class that the MH chain converges to, expressed as the Boltzmann distribution conditioned on the fixed odd spins.\n- Compute the exact fraction of the target Boltzmann probability mass that is unreachable by this chain starting from the given odd-spin constraint, i.e., one minus the total conditioned probability mass of the closed class under the target $\\pi(\\mathbf{s})$.\n\nYour final answer must be a single closed-form analytic expression in terms of $\\beta$, $J$, and $h$. This quantity is dimensionless; express your answer without units. Do not approximate; no rounding is required.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. It is a standard exercise in statistical mechanics and Markov chain Monte Carlo methods, examining the consequences of a reducible Markov chain. Therefore, the problem is deemed valid and a full solution follows.\n\nThe problem asks for three related tasks concerning a Metropolis-Hastings (MH) algorithm with a non-standard proposal mechanism applied to a 1D periodic Ising model with $N=4$ sites. We will address each task in sequence.\n\nFirst, we analyze the structure of the Markov chain induced by the proposal kernel. The state space of the system is the set of all possible spin configurations $\\mathbf{s}=(s_1,s_2,s_3,s_4)$, where $s_i \\in \\{-1, +1\\}$. This gives a total of $2^4 = 16$ possible states. The proposal rule states that at each step, a site from the set $\\{2, 4\\}$ is chosen uniformly at random and its spin is flipped. The spins at odd-indexed sites, $s_1$ and $s_3$, are never proposed to change.\n\nA Markov chain is **irreducible** if it is possible to go from any state to any other state (not necessarily in one step). The given proposal mechanism violates this property. If a chain starts in a state with a specific pair of values for $(s_1, s_3)$, say $(s_1^*, s_3^*)$, any subsequent state generated by the chain will also have its odd-indexed spins fixed to $(s_1^*, s_3^*)$. It is impossible to transition to a state with a different pair of odd-indexed spins, for example $(s_1', s_3')$ where $(s_1', s_3') \\neq (s_1^*, s_3^*)$.\n\nThis restriction partitions the total state space $\\mathcal{S}$ into four disjoint subsets, each corresponding to one of the possible pairs of values for $(s_1, s_3)$: $\\{(+1, +1), (+1, -1), (-1, +1), (-1, -1)\\}$. Within each such subset, any state can be reached from any other state. For example, within the subset where $s_1$ and $s_3$ are fixed, any configuration of $(s_2, s_4)$ can be reached from any other. A transition from $(s_2, s_4)$ to $(-s_2, s_4)$ is possible by flipping $s_2$, and a transition to $(-s_2, -s_4)$ is possible by first flipping $s_2$ and then $s_4$. These transitions are accepted with some non-zero probability by the MH rule. Therefore, all states within such a subset form a **communicating class**. Since no transitions are possible between these classes, each class is **closed**.\n\nThe problem specifies an initial constraint where the odd spins are fixed as $s_1 = +1$ and $s_3 = -1$. The closed communicating class determined by this constraint is the set of all states $\\mathbf{s}$ where $s_1=+1$ and $s_3=-1$, while $s_2$ and $s_4$ can be either $+1$ or $-1$. Let us denote this class by $\\mathcal{C}_{1,-1}$. It contains the following four states:\n$\\mathcal{C}_{1,-1} = \\{ (+1, +1, -1, +1), (+1, +1, -1, -1), (+1, -1, -1, +1), (+1, -1, -1, -1) \\}$.\n\nSecond, we derive the stationary distribution of the Markov chain when it is restricted to this closed class $\\mathcal{C}_{1,-1}$. The MH algorithm is constructed to satisfy the detailed balance condition with respect to the target Boltzmann distribution $\\pi(\\mathbf{s}) \\propto \\exp(-\\beta E(\\mathbf{s}))$. The condition is $\\pi(\\mathbf{s}) P(\\mathbf{s} \\to \\mathbf{s}') = \\pi(\\mathbf{s}') P(\\mathbf{s}' \\to \\mathbf{s})$ for any two states $\\mathbf{s}, \\mathbf{s}'$. When a Markov chain that satisfies detailed balance with respect to a distribution $\\pi$ is reducible, its dynamic evolution, once confined to a closed class $\\mathcal{C}$, will converge to a stationary distribution $\\pi_{\\text{cond}}$ that is the conditional distribution of $\\pi$ restricted to $\\mathcal{C}$.\nFor any state $\\mathbf{s} \\in \\mathcal{C}_{1,-1}$, this stationary distribution is given by:\n$$\n\\pi_{\\text{cond}}(\\mathbf{s}) = \\frac{\\pi(\\mathbf{s})}{\\sum_{\\mathbf{s}' \\in \\mathcal{C}_{1,-1}} \\pi(\\mathbf{s}')} = \\frac{\\exp(-\\beta E(\\mathbf{s}))}{\\sum_{\\mathbf{s}' \\in \\mathcal{C}_{1,-1}} \\exp(-\\beta E(\\mathbf{s}'))}\n$$\nThis is the Boltzmann distribution conditioned on the fixed odd spins $s_1=+1$ and $s_3=-1$.\n\nThird, we compute the fraction of the target Boltzmann probability mass that is unreachable by the chain starting with the given constraint. This fraction is $1$ minus the total probability mass of the accessible class $\\mathcal{C}_{1,-1}$. The total probability mass of $\\mathcal{C}_{1,-1}$ is the sum of the probabilities of all states within it, under the full Boltzmann distribution $\\pi(\\mathbf{s}) = \\exp(-\\beta E(\\mathbf{s}))/Z$, where $Z$ is the full partition function over all $16$ states.\n$$\nP(\\mathcal{C}_{1,-1}) = \\sum_{\\mathbf{s} \\in \\mathcal{C}_{1,-1}} \\pi(\\mathbf{s}) = \\frac{\\sum_{\\mathbf{s} \\in \\mathcal{C}_{1,-1}} \\exp(-\\beta E(\\mathbf{s}))}{\\sum_{\\mathbf{s} \\in \\mathcal{S}} \\exp(-\\beta E(\\mathbf{s}))} = \\frac{Z_{1,-1}}{Z}\n$$\nThe unreachable mass fraction is therefore $1 - P(\\mathcal{C}_{1,-1}) = 1 - \\frac{Z_{1,-1}}{Z}$.\n\nWe must calculate the partial partition function $Z_{1,-1}$ and the full partition function $Z$.\nThe Hamiltonian is $E(\\mathbf{s}) = - J \\left( s_1 s_2 + s_2 s_3 + s_3 s_4 + s_4 s_1 \\right) - h \\left( s_1 + s_2 + s_3 + s_4 \\right)$.\nFor the class $\\mathcal{C}_{1,-1}$, we fix $s_1=+1$ and $s_3=-1$. The energy becomes:\n$E(\\mathbf{s}) = -J((+1)s_2 + s_2(-1) + (-1)s_4 + s_4(+1)) - h((+1) + s_2 + (-1) + s_4) = -J(0) - h(s_2+s_4) = -h(s_2+s_4)$.\nWe sum $\\exp(-\\beta E(\\mathbf{s}))$ over the four states in $\\mathcal{C}_{1,-1}$, which correspond to $(s_2,s_4)$ pairs $(+1,+1), (+1,-1), (-1,+1), (-1,-1)$.\nThe energies are $-2h, 0, 0, 2h$.\n$Z_{1,-1} = \\exp(2\\beta h) + \\exp(0) + \\exp(0) + \\exp(-2\\beta h) = \\exp(2\\beta h) + \\exp(-2\\beta h) + 2 = 2\\cosh(2\\beta h) + 2$.\n\nNext, we calculate the full partition function $Z$. We can do this by summing over all $16$ configurations, classified by their energy.\nThe distinct energy levels and their degeneracies are:\n\\begin{itemize}\n    \\item $k=4$ (+ spins): $1$ state $(1,1,1,1)$; $E = -4J-4h$.\n    \\item $k=0$ (+ spins): $1$ state $(-1,-1,-1,-1)$; $E = -4J+4h$.\n    \\item $k=3$ (+ spins): $4$ states like $(1,1,1,-1)$; $E = -2h$.\n    \\item $k=1$ (+ spins): $4$ states like $(1,-1,-1,-1)$; $E = 2h$.\n    \\item $k=2$ (+ spins), alternating like $(1,-1,1,-1)$: $2$ states; $E = 4J$.\n    \\item $k=2$ (+ spins), adjacent like $(1,1,-1,-1)$: $4$ states; $E = 0$.\n\\end{itemize}\nSumming the Boltzmann factors for all $16$ states:\n$Z = \\exp(-\\beta(-4J-4h)) + \\exp(-\\beta(-4J+4h)) + 4\\exp(-\\beta(-2h)) + 4\\exp(-\\beta(2h)) + 2\\exp(-\\beta(4J)) + 4\\exp(0)$.\n$Z = \\exp(4\\beta J + 4\\beta h) + \\exp(4\\beta J - 4\\beta h) + 4\\exp(2\\beta h) + 4\\exp(-2\\beta h) + 2\\exp(-4\\beta J) + 4$.\nWe can group terms:\n$Z = \\exp(4\\beta J)[\\exp(4\\beta h) + \\exp(-4\\beta h)] + 4[\\exp(2\\beta h) + \\exp(-2\\beta h)] + 2\\exp(-4\\beta J) + 4$.\n$Z = 2\\exp(4\\beta J)\\cosh(4\\beta h) + 8\\cosh(2\\beta h) + 2\\exp(-4\\beta J) + 4$.\n\nThe fraction of unreachable mass is $1 - \\frac{Z_{1,-1}}{Z}$:\n$$\n\\text{Fraction} = 1 - \\frac{2\\cosh(2\\beta h) + 2}{2\\exp(4\\beta J)\\cosh(4\\beta h) + 8\\cosh(2\\beta h) + 2\\exp(-4\\beta J) + 4}\n$$\nThis can be written as $\\frac{Z - Z_{1,-1}}{Z}$:\n$$\n\\text{Fraction} = \\frac{(2\\exp(4\\beta J)\\cosh(4\\beta h) + 8\\cosh(2\\beta h) + 2\\exp(-4\\beta J) + 4) - (2\\cosh(2\\beta h) + 2)}{2\\exp(4\\beta J)\\cosh(4\\beta h) + 8\\cosh(2\\beta h) + 2\\exp(-4\\beta J) + 4}\n$$\n$$\n\\text{Fraction} = \\frac{2\\exp(4\\beta J)\\cosh(4\\beta h) + 6\\cosh(2\\beta h) + 2\\exp(-4\\beta J) + 2}{2\\exp(4\\beta J)\\cosh(4\\beta h) + 8\\cosh(2\\beta h) + 2\\exp(-4\\beta J) + 4}\n$$\nDividing the numerator and denominator by $2$ gives the final expression:\n$$\n\\text{Fraction} = \\frac{\\exp(4\\beta J)\\cosh(4\\beta h) + 3\\cosh(2\\beta h) + \\exp(-4\\beta J) + 1}{\\exp(4\\beta J)\\cosh(4\\beta h) + 4\\cosh(2\\beta h) + \\exp(-4\\beta J) + 2}\n$$",
            "answer": "$$\\boxed{\\frac{\\exp(4\\beta J)\\cosh(4\\beta h) + 3\\cosh(2\\beta h) + \\exp(-4\\beta J) + 1}{\\exp(4\\beta J)\\cosh(4\\beta h) + 4\\cosh(2\\beta h) + \\exp(-4\\beta J) + 2}}$$"
        },
        {
            "introduction": "Beyond ensuring a sampler is ergodic, a central task in practical MCMC is optimizing its efficiency. A valid sampler that explores the state space too slowly is of little practical use. This practice problem delves into a common trade-off: balancing the computational cost of generating a proposal, the size of the proposed move, and the resulting acceptance probability. By modeling the effective sample size per unit of wall-clock time, you will determine the optimal proposal strategy, a crucial skill for designing efficient simulations of complex materials. ",
            "id": "3463564",
            "problem": "In sampling atomic configurations of a crystalline defect with the Metropolis-Hastings algorithm targeting the Boltzmann distribution at temperature $T$, consider a single collective coordinate $q$ describing a local relaxation mode near a mechanically stable configuration. In a harmonic approximation, the potential energy is $E(q) \\approx E_{0} + \\frac{1}{2} k_{\\mathrm{eff}} q^{2}$. Proposals are generated by a symmetric random-walk perturbation $q' = q + \\delta$, where $\\delta$ is drawn from an isotropic distribution with a single tunable scale parameter $s$ so that the typical squared jump distance is $s^{2}$. For a symmetric proposal, the Metropolis-Hastings acceptance probability is $\\alpha(q \\to q') = \\min\\{1, \\exp(-\\beta [E(q') - E(q)])\\}$, where $\\beta = 1/(k_{B} T)$ and $k_{B}$ is the Boltzmann constant.\n\nAssume the following modeling choices, justified by the harmonic approximation and a small-step regime:\n- The mean acceptance as a function of the scale $s$ is $a(s) \\approx \\exp(-\\lambda s^{2})$ for an accurate, fully ab initio energy evaluation, with $\\lambda = \\frac{1}{2} \\beta k_{\\mathrm{eff}}$ a positive constant.\n- For a cheaper surrogate-based proposal that is corrected to maintain detailed balance (e.g., a two-stage scheme where a surrogate filter induces additional rejections), the effective mean acceptance is $a_{\\mathrm{cheap}}(s) \\approx r \\exp(-\\mu s^{2})$ with $0 < r < 1$ and $\\mu > 0$ encapsulating the extra mismatch-induced sensitivity.\n- The expected squared jump distance conditional on acceptance is $\\mathbb{E}[(q' - q)^{2} \\mid \\text{accept}] \\approx s^{2}$ for both proposal mechanisms.\n\nIt is common in Markov chain Monte Carlo to use the expected squared jump distance as a proxy for mixing speed and inverse integrated autocorrelation time. Under this proxy, model the effective sample size per unit wall-clock time for a given $s$ as proportional to\n$$\nF(s) = \\frac{a(s) \\, s^{2}}{c},\n$$\nwhere $c$ is the mean wall-clock time per proposal. For the accurate evaluation, take $c_{\\mathrm{acc}} = 5 \\ \\mathrm{s}$ and $\\lambda_{\\mathrm{acc}} = 4$. For the cheap proposal, take $c_{\\mathrm{ch}} = 0.2 \\ \\mathrm{s}$, $r = 0.6$, and $\\mu = 9$.\n\nStarting from the definition of the Metropolis-Hastings acceptance and the above modeling assumptions grounded in the harmonic approximation, derive the scale $s$ that maximizes $F(s)$ for each mechanism, and from these optima derive the ratio of the maximized effective sample sizes per unit time of the cheap mechanism relative to the accurate mechanism. Express your final answer as a single real number equal to this ratio. Round your answer to three significant figures. The ratio is dimensionless; do not include units.",
            "solution": "The problem is first assessed for validity.\n\n**Step 1: Extract Givens**\n- Algorithm: Metropolis-Hastings targeting the Boltzmann distribution at temperature $T$.\n- Potential energy in harmonic approximation: $E(q) \\approx E_{0} + \\frac{1}{2} k_{\\mathrm{eff}} q^{2}$.\n- Proposal move: $q' = q + \\delta$, with $\\delta$ from a symmetric distribution with scale $s$.\n- Acceptance probability: $\\alpha(q \\to q') = \\min\\{1, \\exp(-\\beta [E(q') - E(q)])\\}$, where $\\beta = 1/(k_{B} T)$.\n- Accurate mechanism mean acceptance: $a_{\\mathrm{acc}}(s) \\approx \\exp(-\\lambda_{\\mathrm{acc}} s^{2})$.\n- Cheap mechanism mean acceptance: $a_{\\mathrm{ch}}(s) \\approx r \\exp(-\\mu s^{2})$.\n- Proxy for effective sample size per unit time: $F(s) = \\frac{a(s) \\, s^{2}}{c}$.\n- Parameters for the accurate mechanism: $c_{\\mathrm{acc}} = 5 \\ \\mathrm{s}$, $\\lambda_{\\mathrm{acc}} = 4$.\n- Parameters for the cheap mechanism: $c_{\\mathrm{ch}} = 0.2 \\ \\mathrm{s}$, $r = 0.6$, $\\mu = 9$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is well-defined and scientifically grounded within the field of computational materials science and statistical mechanics. The Metropolis-Hastings algorithm, the use of a Boltzmann distribution, the harmonic approximation for potential energy near a minimum, and the use of the expected squared jump distance as a proxy for sampling efficiency are all standard concepts and practices. The functional forms for acceptance probability are reasonable approximations for the small-step regime. The problem provides all necessary data and is free from contradictions, ambiguities, or factual errors. Therefore, the problem is deemed valid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full, reasoned solution follows.\n\nThe objective is to find the ratio of the maximized effective sample size per unit time for the cheap mechanism relative to the accurate one. This requires maximizing the function $F(s) = \\frac{a(s) \\, s^{2}}{c}$ for each mechanism.\n\nFirst, let us find the general form of the optimal scale parameter $s$ that maximizes a function of the form $G(s) = s^2 \\exp(-k s^2)$ for some positive constant $k$. The function $F(s)$ for both mechanisms is proportional to this form. To find the maximum, we compute the derivative of $G(s)$ with respect to $s$ and set it to zero.\nUsing the product rule for differentiation:\n$$\n\\frac{dG}{ds} = \\frac{d}{ds} \\left( s^2 \\exp(-k s^2) \\right) = (2s) \\exp(-k s^2) + s^2 \\left( \\exp(-k s^2) \\cdot (-2ks) \\right)\n$$\n$$\n\\frac{dG}{ds} = 2s \\exp(-k s^2) (1 - k s^2)\n$$\nFor a non-trivial maximum ($s > 0$), we set the term $(1 - k s^2)$ to zero:\n$$\n1 - k s^2 = 0 \\implies s^2 = \\frac{1}{k}\n$$\nThe optimal scale parameter is $s_{\\mathrm{opt}} = \\sqrt{1/k}$. The maximum value of $G(s)$ is obtained by substituting $s_{\\mathrm{opt}}^2 = 1/k$ back into the function:\n$$\nG_{\\mathrm{max}} = G(s_{\\mathrm{opt}}) = \\left(\\frac{1}{k}\\right) \\exp\\left(-k \\cdot \\frac{1}{k}\\right) = \\frac{1}{k} \\exp(-1) = \\frac{1}{ke}\n$$\n\nNow, we apply this result to each of the two mechanisms.\n\n**1. Accurate Mechanism**\nThe function to maximize is:\n$$\nF_{\\mathrm{acc}}(s) = \\frac{a_{\\mathrm{acc}}(s) s^2}{c_{\\mathrm{acc}}} = \\frac{\\exp(-\\lambda_{\\mathrm{acc}} s^2) s^2}{c_{\\mathrm{acc}}} = \\frac{1}{c_{\\mathrm{acc}}} \\left( s^2 \\exp(-\\lambda_{\\mathrm{acc}} s^2) \\right)\n$$\nThis corresponds to our general form with $k = \\lambda_{\\mathrm{acc}}$. The optimization of $F_{\\mathrm{acc}}(s)$ is equivalent to optimizing $s^2 \\exp(-\\lambda_{\\mathrm{acc}} s^2)$.\nThe maximum value of $F_{\\mathrm{acc}}(s)$, denoted $F_{\\mathrm{acc, max}}$, is:\n$$\nF_{\\mathrm{acc, max}} = \\frac{1}{c_{\\mathrm{acc}}} \\left( \\frac{1}{\\lambda_{\\mathrm{acc}} e} \\right) = \\frac{1}{c_{\\mathrm{acc}} \\lambda_{\\mathrm{acc}} e}\n$$\nSubstituting the given values $c_{\\mathrm{acc}} = 5$ and $\\lambda_{\\mathrm{acc}} = 4$:\n$$\nF_{\\mathrm{acc, max}} = \\frac{1}{5 \\cdot 4 \\cdot e} = \\frac{1}{20e}\n$$\n\n**2. Cheap Mechanism**\nThe function to maximize is:\n$$\nF_{\\mathrm{ch}}(s) = \\frac{a_{\\mathrm{ch}}(s) s^2}{c_{\\mathrm{ch}}} = \\frac{r \\exp(-\\mu s^2) s^2}{c_{\\mathrm{ch}}} = \\frac{r}{c_{\\mathrm{ch}}} \\left( s^2 \\exp(-\\mu s^2) \\right)\n$$\nThis corresponds to our general form with $k = \\mu$. The optimization of $F_{\\mathrm{ch}}(s)$ is equivalent to optimizing $s^2 \\exp(-\\mu s^2)$, with the result scaled by the prefactor $r/c_{\\mathrm{ch}}$.\nThe maximum value of $F_{\\mathrm{ch}}(s)$, denoted $F_{\\mathrm{ch, max}}$, is:\n$$\nF_{\\mathrm{ch, max}} = \\frac{r}{c_{\\mathrm{ch}}} \\left( \\frac{1}{\\mu e} \\right) = \\frac{r}{c_{\\mathrm{ch}} \\mu e}\n$$\nSubstituting the given values $c_{\\mathrm{ch}} = 0.2$, $r = 0.6$, and $\\mu = 9$:\n$$\nF_{\\mathrm{ch, max}} = \\frac{0.6}{0.2 \\cdot 9 \\cdot e} = \\frac{0.6}{1.8 e} = \\frac{1}{3e}\n$$\n\n**3. Ratio of Maximized Efficiencies**\nThe problem asks for the ratio of the maximized effective sample size of the cheap mechanism to that of the accurate mechanism.\n$$\n\\text{Ratio} = \\frac{F_{\\mathrm{ch, max}}}{F_{\\mathrm{acc, max}}} = \\frac{ \\frac{r}{c_{\\mathrm{ch}} \\mu e} }{ \\frac{1}{c_{\\mathrm{acc}} \\lambda_{\\mathrm{acc}} e} }\n$$\nThe factor of $1/e$ cancels from the numerator and denominator:\n$$\n\\text{Ratio} = \\frac{r c_{\\mathrm{acc}} \\lambda_{\\mathrm{acc}}}{c_{\\mathrm{ch}} \\mu}\n$$\nNow, we substitute the numerical values:\n$$\n\\text{Ratio} = \\frac{0.6 \\cdot 5 \\cdot 4}{0.2 \\cdot 9} = \\frac{12}{1.8} = \\frac{120}{18} = \\frac{20}{3}\n$$\nAs a decimal, this is $20/3 = 6.666...$.\nThe problem asks to round the answer to three significant figures.\n$$\n\\text{Ratio} \\approx 6.67\n$$\nThis result indicates that the cheap, surrogate-assisted sampling strategy is approximately $6.67$ times more efficient than the standard accurate method under the given modeling assumptions, when both are optimally tuned.",
            "answer": "$$\n\\boxed{6.67}\n$$"
        },
        {
            "introduction": "We conclude by tackling one of the most significant challenges in modern computational materials science: the 'curse of dimensionality.' As the number of atoms, and thus degrees of freedom, becomes large, the volume of the configuration space grows exponentially, causing naive MCMC proposal strategies to fail with near-zero acceptance rates. This exercise guides you through the reasoning behind the solution, showing how proposal distributions must be scaled with dimension and revealing the theoretical origins of the celebrated $0.234$ optimal acceptance rate, a vital rule-of-thumb for high-dimensional sampling. ",
            "id": "3463635",
            "problem": "In computational materials science, the equilibrium probability distribution of vibrational displacements in a crystalline solid under the harmonic approximation factorizes across normal modes. Consider a model where the target distribution in dimension $d$ is the product $\\pi_d(x) = \\prod_{i=1}^d f(x_i)$ of identical one-dimensional densities $f$ representing independent normal-mode amplitudes. A random-walk Metropolis sampler with Gaussian proposals uses $y = x + \\sigma_d z$, where $z \\sim \\mathcal{N}(0, I_d)$, and accepts with probability $1 \\wedge \\frac{\\pi_d(y)}{\\pi_d(x)}$ due to proposal symmetry. Define the acceptance probability as $\\alpha(x, y) = 1 \\wedge \\exp\\big(\\sum_{i=1}^d (\\log f(y_i) - \\log f(x_i))\\big)$.\n\nStarting only from the definition of the Metropolis-Hastings acceptance probability, smoothness of $\\log f$ sufficient to justify a Taylor expansion, the Law of Large Numbers and Central Limit Theorem, and the product form of $\\pi_d$, use a diffusion limit argument for $d \\to \\infty$ to reason about how the proposal scale $\\sigma_d$ must depend on $d$ in order to obtain an $O(1)$ acceptance probability, and how the mixing speed of a single coordinate behaves on the diffusive timescale. Then select all statements that are correct about the optimal acceptance rate and scaling in this regime.\n\nA. The proposal standard deviation must scale as $\\sigma_d = \\frac{l}{\\sqrt{d}}$ for some $l > 0$ to keep the acceptance probability non-degenerate as $d \\to \\infty$; under this scaling, the limiting diffusivity of any single coordinate is proportional to $l^2$ times the average acceptance probability, and maximizing this diffusivity yields an optimal acceptance rate approximately $0.234$ as $d \\to \\infty$.\n\nB. Keeping $\\sigma_d$ fixed as $d$ increases leads to acceptance probabilities approaching $1$ because the product target concentrates; this produces optimal performance due to long-distance proposals efficiently traversing high-dimensional space.\n\nC. Under the scaling $\\sigma_d = \\frac{l}{\\sqrt{d}}$, the logarithm of the Metropolis ratio is $O(1)$; the limiting speed of the single-coordinate diffusion is proportional to $l^2$ times the acceptance probability, which is maximized when the acceptance rate is approximately $0.574$.\n\nD. Choosing $\\sigma_d = \\frac{l}{d}$ yields acceptance probabilities close to $1$ but produces a degenerate limiting diffusion with vanishing speed on the diffusive timescale, so this scaling is suboptimal.\n\nE. An optimal acceptance rate near $0.234$ arises only for the standard normal $f$; for other smooth one-dimensional $f$ with product form, the high-dimensional optimal acceptance rate generically differs.\n\nSelect all correct options.",
            "solution": "We begin from first principles: Metropolis-Hastings (MH) for a symmetric proposal accepts a proposed state $y$ from current $x$ with probability $\\alpha(x, y) = 1 \\wedge \\exp\\big(\\sum_{i=1}^d [\\log f(y_i) - \\log f(x_i)]\\big)$, because the proposal density cancels in the ratio. For a random-walk Gaussian proposal $y = x + \\sigma_d z$ with $z \\sim \\mathcal{N}(0, I_d)$ independent of $x$, the log ratio is\n$$\nR_d(x, z; \\sigma_d) = \\sum_{i=1}^d \\big(\\log f(x_i + \\sigma_d z_i) - \\log f(x_i)\\big).\n$$\nAssume $\\log f$ is twice continuously differentiable and $x$ is distributed according to the stationary product measure $\\pi_d$. Write $\\ell(u) = \\log f(u)$ and denote derivatives $\\ell'(u)$ and $\\ell''(u)$. A Taylor expansion yields, for each coordinate $i$,\n$$\n\\ell(x_i + \\sigma_d z_i) - \\ell(x_i) = \\sigma_d z_i \\ell'(x_i) + \\frac{1}{2} \\sigma_d^2 z_i^2 \\ell''(x_i) + r_i,\n$$\nwhere the remainder $r_i$ is $o(\\sigma_d^2)$ under smoothness conditions when $\\sigma_d \\to 0$. Summing over $i$,\n$$\nR_d = \\sigma_d \\sum_{i=1}^d z_i \\ell'(x_i) + \\frac{1}{2} \\sigma_d^2 \\sum_{i=1}^d z_i^2 \\ell''(x_i) + \\sum_{i=1}^d r_i.\n$$\nLet $X \\sim f$ denote a generic one-dimensional marginal at stationarity. Two standard identities follow from normalization and integration by parts:\n- $\\mathbb{E}_f[\\ell'(X)] = 0$,\n- $\\mathbb{E}_f[\\ell''(X)] + \\mathbb{E}_f[(\\ell'(X))^2] = 0$,\nprovided $f$ and its derivatives decay sufficiently fast at infinity for the boundary terms to vanish. Define $I = \\mathbb{E}_f[(\\ell'(X))^2]$, a measure of roughness akin to Fisher information of the location parameter for $f$.\n\nWe seek a scaling $\\sigma_d$ such that $R_d$ converges in distribution to a non-degenerate random variable as $d \\to \\infty$, because the acceptance probability $\\alpha(x, y) = 1 \\wedge e^{R_d}$ will then be non-degenerate (neither going to $0$ nor $1$). Consider the leading terms under the independence structure. Conditional on $x$, $\\sum_{i=1}^d z_i \\ell'(x_i)$ is a sum of $d$ independent mean-zero terms. By the Central Limit Theorem and independence across $i$, one expects\n$$\n\\sum_{i=1}^d z_i \\ell'(x_i) \\approx \\sqrt{d} \\, N_1 \\sqrt{I},\n$$\nwith $N_1 \\sim \\mathcal{N}(0, 1)$, because $\\mathbb{E}_f[(\\ell'(X))^2] = I$ and $z_i$ and $\\ell'(x_i)$ are independent with unit variance for $z_i$. For the second-order term, by the Law of Large Numbers,\n$$\n\\sum_{i=1}^d z_i^2 \\ell''(x_i) \\approx \\sum_{i=1}^d \\big(\\mathbb{E}[z_i^2] \\, \\mathbb{E}_f[\\ell''(X)]\\big) = d \\cdot 1 \\cdot (-I) = - d I,\n$$\nusing $\\mathbb{E}_f[\\ell''(X)] = - I$. Therefore, under the scaling\n$$\n\\sigma_d = \\frac{l}{\\sqrt{d}}, \\quad l > 0,\n$$\nthe leading contribution becomes\n$$\nR_d \\approx \\sigma_d \\sqrt{d} \\, N_1 \\sqrt{I} + \\frac{1}{2} \\sigma_d^2 (- d I) = l N_1 \\sqrt{I} - \\frac{1}{2} l^2 I,\n$$\nwith remainders negligible as $d \\to \\infty$. Hence, in the limit,\n$$\nR_\\infty \\overset{d}{=} l N_1 \\sqrt{I} - \\frac{1}{2} l^2 I,\n$$\nand the acceptance probability (averaged over $x$ and $z$) converges to\n$$\n\\alpha(l) = \\mathbb{E}\\big[ 1 \\wedge \\exp\\big(l N_1 \\sqrt{I} - \\tfrac{1}{2} l^2 I\\big) \\big].\n$$\nA standard Gaussian calculation (completing the square) yields the closed form\n$$\n\\alpha(l) = 2 \\, \\Phi\\!\\Big( - \\frac{l \\sqrt{I}}{2} \\Big),\n$$\nwhere $\\Phi$ is the standard normal cumulative distribution function. This shows that $\\sigma_d = \\frac{l}{\\sqrt{d}}$ produces a non-degenerate acceptance in the limit and quantifies its dependence on $l$.\n\nTo understand mixing, consider the evolution of a single coordinate (say $x_1$). Each proposed move changes $x_1$ by $\\sigma_d z_1$ when the proposal is accepted. The expected squared jump per iteration in $x_1$ is approximately\n$$\n\\mathbb{E}\\big[ (\\sigma_d z_1)^2 \\cdot \\mathbf{1}\\{\\text{accept}\\} \\big] \\approx \\sigma_d^2 \\, \\alpha(l),\n$$\nbecause $z_1 \\sim \\mathcal{N}(0,1)$ is independent of the acceptance indicator asymptotically. On the diffusive time scale of $O(d)$ iterations (natural for high-dimensional product targets), one unit of rescaled time corresponds to $d$ iterations. Under the $\\sigma_d = \\frac{l}{\\sqrt{d}}$ scaling, the variance accumulated per unit rescaled time in a single coordinate is\n$$\nd \\cdot \\sigma_d^2 \\, \\alpha(l) = d \\cdot \\frac{l^2}{d} \\, \\alpha(l) = l^2 \\alpha(l),\n$$\nwhich is $O(1)$. A rigorous diffusion limit argument shows that the first coordinate converges to a Langevin-type diffusion with speed (diffusivity coefficient) proportional to\n$$\nh(l) = l^2 \\, \\alpha(l) = 2 l^2 \\, \\Phi\\!\\Big( - \\frac{l \\sqrt{I}}{2} \\Big).\n$$\nOptimizing mixing corresponds to maximizing $h(l)$ over $l > 0$. Differentiating,\n$$\nh'(l) = 4 l \\, \\Phi\\!\\Big( - \\frac{l \\sqrt{I}}{2} \\Big) - 2 l^2 \\cdot \\frac{\\sqrt{I}}{2} \\, \\phi\\!\\Big( \\frac{l \\sqrt{I}}{2} \\Big),\n$$\nwhere $\\phi$ is the standard normal probability density function. Setting $h'(l) = 0$ and writing $z = \\frac{l \\sqrt{I}}{2}$ gives the optimality condition\n$$\n2 \\, \\Phi(-z) = z \\, \\phi(z).\n$$\nSolving this scalar equation numerically for $z$ yields an acceptance probability\n$$\n\\alpha^\\star = 2 \\, \\Phi(-z^\\star) \\approx 0.234,\n$$\nwhich is remarkably insensitive to the particular choice of $f$ as long as the product structure and regularity hold (the parameter $I$ determines the optimal $l$ but cancels from the optimal acceptance level).\n\nWe now analyze each option:\n\nA. This statement synthesizes the diffusion limit scaling $\\sigma_d = \\frac{l}{\\sqrt{d}}$, the non-degenerate acceptance probability limit $\\alpha(l)$, and the fact that the single-coordinate limiting diffusivity is $h(l) = l^2 \\alpha(l)$. The optimization of $h(l)$ yields an optimal acceptance of approximately $0.234$. This is the core result of the diffusion limit analysis in high dimension and is correct.\n\nB. With $\\sigma_d$ fixed (independent of $d$), the first-order term in $R_d$ scales like $\\sigma_d \\sum_{i=1}^d z_i \\ell'(x_i)$, which is generically $O(\\sqrt{d})$, and the second-order term is $O(d)$ negative. Thus $R_d \\to -\\infty$ in probability, and the acceptance probability goes to $0$ as $d \\to \\infty$. Therefore, acceptance does not approach $1$; performance is poor because proposals are too large relative to concentration of the high-dimensional product measure. This option is incorrect.\n\nC. The scaling $\\sigma_d = \\frac{l}{\\sqrt{d}}$ is correct, and the statement that the logarithm of the Metropolis ratio is $O(1)$ is also correct. However, the claim that the limiting speed is maximized at acceptance approximately $0.574$ is incorrect. The acceptance near $0.574$ is associated with different algorithms (for example, the Metropolis-adjusted Langevin algorithm under certain conditions), not the random-walk Metropolis with Gaussian proposals targeting product measures. For random-walk Metropolis, maximizing $h(l) = l^2 \\alpha(l)$ yields acceptance near $0.234$. Hence this option is incorrect.\n\nD. If $\\sigma_d = \\frac{l}{d}$, then the step size per coordinate is extremely small. The acceptance probability tends to $1$ because $R_d \\to 0$ (both first- and second-order terms vanish as $\\sigma_d \\to 0$ faster than $d^{-1/2}$). However, on the diffusive timescale of $O(d)$ iterations, the variance accumulated in a single coordinate is $d \\cdot \\sigma_d^2 \\, \\alpha \\approx d \\cdot \\frac{l^2}{d^2} \\cdot 1 = \\frac{l^2}{d} \\to 0$. The limiting diffusion thus has zero speed (degenerate), indicating suboptimal scaling. This option is correct.\n\nE. The optimal acceptance rate near $0.234$ arises from the universal scalar equation $2 \\Phi(-z) = z \\phi(z)$ in the diffusion limit for product targets with smooth identically distributed marginals. The parameter $I$ (depending on $f$) affects the optimal $l$ through $z = \\frac{l \\sqrt{I}}{2}$, but the optimal acceptance level $\\alpha^\\star = 2 \\Phi(-z^\\star)$ is universal for this class. Therefore, this option is incorrect.\n\nVerdicts: A is Correct. B is Incorrect. C is Incorrect. D is Correct. E is Incorrect.",
            "answer": "$$\\boxed{AD}$$"
        }
    ]
}