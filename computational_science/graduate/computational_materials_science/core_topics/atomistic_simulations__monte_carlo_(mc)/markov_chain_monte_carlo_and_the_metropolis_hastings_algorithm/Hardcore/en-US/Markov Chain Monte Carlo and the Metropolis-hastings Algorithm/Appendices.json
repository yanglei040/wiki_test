{
    "hands_on_practices": [
        {
            "introduction": "Before an MCMC simulation can be efficient, it must first be correct. This requires the underlying Markov chain to be irreducible, meaning it must be possible to eventually reach any important state from any other. This hands-on problem  serves as a crucial cautionary tale, using a simple, analytically tractable Ising model to reveal how an intuitively reasonable proposal mechanism can violate this principle, trapping the simulation within a disconnected portion of the state space. By working through this exercise, you will gain a concrete understanding of communicating classes and learn to precisely quantify the systematic error introduced by a non-irreducible sampler.",
            "id": "3463572",
            "problem": "Consider a one-dimensional periodic Ising lattice with $N=4$ sites modeling a binary alloy on a ring, with spin variables $s_i \\in \\{-1,+1\\}$ representing local species states at lattice site $i$. The energy of a configuration $\\mathbf{s} = (s_1,s_2,s_3,s_4)$ is given by the nearest-neighbor Ising Hamiltonian with an external field,\n$$\nE(\\mathbf{s}) = - J \\left( s_1 s_2 + s_2 s_3 + s_3 s_4 + s_4 s_1 \\right) - h \\left( s_1 + s_2 + s_3 + s_4 \\right),\n$$\nwhere $J$ and $h$ are material-dependent interaction and field parameters, respectively, and $\\beta$ is the inverse thermal energy with $\\beta = 1/(k_B T)$ for Boltzmann constant $k_B$ and temperature $T$. The target distribution for sampling equilibrium configurations is the Boltzmann distribution\n$$\n\\pi(\\mathbf{s}) \\propto \\exp\\!\\left( - \\beta E(\\mathbf{s}) \\right).\n$$\nA Markov chain Monte Carlo (MCMC) method with the Metropolis-Hastings (MH) algorithm is used to sample from $\\pi(\\mathbf{s})$. The proposal kernel is defined as follows: at each step, one of the even-indexed sites from the set $\\{2,4\\}$ is chosen uniformly at random and its spin is flipped, while odd-indexed spins are never proposed to change. This proposal respects detailed balance locally but violates irreducibility on the full configuration space.\n\nStarting from the initial configuration with odd spins fixed as $s_1 = +1$ and $s_3 = -1$, perform the following tasks:\n\n- Using the definitions of irreducibility and detailed balance for Markov chains, explain why this proposal induces closed communicating classes of states and identify the closed class determined by the odd-spin constraint $s_1 = +1$ and $s_3 = -1$.\n- Derive the stationary distribution supported on this closed class that the MH chain converges to, expressed as the Boltzmann distribution conditioned on the fixed odd spins.\n- Compute the exact fraction of the target Boltzmann probability mass that is unreachable by this chain starting from the given odd-spin constraint, i.e., one minus the total conditioned probability mass of the closed class under the target $\\pi(\\mathbf{s})$.\n\nYour final answer must be a single closed-form analytic expression in terms of $\\beta$, $J$, and $h$. This quantity is dimensionless; express your answer without units. Do not approximate; no rounding is required.",
            "solution": "The problem as stated is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. It is a standard exercise in statistical mechanics and Markov chain Monte Carlo methods, examining the consequences of a reducible Markov chain. Therefore, the problem is deemed valid and a full solution follows.\n\nThe problem asks for three related tasks concerning a Metropolis-Hastings (MH) algorithm with a non-standard proposal mechanism applied to a 1D periodic Ising model with $N=4$ sites. We will address each task in sequence.\n\nFirst, we analyze the structure of the Markov chain induced by the proposal kernel. The state space of the system is the set of all possible spin configurations $\\mathbf{s}=(s_1,s_2,s_3,s_4)$, where $s_i \\in \\{-1, +1\\}$. This gives a total of $2^4 = 16$ possible states. The proposal rule states that at each step, a site from the set $\\{2, 4\\}$ is chosen uniformly at random and its spin is flipped. The spins at odd-indexed sites, $s_1$ and $s_3$, are never proposed to change.\n\nA Markov chain is **irreducible** if it is possible to go from any state to any other state (not necessarily in one step). The given proposal mechanism violates this property. If a chain starts in a state with a specific pair of values for $(s_1, s_3)$, say $(s_1^*, s_3^*)$, any subsequent state generated by the chain will also have its odd-indexed spins fixed to $(s_1^*, s_3^*)$. It is impossible to transition to a state with a different pair of odd-indexed spins, for example $(s_1', s_3')$ where $(s_1', s_3') \\neq (s_1^*, s_3^*)$.\n\nThis restriction partitions the total state space $\\mathcal{S}$ into four disjoint subsets, each corresponding to one of the possible pairs of values for $(s_1, s_3)$: $\\{(+1, +1), (+1, -1), (-1, +1), (-1, -1)\\}$. Within each such subset, any state can be reached from any other state. For example, within the subset where $s_1$ and $s_3$ are fixed, any configuration of $(s_2, s_4)$ can be reached from any other. A transition from $(s_2, s_4)$ to $(-s_2, s_4)$ is possible by flipping $s_2$, and a transition to $(-s_2, -s_4)$ is possible by first flipping $s_2$ and then $s_4$. These transitions are accepted with some non-zero probability by the MH rule. Therefore, all states within such a subset form a **communicating class**. Since no transitions are possible between these classes, each class is **closed**.\n\nThe problem specifies an initial constraint where the odd spins are fixed as $s_1 = +1$ and $s_3 = -1$. The closed communicating class determined by this constraint is the set of all states $\\mathbf{s}$ where $s_1=+1$ and $s_3=-1$, while $s_2$ and $s_4$ can be either $+1$ or $-1$. Let us denote this class by $\\mathcal{C}_{1,-1}$. It contains the following four states:\n$\\mathcal{C}_{1,-1} = \\{ (+1, +1, -1, +1), (+1, +1, -1, -1), (+1, -1, -1, +1), (+1, -1, -1, -1) \\}$.\n\nSecond, we derive the stationary distribution of the Markov chain when it is restricted to this closed class $\\mathcal{C}_{1,-1}$. The MH algorithm is constructed to satisfy the detailed balance condition with respect to the target Boltzmann distribution $\\pi(\\mathbf{s}) \\propto \\exp(-\\beta E(\\mathbf{s}))$. The condition is $\\pi(\\mathbf{s}) P(\\mathbf{s} \\to \\mathbf{s}') = \\pi(\\mathbf{s}') P(\\mathbf{s}' \\to \\mathbf{s})$ for any two states $\\mathbf{s}, \\mathbf{s}'$. When a Markov chain that satisfies detailed balance with respect to a distribution $\\pi$ is reducible, its dynamic evolution, once confined to a closed class $\\mathcal{C}$, will converge to a stationary distribution $\\pi_{\\text{cond}}$ that is the conditional distribution of $\\pi$ restricted to $\\mathcal{C}$.\nFor any state $\\mathbf{s} \\in \\mathcal{C}_{1,-1}$, this stationary distribution is given by:\n$$\n\\pi_{\\text{cond}}(\\mathbf{s}) = \\frac{\\pi(\\mathbf{s})}{\\sum_{\\mathbf{s}' \\in \\mathcal{C}_{1,-1}} \\pi(\\mathbf{s}')} = \\frac{\\exp(-\\beta E(\\mathbf{s}))}{\\sum_{\\mathbf{s}' \\in \\mathcal{C}_{1,-1}} \\exp(-\\beta E(\\mathbf{s}'))}\n$$\nThis is the Boltzmann distribution conditioned on the fixed odd spins $s_1=+1$ and $s_3=-1$.\n\nThird, we compute the fraction of the target Boltzmann probability mass that is unreachable by the chain starting with the given constraint. This fraction is $1$ minus the total probability mass of the accessible class $\\mathcal{C}_{1,-1}$. The total probability mass of $\\mathcal{C}_{1,-1}$ is the sum of the probabilities of all states within it, under the full Boltzmann distribution $\\pi(\\mathbf{s}) = \\exp(-\\beta E(\\mathbf{s}))/Z$, where $Z$ is the full partition function over all $16$ states.\n$$\nP(\\mathcal{C}_{1,-1}) = \\sum_{\\mathbf{s} \\in \\mathcal{C}_{1,-1}} \\pi(\\mathbf{s}) = \\frac{\\sum_{\\mathbf{s} \\in \\mathcal{C}_{1,-1}} \\exp(-\\beta E(\\mathbf{s}))}{\\sum_{\\mathbf{s} \\in \\mathcal{S}} \\exp(-\\beta E(\\mathbf{s}))} = \\frac{Z_{1,-1}}{Z}\n$$\nThe unreachable mass fraction is therefore $1 - P(\\mathcal{C}_{1,-1}) = 1 - \\frac{Z_{1,-1}}{Z}$.\n\nWe must calculate the partial partition function $Z_{1,-1}$ and the full partition function $Z$.\nThe Hamiltonian is $E(\\mathbf{s}) = - J \\left( s_1 s_2 + s_2 s_3 + s_3 s_4 + s_4 s_1 \\right) - h \\left( s_1 + s_2 + s_3 + s_4 \\right)$.\nFor the class $\\mathcal{C}_{1,-1}$, we fix $s_1=+1$ and $s_3=-1$. The energy becomes:\n$E(\\mathbf{s}) = -J((+1)s_2 + s_2(-1) + (-1)s_4 + s_4(+1)) - h((+1) + s_2 + (-1) + s_4) = -J(0) - h(s_2+s_4) = -h(s_2+s_4)$.\nWe sum $\\exp(-\\beta E(\\mathbf{s}))$ over the four states in $\\mathcal{C}_{1,-1}$, which correspond to $(s_2,s_4)$ pairs $(+1,+1), (+1,-1), (-1,+1), (-1,-1)$.\nThe energies are $-2h, 0, 0, 2h$.\n$Z_{1,-1} = \\exp(2\\beta h) + \\exp(0) + \\exp(0) + \\exp(-2\\beta h) = \\exp(2\\beta h) + \\exp(-2\\beta h) + 2 = 2\\cosh(2\\beta h) + 2$.\n\nNext, we calculate the full partition function $Z$. We can do this by summing over all $16$ configurations, classified by their energy.\nThe distinct energy levels and their degeneracies are:\n\\begin{itemize}\n    \\item $k=4$ (+ spins): $1$ state $(1,1,1,1)$; $E = -4J-4h$.\n    \\item $k=0$ (+ spins): $1$ state $(-1,-1,-1,-1)$; $E = -4J+4h$.\n    \\item $k=3$ (+ spins): $4$ states like $(1,1,1,-1)$; $E = -2h$.\n    \\item $k=1$ (+ spins): $4$ states like $(1,-1,-1,-1)$; $E = 2h$.\n    \\item $k=2$ (+ spins), alternating like $(1,-1,1,-1)$: $2$ states; $E = 4J$.\n    \\item $k=2$ (+ spins), adjacent like $(1,1,-1,-1)$: $4$ states; $E = 0$.\n\\end{itemize}\nSumming the Boltzmann factors for all $16$ states:\n$Z = \\exp(-\\beta(-4J-4h)) + \\exp(-\\beta(-4J+4h)) + 4\\exp(-\\beta(-2h)) + 4\\exp(-\\beta(2h)) + 2\\exp(-\\beta(4J)) + 4\\exp(0)$.\n$Z = \\exp(4\\beta J + 4\\beta h) + \\exp(4\\beta J - 4\\beta h) + 4\\exp(2\\beta h) + 4\\exp(-2\\beta h) + 2\\exp(-4\\beta J) + 4$.\nWe can group terms:\n$Z = \\exp(4\\beta J)[\\exp(4\\beta h) + \\exp(-4\\beta h)] + 4[\\exp(2\\beta h) + \\exp(-2\\beta h)] + 2\\exp(-4\\beta J) + 4$.\n$Z = 2\\exp(4\\beta J)\\cosh(4\\beta h) + 8\\cosh(2\\beta h) + 2\\exp(-4\\beta J) + 4$.\n\nThe fraction of unreachable mass is $1 - \\frac{Z_{1,-1}}{Z}$:\n$$\n\\text{Fraction} = 1 - \\frac{2\\cosh(2\\beta h) + 2}{2\\exp(4\\beta J)\\cosh(4\\beta h) + 8\\cosh(2\\beta h) + 2\\exp(-4\\beta J) + 4}\n$$\nThis can be written as $\\frac{Z - Z_{1,-1}}{Z}$:\n$$\n\\text{Fraction} = \\frac{(2\\exp(4\\beta J)\\cosh(4\\beta h) + 8\\cosh(2\\beta h) + 2\\exp(-4\\beta J) + 4) - (2\\cosh(2\\beta h) + 2)}{2\\exp(4\\beta J)\\cosh(4\\beta h) + 8\\cosh(2\\beta h) + 2\\exp(-4\\beta J) + 4}\n$$\n$$\n\\text{Fraction} = \\frac{2\\exp(4\\beta J)\\cosh(4\\beta h) + 6\\cosh(2\\beta h) + 2\\exp(-4\\beta J) + 2}{2\\exp(4\\beta J)\\cosh(4\\beta h) + 8\\cosh(2\\beta h) + 2\\exp(-4\\beta J) + 4}\n$$\nDividing the numerator and denominator by $2$ gives the final expression:\n$$\n\\text{Fraction} = \\frac{\\exp(4\\beta J)\\cosh(4\\beta h) + 3\\cosh(2\\beta h) + \\exp(-4\\beta J) + 1}{\\exp(4\\beta J)\\cosh(4\\beta h) + 4\\cosh(2\\beta h) + \\exp(-4\\beta J) + 2}\n$$",
            "answer": "$$\\boxed{\\frac{\\exp(4\\beta J)\\cosh(4\\beta h) + 3\\cosh(2\\beta h) + \\exp(-4\\beta J) + 1}{\\exp(4\\beta J)\\cosh(4\\beta h) + 4\\cosh(2\\beta h) + \\exp(-4\\beta J) + 2}}$$"
        },
        {
            "introduction": "Once your sampler is correctly exploring the entire state space, the next challenge is to make it do so efficiently. This practice  delves into the practical art of performance tuning by examining the trade-off between the computational cost of generating a proposal and its probability of being accepted. Using a physically-motivated model, you will move beyond simple heuristics to a more sophisticated analysis that optimizes the effective sample size generated per unit of real-world time. Mastering this balance is key to designing MCMC simulations that are not only correct but also computationally feasible.",
            "id": "3463564",
            "problem": "In sampling atomic configurations of a crystalline defect with the Metropolis-Hastings algorithm targeting the Boltzmann distribution at temperature $T$, consider a single collective coordinate $q$ describing a local relaxation mode near a mechanically stable configuration. In a harmonic approximation, the potential energy is $E(q) \\approx E_{0} + \\frac{1}{2} k_{\\mathrm{eff}} q^{2}$. Proposals are generated by a symmetric random-walk perturbation $q' = q + \\delta$, where $\\delta$ is drawn from an isotropic distribution with a single tunable scale parameter $s$ so that the typical squared jump distance is $s^{2}$. For a symmetric proposal, the Metropolis-Hastings acceptance probability is $\\alpha(q \\to q') = \\min\\{1, \\exp(-\\beta [E(q') - E(q)])\\}$, where $\\beta = 1/(k_{B} T)$ and $k_{B}$ is the Boltzmann constant.\n\nAssume the following modeling choices, justified by the harmonic approximation and a small-step regime:\n- The mean acceptance as a function of the scale $s$ is $a(s) \\approx \\exp(-\\lambda s^{2})$ for an accurate, fully ab initio energy evaluation, with $\\lambda = \\frac{1}{2} \\beta k_{\\mathrm{eff}}$ a positive constant.\n- For a cheaper surrogate-based proposal that is corrected to maintain detailed balance (e.g., a two-stage scheme where a surrogate filter induces additional rejections), the effective mean acceptance is $a_{\\mathrm{cheap}}(s) \\approx r \\exp(-\\mu s^{2})$ with $0 < r < 1$ and $\\mu > 0$ encapsulating the extra mismatch-induced sensitivity.\n- The expected squared jump distance conditional on acceptance is $\\mathbb{E}[(q' - q)^{2} \\mid \\text{accept}] \\approx s^{2}$ for both proposal mechanisms.\n\nIt is common in Markov chain Monte Carlo to use the expected squared jump distance as a proxy for mixing speed and inverse integrated autocorrelation time. Under this proxy, model the effective sample size per unit wall-clock time for a given $s$ as proportional to\n$$\nF(s) = \\frac{a(s) \\, s^{2}}{c},\n$$\nwhere $c$ is the mean wall-clock time per proposal. For the accurate evaluation, take $c_{\\mathrm{acc}} = 5 \\ \\mathrm{s}$ and $\\lambda_{\\mathrm{acc}} = 4$. For the cheap proposal, take $c_{\\mathrm{ch}} = 0.2 \\ \\mathrm{s}$, $r = 0.6$, and $\\mu = 9$.\n\nStarting from the definition of the Metropolis-Hastings acceptance and the above modeling assumptions grounded in the harmonic approximation, derive the scale $s$ that maximizes $F(s)$ for each mechanism, and from these optima derive the ratio of the maximized effective sample sizes per unit time of the cheap mechanism relative to the accurate mechanism. Express your final answer as a single real number equal to this ratio. Round your answer to three significant figures. The ratio is dimensionless; do not include units.",
            "solution": "The problem is first assessed for validity.\n\n**Step 1: Extract Givens**\n- Algorithm: Metropolis-Hastings targeting the Boltzmann distribution at temperature $T$.\n- Potential energy in harmonic approximation: $E(q) \\approx E_{0} + \\frac{1}{2} k_{\\mathrm{eff}} q^{2}$.\n- Proposal move: $q' = q + \\delta$, with $\\delta$ from a symmetric distribution with scale $s$.\n- Acceptance probability: $\\alpha(q \\to q') = \\min\\{1, \\exp(-\\beta [E(q') - E(q)])\\}$, where $\\beta = 1/(k_{B} T)$.\n- Accurate mechanism mean acceptance: $a_{\\mathrm{acc}}(s) \\approx \\exp(-\\lambda_{\\mathrm{acc}} s^{2})$.\n- Cheap mechanism mean acceptance: $a_{\\mathrm{ch}}(s) \\approx r \\exp(-\\mu s^{2})$.\n- Proxy for effective sample size per unit time: $F(s) = \\frac{a(s) \\, s^{2}}{c}$.\n- Parameters for the accurate mechanism: $c_{\\mathrm{acc}} = 5 \\ \\mathrm{s}$, $\\lambda_{\\mathrm{acc}} = 4$.\n- Parameters for the cheap mechanism: $c_{\\mathrm{ch}} = 0.2 \\ \\mathrm{s}$, $r = 0.6$, and $\\mu = 9$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is well-defined and scientifically grounded within the field of computational materials science and statistical mechanics. The Metropolis-Hastings algorithm, the use of a Boltzmann distribution, the harmonic approximation for potential energy near a minimum, and the use of the expected squared jump distance as a proxy for sampling efficiency are all standard concepts and practices. The functional forms for acceptance probability are reasonable approximations for the small-step regime. The problem provides all necessary data and is free from contradictions, ambiguities, or factual errors. Therefore, the problem is deemed valid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full, reasoned solution follows.\n\nThe objective is to find the ratio of the maximized effective sample size per unit time for the cheap mechanism relative to the accurate one. This requires maximizing the function $F(s) = \\frac{a(s) \\, s^{2}}{c}$ for each mechanism.\n\nFirst, let us find the general form of the optimal scale parameter $s$ that maximizes a function of the form $G(s) = s^2 \\exp(-k s^2)$ for some positive constant $k$. The function $F(s)$ for both mechanisms is proportional to this form. To find the maximum, we compute the derivative of $G(s)$ with respect to $s$ and set it to zero.\nUsing the product rule for differentiation:\n$$\n\\frac{dG}{ds} = \\frac{d}{ds} \\left( s^2 \\exp(-k s^2) \\right) = (2s) \\exp(-k s^2) + s^2 \\left( \\exp(-k s^2) \\cdot (-2ks) \\right)\n$$\n$$\n\\frac{dG}{ds} = 2s \\exp(-k s^2) (1 - k s^2)\n$$\nFor a non-trivial maximum ($s > 0$), we set the term $(1 - k s^2)$ to zero:\n$$\n1 - k s^2 = 0 \\implies s^2 = \\frac{1}{k}\n$$\nThe optimal scale parameter is $s_{\\mathrm{opt}} = \\sqrt{1/k}$. The maximum value of $G(s)$ is obtained by substituting $s_{\\mathrm{opt}}^2 = 1/k$ back into the function:\n$$\nG_{\\mathrm{max}} = G(s_{\\mathrm{opt}}) = \\left(\\frac{1}{k}\\right) \\exp\\left(-k \\cdot \\frac{1}{k}\\right) = \\frac{1}{k} \\exp(-1) = \\frac{1}{ke}\n$$\n\nNow, we apply this result to each of the two mechanisms.\n\n**1. Accurate Mechanism**\nThe function to maximize is:\n$$\nF_{\\mathrm{acc}}(s) = \\frac{a_{\\mathrm{acc}}(s) s^2}{c_{\\mathrm{acc}}} = \\frac{\\exp(-\\lambda_{\\mathrm{acc}} s^2) s^2}{c_{\\mathrm{acc}}} = \\frac{1}{c_{\\mathrm{acc}}} \\left( s^2 \\exp(-\\lambda_{\\mathrm{acc}} s^2) \\right)\n$$\nThis corresponds to our general form with $k = \\lambda_{\\mathrm{acc}}$. The optimization of $F_{\\mathrm{acc}}(s)$ is equivalent to optimizing $s^2 \\exp(-\\lambda_{\\mathrm{acc}} s^2)$.\nThe maximum value of $F_{\\mathrm{acc}}(s)$, denoted $F_{\\mathrm{acc, max}}$, is:\n$$\nF_{\\mathrm{acc, max}} = \\frac{1}{c_{\\mathrm{acc}}} \\left( \\frac{1}{\\lambda_{\\mathrm{acc}} e} \\right) = \\frac{1}{c_{\\mathrm{acc}} \\lambda_{\\mathrm{acc}} e}\n$$\nSubstituting the given values $c_{\\mathrm{acc}} = 5$ and $\\lambda_{\\mathrm{acc}} = 4$:\n$$\nF_{\\mathrm{acc, max}} = \\frac{1}{5 \\cdot 4 \\cdot e} = \\frac{1}{20e}\n$$\n\n**2. Cheap Mechanism**\nThe function to maximize is:\n$$\nF_{\\mathrm{ch}}(s) = \\frac{a_{\\mathrm{ch}}(s) s^2}{c_{\\mathrm{ch}}} = \\frac{r \\exp(-\\mu s^2) s^2}{c_{\\mathrm{ch}}} = \\frac{r}{c_{\\mathrm{ch}}} \\left( s^2 \\exp(-\\mu s^2) \\right)\n$$\nThis corresponds to our general form with $k = \\mu$. The optimization of $F_{\\mathrm{ch}}(s)$ is equivalent to optimizing $s^2 \\exp(-\\mu s^2)$, with the result scaled by the prefactor $r/c_{\\mathrm{ch}}$.\nThe maximum value of $F_{\\mathrm{ch}}(s)$, denoted $F_{\\mathrm{ch, max}}$, is:\n$$\nF_{\\mathrm{ch, max}} = \\frac{r}{c_{\\mathrm{ch}}} \\left( \\frac{1}{\\mu e} \\right) = \\frac{r}{c_{\\mathrm{ch}} \\mu e}\n$$\nSubstituting the given values $c_{\\mathrm{ch}} = 0.2$, $r = 0.6$, and $\\mu = 9$:\n$$\nF_{\\mathrm{ch, max}} = \\frac{0.6}{0.2 \\cdot 9 \\cdot e} = \\frac{0.6}{1.8 e} = \\frac{1}{3e}\n$$\n\n**3. Ratio of Maximized Efficiencies**\nThe problem asks for the ratio of the maximized effective sample size of the cheap mechanism to that of the accurate mechanism.\n$$\n\\text{Ratio} = \\frac{F_{\\mathrm{ch, max}}}{F_{\\mathrm{acc, max}}} = \\frac{ \\frac{r}{c_{\\mathrm{ch}} \\mu e} }{ \\frac{1}{c_{\\mathrm{acc}} \\lambda_{\\mathrm{acc}} e} }\n$$\nThe factor of $1/e$ cancels from the numerator and denominator:\n$$\n\\text{Ratio} = \\frac{r c_{\\mathrm{acc}} \\lambda_{\\mathrm{acc}}}{c_{\\mathrm{ch}} \\mu}\n$$\nNow, we substitute the numerical values:\n$$\n\\text{Ratio} = \\frac{0.6 \\cdot 5 \\cdot 4}{0.2 \\cdot 9} = \\frac{12}{1.8} = \\frac{120}{18} = \\frac{20}{3}\n$$\nAs a decimal, this is $20/3 = 6.666...$.\nThe problem asks to round the answer to three significant figures.\n$$\n\\text{Ratio} \\approx 6.67\n$$\nThis result indicates that the cheap, surrogate-assisted sampling strategy is approximately $6.67$ times more efficient than the standard accurate method under the given modeling assumptions, when both are optimally tuned.",
            "answer": "$$\n\\boxed{6.67}\n$$"
        },
        {
            "introduction": "Many critical phenomena in materials science, such as phase transitions or defect migration, are governed by rare events that involve crossing substantial energy barriers. This problem  explores the concept of metastability, the primary obstacle to simulating such systems, where a sampler can become trapped for exponentially long times in a local energy minimum. By applying foundational principles from statistical mechanics, you will connect the microscopic details of the Metropolis-Hastings acceptance rule to the macroscopic kinetics of barrier crossing. This exercise builds the essential conceptual framework for diagnosing sampling bottlenecks in complex systems and understanding the motivation behind advanced methods designed to accelerate the exploration of rugged energy landscapes.",
            "id": "3463563",
            "problem": "In computational materials science, metastability arises when sampling thermodynamic states across phases separated by substantial free-energy barriers. Consider a one-dimensional effective reaction coordinate $x \\in \\mathbb{R}$ describing a structural phase transition in a crystalline alloy, with energy landscape $U(x)$ exhibiting two symmetric wells at $x=-1$ and $x=1$ that represent two phases. Assume $U(-1)=U(1)=0$, and there is a single saddle (barrier top) at $x=0$ with $U(0)=\\Delta>0$, with $U''(-1)>0$, $U''(1)>0$, and $U''(0)<0$. The target distribution is the Boltzmann–Gibbs distribution $\\pi_\\beta(x)\\propto \\exp(-\\beta U(x))$ at inverse temperature $\\beta>0$. You run a Metropolis–Hastings sampler with a symmetric Gaussian random-walk proposal $q(x,y)=\\mathcal{N}(x,\\sigma^2)$ of fixed variance $\\sigma^2>0$.\n\nUsing only foundational principles of statistical mechanics and Markov chain Monte Carlo—namely, the Boltzmann–Gibbs law, detailed balance for the Metropolis–Hastings transition kernel, and rare-event reasoning consistent with Eyring–Kramers asymptotics for barrier crossing—determine which of the following statements are correct in the low-temperature limit $\\beta\\to\\infty$ with $\\sigma$ fixed and small. Select all that apply.\n\nA. In the limit $\\beta\\to\\infty$ with $\\sigma$ fixed and small, the expected first hitting time from a small neighborhood of $x=-1$ to a small neighborhood of $x=1$, denoted $\\mathbb{E}[\\tau_{-1\\to 1}]$, grows as $\\mathbb{E}[\\tau_{-1\\to 1}] \\sim C(\\sigma,U)\\,\\exp(\\beta \\Delta)$, where $C(\\sigma,U)$ is a prefactor depending on $\\sigma$ and the local curvatures of $U(x)$ near the well minimum and the saddle, consistent with Eyring–Kramers reasoning.\n\nB. Decreasing the proposal variance $\\sigma^2$ eliminates metastability because it increases acceptance probabilities, making barrier-crossing times scale at most polynomially in $\\beta$.\n\nC. For symmetric random-walk proposals with small $\\sigma$, the acceptance probability of a proposed move that increases the energy by an amount on the order of $\\Delta$ decays like $\\exp(-\\beta \\Delta)$, which explains the exponential suppression of barrier-crossing transitions.\n\nD. If the two wells have equal depths, the stationary distribution assigns equal mass to neighborhoods of $x=-1$ and $x=1$, so the long-time empirical fraction of visits to each well converges to $1/2$ as time grows, even though the expected transition time between wells grows like $\\exp(\\beta \\Delta)$.\n\nE. In a three-well landscape where any path from the left well to the right well must cross two saddles of heights $\\Delta_1$ and $\\Delta_2$ above the left well, the leading exponential factor in the expected left-to-right hitting time is governed by $\\exp\\!\\big(\\beta \\max\\{\\Delta_1,\\Delta_2\\}\\big)$ in the limit $\\beta\\to\\infty$.",
            "solution": "The problem statement is scientifically and mathematically sound. It describes a canonical one-dimensional double-well potential problem, a cornerstone model for studying phase transitions and metastability. The use of the Boltzmann-Gibbs distribution and the Metropolis-Hastings algorithm with a symmetric Gaussian proposal is standard practice in computational statistical mechanics and materials science. The question probes the asymptotic behavior in the well-understood low-temperature limit. All terms are well-defined, and the premises are self-consistent and sufficient for a rigorous analysis.\n\nThe core principles for the analysis are as follows:\n$1$. The target stationary distribution is the Boltzmann-Gibbs distribution, $\\pi_\\beta(x) = Z^{-1} \\exp(-\\beta U(x))$, where $Z = \\int_{-\\infty}^{\\infty} \\exp(-\\beta U(x)) dx$ is the partition function and $\\beta$ is the inverse temperature. In the low-temperature limit ($\\beta \\to \\infty$), this distribution becomes sharply peaked at the global minima of the potential energy function $U(x)$.\n$2$. The Metropolis-Hastings algorithm with a symmetric proposal kernel $q(x,y) = q(y,x)$ uses the acceptance probability $\\alpha(x,y) = \\min\\left(1, \\frac{\\pi_\\beta(y)}{\\pi_\\beta(x)}\\right) = \\min\\left(1, \\exp(-\\beta [U(y)-U(x)])\\right)$. This rule ensures that the generated Markov chain satisfies the detailed balance condition, $\\pi_\\beta(x) P(x,y) = \\pi_\\beta(y) P(y,x)$, where $P(x,y) = q(y,x)\\alpha(x,y)$ is the transition kernel for $x \\neq y$. This guarantees convergence to $\\pi_\\beta(x)$.\n$3$. Metastability occurs when the state space is divided into regions of low energy (wells) separated by high-energy barriers. In the low-temperature limit, transitions between these wells become rare events. The expected time for such a transition, known as the mean first passage time (MFPT) or expected hitting time, is typically governed by an Arrhenius-like law, scaling exponentially with the barrier height. This is the essence of Eyring-Kramers theory.\n\nWith these principles, we evaluate each statement.\n\nA. In the limit $\\beta\\to\\infty$ with $\\sigma$ fixed and small, the expected first hitting time from a small neighborhood of $x=-1$ to a small neighborhood of $x=1$, denoted $\\mathbb{E}[\\tau_{-1\\to 1}]$, grows as $\\mathbb{E}[\\tau_{-1\\to 1}] \\sim C(\\sigma,U)\\,\\exp(\\beta \\Delta)$, where $C(\\sigma,U)$ is a prefactor depending on $\\sigma$ and the local curvatures of $U(x)$ near the well minimum and the saddle, consistent with Eyring–Kramers reasoning.\n\nThis statement is a direct application of the theory of rare events for Markov chains, which is the discrete-time analogue of Eyring-Kramers theory for continuous-time stochastic processes like Langevin dynamics. The transition from the well at $x=-1$ to the well at $x=1$ requires crossing the energy barrier of height $\\Delta = U(0) - U(-1)$. The probability of observing states with energy $E$ above the minimum is proportional to the Boltzmann factor $\\exp(-\\beta E)$. Therefore, the rate of transitions across the barrier is expected to be proportional to $\\exp(-\\beta \\Delta)$. The expected time for the transition is the reciprocal of the rate, and thus it must scale as $\\exp(\\beta \\Delta)$. The prefactor $C(\\sigma, U)$ captures the dynamical details of the process, including the attempt frequency and the geometric properties of the potential landscape. For a random-walk Metropolis sampler, this prefactor depends on the proposal step size $\\sigma$ and the local curvatures of the potential at the minimum (e.g., $U''(-1)$) and the saddle point ($U''(0)$). A smaller $\\sigma$ generally leads to a slower diffusive exploration of the well, increasing the prefactor $C$ and thus the overall transition time, but it does not change the dominant exponential scaling. This statement accurately describes the asymptotic behavior of the transition time.\n**Verdict: Correct**\n\nB. Decreasing the proposal variance $\\sigma^2$ eliminates metastability because it increases acceptance probabilities, making barrier-crossing times scale at most polynomially in $\\beta$.\n\nThis statement is incorrect. While it is true that decreasing the proposal step size $\\sigma$ will increase the acceptance probability for typical moves *within* a potential well (as $U(y)-U(x)$ will be smaller), it does not eliminate the fundamental problem of the energy barrier. Metastability is characterized by the exponential scaling of the transition time with $\\beta$, i.e., $\\mathbb{E}[\\tau] \\sim \\exp(\\beta\\Delta)$. Eliminating it would require the scaling to become polynomial in $\\beta$. Decreasing $\\sigma$ makes the random walk more localized. To cross the barrier, the sampler must diffuse from the bottom of the well near $x=-1$ up to the saddle at $x=0$. A smaller $\\sigma$ means this diffusion process takes more steps and becomes slower. The exponential dependence on $\\beta\\Delta$ is a thermodynamic feature tied to the energy landscape and the Boltzmann distribution, which the Metropolis-Hastings algorithm is designed to sample. It is not removed by changing the proposal step size. In fact, making $\\sigma$ too small is a well-known cause of poor sampling efficiency for multi-well systems, as it worsens the prefactor in the exponential scaling law.\n**Verdict: Incorrect**\n\nC. For symmetric random-walk proposals with small $\\sigma$, the acceptance probability of a proposed move that increases the energy by an amount on the order of $\\Delta$ decays like $\\exp(-\\beta \\Delta)$, which explains the exponential suppression of barrier-crossing transitions.\n\nThis statement correctly identifies the microscopic origin of the macroscopic exponential slowdown. According to the Metropolis-Hastings acceptance rule for a symmetric proposal, a move from a state $x$ to $y$ is accepted with probability $\\alpha(x,y) = \\min(1, \\exp(-\\beta[U(y)-U(x)]))$. If a move is proposed that attempts to surmount a significant fraction of the energy barrier, such that the energy change is $\\delta U = U(y)-U(x) \\approx \\Delta$, the acceptance probability for that move will be approximately $\\exp(-\\beta\\Delta)$. Any successful transition path from one well to the other must involve visiting states of high energy near the saddle point. The acceptance rule penalizes moves to higher energy states with this exponential factor. This penalty is the fundamental mechanism that ensures the resulting Markov chain samples from the Boltzmann distribution, and it is precisely this factor that leads to the exponential rarity of barrier-crossing events. Thus, the statement provides a correct mechanistic explanation for the observed metastability.\n**Verdict: Correct**\n\nD. If the two wells have equal depths, the stationary distribution assigns equal mass to neighborhoods of $x=-1$ and $x=1$, so the long-time empirical fraction of visits to each well converges to $1/2$ as time grows, even though the expected transition time between wells grows like $\\exp(\\beta \\Delta)$.\n\nThis statement correctly distinguishes between the properties of the stationary distribution (an equilibrium concept) and the dynamics of convergence (a kinetic concept). The stationary distribution is $\\pi_\\beta(x) \\propto \\exp(-\\beta U(x))$. The problem states the wells are at $x=-1$ and $x=1$ and have equal depths, $U(-1)=U(1)=0$. The description of the wells as \"symmetric\" with a saddle at $x=0$ implies that the potential function is symmetric, i.e., $U(x)=U(-x)$. Consequently, the stationary distribution $\\pi_\\beta(x)$ is also symmetric. In the limit $\\beta\\to\\infty$, the probability mass concentrates in two regions around $x=-1$ and $x=1$. By symmetry, the total probability mass in a neighborhood of $x=-1$ must equal the mass in a corresponding neighborhood of $x=1$. Since these two wells contain almost all the probability mass for large $\\beta$, the mass in each is approximately $1/2$. The ergodic theorem for Markov chains states that the long-time fraction of visits to any region converges to the stationary probability of that region. Therefore, the fraction of visits to each well will converge to $1/2$. This equilibrium property holds regardless of how slow the transitions between the wells are. The long transition time $\\mathbb{E}[\\tau_{-1\\to 1}] \\sim \\exp(\\beta\\Delta)$ is a measure of how long one must wait to see this equilibrium behavior manifest in the form of transitions.\n**Verdict: Correct**\n\nE. In a three-well landscape where any path from the left well to the right well must cross two saddles of heights $\\Delta_1$ and $\\Delta_2$ above the left well, the leading exponential factor in the expected left-to-right hitting time is governed by $\\exp\\!\\big(\\beta \\max\\{\\Delta_1,\\Delta_2\\}\\big)$ in the limit $\\beta\\to\\infty$.\n\nThis statement describes the principle of the rate-limiting step in a multi-barrier system. Let the energy of the left well be $U_L=0$. The saddles are at energies $\\Delta_1$ and $\\Delta_2$. To travel from the left well to the right well, the system must traverse a path in configuration space. The thermodynamically most difficult part of this path is surmounting the highest energy barrier along the way. The probability of finding the system at an energy $E$ above the ground state is proportional to $\\exp(-\\beta E)$. The rate of a process is limited by its most improbable step. For the transition from left to right, the system must, at some point, populate states at an energy level corresponding to the highest saddle on the path, which has an energy of $\\max\\{\\Delta_1, \\Delta_2\\}$ relative to the starting well. The rate of the overall transition will be dominated by the probability of reaching this highest-energy saddle, so the rate scales as $\\exp(-\\beta \\max\\{\\Delta_1, \\Delta_2\\})$. The expected hitting time is inversely proportional to this rate. Therefore, its leading exponential factor is $\\exp(\\beta \\max\\{\\Delta_1, \\Delta_2\\})$. This is a standard result in transition state theory and the analysis of rare events in complex landscapes.\n**Verdict: Correct**",
            "answer": "$$\\boxed{ACDE}$$"
        }
    ]
}