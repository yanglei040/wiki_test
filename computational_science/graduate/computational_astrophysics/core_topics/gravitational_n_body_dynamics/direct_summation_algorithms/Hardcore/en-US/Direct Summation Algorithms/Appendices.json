{
    "hands_on_practices": [
        {
            "introduction": "The first step in developing any simulation code is verification: ensuring it correctly implements the underlying physical laws. This practice guides you through one of the most fundamental sanity checks for an $N$-body code, which is based on the conservation of momentum. You will implement a direct-summation integrator and use it to demonstrate that if Newton's third law (action-reaction symmetry) is respected, the system's center of mass follows an inertial trajectory, and that even a small violation of this law leads to a detectable, unphysical drift .",
            "id": "3508414",
            "problem": "You are tasked with designing and implementing a direct-summation gravitational $N$-body integrator that quantitatively tests conservation of the system’s center-of-mass motion as a sanity check for force summation correctness. Begin from Newton’s laws of motion and the definition of the center of mass, and use only internal gravitational interactions. Using this fundamental base, derive the fact that the center of mass of a closed system with only internal forces follows inertial motion, and use that theoretical result to define a measurable deviation for a numerical simulation. Your program must directly implement a pairwise force computation (no treecodes or multipole expansions), a time integrator, and a diagnostic that compares the simulated center-of-mass trajectory against the expected inertial trajectory.\n\nThe physical model is Newtonian gravity with Plummer softening, with total gravitational acceleration on particle $i$ due to particle $j$ constructed from the softened Newtonian pairwise law and added over all $j\\neq i$. Use the gravitational constant $G$ in the International System of Units (SI) and express all distances in meters, masses in kilograms, and times in seconds.\n\nYour algorithm must implement two force-accumulation modes:\n- A correct symmetric pairwise accumulation that enforces equal-and-opposite pair contributions for each interacting pair, consistent with Newton’s third law.\n- A perturbed mode that deliberately violates action–reaction symmetry by a small dimensionless parameter $\\eta$ to mimic a buggy force summation. In this mode, for every unordered pair $\\{i,j\\}$, multiply the pair’s contribution to particle $i$’s acceleration by $(1+\\eta)$ and to particle $j$’s acceleration by $(1-\\eta)$.\n\nTime integration must use a symplectic leapfrog scheme. Initialize from given positions and velocities at time $t=0$, and advance in uniform time steps $\\Delta t$ for $n_{\\mathrm{steps}}$ steps. At each step, compute the system’s center-of-mass position $\\mathbf{R}_{\\mathrm{cm}}(t)$ and compare it to the inertial reference trajectory predicted from the initial center-of-mass position $\\mathbf{R}_{\\mathrm{cm}}(0)$ and initial center-of-mass velocity $\\mathbf{V}_{\\mathrm{cm}}(0)$. Define the deviation at time $t$ as the Euclidean norm of the vector difference between the simulated and inertial-reference center-of-mass positions. Your program must report, for each test case, the maximum deviation over the full simulation time.\n\nAll outputs must be reported in SI units. The maximum deviation must be expressed in meters. Use radians implicitly for any vector operations; no angles are directly required. Your program must format each reported value as a decimal float in scientific notation with $6$ significant digits.\n\nTest Suite. Implement the following four test cases; for each case, compute the maximum deviation of the system’s center of mass from its expected inertial trajectory. For all cases, use the gravitational constant $G=6.67430\\times 10^{-11}\\ \\mathrm{m^{3}\\ kg^{-1}\\ s^{-2}}$ and the Plummer softening length $\\epsilon$ provided per case.\n\n- Case $1$ (happy path, correct symmetry):\n  - $N=3$\n  - Masses $\\mathbf{m}=[5\\times 10^{24},\\ 6\\times 10^{24},\\ 7\\times 10^{24}]\\ \\mathrm{kg}$\n  - Initial positions $\\mathbf{r}(0)$ in meters:\n    - $\\mathbf{r}_1=[0,\\ 0,\\ 0]$\n    - $\\mathbf{r}_2=[2\\times 10^{9},\\ 0,\\ 0]$\n    - $\\mathbf{r}_3=[-1\\times 10^{9},\\ 1.5\\times 10^{9},\\ 0]$\n  - Initial velocities $\\mathbf{v}(0)$ in meters per second:\n    - $\\mathbf{v}_1=[0,\\ 300,\\ 0]$\n    - $\\mathbf{v}_2=[0,\\ -100,\\ 0]$\n    - $\\mathbf{v}_3=[50,\\ 0,\\ 0]$\n  - Time step $\\Delta t=2000\\ \\mathrm{s}$, number of steps $n_{\\mathrm{steps}}=1000$\n  - Softening $\\epsilon=10^{6}\\ \\mathrm{m}$\n  - Symmetry perturbation parameter $\\eta=0$\n\n- Case $2$ (action–reaction violation, detect drift):\n  - Identical to Case $1$ except use $\\eta=1\\times 10^{-5}$\n\n- Case $3$ (boundary condition $N=1$):\n  - $N=1$\n  - Masses $\\mathbf{m}=[1\\times 10^{26}]\\ \\mathrm{kg}$\n  - Initial positions $\\mathbf{r}(0)$ in meters:\n    - $\\mathbf{r}_1=[1\\times 10^{8},\\ -2\\times 10^{8},\\ 0]$\n  - Initial velocities $\\mathbf{v}(0)$ in meters per second:\n    - $\\mathbf{v}_1=[1200,\\ -500,\\ 0]$\n  - Time step $\\Delta t=1000\\ \\mathrm{s}$, number of steps $n_{\\mathrm{steps}}=2000$\n  - Softening $\\epsilon=10^{6}\\ \\mathrm{m}$\n  - Symmetry perturbation parameter $\\eta=0$\n\n- Case $4$ (zero total momentum, stationary center of mass expected):\n  - $N=4$\n  - Masses $\\mathbf{m}=[2\\times 10^{25},\\ 3\\times 10^{25},\\ 4\\times 10^{25},\\ 5\\times 10^{25}]\\ \\mathrm{kg}$\n  - Initial positions $\\mathbf{r}(0)$ in meters:\n    - $\\mathbf{r}_1=[1\\times 10^{9},\\ 0,\\ 0]$\n    - $\\mathbf{r}_2=[-1\\times 10^{9},\\ 1\\times 10^{9},\\ 0]$\n    - $\\mathbf{r}_3=[0,\\ -1\\times 10^{9},\\ 0]$\n    - $\\mathbf{r}_4=[2\\times 10^{9},\\ 2\\times 10^{9},\\ 0]$\n  - Initial velocities $\\mathbf{v}(0)$ in meters per second:\n    - $\\mathbf{v}_1=[200,\\ 0,\\ 0]$\n    - $\\mathbf{v}_2=[-100,\\ 100,\\ 0]$\n    - $\\mathbf{v}_3=[0,\\ -50,\\ 0]$\n    - $\\mathbf{v}_4=[-20,\\ -20,\\ 0]$\n  - Time step $\\Delta t=1500\\ \\mathrm{s}$, number of steps $n_{\\mathrm{steps}}=1200$\n  - Softening $\\epsilon=5\\times 10^{5}\\ \\mathrm{m}$\n  - Symmetry perturbation parameter $\\eta=0$\n\nRequired final output format. Your program should produce a single line of output containing the four maximum deviations, in meters, each formatted in scientific notation with $6$ significant digits, as a comma-separated list enclosed in square brackets. For example, the output must have the form \"[x,y,z,w]\" where each of $x$, $y$, $z$, and $w$ is a float written in scientific notation with $6$ significant digits and no units. No other text should be printed.",
            "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a unique solution. It requests the design and implementation of an $N$-body integrator to test the conservation of center-of-mass motion, a direct consequence of Newton's third law, providing a fundamental sanity check for the force summation algorithm. We will proceed with a full solution.\n\n### Theoretical Foundation: Center-of-Mass Motion in a Closed System\n\nThe foundation of this problem lies in Newtonian mechanics. We begin by defining the center of mass (CM) of a system of $N$ particles, where particle $i$ has mass $m_i$ and position vector $\\mathbf{r}_i$.\n\nThe total mass of the system is $M_{\\mathrm{tot}} = \\sum_{i=1}^{N} m_i$.\nThe position of the center of mass, $\\mathbf{R}_{\\mathrm{cm}}$, is the mass-weighted average of the particle positions:\n$$\n\\mathbf{R}_{\\mathrm{cm}}(t) = \\frac{1}{M_{\\mathrm{tot}}} \\sum_{i=1}^{N} m_i \\mathbf{r}_i(t)\n$$\n\nDifferentiating with respect to time $t$ gives the velocity of the center of mass, $\\mathbf{V}_{\\mathrm{cm}}$:\n$$\n\\mathbf{V}_{\\mathrm{cm}}(t) = \\frac{d\\mathbf{R}_{\\mathrm{cm}}}{dt} = \\frac{1}{M_{\\mathrm{tot}}} \\sum_{i=1}^{N} m_i \\frac{d\\mathbf{r}_i}{dt} = \\frac{1}{M_{\\mathrm{tot}}} \\sum_{i=1}^{N} m_i \\mathbf{v}_i(t)\n$$\nThe quantity $\\sum m_i \\mathbf{v}_i$ is the total momentum of the system, $\\mathbf{P}_{\\mathrm{tot}}$, so $\\mathbf{P}_{\\mathrm{tot}} = M_{\\mathrm{tot}} \\mathbf{V}_{\\mathrm{cm}}$.\n\nDifferentiating a second time yields the acceleration of the center of mass, $\\mathbf{A}_{\\mathrm{cm}}$:\n$$\n\\mathbf{A}_{\\mathrm{cm}}(t) = \\frac{d\\mathbf{V}_{\\mathrm{cm}}}{dt} = \\frac{1}{M_{\\mathrm{tot}}} \\sum_{i=1}^{N} m_i \\frac{d\\mathbf{v}_i}{dt} = \\frac{1}{M_{\\mathrm{tot}}} \\sum_{i=1}^{N} m_i \\mathbf{a}_i(t)\n$$\n\nAccording to Newton's second law, the term $m_i \\mathbf{a}_i$ is equal to the total force acting on particle $i$, $\\mathbf{F}_i^{\\mathrm{tot}}$. For a closed system with only internal forces, this force is the vector sum of forces from all other particles $j$ in the system:\n$$\nm_i \\mathbf{a}_i = \\mathbf{F}_i^{\\mathrm{tot}} = \\sum_{j=1, j\\neq i}^{N} \\mathbf{F}_{ij}\n$$\nwhere $\\mathbf{F}_{ij}$ is the force exerted on particle $i$ by particle $j$. Substituting this into the equation for $\\mathbf{A}_{\\mathrm{cm}}$ gives:\n$$\nM_{\\mathrm{tot}} \\mathbf{A}_{\\mathrm{cm}} = \\sum_{i=1}^{N} \\mathbf{F}_i^{\\mathrm{tot}} = \\sum_{i=1}^{N} \\sum_{j=1, j\\neq i}^{N} \\mathbf{F}_{ij}\n$$\n\nThe crucial step is to apply Newton's third law (the law of action and reaction), which states that forces between any two particles are equal and opposite: $\\mathbf{F}_{ij} = -\\mathbf{F}_{ji}$. The double summation on the right can be viewed as a sum over all interacting pairs $\\{i,j\\}$. For each pair, the sum contains both $\\mathbf{F}_{ij}$ and $\\mathbf{F}_{ji}$.\n$$\n\\sum_{i=1}^{N} \\sum_{j=1, j\\neq i}^{N} \\mathbf{F}_{ij} = \\sum_{i  j} (\\mathbf{F}_{ij} + \\mathbf{F}_{ji})\n$$\nBy Newton's third law, each term $(\\mathbf{F}_{ij} + \\mathbf{F}_{ji})$ is identically zero. Therefore, the total internal force on the system is zero:\n$$\n\\sum_{i=1}^{N} \\mathbf{F}_i^{\\mathrm{tot}} = \\mathbf{0}\n$$\nThis implies that $M_{\\mathrm{tot}} \\mathbf{A}_{\\mathrm{cm}} = \\mathbf{0}$, and thus $\\mathbf{A}_{\\mathrm{cm}} = \\mathbf{0}$. This is the law of conservation of momentum.\n\nIf $\\mathbf{A}_{\\mathrm{cm}} = \\mathbf{0}$, then the velocity $\\mathbf{V}_{\\mathrm{cm}}$ is constant and equal to its initial value, $\\mathbf{V}_{\\mathrm{cm}}(0)$. Integrating once more, we find that the center of mass must follow an inertial trajectory (a straight line at constant velocity):\n$$\n\\mathbf{R}_{\\mathrm{cm}}(t) = \\mathbf{R}_{\\mathrm{cm}}(0) + \\mathbf{V}_{\\mathrm{cm}}(0) t\n$$\nThis equation defines the reference trajectory against which the numerical simulation is tested.\n\n### Algorithmic Design and Numerical Implementation\n\nThe task now is to translate this physical principle into a numerical algorithm.\n\n#### 1. Force Calculation with Plummer Softening\n\nThe gravitational force on particle $i$ due to particle $j$ is given by Newton's law of gravitation, modified with Plummer softening to avoid numerical singularities when particles are very close. The force vector is:\n$$\n\\mathbf{F}_{ij} = G m_i m_j \\frac{\\mathbf{r}_j - \\mathbf{r}_i}{\\left( \\|\\mathbf{r}_j - \\mathbf{r}_i\\|^2 + \\epsilon^2 \\right)^{3/2}}\n$$\nwhere $G$ is the gravitational constant and $\\epsilon$ is the softening length. The acceleration of particle $i$ is $\\mathbf{a}_i = \\frac{1}{m_i} \\sum_{j \\neq i} \\mathbf{F}_{ij}$.\n\n#### 2. Action-Reaction Symmetry Violation\n\nThe core of the diagnostic test involves two modes for force accumulation:\n- **Symmetric Mode ($\\eta = 0$):** In a direct summation loop over unique pairs $(i, j)$ with $i  j$, we calculate $\\mathbf{F}_{ij}$ and add it to the total force on particle $i$, and add $\\mathbf{F}_{ji} = -\\mathbf{F}_{ij}$ to the total force on particle $j$. This correctly implements Newton's third law. Total system momentum is conserved, up to numerical error.\n- **Perturbed Mode ($\\eta \\neq 0$):** The problem specifies a deliberate violation of symmetry. For each pair $\\{i,j\\}$ (we use the convention $ij$ to make the choice deterministic), the contribution to particle $i$'s acceleration is scaled by $(1+\\eta)$ and to particle $j$'s acceleration by $(1-\\eta)$. Let $\\mathbf{a}_{ij}$ be the acceleration on $i$ from $j$.\n  - The contribution to $i$'s total acceleration becomes $(1+\\eta)\\mathbf{a}_{ij}$.\n  - The contribution to $j$'s total acceleration becomes $(1-\\eta)\\mathbf{a}_{ji}$.\nThe sum of forces on the pair is no longer zero: $m_i(1+\\eta)\\mathbf{a}_{ij} + m_j(1-\\eta)\\mathbf{a}_{ji} = (1+\\eta)\\mathbf{F}_{ij} + (1-\\eta)\\mathbf{F}_{ji} = ((1+\\eta) - (1-\\eta))\\mathbf{F}_{ij} = 2\\eta\\mathbf{F}_{ij} \\neq \\mathbf{0}$. This creates a net internal force that causes the system's center of mass to accelerate, leading to a measurable deviation from its inertial path.\n\n#### 3. Time Integration: Symplectic Leapfrog (KDK)\n\nWe use a second-order symplectic leapfrog integrator, specifically the \"Kick-Drift-Kick\" (KDK) formulation. It is well-suited for gravitational dynamics due to its good long-term energy and momentum conservation properties (for the symmetric case). The algorithm proceeds as follows, starting with initial positions $\\mathbf{r}(0)$ and velocities $\\mathbf{v}(0)$ at time $t=0$:\n\n1.  **Initial Kick (half-step):** Calculate initial accelerations $\\mathbf{a}(0)$ from $\\mathbf{r}(0)$. Advance velocities by a half time step $\\Delta t/2$:\n    $$\n    \\mathbf{v}(\\Delta t/2) = \\mathbf{v}(0) + \\mathbf{a}(0) \\frac{\\Delta t}{2}\n    $$\n2.  **Main Loop:** For $n = 0, 1, \\dots, n_{\\mathrm{steps}}-1$:\n    a. **Drift (full-step):** Update positions using the half-step velocities:\n       $$\n       \\mathbf{r}((n+1)\\Delta t) = \\mathbf{r}(n\\Delta t) + \\mathbf{v}((n+1/2)\\Delta t) \\Delta t\n       $$\n    b. **Kick (full-step):** Compute new accelerations $\\mathbf{a}((n+1)\\Delta t)$ at the new positions. Update velocities to the next half-step:\n       $$\n       \\mathbf{v}((n+3/2)\\Delta t) = \\mathbf{v}((n+1/2)\\Delta t) + \\mathbf{a}((n+1)\\Delta t) \\Delta t\n       $$\n\n#### 4. Diagnostic Measurement\n\nAt each full time step $t_k = k \\Delta t$ (for $k = 1, \\dots, n_{\\mathrm{steps}}$), after the Drift step, we have the simulated positions $\\mathbf{r}(t_k)$.\n1.  Compute the simulated center-of-mass position:\n    $$\n    \\mathbf{R}_{\\mathrm{cm, sim}}(t_k) = \\frac{1}{M_{\\mathrm{tot}}} \\sum_{i=1}^{N} m_i \\mathbf{r}_i(t_k)\n    $$\n2.  Compute the theoretical inertial reference position:\n    $$\n    \\mathbf{R}_{\\mathrm{cm, ref}}(t_k) = \\mathbf{R}_{\\mathrm{cm}}(0) + \\mathbf{V}_{\\mathrm{cm}}(0) t_k\n    $$\n3.  Calculate the deviation $D(t_k)$ as the Euclidean norm of the difference:\n    $$\n    D(t_k) = \\|\\mathbf{R}_{\\mathrm{cm, sim}}(t_k) - \\mathbf{R}_{\\mathrm{cm, ref}}(t_k)\\|\n    $$\nThe final reported value for each test case is the maximum deviation observed over the entire simulation: $\\max_{k} D(t_k)$.\n\nThis comprehensive approach allows for a direct, quantitative test of the consequences of preserving or violating Newton's third law within a numerical simulation. The case $N=1$ serves as a control, as there are no internal forces, and the deviation should be zero to within machine precision. The case with $\\mathbf{V}_{\\mathrm{cm}}(0)=\\mathbf{0}$ tests for spurious motion of an initially stationary center of mass.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to execute all test cases and print the final result.\n    \"\"\"\n    G = 6.67430e-11  # SI units: m^3 kg^-1 s^-2\n\n    test_cases = [\n        # Case 1 (happy path, correct symmetry)\n        {\n            \"N\": 3,\n            \"masses\": np.array([5e24, 6e24, 7e24]),\n            \"positions\": np.array([\n                [0.0, 0.0, 0.0],\n                [2e9, 0.0, 0.0],\n                [-1e9, 1.5e9, 0.0]\n            ]),\n            \"velocities\": np.array([\n                [0.0, 300.0, 0.0],\n                [0.0, -100.0, 0.0],\n                [50.0, 0.0, 0.0]\n            ]),\n            \"dt\": 2000.0,\n            \"n_steps\": 1000,\n            \"epsilon\": 1e6,\n            \"eta\": 0.0,\n        },\n        # Case 2 (action–reaction violation, detect drift)\n        {\n            \"N\": 3,\n            \"masses\": np.array([5e24, 6e24, 7e24]),\n            \"positions\": np.array([\n                [0.0, 0.0, 0.0],\n                [2e9, 0.0, 0.0],\n                [-1e9, 1.5e9, 0.0]\n            ]),\n            \"velocities\": np.array([\n                [0.0, 300.0, 0.0],\n                [0.0, -100.0, 0.0],\n                [50.0, 0.0, 0.0]\n            ]),\n            \"dt\": 2000.0,\n            \"n_steps\": 1000,\n            \"epsilon\": 1e6,\n            \"eta\": 1e-5,\n        },\n        # Case 3 (boundary condition N=1)\n        {\n            \"N\": 1,\n            \"masses\": np.array([1e26]),\n            \"positions\": np.array([\n                [1e8, -2e8, 0.0]\n            ]),\n            \"velocities\": np.array([\n                [1200.0, -500.0, 0.0]\n            ]),\n            \"dt\": 1000.0,\n            \"n_steps\": 2000,\n            \"epsilon\": 1e6,\n            \"eta\": 0.0,\n        },\n        # Case 4 (zero total momentum)\n        {\n            \"N\": 4,\n            \"masses\": np.array([2e25, 3e25, 4e25, 5e25]),\n            \"positions\": np.array([\n                [1e9, 0.0, 0.0],\n                [-1e9, 1e9, 0.0],\n                [0.0, -1e9, 0.0],\n                [2e9, 2e9, 0.0]\n            ]),\n            \"velocities\": np.array([\n                [200.0, 0.0, 0.0],\n                [-100.0, 100.0, 0.0],\n                [0.0, -50.0, 0.0],\n                [-20.0, -20.0, 0.0]\n            ]),\n            \"dt\": 1500.0,\n            \"n_steps\": 1200,\n            \"epsilon\": 5e5,\n            \"eta\": 0.0,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        max_deviation = run_simulation(\n            masses=case[\"masses\"],\n            positions=case[\"positions\"],\n            velocities=case[\"velocities\"],\n            G=G,\n            dt=case[\"dt\"],\n            n_steps=case[\"n_steps\"],\n            epsilon=case[\"epsilon\"],\n            eta=case[\"eta\"]\n        )\n        results.append(f\"{max_deviation:.6e}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef calculate_accelerations(positions, masses, G, epsilon, eta):\n    \"\"\"\n    Calculates gravitational accelerations using direct summation.\n    \n    This function implements both symmetric force accumulation (eta=0) and a\n    perturbed mode that violates Newton's third law (eta != 0).\n    \"\"\"\n    n_particles = positions.shape[0]\n    accelerations = np.zeros((n_particles, 3))\n    \n    # Iterate over unique pairs of particles (i, j) where i  j\n    for i in range(n_particles):\n        for j in range(i + 1, n_particles):\n            # Vector from particle i to particle j\n            r_ij = positions[j] - positions[i]\n            \n            # Squared distance with softening\n            dist_sq = np.sum(r_ij**2)\n            \n            # Softened inverse cube law factor\n            inv_dist_cubed = (dist_sq + epsilon**2)**(-1.5)\n            \n            # Calculate acceleration contribution for each particle due to the other\n            acc_i_due_to_j = G * masses[j] * inv_dist_cubed * r_ij\n            acc_j_due_to_i = G * masses[i] * inv_dist_cubed * (-r_ij)\n            \n            # Apply symmetric or asymmetric force accumulation\n            # For a pair {i,j}, the contribution to i's acceleration is scaled by (1+eta)\n            # and to j's acceleration by (1-eta).\n            accelerations[i] += (1.0 + eta) * acc_i_due_to_j\n            accelerations[j] += (1.0 - eta) * acc_j_due_to_i\n            \n    return accelerations\n\ndef run_simulation(masses, positions, velocities, G, dt, n_steps, epsilon, eta):\n    \"\"\"\n    Runs a single N-body simulation and returns the max CM deviation.\n    \"\"\"\n    # Defensive copies to avoid modifying the original test case data\n    pos = np.copy(positions)\n    vel = np.copy(velocities)\n    masses_reshaped = masses.reshape(-1, 1)\n    \n    total_mass = np.sum(masses)\n    \n    # Calculate initial center-of-mass position and velocity\n    if total_mass  0:\n        R_cm_0 = np.sum(masses_reshaped * pos, axis=0) / total_mass\n        V_cm_0 = np.sum(masses_reshaped * vel, axis=0) / total_mass\n    else: # Handle case of zero total mass if needed, though not in tests\n        R_cm_0 = np.zeros(3)\n        V_cm_0 = np.zeros(3)\n\n    max_deviation = 0.0\n\n    # Leapfrog Integrator (KDK - Kick-Drift-Kick)\n    \n    # Initial Kick (half-step)\n    acc = calculate_accelerations(pos, masses, G, epsilon, eta)\n    vel += acc * (dt / 2.0)\n    \n    for step in range(n_steps):\n        # Drift (full step)\n        pos += vel * dt\n        \n        # Kick (full step)\n        acc = calculate_accelerations(pos, masses, G, epsilon, eta)\n        vel += acc * dt\n        \n        # --- Diagnostic Calculation ---\n        current_time = (step + 1) * dt\n        \n        # Calculate simulated center-of-mass position\n        R_cm_sim = np.sum(masses_reshaped * pos, axis=0) / total_mass\n        \n        # Calculate theoretical inertial reference position\n        R_cm_ref = R_cm_0 + V_cm_0 * current_time\n        \n        # Calculate deviation and update maximum\n        deviation = np.linalg.norm(R_cm_sim - R_cm_ref)\n        if deviation  max_deviation:\n            max_deviation = deviation\n            \n    return max_deviation\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Once an algorithm is verified to be physically correct, its numerical accuracy must be assessed. This exercise delves into the critical role of floating-point precision in the direct summation algorithm. By comparing the results of force accumulation in single versus double precision, you will quantify how round-off errors can compromise the accuracy of your simulation, a crucial consideration for problems involving large particle numbers or high dynamic range in forces .",
            "id": "3508472",
            "problem": "Consider a self-gravitating $N$-body system under Newtonian gravity modeled in dimensionless $N$-body units where $G = 1$. Let positions $\\mathbf{x}_i \\in \\mathbb{R}^3$ and velocities $\\mathbf{v}_i \\in \\mathbb{R}^3$ be stored in double precision (binary $64$). The gravitational acceleration on particle $i$ due to all other particles is defined by\n$$\n\\mathbf{a}_i = \\sum_{j \\ne i} m_j \\frac{\\mathbf{x}_j - \\mathbf{x}_i}{\\left(\\lVert \\mathbf{x}_j - \\mathbf{x}_i \\rVert^2 + \\epsilon^2\\right)^{3/2}},\n$$\nwhere $m_j$ is the mass of particle $j$, and $\\epsilon  0$ is a Plummer softening length used to prevent singular forces at zero separation. In a direct summation algorithm, the partial sums that accumulate $\\mathbf{a}_i$ over $j$ can be computed in either single precision (binary $32$) or double precision (binary $64$). The positions $\\mathbf{x}_i$ and velocities $\\mathbf{v}_i$ remain stored in double precision for both cases.\n\nYou are to quantify the numerical error introduced by performing the accumulation of the acceleration vector in single precision versus double precision, while all inputs (positions, velocities, masses, softening) are represented in double precision. For each test case described below, compute the following error metric:\n$$\nE_{\\max} = \\max_{i \\in \\{1,\\dots,N\\}} \\left( \\frac{\\lVert \\mathbf{a}_i^{(\\text{single})} - \\mathbf{a}_i^{(\\text{double})} \\rVert_2}{\\lVert \\mathbf{a}_i^{(\\text{double})} \\rVert_2} \\right),\n$$\nwith the following convention for near-zero denominators: if $\\lVert \\mathbf{a}_i^{(\\text{double})} \\rVert_2 \\le \\tau$, replace the ratio by the absolute error $\\lVert \\mathbf{a}_i^{(\\text{single})} - \\mathbf{a}_i^{(\\text{double})} \\rVert_2$. Use the threshold $\\tau = 10^{-300}$. The accelerations $\\mathbf{a}_i^{(\\text{single})}$ and $\\mathbf{a}_i^{(\\text{double})}$ must both use the same contribution formula above, with the only difference being that the accumulation of the sum over $j$ uses a single-precision accumulator for the former and a double-precision accumulator for the latter. Compute contributions in double precision and, for the single-precision accumulation, cast each contribution to single precision before addition.\n\nUse the following test suite. All random values must be generated deterministically using the provided seeds, and all quantities are dimensionless in $N$-body units:\n\n- Test case $1$ (general configuration, moderate $N$): $N = 32$. Positions $\\mathbf{x}_i$ are drawn independently from a uniform distribution on $[-1,1]^3$ with seed $12345$. Masses $m_i$ are drawn independently from a uniform distribution on $[0.5,1.5]$ with the same seed. Velocities $\\mathbf{v}_i$ are drawn independently from a uniform distribution on $[-0.1,0.1]^3$ with the same seed. Use softening $\\epsilon = 10^{-3}$.\n\n- Test case $2$ (pairwise symmetry and cancellation): $N = 20$. Generate $10$ base vectors $\\mathbf{b}_k$ uniformly from $[-0.5,0.5]^3$ with seed $777$, and form positions by pairing $\\mathbf{x}_{2k-1} = \\mathbf{b}_k$ and $\\mathbf{x}_{2k} = -\\mathbf{b}_k$ for $k = 1,\\dots,10$. Set all masses to $m_i = 1$. Velocities $\\mathbf{v}_i$ are drawn independently from a uniform distribution on $[-0.1,0.1]^3$ with the same seed. Use softening $\\epsilon = 10^{-3}$.\n\n- Test case $3$ (larger $N$ to stress accumulation): $N = 256$. Positions $\\mathbf{x}_i$ are drawn independently from a uniform distribution on $[-2,2]^3$ with seed $2021$. Set all masses to $m_i = 1$. Velocities $\\mathbf{v}_i$ are drawn independently from a uniform distribution on $[-0.01,0.01]^3$ with the same seed. Use softening $\\epsilon = 10^{-4}$.\n\n- Test case $4$ (dynamic range of contributions): $N = 64$. Place one particle at the origin with mass $m_1 = 10^6$. The remaining $63$ positions are drawn independently from a uniform distribution on $[0.1,1]^3$ with seed $42$. The remaining masses $m_i$ for $i \\ge 2$ are drawn independently from a uniform distribution on $[0.5,1.5]$ with the same seed. Velocities $\\mathbf{v}_i$ are drawn independently from a uniform distribution on $[-0.01,0.01]^3$ with the same seed. Use softening $\\epsilon = 10^{-2}$.\n\nYour program must implement the direct summation of accelerations for both accumulation precisions and compute $E_{\\max}$ for each test case. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in scientific notation with exactly six digits after the decimal point (e.g., $[1.234567\\text{e-}08,9.876543\\text{e-}07,\\dots]$). The outputs are dimensionless numbers in $N$-body units. No external input is allowed; all data must be generated within the program as specified.",
            "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded in the principles of Newtonian mechanics and numerical analysis, well-posed with all necessary parameters and conditions defined, and objective in its formulation. The task is to quantify the numerical error introduced by using single-precision floating-point arithmetic for the accumulation of gravitational accelerations in an $N$-body system, compared to a double-precision baseline.\n\nThe core of the problem lies in the direct summation calculation of the gravitational acceleration $\\mathbf{a}_i$ on a particle $i$:\n$$\n\\mathbf{a}_i = \\sum_{j \\ne i} m_j \\frac{\\mathbf{x}_j - \\mathbf{x}_i}{\\left(\\lVert \\mathbf{x}_j - \\mathbf{x}_i \\rVert^2 + \\epsilon^2\\right)^{3/2}}\n$$\nwhere $\\mathbf{x}_i$ are positions, $m_j$ are masses, and $\\epsilon$ is the Plummer softening length. The problem requires comparing two computational variants: $\\mathbf{a}_i^{(\\text{double})}$, where the summation is accumulated in double precision (binary64), and $\\mathbf{a}_i^{(\\text{single})}$, where it is accumulated in single precision (binary32). All input quantities ($\\mathbf{x}_i, m_j, \\epsilon$) are provided in double precision.\n\nThe fundamental principle being investigated is the accumulation of round-off error in floating-point summation. When adding a sequence of numbers, the finite precision of the representation can lead to a loss of accuracy. This is particularly pronounced when adding a small number to a large one (absorption) or when summing many terms of alternating sign (catastrophic cancellation). Single-precision arithmetic, having a smaller mantissa (typically $23$ bits) than double-precision ($52$ bits), is far more susceptible to these effects.\n\nThe algorithmic approach to solve this problem is as follows:\n\n1.  **System Initialization**: For each test case, the particle positions $\\mathbf{x}_i$ and masses $m_i$ are generated deterministically using the specified parameters and random seeds. This ensures reproducibility. The use of NumPy's `default_rng` provides a modern and reliable source of pseudo-random numbers.\n\n2.  **Vectorized Calculation of Force Contributions**: The direct summation algorithm has a computational complexity of $O(N^2)$, as it involves computing the interaction between all unique pairs of particles. To implement this efficiently, a vectorized approach is employed.\n    -   First, an $N \\times N \\times 3$ array of displacement vectors, $\\Delta\\mathbf{x}_{ij} = \\mathbf{x}_j - \\mathbf{x}_i$, is computed for all pairs $(i, j)$ using NumPy's broadcasting capabilities.\n    -   From this, an $N \\times N$ matrix of squared distances, $\\lVert \\Delta\\mathbf{x}_{ij} \\rVert^2$, is calculated.\n    -   The softened inverse-cube law term, $(\\lVert \\mathbf{x}_j - \\mathbf{x}_i \\rVert^2 + \\epsilon^2)^{-3/2}$, is then computed element-wise for all pairs, yielding an $N \\times N$ matrix. The diagonal elements corresponding to $i=j$ are set to zero to enforce the $j \\ne i$ condition.\n    -   Finally, all individual pairwise acceleration contributions, $\\mathbf{f}_{ij} = m_j \\frac{\\mathbf{x}_j - \\mathbf{x}_i}{(\\lVert \\mathbf{x}_j - \\mathbf{x}_i \\rVert^2 + \\epsilon^2)^{3/2}}$, are calculated in double precision (`float64`) and stored in a single $N \\times N \\times 3$ array. This array represents all the terms to be summed for each particle $i$.\n\n3.  **Precision-Specific Acceleration Accumulation**:\n    -   **Double-Precision Acceleration $\\mathbf{a}_i^{(\\text{double})}$**: The acceleration vectors for all particles are computed by summing the pre-calculated `float64` contribution terms along the appropriate axis ($j$). The `numpy.sum` function, when applied to a `float64` array, uses a `float64` accumulator by default, correctly modeling the double-precision summation.\n    -   **Single-Precision Acceleration $\\mathbf{a}_i^{(\\text{single})}$**: To model the single-precision accumulation as specified, each double-precision contribution term must be cast to single precision (`float32`) *before* being added to the accumulator. This is achieved by first creating a `float32` copy of the entire $N \\times N \\times 3$ array of contributions. Then, `numpy.sum` is used on this `float32` array. The summation then naturally proceeds using a `float32` accumulator, faithfully replicating the specified numerical process.\n\n4.  **Error Metric Calculation**: The core of the analysis is the computation of the maximum relative error, $E_{\\max}$.\n    $$\n    E_{\\max} = \\max_{i \\in \\{1,\\dots,N\\}} \\left( \\frac{\\lVert \\mathbf{a}_i^{(\\text{single})} - \\mathbf{a}_i^{(\\text{double})} \\rVert_2}{\\lVert \\mathbf{a}_i^{(\\text{double})} \\rVert_2} \\right)\n    $$\n    -   The difference vector $\\mathbf{a}_i^{(\\text{single})} - \\mathbf{a}_i^{(\\text{double})}$ is calculated for each particle. Note that the `float32` result $\\mathbf{a}_i^{(\\text{single})}$ must be cast back to `float64` for this subtraction to preserve the error information.\n    -   The L2 norms (Euclidean norms) of the difference vector and the double-precision acceleration vector are computed for all particles.\n    -   To handle cases where $\\lVert \\mathbf{a}_i^{(\\text{double})} \\rVert_2$ is close to zero (e.g., due to symmetry, as in Test Case 2), a threshold $\\tau = 10^{-300}$ is used. If $\\lVert \\mathbf{a}_i^{(\\text{double})} \\rVert_2 \\le \\tau$, the absolute error $\\lVert \\mathbf{a}_i^{(\\text{single})} - \\mathbf{a}_i^{(\\text{double})} \\rVert_2$ is used instead of the ratio. This prevents division by zero and correctly handles situations where the relative error would be misleadingly large or infinite.\n    -   The final error $E_{\\max}$ is the maximum value found among all particles' individual error metrics.\n\nThis methodology is applied to each of the four specified test cases, and the resulting $E_{\\max}$ values are collected and formatted as required.",
            "answer": "```python\nimport numpy as np\n\ndef generate_test_case_data(case_num):\n    \"\"\"\n    Generates initial conditions for a specified test case.\n    \"\"\"\n    if case_num == 1:\n        N = 32\n        epsilon = 1e-3\n        seed = 12345\n        rng = np.random.default_rng(seed)\n        positions = rng.uniform(-1.0, 1.0, size=(N, 3))\n        masses = rng.uniform(0.5, 1.5, size=N)\n        # Velocities are not needed for acceleration calculation\n    elif case_num == 2:\n        N = 20\n        epsilon = 1e-3\n        seed = 777\n        rng = np.random.default_rng(seed)\n        base_vectors = rng.uniform(-0.5, 0.5, size=(N // 2, 3))\n        positions = np.zeros((N, 3), dtype=np.float64)\n        positions[0::2] = base_vectors\n        positions[1::2] = -base_vectors\n        masses = np.ones(N, dtype=np.float64)\n    elif case_num == 3:\n        N = 256\n        epsilon = 1e-4\n        seed = 2021\n        rng = np.random.default_rng(seed)\n        positions = rng.uniform(-2.0, 2.0, size=(N, 3))\n        masses = np.ones(N, dtype=np.float64)\n    elif case_num == 4:\n        N = 64\n        epsilon = 1e-2\n        seed = 42\n        rng = np.random.default_rng(seed)\n        positions = np.zeros((N, 3), dtype=np.float64)\n        masses = np.zeros(N, dtype=np.float64)\n        \n        # Central massive particle\n        positions[0] = [0.0, 0.0, 0.0]\n        masses[0] = 1e6\n        \n        # Other particles\n        positions[1:] = rng.uniform(0.1, 1.0, size=(N - 1, 3))\n        masses[1:] = rng.uniform(0.5, 1.5, size=N - 1)\n    else:\n        raise ValueError(\"Invalid case number\")\n        \n    return N, positions, masses, epsilon\n\ndef compute_E_max(N, positions, masses, epsilon):\n    \"\"\"\n    Computes accelerations with single and double precision accumulators\n    and returns the maximum relative error E_max.\n    \"\"\"\n    tau = 1e-300\n    \n    # Use broadcasting to compute all pairwise displacement vectors\n    # dx[i, j, :] = positions[j] - positions[i]\n    dx = positions[None, :, :] - positions[:, None, :]\n    \n    # Compute squared distances\n    r_sq = np.sum(dx**2, axis=2)\n    \n    # Compute the softened inverse cube law term\n    # Add a small value to r_sq to avoid division by zero if epsilon is 0, though not required here.\n    inv_r3_soft = (r_sq + epsilon**2)**(-1.5)\n    \n    # Set diagonal elements to 0 to exclude self-interaction (j != i)\n    # This is an efficient way to handle the j != i condition.\n    np.fill_diagonal(inv_r3_soft, 0.0)\n    \n    # Compute all pairwise acceleration contributions in double precision\n    # terms[i, j, :] is the acceleration on particle i due to particle j\n    # Broadcasting masses (1, N) * inv_r3_soft (N, N) - (N, N)\n    # Then expand dims to (N, N, 1) and multiply by dx (N, N, 3)\n    terms = (masses[None, :] * inv_r3_soft)[:, :, None] * dx\n\n    # 1. Double-precision accumulation\n    # np.sum on a float64 array uses a float64 accumulator\n    a_double = np.sum(terms, axis=1, dtype=np.float64)\n\n    # 2. Single-precision accumulation\n    # Per the problem, cast each term to float32 BEFORE summing.\n    # This is achieved by creating a float32 copy of the terms array.\n    # np.sum on a float32 array will use a float32 accumulator.\n    terms_single = terms.astype(np.float32)\n    a_single = np.sum(terms_single, axis=1)\n\n    # Calculate the error metric E_max\n    # Cast a_single back to float64 for a high-precision difference\n    diff_a = a_single.astype(np.float64) - a_double\n    \n    norm_diff_a = np.linalg.norm(diff_a, axis=1)\n    norm_a_double = np.linalg.norm(a_double, axis=1)\n    \n    # Apply the threshold condition to avoid division by zero/small numbers\n    # Use absolute error where norm_a_double is small\n    errors = np.where(\n        norm_a_double  tau,\n        norm_diff_a / norm_a_double,\n        norm_diff_a\n    )\n    \n    E_max = np.max(errors)\n    \n    return E_max\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [1, 2, 3, 4]\n    results = []\n\n    for case_num in test_cases:\n        N, positions, masses, epsilon = generate_test_case_data(case_num)\n        E_max = compute_E_max(N, positions, masses, epsilon)\n        results.append(E_max)\n\n    # Format output as a list of strings in scientific notation\n    formatted_results = [f\"{res:.6e}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A correct and accurate simulation is only useful if it runs in a reasonable amount of time, which for direct summation requires parallelization. This final practice addresses the performance of a parallel direct summation algorithm, focusing on the problem of load balancing. You will model and compare a simple static task distribution against a dynamic \"work-stealing\" scheme, providing insight into how to design efficient parallel strategies when computational costs per particle are non-uniform .",
            "id": "3508377",
            "problem": "Consider a system of $N$ gravitating particles in a direct-summation $N$-body integrator. Let the position of particle $i$ be $\\mathbf{r}_i$, its mass be $m_i$, and the gravitational constant be $G$. The acceleration of particle $i$ due to all other particles under Newton's law of universal gravitation is\n$$\n\\mathbf{a}_i = G \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{N} m_j \\frac{\\mathbf{r}_j - \\mathbf{r}_i}{\\left\\lVert \\mathbf{r}_j - \\mathbf{r}_i \\right\\rVert^3}.\n$$\nIn a block time-step scheme, a subset of particles is active at any given global step. Denote the number of active particles by $M$, where $M \\le N$. For each active particle $i$, the direct summation requires evaluating a sum over $N$ sources, so the computational cost per active particle scales linearly with $N$. In heterogeneous astrophysical scenarios, per-particle cost varies due to, for example, adaptive softening, neighbor-dependent corrections, or special-function evaluations. Consequently, the set of $M$ tasks (one per active particle) has a distribution of task durations.\n\nYou are asked to design and evaluate a dynamic load balancing scheme based on work stealing for distributing the $M$ active-particle tasks across $P$ workers. You will implement and compare two scheduling strategies:\n\n- Static contiguous assignment: partition the $M$ tasks into $P$ contiguous blocks, with block sizes differing by at most one, and assign one block to each worker. There is no scheduling overhead in this scheme.\n- Dynamic work stealing with chunking: maintain a global pool of remaining tasks, grouped into fixed-size chunks of size $s$ tasks per chunk. At any moment, the worker with the earliest availability time acquires the next chunk from the pool and incurs a per-acquisition overhead cost $c_{\\mathrm{steal}}$ due to synchronization and communication. The worker's finish time is increased by the sum of task durations in the acquired chunk plus $c_{\\mathrm{steal}}$. This repeats until all tasks are acquired.\n\nThe per-task duration is modeled deterministically to capture both baseline direct-summation cost and heterogeneity. For task index $i \\in \\{1,2,\\dots,M\\}$, define\n$$\nu_i = \\mathrm{frac}\\left(\\frac{\\kappa i}{M}\\right), \\quad \\text{with } \\kappa \\text{ a fixed positive integer coprime to } M,\n$$\n$$\nh_i(z) = \n\\begin{cases}\n(1 - \\tilde{u}_i)^{-z} - 1,  z  0, \\\\\n0,  z = 0,\n\\end{cases}\n\\quad \\text{where } \\tilde{u}_i = \\min\\{u_i, 1 - 10^{-4}\\},\n$$\nand the task duration\n$$\nt_i = \\alpha N + \\beta + \\gamma \\, h_i(z).\n$$\nHere, $\\alpha$ and $\\beta$ define the baseline linear-in-$N$ cost for one direct summation over $N$ sources; $\\gamma$ controls the amplitude of heterogeneity; $z$ controls tail heaviness; and $\\kappa$ determines the low-discrepancy sequence driving deterministic heterogeneity. All durations are dimensionless computational time units, and all metrics requested below are dimensionless.\n\nYour program must:\n- Implement the static contiguous assignment and compute the static makespan\n$$\nT_{\\mathrm{static}} = \\max_{p \\in \\{1,\\dots,P\\}} \\sum_{i \\in \\mathcal{B}_p} t_i,\n$$\nwhere $\\mathcal{B}_p$ is the contiguous block of tasks assigned to worker $p$.\n- Implement the dynamic work-stealing list scheduling with chunk size $s$ and per-chunk overhead $c_{\\mathrm{steal}}$, and compute the dynamic makespan $T_{\\mathrm{dyn}}$ and the total dynamic overhead $O_{\\mathrm{dyn}} = c_{\\mathrm{steal}} \\times C$, where $C$ is the total number of chunk acquisitions. Also compute the dynamic overhead fraction\n$$\nF_{\\mathrm{over}} = \\frac{O_{\\mathrm{dyn}}}{O_{\\mathrm{dyn}} + \\sum_{i=1}^{M} t_i}.\n$$\n- Report the effectiveness of dynamic load balancing by the speedup\n$$\nS = \\frac{T_{\\mathrm{static}}}{T_{\\mathrm{dyn}}}.\n$$\n\nTest suite. Use the following set of parameter values, which together probe balanced and imbalanced workloads, low and high overhead regimes, and small active subsets. For each case, set $\\kappa = 9973$.\n\n- Case $1$ (balanced baseline, moderate overhead):\n  - $N = 8192$, $M = 1024$, $P = 16$, $s = 8$, $c_{\\mathrm{steal}} = 0.5$, $\\alpha = 3 \\times 10^{-6}$, $\\beta = 1.0$, $\\gamma = 0.05$, $z = 0$.\n- Case $2$ (heavy-tailed heterogeneity, low chunk size):\n  - $N = 65536$, $M = 4096$, $P = 64$, $s = 4$, $c_{\\mathrm{steal}} = 1.0$, $\\alpha = 3 \\times 10^{-6}$, $\\beta = 1.0$, $\\gamma = 10.0$, $z = 3$.\n- Case $3$ (mild heterogeneity, high overhead, fine-grained chunks):\n  - $N = 32768$, $M = 2048$, $P = 32$, $s = 1$, $c_{\\mathrm{steal}} = 5.0$, $\\alpha = 3 \\times 10^{-6}$, $\\beta = 1.0$, $\\gamma = 0.2$, $z = 1$.\n- Case $4$ (small active subset, many workers):\n  - $N = 10000$, $M = 10$, $P = 64$, $s = 1$, $c_{\\mathrm{steal}} = 0.1$, $\\alpha = 3 \\times 10^{-6}$, $\\beta = 1.0$, $\\gamma = 0.5$, $z = 2$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, formatted as\n$$\n\\left[ S_1, F_{\\mathrm{over},1}, S_2, F_{\\mathrm{over},2}, S_3, F_{\\mathrm{over},3}, S_4, F_{\\mathrm{over},4} \\right],\n$$\nwhere $S_k$ and $F_{\\mathrm{over},k}$ are the speedup and overhead fraction for case $k \\in \\{1,2,3,4\\}$. All outputs must be decimal numbers (no percentage signs).",
            "solution": "The problem requires the design and evaluation of two distinct load balancing schemes for parallel computation, contextualized within an $N$-body simulation in computational astrophysics. The two schemes are a static contiguous block assignment and a dynamic work-stealing algorithm with chunking. The problem is scientifically grounded in established principles of parallel computing and performance modeling, is mathematically well-posed, and all parameters and objectives are defined with precision. The problem statement is therefore deemed valid and a full solution is presented below.\n\nOur first step is to compute the duration for each of the $M$ computational tasks. The problem defines a deterministic model for task durations, which captures a baseline cost proportional to $N$ plus a heterogeneous component. The duration $t_i$ for task $i \\in \\{1, 2, \\dots, M\\}$ is given by the formula:\n$$\nt_i = \\alpha N + \\beta + \\gamma h_i(z)\n$$\nHere, $\\alpha$, $\\beta$, $\\gamma$, $z$, and $N$ are provided parameters for each test case. The heterogeneity is introduced by the term $h_i(z)$, which itself depends on a low-discrepancy sequence $u_i$. The procedure for calculating the vector of $M$ task durations is as follows:\n\n1.  A low-discrepancy sequence is generated using the formula $u_i = \\mathrm{frac}(\\frac{\\kappa i}{M}) = \\frac{\\kappa i}{M} - \\lfloor \\frac{\\kappa i}{M} \\rfloor$. This is computed for each task index $i$ from $1$ to $M$. The constant $\\kappa$ is given as $9973$, a prime number that is coprime to all specified values of $M$, ensuring the sequence covers the interval $[0,1)$ with good distribution properties.\n\n2.  To prevent numerical overflow when $u_i$ is close to $1$, a capped value, $\\tilde{u}_i$, is used: $\\tilde{u}_i = \\min\\{u_i, 1 - 10^{-4}\\}$.\n\n3.  The heterogeneity function $h_i(z)$ is then evaluated. If the tail-heaviness parameter $z$ is $0$, then $h_i(0) = 0$. For $z  0$, the function is $h_i(z) = (1 - \\tilde{u}_i)^{-z} - 1$.\n\n4.  These components are combined to find the total duration $t_i$ for each task. This results in a complete list of task durations $\\{t_1, t_2, \\dots, t_M\\}$, which serves as the input for the scheduling algorithms. Note that while the problem defines task indices from $1$ to $M$, standard programming array indices often start from $0$. This offset must be correctly handled.\n\nWith the task durations established, we proceed to simulate the two scheduling strategies.\n\n**1. Static Contiguous Assignment**\n\nIn this strategy, the ordered list of $M$ tasks is partitioned into $P$ contiguous blocks, and one block is assigned to each of the $P$ workers. The makespan, $T_{\\mathrm{static}}$, is the maximum execution time over all workers. To distribute the tasks as evenly as possible, the block sizes will differ by at most one.\n\nLet $m_0 = \\lfloor M/P \\rfloor$ and $r = M \\pmod P$.\n- The first $r$ workers (indexed $p=0, \\dots, r-1$) are each assigned a block of $m_0+1$ tasks.\n- The remaining $P-r$ workers (indexed $p=r, \\dots, P-1$) are each assigned a block of $m_0$ tasks.\n\nThe total workload for worker $p$, denoted $W_p$, is the sum of the durations of the tasks in its assigned block, $\\mathcal{B}_p$:\n$$\nW_p = \\sum_{i \\in \\mathcal{B}_p} t_i\n$$\nThe problem states that there is no scheduling overhead in this scheme. Therefore, the static makespan is simply the maximum workload among all workers.\n$$\nT_{\\mathrm{static}} = \\max_{p \\in \\{0, 1, \\dots, P-1\\}} W_p\n$$\n\n**2. Dynamic Work Stealing with Chunking**\n\nThis scheme uses a global pool of tasks, grouped into fixed-size chunks of $s$ tasks each. Idle workers fetch the next available chunk from this pool. This is a form of list scheduling. A per-acquisition overhead cost, $c_{\\mathrm{steal}}$, is incurred each time a worker gets a new chunk.\n\nThe simulation of this process requires tracking the finish time of each of the $P$ workers. Let's denote these by an array $T_{\\mathrm{finish}}$, initialized to all zeros. The total number of chunks is $C = \\lceil M/s \\rceil$.\n\nThe algorithm proceeds as follows:\n1.  Initialize worker finish times: $T_{\\mathrm{finish}, p} = 0$ for all $p \\in \\{0, 1, \\dots, P-1\\}$.\n2.  Iterate through all $C$ chunks. In each iteration:\n    a. Find the worker $p^*$ with the earliest availability (i.e., minimum finish time): $p^* = \\arg\\min_{p} T_{\\mathrm{finish}, p}$.\n    b. This worker acquires the next unassigned chunk. Calculate the sum of task durations within this chunk, $W_{\\mathrm{chunk}}$.\n    c. Update the worker's finish time. The new finish time is its previous finish time plus the overhead and the work for the new chunk: $T_{\\mathrm{finish}, p^*} \\leftarrow T_{\\mathrm{finish}, p^*} + c_{\\mathrm{steal}} + W_{\\mathrm{chunk}}$.\n3.  After all $C$ chunks have been assigned, the dynamic makespan, $T_{\\mathrm{dyn}}$, is the maximum finish time across all workers:\n$$\nT_{\\mathrm{dyn}} = \\max_{p \\in \\{0, 1, \\dots, P-1\\}} T_{\\mathrm{finish}, p}\n$$\nThe total dynamic overhead, $O_{\\mathrm{dyn}}$, is the overhead per chunk acquisition multiplied by the total number of acquisitions:\n$$\nO_{\\mathrm{dyn}} = c_{\\mathrm{steal}} \\times C = c_{\\mathrm{steal}} \\times \\lceil M/s \\rceil\n$$\nThe total computational work is the sum of all individual task durations, $W_{\\mathrm{total}} = \\sum_{i=1}^{M} t_i$. The dynamic overhead fraction, $F_{\\mathrm{over}}$, is the ratio of total overhead to the sum of total overhead and total work.\n$$\nF_{\\mathrm{over}} = \\frac{O_{\\mathrm{dyn}}}{O_{\\mathrm{dyn}} + W_{\\mathrm{total}}}\n$$\n\n**3. Final Metrics**\n\nFor each of the four test cases, we calculate the specified metrics based on the results from the two scheduling simulations:\n- The speedup, $S = T_{\\mathrm{static}} / T_{\\mathrm{dyn}}$, measures the relative performance improvement of the dynamic scheme over the static one. A value $S1$ indicates that dynamic scheduling is more efficient.\n- The overhead fraction, $F_{\\mathrm{over}}$, quantifies the portion of the total computational effort that is expended on the overhead of dynamic scheduling.\n\nThese calculations are performed for each test case, and the resulting pairs $(S_k, F_{\\mathrm{over}, k})$ for $k \\in \\{1, 2, 3, 4\\}$ are formatted into the required output. The special case where $M  P$ (Case $4$) is handled naturally by both algorithms: in the static scheme, $M$ workers get one task each and $P-M$ workers get none; in the dynamic scheme, $M$ workers each acquire one chunk of size $s=1$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\n# from scipy import ...\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (N, M, P, s, c_steal, alpha, beta, gamma, z)\n        (8192, 1024, 16, 8, 0.5, 3e-6, 1.0, 0.05, 0),\n        (65536, 4096, 64, 4, 1.0, 3e-6, 1.0, 10.0, 3),\n        (32768, 2048, 32, 1, 5.0, 3e-6, 1.0, 0.2, 1),\n        (10000, 10, 64, 1, 0.1, 3e-6, 1.0, 0.5, 2),\n    ]\n\n    kappa = 9973\n    all_results = []\n\n    for case in test_cases:\n        N, M, P, s, c_steal, alpha, beta, gamma, z = case\n\n        # 1. Calculate task durations t_i\n        # Using 1-based indexing for i as in the problem formula\n        i_s = np.arange(1, M + 1)\n        \n        # u_i = frac(kappa * i / M)\n        u_i = (kappa * i_s / M) % 1.0\n        \n        # u_tilde_i = min(u_i, 1 - 1e-4)\n        u_tilde_i = np.minimum(u_i, 1.0 - 1e-4)\n        \n        # h_i(z)\n        if z == 0:\n            h_i = np.zeros(M)\n        else:\n            h_i = (1.0 - u_tilde_i)**(-z) - 1.0\n            \n        # t_i\n        task_durations = alpha * N + beta + gamma * h_i\n\n        # 2. Static Contiguous Assignment\n        base_tasks_per_worker = M // P\n        remainder_tasks = M % P\n        worker_loads_static = np.zeros(P)\n        current_task_idx = 0\n        for p in range(P):\n            num_tasks_for_worker = base_tasks_per_worker + 1 if p  remainder_tasks else base_tasks_per_worker\n            if num_tasks_for_worker  0:\n                end_idx = current_task_idx + num_tasks_for_worker\n                worker_loads_static[p] = np.sum(task_durations[current_task_idx:end_idx])\n                current_task_idx = end_idx\n        \n        T_static = np.max(worker_loads_static)\n\n        # 3. Dynamic Work Stealing with Chunking\n        num_chunks = math.ceil(M / s)\n        worker_finish_times = np.zeros(P)\n        task_idx_ptr = 0\n        \n        # The core of a list scheduling simulation\n        for _ in range(num_chunks):\n            # Find the worker with the earliest finish time\n            p_min_idx = np.argmin(worker_finish_times)\n            earliest_finish_time = worker_finish_times[p_min_idx]\n            \n            # Define the current chunk of work\n            start_idx = task_idx_ptr\n            end_idx = min(start_idx + s, M)\n            chunk_duration = np.sum(task_durations[start_idx:end_idx])\n            \n            # Update the worker's finish time. It becomes ready at its previous finish\n            # time, then acquires the chunk (incurring overhead) and works on it.\n            worker_finish_times[p_min_idx] = earliest_finish_time + c_steal + chunk_duration\n            \n            # Advance to the start of the next chunk\n            task_idx_ptr += s\n            \n        T_dyn = np.max(worker_finish_times)\n        \n        # 4. Final Metrics\n        O_dyn = c_steal * num_chunks\n        W_total = np.sum(task_durations)\n        \n        # Handle potential division by zero if W_total and O_dyn are both zero\n        if O_dyn + W_total  0:\n            F_over = O_dyn / (O_dyn + W_total)\n        else:\n            F_over = 0.0\n\n        # Handle potential division by zero if T_dyn is zero\n        S = T_static / T_dyn if T_dyn  0 else 0.0\n\n        all_results.extend([S, F_over])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.6f}' for x in all_results)}]\")\n\nsolve()\n```"
        }
    ]
}