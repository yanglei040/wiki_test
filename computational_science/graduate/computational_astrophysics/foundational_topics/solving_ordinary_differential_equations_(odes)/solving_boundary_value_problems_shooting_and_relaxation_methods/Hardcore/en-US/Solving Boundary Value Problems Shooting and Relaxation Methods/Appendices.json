{
    "hands_on_practices": [
        {
            "introduction": "Many complex systems in astrophysics, from stellar atmospheres to accretion disks, can be modeled by linear boundary value problems (BVPs), often as a first approximation to a more complex nonlinear reality. This exercise is your entry point into solving these problems numerically, focusing on the powerful and versatile finite-difference method. You will learn to transform a continuous differential equation in conservative form, $-(p(x)y')' + q(x)y = g(x)$, into a discrete system of linear equations, a process that forms the bedrock of relaxation methods . Mastering the discretization of the variable-coefficient diffusion term and the correct incorporation of boundary conditions is a fundamental skill for building robust and physically accurate astrophysical codes.",
            "id": "3535597",
            "problem": "Consider the second-order linear boundary value problem (BVP) given by the differential operator on a closed interval, $$-(p(x)\\,y'(x))' + q(x)\\,y(x) = g(x),\\quad x\\in[a,b],$$ subject to Dirichlet boundary conditions $$y(a) = \\alpha,\\qquad y(b) = \\beta.$$ Starting from the conservation-law form and the definition of the derivative, derive a second-order accurate finite-difference discretization on a uniform grid with $$N$$ equal subintervals of width $$h = \\frac{b-a}{N}$$ and nodal points $$x_i = a + i\\,h$$ for $$i\\in\\{0,1,\\dots,N\\}$$. Use a central-difference approximation for the flux $$p(x)\\,y'(x)$$ at the half-grid points $$x_{i\\pm\\frac{1}{2}} = x_i \\pm \\frac{h}{2}$$, and obtain a tridiagonal linear system for the interior unknowns $$\\{y_1,y_2,\\dots,y_{N-1}\\}$$. Specify, in a mathematically precise way, how the Dirichlet boundary conditions $$y_0=\\alpha$$ and $$y_N=\\beta$$ are incorporated into both the coefficient matrix and the right-hand side vector such that the resulting linear system remains tridiagonal.\n\nImplement a complete program that:\n- Constructs the uniform grid for each test case with the specified $$a$$, $$b$$, and $$N$$.\n- Builds the tridiagonal system using the second-order central-difference discretization for $$-(p\\,y')' + q\\,y$$ evaluated at each interior node $$x_i$$.\n- Incorporates Dirichlet boundary conditions into the matrix and the right-hand side without introducing nonzero entries outside the tridiagonal structure.\n- Solves the tridiagonal system for the interior unknowns using a stable direct method that preserves second-order accuracy.\n- Assembles the full solution $$\\{y_0,y_1,\\dots,y_N\\}$$ including boundaries and computes the maximum absolute error against the known analytic solution for each test case.\n\nThe program should address the computational astrophysics context by treating $$p(x)$$ as a possibly non-constant diffusion-like coefficient and $$q(x)$$ as a reaction-like term, both common in linearized transport and diffusion approximations in stellar structure and radiation hydrodynamics. However, the problem itself must be solved in purely mathematical terms.\n\nAll trigonometric functions use angles in radians. No physical units are required in this problem.\n\nUse the following test suite, where $$g(x)$$ is defined by applying the operator $$-(p\\,y')' + q\\,y$$ to the provided analytic $$y(x)$$ (that is, $$g(x) = -\\big(p'(x)\\,y'(x) + p(x)\\,y''(x)\\big) + q(x)\\,y(x)$$), and Dirichlet boundary values are set consistently by $$\\alpha = y(a)$$ and $$\\beta = y(b)$$:\n- Test case $$1$$ (happy path): $$a=0$$, $$b=1$$, $$N=200$$, $$p(x)=1$$, $$q(x)=0$$, $$y(x)=\\sin(\\pi x)$$ with $$y'(x)=\\pi\\cos(\\pi x)$$ and $$y''(x)=-\\pi^2\\sin(\\pi x)$$.\n- Test case $$2$$ (variable-coefficient diffusion and reaction): $$a=0$$, $$b=1$$, $$N=150$$, $$p(x)=1+x$$, $$q(x)=x^2$$, $$y(x)=x(1-x)$$ with $$y'(x)=1-2x$$ and $$y''(x)=-2$$.\n- Test case $$3$$ (stiff reaction term with exponential diffusion): $$a=0$$, $$b=1$$, $$N=400$$, $$p(x)=e^x$$, $$q(x)=1000$$, $$y(x)=\\sin(\\pi x)$$ with $$y'(x)=\\pi\\cos(\\pi x)$$ and $$y''(x)=-\\pi^2\\sin(\\pi x)$$.\n\nFor each test case, compute the maximum absolute error $$\\max_{0\\le i\\le N}\\,\\lvert y_i - y(x_i)\\rvert$$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $$[result_1,result_2,result_3]$$), where each $$result_k$$ is the computed maximum absolute error for test case $$k$$ expressed as a floating-point number.",
            "solution": "The problem requires the derivation and implementation of a second-order accurate finite-difference scheme for a linear, second-order boundary value problem (BVP) in conservation-law form.\n\nThe BVP is defined on the interval $x \\in [a, b]$ by the differential equation\n$$ -(p(x) y'(x))' + q(x) y(x) = g(x) $$\nsubject to Dirichlet boundary conditions\n$$ y(a) = \\alpha, \\quad y(b) = \\beta. $$\n\nWe begin by establishing a uniform grid on the interval $[a,b]$. The interval is divided into $N$ equal subintervals, each of width $h = (b-a)/N$. This defines a set of $N+1$ discrete grid points, or nodes, $x_i = a + i h$ for $i = 0, 1, \\dots, N$. The solution $y(x)$ is approximated at these nodes by the values $y_i \\approx y(x_i)$.\n\nThe core of the method is to replace the continuous differential operator with a discrete approximation at each interior node $x_i$ for $i = 1, \\dots, N-1$. Let the flux be defined as $F(x) = p(x) y'(x)$. The differential equation can then be written as $-F'(x) + q(x) y(x) = g(x)$.\n\nTo maintain second-order accuracy, we employ central-difference approximations. The derivative of the flux, $F'(x_i)$, is approximated at node $x_i$ using the values of the flux at half-grid points, $x_{i \\pm 1/2} = x_i \\pm h/2$:\n$$ F'(x_i) \\approx \\frac{F(x_{i+1/2}) - F(x_{i-1/2})}{h}. $$\nThis approximation is second-order accurate, i.e., the truncation error is $O(h^2)$.\n\nNext, we need to approximate the flux $F(x)$ at these half-grid points. We use a central difference for the derivative $y'(x)$ centered at these same half-points:\n$$ y'(x_{i+1/2}) \\approx \\frac{y(x_{i+1}) - y(x_i)}{h} \\approx \\frac{y_{i+1} - y_i}{h} $$\n$$ y'(x_{i-1/2}) \\approx \\frac{y(x_i) - y(x_{i-1})}{h} \\approx \\frac{y_i - y_{i-1}}{h} $$\nThese are also second-order accurate approximations of the derivative at the midpoint. The flux at the half-points is then approximated by evaluating the coefficient $p(x)$ at these points and using the discrete derivative:\n$$ F(x_{i+1/2}) \\approx p(x_{i+1/2}) \\left(\\frac{y_{i+1} - y_i}{h}\\right) = p_{i+1/2} \\frac{y_{i+1} - y_i}{h} $$\n$$ F(x_{i-1/2}) \\approx p(x_{i-1/2}) \\left(\\frac{y_i - y_{i-1}}{h}\\right) = p_{i-1/2} \\frac{y_i - y_{i-1}}{h} $$\n\nSubstituting these into the approximation for $-F'(x_i)$ gives the discrete form of the second-derivative term:\n$$ -(p y')'_i \\approx -\\frac{1}{h} \\left( p_{i+1/2} \\frac{y_{i+1} - y_i}{h} - p_{i-1/2} \\frac{y_i - y_{i-1}}{h} \\right) = \\frac{1}{h^2} \\left( -p_{i-1/2} y_{i-1} + (p_{i-1/2} + p_{i+1/2}) y_i - p_{i+1/2} y_{i+1} \\right). $$\nCombining this with the other terms in the differential equation, evaluated at node $x_i$ (i.e., $q_i y_i = g_i$), we obtain the complete finite-difference equation for an interior node $i \\in \\{1, \\dots, N-1\\}$:\n$$ \\frac{1}{h^2} \\left( -p_{i-1/2} y_{i-1} + (p_{i-1/2} + p_{i+1/2}) y_i - p_{i+1/2} y_{i+1} \\right) + q_i y_i = g_i. $$\nRearranging this equation to group terms by the unknowns $y_{i-1}$, $y_i$, and $y_{i+1}$ yields:\n$$ \\left(-\\frac{p_{i-1/2}}{h^2}\\right) y_{i-1} + \\left(\\frac{p_{i-1/2} + p_{i+1/2}}{h^2} + q_i\\right) y_i + \\left(-\\frac{p_{i+1/2}}{h^2}\\right) y_{i+1} = g_i. $$\nThis set of $N-1$ linear equations for the $N-1$ interior unknowns $\\{y_1, y_2, \\dots, y_{N-1}\\}$ forms a tridiagonal system. The unknowns $y_0$ and $y_N$ are given by the boundary conditions: $y_0 = \\alpha$ and $y_N = \\beta$.\n\nTo construct the final linear system $A \\mathbf{y}_{\\text{int}} = \\mathbf{b}$, where $\\mathbf{y}_{\\text{int}} = [y_1, \\dots, y_{N-1}]^T$, we must incorporate the boundary conditions.\nFor the first interior node, $i=1$:\n$$ \\left(-\\frac{p_{1/2}}{h^2}\\right) y_0 + \\left(\\frac{p_{1/2} + p_{3/2}}{h^2} + q_1\\right) y_1 + \\left(-\\frac{p_{3/2}}{h^2}\\right) y_2 = g_1. $$\nSince $y_0 = \\alpha$ is known, we move the term involving $y_0$ to the right-hand side:\n$$ \\left(\\frac{p_{1/2} + p_{3/2}}{h^2} + q_1\\right) y_1 + \\left(-\\frac{p_{3/2}}{h^2}\\right) y_2 = g_1 + \\frac{p_{1/2}}{h^2} \\alpha. $$\nThis defines the first row of the $(N-1) \\times (N-1)$ system matrix and the first element of the right-hand side vector.\n\nFor the last interior node, $i=N-1$:\n$$ \\left(-\\frac{p_{N-3/2}}{h^2}\\right) y_{N-2} + \\left(\\frac{p_{N-3/2} + p_{N-1/2}}{h^2} + q_{N-1}\\right) y_{N-1} + \\left(-\\frac{p_{N-1/2}}{h^2}\\right) y_N = g_{N-1}. $$\nSince $y_N = \\beta$ is known, the term involving $y_N$ is moved to the right-hand side:\n$$ \\left(-\\frac{p_{N-3/2}}{h^2}\\right) y_{N-2} + \\left(\\frac{p_{N-3/2} + p_{N-1/2}}{h^2} + q_{N-1}\\right) y_{N-1} = g_{N-1} + \\frac{p_{N-1/2}}{h^2} \\beta. $$\nThis defines the last row of the system. This procedure preserves the tridiagonal structure of the coefficient matrix $A$.\n\nThe resulting tridiagonal system is solved for the vector of interior unknowns $\\mathbf{y}_{\\text{int}}$ using a stable direct solver, such as an LU decomposition algorithm optimized for banded matrices. Finally, the complete numerical solution is assembled by combining the boundary values and the computed interior values: $\\mathbf{y}_{\\text{num}} = [\\alpha, y_1, \\dots, y_{N-1}, \\beta]^T$. The accuracy is assessed by computing the maximum absolute error against the known analytic solution, $\\max_{0 \\le i \\le N} |y_i - y(x_i)|$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef solve_bvp_case(case):\n    \"\"\"\n    Solves a single test case for the BVP -(p*y')' + q*y = g.\n\n    Args:\n        case (dict): A dictionary containing all parameters for the test case,\n                     including functions for p, q, and the analytic solution y\n                     and its derivatives.\n\n    Returns:\n        float: The maximum absolute error between the numerical and analytic solutions.\n    \"\"\"\n    # 1. Unpack parameters and set up grid\n    a, b, N = case[\"a\"], case[\"b\"], case[\"N\"]\n    p_func = case[\"p\"]\n    q_func = case[\"q\"]\n    y_analytic_func = case[\"y_analytic\"]\n    \n    # Per the problem statement, construct g(x) from the analytic solution:\n    # g(x) = -(p(x)y'(x))' + q(x)y(x) = -p'(x)y'(x) - p(x)y''(x) + q(x)y(x)\n    p_prime_func = case[\"p_prime\"]\n    y_prime_func = case[\"y_prime\"]\n    y_double_prime_func = case[\"y_double_prime\"]\n    g_func = lambda x: -(p_prime_func(x) * y_prime_func(x) + p_func(x) * y_double_prime_func(x)) + q_func(x) * y_analytic_func(x)\n\n    h = (b - a) / N\n    h2 = h * h\n    # Grid nodes x_0, ..., x_N\n    x_nodes = np.linspace(a, b, N + 1)\n    # Interior nodes x_1, ..., x_{N-1}\n    x_interior = x_nodes[1:-1]\n    # Half-grid points for p(x) evaluation: x_{1/2}, ..., x_{N-1/2}\n    x_half = a + (np.arange(N) + 0.5) * h\n\n    # 2. Assemble the tridiagonal matrix for interior points (N-1 x N-1)\n    # The finite difference equation for an interior node i is:\n    # (-p_{i-1/2}/h^2) y_{i-1} + ((p_{i-1/2}+p_{i+1/2})/h^2 + q_i) y_i + (-p_{i+1/2}/h^2) y_{i+1} = g_i\n    \n    p_half_vals = p_func(x_half)\n    q_interior_vals = q_func(x_interior)\n    \n    # Lower diagonal (for equations corresponding to y_2 to y_{N-1})\n    lower_diag = -p_half_vals[1:-1] / h2\n    \n    # Main diagonal (for equations corresponding to y_1 to y_{N-1})\n    main_diag = (p_half_vals[:-1] + p_half_vals[1:]) / h2 + q_interior_vals\n    \n    # Upper diagonal (for equations corresponding to y_1 to y_{N-2})\n    upper_diag = -p_half_vals[1:-1] / h2\n    \n    # 3. Assemble the Right-Hand Side (RHS) vector\n    rhs = g_func(x_interior)\n    \n    # Get boundary conditions from analytic solution\n    alpha = y_analytic_func(a)\n    beta = y_analytic_func(b)\n    \n    # Incorporate boundary conditions into the RHS vector\n    # For i=1: rhs[0] should be g_1 - (-p_{1/2}/h^2)*alpha\n    rhs[0] += p_half_vals[0] / h2 * alpha\n    \n    # For i=N-1: rhs[-1] should be g_{N-1} - (-p_{N-1/2}/h^2)*beta\n    rhs[-1] += p_half_vals[-1] / h2 * beta\n\n    # 4. Solve the tridiagonal system using a stable direct solver\n    # The `solve_banded` function requires the matrix diagonals in a specific format.\n    # For a tridiagonal matrix, the band description is (l=1, u=1).\n    # The `ab` matrix shape is (3, N-1).\n    # ab[0, 1:] = upper diagonal\n    # ab[1, :] = main diagonal\n    # ab[2, :-1] = lower diagonal\n    ab = np.zeros((3, N - 1))\n    ab[0, 1:] = upper_diag\n    ab[1, :] = main_diag\n    ab[2, :-1] = lower_diag\n    \n    y_interior = solve_banded((1, 1), ab, rhs)\n    \n    # 5. Assemble the full solution and compute the maximum absolute error\n    y_numerical = np.concatenate(([alpha], y_interior, [beta]))\n    y_exact = y_analytic_func(x_nodes)\n    \n    max_abs_error = np.max(np.abs(y_numerical - y_exact))\n    \n    return max_abs_error\n\ndef solve():\n    \"\"\"\n    Main function to define test cases and compute errors.\n    \"\"\"\n    test_cases = [\n        {\n            \"a\": 0.0, \"b\": 1.0, \"N\": 200,\n            \"p\": lambda x: 1.0,\n            \"q\": lambda x: 0.0,\n            \"y_analytic\": lambda x: np.sin(np.pi * x),\n            \"p_prime\": lambda x: 0.0,\n            \"y_prime\": lambda x: np.pi * np.cos(np.pi * x),\n            \"y_double_prime\": lambda x: -np.pi**2 * np.sin(np.pi * x)\n        },\n        {\n            \"a\": 0.0, \"b\": 1.0, \"N\": 150,\n            \"p\": lambda x: 1.0 + x,\n            \"q\": lambda x: x**2,\n            \"y_analytic\": lambda x: x * (1.0 - x),\n            \"p_prime\": lambda x: 1.0,\n            \"y_prime\": lambda x: 1.0 - 2.0 * x,\n            \"y_double_prime\": lambda x: -2.0\n        },\n        {\n            \"a\": 0.0, \"b\": 1.0, \"N\": 400,\n            \"p\": lambda x: np.exp(x),\n            \"q\": lambda x: 1000.0,\n            \"y_analytic\": lambda x: np.sin(np.pi * x),\n            \"p_prime\": lambda x: np.exp(x),\n            \"y_prime\": lambda x: np.pi * np.cos(np.pi * x),\n            \"y_double_prime\": lambda x: -np.pi**2 * np.sin(np.pi * x)\n        }\n    ]\n\n    # Vectorize lambda functions for numpy array inputs\n    for case in test_cases:\n        for key, func in case.items():\n            if callable(func):\n                case[key] = np.vectorize(func)\n\n    results = []\n    for case in test_cases:\n        error = solve_bvp_case(case)\n        results.append(error)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While linear problems provide a crucial starting point, the universe is fundamentally nonlinear. This practice challenges you to tackle a classic nonlinear BVP from stellar structure theory: the Lane-Emden equation. You will implement and compare two distinct, powerful workhorse methods for solving such problems . The relaxation method extends the finite-difference approach from our first practice into the nonlinear domain using Newton's method, while the multiple shooting method cleverly recasts the BVP as a root-finding problem for a series of initial value problems. By implementing both, you will gain invaluable hands-on insight into their respective strengths, weaknesses, and domains of applicability.",
            "id": "3535567",
            "problem": "Consider the dimensionless Lane-Emden equation of index $n$ for a spherically symmetric polytropic self-gravitating fluid, which arises from combining hydrostatic equilibrium and Newtonian gravitation in spherical symmetry under a polytropic equation of state. The governing ordinary differential equation is\n$$\n\\theta''(x) + \\frac{2}{x}\\,\\theta'(x) + \\theta(x)^n = 0,\n$$\nposed on a finite interval $[a,b]$ with $a0$ to avoid the singularity at $x=0$. The physically regular central conditions at $x=0$ are $\\theta(0)=1$ and $\\theta'(0)=0$, which imply the central series expansion $\\theta(x) \\approx 1 - \\frac{x^2}{6} + \\mathcal{O}(x^4)$ for small $x$. We use this expansion to define the left boundary value at $x=a$ as\n$$\n\\theta(a) = 1 - \\frac{a^2}{6}.\n$$\nWe prescribe a right boundary value at $x=b$,\n$$\n\\theta(b) = \\theta_b,\n$$\nthus obtaining a two-point boundary value problem.\n\nYour task is to construct and implement a multiple shooting scheme that partitions $[a,b]$ into $m$ subintervals, introduces segment initial values as unknowns, integrates each segment, and enforces continuity constraints at the subinterval interfaces together with the boundary conditions. Specifically:\n\n1. Partition the interval $[a,b]$ into $m$ equal-length subintervals with interface points $a = x_0  x_1  \\dots  x_m = b$.\n2. For each segment $i \\in \\{0,1,\\dots,m-1\\}$, introduce the unknown initial state $\\mathbf{y}_i = (\\theta_i,\\phi_i)$ at $x_i$, where $\\phi(x) \\equiv \\theta'(x)$. The initial state at the first segment satisfies the left boundary condition as a constraint $\\theta_0 = 1 - \\frac{a^2}{6}$, while $\\phi_0$ is unknown. For the last segment, the terminal state must satisfy the right boundary condition $\\theta(b) = \\theta_b$.\n3. Integrate the first-order system\n$$\n\\theta'(x) = \\phi(x), \\qquad \\phi'(x) = -\\frac{2}{x}\\,\\phi(x) - \\theta(x)^n\n$$\non each segment $[x_i, x_{i+1}]$ with the segment’s initial condition $\\mathbf{y}_i$ to obtain the segment’s terminal state $\\mathbf{y}_{i}^{\\text{end}}$ at $x_{i+1}$.\n4. Enforce continuity constraints at each interface $x_{i+1}$:\n$$\n\\theta_{i+1} - \\theta_{i}^{\\text{end}} = 0, \\qquad \\phi_{i+1} - \\phi_{i}^{\\text{end}} = 0,\n$$\ntogether with the boundary conditions\n$$\n\\theta_0 - \\left(1 - \\frac{a^2}{6}\\right) = 0, \\qquad \\theta_{m-1}^{\\text{end}} - \\theta_b = 0.\n$$\nSolve the resulting nonlinear system for the unknown segment initial values $\\{(\\theta_i,\\phi_i)\\}_{i=0}^{m-1}$ using a robust nonlinear solver.\n\nAdditionally, implement a relaxation (finite-difference Newton) method for the same boundary value problem on a uniform grid with $N$ nodes $a = x_0  x_1  \\dots  x_{N-1} = b$. Use second-order central differences to discretize $\\theta'(x)$ and $\\theta''(x)$ at interior nodes $i \\in \\{1,2,\\dots,N-2\\}$:\n$$\n\\frac{\\theta_{i+1} - 2\\theta_i + \\theta_{i-1}}{h^2} + \\frac{2}{x_i}\\,\\frac{\\theta_{i+1} - \\theta_{i-1}}{2h} + \\theta_i^n = 0,\n$$\nwith $h = \\frac{b-a}{N-1}$ and the boundary conditions $\\theta_0 = 1 - \\frac{a^2}{6}$ and $\\theta_{N-1} = \\theta_b$. Solve the nonlinear discrete system using Newton’s method with an analytically constructed tridiagonal Jacobian.\n\nYour program must implement both methods and, for each test case in the following test suite, compute:\n- The maximum absolute constraint residual from the multiple shooting solution over all continuity and boundary constraints, expressed as a float.\n- The maximum absolute discrete residual at interior nodes from the relaxation method upon convergence, expressed as a float.\n- The maximum absolute pointwise difference between the multiple shooting solution and the relaxation solution, sampled at the same $N$ uniform grid nodes, expressed as a float.\n\nUse the following test suite, where each test case is given by $(n,a,b,\\theta_b,m,N)$:\n- Test case $1$: $(1, 10^{-3}, 1, 0.8, 5, 101)$.\n- Test case $2$: $(3, 10^{-3}, 1, 0.7, 7, 151)$.\n- Test case $3$: $(5, 10^{-3}, 1, 0.6, 9, 201)$.\n\nAll quantities are dimensionless. Your program should produce a single line of output containing the results for the test suite as a comma-separated list enclosed in square brackets, with each test’s triple enclosed in its own square brackets. For example, the output format should be\n$$\n[\\,[r_{1},\\,s_{1},\\,d_{1}],\\,[r_{2},\\,s_{2},\\,d_{2}],\\,[r_{3},\\,s_{3},\\,d_{3}]\\,],\n$$\nwhere $r_i$ is the multiple shooting maximum constraint residual, $s_i$ is the relaxation maximum residual, and $d_i$ is the maximum absolute difference on the grid, all as floats.",
            "solution": "The user wants to solve the Lane-Emden boundary value problem using two distinct numerical methods: multiple shooting and relaxation (finite differences), and then compare their results.\n\n### **Problem Validation**\n\n1.  **Givens Extraction**:\n    *   **Equation**: The dimensionless Lane-Emden equation of index $n$, $\\theta''(x) + \\frac{2}{x}\\,\\theta'(x) + \\theta(x)^n = 0$, on the interval $[a,b]$ with $a0$.\n    *   **Boundary Conditions**: $\\theta(a) = 1 - \\frac{a^2}{6}$ and $\\theta(b) = \\theta_b$.\n    *   **Multiple Shooting Method**: Partition $[a,b]$ into $m$ subintervals. Solve for the initial state vectors $(\\theta_i, \\phi_i)$ at the start of each segment $i \\in \\{0, \\dots, m-1\\}$ by enforcing the left boundary condition, continuity at segment interfaces, and the right boundary condition. This forms a nonlinear system to be solved.\n    *   **Relaxation Method**: Discretize the domain $[a,b]$ into a uniform grid of $N$ nodes. Apply second-order central difference formulas to the ODE at interior nodes, creating a system of nonlinear algebraic equations. Solve this system using Newton's method with an analytically derived tridiagonal Jacobian.\n    *   **Required Outputs**: For each test case, compute (1) the maximum absolute constraint residual for the multiple shooting method, (2) the maximum absolute discrete residual for the relaxation method, and (3) the maximum absolute pointwise difference between the two solutions on the $N$-point grid.\n    *   **Test Suite**: Three test cases are provided with parameters $(n,a,b,\\theta_b,m,N)$.\n\n2.  **Validation Check**:\n    *   **Scientific Grounding**: The Lane-Emden equation is a cornerstone of stellar structure theory in astrophysics, derived from fundamental physical principles. The problem is scientifically sound.\n    *   **Well-Posedness**: The problem is a well-defined two-point boundary value problem for a second-order ODE. The specification of two boundary conditions is appropriate. The numerical methods proposed are standard and well-suited for this class of problem.\n    *   **Objectivity**: The problem is stated in precise mathematical terms with no subjective language.\n    *   **Completeness and Consistency**: All necessary parameters, equations, and methodological constraints are provided. The descriptions for both numerical schemes are internally consistent and provide a complete basis for implementation.\n    *   **Feasibility**: The parameters are within a reasonable range for numerical computation. The problem avoids the coordinate singularity at $x=0$ by setting the domain on $[a,b]$ with $a0$, which is a standard and valid approach.\n    *   **Conclusion**: The problem is valid, well-posed, and scientifically relevant.\n\n### **Methodology and Implementation Plan**\n\n#### **Method 1: Multiple Shooting**\n\n1.  **State-Space Formulation**: The second-order ODE is converted into a system of two first-order ODEs by defining a state vector $\\mathbf{y}(x) = [\\theta(x), \\phi(x)]^T$, where $\\phi(x) = \\theta'(x)$. The system is:\n    $$\n    \\frac{d\\mathbf{y}}{dx} = \\begin{pmatrix} \\phi(x) \\\\ - \\frac{2}{x}\\phi(x) - \\theta(x)^n \\end{pmatrix}\n    $$\n2.  **Discretization**: The domain $[a,b]$ is divided into $m$ segments with nodes $a = x_0  x_1  \\dots  x_m = b$. The unknowns are the state vectors $\\mathbf{y}_i = [\\theta_i, \\phi_i]^T$ at each node $x_i$ for $i=0, \\dots, m-1$. This gives $2m$ scalar unknowns.\n3.  **Nonlinear System**: A system of $2m$ nonlinear equations is constructed to solve for the unknowns. Let $U = [\\theta_0, \\phi_0, \\theta_1, \\phi_1, \\dots, \\theta_{m-1}, \\phi_{m-1}]^T$ be the vector of unknowns.\n    *   **Left Boundary Condition ($1$ eq.)**: $\\theta_0 - (1 - a^2/6) = 0$.\n    *   **Continuity Conditions ($2(m-1)$ eqs.)**: For each segment $i \\in \\{0, \\dots, m-2\\}$, we integrate the ODE system from $x_i$ to $x_{i+1}$ starting with $\\mathbf{y}_i$. Let the solution at $x_{i+1}$ be $\\mathbf{y}_i^{\\text{end}}$. The continuity constraint is $\\mathbf{y}_{i+1} - \\mathbf{y}_i^{\\text{end}} = \\mathbf{0}$.\n    *   **Right Boundary Condition ($1$ eq.)**: We integrate the last segment from $x_{m-1}$ to $x_m = b$ starting with $\\mathbf{y}_{m-1}$. The first component of the resulting state vector at $x_m$, $\\theta_{m-1}^{\\text{end}}$, must equal $\\theta_b$.\n4.  **Solver**: The system is solved using `scipy.optimize.root`. An initial guess for the solution is formed by a linear interpolation of $\\theta$ between the boundaries and a constant slope for $\\phi$. The ODE segments are integrated using `scipy.integrate.solve_ivp`.\n5.  **Solution Reconstruction**: After finding the unknown initial values $U$, the complete solution on a fine grid is reconstructed by integrating each segment and concatenating the results.\n\n#### **Method 2: Relaxation (Finite Differences)**\n\n1.  **Discretization**: A uniform grid of $N$ points, $x_0, \\dots, x_{N-1}$, is created on $[a,b]$ with spacing $h = (b-a)/(N-1)$. The unknowns are the values $\\theta_i$ at the $N-2$ interior grid points.\n2.  **Finite Difference Approximation**: At each interior node $x_i$, the derivatives $\\theta''(x_i)$ and $\\theta'(x_i)$ are approximated using second-order central difference formulas. This transforms the ODE into a system of $N-2$ nonlinear algebraic equations for the unknown $\\theta_i$:\n    $$\n    \\frac{\\theta_{i+1} - 2\\theta_i + \\theta_{i-1}}{h^2} + \\frac{2}{x_i}\\frac{\\theta_{i+1} - \\theta_{i-1}}{2h} + \\theta_i^n = 0, \\quad \\text{for } i \\in \\{1, \\dots, N-2\\}\n    $$\n    Here, $\\theta_0$ and $\\theta_{N-1}$ are known from the boundary conditions.\n3.  **Newton's Method**: The nonlinear system $\\mathbf{F}(\\mathbf{\\Theta}) = \\mathbf{0}$, where $\\mathbf{\\Theta} = [\\theta_1, \\dots, \\theta_{N-2}]^T$, is solved iteratively using Newton's method:\n    $$\n    \\mathbf{J}(\\mathbf{\\Theta}^{(k)})\\,\\Delta\\mathbf{\\Theta}^{(k)} = -\\mathbf{F}(\\mathbf{\\Theta}^{(k)})\n    $$\n    $$\n    \\mathbf{\\Theta}^{(k+1)} = \\mathbf{\\Theta}^{(k)} + \\Delta\\mathbf{\\Theta}^{(k)}\n    $$\n4.  **Analytical Jacobian**: The Jacobian matrix $\\mathbf{J} = \\partial\\mathbf{F}/\\partial\\mathbf{\\Theta}$ is tridiagonal. Its elements are derived analytically:\n    *   $J_{i,i} = \\frac{\\partial F_i}{\\partial \\theta_i} = -\\frac{2}{h^2} + n\\theta_i^{n-1}$\n    *   $J_{i,i-1} = \\frac{\\partial F_i}{\\partial \\theta_{i-1}} = \\frac{1}{h^2} - \\frac{1}{x_i h}$\n    *   $J_{i,i+1} = \\frac{\\partial F_i}{\\partial \\theta_{i+1}} = \\frac{1}{h^2} + \\frac{1}{x_i h}$\n    The resulting linear system at each Newton step is efficiently solved using `scipy.linalg.solve_banded`. An initial guess for $\\mathbf{\\Theta}$ is provided by linear interpolation.\n\n#### **Comparison**\n\nFinally, for each test case, the maximum absolute residuals for both methods are calculated, and the maximum absolute pointwise difference between the two resulting solution curves is determined on the common $N$-point grid.",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom scipy.optimize import root\nfrom scipy.linalg import solve_banded\n\ndef solve_multiple_shooting(n, a, b, theta_b, m, comparison_grid):\n    \"\"\"\n    Solves the Lane-Emden BVP using a multiple shooting method.\n\n    Returns:\n        float: Maximum absolute residual of the constraint equations.\n        np.ndarray: The solution array for theta on the comparison_grid.\n    \"\"\"\n    # 1. Setup\n    shooting_nodes = np.linspace(a, b, m + 1)\n    \n    # ODE system function\n    def ode_system(x, y, n_param):\n        theta, phi = y\n        if x == 0:\n            return np.array([phi, 0.0])\n        d_theta_dx = phi\n        d_phi_dx = -2.0 / x * phi - np.power(theta, n_param)\n        return np.array([d_theta_dx, d_phi_dx])\n\n    # 2. Residual Function for the nonlinear solver\n    def residual_function(U, n_param, a_param, b_param, theta_b_param, m_param, nodes):\n        residuals = np.zeros(2 * m_param)\n        \n        # Left boundary condition\n        theta_0_bc = 1.0 - a_param**2 / 6.0\n        residuals[0] = U[0] - theta_0_bc\n\n        # Integrate segments and enforce continuity\n        for i in range(m_param - 1):\n            y_start = [U[2*i], U[2*i+1]]\n            t_span = [nodes[i], nodes[i+1]]\n            \n            sol = solve_ivp(ode_system, t_span, y_start, args=(n_param,), dense_output=False, rtol=1e-10, atol=1e-12)\n            \n            # Check if integration was successful\n            if sol.status != 0:\n                return np.full_like(residuals, np.inf)\n\n            y_end = sol.y[:, -1]\n            \n            residuals[2*i + 1] = U[2*(i+1)] - y_end[0]\n            residuals[2*i + 2] = U[2*(i+1)+1] - y_end[1]\n            \n        # Right boundary condition\n        y_start_last = [U[2*(m_param-1)], U[2*(m_param-1)+1]]\n        t_span_last = [nodes[m_param-1], nodes[m_param]]\n        sol_last = solve_ivp(ode_system, t_span_last, y_start_last, args=(n_param,), dense_output=False, rtol=1e-10, atol=1e-12)\n        \n        if sol_last.status != 0:\n            return np.full_like(residuals, np.inf)\n\n        theta_end_last = sol_last.y[0, -1]\n        residuals[2*m_param - 1] = theta_end_last - theta_b_param\n\n        return residuals\n\n    # 3. Initial Guess\n    U_guess = np.zeros(2 * m)\n    theta_0_val = 1.0 - a**2 / 6.0\n    theta_guess_nodes = np.linspace(theta_0_val, theta_b, m + 1)\n    phi_guess_val = (theta_b - theta_0_val) / (b - a)\n\n    for i in range(m):\n        U_guess[2*i] = theta_guess_nodes[i]\n        U_guess[2*i+1] = phi_guess_val\n\n    # 4. Solve the nonlinear system\n    solution = root(residual_function, U_guess, args=(n, a, b, theta_b, m, shooting_nodes), method='hybr', tol=1e-9)\n    U_sol = solution.x\n\n    # 5. Compute max residual\n    final_residuals = residual_function(U_sol, n, a, b, theta_b, m, shooting_nodes)\n    max_residual = np.max(np.abs(final_residuals))\n\n    # 6. Reconstruct full solution on the comparison grid\n    solution_on_grid = np.zeros_like(comparison_grid)\n    for i in range(m):\n        y_start = [U_sol[2*i], U_sol[2*i+1]]\n        t_start, t_end = shooting_nodes[i], shooting_nodes[i+1]\n        \n        is_in_segment = (comparison_grid = t_start)  (comparison_grid = t_end)\n        segment_grid_points = comparison_grid[is_in_segment]\n\n        if len(segment_grid_points)  0:\n            sol_segment = solve_ivp(\n                ode_system, [t_start, t_end], y_start, args=(n,), \n                t_eval=segment_grid_points, dense_output=False, rtol=1e-10, atol=1e-12)\n            solution_on_grid[is_in_segment] = sol_segment.y[0, :]\n            \n    return max_residual, solution_on_grid\n\ndef solve_relaxation(n, a, b, theta_b, N):\n    \"\"\"\n    Solves the Lane-Emden BVP using a finite-difference relaxation method.\n    \n    Returns:\n        float: Maximum absolute residual of the discrete equations.\n        np.ndarray: The solution array for theta on the N-point grid.\n    \"\"\"\n    # 1. Setup\n    x = np.linspace(a, b, N)\n    h = (b - a) / (N - 1)\n    \n    theta_0 = 1.0 - a**2 / 6.0\n    theta_N_1 = theta_b\n    \n    num_interior = N - 2\n\n    # 2. Initial Guess for interior points \n    Theta = np.linspace(theta_0, theta_N_1, N)[1:-1]\n    \n    # 3. Newton's Method\n    max_iter = 100\n    tol = 1e-12\n    \n    for k in range(max_iter):\n        # a. Construct Residual F\n        F = np.zeros(num_interior)\n        \n        theta_full = np.concatenate(([theta_0], Theta, [theta_N_1]))\n        F = ( (theta_full[2:] - 2*theta_full[1:-1] + theta_full[:-2]) / h**2 \n            + (2.0/x[1:-1]) * (theta_full[2:] - theta_full[:-2]) / (2*h)\n            + np.power(Theta, n) )\n        \n        # b. Construct Jacobian J in banded format\n        J_diag = -2.0/h**2 + n * np.power(Theta, n - 1)\n        J_upper = 1.0/h**2 + 1.0/(x[1:-2] * h)\n        J_lower = 1.0/h**2 - 1.0/(x[2:-1] * h)\n\n        ab = np.zeros((3, num_interior))\n        ab[0, 1:] = J_upper\n        ab[1, :] = J_diag\n        ab[2, :-1] = J_lower\n\n        # c. Solve J * dTheta = -F\n        dTheta = solve_banded((1, 1), ab, -F)\n        \n        # d. Update solution\n        Theta += dTheta\n        \n        # e. Check convergence\n        if np.max(np.abs(dTheta))  tol:\n            break\n\n    # 4. Compute final residual\n    theta_final_full = np.concatenate(([theta_0], Theta, [theta_N_1]))\n    F_final = ( (theta_final_full[2:] - 2*theta_final_full[1:-1] + theta_final_full[:-2]) / h**2 \n              + (2.0/x[1:-1]) * (theta_final_full[2:] - theta_final_full[:-2]) / (2*h)\n              + np.power(Theta, n) )\n    \n    max_residual = np.max(np.abs(F_final))\n    \n    # 5. Return full solution\n    return max_residual, theta_final_full\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        (1, 1e-3, 1.0, 0.8, 5, 101),\n        (3, 1e-3, 1.0, 0.7, 7, 151),\n        (5, 1e-3, 1.0, 0.6, 9, 201),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, a, b, theta_b, m, N = case\n        \n        comparison_grid = np.linspace(a, b, N)\n\n        ms_max_residual, ms_solution = solve_multiple_shooting(n, a, b, theta_b, m, comparison_grid)\n        rx_max_residual, rx_solution = solve_relaxation(n, a, b, theta_b, N)\n        \n        max_diff = np.max(np.abs(ms_solution - rx_solution))\n        \n        results.append([ms_max_residual, rx_max_residual, max_diff])\n\n    # Format output as specified: [[r1,s1,d1],[r2,s2,d2],[r3,s3,d3]]\n    print(f\"[{','.join(str(res) for res in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "A truly expert computational astrophysicist doesn't just solve equations; they analyze the solutions to gain deeper physical insight. This advanced practice moves beyond simply finding a solution to asking: \"How does our solution depend on the underlying physical parameters of our model?\" Answering this question efficiently is critical for model calibration, optimization, and uncertainty quantification. In this exercise, you will learn to compute the sensitivity of a shooting method's solution to a continuous parameter field by implementing the elegant and highly efficient adjoint method . By comparing its performance to the brute-force finite-difference approach, you will discover why adjoint-based sensitivity analysis is an indispensable tool in modern computational science.",
            "id": "3535529",
            "problem": "You are given a boundary value problem whose solution is approximated by a shooting method. Consider the nonlinear ordinary differential equation for a scalar field $y(x)$ on the interval $x \\in [0,1]$ with auxiliary variable $v(x)$ defined by $v(x) = \\frac{dy}{dx}$:\n$$\n\\frac{dy}{dx} = v, \\qquad \\frac{dv}{dx} = -\\alpha(x)\\, y^3,\n$$\nwhere $\\alpha(x)$ is a prescribed parameter field. The shooting method specifies the initial condition $y(0) = y_0$ and an initial slope $v(0) = s$, integrates forward to $x=1$, and defines a scalar residual\n$$\nR(\\alpha) = y(1; \\alpha, y_0, s) - y_{\\text{target}},\n$$\nwhich measures the mismatch with the desired terminal value $y(1) \\approx y_{\\text{target}}$. Here $y(1; \\alpha, y_0, s)$ denotes the value of $y$ at $x=1$ obtained by forward integration given the parameter field $\\alpha(x)$, and $y_{\\text{target}}$ is a prescribed target.\n\nYour task is to implement an adjoint sensitivity method to compute the functional derivative $\\frac{dR}{d\\alpha}$, understood as the Fréchet derivative mapping perturbations $\\delta\\alpha(x)$ to $\\delta R$. The adjoint sensitivity method must be derived from first principles by linearizing the forward dynamics and introducing an appropriate adjoint system with terminal condition at $x=1$, allowing the directional derivative in any direction $p(x)$ to be computed as an inner product $\\int_0^1 \\left(\\frac{dR}{d\\alpha}(x)\\right) p(x)\\, dx$. You must also compute the same directional derivative using a finite-difference approximation and quantify the discrepancy between the adjoint result and the finite difference. Finally, you must report a computational advantage metric that estimates the relative number of forward solves required to recover the full gradient field on a grid of $G$ points using finite differences versus adjoint, assuming central differences for each parameter degree of freedom.\n\nBase your derivation and algorithm design on the following foundational principles:\n- The definition of the shooting residual $R(\\alpha)$ as a function of the terminal state $y(1)$.\n- The concept of linearization of an initial value problem $\\frac{d\\mathbf{y}}{dx} = \\mathbf{f}(\\mathbf{y}(x), x, \\alpha(x))$ around a nominal solution to obtain the variational equation for $\\delta \\mathbf{y}$.\n- The adjoint method for converting a terminal functional derivative into an integral against an adjoint field by solving an adjoint ordinary differential equation backward in $x$.\n\nImplement the following in a complete program:\n1. For each test case, integrate the forward system for the given $y_0$, $s$, and $\\alpha(x)$ to obtain $y(1)$ and the residual $R(\\alpha)$.\n2. Derive and implement the adjoint system associated with the residual $R(\\alpha)$, and compute the functional gradient $\\frac{dR}{d\\alpha}(x)$ on a uniform grid $x_i$ for $i=0,\\dots,G-1$ with $G=1001$ points on $[0,1]$.\n3. For a given perturbation direction $p(x)$ and small scalar $\\varepsilon$, compute the adjoint-based directional derivative\n$$\nD_{\\text{adj}} = \\int_0^1 \\left(\\frac{dR}{d\\alpha}(x)\\right) p(x)\\, dx,\n$$\nusing numerical quadrature over the grid. Also compute the central finite-difference approximation\n$$\nD_{\\text{fd}} = \\frac{R(\\alpha + \\varepsilon p) - R(\\alpha - \\varepsilon p)}{2 \\varepsilon},\n$$\nby two additional forward integrations with perturbed parameter fields $\\alpha_{\\pm}(x) = \\alpha(x) \\pm \\varepsilon p(x)$.\n4. Report the absolute discrepancy $\\lvert D_{\\text{adj}} - D_{\\text{fd}} \\rvert$ for each test case. Also report the computational advantage metric $C$, defined as the ratio of the number of forward integrations needed to compute the full gradient at all $G$ grid points using central finite differences to the number needed using the adjoint method. Assume that central finite differences require $2G$ forward solves, while the adjoint method requires one forward solve and one adjoint solve (both independent of $G$), so $C = G$.\n\nUse the following test suite. Each test case is specified by the tuple $(y_0, s, y_{\\text{target}}, \\text{profile\\_id}, \\text{direction\\_id}, \\varepsilon)$, where $\\alpha(x)$ and $p(x)$ are defined by the identifiers:\n- Parameter field profiles $\\alpha(x)$:\n  - $\\text{profile\\_id} = 0$: $\\alpha(x) = 2 + x$.\n  - $\\text{profile\\_id} = 1$: $\\alpha(x) = 0.2 + 0.2 x$.\n  - $\\text{profile\\_id} = 2$: $\\alpha(x) = 5 \\left(1 + x^2\\right)$.\n  - $\\text{profile\\_id} = 3$: $\\alpha(x) = 1$.\n- Direction fields $p(x)$:\n  - $\\text{direction\\_id} = 0$: $p(x) = \\sin(\\pi x)$, with $\\pi$ being the circle constant in radians.\n  - $\\text{direction\\_id} = 1$: $p(x) = x (1 - x)$.\n  - $\\text{direction\\_id} = 2$: $p(x) = \\cos(2 \\pi x)$, angles measured in radians.\n  - $\\text{direction\\_id} = 3$: $p(x) = \\sin(3 \\pi x)$, angles measured in radians.\n\nTest cases:\n- Case 1: $(1.0, 0.3, 0.7, 0, 0, 1\\times 10^{-5})$.\n- Case 2: $(1.0, 0.0, 0.5, 1, 1, 1\\times 10^{-5})$.\n- Case 3: $(0.8, 1.0, 0.1, 2, 2, 1\\times 10^{-6})$.\n- Case 4: $(1.2, -0.5, 1.2, 3, 3, 1\\times 10^{-5})$.\n\nNumerical requirements:\n- Use a uniform grid of $G = 1001$ points on $[0,1]$ for all integrations and quadratures.\n- Use a robust ordinary differential equation integrator with sufficient accuracy to resolve the nonlinear dynamics and adjoint dynamics.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, for each test case in order, the absolute discrepancy $\\lvert D_{\\text{adj}} - D_{\\text{fd}} \\rvert$ as a float, followed by the computational advantage metric $C$ as an integer. For example, the output should be of the form $[e_1,C_1,e_2,C_2,e_3,C_3,e_4,C_4]$.",
            "solution": "The problem requires the computation of the functional derivative of a shooting method residual with respect to a parameter field $\\alpha(x)$ using an adjoint sensitivity method. The correctness of the adjoint derivative is to be verified against a finite-difference approximation, and a computational advantage metric is to be reported.\n\nThe core of the solution lies in deriving and solving the adjoint system of ordinary differential equations (ODEs). The derivation is based on the principle of linearizing the forward dynamics and using an adjoint operator to map the sensitivity from the terminal state back to the parameters distributed along the integration path.\n\nLet the state of the system be described by the vector $\\mathbf{z}(x) = [y(x), v(x)]^T$. The forward dynamics are given by the system of first-order ODEs:\n$$\n\\frac{d\\mathbf{z}}{dx} = \\mathbf{f}(\\mathbf{z}(x), x, \\alpha(x))\n$$\nwhere\n$$\n\\mathbf{f}(\\mathbf{z}, x, \\alpha) = \\begin{pmatrix} v \\\\ -\\alpha(x) y^3 \\end{pmatrix}\n$$\nThe initial conditions are $\\mathbf{z}(0) = [y_0, s]^T$. The shooting residual is a scalar functional of the terminal state:\n$$\nR(\\alpha) = y(1) - y_{\\text{target}} = \\begin{pmatrix} 1  0 \\end{pmatrix} \\mathbf{z}(1) - y_{\\text{target}}\n$$\n\nTo find the Fréchet derivative $\\frac{dR}{d\\alpha}$, we consider a small perturbation $\\delta\\alpha(x)$ in the parameter field, which induces a first-order variation $\\delta\\mathbf{z}(x)$ in the state trajectory. The variation $\\delta\\mathbf{z}$ is governed by the linearized (or variational) equation:\n$$\n\\frac{d(\\delta\\mathbf{z})}{dx} = \\mathbf{J}_{\\mathbf{z}}(x) \\delta\\mathbf{z}(x) + \\mathbf{F}(x) \\delta\\alpha(x)\n$$\nwhere $\\mathbf{J}_{\\mathbf{z}}(x)$ is the Jacobian matrix of $\\mathbf{f}$ with respect to $\\mathbf{z}$, and $\\mathbf{F}(x)$ is the derivative of $\\mathbf{f}$ with respect to $\\alpha$, both evaluated along the nominal forward solution trajectory $(\\mathbf{z}(x), \\alpha(x))$.\nThe Jacobian is:\n$$\n\\mathbf{J}_{\\mathbf{z}}(x) = \\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{z}} = \\begin{pmatrix} \\frac{\\partial v}{\\partial y}  \\frac{\\partial v}{\\partial v} \\\\ \\frac{\\partial(-\\alpha y^3)}{\\partial y}  \\frac{\\partial(-\\alpha y^3)}{\\partial v} \\end{pmatrix} = \\begin{pmatrix} 0  1 \\\\ -3\\alpha(x)y(x)^2  0 \\end{pmatrix}\n$$\nThe sensitivity to $\\alpha$ is:\n$$\n\\mathbf{F}(x) = \\frac{\\partial \\mathbf{f}}{\\partial \\alpha} = \\begin{pmatrix} 0 \\\\ -y(x)^3 \\end{pmatrix}\n$$\nThe variation in the initial condition is zero, so $\\delta\\mathbf{z}(0) = \\mathbf{0}$. The variation in the residual is $\\delta R = \\delta y(1) = \\begin{pmatrix} 1  0 \\end{pmatrix} \\delta\\mathbf{z}(1)$.\n\nThe adjoint method introduces an adjoint state vector $\\mathbf{\\lambda}(x) = [\\lambda_y(x), \\lambda_v(x)]^T$. We seek to relate $\\delta R$ to an integral over $\\delta\\alpha(x)$. We start with the identity:\n$$\n\\int_0^1 \\mathbf{\\lambda}^T \\left( \\frac{d(\\delta\\mathbf{z})}{dx} \\right) dx = \\int_0^1 \\mathbf{\\lambda}^T (\\mathbf{J}_{\\mathbf{z}} \\delta\\mathbf{z} + \\mathbf{F} \\delta\\alpha) dx\n$$\nIntegrating the left-hand side by parts gives:\n$$\n[\\mathbf{\\lambda}^T \\delta\\mathbf{z}]_0^1 - \\int_0^1 \\left( \\frac{d\\mathbf{\\lambda}}{dx} \\right)^T \\delta\\mathbf{z} \\, dx = \\int_0^1 \\mathbf{\\lambda}^T \\mathbf{J}_{\\mathbf{z}} \\delta\\mathbf{z} \\, dx + \\int_0^1 \\mathbf{\\lambda}^T \\mathbf{F} \\delta\\alpha \\, dx\n$$\nUsing $\\delta\\mathbf{z}(0) = \\mathbf{0}$, this simplifies to:\n$$\n\\mathbf{\\lambda}(1)^T \\delta\\mathbf{z}(1) = \\int_0^1 \\left( \\left( \\frac{d\\mathbf{\\lambda}}{dx} \\right)^T + \\mathbf{\\lambda}^T \\mathbf{J}_{\\mathbf{z}} \\right) \\delta\\mathbf{z} \\, dx + \\int_0^1 \\mathbf{\\lambda}^T \\mathbf{F} \\delta\\alpha \\, dx\n$$\nThe adjoint method defines the dynamics of $\\mathbf{\\lambda}$ such that the integral involving $\\delta\\mathbf{z}$ vanishes. This yields the **adjoint ODE system**:\n$$\n\\frac{d\\mathbf{\\lambda}}{dx} = -\\mathbf{J}_{\\mathbf{z}}(x)^T \\mathbf{\\lambda}(x)\n$$\nIn component form:\n$$\n\\frac{d}{dx}\\begin{pmatrix} \\lambda_y \\\\ \\lambda_v \\end{pmatrix} = -\\begin{pmatrix} 0  -3\\alpha y^2 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} \\lambda_y \\\\ \\lambda_v \\end{pmatrix} = \\begin{pmatrix} 3\\alpha(x)y(x)^2 \\lambda_v(x) \\\\ -\\lambda_y(x) \\end{pmatrix}\n$$\nThis ODE is integrated backward in $x$, from $x=1$ to $x=0$. The terminal condition at $x=1$ is chosen to relate the expression to $\\delta R$:\n$$\n\\mathbf{\\lambda}(1) = \\left( \\frac{\\partial R}{\\partial \\mathbf{z}(1)} \\right)^T = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\nWith these definitions, the relationship simplifies to:\n$$\n\\delta R = \\mathbf{\\lambda}(1)^T \\delta\\mathbf{z}(1) = \\int_0^1 \\mathbf{\\lambda}(x)^T \\mathbf{F}(x) \\delta\\alpha(x) \\, dx\n$$\nThe integrand is the inner product of the adjoint state and the forward sensitivity term. The functional derivative $\\frac{dR}{d\\alpha}(x)$ is the kernel of this integral:\n$$\n\\frac{dR}{d\\alpha}(x) = \\mathbf{\\lambda}(x)^T \\mathbf{F}(x) = \\begin{pmatrix} \\lambda_y(x)  \\lambda_v(x) \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -y(x)^3 \\end{pmatrix} = -y(x)^3 \\lambda_v(x)\n$$\nThe directional derivative in a direction $p(x)$ is thus $D_{\\text{adj}} = \\int_0^1 \\left(\\frac{dR}{d\\alpha}(x)\\right) p(x) dx$.\n\nThe algorithm is as follows:\n1.  **Forward Pass**: Solve the forward ODE system for $\\mathbf{z}(x)$ from $x=0$ to $x=1$ on a grid of $G=1001$ points. Store the solution $y(x)$ using dense output for use in the next step.\n2.  **Backward Pass**: Solve the adjoint ODE system for $\\mathbf{\\lambda}(x)$ from $x=1$ to $x=0$, using the terminal condition $\\mathbf{\\lambda}(1)=[1, 0]^T$. The coefficient $\\alpha(x)y(x)^2$ requires the stored solution from the forward pass.\n3.  **Gradient Calculation**: At each grid point $x_i$, compute the gradient $\\frac{dR}{d\\alpha}(x_i) = -y(x_i)^3 \\lambda_v(x_i)$.\n4.  **Adjoint Derivative**: Numerically integrate the product of the gradient and the perturbation direction $p(x)$ to obtain $D_{\\text{adj}} = \\int_0^1 \\frac{dR}{d\\alpha}(x) p(x) dx$ using the trapezoidal rule.\n5.  **Finite Difference Verification**: Compute the central difference approximation $D_{\\text{fd}} = \\frac{R(\\alpha + \\varepsilon p) - R(\\alpha - \\varepsilon p)}{2 \\varepsilon}$ by performing two additional forward solves with perturbed parameter fields $\\alpha_{\\pm}(x) = \\alpha(x) \\pm \\varepsilon p(x)$.\n6.  **Metrics**: Report the absolute discrepancy $|D_{\\text{adj}} - D_{\\text{fd}}|$ and the computational advantage $C$. The value of $C$ is determined by the ratio of computations. To compute the gradient on a grid of $G$ points, central differences require $2G$ forward solves (two for each grid point). The adjoint method requires one forward solve and one backward (adjoint) solve, a total of 2 primary integrations, independent of $G$. The advantage is thus $C = \\frac{2G}{2} = G = 1001$.\n\nThis procedure is implemented for each test case provided.",
            "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It computes the adjoint and finite-difference derivatives, their discrepancy,\n    and a computational advantage metric.\n    \"\"\"\n    \n    # Define grid and parameters\n    G = 1001\n    X_GRID = np.linspace(0.0, 1.0, G)\n    \n    # Define parameter and direction profiles from the problem statement\n    alpha_profiles = {\n        0: lambda x: 2.0 + x,\n        1: lambda x: 0.2 + 0.2 * x,\n        2: lambda x: 5.0 * (1.0 + x**2),\n        3: lambda x: 1.0,\n    }\n    \n    p_directions = {\n        0: lambda x: np.sin(np.pi * x),\n        1: lambda x: x * (1.0 - x),\n        2: lambda x: np.cos(2.0 * np.pi * x),\n        3: lambda x: np.sin(3.0 * np.pi * x),\n    }\n\n    test_cases = [\n        # (y0, s, y_target, profile_id, direction_id, epsilon)\n        (1.0, 0.3, 0.7, 0, 0, 1e-5),\n        (1.0, 0.0, 0.5, 1, 1, 1e-5),\n        (0.8, 1.0, 0.1, 2, 2, 1e-6),\n        (1.2, -0.5, 1.2, 3, 3, 1e-5),\n    ]\n\n    results = []\n\n    def compute_residual(y0, s, y_target, alpha_func):\n        \"\"\"\n        Solves the forward ODE for a given alpha(x) and returns the residual R.\n        \"\"\"\n        def forward_ode(x, state, afunc):\n            y, v = state\n            return [v, -afunc(x) * y**3]\n\n        sol = solve_ivp(\n            lambda x, state: forward_ode(x, state, alpha_func),\n            [0.0, 1.0], [y0, s], method='Radau', rtol=1e-9, atol=1e-11\n        )\n        y_at_1 = sol.y[0, -1]\n        return y_at_1 - y_target\n\n    for y0, s, y_target, profile_id, direction_id, epsilon in test_cases:\n        alpha_func = alpha_profiles[profile_id]\n        p_func = p_directions[direction_id]\n\n        # --- Adjoint Method ---\n\n        # 1. Forward Pass: Solve for the state y(x), v(x)\n        def forward_ode(x, state, afunc):\n            y, v = state\n            return [v, -afunc(x) * y**3]\n\n        fwd_sol = solve_ivp(\n            lambda x, state: forward_ode(x, state, alpha_func),\n            [0.0, 1.0], [y0, s], dense_output=True, method='Radau', rtol=1e-9, atol=1e-11\n        )\n        y_sol_dense = fwd_sol.sol\n\n        # 2. Backward Pass: Solve the adjoint ODE\n        def adjoint_ode(x, lambda_state):\n            lambda_y, lambda_v = lambda_state\n            # Get y(x) and alpha(x) at the current integration point x\n            y_val = y_sol_dense(x)[0]\n            alpha_val = alpha_func(x)\n            \n            d_lambda_y_dx = 3.0 * alpha_val * y_val**2 * lambda_v\n            d_lambda_v_dx = -lambda_y\n            return [d_lambda_y_dx, d_lambda_v_dx]\n\n        # Integrate backwards from x=1 to x=0\n        adj_sol = solve_ivp(\n            adjoint_ode, [1.0, 0.0], [1.0, 0.0], t_eval=X_GRID[::-1],\n            method='Radau', rtol=1e-9, atol=1e-11\n        )\n\n        # 3. Compute Adjoint-based Directional Derivative (D_adj)\n        # Realign solution with X_GRID (from 0 to 1)\n        y_grid = y_sol_dense(X_GRID)[0]\n        lambda_v_grid = adj_sol.y[1, ::-1]\n\n        # Functional derivative dR/dalpha(x) = -y(x)^3 * lambda_v(x)\n        grad_R_grid = -y_grid**3 * lambda_v_grid\n        p_grid = p_func(X_GRID)\n        \n        # Integrate (dR/dalpha) * p(x)\n        D_adj = np.trapz(grad_R_grid * p_grid, x=X_GRID)\n\n        # --- Finite Difference Method for Verification ---\n        \n        # Define perturbed alpha functions\n        alpha_plus = lambda x: alpha_func(x) + epsilon * p_func(x)\n        alpha_minus = lambda x: alpha_func(x) - epsilon * p_func(x)\n\n        # Compute residuals for perturbed alphas\n        R_plus = compute_residual(y0, s, y_target, alpha_plus)\n        R_minus = compute_residual(y0, s, y_target, alpha_minus)\n        \n        # Central difference approximation\n        D_fd = (R_plus - R_minus) / (2.0 * epsilon)\n\n        # --- Report Metrics ---\n        \n        # Absolute discrepancy between the two methods\n        discrepancy = abs(D_adj - D_fd)\n        \n        # Computational advantage C = G\n        C = G\n\n        results.extend([discrepancy, C])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}