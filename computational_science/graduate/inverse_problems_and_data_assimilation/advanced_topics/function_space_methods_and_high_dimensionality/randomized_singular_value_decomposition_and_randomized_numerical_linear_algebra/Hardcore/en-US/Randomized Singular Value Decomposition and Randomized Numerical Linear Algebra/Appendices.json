{
    "hands_on_practices": [
        {
            "introduction": "The best way to understand an algorithm is to build it. This first practice guides you through implementing the workhorse of many randomized linear algebra applications: the Randomized Singular Value Decomposition (RSVD). You will apply your implementation to solve a Tikhonov-regularized inverse problem (), a common task in data assimilation, and gain a practical sense of the algorithm's power by comparing your approximate solution to the exact one.",
            "id": "3416448",
            "problem": "You are given a linear inverse problem with Tikhonov regularization. Let $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, and $\\lambda > 0$. The Tikhonov-regularized solution $x_{\\lambda} \\in \\mathbb{R}^{n}$ is the minimizer of the objective\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2}.\n$$\nYou will compute an approximate Tikhonov solution using a randomized low-rank approximation of $A$ and quantify the error.\n\nStarting from core definitions and well-tested facts, implement the following steps in a principled manner.\n\n1) Implement a Randomized Singular Value Decomposition (RSVD) subspace finder with oversampling and power iterations. Given a target rank $k$, oversampling parameter $p$, and power iteration count $q$, construct orthonormal $Q \\in \\mathbb{R}^{m \\times \\ell}$ with $\\ell = \\min(m,n,k+p)$ that approximately spans the range of $A$, form the small matrix $B = Q^{\\top} A \\in \\mathbb{R}^{\\ell \\times n}$, and then compute the compact Singular Value Decomposition (SVD) $B = \\tilde{U} \\Sigma V^{\\top}$. Form the approximate singular vectors $U \\approx Q \\tilde{U}$ with the leading $k$ columns, and retain the leading $k$ singular values and right singular vectors as an approximate rank-$k$ factorization $A \\approx U_{k} \\Sigma_{k} V_{k}^{\\top}$.\n\n2) Using only the approximate factors $U_{k}$, $\\Sigma_{k}$, and $V_{k}$, compute an approximate Tikhonov solution $\\tilde{x}_{\\lambda}$ obtained by restricting the solution to the subspace spanned by the $k$ leading right singular vectors. You must derive the correct projected expression from the Tikhonov objective and the low-rank factors, without assuming any shortcut formula.\n\n3) Compute the exact Tikhonov solution $x_{\\lambda}$ by solving the Tikhonov problem exactly using a sound numerical linear algebra approach based on the full Singular Value Decomposition (SVD) of $A$.\n\n4) Quantify the following two errors:\n- The solution error in the Euclidean norm $E_{x} = \\|\\tilde{x}_{\\lambda} - x_{\\lambda}\\|_{2}$.\n- The residual-norm discrepancy $E_{r} = \\left| \\|A \\tilde{x}_{\\lambda} - b\\|_{2} - \\|A x_{\\lambda} - b\\|_{2} \\right|$.\n\nYour program must synthesize test data in a scientifically realistic and reproducible way, as follows.\n- Use a fixed pseudorandom seed $0$ for all randomness.\n- To generate $A$, construct thin orthonormal factors $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n \\times r}$ with $r = \\min(m,n)$ by applying the economy-size QR factorization to matrices with independent standard normal entries. Then set $A = U \\operatorname{diag}(s) V^{\\top}$, where the singular values $s \\in \\mathbb{R}^{r}$ are specified by a decaying law:\n  - Geometric decay with ratio $\\rho \\in (0,1)$: $s_{i} = \\max(s_{\\min}, \\rho^{i-1})$ for $i = 1,2,\\dots,r$, with $s_{\\min} > 0$.\n  - Power-law decay with exponent $\\alpha > 0$: $s_{i} = (i)^{-\\alpha}$ for $i = 1,2,\\dots,r$.\n- Generate a ground-truth vector $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$ with independent standard normal entries.\n- Form $b = A x_{\\mathrm{true}} + \\eta$, where the noise vector $\\eta \\in \\mathbb{R}^{m}$ has independent normal entries with mean $0$ and standard deviation equal to a given noise level.\n\nImplement the RSVD with oversampling $p$ and power iterations $q$ using only matrix multiplications and orthonormalizations aligned with the above construction. The projected Tikhonov solution must be derived and evaluated using the approximate rank-$k$ factors. The exact Tikhonov solution must be computed using the full SVD of $A$.\n\nTest suite. Your program must run the following five test cases and collect the requested errors. In each case, the tuple lists $(m,n,\\text{decay type},\\text{decay param},s_{\\min},k,p,q,\\lambda,\\text{noise level})$:\n- Case $1$: $(80,60,\\text{geometric},\\rho=0.9,10^{-12},20,10,1,10^{-1},10^{-3})$.\n- Case $2$: $(60,60,\\text{powerlaw},\\alpha=2.0,10^{-12},60,5,0,10^{-6},0)$.\n- Case $3$: $(80,50,\\text{geometric},\\rho=0.8,10^{-12},5,5,2,10^{1},10^{-4})$.\n- Case $4$: $(40,100,\\text{powerlaw},\\alpha=1.5,10^{-12},25,10,1,10^{-2},5 \\cdot 10^{-3})$.\n- Case $5$: $(120,80,\\text{geometric},\\rho=0.95,10^{-12},15,5,3,10^{-2},10^{-2})$.\n\nOutput specification. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case in the above order, append two floating-point numbers in scientific notation with $10$ significant digits: first $E_{x}$, then $E_{r}$. Therefore, the output will be a list of length $10$ in the order\n$$\n[E_{x}^{(1)}, E_{r}^{(1)}, E_{x}^{(2)}, E_{r}^{(2)}, E_{x}^{(3)}, E_{r}^{(3)}, E_{x}^{(4)}, E_{r}^{(4)}, E_{x}^{(5)}, E_{r}^{(5)}].\n$$\nNo additional text should be printed besides this single line. Angles are not involved, and there are no physical units required in this problem. All floating-point outputs must be printed in scientific notation with $10$ significant digits.",
            "solution": "The problem requires the implementation and comparison of two methods for solving a Tikhonov-regularized linear inverse problem. The objective is to find $x \\in \\mathbb{R}^{n}$ that minimizes\n$$\nJ(x) = \\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2}\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, and $\\lambda > 0$ is the regularization parameter. We will first derive the exact solution using the Singular Value Decomposition (SVD) of $A$, then derive an approximate solution using a Randomized SVD (RSVD), and finally quantify the difference between them.\n\n### Data Generation\nAs per the problem specification, the test data is synthesized in a controlled, reproducible manner. We set the global pseudorandom seed to $0$.\n1.  Let $r = \\min(m, n)$. We generate two random matrices of size $m \\times r$ and $n \\times r$ with entries drawn from the standard normal distribution $\\mathcal{N}(0, 1)$. Applying the economy-size QR factorization to these matrices yields orthonormal matrices $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n \\times r}$.\n2.  A vector of singular values $s \\in \\mathbb{R}^{r}$ is constructed according to either a geometric decay law, $s_i = \\max(s_{\\min}, \\rho^{i-1})$, or a power-law decay, $s_i = i^{-\\alpha}$, for $i=1, \\dots, r$.\n3.  The matrix $A$ is formed as $A = U \\operatorname{diag}(s) V^{\\top}$. By construction, $A = U \\Sigma V^{\\top}$ is the compact SVD of $A$.\n4.  A ground-truth solution $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$ is generated with entries from $\\mathcal{N}(0, 1)$.\n5.  The observation vector $b \\in \\mathbb{R}^{m}$ is formed by $b = A x_{\\mathrm{true}} + \\eta$, where $\\eta \\in \\mathbb{R}^{m}$ is a noise vector with i.i.d. entries from $\\mathcal{N}(0, \\sigma_{\\text{noise}}^2)$, where $\\sigma_{\\text{noise}}$ is the specified noise level.\n\n### Exact Tikhonov Solution via Full SVD\nTo find the exact minimizer $x_{\\lambda}$ of the Tikhonov objective $J(x)$, we leverage the SVD of $A$, which is $A = U \\Sigma V^{\\top}$. We perform a change of variables, letting $x = Vz$, where $z \\in \\mathbb{R}^{r}$. Since $V$ has orthonormal columns, $\\|x\\|_{2} = \\|Vz\\|_{2} = \\|z\\|_{2}$. The objective function becomes:\n$$\nJ(z) = \\|U \\Sigma V^{\\top} (Vz) - b\\|_{2}^{2} + \\lambda^{2} \\|z\\|_{2}^{2} = \\|U \\Sigma z - b\\|_{2}^{2} + \\lambda^{2} \\|z\\|_{2}^{2}\n$$\nUsing the unitary invariance of the Euclidean norm, $\\|U y\\|_{2} = \\|y\\|_{2}$, we can write:\n$$\nJ(z) = \\|\\Sigma z - U^{\\top}b\\|_{2}^{2} + \\lambda^{2}\\|z\\|_{2}^{2}\n$$\nLetting $c = U^{\\top}b$, the objective function is separable in the components of $z$:\n$$\nJ(z) = \\sum_{i=1}^{r} (\\sigma_i z_i - c_i)^2 + \\lambda^{2} \\sum_{i=1}^{r} z_i^2 = \\sum_{i=1}^{r} \\left( (\\sigma_i z_i - c_i)^2 + \\lambda^{2} z_i^2 \\right)\n$$\nTo find the minimum, we take the derivative with respect to each $z_i$ and set it to zero:\n$$\n\\frac{\\partial J}{\\partial z_i} = 2\\sigma_i(\\sigma_i z_i - c_i) + 2\\lambda^2 z_i = 0 \\implies (\\sigma_i^2 + \\lambda^2)z_i = \\sigma_i c_i\n$$\nSolving for $z_i$ gives $z_i = \\frac{\\sigma_i c_i}{\\sigma_i^2 + \\lambda^2}$. Transforming back to the original variable $x = Vz$, the exact Tikhonov solution is:\n$$\nx_{\\lambda} = Vz = \\sum_{i=1}^{r} v_i z_i = \\sum_{i=1}^{r} v_i \\frac{\\sigma_i (u_i^{\\top}b)}{\\sigma_i^2 + \\lambda^2}\n$$\nwhere $u_i$ and $v_i$ are the $i$-th columns of $U$ and $V$, respectively.\n\n### Approximate Tikhonov Solution via Randomized SVD\nThe Randomized SVD (RSVD) provides a computationally efficient way to find a low-rank approximation of $A$. We compute an approximate rank-$k$ factorization $A \\approx U_k \\Sigma_k V_k^{\\top}$.\n\n1.  **Subspace Finding**: We construct an orthonormal matrix $Q \\in \\mathbb{R}^{m \\times \\ell}$ whose columns approximate the range of $A$. Here, $\\ell = \\min(m, n, k+p)$ where $k$ is the target rank and $p$ is an oversampling parameter.\n    a. Draw a Gaussian random matrix $\\Omega \\in \\mathbb{R}^{n \\times \\ell}$.\n    b. Form a sample matrix $Y = A \\Omega$.\n    c. To improve accuracy, especially for slowly decaying singular values, we apply $q$ power iterations. A numerically stable scheme is used:\n       $Q, \\_ = \\text{QR}(Y)$. For $j=1, \\dots, q$: $Q, \\_ = \\text{QR}(A(\\text{QR}(A^\\top Q)[0]))$.\n    d. The final matrix $Q$ is our orthonormal basis.\n\n2.  **Projection**: We project $A$ onto the subspace spanned by $Q$: $B = Q^{\\top}A \\in \\mathbb{R}^{\\ell \\times n}$.\n\n3.  **Factorization**: Compute the SVD of the small matrix $B = \\tilde{U} \\Sigma V^{\\top}$ (where dimensions are $\\ell \\times \\ell$, $\\ell \\times n$, $n \\times n$). We form the approximate left singular vectors $U = Q\\tilde{U}$.\n\n4.  **Truncation**: We take the leading $k$ components to get the rank-$k$ approximation $A \\approx U_k \\Sigma_k V_k^{\\top}$, where $U_k \\in \\mathbb{R}^{m \\times k}$, $\\Sigma_k \\in \\mathbb{R}^{k \\times k}$, and $V_k \\in \\mathbb{R}^{n \\times k}$.\n\nThe problem states that the approximate solution $\\tilde{x}_{\\lambda}$ is computed \"using only the approximate factors\". This is interpreted as replacing $A$ with its approximation $A_k = U_k \\Sigma_k V_k^{\\top}$ in the Tikhonov problem. The solution structure is identical to the exact case, but truncated at rank $k$ and using the approximate factors:\n$$\n\\tilde{x}_{\\lambda} = \\sum_{i=1}^{k} (v_k)_i \\frac{(\\sigma_k)_i ((u_k)_i^{\\top}b)}{(\\sigma_k)_i^2 + \\lambda^2}\n$$\nHere, $(u_k)_i, (\\sigma_k)_i, (v_k)_i$ are the $i$-th approximate singular vectors and value from the RSVD.\n\n### Error Quantification\nThe following two errors are computed to compare the approximate and exact solutions:\n1.  **Solution Error**: The Euclidean distance between the two solution vectors.\n    $E_x = \\|\\tilde{x}_{\\lambda} - x_{\\lambda}\\|_{2}$\n2.  **Residual-Norm Discrepancy**: The absolute difference in the Euclidean norms of the corresponding residuals.\n    $E_r = \\left| \\|A \\tilde{x}_{\\lambda} - b\\|_{2} - \\|A x_{\\lambda} - b\\|_{2} \\right|$\nThese metrics assess how well the randomized approximation scheme performs in the context of Tikhonov regularization.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Tikhonov regularization problem using full SVD and Randomized SVD,\n    and computes the error between the two solutions.\n    \"\"\"\n    \n    class TestCase:\n        \"\"\"Helper class to store test case parameters.\"\"\"\n        def __init__(self, m, n, decay_type, decay_param, s_min, k, p, q, lambda_reg, noise_level):\n            self.m = m\n            self.n = n\n            self.decay_type = decay_type\n            self.decay_param = decay_param\n            self.s_min = s_min\n            self.k = k\n            self.p = p\n            self.q = q\n            self.lambda_reg = lambda_reg\n            self.noise_level = noise_level\n\n    def generate_data(params: TestCase, rng: np.random.Generator):\n        \"\"\"Generates problem data (A, b) and the true SVD of A.\"\"\"\n        m, n = params.m, params.n\n        r = min(m, n)\n        \n        # Orthonormal factors U and V from QR of Gaussian matrices\n        U, _ = np.linalg.qr(rng.standard_normal((m, r)))\n        V, _ = np.linalg.qr(rng.standard_normal((n, r)))\n        \n        # Singular values with specified decay\n        indices = np.arange(1, r + 1)\n        if params.decay_type == 'geometric':\n            s = np.maximum(params.s_min, params.decay_param ** (indices - 1))\n        elif params.decay_type == 'powerlaw':\n            s = indices ** (-params.decay_param)\n        else:\n            raise ValueError(\"Unknown decay type\")\n            \n        A = U @ np.diag(s) @ V.T\n        \n        x_true = rng.standard_normal((n, 1))\n        noise = rng.normal(0, params.noise_level, (m, 1))\n        b = A @ x_true + noise\n        \n        return A, b, (U, s, V.T)\n\n    def randomized_svd(A, k, p, q, rng):\n        \"\"\"\n        Computes an approximate rank-k SVD of A using a randomized algorithm\n        with oversampling and power iterations.\n        \"\"\"\n        m, n = A.shape\n        ell = min(m, n, k + p)\n        \n        # Stage A: Sketching and Power Iterations\n        Omega = rng.standard_normal((n, ell))\n        Y = A @ Omega\n        \n        # Stabilized power iteration scheme\n        Q, _ = np.linalg.qr(Y)\n        for _ in range(q):\n            Y_star = A.T @ Q\n            Q_star, _ = np.linalg.qr(Y_star)\n            Y = A @ Q_star\n            Q, _ = np.linalg.qr(Y)\n            \n        # Stage B: Project and Factorize\n        B = Q.T @ A\n        U_tilde, s_approx, V_T_approx = np.linalg.svd(B, full_matrices=False)\n        U_approx = Q @ U_tilde\n        \n        # Truncate to desired rank k\n        U_k = U_approx[:, :k]\n        s_k = s_approx[:k]\n        V_T_k = V_T_approx[:k, :]\n        \n        return U_k, s_k, V_T_k\n\n    # Use a single RNG for reproducibility across all test cases\n    rng = np.random.default_rng(0)\n\n    test_cases = [\n        TestCase(80, 60, 'geometric', 0.9, 1e-12, 20, 10, 1, 1e-1, 1e-3),\n        TestCase(60, 60, 'powerlaw', 2.0, 1e-12, 60, 5, 0, 1e-6, 0.0),\n        TestCase(80, 50, 'geometric', 0.8, 1e-12, 5, 5, 2, 1e1, 1e-4),\n        TestCase(40, 100, 'powerlaw', 1.5, 1e-12, 25, 10, 1, 1e-2, 5e-3),\n        TestCase(120, 80, 'geometric', 0.95, 1e-12, 15, 5, 3, 1e-2, 1e-2)\n    ]\n    \n    results = []\n\n    for case in test_cases:\n        A, b, (U_true, s_true, V_T_true) = generate_data(case, rng)\n        V_true = V_T_true.T\n        lambda_sq = case.lambda_reg**2\n        \n        # 1. Compute exact Tikhonov solution using full SVD\n        c_true = U_true.T @ b\n        s_true_col = s_true.reshape(-1, 1)\n        d_true = s_true_col / (s_true_col**2 + lambda_sq)\n        x_lambda = V_true @ (d_true * c_true)\n        \n        # 2. Compute approximate Tikhonov solution using RSVD\n        U_k, s_k, V_T_k = randomized_svd(A, case.k, case.p, case.q, rng)\n        V_k = V_T_k.T\n        \n        c_k = U_k.T @ b\n        s_k_col = s_k.reshape(-1, 1)\n        # Add a small epsilon to denominator to avoid division by zero if s_k is zero.\n        d_k = s_k_col / (s_k_col**2 + lambda_sq + np.finfo(float).eps)\n        x_tilde_lambda = V_k @ (d_k * c_k)\n\n        # 3. Quantify errors\n        E_x = np.linalg.norm(x_tilde_lambda - x_lambda)\n        \n        res_tilde = np.linalg.norm(A @ x_tilde_lambda - b)\n        res_exact = np.linalg.norm(A @ x_lambda - b)\n        E_r = np.abs(res_tilde - res_exact)\n        \n        # Format output to scientific notation with 10 significant digits\n        results.append(\"{:.9e}\".format(E_x))\n        results.append(\"{:.9e}\".format(E_r))\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Beyond implementation, a deep understanding of randomized methods requires analyzing their statistical behavior. This practice moves from coding to theoretical derivation, challenging you to compute the exact variance of the Hutchinson trace estimator (). This classic pencil-and-paper exercise reveals how the structure of a matrix and the properties of random probes combine to determine the estimator's reliability, a cornerstone of uncertainty quantification in large-scale models.",
            "id": "3416525",
            "problem": "Consider a linear inverse problem in which a symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ arises as a data-misfit Hessian or a posterior covariance surrogate within a Gaussian data assimilation setting. Suppose one wishes to estimate $\\operatorname{tr}(A)$ using the Hutchinson estimator with Rademacher probes. Let $z^{(1)}, \\ldots, z^{(s)} \\in \\mathbb{R}^{n}$ be independent random vectors with independent and identically distributed entries taking values $\\pm 1$ with equal probability. Define the estimator\n$$\n\\hat{\\tau} \\;=\\; \\frac{1}{s} \\sum_{t=1}^{s} \\left(z^{(t)}\\right)^{\\top} A\\, z^{(t)}.\n$$\nStarting only from the independence of the entries of each $z^{(t)}$, the facts $\\mathbb{E}[z_i]=0$, $\\mathbb{E}[z_i^2]=1$, $\\mathbb{E}[z_i^4]=1$, and basic properties of the trace, derive an exact closed-form expression for the variance of $\\hat{\\tau}$ when $A$ is symmetric. Express your final result explicitly in terms of $s$, the Frobenius norm $\\|A\\|_{F}$, and the diagonal entries of $A$. Your final answer must be a single analytic expression. No numerical evaluation is required.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. The context is standard within randomized numerical linear algebra, and the premises are mathematically sound. We proceed to derive the variance of the Hutchinson trace estimator.\n\nThe estimator for the trace of a symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ is given by\n$$\n\\hat{\\tau} = \\frac{1}{s} \\sum_{t=1}^{s} \\left(z^{(t)}\\right)^{\\top} A\\, z^{(t)}\n$$\nwhere $z^{(1)}, \\ldots, z^{(s)}$ are $s$ independent and identically distributed (i.i.d.) random vectors. Each vector $z^{(t)}$ consists of $n$ i.i.d. Rademacher random variables, $z_i^{(t)}$, satisfying $\\mathbb{E}[z_i] = 0$, $\\mathbb{E}[z_i^2] = 1$, and $\\mathbb{E}[z_i^4] = 1$. Let $Y_t = (z^{(t)})^{\\top} A z^{(t)}$. Since the vectors $z^{(t)}$ are i.i.d., the random variables $Y_t$ are also i.i.d. The variance of the sample mean $\\hat{\\tau}$ is\n$$\n\\operatorname{Var}(\\hat{\\tau}) = \\operatorname{Var}\\left(\\frac{1}{s} \\sum_{t=1}^{s} Y_t\\right) = \\frac{1}{s^2} \\sum_{t=1}^{s} \\operatorname{Var}(Y_t) = \\frac{s}{s^2} \\operatorname{Var}(Y_1) = \\frac{1}{s} \\operatorname{Var}(Y_1)\n$$\nWe only need to compute the variance of a single term $Y = z^{\\top} A z$, where we drop the superscript for simplicity. The variance is given by $\\operatorname{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$.\n\nFirst, we compute the expectation of $Y$.\n$$\n\\mathbb{E}[Y] = \\mathbb{E}\\left[z^{\\top} A z\\right] = \\mathbb{E}\\left[\\sum_{i=1}^{n} \\sum_{j=1}^{n} A_{ij} z_i z_j\\right]\n$$\nBy linearity of expectation,\n$$\n\\mathbb{E}[Y] = \\sum_{i=1}^{n} \\sum_{j=1}^{n} A_{ij} \\mathbb{E}[z_i z_j]\n$$\nThe entries $z_i$ are independent, and $\\mathbb{E}[z_i] = 0$. Thus, for $i \\neq j$, $\\mathbb{E}[z_i z_j] = \\mathbb{E}[z_i]\\mathbb{E}[z_j] = 0 \\cdot 0 = 0$. For $i = j$, we have $\\mathbb{E}[z_i z_i] = \\mathbb{E}[z_i^2] = 1$. This means $\\mathbb{E}[z_i z_j] = \\delta_{ij}$, the Kronecker delta.\nSubstituting this into the expression for $\\mathbb{E}[Y]$:\n$$\n\\mathbb{E}[Y] = \\sum_{i=1}^{n} \\sum_{j=1}^{n} A_{ij} \\delta_{ij} = \\sum_{i=1}^{n} A_{ii} = \\operatorname{tr}(A)\n$$\nThis confirms that $\\hat{\\tau}$ is an unbiased estimator of the trace.\n\nNext, we compute the second moment, $\\mathbb{E}[Y^2]$.\n$$\nY^2 = \\left(\\sum_{i,j} A_{ij} z_i z_j\\right)^2 = \\left(\\sum_{i,j} A_{ij} z_i z_j\\right) \\left(\\sum_{k,l} A_{kl} z_k z_l\\right) = \\sum_{i,j,k,l} A_{ij} A_{kl} z_i z_j z_k z_l\n$$\nTaking the expectation,\n$$\n\\mathbb{E}[Y^2] = \\sum_{i,j,k,l} A_{ij} A_{kl} \\mathbb{E}[z_i z_j z_k z_l]\n$$\nThe expectation $\\mathbb{E}[z_i z_j z_k z_l]$ is non-zero only if each distinct index appears an even number of times in the product, due to the independence of the $z_p$ and $\\mathbb{E}[z_p]=0$. Given that $\\mathbb{E}[z_p^2]=1$ and $\\mathbb{E}[z_p^4]=1$, the non-zero cases for the expectation of the product $z_i z_j z_k z_l$ are:\n1.  All indices are equal: $i=j=k=l$. The expectation is $\\mathbb{E}[z_i^4] = 1$.\n2.  The indices form two distinct pairs. For example, $i=j$ and $k=l$ with $i \\neq k$. The expectation is $\\mathbb{E}[z_i^2 z_k^2] = \\mathbb{E}[z_i^2]\\mathbb{E}[z_k^2] = 1 \\cdot 1 = 1$.\n\nWe now sum the contributions from all non-zero terms:\n\\\nCase 1: $i=j=k=l$.\nThe contribution to $\\mathbb{E}[Y^2]$ is $\\sum_{i=1}^n A_{ii} A_{ii} \\mathbb{E}[z_i^4] = \\sum_{i=1}^n A_{ii}^2$.\n\nCase 2: Indices form two distinct pairs.\n   a) $i=j$ and $k=l$, with $i \\neq k$. The expectation is $\\mathbb{E}[z_i^2 z_k^2]=1$. The contribution is $\\sum_{i \\neq k} A_{ii} A_{kk}$.\n   b) $i=k$ and $j=l$, with $i \\neq j$. The expectation is $\\mathbb{E}[z_i^2 z_j^2]=1$. The contribution is $\\sum_{i \\neq j} A_{ij} A_{ij} = \\sum_{i \\neq j} A_{ij}^2$.\n   c) $i=l$ and $j=k$, with $i \\neq j$. The expectation is $\\mathbb{E}[z_i^2 z_j^2]=1$. The contribution is $\\sum_{i \\neq j} A_{ij} A_{ji}$.\n\nCombining these contributions:\n$$\n\\mathbb{E}[Y^2] = \\sum_{i} A_{ii}^2 + \\sum_{i \\neq k} A_{ii} A_{kk} + \\sum_{i \\neq j} A_{ij}^2 + \\sum_{i \\neq j} A_{ij} A_{ji}\n$$\nWe can simplify this expression. The first two terms combine to form the square of the trace:\n$$\n\\sum_{i} A_{ii}^2 + \\sum_{i \\neq k} A_{ii} A_{kk} = \\sum_{i,k} A_{ii} A_{kk} = \\left(\\sum_i A_{ii}\\right)\\left(\\sum_k A_{kk}\\right) = (\\operatorname{tr}(A))^2\n$$\nSince the matrix $A$ is symmetric, $A_{ij} = A_{ji}$. Therefore, the last term becomes:\n$$\n\\sum_{i \\neq j} A_{ij} A_{ji} = \\sum_{i \\neq j} A_{ij} A_{ij} = \\sum_{i \\neq j} A_{ij}^2\n$$\nSo, the expression for the second moment simplifies to:\n$$\n\\mathbb{E}[Y^2] = (\\operatorname{tr}(A))^2 + \\sum_{i \\neq j} A_{ij}^2 + \\sum_{i \\neq j} A_{ij}^2 = (\\operatorname{tr}(A))^2 + 2\\sum_{i \\neq j} A_{ij}^2\n$$\nNow we compute the variance of $Y$:\n$$\n\\operatorname{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2 = \\left( (\\operatorname{tr}(A))^2 + 2\\sum_{i \\neq j} A_{ij}^2 \\right) - (\\operatorname{tr}(A))^2 = 2\\sum_{i \\neq j} A_{ij}^2\n$$\nThe problem requires the answer in terms of the Frobenius norm $\\|A\\|_F$ and the diagonal entries $A_{ii}$. The squared Frobenius norm is the sum of the squares of all entries:\n$$\n\\|A\\|_F^2 = \\sum_{i=1}^n \\sum_{j=1}^n A_{ij}^2\n$$\nWe can separate this sum into diagonal and off-diagonal terms:\n$$\n\\|A\\|_F^2 = \\sum_{i=1}^n A_{ii}^2 + \\sum_{i \\neq j} A_{ij}^2\n$$\nRearranging this gives an expression for the sum of squared off-diagonal entries:\n$$\n\\sum_{i \\neq j} A_{ij}^2 = \\|A\\|_F^2 - \\sum_{i=1}^n A_{ii}^2\n$$\nSubstituting this into our expression for $\\operatorname{Var}(Y)$:\n$$\n\\operatorname{Var(Y)} = 2 \\left( \\|A\\|_F^2 - \\sum_{i=1}^n A_{ii}^2 \\right)\n$$\nFinally, we find the variance of the estimator $\\hat{\\tau}$:\n$$\n\\operatorname{Var}(\\hat{\\tau}) = \\frac{1}{s}\\operatorname{Var}(Y) = \\frac{2}{s} \\left( \\|A\\|_F^2 - \\sum_{i=1}^n A_{ii}^2 \\right)\n$$\nThis is the desired closed-form expression for the variance of the Hutchinson estimator for a symmetric matrix $A$ using Rademacher probes.",
            "answer": "$$\n\\boxed{\\frac{2}{s} \\left( \\|A\\|_{F}^{2} - \\sum_{i=1}^{n} A_{ii}^2 \\right)}\n$$"
        },
        {
            "introduction": "In real-world scenarios, algorithmic choices are driven by performance trade-offs on physical hardware. This final practice places you in a scenario with a large-scale, out-of-core dataset, where I/O and computational costs are critical (). By creating a performance model for both dense Gaussian and structured SRHT sketches, you will make an evidence-based decision that respects a strict time budget, illustrating the crucial link between algorithmic theory and practical engineering.",
            "id": "3416535",
            "problem": "A data assimilation workflow requires a randomized Singular Value Decomposition (SVD) range-finder for a very large, tall matrix $A \\in \\mathbb{R}^{n \\times d}$ that is stored out-of-core on a single disk. The matrix $A$ has $n = 5 \\times 10^{7}$ rows and $d = 512$ columns and is stored in double precision (each entry is $8$ bytes). The computing system has a sustained sequential disk bandwidth of $B_{\\mathrm{disk}} = 2.0 \\times 10^{9}$ bytes per second and a peak sustained floating-point throughput of $R_{\\mathrm{flop}} = 3.0 \\times 10^{11}$ floating-point operations per second. The available random-access memory (RAM) is $M = 3.2 \\times 10^{10}$ bytes. The target sketch size is $l = 256$, and the algorithm must complete within the time budget $T_{\\max} = 120$ seconds. Assume a single-pass range-finder that forms $Y = A \\Omega$ and does not overlap disk input/output with computation. You may neglect costs due to the subsequent orthonormalization of $Y$ and any small constant-time tasks.\n\nTwo sketching choices are available:\n\n- A Gaussian sketch with $\\Omega \\in \\mathbb{R}^{d \\times l}$ with independent and identically distributed standard normal entries, applied as a dense matrix multiplication.\n\n- A Subsampled Randomized Hadamard Transform (SRHT) sketch defined by $\\Omega_{\\mathrm{SRHT}} = \\sqrt{\\frac{d}{l}} D H R$, where $D$ is a diagonal Rademacher matrix, $H$ is the Walshâ€“Hadamard transform of length $d$, and $R$ selects $l$ columns uniformly without replacement.\n\nYou must model the wall-clock time for each sketch as the sum of the time to read $A$ once from disk and the time to perform the required floating-point operations to form $Y = A \\Omega$. For the Gaussian sketch, treat the dense matrix multiplication as $2ndl$ floating-point operations. For the SRHT sketch, treat the per-row cost of applying $D H$ as $2 d \\log_{2}(d)$ floating-point additions plus $d$ sign flips, for a total of $2 d \\log_{2}(d) + d$ floating-point operations per row, and assume sampling by $R$ is negligible.\n\nThe computation is performed in streaming blocks of $m_{b}$ rows; at any moment, memory must accommodate one block of $A$ with size $m_{b} \\times d$, the corresponding block of $Y$ with size $m_{b} \\times l$, and the sketching matrix (or transform data) of size $d \\times l$. Use double-precision storage for all arrays. Determine a feasible $m_{b}$ under the RAM limit and assume the compute throughput $R_{\\mathrm{flop}}$ is achieved for those blocks.\n\nDecide which sketch meets the time budget, and report the predicted total wall-clock time (in seconds) of the chosen sketch. Round your answer to three significant figures and express the final time in seconds.",
            "solution": "The task is to estimate the wall-clock time for forming $Y = A \\Omega$ as the sum of input/output (I/O) time and compute time, and then to decide which sketching choice (Gaussian or Subsampled Randomized Hadamard Transform (SRHT)) satisfies the time budget $T_{\\max}$.\n\nWe start from the fundamental definitions:\n\n- The total bytes of the matrix $A$ are\n$$\n\\text{bytes}(A) = n d \\times 8.\n$$\n\n- The disk read time for one pass over $A$ at bandwidth $B_{\\mathrm{disk}}$ is\n$$\nT_{\\mathrm{io}} = \\frac{\\text{bytes}(A)}{B_{\\mathrm{disk}}}.\n$$\n\n- For the Gaussian sketch, the floating-point operation count (dense matrix multiplication) is\n$$\n\\text{flops}_{\\mathrm{Gauss}} = 2 n d l,\n$$\nand the corresponding compute time is\n$$\nT_{\\mathrm{comp}}^{\\mathrm{Gauss}} = \\frac{\\text{flops}_{\\mathrm{Gauss}}}{R_{\\mathrm{flop}}}.\n$$\n\n- For the SRHT sketch, applying $D H$ per row costs $2 d \\log_{2}(d) + d$ floating-point operations, hence the total floating-point operation count is\n$$\n\\text{flops}_{\\mathrm{SRHT}} = n \\left( 2 d \\log_{2}(d) + d \\right),\n$$\nand the compute time is\n$$\nT_{\\mathrm{comp}}^{\\mathrm{SRHT}} = \\frac{\\text{flops}_{\\mathrm{SRHT}}}{R_{\\mathrm{flop}}}.\n$$\n\nThe total time for each method is modeled as\n$$\nT_{\\mathrm{total}}^{(\\cdot)} = T_{\\mathrm{io}} + T_{\\mathrm{comp}}^{(\\cdot)},\n$$\nwith no overlap between I/O and computation.\n\nWe must also check the RAM feasibility for streaming blocks of $m_{b}$ rows. At any time, memory must hold the following arrays:\n\n- One block of $A$ of size $m_{b} \\times d$ doubles: $8 m_{b} d$ bytes.\n- One block of $Y$ of size $m_{b} \\times l$ doubles: $8 m_{b} l$ bytes.\n- The sketching matrix or transform data of size $d \\times l$ doubles: $8 d l$ bytes.\n\nThus the RAM constraint is\n$$\n8 \\left( m_{b} d + m_{b} l + d l \\right) \\leq M.\n$$\nSolving for $m_{b}$,\n$$\nm_{b} \\leq \\frac{\\frac{M}{8} - d l}{d + l}.\n$$\nWe will verify that a positive integer $m_{b}$ exists, ensuring streaming feasibility.\n\nNow we substitute the given numerical values:\n\n- $n = 5 \\times 10^{7}$,\n- $d = 512$,\n- $l = 256$,\n- $B_{\\mathrm{disk}} = 2.0 \\times 10^{9} \\ \\text{bytes/s}$,\n- $R_{\\mathrm{flop}} = 3.0 \\times 10^{11} \\ \\text{flops/s}$,\n- $M = 3.2 \\times 10^{10} \\ \\text{bytes}$.\n\nFirst, compute the size of $A$ and the I/O time:\n$$\n\\text{bytes}(A) = n d \\times 8 = \\left( 5 \\times 10^{7} \\right) \\cdot 512 \\cdot 8 \\ \\text{bytes}.\n$$\nNote that $512 \\cdot 8 = 4096$, hence\n$$\n\\text{bytes}(A) = \\left( 5 \\times 10^{7} \\right) \\cdot 4096 = 2.048 \\times 10^{11} \\ \\text{bytes}.\n$$\nTherefore,\n$$\nT_{\\mathrm{io}} = \\frac{2.048 \\times 10^{11}}{2.0 \\times 10^{9}} = 1.024 \\times 10^{2} \\ \\text{s} = 102.4 \\ \\text{s}.\n$$\n\nNext, check the RAM feasibility. Convert $M$ to doubles:\n$$\n\\frac{M}{8} = \\frac{3.2 \\times 10^{10}}{8} = 4.0 \\times 10^{9} \\ \\text{doubles}.\n$$\nCompute $d l$ and $d + l$:\n$$\nd l = 512 \\cdot 256 = 131{,}072, \\quad d + l = 512 + 256 = 768.\n$$\nThen,\n$$\nm_{b} \\leq \\frac{4.0 \\times 10^{9} - 131{,}072}{768}.\n$$\nCompute the numerator:\n$$\n4.0 \\times 10^{9} - 131{,}072 = 3{,}999{,}868{,}928.\n$$\nDivide by $768$:\n$$\nm_{b} \\leq \\frac{3{,}999{,}868{,}928}{768} = 5{,}208{,}149.0\\overline{6}.\n$$\nThus, a large positive integer $m_{b}$ exists (for example, $m_{b} = 5{,}208{,}149$), and streaming is feasible under the RAM limit while maintaining high throughput.\n\nNow compute the Gaussian compute time. The floating-point operations are\n$$\n\\text{flops}_{\\mathrm{Gauss}} = 2 n d l = 2 \\cdot \\left( 5 \\times 10^{7} \\right) \\cdot 512 \\cdot 256.\n$$\nFirst compute $n d$:\n$$\nn d = \\left( 5 \\times 10^{7} \\right) \\cdot 512 = 2.56 \\times 10^{10}.\n$$\nThen multiply by $l$:\n$$\nn d l = \\left( 2.56 \\times 10^{10} \\right) \\cdot 256 = 6.5536 \\times 10^{12}.\n$$\nMultiply by $2$:\n$$\n\\text{flops}_{\\mathrm{Gauss}} = 1.31072 \\times 10^{13}.\n$$\nTherefore,\n$$\nT_{\\mathrm{comp}}^{\\mathrm{Gauss}} = \\frac{1.31072 \\times 10^{13}}{3.0 \\times 10^{11}} = 4.369066\\overline{6} \\times 10^{1} \\ \\text{s} \\approx 43.6906667 \\ \\text{s}.\n$$\nTotal time for Gaussian sketch:\n$$\nT_{\\mathrm{total}}^{\\mathrm{Gauss}} = T_{\\mathrm{io}} + T_{\\mathrm{comp}}^{\\mathrm{Gauss}} = 102.4 + 43.6906667 \\approx 146.0906667 \\ \\text{s}.\n$$\n\nNow compute the SRHT compute time. With $d = 512$, we have $\\log_{2}(d) = 9$. The floating-point operations are\n$$\n\\text{flops}_{\\mathrm{SRHT}} = n \\left( 2 d \\log_{2}(d) + d \\right) = \\left( 5 \\times 10^{7} \\right) \\left( 2 \\cdot 512 \\cdot 9 + 512 \\right).\n$$\nCompute the per-row cost:\n$$\n2 \\cdot 512 \\cdot 9 = 9216, \\quad 9216 + 512 = 9728.\n$$\nThus,\n$$\n\\text{flops}_{\\mathrm{SRHT}} = \\left( 5 \\times 10^{7} \\right) \\cdot 9728 = 4.864 \\times 10^{11}.\n$$\nTherefore,\n$$\nT_{\\mathrm{comp}}^{\\mathrm{SRHT}} = \\frac{4.864 \\times 10^{11}}{3.0 \\times 10^{11}} = 1.621333\\overline{3} \\ \\text{s} \\approx 1.6213333 \\ \\text{s}.\n$$\nTotal time for SRHT:\n$$\nT_{\\mathrm{total}}^{\\mathrm{SRHT}} = T_{\\mathrm{io}} + T_{\\mathrm{comp}}^{\\mathrm{SRHT}} = 102.4 + 1.6213333 \\approx 104.0213333 \\ \\text{s}.\n$$\n\nCompare each total time to the budget $T_{\\max} = 120$ seconds:\n- $T_{\\mathrm{total}}^{\\mathrm{Gauss}} \\approx 146.09 \\ \\text{s} > 120 \\ \\text{s}$, so the Gaussian sketch does not meet the time budget.\n- $T_{\\mathrm{total}}^{\\mathrm{SRHT}} \\approx 104.02 \\ \\text{s} < 120 \\ \\text{s}$, so the SRHT sketch meets the time budget.\n\nTherefore, the SRHT sketch should be chosen. The predicted total wall-clock time is $T_{\\mathrm{total}}^{\\mathrm{SRHT}} \\approx 104.0213333$ seconds. Rounding to three significant figures, the final time is $104$ seconds.",
            "answer": "$$\\boxed{104}$$"
        }
    ]
}