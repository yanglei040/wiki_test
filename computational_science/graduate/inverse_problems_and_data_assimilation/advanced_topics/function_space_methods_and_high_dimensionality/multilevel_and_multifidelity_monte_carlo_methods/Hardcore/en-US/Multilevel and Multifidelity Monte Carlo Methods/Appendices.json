{
    "hands_on_practices": [
        {
            "introduction": "Before exploring advanced methods, it is crucial to understand the performance of the standard single-level Monte Carlo (SLMC) estimator. This fundamental exercise establishes a quantitative baseline by calculating the number of samples required to achieve a desired accuracy for a given variance. Mastering this calculation  clarifies the canonical relationship between computational cost and statistical error, which provides the essential motivation for developing more efficient multilevel and multifidelity strategies.",
            "id": "3405131",
            "problem": "Consider a Bayesian inverse problem in which an unknown parameter $U$ is inferred from data using a well-posed likelihood and prior, yielding a posterior distribution $\\pi$. Let $\\varphi(U)$ be a square-integrable quantity of interest, and denote its posterior expectation by $\\mu=\\mathbb{E}_{\\pi}[\\varphi(U)]$. You plan to estimate $\\mu$ using a single-level Monte Carlo estimator based on $N$ independent and identically distributed samples $U^{(i)}\\sim \\pi$, namely $\\hat{\\mu}_{N}=\\frac{1}{N}\\sum_{i=1}^{N}\\varphi\\!\\left(U^{(i)}\\right)$. Assume the following:\n- The posterior variance of the integrand is known to be $\\operatorname{Var}_{\\pi}[\\varphi(U)]=\\sigma^{2}=4$.\n- The discretization error of the forward model is negligible at the chosen resolution, so that the Monte Carlo estimator is effectively unbiased with respect to $\\mu$.\n- Each sample has unit computational cost, so the total cost is equal to the number of samples $N$.\n\nUsing only the definitions of mean-square error and variance for independent averages, determine the minimal sample size $N$ required to ensure that the mean-square error satisfies $\\mathbb{E}\\!\\left[(\\hat{\\mu}_{N}-\\mu)^{2}\\right]\\le \\varepsilon^{2}$ for tolerance $\\varepsilon=0.01$. Then compute the associated total computational cost under the unit-cost model. Report both the minimal $N$ (dimensionless count) and the cost (in unit-cost tally) as exact integers with no rounding.",
            "solution": "The problem statement is validated as follows.\n\n**Step 1: Extract Givens**\n- The quantity to be estimated is the posterior expectation $\\mu=\\mathbb{E}_{\\pi}[\\varphi(U)]$.\n- The estimator is the single-level Monte Carlo estimator $\\hat{\\mu}_{N}=\\frac{1}{N}\\sum_{i=1}^{N}\\varphi\\!\\left(U^{(i)}\\right)$, where $\\{U^{(i)}\\}_{i=1}^{N}$ are independent and identically distributed (i.i.d.) samples from the posterior distribution $\\pi$.\n- The posterior variance of the quantity of interest is given as $\\operatorname{Var}_{\\pi}[\\varphi(U)] = \\sigma^{2} = 4$.\n- The estimator $\\hat{\\mu}_{N}$ is assumed to be unbiased with respect to $\\mu$.\n- The computational cost of one sample is $1$, so the total cost is $N$.\n- The target for the mean-square error (MSE) is $\\mathbb{E}\\!\\left[(\\hat{\\mu}_{N}-\\mu)^{2}\\right]\\le \\varepsilon^{2}$.\n- The tolerance is specified as $\\varepsilon=0.01$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is based on the fundamental principles of Monte Carlo estimation and error analysis, which are standard in computational statistics and numerical methods for inverse problems. All concepts are well-established.\n- **Well-Posed:** The problem provides all necessary information and asks for a uniquely determinable quantity (the minimal sample size) based on a clear criterion. The structure is logical and leads to a single, stable solution.\n- **Objective:** The problem is stated in precise mathematical language, free of ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will now be provided.\n\nThe mean-square error (MSE) of the estimator $\\hat{\\mu}_{N}$ with respect to the true value $\\mu$ is defined as $\\mathbb{E}\\!\\left[(\\hat{\\mu}_{N}-\\mu)^{2}\\right]$. The MSE can be decomposed into the square of the bias and the variance of the estimator:\n$$\n\\text{MSE} = \\mathbb{E}\\!\\left[(\\hat{\\mu}_{N}-\\mu)^{2}\\right] = \\left(\\mathbb{E}[\\hat{\\mu}_{N}]-\\mu\\right)^{2} + \\operatorname{Var}(\\hat{\\mu}_{N})\n$$\n\nFirst, we determine the bias of the estimator. The expected value of $\\hat{\\mu}_{N}$ is:\n$$\n\\mathbb{E}[\\hat{\\mu}_{N}] = \\mathbb{E}\\left[\\frac{1}{N}\\sum_{i=1}^{N}\\varphi(U^{(i)})\\right]\n$$\nBy the linearity of expectation, this becomes:\n$$\n\\mathbb{E}[\\hat{\\mu}_{N}] = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbb{E}[\\varphi(U^{(i)})]\n$$\nSince each sample $U^{(i)}$ is drawn from the posterior distribution $\\pi$, the expectation of $\\varphi(U^{(i)})$ is the posterior mean $\\mu$:\n$$\n\\mathbb{E}[\\varphi(U^{(i)})] = \\mathbb{E}_{\\pi}[\\varphi(U)] = \\mu \\quad \\text{for all } i \\in \\{1, \\dots, N\\}\n$$\nSubstituting this back, we get:\n$$\n\\mathbb{E}[\\hat{\\mu}_{N}] = \\frac{1}{N}\\sum_{i=1}^{N}\\mu = \\frac{1}{N}(N\\mu) = \\mu\n$$\nThus, the bias of the estimator is $\\mathbb{E}[\\hat{\\mu}_{N}]-\\mu = \\mu-\\mu = 0$. This confirms the problem's statement that the estimator can be treated as unbiased.\n\nWith zero bias, the MSE is equal to the variance of the estimator:\n$$\n\\mathbb{E}\\!\\left[(\\hat{\\mu}_{N}-\\mu)^{2}\\right] = \\operatorname{Var}(\\hat{\\mu}_{N})\n$$\nNext, we compute the variance of the estimator $\\hat{\\mu}_{N}$:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{N}) = \\operatorname{Var}\\left(\\frac{1}{N}\\sum_{i=1}^{N}\\varphi(U^{(i)})\\right)\n$$\nUsing the property $\\operatorname{Var}(aX) = a^{2}\\operatorname{Var}(X)$, we have:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{N}) = \\frac{1}{N^{2}}\\operatorname{Var}\\left(\\sum_{i=1}^{N}\\varphi(U^{(i)})\\right)\n$$\nSince the samples $U^{(i)}$ are independent, the random variables $\\varphi(U^{(i)})$ are also independent. For a sum of independent random variables, the variance of the sum is the sum of the variances:\n$$\n\\operatorname{Var}\\left(\\sum_{i=1}^{N}\\varphi(U^{(i)})\\right) = \\sum_{i=1}^{N}\\operatorname{Var}(\\varphi(U^{(i)}))\n$$\nAs the samples are also identically distributed, the variance of each term $\\varphi(U^{(i)})$ is the same and is equal to the posterior variance $\\sigma^2$:\n$$\n\\operatorname{Var}(\\varphi(U^{(i)})) = \\operatorname{Var}_{\\pi}[\\varphi(U)] = \\sigma^{2} \\quad \\text{for all } i\n$$\nTherefore, the sum of variances is:\n$$\n\\sum_{i=1}^{N}\\operatorname{Var}(\\varphi(U^{(i)})) = \\sum_{i=1}^{N}\\sigma^{2} = N\\sigma^{2}\n$$\nSubstituting this result back into the expression for $\\operatorname{Var}(\\hat{\\mu}_{N})$ gives:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{N}) = \\frac{1}{N^{2}}(N\\sigma^{2}) = \\frac{\\sigma^{2}}{N}\n$$\nThe problem requires that the MSE be no greater than $\\varepsilon^{2}$. We can now write this condition as:\n$$\n\\frac{\\sigma^{2}}{N} \\le \\varepsilon^{2}\n$$\nTo find the minimal sample size $N$, we solve this inequality for $N$:\n$$\nN \\ge \\frac{\\sigma^{2}}{\\varepsilon^{2}}\n$$\nThe problem provides the values $\\sigma^{2}=4$ and $\\varepsilon=0.01$. Substituting these values:\n$$\nN \\ge \\frac{4}{(0.01)^{2}} = \\frac{4}{(10^{-2})^{2}} = \\frac{4}{10^{-4}} = 4 \\times 10^{4} = 40000\n$$\nThe minimal integer sample size $N$ that satisfies this condition is $N=40000$.\n\nThe second part of the problem asks for the associated total computational cost. The problem states that the cost per sample is $1$, and the total cost is equal to the number of samples $N$. Therefore, the total computational cost is also $40000$.\n\nThe two required values are the minimal sample size $N$ and the total cost.\nMinimal sample size: $N = 40000$.\nTotal computational cost: Cost = $40000$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n40000  40000\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "The core idea of Multilevel Monte Carlo (MLMC) is to distribute computational effort intelligently across a hierarchy of model fidelities to minimize cost for a target error. This practice takes you to the heart of MLMC theory, where you will use constrained optimization to determine the optimal number of samples for each level based on how variance and cost scale. By solving this foundational problem , you will derive the key result that dictates the allocation of resources and underpins the remarkable efficiency of the MLMC method.",
            "id": "3405129",
            "problem": "Consider a Multilevel Monte Carlo (MLMC) estimator constructed on a geometric hierarchy with mesh sizes $h_{l}=2^{-l}$ for levels $l=0,1,\\dots,L$. Let the level differences be $\\Delta P_{l}=P_{l}-P_{l-1}$ with $P_{-1}\\equiv 0$, and assume $\\Delta P_{l}$ are independent across $l$. Suppose the variance of each level difference and the cost per sample on each level satisfy\n$$\n\\operatorname{Var}[\\Delta P_{l}]=v_{0}\\,2^{-\\beta l},\\qquad C_{l}=c_{0}\\,2^{\\gamma l},\n$$\nfor fixed positive constants $v_{0}$, $c_{0}$, and exponents $\\beta0$, $\\gamma0$, with $\\beta\\neq\\gamma$. The MLMC estimator is\n$$\n\\widehat{P}_{\\mathrm{ML}}=\\sum_{l=0}^{L}\\frac{1}{N_{l}}\\sum_{n=1}^{N_{l}}\\Delta P_{l}^{(n)},\n$$\nwhere $N_{l}\\in\\mathbb{N}$ denotes the number of samples drawn on level $l$.\n\nUnder independence and finite second moments, the variance of the estimator is\n$$\n\\operatorname{Var}[\\widehat{P}_{\\mathrm{ML}}]=\\sum_{l=0}^{L}\\frac{\\operatorname{Var}[\\Delta P_{l}]}{N_{l}},\n$$\nand the total computational work is\n$$\nW=\\sum_{l=0}^{L}N_{l}\\,C_{l}.\n$$\n\nStarting only from these definitions, choose the sample counts $\\{N_{l}\\}_{l=0}^{L}$ to minimize $W$ subject to the constraint $\\operatorname{Var}[\\widehat{P}_{\\mathrm{ML}}]=\\sigma^{2}$ for a given target variance $\\sigma^{2}0$. Derive the closed-form expression for the optimal real-valued $N_{l}$ as a function of $l$, $L$, $v_{0}$, $c_{0}$, $\\beta$, $\\gamma$, and $\\sigma^{2}$, expressing any sums in closed form. Your final answer must be a single analytic expression giving $N_{l}$ in terms of these parameters.",
            "solution": "The problem is to find the optimal allocation of samples, $\\{N_l\\}_{l=0}^{L}$, that minimizes the total computational work, $W$, for a Multilevel Monte Carlo (MLMC) estimator, subject to a fixed total variance, $\\sigma^2$. This is a classic constrained optimization problem that can be solved using the method of Lagrange multipliers.\n\nFirst, we state the objective function to be minimized and the constraint equation. The total computational work is given by:\n$$\nW(\\{N_l\\}) = \\sum_{l=0}^{L} N_l C_l = \\sum_{l=0}^{L} N_l c_0 2^{\\gamma l}\n$$\nThe constraint on the variance of the MLMC estimator is:\n$$\n\\operatorname{Var}[\\widehat{P}_{\\mathrm{ML}}] = \\sum_{l=0}^{L} \\frac{\\operatorname{Var}[\\Delta P_l]}{N_l} = \\sum_{l=0}^{L} \\frac{v_0 2^{-\\beta l}}{N_l} = \\sigma^2\n$$\nThe problem asks for optimal real-valued $N_l$, which allows the use of differential calculus. We define the Lagrangian, $\\mathcal{L}$, as the sum of the objective function and the constraint function multiplied by a Lagrange multiplier, $\\lambda$:\n$$\n\\mathcal{L}(\\{N_l\\}, \\lambda) = W + \\lambda \\left( \\sum_{l=0}^{L} \\frac{v_0 2^{-\\beta l}}{N_l} - \\sigma^2 \\right) = \\sum_{l=0}^{L} N_l c_0 2^{\\gamma l} + \\lambda \\left( \\sum_{l=0}^{L} \\frac{v_0 2^{-\\beta l}}{N_l} - \\sigma^2 \\right)\n$$\nTo find the minimum, we compute the partial derivative of $\\mathcal{L}$ with respect to each $N_l$ and set it to zero. For each $l \\in \\{0, 1, \\dots, L\\}$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial N_l} = c_0 2^{\\gamma l} - \\lambda \\frac{v_0 2^{-\\beta l}}{N_l^2} = 0\n$$\nSolving for $N_l^2$, we get:\n$$\nN_l^2 = \\lambda \\frac{v_0 2^{-\\beta l}}{c_0 2^{\\gamma l}} = \\lambda \\frac{v_0}{c_0} 2^{-(\\beta+\\gamma)l}\n$$\nFor $N_l$ to be real and positive, we must have $\\lambda  0$. Taking the square root gives the optimal $N_l$ in terms of the unknown multiplier $\\lambda$:\n$$\nN_l = \\sqrt{\\lambda} \\sqrt{\\frac{v_0}{c_0}} 2^{-(\\beta+\\gamma)l/2}\n$$\nThe next step is to determine the value of $\\sqrt{\\lambda}$ by substituting this expression for $N_l$ back into the variance constraint equation:\n$$\n\\sum_{l=0}^{L} \\frac{v_0 2^{-\\beta l}}{N_l} = \\sigma^2\n$$\n$$\n\\sum_{l=0}^{L} \\frac{v_0 2^{-\\beta l}}{\\sqrt{\\lambda} \\sqrt{\\frac{v_0}{c_0}} 2^{-(\\beta+\\gamma)l/2}} = \\sigma^2\n$$\nWe can factor out the terms that do not depend on the summation index $l$:\n$$\n\\frac{1}{\\sqrt{\\lambda}} \\sum_{l=0}^{L} \\frac{v_0}{\\sqrt{v_0/c_0}} \\frac{2^{-\\beta l}}{2^{-(\\beta+\\gamma)l/2}} = \\sigma^2\n$$\n$$\n\\frac{\\sqrt{v_0 c_0}}{\\sqrt{\\lambda}} \\sum_{l=0}^{L} 2^{-\\beta l + (\\beta+\\gamma)l/2} = \\sigma^2\n$$\n$$\n\\frac{\\sqrt{v_0 c_0}}{\\sqrt{\\lambda}} \\sum_{l=0}^{L} 2^{(\\gamma-\\beta)l/2} = \\sigma^2\n$$\nThe sum is a finite geometric series. Let $r = 2^{(\\gamma-\\beta)/2}$. The sum is $\\sum_{l=0}^{L} r^l$. Since the problem states $\\beta \\neq \\gamma$, we have $r \\neq 1$. The closed-form expression for this sum is:\n$$\nS = \\sum_{l=0}^{L} r^l = \\frac{1 - r^{L+1}}{1 - r} = \\frac{1 - (2^{(\\gamma-\\beta)/2})^{L+1}}{1 - 2^{(\\gamma-\\beta)/2}} = \\frac{1 - 2^{(\\gamma-\\beta)(L+1)/2}}{1 - 2^{(\\gamma-\\beta)/2}}\n$$\nSubstituting this back, we can solve for $\\sqrt{\\lambda}$:\n$$\n\\frac{\\sqrt{v_0 c_0}}{\\sqrt{\\lambda}} S = \\sigma^2 \\implies \\sqrt{\\lambda} = \\frac{\\sqrt{v_0 c_0}}{\\sigma^2} S\n$$\nFinally, we substitute this expression for $\\sqrt{\\lambda}$ into our equation for $N_l$:\n$$\nN_l = \\left( \\frac{\\sqrt{v_0 c_0}}{\\sigma^2} S \\right) \\left( \\sqrt{\\frac{v_0}{c_0}} 2^{-(\\beta+\\gamma)l/2} \\right)\n$$\n$$\nN_l = \\frac{S}{\\sigma^2} \\left( \\sqrt{v_0 c_0} \\sqrt{\\frac{v_0}{c_0}} \\right) 2^{-(\\beta+\\gamma)l/2}\n$$\nThe term $\\sqrt{v_0 c_0} \\sqrt{v_0/c_0} = \\sqrt{v_0^2} = v_0$. Thus, the cost constant $c_0$ cancels out, indicating the optimal sample allocation is independent of its absolute value.\n$$\nN_l = \\frac{v_0 S}{\\sigma^2} 2^{-(\\beta+\\gamma)l/2}\n$$\nSubstituting the closed-form expression for the sum $S$, we obtain the final answer for the optimal number of samples on level $l$:\n$$\nN_l = \\frac{v_0}{\\sigma^2} \\left( \\frac{1 - 2^{(\\gamma-\\beta)(L+1)/2}}{1 - 2^{(\\gamma-\\beta)/2}} \\right) 2^{-(\\beta+\\gamma)l/2}\n$$\nThis expression gives the real-valued optimal $N_l$ as a function of the specified parameters $l$, $L$, $v_0$, $\\beta$, $\\gamma$, and $\\sigma^2$. As noted, it is independent of $c_0$.",
            "answer": "$$ \\boxed{ \\frac{v_{0}}{\\sigma^{2}} \\left( \\frac{1 - 2^{\\frac{(\\gamma-\\beta)(L+1)}{2}}}{1 - 2^{\\frac{\\gamma-\\beta}{2}}} \\right) 2^{-\\frac{(\\beta+\\gamma)l}{2}} } $$"
        },
        {
            "introduction": "Multifidelity methods leverage information from cheap, low-fidelity models to accelerate the estimation of expensive, high-fidelity quantities of interest. This exercise explores a powerful multifidelity technique based on control variates, where a correlated surrogate model is used to reduce the variance of the high-fidelity estimate. Working through the derivations in this problem  will provide a concrete understanding of how to optimally combine information from different models to achieve significant variance reduction for a fixed computational budget.",
            "id": "3405112",
            "problem": "Consider an inverse problem in data assimilation where the goal is to estimate the posterior expectation of a scalar quantity of interest produced by a high-fidelity forward model. Let the high-fidelity output be the random variable $Y_h$ with mean $\\mu_h$ and variance $\\sigma_h^2$, and let a correlated, cheaper surrogate be $Y_c$ with mean $\\mu_c$, variance $\\sigma_c^2$, and correlation coefficient $\\rho$ with $Y_h$ when evaluated on the same inputs. Assume $Y_h$ and $Y_c$ are jointly second-order with finite covariance, and that the correlation coefficient $\\rho$ is known and constant over the sampling strategy. The cost per sample of $Y_h$ is $C_h$ and of $Y_c$ is $C_c$. Define the computational budget as $B$, and assume all samples are independent across distinct draws unless explicitly paired.\n\nYou will construct and analyze a multifidelity control variate estimator that uses a paired set of $N_h$ samples, for which both $Y_h$ and $Y_c$ are evaluated on the same inputs, and an additional independent cheap-only set of $N_c$ samples of $Y_c$ used to estimate the cheap-model mean. The estimator is defined as\n$$\n\\widehat{\\mu}_{\\mathrm{MF}} \\;=\\; \\overline{Y}_h \\;-\\; \\beta\\left(\\overline{Y}_c^{(h)} \\;-\\; \\overline{Y}_c\\right),\n$$\nwhere $\\overline{Y}_h$ and $\\overline{Y}_c^{(h)}$ denote sample means over the $N_h$ paired samples, and $\\overline{Y}_c$ denotes the sample mean over the $N_c$ cheap-only samples. The samples used for $\\overline{Y}_c^{(h)}$ and $\\overline{Y}_c$ are independent. The total cost accounting for all evaluations is $B = N_h C_h + (N_h + N_c) C_c$. The baseline single-level Monte Carlo estimator using only the high-fidelity model at the same budget $B$ is $\\widehat{\\mu}_{\\mathrm{SL}} = \\overline{Y}_h$ with $B = N_h^{\\mathrm{SL}} C_h$, and variance $\\sigma_h^2/N_h^{\\mathrm{SL}}$.\n\nStarting only from the definitions of variance, covariance, and the cost budget constraint, and without invoking pre-derived optimal formulas, perform the following:\n1. Derive the variance of $\\widehat{\\mu}_{\\mathrm{MF}}$ as a function of $N_h$, $N_c$, $\\beta$, $\\sigma_h^2$, $\\sigma_c^2$, and $\\rho$.\n2. Determine the choice of $\\beta$ that minimizes this variance for fixed $N_h$ and $N_c$, and obtain the minimized variance in terms of $N_h$, $N_c$, $\\sigma_h^2$, and $\\rho$.\n3. Under the fixed total budget $B$, express the minimized variance solely in terms of the dimensionless ratio $x = N_c/N_h$, the correlation coefficient $\\rho$, and the cost ratio $k = C_h/C_c$.\n4. Define the variance reduction factor $R$ to be the ratio of the baseline single-level high-fidelity variance at cost $B$ to the minimized variance of $\\widehat{\\mu}_{\\mathrm{MF}}$ at the same cost $B$. Obtain $R$ as a function of $x$, $\\rho$, and $k$, and then find the optimal $x$ that maximizes $R$ (equivalently minimizes the multifidelity variance).\n5. With $C_h/C_c = 100$ and $\\rho = 0.9$, compute the variance reduction factor $R$ and the optimal ratio $N_c/N_h$.\n\nRound your two numerical answers to four significant figures. Express your final answers as two dimensionless numbers in the order $R$ and $N_c/N_h$.",
            "solution": "The problem is subjected to validation before proceeding to a solution.\n\n### Step 1: Extract Givens\n- High-fidelity model output: random variable $Y_h$, mean $\\mu_h$, variance $\\sigma_h^2$.\n- Cheap surrogate model output: random variable $Y_c$, mean $\\mu_c$, variance $\\sigma_c^2$.\n- Correlation coefficient between $Y_h$ and $Y_c$: $\\rho$.\n- Cost per sample of $Y_h$: $C_h$.\n- Cost per sample of $Y_c$: $C_c$.\n- Total computational budget: $B$.\n- Number of paired samples $(Y_h, Y_c)$: $N_h$.\n- Number of independent cheap-only samples: $N_c$.\n- Multifidelity estimator: $\\widehat{\\mu}_{\\mathrm{MF}} = \\overline{Y}_h - \\beta(\\overline{Y}_c^{(h)} - \\overline{Y}_c)$.\n- Sample mean over $N_h$ high-fidelity samples: $\\overline{Y}_h$.\n- Sample mean over $N_h$ paired cheap samples: $\\overline{Y}_c^{(h)}$.\n- Sample mean over $N_c$ independent cheap samples: $\\overline{Y}_c$.\n- Total cost constraint: $B = N_h C_h + (N_h + N_c) C_c$.\n- Single-level Monte Carlo estimator: $\\widehat{\\mu}_{\\mathrm{SL}} = \\overline{Y}_h$ with cost $B = N_h^{\\mathrm{SL}} C_h$.\n- Variance of single-level estimator: $\\sigma_h^2/N_h^{\\mathrm{SL}}$.\n- Requirement to round final numerical answers to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is a standard exercise in the analysis of multifidelity Monte Carlo estimators, specifically a variant of the control variate method.\n- **Scientifically Grounded and Objective (Critical)**: The problem is based on established statistical principles of variance and covariance, and the framework of Monte Carlo methods. The concepts are standard in computational science and engineering. The language is precise and objective.\n- **Well-Posed**: The problem is well-defined, with all necessary variables and relationships provided. It asks for a sequence of derivations and optimizations that lead to a unique, meaningful solution.\n- **Completeness and Consistency**: The problem is self-contained. The definitions of the estimators and the cost model are explicit and consistent. The independence of the sample sets is clearly stated.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. A full, reasoned solution is provided below.\n\n### Part 1: Variance of the Multifidelity Estimator\nThe multifidelity estimator is given by $\\widehat{\\mu}_{\\mathrm{MF}} = \\overline{Y}_h - \\beta(\\overline{Y}_c^{(h)} - \\overline{Y}_c)$. To find its variance, we use the properties of variance. The term $\\overline{Y}_c$ is constructed from a set of $N_c$ samples that are independent of the $N_h$ paired samples used for $\\overline{Y}_h$ and $\\overline{Y}_c^{(h)}$. Therefore, the term $(\\overline{Y}_h - \\beta\\overline{Y}_c^{(h)})$ is independent of $\\beta\\overline{Y}_c$.\nThe variance of a sum of independent random variables is the sum of their variances:\n$$\n\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{MF}}) = \\mathrm{Var}(\\overline{Y}_h - \\beta\\overline{Y}_c^{(h)}) + \\mathrm{Var}(\\beta\\overline{Y}_c)\n$$\nThe first term, $\\mathrm{Var}(\\overline{Y}_h - \\beta\\overline{Y}_c^{(h)})$, involves the sample means from $N_h$ independent, identically distributed (i.i.d.) pairs of samples $(Y_{h,i}, Y_{c,i})$.\n$$\n\\overline{Y}_h - \\beta\\overline{Y}_c^{(h)} = \\frac{1}{N_h}\\sum_{i=1}^{N_h} Y_{h,i} - \\beta \\frac{1}{N_h}\\sum_{i=1}^{N_h} Y_{c,i} = \\frac{1}{N_h}\\sum_{i=1}^{N_h} (Y_{h,i} - \\beta Y_{c,i})\n$$\nThe variance is thus:\n$$\n\\mathrm{Var}\\left(\\frac{1}{N_h}\\sum_{i=1}^{N_h} (Y_{h,i} - \\beta Y_{c,i})\\right) = \\frac{1}{N_h^2} \\sum_{i=1}^{N_h} \\mathrm{Var}(Y_{h,i} - \\beta Y_{c,i}) = \\frac{1}{N_h}\\mathrm{Var}(Y_h - \\beta Y_c)\n$$\nWe expand $\\mathrm{Var}(Y_h - \\beta Y_c)$ using the definition of variance and covariance:\n$$\n\\mathrm{Var}(Y_h - \\beta Y_c) = \\mathrm{Var}(Y_h) + \\beta^2\\mathrm{Var}(Y_c) - 2\\beta\\mathrm{Cov}(Y_h, Y_c) = \\sigma_h^2 + \\beta^2\\sigma_c^2 - 2\\beta\\rho\\sigma_h\\sigma_c\n$$\nSo, the first term is $\\frac{1}{N_h}(\\sigma_h^2 - 2\\beta\\rho\\sigma_h\\sigma_c + \\beta^2\\sigma_c^2)$.\n\nThe second term, $\\mathrm{Var}(\\beta\\overline{Y}_c)$, involves the sample mean from $N_c$ i.i.d. samples $Y_{c,j}'$:\n$$\n\\mathrm{Var}(\\beta\\overline{Y}_c) = \\beta^2\\mathrm{Var}(\\overline{Y}_c) = \\beta^2\\mathrm{Var}\\left(\\frac{1}{N_c}\\sum_{j=1}^{N_c} Y_{c,j}'\\right) = \\beta^2 \\frac{1}{N_c}\\mathrm{Var}(Y_c) = \\frac{\\beta^2\\sigma_c^2}{N_c}\n$$\nCombining both terms, the total variance is:\n$$\n\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{MF}}) = \\frac{1}{N_h}(\\sigma_h^2 - 2\\beta\\rho\\sigma_h\\sigma_c + \\beta^2\\sigma_c^2) + \\frac{\\beta^2\\sigma_c^2}{N_c}\n$$\n$$\n\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{MF}}) = \\frac{\\sigma_h^2}{N_h} - \\frac{2\\beta\\rho\\sigma_h\\sigma_c}{N_h} + \\beta^2\\sigma_c^2\\left(\\frac{1}{N_h} + \\frac{1}{N_c}\\right)\n$$\n\n### Part 2: Optimal $\\beta$ and Minimized Variance\nTo find the optimal value of $\\beta$ that minimizes the variance, we differentiate $\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{MF}})$ with respect to $\\beta$ and set the result to zero:\n$$\n\\frac{\\partial}{\\partial\\beta}\\mathrm{Var}(\\widehat{\\mu}_{\\mathrm{MF}}) = -\\frac{2\\rho\\sigma_h\\sigma_c}{N_h} + 2\\beta\\sigma_c^2\\left(\\frac{1}{N_h} + \\frac{1}{N_c}\\right) = 0\n$$\n$$\n\\beta\\sigma_c^2\\left(\\frac{N_c + N_h}{N_h N_c}\\right) = \\frac{\\rho\\sigma_h\\sigma_c}{N_h}\n$$\nSolving for $\\beta$, we find the optimal value $\\beta_{opt}$:\n$$\n\\beta_{opt} = \\frac{\\rho\\sigma_h\\sigma_c}{N_h} \\frac{N_h N_c}{\\sigma_c^2(N_h+N_c)} = \\frac{\\rho\\sigma_h}{\\sigma_c}\\frac{N_c}{N_h+N_c}\n$$\nThe second derivative, $2\\sigma_c^2(1/N_h + 1/N_c)$, is positive, confirming this is a minimum.\nNow, we substitute $\\beta_{opt}$ back into the variance expression. The variance is a quadratic in $\\beta$ of the form $A - 2B\\beta + C\\beta^2$, which has a minimum value of $A - B^2/C$ at $\\beta = B/C$.\nHere, $A = \\frac{\\sigma_h^2}{N_h}$, $B = \\frac{\\rho\\sigma_h\\sigma_c}{N_h}$, and $C = \\sigma_c^2(\\frac{1}{N_h} + \\frac{1}{N_c})$.\nThe minimized variance is:\n$$\n\\mathrm{Var}_{\\mathrm{min}} = \\frac{\\sigma_h^2}{N_h} - \\frac{\\left(\\frac{\\rho\\sigma_h\\sigma_c}{N_h}\\right)^2}{\\sigma_c^2\\left(\\frac{1}{N_h} + \\frac{1}{N_c}\\right)} = \\frac{\\sigma_h^2}{N_h} - \\frac{\\frac{\\rho^2\\sigma_h^2\\sigma_c^2}{N_h^2}}{\\sigma_c^2\\frac{N_h+N_c}{N_h N_c}}\n$$\n$$\n\\mathrm{Var}_{\\mathrm{min}} = \\frac{\\sigma_h^2}{N_h} - \\frac{\\rho^2\\sigma_h^2}{N_h^2} \\frac{N_h N_c}{N_h+N_c} = \\frac{\\sigma_h^2}{N_h} - \\frac{\\rho^2\\sigma_h^2 N_c}{N_h(N_h+N_c)}\n$$\nThis expression can be simplified by factoring out $\\sigma_h^2/N_h$:\n$$\n\\mathrm{Var}_{\\mathrm{min}} = \\frac{\\sigma_h^2}{N_h}\\left(1 - \\frac{\\rho^2 N_c}{N_h+N_c}\\right) = \\frac{\\sigma_h^2}{N_h}\\left(\\frac{N_h + N_c - \\rho^2 N_c}{N_h+N_c}\\right) = \\frac{\\sigma_h^2}{N_h}\\frac{N_h + (1-\\rho^2)N_c}{N_h + N_c}\n$$\n\n### Part 3: Minimized Variance in Dimensionless Ratios\nThe cost budget is $B = N_h C_h + (N_h + N_c) C_c$. We are given $x = N_c/N_h$ and $k = C_h/C_c$.\nWe can express $N_h$ in terms of the total budget $B$.\n$$\nB = N_h C_h + N_h C_c + N_c C_c = N_h (C_h + C_c) + N_c C_c\n$$\nSubstituting $N_c = x N_h$ and $C_h = k C_c$:\n$$\nB = N_h (k C_c + C_c) + x N_h C_c = N_h C_c (k + 1 + x)\n$$\nSolving for $N_h$:\n$$\nN_h = \\frac{B}{C_c(k+1+x)}\n$$\nNow, substitute this into the minimized variance formula. The ratio term becomes $\\frac{N_h + (1-\\rho^2)x N_h}{N_h + x N_h} = \\frac{1+(1-\\rho^2)x}{1+x}$.\n$$\n\\mathrm{Var}_{\\mathrm{min}} = \\frac{\\sigma_h^2}{N_h} \\frac{1+(1-\\rho^2)x}{1+x} = \\sigma_h^2 \\frac{C_c(k+1+x)}{B} \\frac{1+(1-\\rho^2)x}{1+x}\n$$\n$$\n\\mathrm{Var}_{\\mathrm{min}} = \\frac{\\sigma_h^2 C_c}{B} \\frac{(k+1+x)(1+(1-\\rho^2)x)}{1+x}\n$$\n\n### Part 4: Variance Reduction Factor and Optimal Sample Ratio\nThe baseline single-level Monte Carlo variance is $\\mathrm{Var}_{\\mathrm{SL}} = \\sigma_h^2/N_h^{\\mathrm{SL}}$. The budget is $B = N_h^{\\mathrm{SL}} C_h$, so $N_h^{\\mathrm{SL}} = B/C_h$.\n$$\n\\mathrm{Var}_{\\mathrm{SL}} = \\sigma_h^2 \\frac{C_h}{B}\n$$\nThe variance reduction factor $R$ is the ratio $\\mathrm{Var}_{\\mathrm{SL}}/\\mathrm{Var}_{\\mathrm{min}}$:\n$$\nR = \\frac{\\sigma_h^2 C_h/B}{\\frac{\\sigma_h^2 C_c}{B} \\frac{(k+1+x)(1+(1-\\rho^2)x)}{1+x}} = \\frac{C_h/C_c}{\\frac{(k+1+x)(1+(1-\\rho^2)x)}{1+x}}\n$$\n$$\nR(x) = \\frac{k(1+x)}{(k+1+x)(1+(1-\\rho^2)x)}\n$$\nTo maximize $R(x)$, we can minimize its reciprocal, $g(x) = 1/R(x) = \\frac{(k+1+x)(1+(1-\\rho^2)x)}{k(1+x)}$. We differentiate $g(x)$ with respect to $x$ and set to $0$. Let's drop the constant $k$ and minimize $g_0(x) = \\frac{(k+1+x)(1+(1-\\rho^2)x)}{1+x}$.\n$$\ng_0(x) = \\frac{(1-\\rho^2)x^2 + (k+2-(k+1)\\rho^2)x + k+1}{1+x}\n$$\n$$\n\\frac{dg_0}{dx} = \\frac{(1+x)[2(1-\\rho^2)x+k+2-(k+1)\\rho^2] - [(1-\\rho^2)x^2 + (k+2-(k+1)\\rho^2)x + k+1]}{(1+x)^2} = 0\n$$\nThe numerator simplifies to:\n$$\n(1-\\rho^2)x^2 + 2(1-\\rho^2)x + 1-(k+1)\\rho^2 = 0\n$$\nSolving this quadratic equation for $x$:\n$$\nx = \\frac{-2(1-\\rho^2) \\pm \\sqrt{4(1-\\rho^2)^2 - 4(1-\\rho^2)(1-(k+1)\\rho^2)}}{2(1-\\rho^2)}\n$$\n$$\nx = -1 \\pm \\sqrt{1 - \\frac{1-(k+1)\\rho^2}{1-\\rho^2}} = -1 \\pm \\sqrt{\\frac{1-\\rho^2 - 1 + (k+1)\\rho^2}{1-\\rho^2}}\n$$\n$$\nx = -1 \\pm \\sqrt{\\frac{k\\rho^2}{1-\\rho^2}} = -1 \\pm \\frac{\\rho\\sqrt{k}}{\\sqrt{1-\\rho^2}}\n$$\nSince $x=N_c/N_h$ must be non-negative, we take the positive root:\n$$\nx_{opt} = -1 + \\frac{\\rho\\sqrt{k}}{\\sqrt{1-\\rho^2}}\n$$\n\n### Part 5: Numerical Computation\nGiven $k = C_h/C_c = 100$ and $\\rho = 0.9$.\nFirst, we compute the optimal ratio $x_{opt} = N_c/N_h$:\n$$\nx_{opt} = -1 + \\frac{0.9\\sqrt{100}}{\\sqrt{1-0.9^2}} = -1 + \\frac{9}{\\sqrt{1-0.81}} = -1 + \\frac{9}{\\sqrt{0.19}}\n$$\n$$\nx_{opt} \\approx -1 + \\frac{9}{0.43588989} \\approx -1 + 20.647416 \\approx 19.647416\n$$\nRounding to four significant figures, $x_{opt} \\approx 19.65$.\n\nNext, we compute the maximum variance reduction factor $R_{max}$. We can substitute $x_{opt}$ into the expression for $R(x)$.\nPlugging in the numerical values:\n$$\nR(x_{opt}) = \\frac{100(1+19.647416)}{(100+1+19.647416)(1+(1-0.81)(19.647416))}\n$$\n$$\nR(x_{opt}) = \\frac{100(20.647416)}{(120.647416)(1+0.19 \\times 19.647416)}\n$$\n$$\nR(x_{opt}) = \\frac{2064.7416}{(120.647416)(1+3.732909)} = \\frac{2064.7416}{(120.647416)(4.732909)}\n$$\n$$\nR_{max} \\approx \\frac{2064.7416}{570.920} \\approx 3.61587\n$$\nRounding to four significant figures, $R \\approx 3.616$.\n\nThe two requested numerical values are the variance reduction factor $R$ and the optimal ratio $N_c/N_h$.\n$R \\approx 3.616$\n$N_c/N_h \\approx 19.65$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n3.616  19.65\n\\end{pmatrix}\n}\n$$"
        }
    ]
}