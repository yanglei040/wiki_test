{
    "hands_on_practices": [
        {
            "introduction": "The choice of hyperpriors is a critical step in designing a hierarchical Bayesian model. While improper, scale-invariant priors are often used with the intention of being \"non-informative,\" they can introduce unexpected and profound consequences. This first exercise guides you through an analytical derivation to uncover a fundamental scaling ambiguity that arises from this choice, revealing a non-identifiability issue that has significant implications for Empirical Bayes procedures . Understanding this potential pitfall is essential for building robust and interpretable hierarchical models.",
            "id": "3388773",
            "problem": "Consider a linear Gaussian inverse problem in which the observation vector $y \\in \\mathbb{R}^{m}$ is modeled as $y \\mid x, \\sigma^{2} \\sim \\mathcal{N}(A x, \\sigma^{2} I_{m})$, where $A \\in \\mathbb{R}^{m \\times n}$ is known and $I_{m}$ is the $m \\times m$ identity matrix. The unknown state $x \\in \\mathbb{R}^{n}$ is endowed with a Gaussian prior $x \\mid \\tau \\sim \\mathcal{N}(0, \\tau^{-1} L^{-1})$, where $L \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite. The hyperparameters $(\\sigma^{2}, \\tau)$ are given improper scale-invariant hyperpriors $p(\\sigma^{2}) \\propto (\\sigma^{2})^{-1}$ and $p(\\tau) \\propto \\tau^{-1}$ on $(0, \\infty)$. Let $B := A L^{-1} A^{\\top} \\in \\mathbb{R}^{m \\times m}$, which is symmetric positive semidefinite.\n\nStarting from the definitions of the multivariate Gaussian density and marginal likelihood, and assuming that all integrals are well-defined (for example, by imagining Jeffreys-type hyperpriors truncated on $(\\varepsilon, M)$ with fixed, scale-neutral bounds so that neither $c$ nor $y$ affects the truncation), carry out the following steps:\n\n1. Integrate out $x$ to express the marginal likelihood as a two-dimensional integral over $(\\sigma^{2}, \\tau)$:\n   $$\n   p(y \\mid A, L) \\propto \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\left|\\sigma^{2} I_{m} + \\tau^{-1} B \\right|^{-\\frac{1}{2}} \\exp\\!\\left( -\\frac{1}{2} y^{\\top} \\left(\\sigma^{2} I_{m} + \\tau^{-1} B \\right)^{-1} y \\right) \\frac{d\\sigma^{2}}{\\sigma^{2}} \\frac{d\\tau}{\\tau},\n   $$\n   identifying clearly the dependence on $y$ and the hyperparameters.\n2. For any fixed $c > 0$, analyze the scaled data $c y$ and perform a change of variables in $(\\sigma^{2}, \\tau)$ that makes the quadratic form in the exponential independent of $c$, and determine how the Jacobian, the determinant factor, and the hyperpriors transform. Derive the scaling law\n   $$\n   p(c y \\mid A, L) \\;=\\; c^{-p} \\, p(y \\mid A, L),\n   $$\n   valid up to a factor that does not depend on $y$ nor on $c$.\n3. State the minimal conditions under which the above scaling derivation is valid, emphasizing the role of the homogeneity of $\\sigma^{2} I_{m} + \\tau^{-1} B$ in $(\\sigma^{2}, \\tau^{-1})$ and the scale-invariance of the hyperpriors. Discuss the implication of this scaling law for Empirical Bayes (EB) procedures, namely that an overall scale in the hierarchical model cannot be identified from the data when using such improper hyperpriors because the marginal likelihood is homogeneous in $y$.\n   \nReport, as your final answer, the exponent $p$ in terms of $m$ only. No numerical evaluation is required. If you provide a numerical value, it will be marked incorrect. The final answer must be given as a single analytical expression with no units.",
            "solution": "The user has provided a valid problem. The problem is scientifically grounded in the field of Bayesian inverse problems, is well-posed, and uses objective, formal language. I will proceed with the solution.\n\nThe problem asks for a three-part analysis of a hierarchical Bayesian model for a linear Gaussian inverse problem.\n\n### Part 1: Verification of the Marginal Likelihood Expression\n\nFirst, we derive the expression for the marginal likelihood $p(y \\mid A, L)$. The hierarchical model is defined by:\n1.  Likelihood: $y \\mid x, \\sigma^{2} \\sim \\mathcal{N}(A x, \\sigma^{2} I_{m})$\n2.  Prior: $x \\mid \\tau \\sim \\mathcal{N}(0, \\tau^{-1} L^{-1})$\n3.  Hyperpriors: $p(\\sigma^{2}) \\propto (\\sigma^{2})^{-1}$ and $p(\\tau) \\propto \\tau^{-1}$\n\nTo find the marginal likelihood of the data $y$ given the model structure ($A, L$), we must integrate out the latent variable $x$ and the hyperparameters $(\\sigma^2, \\tau)$. We first integrate over $x$ to find the distribution of $y$ conditional on the hyperparameters, $p(y \\mid \\sigma^2, \\tau)$.\n\nThe model can be seen as a linear transformation of a Gaussian random variable $x$ with additive Gaussian noise. The distribution of $y$ is therefore also Gaussian. The mean is:\n$$ \\mathbb{E}[y \\mid \\sigma^2, \\tau] = \\mathbb{E}[Ax + \\epsilon \\mid \\sigma^2, \\tau] = A\\mathbb{E}[x \\mid \\tau] + \\mathbb{E}[\\epsilon \\mid \\sigma^2] = A(0) + 0 = 0 $$\nwhere $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ represents the noise. The covariance is:\n$$ \\text{Cov}(y \\mid \\sigma^2, \\tau) = \\text{Cov}(Ax + \\epsilon) = \\text{Cov}(Ax) + \\text{Cov}(\\epsilon) = A \\text{Cov}(x \\mid \\tau) A^{\\top} + \\text{Cov}(\\epsilon \\mid \\sigma^2) $$\nSubstituting the given covariances $\\text{Cov}(x \\mid \\tau) = \\tau^{-1} L^{-1}$ and $\\text{Cov}(\\epsilon \\mid \\sigma^2) = \\sigma^2 I_m$:\n$$ \\text{Cov}(y \\mid \\sigma^2, \\tau) = A(\\tau^{-1} L^{-1})A^{\\top} + \\sigma^2 I_m = \\tau^{-1} (A L^{-1} A^{\\top}) + \\sigma^2 I_m $$\nUsing the definition $B := A L^{-1} A^{\\top}$, the covariance becomes $\\Sigma_y = \\sigma^2 I_m + \\tau^{-1} B$.\nThus, the distribution of $y$ conditional on the hyperparameters is:\n$$ y \\mid \\sigma^2, \\tau \\sim \\mathcal{N}(0, \\sigma^2 I_m + \\tau^{-1} B) $$\nThe probability density function is:\n$$ p(y \\mid \\sigma^2, \\tau) = (2\\pi)^{-m/2} |\\sigma^2 I_m + \\tau^{-1} B|^{-1/2} \\exp\\left(-\\frac{1}{2} y^\\top (\\sigma^2 I_m + \\tau^{-1} B)^{-1} y\\right) $$\nTo obtain the full marginal likelihood $p(y \\mid A, L)$, we integrate over the hyperpriors for $\\sigma^2$ and $\\tau$:\n$$ p(y \\mid A, L) = \\int_0^\\infty \\int_0^\\infty p(y \\mid \\sigma^2, \\tau) p(\\sigma^2) p(\\tau) \\, d\\sigma^2 d\\tau $$\nSubstituting the densities and dropping the constant factor $(2\\pi)^{-m/2}$ into the proportionality:\n$$ p(y \\mid A, L) \\propto \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\left|\\sigma^{2} I_{m} + \\tau^{-1} B \\right|^{-\\frac{1}{2}} \\exp\\!\\left( -\\frac{1}{2} y^{\\top} \\left(\\sigma^{2} I_{m} + \\tau^{-1} B \\right)^{-1} y \\right) \\frac{d\\sigma^{2}}{\\sigma^{2}} \\frac{d\\tau}{\\tau} $$\nThis matches the expression provided in the problem statement.\n\n### Part 2: Derivation of the Scaling Law\n\nWe now analyze how the marginal likelihood transforms when the data vector $y$ is scaled by a factor $c > 0$. The marginal likelihood for $cy$ is:\n$$ p(c y \\mid A, L) \\propto \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\left|\\sigma^{2} I_{m} + \\tau^{-1} B \\right|^{-\\frac{1}{2}} \\exp\\!\\left( -\\frac{1}{2} (cy)^{\\top} (\\sigma^{2} I_{m} + \\tau^{-1} B )^{-1} (cy) \\right) \\frac{d\\sigma^{2}}{\\sigma^{2}} \\frac{d\\tau}{\\tau} $$\nThe quadratic form in the exponent becomes:\n$$ -\\frac{1}{2} c^2 y^{\\top} (\\sigma^{2} I_{m} + \\tau^{-1} B )^{-1} y $$\nWe perform a change of variables in the integration to absorb the $c^2$ factor. Let the new variables be $(\\sigma'^2, \\tau')$. We seek a transformation such that the argument of the exponential becomes independent of $c$. This requires:\n$$ c^2 (\\sigma^{2} I_{m} + \\tau^{-1} B )^{-1} = (\\sigma'^{2} I_{m} + \\tau'^{-1} B )^{-1} $$\nTaking the matrix inverse of both sides gives:\n$$ c^{-2} (\\sigma^{2} I_{m} + \\tau^{-1} B) = \\sigma'^{2} I_{m} + \\tau'^{-1} B $$\nBy matching the coefficients of the linearly independent matrices $I_m$ and $B$, we define the change of variables:\n$$ \\sigma'^{2} = c^{-2} \\sigma^2 \\quad \\implies \\quad \\sigma^2 = c^2 \\sigma'^2 $$\n$$ \\tau'^{-1} = c^{-2} \\tau^{-1} \\quad \\implies \\quad \\tau^{-1} = c^2 \\tau'^{-1} \\quad \\implies \\quad \\tau = c^{-2} \\tau' $$\nThe integration limits for $(\\sigma'^2, \\tau')$ remain $(0, \\infty)$. We now transform each part of the integrand:\n\n1.  **Exponential Term**: By construction, the exponential term becomes:\n    $$ \\exp\\!\\left( -\\frac{1}{2} y^{\\top} (\\sigma'^{2} I_{m} + \\tau'^{-1} B )^{-1} y \\right) $$\n\n2.  **Determinant Factor**: We use the property $\\det(kM) = k^d \\det(M)$ for a $d \\times d$ matrix $M$. Here, the matrix is $m \\times m$.\n    $$ \\left|\\sigma^{2} I_{m} + \\tau^{-1} B \\right|^{-\\frac{1}{2}} = \\left|c^2 \\sigma'^{2} I_{m} + c^2 \\tau'^{-1} B \\right|^{-\\frac{1}{2}} = \\left|c^2 (\\sigma'^{2} I_{m} + \\tau'^{-1} B) \\right|^{-\\frac{1}{2}} $$\n    $$ = \\left( (c^2)^m \\left|\\sigma'^{2} I_{m} + \\tau'^{-1} B\\right| \\right)^{-\\frac{1}{2}} = (c^{2m})^{-\\frac{1}{2}} \\left|\\sigma'^{2} I_{m} + \\tau'^{-1} B\\right|^{-\\frac{1}{2}} = c^{-m} \\left|\\sigma'^{2} I_{m} + \\tau'^{-1} B\\right|^{-\\frac{1}{2}} $$\n\n3.  **Measure**: The measure is given by the product of the hyperpriors and the differentials, $\\frac{d\\sigma^2}{\\sigma^2} \\frac{d\\tau}{\\tau}$. From the change of variables:\n    $$ \\sigma^2 = c^2 \\sigma'^2 \\implies d\\sigma^2 = c^2 d\\sigma'^2 \\implies \\frac{d\\sigma^2}{\\sigma^2} = \\frac{c^2 d\\sigma'^2}{c^2 \\sigma'^2} = \\frac{d\\sigma'^2}{\\sigma'^2} $$\n    $$ \\tau = c^{-2} \\tau' \\implies d\\tau = c^{-2} d\\tau' \\implies \\frac{d\\tau}{\\tau} = \\frac{c^{-2} d\\tau'}{c^{-2} \\tau'} = \\frac{d\\tau'}{\\tau'} $$\n    The measure is invariant under this transformation: $\\frac{d\\sigma^{2}}{\\sigma^{2}} \\frac{d\\tau}{\\tau} = \\frac{d\\sigma'^{2}}{\\sigma'^{2}} \\frac{d\\tau'}{\\tau'}$.\n\nSubstituting these transformed parts back into the integral for $p(cy \\mid A, L)$:\n$$ p(cy \\mid A, L) \\propto \\int_{0}^{\\infty} \\int_{0}^{\\infty} c^{-m} \\left|\\sigma'^{2} I_{m} + \\tau'^{-1} B\\right|^{-\\frac{1}{2}} \\exp\\!\\left( -\\frac{1}{2} y^{\\top} (\\sigma'^{2} I_{m} + \\tau'^{-1} B )^{-1} y \\right) \\frac{d\\sigma'^{2}}{\\sigma'^{2}} \\frac{d\\tau'}{\\tau'} $$\nThe factor $c^{-m}$ is constant with respect to the integration variables and can be pulled out:\n$$ p(cy \\mid A, L) \\propto c^{-m} \\int_{0}^{\\infty} \\int_{0}^{\\infty} \\left|\\sigma'^{2} I_{m} + \\tau'^{-1} B\\right|^{-\\frac{1}{2}} \\exp\\!\\left( -\\frac{1}{2} y^{\\top} (\\sigma'^{2} I_{m} + \\tau'^{-1} B )^{-1} y \\right) \\frac{d\\sigma'^{2}}{\\sigma'^{2}} \\frac{d\\tau'}{\\tau'} $$\nThe remaining integral is identical in form to the integral for $p(y \\mid A, L)$, with $(\\sigma'^2, \\tau')$ being dummy variables of integration. Therefore, the integral is proportional to $p(y \\mid A, L)$. This establishes the scaling law:\n$$ p(c y \\mid A, L) \\;=\\; c^{-m} \\, p(y \\mid A, L) $$\nComparing this to the form $p(c y \\mid A, L) = c^{-p} \\, p(y \\mid A, L)$, we identify the exponent as $p=m$.\n\n### Part 3: Conditions and Implications\n\n**Minimal Conditions for Validity:**\nThe derivation of the scaling law relies on two crucial properties of the model setup:\n1.  **Homogeneity of the Data Covariance:** The marginal covariance of the data, $\\Sigma_y(\\sigma^2, \\tau^{-1}) = \\sigma^2 I_m + \\tau^{-1} B$, must be a homogeneous function of the hyperparameters being scaled. In this case, it is homogeneous of degree $1$ in the variables $(\\sigma^2, \\tau^{-1})$. This property allows the scaling factor $c$ in the data to be mapped onto a scaling factor for the hyperparameters, $c^{-2}$, which can then be factored out of the covariance matrix.\n2.  **Scale-Invariance of the Hyperpriors:** The integration measure over the hyperparameter space, here $p(\\sigma^2)p(\\tau)d\\sigma^2 d\\tau \\propto \\frac{d\\sigma^2}{\\sigma^2} \\frac{d\\tau}{\\tau}$, must be invariant under the specific scaling transformation induced by scaling the data. As shown in Part 2, the transformation $(\\sigma^2, \\tau) \\to (c^2 \\sigma'^2, c^{-2} \\tau')$ leaves the measure $\\frac{d\\sigma^2}{\\sigma^2}\\frac{d\\tau}{\\tau}$ unchanged. This is a general property of the Jeffreys prior for scale parameters.\n\n**Implication for Empirical Bayes (EB):**\nEmpirical Bayes procedures often involve estimating hyperparameters by maximizing the marginal likelihood $p(y \\mid \\theta)$ with respect to $\\theta = (\\sigma^2, \\tau)$. The derived scaling law, $p(cy \\mid A, L) = c^{-m} p(y \\mid A, L)$, is a symptom of a fundamental non-identifiability issue when using this model structure with improper scale-invariant priors.\n\nThe issue is that there is an inherent ambiguity between the scale of the data $y$ and the overall scale of the model's variance parameters $(\\sigma^2, \\tau)$. The model defined by `{data $y$, parameters $(\\sigma^2, \\tau)$}` is observationally indistinguishable from a model with `{data $cy$, parameters $(c^2 \\sigma^2, c^{-2} \\tau)$}`.\nThis can be demonstrated by examining the posterior distribution of the hyperparameters, $p(\\sigma^2, \\tau \\mid y) \\propto p(y \\mid \\sigma^2, \\tau) p(\\sigma^2) p(\\tau)$. A formal analysis (as performed in the scratchpad) shows that the posterior distribution for hyperparameters given data $cy$, when expressed in terms of rescaled variables $\\tilde{\\sigma}^2 = c^{-2}\\sigma^2$ and $\\tilde{\\tau}=c^2 \\tau$, has the same functional form as the posterior for $(\\tilde{\\sigma}^2, \\tilde{\\tau})$ given data $y$.\n$$ p(\\sigma^2 = c^2 \\tilde{\\sigma}^2, \\tau=c^{-2}\\tilde{\\tau} \\mid y_{new}=cy) \\; d(c^2\\tilde{\\sigma}^2)d(c^{-2}\\tilde{\\tau}) = p(\\tilde{\\sigma}^2, \\tilde{\\tau} \\mid y) \\; d\\tilde{\\sigma}^2 d\\tilde{\\tau} $$\nThis implies that if $(\\hat{\\sigma}^2, \\hat{\\tau})$ is the maximum a posteriori (or maximum likelihood) estimate for data $y$, then $(c^2\\hat{\\sigma}^2, c^{-2}\\hat{\\tau})$ will be the corresponding estimate for data $cy$.\n\nConsequently, the estimated scale of the hyperparameters is entirely determined by the arbitrary scale (or units) of the measurement vector $y$. The data itself provides no information to fix an absolute scale for $\\sigma^2$ and $\\tau$. This is what is meant by \"an overall scale in the hierarchical model cannot be identified\". The estimation of the *ratio* of noise variance to prior variance (e.g., $\\sigma^2 \\tau$) might be well-determined, but their absolute individual values are not identifiable from the data alone without introducing further information, for example through proper, informative hyperpriors that break the scaling symmetry.\n\nThe final answer required is the exponent $p$.\nBased on the derivation in Part 2, $p=m$.",
            "answer": "$$\\boxed{m}$$"
        },
        {
            "introduction": "Moving from theoretical considerations to practical applications, we now explore the primary motivation for using hierarchical models: their ability to adapt to structure in the data. This practice focuses on the important application of image deblurring, where the strength of regularization must balance noise suppression with feature preservation. You will implement a hierarchical Bayesian model for Total Variation (TV) regularization, where the regularization weight is learned from the data, and discover how this enables the reconstruction to automatically adapt to different image types, from sharp edges to smooth regions .",
            "id": "3388760",
            "problem": "Consider a two-dimensional linear inverse problem with periodic boundary conditions. Let the unknown image be denoted by $x \\in \\mathbb{R}^{N_x \\times N_y}$, the observed data by $y \\in \\mathbb{R}^{N_x \\times N_y}$, and the forward operator by $A: \\mathbb{R}^{N_x \\times N_y} \\rightarrow \\mathbb{R}^{N_x \\times N_y}$ defined as periodic convolution with a known blur kernel. Assume additive independent and identically distributed Gaussian noise with zero mean and variance $\\sigma^2$, so the likelihood is $p(y \\mid x) \\propto \\exp\\left(-\\tfrac{1}{2 \\sigma^2} \\lVert A x - y \\rVert_2^2 \\right)$.\n\nAdopt an anisotropic total variation prior on $x$ expressed through the discrete forward differences with periodic wrap:\n- For each pixel $(i,j)$, define forward differences $D_x x[i,j] = x[i, (j+1) \\bmod N_y] - x[i,j]$ and $D_y x[i,j] = x[(i+1) \\bmod N_x, j] - x[i,j]$.\n- The anisotropic total variation is $\\mathrm{TV}(x) = \\sum_{i,j} \\left( \\lvert D_x x[i,j] \\rvert + \\lvert D_y x[i,j] \\rvert \\right)$.\n\nModel a hierarchical prior by placing a Laplace prior on each gradient component with a shared scale parameter (the total variation weight) $\\lambda > 0$, and assign a Gamma hyperprior to $\\lambda$:\n- $p(\\nabla x \\mid \\lambda) \\propto \\left(\\tfrac{\\lambda}{2}\\right)^M \\exp\\left( - \\lambda \\, \\mathrm{TV}(x) \\right)$, where $M = 2 N_x N_y$ is the total number of gradient components.\n- $p(\\lambda) = \\mathrm{Gamma}(\\lambda \\mid a, b)$ with shape $a > 0$ and rate $b > 0$, so $\\log p(\\lambda) = (a-1) \\log \\lambda - b \\lambda + \\mathrm{const}$.\n\nYour task is to:\n- Start from Bayes’ rule and the definitions above as the fundamental base. Derive the joint posterior $p(x,\\lambda \\mid y)$ and the conditional distributions needed for a joint maximum a posteriori (MAP) estimator. Do not assume any normalization constants whose dependence on $\\lambda$ is unknown; instead, use the factorization induced by the Laplace prior on gradient components and the Gamma hyperprior.\n- Show that a coordinate-ascent algorithm alternating between optimizing over $x$ and updating $\\lambda$ emerges naturally. For the $x$-update with fixed $\\lambda$, formulate the subproblem as a variational regularization problem minimizing the sum of a data-fidelity term and a scaled anisotropic total variation. For the $\\lambda$-update with fixed $x$, derive the closed-form maximizer of the conditional posterior over $\\lambda$.\n- Design an algorithmic solution that leverages periodic convolution diagonalization in the two-dimensional Discrete Fourier Transform (DFT) domain for the proximal step of the quadratic data-fidelity term, and a primal-dual splitting method for the total variation term. All steps must be implementable using only array operations and fast Fourier transforms.\n- Implement the algorithm as a complete, runnable program that constructs the forward operator $A$ from a discrete Gaussian blur kernel of size $5 \\times 5$ with standard deviation $1.0$, normalized to unit sum, embedded into the periodic convolution on a grid of size $N_x = N_y = 32$. Use the following numerical choices:\n  - Periodic boundary conditions for all operators.\n  - Primal-dual step sizes $\\tau = 0.25$ and $\\sigma = 0.25$ with extrapolation parameter $\\theta = 1$.\n  - Outer iterations $T = 12$ for the coordinate-ascent loop.\n  - Inner primal-dual iterations $K = 60$ per outer iteration.\n  - Hyperprior parameters $a = 1.1$ and $b = 10^{-3}$.\n  - Random seed fixed to $0$ for reproducibility of synthetic noise.\n- Construct three synthetic ground-truth images on the $32 \\times 32$ grid:\n  1. An “edge-dominated” image equal to zero everywhere except for a centered square of side length $16$ with intensity $1$.\n  2. A “smooth-ramp” image defined by $x[i,j] = \\tfrac{i}{N_x-1}$ for all pixel indices $(i,j)$.\n  3. A “constant” image equal to a constant value $0.5$ everywhere.\n- For each ground-truth image, generate data $y = A x_\\mathrm{true} + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$ for a specified noise standard deviation $\\sigma$ (express noise levels as decimals).\n- Evaluate the posterior’s ability to adapt to edges versus smooth regions using the following test suite. For each test, run the hierarchical MAP algorithm to obtain $(\\hat{x}, \\hat{\\lambda})$ and compute the stated boolean outcomes:\n  - Test $1$ (happy path): Use noise standard deviation $\\sigma = 0.01$. Reconstruct from the edge-dominated and smooth-ramp images. Let $\\hat{\\lambda}_\\mathrm{edge}$ and $\\hat{\\lambda}_\\mathrm{smooth}$ be the estimated weights, and let $\\mathrm{TV}(\\hat{x}_\\mathrm{edge})$ and $\\mathrm{TV}(\\hat{x}_\\mathrm{smooth})$ be the reconstructed anisotropic total variations. Return the list $[\\hat{\\lambda}_\\mathrm{edge} < \\hat{\\lambda}_\\mathrm{smooth}, \\ \\mathrm{TV}(\\hat{x}_\\mathrm{edge}) > \\mathrm{TV}(\\hat{x}_\\mathrm{smooth})]$.\n  - Test $2$ (boundary condition): Use noise standard deviation $\\sigma = 0.01$. Reconstruct from the constant image and from the edge-dominated image. Return the boolean $\\hat{\\lambda}_\\mathrm{constant} > \\hat{\\lambda}_\\mathrm{edge}$.\n  - Test $3$ (significant edge case versus non-adaptive baseline): Use the edge-dominated image with noise standard deviation $\\sigma = 0.01$. Compare the hierarchical MAP reconstruction to a non-hierarchical baseline that fixes the total variation weight to the hyperprior mean $\\lambda_\\mathrm{fixed} = \\tfrac{a}{b}$. Compute the mean squared error $\\mathrm{MSE}(\\hat{x}) = \\tfrac{1}{N_x N_y} \\lVert \\hat{x} - x_\\mathrm{true} \\rVert_2^2$. Return the boolean $\\mathrm{MSE}_\\mathrm{hierarchical} < \\mathrm{MSE}_\\mathrm{fixed}$.\n- The final output format must be a single line containing the results aggregated over the three tests as a comma-separated Python-style list, where:\n  - Test $1$ contributes a list of two booleans.\n  - Tests $2$ and $3$ contribute single booleans.\n  The final printed line must look like $[\\,[b_{11}, b_{12}], b_2, b_3\\,]$ with each $b$ being either $\\mathrm{True}$ or $\\mathrm{False}$.\n\nNo physical units are involved, and no angle units are needed. All numerical answers must be produced as Python booleans as specified. The program must not require any input and must run to completion using only the stated numerical libraries and random seed.",
            "solution": "The user-provided problem is a well-posed and scientifically grounded task in computational inverse problems. It requests the derivation and implementation of a hierarchical Bayesian model for image deblurring. The problem is valid as it is self-contained, mathematically consistent, and all necessary parameters for implementation are provided.\n\n### Hierarchical Bayesian Model Formulation\n\nThe objective is to find a joint maximum a posteriori (MAP) estimate for the unknown image $x$ and the regularization hyperparameter $\\lambda$. The posterior distribution is given by Bayes' rule:\n$$p(x, \\lambda \\mid y) \\propto p(y \\mid x) p(x \\mid \\lambda) p(\\lambda)$$\nThe components of this model are defined as:\n1.  **Likelihood**: The data $y$ is related to the image $x$ via the forward operator $A$ and additive Gaussian noise with variance $\\sigma^2$. The log-likelihood is:\n    $$\\log p(y \\mid x) = -\\frac{1}{2\\sigma^2} \\lVert Ax - y \\rVert_2^2 + C_1$$\n2.  **Prior**: The prior on the image $x$ is specified as a Laplace distribution over its gradient components, weighted by $\\lambda$. This corresponds to an anisotropic Total Variation (TV) prior. With $M = 2N_xN_y$ gradient components, the log-prior is:\n    $$\\log p(x \\mid \\lambda) = M \\log\\lambda - \\lambda \\, \\mathrm{TV}(x) + C_2$$\n    where $\\mathrm{TV}(x) = \\sum_{i,j} (\\lvert (D_x x)_{i,j} \\rvert + \\lvert (D_y x)_{i,j} \\rvert)$.\n3.  **Hyperprior**: The hyperparameter $\\lambda$ is itself given a Gamma distribution, $\\lambda \\sim \\mathrm{Gamma}(a,b)$, with shape $a$ and rate $b$. The log-hyperprior is:\n    $$\\log p(\\lambda) = (a-1)\\log\\lambda - b\\lambda + C_3$$\n\nCombining these terms, the negative log of the joint posterior $J(x, \\lambda) = -\\log p(x, \\lambda \\mid y)$ that we aim to minimize is:\n$$J(x, \\lambda) = \\frac{1}{2\\sigma^2} \\lVert Ax - y \\rVert_2^2 + \\lambda \\, \\mathrm{TV}(x) + b\\lambda - (M+a-1)\\log\\lambda + C_4$$\n\n### Coordinate Ascent for Joint MAP Estimation\n\nA natural approach to minimize $J(x, \\lambda)$ is through a coordinate ascent algorithm, which alternates between optimizing for $x$ while holding $\\lambda$ fixed, and vice-versa.\n\n**1. $x$-subproblem (fixed $\\lambda$):**\nFor a fixed value of the hyperparameter, $\\lambda_k$, the optimization problem for $x$ becomes:\n$$x_{k+1} = \\arg\\min_x \\left\\{ \\frac{1}{2\\sigma^2} \\lVert Ax - y \\rVert_2^2 + \\lambda_k \\mathrm{TV}(x) \\right\\}$$\nThis is a standard convex problem for TV-regularized image reconstruction. It consists of a quadratic data fidelity term and the non-smooth but convex TV regularizer.\n\n**2. $\\lambda$-subproblem (fixed $x$):**\nFor a fixed image estimate, $x_{k+1}$, the optimization problem for $\\lambda$ involves only the terms in $J(x, \\lambda)$ that depend on $\\lambda$:\n$$\\lambda_{k+1} = \\arg\\min_{\\lambda > 0} \\left\\{ \\lambda \\, \\mathrm{TV}(x_{k+1}) + b\\lambda - (M+a-1)\\log\\lambda \\right\\}$$\nThis is a convex function of $\\lambda$. We can find the minimizer by setting its derivative with respect to $\\lambda$ to zero:\n$$\\frac{\\partial}{\\partial\\lambda} \\left[ (\\mathrm{TV}(x_{k+1}) + b)\\lambda - (M+a-1)\\log\\lambda \\right] = \\mathrm{TV}(x_{k+1}) + b - \\frac{M+a-1}{\\lambda} = 0$$\nSolving for $\\lambda$ yields the closed-form update rule:\n$$\\lambda_{k+1} = \\frac{M+a-1}{\\mathrm{TV}(x_{k+1}) + b}$$\nThis update automatically adapts the regularization strength: a high TV value in the current estimate (implying strong edges or noise) leads to a smaller $\\lambda$, relaxing the TV penalty in the next iteration. Conversely, a smooth estimate (low TV) leads to a larger $\\lambda$, enforcing more smoothing.\n\n### Algorithmic Design for the $x$-subproblem\n\nThe $x$-subproblem is of the form $\\min_x G(x) + F(Kx)$, which is amenable to primal-dual splitting methods like the Chambolle-Pock algorithm. We define:\n- $G(x) = \\frac{1}{2\\sigma^2} \\lVert Ax-y \\rVert_2^2$ (smooth, quadratic)\n- $Kx = \\nabla x = (D_y x, D_x x)$ (discrete gradient operator)\n- $F(u) = \\lambda_k \\|u\\|_1$ (non-smooth, separable)\n\nThe iterative updates for the primal variable $x$ and a dual variable $p$ are:\n1.  **Dual Update**: $p^{j+1} = \\mathrm{prox}_{\\sigma_{\\text{pd}} F^*} (p^j + \\sigma_{\\text{pd}} K \\bar{x}^j)$\n2.  **Primal Update**: $x^{j+1} = \\mathrm{prox}_{\\tau G} (x^j - \\tau K^* p^{j+1})$\n3.  **Extrapolation**: $\\bar{x}^{j+1} = x^{j+1} + \\theta(x^{j+1} - x^j)$\n\nThe proximal operators are key:\n- $\\mathrm{prox}_{\\sigma_{\\text{pd}} F^*}$: The convex conjugate of $F(u)=\\lambda_k\\|u\\|_1$ is the indicator function of the $\\ell_\\infty$-ball of radius $\\lambda_k$. The proximal operator is thus a projection onto this ball, which simplifies to an element-wise clipping operation on the dual variable $p$:\n  $$p \\mapsto \\mathrm{clip}(p, -\\lambda_k, \\lambda_k)$$\n- $\\mathrm{prox}_{\\tau G}$: The proximal operator of the quadratic term $G(x)$ is given by:\n  $$\\mathrm{prox}_{\\tau G}(z) = \\arg\\min_x \\left\\{ \\frac{1}{2\\sigma^2}\\lVert Ax-y \\rVert_2^2 + \\frac{1}{2\\tau}\\lVert x-z \\rVert_2^2 \\right\\}$$\n  Due to the periodic boundary conditions, the convolution operator $A$ is diagonalized by the 2D Discrete Fourier Transform (DFT), $\\mathcal{F}$. Let $\\hat{h} = \\mathcal{F}(h)$ be the DFT of the blur kernel $h$. The solution can be computed efficiently in the Fourier domain:\n  $$x = \\mathcal{F}^{-1} \\left( \\frac{\\frac{1}{\\sigma^2}\\overline{\\hat{h}} \\odot \\mathcal{F}(y) + \\frac{1}{\\tau}\\mathcal{F}(z)}{\\frac{1}{\\sigma^2}|\\hat{h}|^2 + \\frac{1}{\\tau}} \\right)$$\n  where $\\odot$ and the fraction bar denote element-wise multiplication and division, respectively, and $\\overline{\\hat{h}}$ is the complex conjugate of $\\hat{h}$. This allows the primal update to be performed with a few FFTs.\n\nThe final algorithm alternates between $T$ outer loops of updating $x$ (via $K$ inner primal-dual iterations) and then updating $\\lambda$ using the closed-form solution.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates a hierarchical Bayesian method for 2D deblurring.\n    \"\"\"\n    # ---- 1. Simulation Setup ----\n    N = 32\n    T = 12  # Outer iterations for hierarchical model\n    K = 60  # Inner primal-dual iterations per outer loop\n    a = 1.1\n    b = 1e-3\n    tau = 0.25 # Primal-dual step size\n    sigma_pd = 0.25 # Primal-dual step size\n    theta = 1.0 # Primal-dual extrapolation parameter\n    noise_sigma = 0.01\n\n    rng = np.random.default_rng(0)\n\n    # ---- 2. Helper Functions and Operators ----\n    def create_gaussian_kernel(size, std_dev):\n        \"\"\"Creates a 2D Gaussian kernel.\"\"\"\n        ax = np.arange(-size // 2 + 1., size // 2 + 1.)\n        xx, yy = np.meshgrid(ax, ax)\n        kernel = np.exp(-(xx**2 + yy**2) / (2. * std_dev**2))\n        return kernel / np.sum(kernel)\n\n    def create_x_true(name, n):\n        \"\"\"Generates the ground-truth images.\"\"\"\n        x_true = np.zeros((n, n), dtype=float)\n        if name == \"edge\":\n            s, h = n // 2, n // 4\n            x_true[s-h:s+h, s-h:s+h] = 1.0\n        elif name == \"smooth\":\n            x_true = np.fromfunction(lambda i, j: i / (n - 1), (n, n), dtype=float)\n        elif name == \"constant\":\n            x_true = np.full((n, n), 0.5)\n        return x_true\n\n    def grad(u):\n        \"\"\"Computes the discrete gradient with periodic boundary conditions.\"\"\"\n        dx = np.roll(u, -1, axis=1) - u\n        dy = np.roll(u, -1, axis=0) - u\n        return np.array([dy, dx])\n\n    def div(p):\n        \"\"\"Computes the negative divergence, adjoint of the gradient.\"\"\"\n        px, py = p[1], p[0]\n        div_x = px - np.roll(px, 1, axis=1)\n        div_y = py - np.roll(py, 1, axis=0)\n        return -(div_x + div_y)\n\n    def tv(u):\n        \"\"\"Computes the anisotropic total variation.\"\"\"\n        g = grad(u)\n        return np.sum(np.abs(g[0])) + np.sum(np.abs(g[1]))\n\n    def solve_reconstruction(y_obs, h_fft, n_val, noise_sig, outer_iters, inner_iters, a_hyper, b_hyper, \n                             tau_pd, sigma_pd_val, theta_val, fixed_lambda=None):\n        \"\"\"Solves the reconstruction problem using coordinate ascent and primal-dual.\"\"\"\n        M = 2 * n_val * n_val\n        h_fft_sq = np.abs(h_fft)**2\n        y_fft = np.fft.fft2(y_obs)\n        \n        prox_g_denom = (1 / noise_sig**2) * h_fft_sq + (1 / tau_pd)\n        prox_g_num_y_part = (1 / noise_sig**2) * np.conj(h_fft) * y_fft\n\n        x = np.copy(y_obs)\n        p = np.zeros((2, n_val, n_val))\n\n        if fixed_lambda is not None:\n            lambda_reg = fixed_lambda\n            run_outer_iters = 1\n        else:\n            lambda_reg = a_hyper / b_hyper\n            run_outer_iters = outer_iters\n\n        for _ in range(run_outer_iters):\n            x_bar = np.copy(x)\n            for _ in range(inner_iters): # Inner loop for x-update\n                grad_x_bar = grad(x_bar)\n                p_new = p + sigma_pd_val * grad_x_bar\n                p = np.clip(p_new, -lambda_reg, lambda_reg)\n\n                x_old = np.copy(x)\n                z = x - tau_pd * div(p)\n                z_fft = np.fft.fft2(z)\n                \n                prox_g_num = prox_g_num_y_part + (1/tau_pd) * z_fft\n                x_fft = prox_g_num / prox_g_denom\n                x = np.real(np.fft.ifft2(x_fft))\n                \n                x_bar = x + theta_val * (x - x_old)\n            \n            if fixed_lambda is None: # Lambda-update\n                current_tv = tv(x)\n                lambda_reg = (M + a_hyper - 1) / (b_hyper + current_tv)\n\n        # Final lambda value for the hierarchical case\n        if fixed_lambda is None:\n            current_tv = tv(x)\n            lambda_reg = (M + a_hyper - 1) / (b_hyper + current_tv)\n            \n        return x, lambda_reg\n\n    # ---- 3. Setup Forward Operator ----\n    kernel_size = 5\n    kernel_std = 1.0\n    kernel = create_gaussian_kernel(kernel_size, kernel_std)\n    \n    padded_kernel = np.zeros((N, N))\n    pad_y, pad_x = (N - kernel_size) // 2, (N - kernel_size) // 2\n    padded_kernel[pad_y:pad_y + kernel_size, pad_x:pad_x + kernel_size] = kernel\n    padded_kernel = np.fft.ifftshift(padded_kernel)\n    h_fft = np.fft.fft2(padded_kernel)\n\n    def A(x_in):\n        return np.real(np.fft.ifft2(h_fft * np.fft.fft2(x_in)))\n\n    # ---- 4. Run Test Cases ----\n    \n    # Test 1: Edge-dominated vs. Smooth-ramp\n    x_true_edge = create_x_true(\"edge\", N)\n    y_edge = A(x_true_edge) + rng.normal(0, noise_sigma, (N, N))\n    x_hat_edge, lambda_hat_edge = solve_reconstruction(\n        y_edge, h_fft, N, noise_sigma, T, K, a, b, tau, sigma_pd, theta)\n    tv_hat_edge = tv(x_hat_edge)\n\n    x_true_smooth = create_x_true(\"smooth\", N)\n    y_smooth = A(x_true_smooth) + rng.normal(0, noise_sigma, (N, N))\n    x_hat_smooth, lambda_hat_smooth = solve_reconstruction(\n        y_smooth, h_fft, N, noise_sigma, T, K, a, b, tau, sigma_pd, theta)\n    tv_hat_smooth = tv(x_hat_smooth)\n    \n    test1_results = [lambda_hat_edge  lambda_hat_smooth, tv_hat_edge > tv_hat_smooth]\n    \n    # Test 2: Constant vs. Edge-dominated\n    x_true_const = create_x_true(\"constant\", N)\n    y_const = A(x_true_const) + rng.normal(0, noise_sigma, (N, N))\n    _, lambda_hat_const = solve_reconstruction(\n        y_const, h_fft, N, noise_sigma, T, K, a, b, tau, sigma_pd, theta)\n    \n    test2_result = lambda_hat_const > lambda_hat_edge\n    \n    # Test 3: Hierarchical vs. Fixed Lambda\n    x_hat_hierarchical = x_hat_edge # Re-use from Test 1\n    mse_hierarchical = np.mean((x_hat_hierarchical - x_true_edge)**2)\n\n    lambda_fixed = a / b\n    x_hat_fixed, _ = solve_reconstruction(\n        y_edge, h_fft, N, noise_sigma, T, K, a, b, tau, sigma_pd, theta, fixed_lambda=lambda_fixed)\n    mse_fixed = np.mean((x_hat_fixed - x_true_edge)**2)\n\n    test3_result = mse_hierarchical  mse_fixed\n    \n    # ---- 5. Final Output ----\n    b11, b12 = test1_results\n    b2 = test2_result\n    b3 = test3_result\n    print(f\"[[{b11}, {b12}], {b2}, {b3}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The joint MAP estimation used in the previous imaging problem is closely related to Type-II Maximum Likelihood, but it is just one of several techniques for approximate inference in hierarchical models. This final capstone exercise provides a comparative numerical study of three powerful algorithms: Type-II Maximum Likelihood, Variational Bayes (VB), and Expectation Propagation (EP). By implementing these methods from first principles and evaluating their performance across a suite of test cases with different characteristics, you will gain a deep, practical understanding of their algorithmic differences and relative merits in solving challenging inverse problems .",
            "id": "3388813",
            "problem": "Implement an end-to-end numerical study of hierarchical Bayesian inference for a linear inverse problem with Gaussian likelihood and a Gaussian prior with unknown precision. The unknown state is a vector $x \\in \\mathbb{R}^{n}$, the observed data is a vector $y \\in \\mathbb{R}^{m}$, and the forward operator is a known matrix $A \\in \\mathbb{R}^{m \\times n}$. The data model is $y = A x + \\varepsilon$ with additive noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$. The prior on the state is $x \\mid \\tau \\sim \\mathcal{N}(0, \\tau^{-1} I_{n})$, where $\\tau$ is the prior precision. The hierarchical hyperpriors are placed on both precisions: $\\tau \\sim \\mathrm{Gamma}(a_{0}, b_{0})$ and $\\beta \\equiv \\sigma^{-2} \\sim \\mathrm{Gamma}(c_{0}, d_{0})$, where $\\mathrm{Gamma}(\\cdot,\\cdot)$ is the gamma distribution in shape–rate parameterization. Your implementation must use Bayes’s rule, the definition of the multivariate normal distribution, and the definition of the gamma distribution as the fundamental starting point.\n\nThe study must implement and compare the following inference schemes for posterior approximation over $(x, \\tau, \\beta)$:\n- Expectation Propagation (EP) in which the approximate posterior factorizes as $q(x) q(\\tau) q(\\beta)$ with $q(x)$ Gaussian and $q(\\tau)$ and $q(\\beta)$ gamma, and the factors associated with $(\\tau, \\sigma^{2})$ are treated as sites. Use moment matching for site updates with damping parameter $\\delta \\in (0,1]$ applied to the site parameters. Initialize the expectations $E[\\tau]$ and $E[\\beta]$ to $1$.\n- Variational Bayes (VB) with the same factorization $q(x) q(\\tau) q(\\beta)$ and coordinate-ascent updates derived from maximizing the Evidence Lower BOund (ELBO), initialized with the same expectations $E[\\tau]$ and $E[\\beta]$ as EP.\n- Type-II Maximum Likelihood (also known as empirical Bayes), which iteratively maximizes the marginal likelihood with respect to $(\\tau,\\beta)$ using an expectation-maximization style fixed-point iteration. Initialize $\\tau$ and $\\beta$ to $1$.\n\nFor all methods, the update steps must be derived from first principles using the definitions above. For all methods, iterate until convergence defined by the stopping rule\n$\\max\\Big( \\big| E[\\tau]_{\\text{new}} - E[\\tau]_{\\text{old}} \\big| / E[\\tau]_{\\text{old}}, \\big| E[\\beta]_{\\text{new}} - E[\\beta]_{\\text{old}} \\big| / E[\\beta]_{\\text{old}} \\Big) \\leq \\epsilon$\nwith tolerance $\\epsilon = 10^{-8}$, or until a maximum of $K = 500$ iterations is reached. For Type-II Maximum Likelihood, interpret $E[\\tau]$ and $E[\\beta]$ as the current point estimates. For EP, set the damping parameter $\\delta = 0.5$. For VB, use $\\delta = 1$.\n\nFor each method, after convergence compute the posterior mean estimate $\\mu \\in \\mathbb{R}^{n}$ of $x$ under the final approximate posterior (for Type-II Maximum Likelihood, compute the posterior mean given the final $(\\tau,\\beta)$). Quantify estimation accuracy using the mean squared error $\\mathrm{MSE} = \\| \\mu - x_{\\mathrm{true}} \\|_{2}^{2} / n$.\n\nUse the following test suite of $3$ cases. In all cases, use the same weakly-informative hyperprior parameters $a_{0} = b_{0} = c_{0} = d_{0} = 10^{-3}$ (shape–rate), and construct $A$ with independent entries as standard Gaussian scaled by $1/\\sqrt{m}$ unless otherwise noted. For reproducibility, set a fixed random seed for each case before generating all random quantities of that case.\n\n- Case $1$ (well-conditioned, overdetermined): seed $0$, dimensions $m = 30$, $n = 20$, true precisions $\\tau_{\\mathrm{true}} = 2.0$, $\\beta_{\\mathrm{true}} = \\sigma_{\\mathrm{true}}^{-2}$ with $\\sigma_{\\mathrm{true}} = 0.1$. Sample $x_{\\mathrm{true}} \\sim \\mathcal{N}(0, \\tau_{\\mathrm{true}}^{-1} I_{n})$, form $y = A x_{\\mathrm{true}} + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\mathrm{true}}^{2} I_{m})$.\n- Case $2$ (underdetermined): seed $1$, dimensions $m = 15$, $n = 40$, true precisions $\\tau_{\\mathrm{true}} = 5.0$, $\\sigma_{\\mathrm{true}} = 0.2$, and the same generation procedure for $x_{\\mathrm{true}}$ and $y$.\n- Case $3$ (ill-conditioned design): seed $2$, dimensions $m = 50$, $n = 50$, true precisions $\\tau_{\\mathrm{true}} = 1.0$, $\\sigma_{\\mathrm{true}} = 0.05$. Create $A$ by first drawing $G \\in \\mathbb{R}^{m \\times n}$ with entries $\\mathcal{N}(0, 1/m)$, then scaling column $j$ by $s_{j} = 10^{-(j-1)/(n-1)}$ so that $A_{:,j} = s_{j} \\, G_{:,j}$.\n\nYour program must:\n- Implement all three solvers using only the foundational definitions stated, with numerically stable linear algebra. For Gaussian posteriors, use the identities for Gaussian conditioning to compute the posterior mean and covariance of $x$ given $(\\tau,\\beta)$ using matrix equations. For the traces required in updates, use $\\mathrm{tr}(A \\Sigma A^{\\top}) = \\mathrm{tr}(\\Sigma A^{\\top} A)$.\n- Use the same initialization $E[\\tau] = 1$ and $E[\\beta] = 1$ for EP and VB, and $\\tau = 1$, $\\beta = 1$ for Type-II Maximum Likelihood.\n- For each case, compute and record three floating-point values: the mean squared error of Expectation Propagation, the mean squared error of Variational Bayes, and the mean squared error of Type-II Maximum Likelihood, in that order.\n\nFinal output format: Your program should produce a single line of output containing the $9$ results as a comma-separated list enclosed in square brackets, ordered as $[\\mathrm{MSE}_{\\mathrm{EP}}^{(1)}, \\mathrm{MSE}_{\\mathrm{VB}}^{(1)}, \\mathrm{MSE}_{\\mathrm{ML}}^{(1)}, \\mathrm{MSE}_{\\mathrm{EP}}^{(2)}, \\mathrm{MSE}_{\\mathrm{VB}}^{(2)}, \\mathrm{MSE}_{\\mathrm{ML}}^{(2)}, \\mathrm{MSE}_{\\mathrm{EP}}^{(3)}, \\mathrm{MSE}_{\\mathrm{VB}}^{(3)}, \\mathrm{MSE}_{\\mathrm{ML}}^{(3)}]$. All values are unitless real numbers and must be output as standard decimal representations.",
            "solution": "The user requests an implementation and comparison of three hierarchical Bayesian inference methods for a linear inverse problem: Expectation Propagation (EP), Variational Bayes (VB), and Type-II Maximum Likelihood (ML-II). Before providing the numerical solution, we must validate the problem statement.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Model:**\n    - Data Model (Likelihood): $y = Ax + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$. This is equivalent to $p(y | x, \\beta) = \\mathcal{N}(y | Ax, \\beta^{-1}I_m)$ with precision $\\beta = \\sigma^{-2}$.\n    - Prior on state: $x \\mid \\tau \\sim \\mathcal{N}(0, \\tau^{-1} I_n)$, where $\\tau$ is the prior precision.\n    - Hyperprior on prior precision: $\\tau \\sim \\mathrm{Gamma}(a_0, b_0)$.\n    - Hyperprior on noise precision: $\\beta \\sim \\mathrm{Gamma}(c_0, d_0)$.\n    - The Gamma distribution is specified in shape-rate parameterization, with density $f(z; a, b) = \\frac{b^a}{\\Gamma(a)} z^{a-1} e^{-bz}$.\n- **Variables and Dimensions:**\n    - State: $x \\in \\mathbb{R}^n$.\n    - Data: $y \\in \\mathbb{R}^m$.\n    - Forward Operator: $A \\in \\mathbb{R}^{m \\times n}$.\n- **Inference Schemes  Approximations:**\n    1.  **Expectation Propagation (EP):**\n        - Approximate posterior: $q(x, \\tau, \\beta) = q(x) q(\\tau) q(\\beta)$.\n        - Factor families: $q(x)$ is Gaussian, $q(\\tau)$ and $q(\\beta)$ are Gamma.\n        - Sites: \"the factors associated with $(\\tau, \\sigma^2)$ are treated as sites.\"\n        - Update rule: Moment matching with damping parameter $\\delta = 0.5$ on site parameters.\n        - Initialization: $E[\\tau] = 1$, $E[\\beta] = 1$.\n    2.  **Variational Bayes (VB):**\n        - Approximate posterior: $q(x, \\tau, \\beta) = q(x) q(\\tau) q(\\beta)$.\n        - Update rule: Coordinate-ascent on the Evidence Lower Bound (ELBO).\n        - \"Damping\": The problem asks to \"use $\\delta = 1$\".\n        - Initialization: $E[\\tau] = 1$, $E[\\beta] = 1$.\n    3.  **Type-II Maximum Likelihood (ML-II) / Empirical Bayes:**\n        - Update rule: Iterative maximization of marginal likelihood $p(y|\\tau,\\beta)$ via an EM-style algorithm.\n        - Initialization: $\\tau=1, \\beta=1$.\n- **Numerical Parameters:**\n    - Stopping tolerance: $\\epsilon = 10^{-8}$ on relative change in expectations. For ML-II, point estimates are used as expectations for this check.\n    - Maximum iterations: $K = 500$.\n    - Hyperprior parameters: $a_0 = b_0 = c_0 = d_0 = 10^{-3}$.\n- **Post-processing:**\n    - Estimate of $x$: Posterior mean $\\mu$.\n    - Accuracy measure: Mean Squared Error $\\mathrm{MSE} = \\| \\mu - x_{\\mathrm{true}} \\|_{2}^{2} / n$.\n- **Test Cases:**\n    - All cases use the same hyperprior parameters. Random numbers are generated with a new seed for each case. $A$ has entries from $\\mathcal{N}(0, 1/m)$ unless specified.\n    - **Case 1:** seed $0$, $m=30, n=20$ (overdetermined), $\\tau_{\\mathrm{true}}=2.0, \\sigma_{\\mathrm{true}}=0.1$.\n    - **Case 2:** seed $1$, $m=15, n=40$ (underdetermined), $\\tau_{\\mathrm{true}}=5.0, \\sigma_{\\mathrm{true}}=0.2$.\n    - **Case 3:** seed $2$, $m=50, n=50$ (ill-conditioned), $\\tau_{\\mathrm{true}}=1.0, \\sigma_{\\mathrm{true}}=0.05$. $A$'s columns are scaled by $s_j = 10^{-(j-1)/(n-1)}$.\n- **Implementation Constraints:**\n    - Derivations must start from Bayes's rule and distribution definitions.\n    - Use matrix identities for Gaussian conditioning and trace computations.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically and mathematically sound. It describes a standard hierarchical Bayesian model for linear regression and asks for the implementation of three well-established, albeit advanced, inference algorithms (VB, EP, ML-II). The model, variables, parameters, and test cases are all defined precisely. There are no contradictions, ambiguities, or claims that violate scientific principles. The problem is self-contained and well-posed as a numerical implementation task.\n\nA subtle point of interpretation exists for the Expectation Propagation algorithm. A from-first-principles derivation of EP for this model's factor graph is highly complex. However, the problem provides a strong clue by specifying a damping parameter $\\delta$ for both VB and EP, with VB using $\\delta=1$. This suggests a pragmatic interpretation: the core iterative update equations for the parameters of the Gamma distributions in EP are the same as those derived for VB, with the key difference being the application of damping ($\\delta=0.5$ for EP, $\\delta=1.0$ for VB). This interpretation is consistent, avoids unnecessary complexity, and makes the comparison meaningful within the context of a numerical study.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. We proceed with the derivation and solution.\n\n### Derivation of Inference Algorithms\n\nThe full joint probability distribution is given by the product of the likelihood, prior, and hyperpriors:\n$$\np(y, x, \\tau, \\beta) = p(y | x, \\beta) p(x | \\tau) p(\\tau) p(\\beta)\n$$\nwhere\n- $p(y | x, \\beta) = \\left(\\frac{\\beta}{2\\pi}\\right)^{m/2} \\exp\\left(-\\frac{\\beta}{2} \\|y - Ax\\|_2^2\\right)$\n- $p(x | \\tau) = \\left(\\frac{\\tau}{2\\pi}\\right)^{n/2} \\exp\\left(-\\frac{\\tau}{2} \\|x\\|_2^2\\right)$\n- $p(\\tau) = \\frac{b_0^{a_0}}{\\Gamma(a_0)} \\tau^{a_0-1} \\exp(-b_0 \\tau)$\n- $p(\\beta) = \\frac{d_0^{c_0}}{\\Gamma(c_0)} \\beta^{c_0-1} \\exp(-d_0 \\beta)$\n\n#### Variational Bayes (VB) and Expectation Propagation (EP)\n\nBoth methods use the same factorized approximate posterior $q(x, \\tau, \\beta) = q(x)q(\\tau)q(\\beta)$, where $q(x)$ is Gaussian, and $q(\\tau), q(\\beta)$ are Gamma distributions.\n$$\nq(x) = \\mathcal{N}(x | \\mu_x, \\Sigma_x), \\quad q(\\tau) = \\mathrm{Gamma}(\\tau | a_\\tau, b_\\tau), \\quad q(\\beta) = \\mathrm{Gamma}(\\beta | a_\\beta, b_\\beta)\n$$\nThe VB updates are derived via Coordinate Ascent on the ELBO. The optimal distribution for one factor $q_j$ is given by $ \\log q_j^*(z_j) = \\mathbb{E}_{q_{\\setminus j}}[\\log p(y, x, \\tau, \\beta)] + \\mathrm{const}$.\n\n**1. Update for $q(x)$:**\n$\\log q^*(x) \\propto \\mathbb{E}_{q(\\tau), q(\\beta)}[\\log p(y|x,\\beta) + \\log p(x|\\tau)]$.\n$\\log q^*(x) \\propto \\mathbb{E}_{\\beta}[-\\frac{\\beta}{2}\\|y-Ax\\|^2] + \\mathbb{E}_{\\tau}[-\\frac{\\tau}{2}\\|x\\|^2] = -\\frac{\\mathbb{E}[\\beta]}{2}\\|y-Ax\\|^2 - \\frac{\\mathbb{E}[\\tau]}{2}\\|x\\|^2$.\nExpanding this quadratic form in $x$ shows that $q^*(x)$ is a Gaussian $\\mathcal{N}(x | \\mu_x, \\Sigma_x)$ with posterior covariance and mean:\n$$\n\\Sigma_x = (\\mathbb{E}[\\beta] A^\\top A + \\mathbb{E}[\\tau] I_n)^{-1}\n$$\n$$\n\\mu_x = \\mathbb{E}[\\beta] \\Sigma_x A^\\top y\n$$\nThe expectations $\\mathbb{E}[\\tau] = a_\\tau/b_\\tau$ and $\\mathbb{E}[\\beta] = a_\\beta/b_\\beta$ are taken with respect to the current $q(\\tau)$ and $q(\\beta)$.\n\n**2. Update for $q(\\tau)$:**\n$\\log q^*(\\tau) \\propto \\mathbb{E}_{q(x)}[\\log p(x|\\tau) + \\log p(\\tau)]$.\n$\\log q^*(\\tau) \\propto \\mathbb{E}_{q(x)}[\\frac{n}{2}\\log\\tau - \\frac{\\tau}{2}x^\\top x] + (a_0-1)\\log\\tau - b_0\\tau$.\n$\\log q^*(\\tau) \\propto (a_0 + n/2 - 1)\\log\\tau - (b_0 + \\frac{1}{2}\\mathbb{E}[x^\\top x])\\tau$.\nThis is the log-kernel of a Gamma distribution, yielding the proposed parameters for the update:\n$$\na_{\\tau, \\text{prop}} = a_0 + n/2\n$$\n$$\nb_{\\tau, \\text{prop}} = b_0 + \\frac{1}{2}\\mathbb{E}[x^\\top x] = b_0 + \\frac{1}{2}(\\mu_x^\\top\\mu_x + \\mathrm{tr}(\\Sigma_x))\n$$\n\n**3. Update for $q(\\beta)$:**\n$\\log q^*(\\beta) \\propto \\mathbb{E}_{q(x)}[\\log p(y|x,\\beta) + \\log p(\\beta)]$.\n$\\log q^*(\\beta) \\propto (\\frac{m}{2} + c_0 - 1)\\log\\beta - (\\frac{1}{2}\\mathbb{E}[\\|y-Ax\\|^2] + d_0)\\beta$.\nThis is also a log-kernel of a Gamma distribution. The proposed parameters are:\n$$\na_{\\beta, \\text{prop}} = c_0 + m/2\n$$\n$$\nb_{\\beta, \\text{prop}} = d_0 + \\frac{1}{2}\\mathbb{E}[\\|y-Ax\\|^2] = d_0 + \\frac{1}{2}(\\|y-A\\mu_x\\|^2 + \\mathrm{tr}(A\\Sigma_x A^\\top))\n$$\nThe term $\\mathrm{tr}(A\\Sigma_x A^\\top)$ can be computed more efficiently as $\\mathrm{tr}(\\Sigma_x A^\\top A)$ when $n  m$.\n\n**Iterative Scheme with Damping:**\nBased on the problem description, we implement a unified algorithm for VB and EP where a damping parameter $\\delta$ is applied to the update of the Gamma parameters $(a,b)$. VB corresponds to $\\delta=1$ (no damping), and EP corresponds to $\\delta=0.5$.\nThe parameters are initialized from the hyperpriors, $a_{\\tau,0}=a_0, b_{\\tau,0}=b_0$, which is consistent with the initial expectation $E[\\tau]=1$.\nThe update step is:\n$$\na_{\\tau, k+1} = (1-\\delta)a_{\\tau, k} + \\delta a_{\\tau, \\text{prop}}, \\quad b_{\\tau, k+1} = (1-\\delta)b_{\\tau, k} + \\delta b_{\\tau, \\text{prop}}\n$$\nand similarly for $a_\\beta, b_\\beta$.\n\n#### Type-II Maximum Likelihood (ML-II)\nThis method finds point estimates of $\\tau$ and $\\beta$ by maximizing the marginal likelihood $p(y|\\tau,\\beta)$, which is equivalent to maximizing $\\log p(y|\\tau,\\beta) = \\log \\int p(y|x,\\beta)p(x|\\tau)dx$. We use an Expectation-Maximization (EM) algorithm, treating $x$ as the latent variable.\n\n**E-Step:** At iteration $k$, compute the expectation of the complete-data log-likelihood with respect to the current posterior over $x$, $p(x|y, \\tau_k, \\beta_k)$. This posterior is Gaussian $\\mathcal{N}(x|\\mu_k, \\Sigma_k)$ with:\n$$\n\\Sigma_k = (\\beta_k A^\\top A + \\tau_k I_n)^{-1}, \\quad \\mu_k = \\beta_k \\Sigma_k A^\\top y\n$$\nThe expected log-likelihood (Q-function), ignoring constants, is:\n$$\nQ(\\tau,\\beta|\\tau_k, \\beta_k) = \\mathbb{E}_{p(x|y,\\tau_k,\\beta_k)}[\\log p(y|x,\\beta) + \\log p(x|\\tau)]\n$$\n$$\n= \\frac{m}{2}\\log\\beta - \\frac{\\beta}{2}\\mathbb{E}[\\|y-Ax\\|^2] + \\frac{n}{2}\\log\\tau - \\frac{\\tau}{2}\\mathbb{E}[\\|x\\|^2]\n$$\n\n**M-Step:** Maximize $Q$ with respect to $\\tau$ and $\\beta$. Setting the partial derivatives to zero yields the update rules:\n$$\n\\frac{\\partial Q}{\\partial \\tau} = \\frac{n}{2\\tau} - \\frac{1}{2}\\mathbb{E}[\\|x\\|^2] = 0 \\implies \\tau_{k+1} = \\frac{n}{\\mathbb{E}[\\|x\\|^2]}\n$$\n$$\n\\frac{\\partial Q}{\\partial \\beta} = \\frac{m}{2\\beta} - \\frac{1}{2}\\mathbb{E}[\\|y-Ax\\|^2] = 0 \\implies \\beta_{k+1} = \\frac{m}{\\mathbb{E}[\\|y-Ax\\|^2]}\n$$\nThe required expectations are computed with $\\mu_k$ and $\\Sigma_k$:\n$\\mathbb{E}[\\|x\\|^2] = \\mu_k^\\top \\mu_k + \\mathrm{tr}(\\Sigma_k)$, and $\\mathbb{E}[\\|y-Ax\\|^2] = \\|y-A\\mu_k\\|^2 + \\mathrm{tr}(A\\Sigma_k A^\\top)$.\n\nThe final derived algorithms are implemented for the specified numerical study.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import inv\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the numerical study for the three test cases.\n    \"\"\"\n\n    def solve_vb_ep(y, A, x_true, params, delta):\n        \"\"\"\n        Solver for Variational Bayes (delta=1.0) and Expectation Propagation (delta=0.5).\n        Implements a damped coordinate ascent algorithm on the variational parameters.\n        \"\"\"\n        m, n = A.shape\n        a0, b0, c0, d0, K_max, epsilon = params\n\n        # Initialize Gamma parameters from hyperpriors.\n        # Initial E[tau]=a0/b0 and E[beta]=c0/d0. Since a0=b0=c0=d0, this is 1.\n        a_tau, b_tau = a0, b0\n        a_beta, b_beta = c0, d0\n\n        # Store initial expectations for the first convergence check.\n        E_tau_old = a_tau / b_tau\n        E_beta_old = a_beta / b_beta\n        \n        AtA = A.T @ A  # Pre-compute for efficiency\n\n        for _ in range(K_max):\n            E_tau = a_tau / b_tau\n            E_beta = a_beta / b_beta\n            \n            # --- Update q(x) ---\n            # Sigma_x = (E[beta] * A^T A + E[tau] * I)^-1\n            Sigma_x_inv = E_beta * AtA + E_tau * np.identity(n)\n            Sigma_x = inv(Sigma_x_inv, overwrite_a=True)\n            \n            # mu_x = E[beta] * Sigma_x * A^T * y\n            mu_x = E_beta * (Sigma_x @ (A.T @ y))\n\n            # --- Update q(tau) and q(beta) ---\n            # Moments for Gamma parameter updates\n            Ex_T_x = np.dot(mu_x, mu_x) + np.trace(Sigma_x)\n            \n            # E[||y - Ax||^2]\n            res = y - A @ mu_x\n            res_norm_sq = np.dot(res, res)\n            if n > m:\n                tr_ASigmaAt = np.trace(A @ Sigma_x @ A.T)\n            else:\n                tr_ASigmaAt = np.trace(AtA @ Sigma_x)\n            E_res_norm_sq = res_norm_sq + tr_ASigmaAt\n\n            # Proposed Gamma parameters from VB derivation\n            a_tau_prop = a0 + n / 2.0\n            b_tau_prop = b0 + Ex_T_x / 2.0\n            \n            a_beta_prop = c0 + m / 2.0\n            b_beta_prop = d0 + E_res_norm_sq / 2.0\n            \n            # Apply damping to the Gamma parameters\n            a_tau = (1.0 - delta) * a_tau + delta * a_tau_prop\n            b_tau = (1.0 - delta) * b_tau + delta * b_tau_prop\n            a_beta = (1.0 - delta) * a_beta + delta * a_beta_prop\n            b_beta = (1.0 - delta) * b_beta + delta * b_beta_prop\n            \n            # --- Convergence Check ---\n            # Update expectations for the next iteration and check convergence\n            E_tau_new = a_tau / b_tau\n            E_beta_new = a_beta / b_beta\n\n            err_tau = abs(E_tau_new - E_tau_old) / E_tau_old if E_tau_old != 0.0 else abs(E_tau_new)\n            err_beta = abs(E_beta_new - E_beta_old) / E_beta_old if E_beta_old != 0.0 else abs(E_beta_new)\n            if max(err_tau, err_beta)  epsilon:\n                break\n            \n            E_tau_old, E_beta_old = E_tau_new, E_beta_new\n\n        # Compute final posterior mean mu_x for MSE calculation\n        E_tau = a_tau / b_tau\n        E_beta = a_beta / b_beta\n        Sigma_x_inv = E_beta * AtA + E_tau * np.identity(n)\n        Sigma_x = inv(Sigma_x_inv, overwrite_a=True)\n        mu_x = E_beta * (Sigma_x @ (A.T @ y))\n        \n        mse = np.sum((mu_x - x_true)**2) / n\n        return mse\n\n    def solve_ml_ii(y, A, x_true, params):\n        \"\"\"\n        Solver for Type-II Maximum Likelihood (Empirical Bayes).\n        Implements an EM-style fixed-point iteration.\n        \"\"\"\n        m, n = A.shape\n        K_max, epsilon = params\n        \n        # Initialize point estimates\n        tau, beta = 1.0, 1.0\n        \n        tau_old, beta_old = tau, beta\n        \n        AtA = A.T @ A # Pre-compute for efficiency\n\n        for _ in range(K_max):\n            # E-step: compute posterior over x given current tau, beta\n            Sigma_k_inv = beta * AtA + tau * np.identity(n)\n            Sigma_k = inv(Sigma_k_inv, overwrite_a=True)\n            mu_k = beta * (Sigma_k @ (A.T @ y))\n            \n            # M-step: update tau and beta\n            Ex_T_x = np.dot(mu_k, mu_k) + np.trace(Sigma_k)\n            \n            res = y - A @ mu_k\n            res_norm_sq = np.dot(res, res)\n            if n > m:\n                tr_ASigmaAt = np.trace(A @ Sigma_k @ A.T)\n            else:\n                tr_ASigmaAt = np.trace(AtA @ Sigma_k)\n            E_res_norm_sq = res_norm_sq + tr_ASigmaAt\n            \n            tau_new = n / Ex_T_x if Ex_T_x > 0 else 0\n            beta_new = m / E_res_norm_sq if E_res_norm_sq > 0 else 0\n            \n            # Convergence Check\n            err_tau = abs(tau_new - tau_old) / tau_old if tau_old != 0 else abs(tau_new)\n            err_beta = abs(beta_new - beta_old) / beta_old if beta_old != 0 else abs(beta_new)\n            if max(err_tau, err_beta)  epsilon:\n                tau, beta = tau_new, beta_new\n                break\n\n            tau, beta = tau_new, beta_new\n            tau_old, beta_old = tau, beta\n\n        # Final estimate for mu after convergence\n        Sigma_final_inv = beta * AtA + tau * np.identity(n)\n        Sigma_final = inv(Sigma_final_inv, overwrite_a=True)\n        mu_final = beta * (Sigma_final @ (A.T @ y))\n\n        mse = np.sum((mu_final - x_true)**2) / n\n        return mse\n\n    # --- Problem Setup ---\n    test_cases = [\n        {'seed': 0, 'm': 30, 'n': 20, 'tau_true': 2.0, 'sigma_true': 0.1, 'A_mode': 'standard'},\n        {'seed': 1, 'm': 15, 'n': 40, 'tau_true': 5.0, 'sigma_true': 0.2, 'A_mode': 'standard'},\n        {'seed': 2, 'm': 50, 'n': 50, 'tau_true': 1.0, 'sigma_true': 0.05, 'A_mode': 'ill-conditioned'},\n    ]\n    \n    a0, b0, c0, d0 = 1e-3, 1e-3, 1e-3, 1e-3\n    K_max = 500\n    epsilon = 1e-8\n    \n    vb_ep_params = (a0, b0, c0, d0, K_max, epsilon)\n    ml_params = (K_max, epsilon)\n    \n    delta_ep = 0.5\n    delta_vb = 1.0\n    \n    all_results = []\n\n    for case in test_cases:\n        rng = np.random.default_rng(case['seed'])\n        m, n = case['m'], case['n']\n        \n        # --- Generate Data ---\n        # Generate A matrix\n        if case['A_mode'] == 'standard':\n            A = rng.standard_normal((m, n)) / np.sqrt(m)\n        elif case['A_mode'] == 'ill-conditioned':\n            G = rng.standard_normal((m, n)) / np.sqrt(m)\n            s = 10**(-np.arange(n) / (n - 1.0))\n            A = G * s\n        \n        # Generate true state x_true\n        tau_true = case['tau_true']\n        sigma_x_true = np.sqrt(1.0 / tau_true)\n        x_true = rng.normal(0, sigma_x_true, n)\n        \n        # Generate observations y\n        sigma_true = case['sigma_true']\n        noise = rng.normal(0, sigma_true, m)\n        y = A @ x_true + noise\n        \n        # --- Run Solvers ---\n        mse_ep = solve_vb_ep(y, A, x_true, vb_ep_params, delta_ep)\n        mse_vb = solve_vb_ep(y, A, x_true, vb_ep_params, delta_vb)\n        mse_ml = solve_ml_ii(y, A, x_true, ml_params)\n        \n        all_results.extend([mse_ep, mse_vb, mse_ml])\n\n    print(f\"[{','.join(f'{r:.10f}' for r in all_results)}]\")\n\nsolve()\n```"
        }
    ]
}