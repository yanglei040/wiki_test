{
    "hands_on_practices": [
        {
            "introduction": "理论为我们提供了框架，说明了Besov空间先验可以实现为小波系数的加权$\\ell^{1}$范数。但是，这些权重具体从何而来？这个基础练习将引导你通过推导，建立Besov范数的抽象定义与小波域中具体惩罚项之间的联系，从而从第一性原理出发，揭示尺度依赖权重的起源。",
            "id": "3367773",
            "problem": "令 $\\mathbb{T}^{d}$ 表示 $d$ 维环面。考虑 $\\mathbb{T}^{d}$ 上的一个正交小波系统，该系统通过对具有 $r$-正则性和至少 $M$ 个消失矩的紧支撑多分辨率分析进行周期化得到，其中 $r  s$ 且 $M  s$，$s \\in \\mathbb{R}$。令 $\\{\\psi_{j,k}^{m}\\}_{j \\geq 0,\\, k \\in \\{0,\\dots,2^{j}-1\\}^{d},\\, m \\in \\{1,\\dots,2^{d}-1\\}}$ 表示尺度为 $j$、位置为 $k$、类型为 $m$ 的小波，以及在最粗尺度上的一组有限的尺度函数。对于 $f \\in \\mathcal{D}'(\\mathbb{T}^{d})$，定义小波系数为 $c_{j,k}^{m} = \\langle f, \\psi_{j,k}^{m} \\rangle$，其中 $\\langle \\cdot,\\cdot \\rangle$ 表示 $L^{2}(\\mathbb{T}^{d})$ 对偶配对。\n\nBesov 空间 $B_{1,1}^{s}(\\mathbb{T}^{d})$ 可以通过二进 Littlewood–Paley 分解 $\\{ \\Delta_{j} \\}_{j \\geq 0}$ 定义为所有 $f \\in \\mathcal{D}'(\\mathbb{T}^{d})$ 的集合，其中 $\\sum_{j \\geq 0} 2^{j s} \\| \\Delta_{j} f \\|_{L^{1}(\\mathbb{T}^{d})}$ 是有限的，这在拟范数等价的意义下成立。你可以使用以下关于 $\\mathbb{T}^{d}$ 上紧支撑正交小波的熟知事实：\n- 对于每个固定的 $j$，集合 $\\{\\psi_{j,k}^{m}\\}_{k,m}$ 构成一个函数子空间的无条件基，这些函数的频率主要集中在二进尺度 $2^{j}$ 处。并且存在与 $j$ 无关的常数 $A_{p},B_{p} \\in (0,\\infty)$，使得对于任意有限支撑的系数数组 $\\{a_{k}^{m}\\}_{k,m}$，\n$$\nA_{p} \\, 2^{j d\\left(\\frac{1}{2}-\\frac{1}{p}\\right)} \\left\\| a \\right\\|_{\\ell^{p}} \\le \\left\\| \\sum_{k,m} a_{k}^{m} \\psi_{j,k}^{m} \\right\\|_{L^{p}(\\mathbb{T}^{d})} \\le B_{p} \\, 2^{j d\\left(\\frac{1}{2}-\\frac{1}{p}\\right)} \\left\\| a \\right\\|_{\\ell^{p}} \\quad \\text{对于 } p \\in [1,\\infty].\n$$\n- Littlewood–Paley 投影算子 $\\Delta_{j}$ 可以通过投影到 $\\{\\psi_{j,k}^{m}\\}_{k,m}$ 的张成空间上来实现（在每个固定尺度上，相差一个一致有界的可逆算子）。\n\n在全文中假设 $f$ 的均值为零，因此在拟范数等价性中可以忽略有限的尺度函数系数集合。证明 $f$ 的 $B_{1,1}^{s}(\\mathbb{T}^{d})$ 拟范数等价于其小波系数的加权 $\\ell^{1}$ 范数，形式如下\n$$\n\\sum_{j \\geq 0} w_{j} \\sum_{k,m} |c_{j,k}^{m}|,\n$$\n并显式地计算权重 $w_{j}$，用 $j$、$s$ 和 $d$ 表示。你的最终答案必须是 $w_{j}$ 作为 $j$、$s$ 和 $d$ 的函数的单个闭式解析表达式（无单位）。",
            "solution": "目标是证明 Besov 拟范数 $\\|f\\|_{B_{1,1}^{s}(\\mathbb{T}^{d})}$ 等价于 $f$ 的小波系数的特定加权 $\\ell^{1}$ 范数，并确定权重 $w_{j}$ 的显式形式。\n\n分析始于 Besov 空间 $B_{1,1}^{s}(\\mathbb{T}^{d})$ 拟范数的定义，该定义由 Littlewood–Paley 分解 $\\{\\Delta_{j}\\}_{j \\geq 0}$ 给出。对于一个均值为零的函数 $f \\in \\mathcal{D}'(\\mathbb{T}^{d})$，此拟范数定义为：\n$$\n\\|f\\|_{B_{1,1}^{s}(\\mathbb{T}^{d})} = \\sum_{j \\geq 0} 2^{j s} \\| \\Delta_{j} f \\|_{L^{1}(\\mathbb{T}^{d})}\n$$\n问题陈述 Littlewood–Paley 投影算子 $\\Delta_{j}$ 可以通过将 $f$ 投影到尺度为 $j$ 的小波的张成空间上来实现。我们将此投影算子表示为 $P_{j}$。对于一个具有小波系数 $c_{j,k}^{m} = \\langle f, \\psi_{j,k}^{m} \\rangle$ 的函数 $f$，这个投影由下式给出：\n$$\nP_{j}f = \\sum_{k \\in \\{0,\\dots,2^{j}-1\\}^{d}} \\sum_{m \\in \\{1,\\dots,2^{d}-1\\}} c_{j,k}^{m} \\psi_{j,k}^{m}\n$$\n问题陈述 $\\Delta_{j}$ 与 $P_{j}$ 通过一致有界的可逆算子相关联。这意味着 $\\Delta_{j}f$ 和 $P_{j}f$ 的范数是等价的。也就是说，存在与 $j$ 和 $f$ 无关的常数 $C_{1}, C_{2} \\in (0,\\infty)$，使得：\n$$\nC_{1} \\| P_{j}f \\|_{L^{1}(\\mathbb{T}^{d})} \\le \\| \\Delta_{j} f \\|_{L^{1}(\\mathbb{T}^{d})} \\le C_{2} \\| P_{j}f \\|_{L^{1}(\\mathbb{T}^{d})}\n$$\n对 $j$ 求和并引入因子 $2^{js}$，我们发现 Besov 拟范数等价于一个涉及小波投影范数的和：\n$$\n\\|f\\|_{B_{1,1}^{s}(\\mathbb{T}^{d})} \\approx \\sum_{j \\geq 0} 2^{j s} \\| P_{j}f \\|_{L^{1}(\\mathbb{T}^{d})}\n$$\n其中 $\\approx$ 表示拟范数等价，意味着两边的比值被正常数从上方和下方界定。\n\n下一步是将小波投影 $P_{j}f$ 的 $L^{1}(\\mathbb{T}^{d})$ 范数与其对应系数 $\\{c_{j,k}^{m}\\}$ 的 $\\ell^{1}$ 范数联系起来。问题提供了一个关键的范数等价关系：\n$$\nA_{p} \\, 2^{j d\\left(\\frac{1}{2}-\\frac{1}{p}\\right)} \\left\\| a \\right\\|_{\\ell^{p}} \\le \\left\\| \\sum_{k,m} a_{k}^{m} \\psi_{j,k}^{m} \\right\\|_{L^{p}(\\mathbb{T}^{d})} \\le B_{p} \\, 2^{j d\\left(\\frac{1}{2}-\\frac{1}{p}\\right)} \\left\\| a \\right\\|_{\\ell^{p}}\n$$\n对于任何有限支撑的系数数组 $\\{a_{k}^{m}\\}$。此结果可推广到适当空间中函数的无限系数序列。我们关心的是 $L^{1}$ 范数，因此我们令 $p=1$。系数为 $a_{k}^{m} = c_{j,k}^{m}$。系数数组的范数是 $\\ell^{1}$ 范数：\n$$\n\\|c_{j}\\|_{\\ell^{1}} = \\sum_{k,m} |c_{j,k}^{m}|\n$$\n将 $p=1$ 代入缩放因子的指数中，得到：\n$$\nj d\\left(\\frac{1}{2}-\\frac{1}{p}\\right) = j d\\left(\\frac{1}{2}-\\frac{1}{1}\\right) = j d\\left(-\\frac{1}{2}\\right) = -\\frac{jd}{2}\n$$\n将此应用于 $P_{j}f = \\sum_{k,m} c_{j,k}^{m} \\psi_{j,k}^{m}$ 的范数等价关系，我们得到：\n$$\nA_{1} \\, 2^{-jd/2} \\sum_{k,m} |c_{j,k}^{m}| \\le \\| P_{j}f \\|_{L^{1}(\\mathbb{T}^{d})} \\le B_{1} \\, 2^{-jd/2} \\sum_{k,m} |c_{j,k}^{m}|\n$$\n这建立了等价关系：\n$$\n\\| P_{j}f \\|_{L^{1}(\\mathbb{T}^{d})} \\approx 2^{-jd/2} \\sum_{k,m} |c_{j,k}^{m}|\n$$\n现在，我们将此结果代回到 Besov 拟范数的表达式中：\n$$\n\\|f\\|_{B_{1,1}^{s}(\\mathbb{T}^{d})} \\approx \\sum_{j \\geq 0} 2^{j s} \\left( 2^{-jd/2} \\sum_{k,m} |c_{j,k}^{m}| \\right)\n$$\n合并指数项，我们得到：\n$$\n\\|f\\|_{B_{1,1}^{s}(\\mathbb{T}^{d})} \\approx \\sum_{j \\geq 0} 2^{js} 2^{-jd/2} \\sum_{k,m} |c_{j,k}^{m}| = \\sum_{j \\geq 0} 2^{j(s - d/2)} \\sum_{k,m} |c_{j,k}^{m}|\n$$\n问题要求一个形如 $\\sum_{j \\geq 0} w_{j} \\sum_{k,m} |c_{j,k}^{m}|$ 的表达式。通过与我们推导出的表达式进行比较，我们可以直接确定权重 $w_{j}$。\n$$\nw_{j} = 2^{j(s - d/2)}\n$$\n这表明 $B_{1,1}^{s}(\\mathbb{T}^{d})$ 拟范数等价于小波系数的加权 $\\ell^{1}$ 范数，其权重由上述表达式给出。对小波系统的条件 $rs$ 和 $Ms$ 是确保此特征描述有效的标准要求。",
            "answer": "$$\n\\boxed{2^{j(s - \\frac{d}{2})}}\n$$"
        },
        {
            "introduction": "理论提供了工具，而直觉则通过实践建立。Besov空间先验具有诸如$p$、$q$和$s$之类的参数，这些参数控制其行为，理解它们的影响是有效模型选择的关键。这项计算练习将让你直接观察到尺度聚合参数$q$的作用，通过比较使用$q=1$和$q=\\infty$先验的重建结果，你将对不同的Besov范数如何在不同尺度上施加稀疏性，以及如何影响具有粗糙和精细特征信号的恢复，形成具体的理解。",
            "id": "3367760",
            "problem": "考虑一个具有恒等正演模型的一维离散逆问题，重点是通过贝索夫空间先验实现的多尺度正则化的作用。设 $N=256$，并设 $u \\in \\mathbb{R}^{N}$ 表示未知信号。设 $W:\\mathbb{R}^{N} \\to \\mathbb{R}^{N}$ 是标准正交哈尔小波变换，它产生 $J=\\log_{2}(N)$ 个尺度的细节系数 $\\{w_{j}\\}_{j=1}^{J}$ 以及最终的尺度系数，其中 $w_{j} \\in \\mathbb{R}^{N/2^{j}}$ 表示尺度 $j$ 处的细节系数（$j=1$ 为最精细尺度）。定义在平滑度 $s=0$、可积指数 $p=2$ 且尺度聚合器 $q \\in \\{1,\\infty\\}$ 下的离散贝索夫半范数为\n$$\n\\|u\\|_{B^{0}_{2,q}} \\;\\propto\\; \\left(\\sum_{j=1}^{J} \\left\\| w_{j} \\right\\|_{2}^{q}\\right)^{1/q}\n\\quad \\text{for } q=1,\n\\qquad\n\\|u\\|_{B^{0}_{2,\\infty}} \\;\\propto\\; \\max_{1 \\le j \\le J} \\left\\| w_{j} \\right\\|_{2},\n$$\n其中 $w = W u$ 且 $\\|\\cdot\\|_{2}$ 表示欧几里得范数。考虑由以下模型生成的带噪观测值 $y \\in \\mathbb{R}^{N}$\n$$\ny \\;=\\; u_{\\text{true}} \\;+\\; \\eta, \\quad \\eta \\sim \\mathcal{N}(0, \\sigma^{2} I_{N}),\n$$\n其中 $\\sigma = 0.05$。对于给定的正则化强度 $\\lambda = 200$，将每个 $q \\in \\{1,\\infty\\}$ 的最大后验（MAP）估计量定义为以下凸优化问题的解\n$$\n\\widehat{u}_{q} \\in \\arg\\min_{u \\in \\mathbb{R}^{N}} \\; \\frac{1}{2\\sigma^{2}} \\|u - y\\|_{2}^{2} \\;+\\; \\lambda \\left(\\sum_{j=1}^{J} \\| (W u)_{j} \\|_{2}^{q}\\right)^{1/q}\n\\quad \\text{for } q=1,\n$$\n和\n$$\n\\widehat{u}_{\\infty} \\in \\arg\\min_{u \\in \\mathbb{R}^{N}} \\; \\frac{1}{2\\sigma^{2}} \\|u - y\\|_{2}^{2} \\;+\\; \\lambda \\max_{1 \\le j \\le J} \\| (W u)_{j} \\|_{2}.\n$$\n假设哈尔变换 $W$ 是严格标准正交的，并且惩罚项不包括最终的尺度系数（仅惩罚细节系数）。\n\n构建一个包含三个合成信号 $u_{\\text{true}}$ 的测试套件，用以探究尺度间的行为：\n\n- 测试用例1（双尺度混合）：定义一个粗尺度阶跃和细尺度尖峰。令 $u_{\\text{true}}[n]=1$ 对 $n \\in \\{64,65,\\dots,191\\}$ 成立，其他情况下 $u_{\\text{true}}[n]=0$。在此基础上叠加两个细尺度尖峰，一个在索引 $120$ 处振幅为 $0.7$，另一个在索引 $200$ 处振幅为 $-0.5$。\n- 测试用例2（仅粗尺度）：与测试1中相同的粗尺度阶跃，但没有任何尖峰。\n- 测试用例3（仅细尺度）：无粗尺度阶跃。两个细尺度尖峰：一个在索引 $50$ 处振幅为 $1.0$，另一个在索引 $180$ 处振幅为 $-0.8$。\n\n对于所有测试，使用固定的随机种子 $r_{0}=12345$ 生成标准差为 $\\sigma=0.05$ 的独立噪声 $\\eta$，以确保可复现性，每个测试用例使用一次新的噪声实现。\n\n对于每个测试用例，计算以下定量输出来比较 $q=1$ 与 $q=\\infty$ 的情况：\n\n1. 重建误差比 $R = \\text{MSE}_{\\infty} / \\text{MSE}_{1}$，其中 $\\text{MSE}_{q} = \\| \\widehat{u}_{q} - u_{\\text{true}} \\|_{2}^{2}/N$。\n2. 尺度间收缩分布比 $D = \\Delta_{\\infty} / \\Delta_{1}$，其中对于每个 $q$，分布 $\\Delta_{q}$ 根据每个尺度上小波域的收缩因子计算得出，\n$$\nr^{(q)}_{j} \\;=\\; \\frac{\\| \\widehat{w}^{(q)}_{j} \\|_{2}}{\\| w^{(y)}_{j} \\|_{2} + \\epsilon},\n\\quad \\epsilon = 10^{-12},\n$$\n其中 $w^{(y)} = W y$，$\\widehat{w}^{(q)} = W \\widehat{u}_{q}$，且\n$$\n\\Delta_{q} \\;=\\; \\max_{j} r^{(q)}_{j} \\;-\\; \\min_{j} r^{(q)}_{j}.\n$$\n\n程序要求：\n\n- 为 $N=256$ 且 $J=\\log_{2}(N)$ 个层级实现标准正交哈尔小波变换及其逆变换。在目标函数中仅惩罚细节系数；不惩罚最终的尺度系数。\n- 使用凸分析方法精确求解 $q=1$ 和 $q=\\infty$ 两种情况下的 MAP 问题，以达到全局最优。不得依赖任何外部数据或输入；固定 $\\sigma=0.05$，$\\lambda=200$，$N=256$ 以及随机种子 $r_{0}=12345$。\n- 对于 $q=1$、$p=2$ 和 $s=0$ 的情况，使用按尺度对细节系数进行的分组。\n- 对于 $q=\\infty$ 的情况，强制执行对分组 $\\ell_{2}$ 范数在所有尺度上的最大值约束。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，顺序如下\n$$\n[ R_{1}, D_{1}, R_{2}, D_{2}, R_{3}, D_{3} ],\n$$\n其中下标表示测试用例编号。将所有六个数字表示为浮点值。此问题不涉及物理单位、角度或百分比。这些值应以至少六位有效数字的精度打印。",
            "solution": "用户提供的问题是计算逆问题领域中一个适定且有科学依据的任务。它要求比较两种基于贝索夫空间先验的正则化方案在信号去噪中的效果。该问题是有效的，因为它是自洽的、数学上一致的，并且可以使用凸优化和小波分析的既定原理通过算法求解。\n\n问题的核心在于求解两个最大后验（MAP）估计问题，这两个问题被表述为凸优化任务。未知信号表示为 $u \\in \\mathbb{R}^{N}$，给定带噪观测值 $y = u_{\\text{true}} + \\eta$，其中 $\\eta$ 是高斯噪声。为获得估计值 $\\widehat{u}$ 而需要最小化的目标函数是：\n$$\nJ(u) = \\frac{1}{2\\sigma^{2}} \\|u - y\\|_{2}^{2} + \\lambda \\mathcal{R}(Wu)\n$$\n此处，$\\|u - y\\|_{2}^{2}$ 是数据保真项（对数似然），$\\mathcal{R}(Wu)$ 是正则化项（对数先验），$\\lambda$ 是正则化参数，而 $W$ 是标准正交哈尔小波变换。\n\n解决此问题的一个关键原则是使用标准正交小波变换 $W$。由于 $W$ 是一个标准正交算子（$W W^T = W^T W = I$），欧几里得范数在此变换下保持不变，即对任意向量 $v$ 都有 $\\|v\\|_{2} = \\|Wv\\|_{2}$。通过变量替换 $w = Wu$ 和 $w_y = Wy$ 将问题转换到小波域，数据保真项变为 $\\|u - y\\|_{2}^{2} = \\|W^T w - W^T w_y\\|_{2}^{2} = \\|W^T(w - w_y)\\|_{2}^{2} = \\|w - w_y\\|_{2}^{2}$。这样，优化问题就可以在小波域中更容易地求解：\n$$\n\\widehat{w} \\in \\arg\\min_{w \\in \\mathbb{R}^{N}} \\; \\frac{1}{2\\sigma^{2}} \\|w - w_y\\|_{2}^{2} + \\lambda \\mathcal{R}(w)\n$$\n原始域中的估计信号通过逆小波变换恢复，即 $\\widehat{u} = W^T \\widehat{w}$。\n\n小波系数 $w$ 由一个尺度系数 $w_c$ 和 $J=\\log_2(N)$ 组细节系数 $\\{w_j\\}_{j=1}^{J}$ 组成。问题规定正则化项 $\\mathcal{R}(w)$ 仅惩罚细节系数。这种结构允许将尺度系数和细节系数的估计解耦。\n- 尺度系数 $w_c$ 不受惩罚，因此其估计值就是最小化数据保真项的值：$\\widehat{w}_c = w_{y,c}$。\n- 细节系数通过求解优化问题的剩余部分来估计。\n\n贝索夫先验 $\\|u\\|_{B^0_{2,q}}$ 中的参数 $q$ 的两种情况定义了惩罚项 $\\mathcal{R}(w)$ 的不同结构，从而导致不同的解。\n\n**情况1：$q=1$（组稀疏先验）**\n正则化项为 $\\mathcal{R}(w) = \\sum_{j=1}^{J} \\|w_j\\|_2$。针对细节系数的优化问题变为：\n$$\n\\min_{\\{w_j\\}_{j=1}^J} \\sum_{j=1}^{J} \\left( \\frac{1}{2\\sigma^2} \\|w_j - w_{y,j}\\|_2^2 + \\lambda \\|w_j\\|_2 \\right)\n$$\n这个问题是完全可分的，意味着我们可以对每个尺度 $j$ 的细节系数向量 $w_j$ 进行独立求解：\n$$\n\\widehat{w}_j = \\arg\\min_{w_j} \\frac{1}{2\\sigma^2} \\|w_j - w_{y,j}\\|_2^2 + \\lambda \\|w_j\\|_2\n$$\n这是一个标准问题，其解由组软阈值算子给出。每个组 $w_j$ 的解是：\n$$\n\\widehat{w}_{1,j} = \\left(1 - \\frac{\\lambda \\sigma^2}{\\|w_{y,j}\\|_2}\\right)_+ w_{y,j}\n$$\n其中 $(x)_+ = \\max(x, 0)$。该算子将整个向量 $w_{y,j}$ 向零收缩，如果其范数低于阈值 $\\alpha = \\lambda \\sigma^2$，则将其设为零。\n\n**情况2：$q=\\infty$（一致收缩先验）**\n正则化项为 $\\mathcal{R}(w) = \\max_{1 \\le j \\le J} \\|w_j\\|_2$。优化问题为：\n$$\n\\min_{\\{w_j\\}_{j=1}^J} \\frac{1}{2\\sigma^2} \\sum_{j=1}^{J} \\|w_j - w_{y,j}\\|_2^2 + \\lambda \\max_{1 \\le j \\le J} \\|w_j\\|_2\n$$\n这个问题不能按尺度分离。可以通过引入辅助变量 $t = \\max_j \\|w_j\\|_2$ 将其表述为一个约束优化问题：\n$$\n\\min_{\\{w_j\\}, t \\ge 0} \\frac{1}{2\\sigma^2} \\sum_{j=1}^{J} \\|w_j - w_{y,j}\\|_2^2 + \\lambda t \\quad \\text{subject to} \\quad \\|w_j\\|_2 \\le t \\quad \\forall j=1,\\dots,J.\n$$\n对于一个固定的 $t$，最优的 $w_j$ 是 $w_{y,j}$ 在半径为 $t$ 的欧几里得球上的投影。将其代回目标函数，得到一个关于 $t$ 的一维凸优化问题。该目标函数对 $t$ 的导数给出了条件 $\\sum_{j=1}^J (\\|w_{y,j}\\|_2 - t)_+ = \\lambda \\sigma^2$。这个关于最优阈值 $t_{opt}$ 的方程可以被高效求解。函数 $h(t) = \\sum_{j=1}^J (\\|w_{y,j}\\|_2 - t)_+ - \\lambda \\sigma^2$ 是连续且单调递减的，因此它的根可以通过二分法搜索找到。一旦找到 $t_{opt}$，估计的细节系数由下式给出：\n$$\n\\widehat{w}_{\\infty,j} = \\min\\left(1, \\frac{t_{opt}}{\\|w_{y,j}\\|_2}\\right) w_{y,j}\n$$\n该算子为所有尺度上估计的细节系数的范数提供了一个统一的上界 $t_{opt}$。\n\n实现过程将首先定义标准正交哈尔小波变换及其逆变换。然后，对于每个测试用例，生成带噪数据，并使用上述推导的方法在小波域中求解两个 MAP 问题。最后，根据结果计算所需的定量指标（$R$ 和 $D$）。",
            "answer": "```python\nimport numpy as np\n\ndef haar_dwt(x):\n    \"\"\"\n    Computes the orthonormal Haar wavelet transform of a 1D signal.\n    \n    Args:\n        x (np.ndarray): Input signal of length 2^k.\n    \n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The final scaling coefficient (length 1).\n            - list[np.ndarray]: A list of detail coefficients, ordered from\n              finest scale (j=1) to coarsest (j=J).\n    \"\"\"\n    level = int(np.log2(len(x)))\n    a = np.copy(x).astype(float)\n    detail_coeffs_by_scale = []\n    for _ in range(level):\n        d = (a[0::2] - a[1::2]) / np.sqrt(2)\n        a = (a[0::2] + a[1::2]) / np.sqrt(2)\n        detail_coeffs_by_scale.append(d)\n    return a, detail_coeffs_by_scale\n\ndef haar_idwt(scaling_coeffs, detail_coeffs_by_scale):\n    \"\"\"\n    Computes the inverse orthonormal Haar wavelet transform.\n    \n    Args:\n        scaling_coeffs (np.ndarray): The final scaling coefficient.\n        detail_coeffs_by_scale (list[np.ndarray]): List of detail coefficients\n            from finest to coarsest scale.\n            \n    Returns:\n        np.ndarray: The reconstructed signal.\n    \"\"\"\n    level = len(detail_coeffs_by_scale)\n    a = np.copy(scaling_coeffs).astype(float)\n    for i in range(level - 1, -1, -1):\n        d = detail_coeffs_by_scale[i]\n        signal_len = len(d)\n        new_a = np.zeros(2 * signal_len, dtype=float)\n        a_plus_d = (a + d) / np.sqrt(2)\n        a_minus_d = (a - d) / np.sqrt(2)\n        new_a[0::2] = a_plus_d\n        new_a[1::2] = a_minus_d\n        a = new_a\n    return a\n\ndef find_t_opt_for_q_inf(w_y_d_list, alpha):\n    \"\"\"\n    Finds the optimal threshold t for the q=inf case by solving\n    sum_j (||w_{y,j}|| - t)_+ = alpha using bisection.\n    \"\"\"\n    z_norms = np.array([np.linalg.norm(d) for d in w_y_d_list])\n\n    def h(t, norms, alpha_val):\n        return np.sum(np.maximum(0, norms - t)) - alpha_val\n\n    if h(0, z_norms, alpha) = 0:\n        return 0.0\n\n    low_t = 0.0\n    # A safe upper bound for t is the maximum norm, as h(t) would be = 0.\n    high_t = np.max(z_norms) \n    \n    for _ in range(100): # 100 iterations for high precision\n        mid_t = (low_t + high_t) / 2.0\n        if mid_t == low_t or mid_t == high_t: # Converged\n            break\n        if h(mid_t, z_norms, alpha) > 0:\n            low_t = mid_t\n        else:\n            high_t = mid_t\n            \n    return (low_t + high_t) / 2.0\n\ndef solve():\n    \"\"\"\n    Main solver function to run the specified analysis.\n    \"\"\"\n    N = 256\n    sigma = 0.05\n    lambda_reg = 200.0\n    epsilon = 1e-12\n    J = int(np.log2(N))\n\n    # Test cases setup\n    u_true_cases = []\n    # Case 1: two-scale mixture\n    u1 = np.zeros(N, dtype=float)\n    u1[64:192] = 1.0\n    u1[120] += 0.7\n    u1[200] += -0.5\n    u_true_cases.append(u1)\n    # Case 2: coarse-only\n    u2 = np.zeros(N, dtype=float)\n    u2[64:192] = 1.0\n    u_true_cases.append(u2)\n    # Case 3: fine-only\n    u3 = np.zeros(N, dtype=float)\n    u3[50] = 1.0\n    u3[180] = -0.8\n    u_true_cases.append(u3)\n\n    rng = np.random.default_rng(12345)\n    final_results = []\n    \n    for u_true in u_true_cases:\n        noise = rng.normal(loc=0.0, scale=sigma, size=N)\n        y = u_true + noise\n\n        # Decompose noisy signal into wavelet coefficients\n        w_y_c, w_y_d_list = haar_dwt(y)\n        \n        # --- Solve for q=1 ---\n        alpha = lambda_reg * sigma**2\n        w_hat_1_d_list = []\n        for w_y_j in w_y_d_list:\n            norm_w_y_j = np.linalg.norm(w_y_j)\n            shrinkage = max(0.0, 1.0 - alpha / (norm_w_y_j + epsilon))\n            w_hat_1_d_list.append(shrinkage * w_y_j)\n        u_hat_1 = haar_idwt(w_y_c, w_hat_1_d_list)\n\n        # --- Solve for q=inf ---\n        t_opt = find_t_opt_for_q_inf(w_y_d_list, alpha)\n        w_hat_inf_d_list = []\n        for w_y_j in w_y_d_list:\n            norm_w_y_j = np.linalg.norm(w_y_j)\n            shrinkage = min(1.0, t_opt / (norm_w_y_j + epsilon))\n            w_hat_inf_d_list.append(shrinkage * w_y_j)\n        u_hat_inf = haar_idwt(w_y_c, w_hat_inf_d_list)\n\n        # --- Calculate metrics ---\n        # 1. Reconstruction error ratio R\n        mse_1 = np.mean((u_hat_1 - u_true)**2)\n        mse_inf = np.mean((u_hat_inf - u_true)**2)\n        R = mse_inf / (mse_1 + epsilon)\n\n        # 2. Inter-scale shrinkage spread ratio D\n        r1_j_vals = []\n        rinf_j_vals = []\n        for j in range(J):\n            norm_wyj = np.linalg.norm(w_y_d_list[j])\n            \n            norm_w1j_hat = np.linalg.norm(w_hat_1_d_list[j])\n            r1_j_vals.append(norm_w1j_hat / (norm_wyj + epsilon))\n\n            norm_winfj_hat = np.linalg.norm(w_hat_inf_d_list[j])\n            rinf_j_vals.append(norm_winfj_hat / (norm_wyj + epsilon))\n\n        delta_1 = np.max(r1_j_vals) - np.min(r1_j_vals)\n        delta_inf = np.max(rinf_j_vals) - np.min(rinf_j_vals)\n        D = delta_inf / (delta_1 + epsilon)\n        \n        final_results.extend([R, D])\n\n    # Format the final output string exactly as required\n    output_str = \",\".join([f\"{val:.10f}\" for val in final_results])\n    print(f\"[{output_str}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "我们已经理解了Besov先验的结构并对其行为建立了一些直觉，现在让我们看看它们如何在一个大规模、先进的应用中被部署。许多现实世界的反问题需要复杂的优化算法来求解。这个最终的实践问题将挑战你将Besov空间先验整合到近端梯度法中，这是一种用于非光滑优化的强大算法。通过推导四维变分（4D-Var）数据同化问题的更新步骤，你将在惩罚泛函与在高维环境中寻找稀疏解的实用算法之间架起一座桥梁。",
            "id": "3367744",
            "problem": "考虑一个四维变分 (4D-Var) 数据同化问题，该问题是针对初始条件 $x \\in \\mathbb{R}^{n}$ 在一个包含观测时间 $\\{t_{i}\\}_{i=1}^{m}$ 的同化窗口上提出的。令从初始时间到时间 $t_{i}$ 的线性化模型传播算子为 $M_{0 \\to t_{i}} \\in \\mathbb{R}^{n \\times n}$，在时间 $t_{i}$ 的线性化观测算子为 $H_{t_{i}} \\in \\mathbb{R}^{p_{i} \\times n}$。时间 $t_{i}$ 的观测值为 $y_{t_{i}} \\in \\mathbb{R}^{p_{i}}$，其观测误差协方差 $R_{t_{i}} \\in \\mathbb{R}^{p_{i} \\times p_{i}}$ 是正定的。初始条件的高斯背景（先验）给定，其均值为 $x_{b} \\in \\mathbb{R}^{n}$，协方差 $B \\in \\mathbb{R}^{n \\times n}$ 是正定的。此外，假设在小波系数上有一个促进稀疏性的 Besov 空间先验：令 $W \\in \\mathbb{R}^{n \\times n}$ 为一个标准正交小波变换矩阵，$w \\in \\mathbb{R}^{n}$ 为一个正权重向量，并考虑加权 $\\ell^{1}$ 惩罚项 $\\lambda \\sum_{j=1}^{n} w_{j} |(W x)_{j}|$，其中 $\\lambda  0$。\n\n初始条件的增量 4D-Var 目标函数是一个光滑的数据失配加背景项与一个非光滑的加权 $\\ell^{1}$ 小波惩罚项之和：\n$$\nJ(x) \\;=\\; \\frac{1}{2}\\,(x - x_{b})^{\\top} B^{-1} (x - x_{b}) \\;+\\; \\frac{1}{2} \\sum_{i=1}^{m} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr)^{\\top} R_{t_{i}}^{-1} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr) \\;+\\; \\lambda \\sum_{j=1}^{n} w_{j} \\bigl|(W x)_{j}\\bigr| .\n$$\n\n从高斯似然、二次背景和可分离加权 $\\ell^{1}$ 惩罚下的最大后验估计的第一性原理出发，推导从当前迭代点 $x^{(k)}$ 到下一个迭代点 $x^{(k+1)}$ 的单次近端梯度 Gauss–Newton 更新的显式解析表达式。在整个窗口上使用 Gauss–Newton 线性化（即，在第 $k$ 次迭代中将 $M_{0 \\to t_{i}}$ 和 $H_{t_{i}}$ 视为固定的线性算子），从而使光滑部分的梯度有一个 Lipschitz 常数，该常数受 Gauss–Newton 矩阵的谱范数限制。用 $\\alpha_{k}  0$ 表示步长，其选择为第 $k$ 次迭代时此 Lipschitz 常数的任何有效全局上界的倒数。用 $B$, $x_{b}$, $\\{M_{0 \\to t_{i}}, H_{t_{i}}, R_{t_{i}}, y_{t_{i}}\\}_{i=1}^{m}$, $\\lambda$, $w$ 和 $W$ 来表示你的最终更新表达式，并使用由加权 $\\ell^{1}$ 惩罚和 $W$ 的标准正交性导出的分量软阈值法。\n\n你的最终答案必须是关于 $x^{(k+1)}$ 的单个闭式解析表达式。不要提供不等式或中间方程。不要包含任何单位。如果在推导过程中引入任何辅助符号，则必须在最终表达式中将其完全消除。",
            "solution": "该问题要求为一个包含促进稀疏性的 Besov 空间先验的增量四维变分 (4D-Var) 数据同化问题推导单步更新。该更新将使用近端梯度法执行。\n\n首先，我们验证问题陈述的有效性。\n给定的量有：\n- 状态向量 $x \\in \\mathbb{R}^{n}$。\n- 包含观测时间 $\\{t_{i}\\}_{i=1}^{m}$ 的同化窗口。\n- 线性化模型传播算子 $M_{0 \\to t_{i}} \\in \\mathbb{R}^{n \\times n}$。\n- 线性化观测算子 $H_{t_{i}} \\in \\mathbb{R}^{p_{i} \\times n}$。\n- 观测向量 $y_{t_{i}} \\in \\mathbb{R}^{p_{i}}$。\n- 正定观测误差协方差矩阵 $R_{t_{i}} \\in \\mathbb{R}^{p_{i} \\times p_{i}}$。\n- 背景（先验）平均状态 $x_{b} \\in \\mathbb{R}^{n}$。\n- 正定背景误差协方差矩阵 $B \\in \\mathbb{R}^{n \\times n}$。\n- 标准正交小波变换矩阵 $W \\in \\mathbb{R}^{n \\times n}$。\n- 正权重向量 $w \\in \\mathbb{R}^{n}$。\n- 正则化参数 $\\lambda  0$。\n- 待最小化的目标函数：\n$$\nJ(x) \\;=\\; \\frac{1}{2}\\,(x - x_{b})^{\\top} B^{-1} (x - x_{b}) \\;+\\; \\frac{1}{2} \\sum_{i=1}^{m} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr)^{\\top} R_{t_{i}}^{-1} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr) \\;+\\; \\lambda \\sum_{j=1}^{n} w_{j} \\bigl|(W x)_{j}\\bigr| .\n$$\n- 一个迭代点 $x^{(k)}$ 和一个步长 $\\alpha_k$。\n\n该问题具有科学依据，代表了高等数据同化中的一个标准提法。该问题是适定的，因为目标函数 $J(x)$ 是严格凸的（由于矩阵 $B$ 是正定的），因此有唯一的最小化子。所有项在数学上和物理上都是一致的。因此，该问题是有效的。\n\n目标函数 $J(x)$ 可以从最大后验 (MAP) 估计的角度来解释。最小化 $J(x)$ 的解 $x$ 是在给定观测值条件下最大化后验概率密度的状态。根据 Bayes 定理，后验概率 $p(x|y)$ 由 $p(x|y) \\propto p(y|x) p(x)$ 给出，其中 $p(y|x)$ 是似然， $p(x)$ 是状态的先验概率。最小化 $J(x)$ 等价于最小化后验概率的负对数。各个项对应于：\n$1$. 项 $\\frac{1}{2} \\sum_{i=1}^{m} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr)^{\\top} R_{t_{i}}^{-1} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr)$\n是负对数似然，假设观测误差是均值为零、协方差为 $R_{t_{i}}$ 的独立高斯随机变量。\n$2$. 项 $\\frac{1}{2}\\,(x - x_{b})^{\\top} B^{-1} (x - x_{b})$ 是状态 $x$ 的高斯先验的负对数，其均值为 $x_b$，协方差为 $B$。\n$3$. 项 $\\lambda \\sum_{j=1}^{n} w_{j} \\bigl|(W x)_{j}\\bigr|$ 是小波系数 $(Wx)_j$ 上的可分离 Laplace（或指数）先验的负对数，这会促进稀疏性。\n\n目标函数具有复合结构 $J(x) = f(x) + g(x)$，其中 $f(x)$ 是一个光滑、可微的凸函数，而 $g(x)$ 是一个凸的、不可微的函数。\n光滑部分是：\n$$\nf(x) = \\frac{1}{2}\\,(x - x_{b})^{\\top} B^{-1} (x - x_{b}) + \\frac{1}{2} \\sum_{i=1}^{m} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr)^{\\top} R_{t_{i}}^{-1} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr).\n$$\n非光滑部分是加权 $\\ell^{1}$ 惩罚项：\n$$\ng(x) = \\lambda \\sum_{j=1}^{n} w_{j} \\bigl|(W x)_{j}\\bigr| = \\lambda \\| \\text{diag}(w) W x \\|_1.\n$$\n近端梯度法适用于最小化此类复合函数。从迭代点 $x^{(k)}$到 $x^{(k+1)}$ 的更新由下式给出：\n$$\nx^{(k+1)} = \\text{prox}_{\\alpha_k g}\\left(x^{(k)} - \\alpha_k \\nabla f(x^{(k)})\\right),\n$$\n其中 $\\alpha_k$ 是步长，$\\text{prox}_{\\alpha_k g}$ 是函数 $\\alpha_k g$ 的近端算子。\n\n首先，我们计算光滑部分 $\\nabla f(x)$ 的梯度。\n背景项的梯度是 $\\nabla_x \\left( \\frac{1}{2}(x - x_b)^{\\top}B^{-1}(x - x_b) \\right) = B^{-1}(x - x_b)$。\n观测项的梯度是 $\\nabla_x \\left( \\frac{1}{2} \\sum_{i=1}^{m} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr)^{\\top} R_{t_{i}}^{-1} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr) \\right) = \\sum_{i=1}^{m} (H_{t_{i}} M_{0 \\to t_{i}})^{\\top} R_{t_{i}}^{-1} (H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}})$。\n将它们结合起来，$f(x)$ 的完整梯度是：\n$$\n\\nabla f(x) = B^{-1}(x - x_b) + \\sum_{i=1}^{m} M_{0 \\to t_{i}}^{\\top} H_{t_{i}}^{\\top} R_{t_{i}}^{-1} (H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}).\n$$\n问题陈述指出，步长 $\\alpha_k$ 是基于 $\\nabla f(x)$ 的 Lipschitz 常数选择的，该常数由 Hessian 矩阵 $\\nabla^2 f(x)$ 的谱范数给出。由于 $f(x)$ 是关于 $x$ 的二次函数，其 Hessian 矩阵是常数：\n$$\n\\nabla^2 f(x) = B^{-1} + \\sum_{i=1}^{m} M_{0 \\to t_{i}}^{\\top} H_{t_{i}}^{\\top} R_{t_{i}}^{-1} H_{t_{i}} M_{0 \\to t_{i}}.\n$$\n这就是问题中提到的 Gauss-Newton 矩阵。步长 $\\alpha_k$ 是其谱范数上界的倒数。\n\n接下来，我们推导近端算子 $\\text{prox}_{\\alpha_k g}(z)$，其定义为：\n$$\n\\text{prox}_{\\alpha_k g}(z) = \\underset{u \\in \\mathbb{R}^n}{\\arg\\min} \\left( \\alpha_k g(u) + \\frac{1}{2} \\|u - z\\|_2^2 \\right).\n$$\n代入 $g(u) = \\lambda \\sum_{j=1}^{n} w_{j} |(W u)_{j}|$，最小化问题变为：\n$$\n\\underset{u \\in \\mathbb{R}^n}{\\arg\\min} \\left( \\alpha_k \\lambda \\sum_{j=1}^{n} w_{j} |(W u)_{j}| + \\frac{1}{2} \\|u - z\\|_2^2 \\right).\n$$\n我们使用小波变换矩阵 $W$ 是标准正交的（$W^{\\top}W = WW^{\\top} = I$）这一性质。这意味着 $W$ 是一个欧几里得等距变换，即对于任何向量 $v$ 都有 $\\|v\\|_2 = \\|Wv\\|_2$。\n令 $\\theta = Wu$。那么 $u = W^{\\top}\\theta$。项 $\\|u - z\\|_2^2$ 可以重写为 $\\|W(u-z)\\|_2^2 = \\|Wu - Wz\\|_2^2 = \\|\\theta - Wz\\|_2^2$。\n最小化问题可以用小波系数 $\\theta$ 来重新表述：\n$$\n\\underset{\\theta \\in \\mathbb{R}^n}{\\arg\\min} \\left( \\alpha_k \\lambda \\sum_{j=1}^{n} w_{j} |\\theta_{j}| + \\frac{1}{2} \\|\\theta - Wz\\|_2^2 \\right).\n$$\n这个问题是可分离的，意味着我们可以对每个分量 $\\theta_j$ 独立求解：\n$$\n\\underset{\\theta_j \\in \\mathbb{R}}{\\arg\\min} \\left( \\alpha_k \\lambda w_{j} |\\theta_{j}| + \\frac{1}{2} (\\theta_j - (Wz)_j)^2 \\right).\n$$\n这个标量问题的解由软阈值算子给出，我们记为 $S$。最优的 $\\theta_j$ 是：\n$$\n\\theta_j^* = S_{\\alpha_k \\lambda w_j}((Wz)_j) = \\text{sign}((Wz)_j) \\max(0, |(Wz)_j| - \\alpha_k \\lambda w_j).\n$$\n以向量形式表示，$\\theta$ 的解是 $\\theta^* = S_{\\alpha_k \\lambda w}(Wz)$，其中算子 $S$ 以阈值向量 $\\alpha_k \\lambda w$ 逐分量地作用。\n为了找到 $u$ 的解，我们进行逆变换：$u^* = W^{\\top}\\theta^* = W^{\\top}S_{\\alpha_k \\lambda w}(Wz)$。\n所以，近端算子是：\n$$\n\\text{prox}_{\\alpha_k g}(z) = W^{\\top}S_{\\alpha_k \\lambda w}(Wz).\n$$\n现在，我们可以写出完整的近端梯度更新步骤。近端算子的参数是 $z = x^{(k)} - \\alpha_k \\nabla f(x^{(k)})$。\n代入 $\\nabla f(x^{(k)})$ 的表达式，我们得到：\n$$\nz = x^{(k)} - \\alpha_k \\left( B^{-1}(x^{(k)} - x_b) + \\sum_{i=1}^{m} M_{0 \\to t_{i}}^{\\top} H_{t_{i}}^{\\top} R_{t_{i}}^{-1} (H_{t_{i}} M_{0 \\to t_{i}} x^{(k)} - y_{t_{i}}) \\right).\n$$\n那么 $x^{(k+1)}$ 的更新就是 $\\text{prox}_{\\alpha_k g}(z)$：\n$$\nx^{(k+1)} = W^{\\top}S_{\\alpha_k \\lambda w}\\left( W \\left[ x^{(k)} - \\alpha_k \\left( B^{-1}(x^{(k)} - x_b) + \\sum_{i=1}^{m} M_{0 \\to t_{i}}^{\\top} H_{t_{i}}^{\\top} R_{t_{i}}^{-1} (H_{t_{i}} M_{0 \\to t_{i}} x^{(k)} - y_{t_{i}}) \\right) \\right] \\right).\n$$\n这就是从当前迭代点 $x^{(k)}$ 到下一个迭代点 $x^{(k+1)}$ 的单次近端梯度更新的显式解析表达式，满足问题陈述的所有条件。软阈值算子 $S_{\\tau}(v)$ 被理解为分量函数 $(S_{\\tau}(v))_j = \\text{sign}(v_j) \\max(0, |v_j|-\\tau_j)$。",
            "answer": "$$\n\\boxed{\nx^{(k+1)} = W^{\\top} S_{\\alpha_k \\lambda w}\\left( W \\left( x^{(k)} - \\alpha_k \\left[ B^{-1}(x^{(k)} - x_b) + \\sum_{i=1}^{m} M_{0 \\to t_i}^{\\top} H_{t_i}^{\\top} R_{t_i}^{-1} (H_{t_i} M_{0 \\to t_i} x^{(k)} - y_{t_i}) \\right] \\right) \\right)\n}\n$$"
        }
    ]
}