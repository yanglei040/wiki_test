{
    "hands_on_practices": [
        {
            "introduction": "Before using a prior in an inverse problem, it is essential to understand the properties, such as smoothness, that it imposes on the unknown function. This exercise  delves into this by asking you to characterize the space of functions that are 'plausible' under a Matérn-type SPDE prior. By deriving the prior's Cameron–Martin space, you will establish a fundamental link between the operator defining the SPDE and the regularity of the function, connecting it directly to the theory of Sobolev spaces.",
            "id": "3377254",
            "problem": "Consider a bounded, connected domain $\\Omega \\subset \\mathbb{R}^{d}$ with $\\mathcal{C}^{\\infty}$ boundary, and let $-\\Delta$ denote the negative Laplacian with homogeneous Neumann boundary conditions on $\\partial \\Omega$. The operator $-\\Delta$ is self-adjoint and nonnegative on $L^{2}(\\Omega)$, with a complete orthonormal basis of eigenfunctions $\\{e_{j}\\}_{j=0}^{\\infty}$ and corresponding eigenvalues $\\{\\lambda_{j}\\}_{j=0}^{\\infty}$ satisfying $0=\\lambda_{0}\\leq \\lambda_{1}\\leq \\lambda_{2}\\leq \\cdots$, where $e_{0}$ is constant on $\\Omega$. Fix $\\kappa0$ and $\\alpha0$, and define the positive self-adjoint operator $(\\kappa^{2}-\\Delta)$ on $L^{2}(\\Omega)$ via spectral calculus, so that for $s \\in \\mathbb{R}$,\n$$\n(\\kappa^{2}-\\Delta)^{s} e_{j} = (\\kappa^{2}+\\lambda_{j})^{s} e_{j}, \\quad j=0,1,2,\\dots.\n$$\nConsider the centered Gaussian prior on $L^{2}(\\Omega)$ defined as the solution $u$ of the Stochastic Partial Differential Equation (SPDE) $(\\kappa^{2}-\\Delta)^{\\alpha/2} u = W$, where $W$ is Gaussian white noise on $\\Omega$ (zero mean, covariance given by the identity operator on $L^{2}(\\Omega)$). This prior is a Matérn-type Gaussian field with covariance operator $C=(\\kappa^{2}-\\Delta)^{-\\alpha}$.\n\nUsing the definition of the Cameron–Martin space of a centered Gaussian measure with covariance operator $C$—namely, that the Cameron–Martin space is the range of $C^{1/2}$ endowed with the inner product induced by $C^{-1/2}$—derive the explicit form of the Cameron–Martin space for this prior and relate it to the Sobolev space $H^{\\alpha}(\\Omega)$ (also called the Bessel potential space of order $\\alpha$). In your derivation, work from first principles:\n- Use the spectral decomposition of the Neumann Laplacian and the functional calculus for self-adjoint operators.\n- Use the definition of the Cameron–Martin space of a Gaussian measure on a Hilbert space and its norm.\n- Use the equivalence of Bessel potential norms and spectral characterizations of Sobolev spaces on bounded domains with smooth boundary.\n\nYour final answer must be a single closed-form analytic expression for the Cameron–Martin norm squared of an element $u \\in L^{2}(\\Omega)$ written in terms of the Neumann eigenpairs $\\{(\\lambda_{j},e_{j})\\}$ and the expansion coefficients $u_{j}=\\langle u,e_{j}\\rangle_{L^{2}(\\Omega)}$. Do not include any equations or inequalities in the final answer; provide only the expression. No numerical rounding is required. If you introduce any acronyms, define them on first use (for example, Stochastic Partial Differential Equation (SPDE), Reproducing Kernel Hilbert Space (RKHS)).",
            "solution": "The problem statement is found to be valid as it is scientifically grounded, well-posed, and objective. It provides a complete and consistent setup within the established mathematical framework of functional analysis and stochastic partial differential equations for defining Gaussian priors.\n\nThe objective is to derive the Cameron–Martin norm for a centered Gaussian measure on $L^{2}(\\Omega)$ whose covariance operator is $C = (\\kappa^{2}-\\Delta)^{-\\alpha}$. Here, $-\\Delta$ is the negative Laplacian with homogeneous Neumann boundary conditions on a smooth, bounded, connected domain $\\Omega \\subset \\mathbb{R}^{d}$. The parameters $\\kappa$ and $\\alpha$ are strictly positive real numbers.\n\nLet $\\{e_{j}\\}_{j=0}^{\\infty}$ be the complete orthonormal basis of eigenfunctions of $-\\Delta$ in $L^{2}(\\Omega)$, with corresponding eigenvalues $0=\\lambda_{0}\\leq \\lambda_{1}\\leq \\lambda_{2}\\leq \\cdots$.\nThe problem states that the operator $(\\kappa^{2}-\\Delta)^{s}$ for any $s \\in \\mathbb{R}$ is defined via its action on the eigenfunctions:\n$$\n(\\kappa^{2}-\\Delta)^{s} e_{j} = (\\kappa^{2}+\\lambda_{j})^{s} e_{j}\n$$\nThe covariance operator is given as $C = (\\kappa^{2}-\\Delta)^{-\\alpha}$. Its action on an eigenfunction is:\n$$\nC e_{j} = (\\kappa^{2}+\\lambda_{j})^{-\\alpha} e_{j}\n$$\nAccording to the problem's definition, the Cameron–Martin space, which we denote as $\\mathcal{H}_{CM}$, is the range of the operator $C^{1/2}$. Using the same functional calculus, the operator $C^{1/2}$ is given by:\n$$\nC^{1/2} = \\left( (\\kappa^{2}-\\Delta)^{-\\alpha} \\right)^{1/2} = (\\kappa^{2}-\\Delta)^{-\\alpha/2}\n$$\nThe action of $C^{1/2}$ on an eigenfunction is:\n$$\nC^{1/2} e_{j} = (\\kappa^{2}+\\lambda_{j})^{-\\alpha/2} e_{j}\n$$\nAn element $u \\in L^{2}(\\Omega)$ belongs to the Cameron–Martin space $\\mathcal{H}_{CM}$ if and only if it is in the range of $C^{1/2}$. This means there must exist some element $v \\in L^{2}(\\Omega)$ such that $u = C^{1/2}v$.\n\nLet us expand both $u$ and $v$ in the eigenbasis $\\{e_{j}\\}$:\n$$\nu = \\sum_{j=0}^{\\infty} u_{j} e_{j}, \\quad \\text{where } u_{j} = \\langle u, e_{j} \\rangle_{L^{2}(\\Omega)}\n$$\n$$\nv = \\sum_{j=0}^{\\infty} v_{j} e_{j}, \\quad \\text{where } v_{j} = \\langle v, e_{j} \\rangle_{L^{2}(\\Omega)}\n$$\nApplying the operator $C^{1/2}$ to $v$ yields:\n$$\nu = C^{1/2}v = C^{1/2}\\left(\\sum_{j=0}^{\\infty} v_{j} e_{j}\\right) = \\sum_{j=0}^{\\infty} v_{j} \\left(C^{1/2}e_{j}\\right) = \\sum_{j=0}^{\\infty} v_{j} (\\kappa^{2}+\\lambda_{j})^{-\\alpha/2} e_{j}\n$$\nBy comparing the coefficients of the expansion for $u$, we find the relationship between the coefficients $u_{j}$ and $v_{j}$:\n$$\nu_{j} = v_{j} (\\kappa^{2}+\\lambda_{j})^{-\\alpha/2}\n$$\nThis implies that for a given $u$, the coefficients of the pre-image $v$ are $v_{j} = u_{j} (\\kappa^{2}+\\lambda_{j})^{\\alpha/2}$. For $u$ to be in $\\mathcal{H}_{CM}$, the corresponding $v$ must be an element of $L^{2}(\\Omega)$. A function is in $L^{2}(\\Omega)$ if and only if the sum of the squares of its Fourier coefficients is finite. Therefore, we must have:\n$$\n\\|v\\|_{L^{2}(\\Omega)}^{2} = \\sum_{j=0}^{\\infty} v_{j}^{2} = \\sum_{j=0}^{\\infty} \\left(u_{j} (\\kappa^{2}+\\lambda_{j})^{\\alpha/2}\\right)^{2} = \\sum_{j=0}^{\\infty} u_{j}^{2} (\\kappa^{2}+\\lambda_{j})^{\\alpha}  \\infty\n$$\nThis condition characterizes the elements $u$ that constitute the Cameron–Martin space $\\mathcal{H}_{CM}$.\n\nThe problem also specifies that the Cameron–Martin space is endowed with the inner product $\\langle u_1, u_2 \\rangle_{\\mathcal{H}_{CM}} = \\langle C^{-1/2}u_1, C^{-1/2}u_2 \\rangle_{L^{2}(\\Omega)}$. For any $u \\in \\mathcal{H}_{CM}$, we have $u = C^{1/2}v$ for some $v \\in L^2(\\Omega)$, which implies $v = C^{-1/2}u$. The Cameron–Martin norm squared of $u$ is then:\n$$\n\\|u\\|_{\\mathcal{H}_{CM}}^{2} = \\langle u, u \\rangle_{\\mathcal{H}_{CM}} = \\langle C^{-1/2}u, C^{-1/2}u \\rangle_{L^{2}(\\Omega)} = \\|C^{-1/2}u\\|_{L^{2}(\\Omega)}^{2} = \\|v\\|_{L^{2}(\\Omega)}^{2}\n$$\nUsing our previously derived expression for $\\|v\\|_{L^{2}(\\Omega)}^{2}$ in terms of the coefficients of $u$, we obtain the explicit form of the Cameron–Martin norm squared:\n$$\n\\|u\\|_{\\mathcal{H}_{CM}}^{2} = \\sum_{j=0}^{\\infty} u_{j}^{2} (\\kappa^{2}+\\lambda_{j})^{\\alpha}\n$$\nThis space is equivalent to the Sobolev space $H^{\\alpha}(\\Omega)$ (also known as a Bessel potential space). A standard characterization of the norm on $H^{\\alpha}(\\Omega)$ using the Neumann eigenpairs is:\n$$\n\\|u\\|_{H^{\\alpha}(\\Omega)}^{2} = \\sum_{j=0}^{\\infty} u_{j}^{2} (1+\\lambda_{j})^{\\alpha}\n$$\nSince $\\kappa0$ and $\\lambda_j \\ge 0$, there exist positive constants $c_{1}$ and $c_{2}$ such that $c_{1}(1+\\lambda_{j}) \\leq (\\kappa^{2}+\\lambda_{j}) \\leq c_{2}(1+\\lambda_{j})$ for all $j \\geq 0$. For instance, one can choose $c_{1} = \\min(1, \\kappa^{2})$ and $c_{2} = \\max(1, \\kappa^{2})$. This implies that the norm $\\|\\cdot\\|_{\\mathcal{H}_{CM}}$ is equivalent to the standard norm on $H^{\\alpha}(\\Omega)$. Hence, the Cameron–Martin space is, as a set of functions, identical to the Sobolev space $H^{\\alpha}(\\Omega)$. The derived expression is the squared norm on this space, parameterized by $\\kappa$ and $\\alpha$.\n\nThe final answer must be the analytical expression for the Cameron–Martin norm squared.",
            "answer": "$$\n\\boxed{\\sum_{j=0}^{\\infty} (\\kappa^{2} + \\lambda_{j})^{\\alpha} u_{j}^{2}}\n$$"
        },
        {
            "introduction": "The power of the Bayesian framework lies in its ability to systematically update our knowledge in light of new data. This practice  translates this principle into the function-space setting of SPDE-based inverse problems. You will first derive the general form of the posterior distribution for a linear Gaussian problem on a Hilbert space and then apply it to a concrete example, reinforcing the core mechanism of how the prior and likelihood combine to form the posterior.",
            "id": "3377269",
            "problem": "Consider a Bayesian linear inverse problem on a separable Hilbert space $X$ with inner product $\\langle \\cdot,\\cdot\\rangle$ and induced norm $|\\cdot|$. Let the unknown $u \\in X$ be endowed with a mean-zero Gaussian prior $\\mathcal{N}(0,C)$ with a bounded, self-adjoint, strictly positive covariance operator $C: X \\to X$. Observations $y \\in \\mathbb{R}^{m}$ follow the linear model $y = G u + \\eta$, where $G: X \\to \\mathbb{R}^{m}$ is a bounded linear forward map and the observational noise $\\eta \\sim \\mathcal{N}(0,\\Gamma)$ is Gaussian with a symmetric positive-definite covariance matrix $\\Gamma \\in \\mathbb{R}^{m \\times m}$, independent of $u$. \n\n1. Starting from Bayes’ rule and the definition of Gaussian measures on Hilbert spaces, construct the posterior measure on $X$ and identify its mean and covariance in operator form. In particular, express the posterior in terms of $C$, $G$, $\\Gamma$, and $y$, and explicitly identify the posterior precision operator.\n\n2. Now specialize to a stochastic partial differential equation (SPDE) prior that is discretization-invariant. Let $X = L^{2}((0,1))$ with periodic boundary conditions. Define the prior by the linear SPDE $(\\tau^{2} I - \\Delta) u = W$, where $\\tau  0$, $\\Delta$ is the Laplacian on the one-dimensional torus, and $W$ is Gaussian white noise on $(0,1)$. This induces a Gaussian prior $\\mathcal{N}(0,C)$ with covariance operator $C = (\\tau^{2} I - \\Delta)^{-1}$ on $X$. Consider a single scalar observation equal to the spatial average\n$$\ny = \\int_{0}^{1} u(x)\\, dx + \\eta,\n$$\nwith $\\eta \\sim \\mathcal{N}(0,\\sigma^{2})$ independent of $u$ and $\\sigma^{2}  0$. Using your result from part 1, determine the posterior mean function $m(x)$ and evaluate it at the point $x_{0} = 0$ as a closed-form analytic expression in terms of $y$, $\\tau$, and $\\sigma$. \n\nYour final answer must be a single closed-form analytic expression for $m(0)$. Do not include any units. No rounding is required.",
            "solution": "The user has provided a two-part problem concerning Bayesian inverse problems on Hilbert spaces. The problem is determined to be valid as it is mathematically well-posed, scientifically grounded in the established theory of inverse problems and functional analysis, and contains a complete and consistent set of givens.\n\n### Part 1: General Posterior Formulation\n\nWe are given a Bayesian linear inverse problem defined on a separable Hilbert space $X$. The unknown $u \\in X$ has a prior distribution given by a mean-zero Gaussian measure, $u \\sim \\mathcal{N}(0, C)$, where $C$ is the covariance operator. The observations $y \\in \\mathbb{R}^m$ are related to $u$ through the linear model $y = Gu + \\eta$, where $G: X \\to \\mathbb{R}^m$ is a bounded linear operator and $\\eta \\sim \\mathcal{N}(0, \\Gamma)$ is Gaussian noise, independent of $u$.\n\nAccording to Bayes' rule, the posterior measure $\\pi_{\\text{post}}(u|y)$ is proportional to the product of the likelihood $\\pi_{\\text{like}}(y|u)$ and the prior $\\pi_{\\text{prior}}(u)$:\n$$\n\\pi_{\\text{post}}(u|y) \\propto \\pi_{\\text{like}}(y|u) \\pi_{\\text{prior}}(u).\n$$\nFor Gaussian distributions, it is convenient to work with the negative logarithm of the probability densities, which define potential functionals.\n\nThe prior $\\mathcal{N}(0, C)$ is formally associated with the potential $I_{\\text{prior}}(u) = \\frac{1}{2}\\langle u, C^{-1}u \\rangle$, where $C^{-1}$ is the prior precision operator. The \"density\" is proportional to $\\exp(-I_{\\text{prior}}(u))$.\n\nThe likelihood arises from the noise model. Given $u$, the observation $y$ is distributed as $\\mathcal{N}(Gu, \\Gamma)$. The negative log-likelihood, viewed as a functional of $u$, is the data-misfit functional:\n$$\nI_{\\text{like}}(u) = \\frac{1}{2} (y-Gu)^T \\Gamma^{-1} (y-Gu).\n$$\nHere, we interpret elements of $\\mathbb{R}^m$ as column vectors and $\\Gamma^{-1}$ is a symmetric positive-definite matrix. This expression can be written using the inner product on $\\mathbb{R}^m$, denoted $(\\cdot, \\cdot)_{\\mathbb{R}^m}$, as $\\frac{1}{2}(y-Gu, \\Gamma^{-1}(y-Gu))_{\\mathbb{R}^m}$.\n\nThe posterior potential $I_{\\text{post}}(u)$ is the sum of the prior and likelihood potentials (up to constants):\n$$\nI_{\\text{post}}(u) = \\frac{1}{2}\\langle u, C^{-1}u \\rangle + \\frac{1}{2}(y-Gu, \\Gamma^{-1}(y-Gu))_{\\mathbb{R}^m}.\n$$\nWe expand the second term:\n$$\n(y-Gu, \\Gamma^{-1}(y-Gu))_{\\mathbb{R}^m} = (y, \\Gamma^{-1}y)_{\\mathbb{R}^m} - 2(Gu, \\Gamma^{-1}y)_{\\mathbb{R}^m} + (Gu, \\Gamma^{-1}Gu)_{\\mathbb{R}^m}.\n$$\nLet $G^*: \\mathbb{R}^m \\to X$ be the adjoint operator of $G$, defined by $\\langle G^*v, u \\rangle = (v, Gu)_{\\mathbb{R}^m}$ for all $v \\in \\mathbb{R}^m, u \\in X$. Using the adjoint, we can rewrite the terms involving $u$:\n\\begin{align*}\n(Gu, \\Gamma^{-1}y)_{\\mathbb{R}^m} = (y, \\Gamma^{-1}Gu)_{\\mathbb{R}^m} = \\langle G^*(\\Gamma^{-1}y), u \\rangle, \\\\\n(Gu, \\Gamma^{-1}Gu)_{\\mathbb{R}^m} = \\langle G^*(\\Gamma^{-1}Gu), u \\rangle = \\langle (G^*\\Gamma^{-1}G)u, u \\rangle.\n\\end{align*}\nSince $G^*\\Gamma^{-1}G$ is a self-adjoint operator, this last term is equal to $\\langle u, (G^*\\Gamma^{-1}G)u \\rangle$.\n\nSubstituting these back into the posterior potential and ignoring terms that do not depend on $u$:\n$$\nI_{\\text{post}}(u) \\propto \\frac{1}{2}\\langle u, C^{-1}u \\rangle + \\frac{1}{2}\\langle u, (G^*\\Gamma^{-1}G)u \\rangle - \\langle u, G^*(\\Gamma^{-1}y) \\rangle.\n$$\nThis can be written as:\n$$\nI_{\\text{post}}(u) = \\frac{1}{2}\\left\\langle u, (C^{-1} + G^*\\Gamma^{-1}G)u \\right\\rangle - \\left\\langle u, G^*(\\Gamma^{-1}y) \\right\\rangle.\n$$\nThe posterior distribution is also Gaussian, $\\mathcal{N}(m_{\\text{post}}, C_{\\text{post}})$, with a potential of the form:\n$$\nI_{\\text{post}}(u) = \\frac{1}{2}\\langle u-m_{\\text{post}}, C_{\\text{post}}^{-1}(u-m_{\\text{post}}) \\rangle = \\frac{1}{2}\\langle u, C_{\\text{post}}^{-1}u \\rangle - \\langle u, C_{\\text{post}}^{-1}m_{\\text{post}} \\rangle + \\text{const}.\n$$\nBy comparing the quadratic and linear terms in $u$ between the two expressions for $I_{\\text{post}}(u)$, we can identify the posterior precision operator $C_{\\text{post}}^{-1}$ and the posterior mean $m_{\\text{post}}$.\n\nThe **posterior precision operator** is:\n$$\nC_{\\text{post}}^{-1} = C^{-1} + G^*\\Gamma^{-1}G.\n$$\nThe **posterior covariance operator** is the inverse of the precision operator:\n$$\nC_{\\text{post}} = (C^{-1} + G^*\\Gamma^{-1}G)^{-1}.\n$$\nThe **posterior mean** $m_{\\text{post}}$ is determined by the relation:\n$$\nC_{\\text{post}}^{-1}m_{\\text{post}} = G^*(\\Gamma^{-1}y).\n$$\nTherefore,\n$$\nm_{\\text{post}} = C_{\\text{post}} \\left( G^*(\\Gamma^{-1}y) \\right) = (C^{-1} + G^*\\Gamma^{-1}G)^{-1} G^*(\\Gamma^{-1}y).\n$$\n\n### Part 2: Application to an SPDE Prior\n\nWe now specialize these results to the given problem.\nThe space is $X=L^2((0,1))$ with periodic boundary conditions.\nThe prior is defined by $(\\tau^2 I - \\Delta)u = W$, which gives a prior precision operator $C^{-1} = \\tau^2 I - \\Delta$, where $\\tau  0$ and $\\Delta$ is the Laplacian. The prior covariance is $C = (\\tau^2 I - \\Delta)^{-1}$.\nThe observation is a single scalar $y = \\int_0^1 u(x) dx + \\eta$, where $\\eta \\sim \\mathcal{N}(0, \\sigma^2)$.\nFrom this, we identify the components:\n- The forward operator $G: L^2((0,1)) \\to \\mathbb{R}$ is $Gu = \\int_0^1 u(x) dx$.\n- The noise covariance is the $1 \\times 1$ matrix $\\Gamma = [\\sigma^2]$, so its inverse is $\\Gamma^{-1} = [1/\\sigma^2] = [\\sigma^{-2}]$.\n\nFirst, we find the adjoint operator $G^*: \\mathbb{R} \\to L^2((0,1))$. For any $z \\in \\mathbb{R}$ and $u \\in L^2((0,1))$, the adjoint must satisfy $\\langle G^*z, u \\rangle = (z, Gu)_{\\mathbb{R}}$. The inner product on $L^2((0,1))$ is $\\langle f,g \\rangle = \\int_0^1 f(x)g(x) dx$ (for a real space).\n$$\n(z, Gu)_{\\mathbb{R}} = z \\cdot Gu = z \\int_0^1 u(x) dx = \\int_0^1 z u(x) dx.\n$$\nComparing $\\langle G^*z, u \\rangle = \\int_0^1 (G^*z)(x) u(x) dx$ with the above, we see that $(G^*z)(x) = z$. Let $\\mathbf{1}(x)$ be the function that is identically $1$ for $x \\in (0,1)$. Then $G^*z = z \\cdot \\mathbf{1}$.\n\nNow we can write the terms for the posterior mean from Part 1:\n- $G^*\\Gamma^{-1}y = G^*(\\sigma^{-2}y) = \\sigma^{-2}y \\cdot \\mathbf{1}$. This is a constant function with value $\\sigma^{-2}y$.\n- $G^*\\Gamma^{-1}G$ is an operator on $L^2((0,1))$. For a function $v \\in L^2((0,1))$,\n  $(G^*\\Gamma^{-1}G)v = G^*(\\sigma^{-2}Gv) = G^*(\\sigma^{-2} \\int_0^1 v(x') dx') = (\\sigma^{-2} \\int_0^1 v(x') dx') \\cdot \\mathbf{1}$.\n  This can be written as the rank-1 operator $\\sigma^{-2} \\mathbf{1} \\otimes \\mathbf{1}$, where $(\\mathbf{1} \\otimes \\mathbf{1})v = \\langle \\mathbf{1}, v \\rangle \\mathbf{1}$.\n\nThe posterior mean $m(x)$ satisfies the equation:\n$$\n(\\tau^2 I - \\Delta + \\sigma^{-2} \\mathbf{1} \\otimes \\mathbf{1}) m = \\sigma^{-2}y \\cdot \\mathbf{1}.\n$$\nLet's analyze the operators in the Fourier basis for the 1D torus. The eigenfunctions of $-\\Delta$ are $\\{e^{i2\\pi kx}\\}_{k \\in \\mathbb{Z}}$ with eigenvalues $(2\\pi k)^2$. The function $\\mathbf{1}(x)$ is the $k=0$ eigenfunction $e_0(x)=1$.\nThe operator $C = (\\tau^2 I - \\Delta)^{-1}$ is diagonal in this basis. Its eigenvalues are $(\\tau^2 + (2\\pi k)^2)^{-1}$.\nLet's find the posterior mean $m(x)$ by solving for it directly. From the equation $C_{\\text{post}}^{-1}m = G^*\\Gamma^{-1}y$:\n$$\n(\\tau^2 I - \\Delta)m(x) + (G^*\\Gamma^{-1}G)m(x) = (\\sigma^{-2}y) \\cdot \\mathbf{1}(x)\n$$\n$$\n(\\tau^2 I - \\Delta)m(x) + \\sigma^{-2} \\left(\\int_0^1 m(x')dx'\\right) \\cdot \\mathbf{1}(x) = \\sigma^{-2}y \\cdot \\mathbf{1}(x)\n$$\n$$\n(\\tau^2 I - \\Delta)m(x) = \\left( \\sigma^{-2}y - \\sigma^{-2}\\int_0^1 m(x')dx' \\right) \\cdot \\mathbf{1}(x)\n$$\nThe right-hand side is a constant function. Let's denote this constant by $K$.\n$$\n(\\tau^2 I - \\Delta)m(x) = K\n$$\nApplying the prior covariance operator $C = (\\tau^2 I - \\Delta)^{-1}$ to both sides gives:\n$$\nm(x) = C(K \\cdot \\mathbf{1}) = K \\cdot C(\\mathbf{1}).\n$$\nSince $\\mathbf{1}(x)$ is the eigenfunction of $\\Delta$ corresponding to eigenvalue $0$, it is also an eigenfunction of $C$.\n$$\nC\\mathbf{1} = (\\tau^2 I - \\Delta)^{-1}\\mathbf{1} = (\\tau^2 - 0)^{-1}\\mathbf{1} = \\tau^{-2}\\mathbf{1}.\n$$\nSo, $m(x) = K \\tau^{-2}$. This shows that the posterior mean $m(x)$ must be a constant function. Let $m(x) = M$.\nThen the integral $\\int_0^1 m(x')dx' = \\int_0^1 M dx' = M$.\nThe Laplacian of a constant is zero, $\\Delta M = 0$.\nWe substitute $m(x) = M$ back into the equation for $m(x)$:\n$$\n(\\tau^2 I - \\Delta)M = \\tau^2 M.\n$$\nThe equation becomes:\n$$\n\\tau^2 M = \\sigma^{-2}y - \\sigma^{-2}M.\n$$\nWe can now solve for the constant $M$:\n$$\n\\tau^2 M + \\sigma^{-2}M = \\sigma^{-2}y\n$$\n$$\nM (\\tau^2 + \\sigma^{-2}) = \\sigma^{-2}y\n$$\n$$\nM \\left(\\frac{\\tau^2\\sigma^2+1}{\\sigma^2}\\right) = \\frac{y}{\\sigma^2}\n$$\n$$\nM = \\frac{y}{\\tau^2\\sigma^2 + 1}.\n$$\nThus, the posterior mean function is the constant function $m(x) = \\frac{y}{1+\\sigma^2\\tau^2}$.\nThe problem asks for this function evaluated at $x_0 = 0$. Since the function is constant, the value is independent of $x$:\n$$\nm(0) = \\frac{y}{1+\\sigma^2\\tau^2}.\n$$\nThis is the final closed-form analytic expression for the posterior mean at $x_0=0$.",
            "answer": "$$ \\boxed{ \\frac{y}{1 + \\sigma^{2} \\tau^{2}} } $$"
        },
        {
            "introduction": "A key advantage of formulating inverse problems on function spaces is that the inference should be robust to the choice of discretization, a property known as mesh-invariance. This practice  provides a powerful and concrete illustration of this principle by highlighting a common implementation pitfall. By comparing a naive discretization of the observational noise with a theoretically sound one, you will see firsthand how improper scaling can lead to a posterior that collapses, underscoring the importance of consistent discretization of all components of the model.",
            "id": "3377260",
            "problem": "Consider the one-dimensional spatial domain $[0,1]$ and a uniform partition into $n$ non-overlapping cells of equal length $h = 1/n$. Let $\\{\\psi_{j}\\}_{j=1}^{n}$ denote the piecewise-constant basis functions, where $\\psi_{j}(x) = 1$ on cell $j$ and $\\psi_{j}(x) = 0$ elsewhere. In this basis, the finite-dimensional mass matrix is $M_{h} = h I_{n}$, where $I_{n}$ is the $n \\times n$ identity matrix. \n\nAdopt a zero-order Stochastic Partial Differential Equation (SPDE) Gaussian prior for the cellwise-constant field $u_{h}(x) = \\sum_{j=1}^{n} c_{j} \\psi_{j}(x)$ by placing a Gaussian prior on the coefficient vector $c \\in \\mathbb{R}^{n}$ as $c \\sim \\mathcal{N}(0, Q_{h}^{-1})$ with prior precision $Q_{h} = \\kappa^{2} M_{h}$, where $\\kappa  0$ is fixed and the prior is discretization-invariant in the $L^{2}$ metric induced by $M_{h}$.\n\nAssume a Galerkin-projected observation model in $L^{2}$: the observed data $y_{h} \\in \\mathbb{R}^{n}$ are given by\n$$\ny_{h} = H_{h} c + \\varepsilon_{h}, \\quad H_{h} = M_{h},\n$$\nand the observational noise $\\varepsilon_{h}$ is the projection of $L^{2}$ white noise with amplitude $\\sigma  0$. In the correct discretization, this yields the discrete noise covariance\n$$\n\\Gamma_{h} = \\sigma^{2} M_{h}.\n$$\nSuppose the underlying truth is the spatially constant function $u^{\\star}(x) = 1$. Then its coefficient vector relative to $\\{\\psi_{j}\\}$ is $c^{\\star} = \\mathbf{1}$ (the vector of all ones), and the noise-free data are $y_{h} = M_{h} c^{\\star} = h \\mathbf{1}$.\n\nDefine the posterior mean under Gaussian linear inversion by\n$$\nm_{h}(\\Gamma) = \\left(Q_{h} + H_{h}^{\\top} \\Gamma^{-1} H_{h}\\right)^{-1} H_{h}^{\\top} \\Gamma^{-1} y_{h}.\n$$\n\nConsider two cases:\n- Incorrect noise scaling: use $\\Gamma_{h}^{\\mathrm{wrong}} = \\sigma^{2} I_{n}$.\n- Correct noise scaling: use $\\Gamma_{h}^{\\mathrm{corr}} = \\sigma^{2} M_{h}$.\n\nStarting from first principles of Gaussian conditioning in linear inverse problems and the definitions above, derive closed-form expressions for $m_{h}(\\Gamma_{h}^{\\mathrm{wrong}})$ and $m_{h}(\\Gamma_{h}^{\\mathrm{corr}})$, and determine the limit of the posterior mean field as the mesh is refined ($h \\to 0$). For concreteness, report the value of the posterior mean at any fixed point $x \\in (0,1)$ in each case, which is equal to the corresponding cellwise coefficient due to the piecewise-constant parameterization.\n\nYour final answer must be a single row matrix whose first entry is the limit of the incorrectly scaled posterior mean at any $x \\in (0,1)$ as $h \\to 0$, and whose second entry is the correctly scaled posterior mean at any $x \\in (0,1)$ (which is independent of $h$). No rounding is required, and no units are involved in the final expression.",
            "solution": "The problem asks for the derivation of the posterior mean of a Gaussian random field under two different assumptions about the observational noise covariance structure, and to analyze the behavior of this mean as the discretization mesh is refined.\n\nThe Bayesian posterior mean for a linear Gaussian inverse problem is given by the formula:\n$$\nm_{h}(\\Gamma) = \\left(Q_{h} + H_{h}^{\\top} \\Gamma^{-1} H_{h}\\right)^{-1} H_{h}^{\\top} \\Gamma^{-1} y_{h}\n$$\nThe problem provides the following finite-dimensional representations corresponding to a piecewise-constant discretization on a uniform mesh of size $h = \\frac{1}{n}$ on the domain $[0,1]$:\n- Prior precision matrix: $Q_{h} = \\kappa^{2} M_{h} = \\kappa^{2} h I_{n}$\n- Observation operator: $H_{h} = M_{h} = h I_{n}$\n- Noise-free data (for true field $u^{\\star}(x)=1$): $y_{h} = M_{h} c^{\\star} = h \\mathbf{1}$, where $\\mathbf{1}$ is the $n$-vector of all ones.\n\nWe will analyze the two specified cases for the noise covariance matrix $\\Gamma$.\n\n### Case 1: Incorrect Noise Scaling\n\nIn this case, the noise covariance is assumed to be $\\Gamma_{h}^{\\mathrm{wrong}} = \\sigma^{2} I_{n}$. We substitute this and the other given matrices into the posterior mean formula.\n\nFirst, we compute the inverse of the noise covariance:\n$$\n\\left(\\Gamma_{h}^{\\mathrm{wrong}}\\right)^{-1} = \\left(\\sigma^{2} I_{n}\\right)^{-1} = \\frac{1}{\\sigma^{2}} I_{n}\n$$\nNext, we compute the term $H_{h}^{\\top} \\left(\\Gamma_{h}^{\\mathrm{wrong}}\\right)^{-1} H_{h}$, which represents the precision of the data:\n$$\nH_{h}^{\\top} \\left(\\Gamma_{h}^{\\mathrm{wrong}}\\right)^{-1} H_{h} = (h I_{n})^{\\top} \\left(\\frac{1}{\\sigma^{2}} I_{n}\\right) (h I_{n}) = h^{2} \\frac{1}{\\sigma^{2}} I_{n}^{\\top} I_{n} I_{n} = \\frac{h^{2}}{\\sigma^{2}} I_{n}\n$$\nThe total posterior precision matrix, $\\Lambda_{h}^{\\mathrm{wrong}}$, is the sum of the prior precision and the data precision:\n$$\n\\Lambda_{h}^{\\mathrm{wrong}} = Q_{h} + H_{h}^{\\top} \\left(\\Gamma_{h}^{\\mathrm{wrong}}\\right)^{-1} H_{h} = \\kappa^{2} h I_{n} + \\frac{h^{2}}{\\sigma^{2}} I_{n} = \\left(\\kappa^{2} h + \\frac{h^{2}}{\\sigma^{2}}\\right) I_{n}\n$$\nThe inverse of the posterior precision is:\n$$\n\\left(\\Lambda_{h}^{\\mathrm{wrong}}\\right)^{-1} = \\frac{1}{\\kappa^{2} h + \\frac{h^{2}}{\\sigma^{2}}} I_{n}\n$$\nNext, we compute the term $H_{h}^{\\top} \\left(\\Gamma_{h}^{\\mathrm{wrong}}\\right)^{-1} y_{h}$:\n$$\nH_{h}^{\\top} \\left(\\Gamma_{h}^{\\mathrm{wrong}}\\right)^{-1} y_{h} = (h I_{n})^{\\top} \\left(\\frac{1}{\\sigma^{2}} I_{n}\\right) (h \\mathbf{1}) = \\frac{h^{2}}{\\sigma^{2}} I_{n} \\mathbf{1} = \\frac{h^{2}}{\\sigma^{2}} \\mathbf{1}\n$$\nFinally, we assemble the posterior mean vector $m_{h}(\\Gamma_{h}^{\\mathrm{wrong}})$:\n$$\nm_{h}(\\Gamma_{h}^{\\mathrm{wrong}}) = \\left(\\Lambda_{h}^{\\mathrm{wrong}}\\right)^{-1} \\left(H_{h}^{\\top} \\left(\\Gamma_{h}^{\\mathrm{wrong}}\\right)^{-1} y_{h}\\right) = \\left(\\frac{1}{\\kappa^{2} h + \\frac{h^{2}}{\\sigma^{2}}} I_{n}\\right) \\left(\\frac{h^{2}}{\\sigma^{2}} \\mathbf{1}\\right)\n$$\n$$\nm_{h}(\\Gamma_{h}^{\\mathrm{wrong}}) = \\frac{\\frac{h^{2}}{\\sigma^{2}}}{\\kappa^{2} h \\sigma^{2} + h^{2}} \\mathbf{1} = \\frac{h}{\\kappa^{2} \\sigma^{2} + h} \\mathbf{1}\n$$\nSince the basis functions are piecewise-constant, the value of the posterior mean field at any point $x \\in (0,1)$ is given by the value of the coefficient in the corresponding cell. This value is $\\frac{h}{\\kappa^{2} \\sigma^{2} + h}$.\n\nTo determine the behavior as the mesh is refined, we take the limit as $h \\to 0$:\n$$\n\\lim_{h \\to 0} \\frac{h}{\\kappa^{2} \\sigma^{2} + h} = \\frac{0}{\\kappa^{2} \\sigma^{2} + 0} = 0\n$$\nWith the incorrect noise scaling, the posterior mean collapses to the prior mean ($0$) as the discretization becomes finer.\n\n### Case 2: Correct Noise Scaling\n\nIn this case, the noise covariance is $\\Gamma_{h}^{\\mathrm{corr}} = \\sigma^{2} M_{h} = \\sigma^{2} h I_{n}$. This corresponds to a discretization-invariant representation of $L^{2}$ white noise.\n\nFirst, we find the inverse of the noise covariance:\n$$\n\\left(\\Gamma_{h}^{\\mathrm{corr}}\\right)^{-1} = \\left(\\sigma^{2} h I_{n}\\right)^{-1} = \\frac{1}{\\sigma^{2} h} I_{n}\n$$\nNext, we compute the data precision term $H_{h}^{\\top} \\left(\\Gamma_{h}^{\\mathrm{corr}}\\right)^{-1} H_{h}$:\n$$\nH_{h}^{\\top} \\left(\\Gamma_{h}^{\\mathrm{corr}}\\right)^{-1} H_{h} = (h I_{n})^{\\top} \\left(\\frac{1}{\\sigma^{2} h} I_{n}\\right) (h I_{n}) = h \\left(\\frac{1}{\\sigma^{2} h}\\right) h I_{n} = \\frac{h}{\\sigma^{2}} I_{n}\n$$\nThe posterior precision matrix, $\\Lambda_{h}^{\\mathrm{corr}}$, is:\n$$\n\\Lambda_{h}^{\\mathrm{corr}} = Q_{h} + H_{h}^{\\top} \\left(\\Gamma_{h}^{\\mathrm{corr}}\\right)^{-1} H_{h} = \\kappa^{2} h I_{n} + \\frac{h}{\\sigma^{2}} I_{n} = h \\left(\\kappa^{2} + \\frac{1}{\\sigma^{2}}\\right) I_{n}\n$$\nThe inverse of the posterior precision is:\n$$\n\\left(\\Lambda_{h}^{\\mathrm{corr}}\\right)^{-1} = \\frac{1}{h \\left(\\kappa^{2} + \\frac{1}{\\sigma^{2}}\\right)} I_{n}\n$$\nNext, we compute the term $H_{h}^{\\top} \\left(\\Gamma_{h}^{\\mathrm{corr}}\\right)^{-1} y_{h}$:\n$$\nH_{h}^{\\top} \\left(\\Gamma_{h}^{\\mathrm{corr}}\\right)^{-1} y_{h} = (h I_{n})^{\\top} \\left(\\frac{1}{\\sigma^{2} h} I_{n}\\right) (h \\mathbf{1}) = h \\left(\\frac{1}{\\sigma^{2} h}\\right) h \\mathbf{1} = \\frac{h}{\\sigma^{2}} \\mathbf{1}\n$$\nFinally, we assemble the posterior mean vector $m_{h}(\\Gamma_{h}^{\\mathrm{corr}})$:\n$$\nm_{h}(\\Gamma_{h}^{\\mathrm{corr}}) = \\left(\\Lambda_{h}^{\\mathrm{corr}}\\right)^{-1} \\left(H_{h}^{\\top} \\left(\\Gamma_{h}^{\\mathrm{corr}}\\right)^{-1} y_{h}\\right) = \\left(\\frac{1}{h \\left(\\kappa^{2} + \\frac{1}{\\sigma^{2}}\\right)} I_{n}\\right) \\left(\\frac{h}{\\sigma^{2}} \\mathbf{1}\\right)\n$$\n$$\nm_{h}(\\Gamma_{h}^{\\mathrm{corr}}) = \\frac{\\frac{h}{\\sigma^{2}}}{h \\left(\\kappa^{2} + \\frac{1}{\\sigma^{2}}\\right)} \\mathbf{1} = \\frac{\\frac{1}{\\sigma^{2}}}{\\kappa^{2} + \\frac{1}{\\sigma^{2}}} \\mathbf{1} = \\frac{1}{1 + \\kappa^{2} \\sigma^{2}} \\mathbf{1}\n$$\nThe value of the posterior mean field at any point $x \\in (0,1)$ is the constant coefficient $\\frac{1}{1 + \\kappa^{2} \\sigma^{2}}$.\nThis expression is independent of the mesh size $h$. Therefore, its limit as $h \\to 0$ is the value itself. This demonstrates that the correctly scaled (discretization-invariant) prior and likelihood lead to a well-defined posterior in the continuum limit. The resulting posterior mean of $\\frac{1}{1+\\kappa^{2}\\sigma^{2}}$ is a precision-weighted average of the prior mean ($0$) and the value implied by the data ($1$).\n\n### Summary of Results\n- Incorrectly scaled posterior mean at $x \\in (0,1)$: $\\lim_{h \\to 0} \\frac{h}{\\kappa^{2} \\sigma^{2} + h} = 0$.\n- Correctly scaled posterior mean at $x \\in (0,1)$: $\\frac{1}{1 + \\kappa^{2} \\sigma^{2}}$, which is independent of $h$.\n\nThe final answer is a row matrix containing these two values.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0  \\frac{1}{1 + \\kappa^{2} \\sigma^{2}}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}