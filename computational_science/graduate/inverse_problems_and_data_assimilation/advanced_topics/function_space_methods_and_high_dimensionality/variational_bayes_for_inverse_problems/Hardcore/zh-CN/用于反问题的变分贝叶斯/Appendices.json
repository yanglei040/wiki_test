{
    "hands_on_practices": [
        {
            "introduction": "理解变分贝叶斯的核心是掌握其近似的本质。这个基础练习旨在通过一个简单的线性高斯模型来揭示这一点，在此模型中，精确的后验分布是可解析计算的。通过直接比较精确解和均值场变分近似解，我们可以量化由忽略后验相关性这一核心假设所引入的“近似差距”，从而为理解变分贝叶斯的优势和局限性打下坚实的基础 。",
            "id": "3430192",
            "problem": "考虑由观测模型 $y = A x + \\varepsilon$ 定义的线性逆问题，其中 $x \\in \\mathbb{R}^2$，$A \\in \\mathbb{R}^{2 \\times 2}$，$x \\sim \\mathcal{N}(m_0, C_0)$ 且 $\\varepsilon \\sim \\mathcal{N}(0, R)$。设各量具体如下：$A = I_2$，$m_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，$C_0 = \\begin{pmatrix} 2  \\frac{6}{5} \\\\ \\frac{6}{5}  1 \\end{pmatrix}$，$R = \\frac{1}{2} I_2$，以及观测数据 $y = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。\n\n以贝叶斯法则和高斯密度的定义为基本依据，完成以下任务：\n\n1.  推导精确后验分布 $p(x \\mid y)$，并以闭式形式计算其均值和协方差。\n\n2.  考虑一个完全因子分解的高斯分布的变分族 $q(x) = \\mathcal{N}(m, \\mathrm{diag}(s_1^2, s_2^2))$。通过最小化 Kullback–Leibler (KL) 散度 $\\mathrm{KL}(q \\| p(x \\mid y))$，以闭式形式确定最优的 $m$、$s_1^2$ 和 $s_2^2$。\n\n3.  将“近似差距”定义为 Kullback–Leibler 散度 $\\mathrm{KL}(q^\\star \\| p(x \\mid y))$，其中 $q^\\star$ 是在第 2 部分中找到的最优平均场高斯分布。为该值提供一个单一的闭式解析表达式。\n\n仅报告最终的近似差距作为您的最终答案。不要四舍五入；请提供精确的解析表达式。无需单位。",
            "solution": "该问题要求对一个线性贝叶斯逆问题进行三部分分析：首先，求出精确的后验分布；其次，确定最优的平均场变分近似；第三，计算该近似与真实后验之间的 Kullback–Leibler (KL) 散度。\n\n观测模型由 $y = A x + \\varepsilon$ 给出，其中 $x \\in \\mathbb{R}^2$。\n$x$ 的先验是一个高斯分布：$p(x) = \\mathcal{N}(x \\mid m_0, C_0)$。\n噪声 $\\varepsilon$ 也是高斯分布的：$\\varepsilon \\sim \\mathcal{N}(0, R)$。\n这意味着似然函数为 $p(y \\mid x) = \\mathcal{N}(y \\mid Ax, R)$。\n\n所提供的值为：\n$A = I_2 = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$\n$m_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n$C_0 = \\begin{pmatrix} 2  \\frac{6}{5} \\\\ \\frac{6}{5}  1 \\end{pmatrix}$\n$R = \\frac{1}{2} I_2 = \\begin{pmatrix} \\frac{1}{2}  0 \\\\ 0  \\frac{1}{2} \\end{pmatrix}$\n$y = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$\n\n该问题是有效的，因为它是贝叶斯统计和机器学习中的一个标准、适定的问题，提供了所有必要的信息，没有内部矛盾或科学缺陷。\n\n**第1部分：精确后验分布**\n\n对于线性高斯模型，后验分布 $p(x \\mid y)$ 也是一个高斯分布，我们将其表示为 $\\mathcal{N}(x \\mid m_p, C_p)$。后验协方差 $C_p$ 和均值 $m_p$ 由标准的贝叶斯更新公式给出：\n$$C_p^{-1} = C_0^{-1} + A^T R^{-1} A$$\n$$m_p = C_p (C_0^{-1}m_0 + A^T R^{-1} y)$$\n\n首先，我们计算逆矩阵 $C_0^{-1}$ 和 $R^{-1}$。\n$C_0$ 的行列式为 $\\det(C_0) = (2)(1) - (\\frac{6}{5})^2 = 2 - \\frac{36}{25} = \\frac{50-36}{25} = \\frac{14}{25}$。\n$C_0$ 的逆矩阵为：\n$$C_0^{-1} = \\frac{1}{14/25} \\begin{pmatrix} 1  -\\frac{6}{5} \\\\ -\\frac{6}{5}  2 \\end{pmatrix} = \\frac{25}{14} \\begin{pmatrix} 1  -\\frac{6}{5} \\\\ -\\frac{6}{5}  2 \\end{pmatrix} = \\begin{pmatrix} \\frac{25}{14}  -\\frac{15}{7} \\\\ -\\frac{15}{7}  \\frac{25}{7} \\end{pmatrix}$$\n$R$ 的逆矩阵为：\n$$R^{-1} = (\\frac{1}{2} I_2)^{-1} = 2 I_2 = \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix}$$\n\n现在我们可以计算后验精度矩阵 $C_p^{-1}$：\n$$C_p^{-1} = C_0^{-1} + A^T R^{-1} A = C_0^{-1} + I_2 (2I_2) I_2 = C_0^{-1} + 2I_2$$\n$$C_p^{-1} = \\begin{pmatrix} \\frac{25}{14}  -\\frac{15}{7} \\\\ -\\frac{15}{7}  \\frac{25}{7} \\end{pmatrix} + \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix} = \\begin{pmatrix} \\frac{25}{14} + \\frac{28}{14}  -\\frac{15}{7} \\\\ -\\frac{15}{7}  \\frac{25}{7} + \\frac{14}{7} \\end{pmatrix} = \\begin{pmatrix} \\frac{53}{14}  -\\frac{15}{7} \\\\ -\\frac{15}{7}  \\frac{39}{7} \\end{pmatrix}$$\n\n接下来，我们计算后验均值 $m_p$。由于 $m_0 = 0$，公式简化为 $m_p = C_p (A^T R^{-1} y)$。\n我们先计算项 $A^T R^{-1} y$：\n$$A^T R^{-1} y = I_2 (2I_2) \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix}$$\n所以，$m_p = C_p \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix}$，这等价于求解方程组 $C_p^{-1} m_p = \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix}$。\n为了求得 $m_p$，我们首先需要 $C_p = (C_p^{-1})^{-1}$。$C_p^{-1}$ 的行列式是：\n$$\\det(C_p^{-1}) = \\left(\\frac{53}{14}\\right)\\left(\\frac{39}{7}\\right) - \\left(-\\frac{15}{7}\\right)^2 = \\frac{2067}{98} - \\frac{225}{49} = \\frac{2067 - 450}{98} = \\frac{1617}{98} = \\frac{33}{2}$$\n后验协方差矩阵 $C_p$ 是：\n$$C_p = \\frac{1}{33/2} \\begin{pmatrix} \\frac{39}{7}  \\frac{15}{7} \\\\ \\frac{15}{7}  \\frac{53}{14} \\end{pmatrix} = \\frac{2}{33} \\begin{pmatrix} \\frac{78}{14}  \\frac{30}{14} \\\\ \\frac{30}{14}  \\frac{53}{14} \\end{pmatrix} = \\frac{1}{231} \\begin{pmatrix} 78  30 \\\\ 30  53 \\end{pmatrix}$$\n现在我们计算后验均值 $m_p$：\n$$m_p = \\frac{1}{231} \\begin{pmatrix} 78  30 \\\\ 30  53 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix} = \\frac{1}{231} \\begin{pmatrix} 156 - 60 \\\\ 60 - 106 \\end{pmatrix} = \\frac{1}{231} \\begin{pmatrix} 96 \\\\ -46 \\end{pmatrix} = \\begin{pmatrix} \\frac{32}{77} \\\\ -\\frac{46}{231} \\end{pmatrix}$$\n因此，精确后验分布为 $p(x \\mid y) = \\mathcal{N}(x \\mid m_p, C_p)$，其中 $m_p$ 和 $C_p$ 如上推导。\n\n**第2部分：最优变分近似**\n\n我们考虑一个完全因子分解的高斯分布的变分族 $q(x) = q_1(x_1)q_2(x_2) = \\mathcal{N}(x \\mid m, \\mathrm{diag}(s_1^2, s_2^2))$。我们最小化 $\\mathrm{KL}(q \\| p(x \\mid y))$。对于一个高斯后验分布 $p(x \\mid y) = \\mathcal{N}(x \\mid m_p, C_p)$，已知最小化此 KL 散度的 $q$ 的最优参数为：\n1.  变分分布的均值等于真实后验的均值：$m^\\star = m_p$。\n2.  各因子的方差是后验精度矩阵 $C_p^{-1}$ 对角元素的倒数。\n所以，$s_1^2 = 1 / (C_p^{-1})_{11}$ 且 $s_2^2 = 1 / (C_p^{-1})_{22}$。\n\n使用第1部分中的后验精度矩阵 $C_p^{-1}$：\n$$(C_p^{-1})_{11} = \\frac{53}{14} \\quad \\text{和} \\quad (C_p^{-1})_{22} = \\frac{39}{7}$$\n最优方差为：\n$$s_1^2 = \\frac{1}{53/14} = \\frac{14}{53}$$\n$$s_2^2 = \\frac{1}{39/7} = \\frac{7}{39}$$\n最优变分分布为 $q^\\star(x) = \\mathcal{N}(x \\mid m^\\star, C_q^\\star)$，其中 $m^\\star = m_p$ 且 $C_q^\\star = \\mathrm{diag}(\\frac{14}{53}, \\frac{7}{39})$。\n\n**第3部分：近似差距**\n\n近似差距定义为 $\\mathrm{KL}(q^\\star \\| p(x \\mid y))$。对于两个维度为 $d$ 的多元高斯分布 $q = \\mathcal{N}(m_q, C_q)$ 和 $p = \\mathcal{N}(m_p, C_p)$，它们之间的 KL 散度的通用公式为：\n$$\\mathrm{KL}(q \\| p) = \\frac{1}{2} \\left[ \\ln\\left(\\frac{\\det C_p}{\\det C_q}\\right) - d + \\mathrm{tr}(C_p^{-1} C_q) + (m_p - m_q)^T C_p^{-1} (m_p - m_q) \\right]$$\n在我们的情况下，$d=2$，$q=q^\\star$，$p=p(x \\mid y)$。从第2部分我们知道 $m_q = m_p$，所以 $(m_p - m_q)^T C_p^{-1} (m_p - m_q)$ 项为零。公式简化为：\n$$\\mathrm{KL}(q^\\star \\| p) = \\frac{1}{2} \\left[ \\ln\\left(\\frac{\\det C_p}{\\det C_q^\\star}\\right) - 2 + \\mathrm{tr}(C_p^{-1} C_q^\\star) \\right]$$\n我们来计算迹项。设 $P = C_p^{-1}$。那么 $C_q^\\star = \\mathrm{diag}(1/P_{11}, 1/P_{22})$。\n$$\\mathrm{tr}(C_p^{-1} C_q^\\star) = \\mathrm{tr}\\left( \\begin{pmatrix} P_{11}  P_{12} \\\\ P_{21}  P_{22} \\end{pmatrix} \\begin{pmatrix} 1/P_{11}  0 \\\\ 0  1/P_{22} \\end{pmatrix} \\right) = \\mathrm{tr}\\left( \\begin{pmatrix} 1  P_{12}/P_{22} \\\\ P_{21}/P_{11}  1 \\end{pmatrix} \\right) = 1+1=2$$\n迹项精确等于维度 $d$。KL 散度表达式变得更简单：\n$$\\mathrm{KL}(q^\\star \\| p) = \\frac{1}{2} \\left[ \\ln\\left(\\frac{\\det C_p}{\\det C_q^\\star}\\right) - 2 + 2 \\right] = \\frac{1}{2} \\ln\\left(\\frac{\\det C_p}{\\det C_q^\\star}\\right)$$\n我们有 $\\det C_p = 1/\\det(C_p^{-1})$。$C_q^\\star$ 的行列式是 $(\\det C_q^\\star) = s_1^2 s_2^2 = (1/P_{11})(1/P_{22})$。\n所以行列式的比值为：\n$$\\frac{\\det C_p}{\\det C_q^\\star} = \\frac{1/\\det(P)}{1/(P_{11}P_{22})} = \\frac{P_{11}P_{22}}{\\det(P)} = \\frac{P_{11}P_{22}}{P_{11}P_{22} - P_{12}^2}$$\n代入 $P_{ij} = (C_p^{-1})_{ij}$ 的值：\n$P_{11} = 53/14$，$P_{22} = 39/7$，以及 $P_{12} = -15/7$。\n$$P_{11}P_{22} = \\left(\\frac{53}{14}\\right)\\left(\\frac{39}{7}\\right) = \\frac{2067}{98}$$\n$$P_{12}^2 = \\left(-\\frac{15}{7}\\right)^2 = \\frac{225}{49}$$\n比值为：\n$$\\frac{P_{11}P_{22}}{P_{11}P_{22} - P_{12}^2} = \\frac{2067/98}{2067/98 - 225/49} = \\frac{2067/98}{(2067-450)/98} = \\frac{2067}{1617}$$\n这个分数可以通过分子分母同除以它们的最大公约数 3 来化简。\n$$\\frac{2067 \\div 3}{1617 \\div 3} = \\frac{689}{539}$$\n最终的近似差距为：\n$$\\mathrm{KL}(q^\\star \\| p(x \\mid y)) = \\frac{1}{2} \\ln\\left(\\frac{689}{539}\\right)$$\n这也可以表示为 $-\\frac{1}{2} \\ln(1 - \\rho_P^2)$，其中 $\\rho_P = P_{12}/\\sqrt{P_{11}P_{22}}$ 是与精度矩阵相关的相关系数。这个非零值反映了因忽略 $x_1$ 和 $x_2$ 之间的后验相关性而损失的信息。",
            "answer": "$$\\boxed{\\frac{1}{2} \\ln\\left(\\frac{689}{539}\\right)}$$"
        },
        {
            "introduction": "在掌握了线性模型后，我们将复杂性提升一步，探索非线性逆问题。在这个练习中，精确的后验分布通常是难以处理的，因此近似方法变得至关重要。我们将推导并比较两种广泛使用的技术——基于线性化的变分贝叶斯近似和拉普拉斯近似——来处理一个简单的非线性问题 。这个对比练习揭示了不同方法如何捕捉后验曲率，并突出了在非线性场景中应用变分近似时需要考虑的关键细节。",
            "id": "3430120",
            "problem": "考虑一个贝叶斯逆问题，其未知量为标量 $x \\in \\mathbb{R}$，高斯先验为 $x \\sim \\mathcal{N}(0,\\sigma_{0}^{2})$，观测模型为 $y = G(x) + \\eta$，其中非线性前向映射为 $G(x) = x^{2}$，加性高斯噪声为 $\\eta \\sim \\mathcal{N}(0,\\sigma^{2})$。令负对数后验（不计加性常数）表示为 $\\Phi(x)$。最大后验 (MAP) 估计量 $x_{\\mathrm{MAP}}$ 定义为 $\\Phi(x)$ 的最小化子。你可以假设 $y  0$ 且 $0  2 y  \\sigma^{2}/\\sigma_{0}^{2}$，从而 $x_{\\mathrm{MAP}} = 0$ 是 $\\Phi(x)$ 的一个局部极小值点。\n\n从高斯先验和高斯似然的定义出发，仅使用微分和链式法则的基本原理，执行以下操作：\n\n1.  显式推导 $\\Phi(x)$ 并获得驻点条件 $\\nabla \\Phi(x) = 0$，以验证 $x = 0$ 是一个驻点。在给定的关于 $y$ 的假设下，论证 $x_{\\mathrm{MAP}} = 0$ 是一个局部极小值点。\n2.  在 $x_{\\mathrm{MAP}}$ 处推导Hessian矩阵 $\\nabla^{2}\\Phi(x)$，并将后验的拉普拉斯近似写成一个均值为 $x_{\\mathrm{MAP}}$、协方差为 $\\Sigma_{\\mathrm{Lap}} = [\\nabla^{2}\\Phi(x_{\\mathrm{MAP}})]^{-1}$ 的高斯分布。\n3.  考虑一个高斯变分贝叶斯 (VB) 近似 $q(x) = \\mathcal{N}(\\mu,v)$，该近似通过最小化从 $q(x)$ 到精确后验的Kullback-Leibler (KL) 散度得到。在此过程中，遵循标准高斯VB的做法，将前向映射 $G(x)$ 在变分均值 $\\mu$ 周围线性化为 $G(x) \\approx G(\\mu) + G'(\\mu)(x-\\mu)$。使用此线性化，推导隐含的似然的二次代理和相应的高斯后验近似，并求出在 $\\mu = x_{\\mathrm{MAP}}$ 处的VB协方差，记为 $\\Sigma_{\\mathrm{VB}}$。\n4.  计算比率 $\\rho = \\Sigma_{\\mathrm{VB}}/\\Sigma_{\\mathrm{Lap}}$，并将其表示为 $\\sigma_{0}^{2}$、$\\sigma^{2}$ 和 $y$ 的单一闭式函数。\n\n你的最终答案必须是 $\\rho$ 的单一解析表达式。不需要四舍五入，也不涉及单位。",
            "solution": "该问题要求进行一个四部分的推导，为一个特定的非线性贝叶斯逆问题比较拉普拉斯近似和变分贝叶斯 (VB) 近似。我们将按要求逐步进行。\n\n$x$ 的先验概率密度函数 (PDF) 由 $x \\sim \\mathcal{N}(0, \\sigma_{0}^{2})$ 给出，即 $p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_{0}^{2}}} \\exp\\left(-\\frac{x^2}{2\\sigma_{0}^{2}}\\right)$。\n观测模型为 $y = x^2 + \\eta$，其中噪声 $\\eta \\sim \\mathcal{N}(0, \\sigma^2)$。这定义了似然PDF为 $p(y|x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - x^2)^2}{2\\sigma^2}\\right)$。\n\n根据贝叶斯定理，后验PDF与似然和先验的乘积成正比：$p(x|y) \\propto p(y|x)p(x)$。负对数后验，记为 $\\Phi(x)$，可以通过取负对数并舍弃任何不依赖于 $x$ 的项来写出：\n$$\n\\Phi(x) = -\\ln(p(y|x)) - \\ln(p(x)) + \\text{const}\n$$\n$$\n\\Phi(x) = \\frac{(y - x^2)^2}{2\\sigma^2} + \\frac{x^2}{2\\sigma_0^2}\n$$\n\n**第1部分：MAP估计的分析**\n\n为了找到 $\\Phi(x)$ 的驻点，我们计算它关于 $x$ 的一阶导数，记为 $\\nabla \\Phi(x)$：\n$$\n\\nabla \\Phi(x) = \\frac{d\\Phi}{dx} = \\frac{1}{2\\sigma^2} \\cdot 2(y - x^2) \\cdot (-2x) + \\frac{2x}{2\\sigma_0^2}\n$$\n$$\n\\nabla \\Phi(x) = -\\frac{2x(y - x^2)}{\\sigma^2} + \\frac{x}{\\sigma_0^2} = x \\left( \\frac{2x^2 - 2y}{\\sigma^2} + \\frac{1}{\\sigma_0^2} \\right)\n$$\n驻点条件是 $\\nabla \\Phi(x) = 0$。通过观察可知，一个解是 $x=0$。因此，$x=0$ 是一个驻点。\n\n为了确定此驻点的性质，我们计算 $\\Phi(x)$ 的二阶导数，即Hessian矩阵 $\\nabla^2 \\Phi(x)$：\n$$\n\\nabla^2 \\Phi(x) = \\frac{d^2\\Phi}{dx^2} = \\frac{d}{dx} \\left( \\frac{2x^3 - 2xy}{\\sigma^2} + \\frac{x}{\\sigma_0^2} \\right)\n$$\n$$\n\\nabla^2 \\Phi(x) = \\frac{6x^2 - 2y}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\n$$\n我们在驻点 $x=0$ 处计算Hessian矩阵：\n$$\n\\nabla^2 \\Phi(0) = \\frac{6(0)^2 - 2y}{\\sigma^2} + \\frac{1}{\\sigma_0^2} = \\frac{1}{\\sigma_0^2} - \\frac{2y}{\\sigma^2}\n$$\n要使 $x=0$ 成为局部极小值点，二阶导数检验要求 $\\nabla^2 \\Phi(0)  0$。这意味着：\n$$\n\\frac{1}{\\sigma_0^2} - \\frac{2y}{\\sigma^2}  0 \\implies \\frac{1}{\\sigma_0^2}  \\frac{2y}{\\sigma^2} \\implies 2y  \\frac{\\sigma^2}{\\sigma_0^2}\n$$\n这正是问题陈述中给出的条件。因此，在给定的假设下，$x=0$ 是 $\\Phi(x)$ 的一个局部极小值点，我们将其确定为局部最大后验估计，$x_{\\mathrm{MAP}} = 0$。\n\n**第2部分：拉普拉斯近似的协方差 $\\Sigma_{\\mathrm{Lap}}$**\n\n拉普拉斯近似将后验分布建模为以MAP估计为中心的高斯分布，其精度（逆协方差）由在MAP估计点处求值的负对数后验的Hessian矩阵给出。\n对于我们的标量情况，精度为 $\\Sigma_{\\mathrm{Lap}}^{-1} = \\nabla^2 \\Phi(x_{\\mathrm{MAP}})$。使用第1部分中 $x_{\\mathrm{MAP}} = 0$ 的结果：\n$$\n\\Sigma_{\\mathrm{Lap}}^{-1} = \\nabla^2 \\Phi(0) = \\frac{1}{\\sigma_0^2} - \\frac{2y}{\\sigma^2}\n$$\n拉普拉斯近似的协方差是此精度的倒数：\n$$\n\\Sigma_{\\mathrm{Lap}} = \\left( \\frac{1}{\\sigma_0^2} - \\frac{2y}{\\sigma^2} \\right)^{-1} = \\left( \\frac{\\sigma^2 - 2y\\sigma_0^2}{\\sigma_0^2\\sigma^2} \\right)^{-1} = \\frac{\\sigma_0^2\\sigma^2}{\\sigma^2 - 2y\\sigma_0^2}\n$$\n\n**第3部分：变分贝叶斯协方差 $\\Sigma_{\\mathrm{VB}}$**\n\n高斯变分贝叶斯近似考虑一族高斯分布 $q(x) = \\mathcal{N}(\\mu, v)$，并寻求最小化KL散度 $\\mathrm{KL}(q(x) \\| p(x|y))$。问题指定使用前向映射 $G(x) = x^2$ 在变分均值 $\\mu$ 周圍的线性化：\n$$\nG(x) \\approx G(\\mu) + G'(\\mu)(x-\\mu)\n$$\n由于 $G'(\\mu) = 2\\mu$，线性化为 $G(x) \\approx \\mu^2 + 2\\mu(x-\\mu)$。\n我们将此代入似然项。近似的负对数后验，我们记为 $\\tilde{\\Phi}(x; \\mu)$，然后变为：\n$$\n\\tilde{\\Phi}(x; \\mu) = \\frac{(y - (\\mu^2 + 2\\mu(x-\\mu)))^2}{2\\sigma^2} + \\frac{x^2}{2\\sigma_0^2}\n$$\n该表达式是关于 $x$ 的二次式。相应的高斯后验近似的精度由 $\\frac{1}{2}x^2$ 的系数给出。展开似然中的项：\n$$\n\\frac{(y - \\mu^2 + 2\\mu^2 - 2\\mu x)^2}{2\\sigma^2} = \\frac{((y+\\mu^2) - 2\\mu x)^2}{2\\sigma^2} = \\frac{1}{2\\sigma^2}((y+\\mu^2)^2 - 4\\mu(y+\\mu^2)x + 4\\mu^2 x^2)\n$$\n包含 $x^2$ 的项是 $\\frac{4\\mu^2 x^2}{2\\sigma^2} = \\frac{1}{2}x^2\\left(\\frac{4\\mu^2}{\\sigma^2}\\right)$。\n将近似似然和先验中的二次项相加，$\\tilde{\\Phi}(x; \\mu)$ 中的总二次项为 $\\frac{1}{2}x^2\\left(\\frac{4\\mu^2}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right)$。因此，此VB后验近似的精度，作为线性化点 $\\mu$ 的函数，是：\n$$\nv^{-1}(\\mu) = \\frac{4\\mu^2}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\n$$\n问题要求计算在 $\\mu = x_{\\mathrm{MAP}} = 0$ 处求值的VB协方差。代入 $\\mu=0$：\n$$\n\\Sigma_{\\mathrm{VB}}^{-1} = v^{-1}(0) = \\frac{4(0)^2}{\\sigma^2} + \\frac{1}{\\sigma_0^2} = \\frac{1}{\\sigma_0^2}\n$$\nVB协方差 $\\Sigma_{\\mathrm{VB}}$ 是此精度的倒数：\n$$\n\\Sigma_{\\mathrm{VB}} = \\left(\\frac{1}{\\sigma_0^2}\\right)^{-1} = \\sigma_0^2\n$$\n这个结果表明，当这个特定的VB近似以 $\\mu=0$ 为中心时，其协方差恢复为先验协方差。这是因为前向映射的导数 $G'(0)=0$，所以线性化模型显示出对 $x$ 没有依赖性，数据没有提供关于 $x$ 精度的信息。\n\n**第4部分：比率 $\\rho = \\Sigma_{\\mathrm{VB}}/\\Sigma_{\\mathrm{Lap}}$**\n\n我们现在计算两个推导出的协方差的比率。\n$$\n\\rho = \\frac{\\Sigma_{\\mathrm{VB}}}{\\Sigma_{\\mathrm{Lap}}} = \\Sigma_{\\mathrm{VB}} \\cdot \\Sigma_{\\mathrm{Lap}}^{-1}\n$$\n代入我们找到的表达式：\n$$\n\\rho = (\\sigma_0^2) \\cdot \\left(\\frac{1}{\\sigma_0^2} - \\frac{2y}{\\sigma^2}\\right)\n$$\n將 $\\sigma_0^2$ 乘入括号：\n$$\n\\rho = \\sigma_0^2 \\cdot \\frac{1}{\\sigma_0^2} - \\sigma_0^2 \\cdot \\frac{2y}{\\sigma^2}\n$$\n$$\n\\rho = 1 - \\frac{2y\\sigma_0^2}{\\sigma^2}\n$$\n这就是比率 $\\rho$ 作为 $\\sigma_0^2$、$\\sigma^2$ 和 $y$ 的函数的最终闭式表达式。注意，给定的条件 $0  2y  \\sigma^2/\\sigma_0^2$ 意味着 $0  \\frac{2y\\sigma_0^2}{\\sigma^2}  1$，这确保了 $0  \\rho  1$。这表明，对于此问题，拉普拉斯近似正确地估计了一个比先验方差小的后验方差（由于模型看到了非零的y），而VB近似则错误地将后验方差估计为与先验方差相等。",
            "answer": "$$\\boxed{1 - \\frac{2y\\sigma_{0}^{2}}{\\sigma^{2}}}$$"
        },
        {
            "introduction": "这个综合性练习将理论付诸实践，要求我们为一个由偏微分方程（PDE）约束的逆问题构建并实施一个完整的变分贝叶斯解决方案。推断空间分布的参数场是地球物理、医学成像等众多科学领域的常见挑战。此练习将指导你结合有限元方法来离散化连续问题，为函数参数设置高斯马尔可夫随机场（GMRF）先验，并最终计算证据下界（ELBO）以评估模型 。通过完成这个练习，你将获得将变分推断应用于复杂科学模型的宝贵实践经验。",
            "id": "3430113",
            "problem": "考虑一个在单位区间上的一维偏微分方程中的分布式参数场的反问题。设空间域为 $[0,1]$，离散为 $N$ 个均匀单元，网格宽度为 $h = 1/N$，节点为 $s_j = j h$，其中 $j \\in \\{0,1,\\dots,N\\}$。正向模型是具有齐次狄利克雷边界条件的泊松方程的弱形式：寻找状态 $u \\in H_0^1([0,1])$，使得对于所有测试函数 $v \\in H_0^1([0,1])$，都有\n$$\n\\int_0^1 u'(s) v'(s) \\, ds = \\int_0^1 x(s) v(s) \\, ds,\n$$\n其中 $x$ 是一个待从数据中推断的分布式参数场。观测值是 $u$ 的内部节点值，收集到一个向量 $y \\in \\mathbb{R}^m$ 中，其中 $m = N-1$，带有方差为 $\\sigma^2$ 的附加独立高斯噪声，因此似然函数为 $p(y \\mid c) = \\mathcal{N}(y \\mid H c, R)$，其中 $R = \\sigma^2 I_m$，$c \\in \\mathbb{R}^{n}$ 是参数 $x$ 在维度为 $n = N+1$ 的有限元基中的系数。\n\n将参数 $x$ 表示在一个连续分片线性有限元(FE)基 $\\{\\psi_j\\}_{j=0}^N$ 中，即 $x(s) \\approx \\sum_{j=0}^N c_j \\psi_j(s)$，并在相同的网格上使用满足 $s=0$ 和 $s=1$ 处齐次狄利克雷边界条件的有限元基 $\\{\\phi_i\\}_{i=1}^{N-1}$ 来离散化状态 $u$。伽辽金离散化为 $u$ 的内部自由度产生了一个线性系统，\n$$\nK_u \\, u = M_{ux} \\, c,\n$$\n其中 $K_u \\in \\mathbb{R}^{(N-1)\\times(N-1)}$ 是 $u$ 的刚度矩阵，$M_{ux} \\in \\mathbb{R}^{(N-1)\\times(N+1)}$ 是从 $x$ 到 $u$ 的矩形质量耦合矩阵，定义为\n$$\n(K_u)_{ij} = \\int_0^1 \\phi_i'(s)\\phi_j'(s)\\,ds, \\quad (M_{ux})_{ij} = \\int_0^1 \\phi_i(s)\\psi_j(s)\\,ds.\n$$\n令观测算子为 $u$ 内部节点值上的恒等算子，因此观测矩阵 $P \\in \\mathbb{R}^{m \\times (N-1)}$ 是单位矩阵，从 $c$ 到观测值均值的前向映射为\n$$\nH = P \\, K_u^{-1} \\, M_{ux} \\in \\mathbb{R}^{m \\times n}.\n$$\n\n对 $c$ 施加一个零均值、精度矩阵为 $A_{\\mathrm{prior}} = \\tau K_x + \\delta M_x$ 的高斯先验，其中 $K_x \\in \\mathbb{R}^{n \\times n}$ 和 $M_x \\in \\mathbb{R}^{n \\times n}$ 是在完整网格上针对基 $\\{\\psi_j\\}_{j=0}^N$ 的标准有限元刚度矩阵和质量矩阵，$\\tau  0, \\delta  0$ 是固定的超参数。即 $p(c) = \\mathcal{N}(0, A_{\\mathrm{prior}}^{-1})$。该先验对应于一个高斯马尔可夫随机场(GMRF)，其平滑度由 $\\tau$ 控制，局部方差由 $\\delta$ 控制。\n\n使用变分贝叶斯(VB)方法，在全协方差高斯密度族 $q(c) = \\mathcal{N}(m, C)$ 内近似后验分布 $p(c \\mid y)$，其中 $m \\in \\mathbb{R}^n$，$C \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。从以下基本定义开始：\n- 后验分布的贝叶斯法则 $p(c \\mid y) \\propto p(y \\mid c) p(c)$。\n- Kullback–Leibler 散度 $\\mathrm{KL}(q \\| p) = \\mathbb{E}_q[\\log q(c)] - \\mathbb{E}_q[\\log p(c \\mid y)]$。\n- 证据下界 (ELBO)，$\\mathcal{L}(q) = \\mathbb{E}_q[\\log p(y \\mid c)] + \\mathbb{E}_q[\\log p(c)] - \\mathbb{E}_q[\\log q(c)]$，对于任何密度 $q$ 都满足 $\\log p(y) \\ge \\mathcal{L}(q)$。\n\n你的任务：\n- 从第一性原理和上述定义出发，推导此线性高斯反问题的高斯变分近似 $q(c) = \\mathcal{N}(m, C)$，并完全用有限元系统矩阵 $K_u, M_{ux}, K_x, M_x$、超参数 $\\tau, \\delta$ 和噪声方差 $\\sigma^2$ 表示。你必须展示如何在有限元基中表示 $q(c)$，以及如何通过组装好的有限元质量和刚度矩阵计算所有必要的内积，除了上述给出的定义外，不使用任何预先推导的后验分布闭式恒等式。\n- 用 $m, C$、先验精度矩阵 $A_{\\mathrm{prior}}$、前向算子 $H$、观测协方差 $R$ 和观测数据 $y$ 显式地构造 ELBO $\\mathcal{L}(q)$。你的推导必须仅使用线性代数、多元高斯密度的定义以及有限元内积的标准性质。\n- 实现一个程序，该程序为 $[0,1]$ 上的均匀网格和连续分片线性基函数组装有限元矩阵，计算前向算子 $H$，构造高斯变分近似 $q(c)$，并评估 ELBO。\n\n所有测试用例的数据生成协议：\n- 对所有 $s \\in [0,1]$，使用制造的真值 $x_{\\mathrm{true}}(s) = \\sin(2\\pi s)$。\n- 通过节点插值在有限元基中表示 $x_{\\mathrm{true}}$，即对所有 $j \\in \\{0,\\dots,N\\}$，设置 $c_{\\mathrm{true}, j} = x_{\\mathrm{true}}(s_j)$。\n- 生成零噪声实现的合成观测值，即精确设置 $y = H c_{\\mathrm{true}}$。\n- 在内部节点上使用恒等观测算子，因此观测维度为 $m = N-1$。\n- 所有角度必须以弧度为单位。\n- 本问题中没有物理单位；所有量均为无量纲。\n\n测试套件：\n- 案例A（理想路径）：$N = 8$，$\\tau = 1.0$，$\\delta = 10^{-2}$，$\\sigma = 0.1$。\n- 案例B（小网格边界情况）：$N = 3$，$\\tau = 0.5$，$\\delta = 0.05$，$\\sigma = 0.2$。\n- 案例C（高平滑和高噪声）：$N = 12$，$\\tau = 5.0$，$\\delta = 10^{-3}$，$\\sigma = 0.5$。\n\n对于每种情况，计算上述构造的高斯变分近似 $q(c)$ 的 ELBO 值 $\\mathcal{L}(q)$。你的程序必须：\n- 从有限元内积组装 $K_u, M_{ux}, K_x, M_x$。\n- 构建 $H = K_u^{-1} M_{ux}$，$A_{\\mathrm{prior}} = \\tau K_x + \\delta M_x$ 和 $R = \\sigma^2 I$。\n- 通过在高斯族内最小化 Kullback–Leibler 散度来构造 $q(c) = \\mathcal{N}(m, C)$。\n- 根据其定义的期望项精确评估 $\\mathcal{L}(q)$。\n\n最终输出规范：\n- 你的程序应生成单行输出，其中包含测试案例A、案例B和案例C的三个ELBO值，按此顺序排列，形式为用方括号括起来的逗号分隔列表，每个值四舍五入到六位小数。例如，使用占位符数字的输出将如下所示：$[1.234000,-0.567890,2.000000]$。",
            "solution": "该问题要求推导并实现一个在线性高斯反问题中的变分贝叶斯(VB)近似，该问题在有限元(FE)框架下构建。目标是用一个易于处理的高斯密度 $q(c) = \\mathcal{N}(c \\mid m, C)$ 来近似后验分布 $p(c \\mid y)$，并计算此近似的证据下界(ELBO)。\n\n首先，我们通过最大化ELBO $\\mathcal{L}(q)$ 来推导变分分布 $q(c)$ 的最优参数 $m$ 和 $C$。ELBO的定义为：\n$$\n\\mathcal{L}(q) = \\mathbb{E}_q[\\log p(y \\mid c)] + \\mathbb{E}_q[\\log p(c)] - \\mathbb{E}_q[\\log q(c)]\n$$\n问题指定了高斯似然 $p(y \\mid c) = \\mathcal{N}(y \\mid Hc, R)$，零均值高斯先验 $p(c) = \\mathcal{N}(c \\mid 0, A_{\\mathrm{prior}}^{-1})$，以及高斯变分族 $q(c) = \\mathcal{N}(c \\mid m, C)$。对数概率密度函数(log-PDFs)在忽略加性常数的情况下为：\n$$\n\\log p(y \\mid c) \\propto -\\frac{1}{2}(y - Hc)^T R^{-1} (y - Hc)\n$$\n$$\n\\log p(c) \\propto -\\frac{1}{2}c^T A_{\\mathrm{prior}} c\n$$\n$$\n\\log q(c) \\propto -\\frac{1}{2}(c - m)^T C^{-1} (c - m)\n$$\n为了关于变分参数 $m$ 和 $C$ 最大化ELBO，我们可以分析 $\\mathcal{L}(q)$ 中依赖于它们的项。将对数似然和对数先验项在期望内合并，得到 $\\mathbb{E}_q[\\log p(y,c)] = \\mathbb{E}_q[\\log p(y \\mid c) + \\log p(c)]$。期望内的表达式为：\n$$\n\\log p(y \\mid c) + \\log p(c) \\propto -\\frac{1}{2} \\left[ (y - Hc)^T R^{-1} (y - Hc) + c^T A_{\\mathrm{prior}} c \\right]\n$$\n$$\n= -\\frac{1}{2} \\left[ y^T R^{-1} y - 2y^T R^{-1} Hc + c^T H^T R^{-1} Hc + c^T A_{\\mathrm{prior}} c \\right]\n$$\n$$\n= -\\frac{1}{2} \\left[ c^T (H^T R^{-1} H + A_{\\mathrm{prior}}) c - 2y^T R^{-1} Hc \\right] + \\text{const.}\n$$\n最大化ELBO等价于最小化Kullback-Leibler散度 $\\mathrm{KL}(q \\| p)$，这涉及将 $q(c)$ 拟合到真实后验 $p(c \\mid y) \\propto p(y \\mid c) p(c)$。后验对数密度是关于 $c$ 的二次型，这意味着后验是高斯分布。最优的 $q(c)$ 将精确匹配此后验。$c$ 的二次项和线性项决定了这个高斯分布的参数。后验的精度（逆协方差）矩阵是二次项 $c^T(\\cdot)c$ 的系数，其均值与线性项有关。\n从上述表达式可以清楚地看出，后验的精度矩阵是 $C^{-1} = H^T R^{-1} H + A_{\\mathrm{prior}}$。\n\n为了找到均值 $m$，我们可以对包含 $c$ 的项进行配方。方括号内的表达式形式为 $c^T C^{-1} c - 2b^T c$，其中 $b = H^T R^{-1} y$。配方后得到 $(c-m)^T C^{-1} (c-m) - m^T C^{-1} m$，其中 $C^{-1}m = b$。\n因此，最优均值 $m$ 满足：\n$$\n(H^T R^{-1} H + A_{\\mathrm{prior}}) m = H^T R^{-1} y\n$$\n$$\nm = (H^T R^{-1} H + A_{\\mathrm{prior}})^{-1} H^T R^{-1} y\n$$\n这定义了最优变分分布 $q(c) = \\mathcal{N}(m, C)$，其中：\n$$\nC = (H^T R^{-1} H + A_{\\mathrm{prior}})^{-1}\n$$\n$$\nm = C (H^T R^{-1} y)\n$$\n这些是高斯近似 $q(c)$ 的参数。由于真实后验是高斯分布，此变分近似是精确的，即 $q(c) = p(c \\mid y)$。\n\n接下来，我们使用已确定的最优参数推导ELBO $\\mathcal{L}(q)$ 的显式公式。我们必须完整地评估ELBO定义中的每一项期望，包括所有归一化常数。\n\n对数概率密度函数为：\n$$\n\\log p(y \\mid c) = -\\frac{m_{\\mathrm{dim}}}{2} \\log(2\\pi) - \\frac{1}{2} \\log \\det(R) - \\frac{1}{2}(y - Hc)^T R^{-1} (y - Hc)\n$$\n$$\n\\log p(c) = -\\frac{n}{2} \\log(2\\pi) + \\frac{1}{2} \\log \\det(A_{\\mathrm{prior}}) - \\frac{1}{2}c^T A_{\\mathrm{prior}} c\n$$\n$$\n\\log q(c) = -\\frac{n}{2} \\log(2\\pi) - \\frac{1}{2} \\log \\det(C) - \\frac{1}{2}(c - m)^T C^{-1} (c - m)\n$$\n其中 $n=N+1$ 是参数向量 $c$ 的维度，$m_{\\mathrm{dim}}=N-1$ 是观测向量 $y$ 的维度。\n\n我们计算每一项关于 $q(c) = \\mathcal{N}(c \\mid m, C)$ 的期望。\n1. 对数似然的期望：\n使用性质 $\\mathbb{E}_q[(z-Ac)^T B (z-Ac)] = (z-Am)^T B (z-Am) + \\mathrm{Tr}(A^T B A C)$，我们得到：\n$$\n\\mathbb{E}_q[\\log p(y \\mid c)] = -\\frac{m_{\\mathrm{dim}}}{2}\\log(2\\pi) - \\frac{1}{2}\\log\\det(R) - \\frac{1}{2} \\left[ (y - Hm)^T R^{-1} (y - Hm) + \\mathrm{Tr}(H^T R^{-1} H C) \\right]\n$$\n2. 对数先验的期望：\n使用性质 $\\mathbb{E}_q[c^T A c] = m^T A m + \\mathrm{Tr}(AC)$，我们得到：\n$$\n\\mathbb{E}_q[\\log p(c)] = -\\frac{n}{2}\\log(2\\pi) + \\frac{1}{2}\\log\\det(A_{\\mathrm{prior}}) - \\frac{1}{2} \\left[ m^T A_{\\mathrm{prior}} m + \\mathrm{Tr}(A_{\\mathrm{prior}} C) \\right]\n$$\n3. 负对数变分密度的期望（$q$ 的熵）：\n$$\n-\\mathbb{E}_q[\\log q(c)] = \\frac{n}{2}\\log(2\\pi) + \\frac{1}{2}\\log\\det(C) + \\frac{1}{2}\\mathbb{E}_q[(c - m)^T C^{-1} (c - m)]\n$$\n期望项计算为 $\\mathbb{E}_q[\\mathrm{Tr}(C^{-1}(c-m)(c-m)^T)] = \\mathrm{Tr}(C^{-1}\\mathbb{E}_q[(c-m)(c-m)^T]) = \\mathrm{Tr}(C^{-1}C) = \\mathrm{Tr}(I_n) = n$。\n所以，$-\\mathbb{E}_q[\\log q(c)] = \\frac{n}{2}\\log(2\\pi) + \\frac{1}{2}\\log\\det(C) + \\frac{n}{2}$。\n\n将这三部分相加得到 ELBO $\\mathcal{L}(q)$：\n$$\n\\mathcal{L}(q) = \\left(-\\frac{m_{\\mathrm{dim}}}{2}\\log(2\\pi) - \\frac{1}{2}\\log\\det R - \\frac{1}{2}(y - Hm)^T R^{-1} (y - Hm) - \\frac{1}{2}\\mathrm{Tr}(H^T R^{-1} H C)\\right)\n$$\n$$\n+ \\left(-\\frac{n}{2}\\log(2\\pi) + \\frac{1}{2}\\log\\det A_{\\mathrm{prior}} - \\frac{1}{2}m^T A_{\\mathrm{prior}} m - \\frac{1}{2}\\mathrm{Tr}(A_{\\mathrm{prior}} C)\\right)\n$$\n$$\n+ \\left(\\frac{n}{2}\\log(2\\pi) + \\frac{1}{2}\\log\\det C + \\frac{n}{2}\\right)\n$$\n合并各项，$\\pm \\frac{n}{2}\\log(2\\pi)$ 项抵消。我们也可以组合迹项：\n$$\n-\\frac{1}{2}\\mathrm{Tr}(H^T R^{-1} H C) - \\frac{1}{2}\\mathrm{Tr}(A_{\\mathrm{prior}} C) = -\\frac{1}{2}\\mathrm{Tr}((H^T R^{-1} H + A_{\\mathrm{prior}})C) = -\\frac{1}{2}\\mathrm{Tr}(C^{-1}C) = -\\frac{n}{2}\n$$\nELBO 简化为：\n$$\n\\mathcal{L}(q) = -\\frac{m_{\\mathrm{dim}}}{2}\\log(2\\pi) - \\frac{1}{2}\\log\\det R - \\frac{1}{2}(y - Hm)^T R^{-1} (y - Hm) - \\frac{1}{2}m^T A_{\\mathrm{prior}} m + \\frac{1}{2}\\log\\det A_{\\mathrm{prior}} + \\frac{1}{2}\\log\\det C\n$$\n这是将要实现的ELBO的最终表达式。由于 $R = \\sigma^2 I_{m_{\\mathrm{dim}}}$，我们有 $R^{-1} = (1/\\sigma^2)I_{m_{\\mathrm{dim}}}$ 和 $\\log\\det R = m_{\\mathrm{dim}}\\log(\\sigma^2) = 2m_{\\mathrm{dim}}\\log\\sigma$。\n\n有限元矩阵是为一维均匀网格上的分片线性（“帽子”）基函数组装的。对于单元尺寸 $h=1/N$，一个单元上的局部刚度矩阵和质量矩阵分别是 $K^e = \\frac{1}{h}\\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix}$ 和 $M^e = \\frac{h}{6}\\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}$。这些被组装成全局矩阵。\n- $K_x$ 和 $M_x$：作用于所有 $n=N+1$ 个节点的基函数的标准刚度矩阵和质量矩阵。\n- $K_u$：状态空间的刚度矩阵，对应于内部节点。这是 $K_x$ 中对应于节点 $1, \\dots, N-1$ 的子矩阵。\n- $M_{ux}$：耦合参数和状态空间的质量矩阵。这对应于 $M_x$ 中对应内部节点的行和所有节点的列。\n利用这些矩阵，我们构造 $A_{\\mathrm{prior}} = \\tau K_x + \\delta M_x$ 和 $H = K_u^{-1} M_{ux}$，然后继续计算变分参数和ELBO值。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef assemble_fem_matrices(N):\n    \"\"\"\n    Assembles the 1D Finite Element matrices for a uniform mesh.\n    \n    Args:\n        N (int): The number of uniform elements on the interval [0, 1].\n\n    Returns:\n        tuple: A tuple containing (Kx, Mx, Ku, Mux).\n            Kx (ndarray): Full stiffness matrix for parameters (n x n).\n            Mx (ndarray): Full mass matrix for parameters (n x n).\n            Ku (ndarray): Stiffness matrix for state with Dirichlet BCs (m x m).\n            Mux (ndarray): Coupling mass matrix (m x n).\n    \"\"\"\n    h = 1.0 / N\n    n = N + 1\n    m_dim = N - 1\n\n    # Assemble full stiffness matrix Kx\n    Kx = np.zeros((n, n), dtype=float)\n    diag_main = np.full(n, 2.0 / h)\n    diag_off = np.full(n - 1, -1.0 / h)\n    Kx += np.diag(diag_main)\n    Kx += np.diag(diag_off, k=1)\n    Kx += np.diag(diag_off, k=-1)\n    # Boundary nodes\n    Kx[0, 0] = 1.0 / h\n    Kx[n - 1, n - 1] = 1.0 / h\n    \n    # Assemble full mass matrix Mx\n    Mx = np.zeros((n, n), dtype=float)\n    diag_main = np.full(n, 4.0 * h / 6.0)\n    diag_off = np.full(n - 1, 1.0 * h / 6.0)\n    Mx += np.diag(diag_main)\n    Mx += np.diag(diag_off, k=1)\n    Mx += np.diag(diag_off, k=-1)\n    # Boundary nodes\n    Mx[0, 0] = 2.0 * h / 6.0\n    Mx[n - 1, n - 1] = 2.0 * h / 6.0\n\n    # Extract submatrices for the state space\n    # Interior nodes are indexed 1 to N-1\n    if m_dim > 0:\n        Ku = Kx[1:N, 1:N].copy()\n        Mux = Mx[1:N, :].copy()\n    else: # Case N=1\n        Ku = np.array([[]])\n        Mux = np.array([[]])\n\n    return Kx, Mx, Ku, Mux\n\n\ndef compute_elbo(N, tau, delta, sigma):\n    \"\"\"\n    Computes the ELBO for the variational approximation.\n    \n    Args:\n        N (int): Number of elements.\n        tau (float): Prior hyperparameter for stiffness.\n        delta (float): Prior hyperparameter for mass.\n        sigma (float): Standard deviation of observation noise.\n\n    Returns:\n        float: The computed ELBO value.\n    \"\"\"\n    n = N + 1\n    m_dim = N - 1\n\n    # 1. Assemble FE matrices\n    Kx, Mx, Ku, Mux = assemble_fem_matrices(N)\n\n    # Handle N=1, where m_dim=0, so y is empty\n    if m_dim = 0:\n        # Simplified case where there are no observations\n        # The posterior is the prior, q=p, ELBO=log(marginal_prior)\n        # This case is not tested but handled for completeness.\n        A_prior_empty = tau * Kx + delta * Mx\n        (sign_Ap, logdet_Ap) = np.linalg.slogdet(A_prior_empty)\n        # C = inv(A_prior) so logdet(C) = -logdet(A_prior)\n        elbo = 0.5 * logdet_Ap + 0.5 * (-logdet_Ap) # = 0\n        return 0.0\n\n\n    # 2. Construct operators for the Bayesian model\n    A_prior = tau * Kx + delta * Mx\n    # R is sigma^2 * I_m, R_inv is (1/sigma^2) * I_m\n    \n    # Forward operator H = K_u^{-1} M_ux\n    Ku_inv = np.linalg.inv(Ku)\n    H = Ku_inv @ Mux\n\n    # 3. Generate synthetic data\n    s_nodes = np.linspace(0, 1, n)\n    c_true = np.sin(2 * np.pi * s_nodes)\n    y = H @ c_true\n\n    # 4. Compute optimal variational parameters m and C\n    # C_inv = H^T R^{-1} H + A_prior\n    # R_inv is a scalar matrix (1/sigma^2) * I\n    H_T_Rinv_H = (1 / sigma**2) * (H.T @ H)\n    C_inv = H_T_Rinv_H + A_prior\n    C = np.linalg.inv(C_inv)\n    \n    # m = C H^T R^{-1} y\n    H_T_Rinv_y = (1 / sigma**2) * (H.T @ y)\n    m_var = C @ H_T_Rinv_y\n\n    # 5. Evaluate the ELBO\n    # ELBO = -m_dim/2*log(2pi) - 1/2*logdet(R) - 1/2*(y-Hm)^T R^-1 (y-Hm)\n    #        - 1/2*m^T A_prior m + 1/2*logdet(A_prior) + 1/2*logdet(C)\n    \n    # Term 1: -m_dim/2 * log(2*pi)\n    term1 = -m_dim / 2.0 * np.log(2 * np.pi)\n    \n    # Term 2: -1/2 * logdet(R) = -1/2 * m_dim * log(sigma^2)\n    log_det_R = m_dim * np.log(sigma**2)\n    term2 = -0.5 * log_det_R\n    \n    # Term 3: -1/2 * (y - Hm)^T R^-1 (y-Hm)\n    residual = y - H @ m_var\n    term3 = -0.5 * (1 / sigma**2) * (residual.T @ residual)\n    \n    # Term 4: -1/2 * m^T A_prior m\n    term4 = -0.5 * (m_var.T @ A_prior @ m_var)\n    \n    # Term 5: 1/2 * logdet(A_prior)\n    sign_Ap, logdet_Ap = np.linalg.slogdet(A_prior)\n    if sign_Ap = 0:\n        raise ValueError(\"A_prior is not positive definite.\")\n    term5 = 0.5 * logdet_Ap\n    \n    # Term 6: 1/2 * logdet(C)\n    sign_C, logdet_C = np.linalg.slogdet(C)\n    if sign_C = 0:\n        raise ValueError(\"C is not positive definite.\")\n    term6 = 0.5 * logdet_C\n\n    elbo = term1 + term2 + term3 + term4 + term5 + term6\n    \n    return elbo\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'N': 8, 'tau': 1.0, 'delta': 10**-2, 'sigma': 0.1},      # Case A\n        {'N': 3, 'tau': 0.5, 'delta': 0.05, 'sigma': 0.2},       # Case B\n        {'N': 12, 'tau': 5.0, 'delta': 10**-3, 'sigma': 0.5},    # Case C\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_elbo(\n            N=case['N'],\n            tau=case['tau'],\n            delta=case['delta'],\n            sigma=case['sigma']\n        )\n        results.append(f\"{result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}