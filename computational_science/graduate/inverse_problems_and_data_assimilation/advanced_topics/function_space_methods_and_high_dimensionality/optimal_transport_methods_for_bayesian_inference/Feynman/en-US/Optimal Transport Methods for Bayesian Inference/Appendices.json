{
    "hands_on_practices": [
        {
            "introduction": "To begin our hands-on exploration, we will start with the fundamental building block of optimal transport: the discrete case. Before tackling the complexities of continuous distributions, it is crucial to understand how transport plans work in a finite setting. This exercise  guides you through formulating the classic Kantorovich problem as a linear program, translating the abstract concept of a coupling into a concrete optimization problem that can be solved to find the most efficient way to redistribute mass between two discrete probability distributions.",
            "id": "3408143",
            "problem": "Consider Bayesian inference in a discrete setting in which a discrete prior distribution supported on particles $\\{x_i\\}_{i=1}^{m}$ with probabilities $\\{p_i\\}_{i=1}^{m}$ is updated to a discrete posterior distribution supported on particles $\\{y_j\\}_{j=1}^{n}$ with probabilities $\\{q_j\\}_{j=1}^{n}$. Under the quadratic transportation cost, the Bayesian update can be represented by an optimal transport plan that couples the prior to the posterior by solving a linear program.\n\nTask 1 (formulation): Starting only from the Kantorovich formulation of optimal transport, write the linear program that seeks a coupling matrix $\\Gamma=\\{\\gamma_{ij}\\} \\in \\mathbb{R}_{+}^{m\\times n}$ minimizing the expected quadratic cost, subject to mass conservation with respect to the prior and posterior probabilities. Clearly state the objective and constraints in mathematical form.\n\nTask 2 (computation on a small example): Let $m=3$ and $n=2$. Consider one-dimensional particles and probabilities\n- prior support $\\{x_1,x_2,x_3\\}=\\{0,1,3\\}$ with probabilities $\\{p_1,p_2,p_3\\}=\\left\\{\\frac{1}{2},\\frac{1}{3},\\frac{1}{6}\\right\\}$,\n- posterior support $\\{y_1,y_2\\}=\\left\\{\\frac{1}{2},\\frac{5}{2}\\right\\}$ with probabilities $\\{q_1,q_2\\}=\\left\\{\\frac{2}{3},\\frac{1}{3}\\right\\}$.\n\nUse the quadratic cost $c_{ij}=\\lVert x_i-y_j \\rVert^{2}$ on $\\mathbb{R}$. Compute an optimal coupling $\\Gamma^{\\star}$ for this instance by solving the linear program from Task 1. Then compute the resulting optimal total quadratic transport cost $\\sum_{i=1}^{3}\\sum_{j=1}^{2}\\gamma^{\\star}_{ij}\\,\\lVert x_i-y_j \\rVert^{2}$.\n\nAnswer specification: Report only the optimal total quadratic transport cost as an exact value in simplest fractional form. Do not round. Do not include units or any additional text in your final answer.",
            "solution": "The problem is valid as it is scientifically grounded in the theory of optimal transport, is well-posed with all necessary information provided, and is expressed objectively. The problem consists of two tasks: first, to formulate a linear program for the discrete optimal transport problem, and second, to solve a specific instance of this problem.\n\n**Task 1: Formulation of the Linear Program**\n\nThe problem of finding an optimal transport plan between two discrete probability measures is known as the Kantorovich problem. Let the prior measure be $\\mu = \\sum_{i=1}^{m} p_i \\delta_{x_i}$ and the posterior measure be $\\nu = \\sum_{j=1}^{n} q_j \\delta_{y_j}$, where $\\delta_z$ denotes the Dirac delta measure at point $z$. The probabilities satisfy $\\sum_{i=1}^{m} p_i = 1$ and $\\sum_{j=1}^{n} q_j = 1$.\n\nA coupling, or transport plan, between $\\mu$ and $\\nu$ is a measure on the product space whose marginals are $\\mu$ and $\\nu$. In the discrete case, this corresponds to a matrix $\\Gamma = \\{\\gamma_{ij}\\} \\in \\mathbb{R}^{m \\times n}$, where $\\gamma_{ij}$ represents the amount of mass transported from location $x_i$ to location $y_j$. For $\\Gamma$ to be a valid coupling, it must satisfy the marginal constraints:\n1.  The sum of mass transported from $x_i$ must equal the mass $p_i$ at that point. This corresponds to the row sums of $\\Gamma$:\n    $$ \\sum_{j=1}^{n} \\gamma_{ij} = p_i, \\quad \\text{for } i = 1, 2, \\ldots, m $$\n2.  The sum of mass transported to $y_j$ must equal the mass $q_j$ at that point. This corresponds to the column sums of $\\Gamma$:\n    $$ \\sum_{i=1}^{m} \\gamma_{ij} = q_j, \\quad \\text{for } j = 1, 2, \\ldots, n $$\n3.  The transported mass cannot be negative:\n    $$ \\gamma_{ij} \\ge 0, \\quad \\text{for all } i, j $$\n\nThe cost of transporting a unit of mass from $x_i$ to $y_j$ is given by the squared Euclidean distance, $c_{ij} = \\lVert x_i - y_j \\rVert^2$. The total transportation cost is the expected cost with respect to the coupling $\\Gamma$. The goal is to find the coupling $\\Gamma^{\\star}$ that minimizes this total cost.\n\nThis problem can be formulated as a linear program:\n\n**Objective Function:**\nMinimize the total transport cost $Z$:\n$$ \\min_{\\Gamma} Z = \\sum_{i=1}^{m} \\sum_{j=1}^{n} c_{ij} \\gamma_{ij} $$\n\n**Constraints:**\nSubject to:\n$$\n\\begin{align*}\n\\sum_{j=1}^{n} \\gamma_{ij} &= p_i, & \\forall i \\in \\{1, \\ldots, m\\} \\\\\n\\sum_{i=1}^{m} \\gamma_{ij} &= q_j, & \\forall j \\in \\{1, \\ldots, n\\} \\\\\n\\gamma_{ij} &\\ge 0, & \\forall i \\in \\{1, \\ldots, m\\}, \\forall j \\in \\{1, \\ldots, n\\}\n\\end{align*}\n$$\n\n**Task 2: Computation on a Small Example**\n\nWe are given the following instance:\n-   Prior support $\\{x_1, x_2, x_3\\} = \\{0, 1, 3\\}$ with probabilities $\\{p_1, p_2, p_3\\} = \\{\\frac{1}{2}, \\frac{1}{3}, \\frac{1}{6}\\}$.\n-   Posterior support $\\{y_1, y_2\\} = \\{\\frac{1}{2}, \\frac{5}{2}\\}$ with probabilities $\\{q_1, q_2\\} = \\{\\frac{2}{3}, \\frac{1}{3}\\}$.\n\nFirst, we compute the cost matrix $C = \\{c_{ij}\\}$ where $c_{ij} = (x_i - y_j)^2$:\n$c_{11} = (0 - \\frac{1}{2})^2 = \\frac{1}{4}$\n$c_{12} = (0 - \\frac{5}{2})^2 = \\frac{25}{4}$\n$c_{21} = (1 - \\frac{1}{2})^2 = (\\frac{1}{2})^2 = \\frac{1}{4}$\n$c_{22} = (1 - \\frac{5}{2})^2 = (-\\frac{3}{2})^2 = \\frac{9}{4}$\n$c_{31} = (3 - \\frac{1}{2})^2 = (\\frac{5}{2})^2 = \\frac{25}{4}$\n$c_{32} = (3 - \\frac{5}{2})^2 = (\\frac{1}{2})^2 = \\frac{1}{4}$\n\nThe cost matrix is:\n$$ C = \\begin{pmatrix} \\frac{1}{4} & \\frac{25}{4} \\\\ \\frac{1}{4} & \\frac{9}{4} \\\\ \\frac{25}{4} & \\frac{1}{4} \\end{pmatrix} $$\n\nThe linear program is to minimize $Z = \\frac{1}{4}\\gamma_{11} + \\frac{25}{4}\\gamma_{12} + \\frac{1}{4}\\gamma_{21} + \\frac{9}{4}\\gamma_{22} + \\frac{25}{4}\\gamma_{31} + \\frac{1}{4}\\gamma_{32}$ subject to:\n\\begin{align*}\n\\gamma_{11} + \\gamma_{12} &= \\frac{1}{2} \\\\\n\\gamma_{21} + \\gamma_{22} &= \\frac{1}{3} \\\\\n\\gamma_{31} + \\gamma_{32} &= \\frac{1}{6} \\\\\n\\gamma_{11} + \\gamma_{21} + \\gamma_{31} &= \\frac{2}{3} \\\\\n\\gamma_{12} + \\gamma_{22} + \\gamma_{32} &= \\frac{1}{3} \\\\\n\\gamma_{ij} &\\ge 0\n\\end{align*}\nWe use the row sum constraints to express $\\gamma_{i2}$ in terms of $\\gamma_{i1}$: $\\gamma_{12} = \\frac{1}{2} - \\gamma_{11}$, $\\gamma_{22} = \\frac{1}{3} - \\gamma_{21}$, $\\gamma_{32} = \\frac{1}{6} - \\gamma_{31}$. Substituting these into the objective function:\n$$ Z = \\frac{1}{4}\\gamma_{11} + \\frac{25}{4}(\\frac{1}{2} - \\gamma_{11}) + \\frac{1}{4}\\gamma_{21} + \\frac{9}{4}(\\frac{1}{3} - \\gamma_{21}) + \\frac{25}{4}\\gamma_{31} + \\frac{1}{4}(\\frac{1}{6} - \\gamma_{31}) $$\n$$ Z = (\\frac{1}{4} - \\frac{25}{4})\\gamma_{11} + (\\frac{1}{4} - \\frac{9}{4})\\gamma_{21} + (\\frac{25}{4} - \\frac{1}{4})\\gamma_{31} + \\frac{25}{8} + \\frac{9}{12} + \\frac{1}{24} $$\n$$ Z = -6\\gamma_{11} - 2\\gamma_{21} + 6\\gamma_{31} + \\frac{75+18+1}{24} = -6\\gamma_{11} - 2\\gamma_{21} + 6\\gamma_{31} + \\frac{47}{12} $$\nThe remaining independent constraints are:\n\\begin{align*}\n\\gamma_{11} + \\gamma_{21} + \\gamma_{31} &= \\frac{2}{3} \\\\\n0 \\le \\gamma_{11} \\le \\frac{1}{2} \\\\\n0 \\le \\gamma_{21} \\le \\frac{1}{3} \\\\\n0 \\le \\gamma_{31} \\le \\frac{1}{6}\n\\end{align*}\nTo minimize $Z$, we need to make $\\gamma_{11}$ and $\\gamma_{21}$ as large as possible (due to negative coefficients $-6$ and $-2$) and $\\gamma_{31}$ as small as possible (due to positive coefficient $6$).\nLet's try to set $\\gamma_{31}$ to its minimum value, $\\gamma_{31} = 0$.\nThe constraint becomes $\\gamma_{11} + \\gamma_{21} = \\frac{2}{3}$.\nTo minimize the objective, we prioritize making $\\gamma_{11}$ largest. We set $\\gamma_{11}$ to its upper bound: $\\gamma_{11} = \\frac{1}{2}$.\nThis implies $\\gamma_{21} = \\frac{2}{3} - \\gamma_{11} = \\frac{2}{3} - \\frac{1}{2} = \\frac{1}{6}$.\nLet's check if this solution is feasible:\n- $\\gamma_{11} = \\frac{1}{2}$, which is in $[0, \\frac{1}{2}]$.\n- $\\gamma_{21} = \\frac{1}{6}$, which is in $[0, \\frac{1}{3}]$.\n- $\\gamma_{31} = 0$, which is in $[0, \\frac{1}{6}]$.\nThe solution is feasible and, by our greedy construction based on the objective function's coefficients, it is optimal.\n\nNow we find the remaining components of the optimal coupling matrix $\\Gamma^{\\star}$:\n$\\gamma^{\\star}_{11} = \\frac{1}{2}$\n$\\gamma^{\\star}_{21} = \\frac{1}{6}$\n$\\gamma^{\\star}_{31} = 0$\n$\\gamma^{\\star}_{12} = p_1 - \\gamma^{\\star}_{11} = \\frac{1}{2} - \\frac{1}{2} = 0$\n$\\gamma^{\\star}_{22} = p_2 - \\gamma^{\\star}_{21} = \\frac{1}{3} - \\frac{1}{6} = \\frac{1}{6}$\n$\\gamma^{\\star}_{32} = p_3 - \\gamma^{\\star}_{31} = \\frac{1}{6} - 0 = \\frac{1}{6}$\n\nThe optimal coupling matrix is:\n$$ \\Gamma^{\\star} = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ \\frac{1}{6} & \\frac{1}{6} \\\\ 0 & \\frac{1}{6} \\end{pmatrix} $$\n\nFinally, we compute the optimal total transport cost by substituting $\\Gamma^{\\star}$ into the objective function:\n$$ Z^{\\star} = \\sum_{i=1}^{3}\\sum_{j=1}^{2} c_{ij} \\gamma^{\\star}_{ij} $$\n$$ Z^{\\star} = c_{11}\\gamma^{\\star}_{11} + c_{21}\\gamma^{\\star}_{21} + c_{22}\\gamma^{\\star}_{22} + c_{32}\\gamma^{\\star}_{32} $$\n$$ Z^{\\star} = (\\frac{1}{4})(\\frac{1}{2}) + (\\frac{1}{4})(\\frac{1}{6}) + (\\frac{9}{4})(\\frac{1}{6}) + (\\frac{1}{4})(\\frac{1}{6}) $$\n$$ Z^{\\star} = \\frac{1}{8} + \\frac{1}{24} + \\frac{9}{24} + \\frac{1}{24} $$\n$$ Z^{\\star} = \\frac{3}{24} + \\frac{1+9+1}{24} = \\frac{3}{24} + \\frac{11}{24} = \\frac{14}{24} $$\nSimplifying the fraction gives the optimal cost:\n$$ Z^{\\star} = \\frac{7}{12} $$\nThis result can be verified using the reduced objective function:\n$Z^{\\star} = -6(\\frac{1}{2}) - 2(\\frac{1}{6}) + 6(0) + \\frac{47}{12} = -3 - \\frac{1}{3} + \\frac{47}{12} = -\\frac{10}{3} + \\frac{47}{12} = -\\frac{40}{12} + \\frac{47}{12} = \\frac{7}{12}$.\nThe calculations are consistent.",
            "answer": "$$\\boxed{\\frac{7}{12}}$$"
        },
        {
            "introduction": "Moving from a purely discrete world, we now consider a more realistic scenario common in Bayesian inference: updating a discrete prior belief (represented by particles) to a continuous posterior distribution. This practice  reveals a critical insight into the theory of optimal transport by demonstrating why a simple, deterministic mapping (a Monge map) is insufficient for this task. By working through this problem, you will not only understand the necessity of the more general Kantorovich probabilistic coupling but also learn how to compute the squared Wasserstein-$2$ distance in a continuous one-dimensional setting using quantile functions.",
            "id": "3408183",
            "problem": "Consider a scalar inverse problem in data assimilation where the unknown parameter is $x \\in \\mathbb{R}$. An initial prior belief is represented by the two-point probability measure $\\mu = p\\,\\delta_{-a} + (1-p)\\,\\delta_{a}$ with $a \\in (0,\\infty)$ and $p \\in (0,1)$. Suppose one wishes to employ Optimal Transport (OT) methods to map the prior $\\mu$ into a target posterior distribution $\\nu$ that is absolutely continuous with respect to Lebesgue measure (for example, a Gaussian posterior approximation commonly used in data assimilation). Let the transport cost be the quadratic cost $c(x,y) = (x-y)^{2}$.\n\nTasks:\n- Using only the core definitions of pushforward measure, Dirac measure, and the Monge formulation, show that no deterministic measurable transport map $T:\\mathbb{R}\\to\\mathbb{R}$ can satisfy $T_{\\#}\\mu=\\nu$ when $\\nu$ admits a density. Conclude that the Monge problem has no solution in this setting and that one must instead resort to a Kantorovich transport plan (a probabilistic coupling).\n- Formulate the Kantorovich problem for the pair $(\\mu,\\nu)$ and the cost $c(x,y)=(x-y)^{2}$, and recall that the minimal value of this problem equals the squared Wasserstein-$2$ (W2) distance $W_{2}^{2}(\\mu,\\nu)$.\n- Specialize to the concrete case $p=\\tfrac{1}{2}$, $a=1$, and $\\nu=\\mathcal{N}(0,1)$ (the standard normal distribution). Using only foundational properties of one-dimensional optimal transport (in particular, the monotone rearrangement or quantile representation) and basic calculus, derive a closed-form analytic expression for the minimal Kantorovich cost $W_{2}^{2}(\\mu,\\nu)$.\n\nAnswer specification:\n- Your final answer must be a single closed-form analytic expression.\n- No rounding is required and no units are to be included in the final answer.",
            "solution": "We proceed from first principles appropriate to optimal transport for Bayesian inference, starting with the core definitions of pushforward measures, Monge and Kantorovich formulations, and the one-dimensional monotone rearrangement characterization of the Wasserstein-$2$ (W2) distance.\n\n1. Nonexistence of a Monge transport map from a discrete prior to a continuous posterior.\n- Let $\\mu = p\\,\\delta_{-a} + (1-p)\\,\\delta_{a}$ with $p \\in (0,1)$ and $a \\in (0,\\infty)$. By definition, a Dirac measure $\\delta_{x_{0}}$ assigns unit mass to the point $x_{0}$. The pushforward of a measure $\\mu$ under a measurable map $T:\\mathbb{R}\\to\\mathbb{R}$ is the measure $T_{\\#}\\mu$ defined by $T_{\\#}\\mu(B) = \\mu(T^{-1}(B))$ for all Borel sets $B \\subset \\mathbb{R}$.\n- If $\\mu$ is finitely supported, say $\\mu=\\sum_{i=1}^{m}\\alpha_{i}\\,\\delta_{x_{i}}$ with $\\alpha_{i}>0$, $\\sum_{i}\\alpha_{i}=1$, then for any measurable $T$ we have\n$$\nT_{\\#}\\mu \\;=\\; \\sum_{i=1}^{m}\\alpha_{i}\\,\\delta_{T(x_{i})}.\n$$\nThus $T_{\\#}\\mu$ is also finitely supported with at most $m$ atoms. In particular, if $m=2$, then $T_{\\#}\\mu$ has support of cardinality at most $2$.\n- Suppose the target posterior $\\nu$ is absolutely continuous with respect to Lebesgue measure and admits a density $\\rho_{\\nu}(y)$, so that for every singleton $\\{y_{0}\\}$ we have $\\nu(\\{y_{0}\\})=0$. A finitely supported measure cannot equal an absolutely continuous measure. Therefore there is no measurable map $T$ such that $T_{\\#}\\mu=\\nu$ in this setting.\n- Consequently, the Monge problem, which seeks a deterministic measurable $T$ with $T_{\\#}\\mu=\\nu$ minimizing the transport cost $\\int (x-T(x))^{2}\\,\\mathrm{d}\\mu(x)$, has no solution. One must instead use the Kantorovich formulation, which allows mass splitting via couplings.\n\n2. Kantorovich formulation and the Wasserstein-$2$ distance.\n- The Kantorovich problem seeks a coupling $\\gamma$ on $\\mathbb{R}\\times\\mathbb{R}$ with marginals $\\mu$ and $\\nu$, i.e., $\\gamma \\in \\Gamma(\\mu,\\nu)$ where\n$$\n\\Gamma(\\mu,\\nu) \\;=\\; \\left\\{ \\gamma \\in \\mathcal{P}(\\mathbb{R}\\times\\mathbb{R}) \\,:\\, \\gamma(A\\times \\mathbb{R})=\\mu(A),\\; \\gamma(\\mathbb{R}\\times B)=\\nu(B)\\;\\text{for all Borel }A,B \\right\\},\n$$\nto minimize the expected cost\n$$\n\\inf_{\\gamma \\in \\Gamma(\\mu,\\nu)} \\int_{\\mathbb{R}\\times\\mathbb{R}} (x-y)^{2}\\,\\mathrm{d}\\gamma(x,y).\n$$\n- By definition, this infimum equals the square of the Wasserstein-$2$ (W2) distance:\n$$\nW_{2}^{2}(\\mu,\\nu) \\;=\\; \\inf_{\\gamma \\in \\Gamma(\\mu,\\nu)} \\int_{\\mathbb{R}\\times\\mathbb{R}} (x-y)^{2}\\,\\mathrm{d}\\gamma(x,y).\n$$\nSince $\\nu$ is absolutely continuous, an optimal coupling exists and in one dimension is induced by the monotone rearrangement coupling.\n\n3. Computing $W_{2}^{2}$ for $p=\\tfrac{1}{2}$, $a=1$, and $\\nu=\\mathcal{N}(0,1)$.\n- In one dimension, the optimal coupling for the quadratic cost is given by the increasing rearrangement, and one can write\n$$\nW_{2}^{2}(\\mu,\\nu) \\;=\\; \\int_{0}^{1} \\bigl(F_{\\mu}^{-1}(u) - F_{\\nu}^{-1}(u)\\bigr)^{2}\\,\\mathrm{d}u,\n$$\nwhere $F_{\\mu}^{-1}$ and $F_{\\nu}^{-1}$ are the quantile functions of $\\mu$ and $\\nu$. This is a standard, well-tested result in one-dimensional optimal transport.\n- For $\\mu=\\tfrac{1}{2}\\delta_{-1}+\\tfrac{1}{2}\\delta_{1}$, the quantile function is\n$$\nF_{\\mu}^{-1}(u) \\;=\\; \\begin{cases}\n-1, & u \\in [0,\\tfrac{1}{2}),\\\\\n1, & u \\in [\\tfrac{1}{2},1].\n\\end{cases}\n$$\nFor $\\nu=\\mathcal{N}(0,1)$, $F_{\\nu}^{-1}(u)=\\Phi^{-1}(u)$ where $\\Phi$ is the standard normal cumulative distribution function and $\\Phi^{-1}$ its quantile.\n- Therefore,\n$$\nW_{2}^{2}(\\mu,\\nu) \\;=\\; \\int_{0}^{\\tfrac{1}{2}} \\bigl(-1 - \\Phi^{-1}(u)\\bigr)^{2} \\,\\mathrm{d}u \\;+\\; \\int_{\\tfrac{1}{2}}^{1} \\bigl(1 - \\Phi^{-1}(u)\\bigr)^{2} \\,\\mathrm{d}u.\n$$\nUsing the symmetry $\\Phi^{-1}(1-u)=-\\Phi^{-1}(u)$ and the substitution $v=1-u$ in the second integral, the two integrals are equal, yielding\n$$\nW_{2}^{2}(\\mu,\\nu) \\;=\\; 2 \\int_{0}^{\\tfrac{1}{2}} \\bigl(\\Phi^{-1}(u)+1\\bigr)^{2}\\,\\mathrm{d}u.\n$$\n- Make the change of variable $z=\\Phi^{-1}(u)$, so that $\\mathrm{d}u=\\phi(z)\\,\\mathrm{d}z$, where $\\phi(z)=\\tfrac{1}{\\sqrt{2\\pi}}\\exp(-z^{2}/2)$ is the standard normal density. As $u$ goes from $0$ to $\\tfrac{1}{2}$, $z$ goes from $-\\infty$ to $0$. Hence\n$$\nW_{2}^{2}(\\mu,\\nu) \\;=\\; 2 \\int_{-\\infty}^{0} (z+1)^{2}\\,\\phi(z)\\,\\mathrm{d}z \\;=\\; 2 \\left[ \\int_{-\\infty}^{0} z^{2}\\phi(z)\\,\\mathrm{d}z \\;+\\; 2 \\int_{-\\infty}^{0} z\\,\\phi(z)\\,\\mathrm{d}z \\;+\\; \\int_{-\\infty}^{0} \\phi(z)\\,\\mathrm{d}z \\right].\n$$\nWe now evaluate each term using basic properties of the standard normal:\n- Since $z^{2}\\phi(z)$ is even and $\\int_{\\mathbb{R}} z^{2}\\phi(z)\\,\\mathrm{d}z = 1$, we have\n$$\n\\int_{-\\infty}^{0} z^{2}\\phi(z)\\,\\mathrm{d}z \\;=\\; \\tfrac{1}{2}.\n$$\n- Using $\\tfrac{\\mathrm{d}}{\\mathrm{d}z}\\phi(z)=-z\\phi(z)$, we obtain\n$$\n\\int_{-\\infty}^{0} z\\,\\phi(z)\\,\\mathrm{d}z \\;=\\; -\\phi(0) \\;=\\; -\\tfrac{1}{\\sqrt{2\\pi}}.\n$$\n- By symmetry,\n$$\n\\int_{-\\infty}^{0} \\phi(z)\\,\\mathrm{d}z \\;=\\; \\tfrac{1}{2}.\n$$\nSubstituting,\n$$\nW_{2}^{2}(\\mu,\\nu) \\;=\\; 2 \\left[ \\tfrac{1}{2} \\;+\\; 2\\left(-\\tfrac{1}{\\sqrt{2\\pi}}\\right) \\;+\\; \\tfrac{1}{2} \\right] \\;=\\; 2 \\left[ 1 \\;-\\; \\tfrac{2}{\\sqrt{2\\pi}} \\right] \\;=\\; 2 \\;-\\; \\tfrac{4}{\\sqrt{2\\pi}}.\n$$\nThis value is the minimal Kantorovich transport cost for mapping the two-point prior $\\mu$ to the continuous target $\\nu$ under quadratic cost, and it quantifies the necessity (and the price) of resorting to a probabilistic coupling when a Monge map does not exist.",
            "answer": "$$\\boxed{2-\\frac{4}{\\sqrt{2\\pi}}}$$"
        },
        {
            "introduction": "Our final practice delves into a powerful computational technique that makes optimal transport practical for large-scale data assimilation and machine learning problems. While the classic OT formulation is elegant, solving it can be computationally prohibitive. This exercise  introduces entropic regularization, which leads to the highly efficient Sinkhorn algorithm. You will implement this algorithm to approximate the transport map between empirical distributions and derive a theoretical bound on the approximation bias, gaining direct experience with the trade-offs between computational speed and accuracy in modern OT methods.",
            "id": "3408171",
            "problem": "Consider a Bayesian inverse problem in one spatial dimension where the unknown parameter is $\\theta \\in \\mathbb{R}$. Let the prior be Gaussian $\\mu = \\mathcal{N}(0,\\tau^2)$ with $\\tau > 0$, and the observation model be $y_{\\text{obs}} = G \\theta + \\eta$ with $G(\\theta) = \\theta$ and zero-mean Gaussian noise $\\eta \\sim \\mathcal{N}(0,\\sigma^2)$, with $\\sigma > 0$. By Bayes’ rule, the posterior is Gaussian $\\nu = p(\\theta \\mid y_{\\text{obs}}) = \\mathcal{N}(m,s^2)$, where $s^2 = \\left(\\tau^{-2} + \\sigma^{-2}\\right)^{-1}$ and $m = s^2 \\, \\sigma^{-2} \\, y_{\\text{obs}}$. We consider approaching Bayesian inference via Optimal Transport (OT), using an entropic-regularized approximation (Sinkhorn) to a transport map from the prior to the posterior.\n\nFundamental definitions to use:\n- A coupling $\\pi$ between two probability measures $\\mu$ and $\\nu$ on $\\mathbb{R}$ is a joint probability measure on $\\mathbb{R} \\times \\mathbb{R}$ with marginals $\\mu$ and $\\nu$. The (Kantorovich) optimal transport problem for a cost $c(x,y)$ seeks $\\inf_{\\pi \\in \\Pi(\\mu,\\nu)} \\int c(x,y) \\, d\\pi(x,y)$.\n- The entropic-regularized OT problem replaces the objective with $\\int c(x,y) \\, d\\pi(x,y) + \\varepsilon \\, \\mathrm{KL}(\\pi \\,\\|\\, \\mu \\otimes \\nu)$ for a regularization parameter $\\varepsilon > 0$, where $\\mathrm{KL}$ denotes Kullback–Leibler divergence, and $\\mu \\otimes \\nu$ is the product measure. In the discrete, uniform-weight case and with quadratic cost $c(x,y) = \\tfrac{1}{2}\\lVert x - y \\rVert^2$, the Gibbs kernel is $K_{ij} = \\exp\\!\\left(-C_{ij}/\\varepsilon\\right)$ with $C_{ij} = \\tfrac{1}{2}(x_i - y_j)^2$, and Sinkhorn scaling yields a coupling $\\pi_\\varepsilon$ with the prescribed marginals.\n- The barycentric projection transport approximation associated with $\\pi_\\varepsilon$ is the map $T_\\varepsilon(x) = \\mathbb{E}[Y \\mid X=x]$ computed from $\\pi_\\varepsilon$. The corresponding approximate posterior is the pushforward $T_\\varepsilon \\# \\mu$.\n\nDerivation task:\n1. Starting from Bayes’ rule for $\\nu$, the Kantorovich formulation of OT, and the definition of entropic regularization, justify the construction of the discrete Sinkhorn coupling $\\pi_\\varepsilon$ between empirical measures $\\hat{\\mu}$ and $\\hat{\\nu}$ (formed by independent and identically distributed samples from $\\mu$ and $\\nu$) for the quadratic cost $c(x,y) = \\tfrac{1}{2}\\lVert x - y \\rVert^2$, and define the barycentric map $T_\\varepsilon$.\n2. For a Lipschitz function $f:\\mathbb{R} \\to \\mathbb{R}$ with Lipschitz constant $L$, derive a bound on the bias in estimating $\\mathbb{E}_{\\nu}[f(\\theta)]$ by $\\mathbb{E}_{T_\\varepsilon \\# \\mu}[f(\\theta)]$, expressed in terms of $\\varepsilon$ and $L$, as $\\varepsilon \\to 0$. Begin from the Kantorovich–Rubinstein-type inequality $|\\mathbb{E}_{\\nu}[f] - \\mathbb{E}_{T_\\varepsilon \\# \\mu}[f]| \\le L \\, \\mathbb{E}_{\\pi_\\varepsilon}[\\lVert Y - T_\\varepsilon(X)\\rVert]$, and, using only standard convexity and concentration arguments applicable to the quadratic cost, obtain an explicit scaling with $\\varepsilon$. Provide clear statements of any conditions required for your bound to hold, and ensure each step follows from a foundational principle or a well-tested result (for example, properties of log-concave measures or Poincaré inequalities).\n3. Specialize the preceding bound to the one-dimensional case and functions $f$ with known Lipschitz constants.\n\nNumerical experiment and test suite:\n- Fix $d=1$ (one-dimensional parameter), $\\tau = 1$, $\\sigma = 0.3$, and $y_{\\text{obs}} = 0.7$. These define the posterior $\\nu = \\mathcal{N}(m,s^2)$ with $s^2 = (1/\\tau^2 + 1/\\sigma^2)^{-1}$ and $m = s^2(y_{\\text{obs}}/\\sigma^2)$. Draw $n=128$ independent and identically distributed samples $\\{x_i\\}_{i=1}^n$ from $\\mu = \\mathcal{N}(0,\\tau^2)$ and $\\{y_j\\}_{j=1}^n$ from $\\nu = \\mathcal{N}(m,s^2)$ to form empirical measures $\\hat{\\mu}$ and $\\hat{\\nu}$.\n- For each $\\varepsilon \\in \\{1.0, 0.5, 0.2\\}$, compute the Sinkhorn coupling $\\pi_\\varepsilon$ between $\\hat{\\mu}$ and $\\hat{\\nu}$ with cost $c(x,y) = \\tfrac{1}{2}(x - y)^2$, and the associated barycentric map values $T_\\varepsilon(x_i)$. Consider the functions:\n  - $f_1(\\theta) = \\theta$ with Lipschitz constant $L_1 = 1$,\n  - $f_2(\\theta) = |\\theta|$ with Lipschitz constant $L_2 = 1$,\n  - $f_3(\\theta) = \\sin(2\\theta)$ with Lipschitz constant $L_3 = 2$.\n- For each pair $(\\varepsilon, f_k)$ with $k \\in \\{1,2,3\\}$, compute the empirical bias $B(\\varepsilon,f_k) = \\left|\\frac{1}{n}\\sum_{j=1}^n f_k(y_j) - \\frac{1}{n}\\sum_{i=1}^n f_k\\big(T_\\varepsilon(x_i)\\big)\\right|$. Using the one-dimensional $\\varepsilon$-scaling you derived, construct an explicit, computable upper bound $U(\\varepsilon,f_k)$ based only on $\\varepsilon$ and $L_k$ (no unobservable constants), and test whether $B(\\varepsilon,f_k) \\leq U(\\varepsilon,f_k) + \\delta$ holds for a fixed numerical tolerance $\\delta = 0.05$ to account for sampling error and finite-iteration numerics.\n- The test suite consists of the $9$ cases from the Cartesian product of $\\{\\varepsilon \\in \\{1.0, 0.5, 0.2\\}\\}$ and $\\{f_1,f_2,f_3\\}$. For each case, the program must output a boolean indicating whether the empirical inequality is satisfied.\n\nFinal output format:\n- Your program should produce a single line of output containing a comma-separated list of the $9$ booleans in the order $\\varepsilon = 1.0, 0.5, 0.2$ and $f_1, f_2, f_3$ nested in that order, enclosed in square brackets (for example, $[{\\tt True},{\\tt False},\\dots]$). No physical units are involved, and no angles appear, so no unit conversions are required.",
            "solution": "The problem is valid. It is a well-posed and scientifically grounded exercise in computational statistics, connecting Bayesian inference with computational optimal transport. All definitions are standard, and all parameters for the numerical experiment are provided.\n\n### 1. Justification of the Discrete Sinkhorn Method\n\nThe task is to approximate the Bayesian posterior $\\nu = \\mathcal{N}(m,s^2)$ starting from the prior $\\mu = \\mathcal{N}(0,\\tau^2)$ using an optimal transport map. The classical (Kantorovich) optimal transport problem seeks a coupling $\\pi$ between $\\mu$ and $\\nu$ that minimizes the total transport cost, $\\inf_{\\pi \\in \\Pi(\\mu,\\nu)} \\int c(x,y) \\, d\\pi(x,y)$. For the quadratic cost $c(x,y) = \\frac{1}{2}(x-y)^2$, Brenier's theorem states that the optimal coupling is deterministic, $\\pi = (\\mathrm{Id} \\times T_0)_\\# \\mu$, where $T_0: \\mathbb{R} \\to \\mathbb{R}$ is the optimal transport map.\n\nThe entropic regularization of this problem introduces a penalty term, modifying the objective to finding a coupling $\\pi_\\varepsilon$ that minimizes $\\int c(x,y) \\, d\\pi(x,y) + \\varepsilon \\, \\mathrm{KL}(\\pi \\,\\|\\, \\mu \\otimes \\nu)$, where $\\varepsilon > 0$ is the regularization strength. This regularized problem is strictly convex and always has a unique solution $\\pi_\\varepsilon$, which is absolutely continuous with respect to $\\mu \\otimes \\nu$. The optimal coupling $\\pi_\\varepsilon$ is no longer deterministic but represents a \"blurred\" version of the optimal map $T_0$.\n\nFor numerical computation, we discretize the measures $\\mu$ and $\\nu$ by drawing $n$ independent and identically distributed samples $\\{x_i\\}_{i=1}^n$ from $\\mu$ and $\\{y_j\\}_{j=1}^n$ from $\\nu$. This yields the empirical measures $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n \\delta_{x_i}$ and $\\hat{\\nu} = \\frac{1}{n}\\sum_{j=1}^n \\delta_{y_j}$. The discrete version of the entropic OT problem is to find a transport plan (a matrix) $\\pi_\\varepsilon \\in \\mathbb{R}^{n \\times n}$ that minimizes:\n$$\n\\sum_{i,j=1}^n \\pi_{ij} C_{ij} + \\varepsilon \\sum_{i,j=1}^n \\pi_{ij} \\log(n^2 \\pi_{ij})\n$$\nsubject to the marginal constraints $\\sum_{j=1}^n \\pi_{ij} = \\frac{1}{n}$ for all $i$ and $\\sum_{i=1}^n \\pi_{ij} = \\frac{1}{n}$ for all $j$. The cost matrix is $C_{ij} = \\frac{1}{2}(x_i - y_j)^2$.\n\nThe solution to this minimization problem has a specific structure: $\\pi_{ij} = u_i K_{ij} v_j$, where $K_{ij} = \\exp(-C_{ij}/\\varepsilon)$ is the Gibbs kernel, and $u, v \\in \\mathbb{R}^n$ are scaling vectors. Sinkhorn's algorithm is an iterative procedure to find these vectors. Given target uniform marginals $a=b=\\frac{1}{n}\\mathbf{1}$, the algorithm alternates between scaling the rows and columns of $K$:\n$u \\leftarrow \\frac{a}{K v}$ and $v \\leftarrow \\frac{b}{K^T u}$\n(where division is element-wise) until convergence. Once the optimal coupling $\\pi_\\varepsilon$ is found, the barycentric map $T_\\varepsilon$ is defined as the conditional expectation of the target variable $Y$ given the source variable $X$. For the discrete samples, this is:\n$$\nT_\\varepsilon(x_i) = \\mathbb{E}_{\\pi_\\varepsilon}[Y | X=x_i] = \\sum_{j=1}^n y_j P(Y=y_j | X=x_i) = \\sum_{j=1}^n y_j \\frac{\\pi_{ij}}{\\sum_{k=1}^n \\pi_{ik}} = \\sum_{j=1}^n y_j \\frac{\\pi_{ij}}{1/n} = n \\sum_{j=1}^n y_j \\pi_{ij}\n$$\nThe pushforward measure $T_\\varepsilon \\# \\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n \\delta_{T_\\varepsilon(x_i)}$ is then used as the transport-based approximation of the posterior $\\hat{\\nu}$.\n\n### 2. Derivation of the Bias Bound\n\nWe aim to bound the bias $|\\mathbb{E}_{\\nu}[f(\\theta)] - \\mathbb{E}_{T_\\varepsilon \\# \\mu}[f(\\theta)]|$ for a function $f$ with Lipschitz constant $L$. The problem provides the starting point:\n$$\n|\\mathbb{E}_{\\nu}[f] - \\mathbb{E}_{T_\\varepsilon \\# \\mu}[f]| = \\left| \\iint (f(y) - f(T_\\varepsilon(x))) d\\pi_\\varepsilon(x,y) \\right| \\le L \\iint |y - T_\\varepsilon(x)| d\\pi_\\varepsilon(x,y) = L \\, \\mathbb{E}_{\\pi_\\varepsilon}[|Y - T_\\varepsilon(X)|]\n$$\nBy applying Jensen's inequality to the concave function $\\phi(z)=\\sqrt{z}$, we have $(\\mathbb{E}[Z])^2 \\le \\mathbb{E}[Z^2]$. Thus:\n$$\n\\mathbb{E}_{\\pi_\\varepsilon}[|Y - T_\\varepsilon(X)|] \\le \\sqrt{\\mathbb{E}_{\\pi_\\varepsilon}[|Y - T_\\varepsilon(X)|^2]}\n$$\nBy definition, $T_\\varepsilon(X) = \\mathbb{E}_{\\pi_\\varepsilon}[Y|X]$. Therefore, the term inside the square root is the variance of $Y$ conditioned on $X$, averaged over $X$:\n$$\n\\mathbb{E}_{\\pi_\\varepsilon}[|Y - T_\\varepsilon(X)|^2] = \\mathbb{E}_{X \\sim \\mu}[\\mathrm{Var}_{\\pi_\\varepsilon(Y|X)}(Y)]\n$$\nTo bound this variance, we analyze the structure of the conditional distribution $\\pi_\\varepsilon(y|x)$. Its density with respect to the Lebesgue measure is proportional to $e^{(-c(x,y) + \\phi(x) + \\psi(y))/\\varepsilon}$, where $\\phi, \\psi$ are the dual OT potentials. A more direct argument relies on the fact that the conditional measure $d\\pi_\\varepsilon(y|x) \\propto e^{-c(x,y)/\\varepsilon} e^{\\psi(y)/\\varepsilon} d\\nu(y)$. The negative logarithm of its density is $V_x(y) = \\frac{c(x,y)}{\\varepsilon} - \\frac{\\psi(y)}{\\varepsilon} - \\log p_\\nu(y)$, where $p_\\nu(y)$ is the density of $\\nu=\\mathcal{N}(m,s^2)$. The negative log-density of $\\nu$ is $\\frac{1}{2s^2}(y-m)^2$ (up to a constant). So,\n$$\nV_x(y) = \\frac{1}{2\\varepsilon}(y-x)^2 - \\frac{\\psi(y)}{\\varepsilon} + \\frac{1}{2s^2}(y-m)^2 + \\text{const}\n$$\nA key property in OT theory is that for a quadratic cost and log-concave measures (which a Gaussian is), the dual potential $\\psi(y)$ is a concave function. Therefore, its second derivative $\\psi''(y) \\le 0$. The second derivative of $V_x(y)$ with respect to $y$ is:\n$$\nV_x''(y) = \\frac{1}{\\varepsilon} - \\frac{\\psi''(y)}{\\varepsilon} + \\frac{1}{s^2} \\ge \\frac{1}{\\varepsilon} + \\frac{1}{s^2}\n$$\nThis shows that the conditional distribution $\\pi_\\varepsilon(y|x)$ is strongly log-concave with a parameter $\\lambda \\ge \\frac{1}{\\varepsilon} + \\frac{1}{s^2}$. The Brascamp-Lieb inequality states that the variance of a strongly log-concave distribution with parameter $\\lambda$ is bounded by $1/\\lambda$. Thus,\n$$\n\\mathrm{Var}_{\\pi_\\varepsilon(Y|X=x)}(Y) \\le \\left(\\frac{1}{\\varepsilon} + \\frac{1}{s^2}\\right)^{-1} = \\frac{\\varepsilon s^2}{\\varepsilon + s^2}\n$$\nThis bound is uniform in $x$. Taking the expectation over $X \\sim \\mu$ gives:\n$$\n\\mathbb{E}_{X \\sim \\mu}[\\mathrm{Var}_{\\pi_\\varepsilon(Y|X)}(Y)] \\le \\frac{\\varepsilon s^2}{\\varepsilon + s^2}\n$$\nSubstituting this back into our chain of inequalities, we obtain the final bound on the bias:\n$$\n|\\mathbb{E}_{\\nu}[f] - \\mathbb{E}_{T_\\varepsilon \\# \\mu}[f]| \\le L \\sqrt{\\frac{\\varepsilon s^2}{\\varepsilon + s^2}}\n$$\n\n### 3. Specialization and Numerical Test\n\nFor the one-dimensional numerical experiment, the posterior variance is $s^2 = (1/\\tau^2 + 1/\\sigma^2)^{-1}$ and the posterior mean is $m = s^2(y_{\\text{obs}}/\\sigma^2)$. The bound derived above is directly applicable. For a function $f_k$ with Lipschitz constant $L_k$, the computable upper bound is:\n$$\nU(\\varepsilon, f_k) = L_k \\sqrt{\\frac{\\varepsilon s^2}{\\varepsilon + s^2}}\n$$\nThe numerical test involves computing the empirical bias $B(\\varepsilon, f_k) = |\\frac{1}{n}\\sum_j f_k(y_j) - \\frac{1}{n}\\sum_i f_k(T_\\varepsilon(x_i))|$ and checking if the inequality $B(\\varepsilon, f_k) \\le U(\\varepsilon, f_k) + \\delta$ holds, where $\\delta = 0.05$ is a tolerance to account for statistical fluctuations from sampling and numerical errors from the finite-iteration Sinkhorn algorithm.\n\nFor the special case of $f_1(\\theta) = \\theta$, the theoretical bias is exactly zero, as shown by:\n$$\n\\mathbb{E}_{T_\\varepsilon \\# \\mu}[\\theta] = \\int T_\\varepsilon(x)d\\mu(x) = \\iint y \\, d\\pi_\\varepsilon(y|x)d\\mu(x) = \\iint y \\, d\\pi_\\varepsilon(x,y) = \\int y \\, d\\nu(y) = \\mathbb{E}_\\nu[\\theta]\n$$\nThus, $\\mathbb{E}_{\\nu}[f_1] - \\mathbb{E}_{T_\\varepsilon \\# \\mu}[f_1] = 0$. The empirical bias $B(\\varepsilon,f_1)$ should be close to zero, limited only by numerical precision. The bound $U(\\varepsilon, f_1)$ is non-zero, so the inequality is expected to hold.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimal transport problem for Bayesian inference as specified.\n    \"\"\"\n    # Problem Parameters\n    tau = 1.0\n    sigma = 0.3\n    y_obs = 0.7\n    n = 128\n    delta = 0.05\n    \n    # Random number generator for reproducibility\n    rng = np.random.default_rng(42)\n\n    # Posterior distribution parameters\n    tau_sq_inv = 1.0 / (tau**2)\n    sigma_sq_inv = 1.0 / (sigma**2)\n    s_sq = 1.0 / (tau_sq_inv + sigma_sq_inv)\n    m = s_sq * (y_obs / sigma**2)\n    s = np.sqrt(s_sq)\n\n    # Generate samples from prior and posterior\n    # x ~ N(0, tau^2)\n    samples_x = rng.normal(loc=0.0, scale=tau, size=n).reshape(-1, 1)\n    # y ~ N(m, s^2)\n    samples_y = rng.normal(loc=m, scale=s, size=n).reshape(-1, 1)\n\n    # Define test functions and their Lipschitz constants\n    f1 = lambda theta: theta\n    f2 = lambda theta: np.abs(theta)\n    f3 = lambda theta: np.sin(2 * theta)\n    functions = [(f1, 1.0), (f2, 1.0), (f3, 2.0)]\n\n    # Epsilon values for regularization\n    epsilons = [1.0, 0.5, 0.2]\n    \n    # Store results\n    results = []\n    \n    # Helper for Sinkhorn algorithm\n    def compute_sinkhorn_coupling(x, y, epsilon, num_iter=1000, tol=1e-9):\n        \"\"\"\n        Computes the entropic OT coupling using Sinkhorn's algorithm.\n        \"\"\"\n        n_samples = x.shape[0]\n        \n        # Cost matrix C_ij = 0.5 * (x_i - y_j)^2\n        C = 0.5 * (x - y.T)**2\n        \n        # Gibbs kernel\n        K = np.exp(-C / epsilon)\n        \n        # Target marginals (uniform)\n        a = np.ones(n_samples) / n_samples\n        b = np.ones(n_samples) / n_samples\n        \n        # Sinkhorn iterations\n        v = np.ones(n_samples)\n        for _ in range(num_iter):\n            u = a / (K @ v + tol)\n            v = b / (K.T @ u + tol)\n            \n        # Coupling matrix P_ij = u_i * K_ij * v_j\n        pi_eps = np.diag(u) @ K @ np.diag(v)\n        return pi_eps\n\n    # Main loop for experiments\n    for epsilon in epsilons:\n        # 1. Compute Sinkhorn coupling\n        pi_epsilon = compute_sinkhorn_coupling(samples_x, samples_y, epsilon)\n        \n        # 2. Compute barycentric map T_eps(x_i)\n        # T_eps(x_i) = n * sum_j(y_j * pi_ij)\n        T_eps_x = n * (pi_epsilon @ samples_y)\n        \n        for f, L in functions:\n            # 3. Compute empirical bias B(epsilon, f_k)\n            # B = | E_nu[f(y)] - E_{T#mu}[f(T(x))] |\n            # E_nu[f(y)] ~= (1/n) * sum(f(y_j))\n            # E_{T#mu}[f(T(x))] ~= (1/n) * sum(f(T(x_i)))\n            \n            # For f1(theta)=theta, the bias is analytically 0.\n            # Numerically, it's |mean(y) - mean(T_eps(x))|.\n            # Since sum_i T_eps(x_i) = sum_j y_j due to marginal constraints,\n            # this difference will be due to floating point and convergence error.\n            \n            bias_empirical = np.abs(np.mean(f(samples_y)) - np.mean(f(T_eps_x)))\n            \n            # 4. Compute theoretical bound U(epsilon, f_k)\n            # U = L * sqrt( (epsilon * s^2) / (epsilon + s^2) )\n            bound_theoretical = L * np.sqrt((epsilon * s_sq) / (epsilon + s_sq))\n            \n            # 5. Check inequality B <= U + delta\n            is_satisfied = bias_empirical <= bound_theoretical + delta\n            results.append(is_satisfied)\n            \n    # Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}