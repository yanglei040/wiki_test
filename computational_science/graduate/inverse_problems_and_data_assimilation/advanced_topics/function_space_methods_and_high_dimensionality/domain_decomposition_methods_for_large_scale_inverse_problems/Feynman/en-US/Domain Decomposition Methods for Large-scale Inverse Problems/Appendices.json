{
    "hands_on_practices": [
        {
            "introduction": "This first practice lays the algebraic groundwork for non-overlapping domain decomposition, also known as substructuring. By working through a small, discrete system, you will perform the key operation of block Gaussian elimination to remove the interior unknowns and derive the Schur complement system on the interface. Mastering this exercise provides essential intuition for how large, coupled systems are reduced to smaller, more manageable problems defined only on the boundaries between subdomains .",
            "id": "3377575",
            "problem": "Consider a single linearized Gauss–Newton step for a Tikhonov-regularized inverse source identification problem constrained by a Poisson-type partial differential equation. The computational domain is decomposed into two non-overlapping subdomains, and the state is discretized with $6$ degrees of freedom, grouped into two subdomain vectors of size $3$ each. Interface continuity is imposed via two equality constraints, one per interface degree of freedom. In the standard Lagrange-multiplier formulation, the linear system takes the saddle-point form\n$$\n\\begin{pmatrix}\n\\mathbf{K} & \\mathbf{B}^{\\top} \\\\\n\\mathbf{B} & \\mathbf{0}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\mathbf{u} \\\\\n\\boldsymbol{\\lambda}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\mathbf{f} \\\\\n\\mathbf{0}\n\\end{pmatrix},\n$$\nwhere $\\mathbf{u}\\in\\mathbb{R}^{6}$ collects the subdomain state unknowns, $\\boldsymbol{\\lambda}\\in\\mathbb{R}^{2}$ are the interface Lagrange multipliers, $\\mathbf{K}\\in\\mathbb{R}^{6\\times 6}$ is block diagonal with two $3\\times 3$ subdomain operators, and $\\mathbf{B}\\in\\mathbb{R}^{2\\times 6}$ enforces interface continuity.\n\nAssume that each subdomain operator is the standard one-dimensional Poisson stiffness on a uniform grid with Dirichlet boundary conditions, namely\n$$\n\\mathbf{K}_1=\\mathbf{K}_2=\\begin{pmatrix}\n2 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 2\n\\end{pmatrix},\n$$\nand that\n$$\n\\mathbf{K}=\\mathrm{diag}(\\mathbf{K}_1,\\mathbf{K}_2).\n$$\nLet the ordering of $\\mathbf{u}$ be $\\mathbf{u}=\\begin{pmatrix}u_{1,a} & u_{1,b} & u_{1,c} & u_{2,a} & u_{2,b} & u_{2,c}\\end{pmatrix}^{\\top}$, and impose the two interface continuity constraints $u_{1,b}-u_{2,a}=0$ and $u_{1,c}-u_{2,b}=0$, i.e.,\n$$\n\\mathbf{B}=\\begin{pmatrix}\n0 & 1 & 0 & -1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & -1 & 0\n\\end{pmatrix}.\n$$\nTake the right-hand side to be\n$$\n\\mathbf{f}=\\begin{pmatrix}1 & 0 & 0 & 0 & 1 & 0\\end{pmatrix}^{\\top}.\n$$\n\nStarting from the first-order optimality (Karush–Kuhn–Tucker) conditions for the constrained least-squares subproblem and using block Gaussian elimination as the only algebraic tool, eliminate the interior state variables $\\mathbf{u}$ to form the Schur complement on the interface multipliers, and then solve the reduced system for $\\boldsymbol{\\lambda}$. Back-substitute to recover $\\mathbf{u}$. Finally, report the determinant of the $2\\times 2$ Schur complement matrix on the interface multipliers as an exact rational number. No rounding is required. Your final answer must be a single real number without units.",
            "solution": "The problem statement has been validated and is deemed a well-posed problem in numerical linear algebra, arising from the domain decomposition of a partial differential equation. We may therefore proceed with a formal solution.\n\nThe system to be solved is a saddle-point linear system given by the Karush–Kuhn–Tucker (KKT) conditions for a constrained optimization problem. The system is expressed in block matrix form as:\n$$\n\\begin{pmatrix}\n\\mathbf{K} & \\mathbf{B}^{\\top} \\\\\n\\mathbf{B} & \\mathbf{0}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\mathbf{u} \\\\\n\\boldsymbol{\\lambda}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\mathbf{f} \\\\\n\\mathbf{0}\n\\end{pmatrix}\n$$\nThis corresponds to two block equations:\n$$\n\\mathbf{K}\\mathbf{u} + \\mathbf{B}^{\\top}\\boldsymbol{\\lambda} = \\mathbf{f} \\quad (1)\n$$\n$$\n\\mathbf{B}\\mathbf{u} = \\mathbf{0} \\quad (2)\n$$\nThe task is to solve this system by first forming a Schur complement system for the Lagrange multipliers $\\boldsymbol{\\lambda}$. This is achieved through block Gaussian elimination.\n\nFirst, we must establish that the matrix $\\mathbf{K}$ is invertible. The matrix $\\mathbf{K}$ is given as $\\mathbf{K}=\\mathrm{diag}(\\mathbf{K}_1,\\mathbf{K}_2)$, where:\n$$\n\\mathbf{K}_1=\\mathbf{K}_2=\\begin{pmatrix} 2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2 \\end{pmatrix}\n$$\nThis is a symmetric tridiagonal matrix. Its determinant is $\\det(\\mathbf{K}_1) = 2(2 \\cdot 2 - (-1)(-1)) - (-1)(-1 \\cdot 2 - 0) = 2(3) - 2 = 4$. Since the determinant is non-zero, $\\mathbf{K}_1$ is invertible. As $\\mathbf{K}_2 = \\mathbf{K}_1$, it is also invertible. Consequently, $\\mathbf{K}$, being block diagonal with invertible blocks, is itself invertible.\n\nFrom equation (1), we can formally express $\\mathbf{u}$ as:\n$$\n\\mathbf{u} = \\mathbf{K}^{-1}(\\mathbf{f} - \\mathbf{B}^{\\top}\\boldsymbol{\\lambda})\n$$\nSubstituting this expression for $\\mathbf{u}$ into equation (2) eliminates the state variables $\\mathbf{u}$:\n$$\n\\mathbf{B} \\left( \\mathbf{K}^{-1}(\\mathbf{f} - \\mathbf{B}^{\\top}\\boldsymbol{\\lambda}) \\right) = \\mathbf{0}\n$$\nBy linearity, we can distribute $\\mathbf{B}$:\n$$\n\\mathbf{B}\\mathbf{K}^{-1}\\mathbf{f} - \\mathbf{B}\\mathbf{K}^{-1}\\mathbf{B}^{\\top}\\boldsymbol{\\lambda} = \\mathbf{0}\n$$\nRearranging this equation yields the Schur complement system for $\\boldsymbol{\\lambda}$:\n$$\n(\\mathbf{B}\\mathbf{K}^{-1}\\mathbf{B}^{\\top})\\boldsymbol{\\lambda} = \\mathbf{B}\\mathbf{K}^{-1}\\mathbf{f}\n$$\nThe Schur complement matrix, which we denote by $\\mathbf{S}$, is the $2 \\times 2$ matrix on the left-hand side:\n$$\n\\mathbf{S} = \\mathbf{B}\\mathbf{K}^{-1}\\mathbf{B}^{\\top}\n$$\nThe problem requires the calculation of the determinant of $\\mathbf{S}$. To compute $\\mathbf{S}$, we first need the inverse of $\\mathbf{K}$. Since $\\mathbf{K} = \\mathrm{diag}(\\mathbf{K}_1, \\mathbf{K}_2)$, its inverse is $\\mathbf{K}^{-1} = \\mathrm{diag}(\\mathbf{K}_1^{-1}, \\mathbf{K}_2^{-1})$. We compute $\\mathbf{K}_1^{-1}$ using the adjugate method:\n$$\n\\mathbf{K}_1^{-1} = \\frac{1}{\\det(\\mathbf{K}_1)}\\mathrm{adj}(\\mathbf{K}_1)^{\\top} = \\frac{1}{4} \\begin{pmatrix} 3 & 2 & 1 \\\\ 2 & 4 & 2 \\\\ 1 & 2 & 3 \\end{pmatrix}\n$$\nAs $\\mathbf{K}_1 = \\mathbf{K}_2$, we have $\\mathbf{K}_2^{-1} = \\mathbf{K}_1^{-1}$. Thus,\n$$\n\\mathbf{K}^{-1} = \\frac{1}{4} \\begin{pmatrix}\n3 & 2 & 1 & 0 & 0 & 0 \\\\\n2 & 4 & 2 & 0 & 0 & 0 \\\\\n1 & 2 & 3 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 3 & 2 & 1 \\\\\n0 & 0 & 0 & 2 & 4 & 2 \\\\\n0 & 0 & 0 & 1 & 2 & 3\n\\end{pmatrix}\n$$\nThe matrix $\\mathbf{B}$ is given as:\n$$\n\\mathbf{B}=\\begin{pmatrix}\n0 & 1 & 0 & -1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & -1 & 0\n\\end{pmatrix}\n$$\nWe now compute the product $\\mathbf{B}\\mathbf{K}^{-1}$:\n$$\n\\mathbf{B}\\mathbf{K}^{-1} = \\begin{pmatrix}\n0 & 1 & 0 & -1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & -1 & 0\n\\end{pmatrix} \\frac{1}{4} \\begin{pmatrix}\n3 & 2 & 1 & 0 & 0 & 0 \\\\\n2 & 4 & 2 & 0 & 0 & 0 \\\\\n1 & 2 & 3 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 3 & 2 & 1 \\\\\n0 & 0 & 0 & 2 & 4 & 2 \\\\\n0 & 0 & 0 & 1 & 2 & 3\n\\end{pmatrix}\n$$\nThe first row of the result is $\\frac{1}{4}$ times (row $2$ of $\\mathbf{K}^{-1}$'s block - row $4$ of $\\mathbf{K}^{-1}$'s block), and the second row is $\\frac{1}{4}$ times (row $3$ of $\\mathbf{K}^{-1}$'s block - row $5$ of $\\mathbf{K}^{-1}$'s block). This yields:\n$$\n\\mathbf{B}\\mathbf{K}^{-1} = \\frac{1}{4} \\begin{pmatrix}\n2 & 4 & 2 & -3 & -2 & -1 \\\\\n1 & 2 & 3 & -2 & -4 & -2\n\\end{pmatrix}\n$$\nNext, we multiply this result by $\\mathbf{B}^{\\top}$ to form $\\mathbf{S}$:\n$$\n\\mathbf{B}^{\\top} = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ -1 & 0 \\\\ 0 & -1 \\\\ 0 & 0 \\end{pmatrix}\n$$\n$$\n\\mathbf{S} = (\\mathbf{B}\\mathbf{K}^{-1})\\mathbf{B}^{\\top} = \\frac{1}{4} \\begin{pmatrix}\n2 & 4 & 2 & -3 & -2 & -1 \\\\\n1 & 2 & 3 & -2 & -4 & -2\n\\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ -1 & 0 \\\\ 0 & -1 \\\\ 0 & 0 \\end{pmatrix}\n$$\nPerforming the matrix multiplication:\n$$\nS_{11} = \\frac{1}{4} (2(0) + 4(1) + 2(0) - 3(-1) - 2(0) - 1(0)) = \\frac{1}{4}(4+3) = \\frac{7}{4}\n$$\n$$\nS_{12} = \\frac{1}{4} (2(0) + 4(0) + 2(1) - 3(0) - 2(-1) - 1(0)) = \\frac{1}{4}(2+2) = \\frac{4}{4} = 1\n$$\n$$\nS_{21} = \\frac{1}{4} (1(0) + 2(1) + 3(0) - 2(-1) - 4(0) - 2(0)) = \\frac{1}{4}(2+2) = \\frac{4}{4} = 1\n$$\n$$\nS_{22} = \\frac{1}{4} (1(0) + 2(0) + 3(1) - 2(0) - 4(-1) - 2(0)) = \\frac{1}{4}(3+4) = \\frac{7}{4}\n$$\nThe Schur complement matrix is therefore:\n$$\n\\mathbf{S} = \\begin{pmatrix} \\frac{7}{4} & 1 \\\\ 1 & \\frac{7}{4} \\end{pmatrix}\n$$\nThe problem asks for the determinant of this matrix.\n$$\n\\det(\\mathbf{S}) = \\left(\\frac{7}{4}\\right)\\left(\\frac{7}{4}\\right) - (1)(1) = \\frac{49}{16} - 1 = \\frac{49}{16} - \\frac{16}{16} = \\frac{33}{16}\n$$\nThis is the required value. For completeness, one could solve for $\\boldsymbol{\\lambda}$ using $\\mathbf{S}\\boldsymbol{\\lambda} = \\mathbf{B}\\mathbf{K}^{-1}\\mathbf{f}$ and then back-substitute to find $\\mathbf{u} = \\mathbf{K}^{-1}(\\mathbf{f} - \\mathbf{B}^{\\top}\\boldsymbol{\\lambda})$. However, the final answer requested is solely the determinant of $\\mathbf{S}$.",
            "answer": "$$\\boxed{\\frac{33}{16}}$$"
        },
        {
            "introduction": "Real-world problems often involve complex geometries where forcing a matching grid across subdomain interfaces is impractical or inefficient. This practice introduces mortar methods, a powerful and flexible technique for coupling subdomains with non-matching grids. Through this coding exercise, you will implement the necessary interpolation operators and investigate how the choice of the mortar space impacts the consistency of the resulting discrete system, a crucial consideration for the accuracy of inverse problem gradients .",
            "id": "3377564",
            "problem": "Consider a linear elliptic inverse problem posed on two subdomains sharing an interface. Let the interface be the open segment $\\Gamma = (0,1)$ parameterized by $s \\in (0,1)$. The subdomains are coupled through the interface by enforcing continuity conditions. Domain decomposition methods produce interface operators that act on traces of the state variable along $\\Gamma$. To model this setting abstractly yet realistically, assume that the action of the discrete Dirichlet-to-Neumann map on each side of the interface is approximated by a symmetric positive definite finite difference operator of the form\n$$\n\\mathsf{S}_n = \\frac{1}{h_n^2} \\operatorname{tridiag}(-1,2,-1) + \\mathsf{I}_n,\n$$\nwith homogeneous Dirichlet conditions at $s=0$ and $s=1$, where $n$ is the number of interface grid points on a given side, $h_n = \\frac{1}{n-1}$ is the uniform grid spacing, and $\\mathsf{I}_n$ is the $n \\times n$ identity matrix. This is a well-tested surrogate for an interface Schur complement associated with an elliptic partial differential equation.\n\nLet $\\theta \\in \\mathbb{R}^m$ parameterize a Dirichlet control on the interface through a mortar space of dimension $m$, spanned by continuous, piecewise linear (hat) functions on a uniform partition of $[0,1]$ with nodes $\\{t_k\\}_{k=0}^{m-1}$, where $t_k = \\frac{k}{m-1}$. The control $\\theta$ is identified with its nodal values at the mortar nodes.\n\nFor two nonmatching interface grids of sizes $n_1$ and $n_2$ (with $n_1 \\neq n_2$), define linear interpolation (prolongation) matrices $\\mathsf{P}_1 \\in \\mathbb{R}^{n_1 \\times m}$ and $\\mathsf{P}_2 \\in \\mathbb{R}^{n_2 \\times m}$ that map the mortar coefficients $\\theta$ to the trace values sampled at the corresponding interface grid nodes for each subdomain. Let $\\mathsf{I}_{\\text{eval}\\leftarrow n}$ denote the linear interpolation matrix from a grid of size $n$ to a common evaluation grid of size $n_{\\text{eval}}$ (uniform on $[0,1]$). This defines the composite operator\n$$\n\\mathsf{R}_m = \\mathsf{I}_{\\text{eval}\\leftarrow n_1} \\, \\mathsf{S}_{n_1} \\, \\mathsf{P}_1 \\;+\\; \\mathsf{I}_{\\text{eval}\\leftarrow n_2} \\, \\mathsf{S}_{n_2} \\, \\mathsf{P}_2 \\;\\in\\; \\mathbb{R}^{n_{\\text{eval}} \\times m}.\n$$\nConsider the quadratic cost functional\n$$\nJ(\\theta) \\;=\\; \\frac{1}{2} \\, \\lVert \\mathsf{R}_m \\, \\theta \\rVert_2^2,\n$$\nwhich penalizes the squared $L^2$-norm of the flux mismatch evaluated on the common evaluation grid. This abstractly mimics the residual in an interface coupling condition and is a standard structure in reduced-space inverse problems. For this linear least-squares problem, the reduced gradient with respect to the control is\n$$\nJ'(\\theta) \\;=\\; \\mathsf{R}_m^\\top \\mathsf{R}_m \\, \\theta.\n$$\n\nTo quantify the consistency of $J'(\\theta)$ under nonmatching grids and a finite-dimensional mortar space, define a high-fidelity reference with a matching interface grid of size $n_{\\text{ref}}$ for both sides, the same operator construction $\\mathsf{S}_{n_{\\text{ref}}}$, and a mortar-to-reference prolongation $\\mathsf{P}_{\\text{ref}} \\in \\mathbb{R}^{n_{\\text{ref}} \\times m}$ built by linear interpolation from the mortar nodes to the reference grid. The corresponding reference operator is\n$$\n\\mathsf{R}_{\\text{true}} \\;=\\; 2 \\, \\mathsf{I}_{\\text{eval}\\leftarrow n_{\\text{ref}}} \\, \\mathsf{S}_{n_{\\text{ref}}} \\, \\mathsf{P}_{\\text{ref}} \\;\\in\\; \\mathbb{R}^{n_{\\text{eval}} \\times m},\n$$\nand the associated reference gradient in the mortar space is\n$$\nJ'_{\\text{true}}(\\theta) \\;=\\; \\mathsf{R}_{\\text{true}}^\\top \\mathsf{R}_{\\text{true}} \\, \\theta.\n$$\nNote that the factor $2$ represents the sum of identical contributions from the two sides in the matching-grid limit.\n\nYour tasks are:\n1. Starting from the definition of $J(\\theta)$ for a linear least-squares problem, derive the expression for the reduced gradient $J'(\\theta)$ in both the nonmatching-grid mortar setting and the high-fidelity reference setting. Explain why the difference $J'(\\theta) - J'_{\\text{true}}(\\theta)$ measures the consistency error of the gradient due to both nonmatching grids and the finite mortar space dimension $m$.\n2. Explain, using interpolation error estimates for continuous, piecewise linear basis functions on a uniform partition, why the mortar space dimension $m$ controls the consistency error, and discuss the expected asymptotic behavior of $\\lVert J'(\\theta) - J'_{\\text{true}}(\\theta) \\rVert_2$ as $m$ increases under fixed $n_1$, $n_2$, and $n_{\\text{eval}}$.\n3. Implement a program that constructs the operators described above and evaluates the relative consistency error of the gradient for a fixed test control $\\theta$ defined by the smooth function $\\theta_k = \\sin(\\pi t_k)$ at the mortar nodes. Use homogeneous Dirichlet conditions at $s=0$ and $s=1$ in all finite difference operators.\n\nUse the following test suite of parameters to probe the effect of the mortar space dimension:\n- Nonmatching interface grid sizes: $n_1 = 53$, $n_2 = 77$.\n- Evaluation grid size: $n_{\\text{eval}} = 301$.\n- Reference grid size: $n_{\\text{ref}} = 401$.\n- Mortar space dimensions: $m \\in \\{2, 5, 10, 20, 40\\}$.\n- Test control: $\\theta_k = \\sin(\\pi t_k)$ for $k = 0, 1, \\dots, m-1$.\n\nFor each mortar dimension $m$ in the test suite, compute the relative $2$-norm of the gradient consistency error\n$$\n\\varepsilon(m) \\;=\\; \\frac{\\lVert J'(\\theta) - J'_{\\text{true}}(\\theta) \\rVert_2}{\\lVert J'_{\\text{true}}(\\theta) \\rVert_2}.\n$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each result corresponds to $\\varepsilon(m)$ for the mortar dimensions listed above, in the same order. All outputs must be real numbers with default Python floating-point formatting. No physical units are involved in this problem; all quantities are nondimensional.",
            "solution": "The problem is assessed to be valid. It is scientifically grounded in the numerical analysis of partial differential equations and inverse problems, specifically using concepts from domain decomposition and mortar methods. The problem is well-posed, objective, and contains all necessary information for a unique solution.\n\n### Part 1: Derivation of the Reduced Gradient\n\nThe quadratic cost functional is given by\n$$\nJ(\\theta) = \\frac{1}{2} \\lVert \\mathsf{R}_m \\theta \\rVert_2^2\n$$\nwhere $\\theta \\in \\mathbb{R}^m$ is the vector of control parameters and $\\mathsf{R}_m \\in \\mathbb{R}^{n_{\\text{eval}} \\times m}$ is a linear operator.\n\nThe squared $L^2$-norm can be expressed using the inner product (dot product) as $\\lVert v \\rVert_2^2 = v^\\top v$. Thus, the functional can be written as:\n$$\nJ(\\theta) = \\frac{1}{2} (\\mathsf{R}_m \\theta)^\\top (\\mathsf{R}_m \\theta)\n$$\nUsing the property of the transpose of a product, $(AB)^\\top = B^\\top A^\\top$, we have:\n$$\nJ(\\theta) = \\frac{1}{2} \\theta^\\top \\mathsf{R}_m^\\top \\mathsf{R}_m \\theta\n$$\nThis is a quadratic form in $\\theta$. To find the gradient, which the problem denotes as $J'(\\theta)$, we can take the derivative with respect to each component $\\theta_k$ of the vector $\\theta$. A standard result from matrix calculus for a quadratic form $f(x) = \\frac{1}{2} x^\\top A x$ with a symmetric matrix $A$ is that $\\nabla f(x) = A x$. The matrix $\\mathsf{H} = \\mathsf{R}_m^\\top \\mathsf{R}_m$ is symmetric, since $(\\mathsf{R}_m^\\top \\mathsf{R}_m)^\\top = \\mathsf{R}_m^\\top (\\mathsf{R}_m^\\top)^\\top = \\mathsf{R}_m^\\top \\mathsf{R}_m$.\n\nTherefore, the gradient of $J(\\theta)$ is:\n$$\nJ'(\\theta) = \\nabla_\\theta J(\\theta) = \\mathsf{R}_m^\\top \\mathsf{R}_m \\theta\n$$\nThis confirms the expression given in the problem statement for the nonmatching-grid mortar setting.\n\nThe derivation for the high-fidelity reference setting is identical. We simply replace the nonmatching operator $\\mathsf{R}_m$ with the reference operator $\\mathsf{R}_{\\text{true}}$:\n$$\nJ_{\\text{true}}(\\theta) = \\frac{1}{2} \\lVert \\mathsf{R}_{\\text{true}} \\theta \\rVert_2^2 = \\frac{1}{2} \\theta^\\top \\mathsf{R}_{\\text{true}}^\\top \\mathsf{R}_{\\text{true}} \\theta\n$$\nThe corresponding gradient is:\n$$\nJ'_{\\text{true}}(\\theta) = \\mathsf{R}_{\\text{true}}^\\top \\mathsf{R}_{\\text{true}} \\theta\n$$\nThis confirms the second expression provided.\n\nThe difference $J'(\\theta) - J'_{\\text{true}}(\\theta)$ represents the consistency error of the gradient. In optimization algorithms for inverse problems, the gradient $J'(\\theta)$ is a crucial quantity that determines search directions. Any error in its computation can lead to slower convergence or convergence to a non-optimal solution. This error arises because the operator $\\mathsf{R}_m$, used in the practical nonmatching-grid setting, is an approximation to the ideal operator, here proxied by $\\mathsf{R}_{\\text{true}}$. The structure of $\\mathsf{R}_m$ combines two different discretizations (on grids of size $n_1$ and $n_2$), whereas $\\mathsf{R}_{\\text{true}}$ is based on a single, finer, matched discretization ($n_{\\text{ref}}$). Furthermore, both operators depend on the projection from the abstract mortar control space of dimension $m$ onto the physical interface grids. The difference $J'(\\theta) - J'_{\\text{true}}(\\theta) = (\\mathsf{R}_m^\\top \\mathsf{R}_m - \\mathsf{R}_{\\text{true}}^\\top \\mathsf{R}_{\\text{true}}) \\theta$ thus measures the combined effect of these two sources of numerical approximation error: the geometric inconsistency of nonmatching grids and the finite-dimensional representation of the control function via the mortar space.\n\n### Part 2: Role of Mortar Space Dimension and Asymptotic Behavior\n\nThe dimension of the mortar space, $m$, plays a critical role in controlling the consistency error. The control vector $\\theta \\in \\mathbb{R}^m$ does not represent the interface function directly, but rather its coefficients in a basis of piecewise linear hat functions, $\\{B_k^m(s)\\}_{k=0}^{m-1}$, defined on a uniform partition of $[0,1]$ with spacing $h_m = 1/(m-1)$. The continuous function on the interface is $u_m(s) = \\sum_{k=0}^{m-1} \\theta_k B_k^m(s)$.\n\nThe test control is chosen as $\\theta_k = \\sin(\\pi t_k)$, where $t_k = k/(m-1)$ are the mortar nodes. This means that $u_m(s)$ is the piecewise linear interpolant of the smooth function $u(s) = \\sin(\\pi s)$. The accuracy of this interpolation is governed by the mortar grid spacing $h_m$. According to standard numerical analysis theory, for a sufficiently smooth function like $u(s)=\\sin(\\pi s)$, the interpolation error is bounded. For instance, the maximum error is bounded by:\n$$\n\\lVert u(s) - u_m(s) \\rVert_{L^\\infty[0,1]} \\leq C h_m^2 \\lVert u''(s) \\rVert_{L^\\infty[0,1]} = O(1/m^2)\n$$\nwhere $C$ is a constant independent of $m$.\n\nBoth the nonmatching operator $\\mathsf{R}_m$ and the reference operator $\\mathsf{R}_{\\text{true}}$ act on this same underlying function $u_m(s)$, albeit through different discretization and projection steps. As $m$ increases, $u_m(s)$ becomes a progressively better approximation of the smooth function $u(s) = \\sin(\\pi s)$.\n\nThe gradient consistency error, $\\lVert J'(\\theta) - J'_{\\text{true}}(\\theta) \\rVert_2$, quantifies the difference between two distinct discrete approximations of the gradient of a continuous functional. As $m$ increases, the inputs to these discrete systems (the vectors $\\mathsf{P}_1\\theta$, $\\mathsf{P}_2\\theta$, $\\mathsf{P}_{\\text{ref}}\\theta$) become more accurate discrete representations of the same underlying smooth function $u(s)$. Consequently, the distinct ways in which the nonmatching and reference systems process this function should yield results that are closer to each other. The error between the two discrete models is expected to decrease as the function they are modeling becomes better resolved by the control parameterization.\n\nTherefore, for small to moderate values of $m$, where the mortar space provides the coarsest approximation in the system (i.e., $h_m > h_{n_1}, h_{n_2}$), we expect the consistency error $\\lVert J'(\\theta) - J'_{\\text{true}}(\\theta) \\rVert_2$ to decrease as $m$ increases. The rate of convergence is related to the approximation properties of the piecewise linear basis and the order of the operators involved. Since the operator $\\mathsf{S}_n$ involves a second-order finite difference, the error in the gradient may be expected to scale with a power of $h_m$, i.e., $O(m^{-p})$ for some $p>0$.\n\nHowever, this improvement will not continue indefinitely. The discretizations on the physical grids ($n_1$, $n_2$, $n_{\\text{ref}}$) are fixed. Once the mortar grid becomes significantly finer than the physical grids (i.e., $m \\gg n_1, n_2$), the error will no longer be dominated by the mortar approximation quality. Instead, it will be dominated by the inherent difference between the nonmatching discretization (averaging over grids $n_1, n_2$) and the reference discretization ($n_{\\text{ref}}$). At this point, further increases in $m$ will yield diminishing returns, and the error $\\varepsilon(m)$ will plateau. The error will converge to a non-zero value that represents the irreducible consistency error for the fixed grids $n_1$ and $n_2$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the relative consistency error of the gradient for a mortar-based\n    domain decomposition problem with nonmatching grids.\n    \"\"\"\n\n    def construct_s_operator(n):\n        \"\"\"\n        Constructs the symmetric positive definite operator S_n.\n        S_n = (1/h^2) * tridiag(-1, 2, -1) + I_n\n        \"\"\"\n        if n <= 1:\n            return np.array([[1.0]])\n        h = 1.0 / (n - 1)\n        h2_inv = 1.0 / (h * h)\n        \n        diag_main = (2.0 * h2_inv + 1.0) * np.ones(n)\n        diag_off = -h2_inv * np.ones(n - 1)\n        \n        s_op = np.diag(diag_main) + np.diag(diag_off, k=1) + np.diag(diag_off, k=-1)\n        return s_op\n\n    def construct_interpolation_matrix(target_nodes, source_nodes):\n        \"\"\"\n        Constructs a linear interpolation matrix P that maps function values from\n        source_nodes to target_nodes.\n        Resulting matrix has shape (len(target_nodes), len(source_nodes)).\n        \"\"\"\n        n_target = len(target_nodes)\n        n_source = len(source_nodes)\n        \n        if n_source <= 1:\n            if n_source == 1:\n                return np.ones((n_target, 1))\n            else:\n                return np.zeros((n_target, 0))\n\n        p_matrix = np.zeros((n_target, n_source))\n        h_source = 1.0 / (n_source - 1)\n        \n        for j in range(n_target):\n            s_j = target_nodes[j]\n            \n            # Handle edge case where s_j is exactly 1.0\n            if s_j >= 1.0:\n                p_matrix[j, -1] = 1.0\n                continue\n\n            # Find k such that s_j is in [source_nodes[k], source_nodes[k+1]]\n            k_float = s_j / h_source\n            k = int(k_float)\n            \n            # Linear interpolation weights\n            # The value at s_j is w * source[k] + (1-w) * source[k+1]\n            # No, it's (1-alpha)*source[k] + alpha*source[k+1] where alpha = (s_j-t_k)/h\n            alpha = k_float - k\n\n            p_matrix[j, k] = 1.0 - alpha\n            if k + 1 < n_source:\n                p_matrix[j, k + 1] = alpha\n                \n        return p_matrix\n\n    # --- Problem Parameters ---\n    n1 = 53\n    n2 = 77\n    neval = 301\n    nref = 401\n    m_values = [2, 5, 10, 20, 40]\n    \n    results = []\n\n    # --- Pre-construct grids and S operators which are constant for all m ---\n    n1_nodes = np.linspace(0, 1, n1)\n    n2_nodes = np.linspace(0, 1, n2)\n    neval_nodes = np.linspace(0, 1, neval)\n    nref_nodes = np.linspace(0, 1, nref)\n\n    S1 = construct_s_operator(n1)\n    S2 = construct_s_operator(n2)\n    S_ref = construct_s_operator(nref)\n\n    I_eval1 = construct_interpolation_matrix(neval_nodes, n1_nodes)\n    I_eval2 = construct_interpolation_matrix(neval_nodes, n2_nodes)\n    I_eval_ref = construct_interpolation_matrix(neval_nodes, nref_nodes)\n\n    for m in m_values:\n        # --- Construct m-dependent entities ---\n        m_nodes = np.linspace(0, 1, m)\n        theta = np.sin(np.pi * m_nodes)\n\n        # Prolongation operators from mortar space to interface grids\n        P1 = construct_interpolation_matrix(n1_nodes, m_nodes)\n        P2 = construct_interpolation_matrix(n2_nodes, m_nodes)\n        P_ref = construct_interpolation_matrix(nref_nodes, m_nodes)\n        \n        # --- Construct composite operators R_m and R_true ---\n        # Nonmatching-grid operator\n        R_m = (I_eval1 @ S1 @ P1) + (I_eval2 @ S2 @ P2)\n        \n        # High-fidelity reference operator\n        R_true = 2.0 * I_eval_ref @ S_ref @ P_ref\n\n        # --- Compute gradients ---\n        # Gradient for nonmatching-grid functional\n        grad_J = R_m.T @ R_m @ theta\n        \n        # Gradient for reference functional\n        grad_J_true = R_true.T @ R_true @ theta\n\n        # --- Compute relative consistency error ---\n        error_norm = np.linalg.norm(grad_J - grad_J_true)\n        ref_norm = np.linalg.norm(grad_J_true)\n        \n        relative_error = 0.0\n        if ref_norm > 1e-15: # Avoid division by zero\n            relative_error = error_norm / ref_norm\n        \n        results.append(relative_error)\n\n    # --- Format and print the final output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "This final practice synthesizes the concepts of domain decomposition within the full context of a nonlinear inverse problem. You will implement a Gauss-Newton optimization step where the computationally intensive tasks—solving the forward PDE and the corresponding sensitivity equations—are handled by a domain decomposition solver. This exercise highlights the practical power of DD methods as engines within larger computational frameworks for parameter estimation and data assimilation .",
            "id": "3377614",
            "problem": "Consider the one-dimensional nonlinear diffusion boundary value problem\n$$ -\\frac{d}{dx}\\big(\\kappa(m(x)) \\frac{du(x)}{dx}\\big) = f(x), \\quad x \\in (0,1), \\quad u(0)=0,\\;u(1)=0, $$\nwhere the diffusion is nonlinear in a scalar parameter field via $\\kappa(m)=\\exp(m)$. The domain $(0,1)$ is decomposed into two non-overlapping subdomains $\\Omega_1=(0,0.5]$ and $\\Omega_2=[0.5,1)$ with continuity of state and flux at the interface. The parameter field is restricted to be piecewise constant across the decomposition, $m(x)=m_1$ on $\\Omega_1$ and $m(x)=m_2$ on $\\Omega_2$, so that $\\kappa(x)=\\exp(m_1)$ on $\\Omega_1$ and $\\kappa(x)=\\exp(m_2)$ on $\\Omega_2$.\n\nDiscretize the interval $[0,1]$ using a uniform grid with $N=33$ nodes at points $x_i = i h$, where $h = 1/(N-1)$ and $i \\in \\{0,1,\\dots,N-1\\}$. Let the interface be at node index $i_{\\mathrm{I}} = 16$ (i.e., $x_{i_{\\mathrm{I}}}=0.5$). Approximate the diffusion operator using the standard second-order conservative finite-difference stencil with variable coefficients, where edge-centered coefficients $\\kappa_{i+\\frac{1}{2}}$ are constant on each subdomain and taken to be $\\exp(m_1)$ for edges $i+\\frac{1}{2} \\in \\{0.5,1.5,\\dots,15.5\\}$ and $\\exp(m_2)$ for edges $i+\\frac{1}{2} \\in \\{16.5,17.5,\\dots,31.5\\}$. Use source term values $f_i = 1$ at all interior nodes $i \\in \\{1,2,\\dots,N-2\\}$ and $f_0=f_{N-1}=0$. Impose Dirichlet boundary conditions at $x_0=0$ and $x_{N-1}=1$. Enforce continuity of the discrete solution at the interface and the discrete balance of outward fluxes across the interface node using a Schur-complement or substructuring argument that couples the two subdomain solves through the interface unknown.\n\nDefine the observation operator that samples the discrete state at node indices $\\{6,12,20,26\\}$, corresponding to spatial locations $\\{x_6,x_{12},x_{20},x_{26}\\} = \\{6h,12h,20h,26h\\}$. Generate synthetic data by solving the forward problem with the ground-truth parameters $m_{\\mathrm{true}} = [0.2,-0.1]$ to obtain the discrete solution $u^{\\mathrm{true}}$, extracting its observed components $y^{\\mathrm{true}} \\in \\mathbb{R}^4$, and adding a fixed noise vector $\\eta = [10^{-4},-2\\cdot 10^{-4},1.5\\cdot 10^{-4},-10^{-4}]$ (in the same units as $u$) to obtain data $d = y^{\\mathrm{true}} + \\eta$.\n\nFor a given initial guess $m^{(0)}=[m^{(0)}_1,m^{(0)}_2]$, perform one Gauss–Newton step for the Tikhonov-regularized least-squares problem with zero-order regularization and identity data covariance,\n$$ \\min_{m \\in \\mathbb{R}^2} \\; \\frac{1}{2}\\|F(m)-d\\|_2^2 + \\frac{\\gamma}{2}\\|m - m_{\\mathrm{ref}}\\|_2^2, $$\nwhere $F(m)$ maps the parameter pair $m$ to the $4$ observations, $m_{\\mathrm{ref}}=[0,0]$, and $\\gamma>0$ is the regularization parameter. Use the Gauss–Newton linearization of the data misfit at $m^{(0)}$ and compute the parameter increment by solving the $2\\times 2$ linear system that arises from the normal equations with the Gauss–Newton approximation to the Hessian. The Jacobian action must be obtained via sensitivity equations derived from the linearization of the discrete forward model, and all forward and sensitivity linear systems must be solved using the two-subdomain decomposition with coupling at the interface as described above. After computing the updated parameter $m^{(1)}=m^{(0)}+\\delta m$, report the change in the data misfit,\n$$ \\Delta \\Phi = \\frac{1}{2}\\|F(m^{(0)})-d\\|_2^2 - \\frac{1}{2}\\|F(m^{(1)})-d\\|_2^2. $$\n\nYour program must implement the above and run the following test suite of three cases, each specified by an initial guess and a regularization parameter:\n- Case A (happy path): $m^{(0)} = [0.0,0.0]$, $\\gamma = 10^{-2}$.\n- Case B (high-contrast initial guess): $m^{(0)} = [-1.0,1.0]$, $\\gamma = 10^{-2}$.\n- Case C (strong regularization): $m^{(0)} = [0.0,0.0]$, $\\gamma = 1.0$.\n\nAll computations are nondimensional; no physical units are required. Angles are not involved. Percentages are not involved.\n\nYour program should produce a single line of output containing the three misfit decreases for Cases A, B, and C, respectively, as a comma-separated list enclosed in square brackets, with each number printed as a decimal float rounded to six digits after the decimal point (e.g., $[0.123456,0.000001,0.987654]$).",
            "solution": "The problem requires performing one Gauss-Newton optimization step for a parameter estimation problem governed by a one-dimensional nonlinear diffusion equation. The solution process involves several stages: numerical discretization of the PDE, implementation of a domain decomposition solver, generation of synthetic data, derivation and solution of sensitivity equations for the Jacobian, and finally, the assembly and solution of the Gauss-Newton system.\n\nFirst, we define the discretized forward problem. The governing equation is\n$$ -\\frac{d}{dx}\\left(\\kappa(m(x)) \\frac{du(x)}{dx}\\right) = f(x), \\quad x \\in (0,1) $$\nwith boundary conditions $u(0)=0$ and $u(1)=0$. The domain is discretized with $N=33$ nodes $x_i = i h$ for $i \\in \\{0, 1, \\dots, N-1\\}$, where $h=1/(N-1) = 1/32$. The parameter field is piecewise constant: $m(x)=m_1$ on $\\Omega_1=(0, 0.5]$ and $m(x)=m_2$ on $\\Omega_2=[0.5, 1)$, so the diffusion coefficient is $\\kappa(x)=\\kappa_1=\\exp(m_1)$ on $\\Omega_1$ and $\\kappa(x)=\\kappa_2=\\exp(m_2)$ on $\\Omega_2$.\n\nUsing a conservative finite-difference scheme on the interior nodes $i \\in \\{1, 2, \\dots, N-2\\}$, we obtain the discrete equation:\n$$ -\\frac{1}{h}\\left( \\kappa_{i+\\frac{1}{2}} \\frac{u_{i+1}-u_i}{h} - \\kappa_{i-\\frac{1}{2}} \\frac{u_i-u_{i-1}}{h} \\right) = f_i $$\nThis forms a system of linear equations $A(m)u_{int} = f_{int}$ for the vector of $N-2=31$ interior unknowns $u_{int} = [u_1, u_2, \\dots, u_{31}]^T$. The source term is $f_i=1$ for all interior nodes. The boundary conditions $u_0=0$ and $u_{32}=0$ are incorporated.\n\nThe problem specifies a domain decomposition approach based on substructuring. The interior nodes are partitioned into three sets: unknowns in $\\Omega_1$ ($u_{\\Omega_1} = [u_1, \\dots, u_{15}]^T$), the unknown at the interface $i_I=16$ ($u_I = u_{16}$), and unknowns in $\\Omega_2$ ($u_{\\Omega_2} = [u_{17}, \\dots, u_{31}]^T$). The system is reordered as:\n$$\n\\begin{pmatrix}\nA_{11} & 0 & A_{1I} \\\\\n0 & A_{22} & A_{2I} \\\\\nA_{I1} & A_{I2} & A_{II}\n\\end{pmatrix}\n\\begin{pmatrix}\nu_{\\Omega_1} \\\\\nu_{\\Omega_2} \\\\\nu_I\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nf_{\\Omega_1} \\\\\nf_{\\Omega_2} \\\\\nf_I\n\\end{pmatrix}\n$$\nThe diagonal blocks $A_{11}$ and $A_{22}$ are $15 \\times 15$ symmetric tridiagonal matrices representing the discrete Laplacian on each subdomain with Dirichlet conditions at the exterior boundary and the interface:\n$$ A_{11} = \\frac{\\kappa_1}{h^2} \\text{tridiag}(-1, 2, -1), \\quad A_{22} = \\frac{\\kappa_2}{h^2} \\text{tridiag}(-1, 2, -1) $$\nThe coupling blocks arise from the finite-difference equation at the interface node $i_I=16$:\n$$ \\frac{1}{h^2} (-\\kappa_1 u_{15} + (\\kappa_1 + \\kappa_2) u_{16} - \\kappa_2 u_{17}) = f_{16} $$\nThis defines $A_{II} = (\\kappa_1+\\kappa_2)/h^2$ and the off-diagonal coupling vectors.\n\nThis block system is solved using a Schur complement reduction. The interface unknown $u_I$ is found by solving the Schur complement system:\n$$ S u_I = f_I - A_{I1} A_{11}^{-1} f_{\\Omega_1} - A_{I2} A_{22}^{-1} f_{\\Omega_2} $$\nwhere the Schur complement $S$ is a scalar given by $S = A_{II} - A_{I1} A_{11}^{-1} A_{1I} - A_{I2} A_{22}^{-1} A_{2I}$. The required matrix-vector products involving the inverse of $A_{11}$ and $A_{22}$ are computed by solving linear systems on each subdomain. Once $u_I$ is known, the subdomain solutions are found by back-substitution:\n$$ u_{\\Omega_1} = A_{11}^{-1} (f_{\\Omega_1} - A_{1I} u_I), \\quad u_{\\Omega_2} = A_{22}^{-1} (f_{\\Omega_2} - A_{2I} u_I) $$\nThis procedure defines the forward operator $F(m)$, which maps the parameter vector $m=[m_1, m_2]^T$ to the vector of $4$ observations at nodes $\\{6, 12, 20, 26\\}$.\n\nSynthetic data $d$ are generated by computing $d = F(m_{\\mathrm{true}}) + \\eta$, with $m_{\\mathrm{true}}=[0.2,-0.1]$ and $\\eta = [10^{-4}, -2 \\cdot 10^{-4}, 1.5 \\cdot 10^{-4}, -10^{-4}]$. The inverse problem is to find $m$ that minimizes the Tikhonov-regularized cost function:\n$$ \\min_{m \\in \\mathbb{R}^2} \\; \\mathcal{J}(m) = \\frac{1}{2}\\|F(m)-d\\|_2^2 + \\frac{\\gamma}{2}\\|m - m_{\\mathrm{ref}}\\|_2^2 $$\nwith $m_{\\mathrm{ref}}=[0,0]$. We perform one Gauss-Newton step starting from an initial guess $m^{(0)}$. The parameter update $\\delta m$ is found by solving the $2 \\times 2$ linear system originating from the normal equations:\n$$ (J^T J + \\gamma I) \\delta m = J^T(d - F(m^{(0)})) - \\gamma(m^{(0)} - m_{\\mathrm{ref}}) $$\nwhere $J$ is the Jacobian matrix of $F$ evaluated at $m^{(0)}$, and $I$ is the $2 \\times 2$ identity matrix.\n\nThe columns of the Jacobian, $J_j = \\partial F / \\partial m_j$ for $j \\in \\{1,2\\}$, are computed using sensitivity analysis. Differentiating the forward system $A(m)u_{int}(m) = f_{int}$ with respect to $m_j$ yields a linear system for the sensitivity vector $s_j = \\partial u_{int} / \\partial m_j$:\n$$ A(m) s_j = -\\frac{\\partial A(m)}{\\partial m_j} u_{int}(m) =: g_j $$\nThis system for $s_j$ involves the same matrix $A(m)$ as the forward problem and is solved efficiently using the same Schur complement method. The right-hand side vector $g_j$ is assembled from the derivative of the discrete operator. For $j=1$, the non-zero components of $g_1$ correspond to equations for nodes $i \\in \\{1, \\dots, 16\\}$, and for $j=2$, they correspond to nodes $i \\in \\{16, \\dots, 31\\}$. Specifically, using the fact that $(A u)_i = f_i$, the components of the sensitivity right-hand-sides at $m^{(0)}$ are:\n$$ (g_1)_i = \\begin{cases} -f_i = -1 & i \\in \\{1, \\dots, 15\\} \\\\ -\\frac{\\kappa_1}{h^2}(-u_{15}+u_{16}) & i=16 \\\\ 0 & \\text{otherwise} \\end{cases} \\quad \\text{and} \\quad (g_2)_i = \\begin{cases} -\\frac{\\kappa_2}{h^2}(u_{16}-u_{17}) & i=16 \\\\ -f_i = -1 & i \\in \\{17, \\dots, 31\\} \\\\ 0 & \\text{otherwise} \\end{cases} $$\nAfter solving for $s_1$ and $s_2$, the Jacobian columns are obtained by applying the observation operator: $J_j = s_j|_{\\text{obs_indices}}$.\n\nThe algorithmic procedure for each test case is as follows:\n1.  Given $m^{(0)}$ and $\\gamma$, solve the forward problem for $u^{(0)}=u(m^{(0)})$ to find the predicted data $F(m^{(0)})$ and the initial data misfit $\\Phi_0 = \\frac{1}{2}\\|F(m^{(0)})-d\\|_2^2$.\n2.  Compute the sensitivity right-hand-sides $g_1$ and $g_2$ using $u^{(0)}$ and $m^{(0)}$.\n3.  Solve the sensitivity systems $A(m^{(0)})s_j=g_j$ for $s_1$ and $s_2$.\n4.  Form the $4 \\times 2$ Jacobian matrix $J$ from the observed components of $s_1$ and $s_2$.\n5.  Assemble and solve the $2 \\times 2$ Gauss-Newton system for the parameter update $\\delta m$.\n6.  Compute the updated parameter $m^{(1)} = m^{(0)} + \\delta m$.\n7.  Solve the forward problem for $u^{(1)}=u(m^{(1)})$ to find the new predicted data $F(m^{(1)})$ and the final data misfit $\\Phi_1 = \\frac{1}{2}\\|F(m^{(1)})-d\\|_2^2$.\n8.  The final result is the change in data misfit, $\\Delta \\Phi = \\Phi_0 - \\Phi_1$.\nThis procedure is repeated for each of the three test cases specified.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the entire process for the given test cases.\n    \"\"\"\n    N = 33\n    h = 1.0 / (N - 1)\n    i_I = 16  # Interface node index\n\n    # Observation nodes (1-based index)\n    obs_nodes = np.array([6, 12, 20, 26])\n    # Corresponding 0-based index in the interior solution vector u_int[0...30]\n    obs_indices_in_u_int = obs_nodes - 1\n\n    # Ground truth and data generation\n    m_true = np.array([0.2, -0.1])\n    eta = np.array([1e-4, -2e-4, 1.5e-4, -1e-4])\n    m_ref = np.array([0.0, 0.0])\n\n    u_true_int, _ = solve_forward(m_true, N, h, i_I)\n    y_true = u_true_int[obs_indices_in_u_int]\n    d = y_true + eta\n    \n    # Test cases\n    test_cases = [\n        # Case A\n        {'m0': np.array([0.0, 0.0]), 'gamma': 1e-2},\n        # Case B\n        {'m0': np.array([-1.0, 1.0]), 'gamma': 1e-2},\n        # Case C\n        {'m0': np.array([0.0, 0.0]), 'gamma': 1.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        m0 = case['m0']\n        gamma = case['gamma']\n\n        # 1. Evaluate forward model and Jacobian at m0\n        u0_int, F_m0 = solve_forward(m0, N, h, i_I, obs_indices_in_u_int)\n        \n        g1_rhs = compute_sensitivity_rhs(m0, u0_int, 1, h, N, i_I)\n        g2_rhs = compute_sensitivity_rhs(m0, u0_int, 2, h, N, i_I)\n        \n        s1_int, J_col1 = solve_forward(m0, N, h, i_I, obs_indices_in_u_int, rhs=g1_rhs)\n        s2_int, J_col2 = solve_forward(m0, N, h, i_I, obs_indices_in_u_int, rhs=g2_rhs)\n        \n        J = np.vstack((J_col1, J_col2)).T\n\n        # 2. Compute misfit and Gauss-Newton step\n        residual = d - F_m0\n        phi0 = 0.5 * np.dot(residual, residual)\n\n        H_GN = J.T @ J + gamma * np.eye(2)\n        rhs_GN = J.T @ residual - gamma * (m0 - m_ref)\n        \n        delta_m = np.linalg.solve(H_GN, rhs_GN)\n        m1 = m0 + delta_m\n\n        # 3. Evaluate forward model at m1 and compute final misfit\n        _, F_m1 = solve_forward(m1, N, h, i_I, obs_indices_in_u_int)\n        phi1 = 0.5 * np.dot(d - F_m1, d - F_m1)\n\n        delta_phi = phi0 - phi1\n        results.append(delta_phi)\n\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\n\ndef solve_schur_system(k1, k2, h, i_I, rhs_int):\n    \"\"\"\n    Solves a linear system using the Schur complement (substructuring) method.\n    `rhs_int` is the right-hand side for the interior nodes [1, ..., N-2].\n    \"\"\"\n    num_sub_nodes = i_I - 1  # 15 nodes in each subdomain interior\n\n    # Partition the RHS\n    rhs1 = rhs_int[:num_sub_nodes]\n    rhs_I = rhs_int[num_sub_nodes]\n    rhs2 = rhs_int[num_sub_nodes+1:]\n\n    # Build subdomain matrices A11 and A22 (banded format for solve_banded)\n    # A = diag(d) + diag(e, 1) + diag(e, -1) has banded form:\n    # [e, d, e] -> ab = [[0, *e], [d], [*e, 0]]\n    A11_banded = np.zeros((3, num_sub_nodes))\n    A11_banded[0, 1:] = -1.0\n    A11_banded[1, :] = 2.0\n    A11_banded[2, :-1] = -1.0\n    A11_banded *= k1 / h**2\n\n    A22_banded = np.zeros((3, num_sub_nodes))\n    A22_banded[0, 1:] = -1.0\n    A22_banded[1, :] = 2.0\n    A22_banded[2, :-1] = -1.0\n    A22_banded *= k2 / h**2\n\n    # Coupling vectors\n    A1I = np.zeros(num_sub_nodes); A1I[-1] = -k1 / h**2\n    A2I = np.zeros(num_sub_nodes); A2I[0] = -k2 / h**2\n    AI1 = A1I\n    AI2 = A2I\n\n    # Interface 'matrix'\n    AII = (k1 + k2) / h**2\n\n    # Solve for constituents of Schur complement system\n    # v1 = A11^{-1} f1\n    v1 = solve_banded((1, 1), A11_banded, rhs1)\n    # v2 = A22^{-1} f2\n    v2 = solve_banded((1, 1), A22_banded, rhs2)\n    # w1 = A11^{-1} A1I\n    w1 = solve_banded((1, 1), A11_banded, A1I)\n    # w2 = A22^{-1} A2I\n    w2 = solve_banded((1, 1), A22_banded, A2I)\n    \n    # Schur complement and its RHS (both are scalars)\n    S = AII - np.dot(AI1, w1) - np.dot(AI2, w2)\n    rhs_S = rhs_I - np.dot(AI1, v1) - np.dot(AI2, v2)\n    \n    # Solve for interface unknown\n    u_I = rhs_S / S\n    \n    # Back-substitute for subdomain unknowns\n    u1 = v1 - w1 * u_I\n    u2 = v2 - w2 * u_I\n    \n    # Assemble full interior solution\n    u_int = np.concatenate((u1, [u_I], u2))\n    return u_int\n\n\ndef solve_forward(m, N, h, i_I, obs_indices_in_u_int=None, rhs=None):\n    \"\"\"\n    Solves the forward or sensitivity problem. If rhs is None, solves the\n    forward problem with source f=1. Otherwise, solves with the given rhs.\n    Returns the full interior solution and optionally the observed values.\n    \"\"\"\n    k1, k2 = np.exp(m[0]), np.exp(m[1])\n    num_int_nodes = N - 2\n\n    if rhs is None:\n        # Standard forward problem RHS (f_i = 1 for all interior nodes)\n        rhs = np.ones(num_int_nodes)\n        \n    u_int = solve_schur_system(k1, k2, h, i_I, rhs)\n\n    if obs_indices_in_u_int is not None:\n        return u_int, u_int[obs_indices_in_u_int]\n    return u_int, None\n\n\ndef compute_sensitivity_rhs(m, u_int, param_idx, h, N, i_I):\n    \"\"\"\n    Computes the right-hand side vector g_j for the sensitivity equation.\n    param_idx is 1 for m1, 2 for m2.\n    \"\"\"\n    k1, k2 = np.exp(m[0]), np.exp(m[1])\n    num_int_nodes = N - 2\n    num_sub_nodes = i_I - 1\n\n    g = np.zeros(num_int_nodes)\n    \n    u_full = np.concatenate(([0], u_int, [0]))\n    u_15 = u_full[i_I - 1]\n    u_16 = u_full[i_I]\n    u_17 = u_full[i_I + 1]\n\n    if param_idx == 1:\n        g[:num_sub_nodes] = -1.0 # correspoding to nodes 1...15\n        g[num_sub_nodes] = -(k1 / h**2) * (-u_15 + u_16)\n    elif param_idx == 2:\n        g[num_sub_nodes] = -(k2 / h**2) * (u_16 - u_17)\n        g[num_sub_nodes+1:] = -1.0 # correspoding to nodes 17...31\n    \n    return g\n\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}