{
    "hands_on_practices": [
        {
            "introduction": "The Preconditioned Crank–Nicolson (pCN) algorithm is a cornerstone of dimension-independent sampling, particularly for Bayesian inverse problems where the prior distribution contains crucial information about the function-space structure of the unknown. This exercise  provides a foundational theoretical analysis of the pCN method. By deriving the lag-1 autocorrelation in an idealized setting, you will prove that the sampler's mixing performance is independent of the discretization dimension, revealing the core principle that enables scalability to high-dimensional state spaces.",
            "id": "3376428",
            "problem": "Consider a Bayesian linear inverse problem over a separable Hilbert space $\\mathcal{H}$ with Gaussian prior $u \\sim \\mathcal{N}(0,\\mathcal{C})$, where $\\mathcal{C}:\\mathcal{H}\\to\\mathcal{H}$ is a trace-class, self-adjoint, positive-definite covariance operator. Suppose the likelihood is flat (equivalently, the forward operator is the zero map so the data contain no information), so the posterior equals the prior. Let $\\{(\\lambda_{k},e_{k})\\}_{k\\geq 1}$ denote the eigenpairs of $\\mathcal{C}$ with $\\lambda_{k}>0$ and $\\{e_{k}\\}$ forming an orthonormal basis of $\\mathcal{H}$. For a fixed truncation level $J\\in\\mathbb{N}$, define the Karhunen–Loève (KL) truncation\n$$\nu^{(J)}=\\sum_{k=1}^{J}\\sqrt{\\lambda_{k}}\\,\\xi_{k}\\,e_{k},\\quad \\xi_{k}\\stackrel{\\text{i.i.d.}}{\\sim}\\mathcal{N}(0,1).\n$$\nLet $\\ell:\\mathcal{H}\\to\\mathbb{R}$ be a continuous linear functional with finite prior variance $\\sigma_{\\ell}^{2}=\\langle \\ell,\\mathcal{C}\\ell\\rangle$, where $\\langle\\cdot,\\cdot\\rangle$ denotes the dual pairing induced by $\\mathcal{H}$. Consider the Preconditioned Crank–Nicolson (pCN) Markov chain with parameter $\\beta\\in(0,1)$, defined on the truncated space by the update\n$$\nu_{n+1}^{(J)}=\\sqrt{1-\\beta^{2}}\\,u_{n}^{(J)}+\\beta\\,\\xi_{n+1}^{(J)},\n$$\nwhere $\\xi_{n+1}^{(J)}\\sim\\mathcal{N}(0,\\mathcal{C}^{(J)})$ is independent of $u_{n}^{(J)}$, and $\\mathcal{C}^{(J)}$ is the covariance corresponding to the $J$-mode truncation. In the zero-information likelihood setting described above, the pCN proposal is always accepted, so this defines the exact transition of the Markov chain.\n\nLet $s_{n}^{(J)}=\\ell(u_{n}^{(J)})$ denote the scalar time series obtained by applying $\\ell$ to the chain. Working under stationarity of the chain, derive the lag-$1$ autocorrelation of $s_{n}^{(J)}$, defined by\n$$\n\\rho_{1}^{(J)}=\\frac{\\operatorname{Cov}\\big(s_{n+1}^{(J)},s_{n}^{(J)}\\big)}{\\operatorname{Var}\\big(s_{n}^{(J)}\\big)},\n$$\nas an analytic expression in terms of $\\beta$ only. Prove that this expression is independent of the truncation level $J$. Express your final answer as a closed-form symbolic expression. No rounding is required, and no units are to be included in the final expression.",
            "solution": "The problem is determined to be valid as it is scientifically grounded in the theory of Bayesian inverse problems and Markov chain Monte Carlo methods, is well-posed with a clear objective, and is expressed in unambiguous, formal language. We may therefore proceed with the derivation.\n\nOur objective is to compute the lag-$1$ autocorrelation of the scalar time series $s_{n}^{(J)} = \\ell(u_{n}^{(J)})$, which is defined as:\n$$\n\\rho_{1}^{(J)} = \\frac{\\operatorname{Cov}\\big(s_{n+1}^{(J)}, s_{n}^{(J)}\\big)}{\\operatorname{Var}\\big(s_{n}^{(J)}\\big)}\n$$\nThe problem specifies that the Markov chain is in its stationary state. The Preconditioned Crank–Nicolson (pCN) algorithm is designed to have the target distribution as its invariant measure. In this case, with a flat likelihood, the target distribution is the prior. For the $J$-dimensional truncated system, the target distribution is $\\mathcal{N}(0, \\mathcal{C}^{(J)})$, where $\\mathcal{C}^{(J)}$ is the covariance operator corresponding to the $J$-mode truncation. Therefore, for any time index $n$ in the stationary regime, the state $u_{n}^{(J)}$ is a random variable drawn from this distribution:\n$$\nu_{n}^{(J)} \\sim \\mathcal{N}(0, \\mathcal{C}^{(J)})\n$$\nThe scalar time series is obtained by applying the continuous linear functional $\\ell$ to the state, $s_{n}^{(J)} = \\ell(u_{n}^{(J)})$. Since $u_{n}^{(J)}$ is a mean-zero Gaussian random variable and $\\ell$ is linear, $s_{n}^{(J)}$ is also a mean-zero Gaussian random variable. We can formally show this by computing its expectation:\n$$\n\\mathbb{E}\\big[s_{n}^{(J)}\\big] = \\mathbb{E}\\big[\\ell(u_{n}^{(J)})\\big] = \\ell\\big(\\mathbb{E}[u_{n}^{(J)}]\\big) = \\ell(0) = 0\n$$\nThe denominator in the definition of $\\rho_{1}^{(J)}$ is the variance of $s_{n}^{(J)}$. As the mean is zero, the variance is given by:\n$$\n\\operatorname{Var}\\big(s_{n}^{(J)}\\big) = \\mathbb{E}\\big[(s_{n}^{(J)})^{2}\\big]\n$$\nThis is the variance of a linear functional applied to a Gaussian random variable, which is given by the quadratic form associated with the covariance operator:\n$$\n\\operatorname{Var}\\big(s_{n}^{(J)}\\big) = \\langle \\ell, \\mathcal{C}^{(J)}\\ell \\rangle\n$$\nWe assume $\\ell$ is not the zero functional, ensuring that $\\operatorname{Var}(s_{n}^{(J)}) > 0$ for any $J$ where the support of the truncated prior has a non-trivial projection onto $\\ell$. This is guaranteed for any $J\\ge 1$ if we assume that the first component of $\\ell$ with respect to the basis $\\{e_k\\}$ is non-zero. The problem's condition that $\\sigma_{\\ell}^{2}=\\langle \\ell,\\mathcal{C}\\ell\\rangle$ is finite implies $\\ell$ is in a suitable domain, and assuming $\\ell \\neq 0$ implies $\\sigma_{\\ell}^2 > 0$.\n\nNext, we address the numerator, the covariance term $\\operatorname{Cov}(s_{n+1}^{(J)}, s_{n}^{(J)})$. Since the means are zero, this simplifies to:\n$$\n\\operatorname{Cov}\\big(s_{n+1}^{(J)}, s_{n}^{(J)}\\big) = \\mathbb{E}\\big[s_{n+1}^{(J)}s_{n}^{(J)}\\big]\n$$\nWe use the pCN update rule for $u_{n+1}^{(J)}$ given in the problem statement:\n$$\nu_{n+1}^{(J)} = \\sqrt{1-\\beta^{2}}\\,u_{n}^{(J)} + \\beta\\,\\xi_{n+1}^{(J)}\n$$\nwhere $\\beta \\in (0,1)$ and the innovation term $\\xi_{n+1}^{(J)} \\sim \\mathcal{N}(0, \\mathcal{C}^{(J)})$ is independent of $u_{n}^{(J)}$. Applying the linear functional $\\ell$ to both sides of the update equation yields an expression for $s_{n+1}^{(J)}$:\n$$\ns_{n+1}^{(J)} = \\ell\\left(\\sqrt{1-\\beta^{2}}\\,u_{n}^{(J)} + \\beta\\,\\xi_{n+1}^{(J)}\\right) = \\sqrt{1-\\beta^{2}}\\,\\ell(u_{n}^{(J)}) + \\beta\\,\\ell(\\xi_{n+1}^{(J)})\n$$\nRecognizing that $\\ell(u_{n}^{(J)}) = s_{n}^{(J)}$, we have the following autoregressive relationship for the scalar time series:\n$$\ns_{n+1}^{(J)} = \\sqrt{1-\\beta^{2}}\\,s_{n}^{(J)} + \\beta\\,\\ell(\\xi_{n+1}^{(J)})\n$$\nWe now compute the expectation $\\mathbb{E}[s_{n+1}^{(J)}s_{n}^{(J)}]$:\n$$\n\\mathbb{E}\\big[s_{n+1}^{(J)}s_{n}^{(J)}\\big] = \\mathbb{E}\\left[\\left(\\sqrt{1-\\beta^{2}}\\,s_{n}^{(J)} + \\beta\\,\\ell(\\xi_{n+1}^{(J)})\\right) s_{n}^{(J)}\\right]\n$$\nBy linearity of expectation:\n$$\n\\mathbb{E}\\big[s_{n+1}^{(J)}s_{n}^{(J)}\\big] = \\sqrt{1-\\beta^{2}}\\,\\mathbb{E}\\big[(s_{n}^{(J)})^{2}\\big] + \\beta\\,\\mathbb{E}\\big[\\ell(\\xi_{n+1}^{(J)})s_{n}^{(J)}\\big]\n$$\nThe first term is recognizable as $\\sqrt{1-\\beta^{2}}\\,\\operatorname{Var}(s_{n}^{(J)})$. In the second term, we analyze the expectation $\\mathbb{E}[\\ell(\\xi_{n+1}^{(J)})s_{n}^{(J)}]$. By construction, the innovation $\\xi_{n+1}^{(J)}$ is independent of the current state $u_{n}^{(J)}$. Consequently, any function of $\\xi_{n+1}^{(J)}$, such as $\\ell(\\xi_{n+1}^{(J)})$, is independent of any function of $u_{n}^{(J)}$, such as $s_{n}^{(J)}$. Therefore, the expectation of the product is the product of the expectations:\n$$\n\\mathbb{E}\\big[\\ell(\\xi_{n+1}^{(J)})s_{n}^{(J)}\\big] = \\mathbb{E}\\big[\\ell(\\xi_{n+1}^{(J)})\\big] \\mathbb{E}\\big[s_{n}^{(J)}\\big]\n$$\nWe have already established that $\\mathbb{E}[s_{n}^{(J)}] = 0$. The expectation of the innovation term is also zero:\n$$\n\\mathbb{E}\\big[\\ell(\\xi_{n+1}^{(J)})\\big] = \\ell\\left(\\mathbb{E}[\\xi_{n+1}^{(J)}]\\right) = \\ell(0) = 0\n$$\nThus, the second term vanishes entirely:\n$$\n\\beta\\,\\mathbb{E}\\big[\\ell(\\xi_{n+1}^{(J)})s_{n}^{(J)}\\big] = \\beta \\cdot 0 \\cdot 0 = 0\n$$\nThis leaves us with the final expression for the covariance:\n$$\n\\operatorname{Cov}\\big(s_{n+1}^{(J)}, s_{n}^{(J)}\\big) = \\sqrt{1-\\beta^{2}}\\,\\operatorname{Var}\\big(s_{n}^{(J)}\\big)\n$$\nWe can now assemble the lag-$1$ autocorrelation:\n$$\n\\rho_{1}^{(J)} = \\frac{\\operatorname{Cov}\\big(s_{n+1}^{(J)}, s_{n}^{(J)}\\big)}{\\operatorname{Var}\\big(s_{n}^{(J)}\\big)} = \\frac{\\sqrt{1-\\beta^{2}}\\,\\operatorname{Var}\\big(s_{n}^{(J)}\\big)}{\\operatorname{Var}\\big(s_{n}^{(J)}\\big)}\n$$\nProvided $\\operatorname{Var}(s_{n}^{(J)}) \\neq 0$, we cancel the variance terms to obtain:\n$$\n\\rho_{1}^{(J)} = \\sqrt{1-\\beta^{2}}\n$$\nThis result is a function only of the pCN parameter $\\beta$ and is indeed independent of the truncation level $J$, the covariance operator $\\mathcal{C}$, and the choice of linear functional $\\ell$. This property is a hallmark of the pCN method in this idealized setting, where the autoregressive structure of the algorithm directly translates to the autoregressive structure of any linear observable.",
            "answer": "$$\\boxed{\\sqrt{1-\\beta^{2}}}$$"
        },
        {
            "introduction": "While pCN is robust, it can be inefficient when the posterior is substantially different from the prior. The function-space Metropolis-Adjusted Langevin Algorithm (MALA) incorporates gradient information from the likelihood to propose more efficient moves. This hands-on exercise  guides you through implementing a MALA sampler for a PDE-constrained inverse problem, where the key is to precondition the Langevin dynamics with the prior covariance. You will empirically verify the algorithm's dimension-independence by observing that a fixed step size yields a stable acceptance rate as the discretization of the underlying function space is refined.",
            "id": "3376373",
            "problem": "Consider the one-dimensional diffusion initial value problem on the unit interval with periodic boundary conditions, defined by the partial differential equation (PDE) $\\,\\partial_t y - \\partial_x\\!\\left(u(x)\\,\\partial_x y\\right)=0\\,$ for $\\,x\\in[0,1]\\,$ and $\\,t\\ge 0\\,$, where $\\,u(x)\\,$ is the spatially varying diffusion coefficient to be inferred from observations. Let the initial condition be $\\,y(x,0)=y_0(x)\\,$ with a smooth $\\,y_0\\,$. Assume observations are approximated at a small time $\\,T>0\\,$ by the linearized relation\n$$\ny(x,T)\\approx y_0(x) + T\\,\\partial_x\\!\\left(u(x)\\,\\partial_x y_0(x)\\right),\n$$\nso the discrete data vector $\\,d\\,$ is modeled by\n$$\nd = A u + \\varepsilon,\\quad A u := T\\,D u,\\quad D u := \\partial_x u \\cdot \\partial_x y_0 + u \\cdot \\partial_{xx} y_0,\n$$\nwhere $\\,\\varepsilon\\,$ is mean-zero Gaussian noise with covariance $\\,\\Gamma = \\sigma^2 I\\,$, and $\\,I\\,$ is the identity on the discrete grid.\n\nAdopt a Gaussian prior for $\\,u\\,$ with mean $\\,m(x)\\,$ and covariance operator $\\,\\mathcal{C} = \\left(\\alpha I - \\Delta\\right)^{-\\kappa}\\,$ on $L^2([0,1])$ with periodic boundary conditions, where $\\,\\alpha>0\\,$, $\\,\\kappa\\ge 1\\,$, and $\\,\\Delta\\,$ is the Laplacian. The negative log-posterior (up to an additive constant) for a discrete grid with spacing $\\,h\\,$ is\n$$\nJ(u) = \\frac{1}{2}\\,\\left\\langle A u - d,\\,\\Gamma^{-1}(A u - d)\\right\\rangle_{L^2} + \\frac{1}{2}\\,\\left\\langle u - m,\\,\\mathcal{C}^{-1}(u - m)\\right\\rangle_{L^2},\n$$\nwhere the $L^2$ inner product is discretized by $\\,\\langle v,w\\rangle_{L^2} \\approx h \\sum_j v_j w_j\\,$.\n\nConstruct a function-space Metropolis-Adjusted Langevin Algorithm (MALA) that updates $\\,u\\,$ via the preconditioned proposal\n$$\nu' = u + \\eta\\,\\mathcal{C}\\,\\nabla \\Phi(u) + \\sqrt{2\\eta}\\,\\mathcal{C}^{1/2}\\,\\xi,\n$$\nwhere $\\,\\Phi(u)\\,$ is the log-posterior (so $\\,\\nabla \\Phi(u) = -\\nabla J(u)\\,$), $\\,\\eta>0\\,$ is the step size, and $\\xi\\sim \\mathcal{N}(0,I)\\,$ is a white noise on the grid. Use the adjoint $\\,A^*\\,$ defined by the $L^2$ inner product to compute\n$$\n\\nabla J(u) = A^*\\Gamma^{-1}(Au-d) + \\mathcal{C}^{-1}(u-m).\n$$\nThe resulting proposal density is Gaussian with mean $\\,u + \\eta\\,g(u)\\,$ and covariance $\\,2\\eta\\,\\mathcal{C}\\,$, where $\\,g(u)=-\\mathcal{C}\\,\\nabla J(u)\\,$, and the MALA acceptance probability is\n$$\na(u,u') = \\min\\left\\{1,\\ \\exp\\!\\Big(\\Phi(u')-\\Phi(u) + \\log q(u\\mid u') - \\log q(u'\\mid u)\\Big)\\right\\},\n$$\nwith\n$$\n\\log q(u'\\mid u) = -\\frac{1}{4\\eta}\\,\\left\\langle u' - u - \\eta g(u),\\,\\mathcal{C}^{-1}\\left(u' - u - \\eta g(u)\\right)\\right\\rangle_{L^2} + \\text{const}.\n$$\n\nYour tasks are:\n- Derive the explicit discrete forms of $\\,D\\,$ and its $L^2$ adjoint $\\,D^*\\,$ under periodic boundary conditions, starting from the PDE and integration by parts.\n- Use these to implement $\\,A\\,$ and $\\,A^*\\,$ on a uniform grid of size $\\,N\\,$ with spacing $\\,h=1/N\\,$.\n- Implement $\\,\\mathcal{C}\\,$, $\\,\\mathcal{C}^{-1}\\,$, and $\\,\\mathcal{C}^{1/2}\\,$ using the discrete Fourier transform diagonalization of $\\,\\alpha I - \\Delta\\,$ under periodic boundary conditions. Use $\\,\\kappa=1\\,$.\n- Construct and run the function-space MALA sampler with the acceptance rule above, and compute the empirical acceptance rate over a fixed number of iterations for each test case.\n\nIdentify and justify a scaling of the step size $\\,\\eta\\,$ that preserves dimension-independent (DI) behavior as the grid size $\\,N\\,$ increases, in the sense that acceptance rates remain approximately stable across refinements. Demonstrate this empirically by running your sampler with a single constant choice of $\\,\\eta\\,$ across all grids.\n\nParameters for the test suite:\n- Use $\\,y_0(x) = \\sin(2\\pi x) + \\tfrac{1}{2}\\cos(6\\pi x)\\,$; discretize it on the grid.\n- Set $\\,T = 10^{-2}\\,$, $\\,\\sigma = 5\\times 10^{-2}\\,$, $\\,\\alpha = 1\\,$, $\\,\\kappa = 1\\,$, and prior mean $\\,m(x)\\equiv 1\\,$.\n- Generate synthetic data $\\,d\\,$ by drawing a ground truth $\\,u^\\dagger\\,$ from the prior (with small amplitude to keep $\\,u^\\dagger>0\\,$) and setting $\\,d = A u^\\dagger + \\varepsilon\\,$ with $\\,\\varepsilon\\sim \\mathcal{N}(0,\\sigma^2 I)\\,$.\n- Use a fixed random seed to ensure reproducibility.\n- Run the MALA chain from initialization $\\,u_0=m\\,$ for $\\,K\\,$ iterations with the same constant $\\,\\eta\\,$ for all grids.\n\nTest cases:\n- Case 1: $\\,N=32\\,$.\n- Case 2: $\\,N=64\\,$.\n- Case 3: $\\,N=128\\,$.\n\nOutput specification:\n- For each test case, compute the empirical acceptance rate as a float in $\\,[0,1]\\,$, rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, for example $\\,\\left[r_1,r_2,r_3\\right]\\,$.\n\nThe program must be a complete, runnable script that performs all computations and outputs only the specified final line.",
            "solution": "The problem requires the implementation and analysis of a function-space Metropolis-Adjusted Langevin Algorithm (MALA) for a Bayesian inverse problem involving a one-dimensional diffusion equation. The core tasks are to derive and implement the necessary discrete operators, construct the MALA sampler, and demonstrate its dimension-independent convergence properties.\n\n### 1. Discretization and Operator Derivations\n\nWe operate on a uniform grid of $N$ points $x_j = j h$ for $j=0, \\dots, N-1$, with grid spacing $h=1/N$. Functions are represented as vectors of their values at these grid points. The $L^2$ inner product is discretized as $\\langle v, w \\rangle_{L^2} \\approx h \\sum_{j=0}^{N-1} v_j w_j$.\n\n**Forward Operator $A$ and its Adjoint $A^*$**\n\nThe forward operator is defined by $A u = T D u$, where $D u = (\\partial_x y_0) (\\partial_x u) + (\\partial_{xx} y_0) u$.\nTo discretize $D$, we use a second-order central difference scheme for the derivative $\\partial_x u$, which is appropriate for periodic boundary conditions:\n$$\n(\\partial_x u)_j \\approx \\frac{u_{j+1} - u_{j-1}}{2h}\n$$\nwhere indices are taken modulo $N$. Let $\\mathbf{u}$, $\\mathbf{\\partial_x y_0}$, and $\\mathbf{\\partial_{xx} y_0}$ be the vectors representing the functions on the grid. The discrete operator $D$ acting on $\\mathbf{u}$ is:\n$$\n(D \\mathbf{u})_j = (\\mathbf{\\partial_x y_0})_j \\left( \\frac{\\mathbf{u}_{j+1} - \\mathbf{u}_{j-1}}{2h} \\right) + (\\mathbf{\\partial_{xx} y_0})_j \\mathbf{u}_j\n$$\nThe operator $A$ is then simply $A = T D$.\n\nTo find the $L^2$-adjoint $D^*$, we use the definition $\\langle Dv, w \\rangle_{L^2} = \\langle v, D^*w \\rangle_{L^2}$. At the continuous level, using integration by parts and periodicity:\n$$\n\\begin{align*}\n\\langle Du, v \\rangle_{L^2} &= \\int_0^1 \\left( (\\partial_x y_0)(\\partial_x u) + (\\partial_{xx} y_0) u \\right) v \\, dx \\\\\n&= \\int_0^1 (\\partial_x y_0) v (\\partial_x u) \\, dx + \\int_0^1 u (\\partial_{xx} y_0) v \\, dx \\\\\n&= \\left[ u (\\partial_x y_0) v \\right]_0^1 - \\int_0^1 u \\, \\partial_x ((\\partial_x y_0) v) \\, dx + \\int_0^1 u (\\partial_{xx} y_0) v \\, dx \\\\\n&= -\\int_0^1 u \\left( (\\partial_{xx} y_0) v + (\\partial_x y_0) (\\partial_x v) \\right) \\, dx + \\int_0^1 u (\\partial_{xx} y_0) v \\, dx \\\\\n&= \\int_0^1 u \\left( -(\\partial_x y_0) (\\partial_x v) \\right) \\, dx\n\\end{align*}\n$$\nThus, we identify the adjoint operator as $D^* v = -(\\partial_x y_0) (\\partial_x v)$. Its discrete form, using the same central difference scheme for $\\partial_x v$, is:\n$$\n(D^* \\mathbf{v})_j = -(\\mathbf{\\partial_x y_0})_j \\left( \\frac{\\mathbf{v}_{j+1} - \\mathbf{v}_{j-1}}{2h} \\right)\n$$\nThe adjoint of the full forward operator is $A^* = (TD)^* = T D^*$, as $T$ is a real scalar.\n\n### 2. Prior Covariance Operators in Fourier Space\n\nThe prior covariance $\\mathcal{C} = (\\alpha I - \\Delta)^{-\\kappa}$ with $\\kappa=1$ is defined on a space of periodic functions. Such translation-invariant operators are diagonalized by the Fourier basis. The operator $\\mathcal{C}$ and its powers ($\\mathcal{C}^{-1}$, $\\mathcal{C}^{1/2}$) can be efficiently applied using the Fast Fourier Transform (FFT).\n\nThe eigenvalues of the negative Laplacian $-\\Delta$ on the periodic interval $[0,1]$ are $(2\\pi k)^2$ for integer $k$. On a discrete grid of size $N$ with spacing $h$, the corresponding discrete wavenumbers are $q_k = 2\\pi k'$, where $k' = \\texttt{fftfreq}(N, h)$. The discrete eigenvalues of $-\\Delta$ are $\\lambda_k = q_k^2$.\n\nThe operator $\\mathcal{C}^{-1} = \\alpha I - \\Delta$ is a multiplication operator in Fourier space with eigenvalues $\\mu_k = \\alpha + \\lambda_k = \\alpha + (2\\pi k')^2$. Consequently, the action of $\\mathcal{C}$, $\\mathcal{C}^{-1}$, and $\\mathcal{C}^{1/2}$ on a vector $\\mathbf{v}$ is computed as:\n1. Compute the Discrete Fourier Transform: $\\hat{\\mathbf{v}} = \\text{FFT}(\\mathbf{v})$.\n2. Multiply the Fourier coefficients $\\hat{\\mathbf{v}}_k$ by the corresponding eigenvalues:\n   - For $\\mathcal{C}\\mathbf{v}$: multiply by $\\mu_k^{-\\kappa} = (\\alpha + \\lambda_k)^{-1}$ (since $\\kappa=1$).\n   - For $\\mathcal{C}^{-1}\\mathbf{v}$: multiply by $\\mu_k^{\\kappa} = \\alpha + \\lambda_k$.\n   - For $\\mathcal{C}^{1/2}\\mathbf{v}$: multiply by $\\mu_k^{-\\kappa/2} = (\\alpha + \\lambda_k)^{-1/2}$.\n3. Compute the Inverse Discrete Fourier Transform of the result.\n\nThis procedure provides a matrix-free and efficient ($O(N \\log N)$) implementation of these operators.\n\n### 3. MALA Sampler and Acceptance Probability\n\nThe function-space MALA proposes a new state $u'$ from the current state $u$ via a preconditioned Langevin-type update:\n$$\nu' = u + \\eta\\,g(u) + \\sqrt{2\\eta}\\,\\mathcal{C}^{1/2}\\,\\xi\n$$\nwhere $\\eta$ is the step size, $\\xi \\sim \\mathcal{N}(0,I)$ is discrete white noise, and $g(u) = -\\mathcal{C}\\,\\nabla \\Phi(u) = -\\mathcal{C}(-\\nabla J(u)) = \\mathcal{C}\\,\\nabla J(u)$. The problem statement has a slight sign inconsistency; the proposal should be a step in the direction of increasing log-posterior $\\Phi$. We use the standard form for MALA which is a gradient ascent on $\\Phi$:\n$u' = u + \\eta\\,\\mathcal{C}\\,\\nabla\\Phi(u) + \\dots = u - \\eta\\,\\mathcal{C}\\,\\nabla J(u) + \\dots$.\nHowever, the problem defines $g(u)=-\\mathcal{C}\\nabla J(u)$ and uses a plus sign in the proposal. To be consistent with the provided formulas, we will follow the problem statement: $u' = u + \\eta\\,g(u) + \\dots$.\n\nThe gradient of the negative log-posterior $J(u)$ with respect to the $L^2$ inner product is:\n$$\n\\nabla J(u) = A^*\\Gamma^{-1}(Au-d) + \\mathcal{C}^{-1}(u-m) = \\frac{1}{\\sigma^2}A^*(Au-d) + \\mathcal{C}^{-1}(u-m)\n$$\nThe acceptance probability is $a(u,u') = \\min\\{1, \\exp(\\log\\alpha)\\}$ where the log-ratio is:\n$$\n\\log\\alpha = \\Phi(u') - \\Phi(u) + \\log q(u\\mid u') - \\log q(u'\\mid u)\n$$\nUsing $\\Phi(u) = -J(u)$, this becomes:\n$$\n\\log\\alpha = J(u) - J(u') + \\log q(u\\mid u') - \\log q(u'\\mid u)\n$$\nThe log-proposal densities are given by:\n$$\n\\log q(v \\mid w) = -\\frac{1}{4\\eta}\\,\\left\\langle v - w - \\eta g(w),\\,\\mathcal{C}^{-1}\\left(v - w - \\eta g(w)\\right)\\right\\rangle_{L^2} + \\text{const}\n$$\nTo compute this term, we calculate the residual $r = v - w - \\eta g(w)$ and then evaluate the inner product $\\langle r, \\mathcal{C}^{-1}r \\rangle_{L^2} = \\langle \\mathcal{C}^{-1/2}r, \\mathcal{C}^{-1/2}r \\rangle_{L^2}$, which discretizes to $h \\sum_j (\\mathcal{C}^{-1/2}r)_j^2$. All components are thus computed using the operators derived above.\n\n### 4. Justification for Dimension-Independent Step Size\n\nStandard high-dimensional MCMC methods, like random walk Metropolis or un-preconditioned MALA, suffer from the curse of dimensionality: to maintain a constant acceptance rate, the step size $\\eta$ must shrink as the state-space dimension $N$ increases (e.g., $\\eta \\propto N^{-1/3}$ for standard MALA).\n\nThe function-space MALA implemented here is specifically designed to overcome this issue. The key is the preconditioning of the update by the prior covariance operator $\\mathcal{C}$. The proposal consists of two parts: a drift term $\\eta g(u) = -\\eta \\mathcal{C} \\nabla J(u)$ and a diffusion term $\\sqrt{2\\eta}\\mathcal{C}^{1/2}\\xi$. Both parts are scaled by operators related to $\\mathcal{C}$. This is equivalent to performing a standard MALA on a \"whitened\" parameter $v$, where $u = m + \\mathcal{C}^{1/2} v$, and the prior on $v$ is the standard Gaussian $\\mathcal{N}(0, I)$.\n\nBy incorporating the prior structure into the proposal mechanism, the algorithm proposes steps that are natural for the function space defined by the prior. The sampler is effectively exploring the space of functions rather than a high-dimensional vector space. As the discretization is refined (i.e., $N$ increases), the sampler's performance (including its acceptance rate for a fixed $\\eta$) is expected to remain stable, provided the underlying function-space problem is well-posed. This dimension-independent behavior is a hallmark of function-space MCMC methods. We demonstrate this by using a single constant step size $\\eta$ for all grid sizes ($N=32, 64, 128$) and observing that the empirical acceptance rates are approximately constant.",
            "answer": "```python\nimport numpy as np\nfrom numpy.fft import fft, ifft, fftfreq\n\n# This program implements a function-space Metropolis-Adjusted Langevin Algorithm (MALA)\n# to solve a Bayesian inverse problem for the diffusion coefficient in a 1D PDE.\n\ndef solve():\n    \"\"\"\n    Main function to run the MALA simulation for specified test cases and print the results.\n    \"\"\"\n    # Problem parameters\n    T = 1e-2\n    SIGMA = 5e-2\n    ALPHA = 1.0\n    KAPPA = 1.0\n    PRIOR_MEAN_VAL = 1.0\n    K_ITER = 10000  # Number of MCMC iterations\n    ETA = 0.04      # MALA step size, constant across all dimensions\n    SEED = 42\n\n    def y0_func(x):\n        return np.sin(2 * np.pi * x) + 0.5 * np.cos(6 * np.pi * x)\n\n    def dy0dx_func(x):\n        return 2 * np.pi * np.cos(2 * np.pi * x) - 3 * np.pi * np.sin(6 * np.pi * x)\n\n    def d2y0dx2_func(x):\n        return -4 * np.pi**2 * np.sin(2 * np.pi * x) - 18 * np.pi**2 * np.cos(6 * np.pi * x)\n\n    def get_fourier_multipliers(N, h, alpha, kappa):\n        \"\"\"Computes Fourier multipliers for C, C^-1, C^1/2, C^-1/2.\"\"\"\n        fourier_freqs = 2 * np.pi * fftfreq(N, h)\n        lambda_k = fourier_freqs**2  # Eigenvalues of -Delta\n        mu_k = alpha + lambda_k     # Eigenvalues of (alpha*I - Delta)\n        \n        # Regularize to avoid division by zero, although not expected for alpha > 0\n        mu_k[mu_k == 0] = 1e-16\n\n        C_mult = mu_k**(-kappa)\n        C_inv_mult = mu_k**(kappa)\n        C_half_mult = mu_k**(-kappa / 2.0)\n        C_half_inv_mult = mu_k**(kappa / 2.0)\n        return C_mult, C_inv_mult, C_half_mult, C_half_inv_mult\n\n    def apply_fourier_op(v, multipliers):\n        \"\"\"Applies a Fourier-diagonal operator.\"\"\"\n        return np.real(ifft(multipliers * fft(v)))\n\n    def apply_A(u, T, dy0_dx, d2y0_dx2, h):\n        \"\"\"Applies the forward operator A.\"\"\"\n        # Central difference for du/dx with periodic boundary conditions\n        du_dx = (np.roll(u, -1) - np.roll(u, 1)) / (2 * h)\n        D_u = dy0_dx * du_dx + d2y0_dx2 * u\n        return T * D_u\n\n    def apply_A_star(v, T, dy0_dx, h):\n        \"\"\"Applies the adjoint operator A*.\"\"\"\n        # Central difference for dv/dx with periodic boundary conditions\n        dv_dx = (np.roll(v, -1) - np.roll(v, 1)) / (2 * h)\n        D_star_v = -dy0_dx * dv_dx\n        return T * D_star_v\n\n    def compute_J(u, d, m, T, dy0_dx, d2y0_dx2, h, sigma, C_half_inv_mult):\n        \"\"\"Computes the negative log-posterior J(u).\"\"\"\n        Au_minus_d = apply_A(u, T, dy0_dx, d2y0_dx2, h) - d\n        misfit_term = (1.0 / (2.0 * sigma**2)) * h * np.sum(Au_minus_d**2)\n        \n        u_minus_m = u - m\n        C_half_inv_u_minus_m = apply_fourier_op(u_minus_m, C_half_inv_mult)\n        reg_term = 0.5 * h * np.sum(C_half_inv_u_minus_m**2)\n\n        return misfit_term + reg_term\n\n    def run_mala_for_N(N, K, eta):\n        \"\"\"Runs the MALA sampler for a given grid size N.\"\"\"\n        h = 1.0 / N\n        x_grid = np.arange(N) * h\n        \n        dy0_dx = dy0dx_func(x_grid)\n        d2y0_dx2 = d2y0dx2_func(x_grid)\n        m = np.full(N, PRIOR_MEAN_VAL)\n\n        C_mult, C_inv_mult, C_half_mult, C_half_inv_mult = get_fourier_multipliers(N, h, ALPHA, KAPPA)\n\n        rng = np.random.default_rng(SEED)\n        \n        # Generate synthetic data\n        zeta_true = rng.standard_normal(N)\n        u_true = m + apply_fourier_op(zeta_true, C_half_mult)\n        d_true = apply_A(u_true, T, dy0_dx, d2y0_dx2, h)\n        noise = rng.normal(0, SIGMA, N)\n        d = d_true + noise\n\n        # MALA chain initialization\n        u = np.copy(m)\n        accepted_count = 0\n        J_u = compute_J(u, d, m, T, dy0_dx, d2y0_dx2, h, SIGMA, C_half_inv_mult)\n        \n        for _ in range(K):\n            # 1. Compute gradient of J\n            Au_minus_d = apply_A(u, T, dy0_dx, d2y0_dx2, h) - d\n            grad_misfit = apply_A_star(Au_minus_d / SIGMA**2, T, dy0_dx, h)\n            grad_reg = apply_fourier_op(u - m, C_inv_mult)\n            grad_J = grad_misfit + grad_reg\n\n            # 2. Compute proposal drift term g(u)\n            g_u = -apply_fourier_op(grad_J, C_mult)\n\n            # 3. Generate proposal u'\n            xi = rng.standard_normal(N)\n            noise_term = np.sqrt(2 * eta) * apply_fourier_op(xi, C_half_mult)\n            u_proposal = u + eta * g_u + noise_term\n            \n            # 4. Compute acceptance probability\n            J_u_proposal = compute_J(u_proposal, d, m, T, dy0_dx, d2y0_dx2, h, SIGMA, C_half_inv_mult)\n            \n            # log q(u'|u) term\n            log_q_num_term = u_proposal - u - eta * g_u\n            C_half_inv_log_q_num = apply_fourier_op(log_q_num_term, C_half_inv_mult)\n            log_q_u_prime_given_u = -1.0/(4.0*eta) * h * np.sum(C_half_inv_log_q_num**2)\n            \n            # log q(u|u') term requires g(u')\n            Au_prop_minus_d = apply_A(u_proposal, T, dy0_dx, d2y0_dx2, h) - d\n            grad_misfit_prop = apply_A_star(Au_prop_minus_d / SIGMA**2, T, dy0_dx, h)\n            grad_reg_prop = apply_fourier_op(u_proposal - m, C_inv_mult)\n            grad_J_prop = grad_misfit_prop + grad_reg_prop\n            g_u_prime = -apply_fourier_op(grad_J_prop, C_mult)\n            \n            log_q_den_term = u - u_proposal - eta * g_u_prime\n            C_half_inv_log_q_den = apply_fourier_op(log_q_den_term, C_half_inv_mult)\n            log_q_u_given_u_prime = -1.0/(4.0*eta) * h * np.sum(C_half_inv_log_q_den**2)\n            \n            log_alpha = (J_u - J_u_proposal) + log_q_u_given_u_prime - log_q_u_prime_given_u\n            \n            # 5. Accept/Reject\n            if np.log(rng.uniform(0, 1))  log_alpha:\n                u = u_proposal\n                J_u = J_u_proposal\n                accepted_count += 1\n                \n        return accepted_count / K\n\n    test_cases_N = [32, 64, 128]\n    results = []\n    \n    for N_val in test_cases_N:\n        rate = run_mala_for_N(N_val, K_ITER, ETA)\n        results.append(rate)\n        \n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world inverse problems often have a specific structure where observational data is informative only about a low-dimensional subspace of the high-dimensional unknown. The Dimension-Independent Likelihood-Informed (DILI) sampler is an advanced hybrid method designed to exploit this structure for maximum efficiency. In this practical exercise , you will construct a DILI sampler that applies a fast, gradient-based MALA update on the data-informed subspace while using a robust pCN update on its complement. By implementing this sampler for a model atmospheric retrieval problem, you will verify its excellent, dimension-independent mixing properties by analyzing the integrated autocorrelation times of different components of the solution.",
            "id": "3376410",
            "problem": "You are given a Bayesian linear inverse problem motivated by atmospheric retrieval in data assimilation (DA). The unknown state $u \\in \\mathbb{R}^N$ is a discretized vertical profile over altitude. The data model is\n$$\ny = H u + \\eta,\n$$\nwhere $H \\in \\mathbb{R}^{m \\times N}$ is a linearized radiative transfer operator with $m$ channels, and $\\eta \\sim \\mathcal{N}(0, R)$ is additive observation noise with covariance $R = \\sigma^2 I_m$ for a known $\\sigma  0$. The prior on $u$ is Gaussian $\\mathcal{N}(0, C)$, and in this exercise you may take $C = I_N$ without loss of generality.\n\nThe Likelihood-Informed Subspace (LIS) is defined as the linear subspace of $\\mathbb{R}^N$ spanned by the dominant eigenvectors of the prior-preconditioned Gauss–Newton Hessian\n$$\n\\mathcal{S} \\equiv C^{1/2} H^\\top R^{-1} H C^{1/2},\n$$\nso that the LIS is the range of $\\mathcal{S}$ and its dimension is the number of strictly positive eigenvalues of $\\mathcal{S}$. Equivalently, if $J \\equiv R^{-1/2} H C^{1/2}$, then $\\mathcal{S} = C^{1/2} H^\\top R^{-1} H C^{1/2}$ and the nonzero eigenvalues of $\\mathcal{S}$ coincide with the squared singular values of $J$. In practice, to define a numerical LIS of dimension $k$, one retains the eigenmodes whose eigenvalues exceed a given tolerance $\\tau  0$.\n\nYour tasks are:\n\n1) Starting from the definitions above (no shortcut formulas), derive how the LIS dimension scales with the rank of the linearized radiative transfer operator. In particular, show that with $C = I_N$ one has\n$$\n\\dim(\\text{LIS}) \\le \\operatorname{rank}(H) \\le m,\n$$\nand for any tolerance $\\tau  0$, the numerical LIS dimension $k(\\tau)$ is the count of eigenvalues of $H^\\top R^{-1} H$ that exceed $\\tau$.\n\n2) Construct a DA-driven numerical LIS for atmospheric retrieval by instantiating a concrete family of $H$ operators arising from smooth weighting functions over altitude. Consider an altitude domain $[0, Z_{\\max}]$ with $Z_{\\max} = 60$ (in $\\mathrm{km}$), a uniform grid with $N$ points, and channel centers $z_j$ uniformly spaced in $(0,Z_{\\max})$ for $j=1,\\dots,m$. Define each row of $H$ by a normalized Gaussian-weighting discretization:\n$$\nH_{j,i} \\propto \\exp\\Big(-\\tfrac{1}{2}\\big(\\tfrac{z_i - z_j}{L}\\big)^2\\Big),\n$$\nwhere $L0$ is a fixed width (in $\\mathrm{km}$) and $z_i$ are the grid points. Normalize each row so that its Euclidean norm is $1$. Use $C = I_N$ and $R = \\sigma^2 I_m$ with a specified $\\sigma$.\n\n3) Implement a Dimension-Independent Likelihood-Informed (DILI) Metropolis–Hastings proposal in the following sense: Let $\\Phi \\in \\mathbb{R}^{N \\times k}$ be the matrix whose columns are the $k$ orthonormal LIS basis vectors. Decompose $u$ into LIS and its orthogonal complement,\n$$\nu = \\Phi a + u_\\perp, \\quad a = \\Phi^\\top u, \\quad u_\\perp = u - \\Phi a,\n$$\nand propose a new state by combining:\n- a Metropolis-Adjusted Langevin Algorithm (MALA) step on the $k$-dimensional LIS coordinates $a$ with step-size $h0$, using the gradient of the full posterior log-density restricted to the LIS,\n- a Preconditioned Crank–Nicolson (pCN) step on the orthogonal complement $u_\\perp$ with parameter $\\beta \\in (0,1)$.\n\nFormally, with $c = \\sqrt{1-\\beta^2}$ and $\\xi \\sim \\mathcal{N}(0,I)$:\n- LIS update: propose\n$$\na' = a + \\tfrac{h}{2}\\,\\nabla_a \\log \\pi(u) + \\sqrt{h}\\,\\xi_k, \\quad \\nabla_a \\log \\pi(u) = \\Phi^\\top \\nabla_u \\log \\pi(u),\n$$\n- Complement update: propose\n$$\nu_\\perp' = c\\, u_\\perp + \\beta\\, \\xi_\\perp, \\quad \\xi_\\perp = \\xi - \\Phi(\\Phi^\\top \\xi).\n$$\nRecombine $u' = \\Phi a' + u_\\perp'$, and accept or reject with the Metropolis–Hastings acceptance probability using the exact posterior density and the exact joint proposal density (the product of the LIS MALA Gaussian and the complement pCN Gaussian).\n\n4) Verification of dimension-independent mixing: For fixed $m$ and fixed LIS tolerance $\\tau$, empirically demonstrate that as $N$ increases, the DILI sampler’s mixing remains stable. Quantify this by reporting:\n- the numerical LIS dimension $k$,\n- the overall acceptance rate,\n- the integrated autocorrelation time of a scalar functional aligned with the LIS (the first LIS coordinate $a_1$),\n- the integrated autocorrelation time of a scalar functional aligned with the complement (a unit vector in the orthogonal complement, constructed by projecting a standard normal vector onto the complement).\n\nUse the following test suite of parameter values:\n\n- Case A (happy path): $N=64$, $m=12$, $\\sigma=0.05$, $L=8$, $\\tau=10^{-8}$, total iterations $T=3000$, burn-in $B=1000$, MALA step-size $h=0.1$, pCN parameter $\\beta=0.2$.\n- Case B (dimension increase): $N=128$, $m=12$, $\\sigma=0.05$, $L=8$, $\\tau=10^{-8}$, $T=3000$, $B=1000$, $h=0.1$, $\\beta=0.2$.\n- Case C (dimension increase): $N=256$, $m=12$, $\\sigma=0.05$, $L=8$, $\\tau=10^{-8}$, $T=3000$, $B=1000$, $h=0.1$, $\\beta=0.2$.\n- Case D (low-rank boundary): $N=64$, $m=4$, $\\sigma=0.05$, $L=8$, $\\tau=10^{-8}$, $T=3000$, $B=1000$, $h=0.1$, $\\beta=0.2$.\n\nIn all cases, construct a synthetic truth $u^\\dagger$ as the sum of two altitude-localized Gaussian bumps and generate data $y = H u^\\dagger + \\eta$ with $\\eta \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ using a fixed random seed.\n\nYour program must compute, for each case, the list $[k, \\text{acc}, \\text{iact\\_lis}, \\text{iact\\_perp}]$, where $k$ is an integer, and the remaining entries are floating-point values. Your program should produce a single line of output containing the results for all cases as a comma-separated list of these four-element lists enclosed in square brackets, for example:\n\"[ [k_A,acc_A,iact_lis_A,iact_perp_A],[k_B,acc_B,iact_lis_B,iact_perp_B],[k_C,acc_C,iact_lis_C,iact_perp_C],[k_D,acc_D,iact_lis_D,iact_perp_D] ]\".",
            "solution": "The supplied problem is a valid, well-posed, and scientifically grounded exercise in computational statistics and inverse problems. It asks for the derivation, implementation, and verification of a dimension-independent MCMC sampler for a Bayesian linear inverse problem. All necessary parameters and definitions are provided, and the tasks are logically structured and technically feasible.\n\n### Task 1: Theoretical Derivation of LIS Dimension\n\nWe are given the Likelihood-Informed Subspace (LIS) as the range of the prior-preconditioned Gauss-Newton Hessian, $\\mathcal{S} \\equiv C^{1/2} H^\\top R^{-1} H C^{1/2}$. The dimension of the LIS is the number of strictly positive eigenvalues of $\\mathcal{S}$, which is equal to its rank, $\\dim(\\text{LIS}) = \\operatorname{rank}(\\mathcal{S})$.\n\nWe are asked to show two properties under the specific condition $C = I_N$.\n\n**First Property:** Show that $\\dim(\\text{LIS}) \\le \\operatorname{rank}(H) \\le m$.\n\nWith the prior covariance $C = I_N$, the matrix $\\mathcal{S}$ simplifies to:\n$$\n\\mathcal{S} = I_N^{1/2} H^\\top R^{-1} H I_N^{1/2} = H^\\top R^{-1} H\n$$\nThe dimension of the LIS is the rank of this matrix: $\\dim(\\text{LIS}) = \\operatorname{rank}(H^\\top R^{-1} H)$.\n\nThe noise covariance $R$ is given as $R = \\sigma^2 I_m$ with $\\sigma  0$. Its inverse is $R^{-1} = \\frac{1}{\\sigma^2} I_m$. Since $\\sigma^2  0$, $R^{-1}$ is a full-rank diagonal matrix of size $m \\times m$. The rank of a product of matrices is at most the minimum of their ranks. The rank of $R^{-1}H$ is therefore equal to the rank of $H$, as multiplying by a full-rank square matrix does not change the rank.\n$$\n\\operatorname{rank}(R^{-1}H) = \\operatorname{rank}(H)\n$$\nNow we consider the rank of $\\mathcal{S} = H^\\top (R^{-1} H)$. Using the fundamental linear algebra identity that for any matrix $A$, $\\operatorname{rank}(A^\\top A) = \\operatorname{rank}(A)$, we can set $A = R^{-1/2}H$. Then $A^\\top A = H^\\top (R^{-1/2})^\\top R^{-1/2} H = H^\\top R^{-1} H = \\mathcal{S}$.\nTherefore,\n$$\n\\dim(\\text{LIS}) = \\operatorname{rank}(\\mathcal{S}) = \\operatorname{rank}((R^{-1/2}H)^\\top (R^{-1/2}H)) = \\operatorname{rank}(R^{-1/2}H) = \\operatorname{rank}(H)\n$$\nThe operator $H$ is a matrix of size $m \\times N$. The rank of any matrix cannot exceed the number of its rows or columns. In particular, $\\operatorname{rank}(H) \\le m$.\nCombining these results, we have established the desired inequality:\n$$\n\\dim(\\text{LIS}) = \\operatorname{rank}(H) \\le m\n$$\n\n**Second Property:** Show that the numerical LIS dimension $k(\\tau)$ is the count of eigenvalues of $H^\\top R^{-1} H$ that exceed $\\tau$.\n\nThe numerical LIS of dimension $k$ is defined by retaining the eigenmodes of $\\mathcal{S}$ whose corresponding eigenvalues exceed a tolerance $\\tau  0$.\nGiven $C=I_N$, we have $\\mathcal{S} = H^\\top R^{-1} H$.\nBy definition, the numerical LIS dimension, which we denote as $k(\\tau)$, is the number of eigenvalues $\\lambda_i$ of $\\mathcal{S}$ such that $\\lambda_i  \\tau$.\nTherefore, $k(\\tau)$ is precisely the count of eigenvalues of $H^\\top R^{-1} H$ that are strictly greater than $\\tau$. This follows directly from the definitions provided.\n\n### Task 2: Construction of the Numerical Model\n\nWe construct the forward operator $H$ for a discretized 1D atmospheric profile.\nThe altitude domain is $[0, Z_{\\max}]$ with $Z_{\\max} = 60 \\, \\mathrm{km}$.\nThis domain is discretized into $N$ grid cells, with centers at $z_i = (i-0.5) \\Delta z$ for $i=1, \\dots, N$, where $\\Delta z = Z_{\\max}/N$.\nThe measurement has $m$ channels, with centers $z_j = j \\Delta z_{ch}$ for $j=1, \\dots, m$, where $\\Delta z_{ch} = Z_{\\max}/(m+1)$, placing them uniformly in the open interval $(0, Z_{\\max})$.\n\nEach row of the operator $H$ represents the sensitivity of a measurement channel to the atmospheric state at different altitudes. The entry $H_{j,i}$ links the $j$-th channel to the $i$-th grid point. It is defined by a Gaussian weighting function:\n$$\nH_{j,i}' = \\exp\\Big(-\\frac{1}{2}\\left(\\frac{z_i - z_j}{L}\\right)^2\\Big)\n$$\nwhere $L$ is a characteristic width. As required, each row is then normalized to have a Euclidean norm of $1$:\n$$\nH_{j,i} = \\frac{H_{j,i}'}{\\sqrt{\\sum_{p=1}^N (H_{j,p}')^2}}\n$$\nThe other components of the Bayesian model are the prior covariance $C = I_N$ and the observation error covariance $R = \\sigma^2 I_m$.\n\n### Task 3: The DILI Metropolis-Hastings Sampler\n\nThe goal is to sample from the posterior distribution $\\pi(u|y) \\propto \\pi(y|u) \\pi_0(u)$, where $\\pi_0(u)$ is the prior and $\\pi(y|u)$ is the likelihood.\nGiven the Gaussian assumptions, the posterior log-density is:\n$$\n\\log \\pi(u|y) = -\\frac{1}{2} (y - Hu)^\\top R^{-1} (y - Hu) - \\frac{1}{2} u^\\top C^{-1} u + \\text{constant}\n$$\nSubstituting $R = \\sigma^2 I_m$ and $C = I_N$:\n$$\n\\log \\pi(u|y) = -\\frac{1}{2\\sigma^2} \\|y - Hu\\|_2^2 - \\frac{1}{2} \\|u\\|_2^2 + \\text{constant}\n$$\nThe gradient of the log-posterior with respect to $u$ is:\n$$\n\\nabla_u \\log \\pi(u) = \\frac{1}{\\sigma^2} H^\\top (y - Hu) - u\n$$\n\nThe DILI sampler operates on a decomposition of the state $u$ into components within the LIS and its orthogonal complement.\n1.  **LIS Basis**: Compute the matrix $\\mathcal{M} = H^\\top R^{-1} H = \\frac{1}{\\sigma^2}H^\\top H$. Find its eigenvalues $\\lambda_i$ and corresponding orthonormal eigenvectors $v_i$. The LIS basis is formed by the eigenvectors $\\{v_i\\}$ for which $\\lambda_i  \\tau$. Let there be $k$ such eigenvectors. The basis matrix is $\\Phi = [v_1, \\dots, v_k] \\in \\mathbb{R}^{N \\times k}$.\n\n2.  **State Decomposition**: Any state $u \\in \\mathbb{R}^N$ is decomposed as $u = \\Phi a + u_\\perp$, where $a = \\Phi^\\top u$ are the coordinates in the LIS and $u_\\perp = (I - \\Phi\\Phi^\\top)u$ is the component in the orthogonal complement.\n\n3.  **Proposal Generation**: Given the current state $u$, a new state $u'$ is proposed.\n    *   **LIS Component (MALA)**: A Metropolis-Adjusted Langevin Algorithm step is taken for the LIS coordinates $a$. The projected gradient is $\\nabla_a \\log \\pi(u) = \\Phi^\\top \\nabla_u \\log \\pi(u)$. The proposal $a'$ is:\n        $$\n        a' = a + \\frac{h}{2} \\nabla_a \\log \\pi(u) + \\sqrt{h}\\,\\xi_k, \\quad \\xi_k \\sim \\mathcal{N}(0, I_k)\n        $$\n    *   **Complement Component (pCN)**: A Preconditioned Crank-Nicolson step is taken for $u_\\perp$. This step preserves the prior distribution on the complement space.\n        $$\n        u_\\perp' = \\sqrt{1-\\beta^2}\\, u_\\perp + \\beta\\, \\xi_\\perp, \\quad \\xi_\\perp = (I-\\Phi\\Phi^\\top)\\xi, \\quad \\xi \\sim \\mathcal{N}(0, I_N)\n        $$\n\n4.  **Recombination**: The proposed state is $u' = \\Phi a' + u_\\perp'$.\n\n5.  **Acceptance Probability**: The proposed move is accepted with probability $\\alpha = \\min(1, A)$, where the ratio $A$ is given by\n    $$\n    A = \\frac{\\pi(u'|y) q(u|u')}{\\pi(u|y) q(u'|u)}\n    $$\n    The joint proposal density is $q(u'|u) = q_a(a'|u) q_\\perp(u_\\perp'|u_\\perp)$. The log of the acceptance ratio is:\n    $$\n    \\log A = (\\log \\pi(u'|y) - \\log \\pi(u|y)) + (\\log q_a(a|u') - \\log q_a(a'|u)) + (\\log q_\\perp(u_\\perp|u'_\\perp) - \\log q_\\perp(u'_\\perp|u_\\perp))\n    $$\n    The terms are:\n    *   Log-posterior ratio: $(\\log\\pi(u'|y) - \\log\\pi(u|y))$\n    *   MALA proposal ratio: \n    $$\n    -\\frac{1}{2h} \\|a - (a' + \\frac{h}{2}\\nabla_{a'}\\log\\pi(u'))\\|^2 + \\frac{1}{2h} \\|a' - (a + \\frac{h}{2}\\nabla_a\\log\\pi(u))\\|^2\n    $$\n    *   pCN proposal ratio: \n    $$\n    -\\frac{1}{2\\beta^2}\\|u_\\perp - \\sqrt{1-\\beta^2}u_\\perp'\\|^2 + \\frac{1}{2\\beta^2}\\|u_\\perp' - \\sqrt{1-\\beta^2}u_\\perp\\|^2\n    $$\n    This full expression must be computed for the Metropolis-Hastings test.\n\n### Task 4: Verification of Dimension-Independent Mixing\n\nTo verify the sampler's performance as the state dimension $N$ increases, we run simulations for the specified test cases. For each case, we construct a synthetic truth $u^\\dagger$ as a sum of two Gaussian bumps, generate synthetic data $y = H u^\\dagger + \\eta$, and then run the DILI-MCMC sampler. We analyze the resulting chain after discarding a burn-in period. The following metrics are computed:\n*   **$k$**: The numerical dimension of the LIS, determined by the eigenvalue cutoff $\\tau$.\n*   **Acceptance rate**: The fraction of proposed MCMC moves that are accepted.\n*   **Integrated Autocorrelation Time (IACT)**: A measure of the number of samples required to obtain an independent sample. We compute it for two specific observables:\n    1.  **IACT LIS**: The IACT of the time series of the first LIS coordinate, $t \\mapsto a_1(t) = (\\Phi^\\top u_t)_1$.\n    2.  **IACT Perp**: The IACT of the time series of the projection of the state onto a fixed random vector in the orthogonal complement, $t \\mapsto c(t) = \\hat{v}_\\perp^\\top u_t$, where $\\hat{v}_\\perp$ is a normalized random vector from the complement space.\n\nThe expectation is that for fixed $m$, $\\sigma$, and other sampler parameters, the acceptance rate and IACTs will remain stable as $N$ increases, demonstrating the dimension-independent nature of the sampler's mixing.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import eigh\n\ndef calculate_iact(series):\n    \"\"\"\n    Calculates the Integrated Autocorrelation Time (IACT) of a time series.\n    Uses FFT for efficient computation of the autocorrelation function (ACF).\n    \"\"\"\n    n = len(series)\n    if n  2:\n        return 1.0\n    \n    mean = np.mean(series)\n    series_centered = series - mean\n    \n    # Calculate autocovariance using FFT\n    n_fft = 1  (2 * n - 1).bit_length() # Next power of 2 for 2*n-1\n    fft_series = np.fft.fft(series_centered, n_fft)\n    autocov = np.fft.ifft(fft_series * np.conj(fft_series)).real\n    autocov = autocov[:n]\n\n    # ACF is normalized autocovariance\n    if autocov[0]  1e-10: # Avoid division by zero if variance is tiny\n        return 1.0\n    rho = autocov / autocov[0]\n    \n    # Sum up to the first negative value (a simple but common windowing method)\n    iact_val = 1.0\n    for k in range(1, n):\n        if rho[k]  0:\n            break\n        iact_val += 2.0 * rho[k]\n    \n    return iact_val\n\ndef run_case(params):\n    \"\"\"\n    Runs the full simulation for a single parameter case.\n    \"\"\"\n    N, m, sigma, L, tau, T, B, h, beta = params\n    \n    # 2) Construct the numerical model\n    Z_max = 60.0\n    z_grid = np.linspace(0, Z_max, N, endpoint=False) + Z_max / (2 * N)\n    \n    # Place channel centers uniformly in (0, Z_max)\n    ch_centers = np.linspace(0, Z_max, m + 2)[1:-1]\n    \n    H = np.zeros((m, N))\n    for j in range(m):\n        row = np.exp(-0.5 * ((z_grid - ch_centers[j]) / L)**2)\n        norm = np.linalg.norm(row)\n        if norm > 0:\n            H[j, :] = row / norm\n\n    # Generate synthetic truth and data\n    u_true = 1.0 * np.exp(-0.5 * ((z_grid - 15.0) / 5.0)**2) + \\\n             0.8 * np.exp(-0.5 * ((z_grid - 40.0) / 5.0)**2)\n    \n    R_inv = (1.0 / sigma**2) * np.identity(m)\n    \n    # Use pre-seeded randomness for data generation\n    noise = np.random.normal(0, sigma, m)\n    y = H @ u_true + noise\n\n    # --- LIS computation ---\n    # S = C^(1/2) H^T R^(-1) H C^(1/2) with C=I\n    S_matrix = H.T @ R_inv @ H\n    \n    # Find eigenvalues and eigenvectors\n    # Use eigh for symmetric matrices\n    eigenvalues, eigenvectors = eigh(S_matrix)\n    \n    # Sort in descending order\n    idx = np.argsort(eigenvalues)[::-1]\n    eigenvalues = eigenvalues[idx]\n    eigenvectors = eigenvectors[:, idx]\n    \n    # Determine numerical LIS dimension k\n    k = np.sum(eigenvalues > tau)\n    if k == 0:\n        # Handle the case of no dominant eigenvalues to avoid errors\n        Phi = np.zeros((N, 1))\n    else:\n        Phi = eigenvectors[:, :k]\n\n    # --- DILI-MCMC Sampler ---\n    \n    # Posterior log-density and its gradient\n    def log_post(u):\n        residual = y - H @ u\n        log_likelihood = -0.5 * (1/sigma**2) * np.dot(residual, residual)\n        log_prior = -0.5 * np.dot(u, u)\n        return log_likelihood + log_prior\n\n    def grad_log_post(u):\n        return (1/sigma**2) * H.T @ (y - H @ u) - u\n\n    # MCMC initialization\n    u = np.random.randn(N) # Start from a random sample from the prior\n    chain = np.zeros((T, N))\n    accept_count = 0\n    c_beta = np.sqrt(1 - beta**2)\n\n    for i in range(T):\n        # 1. Decompose current state\n        a = Phi.T @ u\n        u_perp = u - Phi @ a\n        \n        # 2. Propose new state\n        # LIS proposal (MALA)\n        grad_u = grad_log_post(u)\n        grad_a = Phi.T @ grad_u\n        xi_k = np.random.randn(k)\n        if k > 0:\n            a_prime = a + (h / 2.0) * grad_a + np.sqrt(h) * xi_k\n        else:\n            a_prime = a\n\n        # Complement proposal (pCN)\n        xi_N = np.random.randn(N)\n        xi_perp = xi_N - Phi @ (Phi.T @ xi_N)\n        u_perp_prime = c_beta * u_perp + beta * xi_perp\n        \n        # 3. Recombine\n        u_prime = Phi @ a_prime + u_perp_prime\n\n        # 4. Acceptance probability\n        log_post_u = log_post(u)\n        log_post_u_prime = log_post(u_prime)\n\n        # MALA proposal log-ratio term\n        grad_u_prime = grad_log_post(u_prime)\n        grad_a_prime = Phi.T @ grad_u_prime\n        \n        log_q_ratio_mala = 0.0\n        if k > 0:\n            rev_term = a - (a_prime + (h/2.0) * grad_a_prime)\n            fwd_term = a_prime - (a + (h/2.0) * grad_a)\n            log_q_ratio_mala = (-0.5/h) * (np.dot(rev_term, rev_term) - np.dot(fwd_term, fwd_term))\n\n        # pCN proposal log-ratio term\n        rev_term_pcn = u_perp - c_beta * u_perp_prime\n        fwd_term_pcn = u_perp_prime - c_beta * u_perp\n        log_q_ratio_pcn = (-0.5/beta**2) * (np.dot(rev_term_pcn, rev_term_pcn) - np.dot(fwd_term_pcn, fwd_term_pcn))\n\n        log_A = (log_post_u_prime - log_post_u) + log_q_ratio_mala + log_q_ratio_pcn\n        \n        if np.log(np.random.rand())  log_A:\n            u = u_prime\n            accept_count += 1\n            \n        chain[i, :] = u\n\n    # --- Analysis ---\n    chain = chain[B:] # Discard burn-in\n\n    # Acceptance rate\n    acc_rate = accept_count / T\n\n    # IACT for LIS component\n    if k > 0:\n        a1_series = chain @ Phi[:, 0]\n        iact_lis = calculate_iact(a1_series)\n    else:\n        iact_lis = 1.0 # Or some other placeholder if LIS is empty\n    \n    # IACT for complement component\n    v_rand = np.random.randn(N)\n    v_perp = v_rand - Phi @ (Phi.T @ v_rand)\n    v_perp_norm = np.linalg.norm(v_perp)\n    if v_perp_norm > 1e-9:\n        v_perp_unit = v_perp / v_perp_norm\n        perp_series = chain @ v_perp_unit\n        iact_perp = calculate_iact(perp_series)\n    else:\n        iact_perp = 1.0 # No component in perp direction\n        \n    return [k, acc_rate, iact_lis, iact_perp]\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Fix the seed for reproducibility of the entire process\n    np.random.seed(42)\n\n    # Define test cases from the problem statement.\n    # (N, m, sigma, L, tau, T, B, h, beta)\n    test_cases = [\n        (64, 12, 0.05, 8.0, 1e-8, 3000, 1000, 0.1, 0.2), # Case A\n        (128, 12, 0.05, 8.0, 1e-8, 3000, 1000, 0.1, 0.2), # Case B\n        (256, 12, 0.05, 8.0, 1e-8, 3000, 1000, 0.1, 0.2), # Case C\n        (64, 4, 0.05, 8.0, 1e-8, 3000, 1000, 0.1, 0.2),  # Case D\n    ]\n\n    results = []\n    for params in test_cases:\n        # We need to re-seed before data generation and MCMC start for each case\n        # to make them independent experiments on the same footing.\n        # But the problem asks for stability, which is often tested by\n        # letting the PRNG state evolve. We will go with a single seed at the top\n        # as it is a common setup for such scripts.\n        result_list = run_case(params)\n        results.append(result_list)\n    \n    # Format the final output string as specified: \"[[k,val,val,val],[...]]\"\n    # Using str().replace() is a robust way to get the exact format.\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n\n```"
        }
    ]
}