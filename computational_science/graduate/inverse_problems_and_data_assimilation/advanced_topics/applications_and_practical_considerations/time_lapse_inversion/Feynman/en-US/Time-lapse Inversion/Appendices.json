{
    "hands_on_practices": [
        {
            "introduction": "At the heart of many time-lapse inversion methods lies a linear Gaussian model, which provides a powerful framework for both estimation and uncertainty analysis. This exercise focuses on a critical practical aspect of solving such problems: numerical conditioning. You will derive a preconditioning, or \"whitening,\" transformation based on the prior covariance structure and quantitatively analyze how it improves the spectral properties of the Hessian matrix, a key step for designing efficient iterative solvers .",
            "id": "3427717",
            "problem": "Consider a linear Gaussian time-lapse inversion problem for a spatio-temporal parameter field, where the parameter vector $x \\in \\mathbb{R}^{n}$ is formed by stacking $x(t) \\in \\mathbb{R}^{n_s}$ for $t=1,\\dots,n_t$. Assume $n_t = 2$ time steps and $n_s = 2$ spatial modes so that $n = 4$. The observation model is $d = G x + \\epsilon$, with $G = I_{t} \\otimes S$, where $I_{t}$ is the $2 \\times 2$ identity matrix and $S$ is a fixed $2 \\times 2$ spatial sensitivity matrix. The data noise is zero-mean Gaussian with covariance $C_{d} = I$, and the prior on $x$ is zero-mean Gaussian with separable space-time covariance $C = C_{t} \\otimes C_{s}$, where $C_{t}$ and $C_{s}$ are symmetric positive definite.\n\nStarting from the Maximum A Posteriori (MAP) estimation framework for a linear Gaussian model, the normal equations for the minimizer of the posterior cost can be written in terms of the Hessian $H = G^{\\top} C_{d}^{-1} G + C^{-1}$. You are to derive the whitening (preconditioning) transformation using $C_{t}^{-1/2}$ and $C_{s}^{-1/2}$ that maps $x$ to $y$ such that the prior term becomes the identity in the transformed variables, and then express the preconditioned Hessian spectrum in the joint eigenbasis of space and time.\n\nAssume the following spectral structure:\n- $C_{s}$ has eigen-decomposition $C_{s} = U_{s} \\Lambda_{s} U_{s}^{\\top}$ with eigenvalues $\\lambda_{s}^{(1)} = 5$ and $\\lambda_{s}^{(2)} = 0.5$.\n- $C_{t}$ has eigen-decomposition $C_{t} = U_{t} \\Lambda_{t} U_{t}^{\\top}$ with eigenvalues $\\lambda_{t}^{(1)} = 3$ and $\\lambda_{t}^{(2)} = 1$.\n- $S^{\\top} S$ shares the same eigenvectors $U_{s}$ as $C_{s}$ (consistent with jointly stationary spatial priors and a spatially invariant linear sensitivity), with eigenvalues $\\mu^{(1)} = 0.2$ and $\\mu^{(2)} = 2$.\n\nUsing these assumptions:\n1. Derive the preconditioned variable transformation $y = (C_{t}^{-1/2} \\otimes C_{s}^{-1/2}) x$, and show how the Hessian transforms.\n2. Give the eigenvalues of the original Hessian $H$ in the joint eigenbasis $U_{t} \\otimes U_{s}$, and the eigenvalues of the preconditioned Hessian in the whitened variables.\n3. Compute the spectral condition numbers $\\kappa(H)$ and $\\kappa(\\hat{H})$ (the ratio of the largest to smallest eigenvalue for each Hessian), and report the conditioning improvement factor $r = \\kappa(H) / \\kappa(\\hat{H})$.\n\nReport $r$ as a dimensionless number. No rounding is required; provide the exact value.",
            "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It presents a standard linear Gaussian inverse problem in time-lapse imaging, with all necessary parameters and assumptions clearly defined. We can proceed with the solution.\n\nThe problem asks for the derivation of a preconditioning transformation, the spectra of the original and preconditioned Hessians, and the resulting improvement in the spectral condition number.\n\nThe Maximum A Posteriori (MAP) estimate for the parameter vector $x \\in \\mathbb{R}^{n}$ is the minimizer of the posterior cost function $J(x)$:\n$$J(x) = \\frac{1}{2} \\|d - Gx\\|_{C_d^{-1}}^2 + \\frac{1}{2} \\|x - x_{prior}\\|_{C^{-1}}^2$$\nGiven a zero-mean prior ($x_{prior}=0$) and data noise covariance $C_d = I$ (the identity matrix), the cost function simplifies to:\n$$J(x) = \\frac{1}{2} (d - Gx)^{\\top} (d - Gx) + \\frac{1}{2} x^{\\top} C^{-1} x$$\nThe Hessian of this quadratic cost function is given by:\n$$H = G^{\\top} C_d^{-1} G + C^{-1} = G^{\\top}G + C^{-1}$$\nUsing the provided Kronecker product structures for the forward operator $G = I_t \\otimes S$ and the prior covariance $C = C_t \\otimes C_s$, we can express the Hessian as:\n$$H = (I_t \\otimes S)^{\\top}(I_t \\otimes S) + (C_t \\otimes C_s)^{-1}$$\nUsing the properties $(A \\otimes B)^{\\top} = A^{\\top} \\otimes B^{\\top}$ and $(A \\otimes B)^{-1} = A^{-1} \\otimes B^{-1}$, and that $I_t^{\\top} = I_t$, this becomes:\n$$H = (I_t^{\\top}I_t \\otimes S^{\\top}S) + (C_t^{-1} \\otimes C_s^{-1}) = I_t \\otimes (S^{\\top}S) + C_t^{-1} \\otimes C_s^{-1}$$\n\n1. **Preconditioning Transformation and Transformed Hessian**\n\nThe goal of preconditioning is to transform the variables to improve the spectral properties of the Hessian. We introduce a change of variables $x = Ay$, where $A$ is the preconditioner. The cost function in terms of the new variable $y$ is:\n$$J(y) = \\frac{1}{2} (d - GAy)^{\\top} (d - GAy) + \\frac{1}{2} (Ay)^{\\top} C^{-1} (Ay) = \\frac{1}{2} \\|d - (GA)y\\|^2 + \\frac{1}{2} y^{\\top} (A^{\\top}C^{-1}A) y$$\nThe problem specifies a whitening transformation that makes the prior term become the identity. This means we require $A^{\\top}C^{-1}A = I$. A standard choice for $A$ that satisfies this is $A = C^{1/2}$, the symmetric positive definite square root of $C$. This leads to the transformation from $x$ to $y$ as $y = C^{-1/2}x$.\n\nAs specified, $C^{-1/2} = (C_t \\otimes C_s)^{-1/2} = C_t^{-1/2} \\otimes C_s^{-1/2}$. This confirms the transformation $y = (C_t^{-1/2} \\otimes C_s^{-1/2}) x$ is the correct whitening transformation.\n\nThe Hessian of $J(y)$ is the preconditioned Hessian $\\hat{H}$. The Hessian transforms under a linear change of variables $x=Ay$ as $\\hat{H} = A^{\\top}HA$. Substituting $A=C^{1/2}$ and the expression for $H$:\n$$\\hat{H} = (C^{1/2})^{\\top} (G^{\\top}G + C^{-1}) C^{1/2} = C^{1/2} G^{\\top}G C^{1/2} + C^{1/2} C^{-1} C^{1/2} = C^{1/2} G^{\\top}G C^{1/2} + I$$\nNow, we substitute the Kronecker product forms:\n$$\\hat{H} = (C_t^{1/2} \\otimes C_s^{1/2})(I_t \\otimes S^{\\top}S)(C_t^{1/2} \\otimes C_s^{1/2}) + I$$\nUsing the mixed-product property $(A \\otimes B)(C \\otimes D) = (AC) \\otimes (BD)$:\n$$\\hat{H} = (C_t^{1/2} I_t C_t^{1/2}) \\otimes (C_s^{1/2} S^{\\top}S C_s^{1/2}) + I_{n_t n_s}$$\n$$\\hat{H} = C_t \\otimes (C_s^{1/2} S^{\\top}S C_s^{1/2}) + I$$\nThis is the expression for the transformed (preconditioned) Hessian.\n\n2. **Eigenvalues of the Original and Preconditioned Hessians**\n\nThe problem states that $C_s$ and $S^{\\top}S$ share the same eigenvectors $U_s$, and $C_t$ has eigenvectors $U_t$. The joint eigenbasis for the spatio-temporal system is $U = U_t \\otimes U_s$. The vectors in this basis diagonalize both terms in the original Hessian $H = I_t \\otimes (S^{\\top}S) + C_t^{-1} \\otimes C_s^{-1}$.\n\nAn eigenvector $v_{i,j} = u_{t,i} \\otimes u_{s,j}$ of $H$ corresponds to an eigenvalue $\\theta_{i,j}$.\n$$H v_{i,j} = (I_t u_{t,i} \\otimes S^{\\top}S u_{s,j}) + (C_t^{-1} u_{t,i} \\otimes C_s^{-1} u_{s,j})$$\n$$H v_{i,j} = (1 \\cdot u_{t,i} \\otimes \\mu^{(j)} u_{s,j}) + ((\\lambda_t^{(i)})^{-1} u_{t,i} \\otimes (\\lambda_s^{(j)})^{-1} u_{s,j})$$\n$$H v_{i,j} = \\left(\\mu^{(j)} + \\frac{1}{\\lambda_t^{(i)} \\lambda_s^{(j)}}\\right) v_{i,j}$$\nThe eigenvalues of $H$ are $\\theta_{i,j} = \\mu^{(j)} + \\frac{1}{\\lambda_t^{(i)} \\lambda_s^{(j)}}$ for $i,j \\in \\{1,2\\}$.\nUsing the given values: $\\lambda_t^{(1)}=3, \\lambda_t^{(2)}=1$; $\\lambda_s^{(1)}=5, \\lambda_s^{(2)}=0.5$; $\\mu^{(1)}=0.2, \\mu^{(2)}=2$.\n- $\\theta_{1,1} = \\mu^{(1)} + (\\lambda_t^{(1)}\\lambda_s^{(1)})^{-1} = 0.2 + (3 \\times 5)^{-1} = \\frac{1}{5} + \\frac{1}{15} = \\frac{3+1}{15} = \\frac{4}{15}$.\n- $\\theta_{1,2} = \\mu^{(2)} + (\\lambda_t^{(1)}\\lambda_s^{(2)})^{-1} = 2 + (3 \\times 0.5)^{-1} = 2 + \\frac{1}{1.5} = 2 + \\frac{2}{3} = \\frac{8}{3}$.\n- $\\theta_{2,1} = \\mu^{(1)} + (\\lambda_t^{(2)}\\lambda_s^{(1)})^{-1} = 0.2 + (1 \\times 5)^{-1} = \\frac{1}{5} + \\frac{1}{5} = \\frac{2}{5}$.\n- $\\theta_{2,2} = \\mu^{(2)} + (\\lambda_t^{(2)}\\lambda_s^{(2)})^{-1} = 2 + (1 \\times 0.5)^{-1} = 2 + 2 = 4$.\nThe set of eigenvalues for $H$ is $\\left\\{\\frac{4}{15}, \\frac{2}{5}, \\frac{8}{3}, 4\\right\\}$.\n\nFor the preconditioned Hessian $\\hat{H} = C^{1/2} H C^{1/2}$, its eigenvalues can be found by analyzing $\\hat{H} = C_t \\otimes (C_s^{1/2} S^{\\top}S C_s^{1/2}) + I$. Let $\\hat{\\theta}_{i,j}$ be the eigenvalues of $\\hat{H}$. These are $1$ plus the eigenvalues of $C_t \\otimes (C_s^{1/2} S^{\\top}S C_s^{1/2})$. The eigenvalues of $C_s^{1/2} S^{\\top}S C_s^{1/2}$ are the same as those of $S^{\\top}S C_s$. Since $S^{\\top}S$ and $C_s$ commute, the eigenvalues of their product are the products of their eigenvalues, $\\nu_j = \\mu^{(j)} \\lambda_s^{(j)}$.\n- $\\nu_1 = \\mu^{(1)}\\lambda_s^{(1)} = 0.2 \\times 5 = 1$.\n- $\\nu_2 = \\mu^{(2)}\\lambda_s^{(2)} = 2 \\times 0.5 = 1$.\nThe eigenvalues of $C_s^{1/2} S^{\\top}S C_s^{1/2}$ are both $1$, so this matrix is the identity $I_s$.\nThe preconditioned Hessian simplifies to:\n$$\\hat{H} = C_t \\otimes I_s + I_t \\otimes I_s = (C_t + I_t) \\otimes I_s$$\nThe eigenvalues of $C_t+I_t$ are $\\lambda_t^{(i)} + 1$, which are $3+1=4$ and $1+1=2$. The eigenvalues of $I_s$ are $1$ (with multiplicity $2$). The eigenvalues of the Kronecker product are the products of the eigenvalues of the constituent matrices. Therefore, the eigenvalues of $\\hat{H}$ are:\n- $\\hat{\\theta}_{1,j} = (\\lambda_t^{(1)} + 1) \\times 1 = 4$ (for $j=1,2$).\n- $\\hat{\\theta}_{2,j} = (\\lambda_t^{(2)} + 1) \\times 1 = 2$ (for $j=1,2$).\nThe set of eigenvalues for $\\hat{H}$ is $\\{2, 2, 4, 4\\}$.\n\n3. **Condition Numbers and Improvement Factor**\n\nThe spectral condition number $\\kappa$ of a positive definite matrix is the ratio of its largest to its smallest eigenvalue.\nFor the original Hessian $H$:\n- $\\lambda_{max}(H) = \\max\\left\\{\\frac{4}{15}, \\frac{2}{5}, \\frac{8}{3}, 4\\right\\} = 4$.\n- $\\lambda_{min}(H) = \\min\\left\\{\\frac{4}{15}, \\frac{2}{5}, \\frac{8}{3}, 4\\right\\} = \\frac{4}{15}$.\n- $\\kappa(H) = \\frac{\\lambda_{max}(H)}{\\lambda_{min}(H)} = \\frac{4}{4/15} = 15$.\n\nFor the preconditioned Hessian $\\hat{H}$:\n- $\\lambda_{max}(\\hat{H}) = \\max\\{2, 4\\} = 4$.\n- $\\lambda_{min}(\\hat{H}) = \\min\\{2, 4\\} = 2$.\n- $\\kappa(\\hat{H}) = \\frac{\\lambda_{max}(\\hat{H})}{\\lambda_{min}(\\hat{H})} = \\frac{4}{2} = 2$.\n\nThe conditioning improvement factor $r$ is the ratio of the original condition number to the preconditioned one:\n$$r = \\frac{\\kappa(H)}{\\kappa(\\hat{H})} = \\frac{15}{2} = 7.5$$",
            "answer": "$$\n\\boxed{7.5}\n$$"
        },
        {
            "introduction": "Moving from theory to practice, time-lapse changes can be estimated using several distinct strategies, each with its own assumptions and computational trade-offs. This hands-on programming exercise guides you through the implementation and comparison of three workhorse methods: stabilized difference inversion (SDI), joint inversion (JI), and four-dimensional variational data assimilation (4D-Var). By applying these techniques to a synthetic benchmark and evaluating them with quantitative metrics, you will gain practical insight into their relative performance and characteristics .",
            "id": "3427721",
            "problem": "Consider the following linear, Gaussian, one-dimensional time-lapse inverse problem with a static observation operator and a simple identity dynamics model. Let the spatial grid be of size $N$, with spatial coordinates $s_i \\in [0,1]$ for $i \\in \\{1,\\dots,N\\}$ uniformly spaced, and let the number of time steps be $T$. The unknown state at time $k$ is $x_k \\in \\mathbb{R}^N$, and the observed data at time $k$ is $y_k \\in \\mathbb{R}^M$, given by the linear forward model\n$$\ny_k = H x_k + \\varepsilon_k, \\quad \\varepsilon_k \\sim \\mathcal{N}(0, R),\n$$\nwhere $H \\in \\mathbb{R}^{M \\times N}$ is fixed across time and $R = \\sigma^2 I_M$ is the observation noise covariance with $\\sigma > 0$. Assume a simple identity dynamics model $F = I_N$ so that the model evolution is $x_{k+1} \\approx x_k$ up to model error. Let $D \\in \\mathbb{R}^{N \\times N}$ denote the first-order periodic-difference operator on the spatial grid, i.e., $(D x)_i = x_i - x_{i-1}$ with $x_0 \\equiv x_N$, and define the spatial roughness penalty via $D^T D$. Let $T_{\\text{op}} \\in \\mathbb{R}^{N(T-1) \\times NT}$ denote the temporal first-difference operator acting on the stacked state $X = [x_1^T,\\dots,x_T^T]^T$, i.e., $T_{\\text{op}} X = [ (x_2 - x_1)^T, \\dots, (x_T - x_{T-1})^T ]^T$.\n\nYou will generate synthetic truth $x_k^{\\text{true}}$ on a fixed grid and corresponding noisy observations, and then compute and compare reconstructions using three methods: stabilized difference inversion, joint inversion, and four-dimensional variational data assimilation (4D-Var). For each method, you must derive the corresponding linear estimator from first principles and implement it to obtain the reconstruction of the final-time state $\\hat{x}_T$. You must then report three quantitative metrics for the final time: the root-mean-square error (RMSE) between $\\hat{x}_T$ and $x_T^{\\text{true}}$, an average resolution width computed from the final-time resolution matrix, and an average posterior uncertainty computed from the final-time posterior covariance.\n\nThe required fundamental bases to use are: linear Gaussian models and their equivalence to Tikhonov regularization, properties of first-order spatial and temporal difference operators, and the variational formulations of joint inversion and four-dimensional variational data assimilation.\n\nDefinitions and required constructions:\n- Synthetic truth generation: Let $N = 20$ and $T = 3$. Define $s_i = \\frac{i-1}{N-1}$ for $i \\in \\{1,\\dots,N\\}$. Define \n$$\nx_1^{\\text{true}}(s) = 0.1 \\sin(2 \\pi s) + \\exp\\!\\left(-\\frac{(s-0.30)^2}{2 \\cdot 0.05^2}\\right),\n$$\n$$\nx_2^{\\text{true}}(s) = 0.1 \\sin(2 \\pi s) + 0.8\\,\\exp\\!\\left(-\\frac{(s-0.50)^2}{2 \\cdot 0.05^2}\\right),\n$$\n$$\nx_3^{\\text{true}}(s) = 0.1 \\sin(2 \\pi s) + 0.6\\,\\exp\\!\\left(-\\frac{(s-0.70)^2}{2 \\cdot 0.05^2}\\right),\n$$\nand set the discrete vectors $x_k^{\\text{true}} \\in \\mathbb{R}^N$ by sampling at $s_i$. Let the observation matrix $H \\in \\mathbb{R}^{M \\times N}$ be defined via point-spread rows centered at $r_m = \\frac{m-1}{M-1}$, $m \\in \\{1,\\dots,M\\}$, with \n$$\nH_{m i} = \\exp\\!\\left(-\\frac{(s_i - r_m)^2}{2 \\cdot \\sigma_h^2}\\right), \\quad \\sigma_h = 0.05.\n$$\nGenerate noisy data $y_k = H x_k^{\\text{true}} + \\varepsilon_k$, with $\\varepsilon_k \\sim \\mathcal{N}(0, \\sigma^2 I_M)$ drawn independently across times.\n\n- Stabilized difference inversion (SDI): First, estimate $x_1$ by minimizing the Tikhonov functional\n$$\nJ_{\\text{SDI},1}(x) = \\| y_1 - H x \\|_{R^{-1}}^2 + \\lambda_s \\| D x \\|_2^2,\n$$\nthen for $k \\in \\{2,\\dots,T\\}$ estimate the change $\\Delta x_k$ by minimizing\n$$\nJ_{\\text{SDI},\\Delta}(\\Delta x) = \\| (y_k - y_{k-1}) - H \\Delta x \\|_{R^{-1}}^2 + \\lambda_{\\Delta} \\| D \\Delta x \\|_2^2.\n$$\nSet $\\hat{x}_1$ as the minimizer of $J_{\\text{SDI},1}$, set $\\widehat{\\Delta x}_k$ as the minimizer of $J_{\\text{SDI},\\Delta}$, and define the final estimate $\\hat{x}_T = \\hat{x}_1 + \\sum_{k=2}^T \\widehat{\\Delta x}_k$. For the SDI resolution width metric, use the model resolution of the final change estimator at time $T$, i.e., the resolution matrix of the linear estimator mapping $y_T - y_{T-1}$ to $\\widehat{\\Delta x}_T$.\n\n- Joint inversion (JI): Stack the states into $X = [x_1^T,\\dots,x_T^T]^T \\in \\mathbb{R}^{NT}$ and minimize\n$$\nJ_{\\text{JI}}(X) = \\sum_{k=1}^T \\| y_k - H x_k \\|_{R^{-1}}^2 + \\lambda_s \\sum_{k=1}^T \\| D x_k \\|_2^2 + \\lambda_t \\sum_{k=1}^{T-1} \\| x_{k+1} - x_k \\|_2^2.\n$$\nLet $\\hat{X}$ denote the minimizer and extract $\\hat{x}_T$ as its final $N$-block.\n\n- Four-dimensional variational data assimilation (4D-Var): Stack $X$ as above and minimize\n$$\nJ_{\\text{4D}}(X) = \\sum_{k=1}^T \\| y_k - H x_k \\|_{R^{-1}}^2 + \\beta \\| D x_1 \\|_2^2 + \\sum_{k=1}^{T-1} \\| x_{k+1} - x_k \\|_{Q^{-1}}^2,\n$$\nwhere $Q = q I_N$ so that $Q^{-1} = \\frac{1}{q} I_N$, $\\beta > 0$, and $q > 0$.\n\nMetrics to compute at the final time $T$:\n- RMSE: \n$$\n\\text{RMSE} = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N \\left( \\hat{x}_{T,i} - x_{T,i}^{\\text{true}} \\right)^2 }.\n$$\n- Average resolution width: Let $W$ denote the linear estimator mapping stacked data $Y = [y_1^T,\\dots,y_T^T]^T$ to the stacked estimate $\\hat{X}$ for the method in question, and $G$ the block observation operator mapping $X$ to $Y$. The model resolution matrix is $R_{\\text{mod}} = W G$. Extract the $N \\times N$ final-time sub-block $R_T$ mapping the true $x_T$ to the estimated $\\hat{x}_T$. For each row $i$, form nonnegative weights $a_{i j} = |(R_T)_{i j}|$, compute the second central moment \n$$\nw_i = \\sqrt{ \\frac{\\sum_{j=1}^N a_{i j} (s_j - s_i)^2}{\\sum_{j=1}^N a_{i j}} },\n$$\nand report the average width $\\frac{1}{N} \\sum_{i=1}^N w_i$. For SDI, use $R_T$ as the resolution of the final change estimator $\\widehat{\\Delta x}_T$ with respect to $x_T$ via $y_T - y_{T-1}$.\n- Average posterior uncertainty: In the linear Gaussian setting, the posterior covariance for the stacked state is \n$$\n\\Sigma_{\\text{post}} = \\left(G^T R^{-1} G + \\Gamma \\right)^{-1},\n$$\nwhere $\\Gamma$ is the prior precision implied by each method. Extract the final-time $N \\times N$ sub-block and report the average marginal posterior standard deviation, i.e.,\n$$\n\\frac{1}{N} \\sum_{i=1}^N \\sqrt{ (\\Sigma_{\\text{post},T})_{i i} }.\n$$\nFor SDI, approximate the final-time posterior variance as the sum of the marginal posterior variances of $\\hat{x}_1$ and the $\\widehat{\\Delta x}_k$ for $k \\in \\{2,\\dots,T\\}$, assuming independence across these Gaussian estimators due to independent data; report the average of the square roots of the diagonal entries of this sum.\n\nYour program must implement the three estimators by forming and solving the corresponding normal equations derived from the minimizers above. Use the following three test cases, each specified by $(M, \\sigma, \\lambda_s, \\lambda_{\\Delta}, \\lambda_t, \\beta, q)$:\n- Test case $1$: $(20, 0.01, 0.01, 0.01, 0.01, 0.01, 0.005)$.\n- Test case $2$: $(20, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01)$.\n- Test case $3$: $(10, 0.02, 0.01, 0.01, 0.005, 0.01, 0.005)$.\n\nAll quantities are dimensionless and thus require no physical unit conversion. Angles do not appear. Percentages do not appear.\n\nFinal output format: Your program should produce a single line of output containing the aggregated results as a comma-separated list enclosed in square brackets. For each test case in the order $1,2,3$, append the nine floats in this order: $\\text{RMSE}_{\\text{SDI}}$, $\\text{ResWidth}_{\\text{SDI}}$, $\\text{PostStd}_{\\text{SDI}}$, $\\text{RMSE}_{\\text{JI}}$, $\\text{ResWidth}_{\\text{JI}}$, $\\text{PostStd}_{\\text{JI}}$, $\\text{RMSE}_{\\text{4D}}$, $\\text{ResWidth}_{\\text{4D}}$, $\\text{PostStd}_{\\text{4D}}$. Therefore the final output line contains $27$ floats inside a single list, in the exact specified order.",
            "solution": "The user has provided a well-defined time-lapse inverse problem and requested a comparison of three reconstruction methods: Stabilized Difference Inversion (SDI), Joint Inversion (JI), and Four-Dimensional Variational Data Assimilation (4D-Var). The problem is scientifically grounded, mathematically consistent, complete, and well-posed. All necessary parameters, models, and metric definitions are provided. Thus, the problem is deemed valid and a full solution can be constructed.\n\nThe core of the problem lies in finding the minimizer of three different Tikhonov-style quadratic cost functions, which corresponds to solving a system of linear normal equations. From these solutions, we derive the estimators and the associated statistical and resolution metrics.\n\nLet the stacked state vector be $X = [x_1^T, \\dots, x_T^T]^T \\in \\mathbb{R}^{NT}$ and the stacked data vector be $Y = [y_1^T, \\dots, y_T^T]^T \\in \\mathbb{R}^{MT}$. The forward model can be written as $Y = \\mathcal{H} X + \\mathcal{E}$, where $\\mathcal{H} = I_T \\otimes H$ is the block-diagonal observation operator and $\\mathcal{E}$ is the stacked noise vector with covariance $\\mathcal{R} = I_T \\otimes R = \\sigma^2 I_{MT}$.\n\nThe general form of the posterior probability density function in a linear Gaussian framework is $p(X|Y) \\propto \\exp(-\\frac{1}{2} J(X))$, where $J(X)$ is the objective function. The objective function is the sum of a data misfit term and a prior term:\n$$\nJ(X) = (Y - \\mathcal{H}X)^T \\mathcal{R}^{-1} (Y - \\mathcal{H}X) + (X - X_{\\text{prior}})^T \\Gamma (X - X_{\\text{prior}})\n$$\nwhere $X_{\\text{prior}}$ is the prior mean (assumed to be zero here) and $\\Gamma$ is the prior precision (inverse covariance) matrix. The minimizer of $J(X)$, which is the maximum a posteriori (MAP) estimate $\\hat{X}$, is found by setting the gradient $\\nabla_X J(X)$ to zero. This leads to the normal equations:\n$$\n(\\mathcal{H}^T \\mathcal{R}^{-1} \\mathcal{H} + \\Gamma) \\hat{X} = \\mathcal{H}^T \\mathcal{R}^{-1} Y\n$$\nThe Hessian of $\\frac{1}{2} J(X)$ is the posterior precision matrix $\\Sigma_{\\text{post}}^{-1} = (\\mathcal{H}^T \\mathcal{R}^{-1} \\mathcal{H} + \\Gamma)$. The posterior covariance is its inverse, $\\Sigma_{\\text{post}}$. The model resolution matrix, mapping the true state $X^{\\text{true}}$ to the estimate $\\hat{X}$ in the absence of noise, is $R_{\\text{mod}} = \\Sigma_{\\text{post}} \\mathcal{H}^T \\mathcal{R}^{-1} \\mathcal{H}$.\n\nWe will now apply this framework to each of the three methods.\n\n**1. Stabilized Difference Inversion (SDI)**\n\nSDI operates sequentially. First, an estimate for the initial state $x_1$ is computed. Then, estimates for the changes $\\Delta x_k = x_k - x_{k-1}$ are computed for subsequent times.\n\nThe cost function for the initial state $x_1$ is:\n$$\nJ_{\\text{SDI},1}(x_1) = \\| y_1 - H x_1 \\|_{R^{-1}}^2 + \\lambda_s \\| D x_1 \\|_2^2 = \\frac{1}{\\sigma^2} \\| y_1 - H x_1 \\|_2^2 + \\lambda_s x_1^T D^T D x_1\n$$\nThis corresponds to a prior precision $\\Gamma_1 = \\lambda_s D^T D$. The normal equations are:\n$$\n\\left(\\frac{1}{\\sigma^2} H^T H + \\lambda_s D^T D\\right) \\hat{x}_1 = \\frac{1}{\\sigma^2} H^T y_1\n$$\nThe posterior covariance for $x_1$ is $\\Sigma_{\\text{post},1} = \\left(\\frac{1}{\\sigma^2} H^T H + \\lambda_s D^T D\\right)^{-1}$.\n\nFor $k \\in \\{2, \\dots, T\\}$, the change $\\Delta x_k$ is estimated by minimizing:\n$$\nJ_{\\text{SDI},\\Delta}(\\Delta x) = \\| (y_k - y_{k-1}) - H \\Delta x \\|_{R^{-1}}^2 + \\lambda_{\\Delta} \\| D \\Delta x \\|_2^2 = \\frac{1}{\\sigma^2} \\| \\Delta y_k - H \\Delta x \\|_2^2 + \\lambda_{\\Delta} \\Delta x^T D^T D \\Delta x\n$$\nwhere $\\Delta y_k = y_k - y_{k-1}$. The normal equations for the estimate $\\widehat{\\Delta x}_k$ are:\n$$\n\\left(\\frac{1}{\\sigma^2} H^T H + \\lambda_{\\Delta} D^T D\\right) \\widehat{\\Delta x}_k = \\frac{1}{\\sigma^2} H^T \\Delta y_k\n$$\nThe posterior covariance for $\\Delta x_k$ is $\\Sigma_{\\text{post},\\Delta} = \\left(\\frac{1}{\\sigma^2} H^T H + \\lambda_{\\Delta} D^T D\\right)^{-1}$. Note that this formulation implicitly assumes the noise on $\\Delta y_k$ has covariance $R$, whereas it is actually $2R$. We adhere to the problem's definition.\n\nThe final state estimate is $\\hat{x}_T = \\hat{x}_1 + \\sum_{k=2}^T \\widehat{\\Delta x}_k$.\nThe final-time posterior covariance is approximated by summing the covariances of the independent estimators:\n$$\n\\Sigma_{\\text{post},T}^{\\text{SDI}} \\approx \\Sigma_{\\text{post},1} + \\sum_{k=2}^T \\Sigma_{\\text{post},\\Delta} = \\Sigma_{\\text{post},1} + (T-1)\\Sigma_{\\text{post},\\Delta}\n$$\nThe model resolution matrix for the final change estimator $\\widehat{\\Delta x}_T$ with respect to $x_T$ is:\n$$\nR_T^{\\text{SDI}} = \\Sigma_{\\text{post},\\Delta} \\left(\\frac{1}{\\sigma^2} H^T H\\right)\n$$\n\n**2. Joint Inversion (JI)**\n\nJI estimates all states $X = [x_1^T, \\dots, x_T^T]^T$ simultaneously. The cost function is:\n$$\nJ_{\\text{JI}}(X) = \\sum_{k=1}^T \\frac{1}{\\sigma^2}\\| y_k - H x_k \\|_2^2 + \\lambda_s \\sum_{k=1}^T \\| D x_k \\|_2^2 + \\lambda_t \\sum_{k=1}^{T-1} \\| x_{k+1} - x_k \\|_2^2\n$$\nThis can be written in matrix form as:\n$$\nJ_{\\text{JI}}(X) = \\frac{1}{\\sigma^2}\\|Y - \\mathcal{H}X\\|_2^2 + \\lambda_s X^T (I_T \\otimes D^T D) X + \\lambda_t X^T (T_{\\text{op}}^T T_{\\text{op}}) X\n$$\nwhere $T_{\\text{op}}$ is the temporal first-difference operator. The prior precision matrix is $\\Gamma_{\\text{JI}} = \\lambda_s (I_T \\otimes D^T D) + \\lambda_t (T_{\\text{op}}^T T_{\\text{op}})$. The normal equations are:\n$$\n\\left( \\frac{1}{\\sigma^2} \\mathcal{H}^T \\mathcal{H} + \\Gamma_{\\text{JI}} \\right) \\hat{X} = \\frac{1}{\\sigma^2} \\mathcal{H}^T Y\n$$\nLet $\\mathcal{A}_{\\text{JI}} = \\left( \\frac{1}{\\sigma^2} \\mathcal{H}^T \\mathcal{H} + \\Gamma_{\\text{JI}} \\right)$. The posterior covariance for the entire state history is $\\Sigma_{\\text{post}}^{\\text{JI}} = \\mathcal{A}_{\\text{JI}}^{-1}$. The model resolution matrix is $R_{\\text{mod}}^{\\text{JI}} = \\Sigma_{\\text{post}}^{\\text{JI}} \\left(\\frac{1}{\\sigma^2} \\mathcal{H}^T \\mathcal{H}\\right)$. We extract the final-time $N \\times N$ sub-blocks from $\\Sigma_{\\text{post}}^{\\text{JI}}$ and $R_{\\text{mod}}^{\\text{JI}}$ to compute the metrics.\n\n**3. Four-Dimensional Variational Data Assimilation (4D-Var)**\n\n4D-Var is similar to JI but uses a different prior structure, typically informed by a state-space model:\n$$\nJ_{\\text{4D}}(X) = \\sum_{k=1}^T \\frac{1}{\\sigma^2}\\| y_k - H x_k \\|_2^2 + \\beta \\| D x_1 \\|_2^2 + \\sum_{k=1}^{T-1} \\frac{1}{q} \\| x_{k+1} - x_k \\|_2^2\n$$\nThe prior penalizes the roughness of the initial state $x_1$ and the temporal changes $x_{k+1}-x_k$. In matrix form, the prior precision matrix is $\\Gamma_{\\text{4D}} = \\beta \\text{diag}(D^T D, 0, \\dots, 0) + \\frac{1}{q} T_{\\text{op}}^T T_{\\text{op}}$. The normal equations are:\n$$\n\\left( \\frac{1}{\\sigma^2} \\mathcal{H}^T \\mathcal{H} + \\Gamma_{\\text{4D}} \\right) \\hat{X} = \\frac{1}{\\sigma^2} \\mathcal{H}^T Y\n$$\nLet $\\mathcal{A}_{\\text{4D}} = \\left( \\frac{1}{\\sigma^2} \\mathcal{H}^T \\mathcal{H} + \\Gamma_{\\text{4D}} \\right)$. The posterior covariance is $\\Sigma_{\\text{post}}^{\\text{4D}} = \\mathcal{A}_{\\text{4D}}^{-1}$ and the model resolution matrix is $R_{\\text{mod}}^{\\text{4D}} = \\Sigma_{\\text{post}}^{\\text{4D}} \\left(\\frac{1}{\\sigma^2} \\mathcal{H}^T \\mathcal{H}\\right)$. Again, we extract the final-time sub-blocks for metric calculations.\n\n**Metric Calculations**\n\nFor each method, the three required metrics are computed for the final time step $T$:\n- **RMSE**: Calculated directly using the estimated final state $\\hat{x}_T$ and the true state $x_T^{\\text{true}}$.\n- **Average Resolution Width**: For a given final-time model resolution matrix $R_T$, the width for each grid point $i$ is calculated as the weighted standard deviation of grid coordinates, where weights are the absolute values of the $i$-th row of $R_T$. The final metric is the average of these widths over all grid points.\n- **Average Posterior Uncertainty**: For a given final-time posterior covariance matrix $\\Sigma_{\\text{post}, T}$, the uncertainty for each grid point $i$ is the square root of the corresponding diagonal element (the marginal posterior variance). The final metric is the average of these standard deviations.\n\nThe following Python program implements these derivations to solve for the estimators and compute the specified metrics for each test case.",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import block_diag\n\ndef calculate_average_resolution_width(R_T, s):\n    \"\"\"Computes the average resolution width.\"\"\"\n    N = R_T.shape[0]\n    widths = np.zeros(N)\n    for i in range(N):\n        abs_row = np.abs(R_T[i, :])\n        denominator = np.sum(abs_row)\n        if denominator > np.finfo(float).eps:\n            s_i = s[i]\n            numerator = np.sum(abs_row * (s - s_i)**2)\n            widths[i] = np.sqrt(numerator / denominator)\n        else:\n            widths[i] = 0.0\n    return np.mean(widths)\n\ndef calculate_average_posterior_std(Sigma_T):\n    \"\"\"Computes the average posterior standard deviation.\"\"\"\n    variances = np.diag(Sigma_T)\n    # Ensure variances are non-negative before taking sqrt\n    return np.mean(np.sqrt(np.maximum(0, variances)))\n\ndef solve():\n    \"\"\"\n    Main function to solve the time-lapse inverse problem for the given test cases.\n    \"\"\"\n    N = 20\n    T = 3\n    sigma_h = 0.05\n    \n    # Use a fixed seed for reproducibility of synthetic noise\n    rng = np.random.default_rng(0)\n\n    # Spatial grid\n    s = np.linspace(0, 1, N)\n\n    # True model generation\n    x_true = np.zeros((T, N))\n    x_true[0, :] = 0.1 * np.sin(2 * np.pi * s) + np.exp(-(s - 0.3)**2 / (2 * 0.05**2))\n    x_true[1, :] = 0.1 * np.sin(2 * np.pi * s) + 0.8 * np.exp(-(s - 0.5)**2 / (2 * 0.05**2))\n    x_true[2, :] = 0.1 * np.sin(2 * np.pi * s) + 0.6 * np.exp(-(s - 0.7)**2 / (2 * 0.05**2))\n\n    # Spatial difference operator D\n    D = np.eye(N) - np.roll(np.eye(N), 1, axis=0)\n    D[0, N-1] = -1\n    DtD = D.T @ D\n    \n    # Temporal difference operator T_op\n    T_op_blocks = []\n    for k in range(T - 1):\n        row = [np.zeros((N, N))] * T\n        row[k] = -np.eye(N)\n        row[k+1] = np.eye(N)\n        T_op_blocks.append(np.hstack(row))\n    T_op = np.vstack(T_op_blocks)\n    T_op_t_T_op = T_op.T @ T_op\n\n    test_cases = [\n        # (M, sigma, lambda_s, lambda_delta, lambda_t, beta, q)\n        (20, 0.01, 0.01, 0.01, 0.01, 0.01, 0.005),\n        (20, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01),\n        (10, 0.02, 0.01, 0.01, 0.005, 0.01, 0.005),\n    ]\n\n    results = []\n\n    for M, sigma, lambda_s, lambda_delta, lambda_t, beta, q in test_cases:\n        # Observation operator H\n        r = np.linspace(0, 1, M)\n        H = np.exp(-(s[np.newaxis, :] - r[:, np.newaxis])**2 / (2 * sigma_h**2))\n        HtH = H.T @ H\n\n        # Generate noisy data\n        y = np.zeros((T, M))\n        for k in range(T):\n            noise = rng.normal(0, sigma, M)\n            y[k, :] = H @ x_true[k, :] + noise\n\n        # --- 1. Stabilized Difference Inversion (SDI) ---\n        # Estimators\n        A1_sdi = (1/sigma**2) * HtH + lambda_s * DtD\n        A_delta_sdi = (1/sigma**2) * HtH + lambda_delta * DtD\n        \n        inv_A1_sdi = np.linalg.inv(A1_sdi)\n        inv_A_delta_sdi = np.linalg.inv(A_delta_sdi)\n        \n        rhs1 = (1/sigma**2) * H.T @ y[0]\n        x1_hat_sdi = inv_A1_sdi @ rhs1\n        \n        delta_x_hat_sum = np.zeros(N)\n        for k in range(1, T):\n            delta_y = y[k] - y[k-1]\n            rhs_delta = (1/sigma**2) * H.T @ delta_y\n            delta_x_hat_k = inv_A_delta_sdi @ rhs_delta\n            delta_x_hat_sum += delta_x_hat_k\n            \n        xT_hat_sdi = x1_hat_sdi + delta_x_hat_sum\n        \n        # Metrics for SDI\n        rmse_sdi = np.sqrt(np.mean((xT_hat_sdi - x_true[T-1, :])**2))\n        \n        Sigma_post_1_sdi = inv_A1_sdi\n        Sigma_post_delta_sdi = inv_A_delta_sdi\n        Sigma_post_T_sdi = Sigma_post_1_sdi + (T-1) * Sigma_post_delta_sdi\n        post_std_sdi = calculate_average_posterior_std(Sigma_post_T_sdi)\n\n        R_T_sdi = Sigma_post_delta_sdi @ ((1/sigma**2) * HtH)\n        res_width_sdi = calculate_average_resolution_width(R_T_sdi, s)\n        \n        results.extend([rmse_sdi, res_width_sdi, post_std_sdi])\n        \n        # --- Common matrices for JI and 4D-Var ---\n        NT = N * T\n        H_cal = block_diag(*([H] * T))\n        H_cal_T_H_cal = block_diag(*([HtH] * T))\n        H_cal_T_y = np.concatenate([H.T @ yk for yk in y])\n\n        # --- 2. Joint Inversion (JI) ---\n        Gamma_JI = lambda_s * block_diag(*([DtD] * T)) + lambda_t * T_op_t_T_op\n        A_JI = (1/sigma**2) * H_cal_T_H_cal + Gamma_JI\n        rhs_JI = (1/sigma**2) * H_cal_T_y\n        \n        X_hat_ji = np.linalg.solve(A_JI, rhs_JI)\n        xT_hat_ji = X_hat_ji[(T-1)*N:]\n        \n        # Metrics for JI\n        rmse_ji = np.sqrt(np.mean((xT_hat_ji - x_true[T-1,:])**2))\n        \n        Sigma_post_JI = np.linalg.inv(A_JI)\n        Sigma_post_T_ji = Sigma_post_JI[(T-1)*N:, (T-1)*N:]\n        post_std_ji = calculate_average_posterior_std(Sigma_post_T_ji)\n        \n        R_mod_JI = Sigma_post_JI @ ((1/sigma**2) * H_cal_T_H_cal)\n        R_TT_ji = R_mod_JI[(T-1)*N:, (T-1)*N:]\n        res_width_ji = calculate_average_resolution_width(R_TT_ji, s)\n        \n        results.extend([rmse_ji, res_width_ji, post_std_ji])\n        \n        # --- 3. Four-Dimensional Variational (4D-Var) ---\n        D1tD1 = np.zeros((NT, NT))\n        D1tD1[:N, :N] = DtD\n        Gamma_4D = beta * D1tD1 + (1/q) * T_op_t_T_op\n        A_4D = (1/sigma**2) * H_cal_T_H_cal + Gamma_4D\n        rhs_4D = (1/sigma**2) * H_cal_T_y\n\n        X_hat_4d = np.linalg.solve(A_4D, rhs_4D)\n        xT_hat_4d = X_hat_4d[(T-1)*N:]\n\n        # Metrics for 4D-Var\n        rmse_4d = np.sqrt(np.mean((xT_hat_4d - x_true[T-1,:])**2))\n\n        Sigma_post_4D = np.linalg.inv(A_4D)\n        Sigma_post_T_4d = Sigma_post_4D[(T-1)*N:, (T-1)*N:]\n        post_std_4d = calculate_average_posterior_std(Sigma_post_T_4d)\n        \n        R_mod_4D = Sigma_post_4D @ ((1/sigma**2) * H_cal_T_H_cal)\n        R_TT_4d = R_mod_4D[(T-1)*N:, (T-1)*N:]\n        res_width_4d = calculate_average_resolution_width(R_TT_4d, s)\n        \n        results.extend([rmse_4d, res_width_4d, post_std_4d])\n\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "To capture the complex structure of dynamic changes, modern inversion techniques often employ regularization beyond simple smoothness. This exercise introduces advanced, non-smooth priors, such as the nuclear norm to enforce low-rank structure across time and the $\\ell_1$ norm to promote sparse changes. You will derive the fundamental building blocks—proximal operators—for these regularizers and assemble them within a state-of-the-art primal-dual optimization algorithm, gaining experience with the mathematical tools that underpin much of contemporary inverse problem theory .",
            "id": "3427752",
            "problem": "Consider a linearized time-lapse inversion model in which a sequence of subsurface states across $T$ survey times and $P$ spatial parameters is collected into a matrix $M \\in \\mathbb{R}^{P \\times T}$. Let $x \\in \\mathbb{R}^{PT}$ denote the column-wise vectorization of $M$, written $x = \\operatorname{vec}(M)$. The observed data $d \\in \\mathbb{R}^{n}$ are linked to $x$ by a known linear forward operator $A \\in \\mathbb{R}^{n \\times PT}$ via $d \\approx A x$, with additive noise. We seek the minimum of the composite convex objective\n$$\n\\min_{x \\in \\mathbb{R}^{PT}} \\;\\; \\frac{1}{2}\\|A x - d\\|_{2}^{2} \\;+\\; \\alpha \\|R x\\|_{*} \\;+\\; \\beta \\|L x\\|_{1},\n$$\nwhere $R : \\mathbb{R}^{PT} \\to \\mathbb{R}^{P \\times T}$ is the reshaping operator that maps $x$ to $M = R x$, the symbol $\\|\\cdot\\|_{*}$ denotes the nuclear norm (sum of singular values), $L \\in \\mathbb{R}^{Q \\times PT}$ is a fixed linear operator capturing sparse innovations (for example, temporal differencing), and $\\alpha > 0$, $\\beta > 0$ are regularization parameters.\n\nTasks:\n1) Starting from the definition of the proximal operator for a proper, closed, convex function $\\varphi$,\n$$\n\\operatorname{prox}_{\\gamma \\varphi}(Z) \\;=\\; \\arg\\min_{X} \\left\\{ \\varphi(X) \\;+\\; \\frac{1}{2 \\gamma}\\|X - Z\\|_{F}^{2} \\right\\},\n$$\nderive the proximal operator of the nuclear norm $\\operatorname{prox}_{\\gamma \\alpha \\|\\cdot\\|_{*}}(Z)$ as a singular-value transformation. Your derivation must begin from first principles in convex analysis and matrix inequalities without assuming the final form.\n\n2) Using the same proximal definition, derive the proximal operator of the $\\ell_{1}$ norm $\\operatorname{prox}_{\\gamma \\beta \\|\\cdot\\|_{1}}(z)$ as a coordinate-wise transformation, starting from subdifferential calculus for the absolute value and without assuming the final form.\n\n3) Define the stacked linear operator $K : \\mathbb{R}^{PT} \\to \\mathbb{R}^{P \\times T} \\times \\mathbb{R}^{Q}$ by $K x = (R x, L x)$ and the convex function $F(U, v) = \\alpha \\|U\\|_{*} + \\beta \\|v\\|_{1}$, and $G(x) = \\frac{1}{2}\\|A x - d\\|_{2}^{2}$. Assemble a primal–dual hybrid gradient (PDHG, also known as the Chambolle–Pock method) iteration for minimizing $G(x) + F(K x)$. Your assembly must explicitly specify:\n- The dual updates using proximal operators of the convex conjugates of the nuclear and $\\ell_{1}$ norms, expressed using the Moreau decomposition starting from its definition.\n- The primal update using the proximal operator of $G$ obtained by solving the corresponding optimality condition.\n- The over-relaxation step.\n- The spectral step-size condition on $\\tau, \\sigma > 0$ that ensures convergence in terms of the operator norm of $K$.\n\n4) Specialize to a minimal time-lapse configuration with $P = 2$ and $T = 2$, so $x \\in \\mathbb{R}^{4}$. Let $R : \\mathbb{R}^{4} \\to \\mathbb{R}^{2 \\times 2}$ be the orthonormal reshaping operator $R x = M$ that places the entries of $x$ into a $2 \\times 2$ matrix by columns, and let the temporal differencing operator $L \\in \\mathbb{R}^{2 \\times 4}$ act as\n$$\nL = \\begin{bmatrix}\n-1 & 0 & 1 & 0 \\\\\n0 & -1 & 0 & 1\n\\end{bmatrix},\n$$\nso that $L x$ equals the element-wise difference between the second and first time frames. Using your step-size condition from Task $3$, compute the exact supremum of the admissible product $\\tau \\sigma$ over all positive step sizes $\\tau, \\sigma$ that satisfy the convergence condition. Express your final answer as a single reduced fraction with no units. Do not approximate. Provide this single fraction as your final answer.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the field of convex optimization and inverse problems, well-posed, and expressed in objective, formal language. It is self-contained and free of contradictions or ambiguities. The tasks require rigorous mathematical derivations and a specific calculation based on established theories, which are appropriate for a formal scientific context.\n\nThe solution proceeds by addressing each of the four tasks in sequence.\n\nTask 1: Derivation of the Proximal Operator of the Nuclear Norm\n\nThe proximal operator of the function $\\varphi(X) = \\alpha \\|X\\|_{*}$ with scaling parameter $\\gamma > 0$ is defined as:\n$$\n\\operatorname{prox}_{\\gamma \\alpha \\|\\cdot\\|_{*}}(Z) = \\arg\\min_{X \\in \\mathbb{R}^{P \\times T}} \\left\\{ \\alpha \\|X\\|_{*} + \\frac{1}{2 \\gamma}\\|X - Z\\|_{F}^{2} \\right\\}\n$$\nLet the Singular Value Decomposition (SVD) of the matrix $Z \\in \\mathbb{R}^{P \\times T}$ be $Z = U \\Sigma V^T$, where $U \\in \\mathbb{R}^{P \\times P}$ and $V \\in \\mathbb{R}^{T \\times T}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{P \\times T}$ is a rectangular diagonal matrix with non-negative real numbers $\\sigma_i$ on the diagonal, which are the singular values of $Z$. Let the SVD of the variable $X$ be $X = U_X S_X V_X^T$, where $S_X$ contains the singular values $s_i$ of $X$.\n\nThe objective function can be rewritten using the trace inner product:\n$$\n\\|X - Z\\|_{F}^{2} = \\operatorname{Tr}((X-Z)^T(X-Z)) = \\operatorname{Tr}(X^TX) - 2\\operatorname{Tr}(X^TZ) + \\operatorname{Tr}(Z^TZ) = \\|X\\|_{F}^{2} - 2\\operatorname{Tr}(X^TZ) + \\|Z\\|_{F}^{2}\n$$\nThe nuclear norm is the sum of singular values, $\\|X\\|_{*} = \\sum_i s_i$. The Frobenius norm is the square root of the sum of squares of singular values, $\\|X\\|_{F}^{2} = \\sum_i s_i^2$. The objective function to minimize is:\n$$\nJ(X) = \\alpha \\sum_i s_i + \\frac{1}{2\\gamma} \\left( \\sum_i s_i^2 - 2\\operatorname{Tr}(X^TZ) + \\sum_i \\sigma_i^2 \\right)\n$$\nTo minimize $J(X)$, we must maximize the term $\\operatorname{Tr}(X^TZ)$. According to the von Neumann trace inequality, for any two matrices $A$ and $B$, $\\operatorname{Tr}(A^T B) \\le \\sum_i \\sigma_i(A) \\sigma_i(B)$, with equality holding if and only if there exist unitary matrices $U_{AB}$ and $V_{AB}$ such that $A = U_{AB} \\Sigma_A V_{AB}^T$ and $B = U_{AB} \\Sigma_B V_{AB}^T$, i.e., they share the same singular vectors.\nThus, $\\operatorname{Tr}(X^TZ)$ is maximized when $X$ and $Z$ share the same singular vectors. We can therefore set $U_X = U$ and $V_X = V$, which means the optimal $X$ has the form $X = U S V^T$ for some diagonal matrix $S$ with non-negative entries $s_i$.\nUnder this substitution, $\\operatorname{Tr}(X^TZ) = \\operatorname{Tr}((USV^T)^T (U\\Sigma V^T)) = \\operatorname{Tr}(VS^T U^T U \\Sigma V^T) = \\operatorname{Tr}(V S \\Sigma V^T) = \\operatorname{Tr}(S \\Sigma V^T V) = \\operatorname{Tr}(S \\Sigma) = \\sum_i s_i \\sigma_i$.\nThe minimization problem decouples and becomes a set of independent scalar minimization problems for each singular value $s_i \\ge 0$:\n$$\n\\min_{s_i \\ge 0} \\left\\{ \\alpha s_i + \\frac{1}{2\\gamma} (s_i^2 - 2s_i\\sigma_i + \\sigma_i^2) \\right\\} = \\min_{s_i \\ge 0} \\left\\{ \\alpha s_i + \\frac{1}{2\\gamma} (s_i - \\sigma_i)^2 \\right\\}\n$$\nLet's find the unconstrained minimum by setting the derivative with respect to $s_i$ to zero:\n$$\n\\frac{d}{ds_i} \\left( \\alpha s_i + \\frac{1}{2\\gamma} (s_i - \\sigma_i)^2 \\right) = \\alpha + \\frac{1}{\\gamma}(s_i - \\sigma_i) = 0\n$$\nThis gives $s_i = \\sigma_i - \\gamma\\alpha$. Since singular values must be non-negative ($s_i \\ge 0$), the solution is the projection of this result onto the non-negative half-line:\n$$\ns_i = \\max(0, \\sigma_i - \\gamma\\alpha) \\equiv (\\sigma_i - \\gamma\\alpha)_+\n$$\nThis operation is known as soft-thresholding. The proximal operator for the nuclear norm applies soft-thresholding to the singular values of the input matrix $Z$. If $Z=U \\operatorname{diag}(\\sigma_i) V^T$, then\n$$\n\\operatorname{prox}_{\\gamma \\alpha \\|\\cdot\\|_{*}}(Z) = U \\operatorname{diag}((\\sigma_i - \\gamma\\alpha)_+) V^T\n$$\nThis is the singular value thresholding (SVT) operator, often denoted $\\mathcal{D}_{\\gamma\\alpha}(Z)$ or $\\operatorname{SVT}_{\\gamma\\alpha}(Z)$.\n\nTask 2: Derivation of the Proximal Operator of the $\\ell_1$ Norm\n\nThe proximal operator of the function $\\varphi(x) = \\beta \\|x\\|_{1}$ with scaling parameter $\\gamma > 0$ is defined as:\n$$\n\\operatorname{prox}_{\\gamma \\beta \\|\\cdot\\|_{1}}(z) = \\arg\\min_{x \\in \\mathbb{R}^{PT}} \\left\\{ \\beta \\|x\\|_{1} + \\frac{1}{2 \\gamma}\\|x - z\\|_{2}^{2} \\right\\}\n$$\nThe objective function is separable with respect to the components of $x$. We can write it as a sum over the components $x_i$:\n$$\n\\sum_{i=1}^{PT} \\left( \\beta |x_i| + \\frac{1}{2\\gamma}(x_i - z_i)^2 \\right)\n$$\nWe can minimize each term independently. For each component $i$, we solve:\n$$\n\\min_{x_i \\in \\mathbb{R}} \\left\\{ \\beta |x_i| + \\frac{1}{2\\gamma}(x_i - z_i)^2 \\right\\}\n$$\nLet $\\phi_i(x_i) = \\beta |x_i| + \\frac{1}{2\\gamma}(x_i - z_i)^2$. Since this function is convex, a point $x_i^*$ is a minimizer if and only if $0$ is in the subdifferential of $\\phi_i$ at $x_i^*$. The optimality condition is $0 \\in \\partial \\phi_i(x_i^*)$.\nThe subdifferential is $\\partial \\phi_i(x_i) = \\beta \\partial|x_i| + \\frac{1}{\\gamma}(x_i - z_i)$. The subdifferential of the absolute value function is:\n$$\n\\partial|u| = \\begin{cases} \\{1\\} & \\text{if } u > 0 \\\\ \\{-1\\} & \\text{if } u < 0 \\\\ [-1, 1] & \\text{if } u = 0 \\end{cases}\n$$\nWe analyze three cases for the optimal $x_i^*$:\nCase 1: $x_i^* > 0$. Then $\\partial|x_i^*| = \\{1\\}$, and the optimality condition becomes $0 = \\beta \\cdot 1 + \\frac{1}{\\gamma}(x_i^* - z_i)$, which yields $x_i^* = z_i - \\gamma\\beta$. For this case to be self-consistent, we must have $x_i^* > 0$, so $z_i - \\gamma\\beta > 0$, or $z_i > \\gamma\\beta$.\nCase 2: $x_i^* < 0$. Then $\\partial|x_i^*| = \\{-1\\}$, and the optimality condition is $0 = \\beta \\cdot (-1) + \\frac{1}{\\gamma}(x_i^* - z_i)$, which yields $x_i^* = z_i + \\gamma\\beta$. For self-consistency, we need $x_i^* < 0$, so $z_i + \\gamma\\beta < 0$, or $z_i < -\\gamma\\beta$.\nCase 3: $x_i^* = 0$. Then $\\partial|x_i^*| = [-1, 1]$, and the optimality condition is $0 \\in \\beta [-1, 1] + \\frac{1}{\\gamma}(0 - z_i)$, which simplifies to $\\frac{z_i}{\\gamma} \\in [-\\beta, \\beta]$, or $|z_i| \\le \\gamma\\beta$.\n\nCombining these three cases yields the solution for $x_i^*$:\n$$\nx_i^* = \\begin{cases} z_i - \\gamma\\beta & \\text{if } z_i > \\gamma\\beta \\\\ z_i + \\gamma\\beta & \\text{if } z_i < -\\gamma\\beta \\\\ 0 & \\text{if } |z_i| \\le \\gamma\\beta \\end{cases}\n$$\nThis is the component-wise soft-thresholding operator, which can be written compactly as:\n$$\nx_i^* = \\operatorname{sign}(z_i) \\max(|z_i| - \\gamma\\beta, 0)\n$$\nThe proximal operator is therefore the element-wise application of this function, denoted $S_{\\gamma\\beta}(z)$.\n\nTask 3: Primal-Dual Hybrid Gradient (PDHG) Iteration\n\nThe problem is to minimize $G(x) + F(Kx)$. The PDHG (Chambolle-Pock) algorithm for this problem structure, with primal variable $x$, dual variable $y = (Y, q)$, and over-relaxation parameter $\\theta=1$, is given by the following iteration sequence, starting from $(x^k, y^k)$:\nPrimal update: $x^{k+1} = \\operatorname{prox}_{\\tau G}(x^k - \\tau K^* y^k)$\nOver-relaxation: $\\bar{x}^{k+1} = x^{k+1} + (x^{k+1} - x^k) = 2x^{k+1} - x^k$\nDual update: $y^{k+1} = \\operatorname{prox}_{\\sigma F^*}(y^k + \\sigma K \\bar{x}^{k+1})$\n\nLet's specify each component for the given problem.\nThe dual variable is $y=(Y,q)$, where $Y\\in\\mathbb{R}^{P\\times T}$ is the dual to $Rx$ and $q\\in\\mathbb{R}^Q$ is the dual to $Lx$. The operator $K$ is $Kx = (Rx, Lx)$, and its adjoint is $K^*(Y,q) = R^*Y + L^*q$.\n\nDual Updates:\nThe function $F(U, v) = \\alpha\\|U\\|_{*} + \\beta\\|v\\|_{1}$ is separable. Its convex conjugate is $F^*(Y,q) = (\\alpha\\|\\cdot\\|_{*})^*(Y) + (\\beta\\|\\cdot\\|_{1})^*(q)$. The proximal operator of $F^*$ is also separable:\n$$\n\\operatorname{prox}_{\\sigma F^*}(Y,q) = (\\operatorname{prox}_{\\sigma (\\alpha\\|\\cdot\\|_{*})^*}(Y), \\operatorname{prox}_{\\sigma (\\beta\\|\\cdot\\|_{1})^*}(q))\n$$\nWe use the Moreau decomposition identity: $\\operatorname{prox}_{\\gamma \\phi^*}(z) = z - \\gamma \\operatorname{prox}_{\\phi/\\gamma}(z/\\gamma)$.\n\nFor the nuclear norm part, let $f_1(U) = \\alpha\\|U\\|_*$. The dual update for $Y$ is:\n$Y^{k+1} = \\operatorname{prox}_{\\sigma f_1^*}(Y^k + \\sigma R \\bar{x}^{k+1})$.\nUsing Moreau's identity with $\\phi = f_1$ and $\\gamma = \\sigma$:\n$Y^{k+1} = (Y^k + \\sigma R \\bar{x}^{k+1}) - \\sigma \\operatorname{prox}_{f_1/\\sigma}((Y^k + \\sigma R \\bar{x}^{k+1})/\\sigma) = (Y^k + \\sigma R \\bar{x}^{k+1}) - \\sigma \\operatorname{prox}_{(\\alpha/\\sigma)\\|\\cdot\\|_{*}}((Y^k + \\sigma R \\bar{x}^{k+1})/\\sigma)$.\nUsing the result from Task 1, this is equivalent to projection onto the dual norm ball. The dual norm of the nuclear norm is the operator norm $\\|\\cdot\\|_{op}$. Thus, $(\\alpha\\|U\\|_*)^* = I_{\\|Y\\|_{op} \\le \\alpha}(Y)$, the indicator function of the set of matrices with operator norm at most $\\alpha$. The proximal operator of an indicator function is the Euclidean projection onto that set.\n$$\nY^{k+1} = \\mathcal{P}_{\\|Y\\|_{op} \\le \\alpha} (Y^k + \\sigma R \\bar{x}^{k+1})\n$$\nwhere $\\mathcal{P}$ denotes projection, which clips the singular values of its matrix argument at $\\alpha$.\n\nFor the $\\ell_1$ norm part, let $f_2(v) = \\beta\\|v\\|_1$. The dual update for $q$ is:\n$q^{k+1} = \\operatorname{prox}_{\\sigma f_2^*}(q^k + \\sigma L \\bar{x}^{k+1})$.\nThe conjugate of the $\\ell_1$ norm is the indicator function of the $\\ell_\\infty$ norm ball: $(\\beta\\|v\\|_1)^* = I_{\\|q\\|_{\\infty} \\le \\beta}(q)$. The proximal operator is the projection onto this ball.\n$$\nq^{k+1} = \\mathcal{P}_{\\|q\\|_{\\infty} \\le \\beta} (q^k + \\sigma L \\bar{x}^{k+1})\n$$\nThis projection clips the components of its vector argument at $\\beta$: $(q^{k+1})_i = \\operatorname{sign}((q^k + \\sigma L \\bar{x}^{k+1})_i) \\min(|(q^k + \\sigma L \\bar{x}^{k+1})_i|, \\beta)$.\n\nPrimal Update:\nThe primal update is $x^{k+1} = \\operatorname{prox}_{\\tau G}(x^k - \\tau K^* y^{k+1})$, where $K^*y^{k+1} = R^*Y^{k+1} + L^*q^{k+1}$. Let $z^k = x^k - \\tau(R^*Y^{k+1} + L^*q^{k+1})$.\nThe proximal operator is $x^{k+1} = \\arg\\min_x \\{G(x) + \\frac{1}{2\\tau}\\|x - z^k\\|_2^2\\}$.\nWith $G(x) = \\frac{1}{2}\\|Ax-d\\|_2^2$, this is a standard least-squares problem:\n$$\nx^{k+1} = \\arg\\min_x \\left\\{ \\frac{1}{2}\\|Ax - d\\|_{2}^{2} + \\frac{1}{2\\tau}\\|x - z^k\\|_{2}^{2} \\right\\}\n$$\nThe optimality condition is found by setting the gradient to zero:\n$A^T(Ax^{k+1} - d) + \\frac{1}{\\tau}(x^{k+1} - z^k) = 0$.\n$(A^TA + \\frac{1}{\\tau}I)x^{k+1} = A^Td + \\frac{1}{\\tau}z^k$.\nSolving for $x^{k+1}$:\n$$\nx^{k+1} = \\left(A^T A + \\frac{1}{\\tau} I \\right)^{-1} \\left(A^T d + \\frac{1}{\\tau} (x^k - \\tau(R^*Y^{k+1} + L^*q^{k+1})) \\right)\n$$\nThis involves solving a linear system at each iteration.\n\nOver-relaxation Step:\nThe over-relaxed primal variable used in the next dual update is computed as:\n$$\n\\bar{x}^{k+1} = 2x^{k+1} - x^k\n$$\n\nSpectral Step-Size Condition:\nFor the PDHG algorithm with $\\theta=1$, a sufficient condition for convergence for step sizes $\\tau, \\sigma > 0$ is:\n$$\n\\tau \\sigma \\|K\\|_{op}^2 < 1\n$$\nwhere $\\|K\\|_{op}$ is the operator norm of $K$ from $\\mathbb{R}^{PT}$ to $\\mathbb{R}^{P \\times T} \\times \\mathbb{R}^{Q}$ (with the latter having the standard product space norm). It is defined by $\\|K\\|_{op}^2 = \\lambda_{\\max}(K^*K)$, where $\\lambda_{\\max}$ is the maximum eigenvalue.\nWe have $K^*K = (R^*, L^T)\\begin{pmatrix} R \\\\ L \\end{pmatrix} = R^*R + L^TL$.\nSo the condition is $\\tau \\sigma \\lambda_{\\max}(R^*R + L^TL) < 1$.\n\nTask 4: Supremum of the Step-Size Product\n\nWe are given $P=2$, $T=2$, so $x \\in \\mathbb{R}^4$. The reshaping operator $R: \\mathbb{R}^4 \\to \\mathbb{R}^{2 \\times 2}$ is orthonormal. An operator is orthonormal if $R^*R=I$. Thus, for our specific $R$, we have $R^*R = I_4$, the $4 \\times 4$ identity matrix.\n\nThe temporal differencing operator is $L = \\begin{bmatrix} -1 & 0 & 1 & 0 \\\\ 0 & -1 & 0 & 1 \\end{bmatrix}$.\nWe need to compute $\\|K\\|_{op}^2 = \\lambda_{\\max}(R^*R + L^TL) = \\lambda_{\\max}(I_4 + L^TL)$.\nThe eigenvalues of $I_4 + L^TL$ are $1 + \\lambda_i(L^TL)$, where $\\lambda_i(L^TL)$ are the eigenvalues of $L^TL$. We need the maximum eigenvalue of $L^TL$.\nThe non-zero eigenvalues of $L^TL$ (a $4 \\times 4$ matrix) are the same as the non-zero eigenvalues of $LL^T$ (a $2 \\times 2$ matrix). Let's compute $LL^T$:\n$$\nL^T = \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\\\ 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\n$$\n$$\nLL^T = \\begin{bmatrix} -1 & 0 & 1 & 0 \\\\ 0 & -1 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\\\ 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} (-1)(-1) + (1)(1) & 0 \\\\ 0 & (-1)(-1) + (1)(1) \\end{bmatrix} = \\begin{bmatrix} 2 & 0 \\\\ 0 & 2 \\end{bmatrix}\n$$\nThe matrix $LL^T$ is a diagonal matrix, so its eigenvalues are its diagonal entries: $\\lambda_1 = 2$ and $\\lambda_2 = 2$.\nThe non-zero eigenvalues of $L^TL$ are therefore $2$ and $2$. Since $L^TL$ is $4 \\times 4$, its other two eigenvalues are $0$. The set of eigenvalues of $L^TL$ is $\\{2, 2, 0, 0\\}$.\nThe maximum eigenvalue is $\\lambda_{\\max}(L^TL) = 2$.\nNow we can compute $\\|K\\|_{op}^2$:\n$$\n\\|K\\|_{op}^2 = 1 + \\lambda_{\\max}(L^TL) = 1 + 2 = 3\n$$\nThe convergence condition becomes $\\tau \\sigma \\cdot 3 < 1$, which simplifies to:\n$$\n\\tau\\sigma < \\frac{1}{3}\n$$\nThe problem asks for the exact supremum of the admissible product $\\tau\\sigma$. The set of admissible products is the interval $(0, 1/3)$. The supremum of this set is the least upper bound, which is $1/3$.\nThe supremum of the product $\\tau\\sigma$ is $\\frac{1}{3}$.",
            "answer": "$$\\boxed{\\frac{1}{3}}$$"
        }
    ]
}