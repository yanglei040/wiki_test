{
    "hands_on_practices": [
        {
            "introduction": "模型分辨率矩阵 $R$ 的核心作用是描述真实模型如何映射到我们估计的模型上。它的非对角元素 $R_{ij}$（其中 $i \\neq j$）量化了所谓的“串扰”效应，即真实模型参数 $m_j$ 中的一个单元扰动会对另一个参数的估计值 $\\hat{m}_i$ 产生多大影响。本练习  将指导您在一个通用正则化最小二乘框架下，通过计算具体的 $R_{ij}$ 值来探索这种参数间的相互影响。",
            "id": "3403399",
            "problem": "考虑一个线性反演问题，其正演算子为 $G \\in \\mathbb{R}^{m \\times n}$，模型向量为 $m \\in \\mathbb{R}^{n}$，数据向量为 $d \\in \\mathbb{R}^{m}$，它们满足 $d = G m + \\varepsilon$。其中 $\\varepsilon$ 是加性噪声，其均值为零，协方差矩阵为 $C_d \\in \\mathbb{R}^{m \\times m}$，该矩阵是对称正定的。设估计量 $\\hat{m}$ 定义为严格凸二次目标函数的最小化子：\n$$\nJ(m) = \\| C_d^{-1/2} (G m - d)\\|_2^2 + \\| L m \\|_2^2,\n$$\n其中 $L \\in \\mathbb{R}^{r \\times n}$ 是一个固定的正则化算子（任何期望的缩放都已并入 $L$ 中）。矩阵 $L$ 可以是矩形的，并假设乘积 $L^\\top L$ 是良定义的。假设 $G$、$C_d$ 和 $L$ 使得最小化子是唯一的。\n\n从此定义和标准的最小二乘最优性条件出发，定义模型分辨率矩阵 $R \\in \\mathbb{R}^{n \\times n}$，通过矩阵元 $R_{ij}$ 解释参数 $i$ 和 $j$ 之间的串扰系数，并在理想化的无噪声设置 $d = G m_{\\text{true}}$ 下，计算第 $i$ 个估计模型分量 $\\hat{m}_i$ 对施加于第 $j$ 个真实模型分量 $m_j$ 的单位扰动的响应。对整数 $i$ 和 $j$ 使用从零开始的索引约定。\n\n您的程序必须：\n- 仅从所述目标函数的最优性条件推导出 $R$ 的表达式，除了给定假设外，不假设 $G$、$C_d$ 或 $L$ 具有任何特定结构。\n- 实现一个函数，该函数在给定 $G$、$C_d$、$L$ 和索引 $(i,j)$ 的情况下，返回标量串扰系数 $R_{ij}$，该系数等于当 $m_{\\text{true}}$ 的所有其他分量为零时，$\\hat{m}_i$ 对 $m_j$ 处单位扰动的响应。\n- 使用以下测试套件，其中每个情况都明确指定 $(G, C_d, L, i, j)$ 为实值数值数组和整数。所有数组都是无量纲的，不涉及物理单位。\n\n测试套件（从零开始的索引）：\n1. 情况 A（单位分辨率，对角线上）：\n   - $G = I_3$\n   - $C_d = I_3$\n   - $L = 0_{3 \\times 3}$（$3 \\times 3$ 零矩阵）\n   - $i = 1$, $j = 1$\n2. 情况 B（单位分辨率，非对角线上）：\n   - $G = I_3$\n   - $C_d = I_3$\n   - $L = 0_{3 \\times 3}$\n   - $i = 0$, $j = 2$\n3. 情况 C（带有一阶差分正则化的模糊正演算子）：\n   - $G = \\begin{bmatrix}\n     1  0.5  0  0 \\\\\n     0.5  1  0.5  0 \\\\\n     0  0.5  1  0.5 \\\\\n     0  0  0.5  1\n     \\end{bmatrix}$\n   - $C_d = I_4$\n   - $L = \\begin{bmatrix}\n     -1  1  0  0 \\\\\n     0  -1  1  0 \\\\\n     0  0  -1  1\n     \\end{bmatrix}$\n   - $i = 1$, $j = 2$\n4. 情况 D（带有各向异性数据协方差和一阶差分正则化的单位正演算子）：\n   - $G = I_4$\n   - $C_d = \\mathrm{diag}(0.04, 1.0, 0.25, 9.0)$\n   - $L = \\begin{bmatrix}\n     -1  1  0  0 \\\\\n     0  -1  1  0 \\\\\n     0  0  -1  1\n     \\end{bmatrix}$\n   - $i = 3$, $j = 0$\n5. 情况 E（由单位正则化稳定的秩亏正演算子）：\n   - $G = \\begin{bmatrix}\n     1  1 \\\\\n     2  2\n     \\end{bmatrix}$\n   - $C_d = I_2$\n   - $L = I_2$\n   - $i = 0$, $j = 1$\n6. 情况 F（强单位正则化抑制分辨率）：\n   - $G = I_2$\n   - $C_d = I_2$\n   - $L = 100\\, I_2$\n   - $i = 1$, $j = 1$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含按顺序对应于情况 A-F 的六个标量结果，形式为逗号分隔的列表并用方括号括起来，例如，“[rA,rB,rC,rD,rE,rF]”。每个条目必须是一个实数。",
            "solution": "我们从给定的二次目标函数开始\n$$\nJ(m) = \\| C_d^{-1/2} (G m - d) \\|_2^2 + \\| L m \\|_2^2,\n$$\n其中 $G \\in \\mathbb{R}^{m \\times n}$，$C_d \\in \\mathbb{R}^{m \\times m}$ 是对称正定矩阵，$L \\in \\mathbb{R}^{r \\times n}$ 是正则化算子。最小化子 $\\hat{m}$ 满足一阶最优性条件，该条件通过将 $J(m)$ 对 $m$ 的梯度设为零而获得。使用二次型的标准性质，\n$$\n\\nabla_m J(m) = 2 G^\\top C_d^{-1} (G m - d) + 2 L^\\top (L m).\n$$\n令 $\\nabla_m J(\\hat{m}) = 0$ 得到正规方程组\n$$\n\\left(G^\\top C_d^{-1} G + L^\\top L \\right) \\hat{m} = G^\\top C_d^{-1} d.\n$$\n定义对称正定矩阵\n$$\nA \\equiv G^\\top C_d^{-1} G + L^\\top L \\in \\mathbb{R}^{n \\times n},\n$$\n在给定假设下，该矩阵是可逆的，因为即使 $G^\\top C_d^{-1} G$ 是秩亏的，$L^\\top L$ 也提供了稳定性；而如果 $G^\\top C_d^{-1} G$ 本身就是正定的，则不需要 $L^\\top L$。于是\n$$\n\\hat{m} = A^{-1} G^\\top C_d^{-1} d.\n$$\n为了分析模型分辨率，我们考虑理想的无噪声情况，其中对于某个固定的真实模型 $m_{\\text{true}}$ 有 $d = G m_{\\text{true}}$。代入 $d$，\n$$\n\\hat{m} = A^{-1} G^\\top C_d^{-1} G \\, m_{\\text{true}}.\n$$\n这表明估计量是 $m_{\\text{true}}$ 的一个线性映射。因此，模型分辨率矩阵定义为\n$$\nR \\equiv A^{-1} G^\\top C_d^{-1} G = \\left(G^\\top C_d^{-1} G + L^\\top L\\right)^{-1} G^\\top C_d^{-1} G \\in \\mathbb{R}^{n \\times n}.\n$$\n根据构造，\n$$\n\\hat{m} = R \\, m_{\\text{true}}.\n$$\n为了解释串扰，考虑在真实模型的第 $j$ 个分量上施加一个单位扰动，同时保持所有其他分量为零：\n$$\nm_{\\text{true}} = e_j,\n$$\n其中 $e_j$ 是 $\\mathbb{R}^{n}$ 中的第 $j$ 个标准基向量。则\n$$\n\\hat{m} = R e_j,\n$$\n并且估计的第 $i$ 个分量变为\n$$\n\\hat{m}_i = e_i^\\top \\hat{m} = e_i^\\top R e_j = R_{ij}.\n$$\n因此，从参数 $j$ 到估计参数 $i$ 的串扰系数恰好是矩阵元 $R_{ij}$。具体来说，$R_{ij}$ 量化了 $\\hat{m}_i$ 对 $m_j$ 处单位扰动的响应。\n\n算法计算过程如下：\n1. 给定 $G$、$C_d$ 和 $L$，通过对对称正定矩阵 $C_d$ 求逆来构成 $C_d^{-1}$。\n2. 构成 $A = G^\\top C_d^{-1} G + L^\\top L$ 和 $B = G^\\top C_d^{-1} G$。\n3. 使用稳定的线性求解器求解线性矩阵方程\n$$\nA R = B\n$$\n以得到 $R$；这等效于 $R = A^{-1} B$，但无需显式计算 $A$ 的逆。\n4. 在请求的从零开始的索引 $(i, j)$ 处提取标量 $R_{ij}$。\n\n测试套件讨论：\n- 在情况 A 中，有 $G = I_3$，$C_d = I_3$，以及 $L = 0_{3 \\times 3}$，我们得到 $A = I_3$ 和 $B = I_3$，因此 $R = I_3$。对角线上的响应 $R_{11}$ 等于 $1$。\n- 在情况 B 中，使用相同的算子，非对角线上的元素 $R_{02}$ 等于 $0$。\n- 在情况 C 中，正演算子 $G$ 耦合了相邻参数，一阶差分正则化 $L$ 惩罚了粗糙度，导致了一个非平凡的 $R$，其对角线和非对角线上都有影响；$R_{12}$ 捕捉了从参数 $2$ 到估计分量 $1$ 的串扰。\n- 在情况 D 中，即使 $G = I_4$，各向异性的数据协方差 $C_d$ 也会不等地加权数据，并且粗糙度惩罚 $L$ 通过 $L^\\top L$ 耦合参数，从而产生非对角线的分辨率项，例如 $R_{30}$。\n- 在情况 E 中，$G$ 是秩亏的，但通过 $L = I_2$ 的稳定化产生了一个良定义的 $R$；值 $R_{01}$ 表明第二个真实参数如何影响第一个估计参数。\n- 在情况 F 中，有强正则化 $L = 100 I_2$，我们有 $A \\approx 10000 I_2 + I_2$，所以 $R \\approx (10001 I_2)^{-1} I_2$，使得对角线元素接近 $1/10001$，非对角线元素接近零，这反映了分辨率被强烈抑制。\n\n该程序实现了上述步骤，计算了情况 A-F 的六个所请求的串扰系数 $R_{ij}$，并以逗号分隔的列表形式，用方括号括起来，在单行上打印它们。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_resolution_entry(G: np.ndarray, Cd: np.ndarray, L: np.ndarray, i: int, j: int) - float:\n    \"\"\"\n    Compute the cross-talk coefficient R_{ij} for the regularized least-squares estimator:\n        R = (G^T Cd^{-1} G + L^T L)^{-1} G^T Cd^{-1} G\n    Returns the scalar entry at zero-based indices (i, j).\n    \"\"\"\n    # Invert data covariance (assumed symmetric positive definite)\n    Cd_inv = np.linalg.inv(Cd)\n\n    # Assemble A and B\n    GT_CdInv = G.T @ Cd_inv\n    B = GT_CdInv @ G\n    A = B + (L.T @ L)\n\n    # Solve A R = B for R without explicitly inverting A\n    R = np.linalg.solve(A, B)\n\n    return float(R[i, j])\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Case A: Identity resolution, on-diagonal\n    G_A = np.eye(3)\n    Cd_A = np.eye(3)\n    L_A = np.zeros((3, 3))\n    i_A, j_A = 1, 1\n\n    # Case B: Identity resolution, off-diagonal\n    G_B = np.eye(3)\n    Cd_B = np.eye(3)\n    L_B = np.zeros((3, 3))\n    i_B, j_B = 0, 2\n\n    # Case C: Blurred forward operator with first-difference regularization\n    G_C = np.array([\n        [1.0, 0.5, 0.0, 0.0],\n        [0.5, 1.0, 0.5, 0.0],\n        [0.0, 0.5, 1.0, 0.5],\n        [0.0, 0.0, 0.5, 1.0]\n    ])\n    Cd_C = np.eye(4)\n    L_C = np.array([\n        [-1.0,  1.0,  0.0,  0.0],\n        [ 0.0, -1.0,  1.0,  0.0],\n        [ 0.0,  0.0, -1.0,  1.0]\n    ])\n    i_C, j_C = 1, 2\n\n    # Case D: Identity forward operator, anisotropic Cd, first-difference L\n    G_D = np.eye(4)\n    Cd_D = np.diag([0.04, 1.0, 0.25, 9.0])\n    L_D = np.array([\n        [-1.0,  1.0,  0.0,  0.0],\n        [ 0.0, -1.0,  1.0,  0.0],\n        [ 0.0,  0.0, -1.0,  1.0]\n    ])\n    i_D, j_D = 3, 0\n\n    # Case E: Rank-deficient G, stabilized by identity L\n    G_E = np.array([\n        [1.0, 1.0],\n        [2.0, 2.0]\n    ])\n    Cd_E = np.eye(2)\n    L_E = np.eye(2)\n    i_E, j_E = 0, 1\n\n    # Case F: Strong identity regularization suppressing resolution\n    G_F = np.eye(2)\n    Cd_F = np.eye(2)\n    L_F = 100.0 * np.eye(2)\n    i_F, j_F = 1, 1\n\n    test_cases = [\n        (G_A, Cd_A, L_A, i_A, j_A),\n        (G_B, Cd_B, L_B, i_B, j_B),\n        (G_C, Cd_C, L_C, i_C, j_C),\n        (G_D, Cd_D, L_D, i_D, j_D),\n        (G_E, Cd_E, L_E, i_E, j_E),\n        (G_F, Cd_F, L_F, i_F, j_F),\n    ]\n\n    results = []\n    for G, Cd, L, i, j in test_cases:\n        res_ij = compute_resolution_entry(G, Cd, L, i, j)\n        results.append(res_ij)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "在理解了单个元素间的串扰之后，我们可以进一步评估对单个模型参数的整体分辨率。模型分辨率矩阵的一列（或在对称情况下的一行）被称为点扩散函数（PSF），它展示了真实模型中的一个点状特征在反演结果中是如何被“模糊”或“扩散”的。这个练习  的重点是计算点扩散函数，并使用其二阶中心矩来量化其宽度，从而得到一个衡量分辨率好坏的直观指标。",
            "id": "3403453",
            "problem": "考虑一个一维离散线性反演问题，该问题定义在一个包含 $N$ 个未知数 $\\{m_k\\}_{k=0}^{N-1}$ 的均匀网格上，网格间距为单位长度。设正演算子为单位算子，因此在无噪声的情况下，数据等于模型。并假设数据协方差为单位矩阵。一个带有一阶差分稳定器的标准Tikhonov正则化估计器，通过最小化一个二次目标函数来求解 $\\hat{m}$，该目标函数惩罚由一阶差分算子度量的模型粗糙度。模型分辨率矩阵编码了在无噪声数据的期望下，真实模型如何映射到估计模型。索引 $i$ 处的点扩散函数是模型分辨率矩阵的第 $i$ 行；它量化了索引 $i$ 处的一个单位脉冲信号被估计器模糊化的程度。你的任务是推导、计算并量化这种点扩散函数的宽度，将其表示为正则化强度的函数。\n\n从Tikhonov正则化和模型分辨率矩阵的基本定义出发，执行以下操作：\n\n- 推导模型分辨率矩阵，针对正演算子和数据协方差均为单位矩阵的特殊情况。正则化算子 $L$ 是一阶差分算子，其元素为 $(L)_{r,r}=-1$ 和 $(L)_{r,r+1}=1$（对于 $r \\in \\{0,\\dots,N-2\\}$），所有其他元素为零。不要先验地假设分辨率矩阵的任何表达式；应从定义Tikhonov估计器的变分公式以及分辨率矩阵在无噪声情况下从真实模型到期望估计的映射这一定义出发。\n\n- 解释如何计算第 $i$ 个点扩散函数（定义为模型分辨率矩阵的第 $i$ 行），要求不显式构建完整的逆矩阵，仅使用线性代数恒等式和系统的对称性质。\n\n- 使用二阶中心矩为以索引 $i$ 为中心的离散点扩散函数 $p=\\{p_k\\}_{k=0}^{N-1}$ 定义一个定量宽度度量。具体来说，设 $\\alpha = \\sum_{k=0}^{N-1} p_k$，定义方差 $\\sigma^2=\\frac{1}{\\alpha}\\sum_{k=0}^{N-1} (k-i)^2 p_k$，宽度为 $w=2\\sqrt{\\sigma^2}$。所有距离都以网格索引为单位，此处为无量纲单位。\n\n然后，实现一个程序，对于下面测试套件中的每一组参数，构建 $L$，计算给定索引 $i$ 和正则化参数 $\\lambda$ 对应的点扩散函数，并输出其宽度 $w(\\lambda)$（如上定义）。使用网格索引为从 $0$ 到 $N-1$ 的整数的约定。\n\n测试套件（每个元组为 $(N,i,\\lambda)$）：\n\n- 情况A（内部索引，平滑度增加）：\n  - $(N,i,\\lambda)=(51,25,0)$\n  - $(N,i,\\lambda)=(51,25,0.2)$\n  - $(N,i,\\lambda)=(51,25,1.0)$\n  - $(N,i,\\lambda)=(51,25,5.0)$\n- 情况B（左边界索引）：\n  - $(N,i,\\lambda)=(51,0,0)$\n  - $(N,i,\\lambda)=(51,0,1.0)$\n  - $(N,i,\\lambda)=(51,0,5.0)$\n- 情况C（不同网格尺寸，内部索引）：\n  - $(N,i,\\lambda)=(21,10,0)$\n  - $(N,i,\\lambda)=(21,10,0.5)$\n  - $(N,i,\\lambda)=(21,10,2.0)$\n\n你的程序应生成单行输出，其中包含按上述顺序列出的10个计算出的宽度，四舍五入到6位小数，格式为用方括号括起来的逗号分隔列表（例如，$[0.000000,0.123456,\\dots]$）。不应打印任何其他文本。所有计算都是无量纲的，不需要物理单位。不涉及角度。",
            "solution": "任务是推导并计算一个Tikhonov正则化反演问题的模型点扩散函数（PSF）的宽度。解决方案包括三个主要阶段：首先，从基本原理推导模型分辨率矩阵；其次，描述一种计算单个PSF（分辨率矩阵的一行）的高效方法；第三，量化计算出的PSF的宽度。\n\n**1. 模型分辨率矩阵 ($R$) 的推导**\n\n模型向量 $m$ 的Tikhonov正则化估计 $\\hat{m}$ 是使二次目标函数 $J(m)$ 最小化的向量：\n$$\nJ(m) = \\| Gm - d \\|^2_{C_d^{-1}} + \\lambda^2 \\| Lm \\|^2\n$$\n这里，$G$ 是正演算子，$d$ 是数据向量，$C_d$ 是数据协方差矩阵，$L$ 是正则化算子（或稳定器），$\\lambda$ 是在数据保真度与模型简单性（例如，平滑度）之间取得平衡的正则化参数。加权范数的平方定义为 $\\|v\\|^2_W = v^T W v$。\n\n问题指明正演算子为单位算子 $G=I$，数据协方差也为单位矩阵 $C_d=I$。将这些代入目标函数可得：\n$$\nJ(m) = \\| Im - d \\|^2_{I^{-1}} + \\lambda^2 \\| Lm \\|^2 = (m - d)^T (m - d) + \\lambda^2 (Lm)^T (Lm)\n$$\n展开此表达式，我们得到：\n$$\nJ(m) = m^T m - 2m^T d + d^T d + \\lambda^2 m^T L^T L m\n$$\n为了找到最小化 $J(m)$ 的模型估计 $\\hat{m}$，我们计算 $J(m)$ 关于 $m$ 的梯度，并将其设为零。梯度为：\n$$\n\\nabla_m J(m) = 2m - 2d + 2\\lambda^2 L^T L m\n$$\n令 $\\nabla_m J(m) = 0$ 可得：\n$$\n2\\hat{m} - 2d + 2\\lambda^2 L^T L \\hat{m} = 0\n$$\n$$\n(I + \\lambda^2 L^T L) \\hat{m} = d\n$$\n求解正则化估计 $\\hat{m}$，我们得到：\n$$\n\\hat{m} = (I + \\lambda^2 L^T L)^{-1} d\n$$\n模型分辨率矩阵 $R$ 由从真实模型 $m_{\\text{true}}$ 到估计模型期望值 $E[\\hat{m}]$ 的线性映射定义。在无噪声数据的条件下，观测数据向量 $d$ 就是 $d = G m_{\\text{true}}$。给定 $G=I$，这变为 $d = m_{\\text{true}}$。将此代入 $\\hat{m}$ 的表达式，得到无噪声情况下的关系式：\n$$\n\\hat{m} = (I + \\lambda^2 L^T L)^{-1} m_{\\text{true}}\n$$\n通过将其与定义 $\\hat{m} = R m_{\\text{true}}$进行比较，我们确定模型分辨率矩阵为：\n$$\nR = (I + \\lambda^2 L^T L)^{-1}\n$$\n这就是从变分形式推导出的模型分辨率矩阵的表达式。矩阵 $L$ 是 $(N-1) \\times N$ 的一阶差分算子，而 $I$ 是 $N \\times N$ 的单位矩阵。\n\n**2. 点扩散函数的计算**\n\n索引 $i$ 处的点扩散函数（PSF），记为 $p^{(i)}$，被定义为模型分辨率矩阵 $R$ 的第 $i$ 行。直接计算需要先构建矩阵 $A = I + \\lambda^2 L^T L$，然后计算其完整的逆矩阵 $R = A^{-1}$，这是一个复杂度为 $O(N^3)$ 的运算，对于大的 $N$ 来说计算效率低下。\n\n一个更高效的方法利用了矩阵 $A$ 的性质。矩阵 $L^T L$ 是对称的，因为 $(L^T L)^T = L^T (L^T)^T = L^T L$。因此，矩阵 $A = I + \\lambda^2 L^T L$ 也是对称的（$A^T=A$）。对称矩阵的逆矩阵也是对称的，所以 $R$ 是对称的（$R^T=R$）。\n\n$R$ 的第 $i$ 行可以写成 $e_i^T R$，其中 $e_i$ 是一个在索引 $i$ 处为1，其他位置为0的列向量（第 $i$ 个标准基向量）。由于 $R$ 的对称性，第 $i$ 行是第 $i$ 列的转置：\n$$\n(i\\text{-th row of } R) = e_i^T R = (R^T e_i)^T = (R e_i)^T\n$$\n向量 $R e_i$ 是 $R$ 的第 $i$ 列。设该列向量为 $x$。那么 $x = R e_i = A^{-1} e_i$。为了求 $x$，我们可以解等价的线性方程组：\n$$\nAx = e_i\n$$\n解向量 $x$ 包含第 $i$ 个PSF $p^{(i)}$ 的元素。这种求解单个线性系统的方法，其计算成本远低于对矩阵 $A$ 求逆。由于 $A$ 是对称正定（且是带状）矩阵，可以采用专门且高效的线性求解器，其复杂度通常接近 $O(N)$。\n\n因此，计算步骤如下：\n1. 构建 $(N-1) \\times N$ 的一阶差分矩阵 $L$。\n2. 构建 $N \\times N$ 矩阵 $A = I + \\lambda^2 L^T L$。\n3. 定义 $N \\times 1$ 的基向量 $e_i$。\n4. 求解线性方程组 $Ax = e_i$ 以获得PSF向量 $x$。\n\n**3. PSF宽度的量化**\n\n问题为以索引 $i$ 为中心的离散PSF $p = \\{p_k\\}_{k=0}^{N-1}$ 的宽度提供了一个特定的度量。该宽度基于 $p$ 中数值分布的二阶中心矩（方差）来定义。\n\n首先，我们计算PSF各分量的和：\n$$\n\\alpha = \\sum_{k=0}^{N-1} p_k\n$$\n这个和作为一个归一化因子。对于 $\\lambda=0$，$R=I$，所以 $p^{(i)} = e_i^T$ 且 $\\alpha=1$。对于 $\\lambda  0$，PSF会变宽，$\\alpha$ 可能会偏离1。\n\n接下来，方差 $\\sigma^2$ 被计算为与中心 $i$ 距离平方的加权平均值：\n$$\n\\sigma^2 = \\frac{1}{\\alpha} \\sum_{k=0}^{N-1} (k-i)^2 p_k\n$$\n最后，宽度 $w$ 定义为标准差的两倍：\n$$\nw = 2\\sqrt{\\sigma^2}\n$$\n距离 $(k-i)$ 的单位是无量纲的网格索引单位，因此得到的宽度 $w$ 也使用这些单位。在 $\\lambda=0$ 的特殊情况下，PSF是一个理想脉冲（$p_k = \\delta_{ik}$），导致 $\\sigma^2=0$，因此 $w=0$，表示完美的分辨率。随着 $\\lambda$ 的增加，正则化效应增强，PSF变宽，宽度 $w$ 预计会增加。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and prints the widths of point-spread functions for a series of test cases\n    in a Tikhonov-regularized inverse problem.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is (N, i, lambda), where:\n    # N is the number of grid points (size of the model vector).\n    # i is the index for which the point-spread function is computed.\n    # lambda is the regularization parameter.\n    test_cases = [\n        # Case A (interior index, increasing smoothing)\n        (51, 25, 0.0),\n        (51, 25, 0.2),\n        (51, 25, 1.0),\n        (51, 25, 5.0),\n        # Case B (left boundary index)\n        (51, 0, 0.0),\n        (51, 0, 1.0),\n        (51, 0, 5.0),\n        # Case C (different grid size, interior index)\n        (21, 10, 0.0),\n        (21, 10, 0.5),\n        (21, 10, 2.0),\n    ]\n\n    results = []\n    for N, i, lambda_reg in test_cases:\n        # For lambda = 0, the resolution matrix is the identity.\n        # The PSF is a Kronecker delta, so its width is exactly 0.\n        if lambda_reg == 0.0:\n            results.append(0.0)\n            continue\n            \n        # STEP 1: Construct the (N-1)xN first-difference operator L.\n        L = np.zeros((N - 1, N))\n        rows = np.arange(N - 1)\n        L[rows, rows] = -1.0\n        L[rows, rows + 1] = 1.0\n        \n        # STEP 2: Form the matrix A = I + lambda^2 * L^T * L.\n        # This matrix is symmetric and positive definite.\n        LTL = L.T @ L\n        A = np.identity(N) + (lambda_reg**2) * LTL\n        \n        # STEP 3: Define the standard basis vector e_i.\n        e_i = np.zeros(N)\n        e_i[i] = 1.0\n        \n        # STEP 4: Solve the linear system A*p = e_i to find the PSF p.\n        # np.linalg.solve is efficient for such systems.\n        p = np.linalg.solve(A, e_i)\n        \n        # STEP 5: Calculate the width of the PSF p.\n        # The width is defined as 2 * sqrt(variance).\n        \n        # Sum of the PSF components (normalization factor).\n        alpha = np.sum(p)\n\n        # Vector of grid indices [0, 1, ..., N-1].\n        k = np.arange(N)\n        \n        # Variance of the PSF around its center i.\n        # The sum is a weighted average of squared distances from the center.\n        variance = np.sum(((k - i)**2) * p) / alpha\n        \n        # The variance can be slightly negative due to floating-point inaccuracies\n        # if the true value is near zero. np.abs() provides robustness.\n        width = 2.0 * np.sqrt(np.abs(variance))\n        \n        results.append(width)\n\n    # Print the final results in the specified format:\n    # A single line, comma-separated list of widths, rounded to 6 decimal places.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "模型分辨率并非仅仅是评估器的抽象属性，它也直接受到输入数据的质量和配置的影响。这个更高级的练习  引入了数据分辨率矩阵 $H$（或称帽子矩阵）及其对角线元素（即杠杆值），这些值衡量了每个数据点对反演的贡献度。您将通过编程实践，探索高杠杆值数据点与模型分辨率提升（即更局域化的点扩散函数）之间的基本关系，从而为优化实验设计提供深刻见解。",
            "id": "3403493",
            "problem": "给定一个离散一维形式的线性反演问题。设 $m \\in \\mathbb{R}^n$ 为模型向量，$d \\in \\mathbb{R}^m$ 为数据向量，它们通过线性正演算子 $G \\in \\mathbb{R}^{m \\times n}$ 关联，关系式为 $d = G m + \\epsilon$，其中 $\\epsilon$ 是观测噪声，建模为一个协方差矩阵为 $C_d \\in \\mathbb{R}^{m \\times m}$ 的零均值高斯随机向量。考虑使用强度为 $\\lambda \\in \\mathbb{R}_{\\ge 0}$ 的一阶差分粗糙度正则化的带罚项加权最小二乘 (WLS) 估计量：最小化失配泛函\n$$\n\\Phi(m) = \\left\\| C_d^{-\\frac{1}{2}} \\left(d - G m\\right) \\right\\|_2^2 + \\lambda^2 \\left\\| L m \\right\\|_2^2,\n$$\n其中 $L \\in \\mathbb{R}^{(n-1)\\times n}$ 是一阶差分算子，定义为 $(L m)_k = m_{k+1} - m_k$，对于 $k = 0, \\dots, n-2$。令相应的法线矩阵为\n$$\nA = G^\\top C_d^{-1} G + \\lambda^2 L^\\top L.\n$$\n将数据分辨率矩阵 (DRM)，也称为帽子矩阵，定义为\n$$\nH = G A^{-1} G^\\top C_d^{-1},\n$$\n使得预测数据满足 $d_{\\text{pred}} = H d$。将模型分辨率矩阵 (MRM) 定义为\n$$\nR = A^{-1} G^\\top C_d^{-1} G,\n$$\n使得估计模型 $\\hat{m}$ 可写作 $\\hat{m} = R m_{\\text{true}} + A^{-1} G^\\top C_d^{-1} \\epsilon$，且 $R$ 的第 $j$ 列是模型分量 $m_j$ 的点扩散函数。\n\n您的任务是实现一个程序，该程序针对一个特定的矩阵 $G$，在多种情景下计算 $H$ 和 $R$，并定量分析高杠杆率数据点（$H$ 的较大对角线元素）如何转化为 $R$ 中的局域化或非局域化特征。\n\n使用以下基本构造。\n\n- 构建一个包含 $n=40$ 个模型参数的均匀网格，其位置为 $x_j = \\frac{j}{n-1}$（$j = 0, \\dots, n-1$），以及一个包含 $m=40$ 个数据位置的均匀网格，其位置为 $c_i = \\frac{i}{m-1}$（$i = 0, \\dots, m-1$）。\n\n- 通过局域化高斯灵敏度核定义 $G \\in \\mathbb{R}^{m \\times n}$：\n$$\nG_{ij} = \\exp\\!\\left( - \\frac{(x_j - c_i)^2}{2 s^2} \\right), \\quad \\text{其中 } s = 0.08.\n$$\n将 $G$ 的每一行归一化为单位欧几里得范数，以分离数据噪声方差对杠杆率的影响。\n\n- 定义一阶差分算子 $L \\in \\mathbb{R}^{(n-1)\\times n}$ 为 $(L m)_k = m_{k+1} - m_k$。将 $L$ 实现为一个带状矩阵，其主对角线上为 $-1$，上对角线上为 $+1$。\n\n- 对于对角数据协方差 $C_d = \\mathrm{diag}(\\sigma_1^2, \\dots, \\sigma_m^2)$，精确计算 $C_d^{-1}$ 为对角元素为 $\\sigma_i^{-2}$ 的对角矩阵。\n\n定义以下定量度量。\n\n- 杠杆率：对于每个数据索引 $i \\in \\{0,\\dots,m-1\\}$，杠杆率 $\\ell_i$ 是对角元素 $H_{ii}$。\n\n- 点扩散函数 (PSF) 宽度：对于每个模型索引 $j \\in \\{0,\\dots,n-1\\}$，将 PSF 定义为 $R$ 的第 $j$ 列经绝对值归一化后的向量 $p^{(j)} \\in \\mathbb{R}^n$，其分量为\n$$\np^{(j)}_k = \\frac{|R_{k j}|}{\\sum_{u=0}^{n-1} |R_{u j}|},\n$$\n并定义第 $j$ 个 PSF 的局域化宽度为其关于名义中心 $j$ 的离散标准差：\n$$\nw_j = \\sqrt{ \\sum_{k=0}^{n-1} p^{(j)}_k \\, (k - j)^2 }.\n$$\n\n实现以下情景测试套件。在所有情景中，使用与上文定义的相同的 $G$ 和 $L$。对于任何需要来自另一情景的值进行参考比较的情况，您的程序必须在内部计算并存储所需的参考值。\n\n- 测试 1 (基线): $\\lambda = 0.5$ 且对所有 $i$ 有 $\\sigma_i = 1.0$。计算平均 PSF 宽度 $\\overline{w} = \\frac{1}{n} \\sum_{j=0}^{n-1} w_j$。此测试的输出是一个等于 $\\overline{w}$ 的浮点数。\n\n- 测试 2 (单个高精度数据点): $\\lambda = 0.5$ 且对所有 $i$ 有 $\\sigma_i = 1.0$，除了单个数据索引 $k = 19$，此时 $\\sigma_{k} = 0.01$。令 $j_0$ 为使第 $k$ 行灵敏度最大化的模型索引，即 $j_0 = \\operatorname*{arg\\,max}_{0 \\le j  n} G_{k j}$。计算此情景下的 PSF 宽度 $w_{j_0}$，并将其与测试 1 中相同列的基线宽度 $w^{\\mathrm{base}}_{j_0}$ 进行比较。此测试的输出是一个布尔值，当且仅当 $w_{j_0}  w^{\\mathrm{base}}_{j_0}$ 时为真。\n\n- 测试 3 (使用相同高精度数据点的强正则化): $\\lambda = 10.0$ 且 $\\sigma_i$ 与测试 2 中相同。计算同一列 $j_0$ 处的 PSF 宽度，记为 $w^{\\mathrm{strong}}_{j_0}$，并将其与测试 2 中的宽度 $w^{\\mathrm{hi}}_{j_0}$ 进行比较。此测试的输出是一个布尔值，当且仅当 $w^{\\mathrm{strong}}_{j_0} > w^{\\mathrm{hi}}_{j_0}$ 时为真。\n\n- 测试 4 (弱正则化，均匀噪声): $\\lambda = 0.01$ 且对所有 $i$ 有 $\\sigma_i = 1.0$。计算此情景下的平均杠杆率 $\\bar{\\ell} = \\frac{1}{m} \\sum_{i=0}^{m-1} H_{ii}$。此测试的输出是一个等于 $\\bar{\\ell}$ 的浮点数。\n\n您的程序必须生成单行输出，其中包含测试 1 到 4 的结果，格式为方括号内以逗号分隔的列表，顺序为 $[\\overline{w}, \\text{来自测试 2 的布尔值}, \\text{来自测试 3 的布尔值}, \\bar{\\ell}]$。布尔值必须根据您语言的布尔字符串表示形式打印为字面文本 `true` 或 `false`；在 Python 中，它们将显示为 `True` 或 `False`。不涉及物理单位，也不出现角度。所有浮点输出都应以双精度计算；无需四舍五入。",
            "solution": "该求解过程首先设置常数矩阵和参数，然后按顺序执行所描述的四个测试情景。\n\n首先，我们定义基本参数和网格。模型参数的数量为 $n=40$，数据点的数量为 $m=40$。模型参数位于均匀网格 $x_j = \\frac{j}{n-1}$ 上（$j = 0, \\dots, n-1$），数据位于 $c_i = \\frac{i}{m-1}$ 上（$i = 0, \\dots, m-1$）。这些被实现为范围从 0 到 1 的数组。\n\n正演算子 $G \\in \\mathbb{R}^{m \\times n}$ 是使用高斯灵敏度核构建的。元素 $G_{ij}$ 由下式给出：\n$$\nG_{ij} = \\exp\\!\\left( - \\frac{(x_j - c_i)^2}{2 s^2} \\right)\n$$\n其中 $s = 0.08$。根据问题陈述，该矩阵的每一行随后被归一化为单位欧几里得范数。这种归一化确保了在考虑其噪声方差之前，每个数据点都具有同等的内在能力来影响解。\n\n一阶差分算子 $L \\in \\mathbb{R}^{(n-1)\\times n}$ 用于计算 $m_{k+1} - m_k$。这是一个稀疏矩阵，其元素为 $L_{k,k} = -1$ 和 $L_{k, k+1} = 1$（$k=0, \\dots, n-2$），其他位置为零。\n\n问题的核心是求解带罚项的加权最小二乘 (WLS) 问题。解 $\\hat{m}$ 最小化以下泛函：\n$$\n\\Phi(m) = (d - G m)^\\top C_d^{-1} (d - Gm) + \\lambda^2 (L m)^\\top (L m)\n$$\n该最小化问题的法线方程产生估计量 $\\hat{m} = (G^\\top C_d^{-1} G + \\lambda^2 L^\\top L)^{-1} G^\\top C_d^{-1} d$。我们将法线矩阵确定为 $A = G^\\top C_d^{-1} G + \\lambda^2 L^\\top L$。当 $\\lambda > 0$ 时，$A$ 的可逆性得到保证，因为 $G^\\top C_d^{-1} G$ 和 $L^\\top L$ 都是半正定矩阵，并且对于此问题结构，它们的零空间不会完全重叠。\n\n由此，我们可以定义模型分辨率矩阵 (MRM) $R$ 和数据分辨率矩阵 (DRM) $H$：\n$$\nR = A^{-1} G^\\top C_d^{-1} G\n$$\n$$\nH = G A^{-1} G^\\top C_d^{-1}\n$$\nMRM $R$ 描述了真实模型 $m_{\\text{true}}$ 如何映射到估计模型 $\\hat{m}$，因为 $\\hat{m}$ 是 $m_{\\text{true}}$ 的模糊版本加上一个噪声项。$R$ 的第 $j$ 列是第 $j$ 个模型参数的点扩散函数 (PSF)，显示了在 $m_j$ 处的一个类狄拉克δ函数特征将如何被重建。DRM $H$ 将观测数据 $d$ 映射到预测数据 $d_{\\text{pred}} = H d$。其对角线元素 $H_{ii}$ 称为杠杆率，用于衡量数据点 $d_i$ 对其自身预测值的影响。\n\n问题要求计算两个定量度量：\n1.  **PSF 宽度 ($w_j$)**：这衡量了第 $j$ 个 PSF 的局域化程度。它被计算为模型索引的加权标准差，其中权重来源于 $R$ 的第 $j$ 列元素的绝对值。\n    $$\n    w_j = \\sqrt{ \\sum_{k=0}^{n-1} p^{(j)}_k \\, (k - j)^2 } \\quad \\text{其中} \\quad p^{(j)}_k = \\frac{|R_{k j}|}{\\sum_{u=0}^{n-1} |R_{u j}|}\n    $$\n2.  **杠杆率 ($\\ell_i$)**：这仅仅是对角线元素 $H_{ii}$。\n\n现在执行四个测试用例：\n\n**测试 1 (基线):**\n我们设置正则化强度 $\\lambda=0.5$ 并假设均匀的数据不确定性，即对所有 $i$ 有 $\\sigma_i = 1.0$。这意味着 $C_d$ 是单位矩阵 $I$。我们计算 MRM $R_{\\text{base}}$，并为每个模型参数 $j=0, \\dots, n-1$ 计算其 PSF 宽度 $w_j$。此测试的最终结果是这些宽度的平均值 $\\overline{w}$。\n\n**测试 2 (单个高精度数据点):**\n我们保持 $\\lambda=0.5$ 不变，但引入一个非常精确的数据点。第 $k=19$ (0-indexed) 个数据点的不确定性设置为 $\\sigma_{19}=0.01$，而所有其他数据点仍为 $\\sigma_i=1.0$。这使得 $C_d^{-1}$ 的 $(19,19)$ 元素非常大，从而在反演中赋予该数据点很高的权重。我们首先确定对第 $k=19$ 个数据点最敏感的模型参数索引 $j_0$，即 $j_0 = \\operatorname*{arg\\,max}_{j} G_{19,j}$。然后我们计算新的 MRM $R_{\\text{hi}}$ 和相应的 PSF 宽度 $w^{\\text{hi}}_{j_0}$。将此宽度与使用测试 1 中的 $R_{\\text{base}}$ 计算的相同模型参数的基线宽度 $w^{\\text{base}}_{j_0}$ 进行比较。该测试检查是否 $w^{\\text{hi}}_{j_0}  w^{\\text{base}}_{j_0}$，这将表明提高一个位于战略位置的数据点的精度可以改善（局域化）受影响最大的模型参数的分辨率。\n\n**测试 3 (强正则化):**\n在这里，我们将正则化强度大幅增加到 $\\lambda=10.0$，同时保持与测试 2 相同的非均匀噪声模型。大的 $\\lambda$ 会严重惩罚粗糙模型，迫使解变得平滑。我们计算新的 MRM $R_{\\text{strong}}$ 和相同索引 $j_0$ 处的 PSF 宽度 $w^{\\text{strong}}_{j_0}$。该测试检查是否 $w^{\\text{strong}}_{j_0} > w^{\\text{hi}}_{j_0}$。这探究了数据驱动的局域化与正则化引起的平滑化之间的权衡；我们预计强平滑会使 PSF 非局域化，从而增加其宽度。\n\n**测试 4 (弱正则化):**\n最后，我们研究一个具有非常弱的正则化 ($\\lambda=0.01$) 和均匀噪声 ($\\sigma_i=1.0$) 的情况。此情景近似于一个广义逆解。我们计算 DRM $H_{\\text{weak}}$，并计算平均杠杆率 $\\bar{\\ell} = \\frac{1}{m} \\sum_{i=0}^{m-1} (H_{\\text{weak}})_{ii}$。在无正则化 ($\\lambda=0$) 且 $G$ 具有满列秩的极限情况下，帽子矩阵的一个属性是杠杆率之和等于模型参数的数量，即 $\\mathrm{Tr}(H) = n$。当 $m=n=40$ 时，平均杠杆率将为 $\\frac{n}{m} = 1.0$。弱正则化应该会产生一个接近此理论值的结果。\n\n实现了一个 Python 程序来数值执行这些计算。所有矩阵运算都利用 `numpy` 库。收集四个测试的结果并格式化为所需的输出字符串。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef setup_base_matrices(n, m, s):\n    \"\"\"\n    Constructs the scenario-independent matrices G and L.\n    \n    Args:\n        n (int): Number of model parameters.\n        m (int): Number of data points.\n        s (float): Width of the Gaussian kernels in G.\n        \n    Returns:\n        tuple[np.ndarray, np.ndarray]: The normalized G matrix and the L matrix.\n    \"\"\"\n    # Construct grids\n    x = np.linspace(0, 1, n)\n    c = np.linspace(0, 1, m)\n    \n    # Construct G matrix with Gaussian kernels\n    G_raw = np.exp(-((x[None, :] - c[:, None])**2) / (2 * s**2))\n    \n    # Normalize each row of G to unit Euclidean norm\n    row_norms = np.linalg.norm(G_raw, axis=1, keepdims=True)\n    # Avoid division by zero if a row is all zeros (unlikely here)\n    row_norms[row_norms == 0] = 1.0\n    G = G_raw / row_norms\n    \n    # Construct L matrix (first-difference operator)\n    L = np.zeros((n - 1, n))\n    L_main_diag = -np.ones(n - 1)\n    L_super_diag = np.ones(n - 1)\n    np.fill_diagonal(L, L_main_diag)\n    np.fill_diagonal(L[:, 1:], L_super_diag)\n    \n    return G, L\n\ndef compute_analysis_matrices(G, L, lambda_val, sigma_vec):\n    \"\"\"\n    Computes the normal, data resolution (H), and model resolution (R) matrices.\n    \n    Args:\n        G (np.ndarray): The forward operator.\n        L (np.ndarray): The regularization operator.\n        lambda_val (float): The regularization strength.\n        sigma_vec (np.ndarray): Vector of data standard deviations.\n        \n    Returns:\n        tuple[np.ndarray, np.ndarray]: The H and R matrices.\n    \"\"\"\n    m, n = G.shape\n    \n    # Construct inverse of data covariance matrix\n    # Using element-wise operation for efficiency\n    C_d_inv_diag = 1.0 / (sigma_vec**2)\n    # G.T @ (C_d_inv_diag[:, None] * G) is more efficient than G.T @ diag @ G\n    GT_Cd_inv_G = G.T @ (C_d_inv_diag[:, None] * G)\n    \n    # Construct normal matrix A\n    A = GT_Cd_inv_G + lambda_val**2 * (L.T @ L)\n    \n    # Invert A\n    try:\n        A_inv = np.linalg.inv(A)\n    except np.linalg.LinAlgError:\n        # For very small lambda, A can be near-singular. Add a small ridge.\n        # This is a robust alternative to direct inversion for ill-conditioned A.\n        A_inv = np.linalg.inv(A + np.eye(n) * 1e-15)\n\n    # Compute Model Resolution Matrix (R)\n    R = A_inv @ GT_Cd_inv_G\n    \n    # Compute Data Resolution Matrix (H)\n    # H = G @ A_inv @ G.T @ C_d_inv\n    # More efficient calculation:\n    H = G @ (A_inv @ (G.T * C_d_inv_diag[None, :]))\n\n    return H, R\n\ndef calculate_psf_width(R, j):\n    \"\"\"\n    Calculates the point-spread function width for the j-th model parameter.\n    \n    Args:\n        R (np.ndarray): The model resolution matrix.\n        j (int): The 0-based index of the model parameter.\n        \n    Returns:\n        float: The PSF width.\n    \"\"\"\n    n = R.shape[0]\n    R_col_j = R[:, j]\n    \n    # Normalize the j-th column by sum of absolute values\n    sum_abs_R = np.sum(np.abs(R_col_j))\n    if sum_abs_R == 0:\n        return 0.0\n    p_j = np.abs(R_col_j) / sum_abs_R\n    \n    # Calculate width\n    k_indices = np.arange(n)\n    width_sq = np.sum(p_j * (k_indices - j)**2)\n    \n    return np.sqrt(width_sq)\n\ndef solve():\n    \"\"\"\n    Main function to execute the four tests and print the results.\n    \"\"\"\n    # --- Setup ---\n    n, m, s = 40, 40, 0.08\n    G, L = setup_base_matrices(n, m, s)\n    \n    results = []\n    \n    # --- Test 1: Baseline ---\n    lambda1 = 0.5\n    sigma1 = np.ones(m)\n    _, R_base = compute_analysis_matrices(G, L, lambda1, sigma1)\n    \n    widths_base = [calculate_psf_width(R_base, j) for j in range(n)]\n    mean_w_base = np.mean(widths_base)\n    results.append(mean_w_base)\n\n    # --- Test 2: Single high-precision datum ---\n    lambda2 = 0.5\n    k_idx = 19  # 0-based index 19\n    j0 = np.argmax(G[k_idx, :])\n    \n    sigma2 = np.ones(m)\n    sigma2[k_idx] = 0.01\n    \n    _, R_hi = compute_analysis_matrices(G, L, lambda2, sigma2)\n    \n    w_base_j0 = calculate_psf_width(R_base, j0)\n    w_hi_j0 = calculate_psf_width(R_hi, j0)\n    \n    test2_result = w_hi_j0  w_base_j0\n    results.append(test2_result)\n\n    # --- Test 3: Strong regularization ---\n    lambda3 = 10.0\n    sigma3 = sigma2 # Same noise model as Test 2\n    \n    _, R_strong = compute_analysis_matrices(G, L, lambda3, sigma3)\n    \n    w_strong_j0 = calculate_psf_width(R_strong, j0)\n    \n    test3_result = w_strong_j0 > w_hi_j0\n    results.append(test3_result)\n    \n    # --- Test 4: Weak regularization ---\n    lambda4 = 0.01\n    sigma4 = np.ones(m)\n    H_weak, _ = compute_analysis_matrices(G, L,lambda4, sigma4)\n    \n    mean_leverage = np.mean(np.diag(H_weak))\n    results.append(mean_leverage)\n\n    # --- Final Output ---\n    # Python's str(bool) produces 'True' or 'False' as required.\n    print(f\"[{results[0]},{results[1]},{results[2]},{results[3]}]\")\n\nsolve()\n```"
        }
    ]
}