{
    "hands_on_practices": [
        {
            "introduction": "在对形状本身进行优化之前，理解代价函数如何随模型中的辅助参数变化是很有启发性的。本练习将探讨代价函数相对于平滑参数 $\\epsilon$ 和幅度参数 $\\theta$ 的敏感性，这是一个在基于梯度的优化中应用链式法则的直接而重要的实践。通过这个练习，您将学会如何推导和计算这些在模型设置中至关重要的参数的梯度 。",
            "id": "3396660",
            "problem": "考虑一个使用水平集方法（LSM）的二维形状重建问题。一个水平集函数 $\\phi:\\mathbb{R}^2\\rightarrow\\mathbb{R}$ 通过其零水平集来表示一个界面。我们假设一个由 $\\phi(\\mathbf{x}) = \\|\\mathbf{x}\\| - r$ 定义的径向对称界面，其中 $\\mathbf{x} = (x,y)$，$r$ 是一个正半径参数。为了将界面映射到材料属性，我们使用一个平滑的亥维赛德函数 $H_\\epsilon:\\mathbb{R}\\rightarrow\\mathbb{R}$，其平滑参数 $\\epsilon > 0$，定义如下\n$$\nH_\\epsilon(s) = \\frac{1}{2} + \\frac{1}{\\pi}\\arctan\\left(\\frac{s}{\\epsilon}\\right).\n$$\n预测的属性场被建模为 $u(\\mathbf{x};\\epsilon,\\theta,r) = \\theta\\,H_\\epsilon(\\phi(\\mathbf{x}))$，其中 $\\theta$ 是一个标量振幅参数。假设在网格点 $\\mathbf{x}_i$ 处收集测量值 $y_i$，并将其建模为具有参数 $r^\\star$, $\\epsilon^\\star$, $\\theta^\\star$ 的真实界面 $\\phi^\\star(\\mathbf{x})=\\|\\mathbf{x}\\| - r^\\star$ 的带噪观测值。测量模型为\n$$\ny_i = \\theta^\\star H_{\\epsilon^\\star}(\\phi^\\star(\\mathbf{x}_i)) + \\eta_i,\n$$\n其中 $\\eta_i$ 是独立同分布的零均值高斯随机变量，标准差为 $\\sigma$，通过一个固定的伪随机种子使其确定化。\n\n我们考虑对 $\\theta$ 进行 Tikhonov 正则化的标准最小二乘数据失配：\n$$\nJ(\\epsilon,\\theta;r) = \\frac{1}{2}\\sum_{i=1}^M\\left(u(\\mathbf{x}_i;\\epsilon,\\theta,r) - y_i\\right)^2 + \\frac{\\lambda}{2}\\,\\theta^2,\n$$\n其中 $\\lambda>0$ 是正则化权重，$M$ 是网格点的数量，所有求和都在固定的测量网格上进行。目标是研究可微性，并计算 $J$ 关于平滑参数 $\\epsilon$ 和振幅参数 $\\theta$ 的梯度，在每个测试案例中保持 $r$ 固定。假设 $\\epsilon>0$ 且 $\\theta\\in\\mathbb{R}$。\n\n从水平集方法和平滑亥维赛德函数的核心定义出发，并使用微分链式法则，推导出 $\\frac{\\partial J}{\\partial \\epsilon}$ 和 $\\frac{\\partial J}{\\partial \\theta}$ 关于 $\\epsilon$、$\\theta$、$r$ 以及观测数据 $(\\mathbf{x}_i,y_i)$ 的表达式。然后实现一个程序，为指定的测量网格和测试套件计算这些梯度。\n\n使用以下测量设置：\n- 域 $\\Omega = [-1,1]\\times[-1,1]$。\n- 一个 $N\\times N$ 点的均匀笛卡尔网格，其中每个维度 $N=61$，即 $M=N^2$ 个点 $\\mathbf{x}_i$。\n- 真实参数：$r^\\star = 0.45$, $\\epsilon^\\star = 0.02$, $\\theta^\\star = 0.9$。\n- 噪声标准差：$\\sigma = 0.01$，伪随机种子固定为 $0$。\n- 真实水平集：$\\phi^\\star(\\mathbf{x}) = \\|\\mathbf{x}\\| - r^\\star$。\n- 测量模型：$y_i=\\theta^\\star H_{\\epsilon^\\star}(\\phi^\\star(\\mathbf{x}_i)) + \\eta_i$，如上所述。\n\n对于每个测试案例 $(\\epsilon,\\theta,r,\\lambda)$，使用推导出的公式计算并返回序对 $\\left[\\frac{\\partial J}{\\partial \\epsilon},\\frac{\\partial J}{\\partial \\theta}\\right]$。\n\n提供包含以下参数集的测试套件：\n- 测试 $1$: $(\\epsilon,\\theta,r,\\lambda) = (0.05,1.0,0.5,0.01)$。\n- 测试 $2$: $(\\epsilon,\\theta,r,\\lambda) = (1\\times 10^{-4},0.8,0.5,0.01)$。\n- 测试 $3$: $(\\epsilon,\\theta,r,\\lambda) = (0.5,1.2,0.7,0.05)$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，每个测试案例的结果是一个包含两个浮点数的子列表，浮点数四舍五入到六位小数。最终输出格式必须为\n```\n\"[[dJ_depsilon_1,dJ_dtheta_1],[dJ_depsilon_2,dJ_dtheta_2],[dJ_depsilon_3,dJ_dtheta_3]]\"\n```\n每个浮点数四舍五入到六位小数，且无附加文本。",
            "solution": "该问题是有效的，因为它在逆问题领域具有科学依据，是适定的、客观的，并提供了一套完整且一致的定义和数据。因此，我们可以进行推导和求解。\n\n目标是推导代价泛函 $J(\\epsilon,\\theta;r)$ 关于平滑参数 $\\epsilon$ 和振幅参数 $\\theta$ 的偏导数的解析表达式。\n\n代价泛函由下式给出：\n$$\nJ(\\epsilon,\\theta;r) = \\frac{1}{2}\\sum_{i=1}^M\\left(u(\\mathbf{x}_i;\\epsilon,\\theta,r) - y_i\\right)^2 + \\frac{\\lambda}{2}\\,\\theta^2\n$$\n其中 $M$ 是测量点的总数。预测的属性场 $u(\\mathbf{x};\\epsilon,\\theta,r)$ 定义为：\n$$\nu(\\mathbf{x};\\epsilon,\\theta,r) = \\theta\\,H_\\epsilon(\\phi(\\mathbf{x}))\n$$\n对于具有半径 $r$ 的特定测试案例，水平集函数为 $\\phi(\\mathbf{x}) = \\|\\mathbf{x}\\| - r$。平滑亥维赛德函数 $H_\\epsilon(s)$ 为：\n$$\nH_\\epsilon(s) = \\frac{1}{2} + \\frac{1}{\\pi}\\arctan\\left(\\frac{s}{\\epsilon}\\right)\n$$\n\n### 关于 $\\theta$ 的梯度推导\n\n为了求得 $J$ 关于 $\\theta$ 的偏导数，记为 $\\frac{\\partial J}{\\partial \\theta}$，我们逐项对代价泛函进行微分。\n$$\n\\frac{\\partial J}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left[ \\frac{1}{2}\\sum_{i=1}^M\\left(u(\\mathbf{x}_i;\\epsilon,\\theta,r) - y_i\\right)^2 \\right] + \\frac{\\partial}{\\partial \\theta} \\left[ \\frac{\\lambda}{2}\\,\\theta^2 \\right]\n$$\n对求和项应用链式法则，我们得到：\n$$\n\\frac{\\partial J}{\\partial \\theta} = \\sum_{i=1}^M \\left(u(\\mathbf{x}_i;\\epsilon,\\theta,r) - y_i\\right) \\frac{\\partial u(\\mathbf{x}_i;\\epsilon,\\theta,r)}{\\partial \\theta} + \\lambda\\theta\n$$\n接下来，我们求模型 $u$ 关于 $\\theta$ 的偏导数：\n$$\n\\frac{\\partial u(\\mathbf{x}_i;\\epsilon,\\theta,r)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left[ \\theta\\,H_\\epsilon(\\phi(\\mathbf{x}_i)) \\right] = H_\\epsilon(\\phi(\\mathbf{x}_i))\n$$\n因为 $H_\\epsilon(\\phi(\\mathbf{x}_i))$ 不依赖于 $\\theta$。将此结果代回 $\\frac{\\partial J}{\\partial \\theta}$ 的表达式中，得到最终公式：\n$$\n\\frac{\\partial J}{\\partial \\theta} = \\sum_{i=1}^M \\left(u(\\mathbf{x}_i;\\epsilon,\\theta,r) - y_i\\right) H_\\epsilon(\\phi(\\mathbf{x}_i)) + \\lambda\\theta\n$$\n为了实现，我们可以代入 $u$ 的定义：\n$$\n\\frac{\\partial J}{\\partial \\theta} = \\sum_{i=1}^M \\left(\\theta\\,H_\\epsilon(\\phi(\\mathbf{x}_i)) - y_i\\right) H_\\epsilon(\\phi(\\mathbf{x}_i)) + \\lambda\\theta\n$$\n\n### 关于 $\\epsilon$ 的梯度推导\n\n为了求得 $J$ 关于 $\\epsilon$ 的偏导数，记为 $\\frac{\\partial J}{\\partial \\epsilon}$，我们对代价泛函进行微分。Tikhonov 正则化项 $\\frac{\\lambda}{2}\\theta^2$ 不依赖于 $\\epsilon$，因此其导数为 $0$。\n$$\n\\frac{\\partial J}{\\partial \\epsilon} = \\frac{\\partial}{\\partial \\epsilon} \\left[ \\frac{1}{2}\\sum_{i=1}^M\\left(u(\\mathbf{x}_i;\\epsilon,\\theta,r) - y_i\\right)^2 \\right] = \\sum_{i=1}^M \\left(u(\\mathbf{x}_i;\\epsilon,\\theta,r) - y_i\\right) \\frac{\\partial u(\\mathbf{x}_i;\\epsilon,\\theta,r)}{\\partial \\epsilon}\n$$\n我们现在必须求模型 $u$ 关于 $\\epsilon$ 的偏导数：\n$$\n\\frac{\\partial u(\\mathbf{x}_i;\\epsilon,\\theta,r)}{\\partial \\epsilon} = \\frac{\\partial}{\\partial \\epsilon} \\left[ \\theta\\,H_\\epsilon(\\phi(\\mathbf{x}_i)) \\right] = \\theta \\frac{\\partial H_\\epsilon(s_i)}{\\partial \\epsilon}\n$$\n为简化符号，我们令 $s_i = \\phi(\\mathbf{x}_i)$。平滑亥维赛德函数 $H_\\epsilon(s)$ 关于 $\\epsilon$ 的导数使用链式法则求得。$\\arctan(z)$ 关于 $z$ 的导数是 $\\frac{1}{1+z^2}$。\n$$\n\\frac{\\partial H_\\epsilon(s)}{\\partial \\epsilon} = \\frac{\\partial}{\\partial \\epsilon}\\left[ \\frac{1}{2} + \\frac{1}{\\pi}\\arctan\\left(\\frac{s}{\\epsilon}\\right) \\right] = \\frac{1}{\\pi} \\frac{\\partial}{\\partial \\epsilon}\\left[\\arctan\\left(\\frac{s}{\\epsilon}\\right)\\right]\n$$\n$$\n\\frac{\\partial H_\\epsilon(s)}{\\partial \\epsilon} = \\frac{1}{\\pi} \\left( \\frac{1}{1 + (s/\\epsilon)^2} \\right) \\cdot \\frac{\\partial}{\\partial \\epsilon}\\left(\\frac{s}{\\epsilon}\\right) = \\frac{1}{\\pi} \\left( \\frac{\\epsilon^2}{\\epsilon^2 + s^2} \\right) \\cdot \\left( -\\frac{s}{\\epsilon^2} \\right) = -\\frac{s}{\\pi(\\epsilon^2 + s^2)}\n$$\n将此结果代回在每个点 $\\mathbf{x}_i$ 处 $\\frac{\\partial u}{\\partial \\epsilon}$ 的表达式中：\n$$\n\\frac{\\partial u(\\mathbf{x}_i;\\epsilon,\\theta,r)}{\\partial \\epsilon} = \\theta \\left( -\\frac{\\phi(\\mathbf{x}_i)}{\\pi(\\epsilon^2 + \\phi(\\mathbf{x}_i)^2)} \\right)\n$$\n最后，我们得到 $\\frac{\\partial J}{\\partial \\epsilon}$ 的表达式：\n$$\n\\frac{\\partial J}{\\partial \\epsilon} = \\sum_{i=1}^M \\left(u(\\mathbf{x}_i;\\epsilon,\\theta,r) - y_i\\right) \\left( -\\frac{\\theta \\phi(\\mathbf{x}_i)}{\\pi(\\epsilon^2 + \\phi(\\mathbf{x}_i)^2)} \\right)\n$$\n同样，为了实现，代入 $u$ 的定义：\n$$\n\\frac{\\partial J}{\\partial \\epsilon} = \\sum_{i=1}^M \\left(\\theta H_\\epsilon(\\phi(\\mathbf{x}_i)) - y_i\\right) \\left( -\\frac{\\theta \\phi(\\mathbf{x}_i)}{\\pi(\\epsilon^2 + \\phi(\\mathbf{x}_i)^2)} \\right)\n$$\n\n这些推导出的 $\\frac{\\partial J}{\\partial \\theta}$ 和 $\\frac{\\partial J}{\\partial \\epsilon}$ 的表达式在以下程序中实现。该程序首先根据真实参数生成合成测量数据 $y_i$。然后，对每个测试案例，它使用推导出的解析公式计算梯度。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of computing gradients of a least-squares functional\n    in a level-set based inverse problem.\n    \"\"\"\n\n    # --- Measurement Setup ---\n    # Domain and grid\n    N = 61\n    domain = [-1.0, 1.0]\n\n    # Ground truth parameters\n    r_star = 0.45\n    eps_star = 0.02\n    theta_star = 0.9\n\n    # Noise parameters\n    sigma = 0.01\n    seed = 0\n\n    # Test cases from the problem statement\n    test_cases = [\n        # (epsilon, theta, r, lambda)\n        (0.05, 1.0, 0.5, 0.01),\n        (1e-4, 0.8, 0.5, 0.01),\n        (0.5, 1.2, 0.7, 0.05),\n    ]\n\n    def smoothed_heaviside(s, epsilon):\n        \"\"\"Computes the smoothed Heaviside function H_epsilon(s).\"\"\"\n        return 0.5 + (1.0 / np.pi) * np.arctan(s / epsilon)\n\n    def generate_measurement_data():\n        \"\"\"Generates the grid and synthetic measurement data y_i.\"\"\"\n        # Create a uniform Cartesian grid\n        lin_space = np.linspace(domain[0], domain[1], N)\n        xx, yy = np.meshgrid(lin_space, lin_space)\n        grid_points = np.vstack([xx.ravel(), yy.ravel()]).T # Shape (M, 2) where M=N*N\n\n        # Compute ground truth level set function values at grid points\n        phi_star_vals = np.linalg.norm(grid_points, axis=1) - r_star\n\n        # Compute true property field without noise\n        u_true = theta_star * smoothed_heaviside(phi_star_vals, eps_star)\n\n        # Generate deterministic pseudo-random noise\n        rng = np.random.default_rng(seed)\n        noise = rng.normal(loc=0.0, scale=sigma, size=N * N)\n\n        # Generate noisy measurement data\n        y_data = u_true + noise\n        \n        return grid_points, y_data\n\n    def compute_gradients(params, grid_points, y_data):\n        \"\"\"\n        Computes the gradients dJ/d(epsilon) and dJ/d(theta) for a given set of parameters.\n        \"\"\"\n        epsilon, theta, r, lam = params\n        \n        # Calculate level set values for the current test case\n        phi_vals = np.linalg.norm(grid_points, axis=1) - r\n        \n        # Calculate smoothed Heaviside values for the current test case\n        H_eps_vals = smoothed_heaviside(phi_vals, epsilon)\n        \n        # Calculate the predicted property field u\n        u_vals = theta * H_eps_vals\n        \n        # Calculate the residual (u_i - y_i)\n        residual = u_vals - y_data\n        \n        # --- Compute Gradient with respect to theta ---\n        # dJ/d(theta) = sum_i( (u_i - y_i) * d(u_i)/d(theta) ) + lambda * theta\n        # d(u_i)/d(theta) = H_epsilon(phi_i)\n        grad_theta = np.sum(residual * H_eps_vals) + lam * theta\n        \n        # --- Compute Gradient with respect to epsilon ---\n        # dJ/d(epsilon) = sum_i( (u_i - y_i) * d(u_i)/d(epsilon) )\n        # d(u_i)/d(epsilon) = theta * d(H_epsilon)/d(epsilon)\n        # d(H_epsilon)/d(epsilon) = -phi_i / (pi * (epsilon^2 + phi_i^2))\n        \n        dH_deps_vals = -phi_vals / (np.pi * (epsilon**2 + phi_vals**2))\n        du_deps_vals = theta * dH_deps_vals\n        grad_epsilon = np.sum(residual * du_deps_vals)\n        \n        return [grad_epsilon, grad_theta]\n\n    # Generate the measurement data once for all test cases\n    grid_points, y_data = generate_measurement_data()\n    \n    # Process all test cases\n    results_str_list = []\n    for params in test_cases:\n        gradients = compute_gradients(params, grid_points, y_data)\n        \n        # Format the output for each test case as a string \"[val1,val2]\"\n        # with values rounded to six decimal places.\n        formatted_grads = [f\"{g:.6f}\" for g in gradients]\n        sublist_str = f\"[{','.join(formatted_grads)}]\"\n        results_str_list.append(sublist_str)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "基于梯度的方法是形状优化的核心，但当问题受偏微分方程（PDE）约束时，高效计算形状梯度是一项重大挑战。本练习将引导您推导并实现强大的伴随状态法（adjoint-state method），这是一种能够高效计算大规模反问题中梯度的标准技术。掌握伴随法对于任何严肃的PDE约束优化从业者来说都是必不可少的 。",
            "id": "3396641",
            "problem": "考虑一个二维方形域 $\\Omega = [0,1] \\times [0,1]$，其边界条件为齐次狄利克雷边界条件。一个分段常数扩散系数 $a(\\phi)$ 依赖于一个水平集函数 $\\phi:\\Omega \\to \\mathbb{R}$，该函数表示一个未知形状 $\\mathcal{D} = \\{ x \\in \\Omega \\mid \\phi(x)  0 \\}$。正向模型是变系数泊松方程\n$$\n-\\nabla \\cdot \\big(a(\\phi)\\,\\nabla u\\big) = f \\quad \\text{in } \\Omega, \\qquad u = 0 \\quad \\text{on } \\partial \\Omega,\n$$\n其中 $f:\\Omega \\to \\mathbb{R}$ 是一个已知源项。数据同化目标是通过优化水平集函数 $\\phi$ 来重建形状 $\\mathcal{D}$，以最小化数据失配泛函\n$$\nJ(\\phi) = \\tfrac{1}{2}\\int_{\\Omega} \\big(u(\\phi) - u_{\\mathrm{obs}}\\big)^2 \\,\\mathrm{d}x,\n$$\n其中 $u_{\\mathrm{obs}}:\\Omega \\to \\mathbb{R}$ 是从一个不同的真实形状 $\\phi_{\\mathrm{true}}$ 生成的观测状态。系数 $a(\\phi)$ 使用平滑的亥维赛德函数定义，以确保其可微性，\n$$\na(\\phi) = a_{\\mathrm{out}} + \\big(a_{\\mathrm{in}} - a_{\\mathrm{out}}\\big)\\,H_{\\varepsilon}(-\\phi),\n$$\n其中\n$$\nH_{\\varepsilon}(s) = \\tfrac{1}{2} + \\frac{1}{\\pi}\\arctan\\!\\Big(\\frac{s}{\\varepsilon}\\Big),\n$$\n这里 $a_{\\mathrm{in}}  0$ 和 $a_{\\mathrm{out}}  0$ 是常数，$\\varepsilon  0$ 是一个平滑参数。与 $H_{\\varepsilon}$ 相关联的平滑狄拉克δ函数是\n$$\n\\delta_{\\varepsilon}(\\phi) = \\frac{1}{\\pi}\\frac{\\varepsilon}{\\varepsilon^2 + \\phi^2}。\n$$\n目标是，从第一性原理出发，推导在水平集参数化下的伴随状态公式以及关于 $\\phi$ 的形状梯度。仅从正向模型的强形式、数据失配泛函的定义以及标准变分法出发，推导伴随方程和形状梯度 $G(\\phi)$ 的逐点表达式，使得对于任何适当光滑的扰动 $\\eta:\\Omega \\to \\mathbb{R}$，一阶方向导数满足\n$$\n\\mathrm{D}J(\\phi)[\\eta] = \\int_{\\Omega} G(\\phi)\\,\\eta \\,\\mathrm{d}x.\n$$\n然后，您必须实现一个程序，该程序：\n- 在具有 $N \\times N$ 个节点和网格间距 $h = 1/(N-1)$ 的均匀笛卡尔网格上离散化 $\\Omega$。\n- 使用 $a$ 的面心算术平均和齐次狄利克雷边界条件，以通量形式组装 $-\\nabla \\cdot \\big(a(\\phi)\\,\\nabla (\\cdot)\\big)$ 的离散算子。\n- 求解正向问题以计算 $u(\\phi)$。\n- 通过使用真实形状 $\\phi_{\\mathrm{true}}$ 求解相同的正向问题来构建 $u_{\\mathrm{obs}}$。\n- 求解由数据失配残差驱动的伴随问题以获得伴随变量。\n- 使用与水平集参数化和 $a(\\phi)$ 的链式法则一致的推导出的逐点表达式，在网格节点上计算形状梯度 $G(\\phi)$。\n- 通过将预测的方向导数 $\\int_{\\Omega} G(\\phi)\\,\\eta \\,\\mathrm{d}x$ 与小步长 $t$ 的有限差分近似 $\\big(J(\\phi + t\\,\\eta) - J(\\phi)\\big)/t$ 进行比较，来验证伴随状态恒等式。\n\n使用以下确定性规范：\n- 源项 $f(x,y) = 1$ 对所有 $(x,y) \\in \\Omega$ 成立。\n- 观测状态 $u_{\\mathrm{obs}}$ 是用一个由圆形给出的真实形状计算的：$\\phi_{\\mathrm{true}}(x,y) = \\sqrt{(x - 0.5)^2 + (y - 0.5)^2} - r_{\\mathrm{true}}$，其中 $r_{\\mathrm{true}} = 0.25$。\n- 扰动为 $\\eta(x,y) = \\sin(2\\pi x)\\sin(2\\pi y)$。\n- 初始水平集函数 $\\phi$ 要么是圆形 $\\phi(x,y) = \\sqrt{(x - 0.5)^2 + (y - 0.5)^2} - r_0$，要么是常数函数 $\\phi(x,y) \\equiv c_0$，具体由每个测试用例指定。\n- 平滑的亥维赛德函数是 $H_{\\varepsilon}(-\\phi)$，其关于 $\\phi$ 的导数是 $-\\delta_{\\varepsilon}(\\phi)$，其中 $\\delta_{\\varepsilon}(\\phi)$ 如上定义。\n- 内积和积分必须使用每个节点权重为 $h^2$ 的标准黎曼和在网格上进行近似。\n\n您的程序必须为每个测试用例计算伴随预测的方向导数与有限差分近似之间的相对一致性误差，\n$$\nE = \\frac{\\left|\\int_{\\Omega} G(\\phi)\\,\\eta \\,\\mathrm{d}x - \\frac{J(\\phi + t\\,\\eta) - J(\\phi)}{t}\\right|}{\\max\\!\\Big(\\lvert \\int_{\\Omega} G(\\phi)\\,\\eta \\,\\mathrm{d}x \\rvert + \\left\\lvert \\frac{J(\\phi + t\\,\\eta) - J(\\phi)}{t} \\right\\rvert, \\,10^{-12}\\Big)}。\n$$\n\n测试套件：\n- 用例 1：$N = 64$，$\\varepsilon = 0.02$， $t = 10^{-3}$，$a_{\\mathrm{in}} = 5$，$a_{\\mathrm{out}} = 1$，初始 $\\phi$ 是半径为 $r_0 = 0.35$ 的圆形。\n- 用例 2：$N = 64$，$\\varepsilon = 0.005$，$t = 10^{-3}$，$a_{\\mathrm{in}} = 5$，$a_{\\mathrm{out}} = 1$，初始 $\\phi$ 是半径为 $r_0 = 0.35$ 的圆形。\n- 用例 3：$N = 16$，$\\varepsilon = 0.02$，$t = 10^{-3}$，$a_{\\mathrm{in}} = 5$，$a_{\\mathrm{out}} = 1$，初始 $\\phi$ 是半径为 $r_0 = 0.35$ 的圆形。\n- 用例 4：$N = 64$，$\\varepsilon = 0.02$，$t = 10^{-4}$，$a_{\\mathrm{in}} = 5$，$a_{\\mathrm{out}} = 1$，初始 $\\phi$ 是常数 $c_0 = 0.5$（因此 $a(\\phi)$ 是均匀的），对于足够小的 $t$，这应该产生接近于零的方向导数。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含测试套件的四个相对误差，格式为方括号内以逗号分隔的列表（例如，$\\texttt{[e1,e2,e3,e4]}$）。不应打印任何其他文本。",
            "solution": "用户提供的问题是偏微分方程约束优化和反演问题领域中一个适定且科学合理的问题。它要求推导由变系数泊松方程控制的形状优化问题的伴随状态方程，然后通过数值实现来验证所推导的梯度。该问题是自洽的，所有参数、控制方程和数值规范都有明确定义。它遵循了变分法和数值分析的既定原则。因此，该问题被判定为**有效**。\n\n### 使用伴随方法推导形状梯度\n\n目标是求泛函 $J(\\phi)$ 关于水平集函数 $\\phi$ 的梯度。该泛函为\n$$\nJ(\\phi) = \\frac{1}{2}\\int_{\\Omega} \\big(u(\\phi) - u_{\\mathrm{obs}}\\big)^2 \\,\\mathrm{d}x,\n$$\n其中状态变量 $u(\\phi)$ 是正向问题的解：\n$$\n-\\nabla \\cdot \\big(a(\\phi)\\,\\nabla u\\big) = f \\quad \\text{in } \\Omega, \\qquad u = 0 \\quad \\text{on } \\partial \\Omega.\n$$\n正向问题的弱形式是找到 $u \\in H^1_0(\\Omega)$，使得对于所有测试函数 $v \\in H^1_0(\\Omega)$：\n$$\n\\int_{\\Omega} a(\\phi)\\,\\nabla u \\cdot \\nabla v \\,\\mathrm{d}x = \\int_{\\Omega} f\\,v \\,\\mathrm{d}x.\n$$\n\n我们寻求 $J(\\phi)$ 在任意方向 $\\eta$ 上的 Gâteaux 导数，其定义为\n$$\n\\mathrm{D}J(\\phi)[\\eta] = \\lim_{t \\to 0} \\frac{J(\\phi + t\\eta) - J(\\phi)}{t}.\n$$\n对 $J(\\phi)$ 的定义应用链式法则，我们得到：\n$$\n\\mathrm{D}J(\\phi)[\\eta] = \\int_{\\Omega} \\big(u(\\phi) - u_{\\mathrm{obs}}\\big)\\,\\dot{u} \\,\\mathrm{d}x,\n$$\n其中 $\\dot{u} = \\frac{\\mathrm{d}u}{\\mathrm{d}\\phi}[\\eta]$ 是状态 $u$ 关于 $\\phi$ 在方向 $\\eta$ 上的变化的灵敏度。\n\n为了找到 $\\dot{u}$ 的方程，我们将状态方程的弱形式对 $\\phi$ 在方向 $\\eta$ 上求导。右侧与 $\\phi$ 无关，因此其导数为零。对左侧求导得到：\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} \\left( \\int_{\\Omega} a(\\phi+t\\eta)\\,\\nabla u(\\phi+t\\eta) \\cdot \\nabla v \\,\\mathrm{d}x \\right) \\bigg|_{t=0} = 0.\n$$\n应用乘积法则，我们得到切线方程：\n$$\n\\int_{\\Omega} \\dot{a}\\,\\nabla u \\cdot \\nabla v \\,\\mathrm{d}x + \\int_{\\Omega} a(\\phi)\\,\\nabla\\dot{u} \\cdot \\nabla v \\,\\mathrm{d}x = 0,\n$$\n其中 $\\dot{a} = \\frac{\\mathrm{d}a}{\\mathrm{d}\\phi}[\\eta]$。项 $\\dot{a}$ 通过对 $a(\\phi)$ 的定义应用链式法则得到：\n$$\na(\\phi) = a_{\\mathrm{out}} + \\big(a_{\\mathrm{in}} - a_{\\mathrm{out}}\\big)\\,H_{\\varepsilon}(-\\phi).\n$$\n平滑亥维赛德函数 $H_{\\varepsilon}(s)$ 的导数是平滑狄拉克δ函数 $\\delta_{\\varepsilon}(s)$。根据规定，$H_{\\varepsilon}(-\\phi)$ 关于 $\\phi$ 的导数是 $-\\delta_{\\varepsilon}(\\phi)$，其中 $\\delta_{\\varepsilon}(\\phi) = \\frac{1}{\\pi}\\frac{\\varepsilon}{\\varepsilon^2 + \\phi^2}$。因此，\n$$\n\\frac{\\mathrm{d}a(\\phi)}{\\mathrm{d}\\phi} = (a_{\\mathrm{in}} - a_{\\mathrm{out}}) \\frac{\\mathrm{d}}{\\mathrm{d}\\phi} H_{\\varepsilon}(-\\phi) = -(a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi).\n$$\n这意味着 $\\dot{a} = \\frac{\\mathrm{d}a(\\phi)}{\\mathrm{d}\\phi} \\eta = -(a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,\\eta$。\n将此代入切线方程得到：\n$$\n\\int_{\\Omega} a(\\phi)\\,\\nabla\\dot{u} \\cdot \\nabla v \\,\\mathrm{d}x = \\int_{\\Omega} (a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,\\eta\\,(\\nabla u \\cdot \\nabla v) \\,\\mathrm{d}x.\n$$\n\n引入伴随方法以避免显式求解 $\\dot{u}$。我们将伴随状态 $p \\in H^1_0(\\Omega)$ 定义为以下伴随方程的解：找到 $p$，使得对于所有测试函数 $w \\in H^1_0(\\Omega)$，\n$$\n\\int_{\\Omega} a(\\phi)\\,\\nabla p \\cdot \\nabla w \\,\\mathrm{d}x = \\int_{\\Omega} (u - u_{\\mathrm{obs}})\\,w \\,\\mathrm{d}x.\n$$\n伴随方程的强形式为：\n$$\n-\\nabla \\cdot \\big(a(\\phi)\\,\\nabla p\\big) = u - u_{\\mathrm{obs}} \\quad \\text{in } \\Omega, \\qquad p = 0 \\quad \\text{on } \\partial\\Omega.\n$$\n注意微分算子是自伴的。伴随方程的源项是数据失配的残差。\n\n现在，我们策略性地在伴随方程的弱形式中选择测试函数 $w$ 为状态灵敏度，即 $w=\\dot{u}$。这给出：\n$$\n\\int_{\\Omega} a(\\phi)\\,\\nabla p \\cdot \\nabla \\dot{u} \\,\\mathrm{d}x = \\int_{\\Omega} (u - u_{\\mathrm{obs}})\\,\\dot{u} \\,\\mathrm{d}x.\n$$\n右侧正好是方向导数 $\\mathrm{D}J(\\phi)[\\eta]$ 的表达式。因此，\n$$\n\\mathrm{D}J(\\phi)[\\eta] = \\int_{\\Omega} a(\\phi)\\,\\nabla p \\cdot \\nabla \\dot{u} \\,\\mathrm{d}x.\n$$\n因为双线性形式是对称的，我们可以写成 $\\int_{\\Omega} a(\\phi)\\,\\nabla p \\cdot \\nabla \\dot{u} \\,\\mathrm{d}x = \\int_{\\Omega} a(\\phi)\\,\\nabla \\dot{u} \\cdot \\nabla p \\,\\mathrm{d}x$。\n我们现在可以使用切线方程，并选择测试函数为伴随状态，即 $v=p$：\n$$\n\\int_{\\Omega} a(\\phi)\\,\\nabla \\dot{u} \\cdot \\nabla p \\,\\mathrm{d}x = \\int_{\\Omega} (a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,\\eta\\,(\\nabla u \\cdot \\nabla p) \\,\\mathrm{d}x.\n$$\n通过令方向导数的表达式相等，我们得到最终形式：\n$$\n\\mathrm{D}J(\\phi)[\\eta] = \\int_{\\Omega} \\Big( (a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,(\\nabla u \\cdot \\nabla p) \\Big)\\,\\eta \\,\\mathrm{d}x.\n$$\n该表达式符合所需的形式 $\\mathrm{D}J(\\phi)[\\eta] = \\int_{\\Omega} G(\\phi)\\,\\eta \\,\\mathrm{d}x$。因此，我们可以确定逐点形状梯度 $G(\\phi)$ 为：\n$$\nG(\\phi) = (a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,(\\nabla u \\cdot \\nabla p).\n$$\n\n### 用于梯度计算的伴随状态系统总结\n1.  **求解正向问题以得到 $u$**：给定 $\\phi$，求解 $u$：\n    $$-\\nabla \\cdot (a(\\phi)\\nabla u) = f, \\quad u|_{\\partial\\Omega}=0.$$\n2.  **求解伴随问题以得到 $p$**：使用步骤1中的解 $u$，求解 $p$：\n    $$-\\nabla \\cdot (a(\\phi)\\nabla p) = u - u_{\\mathrm{obs}}, \\quad p|_{\\partial\\Omega}=0.$$\n3.  **计算梯度 $G(\\phi)$**：使用 $u$ 和 $p$，计算梯度：\n    $$G(\\phi) = (a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,(\\nabla u \\cdot \\nabla p).$$\n推导至此完成。接下来的 Python 代码以数值方式实现了这个三步过程。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import lil_matrix\nfrom scipy.sparse.linalg import spsolve\n\ndef solve():\n    \"\"\"\n    Main driver function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {'N': 64, 'eps': 0.02, 't': 1e-3, 'a_in': 5.0, 'a_out': 1.0, 'phi_type': 'circle', 'phi_param': 0.35},\n        # Case 2\n        {'N': 64, 'eps': 0.005, 't': 1e-3, 'a_in': 5.0, 'a_out': 1.0, 'phi_type': 'circle', 'phi_param': 0.35},\n        # Case 3\n        {'N': 16, 'eps': 0.02, 't': 1e-3, 'a_in': 5.0, 'a_out': 1.0, 'phi_type': 'circle', 'phi_param': 0.35},\n        # Case 4\n        {'N': 64, 'eps': 0.02, 't': 1e-4, 'a_in': 5.0, 'a_out': 1.0, 'phi_type': 'const', 'phi_param': 0.5},\n    ]\n\n    results = []\n    for params in test_cases:\n        error = run_case(**params)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10e}' for r in results)}]\")\n\ndef run_case(N, eps, t, a_in, a_out, phi_type, phi_param):\n    \"\"\"\n    Executes a single test case for the adjoint verification.\n    \"\"\"\n    h = 1.0 / (N - 1)\n    x = np.linspace(0, 1, N)\n    y = np.linspace(0, 1, N)\n    X, Y = np.meshgrid(x, y)\n\n    # Helper functions for level set, Heaviside, and delta\n    def get_phi(X, Y, type, param):\n        if type == 'circle':\n            radius = param\n            return np.sqrt((X - 0.5)**2 + (Y - 0.5)**2) - radius\n        elif type == 'const':\n            c0 = param\n            return np.full_like(X, c0)\n        \n    def H_eps(s, eps_val):\n        return 0.5 + (1.0 / np.pi) * np.arctan(s / eps_val)\n\n    def delta_eps(s, eps_val):\n        return (1.0 / np.pi) * eps_val / (eps_val**2 + s**2)\n\n    def get_a(phi, a_in_val, a_out_val, eps_val):\n        return a_out_val + (a_in_val - a_out_val) * H_eps(-phi, eps_val)\n\n    # --- Finite Difference Poisson Solver ---\n    def solve_poisson(a_coeff, rhs, h_val, N_val):\n        num_interior = (N_val - 2)**2\n        A = lil_matrix((num_interior, num_interior), dtype=np.float64)\n        \n        # Assemble matrix A using a 5-point stencil for -div(a grad(u))\n        for i in range(1, N_val - 1):\n            for j in range(1, N_val - 1):\n                k = (i - 1) * (N_val - 2) + (j - 1)\n\n                # Face-centered arithmetic means for 'a'\n                a_south = (a_coeff[i-1, j] + a_coeff[i, j]) / 2.0\n                a_north = (a_coeff[i+1, j] + a_coeff[i, j]) / 2.0\n                a_west  = (a_coeff[i, j-1] + a_coeff[i, j]) / 2.0\n                a_east  = (a_coeff[i, j+1] + a_coeff[i, j]) / 2.0\n                \n                A[k, k] = -(a_north + a_south + a_east + a_west)\n\n                if i  1:\n                    A[k, k - (N_val - 2)] = a_south # South neighbor\n                if i  N_val - 2:\n                    A[k, k + (N_val - 2)] = a_north # North neighbor\n                if j  1:\n                    A[k, k - 1] = a_west           # West neighbor\n                if j  N_val - 2:\n                    A[k, k + 1] = a_east           # East neighbor\n\n        A /= h_val**2\n        \n        # Flatten the right-hand side for interior nodes\n        b = rhs[1:-1, 1:-1].flatten()\n        \n        # Solve the linear system\n        u_interior = spsolve(A.tocsr(), b)\n        \n        # Embed solution back into the full grid with boundary conditions\n        u_full = np.zeros((N_val, N_val))\n        u_full[1:-1, 1:-1] = u_interior.reshape((N_val - 2, N_val - 2))\n        return u_full\n\n    # Function to compute the objective functional J\n    def compute_J(u, u_obs, h_val):\n        return 0.5 * np.sum((u - u_obs)**2) * h_val**2\n\n    # --- Main Calculation Steps ---\n    \n    # 1. Compute observed state u_obs\n    f_source = np.ones((N, N))\n    r_true = 0.25\n    phi_true = get_phi(X, Y, 'circle', r_true)\n    a_true = get_a(phi_true, a_in, a_out, eps)\n    u_obs = solve_poisson(a_true, f_source, h, N)\n    \n    # 2. Define initial state phi and perturbation eta\n    phi = get_phi(X, Y, phi_type, phi_param)\n    eta = np.sin(2 * np.pi * X) * np.sin(2 * np.pi * Y)\n\n    # 3. Solve forward problem for u(phi) and compute J(phi)\n    a_phi = get_a(phi, a_in, a_out, eps)\n    u_phi = solve_poisson(a_phi, f_source, h, N)\n    J_phi = compute_J(u_phi, u_obs, h)\n    \n    # 4. Compute finite difference approximation of the directional derivative\n    phi_pert = phi + t * eta\n    a_pert = get_a(phi_pert, a_in, a_out, eps)\n    u_pert = solve_poisson(a_pert, f_source, h, N)\n    J_pert = compute_J(u_pert, u_obs, h)\n    \n    DJ_fd = (J_pert - J_phi) / t\n\n    # 5. Compute adjoint-based directional derivative\n    # 5a. Solve adjoint equation: -div(a grad(p)) = u - u_obs\n    adjoint_rhs = u_phi - u_obs\n    p = solve_poisson(a_phi, adjoint_rhs, h, N)\n\n    # 5b. Compute gradients of u and p\n    grad_u_y, grad_u_x = np.gradient(u_phi, h)\n    grad_p_y, grad_p_x = np.gradient(p, h)\n    \n    # 5c. Compute the shape gradient G(phi)\n    grad_u_dot_grad_p = grad_u_x * grad_p_x + grad_u_y * grad_p_y\n    G_phi = (a_in - a_out) * delta_eps(phi, eps) * grad_u_dot_grad_p\n    \n    # 5d. Compute the directional derivative via inner product\n    DJ_adj = np.sum(G_phi * eta) * h**2\n    \n    # 6. Calculate the relative consistency error\n    numerator = np.abs(DJ_adj - DJ_fd)\n    denominator = max(np.abs(DJ_adj) + np.abs(DJ_fd), 1e-12)\n    error = numerator / denominator\n\n    return error\n\nif __name__ == \"__main__\":\n    solve()\n\n```"
        },
        {
            "introduction": "在学习了如何计算和使用形状梯度之后，一个自然的实际问题是如何平衡数据拟合与正则化。正则化参数 $\\alpha$ 的选择对重构结果的质量至关重要。本练习介绍了一种系统选择 $\\alpha$ 的经典方法——莫罗佐夫差异原理（Morozov discrepancy principle），它将优化算法置于一个寻找最佳超参数的更大框架中 。",
            "id": "3396597",
            "problem": "考虑一个形状重建反问题，其中矩形域中的一个未知子集被隐式地表示为一个标量场的零水平集。设该域为一个大小为 $N \\times N$ 的离散网格，其中 $N = 48$。一个形状由水平集函数 $\\phi : \\{1,\\dots,N\\} \\times \\{1,\\dots,N\\} \\to \\mathbb{R}$ 表示，使得重建的指示场为 $u = H_{\\varepsilon}(\\phi)$，其中 $H_{\\varepsilon}$ 是一个平滑的亥维赛德函数，平滑参数为 $\\varepsilon  0$。观测数据是通过一个已知的线性、对称的前向算子 $A$ 应用于 $u$ 生成的，并受到标准差为 $\\sigma$ 的加性独立高斯噪声的污染。重建问题被定为一个类吉洪诺夫泛函的最小化问题，该泛函包含一个在水平集框架下表示的周长先验。\n\n使用以下基本基础：\n- 通过亥维赛德函数 $H_{\\varepsilon}(\\phi)$ 及其平滑的狄拉克δ函数 $\\delta_{\\varepsilon}(\\phi) = H_{\\varepsilon}'(\\phi)$ 对二值形状进行水平集表示。\n- Morozov差异原则：选择正则化参数，使得数据失配在适当的范数下等于噪声水平。\n- 使用标准变分法，从能量泛函的变分导数获得的梯度流。\n\n设前向算子 $A$ 是一个标准差为 $\\sigma_{b} = 1.2$ 网格单位的高斯模糊，并采用反射边界条件。设平滑的亥维赛德函数和平滑的狄拉克δ函数定义为\n$$\nH_{\\varepsilon}(x) = \\tfrac{1}{2} + \\tfrac{1}{\\pi} \\arctan\\!\\left(\\tfrac{x}{\\varepsilon}\\right), \\quad\n\\delta_{\\varepsilon}(x) = \\tfrac{1}{\\pi} \\tfrac{\\varepsilon}{\\varepsilon^{2} + x^{2}},\n$$\n其中 $\\varepsilon = 1.0$。\n\n设真实形状为一个半径 $r_{\\mathrm{true}} = 12$ 像素、中心位于 $(c_x,c_y) = (24,30)$ 的圆盘，由符号距离水平集 $\\phi_{\\mathrm{true}}(i,j) = \\sqrt{(i-c_x)^2 + (j-c_y)^2} - r_{\\mathrm{true}}$ 表示。初始猜测为一个半径 $r_{0} = 10$ 像素、中心位于 $(c_{0x},c_{0y}) = (16,18)$ 的圆盘，即 $\\phi_{0}(i,j) = \\sqrt{(i-c_{0x})^2 + (j-c_{0y})^2} - r_{0}$。无噪声数据为 $y_{\\mathrm{clean}} = A(H_{\\varepsilon}(\\phi_{\\mathrm{true}}))$，观测数据为 $y = y_{\\mathrm{clean}} + \\eta$，其中 $\\eta$ 的各项是独立的、均值为零、标准差为 $\\sigma$ 的高斯分布。数据点的数量为 $m = N^{2}$。\n\n对于给定的正则化参数 $\\alpha  0$，考虑能量\n$$\nJ(\\phi;\\alpha) = \\tfrac{1}{2} \\lVert A(H_{\\varepsilon}(\\phi)) - y \\rVert_{2}^{2} + \\alpha \\int_{\\Omega} \\delta_{\\varepsilon}(\\phi) \\, \\lvert \\nabla \\phi \\rvert \\, \\mathrm{d}x,\n$$\n其中 $\\Omega$ 是离散网格，$\\lVert \\cdot \\rVert_{2}$ 是 $\\mathbb{R}^{m}$ 上的欧几里得范数。使用变分法和链式法则，针对 $\\phi$ 的、时间步长为 $\\Delta t$ 的梯度下降可以写成离散形式\n$$\n\\phi^{k+1} = \\phi^{k} + \\Delta t \\left( - \\delta_{\\varepsilon}(\\phi^{k}) \\cdot A^{\\top}\\!\\left( A(H_{\\varepsilon}(\\phi^{k})) - y \\right) + \\alpha \\, \\delta_{\\varepsilon}(\\phi^{k}) \\, \\kappa(\\phi^{k}) \\right),\n$$\n其中 $A^{\\top} = A$（高斯模糊的对称性），$\\kappa(\\phi)$ 是零水平集的曲率，由下式给出\n$$\n\\kappa(\\phi) = \\nabla \\cdot \\left( \\frac{\\nabla \\phi}{\\sqrt{\\lvert \\nabla \\phi \\rvert^{2} + \\beta}} \\right),\n$$\n其中有一个小的稳定项 $\\beta = 10^{-8}$。空间导数使用中心差分和反射边界条件进行近似。对于给定的 $\\alpha$，每次评估 $J$ 时使用 $\\Delta t = 0.2$ 并执行 $K = 80$ 次迭代。\n\nMorozov差异原则规定选择 $\\alpha$ 使得残差范数等于噪声水平（可相差一个乘法因子）：选择 $\\alpha$ 使得\n$$\n\\lVert A(H_{\\varepsilon}(\\phi_{\\alpha}^{\\star})) - y \\rVert_{2} \\approx \\tau \\, \\sigma \\, \\sqrt{m},\n$$\n其中 $\\phi_{\\alpha}^{\\star}$ 是从 $\\phi_{0}$ 开始经过 $K$ 次迭代后梯度下降的结果。对 $\\alpha$ 实现一个区间套定和二分法过程，使用初始区间 $[\\alpha_{\\min},\\alpha_{\\max}] = [10^{-6}, 1.0]$，不断将 $\\alpha_{\\max}$ 加倍，直到 $\\alpha_{\\max}$ 处的残差不小于目标差异，或直到 $\\alpha_{\\max}$ 超过 $10^{6}$。在二分法中使用相对容差 $\\rho = 10^{-2}$；如果无法为目标套定区间，则选择能产生最接近目标残差的端点。\n\n测试套件。对于以下每种情况，使用固定的伪随机种子 $s_{0} + i$ 生成噪声，其中 $s_{0} = 12345$，$i$ 是测试用例的从0开始的索引。对于每种情况，按照所述的Morozov差异原则计算选定的正则化参数 $\\alpha^{\\star}$。这些情况是：\n- 情况 1：$(\\sigma,\\tau) = (0.05, 1.0)$。\n- 情况 2：$(\\sigma,\\tau) = (0.01, 1.0)$。\n- 情况 3：$(\\sigma,\\tau) = (0.10, 1.1)$。\n- 情况 4：$(\\sigma,\\tau) = (0.08, 0.9)$。\n\n最终输出格式。您的程序应生成单行输出，其中包含每种情况下选定的 $\\alpha^{\\star}$，形式为方括号括起来的逗号分隔列表，每个数字四舍五入到6位小数（例如，“[0.123456,0.234567,0.345678,0.456789]”）。不应打印任何额外文本。所有角度（如有）均以弧度为单位测量。除了离散网格间距外，没有物理单位；请将所要求的值作为无量纲数报告。确保程序是完全确定性的，并且不需要任何用户输入。",
            "solution": "用户提供的问题是计算反问题领域一个适定的、自包含的练习，具体侧重于使用基于水平集的方法进行形状重建。该问题在科学上是合理的，所有组成部分——物理模型、数学表述和数值算法——都是标准的且定义清晰。任务是使用Morozov差异原则为四种不同情景确定最优正则化参数 $\\alpha$。这将通过为水平集函数实现梯度下降优化和对参数 $\\alpha$ 进行二分搜索来完成。\n\n首先，我们将问题形式化。未知形状由定义在大小为 $N \\times N$（其中 $N=48$）的离散网格 $\\Omega$ 上的函数 $\\phi(x,y)$ 的零水平集表示。形状的指示函数 $u$（在形状内部为1，外部为0）由一个平滑的亥维赛德函数 $H_{\\varepsilon}(\\phi)$ 近似。问题指定：\n$$\nH_{\\varepsilon}(x) = \\tfrac{1}{2} + \\tfrac{1}{\\pi} \\arctan\\!\\left(\\tfrac{x}{\\varepsilon}\\right)\n$$\n平滑宽度为 $\\varepsilon = 1.0$。该函数的导数是平滑的狄拉克δ函数 $\\delta_{\\varepsilon}(x) = H_{\\varepsilon}'(x)$，由下式给出：\n$$\n\\delta_{\\varepsilon}(x) = \\tfrac{1}{\\pi} \\frac{\\varepsilon}{\\varepsilon^{2} + x^{2}}\n$$\n该函数将计算局部化到界面附近（其中 $\\phi \\approx 0$）。\n\n数据 $y$ 是通过将一个线性前向算子 $A$ 应用于真实指示函数 $u_{\\mathrm{true}} = H_{\\varepsilon}(\\phi_{\\mathrm{true}})$ 并添加高斯噪声 $\\eta$ 而生成的。算子 $A$ 是一个标准差为 $\\sigma_b = 1.2$ 网格单位的高斯模糊，并且是对称的（$A=A^\\top$）。真实形状是一个半径 $r_{\\mathrm{true}} = 12$、中心位于 $(c_x, c_y) = (24, 30)$ 的圆盘，而重建的初始猜测是一个半径 $r_0 = 10$、中心位于 $(c_{0x}, c_{0y}) = (16, 18)$ 的圆盘。噪声 $\\eta$ 从均值为0、标准差为 $\\sigma$ 的高斯分布中抽取。\n\n重建的 $\\phi$ 是通过最小化类吉洪诺夫能量泛函 $J(\\phi;\\alpha)$ 找到的：\n$$\nJ(\\phi;\\alpha) = \\underbrace{\\tfrac{1}{2} \\lVert A(H_{\\varepsilon}(\\phi)) - y \\rVert_{2}^{2}}_{\\text{数据保真项}} + \\underbrace{\\alpha \\int_{\\Omega} \\delta_{\\varepsilon}(\\phi) \\, \\lvert \\nabla \\phi \\rvert \\, \\mathrm{d}x}_{\\text{周长正则化}}\n$$\n第一项确保重建的形状在模糊化后与观测数据 $y$ 匹配。第二项是周长惩罚项，它通过偏好具有较短边界长度的形状来对问题进行正则化，从而促进更平滑和更紧凑的重建。$\\alpha0$ 是平衡这两个相互竞争目标的正则化参数。\n\n为了最小化 $J(\\phi;\\alpha)$，我们使用梯度下降算法。$\\phi$ 在一个虚拟时间 $t$ 上的演化遵循能量的负梯度方向。问题提供了离散更新规则：\n$$\n\\phi^{k+1} = \\phi^{k} + \\Delta t \\left( - \\frac{\\delta J}{\\delta \\phi}(\\phi^k) \\right)\n$$\n其中使用了变分导数 $\\frac{\\delta J}{\\delta \\phi}$ 的一种标准的简化形式，从而得到更新方程：\n$$\n\\phi^{k+1} = \\phi^{k} + \\Delta t \\left( -\\delta_{\\varepsilon}(\\phi^{k}) A^{\\top}\\!\\left( A(H_{\\varepsilon}(\\phi^{k})) - y \\right) + \\alpha \\delta_{\\varepsilon}(\\phi^{k}) \\kappa(\\phi^{k}) \\right)\n$$\n这里，$\\Delta t = 0.2$ 是时间步长。括号内的第一项是数据驱动力，将水平集拉向与数据匹配的方向。第二项是几何正则化力，它根据其平均曲率 $\\kappa(\\phi)$ 移动水平集，从而有效地平滑边界。曲率由下式给出：\n$$\n\\kappa(\\phi) = \\nabla \\cdot \\left( \\frac{\\nabla \\phi}{\\sqrt{\\lvert \\nabla \\phi \\rvert^{2} + \\beta}} \\right)\n$$\n其中有一个小的稳定项 $\\beta=10^{-8}$。所有空间导数（梯度 $\\nabla$ 和散度 $\\nabla \\cdot$）都使用带反射边界条件的中心差分来近似，这是通过在微分前对网格进行填充来实现的。\n\n问题的核心是选择参数 $\\alpha$。Morozov差异原则被用于此目的。它规定正则化参数 $\\alpha$ 的选择应使得最优解 $\\phi^{\\star}_{\\alpha}$ 的数据残差的欧几里得范数与噪声的期望范数相匹配。目标被表述为：\n$$\n\\lVert A(H_{\\varepsilon}(\\phi_{\\alpha}^{\\star})) - y \\rVert_{2} = \\tau \\, \\sigma \\, \\sqrt{m}\n$$\n其中 $m = N^2 = 48^2$ 是数据点（像素）的总数，$\\tau$ 是一个接近1的给定因子。项 $\\sigma\\sqrt{m}$ 是噪声向量的期望$L_2$范数。对于每个给定的 $(\\sigma, \\tau)$，我们必须找到对应的 $\\alpha^{\\star}$。\n\n为了找到 $\\alpha^{\\star}$，我们定义一个函数 $D(\\alpha) = \\lVert A(H_{\\varepsilon}(\\phi_{\\alpha}^{\\star})) - y \\rVert_{2}$，其中 $\\phi_{\\alpha}^{\\star}$ 是使用参数 $\\alpha$ 运行梯度下降 $K=80$ 次迭代的结果。函数 $D(\\alpha)$ 通常随 $\\alpha$ 单调递增。寻找 $\\alpha^{\\star}$ 的问题简化为求解标量方程 $D(\\alpha) = \\tau \\sigma \\sqrt{m}$。这通过使用区间套定和二分算法进行数值求解：\n1.  **区间套定**：建立一个初始搜索区间 $[\\alpha_{\\min}, \\alpha_{\\max}] = [10^{-6}, 1.0]$。如果 $\\alpha_{\\max}$ 处的差异小于目标值，则重复将 $\\alpha_{\\max}$ 加倍，直到目标被套定在区间内（即 $D(\\alpha_{\\min})  \\text{目标值}  D(\\alpha_{\\max})$）或 $\\alpha_{\\max}$ 超过 $10^6$ 的上限。如果区间套定失败，则选择搜索范围中能产生最接近目标差异的端点。\n2.  **二分法**：一旦找到一个区间，就使用二分法来缩小该区间，迭代地将其减半，直到中点 $\\alpha_m$ 处的差异在目标值的相对容差 $\\rho=10^{-2}$ 范围内。\n\n每个测试用例的总体算法如下：\n1.  生成真实的符号距离函数 $\\phi_{\\mathrm{true}}$、真实指示函数 $u_{\\mathrm{true}}$ 和无噪声数据 $y_{\\mathrm{clean}} = A(u_{\\mathrm{true}})$。\n2.  使用指定的标准差 $\\sigma$ 和随机种子生成噪声向量 $\\eta$，并构成含噪数据 $y = y_{\\mathrm{clean}} + \\eta$。\n3.  计算目标差异 $T = \\tau \\sigma \\sqrt{m}$。\n4.  执行区间套定和二分搜索以找到 $\\alpha$ 的值 $\\alpha^{\\star}$，该值在指定的容差内满足 $D(\\alpha) = T$。\n5.  存储得到的 $\\alpha^{\\star}$ 并为下一个测试用例重复此过程。\n最后，所有四种情况下计算出的 $\\alpha^{\\star}$ 值将以指定格式报告。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import ndimage\n\ndef solve():\n    \"\"\"\n    Solves the level set shape reconstruction problem by finding the optimal\n    regularization parameter alpha using the Morozov discrepancy principle.\n    \"\"\"\n    \n    # Constants from the problem statement\n    N = 48\n    EPSILON = 1.0\n    SIGMA_B = 1.2\n    R_TRUE = 12.0\n    CX_TRUE, CY_TRUE = 23, 29  # 0-based from 1-based (24, 30)\n    R0 = 10.0\n    CX0, CY0 = 15, 17  # 0-based from 1-based (16, 18)\n    BETA = 1e-8\n    DT = 0.2\n    K = 80\n    ALPHA_MIN_INIT = 1e-6\n    ALPHA_MAX_INIT = 1.0\n    ALPHA_MAX_CAP = 1e6\n    RHO = 1e-2\n    S0 = 12345\n    M = N * N\n\n    # 1. Mathematical Functions\n    def smoothed_heaviside(x):\n        \"\"\" Smoothed Heaviside function H_epsilon(x). \"\"\"\n        return 0.5 + (1.0 / np.pi) * np.arctan(x / EPSILON)\n\n    def smoothed_delta(x):\n        \"\"\" Smoothed Dirac delta function delta_epsilon(x). \"\"\"\n        return (1.0 / np.pi) * (EPSILON / (EPSILON**2 + x**2))\n\n    def create_sdf_disk(n_grid, cx, cy, r):\n        \"\"\" Creates a signed distance function for a disk. \"\"\"\n        y_coords, x_coords = np.mgrid[0:n_grid, 0:n_grid]\n        return np.sqrt((x_coords - cx)**2 + (y_coords - cy)**2) - r\n\n    # 2. Operators and Derivatives\n    def forward_operator_A(u):\n        \"\"\" Forward operator A: Gaussian blur with reflective boundaries. \"\"\"\n        return ndimage.gaussian_filter(u, sigma=SIGMA_B, mode='reflect')\n\n    def gradient_cd_reflect(f):\n        \"\"\" Computes gradient (fy, fx) using central differences and reflective padding. \"\"\"\n        f_padded = np.pad(f, pad_width=1, mode='reflect')\n        fy = (f_padded[2:, 1:-1] - f_padded[:-2, 1:-1]) / 2.0\n        fx = (f_padded[1:-1, 2:] - f_padded[1:-1, :-2]) / 2.0\n        return fy, fx\n\n    def divergence_cd_reflect(fy, fx):\n        \"\"\" Computes divergence of a vector field (fy, fx) using central differences. \"\"\"\n        fy_padded = np.pad(fy, pad_width=1, mode='reflect')\n        fx_padded = np.pad(fx, pad_width=1, mode='reflect')\n        fyy = (fy_padded[2:, 1:-1] - fy_padded[:-2, 1:-1]) / 2.0\n        fxx = (fx_padded[1:-1, 2:] - fx_padded[1:-1, :-2]) / 2.0\n        return fxx + fyy\n\n    def curvature(phi):\n        \"\"\" Computes curvature kappa(phi) of the level set. \"\"\"\n        phi_y, phi_x = gradient_cd_reflect(phi)\n        grad_phi_norm = np.sqrt(phi_x**2 + phi_y**2 + BETA)\n        g_x = phi_x / grad_phi_norm\n        g_y = phi_y / grad_phi_norm\n        return divergence_cd_reflect(g_y, g_x)\n\n    # 3. Optimization and Parameter Search\n    def run_gradient_descent(phi_init, y, alpha):\n        \"\"\" Performs K iterations of gradient descent for a given alpha. \"\"\"\n        phi = phi_init.copy()\n        for _ in range(K):\n            u_k = smoothed_heaviside(phi)\n            delta_k = smoothed_delta(phi)\n            residual = forward_operator_A(u_k) - y\n            data_term_grad = -delta_k * forward_operator_A(residual)\n            kappa_k = curvature(phi)\n            reg_term_grad = alpha * delta_k * kappa_k\n            phi += DT * (data_term_grad + reg_term_grad)\n        return phi\n\n    def get_discrepancy(alpha, phi_init, y, memo):\n        \"\"\" Calculates the residual norm for a given alpha, with memoization. \"\"\"\n        if alpha in memo:\n            return memo[alpha]\n        phi_star = run_gradient_descent(phi_init, y, alpha)\n        u_star = smoothed_heaviside(phi_star)\n        discrepancy = np.linalg.norm(forward_operator_A(u_star) - y)\n        memo[alpha] = discrepancy\n        return discrepancy\n\n    def find_alpha_morozov(y, phi_0, sigma, tau):\n        \"\"\" Finds the optimal alpha using Morozov's principle with bisection. \"\"\"\n        target_discrepancy = tau * sigma * np.sqrt(M)\n        memo = {}\n        \n        alpha_min, alpha_max = ALPHA_MIN_INIT, ALPHA_MAX_INIT\n        d_min = get_discrepancy(alpha_min, phi_0, y, memo)\n        \n        if d_min  target_discrepancy:\n            return alpha_min\n\n        d_max = get_discrepancy(alpha_max, phi_0, y, memo)\n\n        while d_max  target_discrepancy:\n            alpha_max *= 2.0\n            if alpha_max  ALPHA_MAX_CAP:\n                return ALPHA_MAX_CAP\n            d_max = get_discrepancy(alpha_max, phi_0, y, memo)\n            \n        alpha_l, alpha_u = alpha_min, alpha_max\n        for _ in range(50):\n            alpha_m = (alpha_l + alpha_u) / 2.0\n            if alpha_m == alpha_l or alpha_m == alpha_u:\n                break\n            d_m = get_discrepancy(alpha_m, phi_0, y, memo)\n            if abs(d_m - target_discrepancy)  RHO * target_discrepancy:\n                return alpha_m\n            if d_m  target_discrepancy:\n                alpha_l = alpha_m\n            else:\n                alpha_u = alpha_m\n        \n        d_l = get_discrepancy(alpha_l, phi_0, y, memo)\n        d_u = get_discrepancy(alpha_u, phi_0, y, memo)\n        return alpha_l if abs(d_l - target_discrepancy)  abs(d_u - target_discrepancy) else alpha_u\n\n    # 4. Main Execution Loop\n    phi_true = create_sdf_disk(N, CX_TRUE, CY_TRUE, R_TRUE)\n    u_true = smoothed_heaviside(phi_true)\n    y_clean = forward_operator_A(u_true)\n    phi_0 = create_sdf_disk(N, CX0, CY0, R0)\n    \n    test_cases = [\n        (0.05, 1.0), (0.01, 1.0), (0.10, 1.1), (0.08, 0.9)\n    ]\n    \n    results = []\n    for i, (sigma, tau) in enumerate(test_cases):\n        seed = S0 + i\n        rng = np.random.default_rng(seed)\n        noise = rng.normal(0, sigma, (N, N))\n        y = y_clean + noise\n        \n        alpha_star = find_alpha_morozov(y, phi_0, sigma, tau)\n        results.append(\"{:.6f}\".format(alpha_star))\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}