{
    "hands_on_practices": [
        {
            "introduction": "Before confronting the complexities of ill-posed problems, it is instructive to work through a case where an exact, analytical inversion is possible. This exercise guides you through the classical inversion of the Abel integral equation, a fundamental Volterra equation, using foundational principles of calculus. Mastering this derivation provides a benchmark for well-behaved problems and highlights why the inversion of Fredholm equations is often much more challenging .",
            "id": "3391691",
            "problem": "Consider the Abel-type Volterra integral equation of the first kind on the interval $[0,1]$,\n$$\ng(s) \\;=\\; \\int_{0}^{s} \\frac{f(t)}{\\sqrt{s-t}}\\, dt,\\quad s\\in(0,1],\n$$\nwhere $f$ is an unknown function and $g$ is a given function. Assume $f\\in C([0,1])$, $g\\in C^{1}([0,1])$, and $g(0)=0$. Starting only from foundational results of real analysis and integral calculus—specifically, the properties of Volterra integral operators, Fubini’s theorem for iterated integrals, and differentiation under the integral sign—derive an explicit inversion formula that expresses $f(s)$ pointwise for $s\\in(0,1)$ purely in terms of $g$. Your derivation must justify any interchange of the order of integration and any differentiation under the integral sign by citing appropriate regularity or integrability conditions implied by the stated assumptions.\n\nProvide your final result as a single closed-form analytic expression for $f(s)$ in terms of $g$. Do not include intermediate steps, inequalities, or equations in your final answer. No rounding is required.",
            "solution": "The user wants to find the solution $f(s)$ to the Abel-type Volterra integral equation of the first kind:\n$$\ng(s) \\;=\\; \\int_{0}^{s} \\frac{f(t)}{\\sqrt{s-t}}\\, dt, \\quad s\\in(0,1]\n$$\nThe provided assumptions are that $f$ is a continuous function on $[0,1]$ (i.e., $f \\in C([0,1])$), $g$ is a continuously differentiable function on $[0,1]$ (i.e., $g \\in C^1([0,1])$), and $g(0)=0$. The derivation must be based on foundational principles of calculus and real analysis.\n\nThe method to solve this equation involves applying a specific integral operator to both sides of the equation. For an arbitrary $x \\in (0,1]$, we multiply both sides of the equation (with the variable $s$) by the kernel $(x-s)^{-1/2}$ and integrate with respect to $s$ from $0$ to $x$:\n$$\n\\int_{0}^{x} \\frac{g(s)}{\\sqrt{x-s}} ds \\;=\\; \\int_{0}^{x} \\frac{1}{\\sqrt{x-s}} \\left( \\int_{0}^{s} \\frac{f(t)}{\\sqrt{s-t}} dt \\right) ds\n$$\nThe right-hand side (RHS) is a double integral over a triangular region in the $s$-$t$ plane:\n$$\n\\text{RHS} \\;=\\; \\int_{0}^{x} \\int_{0}^{s} \\frac{f(t)}{\\sqrt{(x-s)(s-t)}} dt\\,ds\n$$\nThe domain of integration is $D = \\{(t,s) \\in \\mathbb{R}^2 \\mid 0 \\le s \\le x, 0 \\le t \\le s \\}$. We wish to interchange the order of integration. This is permissible by Fubini's theorem if the integral of the absolute value of the integrand over $D$ is finite. Since $f \\in C([0,1])$, it is bounded on $[0,1]$, so there exists a constant $M > 0$ such that $|f(t)| \\le M$ for all $t \\in [0,1]$.\n$$\n\\int_{0}^{x} \\int_{0}^{s} \\frac{|f(t)|}{\\sqrt{(x-s)(s-t)}} dt\\,ds \\;\\le\\; M \\int_{0}^{x} \\frac{1}{\\sqrt{x-s}} \\left( \\int_{0}^{s} \\frac{1}{\\sqrt{s-t}} dt \\right) ds\n$$\nThe inner integral evaluates to:\n$$\n\\int_{0}^{s} (s-t)^{-1/2} dt \\;=\\; \\left[-2(s-t)^{1/2}\\right]_{t=0}^{t=s} \\;=\\; 2\\sqrt{s}\n$$\nSubstituting this back, the outer integral becomes:\n$$\nM \\int_{0}^{x} \\frac{2\\sqrt{s}}{\\sqrt{x-s}} ds \\;=\\; 2M \\int_{0}^{x} \\sqrt{\\frac{s}{x-s}} ds\n$$\nThis is an improper integral which is convergent. Its value can be shown to be $2M \\cdot (\\pi x/2) = M\\pi x$. Since the integral of the absolute value of the integrand is finite, Fubini's theorem applies.\n\nWe can now change the order of integration. The domain $D$ can be re-described as $\\{(t,s) \\in \\mathbb{R}^2 \\mid 0 \\le t \\le x, t \\le s \\le x \\}$. The RHS integral becomes:\n$$\n\\text{RHS} \\;=\\; \\int_{0}^{x} f(t) \\left( \\int_{t}^{x} \\frac{ds}{\\sqrt{(x-s)(s-t)}} \\right) dt\n$$\nThe inner integral with respect to $s$ is a standard form. Let's evaluate it using the substitution $s = t \\cos^2\\theta + x \\sin^2\\theta$. Then $ds = 2(x-t)\\sin\\theta\\cos\\theta \\, d\\theta$. The limits change from $s=t$ to $\\theta=0$ and from $s=x$ to $\\theta=\\pi/2$. The terms under the square root become:\n$s-t = (x-t)\\sin^2\\theta$\n$x-s = (x-t)\\cos^2\\theta$\nThe inner integral becomes:\n$$\n\\int_{0}^{\\pi/2} \\frac{2(x-t)\\sin\\theta\\cos\\theta \\, d\\theta}{\\sqrt{(x-t)^2 \\sin^2\\theta \\cos^2\\theta}} \\;=\\; \\int_{0}^{\\pi/2} \\frac{2(x-t)\\sin\\theta\\cos\\theta \\, d\\theta}{(x-t)\\sin\\theta\\cos\\theta} \\;=\\; \\int_{0}^{\\pi/2} 2 \\,d\\theta \\;=\\; \\pi\n$$\nThe inner integral evaluates to the constant $\\pi$, a key result in the theory of Abel's equation.\nSubstituting this result back into the expression for the RHS:\n$$\n\\text{RHS} \\;=\\; \\int_{0}^{x} f(t) \\cdot \\pi \\, dt \\;=\\; \\pi \\int_{0}^{x} f(t) dt\n$$\nEquating this with the left-hand side from our first step, we have:\n$$\n\\pi \\int_{0}^{x} f(t) dt \\;=\\; \\int_{0}^{x} \\frac{g(s)}{\\sqrt{x-s}} ds\n$$\nTo recover $f(x)$, we differentiate both sides with respect to $x$. Given that $f(t)$ is continuous, the Fundamental Theorem of Calculus (Part 1) applies to the left side:\n$$\n\\frac{d}{dx} \\left( \\pi \\int_{0}^{x} f(t) dt \\right) \\;=\\; \\pi f(x)\n$$\nEquating the derivatives gives:\n$$\n\\pi f(x) \\;=\\; \\frac{d}{dx} \\left( \\int_{0}^{x} \\frac{g(s)}{\\sqrt{x-s}} ds \\right)\n$$\nThis provides an explicit formula for $f(x)$. The validity of taking the derivative on the right side is guaranteed by the assumption that $g \\in C^1([0,1])$ and $g(0)=0$. To show this, we can integrate by parts the integral on the right:\n$$\n\\int_{0}^{x} \\frac{g(s)}{\\sqrt{x-s}} ds = \\left[g(s)(-2\\sqrt{x-s})\\right]_{s=0}^{s=x} - \\int_{0}^{x} g'(s)(-2\\sqrt{x-s})ds\n$$\nThe boundary term at $s=x$ is $0$. The boundary term at $s=0$ is $-g(0)(-2\\sqrt{x})$, which is $0$ since $g(0)=0$. Thus, the expression becomes $2\\int_{0}^{x} g'(s)\\sqrt{x-s}ds$. The derivative of this expression with respect to $x$ can be taken using the Leibniz integral rule, and it yields $\\int_0^x \\frac{g'(s)}{\\sqrt{x-s}}ds$, which is well-defined because $g'$ is continuous. This confirms the well-definedness of the derivative operator in our solution.\n\nThe problem asks for an expression for $f(s)$. Replacing the variable $x$ with $s$ and the dummy integration variable $s$ with $t$, we obtain the final inversion formula:\n$$\nf(s) \\;=\\; \\frac{1}{\\pi} \\frac{d}{ds} \\int_{0}^{s} \\frac{g(t)}{\\sqrt{s-t}} dt\n$$\nThis formula expresses the unknown function $f(s)$ pointwise in terms of the given function $g(s)$ and standard mathematical operations, as required.",
            "answer": "$$\\boxed{\\frac{1}{\\pi} \\frac{d}{ds} \\int_{0}^{s} \\frac{g(t)}{\\sqrt{s-t}}\\, dt}$$"
        },
        {
            "introduction": "Fredholm integral equations of the first kind are canonical examples of ill-posed problems, and their numerical solution reveals a deep connection between the continuous operator's properties and the discrete matrix's stability. This practice will have you diagnose the source of this ill-posedness by relating the analytical spectrum of an integral operator to the condition number of its Nyström discretization. You will discover firsthand how the decay of the operator's singular values leads to an exploding condition number in the discrete system, making naive inversion impossible .",
            "id": "3391739",
            "problem": "Consider a Fredholm integral equation of the first kind on the interval $[0,1]$,\n$$\n(Kf)(s) = \\int_{0}^{1} k(s,t)\\,f(t)\\,dt = g(s),\n$$\nwhere $k(s,t)$ is a continuous symmetric kernel and $g(s)$ is a known function. In inverse problems, discretization via the Nyström method generates a linear system that approximates the integral operator. Your tasks are as follows:\n\n1. Using the kernel\n$$\nk(s,t) = \\min(s,t) - s\\,t,\n$$\nderive the Nyström discretization with the midpoint quadrature rule on $[0,1]$. Let the quadrature nodes be $t_{j} = \\frac{j-\\frac{1}{2}}{N}$ for $j=1,\\dots,N$ and weights $w_{j} = \\frac{1}{N}$. Collocate the integral equation at the same set of points $s_{i} = t_{i}$, $i=1,\\dots,N$, to obtain the linear system $A\\mathbf{f} \\approx \\mathbf{g}$, where $\\mathbf{f} = \\big(f(t_{1}),\\dots,f(t_{N})\\big)^{\\top}$ and $\\mathbf{g} = \\big(g(s_{1}),\\dots,g(s_{N})\\big)^{\\top}$. Write an explicit formula for the entries $A_{ij}$ in terms of $s_{i}$, $t_{j}$, and $w_{j}$.\n\n2. Starting from the definition of an eigenpair $(\\lambda,\\varphi)$ of the integral operator $K$, that is\n$$\n\\int_{0}^{1} \\big(\\min(s,t) - s\\,t\\big)\\,\\varphi(t)\\,dt = \\lambda\\,\\varphi(s),\n$$\nderive, by differentiating with respect to $s$ and using appropriate boundary conditions implied by the kernel, the ordinary differential equation satisfied by $\\varphi(s)$ and determine the eigenvalues $\\lambda_{n}$ and eigenfunctions $\\varphi_{n}(s)$.\n\n3. Use the spectral characterization from part 2 and the fact that the Nyström discretization with a consistent quadrature rule yields a symmetric positive definite matrix whose spectrum converges to that of the continuous operator to justify the asymptotic behavior of the condition number $\\kappa(A)$ of the Nyström matrix $A$ as $N$ increases. Here the condition number is defined as $\\kappa(A) = \\sigma_{\\max}(A)/\\sigma_{\\min}(A)$, where $\\sigma_{\\max}(A)$ and $\\sigma_{\\min}(A)$ denote the largest and smallest singular values of $A$.\n\nState your final result for the leading-order asymptotic expression of $\\kappa(A)$ as a closed-form analytic expression in $N$. No units are required. Do not provide an inequality. If you must approximate, provide the leading-order term only, without lower-order corrections.",
            "solution": "The user has provided a three-part problem concerning a Fredholm integral equation of the first kind. I will first validate the problem and then proceed to a complete solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Integral Equation:** $(Kf)(s) = \\int_{0}^{1} k(s,t)\\,f(t)\\,dt = g(s)$ for $s \\in [0,1]$.\n-   **Kernel:** $k(s,t) = \\min(s,t) - s\\,t$. It is stated to be continuous and symmetric.\n-   **Part 1 Givens (Nyström Discretization):**\n    -   Quadrature rule: Midpoint rule.\n    -   Quadrature nodes: $t_{j} = \\frac{j-\\frac{1}{2}}{N}$ for $j=1,\\dots,N$.\n    -   Quadrature weights: $w_{j} = \\frac{1}{N}$.\n    -   Collocation points: $s_{i} = t_{i} = \\frac{i-\\frac{1}{2}}{N}$ for $i=1,\\dots,N$.\n    -   Task: Find the explicit formula for the entries $A_{ij}$ of the matrix in the system $A\\mathbf{f} \\approx \\mathbf{g}$.\n-   **Part 2 Givens (Eigenproblem):**\n    -   Eigenvalue equation: $\\int_{0}^{1} \\big(\\min(s,t) - s\\,t\\big)\\,\\varphi(t)\\,dt = \\lambda\\,\\varphi(s)$.\n    -   Task: Derive the differential equation for $\\varphi(s)$ and find the eigenvalues $\\lambda_{n}$ and eigenfunctions $\\varphi_{n}(s)$.\n-   **Part 3 Givens (Condition Number):**\n    -   The Nyström matrix $A$ is stated to be symmetric positive definite.\n    -   The spectrum of $A$ converges to that of the continuous operator $K$.\n    -   Condition number definition: $\\kappa(A) = \\sigma_{\\max}(A)/\\sigma_{\\min}(A)$.\n    -   Task: Justify and find the leading-order asymptotic expression for $\\kappa(A)$ as $N \\to \\infty$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is a standard exercise in the theory of integral equations and their numerical solution. The kernel $k(s,t) = \\min(s,t) - st$ is the Green's function for the second-order differential operator $L=-d^2/ds^2$ with Dirichlet boundary conditions $\\varphi(0)=0$ and $\\varphi(1)=0$. This is a well-established fact in the study of boundary value problems. All parts of the problem are based on sound mathematical principles.\n-   **Well-Posed:** Each part of the problem is well-defined and has a unique, meaningful solution. The steps are logically connected to guide the solver from the continuous problem to the analysis of its discretization.\n-   **Objective:** The problem is stated in precise mathematical language, free from any subjectivity or ambiguity.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is scientifically sound, well-posed, and objective. I will now proceed with providing a complete solution.\n\n### Solution\n\n**Part 1: Nyström Discretization**\n\nThe Fredholm integral equation is given by\n$$\n\\int_{0}^{1} k(s,t)\\,f(t)\\,dt = g(s).\n$$\nThe Nyström method approximates the integral using a quadrature rule. Using the given midpoint rule with nodes $t_j$ and weights $w_j$, the integral is approximated as:\n$$\n\\int_{0}^{1} k(s,t)\\,f(t)\\,dt \\approx \\sum_{j=1}^{N} w_j k(s, t_j) f(t_j).\n$$\nTo obtain a linear system, we collocate the equation at the points $s_i = t_i$. This yields:\n$$\n\\sum_{j=1}^{N} w_j k(s_i, t_j) f(t_j) \\approx g(s_i), \\quad \\text{for } i=1,\\dots,N.\n$$\nThis is a linear system of the form $A\\mathbf{f} \\approx \\mathbf{g}$, where $\\mathbf{f}$ is the vector with entries $f_j = f(t_j)$ and $\\mathbf{g}$ is the vector with entries $g_i = g(s_i)$. The entries of the matrix $A$ are given by\n$$\nA_{ij} = w_j k(s_i, t_j).\n$$\nSubstituting the given expressions for the kernel, weights, nodes, and collocation points:\n-   $k(s_i, t_j) = \\min(s_i, t_j) - s_i t_j$\n-   $w_j = \\frac{1}{N}$\n-   $s_i = t_i = \\frac{i-\\frac{1}{2}}{N}$\n-   $t_j = \\frac{j-\\frac{1}{2}}{N}$\n\nThe matrix entries $A_{ij}$ are:\n$$\nA_{ij} = \\frac{1}{N} \\left[ \\min\\left(\\frac{i-\\frac{1}{2}}{N}, \\frac{j-\\frac{1}{2}}{N}\\right) - \\left(\\frac{i-\\frac{1}{2}}{N}\\right)\\left(\\frac{j-\\frac{1}{2}}{N}\\right) \\right].\n$$\nThis is the explicit formula for the entries of the Nyström matrix $A$.\n\n**Part 2: Eigenvalue Problem for the Integral Operator**\n\nWe are tasked with solving the eigenvalue problem for the integral operator $K$:\n$$\n\\lambda \\varphi(s) = \\int_{0}^{1} (\\min(s,t) - st) \\varphi(t) dt.\n$$\nTo convert this integral equation into a differential equation, we first split the integral at $t=s$:\n$$\n\\lambda \\varphi(s) = \\int_{0}^{s} (t - st) \\varphi(t) dt + \\int_{s}^{1} (s - st) \\varphi(t) dt.\n$$\n$$\n\\lambda \\varphi(s) = (1-s) \\int_{0}^{s} t \\varphi(t) dt + s \\int_{s}^{1} (1-t) \\varphi(t) dt.\n$$\nWe differentiate this equation with respect to $s$ using the Leibniz integral rule:\n$$\n\\lambda \\varphi'(s) = \\frac{d}{ds} \\left[ (1-s) \\int_{0}^{s} t \\varphi(t) dt + s \\int_{s}^{1} (1-t) \\varphi(t) dt \\right]\n$$\n$$\n\\lambda \\varphi'(s) = -\\int_{0}^{s} t\\varphi(t)dt + (1-s)(s\\varphi(s)) + \\int_{s}^{1}(1-t)\\varphi(t)dt + s(-(1-s)\\varphi(s))\n$$\n$$\n\\lambda \\varphi'(s) = -\\int_{0}^{s} t\\varphi(t)dt + s(1-s)\\varphi(s) + \\int_{s}^{1}(1-t)\\varphi(t)dt - s(1-s)\\varphi(s)\n$$\n$$\n\\lambda \\varphi'(s) = \\int_{s}^{1} (1-t)\\varphi(t)dt - \\int_{0}^{s} t\\varphi(t)dt.\n$$\nDifferentiating a second time with respect to $s$:\n$$\n\\lambda \\varphi''(s) = \\frac{d}{ds} \\left[ \\int_{s}^{1} (1-t)\\varphi(t)dt - \\int_{0}^{s} t\\varphi(t)dt \\right]\n$$\n$$\n\\lambda \\varphi''(s) = -(1-s)\\varphi(s) - s\\varphi(s) = -s\\varphi(s) - \\varphi(s) + s\\varphi(s) = -\\varphi(s).\n$$\nThis gives the ordinary differential equation:\n$$\n\\varphi''(s) + \\frac{1}{\\lambda} \\varphi(s) = 0.\n$$\nNext, we determine the boundary conditions. From the expression for $\\lambda \\varphi(s)$, we evaluate at $s=0$:\n$$\n\\lambda \\varphi(0) = (1-0) \\int_{0}^{0} t \\varphi(t) dt + 0 \\cdot \\int_{0}^{1} (1-t) \\varphi(t) dt = 0.\n$$\nFor non-trivial eigensolutions, we must have $\\lambda \\neq 0$, which implies $\\varphi(0)=0$.\nEvaluating at $s=1$:\n$$\n\\lambda \\varphi(1) = (1-1) \\int_{0}^{1} t \\varphi(t) dt + 1 \\cdot \\int_{1}^{1} (1-t) \\varphi(t) dt = 0.\n$$\nThis implies $\\varphi(1)=0$.\nWe now have a Sturm-Liouville problem consisting of the ODE $\\varphi''(s) + \\mu \\varphi(s) = 0$ with $\\mu=1/\\lambda$ and boundary conditions $\\varphi(0)=0$, $\\varphi(1)=0$.\nThe general solution to the ODE is $\\varphi(s) = C_1 \\cos(\\sqrt{\\mu}s) + C_2 \\sin(\\sqrt{\\mu}s)$.\nApplying the boundary conditions:\n-   $\\varphi(0) = 0 \\implies C_1 \\cos(0) + C_2 \\sin(0) = C_1 = 0$.\n-   $\\varphi(s)$ must be of the form $\\varphi(s) = C_2 \\sin(\\sqrt{\\mu}s)$.\n-   $\\varphi(1) = 0 \\implies C_2 \\sin(\\sqrt{\\mu}) = 0$.\nFor a non-trivial solution ($C_2 \\neq 0$), we require $\\sin(\\sqrt{\\mu})=0$. This occurs when $\\sqrt{\\mu} = n\\pi$ for any integer $n$. We take $n=1, 2, 3, \\dots$ for distinct, non-trivial solutions.\nThe eigenvalues of the differential operator are $\\mu_n = (n\\pi)^2$.\nSince $\\lambda = 1/\\mu$, the eigenvalues of the integral operator $K$ are:\n$$\n\\lambda_n = \\frac{1}{(n\\pi)^2}, \\quad n=1, 2, 3, \\dots\n$$\nThe corresponding unnormalized eigenfunctions are:\n$$\n\\varphi_n(s) = \\sin(n\\pi s).\n$$\nThe normalized eigenfunctions are $\\sqrt{2}\\sin(n\\pi s)$.\n\n**Part 3: Asymptotic Behavior of the Condition Number**\n\nThe condition number of the matrix $A$ is defined as $\\kappa(A) = \\sigma_{\\max}(A)/\\sigma_{\\min}(A)$, where $\\sigma_{\\max}(A)$ and $\\sigma_{\\min}(A)$ are the largest and smallest singular values of $A$.\nThe kernel $k(s,t)$ is symmetric. The quadrature rule uses equal weights $w_j = 1/N$. The matrix entries are $A_{ij} = \\frac{1}{N}k(s_i, t_j)$. With $s_i=t_i$, we have $A_{ij} = \\frac{1}{N}k(t_i, t_j)$. Since $k(t_i, t_j)=k(t_j, t_i)$, the matrix $A$ is symmetric.\nThe integral operator $K$ is positive definite on the space of functions orthogonal to the null space (which is only the zero function here), as its eigenvalues $\\lambda_n = 1/(n\\pi)^2$ are all positive. For large enough $N$, the discrete approximation $A$ will also be positive definite. For a symmetric positive definite matrix, its singular values are its eigenvalues. Thus, $\\kappa(A) = \\nu_{\\max}(A)/\\nu_{\\min}(A)$, where $\\nu$ denotes the eigenvalues of $A$.\n\nThe problem states that the spectrum of the Nyström matrix $A$ converges to that of the continuous operator $K$. This is a key result from the theory of numerical integral equations. It implies that for large $N$, the set of $N$ eigenvalues of the matrix $A$, which we denote $\\{\\nu_k\\}_{k=1}^N$ in decreasing order, provides a good approximation to the first $N$ eigenvalues of the operator $K$, $\\{\\lambda_k\\}_{k=1}^N$.\n\n1.  **Largest Eigenvalue:** The largest eigenvalue of $A$, $\\nu_{\\max}(A) = \\nu_1$, will approximate the largest (principal) eigenvalue of $K$. From Part 2, this is $\\lambda_1$.\n    $$\n    \\sigma_{\\max}(A) = \\nu_1 \\approx \\lambda_1 = \\frac{1}{\\pi^2}.\n    $$\n2.  **Smallest Eigenvalue:** The $N$ eigenvalues of the discrete system correspond to the $N$ modes that can be resolved on the grid. The smallest eigenvalue of $A$, $\\nu_{\\min}(A)=\\nu_N$, corresponds to the highest-frequency mode representable by the discretization. It is therefore reasonable to approximate $\\nu_N$ by the $N$-th eigenvalue of the continuous operator, $\\lambda_N$.\n    $$\n    \\sigma_{\\min}(A) = \\nu_N \\approx \\lambda_N = \\frac{1}{(N\\pi)^2}.\n    $$\n3.  **Condition Number:** Using these asymptotic approximations, we can find the leading-order behavior of the condition number $\\kappa(A)$:\n    $$\n    \\kappa(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)} \\approx \\frac{\\lambda_1}{\\lambda_N} = \\frac{1/(\\pi^2)}{1/((N\\pi)^2)} = \\frac{(N\\pi)^2}{\\pi^2} = N^2.\n    $$\nThe severe ill-conditioning, growing quadratically with $N$, is characteristic of the discretization of Fredholm integral equations of the first kind with smooth kernels (so-called ill-posed problems). The leading-order asymptotic expression for the condition number is $N^2$.",
            "answer": "$$\\boxed{N^2}$$"
        },
        {
            "introduction": "Theoretical principles like the Fredholm alternative gain practical meaning through numerical experimentation. This coding-based exercise puts you in the role of a computational scientist, testing how to solve a discretized Fredholm system when the data may or may not satisfy solvability conditions. By using the singular value decomposition (SVD) and implementing solvers like the pseudoinverse and Tikhonov regularization, you will observe the concrete effects of working with data that is inside and outside the operator's range .",
            "id": "3391694",
            "problem": "You are tasked with numerically investigating the Fredholm alternative for a discrete approximation of a compact Fredholm integral operator of the first kind, posed on the interval $[0,1]$. The goal is to construct right-hand sides $f$ that lie in the range of the operator and that do not, test the behavior of common solvers, and compute quantitative artifacts when the solvability condition $\\langle f, \\psi \\rangle = 0$ fails for $\\psi \\in \\ker(A^\\ast)$.\n\nFundamental basis:\n- A Fredholm integral operator of the first kind is defined as $(A u)(x) = \\int_{0}^{1} k(x,y) u(y) \\, dy$ where $k(x,y)$ is a square-integrable kernel and $u$ is an unknown function.\n- For a compact operator $A$ on a Hilbert space, the Fredholm alternative states that the equation $A u = f$ is solvable if and only if $f$ is orthogonal to the kernel of the adjoint, that is, $\\langle f, \\psi \\rangle = 0$ for all $\\psi \\in \\ker(A^\\ast)$. When the solvability condition fails, there is no solution; least-squares or regularized solvers produce projection and bias artifacts.\n\nDiscretization:\n- Consider the separable kernel $k(x,y) = p(x) q(y)$ with $p(x) = \\sin(\\pi x)$ and $q(y) = 1$, which yields the rank-one operator $(A u)(x) = p(x) \\int_{0}^{1} u(y) \\, dy$.\n- Discretize $[0,1]$ uniformly using the composite trapezoidal rule with $N=200$ grid points $x_i = \\frac{i}{N-1}$ for $i=0,1,\\dots,N-1$, spacing $h = \\frac{1}{N-1}$, and quadrature weights $w_0 = w_{N-1} = \\frac{h}{2}$ and $w_i = h$ for $i=1,\\dots,N-2$.\n- Define the discrete operator matrix $A \\in \\mathbb{R}^{N \\times N}$ with entries $A_{ij} = p(x_i) w_j$ so that $(A u)_i \\approx p(x_i) \\sum_{j=0}^{N-1} w_j u_j$.\n- Use the standard Euclidean inner product in $\\mathbb{R}^N$, namely $\\langle a, b \\rangle = a^\\top b$. Under this discrete setting, the adjoint is $A^\\top$ and the kernel of $A^\\top$ equals the orthogonal complement of the left singular vector associated with the nonzero singular value.\n\nSolvers and diagnostics:\n- Use the singular value decomposition to construct the Moore–Penrose pseudoinverse $A^+$ and compute the minimal-norm least-squares solution $u^\\dagger = A^+ f$ and residual $\\|A u^\\dagger - f\\|_2$ for each $f$.\n- Use Tikhonov regularization with parameter $\\alpha = 10^{-3}$, solving $(A^\\top A + \\alpha I) u_{\\alpha} = A^\\top f$ for $u_{\\alpha}$, and compute the residual $\\|A u_{\\alpha} - f\\|_2$.\n- Construct an orthonormal basis $\\{\\psi_k\\}$ for $\\ker(A^\\top)$ from the left singular vectors corresponding to zero singular values and compute the solvability metric $S(f) = \\sum_k |\\langle f, \\psi_k \\rangle|^2$, which quantifies the violation of the solvability condition. For $f$ in the range of $A$, $S(f)$ should be numerically zero; otherwise it is positive.\n\nTest suite:\n- Three right-hand sides $f^{(m)} \\in \\mathbb{R}^N$:\n    1. Case 1 (in-range): $f^{(1)}_i = p(x_i)$.\n    2. Case 2 (out-of-range): $f^{(2)}_i = p(x_i) + 0.7 \\cos(2\\pi x_i)$.\n    3. Case 3 (nearly in-range): $f^{(3)}_i = p(x_i) + 10^{-3} \\cos(2\\pi x_i)$.\n- For each case, compute the following five quantities:\n    1. $S(f^{(m)})$.\n    2. $\\|A u^\\dagger - f^{(m)}\\|_2$.\n    3. $\\|u^\\dagger\\|_2$.\n    4. $\\|A u_{\\alpha} - f^{(m)}\\|_2$.\n    5. $\\|u_{\\alpha}\\|_2$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as\n$$[S(f^{(1)}), \\|A u^\\dagger - f^{(1)}\\|_2, \\|u^\\dagger\\|_2, \\|A u_{\\alpha} - f^{(1)}\\|_2, \\|u_{\\alpha}\\|_2, S(f^{(2)}), \\|A u^\\dagger - f^{(2)}\\|_2, \\|u^\\dagger\\|_2, \\|A u_{\\alpha} - f^{(2)}\\|_2, \\|u_{\\alpha}\\|_2, S(f^{(3)}), \\|A u^\\dagger - f^{(3)}\\|_2, \\|u^\\dagger\\|_2, \\|A u_{\\alpha} - f^{(3)}\\|_2, \\|u_{\\alpha}\\|_2].$$\n- All values must be printed as real numbers (floats). No units are required because all quantities are dimensionless in this discrete setting.\n\nDesign considerations:\n- Ensure numerical robustness by detecting zero singular values using a tolerance proportional to the largest singular value.\n- Provide results for all three cases within a single run and print only the final list in the specified format.",
            "solution": "The problem is valid. It is scientifically grounded in the theory of integral equations and inverse problems, specifically the Fredholm alternative. The setup is well-posed as a numerical investigation, providing all necessary parameters and definitions for a unique and verifiable set of results. The language is objective and precise.\n\nHere follows a step-by-step derivation of the solution.\n\n### 1. Discretization of the Integral Operator\n\nThe continuous Fredholm integral operator of the first kind is given by:\n$$ (A u)(x) = \\int_{0}^{1} k(x,y) u(y) \\, dy $$\nWith the specified separable kernel $k(x,y) = p(x)q(y)$, where $p(x) = \\sin(\\pi x)$ and $q(y) = 1$, the operator becomes:\n$$ (A u)(x) = \\sin(\\pi x) \\int_{0}^{1} u(y) \\, dy $$\nThis shows that the range of the operator $A$ is the space spanned by the function $p(x) = \\sin(\\pi x)$. Any function $f(x)$ for which the equation $Au=f$ has a solution must be a scalar multiple of $\\sin(\\pi x)$.\n\nTo discretize the operator, we represent functions on a uniform grid of $N=200$ points $x_i = \\frac{i}{N-1}$ for $i=0, 1, \\dots, N-1$. The integral is approximated using the composite trapezoidal rule with step size $h = \\frac{1}{N-1}$ and weights $w_j$. The weights are $w_0 = w_{N-1} = \\frac{h}{2}$ and $w_j = h$ for $j=1, \\dots, N-2$.\n\nA function $u(x)$ is represented by a vector $u \\in \\mathbb{R}^N$ with components $u_j = u(x_j)$. The discrete version of the integral is $\\sum_{j=0}^{N-1} w_j u_j$. The action of the discrete operator on a vector $u$ yields a vector $f = Au$, where the $i$-th component is:\n$$ (Au)_i = p(x_i) \\sum_{j=0}^{N-1} w_j u_j $$\nThis can be expressed as a matrix-vector product $f = \\mathbf{A}u$, where the matrix $\\mathbf{A} \\in \\mathbb{R}^{N \\times N}$ has entries:\n$$ A_{ij} = p(x_i) w_j = \\sin\\left(\\frac{\\pi i}{N-1}\\right) w_j $$\nThis matrix $\\mathbf{A}$ is a rank-one matrix, as it is the outer product of two vectors: a vector $p$ with components $p_i = \\sin(\\pi x_i)$ and the vector of quadrature weights $w$.\n\n### 2. Analysis using Singular Value Decomposition (SVD)\n\nThe Singular Value Decomposition (SVD) of the matrix $\\mathbf{A}$ is $\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top$, where $\\mathbf{U}$ and $\\mathbf{V}$ are orthogonal matrices and $\\mathbf{\\Sigma}$ is a diagonal matrix of singular values $s_k \\ge 0$.\n- The columns of $\\mathbf{U}$ are the left singular vectors, denoted $\\{u_k\\}$.\n- The columns of $\\mathbf{V}$ are the right singular vectors, denoted $\\{v_k\\}$.\n\nFor a rank-one matrix like $\\mathbf{A}$, only one singular value, $s_0$, is non-zero. All other singular values $s_1, \\dots, s_{N-1}$ are zero.\n- The range of $\\mathbf{A}$, $R(\\mathbf{A})$, is spanned by the first left singular vector $u_0$.\n- The kernel (null space) of the adjoint matrix $\\mathbf{A}^\\top$ (using the standard Euclidean dot product $\\langle a, b \\rangle = a^\\top b$), denoted $\\ker(\\mathbf{A}^\\top)$, is the orthogonal complement of $R(\\mathbf{A})$. It is spanned by the left singular vectors $u_1, \\dots, u_{N-1}$ corresponding to the zero singular values. The problem denotes this orthonormal basis for $\\ker(\\mathbf{A}^\\top)$ as $\\{\\psi_k\\}$.\n\n### 3. The Fredholm Alternative and the Solvability Metric\n\nThe Fredholm alternative states that the linear system $\\mathbf{A}u = f$ has a solution if and only if the right-hand side $f$ is orthogonal to every vector in the kernel of the adjoint, $\\ker(\\mathbf{A}^\\top)$.\n$$ \\langle f, \\psi \\rangle = 0 \\quad \\forall \\psi \\in \\ker(\\mathbf{A}^\\top) $$\nThe solvability metric $S(f)$ quantifies the violation of this condition. It is the squared norm of the projection of $f$ onto $\\ker(\\mathbf{A}^\\top)$:\n$$ S(f) = \\sum_{k=1}^{N-1} |\\langle f, \\psi_k \\rangle|^2 = \\left\\| \\text{proj}_{\\ker(\\mathbf{A}^\\top)} f \\right\\|_2^2 $$\nwhere the basis vectors $\\psi_k$ are the left singular vectors $u_1, \\dots, u_{N-1}$. If $f \\in R(\\mathbf{A})$, it is orthogonal to $\\ker(\\mathbf{A}^\\top)$, and thus $S(f) = 0$. Otherwise, $S(f) > 0$.\n\n### 4. Numerical Solvers\n\n#### a. Pseudoinverse Solution\n\nWhen $S(f) > 0$, no exact solution exists. The Moore-Penrose pseudoinverse $\\mathbf{A}^+$ provides the minimal-norm least-squares solution $u^\\dagger = \\mathbf{A}^+ f$. This solution minimizes the residual norm $\\|\\mathbf{A}u - f\\|_2$. The pseudoinverse is constructed from the SVD:\n$$ \\mathbf{A}^+ = \\mathbf{V} \\mathbf{\\Sigma}^+ \\mathbf{U}^\\top $$\nwhere $\\mathbf{\\Sigma}^+$ is obtained by taking the reciprocal of the non-zero singular values in $\\mathbf{\\Sigma}$ and leaving the zeros. For our rank-one case, only $1/s_0$ is non-zero in $\\mathbf{\\Sigma}^+$. The residual for this solution is the norm of the projection of $f$ onto $\\ker(\\mathbf{A}^\\top)$:\n$$ \\|\\mathbf{A}u^\\dagger - f\\|_2 = \\left\\| \\text{proj}_{\\ker(\\mathbf{A}^\\top)} f \\right\\|_2 = \\sqrt{S(f)} $$\nThis provides a crucial consistency check for our computations.\n\n#### b. Tikhonov Regularization\n\nTikhonov regularization finds an approximate solution by solving a related, well-posed problem. Given a regularization parameter $\\alpha > 0$, the Tikhonov-regularized solution $u_\\alpha$ is found by solving the regularized normal equations:\n$$ (\\mathbf{A}^\\top \\mathbf{A} + \\alpha \\mathbf{I}) u_\\alpha = \\mathbf{A}^\\top f $$\nThis method yields a stable solution even for ill-posed problems, but it introduces a bias. The solution $u_\\alpha$ minimizes the functional $\\|\\mathbf{A}u - f\\|_2^2 + \\alpha \\|u\\|_2^2$.\n\n### 5. Analysis of Test Cases\n\nWe apply this framework to the three specified right-hand side vectors $f^{(m)}$.\n\n1.  **Case 1: $f^{(1)}_i = p(x_i) = \\sin(\\pi x_i)$**.\n    This vector $f^{(1)}$ is precisely the vector that spans the range of $\\mathbf{A}$. Therefore, it lies in $R(\\mathbf{A})$ and is orthogonal to $\\ker(\\mathbf{A}^\\top)$. We expect $S(f^{(1)}) \\approx 0$ and the least-squares residual $\\|\\mathbf{A}u^\\dagger - f^{(1)}\\|_2 \\approx 0$. An exact solution exists.\n\n2.  **Case 2: $f^{(2)}_i = p(x_i) + 0.7 \\cos(2\\pi x_i)$**.\n    This vector has a component, $g_i = 0.7 \\cos(2\\pi x_i)$, that does not lie in the span of $p(x_i)$. Therefore, $f^{(2)}$ is not in $R(\\mathbf{A})$. We expect a non-zero solvability metric $S(f^{(2)}) > 0$ and a corresponding non-zero residual $\\|\\mathbf{A}u^\\dagger - f^{(2)}\\|_2 > 0$.\n\n3.  **Case 3: $f^{(3)}_i = p(x_i) + 10^{-3} \\cos(2\\pi x_i)$**.\n    This case is similar to the second, representing a scenario with small-amplitude noise or deviation from the ideal model. The out-of-range component is small, so we expect $S(f^{(3)})$ and $\\|\\mathbf{A}u^\\dagger - f^{(3)}\\|_2$ to be positive but significantly smaller than in Case 2.\n\nThe numerical implementation will now compute the five specified quantities for each of these three cases.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Numerically investigates the Fredholm alternative for a discrete integral operator.\n    \"\"\"\n    # 1. Define problem parameters and discretize the domain\n    N = 200\n    alpha = 1e-3\n    x = np.linspace(0, 1, N)\n    h = 1.0 / (N - 1)\n\n    # 2. Construct the discrete operator matrix A\n    # Quadrature weights for the composite trapezoidal rule\n    w = np.full(N, h)\n    w[0] = h / 2.0\n    w[-1] = h / 2.0\n    \n    # Kernel functions p(x) and q(y)=1\n    p_vec = np.sin(np.pi * x)\n    \n    # The matrix A with entries A_ij = p(x_i) * w_j\n    A = np.outer(p_vec, w)\n\n    # 3. Analyze the operator using SVD\n    U, s, Vt = np.linalg.svd(A, full_matrices=True)\n\n    # The basis for ker(A^T) is the set of left singular vectors \n    # corresponding to zero singular values.\n    # For this rank-1 matrix, these are all columns of U from the second one on.\n    Psi_basis = U[:, 1:]\n\n    # 4. Construct the Moore-Penrose pseudoinverse A_plus\n    # Set a tolerance for determining zero singular values\n    tol = s[0] * N * np.finfo(float).eps\n    s_inv = np.zeros_like(s)\n    # The matrix is rank-1, so only handle the first singular value\n    if s[0] > tol:\n        s_inv[0] = 1.0 / s[0]\n    \n    Sigma_plus = np.diag(s_inv)\n    A_plus = Vt.T @ Sigma_plus @ U.T\n\n    # 5. Define the test cases for the right-hand side f\n    f1 = p_vec  # Case 1: In-range\n    noise_vec = np.cos(2 * np.pi * x)\n    f2 = p_vec + 0.7 * noise_vec  # Case 2: Out-of-range\n    f3 = p_vec + 1e-3 * noise_vec # Case 3: Nearly in-range\n    \n    test_cases = [f1, f2, f3]\n    results = []\n\n    for f in test_cases:\n        # 6. Compute the required quantities for each case\n\n        # a. Solvability metric S(f)\n        # S(f) is the squared norm of the projection of f onto ker(A^T)\n        # Using f @ Psi_basis computes dot products of f with each basis vector\n        proj_coeffs = f @ Psi_basis\n        S_f = np.sum(proj_coeffs**2)\n\n        # b. Pseudoinverse solution and its residual\n        u_dagger = A_plus @ f\n        residual_dagger = np.linalg.norm(A @ u_dagger - f)\n        norm_u_dagger = np.linalg.norm(u_dagger)\n\n        # c. Tikhonov regularized solution and its residual\n        # Solve (A^T A + alpha*I) u = A^T f\n        LHS = A.T @ A + alpha * np.identity(N)\n        RHS = A.T @ f\n        u_alpha = linalg.solve(LHS, RHS, assume_a='sym')\n        \n        residual_alpha = np.linalg.norm(A @ u_alpha - f)\n        norm_u_alpha = np.linalg.norm(u_alpha)\n\n        # Append the five diagnostics to the results list\n        results.extend([\n            S_f,\n            residual_dagger,\n            norm_u_dagger,\n            residual_alpha,\n            norm_u_alpha\n        ])\n    \n    # 7. Print results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}