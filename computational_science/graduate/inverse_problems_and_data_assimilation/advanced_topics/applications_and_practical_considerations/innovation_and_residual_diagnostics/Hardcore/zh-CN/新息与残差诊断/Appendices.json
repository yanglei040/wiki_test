{
    "hands_on_practices": [
        {
            "introduction": "在数据同化中，诊断新息（innovation）和残差（residual）的统计特性是验证系统性能和调整误差协方差的关键环节。一个核心原则是，在最优分析的理想条件下，新息和残差的统计量会呈现出特定的、可预测的性质。本练习将引导您推导其中一个最基本的关系——Desroziers诊断 ，即新息向量 $d$ 和分析残差向量 $r_a$ 之间的协方差期望等于观测误差协方差矩阵 $R$。掌握这一推导，将帮助您深刻理解如何利用系统输出的统计量来估计通常难以直接获取的观测误差协方差 $R$。",
            "id": "3390983",
            "problem": "考虑一个线性高斯数据同化设定，其中，真实状态 $x \\in \\mathbb{R}^{n}$ 通过线性观测算子 $H \\in \\mathbb{R}^{m \\times n}$ 进行观测，观测误差为 $v \\in \\mathbb{R}^{m}$。观测方程为 $y = H x + v$，其中 $v$ 是无偏的，其协方差为 $R \\in \\mathbb{R}^{m \\times m}$。一个预报 $x_{f} \\in \\mathbb{R}^{n}$ 是可用的，其预报误差 $e_{f} = x - x_{f}$ 是无偏的，协方差为 $P_{f} \\in \\mathbb{R}^{n \\times n}$，并且与 $v$ 统计独立。分析由线性更新 $x_{a} = x_{f} + K d$ 形成，其中 $K \\in \\mathbb{R}^{n \\times m}$ 是一个增益矩阵，新息 $d \\in \\mathbb{R}^{m}$ 和分析残差 $r_{a} \\in \\mathbb{R}^{m}$ 分别定义为 $d = y - H x_{f}$ 和 $r_{a} = y - H x_{a}$。假设增益 $K$ 是根据最优线性估计器选择的，该估计器在卡尔曼滤波器（KF）的线性高斯假设下最小化分析误差协方差，并且 $e_{f}$ 和 $v$ 是独立且无偏的。\n\n从上述定义出发，仅使用无偏线性估计器和协方差在线性高斯 KF 假设下的标准属性，推导互协方差 $E[d r_{a}^{T}]$ 的一个用问题中的协方差表示的闭式表达式，并以解析形式求出其确切值。然后，简要解释该恒等式如何启发我们根据在 $N$ 个同化循环中产生的数据对 $\\{(d_{i}, r_{a,i})\\}_{i=1}^{N}$ 计算出的样本互协方差，来对观测误差协方差 $R$ 进行经验估计。\n\n你的最终答案必须是上面指定的 $E[d r_{a}^{T}]$ 的单一闭式解析表达式。不需要进行数值计算。如果你选择在解释中提出一个经验估计器，请不要将其包含在最终答案中。",
            "solution": "首先验证问题，以确保其具有科学依据、是良态的且客观。\n\n### 步骤 1：提取已知条件\n- 真实状态：$x \\in \\mathbb{R}^{n}$\n- 观测算子：$H \\in \\mathbb{R}^{m \\times n}$\n- 观测误差：$v \\in \\mathbb{R}^{m}$，满足 $E[v] = 0$，协方差为 $E[v v^T] = R \\in \\mathbb{R}^{m \\times m}$\n- 观测方程：$y = H x + v$\n- 预报：$x_{f} \\in \\mathbb{R}^{n}$\n- 预报误差：$e_{f} = x - x_{f}$，满足 $E[e_{f}] = 0$，协方差为 $E[e_{f} e_{f}^T] = P_{f} \\in \\mathbb{R}^{n \\times n}$\n- 独立性：$e_{f}$ 和 $v$ 统计独立，意味着 $E[e_{f} v^T] = 0$\n- 分析更新方程：$x_{a} = x_{f} + K d$\n- 增益矩阵：$K \\in \\mathbb{R}^{n \\times m}$\n- 新息：$d = y - H x_{f}$\n- 分析残差：$r_{a} = y - H x_{a}$\n- 关键假设：增益 $K$ 是最小化分析误差协方差的最优卡尔曼滤波器增益。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题设置在线性高斯数据同化的标准理论框架内，这是现代估计理论及其在物理科学和工程中应用的基石。所有术语都是标准的且定义明确。这些假设（无偏误差、独立性、最优增益）是推导该领域基础结果的典型假设。问题是自洽的，提供了所有必要信息。它在事实上和科学上都是合理的，其目标是推导残差诊断中的一个已知的、非平凡的恒等式。没有矛盾、歧义或对伪科学的依赖。\n\n### 步骤 3：结论与行动\n问题是有效的。可以基于给定的已知条件以及协方差和线性估计器的标准属性进行严格推导。\n\n### 推导\n我们的目标是推导互协方差 $E[d r_{a}^{T}]$ 的闭式表达式。我们首先用基本的不相关误差源，即预报误差 $e_{f}$ 和观测误差 $v$，来表示新息 $d$ 和分析残差 $r_{a}$。\n\n新息 $d$ 定义为 $d = y - H x_{f}$。代入观测方程 $y = Hx + v$，我们得到：\n$$\nd = (Hx + v) - Hx_{f} = H(x - x_{f}) + v\n$$\n使用预报误差的定义 $e_{f} = x - x_{f}$，上式简化为：\n$$\nd = H e_{f} + v\n$$\n分析残差 $r_{a}$ 定义为 $r_{a} = y - H x_{a}$。我们代入分析更新方程 $x_{a} = x_{f} + K d$：\n$$\nr_{a} = y - H(x_{f} + K d) = (y - H x_{f}) - H K d\n$$\n注意到括号中的项是新息 $d$ 的定义，我们发现分析残差和新息之间存在直接关系：\n$$\nr_{a} = d - H K d = (I - H K) d\n$$\n其中 $I$ 是大小为 $m \\times m$ 的单位矩阵。\n\n现在我们可以写出互协方差 $E[d r_{a}^{T}]$ 的表达式：\n$$\nE[d r_{a}^{T}] = E[d ((I - H K) d)^T]\n$$\n使用乘积转置的性质 $(AB)^T = B^T A^T$，我们得到：\n$$\nE[d r_{a}^{T}] = E[d d^T (I - H K)^T] = E[d d^T (I^T - (HK)^T)] = E[d d^T (I - K^T H^T)]\n$$\n由于 $I$、$K$ 和 $H$ 是常数矩阵，我们可以将 $(I - K^T H^T)$ 项移到期望之外：\n$$\nE[d r_{a}^{T}] = E[d d^T] (I - K^T H^T)\n$$\n项 $E[d d^T]$ 是新息协方差矩阵，我们将其记为 $S$。让我们使用 $d$ 的表达式来计算 $S$：\n$$\nS = E[d d^T] = E[(H e_{f} + v)(H e_{f} + v)^T] = E[H e_{f} e_{f}^T H^T + H e_{f} v^T + v e_{f}^T H^T + v v^T]\n$$\n根据期望的线性性质：\n$$\nS = H E[e_{f} e_{f}^T] H^T + H E[e_{f} v^T] + E[v e_{f}^T] H^T + E[v v^T]\n$$\n我们已知 $e_{f}$ 和 $v$ 是独立的且均值为零。这意味着它们的互协方差为零：$E[e_{f} v^T] = E[e_f]E[v^T] = 0$。类似地，$E[v e_{f}^T] = 0$。我们还已知预报误差协方差的定义 $P_{f} = E[e_{f} e_{f}^T]$ 和观测误差协方差的定义 $R = E[v v^T]$。将这些代入 $S$ 的表达式，得到新息协方差的标准公式：\n$$\nS = H P_{f} H^T + R\n$$\n现在，回到我们的互协方差表达式：\n$$\nE[d r_{a}^{T}] = S (I - K^T H^T) = S - S K^T H^T\n$$\n问题陈述指明增益矩阵 $K$ 在卡尔曼滤波器意义下是最优的。最优卡尔曼增益由下式给出：\n$$\nK = P_{f} H^T (H P_{f} H^T + R)^{-1} = P_{f} H^T S^{-1}\n$$\n我们将此最优增益代入我们的表达式中。首先，让我们求 $K$ 的转置：\n$$\nK^T = (P_{f} H^T S^{-1})^T = (S^{-1})^T (P_{f} H^T)^T = (S^T)^{-1} H P_{f}^T\n$$\n由于协方差矩阵是对称的，因此 $S=S^T$ 且 $P_{f}=P_{f}^T$。所以，$K^T = S^{-1} H P_{f}$。现在我们可以计算项 $S K^T H^T$：\n$$\nS K^T H^T = S (S^{-1} H P_{f}) H^T = (S S^{-1}) H P_{f} H^T = I H P_{f} H^T = H P_{f} H^T\n$$\n最后，我们将此结果代回到 $E[d r_{a}^{T}]$ 的表达式中：\n$$\nE[d r_{a}^{T}] = S - H P_{f} H^T\n$$\n代入 $S = H P_{f} H^T + R$ 的表达式：\n$$\nE[d r_{a}^{T}] = (H P_{f} H^T + R) - H P_{f} H^T = R\n$$\n此推导得出了互协方差的闭式解析表达式。\n\n### $R$ 的经验估计的动机\n恒等式 $E[d r_{a}^{T}] = R$ 是数据同化诊断的基石。它表明，新息向量（预报残差）和分析残差之间的互协方差的理论期望值恰好等于观测误差协方差矩阵 $R$。这个理论结果提供了一种从数据中估计 $R$ 的强大而实用的方法。\n\n根据大数定律，随着样本量的增加，样本均值会收敛到真实的总体均值。如果一个数据同化系统运行 $N$ 个循环，产生一系列新息向量 $\\{d_i\\}_{i=1}^N$ 和相应的分析残差向量 $\\{r_{a,i}\\}_{i=1}^N$，则可以通过计算样本互协方差来计算期望 $E[d r_{a}^{T}]$ 的经验估计值：\n$$\n\\hat{R} = \\frac{1}{N-1} \\sum_{i=1}^{N} d_i r_{a,i}^T\n$$\n这种方法被称为 Desroziers 诊断，它允许仅使用同化系统自身的输出来对 $R$ 进行后验估计。这在实践中非常有价值，因为 $R$（和 $P_f$）通常是未知的，必须进行调整。这种诊断提供了一种数据驱动的方法来检查和完善 $R$ 的设定。然而，认识到这一点至关重要：该估计方法的有效性取决于一个关键假设，即同化中使用的增益 $K$ 是最优的，而这又要求预报误差协方差 $P_f$ 和观测算子 $H$ 被正确设定。模型参数或增益的任何次优性都会使该恒等式失效，并导致对 $R$ 的有偏估计。",
            "answer": "$$\\boxed{R}$$"
        },
        {
            "introduction": "为了进行定量的统计检验，我们通常需要处理具有已知、简单分布的统计量。然而，原始的新息向量 $d$ 的各个分量往往是相关的，并且方差也不同，这使得直接进行检验变得复杂。本练习  介绍了一种关键技术——“预白化”（prewhitening），它利用新息协方差矩阵 $S$ 将相关的残差向量转换为一个新的向量 $\\tilde{d}$，其分量是统计独立且服从标准正态分布的。掌握这个具体的计算实践，对于实施如卡方检验（$\\chi^2$ test）等标准诊断工具至关重要，这些工具有助于评估新息的整体统计一致性。",
            "id": "3391019",
            "problem": "考虑一个数据同化中的线性观测模型，其中预报状态为 $x^{f} \\in \\mathbb{R}^{2}$，真实状态为 $x \\in \\mathbb{R}^{2}$，观测为 $y \\in \\mathbb{R}^{2}$，由 $y = H x + \\epsilon$ 给出。这里，$H \\in \\mathbb{R}^{2 \\times 2}$ 是线性观测算子，$\\epsilon \\in \\mathbb{R}^{2}$ 是观测误差。假设预报误差 $e^{f} = x - x^{f}$ 是一个协方差为 $P_{f} \\in \\mathbb{R}^{2 \\times 2}$ 的零均值高斯随机向量，观测误差 $\\epsilon$ 是一个协方差为 $R \\in \\mathbb{R}^{2 \\times 2}$ 的零均值高斯随机向量，且 $e^{f}$ 与 $\\epsilon$ 统计独立。新息（观测减预报残差）定义为 $d = y - H x^{f}$。在上述正确的模型假设下，$d$ 是一个协方差为 $S = H P_{f} H^{T} + R$ 的零均值高斯随机向量。\n\n在新息和残差诊断的背景下，我们寻求一种预白化变换，将 $d$ 映射到一个白化残差 $\\tilde{d}$，其分量是独立同分布的标准正态变量。使用 Cholesky 分解 $S = L L^{T}$（其中 $L$ 为对角元为正的下三角矩阵）来构造此预白化变换。具体地，取\n$$\nH = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}, \\quad\nP_{f} = \\begin{pmatrix} 3  1 \\\\ 1  1 \\end{pmatrix}, \\quad\nR = \\begin{pmatrix} 1  1 \\\\ 1  4 \\end{pmatrix}.\n$$\n\n任务：\n- 根据给定的 $H$、$P_{f}$ 和 $R$ 计算 $S$。\n- 构造 Cholesky 因子 $L$ 使得 $S = L L^{T}$，其中 $L$ 为对角元为正的下三角矩阵，并定义预白化变换 $\\tilde{d} = L^{-1} d$。\n- 从多元高斯随机向量线性变换的基本性质出发，在正确模型下推导 $\\tilde{d}$ 的分布。以显式闭式形式计算 $\\tilde{d}$ 的协方差矩阵。\n\n你的最终答案必须是 $\\tilde{d}$ 的协方差矩阵，表示为单个闭式解析表达式。不需要四舍五入，也不涉及单位。",
            "solution": "问题陈述已经过验证，被认为是有效的。它具有科学依据，问题设定良好且客观。解决问题所需的所有数据和定义均已提供，不存在内部矛盾或歧义。\n\n问题要求计算白化残差向量 $\\tilde{d}$ 的协方差矩阵。求解过程分为三步：\n1.  计算新息协方差矩阵 $S$。\n2.  对 $S$ 进行 Cholesky 分解，求出下三角矩阵 $L$。\n3.  推导白化残差 $\\tilde{d} = L^{-1} d$ 的协方差矩阵。\n\n**步骤 1：计算新息协方差矩阵 $S$**\n\n新息协方差矩阵 $S$ 由公式 $S = H P_{f} H^{T} + R$ 给出。\n给定的矩阵是：\n$H = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$，这是 $2 \\times 2$ 的单位矩阵 $I$。\n$P_{f} = \\begin{pmatrix} 3  1 \\\\ 1  1 \\end{pmatrix}$\n$R = \\begin{pmatrix} 1  1 \\\\ 1  4 \\end{pmatrix}$\n\n由于 $H$ 是单位矩阵，所以 $H P_{f} H^{T} = I P_{f} I^{T} = P_{f}$。\n因此，$S$ 的表达式简化为 $S = P_{f} + R$。\n代入给定的 $P_{f}$ 和 $R$ 矩阵：\n$$\nS = \\begin{pmatrix} 3  1 \\\\ 1  1 \\end{pmatrix} + \\begin{pmatrix} 1  1 \\\\ 1  4 \\end{pmatrix}\n$$\n执行矩阵加法：\n$$\nS = \\begin{pmatrix} 3+1  1+1 \\\\ 1+1  1+4 \\end{pmatrix} = \\begin{pmatrix} 4  2 \\\\ 2  5 \\end{pmatrix}\n$$\n矩阵 $S$ 是对称的。为确认其为正定矩阵，我们可以检查其行列式：$\\det(S) = (4)(5) - (2)(2) = 20 - 4 = 16 > 0$。由于第一主子式 ($4$) 也为正，$S$ 是正定的，因此存在唯一的 Cholesky 分解。\n\n**步骤 2：构造 Cholesky 因子 $L$**\n\n我们寻求一个对角元为正的下三角矩阵 $L$，使得 $S = L L^{T}$。设 $L$ 为：\n$$\nL = \\begin{pmatrix} l_{11}  0 \\\\ l_{21}  l_{22} \\end{pmatrix}\n$$\n那么 $L L^{T}$ 是：\n$$\nL L^{T} = \\begin{pmatrix} l_{11}  0 \\\\ l_{21}  l_{22} \\end{pmatrix} \\begin{pmatrix} l_{11}  l_{21} \\\\ 0  l_{22} \\end{pmatrix} = \\begin{pmatrix} l_{11}^{2}  l_{11} l_{21} \\\\ l_{11} l_{21}  l_{21}^{2} + l_{22}^{2} \\end{pmatrix}\n$$\n将 $L L^{T}$ 的元素与 $S$ 的元素相等：\n$$\n\\begin{pmatrix} l_{11}^{2}  l_{11} l_{21} \\\\ l_{11} l_{21}  l_{21}^{2} + l_{22}^{2} \\end{pmatrix} = \\begin{pmatrix} 4  2 \\\\ 2  5 \\end{pmatrix}\n$$\n根据 $(1,1)$ 元素：$l_{11}^{2} = 4$。由于 $l_{11}$ 必须为正，我们得到 $l_{11} = 2$。\n根据 $(2,1)$ 元素：$l_{11} l_{21} = 2$。代入 $l_{11} = 2$，得到 $2 l_{21} = 2$，解得 $l_{21} = 1$。\n根据 $(2,2)$ 元素：$l_{21}^{2} + l_{22}^{2} = 5$。代入 $l_{21} = 1$，得到 $1^{2} + l_{22}^{2} = 5$，所以 $l_{22}^{2} = 4$。由于 $l_{22}$ 必须为正，我们得到 $l_{22} = 2$。\n\n因此，Cholesky 因子为：\n$$\nL = \\begin{pmatrix} 2  0 \\\\ 1  2 \\end{pmatrix}\n$$\n\n**步骤 3：推导白化残差 $\\tilde{d}$ 的协方差矩阵**\n\n新息向量 $d = y - Hx^{f}$ 是高斯随机向量 $e^{f} = x - x^{f}$ 和 $\\epsilon$ 的线性组合。具体来说，$d = H(x-x^{f}) + \\epsilon = H e^{f} + \\epsilon$。如问题所述，并在验证中确认，$d$ 是一个协方差矩阵为 $S$ 的零均值高斯随机向量，即 $d \\sim \\mathcal{N}(0, S)$。\n\n白化残差 $\\tilde{d}$ 由线性变换 $\\tilde{d} = L^{-1} d$ 定义。\n多元高斯随机向量的一个基本性质指出，如果随机向量 $z \\sim \\mathcal{N}(\\mu, \\Sigma)$，那么线性变换 $w = A z$ 会得到一个同样是高斯的随机向量 $w$，其分布为 $w \\sim \\mathcal{N}(A\\mu, A\\Sigma A^{T})$。\n\n将此性质应用于 $\\tilde{d}$：\n- $\\tilde{d}$ 的均值为 $E[\\tilde{d}] = L^{-1} E[d] = L^{-1} \\cdot 0 = 0$。\n- $\\tilde{d}$ 的协方差矩阵，我们称之为 $\\text{Cov}(\\tilde{d})$，由下式给出：\n$$\n\\text{Cov}(\\tilde{d}) = (L^{-1}) S (L^{-1})^{T}\n$$\n我们将 $S = L L^{T}$ 代入此表达式：\n$$\n\\text{Cov}(\\tilde{d}) = L^{-1} (L L^{T}) (L^{-1})^{T}\n$$\n使用矩阵恒等式 $(A^{-1})^{T} = (A^{T})^{-1}$，我们可以将 $(L^{-1})^{T}$ 写为 $(L^{T})^{-1}$。\n$$\n\\text{Cov}(\\tilde{d}) = (L^{-1} L) (L^{T} (L^{T})^{-1})\n$$\n由于 $L^{-1}L = I$ 且 $L^{T}(L^{T})^{-1} = I$，其中 $I$ 是单位矩阵，该表达式简化为：\n$$\n\\text{Cov}(\\tilde{d}) = I \\cdot I = I\n$$\n此推导表明，白化残差的协方差矩阵是单位矩阵，这是预白化变换的预期结果。$\\tilde{d}$ 的分量是不相关的，且具有单位方差。由于 $\\tilde{d}$ 是高斯分布的，其分量也是独立的，并且是标准正态变量。\n\n为了提供显式的闭式答案，$\\tilde{d}$ 的协方差矩阵是 $2 \\times 2$ 的单位矩阵：\n$$\n\\text{Cov}(\\tilde{d}) = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\n$$\n这个结果是普适的。为完整起见，我们可以使用前面计算出的具体矩阵来验证它。\n首先，我们求 $L^{-1}$：\n$$\nL^{-1} = \\frac{1}{(2)(2) - (0)(1)} \\begin{pmatrix} 2  0 \\\\ -1  2 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 2  0 \\\\ -1  2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2}  0 \\\\ -\\frac{1}{4}  \\frac{1}{2} \\end{pmatrix}\n$$\n然后，我们计算 $\\text{Cov}(\\tilde{d}) = L^{-1} S (L^{-1})^{T}$：\n$$\nL^{-1} S = \\begin{pmatrix} \\frac{1}{2}  0 \\\\ -\\frac{1}{4}  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 4  2 \\\\ 2  5 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\\\ 0  2 \\end{pmatrix}\n$$\n注意，这个中间结果等于 $L^T$，正如从 $L^{-1}S = L^{-1}LL^T = L^T$ 所预期的那样。\n现在，我们右乘 $(L^{-1})^{T}$：\n$$\n(L^{-1})^{T} = \\begin{pmatrix} \\frac{1}{2}  -\\frac{1}{4} \\\\ 0  \\frac{1}{2} \\end{pmatrix}\n$$\n$$\n\\text{Cov}(\\tilde{d}) = (L^{-1}S)(L^{-1})^{T} = \\begin{pmatrix} 2  1 \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2}  -\\frac{1}{4} \\\\ 0  \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} (2)(\\frac{1}{2}) + (1)(0)  (2)(-\\frac{1}{4}) + (1)(\\frac{1}{2}) \\\\ (0)(\\frac{1}{2}) + (2)(0)  (0)(-\\frac{1}{4}) + (2)(\\frac{1}{2}) \\end{pmatrix}\n$$\n$$\n\\text{Cov}(\\tilde{d}) = \\begin{pmatrix} 1  -\\frac{1}{2} + \\frac{1}{2} \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\n$$\n显式计算证实了一般推导。$\\tilde{d}$ 的协方差矩阵是 $2 \\times 2$ 的单位矩阵。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  0 \\\\\n0  1\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "我们已经建立的理论工具在计算实践中有着强大的应用。一个典型的例子是控制用于求解反问题的迭代算法，以防止对数据中噪声的“过拟合”（overfitting）。当迭代次数过多时，解会开始拟合噪声而非真实的物理信号，因此，在残差（residual）的统计特性与预期的噪声水平一致时及时停止迭代，是至关重要的。\n\n本项高级实践  要求您实现一个智能的自动停止准则。您将把从预白化残差中得到的卡方（$\\chi^2$）统计量序列，与累积和（CUSUM）及广义似然比（GLR）等序贯变化检测方法相结合。这个练习将理论诊断与稳健的计算实践联系起来，展示了如何利用统计原理来提升算法的性能和可靠性。",
            "id": "3390997",
            "problem": "考虑一个迭代逆问题求解器，在每次迭代索引 $k$ 时产生一个残差（新息）向量 $v_k \\in \\mathbb{R}^m$ 和一个相关的正定残差协方差矩阵 $S_k \\in \\mathbb{R}^{m \\times m}$。定义残差的马氏范数平方为 $r_k = \\|v_k\\|_{S_k^{-1}}^2 = v_k^\\top S_k^{-1} v_k$。在噪声一致性残差的原假设下，假设残差服从高斯模型 $v_k \\sim \\mathcal{N}(0, S_k)$，这意味着 $r_k \\sim \\chi^2(m)$，其中 $m$ 是测量维度。对序列 $\\{r_k\\}_{k=1}^K$ 使用基于累积和 (CUSUM) 和广义似然比 (GLR) 的序贯变化检测，以定义一个自动停止准则，该准则在残差达到噪声水平时立即停止迭代，从而避免过拟合。\n\n从高斯残差假设和 $r_k$ 的定义出发，推导：\n- 每次迭代 $k$ 的单边 GLR 统计量，用于检验残差方差尺度相对于噪声一致性基线的增加，并描述如何通过一个小的逐次迭代阈值对序列进行门控，以确保 $r_k$ 在停止前的若干次连续迭代中表现得像 $\\chi^2(m)$ 分布。\n- 一种单边 CUSUM，旨在使用 $\\chi^2(m)$ 分布的基本性质，检测 $r_k$ 均值向噪声一致性水平的下降变化。\n\n然后实现一个组合停止规则，在满足以下两个条件的最小迭代索引 $k$ 处宣布停止：\n- GLR 门控条件在最近 $W$ 次迭代中成立，即直到索引 $k$ 为止的连续 $W$ 次迭代中，每次迭代的 GLR 统计量都低于容差 $\\tau_{\\mathrm{GLR}}$。\n- 下降 CUSUM 统计量在索引 $k$ 或之前已经超过阈值 $h_{\\downarrow}$。\n\n如果不存在这样的 $k \\leq K$，则返回整数 $-1$。\n\n你的程序必须为以下测试套件实现此停止准则。对于每个测试用例，给定 $m$、一个在 $k$ 上恒定的对角协方差 $S_k$ 以及一个预设的 $r_k$ 值目标序列。为确保科学真实性，构造 $v_k$ 使得在给定的 $S_k$ 下可以精确地达到 $r_k$。一种有效的构造方法是选择 $w_k \\in \\mathbb{R}^m$，其中 $w_k = [\\sqrt{r_k}, 0, \\dots, 0]^\\top$，并设置 $v_k = L w_k$，其中 $L$ 是 $S_k$ 的主平方根（对于对角矩阵 $S_k$，$L$ 是由逐元素平方根组成的对角矩阵）。验证计算出的 $r_k = v_k^\\top S_k^{-1} v_k$ 等于预设值。\n\n对所有测试用例使用以下固定的检测参数：窗口长度 $W = 3$，GLR 门控容差 $\\tau_{\\mathrm{GLR}} = 0.25$，以及下降 CUSUM 阈值 $h_{\\downarrow} = 0.08$。\n\n测试套件：\n- 案例 1：$m = 3$，$S_k = \\operatorname{diag}(1, 1, 1)$，$K = 15$，其 $r_k$ 序列为 $[12, 11, 10, 9, 8, 7, 5, 4, 3.5, 3.2, 3.1, 3.0, 2.9, 3.0, 3.1]$。\n- 案例 2：$m = 2$，$S_k = \\operatorname{diag}(1, 1)$，$K = 12$，其 $r_k$ 序列为 $[6.0, 5.8, 5.9, 6.1, 5.7, 5.8, 5.9, 6.0, 5.6, 5.7, 5.8, 6.0]$。\n- 案例 3：$m = 4$，$S_k = \\operatorname{diag}(1, 1, 1, 1)$，$K = 7$，其 $r_k$ 序列为 $[3.9, 3.8, 3.95, 4.05, 4.0, 3.98, 4.02]$。\n- 案例 4：$m = 3$，$S_k = \\operatorname{diag}(1, 1, 1)$，$K = 12$，其 $r_k$ 序列为 $[9.0, 8.5, 7.5, 6.5, 5.0, 4.0, 3.5, 3.2, 2.9, 2.8, 2.95, 2.9]$。\n\n你的程序应生成单行输出，其中包含四个案例的停止索引，以逗号分隔的列表形式并用方括号括起来，顺序如上所列（例如，$[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$）。每个案例的输出必须是整数，用 $-1$ 表示在范围 $K$ 内没有停止。",
            "solution": "该问题要求设计并实现一种序贯变化检测算法，作为迭代逆问题求解器的停止准则。其核心思想是在残差与测量噪声在统计上变得不可区分时停止求解器，从而防止过拟合。这是通过监视残差的马氏范数平方序列 $\\{r_k\\}_{k=1}^K$ 来实现的，其中 $r_k = v_k^\\top S_k^{-1} v_k$。在原假设下，即残差 $v_k$ 是从均值为零、协方差为 $S_k$ 的高斯分布（即 $v_k \\sim \\mathcal{N}(0, S_k)$）中抽取的样本，统计量 $r_k$ 服从自由度为 $m$ 的卡方分布，即 $r_k \\sim \\chi^2(m)$，其中 $m$ 是测量向量的维度。在此假设下，$r_k$ 的期望值为 $E[r_k] = m$。\n\n停止准则基于应用于序列 $\\{r_k\\}$ 的两个统计检验：广义似然比 (GLR) 检验和累积和 (CUSUM) 检验。\n\n### 单边广义似然比 (GLR) 统计量的推导\n\nGLR 检验用作一种门控机制，以确保残差范数 $r_k$ 不会过大，因为过大可能表示残差中仍存在显著的信号分量。我们检验残差方差尺度的增加。设残差的实际协方差为 $\\sigma^2 S_k$。原假设 $H_0$ 指出残差与噪声一致，对应于尺度因子 $\\sigma^2 = 1$。备择假设 $H_1$ 是残差大于噪声水平，对应于 $\\sigma^2 > 1$。\n\n对于给定的观测值 $r_k$，问题是检验 $H_0: \\sigma^2 = 1$ 与 $H_1: \\sigma^2 > 1$。如果 $v_k \\sim \\mathcal{N}(0, \\sigma^2 S_k)$，那么 $r_k = v_k^\\top S_k^{-1} v_k$ 服从 $\\sigma^2 \\chi^2(m)$ 分布。$r_k$ 的概率密度函数由下式给出\n$$p(r_k | \\sigma^2) = \\frac{1}{(\\sigma^2)^{m/2}} \\frac{(r_k)^{m/2 - 1} e^{-r_k/(2\\sigma^2)}}{2^{m/2} \\Gamma(m/2)}$$\n对数似然函数为\n$$\\ell(\\sigma^2; r_k) = -\\frac{m}{2} \\log(\\sigma^2) + (\\frac{m}{2}-1)\\log(r_k) - \\frac{r_k}{2\\sigma^2} - \\log(2^{m/2}\\Gamma(m/2))$$\n$\\sigma^2$ 的最大似然估计 (MLE) 可通过将 $\\ell$ 对 $\\sigma^2$ 的导数设为零求得：\n$$\\frac{\\partial \\ell}{\\partial (\\sigma^2)} = -\\frac{m}{2\\sigma^2} + \\frac{r_k}{2(\\sigma^2)^2} = 0 \\implies \\hat{\\sigma}^2_{\\text{MLE}} = \\frac{r_k}{m}$$\n广义似然比统计量是 $2 \\log \\Lambda$，其中 $\\Lambda = \\frac{\\sup_{\\sigma^2 \\in \\Theta_1} p(r_k|\\sigma^2)}{\\sup_{\\sigma^2 \\in \\Theta_0} p(r_k|\\sigma^2)}$。这里 $\\Theta_0 = \\{1\\}$ 且 $\\Theta_1 = \\{\\sigma^2 | \\sigma^2 > 1\\}$。在 $H_1$ 的受限空间下的 MLE 为 $\\hat{\\sigma}^2_{H_1} = \\max(1, \\hat{\\sigma}^2_{\\text{MLE}}) = \\max(1, r_k/m)$。\n\nGLR 统计量，记为 $g_k$，是：\n$$g_k = 2 \\left( \\ell(\\hat{\\sigma}^2_{H_1}; r_k) - \\ell(1; r_k) \\right)$$\n如果 $r_k/m \\le 1$，则 $\\hat{\\sigma}^2_{H_1} = 1$，且 $g_k = 0$。这表明没有证据支持方差尺度增加。\n如果 $r_k/m > 1$，则 $\\hat{\\sigma}^2_{H_1} = r_k/m$。将此代入对数似然差中：\n$$g_k = 2 \\left[ \\left(-\\frac{m}{2}\\log(\\frac{r_k}{m}) - \\frac{r_k}{2(r_k/m)}\\right) - \\left(-\\frac{m}{2}\\log(1) - \\frac{r_k}{2}\\right) \\right]$$\n$$g_k = 2 \\left[ -\\frac{m}{2}\\log(\\frac{r_k}{m}) - \\frac{m}{2} + \\frac{r_k}{2} \\right] = r_k - m - m\\log(\\frac{r_k}{m})$$\n因此，单边 GLR 统计量为：\n$$ g_k = \\begin{cases} r_k - m - m \\log(r_k/m)  \\text{若 } r_k > m \\\\ 0  \\text{若 } r_k \\le m \\end{cases} $$\nGLR 门控条件要求该统计量在连续 $W$ 次迭代中低于容差 $\\tau_{\\mathrm{GLR}}$。一个小的 $g_k$ 意味着 $r_k$ 接近其期望值 $m$，从而证实残差的行为类似于 $\\chi^2(m)$ 分布。\n\n### 下降 CUSUM 统计量的推导\n\nCUSUM 检验旨在检测 $\\{r_k\\}$ 序列均值的下降偏移。最初，由于残差中存在信号污染，$r_k$ 的值很大。随着迭代求解器的收敛，$r_k$ 的均值预计会向噪声一致性水平 $E[r_k] = m$ 降低。我们需要检测何时达到了这个水平。\n\n用于检测下降偏移的单边 Page CUSUM 统计量在这里是合适的。该统计量累积过程均值已降至目标水平以下的证据。设目标均值为 $\\mu_1 = m$。每次迭代的统计量测量 $r_k$ 与此目标的偏差。下降 CUSUM 统计量 $D_k^{\\downarrow}$ 的更新规则定义为一个递归过程：\n$$ D_0^{\\downarrow} = 0 $$\n$$ D_k^{\\downarrow} = \\max(0, D_{k-1}^{\\downarrow} + m - r_k) \\quad \\text{对于 } k=1, 2, \\dots, K $$\n这个统计量 $D_k^{\\downarrow}$ 在 $r_k$ 小于 $m$ 时累积“欠冲”量 $m - r_k$。如果 $r_k$ 持续大于或等于 $m$，该统计量将保持在或重置为 $0$。一段持续的 $r_k  m$ 时期将导致 $D_k^{\\downarrow}$ 增长。当 $D_k^{\\downarrow}$ 超过预定义阈值 $h_{\\downarrow}$ 时，即检测到变化。\n\n### 组合停止准则\n\n最终的停止规则结合了 GLR 门控和下降 CUSUM 检验。如果迭代 $k$ 是满足以下两个条件的最小索引 $k \\in \\{W, \\dots, K\\}$，则宣布其为停止点：\n\\begin{enumerate}\n    \\item **GLR 门控条件**：在最近 $W$ 次连续迭代中，GLR 统计量一直低于容差 $\\tau_{\\mathrm{GLR}}$。形式上，对于所有 $j \\in \\{k-W+1, \\dots, k\\}$，都有 $g_j  \\tau_{\\mathrm{GLR}}$。\n    \\item **CUSUM 穿越条件**：下降 CUSUM 统计量已超过其检测阈值。形式上，$D_k^{\\downarrow} \\ge h_{\\downarrow}$。\n\\end{enumerate}\n如果在最大迭代次数 $K$ 内不存在这样的索引 $k$，算法将返回 $-1$ 以表示不停止。算法通过计算序列 $\\{g_k\\}$ 和 $\\{D_k^{\\downarrow}\\}$，然后从 $k=W$ 迭代到 $K$，以检查两个条件首次同时满足的情况。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the combined CUSUM-GLR stopping criterion for a suite of test cases.\n    \"\"\"\n\n    # --- Fixed detection parameters ---\n    W = 3\n    tau_glr = 0.25\n    h_cusum_down = 0.08\n\n    # --- Test suite definition ---\n    test_cases = [\n        {\n            \"m\": 3,\n            \"S_k\": np.diag([1.0, 1.0, 1.0]),\n            \"K\": 15,\n            \"r_k_seq\": np.array([12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 5.0, 4.0, 3.5, 3.2, 3.1, 3.0, 2.9, 3.0, 3.1]),\n        },\n        {\n            \"m\": 2,\n            \"S_k\": np.diag([1.0, 1.0]),\n            \"K\": 12,\n            \"r_k_seq\": np.array([6.0, 5.8, 5.9, 6.1, 5.7, 5.8, 5.9, 6.0, 5.6, 5.7, 5.8, 6.0]),\n        },\n        {\n            \"m\": 4,\n            \"S_k\": np.diag([1.0, 1.0, 1.0, 1.0]),\n            \"K\": 7,\n            \"r_k_seq\": np.array([3.9, 3.8, 3.95, 4.05, 4.0, 3.98, 4.02]),\n        },\n        {\n            \"m\": 3,\n            \"S_k\": np.diag([1.0, 1.0, 1.0]),\n            \"K\": 12,\n            \"r_k_seq\": np.array([9.0, 8.5, 7.5, 6.5, 5.0, 4.0, 3.5, 3.2, 2.9, 2.8, 2.95, 2.9]),\n        },\n    ]\n\n    def compute_stop_index(m, r_k_seq, K, W, tau_glr, h_cusum_down):\n        \"\"\"\n        Computes the stopping index for a single test case.\n        \"\"\"\n        if K  W:\n            return -1\n\n        # --- Step 1: Compute GLR and CUSUM statistics for the whole sequence ---\n        glr_stats = np.zeros(K)\n        cusum_stats = np.zeros(K)\n\n        # GLR statistic calculation\n        for i in range(K):\n            r_k = r_k_seq[i]\n            if r_k > m:\n                # Use np.log for natural logarithm\n                glr_stats[i] = r_k - m - m * np.log(r_k / m)\n            else:\n                glr_stats[i] = 0\n\n        # Downward CUSUM statistic calculation\n        d_cusum_prev = 0.0\n        for i in range(K):\n            r_k = r_k_seq[i]\n            d_cusum_current = max(0, d_cusum_prev + m - r_k)\n            cusum_stats[i] = d_cusum_current\n            d_cusum_prev = d_cusum_current\n\n        # --- Step 2: Find the smallest k that satisfies both conditions ---\n        # Iterate k from W to K (using 1-based indexing for k)\n        for k in range(W, K + 1):\n            # Convert 1-based k to 0-based k_idx\n            k_idx = k - 1\n\n            # Condition 1: GLR Gate\n            # The window is for iterations {k-W+1, ..., k}\n            # In 0-based indices, this corresponds to slice [k-W, k-1] or [k_idx-W+1, k_idx]\n            glr_window = glr_stats[k_idx - W + 1 : k_idx + 1]\n            glr_condition_met = np.all(glr_window  tau_glr)\n\n            # Condition 2: CUSUM Crossing\n            cusum_condition_met = cusum_stats[k_idx] >= h_cusum_down\n            \n            if glr_condition_met and cusum_condition_met:\n                return k # Return the 1-based index\n\n        # If loop completes without finding a stopping point\n        return -1\n\n    results = []\n    for case in test_cases:\n        stop_index = compute_stop_index(\n            m=case[\"m\"],\n            r_k_seq=case[\"r_k_seq\"],\n            K=case[\"K\"],\n            W=W,\n            tau_glr=tau_glr,\n            h_cusum_down=h_cusum_down\n        )\n        results.append(stop_index)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}