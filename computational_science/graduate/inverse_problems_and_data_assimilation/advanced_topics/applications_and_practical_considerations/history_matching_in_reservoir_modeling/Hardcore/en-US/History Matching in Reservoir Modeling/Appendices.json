{
    "hands_on_practices": [
        {
            "introduction": "The success of history matching is fundamentally tied to the information content of the data we collect. This practice delves into the crucial pre-study phase of experimental design, where we decide which measurements will most effectively constrain our reservoir model parameters. By working through the principles of D-optimality and identifiability analysis, you will learn how to strategically plan a data acquisition campaign to maximize its value while adhering to operational constraints. ",
            "id": "3389162",
            "problem": "You are given a linearized data assimilation setting for history matching in reservoir modeling under a pilot-point log-permeability parameterization. The forward map from parameters to data is denoted by $G(m)$, where $m \\in \\mathbb{R}^{n_m}$ represents the log-permeability at pilot points. The sensitivity (Jacobian) matrix is $J(m_0) = \\partial G(m)/\\partial m \\big|_{m_0}$ evaluated at a background parameter $m_0$. Assume a single-phase, slightly compressible flow regime where, under linearization and superposition, measurement influence decays with spatial separation and saturates with time according to well-tested exponential forms.\n\nFundamental base and model specification:\n- The response at a well $w$ and time $t$ to a unit perturbation in the log-permeability at pilot point $i$ is proportional to a spatial kernel $K(d_{iw})$ and a temporal gain $T(t)$, where $d_{iw}$ is the Euclidean distance between pilot point $i$ and well $w$. Use $K(d) = \\exp(-d/L)$ and $T(t) = 1 - \\exp(-t/\\tau)$, where $L > 0$ is a spatial influence length and $\\tau > 0$ is a characteristic time constant. Under this assumption, the Jacobian row corresponding to measurement $(w,t)$ has entries\n$$\nJ_{(w,t),i} = K\\!\\left(d_{iw}\\right)\\, T(t) = \\exp\\!\\left(-\\frac{d_{iw}}{L}\\right) \\left(1 - \\exp\\!\\left(-\\frac{t}{\\tau}\\right)\\right).\n$$\n- Measurements have independent Gaussian noise with standard deviation $\\sigma_{(w,t)}$; measurement covariance $R$ is diagonal with entries $\\sigma_{(w,t)}^2$; the inverse covariance (weight) is $W = R^{-1}$. A Gaussian prior on $m$ yields a prior precision matrix $\\Gamma_{\\text{prior}}^{-1} = \\alpha I$, with $\\alpha > 0$.\n\nIdentifiability analysis:\n- Perform Singular Value Decomposition (SVD) of $J$ as $J = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{n_d \\times n_d}$, $\\Sigma \\in \\mathbb{R}^{n_d \\times n_m}$ has nonnegative diagonal singular values $s_1 \\ge s_2 \\ge \\dots$, and $V \\in \\mathbb{R}^{n_m \\times n_m}$. Define a threshold $\\epsilon \\in (0,1)$ and count the number of identifiable directions $N_{\\text{eff}}$ as the number of singular values strictly greater than $\\epsilon s_{\\max}$, i.e.,\n$$\nN_{\\text{eff}} = \\left|\\left\\{ i \\, : \\, s_i > \\epsilon \\, s_{\\max} \\right\\}\\right|, \\quad s_{\\max} = \\max_i s_i.\n$$\n\nD-optimal acquisition design:\n- Define the Fisher Information Matrix (FIM) for a selected subset $\\mathcal{S}$ of measurement opportunities as\n$$\nF(\\mathcal{S}) = \\Gamma_{\\text{prior}}^{-1} + \\sum_{(w,t) \\in \\mathcal{S}} \\frac{1}{\\sigma_{(w,t)}^2} \\, J_{(w,t)}^\\top J_{(w,t)},\n$$\nwhere $J_{(w,t)} \\in \\mathbb{R}^{n_m}$ denotes a row of $J$ for measurement $(w,t)$. The D-optimality criterion seeks to maximize $\\log\\det(F(\\mathcal{S}))$ subject to operational constraints:\n- A global budget constraint: select at most $K$ measurements.\n- Per-time limits: at time $t_k$, select at most $q_k$ measurements.\n- Unavailability: a set $\\mathcal{U}$ of measurement opportunities are not allowed to be selected.\n\nUse a greedy selection algorithm that starts from the prior precision $\\Gamma_{\\text{prior}}^{-1}$ and iteratively adds one admissible measurement at a time to increase $\\log\\det(F)$ the most, stopping when the budget is exhausted or no admissible measurement remains.\n\nEncoding of the acquisition schedule:\n- Enumerate all measurement opportunities with a fixed ordering: time-major, then well index within each time. Encode a selected subset $\\mathcal{S}$ as an integer bitmask $E = \\sum_{j \\in \\mathcal{I}(\\mathcal{S})} 2^j$, where $j$ indexes the measurement opportunity in the fixed ordering and $\\mathcal{I}(\\mathcal{S})$ gives the set of indices of selected measurements.\n\nTolerance:\n- Use $\\epsilon = 10^{-2}$ to define the identifiability threshold.\n\nYour tasks:\n1. Construct $J$ using the specified $K(d)$ and $T(t)$ for each test case, with distances $d_{iw}$ computed from given coordinates.\n2. Compute $N_{\\text{eff}}$ from the SVD of $J$ using the threshold rule with $\\epsilon = 10^{-2}$.\n3. Perform greedy D-optimal acquisition under the constraints to produce the optimized $\\log\\det(F)$ and the encoded schedule integer $E$.\n\nTest suite and parameters:\n- Pilot points are fixed across all cases with $n_m = 5$ and coordinates\n$$\np_0 = (0.0, 0.0), \\quad p_1 = (1.0, 0.0), \\quad p_2 = (0.0, 1.0), \\quad p_3 = (1.0, 1.0), \\quad p_4 = (0.5, 0.6).\n$$\n- Wells are fixed across all cases with $n_w = 4$ and coordinates\n$$\nw_0 = (0.1, 0.2), \\quad w_1 = (0.9, 0.1), \\quad w_2 = (0.2, 0.8), \\quad w_3 = (0.8, 0.9).\n$$\n\nCase $1$ (happy path):\n- Times $t \\in \\{0.5, 1.0, 2.0\\}$, so $n_t = 3$ and $n_d = n_t \\cdot n_w = 12$.\n- Spatial length $L = 0.5$, temporal constant $\\tau = 0.8$.\n- Noise base per well: $\\sigma_{\\text{base}} = [0.04, 0.05, 0.04, 0.05]$; per measurement at time $t$, use $\\sigma_{(w,t)} = \\sigma_{\\text{base},w} \\sqrt{1 + t}$.\n- Prior precision scalar $\\alpha = 0.01$ so $\\Gamma_{\\text{prior}}^{-1} = \\alpha I$.\n- Global budget $K = 5$.\n- Per-time limits $(q_0, q_1, q_2) = (2, 2, 1)$.\n- Unavailable set $\\mathcal{U} = \\{(t = 1.0, w = 3)\\}$, i.e., the well with index $3$ at time $1.0$ cannot be measured.\n\nCase $2$ (boundary constraints, zero acquisition):\n- Times $t \\in \\{0.5\\}$, so $n_t = 1$ and $n_d = 4$.\n- Spatial length $L = 0.5$, temporal constant $\\tau = 0.6$.\n- Noise base per well: $\\sigma_{\\text{base}} = [0.2, 0.2, 0.2, 0.2]$; per measurement $\\sigma_{(w,t)} = \\sigma_{\\text{base},w} \\sqrt{1 + t}$.\n- Prior precision scalar $\\alpha = 0.02$ so $\\Gamma_{\\text{prior}}^{-1} = \\alpha I$.\n- Global budget $K = 0$.\n- Per-time limit $(q_0) = (0)$.\n- Unavailable set $\\mathcal{U} = \\emptyset$.\n\nCase $3$ (near rank-deficiency and limited per-time acquisitions):\n- Times $t \\in \\{0.2, 0.3, 0.4\\}$, so $n_t = 3$ and $n_d = 12$.\n- Spatial length $L = 1.5$, temporal constant $\\tau = 1.5$.\n- Noise base per well: $\\sigma_{\\text{base}} = [0.1, 0.1, 0.12, 0.1]$; per measurement $\\sigma_{(w,t)} = \\sigma_{\\text{base},w} \\sqrt{1 + t}$.\n- Prior precision scalar $\\alpha = 0.0001$ so $\\Gamma_{\\text{prior}}^{-1} = \\alpha I$.\n- Global budget $K = 2$.\n- Per-time limits $(q_0, q_1, q_2) = (1, 1, 0)$.\n- Unavailable set $\\mathcal{U} = \\emptyset$.\n\nAngle units are not applicable. No physical unit conversion is required; all quantities are dimensionless.\n\nRequired output specification:\n- For each case, compute and return three values in order: the identifiable count $N_{\\text{eff}}$ (an integer), the optimized $\\log\\det(F)$ (a float), and the encoded schedule integer $E$ (an integer). Aggregate the results from all three cases into a single list ordered by case and within case by the specified value order. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{result}_1,\\text{result}_2,\\dots,\\text{result}_9]$).",
            "solution": "The problem requires the analysis of a linearized data assimilation setup for reservoir history matching. The solution involves three primary tasks for three distinct test cases: (1) construction of the sensitivity (Jacobian) matrix, (2) identifiability analysis via Singular Value Decomposition (SVD), and (3) D-optimal experimental design using a greedy algorithm.\n\nThe methodological approach for each case is as follows:\n\nFirst, we establish the geometric configuration. The Euclidean distances, $d_{iw}$, between each of the $n_m=5$ pilot points, $p_i$, and $n_w=4$ wells, $w_w$, are computed. These distances are constant across all test cases. The coordinates are provided as:\n- Pilot points: $p_0 = (0.0, 0.0), p_1 = (1.0, 0.0), p_2 = (0.0, 1.0), p_3 = (1.0, 1.0), p_4 = (0.5, 0.6)$.\n- Wells: $w_0 = (0.1, 0.2), w_1 = (0.9, 0.1), w_2 = (0.2, 0.8), w_3 = (0.8, 0.9)$.\n\nThe distance $d_{iw}$ is calculated as $d_{iw} = \\sqrt{(p_{ix} - w_{wx})^2 + (p_{iy} - w_{wy})^2}$.\n\n**1. Jacobian Matrix Construction**\n\nThe Jacobian matrix $J$, with dimensions $n_d \\times n_m$, quantifies the sensitivity of the measurements to perturbations in the log-permeability parameters. The number of measurements is $n_d = n_t \\cdot n_w$, where $n_t$ is the number of measurement times and $n_w$ is the number of wells. The problem specifies a phenomenological model for the entries of $J$. The entry corresponding to a measurement at well $w$ and time $t$, and the $i$-th pilot point parameter, is given by:\n$$\nJ_{(w,t),i} = K(d_{iw}) T(t) = \\exp\\left(-\\frac{d_{iw}}{L}\\right) \\left(1 - \\exp\\left(-\\frac{t}{\\tau}\\right)\\right)\n$$\nwhere $L$ is the spatial influence length and $\\tau$ is the characteristic time constant. For each case, we construct the full $n_d \\times n_m$ matrix $J$ by computing this value for all measurement opportunities (indexed time-major, then well-major) and all pilot points.\n\n**2. Identifiability Analysis**\n\nThe number of effectively identifiable parameter combinations, $N_{\\text{eff}}$, is estimated from the singular value spectrum of the Jacobian matrix $J$. We perform a Singular Value Decomposition (SVD) of $J$:\n$$\nJ = U \\Sigma V^\\top\n$$\nwhere $\\Sigma$ is a rectangular diagonal matrix with non-negative singular values $s_1 \\ge s_2 \\ge \\dots \\ge s_{\\min(n_d, n_m)} \\ge 0$ on its diagonal. The number of identifiable directions is defined as the count of singular values that are strictly greater than a threshold fraction $\\epsilon$ of the maximum singular value, $s_{\\max} = s_1$. Given $\\epsilon = 10^{-2}$, we compute:\n$$\nN_{\\text{eff}} = \\left|\\left\\{ i \\, : \\, s_i > 10^{-2} s_{\\max} \\right\\}\\right|\n$$\nThis value provides insight into the practical rank of the Jacobian and the number of independent parameter directions that can be constrained by the data.\n\n**3. D-Optimal Acquisition Design**\n\nThe goal is to select a subset of measurements $\\mathcal{S}$ that is maximally informative about the parameters, under a set of constraints. We use the D-optimality criterion, which aims to maximize the determinant of the Fisher Information Matrix (FIM). The FIM for a set $\\mathcal{S}$ is:\n$$\nF(\\mathcal{S}) = \\Gamma_{\\text{prior}}^{-1} + \\sum_{(w,t) \\in \\mathcal{S}} \\frac{1}{\\sigma_{(w,t)}^2} J_{(w,t)}^\\top J_{(w,t)}\n$$\nwhere $\\Gamma_{\\text{prior}}^{-1} = \\alpha I$ is the prior precision matrix, $\\sigma_{(w,t)}^2$ is the measurement variance, and $J_{(w,t)}$ is the row of the Jacobian corresponding to the measurement $(w,t)$. Maximizing $\\det(F(\\mathcal{S}))$ is equivalent to minimizing the volume of the posterior uncertainty ellipsoid for the parameters. For numerical stability and convenience, we maximize its logarithm, $\\log\\det(F(\\mathcal{S}))$.\n\nSince finding the optimal $\\mathcal{S}$ is a combinatorial problem, we employ a greedy algorithm:\n1.  Initialize the FIM with the prior information: $F_{current} = \\Gamma_{\\text{prior}}^{-1}$. The set of selected measurements is initially empty, $\\mathcal{S} = \\emptyset$.\n2.  Iteratively add measurements one by one, up to the global budget $K$. In each iteration:\n    a. Identify the set of admissible candidate measurements. A measurement is admissible if it is not in the unavailable set $\\mathcal{U}$, has not already been selected, and its selection does not violate the per-time acquisition limits.\n    b. For each admissible candidate $j$, calculate the potential gain in the objective function: $\\Delta_j = \\log\\det(F_{current} + \\frac{1}{\\sigma_j^2} J_j^\\top J_j) - \\log\\det(F_{current})$.\n    c. Select the measurement $j^*$ that yields the maximum gain, $\\arg\\max_j \\Delta_j$.\n    d. Update the FIM: $F_{current} \\leftarrow F_{current} + \\frac{1}{\\sigma_{j^*}^2} J_{j^*}^\\top J_{j^*}$. Add $j^*$ to $\\mathcal{S}$ and update the count of measurements taken at the corresponding time.\n3.  The process terminates when $K$ measurements are selected or no more admissible measurements can be added.\n4.  The final $\\log\\det(F_{current})$ is the optimized value. The selected schedule is encoded as a bitmask integer $E = \\sum_{j \\in \\mathcal{S}} 2^j$, where $j$ is the zero-based index of the measurement in the time-major, well-major enumeration.\n\nThis procedure is applied to each of the three test cases with their specific parameters ($L, \\tau, \\alpha, K$, etc.).\n\n**Case 1:** A standard scenario with a budget of $K=5$, mixed per-time limits, and one unavailable measurement. All three tasks are performed as described.\n\n**Case 2:** A boundary case where the acquisition budget $K=0$ and the per-time limit is also $0$. The identifiability analysis is performed on the Jacobian. For the acquisition design, no measurements are selected. The final FIM is simply the prior precision matrix $\\Gamma_{\\text{prior}}^{-1}$, and the schedule encoding $E$ is $0$.\n\n**Case 3:** A case designed to be ill-conditioned, with parameters $L$ and $\\tau$ leading to a nearly rank-deficient Jacobian. The budget is small ($K=2$), and per-time limits are restrictive, with one time slot completely disallowed. The standard procedure is followed.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the reservoir modeling problem for three test cases.\n    For each case, it computes:\n    1. Neff: The number of identifiable parameter directions.\n    2. log_det_F: The D-optimal log-determinant of the Fisher Information Matrix.\n    3. E: The integer encoding of the optimal measurement schedule.\n    \"\"\"\n    pilot_points = np.array([\n        [0.0, 0.0], [1.0, 0.0], [0.0, 1.0], [1.0, 1.0], [0.5, 0.6]\n    ])\n    wells = np.array([\n        [0.1, 0.2], [0.9, 0.1], [0.2, 0.8], [0.8, 0.9]\n    ])\n\n    n_m = pilot_points.shape[0]\n    n_w = wells.shape[0]\n\n    # Pre-compute distance matrix d_iw\n    dist_matrix = np.zeros((n_m, n_w))\n    for i in range(n_m):\n        for w in range(n_w):\n            dist_matrix[i, w] = np.linalg.norm(pilot_points[i] - wells[w])\n\n    test_cases = [\n        # Case 1\n        {\n            \"times\": [0.5, 1.0, 2.0],\n            \"L\": 0.5, \"tau\": 0.8,\n            \"sigma_base\": [0.04, 0.05, 0.04, 0.05],\n            \"alpha\": 0.01,\n            \"K\": 5,\n            \"q_k\": [2, 2, 1],\n            \"unavailable\": [(1.0, 3)],\n        },\n        # Case 2\n        {\n            \"times\": [0.5],\n            \"L\": 0.5, \"tau\": 0.6,\n            \"sigma_base\": [0.2, 0.2, 0.2, 0.2],\n            \"alpha\": 0.02,\n            \"K\": 0,\n            \"q_k\": [0],\n            \"unavailable\": [],\n        },\n        # Case 3\n        {\n            \"times\": [0.2, 0.3, 0.4],\n            \"L\": 1.5, \"tau\": 1.5,\n            \"sigma_base\": [0.1, 0.1, 0.12, 0.1],\n            \"alpha\": 0.0001,\n            \"K\": 2,\n            \"q_k\": [1, 1, 0],\n            \"unavailable\": [],\n        },\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        times = case[\"times\"]\n        L, tau = case[\"L\"], case[\"tau\"]\n        sigma_base = np.array(case[\"sigma_base\"])\n        alpha, K = case[\"alpha\"], case[\"K\"]\n        q_k = case[\"q_k\"]\n        unavailable_spec = case[\"unavailable\"]\n        \n        n_t = len(times)\n        n_d = n_t * n_w\n\n        # --- 1. Construct Jacobian J ---\n        J = np.zeros((n_d, n_m))\n        sigmas = np.zeros(n_d)\n        \n        time_map = {t: i for i, t in enumerate(times)}\n        unavailable_indices = {time_map[t] * n_w + w for t, w in unavailable_spec}\n\n        for t_idx, t_val in enumerate(times):\n            temporal_gain = 1.0 - np.exp(-t_val / tau)\n            for w_idx in range(n_w):\n                meas_idx = t_idx * n_w + w_idx\n                spatial_kernel = np.exp(-dist_matrix[:, w_idx] / L)\n                J[meas_idx, :] = spatial_kernel * temporal_gain\n                sigmas[meas_idx] = sigma_base[w_idx] * np.sqrt(1.0 + t_val)\n\n        # --- 2. Compute N_eff ---\n        if J.shape[0] > 0:\n            singular_values = np.linalg.svd(J, compute_uv=False)\n            s_max = singular_values[0] if len(singular_values) > 0 else 0.0\n            if s_max > 0:\n                N_eff = np.sum(singular_values > 1e-2 * s_max)\n            else:\n                N_eff = 0\n        else:\n            N_eff = 0\n\n        # --- 3. Greedy D-optimal Design ---\n        F = alpha * np.identity(n_m)\n        selected_indices = set()\n        \n        counts_per_time = np.zeros(n_t, dtype=int)\n        \n        available_indices = set(range(n_d)) - unavailable_indices\n\n        for _ in range(K):\n            best_gain = -1.0\n            best_idx = -1\n            \n            candidate_indices = []\n            for idx in available_indices:\n                t_idx = idx // n_w\n                if counts_per_time[t_idx] < q_k[t_idx]:\n                    candidate_indices.append(idx)\n            \n            if not candidate_indices:\n                break\n            \n            _, current_log_det_F = np.linalg.slogdet(F)\n\n            for idx in candidate_indices:\n                J_row = J[idx, :].reshape(1, -1)\n                rank_one_update = (J_row.T @ J_row) / (sigmas[idx]**2)\n                F_candidate = F + rank_one_update\n                \n                _, log_det_candidate = np.linalg.slogdet(F_candidate)\n                gain = log_det_candidate - current_log_det_F\n                \n                if gain > best_gain:\n                    best_gain = gain\n                    best_idx = idx\n\n            if best_idx != -1:\n                J_row = J[best_idx, :].reshape(1, -1)\n                rank_one_update = (J_row.T @ J_row) / (sigmas[best_idx]**2)\n                F += rank_one_update\n                \n                selected_indices.add(best_idx)\n                available_indices.remove(best_idx)\n                \n                t_idx = best_idx // n_w\n                counts_per_time[t_idx] += 1\n            else:\n                # No admissible measurement can be added\n                break\n\n        _, final_log_det_F = np.linalg.slogdet(F)\n        \n        E = 0\n        for idx in selected_indices:\n            E += 2**idx\n\n        all_results.extend([int(N_eff), final_log_det_F, int(E)])\n\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Real-world measurement data is rarely perfect and can be contaminated by outliers that corrupt the history matching process. This exercise explores robust data assimilation by contrasting the standard Gaussian error model with a heavy-tailed Student's $t$-distribution, which is less sensitive to anomalous data points. Through a combination of analytical derivation and numerical implementation, you will investigate how the choice of likelihood function fundamentally alters the influence of large residuals and leads to more reliable parameter estimates. ",
            "id": "3389148",
            "problem": "You are modeling history matching for a single-well production rate in a reservoir using a simplified decline-curve forward model. Let the model parameter vector be $m = [k,a]^{\\top}$, where $k$ is a positive amplitude and $a$ is a positive decline rate. Let the forward operator at observation times $t_i$ be $h_i(m) = k \\exp(-a t_i)$, and $h(m) \\in \\mathbb{R}^{n}$ stacks these components for $i = 1,\\dots,n$. Observations $y^{\\mathrm{obs}} \\in \\mathbb{R}^{n}$ are related to the model via $y^{\\mathrm{obs}} = h(m^{\\star}) + \\epsilon$, where $m^{\\star}$ is the unknown true parameter and $\\epsilon$ is measurement error. In robust data assimilation for history matching, a Student’s $t$-distribution with degrees of freedom $\\nu$ is often used for the measurement error to mitigate outlier influence.\n\nStarting from the definition of the Student’s $t$ probability density function (PDF) for independent residuals with degrees of freedom $\\nu$ and a known positive scale $s$ for each observation, and using only fundamental calculus rules (chain rule, product rule) and the definition of the likelihood function, do the following:\n\n1) Derive the negative log-likelihood for the residual vector $r(m) = y^{\\mathrm{obs}} - h(m)$ under independent Student’s $t$ components and compute its gradient with respect to $m$ in terms of the Jacobian $J(m) = \\partial h(m)/\\partial m$. Clearly identify the componentwise “influence function” that maps each residual component $r_i$ to the corresponding contribution in the gradient via the chain rule.\n\n2) For comparison, repeat the derivation for independent Gaussian measurement errors with variance $s^2$ per component, and identify the corresponding influence function.\n\n3) Analyze the qualitative difference between the Student’s $t$ and Gaussian influence functions for large-magnitude residuals (outliers) and for small-magnitude residuals. Explain which likelihood downweights outliers and why.\n\n4) Implement a program that computes, for specified parameter values, the Euclidean norm of the gradient of the negative log-likelihood under the Student’s $t$ model and under the Gaussian model, along with ratios of absolute influence magnitudes for an outlier residual and for a typical residual. Use the following concrete and fully specified setup:\n- Observation times $t = [0,1,2,3,4]$.\n- True parameter $m^{\\star} = [1.5, 0.4]^{\\top}$.\n- Initial guess for linearization and evaluation $m_0 = [1.2, 0.5]^{\\top}$.\n- Baseline noise-free observations $y^{\\mathrm{base}} = h(m^{\\star})$.\n- One outlier is injected at index $i_{\\mathrm{out}} = 2$ (that is, the third observation), by adding a scalar offset $\\Delta_{\\mathrm{out}}$ to that component: $y^{\\mathrm{obs}} = y^{\\mathrm{base}}$ with $y^{\\mathrm{obs}}[i_{\\mathrm{out}}] \\leftarrow y^{\\mathrm{obs}}[i_{\\mathrm{out}}] + \\Delta_{\\mathrm{out}}$.\n- Typical residual index $i_{\\mathrm{typ}} = 0$.\n- Independent and identically distributed scale $s = 0.1$ for all observations.\n- Use the exact Jacobian of $h(m)$ with respect to $m$ implied by the forward model definition.\n\nDefine the test suite as the following four cases, each specified by $(\\nu, \\Delta_{\\mathrm{out}})$:\n- Case A: $(\\nu = 3.0, \\Delta_{\\mathrm{out}} = 1.5)$.\n- Case B: $(\\nu = 30.0, \\Delta_{\\mathrm{out}} = 1.5)$.\n- Case C: $(\\nu = 1.0, \\Delta_{\\mathrm{out}} = 1.5)$.\n- Case D: $(\\nu = 3.0, \\Delta_{\\mathrm{out}} = 0.0)$.\n\nFor each case, compute the following four quantities evaluated at $m_0$:\n- $g^{(t)}$-norm: the Euclidean norm of the gradient of the Student’s $t$ negative log-likelihood.\n- $g^{(g)}$-norm: the Euclidean norm of the gradient of the Gaussian negative log-likelihood.\n- Outlier influence ratio: $\\left|\\psi^{(t)}(r_{i_{\\mathrm{out}}})\\right| \\big/ \\left|\\psi^{(g)}(r_{i_{\\mathrm{out}}})\\right|$, where $\\psi^{(t)}$ and $\\psi^{(g)}$ are the Student’s $t$ and Gaussian influence functions, respectively.\n- Typical influence ratio: $\\left|\\psi^{(t)}(r_{i_{\\mathrm{typ}}})\\right| \\big/ \\left|\\psi^{(g)}(r_{i_{\\mathrm{typ}}})\\right|$.\n\nAll quantities are dimensionless; no physical units are required. The final program must produce a single line of output containing the results for the four cases as a comma-separated list of lists, each inner list containing the four floats in the order specified above. For example, the output format must be exactly of the form\n\"[[g_t_A,g_g_A,ratio_out_A,ratio_typ_A],[g_t_B,g_g_B,ratio_out_B,ratio_typ_B],[g_t_C,g_g_C,ratio_out_C,ratio_typ_C],[g_t_D,g_g_D,ratio_out_D,ratio_typ_D]]\"\nwith no additional spaces or text. All numeric results must be printed as standard decimal representations.",
            "solution": "We begin with the observation model $y^{\\mathrm{obs}} = h(m^{\\star}) + \\epsilon$ and define the residual $r(m) = y^{\\mathrm{obs}} - h(m) \\in \\mathbb{R}^{n}$. The negative log-likelihood functional for data assimilation is defined by $-\\log p(y^{\\mathrm{obs}} \\mid m)$, up to an additive constant independent of $m$. Its gradient with respect to $m$ is obtained by the chain rule, using the Jacobian $J(m) = \\partial h(m)/\\partial m \\in \\mathbb{R}^{n \\times p}$, where $p$ is the number of parameters.\n\nDerivation for Student’s $t$ errors. Assume independent residual components, each following a Student’s $t$ distribution with degrees of freedom $\\nu$ and identical positive scale $s$. The probability density function (PDF) of one component is\n$$\np(r_i) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}\\, s} \\left(1 + \\frac{r_i^2}{\\nu s^2}\\right)^{-\\frac{\\nu+1}{2}},\n$$\nwhere $\\Gamma(\\cdot)$ is the Gamma function. The negative log-likelihood for independent components is\n$$\n\\ell^{(t)}(m) = -\\sum_{i=1}^{n} \\log p(r_i(m)) = C + \\frac{\\nu+1}{2} \\sum_{i=1}^{n} \\log\\left(1 + \\frac{r_i(m)^2}{\\nu s^2}\\right),\n$$\nwhere $C$ is a constant independent of $m$. Differentiating $\\ell^{(t)}(m)$ with respect to $m$ and using the chain rule $\\, \\mathrm{d}r_i/\\mathrm{d}m = -J_i(m)$, where $J_i(m)$ denotes the $i$-th row of $J(m)$, yields\n$$\n\\nabla_m \\ell^{(t)}(m) = \\sum_{i=1}^{n} \\left[ \\frac{\\nu+1}{2} \\cdot \\frac{1}{1 + \\frac{r_i^2}{\\nu s^2}} \\cdot \\frac{2 r_i}{\\nu s^2} \\right] \\left(-J_i(m) \\right)\n= - J(m)^{\\top} \\psi^{(t)}(r(m)),\n$$\nwhere the influence function vector $\\psi^{(t)}(r) \\in \\mathbb{R}^{n}$ is defined componentwise by\n$$\n\\psi^{(t)}_i(r_i) = \\frac{(\\nu+1) r_i}{\\nu s^2 + r_i^2}.\n$$\n\nDerivation for Gaussian errors. For independent Gaussian errors with variance $s^2$, each component has PDF\n$$\np(r_i) = \\frac{1}{\\sqrt{2\\pi} s} \\exp\\left(-\\frac{r_i^2}{2 s^2}\\right).\n$$\nThe negative log-likelihood is\n$$\n\\ell^{(g)}(m) = C' + \\frac{1}{2} \\sum_{i=1}^{n} \\frac{r_i(m)^2}{s^2},\n$$\nwith $C'$ independent of $m$. Differentiation and the chain rule give\n$$\n\\nabla_m \\ell^{(g)}(m) = \\sum_{i=1}^{n} \\left( \\frac{r_i}{s^2} \\right) \\left( -J_i(m) \\right)\n= - J(m)^{\\top} \\psi^{(g)}(r(m)),\n$$\nwhere the Gaussian influence function is\n$$\n\\psi^{(g)}_i(r_i) = \\frac{r_i}{s^2}.\n$$\n\nInfluence analysis. The influence function magnitudes determine how each residual component contributes to the gradient. For small-magnitude residuals with $|r_i| \\ll s \\sqrt{\\nu}$, we have\n$$\n\\psi^{(t)}_i(r_i) \\approx \\frac{\\nu+1}{\\nu s^2} r_i,\n$$\nwhich is approximately linear in $r_i$, similar to the Gaussian case. For large-magnitude residuals with $|r_i| \\gg s \\sqrt{\\nu}$, the Student’s $t$ influence saturates and decays as\n$$\n\\psi^{(t)}_i(r_i) \\approx \\frac{\\nu+1}{r_i},\n$$\nso its magnitude behaves like $(\\nu+1)/|r_i|$ and thus decreases with larger outliers. In contrast, the Gaussian influence function grows linearly with $|r_i|$ as $|r_i|/s^2$. Therefore, the Student’s $t$ likelihood downweights outliers strongly, while the Gaussian likelihood amplifies their effect.\n\nForward operator and Jacobian. With $h_i(m) = k \\exp(-a t_i)$ and $m = [k,a]^{\\top}$, the Jacobian rows are\n$$\n\\frac{\\partial h_i}{\\partial k} = \\exp(-a t_i), \\quad \\frac{\\partial h_i}{\\partial a} = -k t_i \\exp(-a t_i).\n$$\nStacking these over $i=1,\\dots,n$ yields $J(m) \\in \\mathbb{R}^{n \\times 2}$.\n\nAlgorithmic procedure to compute requested outputs for each test case $(\\nu, \\Delta_{\\mathrm{out}})$:\n1) Construct $t = [0,1,2,3,4]$ and compute $y^{\\mathrm{base}} = h(m^{\\star})$ with $m^{\\star} = [1.5, 0.4]^{\\top}$.\n2) Form $y^{\\mathrm{obs}}$ by copying $y^{\\mathrm{base}}$ and adding $\\Delta_{\\mathrm{out}}$ to index $i_{\\mathrm{out}} = 2$.\n3) At $m_0 = [1.2, 0.5]^{\\top}$, compute residuals $r = y^{\\mathrm{obs}} - h(m_0)$ and Jacobian $J(m_0)$.\n4) Compute $\\psi^{(t)}(r)$ using $\\psi^{(t)}_i(r_i) = (\\nu+1) r_i / (\\nu s^2 + r_i^2)$ with $s = 0.1$, and $\\psi^{(g)}(r)$ using $\\psi^{(g)}_i(r_i) = r_i / s^2$.\n5) Compute gradients $\\nabla_m \\ell^{(t)}(m_0) = - J(m_0)^{\\top} \\psi^{(t)}(r)$ and $\\nabla_m \\ell^{(g)}(m_0) = - J(m_0)^{\\top} \\psi^{(g)}(r)$, and their Euclidean norms.\n6) Compute the outlier influence ratio $\\left|\\psi^{(t)}(r_{i_{\\mathrm{out}}})\\right| / \\left|\\psi^{(g)}(r_{i_{\\mathrm{out}}})\\right|$ and the typical influence ratio $\\left|\\psi^{(t)}(r_{i_{\\mathrm{typ}}})\\right| / \\left|\\psi^{(g)}(r_{i_{\\mathrm{typ}}})\\right|$ with $i_{\\mathrm{typ}} = 0$.\n7) Return the four floats for each test case in the specified single-line list-of-lists format.\n\nQualitative expectations. For $(\\nu = 3.0, \\Delta_{\\mathrm{out}} = 1.5)$ and $(\\nu = 1.0, \\Delta_{\\mathrm{out}} = 1.5)$, the outlier influence ratio should be markedly below $1$ due to strong downweighting, with the $\\nu = 1.0$ case yielding the smallest ratio. For $(\\nu = 30.0, \\Delta_{\\mathrm{out}} = 1.5)$, the outlier influence ratio should be closer to $1$, reflecting near-Gaussian behavior. When $\\Delta_{\\mathrm{out}} = 0.0$, both ratios should be close to $1$ because residuals are not dominated by outliers and both models behave similarly near the origin.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef forward_model(m, t):\n    \"\"\"\n    Forward operator h(m): h_i = k * exp(-a * t_i)\n    m: array-like, shape (2,) with m = [k, a]\n    t: array-like of times\n    Returns y_pred of shape (n_times,)\n    \"\"\"\n    k, a = m\n    t = np.asarray(t, dtype=float)\n    return k * np.exp(-a * t)\n\ndef jacobian(m, t):\n    \"\"\"\n    Jacobian J(m) of h(m) with respect to m = [k, a]\n    J[i, 0] = d h_i / d k = exp(-a * t_i)\n    J[i, 1] = d h_i / d a = -k * t_i * exp(-a * t_i)\n    \"\"\"\n    k, a = m\n    t = np.asarray(t, dtype=float)\n    exp_term = np.exp(-a * t)\n    J = np.zeros((t.size, 2), dtype=float)\n    J[:, 0] = exp_term\n    J[:, 1] = -k * t * exp_term\n    return J\n\ndef influence_student_t(r, s, nu):\n    \"\"\"\n    Student-t influence function psi^(t)_i = ((nu+1) * r_i) / (nu * s^2 + r_i^2)\n    r: residual vector\n    s: scalar scale\n    nu: degrees of freedom (scalar)\n    \"\"\"\n    r = np.asarray(r, dtype=float)\n    denom = nu * (s ** 2) + r**2\n    return (nu + 1.0) * r / denom\n\ndef influence_gaussian(r, s):\n    \"\"\"\n    Gaussian influence function psi^(g)_i = r_i / s^2\n    \"\"\"\n    r = np.asarray(r, dtype=float)\n    return r / (s ** 2)\n\ndef gradient_from_influence(m, t, psi):\n    \"\"\"\n    Compute gradient of negative log-likelihood: grad = - J^T * psi\n    \"\"\"\n    J = jacobian(m, t)\n    return - J.T @ psi\n\ndef run_case(nu, delta_out):\n    # Setup constants\n    t = np.array([0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n    m_true = np.array([1.5, 0.4], dtype=float)\n    m0 = np.array([1.2, 0.5], dtype=float)\n    s = 0.1\n    i_out = 2\n    i_typ = 0\n\n    # Build observations with specified outlier\n    y_base = forward_model(m_true, t)\n    y_obs = y_base.copy()\n    y_obs[i_out] += delta_out\n\n    # Residual at m0\n    r = y_obs - forward_model(m0, t)\n\n    # Influences\n    psi_t = influence_student_t(r, s, nu)\n    psi_g = influence_gaussian(r, s)\n\n    # Gradients\n    grad_t = gradient_from_influence(m0, t, psi_t)\n    grad_g = gradient_from_influence(m0, t, psi_g)\n\n    # Norms\n    g_t_norm = float(np.linalg.norm(grad_t))\n    g_g_norm = float(np.linalg.norm(grad_g))\n\n    # Influence ratios\n    # Guard against division by zero in ratios; if denominator is zero, define ratio as np.nan\n    denom_out = abs(psi_g[i_out])\n    denom_typ = abs(psi_g[i_typ])\n    ratio_out = float(abs(psi_t[i_out]) / denom_out) if denom_out != 0.0 else float(\"nan\")\n    ratio_typ = float(abs(psi_t[i_typ]) / denom_typ) if denom_typ != 0.0 else float(\"nan\")\n\n    return [g_t_norm, g_g_norm, ratio_out, ratio_typ]\n\ndef solve():\n    # Define the test cases from the problem statement: (nu, delta_out)\n    test_cases = [\n        (3.0, 1.5),   # Case A\n        (30.0, 1.5),  # Case B\n        (1.0, 1.5),   # Case C\n        (3.0, 0.0),   # Case D\n    ]\n\n    results = []\n    for nu, delta_out in test_cases:\n        result = run_case(nu, delta_out)\n        # Optionally round to a reasonable number of decimals for stable textual output\n        rounded = [round(x, 10) if isinstance(x, float) and np.isfinite(x) else x for x in result]\n        results.append(rounded)\n\n    # Format as required: single line, list of lists, comma-separated, no extra spaces\n    def fmt_list(lst):\n        return \"[\" + \",\".join(str(x) for x in lst) + \"]\"\n    out = \"[\" + \",\".join(fmt_list(r) for r in results) + \"]\"\n    print(out)\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Uncertainty in reservoir modeling extends beyond just the parameter fields; it often includes the empirical laws, such as the petrophysical relationship between permeability and porosity. This advanced practice introduces hierarchical Bayesian modeling, a powerful method to account for this structural uncertainty by treating the parameters of the petrophysical model themselves as random variables to be inferred. You will implement a full hierarchical update and analyze the resulting posterior distribution to understand how assimilating data induces correlations between the reservoir properties and the parameters of the underlying physical law. ",
            "id": "3389135",
            "problem": "Consider a single-phase, incompressible, steady-state reservoir flow in a discretized one-dimensional domain with $N$ grid cells. Let $k$ denote the cell-wise permeability and $\\phi$ denote the cell-wise porosity. To account for uncertainty in the petrophysical relationship between permeability and porosity, introduce a nuisance model in log-space,\n$ \\log k_i = \\beta_0 + \\beta_1 \\log \\phi_i + m_i $,\nwhere $i \\in \\{1,\\dots,N\\}$ indexes the grid cells, $\\beta = [\\beta_0,\\beta_1]^\\top$ are hyperparameters of the petrophysical relationship, and $m \\in \\mathbb{R}^N$ are cell-wise deviations from that relationship.\n\nAssume the following fundamentals and modeling choices:\n- Conservation of mass and Darcy’s law imply that, for small perturbations around a background state and fixed boundary conditions, linearization of the flow map with respect to $\\log k$ yields a data model of the form\n$ y = H \\, \\log k + \\varepsilon $,\nwhere $y \\in \\mathbb{R}^M$ are observed data (for example, pressures or rates), $H \\in \\mathbb{R}^{M \\times N}$ is a known linear sensitivity matrix, and $\\varepsilon \\sim \\mathcal{N}(0, R)$ is additive Gaussian noise with covariance $R \\in \\mathbb{R}^{M \\times M}$.\n- Substitute the petrophysical relationship into the data model to obtain\n$ y = H \\, (P \\, \\beta + m) + \\varepsilon $,\nwhere $P \\in \\mathbb{R}^{N \\times 2}$ is the design matrix with $P_{i,0} = 1$ and $P_{i,1} = \\log \\phi_i$ for $i \\in \\{1,\\dots,N\\}$.\n- Place independent Gaussian priors on $m$ and $\\beta$: $m \\sim \\mathcal{N}(0, C_m)$ and $\\beta \\sim \\mathcal{N}(\\mu_\\beta, C_\\beta)$, where $C_m \\in \\mathbb{R}^{N \\times N}$ and $C_\\beta \\in \\mathbb{R}^{2 \\times 2}$ are positive definite covariance matrices, and $\\mu_\\beta \\in \\mathbb{R}^2$ is the prior mean of $\\beta$.\n\nTask: Starting from Bayes’ rule and the properties of multivariate Gaussian distributions, derive the hierarchical Bayesian update for the joint posterior distribution of $(m, \\beta)$ conditioned on $y$ and $\\phi$. Your derivation must begin from the stated linear-Gaussian likelihood and independent Gaussian priors, and must not assume any pre-derived update formulas. Then, implement a program that:\n- Constructs the matrices and vectors for each test case,\n- Forms the joint prior on $\\theta = [m^\\top, \\beta^\\top]^\\top$,\n- Computes the posterior mean and covariance of $\\theta$ given the data $y$,\n- Extracts the posterior correlation matrix from the posterior covariance,\n- Computes as a scalar summary of posterior coupling between $m$ and $\\beta$ the maximum absolute posterior correlation between any component of $m$ and any component of $\\beta$.\n\nSpecifically, define the scalar coupling metric $c$ as\n$ c = \\max_{i \\in \\{1,\\dots,N\\}, \\, j \\in \\{1,2\\}} \\left| \\rho_{m_i, \\beta_j} \\right| $,\nwhere $\\rho_{m_i, \\beta_j}$ is the correlation coefficient between $m_i$ and $\\beta_j$ computed from the joint posterior covariance.\n\nUse the following global prior specifications for all test cases:\n- Prior mean of $\\beta$: $\\mu_\\beta = \\begin{bmatrix} -11.0 \\\\ 2.5 \\end{bmatrix}$,\n- Prior covariance of $\\beta$: $C_\\beta = \\mathrm{diag}\\left([25.0, 4.0]\\right)$,\n- Prior covariance of $m$: $C_m = \\tau_m^2 I_N$ with $\\tau_m = 1.0$.\n\nFor each test case, the program must first generate $y$ deterministically from a specified ground-truth parameter set $(m^{\\mathrm{true}}, \\beta^{\\mathrm{true}})$ using the exact linear forward model $y = H (P \\beta^{\\mathrm{true}} + m^{\\mathrm{true}})$ (that is, set $\\varepsilon = 0$ to ensure reproducibility), and then perform the posterior update using the specified $R$.\n\nProvide the following four test cases to ensure coverage of different regimes.\n\n- Test case $1$ (general informative case):\n  - $N = 3$, $M = 2$,\n  - $\\phi = [0.22, 0.18, 0.20]$,\n  - $H = \\begin{bmatrix} 1.0 & 0.5 & 0.0 \\\\ 0.0 & 0.5 & 1.0 \\end{bmatrix}$,\n  - $R = \\mathrm{diag}([10^{-4}, 10^{-4}])$,\n  - $\\beta^{\\mathrm{true}} = \\begin{bmatrix} -11.5 \\\\ 2.8 \\end{bmatrix}$,\n  - $m^{\\mathrm{true}} = [0.05, -0.03, 0.00]$.\n\n- Test case $2$ (weakly informative data):\n  - $N = 3$, $M = 2$,\n  - $\\phi = [0.25, 0.15, 0.30]$,\n  - $H = \\begin{bmatrix} 0.5 & 0.2 & 0.1 \\\\ 0.0 & 1.0 & 0.3 \\end{bmatrix}$,\n  - $R = \\mathrm{diag}([10.0, 10.0])$,\n  - $\\beta^{\\mathrm{true}} = \\begin{bmatrix} -11.5 \\\\ 2.8 \\end{bmatrix}$,\n  - $m^{\\mathrm{true}} = [-0.02, 0.04, -0.01]$.\n\n- Test case $3$ (non-identifiable summary observation):\n  - $N = 3$, $M = 1$,\n  - $\\phi = [0.25, 0.18, 0.15]$,\n  - $H = \\begin{bmatrix} 1.0 & 1.0 & 1.0 \\end{bmatrix}$,\n  - $R = \\mathrm{diag}([10^{-6}])$,\n  - $\\beta^{\\mathrm{true}} = \\begin{bmatrix} -11.5 \\\\ 2.8 \\end{bmatrix}$,\n  - $m^{\\mathrm{true}} = [0.02, -0.01, 0.00]$.\n\n- Test case $4$ (direct observation of $\\log k$):\n  - $N = 3$, $M = 3$,\n  - $\\phi = [0.22, 0.18, 0.20]$,\n  - $H = I_3$,\n  - $R = \\mathrm{diag}([10^{-6}, 10^{-6}, 10^{-6}])$,\n  - $\\beta^{\\mathrm{true}} = \\begin{bmatrix} -11.5 \\\\ 2.8 \\end{bmatrix}$,\n  - $m^{\\mathrm{true}} = [0.00, 0.03, -0.02]$.\n\nImplementation requirements:\n- For each test case, construct $P$ with entries $P_{i,0} = 1$ and $P_{i,1} = \\log \\phi_i$, where $\\log$ is the natural logarithm with no units.\n- Form the joint prior on $\\theta = [m^\\top, \\beta^\\top]^\\top$ with mean $\\mu_\\theta = [0^\\top, \\mu_\\beta^\\top]^\\top$ and block-diagonal covariance $\\Sigma_\\theta = \\mathrm{blkdiag}(C_m, C_\\beta)$.\n- Use the specified $H$, $P$, $R$, and the synthetic $y$ generated from the ground truth to compute the posterior mean and covariance for $\\theta$ under the linear-Gaussian model.\n- From the posterior covariance, compute the posterior correlation matrix and then the coupling metric $c$ as defined above.\n\nFinal output format:\n- Your program should produce a single line of output containing the coupling metrics for test cases $1$ through $4$ as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places, for example, $[0.123456,0.234567,0.345678,0.456789]$.",
            "solution": "### Derivation of the Joint Posterior Distribution\n\nThe problem is to find the posterior distribution of the joint parameter vector $(m, \\beta)$ given the data $y$ and porosity $\\phi$. We start from Bayes' rule:\n$$\np(m, \\beta | y, \\phi) \\propto p(y | m, \\beta, \\phi) \\, p(m, \\beta | \\phi)\n$$\nThe problem states that the priors on $m$ and $\\beta$ are independent, hence $p(m, \\beta | \\phi) = p(m) p(\\beta)$. The prior on $m$ does not depend on $\\phi$.\n\nLet us define a single parameter vector $\\theta \\in \\mathbb{R}^{N+2}$ by concatenating $m$ and $\\beta$:\n$$\n\\theta = \\begin{bmatrix} m \\\\ \\beta \\end{bmatrix}\n$$\nThe data model is given by $y = H (P \\beta + m) + \\varepsilon$. We can rewrite this in a standard linear form $y = G \\theta + \\varepsilon$ by defining a new matrix $G \\in \\mathbb{R}^{M \\times (N+2)}$.\nThe term $P\\beta + m$ can be expressed as:\n$$\nP\\beta + m = I_N m + P\\beta = \\begin{bmatrix} I_N & P \\end{bmatrix} \\begin{bmatrix} m \\\\ \\beta \\end{bmatrix} = \\begin{bmatrix} I_N & P \\end{bmatrix} \\theta\n$$\nwhere $I_N$ is the $N \\times N$ identity matrix.\nSubstituting this into the data model gives:\n$$\ny = H \\left( \\begin{bmatrix} I_N & P \\end{bmatrix} \\theta \\right) + \\varepsilon = G \\theta + \\varepsilon\n$$\nwhere we have defined the forward model matrix $G = H \\begin{bmatrix} I_N & P \\end{bmatrix}$.\n\nThe likelihood function is the probability density of the observed data given the parameters $\\theta$. Since the noise $\\varepsilon$ is Gaussian with mean $0$ and covariance $R$, i.e., $\\varepsilon \\sim \\mathcal{N}(0, R)$, the likelihood is:\n$$\np(y|\\theta) = \\mathcal{N}(G\\theta, R) \\propto \\exp\\left( -\\frac{1}{2} (y - G\\theta)^\\top R^{-1} (y - G\\theta) \\right)\n$$\nThe priors are given as $m \\sim \\mathcal{N}(0, C_m)$ and $\\beta \\sim \\mathcal{N}(\\mu_\\beta, C_\\beta)$. Since they are independent, the joint prior on $\\theta$ is also Gaussian:\n$$\np(\\theta) = p(m)p(\\beta) = \\mathcal{N}(\\mu_\\theta, \\Sigma_\\theta)\n$$\nwith a block-structured mean vector $\\mu_\\theta$ and covariance matrix $\\Sigma_\\theta$:\n$$\n\\mu_\\theta = \\begin{bmatrix} 0 \\\\ \\mu_\\beta \\end{bmatrix}, \\quad \\Sigma_\\theta = \\begin{bmatrix} C_m & 0 \\\\ 0 & C_\\beta \\end{bmatrix}\n$$\nThe prior probability density is:\n$$\np(\\theta) \\propto \\exp\\left( -\\frac{1}{2} (\\theta - \\mu_\\theta)^\\top \\Sigma_\\theta^{-1} (\\theta - \\mu_\\theta) \\right)\n$$\nNow we apply Bayes' rule. The posterior distribution $p(\\theta|y)$ is proportional to the product of the likelihood and the prior:\n$$\np(\\theta|y) \\propto \\exp\\left( -\\frac{1}{2} (y - G\\theta)^\\top R^{-1} (y - G\\theta) \\right) \\exp\\left( -\\frac{1}{2} (\\theta - \\mu_\\theta)^\\top \\Sigma_\\theta^{-1} (\\theta - \\mu_\\theta) \\right)\n$$\nThe product of two exponentials is the exponential of the sum of their arguments. Let's focus on the exponent, which we denote as $J(\\theta)$ up to a factor of $-1/2$:\n$$\nJ(\\theta) = (y - G\\theta)^\\top R^{-1} (y - G\\theta) + (\\theta - \\mu_\\theta)^\\top \\Sigma_\\theta^{-1} (\\theta - \\mu_\\theta)\n$$\nExpanding the quadratic forms:\n$$\nJ(\\theta) = (y^\\top R^{-1} y - 2y^\\top R^{-1} G\\theta + \\theta^\\top G^\\top R^{-1} G\\theta) + (\\theta^\\top \\Sigma_\\theta^{-1} \\theta - 2\\mu_\\theta^\\top \\Sigma_\\theta^{-1} \\theta + \\mu_\\theta^\\top \\Sigma_\\theta^{-1} \\mu_\\theta)\n$$\nCollecting terms with respect to $\\theta$:\n$$\nJ(\\theta) = \\theta^\\top (G^\\top R^{-1} G + \\Sigma_\\theta^{-1}) \\theta - 2(y^\\top R^{-1} G + \\mu_\\theta^\\top \\Sigma_\\theta^{-1})\\theta + \\text{constant terms}\n$$\nThe resulting posterior distribution $p(\\theta|y)$ is a multivariate Gaussian, $\\mathcal{N}(\\mu_{\\text{post}}, \\Sigma_{\\text{post}})$, whose density function is proportional to $\\exp\\left(-\\frac{1}{2} (\\theta - \\mu_{\\text{post}})^\\top \\Sigma_{\\text{post}}^{-1} (\\theta - \\mu_{\\text{post}})\\right)$. Expanding this gives:\n$$\n(\\theta - \\mu_{\\text{post}})^\\top \\Sigma_{\\text{post}}^{-1} (\\theta - \\mu_{\\text{post}}) = \\theta^\\top \\Sigma_{\\text{post}}^{-1} \\theta - 2\\mu_{\\text{post}}^\\top \\Sigma_{\\text{post}}^{-1} \\theta + \\text{constant}\n$$\nBy comparing the coefficients of the quadratic and linear terms in $\\theta$ between this form and our expression for $J(\\theta)$, we can identify the posterior inverse covariance and mean.\nThe quadratic term gives the posterior inverse covariance matrix:\n$$\n\\Sigma_{\\text{post}}^{-1} = G^\\top R^{-1} G + \\Sigma_\\theta^{-1}\n$$\nThe posterior covariance is therefore:\n$$\n\\Sigma_{\\text{post}} = (G^\\top R^{-1} G + \\Sigma_\\theta^{-1})^{-1}\n$$\nThe linear term gives the relation:\n$$\n\\mu_{\\text{post}}^\\top \\Sigma_{\\text{post}}^{-1} = y^\\top R^{-1} G + \\mu_\\theta^\\top \\Sigma_\\theta^{-1}\n$$\nTaking the transpose of both sides (and noting that covariance matrices and their inverses are symmetric) yields:\n$$\n\\Sigma_{\\text{post}}^{-1} \\mu_{\\text{post}} = (y^\\top R^{-1} G)^\\top + (\\mu_\\theta^\\top \\Sigma_\\theta^{-1})^\\top = G^\\top R^{-1} y + \\Sigma_\\theta^{-1} \\mu_\\theta\n$$\nFinally, solving for the posterior mean $\\mu_{\\text{post}}$:\n$$\n\\mu_{\\text{post}} = \\Sigma_{\\text{post}} (G^\\top R^{-1} y + \\Sigma_\\theta^{-1} \\mu_\\theta)\n$$\nThis completes the derivation of the posterior mean and covariance for the joint parameter vector $\\theta$.\n\n### Implementation Strategy\n\nThe program will execute the following steps for each test case:\n1.  **Construct Model Components**: Form the matrices and vectors $P$, $G$, $\\mu_\\theta$, and $\\Sigma_\\theta$ based on the given parameters $N, M, \\phi, H$ and the global prior specifications. The matrix $P \\in \\mathbb{R}^{N \\times 2}$ is built with $P_{i,0} = 1$ and $P_{i,1} = \\log \\phi_i$. The forward matrix is then $G = H \\begin{bmatrix} I_N & P \\end{bmatrix}$. The prior mean is $\\mu_\\theta = [0, \\dots, 0, \\mu_{\\beta_0}, \\mu_{\\beta_1}]^\\top$ and prior covariance $\\Sigma_\\theta$ is a block-diagonal matrix constructed from $C_m$ and $C_\\beta$.\n2.  **Generate Synthetic Data**: Create the observation vector $y$ deterministically using the noise-free forward model: $y = H (P \\beta^{\\mathrm{true}} + m^{\\mathrm{true}})$.\n3.  **Compute Posterior Covariance**:\n    -   Calculate the inverse matrices $R^{-1}$ and $\\Sigma_\\theta^{-1}$. Since $R$ and $\\Sigma_\\theta$ are diagonal in all test cases, this is computationally simple.\n    -   Compute the posterior inverse covariance matrix $\\Sigma_{\\text{post}}^{-1} = G^\\top R^{-1} G + \\Sigma_\\theta^{-1}$.\n    -   Invert this matrix to obtain the posterior covariance $\\Sigma_{\\text{post}} = (\\Sigma_{\\text{post}}^{-1})^{-1}$.\n4.  **Compute Posterior Correlation and Coupling Metric**:\n    -   From $\\Sigma_{\\text{post}}$, calculate the posterior correlation matrix $\\rho_{\\text{post}}$. The correlation between $\\theta_i$ and $\\theta_j$ is given by $\\rho_{ij} = (\\Sigma_{\\text{post}})_{ij} / \\sqrt{(\\Sigma_{\\text{post}})_{ii} (\\Sigma_{\\text{post}})_{jj}}$.\n    -   The parameters $m$ correspond to the first $N$ indices of $\\theta$ (indices $0, ..., N-1$) and $\\beta$ to the last two (indices $N, N+1$). The cross-correlations between $m$ and $\\beta$ form an $N \\times 2$ sub-block of $\\rho_{\\text{post}}$.\n    -   Find the maximum absolute value within this $N \\times 2$ sub-block. This is the desired coupling metric $c$.\n\nThis procedure will be applied to each of the four test cases specified.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the hierarchical Bayesian inference problem for the given test cases.\n    \"\"\"\n\n    # Global prior specifications\n    mu_beta = np.array([-11.0, 2.5])\n    C_beta = np.diag([25.0, 4.0])\n    tau_m = 1.0\n\n    # Test cases\n    test_cases = [\n        {\n            \"N\": 3, \"M\": 2,\n            \"phi\": np.array([0.22, 0.18, 0.20]),\n            \"H\": np.array([[1.0, 0.5, 0.0], [0.0, 0.5, 1.0]]),\n            \"R\": np.diag([1e-4, 1e-4]),\n            \"beta_true\": np.array([-11.5, 2.8]),\n            \"m_true\": np.array([0.05, -0.03, 0.00])\n        },\n        {\n            \"N\": 3, \"M\": 2,\n            \"phi\": np.array([0.25, 0.15, 0.30]),\n            \"H\": np.array([[0.5, 0.2, 0.1], [0.0, 1.0, 0.3]]),\n            \"R\": np.diag([10.0, 10.0]),\n            \"beta_true\": np.array([-11.5, 2.8]),\n            \"m_true\": np.array([-0.02, 0.04, -0.01])\n        },\n        {\n            \"N\": 3, \"M\": 1,\n            \"phi\": np.array([0.25, 0.18, 0.15]),\n            \"H\": np.array([[1.0, 1.0, 1.0]]),\n            \"R\": np.diag([1e-6]),\n            \"beta_true\": np.array([-11.5, 2.8]),\n            \"m_true\": np.array([0.02, -0.01, 0.00])\n        },\n        {\n            \"N\": 3, \"M\": 3,\n            \"phi\": np.array([0.22, 0.18, 0.20]),\n            \"H\": np.identity(3),\n            \"R\": np.diag([1e-6, 1e-6, 1e-6]),\n            \"beta_true\": np.array([-11.5, 2.8]),\n            \"m_true\": np.array([0.00, 0.03, -0.02])\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        N = case[\"N\"]\n        M = case[\"M\"]\n        phi = case[\"phi\"]\n        H = case[\"H\"]\n        R = case[\"R\"]\n        beta_true = case[\"beta_true\"]\n        m_true = case[\"m_true\"]\n\n        # 1. Construct Model Components\n        # Design matrix P for beta\n        P = np.ones((N, 2))\n        P[:, 1] = np.log(phi)\n\n        # Forward model matrix G for the joint parameter theta = [m, beta]\n        G = H @ np.hstack([np.identity(N), P])\n\n        # Prior covariance for m\n        C_m = (tau_m**2) * np.identity(N)\n\n        # Joint prior mean mu_theta and covariance Sigma_theta\n        mu_theta = np.concatenate([np.zeros(N), mu_beta])\n        Sigma_theta = np.block([\n            [C_m, np.zeros((N, 2))],\n            [np.zeros((2, N)), C_beta]\n        ])\n\n        # 2. Generate Synthetic Data\n        y = H @ (P @ beta_true + m_true)\n\n        # 3. Compute Posterior Covariance\n        R_inv = np.linalg.inv(R)\n        Sigma_theta_inv = np.linalg.inv(Sigma_theta)\n\n        # Posterior inverse covariance\n        Sigma_post_inv = G.T @ R_inv @ G + Sigma_theta_inv\n\n        # Posterior covariance\n        Sigma_post = np.linalg.inv(Sigma_post_inv)\n\n        # 4. Compute Posterior Correlation and Coupling Metric\n        # Calculate posterior standard deviations\n        std_devs_post = np.sqrt(np.diag(Sigma_post))\n        \n        # Build inverse diagonal matrix of standard deviations\n        D_inv = np.diag(1.0 / std_devs_post)\n\n        # Posterior correlation matrix\n        rho_post = D_inv @ Sigma_post @ D_inv\n\n        # Extract the N x 2 sub-block of correlations between m and beta\n        # m corresponds to indices 0..N-1, beta to N..N+1\n        rho_m_beta = rho_post[0:N, N:N+2]\n        \n        # Compute the coupling metric c\n        c = np.max(np.abs(rho_m_beta))\n        results.append(c)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}