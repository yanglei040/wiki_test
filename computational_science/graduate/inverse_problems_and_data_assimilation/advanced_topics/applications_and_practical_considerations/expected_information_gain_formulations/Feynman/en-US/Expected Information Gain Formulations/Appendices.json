{
    "hands_on_practices": [
        {
            "introduction": "In many scientific applications, correctly characterizing the measurement error is as important as inferring the primary model parameters. This exercise  moves beyond parameter inference to focus on learning a hyperparameter—the observation noise variance $\\sigma^2$. You will perform a complete, analytical derivation of the expected information gain for $\\sigma^2$ using a standard conjugate prior setup, providing foundational practice in manipulating the entropies of key distributions in Bayesian analysis.",
            "id": "3380369",
            "problem": "Consider a calibration experiment in a data assimilation workflow where the only objective is to learn the unknown observation noise variance (hyperparameter) $\\sigma^{2}$. You collect $r$ independent calibration residuals $\\{y_{i}\\}_{i=1}^{r}$ with a known zero-mean signal, modeled as $y_{i} \\mid \\sigma^{2} \\sim \\mathcal{N}(0, \\sigma^{2})$ for $i=1,\\dots,r$. Assume a conjugate prior for the variance, $\\sigma^{2} \\sim \\text{Inverse-Gamma}(a, b)$ with shape $a>0$ and scale $b>0$, where the probability density function is $p(\\sigma^{2}) = \\frac{b^{a}}{\\Gamma(a)}(\\sigma^{2})^{-(a+1)} \\exp\\!\\big(-\\frac{b}{\\sigma^{2}}\\big)$.\n\nAdopt the definition of Expected Information Gain (EIG) as the mutual information between the data and the parameter of interest, which is equivalently the expected Kullback–Leibler divergence (KLD) from the posterior to the prior. That is, for the data vector $Y = (y_{1},\\dots,y_{r})$, $I(Y;\\sigma^{2}) = \\mathbb{E}_{Y}\\!\\left[ \\mathrm{KLD}\\big(p(\\sigma^{2}\\mid Y)\\,\\|\\,p(\\sigma^{2})\\big) \\right]$, where $\\mathrm{KLD}$ denotes the Kullback–Leibler divergence.\n\nStarting from fundamental definitions and conjugacy, derive a closed-form analytic expression for the EIG $I(Y;\\sigma^{2})$ as a function of $a$ and $r$. The final answer must be a single analytic expression. Then, justify—based on first principles of inverse problems and data assimilation—under what conditions it is beneficial to design experiments to learn $\\sigma^{2}$ rather than the forward model parameter $\\theta$ (assume $\\theta$ appears only in non-calibration experiments and is endowed with a proper prior). No numerical approximation is required for the EIG; your final answer must be exact. If you introduce any acronyms, define them at first use.",
            "solution": "The problem statement is first validated against the required criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Objective**: Learn the unknown observation noise variance, $\\sigma^{2}$.\n- **Data**: $r$ independent calibration residuals, $\\{y_{i}\\}_{i=1}^{r}$.\n- **Data Model (Likelihood)**: $y_{i} \\mid \\sigma^{2} \\sim \\mathcal{N}(0, \\sigma^{2})$ for $i=1,\\dots,r$. The data vector is $Y = (y_{1},\\dots,y_{r})$.\n- **Prior Distribution**: $\\sigma^{2} \\sim \\text{Inverse-Gamma}(a, b)$ with shape $a>0$ and scale $b>0$.\n- **Prior Probability Density Function (PDF)**: $p(\\sigma^{2}) = \\frac{b^{a}}{\\Gamma(a)}(\\sigma^{2})^{-(a+1)} \\exp\\!\\big(-\\frac{b}{\\sigma^{2}}\\big)$.\n- **Definition of Expected Information Gain (EIG)**: $I(Y;\\sigma^{2}) = \\mathbb{E}_{Y}\\!\\left[ \\mathrm{KLD}\\big(p(\\sigma^{2}\\mid Y)\\,\\|\\,p(\\sigma^{2})\\big) \\right]$, where KLD is the Kullback–Leibler divergence.\n- **Task 1**: Derive a closed-form analytic expression for $I(Y;\\sigma^{2})$ as a function of $a$ and $r$.\n- **Task 2**: Justify when it is beneficial to design experiments to learn $\\sigma^{2}$ rather than a forward model parameter $\\theta$.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded**: The problem is based on standard, well-established principles of Bayesian inference. The use of a Normal likelihood for residuals and a conjugate Inverse-Gamma prior for the variance is a textbook example in statistics and machine learning. The definition of EIG as the mutual information between data and parameters is also a fundamental concept in information theory and Bayesian experimental design.\n- **Well-Posed**: The problem is clearly stated and provides all necessary information to derive the required expression. The use of a conjugate prior ensures that the posterior distribution is analytically tractable, which allows for a closed-form derivation of the EIG. The question is unambiguous and a unique solution exists.\n- **Objective**: The problem is stated in precise, objective mathematical language, free from subjective or biased terminology.\n\nThe problem does not exhibit any of the flaws listed in the instructions (e.g., scientific unsoundness, incompleteness, ambiguity). It is a standard, non-trivial problem in Bayesian statistics.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\n\nThe solution is derived in two parts as requested.\n\n**Part 1: Derivation of the Expected Information Gain (EIG)**\n\nThe Expected Information Gain (EIG) is the mutual information between the data $Y$ and the parameter $\\sigma^2$, which can be expressed in two equivalent ways:\n$$I(Y; \\sigma^{2}) = H(Y) - \\mathbb{E}_{\\sigma^{2}}[H(Y \\mid \\sigma^{2})]$$\n$$I(Y; \\sigma^{2}) = H(\\sigma^{2}) - \\mathbb{E}_{Y}[H(\\sigma^{2} \\mid Y)]$$\nwhere $H(\\cdot)$ denotes the differential entropy. The first form, involving the entropy of the data, is more direct for this problem. We will compute each term separately.\n\n1.  **Conditional Entropy of Data**: $H(Y \\mid \\sigma^{2})$\n    Given $\\sigma^{2}$, the data $Y = (y_1, \\dots, y_r)$ consists of $r$ independent and identically distributed random variables, with each $y_i \\sim \\mathcal{N}(0, \\sigma^2)$. Therefore, the vector $Y$ follows a multivariate normal distribution, $Y \\mid \\sigma^{2} \\sim \\mathcal{N}(0, \\sigma^{2}I_{r})$, where $I_{r}$ is the $r \\times r$ identity matrix.\n    The differential entropy of a $k$-dimensional multivariate normal distribution $\\mathcal{N}(\\mu, \\Sigma)$ is $\\frac{1}{2}\\ln((2\\pi e)^{k}|\\det(\\Sigma)|)$.\n    For $Y \\mid \\sigma^{2}$, we have dimension $k=r$ and covariance matrix $\\Sigma = \\sigma^2 I_r$, so $\\det(\\Sigma) = (\\sigma^2)^r$.\n    The conditional entropy is:\n    $$H(Y \\mid \\sigma^{2}) = \\frac{1}{2}\\ln\\left((2\\pi e)^{r}(\\sigma^{2})^{r}\\right) = \\frac{r}{2}\\ln(2\\pi e) + \\frac{r}{2}\\ln(\\sigma^{2})$$\n\n2.  **Expected Conditional Entropy**: $\\mathbb{E}_{\\sigma^{2}}[H(Y \\mid \\sigma^{2})]$\n    We take the expectation of $H(Y \\mid \\sigma^{2})$ with respect to the prior distribution of $\\sigma^2$, which is $\\sigma^{2} \\sim \\text{Inverse-Gamma}(a, b)$.\n    $$\\mathbb{E}_{\\sigma^{2}}[H(Y \\mid \\sigma^{2})] = \\mathbb{E}_{\\sigma^{2}}\\left[\\frac{r}{2}\\ln(2\\pi e) + \\frac{r}{2}\\ln(\\sigma^{2})\\right] = \\frac{r}{2}\\ln(2\\pi e) + \\frac{r}{2}\\mathbb{E}_{\\sigma^{2}}[\\ln(\\sigma^{2})]$$\n    For a random variable $X \\sim \\text{Inverse-Gamma}(\\alpha, \\beta)$, the expectation of its logarithm is $\\mathbb{E}[\\ln(X)] = \\ln(\\beta) - \\psi(\\alpha)$, where $\\psi(\\alpha) = \\frac{d}{d\\alpha}\\ln\\Gamma(\\alpha)$ is the digamma function.\n    For our prior with $\\alpha=a$ and $\\beta=b$, we have $\\mathbb{E}_{\\sigma^{2}}[\\ln(\\sigma^{2})] = \\ln(b) - \\psi(a)$.\n    Substituting this into the expression for the expected conditional entropy gives:\n    $$\\mathbb{E}_{\\sigma^{2}}[H(Y \\mid \\sigma^{2})] = \\frac{r}{2}\\ln(2\\pi e) + \\frac{r}{2}(\\ln(b) - \\psi(a)) = \\frac{r}{2}(\\ln(2\\pi) + 1) + \\frac{r}{2}\\ln(b) - \\frac{r}{2}\\psi(a)$$\n\n3.  **Marginal Entropy of Data**: $H(Y)$\n    To find $H(Y)$, we first need the marginal distribution of $Y$, obtained by integrating out $\\sigma^2$: $p(Y) = \\int_{0}^{\\infty} p(Y \\mid \\sigma^{2}) p(\\sigma^{2}) d\\sigma^{2}$.\n    This is a standard hierarchical model. A normal likelihood with an Inverse-Gamma prior on the variance results in a multivariate Student's t-distribution for the marginal.\n    Specifically, if $Y \\mid \\sigma^2 \\sim \\mathcal{N}(0, \\sigma^2 I_r)$ and $\\sigma^2 \\sim \\text{Inverse-Gamma}(a, b)$, then the marginal distribution of $Y$ is a multivariate Student's t-distribution, $Y \\sim t_r(\\mu, \\Sigma, \\nu)$, with parameters:\n    - Degrees of freedom: $\\nu = 2a$\n    - Location: $\\mu = 0$\n    - Scale matrix: $\\Sigma = \\frac{b}{a}I_r$\n    The differential entropy of a multivariate t-distribution with dimension $k$, degrees of freedom $\\nu$, and scale matrix $\\Sigma$ is:\n    $$H = \\ln\\left(\\frac{\\Gamma(\\nu/2)}{\\Gamma((\\nu+k)/2)}\\right) + \\frac{k}{2}\\ln(\\nu\\pi) + \\frac{1}{2}\\ln|\\det(\\Sigma)| + \\frac{\\nu+k}{2}\\left[\\psi\\left(\\frac{\\nu+k}{2}\\right) - \\psi\\left(\\frac{\\nu}{2}\\right)\\right]$$\n    Substituting our parameters $k=r$, $\\nu=2a$, and $\\det(\\Sigma) = \\det\\left(\\frac{b}{a}I_r\\right) = \\left(\\frac{b}{a}\\right)^r$:\n    $$H(Y) = \\ln\\left(\\frac{\\Gamma(a)}{\\Gamma(a+r/2)}\\right) + \\frac{r}{2}\\ln(2a\\pi) + \\frac{1}{2}\\ln\\left(\\left(\\frac{b}{a}\\right)^{r}\\right) + \\frac{2a+r}{2}\\left[\\psi\\left(a+\\frac{r}{2}\\right) - \\psi(a)\\right]$$\n    Simplifying the logarithmic terms:\n    $$H(Y) = \\ln\\Gamma(a) - \\ln\\Gamma\\left(a+\\frac{r}{2}\\right) + \\frac{r}{2}(\\ln(2\\pi)+\\ln a) + \\frac{r}{2}(\\ln b - \\ln a) + \\left(a+\\frac{r}{2}\\right)\\left[\\psi\\left(a+\\frac{r}{2}\\right) - \\psi(a)\\right]$$\n    $$H(Y) = \\ln\\Gamma(a) - \\ln\\Gamma\\left(a+\\frac{r}{2}\\right) + \\frac{r}{2}\\ln(2\\pi) + \\frac{r}{2}\\ln b + \\left(a+\\frac{r}{2}\\right)\\psi\\left(a+\\frac{r}{2}\\right) - \\left(a+\\frac{r}{2}\\right)\\psi(a)$$\n\n4.  **Calculate EIG**: $I(Y; \\sigma^{2}) = H(Y) - \\mathbb{E}_{\\sigma^{2}}[H(Y \\mid \\sigma^{2})]$\n    We now subtract the expression for the expected conditional entropy from the marginal entropy:\n    $$I(Y; \\sigma^{2}) = \\left[ \\ln\\Gamma(a) - \\ln\\Gamma\\left(a+\\frac{r}{2}\\right) + \\frac{r}{2}\\ln(2\\pi) + \\frac{r}{2}\\ln b + \\left(a+\\frac{r}{2}\\right)\\psi\\left(a+\\frac{r}{2}\\right) - \\left(a+\\frac{r}{2}\\right)\\psi(a) \\right] - \\left[ \\frac{r}{2}(\\ln(2\\pi) + 1) + \\frac{r}{2}\\ln(b) - \\frac{r}{2}\\psi(a) \\right]$$\n    Let's group terms for cancellation:\n    $$I(Y; \\sigma^{2}) = (\\ln\\Gamma(a) - \\ln\\Gamma\\left(a+\\frac{r}{2}\\right)) + \\left(\\frac{r}{2}\\ln(2\\pi) - \\frac{r}{2}\\ln(2\\pi)\\right) + \\left(\\frac{r}{2}\\ln b - \\frac{r}{2}\\ln b\\right) + \\left(a+\\frac{r}{2}\\right)\\psi\\left(a+\\frac{r}{2}\\right) - \\left(a+\\frac{r}{2}\\right)\\psi(a) + \\frac{r}{2}\\psi(a) - \\frac{r}{2}$$\n    The $\\ln(2\\pi)$ and $\\ln b$ terms cancel. Combining the $\\psi(a)$ terms:\n    $$-(a+r/2)\\psi(a) + (r/2)\\psi(a) = -a\\psi(a) - (r/2)\\psi(a) + (r/2)\\psi(a) = -a\\psi(a)$$\n    Thus, the final expression for the EIG is:\n    $$I(Y; \\sigma^{2}) = \\ln\\Gamma(a) - \\ln\\Gamma\\left(a+\\frac{r}{2}\\right) + \\left(a+\\frac{r}{2}\\right)\\psi\\left(a+\\frac{r}{2}\\right) - a\\psi(a) - \\frac{r}{2}$$\n    This expression depends only on the prior shape parameter $a$ and the number of data points $r$, not the prior scale parameter $b$, as expected from dimensional analysis considerations.\n\n**Part 2: Justification for Learning $\\sigma^2$**\n\nIn the context of inverse problems and data assimilation, the primary goal is typically to infer a set of forward model parameters, denoted by $\\theta$, from observational data $d$. The relationship is often modeled as $d = G(\\theta) + \\epsilon$, where $G$ is the forward model and $\\epsilon$ is observational noise, commonly assumed to be Gaussian with zero mean and variance $\\sigma^2$. In this setting, $\\sigma^2$ is a hyperparameter that governs how much trust is placed in the data during the inversion for $\\theta$. The posterior distribution of $\\theta$ is given by Bayes' theorem as $p(\\theta \\mid d) \\propto p(d \\mid \\theta) p(\\theta)$, where the likelihood term $p(d \\mid \\theta)$ depends critically on $\\sigma^2$.\n\nIt is beneficial to dedicate experimental effort to learning $\\sigma^2$ rather than (or prior to) learning $\\theta$ under the following conditions, based on first principles:\n\n1.  **When Prior Knowledge of Observation Error is Poor:** If the initial prior distribution for $\\sigma^2$ is vague or highly uncertain (e.g., the shape parameter $a$ in the Inverse-Gamma prior is small), this uncertainty propagates directly into the inference for $\\theta$. An uncertain $\\sigma^2$ means the relative weighting between the prior belief in $\\theta$ and the information from the data $d$ is ill-defined. This leads to a posterior for $\\theta$ that may be inappropriately wide (if $\\sigma^2$ is overestimated) or narrow and potentially biased (if $\\sigma^2$ is underestimated). A dedicated calibration experiment, as described in the problem, serves to reduce the uncertainty in $\\sigma^2$, yielding a more informative posterior for it. This posterior then acts as a much tighter, more reliable prior for $\\sigma^2$ in the main experiment designed to infer $\\theta$, ensuring that the data is weighted correctly and the inference on $\\theta$ is more robust and efficient.\n\n2.  **To Correctly Quantify Uncertainty and Avoid Model-Data Mismatch:** The residual term $\\|d - G(\\theta)\\|^2$ mixes the effects of observation error (random noise) and model inadequacy (the forward model $G$ being an imperfect representation of reality). Without a reliable estimate of the observation noise variance $\\sigma^2$, it is impossible to distinguish between these two sources of error. A calibration experiment, where the \"signal\" $G(\\theta)$ is known (e.g., is zero, as in the problem statement), isolates and quantifies $\\sigma^2$ exclusively. This is a critical first step in many data assimilation systems, which must explicitly specify the observation error covariance matrix (often denoted $R$). An accurate $R$ is a prerequisite for correctly inferring model error statistics (often denoted $Q$) and for obtaining a reliable posterior uncertainty quantification for the parameters $\\theta$.\n\n3.  **For Experimental Design and Cost-Effectiveness:** Experiments designed to be maximally informative for a complex model parameter $\\theta$ can be resource-intensive and expensive. Such experiments may also be suboptimal for constraining the noise variance $\\sigma^2$. It is often far more cost-effective to first conduct a simpler, cheaper calibration experiment focused solely on characterizing the measurement instrument's noise properties ($\\sigma^2$). By \"tying down\" this nuisance parameter beforehand, the subsequent, more expensive experiments targeting $\\theta$ can be designed and analyzed with much greater efficiency, maximizing the scientific return on investment. The information from the expensive data is then used to constrain the parameter of primary interest, $\\theta$, rather than being partially consumed to also learn about the noise level.\n\nIn summary, learning $\\sigma^2$ is not just a secondary task; it is a foundational step that enables robust and efficient inference for the primary model parameters $\\theta$. It is most beneficial when initial uncertainty about the noise level is high and when disentangling different sources of error is crucial for the scientific objective.",
            "answer": "$$\n\\boxed{\\ln\\Gamma(a) - \\ln\\Gamma\\left(a+\\frac{r}{2}\\right) + \\left(a+\\frac{r}{2}\\right)\\psi\\left(a+\\frac{r}{2}\\right) - a\\psi(a) - \\frac{r}{2}}\n$$"
        },
        {
            "introduction": "While analytical derivations provide deep insight, most real-world models are not so tidy, requiring computational methods to estimate expected information gain. This exercise  bridges the gap from theory to computation by guiding you through the derivation of an efficient Monte Carlo estimator for EIG. You will learn how to handle hierarchical models by analytically marginalizing nuisance parameters, a powerful and practical technique for designing experiments in complex settings.",
            "id": "3380344",
            "problem": "Consider a hierarchical linear-Gaussian data model in which the nuisance parameter enters only through the noise distribution. Let the parameter of interest be the vector $\\psi \\in \\mathbb{R}^{n_{\\psi}}$ with prior density $p(\\psi)$ from which one can sample. For a fixed design $d$, the data $Y \\in \\mathbb{R}^{n_{y}}$ are generated via\n- the forward map $H(d) \\in \\mathbb{R}^{n_{y} \\times n_{\\psi}}$ as $Y \\mid \\psi, \\phi, d \\sim \\mathcal{N}\\!\\left(H(d)\\,\\psi,\\;\\phi^{-1}\\,\\Sigma(d)\\right)$, where $\\Sigma(d) \\in \\mathbb{R}^{n_{y} \\times n_{y}}$ is a known positive-definite matrix, and\n- the noise precision $\\phi$ follows a Gamma prior $\\phi \\sim \\text{Gamma}\\!\\left(\\frac{\\nu}{2},\\,\\frac{\\nu}{2}\\right)$ with shape $\\frac{\\nu}{2}$ and rate $\\frac{\\nu}{2}$ for a fixed $\\nu > 0$.\n\nAssume that $\\phi$ is a nuisance parameter that is not of inferential interest and enters only the noise model as specified above, and that $p(\\psi)$ is independent of $\\phi$. The expected information gain $I(\\psi; Y \\mid d)$ is defined as the mutual information between $\\psi$ and $Y$ under this hierarchical model, conditioned on $d$.\n\nStarting from the definition of mutual information and from first principles of probability (such as the law of total probability and Bayes’ rule), derive a Monte Carlo estimator of $I(\\psi; Y \\mid d)$ that marginalizes the nuisance $\\phi$ exactly in the likelihood and avoids sampling $\\phi$ in the inner loop over synthetic data. Your derivation must:\n- begin from the definition $I(\\psi; Y \\mid d) = \\mathbb{E}_{\\psi}\\,\\mathbb{E}_{Y \\mid \\psi, d}\\!\\left[\\ln p(Y \\mid \\psi, d) - \\ln p(Y \\mid d)\\right]$,\n- obtain the $\\phi$-marginalized data likelihood $p(Y \\mid \\psi, d)$ in closed form for the given hierarchical model, and\n- produce a single-loop, sample-reuse Monte Carlo estimator over $\\{\\psi^{(i)}, Y^{(i)}\\}_{i=1}^{M}$ that depends only on the $\\phi$-marginalized likelihood evaluations and does not sample $\\phi$ in the inner loop over $Y$.\n\nExpress your final estimator as a single explicit analytic expression in terms of $H(d)$, $\\Sigma(d)$, $\\nu$, and the multivariate Student’s $t$-distribution density, and ensure that every mathematical symbol is clearly specified. The final answer must be a closed-form expression. No numerical evaluation is required.",
            "solution": "The problem statement is critically validated as scientifically grounded, well-posed, objective, and complete. It describes a standard hierarchical Bayesian model for which the expected information gain is a well-defined quantity. The premises are consistent and the task is a formal derivation within the scope of inverse problems and data assimilation. The problem is therefore deemed valid.\n\nThe objective is to derive a single-loop, sample-reuse Monte Carlo estimator for the expected information gain (EIG) or mutual information $I(\\psi; Y \\mid d)$, which marginalizes out the nuisance parameter $\\phi$ analytically.\n\nThe EIG is defined as:\n$$\nI(\\psi; Y \\mid d) = \\mathbb{E}_{\\psi, Y \\mid d}\\! \\left[ \\ln\\frac{p(Y \\mid \\psi, d)}{p(Y \\mid d)} \\right] = \\mathbb{E}_{\\psi}\\,\\mathbb{E}_{Y \\mid \\psi, d}\\!\\left[\\ln p(Y \\mid \\psi, d) - \\ln p(Y \\mid d)\\right]\n$$\nThe expectation is over the joint distribution $p(\\psi, Y \\mid d) = p(Y \\mid \\psi, d) p(\\psi)$. A Monte Carlo approximation of this expectation is constructed by drawing $M$ samples $\\{\\psi^{(i)}, Y^{(i)}\\}_{i=1}^{M}$ from this joint distribution. This is achieved by first drawing $\\psi^{(i)} \\sim p(\\psi)$ and then drawing $Y^{(i)} \\sim p(Y \\mid \\psi^{(i)}, d)$.\n\nThe derivation proceeds in three main steps:\n1.  Derive the analytical form of the $\\phi$-marginalized data likelihood, $p(Y \\mid \\psi, d)$.\n2.  Formulate the Monte Carlo sampling procedure that avoids sampling $\\phi$.\n3.  Construct the final estimator for $I(\\psi; Y \\mid d)$ using the derived likelihood and a sample-reuse strategy for the evidence term $p(Y \\mid d)$.\n\n**Step 1: Derivation of the Marginal Likelihood $p(Y \\mid \\psi, d)$**\n\nThe marginal likelihood $p(Y \\mid \\psi, d)$ is obtained by integrating the full likelihood $p(Y, \\phi \\mid \\psi, d)$ over the nuisance parameter $\\phi$. Using the chain rule of probability and the a priori independence of $\\psi$ and $\\phi$, we have:\n$$\np(Y \\mid \\psi, d) = \\int_{0}^{\\infty} p(Y, \\phi \\mid \\psi, d) \\, d\\phi = \\int_{0}^{\\infty} p(Y \\mid \\psi, \\phi, d) \\, p(\\phi \\mid \\psi, d) \\, d\\phi = \\int_{0}^{\\infty} p(Y \\mid \\psi, \\phi, d) \\, p(\\phi) \\, d\\phi\n$$\nThe problem specifies the forms of these two densities:\nThe data likelihood is a multivariate normal distribution:\n$$\np(Y \\mid \\psi, \\phi, d) = \\mathcal{N}(Y; H(d)\\psi, \\phi^{-1}\\Sigma(d)) = \\frac{|\\det(\\phi^{-1}\\Sigma(d))|^{-1/2}}{(2\\pi)^{n_y/2}} \\exp\\left(-\\frac{1}{2} (Y-H(d)\\psi)^T (\\phi^{-1}\\Sigma(d))^{-1} (Y-H(d)\\psi)\\right)\n$$\n$$\np(Y \\mid \\psi, \\phi, d) = \\frac{\\phi^{n_y/2}}{ (2\\pi)^{n_y/2} |\\det(\\Sigma(d))|^{1/2}} \\exp\\left(-\\frac{\\phi}{2} (Y-H(d)\\psi)^T \\Sigma(d)^{-1} (Y-H(d)\\psi)\\right)\n$$\nThe prior for the noise precision $\\phi$ is a Gamma distribution:\n$$\np(\\phi) = \\text{Gamma}\\left(\\phi; \\frac{\\nu}{2}, \\frac{\\nu}{2}\\right) = \\frac{(\\nu/2)^{\\nu/2}}{\\Gamma(\\nu/2)} \\phi^{\\frac{\\nu}{2}-1} \\exp\\left(-\\frac{\\nu}{2}\\phi\\right)\n$$\nNow, we compute the integral for $p(Y \\mid \\psi, d)$:\n$$\np(Y \\mid \\psi, d) = \\int_{0}^{\\infty} \\left[ \\frac{\\phi^{n_y/2} \\exp\\left(-\\frac{\\phi}{2} \\delta^2(Y, \\psi)\\right)}{(2\\pi)^{n_y/2} |\\det(\\Sigma(d))|^{1/2}} \\right] \\left[ \\frac{(\\nu/2)^{\\nu/2}}{\\Gamma(\\nu/2)} \\phi^{\\frac{\\nu}{2}-1} \\exp\\left(-\\frac{\\nu}{2}\\phi\\right) \\right] d\\phi\n$$\nwhere $\\delta^2(Y, \\psi) = (Y-H(d)\\psi)^T \\Sigma(d)^{-1} (Y-H(d)\\psi)$ is the squared Mahalanobis distance.\nCollecting terms dependent on $\\phi$:\n$$\np(Y \\mid \\psi, d) = \\frac{(\\nu/2)^{\\nu/2}}{(2\\pi)^{n_y/2} |\\det(\\Sigma(d))|^{1/2} \\Gamma(\\nu/2)} \\int_{0}^{\\infty} \\phi^{\\frac{n_y+\\nu}{2}-1} \\exp\\left(-\\frac{\\phi}{2}(\\delta^2(Y, \\psi) + \\nu)\\right) d\\phi\n$$\nThe integral is the kernel of a Gamma distribution. Using the identity $\\int_0^\\infty x^{a-1} \\exp(-bx) dx = \\frac{\\Gamma(a)}{b^a}$ with $a = \\frac{n_y+\\nu}{2}$ and $b = \\frac{\\delta^2(Y, \\psi) + \\nu}{2}$, the integral evaluates to:\n$$\n\\int_{0}^{\\infty} \\dots d\\phi = \\frac{\\Gamma\\left(\\frac{n_y+\\nu}{2}\\right)}{\\left(\\frac{\\delta^2(Y, \\psi) + \\nu}{2}\\right)^{\\frac{n_y+\\nu}{2}}}\n$$\nSubstituting this back into the expression for $p(Y \\mid \\psi, d)$:\n$$\np(Y \\mid \\psi, d) = \\frac{(\\nu/2)^{\\nu/2} \\Gamma\\left(\\frac{n_y+\\nu}{2}\\right)}{(2\\pi)^{n_y/2} |\\det(\\Sigma(d))|^{1/2} \\Gamma(\\nu/2)} \\frac{2^{\\frac{n_y+\\nu}{2}}}{(\\delta^2(Y, \\psi) + \\nu)^{\\frac{n_y+\\nu}{2}}}\n$$\nSimplifying the constants yields:\n$$\np(Y \\mid \\psi, d) = \\frac{\\Gamma\\left(\\frac{\\nu+n_y}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right) (\\nu\\pi)^{n_y/2} |\\det(\\Sigma(d))|^{1/2}} \\left(1 + \\frac{\\delta^2(Y, \\psi)}{\\nu}\\right)^{-\\frac{\\nu+n_y}{2}}\n$$\nThis is the probability density function (PDF) of a multivariate Student's t-distribution. Let $t_{n_y}(y; \\mu, S, \\nu_{df})$ denote the PDF for a random vector $y \\in \\mathbb{R}^{n_y}$ with location $\\mu$, scale matrix $S$, and $\\nu_{df}$ degrees of freedom. Then we can write:\n$$\np(Y \\mid \\psi, d) = t_{n_y}(Y; H(d)\\psi, \\Sigma(d), \\nu)\n$$\n\n**Step 2: Monte Carlo Sampling Procedure**\n\nThe joint distribution for sampling is $p(\\psi, Y \\mid d) = p(Y \\mid \\psi, d) p(\\psi)$. We generate $M$ samples $\\{\\psi^{(i)}, Y^{(i)}\\}_{i=1}^M$ via ancestral sampling:\n1.  For $i=1, \\dots, M$, draw a parameter sample $\\psi^{(i)}$ from its prior: $\\psi^{(i)} \\sim p(\\psi)$.\n2.  For each $\\psi^{(i)}$, draw a corresponding data sample $Y^{(i)}$ from the marginalized likelihood derived above: $Y^{(i)} \\sim p(Y \\mid \\psi^{(i)}, d) = t_{n_y}(\\cdot; H(d)\\psi^{(i)}, \\Sigma(d), \\nu)$.\nThis procedure does not require sampling the nuisance parameter $\\phi$.\n\n**Step 3: Construction of the Single-Loop Estimator**\n\nThe EIG can be estimated from the generated samples $\\{\\psi^{(i)}, Y^{(i)}\\}_{i=1}^M$ as:\n$$\n\\hat{I}_M(\\psi; Y \\mid d) = \\frac{1}{M} \\sum_{i=1}^M \\left[ \\ln p(Y^{(i)} \\mid \\psi^{(i)}, d) - \\ln p(Y^{(i)} \\mid d) \\right]\n$$\nThe first term, $p(Y^{(i)} \\mid \\psi^{(i)}, d)$, is evaluated directly using the Student's t-distribution PDF:\n$$\np(Y^{(i)} \\mid \\psi^{(i)}, d) = t_{n_y}(Y^{(i)}; H(d)\\psi^{(i)}, \\Sigma(d), \\nu)\n$$\nThe second term, the evidence $p(Y^{(i)} \\mid d)$, is intractable to compute directly. It is defined as the expectation of the likelihood over the prior of $\\psi$:\n$$\np(Y^{(i)} \\mid d) = \\int p(Y^{(i)} \\mid \\psi', d) \\, p(\\psi') \\, d\\psi' = \\mathbb{E}_{\\psi'} \\left[ t_{n_y}(Y^{(i)}; H(d)\\psi', \\Sigma(d), \\nu) \\right]\n$$\nWe can estimate this expectation using a Monte Carlo approximation. A crucial insight for an efficient estimator is to reuse the same set of parameter samples $\\{\\psi^{(j)}\\}_{j=1}^M$ to estimate the evidence for each data point $Y^{(i)}$. This gives the estimator for the evidence:\n$$\n\\hat{p}(Y^{(i)} \\mid d) = \\frac{1}{M} \\sum_{j=1}^M p(Y^{(i)} \\mid \\psi^{(j)}, d) = \\frac{1}{M} \\sum_{j=1}^M t_{n_y}(Y^{(i)}; H(d)\\psi^{(j)}, \\Sigma(d), \\nu)\n$$\nSubstituting these expressions back into the EIG estimator yields the final single-loop, sample-reuse Monte Carlo estimator:\n$$\n\\hat{I}_M(\\psi; Y \\mid d) = \\frac{1}{M} \\sum_{i=1}^{M} \\left[ \\ln t_{n_y}(Y^{(i)}; H(d)\\psi^{(i)}, \\Sigma(d), \\nu) - \\ln \\left( \\frac{1}{M} \\sum_{j=1}^{M} t_{n_y}(Y^{(i)}; H(d)\\psi^{(j)}, \\Sigma(d), \\nu) \\right) \\right]\n$$\nThis expression depends only on evaluations of the $\\phi$-marginalized likelihood.\n\nFor clarity, the multivariate Student's t-distribution density function $t_{n_y}(y; \\mu, S, \\nu_{df})$ for a vector $y \\in \\mathbb{R}^{n_y}$, with location $\\mu$, scale matrix $S$, and $\\nu_{df}$ degrees of freedom, is defined as:\n$$\nt_{n_y}(y; \\mu, S, \\nu_{df}) = \\frac{\\Gamma\\left(\\frac{\\nu_{df}+n_y}{2}\\right)}{\\Gamma\\left(\\frac{\\nu_{df}}{2}\\right) (\\nu_{df}\\pi)^{n_y/2} |\\det(S)|^{1/2}} \\left(1 + \\frac{1}{\\nu_{df}}(y-\\mu)^T S^{-1} (y-\\mu)\\right)^{-\\frac{\\nu_{df}+n_y}{2}}\n$$\nIn our case, the parameters are $y=Y^{(i)}$, $\\mu=H(d)\\psi^{(j)}$, $S=\\Sigma(d)$, and $\\nu_{df}=\\nu$.",
            "answer": "$$\n\\boxed{\\frac{1}{M} \\sum_{i=1}^{M} \\left[ \\ln t_{n_y}(Y^{(i)}; H(d)\\psi^{(i)}, \\Sigma(d), \\nu) - \\ln \\left( \\frac{1}{M} \\sum_{j=1}^{M} t_{n_y}(Y^{(i)}; H(d)\\psi^{(j)}, \\Sigma(d), \\nu) \\right) \\right]}\n$$"
        }
    ]
}