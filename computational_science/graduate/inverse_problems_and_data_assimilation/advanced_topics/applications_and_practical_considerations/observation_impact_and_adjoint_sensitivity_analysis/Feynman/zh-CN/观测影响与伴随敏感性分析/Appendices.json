{
    "hands_on_practices": [
        {
            "introduction": "在任何变分数据同化系统中，第一步都是精确计算成本函数的梯度。本练习将指导你亲手实践伴随方法——计算该梯度的最高效途径——并将其应用于一个线性的四维变分(4D-Var)系统。 更重要的是，本练习强调了通过与有限差分法进行对比（即“梯度检验”）来验证伴随模型正确性的关键步骤，这是确保代码无误的必要环节。",
            "id": "3406525",
            "problem": "考虑由状态转移 $x_{k+1} = A x_k$（初始条件为 $x_0 \\in \\mathbb{R}^n$）和线性观测算子 $h_k(x) = H x$（适用于所有时间索引 $k \\in \\mathbb{N}$）定义的线性时不变离散模型。设 $H \\in \\mathbb{R}^{p \\times n}$ 和 $A \\in \\mathbb{R}^{n \\times n}$ 为固定矩阵。给定背景态 $x_b \\in \\mathbb{R}^n$、一个对称正定 (SPD) 的背景协方差矩阵 $B \\in \\mathbb{R}^{n \\times n}$、一个观测序列 $\\{y_k\\}_{k=0}^{M-1}$（其中 $y_k \\in \\mathbb{R}^p$）以及一个 SPD 的观测误差协方差矩阵 $R \\in \\mathbb{R}^{p \\times p}$，定义四维变分(4D-Var)目标函数\n$$\nJ(x_0) = \\frac{1}{2} (x_0 - x_b)^\\top B^{-1} (x_0 - x_b) + \\frac{1}{2} \\sum_{k=0}^{M-1} \\left(H x_k - y_k\\right)^\\top R^{-1} \\left(H x_k - y_k\\right),\n$$\n其中对于每个 $k \\in \\{0,1,\\dots,M-1\\}$，$x_k = A^k x_0$。\n\n任务：从第一性原理出发，使用伴随方法推导并实现精确梯度 $\\nabla J(x_0)$。然后，通过将根据 $\\nabla J(x_0)$ 计算出的方向导数与 $J(x_0)$ 在几个随机方向上的中心有限差分进行比较，来验证其正确性。使用小步长 $\\varepsilon$ 的中心差分，并对每个方向 $v$，将内积 $v^\\top \\nabla J(x_0)$ 与中心差分\n$$\n\\frac{J(x_0 + \\varepsilon v) - J(x_0 - \\varepsilon v)}{2 \\varepsilon}.\n$$\n进行比较。报告每个测试用例在一组随机方向上的最大相对差异。使用固定的随机种子以保证可复现性。\n\n您必须实现一个程序，为每个测试用例执行以下操作：\n- 构造指定的矩阵和向量。\n- 通过伴随方法计算精确梯度 $\\nabla J(x_0)$。\n- 沿一组固定的随机单位方向计算中心差分方向导数。\n- 为每个测试用例返回定义为\n$$\n\\max_{v \\in \\mathcal{V}} \\frac{\\left|v^\\top \\nabla J(x_0) - \\frac{J(x_0 + \\varepsilon v) - J(x_0 - \\varepsilon v)}{2 \\varepsilon}\\right|}{\\max\\left(1, \\left|v^\\top \\nabla J(x_0)\\right|, \\left|\\frac{J(x_0 + \\varepsilon v) - J(x_0 - \\varepsilon v)}{2 \\varepsilon}\\right|\\right)},\n$$\n的最大相对误差，其中 $\\mathcal{V}$ 是随机方向的集合。使用 $\\varepsilon = 10^{-6}$。\n\n测试套件：\n- 用例 A (单位动力学和完全观测):\n  - 维度: $n = 3$, $p = 3$, 观测次数 $M = 3$ (时间 $k = 0,1,2$)。\n  - 矩阵:\n    $$\n    A = I_3,\\quad H = I_3,\\quad B = I_3,\\quad R = I_3.\n    $$\n  - 向量:\n    $$\n    x_0 = \\begin{bmatrix} 1.0 \\\\ -2.0 \\\\ 0.5 \\end{bmatrix},\\quad x_b = \\begin{bmatrix} 0.5 \\\\ -2.5 \\\\ 1.5 \\end{bmatrix},\n    $$\n    $$\n    y_0 = \\begin{bmatrix} 1.2 \\\\ -1.8 \\\\ 0.7 \\end{bmatrix},\\quad\n    y_1 = \\begin{bmatrix} 0.9 \\\\ -2.3 \\\\ 0.4 \\end{bmatrix},\\quad\n    y_2 = \\begin{bmatrix} 1.1 \\\\ -2.1 \\\\ 0.6 \\end{bmatrix}.\n    $$\n- 用例 B (非正规稳定动力学和部分观测):\n  - 维度: $n = 4$, $p = 2$, $M = 6$ (时间 $k = 0,1,2,3,4,5$)。\n  - 矩阵:\n    $$\n    A = \\begin{bmatrix}\n    0.9  & 0.2  & 0.0  & 0.0\\\\\n    0.0  & 0.9  & 0.2  & 0.0\\\\\n    0.0  & 0.0  & 0.9  & 0.2\\\\\n    0.0  & 0.0  & 0.0  & 0.9\n    \\end{bmatrix},\\quad\n    H = \\begin{bmatrix}\n    1.0  & 0.0  & 0.5  & 0.0\\\\\n    0.0  & 1.0  & 0.0  & 0.5\n    \\end{bmatrix}.\n    $$\n    通过\n    $$\n    M_{\\text{mat}} = \\begin{bmatrix}\n    2.0  & -0.3  & 0.0  & 0.0\\\\\n    -0.3  & 1.5  & 0.0  & 0.0\\\\\n    0.0  & 0.0  & 1.2  & 0.2\\\\\n    0.0  & 0.0  & 0.2  & 1.1\n    \\end{bmatrix},\\quad\n    B = M_{\\text{mat}} M_{\\text{mat}}^\\top + 0.1\\, I_4.\n    $$\n    构造一个 SPD 背景协方差。\n    使用\n    $$\n    R = \\begin{bmatrix}\n    1.0  & 0.2\\\\\n    0.2  & 0.5\n    \\end{bmatrix}.\n    $$\n  - 向量:\n    $$\n    x_0 = \\begin{bmatrix} 0.1 \\\\ -0.9 \\\\ 0.0 \\\\ 0.5 \\end{bmatrix},\\quad\n    x_b = \\begin{bmatrix} 0.0 \\\\ -0.8 \\\\ 0.2 \\\\ 0.3 \\end{bmatrix}.\n    $$\n    从一个固定的真值和一个确定性偏差生成观测值：\n    $$\n    x_0^{\\text{true}} = \\begin{bmatrix} 0.5 \\\\ -1.0 \\\\ 0.3 \\\\ 0.7 \\end{bmatrix},\\quad\n    y_k = H A^k x_0^{\\text{true}} + \\begin{bmatrix} 0.01 k \\\\ -0.005 k \\end{bmatrix},\\quad k \\in \\{0,1,2,3,4,5\\}.\n    $$\n- 用例 C (轻度不稳定的非正规动力学和完全观测，病态背景):\n  - 维度: $n = 3$, $p = 3$, $M = 5$ (时间 $k = 0,1,2,3,4$)。\n  - 矩阵:\n    $$\n    A = \\begin{bmatrix}\n    1.2  & 0.1  & 0.0\\\\\n    0.0  & 0.95  & 0.05\\\\\n    0.0  & 0.0  & 1.05\n    \\end{bmatrix},\\quad\n    H = \\begin{bmatrix}\n    1.0  & 0.0  & 0.0\\\\\n    0.0  & 1.0  & 0.0\\\\\n    0.2  & 0.0  & 1.0\n    \\end{bmatrix}.\n    $$\n    令\n    $$\n    Q = \\begin{bmatrix}\n    1.0  & 0.9  & 0.0\\\\\n    0.9  & 1.0  & 0.0\\\\\n    0.0  & 0.0  & 0.01\n    \\end{bmatrix},\\quad\n    B = Q + 0.1\\, I_3,\\quad\n    R = 0.05\\, I_3.\n    $$\n  - 向量:\n    $$\n    x_0 = \\begin{bmatrix} -0.3 \\\\ 0.2 \\\\ 0.1 \\end{bmatrix},\\quad\n    x_b = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\\quad\n    x_0^{\\text{true}} = \\begin{bmatrix} -0.25 \\\\ 0.15 \\\\ 0.2 \\end{bmatrix},\\quad\n    y_k = H A^k x_0^{\\text{true}},\\quad k \\in \\{0,1,2,3,4\\}.\n    $$\n- 用例 D (无观测：仅有背景项):\n  - 维度: $n = 3$, $p = 2$, $M = 0$。\n  - 矩阵:\n    $$\n    A = \\begin{bmatrix}\n    0.7  & 0.1  & 0.0\\\\\n    0.0  & 0.8  & 0.2\\\\\n    0.0  & 0.0  & 0.9\n    \\end{bmatrix},\\quad\n    H = \\begin{bmatrix}\n    1.0  & 0.0  & 0.0\\\\\n    0.0  & 1.0  & 0.0\n    \\end{bmatrix},\\quad\n    B = \\operatorname{diag}(10.0, 1.0, 0.1),\\quad\n    R = I_2.\n    $$\n  - 向量:\n    $$\n    x_0 = \\begin{bmatrix} 1.0 \\\\ -1.0 \\\\ 0.5 \\end{bmatrix},\\quad\n    x_b = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}.\n    $$\n    没有观测值，因此 $J(x_0)$ 中的求和项为空。\n\n方向导数验证细节：\n- 对每个用例使用固定的种子生成随机方向集 $\\mathcal{V}$。\n- 将每个方向 $v$ 归一化为单位欧几里得范数。\n- 所有用例使用相同的集合基数 $|\\mathcal{V}| = 8$。\n- 使用 $\\varepsilon = 10^{-6}$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含用例 A–D 的最大相对误差，格式为方括号内以逗号分隔的列表，例如\n$$\n[\\text{errA},\\text{errB},\\text{errC},\\text{errD}].\n$$\n每个条目必须是浮点数。不应打印任何额外文本。",
            "solution": "该问题要求推导并实现一个线性时不变系统的四维变分 (4D-Var) 代价函数的梯度。该梯度将使用伴随方法计算，并通过与有限差分近似进行比较来验证。\n\n4D-Var 目标函数由下式给出：\n$$\nJ(x_0) = \\frac{1}{2} (x_0 - x_b)^\\top B^{-1} (x_0 - x_b) + \\frac{1}{2} \\sum_{k=0}^{M-1} \\left(H x_k - y_k\\right)^\\top R^{-1} \\left(H x_k - y_k\\right)\n$$\n其中状态向量 $x_k$ 根据线性模型 $x_{k+1} = A x_k$ 演变，且 $x_k = A^k x_0$。变量 $x_0$ 是控制变量，代表系统的初始状态。\n\n我们寻求 $J$ 关于 $x_0$ 的梯度，记为 $\\nabla J(x_0)$。\n\n### 伴随模型的推导\n可以使用拉格朗日乘子法高效地推导出梯度，该方法引出伴随模型。我们将状态 $x_1, \\dots, x_{M-1}$ 视为受模型动力学施加的约束的自变量。问题是在满足约束条件 $x_k - A x_{k-1} = 0$（对 $k=1, \\dots, M-1$）的情况下最小化 $J$。\n\n拉格朗日函数 $\\mathcal{L}$ 是通过将模型约束（由拉格朗日乘子 $\\eta_k$，即伴随变量，加权）增广到代价函数而构造的：\n$$\n\\mathcal{L}(x_0, \\dots, x_{M-1}, \\eta_1, \\dots, \\eta_{M-1}) = J(x_0, \\dots, x_{M-1}) + \\sum_{k=1}^{M-1} \\eta_k^\\top (A x_{k-1} - x_k)\n$$\n原始无约束代价函数 $J(x_0)$ 的梯度等于拉格朗日函数关于 $x_0$ 的全导数，即 $\\nabla J(x_0) = (d\\mathcal{L}/dx_0)^\\top$。初始状态的一阶微扰 $\\delta x_0$ 会在轨迹中引起微扰 $\\delta x_k$。$\\mathcal{L}$ 的相应变化为：\n$$\n\\delta \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial x_0} \\delta x_0 + \\sum_{k=1}^{M-1} \\frac{\\partial \\mathcal{L}}{\\partial x_k} \\delta x_k\n$$\n偏导数为：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_0} = \\frac{\\partial J}{\\partial x_0} + \\eta_1^\\top A\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_k} = \\frac{\\partial J}{\\partial x_k} - \\eta_k^\\top + \\eta_{k+1}^\\top A, \\quad \\text{for } k=1, \\dots, M-2\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_{M-1}} = \\frac{\\partial J}{\\partial x_{M-1}} - \\eta_{M-1}^\\top\n$$\n$J$ 关于状态 $x_k$ 的偏导数为：\n$$\n\\frac{\\partial J}{\\partial x_0} = B^{-1}(x_0 - x_b) + H^\\top R^{-1}(H x_0 - y_0)\n$$\n$$\n\\frac{\\partial J}{\\partial x_k} = H^\\top R^{-1}(H x_k - y_k), \\quad \\text{for } k=1, \\dots, M-1\n$$\n伴随方法通过要求 $\\mathcal{L}$ 关于中间状态 $x_k$（对于 $k \\ge 1$）的偏导数为零来定义乘子 $\\eta_k$。这种选择巧妙地消除了计算 $k > 0$ 的 $\\delta x_k$ 的需要。令 $\\partial\\mathcal{L}/\\partial x_k = 0$（对 $k=1, \\dots, M-1$）可得到一组关于 $\\eta_k$ 的方程，这些方程必须从后向前（在时间上）求解：\n1. 在 $k=M-1$ 处的终端条件：$\\eta_{M-1} = \\left(\\frac{\\partial J}{\\partial x_{M-1}}\\right)^\\top = H^\\top R^{-1}(H x_{M-1} - y_{M-1})$\n2. 对于 $k=M-2, \\dots, 1$ 的后向递归：$\\eta_k = A^\\top \\eta_{k+1} + \\left(\\frac{\\partial J}{\\partial x_k}\\right)^\\top = A^\\top \\eta_{k+1} + H^\\top R^{-1}(H x_k - y_k)$\n\n通过此选择，$\\delta\\mathcal{L} = (\\partial\\mathcal{L}/\\partial x_0)\\delta x_0$。因此，梯度为：\n$$\n\\nabla J(x_0) = \\left(\\frac{\\partial \\mathcal{L}}{\\partial x_0}\\right)^\\top = \\left(\\frac{\\partial J}{\\partial x_0}\\right)^\\top + A^\\top \\eta_1\n$$\n代入 $(\\partial J/\\partial x_0)^\\top$ 和 $\\eta_1$ 的表达式：\n$$\n\\nabla J(x_0) = B^{-1}(x_0 - x_b) + H^\\top R^{-1}(H x_0 - y_0) + A^\\top \\eta_1\n$$\n我们可以通过定义 $\\eta_M = 0$ 来统一伴随递归。递归式 $\\eta_k = A^\\top\\eta_{k+1} + H^\\top R^{-1}(H x_k - y_k)$ 于是也对 $k=M-1$ 成立。让我们定义 $\\eta_0 = A^\\top \\eta_1 + H^\\top R^{-1}(H x_0 - y_0)$。递归式便可向下延伸至 $k=0$。完整的梯度可以紧凑地写为：\n$$\n\\nabla J(x_0) = B^{-1}(x_0 - x_b) + \\eta_0\n$$\n其中 $\\eta_0$ 是完整后向积分的结果。\n\n### 算法\n计算梯度 $\\nabla J(x_0)$ 的算法是一个两阶段过程：\n\n1.  **前向积分**：从初始条件 $x_0$ 开始，将模型在时间上前向运行，计算状态轨迹 $x_k = A x_{k-1}$（对 $k=1, \\dots, M-1$）。存储轨迹 $\\{x_k\\}_{k=0}^{M-1}$。\n\n2.  **后向（伴随）积分**：\n    a. 在最终时间再加一步处初始化伴随变量：$\\eta_M = 0$。\n    b. 从 $k=M-1$ 向后迭代至 $0$。在每一步中，使用未来的伴随状态 $\\eta_{k+1}$ 和模型状态 $x_k$ 计算当前的伴随状态 $\\eta_k$：\n       $$\n       \\eta_k = A^\\top \\eta_{k+1} + H^\\top R^{-1}(H x_k - y_k)\n       $$\n    计算出的最终变量 $\\eta_0$ 代表了来自观测项的梯度贡献。\n\n3.  **梯度组装**：将来自背景项的梯度与伴随积分的结果相结合：\n    $$\n    \\nabla J(x_0) = B^{-1}(x_0 - x_b) + \\eta_0\n    $$\n\n### 梯度验证\n为验证解析梯度的正确性，我们将其在一个方向 $v$ 上的投影与方向导数的有限差分近似进行比较。$J$ 在 $x_0$ 处沿方向 $v$ 的方向导数由内积 $\\langle \\nabla J(x_0), v \\rangle = v^\\top \\nabla J(x_0)$ 给出。\n一个二阶精确的数值近似由中心有限差分公式给出：\n$$\nD_v J(x_0) \\approx \\frac{J(x_0 + \\varepsilon v) - J(x_0 - \\varepsilon v)}{2\\varepsilon}\n$$\n对于一个小步长 $\\varepsilon$。我们计算在一组随机单位向量 $\\mathcal{V}$ 上的最大相对差异：\n$$\n\\max_{v \\in \\mathcal{V}} \\frac{\\left|v^\\top \\nabla J(x_0) - \\frac{J(x_0 + \\varepsilon v) - J(x_0 - \\varepsilon v)}{2 \\varepsilon}\\right|}{\\max\\left(1, \\left|v^\\top \\nabla J(x_0)\\right|, \\left|\\frac{J(x_0 + \\varepsilon v) - J(x_0 - \\varepsilon v)}{2 \\varepsilon}\\right|\\right)}\n$$\n该误差的一个小值可为推导和实现的梯度的正确性提供高置信度。分母是一个鲁棒的缩放因子，可防止除以零，并且即使在方向导数接近零时也能提供有意义的误差度量。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that sets up and runs all test cases for the 4D-Var gradient check.\n    \"\"\"\n\n    def run_case(n, p, M, A, H, B, R, x0, xb, observations_y, case_name):\n        \"\"\"\n        Runs a single test case: computes analytical gradient, compares with finite\n        differences, and returns the maximum relative error.\n        \n        Args:\n            n (int): State dimension.\n            p (int): Observation dimension.\n            M (int): Number of observation times.\n            A (np.ndarray): State transition matrix.\n            H (np.ndarray): Observation matrix.\n            B (np.ndarray): Background error covariance matrix.\n            R (np.ndarray): Observation error covariance matrix.\n            x0 (np.ndarray): Initial state (control variable).\n            xb (np.ndarray): Background state.\n            observations_y (list): List of observation vectors.\n            case_name (str): Name of the test case for logging.\n            \n        Returns:\n            float: Maximum relative error found across all random directions.\n        \"\"\"\n        \n        B_inv = np.linalg.inv(B)\n        R_inv = np.linalg.inv(R)\n        eps = 1e-6\n        num_directions = 8\n\n        def cost_J(current_x0):\n            \"\"\"Computes the 4D-Var cost function J(x0).\"\"\"\n            \n            # Background term\n            dev_b = current_x0 - xb\n            J_b = 0.5 * dev_b.T @ B_inv @ dev_b\n\n            # Observation term\n            J_o = 0.0\n            if M > 0:\n                xk = np.copy(current_x0)\n                for k in range(M):\n                    yk = observations_y[k]\n                    dev_o = H @ xk - yk\n                    J_o += 0.5 * dev_o.T @ R_inv @ dev_o\n                    if k < M - 1:\n                        xk = A @ xk\n            \n            return J_b + J_o\n\n        def grad_J_adjoint(current_x0):\n            \"\"\"Computes the gradient of J using the adjoint method.\"\"\"\n            \n            # Gradient of the background term\n            grad_b = B_inv @ (current_x0 - xb)\n\n            # Gradient of the observation term\n            grad_o = np.zeros(n)\n            if M > 0:\n                # Forward pass: compute and store the state trajectory\n                trajectory = []\n                xk = np.copy(current_x0)\n                for _ in range(M):\n                    trajectory.append(xk)\n                    if _ < M - 1:\n                        xk = A @ xk\n\n                # Backward (adjoint) pass\n                eta_k_plus_1 = np.zeros(n)\n                for k in range(M - 1, -1, -1):\n                    xk = trajectory[k]\n                    yk = observations_y[k]\n                    dev_o = H @ xk - yk\n                    forcing = H.T @ (R_inv @ dev_o)\n                    eta_k = A.T @ eta_k_plus_1 + forcing\n                    eta_k_plus_1 = eta_k\n                \n                grad_o = eta_k_plus_1\n            \n            return grad_b + grad_o\n\n        # --- Verification ---\n        grad_analytic = grad_J_adjoint(x0)\n        \n        rng = np.random.default_rng(seed=12345)\n        random_directions = rng.standard_normal(size=(num_directions, n))\n        \n        errors = []\n        for v in random_directions:\n            v_norm = v / np.linalg.norm(v)\n\n            # Analytical directional derivative\n            dd_analytic = v_norm.T @ grad_analytic\n\n            # Numerical directional derivative (central difference)\n            J_plus = cost_J(x0 + eps * v_norm)\n            J_minus = cost_J(x0 - eps * v_norm)\n            dd_numeric = (J_plus - J_minus) / (2 * eps)\n\n            # Robust relative error\n            numerator = np.abs(dd_analytic - dd_numeric)\n            denominator = np.max([1.0, np.abs(dd_analytic), np.abs(dd_numeric)])\n            relative_error = numerator / denominator\n            errors.append(relative_error)\n            \n        return np.max(errors)\n\n    # --- Test Case Definitions ---\n    test_suite = []\n\n    # Case A\n    n_a, p_a, M_a = 3, 3, 3\n    A_a = np.identity(n_a)\n    H_a = np.identity(p_a)\n    B_a = np.identity(n_a)\n    R_a = np.identity(p_a)\n    x0_a = np.array([1.0, -2.0, 0.5])\n    xb_a = np.array([0.5, -2.5, 1.5])\n    y_a = [np.array([1.2, -1.8, 0.7]), np.array([0.9, -2.3, 0.4]), np.array([1.1, -2.1, 0.6])]\n    test_suite.append({'n': n_a, 'p': p_a, 'M': M_a, 'A': A_a, 'H': H_a, 'B': B_a, 'R': R_a, 'x0': x0_a, 'xb': xb_a, 'observations_y': y_a, 'case_name': 'A'})\n\n    # Case B\n    n_b, p_b, M_b = 4, 2, 6\n    A_b = np.array([[0.9, 0.2, 0.0, 0.0], [0.0, 0.9, 0.2, 0.0], [0.0, 0.0, 0.9, 0.2], [0.0, 0.0, 0.0, 0.9]])\n    H_b = np.array([[1.0, 0.0, 0.5, 0.0], [0.0, 1.0, 0.0, 0.5]])\n    M_mat_b = np.array([[2.0, -0.3, 0.0, 0.0], [-0.3, 1.5, 0.0, 0.0], [0.0, 0.0, 1.2, 0.2], [0.0, 0.0, 0.2, 1.1]])\n    B_b = M_mat_b @ M_mat_b.T + 0.1 * np.identity(n_b)\n    R_b = np.array([[1.0, 0.2], [0.2, 0.5]])\n    x0_b = np.array([0.1, -0.9, 0.0, 0.5])\n    xb_b = np.array([0.0, -0.8, 0.2, 0.3])\n    x0_true_b = np.array([0.5, -1.0, 0.3, 0.7])\n    y_b = []\n    xk_true = np.copy(x0_true_b)\n    for k in range(M_b):\n        bias = np.array([0.01 * k, -0.005 * k])\n        y_b.append(H_b @ xk_true + bias)\n        xk_true = A_b @ xk_true\n    test_suite.append({'n': n_b, 'p': p_b, 'M': M_b, 'A': A_b, 'H': H_b, 'B': B_b, 'R': R_b, 'x0': x0_b, 'xb': xb_b, 'observations_y': y_b, 'case_name': 'B'})\n\n    # Case C\n    n_c, p_c, M_c = 3, 3, 5\n    A_c = np.array([[1.2, 0.1, 0.0], [0.0, 0.95, 0.05], [0.0, 0.0, 1.05]])\n    H_c = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.2, 0.0, 1.0]])\n    Q_c = np.array([[1.0, 0.9, 0.0], [0.9, 1.0, 0.0], [0.0, 0.0, 0.01]])\n    B_c = Q_c + 0.1 * np.identity(n_c)\n    R_c = 0.05 * np.identity(p_c)\n    x0_c = np.array([-0.3, 0.2, 0.1])\n    xb_c = np.zeros(n_c)\n    x0_true_c = np.array([-0.25, 0.15, 0.2])\n    y_c = []\n    xk_true = np.copy(x0_true_c)\n    for _ in range(M_c):\n        y_c.append(H_c @ xk_true)\n        xk_true = A_c @ xk_true\n    test_suite.append({'n': n_c, 'p': p_c, 'M': M_c, 'A': A_c, 'H': H_c, 'B': B_c, 'R': R_c, 'x0': x0_c, 'xb': xb_c, 'observations_y': y_c, 'case_name': 'C'})\n\n    # Case D\n    n_d, p_d, M_d = 3, 2, 0\n    A_d = np.array([[0.7, 0.1, 0.0], [0.0, 0.8, 0.2], [0.0, 0.0, 0.9]])\n    H_d = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]])\n    B_d = np.diag([10.0, 1.0, 0.1])\n    R_d = np.identity(p_d)\n    x0_d = np.array([1.0, -1.0, 0.5])\n    xb_d = np.zeros(n_d)\n    y_d = []\n    test_suite.append({'n': n_d, 'p': p_d, 'M': M_d, 'A': A_d, 'H': H_d, 'B': B_d, 'R': R_d, 'x0': x0_d, 'xb': xb_d, 'observations_y': y_d, 'case_name': 'D'})\n    \n    # --- Execute and collect results ---\n    results = []\n    for params in test_suite:\n        max_error = run_case(**params)\n        results.append(max_error)\n\n    # --- Print final output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "在掌握了线性模型的伴随方法后，我们必须面对大多数真实世界系统所固有的非线性。应用于非线性模型的伴随方法依赖于切线性近似 (tangent-linear approximation) 的有效性。 本练习旨在构建一个数值诊断工具，用于检验这一核心假设，通过分析近似误差的收敛速度，你将深入理解切线性模型在何种情况下是可靠的，以及其局限性在何处。",
            "id": "3406508",
            "problem": "考虑一个观测算子 $h:\\mathbb{R}^n\\to\\mathbb{R}^m$，它将模型状态向量映射到观测空间。在变分数据同化和反演问题中，伴随灵敏度分析依赖于切线性模型（TLM）。该模型假设，对于围绕基态 $x$ 的足够小的扰动 $\\delta x$，增量 $h(x+\\delta x)-h(x)$ 可以由 $h$ 在 $x$ 处的雅可比矩阵 $H$ 应用于 $\\delta x$ 来很好地近似。切线性假设基于一阶可微性和一阶泰勒定理。该定理指出，如果 $h$ 在 $x$ 处可微，则\n$$\nh(x+\\delta x) = h(x) + H\\,\\delta x + r(\\delta x),\n$$\n其中 $H$ 是 $h$ 在 $x$ 处的雅可比矩阵，余项 $r(\\delta x)$ 满足当 $\\lVert \\delta x\\rVert_2 \\to 0$ 时 $\\lVert r(\\delta x)\\rVert_2 / \\lVert \\delta x\\rVert_2 \\to 0$，其中 $\\lVert\\cdot\\rVert_2$ 表示欧几里得范数。对于二阶连续可微的 $h$， $r(\\delta x)$ 的主导行为是关于 $\\delta x$ 的二阶项。\n\n设计并实现一个诊断程序，通过在一系列步长 $\\alpha$ 上比较非线性增量 $h(x+\\alpha\\,\\delta x)-h(x)$ 与其线性近似 $H\\,\\alpha\\,\\delta x$，来检验切线性假设的有效性。该诊断必须量化：\n- 残差 $\\lVert r(\\alpha)\\rVert_2$ 作为 $\\alpha$ 函数的经验阶 $p$，通过对一组给定的 $\\alpha$ 值，对 $\\log\\big(\\lVert r(\\alpha)\\rVert_2\\big)$ 与 $\\log(\\alpha)$ 进行最小二乘拟合得到。理想情况下，对于光滑非线性函数 $h$ 且残差由二阶项主导时，$p\\approx 2$。\n- 一个单调性指标，用于评估相对误差 $e(\\alpha) = \\lVert r(\\alpha)\\rVert_2 / \\lVert H\\,\\alpha\\,\\delta x\\rVert_2$ 是否随着 $\\alpha$ 的减小而单调非增，这与一阶有效性一致。当 $\\lVert H\\,\\alpha\\,\\delta x\\rVert_2=0$ 时，使用一个小的正则化参数以避免除以零。\n\n将您的推导建立在可微性、雅可比矩阵和泰勒展开的基本定义之上。除了测试套件中指定的内容外，不要假设任何特殊结构。所有量都是无量纲的，不涉及物理单位。如果出现角度，必须以弧度为单位。\n\n实现程序以计算以下测试套件的诊断。对于每种情况，$x\\in\\mathbb{R}^n$，$\\delta x\\in\\mathbb{R}^n$，步长集合为\n$$\n\\alpha \\in \\left\\{10^{-1},\\,5\\cdot 10^{-2},\\,2.5\\cdot 10^{-2},\\,1.25\\cdot 10^{-2},\\,6.25\\cdot 10^{-3},\\,3.125\\cdot 10^{-3},\\,10^{-3}\\right\\}.\n$$\n\n测试用例 A（非线性逻辑斯蒂观测，从 $\\mathbb{R}^2$ 到 $\\mathbb{R}^1$）：\n- 定义 $h(x)=\\sigma(a^\\top x)$，其中 $\\sigma(z)=\\dfrac{1}{1+e^{-z}}$ 且 $a\\in\\mathbb{R}^2$ 是固定的。\n- 在 $x$ 处的雅可比矩阵是 $H(x) = \\sigma(a^\\top x)\\left(1-\\sigma(a^\\top x)\\right)\\,a^\\top \\in \\mathbb{R}^{1\\times 2}$。\n- 使用 $a=\\begin{bmatrix}2\\\\-1\\end{bmatrix}$，$x=\\begin{bmatrix}0.3\\\\-0.2\\end{bmatrix}$，$\\delta x=\\begin{bmatrix}0.4\\\\0.1\\end{bmatrix}$。\n\n测试用例 B（线性观测，从 $\\mathbb{R}^2$ 到 $\\mathbb{R}^2$）：\n- 定义 $h(x)=M\\,x$，其中 $M\\in\\mathbb{R}^{2\\times 2}$ 是固定的，且 $H(x)=M$。\n- 使用 $M=\\begin{bmatrix}1 & 2\\\\-3 & 4\\end{bmatrix}$，$x=\\begin{bmatrix}0.5\\\\-0.4\\end{bmatrix}$，$\\delta x=\\begin{bmatrix}0.2\\\\-0.1\\end{bmatrix}$。\n\n测试用例 C（光滑非线性混合观测，从 $\\mathbb{R}^3$ 到 $\\mathbb{R}^2$）：\n- 定义 $h(x)=\\begin{bmatrix}\\sin(b^\\top x)\\\\x_1^2 + e^{x_2} - \\tanh(x_3)\\end{bmatrix}$，其中 $b\\in\\mathbb{R}^3$ 是固定的，$\\sin(\\cdot)$ 是以弧度为单位的正弦函数，$e^{(\\cdot)}$ 是指数函数，$\\tanh(\\cdot)$ 是双曲正切。双曲正割定义为 $\\operatorname{sech}(z)=\\frac{1}{\\cosh(z)}$。\n- 在 $x$ 处的雅可比矩阵是\n$$\nH(x)=\\begin{bmatrix}\n\\cos(b^\\top x)\\,b^\\top\\\\\n2x_1  & e^{x_2}  & -\\operatorname{sech}^2(x_3)\n\\end{bmatrix}\\in\\mathbb{R}^{2\\times 3}.\n$$\n- 使用 $b=\\begin{bmatrix}1.0\\\\-2.0\\\\0.5\\end{bmatrix}$，$x=\\begin{bmatrix}0.1\\\\-0.3\\\\0.2\\end{bmatrix}$，$\\delta x=\\begin{bmatrix}-0.05\\\\0.02\\\\0.04\\end{bmatrix}$。\n\n对于每个测试用例和每个 $\\alpha$，计算非线性增量 $d_h(\\alpha)=h(x+\\alpha\\,\\delta x)-h(x)$、切线性增量 $t(\\alpha)=H(x)\\,\\alpha\\,\\delta x$ 和残差 $r(\\alpha)=d_h(\\alpha)-t(\\alpha)$。使用欧几里得范数构成 $\\lVert r(\\alpha)\\rVert_2$ 和 $\\lVert t(\\alpha)\\rVert_2$。通过对点 $\\big(\\log(\\alpha),\\log(\\lVert r(\\alpha)\\rVert_2+\\varepsilon)\\big)$ 进行最小二乘法拟合一条直线来估计经验阶 $p$，其中 $\\varepsilon$ 是一个小的正则化项（例如 $\\varepsilon=10^{-12}$）以避免对零取对数。将单调性指标确定为一个布尔值，当且仅当 $e(\\alpha)$ 在 $\\tau=10^{-12}$ 的容差范围内随着 $\\alpha$ 的减小而单调非增时，该值为真。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。输出应包含 $6$ 个项目：对于每个测试用例，按测试用例 A、测试用例 B、测试用例 C 的顺序，依次输出作为浮点数的经验阶 $p$ 和作为布尔值的单调性指标。例如，输出行的格式为 $[p_A,\\mathrm{mono}_A,p_B,\\mathrm{mono}_B,p_C,\\mathrm{mono}_C]$，不含空格。所有浮点数都是无量纲的，必须以程序的默认浮点格式打印，所有布尔值必须打印为 $\\mathrm{True}$ 或 $\\mathrm{False}$。",
            "solution": "该问题要求设计并实现一个诊断测试，以验证切线性假设。该假设是变分数据同化和伴随灵敏度分析的基石。该假设假定，对于一个可微的观测算子 $h: \\mathbb{R}^n \\to \\mathbb{R}^m$，观测空间中的扰动 $h(x+\\delta x) - h(x)$ 可以通过雅可比矩阵 $H$ 对状态扰动 $\\delta x$ 的作用（即 $H\\delta x$）来精确近似。\n\n验证基于 $h$ 在状态 $x$ 周围的一阶泰勒展开：\n$$\nh(x + \\delta x) = h(x) + H \\delta x + r(\\delta x)\n$$\n其中 $H$ 是 $h$ 在 $x$ 处求值的雅可比矩阵，$r(\\delta x)$ 是余项，满足 $\\lim_{\\lVert \\delta x \\rVert_2 \\to 0} \\frac{\\lVert r(\\delta x) \\rVert_2}{\\lVert \\delta x \\rVert_2} = 0$。对于一个二阶连续可微的函数 $h$，余项是扰动的二阶项，即 $\\lVert r(\\delta x) \\rVert_2 = O(\\lVert \\delta x \\rVert_2^2)$。\n\n为了测试这种行为，我们为扰动引入一个缩放参数 $\\alpha$，使得 $\\delta x \\to \\alpha \\delta x$。我们关注的量是：\n1.  非线性增量：$d_h(\\alpha) = h(x + \\alpha \\delta x) - h(x)$。\n2.  切线性增量：$t(\\alpha) = H(x) (\\alpha \\delta x) = \\alpha (H(x) \\delta x)$。\n3.  残差向量：$r(\\alpha) = d_h(\\alpha) - t(\\alpha)$。\n\n该诊断包括两个部分，在一组给定的递减 $\\alpha$ 值序列上进行评估。\n\n**1. 经验收敛阶 ($p$)**\n残差的理论二阶行为意味着，对于较小的 $\\alpha$，残差的欧几里得范数应呈二次方缩放：\n$$\n\\lVert r(\\alpha) \\rVert_2 \\approx C \\alpha^2\n$$\n其中常数 $C$ 取决于 $h$ 的二阶导数、$x$ 和 $\\delta x$。对两边取自然对数，得到一个线性关系：\n$$\n\\log(\\lVert r(\\alpha) \\rVert_2) \\approx \\log(C) + 2 \\log(\\alpha)\n$$\n此方程的形式为 $y = m x + b$，其中 $y = \\log(\\lVert r(\\alpha) \\rVert_2)$，$x = \\log(\\alpha)$，斜率 $m$ 是收敛阶，我们期望其为 $p=2$。\n\n为了经验地估计 $p$，我们为每个给定的步长 $\\alpha_i$ 计算点集 $(\\log(\\alpha_i), \\log(\\lVert r(\\alpha_i) \\rVert_2 + \\varepsilon))$。在范数上加上一个小的正则化常数 $\\varepsilon > 0$，以防止因对 $\\log(0)$ 求值而产生的数值问题，这种情况可能在残差恰好为零时发生（例如，对于线性算子 $h$）。然后，通过线性最小二乘回归，将通过这些点的最佳拟合线的斜率确定为经验阶 $p$。对于线性算子 $h$，残差 $r(\\alpha)$ 恒为零。因此，$\\log(\\lVert r(\\alpha_i) \\rVert_2 + \\varepsilon)$ 是一个常数，得到的斜率 $p$ 将约等于 $0$。\n\n**2. 相对误差的单调性**\n切线性模型作为一阶近似的有效性意味着相对误差应随着扰动大小的减小而减小。我们将相对误差定义为：\n$$\ne(\\alpha) = \\frac{\\lVert r(\\alpha) \\rVert_2}{\\lVert t(\\alpha) \\rVert_2} = \\frac{\\lVert h(x + \\alpha \\delta x) - h(x) - \\alpha H \\delta x \\rVert_2}{\\lVert \\alpha H \\delta x \\rVert_2}\n$$\n假设 $H\\delta x \\neq 0$，则分子是 $O(\\alpha^2)$，而分母是 $O(\\alpha)$。因此，$e(\\alpha) = O(\\alpha)$，这意味着当 $\\alpha \\to 0$ 时，$e(\\alpha)$ 应趋近于 $0$。一个关键的诊断是检查函数 $e(\\alpha)$ 是否随着 $\\alpha$ 的减小而单调非增。\n\n为了实现这个检查，我们为给定的递减步长序列 $\\{\\alpha_1, \\alpha_2, \\dots, \\alpha_k\\}$ 计算 $e(\\alpha_i)$。单调性指标是一个布尔值，当且仅当对于所有 $i \\in \\{1, \\dots, k-1\\}$，都有 $e(\\alpha_{i+1}) \\le e(\\alpha_i) + \\tau$ 时，该值为 `True`，其中 $\\tau$ 是一个小的数值容差，用于解释浮点不精确性。在 $e(\\alpha)$ 的分母中也加入一个小的正则化参数以防止除以零。\n\n整体算法通过遍历每个测试用例来进行。对于每种情况，在基态 $x$ 处评估雅可比矩阵 $H(x)$。然后，对于给定集合中的每个 $\\alpha$，计算并存储残差的范数 $\\lVert r(\\alpha) \\rVert_2$ 和切线性增量的范数 $\\lVert t(\\alpha) \\rVert_2$。最后，使用这些存储的值来计算经验阶 $p$ 和单调性指标。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Computes diagnostic tests for the tangent-linear hypothesis for three test cases.\n    \"\"\"\n    \n    # Define global parameters from the problem statement.\n    alphas = np.array([1e-1, 5e-2, 2.5e-2, 1.25e-2, 6.25e-3, 3.125e-3, 1e-3])\n    epsilon = 1e-12\n    tau = 1e-12\n\n    # --- Test Case A: Nonlinear Logistic Observation ---\n    a_A = np.array([2.0, -1.0])\n    x_A = np.array([0.3, -0.2])\n    dx_A = np.array([0.4, 0.1])\n    \n    def sigma(z):\n        return 1.0 / (1.0 + np.exp(-z))\n        \n    def h_A(x_vec):\n        return np.array([sigma(a_A @ x_vec)])\n        \n    def H_A(x_vec):\n        s_ax = sigma(a_A @ x_vec)\n        return s_ax * (1.0 - s_ax) * a_A.reshape(1, 2)\n\n    # --- Test Case B: Linear Observation ---\n    M_B = np.array([[1.0, 2.0], [-3.0, 4.0]])\n    x_B = np.array([0.5, -0.4])\n    dx_B = np.array([0.2, -0.1])\n\n    def h_B(x_vec):\n        return M_B @ x_vec\n\n    def H_B(x_vec):\n        # Jacobian of a linear map is the matrix itself, independent of x_vec.\n        return M_B\n\n    # --- Test Case C: Smooth Nonlinear Mixed Observation ---\n    b_C = np.array([1.0, -2.0, 0.5])\n    x_C = np.array([0.1, -0.3, 0.2])\n    dx_C = np.array([-0.05, 0.02, 0.04])\n\n    def h_C(x_vec):\n        return np.array([\n            np.sin(b_C @ x_vec),\n            x_vec[0]**2 + np.exp(x_vec[1]) - np.tanh(x_vec[2])\n        ])\n\n    def H_C(x_vec):\n        # sech^2(z) = (1/cosh(z))^2\n        sech_sq_x3 = (1.0 / np.cosh(x_vec[2]))**2\n        return np.array([\n            np.cos(b_C @ x_vec) * b_C,\n            [2.0 * x_vec[0], np.exp(x_vec[1]), -sech_sq_x3]\n        ])\n\n    test_cases = [\n        {'h': h_A, 'H_func': H_A, 'x': x_A, 'dx': dx_A},\n        {'h': h_B, 'H_func': H_B, 'x': x_B, 'dx': dx_B},\n        {'h': h_C, 'H_func': H_C, 'x': x_C, 'dx': dx_C},\n    ]\n\n    results = []\n    for case in test_cases:\n        h, H_func, x, dx = case['h'], case['H_func'], case['x'], case['dx']\n        \n        # Pre-compute constant terms\n        h_x = h(x)\n        H_x = H_func(x)\n        \n        r_norms = []\n        t_norms = []\n        \n        for alpha in alphas:\n            # Nonlinear increment\n            d_h = h(x + alpha * dx) - h_x\n            \n            # Tangent-linear increment\n            t_alpha = H_x @ (alpha * dx)\n            \n            # Residual\n            r_alpha = d_h - t_alpha\n            \n            # Store norms for analysis\n            r_norms.append(np.linalg.norm(r_alpha))\n            t_norms.append(np.linalg.norm(t_alpha))\n            \n        r_norms = np.array(r_norms)\n        t_norms = np.array(t_norms)\n\n        # 1. Compute empirical order p\n        log_alphas = np.log(alphas)\n        log_r_norms = np.log(r_norms + epsilon)\n        # linear regression of log_r_norms on log_alphas\n        lin_reg_result = stats.linregress(log_alphas, log_r_norms)\n        p = lin_reg_result.slope\n        \n        # 2. Compute monotonicity indicator for relative error\n        # Add epsilon to denominator to handle cases where t_norms is zero,\n        # as specified in the problem.\n        relative_errors = r_norms / (t_norms + epsilon)\n        \n        is_monotonic = True\n        # The list of alphas is already sorted in descending order.\n        # We check if the corresponding relative_errors are non-increasing.\n        for i in range(len(relative_errors) - 1):\n            if relative_errors[i+1] > relative_errors[i] + tau:\n                is_monotonic = False\n                break\n        \n        results.append(str(p))\n        results.append(str(is_monotonic))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "现在，我们将伴随方法应用于其核心目标之一：量化单个观测对分析或预报结果的影响。本练习将引导你对比两种评估观测影响的策略：一种是高效的、基于伴随的线性灵敏度近似，另一种是计算成本高昂但结果更精确的“暴力移除”实验。 通过在一个具有饱和效应的非线性观测算子下进行比较，你将直观地看到线性近似在现实场景中的优势与不足，并识别出它何时能提供准确的估计，又在何时可能产生误导。",
            "id": "3406515",
            "problem": "考虑一个具有非线性、饱和观测算子的静态三维变分(3D-Var)资料同化问题。状态向量为 $\\mathbf{x} \\in \\mathbb{R}^n$，观测向量为 $\\mathbf{y} \\in \\mathbb{R}^m$。需要最小化的代价函数为\n$$\nJ(\\mathbf{x}) = \\frac{1}{2}(\\mathbf{x}-\\mathbf{x}_b)^\\top \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\frac{1}{2}\\left(\\mathbf{h}(\\mathbf{x}) - \\mathbf{y}\\right)^\\top \\mathbf{R}^{-1}\\left(\\mathbf{h}(\\mathbf{x}) - \\mathbf{y}\\right),\n$$\n其中 $\\mathbf{x}_b \\in \\mathbb{R}^n$ 是背景场状态，$\\mathbf{B} \\in \\mathbb{R}^{n \\times n}$ 是对称正定的背景场误差协方差，$\\mathbf{R} \\in \\mathbb{R}^{m \\times m}$ 是对称正定的观测误差协方差，观测算子按元素定义为\n$$\n\\mathbf{h}(\\mathbf{x}) = \\tanh(\\mathbf{C}\\mathbf{x}),\n$$\n其中 $\\mathbf{C} \\in \\mathbb{R}^{m \\times n}$，$\\tanh(\\cdot)$ 按元素作用。\n\n定义一个标量预报指标 $f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x}$，其中权重向量 $\\mathbf{w} \\in \\mathbb{R}^n$ 是固定的。目标是使用两种方法量化和比较观测影响：\n- 一种源自最优性条件和高斯-牛顿线性化的切线性伴随敏感性近似。\n- 一种通过剔除一个观测后重新优化代价函数得到的有限差分“蛮力”剔除影响。\n\n使用以下基本要素作为您推导的起点：\n- 最小化子 $\\mathbf{x}_a$ 的最优性条件由梯度为零给出，即 $\\nabla J(\\mathbf{x}_a) = \\mathbf{0}$。\n- $\\mathbf{h}(\\mathbf{x})$ 的雅可比矩阵为 $\\mathbf{H}(\\mathbf{x}) = \\operatorname{diag}\\left(\\operatorname{sech}^2(\\mathbf{C}\\mathbf{x})\\right)\\mathbf{C}$，其中 $\\operatorname{sech}(z) = 1/\\cosh(z)$ 且 $\\operatorname{sech}^2(z) = 1 - \\tanh^2(z)$。\n- $J(\\mathbf{x})$ 的海森矩阵由背景项和观测项组成，后者既包括高斯-牛顿近似，也包括来自 $\\mathbf{h}(\\mathbf{x})$ 的二阶导数贡献。\n\n对于下面指定的每个测试案例，请用纯数学和算法术语执行以下步骤：\n1. 使用包含 $\\mathbf{h}$ 完整二阶导数的海森矩阵，通过二阶方法精确最小化 $J(\\mathbf{x})$（至数值公差），计算分析状态 $\\mathbf{x}_a$。\n2. 对于每个观测索引 $i \\in \\{1,\\dots,m\\}$，通过将第 $i$ 个观测的权重设置为零来定义剔除实验。形式上，引入一个对角缩放向量 $\\boldsymbol{\\alpha} \\in \\mathbb{R}^m$ 应用于观测项，使得修改后的代价函数为\n$$\nJ_{\\boldsymbol{\\alpha}}(\\mathbf{x}) = \\frac{1}{2}(\\mathbf{x}-\\mathbf{x}_b)^\\top \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\frac{1}{2}\\left(\\mathbf{h}(\\mathbf{x}) - \\mathbf{y}\\right)^\\top \\operatorname{diag}(\\boldsymbol{\\alpha})\\mathbf{R}^{-1}\\left(\\mathbf{h}(\\mathbf{x}) - \\mathbf{y}\\right).\n$$\n基线是 $\\boldsymbol{\\alpha} = \\mathbf{1}$；剔除观测 $i$ 会设置 $\\alpha_i = 0$ 同时保持 $j \\neq i$ 的 $\\alpha_j = 1$。计算重新优化后的分析 $\\mathbf{x}_a^{(-i)}$ 和对预报指标的蛮力剔除影响\n$$\n\\Delta f_i^{\\text{bf}} = f(\\mathbf{x}_a^{(-i)}) - f(\\mathbf{x}_a).\n$$\n3. 从第一性原理出发，通过最优性条件和高斯-牛顿近似（在海森矩阵中忽略 $\\mathbf{h}$ 的二阶导数项），推导 $f(\\mathbf{x})$ 对观测缩放参数 $\\alpha_i$ 的切线性伴随敏感性。用此获得剔除影响的一阶近似\n$$\n\\Delta f_i^{\\text{tl}} \\approx \\text{(使用高斯-牛顿逆矩阵、雅可比矩阵和分析新息的适当线性化)}.\n$$\n在分析状态 $\\mathbf{x}_a$ 处实现此近似，并为每个 $i$ 计算 $\\Delta f_i^{\\text{tl}}$。\n4. 通过计算以下指标，比较所有 $i$ 的 $\\Delta f_i^{\\text{bf}}$ 和 $\\Delta f_i^{\\text{tl}}$：\n   - 最大绝对差异 $E_{\\max} = \\max_i \\left|\\Delta f_i^{\\text{bf}} - \\Delta f_i^{\\text{tl}}\\right|$。\n   - 低估计数，定义为满足 $\\left|\\Delta f_i^{\\text{tl}}\\right| < \\left|\\Delta f_i^{\\text{bf}}\\right|\\cdot(1 - \\varepsilon)$ 的索引 $i$ 的数量。\n   - 高估计数，定义为满足 $\\left|\\Delta f_i^{\\text{tl}}\\right| > \\left|\\Delta f_i^{\\text{bf}}\\right|\\cdot(1 + \\varepsilon)$ 的索引 $i$ 的数量。\n使用相对容差 $\\varepsilon = 0.05$ (小数形式)进行分类。任何其余的索引都被认为在容差范围内，不计入任何一类。\n5. 将每个测试案例的结果报告为三元组 $[E_{\\max}, \\text{count\\_under}, \\text{count\\_over}]$。\n\n该问题不涉及物理单位；所有量均为无量纲。\n\n测试套件：\n所有测试都使用 $n = 3$, $m = 3$，以及相同的 $\\mathbf{B}$、$\\mathbf{R}$ 和 $\\mathbf{w}$，但 $\\mathbf{C}$、$\\mathbf{x}_b$、$\\mathbf{x}_t$ 和观测噪声不同。对于每个测试，观测向量由 $\\mathbf{y} = \\tanh(\\mathbf{C}\\mathbf{x}_t) + \\boldsymbol{\\eta}$ 生成。\n\n公共参数：\n- $\\mathbf{B} = \\operatorname{diag}\\left([0.4^2,\\,0.6^2,\\,0.8^2]\\right)$。\n- $\\mathbf{R} = \\operatorname{diag}\\left([0.1^2,\\,0.2^2,\\,0.3^2]\\right)$。\n- $\\mathbf{w} = [1.0,\\,-0.5,\\,0.25]^\\top$。\n\n测试 1 (近线性区域)：\n- $\\mathbf{C} = \\begin{bmatrix} 0.5  & 0.0  & 0.1 \\\\ 0.0  & 0.5  & -0.2 \\\\ 0.1  & -0.1  & 0.4 \\end{bmatrix}$。\n- $\\mathbf{x}_b = [0.2,\\,-0.1,\\,0.05]^\\top$。\n- $\\mathbf{x}_t = [0.25,\\,-0.15,\\,0.10]^\\top$。\n- $\\boldsymbol{\\eta} = [0.01,\\,-0.005,\\,0.02]^\\top$。\n\n测试 2 (中等饱和区域)：\n- $\\mathbf{C} = \\begin{bmatrix} 2.0  & 0.0  & 1.0 \\\\ 0.0  & 2.5  & -1.5 \\\\ 1.0  & -1.0  & 2.0 \\end{bmatrix}$。\n- $\\mathbf{x}_b = [1.0,\\,-1.5,\\,0.8]^\\top$。\n- $\\mathbf{x}_t = [1.2,\\,-1.3,\\,1.0]^\\top$。\n- $\\boldsymbol{\\eta} = [0.05,\\,0.02,\\,-0.04]^\\top$。\n\n测试 3 (强饱和区域)：\n- $\\mathbf{C} = \\begin{bmatrix} 5.0  & -3.0  & 2.0 \\\\ -4.0  & 6.0  & -3.0 \\\\ 3.5  & -2.5  & 5.5 \\end{bmatrix}$。\n- $\\mathbf{x}_b = [2.0,\\,-2.0,\\,2.0]^\\top$。\n- $\\mathbf{x}_t = [2.2,\\,-2.2,\\,2.1]^\\top$。\n- $\\boldsymbol{\\eta} = [0.01,\\,-0.02,\\,0.03]^\\top$。\n\n最终输出格式：\n您的程序应生成单行输出，包含三个测试的结果，形式为逗号分隔的列表并用方括号括起来，其中每个测试案例贡献其自己的方括号括起来的三元组。例如，输出应如下所示：\n$$\n[\\,[E_{\\max}^{(1)},\\,\\text{count\\_under}^{(1)},\\,\\text{count\\_over}^{(1)}],\\,[E_{\\max}^{(2)},\\,\\text{count\\_under}^{(2)},\\,\\text{count\\_over}^{(2)}],\\,[E_{\\max}^{(3)},\\,\\text{count\\_under}^{(3)},\\,\\text{count\\_over}^{(3)}]\\,].\n$$\n所有数字必须是纯小数或整数，不带附加文本。计算必须完全自包含，并能用指定的参数和过程复现。",
            "solution": "目标是在三维变分(3D-Var)资料同化背景下，分析单个观测对一个预报指标的影响。观测算子是非线性的，并表现出饱和特性。我们将比较两种量化此影响的方法：一种是“蛮力”法，涉及在剔除一个观测后重新优化代价函数；另一种是计算成本更低的切线性伴随敏感性近似法。\n\n状态向量为 $\\mathbf{x} \\in \\mathbb{R}^n$，背景场状态为 $\\mathbf{x}_b \\in \\mathbb{R}^n$，观测向量为 $\\mathbf{y} \\in \\mathbb{R}^m$。需要最小化的代价函数是：\n$$\nJ(\\mathbf{x}) = \\frac{1}{2}(\\mathbf{x}-\\mathbf{x}_b)^\\top \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\frac{1}{2}\\left(\\mathbf{h}(\\mathbf{x}) - \\mathbf{y}\\right)^\\top \\mathbf{R}^{-1}\\left(\\mathbf{h}(\\mathbf{x}) - \\mathbf{y}\\right)\n$$\n其中 $\\mathbf{B}$ 和 $\\mathbf{R}$ 分别是背景场和观测的误差协方差矩阵。观测算子是 $\\mathbf{h}(\\mathbf{x}) = \\tanh(\\mathbf{C}\\mathbf{x})$，按元素作用。预报指标是状态的线性函数，$f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x}$。\n\n### 第1部分：完全优化和蛮力影响\n\n为了找到最小化 $J(\\mathbf{x})$ 的分析状态 $\\mathbf{x}_a$，我们必须求解最优性条件 $\\nabla J(\\mathbf{x}_a) = \\mathbf{0}$。这个非线性系统使用二阶数值优化方法求解，具体是牛顿-拉弗森法，它需要代价函数的梯度和海森矩阵。\n\n$J(\\mathbf{x})$ 的梯度是：\n$$\n\\nabla J(\\mathbf{x}) = \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\mathbf{H}(\\mathbf{x})^\\top \\mathbf{R}^{-1}(\\mathbf{h}(\\mathbf{x}) - \\mathbf{y})\n$$\n其中 $\\mathbf{H}(\\mathbf{x}) = \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{x}}$ 是观测算子的雅可比矩阵。给定 $\\mathbf{h}(\\mathbf{x}) = \\tanh(\\mathbf{C}\\mathbf{x})$，其雅可比矩阵为 $\\mathbf{H}(\\mathbf{x}) = \\operatorname{diag}(\\operatorname{sech}^2(\\mathbf{C}\\mathbf{x}))\\mathbf{C}$。\n\n$J(\\mathbf{x})$ 的海森矩阵，即牛顿法所需，是梯度的导数：\n$$\n\\nabla^2 J(\\mathbf{x}) = \\mathbf{B}^{-1} + \\mathbf{H}(\\mathbf{x})^\\top \\mathbf{R}^{-1} \\mathbf{H}(\\mathbf{x}) + \\sum_{k=1}^m \\left[\\mathbf{R}^{-1}(\\mathbf{h}(\\mathbf{x})-\\mathbf{y})\\right]_k \\nabla^2 h_k(\\mathbf{x})\n$$\n前两项代表了海森矩阵的高斯-牛顿近似。第三项涉及观测算子的二阶导数。对于 $h_k(\\mathbf{x}) = \\tanh((\\mathbf{C}\\mathbf{x})_k)$，其海森矩阵为 $\\nabla^2 h_k(\\mathbf{x}) = -2\\tanh((\\mathbf{C}\\mathbf{x})_k)\\operatorname{sech}^2((\\mathbf{C}\\mathbf{x})_k) \\mathbf{C}_{k,:}^\\top \\mathbf{C}_{k,:}$，其中 $\\mathbf{C}_{k,:}$ 是 $\\mathbf{C}$ 的第 $k$ 行。\n\n寻找 $\\mathbf{x}_a$ 的牛顿-拉弗森迭代法是：\n$$\n\\mathbf{x}_{j+1} = \\mathbf{x}_j - [\\nabla^2 J(\\mathbf{x}_j)]^{-1} \\nabla J(\\mathbf{x}_j)\n$$\n从一个初始猜测开始，通常是 $\\mathbf{x}_0 = \\mathbf{x}_b$。\n\n剔除第 $i$ 个观测的蛮力影响 $\\Delta f_i^{\\text{bf}}$，是通过首先找到一个新的分析状态 $\\mathbf{x}_a^{(-i)}$ 来计算的，该状态最小化一个修改后的代价函数 $J^{(-i)}(\\mathbf{x})$，其中第 $i$ 个观测项被省略。这相当于将 $\\mathbf{R}^{-1}$ 的第 $i$ 个对角元素设置为零。我们重新运行完整的牛顿-拉弗森最小化过程以找到 $\\mathbf{x}_a^{(-i)}$。然后，蛮力影响为：\n$$\n\\Delta f_i^{\\text{bf}} = f(\\mathbf{x}_a^{(-i)}) - f(\\mathbf{x}_a) = \\mathbf{w}^\\top (\\mathbf{x}_a^{(-i)} - \\mathbf{x}_a)\n$$\n对每个观测 $i \\in \\{1, \\dots, m\\}$ 重复此过程。\n\n### 第2部分：切线性伴随敏感性影响\n\n这种方法提供了观测影响的一阶近似，而无需重新运行优化。我们分析分析状态 $\\mathbf{x}_a$ 对第 $i$ 个观测的缩放参数 $\\alpha_i$ 的敏感性。修改后的代价函数是：\n$$\nJ_{\\boldsymbol{\\alpha}}(\\mathbf{x}) = \\frac{1}{2}(\\mathbf{x}-\\mathbf{x}_b)^\\top \\mathbf{B}^{-1}(\\mathbf{x}-\\mathbf{x}_b) + \\frac{1}{2}\\left(\\mathbf{h}(\\mathbf{x}) - \\mathbf{y}\\right)^\\top \\operatorname{diag}(\\boldsymbol{\\alpha})\\mathbf{R}^{-1}\\left(\\mathbf{h}(\\mathbf{x}) - \\mathbf{y}\\right)\n$$\n基线分析使用 $\\boldsymbol{\\alpha} = \\mathbf{1}$。剔除观测 $i$ 对应于将 $\\alpha_i$ 从 $1$ 变为 $0$，即 $\\Delta \\alpha_i = -1$。\n\n分析状态 $\\mathbf{x}_a(\\boldsymbol{\\alpha})$ 由最优性条件 $\\nabla_x J_{\\boldsymbol{\\alpha}}(\\mathbf{x}_a(\\boldsymbol{\\alpha})) = \\mathbf{0}$ 定义。我们对这个恒等式关于 $\\alpha_i$求导，并在基线 $\\boldsymbol{\\alpha}=\\mathbf{1}$ 和 $\\mathbf{x}=\\mathbf{x}_a$ 处求值：\n$$\n\\frac{d}{d\\alpha_i} \\left[ \\nabla_x J_{\\boldsymbol{\\alpha}}(\\mathbf{x}_a(\\boldsymbol{\\alpha})) \\right] \\bigg|_{\\boldsymbol{\\alpha}=\\mathbf{1}} = \\left( \\nabla_x^2 J_{\\boldsymbol{\\alpha}} \\right) \\frac{d\\mathbf{x}_a}{d\\alpha_i} + \\frac{\\partial}{\\partial \\alpha_i}(\\nabla_x J_{\\boldsymbol{\\alpha}}) = \\mathbf{0}\n$$\n问题指定使用海森矩阵的高斯-牛顿近似，$\\mathcal{H}_a = \\nabla_x^2 J_{\\boldsymbol{\\alpha}} \\approx \\mathbf{B}^{-1} + \\mathbf{H}(\\mathbf{x}_a)^\\top\\mathbf{R}^{-1}\\mathbf{H}(\\mathbf{x}_a)$。偏导数项（强迫项）是：\n$$\n\\frac{\\partial}{\\partial \\alpha_i}(\\nabla_x J_{\\boldsymbol{\\alpha}}) = \\mathbf{H}(\\mathbf{x}_a)^\\top \\mathbf{e}_i \\mathbf{e}_i^\\top \\mathbf{R}^{-1}(\\mathbf{h}(\\mathbf{x}_a) - \\mathbf{y}) = \\mathbf{H}_{i,:}(\\mathbf{x}_a)^\\top \\frac{h_i(\\mathbf{x}_a) - y_i}{R_{ii}}\n$$\n其中 $\\mathbf{H}_{i,:}$ 是 $\\mathbf{H}$ 的第 $i$ 行， $R_{ii}$ 是 $\\mathbf{R}$ 的第 $i$ 个对角元素（因为给定的 $\\mathbf{R}$ 是对角矩阵）。\n\n求解分析状态的敏感性：\n$$\n\\frac{d\\mathbf{x}_a}{d\\alpha_i} = - \\mathcal{H}_a^{-1} \\mathbf{H}_{i,:}(\\mathbf{x}_a)^\\top \\frac{h_i(\\mathbf{x}_a) - y_i}{R_{ii}}\n$$\n预报指标 $f$ 的敏感性是 $\\frac{df}{d\\alpha_i} = \\mathbf{w}^\\top \\frac{d\\mathbf{x}_a}{d\\alpha_i}$。影响的切线性近似为 $\\Delta f_i^{\\text{tl}} \\approx \\frac{df}{d\\alpha_i} \\Delta\\alpha_i$。由于 $\\Delta\\alpha_i = -1$，我们得到：\n$$\n\\Delta f_i^{\\text{tl}} = \\left( -\\mathbf{w}^\\top \\mathcal{H}_a^{-1} \\mathbf{H}_{i,:}(\\mathbf{x}_a)^\\top \\frac{h_i(\\mathbf{x}_a) - y_i}{R_{ii}} \\right) \\times (-1) = \\mathbf{w}^\\top \\mathcal{H}_a^{-1} \\mathbf{H}_{i,:}(\\mathbf{x}_a)^\\top \\frac{h_i(\\mathbf{x}_a) - y_i}{R_{ii}}\n$$\n对每个观测 $i$ 执行此计算。它重用了分析状态 $\\mathbf{x}_a$ 及其相关量，仅需一次海森矩阵求逆（$\\mathcal{H}_a^{-1}$）即可计算所有观测的影响，使其远比蛮力法高效。\n\n### 第3部分：比较\n\n比较两组影响 $\\{\\Delta f_i^{\\text{bf}}\\}$ 和 $\\{\\Delta f_i^{\\text{tl}}\\}$，以评估切线性近似的准确性。差异来源于两个方面：使用高斯-牛顿海森矩阵（忽略了二阶导数项）和对有限变化（$\\Delta\\alpha_i=-1$）的一阶（线性）近似。随着观测算子变得更加非线性（即在饱和区域），差异预计会增加。\n\n比较指标是：\n1.  最大绝对差异：$E_{\\max} = \\max_i \\left|\\Delta f_i^{\\text{bf}} - \\Delta f_i^{\\text{tl}}\\right|$。\n2.  低估计数：切线性影响的绝对值显著小于蛮力影响的索引 $i$ 的数量：$\\left|\\Delta f_i^{\\text{tl}}\\right| < \\left|\\Delta f_i^{\\text{bf}}\\right|\\cdot(1 - \\varepsilon)$，容差 $\\varepsilon = 0.05$。\n3.  高估计数：切线性影响的绝对值显著大于蛮力影响的索引 $i$ 的数量：$\\left|\\Delta f_i^{\\text{tl}}\\right| > \\left|\\Delta f_i^{\\text{bf}}\\right|\\cdot(1 + \\varepsilon)$。\n\n以下算法为每个测试案例实现了此过程。",
            "answer": "```python\nimport numpy as np\n\ndef h_op(x, C):\n    \"\"\"Computes the nonlinear observation operator h(x) = tanh(Cx).\"\"\"\n    return np.tanh(C @ x)\n\ndef jacobian_h(x, C):\n    \"\"\"Computes the Jacobian of h(x).\"\"\"\n    z = C @ x\n    sech2_z = 1.0 - np.tanh(z)**2\n    # sech2_z is a vector, so diag(sech2_z) is a diagonal matrix\n    H = np.diag(sech2_z) @ C\n    return H\n\ndef gradient(x, xb, y, C, B_inv, R_inv_diag, alpha_diag):\n    \"\"\"Computes the gradient of the cost function J_alpha(x).\"\"\"\n    dx = x - xb\n    grad_b = B_inv @ dx\n    \n    H = jacobian_h(x, C)\n    d_obs = h_op(x, C) - y\n    \n    # Efficient calculation for diagonal R and alpha\n    weighted_innov = alpha_diag * R_inv_diag * d_obs\n    grad_o = H.T @ weighted_innov\n    \n    return grad_b + grad_o\n\ndef hessian(x, xb, y, C, B_inv, R_inv_diag, alpha_diag):\n    \"\"\"Computes the full Hessian of the cost function J_alpha(x).\"\"\"\n    m, n = C.shape\n    H = jacobian_h(x, C)\n    hess_b = B_inv\n    \n    # Gauss-Newton part of the Hessian\n    hess_gn_o = H.T @ np.diag(alpha_diag * R_inv_diag) @ H\n    \n    # Second derivative term\n    hess_s_o = np.zeros((n, n))\n    z = C @ x\n    tanh_z = np.tanh(z)\n    sech2_z = 1.0 - tanh_z**2\n    \n    weighted_innov = alpha_diag * R_inv_diag * (h_op(x, C) - y)\n\n    for k in range(m):\n        C_k_T = C[k, :].reshape(-1, 1)  # k-th row of C as a column vector\n        term = weighted_innov[k] * (-2.0 * tanh_z[k] * sech2_z[k])\n        hess_s_o_k = term * (C_k_T @ C_k_T.T) # Outer product\n        hess_s_o += hess_s_o_k\n        \n    return hess_b + hess_gn_o + hess_s_o\n\ndef minimize_cost(xb, y, C, B_inv, R_inv_diag, alpha_diag):\n    \"\"\"Minimizes the cost function using Newton's method.\"\"\"\n    x_k = xb.copy()\n    max_iter = 50\n    tol = 1e-10\n\n    for _ in range(max_iter):\n        grad = gradient(x_k, xb, y, C, B_inv, R_inv_diag, alpha_diag)\n        if np.linalg.norm(grad) < tol:\n            break\n        \n        hess = hessian(x_k, xb, y, C, B_inv, R_inv_diag, alpha_diag)\n        \n        try:\n            step = np.linalg.solve(hess, grad)\n        except np.linalg.LinAlgError:\n            step = np.linalg.pinv(hess) @ grad\n        \n        x_k = x_k - step\n    \n    return x_k\n\ndef solve():\n    \"\"\"Main solver function to run all test cases.\"\"\"\n    \n    # Common parameters\n    B = np.diag([0.4**2, 0.6**2, 0.8**2])\n    R = np.diag([0.1**2, 0.2**2, 0.3**2])\n    w = np.array([1.0, -0.5, 0.25])\n    \n    m, n = R.shape[0], B.shape[0]\n    B_inv = np.linalg.inv(B)\n    R_inv_diag = 1.0 / np.diag(R)\n    eps = 0.05\n\n    test_cases = [\n        # Test 1 (near-linear)\n        {\n            \"C\": np.array([[0.5, 0.0, 0.1], [0.0, 0.5, -0.2], [0.1, -0.1, 0.4]]),\n            \"xb\": np.array([0.2, -0.1, 0.05]),\n            \"xt\": np.array([0.25, -0.15, 0.10]),\n            \"eta\": np.array([0.01, -0.005, 0.02]),\n        },\n        # Test 2 (moderate saturation)\n        {\n            \"C\": np.array([[2.0, 0.0, 1.0], [0.0, 2.5, -1.5], [1.0, -1.0, 2.0]]),\n            \"xb\": np.array([1.0, -1.5, 0.8]),\n            \"xt\": np.array([1.2, -1.3, 1.0]),\n            \"eta\": np.array([0.05, 0.02, -0.04]),\n        },\n        # Test 3 (strong saturation)\n        {\n            \"C\": np.array([[5.0, -3.0, 2.0], [-4.0, 6.0, -3.0], [3.5, -2.5, 5.5]]),\n            \"xb\": np.array([2.0, -2.0, 2.0]),\n            \"xt\": np.array([2.2, -2.2, 2.1]),\n            \"eta\": np.array([0.01, -0.02, 0.03]),\n        },\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        C, xb, xt, eta = case[\"C\"], case[\"xb\"], case[\"xt\"], case[\"eta\"]\n        y = h_op(xt, C) + eta\n        \n        # 1. Compute baseline analysis xa\n        alpha_full = np.ones(m)\n        xa = minimize_cost(xb, y, C, B_inv, R_inv_diag, alpha_full)\n        \n        # 2. Compute brute-force impacts delta_f_bf\n        delta_f_bf = np.zeros(m)\n        for i in range(m):\n            alpha_removed = np.ones(m)\n            alpha_removed[i] = 0.0\n            xa_minus_i = minimize_cost(xb, y, C, B_inv, R_inv_diag, alpha_removed)\n            delta_f_bf[i] = w.T @ (xa_minus_i - xa)\n\n        # 3. Compute tangent-linear impacts delta_f_tl\n        Ha = jacobian_h(xa, C)\n        Hess_GN = B_inv + Ha.T @ np.diag(R_inv_diag) @ Ha\n        \n        # Adjoint method for efficiency\n        # Solve H_GN * adj = w for the adjoint vector\n        adj = np.linalg.solve(Hess_GN, w)\n        \n        # Project adjoint onto observation space\n        v_adj = Ha @ adj\n        \n        innov_a = h_op(xa, C) - y\n        \n        # delta_f_tl = (H * adj) * innov_a / R_ii\n        delta_f_tl = v_adj * innov_a * R_inv_diag\n\n        # 4. Compare impacts and compute metrics\n        E_max = np.max(np.abs(delta_f_bf - delta_f_tl))\n        \n        abs_bf = np.abs(delta_f_bf)\n        abs_tl = np.abs(delta_f_tl)\n        \n        # Avoid division by zero if an observation has no impact\n        # We only count underestimates/overestimates for non-zero impacts\n        non_zero_impact_mask = abs_bf > 1e-15\n        \n        count_under = np.sum(abs_tl[non_zero_impact_mask] < abs_bf[non_zero_impact_mask] * (1.0 - eps))\n        count_over = np.sum(abs_tl[non_zero_impact_mask] > abs_bf[non_zero_impact_mask] * (1.0 + eps))\n        \n        all_results.append([E_max, int(count_under), int(count_over)])\n\n    results_str_list = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in all_results]\n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n```"
        }
    ]
}