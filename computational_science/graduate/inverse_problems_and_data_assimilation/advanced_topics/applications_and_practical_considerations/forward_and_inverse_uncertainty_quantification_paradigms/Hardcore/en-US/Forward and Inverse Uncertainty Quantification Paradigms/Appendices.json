{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of uncertainty quantification is understanding the duality between a model's sensitivity to its parameters (a forward problem) and the uncertainty in those parameters when inferred from data (an inverse problem). This exercise () provides a direct, analytical link between these two paradigms through the lens of the Fisher Information Matrix. By deriving this matrix for a simple nonlinear model, you will see precisely how the model's forward sensitivities, combined with the experimental design, dictate the fundamental limits of precision in parameter estimation.",
            "id": "3382635",
            "problem": "Consider a sequence of design points $\\{x_{i}\\}_{i=1}^{n}$ treated as fixed and known, and observations $\\{y_{i}\\}_{i=1}^{n}$ generated by the forward model $y_{i} = \\exp(\\theta x_{i}) + \\eta_{i}$, where $\\theta \\in \\mathbb{R}$ is an unknown scalar parameter and the measurement errors $\\{\\eta_{i}\\}_{i=1}^{n}$ are independent and identically distributed (i.i.d.) with $\\eta_{i} \\sim \\mathcal{N}(0,\\sigma^{2})$ for a known noise variance $\\sigma^{2} > 0$. In the forward Uncertainty Quantification (UQ) paradigm, the mapping $\\theta \\mapsto \\{\\exp(\\theta x_{i})\\}_{i=1}^{n}$ propagates parameter uncertainty to data space; in the inverse UQ paradigm for data assimilation, one quantifies parameter uncertainty from data via statistical information in the likelihood. \n\nUsing only the definitions of the likelihood for independent Gaussian noise and the Fisher information, derive the Fisher information $I(\\theta)$ for $\\theta$, expressed in terms of $\\{x_{i}\\}_{i=1}^{n}$, $\\sigma^{2}$, and $\\theta$. Then use $I(\\theta)$ to assess parameter sloppiness by characterizing, in terms of $\\{x_{i}\\}_{i=1}^{n}$ and $\\theta$, when the information is small versus large, and explain how this connects forward sensitivity to inverse uncertainty through the Cramér–Rao lower bound (CRLB). \n\nYour final answer must be the analytical expression for the unique eigenvalue of the Fisher information matrix for $\\theta$ (which in this scalar-parameter setting equals $I(\\theta)$). Do not include units. No numerical evaluation is required, and no rounding is needed. Report only the closed-form expression for the eigenvalue as your final answer.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It provides a complete and consistent setup for a standard problem in statistical inference and uncertainty quantification.\n\nThe problem asks for the derivation of the Fisher information for a parameter $\\theta$ in a nonlinear regression model, and an analysis based on this information.\n\nFirst, we establish the likelihood function for the observations. The forward model is given by $y_i = \\exp(\\theta x_i) + \\eta_i$, where the errors $\\eta_i$ are independent and identically distributed (i.i.d.) from a normal distribution $\\mathcal{N}(0, \\sigma^2)$. The probability density function (PDF) for a single error term $\\eta_i$ is:\n$$p(\\eta_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{\\eta_i^2}{2\\sigma^2}\\right)$$\nSince $\\eta_i = y_i - \\exp(\\theta x_i)$, the conditional PDF for a single observation $y_i$ given the parameter $\\theta$ is:\n$$p(y_i|\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\exp(\\theta x_i))^2}{2\\sigma^2}\\right)$$\nThe observations $\\{y_i\\}_{i=1}^{n}$ are independent, so the joint likelihood function $L(\\theta)$ for the entire dataset is the product of the individual PDFs:\n$$L(\\theta) = \\prod_{i=1}^{n} p(y_i|\\theta) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\exp(\\theta x_i))^2}{2\\sigma^2}\\right)$$\nIt is more convenient to work with the log-likelihood function, $\\ell(\\theta) = \\ln(L(\\theta))$:\n$$\\ell(\\theta) = \\ln\\left( (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\sum_{i=1}^{n}\\frac{(y_i - \\exp(\\theta x_i))^2}{2\\sigma^2}\\right) \\right)$$\n$$\\ell(\\theta) = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (y_i - \\exp(\\theta x_i))^2$$\nThe Fisher information, $I(\\theta)$, for a scalar parameter $\\theta$ is defined as the negative expected value of the second derivative of the log-likelihood function with respect to $\\theta$. The expectation is taken over the distribution of the data $\\{y_i\\}$.\n$$I(\\theta) = -E\\left[ \\frac{\\partial^2 \\ell(\\theta)}{\\partial \\theta^2} \\right]$$\nWe compute the first and second derivatives of $\\ell(\\theta)$. The first derivative (the score function) is:\n$$\\frac{\\partial \\ell}{\\partial \\theta} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\theta} (y_i - \\exp(\\theta x_i))^2$$\nUsing the chain rule, where the derivative of the inner term $(y_i - \\exp(\\theta x_i))$ with respect to $\\theta$ is $-x_i \\exp(\\theta x_i)$:\n$$\\frac{\\partial \\ell}{\\partial \\theta} = -\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2(y_i - \\exp(\\theta x_i))(-x_i \\exp(\\theta x_i))$$\n$$\\frac{\\partial \\ell}{\\partial \\theta} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\exp(\\theta x_i))x_i \\exp(\\theta x_i)$$\nNow, we compute the second derivative:\n$$\\frac{\\partial^2 \\ell}{\\partial \\theta^2} = \\frac{\\partial}{\\partial \\theta} \\left[ \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i - \\exp(\\theta x_i))x_i \\exp(\\theta x_i) \\right]$$\nApplying the product rule to the term $(y_i - \\exp(\\theta x_i))(x_i \\exp(\\theta x_i))$:\n$$\\frac{\\partial^2 \\ell}{\\partial \\theta^2} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left[ (-x_i \\exp(\\theta x_i))(x_i \\exp(\\theta x_i)) + (y_i - \\exp(\\theta x_i))(x_i^2 \\exp(\\theta x_i)) \\right]$$\n$$\\frac{\\partial^2 \\ell}{\\partial \\theta^2} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left[ -x_i^2 \\exp(2\\theta x_i) + y_i x_i^2 \\exp(\\theta x_i) - x_i^2 \\exp(2\\theta x_i) \\right]$$\nThis simplifies to:\n$$\\frac{\\partial^2 \\ell}{\\partial \\theta^2} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left[ y_i x_i^2 \\exp(\\theta x_i) - 2x_i^2 \\exp(2\\theta x_i) \\right]$$\nNext, we take the expectation with respect to the data $\\{y_i\\}$. From the model definition, $y_i = \\exp(\\theta x_i) + \\eta_i$ and $E[\\eta_i] = 0$, which implies $E[y_i] = E[\\exp(\\theta x_i) + \\eta_i] = \\exp(\\theta x_i)$.\n$$E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta^2}\\right] = E\\left[ \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left[ y_i x_i^2 \\exp(\\theta x_i) - 2x_i^2 \\exp(2\\theta x_i) \\right] \\right]$$\n$$E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta^2}\\right] = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left[ E[y_i] x_i^2 \\exp(\\theta x_i) - 2x_i^2 \\exp(2\\theta x_i) \\right]$$\nSubstituting $E[y_i] = \\exp(\\theta x_i)$:\n$$E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta^2}\\right] = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left[ (\\exp(\\theta x_i)) x_i^2 \\exp(\\theta x_i) - 2x_i^2 \\exp(2\\theta x_i) \\right]$$\n$$E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta^2}\\right] = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left[ x_i^2 \\exp(2\\theta x_i) - 2x_i^2 \\exp(2\\theta x_i) \\right] = - \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} x_i^2 \\exp(2\\theta x_i)$$\nFinally, the Fisher information is $I(\\theta) = -E\\left[\\frac{\\partial^2 \\ell}{\\partial \\theta^2}\\right]$:\n$$I(\\theta) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} x_i^2 \\exp(2\\theta x_i)$$\nThis is the expression for the Fisher information. For a scalar parameter, the Fisher information matrix is a $1 \\times 1$ matrix, and its single eigenvalue is equal to $I(\\theta)$ itself.\n\nNow, we assess parameter sloppiness and the connection between forward and inverse UQ.\nParameter sloppiness refers to the situation where the data provides little information about a parameter, making it difficult to estimate precisely. In this context, a small value of $I(\\theta)$ indicates sloppiness.\nFrom the expression for $I(\\theta)$, the information is small if:\n1. The noise variance $\\sigma^2$ is large. More noise obscures the signal, reducing information content.\n2. The design points $\\{x_i\\}$ are chosen such that the sum $\\sum_{i=1}^{n} x_i^2 \\exp(2\\theta x_i)$ is small. This occurs if the magnitudes of the $x_i$ are small (e.g., clustered around $0$). If all $x_i = 0$, $I(\\theta)=0$ and the parameter is non-identifiable. The informativeness of the experiment depends critically on the experimental design $\\{x_i\\}_{i=1}^n$.\n\nThe connection to forward and inverse UQ is established through the Cramér–Rao lower bound (CRLB) and the concept of forward sensitivity. The CRLB states that for any unbiased estimator $\\hat{\\theta}$ of $\\theta$, its variance is bounded from below by the inverse of the Fisher information:\n$$\\text{Var}(\\hat{\\theta}) \\ge \\frac{1}{I(\\theta)}$$\nThe term $\\text{Var}(\\hat{\\theta})$ represents the uncertainty in the estimated parameter (an inverse problem concept). A larger $I(\\theta)$ implies a smaller lower bound on this uncertainty.\n\nThe forward sensitivity of the model prediction $f_i(\\theta) = \\exp(\\theta x_i)$ with respect to the parameter $\\theta$ is its derivative:\n$$\\frac{\\partial f_i(\\theta)}{\\partial \\theta} = x_i \\exp(\\theta x_i)$$\nThis derivative quantifies how much the model output changes for a small change in the parameter (a forward UQ concept).\n\nBy substituting the sensitivity into the expression for $I(\\theta)$, we find the explicit link:\n$$I(\\theta) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left( x_i \\exp(\\theta x_i) \\right)^2 = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\left(\\frac{\\partial f_i(\\theta)}{\\partial \\theta}\\right)^2$$\nThis equation shows that the Fisher information is the sum of the squared forward sensitivities, scaled by the inverse noise variance. Therefore:\n- **High forward sensitivity** (large values of $|\\frac{\\partial f_i}{\\partial \\theta}|$) leads to **large Fisher information** $I(\\theta)$.\n- **Large Fisher information** $I(\\theta)$ leads to a **small CRLB** $\\frac{1}{I(\\theta)}$, implying the potential for **low inverse uncertainty** (low $\\text{Var}(\\hat{\\theta})$).\n\nIn conclusion, the forward sensitivity of the model directly governs the information content of the data, which in turn sets the fundamental limit on the precision of parameter estimation in the inverse problem. Parameter sloppiness occurs when the forward sensitivities are small at the chosen design points, leading to a small Fisher information and consequently large uncertainty in the parameter estimate.\nThe problem requests the unique eigenvalue of the Fisher information matrix for $\\theta$. As demonstrated, this is $I(\\theta)$.",
            "answer": "$$\n\\boxed{\\frac{1}{\\sigma^{2}} \\sum_{i=1}^{n} x_{i}^{2} \\exp(2\\theta x_{i})}\n$$"
        },
        {
            "introduction": "Real-world inverse problems are often ill-posed, challenging the simple notion of a single \"best-fit\" parameter with Gaussian error bars. This practice demonstrates how to analyze such a problem where the forward model is non-identifiable, leading to ambiguity that the Bayesian framework is uniquely suited to capture. By investigating a canonical model with structural non-identifiability (), you will learn to characterize the resulting posterior multimodality and understand its profound implications for the stability and uniqueness of the solution.",
            "id": "3382702",
            "problem": "Consider a scalar Bayesian inverse problem within the forward and inverse uncertainty quantification paradigms for inverse problems and data assimilation. Let the forward map be $G(\\theta) = \\theta^{2}$ with parameter $\\theta \\in \\mathbb{R}$. A single noisy observation $y \\in \\mathbb{R}$ is acquired via the additive Gaussian noise model $y = G(\\theta) + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$ and $\\sigma > 0$ is known. The prior on the parameter is Gaussian $\\theta \\sim \\mathcal{N}(0, \\tau^{2})$ with $\\tau > 0$ known. All quantities are scalars. Throughout, use Bayes’ rule and principled asymptotic approximation methods grounded in first principles; do not assume identifiability where it fails.\n\nThis setting exhibits non-identifiability because $G(\\theta) = G(-\\theta)$, which can lead to ill-posedness manifested as posterior multimodality and large posterior variance. Starting from first principles (Bayes’ rule, properties of Gaussian distributions, and the definition of well-posedness in the sense of Jacques Hadamard), derive an explicit leading-order analytic approximation of the posterior variance $\\mathrm{Var}[\\theta \\mid y]$ in the regime where the posterior is bimodal. In deriving your expression, justify the condition under which bimodality occurs in terms of $y$, $\\sigma$, and $\\tau$, and discuss the stability implications of this non-identifiability (existence, uniqueness, and continuous dependence on data).\n\nYour final answer must be a single closed-form analytic expression for the leading-order approximation of $\\mathrm{Var}[\\theta \\mid y]$ as a function of $y$, $\\sigma$, and $\\tau$. No rounding is required, and no units are involved.",
            "solution": "The problem is assessed to be valid as it represents a well-defined and standard exercise in Bayesian inverse problems. It is scientifically grounded in probability theory and statistics, internally consistent, and contains all necessary information to proceed with a formal derivation.\n\nWe are tasked with analyzing the posterior distribution of a parameter $\\theta$ given an observation $y$. The relationship is defined by the forward model $G(\\theta) = \\theta^{2}$ and the observation model $y = \\theta^{2} + \\varepsilon$, where the noise is Gaussian, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$. The prior belief about the parameter is also Gaussian, $\\theta \\sim \\mathcal{N}(0, \\tau^{2})$.\n\nFirst, we construct the posterior probability density function (PDF) $p(\\theta \\mid y)$ using Bayes' rule:\n$$\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)\n$$\nThe likelihood function $p(y \\mid \\theta)$ is derived from the noise model. Given a value of $\\theta$, $y$ is a random variable distributed as $\\mathcal{N}(\\theta^{2}, \\sigma^{2})$. Thus, the likelihood is:\n$$\np(y \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left( -\\frac{(y - \\theta^{2})^{2}}{2\\sigma^{2}} \\right)\n$$\nThe prior PDF is given as:\n$$\np(\\theta) = \\frac{1}{\\sqrt{2\\pi\\tau^{2}}} \\exp\\left( -\\frac{\\theta^{2}}{2\\tau^{2}} \\right)\n$$\nCombining these, the unnormalized posterior PDF is:\n$$\np(\\theta \\mid y) \\propto \\exp\\left( -\\frac{(y - \\theta^{2})^{2}}{2\\sigma^{2}} \\right) \\exp\\left( -\\frac{\\theta^{2}}{2\\tau^{2}} \\right) = \\exp\\left( -\\left[ \\frac{(y - \\theta^{2})^{2}}{2\\sigma^{2}} + \\frac{\\theta^{2}}{2\\tau^{2}} \\right] \\right)\n$$\nTo find the modes of the posterior distribution, we can find the minima of the negative log-posterior, $\\Phi(\\theta)$, where $p(\\theta \\mid y) \\propto \\exp(-\\Phi(\\theta))$:\n$$\n\\Phi(\\theta) = \\frac{(y - \\theta^{2})^{2}}{2\\sigma^{2}} + \\frac{\\theta^{2}}{2\\tau^{2}}\n$$\nThe modes are stationary points, found by setting the first derivative with respect to $\\theta$ to zero:\n$$\n\\frac{d\\Phi}{d\\theta} = \\frac{2(y - \\theta^{2})(-2\\theta)}{2\\sigma^{2}} + \\frac{2\\theta}{2\\tau^{2}} = -\\frac{2\\theta(y - \\theta^{2})}{\\sigma^{2}} + \\frac{\\theta}{\\tau^{2}} = \\theta \\left( \\frac{2\\theta^{2} - 2y}{\\sigma^{2}} + \\frac{1}{\\tau^{2}} \\right) = 0\n$$\nThis equation has solutions at $\\theta = 0$ and where the term in the parentheses is zero:\n$$\n\\frac{2\\theta^{2}}{\\sigma^{2}} = \\frac{2y}{\\sigma^{2}} - \\frac{1}{\\tau^{2}} \\implies \\theta^{2} = y - \\frac{\\sigma^{2}}{2\\tau^{2}}\n$$\nSo, the stationary points are $\\theta_{0} = 0$ and, provided $y > \\frac{\\sigma^{2}}{2\\tau^{2}}$, $\\theta_{\\pm} = \\pm \\sqrt{y - \\frac{\\sigma^{2}}{2\\tau^{2}}}$.\n\nTo determine if these are modes (local maxima of the PDF, minima of $\\Phi$) or antimodes (local minima of the PDF, maxima of $\\Phi$), we examine the second derivative of $\\Phi(\\theta)$:\n$$\n\\frac{d^{2}\\Phi}{d\\theta^{2}} = \\frac{d}{d\\theta} \\left( \\frac{2\\theta^{3}}{\\sigma^{2}} - \\frac{2y\\theta}{\\sigma^{2}} + \\frac{\\theta}{\\tau^{2}} \\right) = \\frac{6\\theta^{2}}{\\sigma^{2}} - \\frac{2y}{\\sigma^{2}} + \\frac{1}{\\tau^{2}}\n$$\nAt $\\theta = 0$:\n$$\n\\left.\\frac{d^{2}\\Phi}{d\\theta^{2}}\\right|_{\\theta=0} = \\frac{1}{\\tau^{2}} - \\frac{2y}{\\sigma^{2}}\n$$\nFor $\\theta=0$ to be a mode, we require $\\left.\\frac{d^{2}\\Phi}{d\\theta^{2}}\\right|_{\\theta=0} > 0$, which means $y  \\frac{\\sigma^{2}}{2\\tau^{2}}$. In this case, the posterior is unimodal.\nFor $\\theta=0$ to be an antimode, we require $\\left.\\frac{d^{2}\\Phi}{d\\theta^{2}}\\right|_{\\theta=0}  0$, which means $y > \\frac{\\sigma^{2}}{2\\tau^{2}}$.\n\nAt $\\theta_{\\pm} = \\pm \\sqrt{y - \\frac{\\sigma^{2}}{2\\tau^{2}}}$, substituting $\\theta_{\\pm}^{2} = y - \\frac{\\sigma^{2}}{2\\tau^{2}}$:\n$$\n\\left.\\frac{d^{2}\\Phi}{d\\theta^{2}}\\right|_{\\theta_{\\pm}} = \\frac{6}{\\sigma^{2}}\\left(y - \\frac{\\sigma^{2}}{2\\tau^{2}}\\right) - \\frac{2y}{\\sigma^{2}} + \\frac{1}{\\tau^{2}} = \\frac{6y}{\\sigma^{2}} - \\frac{3}{\\tau^{2}} - \\frac{2y}{\\sigma^{2}} + \\frac{1}{\\tau^{2}} = \\frac{4y}{\\sigma^{2}} - \\frac{2}{\\tau^{2}} = 2\\left(\\frac{2y}{\\sigma^{2}} - \\frac{1}{\\tau^{2}}\\right)\n$$\nThe points $\\theta_{\\pm}$ exist only when $y > \\frac{\\sigma^{2}}{2\\tau^{2}}$, which implies $\\frac{2y}{\\sigma^{2}} > \\frac{1}{\\tau^{2}}$. Therefore, $\\left.\\frac{d^{2}\\Phi}{d\\theta^{2}}\\right|_{\\theta_{\\pm}} > 0$.\nThis confirms that when $y > \\frac{\\sigma^{2}}{2\\tau^{2}}$, the posterior distribution is bimodal, with two modes at $\\theta_{\\pm} = \\pm \\sqrt{y - \\frac{\\sigma^{2}}{2\\tau^{2}}}$ and an antimode at $\\theta = 0$.\n\nNow we derive the leading-order approximation for the posterior variance $\\mathrm{Var}[\\theta \\mid y]$ in this bimodal regime. A common principled approximation is the Laplace approximation, which models the posterior as a mixture of Gaussian distributions centered at each mode. Due to the symmetry of the forward model and prior, the posterior is symmetric about $\\theta=0$. Therefore, the posterior can be approximated as a symmetric mixture of two Gaussians:\n$$\np(\\theta \\mid y) \\approx \\frac{1}{2} \\mathcal{N}(\\theta \\mid \\mu_{+}, \\sigma_{L}^{2}) + \\frac{1}{2} \\mathcal{N}(\\theta \\mid \\mu_{-}, \\sigma_{L}^{2})\n$$\nwhere the means $\\mu_{\\pm}$ are the mode locations:\n$$\n\\mu_{\\pm} = \\theta_{\\pm} = \\pm \\sqrt{y - \\frac{\\sigma^{2}}{2\\tau^{2}}}\n$$\nand the local variance $\\sigma_{L}^{2}$ is the inverse of the Hessian of $\\Phi(\\theta)$ evaluated at the modes:\n$$\n\\sigma_{L}^{2} = \\left( \\left.\\frac{d^{2}\\Phi}{d\\theta^{2}}\\right|_{\\theta_{\\pm}} \\right)^{-1} = \\left( 2\\left(\\frac{2y}{\\sigma^{2}} - \\frac{1}{\\tau^{2}}\\right) \\right)^{-1} = \\left( \\frac{4y\\tau^{2} - 2\\sigma^{2}}{\\sigma^{2}\\tau^{2}} \\right)^{-1} = \\frac{\\sigma^{2}\\tau^{2}}{4y\\tau^{2} - 2\\sigma^{2}}\n$$\nThe total variance of this Gaussian mixture model is given by the law of total variance:\n$$\n\\mathrm{Var}[\\theta \\mid y] = \\mathbb{E}[\\mathrm{Var}[\\theta \\mid y, Z]] + \\mathrm{Var}[\\mathbb{E}[\\theta \\mid y, Z]]\n$$\nwhere $Z$ is a latent variable indicating the component of the mixture.\nThe first term is the average of the variances of the components:\n$$\n\\mathbb{E}[\\mathrm{Var}[\\theta \\mid y, Z]] = \\frac{1}{2}\\sigma_{L}^{2} + \\frac{1}{2}\\sigma_{L}^{2} = \\sigma_{L}^{2}\n$$\nThe second term is the variance of the means of the components. The mean of the mixture is $\\frac{1}{2}\\mu_{+} + \\frac{1}{2}\\mu_{-} = 0$.\n$$\n\\mathrm{Var}[\\mathbb{E}[\\theta \\mid y, Z]] = \\mathbb{E}[(\\mathbb{E}[\\theta \\mid y, Z])^{2}] - (\\mathbb{E}[\\mathbb{E}[\\theta \\mid y, Z]])^{2} = \\left( \\frac{1}{2}\\mu_{+}^{2} + \\frac{1}{2}\\mu_{-}^{2} \\right) - 0^{2} = \\mu_{+}^{2}\n$$\nCombining these terms, the total posterior variance is approximated by:\n$$\n\\mathrm{Var}[\\theta \\mid y] \\approx \\mu_{+}^{2} + \\sigma_{L}^{2}\n$$\nSubstituting the expressions for $\\mu_{+}^{2}$ and $\\sigma_{L}^{2}$:\n$$\n\\mathrm{Var}[\\theta \\mid y] \\approx \\left(y - \\frac{\\sigma^{2}}{2\\tau^{2}}\\right) + \\frac{\\sigma^{2}\\tau^{2}}{4y\\tau^{2} - 2\\sigma^{2}}\n$$\nThis expression is the leading-order approximation for the posterior variance in the bimodal regime ($y > \\frac{\\sigma^{2}}{2\\tau^{2}}$).\n\nFinally, we discuss the stability implications in the sense of Hadamard:\n1.  **Existence:** A solution, in the Bayesian sense, is the posterior distribution $p(\\theta \\mid y)$. As the prior is proper and the likelihood is well-defined, the posterior exists for any $y \\in \\mathbb{R}$.\n2.  **Uniqueness:** Uniqueness fails. In the bimodal regime, there are two equally likely solutions for $\\theta$, represented by the modes $\\mu_{\\pm}$. A single point estimate, such as a Maximum A Posteriori (MAP) estimate, is not unique. The posterior mean is $\\mathbb{E}[\\theta \\mid y] = 0$ by symmetry, but this point has minimum posterior probability and is a poor representation of the solution. This lack of a unique, representative point estimate is a manifestation of ill-posedness.\n3.  **Continuous Dependence on Data (Stability):** Stability also fails dramatically. The transition from a unimodal to a bimodal posterior occurs at the critical data value $y_{c} = \\frac{\\sigma^{2}}{2\\tau^{2}}$. As the data $y$ approaches this critical value from above ($y \\to y_{c}^{+}$), the term representing the local variance, $\\sigma_{L}^{2}$, diverges:\n$$\n\\lim_{y \\to (\\frac{\\sigma^{2}}{2\\tau^{2}})^{+}} \\sigma_{L}^{2} = \\lim_{y \\to (\\frac{\\sigma^{2}}{2\\tau^{2}})^{+}} \\frac{\\sigma^{2}\\tau^{2}}{4y\\tau^{2} - 2\\sigma^{2}} = \\frac{\\sigma^{2}\\tau^{2}}{4(\\frac{\\sigma^{2}}{2\\tau^{2}})\\tau^{2} - 2\\sigma^{2}} = \\frac{\\sigma^{2}\\tau^{2}}{2\\sigma^{2} - 2\\sigma^{2}} \\to \\infty\n$$\nThis divergence shows that the posterior variance blows up at the bifurcation point. A small perturbation of the data $y$ around $y_c$ can cause a drastic change in the posterior distribution's shape and a massive change in its variance. This extreme sensitivity to data illustrates a severe lack of stability, a hallmark of ill-posed inverse problems. The non-identifiability introduced by the forward map $G(\\theta) = \\theta^{2}$ is the root cause of this failure of uniqueness and stability.",
            "answer": "$$\n\\boxed{y - \\frac{\\sigma^{2}}{2\\tau^{2}} + \\frac{\\sigma^{2}\\tau^{2}}{4y\\tau^{2} - 2\\sigma^{2}}}\n$$"
        },
        {
            "introduction": "Moving from single- to multi-parameter systems reveals a richer structure of uncertainty, including correlations that determine our ability to disentangle the effects of different parameters. This final exercise () utilizes a tractable multi-parameter linear model to explore this structure. You will contrast the aggregated uncertainty in the forward view with the disentangled information provided by the full posterior covariance, learning how experimental design directly governs our ability to resolve contributions from individual parameters in a coupled system.",
            "id": "3382662",
            "problem": "Consider a coupled multi-physics model that maps a parameter vector to an observed quantity through a sum of two submodels. Let the overall forward map be defined as $\\mathcal{G}(m) = \\mathcal{G}_1(m_1) + \\mathcal{G}_2(m_2)$, where the parameter vector $m \\in \\mathbb{R}^3$ has components $m_s$ (shared across both submodels), $m_a$ (exclusive to the first submodel), and $m_b$ (exclusive to the second submodel). The observation model is linear and given by $y = J m + \\varepsilon$, where $y \\in \\mathbb{R}^3$ is formed by three independent observation experiments, $J \\in \\mathbb{R}^{3 \\times 3}$ is the Jacobian (design) matrix whose columns correspond to sensitivities with respect to $m_s$, $m_a$, and $m_b$, and $\\varepsilon$ is an additive noise term. Assume the following uncertainty characterization: the prior on $m$ is Gaussian with mean $0$ and diagonal covariance $\\Sigma_m = \\mathrm{diag}(v_s, v_a, v_b)$, and the observation noise is Gaussian with mean $0$ and covariance $\\Sigma_\\varepsilon = \\sigma^2 I_3$, where $I_3$ is the $3 \\times 3$ identity matrix. All quantities are dimensionless.\n\nYour task is to implement a program that, for a given set of test cases, performs the following computations:\n- Forward uncertainty quantification aggregation: compute the aggregated predictive variance of the observable $y$ under the prior, defined as the sum of variances of the components of $y$.\n- Inverse uncertainty quantification disentanglement: compute the posterior covariance of $m$ under the linear-Gaussian model and report two quantities:\n  1. The posterior variance of the shared parameter $m_s$.\n  2. A disentanglement index between the exclusive parameters $m_a$ and $m_b$, defined as the absolute posterior correlation between $m_a$ and $m_b$, which must lie in the interval $[0,1]$.\n\nAdditionally, assess structural identifiability by determining whether the model is locally identifiable from noiseless data, which in the linear setting is equivalent to the design matrix $J$ having full column rank. Report this as a boolean value.\n\nUse the following test suite, where each case specifies the design matrix $J$, the prior variances $(v_s, v_a, v_b)$, and the noise standard deviation $\\sigma$:\n- Case 1 (well-conditioned, identifiable):\n  - $J = \\begin{bmatrix}\n  1.0  0.5  0.1 \\\\\n  0.8  0.2  0.7 \\\\\n  0.3  1.0  0.4\n  \\end{bmatrix}$,\n  - $(v_s, v_a, v_b) = (1.0, 1.0, 1.0)$,\n  - $\\sigma = 0.05$.\n- Case 2 (nearly collinear exclusive parameters, weak disentanglement):\n  - $J = \\begin{bmatrix}\n  0.9  1.0  1.02 \\\\\n  0.3  0.5  0.51 \\\\\n  0.6  0.2  0.21\n  \\end{bmatrix}$,\n  - $(v_s, v_a, v_b) = (1.0, 1.0, 1.0)$,\n  - $\\sigma = 0.05$.\n- Case 3 (non-identifiable due to exact collinearity in exclusive parameters):\n  - $J = \\begin{bmatrix}\n  0.7  1.0  2.0 \\\\\n  0.4  0.0  0.0 \\\\\n  0.1  1.0  2.0\n  \\end{bmatrix}$,\n  - $(v_s, v_a, v_b) = (1.0, 1.0, 1.0)$,\n  - $\\sigma = 0.05$.\n\nThe fundamental base you must use consists of the following definitions and principles:\n- The linear observation model $y = J m + \\varepsilon$ with Gaussian prior and Gaussian noise.\n- Bayes' rule for Gaussian models, which implies that the posterior distribution is Gaussian.\n- Structural identifiability in linear models is determined by the column rank of the design matrix $J$.\n\nProgram requirements:\n- For each test case, compute and return a list containing four quantities in the following order:\n  1. Identifiability as a boolean indicating whether $J$ has full column rank.\n  2. Aggregated forward predictive variance of $y$ as a float equal to the sum of the variances of the three components of $y$.\n  3. Posterior variance of the shared parameter $m_s$ as a float.\n  4. Disentanglement index between $m_a$ and $m_b$ as a float equal to the absolute posterior correlation between $m_a$ and $m_b$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of the four quantities described above. For example, the output should look like \"[[case1_result1,case1_result2,case1_result3,case1_result4],[case2_result1,case2_result2,case2_result3,case2_result4],[case3_result1,case3_result2,case3_result3,case3_result4]]\".\n\nNo physical units are involved, and no angles are used. Express all numerical answers as decimal floats or booleans. Ensure scientific realism and internal consistency by adhering to the linear-Gaussian framework described above.",
            "solution": "The problem statement has been validated and is deemed sound. It is a well-posed problem in the field of inverse problems and data assimilation, grounded in the standard principles of linear-Gaussian models and Bayesian inference. All necessary data and definitions are provided, and the problem is free of scientific inaccuracies, contradictions, or ambiguity. We may therefore proceed with a formal solution.\n\nThe problem requires the calculation of four quantities for three distinct test cases within a linear-Gaussian framework. The model is given by $y = J m + \\varepsilon$, where $m \\in \\mathbb{R}^3$ is the parameter vector, $y \\in \\mathbb{R}^3$ is the observation vector, $J \\in \\mathbb{R}^{3 \\times 3}$ is the Jacobian or design matrix, and $\\varepsilon \\in \\mathbb{R}^3$ is the observation noise. The uncertainty is characterized by Gaussian distributions for the prior on $m$ and the noise $\\varepsilon$. Specifically, $m \\sim \\mathcal{N}(0, \\Sigma_m)$ with $\\Sigma_m = \\mathrm{diag}(v_s, v_a, v_b)$, and $\\varepsilon \\sim \\mathcal{N}(0, \\Sigma_\\varepsilon)$ with $\\Sigma_\\varepsilon = \\sigma^2 I_3$. The columns of $J$ and the components of $m$ are ordered as $(m_s, m_a, m_b)$. We will now derive the expressions for the four required quantities.\n\n1.  **Structural Identifiability**\n\nStructural identifiability assesses whether the model parameters can be uniquely determined from noise-free data. In the context of the linear model $y = Jm$, this is equivalent to asking whether the map from parameters $m$ to observations $y$ is injective. This condition holds if and only if the matrix $J$ has full column rank. Since $J$ is a square matrix of size $3 \\times 3$, having full column rank is equivalent to being invertible, having a non-zero determinant, or having a rank of $3$. We will determine identifiability by computing the rank of the matrix $J$ for each test case.\n- If $\\mathrm{rank}(J) = 3$, the model is structurally identifiable.\n- If $\\mathrm{rank}(J)  3$, the model is not structurally identifiable.\n\nThe result is reported as a boolean value.\n\n2.  **Aggregated Forward Predictive Variance**\n\nThis task is an exercise in forward uncertainty quantification, where we propagate the uncertainty from the input parameters $m$ through the model to the output observable $y$. The predictive distribution for $y$ is formed under the prior for $m$. Given the model $y = Jm + \\varepsilon$, where $m$ and $\\varepsilon$ are independent Gaussian random variables with zero mean, the observable $y$ is also a Gaussian random variable with zero mean. Its covariance, $\\Sigma_y$, is calculated as:\n$$ \\Sigma_y = \\mathrm{Cov}(Jm + \\varepsilon) $$\nDue to the independence of $m$ and $\\varepsilon$, the covariance of the sum is the sum of the covariances:\n$$ \\Sigma_y = \\mathrm{Cov}(Jm) + \\mathrm{Cov}(\\varepsilon) $$\nUsing the property of covariance for a linear transformation, $\\mathrm{Cov}(Ax) = A \\mathrm{Cov}(x) A^T$, we get:\n$$ \\Sigma_y = J \\mathrm{Cov}(m) J^T + \\mathrm{Cov}(\\varepsilon) $$\nSubstituting the given prior and noise covariances, $\\Sigma_m$ and $\\Sigma_\\varepsilon$:\n$$ \\Sigma_y = J \\Sigma_m J^T + \\Sigma_\\varepsilon $$\nThe problem asks for the *aggregated* predictive variance, which is defined as the sum of the variances of the components of $y$. This is the sum of the diagonal elements of the predictive covariance matrix $\\Sigma_y$, also known as the trace of the matrix.\n$$ \\text{Aggregated Variance} = \\mathrm{Tr}(\\Sigma_y) = \\mathrm{Tr}(J \\Sigma_m J^T + \\Sigma_\\varepsilon) $$\n\n3.  **Posterior Variance of the Shared Parameter $m_s$**\n\nThis and the following task belong to inverse uncertainty quantification. We use Bayes' rule to find the posterior distribution of the parameters $m$ given the data model. For a linear-Gaussian problem, the posterior distribution is also Gaussian. The posterior covariance matrix, $\\Sigma_{post}$, is given by the standard formula:\n$$ \\Sigma_{post} = \\left( \\Sigma_m^{-1} + J^T \\Sigma_\\varepsilon^{-1} J \\right)^{-1} $$\nThe term inside the parentheses, $H = \\Sigma_m^{-1} + J^T \\Sigma_\\varepsilon^{-1} J$, is the posterior precision matrix, or the Hessian of the negative log-posterior. Let's substitute the specific forms of our covariance matrices:\n- $\\Sigma_m^{-1} = \\mathrm{diag}(v_s^{-1}, v_a^{-1}, v_b^{-1})$\n- $\\Sigma_\\varepsilon^{-1} = (\\sigma^2 I_3)^{-1} = \\frac{1}{\\sigma^2} I_3$\nSubstituting these into the expression for $H$:\n$$ H = \\Sigma_m^{-1} + J^T \\left(\\frac{1}{\\sigma^2} I_3\\right) J = \\Sigma_m^{-1} + \\frac{1}{\\sigma^2} J^T J $$\nThe posterior covariance is then $\\Sigma_{post} = H^{-1}$. It is crucial to note that $\\Sigma_m^{-1}$ is a positive definite matrix (since $v_s, v_a, v_b > 0$) and $J^T J$ is a positive semi-definite matrix. The sum of a positive definite and a positive semi-definite matrix is always positive definite, and therefore, $H$ is always invertible. This ensures that the posterior covariance is well-defined even when $J$ is singular (the non-identifiable case).\n\nThe posterior covariance matrix $\\Sigma_{post}$ has the following structure, where the indices $0, 1, 2$ correspond to $m_s, m_a, m_b$:\n$$ \\Sigma_{post} = \\begin{bmatrix}\n\\mathrm{Var}(m_s|\\text{data})  \\mathrm{Cov}(m_s, m_a|\\text{data})  \\mathrm{Cov}(m_s, m_b|\\text{data}) \\\\\n\\mathrm{Cov}(m_a, m_s|\\text{data})  \\mathrm{Var}(m_a|\\text{data})  \\mathrm{Cov}(m_a, m_b|\\text{data}) \\\\\n\\mathrm{Cov}(m_b, m_s|\\text{data})  \\mathrm{Cov}(m_b, m_a|\\text{data})  \\mathrm{Var}(m_b|\\text{data})\n\\end{bmatrix} $$\nThe posterior variance of the shared parameter $m_s$ is the first diagonal element of this matrix: $(\\Sigma_{post})_{0,0}$.\n\n4.  **Disentanglement Index between $m_a$ and $m_b$**\n\nThe disentanglement index is defined as the absolute value of the posterior correlation between the exclusive parameters $m_a$ and $m_b$. The correlation coefficient $\\rho_{XY}$ between two variables $X$ and $Y$ is given by their covariance divided by the product of their standard deviations. Using the elements of the posterior covariance matrix $\\Sigma_{post}$:\n- Posterior variance of $m_a$: $\\mathrm{Var}(m_a|\\text{data}) = (\\Sigma_{post})_{1,1}$\n- Posterior variance of $m_b$: $\\mathrm{Var}(m_b|\\text{data}) = (\\Sigma_{post})_{2,2}$\n- Posterior covariance of $m_a$ and $m_b$: $\\mathrm{Cov}(m_a, m_b|\\text{data}) = (\\Sigma_{post})_{1,2}$\n\nThe posterior correlation is thus:\n$$ \\rho_{m_a, m_b | \\text{data}} = \\frac{(\\Sigma_{post})_{1,2}}{\\sqrt{(\\Sigma_{post})_{1,1} \\cdot (\\Sigma_{post})_{2,2}}} $$\nThe disentanglement index is the absolute value of this quantity:\n$$ \\text{Index} = \\left| \\rho_{m_a, m_b | \\text{data}} \\right| $$\nThis index provides a measure of how well the data can distinguish between the effects of parameter $m_a$ and $m_b$. An index near $0$ implies good disentanglement (the parameters are nearly independent a posteriori), while an index near $1$ implies strong posterior coupling or confusion between the parameters, often a symptom of poor identifiability.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_quantities(J, prior_variances, sigma):\n    \"\"\"\n    Computes the four required quantities for a single test case.\n\n    Args:\n        J (np.ndarray): The 3x3 Jacobian (design) matrix.\n        prior_variances (tuple): A tuple (v_s, v_a, v_b) of prior variances.\n        sigma (float): The standard deviation of the observation noise.\n\n    Returns:\n        list: A list containing the four results in order:\n              [identifiability, fwd_variance, post_var_ms, disentanglement_index].\n    \"\"\"\n    # Parameter setup\n    v_s, v_a, v_b = prior_variances\n    m = 3 # Number of parameters\n\n    # --- 1. Structural Identifiability ---\n    # Check if the Jacobian matrix J has full column rank.\n    # For a square matrix, this is equivalent to being non-singular.\n    # np.linalg.matrix_rank is the most robust way to check this numerically.\n    rank_J = np.linalg.matrix_rank(J)\n    is_identifiable = (rank_J == m)\n\n    # --- 2. Aggregated Forward Predictive Variance ---\n    # Construct prior parameter covariance and noise covariance matrices.\n    Sigma_m = np.diag([v_s, v_a, v_b])\n    Sigma_eps = (sigma**2) * np.identity(m)\n    \n    # Compute predictive covariance of the observable y: Sigma_y = J * Sigma_m * J^T + Sigma_eps\n    Sigma_y = J @ Sigma_m @ J.T + Sigma_eps\n    \n    # The aggregated variance is the trace of the predictive covariance matrix.\n    fwd_variance = np.trace(Sigma_y)\n\n    # --- 3.  4. Posterior Covariance and Derived Quantities ---\n    # Compute the posterior covariance matrix: Sigma_post = (Sigma_m^-1 + J^T * Sigma_eps^-1 * J)^-1\n    # Inverse of the prior covariance\n    Sigma_m_inv = np.diag([1/v_s, 1/v_a, 1/v_b])\n    \n    # The term J^T * Sigma_eps^-1 * J simplifies to (1/sigma^2) * J^T * J\n    # This is the Fisher Information Matrix for the parameters.\n    H_data = (1 / sigma**2) * J.T @ J\n    \n    # The posterior precision matrix (Hessian of negative log-posterior)\n    H_post = Sigma_m_inv + H_data\n    \n    # The posterior covariance matrix is the inverse of the precision matrix.\n    # This inversion is numerically stable because the prior regularizes the problem,\n    # ensuring H_post is positive definite even if J is singular.\n    Sigma_post = np.linalg.inv(H_post)\n    \n    # Extract posterior variance of the shared parameter m_s (first parameter)\n    # This corresponds to the top-left element of the posterior covariance matrix.\n    post_var_ms = Sigma_post[0, 0]\n    \n    # Compute the disentanglement index between m_a and m_b (second and third parameters)\n    # This is the absolute value of their posterior correlation.\n    cov_ab = Sigma_post[1, 2] # Cov(m_a, m_b)\n    var_a = Sigma_post[1, 1]  # Var(m_a)\n    var_b = Sigma_post[2, 2]  # Var(m_b)\n    \n    # Correlation formula: corr(a, b) = cov(a, b) / sqrt(var(a) * var(b))\n    # Handle potential division by zero, though variances should be positive.\n    if var_a > 0 and var_b > 0:\n        disentanglement_index = np.abs(cov_ab) / np.sqrt(var_a * var_b)\n    else:\n        disentanglement_index = np.nan # Should not happen in this problem\n\n    return [is_identifiable, fwd_variance, post_var_ms, disentanglement_index]\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run computations, and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"J\": np.array([\n                [1.0, 0.5, 0.1],\n                [0.8, 0.2, 0.7],\n                [0.3, 1.0, 0.4]\n            ]),\n            \"prior_variances\": (1.0, 1.0, 1.0),\n            \"sigma\": 0.05\n        },\n        {\n            \"J\": np.array([\n                [0.9, 1.0, 1.02],\n                [0.3, 0.5, 0.51],\n                [0.6, 0.2, 0.21]\n            ]),\n            \"prior_variances\": (1.0, 1.0, 1.0),\n            \"sigma\": 0.05\n        },\n        {\n            \"J\": np.array([\n                [0.7, 1.0, 2.0],\n                [0.4, 0.0, 0.0],\n                [0.1, 1.0, 2.0]\n            ]),\n            \"prior_variances\": (1.0, 1.0, 1.0),\n            \"sigma\": 0.05\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = compute_quantities(case[\"J\"], case[\"prior_variances\"], case[\"sigma\"])\n        all_results.append(result)\n\n    # Format the final output string exactly as required.\n    # The format is a list of lists, represented as a string.\n    # Ex: [[true,1.23,4.56,0.78],[false,2.34,5.67,0.89]]\n    case_strings = []\n    for res in all_results:\n        # The boolean must be lowercase 'true' or 'false'\n        bool_str = str(res[0]).lower()\n        # Create a string for the inner list, e.g., \"[true,1.23,4.56,0.78]\"\n        inner_list_str = f\"[{bool_str},{res[1]},{res[2]},{res[3]}]\"\n        case_strings.append(inner_list_str)\n\n    # Join the inner list strings with commas and enclose in brackets.\n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        }
    ]
}