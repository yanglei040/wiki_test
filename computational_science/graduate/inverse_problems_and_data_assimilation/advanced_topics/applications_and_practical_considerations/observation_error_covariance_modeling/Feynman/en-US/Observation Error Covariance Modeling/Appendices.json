{
    "hands_on_practices": [
        {
            "introduction": "The theoretical requirement that an observation error covariance matrix, $R$, be symmetric and positive definite has direct practical implications. This exercise grounds these abstract properties in concrete calculation, focusing on a simple $2 \\times 2$ case. You will practice computing the inverse of $R$, verifying its positive definiteness via eigenvalues, and assessing its numerical conditioning, which are all fundamental steps in data assimilation systems. ",
            "id": "3406348",
            "problem": "Consider a linear observation model in a variational data assimilation system for a single geophysical state variable, observed simultaneously by two collocated sensors with correlated errors. Let the observation error vector be modeled as a zero-mean Gaussian random vector with covariance matrix $R \\in \\mathbb{R}^{2 \\times 2}$. In scientifically realistic observation error covariance modeling, $R$ must be symmetric and positive definite to reflect physically meaningful variances and covariances and to ensure well-posedness when inverting $R$ in the assimilation cost function. Suppose the reported observation error covariance matrix is\n$$\nR = \\begin{pmatrix}\n4 & 1\\\\\n1 & 1\n\\end{pmatrix},\n$$\nwhere entries have units of the square of the observed quantity (for concreteness, take kelvin squared, $\\mathrm{K}^{2}$).\n\nStarting strictly from foundational definitions in probability and linear algebra—namely, the definition of a covariance matrix for a Gaussian random vector, properties of symmetric matrices, the characterization of positive definiteness through eigenvalues, and the definition of spectral conditioning—answer the following:\n\n1. Compute the inverse $R^{-1}$ exactly, ensuring the result is symmetric as required for an inverse of a symmetric positive definite matrix.\n2. Compute the eigenvalues $\\lambda_{1}$ and $\\lambda_{2}$ of $R$ exactly and use them to verify positive definiteness.\n3. Using the two-norm spectral condition number definition, compute the exact spectral condition number $\\kappa_{2}(R)$ to assess the numerical conditioning relevant to inversion in the assimilation cost function.\n\nExpress all results in exact symbolic form (no rounding). For unit reporting: entries of $R^{-1}$ should be understood to have units $\\mathrm{K}^{-2}$ and eigenvalues $\\lambda_{1}, \\lambda_{2}$ should be understood to have units $\\mathrm{K}^{2}$. Provide the final eigenvalues ordered such that $\\lambda_{1} \\geq \\lambda_{2} > 0$. The final answer must be a single expression comprising the four entries of $R^{-1}$, followed by $\\lambda_{1}$, $\\lambda_{2}$, and $\\kappa_{2}(R)$, arranged as one row using the $\\text{pmatrix}$ environment, in exact form.",
            "solution": "The problem as stated is subjected to validation.\n\n### Step 1: Extract Givens\n- **Model Context**: Linear observation model in variational data assimilation.\n- **State Variable**: A single geophysical state variable.\n- **Observation Setup**: Two collocated sensors with correlated errors.\n- **Error Model**: The observation error vector is a zero-mean Gaussian random vector.\n- **Covariance Matrix**: The observation error covariance matrix is denoted by $R \\in \\mathbb{R}^{2 \\times 2}$.\n- **Properties of $R$**: $R$ must be symmetric and positive definite.\n- **Given Matrix**: The specific covariance matrix is given as $R = \\begin{pmatrix} 4 & 1\\\\ 1 & 1 \\end{pmatrix}$.\n- **Units**: The entries of $R$ are in units of kelvin squared, $\\mathrm{K}^{2}$. Consequently, entries of $R^{-1}$ have units $\\mathrm{K}^{-2}$, and eigenvalues of $R$ have units $\\mathrm{K}^{2}$.\n- **Tasks**:\n    1. Compute the inverse matrix $R^{-1}$.\n    2. Compute the eigenvalues $\\lambda_{1}, \\lambda_{2}$ of $R$ and verify positive definiteness. Eigenvalues must be ordered such that $\\lambda_{1} \\geq \\lambda_{2}$.\n    3. Compute the exact spectral condition number $\\kappa_{2}(R)$ using the two-norm.\n- **Output Format**: A single row matrix containing the four entries of $R^{-1}$, followed by $\\lambda_{1}$, $\\lambda_{2}$, and $\\kappa_{2}(R)$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is well-grounded in the established theory of data assimilation and inverse problems. The requirement for a covariance matrix to be symmetric and positive definite is fundamental. Symmetry ($R_{ij} = \\text{cov}(e_i, e_j) = \\text{cov}(e_j, e_i) = R_{ji}$) follows from the definition of covariance. Positive definiteness ensures that the variance of any linear combination of the observation errors is positive, which is a physical necessity, and it guarantees that the quadratic form in the assimilation cost function, $(\\mathbf{y} - H\\mathbf{x})^T R^{-1} (\\mathbf{y} - H\\mathbf{x})$, has a unique minimum. The given matrix is symmetric, and its diagonal elements (variances) are positive, which is a necessary, though not sufficient, condition for positive definiteness. The problem is scientifically and mathematically sound.\n- **Well-Posedness**: The tasks involve standard, well-defined computations in linear algebra (matrix inversion, eigenvalue calculation, condition number calculation) for a given non-singular matrix. A unique solution exists.\n- **Objectivity**: The problem is stated in precise, objective mathematical terms.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. It is self-contained, scientifically sound, and well-posed. The solution process may proceed.\n\n---\n\nThe solution is performed in three parts as requested.\n\n### Part 1: Computation of the Inverse Matrix $R^{-1}$\nLet the given matrix be $R = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix} = \\begin{pmatrix} 4 & 1 \\\\ 1 & 1 \\end{pmatrix}$. The matrix is symmetric since $b=c$.\nThe inverse of a $2 \\times 2$ matrix is given by the formula:\n$$\nR^{-1} = \\frac{1}{ad - bc} \\begin{pmatrix} d & -b \\\\ -c & a \\end{pmatrix}\n$$\nFirst, we compute the determinant of $R$, denoted $\\det(R)$.\n$$\n\\det(R) = (4)(1) - (1)(1) = 4 - 1 = 3\n$$\nSince $\\det(R) \\neq 0$, the inverse exists. Substituting the values into the formula:\n$$\nR^{-1} = \\frac{1}{3} \\begin{pmatrix} 1 & -1 \\\\ -1 & 4 \\end{pmatrix}\n$$\nDistributing the scalar factor $\\frac{1}{3}$ into the matrix gives the final form of the inverse:\n$$\nR^{-1} = \\begin{pmatrix} \\frac{1}{3} & -\\frac{1}{3} \\\\ -\\frac{1}{3} & \\frac{4}{3} \\end{pmatrix}\n$$\nAs required, the inverse $R^{-1}$ is also a symmetric matrix.\n\n### Part 2: Computation of Eigenvalues and Verification of Positive Definiteness\nThe eigenvalues, $\\lambda$, of the matrix $R$ are the solutions to the characteristic equation $\\det(R - \\lambda I) = 0$, where $I$ is the $2 \\times 2$ identity matrix.\n$$\nR - \\lambda I = \\begin{pmatrix} 4 & 1 \\\\ 1 & 1 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 4-\\lambda & 1 \\\\ 1 & 1-\\lambda \\end{pmatrix}\n$$\nThe determinant is:\n$$\n\\det(R - \\lambda I) = (4-\\lambda)(1-\\lambda) - (1)(1) = 0\n$$\nExpanding this equation gives a quadratic equation for $\\lambda$:\n$$\n4 - 4\\lambda - \\lambda + \\lambda^2 - 1 = 0\n$$\n$$\n\\lambda^2 - 5\\lambda + 3 = 0\n$$\nWe solve for $\\lambda$ using the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$, with $a=1$, $b=-5$, and $c=3$.\n$$\n\\lambda = \\frac{-(-5) \\pm \\sqrt{(-5)^2 - 4(1)(3)}}{2(1)} = \\frac{5 \\pm \\sqrt{25 - 12}}{2} = \\frac{5 \\pm \\sqrt{13}}{2}\n$$\nThis gives the two eigenvalues. Following the requirement to order them such that $\\lambda_{1} \\geq \\lambda_{2}$:\n$$\n\\lambda_{1} = \\frac{5 + \\sqrt{13}}{2}\n$$\n$$\n\\lambda_{2} = \\frac{5 - \\sqrt{13}}{2}\n$$\nTo verify that $R$ is positive definite, we must show that all its eigenvalues are strictly positive.\nFor $\\lambda_{1}$, both $5$ and $\\sqrt{13}$ are positive, so $\\lambda_{1} > 0$.\nFor $\\lambda_{2}$, we need to check if $5 - \\sqrt{13} > 0$. This is equivalent to checking if $5 > \\sqrt{13}$, which is equivalent to $5^2 > 13$, or $25 > 13$. This is true.\nSince both $\\lambda_{1}$ and $\\lambda_{2}$ are strictly positive, the symmetric matrix $R$ is positive definite. This confirms the premise of the problem statement.\n\n### Part 3: Computation of the Spectral Condition Number $\\kappa_{2}(R)$\nThe spectral condition number (using the matrix $2$-norm) of a matrix $R$ is defined as $\\kappa_{2}(R) = \\|R\\|_{2} \\|R^{-1}\\|_{2}$.\nFor a symmetric positive definite matrix, the $2$-norm is equal to its largest eigenvalue ($\\lambda_{\\max}$). Therefore, $\\|R\\|_{2} = \\lambda_{\\max}(R)$.\nThe eigenvalues of the inverse matrix $R^{-1}$ are the reciprocals of the eigenvalues of $R$. Therefore, the largest eigenvalue of $R^{-1}$ is the reciprocal of the smallest eigenvalue of $R$, i.e., $\\lambda_{\\max}(R^{-1}) = 1/\\lambda_{\\min}(R)$.\nThus, for a symmetric positive definite matrix, the condition number simplifies to the ratio of the largest to the smallest eigenvalue:\n$$\n\\kappa_{2}(R) = \\frac{\\lambda_{\\max}(R)}{\\lambda_{\\min}(R)} = \\frac{\\lambda_{1}}{\\lambda_{2}}\n$$\nSubstituting the computed eigenvalues:\n$$\n\\kappa_{2}(R) = \\frac{\\frac{5 + \\sqrt{13}}{2}}{\\frac{5 - \\sqrt{13}}{2}} = \\frac{5 + \\sqrt{13}}{5 - \\sqrt{13}}\n$$\nTo express this in a simplified form, we rationalize the denominator by multiplying the numerator and denominator by the conjugate of the denominator, which is $5 + \\sqrt{13}$:\n$$\n\\kappa_{2}(R) = \\frac{5 + \\sqrt{13}}{5 - \\sqrt{13}} \\times \\frac{5 + \\sqrt{13}}{5 + \\sqrt{13}} = \\frac{(5 + \\sqrt{13})^2}{5^2 - (\\sqrt{13})^2}\n$$\nThe numerator expands to:\n$$\n(5 + \\sqrt{13})^2 = 5^2 + 2(5)(\\sqrt{13}) + (\\sqrt{13})^2 = 25 + 10\\sqrt{13} + 13 = 38 + 10\\sqrt{13}\n$$\nThe denominator is:\n$$\n5^2 - (\\sqrt{13})^2 = 25 - 13 = 12\n$$\nCombining these results:\n$$\n\\kappa_{2}(R) = \\frac{38 + 10\\sqrt{13}}{12}\n$$\nSimplifying the fraction by dividing the numerator and denominator by their greatest common divisor, which is $2$:\n$$\n\\kappa_{2}(R) = \\frac{19 + 5\\sqrt{13}}{6}\n$$\nThis is the exact spectral condition number. A high condition number suggests that the matrix is ill-conditioned, meaning its inverse is sensitive to small perturbations in the matrix entries, which can pose numerical challenges in the cost function minimization.\n\nThe final results are: the entries of $R^{-1}$, the eigenvalues $\\lambda_1, \\lambda_2$, and the condition number $\\kappa_2(R)$. These are collected into the required final format.\nThe four entries of $R^{-1}$ are $(R^{-1})_{11} = \\frac{1}{3}$, $(R^{-1})_{12} = -\\frac{1}{3}$, $(R^{-1})_{21} = -\\frac{1}{3}$, and $(R^{-1})_{22} = \\frac{4}{3}$.\n$\\lambda_{1} = \\frac{5+\\sqrt{13}}{2}$.\n$\\lambda_{2} = \\frac{5-\\sqrt{13}}{2}$.\n$\\kappa_{2}(R) = \\frac{19+5\\sqrt{13}}{6}$.",
            "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{3} & -\\frac{1}{3} & -\\frac{1}{3} & \\frac{4}{3} & \\frac{5 + \\sqrt{13}}{2} & \\frac{5 - \\sqrt{13}}{2} & \\frac{19 + 5\\sqrt{13}}{6} \\end{pmatrix}}$$"
        },
        {
            "introduction": "Modeling observation error correlations is not merely an academic exercise; it directly impacts the quality of the state estimate. This practice provides a clear demonstration of this principle by comparing an optimal estimator that uses the full covariance matrix with a simpler one that ignores correlations. By deriving and comparing the error variances of both estimators, you will quantify the tangible benefit of accurately characterizing the structure of $R$. ",
            "id": "3406402",
            "problem": "Consider a linear observation model in a data assimilation setting,\n$$\ny = H x_{\\text{true}} + \\varepsilon,\n$$\nwhere $x_{\\text{true}} \\in \\mathbb{R}^{2}$ is an unknown state vector, $y \\in \\mathbb{R}^{3}$ is the observed data, and $\\varepsilon \\sim \\mathcal{N}(0, R)$ represents observation errors with a known, non-diagonal covariance matrix $R$. The observation operator $H \\in \\mathbb{R}^{3 \\times 2}$ and the observation error covariance $R \\in \\mathbb{R}^{3 \\times 3}$ are given by\n$$\nH = \\begin{pmatrix}\n1 & 0 \\\\\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix}, \\qquad\nR = \\begin{pmatrix}\n2 & 1 & 0 \\\\\n1 & 2 & 1 \\\\\n0 & 1 & 2\n\\end{pmatrix}.\n$$\nA single realization of the observation is\n$$\ny = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nStarting from the Gaussian likelihood for $\\varepsilon$ and assuming a flat prior on $x_{\\text{true}}$, perform the following:\n\n1) Derive, from first principles, the estimator $\\displaystyle x^{\\ast}$ that minimizes the quadratic form associated with the negative log-likelihood of the observation errors, and compute its value for the provided $H$, $R$, and $y$.\n\n2) Using linear estimator error propagation under the true observation error covariance $R$, derive the error covariance of the estimator in part 1). Then, consider the Ordinary Least Squares (OLS) estimator that ignores the off-diagonal structure of $R$ and uses identity weighting, and derive its error covariance under the true $R$. Use these results to compute the generalized variance ratio\n$$\n\\rho \\equiv \\frac{\\det\\!\\big(\\operatorname{Var}(x_{\\text{OLS}})\\big)}{\\det\\!\\big(\\operatorname{Var}(x^{\\ast})\\big)}.\n$$\n\nExpress the final answer for $\\rho$ as an exact fraction. No rounding is required. The final answer must be a single number.",
            "solution": "The observation model is $y = H x_{\\text{true}} + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, R)$. A flat prior on $x_{\\text{true}}$ implies that the maximum a posteriori estimate equals the maximum likelihood estimate. Under Gaussian errors, the negative log-likelihood (up to an additive constant independent of $x$) is given by the quadratic form\n$$\nJ(x) = (y - Hx)^{\\top} R^{-1} (y - Hx).\n$$\nThe estimator that minimizes $J(x)$ is the Generalized Least Squares (GLS) estimator. We derive it by setting the gradient to zero:\n$$\n\\nabla_x J(x) = -2 H^{\\top} R^{-1} (y - Hx) = 0\n\\quad \\Longrightarrow \\quad\nH^{\\top} R^{-1} H \\, x = H^{\\top} R^{-1} y.\n$$\nAssuming $H^{\\top} R^{-1} H$ is invertible, the minimizing solution is\n$$\nx^{\\ast} = (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} y.\n$$\n\nWe now compute this for the provided $H$, $R$, and $y$.\n\nFirst, compute $R^{-1}$. For\n$$\nR = \\begin{pmatrix}\n2 & 1 & 0 \\\\\n1 & 2 & 1 \\\\\n0 & 1 & 2\n\\end{pmatrix},\n$$\none verifies that\n$$\nR^{-1} = \\frac{1}{4} \\begin{pmatrix}\n3 & -2 & 1 \\\\\n-2 & 4 & -2 \\\\\n1 & -2 & 3\n\\end{pmatrix}.\n$$\nNext, compute $H^{\\top} R^{-1} H$. It is convenient to compute $R^{-1} H$ first. Using\n$$\nH = \\begin{pmatrix}\n1 & 0 \\\\\n1 & 1 \\\\\n0 & 1\n\\end{pmatrix},\n\\quad\nR^{-1} = \\frac{1}{4} \\begin{pmatrix}\n3 & -2 & 1 \\\\\n-2 & 4 & -2 \\\\\n1 & -2 & 3\n\\end{pmatrix},\n$$\nwe obtain\n$$\nR^{-1} H = \\frac{1}{4}\n\\begin{pmatrix}\n1 & -1 \\\\\n2 & 2 \\\\\n-1 & 1\n\\end{pmatrix}.\n$$\nTherefore,\n$$\nH^{\\top} R^{-1} H\n= H^{\\top} (R^{-1} H)\n= \\frac{1}{4}\\begin{pmatrix}\n3 & 1 \\\\\n1 & 3\n\\end{pmatrix}.\n$$\nThe inverse is\n$$\n\\big(H^{\\top} R^{-1} H\\big)^{-1}\n= \\begin{pmatrix}\n\\frac{3}{2} & -\\frac{1}{2} \\\\\n-\\frac{1}{2} & \\frac{3}{2}\n\\end{pmatrix},\n$$\nsince the determinant is $\\frac{1}{2}$ and the adjugate is\n$\\frac{1}{4}\\begin{pmatrix} 3 & -1 \\\\ -1 & 3 \\end{pmatrix}$.\n\nWe also need $H^{\\top} R^{-1} y$. With $y = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$,\n$$\nR^{-1} y\n= \\frac{1}{4}\n\\begin{pmatrix}\n3 \\\\\n-2 \\\\\n1\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{3}{4} \\\\\n-\\frac{1}{2} \\\\\n\\frac{1}{4}\n\\end{pmatrix},\n\\quad\nH^{\\top} R^{-1} y\n= \\begin{pmatrix}\n1 & 1 & 0 \\\\\n0 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{3}{4} \\\\\n-\\frac{1}{2} \\\\\n\\frac{1}{4}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{1}{4} \\\\\n-\\frac{1}{4}\n\\end{pmatrix}.\n$$\nHence,\n$$\nx^{\\ast} = \\begin{pmatrix}\n\\frac{3}{2} & -\\frac{1}{2} \\\\\n-\\frac{1}{2} & \\frac{3}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{1}{4} \\\\\n-\\frac{1}{4}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{1}{2} \\\\\n-\\frac{1}{2}\n\\end{pmatrix}.\n$$\n\nWe now derive the error covariances under the true observation error covariance $R$. For a linear estimator $x = K y$, the estimation error is $x - x_{\\text{true}} = K(H x_{\\text{true}} + \\varepsilon) - x_{\\text{true}} = (K H - I)x_{\\text{true}} + K \\varepsilon$. For unbiasedness with respect to $x_{\\text{true}}$, we require $K H = I$, which is satisfied by both the Generalized Least Squares (GLS) and Ordinary Least Squares (OLS) choices below. Under $\\varepsilon \\sim \\mathcal{N}(0, R)$, the error covariance is\n$$\n\\operatorname{Var}(x) = K R K^{\\top}.\n$$\n\nFor GLS, the estimator is\n$$\nx^{\\ast} = (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} y \\equiv K_{\\text{GLS}} y,\n$$\nwith $K_{\\text{GLS}} = (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1}$. Then\n$$\n\\operatorname{Var}(x^{\\ast})\n= K_{\\text{GLS}} R K_{\\text{GLS}}^{\\top}\n= (H^{\\top} R^{-1} H)^{-1} H^{\\top} R^{-1} R R^{-1} H (H^{\\top} R^{-1} H)^{-1}\n= (H^{\\top} R^{-1} H)^{-1}.\n$$\nWith the computed matrix, this is\n$$\n\\operatorname{Var}(x^{\\ast})\n= \\begin{pmatrix}\n\\frac{3}{2} & -\\frac{1}{2} \\\\\n-\\frac{1}{2} & \\frac{3}{2}\n\\end{pmatrix}.\n$$\n\nFor OLS, which ignores the off-diagonal structure of $R$ and uses identity weighting, the estimator is\n$$\nx_{\\text{OLS}} = (H^{\\top} H)^{-1} H^{\\top} y \\equiv K_{\\text{OLS}} y,\n\\quad\nK_{\\text{OLS}} = (H^{\\top} H)^{-1} H^{\\top}.\n$$\nThen\n$$\n\\operatorname{Var}(x_{\\text{OLS}})\n= K_{\\text{OLS}} R K_{\\text{OLS}}^{\\top}\n= (H^{\\top} H)^{-1} H^{\\top} R H (H^{\\top} H)^{-1}.\n$$\nWe compute the needed matrices. First,\n$$\nH^{\\top} H\n= \\begin{pmatrix}\n2 & 1 \\\\\n1 & 2\n\\end{pmatrix},\n\\quad\n(H^{\\top} H)^{-1}\n= \\frac{1}{3}\n\\begin{pmatrix}\n2 & -1 \\\\\n-1 & 2\n\\end{pmatrix}.\n$$\nNext,\n$$\nR H\n= \\begin{pmatrix}\n3 & 1 \\\\\n3 & 3 \\\\\n1 & 3\n\\end{pmatrix},\n\\quad\nH^{\\top} R H\n= \\begin{pmatrix}\n6 & 4 \\\\\n4 & 6\n\\end{pmatrix}.\n$$\nTherefore,\n$$\n\\operatorname{Var}(x_{\\text{OLS}})\n= \\frac{1}{3}\n\\begin{pmatrix}\n2 & -1 \\\\\n-1 & 2\n\\end{pmatrix}\n\\begin{pmatrix}\n6 & 4 \\\\\n4 & 6\n\\end{pmatrix}\n\\frac{1}{3}\n\\begin{pmatrix}\n2 & -1 \\\\\n-1 & 2\n\\end{pmatrix}\n= \\frac{1}{9}\n\\begin{pmatrix}\n14 & -4 \\\\\n-4 & 14\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{14}{9} & -\\frac{4}{9} \\\\\n-\\frac{4}{9} & \\frac{14}{9}\n\\end{pmatrix}.\n$$\n\nWe now compute the generalized variance ratio\n$$\n\\rho \\equiv \\frac{\\det\\!\\big(\\operatorname{Var}(x_{\\text{OLS}})\\big)}{\\det\\!\\big(\\operatorname{Var}(x^{\\ast})\\big)}.\n$$\nFor a $2 \\times 2$ symmetric matrix\n$\\begin{pmatrix} a & c \\\\ c & a \\end{pmatrix}$, the determinant is $a^{2} - c^{2}$.\n\nFor $\\operatorname{Var}(x^{\\ast})$ we have $a = \\frac{3}{2}$ and $c = -\\frac{1}{2}$, so\n$$\n\\det\\!\\big(\\operatorname{Var}(x^{\\ast})\\big)\n= \\left(\\frac{3}{2}\\right)^{2} - \\left(-\\frac{1}{2}\\right)^{2}\n= \\frac{9}{4} - \\frac{1}{4}\n= 2.\n$$\nFor $\\operatorname{Var}(x_{\\text{OLS}})$ we have $a = \\frac{14}{9}$ and $c = -\\frac{4}{9}$, so\n$$\n\\det\\!\\big(\\operatorname{Var}(x_{\\text{OLS}})\\big)\n= \\left(\\frac{14}{9}\\right)^{2} - \\left(-\\frac{4}{9}\\right)^{2}\n= \\frac{196}{81} - \\frac{16}{81}\n= \\frac{180}{81}\n= \\frac{20}{9}.\n$$\nTherefore,\n$$\n\\rho = \\frac{\\frac{20}{9}}{2} = \\frac{10}{9}.\n$$\nThis ratio exceeds $1$, indicating that properly accounting for correlated observation errors via the non-diagonal $R$ reduces the generalized variance of the estimator relative to Ordinary Least Squares (OLS).",
            "answer": "$$\\boxed{\\frac{10}{9}}$$"
        },
        {
            "introduction": "In many practical applications, assuming observation errors are uncorrelated (i.e., using a diagonal $R$ matrix) is a common simplification. To be a discerning practitioner, it is crucial to understand the error introduced by such an approximation. This exercise guides you through an analytical derivation to quantify the error in the resulting analysis variance, revealing how the impact of this simplification depends on the interplay between the true error correlation, the number of observations, and the prior uncertainty. ",
            "id": "3406328",
            "problem": "Consider a linear Gaussian inverse problem in which a single scalar state $x$ is estimated from $m \\geq 2$ colocated observations. The prior for $x$ is Gaussian with mean $\\mu_{b}$ and variance $p_{b} > 0$. The observation model is $\\mathbf{y} = x \\mathbf{1}_{m} + \\boldsymbol{\\epsilon}$, where $\\mathbf{1}_{m}$ is the $m$-vector of ones and $\\boldsymbol{\\epsilon}$ is zero-mean Gaussian observational error with covariance matrix $R \\in \\mathbb{R}^{m \\times m}$. Assume $R$ has constant variance $\\sigma^{2} > 0$ and constant correlation $\\rho$ off-diagonals, that is,\n$$\nR = \\sigma^{2}\\left[(1-\\rho) I_{m} + \\rho\\, \\mathbf{1}_{m}\\mathbf{1}_{m}^{\\top}\\right],\n$$\nwith $- \\frac{1}{m-1} < \\rho < 1$ to ensure positive definiteness, and $I_{m}$ the $m \\times m$ identity matrix.\n\nStarting from the definitions of a Gaussian prior and a Gaussian likelihood, derive the analysis (posterior) variance $\\operatorname{Var}(x \\mid \\mathbf{y})$ in two cases:\n(i) using the exact covariance $R$ given above, and\n(ii) using the diagonal approximation $R_{d} = \\sigma^{2} I_{m}$ that neglects cross-correlations.\n\nDefine the relative error of the diagonal approximation in the analysis variance as\n$$\n\\mathrm{RE}(\\rho) \\equiv \\frac{P_{a}^{\\mathrm{diag}} - P_{a}^{\\mathrm{true}}}{P_{a}^{\\mathrm{true}}},\n$$\nwhere $P_{a}^{\\mathrm{true}}$ is the analysis variance computed with the exact $R$, and $P_{a}^{\\mathrm{diag}}$ is the analysis variance computed with $R_{d}$. Compute $\\mathrm{RE}(\\rho)$ as a closed-form analytic expression in terms of $\\rho$, $m$, $\\sigma^{2}$, and $p_{b}$. Your final answer should be a single analytic expression; no numerical approximation is required and no units are needed.",
            "solution": "### Step 1: Extract Givens\n- A single scalar state variable: $x$.\n- Number of observations: $m \\geq 2$.\n- Prior for $x$: Gaussian with mean $\\mu_{b}$ and variance $p_{b} > 0$, denoted $p(x) = \\mathcal{N}(x | \\mu_{b}, p_{b})$.\n- Observation model: $\\mathbf{y} = x \\mathbf{1}_{m} + \\boldsymbol{\\epsilon}$, where $\\mathbf{1}_{m}$ is the $m$-vector of ones.\n- Observation error $\\boldsymbol{\\epsilon}$: Zero-mean Gaussian with covariance matrix $R \\in \\mathbb{R}^{m \\times m}$.\n- Structure of the exact covariance matrix $R$:\n$$\nR = \\sigma^{2}\\left[(1-\\rho) I_{m} + \\rho\\, \\mathbf{1}_{m}\\mathbf{1}_{m}^{\\top}\\right]\n$$\nwhere $\\sigma^2 > 0$ is the variance, $\\rho$ is the correlation, $I_m$ is the $m \\times m$ identity matrix.\n- Constraint on $\\rho$: $- \\frac{1}{m-1} < \\rho < 1$ to ensure $R$ is positive definite.\n- Diagonal approximation of the covariance matrix: $R_{d} = \\sigma^{2} I_{m}$.\n- Definition of analysis (posterior) variance using the exact $R$: $P_{a}^{\\mathrm{true}} = \\operatorname{Var}(x \\mid \\mathbf{y})$.\n- Definition of analysis (posterior) variance using the approximate $R_d$: $P_{a}^{\\mathrm{diag}}$.\n- Definition of relative error:\n$$\n\\mathrm{RE}(\\rho) \\equiv \\frac{P_{a}^{\\mathrm{diag}} - P_{a}^{\\mathrm{true}}}{P_{a}^{\\mathrm{true}}}\n$$\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to validation against the specified criteria.\n\n- **Scientifically Grounded**: The problem is set within the standard framework of Bayesian inference for linear Gaussian models, a fundamental topic in statistics, data assimilation, and inverse problems. All concepts (Gaussian prior, likelihood, posterior, covariance matrix) are standard and well-defined. The structure of the covariance matrix $R$ corresponds to the well-known equicorrelation or compound symmetry model. The condition $- \\frac{1}{m-1} < \\rho < 1$ is the correct mathematical condition for the positive definiteness of this matrix. The problem is scientifically sound.\n- **Well-Posed**: The problem asks for the derivation of specific quantities ($P_{a}^{\\mathrm{true}}$, $P_{a}^{\\mathrm{diag}}$, and $\\mathrm{RE}(\\rho)$) based on a complete and consistent set of definitions and equations. A unique, stable, and meaningful solution exists and can be derived from the provided information.\n- **Objective**: The problem is stated using precise mathematical language and definitions. It is free from ambiguity, subjectivity, or opinion-based claims.\n\n### Step 3: Verdict and Action\nThe problem is **valid** as it is scientifically grounded, well-posed, and objective. Proceeding to solution.\n\n### Derivation\nThis is a linear Gaussian inverse problem. The prior distribution for the state $x$ is given by $p(x) = \\mathcal{N}(x | \\mu_b, p_b)$. The likelihood function, derived from the observation model $\\mathbf{y} = x \\mathbf{1}_{m} + \\boldsymbol{\\epsilon}$ where $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, R)$, is $p(\\mathbf{y}|x) = \\mathcal{N}(\\mathbf{y} | x\\mathbf{1}_m, R)$. In the standard formulation $\\mathbf{y} = Hx + \\boldsymbol{\\epsilon}$, the observation operator $H$ which maps the scalar state $x$ to the observation space is the $m$-vector of ones, $H = \\mathbf{1}_{m} \\in \\mathbb{R}^{m \\times 1}$.\n\nThe posterior distribution $p(x|\\mathbf{y})$ is also Gaussian, with a mean (analysis state) $x_a$ and a variance (analysis variance) $P_a$. The inverse of the analysis variance is given by the standard formula:\n$$\nP_a^{-1} = p_b^{-1} + H^{\\top}R^{-1}H\n$$\nSubstituting $p_b$ for the prior variance and $H = \\mathbf{1}_{m}$, this becomes:\n$$\nP_a^{-1} = \\frac{1}{p_b} + \\mathbf{1}_{m}^{\\top}R^{-1}\\mathbf{1}_{m}\n$$\nThe core of the problem is to compute the term $\\mathbf{1}_{m}^{\\top}R^{-1}\\mathbf{1}_{m}$ for the two different choices of $R$.\n\n**(i) Analysis variance with the exact covariance $R$ ($P_{a}^{\\mathrm{true}}$)**\n\nThe exact covariance matrix is $R = \\sigma^{2}\\left[(1-\\rho) I_{m} + \\rho\\, \\mathbf{1}_{m}\\mathbf{1}_{m}^{\\top}\\right]$. To compute $\\mathbf{1}_{m}^{\\top}R^{-1}\\mathbf{1}_{m}$, we can exploit the structure of $R$. The vector $\\mathbf{1}_{m}$ is an eigenvector of $R$. Let's verify this and find the corresponding eigenvalue $\\lambda$:\n$$\nR\\mathbf{1}_{m} = \\sigma^{2}\\left[(1-\\rho) I_{m} + \\rho\\, \\mathbf{1}_{m}\\mathbf{1}_{m}^{\\top}\\right]\\mathbf{1}_{m}\n$$\n$$\nR\\mathbf{1}_{m} = \\sigma^{2}\\left((1-\\rho) I_{m}\\mathbf{1}_{m} + \\rho\\, \\mathbf{1}_{m}(\\mathbf{1}_{m}^{\\top}\\mathbf{1}_{m})\\right)\n$$\nSince $I_{m}\\mathbf{1}_{m} = \\mathbf{1}_{m}$ and $\\mathbf{1}_{m}^{\\top}\\mathbf{1}_{m} = \\sum_{i=1}^{m} 1 = m$, this simplifies to:\n$$\nR\\mathbf{1}_{m} = \\sigma^{2}\\left((1-\\rho)\\mathbf{1}_{m} + \\rho m \\mathbf{1}_{m}\\right) = \\sigma^{2}(1 - \\rho + m\\rho)\\mathbf{1}_{m} = \\sigma^{2}(1 + (m-1)\\rho)\\mathbf{1}_{m}\n$$\nSo, $\\mathbf{1}_{m}$ is an eigenvector of $R$ with eigenvalue $\\lambda = \\sigma^{2}(1 + (m-1)\\rho)$.\nFor an invertible matrix $A$ with eigenvector $v$ and eigenvalue $\\lambda$, its inverse $A^{-1}$ has the same eigenvector $v$ with eigenvalue $1/\\lambda$. Thus, $R^{-1}\\mathbf{1}_{m} = \\frac{1}{\\lambda}\\mathbf{1}_{m}$.\nWe can now compute the required term:\n$$\n\\mathbf{1}_{m}^{\\top}R^{-1}\\mathbf{1}_{m} = \\mathbf{1}_{m}^{\\top}\\left( \\frac{1}{\\sigma^{2}(1 + (m-1)\\rho)} \\mathbf{1}_{m} \\right) = \\frac{1}{\\sigma^{2}(1 + (m-1)\\rho)} (\\mathbf{1}_{m}^{\\top}\\mathbf{1}_{m}) = \\frac{m}{\\sigma^{2}(1 + (m-1)\\rho)}\n$$\nThe inverse analysis variance for the true case is:\n$$\n(P_a^{\\mathrm{true}})^{-1} = \\frac{1}{p_b} + \\frac{m}{\\sigma^{2}(1 + (m-1)\\rho)}\n$$\nSo, the true analysis variance is:\n$$\nP_a^{\\mathrm{true}} = \\left( \\frac{1}{p_b} + \\frac{m}{\\sigma^{2}(1 + (m-1)\\rho)} \\right)^{-1}\n$$\n\n**(ii) Analysis variance with the diagonal approximation $R_d$ ($P_{a}^{\\mathrm{diag}}$)**\n\nThe approximate diagonal covariance matrix is $R_d = \\sigma^2 I_m$. Its inverse is simply $R_d^{-1} = \\frac{1}{\\sigma^2}I_m$.\nWe compute the term $\\mathbf{1}_{m}^{\\top}R_d^{-1}\\mathbf{1}_{m}$:\n$$\n\\mathbf{1}_{m}^{\\top}R_d^{-1}\\mathbf{1}_{m} = \\mathbf{1}_{m}^{\\top}\\left(\\frac{1}{\\sigma^2}I_m\\right)\\mathbf{1}_{m} = \\frac{1}{\\sigma^2}(\\mathbf{1}_{m}^{\\top}\\mathbf{1}_{m}) = \\frac{m}{\\sigma^2}\n$$\nThe inverse analysis variance for the diagonal approximation case is:\n$$\n(P_a^{\\mathrm{diag}})^{-1} = \\frac{1}{p_b} + \\frac{m}{\\sigma^2}\n$$\nSo, the analysis variance under the diagonal approximation is:\n$$\nP_a^{\\mathrm{diag}} = \\left( \\frac{1}{p_b} + \\frac{m}{\\sigma^2} \\right)^{-1}\n$$\n\n**Computation of the Relative Error $\\mathrm{RE}(\\rho)$**\n\nThe relative error is defined as $\\mathrm{RE}(\\rho) = \\frac{P_{a}^{\\mathrm{diag}} - P_{a}^{\\mathrm{true}}}{P_{a}^{\\mathrm{true}}} = \\frac{P_{a}^{\\mathrm{diag}}}{P_{a}^{\\mathrm{true}}} - 1$.\nThe ratio of the variances is the inverse ratio of their inverses:\n$$\n\\frac{P_{a}^{\\mathrm{diag}}}{P_{a}^{\\mathrm{true}}} = \\frac{(P_a^{\\mathrm{true}})^{-1}}{(P_a^{\\mathrm{diag}})^{-1}} = \\frac{\\frac{1}{p_b} + \\frac{m}{\\sigma^{2}(1 + (m-1)\\rho)}}{\\frac{1}{p_b} + \\frac{m}{\\sigma^2}}\n$$\nSubstituting this into the expression for $\\mathrm{RE}(\\rho)$:\n$$\n\\mathrm{RE}(\\rho) = \\frac{\\frac{1}{p_b} + \\frac{m}{\\sigma^{2}(1 + (m-1)\\rho)}}{\\frac{1}{p_b} + \\frac{m}{\\sigma^2}} - 1\n$$\n$$\n\\mathrm{RE}(\\rho) = \\frac{\\left(\\frac{1}{p_b} + \\frac{m}{\\sigma^{2}(1 + (m-1)\\rho)}\\right) - \\left(\\frac{1}{p_b} + \\frac{m}{\\sigma^2}\\right)}{\\frac{1}{p_b} + \\frac{m}{\\sigma^2}}\n$$\n$$\n\\mathrm{RE}(\\rho) = \\frac{\\frac{m}{\\sigma^2(1 + (m-1)\\rho)} - \\frac{m}{\\sigma^2}}{\\frac{1}{p_b} + \\frac{m}{\\sigma^2}} = \\frac{\\frac{m}{\\sigma^2}\\left(\\frac{1}{1 + (m-1)\\rho} - 1\\right)}{\\frac{1}{p_b} + \\frac{m}{\\sigma^2}}\n$$\nSimplifying the term in the numerator's parenthesis:\n$$\n\\frac{1}{1 + (m-1)\\rho} - 1 = \\frac{1 - (1 + (m-1)\\rho)}{1 + (m-1)\\rho} = \\frac{-(m-1)\\rho}{1 + (m-1)\\rho}\n$$\nSubstituting this back:\n$$\n\\mathrm{RE}(\\rho) = \\frac{\\frac{m}{\\sigma^2}\\left(\\frac{-(m-1)\\rho}{1 + (m-1)\\rho}\\right)}{\\frac{\\sigma^2 + m p_b}{p_b \\sigma^2}}\n$$\n$$\n\\mathrm{RE}(\\rho) = \\frac{m}{\\sigma^2} \\frac{-(m-1)\\rho}{1 + (m-1)\\rho} \\cdot \\frac{p_b \\sigma^2}{\\sigma^2 + m p_b}\n$$\n$$\n\\mathrm{RE}(\\rho) = \\frac{m p_b (-(m-1)\\rho)}{(\\sigma^2 + m p_b)(1 + (m-1)\\rho)}\n$$\nThe final expression for the relative error is:\n$$\n\\mathrm{RE}(\\rho) = - \\frac{m p_b (m-1)\\rho}{(\\sigma^2 + m p_b)(1 + (m-1)\\rho)}\n$$\nThis expression is in terms of $\\rho$, $m$, $\\sigma^2$, and $p_b$ as required.",
            "answer": "$$\\boxed{- \\frac{m p_{b} (m-1)\\rho}{(\\sigma^{2} + m p_{b})(1 + (m-1)\\rho)}}$$"
        }
    ]
}