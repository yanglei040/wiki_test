{
    "hands_on_practices": [
        {
            "introduction": "在实验设计中，一个基本的问题是：我们应该重复同一个测量来降低特定方面的噪声，还是应该进行多样化的测量以探索未知参数的不同维度？这个问题探讨了在复制和多样化这两种策略之间的权衡。通过计算两种设计下的预期信息增益（用后验分布到先验分布的Kullback-Leibler散度来衡量），我们可以量化每种策略的价值，并做出最优选择。",
            "id": "3367120",
            "problem": "考虑一个带有二维参数向量 $\\theta \\in \\mathbb{R}^{2}$ 的线性逆问题，该问题用高斯先验 $p(\\theta) = \\mathcal{N}(0,\\Sigma_{0})$ 建模，其中 $\\Sigma_{0} = \\mathrm{diag}(\\alpha,\\beta)$。您可以进行 $N=2$ 次实验，每次实验产生一个标量观测值，其模型为 $y = X \\theta + \\varepsilon$，其中 $X \\in \\mathbb{R}^{N \\times 2}$ 是设计矩阵，其行是所选的设计向量，而 $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^{2} I)$ 是独立高斯噪声。正在考虑两种可选设计：\n\n- 设计 $\\mathrm{A}$ (重复): 两次选择相同的设计向量 $v_{1} = e_{1}$，因此 $X_{\\mathrm{A}}$ 的行为 $e_{1}^{\\top}$ 和 $e_{1}^{\\top}$。\n- 设计 $\\mathrm{B}$ (多样化): 选择正交设计向量 $v_{1} = e_{1}$ 和 $v_{2} = e_{2}$，因此 $X_{\\mathrm{B}}$ 的行为 $e_{1}^{\\top}$ 和 $e_{2}^{\\top}$。\n\n采用贝叶斯实验设计的效用，即从 $\\theta$ 的后验分布到其先验分布的期望 Kullback–Leibler 散度 (KLD)，也就是在指定的线性高斯模型下，$\\theta$ 和数据 $y$ 之间的互信息。从第一性原理和核心定义出发，推导两种设计的效用，并计算其差值\n$$\\Delta U = U_{\\mathrm{B}} - U_{\\mathrm{A}}$$\n对于具体的参数和噪声设置 $\\alpha = 9$, $\\beta = 1$, 和 $\\sigma^{2} = 1$。将您的最终答案表示为包含自然对数的单个闭式解析表达式，不进行四舍五入，也不带单位。",
            "solution": "该问题是有效的，因为它具有科学依据、良定、客观、自洽且一致。它代表了线性逆问题中贝叶斯实验设计的一个标准问题。我们开始求解。\n\n一个实验设计（由设计矩阵 $X$ 表示）的效用，由参数 $\\theta$ 的后验分布到其先验分布的期望 Kullback–Leibler 散度 (KLD) 给出。该量等价于参数 $\\theta$ 和数据 $y$ 之间的互信息。\n对于给定的设计 $X$，效用为 $U(X) = I(\\theta; y | X)$。\n互信息可以用微分熵表示为 $I(\\theta; y) = H(\\theta) - \\mathbb{E}_{y}[H(\\theta|y)]$。\n\n$\\theta \\in \\mathbb{R}^{k}$ 的先验分布为 $p(\\theta) = \\mathcal{N}(\\mu_0, \\Sigma_0)$。一个多元高斯分布的微分熵为 $H(\\theta) = \\frac{1}{2} \\ln \\det(2\\pi e \\Sigma_0)$。在此问题中，$\\theta$ 的维度为 $k=2$，先验均值为 $\\mu_0 = 0$。\n\n观测模型为 $y = X\\theta + \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$。似然函数为 $p(y|\\theta) = \\mathcal{N}(y | X\\theta, \\sigma^2 I)$。\n在此线性高斯模型下，后验分布 $p(\\theta|y)$ 也是高斯分布，$p(\\theta|y) = \\mathcal{N}(\\mu_{post}, \\Sigma_{post})$。后验协方差矩阵 $\\Sigma_{post}$ 由逆先验协方差与来自数据的 Fisher 信息之和的逆给出：\n$$ \\Sigma_{post}^{-1} = \\Sigma_0^{-1} + \\frac{1}{\\sigma^2} X^{\\top}X $$\n关键在于，$\\Sigma_{post}$ 不依赖于具体的数据实现 $y$。后验分布的熵为 $H(p(\\theta|y)) = \\frac{1}{2} \\ln \\det(2\\pi e \\Sigma_{post})$。\n条件熵 $H(\\theta|y)$ 是 $H(p(\\theta|y))$ 在所有可能的数据 $y$ 上的期望：\n$$ \\mathbb{E}_{y}[H(\\theta|y)] = \\mathbb{E}_{y}[H(p(\\theta|y))] $$\n由于 $H(p(\\theta|y))$ 独立于 $y$，期望的计算是平凡的，因此 $\\mathbb{E}_{y}[H(\\theta|y)] = \\frac{1}{2} \\ln \\det(2\\pi e \\Sigma_{post})$。\n\n将熵代入互信息公式：\n$$ U(X) = H(\\theta) - \\mathbb{E}_{y}[H(\\theta|y)] = \\frac{1}{2} \\ln \\det(2\\pi e \\Sigma_0) - \\frac{1}{2} \\ln \\det(2\\pi e \\Sigma_{post}) $$\n$$ U(X) = \\frac{1}{2} \\left[ \\ln(\\det(\\Sigma_0)) - \\ln(\\det(\\Sigma_{post})) \\right] = \\frac{1}{2} \\ln\\left(\\frac{\\det(\\Sigma_0)}{\\det(\\Sigma_{post})}\\right) = \\frac{1}{2} \\ln(\\det(\\Sigma_0 \\Sigma_{post}^{-1})) $$\n代入 $\\Sigma_{post}^{-1}$ 的表达式：\n$$ U(X) = \\frac{1}{2} \\ln\\left(\\det\\left(\\Sigma_0 \\left(\\Sigma_0^{-1} + \\frac{1}{\\sigma^2} X^{\\top}X\\right)\\right)\\right) = \\frac{1}{2} \\ln\\left(\\det\\left(I + \\frac{1}{\\sigma^2} \\Sigma_0 X^{\\top}X\\right)\\right) $$\n该表达式为任何设计 $X$ 提供了效用。我们现在将此公式应用于两种指定的设计，并使用给定的参数值：$\\Sigma_0 = \\mathrm{diag}(\\alpha, \\beta) = \\mathrm{diag}(9, 1)$ 和 $\\sigma^2=1$。效用简化为：\n$$ U(X) = \\frac{1}{2} \\ln(\\det(I + \\Sigma_0 X^{\\top}X)) $$\n\n对于设计 A (重复)，设计矩阵 $X_{\\mathrm{A}}$ 有两个相同的行 $e_1^{\\top} = \\begin{pmatrix} 1  0 \\end{pmatrix}$。\n$$ X_{\\mathrm{A}} = \\begin{pmatrix} 1  0 \\\\ 1  0 \\end{pmatrix} $$\n我们计算矩阵乘积 $X_{\\mathrm{A}}^{\\top}X_{\\mathrm{A}}$：\n$$ X_{\\mathrm{A}}^{\\top}X_{\\mathrm{A}} = \\begin{pmatrix} 1  1 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 1  0 \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ 0  0 \\end{pmatrix} $$\n现在我们可以计算效用 $U_{\\mathrm{A}}$：\n$$ U_{\\mathrm{A}} = \\frac{1}{2} \\ln\\left(\\det\\left( \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} 9  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 2  0 \\\\ 0  0 \\end{pmatrix} \\right)\\right) $$\n$$ U_{\\mathrm{A}} = \\frac{1}{2} \\ln\\left(\\det\\left( \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} 18  0 \\\\ 0  0 \\end{pmatrix} \\right)\\right) $$\n$$ U_{\\mathrm{A}} = \\frac{1}{2} \\ln\\left(\\det\\begin{pmatrix} 19  0 \\\\ 0  1 \\end{pmatrix}\\right) = \\frac{1}{2} \\ln(19) $$\n\n对于设计 B (多样化)，设计矩阵 $X_{\\mathrm{B}}$ 的行为 $e_1^{\\top} = \\begin{pmatrix} 1  0 \\end{pmatrix}$ 和 $e_2^{\\top} = \\begin{pmatrix} 0  1 \\end{pmatrix}$。\n$$ X_{\\mathrm{B}} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I_2 $$\n矩阵乘积 $X_{\\mathrm{B}}^{\\top}X_{\\mathrm{B}}$ 是：\n$$ X_{\\mathrm{B}}^{\\top}X_{\\mathrm{B}} = I_2^{\\top} I_2 = I_2 = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} $$\n现在我们可以计算效用 $U_{\\mathrm{B}}$：\n$$ U_{\\mathrm{B}} = \\frac{1}{2} \\ln\\left(\\det\\left( I_2 + \\Sigma_0 I_2 \\right)\\right) = \\frac{1}{2} \\ln(\\det(I_2 + \\Sigma_0)) $$\n$$ U_{\\mathrm{B}} = \\frac{1}{2} \\ln\\left(\\det\\left( \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} 9  0 \\\\ 0  1 \\end{pmatrix} \\right)\\right) $$\n$$ U_{\\mathrm{B}} = \\frac{1}{2} \\ln\\left(\\det\\begin{pmatrix} 10  0 \\\\ 0  2 \\end{pmatrix}\\right) = \\frac{1}{2} \\ln(20) $$\n\n最后，我们计算效用差 $\\Delta U = U_{\\mathrm{B}} - U_{\\mathrm{A}}$：\n$$ \\Delta U = \\frac{1}{2} \\ln(20) - \\frac{1}{2} \\ln(19) $$\n使用对数性质 $\\ln(a) - \\ln(b) = \\ln(a/b)$，我们得到：\n$$ \\Delta U = \\frac{1}{2} \\left( \\ln(20) - \\ln(19) \\right) = \\frac{1}{2} \\ln\\left(\\frac{20}{19}\\right) $$\n这就是最终的闭式解析表达式。",
            "answer": "$$\\boxed{\\frac{1}{2} \\ln\\left(\\frac{20}{19}\\right)}$$"
        },
        {
            "introduction": "直观上，我们知道随着实验投入的增加，每增加一个单位投入所带来的信息收益会逐渐减少，这就是“收益递减”现象。这个练习旨在通过计算信息增益目标的曲率（二阶导数）来从数学上精确地刻画这一概念。通过分析一个具体的线性高斯贝叶斯逆问题，你将亲手验证增加实验投入（例如，增加独立重复实验的次数）如何导致信息增益的增长速度放缓。",
            "id": "3367052",
            "problem": "考虑一个线性高斯贝叶斯逆问题，其参数向量为 $\\theta \\in \\mathbb{R}^{d}$，服从高斯先验 $\\theta \\sim \\mathcal{N}(0, \\Gamma_{\\text{prior}})$，并有一个线性观测模型 $y = H \\theta + \\eta$。观测噪声是高斯噪声，其协方差为 $\\Gamma_{\\eta}/u$，其中 $u > 0$ 是一个标量努力参数（例如，与独立重复实验的次数成正比），且 $\\eta \\sim \\mathcal{N}(0, \\Gamma_{\\eta}/u)$。在此模型下，对于给定的努力 $u$，期望信息增益（从后验到先验的Kullback–Leibler散度）可以表示为后验和先验协方差行列式的比值。在本问题中，使用以下数据：\n- 维度 $d = 2$。\n- 先验协方差 $\\Gamma_{\\text{prior}} = \\begin{pmatrix} 2  0.6 \\\\ 0.6  1.2 \\end{pmatrix}$。\n- 正向模型矩阵 $H = \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix}$。\n- 噪声协方差 $\\Gamma_{\\eta} = \\begin{pmatrix} 1.0  0.2 \\\\ 0.2  1.5 \\end{pmatrix}$。\n\n从高斯条件化的核心定义和互信息的对数行列式恒等式出发，推导出标量D-最优设计目标 $J(u)$ 作为 $u$ 的函数，并计算其关于 $u$ 的二阶导数以表征曲率（该曲率捕捉了收益递减的特性）。然后，对于特定值 $u = 1$，使用提供的矩阵数值计算曲率 $J''(1)$。\n\n将您的答案四舍五入到 $4$ 位有效数字。将最终答案表示为一个无量纲实数。",
            "solution": "该问题要求推导并评估线性高斯贝叶斯逆问题中D-最优设计目标函数 $J(u)$ 的曲率。目标函数是期望信息增益（EIG），对于此模型，它是从后验到先验的Kullback-Leibler散度，并在所有可能的数据上取平均。\n\n参数向量 $\\theta \\in \\mathbb{R}^{d}$ 的先验分布为 $\\theta \\sim \\mathcal{N}(0, \\Gamma_{\\text{prior}})$。观测模型为 $y = H \\theta + \\eta$，其中噪声 $\\eta$ 的分布为 $\\eta \\sim \\mathcal{N}(0, \\Gamma_{\\eta}/u)$。这里，$u$ 是一个标量努力参数。\n\n在给定观测值 $y$ 的情况下，$\\theta$ 的后验分布也是一个高斯分布，$p(\\theta|y) = \\mathcal{N}(\\mu_{\\text{post}}, \\Gamma_{\\text{post}})$。后验精度（协方差的逆）矩阵是先验精度与来自似然的、依赖于数据的精度之和：\n$$ \\Gamma_{\\text{post}}^{-1} = \\Gamma_{\\text{prior}}^{-1} + H^T (\\Gamma_{\\eta}/u)^{-1} H = \\Gamma_{\\text{prior}}^{-1} + u H^T \\Gamma_{\\eta}^{-1} H $$\n\n我们记为 $J(u)$ 的EIG是参数 $\\theta$ 和数据 $y$ 之间的互信息。对于线性高斯系统，此量与具体的观测值 $y$ 无关，可以表示为先验和后验协方差的形式：\n$$ J(u) = \\frac{1}{2} \\ln \\left( \\frac{\\det(\\Gamma_{\\text{prior}})}{\\det(\\Gamma_{\\text{post}})} \\right) = \\frac{1}{2} \\ln \\left( \\det(\\Gamma_{\\text{post}}^{-1} \\Gamma_{\\text{prior}}) \\right) $$\n代入 $\\Gamma_{\\text{post}}^{-1}$ 的表达式：\n$$ J(u) = \\frac{1}{2} \\ln \\left( \\det\\left( (\\Gamma_{\\text{prior}}^{-1} + u H^T \\Gamma_{\\eta}^{-1} H) \\Gamma_{\\text{prior}} \\right) \\right) = \\frac{1}{2} \\ln \\left( \\det(I + u H^T \\Gamma_{\\eta}^{-1} H \\Gamma_{\\text{prior}}) \\right) $$\n使用 Sylvester 行列式恒等式 $\\det(I + AB) = \\det(I + BA)$，我们可以将其写为：\n$$ J(u) = \\frac{1}{2} \\ln \\left( \\det(I + u \\Gamma_{\\text{prior}} H^T \\Gamma_{\\eta}^{-1} H) \\right) $$\n让我们定义矩阵 $\\mathcal{I} = \\Gamma_{\\text{prior}} H^T \\Gamma_{\\eta}^{-1} H$。虽然 $\\mathcal{I}$ 通常不是对称的，但其特征值是实的、非负的，并且与对称矩阵 $\\mathcal{I}_s = \\Gamma_{\\text{prior}}^{1/2} H^T \\Gamma_{\\eta}^{-1} H \\Gamma_{\\text{prior}}^{1/2}$ 的特征值相同。设 $\\mathcal{I}$ (以及 $\\mathcal{I}_s$) 的特征值为 $\\{\\lambda_i\\}_{i=1}^{d}$。那么行列式可以表示为特征值的乘积：\n$$ J(u) = \\frac{1}{2} \\ln \\left( \\prod_{i=1}^{d} (1 + u \\lambda_i) \\right) = \\frac{1}{2} \\sum_{i=1}^{d} \\ln(1 + u \\lambda_i) $$\n这就是推导出的目标函数。\n\n为了求曲率，我们计算 $J(u)$ 关于 $u$ 的二阶导数。一阶导数是：\n$$ J'(u) = \\frac{dJ}{du} = \\frac{1}{2} \\sum_{i=1}^{d} \\frac{\\lambda_i}{1 + u \\lambda_i} $$\n二阶导数，即曲率，为：\n$$ J''(u) = \\frac{d^2J}{du^2} = \\frac{1}{2} \\sum_{i=1}^{d} \\frac{-(\\lambda_i)(\\lambda_i)}{(1 + u \\lambda_i)^2} = -\\frac{1}{2} \\sum_{i=1}^{d} \\left( \\frac{\\lambda_i}{1 + u \\lambda_i} \\right)^2 $$\n这个 $J''(u)$ 的表达式恒为非正，这正确地捕捉了努力 $u$ 投入的收益递减原理。\n\n现在我们使用给定的矩阵计算 $J''(1)$。维度为 $d=2$。\n- $\\Gamma_{\\text{prior}} = \\begin{pmatrix} 2  0.6 \\\\ 0.6  1.2 \\end{pmatrix}$\n- $H = \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix}$\n- $\\Gamma_{\\eta} = \\begin{pmatrix} 1.0  0.2 \\\\ 0.2  1.5 \\end{pmatrix}$\n\n首先，我们计算矩阵 $\\mathcal{I} = \\Gamma_{\\text{prior}} H^T \\Gamma_{\\eta}^{-1} H$ 并求其特征值。\n\n1.  计算 $\\Gamma_{\\eta}^{-1}$：\n    $\\det(\\Gamma_{\\eta}) = (1.0)(1.5) - (0.2)(0.2) = 1.5 - 0.04 = 1.46$。\n    $$ \\Gamma_{\\eta}^{-1} = \\frac{1}{1.46} \\begin{pmatrix} 1.5  -0.2 \\\\ -0.2  1.0 \\end{pmatrix} $$\n\n2.  计算费雪信息矩阵 $H^T \\Gamma_{\\eta}^{-1} H$：\n    $$ H^T \\Gamma_{\\eta}^{-1} H = \\frac{1}{1.46} \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} 1.5  -0.2 \\\\ -0.2  1.0 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix} $$\n    $$ = \\frac{1}{1.46} \\begin{pmatrix} 1.3  0.8 \\\\ 1.1  1.8 \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix} = \\frac{1}{1.46} \\begin{pmatrix} 2.1  2.9 \\\\ 2.9  4.7 \\end{pmatrix} $$\n\n3.  计算 $\\mathcal{I} = \\Gamma_{\\text{prior}} (H^T \\Gamma_{\\eta}^{-1} H)$：\n    $$ \\mathcal{I} = \\begin{pmatrix} 2  0.6 \\\\ 0.6  1.2 \\end{pmatrix} \\left( \\frac{1}{1.46} \\begin{pmatrix} 2.1  2.9 \\\\ 2.9  4.7 \\end{pmatrix} \\right) $$\n    $$ \\mathcal{I} = \\frac{1}{1.46} \\begin{pmatrix} (2)(2.1) + (0.6)(2.9)  (2)(2.9) + (0.6)(4.7) \\\\ (0.6)(2.1) + (1.2)(2.9)  (0.6)(2.9) + (1.2)(4.7) \\end{pmatrix} $$\n    $$ \\mathcal{I} = \\frac{1}{1.46} \\begin{pmatrix} 4.2 + 1.74  5.8 + 2.82 \\\\ 1.26 + 3.48  1.74 + 5.64 \\end{pmatrix} = \\frac{1}{1.46} \\begin{pmatrix} 5.94  8.62 \\\\ 4.74  7.38 \\end{pmatrix} $$\n\n4.  求 $\\mathcal{I}$ 的特征值。特征值 $\\lambda$ 是特征方程 $\\det(\\mathcal{I} - \\lambda I) = 0$ 的根。令 $A = \\begin{pmatrix} 5.94  8.62 \\\\ 4.74  7.38 \\end{pmatrix}$，那么 $\\mathcal{I}$ 的特征值是 $A$ 的特征值的 $\\frac{1}{1.46}$ 倍。$A$ 的特征方程是 $\\Lambda^2 - \\text{tr}(A)\\Lambda + \\det(A) = 0$。\n    $\\text{tr}(A) = 5.94 + 7.38 = 13.32$。\n    $\\det(A) = (5.94)(7.38) - (8.62)(4.74) = 43.8372 - 40.8588 = 2.9784$。\n    所以，$\\Lambda^2 - 13.32 \\Lambda + 2.9784 = 0$。\n    使用二次公式，$\\Lambda = \\frac{13.32 \\pm \\sqrt{13.32^2 - 4(2.9784)}}{2} = \\frac{13.32 \\pm \\sqrt{165.5088}}{2}$。\n    $\\sqrt{165.5088} \\approx 12.864983$。\n    $\\Lambda_1 = \\frac{13.32 + 12.864983}{2} \\approx 13.0924915$。\n    $\\Lambda_2 = \\frac{13.32 - 12.864983}{2} \\approx 0.2275085$。\n    $\\mathcal{I}$ 的特征值是：\n    $\\lambda_1 = \\Lambda_1 / 1.46 \\approx 13.0924915 / 1.46 \\approx 8.967460$。\n    $\\lambda_2 = \\Lambda_2 / 1.46 \\approx 0.2275085 / 1.46 \\approx 0.155828$。\n\n5.  在 $u=1$ 处计算 $J''(u)$：\n    $$ J''(1) = -\\frac{1}{2} \\left[ \\left( \\frac{\\lambda_1}{1 + \\lambda_1} \\right)^2 + \\left( \\frac{\\lambda_2}{1 + \\lambda_2} \\right)^2 \\right] $$\n    $$ J''(1) \\approx -\\frac{1}{2} \\left[ \\left( \\frac{8.967460}{1 + 8.967460} \\right)^2 + \\left( \\frac{0.155828}{1 + 0.155828} \\right)^2 \\right] $$\n    $$ J''(1) \\approx -\\frac{1}{2} \\left[ \\left( \\frac{8.967460}{9.967460} \\right)^2 + \\left( \\frac{0.155828}{1.155828} \\right)^2 \\right] $$\n    $$ J''(1) \\approx -\\frac{1}{2} \\left[ (0.899679)^2 + (0.134819)^2 \\right] $$\n    $$ J''(1) \\approx -\\frac{1}{2} [ 0.809422 + 0.018176 ] $$\n    $$ J''(1) \\approx -\\frac{1}{2} [ 0.827598 ] \\approx -0.413799 $$\n\n四舍五入到4位有效数字，曲率为 $-0.4138$。",
            "answer": "$$\\boxed{-0.4138}$$"
        },
        {
            "introduction": "现实世界中的实验通常是序贯进行的，我们在每个阶段根据已有的数据来决定是否继续进行下一次测量。本练习将静态设计思想扩展到动态决策情境中，引入了一种基于成本效益分析的自适应停止规则。你将推导在每个阶段的边际预期信息增益，并利用它来确定何时停止实验以最大化净收益，这构成了自适应实验设计的一个基本范例。",
            "id": "3367061",
            "problem": "考虑一个标量参数的序贯贝叶斯实验设计问题。设未知参数 $\\theta \\in \\mathbb{R}$ 服从高斯先验分布 $\\theta \\sim \\mathcal{N}(0,\\sigma_{0}^{2})$，其中 $\\sigma_{0}^{2} > 0$。在每个测量阶段 $k \\in \\{0,1,2,\\dots\\}$，假设我们进行一次线性观测\n$$\ny_{k} = a\\,\\theta + \\epsilon_{k},\n$$\n其中 $a \\in \\mathbb{R}$ 是一个固定的已知设计振幅，$\\epsilon_{k} \\sim \\mathcal{N}(0,\\sigma^{2})$，其中 $\\sigma^{2} > 0$ 已知，且 $\\epsilon_k$ 在不同 $k$ 之间独立，也与 $\\theta$ 独立。由于该模型是线性高斯模型，$\\theta$ 的后验分布在任意次测量后仍然是高斯分布，其更新后的方差由常规的精度加法确定。\n\n将阶段 $k$ 的边际期望信息增益定义为增加第 $(k\\!+\\!1)$ 次观测所带来的不确定性的期望单步减少量。形式上，令 $\\mathrm{MI}$（互信息）以自然对数为底，单位为奈特（nats），并将其定义为当前后验分布与增加一次观测后的后验分布之间的期望 Kullback-Leibler 散度（KLD），或等价地，（微分）熵的期望减少量。即，\n$$\nI_{k} \\equiv \\mathrm{MI}(\\theta; y_{k+1}\\,|\\,\\text{current posterior at stage }k).\n$$\n\n您的任务如下。\n\n1. 从高斯分布的互信息和微分熵的标准定义出发，并根据后验精度的贝叶斯线性高斯更新规则，推导出一个关于当前后验方差 $s_{k}$、设计振幅 $a$ 和噪声方差 $\\sigma^{2}$ 的 $I_{k}$ 的显式公式。不要假设或引用任何预先推导的简化表达式。\n\n2. 考虑一个单步短视最优停止规则，每次观测的阶段成本为 $c > 0$，其单位与信息增益相同（奈特）。在一个边际信息增益不增的序贯无限视界设定中，根据贝叶斯决策理论的第一性原理，推导为什么当边际期望信息增益低于某个阈值 $\\tau^{\\star}$ 时，最优自适应停止规则会停止。请用 $c$ 来表示 $\\tau^{\\star}$。\n\n3. 针对 $a = 1, \\sigma_{0}^{2} = 4, \\sigma^{2} = 1, c = 0.1$（奈特）这一具体数值情况。策略的停止规则如下：在每个阶段 $k$（在进行第 $k+1$ 次测量之前），计算边际期望信息增益 $I_k$。如果 $I_k > c$，则继续进行下一次测量。如果 $I_k \\le c$，则停止实验。$n^{\\star}$ 是根据此规则进行的总测量次数。计算最优策略在停止前将进行的确切测量次数 $n^{\\star}$，以及阈值 $\\tau^{\\star}$。请将阈值以奈特为单位表示，测量次数表示为无量纲整数。\n\n以包含 $\\tau^{\\star}$ 和 $n^{\\star}$ 的行矩阵形式提供最终答案。无需四舍五入，阈值应以奈特为单位表示。",
            "solution": "该问题被评估为有效，因为它在科学上基于贝叶斯统计和信息论，问题表述清晰，客观，并包含得出唯一解所需的所有必要信息。\n\n解答分为三部分，对应于问题陈述中的三个任务。\n\n**第1部分：边际期望信息增益 $I_k$ 的推导**\n\n阶段 $k$ 的边际期望信息增益，记为 $I_k$，是参数 $\\theta$ 与下一次观测 $y_{k+1}$ 之间的互信息，以截至阶段 $k$ 收集到的数据 $D_k = \\{y_1, \\dots, y_k\\}$ 为条件。这可以表示为：\n$$\nI_k = \\mathrm{MI}(\\theta; y_{k+1} | D_k)\n$$\n信息论的一个基本恒等式表明，互信息可以表示为熵的差值：\n$$\n\\mathrm{MI}(X; Y) = H(Y) - H(Y|X)\n$$\n将此应用于我们的条件情景，我们得到：\n$$\nI_k = H(y_{k+1} | D_k) - H(y_{k+1} | \\theta, D_k)\n$$\n其中 $H(\\cdot)$ 表示微分熵，单位为奈特。\n\n在阶段 $k$，经过 $k$ 次观测后，由于模型是线性高斯的，$\\theta$ 的后验分布是高斯分布。我们将其后验表示为 $p(\\theta | D_k) = \\mathcal{N}(\\theta | \\mu_k, s_k)$，其中 $\\mu_k$ 是后验均值，$s_k$ 是后验方差。\n\n首先，我们计算条件熵 $H(y_{k+1} | \\theta, D_k)$。观测模型为 $y_{k+1} = a\\theta + \\epsilon_{k+1}$，其中 $\\epsilon_{k+1} \\sim \\mathcal{N}(0, \\sigma^2)$。由于 $\\epsilon_{k+1}$ 与 $\\theta$ 和过去的数据 $D_k$ 独立，以 $\\theta$ 为条件使得 $y_{k+1}$ 与 $D_k$ 独立。因此，$p(y_{k+1} | \\theta, D_k) = p(y_{k+1} | \\theta) = \\mathcal{N}(y_{k+1} | a\\theta, \\sigma^2)$。高斯分布 $\\mathcal{N}(\\mu, S)$ 的微分熵是 $\\frac{1}{2}\\ln((2\\pi e)^d \\det(S))$，其中 $d$ 是维度。对于标量变量，这是 $\\frac{1}{2}\\ln(2\\pi e S)$。因此，$p(y_{k+1} | \\theta)$ 的熵是：\n$$\nH(y_{k+1} | \\theta, D_k) = \\frac{1}{2}\\ln(2\\pi e \\sigma^2)\n$$\n\n接下来，我们计算边际预测分布的熵 $H(y_{k+1} | D_k)$。分布 $p(y_{k+1} | D_k)$ 是通过对 $\\theta$ 进行边缘化得到的：\n$$\np(y_{k+1} | D_k) = \\int p(y_{k+1} | \\theta) p(\\theta | D_k) d\\theta\n$$\n这是两个高斯分布的卷积。我们有 $\\theta \\sim \\mathcal{N}(\\mu_k, s_k)$ 和 $y_{k+1} | \\theta \\sim \\mathcal{N}(a\\theta, \\sigma^2)$。$y_{k+1}$ 的结果分布也是高斯分布。我们求其均值和方差：\n均值为 $\\mathbb{E}[y_{k+1} | D_k] = \\mathbb{E}[a\\theta | D_k] = a\\mathbb{E}[\\theta | D_k] = a\\mu_k$。\n方差由全方差定律给出：\n$$\n\\mathrm{Var}(y_{k+1} | D_k) = \\mathbb{E}[\\mathrm{Var}(y_{k+1}|\\theta, D_k)|D_k] + \\mathrm{Var}(\\mathbb{E}[y_{k+1}|\\theta, D_k]|D_k)\n$$\n$$\n\\mathrm{Var}(y_{k+1} | D_k) = \\mathbb{E}[\\sigma^2|D_k] + \\mathrm{Var}(a\\theta|D_k) = \\sigma^2 + a^2 \\mathrm{Var}(\\theta|D_k) = \\sigma^2 + a^2 s_k\n$$\n所以，$p(y_{k+1} | D_k) = \\mathcal{N}(y_{k+1} | a\\mu_k, \\sigma^2 + a^2 s_k)$。其熵为：\n$$\nH(y_{k+1} | D_k) = \\frac{1}{2}\\ln(2\\pi e (\\sigma^2 + a^2 s_k))\n$$\n\n将这两个熵的表达式代入 $I_k$ 的公式中：\n$$\nI_k = \\frac{1}{2}\\ln(2\\pi e (\\sigma^2 + a^2 s_k)) - \\frac{1}{2}\\ln(2\\pi e \\sigma^2)\n$$\n$$\nI_k = \\frac{1}{2} \\left( \\ln(\\sigma^2 + a^2 s_k) - \\ln(\\sigma^2) \\right) = \\frac{1}{2} \\ln\\left(\\frac{\\sigma^2 + a^2 s_k}{\\sigma^2}\\right)\n$$\n这简化为边际期望信息增益的最终表达式：\n$$\nI_k = \\frac{1}{2} \\ln\\left(1 + \\frac{a^2 s_k}{\\sigma^2}\\right)\n$$\n此公式仅依赖于当前后验方差 $s_k$、设计振幅 $a$ 和噪声方差 $\\sigma^2$，符合要求。\n\n**第2部分：最优停止阈值 $\\tau^\\star$ 的推导**\n\n在一个短视（单步前瞻）的贝叶斯决策理论框架中，在每个阶段 $k$，我们必须决定是停止还是进行另一次测量。决策基于最大化即时期望净收益。效用以信息单位（奈特）衡量，每次测量有成本 $c > 0$ 奈特。\n\n在阶段 $k$，获取数据 $D_k$ 后，我们面临两个选择：\n1.  **停止**：停止进行测量。额外收益为 $0$，且不产生进一步成本。此决策的净收益为 $0$。\n2.  **继续**：再进行一次测量 $y_{k+1}$。此行动产生一个确定性成本 $c$。期望收益是这次新测量所带来的关于 $\\theta$ 的信息增益，这正是边际期望信息增益 $I_k$。因此，该行动的期望净收益是 $I_k - c$。\n\n一个寻求最大化效用的理性代理人会选择期望净收益更高的行动。代理人当且仅当继续的期望净收益大于停止的净收益时才应该继续：\n$$\nI_k - c > 0 \\implies I_k > c\n$$\n如果 $I_k - c \\le 0 \\implies I_k \\le c$，代理人应该停止。\n停止规则是，一旦下一次测量的期望信息增益 $I_k$ 不再大于获取它的成本 $c$，就停止实验。阈值 $\\tau^\\star$ 是停止与继续决策的临界点。因此，阈值为：\n$$\n\\tau^\\star = c\n$$\n问题陈述中提到边际信息增益 $I_k$ 是不增的。我们可以验证这一点。后验方差根据精度求和法则更新：$\\frac{1}{s_k} = \\frac{1}{s_{k-1}} + \\frac{a^2}{\\sigma^2}$。由于 $a^2 > 0$ 和 $\\sigma^2 > 0$，精度在每一步都严格增加。因此，后验方差 $s_k$ 严格减小：对于所有 $k \\ge 1$，$s_k  s_{k-1}$。信息增益 $I_k = \\frac{1}{2} \\ln(1 + a^2 s_k / \\sigma^2)$ 是 $s_k$ 的单调递增函数。由于 $\\{s_k\\}$ 是一个严格递减序列，$\\{I_k\\}$ 也必定是一个严格递减序列。此性质确保一旦满足停止条件 $I_k \\le \\tau^\\star$，在所有后续阶段该条件都将保持满足，使得停止决策是最终的。\n\n**第3部分：$\\tau^\\star$ 和 $n^\\star$ 的数值计算**\n\n我们给定的具体值为：$a = 1, \\sigma_0^2 = 4, \\sigma^2 = 1, c = 0.1$。停止规则是当 $I_k \\le c$ 时停止。\n\n首先，我们使用第2部分的结果确定停止阈值 $\\tau^\\star$：\n$$\n\\tau^\\star = c = 0.1 \\text{ nats}\n$$\n接下来，我们必须找到测量次数 $n^\\star$。实验从阶段 $k=0$（只有先验）开始。我们计算 $I_k$ 并与 $c$ 比较，直到首次满足 $I_k \\le c$。\n\n令 $P_k = 1/s_k$ 为阶段 $k$ 的后验精度。精度更新规则为 $P_{k} = P_{k-1} + a^2/\\sigma^2$。这是一个等差数列。\n根据给定值，增量为 $a^2/\\sigma^2 = 1^2/1 = 1$。\n初始精度为 $P_0 = 1/\\sigma_0^2 = 1/4 = 0.25$。\n所以，在进行了 $k$ 次测量后，在阶段 $k$ 的精度是 $P_k = P_0 + k \\cdot (a^2/\\sigma^2) = 0.25 + k$。\n阶段 $k$ 的后验方差是 $s_k = 1/P_k = 1/(k+0.25)$。\n\n现在，我们使用第1部分的公式来写出信息增益 $I_k$：\n$$\nI_k = \\frac{1}{2} \\ln\\left(1 + \\frac{a^2 s_k}{\\sigma^2}\\right) = \\frac{1}{2} \\ln\\left(1 + \\frac{1^2 \\cdot \\frac{1}{k+0.25}}{1}\\right) = \\frac{1}{2} \\ln\\left(1 + \\frac{1}{k+0.25}\\right)\n$$\n停止条件是 $I_k \\le 0.1$。我们需要找到满足此不等式的最小整数 $k$：\n$$\n\\frac{1}{2} \\ln\\left(1 + \\frac{1}{k+0.25}\\right) \\le 0.1\n$$\n$$\n\\ln\\left(1 + \\frac{1}{k+0.25}\\right) \\le 0.2\n$$\n对两边取指数（因为 $\\exp(\\cdot)$ 是一个严格递增函数）：\n$$\n1 + \\frac{1}{k+0.25} \\le \\exp(0.2)\n$$\n$$\n\\frac{1}{k+0.25} \\le \\exp(0.2) - 1\n$$\n取倒数会反转不等号（因为两边都为正）：\n$$\nk+0.25 \\ge \\frac{1}{\\exp(0.2) - 1}\n$$\n$$\nk \\ge \\frac{1}{\\exp(0.2) - 1} - 0.25\n$$\n数值上，$\\exp(0.2) \\approx 1.221402758$。\n$$\nk \\ge \\frac{1}{1.221402758 - 1} - 0.25 = \\frac{1}{0.221402758} - 0.25\n$$\n$$\nk \\ge 4.51665 - 0.25 = 4.26665\n$$\n满足 $k \\ge 4.26665$ 的最小整数是 $k=5$。\n这意味着在阶段 $k=5$ 时（即已经进行了5次测量后），我们评估 $I_5$ 并发现它首次满足停止条件。\n$I_4 = \\frac{1}{2} \\ln(1 + \\frac{1}{4.25}) \\approx 0.105 > 0.1 \\implies$ 继续，进行第5次测量。\n$I_5 = \\frac{1}{2} \\ln(1 + \\frac{1}{5.25}) \\approx 0.087 \\le 0.1 \\implies$ 停止。\n因此，策略在进行了5次测量后停止。总测量次数 $n^\\star = 5$。\n\n所需的值为 $\\tau^\\star = 0.1$ 和 $n^\\star=5$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.1  5 \\end{pmatrix}}\n$$"
        }
    ]
}