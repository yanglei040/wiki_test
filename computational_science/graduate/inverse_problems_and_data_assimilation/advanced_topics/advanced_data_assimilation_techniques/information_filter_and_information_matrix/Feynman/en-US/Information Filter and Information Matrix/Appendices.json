{
    "hands_on_practices": [
        {
            "introduction": "The power of the information filter lies in its additive nature, where new data simply adds to the existing information. This first practice grounds your understanding by tasking you with deriving the information update rules directly from Bayes' theorem for a linear-Gaussian model. By working through both a sequential update and a batch update for the same set of observations , you will prove a fundamental property of these systems and solidify your grasp of the core mechanics.",
            "id": "3390747",
            "problem": "Consider a linear Gaussian inverse problem in information form for a state vector $x \\in \\mathbb{R}^{2}$ with prior density proportional to $\\exp\\!\\big(-\\tfrac{1}{2}\\,x^{\\top} Y x + y^{\\top} x\\big)$, where the prior information matrix and prior information vector are\n$$\nY \\,=\\, \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix}, \\qquad y \\,=\\, \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}.\n$$\nYou receive two independent scalar observations of the form $z_{i} = h_{i}^{\\top} x + v_{i}$ with $v_{i} \\sim \\mathcal{N}(0, r_{i})$, with the following data:\n$$\nh_{1} \\,=\\, \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad r_{1} \\,=\\, \\tfrac{1}{2}, \\quad z_{1} \\,=\\, \\tfrac{3}{2}; \n\\qquad\nh_{2} \\,=\\, \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}, \\quad r_{2} \\,=\\, 2, \\quad z_{2} \\,=\\, -\\tfrac{1}{2}.\n$$\nStarting from Bayesâ€™ rule and the Gaussian likelihood, and using only fundamental algebraic manipulations (e.g., expansion and completion of squares) as your base, do the following:\n\n1. Derive the information-form posterior update for a single scalar observation $z = h^{\\top} x + v$ with $v \\sim \\mathcal{N}(0, r)$, expressed as an updated information matrix $Y^{+}$ and updated information vector $y^{+}$ in terms of $Y$, $y$, $h$, $r$, and $z$.\n\n2. Apply your scalar update sequentially for the two observations given above to obtain $Y_{\\mathrm{seq}}$ and $y_{\\mathrm{seq}}$.\n\n3. Now form the multiple-observation model $z = H x + v$ with\n$$\nH \\,=\\, \\begin{pmatrix} h_{1}^{\\top} \\\\ h_{2}^{\\top} \\end{pmatrix} \\,=\\, \\begin{pmatrix} 1 & 1 \\\\ 2 & -1 \\end{pmatrix}, \n\\qquad \nR \\,=\\, \\operatorname{diag}(r_{1}, r_{2}) \\,=\\, \\begin{pmatrix} \\tfrac{1}{2} & 0 \\\\ 0 & 2 \\end{pmatrix}, \n\\qquad \nz \\,=\\, \\begin{pmatrix} z_{1} \\\\ z_{2} \\end{pmatrix} \\,=\\, \\begin{pmatrix} \\tfrac{3}{2} \\\\ -\\tfrac{1}{2} \\end{pmatrix}.\n$$\nDerive, again from the Gaussian likelihood and completion of squares, the batch information-form posterior update to obtain $Y_{\\mathrm{bat}}$ and $y_{\\mathrm{bat}}$.\n\nFinally, define the scalar\n$$\nS \\,=\\, \\big\\|\\, Y_{\\mathrm{seq}} - Y_{\\mathrm{bat}} \\,\\big\\|_{F}^{2} \\;+\\; \\big\\|\\, y_{\\mathrm{seq}} - y_{\\mathrm{bat}} \\,\\big\\|_{2}^{2},\n$$\nwhere $\\|\\cdot\\|_{F}$ is the Frobenius norm and $\\|\\cdot\\|_{2}$ is the Euclidean norm. Compute $S$ exactly. Provide your final result as a single real number. No rounding is required and no units are needed.",
            "solution": "The problem requires the derivation and application of information-form updates for a linear Gaussian system, both sequentially and in batch form, followed by a comparison of the results. The state vector is $x \\in \\mathbb{R}^{2}$.\n\nThe analysis begins with Bayes' rule in logarithmic form, which states that the log-posterior is proportional to the sum of the log-likelihood and the log-prior, up to an additive constant:\n$$\n\\ln p(x|\\text{data}) = \\ln p(\\text{data}|x) + \\ln p(x) + C\n$$\nThe prior probability density $p(x)$ is given in information form, proportional to $\\exp(-\\frac{1}{2}x^{\\top} Y x + y^{\\top} x)$. The argument of the exponential, which we will denote as $J_{\\text{prior}}(x)$, is:\n$$\nJ_{\\text{prior}}(x) = -\\frac{1}{2}x^{\\top} Y x + y^{\\top} x\n$$\n\n1.  Derivation of the scalar update rule.\n\nFor a single scalar observation $z = h^{\\top} x + v$ where the noise $v$ is distributed as $\\mathcal{N}(0, r)$, the likelihood function $p(z|x)$ is a Gaussian:\n$$\np(z|x) = \\frac{1}{\\sqrt{2\\pi r}} \\exp\\left(-\\frac{1}{2r}(z - h^{\\top} x)^2\\right)\n$$\nThe log-likelihood, ignoring constants independent of $x$, is proportional to the argument of the exponential, which we call $J_{\\text{like}}(x)$:\n$$\nJ_{\\text{like}}(x) = -\\frac{1}{2r}(z - h^{\\top} x)^2\n$$\nExpanding this quadratic form:\n$$\nJ_{\\text{like}}(x) = -\\frac{1}{2r}(z^2 - 2z h^{\\top} x + (h^{\\top} x)^2) = -\\frac{1}{2r}(z^2 - 2z x^{\\top} h + x^{\\top} h h^{\\top} x)\n$$\nDropping the term $-\\frac{z^2}{2r}$ which is independent of $x$, we have:\n$$\nJ_{\\text{like}}(x) = \\frac{z}{r} x^{\\top} h - \\frac{1}{2r} x^{\\top} h h^{\\top} x = -\\frac{1}{2} x^{\\top} \\left( \\frac{1}{r} h h^{\\top} \\right) x + \\left( \\frac{z}{r} h \\right)^{\\top} x\n$$\nThe posterior log-density argument $J_{\\text{post}}(x)$ is the sum of the prior and likelihood arguments:\n$$\nJ_{\\text{post}}(x) = J_{\\text{prior}}(x) + J_{\\text{like}}(x) = \\left(-\\frac{1}{2}x^{\\top} Y x + y^{\\top} x\\right) + \\left(-\\frac{1}{2} x^{\\top} \\left(\\frac{1}{r} h h^{\\top}\\right) x + \\left(\\frac{z}{r} h\\right)^{\\top} x\\right)\n$$\nGrouping the quadratic and linear terms in $x$:\n$$\nJ_{\\text{post}}(x) = -\\frac{1}{2} x^{\\top} \\left(Y + \\frac{1}{r} h h^{\\top}\\right) x + \\left(y + \\frac{z}{r} h\\right)^{\\top} x\n$$\nBy comparing this to the general information form $-\\frac{1}{2}x^{\\top} Y^{+} x + (y^{+})^{\\top} x$, we identify the updated information matrix $Y^{+}$ and information vector $y^{+}$:\n$$\nY^{+} = Y + \\frac{1}{r} h h^{\\top}\n$$\n$$\ny^{+} = y + \\frac{z}{r} h\n$$\n\n2.  Sequential application of the scalar update.\n\nWe start with the prior information $Y = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix}$ and $y = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$.\n\nFirst observation update: $h_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, r_{1} = \\frac{1}{2}, z_{1} = \\frac{3}{2}$.\nThe information contribution from this observation is $\\frac{1}{r_1}h_{1}h_{1}^{\\top}$ and $\\frac{z_1}{r_1}h_1$.\n$$\n\\frac{1}{r_1}h_{1}h_{1}^{\\top} = \\frac{1}{1/2}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\begin{pmatrix} 1 & 1 \\end{pmatrix} = 2\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 2 \\\\ 2 & 2 \\end{pmatrix}\n$$\n$$\n\\frac{z_1}{r_1}h_1 = \\frac{3/2}{1/2}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 3\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}\n$$\nThe intermediate posterior information matrix $Y_{1}$ and vector $y_{1}$ are:\n$$\nY_{1} = Y + \\frac{1}{r_1}h_{1}h_{1}^{\\top} = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix} + \\begin{pmatrix} 2 & 2 \\\\ 2 & 2 \\end{pmatrix} = \\begin{pmatrix} 5 & 3 \\\\ 3 & 4 \\end{pmatrix}\n$$\n$$\ny_{1} = y + \\frac{z_1}{r_1}h_1 = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} + \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix}\n$$\nSecond observation update: $h_{2} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}, r_{2} = 2, z_{2} = -\\frac{1}{2}$.\nThis update is applied to the intermediate posterior $(Y_1, y_1)$.\n$$\n\\frac{1}{r_2}h_{2}h_{2}^{\\top} = \\frac{1}{2}\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}\\begin{pmatrix} 2 & -1 \\end{pmatrix} = \\frac{1}{2}\\begin{pmatrix} 4 & -2 \\\\ -2 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & -1 \\\\ -1 & \\frac{1}{2} \\end{pmatrix}\n$$\n$$\n\\frac{z_2}{r_2}h_2 = \\frac{-1/2}{2}\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = -\\frac{1}{4}\\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix}\n$$\nThe final sequential posterior information matrix $Y_{\\mathrm{seq}}$ and vector $y_{\\mathrm{seq}}$ are:\n$$\nY_{\\mathrm{seq}} = Y_{1} + \\frac{1}{r_2}h_{2}h_{2}^{\\top} = \\begin{pmatrix} 5 & 3 \\\\ 3 & 4 \\end{pmatrix} + \\begin{pmatrix} 2 & -1 \\\\ -1 & \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 7 & 2 \\\\ 2 & \\frac{9}{2} \\end{pmatrix}\n$$\n$$\ny_{\\mathrm{seq}} = y_{1} + \\frac{z_2}{r_2}h_2 = \\begin{pmatrix} 4 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2} \\\\ \\frac{5}{4} \\end{pmatrix}\n$$\n\n3.  Derivation and application of the batch update.\n\nFor the multiple-observation model $z = Hx + v$ with $v \\sim \\mathcal{N}(0, R)$, the log-likelihood argument, ignoring constants, is:\n$$\nJ_{\\text{like}}(x) = -\\frac{1}{2}(z - Hx)^{\\top} R^{-1} (z - Hx)\n$$\nExpanding this expression:\n$$\nJ_{\\text{like}}(x) = -\\frac{1}{2}(z^{\\top}R^{-1}z - z^{\\top}R^{-1}Hx - x^{\\top}H^{\\top}R^{-1}z + x^{\\top}H^{\\top}R^{-1}Hx)\n$$\nDropping the term $-\\frac{1}{2}z^{\\top}R^{-1}z$ and combining the two linear terms (which are scalars and transposes of each other):\n$$\nJ_{\\text{like}}(x) = -\\frac{1}{2}x^{\\top}(H^{\\top}R^{-1}H)x + (H^{\\top}R^{-1}z)^{\\top}x\n$$\nThe batch posterior exponent is $J_{\\text{post}}(x) = J_{\\text{prior}}(x) + J_{\\text{like}}(x)$:\n$$\nJ_{\\text{post}}(x) = \\left(-\\frac{1}{2}x^{\\top} Y x + y^{\\top} x\\right) + \\left(-\\frac{1}{2}x^{\\top}(H^{\\top}R^{-1}H)x + (H^{\\top}R^{-1}z)^{\\top}x\\right)\n$$\n$$\nJ_{\\text{post}}(x) = -\\frac{1}{2}x^{\\top}(Y + H^{\\top}R^{-1}H)x + (y + H^{\\top}R^{-1}z)^{\\top}x\n$$\nThis gives the batch update rules:\n$$\nY_{\\mathrm{bat}} = Y + H^{\\top}R^{-1}H\n$$\n$$\ny_{\\mathrm{bat}} = y + H^{\\top}R^{-1}z\n$$\nNow we apply this using the given batch data: $H = \\begin{pmatrix} 1 & 1 \\\\ 2 & -1 \\end{pmatrix}$, $R = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & 2 \\end{pmatrix}$, $z = \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\end{pmatrix}$.\nThe inverse of the covariance matrix is $R^{-1} = \\begin{pmatrix} 2 & 0 \\\\ 0 & \\frac{1}{2} \\end{pmatrix}$.\nFirst, we compute the information contribution matrix $H^{\\top}R^{-1}H$:\n$$\nH^{\\top}R^{-1}H = \\begin{pmatrix} 1 & 2 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 2 & -1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 2 & -\\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 2 & -1 \\end{pmatrix} = \\begin{pmatrix} 4 & 1 \\\\ 1 & \\frac{5}{2} \\end{pmatrix}\n$$\nThen, we compute the information contribution vector $H^{\\top}R^{-1}z$:\n$$\nH^{\\top}R^{-1}z = \\begin{pmatrix} 1 & 2 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} 2 & 0 \\\\ 0 & \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1 & 2 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ -\\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 3-\\frac{1}{2} \\\\ 3+\\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{2} \\\\ \\frac{13}{4} \\end{pmatrix}\n$$\nNow, we find the batch posterior information matrix $Y_{\\mathrm{bat}}$ and vector $y_{\\mathrm{bat}}$:\n$$\nY_{\\mathrm{bat}} = Y + H^{\\top}R^{-1}H = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix} + \\begin{pmatrix} 4 & 1 \\\\ 1 & \\frac{5}{2} \\end{pmatrix} = \\begin{pmatrix} 7 & 2 \\\\ 2 & \\frac{9}{2} \\end{pmatrix}\n$$\n$$\ny_{\\mathrm{bat}} = y + H^{\\top}R^{-1}z = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix} + \\begin{pmatrix} \\frac{5}{2} \\\\ \\frac{13}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{2+5}{2} \\\\ \\frac{-8+13}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2} \\\\ \\frac{5}{4} \\end{pmatrix}\n$$\n\nFinally, we compute the scalar $S$.\nBy comparing the results from the sequential and batch updates, we find that they are identical:\n$$\nY_{\\mathrm{seq}} = \\begin{pmatrix} 7 & 2 \\\\ 2 & \\frac{9}{2} \\end{pmatrix} = Y_{\\mathrm{bat}}\n$$\n$$\ny_{\\mathrm{seq}} = \\begin{pmatrix} \\frac{7}{2} \\\\ \\frac{5}{4} \\end{pmatrix} = y_{\\mathrm{bat}}\n$$\nTherefore, the differences are the zero matrix and zero vector:\n$$\nY_{\\mathrm{seq}} - Y_{\\mathrm{bat}} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\n$$\ny_{\\mathrm{seq}} - y_{\\mathrm{bat}} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThe scalar $S$ is defined as $S = \\big\\|\\, Y_{\\mathrm{seq}} - Y_{\\mathrm{bat}} \\,\\big\\|_{F}^{2} \\;+\\; \\big\\|\\, y_{\\mathrm{seq}} - y_{\\mathrm{bat}} \\,\\big\\|_{2}^{2}$.\nThe Frobenius norm squared of the zero matrix is:\n$$\n\\big\\|\\, Y_{\\mathrm{seq}} - Y_{\\mathrm{bat}} \\,\\big\\|_{F}^{2} = 0^2 + 0^2 + 0^2 + 0^2 = 0\n$$\nThe Euclidean norm squared of the zero vector is:\n$$\n\\big\\|\\, y_{\\mathrm{seq}} - y_{\\mathrm{bat}} \\,\\big\\|_{2}^{2} = 0^2 + 0^2 = 0\n$$\nThus, the value of $S$ is:\n$$\nS = 0 + 0 = 0\n$$\nThis result demonstrates the fundamental property that for linear Gaussian systems, sequential Bayesian updates are equivalent to a single batch update incorporating all data simultaneously.",
            "answer": "$$\\boxed{0}$$"
        },
        {
            "introduction": "A key advantage of the information representation becomes apparent in systems with local interactions, such as discretized physical fields or time-series models. This exercise  explores this by having you construct the prior information matrix for a random-walk process, revealing its sparse, tridiagonal structure. You will then see how local observations contribute information locally, reinforcing the idea that the information matrix directly reflects the conditional dependency graph of the model.",
            "id": "3390748",
            "problem": "Consider a one-dimensional chain of $n=5$ latent states $x_1, x_2, x_3, x_4, x_5$. Assume a random-walk Gaussian prior defined by the transition model $x_{i+1} \\mid x_i \\sim \\mathcal{N}(x_i, q)$ for $i=1,2,3,4$ with process variance $q>0$, and an anchor prior $x_1 \\sim \\mathcal{N}(0, \\sigma_0^2)$ with variance $\\sigma_0^2>0$. This prior induces a joint Gaussian distribution over $\\mathbf{x} = (x_1, x_2, x_3, x_4, x_5)^{\\top}$ whose precision (information) matrix is tridiagonal.\n\nYou collect three local observations with a banded linear observation operator and independent Gaussian noise: $y_1 = x_1 + v_1$, $y_3 = x_3 + v_3$, and $y_{45} = x_4 - x_5 + v_{45}$, where $v_1 \\sim \\mathcal{N}(0, r_1)$, $v_3 \\sim \\mathcal{N}(0, r_3)$, and $v_{45} \\sim \\mathcal{N}(0, r_{45})$, with $r_1>0$, $r_3>0$, and $r_{45}>0$. The observation operator $H$ and noise covariance $R$ are thus\n$$\nH \\;=\\;\n\\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & -1\n\\end{pmatrix},\n\\quad\nR \\;=\\; \\operatorname{diag}(r_1, r_3, r_{45}).\n$$\n\nStarting from first principles of Gaussian models and the definition of the precision (information) matrix as the quadratic form in the negative log-density, derive the prior precision matrix for the random-walk prior on $\\mathbf{x}$ and then derive the posterior precision matrix after assimilating the three observations. Express your final result as a single explicit analytic expression for the posterior precision matrix in terms of $q$, $\\sigma_0^2$, $r_1$, $r_3$, and $r_{45}$.",
            "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution. It is a standard problem in the field of inverse problems and data assimilation, specifically concerning linear-Gaussian state-space models and their representation in the information form.\n\nThe posterior probability density function $p(\\mathbf{x}|\\mathbf{y})$ is, by Bayes' theorem, proportional to the product of the likelihood $p(\\mathbf{y}|\\mathbf{x})$ and the prior $p(\\mathbf{x})$. For joint Gaussian distributions, this relationship is most conveniently expressed in terms of their negative logarithms, which are quadratic forms. The posterior precision (or information) matrix, denoted $\\Lambda_{\\text{post}}$, is the sum of the prior precision matrix, $\\Lambda_{\\text{prior}}$, and the precision matrix associated with the likelihood, $\\Lambda_{\\text{likelihood}}$.\n$$\n\\Lambda_{\\text{post}} = \\Lambda_{\\text{prior}} + \\Lambda_{\\text{likelihood}}\n$$\nWe derive each term separately.\n\nFirst, we derive the prior precision matrix $\\Lambda_{\\text{prior}}$. The prior distribution over the state vector $\\mathbf{x} = (x_1, x_2, x_3, x_4, x_5)^{\\top}$ is given by the chain rule:\n$$\np(\\mathbf{x}) = p(x_1) \\prod_{i=1}^{4} p(x_{i+1}|x_i)\n$$\nThe problem specifies the distributions: $x_1 \\sim \\mathcal{N}(0, \\sigma_0^2)$ and $x_{i+1}|x_i \\sim \\mathcal{N}(x_i, q)$. The negative log-prior, up to an additive constant, is:\n$$\n-\\ln p(\\mathbf{x}) = -\\ln p(x_1) - \\sum_{i=1}^{4} \\ln p(x_{i+1}|x_i) = \\frac{1}{2\\sigma_0^2}x_1^2 + \\sum_{i=1}^{4}\\frac{1}{2q}(x_{i+1}-x_i)^2 + \\text{const.}\n$$\nThis expression is a quadratic form in $\\mathbf{x}$, which can be written as $\\frac{1}{2}\\mathbf{x}^{\\top}\\Lambda_{\\text{prior}}\\mathbf{x}$. By expanding the sum and collecting terms, we can identify the elements of the symmetric matrix $\\Lambda_{\\text{prior}}$.\n$$\n\\frac{1}{2}\\mathbf{x}^{\\top}\\Lambda_{\\text{prior}}\\mathbf{x} = \\frac{1}{2\\sigma_0^2}x_1^2 + \\frac{1}{2q}\\left( (x_2-x_1)^2 + (x_3-x_2)^2 + (x_4-x_3)^2 + (x_5-x_4)^2 \\right)\n$$\nExpanding the squares:\n$$\n= \\frac{1}{2\\sigma_0^2}x_1^2 + \\frac{1}{2q}\\left( (x_1^2 - 2x_1x_2 + x_2^2) + (x_2^2 - 2x_2x_3 + x_3^2) + (x_3^2 - 2x_3x_4 + x_4^2) + (x_4^2 - 2x_4x_5 + x_5^2) \\right)\n$$\nCollecting coefficients for each quadratic term $\\frac{1}{2}x_i x_j$:\n\\begin{itemize}\n    \\item $x_1^2$: $(\\frac{1}{\\sigma_0^2} + \\frac{1}{q})$\n    \\item $x_2^2$: $(\\frac{1}{q} + \\frac{1}{q}) = \\frac{2}{q}$\n    \\item $x_3^2$: $(\\frac{1}{q} + \\frac{1}{q}) = \\frac{2}{q}$\n    \\item $x_4^2$: $(\\frac{1}{q} + \\frac{1}{q}) = \\frac{2}{q}$\n    \\item $x_5^2$: $\\frac{1}{q}$\n    \\item $x_1x_2$: $-\\frac{2}{q}$\n    \\item $x_2x_3$: $-\\frac{2}{q}$\n    \\item $x_3x_4$: $-\\frac{2}{q}$\n    \\item $x_4x_5$: $-\\frac{2}{q}$\n\\end{itemize}\nSince the total quadratic form is $\\frac{1}{2}\\sum_{i,j}(\\Lambda_{\\text{prior}})_{ij}x_i x_j$, the diagonal elements $(\\Lambda_{\\text{prior}})_{ii}$ are the coefficients of $x_i^2$, and the off-diagonal elements $(\\Lambda_{\\text{prior}})_{ij}$ are the coefficients of $x_i x_j$ for $i \\neq j$. This yields the tridiagonal prior precision matrix:\n$$\n\\Lambda_{\\text{prior}} =\n\\begin{pmatrix}\n\\frac{1}{\\sigma_0^2} + \\frac{1}{q} & -\\frac{1}{q} & 0 & 0 & 0 \\\\\n-\\frac{1}{q} & \\frac{2}{q} & -\\frac{1}{q} & 0 & 0 \\\\\n0 & -\\frac{1}{q} & \\frac{2}{q} & -\\frac{1}{q} & 0 \\\\\n0 & 0 & -\\frac{1}{q} & \\frac{2}{q} & -\\frac{1}{q} \\\\\n0 & 0 & 0 & -\\frac{1}{q} & \\frac{1}{q}\n\\end{pmatrix}\n$$\n\nNext, we derive the likelihood precision matrix $\\Lambda_{\\text{likelihood}}$. The observation model is $\\mathbf{y} = H\\mathbf{x} + \\mathbf{v}$, where $\\mathbf{v} \\sim \\mathcal{N}(\\mathbf{0}, R)$. The likelihood function is $p(\\mathbf{y}|\\mathbf{x}) \\sim \\mathcal{N}(H\\mathbf{x}, R)$. The negative log-likelihood, up to a constant, defines the information contributed by the observations:\n$$\n-\\ln p(\\mathbf{y}|\\mathbf{x}) = \\frac{1}{2}(\\mathbf{y} - H\\mathbf{x})^{\\top}R^{-1}(\\mathbf{y} - H\\mathbf{x}) + \\text{const.}\n$$\nThe quadratic part in $\\mathbf{x}$ is $\\frac{1}{2}\\mathbf{x}^{\\top}H^{\\top}R^{-1}H\\mathbf{x}$. Therefore, the likelihood precision matrix is given by:\n$$\n\\Lambda_{\\text{likelihood}} = H^{\\top}R^{-1}H\n$$\nWe are given $H$ and $R$. The inverse of the diagonal noise covariance matrix $R$ is:\n$$\nR^{-1} = \\operatorname{diag}(r_1, r_3, r_{45})^{-1} = \\operatorname{diag}\\left(\\frac{1}{r_1}, \\frac{1}{r_3}, \\frac{1}{r_{45}}\\right)\n$$\nThe transpose of the observation operator $H$ is:\n$$\nH^{\\top} =\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & -1\n\\end{pmatrix}\n$$\nNow we compute the product $H^{\\top}R^{-1}H$:\n$$\nH^{\\top}R^{-1}H =\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{1}{r_1} & 0 & 0 \\\\\n0 & \\frac{1}{r_3} & 0 \\\\\n0 & 0 & \\frac{1}{r_{45}}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & -1\n\\end{pmatrix}\n$$\n$$\n=\n\\begin{pmatrix}\n\\frac{1}{r_1} & 0 & 0 \\\\\n0 & 0 & 0 \\\\\n0 & \\frac{1}{r_3} & 0 \\\\\n0 & 0 & \\frac{1}{r_{45}} \\\\\n0 & 0 & -\\frac{1}{r_{45}}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & -1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{1}{r_1} & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\frac{1}{r_3} & 0 & 0 \\\\\n0 & 0 & 0 & \\frac{1}{r_{45}} & -\\frac{1}{r_{45}} \\\\\n0 & 0 & 0 & -\\frac{1}{r_{45}} & \\frac{1}{r_{45}}\n\\end{pmatrix}\n$$\n\nFinally, the posterior precision matrix $\\Lambda_{\\text{post}}$ is the sum of $\\Lambda_{\\text{prior}}$ and $\\Lambda_{\\text{likelihood}}$:\n$$\n\\Lambda_{\\text{post}} =\n\\begin{pmatrix}\n\\frac{1}{\\sigma_0^2} + \\frac{1}{q} & -\\frac{1}{q} & 0 & 0 & 0 \\\\\n-\\frac{1}{q} & \\frac{2}{q} & -\\frac{1}{q} & 0 & 0 \\\\\n0 & -\\frac{1}{q} & \\frac{2}{q} & -\\frac{1}{q} & 0 \\\\\n0 & 0 & -\\frac{1}{q} & \\frac{2}{q} & -\\frac{1}{q} \\\\\n0 & 0 & 0 & -\\frac{1}{q} & \\frac{1}{q}\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n\\frac{1}{r_1} & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & \\frac{1}{r_3} & 0 & 0 \\\\\n0 & 0 & 0 & \\frac{1}{r_{45}} & -\\frac{1}{r_{45}} \\\\\n0 & 0 & 0 & -\\frac{1}{r_{45}} & \\frac{1}{r_{45}}\n\\end{pmatrix}\n$$\n$$\n\\Lambda_{\\text{post}} =\n\\begin{pmatrix}\n\\frac{1}{\\sigma_0^2} + \\frac{1}{q} + \\frac{1}{r_1} & -\\frac{1}{q} & 0 & 0 & 0 \\\\\n-\\frac{1}{q} & \\frac{2}{q} & -\\frac{1}{q} & 0 & 0 \\\\\n0 & -\\frac{1}{q} & \\frac{2}{q} + \\frac{1}{r_3} & -\\frac{1}{q} & 0 \\\\\n0 & 0 & -\\frac{1}{q} & \\frac{2}{q} + \\frac{1}{r_{45}} & -\\frac{1}{q} - \\frac{1}{r_{45}} \\\\\n0 & 0 & 0 & -\\frac{1}{q} - \\frac{1}{r_{45}} & \\frac{1}{q} + \\frac{1}{r_{45}}\n\\end{pmatrix}\n$$\nThis is the final expression for the posterior precision matrix.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{\\sigma_0^{2}} + \\frac{1}{q} + \\frac{1}{r_1} & -\\frac{1}{q} & 0 & 0 & 0 \\\\\n-\\frac{1}{q} & \\frac{2}{q} & -\\frac{1}{q} & 0 & 0 \\\\\n0 & -\\frac{1}{q} & \\frac{2}{q} + \\frac{1}{r_3} & -\\frac{1}{q} & 0 \\\\\n0 & 0 & -\\frac{1}{q} & \\frac{2}{q} + \\frac{1}{r_{45}} & -\\frac{1}{q} - \\frac{1}{r_{45}} \\\\\n0 & 0 & 0 & -\\frac{1}{q} - \\frac{1}{r_{45}} & \\frac{1}{q} + \\frac{1}{r_{45}}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "Beyond state estimation, the information matrix is a powerful tool for experimental design, helping us decide where to collect data to be most effective. This practice  delves into this application by examining the spectral decomposition of the information matrix, where eigenvalues correspond to the amount of information along different directions in the state space. You will design a targeted observation to reduce uncertainty in the most poorly constrained direction and quantify its precise impact, linking abstract linear algebra to practical decision-making.",
            "id": "3390762",
            "problem": "Consider a linear-Gaussian inverse problem for a state vector $x \\in \\mathbb{R}^{3}$ in a data assimilation setting. After assimilating an initial collection of sensors, the current posterior information (precision) matrix $Y_{\\text{post}}$ has an orthonormal eigenbasis $\\{v_{1}, v_{2}, v_{3}\\}$ with spectral decomposition $Y_{\\text{post}} = U \\,\\mathrm{diag}(\\lambda_{1}, \\lambda_{2}, \\lambda_{3})\\, U^{\\top}$, where $U = [v_{1}\\ v_{2}\\ v_{3}]$ and the eigenvalues are $\\lambda_{1} = 9$, $\\lambda_{2} = 2$, and $\\lambda_{3} = 0.08$. You are considering augmenting the sensing network with one additional scalar sensor producing $y_{\\star}$, modeled by the linear observation operator $H_{\\star}$ and independent Gaussian noise $\\varepsilon_{\\star}$:\n$$\ny_{\\star} = H_{\\star} x + \\varepsilon_{\\star}, \\quad \\varepsilon_{\\star} \\sim \\mathcal{N}(0, R_{\\star}),\n$$\nwhere $H_{\\star} = \\gamma\\, v_{3}^{\\top}$ with $\\gamma = 0.2$ and $R_{\\star} = 0.04$. Starting from first principles of the linear-Gaussian model and the definition of the log-posterior Hessian, derive how the added sensor changes the smallest eigenvalue of the posterior information matrix and compute its exact numerical value. Then, explain qualitatively how this change corresponds to uncertainty reduction along the poorly observed direction $v_{3}$.\n\nYour final answer must be the single numerical value of the new smallest eigenvalue of the posterior information matrix. No rounding is required.",
            "solution": "The problem requires us to determine the effect of a new sensor measurement on the posterior information matrix of a state vector $x \\in \\mathbb{R}^{3}$. We must first derive the update rule for the information matrix, apply it to the given problem, compute the new smallest eigenvalue, and then provide a qualitative interpretation.\n\nThe posterior probability density function (PDF) for the state $x$ after assimilating the new measurement $y_{\\star}$ is given by Bayes' theorem:\n$$\np(x | y_{\\star}, \\text{previous data}) \\propto p(y_{\\star} | x) \\, p(x | \\text{previous data})\n$$\nThe term $p(x | \\text{previous data})$ represents the current posterior belief, which acts as the prior for this update. In a linear-Gaussian framework, this distribution is a Gaussian, $x \\sim \\mathcal{N}(\\mu_{\\text{post}}, P_{\\text{post}})$, where the posterior covariance is $P_{\\text{post}} = Y_{\\text{post}}^{-1}$. The information matrix $Y_{\\text{post}}$ is the Hessian of the negative log-posterior from the previous assimilation steps. The PDF is proportional to $\\exp(-\\frac{1}{2}(x - \\mu_{\\text{post}})^{\\top} Y_{\\text{post}} (x - \\mu_{\\text{post}}))$.\n\nThe term $p(y_{\\star} | x)$ is the likelihood of the new measurement, given by the sensor model $y_{\\star} = H_{\\star}x + \\varepsilon_{\\star}$ with noise $\\varepsilon_{\\star} \\sim \\mathcal{N}(0, R_{\\star})$. This corresponds to the PDF $p(y_{\\star} | x) \\propto \\exp(-\\frac{1}{2}(y_{\\star} - H_{\\star}x)^{\\top} R_{\\star}^{-1} (y_{\\star} - H_{\\star}x))$.\n\nThe new posterior PDF is therefore proportional to the product of these exponential terms. The negative logarithm of the new posterior, denoted as $J_{\\text{new}}(x)$, is (up to an additive constant):\n$$\nJ_{\\text{new}}(x) = \\frac{1}{2}(x - \\mu_{\\text{post}})^{\\top} Y_{\\text{post}} (x - \\mu_{\\text{post}}) + \\frac{1}{2}(y_{\\star} - H_{\\star}x)^{\\top} R_{\\star}^{-1} (y_{\\star} - H_{\\star}x)\n$$\nThe new posterior information matrix, $Y_{\\text{new}}$, is defined as the Hessian of this negative log-posterior, $Y_{\\text{new}} = \\nabla_x^2 J_{\\text{new}}(x)$. To compute the Hessian, we first find the gradient $\\nabla_x J_{\\text{new}}(x)$:\n$$\n\\nabla_x J_{\\text{new}}(x) = Y_{\\text{post}}(x - \\mu_{\\text{post}}) + H_{\\star}^{\\top} R_{\\star}^{-1} (H_{\\star}x - y_{\\star})\n$$\nDifferentiating with respect to $x$ again gives the Hessian:\n$$\n\\nabla_x^2 J_{\\text{new}}(x) = Y_{\\text{post}} + H_{\\star}^{\\top}R_{\\star}^{-1}H_{\\star}\n$$\nThus, the information update rule is:\n$$\nY_{\\text{new}} = Y_{\\text{post}} + H_{\\star}^{\\top}R_{\\star}^{-1}H_{\\star}\n$$\nNow, we apply this rule to the specifics of the problem. We are given the spectral decomposition of the current posterior information matrix:\n$$\nY_{\\text{post}} = \\sum_{i=1}^{3} \\lambda_i v_i v_i^{\\top} = \\lambda_1 v_1 v_1^{\\top} + \\lambda_2 v_2 v_2^{\\top} + \\lambda_3 v_3 v_3^{\\top}\n$$\nwith eigenvalues $\\lambda_1 = 9$, $\\lambda_2 = 2$, and $\\lambda_3 = 0.08$. The eigenvectors $\\{v_1, v_2, v_3\\}$ form an orthonormal basis.\nThe observation model for the new sensor is defined by $H_{\\star} = \\gamma v_3^{\\top}$ with $\\gamma = 0.2$, and the noise variance is $R_{\\star} = 0.04$. Since $R_{\\star}$ is a scalar, its inverse is $R_{\\star}^{-1} = 1/R_{\\star}$.\n\nThe update term is a rank-1 matrix:\n$$\nH_{\\star}^{\\top}R_{\\star}^{-1}H_{\\star} = (\\gamma v_3) (R_{\\star}^{-1}) (\\gamma v_3^{\\top}) = \\frac{\\gamma^2}{R_{\\star}} v_3 v_3^{\\top}\n$$\nSubstituting this into the update rule:\n$$\nY_{\\text{new}} = \\left( \\lambda_1 v_1 v_1^{\\top} + \\lambda_2 v_2 v_2^{\\top} + \\lambda_3 v_3 v_3^{\\top} \\right) + \\frac{\\gamma^2}{R_{\\star}} v_3 v_3^{\\top}\n$$\nWe can collect the terms associated with each eigenvector:\n$$\nY_{\\text{new}} = \\lambda_1 v_1 v_1^{\\top} + \\lambda_2 v_2 v_2^{\\top} + \\left(\\lambda_3 + \\frac{\\gamma^2}{R_{\\star}}\\right) v_3 v_3^{\\top}\n$$\nThis expression is the spectral decomposition of the new information matrix $Y_{\\text{new}}$. The eigenvectors are unchanged: $v_1$, $v_2$, and $v_3$. The new eigenvalues, denoted $\\lambda_i^{\\text{new}}$, are the coefficients of the $v_i v_i^{\\top}$ terms:\n$$\n\\lambda_1^{\\text{new}} = \\lambda_1 = 9\n$$\n$$\n\\lambda_2^{\\text{new}} = \\lambda_2 = 2\n$$\n$$\n\\lambda_3^{\\text{new}} = \\lambda_3 + \\frac{\\gamma^2}{R_{\\star}}\n$$\nNow we compute the numerical value of $\\lambda_3^{\\text{new}}$. We are given $\\lambda_3 = 0.08$, $\\gamma = 0.2$, and $R_{\\star} = 0.04$.\n$$\n\\frac{\\gamma^2}{R_{\\star}} = \\frac{(0.2)^2}{0.04} = \\frac{0.04}{0.04} = 1\n$$\nTherefore, the new third eigenvalue is:\n$$\n\\lambda_3^{\\text{new}} = 0.08 + 1 = 1.08\n$$\nThe set of new eigenvalues is $\\{9, 2, 1.08\\}$. The smallest eigenvalue of the new posterior information matrix is therefore $1.08$.\n\nQualitatively, the eigenvalues of the information matrix are inversely related to the eigenvalues of the covariance matrix ($Y = P^{-1}$). An eigenvalue of the covariance matrix represents the variance (a measure of uncertainty) of the state estimate along the direction of the corresponding eigenvector. A small eigenvalue of the information matrix $\\lambda_i$ thus corresponds to large variance (high uncertainty) in the direction of $v_i$.\nInitially, the smallest eigenvalue was $\\lambda_3 = 0.08$, indicating that the direction $v_3$ was the most poorly observed, i.e., it had the highest posterior uncertainty.\nThe new sensor has an observation operator $H_{\\star} = \\gamma v_3^{\\top}$, which means it measures the component of the state vector $x$ along the direction $v_3$ (since $H_{\\star}x = \\gamma v_3^{\\top}x$). This sensor is specifically designed to gather information about the state in the direction of greatest existing uncertainty.\nThe assimilation of this measurement adds information, specifically along the $v_3$ direction. This is mathematically reflected in the fact that only $\\lambda_3$ is updated, while $\\lambda_1$ and $\\lambda_2$ remain unchanged. The new eigenvalue $\\lambda_3^{\\text{new}} = 1.08$ is significantly larger than the original $\\lambda_3 = 0.08$. This increase in the information eigenvalue corresponds to a significant decrease in the posterior variance (uncertainty) in the $v_3$ direction. The sensor has successfully reduced the uncertainty in the previously most uncertain direction. The smallest eigenvalue of the updated system is now $1.08$, a substantial improvement over the previous $0.08$.",
            "answer": "$$\n\\boxed{1.08}\n$$"
        }
    ]
}