{
    "hands_on_practices": [
        {
            "introduction": "Many advanced optimization algorithms for M-estimation rely on a key subroutine called the proximal operator, which effectively \"denoises\" a value according to the chosen robust penalty function $\\rho$. This exercise provides direct, hands-on practice in deriving and implementing these crucial building blocks for both a convex estimator (Huber) and non-convex estimators (Welsch, Geman-McClure). By doing so, you will gain insight into their mathematical properties and the difference in their computational implementation .",
            "id": "3418049",
            "problem": "Consider the inverse problem in which a data assimilation step is modeled as a robust proximal update on a scalar residual. For a given residual $r \\in \\mathbb{R}$, a positive step size parameter $\\tau \\in \\mathbb{R}_{+}$, and a robust penalty function $\\rho : \\mathbb{R} \\to \\mathbb{R}_{+}$ (an M-estimator), the proximal map is defined as the unique minimizer\n$$\n\\operatorname{prox}_{\\tau \\rho}(r) \\in \\arg\\min_{x \\in \\mathbb{R}} \\left\\{ \\frac{1}{2} (x - r)^2 + \\tau \\, \\rho(x) \\right\\}.\n$$\nThis proximal map is a building block in proximal splitting methods for robust inversion and data assimilation. Starting from the first-order optimality condition for differentiable $\\rho$, namely the vanishing of the derivative of the objective with respect to $x$, derive implementable expressions or algorithms for $\\operatorname{prox}_{\\tau \\rho}(r)$ for the following three M-estimators:\n\n- The Huber penalty with threshold $\\delta \\in \\mathbb{R}_{+}$:\n$$\n\\rho_{\\mathrm{Huber}}(x) = \\begin{cases}\n\\frac{1}{2} x^2, & \\text{if } |x| \\le \\delta, \\\\\n\\delta |x| - \\frac{1}{2} \\delta^2, & \\text{if } |x| > \\delta,\n\\end{cases}\n$$\nwhere $|\\,\\cdot\\,|$ denotes the absolute value.\n\n- The Welsch (Leclerc) penalty with scale $c \\in \\mathbb{R}_{+}$:\n$$\n\\rho_{\\mathrm{Welsch}}(x) = \\frac{c^2}{2}\\left(1 - \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\\right).\n$$\n\n- The Geman–McClure penalty with scale $c \\in \\mathbb{R}_{+}$:\n$$\n\\rho_{\\mathrm{GM}}(x) = \\frac{1}{2} \\frac{x^2}{1 + \\left(\\frac{x}{c}\\right)^2}.\n$$\n\nYour program must implement three proximal operators $x = \\operatorname{prox}_{\\tau \\rho}(r)$, one for each $\\rho$ above, by invoking the first-order optimality condition. Where a closed form exists, use it to obtain an exact, piecewise-defined expression. Where no elementary closed form exists, derive a well-posed scalar root-finding scheme based on the optimality equation, and implement a robust numerical solver (with convergence tolerance and safeguards) that reliably returns the minimizer for any input in the provided test suite. The derivation and implementation must be universal and purely mathematical; no physical units are involved.\n\nTest Suite. Use the following parameter values and residuals to exercise general behavior, boundary conditions, and edge cases:\n\n- Huber penalty with $\\delta = 1.25$ and $\\tau = 0.75$ evaluated at residuals $r \\in \\{-3.00, -\\delta(1+\\tau), 0.00, \\delta(1+\\tau), 2.70\\}$, i.e., $r \\in \\{-3.00, -1.25(1+0.75), 0.00, 1.25(1+0.75), 2.70\\}$.\n\n- Welsch penalty with $c = 1.00$ and $\\tau = 0.50$ evaluated at residuals $r \\in \\{-2.00, -1.00, 0.00, 1.00, 2.00\\}$.\n\n- Geman–McClure penalty with $c = 1.50$ and $\\tau = 0.80$ evaluated at residuals $r \\in \\{-3.00, -1.50, 0.00, 1.50, 3.00\\}$.\n\nFinal Output Format. Your program should produce a single line of output containing the proximal values, ordered as listed above (first all Huber results in the specified $r$ order, then all Welsch results, then all Geman–McClure results), formatted as a comma-separated list enclosed in square brackets, for example $[x_1,x_2,\\dots,x_{15}]$. Each entry $x_i$ must be a real number (a float). No additional text should be printed.",
            "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the established mathematical theory of proximal calculus and robust statistics. It is well-posed, providing all necessary definitions, parameters, and constraints to derive and implement the required proximal operators. The problem is expressed in objective, formal language, with no ambiguities or subjective claims.\n\nThe task is to find the minimizer $x^\\star$ of the objective function $J(x)$:\n$$\nx^\\star = \\operatorname{prox}_{\\tau \\rho}(r) = \\arg\\min_{x \\in \\mathbb{R}} \\left\\{ J(x) = \\frac{1}{2} (x - r)^2 + \\tau \\, \\rho(x) \\right\\}\n$$\nfor a given residual $r$, step size $\\tau > 0$, and a robust penalty function $\\rho(x)$. Assuming $\\rho(x)$ is differentiable at the minimizer, the first-order optimality condition is that the derivative of the objective function $J'(x)$ must be zero:\n$$\nJ'(x) = \\frac{d}{dx} \\left( \\frac{1}{2} (x - r)^2 + \\tau \\, \\rho(x) \\right) = (x - r) + \\tau \\, \\rho'(x) = 0\n$$\nThis optimality equation, $x + \\tau \\rho'(x) = r$, forms the basis for deriving the proximal operators for each specified M-estimator.\n\n### 1. Huber Penalty\n\nThe Huber penalty function, parameterized by a threshold $\\delta > 0$, is given by:\n$$\n\\rho_{\\mathrm{Huber}}(x) = \\begin{cases}\n\\frac{1}{2} x^2, & \\text{if } |x| \\le \\delta, \\\\\n\\delta |x| - \\frac{1}{2} \\delta^2, & \\text{if } |x| > \\delta.\n\\end{cases}\n$$\nThis function is convex and continuously differentiable. Its derivative, $\\psi_{\\mathrm{Huber}}(x) = \\rho'_{\\mathrm{Huber}}(x)$, is:\n$$\n\\psi_{\\mathrm{Huber}}(x) = \\begin{cases}\nx, & \\text{if } |x| \\le \\delta, \\\\\n\\delta \\operatorname{sgn}(x), & \\text{if } |x| > \\delta.\n\\end{cases}\n$$\nThe optimality condition $x - r + \\tau \\psi_{\\mathrm{Huber}}(x) = 0$ is analyzed in three cases based on the value of $x$.\n\nCase 1: $|x| \\le \\delta$. The optimality condition is $x - r + \\tau x = 0$, which yields $(1+\\tau)x = r$, or $x = \\frac{r}{1+\\tau}$. This solution is self-consistent if $|x| \\le \\delta$, which implies $|\\frac{r}{1+\\tau}| \\le \\delta$, or $|r| \\le \\delta(1+\\tau)$.\n\nCase 2: $x > \\delta$. The optimality condition is $x - r + \\tau \\delta = 0$, which yields $x = r - \\tau\\delta$. This is consistent if $x > \\delta$, meaning $r - \\tau\\delta > \\delta$, or $r > \\delta(1+\\tau)$.\n\nCase 3: $x < -\\delta$. The optimality condition is $x - r - \\tau \\delta = 0$, which yields $x = r + \\tau\\delta$. This is consistent if $x < -\\delta$, meaning $r + \\tau\\delta < -\\delta$, or $r < -\\delta(1+\\tau)$.\n\nCombining these mutually exclusive cases provides the complete closed-form solution for the proximal operator of the Huber penalty:\n$$\n\\operatorname{prox}_{\\tau \\rho_{\\mathrm{Huber}}}(r) = \\begin{cases}\nr + \\tau\\delta, & \\text{if } r < -\\delta(1+\\tau) \\\\\n\\frac{r}{1+\\tau}, & \\text{if } |r| \\le \\delta(1+\\tau) \\\\\nr - \\tau\\delta, & \\text{if } r > \\delta(1+\\tau)\n\\end{cases}\n$$\nThis expression is exact and will be implemented directly.\n\n### 2. Welsch (Leclerc) Penalty\n\nThe Welsch penalty, with scale parameter $c > 0$, is defined as:\n$$\n\\rho_{\\mathrm{Welsch}}(x) = \\frac{c^2}{2}\\left(1 - \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\\right)\n$$\nThis function is smooth but non-convex. Its derivative is:\n$$\n\\psi_{\\mathrm{Welsch}}(x) = \\rho'_{\\mathrm{Welsch}}(x) = x \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\n$$\nThe first-order optimality condition is $x - r + \\tau \\psi_{\\mathrm{Welsch}}(x) = 0$, which leads to the nonlinear equation:\n$$\nx \\left(1 + \\tau \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\\right) = r\n$$\nThis equation does not have an elementary closed-form solution for $x$. We must find the root of the function $g(x) = x \\left(1 + \\tau \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\\right) - r$. Note that $x=0$ is the solution if and only if $r=0$. The function $h(x) = x \\left(1 + \\tau \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\\right)$ is odd, so $\\operatorname{prox}(-r) = -\\operatorname{prox}(r)$. We can solve for $r>0$ and extend the solution. For the provided test case parameters ($\\tau=0.5$), the function $h(x)$ is strictly monotonic, guaranteeing a unique solution for any $r$.\n\nWe employ the Newton-Raphson method to solve $g(x) = 0$. The iterative update is $x_{k+1} = x_k - g(x_k)/g'(x_k)$. The derivative $g'(x)$ is:\n$$\ng'(x) = \\frac{d}{dx} \\left[ x \\left(1 + \\tau \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right)\\right) - r \\right] = 1 + \\tau \\exp\\!\\left(-\\left(\\frac{x}{c}\\right)^2\\right) \\left(1 - \\frac{2x^2}{c^2}\\right)\n$$\nA good initial guess is $x_0 = r$, as for large $|x|$, the exponential term vanishes and the equation approximates to $x=r$. The algorithm proceeds by iterating until the change in $x$ is below a specified tolerance.\n\n### 3. Geman–McClure Penalty\n\nThe Geman–McClure penalty, with scale parameter $c > 0$, is:\n$$\n\\rho_{\\mathrm{GM}}(x) = \\frac{1}{2} \\frac{x^2}{1 + \\left(\\frac{x}{c}\\right)^2} = \\frac{c^2}{2} \\frac{x^2}{c^2 + x^2}\n$$\nThis function is also smooth and non-convex. Its derivative is:\n$$\n\\psi_{\\mathrm{GM}}(x) = \\rho'_{\\mathrm{GM}}(x) = \\frac{c^4 x}{(c^2+x^2)^2}\n$$\nThe optimality condition $x - r + \\tau \\psi_{\\mathrm{GM}}(x) = 0$ gives the nonlinear equation:\n$$\nx \\left(1 + \\frac{\\tau c^4}{(c^2+x^2)^2}\\right) = r\n$$\nAs with the Welsch penalty, this requires a numerical solution. We define a function $g(x) = x \\left(1 + \\frac{\\tau c^4}{(c^2+x^2)^2}\\right) - r$ and find its root. The properties are similar to the Welsch case: $x=0$ is the solution for $r=0$, the mapping is odd, and for the given test parameters ($\\tau=0.8$), a unique solution exists.\n\nWe again use the Newton-Raphson method. The derivative $g'(x)$ is:\n$$\ng'(x) = \\frac{d}{dx} \\left[ x \\left(1 + \\frac{\\tau c^4}{(c^2+x^2)^2}\\right) - r \\right] = 1 + \\frac{\\tau c^4 (c^2 - 3x^2)}{(c^2+x^2)^3}\n$$\nThe initial guess is again taken as $x_0=r$. The numerical implementation will find the root of $g(x)$ using this iterative scheme.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef prox_huber(r, tau, delta):\n    \"\"\"\n    Computes the proximal operator for the Huber penalty.\n    \n    prox(r) = argmin_x { 0.5 * (x - r)^2 + tau * rho_huber(x) }\n    \n    This has a closed-form solution.\n    \"\"\"\n    threshold = delta * (1.0 + tau)\n    if r < -threshold:\n        return r + tau * delta\n    elif r > threshold:\n        return r - tau * delta\n    else:\n        return r / (1.0 + tau)\n\ndef prox_welsch(r, tau, c, tol=1e-12, max_iter=100):\n    \"\"\"\n    Computes the proximal operator for the Welsch penalty using Newton's method.\n    \n    prox(r) = argmin_x { 0.5 * (x - r)^2 + tau * rho_welsch(x) }\n    \n    The optimality condition is g(x) = x * (1 + tau*exp(-(x/c)^2)) - r = 0.\n    \"\"\"\n    if r == 0.0:\n        return 0.0\n\n    # The function is odd, so we can solve for abs(r) and apply the sign.\n    r_abs = abs(r)\n    sign = np.sign(r)\n\n    # Initial guess x0 = r. Since we solve for r_abs, x0 = r_abs.\n    x = r_abs\n    c_sq = c * c\n\n    for _ in range(max_iter):\n        x_sq_over_c_sq = (x / c)**2\n        exp_term = np.exp(-x_sq_over_c_sq)\n        \n        # g(x) = x * (1 + tau * exp_term) - r_abs\n        g_x = x * (1.0 + tau * exp_term) - r_abs\n        \n        if abs(g_x) < tol:\n            return x * sign\n\n        # g'(x) = 1 + tau * exp_term * (1 - 2 * (x/c)^2)\n        gp_x = 1.0 + tau * exp_term * (1.0 - 2.0 * x_sq_over_c_sq)\n        \n        # Newton-Raphson update step\n        # Safeguard: gp_x is positive for the problem's parameters.\n        if gp_x == 0.0:\n            break\n        x = x - g_x / gp_x\n\n    return x * sign\n\ndef prox_geman_mcclure(r, tau, c, tol=1e-12, max_iter=100):\n    \"\"\"\n    Computes the proximal operator for the Geman-McClure penalty using Newton's method.\n    \n    prox(r) = argmin_x { 0.5 * (x - r)^2 + tau * rho_gm(x) }\n    \n    The optimality condition is g(x) = x * (1 + tau*c^4 / (c^2+x^2)^2) - r = 0.\n    \"\"\"\n    if r == 0.0:\n        return 0.0\n\n    r_abs = abs(r)\n    sign = np.sign(r)\n    \n    # Initial guess\n    x = r_abs\n    c_sq = c * c\n    tau_c4 = tau * c * c * c * c\n\n    for _ in range(max_iter):\n        x_sq = x * x\n        denom = c_sq + x_sq\n        denom_sq = denom * denom\n        \n        # g(x) = x * (1 + tau*c^4 / (c^2+x^2)^2) - r_abs\n        g_x = x * (1.0 + tau_c4 / denom_sq) - r_abs\n        \n        if abs(g_x) < tol:\n            return x * sign\n        \n        # g'(x) = 1 + tau*c^4 * (c^2 - 3*x^2) / (c^2+x^2)^3\n        denom_cub = denom * denom_sq\n        gp_x = 1.0 + tau_c4 * (c_sq - 3.0 * x_sq) / denom_cub\n\n        # Safeguard: gp_x is positive for the problem's parameters.\n        if gp_x == 0.0:\n            break\n            \n        x = x - g_x / gp_x\n\n    return x * sign\n\ndef solve():\n    \"\"\"\n    Solves the problem by running the test suite for each M-estimator.\n    \"\"\"\n    results = []\n    \n    # Huber penalty test cases\n    delta_huber = 1.25\n    tau_huber = 0.75\n    r_huber_vals = [-3.00, -delta_huber * (1.0 + tau_huber), 0.00, delta_huber * (1.0 + tau_huber), 2.70]\n    for r in r_huber_vals:\n        results.append(prox_huber(r, tau_huber, delta_huber))\n        \n    # Welsch penalty test cases\n    c_welsch = 1.00\n    tau_welsch = 0.50\n    r_welsch_vals = [-2.00, -1.00, 0.00, 1.00, 2.00]\n    for r in r_welsch_vals:\n        results.append(prox_welsch(r, tau_welsch, c_welsch))\n        \n    # Geman-McClure penalty test cases\n    c_gm = 1.50\n    tau_gm = 0.80\n    r_gm_vals = [-3.00, -1.50, 0.00, 1.50, 3.00]\n    for r in r_gm_vals:\n        results.append(prox_geman_mcclure(r, tau_gm, c_gm))\n        \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "With the concept of the proximal operator in hand, we can now construct a complete algorithm for solving a realistic inverse problem. This exercise demonstrates how the Alternating Direction Method of Multipliers (ADMM) uses a \"divide and conquer\" strategy to solve a problem combining a robust M-estimator data term and quadratic regularization . You will derive the ADMM update steps, seeing how a complex, non-smooth optimization is broken down into a standard least-squares problem and a proximal map evaluation, connecting directly to the skills developed in the previous practice.",
            "id": "3418091",
            "problem": "Consider a linear inverse problem in data assimilation where the state vector $x \\in \\mathbb{R}^{n}$ is inferred from noisy observations $y \\in \\mathbb{R}^{m}$ through a known linear observation operator $A \\in \\mathbb{R}^{m \\times n}$. To promote robustness against outliers in the observation errors, the data misfit is modeled by a robust loss function from the class of maximum-likelihood type estimators (M-estimators), denoted by a convex, proper, lower semicontinuous function $\\rho:\\mathbb{R} \\to \\mathbb{R}$ applied elementwise. A quadratic regularization is enforced through a linear regularization operator $L \\in \\mathbb{R}^{p \\times n}$ with strength $\\lambda > 0$. The robust formulation reads\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\sum_{i=1}^{m} \\rho\\!\\left((A x - y)_{i}\\right) + \\lambda \\|L x\\|_{2}^{2}.\n$$\nIntroduce a splitting variable $z \\in \\mathbb{R}^{m}$ to represent the residual, constrained by $z = A x - y$. The Alternating Direction Method of Multipliers (ADMM) is to be applied to the constrained reformulation. Use the scaled-dual ADMM with penalty parameter $\\mu > 0$ and scaled dual variable $u \\in \\mathbb{R}^{m}$ associated with the constraint $A x - y - z = 0$.\n\nStarting from the core definitions of the augmented Lagrangian for equality-constrained convex optimization and the proximal operator of a proper, convex, lower semicontinuous function, derive the explicit ADMM iteration. Your derivation must:\n- Set up the scaled augmented Lagrangian corresponding to the constraint $A x - y - z = 0$.\n- Obtain the $x$-update by minimizing the augmented Lagrangian with respect to $x$, expressed as a closed-form solution of a linear system.\n- Obtain the $z$-update by minimizing with respect to $z$ and express it using the proximal operator of $\\rho$.\n- State the scaled dual variable update.\n\nExpress the three updates in terms of $A$, $L$, $y$, $\\lambda$, $\\mu$, and the current iterates $(x^{k}, z^{k}, u^{k})$. Your final answer must be a single closed-form analytic expression that compactly lists the right-hand sides of the $x$-, $z$-, and scaled-dual updates as a row vector. No numerical approximation is required, and no units are involved. Do not include any equations or inequalities in your final boxed answer; present only the expressions for the updates as requested.",
            "solution": "We begin from the equality-constrained reformulation of the original robust objective by introducing the auxiliary variable $z \\in \\mathbb{R}^{m}$ to represent the observation residual. The problem becomes\n$$\n\\min_{x \\in \\mathbb{R}^{n},\\, z \\in \\mathbb{R}^{m}} \\left[ \\sum_{i=1}^{m} \\rho(z_{i}) + \\lambda \\|L x\\|_{2}^{2} \\right] \\quad \\text{subject to} \\quad A x - y - z = 0.\n$$\nDefine the functions $f(x) = \\lambda \\|L x\\|_{2}^{2}$ and $g(z) = \\sum_{i=1}^{m} \\rho(z_{i})$. The Alternating Direction Method of Multipliers (ADMM) in its scaled-dual form introduces a scaled dual variable $u \\in \\mathbb{R}^{m}$ associated with the linear constraint, and uses an augmented Lagrangian penalty parameter $\\mu > 0$. The scaled augmented Lagrangian is constructed using well-tested convex optimization principles:\n$$\n\\mathcal{L}_{\\mu}^{\\text{scaled}}(x, z, u) = f(x) + g(z) + \\frac{\\mu}{2} \\left\\|A x - y - z + u \\right\\|_{2}^{2} - \\frac{\\mu}{2} \\|u\\|_{2}^{2}.\n$$\nThe ADMM iteration alternates between minimizing $\\mathcal{L}_{\\mu}^{\\text{scaled}}$ with respect to $x$ and $z$, followed by a scaled dual variable update.\n\nFor the $x$-update, we minimize with respect to $x$ while keeping $(z^{k}, u^{k})$ fixed:\n$$\nx^{k+1} \\in \\arg\\min_{x} \\left[ \\lambda \\|L x\\|_{2}^{2} + \\frac{\\mu}{2} \\left\\|A x - y - z^{k} + u^{k} \\right\\|_{2}^{2} \\right].\n$$\nBoth terms are convex and differentiable in $x$. The first-order optimality condition (obtained by setting the gradient to zero) yields\n$$\n\\nabla_{x} \\left[ \\lambda \\|L x\\|_{2}^{2} \\right] + \\nabla_{x} \\left[ \\frac{\\mu}{2} \\left\\|A x - y - z^{k} + u^{k} \\right\\|_{2}^{2} \\right] = 0,\n$$\nwhich simplifies to\n$$\n2 \\lambda L^{\\top} L x + \\mu A^{\\top} \\left( A x - y - z^{k} + u^{k} \\right) = 0.\n$$\nCollecting terms in $x$, we obtain the normal equations\n$$\n\\left( \\mu A^{\\top} A + 2 \\lambda L^{\\top} L \\right) x = \\mu A^{\\top} \\left( y + z^{k} - u^{k} \\right).\n$$\nAssuming the matrix $\\mu A^{\\top} A + 2 \\lambda L^{\\top} L$ is invertible, the $x$-update is given in closed form by\n$$\nx^{k+1} = \\left( \\mu A^{\\top} A + 2 \\lambda L^{\\top} L \\right)^{-1} \\mu A^{\\top} \\left( y + z^{k} - u^{k} \\right).\n$$\n\nFor the $z$-update, we minimize with respect to $z$ while keeping $(x^{k+1}, u^{k})$ fixed:\n$$\nz^{k+1} \\in \\arg\\min_{z} \\left[ \\sum_{i=1}^{m} \\rho(z_{i}) + \\frac{\\mu}{2} \\left\\|A x^{k+1} - y - z + u^{k} \\right\\|_{2}^{2} \\right].\n$$\nDenote $v^{k} = A x^{k+1} - y + u^{k} \\in \\mathbb{R}^{m}$. The problem decouples across coordinates because $g(z) = \\sum_{i=1}^{m} \\rho(z_{i})$ is separable and the quadratic penalty is separable. By the definition of the proximal operator of a proper, convex, lower semicontinuous function $h:\\mathbb{R}^{m} \\to \\mathbb{R}$,\n$$\n\\operatorname{prox}_{\\tau h}(v) := \\arg\\min_{z} \\left\\{ h(z) + \\frac{1}{2 \\tau} \\|z - v\\|_{2}^{2} \\right\\},\n$$\nwe can rewrite the $z$-update as a proximal step with step size $\\tau = \\frac{1}{\\mu}$:\n$$\nz^{k+1} = \\operatorname{prox}_{\\frac{1}{\\mu} g}\\!\\left( v^{k} \\right) = \\operatorname{prox}_{\\frac{1}{\\mu} \\sum_{i=1}^{m} \\rho}\\!\\left( A x^{k+1} - y + u^{k} \\right).\n$$\nBecause $g$ is separable, this proximal operator acts elementwise and is equivalently expressed using the scalar proximal $\\operatorname{prox}_{\\frac{1}{\\mu}\\rho}$ applied coordinatewise:\n$$\nz^{k+1} = \\left[ \\operatorname{prox}_{\\frac{1}{\\mu}\\rho}\\!\\left( (A x^{k+1} - y + u^{k})_{i} \\right) \\right]_{i=1}^{m}.\n$$\n\nFinally, the scaled dual variable update for $u$ follows directly from the standard scaled ADMM definition for the equality constraint $A x - y - z = 0$:\n$$\nu^{k+1} = u^{k} + \\left( A x^{k+1} - y - z^{k+1} \\right).\n$$\n\nCollecting the three updates, we obtain the ADMM iteration written compactly in terms of the current iterates $(x^{k}, z^{k}, u^{k})$, the operators $A$, $L$, the data $y$, and the parameters $(\\lambda, \\mu)$:\n- $x$-update:\n$$\nx^{k+1} = \\left( \\mu A^{\\top} A + 2 \\lambda L^{\\top} L \\right)^{-1} \\mu A^{\\top} \\left( y + z^{k} - u^{k} \\right),\n$$\n- $z$-update:\n$$\nz^{k+1} = \\operatorname{prox}_{\\frac{1}{\\mu}\\rho}\\!\\left( A x^{k+1} - y + u^{k} \\right),\n$$\n- scaled dual update:\n$$\nu^{k+1} = u^{k} + A x^{k+1} - y - z^{k+1}.\n$$\nThese formulas complete the derivation of ADMM for the robust M-estimator data misfit with quadratic regularization under the splitting $z = A x - y$.",
            "answer": "$$\\boxed{\\begin{pmatrix}\n\\left( \\mu A^{\\top} A + 2 \\lambda L^{\\top} L \\right)^{-1} \\mu A^{\\top} \\left( y + z^{k} - u^{k} \\right) & \\operatorname{prox}_{\\frac{1}{\\mu}\\rho}\\!\\left( A x^{k+1} - y + u^{k} \\right) & u^{k} + A x^{k+1} - y - z^{k+1}\n\\end{pmatrix}}$$"
        },
        {
            "introduction": "While non-convex, redescending M-estimators offer the attractive property of completely rejecting gross outliers, this power comes at a cost: the introduction of a non-convex objective function that may have multiple local minima. This practice explores this critical pitfall through a guided numerical experiment where you will visualize the objective function landscape . You will see how a cluster of outliers can create a \"spurious\" basin of attraction, potentially trapping an optimization algorithm and leading to a loss of parameter identifiability.",
            "id": "3418043",
            "problem": "Consider a one-dimensional inverse problem with a nonlinear forward model, where the observation operator is the hyperbolic tangent $F(x) = \\tanh(x)$. Let the data consist of $N_{\\mathrm{in}}$ inliers generated from a true parameter $x^\\star$ without noise, so that each inlier observation equals $y_i = F(x^\\star)$, and $N_{\\mathrm{out}}$ adversarially placed contaminated points, each equaling $y_j = F(x^{\\dagger})$ for some adversarial parameter $x^{\\dagger}$ selected by a contamination process. Assume a robust $M$-estimator (maximum likelihood type estimator) with an $M$-estimation loss $\\rho$ that is continuously differentiable and redescending (its influence function goes to zero for large residuals). Let the objective be the sum of losses,\n$$\nJ(x) = \\sum_{k=1}^{N} \\rho\\!\\left(\\frac{y_k - F(x)}{s}\\right) s^2,\n$$\nwhere $N = N_{\\mathrm{in}} + N_{\\mathrm{out}}$, $s > 0$ is a fixed robust scale, and $\\rho$ is Tukey’s bisquare (also known as Tukey’s biweight), defined for a standardized residual $t$ and threshold $c > 0$ by\n$$\n\\rho(t) =\n\\begin{cases}\n\\dfrac{c^2}{6}\\left[1 - \\left(1 - \\left(\\dfrac{t}{c}\\right)^2\\right)^3\\right], & \\text{if } |t| \\le c, \\\\\n\\dfrac{c^2}{6}, & \\text{if } |t| > c.\n\\end{cases}\n$$\nYou will investigate when a redescending loss produces spurious local minima and when identifiability is lost. The fundamental starting points you should use are (i) the definition of the $M$-estimator as a minimizer of the sum of a prescribed loss of residuals, (ii) the definition and properties of redescending losses, and (iii) the deterministic forward mapping $F(x) = \\tanh(x)$ for the observation operator.\n\nTasks:\n1. Construct a counterexample dataset where a redescending $\\rho$ produces spurious local minima. Use the forward model $F(x) = \\tanh(x)$, an $M$-estimator objective $J(x)$ as above, with fixed scale $s$ and Tukey’s bisquare with threshold $c$. Consider a contamination scheme where $N_{\\mathrm{in}}$ inliers equal $F(x^\\star)$ and $N_{\\mathrm{out}}$ outliers equal $F(x^{\\dagger})$. Explain why, for sufficiently separated $F(x^\\star)$ and $F(x^{\\dagger})$ relative to the threshold $c s$, the objective $J(x)$ can develop at least two local minima near $x^\\star$ and near $x^{\\dagger}$. Your program must detect local minima by evaluating $J(x)$ on a uniform grid and counting points that are strict local minima, and it must report the approximate global minimizer.\n2. Characterize conditions on $F$ and the contamination pattern under which identifiability is lost. Starting from the definitions, argue in terms of the separation $|F(x^\\star) - F(x^{\\dagger})|$ versus the cutoff $c s$, and the sample sizes $N_{\\mathrm{in}}$ and $N_{\\mathrm{out}}$, when the global minimizer of $J(x)$ will concentrate near $x^\\star$ or near $x^{\\dagger}$. Your program must calculate and report a boolean that indicates whether the separation condition $|F(x^\\star) - F(x^{\\dagger})| > c s$ holds, and a boolean that indicates whether identifiability is lost as judged by the location of the global minimizer relative to $x^\\star$.\n\nNumerical instructions:\n- Use a search domain $x \\in [-6, 6]$ sampled on a uniform grid sufficiently fine to detect distinct minima.\n- Implement $F(x) = \\tanh(x)$ exactly.\n- Implement Tukey’s bisquare $\\rho$ as given above, applied to standardized residuals $(y - F(x))/s$, and multiply by $s^2$ inside the sum to put the objective in the original residual units.\n- The detection of local minima should be based on strict inequalities $J(x_{i-1}) > J(x_i) < J(x_{i+1})$ on the grid.\n- Determine the approximate global minimizer $\\hat{x}$ as the grid point with the smallest objective value. Use a tolerance $\\tau = 0.5$ to judge proximity: identifiability is considered lost if $|\\hat{x} - x^\\star| > \\tau$.\n\nTest suite:\nEvaluate the following four cases. In each case, use Tukey’s threshold $c = 4.685$, and report results in the order and types specified below.\n- Case 1 (happy path, no contamination): $x^\\star = 3.0$, $x^{\\dagger} = -3.0$, $N_{\\mathrm{in}} = 30$, $N_{\\mathrm{out}} = 0$, $s = 0.1$.\n- Case 2 (spurious local minima but correct global minimizer): $x^\\star = 3.0$, $x^{\\dagger} = -3.0$, $N_{\\mathrm{in}} = 30$, $N_{\\mathrm{out}} = 5$, $s = 0.1$.\n- Case 3 (identifiability lost under heavy contamination): $x^\\star = 3.0$, $x^{\\dagger} = -3.0$, $N_{\\mathrm{in}} = 10$, $N_{\\mathrm{out}} = 25$, $s = 0.1$.\n- Case 4 (boundary regime with reduced separation relative to cutoff): $x^\\star = 3.0$, $x^{\\dagger} = -3.0$, $N_{\\mathrm{in}} = 20$, $N_{\\mathrm{out}} = 20$, $s = 0.5$.\n\nRequired outputs:\nFor each case, your program must compute and collect the following four quantities in order:\n- The approximate global minimizer $\\hat{x}$ as a float, rounded to three decimals.\n- The number of strict local minima on the grid as an integer.\n- A boolean indicating whether identifiability is lost, defined by $|\\hat{x} - x^\\star| > \\tau$ with $\\tau = 0.5$.\n- A boolean indicating whether the separation condition $|F(x^\\star) - F(x^{\\dagger})| > c s$ holds.\n\nFinal output format:\nYour program should produce a single line of output containing the results aggregated for the four cases, in a comma-separated list enclosed in square brackets, in the following order:\n$$\n[\\hat{x}_1, \\text{nmin}_1, \\text{lost}_1, \\text{sep}_1, \\hat{x}_2, \\text{nmin}_2, \\text{lost}_2, \\text{sep}_2, \\hat{x}_3, \\text{nmin}_3, \\text{lost}_3, \\text{sep}_3, \\hat{x}_4, \\text{nmin}_4, \\text{lost}_4, \\text{sep}_4].\n$$\nNo additional text should be printed. All angles are dimensionless, and there are no physical units in this problem. Floats must be rounded to three decimals as specified; booleans must be literal logical values.",
            "solution": "The validity of the problem statement is confirmed. It is a well-posed, scientifically grounded problem in the field of robust statistics and inverse problems. All necessary definitions, parameters, and numerical instructions are provided, and there are no contradictions or ambiguities.\n\nThe problem investigates the behavior of a robust $M$-estimator for a one-dimensional inverse problem with a nonlinear forward model $F(x) = \\tanh(x)$. The core of the analysis lies in the properties of the objective function $J(x)$, which is constructed using a redescending loss function, specifically Tukey's bisquare loss, $\\rho(t)$.\n\nThe objective function to be minimized is given by:\n$$\nJ(x) = \\sum_{k=1}^{N} \\rho\\!\\left(\\frac{y_k - F(x)}{s}\\right) s^2\n$$\nwhere $y_k$ are the observations, $F(x) = \\tanh(x)$ is the forward model, and $s > 0$ is a fixed scale parameter. The sum is over all $N = N_{\\mathrm{in}} + N_{\\mathrm{out}}$ data points, comprising $N_{\\mathrm{in}}$ inliers with value $y_i = F(x^\\star)$ and $N_{\\mathrm{out}}$ outliers with value $y_j = F(x^{\\dagger})$.\n\nThe Tukey bisquare loss function $\\rho(t)$ for a standardized residual $t$ is defined with a tuning constant $c > 0$ as:\n$$\n\\rho(t) =\n\\begin{cases}\n\\dfrac{c^2}{6}\\left[1 - \\left(1 - \\left(\\dfrac{t}{c}\\right)^2\\right)^3\\right], & \\text{if } |t| \\le c, \\\\\n\\dfrac{c^2}{6}, & \\text{if } |t| > c.\n\\end{cases}\n$$\nA key feature of this loss function is that it is \"redescending.\" This means its corresponding influence function, $\\psi(t) = \\rho'(t)$, is zero for large residuals. Specifically, for $|t| > c$, the loss $\\rho(t)$ becomes constant at its maximum value, $\\frac{c^2}{6}$. Consequently, its derivative $\\psi(t)$ is zero for $|t| > c$. This property allows the estimator to completely reject observations that are sufficiently far from the model prediction. An observation $y_k$ is rejected when its standardized residual $t_k = (y_k - F(x))/s$ satisfies $|t_k| > c$, or equivalently, when the absolute residual $|y_k - F(x)|$ exceeds the cutoff distance $cs$.\n\n**1. Emergence of Spurious Local Minima**\n\nSpurious local minima arise directly from the redescending nature of the loss function when the data is clustered into well-separated groups. Let's analyze the objective function $J(x)$ for a candidate solution $x$ that is close to the true parameter $x^\\star$.\n\nThe data consists of two groups: inliers at $y^\\star = F(x^\\star)$ and outliers at $y^{\\dagger} = F(x^{\\dagger})$.\nWhen the candidate solution $x$ is near $x^\\star$, the residuals for the inliers, $|y^\\star - F(x)|$, are small. In contrast, the residuals for the outliers are approximately $|y^{\\dagger} - F(x)| \\approx |F(x^{\\dagger}) - F(x^\\star)|$.\n\nIf the separation between the inlier and outlier observations is greater than the cutoff distance, i.e., if the condition $|F(x^\\star) - F(x^{\\dagger})| > cs$ holds, then for any $x$ sufficiently close to $x^\\star$, the residuals corresponding to the outliers will satisfy $|F(x^{\\dagger}) - F(x)| > cs$. For these outliers, the loss function $\\rho$ saturates to its constant maximum value, $\\frac{c^2}{6}$.\n\nUnder this separation condition, the objective function in a neighborhood of $x^\\star$ can be approximated as:\n$$\nJ(x) \\approx \\sum_{i=1}^{N_{\\mathrm{in}}} \\rho\\left(\\frac{F(x^\\star) - F(x)}{s}\\right)s^2 + \\sum_{j=1}^{N_{\\mathrm{out}}} \\left(\\frac{c^2}{6}\\right)s^2\n$$\nThe second term is a constant. The first term is minimized when $F(x)$ is closest to $F(x^\\star)$, i.e., at $x = x^\\star$. Thus, a local minimum of $J(x)$ exists near $x^\\star$.\n\nBy a symmetric argument, if we consider a candidate solution $x$ near the adversarial parameter $x^{\\dagger}$, the inliers now appear as outliers. If the separation condition $|F(x^\\star) - F(x^{\\dagger})| > cs$ holds, then for $x$ close to $x^{\\dagger}$, the residuals for the inliers will satisfy $|F(x^\\star) - F(x)| > cs$. These inliers are now rejected. The objective function in a neighborhood of $x^{\\dagger}$ is approximately:\n$$\nJ(x) \\approx \\sum_{i=1}^{N_{\\mathrm{in}}} \\left(\\frac{c^2}{6}\\right)s^2 + \\sum_{j=1}^{N_{\\mathrm{out}}} \\rho\\left(\\frac{F(x^{\\dagger}) - F(x)}{s}\\right)s^2\n$$\nThis creates a second local minimum near $x = x^{\\dagger}$. Therefore, when the data groups are separated by more than the robust cutoff distance $cs$, the objective function develops at least two local minima, one corresponding to each data cluster. This is the mechanism for the creation of spurious minima.\n\n**2. Loss of Identifiability**\n\nIdentifiability is the ability to uniquely determine the true parameter $x^\\star$ from the data. In this context, we consider identifiability to be lost if the global minimizer of $J(x)$, denoted $\\hat{x}$, is located far from $x^\\star$ and instead converges to the adversarial location $x^{\\dagger}$.\n\nAssuming the separation condition $|F(x^\\star) - F(x^{\\dagger})| > cs$ holds, we have established the existence of local minima near $x^\\star$ and $x^{\\dagger}$. To determine which is the global minimum, we compare the values of the objective function at these two points.\n\nThe value of the objective function at the local minimum near $x^\\star$ is approximately:\n$$\nJ(x^\\star) = \\sum_{i=1}^{N_{\\mathrm{in}}} \\rho(0)s^2 + \\sum_{j=1}^{N_{\\mathrm{out}}} \\rho\\left(\\frac{F(x^{\\dagger}) - F(x^\\star)}{s}\\right)s^2 \\approx 0 + N_{\\mathrm{out}} \\left(\\frac{c^2}{6}\\right)s^2\n$$\nsince $\\rho(0)=0$ and the residuals for the outliers exceed the cutoff.\n\nSimilarly, the value at the local minimum near $x^{\\dagger}$ is approximately:\n$$\nJ(x^{\\dagger}) = \\sum_{i=1}^{N_{\\mathrm{in}}} \\rho\\left(\\frac{F(x^\\star) - F(x^{\\dagger})}{s}\\right)s^2 + \\sum_{j=1}^{N_{\\mathrm{out}}} \\rho(0)s^2 \\approx N_{\\mathrm{in}} \\left(\\frac{c^2}{6}\\right)s^2 + 0\n$$\n\nThe global minimizer $\\hat{x}$ will be near $x^\\star$ if $J(x^\\star) < J(x^{\\dagger})$, which implies $N_{\\mathrm{out}} < N_{\\mathrm{in}}$.\nConversely, the global minimizer will be near $x^{\\dagger}$ if $J(x^\\star) > J(x^{\\dagger})$, which implies $N_{\\mathrm{out}} > N_{\\mathrm{in}}$.\n\nTherefore, under the stated separation condition, identifiability is lost when the number of outliers exceeds the number of inliers ($N_{\\mathrm{out}} > N_{\\mathrm{in}}$). The estimator, in its attempt to fit the majority of the data, discards the true inlier cluster and locks onto the larger outlier cluster. The breakdown point of this estimator—the fraction of contamination it can tolerate—is effectively $50\\%$.\n\nWhen the separation condition $|F(x^\\star) - F(x^{\\dagger})| > cs$ does not hold (e.g., if the scale $s$ is large), the two basins of attraction merge. The outliers are not fully rejected and exert a continuous \"pull\" on the solution. The location of the minimum will then depend on a more complex interplay between the locations and relative weights of the data clusters, and the simple comparison above is no longer valid. In such cases, the global minimum might be located somewhere between $x^\\star$ and $x^{\\dagger}$, and may still be far from $x^\\star$, leading to a loss of identifiability according to the problem's criterion $|\\hat{x} - x^\\star| > \\tau$.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the robust estimation problem for the four specified test cases.\n    \"\"\"\n\n    def F(x):\n        \"\"\"The forward model F(x) = tanh(x).\"\"\"\n        return np.tanh(x)\n\n    def tukey_bisquare_rho(t, c):\n        \"\"\"\n        Computes Tukey's bisquare loss function rho(t) for a given constant c.\n        This function is vectorized to work with numpy arrays.\n        \"\"\"\n        c_squared_over_6 = c**2 / 6.0\n        abs_t = np.abs(t)\n        # Initialize rho values to the constant part for |t| > c\n        rho_values = np.full_like(t, c_squared_over_6, dtype=float)\n        # Identify elements where |t| <= c\n        mask = abs_t <= c\n        # Compute rho for these elements\n        t_scaled = t[mask] / c\n        rho_values[mask] = c_squared_over_6 * (1.0 - (1.0 - t_scaled**2)**3)\n        return rho_values\n\n    def objective_function(x, y_data, s, c):\n        \"\"\"\n        Computes the M-estimator objective function J(x).\n        \"\"\"\n        residuals = y_data - F(x)\n        standardized_residuals = residuals / s\n        rho_vals = tukey_bisquare_rho(standardized_residuals, c)\n        return np.sum(rho_vals * s**2)\n\n    # Problem parameters\n    test_cases = [\n        # (x_star, x_dagger, N_in, N_out, s)\n        (3.0, -3.0, 30, 0, 0.1),  # Case 1\n        (3.0, -3.0, 30, 5, 0.1),  # Case 2\n        (3.0, -3.0, 10, 25, 0.1), # Case 3\n        (3.0, -3.0, 20, 20, 0.5), # Case 4\n    ]\n    \n    C_TUKEY = 4.685\n    TAU = 0.5\n    GRID_MIN = -6.0\n    GRID_MAX = 6.0\n    GRID_POINTS = 2001\n    \n    x_grid = np.linspace(GRID_MIN, GRID_MAX, GRID_POINTS)\n    \n    all_results = []\n\n    for case in test_cases:\n        x_star, x_dagger, N_in, N_out, s = case\n        \n        # 1. Construct the dataset\n        y_inliers = np.full(N_in, F(x_star))\n        y_outliers = np.full(N_out, F(x_dagger))\n        y_data = np.concatenate((y_inliers, y_outliers))\n        \n        # 2. Evaluate objective function on the grid\n        J_values = np.array([objective_function(xi, y_data, s, C_TUKEY) for xi in x_grid])\n        \n        # 3. Compute the required outputs\n        \n        # Find approximate global minimizer\n        min_idx = np.argmin(J_values)\n        x_hat = x_grid[min_idx]\n        \n        # Count strict local minima\n        num_local_minima = 0\n        for i in range(1, len(J_values) - 1):\n            if J_values[i-1] > J_values[i] and J_values[i] < J_values[i+1]:\n                num_local_minima += 1\n        \n        # Check if identifiability is lost\n        identifiability_lost = np.abs(x_hat - x_star) > TAU\n        \n        # Check separation condition\n        separation_holds = np.abs(F(x_star) - F(x_dagger)) > C_TUKEY * s\n        \n        # Aggregate results for this case\n        all_results.extend([\n            round(x_hat, 3),\n            num_local_minima,\n            identifiability_lost,\n            separation_holds\n        ])\n\n    # Final print statement in the exact required format.\n    # The format requires a string representation of the list.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}