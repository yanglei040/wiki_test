{
    "hands_on_practices": [
        {
            "introduction": "为建立坚实的基础，我们的第一个练习将探讨正则化背后的根本动机：偏差-方差权衡。该练习将弹性网络简化为其$\\ell_2$分量（即岭回归），以清晰地展示引入惩罚项如何通过降低方差来提高预测稳定性，尽管这会以略微增加偏差为代价。通过完成这项计算()，您将对这一关键概念获得具体且定量的理解。",
            "id": "3377910",
            "problem": "考虑一个二维线性逆问题，其观测算子为单位算子。设数据模型为 $y = x^{\\star} + \\varepsilon$，其中 $x^{\\star} \\in \\mathbb{R}^{2}$ 是未知状态，$y \\in \\mathbb{R}^{2}$ 是观测数据，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{2})$ 是均值为零、协方差为 $\\sigma^{2} I_{2}$ 的加性高斯噪声。我们使用弹性网络（Elastic Net）进行估计，通过将混合惩罚项中的 $\\ell_{1}$ 参数设为零，以分离二次惩罚项的影响，从而得到岭正则化估计量\n$$\n\\widehat{x}_{\\lambda_{2}} \\in \\arg\\min_{x \\in \\mathbb{R}^{2}} \\left\\{ \\frac{1}{2} \\| y - x \\|_{2}^{2} + \\frac{\\lambda_{2}}{2} \\| x \\|_{2}^{2} \\right\\}, \\quad \\lambda_{2} \\ge 0.\n$$\n定义样本内预测为 $\\widehat{y}_{\\lambda_{2}} = \\widehat{x}_{\\lambda_{2}}$，无噪声真值为 $y^{\\star} = x^{\\star}$。预测稳定性由期望样本内预测误差 $\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right]$ 分解为偏差平方和方差分量后的方差项来量化。\n\n从数据模型和 $\\widehat{x}_{\\lambda_{2}}$ 的定义出发，推导 $\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right]$ 的偏差-方差分解，使其成为 $\\lambda_{2}$、$x^{\\star}$ 和 $\\sigma^{2}$ 的显式函数。然后，使用 $x^{\\star} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$ 和 $\\sigma^{2} = 1$ 实例化该示例，以证明将 $\\lambda_{2}$ 从 $0$ 增加到 $1$ 会以增加偏差为代价来提高预测稳定性（减小方差）。通过计算以下比率来量化这种权衡\n$$\nR \\equiv \\frac{\\text{当 } \\lambda_{2}: 0 \\to 1 \\text{ 时方差的减少量}}{\\text{当 } \\lambda_{2}: 0 \\to 1 \\text{ 时偏差平方的增量}}。\n$$\n给出 $R$ 的值，形式为单个精确数值。不包含单位。不提供中间值。最终答案必须是单个实数。",
            "solution": "分析始于验证问题陈述。\n\n### 步骤 1：提取已知条件\n- 数据模型：$y = x^{\\star} + \\varepsilon$，其中 $x^{\\star} \\in \\mathbb{R}^{2}$ 是未知真实状态，$y \\in \\mathbb{R}^{2}$ 是观测值。\n- 噪声模型：$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{2})$，其中 $I_{2}$ 是 $2 \\times 2$ 单位矩阵。\n- 估计量：$\\widehat{x}_{\\lambda_{2}} \\in \\arg\\min_{x \\in \\mathbb{R}^{2}} \\left\\{ \\frac{1}{2} \\| y - x \\|_{2}^{2} + \\frac{\\lambda_{2}}{2} \\| x \\|_{2}^{2} \\right\\}$，对于 $\\lambda_{2} \\ge 0$。\n- 样本内预测：$\\widehat{y}_{\\lambda_{2}} = \\widehat{x}_{\\lambda_{2}}$。\n- 无噪声真值：$y^{\\star} = x^{\\star}$。\n- 待分析量：期望样本内预测误差 $\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right]$ 及其偏差-方差分解。\n- 用于实例化的具体参数：$x^{\\star} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$，$\\sigma^{2} = 1$。\n- 所需计算：比率 $R \\equiv \\frac{\\text{当 } \\lambda_{2}: 0 \\to 1 \\text{ 时方差的减少量}}{\\text{当 } \\lambda_{2}: 0 \\to 1 \\text{ 时偏差平方的增量}}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题具有科学依据，是统计学习理论中岭回归（弹性网络的一个特例）的标准表述。它是适定的，因为代价函数是严格凸的，保证了唯一的最小值。问题是客观的，使用了精确的数学语言。得到唯一解所需的所有数据和定义都已提供，且没有矛盾之处。该问题要求进行严谨的推导和特定的计算，这是逆问题领域的标准练习。该问题并非简单，且可进行科学验证。\n\n### 步骤 3：结论与行动\n问题是有效的。我们继续求解。\n\n第一步是找到估计量 $\\widehat{x}_{\\lambda_{2}}$ 的显式闭式表达式。待最小化的代价函数为\n$$\nJ(x) = \\frac{1}{2} \\| y - x \\|_{2}^{2} + \\frac{\\lambda_{2}}{2} \\| x \\|_{2}^{2}\n$$\n当 $\\lambda_{2} \\ge 0$ 时，该函数是严格凸的。通过将其关于 $x$ 的梯度设为零来找到最小值。\n$$\n\\nabla_{x} J(x) = \\nabla_{x} \\left( \\frac{1}{2} (y - x)^{T}(y - x) + \\frac{\\lambda_{2}}{2} x^{T}x \\right) = -(y - x) + \\lambda_{2} x\n$$\n将梯度设为零：\n$$\n-(y - \\widehat{x}_{\\lambda_{2}}) + \\lambda_{2} \\widehat{x}_{\\lambda_{2}} = 0\n$$\n$$\n-y + \\widehat{x}_{\\lambda_{2}} + \\lambda_{2} \\widehat{x}_{\\lambda_{2}} = 0\n$$\n$$\n(1 + \\lambda_{2}) \\widehat{x}_{\\lambda_{2}} = y\n$$\n求解 $\\widehat{x}_{\\lambda_{2}}$ 可得：\n$$\n\\widehat{x}_{\\lambda_{2}} = \\frac{1}{1 + \\lambda_{2}} y\n$$\n样本内预测为 $\\widehat{y}_{\\lambda_{2}} = \\widehat{x}_{\\lambda_{2}}$。无噪声真值为 $y^{\\star} = x^{\\star}$。期望预测误差为 $\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right]$。我们将此误差分解为其偏差平方和方差分量：\n$$\n\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right] = \\underbrace{\\left\\| \\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] - y^{\\star} \\right\\|_{2}^{2}}_{\\text{偏差平方}} + \\underbrace{\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - \\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] \\|_{2}^{2}\\right]}_{\\text{方差}}\n$$\n首先，我们计算预测的期望值。期望是针对噪声 $\\varepsilon$ 的分布计算的。\n$$\n\\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] = \\mathbb{E}\\left[ \\frac{1}{1 + \\lambda_{2}} y \\right] = \\frac{1}{1 + \\lambda_{2}} \\mathbb{E}[y]\n$$\n使用数据模型 $y = x^{\\star} + \\varepsilon$ 以及 $\\mathbb{E}[\\varepsilon] = 0$ 这一事实：\n$$\n\\mathbb{E}[y] = \\mathbb{E}[x^{\\star} + \\varepsilon] = x^{\\star} + \\mathbb{E}[\\varepsilon] = x^{\\star}\n$$\n因此，期望预测为：\n$$\n\\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] = \\frac{1}{1 + \\lambda_{2}} x^{\\star}\n$$\n现在，我们计算偏差平方，记为 $B^2(\\lambda_2)$：\n$$\nB^2(\\lambda_2) = \\left\\| \\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] - y^{\\star} \\right\\|_{2}^{2} = \\left\\| \\frac{1}{1 + \\lambda_{2}} x^{\\star} - x^{\\star} \\right\\|_{2}^{2} = \\left\\| \\left(\\frac{1}{1 + \\lambda_{2}} - 1\\right) x^{\\star} \\right\\|_{2}^{2} = \\left\\| \\frac{-\\lambda_{2}}{1 + \\lambda_{2}} x^{\\star} \\right\\|_{2}^{2}\n$$\n$$\nB^2(\\lambda_2) = \\left( \\frac{\\lambda_{2}}{1 + \\lambda_{2}} \\right)^{2} \\|x^{\\star}\\|_{2}^{2}\n$$\n接下来，我们计算方差，记为 $V(\\lambda_2)$：\n$$\nV(\\lambda_2) = \\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - \\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] \\|_{2}^{2}\\right] = \\mathbb{E}\\left[ \\left\\| \\frac{1}{1 + \\lambda_{2}} y - \\frac{1}{1 + \\lambda_{2}} x^{\\star} \\right\\|_{2}^{2} \\right]\n$$\n$$\nV(\\lambda_2) = \\mathbb{E}\\left[ \\left\\| \\frac{1}{1 + \\lambda_{2}} (y - x^{\\star}) \\right\\|_{2}^{2} \\right] = \\mathbb{E}\\left[ \\left\\| \\frac{1}{1 + \\lambda_{2}} \\varepsilon \\right\\|_{2}^{2} \\right] = \\frac{1}{(1 + \\lambda_{2})^{2}} \\mathbb{E}[\\|\\varepsilon\\|_{2}^{2}]\n$$\n噪声向量为 $\\varepsilon = (\\varepsilon_1, \\varepsilon_2)^T$，其中 $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ 是相互独立的。$\\varepsilon$ 的期望范数平方为：\n$$\n\\mathbb{E}[\\|\\varepsilon\\|_{2}^{2}] = \\mathbb{E}[\\varepsilon_1^2 + \\varepsilon_2^2] = \\mathbb{E}[\\varepsilon_1^2] + \\mathbb{E}[\\varepsilon_2^2]\n$$\n对于任意均值为 $\\mu_Z$、方差为 $\\sigma_Z^2$ 的随机变量 $Z$，有 $\\mathbb{E}[Z^2] = \\text{Var}(Z) + (\\mathbb{E}[Z])^2$。此处，$\\mathbb{E}[\\varepsilon_i] = 0$ 且 $\\text{Var}(\\varepsilon_i) = \\sigma^2$。所以，$\\mathbb{E}[\\varepsilon_i^2] = \\sigma^2$。\n$$\n\\mathbb{E}[\\|\\varepsilon\\|_{2}^{2}] = \\sigma^2 + \\sigma^2 = 2\\sigma^2\n$$\n将其代回方差表达式：\n$$\nV(\\lambda_2) = \\frac{2\\sigma^2}{(1 + \\lambda_{2})^{2}}\n$$\n完整的偏差-方差分解为：\n$$\n\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right] = \\left( \\frac{\\lambda_{2}}{1 + \\lambda_{2}} \\right)^{2} \\|x^{\\star}\\|_{2}^{2} + \\frac{2\\sigma^2}{(1 + \\lambda_{2})^{2}}\n$$\n现在，我们用给定的参数 $x^{\\star} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$ 和 $\\sigma^{2} = 1$ 进行实例化。\n首先，计算 $\\|x^{\\star}\\|_{2}^{2}$：\n$$\n\\|x^{\\star}\\|_{2}^{2} = 2^2 + 1^2 = 4 + 1 = 5\n$$\n我们评估 $\\lambda_{2} = 0$ 和 $\\lambda_{2} = 1$ 时的偏差平方和方差。\n\n对于 $\\lambda_{2} = 0$：\n偏差平方：$B^2(0) = \\left( \\frac{0}{1 + 0} \\right)^{2} \\times 5 = 0$。\n方差：$V(0) = \\frac{2(1)}{(1 + 0)^{2}} = 2$。\n\n对于 $\\lambda_{2} = 1$：\n偏差平方：$B^2(1) = \\left( \\frac{1}{1 + 1} \\right)^{2} \\times 5 = \\left(\\frac{1}{2}\\right)^{2} \\times 5 = \\frac{5}{4}$。\n方差：$V(1) = \\frac{2(1)}{(1 + 1)^{2}} = \\frac{2}{4} = \\frac{1}{2}$。\n\n当 $\\lambda_2$ 从 $0$ 变为 $1$ 时，偏差平方的增量为：\n$$\n\\Delta B^2 = B^2(1) - B^2(0) = \\frac{5}{4} - 0 = \\frac{5}{4}\n$$\n当 $\\lambda_2$ 从 $0$ 变为 $1$ 时，方差的减少量为：\n$$\n\\Delta V = V(0) - V(1) = 2 - \\frac{1}{2} = \\frac{3}{2}\n$$\n问题要求计算比率 $R$：\n$$\nR = \\frac{\\text{方差减少量}}{\\text{偏差平方增量}} = \\frac{\\Delta V}{\\Delta B^2} = \\frac{3/2}{5/4}\n$$\n$$\nR = \\frac{3}{2} \\times \\frac{4}{5} = \\frac{12}{10} = \\frac{6}{5}\n$$",
            "answer": "$$\n\\boxed{\\frac{6}{5}}\n$$"
        },
        {
            "introduction": "在理解了$\\ell_2$惩罚项的作用后，我们现在来考察完整的弹性网络目标函数，它结合了$\\ell_1$和$\\ell_2$两种正则化。这个练习 () 展示了一个弹性网络未能恢复真实稀疏解的场景，这种现象被称为“分组效应”。通过推导解并分析其几何形状，您将看到一个强的$\\ell_2$惩罚项如何压倒促进稀疏性的$\\ell_1$项，从而迫使相关变量被一同选中。",
            "id": "3377903",
            "problem": "考虑一个单一标量观测的线性逆问题，其模型为 $y = A x^{\\star}$，其中 $A \\in \\mathbb{R}^{1 \\times 2}$，$x^{\\star} \\in \\mathbb{R}^{2}$，且 $y \\in \\mathbb{R}$。令 $A = [\\,1 \\;\\; 1\\,]$，$x^{\\star} = (2,\\,0)^{\\top}$，因此 $y = 2$。估计量 $x \\in \\mathbb{R}^{2}$ 是通过最小化弹性网络目标函数得到的\n$$\nJ(x) \\;=\\; \\frac{1}{2}\\,\\|y - A x\\|_{2}^{2} \\;+\\; \\lambda_{1}\\,\\|x\\|_{1} \\;+\\; \\frac{\\lambda_{2}}{2}\\,\\|x\\|_{2}^{2},\n$$\n其中 $\\lambda_{1} = 1$ 且 $\\lambda_{2} = 10$。从弹性网络目标函数的核心定义和基本凸优化原理（包括一阶最优性和凸性论证）出发，推导出精确的最小化子 $\\hat{x}$ 并计算其值。然后，利用基于凸分析中水平集和法锥几何学的几何推理，解释为什么当 $\\lambda_{2}$ 相对于 $\\lambda_{1}$ 较大时，这个构造为支撑恢复提供了一个反例。\n\n你的最终数值答案应该是最小化子向量 $\\hat{x}$，表示为单个行矩阵。不要四舍五入，请提供精确值。",
            "solution": "用户要求我求解一个弹性网络目标函数的最小化子，并为一个特定解的性质提供几何解释。\n\n### 步骤1：问题验证\n\n**提取的已知条件：**\n- 线性模型：$y = A x^{\\star}$，其中 $A \\in \\mathbb{R}^{1 \\times 2}$，$x^{\\star} \\in \\mathbb{R}^{2}$，$y \\in \\mathbb{R}$。\n- 数据：$A = [\\,1 \\;\\; 1\\,]$，$x^{\\star} = (2,\\,0)^{\\top}$，$y = 2$。\n- 弹性网络目标函数：$J(x) = \\frac{1}{2}\\,\\|y - A x\\|_{2}^{2} + \\lambda_{1}\\,\\|x\\|_{1} + \\frac{\\lambda_{2}}{2}\\,\\|x\\|_{2}^{2}$。\n- 正则化参数：$\\lambda_{1} = 1$，$\\lambda_{2} = 10$。\n- 任务1：推导出精确的最小化子 $\\hat{x}$。\n- 任务2：几何地解释为什么该结果是支撑恢复的一个反例。\n\n**验证：**\n1.  **科学依据：** 该问题是弹性网络正则化的一个标准应用，这是凸优化、统计学和机器学习中的一项成熟技术。所有概念在科学上都是合理的。\n2.  **适定性：** 目标函数 $J(x)$ 是三个凸函数的和：一个凸二次损失、一个凸 $L_1$ 范数和一个严格凸的 $L_2$ 范数平方（因为 $\\lambda_2 = 10 > 0$）。因此，它们的和是严格凸的。一个在紧集上或在 $\\mathbb{R}^n$ 上（如果是强制的，而它确实是）的严格凸函数存在唯一的最小化子。该问题是适定的。\n3.  **客观性：** 问题以精确的数学语言陈述，没有主观性。\n4.  **完整性：** 所有必要的值（$A, y, \\lambda_1, \\lambda_2$）都已提供并且是一致的（$A x^{\\star} = [1\\;1](2,0)^\\top = 2 = y$）。\n\n**结论：** 该问题是有效的，其所有组成部分都是良定义的。\n\n---\n\n### 步骤2：求解推导\n\n需要最小化的目标函数是 $J(x) = \\frac{1}{2}\\,\\|y - A x\\|_{2}^{2} + \\lambda_{1}\\,\\|x\\|_{1} + \\frac{\\lambda_{2}}{2}\\,\\|x\\|_{2}^{2}$。\n令 $x = (x_1, x_2)^{\\top}$。代入给定值 $y=2$, $A = [\\,1 \\;\\; 1\\,]$, $\\lambda_1=1$ 和 $\\lambda_2=10$：\n$$\nJ(x_1, x_2) = \\frac{1}{2}(2 - (x_1 + x_2))^2 + |x_1| + |x_2| + \\frac{10}{2}(x_1^2 + x_2^2)\n$$\n由于绝对值项的存在，该函数是凸的但不可微。一个点 $\\hat{x}$ 是 $J(x)$ 的唯一最小化子，当且仅当零向量是 $J$ 在 $\\hat{x}$ 处的次微分的一个元素，即 $0 \\in \\partial J(\\hat{x})$。\n\n目标函数可以分解为一个可微部分 $f(x) = \\frac{1}{2}(y - Ax)^2 + \\frac{\\lambda_2}{2}\\|x\\|_2^2$ 和一个不可微部分 $g(x) = \\lambda_1 \\|x\\|_1$。次微分是 $\\partial J(x) = \\nabla f(x) + \\partial g(x)$。一阶最优性条件是 $0 \\in \\nabla f(\\hat{x}) + \\lambda_1 \\partial \\|\\hat{x}\\|_1$。\n\n光滑部分 $f(x)$ 的梯度是：\n$$\n\\nabla f(x) = -A^{\\top}(y - Ax) + \\lambda_2 x\n$$\n代入给定值：\n$$\n\\nabla f(x) = -\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}(2 - (x_1 + x_2)) + 10 \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} -(2 - x_1 - x_2) + 10x_1 \\\\ -(2 - x_1 - x_2) + 10x_2 \\end{pmatrix} = \\begin{pmatrix} 11x_1 + x_2 - 2 \\\\ x_1 + 11x_2 - 2 \\end{pmatrix}\n$$\n$L_1$ 范数的次微分是 $\\partial \\|x\\|_1 = \\bigotimes_{i=1}^2 \\partial |x_i|$，其中如果 $z \\neq 0$，则 $\\partial |z| = \\text{sign}(z)$；如果 $z=0$，则 $\\partial |z| = [-1, 1]$。\n\n当 $\\lambda_1=1$ 时，最优性条件 $0 \\in \\nabla f(\\hat{x}) + \\lambda_1 \\partial \\|\\hat{x}\\|_1$ 变为：\n$$\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\in \\begin{pmatrix} 11\\hat{x}_1 + \\hat{x}_2 - 2 \\\\ \\hat{x}_1 + 11\\hat{x}_2 - 2 \\end{pmatrix} + \\begin{pmatrix} \\partial |\\hat{x}_1| \\\\ \\partial |\\hat{x}_2| \\end{pmatrix}\n$$\n这给出了一个包含两个次微分包含关系的系统：\n1. $-(11\\hat{x}_1 + \\hat{x}_2 - 2) \\in \\partial |\\hat{x}_1|$\n2. $-(\\hat{x}_1 + 11\\hat{x}_2 - 2) \\in \\partial |\\hat{x}_2|$\n\n由于问题设置的对称性（$A=[1,1]$），目标函数 $J(x_1, x_2)$ 在交换 $x_1$ 和 $x_2$ 时是对称的。因为 $J(x)$ 是严格凸的，因此有唯一的最小化子，这个最小化子也必须是对称的，即 $\\hat{x}_1 = \\hat{x}_2$。令 $\\hat{x}_1 = \\hat{x}_2 = \\alpha$。解不可能是 $(0,0)^{\\top}$，因为 $J(0,0)=2$，而一个小的扰动，例如对于小的 $\\epsilon>0$，取 $x=(\\epsilon, \\epsilon)$，会得到 $J(\\epsilon, \\epsilon) \\approx \\frac{1}{2}(2-2\\epsilon)^2 + 2\\epsilon \\approx 2 - 4\\epsilon + 2\\epsilon^2 + 2\\epsilon = 2-2\\epsilon  2$。因此，$\\alpha \\neq 0$。\n\n假设 $\\alpha  0$。那么 $\\partial |\\hat{x}_1| = \\{1\\}$ 且 $\\partial |\\hat{x}_2| = \\{1\\}$。包含关系变为等式：\n1. $-(11\\alpha + \\alpha - 2) = 1 \\implies -12\\alpha + 2 = 1 \\implies 12\\alpha = 1 \\implies \\alpha = \\frac{1}{12}$\n2. $-(\\alpha + 11\\alpha - 2) = 1 \\implies -12\\alpha + 2 = 1 \\implies 12\\alpha = 1 \\implies \\alpha = \\frac{1}{12}$\n\n两个方程都一致地得出 $\\alpha = 1/12$。因为这个值是正的，所以我们 $\\alpha  0$ 的假设是有效的。我们找到了一个满足一阶最优性条件的点。由于严格凸性，这个点是唯一的全局最小化子。\n最小化子是 $\\hat{x} = (\\frac{1}{12}, \\frac{1}{12})^{\\top}$。\n\n### 步骤3：支撑恢复失败的几何解释\n\n**支撑恢复：** 真实的稀疏向量是 $x^{\\star} = (2, 0)^{\\top}$。它的支撑集（非零元素的索引集合）是 $\\{1\\}$。估计的向量是 $\\hat{x} = (\\frac{1}{12}, \\frac{1}{12})^{\\top}$。它的支撑集是 $\\{1, 2\\}$。由于支撑集不匹配，弹性网络估计量在这种情况下未能实现支撑恢复。这提供了一个反例。\n\n**几何和分析解释：** 弹性网络估计量 $\\hat{x}$ 是目标函数光滑部分 $f(x) = \\frac{1}{2}\\|y-Ax\\|^2_2 + \\frac{\\lambda_2}{2}\\|x\\|^2_2$ 的一个水平集与非光滑部分 $g(x) = \\lambda_1\\|x\\|_1$ 的一个水平集之间的切点。形式上，这由最优性条件 $-\\nabla f(\\hat{x}) \\in \\partial g(\\hat{x})$ 表示，该条件表明光滑部分的负梯度必须位于点 $\\hat{x}$ 处 $L_1$ 球的法锥内。\n\n让我们研究一下形式为 $\\hat{x} = (c, 0)^{\\top}$ 且 $c0$ 的稀疏解是否可能是最优的。对于这样一个点，由 $\\lambda_1$ 缩放的 $L_1$ 球的法锥是 $\\lambda_1 \\left( \\{1\\} \\times [-1, 1] \\right)$。最优性条件 $-\\nabla f(c,0) \\in \\lambda_1 \\left( \\{1\\} \\times [-1, 1] \\right)$ 变为：\n$$\n\\begin{pmatrix} -(11c + 0 - 2) \\\\ -(c + 0 - 2) \\end{pmatrix} = \\begin{pmatrix} 2 - 11c \\\\ 2 - c \\end{pmatrix} \\in \\begin{pmatrix} 1 \\cdot \\{1\\} \\\\ 1 \\cdot [-1, 1] \\end{pmatrix} = \\begin{pmatrix} \\{1\\} \\\\ [-1, 1] \\end{pmatrix}\n$$\n这得出两个条件：\n1. $2 - 11c = 1 \\implies 11c = 1 \\implies c = \\frac{1}{11}$。\n2. $|2-c| \\le 1$。\n\n将第一个条件得到的 $c=1/11$ 代入第二个条件：\n$$\n|2 - \\frac{1}{11}| = |\\frac{22-1}{11}| = \\frac{21}{11}\n$$\n条件变为 $\\frac{21}{11} \\le 1$，这是不成立的。\n\n从几何上讲，这意味着在 $x_1$ 轴上的点 $(1/11, 0)^{\\top}$ 处，负梯度向量 $-\\nabla f(c,0)$ 指向 $L_1$ 球法锥的外部。梯度的第二个分量 $-(c-2)$ 太大，表明通过使 $x_2$ 非零可以实现目标函数的下降。因此，形式为 $(c,0)^{\\top}$ 的稀疏解不可能是最优的。\n\n这种失败是弹性网络“分组效应”的一种体现。预测变量（$A = [\\,1 \\;\\; 1\\,]$ 的列）是完全相关的。岭惩罚项 $\\frac{\\lambda_2}{2}\\|x\\|_2^2$ 鼓励相关预测变量具有相似的系数值。当 $\\lambda_2$ 相对于 $\\lambda_1$ 较大时，这种分组效应主导了 $L_1$ 惩罚项的稀疏性诱导压力。它迫使 $\\hat{x}_1$ 和 $\\hat{x}_2$ 彼此接近，将 $\\hat{x}_2$ 从零拉开，从而破坏了真实解的稀疏性。问题的对称性（$A_1=A_2=1$）使这种效应成为绝对的，迫使 $\\hat{x}_1 = \\hat{x}_2$，并使得对于任何 $\\lambda_1, \\lambda_2  0$（除非解在原点处是平凡解），支撑恢复都变得不可能。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{12}  \\frac{1}{12} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "我们最后的练习将焦点从估计量的数学性质转移到大规模计算的实践挑战上。在现实世界的应用中，特别是在数据同化领域，选择正确的算法与构建正确的模型同等重要。这个问题 () 要求您比较几种主流的优化方法，在大型稀疏反问题的背景下分析它们的计算复杂度和内存需求，并基于这些实际的权衡来论证您的算法选择。",
            "id": "3377843",
            "problem": "考虑数据同化中的弹性网络正则化反问题：最小化目标函数\n$$\n\\phi(x) \\;=\\; \\tfrac{1}{2}\\,\\|A x - b\\|_2^2 \\;+\\; \\lambda_1 \\,\\|x\\|_1 \\;+\\; \\tfrac{\\lambda_2}{2}\\,\\|x\\|_2^2,\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$ 是一个稀疏观测算子，$b \\in \\mathbb{R}^m$ 是观测数据，$\\lambda_1,\\lambda_2 \\ge 0$ 是正则化参数。假设 $A$ 以压缩稀疏行 (CSR) 格式存储，并令 $\\operatorname{nnz}(A)$ 表示 $A$ 的非零元数量。你可以假设以下基本事实：\n- 当 $A$ 以 CSR/压缩稀疏列 (CSC) 格式存储时，一次稀疏矩阵向量乘法 $\\;v \\mapsto A v\\;$ 或 $\\;w \\mapsto A^\\top w\\;$ 的运算成本为 $\\;O(\\operatorname{nnz}(A))\\;$。\n- 以 CSR 格式存储 $A$ 的内存成本为 $O(\\operatorname{nnz}(A))$，而存储一个 $m$ 维向量或 $n$ 维向量的内存成本分别为 $O(m)$ 或 $O(n)$。\n- 像 $\\,\\lambda_1\\|x\\|_1 + \\tfrac{\\lambda_2}{2}\\|x\\|_2^2\\,$ 这样的可分罚项的近端算子是按分量作用的，其计算成本为 $O(n)$。\n- 交替方向乘子法 (ADMM) 引入大小为 $n$ 的分裂变量和对偶变量，并需要（精确或近似地）求解 $x$-更新步骤中出现的线性系统，该系统通常涉及由单位矩阵的倍数增广的法方程矩阵 $A^\\top A$。\n- 坐标下降法一次更新一个坐标（或一个小块）；维持当前残差 $r = A x - b$ 可以使得每个坐标更新的计算时间与 $A$ 对应列中的非零元数量成正比。\n\n假设算子 $A$ 具有大规模地球物理数据同化中典型的以下性质：\n- $A$ 非常稀疏且具有有界列稀疏度，即 $\\max_{j} \\operatorname{nnz}(A_{:,j}) \\le d_{\\max}$，其中 $d_{\\max}$ 是一个与 $m$ 和 $n$ 无关的适中数值。\n- 列范数 $\\|A_{:,j}\\|_2$ 近似均匀，且 $A$ 没有可利用的快速变换（没有 Toeplitz/卷积结构）。\n- 构造 $A^\\top A$ 或对其进行分解是不可行的，并且没有针对法方程的高质量预条件子可用。\n- 内存非常宝贵：任何需要在 $x$ 之外需要多个额外 $n$ 维向量的方法都是不理想的，而存储单个残差向量 $r \\in \\mathbb{R}^m$ 是可以接受的。\n在这些假设下，比较近端梯度法、ADMM 和坐标下降法的每次迭代时间复杂度和内存占用，并根据算子性质论证算法的选择。\n\n选择唯一一个正确描述了复杂度与内存占用，并给出合理论证的算法推荐的选项。\n\nA. 近端梯度法通过两次稀疏矩阵向量乘法计算梯度 $A^\\top(Ax - b)$，成本为 $O(\\operatorname{nnz}(A))$，外加一个 $O(n)$ 的近端步骤；其内存占用为 $O(\\operatorname{nnz}(A) + m + n)$（用于 $A$、$r$ 和 $x$）。维护残差的坐标下降法每轮成本为 $O(\\operatorname{nnz}(A))$（对列非零元求和），内存占用类似，为 $O(\\operatorname{nnz}(A) + m + n)$。ADMM 需要存储分裂变量和对偶变量（增加 $O(n)$ 内存），并使用共轭梯度法 (CG) 求解 $x$-更新的线性系统，每次迭代成本为 $O(\\operatorname{nnz}(A) \\cdot N_{\\mathrm{CG}})$；在没有好的预条件子的情况下，$N_{\\mathrm{CG}}$ 可能会很大。因此，在所述假设下，近端梯度法是首选（均匀的列范数有利于全局步长；ADMM 的额外内存和内部求解是不合理的）。\n\nB. 因为 $A$ 是稀疏的，且 ADMM 中的增广项改善了条件数，ADMM 在一次 CG 迭代中就能精确完成 $x$-更新，使得每次迭代时间为 $O(\\operatorname{nnz}(A))$，内存为 $O(\\operatorname{nnz}(A))$，因此在所述假设下，ADMM 严格优于近端梯度法和坐标下降法。\n\nC. 坐标下降法的内存占用严格低于近端梯度法，因为它不需要存储残差 $r$；其每轮时间为 $O(n)$，与 $\\operatorname{nnz}(A)$ 无关。因此，坐标下降法是首选。\n\nD. 近端梯度法不实用，因为它需要构造 $A^\\top A$ 来选择步长（Lipschitz 常数），这需要密集的存储空间；ADMM 避免了这一点，从不显式使用 $A^\\top A$，因此是首选，其每次迭代时间为 $O(\\operatorname{nnz}(A))$，内存为 $O(\\operatorname{nnz}(A) + n)$。",
            "solution": "用户提供了一个关于为弹性网络正则化反问题选择合适优化算法的问题，该问题处于大规模数据同化中典型的特定约束条件下。任务是验证问题陈述的有效性，如果有效，则分析三种候选算法——近端梯度法、ADMM 和坐标下降法——的每次迭代复杂度和内存占用，以便为推荐的选择提供理由。\n\n### 问题验证\n\n**第 1 步：提取给定信息**\n- **目标函数：** 最小化 $\\phi(x) = \\tfrac{1}{2}\\,\\|A x - b\\|_2^2 + \\lambda_1 \\,\\|x\\|_1 + \\tfrac{\\lambda_2}{2}\\,\\|x\\|_2^2$。\n- **问题数据：** $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，$x \\in \\mathbb{R}^n$，$\\lambda_1, \\lambda_2 \\ge 0$。\n- **计算成本假设：**\n    - 稀疏矩阵向量乘积 ($A v$ 或 $A^\\top w$)：$O(\\operatorname{nnz}(A))$ 次运算。\n    - 存储：$A$ (CSR) 为 $O(\\operatorname{nnz}(A))$，$m$ 维向量为 $O(m)$，$n$ 维向量为 $O(n)$。\n    - $\\lambda_1\\|x\\|_1 + \\tfrac{\\lambda_2}{2}\\|x\\|_2^2$ 的近端算子：$O(n)$ 且按分量计算。\n    - ADMM 细节：涉及分裂/对偶变量（大小为 $O(n)$）和一个涉及 $A^\\top A$ 的线性系统的 $x$-更新子问题。\n    - 坐标下降法细节：在维持残差 $r=Ax-b$ 的情况下，每次坐标更新的成本与 $A$ 相应列的非零元数量成正比。\n- **$A$ 算子的性质：**\n    - 非常稀疏且列稀疏度有界：$\\max_{j} \\operatorname{nnz}(A_{:,j}) \\le d_{\\max}$。\n    - 近似均匀的列范数 $\\|A_{:,j}\\|_2$。\n    - 无快速变换结构。\n    - 构造或分解 $A^\\top A$ 不可行。\n    - 无用于法方程系统的高质量预条件子。\n- **内存约束：**\n    - 存储超出状态向量 $x$ 所必需的多个额外 $n$ 维向量是不理想的。\n    - 存储单个残差向量 $r \\in \\mathbb{R}^m$ 是可以接受的。\n\n**第 2 步：使用提取的给定信息进行验证**\n问题陈述在科学上是合理的、适定的和客观的。\n- **科学依据：** 该问题描述了一个标准的弹性网络回归问题，这是现代统计学、机器学习和反问题的基石。所列出的算法（近端梯度法、ADMM、坐标下降法）是解决此类问题的经典方法。关于计算成本和算子性质的假设是现实的，并且在大型科学计算领域（尤其是在地球物理学等领域）很常见。\n- **适定性与一致性：** 问题是适定的，要求基于一组清晰的约束和性质对算法进行比较分析。这些约束是自洽的，并为所需的分析提供了充分的基础。例如，关于构造 $A^\\top A$ 不可行的陈述直接影响了方法的选择及其实现（例如，对子问题使用迭代求解器）。\n- **客观性：** 问题以精确、技术性的语言陈述，没有歧义或主观论断。\n\n**第 3 步：结论与行动**\n该问题是有效的。我们继续进行求解。\n\n### 算法分析\n\n目标函数可以分解为 $J(x) = f(x) + h(x)$，其中 $f(x) = \\tfrac{1}{2}\\|Ax - b\\|_2^2$ 是光滑可微部分，而 $h(x) = \\lambda_1\\|x\\|_1 + \\tfrac{\\lambda_2}{2}\\|x\\|_2^2$ 是非光滑（由于 $L_1$ 范数）但可计算近端算子的部分。光滑部分的梯度是 $\\nabla f(x) = A^\\top(Ax-b)$。\n\n**1. 近端梯度法 (PGM)**\nPGM 的迭代更新为 $x^{k+1} = \\operatorname{prox}_{\\alpha h}(x^k - \\alpha \\nabla f(x^k))$，其中 $\\alpha  0$ 是步长。\n- **时间复杂度：** 每次迭代涉及：\n    1.  计算残差 $r^k = Ax^k - b$：一次稀疏矩阵向量乘积 (SpMV)，成本为 $O(\\operatorname{nnz}(A))$。\n    2.  计算梯度 $\\nabla f(x^k) = A^\\top r^k$：一次稀疏矩阵转置向量乘积，成本为 $O(\\operatorname{nnz}(A))$。\n    3.  向量更新 $x^k - \\alpha \\nabla f(x^k)$：一个 $O(n)$ 的运算。\n    4.  近端步骤 $\\operatorname{prox}_{\\alpha h}(\\cdot)$：一个按分量计算的 $O(n)$ 运算，如问题所述。\n    每次迭代的总成本主要由两次 SpMV 决定，得到 $O(\\operatorname{nnz}(A))$。\n- **内存占用：** 我们需要存储矩阵 $A$ ($O(\\operatorname{nnz}(A))$)、向量 $b$ ($O(m)$) 和状态向量 $x$ ($O(n)$)。为了计算更新，我们需要一个用于残差 $r \\in \\mathbb{R}^m$ 的临时向量 ($O(m)$) 和在 $\\mathbb{R}^n$ 中的梯度项 ($O(n)$)。总内存为 $O(\\operatorname{nnz}(A) + m + n)$。\n- **注记：** 步长 $\\alpha$ 取决于 $\\nabla f$ 的 Lipschitz 常数 $L = \\|A^\\top A\\|_2$。尽管构造 $A^\\top A$ 不可行，但可以使用幂法（仅需要与 $A$ 和 $A^\\top$ 的 SpMV）来估计 $L$，或者可以使用回溯线搜索。\n\n**2. 交替方向乘子法 (ADMM)**\n此问题的一个标准分裂是令 $J(x,z) = \\tfrac{1}{2}\\|Ax - b\\|_2^2 + \\tfrac{\\lambda_2}{2}\\|x\\|_2^2 + \\lambda_1\\|z\\|_1$ 并满足约束 $x=z$。\nADMM 迭代涉及 $x$ 和 $z$ 的子问题，以及对偶变量 $u$ 的更新。\n- **$x$-更新：** 这需要最小化一个二次函数，导致线性系统 $(A^\\top A + (\\lambda_2 + \\rho)I) x = \\text{rhs}$，其中 $\\rho0$ 是 ADMM 的罚参数。\n- **时间复杂度：** 由于构造 $A^\\top A$ 不可行且没有好的预条件子，该系统必须迭代求解，通常使用共轭梯度 (CG) 法。每次 CG 迭代需要一次与 $(A^\\top A + (\\lambda_2 + \\rho)I)$ 的矩阵向量乘积，其成本为 $O(\\operatorname{nnz}(A))$（对于 $A^\\top(Av)$）。如果需要 $N_{\\mathrm{CG}}$ 次迭代，$x$-更新的成本为 $O(N_{\\mathrm{CG}} \\cdot \\operatorname{nnz}(A))$。鉴于缺少预条件子，$N_{\\mathrm{CG}}$ 可能会很大，使得每次 ADMM 迭代比一次 PGM 迭代昂贵得多。$z$-更新（软阈值步骤）和 $u$-更新成本很低 ($O(n)$)。总体复杂度由 $x$-更新主导。\n- **内存占用：** 除了存储 $A$、$b$ 和 $x$ 的内存外，ADMM 还需要存储分裂变量 $z \\in \\mathbb{R}^n$ 和对偶变量 $u \\in \\mathbb{R}^n$。这违反了“任何需要在...之外需要多个额外 $n$ 维向量的方法都是不理想的”这一约束。\n\n**3. 坐标下降法 (CD)**\n此方法一次更新一个坐标 $x_j$，循环遍历 $j=1, \\ldots, n$。高效的实现会维护全局残差 $r = Ax-b$。\n- **时间复杂度：**\n    1.  要更新 $x_j$，必须解决一个一维优化问题。所需的关键项是光滑部分的偏导数，这可以与维护的残差 $r$ 相关联。$x_j$ 的更新规则可以在 $O(\\operatorname{nnz}(A_{:,j}))$ 的时间内计算，其中 $A_{:,j}$ 是 $A$ 的第 $j$ 列。这需要预先计算列范数 $\\|A_{:,j}\\|_2^2$，这是一次性的 $O(\\operatorname{nnz}(A))$ 成本。\n    2.  将 $x_j$ 更新 $\\delta_j$ 后，必须更新残差：$r \\rightarrow r + A_{:,j}\\delta_j$。这是一个稀疏向量更新，成本为 $O(\\operatorname{nnz}(A_{:,j}))$。\n    因此，遍历所有 $n$ 个坐标的一轮 (epoch) 成本为 $\\sum_{j=1}^n O(\\operatorname{nnz}(A_{:,j})) = O(\\operatorname{nnz}(A))$。\n- **内存占用：** 为了高效，CD 必须存储残差 $r \\in \\mathbb{R}^m$。所以，它需要存储 $A$ ($O(\\operatorname{nnz}(A))$)、$b$ ($O(m)$)、$x$ ($O(n)$) 和 $r$ ($O(m)$)，外加预计算的列范数 ($O(n)$)。总内存为 $O(\\operatorname{nnz}(A) + m + n)$，与 PGM 相当。\n\n**比较与推荐**\n- **ADMM** 是一个糟糕的选择。它的内存开销（额外的 $2n$ 维向量）被明确不鼓励使用。此外，每次迭代的计算成本很高 ($O(N_{\\mathrm{CG}} \\cdot \\operatorname{nnz}(A))$) 且不可预测，因为在没有好的预条件子的情况下，内部 CG 迭代次数 $N_{\\mathrm{CG}}$ 可能会很大。\n- **PGM 和 CD** 都是强有力的竞争者。它们具有相同的渐近迭代/轮时间复杂度 ($O(\\operatorname{nnz}(A))$) 和内存占用 ($O(\\operatorname{nnz}(A) + m + n)$)。\n- **PGM 和 CD** 之间的选择更为微妙。PGM 使用基于完整梯度的全局步长，如果问题的曲率相对均匀，这可能很有效。“近似均匀的列范数”这一给定性质表明，Hessian 矩阵 $A^\\top A$ 的性状可能相当良好（没有病态差异的对角线元素），使得单一的全局步长有效。CD 使用局部的、坐标级的信息，因此是自适应的，但如果变量高度相关，其贪婪的性质可能会很慢。根据给定的信息，基于均匀列范数有利于全局步长而推荐 PGM 的论点是合理的。\n\n### 逐项分析\n\n**A. 近端梯度法通过两次稀疏矩阵向量乘法计算梯度 $A^\\top(Ax - b)$，成本为 $O(\\operatorname{nnz}(A))$，外加一个 $O(n)$ 的近端步骤；其内存占用为 $O(\\operatorname{nnz}(A) + m + n)$（用于 $A$、$r$ 和 $x$）。维护残差的坐标下降法每轮成本为 $O(\\operatorname{nnz}(A))$（对列非零元求和），内存占用类似，为 $O(\\operatorname{nnz}(A) + m + n)$。ADMM 需要存储分裂变量和对偶变量（增加 $O(n)$ 内存），并使用共轭梯度法 (CG) 求解 $x$-更新的线性系统，每次迭代成本为 $O(\\operatorname{nnz}(A) \\cdot N_{\\mathrm{CG}})$；在没有好的预条件子的情况下，$N_{\\mathrm{CG}}$ 可能会很大。因此，在所述假设下，近端梯度法是首选（均匀的列范数有利于全局步长；ADMM 的额外内存和内部求解是不合理的）。**\n该选项对所有三种算法的时间和内存复杂度进行了准确的总结，与我们的分析一致。它正确地指出了在给定约束下 ADMM 的主要缺点（内存、内部求解器成本）。最终对 PGM 的推荐基于一个合理的启发式理由（“均匀的列范数有利于全局步长”）。此陈述的所有部分在问题背景下都是事实正确且逻辑合理的。\n**结论：正确。**\n\n**B. 因为 $A$ 是稀疏的，且 ADMM 中的增广项改善了条件数，ADMM 在一次 CG 迭代中就能精确完成 $x$-更新，使得每次迭代时间为 $O(\\operatorname{nnz}(A))$，内存为 $O(\\operatorname{nnz}(A))$，因此在所述假设下，ADMM 严格优于近端梯度法和坐标下降法。**\n该选项声称 ADMM 的 $x$-更新在一次 CG 迭代中解决 ($N_{\\mathrm{CG}}=1$)。这通常是错误的。虽然 $\\rho I$ 项改善了条件数，但除非问题是平凡的，否则它不能保证在单步内收敛。$O(\\operatorname{nnz}(A))$ 的内存占用也是不正确的，因为它忽略了向量 $b, x, z, u$ 的存储，它们的总大小为 $O(m+n)$。\n**结论：不正确。**\n\n**C. 坐标下降法的内存占用严格低于近端梯度法，因为它不需要存储残差 $r$；其每轮时间为 $O(n)$，与 $\\operatorname{nnz}(A)$ 无关。因此，坐标下降法是首选。**\n这个选项在两点上是不正确的。首先，针对此问题的 CD 高效实现*需要*维护残差 $r \\in \\mathbb{R}^m$ 以实现较低的每轮成本；否则，计算偏导数将非常耗时。因此，其内存占用与 PGM 相当。其次，每轮时间复杂度是 $O(\\operatorname{nnz}(A))$，而不是 $O(n)$。$O(n)$ 的成本意味着每次坐标更新是 $O(1)$，这不是真的，因为它取决于列稀疏度 $\\operatorname{nnz}(A_{:,j})$。\n**结论：不正确。**\n\n**D. 近端梯度法不实用，因为它需要构造 $A^\\top A$ 来选择步长（Lipschitz 常数），这需要密集的存储空间；ADMM 避免了这一点，从不显式使用 $A^\\top A$，因此是首选，其每次迭代时间为 $O(\\operatorname{nnz}(A))$，内存为 $O(\\operatorname{nnz}(A) + n)$。**\n这个选项基于一个错误的前提。PGM *不* 需要构造矩阵 $A^\\top A$。Lipschitz 常数 $L=\\|A^\\top A\\|_2$ 可以使用幂法来估计，这只涉及矩阵向量乘积。或者，回溯线搜索可以在每次迭代时确定步长，同样无需构造 $A^\\top A$。因此，ADMM 是首选的说法是基于一个有缺陷的论点。所述的 ADMM 时间复杂度也隐含地假设 $N_{\\mathrm{CG}}$ 是一个小常数，这是没有保证的。\n**结论：不正确。**",
            "answer": "$$\\boxed{A}$$"
        }
    ]
}