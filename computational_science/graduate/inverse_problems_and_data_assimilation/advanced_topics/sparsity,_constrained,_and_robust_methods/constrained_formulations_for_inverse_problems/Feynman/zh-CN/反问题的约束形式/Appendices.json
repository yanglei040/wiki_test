{
    "hands_on_practices": [
        {
            "introduction": "在许多逆问题中，解向量的各个分量必须位于物理上合理的范围内，例如非负性或饱和限制。投影梯度下降法是处理这类简单边界（或“箱型”）约束的一种基础且直观的算法。 这项练习将指导您实现该算法的一次迭代，将梯度计算、投影步骤和确保收敛的线搜索结合起来，从而巩固您对如何在约束设定下调整标准优化方法的理解。",
            "id": "3371700",
            "problem": "考虑有限维中的有界约束二次逆问题。设 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^{m}$，且 $\\lambda > 0$。定义 Tikhonov 正则化最小二乘目标函数\n$$\nf(x) \\equiv \\tfrac{1}{2}\\|A x - b\\|_2^2 + \\tfrac{\\lambda}{2}\\|x\\|_2^2, \\quad x \\in \\mathbb{R}^n.\n$$\n施加简单的逐分量边界约束 $l \\le x \\le u$，其中 $l, u \\in \\mathbb{R}^n$ 且对所有 $i$ 都有 $l_i \\le u_i$。到可行集 $[l,u]$ 上的欧几里得投影按分量定义为\n$$\n\\Pi_{[l,u]}(y)_i \\equiv \\min\\{\\max\\{y_i, l_i\\}, u_i\\}, \\quad i = 1,\\dots,n.\n$$\n要求您实现投影梯度法的一次迭代，该迭代使用回溯线搜索并遵循投影映射。从当前迭代点 $x \\in [l,u]$ 开始，计算一个投影试验点\n$$\nx^+(t) \\equiv \\Pi_{[l,u]}\\bigl(x - t \\nabla f(x)\\bigr),\n$$\n并通过 Armijo 回溯选择一个步长 $t$，使得在遵循投影的同时满足充分下降条件：\n$$\nf\\bigl(x^+(t)\\bigr) \\le f(x) + \\sigma \\, \\nabla f(x)^{\\top} \\bigl(x^+(t) - x\\bigr).\n$$\n仅使用以下基本规则和定义：\n- 目标函数 $f$ 如上所述。\n- 欧几里得投影 $\\Pi_{[l,u]}$ 如上定义。\n- 梯度 $\\nabla f(x)$ 必须根据给定的 $f$ 从基本原理推导得出。\n- Armijo 回溯必须从初始步长 $t_0 = 1$ 开始，并以固定因子 $\\beta \\in (0,1)$（此处 $\\beta = 0.5$）缩减步长，直到满足充分下降条件或步长低于最小阈值 $t_{\\min} = 10^{-12}$。\n- Armijo 参数为 $\\sigma = 10^{-4}$。\n\n实现一个程序，对于下方的每个测试用例，从给定的 $x$ 开始，执行恰好一次这样的投影梯度迭代：\n- 计算 $\\nabla f(x)$。\n- 从 $t = t_0$ 开始，使用 $x^+(t)$ 重复评估 Armijo 条件，如果不满足，则缩减 $t \\leftarrow \\beta t$，并始终通过投影 $\\Pi_{[l,u]}$ 维护 $x^+(t)$。\n- 当条件满足或 $t  t_{\\min}$ 时，终止回溯。\n- 返回接受的步长 $t$ 和对应的目标函数值 $f\\bigl(x^+(t)\\bigr)$。\n\n测试套件（每个用例指定 $(A,b,\\lambda,l,u,x)$）：\n1. 理想路径，内部迭代点：\n   - $A = \\begin{bmatrix} 3  0  1 \\\\ 0  2  -1 \\end{bmatrix}$，$b = \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix}$，$\\lambda = 0.1$，\n   - $l = \\begin{bmatrix} -5 \\\\ -5 \\\\ -5 \\end{bmatrix}$，$u = \\begin{bmatrix} 5 \\\\ 5 \\\\ 5 \\end{bmatrix}$，$x = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 1.0 \\end{bmatrix}$。\n2. 激活下界：\n   - $A = \\begin{bmatrix} 2  -1 \\\\ -1  2 \\end{bmatrix}$，$b = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$，$\\lambda = 0.01$，\n   - $l = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$，$u = \\begin{bmatrix} 10 \\\\ 10 \\end{bmatrix}$，$x = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}$。\n3. 激活上界：\n   - $A = \\begin{bmatrix} 1  3 \\\\ 2  0 \\end{bmatrix}$，$b = \\begin{bmatrix} 4 \\\\ -1 \\end{bmatrix}$，$\\lambda = 0.05$，\n   - $l = \\begin{bmatrix} -1 \\\\ -1 \\end{bmatrix}$，$u = \\begin{bmatrix} 0.2 \\\\ 0.3 \\end{bmatrix}$，$x = \\begin{bmatrix} 0.19 \\\\ 0.29 \\end{bmatrix}$。\n4. 零梯度边界情况：\n   - $A = \\begin{bmatrix} 2 \\end{bmatrix}$，$b = \\begin{bmatrix} 4 \\end{bmatrix}$，$\\lambda = 1.0$，\n   - $l = \\begin{bmatrix} 0 \\end{bmatrix}$，$u = \\begin{bmatrix} 10 \\end{bmatrix}$，$x = \\begin{bmatrix} 1.6 \\end{bmatrix}$。\n5. 在一个刚性一维问题中回溯收缩：\n   - $A = \\begin{bmatrix} 3 \\end{bmatrix}$，$b = \\begin{bmatrix} 1 \\end{bmatrix}$，$\\lambda = 1.0$，\n   - $l = \\begin{bmatrix} -5 \\end{bmatrix}$，$u = \\begin{bmatrix} 5 \\end{bmatrix}$，$x = \\begin{bmatrix} 0.0 \\end{bmatrix}$。\n\n不涉及角度单位。不涉及物理单位。\n\n您的程序必须生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表。每个测试用例的结果必须是列表 $[t,f^+]$，其中 $t$ 是接受的步长， $f^+ = f\\bigl(x^+(t)\\bigr)$ 是投影点的目标函数值。$t$ 和 $f^+$ 都必须打印为四舍五入到六位小数的十进制数。因此，最终输出格式为单行，类似于\n\"[ [t_1,f_1], [t_2,f_2], [t_3,f_3], [t_4,f_4], [t_5,f_5] ]\"\n，其中每个 $t_i$ 和 $f_i$ 都四舍五入到六位小数，并且没有额外文本。\n\n对于所有测试用例，回溯的常量必须固定为 $t_0 = 1$，$\\beta = 0.5$，$\\sigma = 10^{-4}$，$t_{\\min} = 10^{-12}$，且均相同。\n\n从基本原理推导所有需要的表达式，并相应地实现该方法。",
            "solution": "用户提供了一个在数值优化领域中定义明确的问题，具体涉及约束逆问题。任务是为有界约束二次目标函数实现一次投影梯度法迭代，该方法带有 Armijo 型回溯线搜索。该问题在科学上是合理的，在数学上是一致的，并为唯一解提供了所有必要的组成部分。\n\n首先，我们必须推导目标函数 $f(x)$ 的梯度，其定义为：\n$$\nf(x) \\equiv \\tfrac{1}{2}\\|A x - b\\|_2^2 + \\tfrac{\\lambda}{2}\\|x\\|_2^2\n$$\n这里，$A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^{m}$，$x \\in \\mathbb{R}^n$，$\\lambda > 0$ 是一个标量正则化参数。平方欧几里得范数可以用内积表示：\n$$\nf(x) = \\tfrac{1}{2}(A x - b)^{\\top}(A x - b) + \\tfrac{\\lambda}{2} x^{\\top}x\n$$\n展开第一项：\n$$\n(A x - b)^{\\top}(A x - b) = (x^{\\top}A^{\\top} - b^{\\top})(A x - b) = x^{\\top}A^{\\top}A x - x^{\\top}A^{\\top}b - b^{\\top}A x + b^{\\top}b\n$$\n由于 $b^{\\top}A x$ 是一个标量，它等于其转置 $(b^{\\top}A x)^{\\top} = x^{\\top}A^{\\top}b$。因此，该表达式简化为：\n$$\nf(x) = \\tfrac{1}{2}(x^{\\top}A^{\\top}A x - 2 b^{\\top}A x + b^{\\top}b) + \\tfrac{\\lambda}{2} x^{\\top}x\n$$\n为了求梯度 $\\nabla f(x)$，我们将 $f(x)$ 对向量 $x$ 求导。使用矩阵微积分的标准结果，其中 $\\nabla_x(x^\\top C x) = (C+C^\\top)x$ 和 $\\nabla_x(c^\\top x) = c$：\n$$\n\\nabla f(x) = \\tfrac{1}{2}(2 A^{\\top}A x - 2 A^{\\top}b) + \\tfrac{\\lambda}{2}(2x)\n$$\n简化此表达式可得到梯度：\n$$\n\\nabla f(x) = A^{\\top}(A x - b) + \\lambda x\n$$\n这就是我们将在算法中使用的解析梯度。\n\n该任务的核心是执行一次投影梯度法的迭代。从一个可行点 $x$（即 $l \\le x \\le u$）开始，该方法按以下步骤进行：\n\n1.  **计算梯度**：在当前迭代点 $x$ 处计算 $g = \\nabla f(x)$。\n\n2.  **回溯线搜索**：找到一个合适的步长 $t > 0$，以确保目标函数充分下降，同时遵守边界约束。搜索从初始步长 $t_0 = 1$ 开始，并以因子 $\\beta = 0.5$ 迭代地减小它，直到满足某个条件。\n\n    对于给定的步长 $t$，首先通过沿负梯度方向移动一步来计算试验点，然后将其投影回可行集 $[l, u]$ 上。可行集是一个超矩形（盒子），投影 $\\Pi_{[l,u]}$ 是逐分量应用的：\n    $$\n    x^+(t) = \\Pi_{[l,u]}(x - t g)\n    $$\n    其中 $(\\Pi_{[l,u]}(y))_i = \\min\\{\\max\\{y_i, l_i\\}, u_i\\}$。\n\n    如果步长 $t$ 满足为投影路径调整的 Armijo 型充分下降条件，则认为该步长是可接受的：\n    $$\n    f(x^+(t)) \\le f(x) + \\sigma g^{\\top}(x^+(t) - x)\n    $$\n    参数 $\\sigma = 10^{-4}$ 控制了何种程度的下降被认为是充分的。项 $g^{\\top}(x^+(t) - x)$ 表示沿从 $x$ 到 $x^+(t)$ 的可行弧线的方向导数。\n\n    回溯过程如下：\n    a.  从 $t=t_0 = 1$ 开始。\n    b.  计算 $x^+(t)$ 和 $f(x^+(t))$。\n    c.  检查 Armijo 条件是否满足。\n    d.  如果满足，则接受步长 $t$，线搜索终止。\n    e.  如果不满足，则减小步长：$t \\leftarrow \\beta t$。\n    f.  重复此过程。如果步长 $t$ 小于最小阈值 $t_{\\min} = 10^{-12}$，则也会终止。如果循环因 $t  t_{\\min}$ 而终止，则返回与此最终的小 $t$ 对应的值。\n\n每个测试用例的最终输出将是接受的步长 $t$ 和新点处的目标函数值 $f(x^+(t))$。\n\n实现将使用 Python 和 `numpy` 库来完成，以进行高效的向量和矩阵运算。一个函数将封装单次迭代的逻辑，并将问题数据 $(A,b,\\lambda,l,u,x)$ 作为输入。该函数将计算梯度，执行如上所述的回溯循环，并返回最终的步长和目标值。主脚本将遍历提供的测试套件，并按指定格式化结果。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the projected gradient method.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            # Happy-path, interior iterate\n            \"A\": np.array([[3.0, 0.0, 1.0], [0.0, 2.0, -1.0]]),\n            \"b\": np.array([1.0, -2.0]),\n            \"lambda\": 0.1,\n            \"l\": np.array([-5.0, -5.0, -5.0]),\n            \"u\": np.array([5.0, 5.0, 5.0]),\n            \"x\": np.array([0.5, -0.5, 1.0]),\n        },\n        {\n            # Lower-bound activation\n            \"A\": np.array([[2.0, -1.0], [-1.0, 2.0]]),\n            \"b\": np.array([0.0, 1.0]),\n            \"lambda\": 0.01,\n            \"l\": np.array([0.0, 0.0]),\n            \"u\": np.array([10.0, 10.0]),\n            \"x\": np.array([0.1, 0.2]),\n        },\n        {\n            # Upper-bound activation\n            \"A\": np.array([[1.0, 3.0], [2.0, 0.0]]),\n            \"b\": np.array([4.0, -1.0]),\n            \"lambda\": 0.05,\n            \"l\": np.array([-1.0, -1.0]),\n            \"u\": np.array([0.2, 0.3]),\n            \"x\": np.array([0.19, 0.29]),\n        },\n        {\n            # Zero-gradient edge case\n            \"A\": np.array([[2.0]]),\n            \"b\": np.array([4.0]),\n            \"lambda\": 1.0,\n            \"l\": np.array([0.0]),\n            \"u\": np.array([10.0]),\n            \"x\": np.array([1.6]),\n        },\n        {\n            # Backtracking shrink in a stiff one-dimensional problem\n            \"A\": np.array([[3.0]]),\n            \"b\": np.array([1.0]),\n            \"lambda\": 1.0,\n            \"l\": np.array([-5.0]),\n            \"u\": np.array([5.0]),\n            \"x\": np.array([0.0]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        t, f_plus = projected_gradient_iteration(\n            case[\"A\"], case[\"b\"], case[\"lambda\"],\n            case[\"l\"], case[\"u\"], case[\"x\"]\n        )\n        results.append(f\"[{t:.6f}, {f_plus:.6f}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[[{','.join(results)}]]\")\n\ndef objective_function(A, b, lam, x):\n    \"\"\"Computes the Tikhonov-regularized objective function.\"\"\"\n    residual = A @ x - b\n    loss = 0.5 * np.linalg.norm(residual)**2\n    reg = (lam / 2.0) * np.linalg.norm(x)**2\n    return loss + reg\n\ndef gradient(A, b, lam, x):\n    \"\"\"Computes the gradient of the objective function.\"\"\"\n    return A.T @ (A @ x - b) + lam * x\n\ndef projection(y, l, u):\n    \"\"\"Projects a vector y onto the box [l, u].\"\"\"\n    return np.minimum(np.maximum(y, l), u)\n\ndef projected_gradient_iteration(A, b, lam, l, u, x):\n    \"\"\"\n    Performs one iteration of projected gradient descent with Armijo backtracking.\n    \"\"\"\n    # Backtracking parameters\n    t_0 = 1.0\n    beta = 0.5\n    sigma = 1e-4\n    t_min = 1e-12\n\n    # Compute values at current point x\n    f_x = objective_function(A, b, lam, x)\n    grad_f_x = gradient(A, b, lam, x)\n    \n    t = t_0\n    while True:\n        # Compute projected trial point\n        x_plus_t = projection(x - t * grad_f_x, l, u)\n        \n        # Compute objective at trial point\n        f_x_plus_t = objective_function(A, b, lam, x_plus_t)\n        \n        # Check Armijo condition\n        armijo_rhs = f_x + sigma * np.dot(grad_f_x, x_plus_t - x)\n        \n        if f_x_plus_t = armijo_rhs:\n            # Step size is accepted\n            return t, f_x_plus_t\n        \n        # If condition is not met, check termination for small step\n        if t  t_min:\n            # Terminate because step size is too small, returning last tried values\n            return t, f_x_plus_t\n            \n        # Reduce step size\n        t *= beta\n\nsolve()\n```"
        },
        {
            "introduction": "除了简单的箱型约束，许多数据同化和地球物理学中的高级模型使用更复杂的几何约束，例如二阶锥，来施加鲁棒不确定性边界等属性。任何基于投影的方法的核心都是能够高效地计算到可行集上的投影。 这项练习要求您从第一性原理出发，利用KKT条件推导到二阶锥上的投影公式，这能让您深入理解凸集的几何结构，并掌握解决更广泛约束问题的关键工具。",
            "id": "3371711",
            "problem": "考虑数据同化中逆问题约束公式化产生的一个投影梯度步，其中松弛变量 $t \\in \\mathbb{R}$ 和状态增量 $u \\in \\mathbb{R}^{n-1}$ 受二阶锥 (SOC) $\\mathcal{K} = \\{(s,x) \\in \\mathbb{R} \\times \\mathbb{R}^{n-1} : \\|x\\|_{2} \\le s\\}$ 的约束。到非空、闭、凸集上的欧几里得投影定义为欧几里得距离平方的最小化子。从这个定义出发，仅使用凸优化的基本原理（到闭凸集上投影的存在性和唯一性、范数的凸性以及卡鲁什-库恩-塔克 (Karush–Kuhn–Tucker) 最优性条件），推导任意点 $(t,u) \\in \\mathbb{R} \\times \\mathbb{R}^{n-1}$ 到 $\\mathcal{K}$ 上的欧几里得投影 $\\Pi_{\\mathcal{K}}(t,u)$ 的显式公式。您的推导必须系统地描述和证明所有三种情况：$t > \\|u\\|_{2}$、$t  -\\|u\\|_{2}$ 以及余下的情况，包括拉格朗日乘子的值和最优化子的结构。\n\n最后，对于 $n = 4$ 和 $(t,u) = (1,(2,-1,2))$ 的具体实例，精确计算投影 $\\Pi_{\\mathcal{K}}(t,u)$。不要四舍五入。将您的最终答案表示为一个包含四个条目的单个显式向量表达式，对应于投影后的标量和投影后的 $(n-1)$ 维向量，不带单位。",
            "solution": "问题要求推导点 $(t,u) \\in \\mathbb{R} \\times \\mathbb{R}^{n-1}$ 到二阶锥 $\\mathcal{K}$ 上的欧几里得投影公式，然后将该公式应用于一个具体案例。二阶锥定义为 $\\mathcal{K} = \\{(s,x) \\in \\mathbb{R} \\times \\mathbb{R}^{n-1} : \\|x\\|_{2} \\le s\\}$。\n\n根据定义，欧几里得投影 $\\Pi_{\\mathcal{K}}(t,u)$ 是 $\\mathcal{K}$ 中唯一的点 $(\\hat{s}, \\hat{x})$，它最小化了到 $(t,u)$ 的欧几里得距离的平方。这可以表述为一个凸优化问题：\n$$\n\\begin{aligned}\n \\underset{s,x}{\\text{minimize}}   f(s,x) = \\frac{1}{2} \\|(s,x) - (t,u)\\|_2^2 = \\frac{1}{2} ((s-t)^2 + \\|x-u\\|_2^2) \\\\\n \\text{subject to}   g(s,x) = \\|x\\|_2 - s \\le 0\n\\end{aligned}\n$$\n目标函数 $f(s,x)$ 是严格凸的，约束集 $\\mathcal{K}$ 是一个非空、闭合的凸集。因此，存在唯一解。我们使用卡鲁什-库恩-塔克 (KKT) 条件来找到这个解。\n\n该问题的拉格朗日函数是：\n$$\n\\mathcal{L}(s, x, \\lambda) = \\frac{1}{2} (s-t)^2 + \\frac{1}{2} \\|x-u\\|_2^2 + \\lambda (\\|x\\|_2 - s)\n$$\n其中 $\\lambda \\in \\mathbb{R}$ 是与不等式约束相关的拉格朗日乘子。最优解点 $(\\hat{s}, \\hat{x})$ 和乘子 $\\lambda$ 的 KKT 条件如下：\n\n$1$. **驻点性 (Stationarity)**：拉格朗日函数对原始变量的梯度必须为零。\n$$\n\\nabla_s \\mathcal{L}(\\hat{s}, \\hat{x}, \\lambda) = (\\hat{s}-t) - \\lambda = 0\n$$\n$$\n\\nabla_x \\mathcal{L}(\\hat{s}, \\hat{x}, \\lambda) = (\\hat{x}-u) + \\lambda \\nabla (\\|\\hat{x}\\|_2) = 0\n$$\n$2$. **原始可行性 (Primal Feasibility)**：点 $(\\hat{s}, \\hat{x})$ 必须在可行集中。\n$$\n\\|\\hat{x}\\|_2 - \\hat{s} \\le 0\n$$\n$3$. **对偶可行性 (Dual Feasibility)**：对于形式为 $g(x) \\le 0$ 的不等式约束，拉格朗日乘子必须为非负。\n$$\n\\lambda \\ge 0\n$$\n$4$. **互补松弛性 (Complementary Slackness)**：\n$$\n\\lambda (\\|\\hat{x}\\|_2 - \\hat{s}) = 0\n$$\n\n根据互补松弛性，要么 $\\lambda=0$，要么 $\\|\\hat{x}\\|_2 - \\hat{s} = 0$。我们分析这些情况来刻画解的特征。\n\n**情况 1：点 $(t,u)$ 在锥内部，即 $t \\ge \\|u\\|_2$。**\n我们检验一个假设，即投影就是点本身，$(\\hat{s}, \\hat{x}) = (t,u)$，这意味着约束是非激活的。根据互补松弛性，这将要求 $\\lambda=0$。\n在假设 $t \\ge \\|u\\|_2$ 的前提下，我们检查 $(\\hat{s}, \\hat{x}) = (t,u)$ 和 $\\lambda=0$ 是否满足 KKT 条件。\n$1$. 驻点性：\n$\\hat{s}-t-\\lambda = t-t-0 = 0$。（满足）\n$x$ 的驻点性条件变为 $\\hat{x}-u = u-u = 0$，我们的选择满足该条件。\n$2$. 原始可行性：$\\|\\hat{x}\\|_2 - \\hat{s} = \\|u\\|_2 - t \\le 0$。根据我们对这种情况的初始假设，这是成立的。\n$3$. 对偶可行性：$\\lambda=0 \\ge 0$。（满足）\n$4$. 互补松弛性：$\\lambda(\\|\\hat{x}\\|_2-\\hat{s})=0(\\|u\\|_2-t)=0$。（满足）\n所有 KKT 条件都得到满足。因此，如果 $t \\ge \\|u\\|_2$，该点就是其自身的投影。\n最优化子是 $(\\hat{s}, \\hat{x}) = (t,u)$。拉格朗日乘子是 $\\lambda=0$。这涵盖了所要求的 $t > \\|u\\|_2$ 的情况。\n\n**情况 2：点 $(t,u)$ 在锥外部。**\n这意味着投影 $(\\hat{s}, \\hat{x})$ 必须位于 $\\mathcal{K}$ 的边界上，因此 $\\|\\hat{x}\\|_2 = \\hat{s}$。根据互补松弛性，这允许 $\\lambda>0$。\n\n**情况 2.1：投影是锥的顶点，即 $t \\le -\\|u\\|_2$。**\n我们检验一个假设，即投影是顶点 $(\\hat{s}, \\hat{x}) = (0,0)$。\n$1$. 驻点性：\n从 $\\hat{s}-t-\\lambda=0$ 且 $\\hat{s}=0$，我们得到 $\\lambda = -t$。\n范数 $\\|x\\|_2$ 在 $x=0$ 处的导数不是一个单一向量，而是一个次微分集，$\\partial\\|0\\|_2 = \\{z \\in \\mathbb{R}^{n-1} : \\|z\\|_2 \\le 1\\}$。$x$ 的驻点性条件变为 $0-u + \\lambda z = 0$，其中某个 $z \\in \\partial\\|0\\|_2$，这意味着 $u=\\lambda z$。\n$2$. 原始可行性：$\\|\\hat{x}\\|_2 - \\hat{s} = \\|0\\|_2 - 0 = 0 \\le 0$。（满足）\n$3$. 对偶可行性：$\\lambda=-t \\ge 0$，这意味着 $t \\le 0$。\n$4$. 互补松弛性：$\\lambda (0-0)=0$。（满足）\n这些条件要求 $u=\\lambda z = (-t)z$ 且 $t \\le 0$。取欧几里得范数可得 $\\|u\\|_2 = \\|(-t)z\\|_2 = |-t| \\|z\\|_2$。由于 $t \\le 0$，我们有 $|-t|=-t$。因此，$\\|u\\|_2 = (-t)\\|z\\|_2$。由于 $\\|z\\|_2 \\le 1$，这导致条件 $\\|u\\|_2 \\le -t$，即 $t \\le -\\|u\\|_2$。\n所以，如果 $t \\le -\\|u\\|_2$，投影就是 $(\\hat{s}, \\hat{x}) = (0,0)$。\n最优化子是 $(\\hat{s}, \\hat{x}) = (0,0)$。拉格朗日乘子是 $\\lambda=-t$。这涵盖了所要求的 $t  -\\|u\\|_2$ 的情况。\n\n**情况 2.2：投影在锥的侧面，即 $-\\|u\\|_2  t  \\|u\\|_2$。**\n在这种情况下，投影在边界上，因此 $\\|\\hat{x}\\|_2 = \\hat{s}$，但不是顶点，所以 $\\hat{x} \\ne 0$。梯度 $\\nabla (\\|\\hat{x}\\|_2)$ 是良定义的，等于 $\\hat{x}/\\|\\hat{x}\\|_2$。我们假设 $u \\ne 0$，因为对于这种情况，$u=0$ 意味着 $t=0$，而 $(0,0)$ 会投影到其自身。\nKKT 条件是：\n$1$. $\\hat{s} = t+\\lambda$。\n$2$. $\\hat{x}-u + \\lambda \\frac{\\hat{x}}{\\|\\hat{x}\\|_2}=0 \\implies u = \\hat{x}\\left(1+\\frac{\\lambda}{\\|\\hat{x}\\|_2}\\right)$。这表明 $u$ 和 $\\hat{x}$ 共线。我们可以写成 $\\hat{x}=\\alpha u$，其中某个 $\\alpha>0$。\n$3$. $\\lambda>0$。\n$4$. $\\|\\hat{x}\\|_2 = \\hat{s}$。\n将 $\\hat{x}=\\alpha u$ 代入第二个驻点性条件，我们得到 $u = \\alpha u(1+\\frac{\\lambda}{\\|\\alpha u\\|_2}) \\implies 1 = \\alpha(1+\\frac{\\lambda}{\\alpha\\|u\\|_2}) = \\alpha + \\frac{\\lambda}{\\|u\\|_2}$。因此，$\\alpha=1-\\frac{\\lambda}{\\|u\\|_2}$。\n现在我们使用边界条件 $\\|\\hat{x}\\|_2 = \\hat{s}$ 以及 $\\hat{s}=t+\\lambda$：\n$\\|\\alpha u\\|_2 = t+\\lambda \\implies \\alpha\\|u\\|_2 = t+\\lambda$。\n代入 $\\alpha$ 的表达式：\n$(1-\\frac{\\lambda}{\\|u\\|_2})\\|u\\|_2 = t+\\lambda \\implies \\|u\\|_2 - \\lambda = t+\\lambda$。\n解出 $\\lambda$ 得 $2\\lambda = \\|u\\|_2-t$，所以 $\\lambda = \\frac{\\|u\\|_2-t}{2}$。\n条件 $\\lambda>0$ 意味着 $\\|u\\|_2-t>0$，即 $t\\|u\\|_2$。此外，对于 $\\alpha>0$，我们需要 $\\lambda  \\|u\\|_2$，这意味着 $\\frac{\\|u\\|_2-t}{2}  \\|u\\|_2 \\implies \\|u\\|_2-t  2\\|u\\|_2 \\implies -t  \\|u\\|_2$，即 $t > -\\|u\\|_2$。\n这证实了此情况在 $-\\|u\\|_2  t  \\|u\\|_2$ 时的有效性。\n现在我们求最优化子 $(\\hat{s}, \\hat{x})$：\n$\\hat{s} = t+\\lambda = t + \\frac{\\|u\\|_2-t}{2} = \\frac{t+\\|u\\|_2}{2}$。\n为了求 $\\hat{x}$，我们首先求 $\\alpha$：\n$\\alpha = 1 - \\frac{\\lambda}{\\|u\\|_2} = 1 - \\frac{(\\|u\\|_2-t)/2}{\\|u\\|_2} = 1 - \\frac{\\|u\\|_2-t}{2\\|u\\|_2} = \\frac{2\\|u\\|_2 - (\\|u\\|_2-t)}{2\\|u\\|_2} = \\frac{t+\\|u\\|_2}{2\\|u\\|_2}$。\n$\\hat{x} = \\alpha u = \\left(\\frac{t+\\|u\\|_2}{2\\|u\\|_2}\\right) u$。\n最优化子是 $(\\hat{s}, \\hat{x}) = \\left(\\frac{t+\\|u\\|_2}{2}, \\left(\\frac{t+\\|u\\|_2}{2\\|u\\|_2}\\right)u\\right)$。\n拉格朗日乘子是 $\\lambda = \\frac{\\|u\\|_2-t}{2}$。\n\n**具体实例计算**\n给定 $n=4$ 和点 $(t,u) = (1, (2,-1,2))$。\n首先，我们计算 $u$ 的范数：\n$$\n\\|u\\|_2 = \\sqrt{2^2+(-1)^2+2^2} = \\sqrt{4+1+4} = \\sqrt{9} = 3\n$$\n我们有 $t=1$。我们将 $t$ 与 $\\|u\\|_2$ 进行比较：\n$t=1$ 和 $\\|u\\|_2=3$。\n由于 $-3 \\le 1 \\le 3$，我们有 $-\\|u\\|_2 \\le t \\le \\|u\\|_2$。这属于情况 2.2。\n投影由以下公式给出：\n$$\n(\\hat{s}, \\hat{x}) = \\left(\\frac{t+\\|u\\|_2}{2}, \\left(\\frac{t+\\|u\\|_2}{2\\|u\\|_2}\\right)u\\right)\n$$\n代入数值 $t=1$，$\\|u\\|_2=3$ 和 $u=(2,-1,2)$：\n标量分量 $\\hat{s}$ 是：\n$$\n\\hat{s} = \\frac{1+3}{2} = \\frac{4}{2} = 2\n$$\n向量分量 $\\hat{x}$ 是：\n$$\n\\hat{x} = \\left(\\frac{1+3}{2 \\cdot 3}\\right)u = \\frac{4}{6}u = \\frac{2}{3}u = \\frac{2}{3}(2, -1, 2) = \\left(\\frac{4}{3}, -\\frac{2}{3}, \\frac{4}{3}\\right)\n$$\n投影点 $\\Pi_{\\mathcal{K}}(t,u)$ 是一个由 $(\\hat{s}, \\hat{x})$ 构成的 $\\mathbb{R}^4$ 中的向量。\n$$\n\\Pi_{\\mathcal{K}}(1, (2,-1,2)) = \\left(2, \\frac{4}{3}, -\\frac{2}{3}, \\frac{4}{3}\\right)\n$$\n这就是该具体实例的最终结果。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 \\\\ \\frac{4}{3} \\\\ -\\frac{2}{3} \\\\ \\frac{4}{3}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "约束公式的一个主要动机是促进解具有特定结构，例如稀疏性。虽然稀疏性的真实度量——$\\ell_0$“范数”——在计算上是难解的，但我们通常使用其最紧的凸代理，即$\\ell_1$范数。理解这种凸松弛何时有效、何时可能失败至关重要。 通过一个精心构造的反例，这个问题展示了$\\ell_1$最小化无法恢复最稀疏解的场景，让您能够通过分析正向算子的性质来探索凸松弛的理论局限，并对逆问题中约束的选择形成批判性视角。",
            "id": "3371703",
            "problem": "考虑一个线性逆问题，其中包含一个合成变换 $W \\in \\mathbb{R}^{n \\times n}$ 和测量值 $b \\in \\mathbb{R}^m$，它们通过 $b = A x_{\\star}$ 相关联，其中 $x_{\\star} \\in \\mathbb{R}^n$ 是某个未知向量，$A \\in \\mathbb{R}^{m \\times n}$ 且 $m  n$。我们比较基数约束公式（寻求满足 $A x = b$ 且 $\\|W x\\|_{0} \\le s$ 的任何 $x$）与凸代理公式（用 $\\|W x\\|_{1}$ 替换 $\\|W x\\|_{0}$）。专注于 $W = I_n$（单位矩阵）的情况，此时稀疏性直接在系数向量 $x$ 中衡量。\n\n设 $m = 3$，$n = 5$，并考虑显式矩阵 $A = [a_1 \\; a_2 \\; a_3 \\; a_4 \\; a_5] \\in \\mathbb{R}^{3 \\times 5}$，其列向量为\n$$\na_1 = \\begin{bmatrix} 0.9 \\\\ 0.05 \\\\ 0 \\end{bmatrix},\\quad\na_2 = \\begin{bmatrix} 0.6 \\\\ -0.05 \\\\ 0.1 \\end{bmatrix},\\quad\na_3 = \\begin{bmatrix} 1 \\\\ 0.2 \\\\ 0 \\end{bmatrix},\\quad\na_4 = \\begin{bmatrix} 1 \\\\ -0.2 \\\\ 0 \\end{bmatrix},\\quad\na_5 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0.2 \\end{bmatrix}.\n$$\n设真实值为 $x_{\\star} = \\begin{bmatrix} 1  1  0  0  0 \\end{bmatrix}^{\\top}$，测量值为 $b = A x_{\\star}$。定义稀疏度预算 $s = 2$。注意，互相关性 $\\mu(A)$ 定义为 $A$ 的不同归一化列向量之间的最大绝对内积。\n\n在此设定下，选择所有正确的陈述：\n- A. 对于 $s = 2$ 的基数约束逆问题，存在唯一的零残差可行解 $x_{\\star}$ 满足 $\\|x\\|_{0} \\le 2$。\n\n- B. 存在 $\\|\\tilde{x}\\|_{0} = 3$ 的 $\\tilde{x}$，使得 $A \\tilde{x} = b$ 且 $\\|\\tilde{x}\\|_{1}  \\|x_{\\star}\\|_{1}$，因此在约束 $A x = b$ 下最小化 $\\|x\\|_{1}$ 不可能得到 $x_{\\star}$。\n\n- C. 对于任何满足 $1.5  \\tau  2$ 的阈值 $\\tau$，凸约束问题 $\\min_{x} \\|A x - b\\|_{2}$（约束条件为 $\\|x\\|_{1} \\le \\tau$）是可行的，但无法得到 $x_{\\star}$；因此，对于这样的 $\\tau$，凸代理无法恢复可行的 $x_{\\star}$。\n\n- D. 互相关性 $\\mu(A)$ 非常高（数值上接近 $1$），并且稀疏度 $s=2$ 时 $\\ell_1$ 恢复的基于相关性的标准充分条件被违反，因此观察到的凸代理失败与基于相关性的理论是一致的。\n\n- E. 选择 $\\tau = \\|x_{\\star}\\|_{1}$ 可以保证凸约束问题 $\\min_{x} \\|A x - b\\|_{2}$（约束条件为 $\\|x\\|_{1} \\le \\tau$）的所有零残差解都与 $x_{\\star}$ 重合，因此当 $\\tau \\ge \\|x_{\\star}\\|_{1}$ 时，凸代理总是能恢复 $x_{\\star}$。\n\n通过选择正确的选项来回答。",
            "solution": "我们从基数约束和凸代理公式的核心定义、过完备系统中的表示的线性代数以及互相关性出发。当 $W = I_n$ 时，约束条件 $\\|x\\|_0 \\le s$ 和 $\\|x\\|_1 \\le \\tau$ 分别简化为对向量 $x$ 本身的稀疏性度量。测量模型为 $b = A x_{\\star}$。\n\n步骤 1：计算 $b$ 并验证 $x_{\\star}$ 的稀疏性。\n根据构造，\n$$\nb = A x_{\\star} = a_1 + a_2\n= \\begin{bmatrix} 0.9 \\\\ 0.05 \\\\ 0 \\end{bmatrix}\n+ \\begin{bmatrix} 0.6 \\\\ -0.05 \\\\ 0.1 \\end{bmatrix}\n= \\begin{bmatrix} 1.5 \\\\ 0 \\\\ 0.1 \\end{bmatrix}。\n$$\n我们有 $\\|x_{\\star}\\|_0 = 2$ 且 $\\|x_{\\star}\\|_1 = |1| + |1| = 2$。\n\n步骤 2：$s=2$ 时最小基数表示的唯一性。\n我们必须证明除了 $x_{\\star}$ 之外，没有其他 2-稀疏的向量 $x$ 满足 $A x = b$。等价地，我们必须证明除了列向量对 $\\{a_1, a_2\\}$ 之外，$b$ 无法由任何其他列向量对表示。\n\n观察到 $b$ 的第三个分量是 $0.1$。在所有列向量中，只有 $a_2$ 和 $a_5$ 的第三个分量非零（分别为 $0.1$ 和 $0.2$）。因此：\n- 任何不包含索引 2 或 5 的列向量对 $\\{i,j\\}$ 都无法表示 $b$，因为它们在第三个分量上的张成空间仅为 $0$，不可能等于 $0.1$。\n- 因此，任何其他的解向量对都必须包含索引 2 或索引 5（或两者都有）。\n\n分情况讨论：\n\n(i) 包含索引 5 但不包含 2 的列向量对：设 $x = \\alpha e_5 + \\beta e_j$，其中 $j \\in \\{1,3,4\\}$。匹配第三个分量可得 $0.2 \\alpha = 0.1$，因此 $\\alpha = 0.5$。现在：\n- 若 $j = 3$，第二个分量为 $0.2 \\beta$，必须等于 $0$，所以 $\\beta = 0$。但此时第一个分量为 $1 \\cdot 0.5 = 0.5 \\ne 1.5$，产生矛盾。\n- 若 $j = 4$，第二个分量为 $-0.2 \\beta = 0$，所以 $\\beta = 0$，同样地，第一个分量不匹配。\n- 若 $j = 1$，第二个分量为 $0.05 \\beta = 0$，所以 $\\beta = 0$，同样地，第一个分量不匹配。\n\n(ii) 列向量对 $\\{2,5\\}$：设 $x = \\alpha e_2 + \\beta e_5$。匹配第二个分量得到 $-0.05 \\alpha + 0 \\cdot \\beta = 0$，所以 $\\alpha = 0$。然后从第三个分量 $0.2 \\beta = 0.1$ 我们得到 $\\beta = 0.5$，但第一个分量变为 $1 \\cdot 0.5 = 0.5 \\ne 1.5$，产生矛盾。\n\n(iii) 包含索引 2 但不包含 5 的列向量对：设 $x = \\alpha e_2 + \\beta e_j$，其中 $j \\in \\{1,3,4\\}$。匹配第三个分量得到 $0.1 \\alpha = 0.1$，所以 $\\alpha = 1$。\n- 若 $j = 1$，第二个分量为 $-0.05 + 0.05 \\beta = 0$，因此 $\\beta = 1$。这得到 $x = e_2 + e_1 = x_{\\star}$，即已知的表示。\n- 若 $j = 3$，第二个分量 $-0.05 + 0.2 \\beta = 0$ 意味着 $\\beta = 0.25$，但此时第一个分量为 $0.6 + 1 \\cdot 0.25 = 0.85 \\ne 1.5$，产生矛盾。\n- 若 $j = 4$，第二个分量 $-0.05 - 0.2 \\beta = 0$ 意味着 $\\beta = -0.25$，但此时第一个分量为 $0.6 + 1 \\cdot (-0.25) = 0.35 \\ne 1.5$，产生矛盾。\n\n因此，唯一的 2-稀疏表示是 $x_{\\star}$，且对于 $s=2$ 的基数约束问题存在唯一的零残差解 $x_{\\star}$。\n\n选项 A 的结论：正确。\n\n步骤 3：构造一个具有更小 $\\ell_1$ 范数的更稠密的解。\n考虑 $\\tilde{x} = \\begin{bmatrix} 0  0  0.5  0.5  0.5 \\end{bmatrix}^{\\top}$。计算\n$$\nA \\tilde{x} = 0.5 a_3 + 0.5 a_4 + 0.5 a_5\n= 0.5 \\left( \\begin{bmatrix} 1 \\\\ 0.2 \\\\ 0 \\end{bmatrix}\n+ \\begin{bmatrix} 1 \\\\ -0.2 \\\\ 0 \\end{bmatrix}\n+ \\begin{bmatrix} 1 \\\\ 0 \\\\ 0.2 \\end{bmatrix} \\right)\n= 0.5 \\begin{bmatrix} 3 \\\\ 0 \\\\ 0.2 \\end{bmatrix}\n= \\begin{bmatrix} 1.5 \\\\ 0 \\\\ 0.1 \\end{bmatrix}\n= b.\n$$\n因此 $\\tilde{x}$ 是一个可行解，其稀疏度为 $\\|\\tilde{x}\\|_0 = 3$，$\\ell_1$ 范数为 $\\|\\tilde{x}\\|_1 = |0.5| + |0.5| + |0.5| = 1.5  \\|x_{\\star}\\|_1 = 2$。\n\n因此，凸规划问题 $\\min_{x} \\|x\\|_1$（约束为 $A x = b$）的任何解都不可能等于 $x_{\\star}$，因为存在一个 $\\ell_1$ 范数严格更小的可行点。这建立了一个具体的例子，说明即使存在唯一的 2-稀疏（基数约束）解 $x_{\\star}$，凸代理也未能恢复它。\n\n选项 B 的结论：正确。\n\n步骤 4：分析对于阈值 $1.5  \\tau  2$ 的 $\\ell_1$ 约束公式。\n对于任何满足 $1.5  \\tau  2$ 的阈值 $\\tau$：\n- 可行性：$\\tilde{x}$ 的范数为 $\\|\\tilde{x}\\|_1 = 1.5  \\tau$，且 $A \\tilde{x} = b$，因此约束集 $\\{x: \\|x\\|_1 \\le \\tau\\}$ 包含一个零残差解，问题 $\\min_{x} \\|A x - b\\|_2$（约束为 $\\|x\\|_1 \\le \\tau$）是可行的，且最优值为 $0$。\n- 排除 $x_{\\star}$：由于 $\\|x_{\\star}\\|_1 = 2$ 且 $\\tau  2$，向量 $x_{\\star}$ 在此约束下是不可行的。因此，对于这样的 $\\tau$，即使存在可行的基数约束解 $x_{\\star}$，凸代理也无法返回 $x_{\\star}$。\n\n选项 C 的结论：正确。\n\n步骤 5：计算互相关性并解释恢复条件。\n互相关性 $\\mu(A)$ 是 $A$ 的不同归一化列向量之间的最大绝对内积。计算范数：\n$$\n\\|a_1\\|_2 = \\sqrt{0.9^2 + 0.05^2} = \\sqrt{0.8125},\\quad\n\\|a_2\\|_2 = \\sqrt{0.6^2 + (-0.05)^2 + 0.1^2} = \\sqrt{0.3725},\\quad\n\\|a_3\\|_2 = \\|a_4\\|_2 = \\|a_5\\|_2 = \\sqrt{1.04}。\n$$\n一些归一化内积非常接近 $1$。例如：\n$$\n\\frac{\\langle a_2, a_5 \\rangle}{\\|a_2\\|_2 \\|a_5\\|_2}\n= \\frac{0.62}{\\sqrt{0.3725} \\sqrt{1.04}}\n\\approx \\frac{0.62}{0.6103 \\times 1.0198} \\approx 0.996,\n$$\n以及\n$$\n\\frac{\\langle a_1, a_3 \\rangle}{\\|a_1\\|_2 \\|a_3\\|_2}\n= \\frac{0.91}{\\sqrt{0.8125} \\sqrt{1.04}} \\approx 0.989。\n$$\n因此 $\\mu(A)$ 在数值上非常接近 $1$（约 $0.996$）。一个关于使用互相关性进行 $\\ell_1$ 最小化精确恢复的充分条件断言：如果一个向量 $x$ 是 $s$-稀疏的，并且满足 $s  \\frac{1 + 1/\\mu(A)}{2}$，那么 $\\ell_1$ 最小化可以精确恢复它。对于 $\\mu(A) \\approx 0.996$，该条件右侧大约是：\n$$\n\\frac{1 + 1/0.996}{2} \\approx \\frac{1 + 1.004}{2} \\approx 1.002,\n$$\n这个值不大于 $s = 2$。因此，该充分条件被违反，$\\ell_1$ 恢复的失败与基于相关性的理论是一致的。此外，已知当 $s \\ge \\frac{1 + 1/\\mu(A)}{2}$ 时，存在 $\\ell_1$ 无法恢复的 $s$-稀疏信号；我们构造的实例恰恰展示了这样一种失败情况。\n\n选项 D 的结论：正确。\n\n步骤 6：关于在 $\\tau = \\|x_{\\star}\\|_1$ 处 $\\ell_1$ 约束阈值的影响。\n如果 $\\tau = \\|x_{\\star}\\|_1 = 2$，那么 $x_{\\star}$ 是可行的，但 $\\tilde{x}$ 也是可行的，且其 $\\ell_1$ 范数严格更小，为 $1.5$。在约束问题 $\\min_{x} \\|A x - b\\|_2$（约束为 $\\|x\\|_1 \\le \\tau$）中，任何满足 $A x = b$ 和 $\\|x\\|_1 \\le \\tau$ 的 $x$ 都会得到零残差；除非明确设定为双标准选择，否则该问题本身并不强制要求在可行点中选择最小的 $\\ell_1$ 范数。因此，无法保证解会与 $x_{\\star}$ 重合。在相关的基追踪公式 $\\min_{x} \\|x\\|_1$（约束为 $A x = b$）中，$x_{\\star}$ 不可能是最优解，因为存在一个 $\\ell_1$ 范数严格更小的可行点。因此，选择 $\\tau = \\|x_{\\star}\\|_1$ 能保证恢复 $x_{\\star}$ 的说法是错误的。\n\n选项 E 的结论：不正确。\n\n逐项分析总结：\n- A: 正确。唯一的 2-稀疏零残差解是 $x_{\\star}$。\n- B: 正确。构造的 $\\tilde{x}$ 表明 $\\ell_1$ 最小化未能返回 $x_{\\star}$。\n- C: 正确。对于任何 $1.5  \\tau  2$，凸约束排除了 $x_{\\star}$，同时允许存在零残差的替代解。\n- D: 正确。$\\mu(A)$ 接近 1，违反了 $s=2$ 时基于相关性的恢复条件。\n- E: 不正确。$\\tau = \\|x_{\\star}\\|_1$ 并不能保证所有零残差解都与 $x_{\\star}$ 重合，也不能使 $x_{\\star}$ 成为 $\\ell_1$ 最小化问题的解。",
            "answer": "$$\\boxed{ABCD}$$"
        }
    ]
}