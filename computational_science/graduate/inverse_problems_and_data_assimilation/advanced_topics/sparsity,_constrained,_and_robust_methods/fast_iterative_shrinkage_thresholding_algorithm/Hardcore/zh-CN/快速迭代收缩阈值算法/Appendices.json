{
    "hands_on_practices": [
        {
            "introduction": "快速迭代收缩阈值算法（FISTA）的核心是利用近端算子来处理像 $\\ell_1$ 范数这样的非光滑项。本练习聚焦于这一核心组件，让您亲手实践软阈值操作，这是在解中促进稀疏性的关键。掌握这一计算是理解 FISTA 行为的第一步。",
            "id": "3446917",
            "problem": "考虑最小化函数 $F(x) = g(x) + \\lambda \\|x\\|_{1}$ 的复合凸优化问题，其中 $g(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ 且 $\\|x\\|_{1} = \\sum_{i} |x_{i}|$。在快速迭代收缩阈值算法 (FISTA) 中，一次迭代使用缩放的 $\\ell_{1}$-范数的近端映射。假设当前梯度步长已产生向量 $z \\in \\mathbb{R}^{7}$，并且你需要计算近端点\n$$\nx = \\operatorname{prox}_{\\tau \\lambda \\|\\cdot\\|_{1}}(z),\n$$\n其定义为函数\n$$\nx \\mapsto \\tau \\lambda \\|x\\|_{1} + \\frac{1}{2}\\|x - z\\|_{2}^{2}\n$$\n的唯一最小化子。设给定数据为\n$$\nz = \\left(1, -\\frac{1}{4}, \\frac{2}{5}, -\\frac{2}{5}, \\frac{7}{15}, -\\frac{9}{10}, 0\\right), \\quad \\lambda = \\frac{3}{5}, \\quad \\tau = \\frac{2}{3}.\n$$\n以最简有理数形式计算精确向量 $x = \\operatorname{prox}_{\\tau \\lambda \\|\\cdot\\|_{1}}(z)$。请将最终答案以单个行向量的形式给出。不要近似；无需四舍五入。",
            "solution": "问题要求计算近端点 $x = \\operatorname{prox}_{\\tau \\lambda \\|\\cdot\\|_{1}}(z)$，它被定义为以下优化问题的唯一最小化子：\n$$\n\\min_{x \\in \\mathbb{R}^{7}} \\left( \\tau \\lambda \\|x\\|_{1} + \\frac{1}{2}\\|x - z\\|_{2}^{2} \\right)\n$$\n该目标函数是可分的，这意味着我们可以独立地对每个分量 $x_i$ 进行最小化：\n$$\nx_i = \\arg\\min_{u \\in \\mathbb{R}} \\left( \\tau \\lambda |u| + \\frac{1}{2}(u - z_i)^2 \\right) \\quad \\text{for } i = 1, \\dots, 7\n$$\n这个一维问题的解由软阈值算子 $S_{\\alpha}(v)$ 给出，其中 $\\alpha = \\tau\\lambda$，其定义为：\n$$\nS_{\\alpha}(v) = \\operatorname{sign}(v) \\max(|v| - \\alpha, 0)\n$$\n这可以分段表示为：\n$$\nS_{\\alpha}(v) = \\begin{cases} v - \\alpha  \\text{if } v  \\alpha \\\\ 0  \\text{if } |v| \\le \\alpha \\\\ v + \\alpha  \\text{if } v  -\\alpha \\end{cases}\n$$\n首先，我们计算阈值参数 $\\alpha$：\n$$\n\\alpha = \\tau \\lambda = \\left(\\frac{2}{3}\\right) \\left(\\frac{3}{5}\\right) = \\frac{2}{5}\n$$\n现在，我们将软阈值算子 $S_{2/5}$ 应用于向量 $z$ 的每个分量：\n$$\nz = \\left(1, -\\frac{1}{4}, \\frac{2}{5}, -\\frac{2}{5}, \\frac{7}{15}, -\\frac{9}{10}, 0\\right)\n$$\n我们逐个计算每个分量 $x_i = S_{2/5}(z_i)$：\n\n对于 $i=1$：$z_1 = 1$。由于 $1  \\frac{2}{5}$，我们有：\n$x_1 = 1 - \\frac{2}{5} = \\frac{3}{5}$。\n\n对于 $i=2$：$z_2 = -\\frac{1}{4}$。由于 $|-\\frac{1}{4}| = \\frac{1}{4}  \\frac{2}{5}$，我们有：\n$x_2 = 0$。\n\n对于 $i=3$：$z_3 = \\frac{2}{5}$。由于 $|\\frac{2}{5}| = \\frac{2}{5}$，满足 $|z_3| \\le \\alpha$，因此：\n$x_3 = 0$。\n\n对于 $i=4$：$z_4 = -\\frac{2}{5}$。由于 $|-\\frac{2}{5}| = \\frac{2}{5}$，满足 $|z_4| \\le \\alpha$，因此：\n$x_4 = 0$。\n\n对于 $i=5$：$z_5 = \\frac{7}{15}$。由于 $\\frac{7}{15} > \\frac{6}{15} = \\frac{2}{5}$，我们有：\n$x_5 = \\frac{7}{15} - \\frac{2}{5} = \\frac{7}{15} - \\frac{6}{15} = \\frac{1}{15}$。\n\n对于 $i=6$：$z_6 = -\\frac{9}{10}$。由于 $-\\frac{9}{10}  -\\frac{4}{10} = -\\frac{2}{5}$，我们有：\n$x_6 = -\\frac{9}{10} + \\frac{2}{5} = -\\frac{9}{10} + \\frac{4}{10} = -\\frac{5}{10} = -\\frac{1}{2}$。\n\n对于 $i=7$：$z_7 = 0$。由于 $|0| \\le \\frac{2}{5}$，因此：\n$x_7 = 0$。\n\n组合这些分量，我们得到结果向量 $x$：\n$$\nx = \\left(\\frac{3}{5}, 0, 0, 0, \\frac{1}{15}, -\\frac{1}{2}, 0\\right)\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{3}{5}  0  0  0  \\frac{1}{15}  -\\frac{1}{2}  0 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "理解了近端映射之后，下一步是观察它如何融入一个完整的算法循环中。本问题将引导您完整地完成单次 FISTA 迭代，从 Nesterov 动量步到最终的近端梯度更新。通过亲手执行这些计算，您将对算法如何向解逼近获得一个具体的认识。",
            "id": "3446895",
            "problem": "考虑一维的最小绝对收缩和选择算子 (LASSO) 问题，其目标是最小化复合函数 $F(x) = g(x) + h(x)$，其中 $g(x) = \\frac{1}{2}\\|Ax - b\\|_{2}^{2}$ 且 $h(x) = \\lambda |x|$。设测量矩阵为标量 $A = a$，其中 $a = 2$，设 $b = 3$，且 $\\lambda = 1$。快速迭代收缩阈值算法 (FISTA) 的初始化迭代值为 $x^{k-1} = 0$、$x^{k} = 1$，加速参数为 $t_{k} = 2$。使用以下具有科学依据的定义：\n\n- 光滑项 $g$ 的梯度定义为 $\\nabla g(x) = A^{\\top}(Ax - b)$。\n- 梯度 $\\nabla g$ 的 Lipschitz 常数 $L$ 是 $A$ 的谱范数的平方，在一维情况下为 $L = a^{2}$。\n- $h$ 的近端算子（参数为 $\\tau  0$）定义为 $\\operatorname{prox}_{\\tau h}(z) = \\arg\\min_{x}\\left\\{\\frac{1}{2}(x - z)^{2} + \\tau \\lambda |x|\\right\\}$。\n- FISTA 的标准 Nesterov 加速序列满足隐式关系 $t_{k+1}^{2} - t_{k+1} = t_{k}^{2}$。\n\n对此 LASSO 实例执行一次完整的 FISTA 迭代，过程如下：首先根据上述隐式关系确定 $t_{k+1}$；然后使用 $x^{k}$、$x^{k-1}$、$t_{k}$ 和 $t_{k+1}$ 构造外推点 $y^{k}$；计算梯度 $\\nabla g(y^{k})$；执行梯度步 $z^{k} = y^{k} - \\frac{1}{L}\\nabla g(y^{k})$；最后应用参数为 $\\tau = \\frac{1}{L}$ 的近端算子以获得 $x^{k+1} = \\operatorname{prox}_{\\frac{1}{L}h}(z^{k})$。请按 $y^{k}$、$\\nabla g(y^{k})$、$x^{k+1}$ 和 $t_{k+1}$ 的顺序，以精确值（无四舍五入）表示您的最终结果。",
            "solution": "本题要求对给定的一维 LASSO 问题执行一次完整的 FISTA 迭代。我们将遵循问题中定义的步骤进行计算。\n\n**1. 计算基本常数**\n根据问题定义，光滑项 $g(x) = \\frac{1}{2}(ax - b)^2$ 的梯度 $\\nabla g$ 的 Lipschitz 常数 $L$ 为 $L = a^2$。给定 $a=2$，我们得到：\n$$L = 2^2 = 4$$\n因此，算法中使用的步长为 $\\frac{1}{L} = \\frac{1}{4}$。\n\n**2. 更新动量参数 $t_{k+1}$**\n动量参数根据关系式 $t_{k+1}^{2} - t_{k+1} = t_{k}^{2}$ 更新。已知 $t_k=2$，我们需求解二次方程 $t_{k+1}^{2} - t_{k+1} - 4 = 0$。取正根可得：\n$$t_{k+1} = \\frac{1 + \\sqrt{1 - 4(1)(-4)}}{2} = \\frac{1 + \\sqrt{17}}{2}$$\n\n**3. 计算外推点 $y^{k}$**\n外推点 $y^k$ 是根据 $x^k=1$、$x^{k-1}=0$ 和 $t_k=2$ 计算的：\n$$y^{k} = x^{k} + \\frac{t_{k}-1}{t_{k+1}}(x^{k}-x^{k-1}) = 1 + \\frac{2-1}{\\frac{1 + \\sqrt{17}}{2}}(1 - 0) = 1 + \\frac{2}{1 + \\sqrt{17}}$$\n对分母进行有理化，得到：\n$$y^{k} = 1 + \\frac{2(1 - \\sqrt{17})}{(1 + \\sqrt{17})(1 - \\sqrt{17})} = 1 + \\frac{\\sqrt{17} - 1}{8} = \\frac{7 + \\sqrt{17}}{8}$$\n\n**4. 计算梯度 $\\nabla g(y^{k})$**\n光滑项的梯度为 $\\nabla g(x) = a(ax - b) = 2(2x - 3)$。在 $y^k$ 处求值：\n$$\\nabla g(y^{k}) = 2\\left(2\\left(\\frac{7 + \\sqrt{17}}{8}\\right) - 3\\right) = 2\\left(\\frac{7 + \\sqrt{17}}{4} - \\frac{12}{4}\\right) = \\frac{\\sqrt{17} - 5}{2}$$\n\n**5. 执行近端梯度步得到 $x^{k+1}$**\n首先，执行梯度下降步得到中间点 $z^k$：\n$$z^{k} = y^{k} - \\frac{1}{L}\\nabla g(y^{k}) = \\frac{7 + \\sqrt{17}}{8} - \\frac{1}{4}\\left(\\frac{\\sqrt{17} - 5}{2}\\right) = \\frac{7 + \\sqrt{17} - (\\sqrt{17} - 5)}{8} = \\frac{12}{8} = \\frac{3}{2}$$\n然后，应用软阈值算子得到 $x^{k+1}$，阈值为 $\\frac{\\lambda}{L} = \\frac{1}{4}$：\n$$x^{k+1} = \\operatorname{prox}_{\\frac{1}{L}h}(z^{k}) = \\operatorname{soft}\\left(\\frac{3}{2}, \\frac{1}{4}\\right) = \\frac{3}{2} - \\frac{1}{4} = \\frac{5}{4}$$\n\n将计算出的四个量按要求顺序整理，即为最终答案。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{7 + \\sqrt{17}}{8}  \\frac{\\sqrt{17} - 5}{2}  \\frac{5}{4}  \\frac{1 + \\sqrt{17}}{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "在许多实际应用中，梯度的 Lipschitz 常数是未知的，这使得固定步长不切实际。本实践问题将从理论计算过渡到编程实现，要求您构建一个带有回溯线搜索的 FISTA 算法，以自适应地确定步长。这个练习将巩固您关于如何为解决实际问题而创建一个稳健高效算法的理解。",
            "id": "3446933",
            "problem": "要求您实现一个基于凸分析的数值方法，以解决一系列合成的稀疏重构问题。考虑最小绝对值收缩和选择算子 (LASSO) 的目标函数\n$$\n\\min_{x \\in \\mathbb{R}^n} F(x) \\equiv g(x) + h(x),\n$$\n其中光滑部分为\n$$\ng(x) \\equiv \\tfrac{1}{2}\\|A x - b\\|_2^2\n$$\n其梯度为\n$$\n\\nabla g(x) = A^\\top (A x - b),\n$$\n非光滑正则化项是缩放的 1-范数\n$$\nh(x) \\equiv \\lambda \\|x\\|_1.\n$$\n$g$ 的梯度是全局 Lipschitz 连续的，存在某个常数 $L^\\star \\in (0,\\infty)$ 满足\n$$\n\\|\\nabla g(x) - \\nabla g(y)\\|_2 \\le L^\\star \\|x - y\\|_2\n$$\n对于所有 $x,y \\in \\mathbb{R}^n$。在此设定下，带回溯的快速迭代收缩阈值算法 (FISTA) 使用以下基本原理：\n- 由 $\\nabla g$ 的 Lipschitz 连续性所隐含的二次上界，它为接受候选步长提供了一个充分下降条件。\n- 缩放的 1-范数的近端算子，\n$$\n\\operatorname{prox}_{\\tau \\|\\cdot\\|_1}(v) = \\arg\\min_{x \\in \\mathbb{R}^n}\\left\\{\\tfrac{1}{2}\\|x - v\\|_2^2 + \\tau \\|x\\|_1\\right\\},\n$$\n也就是软阈值映射。\n- Nesterov 的加速序列，其中 $t_1 = 1$，以及基于 $t_k$ 构建的外推。\n\n您的任务是：\n- 从上述基础出发，根据 $\\nabla g$ 的 Lipschitz 属性推导用于回溯的充分下降条件，以验证关于当前外推点的 $g$ 的局部二次上界模型。\n- 实现用于 LASSO 目标函数 $F(x)$ 的带回溯的快速迭代收缩阈值算法 (FISTA)，假设 Lipschitz 常数 $L^\\star$ 未知。初始化设置 $x_0 = 0$, $y_1 = x_0$ 和 $t_1 = 1$。在每次外层迭代 $k \\in \\{1,2,\\dots\\}$ 中，执行回溯线搜索，从当前估计值 $L_k$ 开始，并将 $L_k$ 乘以一个因子 $\\eta  1$，直到满足充分下降条件。使用与 $h$ 相关联的近端映射，根据外推点和当前的 $L_k$ 形成近端梯度候选点。使用 $t_{k+1} = \\tfrac{1 + \\sqrt{1 + 4 t_k^2}}{2}$ 和通常的外推方法来维持标准的 FISTA 加速更新。\n- 每当回溯线搜索在接受迭代 $k$ 之前将 $L_k$ 乘以 $\\eta$ 时，记为一次“$L_k$ 的增加”。记录在前 $10$ 次外层迭代期间（即对于 $k \\in \\{1,\\dots,10\\}$）发生的此类增加的总次数。\n\n不涉及物理单位。所有角度（如有）均假定为弧度。所有答案必须是实数值。\n\n测试套件和要求的输出：\n实现您的程序以运行以下三个合成测试用例。在每种情况下，通过 $b = A x_{\\mathrm{true}}$ 从一个置入向量 $x_{\\mathrm{true}}$ 生成 $b$（无噪声）。对于每个用例，运行带回溯的 FISTA 恰好 $10$ 次外层迭代，并报告在这 $10$ 次迭代中发生的 $L_k$ 增加的总次数。\n\n- 用例 1（正常路径，中度回溯）：\n  - $A \\in \\mathbb{R}^{6 \\times 10}$：\n    $$\n    A = \\begin{bmatrix}\n    1  0  0  0  0  0  0.5  -0.2  0.3  0.1 \\\\\n    0  1  0  0  0  0  -0.1  0.4  0.2  -0.3 \\\\\n    0  0  1  0  0  0  0.3  -0.4  0.1  0.2 \\\\\n    0  0  0  1  0  0  -0.2  0.1  0.5  -0.1 \\\\\n    0  0  0  0  1  0  0.4  0.3  -0.2  0.2 \\\\\n    0  0  0  0  0  1  -0.3  0.2  0.1  0.4\n    \\end{bmatrix}.\n    $$\n  - $x_{\\mathrm{true}} \\in \\mathbb{R}^{10}$，其元素为\n    $$\n    x_{\\mathrm{true}} = \\begin{bmatrix} 0  1.2  0  -0.7  0  0  0  0  2.0  0 \\end{bmatrix}^\\top.\n    $$\n  - $b = A x_{\\mathrm{true}}$。\n  - $\\lambda = 0.05$，初始 $L_1 = 0.1$，回溯因子 $\\eta = 2.0$。\n\n- 用例 2（边界情况，无需回溯）：\n  - 与用例 1 中相同的 $A$ 和 $x_{\\mathrm{true}}$，其中 $b = A x_{\\mathrm{true}}$。\n  - $\\lambda = 0.05$，初始 $L_1 = 100.0$，回溯因子 $\\eta = 2.0$。\n\n- 用例 3（边缘情况，由于 $\\eta$ 较小和算子范数较大导致多次小幅增加）：\n  - $A \\in \\mathbb{R}^{8 \\times 12}$：\n    $$\n    A = \\begin{bmatrix}\n    3.0  -2.0  1.0  0.0  1.0  -1.0  2.0  -3.0  1.5  -0.5  0.5  -1.5 \\\\\n    0.0  1.0  -1.0  2.0  -2.0  1.0  -1.5  2.5  -1.0  0.5  -0.5  1.0 \\\\\n    1.5  -1.0  0.5  -0.5  1.0  -1.5  2.0  -2.0  1.0  -1.0  0.5  -0.5 \\\\\n    -1.0  2.0  -2.0  1.0  0.0  1.0  -2.5  3.0  -1.5  1.0  -1.0  0.5 \\\\\n    2.0  -1.0  1.5  -1.0  2.0  -2.0  1.0  -1.0  0.5  -0.5  1.0  -1.5 \\\\\n    -2.0  1.0  -0.5  1.5  -1.0  2.0  -1.0  1.0  -0.5  0.5  -1.0  1.0 \\\\\n    1.0  0.5  -1.5  2.0  -2.5  1.5  -1.0  2.0  -1.0  0.5  -0.5  1.0 \\\\\n    -1.5  2.5  -1.0  0.5  1.0  -1.0  2.0  -3.0  1.5  -1.0  1.0  -0.5\n    \\end{bmatrix}.\n    $$\n  - $x_{\\mathrm{true}} \\in \\mathbb{R}^{12}$，其元素为\n    $$\n    x_{\\mathrm{true}} = \\begin{bmatrix} 0  0  2.0  -1.0  0  0  0  3.0  0  0  0  0.5 \\end{bmatrix}^\\top.\n    $$\n  - $b = A x_{\\mathrm{true}}$。\n  - $\\lambda = 0.1$，初始 $L_1 = 1.0$，回溯因子 $\\eta = 1.1$。\n\n对于每个用例，运行带回溯的 FISTA 恰好 $10$ 次外层迭代，并计算在这 $10$ 次迭代中 $L_k$ 被乘以 $\\eta$ 的总次数。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3\\right]$）。每个结果必须是表示其对应测试用例计数的整数。不应打印任何其他文本。",
            "solution": "该问题要求实现带回溯线搜索的快速迭代收缩阈值算法 (FISTA) 来解决 LASSO 优化问题。主要任务是计算在固定迭代次数内，回溯过程中 Lipschitz 常数估计值增加的次数。\n\nLASSO 目标函数由 $F(x) = g(x) + h(x)$ 给出，其中 $g(x) = \\frac{1}{2}\\|A x - b\\|_2^2$ 是光滑的数据保真项，$h(x) = \\lambda \\|x\\|_1$ 是非光滑的稀疏性诱导正则化项。\n\n首先，我们推导回溯线搜索中使用的充分下降条件。光滑项的梯度为 $\\nabla g(x) = A^\\top (A x - b)$。已知 $\\nabla g$ 是 Lipschitz 连续的，常数为 $L^\\star$，这意味着对于任何 $L \\ge L^\\star$ 以下不等式成立（此性质通常被称为下降引理）：\n$$\ng(z) \\le g(y) + \\langle \\nabla g(y), z - y \\rangle + \\frac{L}{2} \\|z - y\\|_2^2\n$$\n这个不等式为函数 $g$ 在点 $y$ 附近提供了点 $z$ 处的一个二次上界。\n\n在 FISTA 的每次迭代 $k$ 中，给定一个外推点 $y_k$，算法通过最小化 $F(x)$ 在 $y_k$ 周围的一个代理上界来寻找一个新点 $x_k$。这导致了近端梯度更新：\n$$\nx_k = \\operatorname{prox}_{h/L}(y_k - \\frac{1}{L}\\nabla g(y_k))\n$$\n由于真实的 Lipschitz 常数 $L^\\star$ 是未知的，因此采用回溯线搜索在每次迭代 $k$ 中寻找合适的步长 $1/L_k$。从 $L_k$ 的一个估计值开始，我们检查二次上界是否对我们的候选点 $x_k$ 成立。通过在下降引理中代入 $z=x_k$ 和 $y=y_k$ 直接推导出充分下降条件。我们要求所选的 $L_k$ 满足：\n$$\ng(x_k) \\le g(y_k) + \\langle \\nabla g(y_k), x_k - y_k \\rangle + \\frac{L_k}{2} \\|x_k - y_k\\|_2^2\n$$\n如果此条件不满足，则估计值 $L_k$ 过小。我们将其增加一个因子 $\\eta  1$（即 $L_k \\leftarrow \\eta L_k$），用新的 $L_k$ 重新计算候选点 $x_k$，并再次检查条件。重复此过程直到条件满足。每次乘以 $\\eta$ 都构成一次我们必须计数的“增加”。\n\n带回溯的 FISTA 完整算法如下：\n\n1.  **初始化**：给定 Lipschitz 常数的初始猜测值 $L_0$，回溯因子 $\\eta  1$。根据题目要求设置 $x_0 = 0$，$y_1 = x_0$ 和 $t_1 = 1$。（在实现中，我们将使用 $x_k, x_{k-1}$ 来表示当前和前一个迭代，以方便更新。）令总增加次数为 $C = 0$。\n\n2.  **迭代**：对于 $k = 1, 2, \\ldots, 10$：\n    a. **回溯线搜索**：\n        i.  从一个试验的 Lipschitz 常数开始，例如 $L_{trial} = L_{k-1}$（对于第一次迭代，使用初始值）。\n        ii. 计算梯度 $\\nabla g(y_k)$。\n        iii. **循环**：\n            1. 计算候选点 $x_{k, trial} = S_{\\lambda/L_{trial}}\\left(y_k - \\frac{1}{L_{trial}}\\nabla g(y_k)\\right)$。\n            2. 检查充分下降条件：如果 $g(x_{k, trial})  g(y_k) + \\langle \\nabla g(y_k), x_{k, trial} - y_k \\rangle + \\frac{L_{trial}}{2} \\|x_{k, trial} - y_k\\|_2^2$，则更新 $L_{trial} \\leftarrow \\eta L_{trial}$，计数器加一 $C \\leftarrow C+1$，并继续循环。\n            3. 否则，退出循环。\n    b. **接受步长和迭代点**：\n        i.  $L_k = L_{trial}$。\n        ii. $x_k = x_{k, trial}$。\n    c. **加速步骤**：更新动量项：\n        i.  $t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}$。\n        ii. $y_{k+1} = x_k + \\frac{t_k - 1}{t_{k+1}}(x_k - x_{k-1})$。\n\n3.  经过 $10$ 次迭代后，计数器 $C$ 的最终值即为给定测试用例的结果。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Runs FISTA with backtracking on a suite of test cases and reports the\n    total number of backtracking steps (increases of L).\n    \"\"\"\n\n    test_cases = [\n        {\n            \"name\": \"Case 1\",\n            \"A\": np.array([\n                [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, -0.2, 0.3, 0.1],\n                [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, -0.1, 0.4, 0.2, -0.3],\n                [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3, -0.4, 0.1, 0.2],\n                [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, -0.2, 0.1, 0.5, -0.1],\n                [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.3, -0.2, 0.2],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, -0.3, 0.2, 0.1, 0.4]\n            ]),\n            \"x_true\": np.array([0.0, 1.2, 0.0, -0.7, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0]),\n            \"lam\": 0.05,\n            \"L_initial\": 0.1,\n            \"eta\": 2.0\n        },\n        {\n            \"name\": \"Case 2\",\n            \"A\": np.array([\n                [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5, -0.2, 0.3, 0.1],\n                [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, -0.1, 0.4, 0.2, -0.3],\n                [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.3, -0.4, 0.1, 0.2],\n                [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, -0.2, 0.1, 0.5, -0.1],\n                [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.4, 0.3, -0.2, 0.2],\n                [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, -0.3, 0.2, 0.1, 0.4]\n            ]),\n            \"x_true\": np.array([0.0, 1.2, 0.0, -0.7, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0]),\n            \"lam\": 0.05,\n            \"L_initial\": 100.0,\n            \"eta\": 2.0\n        },\n        {\n            \"name\": \"Case 3\",\n            \"A\": np.array([\n                [3.0, -2.0, 1.0, 0.0, 1.0, -1.0, 2.0, -3.0, 1.5, -0.5, 0.5, -1.5],\n                [0.0, 1.0, -1.0, 2.0, -2.0, 1.0, -1.5, 2.5, -1.0, 0.5, -0.5, 1.0],\n                [1.5, -1.0, 0.5, -0.5, 1.0, -1.5, 2.0, -2.0, 1.0, -1.0, 0.5, -0.5],\n                [-1.0, 2.0, -2.0, 1.0, 0.0, 1.0, -2.5, 3.0, -1.5, 1.0, -1.0, 0.5],\n                [2.0, -1.0, 1.5, -1.0, 2.0, -2.0, 1.0, -1.0, 0.5, -0.5, 1.0, -1.5],\n                [-2.0, 1.0, -0.5, 1.5, -1.0, 2.0, -1.0, 1.0, -0.5, 0.5, -1.0, 1.0],\n                [1.0, 0.5, -1.5, 2.0, -2.5, 1.5, -1.0, 2.0, -1.0, 0.5, -0.5, 1.0],\n                [-1.5, 2.5, -1.0, 0.5, 1.0, -1.0, 2.0, -3.0, 1.5, -1.0, 1.0, -0.5]\n            ]),\n            \"x_true\": np.array([0.0, 0.0, 2.0, -1.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.5]),\n            \"lam\": 0.1,\n            \"L_initial\": 1.0,\n            \"eta\": 1.1\n        }\n    ]\n\n    results = []\n\n    def soft_threshold(v, tau):\n        return np.sign(v) * np.maximum(np.abs(v) - tau, 0.0)\n\n    for case in test_cases:\n        A = case[\"A\"]\n        x_true = case[\"x_true\"]\n        lam = case[\"lam\"]\n        L_k = case[\"L_initial\"]\n        eta = case[\"eta\"]\n        n_iters = 10\n\n        b = A @ x_true\n        n = A.shape[1]\n        \n        # In FISTA notation, x_k is the main sequence. We need x_{k-1} for the update.\n        # Let's use x_curr for x_k and x_prev for x_{k-1}.\n        x_curr = np.zeros(n)\n        x_prev = np.zeros(n)\n        \n        # y_1 = x_0, t_1 = 1\n        y_k = x_curr \n        t_k = 1.0\n        \n        total_increases = 0\n\n        # Loop for k = 1, ..., 10\n        for _ in range(n_iters):\n            L_inner = L_k # Start with previous L\n            grad_g_y = A.T @ (A @ y_k - b)\n\n            while True:\n                x_k_candidate = soft_threshold(y_k - (1.0 / L_inner) * grad_g_y, lam / L_inner)\n\n                g_x = 0.5 * np.linalg.norm(A @ x_k_candidate - b)**2\n                g_y = 0.5 * np.linalg.norm(A @ y_k - b)**2\n                rhs = g_y + np.dot(grad_g_y, x_k_candidate - y_k) + \\\n                      (L_inner / 2.0) * np.linalg.norm(x_k_candidate - y_k)**2\n\n                if g_x = rhs:\n                    L_k = L_inner\n                    break\n                else:\n                    L_inner *= eta\n                    total_increases += 1\n            \n            # Found a valid L and x_k_candidate. Now update the FISTA sequences.\n            x_next = x_k_candidate\n            t_kp1 = (1.0 + np.sqrt(1.0 + 4.0 * t_k**2)) / 2.0\n            y_kp1 = x_next + ((t_k - 1.0) / t_kp1) * (x_next - x_curr)\n            \n            # Update state for next iteration\n            x_prev = x_curr\n            x_curr = x_next\n            t_k = t_kp1\n            y_k = y_kp1\n\n        results.append(total_increases)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}