{
    "hands_on_practices": [
        {
            "introduction": "We begin with a classic problem in data assimilation, where the goal is to find the most probable state of a system given observations and a background model, while also respecting a physical invariant. This exercise will guide you through the fundamental steps of applying the method of Lagrange multipliers to a constrained quadratic inverse problem . By deriving the Karush-Kuhn-Tucker (KKT) optimality conditions and solving for the multiplier analytically, you will gain a concrete understanding of how an equality constraint modifies the solution and what the Lagrange multiplier represents.",
            "id": "3395254",
            "problem": "Consider a linear inverse problem posed in a data assimilation framework, where a state vector $m \\in \\mathbb{R}^{n}$ is estimated from observations $d \\in \\mathbb{R}^{p}$ through a linear forward operator $F \\in \\mathbb{R}^{p \\times n}$. Assume a positive-definite data precision matrix $W \\in \\mathbb{R}^{p \\times p}$ and a positive-definite background covariance $B \\in \\mathbb{R}^{n \\times n}$ with background state $m_{b} \\in \\mathbb{R}^{n}$. The Maximum A Posteriori (MAP) estimator for $m$ under Gaussian assumptions is obtained by minimizing the quadratic objective\n$$\nJ(m) = \\frac{1}{2} (F m - d)^{\\mathsf{T}} W (F m - d) + \\frac{1}{2} (m - m_{b})^{\\mathsf{T}} B^{-1} (m - m_{b}),\n$$\nsubject to a linear invariant constraint $s^{\\mathsf{T}} m = c$, where $s \\in \\mathbb{R}^{n}$ is a given nonzero vector and $c \\in \\mathbb{R}$ is a prescribed scalar.\n\nStarting from the core definitions of constrained optimization and the Karush–Kuhn–Tucker (KKT) conditions for equality-constrained problems, derive the first-order optimality system that characterizes the constrained MAP solution. Then, solve analytically for the Lagrange multiplier associated with the constraint $s^{\\mathsf{T}} m = c$ in closed form, expressed in terms of $F$, $W$, $B$, $d$, $m_{b}$, $s$, and $c$. Clearly state any assumptions necessary to guarantee the uniqueness of the multiplier. Your final answer must be a single closed-form analytical expression for the multiplier. No rounding is required and no physical units are involved.",
            "solution": "The problem as stated is a classical constrained quadratic optimization problem arising in the context of variational data assimilation. We will first validate the problem statement.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- State vector to be estimated: $m \\in \\mathbb{R}^{n}$.\n- Observation vector: $d \\in \\mathbb{R}^{p}$.\n- Linear forward operator: $F \\in \\mathbb{R}^{p \\times n}$.\n- Data precision matrix: $W \\in \\mathbb{R}^{p \\times p}$, positive-definite.\n- Background covariance matrix: $B \\in \\mathbb{R}^{n \\times n}$, positive-definite.\n- Background state vector: $m_{b} \\in \\mathbb{R}^{n}$.\n- Objective function: $J(m) = \\frac{1}{2} (F m - d)^{\\mathsf{T}} W (F m - d) + \\frac{1}{2} (m - m_{b})^{\\mathsf{T}} B^{-1} (m - m_{b})$.\n- Linear constraint: $s^{\\mathsf{T}} m = c$, where $s \\in \\mathbb{R}^{n}$ is a given nonzero vector and $c \\in \\mathbb{R}$ is a prescribed scalar.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem represents a standard formulation for finding the Maximum A Posteriori (MAP) estimate in Bayesian inference under Gaussian assumptions, commonly known as 3D-Var or 4D-Var in geosciences. The objective function $J(m)$ is proportional to the negative logarithm of the posterior probability density. This is a fundamental and well-established method in inverse problems and data assimilation.\n- **Well-Posed:** The objective function $J(m)$ is a quadratic function of $m$. Its Hessian is given by $\\nabla_m^2 J(m) = F^{\\mathsf{T}} W F + B^{-1}$. Since $B$ is positive-definite, its inverse $B^{-1}$ is also positive-definite. The matrix $W$ is positive-definite, which implies that for any vector $v \\in \\mathbb{R}^n$, the quadratic form $v^{\\mathsf{T}} (F^{\\mathsf{T}} W F) v = (Fv)^{\\mathsf{T}} W (Fv) \\ge 0$, so $F^{\\mathsf{T}} W F$ is positive semi-definite. The sum of a positive-definite matrix ($B^{-1}$) and a positive semi-definite matrix ($F^{\\mathsf{T}} W F$) is a positive-definite matrix. Therefore, the Hessian of $J(m)$ is positive-definite, which makes $J(m)$ a strictly convex function. The constraint $s^{\\mathsf{T}} m = c$ defines a convex set (an affine hyperplane). The minimization of a strictly convex function over a convex set admits a unique solution. The problem is thus well-posed.\n- **Objective:** The problem is formulated using precise, unambiguous mathematical language and standard notation from linear algebra and optimization theory.\n- **Completeness and Consistency:** All necessary components (variables, matrices, their properties, the objective function, and the constraint) are explicitly defined. There are no contradictions in the setup.\n\n**Step 3: Verdict and Action**\nThe problem is valid as it is scientifically grounded, well-posed, objective, and self-contained. We may proceed with the solution.\n\n### Derivation of the Solution\n\nThe problem is to find the vector $m$ that minimizes the objective function $J(m)$ subject to the equality constraint $g(m) = s^{\\mathsf{T}} m - c = 0$. This constrained optimization problem can be solved using the method of Lagrange multipliers.\n\nWe define the Lagrangian function $\\mathcal{L}(m, \\lambda)$ which incorporates the constraint, with $\\lambda \\in \\mathbb{R}$ being the Lagrange multiplier:\n$$\n\\mathcal{L}(m, \\lambda) = J(m) + \\lambda (s^{\\mathsf{T}} m - c)\n$$\nSubstituting the expression for $J(m)$:\n$$\n\\mathcal{L}(m, \\lambda) = \\frac{1}{2} (F m - d)^{\\mathsf{T}} W (F m - d) + \\frac{1}{2} (m - m_{b})^{\\mathsf{T}} B^{-1} (m - m_{b}) + \\lambda (s^{\\mathsf{T}} m - c)\n$$\nThe first-order necessary conditions for optimality (the Karush-Kuhn-Tucker conditions for an equality constraint) state that the gradient of the Lagrangian with respect to both $m$ and $\\lambda$ must be zero at the optimal point $(m^*, \\lambda^*)$.\n\nThe first condition is stationarity with respect to $m$: $\\nabla_m \\mathcal{L}(m, \\lambda) = 0$. We compute the gradient term by term:\n$$\n\\nabla_m \\left( \\frac{1}{2} (F m - d)^{\\mathsf{T}} W (F m - d) \\right) = F^{\\mathsf{T}} W (Fm - d)\n$$\n$$\n\\nabla_m \\left( \\frac{1}{2} (m - m_{b})^{\\mathsf{T}} B^{-1} (m - m_{b}) \\right) = B^{-1} (m - m_b)\n$$\n$$\n\\nabla_m \\left( \\lambda (s^{\\mathsf{T}} m - c) \\right) = \\lambda s\n$$\nCombining these terms, the stationarity condition is:\n$$\n\\nabla_m \\mathcal{L}(m, \\lambda) = F^{\\mathsf{T}} W (Fm - d) + B^{-1} (m - m_b) + \\lambda s = 0\n$$\nThe second condition, $\\nabla_\\lambda \\mathcal{L}(m, \\lambda) = 0$, simply recovers the original constraint:\n$$\ns^{\\mathsf{T}} m - c = 0\n$$\nThis pair of equations forms the first-order optimality system for the constrained MAP solution $(m, \\lambda)$:\n1. $(F^{\\mathsf{T}} W F + B^{-1}) m + \\lambda s = F^{\\mathsf{T}} W d + B^{-1} m_b$\n2. $s^{\\mathsf{T}} m = c$\n\nTo solve for the Lagrange multiplier $\\lambda$, we first solve the first equation for $m$. Let us define the Hessian matrix $H = F^{\\mathsf{T}} W F + B^{-1}$ and the vector $g = F^{\\mathsf{T}} W d + B^{-1} m_b$. The system becomes:\n1. $H m + \\lambda s = g$\n2. $s^{\\mathsf{T}} m = c$\n\nAs established in the validation, the matrix $H$ is positive-definite and therefore invertible. From equation (1), we can express $m$ in terms of $\\lambda$:\n$$\nH m = g - \\lambda s\n$$\n$$\nm = H^{-1} (g - \\lambda s)\n$$\nNow, substitute this expression for $m$ into the constraint equation (2):\n$$\ns^{\\mathsf{T}} \\left( H^{-1} (g - \\lambda s) \\right) = c\n$$\nDistributing the transpose:\n$$\ns^{\\mathsf{T}} H^{-1} g - \\lambda s^{\\mathsf{T}} H^{-1} s = c\n$$\nWe can now isolate and solve for $\\lambda$. Rearranging the terms:\n$$\n\\lambda (s^{\\mathsf{T}} H^{-1} s) = s^{\\mathsf{T}} H^{-1} g - c\n$$\nA unique solution for $\\lambda$ exists if and only if the scalar coefficient $s^{\\mathsf{T}} H^{-1} s \\neq 0$. Since $H$ is positive-definite, its inverse $H^{-1}$ is also positive-definite. The problem states that $s$ is a nonzero vector. For any positive-definite matrix $A$ and any nonzero vector $x$, the quadratic form $x^{\\mathsf{T}} A x$ is strictly positive. Therefore, $s^{\\mathsf{T}} H^{-1} s > 0$, which guarantees the uniqueness of the multiplier $\\lambda$.\n$$\n\\lambda = \\frac{s^{\\mathsf{T}} H^{-1} g - c}{s^{\\mathsf{T}} H^{-1} s}\n$$\nFinally, we substitute the definitions of $H$ and $g$ back into this expression to obtain the closed-form solution for the Lagrange multiplier in terms of the original problem data:\n$$\n\\lambda = \\frac{s^{\\mathsf{T}} (F^{\\mathsf{T}} W F + B^{-1})^{-1} (F^{\\mathsf{T}} W d + B^{-1} m_{b}) - c}{s^{\\mathsf{T}} (F^{\\mathsf{T}} W F + B^{-1})^{-1} s}\n$$\nThis is the analytical expression for the Lagrange multiplier associated with the constraint.",
            "answer": "$$\n\\boxed{\\frac{s^{\\mathsf{T}} (F^{\\mathsf{T}} W F + B^{-1})^{-1} (F^{\\mathsf{T}} W d + B^{-1} m_{b}) - c}{s^{\\mathsf{T}} (F^{\\mathsf{T}} W F + B^{-1})^{-1} s}}\n$$"
        },
        {
            "introduction": "Real-world problems often involve not just equalities, but also inequality constraints, such as requiring physical quantities like concentration or density to be non-negative. This practice extends the KKT framework to handle these inequalities, introducing the crucial concepts of dual feasibility and complementary slackness . By implementing a basic active-set solver, you will explore the combinatorial challenge of identifying which constraints are binding at the optimal solution, a core task in quadratic programming.",
            "id": "3395272",
            "problem": "Consider the quadratic inverse problem with linear equality and inequality constraints. Let $m \\in \\mathbb{R}^n$ denote the model parameters and let $A \\in \\mathbb{R}^{p \\times n}$ and $d \\in \\mathbb{R}^p$ define a least-squares data misfit $J(m) = \\tfrac{1}{2}\\lVert A m - d \\rVert_2^2$. Let $B \\in \\mathbb{R}^{r \\times n}$ and $b \\in \\mathbb{R}^r$ define linear inequality constraints $B m - b \\le 0$ (componentwise), and let $C \\in \\mathbb{R}^{q \\times n}$ and $c \\in \\mathbb{R}^q$ define linear equality constraints $C m - c = 0$. Using the method of Lagrange multipliers and the Karush-Kuhn-Tucker (KKT) conditions, the optimal solution $m^\\star \\in \\mathbb{R}^n$ for this convex problem satisfies a coupled set of equations involving the primal variables $m$, the inequality multipliers $\\lambda \\in \\mathbb{R}^r$, and the equality multipliers $\\nu \\in \\mathbb{R}^q$.\n\nTask 1 (derivation): Starting from the definition of the Lagrangian $\\mathcal{L}(m,\\lambda,\\nu) = \\tfrac{1}{2}\\lVert A m - d \\rVert_2^2 + \\lambda^{\\mathsf{T}} (B m - b) + \\nu^{\\mathsf{T}} (C m - c)$, derive the first-order necessary and sufficient optimality conditions for this convex problem, namely the Karush-Kuhn-Tucker (KKT) conditions, consisting of:\n- Stationarity with respect to $m$,\n- Primal feasibility for both the inequalities and equalities,\n- Dual feasibility for $\\lambda$,\n- Complementary slackness coupling $\\lambda$ with the inequality residuals.\n\nThen write these conditions explicitly as a linear system in block-matrix form by expressing the gradient of the quadratic term via the symmetric positive semidefinite matrix $H = A^{\\mathsf{T}} A$ and vector $g = A^{\\mathsf{T}} d$. Your block system must be explicit in terms of $H$, $B$, $C$, $g$, $b$, and $c$, and it must accommodate an arbitrary set $\\mathcal{I}$ of active inequality constraints (that is, those indices $i$ for which $(B m^\\star - b)_i = 0$).\n\nTask 2 (algorithm and implementation): Implement an algorithm that, given numerical instances $(A,B,C,d,b,c)$, determines an optimal solution $m^\\star$, the active set $\\mathcal{A} = \\{ i \\in \\{0,\\dots,r-1\\} : \\lvert (B m^\\star - b)_i \\rvert \\le \\tau \\}$, and the corresponding Lagrange multipliers $(\\lambda^\\star,\\nu^\\star)$ that satisfy the KKT conditions. Your program must:\n- Use only linear algebra and fundamental definitions; do not call a prepackaged quadratic programming solver.\n- Enumerate candidate active sets $\\mathcal{I} \\subseteq \\{0,\\dots,r-1\\}$ and, for each candidate, solve the associated KKT linear system that enforces $B_{\\mathcal{I}} m = b_{\\mathcal{I}}$ and $C m = c$, together with stationarity, to obtain $(m,\\lambda_{\\mathcal{I}},\\nu)$.\n- Accept a candidate if it is primal feasible (that is, $B m - b \\le \\tau \\mathbf{1}$ and $\\lVert C m - c \\rVert_2 \\le \\tau$), satisfies dual feasibility on the enforced multipliers (that is, $\\lambda_{\\mathcal{I}} \\ge -\\tau \\mathbf{1}$ componentwise), and minimizes the objective $J(m)$ among all accepted candidates.\n- Define the numerical tolerance to be $\\tau = 10^{-8}$ for feasibility and activity tests.\n\nTask 3 (test suite and output): Your program must solve the following three test cases. For each case, it must return:\n- The optimal model $m^\\star$ as a list of floats,\n- The active set $\\mathcal{A}$ as a list of zero-based integer indices,\n- The full inequality multiplier vector $\\lambda^\\star \\in \\mathbb{R}^r$ (zeros for inactive constraints),\n- The equality multiplier vector $\\nu^\\star \\in \\mathbb{R}^q$,\n- The optimal objective value $J(m^\\star)$,\nwith all floats rounded to six decimal places.\n\nUse the following instances:\n\n- Test case 1:\n  - $n = 2$, $p = 2$, $r = 2$, $q = 1$.\n  - $A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 2 \\end{bmatrix}$, $d = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$.\n  - $B = \\begin{bmatrix} -1 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0.5 \\end{bmatrix}$.\n  - $C = \\begin{bmatrix} 1 & 1 \\end{bmatrix}$, $c = \\begin{bmatrix} 1 \\end{bmatrix}$.\n\n- Test case 2:\n  - $n = 2$, $p = 2$, $r = 4$, $q = 0$.\n  - $A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $d = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}$.\n  - $B = \\begin{bmatrix} -1 & 0 \\\\ 0 & -1 \\\\ 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n  - $C$ has no rows, and $c$ has no entries.\n\n- Test case 3:\n  - $n = 3$, $p = 3$, $r = 3$, $q = 1$.\n  - $A = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{bmatrix}$, $d = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n  - $B = \\begin{bmatrix} -1 & 0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & -1 \\end{bmatrix}$ with $b = \\begin{bmatrix} 0 \\\\ 0 \\\\ -0.4 \\end{bmatrix}$, which encodes $m_1 \\ge 0$, $m_2 \\ge 0$, and $m_3 \\ge 0.4$.\n  - $C = \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}$, $c = \\begin{bmatrix} 1 \\end{bmatrix}$.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list of the form $[m^\\star, \\mathcal{A}, \\lambda^\\star, \\nu^\\star, J(m^\\star)]$.\n- All floating-point numbers must be rounded to six decimal places in the printed output.\n- Example structural format (illustrative only): $[[[m\\_1,m\\_2], [i\\_1,i\\_2], [\\lambda\\_1,\\dots], [\\nu\\_1,\\dots], J], \\dots]$.",
            "solution": "The user has provided a well-defined constrained quadratic optimization problem and has requested a solution based on the Karush-Kuhn-Tucker (KKT) conditions, implemented via an active-set enumeration algorithm.\n\n### Problem Statement Validation\n\nThe problem is critically validated against the required criteria.\n\n**Step 1: Extracted Givens**\n- **Objective Function**: Minimize $J(m) = \\tfrac{1}{2}\\lVert A m - d \\rVert_2^2$, where $m \\in \\mathbb{R}^n$, $A \\in \\mathbb{R}^{p \\times n}$, $d \\in \\mathbb{R}^p$.\n- **Inequality Constraints**: $B m - b \\le 0$, where $B \\in \\mathbb{R}^{r \\times n}$, $b \\in \\mathbb{R}^r$.\n- **Equality Constraints**: $C m - c = 0$, where $C \\in \\mathbb{R}^{q \\times n}$, $c \\in \\mathbb{R}^q$.\n- **Lagrangian**: $\\mathcal{L}(m,\\lambda,\\nu) = \\tfrac{1}{2}\\lVert A m - d \\rVert_2^2 + \\lambda^{\\mathsf{T}} (B m - b) + \\nu^{\\mathsf{T}} (C m - c)$, with multipliers $\\lambda \\in \\mathbb{R}^r$ and $\\nu \\in \\mathbb{R}^q$.\n- **Task 1**: Derive the KKT conditions and express them as a block-matrix linear system for a given active set $\\mathcal{I}$.\n- **Task 2**: Implement an active-set enumeration algorithm to find the optimal solution $(m^\\star, \\lambda^\\star, \\nu^\\star)$ by solving the KKT system for each candidate active set and selecting the best feasible solution. The numerical tolerance is defined as $\\tau = 10^{-8}$.\n- **Task 3**: Solve three specific numerical instances and provide the results in a specified format.\n\n**Step 2: Validation Using Extracted Givens**\n- **Scientific Grounding**: The problem is a standard quadratic program (QP), a fundamental topic in convex optimization and inverse problems. The use of Lagrange multipliers and KKT conditions is the standard theoretical approach for solving such problems. The problem is scientifically and mathematically sound.\n- **Well-Posed**: The problem is well-posed. The objective function is convex, and the constraints are linear, defining a convex feasible set. Therefore, a unique global minimum exists, provided the KKT system for the true active set is non-singular.\n- **Objective**: The problem is stated using precise, objective mathematical language.\n- **Completeness and Consistency**: The problem is self-contained. All necessary matrices, vectors, functions, and tasks are explicitly defined. The provided test cases are dimensionally consistent. The prescribed algorithm, while computationally intensive for large $r$, is a valid method for solving the problem and is feasible for the given test cases.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A full, reasoned solution will be provided.\n\n### Task 1: Derivation of the KKT Conditions\n\nThe optimization problem is to minimize a convex quadratic function subject to linear constraints:\n$$\n\\min_{m \\in \\mathbb{R}^n} \\quad J(m) = \\frac{1}{2} \\lVert A m - d \\rVert_2^2 \\\\\n\\text{subject to} \\quad B m - b \\le 0 \\\\\n\\text{and} \\quad C m - c = 0\n$$\nThe Lagrangian for this problem is:\n$$\n\\mathcal{L}(m, \\lambda, \\nu) = J(m) + \\lambda^{\\mathsf{T}} (B m - b) + \\nu^{\\mathsf{T}} (C m - c)\n$$\nwhere $\\lambda \\in \\mathbb{R}^r$ are the Lagrange multipliers for the inequality constraints and $\\nu \\in \\mathbb{R}^q$ are the multipliers for the equality constraints.\n\nFor a convex problem, the Karush-Kuhn-Tucker (KKT) conditions are both necessary and sufficient for optimality. A point $m^\\star$ is an optimal solution if and only if there exist multipliers $\\lambda^\\star$ and $\\nu^\\star$ such that the following five conditions hold:\n\n1.  **Stationarity**: The gradient of the Lagrangian with respect to the primal variables $m$ must vanish at the optimum: $\\nabla_m \\mathcal{L}(m^\\star, \\lambda^\\star, \\nu^\\star) = 0$.\n    The gradient of the objective function is $\\nabla_m J(m) = A^{\\mathsf{T}}(A m - d) = (A^{\\mathsf{T}} A)m - A^{\\mathsf{T}} d$. Let $H = A^{\\mathsf{T}} A$ (a symmetric positive semidefinite matrix) and $g = A^{\\mathsf{T}} d$. Then $\\nabla_m J(m) = H m - g$.\n    The gradients of the constraint terms are $\\nabla_m(\\lambda^{\\mathsf{T}} B m) = B^{\\mathsf{T}} \\lambda$ and $\\nabla_m(\\nu^{\\mathsf{T}} C m) = C^{\\mathsf{T}} \\nu$.\n    The stationarity condition is thus:\n    $$H m^\\star + B^{\\mathsf{T}} \\lambda^\\star + C^{\\mathsf{T}} \\nu^\\star - g = 0$$\n\n2.  **Primal Feasibility (Inequalities)**: The optimal solution must satisfy the inequality constraints.\n    $$B m^\\star - b \\le 0$$\n\n3.  **Primal Feasibility (Equalities)**: The optimal solution must satisfy the equality constraints.\n    $$C m^\\star - c = 0$$\n\n4.  **Dual Feasibility**: The multipliers corresponding to the inequality constraints must be non-negative.\n    $$\\lambda^\\star \\ge 0 \\quad (\\text{component-wise})$$\n\n5.  **Complementary Slackness**: For each inequality constraint, either the constraint holds with equality (is \"active\") or its corresponding multiplier is zero. This is expressed for each component $i \\in \\{0, \\dots, r-1\\}$ as:\n    $$\\lambda_i^\\star (B m^\\star - b)_i = 0$$\n\nTo formulate the linear system for a candidate active set $\\mathcal{I} \\subseteq \\{0, ..., r-1\\}$, we enforce the corresponding inequality constraints as equalities. For any $i \\in \\mathcal{I}$, we set $(B m - b)_i = 0$. For any $i \\notin \\mathcal{I}$, the complementary slackness condition implies $\\lambda_i=0$.\n\nLet $k = |\\mathcal{I}|$ be the number of active inequality constraints. Let $B_{\\mathcal{I}} \\in \\mathbb{R}^{k \\times n}$ and $b_{\\mathcal{I}} \\in \\mathbb{R}^k$ be the rows and entries of $B$ and $b$ corresponding to the indices in $\\mathcal{I}$. Similarly, let $\\lambda_{\\mathcal{I}} \\in \\mathbb{R}^k$ be the vector of active multipliers. The stationarity condition simplifies to $H m + B_{\\mathcal{I}}^{\\mathsf{T}} \\lambda_{\\mathcal{I}} + C^{\\mathsf{T}} \\nu = g$, since all other $\\lambda_i$ are zero.\n\nCombining the modified stationarity equation with the active inequality constraints and the equality constraints, we obtain a coupled linear system for the unknowns $(m, \\lambda_{\\mathcal{I}}, \\nu)$:\n$$\n\\begin{cases}\nH m + B_{\\mathcal{I}}^{\\mathsf{T}} \\lambda_{\\mathcal{I}} + C^{\\mathsf{T}} \\nu = g \\\\\nB_{\\mathcal{I}} m = b_{\\mathcal{I}} \\\\\nC m = c\n\\end{cases}\n$$\nThis can be written in block-matrix form as:\n$$\n\\begin{bmatrix}\nH & B_{\\mathcal{I}}^{\\mathsf{T}} & C^{\\mathsf{T}} \\\\\nB_{\\mathcal{I}} & 0 & 0 \\\\\nC & 0 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\nm \\\\\n\\lambda_{\\mathcal{I}} \\\\\n\\nu\n\\end{bmatrix}\n=\n\\begin{bmatrix}\ng \\\\\nb_{\\mathcal{I}} \\\\\nc\n\\end{bmatrix}\n$$\nThe zero blocks are matrices of appropriate dimensions. This is a symmetric indefinite saddle-point system.\n\n### Task 2: Algorithm Design\n\nThe implemented algorithm follows the user's specification. It performs an exhaustive search over all $2^r$ possible active sets $\\mathcal{I} \\subseteq \\{0, \\dots, r-1\\}$. For each candidate set $\\mathcal{I}$:\n1.  The KKT block-matrix system is constructed. Special care is taken for cases where the active set $\\mathcal{I}$ is empty ($k=0$) or where there are no equality constraints ($q=0$).\n2.  The linear system is solved for $(m, \\lambda_{\\mathcal{I}}, \\nu)$. If the system is singular, the candidate set is discarded.\n3.  The resulting solution is checked for full KKT feasibility:\n    - Primal feasibility: $B m - b \\le \\tau \\mathbf{1}$ and $\\lVert C m - c \\rVert_2 \\le \\tau$.\n    - Dual feasibility: $\\lambda_{\\mathcal{I}} \\ge -\\tau \\mathbf{1}$.\n4.  If the solution is feasible, its objective value $J(m)$ is computed.\n5.  The algorithm tracks the feasible solution that yields the minimum objective value. This solution, being a KKT point for a convex problem, is the global optimum.\n6.  Finally, the true active set $\\mathcal{A}$ is determined from the optimal solution $m^\\star$ by identifying all inequality constraints $i$ for which $|(B m^\\star - b)_i| \\le \\tau$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import chain, combinations\n\ndef solve():\n    \"\"\"\n    Main function to define and solve the test cases for the constrained quadratic inverse problem.\n    \"\"\"\n    \n    # Numerical tolerance for feasibility and active set checks.\n    tau = 1e-8\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            'A': np.array([[1., 0.], [0., 2.]]),\n            'd': np.array([1., 2.]),\n            'B': np.array([[-1., 0.], [0., 1.]]),\n            'b': np.array([0., 0.5]),\n            'C': np.array([[1., 1.]]),\n            'c': np.array([1.])\n        },\n        # Test case 2\n        {\n            'A': np.array([[1., 0.], [0., 1.]]),\n            'd': np.array([2., -1.]),\n            'B': np.array([[-1., 0.], [0., -1.], [1., 0.], [0., 1.]]),\n            'b': np.array([0., 0., 1., 1.]),\n            'C': np.empty((0, 2)),\n            'c': np.empty((0,))\n        },\n        # Test case 3\n        {\n            'A': np.array([[1., 0., 0.], [0., 2., 0.], [0., 0., 3.]]),\n            'd': np.array([1., 1., 1.]),\n            'B': np.array([[-1., 0., 0.], [0., -1., 0.], [0., 0., -1.]]),\n            'b': np.array([0., 0., -0.4]),\n            'C': np.array([[1., 1., 1.]]),\n            'c': np.array([1.])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        solution = solve_qp_by_enumeration(case['A'], case['d'], case['B'], case['b'], case['C'], case['c'], tau)\n        \n        # Format the results as specified\n        m_star = solution['m']\n        lambda_star = solution['lambda']\n        nu_star = solution['nu']\n        J_star = solution['J']\n        \n        # Determine final active set from the optimal solution m_star\n        if m_star is not None and case['B'].shape[0] > 0:\n            final_residuals = case['B'] @ m_star - case['b']\n            active_set = np.where(np.abs(final_residuals) = tau)[0]\n        else:\n            active_set = []\n\n        # Convert to lists with specified rounding\n        m_list = [round(x, 6) for x in m_star] if m_star is not None else []\n        active_set_list = [int(i) for i in active_set]\n        lambda_list = [round(x, 6) for x in lambda_star] if lambda_star is not None else []\n        nu_list = [round(x, 6) for x in nu_star] if nu_star is not None else []\n        J_val = round(J_star, 6) if J_star is not None else float('inf')\n\n        results.append([m_list, active_set_list, lambda_list, nu_list, J_val])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef solve_qp_by_enumeration(A, d, B, b, C, c, tau):\n    \"\"\"\n    Solves a constrained quadratic program by enumerating all possible active sets.\n    \"\"\"\n    n = A.shape[1]\n    r = B.shape[0] if B.ndim > 1 else 0\n    q = C.shape[0] if C.ndim > 1 else 0\n\n    H = A.T @ A\n    g = A.T @ d\n\n    best_solution = {\n        'm': None,\n        'lambda': None,\n        'nu': None,\n        'J': float('inf')\n    }\n\n    indices = range(r)\n    powerset = chain.from_iterable(combinations(indices, k) for k in range(r + 1))\n\n    for I_tuple in powerset:\n        I = list(I_tuple)\n        k = len(I)\n\n        B_I = B[I, :] if k > 0 else np.empty((0, n))\n        b_I = b[I] if k > 0 else np.empty(0)\n\n        # Build KKT system K z = f, where z = [m, lambda_I, nu]^T\n        K_blocks = []\n        top_row = [H]\n        if k > 0: top_row.append(B_I.T)\n        if q > 0: top_row.append(C.T)\n        K_blocks.append(top_row)\n        \n        if k > 0:\n            mid_row = [B_I, np.zeros((k, k))]\n            if q > 0: mid_row.append(np.zeros((k, q)))\n            K_blocks.append(mid_row)\n        \n        if q > 0:\n            bot_row = [C]\n            if k > 0: bot_row.append(np.zeros((q, k)))\n            bot_row.append(np.zeros((q, q)))\n            K_blocks.append(bot_row)\n        \n        K = np.block(K_blocks)\n        if K.shape[0] == 0: continue # Should not happen unless n=0\n\n        f_parts = [g]\n        if k > 0: f_parts.append(b_I)\n        if q > 0: f_parts.append(c)\n        f = np.concatenate(f_parts)\n\n        try:\n            z = np.linalg.solve(K, f)\n        except np.linalg.LinAlgError:\n            continue  # Singular matrix, this active set is not valid.\n\n        m_sol = z[:n]\n        lambda_I_sol = z[n:n+k]\n        nu_sol = z[n+k:]\n\n        # --- Check Feasibility of the Candidate Solution ---\n        # 1. Primal feasibility for all inequality constraints\n        if r > 0 and not np.all(B @ m_sol - b = tau):\n            continue\n        \n        # 2. Primal feasibility for equality constraints (should be guaranteed by solver)\n        if q > 0 and np.linalg.norm(C @ m_sol - c) > tau:\n            continue\n        \n        # 3. Dual feasibility for active inequality multipliers\n        if k > 0 and not np.all(lambda_I_sol >= -tau):\n            continue\n        \n        # --- If feasible, check if it's a better solution ---\n        J_val = 0.5 * np.linalg.norm(A @ m_sol - d)**2\n        \n        if J_val  best_solution['J']:\n            full_lambda = np.zeros(r)\n            if k > 0:\n                full_lambda[I] = lambda_I_sol\n            \n            best_solution['m'] = m_sol\n            best_solution['lambda'] = full_lambda\n            best_solution['nu'] = nu_sol\n            best_solution['J'] = J_val\n            \n    return best_solution\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "While the KKT system provides an exact characterization of the solution, solving it directly can be challenging as the system is larger and indefinite. An alternative and widely used approach is to enforce constraints approximately using a penalty method, which converts the problem into an unconstrained one. This practice asks you to implement both the exact KKT solution and the penalty method , allowing you to numerically verify the fundamental theorem that as the penalty parameter $\\mu \\to \\infty$, the penalized solution converges to the true constrained solution.",
            "id": "3395335",
            "problem": "Consider a linear inverse problem with equality constraints. Let $A \\in \\mathbb{R}^{m \\times n}$, $L \\in \\mathbb{R}^{p \\times n}$, $B \\in \\mathbb{R}^{q \\times n}$, $d \\in \\mathbb{R}^{m}$, $m_{0} \\in \\mathbb{R}^{n}$, and $b \\in \\mathbb{R}^{q}$. The task is to recover an unknown parameter vector $m \\in \\mathbb{R}^{n}$ by minimizing a Tikhonov-regularized least-squares objective subject to linear equality constraints. You must proceed from fundamental principles: the method of Lagrange multipliers and the Karush–Kuhn–Tucker (KKT) conditions for equality-constrained problems, together with the standard normal equations for least squares. Do not assume any formula not derivable from these bases.\n\nYour program must, for each test case below:\n- Derive and implement the exact equality-constrained solution using Lagrange multipliers by forming the KKT optimality system for the regularized least-squares objective subject to the constraints $B m = b$.\n- Derive and implement the penalized unconstrained counterpart that augments the objective with a quadratic penalty $\\frac{1}{2}\\,\\mu^{2}\\,\\lVert B m - b \\rVert_{2}^{2}$, and solve it for a sequence of increasing penalties $\\mu$.\n- For each test, verify two properties as $\\mu$ increases: \n  1. The Euclidean distance $\\lVert m_{\\mu} - m_{\\mathrm{KKT}} \\rVert_{2}$ is monotonically nonincreasing within a small numerical tolerance.\n  2. The constraint residual $\\lVert B m_{\\mu} - b \\rVert_{2}$ is monotonically nonincreasing within a small numerical tolerance.\n- Additionally verify that, at the largest penalty, the penalized solution is close to the KKT solution and the constraint residual is small, both within specified tolerances.\n\nNo physical units or angle units are involved. All computations are to be carried out in real arithmetic.\n\nThe penalized solutions must be computed for the penalty sequence $\\{\\mu\\} = \\{\\,1,\\,10,\\,100,\\,1000,\\,100000\\,\\}$.\n\nFor each test, your program must return a boolean indicating whether all the above checks pass.\n\nProvide the following four test cases. In each, compute the data vector $d$ from a given ground-truth $m_{\\mathrm{true}}$ and an additive noise vector $n$, specifically $d = A\\,m_{\\mathrm{true}} + n$. All vectors and matrices are given explicitly.\n\nTest case $1$ (overdetermined data term, identity regularization, moderate regularization weight):\n- Dimensions: $m = 7$, $n = 5$, $p = 5$, $q = 2$.\n- Matrix\n$$\nA^{(1)} = \\begin{bmatrix}\n1  2  0  0  0.5 \\\\\n0  1  1  0  0 \\\\\n0  0  1  1  0 \\\\\n1  0  0  1  1 \\\\\n0  1  0  0.5  1 \\\\\n0.5  0  1  0  1 \\\\\n1  0.5  0.5  0  0\n\\end{bmatrix}.\n$$\n- Regularization operator $L^{(1)} = I_{5}$ (the $5 \\times 5$ identity).\n- Prior $m_{0}^{(1)} = \\begin{bmatrix} 0  0  0  0  0 \\end{bmatrix}^{\\mathsf{T}}$.\n- Constraint matrix\n$$\nB^{(1)} = \\begin{bmatrix}\n1  1  0  0  0 \\\\\n0  0  1  -1  0\n\\end{bmatrix},\\quad\nb^{(1)} = \\begin{bmatrix} 0.5 \\\\ 0.3 \\end{bmatrix}.\n$$\n- Ground truth and noise:\n$m_{\\mathrm{true}}^{(1)} = \\begin{bmatrix} 1  -0.5  0.3  0  0.8 \\end{bmatrix}^{\\mathsf{T}}$,\n$n^{(1)} = \\begin{bmatrix} 0.01  -0.02  0.015  0  -0.005  0.01  -0.01 \\end{bmatrix}^{\\mathsf{T}}$.\n- Regularization weight $\\alpha^{(1)} = 0.1$.\n\nTest case $2$ (underdetermined data term, first-difference regularization, stronger regularization weight):\n- Dimensions: $m = 3$, $n = 6$, $p = 5$, $q = 2$.\n- Matrix\n$$\nA^{(2)} = \\begin{bmatrix}\n1  0  0  1  0  0 \\\\\n0  1  0  0  1  0 \\\\\n0  0  1  0  0  1\n\\end{bmatrix}.\n$$\n- Regularization operator $L^{(2)} \\in \\mathbb{R}^{5 \\times 6}$ is the first-difference operator:\n$(L^{(2)} m)_{i} = m_{i+1} - m_{i}$ for $i \\in \\{1,2,3,4,5\\}$, i.e.,\n$$\nL^{(2)} = \\begin{bmatrix}\n-1  1  0  0  0  0 \\\\\n0  -1  1  0  0  0 \\\\\n0  0  -1  1  0  0 \\\\\n0  0  0  -1  1  0 \\\\\n0  0  0  0  -1  1\n\\end{bmatrix}.\n$$\n- Prior $m_{0}^{(2)} = \\begin{bmatrix} 0  0  0  0  0  0 \\end{bmatrix}^{\\mathsf{T}}$.\n- Constraint matrix\n$$\nB^{(2)} = \\begin{bmatrix}\n1  1  1  1  1  1 \\\\\n1  0  -1  0  0  0\n\\end{bmatrix},\\quad\nb^{(2)} = \\begin{bmatrix} 0.7 \\\\ -0.1 \\end{bmatrix}.\n$$\n- Ground truth and noise:\n$m_{\\mathrm{true}}^{(2)} = \\begin{bmatrix} 0.2  -0.1  0.3  0.4  -0.2  0.1 \\end{bmatrix}^{\\mathsf{T}}$,\n$n^{(2)} = \\begin{bmatrix} 0  0.01  -0.02 \\end{bmatrix}^{\\mathsf{T}}$.\n- Regularization weight $\\alpha^{(2)} = 1.0$.\n\nTest case $3$ (square data term, identity regularization, small regularization weight, ill-conditioned but full-row-rank constraints):\n- Dimensions: $m = 5$, $n = 5$, $p = 5$, $q = 2$.\n- Matrix\n$$\nA^{(3)} = \\begin{bmatrix}\n1  0.01  0  0  0 \\\\\n0.01  1  0.01  0  0 \\\\\n0  0.01  1  0.01  0 \\\\\n0  0  0.01  1  0.01 \\\\\n0  0  0  0.01  1\n\\end{bmatrix}.\n$$\n- Regularization operator $L^{(3)} = I_{5}$.\n- Prior $m_{0}^{(3)} = \\begin{bmatrix} 0  0  0  0  0 \\end{bmatrix}^{\\mathsf{T}}$.\n- Constraint matrix\n$$\nB^{(3)} = \\begin{bmatrix}\n1  0.0001  0  0  0 \\\\\n0  1  0.0001  0  0\n\\end{bmatrix},\\quad\nb^{(3)} = \\begin{bmatrix} 0.9999 \\\\ -0.9999 \\end{bmatrix}.\n$$\n- Ground truth and noise:\n$m_{\\mathrm{true}}^{(3)} = \\begin{bmatrix} 1  -1  1  -1  1 \\end{bmatrix}^{\\mathsf{T}}$,\n$n^{(3)} = \\begin{bmatrix} 0  0  0  0  0.001 \\end{bmatrix}^{\\mathsf{T}}$.\n- Regularization weight $\\alpha^{(3)} = 0.001$.\n\nTest case $4$ (overdetermined data term, identity regularization, strong regularization weight):\n- Dimensions: $m = 8$, $n = 4$, $p = 4$, $q = 2$.\n- Matrix\n$$\nA^{(4)} = \\begin{bmatrix}\n1  0  1  0 \\\\\n0  1  0  1 \\\\\n1  1  0  0 \\\\\n0  0  1  1 \\\\\n1  0  0  1 \\\\\n0  1  1  0 \\\\\n1  -1  0  0 \\\\\n0  0  1  -1\n\\end{bmatrix}.\n$$\n- Regularization operator $L^{(4)} = I_{4}$.\n- Prior $m_{0}^{(4)} = \\begin{bmatrix} 0  0  0  0 \\end{bmatrix}^{\\mathsf{T}}$.\n- Constraint matrix\n$$\nB^{(4)} = \\begin{bmatrix}\n1  0  0  1 \\\\\n0  1  1  0\n\\end{bmatrix},\\quad\nb^{(4)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.\n$$\n- Ground truth and noise:\n$m_{\\mathrm{true}}^{(4)} = \\begin{bmatrix} 0.5  -0.5  0.5  -0.5 \\end{bmatrix}^{\\mathsf{T}}$,\n$n^{(4)} = \\begin{bmatrix} 0.01  -0.01  0  0.005  -0.005  0  0  0.002 \\end{bmatrix}^{\\mathsf{T}}$.\n- Regularization weight $\\alpha^{(4)} = 10$.\n\nFor each test case $i \\in \\{1,2,3,4\\}$:\n- Form $d^{(i)} = A^{(i)} m_{\\mathrm{true}}^{(i)} + n^{(i)}$.\n- Form and solve the exact equality-constrained problem by the KKT system.\n- Form and solve the penalized problems for the specified sequence of $\\mu$ values.\n- Define the sequences $\\Delta^{(i)}(\\mu) = \\lVert m_{\\mu}^{(i)} - m_{\\mathrm{KKT}}^{(i)} \\rVert_{2}$ and $r^{(i)}(\\mu) = \\lVert B^{(i)} m_{\\mu}^{(i)} - b^{(i)} \\rVert_{2}$.\n- Check that both $\\Delta^{(i)}(\\mu)$ and $r^{(i)}(\\mu)$ are monotonically nonincreasing in $\\mu$ within a small numerical tolerance, and that at the largest penalty both $\\Delta^{(i)}(\\mu_{\\max})$ and $r^{(i)}(\\mu_{\\max})$ are small relative to natural scales.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True,True]\"), where each entry is the boolean result for one test case in the order $i = 1, 2, 3, 4$ indicating whether the test passed all the checks described above.",
            "solution": "The problem requires solving a linear inverse problem with equality constraints. Given the task is to recover a parameter vector $m \\in \\mathbb{R}^{n}$ from data $d \\in \\mathbb{R}^{m}$, the model is formulated as the solution to a constrained optimization problem. The Tikhonov-regularized least-squares objective function to be minimized is\n$$ J(m) = \\frac{1}{2} \\lVert A m - d \\rVert_2^2 + \\frac{1}{2} \\alpha^2 \\lVert L (m - m_0) \\rVert_2^2, $$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ is the forward operator, $\\alpha > 0$ is the regularization weight, $L \\in \\mathbb{R}^{p \\times n}$ is a regularization operator, and $m_0 \\in \\mathbb{R}^n$ is a prior model. The minimization is subject to a set of linear equality constraints,\n$$ B m = b, $$\nwhere $B \\in \\mathbb{R}^{q \\times n}$ is the constraint matrix and $b \\in \\mathbb{R}^q$ is the constraint vector.\n\nWe will derive and implement two approaches to solving this problem: the exact method using Lagrange multipliers and an approximate method using a quadratic penalty function.\n\n**1. Exact Solution via Karush–Kuhn–Tucker (KKT) Conditions**\n\nThe method of Lagrange multipliers is the fundamental tool for solving equality-constrained optimization problems. We introduce a vector of Lagrange multipliers $\\lambda \\in \\mathbb{R}^q$ and form the Lagrangian function, $\\mathcal{L}(m, \\lambda)$:\n$$ \\mathcal{L}(m, \\lambda) = J(m) + \\lambda^{\\mathsf{T}} (B m - b). $$\nThe optimal solution $(m_{\\mathrm{KKT}}, \\lambda^*)$ must satisfy the first-order necessary Karush–Kuhn–Tucker (KKT) conditions, which state that the gradient of the Lagrangian with respect to both $m$ and $\\lambda$ must be zero.\n\nThe gradient of $\\mathcal{L}$ with respect to $m$ is found by differentiating $J(m)$ and the linear term $\\lambda^{\\mathsf{T}} B m$. The objective function $J(m)$ can be expanded as:\n$$ J(m) = \\frac{1}{2}(m^{\\mathsf{T}}A^{\\mathsf{T}}Am - 2d^{\\mathsf{T}}Am + d^{\\mathsf{T}}d) + \\frac{1}{2}\\alpha^2(m-m_0)^{\\mathsf{T}}L^{\\mathsf{T}}L(m-m_0). $$\nIts gradient is:\n$$ \\nabla_m J(m) = A^{\\mathsf{T}}(Am - d) + \\alpha^2 L^{\\mathsf{T}}L(m - m_0). $$\nThus, the stationarity condition, $\\nabla_m \\mathcal{L}(m, \\lambda) = 0$, is:\n$$ \\nabla_m J(m) + B^{\\mathsf{T}}\\lambda = 0 $$\n$$ A^{\\mathsf{T}}(Am - d) + \\alpha^2 L^{\\mathsf{T}}L(m - m_0) + B^{\\mathsf{T}}\\lambda = 0. $$\nRearranging to group terms involving $m$ on the left-hand side gives:\n$$ (A^{\\mathsf{T}}A + \\alpha^2 L^{\\mathsf{T}}L)m + B^{\\mathsf{T}}\\lambda = A^{\\mathsf{T}}d + \\alpha^2 L^{\\mathsf{T}}Lm_0. $$\n\nThe second KKT condition is primal feasibility, obtained by setting the gradient of $\\mathcal{L}$ with respect to $\\lambda$ to zero:\n$$ \\nabla_\\lambda \\mathcal{L}(m, \\lambda) = Bm - b = 0. $$\n\nThese two conditions form a coupled system of linear equations for the unknown model vector $m$ and the Lagrange multiplier vector $\\lambda$. This is the KKT system, which can be written in block matrix form:\n$$\n\\begin{bmatrix}\nA^{\\mathsf{T}}A + \\alpha^2 L^{\\mathsf{T}}L  B^{\\mathsf{T}} \\\\\nB  \\boldsymbol{0}\n\\end{bmatrix}\n\\begin{bmatrix}\nm \\\\\n\\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nA^{\\mathsf{T}}d + \\alpha^2 L^{\\mathsf{T}}Lm_0 \\\\\nb\n\\end{bmatrix}.\n$$\nThis is a symmetric but indefinite linear system of size $(n+q) \\times (n+q)$. Assuming the KKT matrix is invertible, we can solve this system directly to find the exact constrained solution, which we denote $m_{\\mathrm{KKT}}$.\n\n**2. Penalized Unconstrained Approximation**\n\nAn alternative approach is to convert the constrained problem into a sequence of unconstrained problems using a penalty method. The constraints are incorporated into the objective function as a penalty term, weighted by a penalty parameter $\\mu > 0$. The penalized objective function $J_\\mu(m)$ is:\n$$ J_\\mu(m) = J(m) + \\frac{1}{2}\\mu^2 \\lVert Bm - b \\rVert_2^2. $$\nThis is an unconstrained quadratic minimization problem. The solution $m_\\mu$ is found by setting the gradient $\\nabla_m J_\\mu(m)$ to zero. The gradient of the penalty term is:\n$$ \\nabla_m \\left( \\frac{1}{2}\\mu^2 \\lVert Bm-b \\rVert_2^2 \\right) = \\mu^2 B^{\\mathsf{T}}(Bm - b). $$\nAdding this to the gradient of $J(m)$, the optimality condition $\\nabla_m J_\\mu(m) = 0$ becomes:\n$$ A^{\\mathsf{T}}(Am - d) + \\alpha^2 L^{\\mathsf{T}}L(m - m_0) + \\mu^2 B^{\\mathsf{T}}(Bm - b) = 0. $$\nRearranging this equation yields a linear system for $m_\\mu$:\n$$ (A^{\\mathsf{T}}A + \\alpha^2 L^{\\mathsf{T}}L + \\mu^2 B^{\\mathsf{T}}B)m_\\mu = A^{\\mathsf{T}}d + \\alpha^2 L^{\\mathsf{T}}Lm_0 + \\mu^2 B^{\\mathsf{T}}b. $$\nThis system is of size $n \\times n$. The matrix on the left-hand side is symmetric and positive definite (for $\\alpha>0$ or under mild conditions on $A$ and $L$), making it reliably solvable. In theory, as the penalty parameter $\\mu \\to \\infty$, the solution $m_\\mu$ converges to the exact constrained solution $m_{\\mathrm{KKT}}$.\n\n**3. Numerical Verification**\n\nThe implementation will compute $m_{\\mathrm{KKT}}$ by solving the KKT system and a sequence of penalized solutions $m_\\mu$ for $\\mu \\in \\{1,\\,10,\\,100,\\,1000,\\,100000\\}$. The following properties, which reflect the convergence of the penalty method, will be verified for each test case:\n1.  The sequence of Euclidean distances $\\Delta(\\mu) = \\lVert m_\\mu - m_{\\mathrm{KKT}} \\rVert_2$ is monotonically non-increasing as $\\mu$ increases.\n2.  The sequence of constraint residuals $r(\\mu) = \\lVert Bm_\\mu - b \\rVert_2$ is monotonically non-increasing as $\\mu$ increases.\n3.  At the largest penalty, $\\mu_{\\max} = 100000$, the distance $\\Delta(\\mu_{\\max})$ and residual $r(\\mu_{\\max})$ are below specified numerical tolerances, confirming that the penalized solution is a good approximation of the exact constrained solution.\n\nA test case is considered successful if all these checks pass. All numerical calculations will be performed using the provided matrices and parameters for each test case.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the constrained inverse problem using both KKT and penalty methods,\n    and verifies the convergence properties of the penalty method.\n    \"\"\"\n    # Define penalty sequence\n    mus = [1.0, 10.0, 100.0, 1000.0, 100000.0]\n    \n    # Define tolerances for checks\n    mono_tol = 1e-9\n    final_dist_tol = 1e-4\n    final_resid_tol = 1e-6\n\n    # Test Case 1\n    A1 = np.array([\n        [1, 2, 0, 0, 0.5], [0, 1, 1, 0, 0], [0, 0, 1, 1, 0], [1, 0, 0, 1, 1],\n        [0, 1, 0, 0.5, 1], [0.5, 0, 1, 0, 1], [1, 0.5, 0.5, 0, 0]\n    ])\n    L1 = np.identity(5)\n    m0_1 = np.zeros(5)\n    B1 = np.array([[1, 1, 0, 0, 0], [0, 0, 1, -1, 0]])\n    b1 = np.array([0.5, 0.3])\n    m_true1 = np.array([1, -0.5, 0.3, 0, 0.8])\n    n1 = np.array([0.01, -0.02, 0.015, 0, -0.005, 0.01, -0.01])\n    alpha1 = 0.1\n    case1 = (A1, L1, B1, b1, m0_1, m_true1, n1, alpha1)\n\n    # Test Case 2\n    A2 = np.array([[1, 0, 0, 1, 0, 0], [0, 1, 0, 0, 1, 0], [0, 0, 1, 0, 0, 1]])\n    L2 = np.array([\n        [-1, 1, 0, 0, 0, 0], [0, -1, 1, 0, 0, 0], [0, 0, -1, 1, 0, 0],\n        [0, 0, 0, -1, 1, 0], [0, 0, 0, 0, -1, 1]\n    ])\n    m0_2 = np.zeros(6)\n    B2 = np.array([[1, 1, 1, 1, 1, 1], [1, 0, -1, 0, 0, 0]])\n    b2 = np.array([0.7, -0.1])\n    m_true2 = np.array([0.2, -0.1, 0.3, 0.4, -0.2, 0.1])\n    n2 = np.array([0, 0.01, -0.02])\n    alpha2 = 1.0\n    case2 = (A2, L2, B2, b2, m0_2, m_true2, n2, alpha2)\n\n    # Test Case 3\n    A3 = np.array([\n        [1, 0.01, 0, 0, 0], [0.01, 1, 0.01, 0, 0], [0, 0.01, 1, 0.01, 0],\n        [0, 0, 0.01, 1, 0.01], [0, 0, 0, 0.01, 1]\n    ])\n    L3 = np.identity(5)\n    m0_3 = np.zeros(5)\n    B3 = np.array([[1, 0.0001, 0, 0, 0], [0, 1, 0.0001, 0, 0]])\n    b3 = np.array([0.9999, -0.9999])\n    m_true3 = np.array([1, -1, 1, -1, 1])\n    n3 = np.array([0, 0, 0, 0, 0.001])\n    alpha3 = 0.001\n    case3 = (A3, L3, B3, b3, m0_3, m_true3, n3, alpha3)\n\n    # Test Case 4\n    A4 = np.array([\n        [1, 0, 1, 0], [0, 1, 0, 1], [1, 1, 0, 0], [0, 0, 1, 1],\n        [1, 0, 0, 1], [0, 1, 1, 0], [1, -1, 0, 0], [0, 0, 1, -1]\n    ])\n    L4 = np.identity(4)\n    m0_4 = np.zeros(4)\n    B4 = np.array([[1, 0, 0, 1], [0, 1, 1, 0]])\n    b4 = np.array([0, 0])\n    m_true4 = np.array([0.5, -0.5, 0.5, -0.5])\n    n4 = np.array([0.01, -0.01, 0, 0.005, -0.005, 0, 0, 0.002])\n    alpha4 = 10.0\n    case4 = (A4, L4, B4, b4, m0_4, m_true4, n4, alpha4)\n    \n    test_cases = [case1, case2, case3, case4]\n    \n    overall_results = []\n\n    for case in test_cases:\n        A, L, B, b, m0, m_true, n_noise, alpha = case\n        n = A.shape[1]\n        q = B.shape[0]\n        \n        d = A @ m_true + n_noise\n        \n        # --- KKT Solution ---\n        H = A.T @ A + (alpha**2) * (L.T @ L)\n        g = A.T @ d + (alpha**2) * (L.T @ L @ m0)\n        \n        KKT_matrix = np.zeros((n + q, n + q))\n        KKT_matrix[:n, :n] = H\n        KKT_matrix[:n, n:] = B.T\n        KKT_matrix[n:, :n] = B\n        \n        KKT_rhs = np.concatenate([g, b])\n        \n        sol_kkt = np.linalg.solve(KKT_matrix, KKT_rhs)\n        m_kkt = sol_kkt[:n]\n        \n        # --- Penalized Solutions ---\n        m_mus = []\n        for mu in mus:\n            H_mu = H + (mu**2) * (B.T @ B)\n            g_mu = g + (mu**2) * (B.T @ b)\n            m_mu = np.linalg.solve(H_mu, g_mu)\n            m_mus.append(m_mu)\n            \n        # --- Checks ---\n        test_passed = True\n        \n        # Distances and residuals\n        deltas = [np.linalg.norm(m_mu - m_kkt) for m_mu in m_mus]\n        residuals = [np.linalg.norm(B @ m_mu - b) for m_mu in m_mus]\n        \n        # Monotonicity checks\n        for i in range(len(mus) - 1):\n            if deltas[i]  deltas[i+1] - mono_tol:\n                test_passed = False\n                break\n            if residuals[i]  residuals[i+1] - mono_tol:\n                test_passed = False\n                break\n        \n        if not test_passed:\n            overall_results.append(False)\n            continue\n            \n        # Final accuracy check\n        if deltas[-1] > final_dist_tol:\n            test_passed = False\n        if residuals[-1] > final_resid_tol:\n            test_passed = False\n            \n        overall_results.append(test_passed)\n\n    print(f\"[{','.join(map(str, overall_results))}]\")\n\nsolve()\n```"
        }
    ]
}