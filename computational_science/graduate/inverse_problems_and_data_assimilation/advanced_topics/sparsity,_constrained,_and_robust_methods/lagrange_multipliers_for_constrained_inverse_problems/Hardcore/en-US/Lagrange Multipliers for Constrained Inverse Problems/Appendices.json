{
    "hands_on_practices": [
        {
            "introduction": "In many physical systems, certain quantities are conserved. For example, the total mass or energy within a closed domain might be a known constant. This exercise  provides foundational practice in setting up and solving the Karush-Kuhn-Tucker (KKT) system for this classic scenario, leading to an analytical expression for the Lagrange multiplier itself.",
            "id": "3395254",
            "problem": "Consider a linear inverse problem posed in a data assimilation framework, where a state vector $m \\in \\mathbb{R}^{n}$ is estimated from observations $d \\in \\mathbb{R}^{p}$ through a linear forward operator $F \\in \\mathbb{R}^{p \\times n}$. Assume a positive-definite data precision matrix $W \\in \\mathbb{R}^{p \\times p}$ and a positive-definite background covariance $B \\in \\mathbb{R}^{n \\times n}$ with background state $m_{b} \\in \\mathbb{R}^{n}$. The Maximum A Posteriori (MAP) estimator for $m$ under Gaussian assumptions is obtained by minimizing the quadratic objective\n$$\nJ(m) = \\frac{1}{2} (F m - d)^{T} W (F m - d) + \\frac{1}{2} (m - m_{b})^{T} B^{-1} (m - m_{b}),\n$$\nsubject to a linear invariant constraint $s^{T} m = c$, where $s \\in \\mathbb{R}^{n}$ is a given nonzero vector and $c \\in \\mathbb{R}$ is a prescribed scalar.\n\nStarting from the core definitions of constrained optimization and the Karush–Kuhn–Tucker (KKT) conditions for equality-constrained problems, derive the first-order optimality system that characterizes the constrained MAP solution. Then, solve analytically for the Lagrange multiplier associated with the constraint $s^{T} m = c$ in closed form, expressed in terms of $F$, $W$, $B$, $d$, $m_{b}$, $s$, and $c$. Clearly state any assumptions necessary to guarantee the uniqueness of the multiplier. Your final answer must be a single closed-form analytical expression for the multiplier. No rounding is required and no physical units are involved.",
            "solution": "The problem as stated is a classical constrained quadratic optimization problem arising in the context of variational data assimilation. We will first validate the problem statement.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- State vector to be estimated: $m \\in \\mathbb{R}^{n}$.\n- Observation vector: $d \\in \\mathbb{R}^{p}$.\n- Linear forward operator: $F \\in \\mathbb{R}^{p \\times n}$.\n- Data precision matrix: $W \\in \\mathbb{R}^{p \\times p}$, positive-definite.\n- Background covariance matrix: $B \\in \\mathbb{R}^{n \\times n}$, positive-definite.\n- Background state vector: $m_{b} \\in \\mathbb{R}^{n}$.\n- Objective function: $J(m) = \\frac{1}{2} (F m - d)^{T} W (F m - d) + \\frac{1}{2} (m - m_{b})^{T} B^{-1} (m - m_{b})$.\n- Linear constraint: $s^{T} m = c$, where $s \\in \\mathbb{R}^{n}$ is a given nonzero vector and $c \\in \\mathbb{R}$ is a prescribed scalar.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem represents a standard formulation for finding the Maximum A Posteriori (MAP) estimate in Bayesian inference under Gaussian assumptions, commonly known as 3D-Var or 4D-Var in geosciences. The objective function $J(m)$ is proportional to the negative logarithm of the posterior probability density. This is a fundamental and well-established method in inverse problems and data assimilation.\n- **Well-Posed:** The objective function $J(m)$ is a quadratic function of $m$. Its Hessian is given by $\\nabla_m^2 J(m) = F^T W F + B^{-1}$. Since $B$ is positive-definite, its inverse $B^{-1}$ is also positive-definite. The matrix $W$ is positive-definite, which implies that for any vector $v \\in \\mathbb{R}^n$, the quadratic form $v^T (F^T W F) v = (Fv)^T W (Fv) \\ge 0$, so $F^T W F$ is positive semi-definite. The sum of a positive-definite matrix ($B^{-1}$) and a positive semi-definite matrix ($F^T W F$) is a positive-definite matrix. Therefore, the Hessian of $J(m)$ is positive-definite, which makes $J(m)$ a strictly convex function. The constraint $s^T m = c$ defines a convex set (an affine hyperplane). The minimization of a strictly convex function over a convex set admits a unique solution. The problem is thus well-posed.\n- **Objective:** The problem is formulated using precise, unambiguous mathematical language and standard notation from linear algebra and optimization theory.\n- **Completeness and Consistency:** All necessary components (variables, matrices, their properties, the objective function, and the constraint) are explicitly defined. There are no contradictions in the setup.\n\n**Step 3: Verdict and Action**\nThe problem is valid as it is scientifically grounded, well-posed, objective, and self-contained. We may proceed with the solution.\n\n### Derivation of the Solution\n\nThe problem is to find the vector $m$ that minimizes the objective function $J(m)$ subject to the equality constraint $g(m) = s^T m - c = 0$. This constrained optimization problem can be solved using the method of Lagrange multipliers.\n\nWe define the Lagrangian function $\\mathcal{L}(m, \\lambda)$ which incorporates the constraint, with $\\lambda \\in \\mathbb{R}$ being the Lagrange multiplier:\n$$\n\\mathcal{L}(m, \\lambda) = J(m) + \\lambda (s^T m - c)\n$$\nSubstituting the expression for $J(m)$:\n$$\n\\mathcal{L}(m, \\lambda) = \\frac{1}{2} (F m - d)^{T} W (F m - d) + \\frac{1}{2} (m - m_{b})^{T} B^{-1} (m - m_{b}) + \\lambda (s^{T} m - c)\n$$\nThe first-order necessary conditions for optimality (the Karush-Kuhn-Tucker conditions for an equality constraint) state that the gradient of the Lagrangian with respect to both $m$ and $\\lambda$ must be zero at the optimal point $(m^*, \\lambda^*)$.\n\nThe first condition is stationarity with respect to $m$: $\\nabla_m \\mathcal{L}(m, \\lambda) = 0$. We compute the gradient term by term:\n$$\n\\nabla_m \\left( \\frac{1}{2} (F m - d)^{T} W (F m - d) \\right) = F^T W (Fm - d)\n$$\n$$\n\\nabla_m \\left( \\frac{1}{2} (m - m_{b})^{T} B^{-1} (m - m_{b}) \\right) = B^{-1} (m - m_b)\n$$\n$$\n\\nabla_m \\left( \\lambda (s^T m - c) \\right) = \\lambda s\n$$\nCombining these terms, the stationarity condition is:\n$$\n\\nabla_m \\mathcal{L}(m, \\lambda) = F^T W (Fm - d) + B^{-1} (m - m_b) + \\lambda s = 0\n$$\nThe second condition, $\\nabla_\\lambda \\mathcal{L}(m, \\lambda) = 0$, simply recovers the original constraint:\n$$\ns^T m - c = 0\n$$\nThis pair of equations forms the first-order optimality system for the constrained MAP solution $(m, \\lambda)$:\n1. $(F^T W F + B^{-1}) m + \\lambda s = F^T W d + B^{-1} m_b$\n2. $s^T m = c$\n\nTo solve for the Lagrange multiplier $\\lambda$, we first solve the first equation for $m$. Let us define the Hessian matrix $H = F^T W F + B^{-1}$ and the vector $g = F^T W d + B^{-1} m_b$. The system becomes:\n1. $H m + \\lambda s = g$\n2. $s^T m = c$\n\nAs established in the validation, the matrix $H$ is positive-definite and therefore invertible. From equation (1), we can express $m$ in terms of $\\lambda$:\n$$\nH m = g - \\lambda s\n$$\n$$\nm = H^{-1} (g - \\lambda s)\n$$\nNow, substitute this expression for $m$ into the constraint equation (2):\n$$\ns^T \\left( H^{-1} (g - \\lambda s) \\right) = c\n$$\nDistributing the transpose:\n$$\ns^T H^{-1} g - \\lambda s^T H^{-1} s = c\n$$\nWe can now isolate and solve for $\\lambda$. Rearranging the terms:\n$$\n\\lambda (s^T H^{-1} s) = s^T H^{-1} g - c\n$$\nA unique solution for $\\lambda$ exists if and only if the scalar coefficient $s^T H^{-1} s \\neq 0$. Since $H$ is positive-definite, its inverse $H^{-1}$ is also positive-definite. The problem states that $s$ is a nonzero vector. For any positive-definite matrix $A$ and any nonzero vector $x$, the quadratic form $x^T A x$ is strictly positive. Therefore, $s^T H^{-1} s  0$, which guarantees the uniqueness of the multiplier $\\lambda$.\n$$\n\\lambda = \\frac{s^T H^{-1} g - c}{s^T H^{-1} s}\n$$\nFinally, we substitute the definitions of $H$ and $g$ back into this expression to obtain the closed-form solution for the Lagrange multiplier in terms of the original problem data:\n$$\n\\lambda = \\frac{s^{T} (F^{T} W F + B^{-1})^{-1} (F^{T} W d + B^{-1} m_{b}) - c}{s^{T} (F^{T} W F + B^{-1})^{-1} s}\n$$\nThis is the analytical expression for the Lagrange multiplier associated with the constraint.",
            "answer": "$$\n\\boxed{\\frac{s^{T} (F^{T} W F + B^{-1})^{-1} (F^{T} W d + B^{-1} m_{b}) - c}{s^{T} (F^{T} W F + B^{-1})^{-1} s}}\n$$"
        },
        {
            "introduction": "Real-world problems often involve not just equalities, but also physical bounds or inequality constraints, such as requiring concentrations to be non-negative. This introduces the full Karush-Kuhn-Tucker (KKT) conditions, including complementary slackness. This practice  challenges you to move beyond simple cases by implementing an active-set algorithm to solve a general quadratic program, building a tangible connection between the abstract KKT theory and a concrete computational solution.",
            "id": "3395272",
            "problem": "Consider the quadratic inverse problem with linear equality and inequality constraints. Let $m \\in \\mathbb{R}^n$ denote the model parameters and let $A \\in \\mathbb{R}^{p \\times n}$ and $d \\in \\mathbb{R}^p$ define a least-squares data misfit $J(m) = \\tfrac{1}{2}\\lVert A m - d \\rVert_2^2$. Let $B \\in \\mathbb{R}^{r \\times n}$ and $b \\in \\mathbb{R}^r$ define linear inequality constraints $B m - b \\le 0$ (componentwise), and let $C \\in \\mathbb{R}^{q \\times n}$ and $c \\in \\mathbb{R}^q$ define linear equality constraints $C m - c = 0$. Using the method of Lagrange multipliers and the Karush-Kuhn-Tucker (KKT) conditions, the optimal solution $m^\\star \\in \\mathbb{R}^n$ for this convex problem satisfies a coupled set of equations involving the primal variables $m$, the inequality multipliers $\\lambda \\in \\mathbb{R}^r$, and the equality multipliers $\\nu \\in \\mathbb{R}^q$.\n\nTask 1 (derivation): Starting from the definition of the Lagrangian $\\mathcal{L}(m,\\lambda,\\nu) = \\tfrac{1}{2}\\lVert A m - d \\rVert_2^2 + \\lambda^\\top (B m - b) + \\nu^\\top (C m - c)$, derive the first-order necessary and sufficient optimality conditions for this convex problem, namely the Karush-Kuhn-Tucker (KKT) conditions, consisting of:\n- Stationarity with respect to $m$,\n- Primal feasibility for both the inequalities and equalities,\n- Dual feasibility for $\\lambda$,\n- Complementary slackness coupling $\\lambda$ with the inequality residuals.\n\nThen write these conditions explicitly as a linear system in block-matrix form by expressing the gradient of the quadratic term via the symmetric positive semidefinite matrix $H = A^\\top A$ and vector $g = A^\\top d$. Your block system must be explicit in terms of $H$, $B$, $C$, $g$, $b$, and $c$, and it must accommodate an arbitrary set $\\mathcal{I}$ of active inequality constraints (that is, those indices $i$ for which $(B m^\\star - b)_i = 0$).\n\nTask 2 (algorithm and implementation): Implement an algorithm that, given numerical instances $(A,B,C,d,b,c)$, determines an optimal solution $m^\\star$, the active set $\\mathcal{A} = \\{ i \\in \\{0,\\dots,r-1\\} : \\lvert (B m^\\star - b)_i \\rvert \\le \\tau \\}$, and the corresponding Lagrange multipliers $(\\lambda^\\star,\\nu^\\star)$ that satisfy the KKT conditions. Your program must:\n- Use only linear algebra and fundamental definitions; do not call a prepackaged quadratic programming solver.\n- Enumerate candidate active sets $\\mathcal{I} \\subseteq \\{0,\\dots,r-1\\}$ and, for each candidate, solve the associated KKT linear system that enforces $B_{\\mathcal{I}} m = b_{\\mathcal{I}}$ and $C m = c$, together with stationarity, to obtain $(m,\\lambda_{\\mathcal{I}},\\nu)$.\n- Accept a candidate if it is primal feasible (that is, $B m - b \\le \\tau \\mathbf{1}$ and $\\lVert C m - c \\rVert_2 \\le \\tau$), satisfies dual feasibility on the enforced multipliers (that is, $\\lambda_{\\mathcal{I}} \\ge -\\tau \\mathbf{1}$ componentwise), and minimizes the objective $J(m)$ among all accepted candidates.\n- Define the numerical tolerance to be $\\tau = 10^{-8}$ for feasibility and activity tests.\n\nTask 3 (test suite and output): Your program must solve the following three test cases. For each case, it must return:\n- The optimal model $m^\\star$ as a list of floats,\n- The active set $\\mathcal{A}$ as a list of zero-based integer indices,\n- The full inequality multiplier vector $\\lambda^\\star \\in \\mathbb{R}^r$ (zeros for inactive constraints),\n- The equality multiplier vector $\\nu^\\star \\in \\mathbb{R}^q$,\n- The optimal objective value $J(m^\\star)$,\nwith all floats rounded to six decimal places.\n\nUse the following instances:\n\n- Test case 1:\n  - $n = 2$, $p = 2$, $r = 2$, $q = 1$.\n  - $A = \\begin{bmatrix} 1  0 \\\\ 0  2 \\end{bmatrix}$, $d = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$.\n  - $B = \\begin{bmatrix} -1  0 \\\\ 0  1 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0.5 \\end{bmatrix}$.\n  - $C = \\begin{bmatrix} 1  1 \\end{bmatrix}$, $c = \\begin{bmatrix} 1 \\end{bmatrix}$.\n\n- Test case 2:\n  - $n = 2$, $p = 2$, $r = 4$, $q = 0$.\n  - $A = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}$, $d = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix}$.\n  - $B = \\begin{bmatrix} -1  0 \\\\ 0  -1 \\\\ 1  0 \\\\ 0  1 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n  - $C$ has no rows, and $c$ has no entries.\n\n- Test case 3:\n  - $n = 3$, $p = 3$, $r = 3$, $q = 1$.\n  - $A = \\begin{bmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{bmatrix}$, $d = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n  - $B = \\begin{bmatrix} -1  0  0 \\\\ 0  -1  0 \\\\ 0  0  -1 \\end{bmatrix}$ with $b = \\begin{bmatrix} 0 \\\\ 0 \\\\ -0.4 \\end{bmatrix}$, which encodes $m_1 \\ge 0$, $m_2 \\ge 0$, and $m_3 \\ge 0.4$.\n  - $C = \\begin{bmatrix} 1  1  1 \\end{bmatrix}$, $c = \\begin{bmatrix} 1 \\end{bmatrix}$.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list of the form $[m^\\star, \\mathcal{A}, \\lambda^\\star, \\nu^\\star, J(m^\\star)]$.\n- All floating-point numbers must be rounded to six decimal places in the printed output.\n- Example structural format (illustrative only): $[[[m\\_1,m\\_2], [i\\_1,i\\_2], [\\lambda\\_1,\\dots], [\\nu\\_1,\\dots], J], \\dots]$.",
            "solution": "The user has provided a well-defined constrained quadratic optimization problem and has requested a solution based on the Karush-Kuhn-Tucker (KKT) conditions, implemented via an active-set enumeration algorithm.\n\n### Problem Statement Validation\n\nThe problem is critically validated against the required criteria.\n\n**Step 1: Extracted Givens**\n- **Objective Function**: Minimize $J(m) = \\tfrac{1}{2}\\lVert A m - d \\rVert_2^2$, where $m \\in \\mathbb{R}^n$, $A \\in \\mathbb{R}^{p \\times n}$, $d \\in \\mathbb{R}^p$.\n- **Inequality Constraints**: $B m - b \\le 0$, where $B \\in \\mathbb{R}^{r \\times n}$, $b \\in \\mathbb{R}^r$.\n- **Equality Constraints**: $C m - c = 0$, where $C \\in \\mathbb{R}^{q \\times n}$, $c \\in \\mathbb{R}^q$.\n- **Lagrangian**: $\\mathcal{L}(m,\\lambda,\\nu) = \\tfrac{1}{2}\\lVert A m - d \\rVert_2^2 + \\lambda^\\top (B m - b) + \\nu^\\top (C m - c)$, with multipliers $\\lambda \\in \\mathbb{R}^r$ and $\\nu \\in \\mathbb{R}^q$.\n- **Task 1**: Derive the KKT conditions and express them as a block-matrix linear system for a given active set $\\mathcal{I}$.\n- **Task 2**: Implement an active-set enumeration algorithm to find the optimal solution $(m^\\star, \\lambda^\\star, \\nu^\\star)$ by solving the KKT system for each candidate active set and selecting the best feasible solution. The numerical tolerance is defined as $\\tau = 10^{-8}$.\n- **Task 3**: Solve three specific numerical instances and provide the results in a specified format.\n\n**Step 2: Validation Using Extracted Givens**\n- **Scientific Grounding**: The problem is a standard quadratic program (QP), a fundamental topic in convex optimization and inverse problems. The use of Lagrange multipliers and KKT conditions is the standard theoretical approach for solving such problems. The problem is scientifically and mathematically sound.\n- **Well-Posed**: The problem is well-posed. The objective function is convex, and the constraints are linear, defining a convex feasible set. Therefore, a unique global minimum exists, provided the KKT system for the true active set is non-singular.\n- **Objective**: The problem is stated using precise, objective mathematical language.\n- **Completeness and Consistency**: The problem is self-contained. All necessary matrices, vectors, functions, and tasks are explicitly defined. The provided test cases are dimensionally consistent. The prescribed algorithm, while computationally intensive for large $r$, is a valid method for solving the problem and is feasible for the given test cases.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. The numerical solution provided in the original XML was found to be incorrect for two of the three test cases. A corrected implementation and solution are provided below.\n\n### Task 1: Derivation of the KKT Conditions\n\nThe optimization problem is to minimize a convex quadratic function subject to linear constraints:\n$$\n\\min_{m \\in \\mathbb{R}^n} \\quad J(m) = \\frac{1}{2} \\lVert A m - d \\rVert_2^2 \\\\\n\\text{subject to} \\quad B m - b \\le 0 \\\\\n\\text{and} \\quad C m - c = 0\n$$\nThe Lagrangian for this problem is:\n$$\n\\mathcal{L}(m, \\lambda, \\nu) = J(m) + \\lambda^\\top (B m - b) + \\nu^\\top (C m - c)\n$$\nwhere $\\lambda \\in \\mathbb{R}^r$ are the Lagrange multipliers for the inequality constraints and $\\nu \\in \\mathbb{R}^q$ are the multipliers for the equality constraints.\n\nFor a convex problem, the Karush-Kuhn-Tucker (KKT) conditions are both necessary and sufficient for optimality. A point $m^\\star$ is an optimal solution if and only if there exist multipliers $\\lambda^\\star$ and $\\nu^\\star$ such that the following five conditions hold:\n\n1.  **Stationarity**: The gradient of the Lagrangian with respect to the primal variables $m$ must vanish at the optimum: $\\nabla_m \\mathcal{L}(m^\\star, \\lambda^\\star, \\nu^\\star) = 0$.\n    The gradient of the objective function is $\\nabla_m J(m) = A^\\top(A m - d) = (A^\\top A)m - A^\\top d$. Let $H = A^\\top A$ (a symmetric positive semidefinite matrix) and $g = A^\\top d$. Then $\\nabla_m J(m) = H m - g$.\n    The gradients of the constraint terms are $\\nabla_m(\\lambda^\\top B m) = B^\\top \\lambda$ and $\\nabla_m(\\nu^\\top C m) = C^\\top \\nu$.\n    The stationarity condition is thus:\n    $$H m^\\star + B^\\top \\lambda^\\star + C^\\top \\nu^\\star - g = 0$$\n\n2.  **Primal Feasibility (Inequalities)**: The optimal solution must satisfy the inequality constraints.\n    $$B m^\\star - b \\le 0$$\n\n3.  **Primal Feasibility (Equalities)**: The optimal solution must satisfy the equality constraints.\n    $$C m^\\star - c = 0$$\n\n4.  **Dual Feasibility**: The multipliers corresponding to the inequality constraints must be non-negative.\n    $$\\lambda^\\star \\ge 0 \\quad (\\text{component-wise})$$\n\n5.  **Complementary Slackness**: For each inequality constraint, either the constraint holds with equality (is \"active\") or its corresponding multiplier is zero. This is expressed for each component $i \\in \\{0, \\dots, r-1\\}$ as:\n    $$\\lambda_i^\\star (B m^\\star - b)_i = 0$$\n\nTo formulate the linear system for a candidate active set $\\mathcal{I} \\subseteq \\{0, ..., r-1\\}$, we enforce the corresponding inequality constraints as equalities. For any $i \\in \\mathcal{I}$, we set $(B m - b)_i = 0$. For any $i \\notin \\mathcal{I}$, the complementary slackness condition implies $\\lambda_i=0$.\n\nLet $k = |\\mathcal{I}|$ be the number of active inequality constraints. Let $B_{\\mathcal{I}} \\in \\mathbb{R}^{k \\times n}$ and $b_{\\mathcal{I}} \\in \\mathbb{R}^k$ be the rows and entries of $B$ and $b$ corresponding to the indices in $\\mathcal{I}$. Similarly, let $\\lambda_{\\mathcal{I}} \\in \\mathbb{R}^k$ be the vector of active multipliers. The stationarity condition simplifies to $H m + B_{\\mathcal{I}}^\\top \\lambda_{\\mathcal{I}} + C^\\top \\nu = g$, since all other $\\lambda_i$ are zero.\n\nCombining the modified stationarity equation with the active inequality constraints and the equality constraints, we obtain a coupled linear system for the unknowns $(m, \\lambda_{\\mathcal{I}}, \\nu)$:\n$$\n\\begin{cases}\nH m + B_{\\mathcal{I}}^\\top \\lambda_{\\mathcal{I}} + C^\\top \\nu = g \\\\\nB_{\\mathcal{I}} m = b_{\\mathcal{I}} \\\\\nC m = c\n\\end{cases}\n$$\nThis can be written in block-matrix form as:\n$$\n\\begin{bmatrix}\nH  B_{\\mathcal{I}}^\\top  C^\\top \\\\\nB_{\\mathcal{I}}  0  0 \\\\\nC  0  0\n\\end{bmatrix}\n\\begin{bmatrix}\nm \\\\\n\\lambda_{\\mathcal{I}} \\\\\n\\nu\n\\end{bmatrix}\n=\n\\begin{bmatrix}\ng \\\\\nb_{\\mathcal{I}} \\\\\nc\n\\end{bmatrix}\n$$\nThe zero blocks are matrices of appropriate dimensions. This is a symmetric indefinite saddle-point system.\n\n### Task 2: Algorithm Design\n\nThe implemented algorithm follows the user's specification. It performs an exhaustive search over all $2^r$ possible active sets $\\mathcal{I} \\subseteq \\{0, \\dots, r-1\\}$. For each candidate set $\\mathcal{I}$:\n1.  The KKT block-matrix system is constructed. Special care is taken for cases where the active set $\\mathcal{I}$ is empty ($k=0$) or where there are no equality constraints ($q=0$).\n2.  The linear system is solved for $(m, \\lambda_{\\mathcal{I}}, \\nu)$. If the system is singular, the candidate set is discarded.\n3.  The resulting solution is checked for full KKT feasibility:\n    - Primal feasibility: $B m - b \\le \\tau \\mathbf{1}$ and $\\lVert C m - c \\rVert_2 \\le \\tau$.\n    - Dual feasibility: $\\lambda_{\\mathcal{I}} \\ge -\\tau \\mathbf{1}$.\n4.  If the solution is feasible, its objective value $J(m)$ is computed.\n5.  The algorithm tracks the feasible solution that yields the minimum objective value. This solution, being a KKT point for a convex problem, is the global optimum.\n6.  Finally, the true active set $\\mathcal{A}$ is determined from the optimal solution $m^\\star$ by identifying all inequality constraints $i$ for which $|(B m^\\star - b)_i| \\le \\tau$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import chain, combinations\n\ndef solve():\n    \"\"\"\n    Main function to define and solve the test cases for the constrained quadratic inverse problem.\n    \"\"\"\n    \n    # Numerical tolerance for feasibility and active set checks.\n    tau = 1e-8\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1\n        {\n            'A': np.array([[1., 0.], [0., 2.]]),\n            'd': np.array([1., 2.]),\n            'B': np.array([[-1., 0.], [0., 1.]]),\n            'b': np.array([0., 0.5]),\n            'C': np.array([[1., 1.]]),\n            'c': np.array([1.])\n        },\n        # Test case 2\n        {\n            'A': np.array([[1., 0.], [0., 1.]]),\n            'd': np.array([2., -1.]),\n            'B': np.array([[-1., 0.], [0., -1.], [1., 0.], [0., 1.]]),\n            'b': np.array([0., 0., 1., 1.]),\n            'C': np.empty((0, 2)),\n            'c': np.empty((0,))\n        },\n        # Test case 3\n        {\n            'A': np.array([[1., 0., 0.], [0., 2., 0.], [0., 0., 3.]]),\n            'd': np.array([1., 1., 1.]),\n            'B': np.array([[-1., 0., 0.], [0., -1., 0.], [0., 0., -1.]]),\n            'b': np.array([0., 0., -0.4]),\n            'C': np.array([[1., 1., 1.]]),\n            'c': np.array([1.])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        solution = solve_qp_by_enumeration(case['A'], case['d'], case['B'], case['b'], case['C'], case['c'], tau)\n        \n        # Format the results as specified\n        m_star = solution['m']\n        lambda_star = solution['lambda']\n        nu_star = solution['nu']\n        J_star = solution['J']\n        \n        # Determine final active set from the optimal solution m_star\n        if m_star is not None and case['B'].shape[0] > 0:\n            final_residuals = case['B'] @ m_star - case['b']\n            active_set = np.where(np.abs(final_residuals) = tau)[0]\n        else:\n            active_set = []\n\n        # Convert to lists with specified rounding\n        m_list = [round(x, 6) for x in m_star] if m_star is not None else []\n        active_set_list = [int(i) for i in active_set]\n        lambda_list = [round(x, 6) for x in lambda_star] if lambda_star is not None else []\n        nu_list = [round(x, 6) for x in nu_star] if nu_star is not None else []\n        J_val = round(J_star, 6) if J_star is not None else float('inf')\n\n        results.append([m_list, active_set_list, lambda_list, nu_list, J_val])\n    \n    # Custom string representation to match the required output format\n    output_str = \"[\"\n    for i, res in enumerate(results):\n        m_str = f\"[{', '.join(f'{x:.6f}' for x in res[0])}]\"\n        a_str = f\"[{', '.join(map(str, res[1]))}]\"\n        l_str = f\"[{', '.join(f'{x:.6f}' for x in res[2])}]\"\n        n_str = f\"[{', '.join(f'{x:.6f}' for x in res[3])}]\"\n        j_str = f\"{res[4]:.6f}\"\n        output_str += f\"[{m_str}, {a_str}, {l_str}, {n_str}, {j_str}]\"\n        if i  len(results) - 1:\n            output_str += \", \"\n    output_str += \"]\"\n    \n    print(output_str)\n\ndef solve_qp_by_enumeration(A, d, B, b, C, c, tau):\n    \"\"\"\n    Solves a constrained quadratic program by enumerating all possible active sets.\n    \"\"\"\n    n = A.shape[1]\n    r = B.shape[0] if B.ndim > 1 and B.shape[0] > 0 else 0\n    q = C.shape[0] if C.ndim > 1 and C.shape[0] > 0 else 0\n\n    H = A.T @ A\n    g = A.T @ d\n\n    best_solution = {\n        'm': None,\n        'lambda': None,\n        'nu': None,\n        'J': float('inf')\n    }\n\n    indices = range(r)\n    powerset = chain.from_iterable(combinations(indices, k) for k in range(r + 1))\n\n    for I_tuple in powerset:\n        I = list(I_tuple)\n        k = len(I)\n\n        B_I = B[I, :] if k > 0 else np.empty((0, n))\n        b_I = b[I] if k > 0 else np.empty(0)\n\n        # Build KKT system K z = f, where z = [m, lambda_I, nu]^T\n        kkt_size = n + k + q\n        K = np.zeros((kkt_size, kkt_size))\n        f = np.zeros(kkt_size)\n\n        # Top-left block: H\n        K[0:n, 0:n] = H\n        # RHS part for m\n        f[0:n] = g\n\n        # Blocks for active inequality constraints\n        if k > 0:\n            K[0:n, n:n+k] = B_I.T\n            K[n:n+k, 0:n] = B_I\n            f[n:n+k] = b_I\n\n        # Blocks for equality constraints\n        if q > 0:\n            K[0:n, n+k:n+k+q] = C.T\n            K[n+k:n+k+q, 0:n] = C\n            f[n+k:n+k+q] = c\n\n        if kkt_size == 0: continue\n        \n        try:\n            z = np.linalg.solve(K, f)\n        except np.linalg.LinAlgError:\n            continue  # Singular matrix, this active set is not valid.\n\n        m_sol = z[:n]\n        lambda_I_sol = z[n:n+k]\n        nu_sol = z[n+k:]\n\n        # --- Check Feasibility of the Candidate Solution ---\n        # 1. Primal feasibility for all inequality constraints\n        if r > 0 and not np.all(B @ m_sol - b = tau):\n            continue\n        \n        # 2. Primal feasibility for equality constraints\n        if q > 0 and np.linalg.norm(C @ m_sol - c) > tau:\n            continue\n        \n        # 3. Dual feasibility for active inequality multipliers\n        if k > 0 and not np.all(lambda_I_sol >= -tau):\n            continue\n        \n        # --- If feasible, check if it's a better solution ---\n        J_val = 0.5 * np.linalg.norm(A @ m_sol - d)**2\n        \n        if J_val  best_solution['J']:\n            full_lambda = np.zeros(r)\n            if k > 0:\n                full_lambda[I] = lambda_I_sol\n            \n            best_solution['m'] = m_sol\n            best_solution['lambda'] = full_lambda\n            best_solution['nu'] = nu_sol\n            best_solution['J'] = J_val\n            \n    return best_solution\n\nif __name__ == \"__main__\":\n    solve()\n# Expected output: [[0.500000, 0.500000], [1], [0.000000, 1.500000], [0.500000], 0.625000], [[1.000000, 0.000000], [1, 2], [0.000000, 1.000000, 1.000000, 0.000000], [], 1.000000], [[0.280000, 0.320000, 0.400000], [2], [0.000000, 0.000000, 1.320000], [0.720000], 0.344000]]\n```"
        },
        {
            "introduction": "This practice bridges two fundamental paradigms for incorporating information: deterministic constraints and probabilistic priors. By comparing the exact Lagrange multiplier solution for a hard constraint with a Bayesian MAP estimate using an increasingly tight Gaussian prior, you will demonstrate a profound connection. This exercise  reveals that the Lagrange multiplier method can be viewed as the limiting case of a Bayesian model with infinite confidence in its prior information about the constraint.",
            "id": "3395274",
            "problem": "Consider the linear observation model with additive Gaussian noise\n$$\n\\mathbf{y} = \\mathbf{A}\\,\\mathbf{x} + \\boldsymbol{\\varepsilon},\n$$\nwhere $\\mathbf{A} \\in \\mathbb{R}^{2 \\times 2}$, $\\mathbf{x} \\in \\mathbb{R}^{2}$, $\\mathbf{y} \\in \\mathbb{R}^{2}$, and $\\boldsymbol{\\varepsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Gamma}_{\\varepsilon})$. Suppose $\\boldsymbol{\\Gamma}_{\\varepsilon} = \\mathbf{I}_{2}$ and the known data and operators are\n$$\n\\mathbf{A} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}, \\qquad \\mathbf{y} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}.\n$$\nImpose the hard linear constraint $\\mathbf{C}\\,\\mathbf{x} = \\mathbf{d}$ with\n$$\n\\mathbf{C} = \\begin{pmatrix} 1  -1 \\end{pmatrix}, \\qquad \\mathbf{d} = 0.\n$$\n\n1) Using Lagrange multipliers, compute the constrained least-squares solution $\\mathbf{x}_{\\mathrm{c}}$ that minimizes the quadratic loss $J(\\mathbf{x}) = \\tfrac{1}{2}\\|\\mathbf{y} - \\mathbf{A}\\mathbf{x}\\|_{2}^{2}$ subject to $\\mathbf{C}\\mathbf{x} = \\mathbf{d}$.\n\n2) Consider a Maximum A Posteriori (MAP) estimator under an increasingly tight Gaussian prior that penalizes deviations from the constraint, modeled as $\\mathbf{C}\\mathbf{x} - \\mathbf{d} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2}\\mathbf{I}_{1})$, independent of the observational noise. Let $\\lambda = \\sigma^{-2}  0$ denote the prior precision on the constraint. Derive the unconstrained MAP estimate $\\mathbf{x}_{\\lambda}$ that minimizes the objective\n$$\n\\Phi_{\\lambda}(\\mathbf{x}) = \\tfrac{1}{2}\\|\\mathbf{y} - \\mathbf{A}\\mathbf{x}\\|_{2}^{2} + \\tfrac{\\lambda}{2}\\|\\mathbf{C}\\mathbf{x} - \\mathbf{d}\\|_{2}^{2}.\n$$\nExpress $\\mathbf{x}_{\\lambda}$ in closed form as a function of $\\lambda$.\n\n3) Compare the two estimators by taking the limit $\\lambda \\to \\infty$ of $\\mathbf{x}_{\\lambda}$ and relating it to $\\mathbf{x}_{\\mathrm{c}}$.\n\nAnswer specification: Provide your final answer as a single row matrix containing, in order, the two components of $\\mathbf{x}_{\\mathrm{c}}$ followed by the two components of $\\mathbf{x}_{\\lambda}$ expressed in closed form as functions of $\\lambda$. No rounding is required and no units are involved. For example, your final answer should have the form $\\begin{pmatrix} x_{\\mathrm{c},1}  x_{\\mathrm{c},2}  x_{\\lambda,1}  x_{\\lambda,2} \\end{pmatrix}$.",
            "solution": "The problem is validated as scientifically grounded, well-posed, objective, and complete. All necessary information is provided, and the problem asks for the application of standard, well-established methods in optimization and inverse problems. We can proceed with the solution.\n\nThe problem is divided into three parts. We will solve each part sequentially. Let $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$.\n\nPart 1: Constrained Least-Squares Solution $\\mathbf{x}_{\\mathrm{c}}$\n\nThe first part requires minimizing the quadratic loss function $J(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{y} - \\mathbf{A}\\mathbf{x}\\|_{2}^{2}$ subject to the linear equality constraint $\\mathbf{C}\\mathbf{x} = \\mathbf{d}$. This is a constrained optimization problem that can be solved using the method of Lagrange multipliers.\n\nThe Lagrangian function $\\mathcal{L}(\\mathbf{x}, \\mu)$ is defined as:\n$$\n\\mathcal{L}(\\mathbf{x}, \\mu) = J(\\mathbf{x}) + \\boldsymbol{\\mu}^T (\\mathbf{C}\\mathbf{x} - \\mathbf{d})\n$$\nSince the constraint is scalar ($\\mathbf{C} \\in \\mathbb{R}^{1 \\times 2}$, $\\mathbf{d} \\in \\mathbb{R}$), the Lagrange multiplier $\\mu$ is a scalar.\n$$\n\\mathcal{L}(\\mathbf{x}, \\mu) = \\frac{1}{2}(\\mathbf{y} - \\mathbf{A}\\mathbf{x})^T(\\mathbf{y} - \\mathbf{A}\\mathbf{x}) + \\mu(\\mathbf{C}\\mathbf{x} - \\mathbf{d})\n$$\nThe first-order necessary conditions for a minimum are found by setting the gradients of $\\mathcal{L}$ with respect to $\\mathbf{x}$ and $\\mu$ to zero.\n\nThe gradient with respect to $\\mathbf{x}$ is:\n$$\n\\nabla_{\\mathbf{x}} \\mathcal{L}(\\mathbf{x}, \\mu) = \\nabla_{\\mathbf{x}} \\left( \\frac{1}{2}(\\mathbf{y}^T\\mathbf{y} - 2\\mathbf{y}^T\\mathbf{A}\\mathbf{x} + \\mathbf{x}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{x}) \\right) + \\nabla_{\\mathbf{x}}(\\mu\\mathbf{C}\\mathbf{x} - \\mu\\mathbf{d}) = \\mathbf{0}\n$$\n$$\n-\\mathbf{A}^T\\mathbf{y} + \\mathbf{A}^T\\mathbf{A}\\mathbf{x} + \\mathbf{C}^T\\mu = \\mathbf{0} \\implies \\mathbf{A}^T\\mathbf{A}\\mathbf{x} + \\mathbf{C}^T\\mu = \\mathbf{A}^T\\mathbf{y}\n$$\nThe gradient with respect to $\\mu$ gives back the constraint:\n$$\n\\frac{\\partial \\mathcal{L}(\\mathbf{x}, \\mu)}{\\partial \\mu} = \\mathbf{C}\\mathbf{x} - \\mathbf{d} = 0\n$$\nThese two equations form a system of linear equations for the optimal state $\\mathbf{x}_{\\mathrm{c}}$ and the multiplier $\\mu$:\n$$\n\\begin{pmatrix}\n\\mathbf{A}^T\\mathbf{A}  \\mathbf{C}^T \\\\\n\\mathbf{C}  0\n\\end{pmatrix}\n\\begin{pmatrix}\n\\mathbf{x}_{\\mathrm{c}} \\\\\n\\mu\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\mathbf{A}^T\\mathbf{y} \\\\\n\\mathbf{d}\n\\end{pmatrix}\n$$\nWe now substitute the given values:\n$\\mathbf{A} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix}$, $\\mathbf{y} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$, $\\mathbf{C} = \\begin{pmatrix} 1  -1 \\end{pmatrix}$, $\\mathbf{d} = 0$.\nThe matrix $\\mathbf{A}$ is symmetric, so $\\mathbf{A}^T = \\mathbf{A}$.\n$$\n\\mathbf{A}^T\\mathbf{A} = \\mathbf{A}^2 = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} = \\begin{pmatrix} 5  4 \\\\ 4  5 \\end{pmatrix}\n$$\n$$\n\\mathbf{A}^T\\mathbf{y} = \\mathbf{A}\\mathbf{y} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 5 \\end{pmatrix}\n$$\n$$\n\\mathbf{C}^T = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\nThe system of equations becomes:\n$$\n\\begin{pmatrix}\n5  4  1 \\\\\n4  5  -1 \\\\\n1  -1  0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\\nx_2 \\\\\n\\mu\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n4 \\\\\n5 \\\\\n0\n\\end{pmatrix}\n$$\nFrom the third equation, we have the constraint $x_1 - x_2 = 0$, which implies $x_1 = x_2$.\nSubstituting $x_1 = x_2$ into the first two equations:\n1) $5x_1 + 4x_1 + \\mu = 4 \\implies 9x_1 + \\mu = 4$\n2) $4x_1 + 5x_1 - \\mu = 5 \\implies 9x_1 - \\mu = 5$\nAdding these two new equations yields:\n$$\n(9x_1 + \\mu) + (9x_1 - \\mu) = 4 + 5 \\implies 18x_1 = 9 \\implies x_1 = \\frac{1}{2}\n$$\nSince $x_1 = x_2$, we have $x_2 = \\frac{1}{2}$.\nThe constrained least-squares solution is $\\mathbf{x}_{\\mathrm{c}} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix}$.\n\nPart 2: Regularized Least-Squares Solution $\\mathbf{x}_{\\lambda}$\n\nThe second part seeks to minimize the unconstrained objective function:\n$$\n\\Phi_{\\lambda}(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{y} - \\mathbf{A}\\mathbf{x}\\|_{2}^{2} + \\frac{\\lambda}{2}\\|\\mathbf{C}\\mathbf{x} - \\mathbf{d}\\|_{2}^{2}\n$$\nThis is a quadratic function of $\\mathbf{x}$, and its minimum can be found by setting its gradient with respect to $\\mathbf{x}$ to zero.\n$$\n\\nabla_{\\mathbf{x}} \\Phi_{\\lambda}(\\mathbf{x}) = \\nabla_{\\mathbf{x}} \\left( \\frac{1}{2}(\\mathbf{y} - \\mathbf{A}\\mathbf{x})^T(\\mathbf{y} - \\mathbf{A}\\mathbf{x}) \\right) + \\nabla_{\\mathbf{x}} \\left( \\frac{\\lambda}{2}(\\mathbf{C}\\mathbf{x} - \\mathbf{d})^T(\\mathbf{C}\\mathbf{x} - \\mathbf{d}) \\right) = \\mathbf{0}\n$$\nThis gives:\n$$\n(\\mathbf{A}^T\\mathbf{A}\\mathbf{x} - \\mathbf{A}^T\\mathbf{y}) + \\lambda(\\mathbf{C}^T\\mathbf{C}\\mathbf{x} - \\mathbf{C}^T\\mathbf{d}) = \\mathbf{0}\n$$\nRearranging the terms to solve for the minimizer $\\mathbf{x}_{\\lambda}$:\n$$\n(\\mathbf{A}^T\\mathbf{A} + \\lambda\\mathbf{C}^T\\mathbf{C})\\mathbf{x}_{\\lambda} = \\mathbf{A}^T\\mathbf{y} + \\lambda\\mathbf{C}^T\\mathbf{d}\n$$\n$$\n\\mathbf{x}_{\\lambda} = (\\mathbf{A}^T\\mathbf{A} + \\lambda\\mathbf{C}^T\\mathbf{C})^{-1} (\\mathbf{A}^T\\mathbf{y} + \\lambda\\mathbf{C}^T\\mathbf{d})\n$$\nWe substitute the given values, noting that $\\mathbf{d} = 0$:\n$$\n\\mathbf{C}^T\\mathbf{C} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\begin{pmatrix} 1  -1 \\end{pmatrix} = \\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix}\n$$\nThe right-hand side is $\\mathbf{A}^T\\mathbf{y} = \\begin{pmatrix} 4 \\\\ 5 \\end{pmatrix}$.\nThe matrix to be inverted is:\n$$\n\\mathbf{M}_{\\lambda} = \\mathbf{A}^T\\mathbf{A} + \\lambda\\mathbf{C}^T\\mathbf{C} = \\begin{pmatrix} 5  4 \\\\ 4  5 \\end{pmatrix} + \\lambda \\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix} = \\begin{pmatrix} 5+\\lambda  4-\\lambda \\\\ 4-\\lambda  5+\\lambda \\end{pmatrix}\n$$\nThe determinant of this matrix is:\n$$\n\\det(\\mathbf{M}_{\\lambda}) = (5+\\lambda)^2 - (4-\\lambda)^2 = (25+10\\lambda+\\lambda^2) - (16-8\\lambda+\\lambda^2) = 9 + 18\\lambda = 9(1+2\\lambda)\n$$\nThe inverse is:\n$$\n\\mathbf{M}_{\\lambda}^{-1} = \\frac{1}{9(1+2\\lambda)} \\begin{pmatrix} 5+\\lambda  -(4-\\lambda) \\\\ -(4-\\lambda)  5+\\lambda \\end{pmatrix} = \\frac{1}{9(1+2\\lambda)} \\begin{pmatrix} 5+\\lambda  \\lambda-4 \\\\ \\lambda-4  5+\\lambda \\end{pmatrix}\n$$\nNow we compute $\\mathbf{x}_{\\lambda} = \\mathbf{M}_{\\lambda}^{-1}(\\mathbf{A}^T\\mathbf{y})$:\n$$\n\\mathbf{x}_{\\lambda} = \\frac{1}{9(1+2\\lambda)} \\begin{pmatrix} 5+\\lambda  \\lambda-4 \\\\ \\lambda-4  5+\\lambda \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 5 \\end{pmatrix}\n$$\n$$\n\\mathbf{x}_{\\lambda} = \\frac{1}{9(1+2\\lambda)} \\begin{pmatrix} 4(5+\\lambda) + 5(\\lambda-4) \\\\ 4(\\lambda-4) + 5(5+\\lambda) \\end{pmatrix} = \\frac{1}{9(1+2\\lambda)} \\begin{pmatrix} 20 + 4\\lambda + 5\\lambda - 20 \\\\ 4\\lambda - 16 + 25 + 5\\lambda \\end{pmatrix}\n$$\n$$\n\\mathbf{x}_{\\lambda} = \\frac{1}{9(1+2\\lambda)} \\begin{pmatrix} 9\\lambda \\\\ 9\\lambda+9 \\end{pmatrix} = \\frac{9}{9(1+2\\lambda)} \\begin{pmatrix} \\lambda \\\\ \\lambda+1 \\end{pmatrix} = \\frac{1}{1+2\\lambda} \\begin{pmatrix} \\lambda \\\\ \\lambda+1 \\end{pmatrix}\n$$\nSo, the components of $\\mathbf{x}_{\\lambda}$ are:\n$$\nx_{\\lambda,1} = \\frac{\\lambda}{1+2\\lambda}, \\qquad x_{\\lambda,2} = \\frac{\\lambda+1}{1+2\\lambda}\n$$\n\nPart 3: Comparison of Estimators\n\nFinally, we compare the two estimators by taking the limit of $\\mathbf{x}_{\\lambda}$ as the regularization parameter $\\lambda$ approaches infinity. This corresponds to placing an infinitely strong penalty on any deviation from the constraint $\\mathbf{C}\\mathbf{x}=\\mathbf{d}$.\nFor the first component:\n$$\n\\lim_{\\lambda \\to \\infty} x_{\\lambda,1} = \\lim_{\\lambda \\to \\infty} \\frac{\\lambda}{1+2\\lambda} = \\lim_{\\lambda \\to \\infty} \\frac{1}{1/\\lambda+2} = \\frac{1}{0+2} = \\frac{1}{2}\n$$\nFor the second component:\n$$\n\\lim_{\\lambda \\to \\infty} x_{\\lambda,2} = \\lim_{\\lambda \\to \\infty} \\frac{\\lambda+1}{1+2\\lambda} = \\lim_{\\lambda \\to \\infty} \\frac{1+1/\\lambda}{1/\\lambda+2} = \\frac{1+0}{0+2} = \\frac{1}{2}\n$$\nIn the limit, we have:\n$$\n\\lim_{\\lambda \\to \\infty} \\mathbf{x}_{\\lambda} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix}\n$$\nThis result is identical to the constrained least-squares solution $\\mathbf{x}_{\\mathrm{c}} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix}$ found in Part 1. This demonstrates that the regularized least-squares problem with an infinitely large penalty on constraint violation converges to the hard-constrained least-squares problem.\n\nThe final answer requires the components of $\\mathbf{x}_{\\mathrm{c}}$ and $\\mathbf{x}_{\\lambda}(\\lambda)$ in a single row matrix.\n$$\n\\mathbf{x}_{\\mathrm{c},1} = \\frac{1}{2}, \\quad \\mathbf{x}_{\\mathrm{c},2} = \\frac{1}{2}\n$$\n$$\n\\mathbf{x}_{\\lambda,1} = \\frac{\\lambda}{1+2\\lambda}, \\quad \\mathbf{x}_{\\lambda,2} = \\frac{\\lambda+1}{1+2\\lambda}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2}  \\frac{\\lambda}{1 + 2\\lambda}  \\frac{\\lambda + 1}{1 + 2\\lambda} \\end{pmatrix}}\n$$"
        }
    ]
}