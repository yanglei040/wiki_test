{
    "hands_on_practices": [
        {
            "introduction": "投影梯度法的核心在于将优化步骤分解为一个标准的梯度更新和一个投影回可行集的步骤。本练习 () 专注于掌握在概率单纯形上的投影步骤，这在统计学和机器学习中是一个至关重要的约束集。通过从 Karush-Kuhn-Tucker (KKT) 条件出发推导出一个高效的算法，您将获得一个对于高级应用至关重要、兼具实用性与理论基础的工具。",
            "id": "3414855",
            "problem": "考虑在闭凸集 $C \\subset \\mathbb{R}^n$ 上的欧几里得投影，对于任意 $x \\in \\mathbb{R}^n$，其定义为 $P_C(x) = \\arg\\min_{z \\in C} \\frac{1}{2}\\|z - x\\|_2^2$。在许多逆问题和数据同化应用中，决策变量表示 $n$ 个状态上的概率分布，并且必须位于概率单纯形 $C = \\Delta = \\{z \\in \\mathbb{R}^n : z_i \\ge 0,\\ \\sum_{i=1}^n z_i = 1\\}$ 中。投影 $P_\\Delta(x)$ 在投影梯度法中用于在梯度下降步骤之后强制满足可行性。从凸优化的原理和带约束二次最小化的最优性条件出发，推导一个运行时间为 $O(n\\log n)$ 的算法，用于计算任意 $x \\in \\mathbb{R}^n$ 的 $P_\\Delta(x)$。该推导必须从 Karush–Kuhn–Tucker (KKT) 最优性条件开始，并通过证明计算出的点满足最优性的充要条件来论证算法的正确性。\n\n您的程序必须实现最终的 $O(n\\log n)$ 算法，并将其应用于以下 $x \\in \\mathbb{R}^n$ 的输入测试套件：\n- 测试用例 1：$x = [0.2, -0.1, 0.4, 2.0, 0.3]$。\n- 测试用例 2：$x = [0.1, 0.2, 0.3, 0.4, 0.0]$。\n- 测试用例 3：$x = [-1.0, -2.0, -3.0]$。\n- 测试用例 4：$x = [5.0]$。\n- 测试用例 5：$x = [0.5, 0.5, 0.5, 0.5]$。\n- 测试用例 6：$x = [0.0, -0.5, 0.0, 0.7, 0.8]$。\n\n对于每个测试用例，使用您的 $O(n\\log n)$ 算法计算投影向量 $P_\\Delta(x)$。每个结果必须是一个浮点数列表。您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表（例如，$[result1,result2,result3]$，其中每个 $resultk$ 是第 $k$ 个测试用例的投影向量）。此问题不涉及物理单位或角度单位。输出是投影向量本身，可量化为浮点数列表。该测试套件涵盖了一般情况、已可行点、所有条目均为负数的情况、标量边界情况、相同正条目的情况以及包含零和负数的混合情况，从而测试正确性、可行性和边界条件。",
            "solution": "该问题要求推导并实现一个高效算法，用于计算向量 $x \\in \\mathbb{R}^n$ 在概率单纯形 $\\Delta$ 上的欧几里得投影。概率单纯形是集合 $\\Delta = \\{z \\in \\mathbb{R}^n : \\sum_{i=1}^n z_i = 1, z_i \\ge 0 \\text{ for all } i\\}$。投影 $P_\\Delta(x)$ 是以下带约束的二次优化问题的唯一解 $z^*$：\n$$\n\\text{minimize} \\quad f(z) = \\frac{1}{2}\\|z - x\\|_2^2 = \\frac{1}{2}\\sum_{i=1}^n (z_i - x_i)^2\n$$\n$$\n\\text{subject to} \\quad \\sum_{i=1}^n z_i = 1 \\quad \\text{and} \\quad z_i \\ge 0 \\quad \\text{for } i=1, \\dots, n.\n$$\n这是一个凸优化问题，因为目标函数是严格凸的，并且可行集 $\\Delta$ 是凸的。因此，Karush-Kuhn-Tucker (KKT) 条件是确定最优性的充要条件。我们首先构建该问题的拉格朗日函数。\n\n拉格朗日函数 $L(z, \\lambda, \\nu)$ 由下式给出：\n$$\nL(z, \\lambda, \\nu) = \\frac{1}{2}\\sum_{i=1}^n (z_i - x_i)^2 - \\sum_{i=1}^n \\lambda_i z_i - \\nu \\left( \\sum_{i=1}^n z_i - 1 \\right)\n$$\n其中 $\\lambda_i \\ge 0$ 是非负性约束 $z_i \\ge 0$ 的拉格朗日乘子，$\\nu$ 是等式约束 $\\sum_{i=1}^n z_i = 1$ 的拉格朗日乘子。最优点 $z^*$ 的 KKT 条件如下：\n\n1.  **平稳性**：拉格朗日函数关于 $z$ 的梯度在 $z^*$ 处必须为零：\n    $$\n    \\frac{\\partial L}{\\partial z_i}\\bigg|_{z=z^*} = (z_i^* - x_i) - \\lambda_i - \\nu = 0 \\quad \\implies \\quad z_i^* = x_i + \\lambda_i + \\nu\n    $$\n\n2.  **原始可行性**：解 $z^*$ 必须位于可行集 $\\Delta$ 中：\n    $$\n    \\sum_{i=1}^n z_i^* = 1 \\quad \\text{and} \\quad z_i^* \\ge 0 \\quad \\text{for all } i\n    $$\n\n3.  **对偶可行性**：不等式约束的乘子必须为非负：\n    $$\n    \\lambda_i \\ge 0 \\quad \\text{for all } i\n    $$\n\n4.  **互补松弛性**：\n    $$\n    \\lambda_i z_i^* = 0 \\quad \\text{for all } i\n    $$\n\n根据互补松弛性条件，对于每个分量 $i$，要么 $\\lambda_i = 0$，要么 $z_i^* = 0$。\n-   如果 $z_i^* > 0$，则互补松弛性意味着 $\\lambda_i = 0$。平稳性条件简化为 $z_i^* = x_i + \\nu$。由于 $z_i^* > 0$，我们有 $x_i+\\nu > 0$。\n-   如果 $z_i^* = 0$，则 $\\lambda_i \\ge 0$。平稳性条件给出 $0 = x_i + \\lambda_i + \\nu$，这意味着 $\\lambda_i = -x_i - \\nu$。对偶可行性条件 $\\lambda_i \\ge 0$ 意味着 $-x_i - \\nu \\ge 0$，即 $x_i + \\nu \\le 0$。\n\n让我们引入一个单一的阈值参数 $\\theta = -\\nu$。$z_i^*$ 的条件可以统一为：\n-   如果 $x_i - \\theta > 0$，则 $z_i^* > 0$ 且 $\\lambda_i=0$，因此 $z_i^* = x_i - \\theta$。\n-   如果 $x_i - \\theta \\le 0$，则 $z_i^* = 0$。\n\n对于所有的 $i$，这可以简洁地表示为：\n$$\nz_i^* = \\max(0, x_i - \\theta) = (x_i - \\theta)_+\n$$\n问题现在简化为找到标量阈值 $\\theta$ 的正确值。我们可以通过强制执行原始可行性约束 $\\sum_{i=1}^n z_i^* = 1$ 来确定 $\\theta$：\n$$\n\\sum_{i=1}^n \\max(0, x_i - \\theta) = 1\n$$\n令 $g(\\theta) = \\sum_{i=1}^n \\max(0, x_i - \\theta)$。这个函数是连续的、分段线性的，并且是单调非增的。我们需要找到方程 $g(\\theta) = 1$ 的根 $\\theta$。直接求解析解很困难，但我们可以根据 $g(\\theta)$ 的性质设计一个高效的算法。\n\n考虑将 $x$ 的分量按降序排序：$u_1 \\ge u_2 \\ge \\dots \\ge u_n$。随着 $\\theta$ 的减小，项 $\\max(0, u_i - \\theta)$ 将按此顺序变为非零。设 $\\rho$ 是最终解 $z^*$ 中正分量的数量。这些将对应于 $x$ 的 $\\rho$ 个最大分量。因此，对于对应于 $u_1, \\dots, u_\\rho$ 的索引 $i$，我们有 $z_i^* > 0$，而对于对应于 $u_{\\rho+1}, \\dots, u_n$ 的索引，我们有 $z_i^* = 0$。这意味着阈值 $\\theta$ 必须满足 $u_\\rho - \\theta > 0$ 和 $u_{\\rho+1} - \\theta \\le 0$，即 $u_{\\rho+1} \\le \\theta  u_\\rho$。\n\n对于这个假定的 $\\rho$，和约束变为：\n$$\n\\sum_{j=1}^\\rho (u_j - \\theta) + \\sum_{j=\\rho+1}^n 0 = 1\n$$\n$$\n\\left(\\sum_{j=1}^\\rho u_j\\right) - \\rho\\theta = 1\n$$\n对 $\\theta$ 求解，可以为每个可能的 $\\rho$ 得出一个候选阈值：\n$$\n\\theta_\\rho = \\frac{1}{\\rho}\\left(\\sum_{j=1}^\\rho u_j - 1\\right)\n$$\n正确的 $\\rho$ 是使这个 $\\theta_\\rho$ 与假设 $u_{\\rho+1} \\le \\theta_\\rho  u_\\rho$ 一致的那个。我们只需要找到满足条件 $u_\\rho > \\theta_\\rho$ 的最大索引 $\\rho \\in \\{1, \\dots, n\\}$。设这个索引为 $\\rho^*$。\n条件 $u_\\rho > \\theta_\\rho$ 等价于 $\\rho u_\\rho > \\sum_{j=1}^\\rho u_j - 1$。\n我们来证明，如果 $\\rho^*$ 是满足此条件的最大索引，那么第二个条件 $u_{\\rho^*+1} \\le \\theta_{\\rho^*}$ 也成立。使用反证法，假设 $u_{\\rho^*+1} > \\theta_{\\rho^*}$。这意味着 $(\\rho^*+1) u_{\\rho^*+1} > \\rho^* u_{\\rho^*+1} > \\rho^* \\theta_{\\rho^*}$。同时，$(\\sum_{j=1}^{\\rho^*} u_j - 1) + u_{\\rho^*+1} = \\rho^*\\theta_{\\rho^*} + u_{\\rho^*+1}$。\n考虑 $\\rho^*+1$ 的条件：\n$$(\\rho^*+1)u_{\\rho^*+1} > \\sum_{j=1}^{\\rho^*+1} u_j - 1 = \\left(\\sum_{j=1}^{\\rho^*} u_j - 1\\right) + u_{\\rho^*+1} = \\rho^*\\theta_{\\rho^*} + u_{\\rho^*+1}$$\n这简化为 $\\rho^* u_{\\rho^*+1} > \\rho^* \\theta_{\\rho^*}$，即 $u_{\\rho^*+1} > \\theta_{\\rho^*}$，这正是我们的假设。这意味着如果 $u_{\\rho^*+1} > \\theta_{\\rho^*}$，那么 $\\rho^*+1$ 也会满足测试条件，这与 $\\rho^*$ 是满足条件的最大索引相矛盾。因此，我们必然有 $u_{\\rho^*+1} \\le \\theta_{\\rho^*}$。\n\n这导出了以下 $O(n\\log n)$ 算法：\n1.  将输入向量 $x$ 按降序排序得到向量 $u$。此步骤耗时 $O(n\\log n)$。\n2.  找到满足 $u_j > \\frac{1}{j}(\\sum_{i=1}^j u_i - 1)$ 的最大索引 $j \\in \\{1, \\dots, n\\}$，该值即为 $\\rho$。这可以通过从 $j=1$ 迭代到 $n$ 并维护 $u_i$ 的累加和在 $O(n)$ 时间内完成。\n3.  用这个 $\\rho$ 计算最终阈值 $\\theta = \\frac{1}{\\rho}(\\sum_{i=1}^\\rho u_i - 1)$。这需要用到上一步计算出的和，耗时 $O(1)$。\n4.  使用原始向量 $x$ 和阈值 $\\theta$ 计算投影 $z^*$：$z_i^* = \\max(0, x_i - \\theta)$，其中 $i=1, \\dots, n$。此步骤耗时 $O(n)$。\n\n主要步骤是初始排序，因此总时间复杂度为 $O(n\\log n)$。通过这种构造，可以保证最终得到的向量 $z^*$ 满足 KKT 条件，因此是唯一的最优解。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It computes the projection of several vectors onto the probability simplex\n    and prints the results in the specified format.\n    \"\"\"\n\n    def project_to_simplex(x: np.ndarray) - np.ndarray:\n        \"\"\"\n        Computes the Euclidean projection of a vector x onto the probability simplex.\n\n        The algorithm takes O(n log n) time due to the sorting step. The\n        derivation is based on the Karush-Kuhn-Tucker (KKT) conditions for\n        the constrained quadratic optimization problem.\n\n        Args:\n            x: A numpy array representing the vector to be projected.\n\n        Returns:\n            A numpy array representing the projected vector.\n        \"\"\"\n        n = x.shape[0]\n        \n        # If the vector is already in the simplex, return it.\n        # This is an optional optimization, the main algorithm handles this case correctly.\n        if np.sum(x) == 1 and np.all(x >= 0):\n            return x\n\n        # Sort the vector x in descending order.\n        u = np.sort(x)[::-1]\n\n        # Compute the cumulative sum of the sorted vector.\n        cssv = np.cumsum(u)\n        \n        # Find the largest rho such that u_rho > (1/rho) * (sum_{i=1}^{rho} u_i - 1).\n        # This is done in a vectorized way for efficiency.\n        # The equation is rearranged to avoid division inside the loop:\n        # rho * u_rho > sum_{i=1}^{rho} u_i - 1\n        indices = np.arange(1, n + 1)\n        condition = u * indices > cssv - 1\n        \n        # The last index where the condition is true gives us the correct rho.\n        # np.where returns a tuple of arrays, we need the first one.\n        # The set of indices satisfying the condition is never empty for n>=1.\n        rho_idx = np.where(condition)[0][-1]\n        rho = rho_idx + 1\n\n        # Compute the threshold theta using the found rho.\n        theta = (cssv[rho_idx] - 1) / rho\n\n        # Compute the projection by applying the threshold.\n        # z_i = max(x_i - theta, 0)\n        z = np.maximum(x - theta, 0)\n\n        return z\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        np.array([0.2, -0.1, 0.4, 2.0, 0.3]),\n        np.array([0.1, 0.2, 0.3, 0.4, 0.0]),\n        np.array([-1.0, -2.0, -3.0]),\n        np.array([5.0]),\n        np.array([0.5, 0.5, 0.5, 0.5]),\n        np.array([0.0, -0.5, 0.0, 0.7, 0.8]),\n    ]\n\n    results = []\n    for case in test_cases:\n        projected_vector = project_to_simplex(case)\n        # Convert the result to a list of floats for correct string formatting.\n        results.append(str(projected_vector.tolist()))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "在实践中应用投影梯度法，不仅需要投影算子，还需要一个能保证收敛的稳健步长 $\\alpha$ 选择策略。本练习 () 将带您从理论走向完整的实践应用，您将学习如何估计关键的 Lipschitz 常数，并实现一个求解带盒约束的正则化问题的收敛 PGD 算法，这是数据同化中的一个常见场景。",
            "id": "3414873",
            "problem": "考虑在线性逆问题和数据同化中出现的凸二次规划问题：在箱式约束下最小化吉洪诺夫正则化的最小二乘目标函数，\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; J(x) = \\frac{1}{2}\\,\\|A x - b\\|_2^2 + \\frac{\\gamma}{2}\\,\\|x\\|_2^2 \\quad \\text{subject to} \\quad \\ell \\le x \\le u,\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$b \\in \\mathbb{R}^m$，$\\gamma \\ge 0$，且不等式是分量形式的，其中 $\\ell, u \\in \\mathbb{R}^n$ 且对所有 $i$ 都有 $\\ell_i \\le u_i$。$J$ 的梯度是利普希茨连续的，其常数为 $L = \\|Q\\|_2$，其中 $Q = A^\\top A + \\gamma I_n$ 且 $\\|\\cdot\\|_2$ 表示谱范数。投影梯度法迭代如下\n$$\nx^{k+1} = \\Pi_{[\\ell,u]}\\Big(x^k - \\alpha \\,\\nabla J(x^k)\\Big),\n$$\n其中 $\\Pi_{[\\ell,u]}$ 是到箱体 $[\\ell,u]$ 上的欧几里得投影，$\\alpha  0$ 是步长。对于凸目标函数的投影梯度法，标准的全局收敛性保证要求选择 $\\alpha \\in (0, 2/L)$，而保证单调下降的选择是 $\\alpha \\in (0, 1/L]$。\n\n任务：\n- 从谱范数和次乘法矩阵范数的定义出发，推导一个 $L = \\|Q\\|_2$ 的可计算上界 $\\overline{L}$，该上界不需要计算 $Q$ 的特征值。您的推导必须只使用范数和线性算子的基本性质，并且不得假定超出这些性质的公式。您可以假设能够调用一个计算与 $Q$ 的矩阵向量乘积的程序，但不一定能直接访问 $Q$ 本身的元素。\n- 对 $Q$ 使用幂迭代法，推导一个 $L$ 的可计算数值下界 $\\underline{L}$，该下界随着迭代次数的增加而收敛到 $L$。您的推导必须从对称半正定矩阵谱范数的变分特征和瑞利商的定义开始，并且不得先验地假设任何特定的收敛速度。\n- 基于以上内容，解释如何使用一个可证明的上界 $\\overline{L}$ 来为投影梯度法选择一个严格安全的步长 $\\alpha$，以及如何在算法设计中使用来自幂迭代的 $\\underline{L}$ 来评估 $\\overline{L}$ 的保守性。\n\n然后，实现一个程序，该程序：\n- 对每个测试用例，仅使用允许的操作（矩阵向量乘积和基本矩阵范数）计算一个严格上界 $\\overline{L}$ 和一个数值下界 $\\underline{L}$。\n- 以步长 $\\alpha = 1/\\overline{L}$ 运行投影梯度法直到收敛，使用停止准则 $\\|x^{k+1} - x^k\\|_2 \\le \\varepsilon$ 或最大迭代次数 $K_{\\max}$，以先达到的为准。\n- 对每个测试用例，报告三个浮点数：下界 $\\underline{L}$、上界 $\\overline{L}$ 和返回点 $x^{\\star}$ 处的最终目标函数值 $J(x^{\\star})$。\n\n您必须使用的基本原理：\n- 对称半正定矩阵 $Q$ 的变分特征：$L = \\|Q\\|_2 = \\max_{\\|v\\|_2=1} v^\\top Q v$。\n- $v \\ne 0$ 时的瑞利商 $R_Q(v) = \\frac{v^\\top Q v}{v^\\top v}$。\n- 相容矩阵范数的次乘法性和范数优势，包括 $\\|M\\|_2 \\le \\sqrt{\\|M\\|_1 \\|M\\|_\\infty}$ 以及对任意相容矩阵范数 $\\|\\cdot\\|$ 都有 $\\|X+Y\\| \\le \\|X\\| + \\|Y\\|$。\n- 欧几里得投影到闭凸集上的非扩张性以及投影梯度下降法在凸目标函数上的基本性质。\n\n测试套件：\n- 案例1：\n  - $A = \\begin{bmatrix} 3  1  0 \\\\ 1  4  1 \\\\ 0  1  2 \\end{bmatrix}$，$b = \\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\end{bmatrix}$，$\\gamma = 0.2$。\n  - $\\ell = \\begin{bmatrix} -0.5 \\\\ -0.5 \\\\ -0.5 \\end{bmatrix}$，$u = \\begin{bmatrix} 2 \\\\ 2 \\\\ 2 \\end{bmatrix}$，$x^0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n- 案例2：\n  - $A = \\begin{bmatrix} 1  2  3 \\\\ 2  4  6 \\end{bmatrix}$，$b = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$，$\\gamma = 0$。\n  - $\\ell = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$，$u = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$，$x^0 = \\begin{bmatrix} 0.3 \\\\ 0.3 \\\\ 0.3 \\end{bmatrix}$。\n- 案例3：\n  - $A = \\begin{bmatrix} 2  0  -1 \\\\ 0  1  1 \\\\ 1  -1  0 \\\\ 0  2  3 \\end{bmatrix}$，$b = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 2 \\end{bmatrix}$，$\\gamma = 3.0$。\n  - $\\ell = \\begin{bmatrix} -0.1 \\\\ -0.1 \\\\ -0.1 \\end{bmatrix}$，$u = \\begin{bmatrix} 0.1 \\\\ 0.1 \\\\ 0.1 \\end{bmatrix}$，$x^0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n\n所有案例中使用的算法参数：\n- 幂迭代最大迭代次数 $K_{\\mathrm{pow}} = 200$ 和容差 $\\varepsilon_{\\mathrm{pow}} = 10^{-10}$。\n- 投影梯度最大迭代次数 $K_{\\max} = 20000$ 和容差 $\\varepsilon = 10^{-9}$。\n- 对于上界，仅使用可证明有效的不等式。例如，您可以使用 $\\|Q\\|_2 \\le \\sqrt{\\|Q\\|_1 \\|Q\\|_\\infty}$，也可利用 $Q = A^\\top A + \\gamma I_n$ 的结构推断出 $\\|Q\\|_2 \\le \\|A\\|_2^2 + \\gamma \\le \\|A\\|_1 \\|A\\|_\\infty + \\gamma$。您必须选择一个严格的 $\\overline{L}$，并可以取多个严格上界的最小值以减少保守性。\n\n编程和输出要求：\n- 您的程序必须为每个测试用例计算通过对 $Q$ 进行幂迭代得到的最终瑞利商作为数值下界 $\\underline{L}$、按所述选择的严格上界 $\\overline{L}$，以及使用步长 $\\alpha = 1/\\overline{L}$ 运行投影梯度法后的最终目标值 $J(x^\\star)$。\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按顺序包含案例1、案例2和案例3的三元组 $\\left(\\underline{L}, \\overline{L}, J(x^\\star)\\right)$，并展开成一个列表。每个浮点数必须四舍五入到6位小数。例如，输出格式必须严格为 \n$[\\underline{L}_1,\\overline{L}_1,J_1,\\underline{L}_2,\\overline{L}_2,J_2,\\underline{L}_3,\\overline{L}_3,J_3]$。",
            "solution": "该问题是适定的，并具有科学依据。它提出了一个出现在逆问题和数据同化中的标准凸优化问题，即在箱式约束下最小化吉洪诺夫正则化的最小二乘泛函。所有数据、参数和目标都已明确定义，构成了数值优化和科学计算中的一个有效问题。我们开始进行求解。\n\n待最小化的目标函数是\n$$\nJ(x) = \\frac{1}{2}\\,\\|A x - b\\|_2^2 + \\frac{\\gamma}{2}\\,\\|x\\|_2^2\n$$\n约束条件为分量形式的不等式 $\\ell \\le x \\le u$。$J(x)$ 的梯度是\n$$\n\\nabla J(x) = A^\\top(Ax - b) + \\gamma x = (A^\\top A + \\gamma I_n)x - A^\\top b.\n$$\n设海森矩阵为 $Q = A^\\top A + \\gamma I_n$。梯度可以写成 $\\nabla J(x) = Qx - c$，其中 $c = A^\\top b$。由于 $Q$ 是常数， $J(x)$ 的海森矩阵是 $\\nabla^2 J(x) = Q$。矩阵 $A^\\top A$ 总是半正定的。由于 $\\gamma \\ge 0$，矩阵 $Q$ 是对称半正定的。如果 $\\gamma > 0$ 或 $A$ 是满列秩的，则 $Q$ 是正定的，且 $J(x)$ 是严格凸的。\n\n梯度的利普希茨常数 $L$ 是海森矩阵的谱范数（即最大特征值，因为 $Q$ 是对称半正定的）：$L = \\|Q\\|_2 = \\lambda_{\\max}(Q)$。\n\n**可计算上界 $\\overline{L}$ 的推导**\n\n目标是找到 $L = \\|Q\\|_2$ 的一个上界 $\\overline{L}$，该上界不需要进行特征值分解。我们使用矩阵范数的基本性质。\n\n对于任意矩阵 $M \\in \\mathbb{R}^{n \\times n}$，一个已知的将谱范数与 $1$-范数和 $\\infty$-范数联系起来的不等式是 $\\|M\\|_2 \\le \\sqrt{\\|M\\|_1 \\|M\\|_\\infty}$。$1$-范数是最大绝对列和，$\\infty$-范数是最大绝对行和。\n$$\n\\|M\\|_1 = \\max_{1 \\le j \\le n} \\sum_{i=1}^n |M_{ij}|, \\quad \\|M\\|_\\infty = \\max_{1 \\le i \\le n} \\sum_{j=1}^n |M_{ij}|.\n$$\n我们的海森矩阵是 $Q = A^\\top A + \\gamma I_n$。由于 $A^\\top A$ 是对称的，$\\gamma I_n$ 也是对称的，因此 $Q$ 是对称的。对于任何对称矩阵 $M$，我们有 $\\|M\\|_1 = \\|M\\|_\\infty$。\n将此应用于 $Q$，不等式变为 $\\|Q\\|_2 \\le \\sqrt{\\|Q\\|_1 \\|Q\\|_1} = \\|Q\\|_1$。\n因此，$L$ 的一个有效且可计算的上界是 $\\overline{L} = \\|Q\\|_1$。\n$$\nL = \\|Q\\|_2 \\le \\|Q\\|_1 = \\max_{j} \\sum_{i=1}^n |Q_{ij}|.\n$$\n由于问题给出了 $A$ 的显式矩阵，我们可以构造 $Q = A^\\top A + \\gamma I_n$ 并计算其 $1$-范数。\n\n可以使用范数的三角不等式和次乘法性质推导出另一个界：\n$$\nL = \\|Q\\|_2 = \\|A^\\top A + \\gamma I_n\\|_2 \\le \\|A^\\top A\\|_2 + \\|\\gamma I_n\\|_2.\n$$\n我们有当 $\\gamma \\ge 0$ 时 $\\|\\gamma I_n\\|_2 = \\gamma$。此外，$\\|A^\\top A\\|_2 \\le \\|A^\\top\\|_2 \\|A\\|_2$。由于 $\\|A^\\top\\|_2 = \\|A\\|_2$，这简化为 $\\|A^\\top A\\|_2 \\le \\|A\\|_2^2$。这给出了 $L \\le \\|A\\|_2^2 + \\gamma$。虽然这是正确的，但直接计算 $\\|A\\|_2$ 本身就是一个特征值问题。我们可以进一步界定 $\\|A\\|_2 \\le \\sqrt{\\|A\\|_1 \\|A\\|_\\infty}$。这产生了界 $L \\le \\|A\\|_1 \\|A\\|_\\infty + \\gamma$。通过 $1$-范数的次可加性和次乘法性可以证明 $\\|Q\\|_1 = \\|A^\\top A + \\gamma I_n\\|_1 \\le \\|A^\\top A\\|_1 + \\|\\gamma I_n\\|_1 \\le \\|A^\\top\\|_1 \\|A\\|_1 + \\gamma = \\|A\\|_\\infty \\|A\\|_1 + \\gamma$。这表明 $\\|Q\\|_1$ 这个界通常比 $\\|A\\|_1\\|A\\|_\\infty + \\gamma$ 更紧或与之相等。因此，我们将选择 $\\overline{L} = \\|Q\\|_1$ 作为我们的严格且可计算的上界。\n\n**可计算下界 $\\underline{L}$ 的推导**\n\n利普希茨常数 $L$ 是对称半正定矩阵 $Q$ 的最大特征值。最大特征值的变分特征由下式给出\n$$\nL = \\lambda_{\\max}(Q) = \\max_{v \\in \\mathbb{R}^n, v \\ne 0} \\frac{v^\\top Q v}{v^\\top v}.\n$$\n表达式 $R_Q(v) = \\frac{v^\\top Q v}{v^\\top v}$ 是瑞利商。这个特征立即意味着对于任何非零向量 $v \\in \\mathbb{R}^n$，瑞利商 $R_Q(v)$ 都是 $L$ 的一个下界：$R_Q(v) \\le L$。\n\n为了找到一个紧密的下界，我们需要找到一个向量 $v$，它是对应于 $\\lambda_{\\max}(Q)$ 的特征向量的一个良好近似。幂迭代法就是为此设计的。该算法生成一个收敛到主特征向量的向量序列。\n\n过程如下：\n1. 从一个初始向量 $v^0$ 开始，满足 $\\|v^0\\|_2 = 1$（例如，一个随机生成的向量）。\n2. 对于 $k = 0, 1, 2, \\dots$：\n    $$\n    w^{k+1} = Q v^k\n    $$\n    $$\n    v^{k+1} = \\frac{w^{k+1}}{\\|w^{k+1}\\|_2}\n    $$\n3. 向量序列 $\\{v^k\\}$ 收敛到与最大模特征值相关联的特征向量。由于 $Q$ 是半正定的，这个特征值就是 $\\lambda_{\\max}(Q) = L$。\n4. 在每次迭代 $k$ 中，瑞利商 $R_Q(v^k)$ 提供了 $L$ 的一个下界。当 $k \\to \\infty$ 时，$v^k$ 与主特征向量对齐，$R_Q(v^k)$ 收敛到 $L$。可以证明序列 $\\{R_Q(v^k)\\}$ 是非递减的。\n\n因此，通过运行足够步数的幂迭代，最终的瑞利商 $\\underline{L} = R_Q(v^{\\text{final}})$ 提供了 $L$ 的一个高质量数值下界。\n\n**步长选择与保守性评估**\n\n投影梯度法的迭代是 $x^{k+1} = \\Pi_{[\\ell,u]}(x^k - \\alpha \\nabla J(x^k))$。对于具有 $L$-利普希茨连续梯度的凸目标函数 $J$，如果步长 $\\alpha$ 选在区间 $\\alpha \\in (0, 2/L)$ 内，则保证收敛到最小值点。选择 $\\alpha \\in (0, 1/L]$ 还能额外保证目标函数值非增，即 $J(x^{k+1}) \\le J(x^k)$，这是稳定收敛的一个理想属性。\n\n$L$ 的确切值通常是未知的。然而，我们推导出了一个可证明的上界 $\\overline{L}$，使得 $L \\le \\overline{L}$。通过选择步长 $\\alpha = 1/\\overline{L}$，我们确保了 $0  \\alpha \\le 1/L$。因此，这个选择是“严格安全的”，因为它保证落在确保单调收敛的区间内。\n\n梯度法的收敛速度受步长影响。过小的步长会导致收敛非常缓慢。理想的步长接近稳定范围的上限，因此基于 $L$ 的选择（例如 $1/L$）优于基于一个宽松上界 $\\overline{L} \\gg L$ 的选择。\n\n从幂迭代获得的下界 $\\underline{L}$ 提供了对 $L$ 真实值的极佳近似，即 $\\underline{L} \\approx L$。我们可以通过计算比率 $\\overline{L} / \\underline{L}$ 来评估我们的上界 $\\overline{L}$ 的质量，或称“保守性”。\n- 如果 $\\overline{L} / \\underline{L} \\approx 1$，则上界是紧密的，步长 $\\alpha = 1/\\overline{L}$ 接近最优。\n- 如果 $\\overline{L} / \\underline{L} \\gg 1$，则上界是宽松的，步长过于保守，这可能会显著减慢投影梯度算法的收敛速度。这种评估可以为决策提供信息，以决定是寻求一个更紧的 $\\overline{L}$ 界，还是采用像线搜索这样的自适应步长策略。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the constrained quadratic optimization problem using the projected gradient method\n    for three distinct test cases. For each case, it computes lower and upper bounds for the\n    Lipschitz constant of the gradient and the final objective value.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"A\": np.array([[3, 1, 0], [1, 4, 1], [0, 1, 2]], dtype=np.float64),\n            \"b\": np.array([1, 0, -1], dtype=np.float64),\n            \"gamma\": 0.2,\n            \"l\": np.array([-0.5, -0.5, -0.5], dtype=np.float64),\n            \"u\": np.array([2, 2, 2], dtype=np.float64),\n            \"x0\": np.array([0, 0, 0], dtype=np.float64),\n        },\n        {\n            \"A\": np.array([[1, 2, 3], [2, 4, 6]], dtype=np.float64),\n            \"b\": np.array([1, 2], dtype=np.float64),\n            \"gamma\": 0.0,\n            \"l\": np.array([0, 0, 0], dtype=np.float64),\n            \"u\": np.array([1, 1, 1], dtype=np.float64),\n            \"x0\": np.array([0.3, 0.3, 0.3], dtype=np.float64),\n        },\n        {\n            \"A\": np.array([[2, 0, -1], [0, 1, 1], [1, -1, 0], [0, 2, 3]], dtype=np.float64),\n            \"b\": np.array([1, -1, 0, 2], dtype=np.float64),\n            \"gamma\": 3.0,\n            \"l\": np.array([-0.1, -0.1, -0.1], dtype=np.float64),\n            \"u\": np.array([0.1, 0.1, 0.1], dtype=np.float64),\n            \"x0\": np.array([0, 0, 0], dtype=np.float64),\n        },\n    ]\n\n    # Algorithmic Parameters\n    K_pow = 200\n    eps_pow = 1e-10\n    K_max = 20000\n    eps_pgd = 1e-9\n\n    results = []\n\n    for case in test_cases:\n        A = case[\"A\"]\n        b = case[\"b\"]\n        gamma = case[\"gamma\"]\n        l_bound = case[\"l\"]\n        u_bound = case[\"u\"]\n        x = case[\"x0\"].copy()\n        \n        m, n = A.shape\n        \n        # Form the Hessian Q = A^T A + gamma * I\n        Q = A.T @ A + gamma * np.eye(n)\n        \n        # --- 1. Compute rigorous upper bound L_upper ---\n        # L_upper = ||Q||_1, which is the maximum absolute column sum.\n        # For a symmetric matrix Q, ||Q||_1 = ||Q||_inf.\n        L_upper = np.max(np.sum(np.abs(Q), axis=0))\n\n        # --- 2. Compute numerical lower bound L_lower using Power Iteration ---\n        # Start with a random vector\n        np.random.seed(0) # for reproducibility\n        v = np.random.rand(n)\n        v = v / np.linalg.norm(v)\n        \n        for _ in range(K_pow):\n            w = Q @ v\n            v_new = w / np.linalg.norm(w)\n            # Check for convergence\n            if np.linalg.norm(v_new - v)  eps_pow:\n                v = v_new\n                break\n            v = v_new\n        \n        # Final Rayleigh quotient is the lower bound\n        L_lower = v.T @ (Q @ v)\n\n        # --- 3. Run Projected Gradient Method ---\n        alpha = 1.0 / L_upper\n        c = A.T @ b\n\n        for _ in range(K_max):\n            grad_J = Q @ x - c\n            x_unconstrained = x - alpha * grad_J\n            x_next = np.clip(x_unconstrained, l_bound, u_bound)\n            \n            # Check stopping criterion\n            if np.linalg.norm(x_next - x)  eps_pgd:\n                x = x_next\n                break\n            x = x_next\n\n        x_star = x\n\n        # --- 4. Compute final objective value J(x*) ---\n        residual_norm_sq = np.linalg.norm(A @ x_star - b)**2\n        regularizer_norm_sq = np.linalg.norm(x_star)**2\n        J_final = 0.5 * residual_norm_sq + (gamma / 2.0) * regularizer_norm_sq\n        \n        results.extend([L_lower, L_upper, J_final])\n\n    # Format the final output\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "投影梯度法是处理约束的几种方法之一，将其与其他基本方法（如内点法或对数障碍法）进行比较，可以揭示算法设计中的重要权衡。本练习 () 通过将 PGD 与其他优化策略进行对比，拓宽您的视野。通过实现 PGD 和对数障碍法来强制实现正定性并进行比较，您将批判性地评估它们在可行集边界附近的独特行为，并理解它们各自的优缺点。",
            "id": "3414810",
            "problem": "考虑一个具有有界正状态的线性逆问题。设 $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ 是一个已知的满列秩线性观测算子，$\\mathbf{b} \\in \\mathbb{R}^{m}$ 是给定的测量值。目标是通过最小化受严格正性约束的最小二乘失配，从数据中估计状态向量 $\\mathbf{x} \\in \\mathbb{R}^{n}$，即最小化目标函数 $J(\\mathbf{x}) = \\tfrac{1}{2}\\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{b} \\rVert_2^2$，约束条件为 $\\mathbf{x} \\in \\mathbb{R}_{++}^{n}$，其中 $\\mathbb{R}_{++}^{n}$ 表示所有分量均为严格正数的向量集合。在数据同化中，这个严格正性约束模拟了先验知识，即由 $\\mathbf{x}$ 表示的物理量（如浓度或密度）必须是正的。\n\n我们将研究和比较两种算法策略：\n\n1. 投影梯度下降法 (PGD)：迭代形式为 $\\mathbf{x}^{k+1} = \\mathcal{P}_{\\mathbb{R}_{+}^{n}}\\!\\left(\\mathbf{x}^{k} - \\alpha \\nabla J(\\mathbf{x}^{k})\\right)$，其中 $\\nabla J(\\mathbf{x})$ 是 $J$ 的梯度，$\\mathcal{P}_{\\mathbb{R}_{+}^{n}}$ 表示到非负象限 $\\mathbb{R}_{+}^{n} = \\{\\mathbf{x} \\in \\mathbb{R}^{n} : x_i \\ge 0, \\ \\forall i\\}$ 的正交投影。该方法通过投影显式地施加非负性。已知对于步长参数 $\\alpha$ 的选择，若 $\\alpha \\in (0, 1/L)$（其中 $L$ 是梯度 $\\nabla J$ 的任意 Lipschitz 常数），则对于凸函数 $J$，迭代序列在目标函数上呈现下降趋势。\n\n2. 用于严格正性的对数障碍代理函数：定义障碍增广目标函数 $J_{\\mu}(\\mathbf{x}) = \\tfrac{1}{2}\\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{b} \\rVert_2^2 - \\mu \\sum_{i=1}^{n} \\log(x_i)$，其中 $\\mu > 0$ 是一个障碍参数。该代理函数通过惩罚趋近边界 $x_i \\downarrow 0$ 的行为，隐式地强制实现严格正性 $\\mathbf{x} \\in \\mathbb{R}_{++}^{n}$。梯度下降迭代的形式为 $\\mathbf{x}^{k+1} = \\mathbf{x}^{k} - \\alpha \\nabla J_{\\mu}(\\mathbf{x}^{k})$，其中选择步长 $\\alpha > 0$ 以保持正性和下降性。在边界 $x_i \\downarrow 0$ 附近，由于 $-\\log(x_i)$ 导数的发散性，障碍项主导了算法的行为。\n\n仅从以下基本事实出发：最小二乘目标函数 $J(\\mathbf{x}) = \\tfrac{1}{2}\\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{b} \\rVert_2^2$ 的梯度为 $\\nabla J(\\mathbf{x}) = \\mathbf{A}^{\\top}(\\mathbf{A}\\mathbf{x} - \\mathbf{b})$，$\\nabla J$ 的 Lipschitz 常数受谱范数的平方 $L = \\lVert \\mathbf{A} \\rVert_2^2$ 限制，到非负象限的正交投影 $\\mathcal{P}_{\\mathbb{R}_{+}^{n}}$ 是逐分量作用的，以及 $\\log(x)$ 的导数是 $1/x$，完成以下任务：\n\n- 推导此问题的 PGD 更新映射的显式形式，确保步长的选择使用了梯度的有效 Lipschitz 界，并且投影操作是逐分量强制非负性的。\n\n- 推导障碍增广目标函数 $J_{\\mu}(\\mathbf{x})$ 的梯度显式形式，并设计一个与梯度下降迭代兼容的、能保持正性并强制下降的步长选择规则。该规则必须保证，如果 $\\mathbf{x}^{k} \\in \\mathbb{R}_{++}^{n}$，那么 $\\mathbf{x}^{k+1} \\in \\mathbb{R}_{++}^{n}$，同时确保 $J_{\\mu}$ 的目标函数值下降。完成此任务时，除了上述基本事实外，不得使用任何快捷公式。\n\n- 实现这两种算法，使其收敛，并使用基于连续迭代差异的停止规则。对两种方法都使用具有严格正分量的固定初始猜测值。对于 PGD，显式地实现到 $\\mathbb{R}_{+}^{n}$ 的投影。对于障碍法，确保在每次迭代中都强制执行保持正性的步长规则。\n\n- 在以下测试案例集上比较两种方法的近边界行为。此问题中没有物理单位；所有量均为无量纲。\n\n测试集（每个案例由 $(\\mathbf{A}, \\mathbf{b}, \\mu, \\mathbf{x}^{0}, \\text{tolerances})$ 指定）：\n1. 案例 1（一维，边界主导）：$n = 1$，$\\mathbf{A} = [1]$，$\\mathbf{b} = [10^{-6}]$，$\\mu = 10^{-6}$，初始猜测值 $\\mathbf{x}^{0} = [10^{-3}]$，停止容差 $\\epsilon = 10^{-12}$，最大迭代次数 $N_{\\max} = 20000$。\n2. 案例 2（二维，跨坐标病态）：$n = 2$，$\\mathbf{A} = \\mathrm{diag}(1, 10^{-2})$，$\\mathbf{b} = [10^{-3}, 10^{-5}]^{\\top}$，$\\mu = 10^{-5}$，初始猜测值 $\\mathbf{x}^{0} = [10^{-3}, 10^{-3}]^{\\top}$，停止容差 $\\epsilon = 10^{-12}$，最大迭代次数 $N_{\\max} = 20000$。\n3. 案例 3（十维，卷积模糊与真实值中的零）：$n = 10$，$\\mathbf{A} \\in \\mathbb{R}^{10 \\times 10}$ 是与核 $\\mathbf{h} = [0.25, 0.5, 0.25]$ 相关的 Toeplitz 卷积矩阵，边界处进行零填充，即 $(\\mathbf{A}\\mathbf{x})_i = 0.25 x_{i-1} + 0.5 x_i + 0.25 x_{i+1}$，其中 $x_0 = x_{n+1} = 0$，且 $\\mathbf{b} = \\mathbf{A}\\mathbf{x}_{\\mathrm{true}}$，其中 $\\mathbf{x}_{\\mathrm{true}} = [0.0, 0.1, 0.0, 0.2, 0.0, 0.05, 0.0, 0.1, 0.0, 0.0]^{\\top}$。附加噪声可忽略不计，为保证可复现性可取为零。取 $\\mu = 10^{-6}$，初始猜测值 $\\mathbf{x}^{0} = 10^{-3}\\cdot \\mathbf{1}$（所有分量均为 $10^{-3}$ 的向量），停止容差 $\\epsilon = 10^{-12}$，最大迭代次数 $N_{\\max} = 50000$。\n\n对于每个案例，您的程序必须产生三个标量值，以量化近边界行为：\n- 最终 PGD 解的最小分量值，$\\min_i x^{\\star}_{\\mathrm{PGD}, i}$。\n- 最终障碍法解的最小分量值，$\\min_i x^{\\star}_{\\mathrm{bar}, i}$。\n- PGD 解与障碍法解的最终最小二乘失配之差，$J(\\mathbf{x}^{\\star}_{\\mathrm{PGD}}) - J(\\mathbf{x}^{\\star}_{\\mathrm{bar}})$。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含九个结果（每个案例三个，按案例 1、案例 2、案例 3 的顺序列出上述结果），形式为方括号括起来的逗号分隔列表（例如，“[r1,r2,r3,r4,r5,r6,r7,r8,r9]”）。必须只打印这一行。不涉及角度；无需单位转换。所有输出均为无量纲浮点数。",
            "solution": "所提出的问题是逆问题数值优化领域中一个有效、适定且有科学依据的任务。它涉及在正性约束下最小化一个凸二次目标函数，这是数据同化和其他科学领域的标准问题。所有提供的信息都是自洽的、数学上一致的，并且足以推导和实现指定的算法。我们开始进行求解。\n\n核心问题是找到一个状态向量 $\\mathbf{x}$，以最小化最小二乘目标函数：\n$$\nJ(\\mathbf{x}) = \\frac{1}{2}\\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{b} \\rVert_2^2\n$$\n约束条件是 $\\mathbf{x}$ 的所有分量都严格为正，即 $\\mathbf{x} \\in \\mathbb{R}_{++}^{n}$，其中 $(\\mathbf{x})_i = x_i > 0$ 对所有 $i=1, \\dots, n$ 成立。由于最小值点可能位于该集合的边界上，我们实际上是在其闭包 $\\mathbf{x} \\in \\mathbb{R}_{+}^{n}$（其中 $x_i \\ge 0$）中寻求一个解。\n\n### 投影梯度下降法 (PGD)\n\n投影梯度下降法通过迭代优化一个估计值 $\\mathbf{x}^k$，首先沿负梯度方向迈出一步，然后将结果投影到可行集上。\n\n**1. 梯度推导：**\n目标函数 $J(\\mathbf{x})$ 的梯度作为一个基本事实被给出：\n$$\n\\nabla J(\\mathbf{x}) = \\mathbf{A}^{\\top}(\\mathbf{A}\\mathbf{x} - \\mathbf{b})\n$$\n\n**2. PGD 更新规则：**\n迭代更新由 $\\mathbf{x}^{k+1} = \\mathcal{P}_{\\mathbb{R}_{+}^{n}}\\!\\left(\\mathbf{x}^{k} - \\alpha \\nabla J(\\mathbf{x}^{k})\\right)$ 给出。这包括两个步骤：\n- 梯度下降步：$\\mathbf{y}^{k+1} = \\mathbf{x}^{k} - \\alpha \\nabla J(\\mathbf{x}^{k})$。\n- 投影步：$\\mathbf{x}^{k+1} = \\mathcal{P}_{\\mathbb{R}_{+}^{n}}(\\mathbf{y}^{k+1})$。\n\n**3. 步长 $\\alpha$：**\n为保证梯度下降的收敛，步长 $\\alpha$ 必须满足 $\\alpha \\in (0, 2/L)$，其中 $L$ 是梯度 $\\nabla J(\\mathbf{x})$ 的 Lipschitz 常数。问题提供了一个更严格的充分条件 $\\alpha \\in (0, 1/L)$，以及 Lipschitz 常数的一个有效界 $L = \\lVert \\mathbf{A} \\rVert_2^2 = \\sigma_{\\max}^2(\\mathbf{A})$，其中 $\\sigma_{\\max}(\\mathbf{A})$ 是 $\\mathbf{A}$ 的最大奇异值。可以从此区间选择一个固定的步长 $\\alpha$，例如，$\\alpha = c / \\lVert \\mathbf{A} \\rVert_2^2$，其中常数 $c \\in (0, 1)$，如 $c=0.99$。\n\n**4. 投影算子 $\\mathcal{P}_{\\mathbb{R}_{+}^{n}}$：**\n到非负象限 $\\mathbb{R}_{+}^{n}$ 的正交投影是逐分量作用的。对于任意向量 $\\mathbf{y} \\in \\mathbb{R}^n$，其投影 $\\mathbf{x} = \\mathcal{P}_{\\mathbb{R}_{+}^{n}}(\\mathbf{y})$ 定义为：\n$$\nx_i = (\\mathbf{x})_i = \\max(y_i, 0) \\quad \\text{for } i=1, \\dots, n\n$$\n\n**5. 显式 PGD 更新映射：**\n结合这些元素，从 $\\mathbf{x}^k$到 $\\mathbf{x}^{k+1}$ 的显式更新映射为：\n$$\n\\mathbf{x}^{k+1} = \\max\\left(\\mathbf{x}^k - \\alpha \\left( \\mathbf{A}^{\\top}(\\mathbf{A}\\mathbf{x}^k - \\mathbf{b}) \\right), \\mathbf{0}\\right)\n$$\n其中 $\\max$ 函数是逐分量应用的。该算法在 $\\mathbb{R}_{+}^{n}$ 中寻找解。如果真实的最小值点有零分量，该方法能够通过将迭代的相应分量设置为精确的 $0$ 来找到它们。\n\n### 对数障碍法\n\n对数障碍法是一种内点法，它通过一个惩罚项将正性约束整合到目标函数中。这创建了一个代理目标函数 $J_{\\mu}(\\mathbf{x})$，该函数可在没有显式约束的情况下进行最小化，但其结构阻止了迭代接近可行集的边界。\n\n**1. 障碍增广目标函数：**\n代理目标函数定义为：\n$$\nJ_{\\mu}(\\mathbf{x}) = J(\\mathbf{x}) - \\mu \\sum_{i=1}^{n} \\log(x_i) = \\frac{1}{2}\\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{b} \\rVert_2^2 - \\mu \\sum_{i=1}^{n} \\log(x_i)\n$$\n其中 $\\mu > 0$ 是障碍参数。当 $x_i \\to 0^+$ 时，项 $-\\mu \\log(x_i)$ 发散到 $+\\infty$，从而惩罚接近边界的迭代。\n\n**2. 障碍目标函数的梯度：**\n利用梯度算子的线性和给定的基本事实，我们推导 $\\nabla J_{\\mu}(\\mathbf{x})$：\n$$\n\\nabla J_{\\mu}(\\mathbf{x}) = \\nabla \\left(\\frac{1}{2}\\lVert \\mathbf{A}\\mathbf{x} - \\mathbf{b} \\rVert_2^2\\right) - \\nabla \\left(\\mu \\sum_{i=1}^{n} \\log(x_i)\\right)\n$$\n对数障碍项的梯度为：\n$$\n\\nabla \\left(\\mu \\sum_{i=1}^{n} \\log(x_i)\\right) = \\mu \\begin{bmatrix} \\partial/\\partial x_1 \\\\ \\vdots \\\\ \\partial/\\partial x_n \\end{bmatrix} \\sum_{j=1}^{n} \\log(x_j) = \\mu \\begin{bmatrix} 1/x_1 \\\\ \\vdots \\\\ 1/x_n \\end{bmatrix} = \\mu \\mathbf{x}^{\\circ -1}\n$$\n其中 $\\mathbf{x}^{\\circ -1}$ 表示 $\\mathbf{x}$ 的逐分量逆。\n因此，完整的梯度为：\n$$\n\\nabla J_{\\mu}(\\mathbf{x}) = \\mathbf{A}^{\\top}(\\mathbf{A}\\mathbf{x} - \\mathbf{b}) - \\mu \\mathbf{x}^{\\circ -1}\n$$\n\n**3. 保持正性并强制下降的步长规则：**\n更新是一个标准的梯度下降步，$\\mathbf{x}^{k+1} = \\mathbf{x}^k - \\alpha_k \\nabla J_{\\mu}(\\mathbf{x}^k)$。我们必须设计一个选择步长 $\\alpha_k > 0$ 的规则，以保证正性（对所有 $i$，$x_i^{k+1} > 0$）和下降性（$J_{\\mu}(\\mathbf{x}^{k+1})  J_{\\mu}(\\mathbf{x}^k)$）。\n\n- **保持正性：** 对于每个分量 $i$，我们需要 $x_i^{k+1} = x_i^k - \\alpha_k (\\nabla J_{\\mu}(\\mathbf{x}^k))_i > 0$。设 $\\mathbf{g}_{\\mu}^k = \\nabla J_{\\mu}(\\mathbf{x}^k)$。\n  如果 $(g_{\\mu}^k)_i \\le 0$，则第 $i$ 个分量是不减的，对于任何 $\\alpha_k > 0$，条件都成立。\n  如果 $(g_{\\mu}^k)_i > 0$，则必须有 $\\alpha_k  x_i^k / (g_{\\mu}^k)_i$。\n  为了同时满足所有分量，步长必须受这些比率的最小值限制：\n  $$\n  \\alpha_{\\text{max}} = \\min_{i \\text{ s.t. } (g_{\\mu}^k)_i > 0} \\left\\{ \\frac{x_i^k}{(g_{\\mu}^k)_i} \\right\\}\n  $$\n  任何步长 $\\alpha_k \\in (0, \\alpha_{\\text{max}})$ 都将确保 $\\mathbf{x}^{k+1} \\in \\mathbb{R}_{++}^n$。\n\n- **强制下降：** 固定的步长不够稳健，因为 $\\nabla J_{\\mu}$ 的 Lipschitz 常数依赖于 $\\mathbf{x}$，并且在边界附近可能任意大。回溯线搜索是一种标准且有效的方法。规则如下：\n  1.  设置回溯参数：充分下降常数 $c \\in (0, 1)$（例如，$c=10^{-4}$）和收缩因子 $\\tau \\in (0, 1)$（例如，$\\tau=0.5$）。\n  2.  计算搜索方向 $\\mathbf{d}^k = -\\mathbf{g}_{\\mu}^k = -\\nabla J_{\\mu}(\\mathbf{x}^k)$。\n  3.  设置一个初始试探步长 $\\alpha$。一个安全且积极的选择是最大保持正性步长的一部分，例如，$\\alpha = \\beta \\alpha_{\\text{max}}$，其中 $\\beta \\in (0, 1)$（例如，$\\beta=0.99$）。\n  4.  当 Armijo 条件不满足时：\n      $$\n      J_{\\mu}(\\mathbf{x}^k + \\alpha \\mathbf{d}^k) > J_{\\mu}(\\mathbf{x}^k) + c \\alpha (\\mathbf{g}_{\\mu}^k)^{\\top} \\mathbf{d}^k\n      $$\n      这可以简化为：\n      $$\n      J_{\\mu}(\\mathbf{x}^k - \\alpha \\mathbf{g}_{\\mu}^k) > J_{\\mu}(\\mathbf{x}^k) - c \\alpha \\lVert \\mathbf{g}_{\\mu}^k \\rVert_2^2\n      $$\n      缩小步长：$\\alpha \\leftarrow \\tau \\alpha$。\n  5.  设置 $\\alpha_k = \\alpha$。\n\n此过程保证了每一步都严格为正，并减小了障碍目标函数 $J_{\\mu}$ 的值。与 PGD 不同，障碍法总会产生具有严格正分量的解，对于在 $J$ 的真实最小值点中为零的分量，这些分量会随着 $\\mu \\to 0$ 而趋近于零。\n\n### 近边界行为比较\n\n- **PGD：** 投影算子 $\\max(\\cdot, 0)$ 在 $0$ 处不可微，但它允许算法将解向量 $\\mathbf{x}^{\\star}_{\\mathrm{PGD}}$ 的分量精确地置于可行集的边界上，即 $x_i^{\\star} = 0$。当已知真实物理状态具有零值时（例如，某些区域的浓度或密度为零），这是有利的。\n\n- **对数障碍法：** 对数障碍项 $-\\mu \\log(x_i)$ 产生一个梯度分量 $-\\mu/x_i$，当 $x_i \\to 0^+$ 时，它会无限增大。这起到一种排斥力的作用，阻止任何分量变为零。解 $\\mathbf{x}^{\\star}_{\\mathrm{bar}}$ 将始终位于严格内部 $\\mathbb{R}_{++}^n$。$\\mathbf{x}^{\\star}_{\\mathrm{bar}}$ 的最小分量将是小的正值，其大小取决于障碍参数 $\\mu$。因此，在 $\\mathbb{R}_{+}^{n}$ 上最小化的 $J(\\mathbf{x}^{\\star}_{\\mathrm{PGD}})$ 通常会小于或等于 $J(\\mathbf{x}^{\\star}_{\\mathrm{bar}})$，后者是最小化一个扰动目标 $J_\\mu$ 的结果。",
            "answer": "```python\nimport numpy as np\nfrom scipy.linalg import toeplitz\n\ndef solve_pgd(A, b, x0, tol, n_max):\n    \"\"\"\n    Solves the non-negatively constrained least squares problem using Projected Gradient Descent (PGD).\n    min 0.5 * ||Ax - b||^2 s.t. x >= 0.\n    \"\"\"\n    x = np.copy(x0)\n    AtA = A.T @ A\n    Atb = A.T @ b\n    \n    # Lipschitz constant of the gradient is the max eigenvalue of A^T A, which is ||A||_2^2\n    try:\n        # np.linalg.norm(A, 2) can be slow for large matrices, but fine for these cases.\n        L = np.linalg.norm(A, 2)**2\n        if L == 0:  # Handle zero matrix case\n            L = 1.0\n    except np.linalg.LinAlgError:\n        L = np.linalg.norm(AtA, 2) # Fallback if norm(A, 2) fails\n\n    alpha = 0.99 / L  # Fixed step size satisfying alpha  1/L\n\n    for _ in range(n_max):\n        grad = AtA @ x - Atb\n        x_new = x - alpha * grad\n        x_new = np.maximum(x_new, 0) # Projection step\n        \n        if np.linalg.norm(x_new - x)  tol:\n            x = x_new\n            break\n        x = x_new\n        \n    return x\n\ndef solve_barrier(A, b, mu, x0, tol, n_max):\n    \"\"\"\n    Solves the strictly positive least squares problem using a log-barrier method.\n    min 0.5 * ||Ax - b||^2 - mu * sum(log(x_i)).\n    \"\"\"\n    x = np.copy(x0)\n    AtA = A.T @ A\n    Atb = A.T @ b\n\n    # Backtracking line search parameters\n    c1 = 1e-4\n    tau = 0.5\n\n    for k in range(n_max):\n        # Prevent x components from being exactly zero or negative due to numerical error\n        x[x = 0] = np.finfo(float).eps\n\n        # Calculate gradient of barrier objective J_mu\n        grad_mu = (AtA @ x - Atb) - mu / x\n        \n        # Calculate max step size to preserve positivity\n        pos_grad_indices = grad_mu > 0\n        if not np.any(pos_grad_indices):\n            alpha_max_pos = 1.0 # All gradient components non-positive, can take a large step\n        else:\n            alpha_max_pos = np.min(x[pos_grad_indices] / grad_mu[pos_grad_indices])\n        \n        alpha = 0.99 * alpha_max_pos\n\n        # Perform backtracking line search\n        J_mu_k = 0.5 * np.sum((A @ x - b)**2) - mu * np.sum(np.log(x))\n        \n        while True:\n            x_new = x - alpha * grad_mu\n            \n            # Ensure new iterate is strictly positive for log evaluation\n            if np.any(x_new = 0):\n                alpha *= tau\n                if alpha  1e-20: # Step size too small\n                     x_new = x # discard step\n                     break\n                continue\n\n            J_mu_new = 0.5 * np.sum((A @ x_new - b)**2) - mu * np.sum(np.log(x_new))\n            \n            if J_mu_new = J_mu_k - c1 * alpha * np.dot(grad_mu, grad_mu): # Armijo condition check\n                break\n            \n            alpha *= tau\n            if alpha  1e-20: # Step size too small\n                x_new = x # discard step\n                break\n\n        if np.linalg.norm(x_new - x)  tol:\n            x = x_new\n            break\n        x = x_new\n    \n    return x\n\ndef calculate_metrics(A, b, x_pgd, x_bar):\n    \"\"\"Calculates the three required performance metrics.\"\"\"\n    min_pgd = np.min(x_pgd)\n    min_bar = np.min(x_bar)\n    \n    j_pgd = 0.5 * np.linalg.norm(A @ x_pgd - b)**2\n    j_bar = 0.5 * np.linalg.norm(A @ x_bar - b)**2\n    \n    diff_j = j_pgd - j_bar\n    return min_pgd, min_bar, diff_j\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # Test Suite (A, b, mu, x0, tol, n_max)\n    case1_A = np.array([[1.0]])\n    case1_b = np.array([1e-6])\n    case1_mu = 1e-6\n    case1_x0 = np.array([1e-3])\n    case1_tol = 1e-12\n    case1_nmax = 20000\n\n    case2_A = np.diag([1.0, 1e-2])\n    case2_b = np.array([1e-3, 1e-5])\n    case2_mu = 1e-5\n    case2_x0 = np.array([1e-3, 1e-3])\n    case2_tol = 1e-12\n    case2_nmax = 20000\n\n    n3 = 10\n    h = np.array([0.25, 0.5, 0.25])\n    padding = np.zeros(n3 - len(h))\n    first_col = np.concatenate((h, padding))\n    first_row = np.zeros(n3)\n    first_row[0] = h[1]\n    first_row[1] = h[0]\n    case3_A = toeplitz(first_col, first_row)\n    # A manual correction based on the problem description (x_{i-1}, x_i, x_{i+1})\n    # The above generic toeplitz doesn't generate the right structure for convolution.\n    # Let's build it manually.\n    case3_A = np.zeros((n3, n3))\n    for i in range(n3):\n        case3_A[i, i] = 0.5\n        if i > 0:\n            case3_A[i, i-1] = 0.25\n        if i  n3 - 1:\n            case3_A[i, i+1] = 0.25\n\n    case3_x_true = np.array([0.0, 0.1, 0.0, 0.2, 0.0, 0.05, 0.0, 0.1, 0.0, 0.0])\n    case3_b = case3_A @ case3_x_true\n    case3_mu = 1e-6\n    case3_x0 = 1e-3 * np.ones(n3)\n    case3_tol = 1e-12\n    case3_nmax = 50000\n    \n    test_cases = [\n        (case1_A, case1_b, case1_mu, case1_x0, case1_tol, case1_nmax),\n        (case2_A, case2_b, case2_mu, case2_x0, case2_tol, case2_nmax),\n        (case3_A, case3_b, case3_mu, case3_x0, case3_tol, case3_nmax),\n    ]\n\n    results = []\n    for A, b, mu, x0, tol, n_max in test_cases:\n        x_pgd = solve_pgd(A, b, x0, tol, n_max)\n        x_bar = solve_barrier(A, b, mu, x0, tol, n_max)\n        \n        metrics = calculate_metrics(A, b, x_pgd, x_bar)\n        results.extend(metrics)\n\n    # Format output as specified\n    print(f\"[{','.join(f'{v:.15e}' for v in results)}]\")\n\nsolve()\n```"
        }
    ]
}