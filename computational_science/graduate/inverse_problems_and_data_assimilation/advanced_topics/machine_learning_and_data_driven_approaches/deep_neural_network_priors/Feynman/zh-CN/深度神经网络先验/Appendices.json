{
    "hands_on_practices": [
        {
            "introduction": "深度神经网络先验的一个令人惊奇的特性是，即使是未经训练的随机初始化网络也能在逆问题中充当有效的正则化器。这个练习旨在揭示其背后的一个关键原理：谱偏见（spectral bias）。我们将利用神经网络切向核（NTK）的理论框架，通过分析梯度下降过程中不同频率分量的收敛速度，来量化这种网络结构内在的、优先学习低频信息的倾向。通过这个练习，你将深入理解为什么深度图像先验（DIP）能够成功地从噪声或不完整的测量中恢复出清晰的图像 。",
            "id": "3375205",
            "problem": "考虑深度图像先验 (Deep Image Prior, DIP) 重建框架，其中未知图像被参数化为 $x = G_{\\theta}(z)$，$z$ 为固定输入，$\\theta$ 为可训练参数，并通过使用梯度下降法最小化关于 $\\theta$ 的平方数据失配 $\\|A G_{\\theta}(z) - y\\|^{2}$ 来估计。假设生成器 $G_{\\theta}$ 是一个无限宽的、平移等变的卷积网络，在初始化时具有独立同分布的零均值滤波器，因此，在无限宽度和小学习率的极限下，$f_{t} = G_{\\theta(t)}(z)$ 的函数空间梯度流可以很好地近似为\n$$\n\\partial_{t} f_{t} = - \\gamma \\, K \\, \\nabla_{f} \\|A f - y\\|^{2} \\big|_{f = f_{t}},\n$$\n其中 $\\gamma > 0$ 是学习率缩放因子，$K$ 是 $G_{\\theta}$ 在初始化时的神经正切核 (Neural Tangent Kernel, NTK) 算子。设 $A$ 是作用于图像的线性、移位不变前向算子（例如，模糊），并用 $A^{\\top}$ 表示其伴随算子。根据最小二乘梯度的标准性质，有 $\\nabla_{f} \\|A f - y\\|^{2} = 2 A^{\\top} (A f - y)$。假设 $K$ 是移位不变的，具有各向同性高斯傅里叶乘子，而 $A$ 是各向同性高斯模糊，具体为\n$$\n\\widehat{K}(\\omega) = \\exp\\!\\big(- \\sigma^{2} \\|\\omega\\|^{2}\\big), \\quad \\widehat{A}(\\omega) = \\exp\\!\\big(- \\beta \\|\\omega\\|^{2}\\big),\n$$\n其中参数 $\\sigma^{2} > 0$ 和 $\\beta > 0$，$\\omega \\in \\mathbb{R}^{2}$ 表示空间频率。在这些假设下，残差 $\\widehat{r}_{t}(\\omega) \\equiv \\widehat{f}_{t}(\\omega) - \\widehat{f}_{\\infty}(\\omega)$ 的傅里叶模式独立演化，其中 $f_{\\infty}$ 是梯度流的不动点。\n\n从这些前提出发，仅使用线性算子、伴随算子和移位不变算子的傅里叶对角化性质，推导控制 $\\widehat{r}_{t}(\\omega)$ 演化的指数衰减率 $\\lambda(\\omega)$ 的表达式，该演化由以下常微分方程定义\n$$\n\\partial_{t} \\widehat{r}_{t}(\\omega) = - \\lambda(\\omega) \\, \\widehat{r}_{t}(\\omega).\n$$\n然后，对于 $\\sigma^{2} = 10^{-3}$，$\\beta = 5 \\times 10^{-4}$，$\\gamma = 1$，以及两个各向同性频率幅度 $\\|\\omega_{l}\\| = 20$ 和 $\\|\\omega_{h}\\| = 60$，计算比率 $\\lambda(\\omega_{h}) / \\lambda(\\omega_{l})$。将最终数值答案四舍五入到四位有效数字。最终答案必须是一个无单位的实数。",
            "solution": "首先验证用户提供的问题的正确性和可解性。\n\n### 第 1 步：提取已知条件\n- 图像重建框架是深度图像先验 (DIP)：$x = G_{\\theta}(z)$。\n- 目标函数是平方数据失配：$\\|A G_{\\theta}(z) - y\\|^{2}$。\n- 优化方法是关于参数 $\\theta$ 的梯度下降。\n- $f_{t} = G_{\\theta(t)}(z)$ 在无限宽度、小学习率下的梯度流是：\n$$\n\\partial_{t} f_{t} = - \\gamma \\, K \\, \\nabla_{f} \\|A f - y\\|^{2} \\big|_{f = f_{t}}\n$$\n- 学习率缩放因子为 $\\gamma > 0$。\n- 算子 $K$ 是 $G_{\\theta}$ 在初始化时的神经正切核 (NTK)。\n- 最小二乘损失的梯度是：$\\nabla_{f} \\|A f - y\\|^{2} = 2 A^{\\top} (A f - y)$。\n- $A$ 是一个线性、移位不变前向算子，$A^{\\top}$ 是其伴随算子。\n- $K$ 是一个移位不变算子。\n- $K$ 的傅里叶乘子是：$\\widehat{K}(\\omega) = \\exp(-\\sigma^{2} \\|\\omega\\|^{2})$，其中 $\\sigma^2 > 0$。\n- $A$ 的傅里叶乘子是：$\\widehat{A}(\\omega) = \\exp(-\\beta \\|\\omega\\|^{2})$，其中 $\\beta > 0$。\n- 傅里叶域中的残差定义为：$\\widehat{r}_{t}(\\omega) \\equiv \\widehat{f}_{t}(\\omega) - \\widehat{f}_{\\infty}(\\omega)$，其中 $f_{\\infty}$ 是流的不动点。\n- 残差的演化形式为：$\\partial_{t} \\widehat{r}_{t}(\\omega) = - \\lambda(\\omega) \\, \\widehat{r}_{t}(\\omega)$。\n- 参数的数值为：$\\sigma^{2} = 10^{-3}$，$\\beta = 5 \\times 10^{-4}$，$\\gamma = 1$。\n- 给定了两个各向同性频率幅度：$\\|\\omega_{l}\\| = 20$ 和 $\\|\\omega_{h}\\| = 60$。\n- 任务是推导衰减率 $\\lambda(\\omega)$ 的表达式，并计算比率 $\\lambda(\\omega_{h}) / \\lambda(\\omega_{l})$。\n\n### 第 2 步：使用提取的已知条件进行验证\n- **科学依据**：该问题在无限宽度神经网络理论，特别是神经正切核 (NTK) 模型中有充分的理论基础。通过深度图像先验 (DIP) 将其应用于逆问题是一个当前有效的研究领域。所有算子和概念（梯度流、移位不变算子的傅里叶分析、伴随算子）在应用数学和信号处理中都是标准的。\n- **适定性**：该问题是适定的。它提供了推导所求量 $\\lambda(\\omega)$ 和计算最终比率所需的所有必要方程、定义和常数。\n- **客观性**：问题以精确、客观的数学语言陈述。\n- **结论**：该问题被认为是有效的，因为它是自洽的、科学合理的且无歧义的。\n\n### 第 3 步：进行求解\n\n我们从给定的函数 $f_t$ 的梯度流方程开始：\n$$\n\\partial_{t} f_{t} = - \\gamma \\, K \\, \\nabla_{f} \\|A f - y\\|^{2} \\big|_{f = f_{t}}\n$$\n代入给定的梯度表达式 $\\nabla_{f} \\|A f - y\\|^{2} = 2 A^{\\top} (A f - y)$，我们得到：\n$$\n\\partial_{t} f_{t} = - \\gamma \\, K \\, [2 A^{\\top} (A f_{t} - y)] = -2 \\gamma \\, K A^{\\top} (A f_{t} - y)\n$$\n在不动点 $f_{\\infty}$ 处，时间导数为零，即 $\\partial_{t} f_{\\infty} = 0$。这意味着 $f_{\\infty}$ 满足：\n$$\n-2 \\gamma \\, K A^{\\top} (A f_{\\infty} - y) = 0\n$$\n现在我们分析残差 $r_{t} = f_{t} - f_{\\infty}$ 的演化。残差的时间导数为 $\\partial_{t} r_{t} = \\partial_{t} (f_{t} - f_{\\infty}) = \\partial_{t} f_{t}$。我们可以从 $f_t$ 的流方程中减去不动点方程（乘以 1）：\n$$\n\\partial_{t} f_{t} - 0 = -2 \\gamma \\, K A^{\\top} (A f_{t} - y) - [-2 \\gamma \\, K A^{\\top} (A f_{\\infty} - y)]\n$$\n$$\n\\partial_{t} (f_{t} - f_{\\infty}) = -2 \\gamma \\, K A^{\\top} [ (A f_{t} - y) - (A f_{\\infty} - y) ]\n$$\n$$\n\\partial_{t} r_{t} = -2 \\gamma \\, K A^{\\top} (A f_{t} - A f_{\\infty})\n$$\n由于 $A$ 是线性算子，有 $A f_{t} - A f_{\\infty} = A(f_{t} - f_{\\infty}) = A r_{t}$。残差的方程变为：\n$$\n\\partial_{t} r_{t} = -2 \\gamma \\, K A^{\\top} A \\, r_{t}\n$$\n这是一个在函数空间中的线性常微分方程。由于算子 $K$、$A^{\\top}$ 和 $A$ 都是移位不变的，它们可以被傅里叶变换对角化。我们可以通过将方程变换到傅里叶域来求解。设 $\\widehat{r}_{t}(\\omega)$ 是 $r_{t}$ 的傅里叶变换。移位不变算子 $L$ 应用于函数 $g$ 后的傅里叶变换是它们各自傅里叶表示的乘积，即 $\\mathcal{F}\\{Lg\\}(\\omega) = \\widehat{L}(\\omega) \\widehat{g}(\\omega)$。将此应用于我们的方程：\n$$\n\\mathcal{F}\\{\\partial_{t} r_{t}\\}(\\omega) = \\mathcal{F}\\{-2 \\gamma \\, K A^{\\top} A \\, r_{t}\\}(\\omega)\n$$\n$$\n\\partial_{t} \\widehat{r}_{t}(\\omega) = -2 \\gamma \\, \\widehat{(K A^{\\top} A)}(\\omega) \\, \\widehat{r}_{t}(\\omega)\n$$\n复合算子 $K A^{\\top} A$ 的傅里叶乘子是各个傅里叶乘子的乘积：\n$$\n\\widehat{(K A^{\\top} A)}(\\omega) = \\widehat{K}(\\omega) \\, \\widehat{A^{\\top}}(\\omega) \\, \\widehat{A}(\\omega)\n$$\n对于线性算子 $A$，其伴随算子 $A^{\\top}$ 的乘子是其自身乘子的复共轭，即 $\\widehat{A^{\\top}}(\\omega) = (\\widehat{A}(\\omega))^{*}$。给定的 $A$ 的乘子是 $\\widehat{A}(\\omega) = \\exp(-\\beta \\|\\omega\\|^{2})$。由于 $\\beta$ 和 $\\|\\omega\\|^2$ 都是实数，$\\widehat{A}(\\omega)$ 是一个实值函数。因此，$(\\widehat{A}(\\omega))^{*} = \\widehat{A}(\\omega)$，所以 $\\widehat{A^{\\top}}(\\omega) = \\widehat{A}(\\omega)$。\n那么复合乘子是：\n$$\n\\widehat{(K A^{\\top} A)}(\\omega) = \\widehat{K}(\\omega) (\\widehat{A}(\\omega))^{2}\n$$\n代入给定的 $\\widehat{K}(\\omega)$ 和 $\\widehat{A}(\\omega)$ 的形式：\n$$\n\\widehat{(K A^{\\top} A)}(\\omega) = \\exp(-\\sigma^{2} \\|\\omega\\|^{2}) \\left( \\exp(-\\beta \\|\\omega\\|^{2}) \\right)^{2} = \\exp(-\\sigma^{2} \\|\\omega\\|^{2}) \\exp(-2\\beta \\|\\omega\\|^{2})\n$$\n$$\n\\widehat{(K A^{\\top} A)}(\\omega) = \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega\\|^{2})\n$$\n将其代回 $\\widehat{r}_{t}(\\omega)$ 的微分方程：\n$$\n\\partial_{t} \\widehat{r}_{t}(\\omega) = -2 \\gamma \\, \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega\\|^{2}) \\, \\widehat{r}_{t}(\\omega)\n$$\n我们已知该方程的形式为 $\\partial_{t} \\widehat{r}_{t}(\\omega) = - \\lambda(\\omega) \\, \\widehat{r}_{t}(\\omega)$。通过直接比较，我们确定衰减率 $\\lambda(\\omega)$ 为：\n$$\n\\lambda(\\omega) = 2 \\gamma \\, \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega\\|^{2})\n$$\n现在，我们必须计算在给定频率幅度 $\\|\\omega_{h}\\| = 60$ 和 $\\|\\omega_{l}\\| = 20$ 下的比率 $\\lambda(\\omega_{h}) / \\lambda(\\omega_{l})$。\n$$\n\\frac{\\lambda(\\omega_{h})}{\\lambda(\\omega_{l})} = \\frac{2 \\gamma \\, \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega_{h}\\|^{2})}{2 \\gamma \\, \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega_{l}\\|^{2})}\n$$\n项 $2\\gamma$ 被消掉了：\n$$\n\\frac{\\lambda(\\omega_{h})}{\\lambda(\\omega_{l})} = \\exp(-(\\sigma^{2} + 2\\beta) \\|\\omega_{h}\\|^{2} + (\\sigma^{2} + 2\\beta) \\|\\omega_{l}\\|^{2}) = \\exp(-(\\sigma^{2} + 2\\beta) (\\|\\omega_{h}\\|^{2} - \\|\\omega_{l}\\|^{2}))\n$$\n我们代入给定的数值：\n$\\sigma^{2} = 10^{-3}$\n$\\beta = 5 \\times 10^{-4}$\n$\\|\\omega_{l}\\| = 20 \\implies \\|\\omega_{l}\\|^{2} = 400$\n$\\|\\omega_{h}\\| = 60 \\implies \\|\\omega_{h}\\|^{2} = 3600$\n\n首先，我们计算指数中的系数：\n$$\n\\sigma^{2} + 2\\beta = 10^{-3} + 2(5 \\times 10^{-4}) = 10^{-3} + 10 \\times 10^{-4} = 10^{-3} + 10^{-3} = 2 \\times 10^{-3}\n$$\n接下来，我们计算频率幅度平方的差值：\n$$\n\\|\\omega_{h}\\|^{2} - \\|\\omega_{l}\\|^{2} = 3600 - 400 = 3200\n$$\n现在，我们计算指数：\n$$\n-(\\sigma^{2} + 2\\beta) (\\|\\omega_{h}\\|^{2} - \\|\\omega_{l}\\|^{2}) = -(2 \\times 10^{-3}) \\times 3200 = -6.4\n$$\n最后，我们计算比率：\n$$\n\\frac{\\lambda(\\omega_{h})}{\\lambda(\\omega_{l})} = \\exp(-6.4) \\approx 0.0016615555...\n$$\n将此结果四舍五入到四位有效数字，得到 $0.001662$。用科学记数法表示为 $1.662 \\times 10^{-3}$。",
            "answer": "$$\\boxed{1.662 \\times 10^{-3}}$$"
        },
        {
            "introduction": "虽然深度先验功能强大，但它们对应的能量函数通常是非凸的，这给优化算法带来了巨大挑战，因为它们很容易陷入质量较差的局部最小值。本练习将演示一种强大的技术——温度退火（temperature annealing），用于在这些复杂的能量景观中进行导航。通过从一个平滑的、近似凸的先验开始，然后逐步增加其复杂性（降低“温度”），我们可以引导优化过程找到更好的解，从而有效避免在求解初期就陷入糟糕的局部极值 。",
            "id": "3375150",
            "problem": "考虑一个确定性线性逆问题，其中未知状态向量 $x \\in \\mathbb{R}^n$ 通过线性正演算子 $A \\in \\mathbb{R}^{m \\times n}$ 进行观测，并受加性噪声影响，产生数据 $y \\in \\mathbb{R}^m$。最大后验（MAP）估计在一个由深度神经网络（DNN）定义的先验条件下，最小化负对数后验概率。设数据失配项为 $E_{\\text{data}}(x) = \\frac{1}{2}\\|A x - y\\|_2^2$，先验能量项为 $E_{\\phi,\\tau}(x) = \\frac{1}{2}\\|g_{\\phi,\\tau}(x)\\|_2^2$，其中 $g_{\\phi,\\tau}$ 是一个依赖于温度参数 $\\tau > 0$ 的 DNN 映射。需要最小化的总能量为\n$$\nE_{\\text{total}}(x;\\lambda,\\tau) = E_{\\text{data}}(x) + \\lambda\\,E_{\\phi,\\tau}(x),\n$$\n其中 $\\lambda > 0$ 用于权衡先验项相对于数据项的比重。\n\n为探索温度退火方法，我们使用一个基于正弦表示网络（Sinusoidal Representation Network, SIREN）的 DNN 先验，该网络带有一个缩放的正弦激活函数，通过 $\\tau$ 控制平滑度。具体而言，定义 $g_{\\phi,\\tau}:\\mathbb{R}^n\\to\\mathbb{R}^n$ 为\n$$\ng_{\\phi,\\tau}(x) = \\tau \\,\\sin\\!\\left(\\frac{W_1 x + b_1}{\\tau}\\right),\n$$\n其中 $W_1 \\in \\mathbb{R}^{n \\times n}$ 且 $b_1 \\in \\mathbb{R}^n$。正弦函数按分量施加。这种选择产生的先验能量面的非凸性随 $\\tau$ 的减小而增加：当 $\\tau$ 较大时，激活函数近似线性（凸二次先验）；而当 $\\tau$ 较小时，激活函数变得高度振荡，引入许多局部极小值。\n\n从贝叶斯公式和 MAP 的定义出发，推导出 $E_{\\text{total}}$ 的梯度由数据梯度和网络雅可比矩阵引出的先验梯度之和给出。实现一个用于 MAP 的梯度下降求解器，并比较两种策略：\n- 冷启动：在最低温度 $\\tau_{\\text{final}}$ 下直接最小化 $E_{\\text{total}}(x;\\lambda,\\tau_{\\text{final}})$。\n- 退火优化：从一个高温开始，通过一个温度计划表逐步降低 $\\tau$，在每个阶段最小化 $E_{\\text{total}}(x;\\lambda,\\tau)$，并使用前一阶段的极小值点作为初始化。\n\n使用以下精确且自洽的设置，其中 $n = m = 2$：\n- 正演算子 $A = I_2$，即 $2 \\times 2$ 单位矩阵。\n- 真实状态 $x^\\star = [2,-2]^\\top$。\n- 观测值 $y = x^\\star + \\eta$，其中固定噪声 $\\eta = [0.05,0.05]^\\top$。\n- 先验网络权重 $W_1 = I_2$。\n- 最终温度 $\\tau_{\\text{final}} = 0.2$。\n- 偏置 $b_1$ 的选择使得对于一个选定的整数向量 $k \\in \\mathbb{Z}^2$，$x^\\star$ 恰好是 $g_{\\phi,\\tau_{\\text{final}}}$ 的一个零点，即\n$$\nb_1 = k\\,\\pi\\,\\tau_{\\text{final}} - W_1 x^\\star,\n$$\n其中 $k = [3,-3]^\\top$。\n- 冷启动和退火优化的初始化均为 $x_0 = [0,0]^\\top$。\n- 梯度下降步长 $\\alpha = 0.1$。\n- 每个温度阶段的迭代次数 $S = 300$。\n\n对于上面定义的 SIREN 先验，先验能量的梯度为\n$$\n\\nabla E_{\\phi,\\tau}(x) = J_{g_{\\phi,\\tau}}(x)^\\top\\,g_{\\phi,\\tau}(x),\n$$\n其中 $g_{\\phi,\\tau}$ 的雅可比矩阵是\n$$\nJ_{g_{\\phi,\\tau}}(x) = \\cos\\!\\left(\\frac{W_1 x + b_1}{\\tau}\\right)\\,W_1,\n$$\n余弦函数按分量施加。因此，用于梯度下降的总梯度为\n$$\n\\nabla E_{\\text{total}}(x;\\lambda,\\tau) = (A^\\top (A x - y)) + \\lambda\\,\\Big[\\cos\\!\\Big(\\frac{W_1 x + b_1}{\\tau}\\Big)\\,W_1\\Big]^\\top \\Big[\\tau \\,\\sin\\!\\Big(\\frac{W_1 x + b_1}{\\tau}\\Big)\\Big].\n$$\n\n通过在一个温度计划表 $\\{\\tau_1,\\tau_2,\\dots,\\tau_T\\}$ 上迭代梯度下降来构建退火优化，使用在 $\\tau_t$ 处的最终迭代结果作为 $\\tau_{t+1}$ 的初始值。冷启动是在 $\\tau_{\\text{final}}$ 下的单个阶段，其总迭代次数与退火运行相同。\n\n测试套件：\n- 案例 1：$\\lambda = 5.0$，温度计划表 $[5.0,1.0,0.5,0.2]$。\n- 案例 2：$\\lambda = 12.0$，温度计划表 $[5.0,1.0,0.5,0.2]$。\n- 案例 3：$\\lambda = 5.0$，温度计划表 $[0.2,0.2,0.2,0.2]$（无退火）。\n\n对于每种情况，计算欧几里得距离 $D_{\\text{cold}} = \\|x_{\\text{cold}} - x^\\star\\|_2$ 和 $D_{\\text{anneal}} = \\|x_{\\text{anneal}} - x^\\star\\|_2$，并以浮点数 $I = D_{\\text{cold}} - D_{\\text{anneal}}$ 的形式报告改进量。正的 $I$ 值表示退火有助于逃离较差的局部极小值，从而更接近真实值。\n\n最终输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按测试套件案例的顺序排列结果，每个改进值四舍五入到小数点后六位（例如 $[i_1,i_2,i_3]$）。不涉及物理单位。根据构造，三角函数内的所有角度均以弧度为单位。程序必须是自包含的，并且不需要任何输入。",
            "solution": "在提供解决方案之前，对问题进行验证。\n\n### 步骤 1：提取已知条件\n- **问题类型**：未知状态向量 $x \\in \\mathbb{R}^n$ 的确定性线性逆问题。\n- **正演模型**：$y = A x + \\eta$，其中 $y \\in \\mathbb{R}^m$ 是观测值，$A \\in \\mathbb{R}^{m \\times n}$ 是正演算子，$\\eta$ 是加性噪声。\n- **目标函数（总能量）**：$E_{\\text{total}}(x;\\lambda,\\tau) = E_{\\text{data}}(x) + \\lambda\\,E_{\\phi,\\tau}(x)$，其中 $\\lambda > 0$。\n- **数据失配项**：$E_{\\text{data}}(x) = \\frac{1}{2}\\|A x - y\\|_2^2$。\n- **先验能量项**：$E_{\\phi,\\tau}(x) = \\frac{1}{2}\\|g_{\\phi,\\tau}(x)\\|_2^2$。\n- **DNN 先验函数**：$g_{\\phi,\\tau}(x) = \\tau \\,\\sin\\!\\left(\\frac{W_1 x + b_1}{\\tau}\\right)$，其中 $g_{\\phi,\\tau}:\\mathbb{R}^n\\to\\mathbb{R}^n$，$W_1 \\in \\mathbb{R}^{n \\times n}$，$b_1 \\in \\mathbb{R}^n$，$\\tau > 0$ 是温度参数，$\\sin$ 按分量施加。\n- **总梯度**：$\\nabla E_{\\text{total}}(x;\\lambda,\\tau) = (A^\\top (A x - y)) + \\lambda\\,\\Big[\\cos\\!\\Big(\\frac{W_1 x + b_1}{\\tau}\\Big)\\,W_1\\Big]^\\top \\Big[\\tau \\,\\sin\\!\\Big(\\frac{W_1 x + b_1}{\\tau}\\Big)\\Big]$。\n- **数值参数**：\n    - $n = m = 2$。\n    - $A = I_2$（$2 \\times 2$ 单位矩阵）。\n    - 真实状态 $x^\\star = [2,-2]^\\top$。\n    - 固定噪声 $\\eta = [0.05,0.05]^\\top$。\n    - 观测值 $y = x^\\star + \\eta$。\n    - 先验权重 $W_1 = I_2$。\n    - 最终温度 $\\tau_{\\text{final}} = 0.2$。\n    - 偏置向量 $b_1 = k\\,\\pi\\,\\tau_{\\text{final}} - W_1 x^\\star$，其中整数向量 $k = [3,-3]^\\top$。\n    - 初始化 $x_0 = [0,0]^\\top$。\n    - 梯度下降步长 $\\alpha = 0.1$。\n    - 每个温度阶段的迭代次数 $S = 300$。\n- **测试案例**：\n    - 案例 1：$\\lambda = 5.0$，温度计划表 $[5.0,1.0,0.5,0.2]$。\n    - 案例 2：$\\lambda = 12.0$，温度计划表 $[5.0,1.0,0.5,0.2]$。\n    - 案例 3：$\\lambda = 5.0$，温度计划表 $[0.2,0.2,0.2,0.2]$。\n- **任务**：对于每种情况，计算改进量 $I = D_{\\text{cold}} - D_{\\text{anneal}}$，其中 $D_{\\text{cold}} = \\|x_{\\text{cold}} - x^\\star\\|_2$ 和 $D_{\\text{anneal}} = \\|x_{\\text{anneal}} - x^\\star\\|_2$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题是一个在计算科学领域定义明确的数值实验，特别是探索用深度先验进行正则化的逆问题的优化策略。\n- **科学依据**：该框架植根于贝叶斯推断（最大后验估计）和标准优化技术（梯度下降）。使用类 SIREN 网络作为先验是该领域的现代方法。所有数学公式均与已建立的理论一致。\n- **良态的**：该问题为计算任务提供了所有必要的参数和明确的目标。数值过程（梯度下降）保证了解的存在性，并且要求的输出是每种情况下的唯一标量值。\n- **客观性**：所有术语都经过了精确的数学定义。没有主观或模棱两可的陈述。\n- **完整性与一致性**：问题是自包含的。所提供的参数足以实现所需的算法。偏置 $b_1$ 的公式与在 $\\tau_{\\text{final}}$ 下使真实值 $x^\\star$ 成为先验能量景观的一个零点的目标一致，这是构建现实测试案例的常用技术。梯度的推导与能量项的定义一致。\n\n### 步骤 3：结论与行动\n该问题是**有效**的。这是一个合理且良态的计算问题。将提供一个解决方案。\n\n### 解法推导与方法\n该问题要求找到 $x$ 的最大后验（MAP）估计，即最小化总能量 $E_{\\text{total}}(x;\\lambda,\\tau)$。我们将使用梯度下降法，这是一种迭代优化算法，其中估计值 $x_k$ 在每一步 $k$ 按如下方式更新：\n$$\nx_{k+1} = x_k - \\alpha \\nabla E_{\\text{total}}(x_k;\\lambda,\\tau)\n$$\n其中 $\\alpha$ 是步长。该方法的核心是计算梯度 $\\nabla E_{\\text{total}}$。\n\n**1. 梯度推导**\n总能量为 $E_{\\text{total}} = E_{\\text{data}} + \\lambda E_{\\text{prior}}$。其梯度是各部分梯度之和。\n\n- **数据失配项 $E_{\\text{data}}(x)$ 的梯度**：\n数据失配项为 $E_{\\text{data}}(x) = \\frac{1}{2}\\|A x - y\\|_2^2 = \\frac{1}{2}(A x - y)^\\top(A x - y)$。\n关于 $x$ 的梯度是矩阵微积分中的一个标准结果：\n$$\n\\nabla_x E_{\\text{data}}(x) = A^\\top(A x - y).\n$$\n对于 $A=I_2$ 的特殊情况，这简化为 $\\nabla_x E_{\\text{data}}(x) = x - y$。\n\n- **先验能量项 $E_{\\phi,\\tau}(x)$ 的梯度**：\n先验能量为 $E_{\\phi,\\tau}(x) = \\frac{1}{2}\\|g_{\\phi,\\tau}(x)\\|_2^2 = \\frac{1}{2}g_{\\phi,\\tau}(x)^\\top g_{\\phi,\\tau}(x)$。\n使用向量链式法则，其梯度为：\n$$\n\\nabla_x E_{\\phi,\\tau}(x) = J_{g_{\\phi,\\tau}}(x)^\\top g_{\\phi,\\tau}(x),\n$$\n其中 $J_{g_{\\phi,\\tau}}(x)$ 是 $g_{\\phi,\\tau}(x)$ 的雅可比矩阵。\n\n为求雅可比矩阵，令 $v(x) = \\frac{W_1 x + b_1}{\\tau}$。映射为 $g_{\\phi,\\tau}(x) = \\tau \\sin(v(x))$。$v(x)$ 关于 $x$ 的雅可比矩阵是 $J_v(x) = \\frac{1}{\\tau} W_1$。$\\tau \\sin(v)$ 关于 $v$ 的雅可比矩阵是一个对角矩阵，$J_{\\sin}(v) = \\text{diag}(\\tau\\cos(v_1), \\dots, \\tau\\cos(v_n))$。根据链式法则：\n$$\nJ_{g_{\\phi,\\tau}}(x) = J_{\\sin}(v(x)) \\cdot J_v(x) = \\text{diag}\\left(\\tau \\cos\\left(\\frac{W_1 x + b_1}{\\tau}\\right)\\right) \\left(\\frac{1}{\\tau} W_1\\right) = \\text{diag}\\left(\\cos\\left(\\frac{W_1 x + b_1}{\\tau}\\right)\\right) W_1.\n$$\n问题描述中的符号 $\\cos(v)W_1$ 是该乘积的紧凑形式。\n将雅可比矩阵和函数 $g_{\\phi,\\tau}(x)$ 代入先验梯度公式：\n$$\n\\nabla_x E_{\\phi,\\tau}(x) = \\left(\\text{diag}\\left(\\cos(\\dots)\\right) W_1\\right)^\\top \\left(\\tau \\sin(\\dots)\\right) = W_1^\\top \\text{diag}\\left(\\cos(\\dots)\\right) \\left(\\tau \\sin(\\dots)\\right).\n$$\n这可以使用逐元素（Hadamard）乘积 $\\odot$ 来表示：\n$$\n\\nabla_x E_{\\phi,\\tau}(x) = W_1^\\top \\left(\\tau \\cos\\left(\\frac{W_1 x + b_1}{\\tau}\\right) \\odot \\sin\\left(\\frac{W_1 x + b_1}{\\tau}\\right)\\right).\n$$\n使用三角恒等式 $\\sin(2\\theta) = 2\\sin(\\theta)\\cos(\\theta)$，我们得到一个更紧凑的形式：\n$$\n\\nabla_x E_{\\phi,\\tau}(x) = W_1^\\top \\left(\\frac{\\tau}{2} \\sin\\left(2\\frac{W_1 x + b_1}{\\tau}\\right)\\right).\n$$\n对于 $W_1 = I_2$ 的特殊情况，先验梯度简化为 $\\frac{\\tau}{2} \\sin\\left(2\\frac{x + b_1}{\\tau}\\right)$。\n\n- **总梯度**：\n结合这些项，用于梯度下降的完整梯度为：\n$$\n\\nabla E_{\\text{total}}(x;\\lambda,\\tau) = A^\\top(A x - y) + \\lambda W_1^\\top \\left(\\frac{\\tau}{2} \\sin\\left(2\\frac{W_1 x + b_1}{\\tau}\\right)\\right).\n$$\n当 $A=I_2$ 和 $W_1=I_2$ 时，这简化为：\n$$\n\\nabla E_{\\text{total}}(x;\\lambda,\\tau) = (x-y) + \\lambda \\frac{\\tau}{2} \\sin\\left(2\\frac{x + b_1}{\\tau}\\right).\n$$\n求解器将实现这个最终表达式。\n\n**2. 优化策略**\n\n- **退火优化**：这种策略旨在解决能量景观的非凸性问题，这在低温 $\\tau$ 时尤为严重。它从高温开始优化，此时先验近似为二次型，能量景观更平滑，局部极小值更少。然后根据一个计划表 $\\{\\tau_1, \\tau_2, \\dots, \\tau_T\\}$ 逐渐降低温度 $\\tau$。在每个温度 $\\tau_t$ 下，运行梯度下降 $S$ 次迭代，其中第 $t$ 阶段的初始状态是第 $t-1$ 阶段的最终状态。此过程有助于估计值在复杂的能量曲面上导航，避免陷入在低 $\\tau$ 时变得显著的较差局部极小值中。初始状态为 $x_0$。\n\n- **冷启动优化**：相反，此策略直接尝试最小化最终的、高度非凸的目标函数 $E_{\\text{total}}(x;\\lambda, \\tau_{\\text{final}})$。它从相同的初始状态 $x_0$ 开始，并运行梯度下降，总迭代次数与退火计划中的总迭代次数相同（即 $S \\times T$，其中 $T$ 是温度阶段的数量）。这种方法容易陷入靠近初始点的局部极小值中。\n\n**3. 实现细节**\n\n提供的 Python 代码将实现这两种策略。\n首先，定义所有常量参数（$x^\\star$, $\\eta$, $k$, $\\alpha$, $S$, $\\tau_{\\text{final}}$）。预先计算观测值 $y$ 和偏置 $b_1$：\n- $y = x^\\star + \\eta = [2,-2]^\\top + [0.05,0.05]^\\top = [2.05, -1.95]^\\top$。\n- $b_1 = k\\pi\\tau_{\\text{final}} - W_1 x^\\star = [3,-3]^\\top \\pi (0.2) - I_2 [2,-2]^\\top \\approx [-0.115, 0.115]^\\top$。\n\n对于每个测试案例 $(\\lambda, \\{\\tau_t\\})$：\n1.  **退火运行**：初始化 $x_{\\text{anneal}} = x_0 = [0,0]^\\top$。遍历计划表中的每个 $\\tau$。在每个内循环中，运行 $S=300$ 次梯度下降迭代：$x_{\\text{anneal}} \\leftarrow x_{\\text{anneal}} - \\alpha \\nabla E_{\\text{total}}(x_{\\text{anneal}}; \\lambda, \\tau)$。存储最终的 $x_{\\text{anneal}}$。\n2.  **冷启动运行**：初始化 $x_{\\text{cold}} = x_0 = [0,0]^\\top$。温度固定为 $\\tau_{\\text{final}}$（计划表中的最后一个值）。总迭代次数为 $S \\times \\text{len(schedule)}$。在此总迭代次数下，在固定的冷温度下运行梯度下降。存储最终的 $x_{\\text{cold}}$。\n3.  **比较**：计算欧几里得距离 $D_{\\text{anneal}} = \\|x_{\\text{anneal}} - x^\\star\\|_2$ 和 $D_{\\text{cold}} = \\|x_{\\text{cold}} - x^\\star\\|_2$。改进量为 $I = D_{\\text{cold}} - D_{\\text{anneal}}$。\n\n对于案例 3，计划表为 $[0.2, 0.2, 0.2, 0.2]$。“退火”运行变成在相同温度 $\\tau=0.2$ 下的四个梯度下降块序列。这在功能上与冷启动运行（使用 $\\tau=0.2$ 进行 $4 \\times 300$ 次迭代）相同。因此，我们预期 $x_{\\text{anneal}} = x_{\\text{cold}}$ 且改进量 $I$ 为 $0$。这可作为对实现正确性的健全性检查。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the MAP estimation problem using cold start and annealed gradient descent,\n    and computes the improvement offered by annealing.\n    \"\"\"\n    # --- Define problem constants and parameters ---\n    # Dimension of the state space\n    n = 2\n    # Ground truth state vector\n    x_star = np.array([2.0, -2.0])\n    # Additive noise vector\n    eta = np.array([0.05, 0.05])\n    # Observation vector y = x_star + eta\n    y = x_star + eta\n    # Prior network weights W1 (Identity matrix)\n    W1 = np.identity(n)\n    # Final temperature for the prior\n    tau_final = 0.2\n    # Integer vector for bias calculation\n    k_vec = np.array([3.0, -3.0])\n    # Bias vector b1, chosen so g(x_star) at tau_final is zero\n    b1 = k_vec * np.pi * tau_final - W1 @ x_star\n    # Initial state for gradient descent\n    x0 = np.array([0.0, 0.0])\n    # Gradient descent step size (learning rate)\n    alpha = 0.1\n    # Number of iterations per temperature stage\n    S = 300\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        (5.0, [5.0, 1.0, 0.5, 0.2]),\n        (12.0, [5.0, 1.0, 0.5, 0.2]),\n        (5.0, [0.2, 0.2, 0.2, 0.2]),\n    ]\n\n    def grad_E_total(x, tau, lam):\n        \"\"\"\n        Computes the gradient of the total energy E_total.\n        This implementation uses the simplified form derived for A=I and W1=I.\n        \"\"\"\n        # Gradient of the data misfit term: x - y\n        grad_data = x - y\n        \n        # Argument for the trigonometric function in the prior's gradient\n        # Based on the derived formula: (tau/2) * sin(2 * (x + b1) / tau)\n        arg = 2.0 * (x + b1) / tau\n        \n        # Gradient of the prior term\n        grad_prior = lam * (tau / 2.0) * np.sin(arg)\n        \n        # Total gradient is the sum of the two parts\n        return grad_data + grad_prior\n\n    results = []\n    for lam, schedule in test_cases:\n        # --- ANNEALED OPTIMIZATION ---\n        x_anneal = np.copy(x0)\n        # Iterate through the temperature schedule\n        for tau in schedule:\n            # For each temperature, run S steps of gradient descent\n            for _ in range(S):\n                grad = grad_E_total(x_anneal, tau, lam)\n                x_anneal = x_anneal - alpha * grad\n        \n        # Calculate Euclidean distance to the ground truth\n        D_anneal = np.linalg.norm(x_anneal - x_star)\n\n        # --- COLD START OPTIMIZATION ---\n        x_cold = np.copy(x0)\n        # Use the final temperature from the schedule\n        tau_cold = schedule[-1]\n        # Total number of iterations is the same as for the annealed run\n        total_iterations = S * len(schedule)\n        \n        # Run gradient descent for the total number of iterations at the fixed cold temperature\n        for _ in range(total_iterations):\n            grad = grad_E_total(x_cold, tau_cold, lam)\n            x_cold = x_cold - alpha * grad\n            \n        # Calculate Euclidean distance to the ground truth\n        D_cold = np.linalg.norm(x_cold - x_star)\n\n        # --- COMPUTE IMPROVEMENT AND STORE RESULT ---\n        improvement = D_cold - D_anneal\n        results.append(improvement)\n\n    # Format the results to six decimal places for the final output\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    \n    # Final print statement in the exact required format: [i_1,i_2,i_3]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# Execute the solver function\nsolve()\n```"
        },
        {
            "introduction": "一个好的逆问题解不仅要精确，还必须是鲁棒的，即对数据的微小扰动不敏感。本练习将深入探讨使用深度神经网络先验所获得解的对抗脆弱性（adversarial vulnerability）。我们将推导并计算一个度量解对微小数据扰动敏感度的指标，并将其与经典的 Tikhonov 正则化方法进行比较，从而揭示深度先验独特的鲁棒性特征。这个分析对于在安全攸关的应用中评估和信任基于深度学习的重建至关重要 。",
            "id": "3375211",
            "problem": "考虑形如 $y = A x + \\eta$ 的线性逆问题，其中 $y \\in \\mathbb{R}^m$，$x \\in \\mathbb{R}^n$，$A \\in \\mathbb{R}^{m \\times n}$，以及加性噪声 $\\eta \\sim \\mathcal{N}(0, \\sigma^2 I_m)$。根据贝叶斯法则，后验密度满足 $p(x \\mid y) \\propto p(y \\mid x) \\, p(x)$，其中似然 $p(y \\mid x)$ 是高斯的。最大后验（MAP）估计量 $x^\\star(y)$ 是负对数后验的任意一个最小化子。我们研究对抗脆弱性，其定义为在有界扰动 $\\delta y$ 下，MAP 估计的最坏情况变化，即 $\\max_{\\lVert \\delta y \\rVert_2 \\le \\varepsilon} \\lVert x^\\star(y + \\delta y) - x^\\star(y) \\rVert_2$。比较两种先验下的这种敏感性：经典的 Tikhonov 先验和建模为基于能量的先验的深度神经网络（DNN）先验。主要目标是，从第一性原理出发，为这些敏感性推导出易于处理的表达式，并在一个测试集上进行数值计算。\n\n从以下基础出发：(i) 贝叶斯法则 $p(x \\mid y) \\propto p(y \\mid x) p(x)$，(ii) 高斯似然 $p(y \\mid x) \\propto \\exp\\!\\left(-\\frac{1}{2 \\sigma^2} \\lVert A x - y \\rVert_2^2 \\right)$，(iii) MAP 估计量 $x^\\star(y) \\in \\arg\\min_x \\Phi(x; y)$，其中 $\\Phi$ 是负对数后验，以及 (iv) 线性映射算子范数的奇异值分解（SVD）刻画。不要引用任何未经证明的快捷公式；从这些基础以及标准的一阶最优性原理和线性化原理出发，推导你使用的所有表达式。\n\n两种先验定义如下：\n- Tikhonov 先验：$p_{\\text{Tik}}(x) \\propto \\exp\\!\\left(-\\frac{\\beta}{2} \\lVert x \\rVert_2^2\\right)$，参数 $\\beta > 0$。\n- 深度神经网络先验：一个基于能量的模型 $p_\\phi(x) \\propto \\exp\\!\\left(- \\alpha \\, E_\\phi(x)\\right)$，其中 $\\alpha > 0$，能量为 $E_\\phi(x) = \\frac{1}{2} \\lVert x - g_\\phi(x) \\rVert_2^2$，而 $g_\\phi$ 是一个固定的双层前馈网络。函数 $g_\\phi$ 的构造为 $g_\\phi(x) = W_2 \\, \\tanh(W_1 x + b_1) + b_2$，其中 $\\tanh(\\cdot)$ 逐元素应用。对于任意 $x$，$g_\\phi$ 的雅可比矩阵为 $J_g(x) = W_2 \\, \\operatorname{diag}\\!\\left(1 - \\tanh^2(W_1 x + b_1)\\right) W_1$。\n\n任务：\n- 使用 Tikhonov 先验，推导 MAP 目标函数 $\\Phi_{\\text{Tik}}(x; y)$ 以及从 $y$ 到 $x^\\star(y)$ 的相应线性映射。根据优化和 SVD 的事实，推导出精确的最坏情况扰动 $\\delta y$（满足 $\\lVert \\delta y \\rVert_2 \\le \\varepsilon$）以及 MAP 估计中相应的最大偏移。你的推导应从一阶最优性条件开始，进而得到一个线性的 $y \\mapsto x^\\star(y)$ 算子，其谱范数控制着最坏情况的偏移。\n- 使用 DNN 先验，写出 MAP 目标函数 $\\Phi_\\phi(x; y)$。推导一阶最优性条件，并通过在 MAP 点应用局部二次近似（高斯-牛顿线性化），推导出从 $y$ 到 $x^\\star(y)$ 的线性化敏感性映射。使用隐函数定理（IFT）的逻辑，结合高斯-牛顿 Hessian 近似，获得一个易于处理的线性映射，其算子范数是最大偏移的上界。你必须通过忽略 $g_\\phi$ 的二阶导数来明确论证高斯-牛顿近似的合理性，这与标准的高斯-牛顿方法一致。在 Hessian 近似上引入一个小的 Tikhonov 阻尼 $\\gamma = 10^{-6}$ 以确保数值稳定性。\n- 实现一个程序，对下面的每个测试用例，计算：\n  1. Tikhonov 先验下的最大偏移量，记为 $S_{\\text{Tik}}$，对于指定的 $\\varepsilon$。\n  2. DNN 先验下的线性化最大偏移量，记为 $S_{\\text{DNN}}$，对于指定的 $\\varepsilon$。\n  3. 比率 $R = S_{\\text{DNN}} / S_{\\text{Tik}}$，约定如果 $S_{\\text{Tik}} = 0$，则定义 $R$ 为 $0$。\n  \n所有计算都是纯数值的；不涉及物理单位。$\\tanh(\\cdot)$ 中适用的角度是无单位的。最终输出必须是单行，包含一个用方括号括起来的逗号分隔列表，按 $[S_{\\text{Tik}}^{(1)}, S_{\\text{DNN}}^{(1)}, R^{(1)}, S_{\\text{Tik}}^{(2)}, S_{\\text{DNN}}^{(2)}, R^{(2)}, \\dots]$ 的顺序排列所有测试用例的结果。\n\n神经网络 $g_\\phi$ 的参数：\n- 维度：$n = 5$，隐藏层宽度 $h = 3$。\n- $W_1 = \\begin{bmatrix}\n0.5  -0.3  0.1  0.0  0.2 \\\\\n0.0  0.4  -0.2  0.3  0.1 \\\\\n-0.1  0.0  0.2  -0.4  0.3\n\\end{bmatrix}$，$b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix}$，\n- $W_2 = \\begin{bmatrix}\n0.2  -0.1  0.0 \\\\\n0.0  0.3  -0.2 \\\\\n-0.1  0.0  0.25 \\\\\n0.0  -0.2  0.1 \\\\\n0.15  0.05  -0.05\n\\end{bmatrix}$，$b_2 = \\begin{bmatrix} 0.0 \\\\ 0.05 \\\\ -0.03 \\\\ 0.02 \\\\ -0.01 \\end{bmatrix}$。\n\n测试集：\n- 用例 $1$：$m = 5$，$n = 5$，\n  $A = \\begin{bmatrix}\n1.0  0.2  0.0  0.0  0.0 \\\\\n0.0  1.0  0.3  0.0  0.0 \\\\\n0.0  0.0  1.0  0.4  0.0 \\\\\n0.0  0.0  0.0  1.0  0.5 \\\\\n0.1  0.0  0.0  0.0  1.0\n\\end{bmatrix}$，$y = \\begin{bmatrix} 1.0 \\\\ -0.5 \\\\ 0.3 \\\\ 0.2 \\\\ -0.1 \\end{bmatrix}$，$\\sigma = 0.05$，$\\beta = 0.5$，$\\alpha = 0.3$，$\\varepsilon = 0.01$。\n- 用例 $2$：$m = 5$，$n = 5$，\n  $A = \\begin{bmatrix}\n1.0  0.99  0.98  0.0  0.0 \\\\\n0.99  0.98  0.97  0.0  0.0 \\\\\n0.98  0.97  0.96  0.0  0.0 \\\\\n0.0  0.0  0.0  1.0  0.99 \\\\\n0.0  0.0  0.0  0.99  0.98\n\\end{bmatrix}$，$y = \\begin{bmatrix} -0.2 \\\\ 0.1 \\\\ -0.1 \\\\ 0.05 \\\\ -0.05 \\end{bmatrix}$，$\\sigma = 0.05$，$\\beta = 0.5$，$\\alpha = 1.0$，$\\varepsilon = 0.01$。\n- 用例 $3$：$m = 5$，$n = 5$，\n  $A$ 与用例 $2$ 相同，$y = \\begin{bmatrix} 1.0 \\\\ -0.5 \\\\ 0.3 \\\\ 0.2 \\\\ -0.1 \\end{bmatrix}$，$\\sigma = 0.05$，$\\beta = 0.1$，$\\alpha = 0.1$，$\\varepsilon = 0.05$。\n- 用例 $4$：$m = 5$，$n = 5$，\n  $A$ 与用例 $1$ 相同，$y = \\begin{bmatrix} -0.2 \\\\ 0.1 \\\\ -0.1 \\\\ 0.05 \\\\ -0.05 \\end{bmatrix}$，$\\sigma = 0.05$，$\\beta = 2.0$，$\\alpha = 5.0$，$\\varepsilon = 0.0$。\n\n实现要求：\n- 对于 Tikhonov 先验，推导并使用源于一阶最优性条件的从 $y$ 到 $x^\\star(y)$ 的精确线性映射，并计算其诱导的算子范数以获得最坏情况的偏移。\n- 对于 DNN 先验，首先通过使用高斯-牛顿 Hessian 近似 $H(x) \\approx \\frac{1}{\\sigma^2} A^\\top A + \\alpha \\left(I - J_g(x)\\right)^\\top \\left(I - J_g(x)\\right) + \\gamma I$ 的阻尼牛顿法，求解 MAP 估计的一阶最优性条件来计算 $x^\\star(y)$，其中 $\\gamma = 10^{-6}$。然后，使用在 $x^\\star(y)$ 处的隐函数线性化，计算从 $y$ 到 $x^\\star(y)$ 的线性化敏感性映射，并取其算子范数来估计最坏情况的偏移；最后乘以 $\\varepsilon$ 得到 $S_{\\text{DNN}}$。\n- 数值容差：使用基于牛顿步长范数小于 $10^{-10}$ 或最大迭代次数 $200$ 次的停止准则。\n- 最终输出格式必须是单行，包含一个用方括号括起来的逗号分隔列表，结果按 $[S_{\\text{Tik}}^{(1)}, S_{\\text{DNN}}^{(1)}, R^{(1)}, S_{\\text{Tik}}^{(2)}, S_{\\text{DNN}}^{(2)}, R^{(2)}, S_{\\text{Tik}}^{(3)}, S_{\\text{DNN}}^{(3)}, R^{(3)}, S_{\\text{Tik}}^{(4)}, S_{\\text{DNN}}^{(4)}, R^{(4)}]$ 的顺序排列。",
            "solution": "我们首先重申基本的贝叶斯公式。对于 $y = A x + \\eta$ 和 $\\eta \\sim \\mathcal{N}(0, \\sigma^2 I_m)$，似然为 $p(y \\mid x) \\propto \\exp\\!\\left(-\\frac{1}{2 \\sigma^2} \\lVert A x - y \\rVert_2^2 \\right)$。一个先验密度 $p(x)$ 产生后验 $p(x \\mid y) \\propto p(y \\mid x) p(x)$，而最大后验（MAP）估计量 $x^\\star(y)$ 解决优化问题 $x^\\star(y) \\in \\arg\\min_x \\Phi(x; y)$，其中 $\\Phi(x; y) = - \\log p(y \\mid x) - \\log p(x) + \\text{const}(y)$。\n\n经典的 Tikhonov 先验。Tikhonov 先验为 $p_{\\text{Tik}}(x) \\propto \\exp\\!\\left(-\\frac{\\beta}{2} \\lVert x \\rVert_2^2\\right)$，其中 $\\beta > 0$。负对数后验是\n$$\n\\Phi_{\\text{Tik}}(x; y) = \\frac{1}{2 \\sigma^2} \\lVert A x - y \\rVert_2^2 + \\frac{\\beta}{2} \\lVert x \\rVert_2^2.\n$$\n一阶最优性条件是\n$$\n\\nabla_x \\Phi_{\\text{Tik}}(x; y) = \\frac{1}{\\sigma^2} A^\\top (A x - y) + \\beta x = 0.\n$$\n整理后得到一个线性系统\n$$\n\\left( \\frac{1}{\\sigma^2} A^\\top A + \\beta I \\right) x = \\frac{1}{\\sigma^2} A^\\top y.\n$$\n因此，MAP 估计量是数据的线性函数，\n$$\nx^\\star_{\\text{Tik}}(y) = M_{\\text{Tik}} \\, y, \\quad \\text{其中} \\quad M_{\\text{Tik}} = \\left( \\frac{1}{\\sigma^2} A^\\top A + \\beta I \\right)^{-1} \\frac{1}{\\sigma^2} A^\\top.\n$$\n对于满足 $\\lVert \\delta y \\rVert_2 \\le \\varepsilon$ 的任意扰动 $\\delta y$，MAP 估计中引起的变化为\n$$\n\\delta x^\\star_{\\text{Tik}} = M_{\\text{Tik}} \\, \\delta y,\n$$\n因此最坏情况下的偏移量为\n$$\n\\max_{\\lVert \\delta y \\rVert_2 \\le \\varepsilon} \\lVert M_{\\text{Tik}} \\, \\delta y \\rVert_2 = \\varepsilon \\, \\lVert M_{\\text{Tik}} \\rVert_2,\n$$\n其中 $\\lVert \\cdot \\rVert_2$ 是算子范数（最大奇异值）。$\\delta y$ 的最坏情况方向是与 $M_{\\text{Tik}}$ 的最大奇异值相关联的右奇异向量。这源于谱范数的奇异值分解（SVD）刻画。\n\n深度神经网络先验。考虑一个基于能量的先验，其密度为 $p_\\phi(x) \\propto \\exp\\!\\left(- \\alpha E_\\phi(x) \\right)$，$\\alpha > 0$，能量为\n$$\nE_\\phi(x) = \\frac{1}{2} \\lVert x - g_\\phi(x) \\rVert_2^2,\n$$\n其中 $g_\\phi(x) = W_2 \\, \\tanh(W_1 x + b_1) + b_2$。负对数后验是\n$$\n\\Phi_\\phi(x; y) = \\frac{1}{2 \\sigma^2} \\lVert A x - y \\rVert_2^2 + \\alpha \\, E_\\phi(x).\n$$\n其梯度为\n$$\n\\nabla_x \\Phi_\\phi(x; y) = \\frac{1}{\\sigma^2} A^\\top (A x - y) + \\alpha \\, \\nabla_x E_\\phi(x).\n$$\n令 $r(x) = x - g_\\phi(x)$。那么 $E_\\phi(x) = \\frac{1}{2} \\lVert r(x) \\rVert_2^2$，并且\n$$\n\\nabla_x E_\\phi(x) = J_r(x)^\\top r(x) = \\left(I - J_g(x)\\right)^\\top \\left(x - g_\\phi(x)\\right),\n$$\n其中 $J_g(x)$ 是 $g_\\phi$ 在 $x$ 处的雅可比矩阵，而 $J_r(x) = I - J_g(x)$。$E_\\phi$ 的精确 Hessian 矩阵是\n$$\n\\nabla_x^2 E_\\phi(x) = J_r(x)^\\top J_r(x) + \\sum_{i=1}^n r_i(x) \\, \\nabla_x^2 r_i(x),\n$$\n但高斯-牛顿（GN）近似忽略了第二项，得到\n$$\n\\nabla_x^2 E_\\phi(x) \\approx J_r(x)^\\top J_r(x) = \\left(I - J_g(x)\\right)^\\top \\left(I - J_g(x)\\right).\n$$\n当残差适中时，这种近似是标准的，并提供了一个半正定的曲率模型。在一个满足 $\\nabla_x \\Phi_\\phi(x^\\star_\\phi(y); y) = 0$ 的 MAP 点 $x^\\star_\\phi(y)$，负对数后验的高斯-牛顿 Hessian 近似为\n$$\nH(x^\\star_\\phi(y)) \\approx \\frac{1}{\\sigma^2} A^\\top A + \\alpha \\left(I - J_g(x^\\star_\\phi(y))\\right)^\\top \\left(I - J_g(x^\\star_\\phi(y))\\right).\n$$\n为确保数值稳定性，我们添加 Tikhonov 阻尼 $\\gamma I$，其中 $\\gamma = 10^{-6}$：\n$$\n\\widetilde{H}(x^\\star_\\phi(y)) = H(x^\\star_\\phi(y)) + \\gamma I.\n$$\n为了获得 $x^\\star_\\phi(y)$ 对 $y$ 的敏感性，我们使用隐函数定理（IFT）对一阶最优性条件关于 $y$ 求导：\n$$\n\\nabla_x^2 \\Phi_\\phi(x^\\star_\\phi(y); y) \\, \\frac{\\partial x^\\star_\\phi}{\\partial y}(y) + \\frac{\\partial}{\\partial y} \\nabla_x \\Phi_\\phi(x^\\star_\\phi(y); y) = 0.\n$$\n由于 $\\frac{\\partial}{\\partial y} \\left( \\frac{1}{\\sigma^2} A^\\top (A x^\\star_\\phi(y) - y) \\right) = - \\frac{1}{\\sigma^2} A^\\top$ 且先验项仅依赖于 $x$，线性化的敏感性映射为\n$$\n\\frac{\\partial x^\\star_\\phi}{\\partial y}(y) \\approx \\widetilde{H}(x^\\star_\\phi(y))^{-1} \\frac{1}{\\sigma^2} A^\\top.\n$$\n对于满足 $\\lVert \\delta y \\rVert_2 \\le \\varepsilon$ 的小扰动 $\\delta y$，最坏情况的线性化偏移为\n$$\n\\max_{\\lVert \\delta y \\rVert_2 \\le \\varepsilon} \\left\\lVert \\frac{\\partial x^\\star_\\phi}{\\partial y}(y) \\, \\delta y \\right\\rVert_2 \\approx \\varepsilon \\left\\lVert \\widetilde{H}(x^\\star_\\phi(y))^{-1} \\frac{1}{\\sigma^2} A^\\top \\right\\rVert_2.\n$$\n这提供了一个有原则且易于处理的机制，用以通过高斯-牛顿线性化和隐函数定理评估深度神经网络先验下的对抗脆弱性。\n\n算法实现。对于 Tikhonov 先验，计算\n$$\nM_{\\text{Tik}} = \\left( \\frac{1}{\\sigma^2} A^\\top A + \\beta I \\right)^{-1} \\frac{1}{\\sigma^2} A^\\top,\n$$\n并设置 $S_{\\text{Tik}} = \\varepsilon \\, \\lVert M_{\\text{Tik}} \\rVert_2$。对于深度神经网络先验，首先通过使用阻尼牛顿法求解 $\\nabla_x \\Phi_\\phi(x; y) = 0$ 来计算 $x^\\star_\\phi(y)$，其中使用高斯-牛顿 Hessian $\\widetilde{H}(x)$。具体来说，迭代\n$$\n\\widetilde{H}(x_k) \\, \\Delta x_k = - \\nabla_x \\Phi_\\phi(x_k; y), \\quad x_{k+1} = x_k + \\Delta x_k,\n$$\n直到 $\\lVert \\Delta x_k \\rVert_2  10^{-10}$ 或达到最大迭代次数 $200$ 次。在收敛的 $x^\\star_\\phi(y)$ 处，构造\n$$\nM_{\\text{DNN}}(y) \\approx \\widetilde{H}(x^\\star_\\phi(y))^{-1} \\frac{1}{\\sigma^2} A^\\top,\n$$\n并设置 $S_{\\text{DNN}} = \\varepsilon \\, \\lVert M_{\\text{DNN}}(y) \\rVert_2$。最后，定义 $R = S_{\\text{DNN}} / S_{\\text{Tik}}$；如果 $S_{\\text{Tik}} = 0$，则按约定返回 $R = 0$ 以避免未定义的值。\n\n测试集规格。矩阵 $A$、向量 $y$、标量 $\\sigma$、$\\beta$、$\\alpha$ 和 $\\varepsilon$ 在问题陈述中以数值形式给出。深度神经网络先验由数值矩阵 $W_1$、$W_2$ 和向量 $b_1$、$b_2$ 指定，其中 $g_\\phi(x) = W_2 \\, \\tanh(W_1 x + b_1) + b_2$ 且 $J_g(x) = W_2 \\, \\operatorname{diag}(1 - \\tanh^2(W_1 x + b_1)) W_1$。对于每个用例，我们计算 $S_{\\text{Tik}}$、$S_{\\text{DNN}}$ 和 $R$，并按指定顺序将结果报告为单行列表。由于用例 4 的 $\\varepsilon = 0$，因此 $S_{\\text{Tik}}$ 和 $S_{\\text{DNN}}$ 均为 $0$，根据定义约定，$R=0$。\n\n本解决方案整合了统计逆问题原理、最优化理论、高斯-牛顿近似和隐函数定理，推导了敏感性算子，其谱范数决定了经典先验和深度神经网络先验下的最坏情况对抗脆弱性，并随后进行了相符的数值实现。",
            "answer": "```python\nimport numpy as np\n\n# Deep Neural Network prior specification: g_phi(x) = W2 * tanh(W1 x + b1) + b2\ndef g_phi(x, W1, b1, W2, b2):\n    z1 = W1 @ x + b1\n    h = np.tanh(z1)\n    return W2 @ h + b2\n\ndef J_g_phi(x, W1, b1, W2):\n    z1 = W1 @ x + b1\n    dh = 1.0 - np.tanh(z1) ** 2  # elementwise derivative of tanh\n    # J_g = W2 * diag(dh) * W1\n    return W2 @ (np.diag(dh) @ W1)\n\ndef map_tikhonov_operator(A, sigma, beta):\n    # M_Tik = (A^T A / sigma^2 + beta I)^(-1) * (A^T / sigma^2)\n    n = A.shape[1]\n    H = (A.T @ A) / (sigma**2) + beta * np.eye(n)\n    M = np.linalg.solve(H, A.T / (sigma**2))\n    return M\n\ndef newton_map_dnn_xstar(A, y, sigma, alpha, W1, b1, W2, b2, gamma=1e-6, tol=1e-10, maxit=200):\n    n = A.shape[1]\n    x = np.zeros(n)\n    AT = A.T\n    ATA_over_sigma2 = (AT @ A) / (sigma**2)\n    for _ in range(maxit):\n        # Compute g, Jg\n        g = g_phi(x, W1, b1, W2, b2)\n        Jg = J_g_phi(x, W1, b1, W2)\n        # Residual r = x - g(x)\n        r = x - g\n        # Gradient: A^T(Ax - y)/sigma^2 + alpha * (I - Jg)^T * r\n        data_grad = AT @ (A @ x - y) / (sigma**2)\n        I_minus_Jg = np.eye(n) - Jg\n        prior_grad = alpha * (I_minus_Jg.T @ r)\n        grad = data_grad + prior_grad\n        # Gauss-Newton Hessian approx: A^T A / sigma^2 + alpha * (I - Jg)^T (I - Jg) + gamma I\n        H_gn = ATA_over_sigma2 + alpha * (I_minus_Jg.T @ I_minus_Jg) + gamma * np.eye(n)\n        try:\n            dx = np.linalg.solve(H_gn, -grad)\n        except np.linalg.LinAlgError:\n            # Fallback to least-squares solve if ill-conditioned\n            dx = np.linalg.lstsq(H_gn, -grad, rcond=None)[0]\n        x = x + dx\n        if np.linalg.norm(dx)  tol:\n            break\n    return x\n\ndef dnn_sensitivity_operator(A, y, sigma, alpha, W1, b1, W2, b2, gamma=1e-6):\n    # Compute x* via Newton-GN\n    x_star = newton_map_dnn_xstar(A, y, sigma, alpha, W1, b1, W2, b2, gamma=gamma)\n    # Build GN Hessian at x*\n    n = A.shape[1]\n    AT = A.T\n    ATA_over_sigma2 = (AT @ A) / (sigma**2)\n    Jg = J_g_phi(x_star, W1, b1, W2)\n    I_minus_Jg = np.eye(n) - Jg\n    H_gn = ATA_over_sigma2 + alpha * (I_minus_Jg.T @ I_minus_Jg) + gamma * np.eye(n)\n    # Sensitivity M_DNN = H_gn^{-1} * (A^T / sigma^2)\n    try:\n        M = np.linalg.solve(H_gn, AT / (sigma**2))\n    except np.linalg.LinAlgError:\n        M = np.linalg.lstsq(H_gn, AT / (sigma**2), rcond=None)[0]\n    return M\n\ndef spectral_norm(M):\n    # Largest singular value\n    s = np.linalg.svd(M, compute_uv=False)\n    if len(s) == 0:\n        return 0.0\n    return float(s[0])\n\ndef solve():\n    # Network parameters\n    W1 = np.array([\n        [0.5, -0.3, 0.1, 0.0, 0.2],\n        [0.0,  0.4, -0.2, 0.3, 0.1],\n        [-0.1, 0.0, 0.2, -0.4, 0.3]\n    ], dtype=float)\n    b1 = np.array([0.1, -0.2, 0.05], dtype=float)\n    W2 = np.array([\n        [0.2, -0.1,  0.0],\n        [0.0,  0.3, -0.2],\n        [-0.1, 0.0,  0.25],\n        [0.0, -0.2,  0.1],\n        [0.15, 0.05, -0.05]\n    ], dtype=float)\n    b2 = np.array([0.0, 0.05, -0.03, 0.02, -0.01], dtype=float)\n\n    # Test cases\n    A1 = np.array([\n        [1.0, 0.2, 0.0, 0.0, 0.0],\n        [0.0, 1.0, 0.3, 0.0, 0.0],\n        [0.0, 0.0, 1.0, 0.4, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.5],\n        [0.1, 0.0, 0.0, 0.0, 1.0]\n    ], dtype=float)\n    A2 = np.array([\n        [1.0, 0.99, 0.98, 0.0, 0.0],\n        [0.99, 0.98, 0.97, 0.0, 0.0],\n        [0.98, 0.97, 0.96, 0.0, 0.0],\n        [0.0, 0.0, 0.0, 1.0, 0.99],\n        [0.0, 0.0, 0.0, 0.99, 0.98]\n    ], dtype=float)\n    y1 = np.array([1.0, -0.5, 0.3, 0.2, -0.1], dtype=float)\n    y2 = np.array([-0.2, 0.1, -0.1, 0.05, -0.05], dtype=float)\n    sigma = 0.05\n\n    test_cases = [\n        # (A, y, beta, alpha, epsilon)\n        (A1, y1, 0.5, 0.3, 0.01),\n        (A2, y2, 0.5, 1.0, 0.01),\n        (A2, y1, 0.1, 0.1, 0.05),\n        (A1, y2, 2.0, 5.0, 0.0),\n    ]\n\n    results = []\n    for A, y, beta, alpha, eps in test_cases:\n        # Tikhonov operator and sensitivity\n        M_tik = map_tikhonov_operator(A, sigma, beta)\n        s_tik = eps * spectral_norm(M_tik)\n\n        # DNN linearized sensitivity at MAP (GN-IFT)\n        M_dnn = dnn_sensitivity_operator(A, y, sigma, alpha, W1, b1, W2, b2, gamma=1e-6)\n        s_dnn = eps * spectral_norm(M_dnn)\n\n        # Ratio with convention: if s_tik == 0, return 0\n        R = 0.0 if np.isclose(s_tik, 0.0) else (s_dnn / s_tik)\n\n        results.extend([s_tik, s_dnn, R])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}