{
    "hands_on_practices": [
        {
            "introduction": "在解决逆问题时，第一步是建立一个“正演模型”，它能根据系统参数预测观测结果。本练习将通过一个简单的图上电路系统，演示如何推导该模型及其雅可比矩阵。该导数是使用诸如图神经网络（GNN）等基于梯度的方法的关键，因为它将模型输出的微小变化与模型参数的微小变化联系起来。",
            "id": "3386851",
            "problem": "考虑一个一维（$1$D）链式图，其节点索引为 $i \\in \\{0,1,\\dots,N\\}$，边为 $(i,i+1)$，$i \\in \\{0,1,\\dots,N-1\\}$。每条边 $(i,i+1)$ 都有一个严格为正的电导 $w_{i,i+1} \\in \\mathbb{R}_{0}$。定义边界（狄利克雷）电势为 $v_0 = V_L$ 和 $v_N = V_R$，其中 $V_L, V_R \\in \\mathbb{R}$ 是固定且已知的。内部节点电压 $\\{v_i\\}_{i=1}^{N-1}$ 由图上的基尔霍夫电流定律和欧姆定律确定。设输入参数化为对数电导 $x \\in \\mathbb{R}^N$，其分量为 $x_i = \\ln w_{i,i+1}$，$i \\in \\{0,1,\\dots,N-1\\}$，并定义输出 $y \\in \\mathbb{R}^2$ 为边界电流向量 $y = [I_0, I_N]^{\\top}$，其中 $I_0$ 是在节点 $0$ 注入的净电流（流入网络为正），$I_N$ 是在节点 $N$ 注入的净电流（流入网络为正）。这就定义了一个正向映射 $y = H(x)$，即该链的图论狄利克雷-诺伊曼映射。\n\n您正在设计一个图神经网络（GNN）代理模型来模拟 $H(x)$，作为数据同化中逆问题的一部分，其中需要关于 $x$ 的梯度用于反向传播和高斯-牛顿正则化反演。仅从边上的欧姆定律、节点上的基尔霍夫电流定律以及对数参数化所强制的正性这些核心定义出发，显式地推导出离散正向映射 $y = H(x)$（用 $\\{w_{i,i+1}\\}$ 和 $\\{V_L,V_R\\}$ 表示），然后以闭式形式计算在任意 $x$ 处的雅可比矩阵 $\\frac{\\partial y}{\\partial x} \\in \\mathbb{R}^{2 \\times N}$。您的最终雅可比矩阵仅能使用 $\\{w_{i,i+1}\\}_{i=0}^{N-1}$、$V_L$ 和 $V_R$ 来表示。\n\n请给出您最终的雅可比矩阵的单一闭式解析表达式。不要近似。无需单位。不涉及角度。最终答案必须仅为所求的表达式。",
            "solution": "首先根据所需标准对问题陈述进行验证。\n\n### 问题验证\n\n#### 第一步：提取已知条件\n-   **图结构：** 一维（$1$D）链式图，节点索引为 $i \\in \\{0,1,\\dots,N\\}$，边为 $(i,i+1)$，$i \\in \\{0,1,\\dots,N-1\\}$。\n-   **边属性：** 每条边 $(i,i+1)$ 具有严格为正的电导 $w_{i,i+1} \\in \\mathbb{R}_{0}$。\n-   **边界条件：** 狄利克雷边界电势指定为 $v_0 = V_L$ 和 $v_N = V_R$，其中 $V_L, V_R \\in \\mathbb{R}$ 是固定且已知的。\n-   **内部节点物理特性：** 内部节点电压 $\\{v_i\\}_{i=1}^{N-1}$ 由基尔霍夫电流定律和欧姆定律决定。\n-   **输入参数化：** 输入参数是对数电导 $x \\in \\mathbb{R}^N$，其分量为 $x_i = \\ln w_{i,i+1}$，$i \\in \\{0,1,\\dots,N-1\\}$。\n-   **输出定义：** 输出是边界电流向量 $y = [I_0, I_N]^{\\top} \\in \\mathbb{R}^2$。\n-   **电流定义：** $I_0$ 是在节点 $0$ 注入的净电流（流入网络为正），$I_N$ 是在节点 $N$ 注入的净电流（流入网络为正）。\n-   **正向映射：** 输入和输出之间的关系由映射 $y = H(x)$ 表示。\n-   **目标：**\n    1.  用 $\\{w_{i,i+1}\\}$ 和 $\\{V_L, V_R\\}$ 显式推导出正向映射 $y = H(x)$。\n    2.  以闭式形式计算雅可比矩阵 $\\frac{\\partial y}{\\partial x} \\in \\mathbb{R}^{2 \\times N}$。\n    3.  最终的雅可比矩阵仅用 $\\{w_{i,i+1}\\}_{i=0}^{N-1}$、$V_L$ 和 $V_R$ 表示。\n\n#### 第二步：使用提取的已知条件进行验证\n-   **科学依据：** 该问题基于电路的基本原理，即欧姆定律和基尔霍夫电流定律。这些是公认的物理定律。对数参数化是优化和逆问题中强制正性约束的标准有效技术。该问题具有科学合理性。\n-   **适定性：** 该问题描述了一个具有固定狄利克雷边界条件的线性电阻网络。由于所有电导 $w_{i,i+1}$ 均为严格正值，电阻 $1/w_{i,i+1}$ 是有限正值。这保证了内部节点电势的线性方程组有唯一解。因此，正向映射 $H(x)$ 是适定的，其雅可比矩阵是可计算的。该问题是适定的。\n-   **目标明确性：** 问题使用精确的数学语言陈述，所有术语都有明确定义。它没有主观或模糊的陈述。\n\n该问题没有违反任何无效性标准。它是数学物理学中的一个标准的、适定的问题，并且与数据同化和机器学习中的计算方法直接相关。\n\n#### 第三步：结论与行动\n问题有效。将继续进行求解过程。\n\n### 求解推导\n\n求解过程分为两部分：首先，确定正向映射 $y=H(x)$；其次，计算其雅可比矩阵 $\\frac{\\partial y}{\\partial x}$。\n\n#### 第一部分：正向映射 $y = H(x)$ 的推导\n\n该电气网络是一个简单的一维串联电路。根据欧姆定律，从节点 $i$ 流向节点 $i+1$ 的电流为 $I_{i \\to i+1} = w_{i,i+1} (v_i - v_{i+1})$，其中 $v_i$ 是节点 $i$ 的电压，$w_{i,i+1}$ 是边 $(i, i+1)$ 的电导。\n\n对于一个仅在边界（节点 $0$ 和 $N$）有源的一维链，根据基尔霍夫电流定律，在任何内部节点 $j \\in \\{1, \\dots, N-1\\}$，流入的电流等于流出的电流。这意味着整个链中的电流是恒定的。设此恒定电流为 $I_{\\text{chain}}$。\n$$ I_{\\text{chain}} = I_{0 \\to 1} = I_{1 \\to 2} = \\dots = I_{N-1 \\to N} $$\n整个链上的总电压降为 $v_0 - v_N = V_L - V_R$。\n单条边 $(i, i+1)$ 的电阻是其电導的倒数：$R_{i,i+1} = 1/w_{i,i+1}$。\n串联电路的总电阻 $R_{\\text{total}}$ 是各个电阻之和：\n$$ R_{\\text{total}} = \\sum_{k=0}^{N-1} R_{k,k+1} = \\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}} $$\n现在可以使用欧姆定律对整个电路计算出恒定的链电流 $I_{\\text{chain}}$：\n$$ I_{\\text{chain}} = \\frac{V_L - V_R}{R_{\\text{total}}} = \\frac{V_L - V_R}{\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}} $$\n输出向量 $y = [I_0, I_N]^{\\top}$ 由边界电流组成。\n-   $I_0$ 是在节点 $0$ 注入的净电流。此电流必须流入链中，因此它等于从节点 $0$ 流向节点 $1$ 的电流：$I_0 = I_{0 \\to 1} = I_{\\text{chain}}$。\n-   $I_N$ 是在节点 $N$ 注入的净电流。从链中流入节点 $N$ 的电流是 $I_{N-1 \\to N} = I_{\\text{chain}}$。根据“流入网络为正”的约定，$I_N$ 必须是在节点 $N$ 流出网络的电流的负值。流出网络的电流是 $I_{\\text{chain}}$，所以 $I_N = -I_{\\text{chain}}$。\n\n因此，正向映射为：\n$$ y = H(x) = \\begin{pmatrix} I_0 \\\\ I_N \\end{pmatrix} = \\begin{pmatrix} I_{\\text{chain}} \\\\ -I_{\\text{chain}} \\end{pmatrix} = \\frac{V_L - V_R}{\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n该表达式用电导 $\\{w_{i,i+1}\\}$ 表示输出 $y$。\n\n#### 第二部分：雅可比矩阵 $\\frac{\\partial y}{\\partial x}$ 的计算\n\n$y$ 关于 $x$ 的雅可比矩阵是一个 $2 \\times N$ 矩阵，其元素为 $\\frac{\\partial y_m}{\\partial x_j}$，其中 $y_m$ 是 $y$ 的第 $m$ 个分量（$m \\in \\{1,2\\}$），$x_j$ 是 $x$ 的第 $j$ 个分量（$j \\in \\{0, \\dots, N-1\\}$）。分量为 $y_1 = I_0$ 和 $y_2 = I_N$。\n\n已知 $I_N = -I_0$，雅可比矩阵的第二行就是第一行的负值：$\\frac{\\partial I_N}{\\partial x_j} = -\\frac{\\partial I_0}{\\partial x_j}$。我们只需要计算第一行，即导数 $\\frac{\\partial I_0}{\\partial x_j}$。\n\n我们使用链式法则来求 $I_0$ 关于 $x_j$ 的导数：\n$$ \\frac{\\partial I_0}{\\partial x_j} = \\frac{\\partial I_0}{\\partial w_{j,j+1}} \\frac{\\partial w_{j,j+1}}{\\partial x_j} $$\n问题定义了参数化 $x_j = \\ln w_{j,j+1}$，这意味着 $w_{j,j+1} = \\exp(x_j)$。因此，导数为：\n$$ \\frac{\\partial w_{j,j+1}}{\\partial x_j} = \\exp(x_j) = w_{j,j+1} $$\n接下来，我们计算 $I_0$ 关于任意电导 $w_{j,j+1}$ 的导数。令 $S = \\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}$。那么 $I_0 = (V_L - V_R)S^{-1}$。\n$$ \\frac{\\partial I_0}{\\partial w_{j,j+1}} = (V_L - V_R) \\frac{d(S^{-1})}{dS} \\frac{\\partial S}{\\partial w_{j,j+1}} $$\n导数分别为：\n$$ \\frac{d(S^{-1})}{dS} = -S^{-2} $$\n$$ \\frac{\\partial S}{\\partial w_{j,j+1}} = \\frac{\\partial}{\\partial w_{j,j+1}} \\left( \\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}} \\right) = -\\frac{1}{w_{j,j+1}^2} $$\n结合这些结果：\n$$ \\frac{\\partial I_0}{\\partial w_{j,j+1}} = (V_L - V_R) (-S^{-2}) \\left( -\\frac{1}{w_{j,j+1}^2} \\right) = \\frac{V_L - V_R}{S^2 w_{j,j+1}^2} = \\frac{V_L - V_R}{\\left(\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}\\right)^2 w_{j,j+1}^2} $$\n现在我们可以求出 $\\frac{\\partial I_0}{\\partial x_j}$：\n$$ \\frac{\\partial I_0}{\\partial x_j} = \\frac{\\partial I_0}{\\partial w_{j,j+1}} \\frac{\\partial w_{j,j+1}}{\\partial x_j} = \\left( \\frac{V_L - V_R}{\\left(\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}\\right)^2 w_{j,j+1}^2} \\right) w_{j,j+1} = \\frac{V_L - V_R}{w_{j,j+1} \\left(\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}\\right)^2} $$\n该表达式是雅可比矩阵的第 $(1, j+1)$ 个元素（使用从 $0$ 到 $N-1$ 的 $j$ 来索引列）。然后通过组合 $j=0, 1, \\dots, N-1$ 的这些分量来构建完整的雅可比矩阵 $\\frac{\\partial y}{\\partial x}$：\n$$ \\frac{\\partial y}{\\partial x} = \\begin{pmatrix} \\frac{\\partial I_0}{\\partial x_0}  \\frac{\\partial I_0}{\\partial x_1}  \\cdots  \\frac{\\partial I_0}{\\partial x_{N-1}} \\\\ \\frac{\\partial I_N}{\\partial x_0}  \\frac{\\partial I_N}{\\partial x_1}  \\cdots  \\frac{\\partial I_N}{\\partial x_{N-1}} \\end{pmatrix} $$\n代入推导出的表达式：\n$$ \\frac{\\partial y}{\\partial x} = \\frac{V_L - V_R}{\\left(\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}\\right)^2} \\begin{pmatrix} \\frac{1}{w_{0,1}}  \\frac{1}{w_{1,2}}  \\cdots  \\frac{1}{w_{N-1,N}} \\\\ -\\frac{1}{w_{0,1}}  -\\frac{1}{w_{1,2}}  \\cdots  -\\frac{1}{w_{N-1,N}} \\end{pmatrix} $$\n这就是雅可比矩阵的最终闭式表达式，用所需的变量表示。",
            "answer": "$$ \\boxed{\\frac{V_L - V_R}{\\left(\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}\\right)^2} \\begin{pmatrix} \\frac{1}{w_{0,1}}  \\frac{1}{w_{1,2}}  \\cdots  \\frac{1}{w_{N-1,N}} \\\\ -\\frac{1}{w_{0,1}}  -\\frac{1}{w_{1,2}}  \\cdots  -\\frac{1}{w_{N-1,N}} \\end{pmatrix}} $$"
        },
        {
            "introduction": "许多用于逆问题的图神经网络（GNN）的设计灵感来自于“展开”经典迭代算法。为了使这些深度模型稳定有效，其架构必须满足特定的数学条件。本练习通过分析迭代算子的谱半径，探讨了这种展开网络的收敛性，从而确保学习到的求解器不会发散。",
            "id": "3386845",
            "problem": "在一个用于图上线性逆问题的学习型展开图神经网络 (GNN) 中，考虑数据保真函数 $f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{H}\\mathbf{x} - \\mathbf{y}\\|_{2}^{2}$，其中 $\\mathbf{H} \\in \\mathbb{R}^{n \\times n}$ 是对称正定矩阵。该展开架构的一层实现仿射迭代 $\\mathbf{x}_{k+1} = (\\mathbf{I} - \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{H})\\mathbf{x}_{k} + \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{y}$，其中 $\\alpha  0$ 是一个恒定的步长。仅使用实对称矩阵的谱定理、谱半径的定义以及图拉普拉斯算子的基本性质，推导关于 $\\alpha$ 的充要条件，使得线性算子 $\\mathbf{T}(\\alpha) = \\mathbf{I} - \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{H}$ 在欧几里得范数下是一个严格收缩。\n\n将问题特化到 $n = 3$ 的情况，其中 $\\mathbf{H} = \\mathbf{I} + \\mathbf{L}$，$\\mathbf{L}$ 是具有 3 个节点和单位边权重的路径图的组合拉普拉斯算子：\n$$\n\\mathbf{L} = \\begin{pmatrix}\n1  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  1\n\\end{pmatrix}.\n$$\n计算保证 $\\rho(\\mathbf{I} - \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{H})  1$ 的最大容许恒定步长 $\\alpha$。将最终的 $\\alpha$ 报告为一个精确数，不要四舍五入。",
            "solution": "该问题经评估有效。它在科学上基于数值线性代数和优化的原理，特别是求解线性系统的迭代法的收敛性分析。该问题是适定的、客观的，并包含了唯一解所需的所有必要信息。前提是一致的；指定的矩阵 $\\mathbf{H} = \\mathbf{I} + \\mathbf{L}$ 确实是对称正定的，因为组合拉普拉斯算子 $\\mathbf{L}$ 是对称半正定的，意味着其特征值 $\\lambda_i(\\mathbf{L}) \\ge 0$，这又意味着 $\\mathbf{H}$ 的特征值（由 $1 + \\lambda_i(\\mathbf{L})$ 给出）都大于或等于 $1$，因此是正的。\n\n该问题要求两个主要结果：首先，迭代法收敛对步长 $\\alpha$ 的一般条件；其次，计算特定情况下的最大步长。\n\n首先，我们推导迭代是严格收缩的充要条件。迭代格式由下式给出：\n$$\n\\mathbf{x}_{k+1} = (\\mathbf{I} - \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{H})\\mathbf{x}_{k} + \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{y}\n$$\n这是一个形式为 $\\mathbf{x}_{k+1} = \\mathbf{T}\\mathbf{x}_k + \\mathbf{c}$ 的仿射迭代，其中迭代矩阵为 $\\mathbf{T}(\\alpha) = \\mathbf{I} - \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{H}$。对于任何初始向量 $\\mathbf{x}_0$，迭代收敛到唯一不动点的充要条件是算子 $\\mathbf{T}(\\alpha)$ 是一个严格收缩，这在有限维向量空间中等价于其谱半径小于 $1$。即 $\\rho(\\mathbf{T}(\\alpha))  1$。\n\n一个矩阵的谱半径是其特征值绝对值的最大值。设 $\\mu_i$ 是 $\\mathbf{T}(\\alpha)$ 的特征值。条件是 $\\max_i |\\mu_i|  1$。\n\n$\\mathbf{T}(\\alpha) = \\mathbf{I} - \\alpha\\mathbf{H}^{\\top}\\mathbf{H}$ 的特征值与 $\\mathbf{H}^{\\top}\\mathbf{H}$ 的特征值相关。设 $\\nu_i$ 是 $\\mathbf{H}^{\\top}\\mathbf{H}$ 的特征值。那么，$\\mathbf{T}(\\alpha)$ 的特征值为 $\\mu_i = 1 - \\alpha\\nu_i$。\n\n问题陈述 $\\mathbf{H}$ 是一个实对称矩阵。因此，$\\mathbf{H}^{\\top} = \\mathbf{H}$，矩阵 $\\mathbf{H}^{\\top}\\mathbf{H}$ 简化为 $\\mathbf{H}^2$。设 $\\lambda_i$ 是 $\\mathbf{H}$ 的特征值。那么 $\\mathbf{H}^2$ 的特征值为 $\\nu_i = \\lambda_i^2$。因为 $\\mathbf{H}$ 也是正定的，所以它的所有特征值 $\\lambda_i$ 都是严格为正的实数。因此，$\\mathbf{H}^2$ 的特征值 $\\nu_i = \\lambda_i^2$ 也严格为正。\n\n收敛条件 $\\rho(\\mathbf{T}(\\alpha))  1$ 变为 $\\max_i |1 - \\alpha \\lambda_i^2|  1$。这必须对 $\\mathbf{H}$ 的所有特征值 $\\lambda_i$ 都成立。此不等式等价于：\n$$\n-1  1 - \\alpha \\lambda_i^2  1\n$$\n我们对任意给定的特征值 $\\lambda_i  0$ 分别分析这两个不等式。\n1.  $1 - \\alpha \\lambda_i^2  1 \\implies -\\alpha \\lambda_i^2  0$。由于给定 $\\alpha  0$ 且 $\\lambda_i^2  0$，此不等式总是满足的。\n2.  $-1  1 - \\alpha \\lambda_i^2 \\implies 2  \\alpha \\lambda_i^2 \\implies \\alpha  \\frac{2}{\\lambda_i^2}$。\n\n这个条件必须对 $\\mathbf{H}$ 的所有特征值 $\\lambda_i$ 都成立。为确保这一点，$\\alpha$ 必须小于所有上界 $\\frac{2}{\\lambda_i^2}$ 中的最小值。\n$$\n\\alpha  \\min_i \\left(\\frac{2}{\\lambda_i^2}\\right) = \\frac{2}{\\max_i (\\lambda_i^2)} = \\frac{2}{(\\max_i \\lambda_i)^2}\n$$\n由于对于正定矩阵，$\\max_i \\lambda_i = \\lambda_{\\max}(\\mathbf{H})$，这也是其谱半径 $\\rho(\\mathbf{H})$，因此关于 $\\alpha$ 的充要条件是：\n$$\n0  \\alpha  \\frac{2}{(\\lambda_{\\max}(\\mathbf{H}))^2}\n$$\n\n接下来，我们将问题特化到 $n=3$ 且 $\\mathbf{H} = \\mathbf{I} + \\mathbf{L}$ 的情况，其中 $\\mathbf{L}$ 是给定的图拉普拉斯矩阵：\n$$\n\\mathbf{L} = \\begin{pmatrix} 1  -1  0 \\\\ -1  2  -1 \\\\ 0  -1  1 \\end{pmatrix}\n$$\n$\\mathbf{H}$ 的特征值由 $\\lambda_i(\\mathbf{H}) = 1 + \\lambda_i(\\mathbf{L})$ 给出，其中 $\\lambda_i(\\mathbf{L})$ 是 $\\mathbf{L}$ 的特征值。我们通过求解特征方程 $\\det(\\mathbf{L} - \\lambda \\mathbf{I}) = 0$ 来找到 $\\mathbf{L}$ 的特征值。\n$$\n\\det \\begin{pmatrix} 1-\\lambda  -1  0 \\\\ -1  2-\\lambda  -1 \\\\ 0  -1  1-\\lambda \\end{pmatrix} = 0\n$$\n沿第一行展开行列式：\n$$\n(1-\\lambda) \\det \\begin{pmatrix} 2-\\lambda  -1 \\\\ -1  1-\\lambda \\end{pmatrix} - (-1) \\det \\begin{pmatrix} -1  -1 \\\\ 0  1-\\lambda \\end{pmatrix} = 0\n$$\n$$\n(1-\\lambda)[(2-\\lambda)(1-\\lambda) - 1] + [(-1)(1-\\lambda) - 0] = 0\n$$\n$$\n(1-\\lambda)[\\lambda^2 - 3\\lambda + 2 - 1] - (1-\\lambda) = 0\n$$\n$$\n(1-\\lambda)[\\lambda^2 - 3\\lambda + 1] - (1-\\lambda) = 0\n$$\n提出公因式 $(1-\\lambda)$：\n$$\n(1-\\lambda)[(\\lambda^2 - 3\\lambda + 1) - 1] = 0\n$$\n$$\n(1-\\lambda)(\\lambda^2 - 3\\lambda) = 0\n$$\n$$\n\\lambda(1-\\lambda)(\\lambda-3) = 0\n$$\n$\\mathbf{L}$ 的特征值为 $\\lambda(\\mathbf{L}) \\in \\{0, 1, 3\\}$。\n\n因此，$\\mathbf{H} = \\mathbf{I} + \\mathbf{L}$ 的特征值为：\n$$\n\\lambda_1(\\mathbf{H}) = 1 + 0 = 1\n$$\n$$\n\\lambda_2(\\mathbf{H}) = 1 + 1 = 2\n$$\n$$\n\\lambda_3(\\mathbf{H}) = 1 + 3 = 4\n$$\n$\\mathbf{H}$ 的最大特征值为 $\\lambda_{\\max}(\\mathbf{H}) = 4$。\n\n最后，我们计算最大容许步长 $\\alpha$。使用推导出的条件，$\\alpha$ 的容许值区间为：\n$$\n0  \\alpha  \\frac{2}{(\\lambda_{\\max}(\\mathbf{H}))^2} = \\frac{2}{4^2} = \\frac{2}{16} = \\frac{1}{8}\n$$\n问题要求的是保证收敛的最大容许恒定步长 $\\alpha$。该值是开区间 $(0, \\frac{1}{8})$ 的上确界。此区间内的任何步长 $\\alpha$ 都将导致严格收缩，而在边界值 $\\alpha = \\frac{1}{8}$ 时，迭代矩阵的谱半径变为 $\\rho(\\mathbf{T}(\\frac{1}{8}))=1$，这不能保证对所有初始向量都收敛到正确的解。保证严格不等式 $\\rho  1$ 的最大值是该区间的极限。因此，最大容许值是上确界。\n\n最大容许恒定步长是 $\\frac{1}{8}$。",
            "answer": "$$\\boxed{\\frac{1}{8}}$$"
        },
        {
            "introduction": "当图神经网络（GNN）提供了系统状态的先验估计后，最后一步是将此预测与实际测量数据相结合。本练习使用数据同化的基石——卡尔曼滤波器框架，提供了一个具体的示例。你将执行“分析更新”步骤，亲身体验模型预测是如何被观测数据修正的。",
            "id": "3386871",
            "problem": "考虑一个图上的动态逆问题，其中潜状态 $x_t \\in \\mathbb{R}^3$ 定义在一个具有三个节点和边 $\\{(1,2),(2,3)\\}$ 的路径图的节点上。一个图神经网络 (GNN) 先验导出一个线性化单步展开算子 $F \\in \\mathbb{R}^{3 \\times 3}$ 和一个过程噪声协方差 $Q \\in \\mathbb{R}^{3 \\times 3}$，这里使用图拉普拉斯算子 $L \\in \\mathbb{R}^{3 \\times 3}$ 进行建模，其中\n$$\nL \\;=\\; \\begin{bmatrix}\n1  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  1\n\\end{bmatrix}, \\qquad \nF \\;=\\; I - \\alpha L \\;\\;\\text{with}\\;\\; \\alpha \\;=\\; \\frac{1}{2}, \\qquad \nQ \\;=\\; \\sigma_q^2 I,\\;\\; \\sigma_q^2 \\;=\\; \\frac{1}{10}.\n$$\n在时间 $t$，同化数据 $y_t$ 之前，预测均值和协方差（从前一个分析步骤和 GNN 导出的动力学获得）由下式给出\n$$\nx_{t|t-1} \\;=\\; \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix}, \n\\qquad \nP_{t|t-1} \\;=\\; \\begin{bmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{bmatrix}.\n$$\n通过一个线性观测算子 $H \\in \\mathbb{R}^{2 \\times 3}$ 和加性高斯噪声 $\\varepsilon_t \\sim \\mathcal{N}(0,R)$ 在节点 $1$ 和 $3$ 上收集带噪声的观测值，其中\n$$\nH \\;=\\; \\begin{bmatrix} 1  0  0 \\\\ 0  0  1 \\end{bmatrix}, \n\\qquad \nR \\;=\\; \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}, \n\\qquad \ny_t \\;=\\; \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.\n$$\n从线性高斯数据同化模型和联合高斯随机变量的条件化法则出发，推导将预测 $(x_{t|t-1}, P_{t|t-1})$ 与 $y_t$ 融合以获得卡尔曼增益和后验均值的分析步骤。然后针对给定的数值实例计算这些表达式。\n\n将最终答案以单个行矩阵的形式报告，该矩阵首先按列主序排列卡尔曼增益 $K_t \\in \\mathbb{R}^{3 \\times 2}$ 的条目，然后是后验均值 $x_{t|t} \\in \\mathbb{R}^3$ 的三个条目。具体来说，报告\n$$\n\\big[\\, K_{t,(1,1)},\\, K_{t,(2,1)},\\, K_{t,(3,1)},\\, K_{t,(1,2)},\\, K_{t,(2,2)},\\, K_{t,(3,2)},\\, x_{t|t,(1)},\\, x_{t|t,(2)},\\, x_{t|t,(3)} \\,\\big].\n$$\n使用精确值；无需四舍五入。本问题不涉及物理单位。角度单位规范在此不适用。",
            "solution": "本问题要求基于卡尔曼滤波框架求解一个数据同化问题。\n\n分析步骤将预测信息与新的观测值相结合。我们假设一个线性高斯模型。\n状态 $x_t$ 的先验（预测）分布是高斯的：\n$$ x_t \\sim \\mathcal{N}(x_{t|t-1}, P_{t|t-1}) $$\n观测模型是线性的，并带有加性高斯噪声：\n$$ y_t = H x_t + \\varepsilon_t, \\quad \\text{其中} \\quad \\varepsilon_t \\sim \\mathcal{N}(0, R) $$\n假设状态 $x_t$ 和噪声 $\\varepsilon_t$ 不相关。\n\n我们构造一个联合随机向量 $\\begin{pmatrix} x_t \\\\ y_t \\end{pmatrix}$。其分布也是高斯的。\n联合向量的均值为：\n$$ E\\left[ \\begin{pmatrix} x_t \\\\ y_t \\end{pmatrix} \\right] = \\begin{pmatrix} E[x_t] \\\\ E[H x_t + \\varepsilon_t] \\end{pmatrix} = \\begin{pmatrix} x_{t|t-1} \\\\ H E[x_t] + E[\\varepsilon_t] \\end{pmatrix} = \\begin{pmatrix} x_{t|t-1} \\\\ H x_{t|t-1} \\end{pmatrix} $$\n联合向量的协方差为：\n$$ \\text{Cov}\\left( \\begin{pmatrix} x_t \\\\ y_t \\end{pmatrix} \\right) = \\begin{pmatrix} \\text{Cov}(x_t, x_t)  \\text{Cov}(x_t, y_t) \\\\ \\text{Cov}(y_t, x_t)  \\text{Cov}(y_t, y_t) \\end{pmatrix} $$\n我们来计算各个分块：\n- $\\text{Cov}(x_t, x_t) = P_{t|t-1}$\n- $\\text{Cov}(x_t, y_t) = E[(x_t - x_{t|t-1})(y_t - H x_{t|t-1})^T] = E[(x_t - x_{t|t-1})(H x_t + \\varepsilon_t - H x_{t|t-1})^T] = E[(x_t - x_{t|t-1})(H(x_t - x_{t|t-1}) + \\varepsilon_t)^T] = E[(x_t - x_{t|t-1})(x_t - x_{t|t-1})^T H^T] + E[(x_t - x_{t|t-1})\\varepsilon_t^T] = P_{t|t-1} H^T$，因为 $x_t$ 和 $\\varepsilon_t$ 不相关。\n- $\\text{Cov}(y_t, x_t) = (\\text{Cov}(x_t, y_t))^T = H P_{t|t-1}$\n- $\\text{Cov}(y_t, y_t) = E[(y_t - H x_{t|t-1})(y_t - H x_{t|t-1})^T] = E[(H(x_t - x_{t|t-1}) + \\varepsilon_t)(H(x_t - x_{t|t-1}) + \\varepsilon_t)^T] = H E[(x_t - x_{t|t-1})(x_t - x_{t|t-1})^T] H^T + E[\\varepsilon_t \\varepsilon_t^T] = H P_{t|t-1} H^T + R$。\n\n所以联合分布为：\n$$ \\begin{pmatrix} x_t \\\\ y_t \\end{pmatrix} \\sim \\mathcal{N}\\left( \\begin{pmatrix} x_{t|t-1} \\\\ H x_{t|t-1} \\end{pmatrix}, \\begin{pmatrix} P_{t|t-1}  P_{t|t-1} H^T \\\\ H P_{t|t-1}  H P_{t|t-1} H^T + R \\end{pmatrix} \\right) $$\n\n分析步骤需要求出给定观测值 $y_t$ 时 $x_t$ 的条件分布。对于一个通用的分块高斯向量 $\\begin{pmatrix} z_1 \\\\ z_2 \\end{pmatrix}$，其均值为 $\\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}$，协方差为 $\\begin{pmatrix} \\Sigma_{11}  \\Sigma_{12} \\\\ \\Sigma_{21}  \\Sigma_{22} \\end{pmatrix}$，则给定 $z_2$ 时 $z_1$ 的条件分布是高斯分布，其均值为 $\\mu_{1|2} = \\mu_1 + \\Sigma_{12}\\Sigma_{22}^{-1}(z_2 - \\mu_2)$，协方差为 $\\Sigma_{1|2} = \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}$。\n\n将此应用于我们的情况（$z_1=x_t$, $z_2=y_t$），我们得到后验（分析）均值 $x_{t|t}$ 和协方差 $P_{t|t}$：\n$$ x_{t|t} = x_{t|t-1} + (P_{t|t-1} H^T)(H P_{t|t-1} H^T + R)^{-1}(y_t - H x_{t|t-1}) $$\n其中 $K_t = (P_{t|t-1} H^T)(H P_{t|t-1} H^T + R)^{-1}$ 是卡尔曼增益。\n后验均值则由更新方程给出：\n$$ x_{t|t} = x_{t|t-1} + K_t (y_t - H x_{t|t-1}) $$\n\n现在，我们为给定的数值实例计算这些表达式。\n给定的矩阵和向量是：\n$$ P_{t|t-1} = \\begin{bmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{bmatrix}, \\quad H = \\begin{bmatrix} 1  0  0 \\\\ 0  0  1 \\end{bmatrix}, \\quad R = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} $$\n$$ x_{t|t-1} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix}, \\quad y_t = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $$\n\n首先，我们计算卡尔曼增益 $K_t$ 所需的分量。\n$P_{t|t-1} H^T$ 项：\n$$ P_{t|t-1} H^T = \\begin{bmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 0  0 \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ 0  0 \\\\ 0  3 \\end{bmatrix} $$\n新息协方差 $S_t = H P_{t|t-1} H^T + R$：\n$$ H P_{t|t-1} H^T = \\begin{bmatrix} 1  0  0 \\\\ 0  0  1 \\end{bmatrix} \\begin{bmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 0  0 \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1  0  0 \\\\ 0  0  3 \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 0  0 \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ 0  3 \\end{bmatrix} $$\n$$ S_t = \\begin{bmatrix} 1  0 \\\\ 0  3 \\end{bmatrix} + \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 2  0 \\\\ 0  4 \\end{bmatrix} $$\n新息协方差的逆 $S_t^{-1}$：\n$$ S_t^{-1} = \\begin{bmatrix} 2  0 \\\\ 0  4 \\end{bmatrix}^{-1} = \\begin{bmatrix} \\frac{1}{2}  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix} $$\n现在，我们可以计算卡尔曼增益 $K_t = P_{t|t-1} H^T S_t^{-1}$：\n$$ K_t = \\begin{bmatrix} 1  0 \\\\ 0  0 \\\\ 0  3 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{2}  0 \\\\ 0  \\frac{1}{4} \\end{bmatrix} = \\begin{bmatrix} (1)(\\frac{1}{2})+(0)(0)  (1)(0)+(0)(\\frac{1}{4}) \\\\ (0)(\\frac{1}{2})+(0)(0)  (0)(0)+(0)(\\frac{1}{4}) \\\\ (0)(\\frac{1}{2})+(3)(0)  (0)(0)+(3)(\\frac{1}{4}) \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2}  0 \\\\ 0  0 \\\\ 0  \\frac{3}{4} \\end{bmatrix} $$\n\n接下来，我们计算后验均值 $x_{t|t}$。\n我们需要新息项 $y_t - H x_{t|t-1}$：\n$$ H x_{t|t-1} = \\begin{bmatrix} 1  0  0 \\\\ 0  0  1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 2 \\end{bmatrix} $$\n$$ y_t - H x_{t|t-1} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 0 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} $$\n最后，我们计算后验均值 $x_{t|t} = x_{t|t-1} + K_t (y_t - H x_{t|t-1})$：\n$$ x_{t|t} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix} + \\begin{bmatrix} \\frac{1}{2}  0 \\\\ 0  0 \\\\ 0  \\frac{3}{4} \\end{bmatrix} \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix} + \\begin{bmatrix} (\\frac{1}{2})(1)+(0)(-1) \\\\ (0)(1)+(0)(-1) \\\\ (0)(1)+(\\frac{3}{4})(-1) \\end{bmatrix} $$\n$$ x_{t|t} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix} + \\begin{bmatrix} \\frac{1}{2} \\\\ 0 \\\\ -\\frac{3}{4} \\end{bmatrix} = \\begin{bmatrix} 0+\\frac{1}{2} \\\\ 1+0 \\\\ 2-\\frac{3}{4} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ 1 \\\\ \\frac{5}{4} \\end{bmatrix} $$\n\n问题要求将结果表示为单个行矩阵，其中包含按列主序排列的卡尔曼增益 $K_t$ 的条目，后面是后验均值 $x_{t|t}$ 的条目。\n卡尔曼增益为 $K_t = \\begin{bmatrix} K_{t,(1,1)}  K_{t,(1,2)} \\\\ K_{t,(2,1)}  K_{t,(2,2)} \\\\ K_{t,(3,1)}  K_{t,(3,2)} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2}  0 \\\\ 0  0 \\\\ 0  \\frac{3}{4} \\end{bmatrix}$。\n按列主序，条目为：$K_{t,(1,1)}=\\frac{1}{2}$，$K_{t,(2,1)}=0$，$K_{t,(3,1)}=0$，$K_{t,(1,2)}=0$，$K_{t,(2,2)}=0$，$K_{t,(3,2)}=\\frac{3}{4}$。\n后验均值为 $x_{t|t} = \\begin{bmatrix} x_{t|t,(1)} \\\\ x_{t|t,(2)} \\\\ x_{t|t,(3)} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ 1 \\\\ \\frac{5}{4} \\end{bmatrix}$。\n\n最终报告的向量是 $\\big[\\, \\frac{1}{2},\\, 0,\\, 0,\\, 0,\\, 0,\\, \\frac{3}{4},\\, \\frac{1}{2},\\, 1,\\, \\frac{5}{4} \\,\\big]$。",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2}  0  0  0  0  \\frac{3}{4}  \\frac{1}{2}  1  \\frac{5}{4}\n\\end{pmatrix}\n}\n$$"
        }
    ]
}