{
    "hands_on_practices": [
        {
            "introduction": "在深入研究代码实现之前，首要任务是透彻理解物理启发神经网络（PINN）的核心——损失函数。本练习将引导你通过一个经典的泊松方程案例，亲手推导构成损失函数的关键部分：偏微分方程（PDE）残差和边界条件残差。通过这种“纸上谈兵”的实践，你将为后续更复杂的编程任务打下坚实的理论基础，确保你不仅知道“如何做”，更明白“为何如此做”。",
            "id": "3410533",
            "problem": "考虑单位正方形域 $\\Omega = (0,1)\\times(0,1)$ 上的泊松方程，在 $\\partial\\Omega$ 上具有齐次狄利克雷边界条件。控制方程是 $\\Omega$ 内的泊松方程 $-\\Delta u = f$，其中拉普拉斯算子定义为 $\\Delta u = u_{xx} + u_{yy}$，狄利克雷边界条件为在 $\\partial\\Omega$ 上 $u = 0$。在物理启发神经网络 (PINNs) 中，物理损失项通常涉及逐点残差 $r_{\\mathrm{PDE}}(x,y) = -\\Delta u(x,y) - f(x,y)$，而边界损失项则强制在 $\\partial\\Omega$ 上 $r_{\\mathrm{BC}}(x,y) = u(x,y)$。验证自动微分所需的梯度包括 $u_x$、$u_y$、$u_{xx}$、$u_{yy}$，以及物理残差的输入梯度 $\\nabla r_{\\mathrm{PDE}}(x,y) = \\left(\\frac{\\partial r_{\\mathrm{PDE}}}{\\partial x}(x,y), \\frac{\\partial r_{\\mathrm{PDE}}}{\\partial y}(x,y)\\right)$。\n\n给定解析解 $u(x,y) = \\sin(\\pi x)\\sin(\\pi y)$，请设计一个标准测试来验证 PINN 的实现。从拉普拉斯算子和狄利克雷边界条件的基本定义出发，推导源项 $f(x,y)$ 以使 $u$ 满足在 $\\Omega$ 内的 $-\\Delta u = f$，并推导精确残差 $r_{\\mathrm{PDE}}(x,y)$ 和 $r_{\\mathrm{BC}}(x,y)$，以及由此选择所隐含的精确一阶和二阶导数 $u_x$、$u_y$、$u_{xx}$、$u_{yy}$ 和梯度 $\\nabla r_{\\mathrm{PDE}}(x,y)$。\n\n对于给定的 $u$，哪个选项正确地指定了 $f(x,y)$、$r_{\\mathrm{PDE}}(x,y)$、$r_{\\mathrm{BC}}(x,y)$、$u_x(x,y)$、$u_y(x,y)$、$u_{xx}(x,y)$、$u_{yy}(x,y)$ 和 $\\nabla r_{\\mathrm{PDE}}(x,y)$？\n\nA. $f(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，$r_{\\mathrm{PDE}}(x,y) = 0$，$r_{\\mathrm{BC}}(x,y) = 0$ 在 $\\partial\\Omega$ 上，$u_x(x,y) = \\pi \\cos(\\pi x)\\sin(\\pi y)$，$u_y(x,y) = \\pi \\sin(\\pi x)\\cos(\\pi y)$，$u_{xx}(x,y) = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，$u_{yy}(x,y) = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，以及 $\\nabla r_{\\mathrm{PDE}}(x,y) = (0,0)$。\n\nB. $f(x,y) = -2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，$r_{\\mathrm{PDE}}(x,y) = 4\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，$r_{\\mathrm{BC}}(x,y) = 0$ 在 $\\partial\\Omega$ 上，$u_x(x,y) = \\pi \\sin(\\pi x)\\sin(\\pi y)$，$u_y(x,y) = \\pi \\cos(\\pi x)\\cos(\\pi y)$，$u_{xx}(x,y) = \\pi^2 \\cos(\\pi x)\\sin(\\pi y)$，$u_{yy}(x,y) = \\pi^2 \\sin(\\pi x)\\cos(\\pi y)$，以及 $\\nabla r_{\\mathrm{PDE}}(x,y) = \\left(4\\pi^3 \\cos(\\pi x)\\sin(\\pi y), 4\\pi^3 \\sin(\\pi x)\\cos(\\pi y)\\right)$。\n\nC. $f(x,y) = \\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，$r_{\\mathrm{PDE}}(x,y) = \\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，$r_{\\mathrm{BC}}(x,y) = 0$ 在 $\\partial\\Omega$ 上，$u_x(x,y) = \\pi \\cos(\\pi x)\\sin(\\pi y)$，$u_y(x,y) = \\pi \\sin(\\pi x)\\cos(\\pi y)$，$u_{xx}(x,y) = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，$u_{yy}(x,y) = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，以及 $\\nabla r_{\\mathrm{PDE}}(x,y) = \\left(\\pi^3 \\cos(\\pi x)\\sin(\\pi y), \\pi^3 \\sin(\\pi x)\\cos(\\pi y)\\right)$。\n\nD. $f(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，$r_{\\mathrm{PDE}}(x,y) = 0$，$r_{\\mathrm{BC}}(x,y) = \\partial_n u(x,y)$ 在 $\\partial\\Omega$ 上，$u_x(x,y) = \\pi \\cos(\\pi x)\\cos(\\pi y)$，$u_y(x,y) = \\pi \\cos(\\pi x)\\cos(\\pi y)$，$u_{xx}(x,y) = \\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，$u_{yy}(x,y) = \\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，以及 $\\nabla r_{\\mathrm{PDE}}(x,y) = (0,0)$。",
            "solution": "我们从基本定义开始。拉普拉斯算子定义为 $\\Delta u = u_{xx} + u_{yy}$，泊松方程为 $-\\Delta u = f$ 在 $\\Omega = (0,1)\\times(0,1)$ 内，齐次狄利克雷边界条件为在 $\\partial\\Omega$ 上 $u = 0$。对于给定的解析解 $u(x,y) = \\sin(\\pi x)\\sin(\\pi y)$，我们使用链式法则和正弦、余弦函数的标准微分法则来计算所需的导数。\n\n首先，计算一阶导数：\n$u_x(x,y) = \\frac{\\partial}{\\partial x}\\left[\\sin(\\pi x)\\sin(\\pi y)\\right] = \\pi \\cos(\\pi x)\\sin(\\pi y)$，\n$u_y(x,y) = \\frac{\\partial}{\\partial y}\\left[\\sin(\\pi x)\\sin(\\pi y)\\right] = \\pi \\sin(\\pi x)\\cos(\\pi y)$。\n\n其次，计算二阶导数：\n$u_{xx}(x,y) = \\frac{\\partial}{\\partial x}\\left[\\pi \\cos(\\pi x)\\sin(\\pi y)\\right] = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，\n$u_{yy}(x,y) = \\frac{\\partial}{\\partial y}\\left[\\pi \\sin(\\pi x)\\cos(\\pi y)\\right] = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$。\n\n因此，拉普拉斯算子为\n$\\Delta u(x,y) = u_{xx}(x,y) + u_{yy}(x,y) = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y) - \\pi^2 \\sin(\\pi x)\\sin(\\pi y) = -2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$。\n\n根据控制方程 $-\\Delta u = f$，我们得到\n$f(x,y) = -\\Delta u(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$。\n\n定义物理残差 $r_{\\mathrm{PDE}}(x,y) = -\\Delta u(x,y) - f(x,y)$。代入上述表达式，\n$r_{\\mathrm{PDE}}(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y) - 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y) = 0$，\n因此，当使用精确的 $u$ 和一致的 $f$ 时，残差恒为零。\n\n齐次狄利克雷条件的边界残差是在 $\\partial\\Omega$ 上 $r_{\\mathrm{BC}}(x,y) = u(x,y)$。因为当 $x = 0$ 或 $x = 1$ 时 $\\sin(\\pi x) = 0$，以及当 $y = 0$ 或 $y = 1$ 时 $\\sin(\\pi y) = 0$，我们在 $\\partial\\Omega$ 上有 $u = 0$，所以在 $\\partial\\Omega$ 上 $r_{\\mathrm{BC}}(x,y) = 0$。\n\n最后，物理残差相对于输入的梯度是\n$\\nabla r_{\\mathrm{PDE}}(x,y) = \\left(\\frac{\\partial r_{\\mathrm{PDE}}}{\\partial x}(x,y), \\frac{\\partial r_{\\mathrm{PDE}}}{\\partial y}(x,y)\\right)$。\n由于 $r_{\\mathrm{PDE}}(x,y) \\equiv 0$，我们有 $\\frac{\\partial r_{\\mathrm{PDE}}}{\\partial x}(x,y) = 0$ 和 $\\frac{\\partial r_{\\mathrm{PDE}}}{\\partial y}(x,y) = 0$，因此 $\\nabla r_{\\mathrm{PDE}}(x,y) = (0,0)$。\n\n我们现在评估每个选项：\n\n选项 A：它陈述了 $f(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，$r_{\\mathrm{PDE}}(x,y) = 0$，$r_{\\mathrm{BC}}(x,y) = 0$ 在 $\\partial\\Omega$ 上，并列出的 $u_x$、$u_y$、$u_{xx}$、$u_{yy}$ 与上面计算的完全一致。它也正确地指出了 $\\nabla r_{\\mathrm{PDE}}(x,y) = (0,0)$，因为残差恒为零。结论 — 正确。\n\n选项 B：它设定了 $f(x,y) = -2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，这与 $f = -\\Delta u = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$ 相矛盾。使用这个不正确的 $f$，它给出了 $r_{\\mathrm{PDE}}(x,y) = 4\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，这是由符号错误产生的非零残差。此外，它错误地列出了 $u_x$、$u_y$、$u_{xx}$、$u_{yy}$：例如，$u_x$ 被给出为 $\\pi \\sin(\\pi x)\\sin(\\pi y)$，但正确的 $u_x$ 是 $\\pi \\cos(\\pi x)\\sin(\\pi y)$；类似地，$u_{xx}$ 不是 $\\pi^2 \\cos(\\pi x)\\sin(\\pi y)$ 而是 $-\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$。陈述的 $\\nabla r_{\\mathrm{PDE}}$ 也与正确的残差不一致。结论 — 错误。\n\n选项 C：它设置 $f(x,y) = \\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，缺少一个因子 $2$。因此，$r_{\\mathrm{PDE}}(x,y)$ 被给出为 $\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$，这是非零的，与精确残差相矛盾。虽然这个选项列出了正确的 $u_x$、$u_y$、$u_{xx}$ 和 $u_{yy}$，但它从其错误的残差中错误地计算了 $\\nabla r_{\\mathrm{PDE}}$。精确残差的正确梯度是零向量。结论 — 错误。\n\n选项 D：它给出了正确的 $f(x,y)$ 并正确地设置了 $r_{\\mathrm{PDE}}(x,y) = 0$，但它将边界残差定义为在 $\\partial\\Omega$ 上 $r_{\\mathrm{BC}}(x,y) = \\partial_n u(x,y)$，这不是狄利克雷边界残差；对于齐次狄利克雷条件，$r_{\\mathrm{BC}}(x,y)$ 应该是 $u(x,y)$，而不是法向导数。此外，它错误地将 $u_x$ 和 $u_y$ 都列为 $\\pi \\cos(\\pi x)\\cos(\\pi y)$，这不等于正确的一阶导数，并且它错误地将 $u_{xx}$ 和 $u_{yy}$ 的符号记为正的 $\\pi^2 \\sin(\\pi x)\\sin(\\pi y)$ 而不是负的。尽管它给出了 $\\nabla r_{\\mathrm{PDE}}(x,y) = (0,0)$，但其他的不一致之处使得该选项无效。结论 — 错误。\n\n因此，正确选项是A。",
            "answer": "$$\\boxed{A}$$"
        },
        {
            "introduction": "掌握了损失函数的基本构成后，我们将面临一个实际的挑战：如何在不规则的复杂几何域上有效施加边界条件。这个实践练习通过编码实现，对比了两种在PINN中常用的边界约束策略：一种是基于精确的解析符号距离函数，另一种是基于离散边界点的近似欧氏距离。通过比较这两种方法的边界约束违例，你将深刻体会到在处理复杂问题时，不同数值策略在精度和实现上的权衡。",
            "id": "3410603",
            "problem": "考虑一个在星形不规则域上的带狄利克雷边界条件的泊松问题。设计算域为 $\\mathbb{R}^2$ 中的点集 $(x,y)$，在极坐标 $(r,\\theta)$ 下由不等式描述\n$$ r < r_{\\varepsilon,k}(\\theta), \\quad r_{\\varepsilon,k}(\\theta) = 1 + \\varepsilon \\cos(k \\theta), $$\n其中 $\\varepsilon \\in (0,1)$ 控制边界扰动的振幅， $k \\in \\mathbb{N}$ 控制其方位角频率。角度 $\\theta$ 以弧度为单位。边界是由 $r = r_{\\varepsilon,k}(\\theta)$ 定义的闭合曲线。\n\n该泊松问题为\n$$ -\\Delta u(x,y) = f(x,y) \\quad \\text{在域的内部}, \\quad u(x,y) = 0 \\quad \\text{在边界上}。 $$\n我们关注物理启发神经网络（PINNs）中使用的边界强制策略，其中通常通过嵌入一个在边界上为零的因子来构造满足边界条件的试探场。我们比较两种此类策略：\n\n1. 符号水平集因子：定义符号水平集函数\n$$ s_{\\varepsilon,k}(x,y) = \\rho(x,y) - r_{\\varepsilon,k}(\\theta(x,y)), $$\n其中 $\\rho(x,y) = \\sqrt{x^2 + y^2}$ 且 $\\theta(x,y) = \\operatorname{atan2}(y,x)$。根据构造，该函数在边界上满足 $s_{\\varepsilon,k}(x,y) = 0$。符号水平集试探场为\n$$ u_s(x,y) = s_{\\varepsilon,k}(x,y) \\, p(x,y), $$\n其中 $p(x,y)$ 是一个固定的光滑函数，代表一个通用的无约束模型分量，此处取为多项式\n$$ p(x,y) = x^2 + y^2 + x y. $$\n\n2. 离散边界的欧几里得距离因子：使用 $M$ 个离散边界样本点来近似到边界的欧几里得距离。设 $\\{ \\theta_j \\}_{j=0}^{M-1}$ 是在 $[0,2\\pi)$ 中的 $M$ 个均匀分布的角度集合，并定义边界样本点\n$$ b_j = \\big( r_{\\varepsilon,k}(\\theta_j) \\cos \\theta_j,\\; r_{\\varepsilon,k}(\\theta_j) \\sin \\theta_j \\big). $$\n对于任意点 $(x,y)$，定义到边界的近似欧几里得距离为\n$$ d_M(x,y) = \\min_{0 \\le j \\le M-1} \\left\\| (x,y) - b_j \\right\\|_2, $$\n其中 $\\|\\cdot\\|_2$ 表示欧几里得范数。基于距离的试探场为\n$$ u_d(x,y) = d_M(x,y) \\, p(x,y). $$\n\n我们通过在更精细的 $N$ 个边界评估点集上评估两个试探场来评估边界约束的违反情况。设 $\\{ \\varphi_i \\}_{i=0}^{N-1}$ 是在 $[0,2\\pi)$ 中的 $N$ 个均匀分布的角度集合，并定义\n$$ \\tilde{b}_i = \\big( r_{\\varepsilon,k}(\\varphi_i) \\cos \\varphi_i,\\; r_{\\varepsilon,k}(\\varphi_i) \\sin \\varphi_i \\big). $$\n由于边界条件是在边界上 $u=0$，一个自然的边界约束违反情况的离散均方根误差 (RMSE) 指标是\n$$ \\mathrm{RMSE}_s = \\sqrt{ \\frac{1}{N} \\sum_{i=0}^{N-1} \\left( u_s(\\tilde{b}_i) \\right)^2 }, \\quad \\mathrm{RMSE}_d = \\sqrt{ \\frac{1}{N} \\sum_{i=0}^{N-1} \\left( u_d(\\tilde{b}_i) \\right)^2 }. $$\n注意，根据构造 $u_s(\\tilde{b}_i) = s_{\\varepsilon,k}(\\tilde{b}_i) \\, p(\\tilde{b}_i) = 0 \\cdot p(\\tilde{b}_i) = 0$，而如果 $d_M$ 是从边界的粗糙离散化计算得出的，则 $u_d(\\tilde{b}_i)$ 可能为非零值。\n\n你的任务是实现一个程序，对于每个测试用例 $(\\varepsilon,k,M,N)$，计算序对 $(\\mathrm{RMSE}_d,\\mathrm{RMSE}_s)$，并额外输出一个布尔值，该值指示基于距离的策略是否比符号水平集策略具有严格更低的边界约束违反度。该布尔值定义为\n$$ \\text{better} = (\\mathrm{RMSE}_d  \\mathrm{RMSE}_s). $$\n\n角度值必须以弧度处理。不涉及物理单位。所有计算都必须使用双精度浮点算术进行。\n\n测试套件：\n- 案例1（基线不规则度，中等粗采样）：$(\\varepsilon,k,M,N) = (0.20, 3, 24, 360)$。\n- 案例2（增加的不规则度，较精细的粗采样）：$(\\varepsilon,k,M,N) = (0.35, 4, 32, 360)$。\n- 案例3（高不规则度，稀疏粗采样）：$(\\varepsilon,k,M,N) = (0.45, 5, 18, 360)$。\n- 案例4（粗采样等于评估采样，边界边缘情况）：$(\\varepsilon,k,M,N) = (0.30, 3, 360, 360)$。\n\n最终输出格式：\n你的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例的结果必须是 $[\\mathrm{RMSE}_d,\\mathrm{RMSE}_s,\\text{better}]$ 形式的列表，其中 $\\mathrm{RMSE}_d$ 和 $\\mathrm{RMSE}_s$ 是浮点数，$\\text{better}$ 是布尔值。总输出必须是这些列表的列表，例如：\n$$ [ [\\mathrm{RMSE}_{d,1}, \\mathrm{RMSE}_{s,1}, \\text{better}_1], [\\mathrm{RMSE}_{d,2}, \\mathrm{RMSE}_{s,2}, \\text{better}_2], [\\mathrm{RMSE}_{d,3}, \\mathrm{RMSE}_{s,3}, \\text{better}_3], [\\mathrm{RMSE}_{d,4}, \\mathrm{RMSE}_{s,4}, \\text{better}_4] ]. $$",
            "solution": "用户希望在物理启发神经网络（PINNs）的背景下，评估和比较两种强制执行狄利克雷边界条件的策略。这是通过在星形域上为每种策略计算边界约束违反度来完成的。\n\n### 问题验证\n首先根据指定标准验证问题陈述。\n\n**1. 提取已知条件：**\n- **域边界**：在极坐标 $(r, \\theta)$ 中由 $r = r_{\\varepsilon,k}(\\theta) = 1 + \\varepsilon \\cos(k \\theta)$ 定义，其中 $\\varepsilon \\in (0,1)$ 且 $k \\in \\mathbb{N}$。\n- **偏微分方程（PDE）**：泊松问题 $-\\Delta u(x,y) = f(x,y)$，边界条件为 $u(x,y) = 0$。\n- **模型分量**：一个固定的多项式 $p(x,y) = x^2 + y^2 + xy$。\n- **策略1（符号水平集）**：试探场 $u_s(x,y) = s_{\\varepsilon,k}(x,y) \\, p(x,y)$，其中 $s_{\\varepsilon,k}(x,y) = \\sqrt{x^2+y^2} - r_{\\varepsilon,k}(\\operatorname{atan2}(y,x))$。\n- **策略2（离散距离）**：试探场 $u_d(x,y) = d_M(x,y) \\, p(x,y)$，其中 $d_M(x,y) = \\min_{j} \\| (x,y) - b_j \\|_2$ 是对 $M$ 个离散边界点 $\\{b_j\\}$ 的集合求最小值。\n- **误差度量**：在 $N$ 个边界点 $\\{\\tilde{b}_i\\}$ 上评估的均方根误差 (RMSE)。\n  - $\\mathrm{RMSE}_s = \\sqrt{ \\frac{1}{N} \\sum_{i=0}^{N-1} ( u_s(\\tilde{b}_i) )^2 }$\n  - $\\mathrm{RMSE}_d = \\sqrt{ \\frac{1}{N} \\sum_{i=0}^{N-1} ( u_d(\\tilde{b}_i) )^2 }$\n- **比较**：布尔值 $\\text{better} = (\\mathrm{RMSE}_d  \\mathrm{RMSE}_s)$。\n- **测试用例**：\n  - 案例1：$(\\varepsilon,k,M,N) = (0.20, 3, 24, 360)$\n  - 案例2：$(\\varepsilon,k,M,N) = (0.35, 4, 32, 360)$\n  - 案例3：$(\\varepsilon,k,M,N) = (0.45, 5, 18, 360)$\n  - 案例4：$(\\varepsilon,k,M,N) = (0.30, 3, 360, 360)$\n\n**2. 验证结论：**\n问题是**有效的**。它在偏微分方程数值方法领域有科学依据，特别是关于 PINN 中的边界条件强制执行。所有术语和过程在数学上都定义良好，问题是自洽和客观的。该设置对每个测试用例都会产生唯一的、可计算的解。$\\mathrm{RMSE}_s$ 在解析上为零这一事实是该问题的一个关键特征，旨在突出精确强制方法和近似强制方法之间的差异，而不是一个缺陷。\n\n### 解法推导\n\n问题的核心是为给定的测试用例实现两种误差度量 $\\mathrm{RMSE}_s$ 和 $\\mathrm{RMSE}_d$ 的计算。\n\n**1. 符号水平集策略 ($u_s$) 的分析**\n该策略的试探场为 $u_s(x,y) = s_{\\varepsilon,k}(x,y) \\, p(x,y)$。符号水平集函数 $s_{\\varepsilon,k}(x,y)$ 定义为一个点 $(x,y)$ 的径向坐标 $\\rho(x,y) = \\sqrt{x^2+y^2}$ 与该点角度对应的边界半径 $r_{\\varepsilon,k}(\\theta(x,y))$ 之间的差值。\n\n根据构造，对于边界曲线上的任何点 $(x,y)$，其径向坐标 $\\rho(x,y)$ 等于 $r_{\\varepsilon,k}(\\theta(x,y))$。因此，符号水平集函数 $s_{\\varepsilon,k}(x,y)$ 在边界上的所有点恒为零。\n\n误差度量 $\\mathrm{RMSE}_s$ 在一组 $N$ 个点 $\\{\\tilde{b}_i\\}_{i=0}^{N-1}$ 上进行评估，这些点被明确定义为位于边界上。在任何这样的点 $\\tilde{b}_i$ 处，我们有 $s_{\\varepsilon,k}(\\tilde{b}_i) = 0$。因此，试探场的值为 $u_s(\\tilde{b}_i) = s_{\\varepsilon,k}(\\tilde{b}_i) \\cdot p(\\tilde{b}_i) = 0 \\cdot p(\\tilde{b}_i) = 0$。\n\n由于 $\\mathrm{RMSE}_s$ 求和中的每一项都为零，其结果是解析确定的：\n$$ \\mathrm{RMSE}_s = \\sqrt{ \\frac{1}{N} \\sum_{i=0}^{N-1} (0)^2 } = 0 $$\n这对所有测试用例都成立，无论参数 $\\varepsilon$、 $k$、 $M$ 或 $N$ 如何。这个结果说明了使用精确的、解析定义的函数来强制执行边界条件的主要优点：它能完美满足条件，达到机器精度。\n\n**2. 离散距离策略 ($u_d$) 的算法**\n该策略的试探场 $u_d(x,y) = d_M(x,y) \\, p(x,y)$ 依赖于对到边界真实距离的近似。函数 $d_M(x,y)$ 是从点 $(x,y)$ 到预先采样的 $M$ 个边界点 $\\{b_j\\}$ 的有限集合的最小欧几里得距离。这是一个近似值，因为真实距离需要找到在整个连续边界曲线上的最小值。$\\mathrm{RMSE}_d$ 的计算过程对每个测试用例 $(\\varepsilon, k, M, N)$ 如下。\n\n- **步骤 2.1：生成边界样本点**\n  我们生成 $M$ 个离散点 $\\{b_j\\}_{j=0}^{M-1}$ 来定义近似边界。这是通过创建 $M$ 个在 $[0, 2\\pi)$ 中均匀分布的角度 $\\theta_j$ 并计算相应的笛卡尔坐标来完成的：\n  $$ \\theta_j = \\frac{2\\pi j}{M} \\quad \\text{for } j = 0, 1, \\dots, M-1 $$\n  $$ r_j = r_{\\varepsilon,k}(\\theta_j) = 1 + \\varepsilon \\cos(k \\theta_j) $$\n  $$ b_j = (x_j, y_j) = (r_j \\cos \\theta_j, r_j \\sin \\theta_j) $$\n  这 $M$ 个点存储起来，例如，在一个形状为 $(M, 2)$ 的 NumPy 数组中。\n\n- **步骤 2.2：生成边界评估点**\n  类似地，我们生成 $N$ 个点 $\\{\\tilde{b}_i\\}_{i=0}^{N-1}$，用于测量边界误差。\n  $$ \\varphi_i = \\frac{2\\pi i}{N} \\quad \\text{for } i = 0, 1, \\dots, N-1 $$\n  $$ \\tilde{r}_i = r_{\\varepsilon,k}(\\varphi_i) = 1 + \\varepsilon \\cos(k \\varphi_i) $$\n  $$ \\tilde{b}_i = (\\tilde{x}_i, \\tilde{y}_i) = (\\tilde{r}_i \\cos \\varphi_i, \\tilde{r}_i \\sin \\varphi_i) $$\n  这 $N$ 个点存储在一个形状为 $(N, 2)$ 的 NumPy 数组中。\n\n- **步骤 2.3：计算近似距离**\n  对于 $N$ 个评估点中的每一个 $\\tilde{b}_i$，我们计算其到边界的近似距离 $d_M(\\tilde{b}_i)$。这涉及到找到从 $\\tilde{b}_i$ 到 $M$ 个样本点集 $\\{b_j\\}$ 的最小距离。\n  $$ d_M(\\tilde{b}_i) = \\min_{0 \\le j \\le M-1} \\left\\| \\tilde{b}_i - b_j \\right\\|_2 $$\n  这个操作可以被高效地向量化。我们计算一个 $N \\times M$ 的成对欧几里得距离矩阵，包含所有评估点和所有样本点之间的距离。`scipy.spatial.distance.cdist` 函数非常适合这个任务。然后，对每一行（每个评估点）找到最小值。结果是一个包含 $N$ 个距离的向量。\n\n- **步骤 2.4：评估试探场和 RMSE**\n  计算出距离向量 $d_M(\\tilde{b}_i)$ 后，我们为 $N$ 个评估点中的每一个评估多项式项 $p(\\tilde{x}_i, \\tilde{y}_i) = \\tilde{x}_i^2 + \\tilde{y}_i^2 + \\tilde{x}_i \\tilde{y}_i$。然后逐元素计算试探场的值：\n  $$ u_d(\\tilde{b}_i) = d_M(\\tilde{b}_i) \\cdot p(\\tilde{b}_i) $$\n  最后，计算均方根误差：\n  $$ \\mathrm{RMSE}_d = \\sqrt{ \\frac{1}{N} \\sum_{i=0}^{N-1} \\left( u_d(\\tilde{b}_i) \\right)^2 } $$\n  除非一个评估点 $\\tilde{b}_i$ 恰好与某个样本点 $b_j$ 相同，否则距离 $d_M(\\tilde{b}_i)$ 将为非零，导致 $\\mathrm{RMSE}_d > 0$。这个非零误差是离散化边界所引入的几何误差的直接结果。\n\n- **步骤 2.5：特殊情况分析（案例4）**\n  在案例4中，我们有 $M=N=360$。用于采样的角度集合 $\\{\\theta_j\\}$ 与用于评估的角度集合 $\\{\\varphi_i\\}$ 相同。这意味着样本点集 $\\{b_j\\}$ 与评估点集 $\\{\\tilde{b}_i\\}$ 相同。对于任何评估点 $\\tilde{b}_i$，它的确切位置存在于集合 $\\{b_j\\}$ 中。因此，最小距离 $d_M(\\tilde{b}_i)$ 将恰好为 $0$。结果是，对于所有 $i$，$u_d(\\tilde{b}_i) = 0$，并且 $\\mathrm{RMSE}_d = 0$。\n\n**3. 最终比较**\n布尔值 `better` 定义为 $\\mathrm{RMSE}_d  \\mathrm{RMSE}_s$。由于我们已经确定 $\\mathrm{RMSE}_s = 0$，这变成了 $\\mathrm{RMSE}_d  0$。因为 $\\mathrm{RMSE}_d$ 是平方和的平方根，所以它总是一个非负数。\n- 对于案例1、2和3，$M  N$，所以样本点比评估点稀疏。通常，$d_M > 0$，导致 $\\mathrm{RMSE}_d > 0$。比较 $\\mathrm{RMSE}_d  0$ 为 `False`。\n- 对于案例4，我们证明了 $\\mathrm{RMSE}_d = 0$。比较变为 $0  0$，这也是 `False`。\n因此，对于所有测试用例，`better` 的值都应为 `False`，这突出表明，在指定的评估点上，就边界满足性而言，近似距离法永远不会优于解析精确的水平集方法。\n\n如上所述，实现将使用 `numpy` 进行高效的数组操作，并使用 `scipy.spatial.distance.cdist` 进行成对距离计算。根据 NumPy 的默认设置，所有计算都使用双精度浮点算术进行。",
            "answer": "```python\nimport numpy as np\nfrom scipy.spatial import distance\n\ndef solve():\n    \"\"\"\n    Computes and compares boundary constraint violation for two PINN-inspired\n    trial fields on a star-shaped domain for a series of test cases.\n    \"\"\"\n    test_cases = [\n        # (epsilon, k, M, N)\n        (0.20, 3, 24, 360),\n        (0.35, 4, 32, 360),\n        (0.45, 5, 18, 360),\n        (0.30, 3, 360, 360),\n    ]\n\n    results = []\n\n    for eps, k, M, N in test_cases:\n        # Define the boundary radius function r(theta)\n        def r_eps_k(theta):\n            return 1.0 + eps * np.cos(k * theta)\n\n        # Define the polynomial function p(x, y)\n        def p(x, y):\n            # x^2 + y^2 + xy is equivalent to r^2 + r^2 * cos(theta) * sin(theta)\n            # which is r^2 * (1 + 0.5 * sin(2*theta)).\n            # Using the Cartesian form is more direct.\n            return x**2 + y**2 + x * y\n\n        # Strategy 1: Signed Level Set\n        # The signed level set function is zero on the boundary by definition.\n        # Thus, u_s = 0 on the boundary, and RMSE_s is always 0.\n        rmse_s = 0.0\n\n        # Strategy 2: Euclidean Distance from a Discrete Boundary\n        \n        # 1. Generate M discrete boundary sample points (b_j)\n        theta_j = np.linspace(0, 2 * np.pi, M, endpoint=False) # M angles\n        r_j = r_eps_k(theta_j)\n        # Convert polar to Cartesian for the M sample points\n        b_j = np.c_[r_j * np.cos(theta_j), r_j * np.sin(theta_j)]\n\n        # 2. Generate N boundary evaluation points (b_tilde_i)\n        phi_i = np.linspace(0, 2 * np.pi, N, endpoint=False) # N angles\n        r_i_tilde = r_eps_k(phi_i)\n        # Convert polar to Cartesian for the N evaluation points\n        b_i_tilde = np.c_[r_i_tilde * np.cos(phi_i), r_i_tilde * np.sin(phi_i)]\n\n        # 3. Compute the approximate distance d_M for each evaluation point\n        if M  0:\n            # Calculate the NxM matrix of pairwise distances\n            # dist_matrix[i, j] = ||b_i_tilde[i] - b_j[j]||_2\n            dist_matrix = distance.cdist(b_i_tilde, b_j, 'euclidean')\n            # For each evaluation point (row), find the minimum distance to any sample point\n            d_M_values = np.min(dist_matrix, axis=1) # Shape (N,)\n        else: # Handle edge case of M=0\n            d_M_values = np.full(N, np.inf)\n\n        # 4. Evaluate the polynomial p(x,y) at each evaluation point\n        p_values = p(b_i_tilde[:, 0], b_i_tilde[:, 1]) # Shape (N,)\n        \n        # 5. Compute the trial field u_d at each evaluation point\n        u_d_values = d_M_values * p_values\n\n        # 6. Compute the RMSE for the distance-based strategy\n        sum_sq_u_d = np.sum(u_d_values**2)\n        if N  0:\n            rmse_d = np.sqrt(sum_sq_u_d / N)\n        else: # Handle edge case of N=0\n            rmse_d = 0.0\n\n        # Define the boolean 'better'\n        # Since rmse_d is a sqrt of sum of squares, it's non-negative.\n        # rmse_s is 0.0. The comparison rmse_d  rmse_s (i.e., rmse_d  0.0)\n        # can only be False, unless there is some floating point noise\n        # creating a tiny negative number which is not possible here.\n        better = rmse_d  rmse_s\n\n        results.append([rmse_d, rmse_s, better])\n\n    # Format the final output string to match the required format `[[...],[...]]`\n    # The template `','.join(map(str, results))` produces this format without extra spaces.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "当我们能够训练PINN得到一个问题的解之后，一个至关重要的问题随之而来：这个解的可信度有多高？这个高级实践将带你探索如何使用蒙特卡洛丢弃（Monte Carlo dropout）技术来评估PINN的预测不确定性。掌握这项技能对于将PINN从学术练习推向真实的科学与工程应用至关重要，因为在这些领域，提供预测的置信区间是必不可少的。",
            "id": "3410639",
            "problem": "实现一个完整的程序，为一维热方程构建一个带有蒙特卡洛 dropout 的物理启发神经网络 (PINN)，并在保留数据集上通过经验覆盖率校准预测不确定性。控制偏微分方程为单位域 $(x,t) \\in [0,1] \\times [0,1]$ 上的热方程 $u_t = \\alpha u_{xx}$，其狄利克雷边界条件为 $u(0,t) = 0$，$u(1,t) = 0$，初始条件为 $u(x,0) = \\sin(\\pi x)$。这里 $u(x,t)$ 是温度场，$\\alpha > 0$ 是热扩散系数。您必须从第一性原理出发实现以下组件：一个具有双曲正切激活函数的单隐藏层全连接神经网络 $u_\\theta(x,t)$；一个使用热方程残差以及边界和初始条件的物理启发损失函数；以及在推理时使用蒙特卡洛 dropout 来获得预测不确定性。仅使用一致有效的解析解 $u^\\star(x,t) = \\exp(-\\alpha \\pi^2 t)\\sin(\\pi x)$ 来为覆盖率评估生成保留的测试目标。所有量都是无量纲的，因此无需进行物理单位转换。\n\n您的设计应基于以下原则和定义：\n- 热方程 $u_t = \\alpha u_{xx}$ 编码了热量守恒和傅里叶扩散定律；一种物理启发方法会惩罚在配置点上评估该定律得到的均方残差。\n- 物理启发神经网络 (PINN) 是一个神经网络 $u_\\theta(x,t)$，通过最小化一个聚合了数据失配和物理残差的损失函数进行训练。对于本问题，使用以下损失函数\n$$\n\\mathcal{L}(\\theta) = \\lambda_{\\mathrm{pde}} \\, \\mathbb{E}_{(x,t)\\in\\mathcal{C}} \\left[ \\left( \\partial_t u_\\theta(x,t) - \\alpha \\, \\partial_{xx} u_\\theta(x,t) \\right)^2 \\right] + \\lambda_{\\mathrm{ic}} \\, \\mathbb{E}_{x\\in\\mathcal{I}} \\left[ \\left( u_\\theta(x,0) - \\sin(\\pi x) \\right)^2 \\right] + \\lambda_{\\mathrm{bc}} \\, \\left( \\mathbb{E}_{t\\in\\mathcal{B}} \\left[ u_\\theta(0,t)^2 \\right] + \\mathbb{E}_{t\\in\\mathcal{B}} \\left[ u_\\theta(1,t)^2 \\right] \\right),\n$$\n其中 $\\lambda_{\\mathrm{pde}}, \\lambda_{\\mathrm{ic}}, \\lambda_{\\mathrm{bc}}$ 为非负权重，$\\mathcal{C}, \\mathcal{I}, \\mathcal{B}$ 分别为配置、初始和边界采样点的有限集合。\n- 蒙特卡洛 dropout 在推理时使用随机的 dropout 掩码来导出预测分布。对于一个单隐藏层网络，将逐元素的 dropout 掩码 $m$ 应用于隐藏层激活值 $a = \\tanh(z)$，并使用反向缩放，一次前向传播返回 $u_\\theta(x,t)$；重复此过程 $M$ 次，可得到用于计算每个保留点上预测均值和标准差的样本。\n\n需要实现的神经网络模型和导数：\n- 设网络为 $u_\\theta(x,t) = \\mathbf{w}_2^\\top \\left( m \\odot \\tanh(\\mathbf{W}_1 [x,t]^\\top + \\mathbf{b}_1) \\right) + b_2$，其中 $\\mathbf{W}_1 \\in \\mathbb{R}^{H \\times 2}$，$\\mathbf{b}_1 \\in \\mathbb{R}^H$，$\\mathbf{w}_2 \\in \\mathbb{R}^H$，$b_2 \\in \\mathbb{R}$，$H$ 是隐藏层宽度，而 $m \\in \\mathbb{R}^H$ 是一个随机掩码，其条目 $m_i \\in \\{0, 1/(1-p)\\}$，对应 dropout 概率 $p \\in [0,1)$。在蒙特卡洛推理期间，为每个输入样本抽取一个新的 $m$。\n- 物理残差所需的导数必须通过链式法则进行解析计算。记 $s = \\mathbf{W}_1 [x,t]^\\top + \\mathbf{b}_1$，$a = \\tanh(s)$，$\\operatorname{sech}^2(s) = 1 - \\tanh^2(s)$，并用 $\\mathbf{W}_{1x}$ 和 $\\mathbf{W}_{1t}$ 分别表示 $\\mathbf{W}_1$ 对应于 $x$ 和 $t$ 的列。那么对于一个给定的 dropout 掩码 $m$ 和一个隐藏层，实现\n$$\n\\partial_x u_\\theta = \\mathbf{w}_2^\\top \\left( m \\odot \\operatorname{sech}^2(s) \\odot \\mathbf{W}_{1x} \\right), \\quad\n\\partial_t u_\\theta = \\mathbf{w}_2^\\top \\left( m \\odot \\operatorname{sech}^2(s) \\odot \\mathbf{W}_{1t} \\right),\n$$\n$$\n\\partial_{xx} u_\\theta = \\mathbf{w}_2^\\top \\left( m \\odot \\left( -2 \\, \\operatorname{sech}^2(s) \\odot \\tanh(s) \\right) \\odot \\mathbf{W}_{1x} \\odot \\mathbf{W}_{1x} \\right).\n$$\n\n训练和评估协议：\n- 通过在训练期间禁用 dropout 来最小化 $\\mathcal{L}(\\theta)$ 以确保确定性目标，从而训练 PINN。使用小型网络和适度的样本量，以便优化在合理时间内收敛。\n- 在一个保留的测试点网格 $(x,t)$ 上，使用解析解 $u^\\star(x,t)$ 作为目标，评估名义高斯置信区间的经验覆盖率。对于名义水平 $q \\in \\{0.68, 0.95\\}$，使用 z-分数 $z_{0.68} = 1.0$ 和 $z_{0.95} = 1.96$。对于每个测试点，计算蒙特卡洛预测均值 $\\mu$ 和标准差 $\\sigma$，并检查是否满足 $|u^\\star - \\mu| \\le z \\sigma$。经验覆盖率是满足此不等式的保留点所占的比例。如果 $\\sigma = 0$，则将区间视为退化区间；在实现中使用一个小的正值 $\\varepsilon$ 抖动以避免数值问题。\n\n测试套件和要求输出：\n- 使用以下三个测试用例。在所有情况下，时空域为 $[0,1]\\times[0,1]$，隐藏层宽度为 $H = 10$，损失权重为 $\\lambda_{\\mathrm{pde}} = 1$，$\\lambda_{\\mathrm{ic}} = 100$，$\\lambda_{\\mathrm{bc}} = 100$。保留的测试网格应为 $x$ 方向上 $N_x = 20$ 个点和 $t$ 方向上 $N_t = 20$ 个点的均匀网格（总共 $400$ 个点）。对于蒙特卡洛方法，每次前向传播的每个样本使用独立的掩码和反向 dropout 缩放。这三种情况是：\n    - 情况 A (理想情况)：$\\alpha = 0.1$，dropout 概率 $p = 0.1$，蒙特卡洛样本数 $M = 100$。使用 $N_{\\mathcal{C}} = 64$ 个配置点、$N_{\\mathcal{I}} = 32$ 个初始点、$N_{\\mathcal{B}} = 32$ 个边界点进行训练，并优化 $80$ 次迭代。\n    - 情况 B (dropout 敏感性)：重用情况 A 中训练好的权重，不进行进一步优化。使用 dropout 概率 $p = 0.3$，蒙特卡洛样本数 $M = 100$ 进行评估。\n    - 情况 C (改变扩散系数和有限数据)：$\\alpha = 0.2$，dropout 概率 $p = 0.2$，蒙特卡洛样本数 $M = 100$。使用 $N_{\\mathcal{C}} = 32$ 个配置点、$N_{\\mathcal{I}} = 16$ 个初始点、$N_{\\mathcal{B}} = 16$ 个边界点进行训练，并优化 $60$ 次迭代。\n\n您的程序必须：\n- 按照描述实现带有解析导数和物理信息损失的 PINN，并进行确定性训练。\n- 对于每个测试用例，计算两个浮点数：$q=0.68$ 和 $q=0.95$ 的经验覆盖率，按此顺序。\n- 生成单行输出，其中包含六个覆盖率结果，格式为用方括号括起来的逗号分隔列表，顺序为 [A-0.68, A-0.95, B-0.68, B-0.95, C-0.68, C-0.95]。\n\n不允许用户输入；所有常量和随机种子都应在程序内部固定，以保证可复现性。仅使用执行环境中指定的 Python 标准库、NumPy 和 SciPy。在代码中清晰地记录 dropout 的处理、通过链式法则计算导数以及覆盖率指标。本问题陈述中出现的每个数学符号、函数、算子和数字都使用 LaTeX 书写，以确保清晰和精确。",
            "solution": "用户提供的问题是科学机器学习领域中一个定义明确且内容丰富的任务。它要求实现一个物理启发神经网络 (PINN) 来求解一维热方程，并包括一个使用蒙特卡洛 (MC) dropout 进行不确定性量化的机制。问题陈述在科学上是合理的、内部一致且完整的。所有必需的方程、参数和评估协议都已提供，使得该问题完全可形式化且在计算上是可解的。\n\n根据对指定标准的严格检查，该问题是有效的：\n- **科学基础：** 问题的基础是热方程（物理学的基石之一）和 PINN（计算科学中广泛认可的方法论）。提供的解析解和导数公式是正确的。\n- **适定性：** 问题提供了明确的目标（最小化一个已定义的损失函数）和精确的评估程序（经验覆盖率）。三个测试用例所需的所有数据和约束都已指定。\n- **客观性：** 问题使用精确的数学符号和无歧义的指令进行描述，不含任何主观元素。\n- **完整性与一致性：** 虽然优化器的选择没有明确规定，但允许使用 `scipy.optimize`，并且像 L-BFGS-B 这样的标准选择适用于这类问题。迭代次数作为一个明确的停止准则。问题是自洽的，没有矛盾。“迭代次数”的规范被解释为所选 `scipy` 优化器中的 `maxiter` 选项。\n- **可行性：** 网络规模 ($H=10$)、数据集大小和优化迭代次数都被有意地选择得较小，以确保计算可以在标准硬件上于合理的时间内完成，这符合对一个独立脚本的期望。\n\n因此，该问题被认定为 **有效**，并将构建一个完整的解决方案。\n\n### 算法设计与原则\n\n解决方案围绕一个核心的 Python 类 `PINN` 构建，该类封装了神经网络模型及其相关的物理信息逻辑。\n\n1.  **神经网络架构：**\n    PINN 的核心是一个单隐藏层、全连接的神经网络 $u_\\theta(x,t)$，使用双曲正切 ($\\tanh$) 激活函数。参数 $\\theta = \\{\\mathbf{W}_1, \\mathbf{b}_1, \\mathbf{w}_2, b_2\\}$ 使用标准方案（Glorot 初始化）进行初始化，以促进稳定的训练。为了与 `scipy.optimize.minimize` 兼容（该函数对单个变量向量进行操作），实现了辅助方法，将网络的权重矩阵和偏置向量“打包”成一个扁平的一维 NumPy 数组，并再“解包”回去。\n\n2.  **物理信息损失函数：**\n    训练目标是最小化复合损失函数 $\\mathcal{L}(\\theta)$，它是三个均方误差项的加权和：\n    - **PDE 残差损失 ($\\mathcal{L}_{\\mathrm{pde}}$):** 该项强制执行控制物理定律 $u_t - \\alpha u_{xx} = 0$。它在一组从时空域 $[0,1] \\times [0,1]$ 内部采样的配置点 $\\mathcal{C}$ 上进行评估。\n    - **初始条件损失 ($\\mathcal{L}_{\\mathrm{ic}}$):** 该项惩罚与已知初始状态 $u(x,0) = \\sin(\\pi x)$ 的偏差，在一组位于时间 $t=0$ 的点 $\\mathcal{I}$ 上计算。\n    - **边界条件损失 ($\\mathcal{L}_{\\mathrm{bc}}$):** 该项在沿空间边界 $x=0$ 和 $x=1$ 采样的点上强制执行狄利克雷边界条件 $u(0,t)=0$ 和 $u(1,t)=0$。\n\n    PDE 残差所需的导数 $\\partial_t u_\\theta$ 和 $\\partial_{xx} u_\\theta$ 使用问题陈述中指定的链式法则进行解析计算。这些计算在一个专用方法中实现，确保了正确性和效率。按照要求，在训练的损失计算期间禁用 dropout，为优化器提供一个确定性的目标函数。\n\n3.  **优化：**\n    使用 L-BFGS-B 算法进行优化，这是 `scipy.optimize.minimize` 中提供的一种拟牛顿法。它非常适合于本问题中的少量参数（$4H+1 = 41$），并且对于 PINN，其收敛特性通常优于一阶方法。优化器迭代地调整扁平化的参数向量以最小化损失函数 $\\mathcal{L}(\\theta)$。迭代次数由 `maxiter` 选项控制，与每个测试用例的指定一致。\n\n4.  **通过蒙特卡洛 Dropout 进行不确定性量化：**\n    训练后，使用 MC dropout 评估模型的预测不确定性。在推理时，启用 dropout。对于保留测试网格上的每个点，执行 $M$ 次随机前向传播。在每次传播中，将一个不同的随机 dropout 掩码 $m$ 应用于隐藏层的激活值。此过程为每个测试点生成一个包含 $M$ 个预测的集成。\n    - **预测均值** ($\\mu$) 是 $M$ 个预测的平均值。\n    - **预测标准差** ($\\sigma$) 量化了模型的不确定性，由集成中的变异性表示。\n\n5.  **经验覆盖率评估：**\n    通过计算经验覆盖率来校准不确定性估计的质量。对于测试网格上的每个点，我们构建一个名义置信区间 $[\\mu - z\\sigma, \\mu + z\\sigma]$，其中 $z$ 是对应于所需置信水平 $q$ 的标准正态分布的临界值（例如，对于 $q=0.68$，$z=1.0$）。然后我们检查真实的解析解 $u^\\star(x,t)$ 是否落在此区间内。经验覆盖率是满足此条件的测试点所占的比例。该指标评估模型的报告不确定性是否经过良好校准；例如，一个 95% 的名义区间理想情况下应有 95% 的时间包含真实值。在 $\\sigma$ 中加入一个小的抖动 $\\varepsilon$ 以防止在预测方差为零的情况下出现除以零的错误。\n\n该实现遵循三个指定的测试用例，在不同的热扩散系数 ($\\alpha$)、dropout 概率 ($p$) 和训练数据大小的条件下训练和/或评估模型，并以所需的精确格式报告最终的覆盖率指标。固定的随机种子确保了所有随机操作（包括权重初始化、数据采样和 dropout 掩码）的可复现性。",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Implements a PINN for the 1D heat equation, trains it, and evaluates\n    predictive uncertainty calibration using Monte Carlo dropout.\n    \"\"\"\n    # Fix random seed for reproducibility of weight initialization, data sampling, and dropout masks.\n    np.random.seed(42)\n\n    class PINN:\n        \"\"\"\n        Physics-Informed Neural Network for the 1D Heat Equation.\n        Implements the network architecture, analytical derivatives, loss function,\n        and prediction with Monte Carlo dropout.\n        \"\"\"\n        def __init__(self, H=10):\n            self.H = H\n            # Glorot initialization for weights\n            self.W1 = np.random.randn(self.H, 2) * np.sqrt(2 / (2 + self.H))\n            self.b1 = np.zeros((1, self.H))\n            self.w2 = np.random.randn(self.H, 1) * np.sqrt(2 / (self.H + 1))\n            self.b2 = np.zeros((1, 1))\n\n        def pack_params(self):\n            \"\"\"Flattens all model parameters into a single 1D array for the optimizer.\"\"\"\n            return np.concatenate([self.W1.flatten(), self.b1.flatten(), self.w2.flatten(), self.b2.flatten()])\n\n        def unpack_params(self, params_vec):\n            \"\"\"Unpacks a 1D array of parameters back into the model's weight/bias attributes.\"\"\"\n            ptr = 0\n            self.W1 = params_vec[ptr:ptr + self.H * 2].reshape(self.H, 2)\n            ptr += self.H * 2\n            self.b1 = params_vec[ptr:ptr + self.H].reshape(1, self.H)\n            ptr += self.H\n            self.w2 = params_vec[ptr:ptr + self.H].reshape(self.H, 1)\n            ptr += self.H\n            self.b2 = params_vec[ptr:ptr + 1].reshape(1, 1)\n\n        def forward(self, x, t, p_drop=0.0):\n            \"\"\"\n            Performs a forward pass through the network.\n            Handles training (p_drop=0) and inference with MC dropout (p_drop0).\n            \"\"\"\n            X = np.hstack((x, t))\n            N = X.shape[0]\n            \n            s = X @ self.W1.T + self.b1\n            a = np.tanh(s)\n            \n            # Apply dropout if in inference mode (p_drop  0)\n            if p_drop  0:\n                # Inverted dropout: scale by 1/(1-p) during training is standard,\n                # but here we apply it at inference time as per the problem.\n                # A fresh mask is drawn for each sample in the batch.\n                scale = 1.0 / (1.0 - p_drop)\n                mask = np.random.binomial(1, 1.0 - p_drop, size=(N, self.H)) * scale\n                a = a * mask\n            \n            # If p_drop is 0 (training), the mask is effectively all ones.\n            u = a @ self.w2 + self.b2\n            return u.flatten()\n\n        def get_derivatives(self, x, t):\n            \"\"\"\n            Computes the network output and its analytical derivatives w.r.t. x and t.\n            Dropout is disabled (mask 'm' is effectively all ones).\n            \"\"\"\n            X = np.hstack((x, t))\n            s = X @ self.W1.T + self.b1\n            a = np.tanh(s)\n            sech2_s = 1.0 - a**2\n\n            # Extract columns of W1 corresponding to x and t\n            W1x = self.W1[:, 0:1].T  # Shape (1, H)\n            W1t = self.W1[:, 1:2].T  # Shape (1, H)\n\n            # First derivatives (chain rule)\n            u_t = (sech2_s * W1t) @ self.w2\n            u_x = (sech2_s * W1x) @ self.w2\n\n            # Second derivative (chain rule)\n            term_xx = -2.0 * sech2_s * a\n            u_xx = (term_xx * W1x * W1x) @ self.w2\n\n            return u_t.flatten(), u_x.flatten(), u_xx.flatten()\n\n        def loss_function(self, params_vec, alpha, points, lambdas):\n            \"\"\"\n            The objective function for the optimizer.\n            Calculates the weighted sum of PDE, initial, and boundary condition losses.\n            \"\"\"\n            self.unpack_params(params_vec)\n            \n            # Unpack points\n            x_c, t_c = points['c']\n            x_i, t_i = points['i']\n            x_b, t_b = points['b']\n\n            # Unpack loss weights\n            lambda_pde, lambda_ic, lambda_bc = lambdas\n\n            # 1. PDE Residual Loss\n            u_t_c, _, u_xx_c = self.get_derivatives(x_c, t_c)\n            pde_residual = u_t_c - alpha * u_xx_c\n            loss_pde = lambda_pde * np.mean(pde_residual**2)\n\n            # 2. Initial Condition Loss\n            u_i = self.forward(x_i, t_i, p_drop=0.0)\n            ic_target = np.sin(np.pi * x_i.flatten())\n            loss_ic = lambda_ic * np.mean((u_i - ic_target)**2)\n            \n            # 3. Boundary Condition Loss\n            u_b = self.forward(x_b, t_b, p_drop=0.0)\n            loss_bc = lambda_bc * np.mean(u_b**2)\n            \n            return loss_pde + loss_ic + loss_bc\n\n    def train_pinn(H, alpha, N_c, N_i, N_b, iterations, lambdas):\n        \"\"\"\n        Initializes and trains a PINN model.\n        \n        Returns:\n            The trained model parameters as a 1D vector.\n        \"\"\"\n        # Initialize the model\n        model = PINN(H=H)\n        \n        # Generate training points\n        # Collocation points (domain interior)\n        x_c = np.random.rand(N_c, 1)\n        t_c = np.random.rand(N_c, 1)\n        \n        # Initial condition points (t=0)\n        x_i = np.random.rand(N_i, 1)\n        t_i = np.zeros_like(x_i)\n        \n        # Boundary condition points (x=0 and x=1)\n        t_b0 = np.random.rand(N_b // 2, 1)\n        t_b1 = np.random.rand(N_b // 2, 1)\n        x_b = np.vstack([np.zeros_like(t_b0), np.ones_like(t_b1)])\n        t_b = np.vstack([t_b0, t_b1])\n\n        points = {'c': (x_c, t_c), 'i': (x_i, t_i), 'b': (x_b, t_b)}\n        \n        # Initial parameter vector\n        p0 = model.pack_params()\n        \n        # Optimizer call\n        res = minimize(\n            fun=model.loss_function,\n            x0=p0,\n            args=(alpha, points, lambdas),\n            method='L-BFGS-B',\n            options={'maxiter': iterations}\n        )\n        return res.x\n        \n    def evaluate_coverage(params_vec, H, alpha, p_drop, M):\n        \"\"\"\n        Evaluates the empirical coverage of the model's uncertainty estimates.\n        \"\"\"\n        # Create a model with the trained parameters\n        model = PINN(H=H)\n        model.unpack_params(params_vec)\n        \n        # Create the held-out test grid\n        N_x, N_t = 20, 20\n        x_space = np.linspace(0, 1, N_x)\n        t_space = np.linspace(0, 1, N_t)\n        x_grid, t_grid = np.meshgrid(x_space, t_space)\n        x_flat, t_flat = x_grid.flatten()[:, None], t_grid.flatten()[:, None]\n        \n        # Perform Monte Carlo predictions\n        predictions = np.zeros((M, x_flat.shape[0]))\n        for i in range(M):\n            predictions[i, :] = model.forward(x_flat, t_flat, p_drop=p_drop)\n            \n        # Compute predictive mean and standard deviation\n        mu = np.mean(predictions, axis=0)\n        # Add a small epsilon for numerical stability if sigma is zero\n        sigma = np.std(predictions, axis=0) + 1e-8 \n        \n        # Get analytical solution for comparison\n        u_star = np.exp(-alpha * np.pi**2 * t_flat.flatten()) * np.sin(np.pi * x_flat.flatten())\n        \n        # Calculate empirical coverage for two confidence levels\n        z_scores = {'0.68': 1.0, '0.95': 1.96}\n        abs_error = np.abs(u_star - mu)\n        \n        coverage_68 = np.mean(abs_error = z_scores['0.68'] * sigma)\n        coverage_95 = np.mean(abs_error = z_scores['0.95'] * sigma)\n        \n        return coverage_68, coverage_95\n\n    # --- Test Case Execution ---\n    \n    H = 10\n    lambdas = (1.0, 100.0, 100.0) # (lambda_pde, lambda_ic, lambda_bc)\n    results = []\n\n    # Case A: Happy path\n    alpha_A = 0.1\n    p_A = 0.1\n    M_A = 100\n    trained_params_A = train_pinn(H, alpha_A, N_c=64, N_i=32, N_b=32, iterations=80, lambdas=lambdas)\n    cov_68_A, cov_95_A = evaluate_coverage(trained_params_A, H, alpha_A, p_A, M_A)\n    results.extend([cov_68_A, cov_95_A])\n    \n    # Case B: Dropout sensitivity (uses weights from A)\n    alpha_B = 0.1 # alpha is the same for the physics\n    p_B = 0.3\n    M_B = 100\n    cov_68_B, cov_95_B = evaluate_coverage(trained_params_A, H, alpha_B, p_B, M_B)\n    results.extend([cov_68_B, cov_95_B])\n\n    # Case C: Changed diffusivity and limited data\n    alpha_C = 0.2\n    p_C = 0.2\n    M_C = 100\n    trained_params_C = train_pinn(H, alpha_C, N_c=32, N_i=16, N_b=16, iterations=60, lambdas=lambdas)\n    cov_68_C, cov_95_C = evaluate_coverage(trained_params_C, H, alpha_C, p_C, M_C)\n    results.extend([cov_68_C, cov_95_C])\n    \n    # Print the final results in the required format\n    print(f\"[{','.join(f'{r:.4f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}