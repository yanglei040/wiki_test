## 引言
近年来，机器学习与[科学计算](@entry_id:143987)的深度融合正在催生一场科研[范式](@entry_id:161181)的变革。在这场变革的前沿，物理信息神经网络（Physics-Informed Neural Networks, [PINNs](@entry_id:145229)）和数据驱动的科学发现方法正作为一种强大的新工具崭露头角。传统的数值模拟方法在面对稀疏、含噪声的观测数据时常常力不从心，而纯粹的数据驱动模型又因缺乏物理约束而难以保证其预测的物理一致性和泛化能力。物理信息学习正是为了弥合这一鸿沟而生，它巧妙地将物理学第一性原理作为一种强大的[归纳偏置](@entry_id:137419)，直接注入到[机器学习模型](@entry_id:262335)的学习过程之中。

本文旨在为读者提供一个关于[PINNs](@entry_id:145229)及其在科学发现中应用的全面而深入的指南。我们将从基本原理出发，逐步深入到前沿应用和实践挑战。在“**原理与机制**”一章中，我们将解构PINN的核心架构，阐明其如何通过[自动微分](@entry_id:144512)和复合损失函数将数据与物理定律相结合，并探讨其训练动态与潜在的失效模式。随后，在“**应用与跨学科联系**”一章中，我们将展示这些方法在解决[参数识别](@entry_id:275549)、本构关系学习、控制方程发现等复杂[逆问题](@entry_id:143129)时的强大威力，并探索其与地球物理、医学成像等多个学科的[交叉](@entry_id:147634)融合。最后，通过“**动手实践**”部分，您将有机会亲手实现并验证这些强大的概念，将理论知识转化为解决实际问题的能力。通过这一系列的学习，您将不仅掌握一种新的计算方法，更将获得一种融合理论、数据与计算的全新科学探究视角。

## 原理与机制

本章旨在深入剖析[物理信息神经网络](@entry_id:145229)（Physics-Informed Neural Networks, [PINNs](@entry_id:145229)）的核心工作原理与基本机制。在前一章介绍其背景与动机的基础上，我们将系统性地拆解PINN的架构，阐明其各个组成部分的功能，并探讨其在训练动态、约束处理以及从数据中发现科学定律时所面临的理论挑战与解决方案。

### PINN的核心架构

物理信息神经网络的根本思想是将一个连续的、可[微分](@entry_id:158718)的函数——[神经网](@entry_id:276355)络——作为[偏微分方程](@entry_id:141332)（Partial Differential Equation, PDE）解的代理模型。通过优化一个复合损失函数，该网络不仅要拟合观测数据，还必须在其定义域内遵守指定的物理定律。这一框架主要由三大支柱构成：[神经网](@entry_id:276355)络[函数逼近](@entry_id:141329)器、[自动微分](@entry_id:144512)以及复合[损失函数](@entry_id:634569)。

#### [神经网](@entry_id:276355)络作为[函数逼近](@entry_id:141329)器

PINN的核心是一个参数为 $\theta$ 的[神经网](@entry_id:276355)络，它将时空坐标（例如，$(x, t)$）映射到物理场的值 $u_\theta(x, t)$。与传统的、在离散网格上求解数值解的方法不同，PINN提供了一个在整个连续域上定义的解析解的近似。这个代理函数 $u_\theta$ 具备无限阶可微的潜力（取决于[激活函数](@entry_id:141784)的选择），这为精确计算PDE所需的导数提供了基础。

#### [自动微分](@entry_id:144512)：精确计算物理残差

为了评估[神经网](@entry_id:276355)络 $u_\theta$ 在多大程度上满足一个给定的PDE，我们必须计算其关于输入坐标的[偏导数](@entry_id:146280)。传统数值方法，如**[有限差分](@entry_id:167874)**（Finite Differences），通过离散点上的函数值来近似导数，这不可避免地引入了**截断误差**（Truncation Error）。截断误差的大小与离散步长 $h$ 和解的光滑度有关。

例如，考虑用[中心差分格式](@entry_id:747203)来近似函数 $u(x)$ 的[二阶导数](@entry_id:144508)：
$$
\mathcal{D}_h u(x) \equiv \frac{u(x+h)-2u(x)+u(x-h)}{h^{2}}
$$
对于一个[光滑函数](@entry_id:267124)，如 $u(x) = \sin(\pi x)$，其精确的[二阶导数](@entry_id:144508)为 $u_{xx}(x) = -\pi^2 \sin(\pi x)$。通过[泰勒展开](@entry_id:145057)可以证明，该差分近似的[截断误差](@entry_id:140949)是 $D_h(x) = \mathcal{D}_h u(x) - u_{xx}(x) = \frac{h^2}{12}u^{(4)}(x) + \mathcal{O}(h^4)$。这意味着即使对于[光滑函数](@entry_id:267124)，有限差分也存在固有的、与离散化尺度相关的误差 。

PINN通过采用**[自动微分](@entry_id:144512)**（Automatic Differentiation, AD）彻底规避了这个问题。AD是一种计算技术，它通过在计算机程序执行过程中系统地应用[链式法则](@entry_id:190743)，来精确计算函数（在此即为[神经网](@entry_id:276355)络）的导数，其精度可达机器精度。与[数值微分](@entry_id:144452)不同，AD没有截断误差；与[符号微分](@entry_id:177213)不同，它不会导致表达式的爆炸性增长。对于PINN，$u_\theta$ 的计算过程是一个由基本运算构成的[计算图](@entry_id:636350)。AD能够反向遍历这个图，从而精确地计算出 $\partial_t u_\theta, \partial_x u_\theta, \partial_{xx} u_\theta$ 等任意阶导数。这一特性是PINN能够将PDE作为“软约束”[嵌入学习](@entry_id:637654)过程的关键技术前提。

#### 复合损失函数：数据与物理的融合

PINN的训练过程是通过最小化一个精心设计的**复合损失函数** $J(\theta)$ 来驱动的。这个[损失函数](@entry_id:634569)通常是多个分量的加权和，主要包括**数据损失**和**物理损失**。

$$
J(\theta) = \lambda_{\text{data}} \mathcal{L}_{\text{data}}(\theta) + \lambda_{\text{phys}} \mathcal{L}_{\text{phys}}(\theta)
$$

其中 $\lambda_{\text{data}}$ 和 $\lambda_{\text{phys}}$ 是用于平衡不同损失项贡献的超参数。

### 解构[损失函数](@entry_id:634569)

为了深刻理解PINN的学习机制，我们必须从统计和物理的角度剖析其[损失函数](@entry_id:634569)的各个组成部分。

#### 数据损失的统计诠释

数据损失 $\mathcal{L}_{\text{data}}$ 量化了模型预测与观测数据之间的差异。假设我们有一组带噪声的观测数据 $\mathcal{D} = \{(x_i,t_i,y_i)\}_{i=1}^N$，其[生成模型](@entry_id:177561)为 $y_i = u(x_i,t_i) + \eta_i$，其中 $u$ 是真实的物理场，$\eta_i$ 是测量噪声。

最常用的数据损失是**[均方误差](@entry_id:175403)**（Mean Squared Error, MSE）：
$$
\mathcal{L}_{\text{data}}(\theta) = \frac{1}{N} \sum_{i=1}^N (y_i - u_\theta(x_i, t_i))^2
$$
从统计学的角度看，这并非一个随意的选择。如果我们假设噪声 $\eta_i$ 是[独立同分布](@entry_id:169067)的，且服从均值为零的高斯分布（即 $\eta_i \sim \mathcal{N}(0, \sigma^2)$），那么最小化[均方误差](@entry_id:175403)等价于**[最大似然估计](@entry_id:142509)**（Maximum Likelihood Estimation, MLE）。因此，数据损失项的目标信号是观测到的数据 $y_i$，其形式根植于对测量过程噪声模型的假设 。

#### 物理损失：编码先验知识

物理损失 $\mathcal{L}_{\text{phys}}$ 是PINN的核心创新。它通过惩罚[神经网](@entry_id:276355)络解对控制方程的违反程度，将物理先验知识编码到模型中。对于一个形如 $\mathcal{F}(u; x, t) = 0$ 的PDE，其**物理残差**定义为 $r_\theta(x, t) = \mathcal{F}(u_\theta; x, t)$。物理损失通常是残差在定义域内一系列**[配置点](@entry_id:169000)**（collocation points）上的均方值：
$$
\mathcal{L}_{\text{phys}}(\theta) = \frac{1}{N_c} \sum_{j=1}^{N_c} |r_\theta(x_j^c, t_j^c)|^2
$$
在实际应用中，为了提高训练的稳定性和效率，通常会对网络的输入和输出进行归一化处理。例如，将物理坐标 $(x, t)$ 和物理场 $u$ 分别变换为归一化坐标 $(\tilde{x}, \tilde{t})$ 和归一化场 $\tilde{u}$。此时，物理残差必须通过链式法则，用归一化变量的导数来正确表示。

考虑一维粘性[Burgers方程](@entry_id:177995) $u_t + u u_x = \nu u_{xx}$，其中待求解的 $u(x, t)$ 和待推断的粘性系数 $\nu$ 是物理量。假设输入归一化为 $\tilde{x} = (x - \mu_x)/\sigma_x$, $\tilde{t} = (t - \mu_t)/\sigma_t$，网络输出与物理场的变换关系为 $u(x,t) = \mu_u + \sigma_u \tilde{u}_\theta(\tilde{x},\tilde{t})$。[自动微分](@entry_id:144512)计算的是 $\tilde{u}_\theta$ 对 $(\tilde{x}, \tilde{t})$ 的导数。通过[链式法则](@entry_id:190743)，我们可以推导出物理残差的正确形式 ：
$$
\frac{\partial u}{\partial t} = \frac{\sigma_u}{\sigma_t} \frac{\partial \tilde{u}_\theta}{\partial \tilde{t}}, \quad \frac{\partial u}{\partial x} = \frac{\sigma_u}{\sigma_x} \frac{\partial \tilde{u}_\theta}{\partial \tilde{x}}, \quad \frac{\partial^2 u}{\partial x^2} = \frac{\sigma_u}{\sigma_x^2} \frac{\partial^2 \tilde{u}_\theta}{\partial \tilde{x}^2}
$$
代入原方程，得到以归一化导数表示的物理残差 $r_\theta(x,t;\nu)$：
$$
r_\theta(x,t;\nu) = \frac{\sigma_u}{\sigma_t}\frac{\partial \tilde{u}_\theta}{\partial \tilde{t}} + (\mu_u + \sigma_u \tilde{u}_\theta)\frac{\sigma_u}{\sigma_x}\frac{\partial \tilde{u}_\theta}{\partial \tilde{x}} - \nu\frac{\sigma_u}{\sigma_x^2}\frac{\partial^2 \tilde{u}_\theta}{\partial \tilde{x}^2}
$$
这个表达式才是物理损失中需要被最小化的对象。

物理损失项的统计诠释有两种主流观点 ：
1.  **随机PD[E模](@entry_id:160271)型**：可以假设物理定律本身存在一个随机[源项](@entry_id:269111)，即 $\mathcal{F}(u; x, t) = \epsilon(x,t)$，其中 $\epsilon$ 是一个均值为零的[随机场](@entry_id:177952)。如果假设在[配置点](@entry_id:169000)上的 $\epsilon$ 服从高斯分布，则最小化平方残差等价于在此随机PDE模型下的最大似然估计。
2.  **[贝叶斯先验](@entry_id:183712)**：在贝叶斯框架下，物理损失可以被看作是对[函数空间](@entry_id:143478)施加的一个**先验**（Prior）。这个先验使得满足PDE的函数具有更高的概率密度。具体而言，总[损失函数](@entry_id:634569)与参数的负对数[后验概率](@entry_id:153467)成正比，其中数据损失对应[负对数似然](@entry_id:637801)，而物理损失对应负对数先验。这种观点将物理定律视为对解的函数形式的强力约束，而非另一组“数据”。

#### [能量泛函](@entry_id:170311)与[变分形式](@entry_id:166033)

对于某些类型的PDE，物理损失可以采用一种更深刻的形式：**[能量泛函](@entry_id:170311)**（Energy Functional）。许多物理系统都可以通过一个[能量泛函](@entry_id:170311)的最小化来描述，这正是[变分法](@entry_id:163656)的核心思想。例如，对于[源项](@entry_id:269111)为 $f(x)$ 且具有齐次[狄利克雷边界条件](@entry_id:173524)的泊松问题 $- \frac{d}{dx}(k(x) \frac{du}{dx}) = f(x)$，其解等价于最小化以下[能量泛函](@entry_id:170311)的函数 $u(x)$ ：
$$
\mathcal{E}[u] = \int_{0}^{1} \left( \frac{1}{2} k(x) |u'(x)|^2 - f(x) u(x) \right) dx
$$
在这种情况下，PINN的物理损失可以直接设为 $\mathcal{L}_{\text{phys}}(\theta) = \mathcal{E}[u_\theta]$。这种方法的优点在于，它通常只需要计算解的[一阶导数](@entry_id:749425)，而强形式的残差则需要[二阶导数](@entry_id:144508)。通过[分部积分](@entry_id:136350)可知，最小化能量泛函等价于[求解PDE](@entry_id:138485)的**[弱形式](@entry_id:142897)**（Weak Form）。这不仅降低了对网络光滑度的要求，也使得模型对于解中存在尖锐梯度或不连续性的问题更加鲁棒。这种基于能量或弱形式的PINN，有时被称为**[变分PINN](@entry_id:756443)**（Variational [PINNs](@entry_id:145229), VPINNs）。

### 施加约束：边界与初始条件

除了PDE本身，解还必须满足给定的边界条件（Boundary Conditions, BCs）和初始条件（Initial Conditions, ICs）。在PINN框架中，施加这些约束主要有两种策略：**[罚函数法](@entry_id:636090)**（软约束）和**构造法**（硬约束）。

#### 罚函数法 (软约束)

这是最直接的方法，即将边界/初始条件的残差作为额外的惩罚项加入到总[损失函数](@entry_id:634569)中。例如，对于一个在边界 $\partial\Omega$ 上满足 $u=g$ 的狄利克雷边界条件，可以添加一个损失项：
$$
\mathcal{L}_{\text{BC}}(\theta) = \frac{1}{N_b} \sum_{j=1}^{N_b} |u_\theta(x_j^b) - g(x_j^b)|^2
$$
其中 $\{x_j^b\}$ 是从边界[上采样](@entry_id:275608)的点。总[损失函数](@entry_id:634569)变为 $J(\theta) = \lambda_{\text{data}} \mathcal{L}_{\text{data}} + \lambda_{\text{phys}} \mathcal{L}_{\text{phys}} + \lambda_{\text{BC}} \mathcal{L}_{\text{BC}}$。

[罚函数法](@entry_id:636090)的优点是通用性强，适用于各种类型的约束。然而，它也引入了一个关键的挑战：权重 $\lambda_{\text{BC}}$ 的选择。如果 $\lambda_{\text{BC}}$ 太小，边界条件可能无法被很好地满足；如果太大，则会导致[优化问题](@entry_id:266749)变得**病态**（ill-conditioned）。从[数值优化](@entry_id:138060)的角度看，当罚函数权重 $\alpha$ (即 $\lambda_{BC}$) 趋于无穷大时，[优化问题](@entry_id:266749)的Hessian矩阵 $H_{\alpha}$ 的条件数 $\kappa_2(H_{\alpha})$ 会随 $\alpha$ 线性增长。这会导致优化过程变得**刚性**（stiff），梯度更新将主要致力于减小边界残差，而物理残差的收敛则会变得极其缓慢  。一个更稳健但实现更复杂的替代方案是**拉格朗日乘子法**，它通过引入乘子变量将约束转化为一个[鞍点问题](@entry_id:174221)，避免了[罚函数](@entry_id:638029)权重带来的病态性 。

#### 构造法 (硬约束)

另一种更优雅的策略是通过特别设计[神经网](@entry_id:276355)络的结构，使其输出**自动满足**（satisfy by construction）边界或初始条件。这种方法将约束作为一种强的[归纳偏置](@entry_id:137419)硬编码到模型中。

例如，对于一维问题 $y''(x) = \sin(\pi x)$，边界条件为 $y(0)=0, y(1)=0$。我们可以构造一个代理模型 ：
$$
y_c(x) = x(1-x) \mathcal{N}_\theta(x)
$$
其中 $\mathcal{N}_\theta(x)$ 是一个标准的[神经网](@entry_id:276355)络。无论 $\mathcal{N}_\theta(x)$ 的输出是什么，乘子 $x(1-x)$ 都保证了 $y_c(0)=0$ 和 $y_c(1)=0$。这样，[损失函数](@entry_id:634569)中就不再需要边界惩罚项，优化器可以专注于最小化物理残差。

构造法的优点是确保了约束的精确满足，并且避免了罚函数权重调整的麻烦。其缺点在于它不具备通用性，需要为不同类型的边界条件（如诺伊曼或[混合边界条件](@entry_id:176456)）设计不同的构造形式，这可能非常复杂甚至不可行。

### 训练动态、失效模式与缓解策略

构建了PINN的架构和损失函数后，训练过程本身也充满了挑战。其收敛行为深刻地受到PDE算子性质和[神经网](@entry_id:276355)络自身[归纳偏置](@entry_id:137419)的影响。

#### PDE算子的影响

不同类型的PDE（椭圆型、抛物型、双曲型）具有截然不同的数学性质，这直接转化为PINN训练的难易程度。我们可以通过[分析算子](@entry_id:746429) $L$ 对应的 $L^*L$（其中 $L^*$ 是 $L$ 的伴随算子）的谱特性来理解这一点，因为 $L^*L$ 主导了物理损失项Hessian矩阵的结构 。

- **椭圆型方程** (如[泊松方程](@entry_id:143763) $L_e u = -\partial_{xx} u$)：这类算子通常是**强制的**（coercive），$L_e^*L_e$ 的[特征值](@entry_id:154894)谱远离零点且随频率增高而快速增长（如 $k^4$）。这意味着[损失函数](@entry_id:634569)在所有非平凡方向上都有正曲率，有利于梯度优化。
- **[抛物型方程](@entry_id:144670)** (如热传导方程 $L_p u = \partial_t u - \nu \partial_{xx} u$)：这类算子是**耗散的**（dissipative）。$L_p^*L_p$ 的[特征值](@entry_id:154894)谱同样远离零点（如 $\nu^2 k^4 + \omega^2$），确保了除[平凡解](@entry_id:155162)外所有模式都会被阻尼。这同样为优化提供了良好的梯度信息。
- **[双曲型方程](@entry_id:145657)** (如波动方程 $L_h u = \partial_{tt} u - c^2 \partial_{xx} u$)：这类算子是**[能量守恒](@entry_id:140514)的**，而非耗散或平滑的。$L_h^*L_h$ 的谱中存在大量的零点或接近零的[特征值](@entry_id:154894)，它们对应于满足[色散关系](@entry_id:140395) $\omega^2 = c^2k^2$ 的传播波模式。对于这些模式，物理残差为零，导致[损失函数](@entry_id:634569)的梯度也为零。这在[损失景观](@entry_id:635571)中形成了巨大的平坦区域或狭窄的“峡谷”，使得[基于梯度的优化](@entry_id:169228)方法极难取得进展，导致收敛缓慢、不稳定甚至停滞。

#### 谱偏置与高频失效模式

标准的PINN，尤其是使用光滑[激活函数](@entry_id:141784)（如 `[tanh](@entry_id:636446)`）的，表现出一种被称为**谱偏置**（Spectral Bias）的[归纳偏置](@entry_id:137419)：它们在训练初期优先学习目标函数的低频分量，而高频分量的学习则要慢得多。这一现象也被称为**频率原则**（Frequency Principle）。

当需要求解的物理问题包含高频特征时，如[对流](@entry_id:141806)主导问题中的**尖锐锋面**或激波时，谱偏置就成了一个严重障碍。在[对流](@entry_id:141806)远大于[扩散](@entry_id:141445)（即佩克莱数 $Pe \gg 1$）的情况下，解的锋面非常窄，在空间上对应高频成分。标准的PINN训练会优先拟合锋面以外的大片平滑区域，而难以捕捉到锋面处的剧烈变化。此外，如果采用均匀采样[配置点](@entry_id:169000)，位于狭窄锋面区域的点的数量会非常少，使得来自锋面的[误差信号](@entry_id:271594)在总损失中被“稀释”，进一步加剧了这一问题 。

#### 缓解策略

针对谱偏置和高频失效问题，研究者们提出了一系列缓解策略：
1.  **自适应采样**（Adaptive Sampling）：不再均匀采样[配置点](@entry_id:169000)，而是集中在物理残差较大的区域进行采样。例如，通过**基于残差的[自适应加密](@entry_id:746260)**（Residual-based Adaptive Refinement, RAR）等方法，可以在训练过程中动态地将更多计算资源分配给模型难以拟合的区域（如锋面），从而放大来自这些关键区域的梯度信号，迫使网络学习高频特征 。
2.  **弱形式/[变分PINN](@entry_id:756443)**：如前所述，采用[能量泛函](@entry_id:170311)或PDE的[弱形式](@entry_id:142897)作为物理损失，可以将[高阶导数](@entry_id:140882)转移到光滑的测试函数上，从而降低对网络输出光滑度的要求，使其更善于处理具有尖锐梯度的解 。
3.  **架构修改**：设计新的网络架构或[激活函数](@entry_id:141784)，以减轻谱偏置。例如，使用具有多尺度特征的傅里叶特征网络，或设计能够更好地表达高频信息的[激活函数](@entry_id:141784)。

### 从预测到科学发现

PINN的终极愿景不仅是求解已知的PDE，更是从观测数据中**发现**未知的物理定律。这是一个更具挑战性的[逆问题](@entry_id:143129)，即所谓的**算子发现**（Operator Discovery）。然而，从“拟合良好”到“可信发现”之间存在巨大的鸿沟，需要严谨的验证原则。

#### 伪发现的挑战

假设我们试图从数据中发现一个形如 $\partial_t u = \mathcal{L}_\xi[u]$ 的算子，其中 $\mathcal{L}_\xi$ 是从一个包含大量候选物理项（如 $u, u^2, u_x, u u_x, u_{xx}, \dots$）的库中通过[稀疏回归](@entry_id:276495)选出的。此时，我们面临**模型误定**（Model Misspecification）和**[归纳偏置](@entry_id:137419)**相互作用可能导致的**伪发现**（Spurious Discovery）的风险 。

例如，如果训练数据仅来自单一的实验环境，而该环境中的某些非物理因素（如测量噪声的特定模式）与候选库中的某个非物理项（如 $u^2$）恰好存在相关性，那么[优化算法](@entry_id:147840)可能会错误地选择这个非物理项，因为它有助于减小[训练集](@entry_id:636396)上的总损失。即使模型在与训练数据同[分布](@entry_id:182848)的[测试集](@entry_id:637546)上表现良好，这个被“发现”的定律也是错误的。

#### 稳健的验证原则

为了将一个数据驱动的模型提升为一项科学解释，而不仅仅是预测工具，它必须通过一系列严苛的验证协议，证明其捕捉到了系统内在的、可移植的机制 。

1.  **[参数可辨识性](@entry_id:197485)**（Identifiability）：首先必须确保，在理想的无噪声数据下，模型的参数（即所发现定律的系数）是唯一可确定的。如果不同的参数组合能够产生完全相同的观测数据，那么即使模型预测准确，我们也无法对“发现”的物理机制给出唯一、可信的解释 。

2.  **跨环境不变性与可移植性**：真正的物理定律应该在实验条件改变时保持不变。一个关键的验证步骤是，将在一个或多个环境下训练得到的模型，在全新的、[分布](@entry_id:182848)外的（out-of-distribution）环境下进行测试。例如，改变系统的初始条件、边界条件或外部强迫。如果模型（包括其发现的算子系数）在这些**干预**（interventions）下依然能做出准确预测，则表明它学到的是可移植的物理机制，而非特定环境下的[虚假相关](@entry_id:755254)性  。

3.  **与守恒律和对称性的一致性**：物理学的基础是守恒律（如质量、能量、动量守恒）和对称性。一个可信的发现模型必须在经验上再现这些基本物理原则。例如，如果已知系统是[质量守恒](@entry_id:204015)的，那么验证模型在长时间演化中是否保持总质量不变，是检验其物理真实性的一个强有力手段  。

4.  **简约性（[奥卡姆剃刀](@entry_id:147174)）**：在多个能够解释数据的模型中，最简单的那个往往更可能是正确的。在贝叶斯框架下，**[模型证据](@entry_id:636856)**（Bayesian Evidence）或[边际似然](@entry_id:636856) $p(\mathcal{D} | M)$ 自然地实现了这一原则。它通过对所有参数进行积分，自动惩罚了过于复杂的模型，因为这些模型需要“分散”其概率质量以适应大量可能的数据集，从而为任何特定数据集分配的概率较低。证据值更高的模型，通常是在简约性和[拟合优度](@entry_id:637026)之间达到更佳平衡的模型 。

总之，仅靠在同[分布](@entry_id:182848)测试集上取得低误差，远不足以将数据驱动模型擢升为科学发现。只有通过严格的[可辨识性分析](@entry_id:182774)，并在新的实验条件下检验其[不变性](@entry_id:140168)、可移植性以及与基本物理原则的一致性，我们才能建立起对所发现定律的科学信心。