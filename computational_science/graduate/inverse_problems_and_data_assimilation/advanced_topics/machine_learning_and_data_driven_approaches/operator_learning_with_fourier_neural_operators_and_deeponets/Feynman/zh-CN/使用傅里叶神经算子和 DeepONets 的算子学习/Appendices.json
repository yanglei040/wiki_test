{
    "hands_on_practices": [
        {
            "introduction": "傅里叶神经算子 (Fourier Neural Operators, FNO) 的一个关键优势在于其计算效率，尤其是在处理科学问题中常见的高分辨率数据时。本练习将引导你通过推导其计算复杂度，从数学上证明这一效率。通过与标准卷积运算进行比较，你将理解快速傅里叶变换 (Fast Fourier Transform, FFT) 如何为 FNO 带来根本性的渐进优势 。",
            "id": "3407231",
            "problem": "考虑一个具有 $N$ 个等距网格点的一维周期性域，以及一个将 $C_{\\mathrm{in}}$ 个输入通道映射到 $C_{\\mathrm{out}}$ 个输出通道的傅里叶神经算子 (Fourier Neural Operator, FNO) 层。该 FNO 层在频域中实现：它通过快速傅里叶变换 (Fast Fourier Transform, FFT) 计算每个输入通道的离散傅里叶变换 (Discrete Fourier Transform, DFT)，在一个由 $M$ 个保留的傅里叶模式上乘以一个学习到的复值谱权重矩阵，然后计算逆 FFT 以返回到实空间。假设采用以下标准且经过广泛验证的成本模型：对长度为 $N$ 的序列进行一次正向或逆向 FFT 的成本为 $\\alpha N \\log_{2} N$ 次浮点运算 (flops)，且与通道索引无关；在保留模式上的谱乘法每次应用的成本为 $\\beta M C_{\\mathrm{in}} C_{\\mathrm{out}}$ flops，且不依赖于 $N$。作为比较，考虑一个算子层的实空间卷积实现，其核宽度为 $k$（每个输入-输出通道对具有大小为 $k$ 的紧凑模板），其中每个输出通道上每个网格点的成本与输入通道数和模板宽度成正比。特别地，假设实空间卷积的成本为 $\\gamma N k C_{\\mathrm{in}} C_{\\mathrm{out}}$ flops。\n\n仅从 DFT 的线性性质和卷积定理出发，结合上述成本模型，完成以下任务：\n\n(a) 推导 FNO 层的频域实现和实空间卷积实现的 flop 计数的主导项，并将两者表示为 $N$、$C_{\\mathrm{in}}$、$C_{\\mathrm{out}}$、$k$ 以及常数 $\\alpha$、$\\beta$、$\\gamma$ 的函数。您的最终表达式必须使用自然对数 $\\ln$，并且必须明确说明底数转换 $\\log_{2} N = \\ln N / \\ln 2$。在呈现关于 $N$ 的主导依赖关系时，忽略 $N$ 的低阶可加项。\n\n(b) 在 $M$ 与 $N$ 无关，并且在 $N$ 很大时谱乘法项的阶严格低于 FFT 项的阶的假设下，计算交叉网格大小 $N^{\\star}$ 的精确闭式解析表达式，在该大小下，FNO 的频域实现和实空间卷积实现的 flop 计数主导项相等。将 $N^{\\star}$ 表示为关于 $\\alpha$、$\\gamma$、$k$、$C_{\\mathrm{in}}$、$C_{\\mathrm{out}}$ 和 $\\ln 2$ 的单个表达式。使用自然指数 $\\exp(\\cdot)$，且不包含任何单位。最终答案必须是单个闭式解析表达式。",
            "solution": "该问题要求推导和比较两种神经算子层的计算成本（以浮点运算次数 (flops) 衡量）：一种是在频域中实现的傅里叶神经算子 (FNO) 层，另一种是在实空间中实现的标准卷积层。\n\n### (a) 部分：主导项 Flop 计数的推导\n\n首先，我们推导每种实现的主导项 flop 计数，将其表示为给定参数的函数。\n\n**傅里叶神经算子 (FNO) 层成本**\n\nFNO 层的计算涉及三个主要步骤。总 flop 计数（记为 $F_{\\mathrm{FNO}}$）是这些步骤成本的总和。\n\n1.  **正向傅里叶变换**：输入包含 $C_{\\mathrm{in}}$ 个通道，每个通道是一个长度为 $N$ 的序列。对每个通道应用快速傅里叶变换 (FFT)。单次 FFT 的成本给定为 $\\alpha N \\log_{2}(N)$。由于离散傅里叶变换 (DFT) 的线性性质，我们可以独立处理 $C_{\\mathrm{in}}$ 个输入通道中的每一个。\n    这一步的总成本是通道数乘以每个通道的成本：\n    $$C_{\\mathrm{FFT}} = C_{\\mathrm{in}} \\alpha N \\log_{2}(N)$$\n\n2.  **谱乘法**：在频域中，通过与在 $M$ 个保留的傅里叶模式上学习到的权重矩阵相乘，将 $C_{\\mathrm{in}}$ 个变换后的通道映射到 $C_{\\mathrm{out}}$ 个输出通道。此操作的成本是明确给出的，并且与网格大小 $N$ 无关：\n    $$C_{\\mathrm{mul}} = \\beta M C_{\\mathrm{in}} C_{\\mathrm{out}}$$\n\n3.  **逆傅里叶变换**：谱乘法之后，我们在频域中有 $C_{\\mathrm{out}}$ 个通道。为了返回到物理域，对这些通道中的每一个应用逆 FFT (IFFT)。假设 IFFT 的成本与正向 FFT 相同，为 $\\alpha N \\log_{2}(N)$。\n    这一步的总成本是：\n    $$C_{\\mathrm{IFFT}} = C_{\\mathrm{out}} \\alpha N \\log_{2}(N)$$\n\nFNO 层的总 flop 计数是这三项成本的总和：\n$$F_{\\mathrm{FNO}} = C_{\\mathrm{FFT}} + C_{\\mathrm{mul}} + C_{\\mathrm{IFFT}}$$\n$$F_{\\mathrm{FNO}} = C_{\\mathrm{in}} \\alpha N \\log_{2}(N) + \\beta M C_{\\mathrm{in}} C_{\\mathrm{out}} + C_{\\mathrm{out}} \\alpha N \\log_{2}(N)$$\n合并与 $N$ 相关的项：\n$$F_{\\mathrm{FNO}} = \\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}}) N \\log_{2}(N) + \\beta M C_{\\mathrm{in}} C_{\\mathrm{out}}$$\n\n问题要求关于 $N$ 的主导项 flop 计数。$\\beta M C_{\\mathrm{in}} C_{\\mathrm{out}}$ 项相对于 $N$ 是常数，而第一项以 $N \\ln(N)$ 的形式增长。因此，主导项是 $\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}}) N \\log_{2}(N)$。我们必须使用恒等式 $\\log_{2}(N) = \\frac{\\ln(N)}{\\ln(2)}$ 将以 2 为底的对数转换为自然对数。\n\nFNO 的主导项 flop 计数 $F_{\\mathrm{FNO, leading}}$ 为：\n$$F_{\\mathrm{FNO, leading}} = \\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}}) N \\left(\\frac{\\ln(N)}{\\ln(2)}\\right) = \\frac{\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}})}{\\ln(2)} N \\ln(N)$$\n\n**实空间卷积层成本**\n\n核宽度为 $k$ 的实空间卷积层的成本在问题陈述中已直接给出。此操作的成本在网格大小 $N$ 上是线性的。卷积的总 flop 计数 $F_{\\mathrm{conv}}$ 为：\n$$F_{\\mathrm{conv}} = \\gamma N k C_{\\mathrm{in}} C_{\\mathrm{out}}$$\n此表达式是所提供的成本模型中的主导项（也是唯一项）。\n\n### (b) 部分：交叉网格大小 $N^{\\star}$\n\n交叉网格大小 $N^{\\star}$ 是指两种实现的 flop 计数主导项相等时的 $N$ 值。通过设置 $F_{\\mathrm{FNO, leading}} = F_{\\mathrm{conv}}$ 并求解 $N = N^{\\star}$ 来找到该值。\n\n在确定 $F_{\\mathrm{FNO, leading}}$ 时，已经使用了当 $N$ 很大时谱乘法项的阶严格低于 FFT 项的阶这一假设。\n\n令两个主导项成本表达式相等：\n$$F_{\\mathrm{FNO, leading}}(N^{\\star}) = F_{\\mathrm{conv}}(N^{\\star})$$\n$$\\frac{\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}})}{\\ln(2)} N^{\\star} \\ln(N^{\\star}) = \\gamma N^{\\star} k C_{\\mathrm{in}} C_{\\mathrm{out}}$$\n\n由于网格大小 $N^{\\star}$ 必须是正整数，我们可以将等式两边同时除以 $N^{\\star}$（对于 $N^{\\star} \\neq 0$）：\n$$\\frac{\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}})}{\\ln(2)} \\ln(N^{\\star}) = \\gamma k C_{\\mathrm{in}} C_{\\mathrm{out}}$$\n\n现在，我们通过将 $\\ln(N^{\\star})$ 分离到等式一侧来求解它：\n$$\\ln(N^{\\star}) = \\frac{\\gamma k C_{\\mathrm{in}} C_{\\mathrm{out}} \\ln(2)}{\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}})}$$\n\n为了求得 $N^{\\star}$，我们对等式两边取指数：\n$$N^{\\star} = \\exp\\left( \\frac{\\gamma k C_{\\mathrm{in}} C_{\\mathrm{out}} \\ln(2)}{\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}})} \\right)$$\n\n这就是交叉网格大小 $N^{\\star}$ 的最终闭式解析表达式。",
            "answer": "$$\n\\boxed{\\exp\\left(\\frac{\\gamma k C_{\\mathrm{in}} C_{\\mathrm{out}} \\ln(2)}{\\alpha (C_{\\mathrm{in}} + C_{\\mathrm{out}})}\\right)}\n$$"
        },
        {
            "introduction": "现在我们将注意力转向深度算子网络 (DeepONets)，它以其独特的“分支-主干” (branch-trunk) 架构而著称。为了真正理解这些网络如何学习，我们必须深入其训练过程的“引擎盖”之下。本练习要求你推导网络参数的梯度表达式，这是反向传播算法的核心，它将让你具体地理解分支和主干网络如何协同更新以最小化损失函数 。",
            "id": "3407183",
            "problem": "考虑在逆问题和数据同化的背景下，学习一个将输入函数 $u$ 映射到输出函数 $y$ 的非线性算子的任务。一个深度算子网络 (DeepONet) 通过一个分支网络和一个主干网络对该算子进行参数化，具体如下。分支网络将离散化输入 $u$ 编码为一个系数向量 $b(u; \\theta_{b}) \\in \\mathbb{R}^{p}$，而主干网络将查询位置 $x$ 编码为一个基向量 $t(x; \\theta_{t}) \\in \\mathbb{R}^{p}$。算子输出定义为 $G(u)(x) = b(u; \\theta_{b})^{\\top} t(x; \\theta_{t})$，这是分支嵌入和主干嵌入的一种双线性形式。给定一个包含 $N$ 个离散化输入-输出函数对 $\\{(u_{i}, y_{i})\\}_{i=1}^{N}$ 的训练数据集，其中每个 $y_{i}$ 都在 $M$ 个查询位置 $\\{x_{j}\\}_{j=1}^{M}$ 上被观测。训练目标是求积加权的经验均方损失\n$$\nL(\\theta_{b}, \\theta_{t}) = \\frac{1}{2N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\left( G(u_{i})(x_{j}) - y_{i}(x_{j}) \\right)^{2},\n$$\n其中 $w_{j} > 0$ 是给定的在查询位置上的求积权重。假设 $b(u; \\theta_{b})$ 和 $t(x; \\theta_{t})$ 分别对 $\\theta_{b}$ 和 $\\theta_{t}$ 可微，并记雅可比矩阵为 $J_{b}(u; \\theta_{b}) = \\frac{\\partial b(u; \\theta_{b})}{\\partial \\theta_{b}} \\in \\mathbb{R}^{p \\times n_{b}}$ 和 $J_{t}(x; \\theta_{t}) = \\frac{\\partial t(x; \\theta_{t})}{\\partial \\theta_{t}} \\in \\mathbb{R}^{p \\times n_{t}}$，其中 $n_{b}$ 和 $n_{t}$ 分别是参数向量 $\\theta_{b}$ 和 $\\theta_{t}$ 的维度。从第一性原理出发，即微分链式法则和均方损失的定义，推导梯度 $\\nabla_{\\theta_{b}} L$ 和 $\\nabla_{\\theta_{t}} L$ 的反向传播表达式，要求以雅可比矩阵 $J_{b}$ 和 $J_{t}$、嵌入 $b(u_{i}; \\theta_{b})$ 和 $t(x_{j}; \\theta_{t})$ 以及残差 $G(u_{i})(x_{j}) - y_{i}(x_{j})$ 的闭合形式表示。你的最终答案必须是一个单一的解析表达式，使用 LaTeX 的 $\\texttt{pmatrix}$ 环境将两个梯度收集在一个行矩阵中，不包含任何单位。不要进行任何数值近似或四舍五入。",
            "solution": "该问题要求推导 DeepONet 损失函数相对于分支网络参数 $\\theta_{b}$ 和主干网络参数 $\\theta_{t}$ 的梯度。推导将从第一性原理出发，将多变量微积分的链式法则应用于给定的损失函数。\n\n损失函数为求积加权的经验均方误差：\n$$\nL(\\theta_{b}, \\theta_{t}) = \\frac{1}{2N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\left( G(u_{i})(x_{j}) - y_{i}(x_{j}) \\right)^{2}\n$$\n其中 DeepONet 算子定义为 $G(u_{i})(x_{j}) = b(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t})$。为清晰起见，我们将第 $i$ 个输入和第 $j$ 个查询位置的残差记为 $r_{ij} = G(u_{i})(x_{j}) - y_{i}(x_{j})$。于是损失函数可以写为：\n$$\nL(\\theta_{b}, \\theta_{t}) = \\frac{1}{2N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} r_{ij}^{2}\n$$\n我们需要计算梯度 $\\nabla_{\\theta_{b}} L = \\frac{\\partial L}{\\partial \\theta_{b}}$ 和 $\\nabla_{\\theta_{t}} L = \\frac{\\partial L}{\\partial \\theta_{t}}$。它们分别是维度为 $n_{b} \\times 1$ 和 $n_{t} \\times 1$ 的列向量。\n\n首先，我们来推导关于分支网络参数 $\\theta_{b}$ 的梯度。\n使用链式法则，损失函数 $L$ 关于 $\\theta_{b}$ 的梯度为：\n$$\n\\nabla_{\\theta_{b}} L = \\frac{\\partial L}{\\partial \\theta_{b}} = \\frac{1}{2N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\frac{\\partial}{\\partial \\theta_{b}} (r_{ij}^{2})\n$$\n残差平方的导数为 $\\frac{\\partial}{\\partial \\theta_{b}} (r_{ij}^{2}) = 2 r_{ij} \\frac{\\partial r_{ij}}{\\partial \\theta_{b}}$。将其代入方程中得到：\n$$\n\\nabla_{\\theta_{b}} L = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} r_{ij} \\frac{\\partial r_{ij}}{\\partial \\theta_{b}}\n$$\n现在，我们计算残差 $r_{ij} = b(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t}) - y_{i}(x_{j})$ 关于 $\\theta_{b}$ 的导数。项 $y_{i}(x_{j})$ 不依赖于 $\\theta_{b}$，因此其导数为零。\n$$\n\\frac{\\partial r_{ij}}{\\partial \\theta_{b}} = \\frac{\\partial}{\\partial \\theta_{b}} \\left( b(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t}) \\right)\n$$\n为了计算这个梯度，我们可以使用标量积对向量求导的恒等式。给定向量 $\\mathbf{a}$ 和 $\\mathbf{c}$，其中 $\\mathbf{a}$ 是向量 $\\mathbf{x}$ 的函数，它们的点积的梯度是 $\\nabla_{\\mathbf{x}}(\\mathbf{a}(\\mathbf{x})^\\top \\mathbf{c}) = (\\frac{\\partial \\mathbf{a}(\\mathbf{x})}{\\partial \\mathbf{x}})^\\top \\mathbf{c}$。在这里，$\\mathbf{a}$ 对应于 $b(u_{i}; \\theta_{b})$，$\\mathbf{c}$ 对应于 $t(x_{j}; \\theta_{t})$，$\\mathbf{x}$ 对应于 $\\theta_{b}$。项 $\\frac{\\partial b(u_{i}; \\theta_{b})}{\\partial \\theta_{b}}$ 就是问题中定义的雅可比矩阵 $J_{b}(u_{i}; \\theta_{b}) \\in \\mathbb{R}^{p \\times n_{b}}$。因此，\n$$\n\\frac{\\partial r_{ij}}{\\partial \\theta_{b}} = J_{b}(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t})\n$$\n这个结果是一个维度为 $n_{b} \\times 1$ 的列向量。将其代回 $\\nabla_{\\theta_{b}} L$ 的表达式中：\n$$\n\\nabla_{\\theta_{b}} L = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\left( G(u_{i})(x_{j}) - y_{i}(x_{j}) \\right) J_{b}(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t})\n$$\n\n接下来，我们推导关于主干网络参数 $\\theta_{t}$ 的梯度。这个过程与对 $\\theta_{b}$ 的推导是对称的。\n$$\n\\nabla_{\\theta_{t}} L = \\frac{\\partial L}{\\partial \\theta_{t}} = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} r_{ij} \\frac{\\partial r_{ij}}{\\partial \\theta_{t}}\n$$\n我们计算残差 $r_{ij}$ 关于 $\\theta_{t}$ 的导数：\n$$\n\\frac{\\partial r_{ij}}{\\partial \\theta_{t}} = \\frac{\\partial}{\\partial \\theta_{t}} \\left( b(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t}) \\right)\n$$\n在这种情况下，向量 $b(u_{i}; \\theta_{b})$ 相对于 $\\theta_{t}$ 是一个常数。使用一个类似的向量微积分恒等式 $\\nabla_{\\mathbf{x}}(\\mathbf{c}^\\top \\mathbf{a}(\\mathbf{x})) = (\\frac{\\partial \\mathbf{a}(\\mathbf{x})}{\\partial \\mathbf{x}})^\\top \\mathbf{c}$，其中现在 $\\mathbf{a}$ 对应于 $t(x_{j}; \\theta_{t})$，$\\mathbf{c}$ 对应于 $b(u_{i}; \\theta_{b})$，$\\mathbf{x}$ 对应于 $\\theta_{t}$。项 $\\frac{\\partial t(x_{j}; \\theta_{t})}{\\partial \\theta_{t}}$ 是雅可比矩阵 $J_{t}(x_{j}; \\theta_{t}) \\in \\mathbb{R}^{p \\times n_{t}}$。所以，\n$$\n\\frac{\\partial r_{ij}}{\\partial \\theta_{t}} = J_{t}(x_{j}; \\theta_{t})^{\\top} b(u_{i}; \\theta_{b})\n$$\n这是一个维度为 $n_{t} \\times 1$ 的列向量。将其代入 $\\nabla_{\\theta_{t}} L$ 的表达式中：\n$$\n\\nabla_{\\theta_{t}} L = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\left( G(u_{i})(x_{j}) - y_{i}(x_{j}) \\right) J_{t}(x_{j}; \\theta_{t})^{\\top} b(u_{i}; \\theta_{b})\n$$\n这两个关于 $\\nabla_{\\theta_{b}} L$ 和 $\\nabla_{\\theta_{t}} L$ 的表达式是梯度的最终闭合形式反向传播公式，用指定的量来表示。问题要求将它们收集到一个单一的行矩阵中。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\left( G(u_{i})(x_{j}) - y_{i}(x_{j}) \\right) J_{b}(u_{i}; \\theta_{b})^{\\top} t(x_{j}; \\theta_{t}) & \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} w_{j} \\left( G(u_{i})(x_{j}) - y_{i}(x_{j}) \\right) J_{t}(x_{j}; \\theta_{t})^{\\top} b(u_{i}; \\theta_{b}) \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在掌握了基本模型之后，是时候将抽象的算子学习模型与其实际应用——求解微分方程——联系起来了。精确处理边界条件是其中至关重要的一环。本练习将探讨一种精妙的方法，通过模型设计直接施加狄利克雷 (Dirichlet) 边界条件，而非仅仅在损失函数中惩罚边界误差。通过推导新的损失函数，你将看到这种巧妙的重新参数化技巧如何简化训练目标，并确保学到的解在结构上就严格满足问题的物理约束 。",
            "id": "3407257",
            "problem": "考虑一个有界Lipschitz域 $\\Omega \\subset \\mathbb{R}^{d}$，其边界为 $\\partial \\Omega$。令 $\\mathcal{A}_{a}$ 表示一个由系数场 $a \\in \\mathcal{A}$ 参数化的线性一致椭圆微分算子，并考虑带有Dirichlet数据的边值问题\n$$\n\\mathcal{A}_{a} u = f \\quad \\text{in } \\Omega, \n\\qquad\nu = g \\quad \\text{on } \\partial \\Omega,\n$$\n其中输入 $(a,f,g)$ 从支撑在 $\\mathcal{A} \\times L^{2}(\\Omega) \\times H^{1/2}(\\partial \\Omega)$ 上的特定应用分布中抽取。给定一个大小为 $M$ 的训练数据集 $\\mathcal{D} = \\{(a_{i}, f_{i}, g_{i}, y_{i}, \\mathcal{H}_{i})\\}_{i=1}^{M}$，其中 $y_{i} \\in \\mathbb{R}^{p_{i}}$ 是观测值，观测算子为 $\\mathcal{H}_{i}: H^{1}(\\Omega) \\to \\mathbb{R}^{p_{i}}$，且 $y_{i} = \\mathcal{H}_{i}(u_{i}) + \\eta_{i}$，其中 $\\eta_{i}$ 是噪声。令 $\\theta$ 参数化一个傅里叶神经算子（Fourier Neural Operator, FNO）或一个深度算子网络（Deep Operator Network, DeepONet），该算子定义了一个算子学习器 $T_{\\theta}: (a,f,g) \\mapsto u_{\\theta}(\\cdot; a,f,g)$。\n\n您将基于残差最小化和数据同化原理，构建用于训练 $T_{\\theta}$ 的经验风险。您可以使用的基本依据是：Dirichlet边界条件的定义、经验风险最小化的概念、使用残差来满足偏微分方程，以及通过平方范数来整合观测-模型失配。\n\n对于每个训练项 $i \\in \\{1,\\dots,M\\}$，令 $\\mathcal{X}_{i}^{\\mathrm{int}} \\subset \\Omega$ 和 $\\mathcal{X}_{i}^{\\mathrm{b}} \\subset \\partial \\Omega$ 分别为内部和边界配置点的集合，其基数分别为 $N_{i}^{\\mathrm{int}}$ 和 $N_{i}^{\\mathrm{b}}$。定义残差\n$$\nr_{i}^{\\mathrm{int}}(x;\\theta) := \\mathcal{A}_{a_{i}}\\big(u_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big)(x) - f_{i}(x), \\quad x \\in \\mathcal{X}_{i}^{\\mathrm{int}},\n$$\n$$\nr_{i}^{\\mathrm{b}}(x;\\theta) := u_{\\theta}(x;a_{i},f_{i},g_{i}) - g_{i}(x), \\quad x \\in \\mathcal{X}_{i}^{\\mathrm{b}},\n$$\n以及数据失配 $m_{i}(\\theta) := \\mathcal{H}_{i}\\big(u_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big) - y_{i} \\in \\mathbb{R}^{p_{i}}$。考虑增广经验风险\n$$\n\\mathcal{L}_{\\mathrm{aug}}(\\theta) \n:= \\frac{1}{M} \\sum_{i=1}^{M} \\left[\n\\lambda_{\\mathrm{int}} \\cdot \\frac{1}{N_{i}^{\\mathrm{int}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{int}}} \\big| r_{i}^{\\mathrm{int}}(x;\\theta) \\big|^{2}\n\\;+\\;\n\\lambda_{\\mathrm{b}} \\cdot \\frac{1}{N_{i}^{\\mathrm{b}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{b}}} \\big| r_{i}^{\\mathrm{b}}(x;\\theta) \\big|^{2}\n\\;+\\;\n\\lambda_{\\mathrm{data}} \\cdot \\big\\| m_{i}(\\theta) \\big\\|_{2}^{2}\n\\right],\n$$\n其中 $\\lambda_{\\mathrm{int}}, \\lambda_{\\mathrm{b}}, \\lambda_{\\mathrm{data}}$ 是可调的非负权重。\n\n现在，定义一个提升算子 $L: H^{1/2}(\\partial \\Omega) \\to H^{1}(\\Omega)$，使得对所有容许的 $g$ 都有 $(L g)|_{\\partial \\Omega} = g$；并定义一个边界消失标量函数 $b \\in C^{0}(\\overline{\\Omega})$，使得在边界上有 $b|_{\\partial \\Omega} = 0$，并在某个内部参考点 $x^{\\star} \\in \\Omega$ 处有 $b(x^{\\star}) = 1$。通过以下方式对模型输出进行重参数化\n$$\nu_{\\theta}(\\cdot; a,f,g) \\;=\\; L g \\;+\\; b \\, v_{\\theta}(\\cdot; a,f,g),\n$$\n其中 $v_{\\theta}$ 是学习到的齐次分量。对于深度算子网络，这可以通过将主干基与 $b$ 相乘来修改实现；对于傅里叶神经算子，这可以通过将网络的物理空间输出与 $b$ 相乘，并在计算残差之前加上 $L g$ 来实现。\n\n从上述定义出发，除了所述的重参数化之外，不假设任何特定的架构公式，推导提升经验风险 $\\mathcal{L}_{\\mathrm{lift}}(\\theta)$ 的显式闭式解析表达式。该表达式通过将重参数化的 $u_{\\theta}$ 代入 $\\mathcal{L}_{\\mathrm{aug}}(\\theta)$，并利用 $L$ 的Dirichlet性质和 $b$ 的边界消失性质来简化边界项而得到。\n\n您的最终答案必须是关于 $(a_{i}, f_{i}, g_{i}, y_{i}, \\mathcal{H}_{i})$、$L$、$b$ 和 $v_{\\theta}$ 的 $\\mathcal{L}_{\\mathrm{lift}}(\\theta)$ 的单一简化解析表达式，并且不得包含任何其他文本。不需要进行数值评估。请以闭式表达式的形式表达您的最终答案。",
            "solution": "问题陈述已经过验证，被认为是科学上合理的、适定的和客观的。它提出了一个基于科学机器学习和偏微分方程（PDE）数值分析领域既定原则的清晰的符号操作任务。该问题提供了一套完整的定义和约束，这些定义和约束内部一致且与算子学习的主题相关。任务是在对所学函数进行建议的重参数化下，为经验风险泛函推导一个新的表达式。在训练神经算子的背景下，这是一个标准且有意义的过程，特别是对于处理非齐次边界条件。\n\n目标是通过将重参数化的模型输出 $u_{\\theta}$ 代入增广经验风险泛函 $\\mathcal{L}_{\\mathrm{aug}}(\\theta)$，来推导提升经验风险的显式解析表达式，记为 $\\mathcal{L}_{\\mathrm{lift}}(\\theta)$。\n\n增广经验风险定义为：\n$$\n\\mathcal{L}_{\\mathrm{aug}}(\\theta) \n:= \\frac{1}{M} \\sum_{i=1}^{M} \\left[\n\\lambda_{\\mathrm{int}} \\cdot \\frac{1}{N_{i}^{\\mathrm{int}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{int}}} \\big| r_{i}^{\\mathrm{int}}(x;\\theta) \\big|^{2}\n\\;+\\;\n\\lambda_{\\mathrm{b}} \\cdot \\frac{1}{N_{i}^{\\mathrm{b}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{b}}} \\big| r_{i}^{\\mathrm{b}}(x;\\theta) \\big|^{2}\n\\;+\\;\n\\lambda_{\\mathrm{data}} \\cdot \\big\\| m_{i}(\\theta) \\big\\|_{2}^{2}\n\\right]\n$$\n模型输出的重参数化由下式给出：\n$$\nu_{\\theta}(\\cdot; a,f,g) \\;=\\; L g \\;+\\; b \\, v_{\\theta}(\\cdot; a,f,g)\n$$\n其中 $L$ 是一个提升算子，$b$ 是一个边界消失函数。算子学习器现在学习函数 $v_{\\theta}$。\n\n我们将分析此代换对单个训练样本 $i$ 的损失函数三个分量的影响，为简洁起见，在上下文清晰的情况下，省略 $u_{\\theta}$ 和 $v_{\\theta}$ 的参数。\n\n1.  **内部残差项**：\n    内部残差为 $r_{i}^{\\mathrm{int}}(x;\\theta) := \\mathcal{A}_{a_{i}}(u_{\\theta})(x) - f_{i}(x)$。代入 $u_{\\theta}$ 的重参数化：\n    $$\n    r_{i}^{\\mathrm{int}}(x;\\theta) = \\mathcal{A}_{a_{i}}\\big(L g_{i} + b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big)(x) - f_{i}(x)\n    $$\n    问题陈述指明 $\\mathcal{A}_{a}$ 是一个线性微分算子。利用此性质，我们可以将 $\\mathcal{A}_{a_{i}}$ 分配到和式上：\n    $$\n    r_{i}^{\\mathrm{int}}(x;\\theta) = \\mathcal{A}_{a_{i}}(L g_{i})(x) + \\mathcal{A}_{a_{i}}\\big(b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big)(x) - f_{i}(x)\n    $$\n    因此，样本 $i$ 在损失函数中对应的项是：\n    $$\n    \\lambda_{\\mathrm{int}} \\cdot \\frac{1}{N_{i}^{\\mathrm{int}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{int}}} \\left| \\mathcal{A}_{a_{i}}(L g_{i})(x) + \\mathcal{A}_{a_{i}}\\big(b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big)(x) - f_{i}(x) \\right|^{2}\n    $$\n\n2.  **边界残差项**：\n    对于配置点 $x \\in \\mathcal{X}_{i}^{\\mathrm{b}} \\subset \\partial\\Omega$，边界残差为 $r_{i}^{\\mathrm{b}}(x;\\theta) := u_{\\theta}(x) - g_{i}(x)$。代入 $u_{\\theta}$ 的重参数化：\n    $$\n    r_{i}^{\\mathrm{b}}(x;\\theta) = \\big(L g_{i}\\big)(x) + b(x) \\, v_{\\theta}(x;a_{i},f_{i},g_{i}) - g_{i}(x)\n    $$\n    我们现在应用提升算子 $L$ 和边界消失函数 $b$ 的定义性质。\n    -   根据 $L$ 的定义，对于任何 $x \\in \\partial\\Omega$，我们有 $(L g_{i})|_{\\partial\\Omega}(x) = g_{i}(x)$。\n    -   根据 $b$ 的定义，对于任何 $x \\in \\partial\\Omega$，我们有 $b|_{\\partial\\Omega}(x) = 0$。\n    由于配置点 $\\mathcal{X}_{i}^{\\mathrm{b}}$ 位于边界 $\\partial\\Omega$ 上，对于任何 $x \\in \\mathcal{X}_{i}^{\\mathrm{b}}$，我们有：\n    $$\n    r_{i}^{\\mathrm{b}}(x;\\theta) = g_{i}(x) + 0 \\cdot v_{\\theta}(x;a_{i},f_{i},g_{i}) - g_{i}(x) = 0\n    $$\n    对于所有边界配置点，边界残差恒等于零。因此，损失函数中的整个边界项消失了：\n    $$\n    \\lambda_{\\mathrm{b}} \\cdot \\frac{1}{N_{i}^{\\mathrm{b}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{b}}} \\big| r_{i}^{\\mathrm{b}}(x;\\theta) \\big|^{2} = \\lambda_{\\mathrm{b}} \\cdot \\frac{1}{N_{i}^{\\mathrm{b}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{b}}} |0|^{2} = 0\n    $$\n    这是由重参数化带来的核心简化，因为通过构造，Dirichlet边界条件得到了满足。\n\n3.  **数据失配项**：\n    数据失配为 $m_{i}(\\theta) := \\mathcal{H}_{i}(u_{\\theta}) - y_{i}$。代入 $u_{\\theta}$ 的重参数化：\n    $$\n    m_{i}(\\theta) = \\mathcal{H}_{i}\\big(L g_{i} + b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big) - y_{i}\n    $$\n    问题没有指明观测算子 $\\mathcal{H}_{i}$ 是线性的，因此该项无法进一步简化。样本 $i$ 在损失函数中对应的项是：\n    $$\n    \\lambda_{\\mathrm{data}} \\cdot \\big\\| \\mathcal{H}_{i}\\big(L g_{i} + b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big) - y_{i} \\big\\|_{2}^{2}\n    $$\n\n**组装提升经验风险**：\n结合这三项的结果，我们通过对剩余的非零分量求和，并对 $M$ 个训练样本取平均，来构建提升经验风险 $\\mathcal{L}_{\\mathrm{lift}}(\\theta)$。边界项现在已经不存在了。\n$$\n\\mathcal{L}_{\\mathrm{lift}}(\\theta) = \\frac{1}{M} \\sum_{i=1}^{M} \\left[\n\\lambda_{\\mathrm{int}} \\cdot \\frac{1}{N_{i}^{\\mathrm{int}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{int}}} \\left| \\mathcal{A}_{a_{i}}(L g_{i})(x) + \\mathcal{A}_{a_{i}}\\big(b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big)(x) - f_{i}(x) \\right|^{2}\n+\n\\lambda_{\\mathrm{data}} \\cdot \\left\\| \\mathcal{H}_{i}\\big(L g_{i} + b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big) - y_{i} \\right\\|_{2}^{2}\n\\right]\n$$\n此表达式是提升经验风险的最终简化解析形式。",
            "answer": "$$\n\\boxed{\n\\frac{1}{M} \\sum_{i=1}^{M} \\left[\n\\lambda_{\\mathrm{int}} \\cdot \\frac{1}{N_{i}^{\\mathrm{int}}} \\sum_{x \\in \\mathcal{X}_{i}^{\\mathrm{int}}} \\left| \\mathcal{A}_{a_{i}}(L g_{i})(x) + \\mathcal{A}_{a_{i}}\\big(b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big)(x) - f_{i}(x) \\right|^{2}\n+\n\\lambda_{\\mathrm{data}} \\cdot \\left\\| \\mathcal{H}_{i}\\big(L g_{i} + b \\, v_{\\theta}(\\cdot;a_{i},f_{i},g_{i})\\big) - y_{i} \\right\\|_{2}^{2}\n\\right]\n}\n$$"
        }
    ]
}