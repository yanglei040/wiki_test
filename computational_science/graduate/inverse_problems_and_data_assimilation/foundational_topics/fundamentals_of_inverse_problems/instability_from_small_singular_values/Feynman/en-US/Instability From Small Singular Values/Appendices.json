{
    "hands_on_practices": [
        {
            "introduction": "This first exercise provides a stark and quantitative illustration of ill-conditioning in inverse problems. By focusing on a hypothetical scenario where noise aligns with the direction of the smallest singular value, you will perform a direct calculation to estimate the resulting error amplification . This practice is designed to build concrete intuition about how even a small percentage of noise in observations can be magnified into a catastrophically large error in the solution, underscoring the necessity of regularization.",
            "id": "3391315",
            "problem": "Consider the linear inverse problem in a Euclidean space where the observed data vector is given by $b = A x_{\\text{true}} + \\eta$, with $A \\in \\mathbb{R}^{m \\times n}$, $x_{\\text{true}} \\in \\mathbb{R}^{n}$, and additive noise $\\eta \\in \\mathbb{R}^{m}$. The reconstruction $x^{\\dagger}$ is defined to be the minimum-norm least-squares solution obtained via the Moore–Penrose pseudoinverse, that is, $x^{\\dagger} = A^{\\dagger} b$. Assume the singular value decomposition (SVD) of $A$ exists and that the smallest nonzero singular value is $\\sigma_{r} = 10^{-6}$. The noise level is quantified by $\\|\\eta\\|_{2} / \\|b_{\\text{true}}\\|_{2} = 0.01$, where $b_{\\text{true}} = A x_{\\text{true}}$. To isolate the instability arising from small singular values, assume the problem has been nondimensionalized so that the effective singular value associated with the dominant part of $b_{\\text{true}}$ is of order one, i.e., $\\|b_{\\text{true}}\\|_{2} / \\|x_{\\text{true}}\\|_{2} \\approx 1$, and that the dominant contribution to the solution error from the noise is aligned with the right singular vector corresponding to $\\sigma_{r}$.\n\nUsing only foundational properties of the singular value decomposition and the Moore–Penrose pseudoinverse, estimate the expected order of magnitude of the relative error in the reconstruction due solely to the noise,\n$$\n\\frac{\\|x^{\\dagger} - x_{\\text{true}}\\|_{2}}{\\|x_{\\text{true}}\\|_{2}},\n$$\nand report your answer as a single dimensionless number. Do not include any units. No rounding is required unless unavoidable.",
            "solution": "The reconstruction $x^{\\dagger}$ is defined by the Moore–Penrose pseudoinverse as $x^{\\dagger} = A^{\\dagger} b$, where $b = b_{\\text{true}} + \\eta$ and $b_{\\text{true}} = A x_{\\text{true}}$. The error in the reconstruction due to the additive noise is\n$$\nx^{\\dagger} - x_{\\text{true}} = A^{\\dagger}(b_{\\text{true}} + \\eta) - x_{\\text{true}} = A^{\\dagger} A x_{\\text{true}} + A^{\\dagger} \\eta - x_{\\text{true}} = A^{\\dagger} \\eta,\n$$\nsince $A^{\\dagger} A x_{\\text{true}}$ is the orthogonal projection of $x_{\\text{true}}$ onto the range of $A^{\\top}$, which equals $x_{\\text{true}}$ for the minimum-norm solution consistent with $b_{\\text{true}} \\in \\text{range}(A)$.\n\nLet the singular value decomposition (SVD) of $A$ be $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ has diagonal entries $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{r} > 0$, with $r = \\text{rank}(A)$. The Moore–Penrose pseudoinverse is $A^{\\dagger} = V \\Sigma^{\\dagger} U^{\\top}$, where $\\Sigma^{\\dagger}$ has diagonal entries $\\sigma_{i}^{-1}$ for $i \\leq r$ and zeros otherwise.\n\nThe error due to noise is then\n$$\nx^{\\dagger} - x_{\\text{true}} = V \\Sigma^{\\dagger} U^{\\top} \\eta.\n$$\nTaking norms yields\n$$\n\\|x^{\\dagger} - x_{\\text{true}}\\|_{2} = \\|V \\Sigma^{\\dagger} U^{\\top} \\eta\\|_{2} = \\|\\Sigma^{\\dagger} U^{\\top} \\eta\\|_{2}.\n$$\nThe matrix $U$ is orthogonal, so $\\|U^{\\top} \\eta\\|_{2} = \\|\\eta\\|_{2}$. The amplification of a component of $U^{\\top} \\eta$ along the $i$-th left singular vector is by the factor $\\sigma_{i}^{-1}$. Under the stated assumption that the dominant contribution to the solution error from the noise is aligned with the right singular vector corresponding to the smallest singular value $\\sigma_{r}$, the error magnitude is governed by\n$$\n\\|x^{\\dagger} - x_{\\text{true}}\\|_{2} \\approx \\frac{\\|\\eta\\|_{2}}{\\sigma_{r}}.\n$$\nWe seek the relative error. Using the nondimensionalization assumption that the effective singular value associated with the dominant part of $b_{\\text{true}}$ is of order one, we have\n$$\n\\frac{\\|b_{\\text{true}}\\|_{2}}{\\|x_{\\text{true}}\\|_{2}} \\approx 1 \\quad \\Longrightarrow \\quad \\|x_{\\text{true}}\\|_{2} \\approx \\|b_{\\text{true}}\\|_{2}.\n$$\nTherefore,\n$$\n\\frac{\\|x^{\\dagger} - x_{\\text{true}}\\|_{2}}{\\|x_{\\text{true}}\\|_{2}} \\approx \\frac{\\|\\eta\\|_{2}}{\\sigma_{r} \\|x_{\\text{true}}\\|_{2}} \\approx \\frac{\\|\\eta\\|_{2}}{\\sigma_{r} \\|b_{\\text{true}}\\|_{2}} = \\frac{1}{\\sigma_{r}} \\cdot \\frac{\\|\\eta\\|_{2}}{\\|b_{\\text{true}}\\|_{2}}.\n$$\nSubstituting the given values $\\sigma_{r} = 10^{-6}$ and $\\|\\eta\\|_{2} / \\|b_{\\text{true}}\\|_{2} = 0.01$ yields\n$$\n\\frac{\\|x^{\\dagger} - x_{\\text{true}}\\|_{2}}{\\|x_{\\text{true}}\\|_{2}} \\approx \\frac{1}{10^{-6}} \\times 0.01 = 10^{6} \\times 10^{-2} = 10^{4}.\n$$\nThus, the expected order of magnitude of the relative error in the reconstruction due solely to the noise, under the specified assumptions that isolate instability from the smallest singular value, is $1 \\times 10^{4}$.",
            "answer": "$$\\boxed{1 \\times 10^{4}}$$"
        },
        {
            "introduction": "Having seen how small singular values can destabilize a solution, we now turn to a practical remedy. This coding exercise guides you through the implementation of zero-order Tikhonov regularization, a cornerstone technique for solving ill-posed inverse problems . You will use Morozov's discrepancy principle, a data-driven method, to select the optimal regularization parameter, and then analyze how this process effectively suppresses the influence of noise-dominated modes corresponding to small singular values.",
            "id": "3391353",
            "problem": "Consider the linear inverse problem defined by the equation $A x = b$ with additive noise, where $b = A x_{\\mathrm{true}} + \\eta$, and the noise vector $\\eta$ has independent, identically distributed components with variance $\\sigma_{\\eta}^2$. We study zero-order Tikhonov regularization and Morozov’s discrepancy principle to address instability arising from small singular values. The following fundamental base is assumed: the least squares estimator minimizes the squared residual norm, and zero-order Tikhonov regularization minimizes the augmented functional $J(x) = \\|A x - b\\|_2^2 + \\lambda^2 \\|x\\|_2^2$ for a regularization parameter $\\lambda \\ge 0$. The singular value decomposition (SVD) of a matrix $A$ is defined as $A = U \\Sigma V^\\top$, where $U$ and $V$ are orthogonal and $\\Sigma$ is diagonal with nonnegative entries equal to the singular values. Morozov’s discrepancy principle selects $\\lambda$ such that the residual norm $\\|A x_{\\lambda} - b\\|_2$ equals a target norm consistent with the noise level. All angles used in any trigonometric definition in this problem must be interpreted in radians.\n\nYour task is to implement a complete and runnable program that, for each test case, constructs the matrix $A$, the clean signal $x_{\\mathrm{true}}$, and the noise vector $\\eta$ deterministically, computes the regularization parameter $\\lambda$ according to Morozov’s discrepancy principle, and quantifies the suppression of small-singular-value modes. Specifically:\n\n1. For zero-order Tikhonov regularization, define $x_{\\lambda}$ to be the unique minimizer of $J(x) = \\|A x - b\\|_2^2 + \\lambda^2 \\|x\\|_2^2$ when $A$ has full column rank. For each test case, determine $\\lambda$ satisfying Morozov’s discrepancy principle, namely:\n   $$\\|A x_{\\lambda} - b\\|_2 = \\tau \\sqrt{m} \\, \\sigma_{\\eta},$$\n   where $m$ is the number of rows of $A$, and $\\tau > 0$ is a specified discrepancy factor. In each test included below, $\\tau$ is provided explicitly. The target residual norm should be computed and used with the exact Euclidean norm $\\|\\cdot\\|_2$, with no units required beyond the mathematical quantities specified.\n\n2. Quantify the suppression of small-singular-value modes. Given the singular values $\\{\\sigma_i\\}_{i=1}^n$ of $A$, define the mode-wise suppression factor as:\n   $$s_i(\\lambda) = \\frac{\\lambda^2}{\\sigma_i^2 + \\lambda^2},$$\n   and interpret a mode as strongly suppressed if $\\sigma_i \\le \\alpha \\lambda$ with threshold $\\alpha = 0.1$. For each test case, compute:\n   - The count of strongly suppressed modes $k(\\lambda) = \\#\\{i : \\sigma_i \\le 0.1 \\lambda\\}$,\n   - The average suppression across strongly suppressed modes:\n     $$\\bar{s}(\\lambda) = \\frac{1}{k(\\lambda)} \\sum_{\\{i : \\sigma_i \\le 0.1 \\lambda\\}} s_i(\\lambda),$$\n     with the convention that $\\bar{s}(\\lambda) = 0$ when $k(\\lambda) = 0$.\n\nConstruction of $A$, $x_{\\mathrm{true}}$, and $\\eta$ for the test suite must follow these deterministic rules:\n- In all test cases, $A$ is square with $m = n = 10$, and diagonal with diagonal entries equal to the specified singular values $\\{\\sigma_i\\}_{i=1}^{10}$. Thus, $A = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_{10})$, so the singular value decomposition (SVD) has $U = I_{10}$, $V = I_{10}$, and $\\Sigma = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_{10})$.\n- The clean signal must be chosen as $x_{\\mathrm{true}} = (1, \\tfrac{1}{2}, \\tfrac{1}{3}, \\dots, \\tfrac{1}{10})^\\top$.\n- The noise vector must be constructed deterministically from a template vector $w \\in \\mathbb{R}^{10}$ defined by components $w_j = \\sin(j) + \\tfrac{1}{2} \\cos(2 j)$ for $j = 1, 2, \\dots, 10$, with all angles in radians. Normalize this template to satisfy $\\|\\eta\\|_2 = \\sqrt{m}\\, \\sigma_{\\eta}$ by setting:\n  $$\\eta = w \\cdot \\frac{\\sqrt{m}\\, \\sigma_{\\eta}}{\\|w\\|_2}.$$\n- The data vector is $b = A x_{\\mathrm{true}} + \\eta$.\n\nThe program must compute $\\lambda$ by solving the scalar equation in item $1$ above using a robust numerical method that does not require user input. The residual norm must be computed exactly as the Euclidean norm of $A x_{\\lambda} - b$.\n\nTest Suite:\nProvide results for the following four test cases, each with $m = n = 10$ and the same $x_{\\mathrm{true}}$ and noise construction rule as above. For each case, $\\sigma_{\\eta}$ and $\\tau$ are specified, and the diagonal entries of $A$ (the singular values) are given explicitly.\n\n- Test $1$ (ill-conditioned, moderate noise): $\\{\\sigma_i\\} = \\{1.5, 1.0, 0.7, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01, 0.005\\}$, $\\sigma_{\\eta} = 0.02$, $\\tau = 1.0$.\n- Test $2$ (ill-conditioned, high noise): $\\{\\sigma_i\\} = \\{1.5, 1.0, 0.7, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01, 0.005\\}$, $\\sigma_{\\eta} = 0.10$, $\\tau = 1.0$.\n- Test $3$ (well-conditioned, moderate noise): $\\{\\sigma_i\\} = \\{1, 1, 1, 1, 1, 1, 1, 1, 1, 1\\}$, $\\sigma_{\\eta} = 0.02$, $\\tau = 1.0$.\n- Test $4$ (ill-conditioned, near-noiseless): $\\{\\sigma_i\\} = \\{1.5, 1.0, 0.7, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01, 0.005\\}$, $\\sigma_{\\eta} = 10^{-6}$, $\\tau = 1.0$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list of the form $[\\lambda, k(\\lambda), \\bar{s}(\\lambda)]$, where $\\lambda$ and $\\bar{s}(\\lambda)$ are floating-point numbers and $k(\\lambda)$ is an integer. For example, the overall output should look like $[[\\lambda_1,k_1,\\bar{s}_1],[\\lambda_2,k_2,\\bar{s}_2],[\\lambda_3,k_3,\\bar{s}_3],[\\lambda_4,k_4,\\bar{s}_4]]$.",
            "solution": "The problem requires the implementation of zero-order Tikhonov regularization for a linear inverse problem, with the regularization parameter $\\lambda$ determined by Morozov's discrepancy principle. The core of the task is to set up the problem deterministically for several test cases, solve for $\\lambda$, and then quantify the resulting suppression of modes corresponding to small singular values.\n\nThe linear inverse problem is given by $A x = b$, where the data vector $b$ is contaminated by additive noise: $b = A x_{\\mathrm{true}} + \\eta$. Zero-order Tikhonov regularization seeks to find a stable approximate solution by minimizing the functional\n$$J(x) = \\|A x - b\\|_2^2 + \\lambda^2 \\|x\\|_2^2$$\nwhere $\\lambda \\ge 0$ is the regularization parameter. The unique minimizer $x_{\\lambda}$ of $J(x)$, assuming $A$ has full column rank, is the solution to the normal equations $(A^\\top A + \\lambda^2 I) x = A^\\top b$. This yields\n$$x_{\\lambda} = (A^\\top A + \\lambda^2 I)^{-1} A^\\top b$$\n\nFor the specific context of this problem, the matrix $A$ is a diagonal $n \\times n$ matrix with its diagonal entries being the singular values $\\sigma_i$. Thus, $A = \\Sigma = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_n)$. The singular value decomposition (SVD) is trivial, with $U = V = I_n$. Substituting this into the expression for $x_{\\lambda}$ simplifies the calculations significantly:\n$$A^\\top A = \\Sigma^\\top \\Sigma = \\Sigma^2 = \\mathrm{diag}(\\sigma_1^2, \\dots, \\sigma_n^2)$$\n$$x_{\\lambda} = (\\Sigma^2 + \\lambda^2 I)^{-1} \\Sigma b$$\nThis is a diagonal system, so the solution can be expressed component-wise:\n$$(x_{\\lambda})_i = \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda^2} b_i$$\n\nMorozov’s discrepancy principle provides a rule for choosing $\\lambda$. It states that $\\lambda$ should be chosen such that the norm of the residual vector $r(\\lambda) = A x_{\\lambda} - b$ matches a target value related to the expected noise level. The principle is formally stated as:\n$$\\|A x_{\\lambda} - b\\|_2 = \\delta$$\nwhere $\\delta = \\tau \\sqrt{m} \\, \\sigma_{\\eta}$ is the target residual norm. Here, $m$ is the number of rows of $A$, $\\sigma_{\\eta}$ is the standard deviation of the noise components, and $\\tau$ is a given factor.\n\nTo apply this principle, we first derive an explicit expression for the residual norm in terms of $\\lambda$. The $i$-th component of the residual is:\n$$(A x_{\\lambda} - b)_i = \\sigma_i (x_{\\lambda})_i - b_i = \\sigma_i \\left( \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda^2} b_i \\right) - b_i = \\left( \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2} - 1 \\right) b_i = -\\frac{\\lambda^2}{\\sigma_i^2 + \\lambda^2} b_i$$\nThe squared Euclidean norm of the residual vector is then:\n$$\\|A x_{\\lambda} - b\\|_2^2 = \\sum_{i=1}^n \\left( \\frac{\\lambda^2}{\\sigma_i^2 + \\lambda^2} \\right)^2 b_i^2$$\nThis expression defines a function of $\\lambda$. Let $f(\\lambda) = \\|A x_{\\lambda} - b\\|_2$. The function $f(\\lambda)$ is monotonically increasing for $\\lambda \\ge 0$, with $f(0) = 0$ (since $\\sigma_i > 0$ for all $i$, $A$ is invertible) and $\\lim_{\\lambda \\to \\infty} f(\\lambda) = \\|b\\|_2$. Therefore, for a target value $\\delta$ in the range $(0, \\|b\\|_2)$, there exists a unique $\\lambda > 0$ that satisfies the discrepancy principle.\n\nThe task is to solve the scalar equation $f(\\lambda) = \\delta$, or equivalently $g(\\lambda) = f(\\lambda) - \\delta = 0$. This non-linear equation can be solved numerically using a robust root-finding algorithm. Given the monotonic nature of $f(\\lambda)$, Brent's method is an excellent choice, as it guarantees convergence provided an initial interval $[\\lambda_{min}, \\lambda_{max}]$ is found such that $g(\\lambda_{min})$ and $g(\\lambda_{max})$ have opposite signs. We can use a small positive number for $\\lambda_{min}$ (e.g., $10^{-12}$), for which $g(\\lambda_{min}) \\approx -\\delta < 0$. For $\\lambda_{max}$, a sufficiently large value will ensure $f(\\lambda_{max}) \\approx \\|b\\|_2$, making $g(\\lambda_{max}) > 0$ (assuming $\\delta < \\|b\\|_2$, which is expected).\n\nOnce the value of $\\lambda$ is determined for a given test case, we quantify the suppression of modes associated with small singular values. The mode-wise suppression factor for the $i$-th mode is given by $s_i(\\lambda) = \\frac{\\lambda^2}{\\sigma_i^2 + \\lambda^2}$. A mode is defined as \"strongly suppressed\" if its corresponding singular value $\\sigma_i$ is less than or equal to a fraction of the regularization parameter, specifically $\\sigma_i \\le \\alpha \\lambda$ with $\\alpha = 0.1$. The required outputs are:\n1. The count of strongly suppressed modes, $k(\\lambda) = \\#\\{i : \\sigma_i \\le 0.1 \\lambda\\}$.\n2. The average suppression across these modes, $\\bar{s}(\\lambda) = \\frac{1}{k(\\lambda)} \\sum_{\\{i : \\sigma_i \\le 0.1 \\lambda\\}} s_i(\\lambda)$, with the convention that $\\bar{s}(\\lambda) = 0$ if $k(\\lambda)=0$.\n\nThe overall algorithm for each test case is as follows:\n1. Construct the problem parameters: Set $m=n=10$. Construct the true signal $x_{\\mathrm{true}}$ with components $(x_{\\mathrm{true}})_i = 1/i$. Construct the noise template vector $w$ with components $w_j = \\sin(j) + 0.5 \\cos(2j)$.\n2. For the given singular values $\\{\\sigma_i\\}$, noise level $\\sigma_{\\eta}$, and discrepancy factor $\\tau$:\n    a. Form the diagonal matrix $A = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_{10})$.\n    b. Compute the target residual norm $\\delta = \\tau \\sqrt{m} \\sigma_{\\eta}$.\n    c. Construct the noise vector $\\eta = w \\cdot (\\sqrt{m} \\sigma_{\\eta} / \\|w\\|_2)$.\n    d. Construct the data vector $b = A x_{\\mathrm{true}} + \\eta$.\n3. Define the function $g(\\lambda) = \\left( \\sum_{i=1}^n \\left( \\frac{\\lambda^2}{\\sigma_i^2 + \\lambda^2} \\right)^2 b_i^2 \\right)^{1/2} - \\delta$.\n4. Use a numerical root-finder (Brent's method) to solve $g(\\lambda) = 0$ for $\\lambda > 0$.\n5. Using the obtained $\\lambda$, compute $k(\\lambda)$ and $\\bar{s}(\\lambda)$ according to their definitions.\n6. Assemble the results $[\\lambda, k(\\lambda), \\bar{s}(\\lambda)]$ for the test case.\nThis procedure is repeated for all test cases provided in the problem statement.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves the Tikhonov regularization problem with Morozov's discrepancy\n    principle for a set of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([1.5, 1.0, 0.7, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01, 0.005]), 0.02, 1.0),\n        (np.array([1.5, 1.0, 0.7, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01, 0.005]), 0.10, 1.0),\n        (np.array([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]), 0.02, 1.0),\n        (np.array([1.5, 1.0, 0.7, 0.5, 0.2, 0.1, 0.05, 0.02, 0.01, 0.005]), 1e-6, 1.0)\n    ]\n\n    # Universal problem parameters\n    m = 10\n    n = 10\n    alpha = 0.1\n\n    # Construct the true signal x_true\n    x_true = 1.0 / np.arange(1, n + 1)\n\n    # Construct the noise template vector w\n    j = np.arange(1, m + 1)\n    w = np.sin(j) + 0.5 * np.cos(2 * j)\n    w_norm = np.linalg.norm(w)\n    \n    results = []\n\n    for case in test_cases:\n        sigmas, sigma_eta, tau = case\n\n        # 1. Construct problem-specific vectors and matrices\n        A = np.diag(sigmas)\n        \n        # Target residual norm from Morozov's principle\n        delta = tau * np.sqrt(m) * sigma_eta\n        \n        # Construct the noise vector eta\n        eta = w * (np.sqrt(m) * sigma_eta / w_norm)\n        \n        # Construct the data vector b\n        b = A @ x_true + eta\n        \n        # 2. Find the regularization parameter lambda\n        def residual_norm_func(lmbda):\n            # Calculates ||A * x_lambda - b||_2\n            if lmbda <= 0:\n                return 0.0\n            \n            # Since A is diagonal, A*x_lambda - b has components\n            # - (lmbda^2 / (sigma_i^2 + lmbda^2)) * b_i\n            # The norm calculation squares these components, so the sign doesn't matter.\n            filter_factors = (lmbda**2) / (sigmas**2 + lmbda**2)\n            residual_components = filter_factors * b\n            return np.linalg.norm(residual_components)\n\n        def g(lmbda):\n            # Function to find the root of: ||A * x_lambda - b|| - delta = 0\n            return residual_norm_func(lmbda) - delta\n\n        # Find a bracketing interval for the root finder\n        # g(lambda->0) -> -delta < 0\n        # g(lambda->inf) -> ||b|| - delta > 0\n        # We need a large enough upper bound for the bracket.\n        # A value like 10 * max(sigmas) is usually sufficient.\n        lambda_min_bracket = 1e-12\n        lambda_max_bracket = 100 \n        \n        # Check if upper bracket is valid\n        if g(lambda_max_bracket) < 0:\n            # If our initial guess for the bracket is too small, increase it\n            lambda_max_bracket = np.linalg.norm(b) # A very large lambda makes residual norm approach ||b||\n\n        lambda_val = brentq(g, lambda_min_bracket, lambda_max_bracket)\n\n        # 3. Quantify suppression of small-singular-value modes\n        \n        # Find indices of strongly suppressed modes\n        suppressed_indices = np.where(sigmas <= alpha * lambda_val)[0]\n        \n        # Count of strongly suppressed modes\n        k_lambda = len(suppressed_indices)\n        \n        # Average suppression factor\n        if k_lambda == 0:\n            s_bar_lambda = 0.0\n        else:\n            # s_i(lambda) = lambda^2 / (sigma_i^2 + lambda^2)\n            suppressed_sigmas = sigmas[suppressed_indices]\n            suppression_factors = lambda_val**2 / (suppressed_sigmas**2 + lambda_val**2)\n            s_bar_lambda = np.mean(suppression_factors)\n            \n        results.append([lambda_val, k_lambda, s_bar_lambda])\n\n    # Final print statement in the exact required format.\n    # The format required is a list of lists.\n    # The default string representation of a list is what's needed.\n    # The precision of floats is determined by numpy's default string conversion.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The challenge of instability is not unique to the deterministic, least-squares framework. This final practice explores the same fundamental issue through the powerful lens of Bayesian inference . You will analyze how the structure of the forward operator $A$, specifically its rank and singular value spectrum, dictates whether a posterior distribution is well-defined when using an improper prior, providing a profound link between the nullspace of $A$ and the integrability of the posterior. This exercise demonstrates how the choice of a proper prior acts as an inherent form of regularization, guaranteeing a stable and meaningful probabilistic solution.",
            "id": "3391328",
            "problem": "Consider the linear inverse model with additive Gaussian noise, defined by the relation $y = A x + \\varepsilon$, where $A \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^{n}$, $y \\in \\mathbb{R}^{m}$, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ with known noise level $\\sigma > 0$. Adopt the Bayesian viewpoint in data assimilation, where the prior distribution for $x$ is either an improper flat prior $p(x) \\propto 1$ or a proper Gaussian prior $x \\sim \\mathcal{N}(0, \\alpha^{-1} I_n)$ with precision $\\alpha > 0$. The posterior density under the improper flat prior is proportional to the likelihood $p(y \\mid x) \\propto \\exp\\left( -\\|A x - y\\|_2^2 / (2 \\sigma^2) \\right)$. \n\nYour task is to construct and analyze examples that highlight instability from small singular values of the forward operator $A$ under the improper flat prior, showing when the posterior is ill-defined (i.e., not normalizable) and when it is numerically unstable due to near-null directions. Use the Singular Value Decomposition (SVD) of $A$ to reason about the posterior integrability and stability. The program you produce must, for each test case, classify the posterior as follows:\n\n- Output the integer $0$ if the posterior under the improper flat prior is ill-defined (the integral over $x$ of the posterior density fails to be finite).\n- Output the integer $1$ if the posterior under the improper flat prior is well-defined but numerically unstable due to small singular values, as determined by a threshold on the minimum singular value.\n- Output the integer $2$ if the posterior under the improper flat prior is well-defined and numerically stable (all singular values exceed the threshold).\n- Output the integer $3$ if a proper Gaussian prior is used and the posterior is well-defined by construction, even in the presence of nullspace directions in $A$.\n\nStart from the foundational definitions of Bayes’ theorem, Gaussian likelihoods, and properties of linear transformations and orthonormal changes of variables. You must not rely on any pre-supplied target formulas; derive all necessary conditions from these bases.\n\nImplement the decision procedure using the following specification:\n- For improper flat prior cases, the posterior is normalizable if and only if the matrix $A$ has full column rank, equivalently, all $n$ singular values of $A$ are strictly positive and $m \\ge n$. For numerical stability, introduce a threshold $\\tau > 0$ and declare instability if the minimum singular value is less than $\\tau$.\n- For proper Gaussian prior cases with $x \\sim \\mathcal{N}(0, \\alpha^{-1} I_n)$, the posterior is well-defined for any $A$ and any $m, n$, due to the added prior precision $\\alpha > 0$. Classify such cases as $3$.\n\nTest suite:\n- Case $1$ (well-defined and stable improper prior): $m = 3$, $n = 3$, $A = \\mathrm{diag}(1, \\tfrac{1}{4}, \\tfrac{1}{10})$, $y = (1, -1, 0)^\\top$, $\\sigma = 10^{-2}$, $\\tau = 10^{-8}$, improper flat prior.\n- Case $2$ (ill-defined improper prior due to nullspace): $m = 2$, $n = 3$, $A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix}$, $y = (0, 0)^\\top$, $\\sigma = 10^{-2}$, $\\tau = 10^{-8}$, improper flat prior.\n- Case $3$ (well-defined but numerically unstable improper prior due to near-null direction): $m = 3$, $n = 3$, $A = \\mathrm{diag}(1, 10^{-9}, \\tfrac{1}{2})$, $y = (0, 0, 0)^\\top$, $\\sigma = 10^{-2}$, $\\tau = 10^{-8}$, improper flat prior.\n- Case $4$ (ill-defined improper prior due to underdetermined system): $m = 3$, $n = 4$, $A = \\begin{pmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{pmatrix}$, $y = (1, 2, 3)^\\top$, $\\sigma = 10^{-2}$, $\\tau = 10^{-8}$, improper flat prior.\n- Case $5$ (well-defined posterior under proper Gaussian prior): Use the same $A$ and dimensions as Case $2$, but adopt a proper Gaussian prior $x \\sim \\mathcal{N}(0, \\alpha^{-1} I_3)$ with $\\alpha = 10$, $y = (0, 0)^\\top$, $\\sigma = 10^{-2}$, $\\tau = 10^{-8}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1, result_2, result_3, result_4, result_5]$), where each $result_i$ is one of the integers $0$, $1$, $2$, or $3$ defined above.",
            "solution": "The problem asks for an analysis of the posterior distribution in a linear Bayesian inverse problem, focusing on the conditions under which the posterior is well-defined and numerically stable. We are given the model $y = A x + \\varepsilon$, where $A \\in \\mathbb{R}^{m \\times n}$ is the forward operator, $x \\in \\mathbb{R}^{n}$ is the vector of unknown parameters, $y \\in \\mathbb{R}^{m}$ is the data vector, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ is additive Gaussian noise with a known variance $\\sigma^2 > 0$.\n\nThe analysis depends on the choice of the prior distribution for $x$, $p(x)$. We will derive the conditions for a well-defined and stable posterior from foundational principles, specifically Bayes' theorem, which states that the posterior probability density is proportional to the product of the likelihood and the prior:\n$$p(x \\mid y) \\propto p(y \\mid x) p(x)$$\n\nThe likelihood function $p(y \\mid x)$ is determined by the noise model. Since $\\varepsilon = y - Ax$ and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_m)$, the likelihood is a multivariate Gaussian function:\n$$p(y \\mid x) = \\frac{1}{(2\\pi\\sigma^2)^{m/2}} \\exp\\left( -\\frac{1}{2\\sigma^2} \\|y - Ax\\|_2^2 \\right)$$\n\nWe consider two scenarios based on the choice of prior.\n\n**Scenario 1: Improper Flat Prior**\n\nIn this case, the prior is assumed to be uniform over $\\mathbb{R}^n$, i.e., $p(x) \\propto 1$. This is an \"improper\" prior because its integral over $\\mathbb{R}^n$ is not finite. The validity of the resulting posterior hinges on whether the likelihood function itself is integrable.\n\nThe posterior density is proportional to the likelihood:\n$$p(x \\mid y) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\|Ax - y\\|_2^2 \\right)$$\nFor this posterior to be \"well-defined\" or \"normalizable,\" its integral over all possible values of $x$ must be finite:\n$$Z = \\int_{\\mathbb{R}^n} \\exp\\left( -\\frac{1}{2\\sigma^2} \\|Ax - y\\|_2^2 \\right) dx < \\infty$$\nThe term $Z$ is the normalization constant, also known as the evidence. The exponent contains a quadratic form in $x$:\n$$\\|Ax - y\\|_2^2 = (Ax - y)^\\top(Ax - y) = x^\\top A^\\top A x - 2y^\\top A x + y^\\top y$$\nThe integral $\\int_{\\mathbb{R}^n} \\exp(-\\frac{1}{2\\sigma^2}(x^\\top A^\\top A x - 2y^\\top A x + y^\\top y)) dx$ takes the form of a multivariate Gaussian integral. Such an integral converges if and only if the matrix of the quadratic term, $A^\\top A$, is positive definite.\n\nTo analyze the properties of the $n \\times n$ matrix $A^\\top A$, we use the Singular Value Decomposition (SVD) of $A$. Let $A = U S V^\\top$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $S \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix containing the singular values $s_1 \\ge s_2 \\ge \\dots \\ge 0$.\nSubstituting the SVD into the expression for $A^\\top A$:\n$$A^\\top A = (U S V^\\top)^\\top (U S V^\\top) = V S^\\top U^\\top U S V^\\top = V (S^\\top S) V^\\top$$\nThis shows that $A^\\top A$ is similar to $S^\\top S$. $S^\\top S$ is an $n \\times n$ diagonal matrix whose diagonal entries are $s_i^2$ for $i=1, \\dots, \\min(m, n)$ and zero otherwise. The eigenvalues of $A^\\top A$ are therefore $s_i^2 \\ge 0$.\n\nFor $A^\\top A$ to be positive definite, all of its eigenvalues must be strictly positive. This requires that all $n$ of its eigenvalues are greater than $0$. This is only possible if $A$ has full column rank, i.e., $\\text{rank}(A) = n$. This implies two conditions:\n1.  The number of rows must be at least the number of columns, $m \\ge n$. If $m < n$, the rank of $A$ can be at most $m$, so $A^\\top A$ will have at least $n-m$ zero eigenvalues.\n2.  All $n$ singular values of $A$ must be strictly positive, $s_n > 0$.\n\nIf $\\text{rank}(A)  n$, there exists a non-trivial nullspace for $A$. Any vector $x_0$ in the nullspace $\\mathcal{N}(A)$ satisfies $Ax_0 = 0$. The posterior density becomes constant along any direction in the nullspace, and integrating over this infinite direction yields a divergent integral. Therefore, the posterior is ill-defined. This corresponds to **classification 0**.\n\nIf $\\text{rank}(A) = n$, the posterior is well-defined. The stability of the solution is then determined by the conditioning of $A^\\top A$. The solution to the corresponding least-squares problem $x_{LS} = (A^\\top A)^{-1} A^\\top y$ involves the inverse of $A^\\top A$, whose eigenvalues are $1/s_i^2$. If any singular value $s_i$ is very small (but non-zero), the corresponding eigenvalue $1/s_i^2$ is very large. This indicates that small perturbations in the data $y$, amplified by $1/s_i$, can lead to enormous changes in the solution $x$, a hallmark of numerical instability.\nFollowing the problem's specification, we use a threshold $\\tau > 0$.\n- If the minimum singular value $\\min(s_i)  \\tau$, the problem is well-defined but numerically unstable. This corresponds to **classification 1**.\n- If $\\min(s_i) \\ge \\tau$, the problem is well-defined and numerically stable. This corresponds to **classification 2**.\n\n**Scenario 2: Proper Gaussian Prior**\n\nHere, the prior is $x \\sim \\mathcal{N}(0, \\alpha^{-1} I_n)$ for some precision $\\alpha > 0$. The prior density is:\n$$p(x) \\propto \\exp\\left( -\\frac{\\alpha}{2} \\|x\\|_2^2 \\right) = \\exp\\left( -\\frac{\\alpha}{2} x^\\top x \\right)$$\nThe posterior density is:\n$$p(x \\mid y) \\propto \\exp\\left( -\\frac{1}{2\\sigma^2} \\|Ax - y\\|_2^2 \\right) \\exp\\left( -\\frac{\\alpha}{2} \\|x\\|_2^2 \\right)$$\n$$p(x \\mid y) \\propto \\exp\\left( -\\frac{1}{2} \\left( \\frac{1}{\\sigma^2} \\|Ax - y\\|_2^2 + \\alpha \\|x\\|_2^2 \\right) \\right)$$\nThe argument of the exponential is a quadratic function of $x$. The term quadratic in $x$ is:\n$$\\frac{1}{\\sigma^2} x^\\top A^\\top A x + \\alpha x^\\top I_n x = x^\\top \\left( \\frac{1}{\\sigma^2} A^\\top A + \\alpha I_n \\right) x$$\nFor the posterior to be a normalizable Gaussian distribution, the matrix $H = \\frac{1}{\\sigma^2} A^\\top A + \\alpha I_n$ must be positive definite.\nWe know $A^\\top A$ is positive semi-definite (its eigenvalues are $s_i^2 \\ge 0$). The matrix $\\alpha I_n$ is positive definite since $\\alpha > 0$ (its eigenvalues are all $\\alpha$). The sum of a positive semi-definite matrix and a positive definite matrix is always positive definite.\nTherefore, $H$ is always positive definite for any $A$, regardless of its rank. The posterior is always a proper, well-defined Gaussian distribution. The prior \"regularizes\" the problem, ensuring a unique and stable posterior distribution even when $A$ is rank-deficient. This situation is assigned **classification 3**.\n\n**Applying the Decision Logic to Test Cases:**\n\n- **Case 1**: Improper prior. $A = \\mathrm{diag}(1, \\frac{1}{4}, \\frac{1}{10})$, so $m=3, n=3$. The matrix has full rank ($\\text{rank}(A) = 3 = n$). Singular values are $1, 0.25, 0.1$. The minimum singular value is $0.1$. Given $\\tau = 10^{-8}$, we have $0.1 \\ge 10^{-8}$. The posterior is well-defined and stable. **Result: 2**.\n\n- **Case 2**: Improper prior. $A = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix}$, so $m=2, n=3$. Since $m  n$, the rank is at most $2$. $\\text{rank}(A) = 2  n = 3$. The matrix does not have full column rank. The posterior is ill-defined. **Result: 0**.\n\n- **Case 3**: Improper prior. $A = \\mathrm{diag}(1, 10^{-9}, \\frac{1}{2})$, so $m=3, n=3$. The matrix has full rank ($\\text{rank}(A) = 3 = n$). Singular values are $1, 0.5, 10^{-9}$. The minimum singular value is $10^{-9}$. Given $\\tau = 10^{-8}$, we have $10^{-9}  10^{-8}$. The posterior is well-defined but numerically unstable. **Result: 1**.\n\n- **Case 4**: Improper prior. $A = \\begin{pmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  0  1  0 \\end{pmatrix}$, so $m=3, n=4$. Since $m  n$, the rank is at most $3$. $\\text{rank}(A) = 3  n = 4$. The matrix does not have full column rank. The posterior is ill-defined. **Result: 0**.\n\n- **Case 5**: Proper Gaussian prior. The matrix $A$ is the same as in Case $2$, which is rank-deficient. However, because a proper Gaussian prior is used, the posterior is guaranteed to be well-defined by construction. **Result: 3**.\n\nThe final program will implement this decision logic.",
            "answer": "```python\nimport numpy as np\n\ndef classify_posterior(A, tau, prior_type):\n    \"\"\"\n    Classifies the posterior distribution based on the properties of the\n    forward operator A and the choice of prior.\n\n    Args:\n        A (np.ndarray): The forward operator matrix.\n        tau (float): The threshold for numerical stability.\n        prior_type (str): The type of prior ('improper' or 'proper').\n\n    Returns:\n        int: The classification code (0, 1, 2, or 3).\n    \"\"\"\n    if prior_type == 'proper':\n        # For a proper Gaussian prior, the posterior is always well-defined.\n        return 3\n\n    # For an improper flat prior, analyze the matrix A.\n    m, n = A.shape\n\n    # The posterior is normalizable if and only if A has full column rank.\n    # We check this using numpy.linalg.matrix_rank.\n    # Note: np.linalg.matrix_rank determines rank by counting singular values\n    # above a tolerance, which is the standard numerical approach.\n    rank = np.linalg.matrix_rank(A)\n\n    if rank  n:\n        # If A is not full column rank, the posterior is ill-defined.\n        return 0\n    else:\n        # If A is full column rank, the posterior is well-defined.\n        # Now check for numerical stability using singular values.\n        # We only need the singular values, not the full SVD.\n        singular_values = np.linalg.svd(A, compute_uv=False)\n        \n        # The smallest singular value determines stability.\n        # For a full-rank square or tall matrix, all singular values are non-zero.\n        min_singular_value = np.min(singular_values)\n\n        if min_singular_value  tau:\n            # Well-defined but numerically unstable.\n            return 1\n        else:\n            # Well-defined and numerically stable.\n            return 2\n\ndef solve():\n    \"\"\"\n    Runs the analysis on the test suite and prints the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.diag([1.0, 1.0/4.0, 1.0/10.0]),\n            \"tau\": 1e-8,\n            \"prior_type\": \"improper\"\n        },\n        {\n            \"A\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]),\n            \"tau\": 1e-8,\n            \"prior_type\": \"improper\"\n        },\n        {\n            \"A\": np.diag([1.0, 1e-9, 0.5]),\n            \"tau\": 1e-8,\n            \"prior_type\": \"improper\"\n        },\n        {\n            \"A\": np.array([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]]),\n            \"tau\": 1e-8,\n            \"prior_type\": \"improper\"\n        },\n        {\n            \"A\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]),\n            \"tau\": 1e-8,\n            \"prior_type\": \"proper\"\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = classify_posterior(case[\"A\"], case[\"tau\"], case[\"prior_type\"])\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}