{
    "hands_on_practices": [
        {
            "introduction": "This first exercise takes us to the heart of Bayesian inference. We will work through a foundational one-dimensional linear problem to see precisely how prior knowledge, represented by a probability measure, is updated by an observation to yield a posterior measure. By deriving the Radon-Nikodym derivative of the posterior with respect to the prior , you will solidify your understanding of how Bayes' theorem operates at a measure-theoretic level and practice computing posterior expectations in the ubiquitous Gaussian conjugate model.",
            "id": "3414496",
            "problem": "Consider a one-dimensional linear inverse problem with an unknown parameter $U \\in \\mathbb{R}$ modeled as a random variable with a Gaussian prior distribution $U \\sim \\mathcal{N}(m_{0}, s_{0}^{2})$. An observed datum $y \\in \\mathbb{R}$ is generated by the forward model $y = a u + \\eta$, where $u$ denotes a realization of $U$, $a \\in \\mathbb{R}$ is a known scalar, and $\\eta$ is independent additive noise with distribution $\\eta \\sim \\mathcal{N}(0, \\sigma^{2})$. The associated negative log-likelihood is defined by\n$$\n\\Phi(u; y) = \\frac{1}{2 \\sigma^{2}} \\left( y - a u \\right)^{2},\n$$\nand the unnormalized likelihood is given by $L(y \\mid u) = \\exp\\left( - \\Phi(u; y) \\right)$. Let $\\mu_{0}$ denote the prior measure of $U$ on $\\mathbb{R}$ and $\\mu^{y}$ the Bayesian posterior measure given the observation $y$.\n\nStarting from fundamental measure-theoretic Bayes principles and core properties of Gaussian integrals, derive the Radon–Nikodym derivative (RND) $\\frac{d \\mu^{y}}{d \\mu_{0}}(u)$ and its normalizing constant $Z(y)$ defined implicitly by the requirement that $\\mu^{y}$ is a probability measure. Then, using the prior-weighted averaging identity,\n$$\n\\mathbb{E}_{\\mu^{y}}[f(U)] = \\frac{ \\int_{\\mathbb{R}} f(u) \\, L(y \\mid u) \\, d\\mu_{0}(u) }{ \\int_{\\mathbb{R}} L(y \\mid u) \\, d\\mu_{0}(u) },\n$$\nevaluate the posterior expectation for the choice $f(u) = u$, with the specific parameter values\n$$\nm_{0} = 0.8, \\quad s_{0}^{2} = 0.49, \\quad a = 1.5, \\quad \\sigma^{2} = 0.36, \\quad y = 1.9.\n$$\nExpress the final numerical value of $\\mathbb{E}_{\\mu^{y}}[U]$ and round your answer to four significant figures. No physical units are associated with this quantity.",
            "solution": "The user-provided problem is valid as it is scientifically grounded in Bayesian statistics and inverse problem theory, is well-posed with a unique solution, objective, and self-contained with all necessary parameters and definitions.\n\nThe problem requires the derivation of the Radon–Nikodym derivative (RND) of the posterior measure with respect to the prior measure, its normalizing constant, and the subsequent calculation of the posterior expectation $\\mathbb{E}_{\\mu^{y}}[U]$.\n\nThe relationship between the posterior measure $\\mu^{y}$ and the prior measure $\\mu_{0}$ is given by Bayes' theorem. The provided expectation identity,\n$$\n\\mathbb{E}_{\\mu^{y}}[f(U)] = \\frac{ \\int_{\\mathbb{R}} f(u) \\, L(y \\mid u) \\, d\\mu_{0}(u) }{ \\int_{\\mathbb{R}} L(y \\mid u) \\, d\\mu_{0}(u) }\n$$\ncan be compared to the definition of expectation with respect to the posterior measure, $\\mathbb{E}_{\\mu^{y}}[f(U)] = \\int_{\\mathbb{R}} f(u) \\, d\\mu^{y}(u)$.\nBy the properties of measures, this implies that the Radon–Nikodym derivative of $\\mu^{y}$ with respect to $\\mu_{0}$ is given by\n$$\n\\frac{d \\mu^{y}}{d \\mu_{0}}(u) = \\frac{L(y \\mid u)}{Z(y)}\n$$\nwhere $Z(y)$ is the normalizing constant, also known as the evidence or marginal likelihood, defined as\n$$\nZ(y) = \\int_{\\mathbb{R}} L(y \\mid u) \\, d\\mu_{0}(u)\n$$\nsuch that $\\int_{\\mathbb{R}} d\\mu^{y}(u) = 1$.\n\nThe prior on $U$ is Gaussian, $U \\sim \\mathcal{N}(m_{0}, s_{0}^{2})$, so its probability density function (PDF) is $p_{0}(u) = \\frac{1}{\\sqrt{2\\pi s_{0}^{2}}} \\exp\\left(-\\frac{(u-m_{0})^{2}}{2s_{0}^{2}}\\right)$. The prior measure is $d\\mu_{0}(u) = p_{0}(u)du$.\nThe likelihood is given by $L(y \\mid u) = \\exp(-\\Phi(u;y)) = \\exp\\left(-\\frac{(y-au)^{2}}{2\\sigma^{2}}\\right)$.\n\nThe posterior PDF, $p(u \\mid y)$, is proportional to the product of the likelihood and the prior PDF:\n$$\np(u \\mid y) \\propto L(y \\mid u) p_0(u)\n$$\nSubstituting the expressions for the likelihood and prior density, we get:\n$$\np(u \\mid y) \\propto \\exp\\left(-\\frac{(y-au)^{2}}{2\\sigma^{2}}\\right) \\frac{1}{\\sqrt{2\\pi s_{0}^{2}}} \\exp\\left(-\\frac{(u-m_{0})^{2}}{2s_{0}^{2}}\\right)\n$$\nThe posterior is proportional to the exponential of a sum of quadratic terms in $u$. We can ignore constant factors for now and focus on the exponent, denoted as $J(u)$:\n$$\nJ(u) = -\\frac{1}{2} \\left[ \\frac{(y-au)^{2}}{\\sigma^{2}} + \\frac{(u-m_{0})^{2}}{s_{0}^{2}} \\right]\n$$\nExpanding the quadratic terms inside the brackets:\n$$\nJ(u) = -\\frac{1}{2} \\left[ \\frac{y^2-2ayu+a^2u^2}{\\sigma^{2}} + \\frac{u^2-2um_{0}+m_{0}^2}{s_{0}^{2}} \\right]\n$$\nGrouping terms by powers of $u$:\n$$\nJ(u) = -\\frac{1}{2} \\left[ u^2 \\left(\\frac{a^2}{\\sigma^2} + \\frac{1}{s_0^2}\\right) - 2u \\left(\\frac{ay}{\\sigma^2} + \\frac{m_0}{s_0^2}\\right) + \\left(\\frac{y^2}{\\sigma^2} + \\frac{m_0^2}{s_0^2}\\right) \\right]\n$$\nSince $J(u)$ is a quadratic function of $u$, the posterior distribution for $U$ given $y$ is also Gaussian, say $U|y \\sim \\mathcal{N}(m_{\\text{post}}, s_{\\text{post}}^2)$. The PDF of this posterior is of the form $p(u \\mid y) \\propto \\exp\\left(-\\frac{(u-m_{\\text{post}})^2}{2s_{\\text{post}}^2}\\right)$.\nComparing the form of $J(u)$ to the exponent of a Gaussian PDF, we can identify the posterior variance $s_{\\text{post}}^2$ and mean $m_{\\text{post}}$. The term multiplying $u^2$ gives the inverse of the posterior variance (the precision):\n$$\n\\frac{1}{s_{\\text{post}}^2} = \\frac{a^2}{\\sigma^2} + \\frac{1}{s_0^2}\n$$\nSo, the posterior variance is $s_{\\text{post}}^2 = \\left(\\frac{a^2}{\\sigma^2} + \\frac{1}{s_0^2}\\right)^{-1}$.\nThe term multiplying $-2u$ gives the ratio of the posterior mean to the posterior variance:\n$$\n\\frac{m_{\\text{post}}}{s_{\\text{post}}^2} = \\frac{ay}{\\sigma^2} + \\frac{m_0}{s_0^2}\n$$\nSolving for the posterior mean $m_{\\text{post}}$:\n$$\nm_{\\text{post}} = s_{\\text{post}}^2 \\left(\\frac{ay}{\\sigma^2} + \\frac{m_0}{s_0^2}\\right) = \\left(\\frac{a^2}{\\sigma^2} + \\frac{1}{s_0^2}\\right)^{-1} \\left(\\frac{ay}{\\sigma^2} + \\frac{m_0}{s_0^2}\\right)\n$$\nThe posterior expectation is $\\mathbb{E}_{\\mu^{y}}[U] = m_{\\text{post}}$.\n\nThe normalizing constant $Z(y)$ can be found by evaluating the integral $Z(y) = \\int_{\\mathbb{R}} L(y|u) p_0(u) du$. Recognizing that the forward model $y = aU + \\eta$ implies $y$ is a sum of Gaussian random variables, $y$ itself is Gaussian with mean $\\mathbb{E}[y] = a m_0$ and variance $\\text{Var}(y) = a^2 s_0^2 + \\sigma^2$. The marginal PDF of $y$ is $p(y) = \\frac{1}{\\sqrt{2\\pi(a^2s_0^2+\\sigma^2)}} \\exp\\left(-\\frac{(y-am_0)^2}{2(a^2s_0^2+\\sigma^2)}\\right)$.\nFrom Bayes' rule for densities, $p(y) = \\int p(y|u)p(u)du = \\int \\frac{1}{\\sqrt{2\\pi\\sigma^2}} L(y|u) p_0(u) du = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}Z(y)$.\nTherefore, $Z(y) = \\sqrt{2\\pi\\sigma^2}p(y) = \\sqrt{\\frac{\\sigma^2}{a^2s_0^2+\\sigma^2}} \\exp\\left(-\\frac{(y-am_0)^2}{2(a^2s_0^2+\\sigma^2)}\\right)$.\nThe RND is then $\\frac{d\\mu^y}{d\\mu_0}(u) = \\frac{L(y|u)}{Z(y)}$.\n\nWe now proceed with the numerical calculation of the posterior expectation $\\mathbb{E}_{\\mu^{y}}[U] = m_{\\text{post}}$ using the given values:\n$m_{0} = 0.8$, $s_{0}^{2} = 0.49$, $a = 1.5$, $\\sigma^{2} = 0.36$, $y = 1.9$.\n\nFirst, we calculate the components of the posterior precision $\\frac{1}{s_{\\text{post}}^2}$:\n$$\n\\frac{1}{s_{0}^{2}} = \\frac{1}{0.49} = \\frac{100}{49}\n$$\n$$\n\\frac{a^2}{\\sigma^2} = \\frac{(1.5)^2}{0.36} = \\frac{2.25}{0.36} = \\frac{225}{36} = \\frac{25}{4}\n$$\nThe posterior precision is:\n$$\n\\frac{1}{s_{\\text{post}}^2} = \\frac{25}{4} + \\frac{100}{49} = \\frac{25 \\times 49 + 100 \\times 4}{4 \\times 49} = \\frac{1225 + 400}{196} = \\frac{1625}{196}\n$$\nSo, the posterior variance is $s_{\\text{post}}^2 = \\frac{196}{1625}$.\n\nNext, we calculate the terms for the numerator of $m_{\\text{post}}$:\n$$\n\\frac{m_0}{s_0^2} = \\frac{0.8}{0.49} = \\frac{80}{49}\n$$\n$$\n\\frac{ay}{\\sigma^2} = \\frac{1.5 \\times 1.9}{0.36} = \\frac{2.85}{0.36} = \\frac{285}{36} = \\frac{95}{12}\n$$\nThe sum is:\n$$\n\\frac{ay}{\\sigma^2} + \\frac{m_0}{s_0^2} = \\frac{95}{12} + \\frac{80}{49} = \\frac{95 \\times 49 + 80 \\times 12}{12 \\times 49} = \\frac{4655 + 960}{588} = \\frac{5615}{588}\n$$\nFinally, we compute the posterior mean $m_{\\text{post}}$:\n$$\nm_{\\text{post}} = s_{\\text{post}}^2 \\left(\\frac{ay}{\\sigma^2} + \\frac{m_0}{s_0^2}\\right) = \\frac{196}{1625} \\times \\frac{5615}{588}\n$$\nRecognizing that $588 = 3 \\times 196$:\n$$\nm_{\\text{post}} = \\frac{196}{1625} \\times \\frac{5615}{3 \\times 196} = \\frac{5615}{3 \\times 1625}\n$$\nSimplify the fraction:\n$$\n\\frac{5615}{1625} = \\frac{5 \\times 1123}{5 \\times 325} = \\frac{1123}{325}\n$$\nSo,\n$$\nm_{\\text{post}} = \\frac{1}{3} \\times \\frac{1123}{325} = \\frac{1123}{975}\n$$\nPerforming the division to get a numerical value:\n$$\nm_{\\text{post}} = \\frac{1123}{975} \\approx 1.15179487...\n$$\nRounding to four significant figures, we get $1.152$.\n\nThus, the posterior expectation of $U$ is $\\mathbb{E}_{\\mu^{y}}[U] \\approx 1.152$.",
            "answer": "$$\\boxed{1.152}$$"
        },
        {
            "introduction": "Before attempting to solve an inverse problem, it is crucial to understand what information the data can provide about the unknown parameters. This exercise introduces the Fisher Information Matrix ($I(u)$), a central tool for quantifying the information content of an experiment. By calculating the FIM and analyzing its null space , you will learn to identify 'poorly informed directions'—combinations of parameters that the data cannot effectively constrain, a key step in assessing parameter identifiability.",
            "id": "3414516",
            "problem": "Consider a linear-Gaussian data model in which the observations $Y \\in \\mathbb{R}^{3}$ are related to the unknown parameter $u \\in \\mathbb{R}^{4}$ by $Y \\sim \\mathcal{N}(A u, \\Gamma)$, where $A \\in \\mathbb{R}^{3 \\times 4}$ and $\\Gamma \\in \\mathbb{R}^{3 \\times 3}$ are given by\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0  1  0 \\\\\n0  1  0  1 \\\\\n1  1  1  1\n\\end{pmatrix},\n\\qquad\n\\Gamma \\;=\\; \\begin{pmatrix}\n2  0  0 \\\\\n0  3  1 \\\\\n0  1  2\n\\end{pmatrix}.\n$$\nStarting from the fundamental definition of the Fisher Information Matrix (FIM) for a parameter $u$ in a parametric family of probability densities $p(y \\mid u)$ as the expectation of the negative Hessian of the log-likelihood,\n$$\nI(u) \\;=\\; \\mathbb{E}_{Y \\mid u}\\!\\left[ \\;-\\nabla_{u}^{2} \\ln p(Y \\mid u) \\;\\right],\n$$\nderive the Fisher Information Matrix $I(u)$ for this model. Then, compute $I(u)$ explicitly for the matrices $A$ and $\\Gamma$ provided above and use it to characterize the poorly informed directions of $u$ as those in the null space of $I(u)$. Finally, determine the dimension of this null space.\n\nProvide your final answer as an exact integer (no rounding). Do not include any units in your final answer.",
            "solution": "The user wants a detailed solution to the given problem.\n\n### Step 1: Problem Validation\n\nFirst, I will validate the problem statement.\n\n#### **Givens Extraction**\n\nThe problem provides the following information:\n1.  A linear-Gaussian data model: $Y \\sim \\mathcal{N}(A u, \\Gamma)$, where $Y \\in \\mathbb{R}^{3}$ is the observation vector and $u \\in \\mathbb{R}^{4}$ is the unknown parameter vector.\n2.  The forward operator matrix $A \\in \\mathbb{R}^{3 \\times 4}$:\n    $$\n    A \\;=\\; \\begin{pmatrix}\n    1  0  1  0 \\\\\n    0  1  0  1 \\\\\n    1  1  1  1\n    \\end{pmatrix}\n    $$\n3.  The noise covariance matrix $\\Gamma \\in \\mathbb{R}^{3 \\times 3}$:\n    $$\n    \\Gamma \\;=\\; \\begin{pmatrix}\n    2  0  0 \\\\\n    0  3  1 \\\\\n    0  1  2\n    \\end{pmatrix}\n    $$\n4.  The definition of the Fisher Information Matrix (FIM) for a parameter $u$ in a family of probability densities $p(y \\mid u)$:\n    $$\n    I(u) \\;=\\; \\mathbb{E}_{Y \\mid u}\\!\\left[ \\;-\\nabla_{u}^{2} \\ln p(Y \\mid u) \\;\\right]\n    $$\n\n#### **Validation using Extracted Givens**\n\n1.  **Scientific Grounding**: The problem is set within the standard framework of Bayesian inverse problems and statistical inference. The linear-Gaussian model is a fundamental and widely used model. The definition of the Fisher Information Matrix is correct. The problem is scientifically and mathematically sound.\n2.  **Well-Posedness and Consistency**: For the model to be valid, the covariance matrix $\\Gamma$ must be symmetric and positive semi-definite.\n    *   Symmetry: The given matrix $\\Gamma$ is symmetric, as $\\Gamma_{ij} = \\Gamma_{ji}$.\n    *   Positive definiteness: We can check the eigenvalues of $\\Gamma$.\n        $$\n        \\det(\\Gamma - \\lambda I) \\;=\\; \\det \\begin{pmatrix} 2-\\lambda  0  0 \\\\ 0  3-\\lambda  1 \\\\ 0  1  2-\\lambda \\end{pmatrix} \\;=\\; (2-\\lambda) \\left[ (3-\\lambda)(2-\\lambda) - 1 \\right] \\;=\\; (2-\\lambda)(\\lambda^2 - 5\\lambda + 5) \\;=\\; 0\n        $$\n        One eigenvalue is $\\lambda_1 = 2$. The other two are roots of $\\lambda^2 - 5\\lambda + 5 = 0$, which are $\\lambda_{2,3} = \\frac{5 \\pm \\sqrt{25 - 20}}{2} = \\frac{5 \\pm \\sqrt{5}}{2}$. All three eigenvalues, $2$, $\\frac{5 - \\sqrt{5}}{2}$, and $\\frac{5 + \\sqrt{5}}{2}$, are positive. Therefore, $\\Gamma$ is positive definite, which is a stronger condition and confirms it is a valid covariance matrix. The problem is self-contained and internally consistent.\n3.  **Objectivity**: The problem is stated in precise, objective mathematical language, free from ambiguity or subjective claims.\n\nThe problem is valid.\n\n### Step 2: Solution\n\n#### **Derivation of the Fisher Information Matrix**\n\nWe start from the probability density function (PDF) for a multivariate normal distribution. Given $Y \\sim \\mathcal{N}(Au, \\Gamma)$, the PDF is:\n$$\np(y \\mid u) = \\frac{1}{\\sqrt{(2\\pi)^k \\det(\\Gamma)}} \\exp\\left(-\\frac{1}{2} (y - Au)^T \\Gamma^{-1} (y - Au)\\right)\n$$\nwhere $k=3$ is the dimension of $Y$.\n\nThe log-likelihood, $\\mathcal{L}(u) = \\ln p(y \\mid u)$, is:\n$$\n\\mathcal{L}(u) = -\\frac{k}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln(\\det(\\Gamma)) - \\frac{1}{2} (y - Au)^T \\Gamma^{-1} (y - Au)\n$$\nTo find the FIM, we need the Hessian of $\\mathcal{L}(u)$ with respect to $u$. First, we compute the gradient, $\\nabla_u \\mathcal{L}(u)$. The terms not depending on $u$ vanish upon differentiation.\n$$\n\\nabla_u \\mathcal{L}(u) = \\nabla_u \\left( -\\frac{1}{2} (y - Au)^T \\Gamma^{-1} (y - Au) \\right)\n$$\nLet's expand the quadratic form:\n$$\n(y - Au)^T \\Gamma^{-1} (y - Au) = y^T \\Gamma^{-1} y - y^T \\Gamma^{-1} Au - u^T A^T \\Gamma^{-1} y + u^T A^T \\Gamma^{-1} Au\n$$\nSince $\\Gamma$ is symmetric, $\\Gamma^{-1}$ is also symmetric. Thus, $y^T \\Gamma^{-1} Au = (u^T A^T \\Gamma^{-1} y)^T = u^T A^T \\Gamma^{-1} y$. The quadratic form becomes:\n$$\ny^T \\Gamma^{-1} y - 2 u^T A^T \\Gamma^{-1} y + u^T (A^T \\Gamma^{-1} A) u\n$$\nThe gradient with respect to $u$ is:\n$$\n\\nabla_u \\left( y^T \\Gamma^{-1} y - 2 u^T A^T \\Gamma^{-1} y + u^T (A^T \\Gamma^{-1} A) u \\right) = -2 A^T \\Gamma^{-1} y + 2 (A^T \\Gamma^{-1} A) u = 2 A^T \\Gamma^{-1} (Au - y)\n$$\nSo, the gradient of the log-likelihood is:\n$$\n\\nabla_u \\mathcal{L}(u) = -\\frac{1}{2} \\left[ 2 A^T \\Gamma^{-1} (Au - y) \\right] = A^T \\Gamma^{-1} (y - Au)\n$$\nNext, we compute the Hessian matrix, $\\nabla_u^2 \\mathcal{L}(u)$, by differentiating the gradient with respect to $u^T$:\n$$\n\\nabla_u^2 \\mathcal{L}(u) = \\frac{\\partial}{\\partial u^T} \\left( A^T \\Gamma^{-1} y - A^T \\Gamma^{-1} A u \\right)\n$$\nThe term $A^T \\Gamma^{-1} y$ is constant with respect to $u$. The derivative of the linear term $- (A^T \\Gamma^{-1} A) u$ is $-A^T \\Gamma^{-1} A$.\n$$\n\\nabla_u^2 \\mathcal L(u) = -A^T\\Gamma^{-1}A\n$$\nThe Hessian is independent of both the parameter $u$ and the data $y$. Now we apply the definition of the FIM:\n$$\nI(u) = \\mathbb{E}_{Y \\mid u} \\left[ -\\nabla_{u}^{2} \\ln p(Y \\mid u) \\right] = \\mathbb{E}_{Y \\mid u} \\left[ -(-A^T\\Gamma^{-1}A) \\right] = \\mathbb{E}_{Y \\mid u} \\left[ A^T\\Gamma^{-1}A \\right]\n$$\nSince the expression $A^T\\Gamma^{-1}A$ is deterministic and does not depend on the random variable $Y$, the expectation is the expression itself.\n$$\nI(u) = A^T\\Gamma^{-1}A\n$$\nThe FIM is constant, independent of $u$, so we denote it as $I$.\n\n#### **Explicit Computation of the FIM**\n\nWe need to compute $I = A^T\\Gamma^{-1}A$. First, we find the inverse of $\\Gamma$.\n$$\n\\Gamma = \\begin{pmatrix} 2  0  0 \\\\ 0  3  1 \\\\ 0  1  2 \\end{pmatrix}\n$$\nThe determinant is $\\det(\\Gamma) = 2(3 \\cdot 2 - 1 \\cdot 1) = 2(5) = 10$.\nThe inverse is $\\Gamma^{-1} = \\frac{1}{\\det(\\Gamma)} \\text{adj}(\\Gamma)$. The adjugate matrix is the transpose of the cofactor matrix.\nThe cofactor matrix $C$ is:\n$$\nC = \\begin{pmatrix}\n\\det\\begin{pmatrix} 3  1 \\\\ 1  2 \\end{pmatrix}  -\\det\\begin{pmatrix} 0  1 \\\\ 0  2 \\end{pmatrix}  \\det\\begin{pmatrix} 0  3 \\\\ 0  1 \\end{pmatrix} \\\\\n-\\det\\begin{pmatrix} 0  0 \\\\ 1  2 \\end{pmatrix}  \\det\\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix}  -\\det\\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix} \\\\\n\\det\\begin{pmatrix} 0  0 \\\\ 3  1 \\end{pmatrix}  -\\det\\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}  \\det\\begin{pmatrix} 2  0 \\\\ 0  3 \\end{pmatrix}\n\\end{pmatrix} = \\begin{pmatrix} 5  0  0 \\\\ 0  4  -2 \\\\ 0  -2  6 \\end{pmatrix}\n$$\nSince $C$ is symmetric, $\\text{adj}(\\Gamma) = C^T = C$.\n$$\n\\Gamma^{-1} = \\frac{1}{10} \\begin{pmatrix} 5  0  0 \\\\ 0  4  -2 \\\\ 0  -2  6 \\end{pmatrix} = \\begin{pmatrix} 1/2  0  0 \\\\ 0  2/5  -1/5 \\\\ 0  -1/5  3/5 \\end{pmatrix}\n$$\nNow we compute the product $I = A^T \\Gamma^{-1} A$:\n$$\nA^T = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  0  1 \\\\ 0  1  1 \\end{pmatrix}\n$$\nFirst, let's compute $\\Gamma^{-1} A$:\n$$\n\\Gamma^{-1} A = \\begin{pmatrix} 1/2  0  0 \\\\ 0  2/5  -1/5 \\\\ 0  -1/5  3/5 \\end{pmatrix} \\begin{pmatrix} 1  0  1  0 \\\\ 0  1  0  1 \\\\ 1  1  1  1 \\end{pmatrix} = \\begin{pmatrix} 1/2  0  1/2  0 \\\\ -1/5  1/5  -1/5  1/5 \\\\ 3/5  2/5  3/5  2/5 \\end{pmatrix}\n$$\nThen, multiply by $A^T$:\n$$\nI = A^T(\\Gamma^{-1}A) = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 1/2  0  1/2  0 \\\\ -1/5  1/5  -1/5  1/5 \\\\ 3/5  2/5  3/5  2/5 \\end{pmatrix}\n$$\n$$\nI = \\begin{pmatrix}\n\\frac{1}{2}+\\frac{3}{5}  \\frac{2}{5}  \\frac{1}{2}+\\frac{3}{5}  \\frac{2}{5} \\\\\n-\\frac{1}{5}+\\frac{3}{5}  \\frac{1}{5}+\\frac{2}{5}  -\\frac{1}{5}+\\frac{3}{5}  \\frac{1}{5}+\\frac{2}{5} \\\\\n\\frac{1}{2}+\\frac{3}{5}  \\frac{2}{5}  \\frac{1}{2}+\\frac{3}{5}  \\frac{2}{5} \\\\\n-\\frac{1}{5}+\\frac{3}{5}  \\frac{1}{5}+\\frac{2}{5}  -\\frac{1}{5}+\\frac{3}{5}  \\frac{1}{5}+\\frac{2}{5}\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{11}{10}  \\frac{2}{5}  \\frac{11}{10}  \\frac{2}{5} \\\\\n\\frac{2}{5}  \\frac{3}{5}  \\frac{2}{5}  \\frac{3}{5} \\\\\n\\frac{11}{10}  \\frac{2}{5}  \\frac{11}{10}  \\frac{2}{5} \\\\\n\\frac{2}{5}  \\frac{3}{5}  \\frac{2}{5}  \\frac{3}{5}\n\\end{pmatrix}\n$$\n\n#### **Null Space Analysis**\n\nThe poorly informed directions of $u$ correspond to the null space of the Fisher Information Matrix, $\\text{Null}(I)$. The dimension of this space is its nullity. By the rank-nullity theorem, $\\text{dim}(\\text{Null}(I)) = n - \\text{rank}(I)$, where $n=4$ is the dimension of $u$. We find the rank of $I$.\n$$\nI = \\frac{1}{10} \\begin{pmatrix} 11  4  11  4 \\\\ 4  6  4  6 \\\\ 11  4  11  4 \\\\ 4  6  4  6 \\end{pmatrix}\n$$\nWe can see by inspection that row $1$ is identical to row $3$, and row $2$ is identical to row $4$. This means the rank is at most $2$. The first two rows, $[11, 4, 11, 4]$ and $[4, 6, 4, 6]$, are not scalar multiples of each other. Therefore, they are linearly independent. The rank of $I$ is $2$.\nThe dimension of the null space is:\n$$\n\\text{dim}(\\text{Null}(I)) = 4 - \\text{rank}(I) = 4 - 2 = 2\n$$\nAs a verification, since $\\Gamma^{-1}$ is positive definite, the null space of $I = A^T \\Gamma^{-1} A$ is identical to the null space of $A$. We can find the rank of $A$ by row reduction.\n$$\nA = \\begin{pmatrix} 1  0  1  0 \\\\ 0  1  0  1 \\\\ 1  1  1  1 \\end{pmatrix}\n$$\nThe third row is the sum of the first two rows ($R_3 = R_1 + R_2$). Thus, the rows are linearly dependent. The first two rows are linearly independent. The rank of $A$ is $2$.\nBy the rank-nullity theorem for $A: \\mathbb{R}^4 \\to \\mathbb{R}^3$:\n$$\n\\text{dim}(\\text{Null}(A)) = (\\text{number of columns}) - \\text{rank}(A) = 4 - 2 = 2\n$$\nThis confirms our result. The vectors in the null space satisfy $v_1+v_3=0$ and $v_2+v_4=0$. This space is spanned by the basis vectors $(1, 0, -1, 0)^T$ and $(0, 1, 0, -1)^T$. These are the directions in the parameter space $u$ about which the data $Y$ provide no information. The dimension of this space of poorly informed directions is $2$.",
            "answer": "$$\n\\boxed{2}\n$$"
        },
        {
            "introduction": "A successful inversion goes beyond parameter estimation; it involves making reliable predictions and correctly quantifying all sources of uncertainty. This practice problem  addresses the critical, real-world issue of model error and the distinction between posterior uncertainty and predictive uncertainty. You will implement a model that distinguishes between credible intervals for a model quantity and prediction intervals for a future observation, and decompose the total predictive variance into its constituent parts, providing a deeper insight into the limits of our knowledge.",
            "id": "3414519",
            "problem": "You are asked to formalize, analyze, and implement a scalar linear-Gaussian data assimilation problem with explicit model error, focusing on the distinction between credible intervals and prediction intervals, and on a variance decomposition into parametric, discrepancy, and observation components.\n\nConsider a scalar parameter $ \\theta $ representing an unknown system state. You are given:\n- A prior distribution $ \\theta \\sim \\mathcal{N}(\\mu_0,\\tau_0^2) $.\n- A forward model $ m = a\\,\\theta + b $ with known coefficients $ a $ and $ b $.\n- An additive model discrepancy random variable $ \\delta \\sim \\mathcal{N}(0,\\sigma_\\delta^2) $.\n- An additive observation noise random variable $ \\epsilon \\sim \\mathcal{N}(0,\\sigma_\\epsilon^2) $.\n- A single observed datum $ z $ related to $ \\theta $ via the observation equation\n  $$ z = a\\,\\theta + b + \\delta + \\epsilon. $$\nAssume that $ \\theta $, $ \\delta $, and $ \\epsilon $ are mutually independent.\n\nYour tasks are:\n1) Starting only from the core definitions of the normal distribution, Bayes’ rule, and the law of total expectation and the law of total variance, derive the posterior distribution of $ \\theta $ given $ z $. Then derive the conditional distribution of $ m \\mid z $ and of a future replicate observation $ \\tilde z \\mid z $, where $ \\tilde z = m + \\delta' + \\epsilon' $ with $ \\delta' \\sim \\mathcal{N}(0,\\sigma_\\delta^2) $ and $ \\epsilon' \\sim \\mathcal{N}(0,\\sigma_\\epsilon^2) $ independent of everything else.\n2) Define the central credible interval for $ m \\mid z $ at level $ 0.95 $ and the central prediction interval for $ \\tilde z \\mid z $ at level $ 0.95 $, both based on the normal quantile of order $ 1 - \\alpha/2 $ with $ \\alpha = 0.05 $.\n3) Use the law of total variance to obtain a decomposition of the total predictive variance into three nonnegative components attributable to posterior parametric uncertainty, model discrepancy, and observation noise. That is, express\n   $$ \\operatorname{Var}(\\tilde z \\mid z) = v_{\\mathrm{par}} + v_{\\mathrm{disc}} + v_{\\mathrm{obs}}, $$\n   and identify $ v_{\\mathrm{par}} $, $ v_{\\mathrm{disc}} $, and $ v_{\\mathrm{obs}} $ explicitly in terms of $ a $, $ b $, $ \\mu_0 $, $ \\tau_0 $, $ \\sigma_\\delta $, $ \\sigma_\\epsilon $, and $ z $.\n\nImplementation requirements:\n- Implement a program that, for each parameter set in the test suite below, computes:\n  - the lower and upper endpoints of the $ 0.95 $ credible interval for $ m \\mid z $,\n  - the lower and upper endpoints of the $ 0.95 $ prediction interval for $ \\tilde z \\mid z $,\n  - the triple $ (v_{\\mathrm{par}}, v_{\\mathrm{disc}}, v_{\\mathrm{obs}}) $ from the variance decomposition in item 3.\n- For numerical stability and cross-language reproducibility, compute the standard normal quantile using a numerically stable routine. Round every floating-point number in the final output to $ 6 $ decimal places.\n\nTest suite (each tuple is $ (a,b,\\mu_0,\\tau_0,\\sigma_\\delta,\\sigma_\\epsilon,z) $):\n- Case A (general): $ (2.0,\\,1.0,\\,0.0,\\,1.5,\\,0.7,\\,0.5,\\,3.2) $.\n- Case B (no model discrepancy): $ (1.2,\\,-0.5,\\,0.8,\\,0.7,\\,0.0,\\,0.3,\\,0.4) $.\n- Case C (zero sensitivity): $ (0.0,\\,2.0,\\,-1.0,\\,1.0,\\,0.4,\\,0.4,\\,2.5) $.\n- Case D (weakly informative prior): $ (1.5,\\,0.2,\\,0.0,\\,10000.0,\\,0.6,\\,0.4,\\,1.7) $.\n\nFinal output format:\n- Your program must produce a single line of output containing a list of results, one per test case, preserving the order A, B, C, D.\n- The result for each case must be a list of $ 7 $ floats in the fixed order\n  $ [\\text{CI}_{\\mathrm{low}}, \\text{CI}_{\\mathrm{high}}, \\text{PI}_{\\mathrm{low}}, \\text{PI}_{\\mathrm{high}}, v_{\\mathrm{par}}, v_{\\mathrm{disc}}, v_{\\mathrm{obs}}] $,\n  all rounded to $ 6 $ decimal places.\n- The overall output must be a single line with no spaces anywhere, for example:\n  `[[x_{1,1},x_{1,2},...,x_{1,7}],[x_{2,1},...,x_{2,7}],[x_{3,1},...,x_{3,7}],[x_{4,1},...,x_{4,7}]]`\n  but with no spaces.\n\nYour program must not read any input and must use only the parameter values provided above. It must be implemented in Python and use only the specified libraries. The final print must produce exactly one line in the specified format.",
            "solution": "The problem is well-posed, scientifically grounded, and self-contained, presenting a standard problem in Bayesian data assimilation for a linear-Gaussian model. We proceed with the derivation and solution.\n\n### 1. Derivation of Distributions\n\n**1.1. Posterior Distribution of $ \\theta \\mid z $**\n\nThe problem defines the prior distribution for the parameter $ \\theta $ and the observation model.\nThe prior is given as $ \\theta \\sim \\mathcal{N}(\\mu_0, \\tau_0^2) $. The probability density function (PDF) is:\n$$ p(\\theta) \\propto \\exp\\left(-\\frac{1}{2\\tau_0^2}(\\theta - \\mu_0)^2\\right) $$\n\nThe observation equation is $ z = a\\theta + b + \\delta + \\epsilon $. The terms $ \\delta \\sim \\mathcal{N}(0, \\sigma_\\delta^2) $ and $ \\epsilon \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2) $ are independent noise sources. Their sum, let's call it $ \\eta = \\delta + \\epsilon $, is also a normal random variable. By the properties of independent normal variables:\n$ E[\\eta] = E[\\delta] + E[\\epsilon] = 0 + 0 = 0 $\n$ \\operatorname{Var}(\\eta) = \\operatorname{Var}(\\delta) + \\operatorname{Var}(\\epsilon) = \\sigma_\\delta^2 + \\sigma_\\epsilon^2 $\nLet $ \\sigma_{\\text{tot}}^2 = \\sigma_\\delta^2 + \\sigma_\\epsilon^2 $. Thus, $ \\eta \\sim \\mathcal{N}(0, \\sigma_{\\text{tot}}^2) $.\n\nThe observation equation can be rewritten as $ z - (a\\theta + b) = \\eta $. This defines the likelihood of the observation $ z $ given the parameter $ \\theta $, which is $ z \\mid \\theta \\sim \\mathcal{N}(a\\theta + b, \\sigma_{\\text{tot}}^2) $. The likelihood function is:\n$$ p(z \\mid \\theta) \\propto \\exp\\left(-\\frac{1}{2\\sigma_{\\text{tot}}^2}(z - (a\\theta + b))^2\\right) $$\n\nAccording to Bayes' rule, the posterior distribution of $ \\theta $ given $ z $ is proportional to the product of the likelihood and the prior:\n$$ p(\\theta \\mid z) \\propto p(z \\mid \\theta) p(\\theta) $$\n$$ p(\\theta \\mid z) \\propto \\exp\\left(-\\frac{1}{2\\sigma_{\\text{tot}}^2}(z - a\\theta - b)^2\\right) \\exp\\left(-\\frac{1}{2\\tau_0^2}(\\theta - \\mu_0)^2\\right) $$\nThe posterior is proportional to the exponential of a sum of quadratic terms in $ \\theta $. This implies the posterior is also a normal distribution, say $ \\theta \\mid z \\sim \\mathcal{N}(\\mu_1, \\tau_1^2) $. We identify its parameters by analyzing the exponent, focusing on terms involving $ \\theta $:\n$$ -\\frac{1}{2} \\left[ \\frac{(a\\theta - (z-b))^2}{\\sigma_{\\text{tot}}^2} + \\frac{(\\theta - \\mu_0)^2}{\\tau_0^2} \\right] $$\n$$ = -\\frac{1}{2} \\left[ \\frac{a^2\\theta^2 - 2a\\theta(z-b)}{\\sigma_{\\text{tot}}^2} + \\frac{\\theta^2 - 2\\theta\\mu_0}{\\tau_0^2} + \\text{const} \\right] $$\n$$ = -\\frac{1}{2} \\left[ \\left(\\frac{a^2}{\\sigma_{\\text{tot}}^2} + \\frac{1}{\\tau_0^2}\\right)\\theta^2 - 2\\left(\\frac{a(z-b)}{\\sigma_{\\text{tot}}^2} + \\frac{\\mu_0}{\\tau_0^2}\\right)\\theta + \\text{const} \\right] $$\nFor a normal distribution $ \\mathcal{N}(\\mu_1, \\tau_1^2) $, the exponent is $ -\\frac{(\\theta - \\mu_1)^2}{2\\tau_1^2} = -\\frac{1}{2} \\left( \\frac{\\theta^2}{\\tau_1^2} - \\frac{2\\mu_1\\theta}{\\tau_1^2} + \\text{const} \\right) $.\nBy comparing coefficients of $ \\theta^2 $ and $ \\theta $, we find the posterior precision ($ 1/\\tau_1^2 $) and mean ($ \\mu_1 $):\n$$ \\frac{1}{\\tau_1^2} = \\frac{1}{\\tau_0^2} + \\frac{a^2}{\\sigma_{\\text{tot}}^2} \\implies \\tau_1^2 = \\left(\\frac{1}{\\tau_0^2} + \\frac{a^2}{\\sigma_{\\text{tot}}^2}\\right)^{-1} $$\n$$ \\frac{\\mu_1}{\\tau_1^2} = \\frac{\\mu_0}{\\tau_0^2} + \\frac{a(z-b)}{\\sigma_{\\text{tot}}^2} \\implies \\mu_1 = \\tau_1^2 \\left(\\frac{\\mu_0}{\\tau_0^2} + \\frac{a(z-b)}{\\sigma_{\\text{tot}}^2}\\right) $$\nThese are the parameters of the posterior distribution $ \\theta \\mid z \\sim \\mathcal{N}(\\mu_1, \\tau_1^2) $.\n\n**1.2. Conditional Distribution of $ m \\mid z $**\n\nThe forward model is $ m = a\\theta + b $. Since $ m $ is a linear function of $ \\theta $, its conditional distribution $ m \\mid z $ is also normal. We find its mean and variance using properties of expectation and variance:\n$ E[m \\mid z] = E[a\\theta + b \\mid z] = a E[\\theta \\mid z] + b = a\\mu_1 + b $.\n$ \\operatorname{Var}(m \\mid z) = \\operatorname{Var}(a\\theta + b \\mid z) = a^2 \\operatorname{Var}(\\theta \\mid z) = a^2 \\tau_1^2 $.\nThus, $ m \\mid z \\sim \\mathcal{N}(a\\mu_1 + b, a^2\\tau_1^2) $.\n\n**1.3. Conditional Distribution of $ \\tilde z \\mid z $**\n\nA future replicate observation is given by $ \\tilde z = m + \\delta' + \\epsilon' = a\\theta + b + \\delta' + \\epsilon' $, where $ \\delta' \\sim \\mathcal{N}(0, \\sigma_\\delta^2) $ and $ \\epsilon' \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2) $ are independent of all other variables. The distribution of $ \\tilde z $ given $ z $ is a predictive distribution. Given $ z $, the variables $ \\theta $ (whose distribution is $ p(\\theta|z) $), $ \\delta' $, and $ \\epsilon' $ are all independent. $ \\tilde z \\mid z $ is a linear combination of normal random variables, so it is also normal. We find its parameters using the law of total expectation and law of total variance.\n\nThe mean is:\n$ E[\\tilde z \\mid z] = E[a\\theta + b + \\delta' + \\epsilon' \\mid z] = a E[\\theta \\mid z] + b + E[\\delta' \\mid z] + E[\\epsilon' \\mid z] $.\nSince $ \\delta' $ and $ \\epsilon' $ are independent of $ z $, $ E[\\delta' \\mid z] = E[\\delta'] = 0 $ and $ E[\\epsilon' \\mid z] = E[\\epsilon'] = 0 $.\n$ E[\\tilde z \\mid z] = a\\mu_1 + b $.\n\nThe variance is, by independence of $ (\\theta \\mid z) $, $ \\delta' $, and $ \\epsilon' $:\n$ \\operatorname{Var}(\\tilde z \\mid z) = \\operatorname{Var}(a\\theta + b + \\delta' + \\epsilon' \\mid z) = \\operatorname{Var}(a\\theta \\mid z) + \\operatorname{Var}(\\delta' \\mid z) + \\operatorname{Var}(\\epsilon' \\mid z) $.\n$ \\operatorname{Var}(\\tilde z \\mid z) = a^2 \\operatorname{Var}(\\theta \\mid z) + \\operatorname{Var}(\\delta') + \\operatorname{Var}(\\epsilon') = a^2\\tau_1^2 + \\sigma_\\delta^2 + \\sigma_\\epsilon^2 $.\nThus, $ \\tilde z \\mid z \\sim \\mathcal{N}(a\\mu_1 + b, a^2\\tau_1^2 + \\sigma_\\delta^2 + \\sigma_\\epsilon^2) $.\n\n### 2. Credible and Prediction Intervals\n\nA central interval at level $ 1-\\alpha $ for a normal distribution $ \\mathcal{N}(\\mu, \\sigma^2) $ is given by $ [\\mu - q \\sigma, \\mu + q \\sigma] $, where $ q = \\Phi^{-1}(1 - \\alpha/2) $ is the $ (1 - \\alpha/2) $-quantile of the standard normal distribution $ \\mathcal{N}(0, 1) $. For this problem, $ \\alpha = 0.05 $, so we use the quantile $ q_{0.975} = \\Phi^{-1}(0.975) \\approx 1.959964 $.\n\nA **credible interval** is an interval in the domain of a posterior probability distribution. The $ 95\\% $ central credible interval for the model output $ m \\mid z $ is based on its posterior distribution $ \\mathcal{N}(a\\mu_1 + b, a^2\\tau_1^2) $.\nThe endpoints are:\n$ \\text{CI}_{\\text{low/high}} = (a\\mu_1 + b) \\mp q_{0.975} \\sqrt{a^2\\tau_1^2} = (a\\mu_1 + b) \\mp q_{0.975} |a|\\tau_1 $.\n\nA **prediction interval** is an interval in which a future observation will fall with a certain probability. The $ 95\\% $ central prediction interval for the future observation $ \\tilde z \\mid z $ is based on its predictive distribution $ \\mathcal{N}(a\\mu_1 + b, a^2\\tau_1^2 + \\sigma_\\delta^2 + \\sigma_\\epsilon^2) $.\nThe endpoints are:\n$ \\text{PI}_{\\text{low/high}} = (a\\mu_1 + b) \\mp q_{0.975} \\sqrt{a^2\\tau_1^2 + \\sigma_\\delta^2 + \\sigma_\\epsilon^2} $.\nThe prediction interval is always wider than the credible interval (for $ \\sigma_\\delta^2 + \\sigma_\\epsilon^2  0 $) as it accounts for additional sources of uncertainty from model discrepancy and observation noise a future measurement will be subject to.\n\n### 3. Variance Decomposition\n\nThe problem asks for a decomposition of the total predictive variance $ \\operatorname{Var}(\\tilde z \\mid z) $ using the law of total variance: $ \\operatorname{Var}(X) = E[\\operatorname{Var}(X \\mid Y)] + \\operatorname{Var}(E[X \\mid Y]) $. Here, $ X = \\tilde z \\mid z $ and the variable we condition on is the parameter $ \\theta $ (whose uncertainty is described by $ p(\\theta \\mid z) $).\n$ \\operatorname{Var}(\\tilde z \\mid z) = E[\\operatorname{Var}(\\tilde z \\mid \\theta, z) \\mid z] + \\operatorname{Var}(E[\\tilde z \\mid \\theta, z] \\mid z) $.\n\nFirst, we compute the inner terms, conditional on a specific value of $ \\theta $.\n$ E[\\tilde z \\mid \\theta, z] = E[a\\theta + b + \\delta' + \\epsilon' \\mid \\theta, z] = a\\theta + b $, since $ \\theta $ is fixed and $ \\delta', \\epsilon' $ are zero-mean and independent of $ z $.\n$ \\operatorname{Var}(\\tilde z \\mid \\theta, z) = \\operatorname{Var}(a\\theta + b + \\delta' + \\epsilon' \\mid \\theta, z) = \\operatorname{Var}(\\delta' + \\epsilon') = \\operatorname{Var}(\\delta') + \\operatorname{Var}(\\epsilon') = \\sigma_\\delta^2 + \\sigma_\\epsilon^2 $, since $ \\theta $ is fixed and $ \\delta', \\epsilon' $ are independent.\n\nNow, we substitute these back into the law of total variance formula:\nFirst term: $ E[\\operatorname{Var}(\\tilde z \\mid \\theta, z) \\mid z] = E[\\sigma_\\delta^2 + \\sigma_\\epsilon^2 \\mid z] = \\sigma_\\delta^2 + \\sigma_\\epsilon^2 $, since the variances are constants.\nSecond term: $ \\operatorname{Var}(E[\\tilde z \\mid \\theta, z] \\mid z) = \\operatorname{Var}(a\\theta + b \\mid z) = a^2 \\operatorname{Var}(\\theta \\mid z) = a^2\\tau_1^2 $.\n\nCombining them, we get the total predictive variance:\n$ \\operatorname{Var}(\\tilde z \\mid z) = a^2\\tau_1^2 + \\sigma_\\delta^2 + \\sigma_\\epsilon^2 $.\nThis expression naturally decomposes into three non-negative components:\n1.  **Parametric variance ($ v_{\\mathrm{par}} $):** $ v_{\\mathrm{par}} = a^2\\tau_1^2 $. This component represents the uncertainty in the model prediction due to the posterior uncertainty of the parameter $ \\theta $.\n2.  **Discrepancy variance ($ v_{\\mathrm{disc}} $):** $ v_{\\mathrm{disc}} = \\sigma_\\delta^2 $. This component is the uncertainty contribution from the inherent mismatch between the model and the true system (model error).\n3.  **Observation variance ($ v_{\\mathrm{obs}} $):** $ v_{\\mathrm{obs}} = \\sigma_\\epsilon^2 $. This component represents the uncertainty arising from the measurement noise in a future observation.\n\n```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the scalar linear-Gaussian data assimilation problem for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is (a, b, mu0, tau0, sigma_delta, sigma_epsilon, z).\n    test_cases = [\n        (2.0, 1.0, 0.0, 1.5, 0.7, 0.5, 3.2),      # Case A (general)\n        (1.2, -0.5, 0.8, 0.7, 0.0, 0.3, 0.4),      # Case B (no model discrepancy)\n        (0.0, 2.0, -1.0, 1.0, 0.4, 0.4, 2.5),      # Case C (zero sensitivity)\n        (1.5, 0.2, 0.0, 10000.0, 0.6, 0.4, 1.7),  # Case D (weakly informative prior)\n    ]\n\n    results = []\n    # Quantile for 95% central interval\n    q_975 = norm.ppf(0.975)\n\n    for case in test_cases:\n        a, b, mu0, tau0, sigma_delta, sigma_epsilon, z = case\n\n        # 1. Calculate total noise variance from model discrepancy and observation error\n        sigma_tot_sq = sigma_delta**2 + sigma_epsilon**2\n\n        # 2. Compute posterior distribution of theta: theta|z ~ N(mu1, tau1_sq).\n        # We work with precisions for numerical stability, especially for large tau0.\n        tau0_sq = tau0**2\n        \n        prec0 = 1.0 / tau0_sq\n        \n        # Handle cases where data is uninformative or noise is zero\n        if a == 0.0 or sigma_tot_sq == 0.0:\n            mu1 = mu0\n            tau1_sq = tau0_sq\n        else:\n            prec_like = (a**2) / sigma_tot_sq\n            prec1 = prec0 + prec_like\n            tau1_sq = 1.0 / prec1\n            # Posterior mean is the precision-weighted average of prior and data\n            mu1 = tau1_sq * (prec0 * mu0 + a * (z - b) / sigma_tot_sq)\n\n        # 3. Compute posterior distribution of model output m = a*theta + b\n        # m|z ~ N(mu_m, var_m)\n        mu_m = a * mu1 + b\n        var_m = a**2 * tau1_sq\n        std_m = np.sqrt(var_m)\n\n        # 4. Compute 95% Credible Interval (CI) for m|z\n        ci_half_width = q_975 * std_m\n        ci_low = mu_m - ci_half_width\n        ci_high = mu_m + ci_half_width\n\n        # 5. Compute predictive distribution of a future observation z_tilde\n        # z_tilde|z ~ N(mu_m, var_z_tilde)\n        var_z_tilde = var_m + sigma_delta**2 + sigma_epsilon**2\n        std_z_tilde = np.sqrt(var_z_tilde)\n        \n        # 6. Compute 95% Prediction Interval (PI) for z_tilde|z\n        pi_half_width = q_975 * std_z_tilde\n        pi_low = mu_m - pi_half_width\n        pi_high = mu_m + pi_half_width\n\n        # 7. Compute the variance decomposition\n        v_par = var_m\n        v_disc = sigma_delta**2\n        v_obs = sigma_epsilon**2\n        \n        # 8. Store the 7 required floating point numbers for this case\n        case_results = [ci_low, ci_high, pi_low, pi_high, v_par, v_disc, v_obs]\n        results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # The format is [[...],[...],...] with no spaces and 6 decimal places.\n    output_str = \"[\" + \",\".join([\n        \"[\" + \",\".join([f'{val:.6f}' for val in res]) + \"]\" \n        for res in results\n    ]) + \"]\"\n    print(output_str)\n\n# To generate the answer, one would run:\n# solve()\n```",
            "answer": "$$ \\boxed{\\texttt{[[1.411624,4.653241,-0.279624,6.344489,0.683784,0.490000,0.250000],[-0.046927,0.835699,-0.380424,1.169196,0.049448,0.000000,0.090000],[2.000000,2.000000,0.897214,3.102786,0.000000,0.160000,0.160000],[0.499878,2.060372,-0.923487,3.483737,0.158303,0.360000,0.160000]]}} $$"
        }
    ]
}