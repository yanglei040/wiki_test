{
    "hands_on_practices": [
        {
            "introduction": "许多现实世界的逆问题使用广义吉洪诺夫惩罚项 $\\lambda^2 \\|L x\\|^2$ 来融入先验知识，例如解的平滑性。本练习是基础性的，它将展示如何通过巧妙的变量代換，将这个看似复杂的问题转化为更易于分析的标准形式。通过完成这个练习 ，你将掌握这一核心技巧，并深化对正则化作为一种“先验白化”滤波过程的理解。",
            "id": "3419945",
            "problem": "考虑一个数据同化背景下的线性反问题，其中正算子是一个矩阵 $A \\in \\mathbb{R}^{m \\times n}$，未知状态为 $x \\in \\mathbb{R}^{n}$，数据为 $b \\in \\mathbb{R}^{m}$。假设一个广义 Tikhonov 正则化最小二乘估计，其正常数正则化水平为 $\\lambda > 0$，正则化算子 $L \\in \\mathbb{R}^{n \\times n}$ 可逆，由以下变分准则定义\n$$\nJ(x) \\;=\\; \\|A x - b\\|_{2}^{2} \\;+\\; \\lambda^{2}\\,\\|L x\\|_{2}^{2}.\n$$\n仅从准则 $J(x)$ 的定义、变量替换以及奇异值分解 (SVD) 的标准性质出发，完成以下任务：\n\n1. 证明变量变换 $x = L^{-1} z$ 将广义 Tikhonov 准则简化为关于变换后变量 $z$ 的标准形式 Tikhonov 准则，其有效算子为 $A L^{-1}$，即\n$$\nJ(L^{-1} z) \\;=\\; \\|A L^{-1} z - b\\|_{2}^{2} \\;+\\; \\lambda^{2}\\,\\|z\\|_{2}^{2}.\n$$\n\n2. 令 $B := A L^{-1}$。推导\n$$\n\\|B z - b\\|_{2}^{2} \\;+\\; \\lambda^{2}\\,\\|z\\|_{2}^{2}\n$$\n的极小值点 $z_{\\lambda}$ 的正规方程，并利用 $B$ 的奇异值分解 (SVD)，表示为 $B = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{m \\times r}$ 和 $V \\in \\mathbb{R}^{n \\times r}$ 具有标准正交列，$\\Sigma = \\operatorname{diag}(\\sigma_{1},\\dots,\\sigma_{r})$，且 $r = \\operatorname{rank}(B)$，推导出 $z_{\\lambda}$ 在 SVD 坐标下的显式表达式。\n\n3. 通过与 $B$ 值域中的非正则化最小二乘解进行比较来表示 $z_{\\lambda}$，并确定施加于每个谱分量上的乘性衰减。将 Tikhonov 正则化的相关滤波因子定义为在由 $B$ 的右奇异向量张成的子空间中，乘以最小二乘解谱分量的标量。推导它们关于奇异值 $\\sigma_{i}$ 和正则化水平 $\\lambda$ 的解析形式。\n\n4. 从先验白化的可观测性角度解释这些滤波因子：说明通过 $L^{-1}$ 的变换如何白化先验，以及 $A L^{-1}$ 的奇异值与滤波因子一起，如何量化状态空间中的方向相对于先验强度从数据中获取信息的程度。\n\n以滤波因子作为 $\\sigma_{i}$ 和 $\\lambda$ 函数的单个闭式解析表达式的形式给出最终答案。无需进行数值计算。",
            "solution": "该问题是有效的，因为它在科学上基于反问题理论和线性代数的原理，是适定的、客观的且自洽的。我们将按照四个指定部分进行求解。\n\n广义 Tikhonov 准则由下式给出\n$$\nJ(x) = \\|A x - b\\|_{2}^{2} + \\lambda^{2}\\|L x\\|_{2}^{2}\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$x \\in \\mathbb{R}^{n}$，$b \\in \\mathbb{R}^{m}$，$L \\in \\mathbb{R}^{n \\times n}$ 是一个可逆矩阵，$\\lambda > 0$ 是正则化水平。\n\n**1. 变量变换**\n\n我们进行变量替换 $x = L^{-1} z$，其中 $z \\in \\mathbb{R}^{n}$。由于 $L$ 可逆，此变换是 $x$ 空间与 $z$ 空间之间的一个双射。我们将其代入准则 $J(x)$：\n$$\nJ(L^{-1} z) = \\|A (L^{-1} z) - b\\|_{2}^{2} + \\lambda^{2}\\|L (L^{-1} z)\\|_{2}^{2}\n$$\n利用矩阵乘法性质，$A(L^{-1}z) = (AL^{-1})z$。在第二项中，$L(L^{-1}z) = (LL^{-1})z = I_n z = z$，其中 $I_n$ 是 $n \\times n$ 单位矩阵。\n该准则变为：\n$$\nJ(L^{-1} z) = \\|A L^{-1} z - b\\|_{2}^{2} + \\lambda^{2}\\|z\\|_{2}^{2}\n$$\n这就是关于变换后变量 $z$ 且有效算子为 $B = A L^{-1}$ 的标准形式 Tikhonov 准则，证明完毕。\n\n**2. 正规方程与基于 SVD 的解**\n\n令 $\\mathcal{J}(z) = \\|B z - b\\|_{2}^{2} + \\lambda^{2}\\|z\\|_{2}^{2}$，其中 $B = A L^{-1}$。为了找到极小值点 $z_{\\lambda}$，我们必须找到 $\\mathcal{J}(z)$ 相对于 $z$ 的梯度为零的点。我们首先展开平方范数：\n$$\n\\mathcal{J}(z) = (B z - b)^{\\top}(B z - b) + \\lambda^{2} z^{\\top}z = z^{\\top}B^{\\top}Bz - 2 b^{\\top}B z + b^{\\top}b + \\lambda^{2} z^{\\top}z\n$$\n相对于 $z$ 的梯度为：\n$$\n\\nabla_{z} \\mathcal{J}(z) = 2 B^{\\top}B z - 2 B^{\\top}b + 2 \\lambda^{2} z\n$$\n将梯度设为零，$\\nabla_{z} \\mathcal{J}(z) = 0$，得到正规方程：\n$$\n(B^{\\top}B + \\lambda^{2} I) z_{\\lambda} = B^{\\top}b\n$$\n对于 $\\lambda > 0$，矩阵 $(B^{\\top}B + \\lambda^{2} I)$ 是正定的，因此是可逆的，这保证了 $z_{\\lambda}$ 的唯一解。\n\n现在，我们使用 $B$ 的奇异值分解 (SVD) $B = U \\Sigma V^{\\top}$，其中 $U \\in \\mathbb{R}^{m \\times r}$ 和 $V \\in \\mathbb{R}^{n \\times r}$ 具有标准正交列 ($U^{\\top}U = I_r$, $V^{\\top}V = I_r$)，$\\Sigma = \\operatorname{diag}(\\sigma_{1},\\dots,\\sigma_{r})$ 包含正奇异值 $\\sigma_i > 0$，且 $r = \\operatorname{rank}(B)$。\n我们将 SVD 代入正规方程。首先，我们计算 $B^{\\top}B$ 和 $B^{\\top}b$ 这两项：\n$$\nB^{\\top}B = (U \\Sigma V^{\\top})^{\\top}(U \\Sigma V^{\\top}) = V \\Sigma^{\\top} U^{\\top} U \\Sigma V^{\\top} = V \\Sigma^2 V^{\\top}\n$$\n$$\nB^{\\top}b = (U \\Sigma V^{\\top})^{\\top}b = V \\Sigma U^{\\top}b\n$$\n正规方程变为：\n$$\n(V \\Sigma^2 V^{\\top} + \\lambda^{2} I) z_{\\lambda} = V \\Sigma U^{\\top}b\n$$\n解 $z_{\\lambda}$ 位于 $B^{\\top}$ 的值域内，即 $V$ 的列空间。任何与该空间正交的分量都会被 $B^{\\top}B$ 映射为零，并被正则化项置零。我们可以将 $z_{\\lambda}$ 写为 $z_{\\lambda} = V \\alpha$，其中 $\\alpha \\in \\mathbb{R}^{r}$ 是某个系数向量。将此代入方程：\n$$\n(V \\Sigma^2 V^{\\top} + \\lambda^{2} I) V \\alpha = V \\Sigma U^{\\top}b\n$$\n利用 $V^{\\top}V = I_r$，我们得到 $V \\Sigma^2 V^{\\top} V \\alpha = V \\Sigma^2 \\alpha$。项 $\\lambda^{2} I V \\alpha = \\lambda^2 V \\alpha$。因此，\n$$\nV \\Sigma^2 \\alpha + \\lambda^2 V \\alpha = V \\Sigma U^{\\top}b\n$$\n$$\nV (\\Sigma^2 + \\lambda^2 I_r) \\alpha = V \\Sigma U^{\\top}b\n$$\n左乘 $V^{\\top}$ 并利用 $V^{\\top}V = I_r$：\n$$\n(\\Sigma^2 + \\lambda^2 I_r) \\alpha = \\Sigma U^{\\top}b\n$$\n由于 $\\Sigma^2 + \\lambda^2 I_r$ 是一个对角元素为正值 $\\sigma_i^2 + \\lambda^2$ 的对角矩阵，它是可逆的。我们可以解出 $\\alpha$：\n$$\n\\alpha = (\\Sigma^2 + \\lambda^2 I_r)^{-1} \\Sigma U^{\\top}b\n$$\n那么解 $z_{\\lambda}$ 为 $z_{\\lambda} = V \\alpha = V (\\Sigma^2 + \\lambda^2 I_r)^{-1} \\Sigma U^{\\top}b$。\n在 SVD 坐标下（即，作为对奇异分量的求和），这表示为：\n$$\nz_{\\lambda} = \\sum_{i=1}^{r} V_i \\alpha_i = \\sum_{i=1}^{r} V_i \\frac{\\sigma_i}{\\sigma_i^2 + \\lambda^2} (U_i^{\\top}b)\n$$\n其中 $U_i$ 和 $V_i$ 分别是 $U$ 和 $V$ 的第 $i$ 列，$\\alpha_i$ 是 $\\alpha$ 的第 $i$ 个分量。\n\n**3. 滤波因子**\n\n问题 $\\min_{z} \\|Bz - b\\|_2^2$ 的非正则化线性最小二乘解由 $z_{LS} = B^{\\dagger} b$ 给出，其中 $B^{\\dagger}$ 是 $B$ 的 Moore-Penrose 伪逆。利用 SVD，$B^{\\dagger} = V \\Sigma^{-1} U^{\\top}$。因此解为：\n$$\nz_{LS} = V \\Sigma^{-1} U^{\\top} b = \\sum_{i=1}^{r} V_i \\frac{1}{\\sigma_i} (U_i^{\\top}b)\n$$\n该解受限于 $B^{\\top}$ 的值域（即 $V$ 的列向量的张成空间）。项 $\\frac{U_i^{\\top}b}{\\sigma_i}$ 是第 $i$ 个基向量 $V_i$ 的系数，问题中称之为最小二乘解的“第 $i$ 个谱分量”。\n\n现在我们将正则化解 $z_{\\lambda}$ 与最小二乘解 $z_{LS}$ 进行比较。我们可以重写 $z_{\\lambda}$ 的表达式：\n$$\nz_{\\lambda} = \\sum_{i=1}^{r} V_i \\left( \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2} \\right) \\left( \\frac{1}{\\sigma_i} (U_i^{\\top}b) \\right)\n$$\n这表明，正则化解的第 $i$ 个谱分量是通过将最小二乘解的第 $i$ 个谱分量 $\\frac{U_i^{\\top}b}{\\sigma_i}$ 乘以一个乘性标量因子得到的。这些标量就是 Tikhonov 滤波因子，记为 $f_i$：\n$$\nf_i(\\sigma_i, \\lambda) = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2}\n$$\n\n**4. 滤波因子的解释**\n\n广义 Tikhonov 准则可以从贝叶斯视角进行解释。$\\|A x - b\\|_2^2$ 项对应于假设数据存在高斯噪声时的负对数似然，而 $\\lambda^2 \\|L x\\|_2^2$ 项对应于状态 $x$ 的负对数先验，这意味着一个均值为 0、协方差矩阵与 $(L^{\\top}L)^{-1}$ 成比例的高斯先验。\n\n变换 $z = L x$ (及其逆变换 $x = L^{-1}z$) 能有效地“白化”先验。在变换后的坐标 $z$ 中，先验项变为 $\\lambda^2 \\|z\\|_2^2$，这对应于一个具有球形协方差矩阵 $(\\lambda^{-2}I)$ 的高斯先验。$z$ 的分量是先验不相关的，并且具有相同的方差。\n\n变换后的正算子 $B = AL^{-1}$ 的 SVD 提供了系统在这些先验白化坐标下的“可观测性”。奇异值 $\\sigma_i$ 量化了沿右奇异向量 $V_i$ 方向（在 $z$ 空间中）的扰动被正算子 $B$ 放大的程度。大的 $\\sigma_i$ 意味着方向 $V_i$ 在数据中是高度可观测的，而小的 $\\sigma_i$ 则意味着其可观测性很差。\n\n滤波因子 $f_i = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\lambda^2} = \\frac{1}{1 + (\\lambda/\\sigma_i)^2}$ 阐明了数据信息和先验信息之间的权衡。\n- 如果模式 $i$ 是高度可观测的 ($\\sigma_i \\gg \\lambda$)，那么 $f_i \\approx 1$。该分量的正则化解几乎与最小二乘解相同，意味着估计主要由数据驱动。\n- 如果模式 $i$ 的可观测性很差 ($\\sigma_i \\ll \\lambda$)，那么 $f_i \\approx \\sigma_i^2 / \\lambda^2 \\approx 0$。该分量的正则化解被强烈衰减至其先验均值 0。估计由先验决定，有效地滤除了对于该模式不可靠的数据的影响。\n\n因此，先验白化算子 $AL^{-1}$ 的奇异值 $\\sigma_i$ 量化了每个模式从数据中获取信息的程度，而滤波因子则实现了一种加权方案，该方案对可观测性好的模式信任数据，对可观测性差的模式依赖先验，其中正则化参数 $\\lambda$ 为这种权衡设定了阈值。\n\n要求的最终答案是滤波因子的解析表达式。",
            "answer": "$$\n\\boxed{\\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda^{2}}}\n$$"
        },
        {
            "introduction": "在广义框架的基础上，本练习旨在深入探讨由前向算子 $A$ 和惩罚算子 $L$ 的零空间（nullspace）引发的关键细节。通过分析一个简化的对角系统 ，你将揭示这些零空间如何直接控制滤波因子的行为，从而决定解的某个分量是由数据完全决定、被抑制趋向先验，还是完全不受约束。这项探索对于建立广义正则化行为的稳健直觉至关重要。",
            "id": "3419937",
            "problem": "考虑一个离散线性逆问题，其数据模型为 $b \\in \\mathbb{R}^{3}$，正演算子为 $A \\in \\mathbb{R}^{3 \\times 3}$。我们希望通过最小化带有一般罚算子 $L \\in \\mathbb{R}^{3 \\times 3}$ 的 Tikhonov 泛函来寻求一个估计 $x_{\\lambda} \\in \\mathbb{R}^{3}$：\n$$\n\\|A x - b\\|^{2} + \\lambda^{2} \\|L x\\|^{2},\n$$\n其中 $\\lambda > 0$。设 $A$ 和 $L$ 是实对称且可同时对角化的，具体如下：\n$$\nA = \\begin{pmatrix}\n3  0  0 \\\\\n0  \\tfrac{1}{2}  0 \\\\\n0  0  0\n\\end{pmatrix}, \n\\quad\nL = \\begin{pmatrix}\n0  0  0 \\\\\n0  2  0 \\\\\n0  0  1\n\\end{pmatrix},\n\\quad\n\\lambda = 1.\n$$\n假设数据 $b$ 是任意且固定的。使用适用于一般罚项的奇异值分解 (SVD) 和广义奇异值分解 (GSVD) 的推理方法，完全在 $A$ 和 $L$ 的共享特征基中进行运算，以实现以下目标：\n\n1. 从基本原理出发，推导拟合值 $A x_{\\lambda}$ 作为应用于 $b$ 的线性映射的谱形式。特别是，确定由 Tikhonov 泛函的最小化子引起的、$b$ 的分量在每个不变方向上的逐系数衰减。\n\n2. 仔细说明零空间效应：零空间 $\\mathcal{N}(L)$ 和零空间 $\\mathcal{N}(A)$ 及其对每个方向上衰减的影响。\n\n最后，对于给定的 $A$、 $L$ 和 $\\lambda$，计算与 $A$ 和 $L$ 的不变方向相关的三个衰减因子的和。请以精确形式（不要四舍五入）给出你的最终答案。",
            "solution": "我们从带有一般罚算子 $L$ 的 Tikhonov 正则化的定义性变分问题开始：\n$$\nx_{\\lambda} = \\arg\\min_{x \\in \\mathbb{R}^{3}} \\left( \\|A x - b\\|^{2} + \\lambda^{2} \\|L x\\|^{2} \\right).\n$$\n当 $A^{\\mathsf{T}} A + \\lambda^{2} L^{\\mathsf{T}} L$ 是正定矩阵时，这是一个严格凸的二次泛函；更一般地，其最小化子由以下正规方程确定：\n$$\n\\left(A^{\\mathsf{T}} A + \\lambda^{2} L^{\\mathsf{T}} L\\right) x_{\\lambda} = A^{\\mathsf{T}} b.\n$$\n拟合值由下式给出\n$$\nA x_{\\lambda} = A \\left(A^{\\mathsf{T}} A + \\lambda^{2} L^{\\mathsf{T}} L\\right)^{-1} A^{\\mathsf{T}} b,\n$$\n所以从 $b$ 到 $A x_{\\lambda}$ 的映射是线性算子\n$$\nF_{\\lambda} := A \\left(A^{\\mathsf{T}} A + \\lambda^{2} L^{\\mathsf{T}} L\\right)^{-1} A^{\\mathsf{T}}.\n$$\n在 $A$ 和 $L$ 的共享特征基中，每个基向量都是这两个算子的一个不变方向。由于 $A$ 和 $L$ 已被给定为对角矩阵，\n$$\nA = \\mathrm{diag}(a_{1}, a_{2}, a_{3}) = \\mathrm{diag}(3, \\tfrac{1}{2}, 0), \n\\quad\nL = \\mathrm{diag}(\\ell_{1}, \\ell_{2}, \\ell_{3}) = \\mathrm{diag}(0, 2, 1),\n$$\n我们有 $A^{\\mathsf{T}} A = \\mathrm{diag}(a_{1}^{2}, a_{2}^{2}, a_{3}^{2})$ 和 $L^{\\mathsf{T}} L = \\mathrm{diag}(\\ell_{1}^{2}, \\ell_{2}^{2}, \\ell_{3}^{2})$。因此，\n$$\nA^{\\mathsf{T}} A + \\lambda^{2} L^{\\mathsf{T}} L = \\mathrm{diag}\\!\\left(a_{1}^{2} + \\lambda^{2} \\ell_{1}^{2}, \\; a_{2}^{2} + \\lambda^{2} \\ell_{2}^{2}, \\; a_{3}^{2} + \\lambda^{2} \\ell_{3}^{2}\\right).\n$$\n因为在此基下算子是对角的，所以 $F_{\\lambda}$ 对每个分量 $b_{i}$ 的作用是解耦的。具体来说，\n$$\nF_{\\lambda} = \\mathrm{diag}\\!\\left(\\frac{a_{1}^{2}}{a_{1}^{2} + \\lambda^{2} \\ell_{1}^{2}}, \\; \\frac{a_{2}^{2}}{a_{2}^{2} + \\lambda^{2} \\ell_{2}^{2}}, \\; \\frac{a_{3}^{2}}{a_{3}^{2} + \\lambda^{2} \\ell_{3}^{2}}\\right).\n$$\n因此，逐分量的拟合值为\n$$\n\\left(A x_{\\lambda}\\right)_{i} = \\frac{a_{i}^{2}}{a_{i}^{2} + \\lambda^{2} \\ell_{i}^{2}} \\, b_{i},\n$$\n而逐系数的衰减是其对角线元素\n$$\n\\phi_{i}(\\lambda) := \\frac{a_{i}^{2}}{a_{i}^{2} + \\lambda^{2} \\ell_{i}^{2}}, \\quad i = 1,2,3.\n$$\n这些 $\\phi_{i}(\\lambda)$ 是由带一般罚项的 Tikhonov 正则化所引起的滤波因子，在共享特征基中表示。\n\n我们现在来研究零空间。零空间 $\\mathcal{N}(L)$ 对应于 $\\ell_{i}=0$。在这里，$\\ell_{1} = 0$，所以第一个不变方向位于 $\\mathcal{N}(L)$ 内；它是不受惩罚的。在这种情况下，\n$$\n\\phi_{1}(\\lambda) = \\frac{a_{1}^{2}}{a_{1}^{2} + \\lambda^{2} \\cdot 0} = 1,\n$$\n这表明沿此方向的拟合值没有衰减。零空间 $\\mathcal{N}(A)$ 对应于 $a_{i}=0$。在这里，$a_{3} = 0$，所以第三个不变方向位于 $\\mathcal{N}(A)$ 内；它对拟合值没有贡献。在这种情况下，\n$$\n\\phi_{3}(\\lambda) = \\frac{0}{0 + \\lambda^{2} \\ell_{3}^{2}} = 0,\n$$\n这表明沿此方向是完全抑制。第二个方向既不在 $\\mathcal{N}(L)$ 中也不在 $\\mathcal{N}(A)$ 中，我们有一个由正则化引起的真正的衰减：\n$$\n\\phi_{2}(\\lambda) = \\frac{a_{2}^{2}}{a_{2}^{2} + \\lambda^{2} \\ell_{2}^{2}} = \\frac{\\left(\\tfrac{1}{2}\\right)^{2}}{\\left(\\tfrac{1}{2}\\right)^{2} + 1 \\cdot 2^{2}} = \\frac{\\tfrac{1}{4}}{\\tfrac{1}{4} + 4} = \\frac{\\tfrac{1}{4}}{\\tfrac{17}{4}} = \\frac{1}{17}.\n$$\n\n最后，三个衰减因子的和是\n$$\n\\phi_{1}(\\lambda) + \\phi_{2}(\\lambda) + \\phi_{3}(\\lambda) = 1 + \\frac{1}{17} + 0 = \\frac{18}{17}.\n$$\n这个和在这种情况下量化了数据分量在所有不变方向上对拟合值 $A x_{\\lambda}$ 的总体影响，明确地反映了对于一般罚项，$A$ 和 $L$ 两者的零空间效应。",
            "answer": "$$\\boxed{\\frac{18}{17}}$$"
        },
        {
            "introduction": "从理论走向应用，本练习将处理正则化中最关键的实际问题：如何选择最优的正则化参数 $\\alpha$。本练习  将引导你推导并亲手实现广义交叉验证（GCV）方法，这是一种强大且由数据驱动的技术。你将学习如何利用 SVD 将抽象的 GCV 公式转化为高效的算法，从而巩固你从头到尾解决实际逆问题的能力。",
            "id": "3419911",
            "problem": "考虑一个线性离散反问题，即从包含噪声的观测值 $b \\in \\mathbb{R}^m$ 中估计未知的状态向量 $x \\in \\mathbb{R}^n$。这两者通过一个已知的正向算子 $A \\in \\mathbb{R}^{m \\times n}$ 关联，模型为 $b = A x + \\varepsilon$，其中 $\\varepsilon$ 代表加性测量噪声。我们关注零阶吉洪诺夫正则化（zero-order Tikhonov regularization），其中 $x$ 的估计值通过最小化吉洪诺夫泛函 $\\|A x - b\\|_2^2 + \\alpha^2 \\|x\\|_2^2$ 来获得，正则化参数 $\\alpha > 0$。该估计必须使用奇异值分解（Singular Value Decomposition, SVD）来表示，并且必须根据滤波因子给出其解释。拟合质量应使用广义交叉验证（Generalized Cross-Validation, GCV）进行评估（广义交叉验证通过线性预测算子对影响进行惩罚）。\n\n从以下基本前提开始：\n- 离散正向模型 $b = A x + \\varepsilon$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$x \\in \\mathbb{R}^n$，$b \\in \\mathbb{R}^m$。\n- 零阶吉洪诺夫正则化解 $\\hat{x}_\\alpha$ 是 $\\min_{x} \\|A x - b\\|_2^2 + \\alpha^2 \\|x\\|_2^2$ 对 $\\alpha > 0$ 的解。\n- $A$ 的奇异值分解（SVD）为 $A = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{m \\times r}$，$\\Sigma \\in \\mathbb{R}^{r \\times r}$ 是对角矩阵，其对角线元素为正奇异值 $\\sigma_i$，$V \\in \\mathbb{R}^{n \\times r}$，$r = \\mathrm{rank}(A) \\le \\min(m,n)$。\n- 影响矩阵（也称为帽子矩阵）是 $H_\\alpha \\in \\mathbb{R}^{m \\times m}$，使得预测数据为 $\\hat{b}_\\alpha = A \\hat{x}_\\alpha = H_\\alpha b$。\n\n您的任务是：\n1. 从第一性原理出发，推导吉洪诺夫解 $\\hat{x}_\\alpha$ 基于SVD的形式，并根据奇异值 $\\sigma_i$ 识别出相应的滤波因子。\n2. 推导影响矩阵 $H_\\alpha$，并用奇异值 $\\sigma_i$ 和正则化参数 $\\alpha$ 表示其迹。\n3. 推导零阶吉洪诺夫估计器的广义交叉验证（GCV）泛函 $G(\\alpha)$，使其仅用 $A$ 的SVD计算出的量和数据向量 $b$ 来表示，除了SVD所必需的矩阵外，不形成任何大型稠密矩阵。GCV泛函必须推导为一个依赖于残差范数 $\\|A \\hat{x}_\\alpha - b\\|_2$ 和 $\\mathrm{trace}(H_\\alpha)$ 的函数。\n4. 实现一个数值算法，该算法：\n   - 计算 $A$ 的SVD。\n   - 在一个对数间隔的 $\\alpha \\in [10^{-8}, 10^{2}]$ 网格上使用 $200$ 个点评估 $G(\\alpha)$，然后通过在最小化 $\\alpha$ 周围的一个乘法因子为 $10$ 的范围内（并遵守原始边界）的新的 $200$ 点对数网格上搜索来进行优化。\n   - 对每个测试用例，返回最小化正则化参数 $\\alpha^\\star$、平方残差范数 $\\|A \\hat{x}_{\\alpha^\\star} - b\\|_2^2$ 和 $\\mathrm{trace}(H_{\\alpha^\\star})$；所有值必须以双精度计算。\n5. 所有三角函数均使用弧度。\n\n测试套件：\n- 案例 1（理想情况，轻度病态，超定）：设 $m = 50, n = 30$。定义矩阵 $A \\in \\mathbb{R}^{50 \\times 30}$ 为\n  $$A_{ij} = \\frac{1}{1 + |i - j|} + 10^{-3} \\cdot \\sin\\left(\\frac{i + j}{10}\\right), \\quad 1 \\le i \\le 50, \\; 1 \\le j \\le 30,$$\n  其中正弦函数的参数以弧度为单位。定义真实状态 $x_{\\mathrm{true}} \\in \\mathbb{R}^{30}$ 为\n  $$x_{\\mathrm{true},j} = \\sin\\left(\\frac{j}{4}\\right), \\quad 1 \\le j \\le 30,$$\n  其中正弦函数的参数以弧度为单位。定义数据 $b \\in \\mathbb{R}^{50}$ 为\n  $$b_i = \\sum_{j=1}^{30} A_{ij} x_{\\mathrm{true},j} + 10^{-3} \\cdot (-1)^i, \\quad 1 \\le i \\le 50.$$\n- 案例 2（不适定模糊问题，方形系统，噪声更强）：设 $m = n = 40$。定义矩阵 $A \\in \\mathbb{R}^{40 \\times 40}$ 为\n  $$A_{ij} = \\exp\\left( - \\frac{(i - j)^2}{2 \\cdot 25} \\right), \\quad 1 \\le i,j \\le 40.$$\n  定义真实状态 $x_{\\mathrm{true}} \\in \\mathbb{R}^{40}$ 为\n  $$x_{\\mathrm{true},j} = \\cos\\left(\\frac{j}{8}\\right), \\quad 1 \\le j \\le 40,$$\n  其中余弦函数的参数以弧度为单位。定义数据 $b \\in \\mathbb{R}^{40}$ 为\n  $$b_i = \\sum_{j=1}^{40} A_{ij} x_{\\mathrm{true},j} + 10^{-2} \\cdot \\sin\\left(\\frac{i}{3}\\right), \\quad 1 \\le i \\le 40,$$\n  其中正弦函数的参数以弧度为单位。\n- 案例 3（边界情况，$A$ 中无信息）：设 $m = 20, n = 10$。定义 $A$ 为\n  $$A_{ij} = 0, \\quad 1 \\le i \\le 20, \\; 1 \\le j \\le 10,$$\n  并定义 $b \\in \\mathbb{R}^{20}$ 为\n  $$b_i = (-1)^i, \\quad 1 \\le i \\le 20.$$\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含所有三个测试用例的结果，形式为方括号括起来的逗号分隔列表。对于每个测试用例，输出三元组 $[\\alpha^\\star, \\|A \\hat{x}_{\\alpha^\\star} - b\\|_2^2, \\mathrm{trace}(H_{\\alpha^\\star})]$。每个浮点数必须精确到小数点后 $6$ 位。\n- 例如，输出结构必须为\n  $$\\left[ [\\alpha^\\star_1, r^2_1, t_1], [\\alpha^\\star_2, r^2_2, t_2], [\\alpha^\\star_3, r^2_3, t_3] \\right],$$\n  并作为单行打印：\n  $$[[\\alpha^\\star_1,r^2_1,t_1],[\\alpha^\\star_2,r^2_2,t_2],[\\alpha^\\star_3,r^2_3,t_3]].$$\n所有三角函数的参数均为弧度，不涉及物理单位。每个返回的值都是一个浮点数。",
            "solution": "该问题经过严格验证，被认为是有效的。这是一个在反问题领域内定义良好、具有科学依据的问题，提供了所有必要信息，且没有明显的矛盾。\n\n我们的任务是找到线性反问题 $b = Ax + \\varepsilon$ 的零阶吉洪诺夫正则化解 $\\hat{x}_\\alpha$。解 $\\hat{x}_\\alpha$ 是吉洪诺夫泛函的最小化子：\n$$\nJ(x) = \\|A x - b\\|_2^2 + \\alpha^2 \\|x\\|_2^2\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$x \\in \\mathbb{R}^n$，$b \\in \\mathbb{R}^m$，且 $\\alpha > 0$ 是正则化参数。解必须使用 $A$ 的奇异值分解（SVD）来表示，最优的 $\\alpha$ 将通过最小化广义交叉验证（GCV）泛函来找到。\n\n**1. 基于SVD的吉洪诺夫解和滤波因子**\n\n$J(x)$ 的最小化子 $\\hat{x}_\\alpha$ 满足梯度 $\\nabla_x J(x)$ 为零的条件。计算梯度可得：\n$$\n\\nabla_x J(x) = \\nabla_x ( (Ax-b)^\\top(Ax-b) + \\alpha^2 x^\\top x ) = 2 A^\\top (Ax - b) + 2 \\alpha^2 x\n$$\n令梯度为零，得到吉洪诺夫问题的正规方程组：\n$$\n(A^\\top A + \\alpha^2 I_n) \\hat{x}_\\alpha = A^\\top b\n$$\n其中 $I_n$ 是 $n \\times n$ 的单位矩阵。\n\n设 $A$ 的秩为 $r \\le \\min(m, n)$。$A$ 的SVD由 $A = U \\Sigma V^\\top$ 给出，其中 $U \\in \\mathbb{R}^{m \\times r}$ 具有标准正交列 ($u_1, \\ldots, u_r$)，$V \\in \\mathbb{R}^{n \\times r}$ 具有标准正交列 ($v_1, \\ldots, v_r$)，$\\Sigma \\in \\mathbb{R}^{r \\times r}$ 是一个对角矩阵，其对角线元素为正奇异值 $\\sigma_1 \\ge \\sigma_2 \\ge \\ldots \\ge \\sigma_r > 0$。\n\n使用SVD，我们可以表示正规方程组中的各项：\n$$\nA^\\top A = (U \\Sigma V^\\top)^\\top (U \\Sigma V^\\top) = V \\Sigma^\\top U^\\top U \\Sigma V^\\top = V \\Sigma^2 V^\\top\n$$\n$$\nA^\\top b = (U \\Sigma V^\\top)^\\top b = V \\Sigma U^\\top b\n$$\n将这些代入正规方程组可得：\n$$\n(V \\Sigma^2 V^\\top + \\alpha^2 I_n) \\hat{x}_\\alpha = V \\Sigma U^\\top b\n$$\n解 $\\hat{x}_\\alpha$ 必定位于 $V$ 的列张成的空间中，该空间是 $A$ 的零空间的正交补。$x$ 在 $A$ 的零空间中的任何分量都会增加惩罚项 $\\alpha^2 \\|x\\|_2^2$，而不会减小残差项 $\\|A x - b\\|_2^2$。因此，我们可以将解写成基向量 $v_i$ 的线性组合：$\\hat{x}_\\alpha = \\sum_{i=1}^r c_i v_i = Vc$，其中 $c \\in \\mathbb{R}^r$ 是某个系数向量。将此代入方程并使用 $V^\\top V = I_r$：\n$$\n(V \\Sigma^2 V^\\top + \\alpha^2 I_n) V c = V \\Sigma U^\\top b\n$$\n左乘 $V^\\top$：\n$$\nV^\\top(V \\Sigma^2 V^\\top + \\alpha^2 I_n) V c = V^\\top(V \\Sigma U^\\top b)\n$$\n$$\n(\\Sigma^2 + \\alpha^2 I_r) c = \\Sigma U^\\top b\n$$\n由于 $\\sigma_i > 0$ 且 $\\alpha > 0$，矩阵 $(\\Sigma^2 + \\alpha^2 I_r)$ 是对角且可逆的。我们可以解出 $c$：\n$$\nc = (\\Sigma^2 + \\alpha^2 I_r)^{-1} \\Sigma U^\\top b\n$$\n吉洪诺夫解则为 $\\hat{x}_\\alpha = Vc$：\n$$\n\\hat{x}_\\alpha = V (\\Sigma^2 + \\alpha^2 I_r)^{-1} \\Sigma U^\\top b\n$$\n以求和形式表示，$c$ 的第 $i$ 个分量是 $c_i = \\frac{\\sigma_i}{\\sigma_i^2 + \\alpha^2} (u_i^\\top b)$。解变为：\n$$\n\\hat{x}_\\alpha = \\sum_{i=1}^r c_i v_i = \\sum_{i=1}^r \\frac{\\sigma_i}{\\sigma_i^2 + \\alpha^2} (u_i^\\top b) v_i\n$$\n标准的（未正则化的）伪逆解是 $x^\\dagger = A^\\dagger b = \\sum_{i=1}^r \\frac{1}{\\sigma_i} (u_i^\\top b) v_i$。我们可以用 $x^\\dagger$ 的分量来表示 $\\hat{x}_\\alpha$：\n$$\n\\hat{x}_\\alpha = \\sum_{i=1}^r \\left(\\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2}\\right) \\frac{u_i^\\top b}{\\sigma_i} v_i\n$$\n项 $f_i(\\alpha) = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2}$ 是**滤波因子**。它们过滤解的谱分量，衰减那些与小奇异值相关的分量，从而稳定解以抵抗噪声。\n\n**2. 影响矩阵及其迹**\n\n预测数据为 $\\hat{b}_\\alpha = A \\hat{x}_\\alpha$。影响矩阵（或帽子矩阵）$H_\\alpha$ 通过 $\\hat{b}_\\alpha = H_\\alpha b$ 将原始数据 $b$ 与预测数据 $\\hat{b}_\\alpha$ 关联起来。代入 $A$ 和 $\\hat{x}_\\alpha$ 的SVD表达式：\n$$\n\\hat{b}_\\alpha = (U \\Sigma V^\\top) \\left( V (\\Sigma^2 + \\alpha^2 I_r)^{-1} \\Sigma U^\\top b \\right)\n$$\n使用 $V^\\top V = I_r$，这简化为：\n$$\n\\hat{b}_\\alpha = U \\Sigma (\\Sigma^2 + \\alpha^2 I_r)^{-1} \\Sigma U^\\top b = U \\Sigma^2 (\\Sigma^2 + \\alpha^2 I_r)^{-1} U^\\top b\n$$\n由此，我们识别出影响矩阵：\n$$\nH_\\alpha = U \\Sigma^2 (\\Sigma^2 + \\alpha^2 I_r)^{-1} U^\\top = \\sum_{i=1}^r \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2} u_i u_i^\\top\n$$\n影响矩阵的迹利用迹算子的线性和性质 $\\mathrm{trace}(uv^\\top) = v^\\top u$ 来求得：\n$$\n\\mathrm{trace}(H_\\alpha) = \\mathrm{trace} \\left(\\sum_{i=1}^r \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2} u_i u_i^\\top \\right) = \\sum_{i=1}^r \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2} \\mathrm{trace}(u_i u_i^\\top)\n$$\n因为 $u_i^\\top u_i = 1$，我们有 $\\mathrm{trace}(u_i u_i^\\top) = 1$。因此：\n$$\n\\mathrm{trace}(H_\\alpha) = \\sum_{i=1}^r \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2} = \\sum_{i=1}^r f_i(\\alpha)\n$$\n这个量通常被解释为正则化模型的有效自由度。\n\n**3. 广义交叉验证（GCV）泛函**\n\nGCV泛函提供了一种通过最小化以下表达式来估计最优 $\\alpha$ 的方法：\n$$\nG(\\alpha) = \\frac{m \\|A \\hat{x}_\\alpha - b\\|_2^2}{(m - \\mathrm{trace}(H_\\alpha))^2}\n$$\n我们需要用SVD分量来表示分子，即残差 $r_\\alpha = A\\hat{x}_\\alpha - b$ 的平方范数。残差可以写成 $r_\\alpha = \\hat{b}_\\alpha - b = (H_\\alpha - I_m)b$。\n让我们将数据向量 $b$ 分解为其在 $A$ 的列空间（即 $U$ 的列张成的空间）上的投影及其正交补：\n$$\nb = UU^\\top b + (I_m - UU^\\top)b = \\sum_{i=1}^r (u_i^\\top b) u_i + b_{ortho}\n$$\n将 $(H_\\alpha - I_m)$ 应用于 $b$：\n$$\nr_\\alpha = \\left( \\sum_{i=1}^r f_i(\\alpha) u_i u_i^\\top \\right) b - b = \\sum_{i=1}^r f_i(\\alpha) (u_i^\\top b) u_i - \\left(\\sum_{i=1}^r (u_i^\\top b) u_i + b_{ortho}\\right)\n$$\n$$\nr_\\alpha = \\sum_{i=1}^r (f_i(\\alpha) - 1) (u_i^\\top b) u_i - b_{ortho}\n$$\n代入 $f_i(\\alpha) - 1 = \\frac{\\sigma_i^2}{\\sigma_i^2 + \\alpha^2} - 1 = \\frac{-\\alpha^2}{\\sigma_i^2 + \\alpha^2}$：\n$$\nr_\\alpha = \\sum_{i=1}^r \\frac{-\\alpha^2}{\\sigma_i^2 + \\alpha^2} (u_i^\\top b) u_i - b_{ortho}\n$$\n求和中的项与 $b_{ortho}$ 正交。根据勾股定理，平方范数为：\n$$\n\\|r_\\alpha\\|_2^2 = \\left\\| \\sum_{i=1}^r \\frac{-\\alpha^2}{\\sigma_i^2 + \\alpha^2} (u_i^\\top b) u_i \\right\\|_2^2 + \\|b_{ortho}\\|_2^2\n$$\n由于向量 $u_i$ 是标准正交的，这变为：\n$$\n\\|A \\hat{x}_\\alpha - b\\|_2^2 = \\sum_{i=1}^r \\left( \\frac{\\alpha^2}{\\sigma_i^2 + \\alpha^2} \\right)^2 (u_i^\\top b)^2 + \\|(I_m - UU^\\top) b\\|_2^2\n$$\n第二项 $\\|b_{ortho}\\|_2^2$ 是 $b$ 的分量中与 $A$ 的列空间正交的部分的平方范数。它可以高效地计算为 $\\|b\\|_2^2 - \\|UU^\\top b\\|_2^2 = \\|b\\|_2^2 - \\sum_{i=1}^r(u_i^\\top b)^2$。\n\nGCV泛函 $G(\\alpha)$ 的所有分量现在都可以仅使用奇异值 $\\sigma_i$、投影数据分量 $u_i^\\top b$、总大小 $m$ 和 $b$ 的范数来计算。这避免了显式构造像 $H_\\alpha$ 或 $A^\\top A$ 这样的大型矩阵。数值实现将遵循这种基于SVD的公式。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the Tikhonov regularization problem for all test cases.\n    \"\"\"\n\n    def generate_test_cases():\n        \"\"\"\n        Generates the A matrices and b vectors for the three test cases.\n        \"\"\"\n        # Case 1\n        m1, n1 = 50, 30\n        A1 = np.zeros((m1, n1), dtype=np.float64)\n        i_idx, j_idx = np.ogrid[1:m1+1, 1:n1+1]\n        A1 = 1 / (1 + np.abs(i_idx - j_idx)) + 1e-3 * np.sin((i_idx + j_idx) / 10)\n        \n        j_vec1 = np.arange(1, n1 + 1)\n        xtrue1 = np.sin(j_vec1 / 4)\n        \n        i_vec1 = np.arange(1, m1 + 1)\n        noise1 = 1e-3 * ((-1)**i_vec1)\n        b1 = A1 @ xtrue1 + noise1\n\n        # Case 2\n        m2, n2 = 40, 40\n        i_idx2, j_idx2 = np.ogrid[1:m2+1, 1:n2+1]\n        A2 = np.exp(-((i_idx2 - j_idx2)**2) / (2 * 25))\n        \n        j_vec2 = np.arange(1, n2 + 1)\n        xtrue2 = np.cos(j_vec2 / 8)\n        \n        i_vec2 = np.arange(1, m2 + 1)\n        noise2 = 1e-2 * np.sin(i_vec2 / 3)\n        b2 = A2 @ xtrue2 + noise2\n        \n        # Case 3\n        m3, n3 = 20, 10\n        A3 = np.zeros((m3, n3), dtype=np.float64)\n        \n        i_vec3 = np.arange(1, m3 + 1)\n        b3 = (-1)**i_vec3\n        \n        return [(A1, b1), (A2, b2), (A3, b3)]\n\n    def compute_gcv_outputs(A, b):\n        \"\"\"\n        Computes the optimal alpha and corresponding outputs using GCV.\n        \"\"\"\n        m, n = A.shape\n        \n        # Step 1: Compute economy SVD\n        U, s, Vh = np.linalg.svd(A, full_matrices=False)\n        \n        # Step 2: Pre-compute SVD-based quantities\n        btilde = U.T @ b  # Components u_i^T * b\n        b_norm_sq = np.linalg.norm(b)**2\n        b_ortho_norm_sq = b_norm_sq - np.sum(btilde**2)\n        s_sq = s**2\n        k = len(s)\n\n        def gcv_func_vectorized(alphas):\n            \"\"\"\n            Calculates GCV values for a vector of alphas.\n            \"\"\"\n            alphas_sq = alphas[:, np.newaxis]**2  # Shape (num_alphas, 1)\n            f = s_sq / (s_sq + alphas_sq)         # Shape (num_alphas, k) using broadcasting\n            \n            trace_H = np.sum(f, axis=1) # Shape (num_alphas,)\n            \n            # Residual norm calculation\n            # (1-f_i)^2 = (alpha^2 / (sigma_i^2 + alpha^2))^2\n            term1 = np.sum(((1 - f)**2) * (btilde**2), axis=1) # Shape (num_alphas,)\n            residual_norm_sq = term1 + b_ortho_norm_sq\n            \n            denom = m - trace_H\n            # Handle potential division by zero\n            gcv_vals = np.full_like(denom, np.inf)\n            safe_indices = ~np.isclose(denom, 0)\n            gcv_vals[safe_indices] = m * residual_norm_sq[safe_indices] / (denom[safe_indices]**2)\n            \n            return gcv_vals\n\n        # Step 3: Search for optimal alpha\n        # Coarse Search\n        alphas1 = np.logspace(-8, 2, 200)\n        gcv_values1 = gcv_func_vectorized(alphas1)\n        min_idx1 = np.argmin(gcv_values1)\n        alpha_min1 = alphas1[min_idx1]\n        \n        # Refined Search\n        lower_bound = max(1e-8, alpha_min1 / 10)\n        upper_bound = min(1e2, alpha_min1 * 10)\n        alphas2 = np.logspace(np.log10(lower_bound), np.log10(upper_bound), 200)\n        gcv_values2 = gcv_func_vectorized(alphas2)\n        min_idx2 = np.argmin(gcv_values2)\n        alpha_star = alphas2[min_idx2]\n        \n        # Step 4: Calculate final results for alpha_star\n        alpha_star_sq = alpha_star**2\n        f_star = s_sq / (s_sq + alpha_star_sq)\n        \n        trace_H_star = np.sum(f_star)\n        \n        term1_star = np.sum(((1 - f_star)**2) * (btilde**2))\n        residual_norm_sq_star = term1_star + b_ortho_norm_sq\n        \n        # Handle case 3 where A is all zeros, s is empty\n        if k == 0:\n            alpha_star = 100.0 # Upper bound is chosen as GCV is flat\n            residual_norm_sq_star = b_norm_sq\n            trace_H_star = 0.0\n\n        return alpha_star, residual_norm_sq_star, trace_H_star\n\n    test_cases = generate_test_cases()\n    results = []\n\n    for A, b in test_cases:\n        alpha_star, res_norm_sq, trace_H = compute_gcv_outputs(A, b)\n        results.append(f\"[{alpha_star:.6f},{res_norm_sq:.6f},{trace_H:.6f}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}