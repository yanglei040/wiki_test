{
    "hands_on_practices": [
        {
            "introduction": "To truly understand how regularization balances data fit against solution stability, it is invaluable to view the problem through a spectral lens. This exercise guides you through a fundamental derivation that expresses the discrepancy functional in terms of the singular value decomposition (SVD), making the filtering effect of the regularization parameter $\\alpha$ explicit on each data component. This perspective is key to demystifying how the discrepancy principle operates under the hood .",
            "id": "3376643",
            "problem": "Consider a finite-dimensional linear inverse problem in data space of dimension $m$ and parameter space of dimension $n$ with a known forward operator matrix $A \\in \\mathbb{C}^{m \\times n}$. Let the Singular Value Decomposition (SVD), defined as the factorization of a matrix into $A = U \\Sigma V^{*}$ where $U \\in \\mathbb{C}^{m \\times m}$ and $V \\in \\mathbb{C}^{n \\times n}$ are unitary matrices and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal (possibly rectangular) with nonnegative diagonal entries, be given. Denote the nonzero singular values by $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{r}  0$, where $r = \\mathrm{rank}(A)$, and let $\\{u_{i}\\}_{i=1}^{m}$ and $\\{v_{i}\\}_{i=1}^{n}$ denote the left and right singular vectors (columns of $U$ and $V$), respectively. Let $y^{\\delta} \\in \\mathbb{C}^{m}$ denote noisy data with known noise level $\\delta  0$.\n\nA broad class of spectral regularization methods defines, for each regularization parameter $\\alpha  0$, a prediction $A x_{\\alpha}$ in the data space by applying scalar filter factors $g_{\\alpha}(\\sigma)$ to the singular components associated with the range of $A$, while leaving the orthogonal complement unchanged. In particular, if one writes the data coefficients $c_{i} = u_{i}^{*} y^{\\delta}$ for $i = 1, \\dots, m$, then the regularized prediction has the form\n$$\nA x_{\\alpha} = \\sum_{i=1}^{r} g_{\\alpha}(\\sigma_{i}) \\, c_{i} \\, u_{i}.\n$$\nThe discrepancy principle for parameter choice seeks $\\alpha$ such that the residual norm satisfies\n$$\n\\| A x_{\\alpha} - y^{\\delta} \\| = \\tau \\, \\delta,\n$$\nwhere $\\tau \\geq 1$ is a fixed safety factor and $\\|\\cdot\\|$ denotes the Euclidean norm on $\\mathbb{C}^{m}$.\n\nStarting from the definitions above and the orthonormality of the singular vectors, derive a closed-form analytic expression for the squared discrepancy functional\n$$\nD(\\alpha) = \\| A x_{\\alpha} - y^{\\delta} \\|^{2}\n$$\nin terms of the singular values $\\sigma_{i}$, the data coefficients $c_{i} = u_{i}^{*} y^{\\delta}$, and the filter factors $g_{\\alpha}(\\sigma_{i})$. Your final answer must be a single analytic expression for $D(\\alpha)$; do not include an equation or inequality in the final answer. No rounding is required, and there are no physical units.",
            "solution": "The user wants to derive a closed-form analytic expression for the squared discrepancy functional $D(\\alpha) = \\| A x_{\\alpha} - y^{\\delta} \\|^{2}$.\n\nThe problem states that the set of left singular vectors $\\{u_i\\}_{i=1}^{m}$ are the columns of a unitary matrix $U \\in \\mathbb{C}^{m \\times m}$, which implies they form an orthonormal basis for the data space $\\mathbb{C}^m$. Any vector in $\\mathbb{C}^m$, such as the noisy data vector $y^{\\delta}$, can be expressed as a linear combination of these basis vectors. The expansion of $y^{\\delta}$ in this basis is given by:\n$$\ny^{\\delta} = \\sum_{i=1}^{m} (u_{i}^{*} y^{\\delta}) u_{i}\n$$\nThe problem defines the coefficients of this expansion as $c_{i} = u_{i}^{*} y^{\\delta}$. Substituting this definition, we have:\n$$\ny^{\\delta} = \\sum_{i=1}^{m} c_{i} u_{i}\n$$\nThe regularized prediction $A x_{\\alpha}$ is given by the expression:\n$$\nA x_{\\alpha} = \\sum_{i=1}^{r} g_{\\alpha}(\\sigma_{i}) c_{i} u_{i}\n$$\nwhere $r = \\mathrm{rank}(A)$ is the number of nonzero singular values.\n\nThe discrepancy functional $D(\\alpha)$ is the squared Euclidean norm of the residual vector, $A x_{\\alpha} - y^{\\delta}$. We can now write the residual vector by substituting the expressions for $A x_{\\alpha}$ and $y^{\\delta}$:\n$$\nA x_{\\alpha} - y^{\\delta} = \\left( \\sum_{i=1}^{r} g_{\\alpha}(\\sigma_{i}) c_{i} u_{i} \\right) - \\left( \\sum_{i=1}^{m} c_{i} u_{i} \\right)\n$$\nTo combine the terms, we can split the second summation, which runs from $i=1$ to $m$, into two parts: one part from $i=1$ to $r$ and a second part from $i=r+1$ to $m$. This gives:\n$$\n\\sum_{i=1}^{m} c_{i} u_{i} = \\sum_{i=1}^{r} c_{i} u_{i} + \\sum_{i=r+1}^{m} c_{i} u_{i}\n$$\nSubstituting this back into the expression for the residual vector yields:\n$$\nA x_{\\alpha} - y^{\\delta} = \\left( \\sum_{i=1}^{r} g_{\\alpha}(\\sigma_{i}) c_{i} u_{i} \\right) - \\left( \\sum_{i=1}^{r} c_{i} u_{i} + \\sum_{i=r+1}^{m} c_{i} u_{i} \\right)\n$$\nWe can now group the terms corresponding to the same basis vectors $u_{i}$:\n$$\nA x_{\\alpha} - y^{\\delta} = \\sum_{i=1}^{r} (g_{\\alpha}(\\sigma_{i}) c_{i} - c_{i}) u_{i} - \\sum_{i=r+1}^{m} c_{i} u_{i}\n$$\nFactoring out the coefficients $c_{i}$ from the first summation, we get a representation of the residual vector in the orthonormal basis $\\{u_i\\}$:\n$$\nA x_{\\alpha} - y^{\\delta} = \\sum_{i=1}^{r} \\big(g_{\\alpha}(\\sigma_{i}) - 1\\big) c_{i} u_{i} + \\sum_{i=r+1}^{m} (-c_{i}) u_{i}\n$$\nThe squared Euclidean norm of a vector $v = \\sum_{i=1}^{m} \\beta_{i} u_{i}$ expanded in an orthonormal basis $\\{u_i\\}$ is given by the sum of the squared magnitudes of its coefficients, $\\|v\\|^2 = \\sum_{i=1}^{m} |\\beta_{i}|^2$. This is a consequence of the orthonormality property $u_{i}^{*}u_{j} = \\delta_{ij}$, known as Parseval's identity.\n\nApplying this to our residual vector, the coefficients are $\\beta_{i} = (g_{\\alpha}(\\sigma_{i}) - 1) c_{i}$ for $1 \\leq i \\leq r$, and $\\beta_{i} = -c_{i}$ for $r+1 \\leq i \\leq m$.\nTherefore, the squared discrepancy functional $D(\\alpha)$ is:\n$$\nD(\\alpha) = \\| A x_{\\alpha} - y^{\\delta} \\|^{2} = \\sum_{i=1}^{r} \\big| \\big(g_{\\alpha}(\\sigma_{i}) - 1\\big) c_{i} \\big|^{2} + \\sum_{i=r+1}^{m} |-c_{i}|^{2}\n$$\nUsing the property of the complex magnitude $|ab| = |a||b|$, and noting that $|-c_i| = |c_i|$, we can simplify the expression to:\n$$\nD(\\alpha) = \\sum_{i=1}^{r} |g_{\\alpha}(\\sigma_{i}) - 1|^{2} |c_{i}|^{2} + \\sum_{i=r+1}^{m} |c_{i}|^{2}\n$$\nThis is the final closed-form analytic expression for the squared discrepancy functional. The first term represents the squared norm of the \"data error\" component of the residual (the part within the range of $A$), while the second term represents the squared norm of the \"noise\" component of the residual (the part orthogonal to the range of $A$).",
            "answer": "$$\n\\boxed{\\sum_{i=1}^{r} |g_{\\alpha}(\\sigma_{i}) - 1|^{2} |c_{i}|^{2} + \\sum_{i=r+1}^{m} |c_{i}|^{2}}\n$$"
        },
        {
            "introduction": "The classical discrepancy principle performs robustly when the forward model is accurate and the noise level is well-characterized, but what happens when the model itself contains errors? This exercise challenges you to explore this common practical scenario through a carefully constructed thought experiment. You will investigate how model bias can systematically mislead the discrepancy principle into choosing a suboptimal regularization parameter and discover how to diagnose such effects using innovation statistics .",
            "id": "3376624",
            "problem": "Consider a linear inverse problem with two-dimensional observations and a scalar state. The true forward operator is the matrix $A \\in \\mathbb{R}^{2 \\times 1}$ given by $A = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. The observational noise is additive and bounded, and an observation $y_{\\delta} \\in \\mathbb{R}^{2}$ satisfies $y_{\\delta} = A x^{\\dagger} + \\eta$, where $x^{\\dagger} \\in \\mathbb{R}$ is the true state and $\\eta \\in \\mathbb{R}^{2}$ is noise with known norm $\\|\\eta\\|_{2} = \\delta  0$. Suppose the noise realization is aligned with the second coordinate so that $\\eta = \\begin{pmatrix} 0 \\\\ \\delta \\end{pmatrix}$, hence $y_{\\delta} = \\begin{pmatrix} x^{\\dagger} \\\\ \\delta \\end{pmatrix}$. A data analyst uses a biased forward operator $A + \\Delta A = \\tilde{A}$, where $\\tilde{A} = \\begin{pmatrix} 1 \\\\ \\varepsilon \\end{pmatrix}$ with a fixed bias parameter $\\varepsilon \\in [0,1)$.\n\nThe analyst applies Tikhonov regularization with parameter $\\alpha \\geq 0$ defined as the minimizer of the functional\n$$\nJ_{\\alpha}(x) = \\|\\tilde{A} x - y_{\\delta}\\|_{2}^{2} + \\alpha \\, \\|x\\|_{2}^{2}.\n$$\nThe analyst chooses $\\alpha$ using the Morozov discrepancy principle (also called the discrepancy principle), which prescribes selecting $\\alpha$ such that the data misfit equals a prescribed multiple of the noise level:\n$$\n\\|\\tilde{A} \\, x_{\\alpha}(y_{\\delta}) - y_{\\delta}\\|_{2} = \\tau \\, \\delta,\n$$\nwhere $x_{\\alpha}(y_{\\delta})$ is the Tikhonov solution for the parameter $\\alpha$ and $\\tau  1$ is a fixed safety factor. In this problem, take the signal-to-noise ratio to be $x^{\\dagger}/\\delta = 2$ and the safety factor to be $\\tau = \\sqrt{2}$; that is, $x^{\\dagger} = 2 \\delta$ and $\\tau^{2} = 2$.\n\nTasks:\n- Starting from the definition of Tikhonov regularization and the normal equations for the minimizer $x_{\\alpha}(y_{\\delta})$, derive an explicit equation for $\\alpha$ from the discrepancy principle condition above.\n- Solve this equation in closed form to obtain the $\\alpha$ selected by the discrepancy principle as a function of the bias $\\varepsilon \\in [0,1)$.\n- Using the unbiased case $\\varepsilon = 0$ as a reference, identify how the bias alters the selected $\\alpha$ and explain the mechanism by which the discrepancy principle over-regularizes when $\\varepsilon  0$.\n- Propose a statistically grounded diagnostic that could detect such model bias using innovation statistics in a data assimilation setting, and state precisely the scalar statistic to be monitored and its expected value under an unbiased forward model with correctly specified noise.\n\nProvide your final answer as a single closed-form expression for the discrepancy principle parameter $\\alpha$ in terms of $\\varepsilon$ for the setting described, with no numerical approximation. Do not include units. Do not round. Express the final result in its exact analytic form.",
            "solution": "The problem as stated is scientifically grounded, self-contained, and well-posed. It presents a standard, albeit simplified, scenario in inverse problems theory to illustrate the interaction between model error and the Morozov discrepancy principle for parameter selection in Tikhonov regularization. All provided data and definitions are consistent and sufficient to derive a unique solution. We may therefore proceed with the solution.\n\nThe problem asks for several derivations and explanations. We will address them in sequence.\n\nFirst, we derive the Tikhonov regularized solution $x_{\\alpha}(y_{\\delta})$, which we denote as $x_{\\alpha}$ for brevity. The state $x$ is a scalar, so the Tikhonov functional is\n$$\nJ_{\\alpha}(x) = \\|\\tilde{A} x - y_{\\delta}\\|_{2}^{2} + \\alpha \\|x\\|_{2}^{2} = (\\tilde{A}x - y_{\\delta})^{\\top}(\\tilde{A}x - y_{\\delta}) + \\alpha x^2\n$$\nTo find the minimizer, we compute the derivative with respect to $x$ and set it to zero:\n$$\n\\frac{dJ_{\\alpha}}{dx} = \\frac{d}{dx} \\left( x^2 (\\tilde{A}^{\\top}\\tilde{A}) - 2x(\\tilde{A}^{\\top}y_{\\delta}) + y_{\\delta}^{\\top}y_{\\delta} + \\alpha x^2 \\right) = 2x(\\tilde{A}^{\\top}\\tilde{A}) - 2(\\tilde{A}^{\\top}y_{\\delta}) + 2\\alpha x = 0\n$$\nThis yields the normal equation for the scalar case:\n$$\n(\\tilde{A}^{\\top}\\tilde{A} + \\alpha)x_{\\alpha} = \\tilde{A}^{\\top}y_{\\delta}\n$$\nThe solution is\n$$\nx_{\\alpha} = \\frac{\\tilde{A}^{\\top}y_{\\delta}}{\\tilde{A}^{\\top}\\tilde{A} + \\alpha}\n$$\nWe are given the biased forward operator $\\tilde{A} = \\begin{pmatrix} 1 \\\\ \\varepsilon \\end{pmatrix}$ and the observation $y_{\\delta} = \\begin{pmatrix} x^{\\dagger} \\\\ \\delta \\end{pmatrix}$. With $x^{\\dagger} = 2\\delta$, we have $y_{\\delta} = \\begin{pmatrix} 2\\delta \\\\ \\delta \\end{pmatrix} = \\delta \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$. Let's compute the terms in the expression for $x_{\\alpha}$:\n$$\n\\tilde{A}^{\\top}\\tilde{A} = \\begin{pmatrix} 1  \\varepsilon \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\varepsilon \\end{pmatrix} = 1 + \\varepsilon^2\n$$\n$$\n\\tilde{A}^{\\top}y_{\\delta} = \\begin{pmatrix} 1  \\varepsilon \\end{pmatrix} \\begin{pmatrix} 2\\delta \\\\ \\delta \\end{pmatrix} = 2\\delta + \\varepsilon\\delta = \\delta(2+\\varepsilon)\n$$\nSubstituting these into the expression for $x_{\\alpha}$ gives:\n$$\nx_{\\alpha} = \\frac{\\delta(2+\\varepsilon)}{1+\\varepsilon^2+\\alpha}\n$$\nNext, we use the Morozov discrepancy principle to find an equation for $\\alpha$. The principle states that $\\alpha$ is chosen such that the residual norm satisfies $\\|\\tilde{A}x_{\\alpha} - y_{\\delta}\\|_{2} = \\tau\\delta$. Given $\\tau = \\sqrt{2}$, this becomes $\\|\\tilde{A}x_{\\alpha} - y_{\\delta}\\|_{2}^2 = 2\\delta^2$.\n\nTo compute the residual norm, we can decompose the observation $y_{\\delta}$ in the basis of the singular vectors of $\\tilde{A}$. Since $\\tilde{A}$ is a $2 \\times 1$ matrix, it has one non-zero singular value $\\sigma_1 = \\|\\tilde{A}\\|_2 = \\sqrt{\\tilde{A}^{\\top}\\tilde{A}} = \\sqrt{1+\\varepsilon^2}$. The corresponding left singular vector is $u_1 = \\frac{1}{\\sigma_1}\\tilde{A} = \\frac{1}{\\sqrt{1+\\varepsilon^2}}\\begin{pmatrix} 1 \\\\ \\varepsilon \\end{pmatrix}$. The other left singular vector, $u_2$, spans the null space of $\\tilde{A}^{\\top}$ and must be orthogonal to $u_1$. We can choose $u_2 = \\frac{1}{\\sqrt{1+\\varepsilon^2}}\\begin{pmatrix} -\\varepsilon \\\\ 1 \\end{pmatrix}$.\n\nThe residual vector is $r_{\\alpha} = \\tilde{A}x_{\\alpha} - y_{\\delta}$. Its squared norm is $\\|r_{\\alpha}\\|_{2}^2 = \\|(u_1^{\\top}r_{\\alpha})u_1 + (u_2^{\\top}r_{\\alpha})u_2\\|_{2}^2 = (u_1^{\\top}r_{\\alpha})^2 + (u_2^{\\top}r_{\\alpha})^2$.\nThe $u_1$ component is:\n$$\nu_1^{\\top}r_{\\alpha} = u_1^{\\top}(\\tilde{A}x_{\\alpha} - y_{\\delta}) = u_1^{\\top}(\\sigma_1 u_1 x_{\\alpha}) - u_1^{\\top}y_{\\delta} = \\sigma_1 x_{\\alpha} - u_1^{\\top}y_{\\delta}\n$$\nWe have $u_1^{\\top}y_{\\delta} = \\frac{1}{\\sqrt{1+\\varepsilon^2}}\\begin{pmatrix} 1  \\varepsilon \\end{pmatrix} \\begin{pmatrix} 2\\delta \\\\ \\delta \\end{pmatrix} = \\frac{\\delta(2+\\varepsilon)}{\\sqrt{1+\\varepsilon^2}}$.\nSubstituting $x_{\\alpha}$:\n$$\n\\sigma_1 x_{\\alpha} - u_1^{\\top}y_{\\delta} = \\sqrt{1+\\varepsilon^2}\\frac{\\delta(2+\\varepsilon)}{1+\\varepsilon^2+\\alpha} - \\frac{\\delta(2+\\varepsilon)}{\\sqrt{1+\\varepsilon^2}} = \\frac{\\delta(2+\\varepsilon)}{\\sqrt{1+\\varepsilon^2}} \\left( \\frac{1+\\varepsilon^2}{1+\\varepsilon^2+\\alpha} - 1 \\right) = \\frac{\\delta(2+\\varepsilon)}{\\sqrt{1+\\varepsilon^2}} \\left( \\frac{-\\alpha}{1+\\varepsilon^2+\\alpha} \\right)\n$$\nThe $u_2$ component is:\n$$\nu_2^{\\top}r_{\\alpha} = u_2^{\\top}(\\tilde A x_{\\alpha} - y_{\\delta}) = (u_2^{\\top}\\tilde A) x_{\\alpha} - u_2^{\\top}y_{\\delta} = 0 - u_2^{\\top}y_{\\delta}\n$$\n$$\n-u_2^{\\top}y_{\\delta} = -\\frac{1}{\\sqrt{1+\\varepsilon^2}}\\begin{pmatrix} -\\varepsilon  1 \\end{pmatrix} \\begin{pmatrix} 2\\delta \\\\ \\delta \\end{pmatrix} = -\\frac{-2\\delta\\varepsilon + \\delta}{\\sqrt{1+\\varepsilon^2}} = \\frac{\\delta(2\\varepsilon-1)}{\\sqrt{1+\\varepsilon^2}}\n$$\nThe squared norm of the residual is:\n$$\n\\|r_{\\alpha}\\|_{2}^2 = \\left( \\frac{-\\alpha\\delta(2+\\varepsilon)}{\\sqrt{1+\\varepsilon^2}(1+\\varepsilon^2+\\alpha)} \\right)^2 + \\left( \\frac{\\delta(2\\varepsilon-1)}{\\sqrt{1+\\varepsilon^2}} \\right)^2 = \\frac{\\delta^2}{1+\\varepsilon^2} \\left[ \\frac{\\alpha^2(2+\\varepsilon)^2}{(1+\\varepsilon^2+\\alpha)^2} + (2\\varepsilon-1)^2 \\right]\n$$\nSetting this equal to $2\\delta^2$ and canceling $\\delta^2$:\n$$\n\\frac{1}{1+\\varepsilon^2} \\left[ \\frac{\\alpha^2(2+\\varepsilon)^2}{(1+\\varepsilon^2+\\alpha)^2} + (1-2\\varepsilon)^2 \\right] = 2\n$$\n$$\n\\frac{\\alpha^2(2+\\varepsilon)^2}{(1+\\varepsilon^2+\\alpha)^2} = 2(1+\\varepsilon^2) - (1-2\\varepsilon)^2 = 2+2\\varepsilon^2 - (1-4\\varepsilon+4\\varepsilon^2) = 1+4\\varepsilon-2\\varepsilon^2\n$$\nLet a new variable $X = \\frac{\\alpha}{1+\\varepsilon^2+\\alpha}$. The equation becomes:\n$$\nX^2 (2+\\varepsilon)^2 = 1+4\\varepsilon-2\\varepsilon^2\n$$\nSince $\\alpha \\ge 0$ and $\\varepsilon \\in [0,1)$, $X \\ge 0$. We can solve for $X$:\n$$\nX = \\frac{\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon}\n$$\n(We take the positive root, and note $2+\\varepsilon  0$). Now we solve for $\\alpha$ in terms of $X$. From $X = \\frac{\\alpha}{1+\\varepsilon^2+\\alpha}$, we have $X(1+\\varepsilon^2+\\alpha) = \\alpha$, which gives $X(1+\\varepsilon^2) = \\alpha(1-X)$, and thus:\n$$\n\\alpha = \\frac{X(1+\\varepsilon^2)}{1-X}\n$$\nSubstituting the expression for $X$:\n$$\n\\alpha = \\frac{\\frac{\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon}(1+\\varepsilon^2)}{1 - \\frac{\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon}} = \\frac{(1+\\varepsilon^2)\\frac{\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon}}{\\frac{2+\\varepsilon-\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon}}\n$$\nThis simplifies to the final closed-form expression for $\\alpha$:\n$$\n\\alpha(\\varepsilon) = \\frac{(1+\\varepsilon^2)\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon-\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}\n$$\nThis is the required expression. Now we analyze it. For the unbiased case $\\varepsilon=0$:\n$$\n\\alpha(0) = \\frac{(1)\\sqrt{1}}{2-\\sqrt{1}} = 1\n$$\nFor $\\varepsilon0$, the value of $\\alpha$ increases. For example, $\\alpha'(0) = 3$. A larger $\\alpha$ implies stronger regularization (a solution $x_{\\alpha}$ biased more towards $0$). The discrepancy principle causes over-regularization in the presence of model bias. The mechanism is as follows: The total data misfit $\\| \\tilde{A}x_\\alpha - y_\\delta \\|^2$ is composed of a part adjustable by $\\alpha$ (the projection of the residual onto the range of $\\tilde{A}$) and a non-adjustable part (the projection onto the null-space of $\\tilde{A}^\\top$). This latter part, $(u_2^\\top y_\\delta)^2 = \\frac{\\delta^2(1-2\\varepsilon)^2}{1+\\varepsilon^2}$, is an \"irreducible\" misfit due to model error. For $\\varepsilon \\in (0,1)$, this irreducible misfit is smaller than it is for $\\varepsilon=0$. The discrepancy principle, however, demands a fixed total misfit of $2\\delta^2$. To compensate for the smaller irreducible misfit, the principle forces the adjustable part of the misfit to be larger than in the unbiased case. Since the adjustable part is a monotonically increasing function of $\\alpha$, a larger $\\alpha$ is selected. The principle misinterprets part of the signal that the biased model cannot explain as noise, and increases the regularization strength to suppress it.\n\nFinally, we propose a statistically grounded diagnostic for model bias. In a data assimilation setting, one monitors the innovation vector, $d_k = y_k - \\tilde{A}x_{b,k}$, where $y_k$ is the observation at time $k$ and $x_{b,k}$ is the background (forecast) state. The theoretical expectation of the innovation is $E[d_k] = E[y_k - \\tilde{A}x_{b,k}] = E[Ax_{t,k} + \\eta_k - \\tilde{A}x_{b,k}]$. Under the null hypothesis of an unbiased model ($\\tilde{A}=A$), unbiased forecasts ($E[x_{b,k}] = E[x_{t,k}]$), and zero-mean noise ($E[\\eta_k]=0$), the expected innovation is zero: $E[d_k] = A E[x_{t,k}]-A E[x_{b,k}] = 0$.\nIf the model is biased, $E[d_k] = (A-\\tilde{A})E[x_{t,k}] = -(\\Delta A)E[x_{t,k}]$. A persistent non-zero mean innovation is a sign of model bias.\nThe specific scalar statistic to monitor would be a component of the time-averaged innovation vector. Given the structure of the bias in this problem, $\\Delta A = \\begin{pmatrix} 0 \\\\ \\varepsilon \\end{pmatrix}$, the bias only affects the second component.\nThus, the scalar statistic to be monitored is $S = \\text{mean}_{k}\\left((d_k)_2\\right) = \\frac{1}{N}\\sum_{k=1}^{N}(y_k - \\tilde{A}x_{b,k})_2$.\nThe expected value of this statistic under an unbiased forward model ($\\tilde{A}=A=\\begin{pmatrix}1\\\\0\\end{pmatrix}$) and correctly specified, zero-mean noise is:\n$$E[S] = E[(y_k - Ax_{b,k})_2] = E[(Ax_{t,k} + \\eta_k - Ax_{b,k})_2]$$\nSince the second component of $Ax$ is always $0$, this becomes $E[(\\eta_k)_2]$. With correctly specified zero-mean noise, this expected value is $0$. A statistically significant deviation of $S$ from $0$ would indicate a potential bias in the second component of the forward model.",
            "answer": "$$\n\\boxed{\\frac{(1+\\varepsilon^2)\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}{2+\\varepsilon-\\sqrt{1+4\\varepsilon-2\\varepsilon^2}}}\n$$"
        },
        {
            "introduction": "Many real-world inverse problems, from weather forecasting to economic modeling, are inherently dynamic. This capstone exercise bridges theory and practice by tasking you with implementing a sequential version of the discrepancy principle for a time-evolving system. By coding a simulation and analyzing its tracking performance under various conditions, you will gain hands-on experience in how regularization parameters can be adapted on the fly in fields like data assimilation .",
            "id": "3376683",
            "problem": "Consider a sequence of linear inverse problems posed at discrete times $t \\in \\{1,\\dots,T\\}$ with observation model $y_t^\\delta = A_t x_t^\\dagger + \\eta_t$, where $A_t \\in \\mathbb{R}^{m_t \\times n}$ is a known forward operator, $x_t^\\dagger \\in \\mathbb{R}^{n}$ is the unknown truth, $y_t^\\delta \\in \\mathbb{R}^{m_t}$ are noisy observations, and $\\eta_t \\in \\mathbb{R}^{m_t}$ is additive observational noise with zero mean and known positive-definite covariance $R_t \\in \\mathbb{R}^{m_t \\times m_t}$. Assume $R_t$ is diagonal with strictly positive diagonal entries. Define the weighted residual norm $\\|R_t^{-1/2}(A_t x - y_t^\\delta)\\|_2$, where $R_t^{-1/2}$ is the symmetric positive-definite square root of $R_t^{-1}$. \n\nAt each time $t$, estimate $x_t$ by solving a Tikhonov-regularized weighted least-squares subproblem that uses the previous estimate $x_{t-1}$ as a reference:\n$$\nx_t(\\alpha_t) = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2}\\|R_t^{-1/2}(A_t x - y_t^\\delta)\\|_2^2 + \\frac{\\alpha_t}{2} \\|x - x_{t-1}\\|_2^2 \\right\\},\n$$\nwith initialization $x_0 = 0$. The regularization parameter $\\alpha_t  0$ is chosen at each time $t$ by a sequential discrepancy principle:\n$$\n\\|R_t^{-1/2}(A_t x_t(\\alpha_t) - y_t^\\delta)\\|_2 \\approx \\tau \\sqrt{m_t},\n$$\nwhere $\\tau  0$ is a user-specified safety factor and $m_t$ is the number of observations at time $t$. \n\nStarting from the core definitions of weighted least squares, Gaussian noise with covariance $R_t$, and Tikhonov regularization, derive a principled sequential parameter-choice rule that determines $\\alpha_t$ at each time step so that the discrepancy condition is met. Your derivation should justify why, for fixed $(A_t, R_t, y_t^\\delta, x_{t-1})$, the mapping $\\alpha \\mapsto \\|R_t^{-1/2}(A_t x_t(\\alpha) - y_t^\\delta)\\|_2$ is monotone in $\\alpha$ under standard conditions, thereby admitting a one-dimensional root-finding procedure for $\\alpha_t$ at each $t$. Then specify an implementable algorithm that, for each $t$, produces $\\alpha_t$ by bracketing and bisection (or a similarly reliable monotone root finder), and solves the corresponding normal equations for $x_t(\\alpha_t)$.\n\nTo assess tracking performance, assume the unknown truth evolves according to a stable linear Gaussian state model:\n$$\nx_t^\\dagger = a x_{t-1}^\\dagger + w_t, \\quad w_t \\sim \\mathcal{N}(0, q^2 I_n),\n$$\nwith $x_0^\\dagger = 0$. Observations are generated by\n$$\ny_t^\\delta = A_t x_t^\\dagger + \\eta_t, \\quad \\eta_t \\sim \\mathcal{N}(0, \\sigma^2 I_{m_t}),\n$$\nwith known $\\sigma  0$. At each time $t$, $A_t$ is known and may vary with $t$. Let $L = I_n$ be the identity matrix.\n\nImplement the sequential discrepancy principle and report, for each test case below, the following three quantitative metrics aggregated over all times $t \\in \\{1,\\dots,T\\}$:\n- The mean residual ratio $r_{\\mathrm{mean}} = \\frac{1}{T} \\sum_{t=1}^T \\frac{\\|R_t^{-1/2}(A_t x_t(\\alpha_t) - y_t^\\delta)\\|_2}{\\tau \\sqrt{m_t}}$.\n- The mean tracking error $e_{\\mathrm{mean}} = \\frac{1}{T} \\sum_{t=1}^T \\|x_t(\\alpha_t) - x_t^\\dagger\\|_2$.\n- The hit fraction $h = \\frac{1}{T} \\sum_{t=1}^T \\mathbf{1}\\left( \\left| \\frac{\\|R_t^{-1/2}(A_t x_t(\\alpha_t) - y_t^\\delta)\\|_2}{\\tau \\sqrt{m_t}} - 1 \\right| \\le \\gamma \\right)$, with tolerance $\\gamma = 0.05$ (expressed as a decimal, not a percentage).\n\nAll simulations must be deterministic using fixed random seeds. Use the following test suite of parameter values, where $I_k$ denotes the $k \\times k$ identity matrix and all quantities are dimensionless:\n- Case A (happy path, overdetermined): $n = 40$, $m_t \\equiv 30$, $T = 25$, $a = 0.98$, $q = 0.01$, $\\sigma = 0.02$, $\\tau = 1.05$, seed $= 1234$. \n- Case B (edge case, underdetermined and faster dynamics): $n = 40$, $m_t \\equiv 20$, $T = 25$, $a = 0.98$, $q = 0.05$, $\\sigma = 0.02$, $\\tau = 1.05$, seed $= 1235$.\n- Case C (mismatched discrepancy target): $n = 40$, $m_t \\equiv 30$, $T = 25$, $a = 0.98$, $q = 0.01$, $\\sigma = 0.02$, $\\tau = 0.85$, seed $= 1236$.\n\nFor each case, construct $A_t$ as a smoothly time-varying matrix of size $m_t \\times n$ by taking a fixed base matrix with independent standard normal entries and adding a small sinusoidal perturbation of amplitude $0.1$ times an independent perturbation matrix, i.e., $A_t = A^{\\mathrm{base}} + \\epsilon \\sin(2\\pi t/T) A^{\\mathrm{pert}}$ with $\\epsilon = 0.1$. Set $R_t = \\sigma^2 I_{m_t}$ for all $t$.\n\nYour program should:\n- Implement the sequential discrepancy principle to select $\\alpha_t$ at each time using a robust bracketing and bisection scheme on a fixed interval $\\alpha \\in [\\alpha_{\\min}, \\alpha_{\\max}]$ with $\\alpha_{\\min} = 10^{-8}$ and $\\alpha_{\\max} = 10^{8}$, stopping when the absolute discrepancy is below a tolerance of $10^{-6}$ in the residual norm or after a fixed maximum number of iterations. If the target is not bracketed, return the endpoint that yields the closest residual to the target.\n- Compute the three metrics for each case and return them as floating-point numbers.\n\nFinal output format: Your program should produce a single line of output containing the $9$ results (three per case ordered as Case A then Case B then Case C) as a comma-separated list enclosed in square brackets (e.g., \"[rA,eA,hA,rB,eB,hB,rC,eC,hC]\"). No additional text should be printed. All angles, distances, or physical units are not applicable here; all quantities are dimensionless. Ensure numerical reproducibility by using the specified seeds and no external inputs.",
            "solution": "We begin from the definitions of weighted least squares and Tikhonov regularization in a dynamic setting. For each time $t$, the observation model is $y_t^\\delta = A_t x_t^\\dagger + \\eta_t$, with $\\eta_t$ mean-zero Gaussian noise with covariance $R_t$, assumed positive definite and known. The natural data misfit is the squared norm $\\|R_t^{-1/2}(A_t x - y_t^\\delta)\\|_2^2$. Regularization imposes prior information; here we use a quadratic penalty that enforces temporal smoothness relative to the previous estimate $x_{t-1}$, yielding the subproblem\n$$\n\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}\\|R_t^{-1/2}(A_t x - y_t^\\delta)\\|_2^2 + \\frac{\\alpha_t}{2} \\|x - x_{t-1}\\|_2^2.\n$$\nThis is a strictly convex quadratic functional for any $\\alpha_t  0$ and admits a unique minimizer $x_t(\\alpha_t)$. By setting the gradient to zero, we obtain the normal equations:\n$$\n\\left(A_t^\\top R_t^{-1} A_t + \\alpha_t I_n\\right) x_t(\\alpha_t) = A_t^\\top R_t^{-1} y_t^\\delta + \\alpha_t x_{t-1}.\n$$\nBecause $R_t$ is positive definite and $\\alpha_t  0$, the matrix on the left-hand side is symmetric positive definite, and the linear system is well-posed.\n\nThe discrepancy principle prescribes choosing $\\alpha_t$ so that the predicted data misfit matches the noise level (up to a factor $\\tau$), specifically\n$$\n\\|R_t^{-1/2}(A_t x_t(\\alpha_t) - y_t^\\delta)\\|_2 = \\tau \\sqrt{m_t}.\n$$\nTo justify a root-finding procedure for $\\alpha_t$, we examine the scalar function\n$$\n\\phi_t(\\alpha) = \\|R_t^{-1/2}(A_t x_t(\\alpha) - y_t^\\delta)\\|_2 - \\tau \\sqrt{m_t}.\n$$\nWe seek $\\alpha_t$ such that $\\phi_t(\\alpha_t) = 0$. Under standard conditions (e.g., $A_t$ fixed, $R_t \\succ 0$, and $x_{t-1}$ fixed), the function $\\alpha \\mapsto x_t(\\alpha)$ is continuous, and the residual norm $\\alpha \\mapsto \\|R_t^{-1/2}(A_t x_t(\\alpha) - y_t^\\delta)\\|_2$ is nondecreasing in $\\alpha$. A concise argument is as follows. Define $H_t = A_t^\\top R_t^{-1} A_t$ and $b_t = A_t^\\top R_t^{-1} y_t^\\delta$. Then $x_t(\\alpha)$ solves $(H_t + \\alpha I) x = b_t + \\alpha x_{t-1}$. Let $P_t(\\alpha) = A_t (H_t + \\alpha I)^{-1} A_t^\\top R_t^{-1}$. One can show that the residual $r_t(\\alpha) = R_t^{-1/2}(A_t x_t(\\alpha) - y_t^\\delta)$ equals\n$$\nr_t(\\alpha) = \\left(I_{m_t} - R_t^{1/2} A_t (H_t + \\alpha I)^{-1} A_t^\\top R_t^{-1/2}\\right) R_t^{-1/2}(y_t^\\delta - A_t x_{t-1}).\n$$\nThe map $\\alpha \\mapsto (H_t + \\alpha I)^{-1}$ is matrix monotone decreasing on $(0,\\infty)$ in the Loewner order. Therefore $I - R_t^{1/2} A_t (H_t + \\alpha I)^{-1} A_t^\\top R_t^{-1/2}$ is monotone increasing in the Loewner order, implying that the induced residual norm $\\|r_t(\\alpha)\\|_2$ is nondecreasing in $\\alpha$. Continuity follows from continuity of matrix inversion on the positive definite cone. Consequently, a scalar root-finding method such as bisection can be applied to $\\phi_t(\\alpha)$ on any interval where the target is bracketed.\n\nAlgorithmic design. For each time $t$, we proceed as follows:\n- Inputs: $A_t$, $R_t = \\sigma^2 I_{m_t}$, $y_t^\\delta$, previous estimate $x_{t-1}$, target level $\\tau \\sqrt{m_t}$, and bounds $\\alpha_{\\min} = 10^{-8}$, $\\alpha_{\\max} = 10^{8}$.\n- Define a solver that, for a given $\\alpha$, solves\n$$\n(H_t + \\alpha I) x = b_t + \\alpha x_{t-1},\n$$\nwhere $H_t = A_t^\\top R_t^{-1} A_t$ and $b_t = A_t^\\top R_t^{-1} y_t^\\delta$, using a robust linear solver for symmetric positive definite systems (or a general linear solver if symmetry is not enforced numerically).\n- Define the residual norm function $g_t(\\alpha) = \\|R_t^{-1/2}(A_t x_t(\\alpha) - y_t^\\delta)\\|_2$. Compute $\\phi_t(\\alpha) = g_t(\\alpha) - \\tau \\sqrt{m_t}$ at $\\alpha_{\\min}$ and $\\alpha_{\\max}$. If $\\phi_t(\\alpha_{\\min}) \\le 0 \\le \\phi_t(\\alpha_{\\max})$, apply bisection to find $\\alpha_t \\in [\\alpha_{\\min}, \\alpha_{\\max}]$ such that $|\\phi_t(\\alpha_t)|$ is below a small tolerance (e.g., $10^{-6}$ in absolute terms). If the target is not bracketed, select $\\alpha_t$ as whichever endpoint gives $g_t(\\alpha)$ closest to $\\tau \\sqrt{m_t}$.\n- Set $x_t = x_t(\\alpha_t)$ and continue to $t+1$.\n\nTracking performance analysis. The dynamic prior $\\|x - x_{t-1}\\|_2^2$ couples times through the reference $x_{t-1}$ and encourages temporal smoothness. When the process noise standard deviation $q$ is small (slowly varying truth) and $\\tau$ is chosen to approximately match the noise level, the discrepancy principle selects $\\alpha_t$ that balances fit to data and adherence to the prior; the mean residual ratio concentrates near $1$, and the mean tracking error remains small. When dynamics are faster (larger $q$) or the problem is underdetermined ($m_t  n$), the discrepancy principle tends to reduce $\\alpha_t$ to emphasize data fit, thereby allowing faster response, at the cost of increased variance and potentially larger tracking error. If $\\tau$ is set too low relative to the true noise level, the algorithm targets an over-ambitious residual, leading to smaller $\\alpha_t$ and overfitting; the mean residual ratio falls below $1$, and the hit fraction degrades.\n\nImplementation details for the test suite. We simulate $x_t^\\dagger$ via $x_t^\\dagger = a x_{t-1}^\\dagger + w_t$ with $x_0^\\dagger = 0$, $a = 0.98$, and $w_t \\sim \\mathcal{N}(0, q^2 I_n)$. Each $A_t$ is constructed as $A_t = A^{\\mathrm{base}} + \\epsilon \\sin(2\\pi t/T) A^{\\mathrm{pert}}$ with $\\epsilon = 0.1$, where $A^{\\mathrm{base}}$ and $A^{\\mathrm{pert}}$ have independent standard normal entries. Observations are $y_t^\\delta = A_t x_t^\\dagger + \\eta_t$ with $\\eta_t \\sim \\mathcal{N}(0, \\sigma^2 I_{m_t})$. We fix the random seed per case to ensure reproducibility. For each case, we compute:\n- $r_{\\mathrm{mean}} = \\frac{1}{T} \\sum_{t=1}^T \\frac{\\|R_t^{-1/2}(A_t x_t(\\alpha_t) - y_t^\\delta)\\|_2}{\\tau \\sqrt{m_t}}$,\n- $e_{\\mathrm{mean}} = \\frac{1}{T} \\sum_{t=1}^T \\|x_t(\\alpha_t) - x_t^\\dagger\\|_2$,\n- $h = \\frac{1}{T} \\sum_{t=1}^T \\mathbf{1}\\left( \\left| \\frac{\\|R_t^{-1/2}(A_t x_t(\\alpha_t) - y_t^\\delta)\\|_2}{\\tau \\sqrt{m_t}} - 1 \\right| \\le 0.05 \\right)$.\n\nWe then report the $9$ floating-point values in the order $(r_{\\mathrm{mean}}, e_{\\mathrm{mean}}, h)$ for Case A, then Case B, then Case C, as a single comma-separated list enclosed in square brackets, on a single line, with no additional text. The algorithm uses bracketing and bisection for $\\alpha_t$ with bounds $\\alpha_{\\min} = 10^{-8}$, $\\alpha_{\\max} = 10^{8}$, an absolute residual tolerance of $10^{-6}$, and a fixed maximum number of iterations to ensure termination. This design adheres to the sequential discrepancy principle and allows consistent evaluation of tracking performance across the diverse test cases.",
            "answer": "```python\nimport numpy as np\n\ndef simulate_and_evaluate(seed, n, m, T, a, q, sigma, tau):\n    rng = np.random.default_rng(seed)\n    # Generate base and perturbation matrices for A_t\n    A_base = rng.standard_normal((m, n))\n    A_pert = rng.standard_normal((m, n))\n    eps = 0.1\n\n    # True state initialization\n    x_true = np.zeros(n)\n    # Estimator initialization\n    x_est_prev = np.zeros(n)\n\n    # Covariance inverse components\n    R_inv_scalar = 1.0 / (sigma ** 2)\n    target = tau * np.sqrt(m)\n\n    residual_ratios = []\n    tracking_errors = []\n    hits = []\n\n    # Helper: solve for x given alpha\n    def solve_x_for_alpha(A_t, y_t, x_prev, alpha):\n        # Solve (A^T R^{-1} A + alpha I) x = A^T R^{-1} y + alpha x_prev\n        # Use R^{-1} = (1/sigma^2) I\n        H = R_inv_scalar * (A_t.T @ A_t)\n        rhs = R_inv_scalar * (A_t.T @ y_t) + alpha * x_prev\n        # Add alpha*I to H\n        # To improve numerical stability, use np.linalg.solve on the SPD-ish system\n        M = H + alpha * np.eye(n)\n        try:\n            x = np.linalg.solve(M, rhs)\n        except np.linalg.LinAlgError:\n            # Fallback to least squares if numerical issues arise\n            x, *_ = np.linalg.lstsq(M, rhs, rcond=None)\n        return x\n\n    # Helper: compute residual norm for given alpha\n    def residual_norm(A_t, y_t, x_prev, alpha):\n        x = solve_x_for_alpha(A_t, y_t, x_prev, alpha)\n        r = A_t @ x - y_t\n        # Weighted residual norm: ||R^{-1/2} r|| = ||r|| / sigma\n        rn = np.linalg.norm(r) / sigma\n        return rn, x\n\n    # Bisection to find alpha_t\n    def choose_alpha(A_t, y_t, x_prev):\n        alpha_lo = 1e-8\n        alpha_hi = 1e8\n        tol = 1e-6\n        max_it = 60\n\n        rn_lo, x_lo = residual_norm(A_t, y_t, x_prev, alpha_lo)\n        rn_hi, x_hi = residual_norm(A_t, y_t, x_prev, alpha_hi)\n        phi_lo = rn_lo - target\n        phi_hi = rn_hi - target\n\n        # If not bracketed, return endpoint closest to target\n        if phi_lo > 0 and phi_hi > 0:\n            # Both above target; pick smaller alpha to reduce residual\n            return alpha_lo, x_lo, rn_lo\n        if phi_lo  0 and phi_hi  0:\n            # Both below target; pick larger alpha to increase residual\n            return alpha_hi, x_hi, rn_hi\n\n        # Now bracketed: phi_lo = 0 = phi_hi\n        alo, ahi = alpha_lo, alpha_hi\n        x_mid = None\n        rn_mid = None\n        for _ in range(max_it):\n            amid = np.sqrt(alo * ahi)\n            rn_mid, x_mid = residual_norm(A_t, y_t, x_prev, amid)\n            phi_mid = rn_mid - target\n            if abs(phi_mid) = tol:\n                return amid, x_mid, rn_mid\n            if phi_mid > 0:\n                # Residual too large; decrease alpha\n                ahi = amid\n            else:\n                # Residual too small; increase alpha\n                alo = amid\n        # After max iterations, pick the closer endpoint among alo, ahi, amid\n        # Evaluate endpoints to choose best\n        rn_alo, x_alo = residual_norm(A_t, y_t, x_prev, alo)\n        rn_ahi, x_ahi = residual_norm(A_t, y_t, x_prev, ahi)\n        # Choose argument minimizing |rn - target|\n        cands = [(alo, x_alo, rn_alo), (amid, x_mid, rn_mid), (ahi, x_ahi, rn_ahi)]\n        best = min(cands, key=lambda t: abs(t[2] - target))\n        return best\n\n    # Time loop\n    for t in range(1, T + 1):\n        # Time-varying A_t\n        s = np.sin(2.0 * np.pi * t / T)\n        A_t = A_base + eps * s * A_pert\n        # Evolve truth\n        w_t = rng.standard_normal(n) * q\n        x_true = a * x_true + w_t\n        # Observation\n        eta_t = rng.standard_normal(m) * sigma\n        y_t = A_t @ x_true + eta_t\n\n        # Choose alpha via discrepancy principle\n        alpha_t, x_est, rn = choose_alpha(A_t, y_t, x_est_prev)\n        # Update\n        x_est_prev = x_est\n\n        # Metrics\n        residual_ratio = rn / target\n        err = np.linalg.norm(x_est - x_true)\n        hit = 1.0 if abs(residual_ratio - 1.0) = 0.05 else 0.0\n\n        residual_ratios.append(residual_ratio)\n        tracking_errors.append(err)\n        hits.append(hit)\n\n    r_mean = float(np.mean(residual_ratios))\n    e_mean = float(np.mean(tracking_errors))\n    h_frac = float(np.mean(hits))\n    return r_mean, e_mean, h_frac\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (seed, n, m, T, a, q, sigma, tau)\n        (1234, 40, 30, 25, 0.98, 0.01, 0.02, 1.05),  # Case A\n        (1235, 40, 20, 25, 0.98, 0.05, 0.02, 1.05),  # Case B\n        (1236, 40, 30, 25, 0.98, 0.01, 0.02, 0.85),  # Case C\n    ]\n\n    results = []\n    for case in test_cases:\n        seed, n, m, T, a, q, sigma, tau = case\n        r_mean, e_mean, h_frac = simulate_and_evaluate(seed, n, m, T, a, q, sigma, tau)\n        results.extend([r_mean, e_mean, h_frac])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        }
    ]
}