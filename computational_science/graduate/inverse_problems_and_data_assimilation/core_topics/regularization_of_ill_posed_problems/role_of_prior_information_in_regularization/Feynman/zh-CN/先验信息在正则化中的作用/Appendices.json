{
    "hands_on_practices": [
        {
            "introduction": "我们从一个最简单的非平凡案例入手：一个带高斯先验和高斯噪声的一维线性反问题。这个练习将演示高斯先验的方差如何直接控制数据拟合与先验信念之间的平衡。通过推导后验分布，您将看到先验的不确定性如何充当正则化强度的“调节旋钮”，从贝叶斯视角为吉洪诺夫正则化 (Tikhonov regularization) 提供基础直觉。",
            "id": "3418467",
            "problem": "考虑一个一维线性反问题，该问题被构建为一个贝叶斯数据同化任务。设未知状态为 $x \\in \\mathbb{R}$。单个观测值 $y \\in \\mathbb{R}$ 通过模型 $y = h x + \\varepsilon$ 与状态相关联，其中线性正演算子为 $h \\in \\mathbb{R}$，观测误差 $\\varepsilon$ 服从均值为零、方差为 $\\sigma^{2} > 0$ 的高斯分布，即 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$。关于 $x$ 的先验信息服从均值为 $\\mu \\in \\mathbb{R}$、方差为 $C > 0$ 的高斯分布，即 $x \\sim \\mathcal{N}(\\mu, C)$。贝叶斯后验由贝叶斯定理定义：后验密度与似然和先验的乘积成正比。\n\n现在定义一个缩放的先验协方差 $C \\mapsto \\alpha C$，其中 $\\alpha > 0$ 是一个标量。将 $\\alpha$ 视为一个可调参数，用于调整先验不确定性的水平。从高斯似然和高斯先验的定义出发，使用贝叶斯定理，推导出后验均值和后验方差，使其表示为包含 $\\alpha$、$h$、$\\sigma^{2}$、$\\mu$、$C$ 和 $y$ 的闭式解析表达式。然后，通过将后验与负对数后验（这是一个关于 $x$ 的二次准则）的最小化子联系起来，将 $\\alpha$ 的作用解释为由先验不确定性所决定的正则化强度。根据你推导出的表达式，解释改变 $\\alpha$ 如何改变数据失配项和先验项之间的平衡。\n\n将你的最终答案表示为一个双元素行矩阵，依次包含后验均值和后验方差。无需四舍五入。最终答案中无需单位。",
            "solution": "该问题要求推导线性反问题的后验均值和方差，并解释先验不确定性缩放参数 $\\alpha$ 的作用。\n\n首先，我们建立概率框架。状态是一个随机变量 $x \\in \\mathbb{R}$，观测值是一个随机变量 $y \\in \\mathbb{R}$。\n\n关于状态 $x$ 的先验信息由一个高斯分布给出，其均值为 $\\mu$，方差由 $\\alpha C$ 缩放，其中 $\\alpha > 0$ 且 $C > 0$。$x$ 的先验概率密度函数 (PDF) 为：\n$$p(x) = \\mathcal{N}(x; \\mu, \\alpha C) = \\frac{1}{\\sqrt{2\\pi \\alpha C}} \\exp\\left( -\\frac{(x - \\mu)^2}{2 \\alpha C} \\right)$$\n\n正演模型通过线性方程 $y = hx + \\varepsilon$ 将状态 $x$ 与观测值 $y$ 联系起来。观测误差 $\\varepsilon$ 来自一个均值为零、方差为 $\\sigma^2 > 0$ 的高斯分布，即 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$。这定义了似然函数，即给定状态 $x$ 时观测值 $y$ 的条件概率。给定一个 $x$ 值，$y$ 的分布是均值为 $hx$、方差为 $\\sigma^2$ 的高斯分布。似然 PDF 为：\n$$p(y|x) = \\mathcal{N}(y; hx, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(y - hx)^2}{2\\sigma^2} \\right)$$\n\n根据贝叶斯定理，后验 PDF $p(x|y)$ 与似然和先验的乘积成正比：\n$$p(x|y) \\propto p(y|x) p(x)$$\n代入似然和先验的表达式，我们得到：\n$$p(x|y) \\propto \\left[ \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(y - hx)^2}{2\\sigma^2} \\right) \\right] \\left[ \\frac{1}{\\sqrt{2\\pi \\alpha C}} \\exp\\left( -\\frac{(x - \\mu)^2}{2 \\alpha C} \\right) \\right]$$\n由于归一化常数不依赖于 $x$，我们可以合并指数项：\n$$p(x|y) \\propto \\exp\\left( -\\frac{1}{2}\\left[ \\frac{(y - hx)^2}{\\sigma^2} + \\frac{(x - \\mu)^2}{\\alpha C} \\right] \\right)$$\n指数部分是关于 $x$ 的二次函数，这表明后验分布也是高斯分布。设后验分布为 $p(x|y) = \\mathcal{N}(x; \\mu_{\\text{post}}, C_{\\text{post}})$，其形式为：\n$$p(x|y) \\propto \\exp\\left( -\\frac{(x - \\mu_{\\text{post}})^2}{2 C_{\\text{post}}} \\right)$$\n为了找到后验均值 $\\mu_{\\text{post}}$ 和后验方差 $C_{\\text{post}}$，我们可以展开 $p(x|y)$ 表达式中指数的参数，并对 $x$ 进行配方。令 $Q(x)$ 为方括号中的项：\n$$Q(x) = \\frac{(y - hx)^2}{\\sigma^2} + \\frac{(x - \\mu)^2}{\\alpha C}$$\n$$Q(x) = \\frac{y^2 - 2yhx + h^2x^2}{\\sigma^2} + \\frac{x^2 - 2x\\mu + \\mu^2}{\\alpha C}$$\n我们按 $x$ 的幂次对各项进行分组：\n$$Q(x) = x^2 \\left( \\frac{h^2}{\\sigma^2} + \\frac{1}{\\alpha C} \\right) - 2x \\left( \\frac{yh}{\\sigma^2} + \\frac{\\mu}{\\alpha C} \\right) + \\left( \\frac{y^2}{\\sigma^2} + \\frac{\\mu^2}{\\alpha C} \\right)$$\n将其与标准二次型 $\\frac{(x - \\mu_{\\text{post}})^2}{C_{\\text{post}}} = \\frac{1}{C_{\\text{post}}}x^2 - \\frac{2\\mu_{\\text{post}}}{C_{\\text{post}}}x + \\frac{\\mu_{\\text{post}}^2}{C_{\\text{post}}}$ 进行比较，我们可以确定系数。\n$x^2$ 的系数给出了后验方差的倒数：\n$$\\frac{1}{C_{\\text{post}}} = \\frac{h^2}{\\sigma^2} + \\frac{1}{\\alpha C} = \\frac{h^2 \\alpha C + \\sigma^2}{\\sigma^2 \\alpha C}$$\n求解后验方差 $C_{\\text{post}}$：\n$$C_{\\text{post}} = \\frac{\\sigma^2 \\alpha C}{h^2 \\alpha C + \\sigma^2}$$\n$x$ 的系数给出了后验均值与后验方差之比：\n$$\\frac{\\mu_{\\text{post}}}{C_{\\text{post}}} = \\frac{yh}{\\sigma^2} + \\frac{\\mu}{\\alpha C} = \\frac{yh \\alpha C + \\mu \\sigma^2}{\\sigma^2 \\alpha C}$$\n求解后验均值 $\\mu_{\\text{post}}$：\n$$\\mu_{\\text{post}} = C_{\\text{post}} \\left( \\frac{yh \\alpha C + \\mu \\sigma^2}{\\sigma^2 \\alpha C} \\right)$$\n代入 $C_{\\text{post}}$ 的表达式：\n$$\\mu_{\\text{post}} = \\left( \\frac{\\sigma^2 \\alpha C}{h^2 \\alpha C + \\sigma^2} \\right) \\left( \\frac{yh \\alpha C + \\mu \\sigma^2}{\\sigma^2 \\alpha C} \\right)$$\n$$\\mu_{\\text{post}} = \\frac{yh \\alpha C + \\mu \\sigma^2}{h^2 \\alpha C + \\sigma^2}$$\n\n现在，我们通过将贝叶斯公式与正则化框架联系起来，来解释 $\\alpha$ 的作用。$x$ 的最大后验 (MAP) 估计是使 $p(x|y)$ 最大化的值，这等同于最小化负对数后验。负对数后验与我们之前定义的二次函数 $Q(x)$ 成正比，该函数通常称为代价函数 $J(x)$：\n$$J(x) = \\frac{(y - hx)^2}{\\sigma^2} + \\frac{(x - \\mu)^2}{\\alpha C}$$\n该代价函数由两项组成：\n1.  数据失配项：项 $\\frac{(y - hx)^2}{\\sigma^2}$ 衡量了预测值 $hx$ 和观测值 $y$ 之间的平方差，并由观测误差方差的倒数加权。它量化了状态 $x$ 对数据的拟合程度。\n2.  先验/正则化项：项 $\\frac{(x - \\mu)^2}{\\alpha C}$ 惩罚了状态 $x$ 与先验均值 $\\mu$ 的偏差，并由先验方差 $\\alpha C$ 的倒数加权。\n\n参数 $\\alpha$ 直接控制先验分布 $\\mathcal{N}(\\mu, \\alpha C)$ 的方差。因此，它调节了对先验信息的置信水平，并充当正则化参数。\n-   当 $\\alpha \\to \\infty$ 时，先验方差变为无穷大，表示最大不确定性状态或无信息先验。正则化项 $\\frac{(x - \\mu)^2}{\\alpha C} \\to 0$，代价函数由数据失配项主导。后验均值趋近于 $\\lim_{\\alpha \\to \\infty} \\mu_{\\text{post}} = \\frac{yhC}{h^2C} = \\frac{y}{h}$，这是最大似然估计。解完全由数据驱动。\n-   当 $\\alpha \\to 0$ 时，先验方差趋近于零，表示对 $x = \\mu$ 的绝对确定状态。对于任何 $x \\neq \\mu$，正则化项 $\\frac{(x - \\mu)^2}{\\alpha C}$ 变得无穷大，迫使解趋向于先验均值。后验均值趋近于 $\\lim_{\\alpha \\to 0} \\mu_{\\text{post}} = \\frac{\\mu\\sigma^2}{\\sigma^2} = \\mu$。解完全由先验信息驱动，忽略了数据。\n\n总而言之，$\\alpha$ 调整了拟合数据与遵循先验信念之间的平衡。较大的 $\\alpha$ 表示对先验的置信度较低（较弱的正则化），允许后验估计更多地受到观测值 $y$ 的影响。较小的 $\\alpha$ 表示对先验的置信度较高（较强的正则化），将估计值拉近先验均值 $\\mu$。这展示了先验在反问题中既是信息来源又是正则化机制的双重作用。\n\n最终答案由推导出的后验均值和后验方差组成。\n后验均值：$\\mu_{\\text{post}} = \\frac{yh \\alpha C + \\mu \\sigma^2}{h^2 \\alpha C + \\sigma^2}$\n后验方差：$C_{\\text{post}} = \\frac{\\sigma^2 \\alpha C}{h^2 \\alpha C + \\sigma^2}$",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{yh \\alpha C + \\mu \\sigma^2}{h^2 \\alpha C + \\sigma^2}  \\frac{\\sigma^2 \\alpha C}{h^2 \\alpha C + \\sigma^2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "并非所有问题都需要平滑解；有些问题需要稀疏解，即大部分系数为零。本练习将探讨拉普拉斯先验 (Laplace prior) 的使用，与高斯先验不同，它强烈偏好零值。您将推导出著名的软阈值算子 (soft-thresholding operator) 作为此问题最大后验 (MAP) 估计的解，从而揭示拉普拉斯先验与 $\\ell_1$ 正则化之间的直接联系，后者是压缩感知和现代信号处理的基石。",
            "id": "3418473",
            "problem": "考虑一个单系数线性观测模型，其中未知标量状态 $x$ 通过单个带有加性噪声的线性测量 $y$ 进行观测，即 $y = a x + \\varepsilon$。假设噪声 $\\varepsilon$ 服从零均值、方差为 $\\sigma^{2}$ 的高斯分布，并且 $x$ 的先验是拉普拉斯分布，其密度与 $\\exp(-\\lambda |x|)$ 成正比，其中 $\\lambda  0$ 是一个固定的惩罚权重。您的任务是：\n\n1) 仅从适定、下半连续凸函数 $f$ 的邻近算子的定义出发，即对于任意 $\\gamma  0$，$\\operatorname{prox}_{\\gamma f}(v) = \\arg\\min_{x} \\left\\{ \\frac{1}{2} |x - v|^{2} + \\gamma f(x) \\right\\}$，并使用凸分析中的次微分最优性条件，推导一维情况下 $\\operatorname{prox}_{\\gamma \\lambda |\\cdot|}(v)$ 的闭式表达式。\n\n2) 使用贝叶斯定理以及高斯似然和拉普拉斯先验的定义，写出该模型中给定 $y$ 时 $x$ 的负对数后验（不计无关的加性常数），并将最大后验（MAP）估计量描述为该负对数后验的最小化子。通过配方法和重新缩放，将 MAP 问题简化为与第 (1) 部分相匹配的邻近问题，然后获得用 $a$、$\\sigma^{2}$、$\\lambda$ 和 $y$ 表示的 MAP 估计量的显式闭式表达式。\n\n3) 对于 $a = 1.6$、$\\sigma^{2} = 0.8$、$\\lambda = 0.3$ 和 $y = 0.2$ 的情况，计算您的闭式表达式。报告 $x$ 的 MAP 估计量的最终数值，四舍五入到四位有效数字。不需要单位。",
            "solution": "该问题分为三个部分。我们将按顺序解决它们。\n\n**第1部分：邻近算子的推导**\n\n我们被要求推导邻近算子 $\\operatorname{prox}_{\\gamma \\lambda |\\cdot|}(v)$ 的闭式表达式。令阈值参数表示为 $\\alpha = \\gamma\\lambda$。由于 $\\gamma  0$ 且 $\\lambda  0$，我们有 $\\alpha  0$。函数为 $f(x) = |x|$。根据所给定义，邻近算子由一个最小化问题的解给出：\n$$\n\\operatorname{prox}_{\\alpha |\\cdot|}(v) = \\arg\\min_{x} \\left\\{ J(x) = \\frac{1}{2} (x - v)^{2} + \\alpha |x| \\right\\}\n$$\n目标函数 $J(x)$ 是一个严格凸的可微函数（$\\frac{1}{2}(x-v)^2$）和一个凸函数（$\\alpha|x|$）的和。因此，$J(x)$ 是严格凸的，并有唯一的最小化子，我们记为 $x^*$。\n\n我们使用次微分最优性条件，该条件表明 $x^*$ 是最小化子当且仅当 $0$ 属于 $J(x)$ 在 $x^*$ 处的次微分。\n$$\n0 \\in \\partial J(x^*)\n$$\n$J(x)$ 的次微分由其各组成部分的次微分之和给出：\n$$\n\\partial J(x) = \\partial \\left( \\frac{1}{2}(x - v)^2 \\right) + \\partial(\\alpha |x|)\n$$\n第一项是可微的，所以其​​次微分就是它的导数：$x - v$。第二项的次微分是 $\\alpha \\partial|x|$。绝对值函数 $\\partial|x|$ 的次微分定义为：\n$$\n\\partial |x| =\n\\begin{cases}\n    \\{1\\}         \\text{若 } x  0 \\\\\n    \\{-1\\}        \\text{若 } x  0 \\\\\n    [-1, 1]   \\text{若 } x = 0\n\\end{cases}\n$$\n所以，最小化子 $x^*$ 的最优性条件是：\n$$\n0 \\in (x^* - v) + \\alpha \\partial|x^*|\n$$\n这可以重写为：\n$$\nv - x^* \\in \\alpha \\partial|x^*|\n$$\n我们通过考虑 $x^*$ 的三种情况来分析这个条件：\n\n情况1：$x^*  0$。\n在这种情况下，$\\partial|x^*| = \\{1\\}$。条件变为 $v - x^* = \\alpha(1)$，这意味着 $x^* = v - \\alpha$。为使该解与假设 $x^*  0$ 一致，我们必须有 $v - \\alpha  0$，即 $v  \\alpha$。\n\n情况2：$x^*  0$。\n在这种情况下，$\\partial|x^*| = \\{-1\\}$。条件变为 $v - x^* = \\alpha(-1)$，这意味着 $x^* = v + \\alpha$。为使该解与假设 $x^*  0$ 一致，我们必须有 $v + \\alpha  0$，即 $v  -\\alpha$。\n\n情况3：$x^* = 0$。\n在这种情况下，$\\partial|x^*| = [-1, 1]$。条件变为 $v - 0 \\in \\alpha[-1, 1]$，这等价于 $v \\in [-\\alpha, \\alpha]$，即 $|v| \\le \\alpha$。\n\n结合这三种情况，我们得到 $x^*$ 的完整解：\n$$\nx^* =\n\\begin{cases}\n    v - \\alpha   \\text{若 } v  \\alpha \\\\\n    v + \\alpha   \\text{若 } v  -\\alpha \\\\\n    0          \\text{若 } |v| \\le \\alpha\n\\end{cases}\n$$\n这个分段函数被称为软阈值算子，通常表示为 $\\mathcal{S}_{\\alpha}(v)$。它可以写成一个紧凑的形式：\n$$\nx^* = \\operatorname{sgn}(v) \\max(0, |v| - \\alpha)\n$$\n用原始参数 $\\gamma\\lambda$ 替换 $\\alpha$，我们得到邻近算子的最终表达式：\n$$\n\\operatorname{prox}_{\\gamma \\lambda |\\cdot|}(v) = \\operatorname{sgn}(v) \\max(0, |v| - \\gamma\\lambda)\n$$\n\n**第2部分：MAP 估计量的推导**\n\n给定线性观测模型 $y = ax + \\varepsilon$，其中噪声 $\\varepsilon$ 是均值为 $0$、方差为 $\\sigma^2$ 的高斯噪声，即 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$。这意味着给定状态 $x$ 时观测值 $y$ 的似然也是高斯分布：$y|x \\sim \\mathcal{N}(ax, \\sigma^2)$。似然的概率密度函数 (PDF) 为：\n$$\np(y|x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - ax)^2}{2\\sigma^2}\\right)\n$$\n$x$ 的先验是拉普拉斯分布，其密度与 $\\exp(-\\lambda|x|)$ 成正比，其中 $\\lambda  0$：\n$$\np(x) \\propto \\exp(-\\lambda|x|)\n$$\n根据贝叶斯定理，$x$ 在给定 $y$ 的条件下的后验分布与似然和先验的乘积成正比：\n$$\np(x|y) \\propto p(y|x) p(x) \\propto \\exp\\left(-\\frac{(y - ax)^2}{2\\sigma^2}\\right) \\exp(-\\lambda|x|) = \\exp\\left(-\\frac{(y - ax)^2}{2\\sigma^2} - \\lambda|x|\\right)\n$$\n最大后验 (MAP) 估计量 $\\hat{x}_{\\text{MAP}}$ 是使后验概率 $p(x|y)$ 最大化的 $x$ 的值。这等价于最小化负对数后验。负对数后验（不计无关的加性常数）为：\n$$\nJ_{\\text{MAP}}(x) = \\frac{(y - ax)^2}{2\\sigma^2} + \\lambda|x|\n$$\n因此，MAP 估计量由下式给出：\n$$\n\\hat{x}_{\\text{MAP}} = \\arg\\min_x \\left\\{ \\frac{1}{2\\sigma^2}(y - ax)^2 + \\lambda|x| \\right\\}\n$$\n为了将其与第1部分中的邻近算子问题联系起来，我们必须将目标函数 $J_{\\text{MAP}}(x)$ 变换成 $\\frac{1}{2}(x-v)^2 + \\alpha'|x|$ 的形式。我们首先对包含 $x$ 的二次项进行配方：\n$$\n\\frac{1}{2\\sigma^2}(y - ax)^2 = \\frac{1}{2\\sigma^2}(a^2x^2 - 2axy + y^2) = \\frac{a^2}{2\\sigma^2} \\left(x^2 - 2\\frac{y}{a}x + \\left(\\frac{y}{a}\\right)^2\\right) = \\frac{a^2}{2\\sigma^2} \\left(x - \\frac{y}{a}\\right)^2\n$$\n将此代回目标函数：\n$$\nJ_{\\text{MAP}}(x) = \\frac{a^2}{2\\sigma^2} \\left(x - \\frac{y}{a}\\right)^2 + \\lambda|x|\n$$\n最小化此函数等价于最小化同一个函数乘以一个正常数，因为缩放不会改变最小值的位置。我们乘以 $\\frac{\\sigma^2}{a^2}$ 使得平方项的系数等于 $\\frac{1}{2}$：\n$$\n\\hat{x}_{\\text{MAP}} = \\arg\\min_x \\left\\{ \\frac{\\sigma^2}{a^2} \\left[ \\frac{a^2}{2\\sigma^2} \\left(x - \\frac{y}{a}\\right)^2 + \\lambda|x| \\right] \\right\\} = \\arg\\min_x \\left\\{ \\frac{1}{2} \\left(x - \\frac{y}{a}\\right)^2 + \\frac{\\sigma^2\\lambda}{a^2}|x| \\right\\}\n$$\n这个表达式现在是第1部分中邻近问题的形式，即 $\\arg\\min_x \\{ \\frac{1}{2}(x-v)^2 + \\alpha'|x| \\}$，其中对应关系为：\n$$\nv = \\frac{y}{a} \\quad \\text{和} \\quad \\alpha' = \\frac{\\sigma^2\\lambda}{a^2}\n$$\n使用第1部分中推导出的软阈值解，其中阈值参数为 $\\alpha'$：\n$$\n\\hat{x}_{\\text{MAP}} = \\mathcal{S}_{\\alpha'}(v) = \\operatorname{sgn}(v) \\max(0, |v| - \\alpha')\n$$\n代回 $v$ 和 $\\alpha'$ 的表达式，我们得到 MAP 估计量的显式闭式表达式：\n$$\n\\hat{x}_{\\text{MAP}} = \\operatorname{sgn}\\left(\\frac{y}{a}\\right) \\max\\left(0, \\left|\\frac{y}{a}\\right| - \\frac{\\sigma^2\\lambda}{a^2}\\right)\n$$\n\n**第3部分：数值计算**\n\n我们被要求使用给定值计算 $\\hat{x}_{\\text{MAP}}$ 的闭式表达式：$a = 1.6$、$\\sigma^2 = 0.8$、$\\lambda = 0.3$ 和 $y = 0.2$。\n\n首先，我们计算表达式内的各项：\n$$\n\\frac{y}{a} = \\frac{0.2}{1.6} = \\frac{2}{16} = \\frac{1}{8} = 0.125\n$$\n接下来，我们计算阈值参数：\n$$\n\\frac{\\sigma^2\\lambda}{a^2} = \\frac{(0.8)(0.3)}{(1.6)^2} = \\frac{0.24}{2.56} = \\frac{24}{256} = \\frac{3}{32} = 0.09375\n$$\n现在，我们将这些值代入 $\\hat{x}_{\\text{MAP}}$ 的表达式中：\n$$\n\\hat{x}_{\\text{MAP}} = \\operatorname{sgn}(0.125) \\max(0, |0.125| - 0.09375)\n$$\n由于 $0.125  0.09375$，$\\max$ 函数内的项为正。由于 $\\operatorname{sgn}(0.125) = 1$，我们得到：\n$$\n\\hat{x}_{\\text{MAP}} = 1 \\times (0.125 - 0.09375) = 0.03125\n$$\n问题要求结果四舍五入到四位有效数字。数字 $0.03125$ 恰好有四位有效数字（$3, 1, 2, 5$）。因此，不需要进一步的四舍五入。\n$x$ 的 MAP 估计量的数值为 $0.03125$。",
            "answer": "$$\n\\boxed{0.03125}\n$$"
        },
        {
            "introduction": "既然我们已经阐明了先验如何导致正则化，一个关键的实践问题依然存在：如何选择正则化参数 $\\lambda$？这个计算练习将介绍 L 曲线 (L-curve)，这是一种广泛用于完成此任务的启发式方法。通过绘制数据失配与正则化惩罚项之间的权衡曲线，您将实现一种寻找该曲线“拐点”的方法，该拐点通常代表了对 $\\lambda$ 的一个平衡且有效的选择。",
            "id": "3418439",
            "problem": "考虑一个线性逆问题，其观测模型为 $b = A x_{\\mathrm{true}} + \\varepsilon$，其中 $A \\in \\mathbb{R}^{m \\times n}$，$x_{\\mathrm{true}} \\in \\mathbb{R}^n$，以及加性噪声 $\\varepsilon \\in \\mathbb{R}^m$。假设似然为高斯分布，即 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$，且 $x$ 的先验为高斯分布，其均值为 $\\mu \\in \\mathbb{R}^n$，精度算子为 $\\alpha^2 L^{\\top} L$，其中 $L \\in \\mathbb{R}^{p \\times n}$。最大后验（MAP）估计通过最小化负对数后验得到，这等价于 Tikhonov 正则化：最小化泛函 $J_{\\lambda}(x) = \\lVert A x - b \\rVert_2^2 + \\lambda \\lVert L (x - \\mu) \\rVert_2^2$，其中 $\\lambda = \\sigma^2 \\alpha^2  0$。这通过选择 $L$ 和 $\\mu$ 来编码先验信息。对于每个 $\\lambda  0$，最小化子 $x_{\\lambda}$ 满足正规方程 $(A^{\\top} A + \\lambda L^{\\top} L) x_{\\lambda} = A^{\\top} b + \\lambda L^{\\top} L \\mu$。\n\nL-curve 准则通过考虑参数曲线 $\\mathcal{C}(\\lambda) = (\\xi(\\lambda), \\eta(\\lambda))$（其中 $\\xi(\\lambda) = \\log \\lVert A x_{\\lambda} - b \\rVert_2$ 和 $\\eta(\\lambda) = \\log \\lVert L (x_{\\lambda} - \\mu) \\rVert_2$）来评估数据失配和先验惩罚项之间的权衡。L-curve 的“拐角”通常对应于拟合与正则化之间的良好平衡，可以通过最大化曲率来定位。为了获得尺度不变的参数化，定义 $t = \\log \\lambda$ 并考虑平面曲线 $c(t) = (\\xi(t), \\eta(t))$，其中 $\\lambda = e^t$。平面曲线 $c(t) = (x(t), y(t))$ 的曲率 $\\kappa$ 由以下经过充分检验的公式给出\n$$\n\\kappa(t) = \\frac{\\lvert x'(t) y''(t) - y'(t) x''(t) \\rvert}{\\left( x'(t)^2 + y'(t)^2 \\right)^{3/2}}.\n$$\n在此设定中，$x(t) = \\xi(t)$ 且 $y(t) = \\eta(t)$，导数是关于 $t = \\log \\lambda$ 计算的。在数值上，$\\kappa(t)$ 可以通过在 $t$ 的均匀网格上使用有限差分来近似。\n\n您的任务是构建此准则并实现一个程序，对于一小组但多样化的测试用例，计算在 $t = \\log \\lambda$ 的均匀网格上的 $\\kappa(t)$，找到最大化点 $t_{\\star}$（不包括端点），并报告每个测试用例的 $\\lambda_{\\star} = e^{t_{\\star}}$。\n\n请使用以下测试套件、通用定义和数值细节：\n\n所有测试用例的通用定义：\n- 维度 $n = 20$，网格点 $s_i = \\frac{i}{n+1}$，其中 $i = 1, 2, \\dots, n$。\n- 定义 $\\Delta = \\frac{1}{n}$ 和核宽度 $w = 0.08$。\n- 通过下式定义高斯模糊矩阵 $A_0 \\in \\mathbb{R}^{n \\times n}$\n$$\n(A_0)_{ij} = \\exp\\!\\left( - \\frac{(s_i - s_j)^2}{2 w^2} \\right) \\, \\Delta \\quad \\text{for } 1 \\le i,j \\le n.\n$$\n- 通过下式定义“真实”状态 $x_{\\mathrm{true}, i} = \\sin(2 \\pi s_i) + 0.5 \\, s_i$，其中 $i = 1, 2, \\dots, n$。\n- 对于给定的噪声水平 $\\sigma_{\\mathrm{noise}}  0$，通过下式定义一个确定性噪声向量 $\\varepsilon \\in \\mathbb{R}^n$：$\\varepsilon_i = \\sigma_{\\mathrm{noise}} \\, \\frac{\\cos(7 i)}{i}$，其中 $i = 1, 2, \\dots, n$。\n- 对于每个测试用例，数据为 $b = A x_{\\mathrm{true}} + \\varepsilon$，使用指定的 $A$ 和 $\\sigma_{\\mathrm{noise}}$。\n\n定义以下正则化算子和先验均值选项：\n- 单位算子 $L = I_n$（因此 $p = n$）。\n- 一阶差分算子 $D \\in \\mathbb{R}^{(n-1) \\times n}$，其元素为 $D_{k,k} = 1$，$D_{k,k+1} = -1$（$k = 1, 2, \\dots, n-1$），其余元素为零。\n- 先验均值 $\\mu \\in \\mathbb{R}^n$ 如下指定（$\\mu = 0$ 或非零趋势）。\n\n测试套件（四个用例）：\n1. 用例 1（病态前向映射，单位先验）：$A = A_0$，$L = I_n$，$\\mu = 0$，$\\sigma_{\\mathrm{noise}} = 10^{-3}$。\n2. 用例 2（病态前向映射，具有非零均值的光滑先验）：$A = A_0$，$L = D$，$\\mu_i = 0.25 \\, s_i$，$\\sigma_{\\mathrm{noise}} = 10^{-3}$。\n3. 用例 3（良态前向映射，光滑先验）：$A = I_n$，$L = D$，$\\mu = 0$，$\\sigma_{\\mathrm{noise}} = 5 \\cdot 10^{-2}$。\n4. 用例 4（秩亏前向映射，单位先验）：从 $A_0$ 开始，将其最后一列设置为其第一列以获得 $A \\in \\mathbb{R}^{n \\times n}$，即 $A_{:,n} := A_{:,1}$，然后使用 $L = I_n$，$\\mu = 0$，$\\sigma_{\\mathrm{noise}} = 10^{-3}$。\n\n对于每个用例：\n- 在一个 $\\lambda$ 值网格上求解 $x_{\\lambda}$，该网格由 $t = \\log \\lambda$ 上的均匀网格定义，其中 $t \\in [\\log(10^{-10}), \\log(10^2)]$ 且有 $M = 201$ 个网格点。对于每个 $\\lambda = e^t$，计算 $\\rho(\\lambda) = \\lVert A x_{\\lambda} - b \\rVert_2$ 和 $\\eta(\\lambda) = \\lVert L (x_{\\lambda} - \\mu) \\rVert_2$，然后设置 $\\xi(t) = \\log \\rho(\\lambda)$ 和 $\\eta_{\\log}(t) = \\log \\eta(\\lambda)$。\n- 在均匀的 $t$-网格上通过中心有限差分数值近似导数 $\\xi'(t)$、$\\xi''(t)$、$\\eta_{\\log}'(t)$ 和 $\\eta_{\\log}''(t)$。使用以下公式计算曲率\n$$\n\\kappa(t) = \\frac{\\left| \\xi'(t) \\, \\eta_{\\log}''(t) - \\eta_{\\log}'(t) \\, \\xi''(t) \\right|}{\\left( \\xi'(t)^2 + \\eta_{\\log}'(t)^2 \\right)^{3/2}}.\n$$\n- 找出在内部网格点（不包括端点）上最大化 $\\kappa(t)$ 的索引 $k^{\\star}$。报告 $\\lambda_{\\star} = \\exp(t_{k^{\\star}})$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含按顺序排列的对应于用例 1 到 4 的四个 $\\lambda_{\\star}$ 值，四舍五入到六位小数，形式为用方括号括起来的逗号分隔列表（例如，$[0.123456,0.000789,1.234568,0.010000]$）。\n- 不涉及物理单位。不出现角度。不使用百分比。\n- 答案必须是浮点数。\n\n您的实现必须是一个完整、可运行的程序，能够构建矩阵，执行上述计算，并以所需格式输出结果，无需任何外部输入。",
            "solution": "目标是通过实现 L-curve 准则，为四个不同的线性逆问题找到最优正则化参数 $\\lambda_{\\star}$。L-curve 方法为平衡对观测数据的保真度和对解的先验信息的遵循度提供了一种启发式方法。\n\n线性逆问题由方程 $b = A x_{\\mathrm{true}} + \\varepsilon$ 建模，其中 $b \\in \\mathbb{R}^m$ 是观测值，$A \\in \\mathbb{R}^{m \\times n}$ 是前向算子，$x_{\\mathrm{true}} \\in \\mathbb{R}^n$ 是未知的真实状态，$\\varepsilon \\in \\mathbb{R}^m$ 是测量噪声。在贝叶斯框架中，这对应于高斯似然 $\\mathcal{N}(b | A x, \\sigma^2 I)$。我们对解施加一个高斯先验 $x \\sim \\mathcal{N}(\\mu, (\\alpha^2 L^{\\top} L)^{-1})$，其中 $\\mu$ 是先验均值，$L$ 是一个编码结构信息（例如，光滑性）的算子。\n\n最大后验（MAP）估计通过最小化负对数后验得到，从而产生 Tikhonov 正则化泛函：\n$$\nJ_{\\lambda}(x) = \\lVert A x - b \\rVert_2^2 + \\lambda \\lVert L (x - \\mu) \\rVert_2^2\n$$\n在这里，正则化参数 $\\lambda  0$ 平衡了数据失配项 $\\lVert A x - b \\rVert_2^2$ 和正则化（或惩罚）项 $\\lVert L (x - \\mu) \\rVert_2^2$。此泛函的最小化子，记为 $x_{\\lambda}$，是正则化解。对 $J_{\\lambda}(x)$ 关于 $x$ 求梯度并将其设为零，得到正规方程，这是一个关于 $x_{\\lambda}$ 的线性方程组：\n$$\n(A^{\\top} A + \\lambda L^{\\top} L) x_{\\lambda} = A^{\\top} b + \\lambda L^{\\top} L \\mu\n$$\n对于每个候选的 $\\lambda$ 值，都必须求解这个方程组。\n\nL-curve 准则基于在一系列 $\\lambda$ 值上，惩罚项的对数与失配项的对数的参数图。该曲线定义为 $\\mathcal{C}(\\lambda) = (\\log \\lVert A x_\\lambda - b \\rVert_2, \\log \\lVert L(x_\\lambda - \\mu) \\rVert_2)$。这条曲线通常呈“L”形。这个 L 形的拐角被认为代表了最优平衡，其中失配项和惩罚项都不会过大。这个拐角对应于曲线上曲率最大的点。\n\n为了计算这一点，我们使用 $t = \\log \\lambda$ 重新参数化曲线，因此曲线由 $c(t) = (\\xi(t), \\eta_{\\log}(t))$ 给出，其中：\n$$\n\\xi(t) = \\log \\lVert A x_{\\lambda(t)} - b \\rVert_2 \\quad \\text{和} \\quad \\eta_{\\log}(t) = \\log \\lVert L (x_{\\lambda(t)} - \\mu) \\rVert_2, \\quad \\text{其中 } \\lambda(t) = e^t\n$$\n这个平面曲线 $c(t)$ 的曲率 $\\kappa$ 由以下公式给出：\n$$\n\\kappa(t) = \\frac{\\lvert \\xi'(t) \\eta_{\\log}''(t) - \\eta_{\\log}'(t) \\xi''(t) \\rvert}{\\left( \\xi'(t)^2 + \\eta_{\\log}'(t)^2 \\right)^{3/2}}\n$$\n其中导数是关于 $t$ 计算的。\n\n计算步骤如下：\n1. 对于每个测试用例，按规定构建矩阵 $A$ 和 $L$，以及向量 $b$ 和 $\\mu$。\n2. 在区间 $[\\log(10^{-10}), \\log(10^2)]$ 上定义一个 $t = \\log \\lambda$ 的均匀网格，包含 $M=201$ 个点。设此网格为 $\\{t_k\\}_{k=0}^{M-1}$，步长为 $h = t_{k+1} - t_k$。\n3. 对于网格中的每个 $t_k$：\n    a. 计算 $\\lambda_k = e^{t_k}$。\n    b. 求解正规方程 $(A^{\\top} A + \\lambda_k L^{\\top} L) x_{\\lambda_k} = A^{\\top} b + \\lambda_k L^{\\top} L \\mu$ 以得到 $x_{\\lambda_k}$。\n    c. 计算失配范数 $\\rho_k = \\lVert A x_{\\lambda_k} - b \\rVert_2$ 和惩罚范数 $\\eta_k = \\lVert L (x_{\\lambda_k} - \\mu) \\rVert_2$。为避免对数的数值问题，我们在运算前加上机器 epsilon。\n    d. 计算对数-对数坐标：$\\xi_k = \\log(\\rho_k + \\epsilon_{\\text{mach}})$ 和 $\\eta_{\\log,k} = \\log(\\eta_k + \\epsilon_{\\text{mach}})$。\n4. 使用中心有限差分在内部网格点（$k=1, \\dots, M-2$）近似 $\\xi(t)$ 和 $\\eta_{\\log}(t)$ 的一阶和二阶导数：\n    $$\n    f'(t_k) \\approx \\frac{f_{k+1} - f_{k-1}}{2h}, \\quad f''(t_k) \\approx \\frac{f_{k+1} - 2f_k + f_{k-1}}{h^2}\n    $$\n5. 将这些数值导数代入曲率公式，为每个内部点计算 $\\kappa(t_k)$。\n6. 找到在内部点上使曲率 $\\kappa$ 最大化的索引 $k^{\\star}$。最优参数即为 $\\lambda_{\\star} = e^{t_{k^{\\star}}}$。\n\n此过程应用于四个测试用例中的每一个，这些用例探索了不同类型的前向算子（病态、良态、秩亏）和先验信息（单位先验 vs. 光滑先验，零均值 vs. 非零均值）。最终输出将是计算出的四个 $\\lambda_{\\star}$ 值的列表。",
            "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve_for_lambda_star(A, L, b, mu, t_grid):\n    \"\"\"\n    Solves for the optimal lambda using the L-curve curvature criterion.\n    \"\"\"\n    n, p_dim = L.shape[0], L.shape[1]\n    \n    # Pre-compute constant matrices and vectors\n    AtA = A.T @ A\n    LtL = L.T @ L\n    Atb = A.T @ b\n    LtLmu = LtL @ mu\n    \n    misfit_norms = np.zeros(len(t_grid))\n    penalty_norms = np.zeros(len(t_grid))\n    \n    # Epsilon for numerical stability of log\n    mach_eps = np.finfo(float).eps\n\n    for i, t in enumerate(t_grid):\n        lambda_val = np.exp(t)\n        \n        # Solve the normal equations: (AtA + lambda*LtL)x = Atb + lambda*LtLmu\n        lhs_matrix = AtA + lambda_val * LtL\n        rhs_vector = Atb + lambda_val * LtLmu\n        \n        try:\n            x_lambda = linalg.solve(lhs_matrix, rhs_vector, assume_a='sym')\n        except linalg.LinAlgError:\n            # Fallback to pseudo-inverse for singular matrices if necessary\n            x_lambda = linalg.lstsq(lhs_matrix, rhs_vector)[0]\n\n        # Calculate misfit and penalty norms\n        misfit_norms[i] = linalg.norm(A @ x_lambda - b)\n        penalty_norms[i] = linalg.norm(L @ (x_lambda - mu))\n\n    # Compute log-log coordinates\n    xi = np.log(misfit_norms + mach_eps)\n    eta_log = np.log(penalty_norms + mach_eps)\n    \n    # Calculate derivatives using centered finite differences for interior points\n    h = t_grid[1] - t_grid[0]\n    \n    # First derivatives\n    xi_p = (xi[2:] - xi[:-2]) / (2 * h)\n    eta_log_p = (eta_log[2:] - eta_log[:-2]) / (2 * h)\n    \n    # Second derivatives\n    xi_pp = (xi[2:] - 2 * xi[1:-1] + xi[:-2]) / (h**2)\n    eta_log_pp = (eta_log[2:] - 2 * eta_log[1:-1] + eta_log[:-2]) / (h**2)\n    \n    # Calculate curvature for interior points\n    numerator = np.abs(xi_p * eta_log_pp - eta_log_p * xi_pp)\n    denominator = (xi_p**2 + eta_log_p**2)**(3/2)\n    \n    # Avoid division by zero\n    kappa = np.divide(numerator, denominator, out=np.zeros_like(numerator), where=denominator  mach_eps)\n    \n    # Find the index of maximum curvature among interior points\n    # Interior points' indices in the original grid are 1, 2, ..., M-2\n    # The argmax gives an index relative to the kappa array (0, 1, ..., M-3)\n    k_star_interior_idx = np.argmax(kappa)\n    \n    # Map back to the index in the original t_grid\n    t_star_idx = k_star_interior_idx + 1\n    \n    lambda_star = np.exp(t_grid[t_star_idx])\n    \n    return lambda_star\n\ndef solve():\n    # --- Common definitions for all test cases ---\n    n = 20\n    s = np.array([i / (n + 1) for i in range(1, n + 1)])\n    delta = 1.0 / n\n    w = 0.08\n    \n    # Gaussian blur matrix A0\n    i_minus_j = s[:, np.newaxis] - s[np.newaxis, :]\n    A0 = np.exp(-i_minus_j**2 / (2 * w**2)) * delta\n    \n    # True state x_true\n    x_true = np.sin(2 * np.pi * s) + 0.5 * s\n    \n    # Regularization operators\n    L_I = np.identity(n)\n    L_D = np.eye(n - 1, n, k=0) - np.eye(n - 1, n, k=1)\n\n    # Lambda grid\n    t_grid = np.linspace(np.log(10**-10), np.log(10**2), 201)\n    \n    test_cases_params = [\n        # Case 1: ill-conditioned, identity prior\n        {'A': A0, 'L': L_I, 'mu': np.zeros(n), 'sigma_noise': 1e-3},\n        # Case 2: ill-conditioned, smoothness prior, non-zero mean\n        {'A': A0, 'L': L_D, 'mu': 0.25 * s, 'sigma_noise': 1e-3},\n        # Case 3: well-conditioned, smoothness prior\n        {'A': np.identity(n), 'L': L_D, 'mu': np.zeros(n), 'sigma_noise': 5e-2},\n        # Case 4: rank-deficient, identity prior\n        {'A': A0.copy(), 'L': L_I, 'mu': np.zeros(n), 'sigma_noise': 1e-3},\n    ]\n    # Modify A for case 4\n    test_cases_params[3]['A'][:, -1] = test_cases_params[3]['A'][:, 0]\n\n    results = []\n    \n    i_vals = np.arange(1, n + 1)\n        \n    for params in test_cases_params:\n        A = params['A']\n        L = params['L']\n        mu = params['mu']\n        sigma_noise = params['sigma_noise']\n        \n        # Noise vector\n        epsilon = sigma_noise * (np.cos(7 * i_vals) / i_vals)\n        \n        # Data vector b\n        b = A @ x_true + epsilon\n        \n        lambda_star = solve_for_lambda_star(A, L, b, mu, t_grid)\n        results.append(lambda_star)\n\n    # Final print statement in the exact required format\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}