{
    "hands_on_practices": [
        {
            "introduction": "Landweber 迭代的收敛性对其步长 $\\omega$ 的选择极为敏感。本练习旨在将抽象的理论条件 $0  \\omega  \\frac{2}{\\|A\\|^2}$ 具体化。通过构建一个简单的反例 ，你将亲手计算当步长越过稳定边界时，误差如何在迭代中被放大，从而深刻理解为何审慎选择步长是保证算法稳定性的第一步。",
            "id": "3372411",
            "problem": "考虑一个有限维欧几里得空间中的线性逆问题，其数据模型为 $y = A x^{\\ast}$，其中 $A \\in \\mathbb{R}^{2 \\times 2}$，真实状态为 $x^{\\ast} \\in \\mathbb{R}^{2}$。用于估计 $x^{\\ast}$ 的 Landweber 迭代定义为 $x_{k+1} = x_{k} + \\omega A^{\\top} \\big( y - A x_{k} \\big)$，其中 $\\omega > 0$ 是一个固定的步长。对向量使用标准的欧几里得范数，对矩阵使用诱导算子范数，因此 $\\|A\\|$ 等于 $A$ 的最大奇异值。\n\n构造一个明确的例子，其中\n- $A = \\sigma u v^{\\top}$，对于某个 $\\sigma > 0$，其中 $u, v \\in \\mathbb{R}^{2}$ 是单位向量，\n- 选择 $u = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 和 $v = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$，使得 $A = \\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix}$ 有一个等于 $\\|A\\| = \\sigma$ 的单正奇异值，另一个奇异值为零，\n- 设置数据为 $y = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，使得最小范数解为 $x^{\\dagger} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，\n- 在 $x_{0} = v = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 处初始化迭代，\n- 并选择步长 $\\omega = \\dfrac{3}{\\|A\\|^{2}}$。\n\n从 Landweber 迭代的基本定义和奇异值分解（SVD, singular value decomposition）的性质出发，推导沿非零奇异方向的单步误差递推关系，并计算相应标量放大因子的模。你的最终答案必须是等于该模的单个实数。无需四舍五入。",
            "solution": "该问题陈述经核实具有科学依据，是适定、客观且自洽的。所有必要信息均已提供，足以确定唯一解。\n\n线性逆问题由数据模型 $y = A x^{\\ast}$ 给出，其中 $y \\in \\mathbb{R}^{2}$ 是观测数据，$A \\in \\mathbb{R}^{2 \\times 2}$ 是正向算子，$x^{\\ast} \\in \\mathbb{R}^{2}$ 是待估计的真实状态。Landweber 迭代用于寻找 $x^{\\ast}$ 的估计值 $x_k$，其定义为递推关系：\n$$\nx_{k+1} = x_{k} + \\omega A^{\\top} \\big( y - A x_{k} \\big)\n$$\n其中 $k$ 是迭代指数，$x_0$ 是初始猜测，$\\omega > 0$ 是一个固定的步长（松弛参数），$A^{\\top}$ 是 $A$ 的转置。\n\n迭代收敛性的分析是通过研究误差向量的演化来进行的，误差向量定义为 $e_k = x_k - x^{\\dagger}$，其中 $x^{\\dagger}$ 是所求的真实解。在本问题中，指定 $y = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。$Ax=y$ 的所有解的集合是 $A$ 的零空间，记作 $\\text{ker}(A)$。最小范数解 $x^{\\dagger}$ 是 $\\text{ker}(A)$ 中欧几里得范数最小的元素。对于 $A = \\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix}$ 和 $\\sigma>0$，方程 $Ax=y$ 变为 $\\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，这意味着 $\\sigma x_1 = 0$，所以 $x_1=0$。解集为 $\\{ (0,c) \\mid c \\in \\mathbb{R} \\}$。当 $c=0$ 时得到最小范数解，因此 $x^{\\dagger} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$。因此，误差向量就是 $e_k = x_k$。\n\n现在我们可以推导误差递推关系。由于 $y = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ 和 $x^{\\dagger} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$，我们有 $y = A x^{\\dagger}$。我们将此代入 Landweber 迭代公式：\n$$\nx_{k+1} = x_k + \\omega A^{\\top} (A x^{\\dagger} - A x_k)\n$$\n从两边减去 $x^{\\dagger}$：\n$$\nx_{k+1} - x^{\\dagger} = x_k - x^{\\dagger} - \\omega A^{\\top} A (x_k - x^{\\dagger})\n$$\n这就得到了误差传播方程：\n$$\ne_{k+1} = e_k - \\omega A^{\\top} A e_k = \\left(I - \\omega A^{\\top} A\\right) e_k\n$$\n其中 $I$ 是 $2 \\times 2$ 单位矩阵。矩阵 $G = I - \\omega A^{\\top} A$ 是误差传播算子。\n\n问题的行为最好在由 $A$ 的奇异值分解（SVD）定义的坐标系中进行分析。设 $A$ 的 SVD 为 $A = U \\Sigma V^{\\top}$，其中 $U$ 和 $V$ 是正交矩阵，其列分别是左奇异向量和右奇异向量，$\\Sigma$ 是奇异值 $\\sigma_j$ 的对角矩阵。右奇异向量 $\\{v_j\\}$ 构成了定义域 $\\mathbb{R}^2$ 的一个标准正交基。它们是 $A^{\\top} A$ 的特征向量，因为 $A^{\\top} A = (V \\Sigma^{\\top} U^{\\top}) (U \\Sigma V^{\\top}) = V (\\Sigma^{\\top} \\Sigma) V^{\\top}$。矩阵 $\\Sigma^{\\top} \\Sigma$ 是一个对角矩阵，其对角线上的元素为 $\\sigma_j^2$。因此，$A^{\\top} A v_j = \\sigma_j^2 v_j$。\n\n误差传播算子 $G$ 作用于右奇异向量 $v_j$ 的方式如下：\n$$\nG v_j = (I - \\omega A^{\\top} A) v_j = I v_j - \\omega (A^{\\top} A v_j) = v_j - \\omega \\sigma_j^2 v_j = (1 - \\omega \\sigma_j^2) v_j\n$$\n这表明 $A$ 的右奇异向量 $v_j$ 也是误差传播算子 $G$ 的特征向量。对应的特征值为 $\\lambda_j = 1 - \\omega \\sigma_j^2$。如果在第 $k$ 步的误差有沿 $v_j$ 的分量，比如说 $\\alpha_{k,j} v_j$，那么在第 $k+1$ 步，这个分量就变成 $\\alpha_{k+1,j} v_j = \\lambda_j (\\alpha_{k,j} v_j)$。因此，值 $\\lambda_j$ 是误差在 $v_j$ 方向分量的标量放大因子。\n\n问题给出了具体的值。矩阵是 $A = \\begin{pmatrix} \\sigma  0 \\\\ 0  0 \\end{pmatrix}$，其中 $\\sigma > 0$。奇异值为 $\\sigma_1 = \\sigma$ 和 $\\sigma_2 = 0$。相应的右奇异向量可以选择为 $v_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ 和 $v_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$。算子范数为 $\\|A\\| = \\max(\\sigma_1, \\sigma_2) = \\sigma$。\n\n“非零奇异方向”对应于奇异值 $\\sigma_1 = \\sigma$。问题要求的是沿这个方向的标量放大因子，即 $\\lambda_1 = 1 - \\omega \\sigma_1^2 = 1 - \\omega \\sigma^2$。\n\n步长给定为 $\\omega = \\dfrac{3}{\\|A\\|^{2}}$。由于 $\\|A\\| = \\sigma$，我们有 $\\omega = \\dfrac{3}{\\sigma^2}$。\n\n将这个 $\\omega$ 的值代入放大因子的表达式中：\n$$\n\\lambda_1 = 1 - \\left(\\frac{3}{\\sigma^2}\\right) \\sigma^2 = 1 - 3 = -2\n$$\n沿非零奇异方向的误差分量的标量放大因子是 $-2$。\n\n问题要求的是这个标量放大因子的模。\n$$\n| \\lambda_1 | = | -2 | = 2\n$$\n这个结果表明，在第一次奇异向量方向上的误差分量在每次迭代中被放大了 $2$ 倍，导致迭代发散。这是预料之中的，因为所提供的步长 $\\omega = \\frac{3}{\\|A\\|^2}$ 超出了 Landweber 迭代的标准收敛区间 $0  \\omega  \\frac{2}{\\|A\\|^2}$。",
            "answer": "$$\\boxed{2}$$"
        },
        {
            "introduction": "在解决了基本的收敛性问题后，我们面临逆问题中的核心挑战：噪声。本练习  引入了一种强大的正则化思想，即在迭代过程中融入关于解的先验信息。你将比较标准 Landweber 迭代与投影 Landweber 迭代的效果，并量化证明投影操作如何有效地消除特定噪声分量，从而获得更精确的重构结果。",
            "id": "3372385",
            "problem": "考虑一个定义在实希尔伯特空间 $\\mathcal{H}$ 上的线性逆问题，其内积为 $\\langle \\cdot, \\cdot \\rangle$，范数为 $\\|\\cdot\\|$。设 $\\{\\varphi_{L}, \\varphi_{H}\\}$ 是 $\\mathcal{H}$ 的一个二维模型子空间的正交归一基，其中 $\\varphi_{L}$ 是一个低频模态，$\\varphi_{H}$ 是一个高频模态。设 $A : \\mathcal{H} \\to \\mathcal{H}$ 是一个有界、自伴、正算子，用于模拟一个平滑正向映射，使得\n$$\nA \\varphi_{L} = a_{L} \\varphi_{L}, \\qquad A \\varphi_{H} = a_{H} \\varphi_{H},\n$$\n其中 $a_{L}, a_{H} \\in (0,1]$ 且 $a_{H}  a_{L}$。观测数据 $y \\in \\mathcal{H}$ 根据加性噪声模型 $y = A x^{\\dagger} + \\eta$ 生成，其中真实解为 $x^{\\dagger} = s \\varphi_{L}$（$s \\in \\mathbb{R}$），噪声 $\\eta = \\nu \\varphi_{H}$（$\\nu \\in \\mathbb{R}$）仅激励高频模态。\n\n定义 Landweber 迭代（初始迭代点 $x^{0} = 0$）为\n$$\nx^{n+1} = x^{n} + \\omega A \\big( y - A x^{n} \\big), \\qquad n \\in \\mathbb{N},\n$$\n其中步长 $\\omega$ 满足 $0  \\omega  \\frac{2}{\\|A\\|^{2}}$，且 $\\|A\\| = a_{L}$。同时考虑投影到凸集\n$$\nC = \\big\\{ x \\in \\mathcal{H} : \\langle x, \\varphi_{H} \\rangle = 0 \\big\\}\n$$\n上的 Landweber 迭代，其形式为\n$$\nx^{n+1}_{\\mathrm{proj}} = P_{C}\\!\\left( x^{n}_{\\mathrm{proj}} + \\omega A \\big( y - A x^{n}_{\\mathrm{proj}} \\big) \\right),\n$$\n其中 $P_{C}$ 表示到 $C$ 上的正交投影。\n\n仅使用上述核心定义和指定模型，推导在固定迭代次数 $N \\in \\mathbb{N}$ 时，无约束 Landweber 迭代的平方重构误差 $\\|x^{N} - x^{\\dagger}\\|^{2}$ 和投影 Landweber 迭代的平方重构误差 $\\|x^{N}_{\\mathrm{proj}} - x^{\\dagger}\\|^{2}$。然后，通过计算误差减少量来量化投影带来的改进\n$$\n\\Delta E(N) = \\|x^{N} - x^{\\dagger}\\|^{2} - \\|x^{N}_{\\mathrm{proj}} - x^{\\dagger}\\|^{2}.\n$$\n请仅用 $a_{H}$、$\\omega$、$\\nu$ 和 $N$ 给出 $\\Delta E(N)$ 的单个闭式解析表达式。不需要进行数值计算。最终答案无需单位。",
            "solution": "用户希望分析投影对特定线性逆问题的 Landweber 迭代的影响。\n\n## 问题验证\n### 步骤1：提取已知条件\n-   **希尔伯特空间：** $\\mathcal{H}$ 是一个实希尔伯特空间，其内积为 $\\langle \\cdot, \\cdot \\rangle$，范数为 $\\|\\cdot\\|$。\n-   **基：** $\\{\\varphi_{L}, \\varphi_{H}\\}$ 是 $\\mathcal{H}$ 的一个二维子空间的正交归一基。\n-   **正向算子：** $A : \\mathcal{H} \\to \\mathcal{H}$ 是一个有界、自伴、正算子。\n-   **本征关系：** $A \\varphi_{L} = a_{L} \\varphi_{L}$ 和 $A \\varphi_{H} = a_{H} \\varphi_{H}$，其中 $a_{L}, a_{H} \\in (0,1]$ 且 $a_{H}  a_{L}$。\n-   **数据模型：** $y = A x^{\\dagger} + \\eta$。\n-   **真实解：** $x^{\\dagger} = s \\varphi_{L}$，对于某个 $s \\in \\mathbb{R}$。\n-   **噪声：** $\\eta = \\nu \\varphi_{H}$，对于某个 $\\nu \\in \\mathbb{R}$。\n-   **无约束 Landweber 迭代：** $x^{n+1} = x^{n} + \\omega A ( y - A x^{n} )$，初始迭代点 $x^{0} = 0$。\n-   **步长条件：** $0  \\omega  \\frac{2}{\\|A\\|^{2}}$，且 $\\|A\\| = a_{L}$。\n-   **凸集：** $C = \\{ x \\in \\mathcal{H} : \\langle x, \\varphi_{H} \\rangle = 0 \\}$。这是由 $\\varphi_L$ 张成的子空间。\n-   **投影 Landweber 迭代：** $x^{n+1}_{\\mathrm{proj}} = P_{C}( x^{n}_{\\mathrm{proj}} + \\omega A ( y - A x^{n}_{\\mathrm{proj}} ) )$，初始迭代点 $x^{0}_{\\mathrm{proj}} = 0$，其中 $P_C$ 是到 $C$ 上的正交投影。\n-   **目标：** 计算 $\\Delta E(N) = \\|x^{N} - x^{\\dagger}\\|^{2} - \\|x^{N}_{\\mathrm{proj}} - x^{\\dagger}\\|^{2}$，用 $a_{H}$、$\\omega$、$\\nu$ 和 $N$ 表示。\n\n### 步骤2：使用提取的已知条件进行验证\n该问题在数学上是良定的且自洽的。它描述了逆问题中迭代正则化方法分析的一个标准场景，特别是比较标准 Landweber 迭代与其投影变体。该设置是一个简化但严谨的“玩具模型”，通常用于理解此类算法的行为。所有术语和条件都得到了精确定义。该问题在逆问题领域具有科学依据，是客观且适定的。未发现任何缺陷。\n\n### 步骤3：结论与行动\n问题有效。将提供完整解答。\n\n## 求解推导\n任务的核心是推导在固定迭代次数 $N$ 时，无约束和投影 Landweber 迭代的平方重构误差的闭式表达式，然后计算它们的差值。\n\n我们将处理误差向量 $e^n = x^n - x^\\dagger$ 和 $e^n_{\\mathrm{proj}} = x^n_{\\mathrm{proj}} - x^\\dagger$。在正交归一基 $\\{\\varphi_L, \\varphi_H\\}$ 中处理可以简化问题。任何向量 $x \\in \\text{span}\\{\\varphi_L, \\varphi_H\\}$ 都可以写为 $x = x_L \\varphi_L + x_H \\varphi_H$。算子 $A$ 在此类向量上的作用为 $A x = a_L x_L \\varphi_L + a_H x_H \\varphi_H$。\n\n### 1. 无约束 Landweber 迭代分析\n误差 $e^n = x^n - x^\\dagger$ 根据以下递推关系传播：\n$$e^{n+1} = x^{n+1} - x^\\dagger = \\left(x^n + \\omega A(y - Ax^n)\\right) - x^\\dagger$$\n代入 $y = Ax^\\dagger + \\eta$：\n$$e^{n+1} = x^n + \\omega A(Ax^\\dagger + \\eta - Ax^n) - x^\\dagger = (x^n - x^\\dagger) - \\omega A^2(x^n - x^\\dagger) + \\omega A\\eta$$\n$$e^{n+1} = (I - \\omega A^2)e^n + \\omega A\\eta$$\n初始误差为 $e^0 = x^0 - x^\\dagger = 0 - x^\\dagger = -s\\varphi_L$。\n这是一个关于误差向量 $e^n$ 的线性递推关系。其在第 $N$ 步的解为：\n$$e^N = (I - \\omega A^2)^N e^0 + \\sum_{k=0}^{N-1} (I - \\omega A^2)^k (\\omega A\\eta)$$\n由于算子 $I - \\omega A^2$ 在基 $\\{\\varphi_L, \\varphi_H\\}$ 中是对角的，其特征值为 $(1-\\omega a_L^2)$ 和 $(1-\\omega a_H^2)$，因此我们可以分析误差向量的分量。\n初始误差 $e^0$ 的分量为 $e^0_L = -s$ 和 $e^0_H = 0$。\n噪声项为 $\\omega A\\eta = \\omega A(\\nu\\varphi_H) = \\omega \\nu a_H \\varphi_H$。其分量为 $(\\omega A\\eta)_L = 0$ 和 $(\\omega A\\eta)_H = \\omega \\nu a_H$。\n\n沿 $\\varphi_L$ 方向的误差分量为：\n$$e^N_L = (1 - \\omega a_L^2)^N e^0_L + 0 = -s(1 - \\omega a_L^2)^N$$\n沿 $\\varphi_H$ 方向的误差分量为：\n$$e^N_H = (1 - \\omega a_H^2)^N e^0_H + \\sum_{k=0}^{N-1} (1 - \\omega a_H^2)^k (\\omega \\nu a_H) = \\omega \\nu a_H \\sum_{k=0}^{N-1} (1 - \\omega a_H^2)^k$$\n这是一个等比数列。使用求和公式 $\\sum_{k=0}^{N-1} r^k = \\frac{1-r^N}{1-r}$：\n$$e^N_H = \\omega \\nu a_H \\frac{1 - (1 - \\omega a_H^2)^N}{1 - (1 - \\omega a_H^2)} = \\omega \\nu a_H \\frac{1 - (1 - \\omega a_H^2)^N}{\\omega a_H^2} = \\frac{\\nu}{a_H} \\left(1 - (1 - \\omega a_H^2)^N\\right)$$\n总误差向量为 $e^N = e^N_L \\varphi_L + e^N_H \\varphi_H$。由于正交归一性，平方重构误差为 $\\|e^N\\|^2 = (e^N_L)^2 + (e^N_H)^2$。\n$$\\|x^N - x^\\dagger\\|^2 = \\left(-s(1-\\omega a_L^2)^N\\right)^2 + \\left(\\frac{\\nu}{a_H}\\left(1 - (1 - \\omega a_H^2)^N\\right)\\right)^2$$\n$$\\|x^N - x^\\dagger\\|^2 = s^2(1 - \\omega a_L^2)^{2N} + \\frac{\\nu^2}{a_H^2}\\left(1 - (1 - \\omega a_H^2)^N\\right)^2$$\n\n### 2. 投影 Landweber 迭代分析\n集合 $C$ 是由 $\\varphi_L$ 张成的一维子空间。正交投影 $P_C$ 作用于向量 $x = x_L\\varphi_L + x_H\\varphi_H$ 的结果是 $P_C(x) = x_L\\varphi_L$。\n投影迭代的误差传播为：\n$$e^{n+1}_{\\mathrm{proj}} = x^{n+1}_{\\mathrm{proj}} - x^\\dagger = P_C(x^n_{\\mathrm{proj}} + \\omega A (y - A x^n_{\\mathrm{proj}})) - x^\\dagger$$\n由于 $x^\\dagger = s\\varphi_L \\in C$，我们有 $P_C(x^\\dagger) = x^\\dagger$。我们可以写成 $x^\\dagger = P_C(x^\\dagger)$ 并利用 $P_C$ 的线性性质：\n$$e^{n+1}_{\\mathrm{proj}} = P_C\\left(x^n_{\\mathrm{proj}} - x^\\dagger + \\omega A(Ax^\\dagger + \\eta - Ax^n_{\\mathrm{proj}})\\right) = P_C\\left((I-\\omega A^2)e^n_{\\mathrm{proj}} + \\omega A\\eta\\right)$$\n初始误差为 $e^0_{\\mathrm{proj}} = x^0_{\\mathrm{proj}} - x^\\dagger = 0 - s\\varphi_L = -s\\varphi_L$，它属于 $C$。\n我们用归纳法来证明。假设 $e^n_{\\mathrm{proj}} \\in C$。那么对于某个标量 $\\alpha_n$，有 $e^n_{\\mathrm{proj}} = \\alpha_n \\varphi_L$。\n项 $(I - \\omega A^2)e^n_{\\mathrm{proj}} = (I - \\omega A^2)(\\alpha_n \\varphi_L) = (1 - \\omega a_L^2)\\alpha_n \\varphi_L$ 也属于 $C$。\n噪声项为 $\\omega A\\eta = \\omega \\nu a_H \\varphi_H$，它与 $C$ 正交。\n应用投影算子 $P_C$：\n$$e^{n+1}_{\\mathrm{proj}} = P_C\\left((1 - \\omega a_L^2)e^n_{\\mathrm{proj}} + \\omega \\nu a_H \\varphi_H\\right) = (1 - \\omega a_L^2)e^n_{\\mathrm{proj}} + P_C(\\omega \\nu a_H \\varphi_H)$$\n由于 $\\omega \\nu a_H \\varphi_H$ 与子空间 $C$ 正交，其投影为零。因此：\n$$e^{n+1}_{\\mathrm{proj}} = (1 - \\omega a_L^2)e^n_{\\mathrm{proj}}$$\n这是一个简单的等比递推。从 $e^0_{\\mathrm{proj}} = -s\\varphi_L$ 开始：\n$$e^N_{\\mathrm{proj}} = (1 - \\omega a_L^2)^N e^0_{\\mathrm{proj}} = -s(1 - \\omega a_L^2)^N \\varphi_L$$\n投影方法的平方重构误差为：\n$$\\|x^N_{\\mathrm{proj}} - x^\\dagger\\|^2 = \\|e^N_{\\mathrm{proj}}\\|^2 = \\|-s(1 - \\omega a_L^2)^N \\varphi_L\\|^2 = s^2(1 - \\omega a_L^2)^{2N}$$\n\n### 3. 误差减少量 $\\Delta E(N)$ 的计算\n投影带来的改进是两个平方误差之差：\n$$\\Delta E(N) = \\|x^{N} - x^{\\dagger}\\|^{2} - \\|x^{N}_{\\mathrm{proj}} - x^{\\dagger}\\|^{2}$$\n代入推导出的表达式：\n$$\\Delta E(N) = \\left(s^2(1 - \\omega a_L^2)^{2N} + \\frac{\\nu^2}{a_H^2}\\left(1 - (1 - \\omega a_H^2)^N\\right)^2\\right) - \\left(s^2(1 - \\omega a_L^2)^{2N}\\right)$$\n包含真实信号幅度 $s$ 和低频特征值 $a_L$ 的项相互抵消。这是因为两种方法对信号部分的重构是完全相同的；差异完全在于它们如何处理噪声分量。\n$$\\Delta E(N) = \\frac{\\nu^2}{a_H^2}\\left(1 - (1 - \\omega a_H^2)^N\\right)^2$$\n该表达式量化了误差的减少量，这恰好是投影成功消除的、由放大噪声分量贡献的平方误差。正如所要求的，它只依赖于噪声幅度 $\\nu$、高频特征值 $a_H$、步长 $\\omega$ 和迭代次数 $N$。",
            "answer": "$$\\boxed{\\frac{\\nu^2}{a_H^2} \\left( 1 - \\left(1 - \\omega a_H^2\\right)^N \\right)^2}$$"
        },
        {
            "introduction": "正则化理论不仅指导我们如何设计算法，还精确预测了算法的收敛速度。本练习  将带你跨越理论与实践之间的鸿沟。你将通过编写代码来模拟 Landweber 迭代，并根据数值结果凭经验估算其收敛率，然后将其与理论预测值进行比较，从而有力地验证理论的正确性。",
            "id": "3395632",
            "problem": "考虑一个有限维欧几里得空间中的线性反问题，其算子是一个紧、自伴、半正定的算子，由一个具有指定奇异值衰减的矩阵表示。设正演模型为 $y = A x^\\dagger$，其中 $A \\in \\mathbb{R}^{n \\times n}$ 是可对角化的，其奇异值 $\\sigma_i$ 服从幂律衰减 $\\sigma_i \\asymp i^{-p}$（其中 $p > 0$），真解 $x^\\dagger$ 在奇异值分解 (SVD) 基下的系数满足 $c_i \\asymp i^{-(\\mu + 1/2)}$（其中 $\\mu > 0$）。考虑 Landweber 迭代，其初始值为 $x^{(0)} = 0$，松弛参数 $\\omega$ 为常数且满足 $\\omega \\in (0, 2 / \\|A\\|^2)$，迭代公式定义为 $x^{(k+1)} = x^{(k)} + \\omega A^\\top \\left( y - A x^{(k)} \\right)$，其中整数 $k \\ge 0$。您需要根据指定的谱衰减和源条件，经验性地验证 Landweber 迭代误差关于迭代次数 $k$ 的预测代数收敛速率。\n\n您的程序必须：\n- 为每个测试用例构建一个对角矩阵 $A \\in \\mathbb{R}^{n \\times n}$，其对角元为 $\\sigma_i = i^{-p}$，$i \\in \\{1, 2, \\dots, n\\}$，其中 $n$ 由测试套件指定。\n- 构建 $x^\\dagger \\in \\mathbb{R}^n$，其在与 SVD 右奇异向量重合的标准基下的分量为 $c_i = i^{-(\\mu + 1/2)}$。\n- 生成精确数据 $y = A x^\\dagger$。\n- 实现 Landweber 迭代，其中 $x^{(0)} = 0$ 且 $\\omega = 0.9 / \\|A\\|^2$。注意到由于 $A$ 是对角矩阵，其最大奇异值为 $\\sigma_1 = 1$，因此有 $\\|A\\|^2 = 1$，从而 $\\omega = 0.9$。\n- 对每个测试用例中一组指定的迭代次数 $k$，计算误差范数 $\\|x^{(k)} - x^\\dagger\\|_2$。\n- 通过对给定 $k$ 值集合上的 $\\log \\left( \\|x^{(k)} - x^\\dagger\\|_2 \\right)$ 与 $\\log(k)$ 进行最小二乘线性回归，来估计经验收敛指数 $\\alpha_{\\mathrm{est}}$。将模型解释为 $\\|x^{(k)} - x^\\dagger\\|_2 \\approx C k^{-\\alpha}$，因此回归线的斜率等于 $-\\alpha_{\\mathrm{est}}$。\n- 对于每个测试用例，将经验估计指数 $\\alpha_{\\mathrm{est}}$ 与理论预测值 $\\alpha_{\\mathrm{th}} = \\mu / (2 p)$ 进行比较，该理论值是通过结合 Landweber 迭代下的幂律谱衰减和源条件得出的。\n\n使用以下测试套件，该套件旨在探测一系列病态性和光滑性状况，同时避免退化的参数选择：\n- 测试用例 1 (一般情况)：$n = 16384$，$p = 1.5$，$\\mu = 1.0$，$\\omega = 0.9$，迭代次数 $k \\in \\{50, 100, 200, 400, 800, 1600\\}$。\n- 测试用例 2 (轻度病态，较高光滑性)：$n = 16384$，$p = 0.5$，$\\mu = 1.5$，$\\omega = 0.9$，迭代次数 $k \\in \\{50, 100, 200, 400, 800, 1600\\}$。\n- 测试用例 3 (严重病态，较高光滑性)：$n = 16384$，$p = 2.0$，$\\mu = 2.0$，$\\omega = 0.9$，迭代次数 $k \\in \\{50, 100, 200, 400, 800, 1600\\}$。\n\n为保证数值稳定性和效率，您可以利用 $A$ 的对角结构以及 Landweber 迭代在 SVD 基下的闭式表达式，但不得在代码或分析中使用任何预先假定目标收敛速率的公式；验证必须源于所实现的计算。\n\n对于上述三个测试用例中的每一个，您的程序必须输出一个浮点数，该数等于绝对差 $|\\alpha_{\\mathrm{est}} - \\alpha_{\\mathrm{th}}|$ 并四舍五入到三位小数。将这三个数汇总到一行，形式为方括号内由逗号分隔的列表，且无空格。例如，如果三个绝对差分别为 $d_1$、$d_2$ 和 $d_3$，则输出格式必须为 $[d_1,d_2,d_3]$。\n\n此问题不涉及物理单位。所有角度（如果出现）必须以弧度为单位，但此处未使用任何角度。\n\n您的最终程序必须按指定格式精确地产生一行输出。它不能读取任何输入或写入任何文件。",
            "solution": "该问题要求对一类特定的线性反问题，经验性地验证 Landweber 迭代的理论收敛速率。验证将通过模拟迭代过程、从数值结果中估计收敛速率并将其与理论预测进行比较来完成。\n\n该问题定义在有限维空间 $\\mathbb{R}^n$ 中。正演模型是一个线性方程 $y = A x^\\dagger$，其中 $y \\in \\mathbb{R}^n$ 表示观测数据，$x^\\dagger \\in \\mathbb{R}^n$ 是待恢复的真解，$A \\in \\mathbb{R}^{n \\times n}$ 是正演算子。该问题的构造使得其对应的反问题是病态的，这意味着 $y$ 的微小扰动可能导致解的巨大误差。\n\n算子 $A$ 被指定为一个对角矩阵，其对角元 $\\sigma_i$ 代表其奇异值。这些奇异值由幂律衰减定义：\n$$\n\\sigma_i = i^{-p} \\quad \\text{for } i = 1, 2, \\dots, n\n$$\n其中 $p > 0$ 控制病态程度。由于 $A$ 是一个实对角矩阵，它是自伴的 ($A = A^\\top$)。它的特征值为正，因此它是一个正定算子。\n\n真解 $x^\\dagger$ 具有特定的光滑性，其特征是在标准基（对于对角矩阵 $A$，这也是奇异向量基）中系数 $c_i$ 的衰减：\n$$\nc_i = i^{-(\\mu + 1/2)} \\quad \\text{for } i = 1, 2, \\dots, n\n$$\n其中 $\\mu > 0$ 是一个光滑性参数。较大的 $\\mu$ 值对应于“更光滑”的解。数据 $y$ 是精确的，计算方式为 $y = A x^\\dagger$。由于 $A$ 和 $x^\\dagger$ 都在同一基中按分量定义，这可以简化为 $y_i = \\sigma_i c_i$。\n\nLandweber 迭代是求解此类反问题的一种迭代正则化方法。它由以下递推关系定义：\n$$\nx^{(k+1)} = x^{(k)} + \\omega A^\\top \\left( y - A x^{(k)} \\right)\n$$\n初始猜测为 $x^{(0)} = 0$。参数 $\\omega$ 是一个松弛或步长参数，必须满足 $\\omega \\in (0, 2/\\|A\\|^2)$ 以保证收敛。对于给定的算子 $A$，算子范数（最大奇异值）为 $\\|A\\| = \\sigma_1 = 1^{-p} = 1$。问题指定 $\\omega = 0.9 / \\|A\\|^2 = 0.9$。\n\n对于对角算子 $A$，迭代过程解耦为 $n$ 个独立的标量递归，每个分量 $x_i^{(k)}$ 一个：\n$$\nx_i^{(k+1)} = x_i^{(k)} + \\omega \\sigma_i (y_i - \\sigma_i x_i^{(k)}) = (1 - \\omega \\sigma_i^2) x_i^{(k)} + \\omega \\sigma_i y_i\n$$\n在 $y_i = \\sigma_i c_i$ 和初始条件 $x_i^{(0)} = 0$ 的情况下，这个线性递推有闭式解：\n$$\nx_i^{(k)} = c_i \\left( 1 - (1 - \\omega \\sigma_i^2)^k \\right)\n$$\n这个表达式允许在任何步骤 $k$ 直接计算迭代解 $x^{(k)}$，而无需执行完整的迭代过程。误差向量是 $x^{(k)} - x^\\dagger$，其第 $i$ 个分量是 $x_i^{(k)} - c_i = -c_i (1 - \\omega \\sigma_i^2)^k$。因此，误差的 L2 范数的平方为：\n$$\n\\|x^{(k)} - x^\\dagger\\|_2^2 = \\sum_{i=1}^{n} (x_i^{(k)} - c_i)^2 = \\sum_{i=1}^{n} c_i^2 \\left( (1 - \\omega \\sigma_i^2)^k \\right)^2 = \\sum_{i=1}^{n} c_i^2 (1 - \\omega \\sigma_i^2)^{2k}\n$$\n程序为每个指定的迭代次数 $k$ 计算这个误差范数。\n\n为了估计收敛速率，我们假设误差遵循代数衰减定律：$\\|x^{(k)} - x^\\dagger\\|_2 \\approx C k^{-\\alpha}$，其中 $C$ 为常数，$\\alpha > 0$ 为指数。对两边取自然对数可将此关系线性化：\n$$\n\\log\\left(\\|x^{(k)} - x^\\dagger\\|_2\\right) \\approx \\log(C) - \\alpha \\log(k)\n$$\n该方程显示了 $Y = \\log\\left(\\|x^{(k)} - x^\\dagger\\|_2\\right)$ 和 $X = \\log(k)$ 之间的线性关系，斜率为 $-\\alpha$。经验指数 $\\alpha_{\\mathrm{est}}$ 是通过对给定迭代次数 $k_j$ 的点集 $\\{(\\log(k_j), \\log(\\|x^{(k_j)} - x^\\dagger\\|_2))\\}$ 进行最小二乘线性回归来确定的。估计的指数是拟合直线斜率的相反数。\n\n对于应用于谱衰减为 $\\sigma_i \\asymp i^{-p}$ 且解的光滑性（源条件）为 $x^\\dagger_i \\asymp i^{-(\\mu+1/2)}$ 的问题，正则化理论预测了误差范数的收敛速率。预测的指数为：\n$$\n\\alpha_{\\mathrm{th}} = \\frac{\\mu}{2p}\n$$\n每个测试用例的最后一步是计算绝对差 $|\\alpha_{\\mathrm{est}} - \\alpha_{\\mathrm{th}}|$，以量化经验测量值与理论预测值之间的一致性。\n\n程序实现了这整个过程。对于每个测试用例，它构建对应于 $\\sigma_i$ 和 $c_i$ 的向量，使用闭式表达式计算指定 $k$ 值下的误差范数，执行对数-对数线性回归以找到 $\\alpha_{\\mathrm{est}}$，计算 $\\alpha_{\\mathrm{th}}$，并输出它们的绝对差（四舍五入到三位小数）。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and verify Landweber iteration convergence rates.\n    \"\"\"\n\n    # Test suite as defined in the problem statement.\n    # Each tuple is (n, p, mu, omega, k_values)\n    test_cases = [\n        (16384, 1.5, 1.0, 0.9, np.array([50, 100, 200, 400, 800, 1600])),\n        (16384, 0.5, 1.5, 0.9, np.array([50, 100, 200, 400, 800, 1600])),\n        (16384, 2.0, 2.0, 0.9, np.array([50, 100, 200, 400, 800, 1600])),\n    ]\n\n    results = []\n    for n, p, mu, omega, k_values in test_cases:\n        # Construct the problem in the SVD basis (canonical basis here).\n        # The operator A is diagonal with entries sigma_i.\n        # The true solution x_dagger has entries c_i.\n        i = np.arange(1, n + 1, dtype=np.float64)\n        sigma = i**(-p)\n        c = i**(-(mu + 0.5))\n\n        # Compute the error norm ||x^(k) - x_dagger||_2 for each k.\n        # This is done using the closed-form expression for the error in the SVD basis.\n        # Error_i(k) = -c_i * (1 - omega * sigma_i^2)^k\n        # ||Error(k)||_2 = sqrt(sum_i (Error_i(k))^2)\n        error_norms = []\n        for k in k_values:\n            # The term (1 - omega * sigma^2) is raised to the power of k.\n            # This represents the decay of the error components.\n            # err_vec_k = c_i * (1 - omega * sigma_i^2)^k\n            err_vec_k = c * np.power(1.0 - omega * sigma**2, k)\n            \n            # The L2 norm is calculated using numpy.linalg.norm.\n            norm_k = np.linalg.norm(err_vec_k)\n            error_norms.append(norm_k)\n\n        # Estimate the empirical convergence exponent alpha_est.\n        # This is done by fitting a line to log(error) vs log(k).\n        # The model is: log(error) approx log(C) - alpha * log(k).\n        # The slope of the line is -alpha.\n        log_k = np.log(k_values)\n        log_error = np.log(np.array(error_norms))\n\n        # np.polyfit(x, y, 1) returns [slope, intercept] for the best-fit line.\n        slope, _ = np.polyfit(log_k, log_error, 1)\n        alpha_est = -slope\n        \n        # Calculate the theoretical convergence exponent.\n        alpha_th = mu / (2.0 * p)\n\n        # Calculate the absolute difference and round to three decimal places.\n        difference = abs(alpha_est - alpha_th)\n        results.append(round(difference, 3))\n\n    # Format the final output as a comma-separated list in brackets.\n    output_string = \",\".join(map(str, results))\n    print(f\"[{output_string}]\")\n\nsolve()\n```"
        }
    ]
}