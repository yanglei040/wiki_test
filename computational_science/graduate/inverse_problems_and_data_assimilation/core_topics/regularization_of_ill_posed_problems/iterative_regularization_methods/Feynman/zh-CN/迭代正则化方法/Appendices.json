{
    "hands_on_practices": [
        {
            "introduction": "在处理含噪声数据之前，理解迭代方法在理想无噪声情景下的基本机理至关重要。本练习将聚焦于通过选择最佳的固定步长来优化原型算法——Landweber 迭代法的性能 。通过推导最优步长，您将深入理解迭代的收敛速度如何与前向算子的奇异值谱相关联，为后续更复杂的分析奠定理论基石。",
            "id": "3392775",
            "problem": "考虑一个定义在有限维实希尔伯特空间上的线性逆问题，其精确数据模型为 $y = A x^{\\dagger}$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 是一个有界线性算子，$x^{\\dagger} \\in \\mathbb{R}^{n}$ 是未知量。令 $L = \\|A\\|$ 表示由欧几里得范数诱导的 $A$ 的算子范数。假设初始估计 $x_{0}$ 位于 $A$ 的零空间的正交补中，即 $x_{0} \\in \\mathcal{N}(A)^{\\perp}$，因此迭代在 $A$ 具有严格正奇异值的子空间上进行。进一步假设 $A$ 的非零奇异值由一个已知常数 $\\underline{\\sigma} > 0$ 作为下界，因此所有相关奇异值满足 $\\sigma_{i} \\in [\\underline{\\sigma}, L]$。\n\n考虑使用恒定步长 $\\omega > 0$ 的 Landweber 迭代，\n$$\nx_{k+1} = x_{k} + \\omega A^{\\ast} \\big( y - A x_{k} \\big),\n$$\n其中 $A^{\\ast}$ 表示 $A$ 的转置（伴随）。令误差为 $e_{k} = x_{k} - x^{\\dagger}$，并在 $A$ 的奇异向量基中衡量收敛性。\n\n从奇异值分解和诱导算子范数的基本定义出发，推导迭代过程中误差分量的演化。仅使用界 $\\underline{\\sigma}$ 和 $L$，确定能够最大化渐近线性收敛率的恒定步长 $\\omega^{\\star}$，这里的最大化是指最小化所有奇异分量上的最坏情况单次迭代收缩。然后，在步长为 $\\omega^{\\star}$ 的情况下，量化奇异向量基中每个奇异分量所对应的单次迭代收缩因子。\n\n你的最终答案必须是一个单一的解析表达式，并包括：\n- 用 $\\underline{\\sigma}$ 和 $L$ 表示的最优恒定步长 $\\omega^{\\star}$。\n- 在 $\\omega^{\\star}$ 下，奇异值为 $\\sigma \\in [\\underline{\\sigma}, L]$ 的奇异分量的单次迭代收缩因子。\n\n无需数值近似或四舍五入。",
            "solution": "该问题是有效的。这是一个关于 Landweber 迭代收敛性分析的适定的、有科学依据的逆问题领域的问题。所有必要信息均已提供，术语标准且无歧义。\n\n我们首先分析 Landweber 迭代的误差传播。迭代由下式给出\n$$\nx_{k+1} = x_{k} + \\omega A^{\\ast} ( y - A x_{k} ),\n$$\n其中 $y = A x^{\\dagger}$ 是精确数据，$x^{\\dagger}$ 是真实解，$A^{\\ast}$ 是 $A$ 的伴随（转置），$\\omega > 0$ 是一个恒定步长。第 $k$ 次迭代的误差定义为 $e_k = x_k - x^{\\dagger}$。\n\n为了推导误差的演化，我们在迭代方程两边同时减去 $x^{\\dagger}$：\n$$\nx_{k+1} - x^{\\dagger} = x_{k} - x^{\\dagger} + \\omega A^{\\ast} ( y - A x_{k} ).\n$$\n代入误差 $e_{k+1}$ 和 $e_k$ 的定义以及数据模型 $y = A x^{\\dagger}$，我们得到：\n$$\ne_{k+1} = e_k + \\omega A^{\\ast} ( A x^{\\dagger} - A x_{k} ) = e_k - \\omega A^{\\ast} A (x_k - x^{\\dagger}) = e_k - \\omega A^{\\ast} A e_k.\n$$\n这可以写成线性误差更新方程：\n$$\ne_{k+1} = (I - \\omega A^{\\ast} A) e_k,\n$$\n其中 $I$ 是单位算子。\n\n为了分析收敛性，我们研究迭代算子 $R(\\omega) = I - \\omega A^{\\ast} A$ 的谱性质。最好在 $A$ 的奇异值分解（SVD）基中进行分析。设 $A$ 的 SVD 为 $A = U \\Sigma V^{\\ast}$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是包含奇异值 $\\sigma_i \\ge 0$ 的对角矩阵。$V$ 的列，记为 $v_i$，是 $A$ 的右奇异向量，并构成 $\\mathbb{R}^n$ 的一个标准正交基。\n\n算子 $A^{\\ast}A$ 可以用 SVD 表示为：\n$$\nA^{\\ast}A = (V \\Sigma^{\\ast} U^{\\ast}) (U \\Sigma V^{\\ast}) = V \\Sigma^{\\ast} \\Sigma V^{\\ast}.\n$$\n矩阵 $\\Sigma^{\\ast} \\Sigma$ 是一个 $n \\times n$ 的对角矩阵，其对角元素为 $\\sigma_i^2$。右奇异向量 $v_i$ 是 $A^{\\ast}A$ 的特征向量：\n$$\nA^{\\ast} A v_i = (V \\Sigma^{\\ast} \\Sigma V^{\\ast}) v_i = \\sigma_i^2 v_i.\n$$\n因此，奇异向量 $v_i$ 也是误差传播算子 $R(\\omega)$ 的特征向量：\n$$\nR(\\omega) v_i = (I - \\omega A^{\\ast} A) v_i = v_i - \\omega (\\sigma_i^2 v_i) = (1 - \\omega \\sigma_i^2) v_i.\n$$\n$R(\\omega)$ 对应的特征值为 $\\lambda_i = 1 - \\omega \\sigma_i^2$。\n\n问题陈述迭代被限制在 $\\mathcal{N}(A)^{\\perp}$ 上，这是由对应非零奇异值 $\\sigma_i > 0$ 的右奇异向量 $v_i$ 生成的空间。设误差在此基上展开：$e_k = \\sum_{i} c_{k,i} v_i$。系数的演化由下式给出：\n$$\ne_{k+1} = \\sum_{i} c_{k+1,i} v_i = R(\\omega) e_k = \\sum_{i} c_{k,i} R(\\omega) v_i = \\sum_{i} c_{k,i} (1 - \\omega \\sigma_i^2) v_i.\n$$\n因此，沿着每个奇异向量 $v_i$ 的误差分量在每次迭代中被其对应的特征值缩放：$c_{k+1,i} = (1 - \\omega \\sigma_i^2) c_{k,i}$。第 $i$ 个分量的单次迭代收缩因子为 $\\rho_i = |1 - \\omega \\sigma_i^2|$。\n\n为使迭代对所有分量都收敛，我们需要对所有相关的 $i$ 都有 $|\\rho_i|  1$。这意味着对于给定范围 $[\\underline{\\sigma}, L]$ 内的所有奇异值 $\\sigma$，我们需要 $|1 - \\omega \\sigma^2|  1$，其中 $L = \\|A\\| = \\max_i \\sigma_i$ 且 $\\underline{\\sigma} > 0$ 是非零奇异值的下界。此条件等价于 $-1  1 - \\omega \\sigma^2  1$，化简为 $0  \\omega \\sigma^2  2$，即 $0  \\omega  \\frac{2}{\\sigma^2}$。为使该条件对所有 $\\sigma \\in [\\underline{\\sigma}, L]$ 都成立，我们必须满足最严格的情况，即 $\\sigma = L$ 的情况。因此，我们必须有 $0  \\omega  \\frac{2}{L^2}$。\n\n目标是找到最优恒定步长 $\\omega^{\\star}$，以最大化渐近收敛率。这等价于最小化所有可能模式（即所有 $\\sigma \\in [\\underline{\\sigma}, L]$）上的最坏情况（最大）收缩因子。我们必须解决以下极小化极大问题：\n$$\n\\omega^{\\star} = \\arg\\min_{\\omega > 0} \\left( \\max_{\\sigma \\in [\\underline{\\sigma}, L]} |1 - \\omega \\sigma^2| \\right).\n$$\n令 $s = \\sigma^2$。问题变为找到 $\\omega^{\\star}$ 以最小化函数 $g(\\omega) = \\max_{s \\in [\\underline{\\sigma}^2, L^2]} |1 - \\omega s|$。函数 $f(s) = 1 - \\omega s$ 是一条斜率为负的直线。其绝对值 $|f(s)|$ 在区间 $[\\underline{\\sigma}^2, L^2]$ 上的最大值必然出现在某个端点上。因此，\n$$\ng(\\omega) = \\max \\{ |1 - \\omega \\underline{\\sigma}^2|, |1 - \\omega L^2| \\}.\n$$\n当两个端点处的值大小相等时，$g(\\omega)$ 达到最小值：\n$$\n|1 - \\omega \\underline{\\sigma}^2| = |1 - \\omega L^2|.\n$$\n由于 $L > \\underline{\\sigma}$ 且 $\\omega > 0$，我们有 $1 - \\omega \\underline{\\sigma}^2 > 1 - \\omega L^2$。要使它们的大小相等，必须有其中一个值是另一个的相反数：\n$$\n1 - \\omega \\underline{\\sigma}^2 = -(1 - \\omega L^2) = \\omega L^2 - 1.\n$$\n解出 $\\omega$：\n$$\n2 = \\omega L^2 + \\omega \\underline{\\sigma}^2 = \\omega(L^2 + \\underline{\\sigma}^2).\n$$\n这就得出了最优步长：\n$$\n\\omega^{\\star} = \\frac{2}{L^2 + \\underline{\\sigma}^2}.\n$$\n有了这个最优步长，我们可以确定奇异值为 $\\sigma \\in [\\underline{\\sigma}, L]$ 的奇异分量的单次迭代收缩因子。我们将这个因子记为 $\\rho(\\sigma)$，其表达式为：\n$$\n\\rho(\\sigma) = |1 - \\omega^{\\star} \\sigma^2| = \\left| 1 - \\frac{2\\sigma^2}{L^2 + \\underline{\\sigma}^2} \\right|.\n$$\n当使用最优恒定步长 $\\omega^{\\star}$ 时，该表达式给出了指定范围内任意模式 $\\sigma$ 的收缩因子。最坏情况的收缩因子出现在边界 $\\sigma = \\underline{\\sigma}$ 和 $\\sigma = L$ 处，其值为 $\\frac{L^2 - \\underline{\\sigma}^2}{L^2 + \\underline{\\sigma}^2}$。\n\n所要求的两个量是最优步长 $\\omega^{\\star}$ 和一般收缩因子 $\\rho(\\sigma)$。",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2}{L^2 + \\underline{\\sigma}^2}  \\left| 1 - \\frac{2 \\sigma^2}{L^2 + \\underline{\\sigma}^2} \\right| \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "在现实应用中，数据总是被噪声所污染，若不加控制，迭代算法最终会“过拟合”于噪声，从而破坏解的质量。本实践将引导您完成一个数值实验，亲眼见证这一被称为“半收敛”的现象 。通过实现差异原则 (discrepancy principle) 这一停止准则，您将学会如何在恰当的时机终止迭代以获得稳定且有意义的解，这正是迭代正则化的精髓所在。",
            "id": "3392768",
            "problem": "考虑有限维空间中带加性噪声的线性反问题。令 $A \\in \\mathbb{R}^{n \\times n}$ 为一个方形病态矩阵，$x^\\dagger \\in \\mathbb{R}^n$ 为精确解，$y = A x^\\dagger$ 为无噪声数据。含噪数据为 $y^\\delta = y + e^\\delta$，其中 $\\lVert e^\\delta \\rVert_2 = \\delta$。您将研究两种迭代方法的行为：(i) 带有差异原则停止准则的 Landweber 迭代，以及 (ii) 持续进行大量固定次数迭代的无约束梯度下降法。您的目标是构建一个对抗性噪声场景，并量化基于差异原则的停止准则相对于无约束梯度下降法如何避免过拟合。\n\n基本原理和定义：\n- A 的奇异值分解 (SVD) 为 $A = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{n \\times n}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma = \\mathrm{diag}(\\sigma_1,\\ldots,\\sigma_n)$ 且满足 $\\sigma_1 \\ge \\cdots \\ge \\sigma_n > 0$。\n- 为最小化数据失配 $\\frac{1}{2} \\lVert A x - y^\\delta \\rVert_2^2$，步长为 $\\omega \\in (0, 2/\\|A\\|_2^2)$ 的 Landweber 迭代为 $x_{k+1}^\\delta = x_k^\\delta + \\omega A^\\top (y^\\delta - A x_k^\\delta)$，初始值为 $x_0^\\delta = 0$。\n- 带有参数 $\\tau > 1$ 的差异原则在第一个满足 $\\lVert A x_{k_\\ast}^\\delta - y^\\delta \\rVert_2 \\le \\tau \\delta$ 的索引 $k_\\ast$ 处停止。\n- 此处的无约束梯度下降法与 Landweber 更新相同，但会持续进行预设的大量迭代次数 $K$，而不使用 $\\delta$。\n\n对抗性噪声的构建：\n- 您将通过 SVD 模型构建具有预设奇异值衰减的矩阵 $A$：$A = U \\Sigma V^\\top$，其中 $U$ 和 $V$ 是随机正交矩阵，$\\Sigma = \\mathrm{diag}(\\sigma_i)$ 且 $\\sigma_i = i^{-p}$，指数 $p > 0$。\n- 您将通过在右奇异向量基中指定系数 $(\\alpha_i)_{i=1}^n$（其中 $\\alpha_i = i^{-q}$，指数 $q > 0$）来构建 $x^\\dagger$，然后进行归一化使得 $\\lVert x^\\dagger \\rVert_2 = 1$，即 $x^\\dagger = V \\alpha / \\lVert \\alpha \\rVert_2$。\n- 您将选择与最小奇异值对应的左奇异向量对齐的对抗性噪声：$e^\\delta = \\delta u_n$，其中 $u_n$ 是 $U$ 的第 $n$ 列。这使得 $\\lVert e^\\delta \\rVert_2 = \\delta$，并将噪声集中在被放大得最厉害的解分量上。\n\n每个测试用例需要实现的数值任务：\n1. 按照描述构建 $A = U \\Sigma V^\\top$，其中 $U$ 和 $V$ 通过对使用固定种子的随机高斯矩阵进行正交规范化得到。\n2. 按照描述构建 $x^\\dagger$ 和 $y = A x^\\dagger$。\n3. 构建对抗性噪声 $e^\\delta = \\delta u_n$ 和含噪数据 $y^\\delta = y + e^\\delta$。\n4. 选择 Landweber 步长 $\\omega = \\frac{1.9}{\\sigma_1^2}$，其中 $\\sigma_1$ 是 $\\Sigma$ 中的最大奇异值。\n5. 运行带有差异原则停止准则的 Landweber 迭代：在第一个满足 $\\lVert A x_{k_\\ast}^\\delta - y^\\delta \\rVert_2 \\le \\tau \\delta$ 的索引 $k_\\ast$ 处停止，并设置一个足够大的迭代次数安全上限以确保迭代能够终止。\n6. 将相同的迭代运行固定的、大量的迭代次数 $K$（无约束梯度下降法）。\n7. 计算相对重构误差 $\\varepsilon_{\\mathrm{reg}} = \\lVert x_{k_\\ast}^\\delta - x^\\dagger \\rVert_2 / \\lVert x^\\dagger \\rVert_2$ 和 $\\varepsilon_{\\mathrm{unc}} = \\lVert x_{K}^\\delta - x^\\dagger \\rVert_2 / \\lVert x^\\dagger \\rVert_2$，并报告其比率 $r = \\varepsilon_{\\mathrm{unc}} / \\varepsilon_{\\mathrm{reg}}$。\n\n测试套件：\n对于下面的每个元组 $(n, p, q, \\delta, \\tau, \\mathrm{seed}, K)$，执行上述步骤并返回比率 $r$。\n\n- 情况 A（理想情况，中等病态性）：$(n, p, q, \\delta, \\tau, \\mathrm{seed}, K) = (50, 1.5, 1.0, 10^{-3}, 1.1, 0, 20000)$。\n- 情况 B（严重病态性）：$(50, 2.5, 1.0, 10^{-3}, 1.1, 1, 40000)$。\n- 情况 C（较大噪声水平和较温和的衰减）：$(80, 1.2, 0.5, 10^{-2}, 1.05, 2, 15000)$。\n- 情况 D（非常严重的病态性，小噪声）：$(50, 3.0, 1.5, 10^{-4}, 1.2, 3, 60000)$。\n\n答案规格和输出格式：\n- 对于每种情况，结果都是一个浮点数 $r$。\n- 您的程序必须生成单行输出，其中包含四个比率的列表 $[r_A, r_B, r_C, r_D]$，以逗号分隔并用方括号括起。每个比率必须四舍五入到六位小数。\n- 不涉及物理单位。所有角度（如果有）在此处均不相关。\n\n您的程序必须是自包含的，并生成精确指定的最终输出格式，不得包含任何额外文本。",
            "solution": "用户提供的问题是有效的。这是一个在反问题领域中良构的数值实验，旨在阐释迭代正则化的概念和过拟合现象。该问题在科学上基于线性反问题和数值优化的理论，所有参数和程序都有清晰、客观的定义。\n\n该问题研究了形式为 $y^\\delta = A x^\\dagger + e^\\delta$ 的线性反问题，其目标是从含噪数据 $y^\\delta \\in \\mathbb{R}^n$ 中恢复真实解 $x^\\dagger \\in \\mathbb{R}^n$。矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 是病态的，意味着其奇异值迅速衰减。这种病态性是反问题的标志，其中数据的微小扰动可能导致朴素解的巨大误差。噪声项 $e^\\delta$ 的范数已知，为 $\\lVert e^\\delta \\rVert_2 = \\delta$。\n\n我们使用 $A$ 的奇异值分解 (SVD)，$A = U \\Sigma V^\\top$，作为分析的基本工具。这里，$U = [u_1, \\dots, u_n]$ 和 $V = [v_1, \\dots, v_n]$ 是正交矩阵，其列分别是左、右奇异向量。矩阵 $\\Sigma = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_n)$ 包含按非递增顺序排列的奇异值：$\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n > 0$。$A$ 的病态性意味着比率 $\\sigma_1/\\sigma_n$ 很大。一个朴素解 $x_{\\mathrm{naive}} = A^{-1} y^\\delta$ 会放大噪声。使用 SVD，该解为 $x_{\\mathrm{naive}} = V \\Sigma^{-1} U^\\top y^\\delta$。在奇异值 $\\sigma_n$ 最小的 $u_n$ 方向上，噪声分量会被放大 $1/\\sigma_n$ 倍。该问题构建了一个对抗性噪声 $e^\\delta = \\delta u_n$，正是为了最大化这种效应。\n\n该问题比较了两种求解 $x$ 的迭代方法。两者都基于 Landweber 迭代，这是一种应用于最小化数据失配泛函 $J(x) = \\frac{1}{2} \\lVert Ax - y^\\delta \\rVert_2^2$ 的梯度下降法。迭代公式如下：\n$x_{k+1}^\\delta = x_k^\\delta - \\omega \\nabla J(x_k^\\delta) = x_k^\\delta + \\omega A^\\top (y^\\delta - A x_k^\\delta)$\n从 $x_0^\\delta = 0$ 开始。为确保收敛，步长 $\\omega$ 必须选择在 $(0, 2/\\|A\\|_2^2) = (0, 2/\\sigma_1^2)$ 范围内。问题指定 $\\omega = 1.9/\\sigma_1^2$。\n\n在 SVD 基下，Landweber 迭代可以表示为 $x_k^\\delta = \\sum_{i=1}^n c_{k,i} v_i$。其系数演化规律如下：\n$c_{k,i} = \\frac{\\langle y^\\delta, u_i \\rangle}{\\sigma_i} \\left(1 - (1 - \\omega \\sigma_i^2)^k\\right)$\n当迭代次数 $k \\to \\infty$ 时，滤波因子 $(1 - (1 - \\omega \\sigma_i^2)^k)$ 趋近于 1。解的系数 $c_{k,i}$ 收敛到 $\\langle y^\\delta, u_i \\rangle / \\sigma_i$，这正是朴素解 $A^{-1} y^\\delta$ 的系数。对于分量 $i=n$，噪声 $e^\\delta = \\delta u_n$ 导致 $\\langle y^\\delta, u_n \\rangle = \\langle y, u_n \\rangle + \\delta$。系数 $c_{k,n}$ 收敛到 $\\frac{\\langle y, u_n \\rangle}{\\sigma_n} + \\frac{\\delta}{\\sigma_n}$。由于 $\\sigma_n$ 很小，$\\delta/\\sigma_n$ 项会变得非常巨大，导致灾难性的噪声放大。这就是在“无约束梯度下降”情况下发生的事情，该情况下迭代运行了大量的固定步数 $K$，导致其对噪声产生“过拟合”。因此，其结果误差 $\\varepsilon_{\\mathrm{unc}}$ 预计会很大。\n\n第二种方法使用 Landweber 迭代，但结合了一种称为差异原则的停止准则。该原则是一种正则化形式，它规定一旦迭代的数据失配程度与噪声水平相当，就应停止迭代。具体来说，我们在第一个满足以下条件的迭代次数 $k_\\ast$ 处停止：\n$\\lVert A x_{k_\\ast}^\\delta - y^\\delta \\rVert_2 \\le \\tau \\delta$\n其中 $\\tau > 1$ 是一个安全因子。这个停止准则防止了迭代进行足够长的时间以显著放大高频噪声分量。对于小的 $\\sigma_i$（即高频分量），由于 $k_\\ast$ 相对较小，滤波因子 $(1 - (1 - \\omega \\sigma_i^2)^{k_\\ast})$ 会接近于 0。这抑制了噪声分量，特别是对应于 $u_n$ 的分量，从而得到一个稳定、正则化的解 $x_{k_\\ast}^\\delta$，其误差 $\\varepsilon_{\\mathrm{reg}}$ 要小得多。\n\n数值任务是为四个不同的参数集实现这一场景。对于每种情况，我们根据基于 SVD 的模型构建矩阵 $A$、真实解 $x^\\dagger$ 和含噪数据 $y^\\delta$。然后我们运行两种迭代方案：一种由差异原则停止，另一种运行固定的、大量的迭代次数 $K$。最后，我们计算相对重构误差 $\\varepsilon_{\\mathrm{reg}}$ 和 $\\varepsilon_{\\mathrm{unc}}$，以及它们的比率 $r = \\varepsilon_{\\mathrm{unc}} / \\varepsilon_{\\mathrm{reg}}$，该比率量化了使用适当正则化策略的好处。预计该比率将显著大于 1。\n\n实现将按以下步骤进行：\n1. 定义一个函数来处理单个测试用例，该函数以参数 $(n, p, q, \\delta, \\tau, \\mathrm{seed}, K)$ 作为输入。\n2. 在该函数内部，设置随机数生成器种子以确保可复现性。\n3. 通过对随机高斯矩阵应用 QR 分解，生成大小为 $n \\times n$ 的随机正交矩阵 $U$ 和 $V$。\n4. 构建奇异值对角矩阵 $\\Sigma$，其元素为 $\\sigma_i = i^{-p}$，$i \\in \\{1, \\dots, n\\}$。\n5. 将矩阵 $A$ 组装为 $A = U \\Sigma V^\\top$。\n6. 从系数 $\\alpha_i = i^{-q}$ 构建真实解 $x^\\dagger$，并将其归一化以使 $\\lVert x^\\dagger \\rVert_2 = 1$，然后转换到标准基：$x^\\dagger = V (\\alpha / \\lVert \\alpha \\rVert_2)$。无噪声数据为 $y = A x^\\dagger$。\n7. 创建对抗性噪声 $e^\\delta = \\delta u_n$，其中 $u_n$ 是 $U$ 的最后一列。含噪数据变为 $y^\\delta = y + e^\\delta$。\n8. Landweber 步长设置为 $\\omega = 1.9 / \\sigma_1^2$。\n9. 运行带有差异原则的 Landweber 迭代。从 $x_0^\\delta = 0$ 开始，迭代进行直到 $\\lVert A x_k^\\delta - y^\\delta \\rVert_2 \\le \\tau \\delta$。得到的解是 $x_{\\mathrm{reg}} = x_{k_\\ast}^\\delta$。\n10. 将无约束 Landweber 迭代运行固定的 $K$ 步，得到解 $x_{\\mathrm{unc}} = x_K^\\delta$。\n11. 计算相对误差 $\\varepsilon_{\\mathrm{reg}} = \\lVert x_{\\mathrm{reg}} - x^\\dagger \\rVert_2 / \\lVert x^\\dagger \\rVert_2$ 和 $\\varepsilon_{\\mathrm{unc}} = \\lVert x_{\\mathrm{unc}} - x^\\dagger \\rVert_2 / \\lVert x^\\dagger \\rVert_2$。\n12. 计算并返回比率 $r = \\varepsilon_{\\mathrm{unc}} / \\varepsilon_{\\mathrm{reg}}$。\n此过程将对所有四个测试用例重复执行，并按要求格式化最终的比率列表。",
            "answer": "```python\nimport numpy as np\n\ndef run_case(n, p, q, delta, tau, seed, K):\n    \"\"\"\n    Solves a single test case for the iterative regularization problem.\n    \"\"\"\n    # 1. Build A = U Sigma V^T\n    np.random.seed(seed)\n    \n    # Generate random orthogonal matrices U and V\n    H1 = np.random.randn(n, n)\n    U, _ = np.linalg.qr(H1)\n    \n    H2 = np.random.randn(n, n)\n    V, _ = np.linalg.qr(H2)\n    \n    # Generate singular values and Sigma matrix\n    i_vals = np.arange(1, n + 1)\n    sigma_vals = i_vals**(-p)\n    Sigma = np.diag(sigma_vals)\n    \n    A = U @ Sigma @ V.T\n\n    # 2. Construct x_dagger and y\n    alpha_coeffs = i_vals**(-q)\n    alpha_norm_factor = np.linalg.norm(alpha_coeffs)\n    alpha_normalized = alpha_coeffs / alpha_norm_factor\n    \n    x_dagger = V @ alpha_normalized\n    y = A @ x_dagger\n\n    # 3. Construct adversarial noise and noisy data\n    u_n = U[:, -1]\n    e_delta = delta * u_n\n    y_delta = y + e_delta\n\n    # 4. Choose Landweber step size\n    sigma_1 = sigma_vals[0]\n    omega = 1.9 / (sigma_1**2)\n\n    # 5. Run Landweber with discrepancy principle\n    x_k_reg = np.zeros(n)\n    # Use a large enough safety cap for iterations\n    max_iter_reg = max(100000, 2 * K)\n    \n    for _ in range(max_iter_reg):\n        residual = y_delta - A @ x_k_reg\n        residual_norm = np.linalg.norm(residual)\n        \n        if residual_norm = tau * delta:\n            break\n        \n        x_k_reg = x_k_reg + omega * (A.T @ residual)\n    else:\n        # This part should not be reached if the problem is well-posed.\n        # It indicates failure of the discrepancy principle to stop.\n        # Assign NaN or raise an error to signal failure.\n        x_k_reg = np.full(n, np.nan)\n    \n    x_reg = x_k_reg\n\n    # 6. Run unconstrained Landweber for K iterations\n    x_k_unc = np.zeros(n)\n    for _ in range(K):\n        residual = y_delta - A @ x_k_unc\n        x_k_unc = x_k_unc + omega * (A.T @ residual)\n        \n    x_unc = x_k_unc\n\n    # 7. Compute errors and ratio\n    norm_x_dagger = np.linalg.norm(x_dagger) # Should be 1.0 by construction\n    \n    err_reg = np.linalg.norm(x_reg - x_dagger)\n    err_unc = np.linalg.norm(x_unc - x_dagger)\n    \n    eps_reg = err_reg / norm_x_dagger\n    eps_unc = err_unc / norm_x_dagger\n    \n    ratio = eps_unc / eps_reg\n    return ratio\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (n, p, q, delta, tau, seed, K)\n        (50, 1.5, 1.0, 10**-3, 1.1, 0, 20000),  # Case A\n        (50, 2.5, 1.0, 10**-3, 1.1, 1, 40000),  # Case B\n        (80, 1.2, 0.5, 10**-2, 1.05, 2, 15000), # Case C\n        (50, 3.0, 1.5, 10**-4, 1.2, 3, 60000),  # Case D\n    ]\n\n    results = []\n    for case in test_cases:\n        n, p, q, delta, tau, seed, K = case\n        result = run_case(n, p, q, delta, tau, seed, K)\n        results.append(result)\n\n    # Format output as required\n    formatted_results = [f'{r:.6f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```"
        }
    ]
}