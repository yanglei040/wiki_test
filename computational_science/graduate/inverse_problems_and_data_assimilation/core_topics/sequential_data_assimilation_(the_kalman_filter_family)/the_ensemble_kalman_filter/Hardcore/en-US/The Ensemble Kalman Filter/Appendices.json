{
    "hands_on_practices": [
        {
            "introduction": "To truly understand the Ensemble Kalman Filter, we must first master its core computational steps. This exercise  breaks down the analysis update into its essential components, starting from a given forecast ensemble. By manually computing the sample mean, covariance, and the Kalman gain for a small, well-defined system, you will build a concrete understanding of how ensemble statistics translate into a state update.",
            "id": "3425297",
            "problem": "Consider the linear-Gaussian data assimilation setting used in the Ensemble Kalman Filter (EnKF), where the state lives in $\\mathbb{R}^{2}$ and the observation space is $\\mathbb{R}^{1}$. You are given an ensemble of $N=4$ state vectors\n$$\nx^{(1)}=\\begin{pmatrix}2\\\\0\\end{pmatrix},\\quad\nx^{(2)}=\\begin{pmatrix}0\\\\2\\end{pmatrix},\\quad\nx^{(3)}=\\begin{pmatrix}2\\\\4\\end{pmatrix},\\quad\nx^{(4)}=\\begin{pmatrix}4\\\\2\\end{pmatrix},\n$$\nand a linear observation operator\n$$\nH=\\begin{pmatrix}1  1\\end{pmatrix}\\in\\mathbb{R}^{1\\times 2}.\n$$\nAssume the observation error $\\eta$ is additive, independent of the state, and Gaussian, with zero mean and covariance $R\\in\\mathbb{R}^{1\\times 1}$ given by $R=\\frac{4}{3}$.\n\nUsing only foundational definitions for the sample mean, anomalies matrix, sample covariance, and linear-Gaussian update, carry out the following:\n\n1. Compute the sample mean $\\bar{x}\\in\\mathbb{R}^{2}$ and the anomalies matrix $A\\in\\mathbb{R}^{2\\times 4}$ whose $i$-th column is $x^{(i)}-\\bar{x}$.\n2. Compute the sample covariance $P\\in\\mathbb{R}^{2\\times 2}$ defined by $P=\\frac{1}{N-1}AA^{\\top}$.\n3. Compute the state–observation cross-covariance $P^{xy}\\in\\mathbb{R}^{2\\times 1}$ defined by $P^{xy}=\\frac{1}{N-1}AA^{\\top}H^{\\top}$.\n4. In the linear-Gaussian model $y=Hx+\\eta$ with $\\eta\\sim\\mathcal{N}(0,R)$, derive the innovation covariance $S\\in\\mathbb{R}^{1\\times 1}$ and the Kalman gain $K\\in\\mathbb{R}^{2\\times 1}$ using the standard linear-Gaussian Bayesian update reasoning from first principles. Then compute $S$ and $K$ for the given $H$, $P$, and $R$.\n\nYour final answer must be the first component of the Kalman gain vector $K$, expressed as a single exact real number. Do not round and do not include any units in your final answer.",
            "solution": "The problem statement is validated as scientifically grounded, well-posed, objective, self-contained, and consistent. It presents a standard computational exercise within the framework of the Ensemble Kalman Filter, a well-established method in data assimilation. All necessary data, definitions, and constraints are provided, and there are no contradictions or ambiguities. The problem is therefore deemed valid, and a solution is provided below.\n\nThe solution proceeds by following the four steps outlined in the problem statement.\n\n1. Computation of the sample mean $\\bar{x}$ and the anomalies matrix $A$.\nThe sample mean $\\bar{x} \\in \\mathbb{R}^{2}$ is the average of the $N=4$ ensemble members $x^{(i)}$:\n$$\n\\bar{x} = \\frac{1}{N} \\sum_{i=1}^{N} x^{(i)} = \\frac{1}{4} \\left( x^{(1)} + x^{(2)} + x^{(3)} + x^{(4)} \\right)\n$$\nSubstituting the given ensemble vectors:\n$$\n\\bar{x} = \\frac{1}{4} \\left( \\begin{pmatrix}2\\\\0\\end{pmatrix} + \\begin{pmatrix}0\\\\2\\end{pmatrix} + \\begin{pmatrix}2\\\\4\\end{pmatrix} + \\begin{pmatrix}4\\\\2\\end{pmatrix} \\right) = \\frac{1}{4} \\begin{pmatrix}2+0+2+4\\\\0+2+4+2\\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix}8\\\\8\\end{pmatrix} = \\begin{pmatrix}2\\\\2\\end{pmatrix}.\n$$\nThe anomalies matrix $A \\in \\mathbb{R}^{2\\times 4}$ contains the anomaly vectors $a^{(i)} = x^{(i)} - \\bar{x}$ as its columns.\n$$\na^{(1)} = x^{(1)} - \\bar{x} = \\begin{pmatrix}2\\\\0\\end{pmatrix} - \\begin{pmatrix}2\\\\2\\end{pmatrix} = \\begin{pmatrix}0\\\\-2\\end{pmatrix}\n$$\n$$\na^{(2)} = x^{(2)} - \\bar{x} = \\begin{pmatrix}0\\\\2\\end{pmatrix} - \\begin{pmatrix}2\\\\2\\end{pmatrix} = \\begin{pmatrix}-2\\\\0\\end{pmatrix}\n$$\n$$\na^{(3)} = x^{(3)} - \\bar{x} = \\begin{pmatrix}2\\\\4\\end{pmatrix} - \\begin{pmatrix}2\\\\2\\end{pmatrix} = \\begin{pmatrix}0\\\\2\\end{pmatrix}\n$$\n$$\na^{(4)} = x^{(4)} - \\bar{x} = \\begin{pmatrix}4\\\\2\\end{pmatrix} - \\begin{pmatrix}2\\\\2\\end{pmatrix} = \\begin{pmatrix}2\\\\0\\end{pmatrix}\n$$\nAssembling these column vectors into the matrix $A$:\n$$\nA = \\begin{pmatrix} 0  -2  0  2 \\\\ -2  0  2  0 \\end{pmatrix}.\n$$\n\n2. Computation of the sample covariance $P$.\nThe sample covariance matrix $P \\in \\mathbb{R}^{2\\times 2}$ is defined as $P=\\frac{1}{N-1}AA^{\\top}$. With $N=4$, this becomes $P=\\frac{1}{3}AA^{\\top}$.\nFirst, we compute the product $AA^{\\top}$:\n$$\nA A^{\\top} = \\begin{pmatrix} 0  -2  0  2 \\\\ -2  0  2  0 \\end{pmatrix} \\begin{pmatrix} 0  -2 \\\\ -2  0 \\\\ 0  2 \\\\ 2  0 \\end{pmatrix}\n$$\nThe components of the resulting $2 \\times 2$ matrix are:\n$$\n(AA^{\\top})_{11} = (0)(0) + (-2)(-2) + (0)(0) + (2)(2) = 0 + 4 + 0 + 4 = 8\n$$\n$$\n(AA^{\\top})_{12} = (0)(-2) + (-2)(0) + (0)(2) + (2)(0) = 0 + 0 + 0 + 0 = 0\n$$\n$$\n(AA^{\\top})_{21} = (-2)(0) + (0)(-2) + (2)(0) + (0)(0) = 0 + 0 + 0 + 0 = 0\n$$\n$$\n(AA^{\\top})_{22} = (-2)(-2) + (0)(0) + (2)(2) + (0)(0) = 4 + 0 + 4 + 0 = 8\n$$\nSo,\n$$\nAA^{\\top} = \\begin{pmatrix} 8  0 \\\\ 0  8 \\end{pmatrix}.\n$$\nNow, we compute $P$:\n$$\nP = \\frac{1}{3} \\begin{pmatrix} 8  0 \\\\ 0  8 \\end{pmatrix} = \\begin{pmatrix} \\frac{8}{3}  0 \\\\ 0  \\frac{8}{3} \\end{pmatrix}.\n$$\n\n3. Computation of the state–observation cross-covariance $P^{xy}$.\nThe state–observation cross-covariance $P^{xy} \\in \\mathbb{R}^{2\\times 1}$ is defined as $P^{xy}=\\frac{1}{N-1}AA^{\\top}H^{\\top}$. This is equivalent to $P^{xy} = PH^{\\top}$.\nGiven the observation operator $H = \\begin{pmatrix} 1  1 \\end{pmatrix}$, its transpose is $H^{\\top} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nWe can now compute $P^{xy}$:\n$$\nP^{xy} = PH^{\\top} = \\begin{pmatrix} \\frac{8}{3}  0 \\\\ 0  \\frac{8}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} (\\frac{8}{3})(1) + (0)(1) \\\\ (0)(1) + (\\frac{8}{3})(1) \\end{pmatrix} = \\begin{pmatrix} \\frac{8}{3} \\\\ \\frac{8}{3} \\end{pmatrix}.\n$$\n\n4. Derivation and computation of the innovation covariance $S$ and the Kalman gain $K$.\nWe derive the expressions for $S$ and $K$ from first principles in the linear-Gaussian setting. The state $x$ is a random variable with mean $E[x] = \\bar{x}$ and covariance $\\text{cov}(x) = P$. The observation model is $y = Hx + \\eta$, where the observation error $\\eta$ is a random variable with $E[\\eta]=0$ and $\\text{cov}(\\eta)=R$, and $\\eta$ is independent of $x$.\n\nThe innovation covariance $S \\in \\mathbb{R}^{1\\times 1}$ is the covariance of the observation, $S = \\text{cov}(y)$.\n$$\nS = \\text{cov}(Hx + \\eta)\n$$\nSince $x$ and $\\eta$ are independent, the covariance of their sum is the sum of their covariances:\n$$\nS = \\text{cov}(Hx) + \\text{cov}(\\eta)\n$$\nUsing the property of covariance for a linear transformation, $\\text{cov}(Ax) = A \\text{cov}(x) A^{\\top}$, we have $\\text{cov}(Hx) = H \\text{cov}(x) H^{\\top} = HPH^{\\top}$. The covariance of the error is given as $\\text{cov}(\\eta)=R$.\nTherefore, the innovation covariance is:\n$$\nS = HPH^{\\top} + R.\n$$\nThe Kalman gain $K \\in \\mathbb{R}^{2\\times 1}$ is the linear operator that provides the optimal update of the state estimate. It is given by $K = P^{xy}S^{-1}$, where $P^{xy}$ is the cross-covariance between the state $x$ and the observation $y$. Let's derive $P^{xy}$:\n$$\nP^{xy} = \\text{cov}(x, y) = \\text{cov}(x, Hx + \\eta)\n$$\nBy the linearity of covariance:\n$$\nP^{xy} = \\text{cov}(x, Hx) + \\text{cov}(x, \\eta)\n$$\nSince $x$ and $\\eta$ are independent, $\\text{cov}(x, \\eta) = 0$. Using the property $\\text{cov}(x, Ax) = \\text{cov}(x,x)A^{\\top}$, we get:\n$$\nP^{xy} = \\text{cov}(x, x)H^{\\top} = PH^{\\top}.\n$$\nThis confirms the consistency of the provided definition for $P^{xy}$ and the first-principles derivation.\nCombining these results, the Kalman gain is:\n$$\nK = PH^{\\top} (HPH^{\\top} + R)^{-1}.\n$$\nNow we compute the numerical values for $S$ and $K$.\nFirst, compute $HPH^{\\top}$:\n$$\nHPH^{\\top} = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} \\frac{8}{3}  0 \\\\ 0  \\frac{8}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{8}{3}  \\frac{8}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{8}{3} + \\frac{8}{3} = \\frac{16}{3}.\n$$\nNext, compute $S = HPH^{\\top} + R$. Given $R = \\frac{4}{3}$:\n$$\nS = \\frac{16}{3} + \\frac{4}{3} = \\frac{20}{3}.\n$$\nSince $S$ is a scalar ($1 \\times 1$ matrix), its inverse is its reciprocal:\n$$\nS^{-1} = \\left(\\frac{20}{3}\\right)^{-1} = \\frac{3}{20}.\n$$\nFinally, we compute the Kalman gain $K = P^{xy}S^{-1} = PH^{\\top}S^{-1}$:\n$$\nK = \\begin{pmatrix} \\frac{8}{3} \\\\ \\frac{8}{3} \\end{pmatrix} \\left(\\frac{3}{20}\\right) = \\begin{pmatrix} \\frac{8}{3} \\cdot \\frac{3}{20} \\\\ \\frac{8}{3} \\cdot \\frac{3}{20} \\end{pmatrix} = \\begin{pmatrix} \\frac{8}{20} \\\\ \\frac{8}{20} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{5} \\\\ \\frac{2}{5} \\end{pmatrix}.\n$$\nThe problem asks for the first component of the Kalman gain vector $K$.\nThe first component is $K_1 = \\frac{2}{5}$.",
            "answer": "$$\\boxed{\\frac{2}{5}}$$"
        },
        {
            "introduction": "In practical applications with high-dimensional, chaotic models, the raw Ensemble Kalman Filter often suffers from issues like filter divergence due to sampling error. This hands-on practice  introduces two essential modifications used in virtually all operational systems: multiplicative covariance inflation and localization. By implementing an algorithm that incorporates these features for the Lorenz-63 system, you will see precisely how they are used to modulate the forecast error covariance and improve filter performance.",
            "id": "3425319",
            "problem": "You are given the Lorenz-63 dynamical system with state $x = (x,y,z)^\\top \\in \\mathbb{R}^3$ and parameters $\\sigma>0$, $\\rho>0$, $\\beta>0$,\n$$\n\\dot{x} = \\sigma (y - x), \\quad\n\\dot{y} = x(\\rho - z) - y, \\quad\n\\dot{z} = xy - \\beta z,\n$$\ndiscretized in time with step $\\Delta t > 0$. At a given assimilation time, you have a forecast ensemble $\\{x^{f,(j)} \\in \\mathbb{R}^3\\}_{j=1}^N$ that approximates the forecast distribution of the state after one model step. There is a single noisy observation of the first component $x$ with observation operator $H = [1,\\,0,\\,0]$ and scalar observation error variance $R>0$. You are to design an Ensemble Kalman Filter (EnKF) that uses multiplicative inflation and covariance localization to compute the innovation variance for one assimilation step from the given forecast ensemble.\n\nAssume the following modeling principles:\n- The model forecast is deterministic over the single step, so the only source of randomness at analysis time is the ensemble spread and the observation noise.\n- The innovation is the difference between the observation and the model-predicted observation based on the forecast ensemble.\n- The EnKF uses the ensemble to approximate the forecast-error covariance. Multiplicative inflation with parameter $\\lambda \\ge 0$ is applied by scaling forecast ensemble anomalies by a factor $1+\\lambda$ before forming the covariance.\n- Covariance localization is applied by a Schur (Hadamard) product with a localization correlation matrix $C \\in \\mathbb{R}^{3 \\times 3}$. Use a Gaussian taper defined by\n$$\nC_{ij} \\;=\\; \\begin{cases}\n\\exp\\!\\left(-\\tfrac{1}{2}\\left(\\frac{d_{ij}}{L}\\right)^2\\right),  \\text{if } L>0,\\\\\n1,  \\text{if } i=j,\\\\\n0,  \\text{if } i \\ne j, \\text{ and } L=0\n\\end{cases}\n$$\nwhere $d_{ij} = |i-j|$ for state component indices $i,j \\in \\{1,2,3\\}$ corresponding to $(x,y,z)$ respectively, and $L \\ge 0$ is the localization radius (in the same unitless index-distance metric).\n\nStarting from these principles, derive and implement an algorithm to compute the innovation variance for one assimilation step based solely on the provided forecast ensemble, inflation parameter $\\lambda$, localization radius $L$, and observation error variance $R$. The innovation variance must reflect the combined uncertainty from the localized, inflated ensemble-derived forecast covariance and the observation error variance.\n\nDefinitions and constraints for computation:\n- Let the forecast ensemble matrix be $X^f = [x^{f,(1)}, \\dots, x^{f,(N)}] \\in \\mathbb{R}^{3 \\times N}$, its mean be $\\bar{x}^f = \\frac{1}{N}\\sum_{j=1}^N x^{f,(j)} \\in \\mathbb{R}^3$, and its anomalies be $A^f = X^f - \\bar{x}^f \\mathbf{1}^\\top \\in \\mathbb{R}^{3 \\times N}$, where $\\mathbf{1} \\in \\mathbb{R}^{N}$ is the vector of ones. Use the unbiased sample covariance with factor $\\frac{1}{N-1}$.\n- Apply multiplicative inflation by replacing anomalies with $(1+\\lambda)A^f$ before forming the covariance. Then apply covariance localization via Schur product with $C$.\n- Use the linearized-observation and Gaussian-error framework to obtain the innovation variance from the localized, inflated covariance and the observation error variance $R$.\n\nYour program must compute the innovation variance for the following test suite of forecast ensembles and parameters. Each forecast ensemble is given as a $3 \\times N$ matrix whose columns are ensemble members $(x,y,z)^\\top$:\n\n- Test case $1$ (happy path):\n  - Forecast ensemble $X^f \\in \\mathbb{R}^{3 \\times 6}$:\n    $$\n    X^f = \\begin{bmatrix}\n    1.00  1.20  0.80  1.10  0.90  1.05 \\\\\n    1.00  0.90  1.10  1.05  0.95  1.00 \\\\\n    1.00  1.10  0.90  1.05  0.95  0.98\n    \\end{bmatrix}\n    $$\n  - $R = 0.04$, $L = 1.0$, $\\lambda = 0.10$.\n\n- Test case $2$ (no inflation):\n  - Forecast ensemble $X^f \\in \\mathbb{R}^{3 \\times 6}$ is identical to test case $1$.\n  - $R = 0.01$, $L = 0.5$, $\\lambda = 0.0$.\n\n- Test case $3$ (degenerate zero-spread ensemble):\n  - Forecast ensemble $X^f \\in \\mathbb{R}^{3 \\times 5}$:\n    $$\n    X^f = \\begin{bmatrix}\n    2.00  2.00  2.00  2.00  2.00 \\\\\n    -1.00  -1.00  -1.00  -1.00  -1.00 \\\\\n    3.00  3.00  3.00  3.00  3.00\n    \\end{bmatrix}\n    $$\n  - $R = 0.20$, $L = 2.0$, $\\lambda = 0.50$.\n\n- Test case $4$ (localization radius at boundary):\n  - Forecast ensemble $X^f \\in \\mathbb{R}^{3 \\times 4}$:\n    $$\n    X^f = \\begin{bmatrix}\n    -1.00  -1.10  -0.90  -1.05 \\\\\n    0.50  0.45  0.55  0.50 \\\\\n    2.00  2.10  1.90  2.05\n    \\end{bmatrix}\n    $$\n  - $R = 0.50$, $L = 0.0$, $\\lambda = 0.20$.\n\nYour program should:\n- Implement the computation of the innovation variance from the given inputs for each test case, strictly following the design principles above.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of test cases $1$ through $4$, with each entry rounded to $8$ decimal places (for example, $[0.12345678,0.00000000,1.23450000,0.01020304]$).\n\nNo physical units, angles, or percentages are involved in this problem. All numerical values are unitless scalars in $\\mathbb{R}$.",
            "solution": "The problem requires the derivation and implementation of an algorithm to compute the innovation variance for a single assimilation step of an Ensemble Kalman Filter (EnKF). The filter incorporates multiplicative inflation and covariance localization. The state space is $\\mathbb{R}^3$, and the observation consists of the first state component only.\n\nThe derivation proceeds from first principles as laid out in the problem statement.\n\nThe innovation variance, denoted as $S$, is the variance of the innovation term $d = y^o - H \\bar{x}^f$, where $y^o$ is the scalar observation, $H$ is the observation operator, and $\\bar{x}^f$ is the mean of the forecast ensemble. In the standard linear-Gaussian framework underlying the Kalman filter, the innovation covariance is given by the formula:\n$$\nS = H P^f H^\\top + R_{obs}\n$$\nHere, $P^f$ is the forecast error covariance matrix and $R_{obs}$ is the observation error covariance matrix.\n\nAccording to the problem statement:\n- The observation is a scalar measurement of the first state component. The observation operator is therefore a row vector $H = [1, 0, 0]$.\n- The observation error variance is a scalar $R > 0$, so the observation error covariance matrix is simply $R_{obs} = R$.\n\nSubstituting $H$ into the formula for $S$ yields:\n$$\nS = \\begin{bmatrix} 1  0  0 \\end{bmatrix} P^f \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} + R\n$$\nThis matrix multiplication extracts the top-left element, $(P^f)_{11}$, of the forecast error covariance matrix. Thus, the innovation variance is:\n$$\nS = (P^f)_{11} + R\n$$\nThe core of the problem is to determine $(P^f)_{11}$ based on the specified procedure for constructing $P^f$. The procedure involves three steps: forming the sample covariance from ensemble anomalies, applying multiplicative inflation, and then applying covariance localization.\n\n1.  **Ensemble Sample Covariance:** The unbiased sample covariance matrix is computed from the ensemble anomalies $A^f = X^f - \\bar{x}^f \\mathbf{1}^\\top \\in \\mathbb{R}^{3 \\times N}$, where $N$ is the number of ensemble members. The uninflated sample covariance is $\\frac{1}{N-1} A^f (A^f)^\\top$.\n\n2.  **Multiplicative Inflation:** Multiplicative inflation is applied by scaling the anomalies by a factor of $(1+\\lambda)$, where $\\lambda \\ge 0$ is the inflation parameter. The inflated anomalies are $A^{f, \\text{inf}} = (1+\\lambda)A^f$. The inflated sample covariance matrix, let's call it $P^f_{\\text{inf}}$, is then:\n$$\nP^f_{\\text{inf}} = \\frac{1}{N-1} A^{f, \\text{inf}} (A^{f, \\text{inf}})^\\top = \\frac{(1+\\lambda)^2}{N-1} A^f (A^f)^\\top\n$$\n\n3.  **Covariance Localization:** The final forecast covariance matrix $P^f$ is obtained by applying localization to $P^f_{\\text{inf}}$. This is done via a Schur (element-wise) product with a localization matrix $C \\in \\mathbb{R}^{3 \\times 3}$:\n$$\nP^f = C \\circ P^f_{\\text{inf}}\n$$\nThis means that each element of the final covariance matrix is $(P^f)_{ij} = C_{ij} \\cdot (P^f_{\\text{inf}})_{ij}$.\n\nWe are interested in the specific element $(P^f)_{11}$. From the Schur product definition:\n$$\n(P^f)_{11} = C_{11} \\cdot (P^f_{\\text{inf}})_{11}\n$$\nThe localization matrix $C$ is defined based on a Gaussian taper. For any localization radius $L > 0$, its elements are $C_{ij} = \\exp(-\\frac{1}{2}(\\frac{d_{ij}}{L})^2)$, where $d_{ij} = |i-j|$. For the diagonal element $C_{11}$, the distance is $d_{11} = |1-1| = 0$. Therefore:\n$$\nC_{11} = \\exp(0) = 1\n$$\nFor the boundary case $L=0$, the problem defines $C_{ij} = 1$ if $i=j$ and $0$ otherwise. So, $C_{11}=1$ in this case as well. Consequently, for all $L \\ge 0$, the diagonal element $C_{11}$ is always $1$. This is a fundamental property of localization functions, which are designed to be correlation functions and thus have ones on the diagonal, preserving the variances of the state variables.\n\nThis implies that localization has no effect on the diagonal elements of the covariance matrix. So, $(P^f)_{11} = (P^f_{\\text{inf}})_{11}$.\n\nNow we evaluate $(P^f_{\\text{inf}})_{11}$. This is the top-left element of the inflated sample covariance matrix. Let $X^f_1$ denote the first row of the ensemble matrix $X^f$, which contains the ensemble values for the first state component, $x$. The term $\\frac{1}{N-1} A^f (A^f)^\\top$ is the sample covariance matrix. Its $(1,1)$ element is the unbiased sample variance of the first state component. Let us denote this as $\\text{var}(X^f_1)$.\n$$\n(P^f_{\\text{inf}})_{11} = (1+\\lambda)^2 \\cdot \\text{var}(X^f_1) = (1+\\lambda)^2 \\left( \\frac{1}{N-1} \\sum_{j=1}^N (x^{f,(j)}_1 - \\bar{x}^f_1)^2 \\right)\n$$\nwhere $x^{f,(j)}_1$ is the first component of the $j$-th ensemble member and $\\bar{x}^f_1$ is the mean of these components.\n\nCombining these results, the final expression for the innovation variance $S$ is:\n$$\nS = (1+\\lambda)^2 \\cdot \\text{var}(X^f_1) + R\n$$\nThis formula shows that for an observation of a single state variable, the innovation variance depends only on the ensemble variance of that specific variable (inflated) and the observation error variance. The localization radius $L$ and the ensemble spread of the other state variables ($y$ and $z$) do not affect the result. The algorithm is to apply this formula to each test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the innovation variance for a series of EnKF test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"Xf\": np.array([\n                [1.00, 1.20, 0.80, 1.10, 0.90, 1.05],\n                [1.00, 0.90, 1.10, 1.05, 0.95, 1.00],\n                [1.00, 1.10, 0.90, 1.05, 0.95, 0.98]\n            ]),\n            \"R\": 0.04,\n            \"L\": 1.0,\n            \"lambda_\": 0.10\n        },\n        {\n            \"Xf\": np.array([\n                [1.00, 1.20, 0.80, 1.10, 0.90, 1.05],\n                [1.00, 0.90, 1.10, 1.05, 0.95, 1.00],\n                [1.00, 1.10, 0.90, 1.05, 0.95, 0.98]\n            ]),\n            \"R\": 0.01,\n            \"L\": 0.5,\n            \"lambda_\": 0.0\n        },\n        {\n            \"Xf\": np.array([\n                [2.00, 2.00, 2.00, 2.00, 2.00],\n                [-1.00, -1.00, -1.00, -1.00, -1.00],\n                [3.00, 3.00, 3.00, 3.00, 3.00]\n            ]),\n            \"R\": 0.20,\n            \"L\": 2.0,\n            \"lambda_\": 0.50\n        },\n        {\n            \"Xf\": np.array([\n                [-1.00, -1.10, -0.90, -1.05],\n                [0.50, 0.45, 0.55, 0.50],\n                [2.00, 2.10, 1.90, 2.05]\n            ]),\n            \"R\": 0.50,\n            \"L\": 0.0,\n            \"lambda_\": 0.20\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        Xf = case[\"Xf\"]\n        R = case[\"R\"]\n        lambda_ = case[\"lambda_\"]\n\n        # The observation operator H = [1, 0, 0] means we only need the\n        # variance of the first state component.\n        x_component_ensemble = Xf[0, :]\n\n        # Calculate the unbiased sample variance of the first component.\n        # The factor 1/(N-1) is achieved with ddof=1 (delta degrees of freedom).\n        forecast_variance = np.var(x_component_ensemble, ddof=1)\n\n        # The forecast error variance from the ensemble is inflated.\n        # This corresponds to P^f_{11}, as localization does not affect\n        # the diagonal elements of the covariance matrix.\n        inflated_forecast_variance = ((1 + lambda_)**2) * forecast_variance\n\n        # The innovation variance S = H P^f H^T + R, which simplifies to\n        # S = P^f_{11} + R for H=[1,0,0].\n        innovation_variance = inflated_forecast_variance + R\n        \n        results.append(f\"{innovation_variance:.8f}\")\n\n    # Print the final result in the specified format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The effectiveness of the EnKF is fundamentally limited by the subspace spanned by its ensemble members. This exercise  transitions from computation to a deeper conceptual analysis of this limitation, particularly in the context of sparse observations. By deriving the minimum ensemble size required to represent all unobserved directions in the state space—the nullspace of the observation operator $H$—you will uncover the critical theoretical relationship between ensemble size, observational coverage, and the filter's potential to correct the state.",
            "id": "3425309",
            "problem": "Consider a linear inverse problem with state vector $x \\in \\mathbb{R}^{d}$ and observations $y \\in \\mathbb{R}^{r}$ given by $y = H x + v$, where the observation operator $H \\in \\mathbb{R}^{r \\times d}$ has rank $r$ with $r  d$, and the observation noise $v$ is Gaussian with zero mean and covariance $R \\in \\mathbb{R}^{r \\times r}$ that is symmetric positive definite. An Ensemble Kalman Filter (EnKF) is used with $N$ ensemble members $\\{x^{f,(i)}\\}_{i=1}^{N}$ drawn from a full-rank Gaussian prior $\\mathcal{N}(m^{f}, B)$ with $B \\in \\mathbb{R}^{d \\times d}$ symmetric positive definite. Let the ensemble mean be $\\bar{x}^{f} = \\frac{1}{N} \\sum_{i=1}^{N} x^{f,(i)}$ and the anomaly matrix be $A \\in \\mathbb{R}^{d \\times N}$ with columns $a^{(i)} = x^{f,(i)} - \\bar{x}^{f}$, so that $\\sum_{i=1}^{N} a^{(i)} = 0$. The sample forecast covariance is $P^{f} = \\frac{1}{N-1} A A^{\\top}$.\n\nAssume a single linear analysis step of the EnKF is performed with a deterministic Kalman gain and identical observations $y$ used across ensemble members. You may use only the following fundamental facts: (i) the analysis increment of the EnKF is a linear map of the forecast anomalies through the sample covariance $P^{f}$ and the observation operator $H$, (ii) the range of $P^{f}$ equals the span of the columns of $A$, and (iii) the nullspace $\\mathcal{N}(H) = \\{ z \\in \\mathbb{R}^{d} : H z = 0 \\}$ has dimension $d - r$. No other shortcut formulas may be invoked.\n\nPartial observations with rank-deficient $H$ leave directions in $\\mathcal{N}(H)$ unobserved. For the EnKF analysis to be able to represent and adjust any unobserved direction in $\\mathcal{N}(H)$ in one analysis step, the ensemble anomaly subspace must span $\\mathcal{N}(H)$. Derive from first principles the minimal ensemble size $N_{\\mathrm{min}}$ (as a function of $d$ and $r$) such that, under a construction of the ensemble anomalies that includes $d - r$ linearly independent vectors in $\\mathcal{N}(H)$ and satisfies the mean-zero constraint on anomalies, the span of the anomalies contains $\\mathcal{N}(H)$ and hence the EnKF analysis increments can represent all unobserved directions.\n\nProvide your final answer as a closed-form expression for $N_{\\mathrm{min}}$ in terms of $d$ and $r$. No rounding is required, and no units are involved.",
            "solution": "The problem asks for the minimal ensemble size $N_{\\mathrm{min}}$ required for the Ensemble Kalman Filter (EnKF) analysis update to be capable of representing any direction in the nullspace of the observation operator $H$. This capability is contingent on the subspace spanned by the ensemble anomalies containing the nullspace of $H$.\n\nLet the set of $N$ forecast ensemble anomalies be $\\{a^{(1)}, a^{(2)}, \\dots, a^{(N)}\\}$, where each $a^{(i)} \\in \\mathbb{R}^{d}$. By definition, these anomalies satisfy the mean-zero constraint:\n$$ \\sum_{i=1}^{N} a^{(i)} = 0 $$\nLet $S$ be the subspace spanned by these anomalies, commonly referred to as the ensemble subspace:\n$$ S = \\text{span}\\{a^{(1)}, a^{(2)}, \\dots, a^{(N)}\\} $$\nThe problem states that the analysis increments for the EnKF are confined to this subspace $S$. The directions in the state space that are not constrained by the observations $y$ are the vectors in the nullspace of the observation operator $H$, denoted by $\\mathcal{N}(H)$. For the EnKF analysis to be able to make corrections in any of these unobserved directions, the ensemble subspace $S$ must contain the nullspace $\\mathcal{N}(H)$. This gives the fundamental requirement:\n$$ \\mathcal{N}(H) \\subseteq S $$\nThis set inclusion implies a relationship between the dimensions of these two subspaces:\n$$ \\dim(\\mathcal{N}(H)) \\le \\dim(S) $$\nWe will now determine the dimensions of $\\mathcal{N}(H)$ and $S$.\n\nFirst, we determine the dimension of the nullspace, $\\dim(\\mathcal{N}(H))$. The observation operator $H$ is a linear map from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{r}$. The rank-nullity theorem states that $\\text{rank}(H) + \\dim(\\mathcal{N}(H)) = d$. The problem specifies that $\\text{rank}(H) = r$. Therefore:\n$$ r + \\dim(\\mathcal{N}(H)) = d $$\n$$ \\dim(\\mathcal{N}(H)) = d - r $$\n\nNext, we determine an upper bound for the dimension of the ensemble subspace, $\\dim(S)$. The $N$ anomaly vectors that span $S$ are not linearly independent due to the mean-zero constraint $\\sum_{i=1}^{N} a^{(i)} = 0$. This constraint means that any one anomaly vector can be expressed as a linear combination of the others, for example, $a^{(N)} = -\\sum_{i=1}^{N-1} a^{(i)}$. Consequently, the set of $N$ anomalies can span a subspace of dimension at most $N-1$. Thus, we have the inequality:\n$$ \\dim(S) \\le N-1 $$\n\nCombining our findings, we have a chain of inequalities:\n$$ d-r = \\dim(\\mathcal{N}(H)) \\le \\dim(S) \\le N-1 $$\nFrom this, we derive a necessary condition on the ensemble size $N$:\n$$ d-r \\le N-1 $$\n$$ N \\ge d-r+1 $$\nThis establishes that the minimal ensemble size, $N_{\\mathrm{min}}$, must be at least $d-r+1$.\n\nTo show that this lower bound is achievable, we must demonstrate that an ensemble of size $N = d-r+1$ can be constructed to satisfy all the problem's conditions. The conditions are:\n1.  The mean-zero constraint is satisfied.\n2.  The ensemble anomalies $\\{a^{(i)}\\}$ include a subset of $d-r$ linearly independent vectors in $\\mathcal{N}(H)$.\n3.  The span of the anomalies, $S$, contains $\\mathcal{N}(H)$.\n\nLet us construct such an ensemble with $N = d-r+1$ members.\nLet $\\{v_1, v_2, \\dots, v_{d-r}\\}$ be a basis for the nullspace $\\mathcal{N}(H)$. These are $d-r$ linearly independent vectors.\nWe define the first $d-r$ anomalies to be these basis vectors:\n$$ a^{(i)} = v_i \\quad \\text{for } i = 1, 2, \\dots, d-r $$\nThis satisfies condition (2) by construction.\n\nTo satisfy the mean-zero constraint (condition 1), the sum of all $N = d-r+1$ anomalies must be the zero vector. We define the final anomaly, $a^{(d-r+1)}$, to ensure this:\n$$ a^{(d-r+1)} = -\\sum_{i=1}^{d-r} a^{(i)} = -\\sum_{i=1}^{d-r} v_i $$\nWith this definition, the sum of all anomalies is:\n$$ \\sum_{i=1}^{d-r+1} a^{(i)} = \\left(\\sum_{i=1}^{d-r} v_i\\right) + \\left(-\\sum_{i=1}^{d-r} v_i\\right) = 0 $$\nThe mean-zero constraint is satisfied.\n\nFinally, we check if the span of our constructed anomalies contains $\\mathcal{N}(H)$ (condition 3). The ensemble subspace is:\n$$ S = \\text{span}\\{ a^{(1)}, \\dots, a^{(d-r)}, a^{(d-r+1)} \\} = \\text{span}\\{ v_1, \\dots, v_{d-r}, -\\sum_{i=1}^{d-r} v_i \\} $$\nSince the last vector in the set, $-\\sum_{i=1}^{d-r} v_i$, is a linear combination of the first $d-r$ vectors, it does not extend the span. Therefore, the span is determined solely by the basis vectors of $\\mathcal{N}(H)$:\n$$ S = \\text{span}\\{v_1, v_2, \\dots, v_{d-r}\\} = \\mathcal{N}(H) $$\nThis shows that $\\mathcal{N}(H) \\subseteq S$ is satisfied (in this case, with equality).\n\nWe have shown that an ensemble of size $N = d-r+1$ can be constructed to meet all requirements. Since we also proved that any ensemble satisfying the requirements must have a size of at least $d-r+1$, we conclude that the minimal ensemble size is precisely this value.\n$$ N_{\\mathrm{min}} = d-r+1 $$",
            "answer": "$$\n\\boxed{d - r + 1}\n$$"
        }
    ]
}