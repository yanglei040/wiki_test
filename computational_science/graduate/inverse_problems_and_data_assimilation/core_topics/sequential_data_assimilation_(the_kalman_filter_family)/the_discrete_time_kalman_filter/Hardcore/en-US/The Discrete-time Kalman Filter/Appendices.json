{
    "hands_on_practices": [
        {
            "introduction": "Real-world dynamic systems, like a vehicle in motion, evolve continuously in time. However, our digital sensors and processors operate in discrete time steps. This exercise provides a crucial bridge between the continuous-time physics of a system and the discrete-time formulation required by the Kalman filter. You will practice deriving the discrete process noise covariance matrix, $Q_d$, from a continuous-time noise model, a fundamental skill for correctly configuring a filter for physical state estimation .",
            "id": "2753297",
            "problem": "A one-dimensional point mass is modeled for state estimation using the discrete-time Kalman filter (KF). The continuous-time kinematics are defined by the position $p(t)$ and velocity $v(t)$ with dynamics\n$$\\frac{d}{dt}\\begin{pmatrix} p(t) \\\\ v(t) \\end{pmatrix} = \\underbrace{\\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix}}_{F} \\begin{pmatrix} p(t) \\\\ v(t) \\end{pmatrix} + \\underbrace{\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}}_{L} a(t),$$\nwhere $a(t)$ is a zero-mean, scalar, white Gaussian acceleration process with constant one-sided power spectral density $q$ in $\\mathrm{m}^2\\mathrm{s}^{-3}$. The system is sampled with period $\\Delta t = 0.1\\,\\mathrm{s}$. The measurement is position-only with model\n$$y_{k} = \\underbrace{\\begin{pmatrix}1  0\\end{pmatrix}}_{H} \\begin{pmatrix} p_{k} \\\\ v_{k} \\end{pmatrix} + r_{k},$$\nwhere $r_{k}$ is zero-mean white Gaussian measurement noise with variance $R = 0.04$ in $\\mathrm{m}^2$.\n\nStarting from the linear time-invariant state-transition concept and the definition of how continuous-time white Gaussian process noise propagates through a continuous-time system over a finite sampling interval, derive the discrete-time process noise covariance matrix $Q_{d}$ for the sampled system driven by white-noise acceleration. Then, evaluate $Q_{d}$ numerically for $q = 0.5$ using $\\Delta t = 0.1\\,\\mathrm{s}$.\n\nUse the International System of Units (SI) for all quantities (meters and seconds). Report the entries of $Q_{d}$ numerically and round each entry to four significant figures. Do not include units in your final answer.",
            "solution": "The problem as stated is well-posed, scientifically grounded, and provides all necessary information for a unique solution. We will proceed with the derivation.\n\nThe continuous-time dynamics of the system are given by the linear time-invariant (LTI) stochastic differential equation:\n$$ \\frac{d}{dt} x(t) = F x(t) + L w(t) $$\nwhere the state vector is $x(t) = \\begin{pmatrix} p(t) \\\\ v(t) \\end{pmatrix}$, the state dynamics matrix is $F = \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix}$, the noise input matrix is $L = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, and $w(t) = a(t)$ is continuous-time white Gaussian noise with zero mean and a power spectral density (PSD) of $q$. The autocorrelation of this noise process is $E[w(t)w(\\tau)] = q \\delta(t-\\tau)$, where $\\delta(\\cdot)$ is the Dirac delta function.\n\nThe solution to this differential equation, propagating the state from time $t_k$ to $t_{k+1} = t_k + \\Delta t$, is given by the variation of parameters formula:\n$$ x(t_{k+1}) = e^{F(t_{k+1}-t_k)} x(t_k) + \\int_{t_k}^{t_{k+1}} e^{F(t_{k+1}-\\tau)} L w(\\tau) d\\tau $$\nThis can be written in the standard discrete-time form $x_{k+1} = A_d x_k + w_{dk}$, where $x_k \\equiv x(t_k)$, the discrete-time state transition matrix is $A_d = e^{F\\Delta t}$, and the discrete-time process noise vector is:\n$$ w_{dk} = \\int_{t_k}^{t_{k+1}} e^{F(t_{k+1}-\\tau)} L w(\\tau) d\\tau $$\nThe objective is to find the covariance matrix of this discrete-time process noise, $Q_d$, defined as $Q_d = E[w_{dk}w_{dk}^T]$.\n\nWe compute the expectation:\n$$ Q_d = E\\left[ \\left( \\int_{t_k}^{t_{k+1}} e^{F(t_{k+1}-\\tau_1)} L w(\\tau_1) d\\tau_1 \\right) \\left( \\int_{t_k}^{t_{k+1}} e^{F(t_{k+1}-\\tau_2)} L w(\\tau_2) d\\tau_2 \\right)^T \\right] $$\n$$ Q_d = E\\left[ \\int_{t_k}^{t_{k+1}} \\int_{t_k}^{t_{k+1}} e^{F(t_{k+1}-\\tau_1)} L w(\\tau_1) w(\\tau_2) L^T e^{F^T(t_{k+1}-\\tau_2)} d\\tau_1 d\\tau_2 \\right] $$\nBy moving the expectation operator inside the integrals and using the property $E[w(\\tau_1)w(\\tau_2)] = q \\delta(\\tau_1-\\tau_2)$, we get:\n$$ Q_d = \\int_{t_k}^{t_{k+1}} \\int_{t_k}^{t_{k+1}} e^{F(t_{k+1}-\\tau_1)} L (q \\delta(\\tau_1-\\tau_2)) L^T e^{F^T(t_{k+1}-\\tau_2)} d\\tau_1 d\\tau_2 $$\nUsing the sifting property of the Dirac delta function to evaluate the integral over $\\tau_2$:\n$$ Q_d = q \\int_{t_k}^{t_{k+1}} e^{F(t_{k+1}-\\tau)} L L^T e^{F^T(t_{k+1}-\\tau)} d\\tau $$\nTo simplify the integral, we perform a change of variable. Let $s = t_{k+1} - \\tau$. Then $ds = -d\\tau$. The integration limits change from $\\tau=t_k$ to $s=\\Delta t$ and from $\\tau=t_{k+1}$ to $s=0$.\n$$ Q_d = q \\int_{\\Delta t}^{0} e^{Fs} L L^T e^{F^T s} (-ds) = q \\int_{0}^{\\Delta t} e^{Fs} L L^T e^{F^T s} ds $$\nTo evaluate this integral, we must first compute the matrix exponential $e^{Fs}$. The matrix $F$ is nilpotent, as $F^2 = \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}$. The Taylor series for the exponential thus truncates after the linear term:\n$$ e^{Fs} = I + Fs + \\frac{1}{2!} (Fs)^2 + \\dots = I + Fs = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} 0  s \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1  s \\\\ 0  1 \\end{pmatrix} $$\nThe transpose is $e^{F^T s} = (e^{Fs})^T = \\begin{pmatrix} 1  0 \\\\ s  1 \\end{pmatrix}$.\nThe term $L L^T$ is:\n$$ L L^T = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 0  1 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix} $$\nNow we assemble the integrand:\n$$ e^{Fs} L L^T e^{F^T s} = \\begin{pmatrix} 1  s \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ s  1 \\end{pmatrix} = \\begin{pmatrix} 0  s \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ s  1 \\end{pmatrix} = \\begin{pmatrix} s^2  s \\\\ s  1 \\end{pmatrix} $$\nWe now integrate this matrix from $s=0$ to $s=\\Delta t$:\n$$ \\int_{0}^{\\Delta t} \\begin{pmatrix} s^2  s \\\\ s  1 \\end{pmatrix} ds = \\begin{pmatrix} \\int_{0}^{\\Delta t} s^2 ds  \\int_{0}^{\\Delta t} s ds \\\\ \\int_{0}^{\\Delta t} s ds  \\int_{0}^{\\Delta t} 1 ds \\end{pmatrix} = \\begin{pmatrix} \\left[\\frac{s^3}{3}\\right]_{0}^{\\Delta t}  \\left[\\frac{s^2}{2}\\right]_{0}^{\\Delta t} \\\\ \\left[\\frac{s^2}{2}\\right]_{0}^{\\Delta t}  \\left[s\\right]_{0}^{\\Delta t} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\Delta t^3}{3}  \\frac{\\Delta t^2}{2} \\\\ \\frac{\\Delta t^2}{2}  \\Delta t \\end{pmatrix} $$\nFinally, multiplying by the scalar PSD $q$, we obtain the analytical expression for the discrete-time process noise covariance matrix:\n$$ Q_d = q \\begin{pmatrix} \\frac{\\Delta t^3}{3}  \\frac{\\Delta t^2}{2} \\\\ \\frac{\\Delta t^2}{2}  \\Delta t \\end{pmatrix} $$\nNow, we substitute the specified numerical values: $q = 0.5$ and $\\Delta t = 0.1\\,\\mathrm{s}$.\n$$ \\Delta t^3 = (0.1)^3 = 0.001 $$\n$$ \\Delta t^2 = (0.1)^2 = 0.01 $$\n$$ \\Delta t = 0.1 $$\n$$ Q_d = 0.5 \\begin{pmatrix} \\frac{0.001}{3}  \\frac{0.01}{2} \\\\ \\frac{0.01}{2}  0.1 \\end{pmatrix} = 0.5 \\begin{pmatrix} 0.0003333...  0.005 \\\\ 0.005  0.1 \\end{pmatrix} $$\n$$ Q_d = \\begin{pmatrix} 0.0001666...  0.0025 \\\\ 0.0025  0.05 \\end{pmatrix} $$\nRounding each entry to four significant figures as required:\nThe entry $Q_{d,11}$ is $0.0001666...$, which rounds to $0.0001667$.\nThe entry $Q_{d,12} = Q_{d,21}$ is $0.0025$, which is written as $0.002500$ to show four significant figures.\nThe entry $Q_{d,22}$ is $0.05$, which is written as $0.05000$ to show four significant figures.\nThe final numerical matrix is therefore:\n$$ Q_d = \\begin{pmatrix} 0.0001667  0.002500 \\\\ 0.002500  0.05000 \\end{pmatrix} $$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} 0.0001667  0.002500 \\\\ 0.002500  0.05000 \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "A Kalman filter's power is not without limits; its stability hinges on the fundamental properties of the system being modeled. This exercise delves into the critical concept of 'detectability,' exploring a scenario where the filter is doomed to fail. By analyzing a system with an unstable mode that is hidden from the measurements, you will derive how the error covariance can grow without bound, providing a clear illustration of why observability is a cornerstone of stable state estimation .",
            "id": "3424989",
            "problem": "Consider the discrete-time Linear Time-Invariant (LTI) linear-Gaussian state-space model\n$$\nx_{k+1} = A x_{k} + w_{k}, \\quad y_{k} = C x_{k} + v_{k},\n$$\nwhere $x_{k} \\in \\mathbb{R}^{2}$, $y_{k} \\in \\mathbb{R}$, the process noise $w_{k} \\sim \\mathcal{N}(0,Q)$, the observation noise $v_{k} \\sim \\mathcal{N}(0,R)$, and $\\{w_{k}\\}$ and $\\{v_{k}\\}$ are mutually independent and independent of $x_{0}$. Let\n$$\nA = \\begin{pmatrix} \\tfrac{3}{2}  0 \\\\ 0  \\tfrac{1}{2} \\end{pmatrix}, \\quad\nC = \\begin{pmatrix} 0  1 \\end{pmatrix}, \\quad\nQ = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}, \\quad\nR = 1,\n$$\nand the initial a posteriori covariance be\n$$\nP_{0|0} = \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix}.\n$$\nThe Kalman filter (KF) is applied with the usual prediction and update steps. The pair $(A,C)$ is not detectable because the unstable eigenvalue of $A$ corresponds to an unobservable mode. Despite the nonzero observation noise covariance $R$, this lack of detectability can induce unbounded covariance growth along that mode.\n\nStarting from the foundational definitions of the prediction covariance and update covariance recursions of the KF and the given model structure, derive a closed-form analytic expression for the a posteriori variance of the first state component, denoted $[P_{k|k}]_{11}$, after $k$ complete prediction-update cycles, as a function of the integer $k \\ge 0$. Express your final answer in exact form. No rounding is required, and no units are to be reported.",
            "solution": "The problem is valid as it is well-posed, scientifically grounded in the theory of Kalman filtering, and contains a complete and consistent set of givens.\n\nThe evolution of the a posteriori error covariance matrix $P_{k|k}$ in a discrete-time Kalman filter is governed by a two-step recursion for each time step $k \\ge 1$: a prediction step followed by an update step.\n\nThe state-space model is given by:\n$$\nx_{k+1} = A x_{k} + w_{k}, \\quad w_{k} \\sim \\mathcal{N}(0, Q)\n$$\n$$\ny_{k} = C x_{k} + v_{k}, \\quad v_{k} \\sim \\mathcal{N}(0, R)\n$$\nwith the specified matrices:\n$$\nA = \\begin{pmatrix} \\frac{3}{2}  0 \\\\ 0  \\frac{1}{2} \\end{pmatrix}, \\quad C = \\begin{pmatrix} 0  1 \\end{pmatrix}, \\quad Q = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}, \\quad R = 1\n$$\nThe initial a posteriori covariance is:\n$$\nP_{0|0} = \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix}\n$$\nWe seek a closed-form expression for $[P_{k|k}]_{11}$ for $k \\ge 0$.\n\nThe Kalman filter covariance equations are:\n1.  **Prediction:** The a priori covariance for step $k$ is predicted from the a posteriori covariance at step $k-1$:\n    $$\n    P_{k|k-1} = A P_{k-1|k-1} A^T + Q\n    $$\n2.  **Update:** The a posteriori covariance for step $k$ is computed using the a priori covariance $P_{k|k-1}$:\n    $$\n    K_k = P_{k|k-1} C^T (C P_{k|k-1} C^T + R)^{-1}\n    $$\n    $$\n    P_{k|k} = (I - K_k C) P_{k|k-1}\n    $$\nThe problem is significantly simplified by the diagonal structure of the matrices $A$, $Q$, and the initial covariance $P_{0|0}$. We will demonstrate by induction that the covariance matrices $P_{k|k-1}$ and $P_{k|k}$ remain diagonal for all $k \\ge 0$.\n\nLet $P_{k|k} = \\begin{pmatrix} [P_{k|k}]_{11}  [P_{k|k}]_{12} \\\\ [P_{k|k}]_{21}  [P_{k|k}]_{22} \\end{pmatrix}$. For brevity, we denote $p_{11}(k) = [P_{k|k}]_{11}$ and $p_{22}(k) = [P_{k|k}]_{22}$.\n\n**Inductive Proof of Diagonal Structure:**\n**Base Case ($k=0$):** The given initial covariance $P_{0|0} = \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix}$ is diagonal.\n\n**Inductive Step:** Assume that $P_{k-1|k-1}$ is diagonal for some $k \\ge 1$. Let $P_{k-1|k-1} = \\begin{pmatrix} p_{11}(k-1)  0 \\\\ 0  p_{22}(k-1) \\end{pmatrix}$.\n\nFirst, we compute the predicted covariance $P_{k|k-1}$:\n$$\nP_{k|k-1} = \\begin{pmatrix} \\frac{3}{2}  0 \\\\ 0  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} p_{11}(k-1)  0 \\\\ 0  p_{22}(k-1) \\end{pmatrix} \\begin{pmatrix} \\frac{3}{2}  0 \\\\ 0  \\frac{1}{2} \\end{pmatrix}^T + \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\n$$\n$$\nP_{k|k-1} = \\begin{pmatrix} (\\frac{3}{2})^2 p_{11}(k-1)  0 \\\\ 0  (\\frac{1}{2})^2 p_{22}(k-1) \\end{pmatrix} + \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{9}{4} p_{11}(k-1) + 1  0 \\\\ 0  \\frac{1}{4} p_{22}(k-1) + 1 \\end{pmatrix}\n$$\nThus, $P_{k|k-1}$ is also diagonal. Let's denote its components as $p'_{11} = \\frac{9}{4} p_{11}(k-1) + 1$ and $p'_{22} = \\frac{1}{4} p_{22}(k-1) + 1$.\n\nNext, we compute the updated covariance $P_{k|k}$. We first find the innovation covariance $S_k$ and the Kalman gain $K_k$:\n$$\nS_k = C P_{k|k-1} C^T + R = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} p'_{11}  0 \\\\ 0  p'_{22} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + 1 = p'_{22} + 1\n$$\nSince $S_k$ is a scalar, its inverse is simply $1/S_k$.\n$$\nK_k = P_{k|k-1} C^T S_k^{-1} = \\begin{pmatrix} p'_{11}  0 \\\\ 0  p'_{22} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} (p'_{22} + 1)^{-1} = \\begin{pmatrix} 0 \\\\ \\frac{p'_{22}}{p'_{22}+1} \\end{pmatrix}\n$$\nNow we compute the updated covariance $P_{k|k}$:\n$$\nP_{k|k} = (I - K_k C) P_{k|k-1}\n$$\n$$\nI - K_k C = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ \\frac{p'_{22}}{p'_{22}+1} \\end{pmatrix} \\begin{pmatrix} 0  1 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} - \\begin{pmatrix} 0  0 \\\\ 0  \\frac{p'_{22}}{p'_{22}+1} \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  \\frac{1}{p'_{22}+1} \\end{pmatrix}\n$$\n$$\nP_{k|k} = \\begin{pmatrix} 1  0 \\\\ 0  \\frac{1}{p'_{22}+1} \\end{pmatrix} \\begin{pmatrix} p'_{11}  0 \\\\ 0  p'_{22} \\end{pmatrix} = \\begin{pmatrix} p'_{11}  0 \\\\ 0  \\frac{p'_{22}}{p'_{22}+1} \\end{pmatrix}\n$$\nThis shows that $P_{k|k}$ is diagonal. By the principle of mathematical induction, $P_{k|k}$ is diagonal for all $k \\ge 0$.\n\nThe recursions for the diagonal elements are decoupled:\n$$\np_{11}(k) = [P_{k|k}]_{11} = p'_{11} = \\frac{9}{4} p_{11}(k-1) + 1\n$$\n$$\np_{22}(k) = [P_{k|k}]_{22} = \\frac{p'_{22}}{p'_{22}+1} = \\frac{\\frac{1}{4} p_{22}(k-1) + 1}{(\\frac{1}{4} p_{22}(k-1) + 1) + 1} = \\frac{\\frac{1}{4} p_{22}(k-1) + 1}{\\frac{1}{4} p_{22}(k-1) + 2}\n$$\nWe only need to solve the recursion for $p_{11}(k)$. This is a linear first-order non-homogeneous recurrence relation of the form $z_k = a z_{k-1} + b$, with $a = \\frac{9}{4}$ and $b=1$. The initial condition is $p_{11}(0) = [P_{0|0}]_{11} = 0$.\n\nThe general solution to $z_k = a z_{k-1} + b$ for $a \\neq 1$ is $z_k = a^k z_0 + b \\frac{a^k-1}{a-1}$.\nSubstituting $z_k = p_{11}(k)$, $a = \\frac{9}{4}$, $b=1$, and $p_{11}(0)=0$:\n$$\np_{11}(k) = \\left(\\frac{9}{4}\\right)^k \\cdot 0 + 1 \\cdot \\frac{\\left(\\frac{9}{4}\\right)^k - 1}{\\frac{9}{4} - 1}\n$$\n$$\np_{11}(k) = \\frac{\\left(\\frac{9}{4}\\right)^k - 1}{\\frac{5}{4}}\n$$\n$$\np_{11}(k) = \\frac{4}{5} \\left( \\left(\\frac{9}{4}\\right)^k - 1 \\right)\n$$\nThis expression is valid for all integers $k \\ge 0$. For $k=0$, it correctly yields $p_{11}(0) = \\frac{4}{5}(1-1) = 0$. This unbounded growth in variance is a direct consequence of the first state component being associated with an unstable eigenvalue ($|\\frac{3}{2}| > 1$) and being unobservable (as $C$ has a zero in the first position).\nThe final closed-form expression for the a posteriori variance of the first state component is therefore:\n$$\n[P_{k|k}]_{11} = \\frac{4}{5} \\left( \\left(\\frac{9}{4}\\right)^k - 1 \\right)\n$$",
            "answer": "$$\n\\boxed{\\frac{4}{5} \\left( \\left(\\frac{9}{4}\\right)^k - 1 \\right)}\n$$"
        },
        {
            "introduction": "In theory, the Kalman filter's covariance matrix is always mathematically sound, but in practice, the finite precision of computer arithmetic can introduce subtle but catastrophic errors. This practice confronts this numerical fragility head-on by demonstrating how rounding errors in the standard covariance update formula can cause the matrix to lose its essential positive-semidefinite property. This hands-on calculation highlights the necessity of numerically robust 'square-root' filtering techniques in real-world applications to prevent such failures .",
            "id": "3424957",
            "problem": "Consider a linear, time-invariant, discrete-time, Gaussian state-space model. At a given time step, the one-step predicted state $x_{k|k-1}$ is Gaussian with mean $m_{k|k-1}$ and covariance $P_{k|k-1}$. A measurement $y_k$ is obtained according to $y_k = H x_k + v_k$, where $v_k$ is Gaussian, zero-mean, independent of $x_k$, with covariance $R$. Work in exact arithmetic unless otherwise specified.\n\nPart A (exact arithmetic, foundational derivation): Starting from the joint Gaussianity of $(x_k, y_k)$ and the definition of conditional covariance for jointly Gaussian variables, derive the analytic expression for the posterior covariance $P_{k|k}$ and explain why, in exact arithmetic, $P_{k|k}$ is positive semidefinite (PSD). Your derivation must be grounded in the block matrix inversion identity and the Schur complement characterization of positive semidefiniteness.\n\nPart B (finite precision counterexample): Define a finite precision arithmetic model as follows: every scalar product and scalar sum is rounded immediately to $4$ significant figures, and $2 \\times 2$ matrix inversion is performed via the adjugate formula with the same rounding rule applied to all intermediate scalar products, sums, and the reciprocal. In this finite precision model, consider\n$$\nP_{k|k-1} = \\begin{pmatrix} 1.000  0.9990 \\\\ 0.9990  0.9980 \\end{pmatrix}, \\quad\nH = I_2, \\quad\nR = 0.0010\\, I_2.\n$$\nCompute the innovation covariance $S = H P_{k|k-1} H^{\\top} + R$, its inverse $S^{-1}$, the Kalman gain $K = P_{k|k-1} H^{\\top} S^{-1}$, and then the posterior covariance using the naive covariance update $P_{k|k} = (I - K H) P_{k|k-1}$, all under the stated rounding model. Then symmetrize the result by $\\tfrac{1}{2}\\big(P_{k|k} + P_{k|k}^{\\top}\\big)$. Show that the symmetrized matrix is not PSD by computing its smallest eigenvalue.\n\nPart C (remedies via square-root filters): Based on first principles of numerical linear algebra and covariance factorization, outline how a square-root Kalman filter, which propagates a Cholesky factor $S_{k|k}$ with $P_{k|k} = S_{k|k} S_{k|k}^{\\top}$ and updates it via an orthogonal-triangular (QR) factorization, mitigates the finite precision loss of positive semidefiniteness compared to the naive covariance update.\n\nAnswer specification: Report as your final answer the smallest eigenvalue of the symmetrized posterior covariance from Part B. Round your answer to four significant figures. The answer is unitless and must be a single real number.",
            "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. The provided prior covariance matrix $P_{k|k-1}$ is positive semidefinite within the specified finite precision arithmetic model, thus constituting a valid premise.\n\nPart A: Foundational Derivation and Proof of Positive Semidefiniteness\n\nWe begin by establishing the joint distribution of the state $x_k$ and the measurement $y_k$. The state $x_k$ is assumed to have a prior Gaussian distribution from the prediction step, $x_k \\sim \\mathcal{N}(m_{k|k-1}, P_{k|k-1})$. The measurement $y_k$ is related to the state by the linear model $y_k = H x_k + v_k$, where the noise $v_k$ is an independent, zero-mean Gaussian process, $v_k \\sim \\mathcal{N}(0, R)$.\n\nSince $x_k$ and $y_k$ are jointly Gaussian, we can characterize their joint distribution by its mean vector and covariance matrix. Let the joint vector be $Z = \\begin{pmatrix} x_k \\\\ y_k \\end{pmatrix}$.\n\nThe mean of the joint vector is:\n$$\n\\mathbb{E}[Z] = \\begin{pmatrix} \\mathbb{E}[x_k] \\\\ \\mathbb{E}[y_k] \\end{pmatrix} = \\begin{pmatrix} m_{k|k-1} \\\\ \\mathbb{E}[H x_k + v_k] \\end{pmatrix} = \\begin{pmatrix} m_{k|k-1} \\\\ H m_{k|k-1} + \\mathbb{E}[v_k] \\end{pmatrix} = \\begin{pmatrix} m_{k|k-1} \\\\ H m_{k|k-1} \\end{pmatrix}\n$$\n\nThe covariance matrix of the joint vector is a block matrix:\n$$\n\\text{Cov}(Z) = \\begin{pmatrix} \\text{Cov}(x_k)  \\text{Cov}(x_k, y_k) \\\\ \\text{Cov}(y_k, x_k)  \\text{Cov}(y_k) \\end{pmatrix}\n$$\nThe blocks are computed as follows:\n- $\\text{Cov}(x_k) = P_{k|k-1}$\n- $\\text{Cov}(x_k, y_k) = \\text{Cov}(x_k, H x_k + v_k) = \\text{Cov}(x_k, H x_k) + \\text{Cov}(x_k, v_k)$. Since $x_k$ and $v_k$ are independent, $\\text{Cov}(x_k, v_k)=0$. Thus, $\\text{Cov}(x_k, y_k) = \\text{Cov}(x_k, x_k) H^{\\top} = P_{k|k-1} H^{\\top}$.\n- $\\text{Cov}(y_k, x_k) = \\left(\\text{Cov}(x_k, y_k)\\right)^{\\top} = H P_{k|k-1}$.\n- $\\text{Cov}(y_k) = \\text{Cov}(H x_k + v_k) = \\text{Cov}(H x_k) + \\text{Cov}(v_k) = H \\text{Cov}(x_k) H^{\\top} + R = H P_{k|k-1} H^{\\top} + R$. This is the innovation covariance, denoted $S_k$.\n\nSo, the joint covariance matrix is:\n$$\n\\text{Cov}(Z) = \\begin{pmatrix} P_{k|k-1}  P_{k|k-1} H^{\\top} \\\\ H P_{k|k-1}  S_k \\end{pmatrix}\n$$\nThe posterior covariance $P_{k|k}$ is the conditional covariance of $x_k$ given the measurement $y_k$. For a joint Gaussian partitioned vector $\\begin{pmatrix} X_1 \\\\ X_2 \\end{pmatrix}$ with covariance $\\begin{pmatrix} \\Sigma_{11}  \\Sigma_{12} \\\\ \\Sigma_{21}  \\Sigma_{22} \\end{pmatrix}$, the conditional covariance of $X_1$ given $X_2$ is given by the Schur complement of $\\Sigma_{22}$:\n$$\n\\text{Cov}(X_1 | X_2) = \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21}\n$$\nBy identifying $X_1 = x_k$ and $X_2 = y_k$, we substitute the blocks from $\\text{Cov}(Z)$:\n$$\nP_{k|k} = P_{k|k-1} - (P_{k|k-1} H^{\\top}) S_k^{-1} (H P_{k|k-1})\n$$\nThis is a standard form for the updated covariance.\n\nTo prove that $P_{k|k}$ is positive semidefinite (PSD) in exact arithmetic, we use the Schur complement characterization of PSD matrices. A symmetric block matrix $M = \\begin{pmatrix} A  B \\\\ B^{\\top}  D \\end{pmatrix}$ is PSD if and only if $D \\succeq 0$ and its Schur complement $A - B D^{-1} B^{\\top} \\succeq 0$ (assuming $D$ is invertible, i.e., positive definite).\nThe joint covariance matrix $\\text{Cov}(Z)$ is, by definition, a covariance matrix and must therefore be PSD. Let's analyze its blocks:\n- $A = P_{k|k-1}$, which is PSD as a covariance matrix.\n- $B = P_{k|k-1} H^{\\top}$.\n- $D = S_k = H P_{k|k-1} H^{\\top} + R$. Since $P_{k|k-1} \\succeq 0$, then $H P_{k|k-1} H^{\\top} \\succeq 0$. Since $R$ is a covariance matrix, $R \\succeq 0$. The sum of two PSD matrices is PSD, so $S_k \\succeq 0$. Typically $R$ is assumed positive definite, which makes $S_k$ positive definite and thus invertible.\n\nSince $\\text{Cov}(Z)$ is PSD and $S_k$ is positive definite, the Schur complement of $S_k$ in $\\text{Cov}(Z)$ must be PSD. This Schur complement is precisely the expression we derived for the posterior covariance:\n$$\nP_{k|k} = P_{k|k-1} - P_{k|k-1} H^{\\top} S_k^{-1} H P_{k|k-1} \\succeq 0\n$$\nThus, in exact arithmetic, the posterior covariance $P_{k|k}$ is guaranteed to be positive semidefinite.\n\nPart B: Finite Precision Counterexample\n\nWe perform the calculations using the specified finite precision arithmetic model (rounding each scalar product and sum to $4$ significant figures).\nGiven:\n$P_{k|k-1} = \\begin{pmatrix} 1.000  0.9990 \\\\ 0.9990  0.9980 \\end{pmatrix}$, $H = I_2 = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$, $R = \\begin{pmatrix} 0.0010  0 \\\\ 0  0.0010 \\end{pmatrix}$.\n\n1.  Compute the innovation covariance $S = H P_{k|k-1} H^{\\top} + R = P_{k|k-1} + R$:\n    $S = \\begin{pmatrix} 1.000+0.0010  0.9990+0 \\\\ 0.9990+0  0.9980+0.0010 \\end{pmatrix} = \\begin{pmatrix} 1.0010  0.9990 \\\\ 0.9990  0.9990 \\end{pmatrix}$.\n    Rounding the sum $1.0010$ to $4$ sig figs gives $1.001$.\n    $S = \\begin{pmatrix} 1.001  0.9990 \\\\ 0.9990  0.9990 \\end{pmatrix}$.\n\n2.  Compute $S^{-1}$ using the adjugate formula $S^{-1} = \\frac{1}{\\det(S)} \\text{adj}(S)$:\n    - $\\det(S) = (1.001 \\times 0.9990) - (0.9990 \\times 0.9990)$.\n    - Product $1$: $1.001 \\times 0.9990 = 0.999999 \\to 1.000$.\n    - Product $2$: $0.9990 \\times 0.9990 = 0.998001 \\to 0.9980$.\n    - Sum: $1.000 - 0.9980 = 0.0020$. Round to $4$ sig figs - $0.002000$.\n    - $1 / \\det(S) = 1 / 0.002000 = 500.0$.\n    - adj$(S) = \\begin{pmatrix} 0.9990  -0.9990 \\\\ -0.9990  1.001 \\end{pmatrix}$.\n    - $S^{-1} = 500.0 \\times \\begin{pmatrix} 0.9990  -0.9990 \\\\ -0.9990  1.001 \\end{pmatrix} = \\begin{pmatrix} 500.0 \\times 0.9990  500.0 \\times -0.9990 \\\\ 500.0 \\times -0.9990  500.0 \\times 1.001 \\end{pmatrix}$.\n    - Element $(1,1)$: $499.5$. Element $(1,2)$: $-499.5$. Element $(2,1)$: $-499.5$. Element $(2,2)$: $500.5$.\n    - $S^{-1} = \\begin{pmatrix} 499.5  -499.5 \\\\ -499.5  500.5 \\end{pmatrix}$.\n\n3.  Compute the Kalman gain $K = P_{k|k-1} H^{\\top} S^{-1} = P_{k|k-1} S^{-1}$:\n    $K = \\begin{pmatrix} 1.000  0.9990 \\\\ 0.9990  0.9980 \\end{pmatrix} \\begin{pmatrix} 499.5  -499.5 \\\\ -499.5  500.5 \\end{pmatrix}$.\n    - $K_{11} = (1.000 \\times 499.5) + (0.9990 \\times -499.5) = (499.5) + (-499.0005 \\to -499.0) = 0.5 \\to 0.5000$.\n    - $K_{12} = (1.000 \\times -499.5) + (0.9990 \\times 500.5) = (-499.5) + (499.9995 \\to 500.0) = 0.5 \\to 0.5000$.\n    - $K_{21} = (0.9990 \\times 499.5) + (0.9980 \\times -499.5) = (499.0005 \\to 499.0) + (-498.501 \\to -498.5) = 0.5 \\to 0.5000$.\n    - $K_{22} = (0.9990 \\times -499.5) + (0.9980 \\times 500.5) = (-499.0005 \\to -499.0) + (499.499 \\to 499.5) = 0.5 \\to 0.5000$.\n    - $K = \\begin{pmatrix} 0.5000  0.5000 \\\\ 0.5000  0.5000 \\end{pmatrix}$.\n\n4.  Compute the posterior covariance $P_{k|k} = (I - K H) P_{k|k-1} = (I-K)P_{k|k-1}$:\n    - $I - K = \\begin{pmatrix} 1.000 - 0.5000  0 - 0.5000 \\\\ 0 - 0.5000  1.000 - 0.5000 \\end{pmatrix} = \\begin{pmatrix} 0.5000  -0.5000 \\\\ -0.5000  0.5000 \\end{pmatrix}$.\n    - $P_{k|k} = \\begin{pmatrix} 0.5000  -0.5000 \\\\ -0.5000  0.5000 \\end{pmatrix} \\begin{pmatrix} 1.000  0.9990 \\\\ 0.9990  0.9980 \\end{pmatrix}$.\n    - $P_{11} = (0.5000 \\times 1.000) + (-0.5000 \\times 0.9990) = (0.5000) + (-0.4995) = 0.0005 \\to 0.0005000$.\n    - $P_{12} = (0.5000 \\times 0.9990) + (-0.5000 \\times 0.9980) = (0.4995) + (-0.4990) = 0.0005 \\to 0.0005000$.\n    - $P_{21} = (-0.5000 \\times 1.000) + (0.5000 \\times 0.9990) = (-0.5000) + (0.4995) = -0.0005 \\to -0.0005000$.\n    - $P_{22} = (-0.5000 \\times 0.9990) + (0.5000 \\times 0.9980) = (-0.4995) + (0.4990) = -0.0005 \\to -0.0005000$.\n    - $P_{k|k} = \\begin{pmatrix} 0.0005000  0.0005000 \\\\ -0.0005000  -0.0005000 \\end{pmatrix}$.\n\n5.  Symmetrize the result: $P_{k|k}^{\\text{sym}} = \\frac{1}{2}\\big(P_{k|k} + P_{k|k}^{\\top}\\big)$:\n    - $P_{k|k} + P_{k|k}^{\\top} = \\begin{pmatrix} 0.0005000  0.0005000 \\\\ -0.0005000  -0.0005000 \\end{pmatrix} + \\begin{pmatrix} 0.0005000  -0.0005000 \\\\ 0.0005000  -0.0005000 \\end{pmatrix} = \\begin{pmatrix} 0.001000  0 \\\\ 0  -0.001000 \\end{pmatrix}$.\n    - $P_{k|k}^{\\text{sym}} = \\frac{1}{2} \\begin{pmatrix} 0.001000  0 \\\\ 0  -0.001000 \\end{pmatrix} = \\begin{pmatrix} 0.0005000  0 \\\\ 0  -0.0005000 \\end{pmatrix}$.\n\n6.  Compute the smallest eigenvalue: Since the symmetrized matrix is diagonal, its eigenvalues are its diagonal entries: $\\lambda_1 = 0.0005000$ and $\\lambda_2 = -0.0005000$. The smallest eigenvalue is $-0.0005000$. This negative eigenvalue demonstrates that the resulting matrix is not positive semidefinite, a failure of the naive update formula in finite precision.\n\nPart C: Remedies via Square-Root Filters\n\nSquare-root Kalman filters mitigate the loss of positive semidefiniteness by propagating a matrix square root of the covariance, rather than the covariance matrix itself. The fundamental idea is to maintain a factor, such as the Cholesky factor $S_{k|k}$, which is a lower triangular matrix, such that $P_{k|k} = S_{k|k} S_{k|k}^{\\top}$.\n\nThe key advantages of this approach are:\n1.  **Guaranteed Positive Semidefiniteness**: By construction, a matrix of the form $A A^{\\top}$ is always positive semidefinite for any real matrix $A$. This is because for any vector $x$, the quadratic form $x^{\\top}(A A^{\\top})x = (A^{\\top}x)^{\\top}(A^{\\top}x) = \\|A^{\\top}x\\|_2^2$ is always non-negative. As long as the filter computes a real-valued Cholesky factor $S_{k|k}$, the resulting full covariance matrix $P_{k|k} = S_{k|k} S_{k|k}^{\\top}$ is guaranteed to be PSD. This completely prevents the failure mode observed in Part B, where the computed covariance becomes indefinite.\n2.  **Improved Numerical Stability**: The condition number of a matrix $P$ is related to the condition number of its Cholesky factor $S$ by $\\kappa(P) = \\kappa(S)^2$. Numerical operations involving the square-root factor $S$ are performed on a matrix that is typically much better conditioned than $P$, reducing the effects of rounding errors. The catastrophic cancellation observed in Part B, particularly in the subtraction $(I-KH)P_{k|k-1}$, is a symptom of operating on ill-conditioned matrices.\n3.  **Use of Stable Numerical Techniques**: The update step from the prior factor $S_{k|k-1}$ to the posterior factor $S_{k|k}$ is implemented using numerically stable algorithms, most notably those based on orthogonal transformations like QR factorization. The measurement update can be formulated as a problem of finding the Cholesky factor of a sum or difference of matrices, which is accomplished by forming an auxiliary array and applying a sequence of highly stable Givens rotations or Householder reflections to triangularize it. These orthogonal transformations are perfectly conditioned and do not amplify numerical errors, preserving the accuracy of the factor and thus the integrity of the covariance representation.",
            "answer": "$$\\boxed{-0.0005000}$$"
        }
    ]
}