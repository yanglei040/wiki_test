{
    "hands_on_practices": [
        {
            "introduction": "The foundation of any sequential data assimilation method is the forecast step, where the system's state and its uncertainty are propagated forward in time. This first practice provides a direct, hands-on opportunity to implement this crucial step within a stochastic ensemble filter framework. By generating an ensemble and applying a linear model with stochastic noise, you will numerically verify that the sample covariance of the forecast ensemble aligns with the theoretical forecast error covariance, bridging the gap between the abstract formula $\\Phi P^{a} \\Phi^{\\top} + Q$ and its practical implementation .",
            "id": "3422911",
            "problem": "Consider a linear Gaussian state evolution model in discrete time with state dimension $d$ given by $x_{k} = \\Phi x_{k-1} + w_{k}$, where $\\Phi \\in \\mathbb{R}^{d \\times d}$ is a known forecast operator and $w_{k} \\sim \\mathcal{N}(0, Q)$ is Gaussian process noise with covariance $Q \\in \\mathbb{R}^{d \\times d}$, symmetric positive definite. In stochastic ensemble filtering (e.g., Ensemble Kalman Filter (EnKF)), the forecast ensemble at step $k$ is obtained by mapping each analysis ensemble member through $\\Phi$ and adding an independent perturbation $w_{k}^{(i)}$ drawn from $\\mathcal{N}(0, Q)$. A numerically stable method to sample $w_{k}^{(i)}$ is to compute a Cholesky factorization $Q = L L^{\\top}$ with $L$ lower-triangular, and set $w_{k}^{(i)} = L \\xi^{(i)}$ with $\\xi^{(i)} \\sim \\mathcal{N}(0, I_{d})$ independent across ensemble members.\n\nStarting from the foundational facts:\n- If $x_{a}^{(i)} \\sim \\mathcal{N}(m^{a}, P^{a})$ are independent analysis ensemble members, and $w^{(i)} \\sim \\mathcal{N}(0, Q)$ are independent of $x_{a}^{(i)}$, then $x_{f}^{(i)} = \\Phi x_{a}^{(i)} + w^{(i)}$ are independent draws from $\\mathcal{N}(\\Phi m^{a}, \\Phi P^{a} \\Phi^{\\top} + Q)$.\n- For independent, identically distributed Gaussian samples with true covariance $\\Sigma \\in \\mathbb{R}^{d \\times d}$, the unbiased sample covariance $S$ computed from $N$ samples satisfies $(N-1) S \\sim \\mathcal{W}_{d}(\\Sigma, N-1)$ (Wishart distribution), with $\\mathbb{E}[S] = \\Sigma$. For $S$ as above, the componentwise variances of $S$ obey $\\operatorname{Var}(S_{ij}) = \\frac{1}{N-1}\\left(\\Sigma_{ij}^{2} + \\Sigma_{ii}\\Sigma_{jj}\\right)$, implying\n$$\n\\mathbb{E}\\left[\\lVert S - \\Sigma \\rVert_{F}^{2}\\right] = \\frac{1}{N-1}\\left(\\lVert \\Sigma \\rVert_{F}^{2} + (\\operatorname{tr}\\Sigma)^{2}\\right).\n$$\n\nYour task is to implement the stochastic forecast step using Cholesky-based sampling and to verify that the empirical forecast covariance $\\hat{P}^{f}$ computed from the forecast ensemble is consistent with the theoretical forecast covariance $\\Sigma_{f} = \\Phi P^{a} \\Phi^{\\top} + Q$ within sampling error, quantified by the formula above. Specifically, for each test case:\n1. Generate an analysis ensemble $\\{x_{a}^{(i)}\\}_{i=1}^{N}$ with $x_{a}^{(i)} \\sim \\mathcal{N}(m^{a}, P^{a})$, where $m^{a} = 0$ is the zero vector of appropriate dimension.\n2. Sample independent process noises $w^{(i)}$ via $w^{(i)} = L \\xi^{(i)}$ with $Q = L L^{\\top}$ the Cholesky factorization and $\\xi^{(i)} \\sim \\mathcal{N}(0, I_{d})$.\n3. Form forecast ensemble members $x_{f}^{(i)} = \\Phi x_{a}^{(i)} + w^{(i)}$.\n4. Compute the unbiased sample covariance of the forecast ensemble,\n$$\n\\hat{P}^{f} = \\frac{1}{N-1}\\sum_{i=1}^{N}\\left(x_{f}^{(i)} - \\bar{x}_{f}\\right)\\left(x_{f}^{(i)} - \\bar{x}_{f}\\right)^{\\top}, \\quad \\bar{x}_{f} = \\frac{1}{N}\\sum_{i=1}^{N} x_{f}^{(i)}.\n$$\n5. Compute the theoretical forecast covariance $\\Sigma_{f} = \\Phi P^{a} \\Phi^{\\top} + Q$.\n6. Compute the Frobenius-norm discrepancy $\\Delta = \\lVert \\hat{P}^{f} - \\Sigma_{f} \\rVert_{F}$ and the root-mean-square (RMS) sampling error predicted by the Wishart theory,\n$$\n\\varepsilon_{\\mathrm{RMS}} = \\sqrt{\\frac{\\lVert \\Sigma_{f} \\rVert_{F}^{2} + (\\operatorname{tr}\\Sigma_{f})^{2}}{N-1}}.\n$$\nDeclare the test case a pass if $\\Delta \\leq c \\, \\varepsilon_{\\mathrm{RMS}}$ with $c = 3$.\n\nAll quantities are dimensionless; no physical units are required.\n\nImplement your program to evaluate the following test suite. For each case, $d$ is the state dimension, $N$ the ensemble size, and the matrices are given explicitly. In every case, take $m^{a} = 0$:\n- Case 1 (happy path, moderate dimension and ensemble size):\n  - $d = 3$, $N = 400$,\n  - $$ \\Phi = \\begin{bmatrix} 0.9  0.1  0.0 \\\\ 0.0  0.7  0.2 \\\\ 0.0  0.0  0.8 \\end{bmatrix} $$\n  - $$ P^{a} = \\begin{bmatrix} 0.5  0.1  0.0 \\\\ 0.1  0.4  0.05 \\\\ 0.0  0.05  0.3 \\end{bmatrix} $$\n  - $$ Q = \\begin{bmatrix} 0.2  0.05  0.0 \\\\ 0.05  0.1  0.02 \\\\ 0.0  0.02  0.15 \\end{bmatrix} $$\n- Case 2 (boundary, scalar system):\n  - $d = 1$, $N = 200$,\n  - $$ \\Phi = [1.2] $$\n  - $$ P^{a} = [0.25] $$\n  - $$ Q = [0.5] $$\n- Case 3 (higher dimension, large ensemble size):\n  - $d = 5$, $N = 2000$,\n  - $$ \\Phi = \\begin{bmatrix} 0.6  0.2  0.0  0.0  0.0 \\\\ 0.0  0.7  0.1  0.0  0.0 \\\\ 0.0  0.0  0.5  0.2  0.0 \\\\ 0.0  0.0  0.0  0.65  0.1 \\\\ 0.0  0.0  0.0  0.0  0.55 \\end{bmatrix} $$\n  - $$ P^{a} = \\begin{bmatrix} 1.0  0.1  0.0  0.0  0.0 \\\\ 0.1  0.8  0.0  0.0  0.0 \\\\ 0.0  0.0  0.6  0.05  0.0 \\\\ 0.0  0.0  0.05  0.7  0.0 \\\\ 0.0  0.0  0.0  0.0  0.9 \\end{bmatrix} $$\n  - $$ Q = \\begin{bmatrix} 0.3  0.04  0.0  0.0  0.0 \\\\ 0.04  0.25  0.03  0.0  0.0 \\\\ 0.0  0.03  0.2  0.02  0.0 \\\\ 0.0  0.0  0.02  0.15  0.01 \\\\ 0.0  0.0  0.0  0.01  0.35 \\end{bmatrix} $$\n- Case 4 (edge, small ensemble size):\n  - $d = 4$, $N = 30$,\n  - $$ \\Phi = \\begin{bmatrix} 0.85  0.1  0.0  0.0 \\\\ 0.0  0.75  0.15  0.0 \\\\ 0.0  0.0  0.65  0.1 \\\\ 0.0  0.0  0.0  0.7 \\end{bmatrix} $$\n  - $$ P^{a} = \\begin{bmatrix} 0.6  0.05  0.0  0.0 \\\\ 0.05  0.5  0.04  0.0 \\\\ 0.0  0.04  0.4  0.03 \\\\ 0.0  0.0  0.03  0.45 \\end{bmatrix} $$\n  - $$ Q = \\begin{bmatrix} 0.25  0.03  0.0  0.0 \\\\ 0.03  0.2  0.02  0.0 \\\\ 0.0  0.02  0.18  0.01 \\\\ 0.0  0.0  0.01  0.22 \\end{bmatrix} $$\n\nFor reproducibility, the random number generation must be deterministic; use fixed seeds for each case.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each entry a boolean indicating whether the test case passed (e.g., \"[True,False,True,True]\").",
            "solution": "The problem requires the implementation and verification of the stochastic forecast step in an ensemble-based data assimilation framework. This involves advancing an ensemble of state vectors, representing the probability distribution of the system's state, according to a linear stochastic dynamical model. The core of the task is to confirm that the sample covariance of the resulting forecast ensemble is statistically consistent with the theoretical forecast error covariance predicted by the model.\n\nThe system's state evolution is described by the discrete-time linear Gaussian model:\n$$\nx_{k} = \\Phi x_{k-1} + w_{k}\n$$\nHere, $x_{k} \\in \\mathbb{R}^{d}$ is the state vector at time step $k$, $\\Phi \\in \\mathbb{R}^{d \\times d}$ is the deterministic forecast operator (or transition matrix), and $w_{k} \\in \\mathbb{R}^{d}$ is a random process noise vector. The noise is assumed to be drawn from a zero-mean multivariate Gaussian distribution, $w_{k} \\sim \\mathcal{N}(0, Q)$, with a known symmetric positive-definite covariance matrix $Q \\in \\mathbb{R}^{d \\times d}$.\n\nIn an ensemble filter, the probability distribution of the state is represented by a finite set of samples, the ensemble. Let the analysis ensemble at step $k-1$ be $\\{x_{a}^{(i)}\\}_{i=1}^{N}$, where $N$ is the ensemble size. Each member $x_{a}^{(i)}$ is assumed to be an independent draw from the analysis distribution, which is Gaussian: $x_{a}^{(i)} \\sim \\mathcal{N}(m^{a}, P^{a})$. For this problem, the analysis mean is specified as the zero vector, $m^{a} = 0$.\n\nThe forecast step propagates each analysis ensemble member to the next time step, $k$. Each member is transformed by the operator $\\Phi$ and perturbed by an independent realization of the process noise:\n$$\nx_{f}^{(i)} = \\Phi x_{a}^{(i)} + w^{(i)}\n$$\nwhere each $w^{(i)} \\sim \\mathcal{N}(0, Q)$ is drawn independently. The resulting set $\\{x_{f}^{(i)}\\}_{i=1}^{N}$ is the forecast ensemble.\n\nThe theoretical distribution of the forecast state can be derived from the properties of affine transformations of Gaussian random vectors. The mean of the forecast distribution is:\n$$\n\\mathbb{E}[x_{f}] = \\mathbb{E}[\\Phi x_{a} + w] = \\Phi \\mathbb{E}[x_{a}] + \\mathbb{E}[w] = \\Phi m^{a} + 0 = \\Phi m^{a}\n$$\nSince $m^{a} = 0$, the forecast mean is also $0$. The covariance of the forecast distribution, which we denote by $\\Sigma_{f}$, is:\n$$\n\\Sigma_{f} = \\operatorname{Cov}(\\Phi x_{a} + w)\n$$\nBecause the analysis state $x_{a}$ and the process noise $w$ are independent, the covariance of their sum is the sum of their covariances:\n$$\n\\Sigma_{f} = \\operatorname{Cov}(\\Phi x_{a}) + \\operatorname{Cov}(w) = \\Phi \\operatorname{Cov}(x_{a}) \\Phi^{\\top} + Q = \\Phi P^{a} \\Phi^{\\top} + Q\n$$\nThis matrix, $\\Sigma_{f}$, is the exact or theoretical forecast error covariance. The forecast ensemble members $x_{f}^{(i)}$ are thus independent samples from the distribution $\\mathcal{N}(0, \\Sigma_{f})$.\n\nThe algorithm to be implemented follows a sequence of well-defined steps to verify this theoretical result against a numerical simulation.\n\n1.  **Generate the analysis ensemble $\\{x_{a}^{(i)}\\}_{i=1}^{N}$**: To draw samples from $\\mathcal{N}(0, P^{a})$, we first find a matrix $L_{a}$ such that $P^{a} = L_{a} L_{a}^{\\top}$. The Cholesky decomposition provides such a lower-triangular matrix $L_{a}$, provided $P^{a}$ is symmetric and positive-definite. We then generate $N$ independent samples $\\zeta^{(i)}$ from the standard multivariate normal distribution, $\\zeta^{(i)} \\sim \\mathcal{N}(0, I_{d})$, where $I_{d}$ is the $d \\times d$ identity matrix. Each analysis ensemble member is then formed by the transformation $x_{a}^{(i)} = L_{a} \\zeta^{(i)}$.\n\n2.  **Sample the process noise $\\{w^{(i)}\\}_{i=1}^{N}$**: This step is analogous to the first. We compute the Cholesky decomposition of the process noise covariance matrix, $Q = L_{q} L_{q}^{\\top}$. We then generate another set of $N$ independent standard normal vectors $\\xi^{(i)} \\sim \\mathcal{N}(0, I_{d})$ and compute the noise samples as $w^{(i)} = L_{q} \\xi^{(i)}$.\n\n3.  **Form the forecast ensemble $\\{x_{f}^{(i)}\\}_{i=1}^{N}$**: Each forecast member is computed by applying the model dynamics to the corresponding analysis member: $x_{f}^{(i)} = \\Phi x_{a}^{(i)} + w^{(i)}$.\n\n4.  **Compute the empirical forecast covariance $\\hat{P}^{f}$**: From the generated forecast ensemble, we estimate the covariance. The unbiased sample covariance matrix is given by:\n    $$\n    \\hat{P}^{f} = \\frac{1}{N-1}\\sum_{i=1}^{N}\\left(x_{f}^{(i)} - \\bar{x}_{f}\\right)\\left(x_{f}^{(i)} - \\bar{x}_{f}\\right)^{\\top}\n    $$\n    where $\\bar{x}_{f} = \\frac{1}{N}\\sum_{i=1}^{N} x_{f}^{(i)}$ is the sample mean of the forecast ensemble.\n\n5.  **Compute the theoretical forecast covariance $\\Sigma_{f}$**: This is calculated directly from the given matrices using the derived formula: $\\Sigma_{f} = \\Phi P^{a} \\Phi^{\\top} + Q$.\n\n6.  **Compare empirical and theoretical results**: The discrepancy between the sample covariance $\\hat{P}^{f}$ and the true covariance $\\Sigma_{f}$ is a result of sampling error. This error is quantified in two ways. First, the direct discrepancy is measured by the Frobenius norm of the difference: $\\Delta = \\lVert \\hat{P}^{f} - \\Sigma_{f} \\rVert_{F}$. Second, a theoretical estimate of the root-mean-square (RMS) sampling error is computed based on the properties of the Wishart distribution, from which the sample covariance is drawn. The expected squared error is given as $\\mathbb{E}\\left[\\lVert \\hat{P}^{f} - \\Sigma_{f} \\rVert_{F}^{2}\\right] = \\frac{1}{N-1}\\left(\\lVert \\Sigma_{f} \\rVert_{F}^{2} + (\\operatorname{tr}\\Sigma_{f})^{2}\\right)$. The RMS error is the square root of this quantity:\n    $$\n    \\varepsilon_{\\mathrm{RMS}} = \\sqrt{\\frac{\\lVert \\Sigma_{f} \\rVert_{F}^{2} + (\\operatorname{tr}\\Sigma_{f})^{2}}{N-1}}\n    $$\n    A test case is considered to pass if the observed discrepancy $\\Delta$ is within a reasonable multiple of this expected statistical error. The problem specifies this criterion as $\\Delta \\leq c \\, \\varepsilon_{\\mathrm{RMS}}$ with a tolerance factor of $c = 3$, which is analogous to a \"3-sigma\" confidence interval. This check confirms that the simulation behaves as predicted by statistical theory. For reproducibility, the pseudo-random number generator is seeded with a fixed value for each test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the stochastic forecast step verification.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"d\": 3, \"N\": 400, \"seed\": 0,\n            \"Phi\": np.array([\n                [0.9, 0.1, 0.0],\n                [0.0, 0.7, 0.2],\n                [0.0, 0.0, 0.8]\n            ]),\n            \"Pa\": np.array([\n                [0.5, 0.1, 0.0],\n                [0.1, 0.4, 0.05],\n                [0.0, 0.05, 0.3]\n            ]),\n            \"Q\": np.array([\n                [0.2, 0.05, 0.0],\n                [0.05, 0.1, 0.02],\n                [0.0, 0.02, 0.15]\n            ])\n        },\n        {\n            \"d\": 1, \"N\": 200, \"seed\": 1,\n            \"Phi\": np.array([[1.2]]),\n            \"Pa\": np.array([[0.25]]),\n            \"Q\": np.array([[0.5]])\n        },\n        {\n            \"d\": 5, \"N\": 2000, \"seed\": 2,\n            \"Phi\": np.array([\n                [0.6, 0.2, 0.0, 0.0, 0.0],\n                [0.0, 0.7, 0.1, 0.0, 0.0],\n                [0.0, 0.0, 0.5, 0.2, 0.0],\n                [0.0, 0.0, 0.0, 0.65, 0.1],\n                [0.0, 0.0, 0.0, 0.0, 0.55]\n            ]),\n            \"Pa\": np.array([\n                [1.0, 0.1, 0.0, 0.0, 0.0],\n                [0.1, 0.8, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.6, 0.05, 0.0],\n                [0.0, 0.0, 0.05, 0.7, 0.0],\n                [0.0, 0.0, 0.0, 0.0, 0.9]\n            ]),\n            \"Q\": np.array([\n                [0.3, 0.04, 0.0, 0.0, 0.0],\n                [0.04, 0.25, 0.03, 0.0, 0.0],\n                [0.0, 0.03, 0.2, 0.02, 0.0],\n                [0.0, 0.0, 0.02, 0.15, 0.01],\n                [0.0, 0.0, 0.0, 0.01, 0.35]\n            ])\n        },\n        {\n            \"d\": 4, \"N\": 30, \"seed\": 3,\n            \"Phi\": np.array([\n                [0.85, 0.1, 0.0, 0.0],\n                [0.0, 0.75, 0.15, 0.0],\n                [0.0, 0.0, 0.65, 0.1],\n                [0.0, 0.0, 0.0, 0.7]\n            ]),\n            \"Pa\": np.array([\n                [0.6, 0.05, 0.0, 0.0],\n                [0.05, 0.5, 0.04, 0.0],\n                [0.0, 0.04, 0.4, 0.03],\n                [0.0, 0.0, 0.03, 0.45]\n            ]),\n            \"Q\": np.array([\n                [0.25, 0.03, 0.0, 0.0],\n                [0.03, 0.2, 0.02, 0.0],\n                [0.0, 0.02, 0.18, 0.01],\n                [0.0, 0.0, 0.01, 0.22]\n            ])\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        d, N, seed = case[\"d\"], case[\"N\"], case[\"seed\"]\n        Phi, Pa, Q = case[\"Phi\"], case[\"Pa\"], case[\"Q\"]\n        ma = np.zeros(d)\n        \n        # Initialize the random number generator with a fixed seed for reproducibility\n        rng = np.random.default_rng(seed)\n\n        # 1. Generate an analysis ensemble {x_a^(i)} from N(0, Pa)\n        La = np.linalg.cholesky(Pa)\n        zeta = rng.standard_normal(size=(d, N))\n        xa_ensemble = La @ zeta # Each column is a sample x_a^(i)\n\n        # 2. Sample independent process noises {w^(i)} from N(0, Q)\n        Lq = np.linalg.cholesky(Q)\n        xi = rng.standard_normal(size=(d, N))\n        w_ensemble = Lq @ xi # Each column is a sample w^(i)\n\n        # 3. Form forecast ensemble members\n        xf_ensemble = Phi @ xa_ensemble + w_ensemble\n\n        # 4. Compute the unbiased sample covariance of the forecast ensemble\n        # np.cov computes the unbiased covariance by default (ddof=1)\n        P_f_hat = np.cov(xf_ensemble)\n\n        # 5. Compute the theoretical forecast covariance\n        Sigma_f = Phi @ Pa @ Phi.T + Q\n\n        # 6. Compute discrepancy and RMS sampling error\n        # Frobenius-norm discrepancy\n        Delta = np.linalg.norm(P_f_hat - Sigma_f, 'fro')\n\n        # RMS sampling error predicted by Wishart theory\n        norm_Sigma_f_sq = np.linalg.norm(Sigma_f, 'fro')**2\n        tr_Sigma_f_sq = np.trace(Sigma_f)**2\n        eps_rms = np.sqrt((norm_Sigma_f_sq + tr_Sigma_f_sq) / (N - 1))\n\n        # Declare pass if Delta is within 3 * eps_rms\n        c = 3\n        passes = Delta = c * eps_rms\n        results.append(passes)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "Once the forecast is made, the ensemble is updated using observations. A key feature of the stochastic EnKF is the addition of random perturbations to the observations to ensure the analysis covariance is correctly estimated. This exercise uses a simplified scalar model to demonstrate the critical importance of scaling these perturbations correctly, proving that their variance must match the specified observation error variance $R$. Working through this calculation will reveal how incorrect scaling leads to a flawed estimate of the analysis uncertainty .",
            "id": "3422877",
            "problem": "Consider a linear Gaussian data assimilation setting in one spatial dimension with a linear observation operator. The prior (forecast) state is a scalar random variable with distribution $\\mathcal{N}(m^{f},P^{f})$, and the observation is given by $y = H x + v$, where $H$ is a known scalar and the observation error $v$ is distributed as $\\mathcal{N}(0,R)$, independent of the prior state. An Ensemble Kalman Filter (EnKF) with perturbed observations is used. Each analysis ensemble member applies the affine update\n$$\nx^{a} = x^{f} + K\\bigl(y + \\epsilon - H x^{f}\\bigr),\n$$\nwhere $K$ is the Kalman gain computed from the forecast covariance and the specified observation error covariance, the forecast $x^{f}$ is a draw from $\\mathcal{N}(m^{f},P^{f})$, and the perturbations $\\epsilon$ are independently drawn from $\\mathcal{N}(0,\\alpha^{2} R)$ with a fixed scalar $\\alpha \\neq 1$.\n\nStarting only from the linearity of the update map and the independence and second-moment properties of Gaussian random variables, derive the expected analysis covariance as a function of $K$, $H$, $P^{f}$, $R$, and $\\alpha$, by taking the covariance of the affine transformation that maps $\\bigl(x^{f},\\epsilon\\bigr)$ to $x^{a}$. Then, using the specific numerical values $H=1$, $P^{f}=2$, $R=1$, and $\\alpha = \\tfrac{6}{5}$, compute the scalar deviation\n$$\n\\Delta \\equiv \\operatorname{Var}(x^{a}\\,|\\,\\alpha) - \\operatorname{Var}(x^{a}\\,|\\,\\alpha=1),\n$$\nwhere $\\operatorname{Var}(x^{a}\\,|\\,\\alpha)$ denotes the expected analysis variance produced by the stochastic EnKF with perturbed observations scaled by $\\alpha$. The Kalman gain $K$ is to be computed from $P^{f}$, $H$, and $R$.\n\nExpress your final answer as a single real number, and round your result to six significant figures.",
            "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, and all necessary information is provided.\n\n### Step 1: Extract Givens\n- Prior state distribution: $x \\sim \\mathcal{N}(m^{f}, P^{f})$\n- Observation model: $y = H x + v$\n- Observation error distribution: $v \\sim \\mathcal{N}(0, R)$\n- The prior state and observation error are independent.\n- Ensemble Kalman Filter (EnKF) analysis update for each member: $x^{a} = x^{f} + K\\bigl(y + \\epsilon - H x^{f}\\bigr)$\n- Forecast ensemble member: $x^{f}$ is a draw from $\\mathcal{N}(m^{f}, P^{f})$\n- Observation perturbation: $\\epsilon \\sim \\mathcal{N}(0, \\alpha^{2} R)$, with $\\alpha \\neq 1$\n- The perturbations $\\epsilon$ are independent of the forecast members $x^f$.\n- The Kalman gain $K$ is computed from $P^{f}$, $H$, and $R$.\n- Numerical values: $H=1$, $P^{f}=2$, $R=1$, $\\alpha = \\frac{6}{5}$.\n- The objective is to compute the deviation $\\Delta \\equiv \\operatorname{Var}(x^{a}\\,|\\,\\alpha) - \\operatorname{Var}(x^{a}\\,|\\,\\alpha=1)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is set within the standard framework of linear-Gaussian data assimilation and the Ensemble Kalman Filter, a well-established method in statistical estimation. The problem statement is self-contained, providing all necessary definitions, relationships, and numerical values for a unique solution. The model is a simplified but scientifically valid representation used in teaching and research. The terminology is precise and objective. There are no contradictions, missing data, or violations of mathematical or scientific principles. The problem is deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. The solution will proceed.\n\n### Solution Derivation\nThe first step is to derive a general expression for the expected analysis covariance, which in this one-dimensional case is the analysis variance, $\\operatorname{Var}(x^{a})$. The analysis state $x^{a}$ is given as an affine transformation of the random variables $x^{f}$ and $\\epsilon$. We can rewrite the update equation to group these random variables:\n$$\nx^{a} = x^{f} - K H x^{f} + K y + K \\epsilon = (1 - KH)x^{f} + K\\epsilon + Ky\n$$\nIn this expression for a given analysis step, the Kalman gain $K$, the observation operator $H$, and the observation value $y$ are constants. The randomness in $x^{a}$ arises from the forecast ensemble member $x^{f}$ and the random perturbation $\\epsilon$.\n\nThe variance of $x^{a}$ is computed using the properties of variance. For independent random variables $Z_1$ and $Z_2$, and constants $A$, $B$, and $C$, the variance of their linear combination is $\\operatorname{Var}(A Z_1 + B Z_2 + C) = A^2 \\operatorname{Var}(Z_1) + B^2 \\operatorname{Var}(Z_2)$.\nApplying this to the expression for $x^{a}$:\n$$\n\\operatorname{Var}(x^a) = \\operatorname{Var}\\bigl( (1 - KH)x^{f} + K\\epsilon + Ky \\bigr)\n$$\nSince $x^f$ and $\\epsilon$ are independent, and $Ky$ is a constant term that does not affect variance:\n$$\n\\operatorname{Var}(x^a) = \\operatorname{Var}\\bigl( (1 - KH)x^{f} \\bigr) + \\operatorname{Var}(K\\epsilon)\n$$\n$$\n\\operatorname{Var}(x^a) = (1 - KH)^2 \\operatorname{Var}(x^f) + K^2 \\operatorname{Var}(\\epsilon)\n$$\nWe are given that $\\operatorname{Var}(x^f) = P^f$ and $\\operatorname{Var}(\\epsilon) = \\alpha^2 R$. Substituting these into the equation gives the expected analysis variance as a function of the model parameters:\n$$\n\\operatorname{Var}(x^{a}\\,|\\,\\alpha) = (1 - KH)^2 P^f + K^2 \\alpha^2 R\n$$\nThis completes the first part of the problem. Now, we proceed with the numerical calculation.\n\nFirst, we compute the Kalman gain $K$. For a scalar system, the formula is:\n$$\nK = P^f H (H^2 P^f + R)^{-1} = \\frac{P^f H}{H^2 P^f + R}\n$$\nSubstituting the given numerical values $H=1$, $P^f=2$, and $R=1$:\n$$\nK = \\frac{(2)(1)}{(1)^2(2) + 1} = \\frac{2}{2+1} = \\frac{2}{3}\n$$\nNow we can write the expression for $\\operatorname{Var}(x^{a}\\,|\\,\\alpha)$ using the numerical values of $K$, $H$, $P^f$, and $R$:\n$$\n\\operatorname{Var}(x^{a}\\,|\\,\\alpha) = \\left(1 - \\frac{2}{3} \\cdot 1\\right)^2 (2) + \\left(\\frac{2}{3}\\right)^2 \\alpha^2 (1)\n$$\n$$\n\\operatorname{Var}(x^{a}\\,|\\,\\alpha) = \\left(\\frac{1}{3}\\right)^2 (2) + \\frac{4}{9} \\alpha^2 = \\frac{1}{9}(2) + \\frac{4}{9}\\alpha^2 = \\frac{2}{9} + \\frac{4}{9}\\alpha^2\n$$\nThe problem requires computing the deviation $\\Delta = \\operatorname{Var}(x^{a}\\,|\\,\\alpha) - \\operatorname{Var}(x^{a}\\,|\\,\\alpha=1)$ for the specific case where $\\alpha = \\frac{6}{5}$.\n\nFirst, we compute $\\operatorname{Var}(x^{a}\\,|\\,\\alpha=\\frac{6}{5})$:\n$$\n\\operatorname{Var}\\left(x^{a}\\,\\Big|\\,\\alpha=\\frac{6}{5}\\right) = \\frac{2}{9} + \\frac{4}{9}\\left(\\frac{6}{5}\\right)^2 = \\frac{2}{9} + \\frac{4}{9}\\left(\\frac{36}{25}\\right) = \\frac{2}{9} + \\frac{144}{225}\n$$\nTo add these fractions, we find a common denominator, which is $225$:\n$$\n\\operatorname{Var}\\left(x^{a}\\,\\Big|\\,\\alpha=\\frac{6}{5}\\right) = \\frac{2 \\cdot 25}{9 \\cdot 25} + \\frac{144}{225} = \\frac{50}{225} + \\frac{144}{225} = \\frac{194}{225}\n$$\nNext, we compute $\\operatorname{Var}(x^a\\,|\\,\\alpha=1)$. This corresponds to the standard stochastic EnKF where the perturbations have the same variance as the observation error.\n$$\n\\operatorname{Var}(x^{a}\\,|\\,\\alpha=1) = \\frac{2}{9} + \\frac{4}{9}(1)^2 = \\frac{2}{9} + \\frac{4}{9} = \\frac{6}{9} = \\frac{2}{3}\n$$\nFinally, we compute the deviation $\\Delta$:\n$$\n\\Delta = \\operatorname{Var}\\left(x^{a}\\,\\Big|\\,\\alpha=\\frac{6}{5}\\right) - \\operatorname{Var}(x^{a}\\,|\\,\\alpha=1) = \\frac{194}{225} - \\frac{2}{3}\n$$\nAgain, using the common denominator $225$:\n$$\n\\Delta = \\frac{194}{225} - \\frac{2 \\cdot 75}{3 \\cdot 75} = \\frac{194}{225} - \\frac{150}{225} = \\frac{194 - 150}{225} = \\frac{44}{225}\n$$\nTo provide the final answer as a single real number rounded to six significant figures, we perform the division:\n$$\n\\Delta = \\frac{44}{225} \\approx 0.1955555...\n$$\nRounding to six significant figures gives $0.195556$.",
            "answer": "$$\\boxed{0.195556}$$"
        },
        {
            "introduction": "Beyond just the magnitude of observation errors, their structure—specifically, the correlation between different observations—plays a vital role. This final practice explores a common pitfall: ignoring off-diagonal terms in the observation error covariance matrix $R$. Through a carefully designed thought experiment, you will see how failing to generate perturbations with the correct covariance structure introduces significant bias into the system, even in a very simple two-dimensional case . This highlights the necessity of correctly representing error correlations in the assimilation process.",
            "id": "3422876",
            "problem": "Consider a two-dimensional linear-Gaussian data assimilation setting for a stochastic Ensemble Kalman Filter (EnKF) with perturbed observations. Let the observation operator be the identity, $H = I_{2}$, and suppose the forecast ensemble has zero spread, meaning $x_{f}^{(i)} = \\bar{x}_{f}$ for all ensemble members $i$, so that the forecast ensemble anomalies are identically zero. Let the observation errors be Gaussian with zero mean and non-diagonal covariance\n$$\nR \\;=\\; \\begin{pmatrix} 2  1.2 \\\\[4pt] 1.2  1 \\end{pmatrix}.\n$$\nIn the stochastic EnKF with perturbed observations, each ensemble member’s innovation is formed using a synthetic observation perturbation $\\eta^{(i)}$. The unbiased case requires drawing $\\eta^{(i)}$ as a linear transform of standard normal noise $\\xi^{(i)} \\sim \\mathcal{N}(0, I_{2})$ via a matrix $L$ such that $L L^{\\top} = R$ (for example, a Cholesky factor). A common incorrect practice is to ignore the off-diagonal entries of $R$ and draw $\\eta^{(i)} = D \\xi^{(i)}$ with $D = \\mathrm{diag}(\\sqrt{2}, 1)$ so that $\\mathrm{cov}(\\eta^{(i)}) = \\mathrm{diag}(2, 1)$.\n\nUsing only fundamental definitions of covariance and properties of linear transformations of Gaussian random vectors, and the fact that the unbiased sample covariance with factor $1/(N-1)$ is an unbiased estimator of the population covariance, derive the expected $(1,2)$ entry of the ensemble innovation covariance under the incorrect practice described above, and compute its bias relative to the correct $(1,2)$ entry implied by $R$.\n\nState your final answer as the expected bias in the $(1,2)$ entry (incorrect minus correct), as a single real number, rounded to four significant figures. Treat the quantity as unitless.",
            "solution": "This problem asks for the bias in the $(1,2)$ entry of the ensemble innovation covariance matrix that arises from an incorrect procedure for generating observation perturbations in a stochastic Ensemble Kalman Filter (EnKF). The bias is defined as the expected value of the incorrect entry minus the correct value.\n\nFirst, we formalize the quantities involved. The data assimilation is performed in a $2$-dimensional space. The observation operator is the identity matrix, $H = I_{2}$. The true observation error covariance is given by the matrix\n$$\nR \\;=\\; \\begin{pmatrix} 2  1.2 \\\\ 1.2  1 \\end{pmatrix}.\n$$\nA crucial condition is that the forecast ensemble has zero spread, meaning all ensemble members are identical to the ensemble mean: $x_{f}^{(i)} = \\bar{x}_{f}$ for all members $i = 1, \\dots, N$. This implies that the forecast ensemble anomalies, $x_{f}^{(i)} - \\bar{x}_{f}$, are all zero vectors, and consequently, the forecast ensemble covariance matrix $P_{f}^{e}$ is the zero matrix, $P_{f}^{e} = 0$.\n\nIn a stochastic EnKF with perturbed observations, each ensemble member is updated using a synthetic observation, $y^{(i)}$, created by adding a random perturbation, $\\eta^{(i)}$, to the actual observation, $y$. Thus, $y^{(i)} = y + \\eta^{(i)}$. The innovation for the $i$-th ensemble member, $d^{(i)}$, is defined as the difference between its synthetic observation and its projected forecast:\n$$\nd^{(i)} \\;=\\; y^{(i)} - H x_{f}^{(i)}.\n$$\nGiven $H = I_{2}$ and $x_{f}^{(i)} = \\bar{x}_{f}$, this becomes:\n$$\nd^{(i)} \\;=\\; (y + \\eta^{(i)}) - I_{2} \\bar{x}_{f} \\;=\\; y - \\bar{x}_{f} + \\eta^{(i)}.\n$$\nThe ensemble innovation covariance, which we denote as $S^e$, is the sample covariance of these innovation vectors. It is calculated as:\n$$\nS^e \\;=\\; \\frac{1}{N-1} \\sum_{i=1}^{N} (d^{(i)} - \\bar{d})(d^{(i)} - \\bar{d})^T,\n$$\nwhere $\\bar{d}$ is the mean of the innovation ensemble. The mean innovation is:\n$$\n\\bar{d} \\;=\\; \\frac{1}{N} \\sum_{i=1}^{N} d^{(i)} \\;=\\; \\frac{1}{N} \\sum_{i=1}^{N} (y - \\bar{x}_{f} + \\eta^{(i)}) \\;=\\; y - \\bar{x}_{f} + \\frac{1}{N} \\sum_{i=1}^{N} \\eta^{(i)} \\;=\\; y - \\bar{x}_{f} + \\bar{\\eta},\n$$\nwhere $\\bar{\\eta}$ is the sample mean of the perturbations.\n\nNow, we can find the innovation anomaly for member $i$:\n$$\nd^{(i)} - \\bar{d} \\;=\\; (y - \\bar{x}_{f} + \\eta^{(i)}) - (y - \\bar{x}_{f} + \\bar{\\eta}) \\;=\\; \\eta^{(i)} - \\bar{\\eta}.\n$$\nSubstituting this back into the formula for $S^e$, we find that due to the zero forecast spread, the ensemble innovation covariance is exactly the sample covariance of the observation perturbations:\n$$\nS^e \\;=\\; \\frac{1}{N-1} \\sum_{i=1}^{N} (\\eta^{(i)} - \\bar{\\eta})(\\eta^{(i)} - \\bar{\\eta})^T.\n$$\nThe problem asks for the expected value of the $(1,2)$ entry of this matrix, $E[(S^e)_{1,2}]$. Since expectation is a linear operator, this is equal to $(E[S^e])_{1,2}$. The problem statement provides the fact that the sample covariance calculated with the factor $1/(N-1)$ is an unbiased estimator of the population covariance. Therefore, the expected value of the sample covariance matrix $S^e$ is the population covariance of the random vectors $\\eta^{(i)}$:\n$$\nE[S^e] \\;=\\; \\mathrm{cov}(\\eta^{(i)}).\n$$\nThe \"incorrect practice\" described consists of drawing the perturbations $\\eta^{(i)}$ from a distribution that ignores the off-diagonal elements of $R$. Specifically, $\\eta^{(i)} = D \\xi^{(i)}$, where $\\xi^{(i)} \\sim \\mathcal{N}(0, I_{2})$ and $D = \\mathrm{diag}(\\sqrt{2}, 1)$. The covariance of these incorrect perturbations is:\n$$\n\\mathrm{cov}(\\eta^{(i)}) \\;=\\; E[\\eta^{(i)}(\\eta^{(i)})^T] \\;=\\; E[D \\xi^{(i)} (\\xi^{(i)})^T D^T] \\;=\\; D E[\\xi^{(i)}(\\xi^{(i)})^T] D^T \\;=\\; D I_{2} D^T \\;=\\; D^2.\n$$\nCalculating $D^2$:\n$$\nD^2 \\;=\\; \\begin{pmatrix} \\sqrt{2}  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} \\sqrt{2}  0 \\\\ 0  1 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}.\n$$\nSo, under the incorrect practice, the expected ensemble innovation covariance is:\n$$\nE[S^{e}_{\\text{incorrect}}] \\;=\\; \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}.\n$$\nThe expected $(1,2)$ entry is therefore $(E[S^{e}_{\\text{incorrect}}])_{1,2} = 0$.\n\nNext, we identify the \"correct $(1,2)$ entry implied by $R$.\" In an EnKF, the ensemble innovation covariance $S^e$ serves as an estimate for the theoretical innovation covariance, $S_{\\text{theory}} = H P_f H^T + R$, where $P_f$ is the true forecast error covariance. Given the zero spread in the forecast ensemble ($P_f^e = 0$), the forecast error covariance $P_f$ is effectively zero. Therefore, $S_{\\text{theory}} = R$. A correct implementation would generate perturbations from $\\mathcal{N}(0,R)$, and the expected value of the resulting $S^e$ would be $R$. Thus, the correct value for the $(1,2)$ entry of the expected innovation covariance is the $(1,2)$ entry of $R$.\nFrom the definition of $R$, the correct value is $R_{1,2} = 1.2$.\n\nFinally, we compute the bias, defined as the incorrect value minus the correct value:\n$$\n\\text{Bias} \\;=\\; (E[S^{e}_{\\text{incorrect}}])_{1,2} - R_{1,2} \\;=\\; 0 - 1.2 \\;=\\; -1.2.\n$$\nThe problem requires the answer to be rounded to four significant figures. The value $-1.2$ expressed with four significant figures is $-1.200$.",
            "answer": "$$\\boxed{-1.200}$$"
        }
    ]
}