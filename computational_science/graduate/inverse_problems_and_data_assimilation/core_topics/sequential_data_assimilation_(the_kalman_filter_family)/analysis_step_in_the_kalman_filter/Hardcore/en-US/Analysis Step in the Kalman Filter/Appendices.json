{
    "hands_on_practices": [
        {
            "introduction": "Before implementing the full Kalman filter, a foundational check is to ensure the dimensional consistency of all matrix and vector operations. This first exercise focuses on the Kalman gain matrix, $K_k$, which is central to the analysis step. By determining its dimensions , you will gain insight into how it acts as a bridge, transforming the innovation—a residual in the measurement space—into a corrective term for the state vector in the state space.",
            "id": "1587017",
            "problem": "An engineer is developing the control system for a self-balancing unicycle. The core of the balancing problem is modeled as a simple inverted pendulum. To maintain balance, the system needs to estimate the state of the pendulum, which is described by its angle with respect to the vertical, $\\theta$, and its angular velocity, $\\dot{\\theta}$. The engineer decides to implement a discrete-time Kalman filter for this state estimation task.\n\nThe state vector for the system is chosen as $x = \\begin{pmatrix} \\theta \\\\ \\dot{\\theta} \\end{pmatrix}$. The available sensor is an onboard Inertial Measurement Unit (IMU) that provides a direct measurement of the tilt angle $\\theta$ at each discrete time step. The angular velocity $\\dot{\\theta}$ is not directly measured.\n\nThe standard linear state-space representation used for the Kalman filter is:\nState Equation: $x_{k} = A x_{k-1} + B u_{k-1} + w_{k-1}$\nMeasurement Equation: $z_{k} = H x_{k} + v_{k}$\n\nwhere $x_k$ is the state vector at time step $k$, $z_k$ is the measurement vector, $u_k$ is the control input vector, $w_k$ is the process noise vector, and $v_k$ is the measurement noise vector. The matrices $A$, $B$, and $H$ are the state transition matrix, control input matrix, and measurement matrix, respectively. The Kalman filter computes an optimal estimate of the state by blending the prediction from the model with the information from the new measurement. A key component in this process is the Kalman gain matrix, $K_k$.\n\nGiven the definition of the state vector and the description of the measurement, what are the dimensions of the Kalman gain matrix $K_k$?\n\nA. 1 row, 1 column\n\nB. 1 row, 2 columns\n\nC. 2 rows, 1 column\n\nD. 2 rows, 2 columns\n\nE. The dimensions cannot be determined without knowing the matrix $A$.",
            "solution": "The state vector is defined as $x_{k} = \\begin{pmatrix} \\theta \\\\ \\dot{\\theta} \\end{pmatrix}$, so the state dimension is $n=2$, i.e., $x_{k} \\in \\mathbb{R}^{2}$.\n\nThe measurement is a direct observation of the tilt angle $\\theta$ only, so the measurement vector has dimension $m=1$, i.e., $z_{k} \\in \\mathbb{R}^{1}$. Therefore, the measurement matrix must have dimensions $H \\in \\mathbb{R}^{m \\times n} = \\mathbb{R}^{1 \\times 2}$, consistent with $z_{k} = H x_{k} + v_{k}$.\n\nIn the Kalman filter correction (update) step, the innovation is defined as $y_{k} = z_{k} - H \\hat{x}_{k}^{-} \\in \\mathbb{R}^{m}$, and the state update is\n$$\n\\hat{x}_{k} = \\hat{x}_{k}^{-} + K_{k} y_{k},\n$$\nwhere $\\hat{x}_{k}^{-} \\in \\mathbb{R}^{n}$. For the sum to be well-defined, $K_{k} y_{k}$ must be in $\\mathbb{R}^{n}$, and since $y_{k} \\in \\mathbb{R}^{m}$, it follows that $K_{k} \\in \\mathbb{R}^{n \\times m} = \\mathbb{R}^{2 \\times 1}$.\n\nEquivalently, using the standard formula for the Kalman gain,\n$$\nK_{k} = P_{k}^{-} H^{T} \\left(H P_{k}^{-} H^{T} + R\\right)^{-1},\n$$\nwith $P_{k}^{-} \\in \\mathbb{R}^{n \\times n} = \\mathbb{R}^{2 \\times 2}$, $H^{T} \\in \\mathbb{R}^{n \\times m} = \\mathbb{R}^{2 \\times 1}$, and $\\left(H P_{k}^{-} H^{T} + R\\right) \\in \\mathbb{R}^{1 \\times 1}$. Thus $P_{k}^{-} H^{T} \\in \\mathbb{R}^{2 \\times 1}$ and multiplication by the inverse scalar preserves the dimension $\\mathbb{R}^{2 \\times 1}$.\n\nTherefore, the Kalman gain has $2$ rows and $1$ column, corresponding to option C.",
            "answer": "$$\\boxed{C}$$"
        },
        {
            "introduction": "While the textbook formulas for the Kalman filter analysis step are conceptually clear, their direct implementation can suffer from numerical instability, leading to non-physical, non-symmetric, or non-positive definite covariance matrices. This advanced practice challenges you to derive and implement two numerically robust alternatives for the covariance update: a square-root information (SRI) approach and a factorized covariance update using the Joseph form . Mastering these techniques is a critical step towards building professional-grade data assimilation systems that remain reliable even in ill-conditioned scenarios.",
            "id": "3364773",
            "problem": "Consider a linear-Gaussian data assimilation setting suitable for the analysis step in the Kalman Filter (KF). Let the prior (forecast) state be a random vector $x \\in \\mathbb{R}^n$ with Gaussian distribution having mean $x_f \\in \\mathbb{R}^n$ and covariance $P_f \\in \\mathbb{R}^{n \\times n}$ that is Symmetric Positive Definite (SPD). Observations $y \\in \\mathbb{R}^m$ are related to the state by a linear observation operator $H \\in \\mathbb{R}^{m \\times n}$ and additive Gaussian noise $v \\in \\mathbb{R}^m$ with zero mean and SPD covariance $R \\in \\mathbb{R}^{m \\times m}$. The observation model is $y = H x + v$, and the goal of the analysis step is to obtain the posterior covariance $P_a \\in \\mathbb{R}^{n \\times n}$.\n\nStarting from the fundamental base of the linear-Gaussian model, the independence of the prior and observation error, and the fact that the negative log-posterior is a strictly convex quadratic whose Hessian equals the posterior information matrix, derive two numerically stable computation strategies for the analysis covariance that do not require forming any full matrix inverses explicitly:\n- A square-root information approach based on Orthogonal-Triangular decomposition (QR) of a carefully constructed stacked matrix of information square-roots, leveraging the invariance of the Gram matrix under orthogonal transformations.\n- A factorized covariance update using the Schur complement and the Cholesky factor of the innovation covariance, exploiting that the posterior covariance can be expressed as a low-rank reduction of the prior covariance in the space mapped by the observation operator.\n\nImplement both strategies and verify their numerical equivalence on the following test suite. For each test case, compute:\n1. The relative Frobenius norm difference between the two posterior covariance outputs, defined as $\\|P_a^{(\\mathrm{sr})} - P_a^{(\\mathrm{fac})}\\|_F / \\|P_a^{(\\mathrm{fac})}\\|_F$, where $P_a^{(\\mathrm{sr})}$ is obtained via the square-root information QR method and $P_a^{(\\mathrm{fac})}$ via the factorized covariance update.\n2. A boolean indicating whether the trace decreases in the Loewner sense, i.e., whether $\\mathrm{tr}(P_a^{(\\mathrm{fac})}) \\le \\mathrm{tr}(P_f)$ within a small numerical tolerance.\n3. A boolean indicating whether $P_a^{(\\mathrm{fac})}$ is numerically SPD, i.e., its smallest eigenvalue exceeds a small positive tolerance.\n\nYour program must compute these three quantities for each test case and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case’s result is itself a bracketed triple in the order described above, for example, $[\\,[r_1,b_1,c_1],\\,[r_2,b_2,c_2]\\,]$.\n\nUse the following test suite with $n=4$ and $m \\in \\{2,3\\}$:\n\nTest case $1$ (happy path, moderate correlations):\n$$\nP_f^{(1)} = \\begin{bmatrix}\n1.6 & 0.1 & -0.2 & 0.0 \\\\\n0.1 & 1.2 & 0.3 & 0.1 \\\\\n-0.2 & 0.3 & 1.8 & 0.2 \\\\\n0.0 & 0.1 & 0.2 & 1.1\n\\end{bmatrix},\\quad\nH^{(1)} = \\begin{bmatrix}\n1.0 & 0.0 & 0.5 & -0.2 \\\\\n0.1 & 1.0 & -0.3 & 0.0 \\\\\n0.0 & -0.1 & 0.2 & 1.0\n\\end{bmatrix},\\quad\nR^{(1)} = \\begin{bmatrix}\n0.4 & 0 & 0 \\\\\n0 & 0.6 & 0 \\\\\n0 & 0 & 0.9\n\\end{bmatrix}.\n$$\n\nTest case $2$ (near-singular prior covariance, observation targets small-variance directions):\n$$\nP_f^{(2)} = \\begin{bmatrix}\n1.01\\times 10^{-6} & 1.0\\times 10^{-8} & 0 & 0 \\\\\n1.0\\times 10^{-8} & 1.0001\\times 10^{-4} & 0 & 0 \\\\\n0 & 0 & 1.0 & 0.01 \\\\\n0 & 0 & 0.01 & 100.0\n\\end{bmatrix},\\quad\nH^{(2)} = \\begin{bmatrix}\n0.0 & 10.0 & 0.0 & 0.0 \\\\\n0.1 & 0.0 & 0.5 & 0.0\n\\end{bmatrix},\\quad\nR^{(2)} = \\begin{bmatrix}\n0.05 & 0 \\\\\n0 & 0.2\n\\end{bmatrix}.\n$$\n\nTest case $3$ (high observation noise, small analysis effect):\n$$\nP_f^{(3)} = \\begin{bmatrix}\n2.0 & 0.3 & 0.0 & -0.1 \\\\\n0.3 & 1.5 & 0.2 & 0.0 \\\\\n0.0 & 0.2 & 1.0 & 0.3 \\\\\n-0.1 & 0.0 & 0.3 & 1.3\n\\end{bmatrix},\\quad\nH^{(3)} = \\begin{bmatrix}\n0.5 & -0.2 & 1.0 & 0.0 \\\\\n0.0 & 0.7 & 0.0 & -0.5 \\\\\n0.3 & 0.0 & 0.2 & 0.1\n\\end{bmatrix},\\quad\nR^{(3)} = \\begin{bmatrix}\n1000.0 & 0 & 0 \\\\\n0 & 500.0 & 0 \\\\\n0 & 0 & 800.0\n\\end{bmatrix}.\n$$\n\nTest case $4$ (zero observation operator, analysis equals prior):\n$$\nP_f^{(4)} = \\begin{bmatrix}\n1.0 & 0.2 & 0.1 & 0.0 \\\\\n0.2 & 1.3 & 0.0 & 0.2 \\\\\n0.1 & 0.0 & 1.1 & 0.3 \\\\\n0.0 & 0.2 & 0.3 & 1.2\n\\end{bmatrix},\\quad\nH^{(4)} = \\begin{bmatrix}\n0.0 & 0.0 & 0.0 & 0.0 \\\\\n0.0 & 0.0 & 0.0 & 0.0 \\\\\n0.0 & 0.0 & 0.0 & 0.0\n\\end{bmatrix},\\quad\nR^{(4)} = \\begin{bmatrix}\n1.0 & 0 & 0 \\\\\n0 & 1.0 & 0 \\\\\n0 & 0 & 1.0\n\\end{bmatrix}.\n$$\n\nNumerical requirements:\n- All computations must treat matrices as real-valued and must use stable factorizations (Cholesky for SPD matrices, QR for rectangular stacked matrices).\n- Do not form any full matrix inverses directly; only solve triangular systems resulting from factorizations when necessary.\n- Use a numerical tolerance of $10^{-12}$ for the SPD and trace-decrease checks.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a bracketed list $[r,b,c]$ with $r$ a floating-point number and $b,c$ booleans, for example, $[[0.0,True,True],[\\dots]]$.",
            "solution": "The problem requires the derivation and implementation of two numerically stable methods for computing the posterior covariance matrix, $P_a$, in the analysis step of a Kalman filter. This corresponds to a Bayesian update for a linear-Gaussian system.\n\n### Fundamental Principles of Bayesian Analysis\n\nThe foundation of the Kalman filter is Bayesian inference. Given a prior probability distribution for the state, $p(x)$, and a likelihood function based on observations, $p(y|x)$, Bayes' theorem gives the posterior distribution:\n$$p(x|y) = \\frac{p(y|x)p(x)}{p(y)} \\propto p(y|x)p(x)$$\nIn our linear-Gaussian setting, the prior is $x \\sim \\mathcal{N}(x_f, P_f)$ and the observation model is $y = Hx+v$ where the noise is $v \\sim \\mathcal{N}(0,R)$. The prior and observation noise are assumed independent. The respective probability density functions (PDFs), ignoring normalization constants, are:\n$$p(x) \\propto \\exp\\left( -\\frac{1}{2} (x - x_f)^T P_f^{-1} (x - x_f) \\right)$$\n$$p(y|x) \\propto \\exp\\left( -\\frac{1}{2} (y - Hx)^T R^{-1} (y - Hx) \\right)$$\nThe posterior PDF is proportional to their product. Since the product of two Gaussian exponentials is another Gaussian exponential, the posterior is also Gaussian, $x|y \\sim \\mathcal{N}(x_a, P_a)$. The exponent of the posterior PDF is the sum of the exponents of the prior and the likelihood:\n$$J(x) = (x - x_f)^T P_f^{-1} (x - x_f) + (y - Hx)^T R^{-1} (y - Hx)$$\nThe posterior covariance $P_a$ is the inverse of the Hessian of the negative log-posterior, $\\frac{1}{2}J(x)$, with respect to $x$. Expanding the quadratic forms and taking the second derivative with respect to $x$ yields the posterior information matrix (the inverse of the posterior covariance):\n$$\\nabla_x^2 \\left(\\frac{1}{2}J(x)\\right) = P_f^{-1} + H^T R^{-1} H$$\nThus, the posterior covariance is given by:\n$$P_a = (P_f^{-1} + H^T R^{-1} H)^{-1}$$\nDirect computation using this formula is numerically unstable and inefficient, as it requires three matrix inversions. We will derive two stable alternatives that avoid explicit inversion of non-triangular matrices.\n\n### Strategy 1: Square-Root Information (SRI) QR-Based Method\n\nThis approach operates on the information form $P_a^{-1} = P_f^{-1} + H^T R^{-1} H$ and utilizes matrix square-roots and orthogonal decomposition. Let $P_f = L_f L_f^T$ and $R = L_R L_R^T$ be the Cholesky factorizations, where $L_f$ and $L_R$ are lower triangular matrices. Their inverses are related to the information matrices: $P_f^{-1} = (L_f^T)^{-1} L_f^{-1} = (L_f^{-1})^T (L_f^{-1})$ and $R^{-1} = (L_R^{-1})^T (L_R^{-1})$.\nSubstituting these into the expression for $P_a^{-1}$:\n$$P_a^{-1} = (L_f^{-1})^T L_f^{-1} + H^T(L_R^{-1})^T L_R^{-1} H = (L_f^{-1})^T (L_f^{-1}) + (L_R^{-1}H)^T (L_R^{-1}H)$$\nThis sum of outer products can be expressed as the Gram matrix of a stacked matrix $M$:\n$$M = \\begin{bmatrix} L_R^{-1} H \\\\ L_f^{-1} \\end{bmatrix}$$\n$$P_a^{-1} = M^T M$$\nWe now leverage the geometric invariance of the Gram matrix under orthogonal transformations. We perform a thin QR decomposition of the $(m+n) \\times n$ matrix $M$, such that $M = Q R_{u}$, where $Q$ is an $(m+n) \\times n$ matrix with orthonormal columns and $R_u$ is an $n \\times n$ upper triangular matrix. More generally, using a full QR decomposition, $M = \\mathcal{Q} \\begin{bmatrix} R_{a,inv} \\\\ 0 \\end{bmatrix}$, where $\\mathcal{Q}$ is an $(m+n) \\times (m+n)$ orthogonal matrix and $R_{a,inv}$ is the $n \\times n$ upper triangular factor.\nThen the posterior information matrix is:\n$$P_a^{-1} = M^T M = \\left(\\mathcal{Q} \\begin{bmatrix} R_{a,inv} \\\\ 0 \\end{bmatrix}\\right)^T \\left(\\mathcal{Q} \\begin{bmatrix} R_{a,inv} \\\\ 0 \\end{bmatrix}\\right) = \\begin{bmatrix} R_{a,inv}^T & 0 \\end{bmatrix} \\mathcal{Q}^T \\mathcal{Q} \\begin{bmatrix} R_{a,inv} \\\\ 0 \\end{bmatrix} = R_{a,inv}^T R_{a,inv}$$\nThis shows that $R_{a,inv}$ is the upper triangular Cholesky factor of the posterior information matrix $P_a^{-1}$. To obtain $P_a$, we invert this relation:\n$$P_a = (R_{a,inv}^T R_{a,inv})^{-1} = R_{a,inv}^{-1} (R_{a,inv}^T)^{-1} = R_{a,inv}^{-1} (R_{a,inv}^{-1})^T$$\nLet $U_a = R_{a,inv}^{-1}$. $U_a$ is an upper triangular matrix that can be computed efficiently by solving the triangular system $R_{a,inv} U_a = I_n$ using back substitution. The posterior covariance is then $P_a = U_a U_a^T$. The entire procedure avoids general matrix inversions, relying on stable Cholesky and QR factorizations and triangular solves.\n\n### Strategy 2: Factorized Covariance Update via Schur Complement\n\nThis strategy reformulates the problem using the Woodbury matrix identity: $(A+UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1}+VA^{-1}U)^{-1}VA^{-1}$. Applying this to $P_a = (P_f^{-1} + H^T R^{-1} H)^{-1}$ with $A=P_f^{-1}$, $U=H^T$, $C=R^{-1}$, and $V=H$, we get:\n$$P_a = (P_f^{-1})^{-1} - (P_f^{-1})^{-1}H^T((R^{-1})^{-1} + H(P_f^{-1})^{-1}H^T)^{-1}H(P_f^{-1})^{-1}$$\n$$P_a = P_f - P_f H^T (R + H P_f H^T)^{-1} H P_f$$\nThis is the classic covariance update formula. The matrix $S = H P_f H^T + R$ is the innovation covariance, and it relates to the Schur complement of $P_f^{-1}$ in the block matrix $\\begin{bsmallmatrix}-R & H \\\\ H^T & P_f^{-1}\\end{bsmallmatrix}$.\nThe term $K = P_f H^T S^{-1}$ is the Kalman gain. A numerically stable computation proceeds as follows:\n1.  Compute the innovation covariance $S = H P_f H^T + R$.\n2.  Compute the Kalman gain $K$ by solving the linear system $K S = P_f H^T$. Direct inversion $S^{-1}$ is avoided. To solve for $K$, we can solve the equivalent system $S^T K^T = (P_f H^T)^T$. Since $S$ is symmetric ($S^T = S$), this is $S K^T = H P_f$.\n    This $m \\times n$ system is solved efficiently and robustly using the Cholesky factorization of $S = L_S L_S^T$. The system becomes $L_S L_S^T K^T = H P_f$. This is solved in two steps:\n    a. Solve $L_S Y = (H P_f)^T$ for $Y$ using forward substitution.\n    b. Solve $L_S^T K^T = Y$ for $K^T$ using backward substitution.\n3.  Update the covariance. The expression $P_a = P_f - K H P_f$ can suffer from catastrophic cancellation if the update is large. A more robust formulation that guarantees the symmetry and positive semi-definiteness of the result is the Joseph form:\n    $$P_a = (I - KH) P_f (I - KH)^T + K R K^T$$\nThis form expresses $P_a$ as a sum of symmetric positive semi-definite matrices (since $P_f, R$ are SPD), which is numerically favorable for preserving these properties.\n\n### Verification and Numerical Checks\nThe two derived methods, $P_a^{(\\mathrm{sr})}$ (SRI) and $P_a^{(\\mathrm{fac})}$ (Factorized), should yield numerically equivalent results. We verify this by computing:\n1.  **Relative Frobenius Norm Difference**: $\\frac{\\|P_a^{(\\mathrm{sr})} - P_a^{(\\mathrm{fac})}\\|_F}{\\|P_a^{(\\mathrm{fac})}\\|_F}$, which quantifies their numerical discrepancy.\n2.  **Trace Decrease**: Check if $\\mathrm{tr}(P_a) \\le \\mathrm{tr}(P_f)$. The analysis update incorporates new information, which should not increase the total state variance (the trace of the covariance matrix).\n3.  **Symmetric Positive Definite (SPD) Property**: Check if $P_a$ is numerically SPD by verifying that its smallest eigenvalue is positive and greater than a small tolerance.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef compute_pa_sri(Pf, H, R):\n    \"\"\"\n    Computes the posterior covariance using the square-root information (SRI) approach.\n    This method is based on QR decomposition of a stacked matrix of information square-roots,\n    leveraging the invariance of the Gram matrix under orthogonal transformations.\n    \"\"\"\n    n, m = H.shape[1], H.shape[0]\n\n    # Step 1  2: Compute Cholesky factors and their inverses (information square-roots)\n    Lf = cholesky(Pf, lower=True)\n    Lr = cholesky(R, lower=True)\n    I_n = np.identity(n)\n    I_m = np.identity(m)\n    Lf_inv = solve_triangular(Lf, I_n, lower=True)\n    Lr_inv = solve_triangular(Lr, I_m, lower=True)\n\n    # Step 3: Form the stacked matrix M\n    M = np.vstack([\n        Lr_inv @ H,\n        Lf_inv\n    ])\n\n    # Step 4: Perform QR decomposition on M. The upper triangular factor R_ainv\n    # is the Cholesky factor of the posterior information matrix P_a_inv.\n    _, R_ainv_full = np.linalg.qr(M)\n    R_ainv = R_ainv_full[:n, :]\n\n    # Step 5: Invert R_ainv to get Ua, a factor of Pa.\n    # Pa = Ua @ Ua.T where Ua = R_ainv^-1.\n    Ua = solve_triangular(R_ainv, I_n, lower=False)\n\n    # Step 6: Compute the posterior covariance Pa\n    Pa_sr = Ua @ Ua.T\n    return Pa_sr\n\ndef compute_pa_fac(Pf, H, R):\n    \"\"\"\n    Computes the posterior covariance using the factorized covariance update (Joseph form).\n    This method uses the Cholesky factor of the innovation covariance and is known\n    for its numerical stability and preservation of covariance matrix properties.\n    \"\"\"\n    n = Pf.shape[0]\n    I_n = np.identity(n)\n\n    # Step 1: Compute innovation terms\n    PfHt = Pf @ H.T\n    S = H @ PfHt + R\n\n    # Step 2: Compute Kalman gain K by solving S @ K.T = (Pf @ H.T).T using Cholesky factorization\n    try:\n        Ls = cholesky(S, lower=True)\n        # Solve Ls @ Y = PfHt.T for Y\n        Y = solve_triangular(Ls, PfHt.T, lower=True)\n        # Solve Ls.T @ K.T = Y for K.T\n        Kt = solve_triangular(Ls.T, Y, lower=False)\n        K = Kt.T\n    except np.linalg.LinAlgError:\n        # Fallback for ill-conditioned S, though not expected in test cases\n        K = PfHt @ np.linalg.inv(S)\n\n\n    # Step 3: Compute the posterior covariance using the Joseph form\n    IKH = I_n - K @ H\n    Pa_fac = IKH @ Pf @ IKH.T + K @ R @ K.T\n    return Pa_fac\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([[1.6, 0.1, -0.2, 0.0], [0.1, 1.2, 0.3, 0.1], [-0.2, 0.3, 1.8, 0.2], [0.0, 0.1, 0.2, 1.1]]),\n            np.array([[1.0, 0.0, 0.5, -0.2], [0.1, 1.0, -0.3, 0.0], [0.0, -0.1, 0.2, 1.0]]),\n            np.array([[0.4, 0, 0], [0, 0.6, 0], [0, 0, 0.9]])\n        ),\n        (\n            np.array([[1.01e-6, 1.0e-8, 0, 0], [1.0e-8, 1.0001e-4, 0, 0], [0, 0, 1.0, 0.01], [0, 0, 0.01, 100.0]]),\n            np.array([[0.0, 10.0, 0.0, 0.0], [0.1, 0.0, 0.5, 0.0]]),\n            np.array([[0.05, 0], [0, 0.2]])\n        ),\n        (\n            np.array([[2.0, 0.3, 0.0, -0.1], [0.3, 1.5, 0.2, 0.0], [0.0, 0.2, 1.0, 0.3], [-0.1, 0.0, 0.3, 1.3]]),\n            np.array([[0.5, -0.2, 1.0, 0.0], [0.0, 0.7, 0.0, -0.5], [0.3, 0.0, 0.2, 0.1]]),\n            np.array([[1000.0, 0, 0], [0, 500.0, 0], [0, 0, 800.0]])\n        ),\n        (\n            np.array([[1.0, 0.2, 0.1, 0.0], [0.2, 1.3, 0.0, 0.2], [0.1, 0.0, 1.1, 0.3], [0.0, 0.2, 0.3, 1.2]]),\n            np.array([[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]]),\n            np.array([[1.0, 0, 0], [0, 1.0, 0], [0, 0, 1.0]])\n        ),\n    ]\n\n    results = []\n    TOL = 1e-12\n    for Pf, H, R in test_cases:\n        # Compute posterior covariance using both methods\n        Pa_sr = compute_pa_sri(Pf, H, R)\n        Pa_fac = compute_pa_fac(Pf, H, R)\n\n        # The Joseph form in Pa_fac is guaranteed to be symmetric. Symmetrizing Pa_sr,\n        # which comes from Ua @ Ua.T, is good practice for subsequent numerical checks.\n        Pa_sr_symm = (Pa_sr + Pa_sr.T) / 2.0\n        \n        # 1. Relative Frobenius norm difference\n        norm_diff = np.linalg.norm(Pa_sr_symm - Pa_fac, 'fro')\n        norm_fac = np.linalg.norm(Pa_fac, 'fro')\n        rel_diff = norm_diff / norm_fac if norm_fac > 1e-15 else 0.0\n\n        # 2. Trace decrease check\n        # Kalman update should not increase total variance\n        trace_decrease = (np.trace(Pa_fac) - np.trace(Pf)) = TOL\n\n        # 3. SPD check\n        # Smallest eigenvalue must be positive\n        try:\n            eigenvalues = np.linalg.eigvalsh(Pa_fac)\n            is_spd = np.min(eigenvalues) > TOL\n        except np.linalg.LinAlgError:\n            is_spd = False\n\n        results.append(f\"[{rel_diff},{trace_decrease},{is_spd}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```"
        },
        {
            "introduction": "The optimal performance of the standard Kalman filter hinges on the assumption of Gaussian-distributed errors, a condition often violated in practice by the presence of outliers. This exercise explores how to robustify the analysis step by replacing the standard quadratic penalty with a Huber loss function, which is less sensitive to large errors . You will develop an Iteratively Reweighted Least Squares (IRLS) algorithm to solve the new, non-quadratic optimization problem, thereby learning a powerful technique to make state estimation resilient to contaminated data.",
            "id": "3364799",
            "problem": "Consider a linear analysis problem from data assimilation with a Gaussian background and linear observations. Let the state be a vector $x \\in \\mathbb{R}^n$ with a Gaussian prior $x \\sim \\mathcal{N}(x_b, B)$, where $x_b \\in \\mathbb{R}^n$ is the background mean and $B \\in \\mathbb{R}^{n \\times n}$ is the symmetric positive definite background error covariance. Let the observation operator be a linear map $H \\in \\mathbb{R}^{m \\times n}$ and the observation be $y \\in \\mathbb{R}^m$ with nominal observation error covariance $R \\in \\mathbb{R}^{m \\times m}$ that is diagonal and symmetric positive definite. The classical analysis step minimizes the quadratic cost composed of the background and observation terms. In this problem, you must robustify the observation term by replacing the quadratic penalty on the residual with the Huber loss, and derive a modified analysis gain computed via Iteratively Reweighted Least Squares (IRLS).\n\nYou must start from the following base elements only:\n- The Maximum A Posteriori (MAP) estimate for a Gaussian prior and a likelihood that is quadratic in the residual solves a weighted least squares problem.\n- The Huber loss with threshold parameter $\\delta  0$ applied componentwise to a standardized residual $z \\in \\mathbb{R}^m$ is defined by\n$$\n\\rho_\\delta(z_i) = \n\\begin{cases}\n\\tfrac{1}{2} z_i^2,  \\text{if } |z_i| \\le \\delta, \\\\\n\\delta |z_i| - \\tfrac{1}{2} \\delta^2,  \\text{if } |z_i|  \\delta,\n\\end{cases}\n$$\nwith derivative (the score function) \n$$\n\\psi_\\delta(z_i) = \n\\begin{cases}\nz_i,  \\text{if } |z_i| \\le \\delta, \\\\\n\\delta \\, \\mathrm{sign}(z_i),  \\text{if } |z_i|  \\delta.\n\\end{cases}\n$$\n- Iteratively Reweighted Least Squares (IRLS) for robust regression replaces the robust penalty with a sequence of weighted quadratic problems using weights $w_i = \\psi_\\delta(z_i)/z_i$ for $z_i \\neq 0$, and $w_i = 1$ for $z_i = 0$.\n\nYour tasks are:\n1. Define the robust MAP objective with a Gaussian background and a Huber observation penalty applied to the standardized residual $z = R^{-1/2}(y - H x)$, where $R^{-1/2}$ is the diagonal matrix whose entries are the reciprocals of the standard deviations. Derive the linear system solved at each IRLS iteration and express the corresponding symmetric positive semidefinite observation precision $S$ as a function of the diagonal weight matrix $W = \\mathrm{diag}(w_1, \\dots, w_m)$, $R$, and the observation operator $H$.\n2. From first principles, derive an explicit expression for a modified analysis gain $\\tilde{K}$ that maps the innovation $(y - H x_b)$ to the incremental analysis, and that is consistent with the linear-quadratic subproblem formed at each IRLS iteration. Your derivation must not assume any known closed-form Kalman gain expression; instead, it must rely on the MAP normal equations and standard linear algebra identities for blockwise quadratic minimization.\n3. Propose a complete IRLS algorithm that:\n   - Initializes with the standard quadratic analysis (all weights equal to $1$).\n   - Iteratively updates the standardized residuals, Huber weights, the effective observation precision, the modified gain, and the analysis state until convergence in the state with a prescribed tolerance.\n   - Terminates within a maximum number of iterations if the tolerance is not reached.\n4. Implement the algorithm as a program that computes, for each test case in the suite below:\n   - The robust analysis state vector $x_a \\in \\mathbb{R}^n$ at convergence.\n   - The final vector of Huber weights $w \\in \\mathbb{R}^m$ at convergence.\n   - The Frobenius norm of the difference between the final modified gain $\\tilde{K}$ and the standard Kalman gain $K$ computed with the nominal diagonal $R$ (that is, with all weights equal to $1$).\n   Your program must round all floating-point outputs to $6$ decimal places.\n\nTest Suite:\n- Case A (nominal residuals, no outliers): $n = 2$, $m = 2$, $B = \\mathrm{diag}(1, 1)$, $H = I_2$, $R = \\mathrm{diag}(1, 1)$, $x_b = [0, 0]^T$, $y = [0.2, -0.1]^T$, $\\delta = 1.0$, tolerance $= 10^{-10}$, maximum iterations $= 50$.\n- Case B (one large outlier): $n = 2$, $m = 2$, $B = \\mathrm{diag}(1, 1)$, $H = I_2$, $R = \\mathrm{diag}(1, 1)$, $x_b = [0, 0]^T$, $y = [0.2, 10.0]^T$, $\\delta = 1.0$, tolerance $= 10^{-10}$, maximum iterations $= 50$.\n- Case C (boundary at the Huber threshold in one dimension): $n = 1$, $m = 1$, $B = [1]$, $H = [1]$, $R = [1]$, $x_b = [0]$, $y = [2.0]$, $\\delta = 1.0$, tolerance $= 10^{-12}$, maximum iterations $= 50$.\n- Case D (anisotropic background, single observation with potential outlier): $n = 2$, $m = 1$, $B = \\mathrm{diag}(1.0, 0.01)$, $H = \\begin{bmatrix} 1.0  1.0 \\end{bmatrix}$, $R = [0.04]$, $x_b = [0.0, 0.0]^T$, $y = [20.0]$, $\\delta = 1.0$, tolerance $= 10^{-10}$, maximum iterations $= 50$.\n\nFinal Output Format:\n- Your program should produce a single line of output containing a JSON-like representation of a list with one entry per test case.\n- For each test case, output a list of the form $[x\\_a, w, d]$ where $x\\_a$ is the analysis state vector as a list of floats rounded to $6$ decimal places, $w$ is the final weight vector as a list of floats rounded to $6$ decimal places, and $d$ is a single float rounded to $6$ decimal places equal to the Frobenius norm of $\\tilde{K} - K$.\n- The overall output must be a single line string exactly in the form\n$$\n[\\,[x\\_a^{(A)}, w^{(A)}, d^{(A)}],\\,[x\\_a^{(B)}, w^{(B)}, d^{(B)}],\\,[x\\_a^{(C)}, w^{(C)}, d^{(C)}],\\,[x\\_a^{(D)}, w^{(D)}, d^{(D)}]\\,],\n$$\nwith no additional text, where each superscript denotes the corresponding test case result.",
            "solution": "The problem is valid. It presents a well-posed and scientifically grounded task in the field of data assimilation and inverse problems. The givens are formally defined, mutually consistent, and sufficient to derive a unique and meaningful solution. The problem requires the derivation and implementation of a robust analysis scheme using the Huber loss and the Iteratively Reweighted Least Squares (IRLS) method, which are standard techniques in robust statistics and optimization. All terms are defined using standard mathematical notation, and the test cases provide concrete, verifiable scenarios.\n\n### 1. Robust MAP Objective and the IRLS Subproblem\n\nThe classical Maximum A Posteriori (MAP) estimate for a linear-Gaussian problem minimizes a quadratic cost function, $J(x)$. This function is proportional to the negative logarithm of the posterior probability density, arising from a Gaussian prior on the state $x$ and a Gaussian likelihood for the observation $y$. The cost function is the sum of a background term and an observation term:\n$$\nJ(x) = \\frac{1}{2}(x - x_b)^T B^{-1} (x - x_b) + \\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)\n$$\nHere, $x \\in \\mathbb{R}^n$ is the state vector, $x_b \\in \\mathbb{R}^n$ is the background (prior mean), $B \\in \\mathbb{R}^{n \\times n}$ is the background error covariance, $y \\in \\mathbb{R}^m$ is the observation vector, $H \\in \\mathbb{R}^{m \\times n}$ is the linear observation operator, and $R \\in \\mathbb{R}^{m \\times m}$ is the observation error covariance.\n\nTo robustify the analysis against outliers in the observations, the quadratic penalty on the observation residual, $\\frac{1}{2}(y - Hx)^T R^{-1} (y - Hx)$, is replaced. Let $z = R^{-1/2}(y - Hx)$ be the standardized residual vector, with $R^{-1/2}$ being the diagonal matrix with entries $1/\\sigma_i$, where $\\sigma_i^2$ are the diagonal entries of $R$. The quadratic penalty is $\\frac{1}{2} z^T z = \\sum_{i=1}^m \\frac{1}{2} z_i^2$. We replace the term $\\frac{1}{2} z_i^2$ for each component with the Huber loss function $\\rho_\\delta(z_i)$.\n\nThe robust MAP objective function is:\n$$\nJ_{Huber}(x) = \\frac{1}{2}(x - x_b)^T B^{-1} (x - x_b) + \\sum_{i=1}^m \\rho_\\delta \\left( (R^{-1/2}(y - Hx))_i \\right)\n$$\nTo find the minimum, we set the gradient $\\nabla_x J_{Huber}(x)$ to zero. The gradient of the background term is $B^{-1}(x - x_b)$. The gradient of the observation term is found using the chain rule. Let $\\psi_\\delta(u) = \\frac{d\\rho_\\delta(u)}{du}$.\n$$\n\\nabla_x \\left( \\sum_{i=1}^m \\rho_\\delta(z_i) \\right) = \\sum_{i=1}^m \\psi_\\delta(z_i) \\nabla_x z_i = \\left(\\nabla_x z\\right)^T \\psi_\\delta(z)\n$$\nSince $z = R^{-1/2}y - R^{-1/2}Hx$, the Jacobian is $\\nabla_x z = -R^{-1/2}H$. Its transpose is $-H^T R^{-1/2}$. Thus, the gradient of the observation term is $-H^T R^{-1/2} \\psi_\\delta(z)$. The optimality condition is:\n$$\n\\nabla_x J_{Huber}(x) = B^{-1}(x - x_b) - H^T R^{-1/2} \\psi_\\delta(R^{-1/2}(y - Hx)) = 0\n$$\nThis is a system of nonlinear equations for $x$. The Iteratively Reweighted Least Squares (IRLS) method solves this by approximating the score function $\\psi_\\delta(z_i)$ as $\\psi_\\delta(z_i) \\approx w_i z_i$, where the weights are $w_i = \\psi_\\delta(z_i) / z_i$. Let $W = \\mathrm{diag}(w_1, \\dots, w_m)$ be the diagonal matrix of weights. The vector relation becomes $\\psi_\\delta(z) \\approx Wz$. Substituting this into the optimality condition yields a linear system for $x$ at each iteration:\n$$\nB^{-1}(x - x_b) - H^T R^{-1/2} W R^{-1/2} (y - Hx) = 0\n$$\nRearranging terms gives:\n$$\n(B^{-1} + H^T R^{-1/2} W R^{-1/2} H) x = B^{-1} x_b + H^T R^{-1/2} W R^{-1/2} y\n$$\nThis linear system is the normal equation for minimizing the following quadratic cost function at each IRLS step (with $W$ being fixed for that step):\n$$\nJ_{IRLS}(x) = \\frac{1}{2}(x - x_b)^T B^{-1} (x - x_b) + \\frac{1}{2}(y - Hx)^T (R^{-1/2} W R^{-1/2}) (y - Hx)\n$$\nThe Hessian of this objective function is $P_a^{-1} = B^{-1} + H^T (R^{-1/2} W R^{-1/2}) H$. The contribution of the observation term to this posterior precision matrix is the requested symmetric positive semidefinite observation precision $S$. Thus,\n$$\nS = H^T (R^{-1/2} W R^{-1/2}) H\n$$\nSince the weights $w_i = \\psi_\\delta(z_i)/z_i$ satisfy $0 \\le w_i \\le 1$, the matrix $W$ is positive semidefinite, making $S$ positive semidefinite as required.\n\n### 2. Derivation of the Modified Analysis Gain $\\tilde{K}$\n\nThe modified analysis gain $\\tilde{K}$ maps the innovation with respect to the background, $(y - Hx_b)$, to the analysis increment, $(x_a - x_b)$. We derive its expression from the linear system obtained for each IRLS subproblem. Let $x_a$ be the solution for a given weight matrix $W$. Let $\\tilde{R}^{-1} = R^{-1/2} W R^{-1/2}$ be the effective observation precision matrix for the current iteration. The linear system for $x_a$ is:\n$$\n(B^{-1} + H^T \\tilde{R}^{-1} H) x_a = B^{-1} x_b + H^T \\tilde{R}^{-1} y\n$$\nTo find an expression for the increment $x_a - x_b$, we rearrange the equation.\n$$\nB^{-1}(x_a - x_b) + H^T \\tilde{R}^{-1} H x_a = H^T \\tilde{R}^{-1} y\n$$\nWe add and subtract $H^T \\tilde{R}^{-1} H x_b$ on the left side:\n$$\nB^{-1}(x_a - x_b) + H^T \\tilde{R}^{-1} H (x_a - x_b) + H^T \\tilde{R}^{-1} H x_b = H^T \\tilde{R}^{-1} y\n$$\nGrouping the terms with $(x_a - x_b)$ and moving the other terms to the right-hand side:\n$$\n(B^{-1} + H^T \\tilde{R}^{-1} H) (x_a - x_b) = H^T \\tilde{R}^{-1} y - H^T \\tilde{R}^{-1} H x_b\n$$\n$$\n(B^{-1} + H^T \\tilde{R}^{-1} H) (x_a - x_b) = H^T \\tilde{R}^{-1} (y - Hx_b)\n$$\nSolving for the analysis increment $(x_a - x_b)$:\n$$\nx_a - x_b = (B^{-1} + H^T \\tilde{R}^{-1} H)^{-1} H^T \\tilde{R}^{-1} (y - Hx_b)\n$$\nBy definition, $x_a - x_b = \\tilde{K}(y - Hx_b)$. By comparing the two expressions, we identify the modified analysis gain $\\tilde{K}$ for the IRLS subproblem:\n$$\n\\tilde{K} = (B^{-1} + H^T \\tilde{R}^{-1} H)^{-1} H^T \\tilde{R}^{-1}\n$$\nThis derivation uses only first principles of linear algebra and does not presuppose any specific form of the gain matrix. Using the Woodbury matrix identity, this can be transformed into the more familiar Kalman gain form: $\\tilde{K} = B H^T (H B H^T + \\tilde{R})^{-1}$, where $\\tilde{R} = (R^{-1/2}W R^{-1/2})^{-1}$. However, the first form is more convenient computationally if $W$ contains zero entries, as $\\tilde{R}$ would not be defined.\n\n### 3. The IRLS Algorithm\n\nThe algorithm iteratively refines the analysis state by updating the Huber weights.\n\n1.  **Initialization**:\n    a. Calculate the standard Kalman gain $K$ using nominal covariances (i.e., with weights $W=I$): $K = (B^{-1} + H^T R^{-1} H)^{-1} H^T R^{-1}$.\n    b. Compute the initial analysis state $x_a^{(0)}$, which is the standard linear-quadratic solution: $x_a^{(0)} = x_b + K(y-Hx_b)$.\n    c. Set the iteration counter $k=1$.\n\n2.  **Iteration Loop**: For $k=1, \\dots, \\text{max\\_iterations}$:\n    a. Let the analysis from the previous step be $x_a^{\\text{prev}} = x_a^{(k-1)}$.\n    b. **Update Residuals**: Compute the standardized residuals $z = R^{-1/2}(y - H x_a^{\\text{prev}})$.\n    c. **Update Weights**: For each component $i=1, \\dots, m$, calculate the new weight $w_i = \\psi_\\delta(z_i)/z_i$. If $z_i=0$, set $w_i=1$. Collect these into the diagonal matrix $W^{(k)} = \\mathrm{diag}(w_1, \\dots, w_m)$.\n    d. **Update Effective Precision**: Compute the effective observation precision matrix $\\tilde{R}_{(k)}^{-1} = R^{-1/2} W^{(k)} R^{-1/2}$.\n    e. **Update Gain**: Compute the modified analysis gain $\\tilde{K}^{(k)} = (B^{-1} + H^T \\tilde{R}_{(k)}^{-1} H)^{-1} H^T \\tilde{R}_{(k)}^{-1}$.\n    f. **Update Analysis State**: Calculate the new analysis state $x_a^{(k)} = x_b + \\tilde{K}^{(k)}(y - Hx_b)$.\n    g. **Check Convergence**: If the L2-norm of the change in the state is below the tolerance, $\\Vert x_a^{(k)} - x_a^{\\text{prev}} \\Vert_2  \\text{tol}$, terminate the loop.\n\n3.  **Termination**:\n    a. The final robust analysis state is the last computed state, $x_a = x_a^{(k)}$.\n    b. The final Huber weights are the last computed weights, $w = \\mathrm{diag}(W^{(k)})$.\n    c. The final modified gain is $\\tilde{K} = \\tilde{K}^{(k)}$.\n    d. Compute the Frobenius norm of the difference between the final modified gain and the standard Kalman gain: $d = \\Vert \\tilde{K} - K \\Vert_F$.\n\nThis algorithm converges to the solution of the robust MAP problem. The initial state is the BLUE (Best Linear Unbiased Estimate), and subsequent iterations down-weight the influence of observations with large residuals, providing a robust estimate.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a series of robust data assimilation problems using IRLS with the Huber loss.\n    \"\"\"\n\n    test_cases = [\n        # Case A: nominal residuals, no outliers\n        {\n            \"n\": 2, \"m\": 2, \"B\": np.diag([1.0, 1.0]), \"H\": np.eye(2),\n            \"R\": np.diag([1.0, 1.0]), \"x_b\": np.array([0.0, 0.0]),\n            \"y\": np.array([0.2, -0.1]), \"delta\": 1.0,\n            \"tol\": 1e-10, \"max_iter\": 50\n        },\n        # Case B: one large outlier\n        {\n            \"n\": 2, \"m\": 2, \"B\": np.diag([1.0, 1.0]), \"H\": np.eye(2),\n            \"R\": np.diag([1.0, 1.0]), \"x_b\": np.array([0.0, 0.0]),\n            \"y\": np.array([0.2, 10.0]), \"delta\": 1.0,\n            \"tol\": 1e-10, \"max_iter\": 50\n        },\n        # Case C: boundary at the Huber threshold\n        {\n            \"n\": 1, \"m\": 1, \"B\": np.array([[1.0]]), \"H\": np.array([[1.0]]),\n            \"R\": np.array([[1.0]]), \"x_b\": np.array([0.0]),\n            \"y\": np.array([2.0]), \"delta\": 1.0,\n            \"tol\": 1e-12, \"max_iter\": 50\n        },\n        # Case D: anisotropic background with potential outlier\n        {\n            \"n\": 2, \"m\": 1, \"B\": np.diag([1.0, 0.01]), \"H\": np.array([[1.0, 1.0]]),\n            \"R\": np.array([[0.04]]), \"x_b\": np.array([0.0, 0.0]),\n            \"y\": np.array([20.0]), \"delta\": 1.0,\n            \"tol\": 1e-10, \"max_iter\": 50\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # Extract parameters for the current case\n        B = case[\"B\"]\n        H = case[\"H\"]\n        R = case[\"R\"]\n        x_b = case[\"x_b\"]\n        y = case[\"y\"]\n        delta = case[\"delta\"]\n        tol = case[\"tol\"]\n        max_iter = case[\"max_iter\"]\n        m = case[\"m\"]\n\n        # Pre-compute matrix inverses and square roots\n        B_inv = np.linalg.inv(B)\n        R_inv = np.linalg.inv(R)\n        R_inv_sqrt = np.diag(1.0 / np.sqrt(np.diag(R)))\n\n        # --- Standard Kalman Filter Solution (W=I) ---\n        # Calculate standard Kalman gain K\n        Hessian_std = B_inv + H.T @ R_inv @ H\n        K_std = np.linalg.inv(Hessian_std) @ H.T @ R_inv\n        \n        # Initial analysis state (standard quadratic solution)\n        x_a_current = x_b + K_std @ (y - H @ x_b)\n\n        # --- IRLS Algorithm ---\n        w = np.ones(m)\n        K_mod = K_std\n\n        for _ in range(max_iter):\n            x_a_prev = x_a_current\n\n            # 1. Update standardized residuals\n            residuals = y - H @ x_a_prev\n            z = R_inv_sqrt @ residuals\n\n            # 2. Update Huber weights\n            psi_z = np.sign(z) * np.minimum(np.abs(z), delta)\n            \n            # Create a mask for non-zero residuals to avoid division by zero\n            nonzero_mask = np.abs(z) > 1e-15 # A small tolerance for floating point zero\n            w = np.ones_like(z)\n            w[nonzero_mask] = psi_z[nonzero_mask] / z[nonzero_mask]\n            \n            W = np.diag(w)\n\n            # 3. Update effective observation precision and modified gain\n            R_mod_inv = R_inv_sqrt @ W @ R_inv_sqrt\n            Hessian_mod = B_inv + H.T @ R_mod_inv @ H\n            K_mod = np.linalg.inv(Hessian_mod) @ H.T @ R_mod_inv\n\n            # 4. Update analysis state\n            x_a_current = x_b + K_mod @ (y - H @ x_b)\n\n            # 5. Check for convergence\n            if np.linalg.norm(x_a_current - x_a_prev)  tol:\n                break\n        \n        # --- Final Computations ---\n        x_a_final = x_a_current\n        w_final = w\n        \n        # Calculate Frobenius norm of the gain difference\n        d = np.linalg.norm(K_mod - K_std, 'fro')\n\n        # Format results for output\n        x_a_str = f\"[{','.join([f'{val:.6f}' for val in x_a_final])}]\"\n        w_str = f\"[{','.join([f'{val:.6f}' for val in w_final])}]\"\n        d_str = f\"{d:.6f}\"\n        \n        results.append(f\"[{x_a_str},{w_str},{d_str}]\")\n\n    # Print final output in the required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```"
        }
    ]
}