{
    "hands_on_practices": [
        {
            "introduction": "The foundation of tracking uncertainty over time lies in the forecast step, where we predict how the error covariance evolves based on the system's dynamics and inherent model noise. This first exercise  challenges you to derive this propagation from first principles and apply it to a simple two-dimensional system. By calculating the new covariance and identifying the specific directions of growing and shrinking uncertainty, you will gain a concrete understanding of how the state transition matrix $F$ and process noise $Q$ actively shape the geometry of the error distribution.",
            "id": "3421213",
            "problem": "Consider a two-dimensional discrete-time linear stochastic dynamical system with state $x_k \\in \\mathbb{R}^2$ evolving according to $x_{k+1} = F x_k + w_k$, where $F = \\mathrm{diag}(2, 0.5)$ and the process noise $w_k$ is a zero-mean random vector independent of $x_k$ and of the current state estimation error, with covariance $Q = \\mathrm{diag}(1, 0)$. Let the initial state estimation error covariance be\n$$\nP_0 = \\begin{bmatrix} 1 & 0.2 \\\\ 0.2 & 1 \\end{bmatrix}.\n$$\nAssume an unbiased predictor with $\\hat{x}_{k+1|k} = F \\hat{x}_k$ and define the state estimation error as $e_k = x_k - \\hat{x}_k$. Starting only from the definitions of covariance and the given independence assumptions, derive the prediction-step covariance $P_1$ and then determine which orthonormal directions in the state space exhibit growing versus shrinking uncertainty after one propagation step. Provide the final answer as a single row matrix containing, in order: the four entries of $P_1$ arranged row-wise $(P_{1,11}, P_{1,12}, P_{1,21}, P_{1,22})$, followed by the two components of a unit vector for the growing-uncertainty direction, and then the two components of a unit vector for the shrinking-uncertainty direction. Express the final answer exactly; do not round.",
            "solution": "Let the state estimation error at time $k$ after the measurement update (the analysis error) be $e_{k|k} = x_k - \\hat{x}_{k|k}$. Its covariance is the analysis error covariance, $P_{k|k} = E[e_{k|k} e_{k|k}^T]$. At the initial time $k=0$, we are given $P_0$, which we interpret as $P_{0|0}$.\n\nThe state vector evolves according to the linear stochastic equation:\n$$x_{k+1} = F x_k + w_k$$\nThe state estimate is propagated forward in time by the deterministic part of the model. Interpreting $\\hat{x}_k$ as the analysis estimate $\\hat{x}_{k|k}$, the prediction for time $k+1$ is:\n$$\\hat{x}_{k+1|k} = F \\hat{x}_{k|k}$$\nThe prediction error (or forecast error) at time $k+1$ is the difference between the true state and the predicted state:\n$$e_{k+1|k} = x_{k+1} - \\hat{x}_{k+1|k}$$\nSubstituting the expressions for $x_{k+1}$ and $\\hat{x}_{k+1|k}$:\n$$e_{k+1|k} = (F x_k + w_k) - (F \\hat{x}_{k|k}) = F(x_k - \\hat{x}_{k|k}) + w_k = F e_{k|k} + w_k$$\nThe prediction error covariance, $P_{k+1|k}$, is defined as the expected value of the outer product of the prediction error with itself:\n$$P_{k+1|k} = E[e_{k+1|k} e_{k+1|k}^T] = E[(F e_{k|k} + w_k)(F e_{k|k} + w_k)^T]$$\nExpanding the product:\n$$P_{k+1|k} = E[F e_{k|k} e_{k|k}^T F^T + F e_{k|k} w_k^T + w_k e_{k|k}^T F^T + w_k w_k^T]$$\nBy the linearity of the expectation operator:\n$$P_{k+1|k} = E[F e_{k|k} e_{k|k}^T F^T] + E[F e_{k|k} w_k^T] + E[w_k e_{k|k}^T F^T] + E[w_k w_k^T]$$\nUsing the fact that $F$ is a constant matrix:\n$$P_{k+1|k} = F E[e_{k|k} e_{k|k}^T] F^T + F E[e_{k|k} w_k^T] + E[w_k e_{k|k}^T] F^T + E[w_k w_k^T]$$\nWe are given that the process noise $w_k$ is independent of the current state estimation error $e_{k|k}$. Therefore, they are uncorrelated. As the estimation process is unbiased, the error $e_{k|k}$ is a zero-mean random vector. The process noise $w_k$ is also given as zero-mean. Thus, the cross-correlation terms are zero:\n$$E[e_{k|k} w_k^T] = E[e_{k|k}] E[w_k^T] = 0 \\cdot 0^T = 0$$\nThe expression for the covariance simplifies to:\n$$P_{k+1|k} = F E[e_{k|k} e_{k|k}^T] F^T + E[w_k w_k^T]$$\nRecognizing the definitions of the analysis error covariance $P_{k|k} = E[e_{k|k} e_{k|k}^T]$ and the process noise covariance $Q = E[w_k w_k^T]$, we arrive at the standard prediction-step covariance update equation:\n$$P_{k+1|k} = F P_{k|k} F^T + Q$$\nWe need to compute $P_1$, which corresponds to $P_{1|0}$. We use $k=0$:\n$$P_{1|0} = F P_{0|0} F^T + Q$$\nThe given matrices are:\n$$F = \\begin{bmatrix} 2 & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix}, \\quad P_0 = P_{0|0} = \\begin{bmatrix} 1 & \\frac{1}{5} \\\\ \\frac{1}{5} & 1 \\end{bmatrix}, \\quad Q = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix}$$\nFirst, compute the product $F P_0 F^T$:\n$$F P_0 F^T = \\begin{bmatrix} 2 & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix} \\begin{bmatrix} 1 & \\frac{1}{5} \\\\ \\frac{1}{5} & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix} = \\begin{bmatrix} 2 & \\frac{2}{5} \\\\ \\frac{1}{10} & \\frac{1}{2} \\end{bmatrix} \\begin{bmatrix} 2 & 0 \\\\ 0 & \\frac{1}{2} \\end{bmatrix} = \\begin{bmatrix} 4 & \\frac{1}{5} \\\\ \\frac{1}{5} & \\frac{1}{4} \\end{bmatrix}$$\nNext, add the process noise covariance $Q$:\n$$P_1 = P_{1|0} = \\begin{bmatrix} 4 & \\frac{1}{5} \\\\ \\frac{1}{5} & \\frac{1}{4} \\end{bmatrix} + \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix} = \\begin{bmatrix} 5 & \\frac{1}{5} \\\\ \\frac{1}{5} & \\frac{1}{4} \\end{bmatrix}$$\nTo determine the directions of growing versus shrinking uncertainty, we analyze the change in the error covariance matrix, $\\Delta P = P_1 - P_0$. The change in variance in an arbitrary direction given by a unit vector $v$ is $v^T \\Delta P v$. The directions of maximum and minimum change are the eigenvectors of $\\Delta P$.\n$$\\Delta P = P_1 - P_0 = \\begin{bmatrix} 5 & \\frac{1}{5} \\\\ \\frac{1}{5} & \\frac{1}{4} \\end{bmatrix} - \\begin{bmatrix} 1 & \\frac{1}{5} \\\\ \\frac{1}{5} & 1 \\end{bmatrix} = \\begin{bmatrix} 5-1 & \\frac{1}{5}-\\frac{1}{5} \\\\ \\frac{1}{5}-\\frac{1}{5} & \\frac{1}{4}-1 \\end{bmatrix} = \\begin{bmatrix} 4 & 0 \\\\ 0 & -\\frac{3}{4} \\end{bmatrix}$$\nThe matrix $\\Delta P$ is diagonal. Its eigenvalues are the diagonal entries, $\\lambda_1 = 4$ and $\\lambda_2 = -\\frac{3}{4}$. The corresponding eigenvectors are the standard basis vectors.\n\nA positive eigenvalue signifies that the variance (uncertainty) in the direction of the corresponding eigenvector has grown. The growing-uncertainty direction corresponds to $\\lambda_1 = 4$. The eigenvector is:\n$$v_{grow} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$$\nA negative eigenvalue signifies that the variance (uncertainty) has shrunk. The shrinking-uncertainty direction corresponds to $\\lambda_2 = -\\frac{3}{4}$. The eigenvector is:\n$$v_{shrink} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$$\nThese eigenvectors are already unit vectors and are orthogonal, thus a valid set of orthonormal directions.\n\nThe final answer requires the four entries of $P_1$ row-wise, the two components of $v_{grow}$, and the two components of $v_{shrink}$.\nThe entries of $P_1$ are $P_{1,11}=5$, $P_{1,12}=\\frac{1}{5}$, $P_{1,21}=\\frac{1}{5}$, and $P_{1,22}=\\frac{1}{4}$.\nThe components of $v_{grow}$ are $1$ and $0$.\nThe components of $v_{shrink}$ are $0$ and $1$.",
            "answer": "$$\\boxed{\\begin{pmatrix} 5 & \\frac{1}{5} & \\frac{1}{5} & \\frac{1}{4} & 1 & 0 & 0 & 1 \\end{pmatrix}}$$"
        },
        {
            "introduction": "While the forecast step describes how uncertainty grows, the analysis step explains how it is reduced by assimilating new observations. This practice  guides you through the derivation of the optimal Kalman filter update, starting from the basic principle of minimizing the mean-squared estimation error. You will compute the Kalman gain and the resulting analysis error covariance, and by comparing two different but mathematically equivalent formulas for the updated covariance, you will also appreciate the nuances of numerical implementation and stability.",
            "id": "3421248",
            "problem": "Consider a linear-Gaussian data assimilation setting with a two-dimensional state vector $x \\in \\mathbb{R}^{2}$ and a one-dimensional observation $y \\in \\mathbb{R}$. The prior (background) state error is zero-mean with covariance $P^{f} \\in \\mathbb{R}^{2 \\times 2}$, and the observation model is $y = H x + \\epsilon$ with $H \\in \\mathbb{R}^{1 \\times 2}$ and observation noise $\\epsilon$ that is zero-mean, independent of the prior error, with covariance $R \\in \\mathbb{R}^{1 \\times 1}$. You are asked to derive, from first principles, the optimal linear minimum-variance analysis step used in the Kalman Filter (KF), without assuming any pre-given update formulas. Begin from the following fundamental base:\n\n- The Bayesian linear-Gaussian model: if $(x, y)$ are jointly Gaussian with linear observation operator and independent Gaussian noises, then the conditional distribution $p(x \\mid y)$ is Gaussian. The mean of $p(x \\mid y)$ minimizes the mean-squared estimation error over all estimators measurable with respect to $y$.\n- The estimator is restricted to the linear class $x^{a} = x^{f} + K\\,(y - H x^{f})$ where $K \\in \\mathbb{R}^{2 \\times 1}$ is a gain to be determined by minimizing the expected squared analysis error.\n- For any zero-mean random vector $z$, its covariance is defined by $\\mathbb{E}[z z^{\\top}]$.\n- For independent zero-mean random vectors $u$ and $v$, $\\mathbb{E}[u v^{\\top}] = 0$.\n\nUsing only these foundations, do the following:\n\n1. Derive the optimal gain $K$ by minimizing the trace of the analysis error covariance over all $K \\in \\mathbb{R}^{2 \\times 1}$ in the linear estimator $x^{a} = x^{f} + K\\,(y - H x^{f})$.\n2. Derive two mathematically equivalent expressions for the analysis error covariance $P^{a}$: \n   - the standard form based on substituting the optimal $K$ directly into the analysis error covariance,\n   - and the Joseph stabilized form that is bilinear in the prior covariance and the gain and explicitly adds the observation noise contribution.\n3. Numerically evaluate the optimal $K$ and the $P^{a}$ produced by both the standard and the Joseph forms for \n   $$P^{f} = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}, \\quad H = \\begin{bmatrix} 1 & 0 \\end{bmatrix}, \\quad R = 1.$$\n   Verify that the two $P^{a}$ matrices are identical.\n4. Prove that the resulting $P^{a}$ is positive semidefinite by a direct check using necessary and sufficient conditions for $2 \\times 2$ symmetric matrices.\n\nYour final answer must report, in the following order, the six scalars consisting of the two components of $K$ followed by the four entries of $P^{a}$ (which is identical for both standard and Joseph forms), as exact rational numbers. No rounding is required and no units are involved. Express the final answer as a single row using a matrix with one row and six columns. Do not include any text in the final answer.",
            "solution": "We define the analysis error $e^a$ as the difference between the analysis state $x^a$ and the true state $x$. The prior error is $e^f = x^f - x$, where $x^f$ is the prior estimate. The covariance of the prior error is given as $P^f = \\mathbb{E}[e^f (e^f)^\\top]$. The observation model is $y = Hx + \\epsilon$, where $\\epsilon$ is the observation error with covariance $R = \\mathbb{E}[\\epsilon \\epsilon^\\top]$. The prior and observation errors are independent and have zero mean.\n\nThe analysis state $x^a$ is given by the linear estimator $x^a = x^f + K(y - Hx^f)$, where $K \\in \\mathbb{R}^{2 \\times 1}$ is the gain matrix. The analysis error is:\n$$e^a = x^a - x = \\left(x^f + K(y - Hx^f)\\right) - x$$\nSubstitute $y = Hx + \\epsilon$:\n$$e^a = x^f - x + K(Hx + \\epsilon - Hx^f)$$\n$$e^a = (x^f - x) + K(H(x-x^f) + \\epsilon)$$\n$$e^a = e^f - KHe^f + K\\epsilon$$\n$$e^a = (I - KH)e^f + K\\epsilon$$\nwhere $I$ is the $2 \\times 2$ identity matrix.\n\nThe analysis error covariance $P^a$ is defined as $P^a = \\mathbb{E}[e^a (e^a)^\\top]$.\n$$P^a = \\mathbb{E}[((I - KH)e^f + K\\epsilon)((I - KH)e^f + K\\epsilon)^\\top]$$\nExpanding this expression gives:\n$$P^a = \\mathbb{E}[(I-KH)e^f(e^f)^\\top(I-KH)^\\top + (I-KH)e^f\\epsilon^\\top K^\\top + K\\epsilon(e^f)^\\top(I-KH)^\\top + K\\epsilon\\epsilon^\\top K^\\top]$$\nSince the prior error $e^f$ and observation error $\\epsilon$ are independent and have zero mean, the cross-terms $\\mathbb{E}[e^f \\epsilon^\\top]$ and $\\mathbb{E}[\\epsilon (e^f)^\\top]$ are zero matrices. The expression simplifies to:\n$$P^a = (I-KH)\\mathbb{E}[e^f(e^f)^\\top](I-KH)^\\top + K\\mathbb{E}[\\epsilon\\epsilon^\\top]K^\\top$$\n$$P^a = (I-KH)P^f(I-KH)^\\top + KRK^\\top$$\n\n**1. Derivation of the optimal gain $K$**\nThe objective is to find the gain $K$ that minimizes the analysis error variance, which is equivalent to minimizing the trace of the analysis error covariance matrix $P^a$. The cost function is $J(K) = \\text{tr}(P^a)$.\n$$J(K) = \\text{tr}((I-KH)P^f(I-KH)^\\top + KRK^\\top)$$\nUsing the linearity and cyclic properties of the trace operator ($\\text{tr}(A+B) = \\text{tr}(A)+\\text{tr}(B)$, $\\text{tr}(ABC) = \\text{tr}(CAB)$):\n$$J(K) = \\text{tr}(P^f) - 2\\text{tr}(KHP^f) + \\text{tr}(K(HP^fH^\\top + R)K^\\top)$$\nTo find the minimum, we compute the derivative of $J(K)$ with respect to $K$ and set it to zero. Using standard matrix calculus identities, and since $P^f$ and $R$ are symmetric, the term $S = HP^fH^\\top+R$ is also symmetric ($S=S^\\top$).\n$$\\frac{\\partial}{\\partial K} J(K) = -2 (HP^f)^\\top + 2K(HP^fH^\\top+R) = -2 P^f H^\\top + 2K(HP^fH^\\top+R)$$\nSetting the derivative to zero for the optimal gain:\n$$-2 P^f H^\\top + 2K(HP^fH^\\top+R) = 0$$\n$$K(HP^fH^\\top+R) = P^f H^\\top$$\nThe optimal gain is therefore:\n$$K = P^f H^\\top (HP^fH^\\top+R)^{-1}$$\n\n**2. Derivation of two equivalent expressions for $P^a$**\nThe first form, known as the **Joseph stabilized form**, is the expression for $P^a$ derived above, which is valid for any gain $K$:\n$$P^a = (I-KH)P^f(I-KH)^\\top + KRK^\\top$$\nThe second form, often called the **standard form**, is derived by simplifying the expression for $P^a$ using the optimal gain. For the optimal gain, we have $P^fH^\\top = K(HP^fH^\\top+R)$. Substituting this into an expansion of the Joseph form leads to:\n$P^a = P^f - KHP^f - P^fH^\\top K^\\top + K(HP^fH^\\top+R)K^\\top$\n$P^a = P^f - KHP^f - P^fH^\\top K^\\top + (P^fH^\\top)K^\\top = P^f - KHP^f$.\nA cleaner derivation starts from $e^a = (I - KH)e^f + K\\epsilon$ and shows that the error $e^a$ is uncorrelated with the innovation $y - Hx^f$. Another path is to substitute $P^fH^\\top - KHP^fH^\\top - KR = 0$ into the expanded Joseph form. This simplifies to:\n$$P^a = (I-KH)P^f$$\nThis simpler expression is valid only when $K$ is the optimal Kalman gain.\n\n**3. Numerical evaluation**\nGiven values are:\n$$P^f = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix}, \\quad H = \\begin{bmatrix} 1 & 0 \\end{bmatrix}, \\quad R = 1$$\nFirst, we compute the components needed for the optimal gain $K$. Note that $R$ is a scalar, $R=1$.\n$$HP^fH^\\top = \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = 2$$\nThe innovation covariance is $S = HP^fH^\\top+R = 2+1 = 3$.\n$$P^fH^\\top = \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$$\nThe optimal gain $K$ is:\n$$K = P^fH^\\top S^{-1} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} \\frac{1}{3} = \\begin{bmatrix} 2/3 \\\\ 1/3 \\end{bmatrix}$$\nNow we compute $P^a$ using both forms.\nUsing the standard form $P^a = (I-KH)P^f$:\n$$I - KH = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} - \\begin{bmatrix} 2/3 \\\\ 1/3 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 1/3 & 0 \\\\ -1/3 & 1 \\end{bmatrix}$$\n$$P^a = \\begin{bmatrix} 1/3 & 0 \\\\ -1/3 & 1 \\end{bmatrix} \\begin{bmatrix} 2 & 1 \\\\ 1 & 3 \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{3} & \\frac{1}{3} \\\\ -\\frac{2}{3}+1 & -\\frac{1}{3}+3 \\end{bmatrix} = \\begin{bmatrix} 2/3 & 1/3 \\\\ 1/3 & 8/3 \\end{bmatrix}$$\nUsing the Joseph form $P^a = (I-KH)P^f(I-KH)^\\top + KRK^\\top$:\nThe first term is:\n$$(I-KH)P^f(I-KH)^\\top = \\begin{bmatrix} 2/3 & 1/3 \\\\ 1/3 & 8/3 \\end{bmatrix} \\begin{bmatrix} 1/3 & -1/3 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2/9 & -2/9+1/3 \\\\ 1/9 & -1/9+8/3 \\end{bmatrix} = \\begin{bmatrix} 2/9 & 1/9 \\\\ 1/9 & 23/9 \\end{bmatrix}$$\nThe second term is:\n$$KRK^\\top = \\begin{bmatrix} 2/3 \\\\ 1/3 \\end{bmatrix} (1) \\begin{bmatrix} 2/3 & 1/3 \\end{bmatrix} = \\begin{bmatrix} 4/9 & 2/9 \\\\ 2/9 & 1/9 \\end{bmatrix}$$\nAdding the two terms:\n$$P^a = \\begin{bmatrix} 2/9 & 1/9 \\\\ 1/9 & 23/9 \\end{bmatrix} + \\begin{bmatrix} 4/9 & 2/9 \\\\ 2/9 & 1/9 \\end{bmatrix} = \\begin{bmatrix} 6/9 & 3/9 \\\\ 3/9 & 24/9 \\end{bmatrix} = \\begin{bmatrix} 2/3 & 1/3 \\\\ 1/3 & 8/3 \\end{bmatrix}$$\nBoth forms yield the identical matrix for $P^a$.\n\n**4. Proof of positive semidefiniteness**\nThe resulting analysis error covariance matrix is $P^a = \\begin{bmatrix} 2/3 & 1/3 \\\\ 1/3 & 8/3 \\end{bmatrix}$. For a $2 \\times 2$ symmetric matrix to be positive semidefinite, its leading principal minors must be non-negative.\n1. The first leading principal minor is $P^a_{11} = 2/3$. This is positive.\n2. The second leading principal minor is the determinant:\n$$\\det(P^a) = \\left(\\frac{2}{3}\\right)\\left(\\frac{8}{3}\\right) - \\left(\\frac{1}{3}\\right)^2 = \\frac{16}{9} - \\frac{1}{9} = \\frac{15}{9} = \\frac{5}{3}$$\nThis is also positive.\nSince both conditions are met, the matrix $P^a$ is positive semidefinite (and in fact, positive definite).\nThe components of the optimal gain are $K_1=2/3, K_2=1/3$. The entries of the analysis covariance matrix are $P^a_{11}=2/3, P^a_{12}=1/3, P^a_{21}=1/3, P^a_{22}=8/3$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{2}{3} & \\frac{1}{3} & \\frac{2}{3} & \\frac{1}{3} & \\frac{1}{3} & \\frac{8}{3} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Having mastered the individual forecast and analysis steps, we can now explore their combined effect over time to understand the filter's long-term behavior. This exercise  presents a carefully constructed scenario where the system has both stable and unstable components, but process noise and observations are isolated to different modes. By deriving the steady-state error variances, you will uncover the elegant balance the Kalman filter achieves: it uses observations to bound the uncertainty of unstable dynamics while correctly tracking the noise-driven drift in unobserved, stable parts of the state.",
            "id": "3421201",
            "problem": "Consider a discrete-time Linear Gaussian State-Space Model (LGSSM) with state dimension $2$ given by the linear dynamics $x_{k+1} = A x_k + w_k$ and observations $y_k = H x_k + v_k$, where $w_k$ and $v_k$ are zero-mean Gaussian random variables with covariances $Q$ and $R$, respectively. Let the state transition matrix be $A = \\operatorname{diag}(\\lambda_{u}, \\lambda_{s})$ with $|\\lambda_{u}| > 1$ and $|\\lambda_{s}| < 1$, where the first component corresponds to the unstable eigenmode and the second component corresponds to the stable eigenmode. Construct the process noise covariance and the observation operator such that $Q$ excites only the stable mode and $H$ observes only the unstable mode by setting $Q = q_{s} e_{s} e_{s}^{\\top}$, $R = r$, and $H = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$, where $e_{s} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, with $q_{s} > 0$ and $r > 0$. Assume the initial analysis covariance is diagonal, $P^{a}_{0} = \\operatorname{diag}(p_{u,0}, p_{s,0})$ with $p_{u,0} \\ge 0$ and $p_{s,0} \\ge 0$.\n\nStarting from the definitions of the LGSSM and the standard Discrete Kalman Filter (KF) covariance propagation and update, analyze the forecast covariance $P^{f}_{k}$ and analysis covariance $P^{a}_{k}$ evolution along the unstable and stable eigen-directions. In particular, derive the scalar recursions for the variances along each eigen-direction, show whether the off-diagonal covariances remain zero under these assumptions, and determine the steady-state limits of the analysis variances in both the unstable and stable directions (if they exist). Express the final answer as a closed-form analytic expression for the two steady-state analysis variances in terms of $\\lambda_{u}$, $\\lambda_{s}$, $r$, and $q_{s}$.\n\nProvide your final answer as a row matrix using the $\\mathrm{pmatrix}$ environment with two entries corresponding to the unstable and stable steady-state analysis variances, respectively. No numerical rounding is required.",
            "solution": "The Linear Gaussian State-Space Model (LGSSM) specifies $x_{k+1} = A x_k + w_k$ and $y_k = H x_k + v_k$, where $w_k \\sim \\mathcal{N}(0, Q)$ and $v_k \\sim \\mathcal{N}(0, R)$. For a Discrete Kalman Filter (KF), the covariance forecast and analysis update are given by the well-tested formulas\n$$\nP^{f}_{k} = A P^{a}_{k-1} A^{\\top} + Q,\n$$\n$$\nK_{k} = P^{f}_{k} H^{\\top} \\left( H P^{f}_{k} H^{\\top} + R \\right)^{-1},\n$$\n$$\nP^{a}_{k} = \\left( I - K_{k} H \\right) P^{f}_{k}.\n$$\nWe analyze these equations in the coordinates aligned with the eigenmodes of $A$, which here are simply the standard basis since $A = \\operatorname{diag}(\\lambda_{u}, \\lambda_{s})$. By construction, the first coordinate corresponds to the unstable mode with $|\\lambda_{u}| > 1$, and the second to the stable mode with $|\\lambda_{s}| < 1$. The process noise covariance is $Q = q_{s} e_{s} e_{s}^{\\top} = \\operatorname{diag}(0, q_{s})$, so only the stable mode is excited by process noise. The observation operator is $H = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$, so only the unstable mode is observed. The observation noise covariance is the scalar $R = r > 0$.\n\nAssume $P^{a}_{0}$ is diagonal: $P^{a}_{0} = \\operatorname{diag}(p_{u,0}, p_{s,0})$. Because $A$ and $Q$ are diagonal, the forecast covariance remains diagonal:\n$$\nP^{f}_{k} = \\operatorname{diag}\\!\\left( \\lambda_{u}^{2} p^{a}_{u,k-1},\\; \\lambda_{s}^{2} p^{a}_{s,k-1} + q_{s} \\right),\n$$\nwhere $p^{a}_{u,k-1}$ and $p^{a}_{s,k-1}$ denote the analysis variances in the unstable and stable directions at step $k-1$.\n\nNext, compute the Kalman gain. With $H = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$ and diagonal $P^{f}_{k}$, we obtain\n$$\nH P^{f}_{k} H^{\\top} = p^{f}_{u,k},\n$$\nwhere $p^{f}_{u,k} = \\lambda_{u}^{2} p^{a}_{u,k-1}$. Therefore,\n$$\nK_{k} = P^{f}_{k} H^{\\top} \\left( p^{f}_{u,k} + r \\right)^{-1}.\n$$\nSince $H^{\\top} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, the gain vector has the form\n$$\nK_{k} = \\begin{pmatrix} \\dfrac{p^{f}_{u,k}}{p^{f}_{u,k} + r} \\\\[6pt] 0 \\end{pmatrix}.\n$$\nHence, the measurement update acts only on the unstable coordinate; the stable coordinate is unobserved.\n\nTo verify the off-diagonal covariances remain zero, we can use either the simplified covariance update $P^{a}_{k} = (I - K_{k} H) P^{f}_{k}$ or the Joseph form $P^{a}_{k} = (I - K_{k} H) P^{f}_{k} (I - K_{k} H)^{\\top} + K_{k} R K_{k}^{\\top}$. Because $P^{f}_{k}$ is diagonal, $K_{k}$ has a nonzero entry only in its first component, and $H$ has a nonzero entry only in the first component, it follows that $(I - K_{k} H)$ is lower triangular with diagonal entries $\\left(1 - \\frac{p^{f}_{u,k}}{p^{f}_{u,k} + r}\\right)$ and $1$, and zeros off-diagonal except possibly in the $(1,2)$ position, which is zero here since $H$ selects only the first state. Multiplying these diagonal matrices preserves diagonality, and the $K_{k} R K_{k}^{\\top}$ term contributes only to the $(1,1)$ entry. Therefore, if $P^{a}_{0}$ is diagonal, then $P^{f}_{k}$ and $P^{a}_{k}$ remain diagonal for all $k$.\n\nThe scalar variance updates along each eigen-direction are thus:\n- Unstable direction forecast:\n$$\np^{f}_{u,k} = \\lambda_{u}^{2} p^{a}_{u,k-1}.\n$$\n- Unstable direction analysis:\n$$\np^{a}_{u,k} = \\left( 1 - \\frac{p^{f}_{u,k}}{p^{f}_{u,k} + r} \\right) p^{f}_{u,k} = \\frac{r\\, p^{f}_{u,k}}{p^{f}_{u,k} + r} = \\frac{r \\lambda_{u}^{2} p^{a}_{u,k-1}}{\\lambda_{u}^{2} p^{a}_{u,k-1} + r}.\n$$\n- Stable direction forecast:\n$$\np^{f}_{s,k} = \\lambda_{s}^{2} p^{a}_{s,k-1} + q_{s}.\n$$\n- Stable direction analysis (unobserved, so analysis equals forecast):\n$$\np^{a}_{s,k} = p^{f}_{s,k} = \\lambda_{s}^{2} p^{a}_{s,k-1} + q_{s}.\n$$\n\nWe now analyze the existence and values of steady states.\n\nFor the unstable direction, suppose a steady state $p^{a}_{u}$ exists such that $p^{a}_{u,k} \\to p^{a}_{u} > 0$. At steady state, $p^{f}_{u} = \\lambda_{u}^{2} p^{a}_{u}$ and\n$$\np^{a}_{u} = \\frac{r\\, p^{f}_{u}}{p^{f}_{u} + r} = \\frac{r \\lambda_{u}^{2} p^{a}_{u}}{\\lambda_{u}^{2} p^{a}_{u} + r}.\n$$\nAssuming $p^{a}_{u} > 0$, we can divide both sides by $p^{a}_{u}$ to obtain\n$$\n1 = \\frac{r \\lambda_{u}^{2}}{\\lambda_{u}^{2} p^{a}_{u} + r} \\quad \\Longrightarrow \\quad \\lambda_{u}^{2} p^{a}_{u} + r = r \\lambda_{u}^{2}.\n$$\nSolving for $p^{a}_{u}$ yields\n$$\np^{a}_{u} = \\frac{r \\left( \\lambda_{u}^{2} - 1 \\right)}{\\lambda_{u}^{2}}.\n$$\nSince $|\\lambda_{u}| > 1$, we have $\\lambda_{u}^{2} - 1 > 0$, so $p^{a}_{u} > 0$. This steady state reflects that, although the mode is dynamically unstable, the absence of process noise in that direction and continuous observation of it yield a finite steady-state analysis variance.\n\nFor the stable direction, the analysis variance obeys a linear scalar recursion\n$$\np^{a}_{s,k} = \\lambda_{s}^{2} p^{a}_{s,k-1} + q_{s}.\n$$\nWith $|\\lambda_{s}| < 1$, this has the unique fixed point $p^{a}_{s}$ satisfying\n$$\np^{a}_{s} = \\lambda_{s}^{2} p^{a}_{s} + q_{s} \\quad \\Longrightarrow \\quad p^{a}_{s} = \\frac{q_{s}}{1 - \\lambda_{s}^{2}}.\n$$\nThis fixed point is positive since $q_{s} > 0$ and $1 - \\lambda_{s}^{2} > 0$. It captures the balance between the stable decay of uncertainty and the continual excitation by process noise in the stable mode, which is unobserved.\n\nIn summary, with $Q$ exciting only the stable mode and $H$ observing only the unstable mode in this diagonal setting, the off-diagonal covariances remain zero, the unstable analysis variance converges to a finite value determined by the measurement noise and the instability factor, and the stable analysis variance converges to the standard process-noise-balanced value. The requested final closed-form expressions are\n$$\np^{a}_{u} = \\frac{r \\left( \\lambda_{u}^{2} - 1 \\right)}{\\lambda_{u}^{2}}, \\qquad p^{a}_{s} = \\frac{q_{s}}{1 - \\lambda_{s}^{2}}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\dfrac{r\\left(\\lambda_{u}^{2}-1\\right)}{\\lambda_{u}^{2}} & \\dfrac{q_{s}}{1-\\lambda_{s}^{2}}\\end{pmatrix}}$$"
        }
    ]
}