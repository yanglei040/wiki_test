{
    "hands_on_practices": [
        {
            "introduction": "A crucial step in many square-root filter formulations is \"whitening\" the observations, which simplifies the update equations by transforming the observation error covariance matrix $R$ into an identity matrix. This transformation relies on computing the inverse square root, $R^{-1/2}$. This exercise  guides you through the formal derivation of $R^{-1/2}$ using its spectral decomposition. By working through a concrete example with an ill-conditioned matrix, you will also gain insight into the significant numerical challenges that can arise in practical applications and the importance of numerically stable algorithms.",
            "id": "3420548",
            "problem": "In square root formulations of ensemble-based data assimilation, such as the Ensemble Kalman Filter (EnKF) implemented in deterministic Square Root Filter (SRF) form, innovation vectors are often pre-whitened using the inverse square root of the observation error covariance. Let the observation error covariance matrix be a symmetric positive definite (SPD) matrix $R \\in \\mathbb{R}^{n \\times n}$, so that by the spectral theorem there exists an orthonormal eigen-decomposition $R = U \\Lambda U^{\\top}$ with $U \\in \\mathbb{R}^{n \\times n}$ orthogonal and $\\Lambda = \\operatorname{diag}(\\lambda_{1},\\dots,\\lambda_{n})$ strictly positive.\n\nStarting from the definitions of SPD matrices and the spectral theorem, derive the inverse square root $R^{-1/2}$ in terms of $U$ and $\\Lambda$ as the unique SPD matrix that satisfies $R^{-1/2} R R^{-1/2} = I$. Then, compute $R^{-1/2}$ explicitly for the following $2 \\times 2$ case designed to mimic an ill-conditioned observation error covariance:\n- The orthogonal matrix $U$ is a rotation by angle $\\theta = \\pi/6$, namely\n$$\nU = \\begin{pmatrix}\n\\cos(\\pi/6) & -\\sin(\\pi/6) \\\\\n\\sin(\\pi/6) & \\cos(\\pi/6)\n\\end{pmatrix}.\n$$\n- The eigenvalue matrix is\n$$\n\\Lambda = \\begin{pmatrix}\n9 & 0 \\\\\n0 & 10^{-6}\n\\end{pmatrix}.\n$$\n\nProvide the final $R^{-1/2}$ as a single exact analytic $2 \\times 2$ matrix expression with entries written using radicals and rational numbers where appropriate; do not approximate or round. In addition, briefly explain, based on first principles, potential numerical pitfalls that can arise when $R$ is ill-conditioned and strategies to mitigate them in the context of square root filter formulations. The final answer must be the matrix $R^{-1/2}$ only; any discussion should appear solely in the solution.",
            "solution": "The problem requires a two-part derivation and calculation followed by a conceptual explanation.\n\nFirst, we derive the general form of the inverse square root $R^{-1/2}$ for a symmetric positive definite (SPD) matrix $R$.\nGiven that $R$ is an SPD matrix in $\\mathbb{R}^{n \\times n}$, the spectral theorem guarantees the existence of an orthonormal eigen-decomposition $R = U \\Lambda U^{\\top}$, where $U$ is an orthogonal matrix ($U U^{\\top} = U^{\\top} U = I$) whose columns are the eigenvectors of $R$, and $\\Lambda = \\operatorname{diag}(\\lambda_{1},\\dots,\\lambda_{n})$ is a diagonal matrix of the corresponding eigenvalues. Since $R$ is positive definite, all its eigenvalues are strictly positive, i.e., $\\lambda_i > 0$ for all $i=1,\\dots,n$.\n\nWe are looking for a unique SPD matrix $X = R^{-1/2}$ such that $X R X = I$.\nLet's define a candidate matrix using a function of $R$ through its spectral decomposition. For a function $f: \\mathbb{R}^+ \\to \\mathbb{R}$, we can define $f(R)$ as $f(R) = U f(\\Lambda) U^{\\top}$, where $f(\\Lambda) = \\operatorname{diag}(f(\\lambda_1), \\dots, f(\\lambda_n))$.\nFor the inverse square root, the relevant function is $f(x) = x^{-1/2} = 1/\\sqrt{x}$. Since all $\\lambda_i > 0$, this function is well-defined.\nLet's propose $R^{-1/2} = U \\Lambda^{-1/2} U^{\\top}$, where $\\Lambda^{-1/2} = \\operatorname{diag}(\\lambda_1^{-1/2}, \\dots, \\lambda_n^{-1/2})$.\n\nWe must first verify that this matrix satisfies the condition $R^{-1/2} R R^{-1/2} = I$.\nSubstituting the spectral decompositions for $R$ and our proposed $R^{-1/2}$:\n$$\n(U \\Lambda^{-1/2} U^{\\top}) (U \\Lambda U^{\\top}) (U \\Lambda^{-1/2} U^{\\top})\n$$\nUsing the associativity of matrix multiplication and the orthogonality of $U$ (i.e., $U^{\\top} U = I$), we can simplify the expression:\n$$\nU \\Lambda^{-1/2} (U^{\\top} U) \\Lambda (U^{\\top} U) \\Lambda^{-1/2} U^{\\top} = U \\Lambda^{-1/2} I \\Lambda I \\Lambda^{-1/2} U^{\\top} = U (\\Lambda^{-1/2} \\Lambda \\Lambda^{-1/2}) U^{\\top}\n$$\nSince $\\Lambda$, $\\Lambda^{1/2}$, and $\\Lambda^{-1/2}$ are diagonal, they commute. The product inside the parentheses is:\n$$\n\\Lambda^{-1/2} \\Lambda \\Lambda^{-1/2} = \\Lambda^{-1/2} \\Lambda^{1/2} \\Lambda^{1/2} \\Lambda^{-1/2} = (\\Lambda^{-1/2} \\Lambda^{1/2}) (\\Lambda^{1/2} \\Lambda^{-1/2}) = I \\cdot I = I\n$$\nThe product is the identity matrix $I$ because for each diagonal element, we have $\\lambda_i^{-1/2} \\cdot \\lambda_i \\cdot \\lambda_i^{-1/2} = \\lambda_i^0 = 1$.\nThus, the expression simplifies to $U I U^{\\top} = U U^{\\top} = I$, which confirms the condition.\n\nNext, we must verify that our proposed $R^{-1/2}$ is itself SPD.\n1.  Symmetry: We check if $(R^{-1/2})^{\\top} = R^{-1/2}$.\n    $$\n    (U \\Lambda^{-1/2} U^{\\top})^{\\top} = (U^{\\top})^{\\top} (\\Lambda^{-1/2})^{\\top} U^{\\top} = U (\\Lambda^{-1/2}) U^{\\top} = R^{-1/2}\n    $$\n    This holds because $\\Lambda^{-1/2}$ is a diagonal matrix and is therefore symmetric.\n2.  Positive Definiteness: We must show that for any non-zero vector $x \\in \\mathbb{R}^n$, the quadratic form $x^{\\top} R^{-1/2} x$ is strictly positive.\n    $$\n    x^{\\top} R^{-1/2} x = x^{\\top} (U \\Lambda^{-1/2} U^{\\top}) x = (x^{\\top} U) \\Lambda^{-1/2} (U^{\\top} x)\n    $$\n    Let $y = U^{\\top} x$. Since $U$ is orthogonal and thus invertible, $x \\neq 0$ implies $y \\neq 0$. The expression becomes:\n    $$\n    y^{\\top} \\Lambda^{-1/2} y = \\sum_{i=1}^{n} y_i^2 \\lambda_i^{-1/2}\n    $$\n    Given that $\\lambda_i > 0$ for all $i$, we have $\\lambda_i^{-1/2} = 1/\\sqrt{\\lambda_i} > 0$. Since $y \\neq 0$, at least one component $y_i$ is non-zero, so $y_i^2 > 0$. The expression is a sum of non-negative terms, with at least one strictly positive term. Therefore, the sum is strictly positive, $x^{\\top} R^{-1/2} x > 0$, and $R^{-1/2}$ is positive definite.\nThe uniqueness of this SPD inverse square root is a standard result from matrix theory.\n\nNow, we compute $R^{-1/2}$ for the specific $2 \\times 2$ case.\nThe given orthogonal matrix $U$ is a rotation by $\\theta = \\pi/6$:\n$$\nU = \\begin{pmatrix} \\cos(\\pi/6) & -\\sin(\\pi/6) \\\\ \\sin(\\pi/6) & \\cos(\\pi/6) \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{3}}{2} & -\\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{\\sqrt{3}}{2} \\end{pmatrix}\n$$\nIts transpose is:\n$$\nU^{\\top} = \\begin{pmatrix} \\frac{\\sqrt{3}}{2} & \\frac{1}{2} \\\\ -\\frac{1}{2} & \\frac{\\sqrt{3}}{2} \\end{pmatrix}\n$$\nThe eigenvalue matrix is $\\Lambda = \\operatorname{diag}(9, 10^{-6})$.\nWe compute $\\Lambda^{-1/2}$:\n$$\n\\Lambda^{-1/2} = \\operatorname{diag}(9^{-1/2}, (10^{-6})^{-1/2}) = \\operatorname{diag}\\left(\\frac{1}{3}, \\frac{1}{10^{-3}}\\right) = \\begin{pmatrix} \\frac{1}{3} & 0 \\\\ 0 & 1000 \\end{pmatrix}\n$$\nNow we compute $R^{-1/2} = U \\Lambda^{-1/2} U^{\\top}$:\n$$\nR^{-1/2} = \\begin{pmatrix} \\frac{\\sqrt{3}}{2} & -\\frac{1}{2} \\\\ \\frac{1}{2} & \\frac{\\sqrt{3}}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{3} & 0 \\\\ 0 & 1000 \\end{pmatrix} \\begin{pmatrix} \\frac{\\sqrt{3}}{2} & \\frac{1}{2} \\\\ -\\frac{1}{2} & \\frac{\\sqrt{3}}{2} \\end{pmatrix}\n$$\nFirst, multiply the first two matrices:\n$$\nU \\Lambda^{-1/2} = \\begin{pmatrix} \\frac{\\sqrt{3}}{2} \\cdot \\frac{1}{3} & -\\frac{1}{2} \\cdot 1000 \\\\ \\frac{1}{2} \\cdot \\frac{1}{3} & \\frac{\\sqrt{3}}{2} \\cdot 1000 \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{3}}{6} & -500 \\\\ \\frac{1}{6} & 500\\sqrt{3} \\end{pmatrix}\n$$\nNow, multiply this result by $U^{\\top}$:\n$$\nR^{-1/2} = \\begin{pmatrix} \\frac{\\sqrt{3}}{6} & -500 \\\\ \\frac{1}{6} & 500\\sqrt{3} \\end{pmatrix} \\begin{pmatrix} \\frac{\\sqrt{3}}{2} & \\frac{1}{2} \\\\ -\\frac{1}{2} & \\frac{\\sqrt{3}}{2} \\end{pmatrix}\n$$\nWe compute each entry of the resulting $2 \\times 2$ matrix:\nThe entry $(1,1)$ is:\n$$\n\\left(\\frac{\\sqrt{3}}{6}\\right)\\left(\\frac{\\sqrt{3}}{2}\\right) + (-500)\\left(-\\frac{1}{2}\\right) = \\frac{3}{12} + 250 = \\frac{1}{4} + 250 = \\frac{1+1000}{4} = \\frac{1001}{4}\n$$\nThe entry $(1,2)$ is:\n$$\n\\left(\\frac{\\sqrt{3}}{6}\\right)\\left(\\frac{1}{2}\\right) + (-500)\\left(\\frac{\\sqrt{3}}{2}\\right) = \\frac{\\sqrt{3}}{12} - 250\\sqrt{3} = \\sqrt{3}\\left(\\frac{1}{12} - 250\\right) = \\sqrt{3}\\left(\\frac{1 - 3000}{12}\\right) = -\\frac{2999\\sqrt{3}}{12}\n$$\nThe entry $(2,1)$ is, as expected by symmetry, the same as $(1,2)$:\n$$\n\\left(\\frac{1}{6}\\right)\\left(\\frac{\\sqrt{3}}{2}\\right) + (500\\sqrt{3})\\left(-\\frac{1}{2}\\right) = \\frac{\\sqrt{3}}{12} - 250\\sqrt{3} = -\\frac{2999\\sqrt{3}}{12}\n$$\nThe entry $(2,2)$ is:\n$$\n\\left(\\frac{1}{6}\\right)\\left(\\frac{1}{2}\\right) + (500\\sqrt{3})\\left(\\frac{\\sqrt{3}}{2}\\right) = \\frac{1}{12} + \\frac{500 \\cdot 3}{2} = \\frac{1}{12} + 750 = \\frac{1 + 9000}{12} = \\frac{9001}{12}\n$$\nSo the final matrix is:\n$$\nR^{-1/2} = \\begin{pmatrix} \\frac{1001}{4} & -\\frac{2999\\sqrt{3}}{12} \\\\ -\\frac{2999\\sqrt{3}}{12} & \\frac{9001}{12} \\end{pmatrix}\n$$\n\nFinally, we discuss the numerical pitfalls of an ill-conditioned $R$. The condition number of a symmetric matrix $R$ is $\\kappa(R) = \\lambda_{\\max}/\\lambda_{\\min}$. In this example, $\\kappa(R) = 9/10^{-6} = 9 \\times 10^6$, which is large, indicating $R$ is ill-conditioned.\n1.  **Numerical Instability**: Directly forming and inverting $R$ can lead to significant loss of precision. Finite-precision arithmetic can cause small relative errors in the entries of $R$ to be massively amplified in the entries of $R^{-1}$ and consequently $R^{-1/2}$. This is particularly problematic if $R$ is formed from data. Numerical computation of the eigen-decomposition can also be unstable for ill-conditioned matrices, leading to inaccurate eigenvectors or eigenvalues.\n2.  **Loss of Positive Definiteness**: Round-off errors during matrix manipulations (e.g., forming $R = Y Y^{\\top}$ or attempting a Cholesky decomposition $R=LL^{\\top}$) can cause a theoretically SPD matrix to lose this property numerically. The Cholesky algorithm, for instance, would fail if a diagonal element becomes non-positive.\n\nStrategies to mitigate these issues in square root filter formulations include:\n1.  **Avoiding Explicit Matrix Inversion**: The action of $R^{-1/2}$ on a vector (pre-whitening) should be computed without explicitly forming the matrix $R^{-1/2}$. Using the spectral decomposition, the product $y = R^{-1/2}v$ can be computed via three numerically stable steps: $y_1=U^{\\top}v$, $y_2=\\Lambda^{-1/2}y_1$, and $y=Uy_2$. This sequence of a rotation, a diagonal scaling, and another rotation is generally much more stable than forming the dense matrix $R^{-1/2}$ and then multiplying.\n2.  **Regularization**: A common technique is to regularize $R$ by adding a small multiple of the identity matrix, i.e., using $R_{\\alpha} = R + \\alpha I$ for some small $\\alpha > 0$. This ensures that all eigenvalues are bounded below by $\\alpha$, improving the condition number to $\\kappa(R_{\\alpha}) = (\\lambda_{\\max}+\\alpha)/(\\lambda_{\\min}+\\alpha) \\approx \\lambda_{\\max}/\\alpha$. This improves numerical stability at the cost of introducing a small bias.\n3.  **Processing Observations Sequentially**: If $R$ is diagonal (uncorrelated observation errors), it is trivial to invert. If $R$ is non-diagonal but its off-diagonal terms are small, one might process observations serially, treating them as uncorrelated. This avoids dealing with the ill-conditioned full matrix $R$ entirely, though it is an approximation of the true error statistics.\n4.  **Using High-Quality Decompositions**: Employing robust numerical linear algebra library routines, such as those for Singular Value Decomposition (SVD), which is numerically the most stable matrix decomposition and is equivalent to the spectral decomposition for SPD matrices, is crucial for obtaining an accurate decomposition to begin with.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1001}{4} & -\\frac{2999\\sqrt{3}}{12} \\\\ -\\frac{2999\\sqrt{3}}{12} & \\frac{9001}{12} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Having explored the matrix square root, we now apply this concept within a complete filtering algorithm. The Ensemble Transform Kalman Filter (ETKF) is a classic and widely used square-root filter that performs the analysis update in the smaller ensemble space, offering significant computational advantages. This hands-on problem  provides a concrete, step-by-step walkthrough of the ETKF update for a toy system, clarifying how the forecast anomalies are transformed into analysis anomalies using a matrix derived in ensemble space. This practice is essential for demystifying the core mechanics of the ETKF.",
            "id": "3420585",
            "problem": "Consider a linear, Gaussian data assimilation setting with a state vector of dimension $n = 3$, observation dimension $p = 3$, and an ensemble of $m = 4$ members. The observation operator is the identity $H = I_{3}$, and the observation error covariance is $R = I_{3}$. You are given the forecast anomaly matrix $A^{f} \\in \\mathbb{R}^{3 \\times 4}$ (columns are ensemble member anomalies about the forecast mean) as\n$$\nA^{f} = \\sqrt{3}\n\\begin{bmatrix}\n1 & 0 & 0 & -1 \\\\\n0 & 1 & 0 & -1 \\\\\n0 & 0 & 1 & -1\n\\end{bmatrix}.\n$$\nAssume the Ensemble Transform Kalman Filter (ETKF) formulation of the square root filter in ensemble space with normalized anomalies and observation whitening.\n\nUsing fundamental properties of the Kalman filter and the ensemble-space square root construction, carry out the following steps:\n- Form the normalized forecast anomalies $X^{f} = A^{f} / \\sqrt{m - 1}$ and the whitened, projected anomalies $\\tilde{S} = R^{-1/2} H X^{f}$.\n- Using the ensemble-space posterior covariance $\\tilde{P}^{a}$ implied by the Kalman update in the whitened space, construct the symmetric square root transform $T = (\\tilde{P}^{a})^{1/2}$.\n- Update the forecast anomalies via the ETKF transform to obtain the analysis anomalies $A^{a} = A^{f} T$.\n\nFinally, compute the squared Frobenius norm of the analysis anomaly matrix $A^{a}$, defined by $||A^{a}||_{F}^{2} = \\operatorname{tr}\\big((A^{a})^{\\top} A^{a}\\big)$, and provide its exact value. No rounding is necessary, and no physical units are required. Your final answer must be a single real number.",
            "solution": "The objective is to compute the squared Frobenius norm of the analysis anomaly matrix, $||A^{a}||_{F}^{2}$. The analysis anomalies $A^a$ are obtained by transforming the forecast anomalies $A^f$ using a transform matrix $T$.\n\nFirst, we follow the prescribed steps to define the components of the ETKF update. The number of ensemble members is $m=4$. The normalized forecast anomaly matrix $X^{f}$ is defined as:\n$$\nX^{f} = \\frac{A^{f}}{\\sqrt{m-1}} = \\frac{1}{\\sqrt{3}} \\left( \\sqrt{3} \\begin{bmatrix} 1 & 0 & 0 & -1 \\\\ 0 & 1 & 0 & -1 \\\\ 0 & 0 & 1 & -1 \\end{bmatrix} \\right) = \\begin{bmatrix} 1 & 0 & 0 & -1 \\\\ 0 & 1 & 0 & -1 \\\\ 0 & 0 & 1 & -1 \\end{bmatrix}\n$$\nNext, we form the whitened, projected anomalies $\\tilde{S}$. Given the observation operator $H = I_{3}$ and the observation error covariance $R = I_{3}$, we have $R^{-1/2} = (I_{3})^{-1/2} = I_{3}$.\n$$\n\\tilde{S} = R^{-1/2} H X^{f} = I_{3} \\cdot I_{3} \\cdot X^{f} = X^{f} = \\begin{bmatrix} 1 & 0 & 0 & -1 \\\\ 0 & 1 & 0 & -1 \\\\ 0 & 0 & 1 & -1 \\end{bmatrix}\n$$\nThe ETKF updates the forecast anomalies using a transform matrix $T$ such that $A^{a} = A^{f} T$. The matrix $T$ is the symmetric square root of the posterior covariance in the whitened ensemble space, $\\tilde{P}^{a}$.\n$$\nT = (\\tilde{P}^{a})^{1/2}\n$$\nwhere\n$$\n\\tilde{P}^{a} = (I_{m} + \\tilde{S}^{\\top} \\tilde{S})^{-1}\n$$\nHere, $I_m$ is the identity matrix of size $m \\times m$, which is $4 \\times 4$. The squared Frobenius norm is given by the trace of $(A^{a})^{\\top} A^{a}$.\n$$\n||A^{a}||_{F}^{2} = \\operatorname{tr}\\big((A^{a})^{\\top} A^{a}\\big) = \\operatorname{tr}\\big((A^{f} T)^{\\top} (A^{f} T)\\big) = \\operatorname{tr}\\big(T^{\\top} (A^{f})^{\\top} A^{f} T\\big)\n$$\nSince $T$ is symmetric ($T^{\\top} = T$), and using the cyclic property of the trace operator ($\\operatorname{tr}(ABC) = \\operatorname{tr}(BCA)$), we can write:\n$$\n||A^{a}||_{F}^{2} = \\operatorname{tr}\\big((A^{f})^{\\top} A^{f} T^{2}\\big)\n$$\nBy definition, $T^{2} = ((\\tilde{P}^{a})^{1/2})^{2} = \\tilde{P}^{a} = (I_{m} + \\tilde{S}^{\\top} \\tilde{S})^{-1}$. Substituting this into the equation for the norm:\n$$\n||A^{a}||_{F}^{2} = \\operatorname{tr}\\big((A^{f})^{\\top} A^{f} (I_{m} + \\tilde{S}^{\\top} \\tilde{S})^{-1}\\big)\n$$\nWe can relate $(A^{f})^{\\top} A^{f}$ to $\\tilde{S}^{\\top} \\tilde{S}$. From the definition $A^{f} = \\sqrt{m-1} X^{f}$ and for this problem $\\tilde{S} = X^{f}$, we have $A^{f} = \\sqrt{m-1} \\tilde{S}$.\n$$\n(A^{f})^{\\top} A^{f} = (\\sqrt{m-1} \\tilde{S})^{\\top} (\\sqrt{m-1} \\tilde{S}) = (m-1) \\tilde{S}^{\\top} \\tilde{S}\n$$\nSubstituting this yields a more elegant expression for the norm:\n$$\n||A^{a}||_{F}^{2} = \\operatorname{tr}\\big((m-1) \\tilde{S}^{\\top} \\tilde{S} (I_{m} + \\tilde{S}^{\\top} \\tilde{S})^{-1}\\big) = (m-1) \\operatorname{tr}\\big(\\tilde{S}^{\\top} \\tilde{S} (I_{m} + \\tilde{S}^{\\top} \\tilde{S})^{-1}\\big)\n$$\nLet $K = \\tilde{S}^{\\top} \\tilde{S}$. We use the matrix identity $K(I+K)^{-1} = (I+K-I)(I+K)^{-1} = I - (I+K)^{-1}$.\n$$\n||A^{a}||_{F}^{2} = (m-1) \\operatorname{tr}\\big(I_{m} - (I_{m} + \\tilde{S}^{\\top} \\tilde{S})^{-1}\\big) = (m-1) \\Big(\\operatorname{tr}(I_{m}) - \\operatorname{tr}\\big((I_{m} + \\tilde{S}^{\\top} \\tilde{S})^{-1}\\big)\\Big)\n$$\nNow, we compute the necessary matrices. First, $\\tilde{S}^{\\top} \\tilde{S}$:\n$$\n\\tilde{S}^{\\top} \\tilde{S} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ -1 & -1 & -1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 & 0 & -1 \\\\ 0 & 1 & 0 & -1 \\\\ 0 & 0 & 1 & -1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 & -1 \\\\ 0 & 1 & 0 & -1 \\\\ 0 & 0 & 1 & -1 \\\\ -1 & -1 & -1 & 3 \\end{bmatrix}\n$$\nNext, we compute $I_{4} + \\tilde{S}^{\\top} \\tilde{S}$:\n$$\nI_{4} + \\tilde{S}^{\\top} \\tilde{S} = \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} + \\begin{bmatrix} 1 & 0 & 0 & -1 \\\\ 0 & 1 & 0 & -1 \\\\ 0 & 0 & 1 & -1 \\\\ -1 & -1 & -1 & 3 \\end{bmatrix} = \\begin{bmatrix} 2 & 0 & 0 & -1 \\\\ 0 & 2 & 0 & -1 \\\\ 0 & 0 & 2 & -1 \\\\ -1 & -1 & -1 & 4 \\end{bmatrix}\n$$\nLet this matrix be $M$. We need to compute its inverse, $M^{-1} = (I_{4} + \\tilde{S}^{\\top} \\tilde{S})^{-1}$. We can use blockwise matrix inversion. Let $M = \\begin{pmatrix} A & B \\\\ C & D \\end{pmatrix}$ where $A = 2I_{3}$, $B = \\begin{pmatrix} -1 \\\\ -1 \\\\ -1 \\end{pmatrix}$, $C = \\begin{pmatrix} -1 & -1 & -1 \\end{pmatrix}$, and $D = 4$. The inverse is given by standard formulas, and the result is:\n$$\nM^{-1} = (I_{4} + \\tilde{S}^{\\top} \\tilde{S})^{-1} = \\frac{1}{10} \\begin{bmatrix} 6 & 1 & 1 & 2 \\\\ 1 & 6 & 1 & 2 \\\\ 1 & 1 & 6 & 2 \\\\ 2 & 2 & 2 & 4 \\end{bmatrix}\n$$\nThe trace of this matrix is:\n$$\n\\operatorname{tr}\\big((I_{4} + \\tilde{S}^{\\top} \\tilde{S})^{-1}\\big) = \\frac{1}{10} (6+6+6+4) = \\frac{22}{10} = \\frac{11}{5}\n$$\nAlso, $\\operatorname{tr}(I_{4}) = 4$. Substituting these values into our expression for the squared Frobenius norm:\n$$\n||A^{a}||_{F}^{2} = (m-1) \\Big(\\operatorname{tr}(I_{m}) - \\operatorname{tr}\\big((I_{m} + \\tilde{S}^{\\top} \\tilde{S})^{-1}\\big)\\Big) = (4-1) \\left(4 - \\frac{11}{5}\\right)\n$$\n$$\n||A^{a}||_{F}^{2} = 3 \\left(\\frac{20}{5} - \\frac{11}{5}\\right) = 3 \\left(\\frac{9}{5}\\right) = \\frac{27}{5}\n$$\nThe exact value of the squared Frobenius norm of the analysis anomaly matrix is $\\frac{27}{5}$.",
            "answer": "$$\\boxed{\\frac{27}{5}}$$"
        },
        {
            "introduction": "A key feature of the exact Kalman filter is that for linear systems with uncorrelated observation errors, assimilating observations one-by-one (serially) yields the same result as assimilating them all at once (in a batch). This final exercise  investigates whether this fundamental property holds for ensemble square-root filters. By comparing the posterior covariance from both a batch and a serial update, you will verify this equivalence, reinforcing the theoretical consistency between ensemble methods and the underlying Bayesian framework from which they are derived. This builds a deeper understanding of the filter's behavior and its connection to exact solutions.",
            "id": "3420557",
            "problem": "Consider a linear, time-invariant data assimilation setting with a two-dimensional state and a three-member ensemble. Let the prior (forecast) ensemble members be $x_{1}^{f} = \\begin{pmatrix}0 \\\\ 0\\end{pmatrix}$, $x_{2}^{f} = \\begin{pmatrix}2 \\\\ 0\\end{pmatrix}$, and $x_{3}^{f} = \\begin{pmatrix}0 \\\\ 2\\end{pmatrix}$. The ensemble mean is $\\bar{x}^{f} \\in \\mathbb{R}^{2}$, and the ensemble anomaly matrix $A^{f} \\in \\mathbb{R}^{2 \\times 3}$ is defined by column-wise anomalies normalized by $\\sqrt{N-1}$ with $N=3$, so that the sample prior covariance satisfies $P^{f} = A^{f} (A^{f})^{\\top}$. Two independent scalar observations are collected with linear observation operators $H_{1} = \\begin{pmatrix}1 & 0\\end{pmatrix}$ and $H_{2} = \\begin{pmatrix}0 & 1\\end{pmatrix}$ and with observation error covariance $R = I_{2}$. The actual observation values are arbitrary but fixed; the noise is zero-mean Gaussian. Assume a deterministic square-root update is used in both batch and serial forms.\n\nUsing only the foundational linear-Gaussian Bayesian update and the definition of ensemble square-root filtering, do the following:\n\n1. For the batch update using the Ensemble Transform Kalman Filter (ETKF), compute the posterior covariance $P^{a}_{\\mathrm{batch}}$ implied by the ensemble square-root formulation without explicitly constructing a transform in ensemble space. Begin from the linear-Gaussian covariance update and show how it matches the ensemble square-root covariance expression.\n\n2. For the serial square-root update (assimilate the two scalar observations one at a time in the order $H_{1}$ then $H_{2}$), compute the final posterior covariance $P^{a}_{\\mathrm{serial}}$.\n\n3. Compare the two posterior covariances by computing the Frobenius norm of their difference, that is, compute $\\|P^{a}_{\\mathrm{batch}} - P^{a}_{\\mathrm{serial}}\\|_{F}$.\n\nGive your final answer as the exact value of the Frobenius norm (no rounding). No units are required.",
            "solution": "The problem requires the computation and comparison of posterior covariance matrices from batch and serial square-root filtering updates.\n\nFirst, we compute the prior ensemble statistics.\nThe given prior ensemble members are $x_{1}^{f} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, $x_{2}^{f} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}$, and $x_{3}^{f} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}$.\nThe number of ensemble members is $N=3$.\nThe prior ensemble mean $\\bar{x}^{f}$ is:\n$$ \\bar{x}^{f} = \\frac{1}{N} \\sum_{i=1}^{N} x_{i}^{f} = \\frac{1}{3} \\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} \\right) = \\frac{1}{3} \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} $$\nThe prior ensemble anomalies are $x'_{i}{}^{f} = x_{i}^{f} - \\bar{x}^{f}$:\n$$ x'_{1}{}^{f} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - \\frac{1}{3} \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = -\\frac{1}{3} \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} $$\n$$ x'_{2}{}^{f} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix} - \\frac{1}{3} \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 4 \\\\ -2 \\end{pmatrix} $$\n$$ x'_{3}{}^{f} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} - \\frac{1}{3} \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} -2 \\\\ 4 \\end{pmatrix} $$\nThe ensemble anomaly matrix $A^{f}$ has columns defined as the anomalies normalized by $\\sqrt{N-1} = \\sqrt{2}$:\n$$ A^{f} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} -\\frac{2}{3} & \\frac{4}{3} & -\\frac{2}{3} \\\\ -\\frac{2}{3} & -\\frac{2}{3} & \\frac{4}{3} \\end{pmatrix} = \\frac{\\sqrt{2}}{3} \\begin{pmatrix} -1 & 2 & -1 \\\\ -1 & -1 & 2 \\end{pmatrix} $$\nThe prior error covariance matrix $P^{f}$ is given by $P^{f} = A^{f} (A^{f})^{\\top}$:\n$$ P^{f} = \\left(\\frac{\\sqrt{2}}{3}\\right)^2 \\begin{pmatrix} -1 & 2 & -1 \\\\ -1 & -1 & 2 \\end{pmatrix} \\begin{pmatrix} -1 & -1 \\\\ 2 & -1 \\\\ -1 & 2 \\end{pmatrix} = \\frac{2}{9} \\begin{pmatrix} (1+4+1) & (1-2-2) \\\\ (1-2-2) & (1+1+4) \\end{pmatrix} $$\n$$ P^{f} = \\frac{2}{9} \\begin{pmatrix} 6 & -3 \\\\ -3 & 6 \\end{pmatrix} = \\frac{2}{3} \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix} $$\n\nFor later use, we compute the inverse of $P^{f}$:\n$$ \\det(P^{f}) = \\left(\\frac{2}{3}\\right)^2 (4-1) = \\frac{4}{9} \\times 3 = \\frac{4}{3} $$\n$$ (P^{f})^{-1} = \\frac{1}{4/3} \\frac{2}{3} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\frac{3}{4} \\frac{2}{3} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\begin{pmatrix} 1 & 1/2 \\\\ 1/2 & 1 \\end{pmatrix} $$\n\n1. For the batch update, we consider both observations simultaneously. The full observation operator is $H = \\begin{pmatrix} H_1 \\\\ H_2 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2$, and the observation error covariance is $R=I_2$.\nThe problem requires starting from the linear-Gaussian Bayesian update and showing how it matches the ensemble square-root covariance expression.\nThe standard Bayesian posterior covariance update is:\n$$ P^{a} = \\left( (P^{f})^{-1} + H^{\\top} R^{-1} H \\right)^{-1} $$\nThe Ensemble Transform Kalman Filter (ETKF) posterior covariance is expressed as $P^{a} = A^{a}(A^{a})^{\\top}$, where the updated anomaly matrix is $A^{a} = A^{f} T$. The transform matrix $T$ is chosen such that $T T^{\\top} = (I + (H A^{f})^{\\top} R^{-1} H A^{f})^{-1}$. This gives:\n$$ P^{a}_{\\mathrm{ETKF}} = A^{f} (I + (H A^{f})^{\\top} R^{-1} H A^{f})^{-1} (A^{f})^{\\top} $$\nTo show their equivalence, we start with the Kalman gain form of the update, $P^{a} = (I - KH)P^{f}$, where $K = P^{f}H^{\\top}(HP^{f}H^{\\top} + R)^{-1}$.\nSubstitute $P^{f}=A^{f}(A^{f})^{\\top}$:\n$$ P^{a} = A^{f}(A^{f})^{\\top} - A^{f}(A^{f})^{\\top}H^{\\top} (H A^{f}(A^{f})^{\\top}H^{\\top} + R)^{-1} H A^{f}(A^{f})^{\\top} $$\n$$ P^{a} = A^{f} \\left[ I - (A^{f})^{\\top}H^{\\top} (H A^{f}(A^{f})^{\\top}H^{\\top} + R)^{-1} H A^{f} \\right] (A^{f})^{\\top} $$\nLet $Y = H A^{f}$. The expression in the brackets becomes $I - Y^{\\top}(YY^{\\top} + R)^{-1}Y$. Using the Woodbury matrix identity, which states $(A+UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1}+VA^{-1}U)^{-1}VA^{-1}$, one can prove the identity $I - Y^{\\top}(YY^{\\top} + R)^{-1}Y = (I+Y^{\\top}R^{-1}Y)^{-1}$. Thus:\n$$ P^{a} = A^{f} (I+Y^{\\top}R^{-1}Y)^{-1} (A^{f})^{\\top} = A^{f} (I+(HA^{f})^{\\top}R^{-1}HA^{f})^{-1} (A^{f})^{\\top} $$\nThis completes the derivation. Now we compute $P^{a}_{\\mathrm{batch}}$ using the simpler Bayesian update form:\n$$ P^{a}_{\\mathrm{batch}} = \\left( (P^{f})^{-1} + H^{\\top} R^{-1} H \\right)^{-1} = \\left( \\begin{pmatrix} 1 & 1/2 \\\\ 1/2 & 1 \\end{pmatrix} + I_{2}^{\\top} I_{2}^{-1} I_{2} \\right)^{-1} $$\n$$ P^{a}_{\\mathrm{batch}} = \\left( \\begin{pmatrix} 1 & 1/2 \\\\ 1/2 & 1 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right)^{-1} = \\left( \\begin{pmatrix} 2 & 1/2 \\\\ 1/2 & 2 \\end{pmatrix} \\right)^{-1} $$\nThe determinant of this matrix is $(2)(2) - (1/2)(1/2) = 4 - 1/4 = 15/4$.\n$$ P^{a}_{\\mathrm{batch}} = \\frac{1}{15/4} \\begin{pmatrix} 2 & -1/2 \\\\ -1/2 & 2 \\end{pmatrix} = \\frac{4}{15} \\begin{pmatrix} 2 & -1/2 \\\\ -1/2 & 2 \\end{pmatrix} = \\begin{pmatrix} 8/15 & -2/15 \\\\ -2/15 & 8/15 \\end{pmatrix} $$\n\n2. For the serial update, we assimilate the observations one by one. The observation error covariance $R=I_2$ implies the two scalar observations have error variances $r_1=1$ and $r_2=1$ and are uncorrelated.\nFirst, assimilate observation $y_1$ with operator $H_1 = \\begin{pmatrix} 1 & 0 \\end{pmatrix}$ and error variance $r_1=1$. The prior is $P^{f}$. Let the intermediate posterior covariance be $P^{a,1}$.\n$$ P^{a,1} = \\left( (P^{f})^{-1} + H_1^{\\top} r_1^{-1} H_1 \\right)^{-1} $$\n$$ H_1^{\\top} r_1^{-1} H_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} (1) \\begin{pmatrix} 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\n$$ P^{a,1} = \\left( \\begin{pmatrix} 1 & 1/2 \\\\ 1/2 & 1 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\right)^{-1} = \\left( \\begin{pmatrix} 2 & 1/2 \\\\ 1/2 & 1 \\end{pmatrix} \\right)^{-1} $$\nThe determinant is $(2)(1) - (1/2)(1/2) = 2 - 1/4 = 7/4$.\n$$ P^{a,1} = \\frac{1}{7/4} \\begin{pmatrix} 1 & -1/2 \\\\ -1/2 & 2 \\end{pmatrix} = \\frac{4}{7} \\begin{pmatrix} 1 & -1/2 \\\\ -1/2 & 2 \\end{pmatrix} = \\begin{pmatrix} 4/7 & -2/7 \\\\ -2/7 & 8/7 \\end{pmatrix} $$\nNext, assimilate observation $y_2$ with operator $H_2 = \\begin{pmatrix} 0 & 1 \\end{pmatrix}$ and error variance $r_2=1$. The prior for this step is $P^{a,1}$. The final posterior covariance is $P^{a}_{\\mathrm{serial}}$.\n$$ P^{a}_{\\mathrm{serial}} = \\left( (P^{a,1})^{-1} + H_2^{\\top} r_2^{-1} H_2 \\right)^{-1} $$\nFrom the previous calculation, $(P^{a,1})^{-1} = \\begin{pmatrix} 2 & 1/2 \\\\ 1/2 & 1 \\end{pmatrix}$.\n$$ H_2^{\\top} r_2^{-1} H_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} (1) \\begin{pmatrix} 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} $$\n$$ P^{a}_{\\mathrm{serial}} = \\left( \\begin{pmatrix} 2 & 1/2 \\\\ 1/2 & 1 \\end{pmatrix} + \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\right)^{-1} = \\left( \\begin{pmatrix} 2 & 1/2 \\\\ 1/2 & 2 \\end{pmatrix} \\right)^{-1} $$\nThis is the same matrix we inverted for the batch update.\n$$ P^{a}_{\\mathrm{serial}} = \\frac{4}{15} \\begin{pmatrix} 2 & -1/2 \\\\ -1/2 & 2 \\end{pmatrix} = \\begin{pmatrix} 8/15 & -2/15 \\\\ -2/15 & 8/15 \\end{pmatrix} $$\n\n3. Compare the two posterior covariances and compute the Frobenius norm of their difference.\n$$ P^{a}_{\\mathrm{batch}} = \\begin{pmatrix} 8/15 & -2/15 \\\\ -2/15 & 8/15 \\end{pmatrix} $$\n$$ P^{a}_{\\mathrm{serial}} = \\begin{pmatrix} 8/15 & -2/15 \\\\ -2/15 & 8/15 \\end{pmatrix} $$\nThe two matrices are identical, as expected for a linear problem with uncorrelated observation errors.\nThe difference is the zero matrix:\n$$ P^{a}_{\\mathrm{batch}} - P^{a}_{\\mathrm{serial}} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} $$\nThe Frobenius norm of a matrix $M$ is $\\|M\\|_{F} = \\sqrt{\\sum_{i,j} |m_{ij}|^2}$.\n$$ \\|P^{a}_{\\mathrm{batch}} - P^{a}_{\\mathrm{serial}}\\|_{F} = \\sqrt{0^2 + 0^2 + 0^2 + 0^2} = 0 $$",
            "answer": "$$\\boxed{0}$$"
        }
    ]
}