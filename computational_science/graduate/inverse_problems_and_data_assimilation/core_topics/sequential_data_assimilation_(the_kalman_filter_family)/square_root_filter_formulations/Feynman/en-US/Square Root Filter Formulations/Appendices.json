{
    "hands_on_practices": [
        {
            "introduction": "This exercise provides a foundational walkthrough of the Ensemble Transform Kalman Filter (ETKF). By working with a small, well-defined system, you will manually trace the flow of information from the forecast anomalies to the analysis anomalies. This practice is designed to demystify the abstract equations and solidify your understanding of the core matrix transformations that define the square-root update in ensemble space .",
            "id": "3420585",
            "problem": "Consider a linear, Gaussian data assimilation setting with a state vector of dimension $n = 3$, observation dimension $p = 3$, and an ensemble of $m = 4$ members. The observation operator is the identity $H = I_{3}$, and the observation error covariance is $R = I_{3}$. You are given the forecast anomaly matrix $A^{f} \\in \\mathbb{R}^{3 \\times 4}$ (columns are ensemble member anomalies about the forecast mean) as\n$$\nA^{f} = \\sqrt{3}\n\\begin{bmatrix}\n1  0  0  -1 \\\\\n0  1  0  -1 \\\\\n0  0  1  -1\n\\end{bmatrix}.\n$$\nAssume the Ensemble Transform Kalman Filter (ETKF) formulation of the square root filter in ensemble space with normalized anomalies and observation whitening.\n\nUsing fundamental properties of the Kalman filter and the ensemble-space square root construction, carry out the following steps:\n- Form the normalized forecast anomalies $X^{f} = A^{f} / \\sqrt{m - 1}$ and the whitened, projected anomalies $\\tilde{S} = R^{-1/2} H X^{f}$.\n- Using the ensemble-space posterior covariance $\\tilde{P}^{a}$ implied by the Kalman update in the whitened space, construct the symmetric square root transform $T = (\\tilde{P}^{a})^{1/2}$.\n- Update the forecast anomalies via the ETKF transform to obtain the analysis anomalies $A^{a} = A^{f} T$.\n\nFinally, compute the squared Frobenius norm of the analysis anomaly matrix $A^{a}$, defined by $||A^{a}||_{F}^{2} = \\operatorname{tr}\\big((A^{a})^{\\top} A^{a}\\big)$, and provide its exact value. No rounding is necessary, and no physical units are required. Your final answer must be a single real number.",
            "solution": "The user-provided problem is critically validated by following the specified procedure.\n\n### Step 1: Extract Givens\n- State vector dimension: $n = 3$\n- Observation dimension: $p = 3$\n- Ensemble size: $m = 4$\n- Observation operator: $H = I_{3}$ (identity matrix of size $3 \\times 3$)\n- Observation error covariance: $R = I_{3}$ (identity matrix of size $3 \\times 3$)\n- Forecast anomaly matrix: $A^{f} = \\sqrt{3} \\begin{bmatrix} 1  0  0  -1 \\\\ 0  1  0  -1 \\\\ 0  0  1  -1 \\end{bmatrix}$, with dimensions $n \\times m$ ($3 \\times 4$).\n- Filter formulation: Ensemble Transform Kalman Filter (ETKF) in ensemble space with normalized anomalies and observation whitening.\n- Required calculation: The squared Frobenius norm of the analysis anomaly matrix $A^{a}$, defined as $||A^{a}||_{F}^{2} = \\operatorname{tr}\\big((A^{a})^{\\top} A^{a}\\big)$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is set within the established mathematical framework of data assimilation, specifically using the Ensemble Kalman Filter, which is a standard method in geophysical sciences, engineering, and other fields. The formulation is consistent with recognized literature on square root filters (e.g., the ETKF developed by Bishop et al., 2001). All components are mathematically sound.\n- **Well-Posed:** The problem is clearly defined with all necessary matrices and parameters provided. The objective is unambiguous: to compute a specific scalar quantity. The structure guarantees a unique solution.\n- **Objective:** The problem is stated using precise mathematical terminology and symbols, free of any subjective or ambiguous language.\n\n### Step 3: Verdict and Action\nThe problem is scientifically grounded, well-posed, objective, and self-contained. It is a valid exercise in applying the principles of ensemble data assimilation. Therefore, a solution will be provided.\n\n### Solution Derivation\nThe objective is to compute the squared Frobenius norm of the analysis anomaly matrix, $||A^{a}||_{F}^{2}$. The analysis anomalies $A^a$ are obtained by transforming the forecast anomalies $A^f$ using a transform matrix $T$.\n\nFirst, we follow the prescribed steps to define the components of the ETKF update. The number of ensemble members is $m=4$. The normalized forecast anomaly matrix $X^{f}$ is defined as:\n$$\nX^{f} = \\frac{A^{f}}{\\sqrt{m-1}} = \\frac{1}{\\sqrt{3}} \\left( \\sqrt{3} \\begin{bmatrix} 1  0  0  -1 \\\\ 0  1  0  -1 \\\\ 0  0  1  -1 \\end{bmatrix} \\right) = \\begin{bmatrix} 1  0  0  -1 \\\\ 0  1  0  -1 \\\\ 0  0  1  -1 \\end{bmatrix}\n$$\nNext, we form the whitened, projected anomalies $\\tilde{S}$. Given the observation operator $H = I_{3}$ and the observation error covariance $R = I_{3}$, we have $R^{-1/2} = (I_{3})^{-1/2} = I_{3}$.\n$$\n\\tilde{S} = R^{-1/2} H X^{f} = I_{3} \\cdot I_{3} \\cdot X^{f} = X^{f} = \\begin{bmatrix} 1  0  0  -1 \\\\ 0  1  0  -1 \\\\ 0  0  1  -1 \\end{bmatrix}\n$$\nThe ETKF updates the forecast anomalies using a transform matrix $T$ such that $A^{a} = A^{f} T$. The matrix $T$ is the symmetric square root of the posterior covariance in the whitened ensemble space, $\\tilde{P}^{a}$.\n$$\nT = (\\tilde{P}^{a})^{1/2}\n$$\nwhere\n$$\n\\tilde{P}^{a} = (I_{m} + \\tilde{S}^{\\top} \\tilde{S})^{-1}\n$$\nHere, $I_m$ is the identity matrix of size $m \\times m$, which is $4 \\times 4$. The squared Frobenius norm is given by the trace of $(A^{a})^{\\top} A^{a}$.\n$$\n||A^{a}||_{F}^{2} = \\operatorname{tr}\\big((A^{a})^{\\top} A^{a}\\big) = \\operatorname{tr}\\big((A^{f} T)^{\\top} (A^{f} T)\\big) = \\operatorname{tr}\\big(T^{\\top} (A^{f})^{\\top} A^{f} T\\big)\n$$\nSince $T$ is symmetric ($T^{\\top} = T$), and using the cyclic property of the trace operator ($\\operatorname{tr}(ABC) = \\operatorname{tr}(BCA)$), we can write:\n$$\n||A^{a}||_{F}^{2} = \\operatorname{tr}\\big((A^{f})^{\\top} A^{f} T^{2}\\big)\n$$\nBy definition, $T^{2} = ((\\tilde{P}^{a})^{1/2})^{2} = \\tilde{P}^{a} = (I_{m} + \\tilde{S}^{\\top} \\tilde{S})^{-1}$. Substituting this into the equation for the norm:\n$$\n||A^{a}||_{F}^{2} = \\operatorname{tr}\\big((A^{f})^{\\top} A^{f} (I_{m} + \\tilde{S}^{\\top} \\tilde{S})^{-1}\\big)\n$$\nWe can relate $(A^{f})^{\\top} A^{f}$ to $\\tilde{S}^{\\top} \\tilde{S}$. From the definition $A^{f} = \\sqrt{m-1} X^{f}$ and for this problem $\\tilde{S} = X^{f}$, we have $A^{f} = \\sqrt{m-1} \\tilde{S}$.\n$$\n(A^{f})^{\\top} A^{f} = (\\sqrt{m-1} \\tilde{S})^{\\top} (\\sqrt{m-1} \\tilde{S}) = (m-1) \\tilde{S}^{\\top} \\tilde{S}\n$$\nSubstituting this yields a more elegant expression for the norm:\n$$\n||A^{a}||_{F}^{2} = \\operatorname{tr}\\big((m-1) \\tilde{S}^{\\top} \\tilde{S} (I_{m} + \\tilde{S}^{\\top} \\tilde{S})^{-1}\\big) = (m-1) \\operatorname{tr}\\big(\\tilde{S}^{\\top} \\tilde{S} (I_{m} + \\tilde{S}^{\\top} \\tilde{S})^{-1}\\big)\n$$\nLet $K = \\tilde{S}^{\\top} \\tilde{S}$. We use the matrix identity $K(I+K)^{-1} = (I+K-I)(I+K)^{-1} = I - (I+K)^{-1}$.\n$$\n||A^{a}||_{F}^{2} = (m-1) \\operatorname{tr}\\big(I_{m} - (I_{m} + \\tilde{S}^{\\top} \\tilde{S})^{-1}\\big) = (m-1) \\Big(\\operatorname{tr}(I_{m}) - \\operatorname{tr}\\big((I_{m} + \\tilde{S}^{\\top} \\tilde{S})^{-1}\\big)\\Big)\n$$\nNow, we compute the necessary matrices. First, $\\tilde{S}^{\\top} \\tilde{S}$:\n$$\n\\tilde{S}^{\\top} \\tilde{S} = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\\\ -1  -1  -1 \\end{bmatrix} \\begin{bmatrix} 1  0  0  -1 \\\\ 0  1  0  -1 \\\\ 0  0  1  -1 \\end{bmatrix} = \\begin{bmatrix} 1  0  0  -1 \\\\ 0  1  0  -1 \\\\ 0  0  1  -1 \\\\ -1  -1  -1  3 \\end{bmatrix}\n$$\nNext, we compute $I_{4} + \\tilde{S}^{\\top} \\tilde{S}$:\n$$\nI_{4} + \\tilde{S}^{\\top} \\tilde{S} = \\begin{bmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  0  1  0 \\\\ 0  0  0  1 \\end{bmatrix} + \\begin{bmatrix} 1  0  0  -1 \\\\ 0  1  0  -1 \\\\ 0  0  1  -1 \\\\ -1  -1  -1  3 \\end{bmatrix} = \\begin{bmatrix} 2  0  0  -1 \\\\ 0  2  0  -1 \\\\ 0  0  2  -1 \\\\ -1  -1  -1  4 \\end{bmatrix}\n$$\nLet this matrix be $M$. We need to compute its inverse, $M^{-1} = (I_{4} + \\tilde{S}^{\\top} \\tilde{S})^{-1}$. We can use blockwise matrix inversion. Let $M = \\begin{pmatrix} A  B \\\\ C  D \\end{pmatrix}$ where $A = 2I_{3}$, $B = \\begin{pmatrix} -1 \\\\ -1 \\\\ -1 \\end{pmatrix}$, $C = \\begin{pmatrix} -1  -1  -1 \\end{pmatrix}$, and $D = 4$. The inverse is given by standard formulas, and the result is:\n$$\nM^{-1} = (I_{4} + \\tilde{S}^{\\top} \\tilde{S})^{-1} = \\frac{1}{10} \\begin{bmatrix} 6  1  1  2 \\\\ 1  6  1  2 \\\\ 1  1  6  2 \\\\ 2  2  2  4 \\end{bmatrix}\n$$\nThe trace of this matrix is:\n$$\n\\operatorname{tr}\\big((I_{4} + \\tilde{S}^{\\top} \\tilde{S})^{-1}\\big) = \\frac{1}{10} (6+6+6+4) = \\frac{22}{10} = \\frac{11}{5}\n$$\nAlso, $\\operatorname{tr}(I_{4}) = 4$. Substituting these values into our expression for the squared Frobenius norm:\n$$\n||A^{a}||_{F}^{2} = (m-1) \\Big(\\operatorname{tr}(I_{m}) - \\operatorname{tr}\\big((I_{m} + \\tilde{S}^{\\top} \\tilde{S})^{-1}\\big)\\Big) = (4-1) \\left(4 - \\frac{11}{5}\\right)\n$$\n$$\n||A^{a}||_{F}^{2} = 3 \\left(\\frac{20}{5} - \\frac{11}{5}\\right) = 3 \\left(\\frac{9}{5}\\right) = \\frac{27}{5}\n$$\nThe exact value of the squared Frobenius norm of the analysis anomaly matrix is $\\frac{27}{5}$.",
            "answer": "$$\\boxed{\\frac{27}{5}}$$"
        },
        {
            "introduction": "A key step in many square-root filter formulations is the pre-whitening of observations, which requires computing the inverse square root of the observation error covariance matrix, $R^{-1/2}$. This problem focuses on the practical computation of this matrix via spectral decomposition and explores the numerical challenges that arise. By analyzing a hypothetical ill-conditioned matrix, you will gain insight into why direct computation can be unstable and learn about the importance of numerically robust techniques in real-world applications .",
            "id": "3420548",
            "problem": "In square root formulations of ensemble-based data assimilation, such as the Ensemble Kalman Filter (EnKF) implemented in deterministic Square Root Filter (SRF) form, innovation vectors are often pre-whitened using the inverse square root of the observation error covariance. Let the observation error covariance matrix be a symmetric positive definite (SPD) matrix $R \\in \\mathbb{R}^{n \\times n}$, so that by the spectral theorem there exists an orthonormal eigen-decomposition $R = U \\Lambda U^{\\top}$ with $U \\in \\mathbb{R}^{n \\times n}$ orthogonal and $\\Lambda = \\operatorname{diag}(\\lambda_{1},\\dots,\\lambda_{n})$ strictly positive.\n\nStarting from the definitions of SPD matrices and the spectral theorem, derive the inverse square root $R^{-1/2}$ in terms of $U$ and $\\Lambda$ as the unique SPD matrix that satisfies $R^{-1/2} R R^{-1/2} = I$. Then, compute $R^{-1/2}$ explicitly for the following $2 \\times 2$ case designed to mimic an ill-conditioned observation error covariance:\n- The orthogonal matrix $U$ is a rotation by angle $\\theta = \\pi/6$, namely\n$$\nU = \\begin{pmatrix}\n\\cos(\\pi/6)  -\\sin(\\pi/6) \\\\\n\\sin(\\pi/6)  \\cos(\\pi/6)\n\\end{pmatrix}.\n$$\n- The eigenvalue matrix is\n$$\n\\Lambda = \\begin{pmatrix}\n9  0 \\\\\n0  10^{-6}\n\\end{pmatrix}.\n$$\n\nProvide the final $R^{-1/2}$ as a single exact analytic $2 \\times 2$ matrix expression with entries written using radicals and rational numbers where appropriate; do not approximate or round. In addition, briefly explain, based on first principles, potential numerical pitfalls that can arise when $R$ is ill-conditioned and strategies to mitigate them in the context of square root filter formulations. The final answer must be the matrix $R^{-1/2}$ only; any discussion should appear solely in the solution.",
            "solution": "The problem is evaluated as valid as it is scientifically grounded in linear algebra and numerical analysis, well-posed with all necessary information provided, and stated objectively. The premises and objectives are clear, consistent, and formalizable.\n\nThe problem requires a two-part derivation and calculation followed by a conceptual explanation.\n\nFirst, we derive the general form of the inverse square root $R^{-1/2}$ for a symmetric positive definite (SPD) matrix $R$.\nGiven that $R$ is an SPD matrix in $\\mathbb{R}^{n \\times n}$, the spectral theorem guarantees the existence of an orthonormal eigen-decomposition $R = U \\Lambda U^{\\top}$, where $U$ is an orthogonal matrix ($U U^{\\top} = U^{\\top} U = I$) whose columns are the eigenvectors of $R$, and $\\Lambda = \\operatorname{diag}(\\lambda_{1},\\dots,\\lambda_{n})$ is a diagonal matrix of the corresponding eigenvalues. Since $R$ is positive definite, all its eigenvalues are strictly positive, i.e., $\\lambda_i  0$ for all $i=1,\\dots,n$.\n\nWe are looking for a unique SPD matrix $X = R^{-1/2}$ such that $X R X = I$.\nLet's define a candidate matrix using a function of $R$ through its spectral decomposition. For a function $f: \\mathbb{R}^+ \\to \\mathbb{R}$, we can define $f(R)$ as $f(R) = U f(\\Lambda) U^{\\top}$, where $f(\\Lambda) = \\operatorname{diag}(f(\\lambda_1), \\dots, f(\\lambda_n))$.\nFor the inverse square root, the relevant function is $f(x) = x^{-1/2} = 1/\\sqrt{x}$. Since all $\\lambda_i  0$, this function is well-defined.\nLet's propose $R^{-1/2} = U \\Lambda^{-1/2} U^{\\top}$, where $\\Lambda^{-1/2} = \\operatorname{diag}(\\lambda_1^{-1/2}, \\dots, \\lambda_n^{-1/2})$.\n\nWe must first verify that this matrix satisfies the condition $R^{-1/2} R R^{-1/2} = I$.\nSubstituting the spectral decompositions for $R$ and our proposed $R^{-1/2}$:\n$$\n(U \\Lambda^{-1/2} U^{\\top}) (U \\Lambda U^{\\top}) (U \\Lambda^{-1/2} U^{\\top})\n$$\nUsing the associativity of matrix multiplication and the orthogonality of $U$ (i.e., $U^{\\top} U = I$), we can simplify the expression:\n$$\nU \\Lambda^{-1/2} (U^{\\top} U) \\Lambda (U^{\\top} U) \\Lambda^{-1/2} U^{\\top} = U \\Lambda^{-1/2} I \\Lambda I \\Lambda^{-1/2} U^{\\top} = U (\\Lambda^{-1/2} \\Lambda \\Lambda^{-1/2}) U^{\\top}\n$$\nSince $\\Lambda$, $\\Lambda^{1/2}$, and $\\Lambda^{-1/2}$ are diagonal, they commute. The product inside the parentheses is:\n$$\n\\Lambda^{-1/2} \\Lambda \\Lambda^{-1/2} = \\Lambda^{-1/2} \\Lambda^{1/2} \\Lambda^{1/2} \\Lambda^{-1/2} = (\\Lambda^{-1/2} \\Lambda^{1/2}) (\\Lambda^{1/2} \\Lambda^{-1/2}) = I \\cdot I = I\n$$\nThe product is the identity matrix $I$ because for each diagonal element, we have $\\lambda_i^{-1/2} \\cdot \\lambda_i \\cdot \\lambda_i^{-1/2} = \\lambda_i^0 = 1$.\nThus, the expression simplifies to $U I U^{\\top} = U U^{\\top} = I$, which confirms the condition.\n\nNext, we must verify that our proposed $R^{-1/2}$ is itself SPD.\n1.  Symmetry: We check if $(R^{-1/2})^{\\top} = R^{-1/2}$.\n    $$\n    (U \\Lambda^{-1/2} U^{\\top})^{\\top} = (U^{\\top})^{\\top} (\\Lambda^{-1/2})^{\\top} U^{\\top} = U (\\Lambda^{-1/2}) U^{\\top} = R^{-1/2}\n    $$\n    This holds because $\\Lambda^{-1/2}$ is a diagonal matrix and is therefore symmetric.\n2.  Positive Definiteness: We must show that for any non-zero vector $x \\in \\mathbb{R}^n$, the quadratic form $x^{\\top} R^{-1/2} x$ is strictly positive.\n    $$\n    x^{\\top} R^{-1/2} x = x^{\\top} (U \\Lambda^{-1/2} U^{\\top}) x = (x^{\\top} U) \\Lambda^{-1/2} (U^{\\top} x)\n    $$\n    Let $y = U^{\\top} x$. Since $U$ is orthogonal and thus invertible, $x \\neq 0$ implies $y \\neq 0$. The expression becomes:\n    $$\n    y^{\\top} \\Lambda^{-1/2} y = \\sum_{i=1}^{n} y_i^2 \\lambda_i^{-1/2}\n    $$\n    Given that $\\lambda_i  0$ for all $i$, we have $\\lambda_i^{-1/2} = 1/\\sqrt{\\lambda_i}  0$. Since $y \\neq 0$, at least one component $y_i$ is non-zero, so $y_i^2  0$. The expression is a sum of non-negative terms, with at least one strictly positive term. Therefore, the sum is strictly positive, $x^{\\top} R^{-1/2} x  0$, and $R^{-1/2}$ is positive definite.\nThe uniqueness of this SPD inverse square root is a standard result from matrix theory.\n\nNow, we compute $R^{-1/2}$ for the specific $2 \\times 2$ case.\nThe given orthogonal matrix $U$ is a rotation by $\\theta = \\pi/6$:\n$$\nU = \\begin{pmatrix} \\cos(\\pi/6)  -\\sin(\\pi/6) \\\\ \\sin(\\pi/6)  \\cos(\\pi/6) \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{3}}{2}  -\\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{\\sqrt{3}}{2} \\end{pmatrix}\n$$\nIts transpose is:\n$$\nU^{\\top} = \\begin{pmatrix} \\frac{\\sqrt{3}}{2}  \\frac{1}{2} \\\\ -\\frac{1}{2}  \\frac{\\sqrt{3}}{2} \\end{pmatrix}\n$$\nThe eigenvalue matrix is $\\Lambda = \\operatorname{diag}(9, 10^{-6})$.\nWe compute $\\Lambda^{-1/2}$:\n$$\n\\Lambda^{-1/2} = \\operatorname{diag}(9^{-1/2}, (10^{-6})^{-1/2}) = \\operatorname{diag}\\left(\\frac{1}{3}, \\frac{1}{10^{-3}}\\right) = \\begin{pmatrix} \\frac{1}{3}  0 \\\\ 0  1000 \\end{pmatrix}\n$$\nNow we compute $R^{-1/2} = U \\Lambda^{-1/2} U^{\\top}$:\n$$\nR^{-1/2} = \\begin{pmatrix} \\frac{\\sqrt{3}}{2}  -\\frac{1}{2} \\\\ \\frac{1}{2}  \\frac{\\sqrt{3}}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{3}  0 \\\\ 0  1000 \\end{pmatrix} \\begin{pmatrix} \\frac{\\sqrt{3}}{2}  \\frac{1}{2} \\\\ -\\frac{1}{2}  \\frac{\\sqrt{3}}{2} \\end{pmatrix}\n$$\nFirst, multiply the first two matrices:\n$$\nU \\Lambda^{-1/2} = \\begin{pmatrix} \\frac{\\sqrt{3}}{2} \\cdot \\frac{1}{3}  -\\frac{1}{2} \\cdot 1000 \\\\ \\frac{1}{2} \\cdot \\frac{1}{3}  \\frac{\\sqrt{3}}{2} \\cdot 1000 \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{3}}{6}  -500 \\\\ \\frac{1}{6}  500\\sqrt{3} \\end{pmatrix}\n$$\nNow, multiply this result by $U^{\\top}$:\n$$\nR^{-1/2} = \\begin{pmatrix} \\frac{\\sqrt{3}}{6}  -500 \\\\ \\frac{1}{6}  500\\sqrt{3} \\end{pmatrix} \\begin{pmatrix} \\frac{\\sqrt{3}}{2}  \\frac{1}{2} \\\\ -\\frac{1}{2}  \\frac{\\sqrt{3}}{2} \\end{pmatrix}\n$$\nWe compute each entry of the resulting $2 \\times 2$ matrix:\nThe entry $(1,1)$ is:\n$$\n\\left(\\frac{\\sqrt{3}}{6}\\right)\\left(\\frac{\\sqrt{3}}{2}\\right) + (-500)\\left(-\\frac{1}{2}\\right) = \\frac{3}{12} + 250 = \\frac{1}{4} + 250 = \\frac{1+1000}{4} = \\frac{1001}{4}\n$$\nThe entry $(1,2)$ is:\n$$\n\\left(\\frac{\\sqrt{3}}{6}\\right)\\left(\\frac{1}{2}\\right) + (-500)\\left(\\frac{\\sqrt{3}}{2}\\right) = \\frac{\\sqrt{3}}{12} - 250\\sqrt{3} = \\sqrt{3}\\left(\\frac{1}{12} - 250\\right) = \\sqrt{3}\\left(\\frac{1 - 3000}{12}\\right) = -\\frac{2999\\sqrt{3}}{12}\n$$\nThe entry $(2,1)$ is, as expected by symmetry, the same as $(1,2)$:\n$$\n\\left(\\frac{1}{6}\\right)\\left(\\frac{\\sqrt{3}}{2}\\right) + (500\\sqrt{3})\\left(-\\frac{1}{2}\\right) = \\frac{\\sqrt{3}}{12} - 250\\sqrt{3} = -\\frac{2999\\sqrt{3}}{12}\n$$\nThe entry $(2,2)$ is:\n$$\n\\left(\\frac{1}{6}\\right)\\left(\\frac{1}{2}\\right) + (500\\sqrt{3})\\left(\\frac{\\sqrt{3}}{2}\\right) = \\frac{1}{12} + \\frac{500 \\cdot 3}{2} = \\frac{1}{12} + 750 = \\frac{1 + 9000}{12} = \\frac{9001}{12}\n$$\nSo the final matrix is:\n$$\nR^{-1/2} = \\begin{pmatrix} \\frac{1001}{4}  -\\frac{2999\\sqrt{3}}{12} \\\\ -\\frac{2999\\sqrt{3}}{12}  \\frac{9001}{12} \\end{pmatrix}\n$$\n\nFinally, we discuss the numerical pitfalls of an ill-conditioned $R$. The condition number of a symmetric matrix $R$ is $\\kappa(R) = \\lambda_{\\max}/\\lambda_{\\min}$. In this example, $\\kappa(R) = 9/10^{-6} = 9 \\times 10^6$, which is large, indicating $R$ is ill-conditioned.\n1.  **Numerical Instability**: Directly forming and inverting $R$ can lead to significant loss of precision. Finite-precision arithmetic can cause small relative errors in the entries of $R$ to be massively amplified in the entries of $R^{-1}$ and consequently $R^{-1/2}$. This is particularly problematic if $R$ is formed from data. Numerical computation of the eigen-decomposition can also be unstable for ill-conditioned matrices, leading to inaccurate eigenvectors or eigenvalues.\n2.  **Loss of Positive Definiteness**: Round-off errors during matrix manipulations (e.g., forming $R = Y Y^{\\top}$ or attempting a Cholesky decomposition $R=LL^{\\top}$) can cause a theoretically SPD matrix to lose this property numerically. The Cholesky algorithm, for instance, would fail if a diagonal element becomes non-positive.\n\nStrategies to mitigate these issues in square root filter formulations include:\n1.  **Avoiding Explicit Matrix Inversion**: The action of $R^{-1/2}$ on a vector (pre-whitening) should be computed without explicitly forming the matrix $R^{-1/2}$. Using the spectral decomposition, the product $y = R^{-1/2}v$ can be computed via three numerically stable steps: $y_1=U^{\\top}v$, $y_2=\\Lambda^{-1/2}y_1$, and $y=Uy_2$. This sequence of a rotation, a diagonal scaling, and another rotation is generally much more stable than forming the dense matrix $R^{-1/2}$ and then multiplying.\n2.  **Regularization**: A common technique is to regularize $R$ by adding a small multiple of the identity matrix, i.e., using $R_{\\alpha} = R + \\alpha I$ for some small $\\alpha  0$. This ensures that all eigenvalues are bounded below by $\\alpha$, improving the condition number to $\\kappa(R_{\\alpha}) = (\\lambda_{\\max}+\\alpha)/(\\lambda_{\\min}+\\alpha) \\approx \\lambda_{\\max}/\\alpha$. This improves numerical stability at the cost of introducing a small bias.\n3.  **Processing Observations Sequentially**: If $R$ is diagonal (uncorrelated observation errors), it is trivial to invert. If $R$ is non-diagonal but its off-diagonal terms are small, one might process observations serially, treating them as uncorrelated. This avoids dealing with the ill-conditioned full matrix $R$ entirely, though it is an approximation of the true error statistics.\n4.  **Using High-Quality Decompositions**: Employing robust numerical linear algebra library routines, such as those for Singular Value Decomposition (SVD), which is numerically the most stable matrix decomposition and is equivalent to the spectral decomposition for SPD matrices, is crucial for obtaining an accurate decomposition to begin with.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1001}{4}  -\\frac{2999\\sqrt{3}}{12} \\\\ -\\frac{2999\\sqrt{3}}{12}  \\frac{9001}{12} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "This final practice moves from isolated calculations to a comprehensive implementation, integrating the ETKF with other essential components of modern data assimilation. You will develop a program for a Local Ensemble Transform Kalman Filter (LETKF) that incorporates both covariance inflation and localization. This capstone exercise illustrates how the square-root filter serves as a robust engine within a larger, more realistic data assimilation framework designed to handle high-dimensional systems .",
            "id": "3420543",
            "problem": "Consider a one-dimensional chain of $n$ grid points with an identity observation operator, and suppose we aim to perform a Local Ensemble Transform Kalman Filter (LETKF) update using a square-root filter formulation. The fundamental base consists of the linear Gaussian inverse problem and the Kalman filter: given a linear observation operator $H$ and observation error covariance $R$, the posterior mean and covariance are defined by the Gaussian conditioning rules. In ensemble-based data assimilation, the forecast covariance is approximated by the sample covariance of ensemble anomalies.\n\nYou are required to implement a complete program that, for each specified test case, constructs a prior ensemble, applies multiplicative covariance inflation, performs a localized Ensemble Transform Kalman Filter (ETKF) analysis independently at each grid point using an explicitly prescribed localization matrix $C$, and returns the analysis mean vector. The localization is performed in observation space at each grid point $i$ by scaling the observation error covariance according to the corresponding row of $C$.\n\nDefinitions and setup:\n- The state dimension is $n$, and the ensemble size is $N_e$.\n- The observation operator is $H = I_n$, the $n \\times n$ identity matrix, and the observation error covariance is $R = \\sigma^2 I_n$ for a scalar $\\sigma^2$.\n- The prior ensemble matrix is denoted $X^f \\in \\mathbb{R}^{n \\times N_e}$, with forecast mean $\\bar{x}^f \\in \\mathbb{R}^n$ and forecast anomalies $A^f = X^f - \\bar{x}^f \\mathbf{1}^\\top \\in \\mathbb{R}^{n \\times N_e}$, where $\\mathbf{1} \\in \\mathbb{R}^{N_e}$ is the vector of ones. Define the normalized anomalies by $A = \\frac{1}{\\sqrt{N_e - 1}} A^f$.\n- Use multiplicative covariance inflation with factor $\\lambda \\geq 1$, meaning that the anomalies are scaled as $A^f \\leftarrow \\lambda A^f$ prior to the analysis step.\n- The observations are generated as $y = x^{\\text{true}} + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$ and a specified deterministic $x^{\\text{true}}$.\n- The localization matrix $C \\in \\mathbb{R}^{n \\times n}$ is specified by a compactly supported Wendland function on a one-dimensional chain with a support radius parameter $L  0$. For indices $i, j \\in \\{0, 1, \\dots, n-1\\}$, define the distance $d_{ij} = |i - j|$ and the normalized distance $r_{ij} = \\frac{d_{ij}}{L}$. The entries of $C$ are\n$$\nC_{ij} = \\phi(r_{ij}), \\quad \\text{with} \\quad\n\\phi(r) =\n\\begin{cases}\n(1 - r)^4 (1 + 4 r),  0 \\le r \\le 1, \\\\\n0,  r  1.\n\\end{cases}\n$$\n\nLocal ETKF update at each grid point:\n- For each grid point $i \\in \\{0, 1, \\dots, n-1\\}$, form a localized observation selection by including indices $j$ such that $C_{ij}  \\varepsilon_{\\text{loc}}$, where $\\varepsilon_{\\text{loc}}$ is a small positive threshold used to avoid numerical singularities. Define a localized inverse observation covariance at $i$ by a diagonal matrix\n$$\nR_i^{-1} = \\operatorname{diag}\\left( \\frac{C_{ij}^2}{\\sigma^2} \\right)_{j \\in \\mathcal{J}_i},\n$$\nwhere $\\mathcal{J}_i = \\{ j \\mid C_{ij}  \\varepsilon_{\\text{loc}} \\}$.\n- Let $Y_i \\in \\mathbb{R}^{|\\mathcal{J}_i| \\times N_e}$ be the localized normalized ensemble observation anomalies defined by $Y_i = H_{\\mathcal{J}_i} A$, where $H_{\\mathcal{J}_i}$ selects the rows of $A$ corresponding to the indices in $\\mathcal{J}_i$; since $H = I_n$, this simply picks rows $j \\in \\mathcal{J}_i$ from $A$.\n- Let $d_{\\mathcal{J}_i} = y_{\\mathcal{J}_i} - \\bar{x}^f_{\\mathcal{J}_i}$ be the localized innovation vector.\n- In the square-root ETKF, the analysis mean increment at grid point $i$ is obtained via an ensemble-space weight vector $w_i \\in \\mathbb{R}^{N_e}$ computed as\n$$\nw_i = \\left( I_{N_e} + Y_i^\\top R_i^{-1} Y_i \\right)^{-1} Y_i^\\top R_i^{-1} d_{\\mathcal{J}_i}.\n$$\nThe analysis mean at $i$ is then\n$$\nx^a_i = \\bar{x}^f_i + A_{i,:} w_i,\n$$\nwhere $A_{i,:}$ denotes the $i$-th row of the normalized anomalies matrix $A$.\n- The local square-root transform for the anomalies at grid point $i$ is defined by\n$$\nT_i = \\left( I_{N_e} + Y_i^\\top R_i^{-1} Y_i \\right)^{-1/2},\n$$\nwhich is the symmetric inverse square root. The local analysis anomalies at row $i$ are updated by $A^a_{i,:} = A_{i,:} T_i$. Although the final requested output is the analysis mean only, you must implement the computation of $T_i$ to adhere to the square-root filter formulation.\n\nNumerical requirements:\n- Use a small threshold $\\varepsilon_{\\text{loc}}$ to exclude observations with near-zero localization weights; this avoids division by zero in $R_i^{-1}$. If $\\mathcal{J}_i$ is empty, set $x^a_i = \\bar{x}^f_i$ and $T_i = I_{N_e}$.\n- All linear algebra operations must be numerically stable. Compute the inverse square root using the spectral decomposition of the symmetric positive definite matrix $I_{N_e} + Y_i^\\top R_i^{-1} Y_i$.\n\nTest suite:\nImplement the above for the following three test cases. For each case, set the random number generator to the specified seed and define $x^{\\text{true}}$ deterministically by\n$$\nx^{\\text{true}}_k = \\sin\\left( \\frac{2\\pi k}{n} \\right) + \\frac{1}{2} \\cos\\left( \\frac{4\\pi k}{n} \\right), \\quad k = 0, 1, \\dots, n-1.\n$$\nGenerate the prior ensemble as $X^f = x^{\\text{true}} \\mathbf{1}^\\top + \\eta$ with $\\eta_{k,\\ell} \\sim \\mathcal{N}(0, 0.5^2)$ independently for all $k$ and $\\ell$. Generate observations as $y = x^{\\text{true}} + \\varepsilon$ with $\\varepsilon_k \\sim \\mathcal{N}(0, \\sigma^2)$ independently. The parameters for each case are:\n1. Case A (happy path): $n = 8$, $N_e = 10$, $\\lambda = 1.05$, $\\sigma^2 = 0.01$, $L = 2.0$, seed $= 1$.\n2. Case B (boundary localization): $n = 8$, $N_e = 10$, $\\lambda = 1.00$, $\\sigma^2 = 0.01$, $L = 0.5$, seed $= 2$.\n3. Case C (small ensemble and stronger inflation): $n = 12$, $N_e = 6$, $\\lambda = 1.20$, $\\sigma^2 = 0.0625$, $L = 3.0$, seed $= 3$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each result is the analysis mean vector for the corresponding test case, printed as a Python-like list of decimal numbers rounded to six decimal places. For example, the output should look like\n$[ [a_1,\\dots,a_n], [b_1,\\dots,b_n], [c_1,\\dots,c_n] ]$\nwith each bracketed list corresponding to the analysis mean for a test case, in order A, B, C.",
            "solution": "The problem is grounded in the linear-Gaussian inverse problem and the Kalman filter. For a linear observation model with Gaussian errors, the posterior distribution is Gaussian with mean and covariance determined by the Kalman filter. In ensemble-based filtering, we approximate the forecast covariance by the sample covariance of ensemble anomalies.\n\nBegin with the forecast ensemble $X^f \\in \\mathbb{R}^{n \\times N_e}$. Its ensemble mean is $\\bar{x}^f = \\frac{1}{N_e} X^f \\mathbf{1}$ and the anomalies are $A^f = X^f - \\bar{x}^f \\mathbf{1}^\\top$. Multiplicative covariance inflation scales the anomalies by a factor $\\lambda \\geq 1$, i.e., $A^f \\leftarrow \\lambda A^f$, which counteracts underestimation of the forecast spread due to sampling error or model deficiencies. Normalize the anomalies by $A = \\frac{1}{\\sqrt{N_e - 1}} A^f$ so that $A A^\\top$ estimates the prior covariance.\n\nThe Kalman filter analysis increment under identity $H$ and diagonal $R$ can be expressed in ensemble space using the Ensemble Transform Kalman Filter (ETKF). The key idea is to express the posterior mean increment as a linear combination of the anomaly columns. Define the observation-space anomaly matrix $Y = H A$. With $H = I_n$, we have $Y = A$. The posterior mean increment is $\\Delta x = A w$, where the ensemble-space weight $w \\in \\mathbb{R}^{N_e}$ is determined by projecting the innovation through the ensemble subspace. For nonlocalized ETKF, with diagonal $R = \\sigma^2 I_n$, the weight solves\n$$\nw = \\left( I_{N_e} + Y^\\top R^{-1} Y \\right)^{-1} Y^\\top R^{-1} d, \\quad d = y - \\bar{x}^f.\n$$\nThis follows from the identity for the Kalman update $\\Delta x = K d$, the ensemble approximation $P^f \\approx A A^\\top$, and the algebraic reduction using the Sherman–Morrison–Woodbury formula to ensemble space. The analysis anomalies are updated via a symmetric square-root transform to ensure the posterior covariance matches the Kalman analysis covariance within the ensemble subspace:\n$$\nA^a = A T, \\quad T = \\left( I_{N_e} + Y^\\top R^{-1} Y \\right)^{-1/2}.\n$$\nThe inverse square root is computed through spectral decomposition: if $B = I_{N_e} + Y^\\top R^{-1} Y$ has eigen-decomposition $B = Q \\Lambda Q^\\top$ with $Q$ orthogonal and $\\Lambda$ positive diagonal, then $B^{-1/2} = Q \\Lambda^{-1/2} Q^\\top$.\n\nLocalization is introduced to reduce spurious long-range correlations that result from finite ensemble sizes. In the Local Ensemble Transform Kalman Filter (LETKF), the analysis is performed independently at each state location $i$ using only nearby observations determined by a localization matrix. We adopt observation-space localization at each grid point $i$ by scaling the observation errors according to the localization weights, which is equivalent to performing a Schur-product taper in observation-space when $R$ is diagonal. Specifically, for each $i$, define the localized inverse observation covariance\n$$\nR_i^{-1} = \\operatorname{diag}\\left( \\frac{C_{ij}^2}{\\sigma^2} \\right)_{j \\in \\mathcal{J}_i},\n$$\nwhere $\\mathcal{J}_i = \\{ j \\mid C_{ij}  \\varepsilon_{\\text{loc}} \\}$ excludes near-zero weights to avoid numerical issues. The localized observation anomalies are $Y_i = H_{\\mathcal{J}_i} A$, which selects rows of $A$ at the indices in $\\mathcal{J}_i$, and the localized innovation is $d_{\\mathcal{J}_i} = y_{\\mathcal{J}_i} - \\bar{x}^f_{\\mathcal{J}_i}$. The ensemble-space weight for location $i$ is then\n$$\nw_i = \\left( I_{N_e} + Y_i^\\top R_i^{-1} Y_i \\right)^{-1} Y_i^\\top R_i^{-1} d_{\\mathcal{J}_i},\n$$\nand the local analysis mean is\n$$\nx^a_i = \\bar{x}^f_i + A_{i,:} w_i.\n$$\nThe local square-root transform for anomalies is\n$$\nT_i = \\left( I_{N_e} + Y_i^\\top R_i^{-1} Y_i \\right)^{-1/2},\n$$\napplied to the $i$-th row anomalies as $A^a_{i,:} = A_{i,:} T_i$, ensuring consistency with the square-root formulation at each location.\n\nConstructing the localization matrix $C$ on a one-dimensional chain uses a compactly supported Wendland taper that is both positive definite and smooth, ensuring numerical stability in practice. For indices $i, j$, define $d_{ij} = |i - j|$ and $r_{ij} = \\frac{d_{ij}}{L}$, and\n$$\nC_{ij} = \\phi(r_{ij}) =\n\\begin{cases}\n(1 - r_{ij})^4 (1 + 4 r_{ij}),  0 \\le r_{ij} \\le 1, \\\\\n0,  r_{ij}  1.\n\\end{cases}\n$$\n\nAlgorithmic steps for each test case:\n1. Set the random seed and construct the deterministic truth $x^{\\text{true}}_k = \\sin\\left( \\frac{2\\pi k}{n} \\right) + \\frac{1}{2} \\cos\\left( \\frac{4\\pi k}{n} \\right)$.\n2. Generate the prior ensemble $X^f = x^{\\text{true}} \\mathbf{1}^\\top + \\eta$ with independent Gaussian noise $\\eta_{k,\\ell} \\sim \\mathcal{N}(0, 0.25)$.\n3. Generate observations $y = x^{\\text{true}} + \\varepsilon$ with independent $\\varepsilon_k \\sim \\mathcal{N}(0, \\sigma^2)$.\n4. Compute $\\bar{x}^f$ and $A^f$, apply inflation $A^f \\leftarrow \\lambda A^f$, and form $A = \\frac{1}{\\sqrt{N_e - 1}} A^f$.\n5. Build the localization matrix $C$ using the Wendland function with radius $L$.\n6. For each grid point $i$, form the localized index set $\\mathcal{J}_i = \\{ j : C_{ij}  \\varepsilon_{\\text{loc}} \\}$, compute $R_i^{-1}$, $Y_i$, and $d_{\\mathcal{J}_i}$, then compute the weight $w_i$ and the local transform $T_i$ via spectral decomposition. Set $x^a_i = \\bar{x}^f_i + A_{i,:} w_i$. If $\\mathcal{J}_i$ is empty, set $x^a_i = \\bar{x}^f_i$ and $T_i = I_{N_e}$.\n7. Return the analysis mean vector $x^a$.\n\nThe program implements this procedure for the three specified test cases. It prints a single line containing the three analysis mean vectors, each as a Python-like list of decimal numbers rounded to six decimal places, in the order A, B, C. This design ensures coverage of a general case, a boundary localization case, and an edge case with small ensemble size and stronger inflation, thereby testing different facets of the localized square-root ETKF implementation.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef wendland_phi(r: float) - float:\n    \"\"\"\n    Compactly supported Wendland C^2 function in 1D:\n    phi(r) = (1 - r)^4 (1 + 4 r) for r in [0,1], and 0 otherwise.\n    \"\"\"\n    if r = 0.0:\n        return 1.0\n    if r = 1.0:\n        return 0.0\n    t = 1.0 - r\n    return (t**4) * (1.0 + 4.0 * r)\n\ndef build_localization_matrix(n: int, L: float) - np.ndarray:\n    \"\"\"\n    Build the localization matrix C for a 1D chain of length n\n    using the Wendland function with radius L.\n    \"\"\"\n    C = np.zeros((n, n), dtype=float)\n    for i in range(n):\n        for j in range(n):\n            d = abs(i - j)\n            r = d / L if L  0 else np.inf\n            C[i, j] = wendland_phi(r)\n    return C\n\ndef etkf_local_analysis_mean(Xf: np.ndarray, y: np.ndarray, sigma2: float, C: np.ndarray,\n                             inflation: float, eps_loc: float = 1e-8) - np.ndarray:\n    \"\"\"\n    Perform LETKF local analysis at each grid point for mean using square-root ETKF formulas.\n    - Xf: forecast ensemble (n x Ne)\n    - y: observations (n,)\n    - sigma2: observation error variance\n    - C: localization matrix (n x n)\n    - inflation: multiplicative inflation factor lambda\n    - eps_loc: threshold for localization weight inclusion\n    Returns the analysis mean vector (n,).\n    \"\"\"\n    n, Ne = Xf.shape\n    # Forecast mean and anomalies\n    xbar = np.mean(Xf, axis=1)  # (n,)\n    Af = Xf - xbar[:, None]     # (n, Ne)\n    # Apply multiplicative covariance inflation\n    Af *= inflation\n    # Normalized anomalies\n    A = Af / np.sqrt(max(Ne - 1, 1))\n    # Innovations\n    d = y - xbar  # (n,)\n    xa = np.copy(xbar)\n\n    # Precompute identity in ensemble space\n    I_ens = np.eye(Ne)\n\n    for i in range(n):\n        # Localized observation indices\n        loc_weights = C[i, :]\n        J = np.where(loc_weights  eps_loc)[0]\n        if J.size == 0:\n            # No local observations; analysis equals forecast mean\n            xa[i] = xbar[i]\n            continue\n\n        # Local inverse R: diag(C[i,j]^2 / sigma2)\n        r_inv = (loc_weights[J] ** 2) / sigma2  # (m_i,)\n        # Local Y = H_J * A; since H = I, pick rows J\n        Y = A[J, :]  # (m_i, Ne)\n        # Compute M = Y^T R_inv Y, using r_inv diagonal efficiently\n        # Scale rows of Y by r_inv, then Y^T @ scaled_Y\n        Y_scaled = Y * r_inv[:, None]  # (m_i, Ne)\n        M = Y.T @ Y_scaled             # (Ne, Ne)\n        # Compute v = Y^T R_inv d_J\n        d_loc = d[J]                   # (m_i,)\n        v = Y.T @ (r_inv * d_loc)      # (Ne,)\n\n        # Solve for w: (I + M) w = v\n        B = I_ens + M                  # (Ne, Ne)\n        # Use symmetric solver via eigendecomposition for stability\n        # w = B^{-1} v\n        evals, evecs = np.linalg.eigh(B)\n        # Inverse via spectral decomposition\n        inv_evals = 1.0 / evals\n        # Compute w = Q (Lambda^{-1} (Q^T v))\n        tmp = evecs.T @ v\n        w = evecs @ (inv_evals * tmp)\n\n        # Update local mean component\n        xa[i] = xbar[i] + A[i, :] @ w\n\n        # Optional: compute local transform T_i = B^{-1/2}, not used further for mean\n        # We implement to adhere to square-root formulation.\n        # inv_sqrt_evals = 1.0 / np.sqrt(evals)\n        # T_i = Q diag(inv_sqrt_evals) Q^T\n        # If needed for anomalies, one would use A[i,:] @ T_i\n\n    return xa\n\ndef generate_case(n: int, Ne: int, sigma2: float, L: float, seed: int):\n    \"\"\"\n    Generate truth, prior ensemble, observations, and localization matrix for a case.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    k = np.arange(n, dtype=float)\n    x_true = np.sin(2.0 * np.pi * k / n) + 0.5 * np.cos(4.0 * np.pi * k / n)\n    # Prior ensemble: truth plus Gaussian noise N(0, 0.5^2)\n    Xf = x_true[:, None] + rng.normal(loc=0.0, scale=0.5, size=(n, Ne))\n    # Observations: truth plus noise N(0, sigma2)\n    y = x_true + rng.normal(loc=0.0, scale=np.sqrt(sigma2), size=n)\n    # Localization matrix\n    C = build_localization_matrix(n, L)\n    return Xf, y, C, x_true\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: n=8, Ne=10, lambda=1.05, sigma2=0.01, L=2.0, seed=1\n        (8, 10, 0.01, 2.0, 1, 1.05),\n        # Case B: n=8, Ne=10, lambda=1.00, sigma2=0.01, L=0.5, seed=2\n        (8, 10, 0.01, 0.5, 2, 1.00),\n        # Case C: n=12, Ne=6, lambda=1.20, sigma2=0.0625, L=3.0, seed=3\n        (12, 6, 0.0625, 3.0, 3, 1.20),\n    ]\n\n    results_str = []\n    for n, Ne, sigma2, L, seed, lam in test_cases:\n        # Generate case data\n        Xf, y, C, _ = generate_case(n, Ne, sigma2, L, seed)\n        # Perform local ETKF analysis mean computation\n        xa = etkf_local_analysis_mean(Xf, y, sigma2, C, inflation=lam, eps_loc=1e-8)\n        # Format result with six decimal places\n        formatted = \"[\" + \",\".join(f\"{val:.6f}\" for val in xa.tolist()) + \"]\"\n        results_str.append(formatted)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```"
        }
    ]
}