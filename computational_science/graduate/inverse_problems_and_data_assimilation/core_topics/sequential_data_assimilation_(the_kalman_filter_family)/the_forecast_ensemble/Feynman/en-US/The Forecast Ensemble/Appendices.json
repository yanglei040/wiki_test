{
    "hands_on_practices": [
        {
            "introduction": "This first practice grounds the theory of the Ensemble Kalman Filter in a concrete, step-by-step calculation. By manually performing a single analysis update for a small system, you will demystify the core mechanism of how observations are assimilated to correct each member of the forecast ensemble. This exercise is essential for building a tangible understanding of how the Kalman gain, sample covariances, and observation perturbations come together in the update equations .",
            "id": "3425298",
            "problem": "Consider a linear data assimilation setting in which the state vector is $x \\in \\mathbb{R}^{2}$ and the observation model is $y = \\mathbf{H} x + v$, where $v$ is Gaussian observation noise with distribution $v \\sim \\mathcal{N}(0,\\mathbf{R})$. You are given a forecast ensemble with $N=4$ members, the observation operator $\\mathbf{H}$, the observation noise covariance $\\mathbf{R}$, and a realized observation $y$. Perform one analysis step of the stochastic Ensemble Kalman Filter (EnKF), using fixed synthetic observation perturbations for reproducibility, and then compute the first component of the analysis ensemble mean.\n\nThe provided quantities are:\n- Observation operator $\\mathbf{H} = \\begin{pmatrix} 1 & 2 \\end{pmatrix}$.\n- Observation noise covariance $\\mathbf{R} = \\begin{pmatrix} \\frac{1}{25} \\end{pmatrix}$.\n- Realized observation $y = 1$.\n- Forecast ensemble members (written as column vectors):\n$$\nx_{1}^{f} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad\nx_{2}^{f} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad\nx_{3}^{f} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}, \\quad\nx_{4}^{f} = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}.\n$$\n- Synthetic observation perturbations (fixed draws from $\\mathcal{N}(0,\\mathbf{R})$ to be used in the stochastic EnKF update):\n$$\ne_{1} = \\frac{1}{10}, \\quad e_{2} = -\\frac{1}{25}, \\quad e_{3} = 0, \\quad e_{4} = -\\frac{3}{25}.\n$$\n\nFollow the standard stochastic Ensemble Kalman Filter (EnKF) analysis step in the linear-Gaussian case: use the unbiased sample covariance estimator (with denominator $N-1$) to form the forecast sample covariance in state space, construct the corresponding covariance in observation space, compute the gain, and then update each ensemble member using synthetic observation perturbations. After performing these steps, compute the analysis ensemble mean vector. Your task is to provide the first component of the analysis ensemble mean as an exact reduced fraction. No rounding is required, and no units are involved.",
            "solution": "The problem is valid as it presents a well-posed, scientifically grounded scenario in the field of data assimilation, providing all necessary data for a unique solution. We will proceed to solve for the first component of the analysis ensemble mean.\n\nThe analysis step for the stochastic Ensemble Kalman Filter (EnKF) updates each forecast ensemble member $x_j^f$ to an analysis member $x_j^a$ according to the formula:\n$$x_j^a = x_j^f + \\mathbf{K} (y_j - \\mathbf{H} x_j^f)$$\nwhere $y_j = y + e_j$ is the perturbed observation for the $j$-th member, with $e_j$ being a random draw from the observation error distribution $\\mathcal{N}(0,\\mathbf{R})$. For this problem, the perturbations $e_j$ are provided as fixed synthetic values. The Kalman gain $\\mathbf{K}$ is given by:\n$$\\mathbf{K} = \\mathbf{P}^f \\mathbf{H}^T (\\mathbf{H} \\mathbf{P}^f \\mathbf{H}^T + \\mathbf{R})^{-1}$$\nHere, $\\mathbf{P}^f$ is the sample covariance of the forecast ensemble.\n\nThe primary goal is to find the analysis ensemble mean, $\\bar{x}^a = \\frac{1}{N} \\sum_{j=1}^{N} x_j^a$. We can derive a direct update formula for the mean:\n$$ \\bar{x}^a = \\frac{1}{N} \\sum_{j=1}^{N} \\left( x_j^f + \\mathbf{K} (y + e_j - \\mathbf{H} x_j^f) \\right) $$\n$$ \\bar{x}^a = \\left( \\frac{1}{N} \\sum_{j=1}^{N} x_j^f \\right) + \\mathbf{K} \\left( \\frac{1}{N} \\sum_{j=1}^{N} (y + e_j - \\mathbf{H} x_j^f) \\right) $$\n$$ \\bar{x}^a = \\bar{x}^f + \\mathbf{K} \\left( y + \\left(\\frac{1}{N} \\sum_{j=1}^{N} e_j\\right) - \\mathbf{H} \\left(\\frac{1}{N} \\sum_{j=1}^{N} x_j^f\\right) \\right) $$\n$$ \\bar{x}^a = \\bar{x}^f + \\mathbf{K} (y + \\bar{e} - \\mathbf{H} \\bar{x}^f) $$\nwhere $\\bar{e}$ is the mean of the provided synthetic perturbations. We will compute the terms in this equation systematically.\n\nStep 1: Compute the forecast ensemble mean $\\bar{x}^f$.\nThe forecast ensemble has $N=4$ members:\n$$ x_{1}^{f} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad x_{2}^{f} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad x_{3}^{f} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix}, \\quad x_{4}^{f} = \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix} $$\nThe mean is:\n$$ \\bar{x}^f = \\frac{1}{4} \\left( \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} + \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix} \\right) = \\frac{1}{4} \\begin{pmatrix} 1+0+2-1 \\\\ 0+1-1+2 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} $$\n\nStep 2: Compute the sample forecast covariance $\\mathbf{P}^f$.\nThe problem specifies using the unbiased estimator with denominator $N-1 = 3$.\n$$ \\mathbf{P}^f = \\frac{1}{3} \\sum_{j=1}^{4} (x_j^f - \\bar{x}^f)(x_j^f - \\bar{x}^f)^T $$\nThe ensemble perturbations are:\n$$ x_1' = \\begin{pmatrix} 1 - \\frac{1}{2} \\\\ 0 - \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\end{pmatrix}, \\quad x_2' = \\begin{pmatrix} 0 - \\frac{1}{2} \\\\ 1 - \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} $$\n$$ x_3' = \\begin{pmatrix} 2 - \\frac{1}{2} \\\\ -1 - \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{3}{2} \\end{pmatrix}, \\quad x_4' = \\begin{pmatrix} -1 - \\frac{1}{2} \\\\ 2 - \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} -\\frac{3}{2} \\\\ \\frac{3}{2} \\end{pmatrix} $$\nThe sum of outer products is:\n$$ \\sum_{j=1}^{4} x_j' (x_j')^T = \\begin{pmatrix} \\frac{1}{4} & -\\frac{1}{4} \\\\ -\\frac{1}{4} & \\frac{1}{4} \\end{pmatrix} + \\begin{pmatrix} \\frac{1}{4} & -\\frac{1}{4} \\\\ -\\frac{1}{4} & \\frac{1}{4} \\end{pmatrix} + \\begin{pmatrix} \\frac{9}{4} & -\\frac{9}{4} \\\\ -\\frac{9}{4} & \\frac{9}{4} \\end{pmatrix} + \\begin{pmatrix} \\frac{9}{4} & -\\frac{9}{4} \\\\ -\\frac{9}{4} & \\frac{9}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{20}{4} & -\\frac{20}{4} \\\\ -\\frac{20}{4} & \\frac{20}{4} \\end{pmatrix} = \\begin{pmatrix} 5 & -5 \\\\ -5 & 5 \\end{pmatrix} $$\nThus, the sample covariance is:\n$$ \\mathbf{P}^f = \\frac{1}{3} \\begin{pmatrix} 5 & -5 \\\\ -5 & 5 \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{3} & -\\frac{5}{3} \\\\ -\\frac{5}{3} & \\frac{5}{3} \\end{pmatrix} $$\n\nStep 3: Compute the Kalman gain $\\mathbf{K}$.\nFirst, we compute $\\mathbf{P}^f \\mathbf{H}^T$:\n$$ \\mathbf{P}^f \\mathbf{H}^T = \\begin{pmatrix} \\frac{5}{3} & -\\frac{5}{3} \\\\ -\\frac{5}{3} & \\frac{5}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{3} - \\frac{10}{3} \\\\ -\\frac{5}{3} + \\frac{10}{3} \\end{pmatrix} = \\begin{pmatrix} -\\frac{5}{3} \\\\ \\frac{5}{3} \\end{pmatrix} $$\nNext, we compute the term $\\mathbf{H} \\mathbf{P}^f \\mathbf{H}^T$:\n$$ \\mathbf{H} \\mathbf{P}^f \\mathbf{H}^T = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} -\\frac{5}{3} \\\\ \\frac{5}{3} \\end{pmatrix} = -\\frac{5}{3} + \\frac{10}{3} = \\frac{5}{3} $$\nThe observation noise covariance is $\\mathbf{R} = \\frac{1}{25}$. The denominator of the gain expression is:\n$$ \\mathbf{H} \\mathbf{P}^f \\mathbf{H}^T + \\mathbf{R} = \\frac{5}{3} + \\frac{1}{25} = \\frac{125}{75} + \\frac{3}{75} = \\frac{128}{75} $$\nThe inverse is $(\\frac{128}{75})^{-1} = \\frac{75}{128}$.\nNow, we compute the Kalman gain $\\mathbf{K}$:\n$$ \\mathbf{K} = (\\mathbf{P}^f \\mathbf{H}^T) (\\mathbf{H} \\mathbf{P}^f \\mathbf{H}^T + \\mathbf{R})^{-1} = \\begin{pmatrix} -\\frac{5}{3} \\\\ \\frac{5}{3} \\end{pmatrix} \\left(\\frac{75}{128}\\right) = \\begin{pmatrix} -\\frac{5 \\cdot 75}{3 \\cdot 128} \\\\ \\frac{5 \\cdot 75}{3 \\cdot 128} \\end{pmatrix} = \\begin{pmatrix} -\\frac{5 \\cdot 25}{128} \\\\ \\frac{5 \\cdot 25}{128} \\end{pmatrix} = \\begin{pmatrix} -\\frac{125}{128} \\\\ \\frac{125}{128} \\end{pmatrix} $$\n\nStep 4: Compute the terms for the mean update formula.\nThe observation is $y = 1$.\nThe mean of the synthetic perturbations is:\n$$ \\bar{e} = \\frac{1}{4} \\left( \\frac{1}{10} - \\frac{1}{25} + 0 - \\frac{3}{25} \\right) = \\frac{1}{4} \\left( \\frac{1}{10} - \\frac{4}{25} \\right) = \\frac{1}{4} \\left( \\frac{5}{50} - \\frac{8}{50} \\right) = \\frac{1}{4} \\left( -\\frac{3}{50} \\right) = -\\frac{3}{200} $$\nThe mean forecast observation is:\n$$ \\mathbf{H} \\bar{x}^f = \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} = \\frac{1}{2} + \\frac{2}{2} = \\frac{3}{2} $$\nThe average innovation term is:\n$$ y + \\bar{e} - \\mathbf{H} \\bar{x}^f = 1 - \\frac{3}{200} - \\frac{3}{2} = \\frac{200}{200} - \\frac{3}{200} - \\frac{300}{200} = -\\frac{103}{200} $$\n\nStep 5: Compute the analysis ensemble mean $\\bar{x}^a$ and its first component.\n$$ \\bar{x}^a = \\bar{x}^f + \\mathbf{K} (y + \\bar{e} - \\mathbf{H} \\bar{x}^f) = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{1}{2} \\end{pmatrix} + \\begin{pmatrix} -\\frac{125}{128} \\\\ \\frac{125}{128} \\end{pmatrix} \\left( -\\frac{103}{200} \\right) $$\nWe are interested in the first component of $\\bar{x}^a$, denoted $(\\bar{x}^a)_1$:\n$$ (\\bar{x}^a)_1 = \\frac{1}{2} + \\left(-\\frac{125}{128}\\right) \\left(-\\frac{103}{200}\\right) = \\frac{1}{2} + \\frac{125 \\cdot 103}{128 \\cdot 200} $$\nWe simplify the fraction: $\\frac{125}{200} = \\frac{5 \\cdot 25}{8 \\cdot 25} = \\frac{5}{8}$.\n$$ (\\bar{x}^a)_1 = \\frac{1}{2} + \\frac{5 \\cdot 103}{128 \\cdot 8} = \\frac{1}{2} + \\frac{515}{1024} $$\nTo add these fractions, we find a common denominator:\n$$ (\\bar{x}^a)_1 = \\frac{512}{1024} + \\frac{515}{1024} = \\frac{1027}{1024} $$\nThe numerator is $1027 = 13 \\times 79$ and the denominator is $1024 = 2^{10}$. The fraction is irreducible.\nThe first component of the analysis ensemble mean is $\\frac{1027}{1024}$.",
            "answer": "$$\\boxed{\\frac{1027}{1024}}$$"
        },
        {
            "introduction": "A key challenge in practical ensemble data assimilation is the tendency for the ensemble to underestimate the true forecast uncertainty, a problem known as filter divergence or ensemble collapse. This practice explores the mathematical basis of a widely used remedy: multiplicative covariance inflation. By deriving how a simple scaling of ensemble anomalies affects the mean and covariance, you will understand why this technique effectively increases ensemble spread without biasing the central estimate of the state .",
            "id": "3425670",
            "problem": "Consider a forecast ensemble in the Ensemble Kalman Filter (EnKF), with state dimension $n$ and ensemble size $N \\ge 2$. Let the ensemble matrix be $X \\in \\mathbb{R}^{n \\times N}$, the ensemble mean be $x^{\\mathrm{f}} = \\frac{1}{N} X \\mathbf{1}$ where $\\mathbf{1} \\in \\mathbb{R}^{N}$ is the vector of ones, and the anomalies be $A = X - x^{\\mathrm{f}} \\mathbf{1}^{\\top} \\in \\mathbb{R}^{n \\times N}$. The forecast sample covariance is defined by $P = \\frac{1}{N-1} A A^{\\top} \\in \\mathbb{R}^{n \\times n}$.\n\nA multiplicative inflation is applied to the anomalies via a scalar factor $\\lambda > 1$, producing inflated anomalies $A' = \\lambda A$ and inflated ensemble $X' = x^{\\mathrm{f}} \\mathbf{1}^{\\top} + A'$. Starting only from these definitions, determine whether the ensemble mean is preserved under this inflation and derive the transformed forecast sample covariance $P'$, expressed in terms of $\\lambda$ and $P$.\n\nProvide the transformed forecast sample covariance $P'$ as your final answer in a single closed-form analytic expression. No numerical rounding is required, and no units are needed.",
            "solution": "The problem statement has been critically validated and is deemed to be self-contained, scientifically grounded in the field of data assimilation, and mathematically well-posed. All definitions are standard in the context of the Ensemble Kalman Filter (EnKF). The problem is valid, and a solution will be derived.\n\nThe task is twofold: first, to determine if the ensemble mean is invariant under the specified multiplicative inflation, and second, to derive the expression for the new forecast sample covariance matrix $P'$ in terms of the inflation factor $\\lambda$ and the original covariance matrix $P$.\n\nLet us begin by addressing the first part: the preservation of the ensemble mean. The original ensemble mean is given as $x^{\\mathrm{f}} = \\frac{1}{N} X \\mathbf{1}$. The inflated ensemble is defined as $X' = x^{\\mathrm{f}} \\mathbf{1}^{\\top} + A'$, where $A' = \\lambda A$. The new ensemble mean, which we denote as $x'^{\\mathrm{f}}$, is computed by applying the definition of the mean to the new ensemble matrix $X'$:\n$$\nx'^{\\mathrm{f}} = \\frac{1}{N} X' \\mathbf{1}\n$$\nSubstituting the expression for $X'$, we have:\n$$\nx'^{\\mathrm{f}} = \\frac{1}{N} \\left( x^{\\mathrm{f}} \\mathbf{1}^{\\top} + A' \\right) \\mathbf{1}\n$$\nBy distributing the vector $\\mathbf{1}$ on the right, we get:\n$$\nx'^{\\mathrm{f}} = \\frac{1}{N} \\left( x^{\\mathrm{f}} \\mathbf{1}^{\\top} \\mathbf{1} + A' \\mathbf{1} \\right)\n$$\nThe product $\\mathbf{1}^{\\top} \\mathbf{1}$ is the dot product of the vector of ones with itself, which is the sum of its $N$ components, each equal to $1$. Thus, $\\mathbf{1}^{\\top} \\mathbf{1} = N$. The first term becomes $x^{\\mathrm{f}} N$.\n\nFor the second term, $A' \\mathbf{1}$, we first substitute $A' = \\lambda A$:\n$$\nA' \\mathbf{1} = (\\lambda A) \\mathbf{1} = \\lambda (A \\mathbf{1})\n$$\nNow we must evaluate the term $A \\mathbf{1}$. Using the definition of the anomaly matrix, $A = X - x^{\\mathrm{f}} \\mathbf{1}^{\\top}$:\n$$\nA \\mathbf{1} = \\left( X - x^{\\mathrm{f}} \\mathbf{1}^{\\top} \\right) \\mathbf{1} = X \\mathbf{1} - x^{\\mathrm{f}} \\mathbf{1}^{\\top} \\mathbf{1}\n$$\nFrom the definition of the ensemble mean, $x^{\\mathrm{f}} = \\frac{1}{N} X \\mathbf{1}$, we can write $X \\mathbf{1} = N x^{\\mathrm{f}}$. Substituting this and $\\mathbf{1}^{\\top} \\mathbf{1} = N$ into the expression for $A \\mathbf{1}$, we find:\n$$\nA \\mathbf{1} = N x^{\\mathrm{f}} - x^{\\mathrm{f}} N = \\mathbf{0}\n$$\nThis demonstrates a fundamental property: the sum of the anomaly vectors (the columns of $A$) is the zero vector. Consequently, $A' \\mathbf{1} = \\lambda (A \\mathbf{1}) = \\lambda \\mathbf{0} = \\mathbf{0}$.\n\nSubstituting these results back into the expression for the new mean $x'^{\\mathrm{f}}$:\n$$\nx'^{\\mathrm{f}} = \\frac{1}{N} \\left( x^{\\mathrm{f}} N + \\mathbf{0} \\right) = \\frac{1}{N} (N x^{\\mathrm{f}}) = x^{\\mathrm{f}}\n$$\nThis proves that the ensemble mean is preserved under this form of multiplicative inflation, i.e., $x'^{\\mathrm{f}} = x^{\\mathrm{f}}$. This is a critical feature of the inflation technique, ensuring that the central estimate of the state is not shifted, while the spread is increased.\n\nNext, we address the second part: the derivation of the transformed forecast sample covariance $P'$. The definition of the forecast sample covariance applied to the inflated ensemble is:\n$$\nP' = \\frac{1}{N-1} A' (A')^{\\top}\n$$\nWe are given that the inflated anomalies are $A' = \\lambda A$. Substituting this into the definition of $P'$:\n$$\nP' = \\frac{1}{N-1} (\\lambda A) (\\lambda A)^{\\top}\n$$\nUsing the property of the transpose of a product of a scalar and a matrix, $(\\lambda A)^{\\top} = \\lambda A^{\\top}$, we can rewrite the expression as:\n$$\nP' = \\frac{1}{N-1} (\\lambda A) (\\lambda A^{\\top})\n$$\nSince $\\lambda$ is a scalar, we can commute the factors:\n$$\nP' = \\frac{1}{N-1} \\lambda^2 (A A^{\\top})\n$$\nFactoring the scalar term $\\lambda^2$ out of the expression:\n$$\nP' = \\lambda^2 \\left( \\frac{1}{N-1} A A^{\\top} \\right)\n$$\nWe recognize the term in the parentheses as the definition of the original forecast sample covariance, $P = \\frac{1}{N-1} A A^{\\top}$. Therefore, the relationship between the inflated and original covariance matrices is:\n$$\nP' = \\lambda^2 P\n$$\nThis result shows that applying a multiplicative inflation factor $\\lambda$ to the ensemble anomalies results in the forecast error covariance matrix being scaled by a factor of $\\lambda^2$. This is because covariance is a second-order moment, and thus scales with the square of the scaling factor applied to the underlying variables (the anomalies).",
            "answer": "$$\n\\boxed{\\lambda^{2} P}\n$$"
        },
        {
            "introduction": "Moving beyond a single time step, this practice explores the long-term evolution of the forecast ensemble, a process known as \"spin-up\". When an assimilation system is started, the structure of the forecast error covariance, $P_f$, gradually adapts to reflect the underlying dynamics of the model. This simulation-based exercise allows you to quantify how $P_f$ aligns with the system's unstable subspace over successive forecast-analysis cycles, providing insight into the interplay between model dynamics, the observing network, and ensemble statistics .",
            "id": "3425714",
            "problem": "Consider a discrete-time, linear, Gaussian data assimilation cycle for a dynamical system with state vector of dimension $n$, governed by the tangent linear model of the forecast operator. The model state advances as $x_{k+1} = M x_k + w_k$, where $x_k \\in \\mathbb{R}^n$ is the state at cycle $k$, $M \\in \\mathbb{R}^{n \\times n}$ is the tangent linear model (the Jacobian of the nonlinear model $M'(x)$ evaluated along a trajectory, assumed constant for this problem), and $w_k \\sim \\mathcal{N}(0,Q)$ is a Gaussian process noise with covariance $Q \\in \\mathbb{R}^{n \\times n}$. Observations are given by $y_k = H x_k + v_k$, with observation operator $H \\in \\mathbb{R}^{m \\times n}$ and $v_k \\sim \\mathcal{N}(0,R)$ Gaussian observation noise with covariance $R \\in \\mathbb{R}^{m \\times m}$. The forecast error covariance is $P_f \\in \\mathbb{R}^{n \\times n}$ and the analysis error covariance is $P_a \\in \\mathbb{R}^{n \\times n}$.\n\nThe forecast ensemble spin-up from a poor initial prior is characterized by the alignment of $P_f$ with the dynamically unstable subspace of $M$. Let the spectrum of $M$ be real and positive and suppose $M$ is orthogonally diagonalizable as $M = V \\Lambda V^\\top$, where $V \\in \\mathbb{R}^{n \\times n}$ is orthogonal (i.e., $V^\\top V = I$) and $\\Lambda = \\operatorname{diag}(\\lambda_1,\\dots,\\lambda_n)$ with $\\lambda_i > 0$. Define the unstable subspace $\\mathcal{U}$ as the span of eigenvectors of $M$ corresponding to eigenvalues with $\\lambda_i > 1$. Let $r$ be the dimension of $\\mathcal{U}$ (i.e., the number of $\\lambda_i$ with $\\lambda_i > 1$). The alignment of $P_f$ with $\\mathcal{U}$ is quantified via principal angles between subspaces: let $U \\in \\mathbb{R}^{n \\times r}$ be an orthonormal basis of $\\mathcal{U}$ and let $V_r \\in \\mathbb{R}^{n \\times r}$ be the matrix of the top $r$ orthonormal eigenvectors of $P_f$ corresponding to its largest $r$ eigenvalues. The cosines of the principal angles between the subspaces spanned by columns of $U$ and $V_r$ are the singular values of $U^\\top V_r$. Define an alignment threshold $\\tau \\in (0,1)$ and consider $P_f$ aligned with $\\mathcal{U}$ when the smallest singular value of $U^\\top V_r$ is greater than or equal to $\\tau$.\n\nStarting from an initial poor prior covariance $P_f^{(0)} = \\sigma_0^2 I$ with $\\sigma_0^2 > 0$, and given $Q$, $R$, and $H$, the data assimilation cycle proceeds by alternating forecast and analysis steps. The number of cycles needed for $P_f$ to first achieve alignment with $\\mathcal{U}$ (as defined above) depends on the spectrum of $M$ and the geometry of the observing network $H$. Your task is to compute, for a set of specified test cases, the first cycle index $k$ at which the alignment criterion is met, using the standard linear Gaussian Kalman Filter (KF) covariance updates for the forecast and analysis steps. If alignment is not achieved within a specified maximum number of cycles $k_{\\max}$, report $-1$ for that test case.\n\nFor each test case, the system is specified by $n$, the eigenvalues $\\lambda_i$, an orthogonal eigenbasis $V$ constructed deterministically, the observation operator $H$ represented as a selection of state coordinates (rows of the identity), the covariances $Q$ and $R$ taken as scaled identity matrices, and the initial prior variance $\\sigma_0^2$. The forecast and analysis covariances should be updated at each cycle according to the standard Kalman Filter covariance propagation under linear Gaussian assumptions. Use the Joseph-stabilized analysis covariance update. At each cycle, evaluate the alignment criterion using principal angles as defined above, and record the first cycle $k$ where the smallest singular value of $U^\\top V_r$ is at least $\\tau$.\n\nThe angle unit does not apply because the alignment metric is defined via cosines. All outputs are dimensionless floats or integers. There are no physical units in this problem.\n\nImplement this for the following test suite. In all cases, define the orthogonal matrix $V$ deterministically via the $\\operatorname{QR}$ factorization of a fixed random matrix seeded by an integer seed; that is, generate a matrix with independent standard normal entries using a fixed seed, compute its $\\operatorname{QR}$ factorization, and take the $Q$ factor as the orthogonal $V$.\n\nTest suite:\n- Case $1$ (happy path):\n  - $n = 3$, eigenvalues $\\lambda = [1.3, 1.05, 0.8]$, seed for $V$: $123$.\n  - $H$ observes the third coordinate only (i.e., $H$ is the row vector selecting index $3$).\n  - $Q = 10^{-3} I$, $R = 10^{-2} I$, $\\sigma_0^2 = 25$.\n  - Alignment threshold $\\tau = 0.95$, maximum cycles $k_{\\max} = 200$.\n- Case $2$ (partial observation of unstable and stable directions):\n  - $n = 4$, eigenvalues $\\lambda = [1.4, 1.2, 0.9, 0.8]$, seed for $V$: $456$.\n  - $H$ observes coordinates $1$ and $3$ (i.e., $H$ selects indices $1$ and $3$).\n  - $Q = 5 \\times 10^{-4} I$, $R = 2 \\times 10^{-2} I$, $\\sigma_0^2 = 25$.\n  - Alignment threshold $\\tau = 0.95$, maximum cycles $k_{\\max} = 300$.\n- Case $3$ (full observation, potential non-alignment):\n  - $n = 3$, eigenvalues $\\lambda = [1.5, 0.95, 0.7]$, seed for $V$: $789$.\n  - $H = I$ (i.e., all $3$ coordinates observed).\n  - $Q = 10^{-3} I$, $R = 10^{-2} I$, $\\sigma_0^2 = 25$.\n  - Alignment threshold $\\tau = 0.95$, maximum cycles $k_{\\max} = 150$.\n- Case $4$ (weak instability, slow spin-up, edge case):\n  - $n = 5$, eigenvalues $\\lambda = [1.01, 1.0, 0.99, 0.95, 0.9]$, seed for $V$: $101$.\n  - $H$ observes coordinates $2$, $4$, and $5$ (i.e., $H$ selects indices $2$, $4$, and $5$).\n  - $Q = 5 \\times 10^{-4} I$, $R = 10^{-2} I$, $\\sigma_0^2 = 100$.\n  - Alignment threshold $\\tau = 0.95$, maximum cycles $k_{\\max} = 500$.\n\nImplementation requirements:\n- Use the standard Kalman Filter covariance updates for the forecast and analysis steps under linear Gaussian assumptions. For analysis, use the Joseph stabilized form to ensure numerical symmetry and positive semi-definiteness.\n- At each cycle $k$, compute the forecast covariance $P_f^{(k)}$ from the previous analysis covariance $P_a^{(k-1)}$ and then evaluate the alignment metric using principal angles between the unstable subspace of $M$ and the dominant eigen-subspace of $P_f^{(k)}$ of matching dimension $r$.\n- Return the first cycle index $k$ for which the alignment criterion is satisfied, or $-1$ if no alignment occurs within $k_{\\max}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$), where each $result_i$ is the first cycle index $k$ meeting the alignment criterion for the corresponding test case, or $-1$ if not met.",
            "solution": "The problem requires us to determine the number of data assimilation cycles needed for the forecast error covariance, $P_f$, to align with the unstable subspace of the system's dynamics. This involves simulating the evolution of covariance matrices through a linear Kalman Filter cycle and applying a specific geometric alignment criterion at each step.\n\nThe overall approach is structured as follows:\n1.  Define the system model based on the provided parameters for each test case. This includes constructing the tangent linear model matrix $M$, the observation operator $H$, and noise covariance matrices $Q$ and $R$.\n2.  Identify the unstable subspace $\\mathcal{U}$ of the model $M$ and construct its orthonormal basis $U$.\n3.  Initialize the analysis error covariance $P_a$ based on the given poor prior. The problem states we start with an initial prior forecast covariance $P_f^{(0)} = \\sigma_0^2 I$. A standard data assimilation cycle consists of a forecast step followed by an analysis step. We interpret this by setting the initial analysis covariance, at cycle $k=0$, to be $P_a^{(0)} = \\sigma_0^2 I$.\n4.  Iterate through the data assimilation cycles for $k=1, 2, \\dots, k_{\\max}$.\n5.  In each cycle $k$, first perform the forecast step to compute the forecast error covariance $P_f^{(k)}$.\n6.  Then, evaluate the alignment criterion between the dominant subspace of $P_f^{(k)}$ and the unstable subspace $\\mathcal{U}$. If alignment is achieved, the cycle number $k$ is the result.\n7.  If alignment is not achieved, perform the analysis step to compute the analysis error covariance $P_a^{(k)}$, which will be used in the next cycle's forecast step.\n8.  If the loop completes without achieving alignment, the result is $-1$.\n\nThe core scientific principles are rooted in Kalman filtering and linear algebra.\n\n**1. Kalman Filter Covariance Propagation**\n\nThe evolution of the error covariance is governed by the Kalman Filter equations under linear Gaussian assumptions. A cycle consists of two steps:\n\n**Forecast Step:** The analysis error covariance from the previous cycle, $P_a^{(k-1)}$, is propagated forward in time using the tangent linear model $M$. The uncertainty grows due to the system dynamics and is augmented by the process noise $w_k \\sim \\mathcal{N}(0, Q)$. The forecast error covariance for the current cycle $k$ is given by:\n$$P_f^{(k)} = M P_a^{(k-1)} M^\\top + Q$$\n\n**Analysis Step:** The forecast state (and its covariance $P_f^{(k)}$) is updated using an observation $y_k = H x_k + v_k$, where $v_k \\sim \\mathcal{N}(0, R)$. This update reduces the uncertainty. The resulting analysis error covariance $P_a^{(k)}$ is calculated using the Joseph-stabilized form to ensure numerical stability and preserve the positive semi-definiteness of the covariance matrix. The equations are:\n- Kalman Gain: $K_k = P_f^{(k)} H^\\top (H P_f^{(k)} H^\\top + R)^{-1}$\n- Analysis Covariance: $P_a^{(k)} = (I - K_k H) P_f^{(k)} (I - K_k H)^\\top + K_k R K_k^\\top$\n\n**2. Subspace Alignment Criterion**\n\nThe spin-up process involves the forecast error covariance $P_f$ becoming dominated by the fastest-growing error modes, which correspond to the unstable subspace of the dynamics. We quantify this alignment using principal angles between subspaces.\n\n**Unstable Subspace ($\\mathcal{U}$):** The model is given by $M = V \\Lambda V^\\top$, where $\\Lambda = \\operatorname{diag}(\\lambda_1, \\dots, \\lambda_n)$ contains the eigenvalues and the columns of the orthogonal matrix $V$ are the corresponding eigenvectors. The unstable subspace $\\mathcal{U}$ is the span of eigenvectors associated with eigenvalues $\\lambda_i > 1$. Let $r$ be the number of such eigenvalues. An orthonormal basis for $\\mathcal{U}$ is formed by the matrix $U \\in \\mathbb{R}^{n \\times r}$, whose columns are the corresponding $r$ eigenvectors from $V$.\n\n**Dominant Subspace of $P_f$:** The forecast error covariance $P_f^{(k)}$ is a symmetric positive semi-definite matrix. Its eigendecomposition reveals the principal directions of uncertainty. The dominant $r$-dimensional subspace of $P_f^{(k)}$ is spanned by the $r$ eigenvectors corresponding to its $r$ largest eigenvalues. An orthonormal basis for this subspace is the matrix $V_r \\in \\mathbb{R}^{n \\times r}$, whose columns are these $r$ leading eigenvectors.\n\n**Alignment Metric:** The alignment between the two subspaces, spanned by the columns of $U$ and $V_r$ respectively, is measured by their principal angles. The cosines of these angles are the singular values of the matrix $C = U^\\top V_r$. The problem defines alignment as being achieved when the smallest of these cosines (i.e., the smallest singular value of $C$) is greater than or equal to a given threshold $\\tau \\in (0,1)$. The condition is:\n$$ \\min(\\sigma(U^\\top V_r)) \\ge \\tau $$\nwhere $\\sigma(\\cdot)$ denotes the set of singular values.\n\n**3. Algorithmic Implementation**\n\nFor each test case, the algorithm proceeds as follows:\n1.  **Setup**: Construct the matrices $M$, $H$, $Q$, and $R$ from the given parameters ($n$, $\\lambda$, seed, observation indices, noise scales). The orthogonal matrix $V$ is generated deterministically from the given seed by applying QR decomposition to a standard normal random matrix. The basis $U$ for the unstable subspace is extracted from $V$.\n2.  **Initialization**: Set the initial analysis covariance $P_a^{(0)} = \\sigma_0^2 I$.\n3.  **Iteration**: For $k$ from $1$ to $k_{\\max}$:\n    a. Compute $P_f^{(k)} = M P_a^{(k-1)} M^\\top + Q$.\n    b. Compute the eigendecomposition of $P_f^{(k)}$ to find its dominant $r$-dimensional subspace basis, $V_r$.\n    c. Compute the smallest singular value of $U^\\top V_r$. If it is $\\ge \\tau$, return $k$.\n    d. If not aligned, compute the Kalman gain $K_k$ and the next analysis covariance $P_a^{(k)}$ using the Joseph form.\n4.  **Termination**: If the loop finishes, return $-1$.\nThis procedure is implemented for each test case to find the first cycle index of alignment.",
            "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef run_simulation(n, lambdas, seed, H_indices, Q_scale, R_scale, sigma0_sq, tau, k_max):\n    \"\"\"\n    Simulates a Kalman Filter covariance cycle to find the alignment time.\n\n    Args:\n        n (int): State dimension.\n        lambdas (list): Eigenvalues of the model M.\n        seed (int): Seed for generating the orthogonal eigenbasis V.\n        H_indices (list or None): Indices of observed state variables. None for H=I.\n        Q_scale (float): Scaling factor for the process noise covariance Q.\n        R_scale (float): Scaling factor for the observation noise covariance R.\n        sigma0_sq (float): Initial variance for the prior covariance.\n        tau (float): Alignment threshold.\n        k_max (int): Maximum number of cycles.\n\n    Returns:\n        int: The first cycle index k where alignment is met, or -1.\n    \"\"\"\n    # 1. Initialization\n    # Deterministically construct the orthogonal matrix V\n    rng = np.random.default_rng(seed)\n    A = rng.standard_normal((n, n))\n    V, _ = scipy.linalg.qr(A)\n\n    # Construct the model matrix M\n    Lambda = np.diag(lambdas)\n    M = V @ Lambda @ V.T\n\n    # Identify the unstable subspace U from the eigenvectors in V\n    unstable_indices = np.where(np.array(lambdas) > 1.0)[0]\n    r = len(unstable_indices)\n    \n    if r == 0:\n        # No unstable subspace, so alignment as defined is not possible.\n        return -1\n\n    U = V[:, unstable_indices]\n\n    # Construct the observation operator H\n    if H_indices is None:  # Case where H is the identity matrix\n        m = n\n        H = np.eye(n)\n    else:\n        m = len(H_indices)\n        H = np.zeros((m, n))\n        for i, h_idx in enumerate(H_indices):\n            H[i, h_idx] = 1.0\n    \n    # Construct noise covariance matrices Q and R\n    Q = Q_scale * np.eye(n)\n    R = R_scale * np.eye(m)\n\n    # Initialize the analysis error covariance Pa\n    # We interpret the \"initial poor prior covariance\" as the analysis\n    # covariance at cycle k=0.\n    Pa = sigma0_sq * np.eye(n)\n\n    # 2. Iteration through cycles\n    for k in range(1, k_max + 1):\n        # Forecast Step: Propagate covariance\n        Pf = M @ Pa @ M.T + Q\n\n        # Alignment Check\n        # Eigendecomposition of the symmetric matrix Pf\n        # scipy.linalg.eigh returns eigenvalues in ascending order.\n        eigvals, eigvecs = scipy.linalg.eigh(Pf)\n        \n        # Vr is the basis for the dominant subspace of Pf, formed by the\n        # eigenvectors corresponding to the r largest eigenvalues.\n        Vr = eigvecs[:, -r:]\n\n        # Compute the smallest singular value of U.T @ Vr\n        C = U.T @ Vr\n        singular_values = scipy.linalg.svdvals(C)\n        s_min = np.min(singular_values) if singular_values.size > 0 else 0.0\n\n        if s_min >= tau:\n            return k\n\n        # Analysis Step: Update covariance using observation\n        # Kalman Gain: K = Pf H.T (H Pf H.T + R)^-1\n        # Use scipy.linalg.solve for stable inversion\n        H_Pf_HT = H @ Pf @ H.T\n        innovation_cov = H_Pf_HT + R\n        K = scipy.linalg.solve(innovation_cov, Pf @ H.T, assume_a='pos').T\n\n        # Joseph stabilized form for the analysis covariance update\n        I_n = np.eye(n)\n        IKH = I_n - K @ H\n        Pa = IKH @ Pf @ IKH.T + K @ R @ K.T\n\n    # 3. Termination\n    # If alignment is not achieved within k_max cycles\n    return -1\n\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1: n=3, lambdas=[1.3, 1.05, 0.8], seed=123, H=[0,0,1], ...\n        {'n': 3, 'lambdas': [1.3, 1.05, 0.8], 'seed': 123, 'H_indices': [2],\n         'Q_scale': 1e-3, 'R_scale': 1e-2, 'sigma0_sq': 25.0, 'tau': 0.95, 'k_max': 200},\n        \n        # Case 2: n=4, lambdas=[1.4, 1.2, 0.9, 0.8], seed=456, H observes 1&3, ...\n        {'n': 4, 'lambdas': [1.4, 1.2, 0.9, 0.8], 'seed': 456, 'H_indices': [0, 2],\n         'Q_scale': 5e-4, 'R_scale': 2e-2, 'sigma0_sq': 25.0, 'tau': 0.95, 'k_max': 300},\n        \n        # Case 3: n=3, lambdas=[1.5, 0.95, 0.7], seed=789, H=I, ...\n        {'n': 3, 'lambdas': [1.5, 0.95, 0.7], 'seed': 789, 'H_indices': None,\n         'Q_scale': 1e-3, 'R_scale': 1e-2, 'sigma0_sq': 25.0, 'tau': 0.95, 'k_max': 150},\n        \n        # Case 4: n=5, lambdas=[1.01, 1.0, 0.99, ...], seed=101, H observes 2,4,5 ...\n        {'n': 5, 'lambdas': [1.01, 1.0, 0.99, 0.95, 0.9], 'seed': 101, 'H_indices': [1, 3, 4],\n         'Q_scale': 5e-4, 'R_scale': 1e-2, 'sigma0_sq': 100.0, 'tau': 0.95, 'k_max': 500}\n    ]\n    \n    results = []\n    for params in test_cases:\n        result = run_simulation(**params)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}