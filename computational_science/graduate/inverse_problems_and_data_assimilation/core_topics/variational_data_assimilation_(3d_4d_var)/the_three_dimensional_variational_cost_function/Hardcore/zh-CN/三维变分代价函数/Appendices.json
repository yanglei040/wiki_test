{
    "hands_on_practices": [
        {
            "introduction": "我们对三维变分（3D-Var）实际应用的探索始于其最基本的形式：线性高斯情况。该练习  将数据同化问题构建为一个加权最小二乘问题，其目标是找到一个最优状态，以平衡来自先验估计（背景场）和新测量（观测）的信息。通过推导解并针对不同情景进行编程实现，您将对背景误差协方差 $B$ 和观测误差协方差 $R$ 如何决定最终分析场建立核心理解。",
            "id": "3152341",
            "problem": "要求您将三维变分（3DVar）资料同化问题表述为一个加权最小二乘问题，并实现一个程序，为一组测试用例计算其唯一的极小化子。目标是将该公式与加权最小二乘严格联系起来，并量化背景场误差协方差和观测误差协方差的相对大小如何影响结果。\n\n从以下基本基础开始：\n- 向量 $v$ 关于对称正定矩阵 $W$ 的加权平方范数定义为 $\\lVert v \\rVert_{W}^{2} = v^{\\top} W v$。\n- 对于将向量 $x$ 映射到观测值 $Hx$ 的线性模型和数据 $y$，加权最小二乘的目标是最小化一个二次泛函，该泛函是一个数据失配项和一个正则化项的和，每一项都有一个对称正定权重。\n\n在 3DVar 中，分析状态 $x$ 被定义为代价函数\n$$\nJ(x) = \\lVert H x - y \\rVert_{R^{-1}}^{2} + \\lVert x - x_b \\rVert_{B^{-1}}^{2}\n$$\n的极小化子，其中 $x_b$ 是背景（先验）状态，$B$ 是背景场误差协方差，$R$ 是观测误差协方差。$B$ 和 $R$ 都是对称正定的。矩阵 $H$ 是一个已知的线性观测算子。\n\n任务：\n1. 使用以上定义和二次型的基本多元微积分，从第一性原理出发，通过将梯度设置为零，推导 $J(x)$ 的极小化子 $x$ 的充要最优性条件。证明得到的线性系统是对称正定的，因此有唯一解。\n2. 基于您的推导，设计一个算法，该算法仅使用对称正定矩阵的线性系统求解和矩阵乘法来计算极小化子。您的算法不能显式地构造任何矩阵的逆。对于任意维度，只要 $B$ 和 $R$ 是对称正定的，且 $H$ 是任何具有兼容维度的实矩阵，您的算法必须有效。\n3. 实现一个完整、可运行的程序，在以下测试套件上评估您的算法。对于每个测试，计算使 $J(x)$ 最小化的分析向量 $x$：\n   - 测试 A（理想路径，权重均衡）：状态维度 $n = 2$，观测维度 $m = 2$，\n     $$\n     H = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad\n     x_b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad\n     y = \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix},\\quad\n     B = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad\n     R = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}.\n     $$\n   - 测试 B（背景场主导：小 $B$，大 $R$）：$n = 2$，$m = 2$，\n     $$\n     H = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad\n     x_b = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix},\\quad\n     y = \\begin{bmatrix} 100 \\\\ -100 \\end{bmatrix},\\quad\n     B = \\begin{bmatrix} 0.01  0 \\\\ 0  0.01 \\end{bmatrix},\\quad\n     R = \\begin{bmatrix} 10  0 \\\\ 0  10 \\end{bmatrix}.\n     $$\n   - 测试 C（观测主导：大 $B$，小 $R$）：$n = 2$，$m = 2$，\n     $$\n     H = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad\n     x_b = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix},\\quad\n     y = \\begin{bmatrix} 100 \\\\ -100 \\end{bmatrix},\\quad\n     B = \\begin{bmatrix} 100  0 \\\\ 0  100 \\end{bmatrix},\\quad\n     R = \\begin{bmatrix} 0.01  0 \\\\ 0  0.01 \\end{bmatrix}.\n     $$\n   - 测试 D（部分观测，未观测分量由背景场填充）：$n = 3$，$m = 2$，\n     $$\n     H = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\end{bmatrix},\\quad\n     x_b = \\begin{bmatrix} 1 \\\\ 1 \\\\ 5 \\end{bmatrix},\\quad\n     y = \\begin{bmatrix} 10 \\\\ -10 \\end{bmatrix},\\quad\n     B = \\begin{bmatrix} 4  0  0 \\\\ 0  1  0 \\\\ 0  0  0.25 \\end{bmatrix},\\quad\n     R = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}.\n     $$\n   - 测试 E（相关误差）：$n = 2$，$m = 2$，\n     $$\n     H = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad\n     x_b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad\n     y = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix},\\quad\n     B = \\begin{bmatrix} 2  0.6 \\\\ 0.6  1 \\end{bmatrix},\\quad\n     R = \\begin{bmatrix} 0.5  0.2 \\\\ 0.2  0.5 \\end{bmatrix}.\n     $$\n4. 输出规范：\n   - 您的程序必须生成单行输出，其中包含所有测试的聚合结果，形式为方括号内包含的逗号分隔列表。\n   - 列表中的每个元素必须是该测试的分析向量 $x$，以列表形式表示，并按照上面指定的顺序（测试 A 到 E）。\n   - 格式：行内任何地方都没有空格。每个分量打印小数点后恰好 $6$ 位数字。\n   - 所需模式示例（仅为说明）：$[[x_{1,1},x_{1,2}],\\,[x_{2,1},x_{2,2}],\\dots]$。如果一个状态的维度为 $3$，它将作为 $[x_{i,1},x_{i,2},x_{i,3}]$ 出现在外层列表中。\n   - 因此，最终输出必须是一个表示浮点数列表的列表的单个字符串，采用固定的 $6$ 位小数格式，例如，对于两个假设的 $2$ 分量情况，输出为 $[[0.123456,-7.000000],[1.500000,2.250000]]$。\n\n您的解决方案必须是通用的，除了测试套件定义外，不应针对这些特定值进行硬编码。每个测试的输出值必须是浮点数列表，并按照规定进行四舍五入和格式化。",
            "solution": "用户提供的问题是有效的。它在科学上基于资料同化和加权最小二乘估计的原理，是适定的，有唯一解，并且为测试用例提供的所有必要数据都是一致的。\n\n### 第1步：最优性条件的推导\n\n目标是找到使三维变分（3DVar）代价函数 $J(x)$ 最小化的分析状态向量 $x$。代价函数为：\n$$\nJ(x) = \\lVert H x - y \\rVert_{R^{-1}}^{2} + \\lVert x - x_b \\rVert_{B^{-1}}^{2}\n$$\n其中 $x$ 是维度为 $n$ 的状态向量，$x_b$ 是背景状态向量（维度为 $n$），$y$ 是维度为 $m$ 的观测向量，$H$ 是线性观测算子（一个 $m \\times n$ 矩阵），$B$ 是背景场误差协方差矩阵（$n \\times n$），$R$ 是观测误差协方差矩阵（$m \\times m$）。$B$ 和 $R$ 都是对称正定（SPD）矩阵，这意味着它们的逆矩阵 $B^{-1}$ 和 $R^{-1}$ 也是对称正定的。\n\n使用加权平方范数的定义 $\\lVert v \\rVert_{W}^{2} = v^{\\top} W v$，我们可以展开代价函数：\n$$\nJ(x) = (H x - y)^{\\top} R^{-1} (H x - y) + (x - x_b)^{\\top} B^{-1} (x - x_b)\n$$\n为了找到 $J(x)$ 的极小化子，$J(x)$ 是一个关于 $x$ 的二次凸函数，我们计算它关于 $x$ 的梯度并将其设为零。首先，我们展开这两项：\n$$\n(H x - y)^{\\top} R^{-1} (H x - y) = x^{\\top}H^{\\top}R^{-1}Hx - x^{\\top}H^{\\top}R^{-1}y - y^{\\top}R^{-1}Hx + y^{\\top}R^{-1}y\n$$\n$$\n(x - x_b)^{\\top} B^{-1} (x - x_b) = x^{\\top}B^{-1}x - x^{\\top}B^{-1}x_b - x_b^{\\top}B^{-1}x + x_b^{\\top}B^{-1}x_b\n$$\n由于 $x^{\\top}H^{\\top}R^{-1}y$ 是一个标量，它等于其转置，即 $(x^{\\top}H^{\\top}R^{-1}y)^{\\top} = y^{\\top}(R^{-1})^{\\top}H x$。因为 $R^{-1}$ 是对称的，这变为 $y^{\\top}R^{-1}H x$。对于包含 $B^{-1}$ 的项，也有类似的恒等式成立。因此，我们可以将 $J(x)$ 写为：\n$$\nJ(x) = x^{\\top}H^{\\top}R^{-1}Hx - 2 y^{\\top}R^{-1}Hx + y^{\\top}R^{-1}y + x^{\\top}B^{-1}x - 2 x_b^{\\top}B^{-1}x + x_b^{\\top}B^{-1}x_b\n$$\n现在，我们使用标准的矩阵微分恒等式 $\\nabla_x(c^{\\top}x) = c$ 和 $\\nabla_x(x^{\\top}Ax) = (A + A^{\\top})x$ 来计算梯度 $\\nabla_x J(x)$。\n$$\n\\nabla_x J(x) = \\nabla_x(x^{\\top}H^{\\top}R^{-1}Hx) - \\nabla_x(2 y^{\\top}R^{-1}Hx) + \\nabla_x(x^{\\top}B^{-1}x) - \\nabla_x(2 x_b^{\\top}B^{-1}x)\n$$\n矩阵 $H^{\\top}R^{-1}H$ 和 $B^{-1}$ 是对称的。因此，$\\nabla_x(x^{\\top}(H^{\\top}R^{-1}H)x) = 2(H^{\\top}R^{-1}H)x$ 和 $\\nabla_x(x^{\\top}B^{-1}x) = 2B^{-1}x$。\n线性项给出 $\\nabla_x(2 y^{\\top}R^{-1}Hx) = 2(y^{\\top}R^{-1}H)^{\\top} = 2H^{\\top}(R^{-1})^{\\top}y = 2H^{\\top}R^{-1}y$ 和 $\\nabla_x(2 x_b^{\\top}B^{-1}x) = 2B^{-1}x_b$。\n结合这些，梯度为：\n$$\n\\nabla_x J(x) = 2 H^{\\top}R^{-1}H x - 2 H^{\\top}R^{-1}y + 2 B^{-1}x - 2 B^{-1}x_b\n$$\n将梯度设为零，得到最小值的必要条件：\n$$\nH^{\\top}R^{-1}H x + B^{-1}x = H^{\\top}R^{-1}y + B^{-1}x_b\n$$\n在左侧提取因子 $x$，我们得到关于分析状态 $x$ 的线性系统：\n$$\n(H^{\\top}R^{-1}H + B^{-1})x = H^{\\top}R^{-1}y + B^{-1}x_b\n$$\n这就是必要最优性条件。为了证明它对于唯一极小值也是充分的，我们必须证明 $J(x)$ 的 Hessian 矩阵是正定的。Hessian 矩阵是 $\\nabla_x^2 J(x) = 2(H^{\\top}R^{-1}H + B^{-1})$。\n矩阵 $H^{\\top}R^{-1}H + B^{-1}$ 是对称的，因为 $B^{-1}$ 是对称的，并且 $(H^{\\top}R^{-1}H)^{\\top} = H^{\\top}(R^{-1})^{\\top}(H^{\\top})^{\\top} = H^{\\top}R^{-1}H$。\n为了证明正定性，考虑任何非零向量 $v \\in \\mathbb{R}^n$：\n$$\nv^{\\top}(H^{\\top}R^{-1}H + B^{-1})v = v^{\\top}H^{\\top}R^{-1}Hv + v^{\\top}B^{-1}v = (Hv)^{\\top}R^{-1}(Hv) + v^{\\top}B^{-1}v\n$$\n由于 $R$ 是对称正定的，$R^{-1}$ 也是对称正定的，因此 $(Hv)^{\\top}R^{-1}(Hv) \\ge 0$。由于 $B$ 是对称正定的，$B^{-1}$ 也是对称正定的，因此对于 $v \\neq 0$，有 $v^{\\top}B^{-1}v > 0$。\n因此，对于所有 $v \\neq 0$，$v^{\\top}(H^{\\top}R^{-1}H + B^{-1})v > 0$。Hessian 矩阵是正定的，这意味着 $J(x)$ 是一个严格凸函数，并有一个唯一的全局极小值。因此，上面推导的线性系统对于 $x$ 有一个唯一的解。\n\n### 第2步：算法设计\n\n推导出的线性系统涉及矩阵的逆 $B^{-1}$ 和 $R^{-1}$。一个数值稳定且高效的算法应避免显式计算矩阵的逆。一种替代的等价公式，通常称为观测空间解，可以避免这种情况。当观测空间维度 $m$ 远小于状态空间维度 $n$ 时，这种公式尤其有利。我们可以使用 Sherman-Morrison-Woodbury 恒等式来推导它，但更直接的代数验证如下。解可以表示为对背景状态的增量：\n$$\nx = x_b + \\delta x\n$$\n其中，资料同化理论中一个著名的结果给出了增量 $\\delta x$：\n$$\n\\delta x = B H^{\\top} (H B H^{\\top} + R)^{-1} (y - H x_b)\n$$\n这个公式在计算分析状态 $x$ 时，完全不需要构造 $B^{-1}$ 或 $R^{-1}$。唯一需要的矩阵逆是 $(H B H^{\\top} + R)$ 的逆。矩阵 $H B H^{\\top} + R$ 是一个 $m \\times m$ 的矩阵，由于 $R$ 是对称正定的，而 $HBH^{\\top}$ 是对称半正定的，因此保证该矩阵是对称正定的。我们通过求解一个线性系统来代替显式地对这个矩阵求逆。\n\n算法如下：\n1.  计算新息（或观测减预报残差）向量：$d = y - H x_b$。\n2.  计算矩阵乘积 $M_1 = H @ B @ H^{\\top}$。\n3.  构造用于线性系统求解器的矩阵：$M_2 = M_1 + R$。\n4.  求解 $m \\times m$ 线性系统 $M_2 v = d$，以得到向量 $v$。这里，$v = (H B H^{\\top} + R)^{-1} d$。\n5.  计算状态增量 $\\delta x = B @ H^{\\top} @ v$。\n6.  计算最终的分析状态：$x = x_b + \\delta x$。\n\n该算法仅依赖于矩阵乘法和求解一个具有对称正定矩阵的线性系统，完全符合问题的约束条件。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve_3dvar(H: np.ndarray, xb: np.ndarray, y: np.ndarray, B: np.ndarray, R: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the 3D-Var analysis state x that minimizes the cost function J(x).\n\n    The analysis state x is given by the formula:\n    x = xb + B @ H.T @ inv(H @ B @ H.T + R) @ (y - H @ xb)\n    \n    This implementation avoids explicit matrix inversion by solving a linear system.\n\n    Args:\n        H: Observation operator matrix.\n        xb: Background state vector.\n        y: Observation vector.\n        B: Background error covariance matrix.\n        R: Observation error covariance matrix.\n\n    Returns:\n        The analysis state vector x.\n    \"\"\"\n    # 1. Compute the innovation vector\n    d = y - H @ xb\n\n    # 2. Compute the matrix for the linear system in observation space\n    # This matrix is H*B*H' + R\n    HBHt = H @ B @ H.T\n    M = HBHt + R\n    \n    # 3. Solve the linear system M*v = d for v\n    # This is equivalent to v = inv(H*B*H' + R) * d\n    # Since M is symmetric positive definite, we can use a specialized solver.\n    # However, linalg.solve is general and robust.\n    v = linalg.solve(M, d, assume_a='pos')\n\n    # 4. Compute the state increment\n    # This is B*H'*v\n    delta_x = B @ H.T @ v\n\n    # 5. Compute the final analysis state\n    x = xb + delta_x\n\n    return x\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for the 3D-Var solver.\n    \"\"\"\n    # Test cases defined in the problem statement\n    test_cases = [\n        # Test A: Happy path, balanced weights\n        {\n            \"H\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"xb\": np.array([0.0, 0.0]),\n            \"y\": np.array([1.0, -2.0]),\n            \"B\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"R\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n        },\n        # Test B: Background-dominated\n        {\n            \"H\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"xb\": np.array([2.0, 2.0]),\n            \"y\": np.array([100.0, -100.0]),\n            \"B\": np.array([[0.01, 0.0], [0.0, 0.01]]),\n            \"R\": np.array([[10.0, 0.0], [0.0, 10.0]]),\n        },\n        # Test C: Observation-dominated\n        {\n            \"H\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"xb\": np.array([2.0, 2.0]),\n            \"y\": np.array([100.0, -100.0]),\n            \"B\": np.array([[100.0, 0.0], [0.0, 100.0]]),\n            \"R\": np.array([[0.01, 0.0], [0.0, 0.01]]),\n        },\n        # Test D: Partial observation\n        {\n            \"H\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]),\n            \"xb\": np.array([1.0, 1.0, 5.0]),\n            \"y\": np.array([10.0, -10.0]),\n            \"B\": np.array([[4.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.25]]),\n            \"R\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n        },\n        # Test E: Correlated errors\n        {\n            \"H\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"xb\": np.array([0.0, 0.0]),\n            \"y\": np.array([1.0, 2.0]),\n            \"B\": np.array([[2.0, 0.6], [0.6, 1.0]]),\n            \"R\": np.array([[0.5, 0.2], [0.2, 0.5]]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        x_analysis = solve_3dvar(case[\"H\"], case[\"xb\"], case[\"y\"], case[\"B\"], case[\"R\"])\n        results.append(x_analysis)\n\n    # Format the output string as per the requirements\n    # A single line, comma-separated list of lists, no spaces, 6 decimal places.\n    formatted_results = []\n    for res_vec in results:\n        formatted_vec = [f\"{val:.6f}\" for val in res_vec]\n        formatted_results.append(f\"[{','.join(formatted_vec)}]\")\n    \n    final_output = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "虽然前一个练习确定了定义最优分析的线性系统，但对于具有数百万状态变量的实际问题，直接求解该系统是不可行的。本练习  介绍了业务化数据同化中使用的基本计算技术：无矩阵迭代求解器。您将通过创建一个计算Hessian矩阵与向量乘积的程序来实现共轭梯度算法，从而学习如何在不显式构造庞大的Hessian矩阵的情况下找到解。",
            "id": "3426291",
            "problem": "考虑定义在状态向量 $x \\in \\mathbb{R}^n$ 上的三维变分 (3D-Var) 数据同化代价函数\n$$\nJ(x) \\equiv \\tfrac{1}{2}\\,(x - x_b)^\\top B^{-1} (x - x_b) + \\tfrac{1}{2}\\,\\bigl(Hx - y\\bigr)^\\top R^{-1} \\bigl(Hx - y\\bigr),\n$$\n其中 $x_b \\in \\mathbb{R}^n$ 是背景场状态，$y \\in \\mathbb{R}^m$ 是观测向量，$B \\in \\mathbb{R}^{n \\times n}$ 是对称正定的背景误差协方差矩阵，$R \\in \\mathbb{R}^{m \\times m}$ 是对称正定的观测误差协方差矩阵，$H \\in \\mathbb{R}^{m \\times n}$ 是线性观测算子。假设所有矩阵的条件数都足够好，以确保在双精度算術下的数值稳定性。\n\n你的任务是设计一个无矩阵算法，通过求解在 $x_b$ 附近线性化的一阶最优性条件，来计算最小化 $J(x_b + \\delta x)$ 的分析增量 $\\delta x \\in \\mathbb{R}^n$。请使用以下原则作为你的基本依据：\n\n- 可微标量泛函的梯度由其对任意方向的第一变分定义。\n- Hessian-向量积定义为梯度沿任意向量方向的方向导数。\n- 欧几里得内积中伴随算子的线性代数恒等式以及线性映射的微分基本法则。\n\n你必须：\n\n1. 从上面 $J(x)$ 的定义和所列原则出发，推导在 $x_b$ 处 $J$ 的 Hessian-向量积的无矩阵表达式，即一个将任意向量 $v \\in \\mathbb{R}^n$ 映射到 $H_J[v] \\in \\mathbb{R}^n$ 的过程，而无需显式地构造任何稠密的 Hessian 矩阵。你的推导应仅依赖于将 $B^{-1}$、$R^{-1}$、$H$ 和 $H^\\top$ 应用于向量。\n\n2. 实现一个共轭梯度 (CG) 方法，该方法仅使用你的 Hessian-向量积算子和从 $x_b$ 处线性化的一阶最优性条件得到的右端项，来计算最小化 $J(x_b + \\delta x)$ 的增量 $\\delta x$。使用零初始猜测，绝对或相对残差容差最多为 $10^{-10}$，以及最大迭代次数至少为 $100$ 次。不要在 CG 内部显式地构造 Hessian 矩阵。\n\n3. 为了验证，对于每个测试用例，显式地构造与线性化一阶条件相关的稠密线性系统，并使用稠密线性求解器直接求解以获得参考增量 $\\delta x_{\\mathrm{ref}}$。使用相对误差将你的无矩阵 CG 增量 $\\delta x_{\\mathrm{cg}}$ 与 $\\delta x_{\\mathrm{ref}}$ 进行比较\n$$\n\\varepsilon \\equiv \\frac{\\lVert \\delta x_{\\mathrm{cg}} - \\delta x_{\\mathrm{ref}} \\rVert_2}{\\max\\bigl(\\lVert \\delta x_{\\mathrm{ref}} \\rVert_2,\\, 1\\bigr)}.\n$$\n\n4. 所有涉及 $B$ 和 $R$ 的线性求解都应通过求解线性系统来实现（即通过求解线性系统来应用 $B^{-1}$ 和 $R^{-1}$），而不是在 CG 迭代内部进行显式的矩阵求逆。你只能在单独的稠密参考路径中构造显式逆矩阵。\n\n给定以下测试套件。每个测试用例都提供了矩阵 $B$、$R$、$H$ 和向量 $x_b$、$y$，并指定了所有数值。所有数字都以实值单位给出；不需要物理单位。\n\n- 测试用例 1 (理想路径，中等各向异性)：\n  - $n = 5$, $m = 3$。\n  - $B = \\mathrm{diag}(1.0, 2.0, 3.0, 4.0, 5.0)$。\n  - $R = \\mathrm{diag}(0.5, 0.7, 1.1)$。\n  - $H = \\begin{bmatrix}\n    1.0  0.0  0.0  0.0  0.0 \\\\\n    0.0  1.0  1.0  0.0  0.0 \\\\\n    0.0  0.0  0.0  1.0  -1.0\n  \\end{bmatrix}$。\n  - $x_b = \\begin{bmatrix} 0.5 \\\\ -1.0 \\\\ 0.3 \\\\ 0.0 \\\\ 0.2 \\end{bmatrix}$，$y = \\begin{bmatrix} 1.0 \\\\ -0.4 \\\\ 0.1 \\end{bmatrix}$。\n\n- 测试用例 2 (病态条件和由背景稳定的秩亏观测几何)：\n  - $n = 4$, $m = 2$。\n  - $B = \\mathrm{diag}(10^{-3}, 1.0, 10^{2}, 10^{-1})$。\n  - $R = \\mathrm{diag}(10^{-2}, 10^{1})$。\n  - $H = \\begin{bmatrix}\n    1.0  1.0  0.0  0.0 \\\\\n    0.0  0.0  1.0  -1.0\n  \\end{bmatrix}$。\n  - $x_b = \\begin{bmatrix} 1.0 \\\\ -1.0 \\\\ 2.0 \\\\ -2.0 \\end{bmatrix}$，$y = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$。\n\n- 测试用例 3 (边界情况：零观测算子)：\n  - $n = 3$, $m = 2$。\n  - $B = \\mathrm{diag}(2.0, 2.0, 2.0)$。\n  - $R = \\mathrm{diag}(1.0, 1.0)$。\n  - $H = \\begin{bmatrix}\n    0.0  0.0  0.0 \\\\\n    0.0  0.0  0.0\n  \\end{bmatrix}$。\n  - $x_b = \\begin{bmatrix} 3.0 \\\\ -2.0 \\\\ 1.0 \\end{bmatrix}$，$y = \\begin{bmatrix} 1.0 \\\\ -4.0 \\end{bmatrix}$。\n\n- 测试用例 4 (全观测，非对角背景协方差)：\n  - $n = 3$, $m = 3$。\n  - $B = \\begin{bmatrix}\n    2.0  0.5  0.0 \\\\\n    0.5  1.5  0.2 \\\\\n    0.0  0.2  1.0\n  \\end{bmatrix}$。\n  - $R = \\mathrm{diag}(0.05, 0.2, 0.1)$。\n  - $H = I_3$（$3 \\times 3$ 单位矩阵）。\n  - $x_b = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$，$y = \\begin{bmatrix} 1.0 \\\\ -2.0 \\\\ 0.5 \\end{bmatrix}$。\n\n实现细节和要求：\n\n- 对于每个测试用例，定义在 $x_b$ 处线性化的一阶最优性方程的右端项，并仅使用 Hessian-向量積，通过你的无矩阵共轭梯度方法求解。对 $\\delta x$ 使用零初始猜测。\n- 独立地，通过显式构造线性化的正规算子来形成相应的稠密线性系统，并使用稠密求解器求解参考增量 $\\delta x_{\\mathrm{ref}}$。\n- 对于每个测试用例，计算如上定义的标量相对误差 $\\varepsilon$。\n- 最终输出格式：你的程序應生成单行输出，其中包含四个相对误差的逗号分隔列表，并用方括号括起来，例如，“[e1,e2,e3,e4]”。每个 $e_k$ 必须表示为标准或科学记数法的小数。\n\n不涉及角度或物理单位；所有输出都是无量纲的实数。程序必须是自包含的，并且不得读取任何输入。唯一允许的库是 Python 标准库、NumPy 和 SciPy，但除了稠密验证和在 Hessian-向量积中将 $B^{-1}$ 和 $R^{-1}$ 应用于向量所需的基本线性求解之外，你不能调用任何高级线性求解器。",
            "solution": "目标是找到分析增量 $\\delta x \\in \\mathbb{R}^n$，以最小化三维变分 (3D-Var) 代价函数 $J(x_b + \\delta x)$，其中 $x = x_b + \\delta x$。代价函数由下式给出：\n$$\nJ(x) = \\frac{1}{2}(x - x_b)^\\top B^{-1} (x - x_b) + \\frac{1}{2}(Hx - y)^\\top R^{-1} (Hx - y)\n$$\n代入 $x = x_b + \\delta x$ 得到以增量 $\\delta x$ 表示的代价函数：\n$$\nJ(x_b + \\delta x) = \\frac{1}{2}(\\delta x)^\\top B^{-1} (\\delta x) + \\frac{1}{2}(H(x_b + \\delta x) - y)^\\top R^{-1} (H(x_b + \\delta x) - y)\n$$\n令新息向量为 $d = y - Hx_b$。表达式 $H(x_b + \\delta x) - y$ 可简化为 $H\\delta x - d$。作为 $\\delta x$ 的泛函的代价函数（我们记为 $\\mathcal{J}(\\delta x)$）是：\n$$\n\\mathcal{J}(\\delta x) = \\frac{1}{2}(\\delta x)^\\top B^{-1} (\\delta x) + \\frac{1}{2}(H\\delta x - d)^\\top R^{-1} (H\\delta x - d)\n$$\n这是一个关于 $\\delta x$ 的二次泛函。为了找到最小值，我们计算它关于 $\\delta x$ 的梯度并将其设为零。梯度是通过计算对于任意扰动 $u \\in \\mathbb{R}^n$ 的第一变分 $\\delta \\mathcal{J}$ 来找到的：\n$$\n\\delta \\mathcal{J}(\\delta x; u) = \\lim_{\\epsilon \\to 0} \\frac{\\mathcal{J}(\\delta x + \\epsilon u) - \\mathcal{J}(\\delta x)}{\\epsilon}\n$$\n展开 $\\mathcal{J}(\\delta x + \\epsilon u)$ 并收集 $\\epsilon$ 的一阶项：\n$$\n\\mathcal{J}(\\delta x + \\epsilon u) = \\frac{1}{2}(\\delta x + \\epsilon u)^\\top B^{-1} (\\delta x + \\epsilon u) + \\frac{1}{2}(H(\\delta x + \\epsilon u) - d)^\\top R^{-1} (H(\\delta x + \\epsilon u) - d)\n$$\n$$\n= \\mathcal{J}(\\delta x) + \\epsilon u^\\top B^{-1} \\delta x + \\epsilon (Hu)^\\top R^{-1} (H\\delta x - d) + O(\\epsilon^2)\n$$\n利用伴随性质 $(Hu)^\\top = u^\\top H^\\top$，第一变分为：\n$$\n\\delta \\mathcal{J}(\\delta x; u) = u^\\top \\left( B^{-1}\\delta x + H^\\top R^{-1}(H\\delta x - d) \\right)\n$$\n根据定义，梯度 $\\nabla \\mathcal{J}(\\delta x)$ 是对所有 $u$ 都满足 $\\delta \\mathcal{J}(\\delta x; u) = u^\\top \\nabla \\mathcal{J}(\\delta x)$ 的向量。因此：\n$$\n\\nabla \\mathcal{J}(\\delta x) = B^{-1}\\delta x + H^\\top R^{-1}(H\\delta x - d)\n$$\n一阶最优性条件是 $\\nabla \\mathcal{J}(\\delta x) = 0$，这给出：\n$$\nB^{-1}\\delta x + H^\\top R^{-1}H\\delta x - H^\\top R^{-1}d = 0\n$$\n整理后得到最优增量 $\\delta x$ 的线性系统：\n$$\n\\left( B^{-1} + H^\\top R^{-1}H \\right) \\delta x = H^\\top R^{-1}d\n$$\n代入 $d = y - Hx_b$，我们得到最终形式：\n$$\n\\left( B^{-1} + H^\\top R^{-1}H \\right) \\delta x = H^\\top R^{-1}(y - Hx_b)\n$$\n这是一个形式为 $\\mathcal{H}\\delta x = b$ 的线性系统，其中 $\\mathcal{H} = B^{-1} + H^\\top R^{-1}H$ 是代价函数 $\\mathcal{J}$ 的 Hessian 矩阵，而 $b = H^\\top R^{-1}(y - Hx_b)$ 是右端项。由于 $B$ 和 $R$ 是对称正定 (SPD) 矩阵，它们的逆 $B^{-1}$ 和 $R^{-1}$ 也是。Hessian 矩阵 $\\mathcal{H}$ 是一个 SPD 矩阵 ($B^{-1}$) 和一个对称半正定矩阵 ($H^\\top R^{-1}H$) 的和，这使得 $\\mathcal{H}$ 本身也是 SPD 的，从而保证了唯一解的存在。\n\n该问题要求使用一种无矩阵方法，即共轭梯度 (CG) 法。该方法需要一个函数来计算任意向量 $v \\in \\mathbb{R}^n$ 的 Hessian-向量积 $\\mathcal{H}v$，而无需显式构造矩阵 $\\mathcal{H}$。\n\nHessian-向量积 $\\mathcal{H}v$ 是从 Hessian 作为梯度导数的定义中推导出来的。Hessian 作用于向量 $v$ 的结果是梯度在 $v$ 方向上的方向导数。\n$$\n\\mathcal{H}[v] = \\frac{d}{d\\epsilon}\\nabla \\mathcal{J}(\\delta x + \\epsilon v)\\Big|_{\\epsilon=0}\n$$\n由于 $\\nabla\\mathcal{J}$ 是 $\\delta x$ 的线性函数，我们有：\n$$\n\\nabla \\mathcal{J}(\\delta x + \\epsilon v) = B^{-1}(\\delta x + \\epsilon v) + H^\\top R^{-1}(H(\\delta x + \\epsilon v) - d)\n$$\n$$\n= \\nabla\\mathcal{J}(\\delta x) + \\epsilon (B^{-1}v + H^\\top R^{-1}Hv)\n$$\n在 $\\epsilon=0$ 处对 $\\epsilon$ 的导数即为 Hessian-向量积：\n$$\n\\mathcal{H}[v] = B^{-1}v + H^\\top R^{-1}Hv\n$$\n这个操作可以作为一系列无矩阵步骤来实现：\n1. 计算 $v_1 = Hv$。\n2. 通过求解线性系统 $Rv_2 = v_1$ 来计算 $v_2 = R^{-1}v_1$。\n3. 计算 $v_3 = H^\\top v_2$。\n4. 通过求解线性系统 $Bv_4 = v$ 来计算 $v_4 = B^{-1}v$。\n5. 结果是 $\\mathcal{H}v = v_3 + v_4$。\n\n右端项 $b = H^\\top R^{-1}(y - Hx_b)$ 也是在没有显式矩阵求逆的情况下计算的：\n1. 计算新息 $d = y - Hx_b$。\n2. 通过求解 $Rd_R=d$ 来计算 $d_R = R^{-1}d$。\n3. 结果是 $b = H^\\top d_R$。\n\n共轭梯度算法迭代求解系统 $\\mathcal{H}\\delta x = b$。从初始猜测 $\\delta x_0 = 0$ 开始，算法按以下步骤进行：\n1.  初始化：\n    $\\delta x \\leftarrow 0$\n    $r \\leftarrow b - \\mathcal{H}(\\delta x) = b$\n    $p \\leftarrow r$\n    $rs_{\\text{old}} \\leftarrow r^\\top r$\n2.  从 $k=0, 1, 2, \\dots$ 开始迭代，直到收敛：\n    a. 计算 $\\mathcal{H}p$。\n    b. $\\alpha \\leftarrow rs_{\\text{old}} / (p^\\top \\mathcal{H}p)$。\n    c. $\\delta x \\leftarrow \\delta x + \\alpha p$。\n    d. $r \\leftarrow r - \\alpha \\mathcal{H}p$。\n    e. $rs_{\\text{new}} \\leftarrow r^\\top r$。\n    f. 如果 $\\sqrt{rs_{\\text{new}}}$ 低于指定的容差，则停止。\n    g. $p \\leftarrow r + (rs_{\\text{new}} / rs_{\\text{old}}) p$。\n    h. $rs_{\\text{old}} \\leftarrow rs_{\\text{new}}$。\n\n为了验证，显式地构造稠密线性系统。计算矩阵 $B^{-1}$ 和 $R^{-1}$，然后构造稠密 Hessian 矩阵 $\\mathcal{H}_{\\text{dense}} = B^{-1} + H^\\top R^{-1} H$。右端项 $b_{\\text{dense}}$ 也用类似方法计算。然后通过使用直接求解器求解 $\\mathcal{H}_{\\text{dense}}\\delta x_{\\text{ref}} = b_{\\text{dense}}$ 来找到参考解 $\\delta x_{\\text{ref}}$。相对误差 $\\varepsilon = \\lVert \\delta x_{\\mathrm{cg}} - \\delta x_{\\mathrm{ref}} \\rVert_2 / \\max(\\lVert \\delta x_{\\mathrm{ref}} \\rVert_2, 1)$ 量化了无矩阵 CG 解的准确性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the 3D-Var data assimilation problem for a suite of test cases.\n\n    For each case, it computes the analysis increment using a custom matrix-free\n    Conjugate Gradient (CG) solver. It then validates this result against a\n    reference solution obtained from a direct dense linear system solve.\n    Finally, it computes and prints the relative error between the two solutions.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: happy path, moderate anisotropy\n        {\n            \"n\": 5, \"m\": 3,\n            \"B\": np.diag([1.0, 2.0, 3.0, 4.0, 5.0]),\n            \"R\": np.diag([0.5, 0.7, 1.1]),\n            \"H\": np.array([\n                [1.0, 0.0, 0.0, 0.0, 0.0],\n                [0.0, 1.0, 1.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 1.0, -1.0]\n            ]),\n            \"x_b\": np.array([0.5, -1.0, 0.3, 0.0, 0.2]),\n            \"y\": np.array([1.0, -0.4, 0.1])\n        },\n        # Case 2: ill-conditioning\n        {\n            \"n\": 4, \"m\": 2,\n            \"B\": np.diag([1e-3, 1.0, 1e2, 1e-1]),\n            \"R\": np.diag([1e-2, 1e1]),\n            \"H\": np.array([\n                [1.0, 1.0, 0.0, 0.0],\n                [0.0, 0.0, 1.0, -1.0]\n            ]),\n            \"x_b\": np.array([1.0, -1.0, 2.0, -2.0]),\n            \"y\": np.array([0.0, 1.0])\n        },\n        # Case 3: zero observation operator\n        {\n            \"n\": 3, \"m\": 2,\n            \"B\": np.diag([2.0, 2.0, 2.0]),\n            \"R\": np.diag([1.0, 1.0]),\n            \"H\": np.zeros((2, 3)),\n            \"x_b\": np.array([3.0, -2.0, 1.0]),\n            \"y\": np.array([1.0, -4.0])\n        },\n        # Case 4: full observation with non-diagonal background covariance\n        {\n            \"n\": 3, \"m\": 3,\n            \"B\": np.array([\n                [2.0, 0.5, 0.0],\n                [0.5, 1.5, 0.2],\n                [0.0, 0.2, 1.0]\n            ]),\n            \"R\": np.diag([0.05, 0.2, 0.1]),\n            \"H\": np.identity(3),\n            \"x_b\": np.array([0.0, 0.0, 0.0]),\n            \"y\": np.array([1.0, -2.0, 0.5])\n        }\n    ]\n\n    def cg_solver(A_matvec, b, x0, tol=1e-10, max_iter=100):\n        \"\"\"\n        Solves the linear system Ax=b using the Conjugate Gradient method.\n\n        Args:\n            A_matvec: A function that computes the matrix-vector product A*v.\n            b: The right-hand-side vector.\n            x0: The initial guess for the solution.\n            tol: The convergence tolerance.\n            max_iter: The maximum number of iterations.\n\n        Returns:\n            The solution vector x.\n        \"\"\"\n        x = x0.copy()\n        r = b - A_matvec(x)\n        p = r.copy()\n        rs_old = np.dot(r, r)\n\n        norm_b = np.linalg.norm(b)\n        stop_criterion = tol * norm_b if norm_b > 0 else tol\n\n        if np.sqrt(rs_old)  stop_criterion:\n            return x\n\n        for i in range(max_iter):\n            Ap = A_matvec(p)\n            alpha = rs_old / np.dot(p, Ap)\n            x += alpha * p\n            r -= alpha * Ap\n            rs_new = np.dot(r, r)\n\n            if np.sqrt(rs_new)  stop_criterion:\n                break\n            \n            p = r + (rs_new / rs_old) * p\n            rs_old = rs_new\n            \n        return x\n\n    results = []\n    for case in test_cases:\n        B, R, H, x_b, y = case[\"B\"], case[\"R\"], case[\"H\"], case[\"x_b\"], case[\"y\"]\n        n = case[\"n\"]\n\n        # --- Matrix-Free CG Path ---\n\n        def hessian_vector_product(v):\n            \"\"\"Matrix-free Hessian-vector product: (B^-1 + H' R^-1 H) v\"\"\"\n            term1 = np.linalg.solve(B, v)\n            Hv = H @ v\n            R_inv_Hv = np.linalg.solve(R, Hv)\n            term2 = H.T @ R_inv_Hv\n            return term1 + term2\n\n        # Compute right-hand side: H' R^-1 (y - H x_b)\n        innovation = y - H @ x_b\n        R_inv_innovation = np.linalg.solve(R, innovation)\n        rhs = H.T @ R_inv_innovation\n\n        # Solve for the increment using CG\n        delta_x_cg = cg_solver(hessian_vector_product, rhs, np.zeros(n), max_iter=100)\n\n        # --- Dense Reference Path ---\n        \n        # Explicitly form the Hessian and RHS\n        B_inv = np.linalg.inv(B)\n        R_inv = np.linalg.inv(R)\n        \n        Hess_dense = B_inv + H.T @ R_inv @ H\n        rhs_dense = H.T @ R_inv @ (y - H @ x_b)\n        \n        # Solve for reference increment\n        delta_x_ref = np.linalg.solve(Hess_dense, rhs_dense)\n\n        # --- Compare and calculate error ---\n        \n        diff_norm = np.linalg.norm(delta_x_cg - delta_x_ref)\n        ref_norm = np.linalg.norm(delta_x_ref)\n        \n        error = diff_norm / max(ref_norm, 1.0)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "现实世界中的观测过程很少是线性的。这最后一个练习  将我们的框架扩展到处理非线性观测算子，这是在卫星遥感等应用中经常遇到的情况。您将研究以传感器饱和效应为例的非线性如何改变代价函数的地形，并使得像高斯-牛顿法这样的迭代优化方案成为必需，从而深入了解收敛的挑战以及吸引盆地的概念。",
            "id": "3426295",
            "problem": "考虑一个三维变分（3D-Var）数据同化问题，其非线性观测算子包含饱和效应，其灵感来源于遥感中的辐射度削波。从高斯误差假设下的最大后验（MAP）估计出发，推导在存在非线性观测算子时的变分代价，然后构建并分析不同再线性化方案下的最小化策略。您的最终交付成果必须是本问题末尾指定的完整、可运行的程序。\n\n您将获得以下基本要素：\n- 先验（背景）状态被建模为一个高斯随机变量，其均值为 $\\boldsymbol{x}_b \\in \\mathbb{R}^3$，协方差矩阵为 $\\boldsymbol{B} \\in \\mathbb{R}^{3 \\times 3}$，其中 $\\boldsymbol{B}$ 是对称正定矩阵。\n- 观测误差是独立的，且服从高斯分布，其协方差矩阵为 $\\boldsymbol{R} \\in \\mathbb{R}^{3 \\times 3}$，其中 $\\boldsymbol{R}$ 是对称正定矩阵。\n- 观测算子包含辐射度的饱和，这意味着正向算子通过一个线性映射后进行逐元素削波，将状态 $\\boldsymbol{x} \\in \\mathbb{R}^3$ 映射到观测值：\n$$\n\\boldsymbol{h}(\\boldsymbol{x}) = \\mathrm{clip}\\!\\left(\\boldsymbol{A}\\boldsymbol{x};\\, \\boldsymbol{\\ell}, \\boldsymbol{u}\\right),\n$$\n其中 $\\boldsymbol{A} \\in \\mathbb{R}^{3 \\times 3}$，且对于 $\\boldsymbol{z} = \\boldsymbol{A}\\boldsymbol{x}$，$\\mathrm{clip}$ 按分量定义为\n$$\n\\mathrm{clip}(z_i; \\ell_i, u_i) = \\min\\left(\\max(z_i,\\ell_i),u_i\\right).\n$$\n- 在这些假设下，负对数后验与一个代价函数成正比，该代价函数结合了一个二次背景项和一个依赖于 $\\boldsymbol{h}(\\boldsymbol{x})$ 的观测项。\n\n任务1（推导）：\n- 从高斯误差假设和贝叶斯法则出发，推导变分代价函数 $J(\\boldsymbol{x})$ 的显式形式。不要假设 $\\boldsymbol{h}(\\boldsymbol{x})$ 的线性性；在观测算子中保留饱和效应。\n- 对于 $\\boldsymbol{h}(\\boldsymbol{x})$ 的雅可比矩阵几乎处处存在的情况，推导其梯度 $\\nabla J(\\boldsymbol{x})$ 和 Gauss-Newton 近似 Hessian 矩阵。描述饱和的处理方式：对于每个观测分量 $i$，当 $\\boldsymbol{A}\\boldsymbol{x}$ 对应的分量处于或超出饱和边界时，雅可比贡献为零；当其严格在界内时，雅可比贡献为一。\n\n任务2（算法设计）：\n- 在 Gauss-Newton 型最小化中考虑三种再线性化方案：\n  1. 方案 $S_0$（冻结线性化）：在初始猜测点 $\\boldsymbol{x}^{(0)}$ 对 $\\boldsymbol{h}(\\boldsymbol{x})$ 进行一次线性化，并精确求解得到的二次问题。也就是说，使用线性化\n  $$\n  \\boldsymbol{h}(\\boldsymbol{x}) \\approx \\boldsymbol{h}(\\boldsymbol{x}^{(0)}) + \\boldsymbol{J}_h(\\boldsymbol{x}^{(0)})(\\boldsymbol{x}-\\boldsymbol{x}^{(0)}),\n  $$\n  其中 $\\boldsymbol{J}_h$ 是使用上述内部掩码规则计算的 $\\boldsymbol{h}$ 的雅可比矩阵。通过一个线性系统以闭式解求解相应的二次最小化器。\n  2. 方案 $S_1$（完全再线性化）：在每次迭代中，计算 $\\boldsymbol{h}(\\boldsymbol{x}^{(k)})$ 和 $\\boldsymbol{J}_h(\\boldsymbol{x}^{(k)})$，构造 Gauss-Newton 系统，并使用回溯线搜索进行步长更新，以保证 $J(\\boldsymbol{x})$ 的下降。\n  3. 方案 $S_2$（自适应再线性化）：维持当前的线性化点和雅可比矩阵；仅当活动集（饱和掩码）与上一个接受的迭代点相比发生变化时，才进行再线性化。在两次再线性化之间，使用由当前线性化定义的二次代理模型来计算步长，并通过回溯线搜索在真实代价上强制下降。\n- 对于方案 $S_1$ 和 $S_2$，实现一个带有 Armijo 型条件的回溯线搜索。使用步长参数 $\\alpha$ 初始化为 $1$，每次回溯减半，Armijo 参数 $\\gamma = 10^{-4}$。\n\n任务3（带观测饱和的吸引盆分析）：\n- 对以下每个测试案例，通过扫描一个初始猜测点的网格来评估最小化的吸引盆。对于每个初始猜测点，运行三种方案直至收敛（受停止准则约束），并记录运行是否收敛到该测试案例中完全再线性化在整个网格上可达到的最优（最低）代价。\n- 对于一个方案和一个初始猜测点，将“成功”定义为达到的最终代价 $J$ 与该测试案例中所有从网格开始的完全再线性化（$S_1$）运行所得到的最佳代价之差在容差 $\\varepsilon_J = 10^{-6}$ 之内。\n- 迭代方案（$S_1$ 和 $S_2$）的停止准则：最大迭代次数 $N_{\\max} = 50$，以及步长范数阈值 $\\|\\boldsymbol{p}^{(k)}\\|_2 \\le 10^{-8}$。对于 $S_0$，直接求解单个线性系统以获得冻结二次最小化器。\n- 初始猜测网格：使用 $\\{-1.5, -0.75, 0.0, 0.75, 1.5\\}$ 在每个分量上的笛卡尔积，得到 $5^3 = 125$ 个初始猜测点。\n\n测试套件：\n对于每个测试案例，使用状态维度为 $3$，并采用以下固定的矩阵和向量：\n- 背景均值：\n$$\n\\boldsymbol{x}_b = \\begin{bmatrix} -1.0 \\\\ 0.0 \\\\ 1.0 \\end{bmatrix}.\n$$\n- 背景协方差：\n$$\n\\boldsymbol{B} = \\mathrm{diag}\\!\\left(4.0,\\,1.0,\\,9.0\\right).\n$$\n- 观测协方差：\n$$\n\\boldsymbol{R} = \\mathrm{diag}\\!\\left(0.09,\\,0.04,\\,0.16\\right).\n$$\n- 观测算子的线性部分：\n$$\n\\boldsymbol{A} = \\begin{bmatrix}\n1.0  0.5  0.0 \\\\\n0.0  1.0  0.3 \\\\\n0.2  0.0  1.0\n\\end{bmatrix}.\n$$\n- 用于合成（无噪声）观测的真实状态：\n$$\n\\boldsymbol{x}_{\\text{true}} = \\begin{bmatrix} 2.0 \\\\ -1.5 \\\\ 0.5 \\end{bmatrix}.\n$$\n- 对于以下每个测试案例，观测按 $\\boldsymbol{y} = \\mathrm{clip}\\!\\left(\\boldsymbol{A}\\boldsymbol{x}_{\\text{true}};\\boldsymbol{\\ell},\\boldsymbol{u}\\right)$ 合成，边界按案例指定：\n\n测试案例 1（中度饱和）：\n$$\n\\boldsymbol{\\ell} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\\quad\n\\boldsymbol{u} = \\begin{bmatrix} 1.0 \\\\ 1.2 \\\\ 0.8 \\end{bmatrix}.\n$$\n\n测试案例 2（重度饱和）：\n$$\n\\boldsymbol{\\ell} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix},\\quad\n\\boldsymbol{u} = \\begin{bmatrix} 0.6 \\\\ 0.4 \\\\ 0.5 \\end{bmatrix}.\n$$\n\n测试案例 3（无饱和；有效线性）：\n$$\n\\boldsymbol{\\ell} = \\begin{bmatrix} -10.0 \\\\ -10.0 \\\\ -10.0 \\end{bmatrix},\\quad\n\\boldsymbol{u} = \\begin{bmatrix} 10.0 \\\\ 10.0 \\\\ 10.0 \\end{bmatrix}.\n$$\n\n实现细节：\n- 雅可比矩阵使用内部掩码规则：对于每个分量 $i$，如果 $(\\boldsymbol{A}\\boldsymbol{x})_i$ 满足 $\\ell_i  (\\boldsymbol{A}\\boldsymbol{x})_i  u_i$，则局部导数为 $1$；否则为 $0$。那么，$\\boldsymbol{h}$ 在 $\\boldsymbol{x}$ 处的雅可比矩阵为 $\\boldsymbol{J}_h(\\boldsymbol{x}) = \\mathrm{diag}(\\boldsymbol{m}(\\boldsymbol{x}))\\,\\boldsymbol{A}$，其中 $m_i(\\boldsymbol{x})$ 是 $\\{0,1\\}$ 中的内部掩码指示符。\n- 第 $k$ 次迭代的 Gauss-Newton 系统由以下线性系统给出\n$$\n\\left(\\boldsymbol{B}^{-1} + \\boldsymbol{J}_h(\\boldsymbol{x}^{(k)})^\\top \\boldsymbol{R}^{-1}\\boldsymbol{J}_h(\\boldsymbol{x}^{(k)})\\right)\\boldsymbol{p}^{(k)} = -\\nabla J(\\boldsymbol{x}^{(k)}),\n$$\n步长更新为 $\\boldsymbol{x}^{(k+1)} = \\boldsymbol{x}^{(k)} + \\alpha^{(k)}\\boldsymbol{p}^{(k)}$，其中 $\\alpha^{(k)}$ 通过回溯选择以确保下降。\n- 对于 $S_0$，通过求解以下方程获得在 $\\boldsymbol{x}^{(0)}$ 处单次线性化定义的冻结二次代理模型 $Q(\\boldsymbol{x})$ 的闭式解最小化器：\n$$\n\\left(\\boldsymbol{B}^{-1} + \\boldsymbol{J}_0^\\top \\boldsymbol{R}^{-1}\\boldsymbol{J}_0\\right)\\boldsymbol{x}\n= \\boldsymbol{B}^{-1}\\boldsymbol{x}_b + \\boldsymbol{J}_0^\\top \\boldsymbol{R}^{-1}\\left(\\boldsymbol{y} - \\boldsymbol{c}_0\\right),\n$$\n其中 $\\boldsymbol{J}_0 = \\boldsymbol{J}_h(\\boldsymbol{x}^{(0)})$ 且 $\\boldsymbol{c}_0 = \\boldsymbol{h}(\\boldsymbol{x}^{(0)}) - \\boldsymbol{J}_0\\boldsymbol{x}^{(0)}$。\n\n要求输出：\n- 对于每个测试案例，计算每个方案 $S_0$、$S_1$ 和 $S_2$ 的成功初始猜测点的比例，顺序如此。将每个比例表示为 $[0,1]$ 区间内的浮点数。\n- 您的程序应产生单行输出，包含结果，格式为一个由三个列表组成的逗号分隔列表，每个内部列表对应一个测试案例，包含三个浮点数，顺序为 $[S_0, S_1, S_2]$。例如：\n$$\n[\\,[f_{1,0},f_{1,1},f_{1,2}],\\,[f_{2,0},f_{2,1},f_{2,2}],\\,[f_{3,0},f_{3,1},f_{3,2}]\\,].\n$$\n不应打印任何其他文本。",
            "solution": "用户提供了一个明确定义的三维变分（3D-Var）数据同化问题，该问题具有一个带饱和效应的非线性观测算子。任务是推导相应的代价函数及其导数，设计并实现三种不同的最小化策略，并在具有不同非线性程度的一系列测试案例中分析它们的性能，特别是它们的吸引盆。\n\n该问题在科学上基于反问题和数据同化的理论，建立在标准的高斯误差假设下的最大后验（MAP）估计框架之上。在观测算子中包含饱和函数 `clip` 是对传感器非线性的一种现实建模。所有参数、矩阵和算法规范都已提供，使得该问题自洽且适定。代价函数是严格凸的，保证了唯一的最小化子。所提出的最小化方案——冻结线性化（$S_0$）、完全再线性化（$S_1$）和自适应再线性化（$S_2$）——是针对此类非线性最小二乘问题的标准且适用的方法。该问题是有效的，并允许直接求解。\n\n### 任务1：变分代价函数及其导数的推导\n\n该问题从贝叶斯视角处理，目标是找到状态向量 $\\boldsymbol{x} \\in \\mathbb{R}^3$ 的最大后验（MAP）估计。后验概率密度函数 $p(\\boldsymbol{x}|\\boldsymbol{y})$ 由贝叶斯定理给出：\n$$\np(\\boldsymbol{x}|\\boldsymbol{y}) \\propto p(\\boldsymbol{y}|\\boldsymbol{x})p(\\boldsymbol{x})\n$$\n其中 $p(\\boldsymbol{y}|\\boldsymbol{x})$ 是似然，$p(\\boldsymbol{x})$ 是先验概率分布。\n\n先验信息是状态 $\\boldsymbol{x}$ 是一个高斯随机变量，其均值为 $\\boldsymbol{x}_b$（背景状态），协方差矩阵为 $\\boldsymbol{B}$。其概率密度函数为：\n$$\np(\\boldsymbol{x}) \\propto \\exp\\left(-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{x}_b)^\\top \\boldsymbol{B}^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_b)\\right)\n$$\n\n真实状态 $\\boldsymbol{x}$ 与观测 $\\boldsymbol{y}$ 之间的关系由 $\\boldsymbol{y} = \\boldsymbol{h}(\\boldsymbol{x}) + \\boldsymbol{\\epsilon}$ 给出，其中 $\\boldsymbol{\\epsilon}$ 代表观测误差。误差被假定为零均值高斯分布，协方差矩阵为 $\\boldsymbol{R}$。因此，给定状态 $\\boldsymbol{x}$，观测到 $\\boldsymbol{y}$ 的似然为：\n$$\np(\\boldsymbol{y}|\\boldsymbol{x}) \\propto \\exp\\left(-\\frac{1}{2}(\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x}))^\\top \\boldsymbol{R}^{-1} (\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x}))\\right)\n$$\n其中非线性观测算子为 $\\boldsymbol{h}(\\boldsymbol{x}) = \\mathrm{clip}\\!\\left(\\boldsymbol{A}\\boldsymbol{x};\\, \\boldsymbol{\\ell}, \\boldsymbol{u}\\right)$。\n\n结合先验和似然，后验分布为：\n$$\np(\\boldsymbol{x}|\\boldsymbol{y}) \\propto \\exp\\left(-\\frac{1}{2} \\left[ (\\boldsymbol{x} - \\boldsymbol{x}_b)^\\top \\boldsymbol{B}^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_b) + (\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x}))^\\top \\boldsymbol{R}^{-1} (\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x})) \\right] \\right)\n$$\n最大化后验概率等价于最小化其负对数。我们将变分代价函数 $J(\\boldsymbol{x})$ 定义为指数中项的一半：\n$$\nJ(\\boldsymbol{x}) = \\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{x}_b)^\\top \\boldsymbol{B}^{-1} (\\boldsymbol{x} - \\boldsymbol{x}_b) + \\frac{1}{2}(\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x}))^\\top \\boldsymbol{R}^{-1} (\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x}))\n$$\n这是需要最小化的代价函数。第一项 $J_b(\\boldsymbol{x})$ 惩罚对背景状态的偏离，而第二项 $J_o(\\boldsymbol{x})$ 惩罚模型预测的观测与实际观测之间的不匹配。\n\n**代价函数的梯度：**\n梯度 $\\nabla J(\\boldsymbol{x})$ 通过对 $J(\\boldsymbol{x})$ 关于 $\\boldsymbol{x}$ 求导得到：\n$$\n\\nabla J(\\boldsymbol{x}) = \\nabla J_b(\\boldsymbol{x}) + \\nabla J_o(\\boldsymbol{x})\n$$\n背景项的梯度是：\n$$\n\\nabla J_b(\\boldsymbol{x}) = \\boldsymbol{B}^{-1}(\\boldsymbol{x} - \\boldsymbol{x}_b)\n$$\n观测项的梯度需要使用链式法则。令 $\\boldsymbol{d}(\\boldsymbol{x}) = \\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x})$。$\\boldsymbol{d}(\\boldsymbol{x})$ 的雅可比矩阵是 $-\\boldsymbol{J}_h(\\boldsymbol{x})$，其中 $\\boldsymbol{J}_h(\\boldsymbol{x})$ 是 $\\boldsymbol{h}(\\boldsymbol{x})$ 的雅可比矩阵。\n$$\n\\nabla J_o(\\boldsymbol{x}) = (\\nabla \\boldsymbol{d}(\\boldsymbol{x}))^\\top \\boldsymbol{R}^{-1} \\boldsymbol{d}(\\boldsymbol{x}) = (-\\boldsymbol{J}_h(\\boldsymbol{x}))^\\top \\boldsymbol{R}^{-1} (\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x})) = -\\boldsymbol{J}_h(\\boldsymbol{x})^\\top \\boldsymbol{R}^{-1}(\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x}))\n$$\n$\\boldsymbol{h}(\\boldsymbol{x})$ 的雅可比矩阵由内部掩码规则定义。令 $z_i = (\\boldsymbol{A}\\boldsymbol{x})_i$。$\\mathrm{clip}(z_i; \\ell_i, u_i)$ 关于 $z_i$ 的导数在 $\\ell_i  z_i  u_i$ 时为 $1$，否则为 $0$。令 $\\boldsymbol{M}(\\boldsymbol{x})$ 为一个对角矩阵，其第 $i$ 个对角元素是这个指示函数。那么，根据链式法则，$\\boldsymbol{J}_h(\\boldsymbol{x}) = \\boldsymbol{M}(\\boldsymbol{x})\\boldsymbol{A}$。\n合并各项，完整的梯度是：\n$$\n\\nabla J(\\boldsymbol{x}) = \\boldsymbol{B}^{-1}(\\boldsymbol{x} - \\boldsymbol{x}_b) - \\boldsymbol{J}_h(\\boldsymbolx)^\\top \\boldsymbol{R}^{-1}(\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x}))\n$$\n\n**Gauss-Newton 近似 Hessian 矩阵：**\n精确的 Hessian 矩阵是 $\\nabla^2 J(\\boldsymbol{x}) = \\nabla^2 J_b(\\boldsymbol{x}) + \\nabla^2 J_o(\\boldsymbol{x})$。背景项的 Hessian 矩阵就是 $\\boldsymbol{B}^{-1}$。观测项的 Hessian 矩阵是：\n$$\n\\nabla^2 J_o(\\boldsymbol{x}) = \\boldsymbol{J}_h(\\boldsymbol{x})^\\top \\boldsymbol{R}^{-1} \\boldsymbol{J}_h(\\boldsymbol{x}) - \\nabla\\left[\\boldsymbol{J}_h(\\boldsymbol{x})^\\top\\right]\\boldsymbol{R}^{-1}(\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x}))\n$$\nGauss-Newton 近似忽略了第二项，该项涉及 $\\boldsymbol{h}$ 的二阶导数，并由观测残差 $\\boldsymbol{y} - \\boldsymbol{h}(\\boldsymbol{x})$ 缩放。当模型接近线性或解处的残差很小时，这种近似是准确的。由此得到的近似 Hessian 矩阵 $\\boldsymbol{H}_{\\text{GN}}$ 是：\n$$\n\\boldsymbol{H}_{\\text{GN}}(\\boldsymbol{x}) = \\boldsymbol{B}^{-1} + \\boldsymbol{J}_h(\\boldsymbol{x})^\\top \\boldsymbol{R}^{-1} \\boldsymbol{J}_h(\\boldsymbol{x})\n$$\n该 Hessian 矩阵保证是对称正定的（因为 $\\boldsymbol{B}^{-1}$ 和 $\\boldsymbol{R}^{-1}$ 是对称正定的，且 $\\boldsymbol{J}_h^\\top \\boldsymbol{R}^{-1} \\boldsymbol{J}_h$ 是半正定的），这确保了计算出的搜索方向是下降方向。\n\n### 任务2：算法设计与分析\n\n三种最小化方案代表了每次迭代的计算成本与收敛鲁棒性之间的不同权衡。\n\n**方案 $S_0$（冻结线性化）：** 这是最简单的方法。它在初始猜测点 $\\boldsymbol{x}^{(0)}$ 对观测算子 $\\boldsymbol{h}(\\boldsymbol{x})$ 进行一次线性化。得到的代价函数是纯二次的，其最小化子可以通过求解一个 $3 \\times 3$ 的线性系统找到。这种方法计算成本低，但其准确性高度依赖于初始猜测的质量。如果 $\\boldsymbol{x}^{(0)}$ 远离真实解，或者处于一个活动饱和集与解处的饱和集不同的区域，线性化可能会很差，导致求解器收敛到一个次优点。\n\n**方案 $S_1$（完全再线性化）：** 这是标准的 Gauss-Newton 方法。在每次迭代 $k$ 中，它在当前迭代点 $\\boldsymbol{x}^{(k)}$ 处重新线性化 $\\boldsymbol{h}(\\boldsymbol{x})$，构造新的 Gauss-Newton 系统，并计算一个搜索方向。使用回溯线搜索来确保每一步都能稳健地减小真实的非线性代价函数 $J(\\boldsymbol{x})$。这种方法是三种方法中最鲁棒的，因为它不断更新代价函数的局部二次模型。然而，它也是每次迭代计算成本最高的，因为它需要在每一步重新评估雅可比矩阵并构造 Hessian 矩阵。\n\n**方案 $S_2$（自适应再线性化）：** 该方案是一种混合方法，旨在平衡 $S_0$ 和 $S_1$ 的权衡。它认识到雅可比矩阵 $\\boldsymbol{J}_h(\\boldsymbol{x})$ 是分段常数，仅当 $\\boldsymbol{A}\\boldsymbol{x}$ 的某个元素穿过饱和边界时才会改变。因此，只有当当前迭代点的“活动集”（饱和分量的集合）相对于上次线性化的点发生变化时，才执行构造 Hessian 矩阵这一计算密集型步骤。在这些更新之间，算法使用一个“陈旧”的 Hessian 矩阵，但继续在当前迭代点计算梯度。如果迭代点保持在一个饱和状态不变的区域内，这可以节省大量计算，同时仍然允许模型在必要时进行调整。与 $S_1$ 一样，它使用回溯线搜索来保证在真实代价函数上的下降。\n\n对三个测试案例的吸引盆分析将阐明这些权衡。在线性情况（测试案例 3）中，所有饱和都未激活，问题是二次的，预计所有三种方法都将从任何起点收敛到唯一的全局最小值。对于非线性情况（1 和 2），代价函数景观更为复杂，我们预计 $S_1$ 将展现出最大的吸引盆，而 $S_0$ 的吸引盆最小。$S_2$ 的性能预计接近 $S_1$，但 Hessian 更新次数可能更少。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a 3D-Var data assimilation problem with a nonlinear observation operator,\n    evaluating three different minimization schedules.\n    \"\"\"\n\n    # --- System Constants ---\n    X_B = np.array([-1.0, 0.0, 1.0])\n    B_INV = np.diag([0.25, 1.0, 1.0/9.0])\n    R_INV = np.diag([1.0/0.09, 1.0/0.04, 1.0/0.16])\n    A = np.array([[1.0, 0.5, 0.0], [0.0, 1.0, 0.3], [0.2, 0.0, 1.0]])\n    X_TRUE = np.array([2.0, -1.5, 0.5])\n\n    # --- Algorithm Parameters ---\n    GRID_VALS = [-1.5, -0.75, 0.0, 0.75, 1.5]\n    MAX_ITER = 50\n    TOL_STEP = 1e-8\n    TOL_COST = 1e-6\n    ARMIJO_GAMMA = 1e-4\n\n    # --- Test Case Definitions ---\n    test_cases_params = [\n        {'l': np.array([0.0, 0.0, 0.0]), 'u': np.array([1.0, 1.2, 0.8])}, # Case 1\n        {'l': np.array([0.0, 0.0, 0.0]), 'u': np.array([0.6, 0.4, 0.5])}, # Case 2\n        {'l': np.array([-10.0, -10.0, -10.0]), 'u': np.array([10.0, 10.0, 10.0])} # Case 3\n    ]\n\n    # --- Core Mathematical Functions ---\n    def h_op(x, A_mat, l_vec, u_vec):\n        z = A_mat @ x\n        return np.clip(z, l_vec, u_vec)\n\n    def get_mask(x, A_mat, l_vec, u_vec):\n        z = A_mat @ x\n        return ((z > l_vec)  (z  u_vec))\n\n    def j_h_op(x, A_mat, l_vec, u_vec):\n        mask = get_mask(x, A_mat, l_vec, u_vec).astype(float)\n        M = np.diag(mask)\n        return M @ A_mat\n\n    def cost_function(x, y, x_b, B_inv, R_inv, A_mat, l_vec, u_vec):\n        h_x = h_op(x, A_mat, l_vec, u_vec)\n        term_b = 0.5 * (x - x_b).T @ B_inv @ (x - x_b)\n        term_o = 0.5 * (y - h_x).T @ R_inv @ (y - h_x)\n        return term_b + term_o\n\n    def grad_cost_function(x, y, x_b, B_inv, R_inv, A_mat, l_vec, u_vec):\n        h_x = h_op(x, A_mat, l_vec, u_vec)\n        Jh_x = j_h_op(x, A_mat, l_vec, u_vec)\n        grad_b = B_inv @ (x - x_b)\n        grad_o = -Jh_x.T @ R_inv @ (y - h_x)\n        return grad_b + grad_o\n\n    # --- Solver Implementations for Schedules S0, S1, S2 ---\n    def solve_s0(x0, y, x_b, B_inv, R_inv, A_mat, l_vec, u_vec):\n        J0 = j_h_op(x0, A_mat, l_vec, u_vec)\n        h0 = h_op(x0, A_mat, l_vec, u_vec)\n        c0 = h0 - J0 @ x0\n        H_sys = B_inv + J0.T @ R_inv @ J0\n        rhs = B_inv @ x_b + J0.T @ R_inv @ (y - c0)\n        try:\n            x_final = np.linalg.solve(H_sys, rhs)\n        except np.linalg.LinAlgError:\n            x_final = x0 # Return initial guess if solver fails\n        return x_final\n\n    def solve_s1(x0, y, x_b, B_inv, R_inv, A_mat, l_vec, u_vec):\n        xk = np.copy(x0)\n        args_cost = (y, x_b, B_inv, R_inv, A_mat, l_vec, u_vec)\n\n        for _ in range(MAX_ITER):\n            grad_k = grad_cost_function(xk, *args_cost)\n            Jhk = j_h_op(xk, A_mat, l_vec, u_vec)\n            H_gn = B_inv + Jhk.T @ R_inv @ Jhk\n            \n            try:\n                pk = np.linalg.solve(H_gn, -grad_k)\n            except np.linalg.LinAlgError:\n                break\n\n            if np.linalg.norm(pk) = TOL_STEP:\n                break\n\n            alpha = 1.0\n            Jk = cost_function(xk, *args_cost)\n            slope = grad_k.T @ pk\n\n            for _ in range(10): # Max 10 backtracking steps\n                x_new = xk + alpha * pk\n                J_new = cost_function(x_new, *args_cost)\n                if J_new = Jk + ARMIJO_GAMMA * alpha * slope:\n                    break\n                alpha /= 2.0\n            else: # Line search failed\n                break\n            \n            xk = x_new\n\n        return xk, cost_function(xk, *args_cost)\n\n    def solve_s2(x0, y, x_b, B_inv, R_inv, A_mat, l_vec, u_vec):\n        xk = np.copy(x0)\n        args_cost = (y, x_b, B_inv, R_inv, A_mat, l_vec, u_vec)\n        \n        mask_lin = get_mask(xk, A_mat, l_vec, u_vec)\n        j_lin = j_h_op(xk, A_mat, l_vec, u_vec)\n        H_gn_lin = B_inv + j_lin.T @ R_inv @ j_lin\n        \n        for _ in range(MAX_ITER):\n            grad_k = grad_cost_function(xk, *args_cost)\n            \n            try:\n                pk = np.linalg.solve(H_gn_lin, -grad_k)\n            except np.linalg.LinAlgError:\n                break\n            \n            if np.linalg.norm(pk) = TOL_STEP:\n                break\n\n            alpha = 1.0\n            Jk = cost_function(xk, *args_cost)\n            slope = grad_k.T @ pk\n            \n            for _ in range(10):\n                x_new = xk + alpha * pk\n                J_new = cost_function(x_new, *args_cost)\n                if J_new = Jk + ARMIJO_GAMMA * alpha * slope:\n                    break\n                alpha /= 2.0\n            else:\n                break\n            \n            xk = x_new\n            mask_new = get_mask(xk, A_mat, l_vec, u_vec)\n\n            if not np.array_equal(mask_new, mask_lin):\n                mask_lin = mask_new\n                j_lin = j_h_op(xk, A_mat, l_vec, u_vec)\n                H_gn_lin = B_inv + j_lin.T @ R_inv @ j_lin\n\n        return xk, cost_function(xk, *args_cost)\n\n    # --- Main Execution Logic ---\n    x0_grid = []\n    for i in GRID_VALS:\n        for j in GRID_VALS:\n            for k in GRID_VALS:\n                x0_grid.append(np.array([i, j, k]))\n    num_guesses = len(x0_grid)\n\n    overall_results = []\n\n    for params in test_cases_params:\n        l, u = params['l'], params['u']\n        y = h_op(X_TRUE, A, l, u)\n        \n        args = (y, X_B, B_INV, R_INV, A, l, u)\n\n        # Baseline: run S1 on all guesses to find the best attainable cost\n        s1_final_costs = [solve_s1(x0, *args)[1] for x0 in x0_grid]\n        j_best = min(s1_final_costs) if s1_final_costs else float('inf')\n\n        success_counts = [0, 0, 0]\n\n        # Evaluate S0\n        for x0 in x0_grid:\n            x_final_s0 = solve_s0(x0, *args)\n            if cost_function(x_final_s0, *args) = j_best + TOL_COST:\n                success_counts[0] += 1\n        \n        # Evaluate S1\n        for cost in s1_final_costs:\n            if cost = j_best + TOL_COST:\n                success_counts[1] += 1\n\n        # Evaluate S2\n        for x0 in x0_grid:\n            _, j_final_s2 = solve_s2(x0, *args)\n            if j_final_s2 = j_best + TOL_COST:\n                success_counts[2] += 1\n\n        fractions = [count / num_guesses for count in success_counts]\n        overall_results.append(fractions)\n\n    # --- Format and Print Final Output ---\n    list_strs = [f\"[{','.join(map(str, res_list))}]\" for res_list in overall_results]\n    print(f\"[{','.join(list_strs)}]\")\n\nsolve()\n```"
        }
    ]
}