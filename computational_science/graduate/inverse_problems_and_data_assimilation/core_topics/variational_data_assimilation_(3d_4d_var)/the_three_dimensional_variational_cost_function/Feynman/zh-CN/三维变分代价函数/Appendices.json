{
    "hands_on_practices": [
        {
            "introduction": "理论学习之后，最好的巩固方式就是亲手实践。本节的第一个练习将引导你把三维变分（3D-Var）资料同化问题看作一个加权最小二乘问题。通过这个练习，你将从第一性原理出发推导最优解，并编写代码来直观地感受背景误差协方差（$B$）和观测误差协方差（$R$）如何共同决定最终的分析场。",
            "id": "3152341",
            "problem": "要求您将三维变分（3DVar）数据同化问题描述为一个加权最小二乘问题，并编写一个程序，为一组测试用例计算其唯一极小值点。目标是严格地将该公式与加权最小二乘联系起来，并量化背景误差协方差和观测误差协方差的相对大小如何影响结果。\n\n从以下基本概念出发：\n- 向量 $v$ 相对于对称正定矩阵 $W$ 的加权平方范数定义为 $\\lVert v \\rVert_{W}^{2} = v^{\\top} W v$。\n- 对于将向量 $x$ 映射到观测值 $Hx$ 的线性模型和数据 $y$，加权最小二乘的目标是最小化一个二次泛函，该泛函是一个数据失配项和一个正则化项的和，每一项都有一个对称正定权重。\n\n在 3DVar 中，分析状态 $x$ 被定义为代价函数\n$$\nJ(x) = \\lVert H x - y \\rVert_{R^{-1}}^{2} + \\lVert x - x_b \\rVert_{B^{-1}}^{2},\n$$\n的极小值点，其中 $x_b$ 是背景（先验）状态，$B$ 是背景误差协方差，$R$ 是观测误差协方差。$B$ 和 $R$ 都是对称正定矩阵。矩阵 $H$ 是一个已知的线性观测算子。\n\n任务：\n1. 基于上述定义和二次型的多元微积分基本原理，通过将梯度设为零，推导出 $J(x)$ 的极小值点 $x$ 的最优性充分必要条件。证明所得的线性系统是对称正定的，因此有唯一解。\n2. 根据您的推导，设计一个仅使用对称正定矩阵的线性系统求解和矩阵乘法来计算极小值点的算法。您的算法不得显式地构造任何矩阵的逆。您的算法必须对任何维度都有效，只要 $B$ 和 $R$ 是对称正定矩阵，且 $H$ 是任何具有兼容维度的实数矩阵。\n3. 实现一个完整的、可运行的程序，在以下测试套件上评估您的算法。对于每个测试，计算使 $J(x)$ 最小化的分析向量 $x$：\n   - 测试 A（理想情况，平衡权重）：状态维度 $n = 2$，观测维度 $m = 2$，\n     $$\n     H = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad\n     x_b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad\n     y = \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix},\\quad\n     B = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad\n     R = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}.\n     $$\n   - 测试 B（背景场主导：小 $B$，大 $R$）：$n = 2$, $m = 2$,\n     $$\n     H = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad\n     x_b = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix},\\quad\n     y = \\begin{bmatrix} 100 \\\\ -100 \\end{bmatrix},\\quad\n     B = \\begin{bmatrix} 0.01  0 \\\\ 0  0.01 \\end{bmatrix},\\quad\n     R = \\begin{bmatrix} 10  0 \\\\ 0  10 \\end{bmatrix}.\n     $$\n   - 测试 C（观测主导：大 $B$，小 $R$）：$n = 2$, $m = 2$,\n     $$\n     H = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad\n     x_b = \\begin{bmatrix} 2 \\\\ 2 \\end{bmatrix},\\quad\n     y = \\begin{bmatrix} 100 \\\\ -100 \\end{bmatrix},\\quad\n     B = \\begin{bmatrix} 100  0 \\\\ 0  100 \\end{bmatrix},\\quad\n     R = \\begin{bmatrix} 0.01  0 \\\\ 0  0.01 \\end{bmatrix}.\n     $$\n   - 测试 D（部分观测，未观测分量由背景场填充）：$n = 3$, $m = 2$,\n     $$\n     H = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\end{bmatrix},\\quad\n     x_b = \\begin{bmatrix} 1 \\\\ 1 \\\\ 5 \\end{bmatrix},\\quad\n     y = \\begin{bmatrix} 10 \\\\ -10 \\end{bmatrix},\\quad\n     B = \\begin{bmatrix} 4  0  0 \\\\ 0  1  0 \\\\ 0  0  0.25 \\end{bmatrix},\\quad\n     R = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix}.\n     $$\n   - 测试 E（相关误差）：$n = 2$, $m = 2$,\n     $$\n     H = \\begin{bmatrix} 1  0 \\\\ 0  1 \\end{bmatrix},\\quad\n     x_b = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad\n     y = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix},\\quad\n     B = \\begin{bmatrix} 2  0.6 \\\\ 0.6  1 \\end{bmatrix},\\quad\n     R = \\begin{bmatrix} 0.5  0.2 \\\\ 0.2  0.5 \\end{bmatrix}.\n     $$\n4. 输出规范：\n   - 您的程序必须生成单行输出，其中包含汇总所有测试结果的、用方括号括起来的逗号分隔列表。\n   - 列表中的每个元素必须是该测试的分析向量 $x$，以列表形式表示，并与上面指定的顺序（测试 A 到 E）相同。\n   - 格式：行内任何地方都没有空格。每个分量打印小数点后恰好 6 位数字。\n   - 所需格式的示例（仅为说明）：$[[x_{1,1},x_{1,2}],\\,[x_{2,1},x_{2,2}],\\dots]$。如果一个状态的维度为 3，它将在外层列表中显示为 $[x_{i,1},x_{i,2},x_{i,3}]$。\n   - 因此，最终输出必须是一个表示浮点数列表的列表的单个字符串，采用固定的 6 位小数格式，例如，对于两个假设的 2 分量情况：$[[0.123456,-7.000000],[1.500000,2.250000]]$。\n\n除了测试套件的定义外，您的解决方案必须是通用的，而不是针对这些特定值进行硬编码。每个测试的输出值必须是浮点数列表，并按规定进行四舍五入和格式化。",
            "solution": "用户提供的问题是有效的。它在科学上基于数据同化和加权最小二乘估计的原理，是适定的，具有唯一解，并且为测试用例提供的所有必要数据都是一致的。\n\n### 第 1 步：最优性条件的推导\n\n目标是找到最小化三维变分（3DVar）代价函数 $J(x)$ 的分析状态向量 $x$。代价函数给定为：\n$$\nJ(x) = \\lVert H x - y \\rVert_{R^{-1}}^{2} + \\lVert x - x_b \\rVert_{B^{-1}}^{2}\n$$\n其中 $x$ 是维度为 $n$ 的状态向量，$x_b$ 是背景状态向量（维度为 $n$），$y$ 是维度为 $m$ 的观测向量，$H$ 是线性观测算子（一个 $m \\times n$ 矩阵），$B$ 是背景误差协方差矩阵（$n \\times n$），$R$ 是观测误差协方差矩阵（$m \\times m$）。$B$ 和 $R$ 都是对称正定（SPD）矩阵，这意味着它们的逆矩阵 $B^{-1}$ 和 $R^{-1}$ 也是对称正定的。\n\n使用加权平方范数的定义 $\\lVert v \\rVert_{W}^{2} = v^{\\top} W v$，我们可以展开代价函数：\n$$\nJ(x) = (H x - y)^{\\top} R^{-1} (H x - y) + (x - x_b)^{\\top} B^{-1} (x - x_b)\n$$\n为了找到 $J(x)$ 的极小值点，这是一个关于 $x$ 的二次凸函数，我们计算它关于 $x$ 的梯度并将其设为零。首先，我们展开这两项：\n$$\n(H x - y)^{\\top} R^{-1} (H x - y) = x^{\\top}H^{\\top}R^{-1}Hx - x^{\\top}H^{\\top}R^{-1}y - y^{\\top}R^{-1}Hx + y^{\\top}R^{-1}y\n$$\n$$\n(x - x_b)^{\\top} B^{-1} (x - x_b) = x^{\\top}B^{-1}x - x^{\\top}B^{-1}x_b - x_b^{\\top}B^{-1}x + x_b^{\\top}B^{-1}x_b\n$$\n由于 $x^{\\top}H^{\\top}R^{-1}y$ 是一个标量，它等于其转置，即 $(x^{\\top}H^{\\top}R^{-1}y)^{\\top} = y^{\\top}(R^{-1})^{\\top}H x$。因为 $R^{-1}$ 是对称的，所以这变成 $y^{\\top}R^{-1}H x$。一个类似的恒等式也适用于包含 $B^{-1}$ 的项。因此，我们可以将 $J(x)$ 写为：\n$$\nJ(x) = x^{\\top}H^{\\top}R^{-1}Hx - 2 y^{\\top}R^{-1}Hx + y^{\\top}R^{-1}y + x^{\\top}B^{-1}x - 2 x_b^{\\top}B^{-1}x + x_b^{\\top}B^{-1}x_b\n$$\n现在，我们使用标准的矩阵微积分恒等式 $\\nabla_x(c^{\\top}x) = c$ 和 $\\nabla_x(x^{\\top}Ax) = (A + A^{\\top})x$ 来计算梯度 $\\nabla_x J(x)$。\n$$\n\\nabla_x J(x) = \\nabla_x(x^{\\top}H^{\\top}R^{-1}Hx) - \\nabla_x(2 y^{\\top}R^{-1}Hx) + \\nabla_x(x^{\\top}B^{-1}x) - \\nabla_x(2 x_b^{\\top}B^{-1}x)\n$$\n矩阵 $H^{\\top}R^{-1}H$ 和 $B^{-1}$ 是对称的。因此，$\\nabla_x(x^{\\top}(H^{\\top}R^{-1}H)x) = 2(H^{\\top}R^{-1}H)x$ 且 $\\nabla_x(x^{\\top}B^{-1}x) = 2B^{-1}x$。线性项给出 $\\nabla_x(2 y^{\\top}R^{-1}Hx) = 2(y^{\\top}R^{-1}H)^{\\top} = 2H^{\\top}(R^{-1})^{\\top}y = 2H^{\\top}R^{-1}y$ 和 $\\nabla_x(2 x_b^{\\top}B^{-1}x) = 2B^{-1}x_b$。将这些组合起来，梯度为：\n$$\n\\nabla_x J(x) = 2 H^{\\top}R^{-1}H x - 2 H^{\\top}R^{-1}y + 2 B^{-1}x - 2 B^{-1}x_b\n$$\n将梯度设为零，得到最小值的必要条件：\n$$\nH^{\\top}R^{-1}H x + B^{-1}x = H^{\\top}R^{-1}y + B^{-1}x_b\n$$\n在左侧提取 $x$ 因子，我们得到分析状态 $x$ 的线性系统：\n$$\n(H^{\\top}R^{-1}H + B^{-1})x = H^{\\top}R^{-1}y + B^{-1}x_b\n$$\n这是最优性的必要条件。为了证明它也是唯一极小值的充分条件，我们必须证明 $J(x)$ 的海森矩阵是正定的。海森矩阵是 $\\nabla_x^2 J(x) = 2(H^{\\top}R^{-1}H + B^{-1})$。\n矩阵 $H^{\\top}R^{-1}H + B^{-1}$ 是对称的，因为 $B^{-1}$ 是对称的，且 $(H^{\\top}R^{-1}H)^{\\top} = H^{\\top}(R^{-1})^{\\top}(H^{\\top})^{\\top} = H^{\\top}R^{-1}H$。为了证明其正定性，考虑任何非零向量 $v \\in \\mathbb{R}^n$：\n$$\nv^{\\top}(H^{\\top}R^{-1}H + B^{-1})v = v^{\\top}H^{\\top}R^{-1}Hv + v^{\\top}B^{-1}v = (Hv)^{\\top}R^{-1}(Hv) + v^{\\top}B^{-1}v\n$$\n因为 $R$ 是对称正定的，$R^{-1}$ 也是对称正定的，所以 $(Hv)^{\\top}R^{-1}(Hv) \\ge 0$。因为 $B$ 是对称正定的，$B^{-1}$ 也是对称正定的，所以对于 $v \\neq 0$，有 $v^{\\top}B^{-1}v > 0$。因此，对于所有 $v \\neq 0$，有 $v^{\\top}(H^{\\top}R^{-1}H + B^{-1})v > 0$。海森矩阵是正定的，这意味着 $J(x)$ 是一个严格凸函数，并有唯一的全局最小值。因此，上面推导出的线性系统对于 $x$ 有唯一解。\n\n### 第 2 步：算法设计\n\n推导出的线性系统涉及矩阵的逆 $B^{-1}$ 和 $R^{-1}$。一个数值上稳定且高效的算法应避免显式计算矩阵的逆。一种替代的、等价的公式，通常称为观测空间解，可以避免这种情况。当观测空间维度 $m$ 远小于状态空间维度 $n$ 时，这种公式尤其有利。我们可以使用 Sherman-Morrison-Woodbury 恒等式来推导它，但更直接的代数验证如下。解可以表示为对背景状态的增量：\n$$\nx = x_b + \\delta x\n$$\n其中，数据同化理论中的一个著名结果给出了增量 $\\delta x$ 为：\n$$\n\\delta x = B H^{\\top} (H B H^{\\top} + R)^{-1} (y - H x_b)\n$$\n这个公式计算分析状态 $x$ 时无需构造 $B^{-1}$ 或 $R^{-1}$。唯一需要的矩阵逆是 $(H B H^{\\top} + R)$ 的逆。矩阵 $H B H^{\\top} + R$ 是一个 $m \\times m$ 矩阵，保证是对称正定的，因为 $R$ 是对称正定的，而 $HBH^{\\top}$ 是对称半正定的。我们通过求解一个线性系统，而不是显式地对这个矩阵求逆。\n\n算法如下：\n1.  计算新息（或观测减预报残差）向量：$d = y - H x_b$。\n2.  计算矩阵乘积 $M_1 = H @ B @ H^{\\top}$。\n3.  构造用于线性系统求解器的矩阵：$M_2 = M_1 + R$。\n4.  求解 $m \\times m$ 线性系统 $M_2 v = d$ 以得到向量 $v$。这里，$v = (H B H^{\\top} + R)^{-1} d$。\n5.  计算状态增量 $\\delta x = B @ H^{\\top} @ v$。\n6.  计算最终分析状态：$x = x_b + \\delta x$。\n\n该算法仅依赖于矩阵乘法和求解一个带有对称正定矩阵的线性系统，完全符合问题的约束条件。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve_3dvar(H: np.ndarray, xb: np.ndarray, y: np.ndarray, B: np.ndarray, R: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Computes the 3D-Var analysis state x that minimizes the cost function J(x).\n\n    The analysis state x is given by the formula:\n    x = xb + B @ H.T @ inv(H @ B @ H.T + R) @ (y - H @ xb)\n    \n    This implementation avoids explicit matrix inversion by solving a linear system.\n\n    Args:\n        H: Observation operator matrix.\n        xb: Background state vector.\n        y: Observation vector.\n        B: Background error covariance matrix.\n        R: Observation error covariance matrix.\n\n    Returns:\n        The analysis state vector x.\n    \"\"\"\n    # 1. Compute the innovation vector\n    d = y - H @ xb\n\n    # 2. Compute the matrix for the linear system in observation space\n    # This matrix is H*B*H' + R\n    HBHt = H @ B @ H.T\n    M = HBHt + R\n    \n    # 3. Solve the linear system M*v = d for v\n    # This is equivalent to v = inv(H*B*H' + R) * d\n    # Since M is symmetric positive definite, we can use a specialized solver.\n    # However, linalg.solve is general and robust.\n    v = linalg.solve(M, d, assume_a='pos')\n\n    # 4. Compute the state increment\n    # This is B*H'*v\n    delta_x = B @ H.T @ v\n\n    # 5. Compute the final analysis state\n    x = xb + delta_x\n\n    return x\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for the 3D-Var solver.\n    \"\"\"\n    # Test cases defined in the problem statement\n    test_cases = [\n        # Test A: Happy path, balanced weights\n        {\n            \"H\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"xb\": np.array([0.0, 0.0]),\n            \"y\": np.array([1.0, -2.0]),\n            \"B\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"R\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n        },\n        # Test B: Background-dominated\n        {\n            \"H\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"xb\": np.array([2.0, 2.0]),\n            \"y\": np.array([100.0, -100.0]),\n            \"B\": np.array([[0.01, 0.0], [0.0, 0.01]]),\n            \"R\": np.array([[10.0, 0.0], [0.0, 10.0]]),\n        },\n        # Test C: Observation-dominated\n        {\n            \"H\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"xb\": np.array([2.0, 2.0]),\n            \"y\": np.array([100.0, -100.0]),\n            \"B\": np.array([[100.0, 0.0], [0.0, 100.0]]),\n            \"R\": np.array([[0.01, 0.0], [0.0, 0.01]]),\n        },\n        # Test D: Partial observation\n        {\n            \"H\": np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]),\n            \"xb\": np.array([1.0, 1.0, 5.0]),\n            \"y\": np.array([10.0, -10.0]),\n            \"B\": np.array([[4.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.25]]),\n            \"R\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n        },\n        # Test E: Correlated errors\n        {\n            \"H\": np.array([[1.0, 0.0], [0.0, 1.0]]),\n            \"xb\": np.array([0.0, 0.0]),\n            \"y\": np.array([1.0, 2.0]),\n            \"B\": np.array([[2.0, 0.6], [0.6, 1.0]]),\n            \"R\": np.array([[0.5, 0.2], [0.2, 0.5]]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        x_analysis = solve_3dvar(case[\"H\"], case[\"xb\"], case[\"y\"], case[\"B\"], case[\"R\"])\n        results.append(x_analysis)\n\n    # Format the output string as per the requirements\n    # A single line, comma-separated list of lists, no spaces, 6 decimal places.\n    formatted_results = []\n    for res_vec in results:\n        formatted_vec = [f\"{val:.6f}\" for val in res_vec]\n        formatted_results.append(f\"[{','.join(formatted_vec)}]\")\n    \n    final_output = f\"[{','.join(formatted_results)}]\"\n    \n    print(final_output)\n\nsolve()\n```"
        },
        {
            "introduction": "在实际的资料同化应用中，状态向量的维度（$n$）通常非常巨大，以至于显式地构建和存储代价函数的Hessian矩阵是不可行的。第二个练习将带你进入大规模优化的世界，你将学习如何设计一种“无矩阵”算法。具体来说，你将实现一个Hessian-向量乘积算子，并将其嵌入到共轭梯度（CG）迭代求解器中，这是现代变分资料同化系统的核心计算技术之一。",
            "id": "3426291",
            "problem": "考虑定义在状态向量 $x \\in \\mathbb{R}^n$ 上的三维变分 (3D-Var) 数据同化代价函数\n$$\nJ(x) \\equiv \\tfrac{1}{2}\\,(x - x_b)^\\top B^{-1} (x - x_b) + \\tfrac{1}{2}\\,\\bigl(Hx - y\\bigr)^\\top R^{-1} \\bigl(Hx - y\\bigr),\n$$\n其中 $x_b \\in \\mathbb{R}^n$ 是背景场状态，$y \\in \\mathbb{R}^m$ 是观测向量，$B \\in \\mathbb{R}^{n \\times n}$ 是对称正定的背景场误差协方差矩阵，$R \\in \\mathbb{R}^{m \\times m}$ 是对称正定的观测误差协方差矩阵，$H \\in \\mathbb{R}^{m \\times n}$ 是线性观测算子。假设所有矩阵都是足够良态的，以确保在双精度算术下的数值稳定性。\n\n您的任务是设计一个无矩阵算法，通过求解在 $x_b$ 附近线性化的一阶最优性条件，来计算使 $J(x_b + \\delta x)$ 最小化的分析增量 $\\delta x \\in \\mathbb{R}^n$。请使用以下原则作为您的基本依据：\n\n- 可微标量泛函的梯度由其对任意方向的一次变分定义。\n- Hessian-向量积定义为梯度对任意向量的方向导数。\n- 欧几里得内积中伴随算子的线性代数恒等式以及线性映射的基本微分法则。\n\n您必须：\n\n1. 从上面 $J(x)$ 的定义和所列原则出发，推导出 $J$ 在 $x_b$ 处的 Hessian-向量积的无矩阵表达式，即一个将任意向量 $v \\in \\mathbb{R}^n$ 映射到 $H_J[v] \\in \\mathbb{R}^n$ 的过程，而无需显式地构建任何稠密 Hessian 矩阵。您的推导应仅依赖于将 $B^{-1}$、$R^{-1}$、$H$ 和 $H^\\top$ 应用于向量。\n\n2. 实现一个共轭梯度 (CG) 方法，该方法仅使用您的 Hessian-向量积算子和从 $x_b$ 处的线性化一阶最优性条件获得的右端项，来计算最小化 $J(x_b + \\delta x)$ 的增量 $\\delta x$。使用零初始猜测，绝对或相对残差容差最多为 $10^{-10}$，最大迭代次数至少为 100 次。不要在 CG 内部显式地构建 Hessian 矩阵。\n\n3. 为了验证，对于每个测试用例，显式地构建与线性化一阶条件相关的稠密线性系统，并使用稠密线性求解器直接求解，以获得参考增量 $\\delta x_{\\mathrm{ref}}$。使用相对误差将您的无矩阵 CG 增量 $\\delta x_{\\mathrm{cg}}$ 与 $\\delta x_{\\mathrm{ref}}$进行比较\n$$\n\\varepsilon \\equiv \\frac{\\lVert \\delta x_{\\mathrm{cg}} - \\delta x_{\\mathrm{ref}} \\rVert_2}{\\max\\bigl(\\lVert \\delta x_{\\mathrm{ref}} \\rVert_2,\\, 1\\bigr)}.\n$$\n\n4. 通过求解线性系统来实现所有与 $B$ 和 $R$ 相关的线性求解（即，通过求解线性系统来应用 $B^{-1}$ 和 $R^{-1}$），而不是在 CG 迭代内部进行显式矩阵求逆。您只能在单独的稠密参考路径中构建显式逆矩阵。\n\n提供了以下测试套件。每个测试用例都提供了矩阵 $B$、$R$、$H$ 以及向量 $x_b$、$y$，并指定了所有数值条目。所有数字均以实数值单位给出；不需要物理单位。\n\n- 测试用例 1 (正常路径，中等各向异性):\n  - $n = 5$, $m = 3$。\n  - $B = \\mathrm{diag}(1.0, 2.0, 3.0, 4.0, 5.0)$。\n  - $R = \\mathrm{diag}(0.5, 0.7, 1.1)$。\n  - $H = \\begin{bmatrix}\n    1.0  0.0  0.0  0.0  0.0 \\\\\n    0.0  1.0  1.0  0.0  0.0 \\\\\n    0.0  0.0  0.0  1.0  -1.0\n  \\end{bmatrix}$。\n  - $x_b = \\begin{bmatrix} 0.5 \\\\ -1.0 \\\\ 0.3 \\\\ 0.0 \\\\ 0.2 \\end{bmatrix}$, $y = \\begin{bmatrix} 1.0 \\\\ -0.4 \\\\ 0.1 \\end{bmatrix}$。\n\n- 测试用例 2 (病态条件和由背景场稳定的秩亏观测几何):\n  - $n = 4$, $m = 2$。\n  - $B = \\mathrm{diag}(10^{-3}, 1.0, 10^{2}, 10^{-1})$。\n  - $R = \\mathrm{diag}(10^{-2}, 10^{1})$。\n  - $H = \\begin{bmatrix}\n    1.0  1.0  0.0  0.0 \\\\\n    0.0  0.0  1.0  -1.0\n  \\end{bmatrix}$。\n  - $x_b = \\begin{bmatrix} 1.0 \\\\ -1.0 \\\\ 2.0 \\\\ -2.0 \\end{bmatrix}$, $y = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$。\n\n- 测试用例 3 (边缘情况：零观测算子):\n  - $n = 3$, $m = 2$。\n  - $B = \\mathrm{diag}(2.0, 2.0, 2.0)$。\n  - $R = \\mathrm{diag}(1.0, 1.0)$。\n  - $H = \\begin{bmatrix}\n    0.0  0.0  0.0 \\\\\n    0.0  0.0  0.0\n  \\end{bmatrix}$。\n  - $x_b = \\begin{bmatrix} 3.0 \\\\ -2.0 \\\\ 1.0 \\end{bmatrix}$, $y = \\begin{bmatrix} 1.0 \\\\ -4.0 \\end{bmatrix}$。\n\n- 测试用例 4 (全观测，非对角背景协方差):\n  - $n = 3$, $m = 3$。\n  - $B = \\begin{bmatrix}\n    2.0  0.5  0.0 \\\\\n    0.5  1.5  0.2 \\\\\n    0.0  0.2  1.0\n  \\end{bmatrix}$。\n  - $R = \\mathrm{diag}(0.05, 0.2, 0.1)$。\n  - $H = I_3$ ($3 \\times 3$ 单位矩阵)。\n  - $x_b = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$, $y = \\begin{bmatrix} 1.0 \\\\ -2.0 \\\\ 0.5 \\end{bmatrix}$。\n\n实现细节和要求：\n\n- 对于每个测试用例，定义在 $x_b$ 处的线性化一阶最优性方程的右端项，并仅使用 Hessian-向量积，通过您的无矩阵共轭梯度方法求解它。对 $\\delta x$ 使用零初始猜测。\n- 独立地，通过显式构建线性化正规算子来形成相应的稠密线性系统，并使用稠密求解器求解参考增量 $\\delta x_{\\mathrm{ref}}$。\n- 对于每个测试用例，计算如上定义的标量相对误差 $\\varepsilon$。\n- 最终输出格式：您的程序应生成单行输出，其中包含四个相对误差的结果，以逗号分隔的列表形式，并用方括号括起来，例如，“[e1,e2,e3,e4]”。每个 $e_k$ 必须表示为标准或科学记数法的小数。\n\n不涉及角度或物理单位；所有输出都是无量纲实数。程序必须是自包含的，并且不得读取任何输入。唯一允许的库是 Python 标准库、NumPy 和 SciPy，但除了用于稠密验证和在 Hessian-向量积中将 $B^{-1}$ 和 $R^{-1}$ 应用于向量所需的基本线性求解之外，您不得调用任何高级线性求解器。",
            "solution": "目标是找到最小化三维变分 (3D-Var) 代价函数 $J(x_b + \\delta x)$ 的分析增量 $\\delta x \\in \\mathbb{R}^n$，其中 $x = x_b + \\delta x$。代价函数由下式给出：\n$$\nJ(x) = \\frac{1}{2}(x - x_b)^\\top B^{-1} (x - x_b) + \\frac{1}{2}(Hx - y)^\\top R^{-1} (Hx - y)\n$$\n将 $x = x_b + \\delta x$ 代入，得到以增量 $\\delta x$ 表示的代价函数：\n$$\nJ(x_b + \\delta x) = \\frac{1}{2}(\\delta x)^\\top B^{-1} (\\delta x) + \\frac{1}{2}(H(x_b + \\delta x) - y)^\\top R^{-1} (H(x_b + \\delta x) - y)\n$$\n令新息向量为 $d = y - Hx_b$。表达式 $H(x_b + \\delta x) - y$ 简化为 $H\\delta x - d$。作为 $\\delta x$ 的泛函的代价函数，我们记为 $\\mathcal{J}(\\delta x)$，是：\n$$\n\\mathcal{J}(\\delta x) = \\frac{1}{2}(\\delta x)^\\top B^{-1} (\\delta x) + \\frac{1}{2}(H\\delta x - d)^\\top R^{-1} (H\\delta x - d)\n$$\n这是关于 $\\delta x$ 的二次泛函。为了找到最小值，我们计算其关于 $\\delta x$ 的梯度并将其设为零。梯度是通过计算对任意扰动 $u \\in \\mathbb{R}^n$ 的一次变分 $\\delta \\mathcal{J}$ 来找到的：\n$$\n\\delta \\mathcal{J}(\\delta x; u) = \\lim_{\\epsilon \\to 0} \\frac{\\mathcal{J}(\\delta x + \\epsilon u) - \\mathcal{J}(\\delta x)}{\\epsilon}\n$$\n展开 $\\mathcal{J}(\\delta x + \\epsilon u)$ 并收集 $\\epsilon$ 阶的项：\n$$\n\\mathcal{J}(\\delta x + \\epsilon u) = \\frac{1}{2}(\\delta x + \\epsilon u)^\\top B^{-1} (\\delta x + \\epsilon u) + \\frac{1}{2}(H(\\delta x + \\epsilon u) - d)^\\top R^{-1} (H(\\delta x + \\epsilon u) - d)\n$$\n$$\n= \\mathcal{J}(\\delta x) + \\epsilon u^\\top B^{-1} \\delta x + \\epsilon (Hu)^\\top R^{-1} (H\\delta x - d) + O(\\epsilon^2)\n$$\n使用伴随性质 $(Hu)^\\top = u^\\top H^\\top$，一次变分为：\n$$\n\\delta \\mathcal{J}(\\delta x; u) = u^\\top \\left( B^{-1}\\delta x + H^\\top R^{-1}(H\\delta x - d) \\right)\n$$\n根据定义，梯度 $\\nabla \\mathcal{J}(\\delta x)$ 是对所有 $u$ 满足 $\\delta \\mathcal{J}(\\delta x; u) = u^\\top \\nabla \\mathcal{J}(\\delta x)$ 的向量。因此：\n$$\n\\nabla \\mathcal{J}(\\delta x) = B^{-1}\\delta x + H^\\top R^{-1}(H\\delta x - d)\n$$\n一阶最优性条件是 $\\nabla \\mathcal{J}(\\delta x) = 0$，这给出：\n$$\nB^{-1}\\delta x + H^\\top R^{-1}H\\delta x - H^\\top R^{-1}d = 0\n$$\n整理后得到最优增量 $\\delta x$ 的线性系统：\n$$\n\\left( B^{-1} + H^\\top R^{-1}H \\right) \\delta x = H^\\top R^{-1}d\n$$\n代入 $d = y - Hx_b$，我们得到最终形式：\n$$\n\\left( B^{-1} + H^\\top R^{-1}H \\right) \\delta x = H^\\top R^{-1}(y - Hx_b)\n$$\n这是一个形如 $\\mathcal{H}\\delta x = b$ 的线性系统，其中 $\\mathcal{H} = B^{-1} + H^\\top R^{-1}H$ 是代价函数 $\\mathcal{J}$ 的 Hessian 矩阵，而 $b = H^\\top R^{-1}(y - Hx_b)$ 是右端项。由于 $B$ 和 $R$ 是对称正定 (SPD) 的，所以 $B^{-1}$ 和 $R^{-1}$ 也是。Hessian 矩阵 $\\mathcal{H}$ 是一个 SPD 矩阵 ($B^{-1}$) 和一个对称半正定矩阵 ($H^\\top R^{-1}H$) 的和，这使得 $\\mathcal{H}$ 本身也是 SPD 的，并保证了唯一解的存在。\n\n该问题要求使用共轭梯度 (CG) 方法的无矩阵途径。此方法需要一个函数，该函数能为任意向量 $v \\in \\mathbb{R}^n$ 计算 Hessian-向量积 $\\mathcal{H}v$，而无需显式地构建矩阵 $\\mathcal{H}$。\n\nHessian-向量积 $\\mathcal{H}v$ 是从 Hessian 作为梯度导数的定义推导出来的。Hessian 对向量 $v$ 的作用是梯度在 $v$ 方向上的方向导数。\n$$\n\\mathcal{H}[v] = \\frac{d}{d\\epsilon}\\nabla \\mathcal{J}(\\delta x + \\epsilon v)\\Big|_{\\epsilon=0}\n$$\n由于 $\\nabla\\mathcal{J}$ 对 $\\delta x$ 是线性的，我们有：\n$$\n\\nabla \\mathcal{J}(\\delta x + \\epsilon v) = B^{-1}(\\delta x + \\epsilon v) + H^\\top R^{-1}(H(\\delta x + \\epsilon v) - d)\n$$\n$$\n= \\nabla\\mathcal{J}(\\delta x) + \\epsilon (B^{-1}v + H^\\top R^{-1}Hv)\n$$\n在 $\\epsilon=0$ 处对 $\\epsilon$ 的导数就是 Hessian-向量积：\n$$\n\\mathcal{H}[v] = B^{-1}v + H^\\top R^{-1}Hv\n$$\n这个操作可以作为一系列步骤无矩阵地实现：\n1. 计算 $v_1 = Hv$。\n2. 通过求解线性系统 $Rv_2 = v_1$ 来计算 $v_2 = R^{-1}v_1$。\n3. 计算 $v_3 = H^\\top v_2$。\n4. 通过求解线性系统 $Bv_4 = v$ 来计算 $v_4 = B^{-1}v$。\n5. 结果是 $\\mathcal{H}v = v_3 + v_4$。\n\n右端项 $b = H^\\top R^{-1}(y - Hx_b)$ 也是在没有显式矩阵求逆的情况下计算的：\n1. 计算新息 $d = y - Hx_b$。\n2. 通过求解 $Rd_R=d$ 来计算 $d_R = R^{-1}d$。\n3. 结果是 $b = H^\\top d_R$。\n\n共轭梯度算法迭代地求解系统 $\\mathcal{H}\\delta x = b$。从初始猜测 $\\delta x_0 = 0$ 开始，算法按以下步骤进行：\n1.  初始化:\n    $\\delta x \\leftarrow 0$\n    $r \\leftarrow b - \\mathcal{H}(\\delta x) = b$\n    $p \\leftarrow r$\n    $rs_{\\text{old}} \\leftarrow r^\\top r$\n2.  对 $k=0, 1, 2, \\dots$ 进行迭代，直到收敛：\n    a. 计算 $\\mathcal{H}p$。\n    b. $\\alpha \\leftarrow rs_{\\text{old}} / (p^\\top \\mathcal{H}p)$。\n    c. $\\delta x \\leftarrow \\delta x + \\alpha p$。\n    d. $r \\leftarrow r - \\alpha \\mathcal{H}p$。\n    e. $rs_{\\text{new}} \\leftarrow r^\\top r$。\n    f. 如果 $\\sqrt{rs_{\\text{new}}}$ 低于指定的容差，则停止。\n    g. $p \\leftarrow r + (rs_{\\text{new}} / rs_{\\text{old}}) p$。\n    h. $rs_{\\text{old}} \\leftarrow rs_{\\text{new}}$。\n\n为了验证，显式地构建稠密线性系统。计算矩阵 $B^{-1}$ 和 $R^{-1}$，然后是稠密 Hessian 矩阵 $\\mathcal{H}_{\\text{dense}} = B^{-1} + H^\\top R^{-1} H$。右端项 $b_{\\text{dense}}$ 也以类似方式计算。然后通过使用直接求解器求解 $\\mathcal{H}_{\\text{dense}}\\delta x_{\\text{ref}} = b_{\\text{dense}}$ 来找到参考解 $\\delta x_{\\text{ref}}$。相对误差 $\\varepsilon = \\lVert \\delta x_{\\mathrm{cg}} - \\delta x_{\\mathrm{ref}} \\rVert_2 / \\max(\\lVert \\delta x_{\\mathrm{ref}} \\rVert_2, 1)$ 量化了无矩阵 CG 解的准确性。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the 3D-Var data assimilation problem for a suite of test cases.\n\n    For each case, it computes the analysis increment using a custom matrix-free\n    Conjugate Gradient (CG) solver. It then validates this result against a\n    reference solution obtained from a direct dense linear system solve.\n    Finally, it computes and prints the relative error between the two solutions.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: happy path, moderate anisotropy\n        {\n            \"n\": 5, \"m\": 3,\n            \"B\": np.diag([1.0, 2.0, 3.0, 4.0, 5.0]),\n            \"R\": np.diag([0.5, 0.7, 1.1]),\n            \"H\": np.array([\n                [1.0, 0.0, 0.0, 0.0, 0.0],\n                [0.0, 1.0, 1.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 1.0, -1.0]\n            ]),\n            \"x_b\": np.array([0.5, -1.0, 0.3, 0.0, 0.2]),\n            \"y\": np.array([1.0, -0.4, 0.1])\n        },\n        # Case 2: ill-conditioning\n        {\n            \"n\": 4, \"m\": 2,\n            \"B\": np.diag([1e-3, 1.0, 1e2, 1e-1]),\n            \"R\": np.diag([1e-2, 1e1]),\n            \"H\": np.array([\n                [1.0, 1.0, 0.0, 0.0],\n                [0.0, 0.0, 1.0, -1.0]\n            ]),\n            \"x_b\": np.array([1.0, -1.0, 2.0, -2.0]),\n            \"y\": np.array([0.0, 1.0])\n        },\n        # Case 3: zero observation operator\n        {\n            \"n\": 3, \"m\": 2,\n            \"B\": np.diag([2.0, 2.0, 2.0]),\n            \"R\": np.diag([1.0, 1.0]),\n            \"H\": np.zeros((2, 3)),\n            \"x_b\": np.array([3.0, -2.0, 1.0]),\n            \"y\": np.array([1.0, -4.0])\n        },\n        # Case 4: full observation with non-diagonal background covariance\n        {\n            \"n\": 3, \"m\": 3,\n            \"B\": np.array([\n                [2.0, 0.5, 0.0],\n                [0.5, 1.5, 0.2],\n                [0.0, 0.2, 1.0]\n            ]),\n            \"R\": np.diag([0.05, 0.2, 0.1]),\n            \"H\": np.identity(3),\n            \"x_b\": np.array([0.0, 0.0, 0.0]),\n            \"y\": np.array([1.0, -2.0, 0.5])\n        }\n    ]\n\n    def cg_solver(A_matvec, b, x0, tol=1e-10, max_iter=100):\n        \"\"\"\n        Solves the linear system Ax=b using the Conjugate Gradient method.\n\n        Args:\n            A_matvec: A function that computes the matrix-vector product A*v.\n            b: The right-hand-side vector.\n            x0: The initial guess for the solution.\n            tol: The convergence tolerance.\n            max_iter: The maximum number of iterations.\n\n        Returns:\n            The solution vector x.\n        \"\"\"\n        x = x0.copy()\n        r = b - A_matvec(x)\n        p = r.copy()\n        rs_old = np.dot(r, r)\n\n        norm_b = np.linalg.norm(b)\n        stop_criterion = tol * norm_b if norm_b > 0 else tol\n\n        if np.sqrt(rs_old)  stop_criterion:\n            return x\n\n        for i in range(max_iter):\n            Ap = A_matvec(p)\n            alpha = rs_old / np.dot(p, Ap)\n            x += alpha * p\n            r -= alpha * Ap\n            rs_new = np.dot(r, r)\n\n            if np.sqrt(rs_new)  stop_criterion:\n                break\n            \n            p = r + (rs_new / rs_old) * p\n            rs_old = rs_new\n            \n        return x\n\n    results = []\n    for case in test_cases:\n        B, R, H, x_b, y = case[\"B\"], case[\"R\"], case[\"H\"], case[\"x_b\"], case[\"y\"]\n        n = case[\"n\"]\n\n        # --- Matrix-Free CG Path ---\n\n        def hessian_vector_product(v):\n            \"\"\"Matrix-free Hessian-vector product: (B^-1 + H' R^-1 H) v\"\"\"\n            term1 = np.linalg.solve(B, v)\n            Hv = H @ v\n            R_inv_Hv = np.linalg.solve(R, Hv)\n            term2 = H.T @ R_inv_Hv\n            return term1 + term2\n\n        # Compute right-hand side: H' R^-1 (y - H x_b)\n        innovation = y - H @ x_b\n        R_inv_innovation = np.linalg.solve(R, innovation)\n        rhs = H.T @ R_inv_innovation\n\n        # Solve for the increment using CG\n        delta_x_cg = cg_solver(hessian_vector_product, rhs, np.zeros(n), max_iter=100)\n\n        # --- Dense Reference Path ---\n        \n        # Explicitly form the Hessian and RHS\n        B_inv = np.linalg.inv(B)\n        R_inv = np.linalg.inv(R)\n        \n        Hess_dense = B_inv + H.T @ R_inv @ H\n        rhs_dense = H.T @ R_inv @ (y - H @ x_b)\n        \n        # Solve for reference increment\n        delta_x_ref = np.linalg.solve(Hess_dense, rhs_dense)\n\n        # --- Compare and calculate error ---\n        \n        diff_norm = np.linalg.norm(delta_x_cg - delta_x_ref)\n        ref_norm = np.linalg.norm(delta_x_ref)\n        \n        error = diff_norm / max(ref_norm, 1.0)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "掌握了求解方法后，一个更深层次的问题是：我们的同化系统得到的分析结果在多大程度上是可信的？第三个练习旨在探讨一个在评估同化系统时至关重要的概念——“反演犯罪”（inverse crime）。你将设计一个实验，其中用于生成合成观测的算子与用于同化的算子是不同的，以此来量化由于模型不完美而引入的误差，从而更真实地评估分析场的质量。",
            "id": "3426322",
            "problem": "考虑一个具有三维变分 (3DVar) 公式的线性高斯数据同化设置。真实状态 $x^\\mathrm{true} \\in \\mathbb{R}^n$ 通过一个线性观测过程和加性噪声与观测数据相关联。通过最小化一个平衡先验（背景）信息与观测信息的复合目标函数，可以得到一个估计量 $x^\\mathrm{a}$。本问题要求您从第一性原理显式推导 3DVar 估计量，实现一个将用于生成数据的算子与用于同化数据的算子分离开的反“逆犯罪”协议，并量化由算子重用引入的乐观偏差（偏置）。\n\n基本设定：\n- 假设 $x \\in \\mathbb{R}^n$ 上的高斯先验（背景）分布，其均值为 $x^\\mathrm{b} \\in \\mathbb{R}^n$，协方差为 $B \\in \\mathbb{R}^{n \\times n}$，其中 $B$ 是对称正定矩阵。\n- 假设一个带有加性高斯噪声的线性观测模型：数据 $y \\in \\mathbb{R}^m$ 是由真实值通过 $y = H^\\mathrm{train} x^\\mathrm{true} + v$ 生成的，其中 $v \\sim \\mathcal{N}(0, R)$ 且 $R \\in \\mathbb{R}^{m \\times m}$ 是对称正定矩阵。\n- 在同化过程中，建模者可能会在估计量中使用一个（可能不匹配的）线性算子 $H^\\mathrm{test} \\in \\mathbb{R}^{m \\times n}$。这种不匹配是故意的，它实现了一个反“逆犯罪”协议，确保合成数据生成器与反演算子不同。\n\n任务：\n1. 从高斯先验和线性观测模型所隐含的高斯似然出发，推导三维变分 (3DVar) 成本函数，该函数为负对数后验（相差一个加性常数）。然后，通过将其梯度置为零，推导出使用 $H^\\mathrm{test}$ 的最小化子 $x^\\mathrm{a}$ 的一阶最优性条件。\n2. 推导一个可计算的线性代数算法，该算法在给定 $x^\\mathrm{b}$、$B$、$R$、$y$ 和一个选定的线性算子 $H$ 的情况下，返回相应的 3DVar 分析 $x^\\mathrm{a}(H)$。\n3. 对于固定的数据集 $y$，定义“逆犯罪”和反“逆犯罪”分析如下：\n   - “逆犯罪”分析: $x^\\mathrm{a}_\\mathrm{train} := x^\\mathrm{a}(H^\\mathrm{train})$。\n   - 反“逆犯罪”分析: $x^\\mathrm{a}_\\mathrm{test} := x^\\mathrm{a}(H^\\mathrm{test})$。\n4. 将算子重用引入的乐观偏差（偏置）量化为非负标量\n   $$ \\Delta := \\lVert x^\\mathrm{a}_\\mathrm{test} - x^\\mathrm{true} \\rVert_2^2 - \\lVert x^\\mathrm{a}_\\mathrm{train} - x^\\mathrm{true} \\rVert_2^2, $$\n   其中 $\\lVert \\cdot \\rVert_2$ 表示欧几里得范数。$\\Delta$ 的严格正值表示重用数据生成算子可能会低估真实状态误差，低估量为 $\\Delta$（以欧几里得范数平方为单位）。如果 $H^\\mathrm{test} = H^\\mathrm{train}$，则 $\\Delta = 0$。\n\n实现要求：\n- 编写一个完整的程序，对于下方的每个测试用例，使用提供的 $x^\\mathrm{true}$、$H^\\mathrm{train}$ 和 $v$ 构建 $y = H^\\mathrm{train} x^\\mathrm{true} + v$，使用相同的 $B$、$R$、$x^\\mathrm{b}$ 和 $y$ 计算 $x^\\mathrm{a}_\\mathrm{train}$ 和 $x^\\mathrm{a}_\\mathrm{test}$，并返回上面定义的标量 $\\Delta$。\n- 程序必须生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如 $[r_1,r_2,\\dots]$），其中每个 $r_i$ 是一个四舍五入到 8 位小数的浮点数。\n- 本问题不涉及物理单位。\n\n测试套件：\n所有测试用例均使用 $n = 4$ 和 $m = 3$。对于每个用例，计算 $y = H^\\mathrm{train} x^\\mathrm{true} + v$，然后使用 $H^\\mathrm{train}$ 和 $H^\\mathrm{test}$ 进行两种分析。\n\n- 用例 1（理想情况，轻度不匹配）：\n  - $x^\\mathrm{true} = \\begin{bmatrix} 1.0 \\\\ -0.5 \\\\ 0.25 \\\\ 2.0 \\end{bmatrix}$，$x^\\mathrm{b} = \\begin{bmatrix} 0.8 \\\\ -0.4 \\\\ 0.0 \\\\ 1.5 \\end{bmatrix}$。\n  - $B = \\mathrm{diag}\\big(\\,0.5,\\,0.2,\\,0.1,\\,0.3\\,\\big)$，$R = \\mathrm{diag}\\big(\\,0.05,\\,0.1,\\,0.2\\,\\big)$。\n  - $H^\\mathrm{train} = \\begin{bmatrix}\n      1.0  0.0  0.5  0.0 \\\\\n      0.0  1.0  0.0  -0.5 \\\\\n      0.2  0.0  0.3  1.0\n    \\end{bmatrix}$，\n    $H^\\mathrm{test} = \\begin{bmatrix}\n      0.9  0.0  0.6  0.0 \\\\\n      0.0  1.05  0.0  -0.45 \\\\\n      0.25  0.0  0.25  0.95\n    \\end{bmatrix}$。\n  - $v = \\begin{bmatrix} 0.02 \\\\ -0.01 \\\\ 0.03 \\end{bmatrix}$。\n\n- 用例 2（“逆犯罪”基线，无不匹配）：\n  - 与用例 1 中相同的 $x^\\mathrm{true}$、$x^\\mathrm{b}$、$B$、$R$、$H^\\mathrm{train}$ 和 $v$。\n  - $H^\\mathrm{test} = H^\\mathrm{train}$。\n\n- 用例 3（较大不匹配）：\n  - 与用例 1 中相同的 $x^\\mathrm{true}$、$x^\\mathrm{b}$、$B$、$R$、$H^\\mathrm{train}$ 和 $v$。\n  - $H^\\mathrm{test} = \\begin{bmatrix}\n      1.2  -0.1  0.0  0.1 \\\\\n      0.1  0.8  0.2  -0.6 \\\\\n      0.0  0.0  0.5  1.2\n    \\end{bmatrix}$。\n\n- 用例 4（观测主导机制，小噪声，中度不匹配）：\n  - $x^\\mathrm{true} = \\begin{bmatrix} 1.0 \\\\ -0.5 \\\\ 0.25 \\\\ 2.0 \\end{bmatrix}$，$x^\\mathrm{b} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$。\n  - $B = \\mathrm{diag}\\big(\\,5.0,\\,4.0,\\,3.0,\\,2.0\\,\\big)$，$R = \\mathrm{diag}\\big(\\,0.01,\\,0.02,\\,0.03\\,\\big)$。\n  - $H^\\mathrm{train} = \\begin{bmatrix}\n      1.0  0.0  0.5  0.0 \\\\\n      0.0  1.0  0.0  -0.5 \\\\\n      0.2  0.0  0.3  1.0\n    \\end{bmatrix}$，\n    $H^\\mathrm{test} = \\begin{bmatrix}\n      1.05  0.0  0.45  0.0 \\\\\n      0.0  0.95  0.0  -0.55 \\\\\n      0.15  0.05  0.35  0.9\n    \\end{bmatrix}$。\n  - $v = \\begin{bmatrix} 0.005 \\\\ -0.002 \\\\ 0.003 \\end{bmatrix}$。\n\n- 用例 5（背景主导机制，大观测噪声，轻度不匹配）：\n  - $x^\\mathrm{true} = \\begin{bmatrix} 1.0 \\\\ -0.5 \\\\ 0.25 \\\\ 2.0 \\end{bmatrix}$，$x^\\mathrm{b} = \\begin{bmatrix} 0.95 \\\\ -0.45 \\\\ 0.2 \\\\ 1.9 \\end{bmatrix}$。\n  - $B = \\mathrm{diag}\\big(\\,0.05,\\,0.02,\\,0.01,\\,0.03\\,\\big)$，$R = \\mathrm{diag}\\big(\\,5.0,\\,2.0,\\,1.0\\,\\big)$。\n  - $H^\\mathrm{train} = \\begin{bmatrix}\n      1.0  0.0  0.5  0.0 \\\\\n      0.0  1.0  0.0  -0.5 \\\\\n      0.2  0.0  0.3  1.0\n    \\end{bmatrix}$，\n    $H^\\mathrm{test} = \\begin{bmatrix}\n      1.0  0.0  0.55  0.0 \\\\\n      0.0  0.9  0.0  -0.5 \\\\\n      0.2  0.1  0.25  1.1\n    \\end{bmatrix}$。\n  - $v = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix}$。\n\n您的程序必须将 5 个得到的乐观偏差值 $\\Delta$ 聚合成单行输出，格式为 $[r_1,r_2,r_3,r_4,r_5]$，其中每个 $r_i$ 都四舍五入到 8 位小数。不允许有其他额外输出。",
            "solution": "本问题探讨了在线性高斯背景下三维变分 (3DVar) 数据同化系统的公式化和实现。我们的任务是推导 3DVar 估计量，对其进行数值实现，并量化因犯下“逆犯罪”——即使用相同的算子进行数据生成和同化——而产生的分析性“乐观偏差”。\n\n### 任务 1：3DVar 成本函数和最优性条件的推导\n\n3DVar 分析是给定观测值 $y$ 的情况下，使后验概率密度函数 (PDF) $p(x|y)$ 最大化的状态 $x$。根据 Bayes' theorem，后验概率与似然 $p(y|x)$ 和先验 $p(x)$ 的乘积成正比：\n$$\np(x|y) \\propto p(y|x) p(x)\n$$\n最大化 $p(x|y)$ 等价于最小化其负对数。我们将成本函数 $J(x)$ 定义为负对数后验，并忽略任何加性常数。\n\n**1. 先验分布：** 假设状态向量 $x \\in \\mathbb{R}^n$ 服从高斯先验分布，其均值为 $x^\\mathrm{b}$（背景状态），协方差为 $B$。其概率密度函数为：\n$$\np(x) \\propto \\exp\\left( -\\frac{1}{2} (x - x^\\mathrm{b})^T B^{-1} (x - x^\\mathrm{b}) \\right)\n$$\n\n**2. 似然函数：** 同化系统使用一个线性算子 $H \\in \\mathbb{R}^{m \\times n}$（在问题的符号体系中为 $H^\\mathrm{test}$）来模拟状态 $x$ 和观测值 $y$ 之间的关系。观测模型为 $y = Hx + v$，其中测量误差 $v$ 被假定为零均值高斯噪声，协方差为 $R$，即 $v \\sim \\mathcal{N}(0, R)$。因此，给定状态 $x$ 观测到 $y$ 的似然为：\n$$\np(y|x) \\propto \\exp\\left( -\\frac{1}{2} (y - Hx)^T R^{-1} (y - Hx) \\right)\n$$\n注意，在推导成本函数时，我们使用建模者所假设的算子 $H$，即 $H^\\mathrm{test}$。算子 $H^\\mathrm{train}$ 仅用于生成合成的“真实”观测值。\n\n**3. 后验分布和成本函数：** 结合先验和似然，后验概率密度函数为：\n$$\np(x|y) \\propto \\exp\\left( -\\frac{1}{2} (x - x^\\mathrm{b})^T B^{-1} (x - x^\\mathrm{b}) -\\frac{1}{2} (y - Hx)^T R^{-1} (y - Hx) \\right)\n$$\n3DVar 成本函数 $J(x)$ 是此表达式的负对数（在相差一个常数因子和一个 1/2 的缩放因子的情况下）：\n$$\nJ(x) = \\frac{1}{2} (x - x^\\mathrm{b})^T B^{-1} (x - x^\\mathrm{b}) + \\frac{1}{2} (y - Hx)^T R^{-1} (y - Hx)\n$$\n第一项，$J_b(x) = \\frac{1}{2} (x - x^\\mathrm{b})^T B^{-1} (x - x^\\mathrm{b})$，惩罚与背景状态的偏差，由背景误差协方差的逆加权。第二项，$J_o(x) = \\frac{1}{2} (y - Hx)^T R^{-1} (y - Hx)$，惩罚与观测值的不匹配，由观测误差协方差的逆加权。\n\n**4. 一阶最优性条件：** 分析状态 $x^\\mathrm{a}$ 是凸函数 $J(x)$ 的唯一最小化子。我们通过将 $J(x)$ 对 $x$ 的梯度设为零来找到它。\n使用矩阵微积分的标准法则，对于对称矩阵 $A$，有 $\\nabla_x (z^T A z) = 2Az$，以及 $\\nabla_x (c - Mx)^T A (c - Mx) = 2(-M^T)A(c-Mx)$，我们得到：\n$$\n\\nabla_x J(x) = \\frac{1}{2} \\left[ 2B^{-1}(x - x^\\mathrm{b}) \\right] + \\frac{1}{2} \\left[ 2(-H^T) R^{-1} (y - Hx) \\right]\n$$\n$$\n\\nabla_x J(x) = B^{-1}(x - x^\\mathrm{b}) - H^T R^{-1} (y - Hx)\n$$\n将梯度设为零，得到分析状态 $x^\\mathrm{a}$ 的一阶最优性条件：\n$$\n\\nabla_x J(x^\\mathrm{a}) = B^{-1}(x^\\mathrm{a} - x^\\mathrm{b}) - H^T R^{-1} (y - Hx^\\mathrm{a}) = 0\n$$\n\n### 任务 2：可计算算法的推导\n\n为了找到 $x^\\mathrm{a}$ 的一个可计算表达式，我们求解最优性条件以得到 $x^\\mathrm{a}$。\n重新整理各项，将包含 $x^\\mathrm{a}$ 的项收集起来：\n$$\nB^{-1}x^\\mathrm{a} + H^T R^{-1} Hx^\\mathrm{a} = B^{-1}x^\\mathrm{b} + H^T R^{-1}y\n$$\n$$\n(B^{-1} + H^T R^{-1} H) x^\\mathrm{a} = B^{-1}x^\\mathrm{b} + H^T R^{-1}y\n$$\n这给出了 $x^\\mathrm{a}$ 的解：\n$$\nx^\\mathrm{a} = (B^{-1} + H^T R^{-1} H)^{-1} (B^{-1}x^\\mathrm{b} + H^T R^{-1}y)\n$$\n这种形式需要对一个 $n \\times n$ 矩阵求逆，其中 $n$ 是状态空间的维度。对于大规模系统，这在计算上是不可行的。可以使用 Woodbury 矩阵恒等式或通过代数操作推导出一种更高效的形式。这导出了“卡尔曼增益 (Kalman gain)”形式：\n$$\nx^\\mathrm{a} = x^\\mathrm{b} + K(y - Hx^\\mathrm{b})\n$$\n其中 $K$ 是 Kalman gain 矩阵，由下式给出：\n$$\nK = B H^T (H B H^T + R)^{-1}\n$$\n这个公式在数值上更优，因为它需要对一个 $m \\times m$ 矩阵求逆，其中 $m$ 是观测空间的维度，并且通常 $m \\ll n$。项 $d = y - Hx^\\mathrm{b}$ 是新息（innovation）或离差，即观测值与其背景对应量之间的差异。\n\n给定 $x^\\mathrm{b}$、$B$、$R$、$y$ 和算子 $H$ 的可计算算法如下：\n1.  计算新息向量：$d = y - Hx^\\mathrm{b}$。\n2.  计算新息协方差矩阵：$S = HBH^T + R$。\n3.  对新息协方差矩阵求逆得到 $S^{-1}$。\n4.  计算 Kalman gain：$K = B H^T S^{-1}$。\n5.  计算分析状态：$x^\\mathrm{a} = x^\\mathrm{b} + Kd$。\n\n### 任务 3：“逆犯罪”和反“逆犯罪”分析\n\n本问题通过区分用于数据生成的算子（$H^\\mathrm{train}$）和用于同化的算子（$H^\\mathrm{test}$）来定义一个反“逆犯罪”协议。使用任务 2 中的算法，我们定义：\n-   **“逆犯罪”分析（$x^\\mathrm{a}_\\mathrm{train}$）**：使用生成数据的相同算子计算分析。这是一个理想化的情景，在实践中很少发生。\n-   **反“逆犯罪”分析（$x^\\mathrm{a}_\\mathrm{test}$）**：使用一个可能不匹配的算子计算分析。这更真实地反映了模型存在误差的业务设置。\n两种分析都使用相同的背景状态 $x^\\mathrm{b}$、背景误差协方差 $B$、观测误差协方差 $R$ 和观测向量 $y$ 进行计算，而 $y$ 本身是由 $H^\\mathrm{train}$ 生成的。\n\n### 任务 4：量化乐观偏差\n\n“乐观”偏差 $\\Delta$ 衡量了犯下“逆犯罪”（重用 $H^\\mathrm{train}$）导致对真实分析误差低估的程度。它被定义为两种分析的真实误差的欧几里得范数平方之差：\n$$\n\\Delta = \\lVert x^\\mathrm{a}_\\mathrm{test} - x^\\mathrm{true} \\rVert_2^2 - \\lVert x^\\mathrm{a}_\\mathrm{train} - x^\\mathrm{true} \\rVert_2^2\n$$\n正的 $\\Delta$ 值表示基于不完美模型 $x^\\mathrm{a}_\\mathrm{test}$ 的分析误差大于基于完美模型 $x^\\mathrm{a}_\\mathrm{train}$ 的分析误差。因此，在现实设置中，依赖于“逆犯罪”情景下的误差统计数据，将会对真实误差给出一个过于乐观（即太小）的估计。\n\n为了实现该解决方案，对于每个测试用例，我们首先生成观测向量 $y = H^\\mathrm{train} x^\\mathrm{true} + v$。然后，我们应用任务 2 中的算法两次：一次使用 $H = H^\\mathrm{train}$ 找到 $x^\\mathrm{a}_\\mathrm{train}$，另一次使用 $H = H^\\mathrm{test}$ 找到 $x^\\mathrm{a}_\\mathrm{test}$。最后，我们计算两个误差向量、它们的范数平方，以及它们的差值以求得 $\\Delta$。",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_analysis(x_b, B, R, H, y):\n    \"\"\"\n    Computes the 3DVar analysis state using the Kalman gain formulation.\n\n    Args:\n        x_b (np.ndarray): Background state vector (n,).\n        B (np.ndarray): Background error covariance matrix (n, n).\n        R (np.ndarray): Observation error covariance matrix (m, m).\n        H (np.ndarray): Observation operator (m, n).\n        y (np.ndarray): Observation vector (m,).\n\n    Returns:\n        np.ndarray: Analysis state vector (n,).\n    \"\"\"\n    # Innovation vector\n    d = y - H @ x_b\n\n    # Innovation covariance matrix\n    S = H @ B @ H.T + R\n\n    # Kalman gain matrix\n    # K = B @ H.T @ np.linalg.inv(S) is a direct implementation.\n    # Using solve is more numerically stable, equivalent to K = (solve(S.T, B @ H.T).T).\n    # Since S is symmetric, S.T = S.\n    # We want to find K such that KS = BH^T, so solve S^T K^T = H B^T.\n    # S is symmetric, so S K^T = H B. B is also symmetric.\n    K_T = np.linalg.solve(S, H @ B)\n    K = K_T.T\n\n    # Analysis state\n    x_a = x_b + K @ d\n    \n    return x_a\n\ndef solve():\n    \"\"\"\n    Solves the 3DVar problem for all test cases and computes the optimism bias.\n    \"\"\"\n    test_cases = [\n        # Case 1 (happy path, mild mismatch)\n        {\n            \"x_true\": np.array([1.0, -0.5, 0.25, 2.0]),\n            \"x_b\": np.array([0.8, -0.4, 0.0, 1.5]),\n            \"B\": np.diag([0.5, 0.2, 0.1, 0.3]),\n            \"R\": np.diag([0.05, 0.1, 0.2]),\n            \"H_train\": np.array([\n                [1.0, 0.0, 0.5, 0.0],\n                [0.0, 1.0, 0.0, -0.5],\n                [0.2, 0.0, 0.3, 1.0]\n            ]),\n            \"H_test\": np.array([\n                [0.9, 0.0, 0.6, 0.0],\n                [0.0, 1.05, 0.0, -0.45],\n                [0.25, 0.0, 0.25, 0.95]\n            ]),\n            \"v\": np.array([0.02, -0.01, 0.03])\n        },\n        # Case 2 (inverse-crime baseline, no mismatch)\n        {\n            \"x_true\": np.array([1.0, -0.5, 0.25, 2.0]),\n            \"x_b\": np.array([0.8, -0.4, 0.0, 1.5]),\n            \"B\": np.diag([0.5, 0.2, 0.1, 0.3]),\n            \"R\": np.diag([0.05, 0.1, 0.2]),\n            \"H_train\": np.array([\n                [1.0, 0.0, 0.5, 0.0],\n                [0.0, 1.0, 0.0, -0.5],\n                [0.2, 0.0, 0.3, 1.0]\n            ]),\n            \"H_test\": np.array([  # H_test = H_train\n                [1.0, 0.0, 0.5, 0.0],\n                [0.0, 1.0, 0.0, -0.5],\n                [0.2, 0.0, 0.3, 1.0]\n            ]),\n            \"v\": np.array([0.02, -0.01, 0.03])\n        },\n        # Case 3 (larger mismatch)\n        {\n            \"x_true\": np.array([1.0, -0.5, 0.25, 2.0]),\n            \"x_b\": np.array([0.8, -0.4, 0.0, 1.5]),\n            \"B\": np.diag([0.5, 0.2, 0.1, 0.3]),\n            \"R\": np.diag([0.05, 0.1, 0.2]),\n            \"H_train\": np.array([\n                [1.0, 0.0, 0.5, 0.0],\n                [0.0, 1.0, 0.0, -0.5],\n                [0.2, 0.0, 0.3, 1.0]\n            ]),\n            \"H_test\": np.array([\n                [1.2, -0.1, 0.0, 0.1],\n                [0.1, 0.8, 0.2, -0.6],\n                [0.0, 0.0, 0.5, 1.2]\n            ]),\n            \"v\": np.array([0.02, -0.01, 0.03])\n        },\n        # Case 4 (observation-dominant regime)\n        {\n            \"x_true\": np.array([1.0, -0.5, 0.25, 2.0]),\n            \"x_b\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"B\": np.diag([5.0, 4.0, 3.0, 2.0]),\n            \"R\": np.diag([0.01, 0.02, 0.03]),\n            \"H_train\": np.array([\n                [1.0, 0.0, 0.5, 0.0],\n                [0.0, 1.0, 0.0, -0.5],\n                [0.2, 0.0, 0.3, 1.0]\n            ]),\n            \"H_test\": np.array([\n                [1.05, 0.0, 0.45, 0.0],\n                [0.0, 0.95, 0.0, -0.55],\n                [0.15, 0.05, 0.35, 0.9]\n            ]),\n            \"v\": np.array([0.005, -0.002, 0.003])\n        },\n        # Case 5 (background-dominant regime)\n        {\n            \"x_true\": np.array([1.0, -0.5, 0.25, 2.0]),\n            \"x_b\": np.array([0.95, -0.45, 0.2, 1.9]),\n            \"B\": np.diag([0.05, 0.02, 0.01, 0.03]),\n            \"R\": np.diag([5.0, 2.0, 1.0]),\n            \"H_train\": np.array([\n                [1.0, 0.0, 0.5, 0.0],\n                [0.0, 1.0, 0.0, -0.5],\n                [0.2, 0.0, 0.3, 1.0]\n            ]),\n            \"H_test\": np.array([\n                [1.0, 0.0, 0.55, 0.0],\n                [0.0, 0.9, 0.0, -0.5],\n                [0.2, 0.1, 0.25, 1.1]\n            ]),\n            \"v\": np.array([0.1, -0.2, 0.05])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        x_true = case[\"x_true\"]\n        x_b = case[\"x_b\"]\n        B = case[\"B\"]\n        R = case[\"R\"]\n        H_train = case[\"H_train\"]\n        H_test = case[\"H_test\"]\n        v = case[\"v\"]\n\n        # Generate synthetic observations\n        y = H_train @ x_true + v\n\n        # Compute inverse-crime analysis\n        x_a_train = compute_analysis(x_b, B, R, H_train, y)\n\n        # Compute anti-inverse-crime analysis\n        x_a_test = compute_analysis(x_b, B, R, H_test, y)\n\n        # Compute squared errors\n        error_train_sq = np.sum((x_a_train - x_true)**2)\n        error_test_sq = np.sum((x_a_test - x_true)**2)\n\n        # Compute optimism bias\n        delta = error_test_sq - error_train_sq\n        results.append(delta)\n\n    # Format output as specified\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```"
        }
    ]
}