{
    "hands_on_practices": [
        {
            "introduction": "计算梯度的核心动机之一，是理解模型中哪些参数能被数据有效约束。本练习 () 将正向模型的 Fréchet 导数与可辨识性零空间的概念具体联系起来。通过为一个简化的扩散问题实施伴随方法，我们将在计算中探索如何识别那些对于特定观测设置而言“不可见”的参数扰动，从而深入理解灵敏度分析在反问题中的基础作用。",
            "id": "3419154",
            "problem": "考虑在闭区间 $[0,1]$ 上的一个一维稳态扩散模型，其边界条件为齐次狄利克雷（Dirichlet）边界条件。设区间内部被离散为 $n$ 个等距点，间距为 $h = 1/(n+1)$。将微分算子 $x \\mapsto -x''$ 用标准的二阶有限差分矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 来近似，该矩阵定义为：对于有效索引，$A_{ii} = 2/h^2$ 且 $A_{i,i\\pm 1} = -1/h^2$。设参数向量 $m \\in \\mathbb{R}^n$ 表示一个离散化的源项，状态向量 $x \\in \\mathbb{R}^n$ 是以下线性系统的解\n$$\nA x = m.\n$$\n设 $S \\in \\mathbb{R}^{k \\times n}$ 是一个线性观测算子，它从 $n$ 个内部网格点中选择与 $k$ 个传感器位置对应的 $x$ 的坐标子集。前向映射 $F : \\mathbb{R}^n \\to \\mathbb{R}^k$ 定义为\n$$\nF(m) = S x(m) = S A^{-1} m.\n$$\n定义数据失配代价泛函\n$$\n\\Phi(m) = \\tfrac{1}{2} \\| F(m) - d \\|_2^2,\n$$\n其中 $d \\in \\mathbb{R}^k$ 是一个给定的数据向量，$\\|\\cdot\\|_2$ 表示欧几里得范数。本问题中所有量均为无量纲；不需要物理单位。\n\n你的任务是：\n\n1. 从有限维实希尔伯特空间中弗雷歇（Fréchet）导数的定义出发，推导前向映射 $F$ 在点 $m$ 沿方向 $h \\in \\mathbb{R}^n$ 的弗雷歇导数 $F'(m)[h]$ 的表达式。请直接用扰动 $x_h$ 的线性化状态方程 $A x_h = h$ 和观测算子 $S$ 来表示你的推导。\n\n2. 使用基于拉格朗日框架的伴随状态法，推导 $\\Phi(m)$ 关于 $m$ 的梯度。引入一个伴随变量 $\\lambda \\in \\mathbb{R}^n$，建立拉格朗日函数\n$$\n\\mathcal{L}(x,m,\\lambda) = \\tfrac{1}{2} \\| S x - d \\|_2^2 + \\lambda^\\top (A x - m),\n$$\n并推导伴随方程以及用 $\\lambda$ 表示的 $\\nabla \\Phi(m)$ 的最终表达式。从第一性原理出发，仔细论证每一步，并指明矩阵 $A$ 中编码的边界条件。\n\n3. 对可辨识性零空间\n$$\n\\mathcal{N} = \\{ h \\in \\mathbb{R}^n : F'(m)[h] = 0 \\}\n$$\n给出精确的刻画。证明 $\\mathcal{N}$ 与 $m$ 无关，且等于矩阵 $S A^{-1}$ 的零空间。证明以下构造性刻画：如果存在一个向量 $v \\in \\mathbb{R}^n$，它在所有传感器位置上都为零（即 $S v = 0$），那么扰动 $h = A v$ 属于 $\\mathcal{N}$。解释这与可辨识性的关系，以及伴随算子在检测不可观测方向中的作用。\n\n4. 设计一个算法，通过奇异值分解计算雅可比矩阵 $J = S A^{-1}$ 的数值秩，从而计算可辨识性零空间的维度 $\\dim(\\mathcal{N})$。请论证一个阈值规则，其形式为：将严格大于最大奇异值 $10^{-10}$ 倍的奇异值计入秩中，并将零空间维度定义为 $n$ 减去此数值秩。\n\n5. 实现一个完整、可运行的程序，该程序能够：\n   - 为给定的 $n$ 构建矩阵 $A$，并为指定的传感器索引构建选择矩阵 $S$。\n   - 计算雅可比矩阵 $J = S A^{-1}$ 及其奇异值。\n   - 通过上述阈值规则计算零空间维度。\n   - 通过构建一个在传感器位置上恒为零的向量 $v$ 并形成 $h = A v$，然后计算 $\\| J h \\|_2$，来验证任务3中的构造性刻画。\n   - 汇总测试套件的结果，并以确切指定的格式打印它们。\n\n测试套件必须覆盖以下情况：\n\n- 情况1（正常路径）：$n = 8$，传感器位于所有内部网格点。这对应于 $k = n$ 且 $S$ 等于单位矩阵。输出整数 $\\dim(\\mathcal{N})$。\n- 情况2（无数据边缘情况）：$n = 8$，未选择任何传感器。这对应于 $k = 0$ 且 $S$ 等于零行矩阵。输出整数 $\\dim(\\mathcal{N})$。\n- 情况3（部分覆盖和构造性零空间）：$n = 10$，传感器位于索引 $\\{2,5,8\\}$ 处（内部点的零基索引）。构造一个向量 $v \\in \\mathbb{R}^{10}$，使得对于传感器索引，$v_i = 0$，而在其他地方 $v_i$ 非零，设 $h = A v$，并输出浮点数 $\\| J h \\|_2$。\n\n最终输出格式：你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，即 $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$，其中 $\\text{result}_1$ 和 $\\text{result}_2$ 是整数，$\\text{result}_3$ 是浮点数。不应打印任何其他文本。",
            "solution": "该问题陈述是关于一个由线性椭圆偏微分方程控制的反问题中，灵敏度理论与计算的一个有效练习。它在科学上是合理的，数学上是适定的，并且解决该问题所需的所有信息都已提供。\n\n该问题要求从反问题的角度对一个一维稳态扩散问题进行多部分分析。我们将按顺序处理每个任务。\n\n离散化的状态方程由线性系统 $A x = m$ 给出，其中 $x, m \\in \\mathbb{R}^n$。矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 是在区间 $[0,1]$ 上带齐次狄利克雷边界条件的负二阶导数算子 $-d^2/ds^2$ 的有限差分近似。该矩阵是对称正定的，因此是可逆的。从源参数 $m$ 到观测值的前向映射 $F(m)$ 是 $F(m) = S A^{-1} m$，其中 $S \\in \\mathbb{R}^{k \\times n}$ 是一个线性观测算子。\n\n### 1. 前向映射的弗雷歇导数\n\n映射 $F$ 在点 $m$ 的弗雷歇导数是一个线性算子 $F'(m)$，满足\n$$ F(m+h) = F(m) + F'(m)[h] + o(\\|h\\|) $$\n对于一个小的扰动 $h \\in \\mathbb{R}^n$。\n\n前向映射由 $F(m) = S A^{-1} m$ 给出。由于 $A^{-1}$ 和 $S$ 是常数矩阵，映射 $F$ 对 $m$ 是线性的。对于一个线性算子 $L$，其导数就是算子本身，即 $L'(m)[h] = L(h)$。因此，\n$$ F'(m)[h] = S A^{-1} h. $$\n为了按要求用线性化状态方程来表示，我们考虑扰动 $h$ 对状态 $x$ 的影响。原始状态 $x(m)$ 满足 $A x(m) = m$。扰动后的状态 $x(m+h)$ 满足 $A x(m+h) = m+h$。\n\n设 $x_h = x(m+h) - x(m)$ 为状态的扰动。根据矩阵向量乘积的线性性质，\n$$ A x_h = A(x(m+h) - x(m)) = A x(m+h) - A x(m) = (m+h) - m = h. $$\n这就得到了状态扰动 $x_h$ 的线性化状态方程：\n$$ A x_h = h. $$\n由于 $A$ 是可逆的，$x_h = A^{-1} h$。\n\n现在，我们考察观测值的扰动：\n$$ F(m+h) - F(m) = S x(m+h) - S x(m) = S(x(m+h) - x(m)) = S x_h. $$\n代入 $x_h$ 的表达式，我们有\n$$ F(m+h) - F(m) = S (A^{-1} h). $$\n由于关系是完全线性的，项 $o(\\|h\\|)$ 为零。因此， $F$ 在点 $m$ 沿方向 $h$ 的弗雷歇导数是：\n$$ F'(m)[h] = S x_h, \\quad \\text{其中} \\quad A x_h = h. $$\n这个表达式表明，观测值对源项中扰动 $h$ 的灵敏度，是通过用算子 $S$ 观测所产生的状态扰动 $x_h$ 来给出的。值得注意的是，因为 $F$ 是线性的，它的导数 $F'(m)[\\cdot] = SA^{-1}(\\cdot)$ 是一个常数算子，与求导点 $m$ 无关。\n\n### 2. 通过伴随状态法计算代价泛函的梯度\n\n我们寻求在状态方程约束 $A x = m$ 下，代价泛函 $\\Phi(m) = \\frac{1}{2} \\| S x - d \\|_2^2$ 的梯度。我们使用拉格朗日框架。拉格朗日函数给出如下：\n$$ \\mathcal{L}(x, m, \\lambda) = \\tfrac{1}{2} \\| S x - d \\|_2^2 + \\lambda^\\top (A x - m), $$\n其中 $\\lambda \\in \\mathbb{R}^n$ 是伴随状态或拉格朗日乘子。\n\n为了找到简化泛函 $\\Phi(m)$ 的梯度，我们求 $\\mathcal{L}$ 关于其变量的驻点。梯度 $\\nabla \\Phi(m)$ 则由 $\\mathcal{L}$ 关于 $m$ 的偏导数给出，该偏导数在状态方程和伴随方程的解处求值。\n\n1.  **关于 $\\lambda$ 的导数**：取 $\\mathcal{L}$ 关于 $\\lambda$ 的导数并令其为零，可以恢复状态方程。\n    $$ \\nabla_\\lambda \\mathcal{L}(x, m, \\lambda) = A x - m = 0 \\implies A x = m. $$\n\n2.  **关于 $x$ 的导数**：我们计算 $\\mathcal{L}$ 关于 $x$ 沿任意方向 $\\delta x$ 的方向导数。\n    $$ \\delta_x \\mathcal{L} = \\frac{d}{d\\epsilon} \\mathcal{L}(x+\\epsilon\\delta x, m, \\lambda) \\Big|_{\\epsilon=0}. $$\n    $$ \\delta_x \\mathcal{L} = \\frac{d}{d\\epsilon} \\left( \\tfrac{1}{2} (S(x+\\epsilon\\delta x) - d)^\\top (S(x+\\epsilon\\delta x) - d) + \\lambda^\\top (A(x+\\epsilon\\delta x) - m) \\right) \\Big|_{\\epsilon=0} $$\n    $$ = (S x - d)^\\top S \\delta x + \\lambda^\\top A \\delta x = (S^\\top(S x - d) + A^\\top \\lambda)^\\top \\delta x. $$\n    为了使该式对所有 $\\delta x$ 都为零，我们必须有 $\\nabla_x \\mathcal{L} = S^\\top(S x - d) + A^\\top \\lambda = 0$。这就给出了**伴随方程**：\n    $$ A^\\top \\lambda = -S^\\top(S x - d). $$\n    根据定义，矩阵 $A$ 是对称的（$A_{ij} = A_{ji}$），所以 $A^\\top = A$。伴随方程简化为：\n    $$ A \\lambda = -S^\\top(S x - d). $$\n    矩阵 $A$ 的结构意味着其对应的连续问题具有齐次狄利克雷边界条件。由于伴随算子仍然是 $A$，伴随变量 $\\lambda$ 也满足相同的边界条件。\n\n3.  **关于 $m$ 的导数**：我们计算 $\\mathcal{L}$ 关于 $m$ 沿任意方向 $\\delta m$ 的方向导数。\n    $$ \\delta_m \\mathcal{L} = \\frac{d}{d\\epsilon} \\mathcal{L}(x, m+\\epsilon\\delta m, \\lambda) \\Big|_{\\epsilon=0}. $$\n    $$ \\delta_m \\mathcal{L} = \\frac{d}{d\\epsilon} \\left( \\tfrac{1}{2} \\| S x - d \\|_2^2 + \\lambda^\\top (A x - (m+\\epsilon\\delta m)) \\right) \\Big|_{\\epsilon=0} = -\\lambda^\\top \\delta m. $$\n    简化泛函 $\\Phi(m)$ 的梯度由 $\\nabla_m \\mathcal{L}$ 给出，它是对所有 $\\delta m$ 都满足 $(\\nabla_m \\mathcal{L})^\\top \\delta m = -\\lambda^\\top \\delta m$ 的向量。\n    因此，梯度为：\n    $$ \\nabla \\Phi(m) = -\\lambda. $$\n\n总而言之，$\\Phi(m)$ 的梯度通过以下三个步骤计算：\n(i) 求解状态方程以得到 $x$：$A x = m$。\n(ii) 求解伴随方程以得到 $\\lambda$：$A \\lambda = -S^\\top(S x - d)$。\n(iii) 梯度为 $\\nabla \\Phi(m) = -\\lambda$。\n\n### 3. 可辨识性零空间\n\n可辨识性零空间 $\\mathcal{N}$ 是参数扰动 $h$ 的集合，这些扰动是不可观测的，即它们不会引起前向映射输出的任何变化。其定义为：\n$$ \\mathcal{N} = \\{ h \\in \\mathbb{R}^n : F'(m)[h] = 0 \\}. $$\n根据任务1，我们有 $F'(m)[h] = S A^{-1} h$。因此，条件 $F'(m)[h] = 0$ 就是 $S A^{-1} h = 0$。这正是矩阵 $J = S A^{-1}$ 的零空间（或核）的定义。\n$$ \\mathcal{N} = \\text{ker}(S A^{-1}). $$\n由于矩阵 $S$ 和 $A$ 是常数，雅可比矩阵 $J = S A^{-1}$ 也是常数。它不依赖于点 $m$。因此，其零空间 $\\mathcal{N}$ 与 $m$ 无关。\n\n**构造性刻画：**\n题目要求我们证明，如果存在一个向量 $v \\in \\mathbb{R}^n$ 使得 $S v = 0$，那么扰动 $h = A v$ 属于 $\\mathcal{N}$。\n一个满足 $S v = 0$ 的向量 $v$ 对应于一个在所有传感器位置上都为零的状态。\n我们通过将算子 $J = S A^{-1}$ 应用于 $h = A v$ 来检验它是否在零空间 $\\mathcal{N}$ 中：\n$$ J h = (S A^{-1}) h = (S A^{-1}) (A v) = S (A^{-1} A) v = S I v = S v. $$\n根据我们的前提，$S v = 0$。因此，$J h = 0$，这意味着 $h \\in \\text{ker}(J) = \\mathcal{N}$。证明完毕。\n\n这种刻画提供了一种构造零空间元素的方法。任何能够产生一个对传感器“不可见”的状态 $v$（即 $S v = 0$）的源项 $h$ 本身就是一个不可观测的参数扰动。基于梯度的优化算法将无法恢复或校正位于此零空间 $\\mathcal{N}$ 中的参数 $m$ 的分量。这是因为梯度 $\\nabla \\Phi(m) = -\\lambda = A^{-1} S^\\top (S x - d)$ 属于伴随算子 $J^\\top = (S A^{-1})^\\top = (A^{-1})^\\top S^\\top = A^{-1} S^\\top$ 的值域。根据线性代数基本定理，伴随算子的值域是原算子零空间的正交补：$\\text{range}(J^\\top) \\perp \\text{ker}(J)$。因此，基于梯度的更新总是与不可观测方向正交。\n\n### 4. 计算零空间维度的算法\n\n可辨识性零空间的维度 $\\dim(\\mathcal{N})$ 是雅可比矩阵 $J = S A^{-1}$ 的零度。根据秩-零度定理，对于矩阵 $J \\in \\mathbb{R}^{k \\times n}$：\n$$ \\text{rank}(J) + \\text{nullity}(J) = n. $$\n因此，$\\dim(\\mathcal{N}) = \\text{nullity}(J) = n - \\text{rank}(J)$。\n\n矩阵的秩可以从其奇异值可靠地计算出来。秩是非零奇异值的数量。在数值计算中，由于浮点数的不精确性，我们必须使用一个阈值来区分非零和零奇异值。问题提供了一个特定的阈值规则。\n\n算法如下：\n1.  **构造矩阵**：对于给定的网格大小 $n$ 和一组传感器位置，构造有限差分矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 和观测矩阵 $S \\in \\mathbb{R}^{k \\times n}$。\n2.  **计算雅可比矩阵**：计算雅可比矩阵 $J = S A^{-1}$。在数值上，这可以通过先用 `numpy.linalg.inv` 计算 $A^{-1}$，然后乘以 $S$ 来完成。\n3.  **计算奇异值**：计算 $J$ 的奇异值分解（SVD）。设奇异值为 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_p \\ge 0$，其中 $p = \\min(k, n)$。\n4.  **确定数值秩**：找到最大的奇异值 $\\sigma_1$。应用阈值规则：数值秩 $\\text{rank}_{\\text{num}}(J)$ 是满足 $\\sigma_i > 10^{-10} \\sigma_1$ 的奇异值 $\\sigma_i$ 的数量。如果 $J$ 没有非零奇异值（例如，如果它是零矩阵），其秩为 $0$。\n5.  **计算零空间维度**：零空间的维度是 $\\dim(\\mathcal{N}) = n - \\text{rank}_{\\text{num}}(J)$。\n\n### 5. 实现\n最后一步是为指定的测试用例在 Python 中实现此算法。代码将构造矩阵，计算雅可比矩阵及其奇异值，然后根据每个用例的要求确定零空间维度或测试构造性刻画。",
            "answer": "```python\nimport numpy as np\n\ndef construct_A(n):\n    \"\"\"Constructs the finite-difference matrix A for a given n.\"\"\"\n    if n == 0:\n        return np.array([[]])\n    h = 1.0 / (n + 1)\n    h2_inv = 1.0 / (h * h)\n    \n    A = np.zeros((n, n))\n    \n    # Fill diagonal\n    np.fill_diagonal(A, 2.0 * h2_inv)\n    \n    # Fill off-diagonals\n    if n > 1:\n        diag_indices = np.arange(n - 1)\n        A[diag_indices, diag_indices + 1] = -1.0 * h2_inv\n        A[diag_indices + 1, diag_indices] = -1.0 * h2_inv\n        \n    return A\n\ndef construct_S(n, sensor_indices):\n    \"\"\"Constructs the observation matrix S for given n and sensor indices.\"\"\"\n    k = len(sensor_indices)\n    if k == 0:\n        return np.zeros((0, n))\n    \n    S = np.zeros((k, n))\n    for i, sensor_idx in enumerate(sensor_indices):\n        if 0 = sensor_idx  n:\n            S[i, sensor_idx] = 1.0\n    return S\n\ndef get_nullspace_dim(n, sensor_indices):\n    \"\"\"Computes the dimension of the identifiability nullspace.\"\"\"\n    A = construct_A(n)\n    S = construct_S(n, sensor_indices)\n\n    if n == 0:\n        return 0\n    if S.shape[0] == 0: # No sensors\n        return n\n    \n    A_inv = np.linalg.inv(A)\n    J = S @ A_inv\n    \n    # singular values of J\n    singular_values = np.linalg.svd(J, compute_uv=False)\n    \n    if len(singular_values) == 0:\n        numerical_rank = 0\n    else:\n        sigma_max = np.max(singular_values)\n        if sigma_max == 0:\n            numerical_rank = 0\n        else:\n            threshold = 1e-10 * sigma_max\n            numerical_rank = np.sum(singular_values > threshold)\n\n    nullity = n - numerical_rank\n    return nullity\n\ndef solve():\n    \"\"\"\n    Solves the problem for the three specified test cases and prints the results.\n    \"\"\"\n    results = []\n\n    # Case 1: n = 8, sensors at all points\n    n1 = 8\n    sensor_indices1 = list(range(n1))\n    dim_N1 = get_nullspace_dim(n1, sensor_indices1)\n    results.append(int(dim_N1))\n\n    # Case 2: n = 8, no sensors\n    n2 = 8\n    sensor_indices2 = []\n    dim_N2 = get_nullspace_dim(n2, sensor_indices2)\n    results.append(int(dim_N2))\n\n    # Case 3: n = 10, sensors at {2, 5, 8}, constructive nullspace check\n    n3 = 10\n    sensor_indices3 = [2, 5, 8]\n    \n    A3 = construct_A(n3)\n    S3 = construct_S(n3, sensor_indices3)\n    \n    A3_inv = np.linalg.inv(A3)\n    J3 = S3 @ A3_inv\n    \n    # Construct vector v that is zero at sensor locations\n    v3 = np.ones(n3)\n    v3[sensor_indices3] = 0\n    \n    # Construct h = Av\n    h3 = A3 @ v3\n    \n    # Compute the norm ||Jh||_2\n    norm_Jh = np.linalg.norm(J3 @ h3)\n    results.append(float(norm_Jh))\n\n    # Print a single line of output in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "将伴随方法应用于时间依赖系统时，一个核心挑战是为反向积分存储整个正向模拟的状态轨迹，这在计算上可能非常昂贵。本练习 () 演示了一种强大且广泛应用的解决方案：检查点技术 (checkpointing)。我们将为一个演化模型实现离散伴随方法，并利用检查点策略来平衡内存使用和重复计算，这对于大规模数据同化问题是一项至关重要的实践技能。",
            "id": "3419136",
            "problem": "考虑离散时间、时变线性状态演化模型，该模型为一个状态向量 $x_n \\in \\mathbb{R}^d$（其索引为 $n \\in \\{0,1,\\dots,N\\}$）通过显式欧拉时间步长规则定义\n$$\nx_{n+1} = x_n + \\Delta t \\left( A x_n + \\theta B x_n + s \\right),\n$$\n其中 $A \\in \\mathbb{R}^{d \\times d}$ 和 $B \\in \\mathbb{R}^{d \\times d}$ 是固定矩阵，$s \\in \\mathbb{R}^d$ 是固定源项，$\\Delta t  0$ 是固定时间步长，$\\theta \\in \\mathbb{R}$ 是一个标量参数。初始条件为 $x_0 = 0$。设观测算子为 $C \\in \\mathbb{R}^{d \\times d}$，观测值 $y_n \\in \\mathbb{R}^d$（其中 $n \\in \\{0,1,\\dots,N\\}$）由同一模型生成，使用一个固定的“真实”参数 $\\theta_{\\mathrm{true}}$ 和相同的初始条件，且无噪声：\n$$\ny_n = C x_n(\\theta_{\\mathrm{true}}).\n$$\n将每个时间点的数据失配度定义为\n$$\n\\ell_n(x_n) = \\tfrac{1}{2} \\left\\| C x_n - y_n \\right\\|_2^2,\n$$\n并将总目标函数定义为\n$$\nJ(\\theta) = \\sum_{n=0}^{N} \\ell_n(x_n(\\theta)) + \\tfrac{\\alpha}{2} \\theta^2,\n$$\n其中 $\\alpha \\ge 0$ 是一个固定的正则化权重。\n\n您将通过伴随状态法在内存受限的情况下使用检查点技术计算 $J$ 对 $\\theta$ 的敏感度，并通过与有限差分近似进行对比来验证其正确性。\n\n用于推导的基础定义和假设：\n- 泛函 $\\Phi$ 在参数 $u$ 处沿方向 $h$ 的 Fréchet 导数是线性映射 $D\\Phi(u)[h]$，满足当 $\\|h\\|\\to 0$ 时，$\\Phi(u+h) - \\Phi(u) = D\\Phi(u)[h] + o(\\|h\\|)$。\n- 对于离散时间映射 $F_\\theta(x) = x + \\Delta t \\left( A x + \\theta B x + s \\right)$，其关于状态的雅可比矩阵为 $\\partial F_\\theta / \\partial x = I + \\Delta t (A + \\theta B)$，关于参数的偏导数为 $\\partial F_\\theta / \\partial \\theta (x) = \\Delta t\\, B x$。\n- 伴随递归通过对由目标 $J$ 和离散动力学约束构成的拉格朗日函数强制施加平稳性条件而产生，其中引入了伴随变量 $\\lambda_n \\in \\mathbb{R}^d$。\n\n检查点与内存：\n- 当从 $n = N$ 到 $n = 0$ 向后运行伴随模型时，需要访问前向状态 $x_n$ 来评估梯度贡献和伴随更新。当 $N$ 很大时，存储所有 $x_n$ 可能是不可行的。\n- 检查点策略根据内存限制 $\\mathsf{M}$（可存储状态的最大数量），在索引 $\\{n_k\\}$ 处存储一个选定的状态子集 $\\{x_{n_k}\\}$。当所需的状态 $x_n$ 未被存储时，将通过从最近的前一个已存储检查点索引 $n_k \\le n$ 重新运行前向模型直到 $n$ 来按需重新计算它。在原始前向传递之外执行的额外前向步数被追踪为重计算计数。\n\n任务：\n1. 基于提供的基础定义，推导 $\\lambda_n$ 的离散伴随递归关系式，以及用前向状态、伴随变量和模型导数表示的梯度 $\\mathrm{d}J/\\mathrm{d}\\theta$ 的表达式。\n2. 实现一个程序，该程序：\n   - 通过在 $\\theta_{\\mathrm{true}}$ 处运行前向模型来生成观测值 $y_n$。\n   - 使用伴随状态法计算 $\\mathrm{d}J/\\mathrm{d}\\theta$，采用统一的检查点策略，该策略在从 0 到 N（含）的均匀间隔时间索引处存储 $\\mathsf{M}$ 个状态，并在间距不精确时进行去重。当所需的状态 $x_n$ 未被存储时，从最近的前一个检查点重新计算它。\n   - 统计在伴随时间重计算期间，在初始的 N 步前向运行之外执行的额外前向步总数。\n   - 使用小步长 $\\varepsilon$ 计算 $\\mathrm{d}J/\\mathrm{d}\\theta$ 的中心有限差分近似：\n     $$\n     \\mathrm{d}J/\\mathrm{d}\\theta \\approx \\frac{J(\\theta+\\varepsilon) - J(\\theta-\\varepsilon)}{2\\varepsilon}.\n     $$\n   - 报告基于伴随的梯度与有限差分梯度之间的绝对误差、重计算计数、存储的状态数，以及一个布尔值，该值指示绝对误差是否低于指定的容差。\n3. 使用以下固定的模型设置：\n   - 维度 $d = 3$。\n   - 时间步长 $\\Delta t = 0.1$。\n   - 矩阵\n     $$\n     A = \\begin{bmatrix}\n     -0.1  0.2  0 \\\\\n     -0.2  -0.3  0.1 \\\\\n     0  -0.1  -0.2\n     \\end{bmatrix},\\quad\n     B = \\begin{bmatrix}\n     0.5  0  0 \\\\\n     0  0.1  0 \\\\\n     0  0  0.3\n     \\end{bmatrix},\\quad\n     C = I_3,\n     $$\n     和源项\n     $$\n     s = \\begin{bmatrix} 0.1 \\\\ -0.05 \\\\ 0.2 \\end{bmatrix}.\n     $$\n   - 初始条件 $x_0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n   - 真实参数 $\\theta_{\\mathrm{true}} = 0.3$。\n4. 实现以下测试套件（每个测试用例提供 $N$、$\\mathsf{M}$、$\\theta$、$\\alpha$ 和 $\\varepsilon$）：\n   - 测试用例 1 (一般情况)：$N = 50$，$\\mathsf{M} = 6$，$\\theta = 0.2$，$\\alpha = 0.01$，$\\varepsilon = 10^{-6}$。\n   - 测试用例 2 (全内存边界)：$N = 50$，$\\mathsf{M} = 51$，$\\theta = 0.2$，$\\alpha = 0.01$，$\\varepsilon = 10^{-6}$。\n   - 测试用例 3 (最小内存边缘情况)：$N = 50$，$\\mathsf{M} = 1$，$\\theta = 0.2$，$\\alpha = 0.01$，$\\varepsilon = 10^{-6}$。\n   - 测试用例 4 (短时程边界)：$N = 1$，$\\mathsf{M} = 1$，$\\theta = 0.2$，$\\alpha = 0.01$，$\\varepsilon = 10^{-8}$。\n5. 对于每个测试用例，您的程序必须按以下列表格式生成一个结果：\n   $$\n   [g_{\\mathrm{adj}}, g_{\\mathrm{fd}}, \\lvert g_{\\mathrm{adj}} - g_{\\mathrm{fd}} \\rvert, \\text{recompute\\_count}, \\text{memory\\_used}, \\text{match}],\n   $$\n   其中 $g_{\\mathrm{adj}}$ 是基于伴随的梯度，$g_{\\mathrm{fd}}$ 是有限差分梯度，$\\text{recompute\\_count}$ 是一个整数，$\\text{memory\\_used}$ 是一个表示已存储状态数量的整数，$\\text{match}$ 是一个布尔值，指示测试用例 1–3 的绝对误差是否小于 $10^{-8}$，测试用例 4 的绝对误差是否小于 $10^{-10}$。\n6. 最终输出格式要求：您的程序应生成单行输出，其中包含所有测试用例的结果，格式为一个由方括号括起来的逗号分隔列表，每个测试用例的结果本身也按第 5 项中指定的列表格式化。例如，一个有效的格式是\n   $$\n   [[\\cdots],[\\cdots],[\\cdots],[\\cdots]].\n   $$\n\n此问题不涉及任何物理单位或角度单位；所有量均为无单位实数。输出值必须是指定的浮点数、整数或布尔值。",
            "solution": "此问题经评估有效。它在科学上基于最优控制和敏感度分析（特别是伴随状态法）的既定理论，问题陈述清晰客观，定义明确，并为唯一解提供了所有必要的数据和定义。\n\n### 1. 伴随方程和梯度的推导\n\n目标是计算目标函数的梯度 $\\mathrm{d}J/\\mathrm{d}\\theta$：\n$$\nJ(\\theta) = \\sum_{n=0}^{N} \\ell_n(x_n(\\theta)) + \\tfrac{\\alpha}{2} \\theta^2 = \\sum_{n=0}^{N} \\tfrac{1}{2} \\left\\| C x_n(\\theta) - y_n \\right\\|_2^2 + \\tfrac{\\alpha}{2} \\theta^2\n$$\n状态向量 $x_n$ 根据离散时间动力学演化，这些动力学是优化问题的约束条件：\n$$\nx_{n+1} = F_\\theta(x_n) = x_n + \\Delta t \\left( A x_n + \\theta B x_n + s \\right) \\quad \\text{for } n = 0, \\dots, N-1\n$$\n初始条件为 $x_0=0$。\n\n我们使用拉格朗日乘子法来推导伴随方程。拉格朗日函数 $\\mathcal{L}$ 是通过将目标函数与约束条件相结合来构建的，约束条件由伴随变量（拉格朗日乘子）$\\lambda_{n+1} \\in \\mathbb{R}^d$ 加权：\n$$\n\\mathcal{L}(x_1, \\dots, x_N, \\theta, \\lambda_1, \\dots, \\lambda_N) = J(\\theta) - \\sum_{n=0}^{N-1} \\lambda_{n+1}^T \\left( x_{n+1} - F_\\theta(x_n) \\right)\n$$\n代入 $J(\\theta)$ 的表达式，并注意 $x_0=0$ 是一个固定条件：\n$$\n\\mathcal{L} = \\left(\\sum_{n=0}^{N} \\ell_n(x_n)\\right) + \\tfrac{\\alpha}{2} \\theta^2 - \\sum_{n=0}^{N-1} \\lambda_{n+1}^T \\left( x_{n+1} - F_\\theta(x_n) \\right)\n$$\n对于满足状态方程的轨迹 $\\{x_n\\}$，梯度 $\\mathrm{d}J/\\mathrm{d}\\theta$ 是拉格朗日函数关于 $\\theta$ 的偏导数，前提是伴随变量对于每个状态变量 $x_n$ ($n=1, \\dots, N$) 都满足平稳性条件 $\\partial \\mathcal{L} / \\partial x_n = 0$。\n\n**伴随方程：**\n平稳性条件定义了伴随递归。\n对于最终状态 $x_N$ ($n=N$)：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_N} = \\nabla_{x_N} \\ell_N(x_N) - \\lambda_N^T = 0 \\implies \\lambda_N = (\\nabla_{x_N} \\ell_N(x_N))^T\n$$\n对于中间状态 $x_n$ ($n=1, \\dots, N-1$)：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_n} = \\nabla_{x_n} \\ell_n(x_n) - \\lambda_n^T + \\lambda_{n+1}^T \\frac{\\partial F_\\theta(x_n)}{\\partial x_n} = 0\n$$\n这得到了伴随变量的后向递归关系：\n$$\n\\lambda_n = \\left(\\frac{\\partial F_\\theta(x_n)}{\\partial x_n}\\right)^T \\lambda_{n+1} + (\\nabla_{x_n} \\ell_n(x_n))^T\n$$\n让我们确定所需的导数：\n- 失配项 $\\ell_n(x_n) = \\frac{1}{2}(Cx_n-y_n)^T(Cx_n-y_n)$ 的梯度是 $\\nabla_{x_n}\\ell_n(x_n) = (Cx_n-y_n)^T C$。其转置是 $(\\nabla_{x_n}\\ell_n(x_n))^T = C^T(Cx_n-y_n)$。\n- 状态转移映射 $F_\\theta$ 关于状态的雅可比矩阵是 $\\frac{\\partial F_\\theta(x_n)}{\\partial x_n} = I + \\Delta t(A + \\theta B)$。让我们将此矩阵表示为 $M_\\theta$。\n\n完整的伴随系统由一个后向时间演化定义：\n在 $n=N$ 处的终端条件：\n$$\n\\lambda_N = C^T (C x_N - y_N)\n$$\n对于 $n = N-1, \\dots, 0$ 的伴随递归：\n$$\n\\lambda_n = (I + \\Delta t(A + \\theta B))^T \\lambda_{n+1} + C^T (C x_n - y_n)\n$$\n请注意，我们将递归一直进行到 $n=0$，得到 $\\lambda_0$，它表示 $J$ 对初始条件 $x_0$ 的敏感度。虽然这对参数梯度并非严格必要，但它定义了完整的伴随状态轨迹。\n\n**梯度表达式：**\n梯度 $\\mathrm{d}J/\\mathrm{d}\\theta$ 通过对拉格朗日函数关于 $\\theta$ 求导得到：\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}\\theta} = \\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\frac{\\partial}{\\partial\\theta} \\left( \\tfrac{\\alpha}{2}\\theta^2 \\right) + \\sum_{n=0}^{N-1} \\lambda_{n+1}^T \\frac{\\partial F_\\theta(x_n)}{\\partial \\theta}\n$$\n$F_\\theta(x_n)$ 关于 $\\theta$ 的偏导数是：\n$$\n\\frac{\\partial F_\\theta(x_n)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left( x_n + \\Delta t (Ax_n + \\theta Bx_n + s) \\right) = \\Delta t B x_n\n$$\n将此代入梯度表达式，得到基于伴随的梯度的最终公式：\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}\\theta} = \\alpha \\theta + \\Delta t \\sum_{n=0}^{N-1} \\lambda_{n+1}^T B x_n\n$$\n\n### 2. 计算算法\n\n计算过程如下：\n1.  **生成观测值：** “真实”观测值 $y_n$ 是通过使用参数 $\\theta_{\\mathrm{true}}$ 运行前向模型生成的。\n    - 设置 $x_0 = 0$。\n    - 对于 $n=0, \\dots, N-1$：$x_{n+1} = (I + \\Delta t(A+\\theta_{\\mathrm{true}} B))x_n + \\Delta t s$。\n    - 对于 $n=0, \\dots, N$：$y_n = C x_n$。\n\n2.  **有限差分梯度：** 为验证正确性，计算中心有限差分近似 $g_{\\mathrm{fd}}$：\n    - 定义一个函数 `compute_J(theta_eval)`，它使用 `theta_eval` 运行前向模型以获得轨迹 $\\{x_n(\\theta_{\\mathrm{eval}})\\}$，然后计算 $J(\\theta_{\\mathrm{eval}})$。\n    - 计算 $g_{\\mathrm{fd}} = \\frac{J(\\theta+\\varepsilon) - J(\\theta-\\varepsilon)}{2\\varepsilon}$。\n\n3.  **带检查点的伴随状态梯度：**\n    a.  **前向传递与检查点设置：**\n        - 给定参数 $N$ 和 $\\mathsf{M}$，确定检查点索引。统一策略在由 `np.linspace(0, N, M, dtype=int)` 给出的索引 `k` 处存储状态，并移除重复项。设该集合为 $\\mathcal{K}$。\n        - 对给定的 $\\theta$，从 $n=0$ 到 $N$ 运行前向模型。当且仅当 $n \\in \\mathcal{K}$ 时，将状态向量 $x_n$ 存储在内存中。这构成了 $N$ 步的初始前向传递。\n\n    b.  **后向传递与梯度组装：**\n        - 初始化梯度 $g_{\\mathrm{adj}} = \\alpha \\theta$ 和重计算步数计数器为 0。\n        - 为开始递归，获取 $x_N$。如果 $N \\in \\mathcal{K}$，则从内存中检索。否则，找到满足 $n_k  N$ 的最大检查点索引 $n_k \\in \\mathcal{K}$，检索 $x_{n_k}$，并从 $n_k$ 到 $N$ 重新运行前向模型。将重计算步数 $N-n_k$ 添加到计数中。\n        - 初始化伴随变量：$\\lambda_N = C^T(Cx_N - y_N)$。\n        - 从 $n=N$ 向后迭代至 $1$：\n            i. 在步骤 $n$，我们有伴随状态 $\\lambda_n$。\n            ii. 获取状态 $x_{n-1}$。如果 $n-1 \\notin \\mathcal{K}$，找到最近的前一个检查点 $n_k \\le n-1$，检索 $x_{n_k}$，并重新运行前向模型 $(n-1) - n_k$ 步。将此步数添加到重计算计数器。\n            iii. 更新梯度：$g_{\\mathrm{adj}} \\leftarrow g_{\\mathrm{adj}} + \\Delta t \\lambda_n^T B x_{n-1}$。\n            iv. 为下一次迭代更新伴随变量：$\\lambda_{n-1} = (I + \\Delta t(A+\\theta B))^T \\lambda_n + C^T(Cx_{n-1} - y_{n-1})$。\n        - 最终值 $g_{\\mathrm{adj}}$ 即为所求梯度。\n\n    c.  **输出：** 报告计算出的梯度 $g_{\\mathrm{adj}}$ 和 $g_{\\mathrm{fd}}$、它们的绝对差、总重计算计数、存储的状态数（`memory_used`），以及一个布尔值，指示误差是否低于指定容差。\n\n此过程在内存约束下正确地实现了伴随状态法，使用检查点技术来管理后向传递所需的前向轨迹的存储。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases for the adjoint-state method problem.\n    \"\"\"\n\n    # Fixed model settings\n    d = 3\n    dt = 0.1\n    A = np.array([\n        [-0.1, 0.2, 0.0],\n        [-0.2, -0.3, 0.1],\n        [0.0, -0.1, -0.2]\n    ])\n    B = np.array([\n        [0.5, 0.0, 0.0],\n        [0.0, 0.1, 0.0],\n        [0.0, 0.0, 0.3]\n    ])\n    C = np.eye(d)\n    s = np.array([0.1, -0.05, 0.2])\n    x0 = np.zeros(d)\n    theta_true = 0.3\n\n    # Test suite\n    test_cases = [\n        # (N, M, theta, alpha, epsilon, tol)\n        (50, 6, 0.2, 0.01, 1e-6, 1e-8),\n        (50, 51, 0.2, 0.01, 1e-6, 1e-8),\n        (50, 1, 0.2, 0.01, 1e-6, 1e-8),\n        (1, 1, 0.2, 0.01, 1e-8, 1e-10),\n    ]\n\n    all_results = []\n\n    for N, M_mem, theta, alpha, epsilon, tol in test_cases:\n\n        # Step 1: Generate observations y_n using theta_true\n        y_obs = []\n        x_true_traj = [x0]\n        x = x0.copy()\n        \n        M_true = np.eye(d) + dt * (A + theta_true * B)\n\n        for _ in range(N):\n            x = M_true @ x + dt * s\n            x_true_traj.append(x)\n        \n        for x_val in x_true_traj:\n            y_obs.append(C @ x_val)\n\n        # Helper function to run the forward model for a given theta\n        def run_forward(theta_eval):\n            x_traj = [x0]\n            x = x0.copy()\n            M_eval = np.eye(d) + dt * (A + theta_eval * B)\n            for _ in range(N):\n                x = M_eval @ x + dt * s\n                x_traj.append(x)\n            return x_traj\n\n        # Helper function to compute the objective function J(theta)\n        def compute_J(theta_eval):\n            x_traj = run_forward(theta_eval)\n            misfit = 0.0\n            for n in range(N + 1):\n                misfit += 0.5 * np.linalg.norm(C @ x_traj[n] - y_obs[n])**2\n            return misfit + 0.5 * alpha * theta_eval**2\n\n        # Step 2: Compute finite-difference gradient\n        J_plus = compute_J(theta + epsilon)\n        J_minus = compute_J(theta - epsilon)\n        g_fd = (J_plus - J_minus) / (2 * epsilon)\n\n        # Step 3: Compute adjoint-state gradient with checkpointing\n        \n        # 3a: Forward pass and checkpointing\n        checkpoint_indices = sorted(list(set(np.linspace(0, N, M_mem, dtype=int))))\n        memory_used = len(checkpoint_indices)\n        checkpoints = {}\n        \n        x_fwd_traj = [x0]\n        x = x0.copy()\n        if 0 in checkpoint_indices:\n            checkpoints[0] = x0.copy()\n            \n        M_theta = np.eye(d) + dt * (A + theta * B)\n\n        for n in range(N):\n            x = M_theta @ x + dt * s\n            x_fwd_traj.append(x)\n            if (n + 1) in checkpoint_indices:\n                checkpoints[n + 1] = x.copy()\n        \n        recompute_count = 0\n\n        # Helper for on-demand state recomputation\n        memoized_states = {}\n        def get_state(n):\n            nonlocal recompute_count\n            if n in memoized_states:\n                return memoized_states[n]\n            if n in checkpoints:\n                memoized_states[n] = checkpoints[n]\n                return checkpoints[n]\n\n            # Find nearest preceding checkpoint\n            start_n = 0\n            for k in checkpoint_indices:\n                if k  n:\n                    start_n = k\n                else:\n                    break\n            \n            x_re = checkpoints[start_n].copy()\n            for i in range(start_n, n):\n                x_re = M_theta @ x_re + dt * s\n                recompute_count += 1\n            \n            memoized_states[n] = x_re\n            return x_re\n\n        # 3b: Backward pass\n        g_adj = alpha * theta\n        M_theta_T = M_theta.T\n\n        # Initialize adjoint at n=N\n        xN = get_state(N)\n        lam = C.T @ (C @ xN - y_obs[N])\n\n        # Backward recursion from n=N down to 1\n        for n in range(N, 0, -1):\n            # At start of loop, lam is lambda_n\n            \n            # Get state x_{n-1} for gradient and adjoint update\n            x_prev = get_state(n-1)\n\n            # Update gradient: term is dt * lambda_n^T * B * x_{n-1}\n            g_adj += dt * (lam.T @ B @ x_prev)\n            \n            # Update adjoint for next iteration: compute lambda_{n-1}\n            lam = M_theta_T @ lam + C.T @ (C @ x_prev - y_obs[n-1])\n\n        abs_err = abs(g_adj - g_fd)\n        match = abs_err  tol\n\n        all_results.append([g_adj, g_fd, abs_err, recompute_count, memory_used, match])\n\n    # Final print statement\n    result_str = ','.join([str(res) for res in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "许多现实世界中的观测过程并非光滑的，例如仅报告开关状态的二元传感器，这给基于梯度的优化带来了挑战。本练习 () 通过引入一个光滑近似（或称“磨光器”）来处理不可微的观测算子。我们将推导并比较基于最小二乘和基于似然的两种不同目标函数的梯度，从而理解在处理非连续数据时选择合适代价函数的重要性。",
            "id": "3419096",
            "problem": "考虑一个有限维线性正向模型，其中状态向量 $u \\in \\mathbb{R}^m$ 通过一个已知的矩阵 $M \\in \\mathbb{R}^{m \\times n}$ 依赖于控制向量 $\\theta \\in \\mathbb{R}^n$，即 $u = M \\theta$。观测是状态的二元阈值化，逐分量定义为 $H(u)_i = \\mathbf{1}\\{u_i  \\tau\\}$，其中 $\\tau \\in \\mathbb{R}$ 是一个固定阈值，观测数据向量为 $y \\in \\{0,1\\}^m$。由于 $H$ 是不连续且非 Fréchet 可微的，我们引入一个由 $\\epsilon  0$ 参数化的平滑化观测 $H_\\epsilon$，它使用亥维赛德阶跃函数的一个平滑近似。假设 $H_\\epsilon$ 逐分量作用于 $u$ 并且当 $\\epsilon \\to 0$ 时满足 $H_\\epsilon(u) \\to H(u)$。\n\n您的任务是：\n- 从复合函数的 Fréchet 导数、最小二乘数据失配以及线性算子的伴随的定义出发，推导最小二乘目标函数 $J_\\epsilon(\\theta) = \\tfrac{1}{2} \\| H_\\epsilon(M\\theta) - y \\|_2^2$ 关于 $\\theta$ 的梯度，过程中不使用任何现成的快捷公式。清晰地指出由链式法则和伴随状态构造产生的伴随变量。\n- 使用伯努利数据模型和一个与所选 $H_\\epsilon$ 相匹配的平滑链接函数，推导二元数据的负对数似然关于 $\\theta$ 的梯度。\n- 在分布的层面上，分析当 $\\epsilon \\to 0$ 时，由 $H_\\epsilon$ 引起的关于 $u$ 的敏感度的极限行为，并解释上述推导出的梯度在该极限下的情况。\n- 设计一个定量诊断指标，用于衡量使用带有 $H_\\epsilon$ 的最小二乘目标函数相对于使用相同链接函数的负对数似然梯度所产生的梯度偏差；该诊断指标必须是从两个梯度计算得出的无量纲浮点数。\n\n实现要求：\n- 使用逻辑斯谛函数作为平滑核。具体来说，定义 $H_\\epsilon(u)_i = \\sigma\\!\\left(\\frac{u_i - \\tau}{\\epsilon}\\right)$，其中 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$，并逐分量应用。\n- 在您的程序中实现两种梯度计算：一种对应于带有 $H_\\epsilon$ 的最小二乘目标函数，另一种对应于具有相同逻辑斯谛链接函数的伯努利模型下的负对数似然。使用伴随状态视角来构建计算：在观测空间中计算一个伴随变量向量，并应用 $M^\\top$ 以获得关于 $\\theta$ 的梯度。\n- 将诊断指标实现为两个梯度之间的相对差异，定义为 $b = \\frac{\\|g_{\\text{LS}} - g_{\\text{CE}}\\|_2}{\\|g_{\\text{CE}}\\|_2 + \\delta}$，其中 $g_{\\text{LS}}$ 是最小二乘梯度，$g_{\\text{CE}}$ 是负对数似然梯度，$\\|\\cdot\\|_2$ 是欧几里得范数，$\\delta$ 是一个小的正常数以避免除以零。使用 $\\delta = 10^{-12}$。\n\n测试套件：\n- 使用 $m = 4$，$n = 3$，阈值 $\\tau = 0.5$，以及矩阵\n$$\nM = \\begin{bmatrix}\n1.0  -0.5  0.2 \\\\\n0.0  1.0  0.3 \\\\\n0.5  0.5  -0.4 \\\\\n-0.3  0.2  1.0\n\\end{bmatrix}.\n$$\n- 对以下每种情况以及集合 $\\{\\;0.5,\\;0.1,\\;0.02\\;\\}$ 中的每个 $\\epsilon$ 值，评估诊断指标 $b$：\n    1. 顺利路径（一些 $u_i$ 接近 $\\tau$）：$\\theta = \\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ -0.1 \\end{bmatrix}$，$y = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$。\n    2. 边界条件（一个 $u_i$ 恰好在 $\\tau$ 处）：$\\theta = \\begin{bmatrix} 0.0 \\\\ 1.0 \\\\ 0.0 \\end{bmatrix}$，$y = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\\\ 0 \\end{bmatrix}$。\n    3. 一致的阈值以上情况：$\\theta = \\begin{bmatrix} 2.5 \\\\ 2.0 \\\\ 1.0 \\end{bmatrix}$，$y = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$。\n    4. 远低于阈值的不匹配情况：$\\theta = \\begin{bmatrix} -1.0 \\\\ -0.5 \\\\ -0.5 \\end{bmatrix}$，$y = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含一个由方括号括起来的逗号分隔列表形式的结果，其中每个元素本身是一个逗号分隔的浮点数列表，对应于每个 $\\epsilon$ 值（按 $\\epsilon \\in \\{\\;0.5,\\;0.1,\\;0.02\\;\\}$ 的顺序）的诊断指标 $b$。例如，输出的形状应为 $[\\,[b_{1,0.5},b_{1,0.1},b_{1,0.02}],\\,[b_{2,0.5},b_{2,0.1},b_{2,0.02}],\\,[b_{3,0.5},b_{3,0.1},b_{3,0.02}],\\,[b_{4,0.5},b_{4,0.1},b_{4,0.02}]\\,]$，不含任何额外文本。不涉及物理单位；所有输出均为无单位浮点数。",
            "solution": "我们从一个由 $u = M \\theta$ 定义的有限维线性正向模型开始，其中 $M \\in \\mathbb{R}^{m \\times n}$ 是已知的，$\\theta \\in \\mathbb{R}^n$。观测是二元阈值化 $H(u)_i = \\mathbf{1}\\{u_i  \\tau\\}$，其中阈值 $\\tau \\in \\mathbb{R}$ 固定，观测数据为 $y \\in \\{0,1\\}^m$。由于 $H$ 是不连续且非 Fréchet 可微的，我们使用逻辑斯谛函数 $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ 来平滑化 $H$，逐分量定义平滑算子 $H_\\epsilon(u)$ 为\n$$\nH_\\epsilon(u)_i = \\sigma\\!\\left(\\frac{u_i - \\tau}{\\epsilon}\\right).\n$$\n这个 $H_\\epsilon$ 满足当 $\\epsilon \\to 0$ 时 $H_\\epsilon(u) \\to H(u)$ 逐点收敛。\n\n我们定义最小二乘目标函数\n$$\nJ_\\epsilon(\\theta) = \\frac{1}{2} \\| H_\\epsilon(M\\theta) - y \\|_2^2,\n$$\n并使用 Fréchet 导数和伴随状态形式体系来求解 $\\nabla_\\theta J_\\epsilon(\\theta)$。\n\n基本原理：\n- 对于赋范空间之间的一个可微映射 $F: X \\to Y$，其 Fréchet 导数 $DF(x): X \\to Y$ 是唯一的有界线性算子，满足当 $\\|h\\| \\to 0$ 时，$F(x + h) = F(x) + DF(x) h + o(\\|h\\|)$。\n- 对于复合函数，Fréchet 导数的链式法则为 $D(G \\circ F)(x) = DG(F(x)) \\circ DF(x)$。\n- 对于内积空间之间的一个线性映射 $A: X \\to Y$，其伴随 $A^\\top: Y \\to X$ 满足 $\\langle Ax, y \\rangle_Y = \\langle x, A^\\top y \\rangle_X$。\n- 对于带有残差 $r: \\mathbb{R}^n \\to \\mathbb{R}^m$ 的最小二乘目标函数 $J(\\theta) = \\frac{1}{2}\\|r(\\theta)\\|_2^2$，其 Fréchet 导数为 $DJ(\\theta) h = \\langle r(\\theta), Dr(\\theta) h \\rangle$，因此 $\\nabla_\\theta J(\\theta) = (Dr(\\theta))^\\top r(\\theta)$。\n\n将这些应用于 $r(\\theta) = H_\\epsilon(M\\theta) - y$。由于 $H_\\epsilon$ 逐分量作用，其关于 $u$ 的 Fréchet 导数是逐分量的对角阵：\n$$\nD H_\\epsilon(u) = \\operatorname{diag}\\!\\left(h_\\epsilon'(u)\\right),\n$$\n其中\n$$\nh_\\epsilon'(u_i) = \\frac{\\partial}{\\partial u_i} \\sigma\\!\\left(\\frac{u_i - \\tau}{\\epsilon}\\right) = \\sigma\\!\\left(\\frac{u_i - \\tau}{\\epsilon}\\right)\\left(1 - \\sigma\\!\\left(\\frac{u_i - \\tau}{\\epsilon}\\right)\\right)\\frac{1}{\\epsilon}.\n$$\n令 $p = H_\\epsilon(u)$ 表示向量，其分量为 $p_i = \\sigma\\!\\left(\\frac{u_i - \\tau}{\\epsilon}\\right)$。那么 $h_\\epsilon'(u_i) = \\frac{p_i (1 - p_i)}{\\epsilon}$。使用 $u = M\\theta$ 和链式法则，$r$ 在 $\\theta$ 处的 Fréchet 导数是 $Dr(\\theta) = D H_\\epsilon(u) \\, M = \\operatorname{diag}(h_\\epsilon'(u)) M$。因此，\n$$\n\\nabla_\\theta J_\\epsilon(\\theta) = \\left(Dr(\\theta)\\right)^\\top r(\\theta) = M^\\top \\left( \\operatorname{diag}(h_\\epsilon'(u)) \\, (H_\\epsilon(u) - y) \\right).\n$$\n这展示了伴随状态结构：定义伴随变量\n$$\n\\lambda_\\epsilon = \\operatorname{diag}(h_\\epsilon'(u)) \\, (H_\\epsilon(u) - y) \\in \\mathbb{R}^m,\n$$\n那么梯度为 $\\nabla_\\theta J_\\epsilon(\\theta) = M^\\top \\lambda_\\epsilon$。\n\n接下来，考虑使用逻辑斯谛链接函数的伯努利数据模型来处理二元观测。给定 $u$，将 $y_i$ 建模为 $y_i \\sim \\operatorname{Bernoulli}(p_i)$，其中 $p_i = \\sigma\\!\\left(\\frac{u_i - \\tau}{\\epsilon}\\right)$。负对数似然（不计常数项）为\n$$\n\\mathcal{L}_\\epsilon(\\theta) = -\\sum_{i=1}^m \\left[ y_i \\log p_i + (1 - y_i) \\log (1 - p_i) \\right].\n$$\n计算其关于 $u$ 的梯度。我们有\n$$\n\\frac{\\partial \\mathcal{L}_\\epsilon}{\\partial p_i} = -\\frac{y_i}{p_i} + \\frac{1 - y_i}{1 - p_i} = \\frac{p_i - y_i}{p_i(1 - p_i)},\n$$\n和\n$$\n\\frac{\\partial p_i}{\\partial u_i} = \\sigma'\\!\\left(\\frac{u_i - \\tau}{\\epsilon}\\right)\\frac{1}{\\epsilon} = \\frac{p_i(1 - p_i)}{\\epsilon}.\n$$\n因此，\n$$\n\\frac{\\partial \\mathcal{L}_\\epsilon}{\\partial u_i} = \\frac{p_i - y_i}{p_i(1 - p_i)} \\cdot \\frac{p_i(1 - p_i)}{\\epsilon} = \\frac{p_i - y_i}{\\epsilon}.\n$$\n以向量形式表示，其中 $p = H_\\epsilon(u)$ 且 $u = M \\theta$，似然函数的伴随变量为\n$$\n\\mu_\\epsilon = \\frac{p - y}{\\epsilon},\n$$\n且关于 $\\theta$ 的梯度为\n$$\n\\nabla_\\theta \\mathcal{L}_\\epsilon(\\theta) = M^\\top \\mu_\\epsilon = M^\\top \\left( \\frac{p - y}{\\epsilon} \\right).\n$$\n\n当 $\\epsilon \\to 0$ 时的极限分析：\n逻辑斯谛平滑核近似了亥维赛德函数。记 $z_i = \\frac{u_i - \\tau}{\\epsilon}$。那么当 $\\epsilon \\to 0$ 时，$p_i = \\sigma(z_i) \\to \\mathbf{1}\\{u_i  \\tau\\}$。对于最小二乘目标函数，关于 $u$ 的敏感度涉及 $h_\\epsilon'(u_i) = \\frac{p_i(1 - p_i)}{\\epsilon}$。当 $\\epsilon \\to 0$ 时，$p_i(1 - p_i)$ 在远离阈值处趋于零，而在 $u_i = \\tau$ 附近集中，宽度为 $O(\\epsilon)$，振幅为 $O(1)$，因此 $h_\\epsilon'(u_i)$ 形成一个序列，近似于一个以 $u_i = \\tau$ 为中心的狄拉克δ函数。更准确地说，在分布的意义上，对于一个光滑检验函数 $\\varphi$，\n$$\n\\int \\varphi(u_i) \\, h_\\epsilon'(u_i) \\, \\mathrm{d}u_i \\to \\varphi(\\tau),\n$$\n这表明 $h_\\epsilon'$ 近似了亥维赛德阶跃函数的导数，即在阈值处的狄拉克δ函数。因此，伴随变量 $\\lambda_\\epsilon = \\operatorname{diag}(h_\\epsilon'(u)) (p - y)$ 仅在 $u_i$ 处于 $\\tau$ 的 $O(\\epsilon)$ 邻域内的分量上才不可忽略，其量级近似为 $\\frac{|p_i - y_i|}{\\epsilon}$。因此，当数据在阈值处不一致时，随着 $\\epsilon \\to 0$，$\\nabla_\\theta J_\\epsilon(\\theta) = M^\\top \\lambda_\\epsilon$ 变得越来越集中，并且可能发生爆炸。类似地，对于似然函数，在 $p \\neq y$ 的任何地方，$\\mu_\\epsilon = \\frac{p - y}{\\epsilon}$ 都有一个统一的 $1/\\epsilon$ 缩放因子，因此 $\\nabla_\\theta \\mathcal{L}_\\epsilon(\\theta)$ 在不匹配区域也按 $1/\\epsilon$ 的量级缩放，但没有额外的因子 $p(1 - p)$ 将最小二乘梯度局域化到阈值邻域。因此，相对于对数似然梯度，最小二乘梯度在远离阈值处被抑制，而主要在 $u_i \\approx \\tau$ 附近被放大，这在使用平方误差失配处理二元观测时，会在基于梯度的数据同化中引入偏差。\n\n梯度偏差的诊断指标：\n定义一个无量纲标量诊断指标来比较这两个梯度，\n$$\nb(\\theta, \\epsilon) = \\frac{\\| \\nabla_\\theta J_\\epsilon(\\theta) - \\nabla_\\theta \\mathcal{L}_\\epsilon(\\theta) \\|_2}{\\| \\nabla_\\theta \\mathcal{L}_\\epsilon(\\theta) \\|_2 + \\delta},\n$$\n其中小的 $\\delta  0$ 用于避免除以零。这个诊断指标将 $b \\approx 0$ 解释为两者非常一致，而较大的 $b$ 值则表示最小二乘梯度相对于似然梯度存在显著偏差。\n\n算法设计：\n- 计算 $u = M \\theta$。\n- 逐分量计算 $p = \\sigma\\left(\\frac{u - \\tau}{\\epsilon}\\right)$。\n- 计算最小二乘伴随变量 $\\lambda_\\epsilon = \\left( \\frac{p \\odot (1 - p)}{\\epsilon} \\right) \\odot (p - y)$，其中 $\\odot$ 表示逐分量乘法，以及梯度 $g_{\\text{LS}} = M^\\top \\lambda_\\epsilon$。\n- 计算似然伴随变量 $\\mu_\\epsilon = \\frac{p - y}{\\epsilon}$ 和梯度 $g_{\\text{CE}} = M^\\top \\mu_\\epsilon$。\n- 计算诊断指标 $b = \\frac{\\|g_{\\text{LS}} - g_{\\text{CE}}\\|_2}{\\|g_{\\text{CE}}\\|_2 + \\delta}$。\n- 对指定的测试套件和 $\\epsilon$ 值评估 $b$，并按要求的输出格式返回结果。\n\n该测试套件涵盖了：\n- 一个顺利路径，其中一些 $u_i$ 接近 $\\tau$，揭示了 $p(1 - p)$ 的局域化效应。\n- 一个边界情况，其中 $u_i = \\tau$，突显了敏感度的集中性。\n- 一个一致的情况，其中所有 $u_i \\gg \\tau$，此时两个梯度都很小，诊断指标用于测试数值稳定性。\n- 一个不匹配的情况，其中 $u_i \\ll \\tau$ 但 $y_i = 1$，此时两个梯度都可能很大，诊断指标揭示了系统性差异。\n\n不涉及物理单位，所有输出均为无单位浮点数。",
            "answer": "```python\nimport numpy as np\n\ndef logistic(z):\n    # Numerically stable logistic function\n    # For extreme z, np.exp(-z) can overflow; using standard formulation is fine for given test values.\n    return 1.0 / (1.0 + np.exp(-z))\n\ndef gradients(M, theta, y, tau, epsilon):\n    \"\"\"\n    Compute the least-squares gradient (with mollified observation) and the\n    negative log-likelihood gradient (Bernoulli with logistic link).\n    \"\"\"\n    u = M @ theta  # state\n    z = (u - tau) / epsilon\n    p = logistic(z)\n\n    # Least-squares adjoint variable: lambda = diag(h'(u)) * (p - y)\n    # h'(u) = (p*(1-p))/epsilon\n    hprime = (p * (1.0 - p)) / epsilon\n    lam = hprime * (p - y)\n    g_ls = M.T @ lam\n\n    # Negative log-likelihood adjoint variable: mu = (p - y)/epsilon\n    mu = (p - y) / epsilon\n    g_ce = M.T @ mu\n\n    return g_ls, g_ce\n\ndef bias_diagnostic(g_ls, g_ce, delta=1e-12):\n    \"\"\"\n    Relative difference diagnostic between least-squares gradient and\n    cross-entropy (negative log-likelihood) gradient.\n    \"\"\"\n    num = np.linalg.norm(g_ls - g_ce)\n    den = np.linalg.norm(g_ce) + delta\n    return float(num / den)\n\ndef solve():\n    # Define matrix M, threshold tau, epsilon values\n    M = np.array([[1.0, -0.5, 0.2],\n                  [0.0,  1.0, 0.3],\n                  [0.5,  0.5, -0.4],\n                  [-0.3, 0.2, 1.0]], dtype=float)\n    tau = 0.5\n    epsilons = [0.5, 0.1, 0.02]\n\n    # Test cases: (theta, y)\n    test_cases = [\n        (np.array([1.0, 0.0, -0.1], dtype=float), np.array([1.0, 0.0, 1.0, 0.0], dtype=float)),  # Happy path\n        (np.array([0.0, 1.0, 0.0], dtype=float), np.array([0.0, 1.0, 1.0, 0.0], dtype=float)),    # Boundary\n        (np.array([2.5, 2.0, 1.0], dtype=float), np.array([1.0, 1.0, 1.0, 1.0], dtype=float)),    # Consistent above threshold\n        (np.array([-1.0, -0.5, -0.5], dtype=float), np.array([1.0, 1.0, 1.0, 1.0], dtype=float)), # Mismatch far below threshold\n    ]\n\n    results = []\n    for theta, y in test_cases:\n        biases = []\n        for eps in epsilons:\n            g_ls, g_ce = gradients(M, theta, y, tau, eps)\n            b = bias_diagnostic(g_ls, g_ce)\n            biases.append(b)\n        results.append(biases)\n\n    # Format output exactly as required: a single line with a comma-separated list,\n    # where each element is a comma-separated list of floats.\n    # Convert inner lists to strings without spaces for consistency.\n    inner_strs = []\n    for biases in results:\n        inner_strs.append(\"[\" + \",\".join(f\"{x:.12g}\" for x in biases) + \"]\")\n    print(\"[\" + \",\".join(inner_strs) + \"]\")\n\nsolve()\n```"
        }
    ]
}