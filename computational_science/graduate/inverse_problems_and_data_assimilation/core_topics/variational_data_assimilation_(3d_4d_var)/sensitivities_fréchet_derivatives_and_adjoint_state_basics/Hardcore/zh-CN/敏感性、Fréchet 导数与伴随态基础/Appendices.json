{
    "hands_on_practices": [
        {
            "introduction": "这第一个练习旨在解决稳态系统伴随状态敏感性分析的核心问题。在一个更具挑战性的场景中，观测不再是简单的点测量，而是平滑、非线性变换和加权积分等一系列复杂算子的复合结果 。这项练习对于熟练掌握函数空间中的链式法则，以及理解如何为复杂的、更接近现实的观测模型构建伴随算子至关重要。",
            "id": "3419121",
            "problem": "考虑由线性和非线性元素组成的以下参数到状态的映射和混合观测算子。设空间域为 $\\Omega = (0,1)$，带有齐次狄利克雷边界条件。对于参数场 $m \\in L^{2}(\\Omega)$，状态 $u \\in H_{0}^{1}(\\Omega)$ 由以下边值问题确定\n$$\n- \\frac{d^{2}u}{dx^{2}} + u = m \\quad \\text{for } x \\in (0,1), \\quad u(0) = 0, \\quad u(1) = 0.\n$$\n定义平滑算子 $S: L^{2}(\\Omega) \\to L^{2}(\\Omega)$ 如下\n$$\n(Su)(x) = \\int_{0}^{1} s(x,\\xi)\\, u(\\xi)\\, d\\xi,\n$$\n其中 $s \\in L^{2}(\\Omega \\times \\Omega)$ 是一个给定的对称核。定义逐点非线性 $\\Phi: L^{2}(\\Omega) \\to L^{2}(\\Omega)$ 如下\n$$\n(\\Phi v)(x) = \\exp(\\alpha\\, v(x)) - 1,\n$$\n其中 $\\alpha > 0$ 是一个固定常数。设 $W: L^{2}(\\Omega) \\to \\mathbb{R}^{K}$ 和 $E: L^{2}(\\Omega) \\to \\mathbb{R}^{K}$ 是按分量定义的线性映射\n$$\n(Wv)_{i} = \\int_{0}^{1} w_{i}(x)\\, v(x)\\, dx, \\qquad (Em)_{i} = \\int_{0}^{1} e_{i}(x)\\, m(x)\\, dx,\n$$\n对于 $i = 1, \\dots, K$，其中 $w_{i}, e_{i} \\in L^{2}(\\Omega)$ 是给定函数。给定观测值 $y \\in \\mathbb{R}^{K}$ 和一个对称正定加权矩阵 $R \\in \\mathbb{R}^{K \\times K}$，定义混合观测算子如下\n$$\nH(u,m) = W\\big(\\Phi(Su)\\big) + E m,\n$$\n以及数据同化目标泛函如下\n$$\nJ(m) = \\frac{1}{2}\\big(H(u(m),m) - y\\big)^{\\top} R^{-1} \\big(H(u(m),m) - y\\big) + \\frac{\\lambda}{2} \\int_{0}^{1} m(x)^{2}\\, dx,\n$$\n其中 $\\lambda > 0$。\n\n从弗雷歇(Fréchet)导数的定义和基于拉格朗日构造的伴随状态法出发，推导 $J$ 关于 $m$ 的 $L^{2}(\\Omega)$-梯度。您的推导必须明确说明混合观测算子 $H$ 的复合结构，包括线性算子 $W$、非线性逐点映射 $\\Phi$ 和平滑算子 $S$，以及 $H$ 通过 $E$ 对 $m$ 的直接依赖性。将最终结果表示为梯度 $g(x)$ 的单个闭式解析表达式，用 $m$、$u$、一个伴随状态 $p$ 以及给定的算子和函数来表示。您可以引入由 $R^{-1}$ 和残差定义的辅助量，但最终答案必须是 $g(x)$ 的单个表达式。无需进行数值近似。",
            "solution": "目标是推导泛函 $J(m)$ 关于参数场 $m$ 的 $L^{2}(\\Omega)$-梯度。我们将采用基于拉格朗日公式的伴随状态法。问题在于找到函数 $g \\in L^{2}(\\Omega)$，使得对于任意扰动 $\\delta m \\in L^{2}(\\Omega)$，$J$ 的弗雷歇导数由下式给出\n$$\ndJ(m)[\\delta m] = \\langle g, \\delta m \\rangle_{L^{2}} = \\int_{0}^{1} g(x)\\delta m(x)\\, dx.\n$$\n状态变量 $u$ 是参数 $m$ 的函数，记为 $u(m)$，由状态方程定义：\n$$\n- \\frac{d^{2}u}{dx^{2}} + u = m, \\quad u(0)=0, u(1)=0.\n$$\n这构成了优化问题中的一个约束。我们通过将目标泛函 $J(m)$ 与状态方程的弱形式增广来引入拉格朗日泛函 $\\mathcal{L}(u,m,p)$，其中状态方程的弱形式由拉格朗日乘子 $p(x)$ 加权，而 $p(x)$ 将作为伴随状态。伴随状态 $p$ 的空间将在推导过程中确定，但我们预期 $p \\in H_{0}^{1}(\\Omega)$。\n\n拉格朗日泛函定义为：\n$$\n\\mathcal{L}(u, m, p) = J(u,m) + \\int_{0}^{1} p(x) \\left(-\\frac{d^{2}u}{dx^{2}} + u(x) - m(x)\\right) dx.\n$$\n目标泛函 $J$ 通过 $u(m)$ 隐式地依赖于 $m$，写作 $J(u,m)$：\n$$\nJ(u,m) = \\frac{1}{2}\\big(H(u,m) - y\\big)^{\\top} R^{-1} \\big(H(u,m) - y\\big) + \\frac{\\lambda}{2} \\int_{0}^{1} m(x)^{2}\\, dx.\n$$\n为消去 $u$ 的二阶导数，我们对包含 $p \\frac{d^{2}u}{dx^{2}}$ 的项进行两次分部积分。\n\\begin{align*}\n\\int_{0}^{1} -p \\frac{d^{2}u}{dx^{2}} dx = \\left[-p \\frac{du}{dx}\\right]_{0}^{1} + \\int_{0}^{1} \\frac{dp}{dx} \\frac{du}{dx} dx \\\\\n= \\left[-p \\frac{du}{dx}\\right]_{0}^{1} + \\left[\\frac{dp}{dx} u\\right]_{0}^{1} - \\int_{0}^{1} u \\frac{d^{2}p}{dx^{2}} dx.\n\\end{align*}\n由于 $u \\in H_{0}^{1}(\\Omega)$，我们有 $u(0)=u(1)=0$。为确保对任意 $u$ 边界项都为零，我们要求伴随状态 $p$ 也属于 $H_{0}^{1}(\\Omega)$，这意味着 $p(0)=p(1)=0$。做出此选择后，拉格朗日泛函变为：\n$$\n\\mathcal{L}(u, m, p) = J(u,m) + \\int_{0}^{1} u(x)\\left(-\\frac{d^{2}p}{dx^{2}} + p(x)\\right) dx - \\int_{0}^{1} p(x)m(x)\\, dx.\n$$\n$J(m)$ 的梯度可以通过考虑 $\\mathcal{L}$ 关于 $m$ 的全变分来求得。由于 $u=u(m)$ 由状态方程确定，对于任意 $p$，我们有 $\\mathcal{L}(u(m),m,p) = J(m)$。因此，$J$ 关于 $m$ 的全弗雷歇导数等于 $\\mathcal{L}$ 的全弗雷歇导数：\n$$\ndJ(m)[\\delta m] = d\\mathcal{L}(u(m),m,p)[\\delta m] = \\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u] + \\frac{\\partial \\mathcal{L}}{\\partial m}[\\delta m].\n$$\n此处，$\\delta u$ 是对应于参数变化 $\\delta m$ 的状态变化，由线性化的状态方程控制：$-\\frac{d^{2}(\\delta u)}{dx^{2}} + \\delta u = \\delta m$。\n\n伴随状态法通过选择 $p$ 使得包含 $\\delta u$ 的项为零，从而避免了计算 $\\delta u$ 的需要。我们将 $\\mathcal{L}$ 关于 $u$ 的偏弗雷歇导数设为零：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u] = \\frac{\\partial J}{\\partial u}[\\delta u] + \\int_{0}^{1} \\delta u(x)\\left(-\\frac{d^{2}p}{dx^{2}} + p(x)\\right) dx = 0.\n$$\n这必须对所有容许变分 $\\delta u$ 成立。我们来计算 Gâteaux 导数 $\\frac{\\partial J}{\\partial u}[\\delta u]$。令 $\\mathbf{r}(u,m) = H(u,m) - y$。\n$$\n\\frac{\\partial J}{\\partial u}[\\delta u] = \\left\\langle R^{-1}\\mathbf{r}(u,m), \\frac{\\partial H}{\\partial u}[\\delta u] \\right\\rangle_{\\mathbb{R}^{K}}.\n$$\n算子 $H(u,m) = W(\\Phi(Su)) + Em$ 通过项 $W(\\Phi(Su))$ 依赖于 $u$。根据链式法则，它在方向 $\\delta u$ 上关于 $u$ 的弗雷歇导数为：\n$$\n\\frac{\\partial H}{\\partial u}[\\delta u] = W\\left(\\Phi'(Su)[S\\delta u]\\right).\n$$\n非线性 $\\Phi(v)(x) = \\exp(\\alpha v(x)) - 1$ 的导数是乘法算子 $(\\Phi'(v)[\\delta v])(x) = \\alpha \\exp(\\alpha v(x))\\delta v(x)$。因此，\n$$\n\\frac{\\partial H}{\\partial u}[\\delta u] = W\\left(\\alpha \\exp(\\alpha Su) \\cdot (S\\delta u)\\right).\n$$\n项 $\\frac{\\partial J}{\\partial u}[\\delta u]$ 可以写成 $L^{2}(\\Omega)$ 中的一个内积：\n$$\n\\left\\langle R^{-1}\\mathbf{r}, \\frac{\\partial H}{\\partial u}[\\delta u] \\right\\rangle_{\\mathbb{R}^{K}} = \\left\\langle \\left(\\frac{\\partial H}{\\partial u}\\right)^{*} [R^{-1}\\mathbf{r}], \\delta u \\right\\rangle_{L^{2}(\\Omega)}.\n$$\n伴随算子 $(\\frac{\\partial H}{\\partial u})^{*}$ 是各算子伴随的逆序复合：$(\\frac{\\partial H}{\\partial u})^{*} = S^{*} \\circ (\\Phi'(Su))^{*} \\circ W^{*}$。\n-   $W: L^2(\\Omega) \\to \\mathbb{R}^K$ 的伴随算子是 $W^{*}: \\mathbb{R}^K \\to L^2(\\Omega)$，由 $(W^{*}\\mathbf{z})(x) = \\sum_{i=1}^{K} z_{i}w_{i}(x)$ 给出。\n-   导数 $\\Phi'(Su)$ 是一个自伴的乘法算子。\n-   算子 $S$ 是一个具有对称核 $s(x, \\xi)$ 的积分算子，因此它是自伴的：$S^{*} = S$。\n令 $\\mathbf{z} = R^{-1}\\mathbf{r}$。伴随算子作用于 $\\mathbf{z}$ 的结果是：\n$$\n\\left(\\frac{\\partial H}{\\partial u}\\right)^{*}[\\mathbf{z}] = S\\left(\\alpha \\exp(\\alpha Su) \\cdot (W^{*}\\mathbf{z})\\right).\n$$\n关于 $u$ 的变分表达式变为：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u] = \\left\\langle \\left(\\frac{\\partial H}{\\partial u}\\right)^{*}[\\mathbf{z}], \\delta u \\right\\rangle_{L^{2}} + \\left\\langle -\\frac{d^{2}p}{dx^{2}} + p, \\delta u \\right\\rangle_{L^{2}} = 0.\n$$\n由于这对所有 $\\delta u \\in H_0^1(\\Omega)$ 均成立，我们得到 $p \\in H_0^1(\\Omega)$ 的**伴随方程**的强形式：\n$$\n-\\frac{d^{2}p}{dx^{2}} + p = - \\left(\\frac{\\partial H}{\\partial u}\\right)^{*}[R^{-1}(H(u,m)-y)], \\quad p(0)=0, p(1)=0.\n$$\n将 $p$ 定义为此方程的解，拉格朗日泛函的导数简化为：\n$$\ndJ(m)[\\delta m] = \\frac{\\partial \\mathcal{L}}{\\partial m}[\\delta m] = \\frac{\\partial J}{\\partial m}[\\delta m] - \\int_{0}^{1} p(x)\\delta m(x)\\, dx.\n$$\n我们现在计算 $J$ 关于 $m$ 的偏 Gâteaux 导数（保持 $u$ 固定）：\n$$\n\\frac{\\partial J}{\\partial m}[\\delta m] = \\left\\langle R^{-1}\\mathbf{r}(u,m), \\frac{\\partial H}{\\partial m}[\\delta m] \\right\\rangle_{\\mathbb{R}^{K}} + \\lambda \\int_{0}^{1} m(x)\\delta m(x)\\, dx.\n$$\n$H(u,m)$ 关于 $m$ 的偏导数就是 $\\frac{\\partial H}{\\partial m}[\\delta m] = E\\delta m$。\n因此，\n\\begin{align*}\n\\frac{\\partial J}{\\partial m}[\\delta m] = \\left\\langle R^{-1}\\mathbf{r}, E\\delta m \\right\\rangle_{\\mathbb{R}^{K}} + \\lambda \\langle m, \\delta m \\rangle_{L^{2}} \\\\\n= \\left\\langle E^{*}[R^{-1}\\mathbf{r}], \\delta m \\right\\rangle_{L^{2}} + \\langle \\lambda m, \\delta m \\rangle_{L^{2}} \\\\\n= \\left\\langle E^{*}[R^{-1}(H(u,m)-y)] + \\lambda m, \\delta m \\right\\rangle_{L^{2}}.\n\\end{align*}\n伴随算子 $E^{*}: \\mathbb{R}^{K} \\to L^{2}(\\Omega)$ 由 $(E^{*}\\mathbf{z})(x) = \\sum_{i=1}^{K} z_{i}e_{i}(x)$ 给出。\n将此代入 $dJ(m)[\\delta m]$ 的表达式中：\n$$\ndJ(m)[\\delta m] = \\left\\langle E^{*}[R^{-1}(H(u,m)-y)] + \\lambda m - p, \\delta m \\right\\rangle_{L^{2}}.\n$$\n根据 $L^{2}$-梯度的定义 $dJ(m)[\\delta m] = \\langle g, \\delta m \\rangle_{L^{2}}$，我们可以确定梯度 $g(x)$：\n$$\ng(x) = \\lambda m(x) - p(x) + \\left(E^{*}\\left[R^{-1}(H(u(m),m)-y)\\right]\\right)(x).\n$$\n展开包含 $E^{*}$ 的项可得到最终表达式。令向量 $\\mathbf{z} = R^{-1}(H(u(m),m)-y) \\in \\mathbb{R}^K$。那么，\n$$\ng(x) = \\lambda m(x) - p(x) + \\sum_{i=1}^{K} z_{i} e_{i}(x).\n$$\n此表达式按要求提供了梯度 $g(x)$，用参数 $m$、状态 $u$（通过 $H$）和伴随状态 $p$ 来表示。",
            "answer": "$$\n\\boxed{\ng(x) = \\lambda m(x) - p(x) + \\sum_{i=1}^{K} \\left[R^{-1}\\left(H(u(m),m) - y\\right)\\right]_{i} e_{i}(x)\n}\n$$"
        },
        {
            "introduction": "掌握了伴随梯度的推导之后，我们现在将其与参数可辨识性这一基本概念联系起来。本练习将从连续域转向离散的计算环境，你不仅需要推导梯度，还要利用正向和伴随算子的结构来刻画参数空间中的“不可观测”部分 。通过编程实现这一分析，你将具体理解在给定观测数据下，模型的哪些方面能够（或不能够）被有效约束。",
            "id": "3419154",
            "problem": "考虑一个在一维闭区间 $[0,1]$ 上的稳态扩散模型，其具有齐次狄利克雷边界条件。设该区间的内部被离散为 $n$ 个等距点，间距为 $h = 1/(n+1)$，并通过标准的二阶有限差分矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 来近似微分算子 $x \\mapsto -x''$，其中 $A$ 的定义为：对于有效索引，$A_{ii} = 2/h^2$ 且 $A_{i,i\\pm 1} = -1/h^2$。设参数向量 $m \\in \\mathbb{R}^n$ 表示一个离散化的源项，状态向量 $x \\in \\mathbb{R}^n$ 是线性系统\n$$\nA x = m\n$$\n的解。设 $S \\in \\mathbb{R}^{k \\times n}$ 是一个线性观测算子，它从 $n$ 个内部网格点中选择与 $k$ 个传感器位置对应的 $x$ 的坐标子集。正向映射 $F : \\mathbb{R}^n \\to \\mathbb{R}^k$ 定义为\n$$\nF(m) = S x(m) = S A^{-1} m.\n$$\n定义数据失配代价泛函\n$$\n\\Phi(m) = \\tfrac{1}{2} \\| F(m) - d \\|_2^2,\n$$\n其中 $d \\in \\mathbb{R}^k$ 是一个给定的数据向量，$\\|\\cdot\\|_2$ 表示欧几里得范数。本问题中的所有量都是无量纲的；不需要物理单位。\n\n您的任务是：\n\n1. 从有限维实希尔伯特空间中 Fréchet 导数的定义出发，推导正向映射 $F$ 在点 $m$ 沿方向 $h \\in \\mathbb{R}^n$ 的 Fréchet 导数 $F'(m)[h]$ 的表达式。请直接用扰动 $x_h$ 的线性化状态方程 $A x_h = h$ 和观测算子 $S$ 来表示您的推导。\n\n2. 使用基于拉格朗日框架的伴随状态法，推导 $\\Phi(m)$ 关于 $m$ 的梯度。引入一个伴随变量 $\\lambda \\in \\mathbb{R}^n$，建立拉格朗日函数\n$$\n\\mathcal{L}(x,m,\\lambda) = \\tfrac{1}{2} \\| S x - d \\|_2^2 + \\lambda^\\top (A x - m),\n$$\n并推导伴随方程以及 $\\nabla \\Phi(m)$ 关于 $\\lambda$ 的最终表达式。请从第一性原理出发，仔细论证每一步，并指明矩阵 $A$ 中编码的边界条件。\n\n3. 给出可辨识性零空间\n$$\n\\mathcal{N} = \\{ h \\in \\mathbb{R}^n : F'(m)[h] = 0 \\}\n$$\n的精确刻画。证明 $\\mathcal{N}$ 与 $m$ 无关，并且等于矩阵 $S A^{-1}$ 的零空间。证明以下构造性刻画：如果存在一个向量 $v \\in \\mathbb{R}^n$ 在所有传感器位置上均为零（即 $S v = 0$），那么扰动 $h = A v$ 属于 $\\mathcal{N}$。解释这与可辨识性以及伴随算子在检测不可观测方向中的作用有何关系。\n\n4. 设计一个算法，通过计算雅可比矩阵 $J = S A^{-1}$ 的奇异值分解来计算可辨识性零空间 $\\dim(\\mathcal{N})$ 的维数。请论证一个阈值规则：将严格大于最大奇异值 $10^{-10}$ 倍的奇异值计数为对秩有贡献，并将零空间维数定义为 $n$ 减去此数值秩。\n\n5. 实现一个完整的、可运行的程序，该程序能够：\n   - 为给定的 $n$ 和指定的传感器索引构建矩阵 $A$ 和选择矩阵 $S$。\n   - 计算雅可比矩阵 $J = S A^{-1}$ 及其奇异值。\n   - 通过上述阈值规则计算零空间维数。\n   - 通过构建一个在传感器位置上恒为零的向量 $v$，并形成 $h = A v$，然后计算 $\\| J h \\|_2$，来演示任务3中的构造性刻画。\n   - 汇总一个测试套件的结果，并以确切指定的格式打印它们。\n\n该测试套件必须覆盖以下情况：\n\n- 情况 1（正常路径）：$n = 8$，传感器位于所有内部网格点。这对应于 $k = n$ 且 $S$ 等于单位矩阵。输出整数 $\\dim(\\mathcal{N})$。\n- 情况 2（无数据边缘情况）：$n = 8$，未选择任何传感器。这对应于 $k = 0$ 且 $S$ 等于零行矩阵。输出整数 $\\dim(\\mathcal{N})$。\n- 情况 3（部分覆盖和构造性零空间）：$n = 10$，传感器位于索引 $\\{2,5,8\\}$（内部点之间的零基索引）。构造 $v \\in \\mathbb{R}^{10}$，使得对于传感器索引 $v_i = 0$，而在其他地方 $v_i$ 非零，设置 $h = A v$，并输出浮点数 $\\| J h \\|_2$。\n\n最终输出格式：您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表的结果，即 $[\\text{result}_1,\\text{result}_2,\\text{result}_3]$，其中 $\\text{result}_1$ 和 $\\text{result}_2$ 是整数，$\\text{result}_3$ 是浮点数。不应打印任何额外的文本。",
            "solution": "该问题陈述是关于一个由线性椭圆偏微分方程控制的反问题的灵敏度理论和计算的有效练习。它在科学上是合理的，数学上是适定的，并且解决它所需的所有信息都已提供。\n\n该问题要求从反问题的角度对一个一维稳态扩散问题进行多部分分析。我们将按顺序处理每个任务。\n\n离散化的状态方程由线性系统 $A x = m$ 给出，其中 $x, m \\in \\mathbb{R}^n$。矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 是在区间 $[0,1]$ 上带有齐次狄利克雷边界条件的负二阶导数算子 $-d^2/ds^2$ 的有限差分近似。该矩阵是对称正定的，因此是可逆的。从源参数 $m$ 到观测值的正向映射 $F(m)$ 是 $F(m) = S A^{-1} m$，其中 $S \\in \\mathbb{R}^{k \\times n}$ 是一个线性观测算子。\n\n### 1. 正向映射的 Fréchet 导数\n\n映射 $F$ 在点 $m$ 处的 Fréchet 导数是一个线性算子 $F'(m)$，使得\n$$ F(m+h) = F(m) + F'(m)[h] + o(\\|h\\|) $$\n对于一个小的扰动 $h \\in \\mathbb{R}^n$。\n\n正向映射由 $F(m) = S A^{-1} m$ 给出。由于 $A^{-1}$ 和 $S$ 是常数矩阵，映射 $F$ 关于 $m$ 是线性的。对于一个线性算子 $L$，其导数就是算子本身，即 $L'(m)[h] = L(h)$。因此，\n$$ F'(m)[h] = S A^{-1} h. $$\n为了按照要求用线性化状态方程来表示它，让我们考虑扰动 $h$ 对状态 $x$ 的影响。原始状态 $x(m)$ 满足 $A x(m) = m$。扰动后的状态 $x(m+h)$ 满足 $A x(m+h) = m+h$。\n\n令 $x_h = x(m+h) - x(m)$ 为状态的扰动。根据矩阵向量乘积的线性性质，\n$$ A x_h = A(x(m+h) - x(m)) = A x(m+h) - A x(m) = (m+h) - m = h. $$\n这就得到了状态扰动 $x_h$ 的线性化状态方程：\n$$ A x_h = h. $$\n由于 $A$ 是可逆的，所以 $x_h = A^{-1} h$。\n\n现在，我们考察观测值的扰动：\n$$ F(m+h) - F(m) = S x(m+h) - S x(m) = S(x(m+h) - x(m)) = S x_h. $$\n代入 $x_h$ 的表达式，我们有\n$$ F(m+h) - F(m) = S (A^{-1} h). $$\n$o(\\|h\\|)$ 项为零，因为关系是完全线性的。因此，$F$ 在 $m$ 处对方向 $h$ 的 Fréchet 导数是：\n$$ F'(m)[h] = S x_h, \\quad \\text{其中} \\quad A x_h = h. $$\n此表达式表明，观测值对源中扰动 $h$ 的灵敏度是通过用算子 $S$ 观测所产生的状态扰动 $x_h$ 来给出的。值得注意的是，因为 $F$ 是线性的，其导数 $F'(m)[\\cdot] = SA^{-1}(\\cdot)$ 是一个常数算子，与求导点 $m$ 无关。\n\n### 2. 通过伴随状态法计算代价泛函的梯度\n\n我们寻求代价泛函 $\\Phi(m) = \\frac{1}{2} \\| S x - d \\|_2^2$ 在状态方程约束 $A x = m$ 下的梯度。我们使用拉格朗日框架。拉格朗日函数给出如下：\n$$ \\mathcal{L}(x, m, \\lambda) = \\tfrac{1}{2} \\| S x - d \\|_2^2 + \\lambda^\\top (A x - m), $$\n其中 $\\lambda \\in \\mathbb{R}^n$ 是伴随状态或拉格朗日乘子。\n\n为了找到降阶泛函 $\\Phi(m)$ 的梯度，我们求 $\\mathcal{L}$ 关于其变量的驻点。梯度 $\\nabla \\Phi(m)$ 随后由 $\\mathcal{L}$ 关于 $m$ 的偏导数给出，该偏导数在状态和伴随方程的解处求值。\n\n1.  **关于 $\\lambda$ 的导数**：取 $\\mathcal{L}$ 关于 $\\lambda$ 的导数并将其设为零，可恢复状态方程。\n    $$ \\nabla_\\lambda \\mathcal{L}(x, m, \\lambda) = A x - m = 0 \\implies A x = m. $$\n\n2.  **关于 $x$ 的导数**：我们计算 $\\mathcal{L}$ 关于 $x$ 在任意方向 $\\delta x$ 上的方向导数。\n    $$ \\delta_x \\mathcal{L} = \\frac{d}{d\\epsilon} \\mathcal{L}(x+\\epsilon\\delta x, m, \\lambda) \\Big|_{\\epsilon=0}. $$\n    $$ \\delta_x \\mathcal{L} = \\frac{d}{d\\epsilon} \\left( \\tfrac{1}{2} (S(x+\\epsilon\\delta x) - d)^\\top (S(x+\\epsilon\\delta x) - d) + \\lambda^\\top (A(x+\\epsilon\\delta x) - m) \\right) \\Big|_{\\epsilon=0} $$\n    $$ = (S x - d)^\\top S \\delta x + \\lambda^\\top A \\delta x = (S^\\top(S x - d) + A^\\top \\lambda)^\\top \\delta x. $$\n    为了使此式对所有 $\\delta x$ 都为零，我们必须有 $\\nabla_x \\mathcal{L} = S^\\top(S x - d) + A^\\top \\lambda = 0$。这就得到了**伴随方程**：\n    $$ A^\\top \\lambda = -S^\\top(S x - d). $$\n    根据定义，矩阵 $A$ 是对称的（$A_{ij} = A_{ji}$），所以 $A^\\top = A$。伴随方程简化为：\n    $$ A \\lambda = -S^\\top(S x - d). $$\n    矩阵 $A$ 的结构意味着其对应的连续问题具有齐次狄利克雷边界条件。由于伴随算子同样是 $A$，伴随变量 $\\lambda$ 满足相同的边界条件。\n\n3.  **关于 $m$ 的导数**：我们计算 $\\mathcal{L}$ 关于 $m$ 在任意方向 $\\delta m$ 上的方向导数。\n    $$ \\delta_m \\mathcal{L} = \\frac{d}{d\\epsilon} \\mathcal{L}(x, m+\\epsilon\\delta m, \\lambda) \\Big|_{\\epsilon=0}. $$\n    $$ \\delta_m \\mathcal{L} = \\frac{d}{d\\epsilon} \\left( \\tfrac{1}{2} \\| S x - d \\|_2^2 + \\lambda^\\top (A x - (m+\\epsilon\\delta m)) \\right) \\Big|_{\\epsilon=0} = -\\lambda^\\top \\delta m. $$\n    降阶泛函 $\\Phi(m)$ 的梯度由 $\\nabla_m \\mathcal{L}$ 给出，它是满足 $(\\nabla_m \\mathcal{L})^\\top \\delta m = -\\lambda^\\top \\delta m$ 对所有 $\\delta m$ 成立的向量。\n    因此，梯度是：\n    $$ \\nabla \\Phi(m) = -\\lambda. $$\n\n总之，$\\Phi(m)$ 的梯度通过以下三个步骤计算：\n(i) 求解状态方程以获得 $x$：$A x = m$。\n(ii) 求解伴随方程以获得 $\\lambda$：$A \\lambda = -S^\\top(S x - d)$。\n(iii) 梯度为 $\\nabla \\Phi(m) = -\\lambda$。\n\n### 3. 可辨识性零空间\n\n可辨识性零空间 $\\mathcal{N}$ 是指那些不可观测的参数扰动 $h$ 的集合，即它们不会在正向映射的输出中产生任何变化。其定义为：\n$$ \\mathcal{N} = \\{ h \\in \\mathbb{R}^n : F'(m)[h] = 0 \\}. $$\n从任务1可知，$F'(m)[h] = S A^{-1} h$。因此，条件 $F'(m)[h] = 0$ 即为 $S A^{-1} h = 0$。这是矩阵 $J = S A^{-1}$ 的零空间（或核）的定义。\n$$ \\mathcal{N} = \\text{ker}(S A^{-1}). $$\n由于矩阵 $S$ 和 $A$ 是常数，雅可比矩阵 $J = S A^{-1}$ 也是常数。它不依赖于点 $m$。因此，其零空间 $\\mathcal{N}$ 与 $m$ 无关。\n\n**构造性刻画：**\n我们需要证明，如果存在一个向量 $v \\in \\mathbb{R}^n$ 使得 $S v = 0$，那么扰动 $h = A v$ 属于 $\\mathcal{N}$。\n一个满足 $S v = 0$ 的向量 $v$ 对应于一个在所有传感器位置上都为零的状态。\n让我们通过将算子 $J = S A^{-1}$ 应用于 $h = A v$ 来测试它是否在零空间 $\\mathcal{N}$ 中：\n$$ J h = (S A^{-1}) h = (S A^{-1}) (A v) = S (A^{-1} A) v = S I v = S v. $$\n根据我们的前提，$S v = 0$。因此，$J h = 0$，这意味着 $h \\in \\text{ker}(J) = \\mathcal{N}$。证明完毕。\n\n这种刻画提供了一种构造零空间元素的方法。任何能产生一个对传感器“不可见”的状态 $v$（$S v = 0$）的源项 $h$，其本身就是一个不可观测的参数扰动。基于梯度的优化算法将无法恢复或校正位于此零空间 $\\mathcal{N}$ 中的参数 $m$ 的分量。这是因为梯度 $\\nabla \\Phi(m) = -\\lambda = A^{-1} S^\\top (S x - d)$ 属于伴随算子 $J^\\top = (S A^{-1})^\\top = (A^{-1})^\\top S^\\top = A^{-1} S^\\top$ 的值域。根据线性代数基本定理，伴随算子的值域是原始算子零空间的正交补：$\\text{range}(J^\\top) \\perp \\text{ker}(J)$。因此，基于梯度的更新总是与不可观测方向正交。\n\n### 4. 计算零空间维数的算法\n\n可辨识性零空间的维数 $\\dim(\\mathcal{N})$ 是雅可比矩阵 $J = S A^{-1}$ 的零度。根据矩阵 $J \\in \\mathbb{R}^{k \\times n}$ 的秩-零度定理：\n$$ \\text{rank}(J) + \\text{nullity}(J) = n. $$\n因此，$\\dim(\\mathcal{N}) = \\text{nullity}(J) = n - \\text{rank}(J)$。\n\n矩阵的秩可以通过其奇异值可靠地计算。秩是非零奇异值的数量。在数值计算中，由于浮点数的不精确性，我们必须使用一个阈值来区分非零和零奇异值。问题提供了一个特定的阈值规则。\n\n算法如下：\n1.  **构造矩阵**：对于给定的网格大小 $n$ 和一组传感器位置，构造有限差分矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 和观测矩阵 $S \\in \\mathbb{R}^{k \\times n}$。\n2.  **计算雅可比矩阵**：计算雅可比矩阵 $J = S A^{-1}$。在数值上，这可以通过首先使用 `numpy.linalg.inv` 计算 $A^{-1}$，然后乘以 $S$ 来完成。\n3.  **计算奇异值**：计算 $J$ 的奇异值分解（SVD）。令奇异值为 $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_p \\ge 0$，其中 $p = \\min(k, n)$。\n4.  **确定数值秩**：找到最大的奇异值 $\\sigma_1$。应用阈值规则：数值秩 $\\text{rank}_{\\text{num}}(J)$ 是满足 $\\sigma_i > 10^{-10} \\sigma_1$ 的奇异值 $\\sigma_i$ 的数量。如果 $J$ 没有非零奇异值（例如，如果它是零矩阵），其秩为 $0$。\n5.  **计算零空间维数**：零空间的维数是 $\\dim(\\mathcal{N}) = n - \\text{rank}_{\\text{num}}(J)$。\n\n### 5. 实现\n最后一步是为指定的测试用例用 Python 实现此算法。代码将构造矩阵，计算雅可比矩阵及其奇异值，然后根据每个情况的要求确定零空间维数或测试构造性刻画。",
            "answer": "```python\nimport numpy as np\n\ndef construct_A(n):\n    \"\"\"Constructs the finite-difference matrix A for a given n.\"\"\"\n    if n == 0:\n        return np.array([[]])\n    h = 1.0 / (n + 1)\n    h2_inv = 1.0 / (h * h)\n    \n    A = np.zeros((n, n))\n    \n    # Fill diagonal\n    np.fill_diagonal(A, 2.0 * h2_inv)\n    \n    # Fill off-diagonals\n    if n > 1:\n        diag_indices = np.arange(n - 1)\n        A[diag_indices, diag_indices + 1] = -1.0 * h2_inv\n        A[diag_indices + 1, diag_indices] = -1.0 * h2_inv\n        \n    return A\n\ndef construct_S(n, sensor_indices):\n    \"\"\"Constructs the observation matrix S for given n and sensor indices.\"\"\"\n    k = len(sensor_indices)\n    if k == 0:\n        return np.zeros((0, n))\n    \n    S = np.zeros((k, n))\n    for i, sensor_idx in enumerate(sensor_indices):\n        if 0 <= sensor_idx < n:\n            S[i, sensor_idx] = 1.0\n    return S\n\ndef get_nullspace_dim(n, sensor_indices):\n    \"\"\"Computes the dimension of the identifiability nullspace.\"\"\"\n    A = construct_A(n)\n    S = construct_S(n, sensor_indices)\n\n    if n == 0:\n        return 0\n    if S.shape[0] == 0: # No sensors\n        return n\n    \n    A_inv = np.linalg.inv(A)\n    J = S @ A_inv\n    \n    # singular values of J\n    singular_values = np.linalg.svd(J, compute_uv=False)\n    \n    if len(singular_values) == 0:\n        numerical_rank = 0\n    else:\n        sigma_max = np.max(singular_values)\n        if sigma_max == 0:\n            numerical_rank = 0\n        else:\n            threshold = 1e-10 * sigma_max\n            numerical_rank = np.sum(singular_values > threshold)\n\n    nullity = n - numerical_rank\n    return nullity\n\ndef solve():\n    \"\"\"\n    Solves the problem for the three specified test cases and prints the results.\n    \"\"\"\n    results = []\n\n    # Case 1: n = 8, sensors at all points\n    n1 = 8\n    sensor_indices1 = list(range(n1))\n    dim_N1 = get_nullspace_dim(n1, sensor_indices1)\n    results.append(int(dim_N1))\n\n    # Case 2: n = 8, no sensors\n    n2 = 8\n    sensor_indices2 = []\n    dim_N2 = get_nullspace_dim(n2, sensor_indices2)\n    results.append(int(dim_N2))\n\n    # Case 3: n = 10, sensors at {2, 5, 8}, constructive nullspace check\n    n3 = 10\n    sensor_indices3 = [2, 5, 8]\n    \n    A3 = construct_A(n3)\n    S3 = construct_S(n3, sensor_indices3)\n    \n    A3_inv = np.linalg.inv(A3)\n    J3 = S3 @ A3_inv\n    \n    # Construct vector v that is zero at sensor locations\n    v3 = np.ones(n3)\n    v3[sensor_indices3] = 0\n    \n    # Construct h = Av\n    h3 = A3 @ v3\n    \n    # Compute the norm ||Jh||_2\n    norm_Jh = np.linalg.norm(J3 @ h3)\n    results.append(float(norm_Jh))\n\n    # Print a single line of output in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "我们最后的练习将解决一个在大型时变系统中应用伴随方法的关键计算瓶颈：内存限制。为了逆时计算伴随状态，存储整个正向模拟轨迹往往是不可行的。本练习引入了“检查点”技术，它通过增加计算时间来换取内存节省，并要求你实现并分析其性能 。掌握这种方法对于将基于伴随的优化方法扩展到天气预报和气候模拟等实际大规模问题至关重要。",
            "id": "3419136",
            "problem": "考虑一个离散时间、时变线性状态演化模型，该模型通过显式欧拉时间步进格式为状态向量 $x_n \\in \\mathbb{R}^d$（索引 $n \\in \\{0,1,\\dots,N\\}$）定义：\n$$\nx_{n+1} = x_n + \\Delta t \\left( A x_n + \\theta B x_n + s \\right),\n$$\n其中 $A \\in \\mathbb{R}^{d \\times d}$ 和 $B \\in \\mathbb{R}^{d \\times d}$ 是固定矩阵，$s \\in \\mathbb{R}^d$ 是一个固定源项，$\\Delta t > 0$ 是一个固定的时间步长，$\\theta \\in \\mathbb{R}$ 是一个标量参数。初始条件为 $x_0 = 0$。设观测算子为 $C \\in \\mathbb{R}^{d \\times d}$，并设观测值 $y_n \\in \\mathbb{R}^d$（对于 $n \\in \\{0,1,\\dots,N\\}$）是由具有固定“真实”参数 $\\theta_{\\mathrm{true}}$ 和相同初始条件的同一模型生成的，且无噪声：\n$$\ny_n = C x_n(\\theta_{\\mathrm{true}}).\n$$\n定义每个时间点的数据失配为\n$$\n\\ell_n(x_n) = \\tfrac{1}{2} \\left\\| C x_n - y_n \\right\\|_2^2,\n$$\n总目标函数定义为\n$$\nJ(\\theta) = \\sum_{n=0}^{N} \\ell_n(x_n(\\theta)) + \\tfrac{\\alpha}{2} \\theta^2,\n$$\n其中 $\\alpha \\ge 0$ 是一个固定的正则化权重。\n\n您将使用伴随状态法，在内存受限的情况下通过检查点技术计算 $J$ 对 $\\theta$ 的敏感度，并通过与有限差分近似的比较来验证其正确性。\n\n推导的基本定义和假设：\n- 泛函 $\\Phi$ 在参数 $u$ 处沿方向 $h$ 的 Fréchet 导数是满足 $\\Phi(u+h) - \\Phi(u) = D\\Phi(u)[h] + o(\\|h\\|)$（当 $\\|h\\|\\to 0$ 时）的线性映射 $D\\Phi(u)[h]$。\n- 对于离散时间映射 $F_\\theta(x) = x + \\Delta t \\left( A x + \\theta B x + s \\right)$，关于状态的雅可比矩阵是 $\\partial F_\\theta / \\partial x = I + \\Delta t (A + \\theta B)$，关于参数的偏导数是 $\\partial F_\\theta / \\partial \\theta (x) = \\Delta t\\, B x$。\n- 伴随递推通过对由目标函数 $J$ 和离散动力学约束构成的拉格朗日量施加平稳性条件而产生，其中引入了伴随变量 $\\lambda_n \\in \\mathbb{R}^d$。\n\n检查点与内存：\n- 当伴随方程从 $n = N$ 向 $n = 0$ 反向求解时，需要访问前向状态 $x_n$ 来评估梯度贡献和伴随更新。当 $N$ 很大时，存储所有 $x_n$ 可能不可行。\n- 检查点策略根据内存限制 $\\mathsf{M}$（可存储状态的最大数量）存储选定的状态子集 $\\{x_{n_k}\\}$（在索引 $\\{n_k\\}$ 处）。当所需的状态 $x_n$ 未被存储时，它将通过从最近的先前存储的检查点索引 $n_k \\le n$ 开始重新运行前向模型直到 $n$ 来按需重新计算。在原始前向遍之外执行的额外前向步数被记录为重计算计数。\n\n任务：\n1. 根据提供的基本定义，推导 $\\lambda_n$ 的离散伴随递推关系式以及梯度 $\\mathrm{d}J/\\mathrm{d}\\theta$ 关于前向状态、伴随变量和模型导数的表达式。\n2. 实现一个程序，该程序：\n   - 通过在 $\\theta_{\\mathrm{true}}$ 处运行前向模型来生成观测值 $y_n$。\n   - 使用伴随状态法，在均匀检查点策略下计算 $\\mathrm{d}J/\\mathrm{d}\\theta$。该策略在从 $0$ 到 $N$（含）的均匀间隔的时间索引处存储 $\\mathsf{M}$ 个状态，并在间距不精确时进行去重。当需要的 $x_n$ 未存储时，从最近的先前检查点重新计算它。\n   - 统计在伴随时间重计算期间，超出初始 $N$ 步前向运行之外执行的额外前向步数的总数。\n   - 使用小步长 $\\varepsilon$ 计算 $\\mathrm{d}J/\\mathrm{d}\\theta$ 的中心有限差分近似：\n     $$\n     \\mathrm{d}J/\\mathrm{d}\\theta \\approx \\frac{J(\\theta+\\varepsilon) - J(\\theta-\\varepsilon)}{2\\varepsilon}.\n     $$\n   - 报告基于伴随方法的梯度与有限差分梯度之间的绝对误差、重计算计数、存储的状态数以及一个布尔值，该值指示绝对误差是否低于指定的容差。\n3. 使用以下固定的模型设置：\n   - 维度 $d = 3$。\n   - 时间步长 $\\Delta t = 0.1$。\n   - 矩阵\n     $$\n     A = \\begin{bmatrix}\n     -0.1  & 0.2 & 0 \\\\\n     -0.2 & -0.3 & 0.1 \\\\\n     0 & -0.1 & -0.2\n     \\end{bmatrix},\\quad\n     B = \\begin{bmatrix}\n     0.5 & 0 & 0 \\\\\n     0 & 0.1 & 0 \\\\\n     0 & 0 & 0.3\n     \\end{bmatrix},\\quad\n     C = I_3,\n     $$\n     和源项\n     $$\n     s = \\begin{bmatrix} 0.1 \\\\ -0.05 \\\\ 0.2 \\end{bmatrix}.\n     $$\n   - 初始条件 $x_0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$。\n   - 真实参数 $\\theta_{\\mathrm{true}} = 0.3$。\n4. 实现以下测试套件（每个测试用例提供 $N$、$\\mathsf{M}$、$\\theta$、$\\alpha$ 和 $\\varepsilon$）：\n   - 测试用例 1 (一般情况): $N = 50$, $\\mathsf{M} = 6$, $\\theta = 0.2$, $\\alpha = 0.01$, $\\varepsilon = 10^{-6}$。\n   - 测试用例 2 (全内存边界): $N = 50$, $\\mathsf{M} = 51$, $\\theta = 0.2$, $\\alpha = 0.01$, $\\varepsilon = 10^{-6}$。\n   - 测试用例 3 (最小内存边缘情况): $N = 50$, $\\mathsf{M} = 1$, $\\theta = 0.2$, $\\alpha = 0.01$, $\\varepsilon = 10^{-6}$。\n   - 测试用例 4 (短时域边界): $N = 1$, $\\mathsf{M} = 1$, $\\theta = 0.2$, $\\alpha = 0.01$, $\\varepsilon = 10^{-8}$。\n5. 对于每个测试用例，您的程序必须按以下列表格式生成结果\n   $$\n   [g_{\\mathrm{adj}}, g_{\\mathrm{fd}}, \\lvert g_{\\mathrm{adj}} - g_{\\mathrm{fd}} \\rvert, \\text{recompute\\_count}, \\text{memory\\_used}, \\text{match}],\n   $$\n   其中 $g_{\\mathrm{adj}}$ 是基于伴随方法的梯度，$g_{\\mathrm{fd}}$ 是有限差分梯度，$\\text{recompute\\_count}$ 是一个整数，$\\text{memory\\_used}$ 是存储状态数的整数，$\\text{match}$ 是一个布尔值，指示对于测试用例 1-3，绝对误差是否小于 $10^{-8}$，对于测试用例 4，是否小于 $10^{-10}$。\n6. 最终输出格式要求：您的程序应生成单行输出，其中包含所有测试用例的结果，格式为一个逗号分隔的列表，并用方括号括起来，每个测试用例的结果本身按第 5 项中指定的列表格式化。例如，一个有效的形状是\n   $$\n   [[\\cdots],[\\cdots],[\\cdots],[\\cdots]].\n   $$\n\n此问题不涉及物理单位或角度单位；所有量均为无单位实数。输出值必须是指定的浮点数、整数或布尔值。",
            "solution": "此问题经评估有效。它在科学上基于最优控制和敏感度分析（特别是伴随状态法）的既定理论，问题陈述清晰客观、适定，并为获得唯一解提供了所有必要的数据和定义。\n\n### 1. 伴随方程和梯度的推導\n\n目标是计算目标函数：\n$$\nJ(\\theta) = \\sum_{n=0}^{N} \\ell_n(x_n(\\theta)) + \\tfrac{\\alpha}{2} \\theta^2 = \\sum_{n=0}^{N} \\tfrac{1}{2} \\left\\| C x_n(\\theta) - y_n \\right\\|_2^2 + \\tfrac{\\alpha}{2} \\theta^2\n$$\n的梯度 $\\mathrm{d}J/\\mathrm{d}\\theta$。\n状态向量 $x_n$ 根据离散时间动力学演化，这构成了对优化问题的约束：\n$$\nx_{n+1} = F_\\theta(x_n) = x_n + \\Delta t \\left( A x_n + \\theta B x_n + s \\right) \\quad \\text{for } n = 0, \\dots, N-1\n$$\n初始条件为 $x_0=0$。\n\n我们使用拉格朗日乘子法来推导伴随方程。拉格朗日量 $\\mathcal{L}$ 是通过将目标函数与约束项组合而成的，其中约束项由伴随变量（拉格朗日乘子）$\\lambda_{n+1} \\in \\mathbb{R}^d$ 加权：\n$$\n\\mathcal{L}(x_1, \\dots, x_N, \\theta, \\lambda_1, \\dots, \\lambda_N) = J(\\theta) - \\sum_{n=0}^{N-1} \\lambda_{n+1}^T \\left( x_{n+1} - F_\\theta(x_n) \\right)\n$$\n代入 $J(\\theta)$ 的表达式，并注意到 $x_0=0$ 是一个固定条件：\n$$\n\\mathcal{L} = \\left(\\sum_{n=0}^{N} \\ell_n(x_n)\\right) + \\tfrac{\\alpha}{2} \\theta^2 - \\sum_{n=0}^{N-1} \\lambda_{n+1}^T \\left( x_{n+1} - F_\\theta(x_n) \\right)\n$$\n对于满足状态方程的轨迹 $\\{x_n\\}$，如果伴随变量满足对每个状态变量 $x_n$ ($n=1, \\dots, N$) 的平稳性条件 $\\partial \\mathcal{L} / \\partial x_n = 0$，则梯度 $\\mathrm{d}J/\\mathrm{d}\\theta$ 是拉格朗日量关于 $\\theta$ 的偏导数。\n\n**伴随方程：**\n平稳性条件定义了伴随递推关系。\n对于最终状态 $x_N$ ($n=N$)：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_N} = \\nabla_{x_N} \\ell_N(x_N) - \\lambda_N^T = 0 \\implies \\lambda_N = (\\nabla_{x_N} \\ell_N(x_N))^T\n$$\n对于中间状态 $x_n$ ($n=1, \\dots, N-1$)：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_n} = \\nabla_{x_n} \\ell_n(x_n) - \\lambda_n^T + \\lambda_{n+1}^T \\frac{\\partial F_\\theta(x_n)}{\\partial x_n} = 0\n$$\n这得到了伴随变量的反向递推关系：\n$$\n\\lambda_n = \\left(\\frac{\\partial F_\\theta(x_n)}{\\partial x_n}\\right)^T \\lambda_{n+1} + (\\nabla_{x_n} \\ell_n(x_n))^T\n$$\n让我们确定所需的导数：\n- 失配项 $\\ell_n(x_n) = \\frac{1}{2}(Cx_n-y_n)^T(Cx_n-y_n)$ 的梯度是 $\\nabla_{x_n}\\ell_n(x_n) = (Cx_n-y_n)^T C$。其转置是 $(\\nabla_{x_n}\\ell_n(x_n))^T = C^T(Cx_n-y_n)$。\n- 状态转移映射 $F_\\theta$ 关于状态的雅可比矩阵是 $\\frac{\\partial F_\\theta(x_n)}{\\partial x_n} = I + \\Delta t(A + \\theta B)$。我们将此矩阵记为 $M_\\theta$。\n\n完整的伴随系统由一个反向时间演化定义：\n$n=N$ 时的终端条件：\n$$\n\\lambda_N = C^T (C x_N - y_N)\n$$\n$n = N-1, \\dots, 0$ 的伴随递推：\n$$\n\\lambda_n = (I + \\Delta t(A + \\theta B))^T \\lambda_{n+1} + C^T (C x_n - y_n)\n$$\n请注意，我们将递推进行到 $n=0$，得到了 $\\lambda_0$，它表示 $J$ 对初始条件 $x_0$ 的敏感度。虽然这对于参数梯度并非严格必需，但它定义了完整的伴随状态轨迹。\n\n**梯度表达式：**\n梯度 $\\mathrm{d}J/\\mathrm{d}\\theta$ 通过对拉格朗日量关于 $\\theta$ 求导得到：\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}\\theta} = \\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\frac{\\partial}{\\partial\\theta} \\left( \\tfrac{\\alpha}{2}\\theta^2 \\right) + \\sum_{n=0}^{N-1} \\lambda_{n+1}^T \\frac{\\partial F_\\theta(x_n)}{\\partial \\theta}\n$$\n$F_\\theta(x_n)$ 关于 $\\theta$ 的偏导数是：\n$$\n\\frac{\\partial F_\\theta(x_n)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta} \\left( x_n + \\Delta t (Ax_n + \\theta Bx_n + s) \\right) = \\Delta t B x_n\n$$\n将此代入梯度表达式，得到基于伴随方法的梯度的最终公式：\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}\\theta} = \\alpha \\theta + \\Delta t \\sum_{n=0}^{N-1} \\lambda_{n+1}^T B x_n\n$$\n\n### 2. 计算算法\n\n计算过程如下：\n1.  **生成观测值：** “真实”观测值 $y_n$ 是通过使用参数 $\\theta_{\\mathrm{true}}$ 运行前向模型生成的。\n    - 设置 $x_0 = 0$。\n    - 对于 $n=0, \\dots, N-1$：$x_{n+1} = (I + \\Delta t(A+\\theta_{\\mathrm{true}} B))x_n + \\Delta t s$。\n    - 对于 $n=0, \\dots, N$：$y_n = C x_n$。\n\n2.  **有限差分梯度：** 为了验证正确性，计算中心有限差分近似 $g_{\\mathrm{fd}}$：\n    - 定义一个函数 `compute_J(theta_eval)`，该函数使用 `theta_eval` 运行前向模型以获得轨迹 $\\{x_n(\\theta_{\\mathrm{eval}})\\}$，然后计算 $J(\\theta_{\\mathrm{eval}})$。\n    - 计算 $g_{\\mathrm{fd}} = \\frac{J(\\theta+\\varepsilon) - J(\\theta-\\varepsilon)}{2\\varepsilon}$。\n\n3.  **带检查点的伴随状态梯度：**\n    a.  **前向遍与检查点设置：**\n        - 给定参数 $N$ 和 $\\mathsf{M}$，确定检查点索引。均匀策略在 `np.linspace(0, N, M, dtype=int)` 给出的索引 `k` 处存储状态，并移除重复项。设该集合为 $\\mathcal{K}$。\n        - 对于给定的 $\\theta$，从 $n=0$ 到 $N$ 运行前向模型。当且仅当 $n \\in \\mathcal{K}$ 时，将状态向量 $x_n$ 存储在内存中。这构成了初始的 $N$ 步前向遍。\n\n    b.  **反向遍与梯度组装：**\n        - 初始化梯度 $g_{\\mathrm{adj}} = \\alpha \\theta$ 和重计算步数计数器为 $0$。\n        - 为了开始递推，获取 $x_N$。如果 $N \\in \\mathcal{K}$，则从内存中检索。否则，找到满足 $n_k  N$ 的最大检查点索引 $n_k \\in \\mathcal{K}$，检索 $x_{n_k}$，并从 $n_k$ 到 $N$ 重新运行前向模型。重计算的步数 $N-n_k$ 被加到计数器中。\n        - 初始化伴随变量：$\\lambda_N = C^T(Cx_N - y_N)$。\n        - 从 $n=N$ 向下迭代到 $1$：\n            i. 在步骤 $n$，我们有伴随状态 $\\lambda_n$。\n            ii. 获取状态 $x_{n-1}$。如果 $n-1 \\notin \\mathcal{K}$，找到最近的先前检查点 $n_k \\le n-1$，检索 $x_{n_k}$，并重新运行前向模型 $(n-1) - n_k$ 步。将此步数添加到重计算计数器中。\n            iii. 更新梯度：$g_{\\mathrm{adj}} \\leftarrow g_{\\mathrm{adj}} + \\Delta t \\lambda_n^T B x_{n-1}$。\n            iv. 为下一次迭代更新伴随变量：$\\lambda_{n-1} = (I + \\Delta t(A+\\theta B))^T \\lambda_n + C^T(Cx_{n-1} - y_{n-1})$。\n        - 最终值 $g_{\\mathrm{adj}}$ 即为所求梯度。\n\n    c.  **输出：** 报告计算出的梯度 $g_{\\mathrm{adj}}$ 和 $g_{\\mathrm{fd}}$、它们的绝对差、总重计算计数、存储的状态数 (`memory_used`) 以及一个指示误差是否低于指定容差的布尔值。\n\n此过程正确地实现了内存约束下的伴随状态法，使用检查点技术来管理反向遍期间所需的前向轨迹的存储。",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases for the adjoint-state method problem.\n    \"\"\"\n\n    # Fixed model settings\n    d = 3\n    dt = 0.1\n    A = np.array([\n        [-0.1, 0.2, 0.0],\n        [-0.2, -0.3, 0.1],\n        [0.0, -0.1, -0.2]\n    ])\n    B = np.array([\n        [0.5, 0.0, 0.0],\n        [0.0, 0.1, 0.0],\n        [0.0, 0.0, 0.3]\n    ])\n    C = np.eye(d)\n    s = np.array([0.1, -0.05, 0.2])\n    x0 = np.zeros(d)\n    theta_true = 0.3\n\n    # Test suite\n    test_cases = [\n        # (N, M, theta, alpha, epsilon, tol)\n        (50, 6, 0.2, 0.01, 1e-6, 1e-8),\n        (50, 51, 0.2, 0.01, 1e-6, 1e-8),\n        (50, 1, 0.2, 0.01, 1e-6, 1e-8),\n        (1, 1, 0.2, 0.01, 1e-8, 1e-10),\n    ]\n\n    all_results = []\n\n    for N, M_mem, theta, alpha, epsilon, tol in test_cases:\n\n        # Step 1: Generate observations y_n using theta_true\n        y_obs = []\n        x_true_traj = [x0]\n        x = x0.copy()\n        \n        M_true = np.eye(d) + dt * (A + theta_true * B)\n\n        for _ in range(N):\n            x = M_true @ x + dt * s\n            x_true_traj.append(x)\n        \n        for x_val in x_true_traj:\n            y_obs.append(C @ x_val)\n\n        # Helper function to run the forward model for a given theta\n        def run_forward(theta_eval):\n            x_traj = [x0]\n            x = x0.copy()\n            M_eval = np.eye(d) + dt * (A + theta_eval * B)\n            for _ in range(N):\n                x = M_eval @ x + dt * s\n                x_traj.append(x)\n            return x_traj\n\n        # Helper function to compute the objective function J(theta)\n        def compute_J(theta_eval):\n            x_traj = run_forward(theta_eval)\n            misfit = 0.0\n            for n in range(N + 1):\n                misfit += 0.5 * np.linalg.norm(C @ x_traj[n] - y_obs[n])**2\n            return misfit + 0.5 * alpha * theta_eval**2\n\n        # Step 2: Compute finite-difference gradient\n        J_plus = compute_J(theta + epsilon)\n        J_minus = compute_J(theta - epsilon)\n        g_fd = (J_plus - J_minus) / (2 * epsilon)\n\n        # Step 3: Compute adjoint-state gradient with checkpointing\n        \n        # 3a: Forward pass and checkpointing\n        checkpoint_indices = sorted(list(set(np.linspace(0, N, M_mem, dtype=int))))\n        memory_used = len(checkpoint_indices)\n        checkpoints = {}\n        \n        # This forward pass is just for populating checkpoints\n        x = x0.copy()\n        if 0 in checkpoint_indices:\n            checkpoints[0] = x0.copy()\n            \n        M_theta = np.eye(d) + dt * (A + theta * B)\n\n        for n in range(N):\n            x = M_theta @ x + dt * s\n            if (n + 1) in checkpoint_indices:\n                checkpoints[n + 1] = x.copy()\n        \n        recompute_count = 0\n\n        # Helper for on-demand state recomputation\n        memoized_states = {}\n        def get_state(n):\n            nonlocal recompute_count\n            if n in memoized_states:\n                return memoized_states[n]\n            if n in checkpoints:\n                memoized_states[n] = checkpoints[n]\n                return checkpoints[n]\n\n            # Find nearest preceding checkpoint\n            start_n = 0\n            for k in checkpoint_indices:\n                if k  n:\n                    start_n = k\n                else:\n                    break\n            \n            x_re = checkpoints[start_n].copy()\n            for i in range(start_n, n):\n                x_re = M_theta @ x_re + dt * s\n                recompute_count += 1\n            \n            memoized_states[n] = x_re\n            return x_re\n\n        # 3b: Backward pass\n        g_adj = alpha * theta\n        M_theta_T = M_theta.T\n\n        # Initialize adjoint at n=N\n        xN = get_state(N)\n        lam = C.T @ (C @ xN - y_obs[N])\n\n        # Backward recursion from n=N down to 1\n        for n in range(N, 0, -1):\n            # At start of loop, lam is lambda_n\n            \n            # Get state x_{n-1} for gradient and adjoint update\n            x_prev = get_state(n-1)\n\n            # Update gradient: term is dt * lambda_n^T * B * x_{n-1}\n            g_adj += dt * (lam.T @ B @ x_prev)\n            \n            # Update adjoint for next iteration: compute lambda_{n-1}\n            lam = M_theta_T @ lam + C.T @ (C @ x_prev - y_obs[n-1])\n\n        abs_err = abs(g_adj - g_fd)\n        match = abs_err  tol\n\n        all_results.append([g_adj, g_fd, abs_err, recompute_count, memory_used, match])\n\n    # Final print statement\n    result_str = ','.join([str(res) for res in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```"
        }
    ]
}