{
    "hands_on_practices": [
        {
            "introduction": "This practice provides a foundational, hands-on experience in developing the core components of a variational data assimilation system. You will implement the tangent-linear and adjoint models for a physically-motivated, albeit simplified, radiative transfer model. Successfully completing the verification tests—the Taylor test for the tangent-linear model and the inner-product identity for the adjoint—is a critical skill that ensures the accuracy of the gradient calculations essential for minimization algorithms .",
            "id": "3398784",
            "problem": "Consider a one-dimensional, plane-parallel, purely absorbing, non-scattering discrete-layer radiative transfer forward/observation model $H:\\mathbb{R}^N\\to\\mathbb{R}^K$ defined by the following physically consistent construction based on the Beer–Lambert law. Let the state vector be $x\\in\\mathbb{R}^N$, with layer optical depths given by $\\tau_i=\\exp(x_i)$ to enforce positivity. For each spectral channel $k=1,\\dots,K$, define channel-layer optical depths $\\Delta\\tau_{k,i}=w_{k,i}\\,\\tau_i$, where $w_{k,i}>0$ are known channel-layer weighting coefficients. Let the top-of-atmosphere solar source term be $s_k>0$ for channel $k$, and let each layer $i$ have a channel-dependent emission/source term $b_{k,i}\\ge 0$ assumed constant within the layer. Define the upward transmission from the top of layer $i$ to the top of layer $i+1$ as $Z_{k,i}=\\exp(-\\Delta\\tau_{k,i})$, the cumulative upward transmission to just above layer $i$ as $T_{k,i-1}=\\prod_{j=1}^{i-1} Z_{k,j}$ with $T_{k,0}=1$, and the layer $i$ emission escape fraction as $L_{k,i}=1-Z_{k,i}$. The top-of-atmosphere upwelling radiance for channel $k$ is then\n$$\ny_k = s_k \\exp\\Big(-\\sum_{i=1}^N \\Delta\\tau_{k,i}\\Big) + \\sum_{i=1}^N b_{k,i}\\,L_{k,i}\\,T_{k,i-1}.\n$$\nCollecting all channels, the forward model is $H(x)=(y_1,\\dots,y_K)^\\top$.\n\nYou must implement:\n1. The forward model $H(x)$ as defined above.\n2. The directional derivative (tangent-linear action) $D H_x(\\delta)$ for any direction $\\delta\\in\\mathbb{R}^N$, using first principles and the chain rule. The directional derivative must be computed without forming the full Jacobian matrix explicitly.\n3. The adjoint action $A_x(\\eta)=\\big(D H_x\\big)^\\top \\eta$ for any $\\eta\\in\\mathbb{R}^K$, by deriving and coding a mathematically exact reverse-mode/adjoint accumulation that satisfies the inner-product identity\n$$\n\\langle D H_x(\\delta),\\,\\eta\\rangle_{\\mathbb{R}^K}=\\langle \\delta,\\,A_x(\\eta)\\rangle_{\\mathbb{R}^N}.\n$$\n\nVerification via Taylor test: For a set of positive step sizes $\\alpha$, verify that the Taylor remainder metric\n$$\nR(\\alpha) = \\frac{\\left\\|H(x+\\alpha \\delta)-H(x)-D H_x(\\alpha \\delta)\\right\\|_2}{\\alpha}\n$$\nscales linearly in $\\alpha$, i.e., $R(\\alpha)=\\mathcal{O}(\\alpha)$, which corresponds to a slope near $1$ in the log-log relation $\\log R(\\alpha)$ versus $\\log \\alpha$. Implement a numerical estimation of the slope by least-squares fitting of $\\log_{10} R(\\alpha)$ against $\\log_{10} \\alpha$ over the given $\\alpha$ values.\n\nAdjoint-code diagnosis: Implement an adjoint inner-product test by computing the relative discrepancy\n$$\n\\mathrm{err}_{\\mathrm{adj}}=\\frac{\\left|\\langle D H_x(\\delta),\\,\\eta\\rangle - \\langle \\delta,\\,A_x(\\eta)\\rangle\\right|}{\\max\\left(10^{-16},\\,\\left|\\langle D H_x(\\delta),\\,\\eta\\rangle\\right|+\\left|\\langle \\delta,\\,A_x(\\eta)\\rangle\\right|\\right)}.\n$$\nAdditionally, implement a “faulty” adjoint variant that omits the solar-term contribution in the adjoint accumulation, and demonstrate that the inner-product identity fails for this faulty variant while the Taylor test for the forward/tangent-linear remains correct. If the Taylor scaling fails (slope far from $1$), report the adjoint relative error to help diagnose whether the adjoint was implemented incorrectly.\n\nUse the following test suite of parameter sets, each providing $(N,K,w,s,b,x,\\delta,\\eta,\\text{faulty})$, where $N$ is the number of layers, $K$ is the number of channels, $w\\in\\mathbb{R}^{K\\times N}$, $s\\in\\mathbb{R}^K$, $b\\in\\mathbb{R}^{K\\times N}$, $x\\in\\mathbb{R}^N$, $\\delta\\in\\mathbb{R}^N$, and $\\eta\\in\\mathbb{R}^K$. All quantities are dimensionless. The list of step sizes for the Taylor test is $\\alpha\\in\\{10^{-1},\\,5\\times 10^{-2},\\,2.5\\times 10^{-2},\\,1.25\\times 10^{-2},\\,6.25\\times 10^{-3}\\}$:\n\n- Case 1 (general “happy path”, correct adjoint):\n    - $N=6$, $K=4$.\n    - $w_{k,i}=0.2+0.05\\,k+0.03\\,i$ for $k\\in\\{1,2,3,4\\}$, $i\\in\\{1,\\dots,6\\}$.\n    - $s_k=1.0+0.2\\,k$.\n    - $b_{k,i}=0.1+0.05\\,i+0.03\\,k$.\n    - $x=[-0.8,\\,-0.3,\\,0.2,\\,0.7,\\,-0.5,\\,0.1]^\\top$.\n    - $\\delta=[0.1,\\,-0.2,\\,0.05,\\,-0.1,\\,0.2,\\,-0.05]^\\top$.\n    - $\\eta=[0.3,\\,-0.5,\\,0.7,\\,-0.2]^\\top$.\n    - faulty = False.\n\n- Case 2 (boundary: very small optical depths, correct adjoint):\n    - $N=6$, $K=4$.\n    - $w_{k,i}=0.05+0.02\\,k+0.01\\,i$.\n    - $s_k=0.2+0.1\\,k$.\n    - $b_{k,i}=0.02+0.03\\,i+0.01\\,k$.\n    - $x=[-5.5,\\,-4.8,\\,-4.2,\\,-3.9,\\,-3.5,\\,-3.2]^\\top$.\n    - $\\delta=[0.02,\\,-0.01,\\,0.03,\\,-0.02,\\,0.01,\\,-0.03]^\\top$.\n    - $\\eta=[1.0,\\,-0.8,\\,0.6,\\,-0.4]^\\top$.\n    - faulty = False.\n\n- Case 3 (edge: very large optical depths, correct adjoint):\n    - $N=6$, $K=4$.\n    - $w_{k,i}=0.7+0.1\\,k+0.2\\,i$.\n    - $s_k=1.0+0.3\\,k$.\n    - $b_{k,i}=0.4+0.1\\,i+0.05\\,k$.\n    - $x=[3.0,\\,3.5,\\,4.0,\\,4.5,\\,5.0,\\,3.8]^\\top$.\n    - $\\delta=[-0.05,\\,0.1,\\,-0.08,\\,0.06,\\,-0.04,\\,0.02]^\\top$.\n    - $\\eta=[-0.5,\\,0.4,\\,-0.3,\\,0.2]^\\top$.\n    - faulty = False.\n\n- Case 4 (diagnosis: same as Case 1 but faulty adjoint):\n    - Use Case 1 parameters and set faulty = True.\n\nFor each case, compute:\n- The estimated slope (a float) of $\\log_{10} R(\\alpha)$ versus $\\log_{10} \\alpha$ using least-squares over the provided $\\alpha$ values.\n- A boolean indicating whether the Taylor test passed, defined by slope within the interval $[0.9,\\,1.1]$.\n- The adjoint inner-product relative error (a float) computed from the exact adjoint if faulty = False and from the faulty adjoint if faulty = True.\n- A boolean indicating whether the adjoint inner-product test passed, defined by $\\mathrm{err}_{\\mathrm{adj}}<10^{-10}$.\n\nYour program should produce a single line of output containing the results of all cases as a comma-separated list of per-case result lists, each per-case list in the order [slope, taylor_pass_boolean, adjoint_relative_error, adjoint_pass_boolean], enclosed in square brackets. For example, the output must have the form\n$$\n\\texttt{[[slope\\_1,True,err\\_1,True],[slope\\_2,True,err\\_2,True],[slope\\_3,True,err\\_3,True],[slope\\_4,True,err\\_4,False]]}\n$$\nwhere the booleans and floats reflect your computed values. No physical units are required, and angles do not appear; all quantities are dimensionless. The program must be self-contained and require no input.",
            "solution": "The problem statement is assessed to be valid. It is scientifically grounded in the principles of radiative transfer, mathematically well-posed, and expressed in objective, formal language. It presents a complete and consistent setup for a non-trivial computational task involving the implementation of a forward model, its tangent-linear counterpart, and the corresponding adjoint model, along with standard verification procedures.\n\nThe solution proceeds by first deriving the mathematical expressions for the tangent-linear and adjoint models from first principles, and then outlining their implementation.\n\nLet the state vector be $x \\in \\mathbb{R}^N$ and the measurement vector be $y \\in \\mathbb{R}^K$. The forward model is a function $H:\\mathbb{R}^N \\to \\mathbb{R}^K$ mapping the state to the measurements. The components of the state vector, $x_i$, are related to the layer optical depths $\\tau_i$ by the transformation $\\tau_i = \\exp(x_i)$. This ensures that $\\tau_i > 0$.\n\nThe upwelling radiance $y_k$ for channel $k$ is given by:\n$$\ny_k = s_k \\exp\\Big(-\\sum_{i=1}^N \\Delta\\tau_{k,i}\\Big) + \\sum_{i=1}^N b_{k,i}\\,L_{k,i}\\,T_{k,i-1}\n$$\nwhere the intermediate quantities are defined as:\n- Channel-layer optical depth: $\\Delta\\tau_{k,i} = w_{k,i}\\,\\tau_i = w_{k,i} \\exp(x_i)$\n- Layer upward transmission: $Z_{k,i} = \\exp(-\\Delta\\tau_{k,i})$\n- Layer emission escape fraction: $L_{k,i} = 1 - Z_{k,i}$\n- Cumulative upward transmission to just above layer $i$: $T_{k,i} = \\prod_{j=1}^{i} Z_{k,j}$, with the base case $T_{k,0} = 1$. The term $T_{k,i-1}$ in the main equation is thus an empty product for $i=1$, correctly evaluating to $1$.\n\n**1. Forward Model Implementation**\nThe forward model $H(x)$ is implemented by computing the sequence of intermediate variables for each channel $k=1,\\dots,K$ and each layer $i=1,\\dots,N$:\n1.  Compute layer optical depths $\\tau_i = \\exp(x_i)$ for $i=1,\\dots,N$.\n2.  For each channel $k$, compute channel-layer optical depths $\\Delta\\tau_{k,i} = w_{k,i}\\tau_i$.\n3.  For each channel $k$, compute layer transmissions $Z_{k,i} = \\exp(-\\Delta\\tau_{k,i})$.\n4.  For each channel $k$, compute cumulative transmissions $T_{k,i}$ recursively: $T_{k,0}=1$ and $T_{k,i} = T_{k,i-1} Z_{k,i}$ for $i=1,\\dots,N$.\n5.  The radiance $y_k$ is then calculated using the main equation, which can be expressed in terms of the cumulative transmissions as $y_k = s_k T_{k,N} + \\sum_{i=1}^N b_{k,i}(T_{k,i-1} - T_{k,i})$.\nAll intermediate quantities ($\\tau_i, \\Delta\\tau_{k,i}, Z_{k,i}, T_{k,i}$) are stored for use in the tangent-linear and adjoint calculations.\n\n**2. Tangent-Linear Model: Directional Derivative $D H_x(\\delta)$**\nThe tangent-linear model computes the action of the Jacobian of $H$ on a perturbation vector $\\delta \\in \\mathbb{R}^N$, denoted $\\delta y = DH_x(\\delta)$, without explicitly forming the Jacobian matrix. This is achieved by propagating the initial perturbation $\\delta x = \\delta$ through the chain of operations in the forward model. Let $\\delta v$ denote the perturbation of a variable $v$.\n\n1.  Perturbation of layer optical depth: $\\delta\\tau_i = \\frac{d\\tau_i}{dx_i}\\delta x_i = \\exp(x_i)\\delta x_i = \\tau_i \\delta x_i$.\n2.  Perturbation of channel-layer optical depth: $\\delta(\\Delta\\tau_{k,i}) = w_{k,i}\\delta\\tau_i = w_{k,i}\\tau_i\\delta x_i = \\Delta\\tau_{k,i}\\delta x_i$.\n3.  Perturbation of layer transmission: $\\delta Z_{k,i} = \\frac{dZ_{k,i}}{d\\Delta\\tau_{k,i}}\\delta(\\Delta\\tau_{k,i}) = -\\exp(-\\Delta\\tau_{k,i})\\delta(\\Delta\\tau_{k,i}) = -Z_{k,i}\\delta(\\Delta\\tau_{k,i})$.\n4.  Perturbation of cumulative transmission: From $T_{k,i} = T_{k,i-1}Z_{k,i}$, the product rule gives $\\delta T_{k,i} = \\delta T_{k,i-1}Z_{k,i} + T_{k,i-1}\\delta Z_{k,i}$. This is computed recursively starting with $\\delta T_{k,0} = 0$.\n5.  The final radiance perturbation is $\\delta y_k = s_k \\delta T_{k,N} + \\sum_{i=1}^N b_{k,i}(\\delta T_{k,i-1} - \\delta T_{k,i})$.\n\nThis forward propagation of perturbations yields the vector $\\delta y = (\\delta y_1, \\dots, \\delta y_K)^\\top$.\n\n**3. Adjoint Model: Transposed Action $A_x(\\eta) = (D H_x)^\\top \\eta$**\nThe adjoint model calculates the action of the transpose of the Jacobian on a vector $\\eta \\in \\mathbb{R}^K$. This is implemented using reverse-mode differentiation, which propagates sensitivities backwards from the output to the input. Let $\\bar{v}$ denote the derivative of the final scalar objective function $L = \\langle y, \\eta \\rangle = \\sum_k y_k \\eta_k$ with respect to an intermediate variable $v$. The final result is the vector of derivatives $\\bar{x}_j = \\partial L / \\partial x_j$.\n\nThe algorithm proceeds as follows:\n1.  Initialize all adjoint variables ($\\bar{\\tau}_i, \\bar{\\Delta\\tau}_{k,i}, \\dots$) to $0$. The input to the adjoint model is $\\bar{y}_k = \\eta_k$ for $k=1,\\dots,K$.\n2.  The calculation is performed per channel $k$ and the contributions to $\\bar{x}$ are accumulated. For each channel $k$:\n    a. Initialize an array for adjoint cumulative transmissions, $\\bar{T}_k$, of size $N+1$ to zeros.\n    b. For $y_k = s_k T_{k,N} + \\sum_{i=1}^N b_{k,i}(T_{k,i-1} - T_{k,i})$, the adjoint contributions to $\\bar{T}_k$ are:\n       - $\\bar{T}_{k,N} \\mathrel{+}= \\eta_k s_k$. This step is omitted for the faulty adjoint.\n       - For $i=1,\\dots,N$: $\\bar{T}_{k,i-1} \\mathrel{+}= \\eta_k b_{k,i}$ and $\\bar{T}_{k,i} \\mathrel{-}= \\eta_k b_{k,i}$.\n    c. Propagate sensitivities backward through the cumulative transmission calculation. For $i=N,\\dots,1$:\n       From $T_{k,i} = T_{k,i-1}Z_{k,i}$, we have:\n       - $\\bar{Z}_{k,i} \\mathrel{+}= \\bar{T}_{k,i} T_{k,i-1}$\n       - $\\bar{T}_{k,i-1} \\mathrel{+}= \\bar{T}_{k,i} Z_{k,i}$\n    d. Propagate from $\\bar{Z}_{k,i}$ to $\\bar{\\Delta\\tau}_{k,i}$ for $i=1,\\dots,N$:\n       From $Z_{k,i} = \\exp(-\\Delta\\tau_{k,i})$, we have $\\bar{\\Delta\\tau}_{k,i} \\mathrel{+}= \\bar{Z}_{k,i} (-Z_{k,i})$.\n3.  After iterating over all channels $k$, accumulate contributions to $\\bar{\\tau}_i$:\n    From $\\Delta\\tau_{k,i} = w_{k,i}\\tau_i$, we have $\\bar{\\tau}_i \\mathrel{+}= \\sum_k \\bar{\\Delta\\tau}_{k,i} w_{k,i}$.\n4.  Finally, propagate from $\\bar{\\tau}_i$ to $\\bar{x}_i$:\n    From $\\tau_i = \\exp(x_i)$, we have $\\bar{x}_i = \\bar{\\tau}_i \\exp(x_i) = \\bar{\\tau}_i \\tau_i$.\n\nThe resulting vector $\\bar{x}$ is the desired adjoint action $A_x(\\eta)$.\n\n**4. Verification Procedures**\n- **Taylor Test:** The quality of the tangent-linear model is checked by verifying that the model is first-order accurate. The Taylor remainder metric $R(\\alpha) = \\|H(x+\\alpha\\delta) - H(x) - \\alpha DH_x(\\delta)\\|_2 / \\alpha$ should be $\\mathcal{O}(\\alpha)$. This implies that a log-log plot of $R(\\alpha)$ versus $\\alpha$ will have a slope of $1$. We estimate this slope via linear least-squares regression. A slope in the range $[0.9, 1.1]$ indicates a pass.\n- **Adjoint Test:** The correctness of the adjoint model is verified using the fundamental inner product identity $\\langle DH_x(\\delta), \\eta \\rangle = \\langle \\delta, (DH_x)^\\top\\eta \\rangle$. The relative discrepancy between the two sides of this identity is computed. A value below $10^{-10}$ indicates a pass, confirming that the adjoint code is a true transpose of the tangent-linear code. The faulty adjoint, which omits the solar term, is expected to fail this test.",
            "answer": "```python\nimport numpy as np\n\nclass RadiativeTransferModel:\n    \"\"\"\n    Implements a discrete-layer radiative transfer model and its derivatives.\n    \"\"\"\n    def __init__(self, N, K, w, s, b):\n        self.N = N\n        self.K = K\n        self.w = w\n        self.s = s\n        self.b = b\n\n        # Pre-allocate for intermediate variables\n        self.tau = np.zeros(N)\n        self.Delta_tau = np.zeros((K, N))\n        self.Z = np.zeros((K, N))\n        self.T = np.zeros((K, N + 1))\n\n    def forward(self, x):\n        \"\"\"Computes the forward model H(x) and stores intermediate values.\"\"\"\n        self.tau = np.exp(x)\n        self.Delta_tau = self.w * self.tau\n        self.Z = np.exp(-self.Delta_tau)\n        \n        self.T[:, 0] = 1.0\n        for i in range(self.N):\n            self.T[:, i + 1] = self.T[:, i] * self.Z[:, i]\n            \n        y = self.s * self.T[:, self.N]\n        for i in range(self.N):\n            # L_ki * T_{k,i-1} = (1 - Z_ki) * T_{k,i-1} = T_{k,i-1} - T_{k,i}\n            y += self.b[:, i] * (self.T[:, i] - self.T[:, i + 1])\n        \n        return y\n\n    def tangent_linear(self, x, delta_x):\n        \"\"\"Computes the directional derivative DH_x(delta_x).\"\"\"\n        # Ensure forward pass has been run for base state x\n        self.forward(x)\n\n        delta_tau = self.tau * delta_x\n        delta_Delta_tau = self.w * delta_tau\n        delta_Z = -self.Z * delta_Delta_tau\n        \n        delta_T = np.zeros((self.K, self.N + 1))\n        # delta_T[:, 0] is always 0\n        for i in range(self.N):\n            delta_T[:, i + 1] = delta_T[:, i] * self.Z[:, i] + self.T[:, i] * delta_Z[:, i]\n\n        delta_y = self.s * delta_T[:, self.N]\n        for i in range(self.N):\n            delta_y += self.b[:, i] * (delta_T[:, i] - delta_T[:, i + 1])\n            \n        return delta_y\n\n    def adjoint(self, x, eta, faulty=False):\n        \"\"\"Computes the adjoint action (DH_x)^T * eta.\"\"\"\n        # Ensure forward pass has been run for base state x\n        self.forward(x)\n        \n        bar_tau = np.zeros(self.N)\n\n        for k in range(self.K):\n            bar_T_k = np.zeros(self.N + 1)\n            eta_k = eta[k]\n\n            # Contributions from y_k = s_k*T_kN + sum_i b_ki*(T_{k,i-1}-T_{k,i})\n            if not faulty:\n                bar_T_k[self.N] += eta_k * self.s[k]\n            \n            for i in range(self.N): # 1-based index i=1,...,N\n                i_idx = i\n                bar_T_k[i_idx] += eta_k * self.b[k, i_idx]\n                bar_T_k[i_idx + 1] -= eta_k * self.b[k, i_idx]\n\n            bar_Delta_tau_k = np.zeros(self.N)\n            # Propagate backwards from T\n            for i in range(self.N, 0, -1): # 1-based index i=N,...,1\n                i_idx = i - 1\n                \n                # From T_k,i = T_{k,i-1} * Z_{k,i}\n                bar_Z_ki = bar_T_k[i] * self.T[k, i_idx]\n                bar_T_k[i_idx] += bar_T_k[i] * self.Z[k, i_idx]\n\n                # From Z_k,i = exp(-Delta_tau_{k,i})\n                bar_Delta_tau_k[i_idx] += bar_Z_ki * (-self.Z[k, i_idx])\n\n            # Accumulate contributions to bar_tau for channel k\n            # From Delta_tau_ki = w_ki * tau_i\n            bar_tau += bar_Delta_tau_k * self.w[k, :]\n\n        # Final propagation from tau to x\n        # From tau_i = exp(x_i)\n        bar_x = bar_tau * self.tau\n        \n        return bar_x\n\ndef solve():\n    alpha_steps = np.array([1e-1, 5e-2, 2.5e-2, 1.25e-2, 6.25e-3])\n\n    # --- Test Case Data Generation ---\n    test_cases_params = []\n    \n    # Case 1\n    N, K = 6, 4\n    w = np.array([[0.2 + 0.05*(k+1) + 0.03*(i+1) for i in range(N)] for k in range(K)])\n    s = np.array([1.0 + 0.2*(k+1) for k in range(K)])\n    b = np.array([[0.1 + 0.05*(i+1) + 0.03*(k+1) for i in range(N)] for k in range(K)])\n    x = np.array([-0.8, -0.3, 0.2, 0.7, -0.5, 0.1])\n    delta = np.array([0.1, -0.2, 0.05, -0.1, 0.2, -0.05])\n    eta = np.array([0.3, -0.5, 0.7, -0.2])\n    test_cases_params.append({'N':N, 'K':K, 'w':w, 's':s, 'b':b, 'x':x, 'delta':delta, 'eta':eta, 'faulty':False})\n\n    # Case 2\n    N, K = 6, 4\n    w = np.array([[0.05 + 0.02*(k+1) + 0.01*(i+1) for i in range(N)] for k in range(K)])\n    s = np.array([0.2 + 0.1*(k+1) for k in range(K)])\n    b = np.array([[0.02 + 0.03*(i+1) + 0.01*(k+1) for i in range(N)] for k in range(K)])\n    x = np.array([-5.5, -4.8, -4.2, -3.9, -3.5, -3.2])\n    delta = np.array([0.02, -0.01, 0.03, -0.02, 0.01, -0.03])\n    eta = np.array([1.0, -0.8, 0.6, -0.4])\n    test_cases_params.append({'N':N, 'K':K, 'w':w, 's':s, 'b':b, 'x':x, 'delta':delta, 'eta':eta, 'faulty':False})\n\n    # Case 3\n    N, K = 6, 4\n    w = np.array([[0.7 + 0.1*(k+1) + 0.2*(i+1) for i in range(N)] for k in range(K)])\n    s = np.array([1.0 + 0.3*(k+1) for k in range(K)])\n    b = np.array([[0.4 + 0.1*(i+1) + 0.05*(k+1) for i in range(N)] for k in range(K)])\n    x = np.array([3.0, 3.5, 4.0, 4.5, 5.0, 3.8])\n    delta = np.array([-0.05, 0.1, -0.08, 0.06, -0.04, 0.02])\n    eta = np.array([-0.5, 0.4, -0.3, 0.2])\n    test_cases_params.append({'N':N, 'K':K, 'w':w, 's':s, 'b':b, 'x':x, 'delta':delta, 'eta':eta, 'faulty':False})\n\n    # Case 4 (same as 1, but faulty adjoint)\n    params1_copy = test_cases_params[0].copy()\n    params1_copy['faulty'] = True\n    test_cases_params.append(params1_copy)\n    \n    # --- Main Loop ---\n    results = []\n    for params in test_cases_params:\n        p = params\n        model = RadiativeTransferModel(p['N'], p['K'], p['w'], p['s'], p['b'])\n        \n        # --- Taylor Test ---\n        y0 = model.forward(p['x'])\n        dy = model.tangent_linear(p['x'], p['delta'])\n        \n        remainders = []\n        for alpha in alpha_steps:\n            y_alpha = model.forward(p['x'] + alpha * p['delta'])\n            remainder_norm = np.linalg.norm(y_alpha - y0 - alpha * dy)\n            remainders.append(remainder_norm / alpha)\n        \n        log_alphas = np.log10(alpha_steps)\n        log_remainders = np.log10(remainders)\n        slope = np.polyfit(log_alphas, log_remainders, 1)[0]\n        taylor_pass = 0.9 = slope = 1.1\n\n        # --- Adjoint Test ---\n        adj = model.adjoint(p['x'], p['eta'], faulty=p['faulty'])\n        \n        ip1 = np.dot(dy, p['eta'])\n        ip2 = np.dot(p['delta'], adj)\n        \n        adjoint_err = np.abs(ip1 - ip2) / max(1e-16, np.abs(ip1) + np.abs(ip2))\n        adjoint_pass = adjoint_err  1e-10\n        \n        results.append([slope, taylor_pass, adjoint_err, adjoint_pass])\n\n    # --- Formatting Output ---\n    case_results_str = [f'[{r[0]},{str(r[1])},{r[2]},{str(r[3])}]' for r in results]\n    final_str = f\"[{','.join(case_results_str)}]\"\n    print(final_str)\n\nsolve()\n```"
        },
        {
            "introduction": "After learning to construct linearized models, it is crucial to understand the limits of their validity, especially in nonlinear systems. This exercise uses the logistic map, a classic pedagogical model for chaos, to explore how the linear approximation breaks down as prediction horizons or initial errors increase. By deriving a \"radius of validity\" and connecting it to the local Lyapunov exponent, you will gain a deeper intuition for the challenges of data assimilation in systems with sensitive dependence on initial conditions .",
            "id": "3398752",
            "problem": "Consider the discrete-time nonlinear forecast model given by the logistic map, defined by the function $f(x) = r\\,x\\,\\left(1-x\\right)$ for a parameter $r \\in (0,4]$, and the forecast recursion $x_{t+1} = f(x_t)$ for integer time $t \\geq 0$. The observation model is the identity, so that the observation at time $t$ is $y_t = x_t$. The inverse problem of interest is the estimation of the initial condition $x_0$ from a window of observations $\\{y_1,\\dots,y_T\\}$ by minimizing a least-squares objective using a Gauss–Newton method based on linearization of the forecast and observation models about a current iterate (the current estimate of the initial condition).\n\nThe tasks are to connect the local stretching rate (as quantified by the local Lyapunov exponent) with the radius of validity of first-order linearization, and to determine thresholds beyond which a Gauss–Newton step is expected to diverge. Your derivations and computations must obey the following constraints.\n\n1) Use Taylor’s theorem with Lagrange form of the remainder and the chain rule as the only base tools for your analysis of the linearization error of $f$. In particular, consider the single-step Taylor expansion\n$$\nf(x_t + \\delta x_t) = f(x_t) + f'(x_t)\\,\\delta x_t + \\tfrac{1}{2} f''(\\xi_t)\\,(\\delta x_t)^2,\n$$\nwhere $\\xi_t$ lies between $x_t$ and $x_t + \\delta x_t$. Impose a relative smallness condition at each time step $t = 0,1,\\dots,T-1$ that the magnitude of the remainder is bounded by a fixed fraction $\\alpha \\in (0,1)$ of the magnitude of the linear term. From this condition and the chain rule for sensitivities across time, derive a rigorous sufficient bound of the form\n$$\n|\\delta x_0| \\le \\rho(r,\\alpha; x_0,\\dots,x_{T-1}),\n$$\nwhere $\\rho$ is a computable radius of validity that guarantees that the cumulative first-order linearization over the $T$ steps remains within the prescribed relative error. Express $\\rho$ using derivatives of $f$ and the products of these derivatives along the linearized trajectory about the current iterate. Then relate the exponential growth or decay of sensitivities to the finite-time local Lyapunov exponent, defined for a trajectory $\\{x_k\\}_{k=0}^{t-1}$ by\n$$\n\\lambda_t(x_0) = \\frac{1}{t} \\sum_{k=0}^{t-1} \\log \\left| f'(x_k) \\right|.\n$$\nExplain how $\\rho$ scales with $\\exp\\!\\left(-\\sum_{k=0}^{t-1} \\log|f'(x_k)|\\right)$ and thus how it depends on the local Lyapunov exponents.\n\n2) Specialize your derivation to the logistic map with $f'(x) = r(1-2x)$ and $f''(x) = -2r$, and provide an explicit computable expression for the radius $\\rho$ in terms only of $r$, $\\alpha$, the linearized trajectory states $\\{x_t\\}$, and the forward sensitivity products $\\prod_{k=0}^{t-1} f'(x_k)$.\n\n3) For each test case listed below, implement a single Gauss–Newton iteration to estimate the initial condition $x_0$ from identity observations over a window of $T$ steps, using a current iterate $x_0^{(0)}$ and noise-free observations generated from a true initial condition $x_0^\\star$. Specifically, let the residuals be $r_t = x_{t}(x_0^{(0)}) - y_t$ for $t=1,\\dots,T$ and let the Jacobian entries be $J_t = \\frac{\\partial x_t}{\\partial x_0}\\big|_{x_0^{(0)}}$ for $t=1,\\dots,T$. Compute the Gauss–Newton step $\\delta x_0$ for this one-dimensional least-squares problem using the normal equations and update $x_0^{(1)} = x_0^{(0)} + \\delta x_0$. Define the objective function as\n$$\n\\Phi(x_0) = \\tfrac{1}{2}\\sum_{t=1}^{T} \\left(x_t(x_0) - y_t\\right)^2,\n$$\nand determine two booleans:\n- A predicted divergence indicator, which is $\\mathrm{True}$ if $|\\delta x_0|$ exceeds the derived radius $\\rho$.\n- An actual divergence indicator, which is $\\mathrm{True}$ if $\\Phi(x_0^{(1)})  \\Phi(x_0^{(0)})$ or if $x_0^{(1)} \\notin [0,1]$.\n\n4) Use the following test suite, where each case is a tuple $(r, T, x_0^\\star, x_0^{(0)}, \\alpha)$:\n- Case A: $(r,T,x_0^\\star,x_0^{(0)},\\alpha) = (\\,3.2,\\,5,\\,0.61,\\,0.60,\\,0.1\\,)$\n- Case B: $(r,T,x_0^\\star,x_0^{(0)},\\alpha) = (\\,3.9,\\,5,\\,0.61,\\,0.60,\\,0.1\\,)$\n- Case C: $(r,T,x_0^\\star,x_0^{(0)},\\alpha) = (\\,3.9,\\,10,\\,0.61,\\,0.609,\\,0.1\\,)$\n- Case D: $(r,T,x_0^\\star,x_0^{(0)},\\alpha) = (\\,2.5,\\,4,\\,0.20,\\,0.80,\\,0.1\\,)$\n- Case E: $(r,T,x_0^\\star,x_0^{(0)},\\alpha) = (\\,3.5,\\,6,\\,0.5005,\\,0.48,\\,0.1\\,)$\n\nFor each case, compute:\n- The radius $\\rho$ derived in Tasks $1$–$2$ evaluated along the linearized trajectory about $x_0^{(0)}$.\n- The absolute Gauss–Newton step magnitude $|\\delta x_0|$.\n- The predicted divergence boolean.\n- The actual divergence boolean.\n\n5) Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each case, append the four values in this order: $\\rho$, $|\\delta x_0|$, predicted divergence boolean, actual divergence boolean. All floating-point outputs must be rounded to six decimal places. Therefore, the final output must be a flat list with $20$ entries in the order of Cases A through E, for example $[\\rho_A,|\\delta_A|,\\mathrm{pred}_A,\\mathrm{act}_A,\\rho_B,|\\delta_B|,\\mathrm{pred}_B,\\mathrm{act}_B,\\dots]$.\n\nNo physical units are involved. Angles are not involved. Percentages must be expressed as decimals if any appear, but none are required here. Ensure that your implementation is self-contained, deterministic, and requires no user input.",
            "solution": "We begin from the core definitions for the forecast model, observation model, and the linearization framework. The model is the logistic map $f(x) = r\\,x\\,(1-x)$, giving a forecast sequence $x_{t+1} = f(x_t)$. The observation model is the identity, $y_t = x_t$. The inverse problem is the least-squares estimation of $x_0$ from $\\{y_1,\\dots,y_T\\}$, which in one dimension yields a Gauss–Newton step computed from the Jacobian entries $J_t = \\frac{\\partial x_t}{\\partial x_0}$.\n\nPrinciple 1: Taylor’s theorem with remainder and per-step relative error. For a single step at time $t$, Taylor’s theorem with Lagrange remainder gives\n$$\nf(x_t + \\delta x_t) = f(x_t) + f'(x_t)\\,\\delta x_t + \\tfrac{1}{2} f''(\\xi_t)\\,(\\delta x_t)^2,\n$$\nfor some $\\xi_t$ between $x_t$ and $x_t + \\delta x_t$. To guarantee that the first-order linear approximation dominates the second-order term, we impose the sufficient condition that for a fixed $\\alpha \\in (0,1)$,\n$$\n\\left|\\tfrac{1}{2} f''(\\xi_t)\\,(\\delta x_t)^2\\right| \\le \\alpha\\,\\left| f'(x_t)\\,\\delta x_t \\right|, \\quad \\text{for each } t=0,1,\\dots,T-1.\n$$\nCanceling a factor of $|\\delta x_t|$ (assuming $\\delta x_t \\neq 0$; the case $\\delta x_t = 0$ is trivially safe) yields\n$$\n\\frac{1}{2}\\,|f''(\\xi_t)|\\,|\\delta x_t| \\le \\alpha\\,|f'(x_t)|.\n$$\nA sufficient condition that does not require knowledge of $\\xi_t$ is obtained by upper-bounding $|f''(\\xi_t)|$ by a uniform bound at $x_t$, or by using $|f''(\\xi_t)| \\le \\sup_{z \\in I_t} |f''(z)|$ for an interval $I_t$ containing $x_t$ and $x_t + \\delta x_t$. For the logistic map, $f''(x) = -2r$ is constant, so $|f''(\\xi_t)| = 2r$ everywhere. Thus the per-step condition simplifies to\n$$\n|\\delta x_t| \\le \\frac{2\\alpha\\,|f'(x_t)|}{|f''(\\xi_t)|} = \\frac{2\\alpha\\,|r(1-2x_t)|}{2r} = \\alpha\\,|1-2x_t|.\n$$\n\nPrinciple 2: Chain rule for sensitivities. Across $t$ steps, the perturbation propagation to first order satisfies\n$$\n\\delta x_t \\approx \\left(\\prod_{k=0}^{t-1} f'(x_k)\\right)\\,\\delta x_0 \\equiv J_t\\,\\delta x_0,\n$$\nwith the convention that $J_0 = 1$. Combining this with the per-step sufficient condition yields for each $t=0,1,\\dots,T-1$ the bound\n$$\n|J_t|\\,|\\delta x_0| \\le \\alpha\\,|1-2x_t| \\quad \\Rightarrow \\quad |\\delta x_0| \\le \\frac{\\alpha\\,|1-2x_t|}{|J_t|}.\n$$\nHence a sufficient uniform bound over the whole window is obtained by taking the minimum:\n$$\n\\rho(r,\\alpha; x_0,\\dots,x_{T-1}) \\equiv \\min_{0 \\le t \\le T-1} \\frac{\\alpha\\,|1-2x_t|}{\\left|\\prod_{k=0}^{t-1} f'(x_k)\\right|}.\n$$\nThis is the computable radius of validity of the first-order linearization with tolerance $\\alpha$ when linearizing about the trajectory $\\{x_t\\}$ of the current iterate.\n\nPrinciple 3: Relation to local Lyapunov exponents. Define the finite-time local Lyapunov exponent along the linearization trajectory as\n$$\n\\lambda_t(x_0) = \\frac{1}{t} \\sum_{k=0}^{t-1} \\log |f'(x_k)|, \\quad t \\ge 1.\n$$\nNoting that $\\left|\\prod_{k=0}^{t-1} f'(x_k)\\right| = \\exp\\!\\left(\\sum_{k=0}^{t-1} \\log |f'(x_k)|\\right) = \\exp\\!\\left(t\\,\\lambda_t(x_0)\\right)$, we obtain the scaling\n$$\n\\rho_t \\equiv \\frac{\\alpha\\,|1-2x_t|}{|J_t|} = \\alpha\\,|1-2x_t|\\,\\exp\\!\\left(-\\sum_{k=0}^{t-1} \\log |f'(x_k)|\\right) = \\alpha\\,|1-2x_t|\\,\\exp\\!\\left(-t\\,\\lambda_t(x_0)\\right).\n$$\nTherefore,\n$$\n\\rho = \\min_{0 \\le t \\le T-1} \\rho_t,\n$$\nwhich shows that a positive average local Lyapunov exponent over any prefix $(0,\\dots,t-1)$ shrinks the admissible radius exponentially with $t$, while negative average stretching expands the radius. This directly links the local Lyapunov exponents to the radius of validity of linearization.\n\nPrinciple 4: Specialization to the logistic map. For $f(x)=r\\,x(1-x)$, we have $f'(x)=r(1-2x)$ and $f''(x)=-2r$. The explicit per-step radius bound and its aggregation are\n$$\n\\rho_t = \\frac{\\alpha\\,|1-2x_t|}{\\left|\\prod_{k=0}^{t-1} r(1-2x_k)\\right|}, \\quad \\rho = \\min_{0 \\le t \\le T-1} \\rho_t.\n$$\nThis is fully computable from the current linearized trajectory $\\{x_t\\}$ and the sensitivities $J_t$.\n\nPrinciple 5: Gauss–Newton step in one dimension. With identity observations and residuals $r_t = x_t(x_0^{(0)}) - y_t$ for $t=1,\\dots,T$, and Jacobian entries $J_t = \\frac{\\partial x_t}{\\partial x_0}\\big|_{x_0^{(0)}}$, the Gauss–Newton step solves the normal equation\n$$\n\\left(\\sum_{t=1}^T J_t^2\\right)\\,\\delta x_0 = - \\sum_{t=1}^T J_t\\,r_t,\n$$\nso that, provided $\\sum_{t=1}^T J_t^2  0$,\n$$\n\\delta x_0 = - \\frac{\\sum_{t=1}^T J_t\\,r_t}{\\sum_{t=1}^T J_t^2}.\n$$\nWe then form the updated iterate $x_0^{(1)} = x_0^{(0)} + \\delta x_0$ and compute the objective\n$$\n\\Phi(x_0) = \\tfrac{1}{2}\\sum_{t=1}^T \\left(x_t(x_0) - y_t\\right)^2.\n$$\nWe declare a predicted divergence if $|\\delta x_0|  \\rho$, and an actual divergence if $\\Phi(x_0^{(1)})  \\Phi(x_0^{(0)})$ or $x_0^{(1)} \\notin [0,1]$.\n\nAlgorithmic design:\n- Generate the truth trajectory $\\{y_t\\}_{t=1}^T$ from $(r, x_0^\\star)$.\n- Generate the linearization trajectory $\\{x_t\\}_{t=0}^T$ from $(r, x_0^{(0)})$.\n- Compute sensitivities $J_0=1$ and $J_{t+1} = J_t\\,f'(x_t)$ iteratively.\n- Compute the radius $\\rho = \\min_{0 \\le t \\le T-1} \\alpha\\,|1-2x_t| / |J_t|$.\n- Compute the Gauss–Newton step $\\delta x_0$ from $\\{J_t\\}_{t=1}^T$ and residuals.\n- Evaluate predicted and actual divergence.\n- Repeat for each test case.\n- Round floating-point outputs to six decimal places and print a flat list $[\\rho_A,|\\delta_A|,\\mathrm{pred}_A,\\mathrm{act}_A,\\dots]$.\n\nCoverage of test suite:\n- Case A is a moderate-stretching regime with $r=3.2$ and a small initial error; typically $\\lambda_t$ is not strongly positive over short windows, so $\\rho$ is moderate and $|\\delta x_0| \\le \\rho$ is plausible.\n- Cases B and C use $r=3.9$, a strongly chaotic regime. With $T=5$ and especially $T=10$, $\\sum \\log|f'|$ tends to be positive, shrinking $\\rho$ and often producing $|\\delta x_0|  \\rho$.\n- Case D uses $r=2.5$ with a large initial discrepancy; despite weaker stretching, the large residual may produce a step violating the radius or leaving the unit interval.\n- Case E uses $r=3.5$ with $x_0^\\star$ close to $0.5$, where $|f'(x)|$ is small; this tests sensitivity to near-critical points where $|1-2x_t|$ can be small, shrinking $\\rho$ locally and potentially triggering divergence.\n\nThis completes the derivation and algorithmic plan. The accompanying program implements these steps exactly and produces the required single-line output.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef logistic_step(x, r):\n    return r * x * (1.0 - x)\n\ndef logistic_traj_and_sensitivities(r, x0, T):\n    \"\"\"\n    Compute trajectory x[0..T] with x[0]=x0, and sensitivities J[0..T]\n    where J[t] = d x_t / d x0 evaluated along the trajectory of x0.\n    \"\"\"\n    x = np.empty(T + 1, dtype=float)\n    J = np.empty(T + 1, dtype=float)\n    x[0] = x0\n    J[0] = 1.0\n    for t in range(T):\n        # Derivative f'(x_t) = r (1 - 2 x_t)\n        fp = r * (1.0 - 2.0 * x[t])\n        x[t + 1] = logistic_step(x[t], r)\n        J[t + 1] = J[t] * fp\n    return x, J\n\ndef observations_from_truth(r, x0_true, T):\n    y = np.empty(T + 1, dtype=float)\n    y[0] = x0_true\n    for t in range(T):\n        y[t + 1] = logistic_step(y[t], r)\n    # Return y[1..T]\n    return y[1:]\n\ndef compute_radius_alpha(r, alpha, x_traj, J_sens):\n    \"\"\"\n    Compute rho = min_{t=0..T-1} alpha * |1-2 x_t| / |J_t|\n    where x_traj has length T+1, J_sens has length T+1, and we use t = 0..T-1.\n    \"\"\"\n    T = len(x_traj) - 1\n    eps = 1e-15\n    radii = []\n    for t in range(T):\n        numerator = alpha * abs(1.0 - 2.0 * x_traj[t])\n        denom = abs(J_sens[t])\n        # Avoid division by zero; use a very small denominator to keep a very large radius.\n        denom = max(denom, eps)\n        radii.append(numerator / denom)\n    rho = min(radii) if len(radii) > 0 else 0.0\n    return rho\n\ndef gauss_newton_step(r, x0_guess, T, y_obs):\n    \"\"\"\n    One Gauss-Newton step for initial condition estimation with identity observations.\n    Returns:\n      delta_x0, phi_before, phi_after, x0_new\n    \"\"\"\n    # Trajectory and sensitivities at guess\n    xg, Jg = logistic_traj_and_sensitivities(r, x0_guess, T)\n    # Residuals r_t for t=1..T: r_t = x_t - y_t\n    residuals = xg[1:] - y_obs\n    # Jacobian entries J_t for t=1..T are Jg[1:]\n    Jrows = Jg[1:]\n    # Normal equations in 1D\n    JTJ = float(np.dot(Jrows, Jrows))\n    JTr = float(np.dot(Jrows, residuals))\n    # Objective before\n    phi_before = 0.5 * float(np.dot(residuals, residuals))\n    if JTJ = 1e-18:\n        # Ill-conditioned or zero sensitivity: take no step (or treat as divergence later)\n        delta = 0.0\n    else:\n        delta = - JTr / JTJ\n    x0_new = x0_guess + delta\n    # Objective after\n    x_new_traj, _ = logistic_traj_and_sensitivities(r, x0_new, T)\n    res_after = x_new_traj[1:] - y_obs\n    phi_after = 0.5 * float(np.dot(res_after, res_after))\n    return delta, phi_before, phi_after, x0_new, xg, Jg\n\ndef run_case(case):\n    r, T, x0_true, x0_guess, alpha = case\n    # Generate noise-free observations y_t from truth\n    y_obs = observations_from_truth(r, x0_true, T)\n    # Compute one GN step at the guess\n    delta, phi_before, phi_after, x0_new, xg, Jg = gauss_newton_step(r, x0_guess, T, y_obs)\n    # Compute rho at the guess trajectory\n    rho = compute_radius_alpha(r, alpha, xg, Jg)\n    # Predicted divergence if |delta| > rho\n    predicted_diverge = abs(delta) > rho\n    # Actual divergence if objective increases or new iterate leaves [0,1]\n    actual_diverge = (phi_after > phi_before) or (x0_new  0.0) or (x0_new > 1.0)\n    return rho, abs(delta), predicted_diverge, actual_diverge\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (r, T, x0_true, x0_guess, alpha)\n    test_cases = [\n        (3.2, 5, 0.61, 0.60, 0.1),      # Case A\n        (3.9, 5, 0.61, 0.60, 0.1),      # Case B\n        (3.9, 10, 0.61, 0.609, 0.1),    # Case C\n        (2.5, 4, 0.20, 0.80, 0.1),      # Case D\n        (3.5, 6, 0.5005, 0.48, 0.1),    # Case E\n    ]\n\n    results = []\n    for case in test_cases:\n        rho, abs_delta, pred_div, act_div = run_case(case)\n        # Round floats to six decimal places as required\n        rho_r = round(float(rho), 6)\n        abs_delta_r = round(float(abs_delta), 6)\n        results.extend([rho_r, abs_delta_r, str(pred_div), str(act_div)])\n\n    # Final print statement in the exact required format.\n    # Single line, comma-separated list enclosed in square brackets.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```"
        },
        {
            "introduction": "Building upon the understanding of linearization error, this final practice addresses how to design a robust optimization algorithm that explicitly manages this error. You are tasked with developing a trust-region strategy for an incremental 4D-Var setting, where theoretical bounds on model nonlinearity are used to define a region in which the linear approximation is \"trusted\". This exercise bridges the gap between theoretical analysis and the practical implementation of sophisticated, reliable data assimilation schemes .",
            "id": "3398775",
            "problem": "Consider a Four-Dimensional Variational (4D-Var) incremental formulation where the forecast model is a mapping $M:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{p}$ that is twice Fréchet differentiable, and the observation operator is linear $\\mathcal{H}:\\mathbb{R}^{p}\\rightarrow\\mathbb{R}^{q}$. At a linearization point $x\\in\\mathbb{R}^{n}$, define the residual $r_{0}\\in\\mathbb{R}^{q}$ by $r_{0}=\\mathcal{H}M(x)-y$ for given observations $y\\in\\mathbb{R}^{q}$, and define the linearized observation mapping $A\\in\\mathbb{R}^{q\\times n}$ by $A=\\mathcal{H}\\,D M_{x}$, where $D M_{x}$ denotes the Fréchet derivative of $M$ at $x$. The incremental least-squares model for the observation misfit is then the quadratic functional $m(\\delta)=\\tfrac{1}{2}\\|r_{0}+A\\,\\delta\\|_{2}^{2}$ for increments $\\delta\\in\\mathbb{R}^{n}$.\n\nYou are tasked to design a trust-region strategy that selects increments $\\delta$ satisfying the linearization error safeguard\n$$\n\\|M(x+\\delta)-M(x)-D M_{x}\\,\\delta\\|_{2}\\le \\varepsilon,\n$$\nfor a prescribed tolerance $\\varepsilon0$. The only information available about the local nonlinearity of $M$ is a local uniform bound on the operator norm of the second derivative within a ball $\\mathcal{B}(x,R)=\\{z\\in\\mathbb{R}^{n}:\\|z-x\\|_{2}\\le R\\}$, namely\n$$\n\\sup_{\\xi\\in\\mathcal{B}(x,R)}\\|D^{2}M(\\xi)\\|_{\\mathrm{op}}\\le L_{2},\n$$\nwhere $L_{2}0$ is given and $\\|\\cdot\\|_{\\mathrm{op}}$ denotes the induced operator norm on bilinear forms.\n\nYour program must, for each test case, perform the following tasks starting only from: (i) Taylor’s theorem with integral remainder for Fréchet differentiable mappings, (ii) the definition of Lipschitz continuity and operator norms, and (iii) standard convex trust-region optimality conditions for quadratic models.\n\n1. From the facts above, determine a trust-region radius $\\Delta0$ such that every step $\\delta$ with $\\|\\delta\\|_{2}\\le \\Delta$ is guaranteed to satisfy the linearization error safeguard. Express $\\Delta$ in terms of $L_{2}$ and $\\varepsilon$ without using any unproven shortcut formulas.\n\n2. Compute the trust-region step $\\delta_{\\star}$ that minimizes the quadratic model $m(\\delta)=\\tfrac{1}{2}\\|r_{0}+A\\,\\delta\\|_{2}^{2}$ subject to the Euclidean norm constraint $\\|\\delta\\|_{2}\\le \\Delta$. Use only well-founded optimality conditions for convex quadratic trust-region problems.\n\n3. Compute the predicted reduction $\\mathrm{pred}=\\tfrac{1}{2}\\|r_{0}\\|_{2}^{2}-\\tfrac{1}{2}\\|r_{0}+A\\,\\delta_{\\star}\\|_{2}^{2}$.\n\n4. Using only the bound $L_{2}$ and the operator norm of the observation operator $\\|\\mathcal{H}\\|_{\\mathrm{op}}$ (denote this scalar by $H_{\\mathrm{op}}0$), derive a rigorous bound on the discrepancy between the actual observation-space reduction and the predicted reduction, and then produce a conservative acceptance ratio lower bound $\\rho_{\\mathrm{low}}\\in[0,\\infty)$ that depends only on computable quantities. The acceptance ratio lower bound must be constructed so that if $\\rho_{\\mathrm{low}}\\ge \\eta$ for a given threshold $\\eta\\in(0,1)$, then acceptance of the step is guaranteed to be justified with respect to the observation misfit decrease in the presence of model linearization error consistent with the given bounds.\n\n5. Return, for each test case, a triple consisting of the trust-region radius $\\Delta$, the acceptance ratio lower bound $\\rho_{\\mathrm{low}}$, and a boolean acceptance decision $\\rho_{\\mathrm{low}}\\ge \\eta$.\n\nYour solution must treat all vectors and matrices with Euclidean norms and induced operator norms, and must not rely on any unproven or context-external identities. Angles are not involved. No physical units are involved. All scalars must be treated as real numbers. The observation operator norm $H_{\\mathrm{op}}$ is provided directly.\n\nTest Suite. Implement your program to compute results for the following three test cases. For each case, use the data exactly as specified.\n\n- Test case $1$:\n  - $A=\\begin{bmatrix}1.0  0.0\\\\ 0.0  2.0\\end{bmatrix}$,\n  - $r_{0}=\\begin{bmatrix}1.0\\\\ -1.0\\end{bmatrix}$,\n  - $L_{2}=4.0$,\n  - $\\varepsilon=0.05$,\n  - $H_{\\mathrm{op}}=1.5$,\n  - $\\eta=0.3$.\n\n- Test case $2$:\n  - $A=\\begin{bmatrix}2.0  0.5\\\\ 0.0  1.0\\end{bmatrix}$,\n  - $r_{0}=\\begin{bmatrix}2.0\\\\ 1.5\\end{bmatrix}$,\n  - $L_{2}=3.0$,\n  - $\\varepsilon=0.5$,\n  - $H_{\\mathrm{op}}=2.0$,\n  - $\\eta=0.5$.\n\n- Test case $3$:\n  - $A=\\begin{bmatrix}0.1  0.0\\\\ 0.0  0.1\\end{bmatrix}$,\n  - $r_{0}=\\begin{bmatrix}0.05\\\\ -0.02\\end{bmatrix}$,\n  - $L_{2}=100.0$,\n  - $\\varepsilon=0.0001$,\n  - $H_{\\mathrm{op}}=1.0$,\n  - $\\eta=0.1$.\n\nFinal Output Format. Your program should produce a single line of output containing a list of three lists, one per test case, each inner list containing three entries: the trust-region radius $\\Delta$ as a float rounded to six decimal places, the acceptance ratio lower bound $\\rho_{\\mathrm{low}}$ as a float rounded to six decimal places, and the boolean acceptance decision. For example, the output must have the format\n$[\\,[\\Delta_{1},\\rho_{\\mathrm{low},1},\\mathrm{decision}_{1}],\\,[\\Delta_{2},\\rho_{\\mathrm{low},2},\\mathrm{decision}_{2}],\\,[\\Delta_{3},\\rho_{\\mathrm{low},3},\\mathrm{decision}_{3}]\\,]$\nwith no extra whitespace beyond commas and brackets and with floats rounded to six decimal places.",
            "solution": "The problem requires the design and analysis of a trust-region strategy for an incremental 4D-Var formulation. The solution is derived from first principles as specified.\n\n### Step 1: Determination of the Trust-Region Radius $\\Delta$\n\nThe trust-region radius $\\Delta$ must be chosen to satisfy the linearization error safeguard:\n$$\n\\|M(x+\\delta)-M(x)-D M_{x}\\,\\delta\\|_{2}\\le \\varepsilon\n$$\nfor all increments $\\delta$ such that $\\|\\delta\\|_{2}\\le \\Delta$. We are given that the mapping $M:\\mathbb{R}^{n}\\rightarrow\\mathbb{R}^{p}$ is twice Fréchet differentiable. By Taylor's theorem with integral remainder, the linearization error term can be written as:\n$$\nM(x+\\delta) - M(x) - D M_{x}\\,\\delta = \\int_{0}^{1}(1-t) D^{2}M(x+t\\delta)(\\delta, \\delta) \\, dt\n$$\nwhere $D^{2}M(\\xi)$ is the second Fréchet derivative (a bilinear form) of $M$ at a point $\\xi$. Taking the Euclidean norm of both sides and applying the triangle inequality for Bochner integrals, we have:\n$$\n\\|M(x+\\delta) - M(x) - D M_{x}\\,\\delta\\|_{2} = \\left\\| \\int_{0}^{1}(1-t) D^{2}M(x+t\\delta)(\\delta, \\delta) \\, dt \\right\\|_{2} \\le \\int_{0}^{1} \\|(1-t) D^{2}M(x+t\\delta)(\\delta, \\delta)\\|_{2} \\, dt\n$$\nSince $t \\in [0,1]$, $(1-t)$ is a non-negative scalar. Using the definition of the operator norm for a bilinear form, $\\|B(v,v)\\| \\le \\|B\\|_{\\mathrm{op}}\\|v\\|^2$, we get:\n$$\n\\int_{0}^{1} (1-t) \\|D^{2}M(x+t\\delta)(\\delta, \\delta)\\|_{2} \\, dt \\le \\int_{0}^{1} (1-t) \\|D^{2}M(x+t\\delta)\\|_{\\mathrm{op}}\\|\\delta\\|_{2}^{2} \\, dt\n$$\nThe problem provides a uniform bound on the operator norm of the second derivative, $\\sup_{\\xi\\in\\mathcal{B}(x,R)}\\|D^{2}M(\\xi)\\|_{\\mathrm{op}}\\le L_{2}$. If we restrict $\\delta$ such that $\\|\\delta\\|_{2} \\le R$, then for all $t\\in[0,1]$, the point $x+t\\delta$ lies within the ball $\\mathcal{B}(x,R)$, and we can use the bound $L_2$.\n$$\n\\|M(x+\\delta) - M(x) - D M_{x}\\,\\delta\\|_{2} \\le \\int_{0}^{1} (1-t) L_{2} \\|\\delta\\|_{2}^{2} \\, dt = L_{2} \\|\\delta\\|_{2}^{2} \\int_{0}^{1} (1-t) \\, dt\n$$\nThe integral evaluates to $\\int_{0}^{1} (1-t) \\, dt = [t - \\frac{t^2}{2}]_{0}^{1} = \\frac{1}{2}$. This yields the standard second-order Taylor error bound:\n$$\n\\|M(x+\\delta) - M(x) - D M_{x}\\,\\delta\\|_{2} \\le \\frac{1}{2} L_{2} \\|\\delta\\|_{2}^{2}\n$$\nTo satisfy the safeguard $\\|M(x+\\delta)-M(x)-D M_{x}\\,\\delta\\|_{2}\\le \\varepsilon$, it is sufficient to impose the condition:\n$$\n\\frac{1}{2} L_{2} \\|\\delta\\|_{2}^{2} \\le \\varepsilon \\implies \\|\\delta\\|_{2}^{2} \\le \\frac{2\\varepsilon}{L_{2}}\n$$\nThus, any $\\delta$ satisfying $\\|\\delta\\|_{2} \\le \\sqrt{2\\varepsilon/L_{2}}$ will meet the safeguard. We define the trust-region radius as this upper bound:\n$$\n\\Delta = \\sqrt{\\frac{2\\varepsilon}{L_{2}}}\n$$\n\n### Step 2: Computation of the Trust-Region Step $\\delta_{\\star}$\n\nThe trust-region step $\\delta_{\\star}$ is the solution to the convex quadratic minimization problem:\n$$\n\\min_{\\delta \\in \\mathbb{R}^n} \\quad m(\\delta) = \\frac{1}{2} \\|r_{0} + A\\delta\\|_{2}^{2} \\quad \\text{subject to} \\quad \\|\\delta\\|_{2} \\le \\Delta\n$$\nThe objective function can be expanded as $m(\\delta) = \\frac{1}{2} (r_0^T r_0 + 2r_0^T A\\delta + \\delta^T A^T A \\delta)$. Let $g = A^T r_0$ and $B = A^T A$. The problem becomes $\\min_{\\delta} g^T\\delta + \\frac{1}{2}\\delta^T B \\delta$ subject to $\\|\\delta\\|_2 \\le \\Delta$, ignoring the constant term. The Hessian $B=A^TA$ is symmetric and positive semi-definite, making the problem convex.\n\nThe Karush-Kuhn-Tucker (KKT) optimality conditions state that there exists a Lagrange multiplier $\\lambda \\ge 0$ such that the optimal solution $\\delta_{\\star}$ satisfies:\n1.  $(B + \\lambda I)\\delta_{\\star} = -g$\n2.  $\\|\\delta_{\\star}\\|_{2} \\le \\Delta$\n3.  $\\lambda \\ge 0$\n4.  $\\lambda(\\|\\delta_{\\star}\\|_{2} - \\Delta) = 0$\n\nTwo cases arise:\n-   **Interior solution**: First, we compute the unconstrained minimizer $\\delta_u$, which corresponds to $\\lambda=0$. This requires solving $B\\delta_u = -g$. If $B$ is invertible (which it is for the given test cases, as $A$ is full rank), $\\delta_u = -B^{-1}g$. If $\\|\\delta_u\\|_{2} \\le \\Delta$, this is the optimal solution, so $\\delta_{\\star} = \\delta_u$.\n-   **Boundary solution**: If $\\|\\delta_u\\|_{2} \\ge \\Delta$, the constraint must be active, meaning $\\|\\delta_{\\star}\\|_{2} = \\Delta$ and $\\lambda > 0$. The solution is given by $\\delta_{\\star}(\\lambda) = -(B + \\lambda I)^{-1}g$. We must find the unique $\\lambda > 0$ that satisfies the secular equation $\\|\\delta_{\\star}(\\lambda)\\|_{2} = \\Delta$. This is a nonlinear scalar equation that can be solved efficiently using a root-finding algorithm like the bisection method or Brent's method on the function $\\phi(\\lambda) = \\|-(B + \\lambda I)^{-1}g\\|_{2} - \\Delta$.\n\n### Step 3: Computation of the Predicted Reduction `pred`\n\nThe predicted reduction is the decrease in the quadratic model $m(\\delta)$ when moving from $\\delta=0$ to $\\delta=\\delta_{\\star}$:\n$$\n\\mathrm{pred} = m(0) - m(\\delta_{\\star}) = \\frac{1}{2}\\|r_{0}\\|_{2}^{2} - \\frac{1}{2}\\|r_{0} + A\\delta_{\\star}\\|_{2}^{2}\n$$\nSince $\\delta_{\\star}$ minimizes $m(\\delta)$ over the trust region, and $\\delta=0$ is in the trust region, it is guaranteed that $m(\\delta_{\\star}) \\le m(0)$, so $\\mathrm{pred} \\ge 0$.\n\n### Step 4: Derivation of the Acceptance Ratio Lower Bound $\\rho_{\\mathrm{low}}$\n\nThe actual reduction is the decrease in the true observation misfit:\n$$\n\\mathrm{ared} = \\frac{1}{2}\\|r_{0}\\|_{2}^{2} - \\frac{1}{2}\\|\\mathcal{H}M(x+\\delta_{\\star}) - y\\|_{2}^{2}\n$$\nThe acceptance ratio is $\\rho = \\mathrm{ared} / \\mathrm{pred}$. We seek a computable lower bound $\\rho_{\\mathrm{low}}$.\nLet's analyze the term for the new residual:\n$$\n\\mathcal{H}M(x+\\delta_{\\star}) - y = \\mathcal{H}M(x) - y + \\mathcal{H}(M(x+\\delta_{\\star}) - M(x)) = r_0 + \\mathcal{H}(D M_{x}\\delta_{\\star} + e_M(\\delta_{\\star}))\n$$\nwhere $e_M(\\delta_{\\star}) = M(x+\\delta_{\\star}) - M(x) - D M_{x}\\delta_{\\star}$ is the model linearization error. Using the linearity of $\\mathcal{H}$ and the definition $A = \\mathcal{H} D M_x$:\n$$\n\\mathcal{H}M(x+\\delta_{\\star}) - y = r_0 + A\\delta_{\\star} + \\mathcal{H}e_M(\\delta_{\\star})\n$$\nThe discrepancy between the actual and predicted reductions is:\n$$\n\\mathrm{ared} - \\mathrm{pred} = \\left(\\frac{1}{2}\\|r_{0}\\|_{2}^{2} - \\frac{1}{2}\\|\\dots\\|_{2}^{2}\\right) - \\left(\\frac{1}{2}\\|r_{0}\\|_{2}^{2} - \\frac{1}{2}\\|r_{0} + A\\delta_{\\star}\\|_{2}^{2}\\right) = \\frac{1}{2}\\|r_{0} + A\\delta_{\\star}\\|_{2}^{2} - \\frac{1}{2}\\|r_{0} + A\\delta_{\\star} + \\mathcal{H}e_M(\\delta_{\\star})\\|_{2}^{2}\n$$\nLet $u = r_0 + A\\delta_{\\star}$ and $v = \\mathcal{H}e_M(\\delta_{\\star})$. The difference is $\\frac{1}{2}(\\|u\\|_{2}^{2} - \\|u+v\\|_{2}^{2}) = -u^T v - \\frac{1}{2}\\|v\\|_{2}^{2}$.\nTo find a lower bound on `ared`, we need an upper bound on the absolute value of this difference:\n$$\n|\\mathrm{ared} - \\mathrm{pred}| = |-u^T v - \\frac{1}{2}\\|v\\|_{2}^{2}| \\le |u^T v| + \\frac{1}{2}\\|v\\|_{2}^{2} \\le \\|u\\|_{2}\\|v\\|_{2} + \\frac{1}{2}\\|v\\|_{2}^{2}\n$$\nWe have bounds on $\\|v\\|_{2}$:\n$$\n\\|v\\|_{2} = \\|\\mathcal{H}e_M(\\delta_{\\star})\\|_{2} \\le \\|\\mathcal{H}\\|_{\\mathrm{op}} \\|e_M(\\delta_{\\star})\\|_{2} = H_{\\mathrm{op}} \\|e_M(\\delta_{\\star})\\|_{2}\n$$\nFrom Step 1, $\\|e_M(\\delta_{\\star})\\|_{2} \\le \\frac{1}{2}L_{2}\\|\\delta_{\\star}\\|_{2}^{2}$. Combining these, we get an upper bound on $\\|v\\|_2$:\n$$\n\\|v\\|_{2} \\le \\frac{1}{2} H_{\\mathrm{op}} L_{2} \\|\\delta_{\\star}\\|_{2}^{2}\n$$\nLet this bound be $C_v = \\frac{1}{2} H_{\\mathrm{op}} L_2 \\|\\delta_{\\star}\\|_{2}^{2}$. Then the absolute difference between `ared` and `pred` is bounded by:\n$$\n|\\mathrm{ared} - \\mathrm{pred}| \\le \\|r_{0} + A\\delta_{\\star}\\|_{2} C_v + \\frac{1}{2}C_v^2\n$$\nLet this upper bound be $C_{err}$. Since $\\mathrm{ared} \\ge \\mathrm{pred} - C_{err}$, we can form a lower bound on the ratio $\\rho = \\mathrm{ared}/\\mathrm{pred}$ (assuming $\\mathrm{pred}  0$):\n$$\n\\rho \\ge \\frac{\\mathrm{pred} - C_{err}}{\\mathrm{pred}} = 1 - \\frac{C_{err}}{\\mathrm{pred}}\n$$\nGiven the requirement that $\\rho_{\\mathrm{low}} \\in [0, \\infty)$, we define our conservative lower bound as:\n$$\n\\rho_{\\mathrm{low}} = \\max\\left(0, 1 - \\frac{\\|r_{0} + A\\delta_{\\star}\\|_{2} C_v + \\frac{1}{2}C_v^2}{\\mathrm{pred}}\\right), \\quad \\text{where } C_v = \\frac{1}{2}H_{\\mathrm{op}}L_{2}\\|\\delta_{\\star}\\|_{2}^{2}\n$$\nIf $\\mathrm{pred}=0$, which occurs if and only if $\\delta_{\\star}=0$ (for the given full-rank $A$), then $\\mathrm{ared}$ is also $0$. The ratio is indeterminate $0/0$, conventionally taken as $1$. In this case, $C_v=0$ and $C_{err}=0$, so our formula is consistent if we define $\\rho_{\\mathrm{low}}=1$ when $\\mathrm{pred}=0$.",
            "answer": "```python\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves the trust-region problem for the given test cases.\n    \"\"\"\n    test_cases = [\n        {\"A\": np.array([[1.0, 0.0], [0.0, 2.0]]), \"r0\": np.array([1.0, -1.0]), \"L2\": 4.0, \"epsilon\": 0.05, \"H_op\": 1.5, \"eta\": 0.3},\n        {\"A\": np.array([[2.0, 0.5], [0.0, 1.0]]), \"r0\": np.array([2.0, 1.5]), \"L2\": 3.0, \"epsilon\": 0.5, \"H_op\": 2.0, \"eta\": 0.5},\n        {\"A\": np.array([[0.1, 0.0], [0.0, 0.1]]), \"r0\": np.array([0.05, -0.02]), \"L2\": 100.0, \"epsilon\": 0.0001, \"H_op\": 1.0, \"eta\": 0.1},\n    ]\n\n    results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        r0 = case[\"r0\"]\n        L2 = case[\"L2\"]\n        epsilon = case[\"epsilon\"]\n        H_op = case[\"H_op\"]\n        eta = case[\"eta\"]\n\n        # 1. Determine trust-region radius Delta\n        Delta = np.sqrt(2 * epsilon / L2)\n\n        # 2. Compute the trust-region step delta_star\n        B = A.T @ A\n        g = A.T @ r0\n        \n        delta_u = -np.linalg.solve(B, g)\n        norm_delta_u = np.linalg.norm(delta_u)\n        \n        if norm_delta_u = Delta:\n            delta_star = delta_u\n        else:\n            # Boundary solution: solve secular equation for lambda > 0\n            def phi(lam):\n                delta_lam = -np.linalg.solve(B + lam * np.identity(B.shape[0]), g)\n                return np.linalg.norm(delta_lam) - Delta\n\n            # Find a bracketing interval for the root. Since phi(0) > 0 and phi(inf)  0,\n            # a root must exist for lambda > 0.\n            lambda_upper_bound = 2 * np.linalg.norm(g) / Delta + 1000\n            lam_star = brentq(phi, 1e-9, lambda_upper_bound)\n            delta_star = -np.linalg.solve(B + lam_star * np.identity(B.shape[0]), g)\n\n        # 3. Compute predicted reduction 'pred'\n        norm_r0_sq = r0.T @ r0\n        res_delta_star = r0 + A @ delta_star\n        norm_res_delta_star_sq = res_delta_star.T @ res_delta_star\n        pred = 0.5 * (norm_r0_sq - norm_res_delta_star_sq)\n\n        # 4. Compute acceptance ratio lower bound rho_low\n        if pred  1e-12: # Handles the case delta_star is effectively zero\n            rho_low = 1.0\n        else:\n            norm_delta_star = np.linalg.norm(delta_star)\n            Cv = 0.5 * H_op * L2 * norm_delta_star**2\n            C_err = np.linalg.norm(res_delta_star) * Cv + 0.5 * Cv**2\n            rho_low = max(0.0, 1.0 - C_err / pred)\n\n        # 5. Acceptance decision\n        decision = bool(rho_low >= eta)\n        results.append([Delta, rho_low, decision])\n\n    # Final print statement in the exact required format\n    final_output_str = \",\".join(\n        [\n            f\"[{r[0]:.6f},{r[1]:.6f},{str(r[2])}]\"\n            for r in results\n        ]\n    )\n    print(f\"[{final_output_str}]\")\n\nsolve()\n```"
        }
    ]
}