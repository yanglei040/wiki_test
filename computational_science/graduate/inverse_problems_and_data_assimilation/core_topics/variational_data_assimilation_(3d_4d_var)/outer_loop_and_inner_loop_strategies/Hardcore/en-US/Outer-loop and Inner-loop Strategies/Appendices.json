{
    "hands_on_practices": [
        {
            "introduction": "This first exercise focuses on the engine of the outer-loop/inner-loop strategy: the minimization problem solved within the inner loop. By working through a concrete two-dimensional example, you will practice computing the key components that drive the optimization. You will calculate the gradient of the incremental cost function, which indicates the steepest descent direction, and solve for the Gauss-Newton step that directly finds the minimum of this quadratic approximation .",
            "id": "3409171",
            "problem": "Consider a single-observation incremental variational data assimilation inner-loop arising within an outer-loop/inner-loop strategy. Let the observation operator be linear, given by the row vector $\\,\\mathbf{H} = [\\,1,\\,2\\,]\\,$ mapping $\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$. The background-error covariance is $\\,\\mathbf{B}=\\mathrm{diag}(1,4)\\,$ and the observation-error variance is $\\,\\mathbf{R}=0.5\\,$ (a scalar). Suppose the current outer-loop linearization point is $\\,\\mathbf{x}_{k}=\\begin{pmatrix}1\\\\-1\\end{pmatrix}\\,$, the background state is $\\,\\mathbf{x}_{b}=\\begin{pmatrix}0\\\\0\\end{pmatrix}\\,$, and the observed value is $\\,y=1\\,$. \n\nDefine the incremental inner-loop cost (as used in the Three-Dimensional Variational (3D-Var) setting after linearization of a possibly nonlinear observation operator) by\n$$\nJ(\\delta \\mathbf{x}) \\;=\\; \\tfrac{1}{2}\\,(\\delta \\mathbf{x}-\\delta \\mathbf{x}_{b})^{\\top}\\,\\mathbf{B}^{-1}\\,(\\delta \\mathbf{x}-\\delta \\mathbf{x}_{b})\n\\;+\\;\\tfrac{1}{2}\\,\\bigl(\\mathbf{H}\\,\\delta \\mathbf{x}-\\mathbf{d}\\bigr)^{\\top}\\,\\mathbf{R}^{-1}\\,\\bigl(\\mathbf{H}\\,\\delta \\mathbf{x}-\\mathbf{d}\\bigr),\n$$\nwhere $\\,\\delta \\mathbf{x}=\\mathbf{x}-\\mathbf{x}_{k}\\,$, $\\,\\delta \\mathbf{x}_{b}=\\mathbf{x}_{b}-\\mathbf{x}_{k}\\,$, and the innovation at the linearization point is $\\,\\mathbf{d} = y - \\mathbf{H}\\,\\mathbf{x}_{k}\\,$.\n\n- Compute the inner-loop gradient $\\,\\nabla J(\\delta \\mathbf{x})\\,$ evaluated at the initial inner-loop iterate $\\,\\delta \\mathbf{x}=\\mathbf{0}\\,$.\n- Compute the Gauss–Newton step $\\,\\delta \\mathbf{x}^{\\star}\\,$ that minimizes $\\,J(\\delta \\mathbf{x})\\,$.\n\nProvide the final answer as a single row matrix $\\,\\begin{pmatrix} g_{1}  g_{2}  \\delta x_{1}^{\\star}  \\delta x_{2}^{\\star}\\end{pmatrix}\\,$ containing the two components of the gradient at $\\,\\delta \\mathbf{x}=\\mathbf{0}\\,$ followed by the two components of the minimizing step. Express all values exactly; no rounding is required.",
            "solution": "The problem asks for two quantities related to an incremental variational data assimilation cost function, $J(\\delta \\mathbf{x})$. First, the gradient of the cost function evaluated at the initial inner-loop iterate, $\\delta \\mathbf{x} = \\mathbf{0}$. Second, the Gauss-Newton step, $\\delta \\mathbf{x}^{\\star}$, which minimizes $J(\\delta \\mathbf{x})$.\n\nThe cost function is given by\n$$\nJ(\\delta \\mathbf{x}) = \\frac{1}{2}(\\delta \\mathbf{x}-\\delta \\mathbf{x}_{b})^{\\top}\\mathbf{B}^{-1}(\\delta \\mathbf{x}-\\delta \\mathbf{x}_{b}) + \\frac{1}{2}(\\mathbf{H}\\delta \\mathbf{x}-\\mathbf{d})^{\\top}\\mathbf{R}^{-1}(\\mathbf{H}\\delta \\mathbf{x}-\\mathbf{d})\n$$\nThe provided data are:\n- Observation operator: $\\mathbf{H} = \\begin{pmatrix} 1  2 \\end{pmatrix}$\n- Background-error covariance: $\\mathbf{B} = \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix}$\n- Observation-error variance: $R = 0.5$\n- Outer-loop linearization point: $\\mathbf{x}_{k} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$\n- Background state: $\\mathbf{x}_{b} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n- Observation: $y = 1$\n\nFirst, we must compute the defined increments $\\delta \\mathbf{x}_{b}$ and $\\mathbf{d}$.\nThe background increment is $\\delta \\mathbf{x}_{b} = \\mathbf{x}_{b} - \\mathbf{x}_{k}$:\n$$\n\\delta \\mathbf{x}_{b} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}\n$$\nThe innovation at the linearization point, $\\mathbf{d}$, is defined as $\\mathbf{d} = y - \\mathbf{H}\\mathbf{x}_{k}$. Since $y$ is a scalar and $\\mathbf{H}\\mathbf{x}_k$ results in a scalar, $\\mathbf{d}$ is a scalar.\n$$\n\\mathbf{H}\\mathbf{x}_{k} = \\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = (1)(1) + (2)(-1) = 1 - 2 = -1\n$$\n$$\n\\mathbf{d} = 1 - (-1) = 2\n$$\n\nNext, we compute the gradient of the cost function, $\\nabla J(\\delta \\mathbf{x})$. Using standard rules of matrix calculus for quadratic forms, the gradient is:\n$$\n\\nabla J(\\delta \\mathbf{x}) = \\mathbf{B}^{-1}(\\delta \\mathbf{x} - \\delta \\mathbf{x}_{b}) + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}(\\mathbf{H}\\delta \\mathbf{x} - \\mathbf{d})\n$$\nWe need to evaluate this gradient at $\\delta \\mathbf{x} = \\mathbf{0}$:\n$$\n\\nabla J(\\mathbf{0}) = \\mathbf{B}^{-1}(\\mathbf{0} - \\delta \\mathbf{x}_{b}) + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}(\\mathbf{H}\\mathbf{0} - \\mathbf{d}) = -\\mathbf{B}^{-1}\\delta \\mathbf{x}_{b} - \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{d}\n$$\nWe need the inverse matrices $\\mathbf{B}^{-1}$ and $\\mathbf{R}^{-1}$.\n$$\n\\mathbf{B} = \\begin{pmatrix} 1  0 \\\\ 0  4 \\end{pmatrix} \\implies \\mathbf{B}^{-1} = \\begin{pmatrix} 1^{-1}  0 \\\\ 0  4^{-1} \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{pmatrix}\n$$\n$$\nR = 0.5 = \\frac{1}{2} \\implies R^{-1} = 2\n$$\nNow, we can substitute the values into the expression for $\\nabla J(\\mathbf{0})$:\n$$\n\\nabla J(\\mathbf{0}) = - \\begin{pmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} (2) (2)\n$$\n$$\n\\nabla J(\\mathbf{0}) = - \\begin{pmatrix} -1 \\\\ \\frac{1}{4} \\end{pmatrix} - 4 \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -\\frac{1}{4} \\end{pmatrix} - \\begin{pmatrix} 4 \\\\ 8 \\end{pmatrix}\n$$\n$$\n\\nabla J(\\mathbf{0}) = \\begin{pmatrix} 1 - 4 \\\\ -\\frac{1}{4} - 8 \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ -\\frac{1}{4} - \\frac{32}{4} \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ -\\frac{33}{4} \\end{pmatrix}\n$$\nThe components of the gradient at $\\delta \\mathbf{x}=\\mathbf{0}$ are $g_1 = -3$ and $g_2 = -\\frac{33}{4}$.\n\nTo find the minimizer $\\delta \\mathbf{x}^{\\star}$, we set the gradient $\\nabla J(\\delta \\mathbf{x})$ to zero.\n$$\n\\nabla J(\\delta \\mathbf{x}^{\\star}) = \\mathbf{B}^{-1}(\\delta \\mathbf{x}^{\\star} - \\delta \\mathbf{x}_{b}) + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}(\\mathbf{H}\\delta \\mathbf{x}^{\\star} - \\mathbf{d}) = \\mathbf{0}\n$$\nRearranging the terms gives a linear system for $\\delta \\mathbf{x}^{\\star}$:\n$$\n(\\mathbf{B}^{-1} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H})\\delta \\mathbf{x}^{\\star} = \\mathbf{B}^{-1}\\delta \\mathbf{x}_{b} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{d}\n$$\nThe matrix $(\\mathbf{B}^{-1} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H})$ is the Hessian of the cost function, $\\nabla^2 J$. The right-hand side is $-\\nabla J(\\mathbf{0})$. So, the system is $\\nabla^2 J \\, \\delta \\mathbf{x}^{\\star} = -\\nabla J(\\mathbf{0})$.\n\nLet's compute the Hessian matrix:\n$$\n\\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} (2) \\begin{pmatrix} 1  2 \\end{pmatrix} = 2 \\begin{pmatrix} (1)(1)  (1)(2) \\\\ (2)(1)  (2)(2) \\end{pmatrix} = 2 \\begin{pmatrix} 1  2 \\\\ 2  4 \\end{pmatrix} = \\begin{pmatrix} 2  4 \\\\ 4  8 \\end{pmatrix}\n$$\n$$\n\\nabla^2 J = \\mathbf{B}^{-1} + \\mathbf{H}^{\\top}\\mathbf{R}^{-1}\\mathbf{H} = \\begin{pmatrix} 1  0 \\\\ 0  \\frac{1}{4} \\end{pmatrix} + \\begin{pmatrix} 2  4 \\\\ 4  8 \\end{pmatrix} = \\begin{pmatrix} 3  4 \\\\ 4  8+\\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 3  4 \\\\ 4  \\frac{33}{4} \\end{pmatrix}\n$$\nThe right-hand side is $-\\nabla J(\\mathbf{0}) = -\\begin{pmatrix} -3 \\\\ -\\frac{33}{4} \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ \\frac{33}{4} \\end{pmatrix}$.\nThe linear system to solve for $\\delta \\mathbf{x}^{\\star} = \\begin{pmatrix} \\delta x_1^{\\star} \\\\ \\delta x_2^{\\star} \\end{pmatrix}$ is:\n$$\n\\begin{pmatrix} 3  4 \\\\ 4  \\frac{33}{4} \\end{pmatrix} \\begin{pmatrix} \\delta x_1^{\\star} \\\\ \\delta x_2^{\\star} \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ \\frac{33}{4} \\end{pmatrix}\n$$\nThis represents two simultaneous equations:\n1) $3\\delta x_1^{\\star} + 4\\delta x_2^{\\star} = 3$\n2) $4\\delta x_1^{\\star} + \\frac{33}{4}\\delta x_2^{\\star} = \\frac{33}{4}$\n\nFrom equation (1), we express $\\delta x_1^{\\star}$:\n$$\n\\delta x_1^{\\star} = \\frac{3 - 4\\delta x_2^{\\star}}{3} = 1 - \\frac{4}{3}\\delta x_2^{\\star}\n$$\nSubstitute this into equation (2):\n$$\n4\\left(1 - \\frac{4}{3}\\delta x_2^{\\star}\\right) + \\frac{33}{4}\\delta x_2^{\\star} = \\frac{33}{4}\n$$\n$$\n4 - \\frac{16}{3}\\delta x_2^{\\star} + \\frac{33}{4}\\delta x_2^{\\star} = \\frac{33}{4}\n$$\n$$\n4 - \\frac{33}{4} = \\left(\\frac{16}{3} - \\frac{33}{4}\\right)\\delta x_2^{\\star}\n$$\n$$\n\\frac{16 - 33}{4} = \\left(\\frac{64 - 99}{12}\\right)\\delta x_2^{\\star}\n$$\n$$\n\\frac{-17}{4} = \\frac{-35}{12}\\delta x_2^{\\star}\n$$\n$$\n\\delta x_2^{\\star} = \\frac{-17}{4} \\cdot \\frac{12}{-35} = \\frac{17 \\cdot 3}{35} = \\frac{51}{35}\n$$\nNow, substitute $\\delta x_2^{\\star}$ back to find $\\delta x_1^{\\star}$:\n$$\n\\delta x_1^{\\star} = 1 - \\frac{4}{3}\\left(\\frac{51}{35}\\right) = 1 - \\frac{4 \\cdot 17}{35} = 1 - \\frac{68}{35} = \\frac{35-68}{35} = -\\frac{33}{35}\n$$\nSo the minimizer is $\\delta \\mathbf{x}^{\\star} = \\begin{pmatrix} -\\frac{33}{35} \\\\ \\frac{51}{35} \\end{pmatrix}$.\n\nThe final answer requires a row matrix containing the two components of the gradient followed by the two components of the minimizing step: $\\begin{pmatrix} g_1  g_2  \\delta x_1^{\\star}  \\delta x_2^{\\star} \\end{pmatrix}$.\n$$\n\\begin{pmatrix} -3  -\\frac{33}{4}  -\\frac{33}{35}  \\frac{51}{35} \\end{pmatrix}\n$$",
            "answer": "$$\n\\boxed{\\begin{pmatrix} -3  -\\frac{33}{4}  -\\frac{33}{35}  \\frac{51}{35} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "Having mastered the mechanics of the inner loop, we now address the reason for the outer loop's existence: nonlinearity. This practice problem introduces a simple scalar system with a nonlinear observation operator, requiring you to perform one complete outer-loop iteration. You will linearize the operator, formulate and solve the resulting quadratic inner-loop problem for the state increment, and finally update the state estimate, thus completing a full Gauss-Newton cycle .",
            "id": "3409194",
            "problem": "Consider a scalar variational data assimilation problem in which the state variable is $x \\in \\mathbb{R}$ and the observation operator is nonlinear, given by $\\mathcal{H}(x)=\\sin(x)$. The standard variational data assimilation cost functional is\n$$\nJ(x) = \\frac{1}{2} (x - x_{b})^{\\top} B^{-1} (x - x_{b}) + \\frac{1}{2} \\big(y - \\mathcal{H}(x)\\big)^{\\top} R^{-1} \\big(y - \\mathcal{H}(x)\\big),\n$$\nwhere $x_{b}$ is the background (prior) state, $B$ is the background error covariance, $y$ is the observation, and $R$ is the observation error covariance. In the incremental outer-loop and inner-loop strategy, one linearizes the observation operator at the current outer-loop linearization point $x_{0}$, writes $x = x_{0} + \\delta x$, and solves the inner-loop quadratic problem for the increment $\\delta x$ before updating $x$ and relinearizing.\n\nGiven the scalar data $x_{b} = 0$, $B = 1$, $R = 0.1$, $y = 0.5$, perform a single outer-loop step starting from $x_{0} = x_{b}$ as follows:\n- Using a first-order Taylor linearization of $\\mathcal{H}(x)$ at $x_{0}$, formulate the corresponding inner-loop quadratic cost in terms of the increment $\\delta x$, and analytically minimize it to obtain the inner-loop solution $\\delta x$.\n- Apply the outer-loop update $x_{1} = x_{0} + \\delta x$.\n\nExpress your final numerical values exactly as fractions if possible. No rounding is required. The final answer must consist of the pair $\\delta x$ and $x_{1}$ written as a single row matrix.",
            "solution": "The problem asks us to perform one full outer-loop step of an incremental variational data assimilation scheme. This involves linearizing the nonlinear observation operator, solving the resulting quadratic inner-loop problem, and updating the state estimate.\n\nThe scalar cost functional is given by:\n$$\nJ(x) = \\frac{1}{2} \\frac{(x - x_{b})^{2}}{B} + \\frac{1}{2} \\frac{(y - \\mathcal{H}(x))^{2}}{R}\n$$\nThe data provided are:\n-   Nonlinear observation operator: $\\mathcal{H}(x) = \\sin(x)$\n-   Background state: $x_{b} = 0$\n-   Background error variance: $B = 1$\n-   Observation: $y = 0.5$\n-   Observation error variance: $R = 0.1$\n-   The initial state for the outer loop is $x_{0} = x_{b} = 0$.\n\n**Step 1: Linearize the Observation Operator**\nWe linearize $\\mathcal{H}(x)$ around the current state estimate $x_{0} = 0$. The linearized operator, denoted by the scalar $H$, is the derivative of $\\mathcal{H}(x)$ evaluated at $x_{0}$:\n$$\nH = \\left.\\frac{d\\mathcal{H}}{dx}\\right|_{x=x_0} = \\left.\\cos(x)\\right|_{x=0} = \\cos(0) = 1\n$$\nThe Taylor approximation of $\\mathcal{H}(x)$ for an increment $\\delta x$ is:\n$$\n\\mathcal{H}(x_{0} + \\delta x) \\approx \\mathcal{H}(x_{0}) + H \\delta x = \\sin(0) + (1) \\delta x = \\delta x\n$$\n\n**Step 2: Formulate and Solve the Inner-Loop Problem**\nWe substitute the linearized operator into the cost function to form the inner-loop cost function, $J_{inner}(\\delta x)$, which is quadratic in the increment $\\delta x$.\nThe state is $x = x_0 + \\delta x = \\delta x$. The background term is $x - x_b = \\delta x - 0 = \\delta x$.\nThe observation term is $y - \\mathcal{H}(x) \\approx y - (\\mathcal{H}(x_0) + H\\delta x) = 0.5 - (0 + 1 \\cdot \\delta x) = 0.5 - \\delta x$.\nSo, the inner-loop cost is:\n$$\nJ_{inner}(\\delta x) = \\frac{1}{2} \\frac{(\\delta x)^2}{B} + \\frac{1}{2} \\frac{(0.5 - \\delta x)^2}{R}\n$$\nSubstituting $B=1$ and $R=0.1$:\n$$\nJ_{inner}(\\delta x) = \\frac{1}{2} (\\delta x)^2 + \\frac{1}{2} \\frac{(0.5 - \\delta x)^2}{0.1} = \\frac{1}{2}(\\delta x)^2 + 5(0.5 - \\delta x)^2\n$$\nTo find the minimum, we set the derivative with respect to $\\delta x$ to zero:\n$$\n\\frac{dJ_{inner}}{d(\\delta x)} = \\delta x + 5 \\cdot 2(0.5 - \\delta x)(-1) = \\delta x - 10(0.5 - \\delta x) = \\delta x - 5 + 10\\delta x = 11\\delta x - 5\n$$\nSetting the derivative to zero:\n$$\n11\\delta x - 5 = 0 \\implies \\delta x = \\frac{5}{11}\n$$\n\n**Step 3: Update the State Estimate**\nThe outer-loop update rule is $x_{1} = x_{0} + \\delta x$.\n$$\nx_{1} = 0 + \\frac{5}{11} = \\frac{5}{11}\n$$\nThe required pair is $(\\delta x, x_1)$.",
            "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{5}{11}  \\frac{5}{11} \\end{pmatrix}}\n$$"
        },
        {
            "introduction": "The power of variational methods extends beyond simple state estimation to include model improvement through parameter estimation. This advanced exercise demonstrates how the outer-loop/inner-loop framework can be applied to a joint state-parameter estimation problem. By augmenting the state vector to include an unknown model parameter, you will see how the same principles of linearization and incremental updates can be used to simultaneously refine both the state and the model itself .",
            "id": "3409148",
            "problem": "Consider joint state–parameter variational data assimilation for a single scalar observation with the observation operator defined by $h(x,p)=p\\,x$. Let the augmented control vector be $z=\\begin{pmatrix}x  p\\end{pmatrix}^{\\top}$. Assume a Gaussian prior $z\\sim\\mathcal{N}(z_{b},B)$ with $z_{b}=\\begin{pmatrix}x_{b}  p_{b}\\end{pmatrix}^{\\top}$ and $B=\\mathrm{diag}(1,1)$, and a Gaussian likelihood $y\\mid z\\sim\\mathcal{N}(h(x,p),R)$ with $R=0.1$. The Maximum A Posteriori (MAP) estimator arises from minimizing the quadratic Bayesian objective\n$$\nJ(z)=\\frac{1}{2}\\,(z-z_{b})^{\\top}B^{-1}(z-z_{b})+\\frac{1}{2}\\,\\big(y-h(x,p)\\big)^{\\top}R^{-1}\\big(y-h(x,p)\\big).\n$$\nAn outer-loop and inner-loop strategy is employed: at outer-loop index $k$, the observation operator is linearized at the current iterate $z^{k}$, and the inner loop solves exactly the linearized Gaussian least-squares problem for the increment $\\delta z=z-z^{k}$ (equivalently, solves the inner symmetric block system associated with the linearized Karush–Kuhn–Tucker conditions). Starting from the background $x_{b}=1$ and $p_{b}=2$ with the scalar observation $y=3$, perform one outer-loop update by linearizing at $z^{0}=z_{b}$ and solving the inner problem exactly to obtain $z^{1}=z^{0}+\\delta z$. Express your final answer as the exact pair $\\begin{pmatrix}x^{1}  p^{1}\\end{pmatrix}$ using row-matrix notation. No rounding is required, and no physical units are involved.",
            "solution": "The fundamental base is Bayesian inference under Gaussian prior and Gaussian likelihood. The joint Maximum A Posteriori (MAP) estimator minimizes\n$$\nJ(z)=\\frac{1}{2}\\,(z-z_{b})^{\\top}B^{-1}(z-z_{b})+\\frac{1}{2}\\,\\big(y-h(x,p)\\big)^{\\top}R^{-1}\\big(y-h(x,p)\\big),\n$$\nwhere $z=\\begin{pmatrix}x  p\\end{pmatrix}^{\\top}$, $z_{b}=\\begin{pmatrix}x_{b}  p_{b}\\end{pmatrix}^{\\top}$, $B=\\mathrm{diag}(1,1)$, $R=0.1$, and $h(x,p)=p\\,x$. The outer-loop and inner-loop approach applies a Gauss–Newton linearization of the observation operator in the outer loop and solves an incremental quadratic problem in the inner loop. At outer-loop index $k$ with current iterate $z^{k}$, define the increment $\\delta z=z-z^{k}$. The linearization of $h$ at $z^{k}$ is\n$$\nh(x,p)\\approx h(x^{k},p^{k})+G^{k}\\,\\delta z,\n$$\nwhere $G^{k}$ is the Jacobian of $h$ with respect to $(x,p)$ at $z^{k}$:\n$$\nG^{k}=\\begin{pmatrix}\\frac{\\partial h}{\\partial x}  \\frac{\\partial h}{\\partial p}\\end{pmatrix}\\bigg|_{(x^{k},p^{k})}=\\begin{pmatrix}p^{k}  x^{k}\\end{pmatrix}.\n$$\nStarting at $k=0$ with $z^0 = z_b$, the incremental cost function for $\\delta z$ is\n$$ J_{0}(\\delta z)=\\frac{1}{2}\\,(z^0+\\delta z-z_{b})^{\\top}B^{-1}(z^0+\\delta z-z_{b})+\\frac{1}{2}\\,\\big(y-(h(z^0) + G^0 \\delta z)\\big)^{\\top}R^{-1}\\big(y-(h(z^0) + G^0 \\delta z)\\big). $$\nSince $z^0 - z_b = 0$, this simplifies to\n$$ J_0(\\delta z)=\\frac{1}{2}\\,\\delta z^{\\top}B^{-1}\\delta z+\\frac{1}{2}\\,\\big(d^{0}-G^{0}\\delta z\\big)^{\\top}R^{-1}\\big(d^{0}-G^{0}\\delta z\\big), $$\nwhere the innovation is $d^{0}=y-h(x^{0},p^{0})$. Setting the gradient with respect to $\\delta z$ to zero yields the normal equations\n$$\n\\big(B^{-1}+(G^{0})^{\\top}R^{-1}G^{0}\\big)\\,\\delta z=(G^{0})^{\\top}R^{-1}d^{0}.\n$$\nWe now instantiate the given data at the background iterate $z^{0}=z_{b}$:\n$$\nx^{0}=x_{b}=1,\\qquad p^{0}=p_{b}=2,\\qquad B=\\mathrm{diag}(1,1),\\qquad R=0.1.\n$$\nCompute the Jacobian at $z^{0}$:\n$$\nG^{0}=\\begin{pmatrix}p^{0}  x^{0}\\end{pmatrix}=\\begin{pmatrix}2  1\\end{pmatrix}.\n$$\nCompute the innovation:\n$$\nd^{0}=y-h(x^{0},p^{0})=3-(p^{0}x^{0})=3-(2\\cdot 1)=1.\n$$\nCompute the matrices appearing in the normal equations. First,\n$$\nB^{-1}=\\mathrm{diag}(1,1),\n$$\nand\n$$\nR^{-1}=\\frac{1}{0.1}=10.\n$$\nThen\n$$\n(G^{0})^{\\top}R^{-1}G^{0}=10\\,(G^{0})^{\\top}G^{0}=10\\begin{pmatrix}2 \\\\ 1\\end{pmatrix}\\begin{pmatrix}2  1\\end{pmatrix}=10\\begin{pmatrix}4  2 \\\\ 2  1\\end{pmatrix}=\\begin{pmatrix}40  20 \\\\ 20  10\\end{pmatrix}.\n$$\nTherefore, the Hessian matrix of the inner-loop problem is\n$$\nM:=B^{-1}+(G^{0})^{\\top}R^{-1}G^{0}=\\begin{pmatrix}1  0 \\\\ 0  1\\end{pmatrix}+\\begin{pmatrix}40  20 \\\\ 20  10\\end{pmatrix}=\\begin{pmatrix}41  20 \\\\ 20  11\\end{pmatrix}.\n$$\nThe right-hand side is\n$$\nb:=(G^{0})^{\\top}R^{-1}d^{0}=10\\,d^{0}\\,(G^{0})^{\\top}=10\\cdot 1\\cdot \\begin{pmatrix}2 \\\\ 1\\end{pmatrix}=\\begin{pmatrix}20 \\\\ 10\\end{pmatrix}.\n$$\nWe solve the $2\\times 2$ linear system $M\\,\\delta z=b$. The determinant is\n$$\n\\det(M)=41\\cdot 11-20\\cdot 20=451-400=51.\n$$\nHence,\n$$\nM^{-1}=\\frac{1}{\\det(M)}\\begin{pmatrix}11  -20 \\\\ -20  41\\end{pmatrix}=\\frac{1}{51}\\begin{pmatrix}11  -20 \\\\ -20  41\\end{pmatrix}.\n$$\nMultiplying by $b$ gives\n$$\n\\delta z=M^{-1}b=\\frac{1}{51}\\begin{pmatrix}11  -20 \\\\ -20  41\\end{pmatrix}\\begin{pmatrix}20 \\\\ 10\\end{pmatrix}=\\frac{1}{51}\\begin{pmatrix}11\\cdot 20-20\\cdot 10 \\\\ -20\\cdot 20+41\\cdot 10\\end{pmatrix}=\\frac{1}{51}\\begin{pmatrix}220-200 \\\\ -400+410\\end{pmatrix}=\\frac{1}{51}\\begin{pmatrix}20 \\\\ 10\\end{pmatrix}.\n$$\nThus,\n$$\n\\delta x=\\frac{20}{51},\\qquad \\delta p=\\frac{10}{51}.\n$$\nThe outer-loop update is\n$$\nz^{1}=z^{0}+\\delta z=\\begin{pmatrix}x^{0} \\\\ p^{0}\\end{pmatrix}+\\begin{pmatrix}\\delta x \\\\ \\delta p\\end{pmatrix}=\\begin{pmatrix}1 \\\\ 2\\end{pmatrix}+\\begin{pmatrix}\\frac{20}{51} \\\\ \\frac{10}{51}\\end{pmatrix}=\\begin{pmatrix}\\frac{71}{51} \\\\ \\frac{112}{51}\\end{pmatrix}.\n$$\nIn row-matrix notation for the requested pair $\\begin{pmatrix}x^{1}  p^{1}\\end{pmatrix}$, the exact result is\n$$\n\\begin{pmatrix}\\frac{71}{51}  \\frac{112}{51}\\end{pmatrix}.\n$$",
            "answer": "$$\\boxed{\\begin{pmatrix}\\frac{71}{51}  \\frac{112}{51}\\end{pmatrix}}$$"
        }
    ]
}