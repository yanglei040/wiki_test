{
    "hands_on_practices": [
        {
            "introduction": "Before using a gradient in an optimization, you must be confident it is correct. This is especially true for adjoint-based gradients, where complex code can easily hide subtle bugs. The directional derivative test, or Taylor test, provides the gold standard for verification by comparing the action of the adjoint-computed gradient on a vector, $\\nabla J(p) \\cdot \\delta p$, against a numerical finite difference approximation. This practice explores the theoretical underpinnings of this essential diagnostic tool, forcing a confrontation with the trade-offs between truncation error and floating-point precision that govern its accuracy. ",
            "id": "3364135",
            "problem": "Consider an inverse problem in which a parameter vector $p \\in \\mathbb{R}^n$ is estimated by minimizing a differentiable cost functional $J(p)$ that depends on the solution of a numerical model. An adjoint method is implemented to compute the gradient $\\nabla J(p)$ with respect to $p$. To assess the correctness of the adjoint-based gradient, a directional derivative test is performed by selecting a direction $\\delta p \\in \\mathbb{R}^n$ and comparing the inner product $\\nabla J(p) \\cdot \\delta p$ against a finite difference approximation of the directional derivative of $J$ along $\\delta p$.\n\nUsing the definition of the directional derivative and the Taylor expansion of $J$ about $p$, reason from first principles about the expected behavior of such a test. In particular, analyze how the choice of the step size $\\epsilon  0$ affects truncation error and floating-point round-off error, and explain how these error mechanisms influence accuracy and the ability to detect adjoint implementation errors.\n\nWhich of the following statements about the directional derivative test and the role of $\\epsilon$ are correct? Select all that apply.\n\nA. If $J$ is twice continuously differentiable, the mismatch\n$$E_{\\mathrm{fwd}}(\\epsilon) = \\left\\lvert \\nabla J(p)\\cdot \\delta p - \\frac{J(p+\\epsilon\\,\\delta p)-J(p)}{\\epsilon} \\right\\rvert$$\ndecreases proportionally to $\\epsilon$ as $\\epsilon \\to 0$ until floating-point round-off dominates, so a log-log plot of $E_{\\mathrm{fwd}}(\\epsilon)$ versus $\\epsilon$ exhibits slope $1$ in the truncation-dominated regime. Using the central difference\n$$\\frac{J(p+\\epsilon\\,\\delta p)-J(p-\\epsilon\\,\\delta p)}{2\\epsilon}$$\ninstead yields a truncation-dominated slope of $2$.\n\nB. If the numerical evaluation of $J$ incurs floating-point round-off on the order of the machine precision $\\varepsilon_{\\mathrm{mach}}$, the round-off component of the finite difference error scales like $\\mathcal{O}(\\varepsilon_{\\mathrm{mach}}/\\epsilon)$, so choosing $\\epsilon$ too small causes the test error to grow due to catastrophic cancellation.\n\nC. Balancing truncation error against round-off error yields an optimal step size of $\\epsilon \\sim \\sqrt{\\varepsilon_{\\mathrm{mach}}}$ (after appropriate scaling to the magnitudes of $p$ and $\\delta p$) for the forward difference, whereas for the central difference the optimal step size is $\\epsilon \\sim \\varepsilon_{\\mathrm{mach}}^{1/3}$.\n\nD. A sign error in the adjoint implementation (i.e., returning $-\\nabla J(p)$) will be invisible to the central difference test but detectable by the forward difference test.\n\nE. For robustness, $\\delta p$ should be scaled so that $\\lVert \\delta p \\rVert = 1$ in the chosen parameter norm and multiple random directions should be tested; if the adjoint is correct, the mismatch exhibits the expected $\\mathcal{O}(\\epsilon)$ or $\\mathcal{O}(\\epsilon^2)$ decay with $\\epsilon$ in the truncation-dominated regime, whereas a lack of such decay across directions suggests an adjoint error.\n\nF. If the model solver employs adaptive tolerances that vary with $p$ and with the perturbation $\\epsilon\\,\\delta p$, the directional derivative test remains valid because solver tolerances do not affect differentiability and therefore cannot distort the error behavior.\n\nG. Using the central difference eliminates truncation error completely, so the only remaining error source is round-off; therefore, smaller $\\epsilon$ always improves the accuracy of the test.\n\nH. If $J$ contains nondifferentiable components with respect to $p$, such as exact thresholding or a pointwise maximum, the directional derivative test may not exhibit the expected truncation-rate behavior with $\\epsilon$; in such cases, smoothing or reformulation is required for a reliable adjoint test.",
            "solution": "The problem statement is a valid and well-posed question in the field of numerical optimization and inverse problems. It asks for a first-principles analysis of the directional derivative test, a standard method for verifying the implementation of an adjoint-based gradient computation.\n\n### **Problem Validation**\n\n**Step 1: Extract Givens**\n- The parameter to be estimated is a vector $p \\in \\mathbb{R}^n$.\n- The objective is to minimize a differentiable cost functional $J(p)$.\n- The gradient $\\nabla J(p)$ is computed using an adjoint method.\n- A directional derivative test is used for verification.\n- A direction vector is chosen: $\\delta p \\in \\mathbb{R}^n$.\n- The test compares the inner product $\\nabla J(p) \\cdot \\delta p$ with a finite difference approximation of the directional derivative of $J$ along $\\delta p$.\n- The finite difference step size is $\\epsilon  0$.\n- The task is to analyze the expected behavior of this test, focusing on the roles of truncation error and round-off error as functions of $\\epsilon$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It describes a fundamental sanity check for gradient-based optimization codes, known as a Taylor test or finite difference check. The concepts of Taylor expansion, directional derivatives, truncation error, and floating-point round-off error are cornerstone principles of calculus and numerical analysis. The setup is self-contained and free of contradictions or ambiguities.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We will proceed with the derivation and analysis.\n\n### **First-Principles Derivation**\n\nThe core of the analysis rests on Taylor's theorem. The directional derivative of a differentiable functional $J(p)$ at a point $p$ in the direction $\\delta p$ is defined as:\n$$ D_{\\delta p}J(p) = \\lim_{\\epsilon \\to 0} \\frac{J(p+\\epsilon\\,\\delta p) - J(p)}{\\epsilon} $$\nFor a differentiable functional, this is equivalent to the inner product of the gradient and the direction vector:\n$$ D_{\\delta p}J(p) = \\nabla J(p) \\cdot \\delta p $$\nThe adjoint method provides a computed value for $\\nabla J(p)$, and thus for $\\nabla J(p) \\cdot \\delta p$. The directional derivative test validates this computed value against a finite difference approximation of the derivative.\n\nLet us analyze the two most common finite difference schemes. We assume $J$ is at least three times continuously differentiable ($C^3$) for this analysis.\n\n**1. Forward Difference Approximation:**\nThe approximation is given by:\n$$ D_{\\mathrm{fwd}}(\\epsilon) = \\frac{J(p+\\epsilon\\,\\delta p) - J(p)}{\\epsilon} $$\nTo analyze the error, we use the Taylor expansion of $J(p+\\epsilon\\,\\delta p)$ around $p$:\n$$ J(p+\\epsilon\\,\\delta p) = J(p) + \\epsilon (\\nabla J(p) \\cdot \\delta p) + \\frac{\\epsilon^2}{2} (\\delta p^T H(p) \\delta p) + \\mathcal{O}(\\epsilon^3) $$\nwhere $H(p)$ is the Hessian matrix of $J$ at $p$. Rearranging and dividing by $\\epsilon$ gives:\n$$ \\frac{J(p+\\epsilon\\,\\delta p) - J(p)}{\\epsilon} = \\nabla J(p) \\cdot \\delta p + \\frac{\\epsilon}{2} (\\delta p^T H(p) \\delta p) + \\mathcal{O}(\\epsilon^2) $$\nThe mismatch defined in option A is the absolute difference between the adjoint-based result and this approximation. If the adjoint is correct, the mismatch is purely the approximation error:\n$$ E_{\\mathrm{fwd}}(\\epsilon) = \\left\\lvert \\nabla J(p)\\cdot \\delta p - D_{\\mathrm{fwd}}(\\epsilon) \\right\\rvert = \\left\\lvert -\\frac{\\epsilon}{2} (\\delta p^T H(p) \\delta p) - \\mathcal{O}(\\epsilon^2) \\right\\rvert $$\nThis is the **truncation error**, and it is of order $\\mathcal{O}(\\epsilon)$.\n\n**2. Central Difference Approximation:**\nThe approximation is given by:\n$$ D_{\\mathrm{cen}}(\\epsilon) = \\frac{J(p+\\epsilon\\,\\delta p) - J(p-\\epsilon\\,\\delta p)}{2\\epsilon} $$\nTo analyze this, we need the Taylor expansions for both $J(p+\\epsilon\\,\\delta p)$ and $J(p-\\epsilon\\,\\delta p)$:\n$$ J(p+\\epsilon\\,\\delta p) = J(p) + \\epsilon (\\nabla J \\cdot \\delta p) + \\frac{\\epsilon^2}{2} (\\delta p^T H \\delta p) + \\frac{\\epsilon^3}{6} T(p)(\\delta p, \\delta p, \\delta p) + \\mathcal{O}(\\epsilon^4) $$\n$$ J(p-\\epsilon\\,\\delta p) = J(p) - \\epsilon (\\nabla J \\cdot \\delta p) + \\frac{\\epsilon^2}{2} (\\delta p^T H \\delta p) - \\frac{\\epsilon^3}{6} T(p)(\\delta p, \\delta p, \\delta p) + \\mathcal{O}(\\epsilon^4) $$\nwhere $T(p)$ represents the third-order tensor of derivatives. Subtracting the second from the first:\n$$ J(p+\\epsilon\\,\\delta p) - J(p-\\epsilon\\,\\delta p) = 2\\epsilon (\\nabla J \\cdot \\delta p) + \\frac{\\epsilon^3}{3} T(p)(\\delta p, \\delta p, \\delta p) + \\mathcal{O}(\\epsilon^5) $$\nDividing by $2\\epsilon$ gives:\n$$ \\frac{J(p+\\epsilon\\,\\delta p) - J(p-\\epsilon\\,\\delta p)}{2\\epsilon} = \\nabla J(p) \\cdot \\delta p + \\frac{\\epsilon^2}{6} T(p)(\\delta p, \\delta p, \\delta p) + \\mathcal{O}(\\epsilon^4) $$\nThe **truncation error** for the central difference is therefore of order $\\mathcal{O}(\\epsilon^2)$.\n\n**3. Round-off Error:**\nNumerical evaluation of $J(p)$ using floating-point arithmetic introduces an error. Let $\\tilde{J}(p)$ be the computed value. The error is typically on the order of machine precision, $\\varepsilon_{\\mathrm{mach}}$, times the magnitude of the function value. Thus, $\\tilde{J}(p) \\approx J(p) + \\mathcal{O}(\\varepsilon_{\\mathrm{mach}})$.\nFor the forward difference, the error in the numerator is $\\tilde{J}(p+\\epsilon\\,\\delta p) - \\tilde{J}(p)$. As $\\epsilon \\to 0$, these two values become very close. The subtraction of nearly equal numbers results in a loss of significant digits, an effect known as catastrophic cancellation. The resulting error in the numerator is dominated by the floating-point errors, which are of magnitude $\\mathcal{O}(\\varepsilon_{\\mathrm{mach}})$. Propagating this through the division by $\\epsilon$, the round-off error contribution to the total error is:\n$$ E_{\\mathrm{round}}(\\epsilon) \\sim \\frac{\\mathcal{O}(\\varepsilon_{\\mathrm{mach}})}{\\epsilon} $$\nThis analysis holds for both forward and central differences.\n\n**4. Optimal Step Size:**\nThe total error $E(\\epsilon)$ is the sum of the truncation and round-off errors.\n- For forward differences: $E_{\\mathrm{fwd}}(\\epsilon) \\approx C_1 \\epsilon + C_2 \\frac{\\varepsilon_{\\mathrm{mach}}}{\\epsilon}$. Minimizing this with respect to $\\epsilon$ yields $C_1 \\approx C_2 \\varepsilon_{\\mathrm{mach}} / \\epsilon^2$, which implies $\\epsilon_{\\mathrm{opt}} \\sim \\sqrt{\\varepsilon_{\\mathrm{mach}}}$.\n- For central differences: $E_{\\mathrm{cen}}(\\epsilon) \\approx C_3 \\epsilon^2 + C_4 \\frac{\\varepsilon_{\\mathrm{mach}}}{\\epsilon}$. Minimizing this yields $2C_3\\epsilon \\approx C_4 \\varepsilon_{\\mathrm{mach}} / \\epsilon^2$, which implies $\\epsilon_{\\mathrm{opt}} \\sim \\varepsilon_{\\mathrm{mach}}^{1/3}$.\n\n### **Option-by-Option Analysis**\n\n**A.** If $J$ is twice continuously differentiable, the mismatch $E_{\\mathrm{fwd}}(\\epsilon) = \\left\\lvert \\nabla J(p)\\cdot \\delta p - \\frac{J(p+\\epsilon\\,\\delta p)-J(p)}{\\epsilon} \\right\\rvert$ decreases proportionally to $\\epsilon$ as $\\epsilon \\to 0$ until floating-point round-off dominates, so a log-log plot of $E_{\\mathrm{fwd}}(\\epsilon)$ versus $\\epsilon$ exhibits slope $1$ in the truncation-dominated regime. Using the central difference instead yields a truncation-dominated slope of $2$.\n- **Analysis:** Our first-principles derivation shows that the truncation error for the forward difference is $\\mathcal{O}(\\epsilon)$, which corresponds to a slope of $1$ on a log-log plot ($\\log E \\approx \\log C + 1 \\cdot \\log \\epsilon$). The derivation also shows that for the central difference, the truncation error is $\\mathcal{O}(\\epsilon^2)$, corresponding to a slope of $2$ ($\\log E \\approx \\log C' + 2 \\cdot \\log \\epsilon$). This statement is entirely consistent with the derivation.\n- **Verdict:** **Correct**.\n\n**B.** If the numerical evaluation of $J$ incurs floating-point round-off on the order of the machine precision $\\varepsilon_{\\mathrm{mach}}$, the round-off component of the finite difference error scales like $\\mathcal{O}(\\varepsilon_{\\mathrm{mach}}/\\epsilon)$, so choosing $\\epsilon$ too small causes the test error to grow due to catastrophic cancellation.\n- **Analysis:** Our derivation of the round-off error confirms this behavior. The subtraction of nearly equal numbers $J(p+\\epsilon\\delta p)$ and $J(p)$ or $J(p-\\epsilon\\delta p)$ leads to catastrophic cancellation, causing the round-off error in the numerator to be roughly constant at $\\mathcal{O}(\\varepsilon_{\\mathrm{mach}})$. When divided by $\\epsilon$, this yields an error contribution that scales as $\\mathcal{O}(\\varepsilon_{\\mathrm{mach}}/\\epsilon)$ and grows without bound as $\\epsilon \\to 0$.\n- **Verdict:** **Correct**.\n\n**C.** Balancing truncation error against round-off error yields an optimal step size of $\\epsilon \\sim \\sqrt{\\varepsilon_{\\mathrm{mach}}}$ (after appropriate scaling to the magnitudes of $p$ and $\\delta p$) for the forward difference, whereas for the central difference the optimal step size is $\\epsilon \\sim \\varepsilon_{\\mathrm{mach}}^{1/3}$.\n- **Analysis:** Our analysis of the optimal step size by balancing the competing error terms confirms these dependencies. For the forward difference, balancing $\\mathcal{O}(\\epsilon)$ against $\\mathcal{O}(\\varepsilon_{\\mathrm{mach}}/\\epsilon)$ gives $\\epsilon_{\\mathrm{opt}} \\propto \\varepsilon_{\\mathrm{mach}}^{1/2}$. For the central difference, balancing $\\mathcal{O}(\\epsilon^2)$ against $\\mathcal{O}(\\varepsilon_{\\mathrm{mach}}/\\epsilon)$ gives $\\epsilon_{\\mathrm{opt}} \\propto \\varepsilon_{\\mathrm{mach}}^{1/3}$. The scaling note is also appropriate.\n- **Verdict:** **Correct**.\n\n**D.** A sign error in the adjoint implementation (i.e., returning $-\\nabla J(p)$) will be invisible to the central difference test but detectable by the forward difference test.\n- **Analysis:** A sign error means the computed quantity is $-\\nabla J(p) \\cdot \\delta p$. The test compares this to the finite difference approximation, which approximates the true value, $+\\nabla J(p) \\cdot \\delta p$. The mismatch for the central difference test would be $|(-\\nabla J \\cdot \\delta p) - (\\nabla J \\cdot \\delta p + \\mathcal{O}(\\epsilon^2))| \\approx | -2 \\nabla J \\cdot \\delta p |$. This is a large, $\\mathcal{O}(1)$ error that does not vanish as $\\epsilon \\to 0$. The same holds for the forward difference. A sign error is one of the most glaring errors and is easily detected by both methods. The statement is false.\n- **Verdict:** **Incorrect**.\n\n**E.** For robustness, $\\delta p$ should be scaled so that $\\lVert \\delta p \\rVert = 1$ in the chosen parameter norm and multiple random directions should be tested; if the adjoint is correct, the mismatch exhibits the expected $\\mathcal{O}(\\epsilon)$ or $\\mathcal{O}(\\epsilon^2)$ decay with $\\epsilon$ in the truncation-dominated regime, whereas a lack of such decay across directions suggests an adjoint error.\n- **Analysis:** This statement describes standard best practices for conducting a directional derivative test. Scaling $\\delta p$ ensures the magnitude of the perturbation is controlled. Testing multiple random directions is crucial because an error in the gradient computation might be orthogonal to a single, chosen $\\delta p$, thus being missed by the test. Observing the theoretically-predicted convergence rate (slope on a log-log plot) is the primary indicator of a correct implementation. A failure to observe this rate is a strong indication of an error.\n- **Verdict:** **Correct**.\n\n**F.** If the model solver employs adaptive tolerances that vary with $p$ and with the perturbation $\\epsilon\\,\\delta p$, the directional derivative test remains valid because solver tolerances do not affect differentiability and therefore cannot distort the error behavior.\n- **Analysis:** This statement is fundamentally incorrect. If the solver's behavior (e.g., mesh refinement, number of time steps) changes discretely as $p$ varies, the numerically evaluated functional $J(p)$ is no longer a smooth function of $p$. It acquires a \"numerical noise\" or fine-scale non-smoothness. When a finite difference is taken across one of these discrete changes, the result does not approximate a derivative and will not converge as predicted by Taylor's theorem. This is a well-known and serious issue in practice that can invalidate the test.\n- **Verdict:** **Incorrect**.\n\n**G.** Using the central difference eliminates truncation error completely, so the only remaining error source is round-off; therefore, smaller $\\epsilon$ always improves the accuracy of the test.\n- **Analysis:** This statement contains multiple falsehoods. First, the central difference does not eliminate truncation error; it reduces it to a higher order, $\\mathcal{O}(\\epsilon^2)$. Second, because truncation error is not zero, round-off is not the only error source. Third, because of the $\\mathcal{O}(\\varepsilon_{\\mathrm{mach}}/\\epsilon)$ round-off component, making $\\epsilon$ arbitrarily small will eventually cause the total error to increase, not improve accuracy indefinitely.\n- **Verdict:** **Incorrect**.\n\n**H.** If $J$ contains nondifferentiable components with respect to $p$, such as exact thresholding or a pointwise maximum, the directional derivative test may not exhibit the expected truncation-rate behavior with $\\epsilon$; in such cases, smoothing or reformulation is required for a reliable adjoint test.\n- **Analysis:** The directional derivative test is predicated on the validity of the Taylor series expansion, which requires the functional to be sufficiently smooth (differentiable). At points of non-differentiability (kinks or jumps), the gradient is not uniquely defined, and the Taylor approximation breaks down. A finite difference stencil that straddles such a non-differentiability will not converge at the expected rate. The standard remedy is to replace the non-differentiable component with a smooth approximation (e.g., replacing $\\max(x, 0)$ with a softplus function) before applying the test. This statement accurately captures this critical limitation and its solution.\n- **Verdict:** **Correct**.",
            "answer": "$$\\boxed{ABCEH}$$"
        },
        {
            "introduction": "With a robust verification method in hand, we can proceed to implementation. This exercise provides a complete, hands-on opportunity to build an adjoint solver from first principles for a modern deep learning architecture: the Neural Ordinary Differential Equation (Neural ODE). You will apply the continuous adjoint method to derive the backward-in-time dynamics of the adjoint state and the integral for the parameter gradient. By implementing the forward and backward integrators and verifying your results, you will gain tangible experience with the end-to-end workflow of adjoint-based sensitivity analysis for dynamical systems. ",
            "id": "3333095",
            "problem": "Consider a simple two-species cellular system where the state vector $x(t) \\in \\mathbb{R}^2$ represents nondimensionalized concentrations of messenger ribonucleic acid (mRNA) and protein. The system evolves according to an ordinary differential equation $dx/dt = f_{\\theta}(x,t)$, where $f_{\\theta}$ is parametrized by a small feedforward neural network intended to approximate nonlinear reaction kinetics. Time is measured in seconds. The neural network is defined as follows: the hidden pre-activation is $z = W_1 x + b_1$, the hidden activation is $h = \\tanh(z)$, and the output is\n$$\nf_{\\theta}(x,t) = W_2 h + b_2 - k_d \\odot x,\n$$\nwhere $W_1 \\in \\mathbb{R}^{3 \\times 2}$, $b_1 \\in \\mathbb{R}^{3}$, $W_2 \\in \\mathbb{R}^{2 \\times 3}$, $b_2 \\in \\mathbb{R}^{2}$, $k_d \\in \\mathbb{R}^{2}$, and $\\odot$ denotes elementwise multiplication. The loss is defined on the terminal state at time $T$ by\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{2} \\| x(T; \\theta) - x_{\\mathrm{target}} \\|_2^2.\n$$\nThe initial condition is $x(0) = x_0$. The goal is to compute the gradient $d\\mathcal{L}/d\\theta$ using the adjoint method and to compare it with a finite difference approximation of the gradient to assess correctness and the effect of solver discretization on bias.\n\nStarting from the chain rule of calculus and the definition of the adjoint sensitivity for ordinary differential equations (ODEs), derive the necessary relationships to implement an adjoint-based gradient computation for $d\\mathcal{L}/d\\theta$ that is consistent with the continuous-time neural ODE model stated above. Your program must:\n- Integrate the forward dynamics using a fixed-step, explicit fourth-order Runge–Kutta method.\n- Implement the adjoint integration backward in time using a first-order explicit method with the Jacobian $ \\partial f_{\\theta} / \\partial x $ evaluated along the saved forward trajectory.\n- Accumulate the parameter gradient via a time integral of the instantaneous sensitivity $a(t)^\\top \\, \\partial f_{\\theta} / \\partial \\theta$, where $a(t)$ denotes the adjoint state, and verify the result by central finite differences computed by re-integrating the forward dynamics for parameter perturbations.\n\nYour implementation must be entirely self-contained, making no calls to external data. The forward model and all parameters are specified numerically as follows:\n- Model parameters $\\theta$:\n  - $W_1 = \\begin{bmatrix} 0.8  -0.5 \\\\ 0.3  0.9 \\\\ -0.7  0.2 \\end{bmatrix}$,\n  - $b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix}$,\n  - $W_2 = \\begin{bmatrix} 0.5  -0.3  0.1 \\\\ -0.4  0.6  -0.2 \\end{bmatrix}$,\n  - $b_2 = \\begin{bmatrix} 0.0 \\\\ 0.05 \\end{bmatrix}$,\n  - $k_d = \\begin{bmatrix} 0.3 \\\\ 0.5 \\end{bmatrix}$.\n- Initial state and target:\n  - $x_0 = \\begin{bmatrix} 0.5 \\\\ 0.2 \\end{bmatrix}$,\n  - $x_{\\mathrm{target}} = \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix}$.\n- Final time: $T = 2.0$ seconds.\n\nYou must use a parameter scaling factor $s$ to explore regimes near-linear in the neural component, by scaling the weights and biases as $\\{ W_1, b_1, W_2, b_2 \\} \\mapsto s \\cdot \\{ W_1, b_1, W_2, b_2 \\}$ while leaving $k_d$ unchanged.\n\nDefine the following test suite of cases, each specified by a tuple $(N, \\epsilon, \\tau, s)$ where $N$ is the number of Runge–Kutta steps for the forward integration (constant step size $\\Delta t = T/N$), $\\epsilon$ is the finite-difference step size, $\\tau$ is the absolute tolerance for gradient agreement, and $s$ is the scaling factor:\n- Case $1$: $(N=\\ 500,\\ \\epsilon=\\ 10^{-6},\\ \\tau=\\ 2 \\times 10^{-2},\\ s=\\ 1.0)$,\n- Case $2$: $(N=\\ 50,\\ \\epsilon=\\ 10^{-6},\\ \\tau=\\ 2 \\times 10^{-2},\\ s=\\ 1.0)$,\n- Case $3$: $(N=\\ 20,\\ \\epsilon=\\ 10^{-6},\\ \\tau=\\ 5 \\times 10^{-2},\\ s=\\ 1.0)$,\n- Case $4$: $(N=\\ 60,\\ \\epsilon=\\ 10^{-6},\\ \\tau=\\ 1 \\times 10^{-2},\\ s=\\ 0.05)$.\n\nFor each case, compute:\n- The adjoint-based gradient $d\\mathcal{L}/d\\theta$ using the continuous adjoint with backward explicit integration of the adjoint.\n- The central finite-difference approximation of $d\\mathcal{L}/d\\theta$ using perturbations $\\pm \\epsilon$ applied to each scalar component of $\\theta$ (with the scaling $s$ applied before perturbation).\n- The maximum absolute componentwise difference between the two gradient vectors.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is a boolean indicating whether the maximum absolute difference for that case is strictly less than the prescribed tolerance $\\tau$. For example, the output format must be exactly like $[b_1,b_2,b_3,b_4]$, where each $b_i$ is either $\\mathrm{True}$ or $\\mathrm{False}$ with no spaces. All numeric answers must be unitless scalars except $T$, which is specified in seconds. Angles are not used in this problem. All results are deterministic given the data above and do not require any user input.",
            "solution": "The problem requires the computation of the gradient of a loss function with respect to the parameters of a neural ordinary differential equation (ODE) model. The gradient is to be calculated using the continuous adjoint sensitivity method and verified against a finite difference approximation.\n\n### Principle-Based Design: Adjoint Sensitivity Analysis\n\nThe system is described by an ODE $\\frac{dx}{dt} = f_{\\theta}(x,t)$ with an initial condition $x(0) = x_0$. The objective is to compute the gradient of a loss function $\\mathcal{L}(\\theta) = g(x(T))$ that depends on the state at the final time $T$. By the chain rule, this gradient is:\n$$\n\\frac{d\\mathcal{L}}{d\\theta} = \\frac{\\partial g}{\\partial x(T)} \\frac{dx(T)}{d\\theta}\n$$\nThe term $\\frac{dx(T)}{d\\theta}$ represents the sensitivity of the final state to parameter changes. Its direct computation requires integrating a sensitivity equation for each parameter, which can be computationally expensive. The adjoint method provides a more efficient alternative, especially for a large number of parameters.\n\nThe adjoint method introduces an adjoint state vector, $a(t) \\in \\mathbb{R}^n$, which is the solution to the following terminal value problem:\n$$\n\\frac{da}{dt} = - \\left( \\frac{\\partial f_{\\theta}}{\\partial x} \\right)^\\top a(t) \\quad \\text{with} \\quad a(T) = \\left( \\frac{\\partial g}{\\partial x(T)} \\right)^\\top\n$$\nThe gradient of the loss function with respect to the parameters $\\theta$ is then given by the integral:\n$$\n\\frac{d\\mathcal{L}}{d\\theta} = \\int_0^T a(t)^\\top \\frac{\\partial f_{\\theta}(x(t), t)}{\\partial \\theta} dt\n$$\nThis formulation requires a single backward-in-time integration of the adjoint ODE,\nre-using the state trajectory $x(t)$ obtained from a forward-in-time integration of the original ODE.\n\n### Application to the Specific Neural ODE Model\n\nThe problem provides a specific form for the dynamics, parameters, and loss function.\n\n1.  **System Dynamics**: The state is $x(t) \\in \\mathbb{R}^2$. The dynamics function is $f_{\\theta}(x,t) = W_2 h + b_2 - k_d \\odot x$, where $h = \\tanh(z)$ and $z = W_1 x + b_1$. The parameters are $\\theta = \\{W_1, b_1, W_2, b_2, k_d\\}$.\n\n2.  **Loss Function and Adjoint Terminal Condition**: The loss is $\\mathcal{L}(\\theta) = \\frac{1}{2} \\| x(T) - x_{\\mathrm{target}} \\|_2^2$. The gradient of the loss with respect to the final state is $\\frac{\\partial \\mathcal{L}}{\\partial x(T)} = (x(T) - x_{\\mathrm{target}})^\\top$. Therefore, the terminal condition for the adjoint state $a(t) \\in \\mathbb{R}^2$ is:\n    $$\n    a(T) = x(T) - x_{\\mathrm{target}}\n    $$\n\n3.  **Jacobian of the Dynamics ($\\partial f_{\\theta} / \\partial x$)**: To define the adjoint ODE, we first need the Jacobian of $f_{\\theta}$ with respect to the state $x$. Using the chain rule:\n    $$\n    \\frac{\\partial f_{\\theta}}{\\partial x} = W_2 \\frac{\\partial h}{\\partial z} \\frac{\\partial z}{\\partial x} - \\frac{\\partial (k_d \\odot x)}{\\partial x}\n    $$\n    The derivatives are $\\frac{\\partial z}{\\partial x} = W_1$, $\\frac{\\partial(k_d \\odot x)}{\\partial x} = \\text{diag}(k_d)$, and $\\frac{\\partial h}{\\partial z} = \\text{diag}(1 - \\tanh^2(z)) = \\text{diag}(1 - h^2)$, where the square is elementwise. The full Jacobian is a $2 \\times 2$ matrix:\n    $$\n    J(x, \\theta) = \\frac{\\partial f_{\\theta}}{\\partial x} = W_2 \\text{diag}(1 - h^2) W_1 - \\text{diag}(k_d)\n    $$\n    The adjoint ODE is thus $\\frac{da}{dt} = -J(x(t), \\theta)^\\top a(t)$.\n\n4.  **Parameter Gradients ($\\partial f_{\\theta} / \\partial \\theta$)**: We need the partial derivatives of $f_{\\theta}$ with respect to each block of parameters.\n    -   **Gradient w.r.t. $W_2$**: $\\frac{\\partial f_{\\theta}}{\\partial (W_2)_{ij}} = e_i h_j^\\top$, where $e_i$ is the $i$-th standard basis vector. The integrand for the gradient of the matrix $W_2$ is $a h^\\top$.\n        $$ \\frac{d\\mathcal{L}}{dW_2} = \\int_0^T a(t) h(t)^\\top dt $$\n    -   **Gradient w.r.t. $b_2$**: $\\frac{\\partial f_{\\theta}}{\\partial b_2} = I$, the identity matrix. The integrand for the gradient of the vector $b_2$ is $a$.\n        $$ \\frac{d\\mathcal{L}}{db_2} = \\int_0^T a(t) dt $$\n    -   **Gradient w.r.t. $W_1$**: Using the chain rule, $\\frac{\\partial f_{\\theta}}{\\partial W_1} = W_2 \\frac{\\partial h}{\\partial z} \\frac{\\partial z}{\\partial W_1}$. The integrand for the gradient of the matrix $W_1$ is $\\text{diag}(1-h^2) (W_2^\\top a) x^\\top$.\n        $$ \\frac{d\\mathcal{L}}{dW_1} = \\int_0^T \\text{diag}(1 - h(t)^2) (W_2^\\top a(t)) x(t)^\\top dt $$\n    -   **Gradient w.r.t. $b_1$**: Similarly, the integrand for the vector $b_1$ is $\\text{diag}(1-h^2) (W_2^\\top a)$.\n        $$ \\frac{d\\mathcal{L}}{db_1} = \\int_0^T \\text{diag}(1 - h(t)^2) W_2^\\top a(t) dt $$\n    -   **Gradient w.r.t. $k_d$**: $\\frac{\\partial f_{\\theta}}{\\partial k_d} = -\\text{diag}(x)$. The integrand for the vector $k_d$ is $-(a \\odot x)$.\n        $$ \\frac{d\\mathcal{L}}{dk_d} = \\int_0^T -(a(t) \\odot x(t)) dt $$\n\n### Algorithmic Design and Numerical Implementation\n\nThe algorithm proceeds in three main stages: a forward pass to solve for the state trajectory, a backward pass to solve for the adjoint trajectory and accumulate the parameter gradients, and a verification step using finite differences.\n\n1.  **Forward Pass**: The state ODE $\\frac{dx}{dt} = f_{\\theta}(x,t)$ is integrated from $t=0$ to $t=T$ using the explicit $4$-th order Runge–Kutta (RK4) method with a fixed step size $\\Delta t = T/N$. The states $\\{x_0, x_1, \\dots, x_N\\}$ at time points $\\{t_0, t_1, \\dots, t_N\\}$ are stored for use in the backward pass.\n\n2.  **Backward Pass (Adjoint and Gradients)**: The adjoint ODE and parameter gradient integrals are solved simultaneously.\n    -   The adjoint ODE is integrated backward from $t=T$ to $t=0$. The problem specifies a \"first-order explicit method\". This is implemented as an explicit Euler method for the time-reversed system. Given $a_i$ at time $t_i$, the state at the previous time step $t_{i-1}$ is approximated as:\n        $$ a_{i-1} = a_i - \\Delta t \\left( \\frac{da}{dt} \\right)\\bigg|_{t_i} = a_i - \\Delta t \\left( -J(x_i, \\theta)^\\top a_i \\right) = a_i + \\Delta t J(x_i, \\theta)^\\top a_i $$\n    -   The parameter gradient integrals are approximated using a simple quadrature rule, consistent with the backward Euler integration. A right-hand Riemann sum is used, accumulating the gradient contribution at each time step $t_i$ from $i=N$ down to $i=1$:\n        $$ \\frac{d\\mathcal{L}}{d\\theta} \\approx \\sum_{i=1}^N \\left( a_i^\\top \\frac{\\partial f_{\\theta}}{\\partial \\theta}\\bigg|_{x_i} \\right) \\Delta t $$\n    -   The process starts with $a_N = x_N - x_{\\text{target}}$ and iterates backward, updating the adjoint state and adding to the total gradient at each step.\n\n3.  **Verification via Finite Differences**: The computed adjoint gradient is compared against a central finite-difference approximation. For each scalar parameter $\\theta_j$, its gradient component is approximated by:\n    $$\n    \\frac{d\\mathcal{L}}{d\\theta_j} \\approx \\frac{\\mathcal{L}(\\theta + \\epsilon e_j) - \\mathcal{L}(\\theta - \\epsilon e_j)}{2\\epsilon}\n    $$\n    where $e_j$ is a standard basis vector and $\\epsilon$ is a small perturbation. This requires re-integrating the forward ODE twice for each parameter. The maximum absolute difference between the adjoint and finite-difference gradient vectors is then compared against a given tolerance $\\tau$. This comparison assesses the accuracy of the continuous adjoint method under a specific numerical discretization. Discrepancies are expected due to the \"optimize-then-discretize\" nature of the continuous adjoint method versus the \"discretize-then-optimize\" nature implicitly defined by the finite-difference check on the RK4 solver. These discrepancies are expected to decrease as the integration step size $\\Delta t$ approaches $0$. The test cases with varying $N$ and nonlinearity scaling factor $s$ are designed to explore this behavior.",
            "answer": "```python\nimport numpy as np\n\nclass ModelParams:\n    \"\"\"\n    A helper class to manage model parameters, including packing to a flat vector\n    and unpacking from it, and applying scaling.\n    \"\"\"\n    def __init__(self, W1, b1, W2, b2, kd):\n        self.W1 = np.array(W1, dtype=np.float64)\n        self.b1 = np.array(b1, dtype=np.float64)\n        self.W2 = np.array(W2, dtype=np.float64)\n        self.b2 = np.array(b2, dtype=np.float64)\n        self.kd = np.array(kd, dtype=np.float64)\n        \n        self.shapes = [self.W1.shape, self.b1.shape, self.W2.shape, self.b2.shape, self.kd.shape]\n        self.sizes = [p.size for p in [self.W1, self.b1, self.W2, self.b2, self.kd]]\n        self.total_size = sum(self.sizes)\n\n    def pack(self):\n        \"\"\"Packs all parameters into a single flat numpy array.\"\"\"\n        return np.concatenate([p.flatten() for p in [self.W1, self.b1, self.W2, self.b2, self.kd]])\n\n    @classmethod\n    def from_flat(cls, theta_flat):\n        \"\"\"Creates a ModelParams object from a flat numpy array.\"\"\"\n        theta_flat = np.array(theta_flat, dtype=np.float64)\n        # Fixed shapes from the problem description\n        shapes = [(3, 2), (3,), (2, 3), (2,), (2,)]\n        sizes = [np.prod(s) for s in shapes]\n        \n        ptr = 0\n        unpacked_params = []\n        for i, shape in enumerate(shapes):\n            size = sizes[i]\n            param_flat = theta_flat[ptr:ptr+size]\n            unpacked_params.append(param_flat.reshape(shape))\n            ptr += size\n        \n        return cls(*unpacked_params)\n\n    def scale(self, s):\n        \"\"\"Applies scaling factor s to network weights and biases.\"\"\"\n        return ModelParams(self.W1 * s, self.b1 * s, self.W2 * s, self.b2 * s, self.kd)\n\ndef ode_func(x, params: ModelParams):\n    \"\"\"The ODE function dx/dt = f(x, t, theta).\"\"\"\n    z = params.W1 @ x + params.b1\n    h = np.tanh(z)\n    return params.W2 @ h + params.b2 - params.kd * x\n\ndef run_rk4(x0, T, N, params: ModelParams):\n    \"\"\"Integrates the ODE using a 4th-order Runge-Kutta method.\"\"\"\n    dt = T / N\n    x_traj = np.zeros((N + 1, x0.shape[0]), dtype=np.float64)\n    x_traj[0] = x0\n    x = x0.copy()\n    \n    for i in range(N):\n        k1 = ode_func(x, params)\n        k2 = ode_func(x + 0.5 * dt * k1, params)\n        k3 = ode_func(x + 0.5 * dt * k2, params)\n        k4 = ode_func(x + dt * k3, params)\n        x += (dt / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)\n        x_traj[i+1] = x\n        \n    return x_traj\n\ndef compute_loss(x_final, xtarget):\n    \"\"\"Computes the loss function L.\"\"\"\n    return 0.5 * np.sum((x_final - xtarget)**2)\n\ndef compute_dfdx(x, params: ModelParams):\n    \"\"\"Computes the Jacobian of the ODE function w.r.t. x.\"\"\"\n    z = params.W1 @ x + params.b1\n    h = np.tanh(z)\n    diag_1_minus_h2 = np.diag(1 - h**2)\n    return params.W2 @ diag_1_minus_h2 @ params.W1 - np.diag(params.kd)\n\ndef compute_adjoint_gradient(x0, T, N, params: ModelParams, xtarget):\n    \"\"\"Computes the gradient dL/dtheta using the adjoint method.\"\"\"\n    dt = T / N\n    x_traj = run_rk4(x0, T, N, params)\n    \n    grad_W1 = np.zeros_like(params.W1)\n    grad_b1 = np.zeros_like(params.b1)\n    grad_W2 = np.zeros_like(params.W2)\n    grad_b2 = np.zeros_like(params.b2)\n    grad_kd = np.zeros_like(params.kd)\n    \n    a = x_traj[N] - xtarget\n    \n    for i in range(N, 0, -1):\n        x = x_traj[i]\n        \n        z = params.W1 @ x + params.b1\n        h = np.tanh(z)\n        \n        # Accumulate gradient contributions from time t_i\n        grad_b2 += a * dt\n        grad_W2 += np.outer(a, h) * dt\n        grad_kd += -(a * x) * dt\n        \n        v = (1 - h**2) * (params.W2.T @ a)\n        grad_b1 += v * dt\n        grad_W1 += np.outer(v, x) * dt\n        \n        # Update adjoint state from t_i to t_{i-1}\n        jacobian = compute_dfdx(x, params)\n        a = a + dt * (jacobian.T @ a)\n        \n    packed_grads = np.concatenate([\n        grad_W1.flatten(), grad_b1.flatten(), grad_W2.flatten(),\n        grad_b2.flatten(), grad_kd.flatten()\n    ])\n    return packed_grads\n\ndef compute_fd_gradient(x0, T, N, base_params: ModelParams, xtarget, epsilon):\n    \"\"\"Computes the gradient dL/dtheta using central finite differences.\"\"\"\n    base_theta = base_params.pack()\n    grad = np.zeros_like(base_theta)\n    \n    def loss_func(theta_flat):\n        params_pert = ModelParams.from_flat(theta_flat)\n        x_traj = run_rk4(x0, T, N, params_pert)\n        return compute_loss(x_traj[-1], xtarget)\n\n    for i in range(len(base_theta)):\n        theta_plus = base_theta.copy()\n        theta_plus[i] += epsilon\n        \n        theta_minus = base_theta.copy()\n        theta_minus[i] -= epsilon\n        \n        loss_plus = loss_func(theta_plus)\n        loss_minus = loss_func(theta_minus)\n        \n        grad[i] = (loss_plus - loss_minus) / (2 * epsilon)\n        \n    return grad\n\ndef solve():\n    \"\"\"Main function to run test cases and produce the final output.\"\"\"\n    W1_base = np.array([[0.8, -0.5], [0.3, 0.9], [-0.7, 0.2]])\n    b1_base = np.array([0.1, -0.2, 0.05])\n    W2_base = np.array([[0.5, -0.3, 0.1], [-0.4, 0.6, -0.2]])\n    b2_base = np.array([0.0, 0.05])\n    kd_base = np.array([0.3, 0.5])\n    base_params_obj = ModelParams(W1_base, b1_base, W2_base, b2_base, kd_base)\n\n    x0 = np.array([0.5, 0.2])\n    xtarget = np.array([0.1, -0.1])\n    T = 2.0\n\n    test_cases = [\n        (500, 1e-6, 2e-2, 1.0),\n        (50, 1e-6, 2e-2, 1.0),\n        (20, 1e-6, 5e-2, 1.0),\n        (60, 1e-6, 1e-2, 0.05),\n    ]\n\n    results = []\n    \n    for N, epsilon, tau, s in test_cases:\n        scaled_params = base_params_obj.scale(s)\n        \n        grad_adj = compute_adjoint_gradient(x0, T, N, scaled_params, xtarget)\n        grad_fd = compute_fd_gradient(x0, T, N, scaled_params, xtarget, epsilon)\n\n        max_abs_diff = np.max(np.abs(grad_adj - grad_fd))\n        \n        results.append(max_abs_diff  tau)\n        \n    bool_str_results = [str(b) for b in results]\n    print(f\"[{','.join(bool_str_results)}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "We now advance to a classic and powerful application of adjoint methods: solving a PDE-constrained inverse problem. This exercise tasks you with recovering an unknown conductivity field in an electrical impedance tomography setting, a problem central to both geophysics and medical imaging. The challenge is twofold: first, deriving the discrete adjoint system corresponding to the discretized PDE; and second, computing the gradient for a smoothed Total Variation (TV) regularization term, which is essential for recovering sharp, physically realistic features in the conductivity field. This practice integrates system discretization, adjoint derivation for both the state equation and the regularizer, and the use of the resulting gradient in an iterative optimization scheme. ",
            "id": "3364106",
            "problem": "Consider the following discrete inverse problem for recovering an unknown scalar conductivity field over a two-dimensional unit square. Let the physical domain be the unit square with zero Dirichlet boundary condition on all sides, and consider the elliptic partial differential equation\n$$\n-\\nabla\\cdot\\left(\\sigma(x,y)\\,\\nabla u(x,y)\\right) = s(x,y) \\quad \\text{in the interior}, \\quad u(x,y)=0 \\quad \\text{on the boundary},\n$$\nwhere $u$ is the state, $\\sigma$ is the positive conductivity to be inferred, and $s$ is a known source. The observation operator selects a fixed set of interior grid-point values of $u$.\n\nYou are required to work in a dimensionless setting and discretize the problem on a uniform Cartesian grid with $N_x=N_y=32$ cell centers, mesh size $h=1/(N_x)$ in both directions, and enforce zero Dirichlet boundary conditions using a symmetric two-point flux approximation with harmonic face averaging for interior faces. Let the unknown discrete conductivity $\\sigma\\in\\mathbb{R}^{N_xN_y}$ be cell-centered and strictly positive. The discrete forward operator $A(\\sigma)$ is the sparse stiffness matrix resulting from this flux-consistent discretization, and the discrete forward state $u(\\sigma)$ solves\n$$\nA(\\sigma)\\,u = b,\n$$\nwhere $b$ is the discrete load vector representing the source.\n\nDefine the data-misfit term using a selection operator $P$ that extracts interior values of $u$ at $16$ evenly spaced interior grid indices (avoid the outermost cells to remain inside the domain). Define the objective functional\n$$\nJ(\\sigma) = \\frac{1}{2}\\,\\|P\\,u(\\sigma) - d\\|_2^2 + \\alpha \\,\\mathrm{TV}_\\varepsilon(\\sigma),\n$$\nwhere $\\alpha0$ is a regularization weight, $d$ are noiseless synthetic data generated from a known $\\sigma_{\\mathrm{true}}$ using the same forward model, and $\\mathrm{TV}_\\varepsilon$ is a smoothed isotropic total variation defined on the grid by forward finite differences in each coordinate direction and an isotropic Euclidean norm with smoothing parameter $\\varepsilon0$:\n$$\n\\mathrm{TV}_\\varepsilon(\\sigma) = \\sum_{i,j} \\sqrt{\\left(D_x \\sigma\\right)_{i,j}^2 + \\left(D_y \\sigma\\right)_{i,j}^2 + \\varepsilon^2}.\n$$\nHere, $(D_x \\sigma)_{i,j}$ and $(D_y \\sigma)_{i,j}$ denote forward differences in the $x$ and $y$ directions, respectively, with homogeneous Neumann extension for differences crossing the boundary. The source $s(x,y)$ is a normalized Gaussian centered at $(x_c,y_c)=(0.3,0.7)$ with standard deviation $0.07$, and its discrete counterpart $b$ is obtained by pointwise evaluation at cell centers.\n\nTasks:\n- Starting from the Lagrangian with the partial differential equation constraint and the standard first-order optimality principles, derive the continuous adjoint equation and the continuous expression for the gradient of the data-misfit term with respect to $\\sigma$, expressed in terms of the state $u$ and the adjoint state $\\lambda$. Then, derive a consistent discrete adjoint system and the corresponding discrete gradient for the data-misfit part under the specified flux-consistent discretization with harmonic averaging at faces.\n- Starting from the definition of $\\mathrm{TV}_\\varepsilon(\\sigma)$, derive the discrete gradient of the smoothed isotropic total variation using the discrete divergence as the negative adjoint of the forward-difference gradient under homogeneous Neumann conditions.\n- Implement a solver that:\n  1. Assembles $A(\\sigma)$ for any admissible $\\sigma$ and solves the forward problem.\n  2. Constructs the discrete adjoint right-hand side using the observation operator and solves the adjoint problem.\n  3. Computes $J(\\sigma)$ and its gradient $\\nabla J(\\sigma)$ using the derived formulas.\n  4. Performs a central finite-difference directional-derivative test for the full objective (data-misfit plus smoothed total variation) at an initial guess $\\sigma_0$ against a random direction $\\delta\\sigma$, using a step size $h_{\\mathrm{fd}}=10^{-4}$ scaled so that $\\sigma_0 \\pm h_{\\mathrm{fd}}\\,\\delta\\sigma$ remain strictly positive. Report the relative gradient error defined by\n     $$\n     E_{\\mathrm{rel}} = \\frac{\\left|\\langle \\nabla J(\\sigma_0), \\delta\\sigma\\rangle - \\frac{J(\\sigma_0+h_{\\mathrm{fd}}\\delta\\sigma)-J(\\sigma_0-h_{\\mathrm{fd}}\\delta\\sigma)}{2h_{\\mathrm{fd}}}\\right|}{\\max\\left\\{1, \\left|\\frac{J(\\sigma_0+h_{\\mathrm{fd}}\\delta\\sigma)-J(\\sigma_0-h_{\\mathrm{fd}}\\delta\\sigma)}{2h_{\\mathrm{fd}}}\\right|, \\left|\\langle \\nabla J(\\sigma_0), \\delta\\sigma\\rangle\\right|\\right\\}}.\n     $$\n  5. Runs a fixed-budget gradient-descent inversion with Armijo backtracking line search for $10$ iterations to minimize $J(\\sigma)$, enforcing positivity of $\\sigma$ by projection onto $[\\sigma_{\\min},\\infty)$ with $\\sigma_{\\min}=0.2$ after each update. Use an initial step size of $10^{-1}$, Armijo parameter $c=10^{-4}$, and backtracking factor $\\beta=1/2$.\n  6. Computes an edge-sharpness metric of the recovered $\\sigma$ after the $10$ iterations by averaging the magnitude of the discrete gradient $\\sqrt{\\left(D_x \\sigma\\right)_{i,j}^2 + \\left(D_y \\sigma\\right)_{i,j}^2}$ over a two-cell-wide band around the boundary of a circular inclusion in the truth model, where the truth is $\\sigma_{\\mathrm{true}}=2$ inside a disk centered at $(0.5,0.5)$ of radius $0.25$ and $\\sigma_{\\mathrm{true}}=1$ outside. The band is defined by $\\{(x_i,y_j): | \\sqrt{(x_i-0.5)^2+(y_j-0.5)^2} - 0.25| \\le 2h\\}$.\n\nTest suite and required outputs:\n- Use $\\alpha=10^{-3}$, the initial guess $\\sigma_0 \\equiv 1$, and noiseless data $d$ generated by solving the forward problem with $\\sigma_{\\mathrm{true}}$ defined above.\n- Evaluate the following four cases:\n  1. Gradient check relative error $E_{\\mathrm{rel}}$ with $\\varepsilon=10^{-1}$.\n  2. Gradient check relative error $E_{\\mathrm{rel}}$ with $\\varepsilon=10^{-3}$.\n  3. Edge-sharpness metric after $10$ iterations of inversion with $\\varepsilon=10^{-1}$.\n  4. Edge-sharpness metric after $10$ iterations of inversion with $\\varepsilon=10^{-3}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order listed above, for example: $[r_1,r_2,r_3,r_4]$, where each $r_k$ is a floating-point number.\n\nAll quantities are dimensionless and should be treated as such. Angles, if any, are not used. Percentages are not used. The final output must strictly follow the specified single-line format.",
            "solution": "The user wants to solve a discrete inverse problem.\n\n**1. Problem Validation**\n\n*   **Step 1: Extract Givens**\n    *   **PDE:** $-\\nabla\\cdot\\left(\\sigma(x,y)\\,\\nabla u(x,y)\\right) = s(x,y)$ on the unit square.\n    *   **Boundary Conditions:** $u(x,y)=0$ on all boundaries.\n    *   **Discretization:** Uniform Cartesian grid, $N_x=N_y=32$ cell centers. Mesh size $h=1/N_x$. Symmetric two-point flux approximation with harmonic face averaging for interior faces.\n    *   **Unknown:** Cell-centered conductivity $\\sigma\\in\\mathbb{R}^{N_xN_y}$, $\\sigma0$.\n    *   **Forward Model:** $A(\\sigma)\\,u = b$, where $A(\\sigma)$ is the sparse stiffness matrix and $b$ is the discrete load vector.\n    *   **Objective Functional:** $J(\\sigma) = \\frac{1}{2}\\,\\|P\\,u(\\sigma) - d\\|_2^2 + \\alpha \\,\\mathrm{TV}_\\varepsilon(\\sigma)$.\n    *   **Observation Operator $P$:** Selects values of $u$ at $16$ evenly spaced interior grid indices.\n    *   **Data $d$:** Noiseless synthetic data, $d = P u(\\sigma_{\\mathrm{true}})$.\n    *   **Regularization:** Smoothed isotropic total variation, $\\mathrm{TV}_\\varepsilon(\\sigma) = \\sum_{i,j} \\sqrt{\\left(D_x \\sigma\\right)_{i,j}^2 + \\left(D_y \\sigma\\right)_{i,j}^2 + \\varepsilon^2}$.\n    *   **Difference Operators:** $D_x, D_y$ are forward finite differences with homogeneous Neumann extension.\n    *   **Source Term $s(x,y)$:** Normalized Gaussian centered at $(x_c,y_c)=(0.3,0.7)$ with standard deviation $0.07$. Discrete version $b$ by pointwise evaluation at cell centers.\n    *   **True Conductivity $\\sigma_{\\mathrm{true}}$:** Value of $2$ inside a disk centered at $(0.5,0.5)$ with radius $0.25$, and $1$ outside.\n    *   **Parameters:** $\\alpha=10^{-3}$, initial guess $\\sigma_0 \\equiv 1$, $\\sigma_{\\min}=0.2$.\n    *   **Gradient Check:** Central finite-difference test with step size $h_{\\mathrm{fd}}=10^{-4}$.\n    *   **Inversion Algorithm:** Gradient descent for $10$ iterations with Armijo backtracking line search (initial step $10^{-1}$, $c=10^{-4}$, $\\beta=1/2$).\n    *   **Output Metric:** Edge-sharpness metric based on average gradient magnitude in a band around the true discontinuity.\n    *   **Test Cases:** Four values to compute based on $\\varepsilon=10^{-1}$ and $\\varepsilon=10^{-3}$: two gradient check errors and two sharpness metrics.\n\n*   **Step 2: Validate Using Extracted Givens**\n    *   The problem is **scientifically grounded**, based on standard elliptic PDE theory and Tikhonov regularization for inverse problems.\n    *   The problem is **well-posed** in the sense that the forward model is a standard solvable BVP, and the optimization problem is structured for numerical solution, which is standard practice for ill-posed inverse problems.\n    *   The problem is **objective** and described with precise mathematical formulations.\n    *   There is a minor ambiguity in how the symmetric discretization matrix $A(\\sigma)$ is constructed at the boundaries. The phrase \"for interior faces\" implies a different treatment for boundary faces. A standard finite-volume method that preserves matrix symmetry for Dirichlet boundaries involves specific transmissibility coefficients for faces adjacent to the boundary. This interpretation is consistent with the problem's overall structure and is assumed for the solution. The discretization of the source term $b$ is also taken to be $b_i = s(x_i, y_i) h^2$ as per standard finite volume practice. These are reasonable resolutions of minor ambiguities.\n    *   The problem does not violate any of the invalidity criteria. It is a well-defined, non-trivial, and standard problem in computational science and engineering.\n\n*   **Step 3: Verdict and Action**\n    *   The problem is deemed **valid**. A solution will be provided.\n\n**2. Theoretical Derivations**\n\n**2.1. Continuous Adjoint-State Method**\n\nLet the problem domain be $\\Omega=(0,1)^2$. The objective functional is $J(\\sigma) = \\frac{1}{2} \\int_\\Omega (P u - d)^2 d\\mathbf{x} + \\alpha \\mathrm{TV}_\\varepsilon(\\sigma)$, where $P$ is an observation operator. The state $u$ is constrained by the PDE $-\\nabla\\cdot(\\sigma\\nabla u) = s$ in $\\Omega$ and $u=0$ on $\\partial\\Omega$.\n\nWe form the Lagrangian $\\mathcal{L}$ by augmenting the objective functional with the PDE constraint using an adjoint state (or Lagrange multiplier) $\\lambda$:\n$$\n\\mathcal{L}(u, \\sigma, \\lambda) = J(u, \\sigma) - \\int_\\Omega \\lambda \\left[ \\nabla\\cdot(\\sigma\\nabla u) + s \\right] d\\mathbf{x}\n$$\nThe first-order optimality conditions require the Fréchet derivatives of $\\mathcal{L}$ with respect to each variable to be zero. We focus on the data-misfit part first. Let $J_m(u) = \\frac{1}{2}\\int_\\Omega (Pu-d)^2 d\\mathbf{x}$.\n\nThe derivative with respect to the state $u$ in direction $\\delta u$ (where $\\delta u = 0$ on $\\partial\\Omega$) is:\n$$\nD_u\\mathcal{L}[\\delta u] = \\int_\\Omega (Pu-d) P\\delta u \\,d\\mathbf{x} - \\int_\\Omega \\lambda \\nabla\\cdot(\\sigma\\nabla \\delta u) \\,d\\mathbf{x} = 0\n$$\nUsing Green's first identity twice on the second term and choosing the adjoint boundary condition $\\lambda=0$ on $\\partial\\Omega$:\n$$\n\\int_\\Omega \\lambda \\nabla\\cdot(\\sigma\\nabla \\delta u) \\,d\\mathbf{x} = \\int_\\Omega \\delta u \\nabla\\cdot(\\sigma\\nabla \\lambda) \\,d\\mathbf{x}\n$$\nThus, the condition becomes:\n$$\n\\int_\\Omega \\left[ P^T(Pu-d) - \\nabla\\cdot(\\sigma\\nabla \\lambda) \\right] \\delta u \\,d\\mathbf{x} = 0\n$$\nSince this must hold for all admissible $\\delta u$, we obtain the **adjoint equation**:\n$$\n-\\nabla\\cdot(\\sigma\\nabla \\lambda) = P^T(Pu-d) \\quad \\text{in } \\Omega, \\quad \\lambda = 0 \\quad \\text{on } \\partial\\Omega\n$$\nHere, $P^T$ is the adjoint of the observation operator. If $P$ extracts point values, $P^T$ injects weighted Dirac delta functions at those points.\n\nNext, we take the derivative with respect to the control variable $\\sigma$ in direction $\\delta\\sigma$. The derivative of the data-misfit contribution is:\n$$\nD_\\sigma\\mathcal{L}[\\delta\\sigma] = - \\int_\\Omega \\lambda \\nabla\\cdot(\\delta\\sigma\\nabla u) \\,d\\mathbf{x}\n$$\nUsing Green's first identity:\n$$\nD_\\sigma\\mathcal{L}[\\delta\\sigma] = \\int_\\Omega \\nabla\\lambda \\cdot (\\delta\\sigma \\nabla u) \\,d\\mathbf{x} = \\int_\\Omega (\\nabla u \\cdot \\nabla \\lambda) \\delta\\sigma \\,d\\mathbf{x}\n$$\nFrom this, we identify the gradient of the data-misfit term with respect to $\\sigma$:\n$$\n\\nabla_\\sigma J_m(\\sigma) = \\nabla u \\cdot \\nabla \\lambda\n$$\n\n**2.2. Discrete Adjoint and Gradient of Data Misfit**\n\nLet the discrete state $u \\in\\mathbb{R}^N$ (where $N=N_x N_y$) solve the linear system $A(\\sigma)u=b$. The discrete objective is $J(\\sigma) = \\frac{1}{2}\\|Pu-d\\|_2^2 + \\alpha \\mathrm{TV}_\\varepsilon(\\sigma)$. The discrete Lagrangian for the data-misfit term is:\n$$\n\\mathcal{L}_d(u, \\sigma, \\lambda) = \\frac{1}{2}(Pu-d)^T(Pu-d) + \\lambda^T(A(\\sigma)u-b)\n$$\nSetting the gradient with respect to $u$ to zero gives the discrete adjoint equation:\n$$\n\\frac{\\partial \\mathcal{L}_d}{\\partial u} = P^T(Pu-d) + A(\\sigma)^T\\lambda = 0 \\implies A(\\sigma)^T\\lambda = -P^T(Pu-d)\n$$\nThe discretization chosen (finite volume with harmonic averaging and specific boundary treatment) yields a symmetric stiffness matrix $A(\\sigma)$, so $A(\\sigma)^T=A(\\sigma)$. The adjoint state $\\lambda$ is found by solving:\n$$\nA(\\sigma)\\lambda = -P^T(Pu-d)\n$$\nThe gradient of $\\mathcal{L}_d$ with respect to a single conductivity parameter $\\sigma_k$ is:\n$$\n\\frac{\\partial \\mathcal{L}_d}{\\partial \\sigma_k} = \\lambda^T \\frac{\\partial A(\\sigma)}{\\partial \\sigma_k} u\n$$\nLet's make this more explicit. The energy associated with the operator is $\\frac{1}{2}u^T A(\\sigma) u$. For our discretization, this can be expressed as a sum over faces:\n$$\n\\frac{1}{2} u^T A(\\sigma) u = \\frac{1}{2} \\sum_{\\langle k,m \\rangle} T_{km}(\\sigma_k, \\sigma_m)(u_k-u_m)^2 + \\frac{1}{2} \\sum_{k \\in \\partial\\Omega_d} \\sum_{f \\in \\partial B_k} T_{kf}(\\sigma_k) u_k^2\n$$\nwhere $\\langle k,m \\rangle$ denotes an interior face between cells $k$ and $m$, $\\partial\\Omega_d$ is the set of discrete boundary cells, and $\\partial B_k$ are the boundary faces of cell $k$. The transmissibilities are $T_{km} = \\frac{2\\sigma_k\\sigma_m}{\\sigma_k+\\sigma_m}$ for interior faces and $T_{kf}=2\\sigma_k$ for boundary faces.\nThe term $\\lambda^T A(\\sigma) u$ has a similar structure:\n$$\n\\lambda^T A(\\sigma) u = \\sum_{\\langle k,m \\rangle} T_{km}(\\lambda_k-\\lambda_m)(u_k-u_m) + \\sum_{k \\in \\partial\\Omega_d} \\sum_{f \\in \\partial B_k} T_{kf} \\lambda_k u_k\n$$\nDifferentiating with respect to $\\sigma_k$ yields the $k$-th component of the gradient:\n$$\n(\\nabla_\\sigma J_m)_k = \\sum_{m \\sim k} \\frac{\\partial T_{km}}{\\partial \\sigma_k} (\\lambda_k-\\lambda_m)(u_k-u_m) + \\sum_{f \\in \\partial B_k} \\frac{\\partial T_{kf}}{\\partial \\sigma_k} \\lambda_k u_k\n$$\nWith $\\frac{\\partial T_{km}}{\\partial \\sigma_k} = \\frac{2\\sigma_m^2}{(\\sigma_k+\\sigma_m)^2}$ and $\\frac{\\partial T_{kf}}{\\partial \\sigma_k} = 2$, this becomes:\n$$\n(\\nabla_\\sigma J_m)_k = \\sum_{m \\sim k} \\frac{2\\sigma_m^2}{(\\sigma_k+\\sigma_m)^2} (\\lambda_k-\\lambda_m)(u_k-u_m) + \\sum_{f \\in \\partial B_k} 2\\lambda_k u_k\n$$\nwhere the first sum is over interior neighbors of cell $k$ and the second is over its boundary faces.\n\n**2.3. Discrete Gradient of Smoothed Total Variation**\n\nThe smoothed TV term is $R(\\sigma) = \\mathrm{TV}_\\varepsilon(\\sigma) = \\sum_{i,j} \\sqrt{(D_x\\sigma)_{ij}^2 + (D_y\\sigma)_{ij}^2 + \\varepsilon^2}$. Let $G\\sigma = \\begin{pmatrix} D_x \\sigma \\\\ D_y \\sigma \\end{pmatrix}$. Let $w_{ij}(\\sigma) = \\sqrt{(D_x\\sigma)_{ij}^2 + (D_y\\sigma)_{ij}^2 + \\varepsilon^2}$.\nThe gradient of $R$ with respect to $\\sigma_k$ is found via the chain rule:\n$$\n\\frac{\\partial R}{\\partial \\sigma_k} = \\sum_{i,j} \\frac{\\partial w_{ij}}{\\partial \\sigma_k} = \\sum_{i,j} \\frac{1}{w_{ij}} \\left( (D_x\\sigma)_{ij} \\frac{\\partial(D_x\\sigma)_{ij}}{\\partial \\sigma_k} + (D_y\\sigma)_{ij} \\frac{\\partial(D_y\\sigma)_{ij}}{\\partial \\sigma_k} \\right)\n$$\nThis expression can be written compactly using the formal adjoint (transpose) of the difference operators. Let $v_x = (D_x\\sigma)./w$ and $v_y = (D_y\\sigma)./w$ be vector fields. The gradient is then:\n$$\n\\nabla R(\\sigma) = -(D_x^T v_x + D_y^T v_y) = -\\mathrm{div}(v)\n$$\nThe forward difference operators $D_x, D_y$ are defined with homogeneous Neumann extension, meaning $(D_x\\sigma)_{N_x-1,j}=0$ and $(D_y\\sigma)_{i,N_y-1}=0$. The adjoints $D_x^T, D_y^T$ are backward difference operators whose structure at the boundaries is determined by the discrete integration-by-parts identity $\\langle D\\sigma, v \\rangle = \\langle \\sigma, D^T v \\rangle$. For the specified $D_x$, the component $(D_x^T v_x)_{i,j}$ is:\n\\begin{itemize}\n    \\item $(v_x)_{i-1,j} - (v_x)_{i,j}$ for $i \\in \\{1,\\dots,N_x-2\\}$\n    \\item $- (v_x)_{0,j}$ for $i=0$\n    \\item $(v_x)_{N_x-2,j}$ for $i=N_x-1$\n\\end{itemize}\nAn analogous form applies to $D_y^T$.\n\n**3. Implementation Plan**\nThe solution will be implemented in Python using `numpy` and `scipy`.\n1.  **Grid and Model Setup:** A class `InverseProblem` will encapsulate the grid, physical parameters, and discretization details.\n2.  **Matrix Assembly:** A method `assemble_A(sigma)` will construct the sparse stiffness matrix $A$ based on the derived finite-volume scheme.\n3.  **Solvers:** `scipy.sparse.linalg.spsolve` will be used for both the forward ($A u = b$) and adjoint ($A \\lambda = g$) systems.\n4.  **Objective and Gradient:** Methods `compute_objective(sigma)` and `compute_gradient(sigma)` will implement the full objective functional $J(\\sigma)$ and its analytic gradient $\\nabla J(\\sigma) = \\nabla J_m + \\alpha \\nabla R$.\n5.  **Gradient Check:** A function will compare the analytic gradient's projection onto a random direction with a central finite-difference approximation.\n6.  **Optimization:** A fixed-iteration gradient descent loop will be implemented, including an Armijo backtracking line search and positivity projection.\n7.  **Post-processing:** The edge-sharpness metric will be calculated on the final inverted conductivity field.\n8.  **Main script:** The `solve` function will orchestrate these steps for the two specified values of $\\varepsilon$ and print the four required results.",
            "answer": "```python\nimport numpy as np\nfrom scipy.sparse import lil_matrix, csc_matrix\nfrom scipy.sparse.linalg import spsolve\n\nclass InverseProblem:\n    def __init__(self, Nx=32, Ny=32, alpha=1e-3, epsilon=1e-1):\n        self.Nx = Nx\n        self.Ny = Ny\n        self.N = Nx * Ny\n        self.h = 1.0 / Nx\n\n        self.alpha = alpha\n        self.epsilon = epsilon\n        \n        self.x = np.linspace(0.5 * self.h, 1.0 - 0.5 * self.h, Nx)\n        self.y = np.linspace(0.5 * self.h, 1.0 - 0.5 * self.h, Ny)\n        self.xx, self.yy = np.meshgrid(self.x, self.y)\n        \n        # Source term\n        s = np.exp(-((self.xx - 0.3)**2 + (self.yy - 0.7)**2) / (2 * 0.07**2))\n        s /= np.sum(s) * (self.h**2) # Normalize\n        self.b = (self.h**2 * s).flatten()\n\n        # True conductivity\n        radius_sq = 0.25**2\n        self.sigma_true = np.ones((Ny, Nx))\n        self.sigma_true[(self.xx - 0.5)**2 + (self.yy - 0.5)**2  radius_sq] = 2.0\n        self.sigma_true = self.sigma_true.flatten()\n\n        # Observation operator P\n        obs_indices_1d = np.round(np.linspace(4, self.Nx - 5, 4)).astype(int)\n        self.obs_indices = []\n        for i in obs_indices_1d:\n            for j in obs_indices_1d:\n                self.obs_indices.append(j * self.Nx + i)\n        \n        self.P = lil_matrix((len(self.obs_indices), self.N))\n        for i, idx in enumerate(self.obs_indices):\n            self.P[i, idx] = 1.0\n        self.P = self.P.tocsc()\n        \n        # Generate synthetic data\n        u_true = self.solve_forward(self.sigma_true)\n        self.d_obs = self.P @ u_true\n\n    def _to_1d(self, i, j):\n        return j * self.Nx + i\n\n    def assemble_A(self, sigma):\n        A = lil_matrix((self.N, self.N))\n        sigma_2d = sigma.reshape((self.Ny, self.Nx))\n\n        for j in range(self.Ny):\n            for i in range(self.Nx):\n                k = self._to_1d(i, j)\n                diag_val = 0.0\n                \n                # East neighbor\n                if i  self.Nx - 1:\n                    m = self._to_1d(i + 1, j)\n                    T_em = 2 * sigma_2d[j, i] * sigma_2d[j, i + 1] / (sigma_2d[j, i] + sigma_2d[j, i + 1])\n                    A[k, m] = -T_em\n                    A[m, k] = -T_em\n                    diag_val += T_em\n                else: # Boundary\n                    diag_val += 2 * sigma_2d[j, i]\n\n                # West neighbor is handled by symmetry\n                if i == 0: # Boundary\n                    diag_val += 2 * sigma_2d[j, i]\n\n                # North neighbor\n                if j  self.Ny - 1:\n                    m = self._to_1d(i, j + 1)\n                    T_nm = 2 * sigma_2d[j, i] * sigma_2d[j + 1, i] / (sigma_2d[j, i] + sigma_2d[j + 1, i])\n                    A[k, m] = -T_nm\n                    A[m, k] = -T_nm\n                    diag_val += T_nm\n                else: # Boundary\n                    diag_val += 2 * sigma_2d[j, i]\n\n                # South neighbor is handled by symmetry\n                if j == 0: # Boundary\n                    diag_val += 2 * sigma_2d[j, i]\n\n                A[k, k] = diag_val\n        return A.tocsc()\n\n    def solve_forward(self, sigma):\n        A = self.assemble_A(sigma)\n        return spsolve(A, self.b)\n\n    def solve_adjoint(self, sigma, u):\n        A = self.assemble_A(sigma)\n        residual = self.P @ u - self.d_obs\n        adjoint_rhs = -self.P.T @ residual\n        return spsolve(A, adjoint_rhs)\n\n    def compute_objective(self, sigma):\n        # Data misfit term\n        u = self.solve_forward(sigma)\n        residual = self.P @ u - self.d_obs\n        J_misfit = 0.5 * np.dot(residual, residual)\n        \n        # Regularization term\n        sigma_2d = sigma.reshape((self.Ny, self.Nx))\n        Dx_sigma = np.zeros_like(sigma_2d)\n        Dy_sigma = np.zeros_like(sigma_2d)\n        Dx_sigma[:, :-1] = sigma_2d[:, 1:] - sigma_2d[:, :-1]\n        Dy_sigma[:-1, :] = sigma_2d[1:, :] - sigma_2d[:-1, :]\n        \n        tv_integrand = np.sqrt(Dx_sigma**2 + Dy_sigma**2 + self.epsilon**2)\n        J_tv = np.sum(tv_integrand)\n        \n        return J_misfit + self.alpha * J_tv\n\n    def compute_gradient(self, sigma):\n        u = self.solve_forward(sigma)\n        lam = self.solve_adjoint(sigma, u)\n        \n        sigma_2d = sigma.reshape((self.Ny, self.Nx))\n        u_2d = u.reshape((self.Ny, self.Nx))\n        lam_2d = lam.reshape((self.Ny, self.Nx))\n        \n        # Misfit gradient\n        grad_J_misfit = np.zeros_like(sigma_2d)\n        for j in range(self.Ny):\n            for i in range(self.Nx):\n                grad_val = 0.0\n                s_k = sigma_2d[j, i]\n                # East\n                if i  self.Nx - 1:\n                    s_m = sigma_2d[j, i+1]\n                    dT_dsk = 2 * s_m**2 / (s_k + s_m)**2\n                    grad_val += dT_dsk * (u_2d[j,i] - u_2d[j,i+1]) * (lam_2d[j,i] - lam_2d[j,i+1])\n                else: grad_val += 2 * u_2d[j, i] * lam_2d[j, i]\n                # West\n                if i > 0:\n                    s_m = sigma_2d[j, i-1]\n                    dT_dsk = 2 * s_m**2 / (s_k + s_m)**2\n                    grad_val += dT_dsk * (u_2d[j,i] - u_2d[j,i-1]) * (lam_2d[j,i] - lam_2d[j,i-1])\n                else: grad_val += 2 * u_2d[j, i] * lam_2d[j, i]\n                # North\n                if j  self.Ny - 1:\n                    s_m = sigma_2d[j+1, i]\n                    dT_dsk = 2 * s_m**2 / (s_k + s_m)**2\n                    grad_val += dT_dsk * (u_2d[j,i] - u_2d[j+1,i]) * (lam_2d[j,i] - lam_2d[j+1,i])\n                else: grad_val += 2 * u_2d[j, i] * lam_2d[j, i]\n                # South\n                if j > 0:\n                    s_m = sigma_2d[j-1, i]\n                    dT_dsk = 2 * s_m**2 / (s_k + s_m)**2\n                    grad_val += dT_dsk * (u_2d[j,i] - u_2d[j-1,i]) * (lam_2d[j,i] - lam_2d[j-1,i])\n                else: grad_val += 2 * u_2d[j, i] * lam_2d[j, i]\n                \n                grad_J_misfit[j, i] = grad_val\n\n        # TV gradient\n        grad_J_tv = np.zeros_like(sigma_2d)\n        Dx_sigma = np.zeros_like(sigma_2d); Dy_sigma = np.zeros_like(sigma_2d)\n        Dx_sigma[:, :-1] = sigma_2d[:, 1:] - sigma_2d[:, :-1]\n        Dy_sigma[:-1, :] = sigma_2d[1:, :] - sigma_2d[:-1, :]\n        \n        w = np.sqrt(Dx_sigma**2 + Dy_sigma**2 + self.epsilon**2)\n        vx = Dx_sigma / w; vy = Dy_sigma / w\n        \n        vx_padded = np.pad(vx[:, :-1], ((0, 0), (1, 0)))\n        vy_padded = np.pad(vy[:-1, :], ((1, 0), (0, 0)))\n        \n        grad_J_tv = -(vx - vx_padded) - (vy - vy_padded)\n        \n        return grad_J_misfit.flatten() + self.alpha * grad_J_tv.flatten()\n\n\ndef solve():\n    np.random.seed(42)\n    s0 = np.ones(32*32)\n    ds = np.random.randn(32*32)\n    h_fd = 1e-4\n    \n    results = []\n    \n    # Cases 1 and 3: epsilon = 1e-1\n    epsilon1 = 1e-1\n    model1 = InverseProblem(epsilon=epsilon1, alpha=1e-3)\n\n    # Grad check\n    grad_J = model1.compute_gradient(s0)\n    J_plus = model1.compute_objective(s0 + h_fd * ds)\n    J_minus = model1.compute_objective(s0 - h_fd * ds)\n    \n    fd_deriv = (J_plus - J_minus) / (2 * h_fd)\n    adj_deriv = np.dot(grad_J, ds)\n    \n    rel_error1 = np.abs(adj_deriv - fd_deriv) / np.max([1.0, np.abs(adj_deriv), np.abs(fd_deriv)])\n    results.append(rel_error1)\n    \n    # Inversion\n    sigma_k = np.copy(s0)\n    c_armijo = 1e-4\n    beta_armijo = 0.5\n    sigma_min = 0.2\n    \n    for _ in range(10):\n        J_k = model1.compute_objective(sigma_k)\n        grad_J_k = model1.compute_gradient(sigma_k)\n        pk = -grad_J_k\n        \n        tk = 1e-1\n        while model1.compute_objective(sigma_k + tk * pk) > J_k + c_armijo * tk * np.dot(grad_J_k, pk):\n            tk *= beta_armijo\n        \n        sigma_k += tk * pk\n        sigma_k = np.maximum(sigma_k, sigma_min)\n    \n    final_sigma1 = sigma_k.reshape((32, 32))\n    \n    # Edge sharpness\n    Dx_s = np.zeros_like(final_sigma1); Dy_s = np.zeros_like(final_sigma1)\n    Dx_s[:, :-1] = final_sigma1[:, 1:] - final_sigma1[:, :-1]\n    Dy_s[:-1, :] = final_sigma1[1:, :] - final_sigma1[:-1, :]\n    grad_mag = np.sqrt(Dx_s**2 + Dy_s**2)\n    \n    band_radius = np.sqrt((model1.xx - 0.5)**2 + (model1.yy - 0.5)**2)\n    band_mask = np.abs(band_radius - 0.25) = (2 * model1.h)\n    \n    sharpness1 = np.mean(grad_mag[band_mask])\n    \n    # Cases 2 and 4: epsilon = 1e-3\n    epsilon2 = 1e-3\n    model2 = InverseProblem(epsilon=epsilon2, alpha=1e-3)\n\n    # Grad check\n    grad_J = model2.compute_gradient(s0)\n    J_plus = model2.compute_objective(s0 + h_fd * ds)\n    J_minus = model2.compute_objective(s0 - h_fd * ds)\n    \n    fd_deriv = (J_plus - J_minus) / (2 * h_fd)\n    adj_deriv = np.dot(grad_J, ds)\n\n    rel_error2 = np.abs(adj_deriv - fd_deriv) / np.max([1.0, np.abs(adj_deriv), np.abs(fd_deriv)])\n    \n    # Inversion\n    sigma_k = np.copy(s0)\n    for _ in range(10):\n        J_k = model2.compute_objective(sigma_k)\n        grad_J_k = model2.compute_gradient(sigma_k)\n        pk = -grad_J_k\n        \n        tk = 1e-1\n        while model2.compute_objective(sigma_k + tk * pk) > J_k + c_armijo * tk * np.dot(grad_J_k, pk):\n            tk *= beta_armijo\n        \n        sigma_k += tk * pk\n        sigma_k = np.maximum(sigma_k, sigma_min)\n\n    final_sigma2 = sigma_k.reshape((32, 32))\n\n    # Edge sharpness\n    Dx_s = np.zeros_like(final_sigma2); Dy_s = np.zeros_like(final_sigma2)\n    Dx_s[:, :-1] = final_sigma2[:, 1:] - final_sigma2[:, :-1]\n    Dy_s[:-1, :] = final_sigma2[1:, :] - final_sigma2[:-1, :]\n    grad_mag = np.sqrt(Dx_s**2 + Dy_s**2)\n    \n    sharpness2 = np.mean(grad_mag[band_mask])\n    \n    results.insert(1, rel_error2)\n    results.append(sharpness1)\n    results.append(sharpness2)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}