{
    "hands_on_practices": [
        {
            "introduction": "列文伯格-马夸尔特（Levenberg-Marquardt, LM）算法是对高斯-牛顿（Gauss-Newton, GN）方法的改进，因此要透彻理解LM算法，我们必须先掌握其基础。本练习将重点关注纯粹的高斯-牛顿步骤，它通过最小化问题的线性化残差模型来推导。通过从第一性原理出发推导正规方程，并将其应用于一个具体实例，您将为高斯-牛顿法如何预测优化进程建立坚实的直觉，并理解为何这种预测构成了更高级阻尼策略的基础。",
            "id": "3396998",
            "problem": "考虑一个非线性最小二乘反问题，其目标是最小化函数 $\\Phi(x) = \\tfrac{1}{2}\\|r(x)\\|^{2}$，其中 $r(x) \\in \\mathbb{R}^{m}$ 是残差向量，$J(x) \\in \\mathbb{R}^{m \\times n}$ 是其关于参数向量 $x \\in \\mathbb{R}^{n}$ 的雅可比矩阵。在当前迭代点 $x$ 处，高斯-牛顿 (Gauss-Newton, GN) 步 $s \\in \\mathbb{R}^{n}$ 定义为线性化模型 $\\tfrac{1}{2}\\|r + J s\\|^{2}$ 关于步长 $s$ 的最小化子。Levenberg-Marquardt (LM) 方法引入了一个阻尼参数来稳定这一步，但在这里，您将专注于无阻尼的 GN 步，并在线性化模型中根据残差减少来解释它。\n\n给定一个双参数拟合问题，其中 $m = 3$ 和 $n = 2$，在当前迭代点处计算的雅可比矩阵和残差向量为\n$$\nJ = \\begin{pmatrix}\n1  0 \\\\\n0  2 \\\\\n1  1\n\\end{pmatrix}, \n\\qquad\nr = \\begin{pmatrix}\n-1 \\\\\n2 \\\\\n0\n\\end{pmatrix}.\n$$\n\n任务：\n1) 从线性化最小二乘模型 $\\tfrac{1}{2}\\|r + J s\\|^{2}$ 的定义出发，推导出刻画高斯-牛顿步 $s$ 的正规方程组（不要假设任何公式；请从第一性原理进行推导）。\n2) 使用给定的 $J$ 和 $r$，计算精确的高斯-牛顿步 $s$。\n3) 通过计算线性化目标函数的精确减少量 $\\Delta_{\\mathrm{GN}} = \\tfrac{1}{2}\\|r\\|^{2} - \\tfrac{1}{2}\\|r + J s\\|^{2}$（其中 $s$ 是第2部分得到的高斯-牛顿步），从预测的残差减少角度解释该步长。\n\n您的最终答案应为 $\\Delta_{\\mathrm{GN}}$ 的精确值，以最简分数形式表示。请勿四舍五入。无需单位。",
            "solution": "该问题是良定的，有科学依据，并包含了获得唯一解所需的所有信息。我们将按顺序解决这三个任务。\n\n**1) 正规方程组的推导**\n\n高斯-牛顿 (GN) 步，记为 $s \\in \\mathbb{R}^{n}$，被定义为在当前迭代点 $x$ 处线性化最小二乘目标函数的最小化子。这个目标函数，我们称之为 $L(s)$，通过使用残差向量 $r(x+s)$ 的一阶泰勒展开 $r(x) + J(x)s$ 来近似，从而在 $x$ 的邻域内对成本函数 $\\Phi(x)$ 进行建模。为简化符号，我们令 $r = r(x)$ 和 $J = J(x)$。需要最小化的函数是：\n$$\nL(s) = \\frac{1}{2} \\| r + J s \\|^{2}\n$$\n欧几里得范数的平方可以表示为点积，或等价地，使用向量转置：\n$$\nL(s) = \\frac{1}{2} (r + J s)^{T} (r + J s)\n$$\n展开转置乘积得到：\n$$\nL(s) = \\frac{1}{2} (r^{T} + s^{T}J^{T}) (r + J s) = \\frac{1}{2} (r^{T}r + r^{T}Js + s^{T}J^{T}r + s^{T}J^{T}J s)\n$$\n由于 $r^{T}Js$ 是一个标量（$1 \\times 1$ 矩阵），其转置等于自身：$(r^{T}Js)^{T} = s^{T}J^{T}r$。因此，中间两项是相同的。目标函数简化为：\n$$\nL(s) = \\frac{1}{2} (r^{T}r + 2s^{T}J^{T}r + s^{T}J^{T}J s)\n$$\n为了找到最小化 $L(s)$ 的向量 $s$，我们必须通过将 $L(s)$ 关于 $s$ 的梯度设置为零向量来找到临界点。梯度 $\\nabla_s L(s)$ 通过对 $s$ 的每个分量求导来计算：\n$$\n\\nabla_s L(s) = \\frac{1}{2} \\nabla_s(r^{T}r) + \\nabla_s(s^{T}J^{T}r) + \\frac{1}{2} \\nabla_s(s^{T}J^{T}J s)\n$$\n各项的计算如下：\n- 项 $r^{T}r$ 关于 $s$ 是常数，所以其梯度为 $0$。\n- 线性项 $s^{T}a$ 的梯度是 $a$。这里，$a = J^{T}r$，所以 $\\nabla_s(s^{T}J^{T}r) = J^{T}r$。\n- 二次型 $s^{T}As$ 的梯度是 $(A+A^{T})s$。这里，矩阵是 $J^{T}J$，它是对称的，所以 $\\nabla_s(s^{T}J^{T}J s) = 2J^{T}J s$。\n\n结合这些结果，梯度为：\n$$\n\\nabla_s L(s) = J^{T}r + J^{T}J s\n$$\n将梯度设为零以求最小值，得到一阶必要条件：\n$$\nJ^{T}J s + J^{T}r = 0\n$$\n重新整理此方程，得到称为**正规方程组**的线性方程组：\n$$\nJ^{T}J s = -J^{T}r\n$$\n推导至此完成。\n\n**2) 高斯-牛顿步的计算**\n\n给定雅可比矩阵 $J$ 和残差向量 $r$：\n$$\nJ = \\begin{pmatrix} 1  0 \\\\ 0  2 \\\\ 1  1 \\end{pmatrix}, \\qquad r = \\begin{pmatrix} -1 \\\\ 2 \\\\ 0 \\end{pmatrix}\n$$\n首先，我们计算矩阵 $J^{T}J$：\n$$\nJ^{T}J = \\begin{pmatrix} 1  0  1 \\\\ 0  2  1 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  2 \\\\ 1  1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 0 + 1 \\cdot 1  1 \\cdot 0 + 0 \\cdot 2 + 1 \\cdot 1 \\\\ 0 \\cdot 1 + 2 \\cdot 0 + 1 \\cdot 1  0 \\cdot 0 + 2 \\cdot 2 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 2  1 \\\\ 1  5 \\end{pmatrix}\n$$\n接下来，我们计算向量 $-J^{T}r$：\n$$\n-J^{T}r = - \\begin{pmatrix} 1  0  1 \\\\ 0  2  1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 2 \\\\ 0 \\end{pmatrix} = - \\begin{pmatrix} 1(-1) + 0(2) + 1(0) \\\\ 0(-1) + 2(2) + 1(0) \\end{pmatrix} = - \\begin{pmatrix} -1 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -4 \\end{pmatrix}\n$$\n现在我们求解正规方程组 $J^{T}J s = -J^{T}r$ 以得到 $s = \\begin{pmatrix} s_1 \\\\ s_2 \\end{pmatrix}$：\n$$\n\\begin{pmatrix} 2  1 \\\\ 1  5 \\end{pmatrix} \\begin{pmatrix} s_1 \\\\ s_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -4 \\end{pmatrix}\n$$\n这代表了以下两个线性方程组成的方程组：\n1. $2s_1 + s_2 = 1$\n2. $s_1 + 5s_2 = -4$\n\n从第一个方程，我们可以用 $s_1$ 表示 $s_2$：$s_2 = 1 - 2s_1$。\n将此代入第二个方程：\n$$\ns_1 + 5(1 - 2s_1) = -4 \\implies s_1 + 5 - 10s_1 = -4 \\implies -9s_1 = -9 \\implies s_1 = 1\n$$\n将 $s_1 = 1$ 代回 $s_2$ 的表达式中：\n$$\ns_2 = 1 - 2(1) = -1\n$$\n因此，精确的高斯-牛顿步为 $s = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$。\n\n**3) 预测残差减少量的计算**\n\n预测的残差减少量 $\\Delta_{\\mathrm{GN}}$ 是线性化目标函数值从零步移动到高斯-牛顿步 $s$ 时的减少量：\n$$\n\\Delta_{\\mathrm{GN}} = L(0) - L(s) = \\frac{1}{2}\\|r\\|^{2} - \\frac{1}{2}\\|r + J s\\|^{2}\n$$\n我们将分别计算每一项。首先，线性化目标的初始值：\n$$\n\\frac{1}{2}\\|r\\|^{2} = \\frac{1}{2} \\left( (-1)^{2} + 2^{2} + 0^{2} \\right) = \\frac{1}{2}(1 + 4 + 0) = \\frac{5}{2}\n$$\n接下来，我们计算预测的残差向量 $r + Js$：\n$$\nJs = \\begin{pmatrix} 1  0 \\\\ 0  2 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(-1) \\\\ 0(1) + 2(-1) \\\\ 1(1) + 1(-1) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix}\n$$\n$$\nr + Js = \\begin{pmatrix} -1 \\\\ 2 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ -2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n那么线性化目标的新值为：\n$$\n\\frac{1}{2}\\|r + J s\\|^{2} = \\frac{1}{2}\\| \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\|^{2} = \\frac{1}{2}(0) = 0\n$$\n最后，我们计算减少量 $\\Delta_{\\mathrm{GN}}$：\n$$\n\\Delta_{\\mathrm{GN}} = \\frac{5}{2} - 0 = \\frac{5}{2}\n$$\n预测减少量的值恰好是 $\\frac{5}{2}$。这个显著的减少（它将线性化残差驱动为零）表明原始残差向量 $r$ 位于 $-J$ 的列空间中，这代表了一个特殊情况，即线性化问题是相容的并且有一个零残差解。",
            "answer": "$$\n\\boxed{\\frac{5}{2}}\n$$"
        },
        {
            "introduction": "高斯-牛顿法虽然高效，但在其基本假设被违背时可能会彻底失败，本练习旨在探究这样一个失效案例。我们将分析一个精心构建的场景，其中高斯-牛顿法对Hessian矩阵的近似（$J^T J$）与目标函数的真实Hessian矩阵相比，包含了错误的曲率信息。这个练习揭示了为何盲目应用高斯-牛顿法会导致迭代发散，从而有力地说明了为何需要一种如列文伯格-马夸尔特算法中的阻尼参数那样的稳定机制。",
            "id": "3247339",
            "problem": "考虑一个单变量非线性最小二乘问题，其残差为 $r(x) = x^3 - 10$，目标函数为 $f(x) = \\tfrac{1}{2}\\,r(x)^2$。令 $J(x)$ 表示 $r(x)$ 的雅可比矩阵（在此为标量导数），令 $H(x)$ 表示 $f(x)$ 的精确海森矩阵（二阶导数）。取初始点 $x_0 = 0.5$。\n\n考虑使用两种方法来最小化 $f(x)$：\n- 用于无约束最小化的完整牛顿法，该方法在步长的线性系统中使用精确的海森矩阵 $H(x)$。\n- Levenberg–Marquardt (LM) 方法，一种阻尼高斯-牛顿方案，对于给定的阻尼参数 $\\lambda  0$，它在每次迭代中通过求解 $(J(x)^\\top J(x) + \\lambda I)\\,p = -J(x)^\\top r(x)$ 来计算 $p$，并调整 $\\lambda$ 以实现 $f$ 的下降。\n\n仅使用梯度、海森矩阵和雅可比矩阵的定义，以及标准的泰勒下降推理，分析这些方法在 $x_0$ 处的行为。选择所有正确的陈述。\n\nA. 在 $x_0 = 0.5$ 处，精确的海森矩阵 $H(x_0)$ 为负，而 $J(x_0)^\\top J(x_0)$ 为正，因此两者甚至符号都不同。\n\nB. 从 $x_0 = 0.5$ 开始，用于最小化 $f$ 的纯完整牛顿步会移动到更小的 $x$ 并增加 $f$，而对于足够大的 $\\lambda$，Levenberg–Marquardt 步通过移动到更大的 $x$ 产生 $f$ 的下降。\n\nC. 在极小值点 $x^\\star = \\sqrt[3]{10}$ 处，有 $H(x^\\star) = J(x^\\star)^\\top J(x^\\star)$；因此，在 $x^\\star$ 附近，完整牛顿法和 Levenberg–Marquardt 法都表现出相同的局部二次收敛性（假设 LM 阻尼在成功步之后趋于 $0$）。\n\nD. 对于任何固定的 $\\lambda  0$，Levenberg–Marquardt 步等于负梯度步，即 $p = -\\nabla f(x)$。\n\nE. 如果从一个负的 $x_0$ 开始，完整牛顿步保证是一个下降方向，因为在最小二乘问题中，真实的海森矩阵总是正的。",
            "solution": "问题陈述是有效的。它提出了一个定义明确的数值优化数学问题，没有科学或逻辑上的不一致之处。\n\n给定残差函数 $r(x) = x^3 - 10$ 和目标函数 $f(x) = \\frac{1}{2}r(x)^2 = \\frac{1}{2}(x^3 - 10)^2$。初始点为 $x_0 = 0.5$。我们必须分析两种优化方法：完整牛顿法和 Levenberg-Marquardt (LM) 方法。\n\n首先，我们计算 $r(x)$ 和 $f(x)$ 的必要导数。\n$r(x)$ 的雅可比矩阵是它的一阶导数：\n$$J(x) = \\frac{dr}{dx} = 3x^2$$\n$f(x)$ 的梯度是它的一阶导数：\n$$\\nabla f(x) = \\frac{df}{dx} = r(x) \\frac{dr}{dx} = (x^3 - 10)(3x^2) = 3x^5 - 30x^2$$\n作为检验，最小二乘问题的梯度由 $\\nabla f(x) = J(x)^\\top r(x)$ 给出。在这个一维情况下，即为 $J(x)r(x) = (3x^2)(x^3 - 10)$，结果相符。\n\n$f(x)$ 的精确海森矩阵是它的二阶导数：\n$$H(x) = \\frac{d^2f}{dx^2} = \\frac{d}{dx}(3x^5 - 30x^2) = 15x^4 - 60x$$\n最小二乘目标函数的海森矩阵的通用公式为 $H(x) = J(x)^\\top J(x) + \\sum_i r_i(x) \\nabla^2 r_i(x)$。这里，对于单个残差 $r(x)$，该公式变为 $H(x) = (J(x))^2 + r(x) \\frac{d^2r}{dx^2}$。\n残差的二阶导数是 $\\frac{d^2r}{dx^2} = 6x$。\n因此，$H(x) = (3x^2)^2 + (x^3 - 10)(6x) = 9x^4 + 6x^4 - 60x = 15x^4 - 60x$。这证实了我们的计算。\n\n海森矩阵的高斯-牛顿近似由 $J(x)^\\top J(x)$ 给出。在一维情况下，这是：\n$$J(x)^\\top J(x) = (J(x))^2 = (3x^2)^2 = 9x^4$$\n\n现在，我们在初始点 $x_0 = 0.5$ 处计算这些量：\n- 雅可比矩阵：$J(x_0) = J(0.5) = 3(0.5)^2 = 3(0.25) = 0.75 = \\frac{3}{4}$\n- 梯度：$\\nabla f(x_0) = \\nabla f(0.5) = 3(0.5)^5 - 30(0.5)^2 = 3(\\frac{1}{32}) - 30(\\frac{1}{4}) = \\frac{3}{32} - \\frac{240}{32} = -\\frac{237}{32}$\n- 精确海森矩阵：$H(x_0) = H(0.5) = 15(0.5)^4 - 60(0.5) = 15(\\frac{1}{16}) - 30 = \\frac{15 - 480}{16} = -\\frac{465}{16} = -29.0625$\n- 高斯-牛顿海森矩阵近似：$J(x_0)^\\top J(x_0) = 9(0.5)^4 = 9(\\frac{1}{16}) = \\frac{9}{16} = 0.5625$\n\n有了这些值，我们可以分析每个选项。\n\n**A. 在 $x_0 = 0.5$ 处，精确的海森矩阵 $H(x_0)$ 为负，而 $J(x_0)^\\top J(x_0)$ 为正，因此两者甚至符号都不同。**\n根据我们的计算，$H(x_0) = -\\frac{465}{16}$，是负数。\n而 $J(x_0)^\\top J(x_0) = \\frac{9}{16}$，是正数。\n该陈述正确地指出一个为负一个为正，因此它们符号不同。这突显了高斯-牛顿近似效果不佳的情况，因为它未能捕捉目标函数 $f(x)$ 在 $x_0$ 处的局部曲率。\n**结论：正确。**\n\n**B. 从 $x_0 = 0.5$ 开始，用于最小化 $f$ 的纯完整牛顿步会移动到更小的 $x$ 并增加 $f$，而对于足够大的 $\\lambda$，Levenberg–Marquardt 步通过移动到更大的 $x$ 产生 $f$ 的下降。**\n让我们分析完整牛顿步 $p_N$，它通过求解 $H(x_0) p_N = -\\nabla f(x_0)$ 得到。\n$$(-\\frac{465}{16}) p_N = -(-\\frac{237}{32}) = \\frac{237}{32}$$\n$$p_N = \\frac{237}{32} \\cdot (-\\frac{16}{465}) = -\\frac{237}{2 \\cdot 465} = -\\frac{237}{930} \\approx -0.255$$\n由于 $p_N  0$，新点 $x_1 = x_0 + p_N$ 将小于 $x_0$。\n为了确定 $f$ 是否增加，我们检查方向导数的符号 $\\nabla f(x_0)^\\top p_N$。\n$$\\nabla f(x_0)^\\top p_N = (-\\frac{237}{32}) \\cdot (-\\frac{237}{930}) = \\frac{237^2}{32 \\cdot 930} > 0$$\n正的方向导数意味着函数沿此步长方向增加。这是预料之中的，因为只有当海森矩阵是正定时，牛顿步才是一个下降方向，而这里的情况并非如此（$H(x_0)  0$）。\n\n现在，我们来分析 Levenberg-Marquardt 步 $p_{LM}$，它通过求解 $(J(x_0)^\\top J(x_0) + \\lambda I) p_{LM} = -J(x_0)^\\top r(x_0) = -\\nabla f(x_0)$（其中 $\\lambda  0$）得到。\n$$(\\frac{9}{16} + \\lambda) p_{LM} = \\frac{237}{32}$$\n由于 $\\lambda  0$，项 $(\\frac{9}{16} + \\lambda)$ 是正的。右侧也是正的。因此，$p_{LM}  0$。一个正的步长意味着该方法向更大的 $x$ 移动。\n方向导数是 $\\nabla f(x_0)^\\top p_{LM}$：\n$$\\nabla f(x_0)^\\top p_{LM} = (-\\frac{237}{32}) \\cdot \\left( \\frac{237/32}{9/16 + \\lambda} \\right) = - \\frac{(237/32)^2}{9/16 + \\lambda}  0$$\n由于方向导数是负的，对于任何 $\\lambda  0$，LM 步都是一个下降方向。LM 算法会调整 $\\lambda$ 以确保步长能导致函数值下降。当 $\\lambda \\to \\infty$ 时，步长方向接近负梯度方向，$p_{LM} \\approx \\frac{1}{\\lambda}(-\\nabla f(x_0))$，这是最速下降方向，保证了足够小的步长能使函数局部下降。方向 $-\\nabla f(x_0)$ 是正的，这与 $p_{LM}  0$ 一致。\n陈述的两个部分都正确。\n**结论：正确。**\n\n**C. 在极小值点 $x^\\star = \\sqrt[3]{10}$ 处，有 $H(x^\\star) = J(x^\\star)^\\top J(x^\\star)$；因此，在 $x^\\star$ 附近，完整牛顿法和 Levenberg–Marquardt 法都表现出相同的局部二次收敛性（假设 LM 阻尼在成功步之后趋于 $0$）。**\n当残差 $r(x) = x^3 - 10$ 为零时，目标函数 $f(x) = \\frac{1}{2}(x^3 - 10)^2$ 达到最小值。这发生在 $x^\\star = \\sqrt[3]{10}$。这是一个“零残差”问题。\n精确海森矩阵为 $H(x) = J(x)^\\top J(x) + r(x)r''(x)$。在极小值点 $x^\\star$ 处，由于 $r(x^\\star) = 0$，第二项消失：\n$$H(x^\\star) = J(x^\\star)^\\top J(x^\\star) + 0 \\cdot r''(x^\\star) = J(x^\\star)^\\top J(x^\\star)$$\n所以陈述的第一部分是正确的。\n如果解处的海森矩阵 $H(x^\\star)$ 是正定的，完整牛顿法表现出局部二次收敛性。这里，$H(x^\\star) = J(x^\\star)^\\top J(x^\\star) = 9(x^\\star)^4 = 9(10^{4/3})  0$，所以条件满足。\nLevenberg-Marquardt 方法，当阻尼参数 $\\lambda$ 趋于 0 时（这在接近解的成功步中会发生），就变成了高斯-牛顿法。步长通过 $(J(x)^\\top J(x)) p = -J(x)^\\top r(x)$ 计算。\n对于零残差问题，高斯-牛顿法本身就表现出二次收敛性。这是因为在解附近，高斯-牛顿海森矩阵近似 $J(x)^\\top J(x)$ 是对真实海森矩阵 $H(x)$ 的一个非常好的近似，因为 $H(x) - J(x)^\\top J(x) = r(x)r''(x)$ 且 $r(x) \\to 0$。\n由于当 $\\lambda \\to 0$ 时，LM 方法变成高斯-牛顿法，并且对于这个零残差问题，高斯-牛顿法的迭代与完整牛顿法的迭代在渐近上是相同的，因此两种方法将共享相同的局部二次收敛速度。\n**结论：正确。**\n\n**D. 对于任何固定的 $\\lambda  0$，Levenberg–Marquardt 步等于负梯度步，即 $p = -\\nabla f(x)$。**\nLM 步长 $p$ 由 $(J(x)^\\top J(x) + \\lambda I) p = -\\nabla f(x)$ 定义。\n负梯度步长就是 $-\\nabla f(x)$。如果该陈述为真，将 $p = -\\nabla f(x)$ 代入 LM 方程必须得到一个恒等式。\n$$(J(x)^\\top J(x) + \\lambda I)(-\\nabla f(x)) = -\\nabla f(x)$$\n假设 $\\nabla f(x) \\neq 0$，我们可以除以 $-\\nabla f(x)$ 得到：\n$$J(x)^\\top J(x) + \\lambda I = I$$\n在这个一维情况下，这意味着 $9x^4 + \\lambda = 1$。这个方程对于任意 $x$ 和任何固定的 $\\lambda  0$ 并不成立。例如，在 $x_0=0.5$ 处，它将要求 $9/16 + \\lambda = 1$，即 $\\lambda = 7/16$。这对于“任何固定的 $\\lambda  0$”并不成立。\n在极限情况 $\\lambda \\to \\infty$ 下，LM 步长变为 $p_{LM} \\approx \\frac{1}{\\lambda}(-\\nabla f(x))$，这是一个*缩放后*的负梯度步，而不是与之相等。\n**结论：错误。**\n\n**E. 如果从一个负的 $x_0$ 开始，完整牛顿步保证是一个下降方向，因为在最小二乘问题中，真实的海森矩阵总是正的。**\n该陈述基于一个给定的原因得出一个结论。我们必须对两者进行评估。\n给出的原因是“在最小二乘问题中，真实的海森矩阵总是正的”。这是一个错误的普遍性陈述。海森矩阵是 $H(x) = J(x)^\\top J(x) + \\sum_i r_i(x) \\nabla^2 r_i(x)$。虽然 $J(x)^\\top J(x)$ 是半正定的，但涉及残差的第二项可能为负，使得整个海森矩阵不定或负定。我们自己的问题就提供了一个反例：在 $x_0=0.5$ 处，我们发现 $H(0.5)  0$。\n因为所提供的理由从根本上是错误的，所以整个逻辑陈述是无效的。\n尽管结论（“如果从一个负的 $x_0$ 开始，完整牛顿步保证是一个下降方向”）对于这个特定问题可能是正确的（对于 $x  0$，$H(x) = 15x^4 - 60x = 15x(x^3-4)$，其中 $x$ 和 $(x^3-4)$ 都是负的，使得 $H(x)0$，这保证了一个下降方向），但不能从一个错误的假设中得出正确的结论。该陈述的推理是有缺陷的，从科学的角度来看是不可接受的。\n**结论：错误。**",
            "answer": "$$\\boxed{ABC}$$"
        },
        {
            "introduction": "理论是必不可少的，但亲手实现算法才能将理解固化，这最后一个练习将带您从概念走向代码。您将实现一个完整的列文伯格-马夸尔特算法，包括由增益比（gain ratio）控制的关键自适应阻尼策略，该比率用于衡量预测进展与实际进展的一致性。通过构建算法并针对从理想情况到病态条件等多种场景进行测试，您将获得关于LM算法核心机制的实践经验，并理解它为何能成为科学计算和数据同化领域中一个稳健且广泛应用的工具。",
            "id": "3396965",
            "problem": "考虑非线性最小二乘反问题，该问题旨在寻找参数 $\\theta \\in \\mathbb{R}^p$ 以最小化目标函数 $\\Phi(\\theta)$，其定义为 $\\Phi(\\theta) = \\dfrac{1}{2} \\sum_{i=1}^{n} r_i(\\theta)^2$，其中残差为 $r_i(\\theta) = y_i - f(x_i, \\theta)$。从线性化 $r(\\theta + s) \\approx r(\\theta) + J(\\theta) s$ 出发，其中 $J(\\theta)$ 是 $r(\\theta)$ 的雅可比矩阵，并利用当前迭代点 $\\theta$ 附近目标函数的二次模型，推导通过最小化正则化模型 $m(s)$ 得到的阻尼高斯-牛顿步，$m(s)$ 定义为 $m(s) = \\dfrac{1}{2} \\| r(\\theta) + J(\\theta) s \\|_2^2 + \\dfrac{1}{2} \\lambda \\| s \\|_2^2$，其中 $\\lambda  0$ 是一个标量阻尼参数。然后，将增益比 $\\rho$ 定义为实际下降量与在二次模型下计算的预测下降量之商。基于 $\\rho$ 提供一个有原则的 $s$ 接受准则，并提供一个跨迭代调整 $\\lambda$ 的乘法策略，该策略在二次模型可靠时减小 $\\lambda$，在二次模型不可靠时增大 $\\lambda$。\n\n您的推导必须从线性化 $r(\\theta + s) \\approx r(\\theta) + J(\\theta) s$ 和 $\\Phi(\\theta)$ 的定义开始，并使用数值优化最小二乘问题的成熟结论进行。在未说明最终步长为何由最小化 $m(s)$ 得出之前，您不得假定任何特定的步长公式。\n\n完成推导后，实现一个程序，应用带有由增益比控制的自适应阻尼策略的 Levenberg-Marquardt 方法，在以下数据同化设定中拟合参数。正向模型为 $f(x, \\theta)$，其中 $f(x, \\theta) = \\theta_1 \\exp(\\theta_2 x)$，因此参数向量为 $\\theta = [\\theta_1, \\theta_2]^\\top$。对于每次迭代，通过求解阻尼正规方程组计算步长 $s$，并根据实际下降量和预测下降量计算增益比 $\\rho$。使用接受准则：如果 $\\rho  0$，则接受 $s$；否则，拒绝 $s$。使用自适应阻尼规则：如果 $\\rho  0.75$，设置 $\\lambda \\leftarrow \\lambda / 2$；如果 $\\rho  0.25$，设置 $\\lambda \\leftarrow 2 \\lambda$；否则，保持 $\\lambda$ 不变。如果一个步长被拒绝，设置 $\\lambda \\leftarrow 2 \\lambda$ 并在下一次迭代中重新计算。当 $\\| \\nabla \\Phi(\\theta) \\|_\\infty \\leq 10^{-8}$，或 $\\| s \\|_2 \\leq 10^{-10}$，或迭代次数达到 $50$ 时终止。\n\n该程序必须评估三个测试用例以测试方法的覆盖范围：\n\n- 测试用例1（理想路径）。从含中等噪声的数据中恢复参数。设 $n = 7$， $x = [0, 0.5, 1, 1.5, 2, 2.5, 3]$，真实参数为 $\\theta^\\star = [2, -0.5]^\\top$，观测值为 $y_i = \\theta_1^\\star \\exp(\\theta_2^\\star x_i) + \\epsilon_i$，固定扰动为 $\\epsilon = [0.05, -0.02, 0.04, -0.03, 0.01, -0.02, 0]$。使用初始猜测值 $\\theta_0 = [1, 0]^\\top$ 和初始阻尼 $\\lambda_0 = 10^{-2}$。\n\n- 测试用例2（近完美拟合边界条件）。模型相同，数据无噪声，真实参数不同。设 $n = 7$， $x = [0, 0.5, 1, 1.5, 2, 2.5, 3]$，真实参数为 $\\theta^\\star = [1.5, 0.2]^\\top$，观测值为 $y_i = \\theta_1^\\star \\exp(\\theta_2^\\star x_i)$。使用初始猜测值 $\\theta_0 = [1, 0]^\\top$ 和初始阻尼 $\\lambda_0 = 10^{-2}$。\n\n- 测试用例3（病态雅可比矩阵的重要边缘情况）。所有输入均为零，因此雅可比矩阵的第二列为零。设 $n = 6$，$x_i = 0$ 对所有 $i$ 成立，真实参数为 $\\theta^\\star = [3, -1]^\\top$，观测值为 $y_i = \\theta_1^\\star$。使用初始猜测值 $\\theta_0 = [1, 1]^\\top$ 和初始阻尼 $\\lambda_0 = 10^{-2}$。\n\n对于每个测试用例，按规定运行算法并返回一个列表，其中包含最终目标值 $\\Phi(\\theta_{\\text{final}})$、最终阻尼参数 $\\lambda_{\\text{final}}$ 和执行的迭代次数。您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，其中每个元素本身是 $[\\Phi(\\theta_{\\text{final}}), \\lambda_{\\text{final}}, \\text{iterations}]$ 形式的列表。",
            "solution": "用户提供了一个在数值优化和反问题领域中表述清晰的适定问题。该问题具有科学依据、内容独立且可形式化。它被验证为一个有效的问题。\n\n### 第一部分：Levenberg-Marquardt 步长和自适应策略的推导\n\n目标是找到参数向量 $\\theta \\in \\mathbb{R}^p$，以最小化非线性最小二乘目标函数：\n$$\n\\Phi(\\theta) = \\frac{1}{2} \\sum_{i=1}^{n} r_i(\\theta)^2 = \\frac{1}{2} \\|r(\\theta)\\|_2^2\n$$\n其中 $r(\\theta)$ 是残差向量。Levenberg-Marquardt (LM) 方法是一种迭代算法，在每次迭代 $k$ 时，找到一个步长 $s$ 来更新当前的参数估计 $\\theta_k$ 为 $\\theta_{k+1} = \\theta_k + s$。\n\n推导从残差向量 $r(\\theta)$ 在当前迭代点 $\\theta$ 附近的一阶泰勒展开开始：\n$$\nr(\\theta + s) \\approx r(\\theta) + J(\\theta) s\n$$\n这里，$J(\\theta)$ 是 $r(\\theta)$ 的雅可比矩阵，其元素为 $J_{ij}(\\theta) = \\frac{\\partial r_i}{\\partial \\theta_j}$。\n\n将此线性化代入目标函数 $\\Phi(\\theta+s)$，得到目标函数的一个二次模型：\n$$\n\\Phi(\\theta + s) \\approx \\frac{1}{2} \\|r(\\theta) + J(\\theta) s\\|_2^2\n$$\n直接对该二次模型关于 $s$ 进行最小化可得到高斯-牛顿步。然而，如果雅可比矩阵 $J(\\theta)$ 是病态的（即 $J(\\theta)^\\top J(\\theta)$ 是奇异或近奇异的），高斯-牛顿法可能会不稳定。\n\nLevenberg-Marquardt 方法通过引入一个吉洪诺夫式正则化项来解决这个问题，这在当前迭代点周围创建了一个信赖域。步长 $s$ 通过最小化一个正则化的二次模型 $m(s)$ 来找到：\n$$\nm(s) = \\frac{1}{2} \\|r(\\theta) + J(\\theta) s\\|_2^2 + \\frac{1}{2} \\lambda \\|s\\|_2^2\n$$\n其中 $\\lambda  0$ 是一个标量阻尼参数。为简化符号，我们令 $r = r(\\theta)$ 和 $J = J(\\theta)$。\n\n为了找到最小化 $m(s)$ 的步长 $s$，我们计算 $m(s)$ 关于 $s$ 的梯度并令其为零。首先，我们展开 $m(s)$ 的表达式：\n$$\nm(s) = \\frac{1}{2} (r + Js)^\\top (r + Js) + \\frac{1}{2} \\lambda s^\\top s\n$$\n$$\nm(s) = \\frac{1}{2} (r^\\top r + r^\\top J s + s^\\top J^\\top r + s^\\top J^\\top J s) + \\frac{1}{2} \\lambda s^\\top s\n$$\n由于 $r^\\top J s$ 是一个标量，它等于其转置 $s^\\top J^\\top r$。因此：\n$$\nm(s) = \\frac{1}{2} r^\\top r + r^\\top J s + \\frac{1}{2} s^\\top J^\\top J s + \\frac{1}{2} \\lambda s^\\top I s\n$$\n$$\nm(s) = \\frac{1}{2} \\|r\\|_2^2 + (J^\\top r)^\\top s + \\frac{1}{2} s^\\top (J^\\top J + \\lambda I) s\n$$\n$m(s)$ 关于 $s$ 的梯度是：\n$$\n\\nabla_s m(s) = J^\\top r + (J^\\top J + \\lambda I) s\n$$\n令梯度为零，$\\nabla_s m(s) = 0$，得到驻点：\n$$\n(J^\\top J + \\lambda I) s = -J^\\top r\n$$\n这就是**阻尼正规方程组**。这个线性系统的解 $s$ 就是 Levenberg-Marquardt 步长。项 $J^\\top J$ 是 $\\Phi(\\theta)$ 的海森矩阵的高斯-牛顿近似，而 $J^\\top r$ 是 $\\Phi(\\theta)$ 的梯度。对于 $\\lambda  0$，矩阵 $(J^\\top J + \\lambda I)$ 保证是对称正定的，从而确保存在唯一的解 $s$，并且它是一个下降方向。\n\n步长 $s$ 的有效性通过比较目标函数的实际下降量与模型预测的下降量来评估。这种比较由**增益比** $\\rho$ 来量化。\n\n**实际下降量**是目标函数的真实变化：\n$$\n\\Delta\\Phi_{\\text{actual}} = \\Phi(\\theta) - \\Phi(\\theta+s) = \\frac{1}{2} \\|r(\\theta)\\|_2^2 - \\frac{1}{2} \\|r(\\theta+s)\\|_2^2\n$$\n**预测下降量**是受信任模型 $m(s)$ 从 $s=0$ 到计算出的步长 $s$ 的变化：\n$$\n\\Delta\\Phi_{\\text{pred}} = m(0) - m(s) = \\frac{1}{2} \\|r\\|_2^2 - \\left( \\frac{1}{2} \\|r+Js\\|_2^2 + \\frac{1}{2}\\lambda\\|s\\|_2^2 \\right)\n$$\n使用以 $s$ 表示的 $m(s)$ 的表达式，可以简化。我们已经得到 $\\nabla_s m(s) = J^\\top r + (J^\\top J + \\lambda I) s = 0$。利用这一点，预测下降量可以写为：\n$$\n\\Delta\\Phi_{\\text{pred}} = -\\frac{1}{2} s^\\top (J^\\top r) = \\frac{1}{2} s^\\top (J^\\top J + \\lambda I) s\n$$\n对于 $\\lambda0$ 和 $s \\neq 0$，该表达式保证为非负。\n\n**增益比** $\\rho$ 定义为：\n$$\n\\rho = \\frac{\\Delta\\Phi_{\\text{actual}}}{\\Delta\\Phi_{\\text{pred}}} = \\frac{\\Phi(\\theta) - \\Phi(\\theta+s)}{-\\frac{1}{2} s^\\top (J(\\theta)^\\top r(\\theta))}\n$$\n$\\rho$ 的值表明了二次模型近似的质量：\n- $\\rho \\approx 1$：模型是一个极好的近似。\n- $\\rho  0$：步长导致了目标函数的减小。\n- $\\rho \\le 0$：步长导致了目标函数的增加（或没有变化）。\n\n基于增益比 $\\rho$，可以制定一个有原则的**接受准则**和**阻尼策略**：\n1.  **接受准则**：如果步长 $s$ 导致目标函数减小，即 $\\rho  0$，则接受该步长。如果步长被接受，则更新参数：$\\theta_{k+1} = \\theta_k + s$。如果步长被拒绝（$\\rho \\le 0$），则不更新参数：$\\theta_{k+1} = \\theta_k$。\n2.  **自适应阻尼策略**：根据模型的可靠性（由 $\\rho$ 指示）调整阻尼参数 $\\lambda$。\n    - 如果 $\\rho$ 很大（例如 $\\rho  0.75$），则模型高度可靠。我们可以更激进，通过减小阻尼来更接近收敛更快的的高斯-牛顿法：$\\lambda \\leftarrow \\lambda / 2$。\n    - 如果 $\\rho$ 很小或为负（例如 $\\rho  0.25$），则模型不可靠。线性化效果差，因此我们必须更保守，通过增加阻尼来采取一个更接近最速下降方向的更小步长：$\\lambda \\leftarrow 2 \\lambda$。这也包括拒绝步长的情况。\n    - 如果 $\\rho$ 在一个中间范围（例如 $0.25 \\le \\rho \\le 0.75$），则模型是充分的，阻尼参数可以保持不变。\n\n步长计算、接受准则和自适应阻尼的这种组合构成了 Levenberg-Marquardt 算法的核心。\n\n### 第二部分：实现\n以下代码实现了带有上述自适应策略的 Levenberg-Marquardt 算法，以解决三个指定的测试用例。",
            "answer": "```python\nimport numpy as np\n\ndef run_lm(case_params):\n    \"\"\"\n    Executes the Levenberg-Marquardt algorithm for a given test case.\n\n    Args:\n        case_params (tuple): A tuple containing the problem setup:\n            (x data, y data, initial theta, initial lambda, max iterations,\n             gradient tolerance, step tolerance).\n\n    Returns:\n        list: A list containing the final objective function value,\n              the final damping parameter, and the number of iterations performed.\n    \"\"\"\n    x, y, theta0, lambda0, max_iter, grad_tol, step_tol = case_params\n\n    theta = np.copy(theta0).astype(float)\n    lam = float(lambda0)\n\n    def forward_model(x_vals, p):\n        return p[0] * np.exp(p[1] * x_vals)\n\n    def jacobian(x_vals, p):\n        J = np.zeros((len(x_vals), 2))\n        exp_term = np.exp(p[1] * x_vals)\n        J[:, 0] = -exp_term\n        J[:, 1] = -p[0] * x_vals * exp_term\n        return J\n\n    for k in range(max_iter):\n        # 1. Compute residuals, objective function, and gradient\n        r = y - forward_model(x, theta)\n        phi = 0.5 * np.dot(r, r)\n        J = jacobian(x, theta)\n        g = np.dot(J.T, r)  # Gradient of the objective function\n\n        # 2. Check for convergence based on gradient norm\n        if np.linalg.norm(g, np.inf) = grad_tol:\n            return [phi, lam, k]\n\n        # 3. Form and solve the damped normal equations\n        A = np.dot(J.T, J)\n        I = np.identity(A.shape[0])\n        b = -g\n\n        try:\n            s = np.linalg.solve(A + lam * I, b)\n        except np.linalg.LinAlgError:\n            # If the matrix is singular even with damping, increase damping significantly\n            lam *= 10\n            continue \n\n        # 4. Check for convergence based on step size\n        if np.linalg.norm(s) = step_tol:\n            return [phi, lam, k]\n\n        # 5. Calculate predicted reduction and gain ratio\n        # Predicted reduction: delta_pred = -0.5 * s^T * g\n        pred_reduction = -0.5 * np.dot(s, g)\n\n        theta_new = theta + s\n        r_new = y - forward_model(x, theta_new)\n        phi_new = 0.5 * np.dot(r_new, r_new)\n        actual_reduction = phi - phi_new\n\n        # Handle potential division by zero if pred_reduction is numerically zero\n        if pred_reduction = np.finfo(float).eps:\n            # If predicted reduction is negligible, implies g or s is near zero,\n            # which should be caught by termination criteria.\n            # A negative rho indicates a failed step.\n            rho = -1.0\n        else:\n            rho = actual_reduction / pred_reduction\n\n        # 6. Update parameters (theta) based on acceptance rule\n        if rho > 0:\n            theta = theta_new\n\n        # 7. Update damping parameter (lambda) based on adaptive strategy\n        if rho > 0.75:\n            lam = lam / 2.0\n        elif rho  0.25: # This also covers the rejection case where rho = 0\n            lam = lam * 2.0\n            \n    # 8. Reached max iterations, return current state\n    final_r = y - forward_model(x, theta)\n    final_phi = 0.5 * np.dot(final_r, final_r)\n    return [final_phi, lam, max_iter]\n\n\ndef solve():\n    \"\"\"\n    Defines the test cases and runs the LM algorithm for each, printing the results.\n    \"\"\"\n    # Common parameters for all test cases\n    max_iter = 50\n    grad_tol = 1e-8\n    step_tol = 1e-10\n\n    # Test Case 1: Happy path with noisy data\n    x1 = np.array([0, 0.5, 1, 1.5, 2, 2.5, 3])\n    theta_star1 = np.array([2.0, -0.5])\n    epsilon1 = np.array([0.05, -0.02, 0.04, -0.03, 0.01, -0.02, 0.0])\n    y1 = theta_star1[0] * np.exp(theta_star1[1] * x1) + epsilon1\n    theta0_1 = np.array([1.0, 0.0])\n    lambda0_1 = 1e-2\n\n    # Test Case 2: Near-perfect fit with noiseless data\n    x2 = np.array([0, 0.5, 1, 1.5, 2, 2.5, 3])\n    theta_star2 = np.array([1.5, 0.2])\n    y2 = theta_star2[0] * np.exp(theta_star2[1] * x2)\n    theta0_2 = np.array([1.0, 0.0])\n    lambda0_2 = 1e-2\n\n    # Test Case 3: Ill-conditioned Jacobian\n    n3 = 6\n    x3 = np.zeros(n3)\n    theta_star3 = np.array([3.0, -1.0])\n    y3 = np.full(n3, theta_star3[0])\n    theta0_3 = np.array([1.0, 1.0])\n    lambda0_3 = 1e-2\n    \n    test_cases = [\n        (x1, y1, theta0_1, lambda0_1, max_iter, grad_tol, step_tol),\n        (x2, y2, theta0_2, lambda0_2, max_iter, grad_tol, step_tol),\n        (x3, y3, theta0_3, lambda0_3, max_iter, grad_tol, step_tol)\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_lm(case)\n        # Format numbers to avoid excessive precision in output string\n        result_formatted = [\n            float(f\"{result[0]:.6g}\"),\n            float(f\"{result[1]:.6g}\"),\n            int(result[2])\n        ]\n        results.append(result_formatted)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```"
        }
    ]
}