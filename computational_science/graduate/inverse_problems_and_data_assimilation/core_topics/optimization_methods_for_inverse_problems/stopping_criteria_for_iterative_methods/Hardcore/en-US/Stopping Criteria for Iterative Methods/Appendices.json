{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of regularization for inverse problems is the discrepancy principle, which provides a statistically motivated rule for when to stop iterating. The core idea is to halt the process as soon as the data misfit becomes comparable to the expected noise level, preventing the algorithm from fitting noise. This first practice guides you through implementing this principle for a linear inverse problem and critically examines its performance when the noise model is misspecified, a common challenge in practical applications .",
            "id": "3423231",
            "problem": "Consider the linear observation model in data assimilation, where the measurement vector $y \\in \\mathbb{R}^m$ is given by $y = A x^\\star + \\varepsilon$. The matrix $A \\in \\mathbb{R}^{m \\times n}$ is a known forward operator, $x^\\star \\in \\mathbb{R}^n$ is the unknown state, and $\\varepsilon \\in \\mathbb{R}^m$ is additive noise modeled as a zero-mean multivariate Gaussian with covariance $\\Gamma \\in \\mathbb{R}^{m \\times m}$, i.e., $\\varepsilon \\sim \\mathcal{N}(0, \\Gamma)$. The weighted residual at iteration $k$ is $r_k = y - A x_k$. Define the weighted residual statistic with an estimated covariance $\\Gamma_{\\text{est}}$ as $S_k(\\Gamma_{\\text{est}}) = r_k^\\top \\Gamma_{\\text{est}}^{-1} r_k$.\n\nUnder the hypothesis that $x_k$ adequately explains the data and $\\Gamma_{\\text{est}} = \\Gamma$, the statistic $S_k(\\Gamma_{\\text{est}})$ is approximately distributed as a chi-square random variable with $m$ degrees of freedom. A common discrepancy-based stopping rule is to terminate iterations at the earliest $k$ such that $S_k(\\Gamma_{\\text{est}})$ falls below the upper quantile of the chi-square distribution at probability $0.95$ with $m$ degrees of freedom. The aim is to compute the earliest iteration index where this occurs and to reason about the Type I and Type II error trade-offs introduced by using $\\Gamma_{\\text{est}}$ instead of $\\Gamma$.\n\nUse the following setup to make the computation concrete and reproducible:\n\n- Let $m = 500$ and $n = 50$.\n- Generate $A$ with entries drawn independently from a standard normal distribution and then normalize each column to have unit Euclidean norm.\n- Generate the true covariance $\\Gamma$ as a diagonal matrix with entries $\\Gamma_{ii} = 0.5 + 1.5 u_i$, where $u_i$ are independent draws from a uniform distribution on $[0,1]$.\n- Generate $x^\\star$ with entries drawn independently from a standard normal distribution.\n- Generate the noise vector $\\varepsilon$ as independent Gaussian entries with variances equal to the corresponding diagonal entries of $\\Gamma$.\n- Form $y = A x^\\star + \\varepsilon$.\n- Initialize $x_0 = 0$ and iterate using the covariance-weighted Landweber iteration\n  $$x_{k+1} = x_k + \\alpha A^\\top \\Gamma_{\\text{est}}^{-1} r_k,$$\n  where $r_k = y - A x_k$, and the step size $\\alpha$ is chosen as\n  $$\\alpha = \\frac{0.95}{\\lambda_{\\max}\\!\\left(A^\\top \\Gamma_{\\text{est}}^{-1} A\\right)},$$\n  with $\\lambda_{\\max}(\\cdot)$ denoting the largest eigenvalue. This choice ensures convergence for a convex quadratic objective with Lipschitz-continuous gradient.\n\n- Use the chi-square upper quantile at probability $0.95$ with $m$ degrees of freedom, denoted $q_{0.95}(m)$, as the stopping threshold:\n  $$q_{0.95}(m) = \\inf\\{q \\in \\mathbb{R} : \\mathbb{P}(\\chi^2_m \\le q) \\ge 0.95\\}.$$\n\n- The iterative method should run up to a maximum of $K_{\\max} = 200$ iterations. The earliest iteration index is the smallest $k \\in \\{0,1,2,\\dots,K_{\\max}\\}$ for which $S_k(\\Gamma_{\\text{est}}) \\le q_{0.95}(m)$. If no such $k$ exists by $K_{\\max}$, report $-1$.\n\nTo evaluate different facets of the stopping criterion, use a test suite that varies the estimated covariance through a scalar mis-specification factor $s$, defining $\\Gamma_{\\text{est}} = s \\Gamma$:\n\n- Test case $1$: $s = 1.0$ (correctly specified covariance, general case).\n- Test case $2$: $s = 2.0$ (over-dispersed estimated covariance, easier stopping; boundary of conservative stopping).\n- Test case $3$: $s = 0.5$ (under-dispersed estimated covariance; difficult stopping).\n- Test case $4$: $s = 0.25$ (severely under-dispersed estimated covariance; edge case where stopping may never occur within $K_{\\max}$).\n\nAll pseudo-random draws must be generated with a fixed seed equal to $314159$ to ensure reproducibility. There are no physical units involved in this problem. Angles and percentages do not appear in the inputs or outputs.\n\nYour program should compute, for each test case, the earliest iteration index $k$ for which $S_k(\\Gamma_{\\text{est}}) \\le q_{0.95}(m)$ holds or return $-1$ if not achieved by $K_{\\max}$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[k_1,k_2,k_3,k_4]$).",
            "solution": "### Principle-Based Design\nThe problem requires the implementation of an iterative algorithm to solve a linear inverse problem, with a stopping rule based on statistical properties of the residual. We will address this by first generating the synthetic data as specified, then implementing the iterative solver for each test case, and finally collecting the results.\n\n**1. Data Generation**\nFirst, we establish a reproducible pseudo-random environment using the specified seed. We generate the problem a single time, as it is common to all test cases.\n- The forward operator $A \\in \\mathbb{R}^{500 \\times 50}$ is generated with entries from a standard normal distribution. Its columns are then normalized. Normalizing columns is crucial as it standardizes the influence of each component of the state vector $x$ on the measurements $y$.\n- The true state vector $x^\\star \\in \\mathbb{R}^{50}$ is drawn from a standard normal distribution.\n- The true noise covariance matrix $\\Gamma \\in \\mathbb{R}^{500 \\times 500}$ is diagonal. Its diagonal entries $\\Gamma_{ii}$ are drawn from a scaled uniform distribution, ensuring heterogeneous noise variances across measurements, which is a realistic scenario.\n- The noise vector $\\varepsilon \\in \\mathbb{R}^{500}$ is generated from a multivariate normal distribution $\\mathcal{N}(0, \\Gamma)$. Since $\\Gamma$ is diagonal, this is equivalent to generating each component $\\varepsilon_i$ independently from $\\mathcal{N}(0, \\Gamma_{ii})$.\n- The measurement vector is then synthesized as $y = Ax^\\star + \\varepsilon$.\n\n**2. Iterative Solution and Stopping Criterion**\nThe problem uses the covariance-weighted Landweber iteration. This method is a gradient descent algorithm for minimizing the weighted least-squares functional $J(x) = \\frac{1}{2} (y - Ax)^\\top \\Gamma_{\\text{est}}^{-1} (y - Ax)$. The gradient of this functional is $\\nabla_x J(x) = -A^\\top \\Gamma_{\\text{est}}^{-1} (y - Ax)$. The gradient descent update is $x_{k+1} = x_k - \\alpha \\nabla_x J(x_k)$, which gives the specified iterative formula:\n$$ x_{k+1} = x_k + \\alpha A^\\top \\Gamma_{\\text{est}}^{-1} r_k $$\nwhere $r_k = y - A x_k$ is the residual at iteration $k$.\n\nThe step size $\\alpha$ is chosen to guarantee convergence. The gradient $\\nabla_x J(x)$ is Lipschitz-continuous with constant $L = \\lambda_{\\max}(A^\\top \\Gamma_{\\text{est}}^{-1} A)$. For convergence, the step size must satisfy $0  \\alpha  2/L$. The choice $\\alpha = 0.95/L$ falls within this range and provides a robust descent rate.\n\nThe stopping criterion is an implementation of the discrepancy principle. The statistic $S_k(\\Gamma_{\\text{est}}) = r_k^\\top \\Gamma_{\\text{est}}^{-1} r_k$ measures the squared Mahalanobis distance of the residual. Under the ideal hypothesis that the iterate $x_k$ has converged to a solution that explains the data perfectly up to the noise, i.e., $r_k \\approx \\varepsilon$, and that the estimated covariance is correct, i.e., $\\Gamma_{\\text{est}} = \\Gamma$, the quantity $r_k^\\top \\Gamma^{-1} r_k$ would approximate $\\varepsilon^\\top \\Gamma^{-1} \\varepsilon$. This latter quantity follows a chi-square distribution with $m$ degrees of freedom, $\\chi^2_m$. The stopping rule $S_k(\\Gamma_{\\text{est}}) \\le q_{0.95}(m)$ tests whether the weighted residual norm is statistically plausible given the noise model. We stop when the residual is small enough to be statistically indistinguishable from the assumed noise. The $95\\%$ quantile is used to provide a high-confidence bound.\n\n**3. Analysis of Covariance Mis-specification ($s$)**\nThe test cases explore the impact of mis-specifying the noise covariance magnitude.\n- **Case $s = 1.0$ ($\\Gamma_{\\text{est}} = \\Gamma$)**: This is the ideal case where the statistical model used for inversion matches reality. The stopping criterion is expected to perform as designed, halting the iteration when the residual is consistent with the true noise level.\n- **Case $s = 2.0$ ($\\Gamma_{\\text{est}} = 2\\Gamma$)**: Here, we overestimate the noise variance. This makes the inverse covariance $\\Gamma_{\\text{est}}^{-1} = \\frac{1}{2}\\Gamma^{-1}$ smaller. The stopping statistic $S_k$ becomes $S_k(2\\Gamma) = r_k^\\top (2\\Gamma)^{-1} r_k = \\frac{1}{2} r_k^\\top \\Gamma^{-1} r_k$. The criterion becomes easier to satisfy for a given residual $r_k$. This risks a Type I error: stopping prematurely before $x_k$ has converged sufficiently close to $x^\\star$, because the algorithm is too \"tolerant\" of large residuals.\n- **Case $s = 0.5$ ($\\Gamma_{\\text{est}} = 0.5\\Gamma$)**: Here, we underestimate the noise variance. The inverse covariance $\\Gamma_{\\text{est}}^{-1} = 2\\Gamma^{-1}$ is larger. The statistic becomes $S_k(0.5\\Gamma) = 2 r_k^\\top \\Gamma^{-1} r_k$. The criterion is now harder to satisfy. The algorithm becomes less \"tolerant\" of residuals and may continue iterating long after a reasonable solution is found, trying to fit the noise in the data. This risks a Type II error: failing to stop in a timely manner, which can lead to overfitting.\n- **Case $s = 0.25$ ($\\Gamma_{\\text{est}} = 0.25\\Gamma$)**: This is a severe underestimation of noise variance. The statistic $S_k(0.25\\Gamma) = 4 r_k^\\top \\Gamma^{-1} r_k$ is significantly inflated. It is plausible that the residual norm can never be reduced enough to satisfy the stopping criterion, especially since the true noise floor is $\\mathbb{E}[\\varepsilon^\\top\\Gamma^{-1}\\varepsilon] = m$. The stopping criterion would require $4 r_k^\\top \\Gamma^{-1} r_k \\le q_{0.95}(m)$, or $r_k^\\top \\Gamma^{-1} r_k \\le q_{0.95}(m)/4$. For $m=500$, $q_{0.95}(500) \\approx 545$. This requires the residual norm to be much smaller than its expected value, which may be unattainable.\n\n**4. Computational Algorithm**\nFor each value of $s$ in the test suite:\n1.  Set $\\Gamma_{\\text{est}} = s \\Gamma$. Since $\\Gamma$ is diagonal, $\\Gamma_{\\text{est}}$ is also diagonal and its inverse is trivially computed.\n2.  Compute the matrix $H = A^\\top \\Gamma_{\\text{est}}^{-1} A$. Since this matrix is symmetric, its largest eigenvalue $\\lambda_{\\max}(H)$ is computed efficiently.\n3.  Calculate the step size $\\alpha = 0.95 / \\lambda_{\\max}(H)$.\n4.  Determine the stopping threshold $q_{0.95}(m)$ from the $\\chi^2_m$ distribution.\n5.  Initialize $x_k = \\vec{0}$ and set a flag `found_k = -1`.\n6.  Begin the iteration from $k=0$ to $K_{\\max}=200$.\n7.  At each iteration $k$, check the stopping condition. If $S_k(\\Gamma_{\\text{est}}) \\le q_{0.95}(m)$, store $k$ as the result, and break the inner loop for this test case.\n8.  If the condition is not met, update the state $x_{k+1} = x_k + \\alpha A^\\top \\Gamma_{\\text{est}}^{-1} r_k$.\n9.  After the loop, append the found index (or $-1$ if not found) to the results list.\nThis procedure will be repeated for all four values of $s$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Solves the data assimilation problem with a chi-square stopping rule\n    for different covariance mis-specification factors.\n    \"\"\"\n    # Define parameters from the problem statement\n    m = 500\n    n = 50\n    K_max = 200\n    seed = 314159\n    \n    # Test cases for covariance mis-specification factor 's'\n    test_cases = [1.0, 2.0, 0.5, 0.25]\n\n    # --- 1. Data Generation (common for all test cases) ---\n    rng = np.random.default_rng(seed)\n    \n    # Generate A and normalize its columns\n    A = rng.normal(size=(m, n))\n    col_norms = np.linalg.norm(A, axis=0)\n    A = A / col_norms\n    \n    # Generate true state x_star\n    x_star = rng.normal(size=n)\n    \n    # Generate true diagonal covariance Gamma\n    u = rng.uniform(size=m)\n    gamma_diag = 0.5 + 1.5 * u\n    \n    # Generate noise epsilon from N(0, Gamma)\n    epsilon = rng.normal(loc=0.0, scale=np.sqrt(gamma_diag))\n    \n    # Form the measurement vector y\n    y = A @ x_star + epsilon\n    \n    # --- 2. Iteration and Stopping Criterion Evaluation ---\n    \n    # Calculate the chi-square stopping threshold\n    # q_{0.95}(m)\n    q_threshold = stats.chi2.ppf(0.95, df=m)\n    \n    results = []\n    \n    for s in test_cases:\n        # Initialize x_k for the current test case\n        x_k = np.zeros(n)\n        \n        # Define estimated covariance and its inverse\n        gamma_est_diag = s * gamma_diag\n        gamma_est_inv_diag = 1.0 / gamma_est_diag\n        \n        # Calculate the step size alpha\n        # H = A^T * Gamma_est^{-1} * A\n        # This is an efficient way to compute for diagonal Gamma_est^{-1}\n        # It's A.T @ (D * A) where D is the diagonal matrix\n        H = (A.T * gamma_est_inv_diag) @ A\n        \n        # The matrix H is symmetric, use eigvalsh for efficiency\n        lambda_max = np.linalg.eigvalsh(H)[-1]\n        alpha = 0.95 / lambda_max\n        \n        found_k = -1\n\n        for k in range(K_max + 1):\n            # Calculate residual r_k\n            r_k = y - A @ x_k\n            \n            # Calculate the stopping statistic S_k\n            # S_k = r_k^T * Gamma_est^{-1} * r_k\n            S_k = np.sum(r_k**2 * gamma_est_inv_diag)\n            \n            # Check the stopping criterion\n            if S_k = q_threshold:\n                found_k = k\n                break\n            \n            # If not stopping, perform the Landweber update\n            # We don't need to update if k == K_max, but it does no harm\n            grad = A.T @ (gamma_est_inv_diag * r_k)\n            x_k = x_k + alpha * grad\n\n        results.append(found_k)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While the discrepancy principle is powerful, real-world applications often involve nonlinear models and require additional safeguards against numerical stagnation. This exercise extends our understanding to the nonlinear domain by implementing a hybrid stopping strategy for a Gauss-Newton method. You will combine the statistical chi-square misfit test with a numerical convergence criterion based on step size, creating a robust rule that handles both statistical consistency and the algorithm's local convergence behavior .",
            "id": "3423267",
            "problem": "Consider a nonlinear parameter estimation problem in which $m$ observations $y_i$ are modeled as the output of a parametric forward model $g(x;t_i)$ corrupted by additive noise. Assume Independent and Identically Distributed (IID) Gaussian noise with mean $0$ and variance $\\sigma^2$, and define the residual vector $r(x) \\in \\mathbb{R}^m$ by $r_i(x) = y_i - g(x;t_i)$ for $i \\in \\{1,\\dots,m\\}$. Let the scaled sum of squares be $S(x) = \\|r(x)\\|_2^2 / \\sigma^2$. Under the stated assumptions, $S(x^\\star)$ evaluated at the true parameter $x^\\star$ follows a chi-square distribution with $m$ degrees of freedom. In iterative methods for inverse problems and data assimilation, one seeks a principled stopping rule that balances statistical consistency with numerical stability. Design a combined stopping criterion that uses both a chi-square misfit test and a small-step condition. Specifically, define a sequence of iterates $x_k \\in \\mathbb{R}^p$ ($p \\in \\mathbb{N}$) generated by a Gauss–Newton type update and stop at the smallest index $k$ such that either the chi-square misfit test passes or the step is sufficiently small. The misfit test uses the upper $(1-\\alpha)$-quantile $q_{m,1-\\alpha}$ of the chi-square distribution with $m$ degrees of freedom, and the small-step condition is $\\|x_{k+1} - x_k\\|_2 \\le \\varepsilon \\,(1 + \\|x_k\\|_2)$ for given $\\alpha \\in (0,1)$ and $\\varepsilon > 0$. If neither condition is met by a prescribed maximum iteration $k_{\\max}$, then return $k_{\\max}$.\n\nImplement this stopping rule for the following data assimilation scenario. Use the nonlinear forward model $g(x;t) = x_1 \\exp(-x_2 t)$ with parameter $x = (x_1,x_2) \\in \\mathbb{R}^2$. At iteration $k$, compute the Gauss–Newton step $s_k \\in \\mathbb{R}^2$ by solving the linear system $(J(x_k)^\\top J(x_k) + \\lambda I) s_k = - J(x_k)^\\top r(x_k)$, where $J(x_k) \\in \\mathbb{R}^{m \\times 2}$ is the Jacobian matrix of $g$ at $x_k$, $I \\in \\mathbb{R}^{2 \\times 2}$ is the identity matrix, and $\\lambda > 0$ is a fixed damping parameter. Update with $x_{k+1} = x_k + s_k$. The Euclidean norm $\\|\\cdot\\|_2$ must be used throughout.\n\nUse the following test suite of three cases to compute the stopping index, defined as the smallest $k \\in \\{0,1,\\dots,k_{\\max}\\}$ satisfying either $S(x_k) \\le q_{m,1-\\alpha}$ or $\\|x_{k+1} - x_k\\|_2 \\le \\varepsilon \\,(1 + \\|x_k\\|_2)$; if neither condition is met by $k_{\\max}$, output $k_{\\max}$. For each case, construct the observed data $y_i$ deterministically from a true parameter $x^\\mathrm{true}$ and a specified noise vector $v \\in \\mathbb{R}^m$ via $y_i = g(x^\\mathrm{true};t_i) + v_i$.\n\nCase $1$ (statistically consistent noise, misfit test expected to pass):\n- $m = 20$, $t_i$ linearly spaced in $[0,2]$ for $i \\in \\{1,\\dots,20\\}$,\n- $x^\\mathrm{true} = (1.0, 0.5)$,\n- $\\sigma = 0.05$, $v_i = \\sigma \\cdot 0.3 \\cdot (-1)^i$,\n- initial guess $x_0 = (0.8, 0.8)$,\n- $\\alpha = 0.05$, $\\varepsilon = 10^{-6}$, $k_{\\max} = 50$, $\\lambda = 10^{-4}$.\n\nCase $2$ (mismatched noise scale, small-step condition expected to trigger):\n- $m = 20$, $t_i$ linearly spaced in $[0,2]$ for $i \\in \\{1,\\dots,20\\}$,\n- $x^\\mathrm{true} = (1.0, 0.5)$,\n- $\\sigma = 0.02$, $v_i = 0.15 \\cos(2 t_i)$,\n- initial guess $x_0 = (0.9, 0.3)$,\n- $\\alpha = 0.05$, $\\varepsilon = 10^{-6}$, $k_{\\max} = 100$, $\\lambda = 10^{-3}$.\n\nCase $3$ (neither condition met by the iteration cap):\n- $m = 5$, $t_i$ linearly spaced in $[0,1]$ for $i \\in \\{1,\\dots,5\\}$,\n- $x^\\mathrm{true} = (1.0, 0.5)$,\n- $\\sigma = 0.01$, $v_i = 0.1 + 0.05 t_i$,\n- initial guess $x_0 = (0.2, 1.5)$,\n- $\\alpha = 0.01$, $\\varepsilon = 10^{-12}$, $k_{\\max} = 3$, $\\lambda = 10^{-3}$.\n\nYour program must compute the stopping index for each case using the stated combined rule and produce a single line of output containing the three integer indices as a comma-separated list enclosed in square brackets, for example, $[k_1,k_2,k_3]$. No physical units are involved, and all angles, if any, must be treated as real numbers without angle units. The final outputs are integers.",
            "solution": "The problem requires the implementation of a combined stopping rule for a damped Gauss-Newton iterative method applied to a nonlinear parameter estimation problem. The solution involves developing the algorithm based on the provided mathematical framework and executing it for three distinct test cases.\n\nFirst, we formalize the components of the iterative algorithm. The forward model is given by $g(x;t) = x_1 \\exp(-x_2 t)$, where $x = (x_1, x_2)$ is the parameter vector. The Jacobian matrix $J(x) \\in \\mathbb{R}^{m \\times 2}$ has entries corresponding to the partial derivatives of the model function with respect to the parameters, evaluated at each time point $t_i$. The columns of the Jacobian are:\n$$\n\\frac{\\partial g}{\\partial x_1}(x;t) = \\exp(-x_2 t)\n$$\n$$\n\\frac{\\partial g}{\\partial x_2}(x;t) = -x_1 t \\exp(-x_2 t)\n$$\nThe residual vector for a given parameter estimate $x$ is $r(x)$, with components $r_i(x) = y_i - g(x; t_i)$, where $y_i$ are the observed data. The quantity $S(x) = \\|r(x)\\|_2^2 / \\sigma^2$ is the scaled sum of squared residuals, which serves as our misfit function.\n\nThe iterative procedure starts with an initial guess $x_0$ and generates a sequence of parameter estimates $x_k$ for $k=0, 1, 2, \\dots$. At each iteration $k$, the next iterate $x_{k+1}$ is found by computing a step $s_k$ and setting $x_{k+1} = x_k + s_k$. The step $s_k$ is obtained by solving the damped normal equations, a characteristic of the Levenberg-Marquardt algorithm (a modification of the Gauss-Newton method):\n$$\n(J(x_k)^\\top J(x_k) + \\lambda I) s_k = - J(x_k)^\\top r(x_k)\n$$\nHere, $J(x_k)$ is the Jacobian evaluated at $x_k$, $r(x_k)$ is the residual at $x_k$, $I$ is the $2 \\times 2$ identity matrix, and $\\lambda > 0$ is a damping parameter that ensures the system is well-conditioned and promotes stability.\n\nThe core of the problem is the combined stopping criterion. The iteration halts at the first index $k$ (starting from $k=0$) that satisfies one of two conditions:\n1.  **Chi-square Misfit Test**: $S(x_k) \\le q_{m,1-\\alpha}$. This is a form of Morozov's discrepancy principle. It stops the iteration when the model's prediction error (the misfit) is statistically consistent with the expected noise level in the data. The threshold $q_{m,1-\\alpha}$ is the $(1-\\alpha)$ quantile of the chi-square distribution with $m$ degrees of freedom, which is the distribution of $S(x^\\star)$ at the true parameter $x^\\star$ under the problem's assumptions.\n2.  **Small-Step Condition**: $\\|x_{k+1} - x_k\\|_2 \\le \\varepsilon \\,(1 + \\|x_k\\|_2)$. This condition signals that the algorithm has converged to a stationary point, as subsequent iterations produce negligible changes in the parameter vector. The scaling factor $(1 + \\|x_k\\|_2)$ provides a relative tolerance that is effective for both small and large parameter norms.\n\nThe overall algorithm proceeds as follows for $k = 0, 1, \\dots, k_{\\max}-1$:\n1.  Given the current iterate $x_k$, calculate the residual $r(x_k)$ and the misfit $S(x_k) = \\|r(x_k)\\|_2^2 / \\sigma^2$.\n2.  Check the misfit test: If $S(x_k) \\le q_{m,1-\\alpha}$, the stopping index is $k$. Terminate.\n3.  If the misfit test fails, compute the Gauss-Newton step $s_k$ by solving the linear system described above.\n4.  Check the small-step condition: If $\\|s_k\\|_2 \\le \\varepsilon(1 + \\|x_k\\|_2)$, the stopping index is $k$. Terminate.\n5.  If both tests fail, update the parameter estimate: $x_{k+1} = x_k + s_k$. Proceed to the next iteration with $k \\leftarrow k+1$.\nIf the loop completes up to $k=k_{\\max}-1$ without either condition being met, the process stops, and the returned index is $k_{\\max}$.\n\nThis algorithm is applied to each of the three test cases specified, using their respective data and control parameters, to determine the stopping index in each scenario.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Solves for the stopping index in three data assimilation scenarios using a combined\n    stopping criterion for a damped Gauss-Newton iteration.\n    \"\"\"\n\n    def g(x, t):\n        \"\"\"\n        The nonlinear forward model g(x;t) = x1 * exp(-x2 * t).\n\n        Args:\n            x (np.ndarray): Parameter vector [x1, x2].\n            t (np.ndarray): Time points.\n\n        Returns:\n            np.ndarray: Model output.\n        \"\"\"\n        x1, x2 = x\n        return x1 * np.exp(-x2 * t)\n\n    def jacobian(x, t):\n        \"\"\"\n        Computes the Jacobian matrix of the forward model g(x;t).\n\n        Args:\n            x (np.ndarray): Parameter vector [x1, x2].\n            t (np.ndarray): Time points.\n\n        Returns:\n            np.ndarray: The m x 2 Jacobian matrix.\n        \"\"\"\n        x1, x2 = x\n        m = len(t)\n        J = np.zeros((m, 2))\n        exp_term = np.exp(-x2 * t)\n        J[:, 0] = exp_term\n        J[:, 1] = -x1 * t * exp_term\n        return J\n\n    def run_iteration(params):\n        \"\"\"\n        Executes the iterative algorithm for a single test case.\n        \"\"\"\n        m, t, x_true, sigma, v, x0, alpha, epsilon, k_max, lam = params\n\n        # Generate observed data y\n        y = g(x_true, t) + v\n\n        # Calculate the chi-square quantile for the misfit test\n        q_chi2 = chi2.ppf(1 - alpha, df=m)\n\n        # Identity matrix for the damped system\n        I = np.identity(2)\n\n        # Initialize the iteration\n        x_k = np.array(x0, dtype=float)\n        \n        for k in range(k_max):\n            # Compute residual and scaled sum of squares (misfit) at x_k\n            r_k = y - g(x_k, t)\n            S_k = np.linalg.norm(r_k)**2 / sigma**2\n\n            # 1. Chi-square Misfit Test\n            if S_k = q_chi2:\n                return k\n\n            # Compute the Jacobian at x_k\n            J_k = jacobian(x_k, t)\n\n            # Formulate and solve the damped linear system for the step s_k\n            # (J^T J + lambda*I) s_k = -J^T r_k\n            A = J_k.T @ J_k + lam * I\n            b = -J_k.T @ r_k\n            s_k = np.linalg.solve(A, b)\n\n            # 2. Small-Step Condition\n            step_norm = np.linalg.norm(s_k)\n            x_k_norm = np.linalg.norm(x_k)\n            threshold = epsilon * (1.0 + x_k_norm)\n            \n            if step_norm = threshold:\n                return k\n\n            # Update the parameter estimate for the next iteration\n            x_k += s_k\n            \n        # If loop completes without stopping, return k_max\n        return k_max\n\n    # Define the test cases from the problem statement.\n    case1_t = np.linspace(0, 2, 20)\n    case1_v = 0.05 * 0.3 * ((-1)**np.arange(1, 21))\n    \n    case2_t = np.linspace(0, 2, 20)\n    case2_v = 0.15 * np.cos(2 * case2_t)\n\n    case3_t = np.linspace(0, 1, 5)\n    case3_v = 0.1 + 0.05 * case3_t\n\n    test_cases = [\n        # Case 1\n        (20, case1_t, np.array([1.0, 0.5]), 0.05, case1_v, \n         np.array([0.8, 0.8]), 0.05, 1e-6, 50, 1e-4),\n        # Case 2\n        (20, case2_t, np.array([1.0, 0.5]), 0.02, case2_v, \n         np.array([0.9, 0.3]), 0.05, 1e-6, 100, 1e-3),\n        # Case 3\n        (5, case3_t, np.array([1.0, 0.5]), 0.01, case3_v,\n         np.array([0.2, 1.5]), 0.01, 1e-12, 3, 1e-3),\n    ]\n\n    results = []\n    for case in test_cases:\n        stop_index = run_iteration(case)\n        results.append(stop_index)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "What happens when a reliable estimate of the noise level is unavailable, rendering the discrepancy principle impractical? In such cases, we can turn to heuristic methods that monitor the geometry of the convergence path itself. This practice introduces a powerful technique for detecting semi-convergence—the point where an iteration transitions from fitting the signal to amplifying noise—by analyzing the curvature of the log-residual norm sequence, offering a valuable tool for regularization without explicit noise statistics .",
            "id": "3423234",
            "problem": "Consider the linear inverse problem of recovering an unknown vector $x^\\dagger \\in \\mathbb{R}^n$ from noisy data $y \\in \\mathbb{R}^n$ satisfying $y = A x^\\dagger + e$, where $A : \\mathbb{R}^n \\to \\mathbb{R}^n$ is a compact linear operator with smoothing action, and $e$ is a mean-zero random noise vector. Iterative gradient methods are known to exhibit semi-convergence: the reconstruction error $\\|x_k - x^\\dagger\\|_2$ initially decreases with iteration index $k$ and then increases as iterations begin to fit noise. A practical stopping criterion is required to terminate the iteration near the onset of semi-convergence. You will design and evaluate such a criterion that detects the onset by tracking the discrete curvature of the logarithm of the residual norm versus iteration index.\n \nStart from the following foundational elements, which are valid in linear inverse problems and data assimilation:\n- The Landweber iteration for minimizing the quadratic data misfit $\\tfrac{1}{2}\\|A x - y\\|_2^2$ updates $x_{k+1} = x_k + \\alpha A^\\top (y - A x_k)$ with step size $\\alpha \\in (0, 2/\\|A\\|_2^2)$, where $\\|\\cdot\\|_2$ denotes the spectral norm for operators and the Euclidean norm for vectors, and $A^\\top$ is the adjoint.\n- The residual at iteration $k$ is $r_k = y - A x_k$, and its Euclidean norm is $\\|r_k\\|_2$.\n- The discrete second finite difference of a sequence $s_k$ is $\\Delta^2 s_k = s_{k-1} - 2 s_k + s_{k+1}$, which captures the curvature of $s_k$ with respect to the discrete index $k$.\n\nYou will implement and assess a semi-convergence detector based on the curvature of $s_k = \\log \\|r_k\\|_2$:\n- Compute $s_k = \\log(\\|r_k\\|_2)$ for $k = 0, 1, \\dots, K_{\\max}$, with a safeguard so that the logarithm argument is strictly positive.\n- Compute the centered discrete curvature $c_k = \\Delta^2 s_k = s_{k-1} - 2 s_k + s_{k+1}$ for $k = 1, \\dots, K_{\\max} - 1$.\n- Smooth $c_k$ by a moving average of window length $w$ to form $\\tilde{c}_k$.\n- Estimate a baseline mean $\\mu_0$ and standard deviation $\\sigma_0$ from the first $K_{\\mathrm{burn}}$ curvature samples of $\\tilde{c}_k$.\n- Define a curvature-threshold detector: the detection index $k_{\\mathrm{det}}$ is the smallest $k \\in \\{K_{\\mathrm{burn}}, \\dots, K_{\\max}-2\\}$ such that $\\tilde{c}_k > \\mu_0 + \\gamma \\sigma_0$ and $\\tilde{c}_k > 0$. If no such $k$ exists, set $k_{\\mathrm{det}} = K_{\\max} + 1$ (meaning no detection).\n\nFor the forward operator $A$, use a periodic one-dimensional convolution $A x = h \\ast x$ with a symmetric Gaussian kernel $h$ of length $n$ and standard deviation $\\sigma_h$, normalized so that $\\sum_{i=1}^n h_i = 1$. Implement $A$ and $A^\\top$ via the Discrete Fourier Transform using circular convolution so that $A^\\top = A$ due to symmetry. Choose the Landweber step size $\\alpha = \\frac{1.9}{\\|A\\|_2^2}$, where $\\|A\\|_2$ equals the maximum modulus of the discrete Fourier transform of $h$ under periodic boundary conditions. Initialize $x_0 = 0$ and run $K_{\\max}$ iterations.\n\nConstruct a nontrivial ground truth $x^\\dagger$ consisting of a mix of features (for example, a block, a Gaussian bump, and a few spikes), ensuring $x^\\dagger \\in \\mathbb{R}^n$ is not identically zero. Generate $y^\\dagger = A x^\\dagger$ and corrupt it with noise $e$ to obtain $y = y^\\dagger + e$.\n\nNoise models to be considered:\n- White Gaussian noise: $e \\sim \\mathcal{N}(0, \\sigma^2 I)$.\n- Colored noise with first-order autoregressive correlation (autoregressive of order one (AR(1))): across the spatial index $i = 1, \\dots, n$, construct a stationary process satisfying $e_i = \\rho e_{i-1} + \\eta_i$ with $\\eta_i \\sim \\mathcal{N}(0, \\tau^2)$ independent and identically distributed, where $|\\rho|  1$ and $\\tau$ is chosen so that $\\mathrm{Var}(e_i)$ attains a specified scale. Finally, rescale the realized $e$ to enforce a prescribed relative noise level $\\varepsilon = \\|e\\|_2 / \\|y^\\dagger\\|_2$.\n- The zero-noise boundary case: set $\\varepsilon = 0$ so $y = y^\\dagger$.\n\nFor each noisy instance, define the optimal iteration index $k_{\\mathrm{opt}}$ as the minimizer of the reconstruction error $\\|x_k - x^\\dagger\\|_2$ over $k \\in \\{0, 1, \\dots, K_{\\max}\\}$. A detection is called a false positive if $k_{\\mathrm{det}}  k_{\\mathrm{opt}} - \\delta$, where $\\delta$ is a nonnegative tolerance in iterations.\n\nTasks to implement:\n- Implement the Landweber iteration, the residual curvature-based detector, and the noise generators as specified above.\n- For each test case below, conduct $N_{\\mathrm{trials}}$ independent trials and estimate the false positive rate as the fraction of trials for which a false positive occurs. Also compute, across those trials where a detection occurs (i.e., $k_{\\mathrm{det}} \\le K_{\\max}$), the mean signed detection offset $\\overline{\\Delta k}$ defined as the arithmetic mean of $k_{\\mathrm{det}} - k_{\\mathrm{opt}}$ over detected trials. If no trial has a detection, define $\\overline{\\Delta k} = 0$.\n\nTest suite and parameters:\n- Use $n = 128$, $\\sigma_h = 2.0$, $K_{\\max} = 150$, window length $w = 5$, burn-in $K_{\\mathrm{burn}} = 10$, threshold parameter $\\gamma = 3.0$, tolerance $\\delta = 2$, and $N_{\\mathrm{trials}} = 20$. Use the Euclidean norm and natural logarithm.\n- Test case $1$ (happy path): white noise with relative level $\\varepsilon = 0.02$.\n- Test case $2$ (colored noise): AR(1) with parameter $\\rho = 0.8$ and relative level $\\varepsilon = 0.02$.\n- Test case $3$ (strongly colored noise): AR(1) with parameter $\\rho = 0.95$ and relative level $\\varepsilon = 0.02$.\n- Test case $4$ (boundary): zero noise with $\\varepsilon = 0$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order and with no spaces: $[\\mathrm{fp}_1,\\overline{\\Delta k}_1,\\mathrm{fp}_2,\\overline{\\Delta k}_2,\\mathrm{fp}_3,\\overline{\\Delta k}_3,\\mathrm{fp}_4,\\overline{\\Delta k}_4]$, where $\\mathrm{fp}_j$ is the estimated false positive rate as a decimal for test case $j$ and $\\overline{\\Delta k}_j$ is the mean signed detection offset for test case $j$ as defined above.\n- No physical units are involved in this problem. Angles do not appear. All probabilities or rates must be expressed as decimals, not percentages.\n\nYour program must be fully deterministic and self-contained, using fixed pseudo-random number generator seeds of your choice to ensure reproducibility across runs.",
            "solution": "The core of the problem is to regularize the solution of the ill-posed linear system $y = A x^\\dagger + e$ by means of early termination of an iterative scheme. The chosen iterative method is the Landweber iteration, a form of gradient descent applied to minimize the data-misfit functional $J(x) = \\frac{1}{2}\\|A x - y\\|_2^2$. The gradient of $J(x)$ is $\\nabla J(x) = A^\\top(A x - y)$, leading to the update rule:\n$$\nx_{k+1} = x_k - \\alpha \\nabla J(x_k) = x_k + \\alpha A^\\top (y - A x_k)\n$$\nwhere $k$ is the iteration index and $\\alpha$ is the step size. For convergence, the step size must satisfy $\\alpha \\in (0, 2/\\|A\\|_2^2)$, where $\\|A\\|_2$ is the spectral norm of the operator $A$. The problem specifies a step size of $\\alpha = 1.9/\\|A\\|_2^2$.\n\nA key phenomenon in the iterative solution of ill-posed problems with noisy data is semi-convergence. The reconstruction error, $\\|x_k - x^\\dagger\\|_2$, initially decreases as the iterates $x_k$ approximate the true solution $x^\\dagger$. However, after a certain number of iterations, the iterates begin to fit the noise component $e$ in the data $y$. Because the operator $A$ has a smoothing action, its inverse (or pseudo-inverse) is an amplifying one. This amplification applies to the noise, causing the error $\\|x_k - x^\\dagger\\|_2$ to grow, a phenomenon known as noise amplification. The optimal iteration index, $k_{\\mathrm{opt}}$, is the one that minimizes this error: $k_{\\mathrm{opt}} = \\arg\\min_k \\|x_k - x^\\dagger\\|_2$. The goal of a stopping criterion is to estimate $k_{\\mathrm{opt}}$ without knowledge of the true solution $x^\\dagger$.\n\nThe proposed stopping criterion is based on the behavior of the residual norm $\\|r_k\\|_2 = \\|y - A x_k\\|_2$. As the iteration progresses, the residual norm decreases. The rate of this decrease changes as the iteration transitions from fitting the signal to fitting the noise. The criterion formalizes this by monitoring the curvature of the sequence $s_k = \\log \\|r_k\\|_2$. A large positive curvature suggests that the logarithmic residual decay is slowing down abruptly, which is taken as a heuristic indicator for the onset of noise amplification.\n\nThe implementation plan is as follows:\n\n1.  **Operator Construction**: The forward operator $A$ is a one-dimensional circular convolution with a symmetric Gaussian kernel $h$. According to the convolution theorem, the Fourier transform of a convolution is the product of the Fourier transforms. Thus, the action of $A$ can be efficiently computed using the Fast Fourier Transform (FFT): $A x = \\mathcal{F}^{-1}(\\hat{h} \\cdot \\hat{x})$, where $\\mathcal{F}$ denotes the FFT and $\\hat{h}, \\hat{x}$ are the Fourier representations of $h$ and $x$, respectively. Because the kernel $h$ is real and symmetric, the operator $A$ is self-adjoint, i.e., $A^\\top = A$. The spectral norm $\\|A\\|_2$ is the largest singular value of $A$. For a circulant matrix defined by convolution, the singular values are the magnitudes of the eigenvalues, which are the entries of the transfer function $\\hat{h}$. Therefore, $\\|A\\|_2 = \\max_j |\\hat{h}_j|$.\n\n2.  **Simulation Loop**: For each test case, we perform $N_{\\mathrm{trials}}=20$ independent simulations. In each trial:\n    a. A specific noise vector $e$ is generated according to the test case (white, AR(1), or zero) and scaled to ensure the relative noise level $\\varepsilon = \\|e\\|_2 / \\|y^\\dagger\\|_2$, where $y^\\dagger = A x^\\dagger$. The AR(1) noise is generated by filtering a white noise sequence, with a burn-in period removed to ensure stationarity.\n    b. The Landweber iteration is run for $K_{\\max}=150$ steps, starting from $x_0 = 0$. At each step $k$, we store the reconstruction error $\\|x_k - x^\\dagger\\|_2$ and the residual norm $\\|r_k\\|_2$.\n    c. From the stored errors, the optimal iteration index $k_{\\mathrm{opt}}$ is found.\n    d. The stopping criterion is applied to find a detection index $k_{\\mathrm{det}}$. This involves computing the sequence $s_k = \\log(\\|r_k\\|_2 + \\epsilon_{\\text{mach}})$, where $\\epsilon_{\\text{mach}}$ is a small machine epsilon to prevent $\\log(0)$. Then, the discrete curvature $c_k = s_{k-1} - 2s_k + s_{k+1}$ is computed for $k=1, \\dots, K_{\\max}-1$. This sequence is smoothed with a moving average of window size $w=5$ to yield $\\tilde{c}_k$. Baseline statistics, mean $\\mu_0$ and standard deviation $\\sigma_0$, are computed from the first $K_{\\mathrm{burn}}=10$ samples of $\\{\\tilde{c}_k\\}$. The detection index $k_{\\mathrm{det}}$ is the first $k \\ge K_{\\mathrm{burn}}$ for which $\\tilde{c}_k > \\mu_0 + \\gamma \\sigma_0$ and $\\tilde{c}_k > 0$. If no such $k$ is found, $k_{\\mathrm{det}} = K_{\\max}+1$.\n\n3.  **Evaluation**: After all trials for a test case are complete, two metrics are calculated:\n    a. The false positive rate ($\\mathrm{fp}$): the fraction of trials where the detection occurred prematurely, i.e., $k_{\\mathrm{det}}  k_{\\mathrm{opt}} - \\delta$, with tolerance $\\delta=2$.\n    b. The mean signed detection offset ($\\overline{\\Delta k}$): the average of the difference $k_{\\mathrm{det}} - k_{\\mathrm{opt}}$ over all trials in which a detection occurred ($k_{\\mathrm{det}} \\le K_{\\max}$). If no detections occur, this is $0$.\n\nThe final code will implement this logic in a structured, deterministic manner, using a fixed seed for the pseudo-random number generator to ensure reproducibility.",
            "answer": "```python\nimport numpy as np\nfrom scipy.signal import lfilter\n\ndef solve():\n    \"\"\"\n    Main function to run the entire simulation as specified in the problem.\n    \"\"\"\n\n    # --- Global parameters ---\n    PARAMS = {\n        'n': 128,              # Signal dimension\n        'sigma_h': 2.0,        # Gaussian kernel standard deviation\n        'K_max': 150,          # Maximum number of iterations\n        'w': 5,                # Moving average window length\n        'K_burn': 10,          # Burn-in period for statistics\n        'gamma': 3.0,          # Threshold parameter\n        'delta': 2,            # False positive tolerance\n        'N_trials': 20,        # Number of trials per test case\n        'rng_seed': 42         # Seed for reproducibility\n    }\n\n    # --- Test case definitions ---\n    TEST_CASES = [\n        {'noise_type': 'white', 'epsilon': 0.02, 'rho': None},\n        {'noise_type': 'ar1', 'epsilon': 0.02, 'rho': 0.8},\n        {'noise_type': 'ar1', 'epsilon': 0.02, 'rho': 0.95},\n        {'noise_type': 'zero', 'epsilon': 0.0, 'rho': None},\n    ]\n\n    def create_operator(n, sigma_h):\n        \"\"\"Creates the convolution operator A, its adjoint AT, and its norm.\"\"\"\n        x_grid = np.arange(n)\n        kernel = np.exp(-((x_grid - n // 2)**2) / (2 * sigma_h**2))\n        kernel = np.roll(kernel, -n // 2)\n        kernel /= np.sum(kernel)\n        h_hat = np.fft.fft(kernel)\n        A_norm = np.max(np.abs(h_hat))\n        \n        def A(x):\n            return np.fft.ifft(h_hat * np.fft.fft(x)).real\n        \n        AT = A  # Operator is self-adjoint\n        return A, AT, A_norm\n\n    def create_ground_truth(n):\n        \"\"\"Creates a nontrivial ground truth signal x_dagger.\"\"\"\n        x_dagger = np.zeros(n)\n        x_dagger[n//6:n//3] = 1.0\n        x_grid = np.arange(n)\n        x_dagger += 1.5 * np.exp(-((x_grid - n // 2)**2) / (2 * 5.0**2))\n        x_dagger[int(n * 0.7)] = -1.0\n        x_dagger[int(n * 0.8)] = 0.8\n        return x_dagger\n\n    def generate_noise(noise_type, y_dagger, epsilon, n, rho, rng):\n        \"\"\"Generates noise vector e with a prescribed relative norm.\"\"\"\n        if noise_type == 'zero' or epsilon == 0:\n            return np.zeros(n)\n\n        y_dagger_norm = np.linalg.norm(y_dagger)\n        target_noise_norm = epsilon * y_dagger_norm\n        if y_dagger_norm == 0:\n            return np.zeros(n)\n\n        if noise_type == 'white':\n            e_unscaled = rng.standard_normal(n)\n        elif noise_type == 'ar1':\n            burn_in = 2 * n\n            eta = rng.standard_normal(n + burn_in)\n            ar_process_full = lfilter([1.0], [1.0, -rho], eta)\n            e_unscaled = ar_process_full[burn_in:]\n        else:\n            raise ValueError(f\"Unknown noise type: {noise_type}\")\n\n        e_norm = np.linalg.norm(e_unscaled)\n        e = e_unscaled * (target_noise_norm / e_norm) if e_norm  0 else np.zeros(n)\n        return e\n\n    def run_single_trial(A, AT, A_norm, x_dagger, case_params, common_params, rng):\n        \"\"\"Runs one full simulation trial and returns k_opt and k_det.\"\"\"\n        # Unpack parameters\n        n, K_max, w = common_params['n'], common_params['K_max'], common_params['w']\n        K_burn, gamma = common_params['K_burn'], common_params['gamma']\n        \n        # 1. Generate data\n        y_dagger = A(x_dagger)\n        e = generate_noise(case_params['noise_type'], y_dagger, case_params['epsilon'], n, case_params['rho'], rng)\n        y = y_dagger + e\n\n        # 2. Run Landweber iteration\n        alpha = 1.9 / (A_norm**2)\n        x_k = np.zeros(n)\n        errors = []\n        residuals = []\n        for _ in range(K_max + 1):\n            errors.append(np.linalg.norm(x_k - x_dagger))\n            r_k = y - A(x_k)\n            residuals.append(np.linalg.norm(r_k))\n            x_k = x_k + alpha * AT(r_k)\n\n        # 3. Find optimal stopping index\n        k_opt = np.argmin(errors)\n\n        # 4. Implement curvature detector\n        log_res_arr = np.log(np.array(residuals) + np.finfo(float).eps)\n        curvatures = log_res_arr[:-2] - 2 * log_res_arr[1:-1] + log_res_arr[2:]\n\n        smoothed_curvatures = np.zeros_like(curvatures)\n        for i in range(len(curvatures)):\n            start = max(0, i - (w - 1) // 2)\n            end = min(len(curvatures), i + w // 2 + 1)\n            smoothed_curvatures[i] = np.mean(curvatures[start:end])\n        \n        if K_burn  0 and len( smoothed_curvatures) = K_burn:\n            mu0 = np.mean(smoothed_curvatures[:K_burn])\n            sigma0 = np.std(smoothed_curvatures[:K_burn])\n        else:\n            mu0, sigma0 = 0.0, 0.0\n\n        k_det = K_max + 1\n        for k in range(K_burn, K_max - 1): # k is in {K_burn, ..., K_max-2}\n            c_tilde_k = smoothed_curvatures[k - 1]\n            threshold = mu0 + gamma * sigma0 if sigma0  0 else mu0\n            if c_tilde_k  threshold and c_tilde_k  0:\n                k_det = k\n                break\n        return k_opt, k_det\n\n    # --- Main execution logic ---\n    rng = np.random.default_rng(PARAMS['rng_seed'])\n    A, AT, A_norm = create_operator(PARAMS['n'], PARAMS['sigma_h'])\n    x_dagger = create_ground_truth(PARAMS['n'])\n\n    all_results = []\n    \n    for case in TEST_CASES:\n        false_positives = 0\n        detection_offsets = []\n        \n        for _ in range(PARAMS['N_trials']):\n            k_opt, k_det = run_single_trial(A, AT, A_norm, x_dagger, case, PARAMS, rng)\n            \n            if k_det  k_opt - PARAMS['delta']:\n                false_positives += 1\n            \n            if k_det = PARAMS['K_max']:\n                detection_offsets.append(k_det - k_opt)\n        \n        fp_rate = false_positives / PARAMS['N_trials']\n        mean_offset = np.mean(detection_offsets) if detection_offsets else 0.0\n            \n        all_results.extend([fp_rate, mean_offset])\n        \n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```"
        }
    ]
}