{
    "hands_on_practices": [
        {
            "introduction": "A cornerstone of regularization for inverse problems is the discrepancy principle, which stops an iteration once the residual is statistically consistent with the known noise level. This practice provides a hands-on implementation of this principle using a chi-square test for a linear inverse problem solved with the Landweber method. By systematically mis-specifying the noise covariance, you will directly investigate the method's sensitivity to model errors and gain crucial insight into the trade-offs between premature stopping (a Type I error) and overfitting the noise (a Type II error) .",
            "id": "3423231",
            "problem": "Consider the linear observation model in data assimilation, where the measurement vector $y \\in \\mathbb{R}^m$ is given by $y = A x^\\star + \\varepsilon$. The matrix $A \\in \\mathbb{R}^{m \\times n}$ is a known forward operator, $x^\\star \\in \\mathbb{R}^n$ is the unknown state, and $\\varepsilon \\in \\mathbb{R}^m$ is additive noise modeled as a zero-mean multivariate Gaussian with covariance $\\Gamma \\in \\mathbb{R}^{m \\times m}$, i.e., $\\varepsilon \\sim \\mathcal{N}(0, \\Gamma)$. The weighted residual at iteration $k$ is $r_k = y - A x_k$. Define the weighted residual statistic with an estimated covariance $\\Gamma_{\\text{est}}$ as $S_k(\\Gamma_{\\text{est}}) = r_k^\\top \\Gamma_{\\text{est}}^{-1} r_k$.\n\nUnder the hypothesis that $x_k$ adequately explains the data and $\\Gamma_{\\text{est}} = \\Gamma$, the statistic $S_k(\\Gamma_{\\text{est}})$ is approximately distributed as a chi-square random variable with $m$ degrees of freedom. A common discrepancy-based stopping rule is to terminate iterations at the earliest $k$ such that $S_k(\\Gamma_{\\text{est}})$ falls below the upper quantile of the chi-square distribution at probability $0.95$ with $m$ degrees of freedom. The aim is to compute the earliest iteration index where this occurs and to reason about the Type I and Type II error trade-offs introduced by using $\\Gamma_{\\text{est}}$ instead of $\\Gamma$.\n\nUse the following setup to make the computation concrete and reproducible:\n\n- Let $m = 500$ and $n = 50$.\n- Generate $A$ with entries drawn independently from a standard normal distribution and then normalize each column to have unit Euclidean norm.\n- Generate the true covariance $\\Gamma$ as a diagonal matrix with entries $\\Gamma_{ii} = 0.5 + 1.5 u_i$, where $u_i$ are independent draws from a uniform distribution on $[0,1]$.\n- Generate $x^\\star$ with entries drawn independently from a standard normal distribution.\n- Generate the noise vector $\\varepsilon$ as independent Gaussian entries with variances equal to the corresponding diagonal entries of $\\Gamma$.\n- Form $y = A x^\\star + \\varepsilon$.\n- Initialize $x_0 = 0$ and iterate using the covariance-weighted Landweber iteration\n  $$x_{k+1} = x_k + \\alpha A^\\top \\Gamma_{\\text{est}}^{-1} r_k,$$\n  where $r_k = y - A x_k$, and the step size $\\alpha$ is chosen as\n  $$\\alpha = \\frac{0.95}{\\lambda_{\\max}\\!\\left(A^\\top \\Gamma_{\\text{est}}^{-1} A\\right)},$$\n  with $\\lambda_{\\max}(\\cdot)$ denoting the largest eigenvalue. This choice ensures convergence for a convex quadratic objective with Lipschitz-continuous gradient.\n\n- Use the chi-square upper quantile at probability $0.95$ with $m$ degrees of freedom, denoted $q_{0.95}(m)$, as the stopping threshold:\n  $$q_{0.95}(m) = \\inf\\{q \\in \\mathbb{R} : \\mathbb{P}(\\chi^2_m \\le q) \\ge 0.95\\}.$$\n\n- The iterative method should run up to a maximum of $K_{\\max} = 200$ iterations. The earliest iteration index is the smallest $k \\in \\{0,1,2,\\dots,K_{\\max}\\}$ for which $S_k(\\Gamma_{\\text{est}}) \\le q_{0.95}(m)$. If no such $k$ exists by $K_{\\max}$, report $-1$.\n\nTo evaluate different facets of the stopping criterion, use a test suite that varies the estimated covariance through a scalar mis-specification factor $s$, defining $\\Gamma_{\\text{est}} = s \\Gamma$:\n\n- Test case $1$: $s = 1.0$ (correctly specified covariance, general case).\n- Test case $2$: $s = 2.0$ (over-dispersed estimated covariance, easier stopping; boundary of conservative stopping).\n- Test case $3$: $s = 0.5$ (under-dispersed estimated covariance; difficult stopping).\n- Test case $4$: $s = 0.25$ (severely under-dispersed estimated covariance; edge case where stopping may never occur within $K_{\\max}$).\n\nAll pseudo-random draws must be generated with a fixed seed equal to $314159$ to ensure reproducibility. There are no physical units involved in this problem. Angles and percentages do not appear in the inputs or outputs.\n\nYour program should compute, for each test case, the earliest iteration index $k$ for which $S_k(\\Gamma_{\\text{est}}) \\le q_{0.95}(m)$ holds or return $-1$ if not achieved by $K_{\\max}$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[k_1,k_2,k_3,k_4]$).",
            "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Model**: Linear observation model $y = A x^\\star + \\varepsilon$.\n- **Dimensions**: $m = 500$ (measurement space), $n = 50$ (state space).\n- **Forward Operator**: $A \\in \\mathbb{R}^{m \\times n}$, generated with entries from $\\mathcal{N}(0, 1)$ and columns normalized to unit Euclidean norm.\n- **True State**: $x^\\star \\in \\mathbb{R}^n$, with entries from $\\mathcal{N}(0, 1)$.\n- **Noise Model**: $\\varepsilon \\in \\mathbb{R}^m$, with $\\varepsilon \\sim \\mathcal{N}(0, \\Gamma)$.\n- **True Noise Covariance**: $\\Gamma \\in \\mathbb{R}^{m \\times m}$ is diagonal, with $\\Gamma_{ii} = 0.5 + 1.5 u_i$ where $u_i \\sim U[0, 1]$.\n- **Data Vector**: $y = A x^\\star + \\varepsilon$.\n- **Iteration**: Covariance-weighted Landweber iteration:\n  $$x_{k+1} = x_k + \\alpha A^\\top \\Gamma_{\\text{est}}^{-1} r_k$$\n- **Initial State**: $x_0 = 0$.\n- **Residual**: $r_k = y - A x_k$.\n- **Estimated Covariance**: $\\Gamma_{\\text{est}} = s \\Gamma$ for a scalar mis-specification factor $s$.\n- **Step Size**: $\\alpha = \\frac{0.95}{\\lambda_{\\max}(A^\\top \\Gamma_{\\text{est}}^{-1} A)}$.\n- **Stopping Statistic**: $S_k(\\Gamma_{\\text{est}}) = r_k^\\top \\Gamma_{\\text{est}}^{-1} r_k$.\n- **Stopping Rule**: Terminate at the smallest $k \\in \\{0, 1, \\dots, K_{\\max}\\}$ such that $S_k(\\Gamma_{\\text{est}}) \\le q_{0.95}(m)$.\n- **Stopping Threshold**: $q_{0.95}(m)$ is the upper quantile of the $\\chi^2_m$ distribution at probability $0.95$.\n- **Maximum Iterations**: $K_{\\max} = 200$.\n- **Return Value on Failure**: If the condition is not met by $K_{\\max}$, return $-1$.\n- **Random Seed**: $314159$.\n- **Test Cases**:\n    - Case 1: $s = 1.0$\n    - Case 2: $s = 2.0$\n    - Case 3: $s = 0.5$\n    - Case 4: $s = 0.25$\n- **Output Format**: A comma-separated list of the earliest iteration indices $[k_1, k_2, k_3, k_4]$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Soundness**: The problem is scientifically and mathematically sound. It employs a standard linear inverse problem formulation, a well-known iterative solver (Landweber iteration, a form of gradient descent), and a widely used statistical stopping rule (the discrepancy principle based on a chi-square test). The choice of step size is theoretically motivated to ensure convergence.\n2.  **Non-Formalizable or Irrelevant**: The problem is directly formalizable and is central to the topic of stopping criteria for iterative methods in inverse problems and data assimilation.\n3.  **Incomplete or Contradictory Setup**: The problem is fully specified. All parameters, constants, and data generation procedures are defined. There are no contradictions.\n4.  **Unrealistic or Infeasible**: The setup is a common and realistic simulation for testing numerical methods. The specified matrix and vector generation is computationally feasible.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. The use of a fixed random seed ensures a single, deterministic, and reproducible solution. The maximum iteration count prevents infinite execution.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem is non-trivial. It requires the implementation of a numerical algorithm and an understanding of the interplay between the iterative solver, statistical error models, and stopping rules. The analysis of covariance mis-specification is a core concept in the field.\n7.  **Outside Scientific Verifiability**: The problem is entirely verifiable through mathematical computation.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be provided.\n\n### Principle-Based Design\nThe problem requires the implementation of an iterative algorithm to solve a linear inverse problem, with a stopping rule based on statistical properties of the residual. We will address this by first generating the synthetic data as specified, then implementing the iterative solver for each test case, and finally collecting the results.\n\n**1. Data Generation**\nFirst, we establish a reproducible pseudo-random environment using the specified seed. We generate the problem a single time, as it is common to all test cases.\n- The forward operator $A \\in \\mathbb{R}^{500 \\times 50}$ is generated with entries from a standard normal distribution. Its columns are then normalized. Normalizing columns is crucial as it standardizes the influence of each component of the state vector $x$ on the measurements $y$.\n- The true state vector $x^\\star \\in \\mathbb{R}^{50}$ is drawn from a standard normal distribution.\n- The true noise covariance matrix $\\Gamma \\in \\mathbb{R}^{500 \\times 500}$ is diagonal. Its diagonal entries $\\Gamma_{ii}$ are drawn from a scaled uniform distribution, ensuring heterogeneous noise variances across measurements, which is a realistic scenario.\n- The noise vector $\\varepsilon \\in \\mathbb{R}^{500}$ is generated from a multivariate normal distribution $\\mathcal{N}(0, \\Gamma)$. Since $\\Gamma$ is diagonal, this is equivalent to generating each component $\\varepsilon_i$ independently from $\\mathcal{N}(0, \\Gamma_{ii})$.\n- The measurement vector is then synthesized as $y = Ax^\\star + \\varepsilon$.\n\n**2. Iterative Solution and Stopping Criterion**\nThe problem uses the covariance-weighted Landweber iteration. This method is a gradient descent algorithm for minimizing the weighted least-squares functional $J(x) = \\frac{1}{2} (y - Ax)^\\top \\Gamma_{\\text{est}}^{-1} (y - Ax)$. The gradient of this functional is $\\nabla_x J(x) = -A^\\top \\Gamma_{\\text{est}}^{-1} (y - Ax)$. The gradient descent update is $x_{k+1} = x_k - \\alpha \\nabla_x J(x_k)$, which gives the specified iterative formula:\n$$ x_{k+1} = x_k + \\alpha A^\\top \\Gamma_{\\text{est}}^{-1} r_k $$\nwhere $r_k = y - A x_k$ is the residual at iteration $k$.\n\nThe step size $\\alpha$ is chosen to guarantee convergence. The gradient $\\nabla_x J(x)$ is Lipschitz-continuous with constant $L = \\lambda_{\\max}(A^\\top \\Gamma_{\\text{est}}^{-1} A)$. For convergence, the step size must satisfy $0 < \\alpha < 2/L$. The choice $\\alpha = 0.95/L$ falls within this range and provides a robust descent rate.\n\nThe stopping criterion is an implementation of the discrepancy principle. The statistic $S_k(\\Gamma_{\\text{est}}) = r_k^\\top \\Gamma_{\\text{est}}^{-1} r_k$ measures the squared Mahalanobis distance of the residual. Under the ideal hypothesis that the iterate $x_k$ has converged to a solution that explains the data perfectly up to the noise, i.e., $r_k \\approx \\varepsilon$, and that the estimated covariance is correct, i.e., $\\Gamma_{\\text{est}} = \\Gamma$, the quantity $r_k^\\top \\Gamma^{-1} r_k$ would approximate $\\varepsilon^\\top \\Gamma^{-1} \\varepsilon$. This latter quantity follows a chi-square distribution with $m$ degrees of freedom, $\\chi^2_m$. The stopping rule $S_k(\\Gamma_{\\text{est}}) \\le q_{0.95}(m)$ tests whether the weighted residual norm is statistically plausible given the noise model. We stop when the residual is small enough to be statistically indistinguishable from the assumed noise. The $95\\%$ quantile is used to provide a high-confidence bound.\n\n**3. Analysis of Covariance Mis-specification ($s$)**\nThe test cases explore the impact of mis-specifying the noise covariance magnitude.\n- **Case $s = 1.0$ ($\\Gamma_{\\text{est}} = \\Gamma$)**: This is the ideal case where the statistical model used for inversion matches reality. The stopping criterion is expected to perform as designed, halting the iteration when the residual is consistent with the true noise level.\n- **Case $s = 2.0$ ($\\Gamma_{\\text{est}} = 2\\Gamma$)**: Here, we overestimate the noise variance. This makes the inverse covariance $\\Gamma_{\\text{est}}^{-1} = \\frac{1}{2}\\Gamma^{-1}$ smaller. The stopping statistic $S_k$ becomes $S_k(2\\Gamma) = r_k^\\top (2\\Gamma)^{-1} r_k = \\frac{1}{2} r_k^\\top \\Gamma^{-1} r_k$. The criterion becomes easier to satisfy for a given residual $r_k$. This risks a Type I error: stopping prematurely before $x_k$ has converged sufficiently close to $x^\\star$, because the algorithm is too \"tolerant\" of large residuals.\n- **Case $s = 0.5$ ($\\Gamma_{\\text{est}} = 0.5\\Gamma$)**: Here, we underestimate the noise variance. The inverse covariance $\\Gamma_{\\text{est}}^{-1} = 2\\Gamma^{-1}$ is larger. The statistic becomes $S_k(0.5\\Gamma) = 2 r_k^\\top \\Gamma^{-1} r_k$. The criterion is now harder to satisfy. The algorithm becomes less \"tolerant\" of residuals and may continue iterating long after a reasonable solution is found, trying to fit the noise in the data. This risks a Type II error: failing to stop in a timely manner, which can lead to overfitting.\n- **Case $s = 0.25$ ($\\Gamma_{\\text{est}} = 0.25\\Gamma$)**: This is a severe underestimation of noise variance. The statistic $S_k(0.25\\Gamma) = 4 r_k^\\top \\Gamma^{-1} r_k$ is significantly inflated. It is plausible that the residual norm can never be reduced enough to satisfy the stopping criterion, especially since the true noise floor is $\\mathbb{E}[\\varepsilon^\\top\\Gamma^{-1}\\varepsilon] = m$. The stopping criterion would require $4 r_k^\\top \\Gamma^{-1} r_k \\le q_{0.95}(m)$, or $r_k^\\top \\Gamma^{-1} r_k \\le q_{0.95}(m)/4$. For $m=500$, $q_{0.95}(500) \\approx 545$. This requires the residual norm to be much smaller than its expected value, which may be unattainable.\n\n**4. Computational Algorithm**\nFor each value of $s$ in the test suite:\n1.  Set $\\Gamma_{\\text{est}} = s \\Gamma$. Since $\\Gamma$ is diagonal, $\\Gamma_{\\text{est}}$ is also diagonal and its inverse is trivially computed.\n2.  Compute the matrix $H = A^\\top \\Gamma_{\\text{est}}^{-1} A$. Since this matrix is symmetric, its largest eigenvalue $\\lambda_{\\max}(H)$ is computed efficiently.\n3.  Calculate the step size $\\alpha = 0.95 / \\lambda_{\\max}(H)$.\n4.  Determine the stopping threshold $q_{0.95}(m)$ from the $\\chi^2_m$ distribution.\n5.  Initialize $x_k = \\vec{0}$ and set a flag `found_k = -1`.\n6.  Begin the iteration from $k=0$ to $K_{\\max}=200$.\n7.  At each iteration $k$, check the stopping condition. If $S_k(\\Gamma_{\\text{est}}) \\le q_{0.95}(m)$, store $k$ as the result, and break the inner loop for this test case.\n8.  If the condition is not met, update the state $x_{k+1} = x_k + \\alpha A^\\top \\Gamma_{\\text{est}}^{-1} r_k$.\n9.  After the loop, append the found index (or $-1$ if not found) to the results list.\nThis procedure will be repeated for all four values of $s$.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Solves the data assimilation problem with a chi-square stopping rule\n    for different covariance mis-specification factors.\n    \"\"\"\n    # Define parameters from the problem statement\n    m = 500\n    n = 50\n    K_max = 200\n    seed = 314159\n    \n    # Test cases for covariance mis-specification factor 's'\n    test_cases = [1.0, 2.0, 0.5, 0.25]\n\n    # --- 1. Data Generation (common for all test cases) ---\n    rng = np.random.default_rng(seed)\n    \n    # Generate A and normalize its columns\n    A = rng.normal(size=(m, n))\n    col_norms = np.linalg.norm(A, axis=0)\n    A = A / col_norms\n    \n    # Generate true state x_star\n    x_star = rng.normal(size=n)\n    \n    # Generate true diagonal covariance Gamma\n    u = rng.uniform(size=m)\n    gamma_diag = 0.5 + 1.5 * u\n    \n    # Generate noise epsilon from N(0, Gamma)\n    epsilon = rng.normal(loc=0.0, scale=np.sqrt(gamma_diag))\n    \n    # Form the measurement vector y\n    y = A @ x_star + epsilon\n    \n    # --- 2. Iteration and Stopping Criterion Evaluation ---\n    \n    # Calculate the chi-square stopping threshold\n    # q_{0.95}(m)\n    q_threshold = stats.chi2.ppf(0.95, df=m)\n    \n    results = []\n    \n    for s in test_cases:\n        # Initialize x_k for the current test case\n        x_k = np.zeros(n)\n        \n        # Define estimated covariance and its inverse\n        gamma_est_diag = s * gamma_diag\n        gamma_est_inv_diag = 1.0 / gamma_est_diag\n        \n        # Calculate the step size alpha\n        # H = A^T * Gamma_est^{-1} * A\n        # This is an efficient way to compute for diagonal Gamma_est^{-1}\n        # It's A.T @ (D * A) where D is the diagonal matrix\n        H = (A.T * gamma_est_inv_diag) @ A\n        \n        # The matrix H is symmetric, use eigvalsh for efficiency\n        lambda_max = np.linalg.eigvalsh(H)[-1]\n        alpha = 0.95 / lambda_max\n        \n        found_k = -1\n\n        for k in range(K_max + 1):\n            # Calculate residual r_k\n            r_k = y - A @ x_k\n            \n            # Calculate the stopping statistic S_k\n            # S_k = r_k^T * Gamma_est^{-1} * r_k\n            S_k = np.sum(r_k**2 * gamma_est_inv_diag)\n            \n            # Check the stopping criterion\n            if S_k = q_threshold:\n                found_k = k\n                break\n            \n            # If not stopping, perform the Landweber update\n            # We don't need to update if k == K_max, but it does no harm\n            grad = A.T @ (gamma_est_inv_diag * r_k)\n            x_k = x_k + alpha * grad\n\n        results.append(found_k)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "While statistical criteria are powerful, real-world applications often benefit from more robust strategies, especially for nonlinear problems. This exercise challenges you to implement a hybrid stopping rule that combines the statistical rigor of a chi-square misfit test with the numerical stability of a small-step convergence condition. By applying this combined criterion to a nonlinear parameter estimation problem solved with a Gauss-Newton method, you will develop a practical skill set for terminating iterations in complex data assimilation scenarios where either statistical consistency or numerical convergence may be the limiting factor .",
            "id": "3423267",
            "problem": "Consider a nonlinear parameter estimation problem in which $m$ observations $y_i$ are modeled as the output of a parametric forward model $g(x;t_i)$ corrupted by additive noise. Assume Independent and Identically Distributed (IID) Gaussian noise with mean $0$ and variance $\\sigma^2$, and define the residual vector $r(x) \\in \\mathbb{R}^m$ by $r_i(x) = y_i - g(x;t_i)$ for $i \\in \\{1,\\dots,m\\}$. Let the scaled sum of squares be $S(x) = \\|r(x)\\|_2^2 / \\sigma^2$. Under the stated assumptions, $S(x^\\star)$ evaluated at the true parameter $x^\\star$ follows a chi-square distribution with $m$ degrees of freedom. In iterative methods for inverse problems and data assimilation, one seeks a principled stopping rule that balances statistical consistency with numerical stability. Design a combined stopping criterion that uses both a chi-square misfit test and a small-step condition. Specifically, define a sequence of iterates $x_k \\in \\mathbb{R}^p$ ($p \\in \\mathbb{N}$) generated by a Gauss–Newton type update and stop at the smallest index $k$ such that either the chi-square misfit test passes or the step is sufficiently small. The misfit test uses the upper $(1-\\alpha)$-quantile $q_{m,1-\\alpha}$ of the chi-square distribution with $m$ degrees of freedom, and the small-step condition is $\\|x_{k+1} - x_k\\|_2 \\le \\varepsilon \\,(1 + \\|x_k\\|_2)$ for given $\\alpha \\in (0,1)$ and $\\varepsilon  0$. If neither condition is met by a prescribed maximum iteration $k_{\\max}$, then return $k_{\\max}$.\n\nImplement this stopping rule for the following data assimilation scenario. Use the nonlinear forward model $g(x;t) = x_1 \\exp(-x_2 t)$ with parameter $x = (x_1,x_2) \\in \\mathbb{R}^2$. At iteration $k$, compute the Gauss–Newton step $s_k \\in \\mathbb{R}^2$ by solving the linear system $(J(x_k)^\\top J(x_k) + \\lambda I) s_k = - J(x_k)^\\top r(x_k)$, where $J(x_k) \\in \\mathbb{R}^{m \\times 2}$ is the Jacobian matrix of $g$ at $x_k$, $I \\in \\mathbb{R}^{2 \\times 2}$ is the identity matrix, and $\\lambda  0$ is a fixed damping parameter. Update with $x_{k+1} = x_k + s_k$. The Euclidean norm $\\|\\cdot\\|_2$ must be used throughout.\n\nUse the following test suite of three cases to compute the stopping index, defined as the smallest $k \\in \\{0,1,\\dots,k_{\\max}\\}$ satisfying either $S(x_k) \\le q_{m,1-\\alpha}$ or $\\|x_{k+1} - x_k\\|_2 \\le \\varepsilon \\,(1 + \\|x_k\\|_2)$; if neither condition is met by $k_{\\max}$, output $k_{\\max}$. For each case, construct the observed data $y_i$ deterministically from a true parameter $x^\\mathrm{true}$ and a specified noise vector $v \\in \\mathbb{R}^m$ via $y_i = g(x^\\mathrm{true};t_i) + v_i$.\n\nCase $1$ (statistically consistent noise, misfit test expected to pass):\n- $m = 20$, $t_i$ linearly spaced in $[0,2]$ for $i \\in \\{1,\\dots,20\\}$,\n- $x^\\mathrm{true} = (1.0, 0.5)$,\n- $\\sigma = 0.05$, $v_i = \\sigma \\cdot 0.3 \\cdot (-1)^i$,\n- initial guess $x_0 = (0.8, 0.8)$,\n- $\\alpha = 0.05$, $\\varepsilon = 10^{-6}$, $k_{\\max} = 50$, $\\lambda = 10^{-4}$.\n\nCase $2$ (mismatched noise scale, small-step condition expected to trigger):\n- $m = 20$, $t_i$ linearly spaced in $[0,2]$ for $i \\in \\{1,\\dots,20\\}$,\n- $x^\\mathrm{true} = (1.0, 0.5)$,\n- $\\sigma = 0.02$, $v_i = 0.15 \\cos(2 t_i)$,\n- initial guess $x_0 = (0.9, 0.3)$,\n- $\\alpha = 0.05$, $\\varepsilon = 10^{-6}$, $k_{\\max} = 100$, $\\lambda = 10^{-3}$.\n\nCase $3$ (neither condition met by the iteration cap):\n- $m = 5$, $t_i$ linearly spaced in $[0,1]$ for $i \\in \\{1,\\dots,5\\}$,\n- $x^\\mathrm{true} = (1.0, 0.5)$,\n- $\\sigma = 0.01$, $v_i = 0.1 + 0.05 t_i$,\n- initial guess $x_0 = (0.2, 1.5)$,\n- $\\alpha = 0.01$, $\\varepsilon = 10^{-12}$, $k_{\\max} = 3$, $\\lambda = 10^{-3}$.\n\nYour program must compute the stopping index for each case using the stated combined rule and produce a single line of output containing the three integer indices as a comma-separated list enclosed in square brackets, for example, $[k_1,k_2,k_3]$. No physical units are involved, and all angles, if any, must be treated as real numbers without angle units. The final outputs are integers.",
            "solution": "### Problem Validation\nThe problem is scientifically and mathematically sound. It details a standard nonlinear inverse problem using a damped Gauss-Newton (Levenberg-Marquardt) method. The core task involves implementing a hybrid stopping criterion, combining a statistically motivated chi-square misfit test with a numerically motivated small-step condition. All parameters, equations, and test cases are specified completely and without contradiction, making the problem well-posed and algorithmically verifiable. The setup is a realistic and non-trivial numerical experiment common in scientific computing and data assimilation. The problem is deemed **valid**.\n\n### Principle-Based Design\nThe solution requires implementing a combined stopping rule for a damped Gauss-Newton iterative method applied to a nonlinear parameter estimation problem. The solution involves developing the algorithm based on the provided mathematical framework and executing it for three distinct test cases.\n\nFirst, we formalize the components of the iterative algorithm. The forward model is given by $g(x;t) = x_1 \\exp(-x_2 t)$, where $x = (x_1, x_2)$ is the parameter vector. The Jacobian matrix $J(x) \\in \\mathbb{R}^{m \\times 2}$ has entries corresponding to the partial derivatives of the model function with respect to the parameters, evaluated at each time point $t_i$. The columns of the Jacobian are:\n$$\n\\frac{\\partial g}{\\partial x_1}(x;t) = \\exp(-x_2 t)\n$$\n$$\n\\frac{\\partial g}{\\partial x_2}(x;t) = -x_1 t \\exp(-x_2 t)\n$$\nThe residual vector for a given parameter estimate $x$ is $r(x)$, with components $r_i(x) = y_i - g(x; t_i)$, where $y_i$ are the observed data. The quantity $S(x) = \\|r(x)\\|_2^2 / \\sigma^2$ is the scaled sum of squared residuals, which serves as our misfit function.\n\nThe iterative procedure starts with an initial guess $x_0$ and generates a sequence of parameter estimates $x_k$ for $k=0, 1, 2, \\dots$. At each iteration $k$, the next iterate $x_{k+1}$ is found by computing a step $s_k$ and setting $x_{k+1} = x_k + s_k$. The step $s_k$ is obtained by solving the damped normal equations, a characteristic of the Levenberg-Marquardt algorithm (a modification of the Gauss-Newton method):\n$$\n(J(x_k)^\\top J(x_k) + \\lambda I) s_k = - J(x_k)^\\top r(x_k)\n$$\nHere, $J(x_k)$ is the Jacobian evaluated at $x_k$, $r(x_k)$ is the residual at $x_k$, $I$ is the $2 \\times 2$ identity matrix, and $\\lambda  0$ is a damping parameter that ensures the system is well-conditioned and promotes stability.\n\nThe core of the problem is the combined stopping criterion. The iteration halts at the first index $k$ (starting from $k=0$) that satisfies one of two conditions:\n1.  **Chi-square Misfit Test**: $S(x_k) \\le q_{m,1-\\alpha}$. This is a form of Morozov's discrepancy principle. It stops the iteration when the model's prediction error (the misfit) is statistically consistent with the expected noise level in the data. The threshold $q_{m,1-\\alpha}$ is the $(1-\\alpha)$ quantile of the chi-square distribution with $m$ degrees of freedom, which is the distribution of $S(x^\\star)$ at the true parameter $x^\\star$ under the problem's assumptions.\n2.  **Small-Step Condition**: $\\|x_{k+1} - x_k\\|_2 \\le \\varepsilon \\,(1 + \\|x_k\\|_2)$. This condition signals that the algorithm has converged to a stationary point, as subsequent iterations produce negligible changes in the parameter vector. The scaling factor $(1 + \\|x_k\\|_2)$ provides a relative tolerance that is effective for both small and large parameter norms.\n\nThe overall algorithm proceeds as follows for $k = 0, 1, \\dots, k_{\\max}-1$:\n1.  Given the current iterate $x_k$, calculate the residual $r(x_k)$ and the misfit $S(x_k) = \\|r(x_k)\\|_2^2 / \\sigma^2$.\n2.  Check the misfit test: If $S(x_k) \\le q_{m,1-\\alpha}$, the stopping index is $k$. Terminate.\n3.  If the misfit test fails, compute the Gauss-Newton step $s_k$ by solving the linear system described above.\n4.  Check the small-step condition: If $\\|s_k\\|_2 \\le \\varepsilon(1 + \\|x_k\\|_2)$, the stopping index is $k$. Terminate.\n5.  If both tests fail, update the parameter estimate: $x_{k+1} = x_k + s_k$. Proceed to the next iteration with $k \\leftarrow k+1$.\nIf the loop completes up to $k=k_{\\max}-1$ without either condition being met, the process stops, and the returned index is $k_{\\max}$.\n\nThis algorithm is applied to each of the three test cases specified, using their respective data and control parameters, to determine the stopping index in each scenario.",
            "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef solve():\n    \"\"\"\n    Solves for the stopping index in three data assimilation scenarios using a combined\n    stopping criterion for a damped Gauss-Newton iteration.\n    \"\"\"\n\n    def g(x, t):\n        \"\"\"\n        The nonlinear forward model g(x;t) = x1 * exp(-x2 * t).\n\n        Args:\n            x (np.ndarray): Parameter vector [x1, x2].\n            t (np.ndarray): Time points.\n\n        Returns:\n            np.ndarray: Model output.\n        \"\"\"\n        x1, x2 = x\n        return x1 * np.exp(-x2 * t)\n\n    def jacobian(x, t):\n        \"\"\"\n        Computes the Jacobian matrix of the forward model g(x;t).\n\n        Args:\n            x (np.ndarray): Parameter vector [x1, x2].\n            t (np.ndarray): Time points.\n\n        Returns:\n            np.ndarray: The m x 2 Jacobian matrix.\n        \"\"\"\n        x1, x2 = x\n        m = len(t)\n        J = np.zeros((m, 2))\n        exp_term = np.exp(-x2 * t)\n        J[:, 0] = exp_term\n        J[:, 1] = -x1 * t * exp_term\n        return J\n\n    def run_iteration(params):\n        \"\"\"\n        Executes the iterative algorithm for a single test case.\n        \"\"\"\n        m, t, x_true, sigma, v, x0, alpha, epsilon, k_max, lam = params\n\n        # Generate observed data y\n        y = g(x_true, t) + v\n\n        # Calculate the chi-square quantile for the misfit test\n        q_chi2 = chi2.ppf(1 - alpha, df=m)\n\n        # Identity matrix for the damped system\n        I = np.identity(2)\n\n        # Initialize the iteration\n        x_k = np.array(x0, dtype=float)\n        \n        for k in range(k_max):\n            # Compute residual and scaled sum of squares (misfit) at x_k\n            r_k = y - g(x_k, t)\n            S_k = np.linalg.norm(r_k)**2 / sigma**2\n\n            # 1. Chi-square Misfit Test\n            if S_k = q_chi2:\n                return k\n\n            # Compute the Jacobian at x_k\n            J_k = jacobian(x_k, t)\n\n            # Formulate and solve the damped linear system for the step s_k\n            # (J^T J + lambda*I) s_k = -J^T r_k\n            A = J_k.T @ J_k + lam * I\n            b = -J_k.T @ r_k\n            s_k = np.linalg.solve(A, b)\n\n            # 2. Small-Step Condition\n            step_norm = np.linalg.norm(s_k)\n            x_k_norm = np.linalg.norm(x_k)\n            threshold = epsilon * (1.0 + x_k_norm)\n            \n            if step_norm = threshold:\n                return k\n\n            # Update the parameter estimate for the next iteration\n            x_k += s_k\n            \n        # If loop completes without stopping, return k_max\n        return k_max\n\n    # Define the test cases from the problem statement.\n    case1_t = np.linspace(0, 2, 20)\n    case1_v = 0.05 * 0.3 * ((-1)**np.arange(1, 21))\n    \n    case2_t = np.linspace(0, 2, 20)\n    case2_v = 0.15 * np.cos(2 * case2_t)\n\n    case3_t = np.linspace(0, 1, 5)\n    case3_v = 0.1 + 0.05 * case3_t\n\n    test_cases = [\n        # Case 1\n        (20, case1_t, np.array([1.0, 0.5]), 0.05, case1_v, \n         np.array([0.8, 0.8]), 0.05, 1e-6, 50, 1e-4),\n        # Case 2\n        (20, case2_t, np.array([1.0, 0.5]), 0.02, case2_v, \n         np.array([0.9, 0.3]), 0.05, 1e-6, 100, 1e-3),\n        # Case 3\n        (5, case3_t, np.array([1.0, 0.5]), 0.01, case3_v,\n         np.array([0.2, 1.5]), 0.01, 1e-12, 3, 1e-3),\n    ]\n\n    results = []\n    for case in test_cases:\n        stop_index = run_iteration(case)\n        results.append(stop_index)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "This practice introduces a powerful and theoretically elegant alternative to discrepancy-based methods known as the Lepskii balancing principle. Instead of comparing the residual to a fixed noise threshold, this rule works by comparing the solutions themselves across a range of regularization parameters to find the optimal point of balance between bias and variance. Implementing this a-posteriori rule for Tikhonov regularization will equip you with an advanced technique for parameter selection that adapts to the specific problem structure and avoids direct reliance on precise noise level knowledge .",
            "id": "3423230",
            "problem": "Consider a linear inverse problem in a finite-dimensional real Hilbert space with Euclidean norm. Let $A \\in \\mathbb{R}^{m \\times n}$ be a known forward operator, $x^{\\ast} \\in \\mathbb{R}^{n}$ be the unknown true state, and noisy data $b^{\\delta} \\in \\mathbb{R}^{m}$ obey the additive noise model $b^{\\delta} = A x^{\\ast} + \\eta$ with $\\|\\eta\\|_{2} \\leq \\delta$, where $\\delta \\geq 0$ is the known noise level. For a sequence of positive regularization parameters $\\{\\alpha_{j}\\}_{j=0}^{J-1}$ with $\\alpha_{j}  0$, define the Tikhonov-regularized iterates $x_{j} \\in \\mathbb{R}^{n}$ by the unique minimizers of the quadratic functional (equivalently by the normal equations) such that $x_{j}$ solves\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\|A x - b^{\\delta}\\|_{2}^{2} + \\alpha_{j} \\|x\\|_{2}^{2},\n$$\nwhich is equivalently given by\n$$\n(A^{\\top} A + \\alpha_{j} I) x_{j} = A^{\\top} b^{\\delta}.\n$$\nA Lepskii-type stopping rule selects the smallest index $k$ such that, for all $j  k$, the stabilization condition\n$$\n\\|x_{k} - x_{j}\\|_{2} \\leq c \\, \\theta(j)\n$$\nholds, where $c \\geq 1$ is a given safety factor, and $\\theta(j)$ is a noise-dependent bound that depends on the regularization parameter $\\alpha_{j}$ and the known noise level $\\delta$. Your task is to compute an admissible, tight bound $\\theta(j)$ based on the spectral characterization of the Tikhonov solution operator, and then implement the Lepskii-type rule to determine the stopping index $k$.\n\nBase your bound $\\theta(j)$ on the following principle: for each $\\alpha_{j}$, the Tikhonov solution mapping from $b^{\\delta}$ to $x_{j}$ is linear, and the perturbation induced by the noise $\\eta$ is controlled by the operator norm of the linear filter that maps $\\eta$ to the solution perturbation at $\\alpha_{j}$. Use this operator norm together with the known noise level $\\delta$ to construct $\\theta(j)$, in a way that does not rely on unknown quantities. The norm throughout is the Euclidean norm, and all matrix norms are the spectral norm (operator $2$-norm).\n\nImplement the following algorithmic steps:\n- For each test case, construct $b^{\\delta} = A x^{\\ast} + \\eta$ from the provided $A$, $x^{\\ast}$, and $\\eta$.\n- For each $j$ in $\\{0,1,\\dots,J-1\\}$, compute $x_{j}$ by solving $(A^{\\top} A + \\alpha_{j} I) x_{j} = A^{\\top} b^{\\delta}$.\n- For each $j$, compute an admissible bound $\\theta(j)$ from the spectral characterization of the Tikhonov filter at $\\alpha_{j}$ and the noise level $\\delta$.\n- Find the smallest $k$ such that for all $j  k$, $\\|x_{k} - x_{j}\\|_{2} \\leq c \\, \\theta(j)$. If no such $k$ exists, set $k := J-1$.\n\nUse the Euclidean norm $\\|\\cdot\\|_{2}$ for all vector computations. The singular value decomposition is permitted for spectral norm calculations.\n\nTest suite:\n- Test $1$:\n  - $A = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  0.1 \\end{bmatrix}$,\n  - $x^{\\ast} = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$,\n  - $\\eta = \\begin{bmatrix} 0.01 \\\\ 0.0 \\end{bmatrix}$, so $\\delta = 0.01$,\n  - $\\{\\alpha_{j}\\}_{j=0}^{5} = \\{1.0,\\,0.5,\\,0.25,\\,0.125,\\,0.0625,\\,0.03125\\}$,\n  - $c = 1.2$.\n- Test $2$ (boundary case with exact data):\n  - $A = \\begin{bmatrix} 2.0  0.0 \\\\ 0.0  0.5 \\end{bmatrix}$,\n  - $x^{\\ast} = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$,\n  - $\\eta = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$, so $\\delta = 0.0$,\n  - $\\{\\alpha_{j}\\}_{j=0}^{2} = \\{1.0,\\,0.1,\\,0.01\\}$,\n  - $c = 1.5$.\n- Test $3$ (ill-conditioned case):\n  - $A = \\begin{bmatrix} 1.0  0.0  0.0 \\\\ 0.0  0.01  0.0 \\\\ 0.0  0.0  0.001 \\end{bmatrix}$,\n  - $x^{\\ast} = \\begin{bmatrix} 1.0 \\\\ 1.0 \\\\ 1.0 \\end{bmatrix}$,\n  - $\\eta = \\begin{bmatrix} 0.0 \\\\ 0.005 \\\\ 0.0 \\end{bmatrix}$, so $\\delta = 0.005$,\n  - $\\{\\alpha_{j}\\}_{j=0}^{3} = \\{1.0,\\,0.5,\\,0.25,\\,0.125\\}$,\n  - $c = 1.0$.\n- Test $4$:\n  - $A = \\begin{bmatrix} 1.0  0.2  0.0 \\\\ 0.0  0.5  0.0 \\\\ 0.0  0.0  0.3 \\end{bmatrix}$,\n  - $x^{\\ast} = \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ -1.0 \\end{bmatrix}$,\n  - $\\eta = \\begin{bmatrix} 0.02 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$, so $\\delta = 0.02$,\n  - $\\{\\alpha_{j}\\}_{j=0}^{3} = \\{1.0,\\,0.3,\\,0.09,\\,0.027\\}$,\n  - $c = 1.3$.\n\nYour program should produce a single line of output containing the stopping indices for the above tests as a comma-separated list enclosed in square brackets, in the form $[k_{1},k_{2},k_{3},k_{4}]$, where each $k_{i}$ is an integer. No other output is permitted. Angles are not involved. No physical units are involved. All computations use real numbers with the Euclidean norm.",
            "solution": "The problem is valid. It is scientifically grounded in the theory of inverse problems and Tikhonov regularization, well-posed with a clear objective, and provides all necessary information to derive a unique solution.\n\nThe task is to implement a Lepskii-type stopping rule for Tikhonov regularization. The rule selects an optimal regularization parameter $\\alpha_k$ from a given sequence $\\{\\alpha_j\\}_{j=0}^{J-1}$. The core of the task is to first derive an appropriate bound $\\theta(j)$ and then use it to find the stopping index $k$.\n\nFirst, we formalize the problem. The Tikhonov-regularized solution $x_j$ for a given regularization parameter $\\alpha_j > 0$ is the minimizer of the functional $\\|A x - b^{\\delta}\\|_{2}^{2} + \\alpha_j \\|x\\|_{2}^{2}$. The solution is given by the normal equations:\n$$\n(A^{\\top} A + \\alpha_j I) x_j = A^{\\top} b^{\\delta}\n$$\nwhere $I$ is the identity matrix. The solution can be written as:\n$$\nx_j = (A^{\\top} A + \\alpha_j I)^{-1} A^{\\top} b^{\\delta}\n$$\nThe noisy data is modeled as $b^{\\delta} = A x^{\\ast} + \\eta$, where $x^{\\ast}$ is the true solution and $\\eta$ is the noise vector, with a known bound on its norm, $\\|\\eta\\|_{2} \\leq \\delta$.\n\nWe can decompose the solution $x_j$ into two parts: one part corresponding to the noise-free data $b=Ax^{\\ast}$ and one part due to the noise $\\eta$.\nLet $x_j^{\\ast}$ be the \"ideal\" regularized solution that would be obtained with noise-free data:\n$$\nx_j^{\\ast} = (A^{\\top} A + \\alpha_j I)^{-1} A^{\\top} (A x^{\\ast})\n$$\nThe actual solution $x_j$ is then:\n$$\nx_j = (A^{\\top} A + \\alpha_j I)^{-1} A^{\\top} (A x^{\\ast} + \\eta) = x_j^{\\ast} + (A^{\\top} A + \\alpha_j I)^{-1} A^{\\top} \\eta\n$$\nThe perturbation in the solution due to noise is therefore $x_j - x_j^{\\ast} = (A^{\\top} A + \\alpha_j I)^{-1} A^{\\top} \\eta$. Let us define the linear filter operator $R_j = (A^{\\top} A + \\alpha_j I)^{-1} A^{\\top}$. The noise-induced perturbation is $R_j \\eta$.\n\nThe problem states that the bound $\\theta(j)$ should be constructed based on the operator norm of this filter and the known noise level $\\delta$. The norm of the perturbation can be bounded as:\n$$\n\\|x_j - x_j^{\\ast}\\|_{2} = \\|R_j \\eta\\|_{2} \\leq \\|R_j\\|_{2} \\|\\eta\\|_{2} \\leq \\|R_j\\|_{2} \\delta\n$$\nHere, $\\|R_j\\|_{2}$ is the spectral norm (operator $2$-norm) of the matrix $R_j$. The problem asks us to construct $\\theta(j)$ from this. The most direct and admissible choice for a noise-dependent bound based on this principle is to define $\\theta(j)$ as a bound on the noise propagation:\n$$\n\\theta(j) = \\|R_j\\|_{2} \\delta\n$$\nTo compute $\\|R_j\\|_{2}$, we use the Singular Value Decomposition (SVD) of the matrix $A$. Let $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix of singular values $\\sigma_i \\geq 0$.\n\nSubstituting the SVD into the expression for $R_j$:\n$$\nA^{\\top}A = (U \\Sigma V^{\\top})^{\\top} (U \\Sigma V^{\\top}) = V \\Sigma^{\\top} U^{\\top} U \\Sigma V^{\\top} = V \\Sigma^{\\top} \\Sigma V^{\\top}\n$$\nSince $A^{\\top}A$ is an $n \\times n$ matrix, $\\Sigma^{\\top}\\Sigma$ is an $n \\times n$ diagonal matrix with entries $\\sigma_i^2$.\n$$\nA^{\\top}A + \\alpha_j I = V (\\Sigma^{\\top}\\Sigma + \\alpha_j I) V^{\\top}\n$$\nThe inverse is:\n$$\n(A^{\\top}A + \\alpha_j I)^{-1} = V (\\Sigma^{\\top}\\Sigma + \\alpha_j I)^{-1} V^{\\top} = V \\text{diag}\\left(\\frac{1}{\\sigma_i^2 + \\alpha_j}\\right) V^{\\top}\n$$\nNow, we form $R_j$:\n$$\nR_j = (A^{\\top}A + \\alpha_j I)^{-1} A^{\\top} = \\left(V \\text{diag}\\left(\\frac{1}{\\sigma_i^2 + \\alpha_j}\\right) V^{\\top}\\right) (V \\Sigma^{\\top} U^{\\top}) = V \\text{diag}\\left(\\frac{\\sigma_i}{\\sigma_i^2 + \\alpha_j}\\right) U^{\\top}\n$$\nThis is the SVD of $R_j$. The singular values of $R_j$ are the non-negative values $g_j(\\sigma_i) = \\frac{\\sigma_i}{\\sigma_i^2 + \\alpha_j}$. The spectral norm is the largest of these singular values:\n$$\n\\|R_j\\|_{2} = \\max_{i} \\left\\{ \\frac{\\sigma_i}{\\sigma_i^2 + \\alpha_j} \\right\\}\n$$\nwhere the maximum is taken over all non-zero singular values $\\sigma_i$ of $A$.\n\nThis gives us a computable expression for $\\theta(j)$:\n$$\n\\theta(j) = \\delta \\cdot \\max_{i} \\left\\{ \\frac{\\sigma_i}{\\sigma_i^2 + \\alpha_j} \\right\\}\n$$\n\nThe complete algorithm is as follows:\n1.  For each test case, given $A$, $x^{\\ast}$, $\\eta$, $\\{\\alpha_j\\}_{j=0}^{J-1}$, and $c$.\n2.  Compute the noisy data $b^{\\delta} = A x^{\\ast} + \\eta$ and the noise level $\\delta = \\|\\eta\\|_{2}$.\n3.  Compute the singular values $\\sigma_i$ of $A$.\n4.  Compute the right-hand side of the normal equations: $v = A^{\\top}b^{\\delta}$.\n5.  For each $j \\in \\{0, 1, \\dots, J-1\\}$:\n    a. Form the matrix $M_j = A^{\\top}A + \\alpha_j I$.\n    b. Solve the linear system $M_j x_j = v$ to find the Tikhonov iterate $x_j$. Store all $x_j$.\n6.  Implement the Lepskii-type stopping rule:\n    a. Iterate with index $k$ from $0$ to $J-2$.\n    b. For each $k$, assume it is a valid stopping index. Set a flag `is_k_valid = True`.\n    c. Iterate with index $j$ from $k+1$ to $J-1$.\n    d. Calculate the norm of the difference: $d_{kj} = \\|x_k - x_j\\|_{2}$.\n    e. Calculate the bound $\\theta(j) = \\delta \\cdot \\max_{i} \\{ \\frac{\\sigma_i}{\\sigma_i^2 + \\alpha_j} \\}$.\n    f. Check the condition: if $d_{kj} > c \\cdot \\theta(j)$, the condition fails for this $k$. Set `is_k_valid = False` and break the inner loop over $j$.\n    g. If the inner loop completes and `is_k_valid` remains `True`, then $k$ is the smallest stopping index that satisfies the condition for all $j>k$. The search is complete, and this $k$ is the result.\n7. If the outer loop over $k$ completes without finding such an index, the rule specifies that the stopping index is $k = J-1$.\n\nThis procedure is implemented for each of the test cases provided.",
            "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef find_stopping_index(A, x_star, eta, alphas, c):\n    \"\"\"\n    Implements the Lepskii-type stopping rule for Tikhonov regularization.\n\n    Args:\n        A (np.ndarray): The forward operator matrix.\n        x_star (np.ndarray): The true state vector.\n        eta (np.ndarray): The noise vector.\n        alphas (list or np.ndarray): Sequence of regularization parameters.\n        c (float): The safety factor.\n\n    Returns:\n        int: The stopping index k.\n    \"\"\"\n    m, n = A.shape\n    J = len(alphas)\n\n    # Step 1: Construct b_delta and find noise level delta\n    b_delta = A @ x_star + eta\n    delta = np.linalg.norm(eta)\n\n    # Step 2: Pre-compute necessary components\n    AtA = A.T @ A\n    Atb = A.T @ b_delta\n    \n    # Use SVD to get singular values for norm calculation of the filter operator\n    # We only need the singular values, not U and V.\n    # Exclude tiny singular values that are numerical artifacts\n    sigmas = np.linalg.svd(A, compute_uv=False)\n    sigmas = sigmas[sigmas > 1e-15] \n\n    # Step 3: Compute all Tikhonov solutions x_j\n    solutions = []\n    for alpha_j in alphas:\n        # Tikhonov normal equation: (A^T A + alpha_j I) x_j = A^T b_delta\n        I = np.identity(n)\n        matrix_to_invert = AtA + alpha_j * I\n        x_j = np.linalg.solve(matrix_to_invert, Atb)\n        solutions.append(x_j)\n\n    # Step 4: Find the stopping index k\n    for k in range(J - 1):\n        is_k_valid = True\n        for j in range(k + 1, J):\n            # Calculate the norm of the difference between iterates\n            diff_norm = np.linalg.norm(solutions[k] - solutions[j])\n\n            # Calculate the bound theta(j)\n            alpha_j = alphas[j]\n            \n            if delta == 0.0:\n                # If data is exact, the bound is 0. The condition becomes\n                # ||x_k - x_j|| = 0, which is only true if x_k = x_j.\n                # Since alphas are distinct, this is generally not true.\n                theta_j = 0.0\n            else:\n                if len(sigmas) > 0:\n                    filter_gains = sigmas / (sigmas**2 + alpha_j)\n                    norm_Rj = np.max(filter_gains)\n                else: # Zero matrix case\n                    norm_Rj = 0.0\n                theta_j = delta * norm_Rj\n            \n            # Check the Lepskii-type condition\n            if diff_norm > c * theta_j:\n                is_k_valid = False\n                break  # This k is not the stopping index, try the next k\n\n        if is_k_valid:\n            # Smallest k found that satisfies the condition for all j > k\n            return k\n\n    # If no such k is found in the loop, return the last index as per the rule\n    return J - 1\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for the Lepskii-type stopping rule.\n    \"\"\"\n    test_cases = [\n        {\n            \"A\": np.array([[1.0, 0.0], [0.0, 0.1]]),\n            \"x_star\": np.array([1.0, 1.0]),\n            \"eta\": np.array([0.01, 0.0]),\n            \"alphas\": [1.0, 0.5, 0.25, 0.125, 0.0625, 0.03125],\n            \"c\": 1.2\n        },\n        {\n            \"A\": np.array([[2.0, 0.0], [0.0, 0.5]]),\n            \"x_star\": np.array([1.0, -1.0]),\n            \"eta\": np.array([0.0, 0.0]),\n            \"alphas\": [1.0, 0.1, 0.01],\n            \"c\": 1.5\n        },\n        {\n            \"A\": np.array([[1.0, 0.0, 0.0], [0.0, 0.01, 0.0], [0.0, 0.0, 0.001]]),\n            \"x_star\": np.array([1.0, 1.0, 1.0]),\n            \"eta\": np.array([0.0, 0.005, 0.0]),\n            \"alphas\": [1.0, 0.5, 0.25, 0.125],\n            \"c\": 1.0\n        },\n        {\n            \"A\": np.array([[1.0, 0.2, 0.0], [0.0, 0.5, 0.0], [0.0, 0.0, 0.3]]),\n            \"x_star\": np.array([1.0, 2.0, -1.0]),\n            \"eta\": np.array([0.02, 0.0, 0.0]),\n            \"alphas\": [1.0, 0.3, 0.09, 0.027],\n            \"c\": 1.3\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        k = find_stopping_index(\n            case[\"A\"],\n            case[\"x_star\"],\n            case[\"eta\"],\n            case[\"alphas\"],\n            case[\"c\"]\n        )\n        results.append(k)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```"
        }
    ]
}