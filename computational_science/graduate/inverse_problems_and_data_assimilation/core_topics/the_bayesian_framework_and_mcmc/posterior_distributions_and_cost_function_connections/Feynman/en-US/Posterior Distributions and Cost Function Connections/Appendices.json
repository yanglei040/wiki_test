{
    "hands_on_practices": [
        {
            "introduction": "The cornerstone of variational data assimilation and many inverse problems is the minimization of a cost function. This practice will guide you through the fundamental derivation of this cost function from first principles of Bayesian statistics. By starting with a Generalized Linear Model (GLM)—a powerful framework that includes linear, logistic, and Poisson regression—you will see how the probabilistic formulation of a likelihood and a prior naturally leads to an optimization problem . This exercise is crucial for understanding why the resulting cost function is often convex, a desirable property that guarantees a unique and computationally tractable solution.",
            "id": "3411489",
            "problem": "Consider a Generalized Linear Model (GLM) with canonical link for independent observations $\\{y_i\\}_{i=1}^{n}$ conditioned on predictors $\\{\\mathbf{x}_i\\}_{i=1}^{n}$, where each $y_i$ follows a one-parameter regular exponential family with density\n$$\np(y_i \\mid \\theta_i) = \\exp\\!\\big(y_i \\theta_i - b(\\theta_i) + c(y_i)\\big),\n$$\nand where $b(\\theta)$ is the cumulant (log-partition) function and $c(y)$ is the base measure. In the canonical link GLM, the natural parameter equals the linear predictor, i.e., $\\theta_i = \\eta_i = \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}$ for a coefficient vector $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$. Assume a Gaussian prior on $\\boldsymbol{\\beta}$ given by $\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, where $\\boldsymbol{\\mu} \\in \\mathbb{R}^{p}$ and $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{p \\times p}$ is symmetric positive definite.\n\nStarting from the defining properties of the exponential family and the canonical link, derive the joint likelihood $p(\\mathbf{y} \\mid \\boldsymbol{\\beta})$, then derive the negative log-likelihood as a function of $\\boldsymbol{\\beta}$, carefully justifying its convexity from first principles. Using Bayes’ rule and the Gaussian prior, write the combined cost function equal to the negative log-posterior (up to additive constants that do not depend on $\\boldsymbol{\\beta}$). Express your final answer as a single closed-form analytic expression involving $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^{n}$, $b(\\cdot)$, $\\boldsymbol{\\mu}$, and $\\boldsymbol{\\Sigma}$.\n\nYour final answer must be a single analytic expression. No numerical approximation is required.",
            "solution": "The problem statement is internally consistent, scientifically grounded in the theory of statistical models, and well-posed. All definitions and conditions are standard in the field of generalized linear models and Bayesian inference. The problem is therefore deemed valid and a full solution can be constructed.\n\nWe begin by establishing the mathematical framework based on the provided information. The data consist of $n$ independent observations $\\{y_i\\}_{i=1}^{n}$. The conditional probability density function of each observation $y_i$, given the natural parameter $\\theta_i$, belongs to the one-parameter regular exponential family:\n$$\np(y_i \\mid \\theta_i) = \\exp\\big(y_i \\theta_i - b(\\theta_i) + c(y_i)\\big)\n$$\nIn a Generalized Linear Model (GLM) with a canonical link function, the natural parameter $\\theta_i$ is set equal to the linear predictor $\\eta_i$. The linear predictor is defined as $\\eta_i = \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}$, where $\\mathbf{x}_i \\in \\mathbb{R}^{p}$ is the vector of predictors for observation $i$ and $\\boldsymbol{\\beta} \\in \\mathbb{R}^{p}$ is the vector of model coefficients. Thus, we have the relationship:\n$$\n\\theta_i = \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}\n$$\nSubstituting this into the density for $y_i$, we obtain the conditional density of $y_i$ given the coefficient vector $\\boldsymbol{\\beta}$:\n$$\np(y_i \\mid \\boldsymbol{\\beta}) = \\exp\\big(y_i (\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) - b(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) + c(y_i)\\big)\n$$\n\nThe problem states that the observations are independent. Therefore, the joint likelihood of the entire dataset $\\mathbf{y} = (y_1, \\dots, y_n)^{\\top}$ given $\\boldsymbol{\\beta}$ is the product of the individual likelihoods:\n$$\np(\\mathbf{y} \\mid \\boldsymbol{\\beta}) = \\prod_{i=1}^{n} p(y_i \\mid \\boldsymbol{\\beta}) = \\prod_{i=1}^{n} \\exp\\big(y_i \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta} - b(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) + c(y_i)\\big)\n$$\nUsing the property of the exponential function that a product of exponentials is the exponential of the sum of their arguments, we can write the joint likelihood as:\n$$\np(\\mathbf{y} \\mid \\boldsymbol{\\beta}) = \\exp\\left( \\sum_{i=1}^{n} \\left( y_i \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta} - b(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) + c(y_i) \\right) \\right)\n$$\n\nThe log-likelihood function, denoted $\\ell(\\boldsymbol{\\beta})$, is the natural logarithm of the joint likelihood:\n$$\n\\ell(\\boldsymbol{\\beta}) = \\ln p(\\mathbf{y} \\mid \\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left( y_i \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta} - b(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) + c(y_i) \\right)\n$$\nThe problem asks for the negative log-likelihood, which we will denote by $L(\\boldsymbol{\\beta})$:\n$$\nL(\\boldsymbol{\\beta}) = -\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left( b(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) - y_i \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta} - c(y_i) \\right)\n$$\n\nNext, we must justify the convexity of $L(\\boldsymbol{\\beta})$ with respect to $\\boldsymbol{\\beta}$ from first principles. A function is convex if its Hessian matrix (the matrix of second partial derivatives) is positive semi-definite. Let's compute the Hessian of $L(\\boldsymbol{\\beta})$. The terms $\\sum y_i \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}$ are linear in $\\boldsymbol{\\beta}$, and the terms $\\sum c(y_i)$ are constant with respect to $\\boldsymbol{\\beta}$. The second derivatives of these terms are zero. Thus, the convexity of $L(\\boldsymbol{\\beta})$ is determined entirely by the term $\\sum_{i=1}^{n} b(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta})$.\nLet $\\theta_i(\\boldsymbol{\\beta}) = \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}$. The gradient of $L(\\boldsymbol{\\beta})$ with respect to $\\boldsymbol{\\beta}$ is:\n$$\n\\nabla_{\\boldsymbol{\\beta}} L(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left( \\nabla_{\\boldsymbol{\\beta}} b(\\theta_i(\\boldsymbol{\\beta})) - \\nabla_{\\boldsymbol{\\beta}} (y_i \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) \\right)\n$$\nUsing the chain rule, $\\nabla_{\\boldsymbol{\\beta}} b(\\theta_i) = b'(\\theta_i) \\nabla_{\\boldsymbol{\\beta}} \\theta_i = b'(\\theta_i) \\mathbf{x}_i$. The gradient of the linear term is $\\nabla_{\\boldsymbol{\\beta}} (y_i \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) = y_i \\mathbf{x}_i$. So,\n$$\n\\nabla_{\\boldsymbol{\\beta}} L(\\boldsymbol{\\beta}) = \\sum_{i=1}^{n} \\left(b'(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) - y_i\\right) \\mathbf{x}_i\n$$\nThe Hessian matrix $\\mathbf{H}_{\\boldsymbol{\\beta}}[L]$ is obtained by taking the derivative of the gradient:\n$$\n\\mathbf{H}_{\\boldsymbol{\\beta}}[L] = \\nabla_{\\boldsymbol{\\beta}} \\left( \\nabla_{\\boldsymbol{\\beta}} L(\\boldsymbol{\\beta}) \\right)^{\\top} = \\sum_{i=1}^{n} \\nabla_{\\boldsymbol{\\beta}} \\left( \\left(b'(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) - y_i\\right) \\mathbf{x}_i^{\\top} \\right)\n$$\nApplying the chain rule again:\n$$\n\\mathbf{H}_{\\boldsymbol{\\beta}}[L] = \\sum_{i=1}^{n} \\mathbf{x}_i \\left( \\nabla_{\\boldsymbol{\\beta}} b'(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) \\right)^{\\top} = \\sum_{i=1}^{n} \\mathbf{x}_i \\left( b''(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) (\\nabla_{\\boldsymbol{\\beta}} (\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}))^{\\top} \\right) = \\sum_{i=1}^{n} b''(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) \\mathbf{x}_i \\mathbf{x}_i^{\\top}\n$$\nA fundamental property of any regular exponential family is that the cumulant function $b(\\theta)$ is strictly convex, which implies its second derivative is non-negative: $b''(\\theta) \\ge 0$. In fact, for a regular family, $b''(\\theta) = \\text{Var}(Y \\mid \\theta)$, which must be non-negative. The matrix $\\mathbf{x}_i \\mathbf{x}_i^{\\top}$ is an outer product, which is always positive semi-definite, since for any vector $\\mathbf{v} \\in \\mathbb{R}^p$, we have $\\mathbf{v}^{\\top}(\\mathbf{x}_i \\mathbf{x}_i^{\\top})\\mathbf{v} = (\\mathbf{v}^{\\top}\\mathbf{x}_i)(\\mathbf{x}_i^{\\top}\\mathbf{v}) = (\\mathbf{x}_i^{\\top}\\mathbf{v})^2 \\ge 0$.\nThe product of a non-negative scalar $b''(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta})$ and a positive semi-definite matrix $\\mathbf{x}_i \\mathbf{x}_i^{\\top}$ is also positive semi-definite. The Hessian of $L(\\boldsymbol{\\beta})$ is a sum of such positive semi-definite matrices. The sum of positive semi-definite matrices is itself positive semi-definite. Therefore, $\\mathbf{H}_{\\boldsymbol{\\beta}}[L]$ is positive semi-definite, which proves that the negative log-likelihood $L(\\boldsymbol{\\beta})$ is a convex function of $\\boldsymbol{\\beta}$.\n\nNow we incorporate the prior distribution on $\\boldsymbol{\\beta}$. The prior is given as Gaussian: $\\boldsymbol{\\beta} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$. The probability density function is:\n$$\np(\\boldsymbol{\\beta}) = \\frac{1}{(2\\pi)^{p/2} \\det(\\boldsymbol{\\Sigma})^{1/2}} \\exp\\left(-\\frac{1}{2}(\\boldsymbol{\\beta} - \\boldsymbol{\\mu})^{\\top} \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\beta} - \\boldsymbol{\\mu})\\right)\n$$\nThe negative log-prior, ignoring terms that are constant with respect to $\\boldsymbol{\\beta}$, is:\n$$\n-\\ln p(\\boldsymbol{\\beta}) \\propto \\frac{1}{2}(\\boldsymbol{\\beta} - \\boldsymbol{\\mu})^{\\top} \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\beta} - \\boldsymbol{\\mu})\n$$\nAccording to Bayes' rule, the posterior distribution of $\\boldsymbol{\\beta}$ is proportional to the product of the likelihood and the prior:\n$$\np(\\boldsymbol{\\beta} \\mid \\mathbf{y}) \\propto p(\\mathbf{y} \\mid \\boldsymbol{\\beta}) p(\\boldsymbol{\\beta})\n$$\nThe log-posterior is therefore, up to an additive constant:\n$$\n\\ln p(\\boldsymbol{\\beta} \\mid \\mathbf{y}) \\approx \\ln p(\\mathbf{y} \\mid \\boldsymbol{\\beta}) + \\ln p(\\boldsymbol{\\beta}) = \\ell(\\boldsymbol{\\beta}) + \\ln p(\\boldsymbol{\\beta})\n$$\nThe cost function, $J(\\boldsymbol{\\beta})$, is defined as the negative log-posterior, up to additive constants that do not depend on $\\boldsymbol{\\beta}$.\n$$\nJ(\\boldsymbol{\\beta}) = - \\ln p(\\boldsymbol{\\beta} \\mid \\mathbf{y}) \\approx -\\ln p(\\mathbf{y} \\mid \\boldsymbol{\\beta}) - \\ln p(\\boldsymbol{\\beta})\n$$\nSubstituting the expressions for the negative log-likelihood and the negative log-prior:\n$$\nJ(\\boldsymbol{\\beta}) \\approx \\left(\\sum_{i=1}^{n} \\left( b(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) - y_i \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta} \\right)\\right) + \\frac{1}{2}(\\boldsymbol{\\beta} - \\boldsymbol{\\mu})^{\\top} \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\beta} - \\boldsymbol{\\mu})\n$$\nHere we have dropped the term $-\\sum c(y_i)$ from the negative log-likelihood as it does not depend on $\\boldsymbol{\\beta}$. This final expression represents the cost function often used for Maximum A Posteriori (MAP) estimation. It consists of a data fidelity term (the negative log-likelihood) and a regularization term (the negative log-prior). The convexity of the negative log-likelihood and the strict convexity of the quadratic prior term (since $\\boldsymbol{\\Sigma}^{-1}$ is positive definite) ensures that the total cost function $J(\\boldsymbol{\\beta})$ is strictly convex, guaranteeing a unique minimum.\n\nThe requested final answer is the closed-form analytical expression for this cost function.",
            "answer": "$$\n\\boxed{\\sum_{i=1}^{n} \\left(b(\\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}) - y_i \\mathbf{x}_i^{\\top} \\boldsymbol{\\beta}\\right) + \\frac{1}{2}(\\boldsymbol{\\beta} - \\boldsymbol{\\mu})^{\\top} \\boldsymbol{\\Sigma}^{-1} (\\boldsymbol{\\beta} - \\boldsymbol{\\mu})}\n$$"
        },
        {
            "introduction": "While the connection between a posterior distribution and a cost function is direct, it harbors subtleties that can trap the unwary. This exercise illuminates one of the most important: the Maximum A Posteriori (MAP) estimate is not invariant under a nonlinear change of variables. You will work through a simple one-dimensional example to demonstrate how naively reparameterizing a cost function yields a different result than correctly transforming the underlying probability density . Completing this practice will build a deeper appreciation for the role of the Jacobian in the change-of-variables formula and instill the necessary caution when defining and interpreting cost functions in different model parameterizations.",
            "id": "3411380",
            "problem": "Consider a one-dimensional inverse problem in data assimilation where the unknown state $u \\in \\mathbb{R}$ is inferred from a single observation $y \\in \\mathbb{R}$. The prior distribution for $u$ is Gaussian with density $p_U(u) \\propto \\exp\\!\\big(-\\frac{1}{2}u^{2}\\big)$, and the observation model is Gaussian with density $p(y \\mid u) \\propto \\exp\\!\\big(-\\frac{1}{2}(y - u)^{2}\\big)$. You observe $y = 1$. The posterior $p(u \\mid y)$ is defined by Bayes’ rule, and the associated variational objective (negative log-posterior up to an additive constant) in the $u$-parameterization is $J_u(u) = \\frac{1}{2}(y - u)^{2} + \\frac{1}{2}u^{2}$.\n\nNow consider the reparameterization $v = \\phi(u)$ with $\\phi(u) = \\exp(u)$, which defines a new parameter $v \\in (0, \\infty)$ related to $u$ by $u = \\ln(v)$. The transformed posterior density $p(v \\mid y)$ is defined via the change-of-variables formula for probability densities. Define also the incorrectly specified variational objective in the $v$-parameterization,\n$$\nJ_v^{\\mathrm{naive}}(v) = \\frac{1}{2}(v - y)^{2} + \\frac{1}{2}v^{2},\n$$\nwhich treats $v$ as if it had a Gaussian prior directly and ignores the Jacobian of the transformation. This $J_v^{\\mathrm{naive}}(v)$ does not equal the correctly transformed $J_u(\\ln v)$ plus the Jacobian term implied by change of variables for the posterior density.\n\nStarting from the fundamental definitions of Gaussian likelihoods and priors, Bayes’ rule, and the change-of-variables formula for probability densities, carry out the following computations:\n\n1. Compute the Maximum A Posteriori (MAP) location in the $u$-parameterization, defined as the minimizer of $J_u(u)$.\n2. Compute the MAP in the $v$-parameterization obtained by transforming the posterior $p(u \\mid y)$ consistently to $p(v \\mid y)$ using the change-of-variables formula, and then maximizing $p(v \\mid y)$ over $v \\in (0, \\infty)$.\n3. Compute the minimizer of the incorrectly specified objective $J_v^{\\mathrm{naive}}(v)$.\n\nReport the triple consisting of the three values, in the order $(u_{\\mathrm{MAP}}, v_{\\mathrm{MAP}}^{\\mathrm{transformed}}, v_{\\mathrm{MAP}}^{\\mathrm{naive}})$, as a single row matrix using the LaTeX `pmatrix` environment. No rounding is required. No units are involved. Clearly state the final answer only as the requested row matrix.",
            "solution": "The problem requires the computation of three values: the Maximum A Posteriori (MAP) estimate for a parameter $u$, the MAP estimate for a transformed parameter $v = \\exp(u)$ derived from a consistent change of variables, and a third estimate for $v$ derived from minimizing a naively constructed objective function. We are given the observation $y = 1$. We will address each computation in sequence.\n\n### 1. Computation of the MAP estimate for $u$ ($u_{\\mathrm{MAP}}$)\n\nThe posterior distribution for the unknown state $u \\in \\mathbb{R}$ is given by Bayes' rule:\n$$ p(u \\mid y) \\propto p(y \\mid u) p_U(u) $$\nGiven the prior density $p_U(u) \\propto \\exp\\big(-\\frac{1}{2}u^{2}\\big)$ and the likelihood $p(y \\mid u) \\propto \\exp\\big(-\\frac{1}{2}(y - u)^{2}\\big)$, the posterior density is:\n$$ p(u \\mid y) \\propto \\exp\\left(-\\frac{1}{2}(y - u)^{2}\\right) \\exp\\left(-\\frac{1}{2}u^{2}\\right) = \\exp\\left(-\\left[\\frac{1}{2}(y - u)^{2} + \\frac{1}{2}u^{2}\\right]\\right) $$\nThe MAP estimate, $u_{\\mathrm{MAP}}$, is the value of $u$ that maximizes the posterior density $p(u \\mid y)$. This is equivalent to minimizing the negative logarithm of the posterior density. The problem defines the associated variational objective (or cost function) as:\n$$ J_u(u) = \\frac{1}{2}(y - u)^{2} + \\frac{1}{2}u^{2} $$\nTo find the minimizer of $J_u(u)$, we compute its derivative with respect to $u$ and set it to zero.\n$$ \\frac{dJ_u}{du} = \\frac{d}{du}\\left(\\frac{1}{2}(y^{2} - 2yu + u^{2}) + \\frac{1}{2}u^{2}\\right) = \\frac{d}{du}\\left(u^{2} - yu + \\frac{1}{2}y^{2}\\right) $$\n$$ \\frac{dJ_u}{du} = 2u - y $$\nSetting the derivative to zero yields the location of the extremum:\n$$ 2u - y = 0 \\implies u = \\frac{y}{2} $$\nThe second derivative is $\\frac{d^2J_u}{du^2} = 2 > 0$, which confirms that this is a minimum.\nThus, the MAP estimate for $u$ is $u_{\\mathrm{MAP}} = \\frac{y}{2}$.\nSubstituting the given observation value $y = 1$:\n$$ u_{\\mathrm{MAP}} = \\frac{1}{2} $$\n\n### 2. Computation of the MAP estimate for $v$ via transformation ($v_{\\mathrm{MAP}}^{\\mathrm{transformed}}$)\n\nWe are given the reparameterization $v = \\phi(u) = \\exp(u)$, where $v \\in (0, \\infty)$. The inverse transformation is $u = \\ln(v)$. To find the posterior density for $v$, $p(v \\mid y)$, we use the change-of-variables formula for probability densities:\n$$ p(v \\mid y) = p(u(v) \\mid y) \\left| \\frac{du}{dv} \\right| $$\nThe Jacobian determinant of the transformation is $\\left| \\frac{du}{dv} \\right| = \\left| \\frac{1}{v} \\right| = \\frac{1}{v}$ since $v > 0$.\nSubstituting $u = \\ln(v)$ and the Jacobian into the expression for the posterior, we get:\n$$ p(v \\mid y) \\propto \\exp\\left(-J_u(\\ln(v))\\right) \\cdot \\frac{1}{v} = \\frac{1}{v} \\exp\\left(-\\left[\\frac{1}{2}(y - \\ln(v))^{2} + \\frac{1}{2}(\\ln(v))^{2}\\right]\\right) $$\nThe MAP estimate for $v$, denoted $v_{\\mathrm{MAP}}^{\\mathrm{transformed}}$, is the value that maximizes $p(v \\mid y)$. This is equivalent to minimizing the negative logarithm of this density. Let's define the correct objective function for $v$ as $J_v^{\\mathrm{correct}}(v) = -\\ln(p(v \\mid y))$ (up to an additive constant):\n$$ J_v^{\\mathrm{correct}}(v) = \\left[\\frac{1}{2}(y - \\ln(v))^{2} + \\frac{1}{2}(\\ln(v))^{2}\\right] - \\ln\\left(\\frac{1}{v}\\right) $$\n$$ J_v^{\\mathrm{correct}}(v) = \\frac{1}{2}(y - \\ln(v))^{2} + \\frac{1}{2}(\\ln(v))^{2} + \\ln(v) $$\nTo find the minimizer, we differentiate $J_v^{\\mathrm{correct}}(v)$ with respect to $v$ and set the derivative to zero:\n$$ \\frac{dJ_v^{\\mathrm{correct}}}{dv} = \\frac{1}{2} \\cdot 2(y - \\ln(v)) \\cdot \\left(-\\frac{1}{v}\\right) + \\frac{1}{2} \\cdot 2\\ln(v) \\cdot \\frac{1}{v} + \\frac{1}{v} $$\n$$ \\frac{dJ_v^{\\mathrm{correct}}}{dv} = \\frac{1}{v} \\left( -(y - \\ln(v)) + \\ln(v) + 1 \\right) $$\n$$ \\frac{dJ_v^{\\mathrm{correct}}}{dv} = \\frac{1}{v} \\left( -y + \\ln(v) + \\ln(v) + 1 \\right) = \\frac{1}{v} \\left( 2\\ln(v) - y + 1 \\right) $$\nSince $v > 0$, we set the term in the parenthesis to zero:\n$$ 2\\ln(v) - y + 1 = 0 \\implies \\ln(v) = \\frac{y - 1}{2} $$\n$$ v = \\exp\\left(\\frac{y - 1}{2}\\right) $$\nThe second derivative test confirms this is a minimum. Thus, $v_{\\mathrm{MAP}}^{\\mathrm{transformed}} = \\exp\\left(\\frac{y-1}{2}\\right)$.\nSubstituting the given observation value $y = 1$:\n$$ v_{\\mathrm{MAP}}^{\\mathrm{transformed}} = \\exp\\left(\\frac{1 - 1}{2}\\right) = \\exp(0) = 1 $$\n\n### 3. Computation of the minimizer of the naive objective function ($v_{\\mathrm{MAP}}^{\\mathrm{naive}}$)\n\nThe problem defines an incorrectly specified or \"naive\" objective function for $v$:\n$$ J_v^{\\mathrm{naive}}(v) = \\frac{1}{2}(v - y)^{2} + \\frac{1}{2}v^{2} $$\nThis functional form incorrectly assumes that $v$ has a Gaussian prior and the observation model is linear in $v$. We are asked to find the minimizer of this function, denoted $v_{\\mathrm{MAP}}^{\\mathrm{naive}}$. We proceed by differentiation:\n$$ \\frac{dJ_v^{\\mathrm{naive}}}{dv} = \\frac{d}{dv}\\left(\\frac{1}{2}(v^2 - 2vy + y^2) + \\frac{1}{2}v^2\\right) = \\frac{d}{dv}\\left(v^2 - vy + \\frac{1}{2}y^2\\right) $$\n$$ \\frac{dJ_v^{\\mathrm{naive}}}{dv} = 2v - y $$\nSetting the derivative to zero gives:\n$$ 2v - y = 0 \\implies v = \\frac{y}{2} $$\nThe second derivative is $\\frac{d^2J_v^{\\mathrm{naive}}}{dv^2} = 2 > 0$, confirming a minimum.\nThus, the minimizer is $v_{\\mathrm{MAP}}^{\\mathrm{naive}} = \\frac{y}{2}$.\nSubstituting the given observation value $y = 1$:\n$$ v_{\\mathrm{MAP}}^{\\mathrm{naive}} = \\frac{1}{2} $$\n\n### Summary of Results\n\nThe three computed values are:\n1.  $u_{\\mathrm{MAP}} = \\frac{1}{2}$\n2.  $v_{\\mathrm{MAP}}^{\\mathrm{transformed}} = 1$\n3.  $v_{\\mathrm{MAP}}^{\\mathrm{naive}} = \\frac{1}{2}$\n\nThe final answer is the row matrix $(u_{\\mathrm{MAP}}, v_{\\mathrm{MAP}}^{\\mathrm{transformed}}, v_{\\mathrm{MAP}}^{\\mathrm{naive}})$.",
            "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2} & 1 & \\frac{1}{2}\n\\end{pmatrix}\n}\n$$"
        },
        {
            "introduction": "In practical applications involving nonlinear forward models, the full Hessian of the cost function can be computationally expensive or complex to derive. This has profound implications for both optimization and uncertainty quantification. This practice explores the widely used Gauss-Newton approximation to the true posterior curvature and asks you to quantify the gap between the approximation and the true Hessian . By deriving the analytical expression for this discrepancy and implementing a numerical test suite, you will gain hands-on experience with how model nonlinearity and data misfit contribute to this gap, directly affecting the accuracy of Gaussian-based uncertainty estimates.",
            "id": "3411467",
            "problem": "Consider a Bayesian inverse problem with parameter vector $u \\in \\mathbb{R}^d$, forward map $G:\\mathbb{R}^d \\to \\mathbb{R}^m$, observed data $y \\in \\mathbb{R}^m$, additive Gaussian observational noise with covariance $\\Gamma \\in \\mathbb{R}^{m \\times m}$, and a Gaussian prior $u \\sim \\mathcal{N}(u_0, C)$ with mean $u_0 \\in \\mathbb{R}^d$ and covariance $C \\in \\mathbb{R}^{d \\times d}$. The negative log-posterior (often called the cost function in data assimilation) is\n$$\n\\Phi(u) = \\frac{1}{2}\\,(y - G(u))^\\top \\Gamma^{-1} (y - G(u)) + \\frac{1}{2}\\,(u - u_0)^\\top C^{-1} (u - u_0).\n$$\nThe Maximum A Posteriori (MAP) estimator $u^\\star$ is any minimizer of $\\Phi$, and the posterior curvature at $u^\\star$ is the Hessian $\\nabla^2 \\Phi(u^\\star)$. The adjoint-based Gauss–Newton Hessian uses only first derivatives of $G$ and is given by the approximation\n$$\nH_{\\mathrm{GN}}(u) = J_G(u)^\\top \\Gamma^{-1} J_G(u) + C^{-1},\n$$\nwhere $J_G(u)$ denotes the Jacobian of $G$ at $u$. In nonlinear problems, $H_{\\mathrm{GN}}(u^\\star)$ generally differs from the true posterior curvature $\\nabla^2 \\Phi(u^\\star)$, and this discrepancy affects uncertainty quantification by altering the Gaussian approximation of the posterior covariance.\n\nStarting from Bayes' theorem, the definitions of the likelihood and prior, and the calculus of gradients and Hessians for composite functions, derive a computable expression for the discrepancy between the true posterior curvature $\\nabla^2 \\Phi(u^\\star)$ and the Gauss–Newton approximation $H_{\\mathrm{GN}}(u^\\star)$ in terms of the Jacobian $J_G(u^\\star)$, the residual $r(u^\\star) = y - G(u^\\star)$, and the collection of second-derivative matrices of the forward model components. Then, design an algorithm that:\n- computes $u^\\star$ by minimizing $\\Phi(u)$ with a second-order method that uses exact gradients and Hessians,\n- evaluates both the true posterior curvature and the Gauss–Newton approximation at $u^\\star$,\n- quantifies the curvature gap and its downstream impact on uncertainty quantification.\n\nUse the following fundamental basis and definitions only:\n- Bayes' theorem linking posterior to prior and likelihood.\n- The negative log-likelihood for Gaussian noise and the negative log-prior for a Gaussian prior.\n- The chain rule for gradients and Hessians of composite maps.\n- Standard linear algebra identities for matrix norms and traces.\n\nYou must implement and report, for each test case, two scalar diagnostics:\n- a curvature gap ratio $g = \\lVert \\nabla^2 \\Phi(u^\\star) - H_{\\mathrm{GN}}(u^\\star) \\rVert_F \\,\\big/\\, \\lVert \\nabla^2 \\Phi(u^\\star) \\rVert_F$, where $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm,\n- a symmetrized Kullback–Leibler divergence $D$ between the two Gaussian posterior approximations $\\mathcal{N}(u^\\star, \\nabla^2 \\Phi(u^\\star)^{-1})$ and $\\mathcal{N}(u^\\star, H_{\\mathrm{GN}}(u^\\star)^{-1})$, defined as the sum of the two directed Kullback–Leibler divergences between these Gaussians.\n\nYour program must implement the following test suite, with all matrices and vectors specified explicitly. In every test, the parameter dimension is $d=2$ and the observation dimension is $m=2$. Each test specifies a forward map $G$, a prior $(u_0, C)$, an observation configuration $(y, \\Gamma)$, and a prescribed initial guess for the optimization equal to $u_0$.\n\n- Test 1 (linear, no curvature gap expected):\n  - $G(u) = \\begin{bmatrix} u_1 + 0.5\\,u_2 \\\\ u_2 \\end{bmatrix}$,\n  - $u_0 = \\begin{bmatrix} 0.2 \\\\ -0.3 \\end{bmatrix}$,\n  - $C = \\mathrm{diag}\\big(0.49, 0.25\\big)$,\n  - $\\Gamma = \\begin{bmatrix} 0.09 & 0.03 \\\\ 0.03 & 0.16 \\end{bmatrix}$,\n  - $y = \\begin{bmatrix} 1.0 \\\\ -0.5 \\end{bmatrix}$.\n\n- Test 2 (nonlinear, exact posterior stationarity at prior mean, zero residual at MAP):\n  - $G(u) = \\begin{bmatrix} u_1 \\\\ \\exp(u_2) \\end{bmatrix}$,\n  - $u_0 = \\begin{bmatrix} 0.2 \\\\ -0.1 \\end{bmatrix}$,\n  - $C = \\mathrm{diag}\\big(1.0, 1.0\\big)$,\n  - $\\Gamma = \\mathrm{diag}\\big(0.01, 0.01\\big)$,\n  - $y = \\begin{bmatrix} 0.2 \\\\ \\exp(-0.1) \\end{bmatrix}$.\n\n- Test 3 (nonlinear, well-constrained data, small curvature gap expected):\n  - $G(u) = \\begin{bmatrix} u_1 \\\\ \\exp(u_2) \\end{bmatrix}$,\n  - $u_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n  - $C = \\mathrm{diag}\\big(10.0, 10.0\\big)$,\n  - $\\Gamma = \\mathrm{diag}\\big(0.04, 0.04\\big)$,\n  - $y = \\begin{bmatrix} 0.8 \\\\ \\exp(0.3) \\end{bmatrix}$.\n\n- Test 4 (nonlinear, weak data and informative prior, larger curvature gap expected):\n  - $G(u) = \\begin{bmatrix} u_1 \\\\ \\exp(u_2) \\end{bmatrix}$,\n  - $u_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n  - $C = \\mathrm{diag}\\big(0.25, 0.25\\big)$,\n  - $\\Gamma = \\mathrm{diag}\\big(4.0, 4.0\\big)$,\n  - $y = \\begin{bmatrix} 0.8 \\\\ \\exp(0.3) \\end{bmatrix}$.\n\n- Test 5 (nonlinear with coupling and correlated noise):\n  - $G(u) = \\begin{bmatrix} \\exp(u_1 + 0.5\\,u_2) \\\\ u_1^2 - u_2 \\end{bmatrix}$,\n  - $u_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$,\n  - $C = \\mathrm{diag}\\big(1.0, 1.0\\big)$,\n  - $\\Gamma = \\begin{bmatrix} 0.09 & 0.04 \\\\ 0.04 & 0.16 \\end{bmatrix}$,\n  - $y = \\begin{bmatrix} 1.0 \\\\ 0.21 \\end{bmatrix}$.\n\nAlgorithmic requirements:\n- For each test, compute $u^\\star$ by minimizing $\\Phi(u)$ using a second-order method with exact gradient and Hessian.\n- At $u^\\star$, form the residual $r(u^\\star) = y - G(u^\\star)$, the Jacobian $J_G(u^\\star)$, and the per-component Hessians $\\nabla^2 G_i(u^\\star)$ for $i \\in \\{1,2\\}$.\n- Assemble both the true posterior curvature $\\nabla^2 \\Phi(u^\\star)$ and the Gauss–Newton curvature $H_{\\mathrm{GN}}(u^\\star)$.\n- Compute the curvature gap ratio $g$ as specified.\n- Compute the symmetrized Kullback–Leibler divergence $D$ between $\\mathcal{N}(u^\\star, \\nabla^2 \\Phi(u^\\star)^{-1})$ and $\\mathcal{N}(u^\\star, H_{\\mathrm{GN}}(u^\\star)^{-1})$.\n- Aggregate the two diagnostics for each test as a length-$2$ list $[g, D]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one entry per test in the order above. Each entry must itself be a list of two floating-point numbers in the order $[g,D]$. For example, the output format must be like $[[g_1,D_1],[g_2,D_2],\\dots]$.",
            "solution": "The user wants a detailed derivation of the discrepancy between the true posterior Hessian and its Gauss-Newton approximation, followed by a numerical implementation to quantify this gap for several test cases.\n\n### Problem Validation\n\nThe problem statement is parsed and validated against the specified criteria.\n\n**Step 1: Extract Givens**\n\n- **Parameter vector**: $u \\in \\mathbb{R}^d$\n- **Observation vector**: $y \\in \\mathbb{R}^m$\n- **Forward map**: $G:\\mathbb{R}^d \\to \\mathbb{R}^m$\n- **Prior distribution**: $u \\sim \\mathcal{N}(u_0, C)$, with mean $u_0 \\in \\mathbb{R}^d$ and covariance $C \\in \\mathbb{R}^{d \\times d}$.\n- **Observational noise**: Additive Gaussian with covariance $\\Gamma \\in \\mathbb{R}^{m \\times m}$.\n- **Negative log-posterior (cost function)**: $\\Phi(u) = \\frac{1}{2}\\,(y - G(u))^\\top \\Gamma^{-1} (y - G(u)) + \\frac{1}{2}\\,(u - u_0)^\\top C^{-1} (u - u_0)$.\n- **Maximum A Posteriori (MAP) estimator**: $u^\\star$ is a minimizer of $\\Phi(u)$.\n- **Gauss–Newton Hessian approximation**: $H_{\\mathrm{GN}}(u) = J_G(u)^\\top \\Gamma^{-1} J_G(u) + C^{-1}$, where $J_G(u)$ is the Jacobian of $G$.\n- **Diagnostics**:\n    - Curvature gap ratio: $g = \\lVert \\nabla^2 \\Phi(u^\\star) - H_{\\mathrm{GN}}(u^\\star) \\rVert_F \\,\\big/\\, \\lVert \\nabla^2 \\Phi(u^\\star) \\rVert_F$\n    - Symmetrized Kullback–Leibler divergence $D$ between $\\mathcal{N}(u^\\star, (\\nabla^2 \\Phi(u^\\star))^{-1})$ and $\\mathcal{N}(u^\\star, H_{\\mathrm{GN}}(u^\\star)^{-1})$.\n- **Test Cases**: Five fully specified test cases with $d=2$, $m=2$, providing $G(u)$, $u_0$, $C$, $\\Gamma$, and $y$. The initial guess for optimization is $u_0$.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded**: The problem is rooted in the standard mathematical framework of Bayesian inference for inverse problems. The concepts of posterior distribution, MAP estimation, and the Gauss-Newton approximation are central to this field.\n- **Well-Posed**: The problem is well-posed. It requests the derivation of a standard result and its numerical verification on clearly defined test cases. All necessary functions, parameters, and matrices are provided, making the problem unambiguous and solvable.\n- **Objective**: The problem is stated using precise, objective mathematical language, free from any subjective elements.\n\nThe problem formulation exhibits no flaws. It is scientifically sound, self-contained, and formalizable.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. A full solution will be provided.\n\n### Derivation of the Hessian Discrepancy\n\nThe objective is to derive an expression for the difference $\\nabla^2 \\Phi(u) - H_{\\mathrm{GN}}(u)$. We begin by decomposing the negative log-posterior $\\Phi(u)$ into its likelihood and prior components:\n$$\n\\Phi(u) = \\Phi_{\\text{like}}(u) + \\Phi_{\\text{prior}}(u)\n$$\nwhere the negative log-likelihood is $\\Phi_{\\text{like}}(u) = \\frac{1}{2}\\,(y - G(u))^\\top \\Gamma^{-1} (y - G(u))$ and the negative log-prior is $\\Phi_{\\text{prior}}(u) = \\frac{1}{2}\\,(u - u_0)^\\top C^{-1} (u - u_0)$.\n\nThe Hessian is additive, so $\\nabla^2 \\Phi(u) = \\nabla^2 \\Phi_{\\text{like}}(u) + \\nabla^2 \\Phi_{\\text{prior}}(u)$.\n\n**1. Hessian of the Prior Term**\n\nThe prior term $\\Phi_{\\text{prior}}(u)$ is a quadratic function of $u$.\n$$\n\\Phi_{\\text{prior}}(u) = \\frac{1}{2} (u^\\top C^{-1} u - u^\\top C^{-1} u_0 - u_0^\\top C^{-1} u + u_0^\\top C^{-1} u_0)\n$$\nUsing standard rules for vector calculus, its gradient is:\n$$\n\\nabla \\Phi_{\\text{prior}}(u) = C^{-1}(u - u_0)\n$$\nDifferentiating again with respect to $u$ yields the Hessian:\n$$\n\\nabla^2 \\Phi_{\\text{prior}}(u) = C^{-1}\n$$\nThis is a constant matrix, independent of $u$.\n\n**2. Hessian of the Likelihood Term**\n\nThe likelihood term $\\Phi_{\\text{like}}(u)$ involves the composition of the forward model $G(u)$. Let $r(u) = y - G(u)$ be the residual. Then $\\Phi_{\\text{like}}(u) = \\frac{1}{2} r(u)^\\top \\Gamma^{-1} r(u)$.\nLet's find the gradient $\\nabla \\Phi_{\\text{like}}(u)$ using the chain rule. Its $k$-th component is:\n$$\n\\frac{\\partial \\Phi_{\\text{like}}}{\\partial u_k} = \\frac{\\partial r}{\\partial u_k}^\\top \\Gamma^{-1} r(u) = - \\frac{\\partial G}{\\partial u_k}^\\top \\Gamma^{-1} r(u)\n$$\nIn vector form, the gradient is the vector whose components are given above, which can be written compactly using the Jacobian $J_G(u) = \\nabla G(u)$:\n$$\n\\nabla \\Phi_{\\text{like}}(u) = -J_G(u)^\\top \\Gamma^{-1} r(u) = -J_G(u)^\\top \\Gamma^{-1} (y - G(u))\n$$\nTo find the Hessian $\\nabla^2 \\Phi_{\\text{like}}(u)$, we differentiate the gradient with respect to $u$. In component form, the $(k,l)$-th entry of the Hessian is:\n$$\n\\frac{\\partial^2 \\Phi_{\\text{like}}}{\\partial u_l \\partial u_k} = \\frac{\\partial}{\\partial u_l} \\left( -\\sum_{i,j} \\frac{\\partial G_i}{\\partial u_k} (\\Gamma^{-1})_{ij} r_j(u) \\right)\n$$\nUsing the product rule:\n$$\n\\frac{\\partial^2 \\Phi_{\\text{like}}}{\\partial u_l \\partial u_k} = - \\sum_{i,j} \\left( \\frac{\\partial^2 G_i}{\\partial u_l \\partial u_k} (\\Gamma^{-1})_{ij} r_j(u) + \\frac{\\partial G_i}{\\partial u_k} (\\Gamma^{-1})_{ij} \\frac{\\partial r_j}{\\partial u_l} \\right)\n$$\nSince $\\frac{\\partial r_j}{\\partial u_l} = -\\frac{\\partial G_j}{\\partial u_l}$, this becomes:\n$$\n\\frac{\\partial^2 \\Phi_{\\text{like}}}{\\partial u_l \\partial u_k} = \\sum_{i,j} \\frac{\\partial G_i}{\\partial u_k} (\\Gamma^{-1})_{ij} \\frac{\\partial G_j}{\\partial u_l} - \\sum_{i,j} \\frac{\\partial^2 G_i}{\\partial u_l \\partial u_k} (\\Gamma^{-1})_{ij} r_j(u)\n$$\nRe-assembling this into matrix form:\nThe first term is the $(k,l)$-th entry of $J_G(u)^\\top \\Gamma^{-1} J_G(u)$.\nThe second term can be written as a sum over the components of the residual. Let $H_i(u) = \\nabla^2 G_i(u)$ be the Hessian of the $i$-th component of the forward map. Then the second term is:\n$$\n- \\sum_{i,j} (\\Gamma^{-1})_{ij} r_j(u) (H_i(u))_{lk} = - \\sum_i \\left( \\sum_j (\\Gamma^{-1})_{ij} r_j(u) \\right) (H_i(u))_{lk}\n$$\nThe term in parentheses is the $i$-th component of the vector $\\Gamma^{-1} r(u)$. Let this vector be $w(u) = \\Gamma^{-1}(y - G(u))$, often called the vector of weighted residuals.\nThus, the second term is $-\\sum_{i=1}^m w_i(u) H_i(u)$.\n\nCombining these, the Hessian of the likelihood term is:\n$$\n\\nabla^2 \\Phi_{\\text{like}}(u) = J_G(u)^\\top \\Gamma^{-1} J_G(u) - \\sum_{i=1}^m w_i(u) \\nabla^2 G_i(u)\n$$\n\n**3. Total Hessian and Discrepancy**\n\nThe full Hessian of the negative log-posterior is:\n$$\n\\nabla^2 \\Phi(u) = \\nabla^2 \\Phi_{\\text{like}}(u) + \\nabla^2 \\Phi_{\\text{prior}}(u) = \\left( J_G(u)^\\top \\Gamma^{-1} J_G(u) - \\sum_{i=1}^m w_i(u) \\nabla^2 G_i(u) \\right) + C^{-1}\n$$\nRearranging terms to group the Gauss-Newton part:\n$$\n\\nabla^2 \\Phi(u) = \\underbrace{J_G(u)^\\top \\Gamma^{-1} J_G(u) + C^{-1}}_{H_{\\mathrm{GN}}(u)} - \\sum_{i=1}^m \\left[\\Gamma^{-1}(y - G(u))\\right]_i \\nabla^2 G_i(u)\n$$\nThe discrepancy between the true Hessian and the Gauss-Newton approximation is therefore:\n$$\n\\nabla^2 \\Phi(u) - H_{\\mathrm{GN}}(u) = - \\sum_{i=1}^m \\left[\\Gamma^{-1}(y - G(u))\\right]_i \\nabla^2 G_i(u)\n$$\nThis expression is evaluated at the MAP estimate $u^\\star$ to quantify the curvature gap. The gap depends linearly on the weighted residuals and on the second derivatives (nonlinearity) of the forward model. The gap vanishes if either the forward model is linear ($\\nabla^2 G_i(u) = 0$ for all $i$) or if the model fits the data perfectly at $u^\\star$ ($G(u^\\star) = y$, so the residual is zero).\n\n### Algorithmic Design and Implementation\n\nThe algorithm proceeds as follows for each test case:\n\n1.  **Optimization**: The MAP estimate $u^\\star$ is found by minimizing $\\Phi(u)$ starting from the prescribed initial guess $u_0$. We use a trust-region Newton-conjugate-gradient method (`trust-ncg` in `scipy.optimize.minimize`), which is a second-order method that requires the exact gradient $\\nabla\\Phi(u)$ and Hessian $\\nabla^2\\Phi(u)$. These are implemented as Python functions based on the derivations above.\n\n2.  **Curvature Evaluation**: Once $u^\\star$ is determined, we evaluate the two curvature matrices:\n    - The true posterior curvature, $\\nabla^2\\Phi(u^\\star)$, is computed directly using the Hessian function developed for the optimizer.\n    - The Gauss-Newton approximation, $H_{\\mathrm{GN}}(u^\\star)$, is computed using its definition: $H_{\\mathrm{GN}}(u^\\star) = J_G(u^\\star)^\\top \\Gamma^{-1} J_G(u^\\star) + C^{-1}$.\n\n3.  **Diagnostic Computation**:\n    - **Curvature Gap Ratio ($g$)**: The relative difference between the two Hessians is measured in the Frobenius norm:\n      $$ g = \\frac{\\lVert \\nabla^2 \\Phi(u^\\star) - H_{\\mathrm{GN}}(u^\\star) \\rVert_F}{\\lVert \\nabla^2 \\Phi(u^\\star) \\rVert_F} $$\n    - **Symmetrized KL Divergence ($D$)**: The Kullback-Leibler divergence between two multivariate Gaussian distributions $\\mathcal{P}_1 = \\mathcal{N}(\\mu, \\Sigma_1)$ and $\\mathcal{P}_2 = \\mathcal{N}(\\mu, \\Sigma_2)$ with the same mean is $D_{\\mathrm{KL}}(\\mathcal{P}_1 \\| \\mathcal{P}_2) = \\frac{1}{2} \\left( \\mathrm{tr}(\\Sigma_2^{-1} \\Sigma_1) - d + \\log(\\det(\\Sigma_2)/\\det(\\Sigma_1)) \\right)$. We define our two Gaussian approximations as $\\mathcal{P}_{\\text{true}} = \\mathcal{N}(u^\\star, (\\nabla^2 \\Phi(u^\\star))^{-1})$ and $\\mathcal{P}_{\\text{GN}} = \\mathcal{N}(u^\\star, H_{\\mathrm{GN}}(u^\\star)^{-1})$. The symmetrized divergence is $D = D_{\\mathrm{KL}}(\\mathcal{P}_{\\text{true}} \\| \\mathcal{P}_{\\text{GN}}) + D_{\\mathrm{KL}}(\\mathcal{P}_{\\text{GN}} \\| \\mathcal{P}_{\\text{true}})$. The logarithmic terms cancel, yielding the simplified expression:\n      $$ D = \\frac{1}{2} \\left( \\mathrm{tr}\\left(H_{\\mathrm{GN}}(u^\\star) (\\nabla^2 \\Phi(u^\\star))^{-1}\\right) + \\mathrm{tr}\\left(\\nabla^2 \\Phi(u^\\star) (H_{\\mathrm{GN}}(u^\\star))^{-1}\\right) - 2d \\right) $$\n      where $d=2$ is the parameter dimension.\n\n4.  **Aggregation**: The pair of diagnostics $[g, D]$ is computed for each test case and collected into a final list of lists for output.\n\nThis procedure is implemented in the Python code below, with specific functions for each test case's forward model and its derivatives.",
            "answer": "[[0.00000000,0.00000000],[0.00000000,0.00000000],[0.00162095,0.00000001],[0.05268481,0.00014022],[0.20786576,0.00392652]]"
        }
    ]
}