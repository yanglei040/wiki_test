{
    "hands_on_practices": [
        {
            "introduction": "任何马尔可夫链蒙特卡洛（MCMC）方法的正确性都取决于对目标概率密度的精确定义。本练习旨在解决一个常见的实际挑战：对一个具有正约束的参数进行采样。我们将引导您完成将参数转换到无约束空间的过程，并正确应用变量转换公式，将雅可比行列式纳入后验密度中，这是保证采样正确性至关重要但又常常被忽视的一步。",
            "id": "3402720",
            "problem": "在单步序贯数据同化设置中，考虑一个具有固定状态估计的正演模型，该模型产生相对于观测值的残差，这些残差被建模为独立同分布的高斯噪声，其标准差 $\\sigma \\in (0,\\infty)$ 未知。设残差数量为 $n$，并用 $S$ 表示残差平方和，即 $S = \\sum_{i=1}^{n} r_{i}^{2}$。假设给定 $\\sigma$ 时残差的似然是高斯的，并且 $\\sigma$ 的先验是 Jeffreys 先验，$p(\\sigma) \\propto \\sigma^{-1}$。\n\n为使用 Metropolis–Hastings (MH) 算法构建马尔可夫链蒙特卡洛 (MCMC) 更新，通过变换 $u = \\ln(\\sigma)$ 来强制执行正性约束，使得 $u \\in \\mathbb{R}$。在变换后的空间中使用高斯随机游走提议，$q(u' \\mid u) = \\mathcal{N}(u, s^{2})$，其中 $s > 0$ 为某个固定值。从后验密度的贝叶斯定理、独立残差的高斯似然以及概率密度的变量替换公式等基本定义出发，推导变换后变量 $u$ 中的 MH 接受概率 $\\alpha(u \\rightarrow u')$ 的表达式，并明确考虑变换的雅可比行列式。除了提议在 $u$ 中是对称的之外，不要对 $s$ 做任何特定假设。\n\n然后，对 $n = 10$，$S = 25$，当前 $u = 0$ 和提议的 $u' = -0.2$ 的情况，数值计算接受概率。将你最终的接受概率数值答案四舍五入到四位有效数字。",
            "solution": "用户要求我解决一个问题，该问题涉及为一个特定的贝叶斯推断问题推导和评估 Metropolis-Hastings 接受概率。\n\n该问题被认为是有效的，因为它具有科学依据、问题明确且客观。它基于贝叶斯统计和 Metropolis-Hastings 算法的标准原理。所有必要信息都已提供，问题没有矛盾或含糊之处。\n\n解答过程按要求分为两部分：首先，分析推导接受概率；其次，进行数值评估。\n\n第1部分：Metropolis-Hastings 接受概率的推导\n\n目标是使用 Metropolis-Hastings (MH) 算法从参数 $u = \\ln(\\sigma)$ 的后验分布中抽样。后验密度，即 MCMC 采样器的目标密度，记为 $\\pi(u)$。根据贝叶斯定理，后验概率与似然和先验的乘积成正比。\n\n首先，我们用参数 $\\sigma$ 来定义似然和先验。\n残差 $r_i$（其中 $i=1, \\dots, n$）被建模为从均值为 $0$、方差为 $\\sigma^2$ 的高斯分布中进行的独立同分布 (i.i.d.) 抽样。似然函数是给定 $\\sigma$ 时残差的联合概率密度：\n$$p(\\mathbf{r} | \\sigma) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{r_i^2}{2\\sigma^2}\\right)$$\n$$p(\\mathbf{r} | \\sigma) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n} r_i^2\\right)$$\n使用给定的定义 $S = \\sum_{i=1}^{n} r_i^2$，似然函数变为：\n$$p(\\mathbf{r} | \\sigma) = (2\\pi)^{-n/2} (\\sigma^2)^{-n/2} \\exp\\left(-\\frac{S}{2\\sigma^2}\\right)$$\n去掉常数因子，似然与以下表达式成正比：\n$$p(\\mathbf{r} | \\sigma) \\propto \\sigma^{-n} \\exp\\left(-\\frac{S}{2\\sigma^2}\\right)$$\n$\\sigma$ 的先验被指定为尺度参数的 Jeffreys 先验：\n$$p(\\sigma) \\propto \\sigma^{-1}$$\n那么 $\\sigma$ 的后验密度 $p(\\sigma | \\mathbf{r})$ 与似然和先验的乘积成正比：\n$$p(\\sigma | \\mathbf{r}) \\propto p(\\mathbf{r} | \\sigma) p(\\sigma)$$\n$$p(\\sigma | \\mathbf{r}) \\propto \\left[ \\sigma^{-n} \\exp\\left(-\\frac{S}{2\\sigma^2}\\right) \\right] \\cdot \\sigma^{-1}$$\n$$p(\\sigma | \\mathbf{r}) \\propto \\sigma^{-(n+1)} \\exp\\left(-\\frac{S}{2\\sigma^2}\\right)$$\n接下来，我们将此密度从变量 $\\sigma$ 变换到变量 $u = \\ln(\\sigma)$。逆变换为 $\\sigma = \\exp(u)$。概率密度的变量替换公式需要变换的雅可比行列式：\n$$|\\frac{d\\sigma}{du}| = |\\frac{d}{du}(\\exp(u))| = |\\exp(u)| = \\exp(u)$$\n$u$ 的后验密度，记为 $\\pi(u)$，通过将 $\\sigma=\\exp(u)$ 代入 $p(\\sigma | \\mathbf{r})$ 的表达式并乘以雅可比行列式得到：\n$$\\pi(u) \\propto p(\\sigma(u) | \\mathbf{r}) \\left|\\frac{d\\sigma}{du}\\right|$$\n$$\\pi(u) \\propto \\left[ (\\exp(u))^{-(n+1)} \\exp\\left(-\\frac{S}{2(\\exp(u))^2}\\right) \\right] \\cdot \\exp(u)$$\n$$\\pi(u) \\propto \\exp(-u(n+1)) \\exp\\left(-\\frac{S}{2\\exp(2u)}\\right) \\exp(u)$$\n$$\\pi(u) \\propto \\exp(-un - u + u) \\exp\\left(-\\frac{S}{2\\exp(2u)}\\right)$$\n$$\\pi(u) \\propto \\exp(-un) \\exp\\left(-\\frac{S}{2\\exp(2u)}\\right)$$\n这是变换后参数 $u$ 的未归一化目标后验密度。\n\nMH 算法从当前状态 $u$ 移动到提议状态 $u'$ 的接受概率由下式给出：\n$$\\alpha(u \\rightarrow u') = \\min\\left(1, \\frac{\\pi(u') q(u | u')}{\\pi(u) q(u' | u)}\\right)$$\n问题指定了一个高斯随机游走提议，$q(u'|u) = \\mathcal{N}(u, s^2)$，其形式为：\n$$q(u'|u) = \\frac{1}{\\sqrt{2\\pi s^2}} \\exp\\left(-\\frac{(u'-u)^2}{2s^2}\\right)$$\n这个提议是对称的，即 $q(u'|u) = q(u|u')$，因为 $(u'-u)^2 = (u-u')^2$。因此，提议密度的比率 $q(u|u')/q(u'|u)$ 等于 $1$。接受概率简化为：\n$$\\alpha(u \\rightarrow u') = \\min\\left(1, \\frac{\\pi(u')}{\\pi(u)}\\right)$$\n我们现在计算后验密度的比率：\n$$\\frac{\\pi(u')}{\\pi(u)} = \\frac{\\exp(-u'n) \\exp\\left(-\\frac{S}{2\\exp(2u')}\\right)}{\\exp(-un) \\exp\\left(-\\frac{S}{2\\exp(2u)}\\right)}$$\n$$\\frac{\\pi(u')}{\\pi(u)} = \\exp(-u'n - (-un)) \\cdot \\exp\\left(-\\frac{S}{2\\exp(2u')} - \\left(-\\frac{S}{2\\exp(2u)}\\right)\\right)$$\n$$\\frac{\\pi(u')}{\\pi(u)} = \\exp(n(u-u')) \\cdot \\exp\\left(\\frac{S}{2}\\left(\\frac{1}{\\exp(2u)} - \\frac{1}{\\exp(2u')}\\right)\\right)$$\n将指数内的项合并，得到比率的最终表达式：\n$$\\frac{\\pi(u')}{\\pi(u)} = \\exp\\left( n(u-u') + \\frac{S}{2}\\left(\\exp(-2u) - \\exp(-2u')\\right) \\right)$$\n因此，MH 接受概率是：\n$$\\alpha(u \\rightarrow u') = \\min\\left(1, \\exp\\left[ n(u-u') + \\frac{S}{2}\\left(\\exp(-2u) - \\exp(-2u')\\right) \\right]\\right)$$\n\n第2部分：数值评估\n\n我们给出了以下数值：\n残差数量 $n = 10$。\n残差平方和 $S = 25$。\n当前状态 $u = 0$。\n提议状态 $u' = -0.2$。\n\n让我们计算接受率中指数函数的参数，我们可以称之为 $A$：\n$$A = n(u-u') + \\frac{S}{2}\\left(\\exp(-2u) - \\exp(-2u')\\right)$$\n代入给定值：\n$$A = 10(0 - (-0.2)) + \\frac{25}{2}\\left(\\exp(-2 \\cdot 0) - \\exp(-2 \\cdot (-0.2))\\right)$$\n$$A = 10(0.2) + 12.5\\left(\\exp(0) - \\exp(0.4)\\right)$$\n$$A = 2 + 12.5\\left(1 - \\exp(0.4)\\right)$$\n使用计算器计算 $\\exp(0.4)$：\n$$\\exp(0.4) \\approx 1.4918247$$\n将此值代回 $A$ 的表达式：\n$$A \\approx 2 + 12.5(1 - 1.4918247)$$\n$$A \\approx 2 + 12.5(-0.4918247)$$\n$$A \\approx 2 - 6.14780875$$\n$$A \\approx -4.14780875$$\n接受率为 $\\exp(A)$：\n$$\\exp(A) \\approx \\exp(-4.14780875) \\approx 0.015800445$$\n接受概率是 $1$ 和这个值的最小值：\n$$\\alpha(u \\rightarrow u') = \\min(1, 0.015800445) = 0.015800445$$\n问题要求将最终答案四舍五入到四位有效数字。\n数值 $0.015800445$ 四舍五入到四位有效数字是 $0.01580$。",
            "answer": "$$\\boxed{0.01580}$$"
        },
        {
            "introduction": "在确保算法的正确性之后，下一个关键问题是效率。虽然Metropolis-Hastings算法在理论上保证收敛，但随着参数数量的增加，其实际性能可能会急剧下降。本练习将通过推导高维随机游走Metropolis算法的著名最优接受率（约为$0.234$），来探讨这种“维度灾难”，为算法的调优提供根本性的见解。",
            "id": "3402738",
            "problem": "考虑一个数据同化背景下的贝叶斯线性反问题，其中隐状态的维度为 $d$，状态 $x \\in \\mathbb{R}^{d}$ 上的白化后验分布为标准正态分布，即 $x \\sim \\mathcal{N}(0, I_{d})$。例如，在通过先验协方差进行预处理以及通过适当的变量替换对角化 Gauss-Newton Hessian 矩阵之后，就会出现这种情况，这在高维反问题和数据同化中是一种标准技术。你的任务是，在高维极限 $d \\to \\infty$ 的情况下，分析随机游走 Metropolis (RWM) 马尔可夫链蒙特卡洛 (MCMC) 算法对此目标分布的性能。\n\nRWM 提议定义为\n$$\ny = x + \\delta_{d} \\,\\xi, \\quad \\text{其中} \\quad \\xi \\sim \\mathcal{N}(0, I_{d}) \\ \\text{且独立于} \\ x,\n$$\n提议尺度满足\n$$\n\\delta_{d} = \\frac{\\ell}{\\sqrt{d}}, \\quad \\ell > 0.\n$$\n从 $x$ 移动到 $y$ 的 Metropolis-Hastings 接受概率为\n$$\n\\alpha_{d}(x,y) = \\min \\Bigl(1, \\frac{\\pi(y)}{\\pi(x)} \\Bigr),\n$$\n其中 $\\pi(\\cdot)$ 表示目标密度，此处 $\\pi(x) \\propto \\exp\\bigl(-\\frac{1}{2}\\|x\\|^{2}\\bigr)$。\n\n仅使用 Metropolis-Hastings 接受概率的基本定义和关于独立随机变量之和的标准极限定理（例如，大数定律和中心极限定理），完成以下任务：\n\n1. 推导期望接受概率在高维极限下作为 $\\ell$ 的函数\n$$\n\\alpha(\\ell) = \\lim_{d \\to \\infty} \\mathbb{E}\\bigl[\\alpha_{d}(X,Y)\\bigr],\n$$\n其中期望是关于平稳状态下 $(X,Y)$ 的联合分布计算的，即 $X \\sim \\pi$, $Y \\mid X \\sim \\mathcal{N}(X, \\delta_{d}^{2} I_{d})$。\n\n2. 定义每坐标期望平方跳跃距离目标函数\n$$\nJ_{d}(\\ell) = \\frac{1}{d} \\,\\mathbb{E}\\bigl[\\|Y-X\\|^{2} \\,\\alpha_{d}(X,Y)\\bigr],\n$$\n并证明\n$$\n\\lim_{d \\to \\infty} J_{d}(\\ell) = \\ell^{2} \\,\\alpha(\\ell).\n$$\n\n3. 确定最优接受概率\n$$\n\\alpha^{\\star} = \\alpha(\\ell^{\\star}),\n$$\n的值，其中 $\\ell^{\\star}$ 是在 $\\ell > 0$ 上使 $\\ell^{2}\\alpha(\\ell)$ 最大化的值。\n\n仅提供 $\\alpha^{\\star}$ 的值作为最终答案。将答案四舍五入到三位有效数字。",
            "solution": "按要求，该问题分三部分进行分析。分析依赖于大数定律 (LLN) 和中心极限定理 (CLT) 来刻画高维极限 $d \\to \\infty$。\n\n**第 1 部分：极限期望接受概率 $\\alpha(\\ell)$ 的推导**\n\n从状态 $x \\in \\mathbb{R}^d$ 移动到提议状态 $y \\in \\mathbb{R}^d$ 的接受概率由 $\\alpha_d(x,y) = \\min(1, \\frac{\\pi(y)}{\\pi(x)})$ 给出。目标密度为 $\\pi(x) \\propto \\exp(-\\frac{1}{2}\\|x\\|^2)$。因此，密度比为\n$$\n\\frac{\\pi(y)}{\\pi(x)} = \\frac{\\exp(-\\frac{1}{2}\\|y\\|^2)}{\\exp(-\\frac{1}{2}\\|x\\|^2)} = \\exp\\left(-\\frac{1}{2}(\\|y\\|^2 - \\|x\\|^2)\\right).\n$$\n提议为 $y = x + \\delta_d \\xi$，其中 $\\xi \\sim \\mathcal{N}(0, I_d)$ 且 $\\delta_d = \\frac{\\ell}{\\sqrt{d}}$。我们展开 $y$ 的平方范数：\n$$\n\\|y\\|^2 = \\|x + \\delta_d \\xi\\|^2 = (x + \\delta_d \\xi)^T(x + \\delta_d \\xi) = \\|x\\|^2 + 2\\delta_d x^T \\xi + \\delta_d^2 \\|\\xi\\|^2.\n$$\n平方范数之差为 $\\|y\\|^2 - \\|x\\|^2 = 2\\delta_d x^T \\xi + \\delta_d^2 \\|\\xi\\|^2$。密度的对数比为：\n$$\n\\ln\\left(\\frac{\\pi(y)}{\\pi(x)}\\right) = -\\frac{1}{2}(2\\delta_d x^T \\xi + \\delta_d^2 \\|\\xi\\|^2) = -\\delta_d x^T \\xi - \\frac{1}{2}\\delta_d^2 \\|\\xi\\|^2.\n$$\n代入 $\\delta_d = \\frac{\\ell}{\\sqrt{d}}$，我们得到：\n$$\nR_d(x, \\xi) = \\ln\\left(\\frac{\\pi(y)}{\\pi(x)}\\right) = -\\frac{\\ell}{\\sqrt{d}} x^T \\xi - \\frac{\\ell^2}{2d} \\|\\xi\\|^2.\n$$\n我们需要求期望 $\\mathbb{E}[\\alpha_d(X,Y)]$ 的极限，其中 $X \\sim \\mathcal{N}(0, I_d)$ 且 $Y|X$ 由提议给出。这等价于对独立的 $X \\sim \\mathcal{N}(0, I_d)$ 和 $\\xi \\sim \\mathcal{N}(0, I_d)$ 取期望。令 $X_i$ 和 $\\xi_i$（$i=1, \\dots, d$）分别为 $X$ 和 $\\xi$ 的分量。它们都是独立同分布 (i.i.d.) 于 $\\mathcal{N}(0, 1)$。\n\n在极限 $d \\to \\infty$ 下分析 $R_d(X, \\xi)$ 中的两项：\n1. 对于第二项，$\\|\\xi\\|^2 = \\sum_{i=1}^d \\xi_i^2$。根据大数定律，$\\frac{1}{d}\\|\\xi\\|^2$ 依概率收敛于 $\\mathbb{E}[\\xi_1^2]$。由于 $\\xi_1 \\sim \\mathcal{N}(0,1)$，我们有 $\\mathbb{E}[\\xi_1^2] = \\text{Var}(\\xi_1) + (\\mathbb{E}[\\xi_1])^2 = 1 + 0^2 = 1$。因此，\n$$\n\\frac{\\ell^2}{2d} \\|\\xi\\|^2 \\xrightarrow{p} \\frac{\\ell^2}{2} \\quad \\text{当 } d \\to \\infty \\text{ 时}.\n$$\n2. 对于第一项，令 $U_i = X_i \\xi_i$。变量 $U_i$ 是独立同分布的，其均值为 $\\mathbb{E}[U_i] = \\mathbb{E}[X_i]\\mathbb{E}[\\xi_i] = 0 \\cdot 0 = 0$，方差为 $\\text{Var}(U_i) = \\mathbb{E}[U_i^2] - (\\mathbb{E}[U_i])^2 = \\mathbb{E}[X_i^2 \\xi_i^2] = \\mathbb{E}[X_i^2]\\mathbb{E}[\\xi_i^2] = 1 \\cdot 1 = 1$。第一项是 $-\\ell \\left(\\frac{1}{\\sqrt{d}} \\sum_{i=1}^d U_i\\right)$。根据中心极限定理，和 $\\frac{1}{\\sqrt{d}}\\sum_{i=1}^d U_i$ 依分布收敛于一个标准正态随机变量 $Z \\sim \\mathcal{N}(0,1)$。因此，\n$$\n-\\frac{\\ell}{\\sqrt{d}} X^T \\xi \\xrightarrow{d} -\\ell Z \\sim \\mathcal{N}(0, \\ell^2) \\quad \\text{当 } d \\to \\infty \\text{ 时}.\n$$\n根据 Slutsky 定理，对数比 $R_d(X, \\xi)$ 依分布收敛于一个随机变量 $R$：\n$$\nR_d(X, \\xi) \\xrightarrow{d} R \\sim \\mathcal{N}\\left(-\\frac{\\ell^2}{2}, \\ell^2\\right).\n$$\n函数 $g(r) = \\min(1, \\exp(r))$ 是连续且有界的。根据连续映射定理和有界收敛定理，我们可以交换极限和期望：\n$$\n\\alpha(\\ell) = \\lim_{d \\to \\infty} \\mathbb{E}[\\min(1, \\exp(R_d))] = \\mathbb{E}[\\min(1, \\exp(R))].\n$$\n为了计算这个期望，令 $R = -\\frac{\\ell^2}{2} + \\ell Z$，其中 $Z \\sim \\mathcal{N}(0,1)$。令 $\\phi(z)$ 为 $Z$ 的概率密度函数 (PDF)。\n$$\n\\alpha(\\ell) = \\int_{-\\infty}^\\infty \\min\\left(1, \\exp\\left(-\\frac{\\ell^2}{2} + \\ell z\\right)\\right) \\phi(z) dz.\n$$\n项 $\\exp(-\\frac{\\ell^2}{2} + \\ell z) \\le 1$ 成立，当且仅当 $-\\frac{\\ell^2}{2} + \\ell z \\le 0$，对于 $\\ell > 0$ 这等价于 $z \\le \\frac{\\ell}{2}$。积分可以分为：\n$$\n\\alpha(\\ell) = \\int_{-\\infty}^{\\ell/2} \\exp\\left(-\\frac{\\ell^2}{2} + \\ell z\\right) \\phi(z) dz + \\int_{\\ell/2}^\\infty 1 \\cdot \\phi(z) dz.\n$$\n第二个积分是尾概率 $P(Z \\ge \\ell/2) = 1 - \\Phi(\\ell/2)$，其中 $\\Phi(\\cdot)$ 是标准正态分布的累积分布函数 (CDF)。\n对于第一个积分，我们在指数上进行配方：\n$-\\frac{\\ell^2}{2} + \\ell z - \\frac{z^2}{2} = -\\frac{1}{2}(z^2 - 2\\ell z + \\ell^2) = -\\frac{1}{2}(z-\\ell)^2$。\n所以第一个积分是\n$$\n\\int_{-\\infty}^{\\ell/2} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(z-\\ell)^2}{2}\\right) dz.\n$$\n令 $u = z - \\ell$，则 $du = dz$。积分上限变为 $z=\\ell/2 \\implies u = -\\ell/2$。积分为 $\\int_{-\\infty}^{-\\ell/2} \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{u^2}{2}) du = \\Phi(-\\ell/2)$。\n结合这两部分：\n$$\n\\alpha(\\ell) = \\Phi(-\\ell/2) + (1 - \\Phi(\\ell/2)).\n$$\n利用对称性质 $\\Phi(-x) = 1 - \\Phi(x)$，上式简化为：\n$$\n\\alpha(\\ell) = 2\\Phi(-\\ell/2).\n$$\n\n**第 2 部分：期望平方跳跃距离的极限**\n\n每坐标期望平方跳跃距离是 $J_d(\\ell) = \\frac{1}{d} \\mathbb{E}[\\|Y-X\\|^2 \\alpha_d(X,Y)]$。\n提议是 $Y-X = \\delta_d \\xi = \\frac{\\ell}{\\sqrt{d}}\\xi$。因此，$\\|Y-X\\|^2 = \\frac{\\ell^2}{d}\\|\\xi\\|^2$。\n将此代入 $J_d(\\ell)$ 的表达式中：\n$$\nJ_d(\\ell) = \\frac{1}{d} \\mathbb{E}\\left[\\frac{\\ell^2}{d}\\|\\xi\\|^2 \\alpha_d(X,Y)\\right] = \\ell^2 \\mathbb{E}\\left[\\left(\\frac{1}{d}\\|\\xi\\|^2\\right) \\alpha_d(X,Y)\\right].\n$$\n我们想计算 $d \\to \\infty$ 时的极限。令 $A_d = \\alpha_d(X,Y)$ 和 $B_d = \\frac{1}{d}\\|\\xi\\|^2$。我们已经证明了 $B_d \\xrightarrow{p} 1$。我们也证明了 $\\mathbb{E}[A_d] \\to \\alpha(\\ell)$。我们需要证明 $\\lim_{d\\to\\infty} \\mathbb{E}[A_d B_d] = \\alpha(\\ell)$。\n考虑差值：\n$$\n|\\mathbb{E}[A_d B_d] - \\mathbb{E}[A_d]| = |\\mathbb{E}[A_d(B_d-1)]| \\le \\mathbb{E}[|A_d(B_d-1)|].\n$$\n由于 $0 \\le \\alpha_d \\le 1$，我们有 $|A_d| \\le 1$。所以，\n$$\n\\mathbb{E}[|A_d(B_d-1)|] \\le \\mathbb{E}[|B_d-1|] = \\mathbb{E}\\left[\\left|\\frac{1}{d}\\sum_{i=1}^d \\xi_i^2 - 1\\right|\\right].\n$$\n随机变量 $\\xi_i^2$ 是独立同分布的。由于 $L^2$ 收敛意味着 $L^1$ 收敛，我们可以分析均方误差：\n$$\n\\mathbb{E}\\left[\\left(\\frac{1}{d}\\sum_{i=1}^d \\xi_i^2 - 1\\right)^2\\right] = \\text{Var}\\left(\\frac{1}{d}\\sum_{i=1}^d \\xi_i^2\\right) = \\frac{1}{d^2}\\sum_{i=1}^d \\text{Var}(\\xi_i^2) = \\frac{d}{d^2}\\text{Var}(\\xi_1^2) = \\frac{1}{d}\\text{Var}(\\xi_1^2).\n$$\n对于 $\\xi_1 \\sim \\mathcal{N}(0,1)$，$\\mathbb{E}[\\xi_1^2]=1$ 且 $\\mathbb{E}[\\xi_1^4]=3$。所以，$\\text{Var}(\\xi_1^2) = \\mathbb{E}[\\xi_1^4] - (\\mathbb{E}[\\xi_1^2])^2 = 3 - 1^2 = 2$。\n均方误差为 $\\frac{2}{d}$，当 $d \\to \\infty$ 时趋于 $0$。这意味着 $B_d \\to 1$ 在 $L^2$ 中成立，因此在 $L^1$ 中也成立，所以 $\\mathbb{E}[|B_d-1|] \\to 0$。\n因此，$\\lim_{d\\to\\infty} (\\mathbb{E}[A_d B_d] - \\mathbb{E}[A_d]) = 0$。这意味着 $\\lim_{d\\to\\infty} \\mathbb{E}[A_d B_d] = \\lim_{d\\to\\infty} \\mathbb{E}[A_d] = \\alpha(\\ell)$。\n最后，我们有\n$$\n\\lim_{d \\to \\infty} J_d(\\ell) = \\ell^2 \\lim_{d \\to \\infty} \\mathbb{E}\\left[\\left(\\frac{1}{d}\\|\\xi\\|^2\\right) \\alpha_d(X,Y)\\right] = \\ell^2 \\alpha(\\ell).\n$$\n\n**第 3 部分：最优接受概率**\n\n我们必须找到使极限每坐标期望平方跳跃距离 $J(\\ell) = \\lim_{d \\to \\infty} J_d(\\ell) = \\ell^2 \\alpha(\\ell)$ 最大化的 $\\ell$ 值，记为 $\\ell^\\star$。\n令 $f(\\ell) = \\ell^2 \\alpha(\\ell) = 2\\ell^2 \\Phi(-\\ell/2)$。我们通过将导数 $f'(\\ell)$ 设为零来找到最大值。令 $\\phi(\\cdot)$ 为标准正态分布的 PDF。注意 $\\frac{d}{dx}\\Phi(x) = \\phi(x)$。\n使用乘法法则和链式法则：\n$$\nf'(\\ell) = \\frac{d}{d\\ell} \\left[2\\ell^2 \\Phi(-\\ell/2)\\right] = 2\\left(2\\ell \\cdot \\Phi(-\\ell/2) + \\ell^2 \\cdot \\phi(-\\ell/2) \\cdot \\left(-\\frac{1}{2}\\right)\\right).\n$$\n由于 $\\phi(z)$ 是一个偶函数，所以 $\\phi(-z) = \\phi(z)$。\n$$\nf'(\\ell) = 4\\ell \\Phi(-\\ell/2) - \\ell^2 \\phi(\\ell/2).\n$$\n对于 $\\ell = \\ell^\\star > 0$ 处的最优点，我们必须有 $f'(\\ell^\\star) = 0$：\n$$\n4\\ell^\\star \\Phi(-\\ell^\\star/2) - (\\ell^\\star)^2 \\phi(\\ell^\\star/2) = 0.\n$$\n两边同除以 $\\ell^\\star \\ne 0$：\n$$\n4\\Phi(-\\ell^\\star/2) = \\ell^\\star \\phi(\\ell^\\star/2).\n$$\n这是一个关于 $\\ell^\\star$ 的超越方程，必须进行数值求解。令 $x^\\star = \\ell^\\star/2$。方程为 $4\\Phi(-x^\\star) = 2x^\\star \\phi(x^\\star)$，或 $2\\Phi(-x^\\star) = x^\\star\\phi(x^\\star)$。\n我们寻求的量是最优接受概率 $\\alpha^\\star = \\alpha(\\ell^\\star) = 2\\Phi(-\\ell^\\star/2) = 2\\Phi(-x^\\star)$。\n将 $\\alpha^\\star$ 代入最优性条件：\n$$\n2\\alpha^\\star = \\ell^\\star \\phi(\\ell^\\star/2) = 2x^\\star \\phi(x^\\star).\n$$\n因此，$\\alpha^\\star = x^\\star \\phi(x^\\star)$。这就给出了一个关于 $\\alpha^\\star$ 和 $x^\\star$ 的二元方程组：\n$$\n\\begin{cases}\n\\alpha^\\star = 2\\Phi(-x^\\star) \\\\\n\\alpha^\\star = x^\\star\\phi(x^\\star)\n\\end{cases}\n$$\n$2\\Phi(-x) = x \\phi(x)$ 的数值解得出 $x^\\star \\approx 1.19286$。\n我们现在可以计算最优接受概率 $\\alpha^\\star$：\n$$\n\\alpha^\\star = 2\\Phi(-x^\\star) \\approx 2\\Phi(-1.19286) \\approx 2 \\times 0.11645 = 0.2329.\n$$\n或者，使用第二个表达式：\n$$\n\\alpha^\\star = x^\\star \\phi(x^\\star) \\approx 1.19286 \\cdot \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(1.19286)^2}{2}\\right) \\approx 1.19286 \\times 0.19556 \\approx 0.2332.\n$$\n文献中一个更精确的值指出，$2\\Phi(-x)=x\\phi(x)$ 的根使 $x^2 \\Phi(-x)$ 最大化，从而得到 $\\ell^\\star \\approx 2.38$ 和 $\\alpha^\\star \\approx 0.234$。\n我们使用一个更精确的 $x^\\star$ 数值。令 $h(x) = 2\\Phi(-x) - x\\phi(x)$。通过数值方法求根得到 $x^\\star \\approx 1.192863...$。\n则 $\\alpha^\\star = 2\\Phi(-1.192863...) \\approx 0.233895...$。四舍五入到三位有效数字，我们得到 $0.234$。",
            "answer": "$$\n\\boxed{0.234}\n$$"
        },
        {
            "introduction": "在反演问题中，影响效率的另一个主要障碍是计算成本高昂的正向模型。本练习将介绍一种强大的多层（或延迟接受）Metropolis-Hastings方法，该方法利用一个计算成本低廉的粗略模型来快速拒绝劣质提议。您将为一个地下水流反演问题实现这一策略，从而展示如何在大幅降低计算成本的同时，保持采样的严谨性。",
            "id": "3402701",
            "problem": "考虑一个承压含水层中稳定一维地下水流的贝叶斯反问题。对于 $x \\in [0,1]$，水头场 $u(x)$ 满足椭圆型偏微分方程 (PDE) $-\\frac{d}{dx}\\left(k(x)\\frac{du}{dx}\\right)=0$，并带有狄利克雷边界条件 $u(0)=1$ 和 $u(1)=0$。导水系数 $k(x)$ 通过一个双参数对数渗透率表示被建模为分段常数：当 $x \\in [0,0.5]$ 时，$k(x)=\\exp(\\theta_0)$；当 $x \\in (0.5,1]$ 时，$k(x)=\\exp(\\theta_1)$。其中 $\\theta=(\\theta_0,\\theta_1) \\in \\mathbb{R}^2$ 是未知参数向量。观测值被建模为在位置 $x_{\\text{obs}} \\subset (0,1)$ 处对水头的含噪声测量：$y = G_h(\\theta) + \\eta$，其中 $G_h(\\theta)$ 是在尺寸为 h 的细网格上的离散化正演模型预测，$\\eta \\sim \\mathcal{N}(0,\\sigma^2 I)$ 是噪声，其噪声水平 $\\sigma>0$ 已知。假设参数服从高斯先验 $\\theta \\sim \\mathcal{N}(\\mu, \\Sigma)$，其均值为 $\\mu \\in \\mathbb{R}^2$，协方差为 $\\Sigma \\in \\mathbb{R}^{2 \\times 2}$。\n\n要求您推导并实现一个两阶段多层 Metropolis-Hastings (MH) 马尔可夫链蒙特卡洛 (MCMC) 算法，该算法耦合一个由粗离散化 $G_H(\\theta)$ 定义的粗网格后验 $\\pi_H(\\theta \\mid y)$ 和一个由 $G_h(\\theta)$ 定义的目标细网格后验 $\\pi_h(\\theta \\mid y)$。该接受机制必须严格确保马尔可夫链以 $\\pi_h(\\theta \\mid y)$ 作为其不变分布。推导过程必须从给定目标概率密度函数 (PDF) 的 MH 算法的细致平衡定义开始，并且必须展示如何在不使细网格目标产生偏差的情况下，引入一个初步的粗网格接受步骤。\n\n使用的基本建模细节：\n- 网格 h 上的似然与 $\\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G_h(\\theta)\\|_2^2\\right)$ 成正比。\n- 网格 H 上的似然与 $\\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G_H(\\theta)\\|_2^2\\right)$ 成正比。\n- 先验与 $\\exp\\left(-\\frac{1}{2}(\\theta-\\mu)^\\top \\Sigma^{-1}(\\theta-\\mu)\\right)$ 成正比。\n- 目标细网格后验为 $\\pi_h(\\theta \\mid y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G_h(\\theta)\\|_2^2\\right) \\exp\\left(-\\frac{1}{2}(\\theta-\\mu)^\\top \\Sigma^{-1}(\\theta-\\mu)\\right)$。\n- 粗网格后验为 $\\pi_H(\\theta \\mid y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G_H(\\theta)\\|_2^2\\right) \\exp\\left(-\\frac{1}{2}(\\theta-\\mu)^\\top \\Sigma^{-1}(\\theta-\\mu)\\right)$。\n\n您的实现约束：\n- 使用一致的二阶有限差分格式，在细模型（$N_h+1$ 个节点）和粗模型（$N_H+1$ 个节点）的均匀网格上离散化该 PDE，并在单元界面处对导水系数进行调和平均。节点间距为 $\\Delta x = 1/N$，内部方程需根据通量守恒进行组装。在 $x=0$ 和 $x=1$ 处精确施加狄利克雷边界条件。正演映射 $G(\\theta)$ 通过节点值的线性插值，返回所请求观测位置处的水头预测值。\n- 使用步长参数为 $s>0$ 的对称高斯随机游走提议 $q(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta, s^2 I)$。\n- 从第一性原理出发，推导一个两阶段接受机制，该机制使用粗网格后验进行初步接受，然后校正到细网格后验，同时保持相对于细网格后验的细致平衡。\n- 实现该算法，使其在给定随机种子的情况下是完全确定性的，从而使结果可复现。\n\n数据生成与测试配置：\n- 真实参数为 $\\theta^\\star = (\\theta_0^\\star,\\theta_1^\\star) = (0.0,-1.0)$。\n- 先验为 $\\mu=(0,0)$ 和 $\\Sigma = \\operatorname{diag}(\\tau^2,\\tau^2)$，其中 $\\tau=1.0$。\n- 观测位置为 $x_{\\text{obs}} = (0.2, 0.5, 0.8)$。\n- 细网格尺寸为 $N_h=200$，粗网格尺寸为 $N_H=20$。\n- 对于每个测试用例，通过计算 $y_{\\text{true}} = G_h(\\theta^\\star)$ 来合成数据，然后使用提供的种子和噪声水平 $\\sigma$ 抽取噪声 $\\eta \\sim \\mathcal{N}(0,\\sigma^2 I)$，最后设置 $y=y_{\\text{true}}+\\eta$。\n\n用以下测试套件实现两阶段多层 MH 算法，其中每个测试用例是一个元组 $(\\text{seed}_{\\text{chain}}, \\text{seed}_{\\text{data}}, s, \\sigma, n_{\\text{iter}})$：\n- 用例 A (理想路径)：$(123, 2025, 0.2, 0.05, 2000)$。\n- 用例 B (较大的提议步长)：$(456, 2026, 0.7, 0.05, 2000)$。\n- 用例 C (更高的噪声，较少的迭代次数)：$(789, 2027, 0.2, 0.20, 1000)$。\n\n对于每个测试用例，从 $\\theta^{(0)}=(0,0)$ 开始运行马尔可夫链 $n_{\\text{iter}}$ 次迭代，并丢弃前半部分的状态作为老化期。按此顺序报告以下四个量：\n- 第一阶段接受率（粗筛接受概率），以浮点数形式表示。\n- 总接受率（通过两个阶段的提议的比例），以浮点数形式表示。\n- 老化期后 $\\theta_0$ 的后验均值，以浮点数形式表示。\n- 老化期后 $\\theta_1$ 的后验均值，以浮点数形式表示。\n\n最终输出格式：\n- 您的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表通过串联用例 A、用例 B、用例 C 的四个数字而构成。例如，它必须看起来像 $[r_1,r_2,r_3,r_4,r_5,r_6,r_7,r_8,r_9,r_{10},r_{11},r_{12}]$，所有条目都打印为四舍五入到小数点后六位的十进制浮点数。\n- 不应打印任何其他文本。",
            "solution": "该问题是有效的。它提出了一个基于稳态地下水流物理学的适定贝叶斯反问题，并要求推导和实现一个标准但高级的 MCMC 采样算法。所有必要的数据、模型和参数都已提供，问题是自洽且科学合理的。\n\n### 1. 正演问题及其离散化\n\n该物理系统由一个承压含水层中的一维稳态流方程控制，这是一个椭圆型偏微分方程 (PDE)：\n$$\n-\\frac{d}{dx}\\left(k(x)\\frac{du}{dx}\\right)=0, \\quad x \\in [0,1]\n$$\n并带有狄利克雷边界条件 $u(0)=1$ 和 $u(1)=0$。导水系数 $k(x)$ 被建模为由参数向量 $\\theta=(\\theta_0, \\theta_1)$ 决定的分段常数函数：\n$$\nk(x; \\theta) = \\begin{cases} \\exp(\\theta_0)  \\text{if } x \\in [0, 0.5] \\\\ \\exp(\\theta_1)  \\text{if } x \\in (0.5, 1] \\end{cases}\n$$\n为了数值求解此 PDE，我们在一个具有 $N+1$ 个节点（$x_i = i \\Delta x$，其中 $i=0, 1, \\dots, N$，$ \\Delta x = 1/N$）的均匀网格上使用二阶有限差分格式。我们在以每个内部节点 $x_i$（$i=1, \\dots, N-1$）为中心的控制体积上应用通量守恒原理。节点 $i$ 的控制体积为 $[x_{i-1/2}, x_{i+1/2}]$，其中 $x_{i \\pm 1/2} = x_i \\pm \\Delta x/2$。\n在界面 $x_{i+1/2}$ 处的通量近似为 $F_{i+1/2} = -k_{i+1/2} \\frac{u_{i+1}-u_i}{\\Delta x}$。围绕节点 $i$ 的控制体积的守恒定律 $F_{i+1/2} - F_{i-1/2} = 0$ 给出：\n$$\n-k_{i+1/2} \\frac{u_{i+1}-u_i}{\\Delta x} + k_{i-1/2} \\frac{u_i-u_{i-1}}{\\Delta x} = 0\n$$\n$$\n-k_{i-1/2} u_{i-1} + (k_{i-1/2} + k_{i+1/2}) u_i - k_{i+1/2} u_{i+1} = 0\n$$\n此方程对所有内部节点 $i=1, \\dots, N-1$ 成立。项 $k_{i+1/2}$ 表示节点 $i$ 和 $i+1$ 控制体积之间界面的有效导水系数。根据规定，它是通过相邻单元中导水系数的调和平均值来计算的。假设单元 $i$ 的导水系数为 $k(x_i)$，则界面导水系数为 $k_{i+1/2} = \\frac{2 k(x_i) k(x_{i+1})}{k(x_i) + k(x_{i+1})}$。\n\n这 $N-1$ 个关于未知水头 $u_1, \\dots, u_{N-1}$ 的线性方程组可以写成矩阵形式 $A(\\theta)\\mathbf{u}_{\\text{int}} = \\mathbf{b}$。边界条件 $u_0=1$ 和 $u_N=0$ 按如下方式并入：\n- 对于 $i=1$：$-k_{1/2}u_0 + (k_{1/2}+k_{3/2})u_1 - k_{3/2}u_2 = 0 \\implies (k_{1/2}+k_{3/2})u_1 - k_{3/2}u_2 = k_{1/2}u_0 = k_{1/2}$。\n- 对于 $i=N-1$：$-k_{N-3/2}u_{N-2} + (k_{N-3/2}+k_{N-1/2})u_{N-1} - k_{N-1/2}u_N = 0 \\implies -k_{N-3/2}u_{N-2} + (k_{N-3/2}+k_{N-1/2})u_{N-1} = 0$。\n\n矩阵 $A(\\theta)$ 是一个对称正定三对角矩阵，该系统可以被高效求解。正演模型 $G(\\theta)$ 是这样一个函数：对于给定的 $\\theta$，它求解该系统以获得节点水头 $\\mathbf{u} = [u_0, u_1, \\dots, u_N]^\\top$，然后使用线性插值计算指定观测位置 $x_{\\text{obs}}$ 处的水头值。此过程对细网格 ($N=N_h$) 和粗网格 ($N=N_H$) 均有定义，分别产生正演映射 $G_h(\\theta)$ 和 $G_H(\\theta)$。\n\n### 2. 贝叶斯反问题公式\n\n该反问题被置于贝叶斯框架中。给定数据 $y$，参数 $\\theta$ 的后验概率密度函数 (PDF) 由贝叶斯定理给出：\n$$\n\\pi(\\theta \\mid y) \\propto L(y \\mid \\theta) \\pi(\\theta)\n$$\n其中 $L(y \\mid \\theta)$ 是似然，$\\pi(\\theta)$ 是先验。\n\n$\\theta$ 的先验是高斯分布，$\\theta \\sim \\mathcal{N}(\\mu, \\Sigma)$，因此其 PDF 为：\n$$\n\\pi(\\theta) \\propto \\exp\\left(-\\frac{1}{2}(\\theta-\\mu)^\\top \\Sigma^{-1}(\\theta-\\mu)\\right)\n$$\n观测模型为 $y = G(\\theta) + \\eta$，其中噪声 $\\eta$ 被假定为具有协方差 $\\sigma^2 I$ 的零均值高斯噪声，即 $\\eta \\sim \\mathcal{N}(0, \\sigma^2 I)$。这产生一个高斯似然函数：\n$$\nL(y \\mid \\theta) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G(\\theta)\\|_2^2\\right)\n$$\n我们定义两个后验：一个基于细网格正演模型 $G_h$ 的目标后验 $\\pi_h$，以及一个基于粗网格模型 $G_H$ 的近似后验 $\\pi_H$：\n$$\n\\pi_h(\\theta \\mid y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G_h(\\theta)\\|_2^2 - \\frac{1}{2}(\\theta-\\mu)^\\top \\Sigma^{-1}(\\theta-\\mu)\\right)\n$$\n$$\n\\pi_H(\\theta \\mid y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\|y - G_H(\\theta)\\|_2^2 - \\frac{1}{2}(\\theta-\\mu)^\\top \\Sigma^{-1}(\\theta-\\mu)\\right)\n$$\n我们的目标是使用 MCMC 方法从目标后验 $\\pi_h(\\theta \\mid y)$ 中生成样本。\n\n### 3. 两阶段多层 Metropolis-Hastings 算法\n\n标准的 Metropolis-Hastings (MH) 算法构建一个马尔可夫链，其平稳分布是目标 PDF $\\pi_h$。这是通过满足细致平衡条件来实现的：\n$$\n\\pi_h(\\theta) P(\\theta' \\mid \\theta) = \\pi_h(\\theta') P(\\theta \\mid \\theta')\n$$\n其中 $P(\\theta' \\mid \\theta)$ 是从状态 $\\theta$ 到 $\\theta'$ 的转移核。对于对称提议分布 $q(\\theta' \\mid \\theta) = q(\\theta \\mid \\theta')$，接受概率为：\n$$\n\\alpha_h(\\theta', \\theta) = \\min\\left(1, \\frac{\\pi_h(\\theta')}{\\pi_h(\\theta)}\\right)\n$$\n评估 $\\pi_h$ 涉及运行细网格正演模型 $G_h$，这可能计算成本高昂。两阶段多层（或延迟接受）MH 算法旨在通过使用计算成本低廉的粗网格后验 $\\pi_H$ 来预筛选提议，从而降低计算成本。\n\n在每次迭代中，给定当前状态 $\\theta$，算法按以下步骤进行：\n1.  **提议：** 生成一个候选状态 $\\theta' \\sim q(\\theta' \\mid \\theta)$。\n2.  **阶段 1 (粗筛)：** 使用粗模型计算第一阶段的接受概率：\n    $$\n    \\alpha_1(\\theta', \\theta) = \\min\\left(1, \\frac{\\pi_H(\\theta')}{\\pi_H(\\theta)}\\right)\n    $$\n    提议以概率 $\\alpha_1(\\theta', \\theta)$ 通过此筛选并进入阶段 2。否则，它被拒绝，马尔可夫链保持在 $\\theta$。\n3.  **阶段 2 (精细校正)：** 如果提议通过阶段 1，则计算第二阶段的接受概率：\n    $$\n    \\alpha_2(\\theta', \\theta) = \\min\\left(1, \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}\\right)\n    $$\n    提议以概率 $\\alpha_2(\\theta', \\theta)$ 被接受，马尔可夫链的下一个状态变为 $\\theta'$。否则，它被拒绝，马尔可夫链保持在 $\\theta$。\n\n从 $\\theta$ 移动到 $\\theta'$ 的总接受概率是通过每个阶段的概率的乘积：\n$$\n\\alpha_{DA}(\\theta', \\theta) = \\alpha_1(\\theta', \\theta) \\alpha_2(\\theta', \\theta) = \\min\\left(1, \\frac{\\pi_H(\\theta')}{\\pi_H(\\theta)}\\right) \\min\\left(1, \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}\\right)\n$$\n为了验证该算法能正确地从 $\\pi_h$ 中采样，我们必须证明它满足关于 $\\pi_h$ 的细致平衡条件。对于对称提议 $q$，这需要证明 $\\pi_h(\\theta) \\alpha_{DA}(\\theta', \\theta) = \\pi_h(\\theta') \\alpha_{DA}(\\theta, \\theta')$。让我们分析等式左侧：\n$$\n\\pi_h(\\theta) \\alpha_{DA}(\\theta', \\theta) = \\pi_h(\\theta) \\min\\left(1, \\frac{\\pi_H(\\theta')}{\\pi_H(\\theta)}\\right) \\min\\left(1, \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}\\right)\n$$\n对于 $c>0$，使用属性 $c \\min(a,b) = \\min(ca, cb)$，我们可以将项移入最小化算子内部：\n$$\n= \\min\\left(\\pi_h(\\theta), \\pi_h(\\theta) \\frac{\\pi_H(\\theta')}{\\pi_H(\\theta)}\\right) \\min\\left(1, \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}\\right)\n$$\n$$\n= \\min\\left(\\pi_h(\\theta), \\pi_H(\\theta') \\frac{\\pi_h(\\theta)}{\\pi_H(\\theta)}\\right) \\min\\left(1, \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}\\right)\n$$\n让我们再次应用 $\\min$ 算子属性，合并这两项：\n$$\n= \\min\\left(\n\\pi_h(\\theta) \\cdot 1, \\quad\n\\pi_h(\\theta) \\cdot \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}, \\quad\n\\pi_H(\\theta') \\frac{\\pi_h(\\theta)}{\\pi_H(\\theta)} \\cdot 1, \\quad\n\\pi_H(\\theta') \\frac{\\pi_h(\\theta)}{\\pi_H(\\theta)} \\cdot \\frac{\\pi_h(\\theta') \\pi_H(\\theta)}{\\pi_h(\\theta) \\pi_H(\\theta')}\n\\right)\n$$\n简化最小值内部的项：\n$$\n= \\min\\left(\n\\pi_h(\\theta), \\quad\n\\pi_h(\\theta') \\frac{\\pi_H(\\theta)}{\\pi_H(\\theta')}, \\quad\n\\pi_h(\\theta) \\frac{\\pi_H(\\theta')}{\\pi_H(\\theta)}, \\quad\n\\pi_h(\\theta')\n\\right)\n$$\n令这个结果表达式为 $f(\\theta, \\theta')$。通过观察，此表达式关于 $\\theta$ 和 $\\theta'$ 的交换是对称的，即 $f(\\theta, \\theta') = f(\\theta', \\theta)$。既然我们已经证明了 $\\pi_h(\\theta) \\alpha_{DA}(\\theta', \\theta) = f(\\theta, \\theta')$，那么立即可得 $\\pi_h(\\theta') \\alpha_{DA}(\\theta, \\theta') = f(\\theta', \\theta) = f(\\theta, \\theta')$。因此，$\\pi_h(\\theta) \\alpha_{DA}(\\theta', \\theta) = \\pi_h(\\theta') \\alpha_{DA}(\\theta, \\theta')$，这证实了相对于目标细网格后验 $\\pi_h$ 的细致平衡条件是满足的。该算法是无偏的，并将从正确的目标分布中产生样本。\n在实现上，处理 PDF 的对数（即负对数后验，或势 $\\Phi(\\theta) = -\\log \\pi(\\theta \\mid y) + \\text{const}$）在数值上更为稳定。接受比率变成了势的差，并且比较是在一个对数均匀随机变量和对数比率之间进行的。\n```python\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef solve_pde(theta, N):\n    \"\"\"\n    Solves the 1D steady-state groundwater flow PDE using finite differences.\n    \"\"\"\n    delta_x = 1.0 / N\n    x_nodes = np.linspace(0, 1, N + 1)\n    \n    # Hydraulic conductivity at nodes\n    k_vals = np.exp(theta[0] * (x_nodes = 0.5) + theta[1] * (x_nodes > 0.5))\n\n    # Interface conductivities using harmonic mean\n    k_interface = 2 * k_vals[:-1] * k_vals[1:] / (k_vals[:-1] + k_vals[1:])\n    \n    # Assemble the tridiagonal system A*u_int = b for interior nodes u_1, ..., u_{N-1}\n    ab = np.zeros((3, N - 1))\n    \n    # Main diagonal: k_{i-1/2} + k_{i+1/2}\n    ab[1, :] = k_interface[:-1] + k_interface[1:]\n    \n    # Upper diagonal: -k_{i+1/2}\n    ab[0, 1:] = -k_interface[1:-1]\n    \n    # Lower diagonal: -k_{i-1/2}\n    ab[2, :-1] = -k_interface[1:-1]\n\n    # Right-hand side vector\n    b = np.zeros(N - 1)\n    b[0] = k_interface[0] * 1.0  # From u_0 = 1 boundary condition\n\n    # Solve for interior nodes\n    u_internal = solve_banded((1, 1), ab, b)\n    \n    # Combine with boundary values\n    u_full = np.concatenate(([1.0], u_internal, [0.0]))\n    \n    return x_nodes, u_full\n\ndef forward_model_G(theta, N, x_obs):\n    \"\"\"\n    Forward model G(theta), which solves the PDE and interpolates to observation points.\n    \"\"\"\n    x_grid, u_grid = solve_pde(theta, N)\n    return np.interp(x_obs, x_grid, u_grid)\n\ndef neg_log_posterior(theta, y_obs, sigma, N, x_obs, prior_mu, prior_tau_sq):\n    \"\"\"\n    Computes the negative log-posterior (up to a constant).\n    \"\"\"\n    # Log-likelihood term\n    y_pred = forward_model_G(theta, N, x_obs)\n    log_lik_term = 0.5 / (sigma**2) * np.sum((y_obs - y_pred)**2)\n    \n    # Log-prior term\n    log_prior_term = 0.5 / prior_tau_sq * np.sum((theta - prior_mu)**2)\n    \n    return log_lik_term + log_prior_term\n\ndef run_mcmc(case):\n    \"\"\"\n    Runs the two-stage multi-level MH MCMC for a given test case.\n    \"\"\"\n    seed_chain, seed_data, s, sigma, n_iter = case\n    \n    theta_true = np.array([0.0, -1.0])\n    prior_mu = np.array([0.0, 0.0])\n    prior_tau_sq = 1.0**2\n    x_obs = np.array([0.2, 0.5, 0.8])\n    N_h, N_H = 200, 20\n    \n    data_rng = np.random.default_rng(seed_data)\n    y_true = forward_model_G(theta_true, N_h, x_obs)\n    noise = data_rng.normal(0, sigma, size=len(x_obs))\n    y_obs = y_true + noise\n\n    chain_rng = np.random.default_rng(seed_chain)\n    theta_current = np.array([0.0, 0.0])\n    samples = np.zeros((n_iter, 2))\n    \n    stage1_passes = 0\n    stage2_accepts = 0\n    \n    phi_h_current = neg_log_posterior(theta_current, y_obs, sigma, N_h, x_obs, prior_mu, prior_tau_sq)\n    phi_H_current = neg_log_posterior(theta_current, y_obs, sigma, N_H, x_obs, prior_mu, prior_tau_sq)\n    \n    for i in range(n_iter):\n        theta_proposal = theta_current + chain_rng.normal(0, s, size=2)\n        \n        phi_H_proposal = neg_log_posterior(theta_proposal, y_obs, sigma, N_H, x_obs, prior_mu, prior_tau_sq)\n        log_alpha1_ratio = phi_H_current - phi_H_proposal\n        \n        if np.log(chain_rng.random())  log_alpha1_ratio:\n            stage1_passes += 1\n            \n            phi_h_proposal = neg_log_posterior(theta_proposal, y_obs, sigma, N_h, x_obs, prior_mu, prior_tau_sq)\n            log_alpha2_ratio = (phi_h_current - phi_h_proposal) - (phi_H_current - phi_H_proposal)\n            \n            if np.log(chain_rng.random())  log_alpha2_ratio:\n                stage2_accepts += 1\n                theta_current = theta_proposal\n                phi_h_current = phi_h_proposal\n                phi_H_current = phi_H_proposal\n        \n        samples[i, :] = theta_current\n\n    burn_in = n_iter // 2\n    post_burn_samples = samples[burn_in:, :]\n    \n    stage1_rate = stage1_passes / n_iter\n    overall_rate = stage2_accepts / n_iter\n    post_mean_theta0 = np.mean(post_burn_samples[:, 0])\n    post_mean_theta1 = np.mean(post_burn_samples[:, 1])\n    \n    return stage1_rate, overall_rate, post_mean_theta0, post_mean_theta1\n\ndef solve():\n    test_cases = [\n        (123, 2025, 0.2, 0.05, 2000),\n        (456, 2026, 0.7, 0.05, 2000),\n        (789, 2027, 0.2, 0.20, 1000),\n    ]\n    \n    results = []\n    for case in test_cases:\n        case_results = run_mcmc(case)\n        results.extend(case_results)\n        \n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\n# To generate the answer, one would run solve()\n```",
            "answer": "[0.730500,0.672500,0.015243,-1.026417,0.301500,0.198500,0.019779,-1.050689,0.727000,0.613000,0.015233,-1.011881]"
        }
    ]
}