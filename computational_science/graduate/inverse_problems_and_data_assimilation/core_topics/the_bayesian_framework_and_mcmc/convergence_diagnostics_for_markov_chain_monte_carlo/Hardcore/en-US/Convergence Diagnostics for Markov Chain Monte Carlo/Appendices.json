{
    "hands_on_practices": [
        {
            "introduction": "After an MCMC sampler has run, a critical first step is to assess the efficiency of the resulting chain. The presence of autocorrelation means that our samples are not independent, reducing the amount of information we gain with each iteration. This practice guides you through the process of quantifying this inefficiency by calculating the Monte Carlo Standard Error (MCSE) and the Effective Sample Size (ESS) directly from empirical autocorrelation data . By deriving these estimators from the foundational Markov chain central limit theorem, you will develop a concrete understanding of how serial correlation impacts the precision of posterior estimates.",
            "id": "3372636",
            "problem": "In a Bayesian inverse problem for a linear observation model, a Markov chain Monte Carlo (MCMC) algorithm produces a stationary sequence $\\{Y_{t}\\}_{t=1}^{N}$ of evaluations of a scalar posterior functional $Y_{t} = g(\\theta_{t})$, with $N = 50000$. Assume that the Markov chain central limit theorem holds for $\\{Y_{t}\\}$ and that stationarity is achieved. You are given the empirical marginal variance estimate $s^{2} = 2.25$ and empirical autocorrelations $\\{\\hat{\\rho}_{k}\\}_{k=1}^{L}$ up to lag $L = 10$:\n$$\n\\hat{\\rho}_{1} = 0.65,\\quad \\hat{\\rho}_{2} = 0.48,\\quad \\hat{\\rho}_{3} = 0.36,\\quad \\hat{\\rho}_{4} = 0.27,\\quad \\hat{\\rho}_{5} = 0.20,\\\\\n\\hat{\\rho}_{6} = 0.14,\\quad \\hat{\\rho}_{7} = 0.10,\\quad \\hat{\\rho}_{8} = 0.07,\\quad \\hat{\\rho}_{9} = 0.04,\\quad \\hat{\\rho}_{10} = 0.02.\n$$\nStarting only from the definition of the autocovariance function of a strictly stationary process and the Markov chain central limit theoremâ€™s characterization of the asymptotic variance of the sample mean in terms of the spectral density at zero, derive a consistent large-sample estimator for the effective sample size (ESS) and the Monte Carlo standard error (MCSE) of the posterior mean $\\bar{Y}_{N} = \\frac{1}{N}\\sum_{t=1}^{N} Y_{t}$ that uses the empirical autocorrelations $\\{\\hat{\\rho}_{k}\\}_{k=1}^{L}$ via a finite-lag truncation. Then, using the numbers above, compute the numerical values of the ESS and the MCSE. Finally, justify in words how the overlapping batch means (OBM) method can stabilize estimation of the MCSE when the empirical autocorrelations are noisy at moderate and large lags, connecting your justification to consistency for the spectral density at frequency zero.\n\nReport only the MCSE value as your final numerical answer, rounded to four significant figures.",
            "solution": "The problem is valid as it is scientifically grounded in the statistical theory of Markov chain Monte Carlo (MCMC) methods, is well-posed with sufficient information for a unique solution, and is expressed in objective, formal language. It presents a standard, non-trivial task in the analysis of MCMC output.\n\nWe begin by deriving the estimators for the effective sample size (ESS) and the Monte Carlo standard error (MCSE). Let $\\{Y_{t}\\}_{t=1}^{N}$ be a weakly stationary sequence with mean $\\mu = \\mathbb{E}[Y_{t}]$, variance $\\sigma^{2} = \\text{Var}(Y_{t})$, and autocovariance function $\\gamma_{k} = \\text{Cov}(Y_{t}, Y_{t+k})$. The sample mean is given by $\\bar{Y}_{N} = \\frac{1}{N}\\sum_{t=1}^{N} Y_{t}$.\n\nThe variance of the sample mean is\n$$\n\\text{Var}(\\bar{Y}_{N}) = \\text{Var}\\left(\\frac{1}{N}\\sum_{t=1}^{N} Y_{t}\\right) = \\frac{1}{N^{2}}\\sum_{t=1}^{N}\\sum_{s=1}^{N} \\text{Cov}(Y_{t}, Y_{s}) = \\frac{1}{N^{2}}\\sum_{t=1}^{N}\\sum_{s=1}^{N} \\gamma_{t-s}.\n$$\nFor a large sample size $N$, this variance can be approximated by\n$$\n\\text{Var}(\\bar{Y}_{N}) \\approx \\frac{1}{N} \\sum_{k=-(N-1)}^{N-1} \\left(1 - \\frac{|k|}{N}\\right) \\gamma_{k} \\approx \\frac{1}{N} \\sum_{k=-\\infty}^{\\infty} \\gamma_{k}.\n$$\nThe Markov chain central limit theorem (CLT) states that, under suitable conditions,\n$$\n\\sqrt{N}(\\bar{Y}_{N} - \\mu) \\xrightarrow{d} \\mathcal{N}(0, \\sigma_{\\text{asy}}^{2}),\n$$\nwhere the asymptotic variance $\\sigma_{\\text{asy}}^{2}$ is given by the sum of all autocovariances:\n$$\n\\sigma_{\\text{asy}}^{2} = \\sum_{k=-\\infty}^{\\infty} \\gamma_{k}.\n$$\nThis connects to the spectral density of the process, $f(\\omega) = \\frac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty} \\gamma_{k} \\exp(-ik\\omega)$. At frequency $\\omega=0$, we have $f(0) = \\frac{1}{2\\pi}\\sum_{k=-\\infty}^{\\infty} \\gamma_{k}$, which directly yields $\\sigma_{\\text{asy}}^{2} = 2\\pi f(0)$.\n\nDue to stationarity, $\\gamma_{k} = \\gamma_{-k}$. We can rewrite the asymptotic variance using the autocorrelation function $\\rho_{k} = \\gamma_{k}/\\gamma_{0} = \\gamma_{k}/\\sigma^{2}$ as:\n$$\n\\sigma_{\\text{asy}}^{2} = \\gamma_{0} + \\sum_{k=1}^{\\infty} \\gamma_{k} + \\sum_{k=-\\infty}^{-1} \\gamma_{k} = \\gamma_{0} + 2\\sum_{k=1}^{\\infty} \\gamma_{k} = \\sigma^{2}\\left(1 + 2\\sum_{k=1}^{\\infty} \\rho_{k}\\right).\n$$\nThe term $\\tau = 1 + 2\\sum_{k=1}^{\\infty} \\rho_{k}$ is known as the integrated autocorrelation time (IACT).\n\nThe Monte Carlo standard error (MCSE) is the standard deviation of the estimator $\\bar{Y}_{N}$, which for large $N$ is\n$$\n\\text{MCSE}(\\bar{Y}_{N}) = \\sqrt{\\text{Var}(\\bar{Y}_{N})} \\approx \\sqrt{\\frac{\\sigma_{\\text{asy}}^{2}}{N}} = \\sqrt{\\frac{\\sigma^{2}}{N}\\left(1 + 2\\sum_{k=1}^{\\infty} \\rho_{k}\\right)}.\n$$\nThe effective sample size (ESS) is defined as the size of an equivalent independent and identically distributed (i.i.d.) sample that would yield the same variance for the sample mean. For an i.i.d. sample of size $M$, the variance of the mean is $\\sigma^{2}/M$. Equating this to the MCMC variance of the mean gives:\n$$\n\\frac{\\sigma^{2}}{\\text{ESS}} = \\text{Var}(\\bar{Y}_{N}) \\approx \\frac{\\sigma_{\\text{asy}}^{2}}{N} \\implies \\text{ESS} = \\frac{N\\sigma^{2}}{\\sigma_{\\text{asy}}^{2}} = \\frac{N}{1 + 2\\sum_{k=1}^{\\infty} \\rho_{k}}.\n$$\nTo construct consistent estimators, we replace the theoretical quantities with their empirical counterparts. The marginal variance $\\sigma^{2}$ is estimated by the sample variance $s^{2}$. The autocorrelations $\\rho_{k}$ are estimated by the sample autocorrelations $\\hat{\\rho}_{k}$. The problem specifies using a finite-lag truncation, which means we approximate the infinite sum of autocorrelations by a finite sum up to a lag $L$. The resulting estimator for the IACT is:\n$$\n\\hat{\\tau}_{L} = 1 + 2\\sum_{k=1}^{L} \\hat{\\rho}_{k}.\n$$\nThis leads to the following large-sample estimators:\n$$\n\\widehat{\\text{ESS}} = \\frac{N}{\\hat{\\tau}_{L}} = \\frac{N}{1 + 2\\sum_{k=1}^{L} \\hat{\\rho}_{k}}\n$$\n$$\n\\widehat{\\text{MCSE}}(\\bar{Y}_{N}) = \\sqrt{\\frac{s^{2}}{\\widehat{\\text{ESS}}}} = \\sqrt{\\frac{s^{2}\\hat{\\tau}_{L}}{N}} = \\sqrt{\\frac{s^{2}}{N}\\left(1 + 2\\sum_{k=1}^{L} \\hat{\\rho}_{k}\\right)}.\n$$\nNow, we compute the numerical values using the provided data: $N = 50000$, $s^{2} = 2.25$, and sample autocorrelations up to lag $L = 10$.\nFirst, we sum the given empirical autocorrelations:\n$$\n\\sum_{k=1}^{10} \\hat{\\rho}_{k} = 0.65 + 0.48 + 0.36 + 0.27 + 0.20 + 0.14 + 0.10 + 0.07 + 0.04 + 0.02 = 2.33.\n$$\nNext, we compute the estimated IACT:\n$$\n\\hat{\\tau}_{10} = 1 + 2\\sum_{k=1}^{10} \\hat{\\rho}_{k} = 1 + 2(2.33) = 1 + 4.66 = 5.66.\n$$\nUsing this, we calculate the effective sample size:\n$$\n\\widehat{\\text{ESS}} = \\frac{N}{\\hat{\\tau}_{10}} = \\frac{50000}{5.66} \\approx 8833.922.\n$$\nFinally, we compute the Monte Carlo standard error:\n$$\n\\widehat{\\text{MCSE}}(\\bar{Y}_{N}) = \\sqrt{\\frac{s^{2}}{\\widehat{\\text{ESS}}}} = \\sqrt{\\frac{2.25}{50000 / 5.66}} = \\sqrt{\\frac{2.25 \\times 5.66}{50000}} = \\sqrt{\\frac{12.735}{50000}} = \\sqrt{0.0002547} \\approx 0.0159593.\n$$\nRounding to four significant figures, the MCSE is $0.01596$.\n\nFinally, we address the role of the overlapping batch means (OBM) method. The simple finite-lag truncation estimator for the asymptotic variance is $\\hat{\\sigma}_{\\text{asy, L}}^{2} = s^{2}(1 + 2\\sum_{k=1}^{L} \\hat{\\rho}_{k}) = \\sum_{k=-L}^{L} \\hat{\\gamma}_{k}$. This estimator is known to have high variance because the sample autocovariances $\\hat{\\gamma}_{k}$ are themselves noisy, especially for moderate to large lags $k$. Summing these noisy terms directly can lead to unstable estimates of $\\sigma_{\\text{asy}}^{2}$. A negative estimate is even possible if the sum of negative sample correlations is large enough, which is nonsensical for a variance.\n\nThe OBM method offers a more stable alternative. It estimates $\\sigma_{\\text{asy}}^{2}$ using the variance of the means of overlapping batches of observations. For a batch size $b$, the OBM estimator for $\\sigma_{\\text{asy}}^{2}$ is consistent for $N \\to \\infty$ if $b \\to \\infty$ and $b/N \\to 0$. The critical insight is that the OBM estimator is equivalent to a kernel-based spectral density estimator at frequency zero. Specifically, the OBM variance estimator can be shown to be of the form:\n$$\n\\hat{\\sigma}_{\\text{OBM}}^{2} \\approx \\sum_{k=-(b-1)}^{b-1} K\\left(\\frac{k}{b}\\right) \\hat{\\gamma}_{k},\n$$\nwhere $K(x) = 1 - |x|$ for $|x| \\le 1$ is the Bartlett (or triangular) kernel. In contrast, the finite-lag truncation method corresponds to using a rectangular kernel, $K(x) = 1$ for $|x| \\le 1$.\n\nThe Bartlett kernel stabilizes the estimation of $\\sigma_{\\text{asy}}^{2} = 2\\pi f(0)$ by down-weighting the contributions from noisy, high-lag autocovariances. The weights decrease linearly from $1$ at lag $k=0$ to $0$ at lags $|k|=b$. This tapering reduces the variance of the overall estimator compared to the rectangular kernel, which gives equal weight to all lags up to the truncation point $L$. By systematically reducing the influence of the least reliable terms in the sum, the OBM method provides a more robust and statistically consistent estimate of the spectral density at zero, and therefore a more stable estimate of the MCSE.",
            "answer": "$$\\boxed{0.01596}$$"
        },
        {
            "introduction": "While single-chain diagnostics assess efficiency, the Gelman-Rubin statistic, or $\\hat{R}$, addresses the more fundamental question of convergence by comparing the variance within multiple parallel chains to the variance between them. This conceptual exercise challenges you to extend this powerful idea from a single scalar parameter to a high-dimensional parameter vector $\\theta$ . You will explore how to construct a multivariate $\\hat{R}$ that is sensitive to the worst-mixed direction in the parameter space and discover the importance of ensuring the diagnostic is invariant under reparameterization.",
            "id": "3372662",
            "problem": "Consider an inverse problem in data assimilation with a posterior distribution for a parameter vector $\\theta \\in \\mathbb{R}^p$. You run $m$ independent Markov chains, each of length $n$, targeting this posterior. Let $\\bar{\\theta}_{\\cdot j}$ denote the sample mean of chain $j \\in \\{1,\\dots,m\\}$ and $\\bar{\\theta}_{\\cdot\\cdot}$ the overall mean across chains. Let $S_j$ be the unbiased within-chain sample covariance from chain $j$. Define the within-chain covariance estimator $W = \\frac{1}{m} \\sum_{j=1}^m S_j$ and the between-chain covariance estimator $B = \\frac{n}{m-1} \\sum_{j=1}^m (\\bar{\\theta}_{\\cdot j} - \\bar{\\theta}_{\\cdot\\cdot})(\\bar{\\theta}_{\\cdot j} - \\bar{\\theta}_{\\cdot\\cdot})^\\top$. Consider the pooled covariance estimator $\\hat{V} = \\frac{n-1}{n} W + \\frac{1}{n} B$. For any nonzero $a \\in \\mathbb{R}^p$, define the scalar functional $y = a^\\top \\theta$ and the corresponding univariate within- and between-chain variance estimators $W_a = a^\\top W a$ and $B_a = a^\\top B a$, with pooled variance $\\hat{V}_a = \\frac{n-1}{n} W_a + \\frac{1}{n} B_a$.\n\nStarting from the definitions of covariance matrices and the Rayleigh quotient for symmetric matrices, reason about a multivariate potential scale reduction factor that captures the worst-case lack of convergence across all linear functionals $a^\\top \\theta$. Which of the following statements are correct? Select all that apply.\n\nA. A valid multivariate $\\hat{R}$ is given by $\\hat{R}_{\\mathrm{mv}} = \\sqrt{\\lambda_{\\max}(W^{-1} \\hat{V})}$, and it equals $\\sup_{a \\neq 0} \\hat{R}(a^\\top \\theta)$, where $\\hat{R}(a^\\top \\theta) = \\sqrt{\\hat{V}_a / W_a}$ is the univariate potential scale reduction factor for the scalar functional $a^\\top \\theta$.\n\nB. The alternative statistic $\\sqrt{\\operatorname{tr}(\\hat{V}) / \\operatorname{tr}(W)}$ upper-bounds $\\hat{R}(a^\\top \\theta)$ for every nonzero $a$, and it is invariant under any invertible linear reparameterization $\\tilde{\\theta} = T \\theta$ with $T \\in \\mathbb{R}^{p \\times p}$ invertible.\n\nC. The statistic in Option A is invariant under any invertible linear reparameterization $\\tilde{\\theta} = T \\theta$, in the sense that replacing $(W, \\hat{V})$ by $(T W T^\\top, T \\hat{V} T^\\top)$ leaves $\\lambda_{\\max}(W^{-1} \\hat{V})$ unchanged.\n\nD. The statistic $\\sqrt{ \\left( \\det \\hat{V} / \\det W \\right)^{1/p} }$ equals $\\sup_{a \\neq 0} \\hat{R}(a^\\top \\theta)$ and is therefore strictly more sensitive than Option A to poorly mixing directions.\n\nE. The leading eigenvector of $W^{-1} B$ identifies a linear functional $a^\\star$ such that $\\hat{R}(a^{\\star\\top} \\theta)$ attains the maximum over all $a \\neq 0$. In data assimilation, this connects the diagnostic to the worst-mixed linear predictions and can guide reparameterization or preconditioning to improve mixing along that direction.",
            "solution": "The problem statement will first be validated for scientific soundness and logical consistency.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Parameter vector: $\\theta \\in \\mathbb{R}^p$\n- Number of independent Markov chains: $m$\n- Length of each chain: $n$\n- Sample mean of chain $j$: $\\bar{\\theta}_{\\cdot j}$ for $j \\in \\{1,\\dots,m\\}$\n- Overall sample mean: $\\bar{\\theta}_{\\cdot\\cdot}$\n- Unbiased within-chain sample covariance from chain $j$: $S_j$\n- Within-chain covariance estimator: $W = \\frac{1}{m} \\sum_{j=1}^m S_j$\n- Between-chain covariance estimator: $B = \\frac{n}{m-1} \\sum_{j=1}^m (\\bar{\\theta}_{\\cdot j} - \\bar{\\theta}_{\\cdot\\cdot})(\\bar{\\theta}_{\\cdot j} - \\bar{\\theta}_{\\cdot\\cdot})^\\top$\n- Pooled covariance estimator: $\\hat{V} = \\frac{n-1}{n} W + \\frac{1}{n} B$\n- A nonzero vector: $a \\in \\mathbb{R}^p$\n- A scalar functional: $y = a^\\top \\theta$\n- Univariate variance estimators: $W_a = a^\\top W a$, $B_a = a^\\top B a$, $\\hat{V}_a = \\frac{n-1}{n} W_a + \\frac{1}{n} B_a$\n- The task is to reason about a multivariate potential scale reduction factor that captures the worst-case lack of convergence across all linear functionals $a^\\top \\theta$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem describes the multivariate extension of the Gelman-Rubin convergence diagnostic, often denoted $\\hat{R}$ or PSRF. The definitions of the within-chain covariance $W$, between-chain covariance $B$, and the pooled variance estimator $\\hat{V}$ are standard in the literature on MCMC diagnostics (e.g., Gelman et al., \"Bayesian Data Analysis\"; Brooks and Gelman, \"General Methods for Monitoring Convergence of Iterative Simulations\"). The concepts are fundamental to statistical computing and its application in fields like data assimilation. The problem is scientifically sound.\n- **Well-Posed:** The problem is well-defined. It asks to evaluate statements concerning a statistic that maximizes a specific ratio over all possible linear projections. This corresponds to a well-posed generalized eigenvalue problem in linear algebra, which has a unique and meaningful solution.\n- **Objective:** The problem is stated in precise mathematical language, free from any subjective or ambiguous terminology.\n- **Completeness and Consistency:** All necessary definitions and variables are provided. The relationships between the univariate and multivariate statistics are clearly laid out. The setup is internally consistent and complete.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is a well-formulated question about a standard, advanced topic in MCMC convergence diagnostics. The solution process will now proceed.\n\n### Derivation and Solution\n\nThe univariate potential scale reduction factor (PSRF) for a scalar quantity is given by $\\hat{R} = \\sqrt{\\frac{\\hat{V}}{W}}$, where $\\hat{V}$ is an estimate of the marginal posterior variance and $W$ is the average within-chain variance. For a linear functional $y = a^\\top \\theta$, the corresponding estimators for variance are $W_a = a^\\top W a$ and $\\hat{V}_a = a^\\top \\hat{V} a$. The univariate PSRF for this functional is thus:\n$$ \\hat{R}(a^\\top \\theta) = \\sqrt{\\frac{\\hat{V}_a}{W_a}} = \\sqrt{\\frac{a^\\top \\hat{V} a}{a^\\top W a}} $$\nThe problem asks for a multivariate measure that captures the \"worst-case\" lack of convergence. This corresponds to finding the linear functional $a^\\top \\theta$ that maximizes this ratio. We seek to compute:\n$$ \\hat{R}_{\\mathrm{mv}}^2 = \\sup_{a \\neq 0} \\frac{a^\\top \\hat{V} a}{a^\\top W a} $$\nThis expression is a generalized Rayleigh quotient for the pair of symmetric matrices $(\\hat{V}, W)$. Assuming the chains are not degenerate, the within-chain covariance matrix $W$ is positive definite and thus invertible. The supremum of this quotient is given by the largest eigenvalue, $\\lambda_{\\max}$, of the generalized eigenvalue problem $\\hat{V} a = \\lambda W a$. This can be rewritten as a standard eigenvalue problem:\n$$ W^{-1} \\hat{V} a = \\lambda a $$\nTherefore, the supremum is $\\lambda_{\\max}(W^{-1} \\hat{V})$. The worst-case potential scale reduction factor is the square root of this value:\n$$ \\hat{R}_{\\mathrm{mv}} = \\sqrt{\\lambda_{\\max}(W^{-1} \\hat{V})} $$\n\nWith this derivation, we can now evaluate each option.\n\n### Option-by-Option Analysis\n\n**A. A valid multivariate $\\hat{R}$ is given by $\\hat{R}_{\\mathrm{mv}} = \\sqrt{\\lambda_{\\max}(W^{-1} \\hat{V})}$, and it equals $\\sup_{a \\neq 0} \\hat{R}(a^\\top \\theta)$, where $\\hat{R}(a^\\top \\theta) = \\sqrt{\\hat{V}_a / W_a}$ is the univariate potential scale reduction factor for the scalar functional $a^\\top \\theta$.**\n\nAs shown in the derivation above, the multivariate PSRF that represents the worst-case (maximum) PSRF over all one-dimensional projections $a^\\top\\theta$ is precisely $\\sup_{a \\neq 0} \\hat{R}(a^\\top \\theta)$. We demonstrated that this supremum is equal to $\\sqrt{\\lambda_{\\max}(W^{-1} \\hat{V})}$. The statement is a direct and correct consequence of the definition of a \"worst-case\" multivariate diagnostic based on the univariate $\\hat{R}$.\n\n**Verdict: Correct.**\n\n**B. The alternative statistic $\\sqrt{\\operatorname{tr}(\\hat{V}) / \\operatorname{tr}(W)}$ upper-bounds $\\hat{R}(a^\\top \\theta)$ for every nonzero $a$, and it is invariant under any invertible linear reparameterization $\\tilde{\\theta} = T \\theta$ with $T \\in \\mathbb{R}^{p \\times p}$ invertible.**\n\nLet's test the first part: the claim that $\\frac{\\operatorname{tr}(\\hat{V})}{\\operatorname{tr}(W)} \\ge \\frac{a^\\top \\hat{V} a}{a^\\top W a}$ for all $a \\neq 0$. This is generally false. Consider a diagonal case where $W = \\begin{pmatrix} 1  0 \\\\ 0  100 \\end{pmatrix}$ and $\\hat{V} = \\begin{pmatrix} 10  0 \\\\ 0  100 \\end{pmatrix}$. We have $\\operatorname{tr}(W) = 101$ and $\\operatorname{tr}(\\hat{V}) = 110$. The ratio of traces is $\\frac{110}{101} \\approx 1.089$. Now, choose $a = (1, 0)^\\top$. The ratio $\\frac{a^\\top \\hat{V} a}{a^\\top W a} = \\frac{10}{1} = 10$. Since $10 > 1.089$, the ratio of traces does not provide an upper bound for the Rayleigh quotient.\n\nNow, let's test the second part: invariance under reparameterization $\\tilde{\\theta} = T \\theta$. The covariance matrices transform as $\\tilde{W} = T W T^\\top$ and $\\tilde{\\hat{V}} = T \\hat{V} T^\\top$. The statistic becomes $\\sqrt{\\operatorname{tr}(\\tilde{\\hat{V}}) / \\operatorname{tr}(\\tilde{W})} = \\sqrt{\\operatorname{tr}(T \\hat{V} T^\\top) / \\operatorname{tr}(T W T^\\top)}$. In general, $\\operatorname{tr}(AXA^\\top) \\neq \\operatorname{tr}(X)$. For example, if we rescale a single parameter, e.g., $T = \\mathrm{diag}(10, 1, \\dots, 1)$, the trace will change significantly, and so will the ratio. Therefore, the statistic is not invariant.\n\n**Verdict: Incorrect.**\n\n**C. The statistic in Option A is invariant under any invertible linear reparameterization $\\tilde{\\theta} = T \\theta$, in the sense that replacing $(W, \\hat{V})$ by $(T W T^\\top, T \\hat{V} T^\\top)$ leaves $\\lambda_{\\max}(W^{-1} \\hat{V})$ unchanged.**\n\nUnder the reparameterization $\\tilde{\\theta} = T \\theta$, the new covariance matrices are $\\tilde{W} = TWT^\\top$ and $\\tilde{\\hat{V}} = T\\hat{V}T^\\top$. The statistic from Option A, applied to the reparameterized problem, is $\\lambda_{\\max}((\\tilde{W})^{-1} \\tilde{\\hat{V}})$. Let's compute this matrix:\n$$ (\\tilde{W})^{-1} \\tilde{\\hat{V}} = (T W T^\\top)^{-1} (T \\hat{V} T^\\top) = ((T^\\top)^{-1} W^{-1} T^{-1}) (T \\hat{V} T^\\top) = (T^\\top)^{-1} W^{-1} (T^{-1} T) \\hat{V} T^\\top = (T^\\top)^{-1} (W^{-1} \\hat{V}) T^\\top $$\nLet $M = W^{-1} \\hat{V}$ and $P = T^\\top$. The new matrix is $P^{-1} M P$. This is a similarity transformation of the original matrix $M$. A fundamental property of similarity transformations is that they preserve eigenvalues. Therefore, the eigenvalues of $(\\tilde{W})^{-1} \\tilde{\\hat{V}}$ are identical to the eigenvalues of $W^{-1} \\hat{V}$. Consequently, their maximum eigenvalues are equal: $\\lambda_{\\max}((\\tilde{W})^{-1} \\tilde{\\hat{V}}) = \\lambda_{\\max}(W^{-1} \\hat{V})$. The statistic is indeed invariant under linear reparameterizations, which is a desirable property for a convergence diagnostic.\n\n**Verdict: Correct.**\n\n**D. The statistic $\\sqrt{ \\left( \\det \\hat{V} / \\det W \\right)^{1/p} }$ equals $\\sup_{a \\neq 0} \\hat{R}(a^\\top \\theta)$ and is therefore strictly more sensitive than Option A to poorly mixing directions.**\n\nThe first claim is that $\\sqrt{(\\det(\\hat{V}) / \\det(W))^{1/p}} = \\sup_{a \\neq 0} \\hat{R}(a^\\top \\theta)$. From Option A, we know the right-hand side is $\\sqrt{\\lambda_{\\max}(W^{-1} \\hat{V})}$. The claim is thus equivalent to $(\\det(\\hat{V}) / \\det(W))^{1/p} = \\lambda_{\\max}(W^{-1} \\hat{V})$.\nUsing the property $\\det(AB) = \\det(A)\\det(B)$, we have $\\det(\\hat{V}) / \\det(W) = \\det(\\hat{V}) \\det(W^{-1}) = \\det(W^{-1}\\hat{V})$.\nLet $M = W^{-1}\\hat{V}$. Its determinant is the product of its eigenvalues: $\\det(M) = \\prod_{i=1}^p \\lambda_i$.\nThe claim becomes $(\\prod_{i=1}^p \\lambda_i)^{1/p} = \\lambda_{\\max}$. The left side is the geometric mean of the eigenvalues of $M$. The geometric mean of a set of non-negative numbers is less than or equal to their maximum value, with equality holding only if all numbers are identical. In general, the eigenvalues will not be identical, so $(\\prod_{i=1}^p \\lambda_i)^{1/p} \\le \\lambda_{\\max}$. The first part of the statement is false.\nBecause the first part is false, the second part (\"and is therefore strictly more sensitive\") is nonsensical. In fact, by averaging over all eigendirections, the determinant-based statistic is generally *less* sensitive to a single poorly mixing direction than the maximum eigenvalue statistic.\n\n**Verdict: Incorrect.**\n\n**E. The leading eigenvector of $W^{-1} B$ identifies a linear functional $a^\\star$ such that $\\hat{R}(a^{\\star\\top} \\theta)$ attains the maximum over all $a \\neq 0$. In data assimilation, this connects the diagnostic to the worst-mixed linear predictions and can guide reparameterization or preconditioning to improve mixing along that direction.**\n\nThe vector $a^\\star$ that maximizes $\\hat{R}(a^\\top \\theta)$ is the eigenvector corresponding to the largest eigenvalue of $W^{-1}\\hat{V}$. Let's examine the relationship between the eigenvectors of $W^{-1} \\hat{V}$ and $W^{-1} B$.\nWe have the definition $\\hat{V} = \\frac{n-1}{n} W + \\frac{1}{n} B$. Pre-multiplying by $W^{-1}$ gives:\n$$ W^{-1} \\hat{V} = \\frac{n-1}{n} W^{-1}W + \\frac{1}{n} W^{-1}B = \\frac{n-1}{n} I + \\frac{1}{n} W^{-1}B $$\nwhere $I$ is the identity matrix. If $v$ is an eigenvector of $W^{-1}B$ with eigenvalue $\\mu$, then $(W^{-1}B)v = \\mu v$. It follows that:\n$$ (W^{-1}\\hat{V})v = \\left(\\frac{n-1}{n} I + \\frac{1}{n} W^{-1}B\\right)v = \\frac{n-1}{n}v + \\frac{1}{n}\\mu v = \\left(\\frac{n-1+\\mu}{n}\\right)v $$\nThis shows that $v$ is also an eigenvector of $W^{-1}\\hat{V}$, with eigenvalue $\\lambda = \\frac{n-1+\\mu}{n}$. Since this relationship is linear and increasing in $\\mu$, the eigenvector corresponding to the largest eigenvalue of $W^{-1} B$ is the same as the one corresponding to the largest eigenvalue of $W^{-1}\\hat{V}$. Thus, the leading eigenvector of $W^{-1} B$ does indeed identify the direction of worst convergence, $a^\\star$.\n\nThe second part of the statement provides the correct interpretation and application of this finding. Identifying the \"worst-mixed\" linear combination of parameters ($a^{\\star\\top} \\theta$) is extremely valuable for diagnosing MCMC performance. This direction points to the specific way in which the parameter space is difficult to explore (e.g., due to high posterior correlations). This information can then be used to improve the sampler, for instance by re-parameterizing the model to reduce these correlations or by building a more efficient proposal mechanism (preconditioning) for the MCMC algorithm. This is a key practical use of the multivariate PSRF diagnostic in complex applications like data assimilation.\n\n**Verdict: Correct.**",
            "answer": "$$\\boxed{ACE}$$"
        },
        {
            "introduction": "Standard convergence diagnostics treat all parameter directions equally, but in the context of inverse problems, this is often not ideal. The available data may strongly constrain certain parameter combinations while leaving others almost entirely unresolved. This advanced practice asks you to bridge the gap between statistical diagnostics and inverse problem theory by developing a custom, sensitivity-weighted $\\hat{R}$ statistic . You will use the forward model's Jacobian to construct an information matrix that quantifies the data's influence, and then use its eigensystem to build a diagnostic that intelligently prioritizes convergence along the most data-informed directions.",
            "id": "3372654",
            "problem": "You are given $m$ parallel chains from a Markov chain Monte Carlo (MCMC) algorithm targeting a Bayesian inverse problem governed by a differentiable forward operator $\\mathcal{F}:\\mathbb{R}^d \\to \\mathbb{R}^p$ with observation $y \\in \\mathbb{R}^p$ and observational noise covariance $R \\in \\mathbb{R}^{p \\times p}$. Let $S = \\partial \\mathcal{F}/\\partial \\theta \\in \\mathbb{R}^{p \\times d}$ denote the Jacobian matrix of forward sensitivities at a fixed linearization point. Consider $m$ chains of length $n$ in parameter space $\\mathbb{R}^d$, organized as $\\{\\theta_{j,t}\\}_{j=1,\\dots,m;\\, t=1,\\dots,n}$ with $\\theta_{j,t} \\in \\mathbb{R}^d$. The Potential Scale Reduction Factor (PSRF) is a univariate convergence diagnostic defined from basic sample mean and variance summaries across chains for a scalar target, and the multivariate extension can be constructed from projections onto linear functionals of $\\theta$.\n\nStarting from the following base elements only:\n- The definition of the univariate chain-wise sample mean and unbiased sample variance for scalar sequences.\n- The standard univariate Potential Scale Reduction Factor (PSRF), which compares an estimated marginal variance to the within-chain variance using the between-chain variance and the within-chain variance of scalar projections.\n- The linear Gaussian local approximation to the observation model $y \\approx \\mathcal{F}(\\theta^\\star) + S(\\theta - \\theta^\\star) + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0,R)$, which implies the local information metric on parameter perturbations induced by the positive semidefinite matrix $J = S^\\top R^{-1} S \\in \\mathbb{R}^{d \\times d}$.\n\nDerive a scalar convergence diagnostic that emphasizes convergence along directions most informed by the observation $y$. In particular, you must:\n- Justify projecting the multivariate chains onto the eigendirections $v_i \\in \\mathbb{R}^d$ of the information matrix $J$ and diagnosing univariate PSRF along each such direction.\n- Justify combining the directional PSRF values into a single scalar by weighting them according to the corresponding eigenvalues $\\lambda_i \\ge 0$ of $J$, thereby prioritizing directions with greater information.\n- Justify sensible handling of degeneracies, including the cases where some $\\lambda_i = 0$ and numerical issues such as zero within-chain variance in a projected direction.\n\nYour program must implement the resulting sensitivity-weighted PSRF diagnostic as a deterministic function of $(\\{\\theta_{j,t}\\}, S, R)$ for each test case below. All computations must be performed using real arithmetic. If all eigenvalues of $J$ are $0$ (that is, $J$ is the zero matrix), then define the weighted diagnostic to be $1$ by convention because $y$ provides no local information about $\\theta$ through $S$.\n\nDefinitions for test data generation:\n- For any integer $n \\ge 1$, define time indices $t \\in \\{0,1,\\dots,n-1\\}$.\n- For any integer $m \\ge 1$, define chain indices $j \\in \\{0,1,\\dots,m-1\\}$.\n- For any dimension $d \\ge 1$, $\\theta_{j,t} \\in \\mathbb{R}^d$.\n- Let $\\varphi_j$ denote a phase defined as $\\varphi_j = j \\pi / 6$.\n- For each test, the chains are generated deterministically by sinusoidal formulas with specified means and amplitudes, ensuring stationarity about the chain-specific mean.\n\nUnivariate PSRF ingredients for a scalar projection $x_{j,t} \\in \\mathbb{R}$:\n- For each chain $j$, sample mean $\\mu_j = \\frac{1}{n}\\sum_{t=1}^n x_{j,t}$ and unbiased sample variance $s_j^2 = \\frac{1}{n-1}\\sum_{t=1}^n (x_{j,t} - \\mu_j)^2$.\n- Pooled within-chain variance $W = \\frac{1}{m}\\sum_{j=1}^m s_j^2$.\n- Between-chain variance $B = \\frac{n}{m-1} \\sum_{j=1}^m (\\mu_j - \\bar{\\mu})^2$ with $\\bar{\\mu} = \\frac{1}{m}\\sum_{j=1}^m \\mu_j$.\n- The univariate PSRF uses these quantities to compare an estimated marginal variance to $W$.\n\nNumerical stability requirements:\n- When forming any ratio involving a within-chain variance, add a stabilizer $\\varepsilon = 10^{-12}$ to the denominator.\n- If both the between-chain and within-chain variances are numerically zero in a projection, treat the PSRF in that projection as $1$.\n\nTest suite:\n- In all tests, the number of chains is $m = 3$ and the number of iterations per chain is $n = 200$. The parameter dimension is $d = 2$ and the observation dimension is $p = 2$. The observation noise covariance is $R = I_2$, the $2 \\times 2$ identity matrix.\n- Test $1$ (well-converged, informative first direction):\n  - Sensitivity matrix $S_1 = \\begin{bmatrix} 3  0 \\\\ 0  1 \\end{bmatrix}$.\n  - For each chain index $j \\in \\{0,1,2\\}$ and time $t \\in \\{0,1,\\dots,199\\}$:\n    - First component: $\\theta_{j,t}^{(1)} = 0 + 0.2 \\sin\\left(2\\pi t / 200 + \\varphi_j\\right)$.\n    - Second component: $\\theta_{j,t}^{(2)} = 0 + 0.2 \\cos\\left(2\\pi t / 200 + \\varphi_j\\right)$.\n- Test $2$ (poor convergence in the most informative direction):\n  - Sensitivity matrix $S_2 = \\begin{bmatrix} 3  0 \\\\ 0  1 \\end{bmatrix}$.\n  - Chain-specific means along the first component are $\\mu^{(1)}_0 = -0.3$, $\\mu^{(1)}_1 = 0.0$, $\\mu^{(1)}_2 = 0.3$. Along the second component, all chain means are $0$.\n  - For each chain index $j \\in \\{0,1,2\\}$ and time $t \\in \\{0,1,\\dots,199\\}$:\n    - First component: $\\theta_{j,t}^{(1)} = \\mu^{(1)}_j + 0.2 \\sin\\left(2\\pi t / 200 + \\varphi_j\\right)$.\n    - Second component: $\\theta_{j,t}^{(2)} = 0 + 0.2 \\cos\\left(2\\pi t / 200 + \\varphi_j\\right)$.\n- Test $3$ (uninformative sensitivities):\n  - Sensitivity matrix $S_3 = \\begin{bmatrix} 0  0 \\\\ 0  0 \\end{bmatrix}$.\n  - Chains are generated exactly as in Test $1$.\n\nYour task:\n- Implement a program that computes the sensitivity-weighted PSRF diagnostic as a single scalar for each test using only the inputs above.\n- The result for each test must be a floating-point number. When reporting the final results, round each number to exactly $6$ decimal places.\n- Final output format: Your program should produce a single line of output containing the three results as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3]$ with each $r_i$ showing exactly $6$ digits after the decimal point.",
            "solution": "The problem requires the derivation and implementation of a sensitivity-weighted Potential Scale Reduction Factor (PSRF) as a scalar convergence diagnostic for multivariate Markov chain Monte Carlo (MCMC) chains. The derivation must be justified based on first principles and the provided information.\n\nLet the MCMC chains be denoted by $\\{\\theta_{j,t} \\in \\mathbb{R}^d\\}_{j=1,\\dots,m; t=1,\\dots,n}$, representing $m$ chains of length $n$ in a $d$-dimensional parameter space. We are given the Jacobian of the forward model, $S \\in \\mathbb{R}^{p \\times d}$, and the observation noise covariance, $R \\in \\mathbb{R}^{p \\times p}$.\n\nThe derivation proceeds in three parts as requested: justifying the choice of projection directions, justifying the weighted combination of directional diagnostics, and justifying the handling of degeneracies.\n\n### Justification for Projection onto Eigendirections of the Information Matrix $J$\n\nThe foundation for this approach is the local behavior of the posterior distribution in a Bayesian inverse problem. We start with the provided linear Gaussian approximation of the forward model $\\mathcal{F}$ around a linearization point $\\theta^\\star$:\n$$y \\approx \\mathcal{F}(\\theta^\\star) + S(\\theta - \\theta^\\star) + \\varepsilon, \\quad \\text{with } \\varepsilon \\sim \\mathcal{N}(0,R)$$\nThe likelihood of the parameters $\\theta$ given the observation $y$ is determined by the distribution of the residual noise. Assuming the approximation is adequate, the negative log-likelihood function $\\mathcal{L}(\\theta) = -\\log p(y|\\theta)$ is locally quadratic in $\\theta$:\n$$ \\mathcal{L}(\\theta) \\approx \\frac{1}{2} (S(\\theta - \\theta^\\star) - \\delta y)^\\top R^{-1} (S(\\theta - \\theta^\\star) - \\delta y) + \\text{const.} $$\nwhere $\\delta y = y - \\mathcal{F}(\\theta^\\star)$ is the data residual at the linearization point. The Hessian of this negative log-likelihood with respect to $\\theta$ is given by:\n$$ H_{\\mathcal{L}} = \\frac{\\partial^2 \\mathcal{L}}{\\partial \\theta^2} = S^\\top R^{-1} S $$\nThis matrix is precisely the information matrix $J$ provided in the problem statement. In the Bayesian context, the posterior distribution is $p(\\theta|y) \\propto p(y|\\theta)p(\\theta)$. The local shape of the posterior distribution near a mode is largely determined by the likelihood. The matrix $J$ quantifies the curvature of the negative log-posterior (assuming a locally flat prior), which corresponds to the precision (inverse covariance) of a local Gaussian approximation to the posterior.\n\nThe information matrix $J$ is symmetric and positive semidefinite. It admits an eigendecomposition $J = V \\Lambda V^\\top$, where $V = [v_1, \\dots, v_d]$ is an orthogonal matrix whose columns are the eigenvectors $v_i$, and $\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_d)$ is a diagonal matrix of the corresponding non-negative eigenvalues $\\lambda_i \\ge 0$.\n\nThe eigenvectors $v_i$ form an orthonormal basis for the parameter space $\\mathbb{R}^d$. The associated eigenvalues $\\lambda_i$ measure the amount of information the data $y$ provides about parameter perturbations along these directions. A large eigenvalue $\\lambda_i$ implies that the posterior is sharply constrained in the direction $v_i$, meaning the data are highly informative. Conversely, a small or zero eigenvalue $\\lambda_i$ implies that the data provide little to no information about parameters in direction $v_i$, and the posterior variance in this direction is large (i.e., determined by the prior).\n\nConvergence of MCMC chains can be anisotropic. It is critical for the chains to converge in directions well-informed by the data, as these define the key features of the posterior landscape. Diagnosing convergence separately along each eigendirection $v_i$ of $J$ allows us to decouple the problem into a set of one-dimensional analyses, each corresponding to a direction with a specific, quantifiable level of information content. This is a superior strategy to examining convergence along arbitrary axes (e.g., the original parameter coordinates) which may be misaligned with the directions of high and low posterior uncertainty. Therefore, projecting the $d$-dimensional chains $\\theta_{j,t}$ onto each eigenvector $v_i$ to obtain scalar chains $x_{j,t}^{(i)} = v_i^\\top \\theta_{j,t}$ is a statistically and scientifically principled approach to multivariate convergence diagnosis.\n\n### Justification for a Weighted Combination of Directional PSRFs\n\nAfter projecting the chains, we compute a univariate PSRF, denoted $\\hat{R}_i$, for each of the $d$ scalar-valued chain sets $\\{x_{j,t}^{(i)}\\}$. This yields a vector of diagnostic values $[\\hat{R}_1, \\dots, \\hat{R}_d]$. To create a single scalar summary, we must combine these values.\n\nA simple average would treat all directions equally, which is inappropriate. As established, convergence is more critical in directions that the data strongly constrain. A simple maximum would be overly sensitive to slow mixing in an uninformative direction, which may not be a practical concern.\n\nThe eigenvalues $\\lambda_i$ of the information matrix $J$ provide a natural and quantitative measure of the \"importance\" of each direction $v_i$. They are non-negative and directly proportional to the amount of information contributed by the data along each respective eigenvector. It is therefore logical to use these eigenvalues as weights to construct a combined diagnostic.\n\nWe define the sensitivity-weighted PSRF, $\\hat{R}_w$, as the weighted average of the directional PSRFs, where the weights are the eigenvalues:\n$$ \\hat{R}_w = \\frac{\\sum_{i=1}^d \\lambda_i \\hat{R}_i}{\\sum_{i=1}^d \\lambda_i} $$\nThis formulation ensures that the overall diagnostic is most sensitive to a lack of convergence (i.e., a large $\\hat{R}_i$) in directions where the corresponding eigenvalue $\\lambda_i$ is large. Poor convergence in directions with small or zero eigenvalues will have little impact on $\\hat{R}_w$. This aligns with the goal of ensuring that the chains have adequately explored the region of high posterior probability identified by the observational data.\n\n### Handling of Degeneracies and Numerical Issues\n\nThe proposed formulation requires careful handling of several special cases.\n\n1.  **Directions with Zero Information ($\\lambda_i = 0$):** If an eigenvalue $\\lambda_i$ is zero, its contribution to both the numerator and the denominator of the $\\hat{R}_w$ expression is zero. The diagnostic thus naturally and correctly ignores directions that are completely unconstrained by the data according to the linear approximation.\n\n2.  **No Information in Any Direction ($J=0$):** If $S=0$ or for other reasons $J$ is the zero matrix, all eigenvalues are zero ($\\lambda_i=0$ for all $i$). In this case, the denominator $\\sum_i \\lambda_i$ is zero, and the weighted average is undefined. This scenario signifies that the data provide no local information about any parameter direction. As specified in the problem statement, we adopt the convention that the diagnostic value is $1$. This is sensible because a PSRF value of $1$ signifies ideal convergence; if there is no information to guide convergence, there is no data-informed convergence to diagnose, so we can consider it trivially \"converged\" from the data's perspective.\n\n3.  **Zero Within-Chain Variance ($W_i=0$):** The standard PSRF calculation involves division by the pooled within-chain variance, $W_i$. If all chains in a given projection $i$ are static or have collapsed to single points, $W_i$ can be zero, leading to division by zero. The problem specifies adding a small stabilizer $\\varepsilon = 10^{-12}$ to the denominator:\n    $$ \\hat{R}_i = \\sqrt{\\frac{\\frac{n-1}{n} W_i + \\frac{1}{n} B_i}{W_i + \\varepsilon}} $$\n    This prevents numerical failure. If $W_i$ is very small and the between-chain variance $B_i$ is not, $\\hat{R}_i$ will become very large, correctly signaling a severe convergence problem (separated, non-moving chains).\n\n4.  **Zero Between- and Within-Chain Variance ($B_i=0$ and $W_i=0$):** If both $B_i$ and $W_i$ are numerically zero for a projection $i$, it implies that all chains have converged to the exact same point. This is a state of ideal convergence. The problem specifies that in this case, we should set $\\hat{R}_i = 1$. This convention is consistent with the interpretation of PSRF and avoids potential numerical artifacts from the stabilized formula, which would yield $\\sqrt{(n-1)/n} \\approx 1$.\n\nThis complete set of justifications and rules provides a robust and theoretically sound basis for the sensitivity-weighted PSRF diagnostic.\n\n### Implementation Algorithm\n\nBased on the above, the algorithm for a given test case $(\\{\\theta_{j,t}\\}, S, R)$ is as follows:\n1.  Read the input data: MCMC chains $\\{\\theta_{j,t}\\}$, sensitivity matrix $S$, and observation noise covariance $R$. The parameters $m$, $n$, and $d$ are inferred from the dimensions of the chains array.\n2.  Compute the information matrix $J = S^\\top R^{-1} S$.\n3.  Compute the eigenvalues $\\lambda_i$ and corresponding eigenvectors $v_i$ of $J$.\n4.  If the sum of absolute eigenvalues $\\sum_i |\\lambda_i|$ is close to zero (i.e., $J \\approx 0$), return a value of $1.0$ and terminate.\n5.  Initialize `weighted_psrf_sum = 0.0` and `lambda_sum = 0.0`.\n6.  For each eigenvector $v_i$:\n    a. Let $\\lambda_i$ be the corresponding eigenvalue. If $\\lambda_i \\le 0$, skip this direction as it contains no information.\n    b. Project the chains onto this direction: $x_{j,t}^{(i)} = v_i^\\top \\theta_{j,t}$ for all $j,t$.\n    c. Compute the univariate PSRF, $\\hat{R}_i$, for this projected scalar data:\n        i.   For each chain $j$, compute its mean $\\mu_j$ and unbiased sample variance $s_j^2$.\n        ii.  Calculate the pooled within-chain variance $W_i = \\frac{1}{m}\\sum_j s_j^2$.\n        iii. Calculate the between-chain variance $B_i = \\frac{n}{m-1}\\sum_j(\\mu_j - \\bar{\\mu})^2$, where $\\bar{\\mu}=\\frac{1}{m}\\sum_j\\mu_j$.\n        iv.  If $W_i  \\varepsilon$ and $B_i  \\varepsilon$ (where $\\varepsilon=10^{-12}$), set $\\hat{R}_i = 1.0$.\n        v.   Otherwise, compute $\\hat{R}_i = \\sqrt{(\\frac{n-1}{n} W_i + \\frac{1}{n} B_i) / (W_i + \\varepsilon)}$.\n    d. Update the weighted sum: `weighted_psrf_sum += \\lambda_i * \\hat{R}_i`.\n    e. Update the sum of weights: `lambda_sum += \\lambda_i`.\n7.  If `lambda_sum` is zero (e.g., all eigenvalues were non-positive), return $1.0$. Otherwise, compute the final diagnostic as $\\hat{R}_w = \\text{weighted\\_psrf\\_sum} / \\text{lambda\\_sum}$.\n8.  Return $\\hat{R}_w$.\nThis procedure is then applied to each test case.",
            "answer": "[0.997500,2.205244,1.000000]"
        }
    ]
}