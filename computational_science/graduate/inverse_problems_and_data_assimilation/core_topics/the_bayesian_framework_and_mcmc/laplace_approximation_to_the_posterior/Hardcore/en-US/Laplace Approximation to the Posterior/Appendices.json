{
    "hands_on_practices": [
        {
            "introduction": "To build a solid foundation, we begin with the core mechanics of the Laplace approximation in a simple one-parameter setting. This exercise asks you to approximate a posterior distribution by performing a second-order Taylor expansion of its logarithm around the mode, or Maximum A Posteriori (MAP) estimate. By applying this to a Beta-Binomial model, where the exact posterior is known, you will gain a clear, step-by-step understanding of how a potentially complex posterior density can be approximated by a much simpler Gaussian distribution. ",
            "id": "3281857",
            "problem": "A coin with unknown bias $\\theta \\in (0,1)$ is modeled using a Beta-Binomial framework: prior $\\theta \\sim \\mathrm{Beta}(\\alpha,\\beta)$ and, conditional on $\\theta$, the number of heads $X$ in $n$ independent tosses follows $X \\mid \\theta \\sim \\mathrm{Binomial}(n,\\theta)$. You observe $n=200$ tosses and $k=120$ heads. The prior is $\\mathrm{Beta}(\\alpha,\\beta)$ with $\\alpha=8$ and $\\beta=12$. Let $\\pi(\\theta \\mid k)$ denote the posterior density for $\\theta$ given the data.\n\nUsing Taylor's theorem, perform a second-order expansion of the log-posterior $\\ln \\pi(\\theta \\mid k)$ around its Maximum A Posteriori (MAP) value and thereby obtain the Laplace (Gaussian) approximation to the posterior. Use this approximation to estimate the posterior probability that $\\theta > 0.5$. Your reasoning must start from the fundamental definitions of the Beta prior and Binomial likelihood, proceed through the construction of the log-posterior, and then use Taylor's theorem to derive the quadratic approximation around the MAP. Approximating the posterior by a Gaussian on $\\mathbb{R}$ is acceptable here, provided you justify that the MAP is well within $(0,1)$ relative to the curvature. Express the final probability as a decimal and round your answer to four significant figures.",
            "solution": "The user wants to find the Laplace approximation for a Beta-Binomial posterior distribution and use it to estimate a probability.\n\nHere's the step-by-step derivation:\n\n### Step 1: Problem Validation\n\n**1. Extract Givens:**\n*   Model: Beta-Binomial framework.\n*   Prior: $\\theta \\sim \\mathrm{Beta}(\\alpha, \\beta)$ with $\\alpha = 8$ and $\\beta = 12$.\n*   Likelihood: $X \\mid \\theta \\sim \\mathrm{Binomial}(n, \\theta)$.\n*   Data: $n = 200$ tosses, $k = 120$ heads.\n*   Posterior density: $\\pi(\\theta \\mid k)$.\n*   Task:\n    1.  Perform a second-order Taylor expansion of $\\ln \\pi(\\theta \\mid k)$ around the Maximum A Posteriori (MAP) value.\n    2.  Derive the Laplace (Gaussian) approximation to the posterior.\n    3.  Estimate the posterior probability $P(\\theta > 0.5 \\mid k)$ using this approximation.\n    4.  Round the final answer to four significant figures.\n\n**2. Validate Using Extracted Givens:**\n*   **Scientifically Grounded:** The problem uses standard, well-established models (Beta-Binomial) and methods (Laplace approximation) from Bayesian statistics. It is scientifically sound.\n*   **Well-Posed:** The problem provides all necessary information ($\\alpha, \\beta, n, k$) to uniquely determine the posterior distribution and its approximation. The question is specific and has a unique answer.\n*   **Objective:** The problem is stated using precise mathematical and statistical terminology.\n*   **Completeness and Consistency:** The given parameters ($\\alpha=8, \\beta=12$) and data ($n=200, k=120$) are valid and consistent within the specified models. For instance, $0 \\le k \\le n$.\n*   **Relevance:** The problem directly addresses the application of Taylor's theorem to construct an approximation, which is a core concept in numerical methods and scientific computing.\n\n**3. Verdict and Action:**\nThe problem is valid. We will proceed with the solution.\n\n### Step 2: Derivation of the Solution\n\nThe solution proceeds by first identifying the posterior distribution, then finding its mode (the MAP), and finally constructing a Gaussian approximation around that mode.\n\n**1. Posterior Distribution**\nThe prior probability density function (PDF) for $\\theta$ is the Beta distribution:\n$$ \\pi(\\theta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1} $$\nFor this problem, $\\alpha = 8$ and $\\beta = 12$.\n\nThe likelihood of observing $k=120$ heads in $n=200$ tosses is given by the Binomial probability mass function:\n$$ P(k \\mid \\theta) = \\binom{n}{k} \\theta^k (1-\\theta)^{n-k} $$\n\nAccording to Bayes' theorem, the posterior PDF is proportional to the product of the likelihood and the prior:\n$$ \\pi(\\theta \\mid k) \\propto P(k \\mid \\theta) \\pi(\\theta) $$\n$$ \\pi(\\theta \\mid k) \\propto \\left[ \\binom{n}{k} \\theta^k (1-\\theta)^{n-k} \\right] \\left[ \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1} \\right] $$\nIgnoring the constants that do not depend on $\\theta$, we get the kernel of the posterior:\n$$ \\pi(\\theta \\mid k) \\propto \\theta^k (1-\\theta)^{n-k} \\theta^{\\alpha-1} (1-\\theta)^{\\beta-1} $$\n$$ \\pi(\\theta \\mid k) \\propto \\theta^{k+\\alpha-1} (1-\\theta)^{n-k+\\beta-1} $$\nThis is the kernel of a Beta distribution with updated parameters $\\alpha' = k+\\alpha$ and $\\beta' = n-k+\\beta$.\nSubstituting the given values:\n$$ \\alpha' = 120 + 8 = 128 $$\n$$ \\beta' = (200 - 120) + 12 = 80 + 12 = 92 $$\nThus, the exact posterior distribution is $\\theta \\mid k \\sim \\mathrm{Beta}(128, 92)$.\n\n**2. Log-Posterior and its Derivatives**\nThe log-posterior, denoted $L(\\theta) = \\ln \\pi(\\theta \\mid k)$, is (up to an additive constant):\n$$ L(\\theta) = (\\alpha' - 1) \\ln(\\theta) + (\\beta' - 1) \\ln(1-\\theta) $$\n$$ L(\\theta) = (128 - 1) \\ln(\\theta) + (92 - 1) \\ln(1-\\theta) = 127 \\ln(\\theta) + 91 \\ln(1-\\theta) $$\n\nThe first derivative with respect to $\\theta$ is:\n$$ \\frac{dL}{d\\theta} = \\frac{127}{\\theta} - \\frac{91}{1-\\theta} $$\n\nThe second derivative is:\n$$ \\frac{d^2L}{d\\theta^2} = -\\frac{127}{\\theta^2} - \\frac{91}{(1-\\theta)^2} $$\n\n**3. Maximum A Posteriori (MAP) Estimate**\nThe MAP estimate, $\\hat{\\theta}_{MAP}$, is the mode of the posterior distribution, found by setting the first derivative of the log-posterior to zero:\n$$ \\frac{127}{\\hat{\\theta}_{MAP}} - \\frac{91}{1-\\hat{\\theta}_{MAP}} = 0 $$\n$$ 127(1-\\hat{\\theta}_{MAP}) = 91\\hat{\\theta}_{MAP} $$\n$$ 127 - 127\\hat{\\theta}_{MAP} = 91\\hat{\\theta}_{MAP} $$\n$$ 127 = (91 + 127)\\hat{\\theta}_{MAP} = 218\\hat{\\theta}_{MAP} $$\n$$ \\hat{\\theta}_{MAP} = \\frac{127}{218} $$\n\n**4. Taylor Expansion and Laplace Approximation**\nThe Laplace approximation is based on a second-order Taylor series expansion of the log-posterior $L(\\theta)$ around its maximum $\\hat{\\theta}_{MAP}$:\n$$ L(\\theta) \\approx L(\\hat{\\theta}_{MAP}) + \\frac{dL}{d\\theta}\\bigg|_{\\hat{\\theta}_{MAP}} (\\theta - \\hat{\\theta}_{MAP}) + \\frac{1}{2} \\frac{d^2L}{d\\theta^2}\\bigg|_{\\hat{\\theta}_{MAP}} (\\theta - \\hat{\\theta}_{MAP})^2 $$\nSince $\\frac{dL}{d\\theta}\\big|_{\\hat{\\theta}_{MAP}} = 0$ by definition of the MAP, this simplifies to:\n$$ L(\\theta) \\approx L(\\hat{\\theta}_{MAP}) + \\frac{1}{2} \\left( \\frac{d^2L}{d\\theta^2}\\bigg|_{\\hat{\\theta}_{MAP}} \\right) (\\theta - \\hat{\\theta}_{MAP})^2 $$\nThe posterior density is $\\pi(\\theta \\mid k) = \\exp(L(\\theta))$. Approximating this gives:\n$$ \\pi(\\theta \\mid k) \\approx \\exp\\left(L(\\hat{\\theta}_{MAP})\\right) \\exp\\left( \\frac{1}{2} \\left( \\frac{d^2L}{d\\theta^2}\\bigg|_{\\hat{\\theta}_{MAP}} \\right) (\\theta - \\hat{\\theta}_{MAP})^2 \\right) $$\nThis expression has the form of a Gaussian (Normal) density function, $N(\\mu, \\sigma^2)$, which is proportional to $\\exp\\left(-\\frac{(\\theta-\\mu)^2}{2\\sigma^2}\\right)$.\nBy comparing the exponents, we can identify the mean $\\mu$ and variance $\\sigma^2$ of the approximating Gaussian distribution:\n$$ \\mu = \\hat{\\theta}_{MAP} = \\frac{127}{218} $$\n$$ -\\frac{1}{\\sigma^2} = \\frac{d^2L}{d\\theta^2}\\bigg|_{\\hat{\\theta}_{MAP}} $$\nLet's compute the second derivative at the MAP:\n$$ \\frac{d^2L}{d\\theta^2}\\bigg|_{\\hat{\\theta}_{MAP}} = -\\frac{127}{\\hat{\\theta}_{MAP}^2} - \\frac{91}{(1-\\hat{\\theta}_{MAP})^2} $$\nWe have $\\hat{\\theta}_{MAP} = \\frac{127}{218}$ and $1 - \\hat{\\theta}_{MAP} = 1 - \\frac{127}{218} = \\frac{91}{218}$.\n$$ \\frac{d^2L}{d\\theta^2}\\bigg|_{\\hat{\\theta}_{MAP}} = -\\frac{127}{(127/218)^2} - \\frac{91}{(91/218)^2} = -\\frac{218^2}{127} - \\frac{218^2}{91} $$\n$$ = -218^2 \\left( \\frac{1}{127} + \\frac{1}{91} \\right) = -218^2 \\left( \\frac{91+127}{127 \\times 91} \\right) = -218^2 \\left( \\frac{218}{11557} \\right) = -\\frac{218^3}{11557} $$\nThe variance of the Gaussian approximation is therefore:\n$$ \\sigma^2 = - \\left( \\frac{d^2L}{d\\theta^2}\\bigg|_{\\hat{\\theta}_{MAP}} \\right)^{-1} = \\frac{11557}{218^3} $$\nThe Laplace approximation to the posterior is $\\theta \\mid k \\approx N(\\mu, \\sigma^2)$ with:\n$$ \\mu = \\frac{127}{218} \\approx 0.5825688 $$\n$$ \\sigma^2 = \\frac{11557}{10365032} \\approx 0.00111596 $$\nThe parameters of the posterior Beta distribution, $\\alpha' = 128$ and $\\beta' = 92$, are large, and the mode $\\hat{\\theta}_{MAP} \\approx 0.583$ is well away from the boundaries $0$ and $1$ relative to the standard deviation $\\sigma = \\sqrt{\\sigma^2} \\approx 0.0334$. This justifies using a Gaussian approximation on the real line.\n\n**5. Probability Calculation**\nWe need to estimate $P(\\theta > 0.5 \\mid k)$ using the Gaussian approximation $Z \\sim N(\\mu, \\sigma^2)$, where $Z$ is our approximation of $\\theta$. We standardize the variable:\n$$ P(Z > 0.5) = P\\left(\\frac{Z - \\mu}{\\sigma} > \\frac{0.5 - \\mu}{\\sigma}\\right) $$\nLet's calculate the z-score:\n$$ z = \\frac{0.5 - \\mu}{\\sigma} = \\frac{0.5 - \\frac{127}{218}}{\\sqrt{\\frac{11557}{218^3}}} = \\frac{\\frac{109-127}{218}}{\\frac{\\sqrt{11557}}{218\\sqrt{218}}} = \\frac{-18}{\\frac{\\sqrt{11557}}{\\sqrt{218}}} = -18\\sqrt{\\frac{218}{11557}} $$\n$$ z \\approx -2.47163 $$\nThe probability is $P(Y > z)$ where $Y \\sim N(0, 1)$. By symmetry of the standard normal distribution, $P(Y > z) = P(Y < -z) = \\Phi(-z)$, where $\\Phi$ is the standard normal cumulative distribution function (CDF).\n$$ P(\\theta > 0.5 \\mid k) \\approx \\Phi(-z) = \\Phi(2.47163) $$\nUsing a standard normal CDF calculator, $\\Phi(2.47163) \\approx 0.993275$.\nRounding to four significant figures, the probability is $0.9933$.",
            "answer": "$$\n\\boxed{0.9933}\n$$"
        },
        {
            "introduction": "An approximation is only as good as its ability to capture the essential features of the true distribution. This practice moves from the \"how\" to the \"why,\" exploring a key limitation of the Laplace approximation: its inherent symmetry. You will investigate a scenario where the true posterior is skewed, causing its mean to differ from its mode.  By calculating the expected squared loss under both the exact posterior and the Laplace approximation, you will quantify the practical consequences of this discrepancy and develop a critical perspective on when the approximation is reliable.",
            "id": "3137250",
            "problem": "Consider the following Bayesian one-parameter model, where independent observations are drawn from a Poisson distribution and the prior is Gamma. Let $\\{y_i\\}_{i=1}^n$ be independent draws from $\\text{Poisson}(\\lambda)$ with unknown rate $\\lambda > 0$, and use a Gamma prior $\\lambda \\sim \\text{Gamma}(a,b)$ with shape $a > 0$ and rate $b > 0$. Use Bayes' theorem together with the definitions of the Poisson likelihood and Gamma prior as the fundamental base. The Maximum A Posteriori (MAP) estimator is the posterior mode. The Laplace approximation approximates the posterior density near the MAP by a Gaussian distribution centered at the mode, with variance equal to the inverse of the negative second derivative (the negative Hessian for scalar) of the log-posterior at the mode.\n\nYour tasks:\n- Derive the posterior density up to a normalizing constant and express it in a recognized parametric form.\n- Derive the posterior mean and the posterior mode by differentiating the log-posterior.\n- Using the Laplace approximation centered at the posterior mode, derive the scalar Gaussian variance from the second derivative of the log-posterior at the mode.\n- For the convex risk given by the squared loss $L(\\lambda; t) = (\\lambda - t)^2$, compute the expected loss under the exact posterior and under the Laplace approximation. Use only the definitions of expectation for these distributions and quantities you derived.\n- Implement a program that, for each test case in the test suite below, computes the absolute difference between the exact expected loss and the Laplace-approximated expected loss.\n\nTest suite:\n- Case $1$: Prior $(a,b) = (2.5, 1.0)$, data $y = (0,1,2)$, loss target $t$ equal to the posterior mode of this case.\n- Case $2$: Prior $(a,b) = (0.3, 1.0)$, data $y = (1)$, loss target $t$ equal to the sample mean of the data for this case.\n- Case $3$: Prior $(a,b) = (2.0, 1.0)$, data $y = (10,12,9,11,8)$, loss target $t = 10.0$.\n- Case $4$: Prior $(a,b) = (3.0, 0.5)$, data $y = (0,0,0,1)$, loss target $t = 0.8$.\n\nProgram requirements:\n- Compute, for each case, the posterior shape $\\alpha$ and rate $\\beta$, the posterior mean and mode, the Laplace variance at the mode, and the expected squared loss under the exact posterior and under the Laplace approximation.\n- For each case, output the absolute difference between the exact expected loss and the Laplace-approximated expected loss as a floating-point number.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each number rounded to six decimal places (e.g., $[0.123456,0.000001,2.718282,1.000000]$).\n\nAll mathematical symbols, variables, functions, operators, and numbers in this problem statement are written using LaTeX. There are no physical units, angles, or percentages involved in this problem; all numerical outputs must be real numbers without units.",
            "solution": "The problem requires a thorough analysis of a Bayesian one-parameter model involving a Poisson likelihood and a Gamma prior. This framework is a classic example of conjugate families in Bayesian statistics. We will proceed by first validating the problem statement, which is found to be sound, and then systematically deriving the required quantities.\n\nThe model is specified as follows:\n- The data $\\{y_i\\}_{i=1}^n$ are independent and identically distributed draws from a Poisson distribution with parameter $\\lambda$: $y_i \\mid \\lambda \\sim \\text{Poisson}(\\lambda)$.\n- The prior distribution for the unknown parameter $\\lambda$ is a Gamma distribution with shape parameter $a$ and rate parameter $b$: $\\lambda \\sim \\text{Gamma}(a, b)$.\n\nThe probability mass function (PMF) for a Poisson distribution is $P(y \\mid \\lambda) = \\frac{e^{-\\lambda}\\lambda^y}{y!}$ for $y \\in \\{0, 1, 2, \\dots\\}$. The probability density function (PDF) for a Gamma distribution is $p(\\lambda \\mid a, b) = \\frac{b^a}{\\Gamma(a)}\\lambda^{a-1}e^{-b\\lambda}$ for $\\lambda > 0$.\n\nFirst, we derive the posterior density of $\\lambda$. By Bayes' theorem, the posterior density is proportional to the product of the likelihood and the prior density:\n$$ p(\\lambda \\mid y_1, \\dots, y_n) \\propto P(y_1, \\dots, y_n \\mid \\lambda) \\, p(\\lambda) $$\nGiven that the observations are independent, the likelihood is the product of the individual PMFs:\n$$ P(y_1, \\dots, y_n \\mid \\lambda) = \\prod_{i=1}^n \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!} = \\frac{e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n y_i}}{\\prod_{i=1}^n y_i!} $$\nAs a function of $\\lambda$, the likelihood is proportional to $e^{-n\\lambda} \\lambda^{\\sum y_i}$. The prior density is proportional to $\\lambda^{a-1}e^{-b\\lambda}$.\nCombining these, the posterior density is:\n$$ p(\\lambda \\mid \\mathbf{y}) \\propto \\left(e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n y_i}\\right) \\left(\\lambda^{a-1}e^{-b\\lambda}\\right) $$\n$$ p(\\lambda \\mid \\mathbf{y}) \\propto \\lambda^{(\\sum y_i + a) - 1} e^{-(n+b)\\lambda} $$\nThis expression is the kernel of a Gamma distribution. Thus, the posterior distribution is also a Gamma distribution, which demonstrates the conjugacy of the Gamma prior for the Poisson likelihood. The posterior distribution is $\\lambda \\mid \\mathbf{y} \\sim \\text{Gamma}(\\alpha, \\beta)$, with updated parameters:\n- Posterior shape: $\\alpha = \\sum_{i=1}^n y_i + a$\n- Posterior rate: $\\beta = n + b$\n\nSecond, we derive the posterior mean and posterior mode. For a Gamma distribution $\\text{Gamma}(\\alpha, \\beta)$, the mean and mode are well-known quantities.\n- The posterior mean is $E[\\lambda \\mid \\mathbf{y}] = \\frac{\\alpha}{\\beta} = \\frac{\\sum y_i + a}{n + b}$.\n- The posterior mode (Maximum A Posteriori estimate, $\\lambda_{\\text{MAP}}$) is found by maximizing the posterior density. For a Gamma distribution with shape $\\alpha > 1$, the mode is $\\lambda_{\\text{MAP}} = \\frac{\\alpha - 1}{\\beta} = \\frac{\\sum y_i + a - 1}{n + b}$. All test cases provided in the problem satisfy the condition $\\alpha > 1$.\n\nThird, we derive the variance for the Laplace approximation. The Laplace approximation approximates the posterior density with a Gaussian (Normal) distribution centered at the posterior mode, $\\lambda_{\\text{MAP}}$. The variance, which we denote $\\sigma_L^2$, is the inverse of the negative of the second derivative of the log-posterior density evaluated at the mode.\nThe log-posterior, up to an additive constant, is:\n$$ \\log p(\\lambda \\mid \\mathbf{y}) = (\\alpha-1)\\log\\lambda - \\beta\\lambda + C $$\nThe first derivative with respect to $\\lambda$ is:\n$$ \\frac{d}{d\\lambda} \\log p(\\lambda \\mid \\mathbf{y}) = \\frac{\\alpha-1}{\\lambda} - \\beta $$\nThe second derivative is:\n$$ \\frac{d^2}{d\\lambda^2} \\log p(\\lambda \\mid \\mathbf{y}) = -\\frac{\\alpha-1}{\\lambda^2} $$\nEvaluating the negative of the second derivative (the observed Fisher information) at the mode $\\lambda_{\\text{MAP}} = \\frac{\\alpha - 1}{\\beta}$:\n$$ J(\\lambda_{\\text{MAP}}) = -\\left(-\\frac{\\alpha-1}{(\\frac{\\alpha-1}{\\beta})^2}\\right) = \\frac{\\alpha-1}{(\\alpha-1)^2 / \\beta^2} = \\frac{\\beta^2}{\\alpha-1} $$\nThe variance of the Laplace approximation is the inverse of this quantity:\n$$ \\sigma_L^2 = [J(\\lambda_{\\text{MAP}})]^{-1} = \\frac{\\alpha-1}{\\beta^2} $$\nThus, the Laplace approximation to the posterior is $\\lambda_{\\text{approx}} \\sim N(\\mu_L, \\sigma_L^2)$, where the mean is $\\mu_L = \\lambda_{\\text{MAP}} = \\frac{\\alpha-1}{\\beta}$ and the variance is $\\sigma_L^2 = \\frac{\\alpha-1}{\\beta^2}$.\n\nFourth, we compute the expected squared loss $L(\\lambda; t) = (\\lambda - t)^2$. The expected loss under a probability distribution for $\\lambda$ is given by $E[(\\lambda-t)^2]$. This can be expanded using the definition of variance, $\\text{Var}(\\lambda) = E[\\lambda^2] - (E[\\lambda])^2$:\n$$ E[(\\lambda-t)^2] = E[\\lambda^2 - 2t\\lambda + t^2] = E[\\lambda^2] - 2tE[\\lambda] + t^2 $$\n$$ E[(\\lambda-t)^2] = (\\text{Var}(\\lambda) + (E[\\lambda])^2) - 2tE[\\lambda] + t^2 = \\text{Var}(\\lambda) + (E[\\lambda] - t)^2 $$\nThis formula relates the expected loss to the variance and the squared bias of the distribution's mean relative to the target $t$.\n\nWe apply this formula to both the exact posterior and the Laplace approximation.\n- For the exact posterior, $\\lambda \\mid \\mathbf{y} \\sim \\text{Gamma}(\\alpha, \\beta)$:\n  - Mean: $E_{\\text{post}}[\\lambda] = \\frac{\\alpha}{\\beta}$\n  - Variance: $\\text{Var}_{\\text{post}}(\\lambda) = \\frac{\\alpha}{\\beta^2}$\n  - Expected loss: $E_{\\text{exact}} = \\text{Var}_{\\text{post}}(\\lambda) + (E_{\\text{post}}[\\lambda] - t)^2 = \\frac{\\alpha}{\\beta^2} + \\left(\\frac{\\alpha}{\\beta} - t\\right)^2$.\n- For the Laplace approximation, $\\lambda_{\\text{approx}} \\sim N(\\mu_L, \\sigma_L^2)$:\n  - Mean: $E_{\\text{Laplace}}[\\lambda] = \\mu_L = \\frac{\\alpha-1}{\\beta}$\n  - Variance: $\\text{Var}_{\\text{Laplace}}(\\lambda) = \\sigma_L^2 = \\frac{\\alpha-1}{\\beta^2}$\n  - Expected loss: $E_{\\text{Laplace}} = \\text{Var}_{\\text{Laplace}}(\\lambda) + (E_{\\text{Laplace}}[\\lambda] - t)^2 = \\frac{\\alpha-1}{\\beta^2} + \\left(\\frac{\\alpha-1}{\\beta} - t\\right)^2$.\n\nThe program will implement these final formulas to compute the absolute difference $|E_{\\text{exact}} - E_{\\text{Laplace}}|$ for each test case.",
            "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite by calculating the absolute difference\n    between the expected squared loss under the exact posterior and under the Laplace approximation.\n    \"\"\"\n    \n    # Test suite definition: (prior_a, prior_b, data_y, t_config)\n    # t_config is a tuple (type, value) where type is 'mode', 'mean', or 'value'.\n    test_cases = [\n        (2.5, 1.0, [0, 1, 2], ('mode', None)),\n        (0.3, 1.0, [1], ('mean', None)),\n        (2.0, 1.0, [10, 12, 9, 11, 8], ('value', 10.0)),\n        (3.0, 0.5, [0, 0, 0, 1], ('value', 0.8)),\n    ]\n\n    results = []\n\n    for case in test_cases:\n        a_prior, b_prior, y, t_config = case\n        \n        # Convert y to a numpy array for easier calculations\n        y = np.array(y)\n        \n        # Calculate sufficient statistics from data\n        n = len(y)\n        sum_y = np.sum(y)\n        \n        # Calculate posterior parameters\n        # Posterior is Gamma(alpha, beta)\n        alpha_post = sum_y + a_prior\n        beta_post = float(n + b_prior)\n\n        # Ensure posterior mode is well-defined (alpha > 1)\n        if alpha_post <= 1:\n            # This case is not expected based on problem validation\n            # but good practice to handle.\n            results.append(np.nan)\n            continue\n\n        # Determine the loss target t based on the configuration\n        t_type, t_val = t_config\n        t = 0.0\n        if t_type == 'value':\n            t = t_val\n        elif t_type == 'mode':\n            # Posterior mode (MAP)\n            t = (alpha_post - 1) / beta_post\n        elif t_type == 'mean':\n            # Sample mean of the data\n            t = np.mean(y)\n\n        # === Calculations for the exact posterior: Gamma(alpha_post, beta_post) ===\n        \n        # Mean of the exact posterior\n        mean_exact = alpha_post / beta_post\n        # Variance of the exact posterior\n        var_exact = alpha_post / (beta_post**2)\n        # Expected squared loss for the exact posterior\n        expected_loss_exact = var_exact + (mean_exact - t)**2\n\n        # === Calculations for the Laplace approximation: Normal(mu_L, sigma_L^2) ===\n        \n        # Mean of the Laplace approximation is the posterior mode\n        mean_laplace = (alpha_post - 1) / beta_post\n        # Variance of the Laplace approximation\n        var_laplace = (alpha_post - 1) / (beta_post**2)\n        # Expected squared loss for the Laplace approximation\n        expected_loss_laplace = var_laplace + (mean_laplace - t)**2\n\n        # Calculate the absolute difference between the two expected losses\n        abs_diff = abs(expected_loss_exact - expected_loss_laplace)\n        results.append(abs_diff)\n    \n    # Format the output as a comma-separated list of strings with 6 decimal places\n    output_str = \",\".join([f\"{res:.6f}\" for res in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```"
        },
        {
            "introduction": "The true power of the Laplace approximation is its applicability to high-dimensional and even infinite-dimensional inverse problems. This advanced practice challenges you to generalize the concept from a single parameter to an entire unknown function.  You will derive the MAP estimate and the Hessian, which is now a differential operator, for a nonlinear inverse problem defined on a function space. Performing a spectral analysis of this Hessian operator will reveal the posterior covariance structure, providing a powerful tool for quantifying uncertainty in complex models.",
            "id": "3395972",
            "problem": "Consider an inverse problem on the one-dimensional periodic domain $D = [0, 2\\pi]$ with periodic boundary conditions. The unknown field is $u \\in L^{2}(D)$, and the forward map is the nonlinear operator $G(u): L^{2}(D) \\to L^{2}(D)$ given by $G(u)(x) = u(x)^{2}$. Observations are modeled by $y = G(u_{\\mathrm{true}}) + \\eta$, where $\\eta$ is a mean-zero Gaussian noise field with covariance operator $\\Gamma = \\sigma^{2} I$, meaning the data misfit is measured in the weighted $L^{2}$ norm induced by $\\Gamma^{-1} = \\sigma^{-2} I$. The prior on $u$ is Gaussian $\\mathcal{N}(0, \\mathcal{C}_{0})$ with precision operator $\\mathcal{C}_{0}^{-1} = \\alpha (I - \\ell^{2} \\Delta)$, where $\\alpha > 0$, $\\ell > 0$, and $\\Delta$ is the Laplacian on $D$ with periodic boundary conditions.\n\nLet the observed data be spatially constant, $y(x) = y_{0}$ for all $x \\in D$, with $y_{0} > \\frac{\\sigma^{2}}{2} \\alpha$. Work at the level of the Maximum A Posteriori (MAP) estimate and the Laplace approximation to the posterior. Starting from the negative log posterior functional\n$$\n\\Phi(u) = \\frac{1}{2\\sigma^{2}} \\int_{D} \\big(y(x) - u(x)^{2}\\big)^{2} \\, dx + \\frac{1}{2} \\langle u, \\mathcal{C}_{0}^{-1} u \\rangle_{L^{2}(D)},\n$$\nperform the following:\n\n- Derive the first-order optimality condition and solve for the spatially constant MAP solution $u^{*}$ under the ansatz $u(x) \\equiv c$, expressing $c$ in terms of $\\alpha$, $\\sigma^{2}$, and $y_{0}$.\n\n- Compute the Hessian operator $H = D^{2} \\Phi(u^{*})$ at the MAP, carefully accounting for contributions from both the prior and the data misfit.\n\n- Carry out the spectral analysis of $H$ in the Fourier basis $\\{\\phi_{k}(x)\\}_{k \\in \\mathbb{Z}}$ with $\\phi_{k}(x) = (2\\pi)^{-1/2} \\exp(i k x)$, and deduce the Laplace-approximate posterior covariance operator $\\mathcal{C}_{\\mathrm{post}} \\approx H^{-1}$.\n\nReport, as your final answer, a single closed-form analytic expression for the eigenvalues $c_{k}$ of $\\mathcal{C}_{\\mathrm{post}}$ as a function of the Fourier mode $k \\in \\mathbb{Z}$. No rounding is required. Express the answer in terms of $\\alpha$, $\\ell$, $\\sigma^{2}$, $y_{0}$, and $k$.",
            "solution": "The user wants to find the eigenvalues of the Laplace-approximate posterior covariance operator for a given one-dimensional inverse problem.\n\n### Step 0: Problem Validation\n\n**Step 1: Extract Givens**\n-   Domain: $D = [0, 2\\pi]$, periodic.\n-   Unknown field: $u \\in L^{2}(D)$.\n-   Forward operator: $G(u)(x) = u(x)^{2}$.\n-   Observation model: $y = G(u_{\\mathrm{true}}) + \\eta$.\n-   Noise model: $\\eta$ is mean-zero Gaussian with covariance $\\Gamma = \\sigma^{2} I$.\n-   Prior model: $u \\sim \\mathcal{N}(0, \\mathcal{C}_{0})$ with precision operator $\\mathcal{C}_{0}^{-1} = \\alpha (I - \\ell^{2} \\Delta)$, where $\\alpha > 0$, $\\ell > 0$.\n-   Observed data: $y(x) = y_{0}$ (constant).\n-   Condition: $y_{0} > \\frac{\\sigma^{2}}{2} \\alpha$.\n-   Negative log-posterior functional: $\\Phi(u) = \\frac{1}{2\\sigma^{2}} \\int_{D} (y(x) - u(x)^{2})^{2} \\, dx + \\frac{1}{2} \\langle u, \\mathcal{C}_{0}^{-1} u \\rangle_{L^{2}(D)}$.\n-   Fourier basis: $\\phi_{k}(x) = (2\\pi)^{-1/2} \\exp(i k x)$ for $k \\in \\mathbb{Z}$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is a standard formulation of a Bayesian inverse problem in a function space setting. The forward model, prior (a member of the Whittle-Matérn family), and noise model are all conventional and mathematically sound. The objective is to apply the Laplace approximation, a well-established method in this field. The problem is a valid scientific exercise.\n-   **Well-Posed:** The problem is well-posed. The provided condition $y_{0} > \\frac{\\sigma^{2}}{2} \\alpha$ is shown later in the analysis to be precisely the condition required for the Hessian at the MAP estimate to be positive definite, ensuring the MAP estimate is a local minimizer and that a meaningful Laplace approximation exists.\n-   **Objective:** The problem is stated in precise, objective mathematical language.\n-   **Completeness and Consistency:** All necessary definitions and parameters are provided, and no contradictions are apparent.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The solution will now be derived.\n\n### Step 1: Derive and Solve for the MAP Estimate $u^*$\n\nThe Maximum A Posteriori (MAP) estimate $u^*$ is the function $u$ that minimizes the negative log-posterior functional $\\Phi(u)$. We find this by setting the Fréchet derivative of $\\Phi(u)$ with respect to $u$ to zero. The Gateaux derivative of $\\Phi(u)$ in a direction $v \\in L^2(D)$ is given by:\n$$\n\\langle D\\Phi(u), v \\rangle_{L^2} = \\frac{d}{d\\epsilon}\\bigg|_{\\epsilon=0} \\Phi(u+\\epsilon v)\n$$\nFor the data misfit term:\n$$\n\\frac{d}{d\\epsilon}\\bigg|_{\\epsilon=0} \\frac{1}{2\\sigma^{2}} \\int_{D} \\left(y(x) - (u(x)+\\epsilon v(x))^{2}\\right)^{2} \\, dx = \\frac{1}{2\\sigma^{2}} \\int_{D} 2(y-u^2)(-2uv) \\, dx = \\left\\langle -\\frac{2u(y-u^2)}{\\sigma^2}, v \\right\\rangle_{L^2}\n$$\nFor the prior term, which is a quadratic form:\n$$\n\\frac{d}{d\\epsilon}\\bigg|_{\\epsilon=0} \\frac{1}{2} \\langle u+\\epsilon v, \\mathcal{C}_{0}^{-1}(u+\\epsilon v) \\rangle_{L^2} = \\langle \\mathcal{C}_{0}^{-1} u, v \\rangle_{L^2}\n$$\nsince $\\mathcal{C}_{0}^{-1}$ is self-adjoint. The first-order optimality condition $D\\Phi(u)=0$ is therefore:\n$$\n\\mathcal{C}_{0}^{-1} u - \\frac{2u(y-u^2)}{\\sigma^2} = 0\n$$\nWe look for a spatially constant solution $u^*(x) = c$ using the given constant data $y(x) = y_0$. For a constant function $c$, the Laplacian $\\Delta c = 0$. The optimality condition becomes an algebraic equation for the constant $c$:\n$$\n\\alpha(I - \\ell^2 \\Delta) c - \\frac{2c(y_0 - c^2)}{\\sigma^2} = 0 \\implies \\alpha c - \\frac{2c(y_0 - c^2)}{\\sigma^2} = 0\n$$\nFactoring out $c$:\n$$\nc \\left( \\alpha - \\frac{2(y_0 - c^2)}{\\sigma^2} \\right) = 0\n$$\nThis equation has solutions $c=0$ and $\\alpha - \\frac{2(y_0 - c^2)}{\\sigma^2} = 0$. The second case gives:\n$$\n\\alpha \\sigma^2 = 2(y_0 - c^2) \\implies c^2 = y_0 - \\frac{\\alpha \\sigma^2}{2}\n$$\nThe problem states that $y_0 > \\frac{\\alpha \\sigma^2}{2}$, which ensures that $y_0 - \\frac{\\alpha \\sigma^2}{2} > 0$. Therefore, real solutions for $c$ exist: $c = \\pm \\sqrt{y_0 - \\frac{\\alpha \\sigma^2}{2}}$. A stability analysis (performed below by inspecting the Hessian) confirms that these non-zero solutions correspond to local minima of $\\Phi(u)$, whereas $c = 0$ does not. We select one of these as our MAP point, $u^*$. The specific choice of sign does not affect the Hessian. Let's denote $(u^*)^2 = y_0 - \\frac{\\alpha \\sigma^2}{2}$.\n\n### Step 2: Compute the Hessian Operator $H$\n\nThe Hessian $H = D^2\\Phi(u^*)$ is the second Fréchet derivative of $\\Phi$ evaluated at $u^*$. We differentiate the expression for $D\\Phi(u)$ with respect to $u$. The derivative of the prior term $\\mathcal{C}_{0}^{-1}u$ is simply the operator $\\mathcal{C}_{0}^{-1}$. The derivative of the data misfit term $-\\frac{2}{\\sigma^2}(yu-u^3)$ with respect to $u$ is multiplication by the function $-\\frac{2}{\\sigma^2}(y-3u^2)$.\nThus, the Hessian operator $H(u)$ acting on a function $h$ is:\n$$\nH(u)h = \\mathcal{C}_{0}^{-1} h - \\frac{2(y - 3u^2)}{\\sigma^2} h = \\left( \\mathcal{C}_{0}^{-1} - \\frac{2(y - 3u^2)}{\\sigma^2}I \\right)h\n$$\nWe evaluate this at the constant MAP solution $u^*$ and constant data $y_0$:\n$$\nH = \\mathcal{C}_{0}^{-1} - \\frac{2(y_0 - 3(u^*)^2)}{\\sigma^2}I\n$$\nSubstitute $(u^*)^2 = y_0 - \\frac{\\alpha \\sigma^2}{2}$ into the expression:\n$$\ny_0 - 3(u^*)^2 = y_0 - 3\\left(y_0 - \\frac{\\alpha \\sigma^2}{2}\\right) = -2y_0 + \\frac{3\\alpha \\sigma^2}{2}\n$$\nNow substitute this back into the expression for $H$, along with the definition $\\mathcal{C}_{0}^{-1} = \\alpha(I - \\ell^2 \\Delta)$:\n$$\nH = \\alpha(I - \\ell^2 \\Delta) - \\frac{2}{\\sigma^2}\\left(-2y_0 + \\frac{3\\alpha \\sigma^2}{2}\\right)I\n$$\n$$\nH = \\alpha I - \\alpha \\ell^2 \\Delta - \\left(-\\frac{4y_0}{\\sigma^2} + 3\\alpha\\right)I = (\\alpha + \\frac{4y_0}{\\sigma^2} - 3\\alpha)I - \\alpha \\ell^2 \\Delta\n$$\n$$\nH = \\left(\\frac{4y_0}{\\sigma^2} - 2\\alpha\\right)I - \\alpha \\ell^2 \\Delta\n$$\n\n### Step 3: Spectral Analysis and Posterior Covariance\n\nThe Laplace approximation to the posterior distribution is a Gaussian $\\mathcal{N}(u^*, \\mathcal{C}_{\\mathrm{post}})$ where the posterior covariance is $\\mathcal{C}_{\\mathrm{post}} \\approx H^{-1}$. We are asked for the eigenvalues of $\\mathcal{C}_{\\mathrm{post}}$, which are the reciprocals of the eigenvalues of $H$.\nWe find the eigenvalues of $H$ by acting on the Fourier basis functions $\\phi_k(x) = (2\\pi)^{-1/2} \\exp(i k x)$. These are eigenfunctions of the Laplacian: $\\Delta \\phi_k(x) = -k^2 \\phi_k(x)$.\nSince $H$ is a linear differential operator with constant coefficients, it is diagonal in the Fourier basis. Let $\\lambda_k$ be the eigenvalue of $H$ corresponding to the eigenfunction $\\phi_k$:\n$$\nH \\phi_k = \\left[\\left(\\frac{4y_0}{\\sigma^2} - 2\\alpha\\right)I - \\alpha \\ell^2 \\Delta\\right] \\phi_k\n$$\n$$\nH \\phi_k = \\left(\\frac{4y_0}{\\sigma^2} - 2\\alpha\\right)\\phi_k - \\alpha \\ell^2 (-k^2 \\phi_k)\n$$\n$$\nH \\phi_k = \\left(\\frac{4y_0}{\\sigma^2} - 2\\alpha + \\alpha \\ell^2 k^2\\right)\\phi_k\n$$\nThe eigenvalues of $H$ are therefore:\n$$\n\\lambda_k = \\frac{4y_0}{\\sigma^2} - 2\\alpha + \\alpha \\ell^2 k^2, \\quad \\text{for } k \\in \\mathbb{Z}\n$$\nThe condition $y_0 > \\frac{\\sigma^2}{2} \\alpha$ implies $\\frac{4y_0}{\\sigma^2} - 2\\alpha > 2\\alpha - 2\\alpha = 0$, which ensures that $\\lambda_0 > 0$. Since $\\alpha > 0$ and $\\ell > 0$, the term $\\alpha \\ell^2 k^2 \\ge 0$, so all eigenvalues $\\lambda_k$ are positive. This confirms that $H$ is positive definite and $u^*$ is a local minimum.\nThe eigenvalues $c_k$ of the Laplace-approximate posterior covariance operator $\\mathcal{C}_{\\mathrm{post}} \\approx H^{-1}$ are the reciprocals of the eigenvalues $\\lambda_k$:\n$$\nc_k = (\\lambda_k)^{-1} = \\frac{1}{\\frac{4y_0}{\\sigma^2} - 2\\alpha + \\alpha \\ell^2 k^2}\n$$\nThis is the final closed-form expression for the eigenvalues of the posterior covariance.",
            "answer": "$$\n\\boxed{\\left( \\frac{4y_{0}}{\\sigma^{2}} - 2\\alpha + \\alpha \\ell^{2} k^{2} \\right)^{-1}}\n$$"
        }
    ]
}