{"hands_on_practices": [{"introduction": "本练习聚焦于分裂 Bregman 迭代中的二次子问题，即 $u$ 变量的更新。通过将目标函数的梯度设为零，我们将推导出求解此更新步骤的正规方程，它构成了一个线性系统。理解这一推导过程对于掌握该方法的主要计算瓶颈至关重要，因为它揭示了每次迭代的核心计算任务。[@problem_id:3480428]", "problem": "考虑压缩感知中的复合正则化问题，该问题旨在通过最小化一个最小二乘数据保真项和多个通过线性算子施加的非光滑正则化项之和，从数据 $b \\in \\mathbb{R}^{m}$ 中寻求一个估计 $u \\in \\mathbb{R}^{n}$。具体来说，设 $A \\in \\mathbb{R}^{m \\times n}$ 是一个已知的传感矩阵，并设 $\\{K_{i}\\}_{i=1}^{r}$ 是一组已知的线性算子，其中 $K_{i} \\in \\mathbb{R}^{p_{i} \\times n}$。数据保真项为 $f(u) = \\tfrac{1}{2}\\|A u - b\\|_{2}^{2}$。为了处理具有多个变换的非光滑项，可以应用分裂Bregman方法，引入分裂变量 $\\{d_{i}\\}_{i=1}^{r}$ 和Bregman变量 $\\{b_{i}\\}_{i=1}^{r}$。在第 $k$ 次迭代中，给定固定的 $\\{d_{i}^{k}\\}$ 和 $\\{b_{i}^{k}\\}$，$u$-更新步骤求解以下严格凸二次子问题\n$$\nu^{k+1} \\in \\arg\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ \\tfrac{1}{2}\\|A u - b\\|_{2}^{2} + \\tfrac{\\mu}{2} \\sum_{i=1}^{r} \\|K_{i} u - d_{i}^{k} + b_{i}^{k}\\|_{2}^{2} \\right\\},\n$$\n其中 $\\mu  0$ 是一个固定的惩罚参数。假设矩阵\n$$\nA^{\\top} A + \\mu \\sum_{i=1}^{r} K_{i}^{\\top} K_{i}\n$$\n是可逆的，例如，如果 $A$ 具有满列秩或者 $\\sum_{i=1}^{r} K_{i}^{\\top} K_{i}$ 是正定的，则该条件可以得到保证。从多元微积分和线性代数的基本事实出发——即可微严格凸函数在其梯度为零处达到其唯一的极小值点，以及对于任何维度兼容的矩阵 $M$ 和向量 $x,y$，有 $\\nabla_{x} \\left(\\tfrac{1}{2}\\|M x - y\\|_{2}^{2}\\right) = M^{\\top} (M x - y)$——推导 $u$-更新的正规方程，并显式求解 $u^{k+1}$。\n\n你的最终答案必须是 $u^{k+1}$ 关于 $A$、$b$、$\\{K_{i}\\}$、$\\{d_{i}^{k}\\}$、$\\{b_{i}^{k}\\}$ 和 $\\mu$ 的单一闭式解析表达式。不需要数值近似或四舍五入。", "solution": "该问题要求推导出一个向量 $u^{k+1}$ 的闭式表达式，该向量能够最小化在复合正则化的分裂Bregman方法中出现的特定目标函数。\n\n需要对 $u \\in \\mathbb{R}^{n}$ 最小化的目标函数由下式给出\n$$\n\\mathcal{L}(u) = \\frac{1}{2}\\|A u - b\\|_{2}^{2} + \\frac{\\mu}{2} \\sum_{i=1}^{r} \\|K_{i} u - d_{i}^{k} + b_{i}^{k}\\|_{2}^{2}\n$$\n其中 $A$、$b$、$\\mu$、$\\{K_i\\}$、$\\{d_i^k\\}$ 和 $\\{b_i^k\\}$ 均为给定值。问题陈述该函数关于 $u$ 是严格凸且可微的。根据微积分的基本原理，一个严格凸可微函数的唯一极小值点 $u^{k+1}$ 位于其关于目标变量的梯度等于零向量的点。因此，我们必须求解方程 $\\nabla_{u} \\mathcal{L}(u^{k+1}) = 0$。\n\n我们可以通过分别计算每一项的梯度来计算 $\\mathcal{L}(u)$ 的梯度。目标函数是两个分量之和。\n第一个分量是数据保真项，$\\mathcal{L}_1(u) = \\frac{1}{2}\\|A u - b\\|_{2}^{2}$。\n第二个分量是正则化项，$\\mathcal{L}_2(u) = \\frac{\\mu}{2} \\sum_{i=1}^{r} \\|K_{i} u - d_{i}^{k} + b_{i}^{k}\\|_{2}^{2}$。\n\n问题给出了二次型的梯度公式：对于矩阵 $M$ 和向量 $x, y$，我们有 $\\nabla_{x} \\left(\\frac{1}{2}\\|M x - y\\|_{2}^{2}\\right) = M^{\\top} (M x - y)$。\n\n将此公式应用于第一个分量 $\\mathcal{L}_1(u)$，其中 $M=A$，$x=u$，$y=b$，我们得到其梯度：\n$$\n\\nabla_{u} \\mathcal{L}_1(u) = A^{\\top}(A u - b)\n$$\n\n对于第二个分量 $\\mathcal{L}_2(u)$，我们利用梯度算子的线性性质。和的梯度是梯度的和：\n$$\n\\nabla_{u} \\mathcal{L}_2(u) = \\nabla_{u} \\left( \\frac{\\mu}{2} \\sum_{i=1}^{r} \\|K_{i} u - d_{i}^{k} + b_{i}^{k}\\|_{2}^{2} \\right) = \\frac{\\mu}{2} \\sum_{i=1}^{r} \\nabla_{u} \\|K_{i} u - (d_{i}^{k} - b_{i}^{k})\\|_{2}^{2}\n$$\n将所提供的梯度公式应用于求和中的每一项，其中 $M = K_{i}$，$x = u$，$y = d_{i}^{k} - b_{i}^{k}$，我们得到：\n$$\n\\nabla_{u} \\|K_{i} u - (d_{i}^{k} - b_{i}^{k})\\|_{2}^{2} = 2 K_{i}^{\\top}(K_{i} u - (d_{i}^{k} - b_{i}^{k}))\n$$\n将其代回 $\\nabla_{u} \\mathcal{L}_2(u)$ 的表达式中：\n$$\n\\nabla_{u} \\mathcal{L}_2(u) = \\frac{\\mu}{2} \\sum_{i=1}^{r} 2 K_{i}^{\\top}(K_{i} u - d_{i}^{k} + b_{i}^{k}) = \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(K_{i} u - d_{i}^{k} + b_{i}^{k})\n$$\n\n$\\mathcal{L}(u)$ 的总梯度是其各分量梯度之和：\n$$\n\\nabla_{u} \\mathcal{L}(u) = \\nabla_{u} \\mathcal{L}_1(u) + \\nabla_{u} \\mathcal{L}_2(u) = A^{\\top}(A u - b) + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(K_{i} u - d_{i}^{k} + b_{i}^{k})\n$$\n在 $u = u^{k+1}$ 处将梯度设为零，得到 $u$-更新的正规方程：\n$$\nA^{\\top}(A u^{k+1} - b) + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(K_{i} u^{k+1} - d_{i}^{k} + b_{i}^{k}) = 0\n$$\n为了求解 $u^{k+1}$，我们通过分配项并组合包含 $u^{k+1}$ 的项来重新整理这个方程：\n$$\nA^{\\top}A u^{k+1} - A^{\\top}b + \\mu \\sum_{i=1}^{r} \\left( K_{i}^{\\top}K_{i} u^{k+1} - K_{i}^{\\top}(d_{i}^{k} - b_{i}^{k}) \\right) = 0\n$$\n$$\nA^{\\top}A u^{k+1} + \\mu \\left( \\sum_{i=1}^{r} K_{i}^{\\top}K_{i} \\right) u^{k+1} - A^{\\top}b - \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(d_{i}^{k} - b_{i}^{k}) = 0\n$$\n将所有不含 $u^{k+1}$ 的项移到等式右侧：\n$$\nA^{\\top}A u^{k+1} + \\mu \\left( \\sum_{i=1}^{r} K_{i}^{\\top}K_{i} \\right) u^{k+1} = A^{\\top}b + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(d_{i}^{k} - b_{i}^{k})\n$$\n在等式左侧提出 $u^{k+1}$：\n$$\n\\left( A^{\\top}A + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}K_{i} \\right) u^{k+1} = A^{\\top}b + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(d_{i}^{k} - b_{i}^{k})\n$$\n问题陈述保证了矩阵 $\\left( A^{\\top}A + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}K_{i} \\right)$ 是可逆的。因此，我们可以用其逆矩阵左乘方程两边来分离出 $u^{k+1}$：\n$$\nu^{k+1} = \\left( A^{\\top}A + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}K_{i} \\right)^{-1} \\left( A^{\\top}b + \\mu \\sum_{i=1}^{r} K_{i}^{\\top}(d_{i}^{k} - b_{i}^{k}) \\right)\n$$\n此表达式即为 $u$-更新 $u^{k+1}$ 的显式闭式解。", "answer": "$$\n\\boxed{\\left( A^{\\top} A + \\mu \\sum_{i=1}^{r} K_{i}^{\\top} K_{i} \\right)^{-1} \\left( A^{\\top} b + \\mu \\sum_{i=1}^{r} K_{i}^{\\top} (d_{i}^{k} - b_{i}^{k}) \\right)}\n$$", "id": "3480428"}, {"introduction": "此练习处理分裂 Bregman 迭代中用于处理非光滑项的邻近子问题，即 $d$ 变量的更新。你将从第一性原理出发，为加权 $\\ell_1$ 范数推导其邻近算子，即著名的软阈值算子 (soft-thresholding operator)。这个算子是处理各种稀疏性问题的基石，掌握其推导是实现分裂 Bregman 算法的必备技能。[@problem_id:3480434]", "problem": "考虑压缩感知中的复合正则化最小二乘问题，其中包含线性测量算子 $A \\in \\mathbb{R}^{m \\times n}$，数据 $y \\in \\mathbb{R}^{m}$，以及对辅助分裂变量 $d \\in \\mathbb{R}^{p}$ 的稀疏性促进惩罚项：\n$$\n\\min_{u \\in \\mathbb{R}^{n},\\, d \\in \\mathbb{R}^{p}} \\; \\frac{1}{2}\\|Au - y\\|_{2}^{2} \\;+\\; \\lambda \\sum_{j=1}^{p} w_{j} |d_{j}| \\quad \\text{以} \\quad d = Ku \\text{为约束条件},\n$$\n其中 $K \\in \\mathbb{R}^{p \\times n}$ 是一个固定的线性算子，且对所有 $j$ 都有 $w_{j}  0$。使用变量分裂和分裂Bregman方法（已知其等价于交替方向乘子法（ADMM）的一种特定形式），在迭代 $k$ 次时，$d$ 的更新是通过对固定的 $u^{k}$ 和一个Bregman/对偶变量 $b^{k}$ 进行最小化得到的：\n$$\n\\min_{d \\in \\mathbb{R}^{p}} \\; \\lambda \\sum_{j=1}^{p} w_{j} |d_{j}| \\;+\\; \\frac{\\mu}{2} \\left\\| d - \\left(Ku^{k} + b^{k}\\right) \\right\\|_{2}^{2},\n$$\n其中惩罚参数 $\\mu  0$ 是一个固定的值。从正常、闭、凸函数 $g$ 的邻近算子定义出发：\n$$\n\\mathrm{prox}_{\\tau g}(z) \\;=\\; \\arg\\min_{x} \\left\\{ g(x) + \\frac{1}{2\\tau}\\|x - z\\|_{2}^{2} \\right\\},\n$$\n并且仅使用凸分析的基本原理（次微分最优性条件和可分性），推导加权 $\\ell_{1}$ 函数 $g(d) = \\sum_{j=1}^{p} w_{j}|d_{j}|$ 的邻近算子的闭式表达式，并证明 $d$ 的更新等于 $\\mathrm{prox}_{(\\lambda/\\mu) g}\\!\\left(Ku^{k}+b^{k}\\right)$。\n\n然后，将您推导出的公式应用于以下 $p = 4$ 的具体实例：\n- 权重 $w = (w_{1}, w_{2}, w_{3}, w_{4}) = (1, 2, 0.5, 3)$,\n- 参数 $\\lambda = 1$ 和 $\\mu = 2$,\n- 参数 $z = Ku^{k} + b^{k} = (2, -0.3, 0, 1.5)$。\n\n计算更新后的 $d^{k+1}$。请使用标准数学符号将您的最终答案表示为单行向量。无需四舍五入，且不涉及物理单位。", "solution": "该问题被认定为有效，因为它科学地基于凸优化和压缩感知的原理，问题设定良好（well-posed）并提供了所有必要信息，并且使用客观、明确的数学语言进行表述。\n\n该问题要求完成两个主要任务：首先，推导加权 $\\ell_1$ 范数的邻近算子的闭式表达式，并将其与分裂Bregman方法中的 $d$ 更新联系起来；其次，将此公式应用于一个具体的数值实例。\n\n**第1部分：邻近算子的推导及其与分裂Bregman更新的关系**\n\n令 $g(d) = \\sum_{j=1}^{p} w_{j} |d_{j}|$，其中 $w_j  0$。带有参数 $\\tau  0$ 的 $g$ 的邻近算子定义为：\n$$\n\\mathrm{prox}_{\\tau g}(z) = \\arg\\min_{x \\in \\mathbb{R}^{p}} \\left\\{ \\tau g(x) + \\frac{1}{2}\\|x - z\\|_{2}^{2} \\right\\}\n$$\n代入 $g(x)$ 的表达式，目标函数为：\n$$\nF(x) = \\tau \\sum_{j=1}^{p} w_{j} |x_{j}| + \\frac{1}{2} \\sum_{j=1}^{p} (x_{j} - z_{j})^{2}\n$$\n该目标函数关于分量 $x_j$ 是可分的。因此，可以对每个分量独立地进行最小化：\n$$\n(\\mathrm{prox}_{\\tau g}(z))_j = \\arg\\min_{x_j \\in \\mathbb{R}} \\left\\{ \\tau w_j |x_j| + \\frac{1}{2}(x_j - z_j)^2 \\right\\}\n$$\n令 $x_j^*$ 为最小化子。由于目标函数是凸的，最优性条件是在 $x_j^*$ 处，0 必须位于目标函数的次微分中。$|x_j|$ 的次微分是：\n$$\n\\partial|x_j| = \\begin{cases} \\{\\mathrm{sgn}(x_j)\\}  \\text{若 } x_j \\neq 0 \\\\ [-1, 1]  \\text{若 } x_j = 0 \\end{cases}\n$$\n最优性条件是在 $x_j^*$ 处计算的 $0 \\in \\partial \\left( \\tau w_j |x_j| + \\frac{1}{2}(x_j - z_j)^2 \\right)$。这给出：\n$$\n0 \\in \\tau w_j \\partial|x_j^*| + (x_j^* - z_j)\n$$\n重新整理，我们得到：\n$$\nz_j - x_j^* \\in \\tau w_j \\partial|x_j^*|\n$$\n我们根据 $x_j^*$ 的值分三种情况分析此条件：\n\n1.  **情况1：$x_j^*  0$**。此时 $\\partial|x_j^*| = \\{1\\}$。条件变为 $z_j - x_j^* = \\tau w_j$，这意味着 $x_j^* = z_j - \\tau w_j$。这种情况仅在 $x_j^*  0$ 时成立，因此我们要求 $z_j - \\tau w_j  0$，即 $z_j  \\tau w_j$。\n\n2.  **情况2：$x_j^*  0$**。此时 $\\partial|x_j^*| = \\{-1\\}$。条件变为 $z_j - x_j^* = -\\tau w_j$，这意味着 $x_j^* = z_j + \\tau w_j$。这种情况仅在 $x_j^*  0$ 时成立，因此我们要求 $z_j + \\tau w_j  0$，即 $z_j  -\\tau w_j$。\n\n3.  **情况3：$x_j^* = 0$**。此时 $\\partial|x_j^*| = [-1, 1]$。条件变为 $z_j - 0 \\in \\tau w_j [-1, 1]$，这等价于 $|z_j| \\le \\tau w_j$。\n\n综合这三种情况，我们得到对于任意 $z_j$ 的 $x_j^*$ 的解：\n$$\nx_j^* = \\begin{cases} z_j - \\tau w_j  \\text{若 } z_j  \\tau w_j \\\\ 0  \\text{若 } |z_j| \\le \\tau w_j \\\\ z_j + \\tau w_j  \\text{若 } z_j  -\\tau w_j \\end{cases}\n$$\n这就是众所周知的逐元素软阈值算子，可以紧凑地写为：\n$$\nx_j^* = \\mathrm{sgn}(z_j) \\max(0, |z_j| - \\tau w_j)\n$$\n因此，加权 $\\ell_1$ 范数的邻近算子是一个向量值函数，其第 $j$ 个分量是对 $z_j$ 进行阈值为 $\\tau w_j$ 的软阈值操作。\n\n接下来，我们将其与分裂Bregman方法中的 $d$ 更新联系起来。该更新由以下问题的解给出：\n$$\nd^{k+1} = \\arg\\min_{d \\in \\mathbb{R}^{p}} \\; \\lambda \\sum_{j=1}^{p} w_{j} |d_{j}| \\;+\\; \\frac{\\mu}{2} \\left\\| d - z \\right\\|_{2}^{2}\n$$\n其中我们记 $z = Ku^{k} + b^{k}$。我们可以通过除以常数 $\\mu  0$ 来重写目标函数，而不会改变最小化子：\n$$\nd^{k+1} = \\arg\\min_{d \\in \\mathbb{R}^{p}} \\; \\frac{\\lambda}{\\mu} \\sum_{j=1}^{p} w_{j} |d_{j}| \\;+\\; \\frac{1}{2} \\left\\| d - z \\right\\|_{2}^{2}\n$$\n这个表达式正是 $\\mathrm{prox}_{\\tau g}(z)$ 的定义，其中 $g(d) = \\sum_{j=1}^{p} w_j |d_j|$ 且参数 $\\tau = \\lambda/\\mu$。因此，分裂Bregman的 $d$ 更新确实是由一个邻近算子给出的：\n$$\nd^{k+1} = \\mathrm{prox}_{(\\lambda/\\mu) g}\\!\\left(Ku^{k}+b^{k}\\right)\n$$\n\n**第2部分：应用于具体实例**\n\n我们已知以下值：\n- 权重：$w = (1, 2, 0.5, 3)$\n- 参数：$\\lambda = 1$, $\\mu = 2$\n- 参数向量：$z = Ku^{k} + b^{k} = (2, -0.3, 0, 1.5)$\n\n首先，我们计算邻近算子参数 $\\tau$：\n$$\n\\tau = \\frac{\\lambda}{\\mu} = \\frac{1}{2} = 0.5\n$$\n接下来，我们计算阈值向量 $T$，其中 $T_j = \\tau w_j$：\n$$\nT = \\tau w = 0.5 \\times (1, 2, 0.5, 3) = (0.5, 1.0, 0.25, 1.5)\n$$\n现在我们将软阈值公式 $d_j^{k+1} = \\mathrm{sgn}(z_j) \\max(0, |z_j| - T_j)$ 应用于每个分量 $j=1, 2, 3, 4$：\n\n- 对于 $j=1$：$z_1 = 2$ 且 $T_1 = 0.5$。因为 $z_1  T_1$，我们有：\n  $$ d_1^{k+1} = z_1 - T_1 = 2 - 0.5 = 1.5 = \\frac{3}{2} $$\n\n- 对于 $j=2$：$z_2 = -0.3$ 且 $T_2 = 1.0$。因为 $|z_2| = 0.3 \\le T_2$，我们有：\n  $$ d_2^{k+1} = 0 $$\n\n- 对于 $j=3$：$z_3 = 0$ 且 $T_3 = 0.25$。因为 $|z_3| = 0 \\le T_3$，我们有：\n  $$ d_3^{k+1} = 0 $$\n\n- 对于 $j=4$：$z_4 = 1.5$ 且 $T_4 = 1.5$。因为 $|z_4| = 1.5 \\le T_4$，我们有：\n  $$ d_4^{k+1} = 0 $$\n\n综合这些分量，更新后的向量 $d^{k+1}$ 是：\n$$\nd^{k+1} = \\left(\\frac{3}{2}, 0, 0, 0\\right)\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{3}{2}  0  0  0\n\\end{pmatrix}\n}\n$$", "id": "3480434"}, {"introduction": "在分别掌握了各个子问题的求解方法后，本综合练习将带你实现一个完整的分裂 Bregman 算法。你将为一个结合了 $\\ell_1$ 范数和全变分（Total Variation, TV）的复合正则化问题编写代码。此外，本练习还将引导你探索一个关键的实践问题：如何自适应地调整惩罚参数 $\\mu_i$ 以避免收敛过程中的振荡，从而提高算法的稳定性和效率。[@problem_id:3480433]", "problem": "考虑在压缩感知和稀疏优化中的复合凸优化问题：最小化一个二次数据保真项和两个稀疏性促进项之和，即逐项一范数和离散一维全变分 (TV)。令 $x \\in \\mathbb{R}^n$ 表示待恢复的信号，令 $A \\in \\mathbb{R}^{m \\times n}$ 为测量矩阵，令 $b \\in \\mathbb{R}^m$ 为测量数据。令 $W \\in \\mathbb{R}^{n \\times n}$ 为单位矩阵（因此 $\\|W x\\|_1 = \\|x\\|_1$），令 $D \\in \\mathbb{R}^{(n-1) \\times n}$ 为前向差分算子，其定义为 $(D x)_i = x_{i+1} - x_{i}$，其中 $i = 1, \\dots, n-1$。该优化问题为\n$$\n\\min_{x \\in \\mathbb{R}^n} \\left\\{ \\frac{1}{2} \\| A x - b \\|_2^2 + \\lambda_1 \\| W x \\|_1 + \\lambda_2 \\| D x \\|_1 \\right\\},\n$$\n其中参数 $\\lambda_1, \\lambda_2$ 为非负。\n\n采用分裂布雷格曼（Split Bregman, SB）方法来处理非光滑复合正则化。引入辅助变量 $d_1 \\in \\mathbb{R}^n$ 和 $d_2 \\in \\mathbb{R}^{n-1}$，以及 Bregman 变量 $b_1 \\in \\mathbb{R}^n$ 和 $b_2 \\in \\mathbb{R}^{n-1}$，以及惩罚参数 $\\mu_1, \\mu_2  0$。该方法在求解关于 $x$ 的二次子问题、对 $d_1$ 和 $d_2$ 执行分量软阈值更新以及 Bregman 变量更新之间交替进行。定义局部残差\n$$\nr_1^k = \\| d_1^k - W x^k \\|_2, \\quad r_2^k = \\| d_2^k - D x^k \\|_2,\n$$\n其中上标 $k$ 表示迭代索引。一种有时会尝试的朴素选择是根据 $r_i^k$ 的倒数来更新 $\\mu_i$，例如，$\\mu_i^{k+1} = \\kappa_i / (r_i^k + \\varepsilon)$，其中 $\\varepsilon  0$ 是一个小数，$\\kappa_i  0$ 是常数。在实践中，由于参数变化过于剧烈，这种朴素的规则可能导致目标序列产生振荡。\n\n您的任务是构建一个反例，以证明对 $\\mu_1, \\mu_2$ 进行朴素的残差倒数更新会如何在目标序列中引发振荡，并提出和实现使用局部平滑残差的自适应规则来更新 $\\mu_1, \\mu_2$，以减轻这种振荡。具体而言：\n\n- 使用一维分段常数基准真相 $x^\\star$ 和一个随机高斯测量矩阵 $A$（其列被归一化为单位二范数）来生成合成测量值 $b = A x^\\star + \\eta$，其中 $\\eta$ 是具有指定标准差的高斯噪声。使用 $W = I$ 和如上定义的前向差分算子 $D$。\n- 实现具有两种 $\\mu_1, \\mu_2$ 更新策略的分裂布雷格曼方法：\n  1. 朴素策略：$\\mu_i^{k+1} = \\kappa_i / (r_i^k + \\varepsilon)$，并裁剪到固定区间 $[\\mu_{\\min}, \\mu_{\\max}]$ 内。\n  2. 自适应策略：维持残差的指数加权移动平均值 $\\bar{r}_i^k$，并更新\n     $$\n     \\mu_i^{k+1} = \\operatorname{clip}\\!\\left( \\mu_i^k \\exp\\!\\left( \\eta \\log \\frac{r_i^k + \\varepsilon}{\\bar{r}_i^{k+1} + \\varepsilon} \\right), \\, \\mu_{\\min}, \\, \\mu_{\\max} \\right),\n     $$\n     其中 $\\bar{r}_i^{k+1} = (1 - \\alpha) \\bar{r}_i^k + \\alpha r_i^k$，$\\alpha \\in (0,1)$，并且为实现阻尼效果，选择一个较小的 $\\eta  0$。$\\operatorname{clip}(\\cdot)$ 表示到区间 $[\\mu_{\\min}, \\mu_{\\max}]$ 上的投影。\n\n- 对每种策略，计算目标序列\n  $$\n  J^k = \\frac{1}{2} \\| A x^k - b \\|_2^2 + \\lambda_1 \\| x^k \\|_1 + \\lambda_2 \\| D x^k \\|_1,\n  $$\n  并通过计算连续差分 $\\Delta^k = J^{k+1} - J^k$ 中超过容差阈值 $t  0$ 的符号变化次数来量化振荡（忽略幅值小于或等于 $t$ 的差分）。振荡计数定义为 $\\Delta^{k}$ 和 $\\Delta^{k-1}$ 符号相反（不包括接近零的差分）的索引 $k$ 的数量。\n\n- 按如下方式为每个测试用例定义布尔决策：如果朴素策略表现出至少指定数量的振荡符号变化，并且自适应策略表现出的振荡次数严格少于该数量的三分之一，则输出 $\\texttt{True}$；否则输出 $\\texttt{False}$。\n\n从基本事实出发：目标函数的凸性、一范数的邻近映射是软阈值函数，以及前向差分算子的性质。您的程序必须基于这些原理实现分裂布雷格曼迭代，构建反例，并根据上述规则对以下参数设置的测试套件进行振荡评估：\n\n- 测试用例 1（理想路径）：$n = 64$，$m = 32$，噪声标准差 $\\sigma = 0.01$，$\\lambda_1 = 0.075$，$\\lambda_2 = 0.20$，初始惩罚 $\\mu_1^0 = 0.10$，$\\mu_2^0 = 0.10$，朴素策略常数 $\\kappa_1 = 0.75$，$\\kappa_2 = 0.75$，裁剪区间 $[\\mu_{\\min}, \\mu_{\\max}] = [10^{-4}, 50]$，最大迭代次数 $K = 60$，振荡容差 $t = 10^{-6}$，朴素策略振荡决策阈值为 $6$。\n- 测试用例 2（强正则化和近乎无噪声数据的边界条件）：$n = 64$，$m = 64$，$\\sigma = 10^{-6}$，$\\lambda_1 = 0.25$，$\\lambda_2 = 0.60$，$\\mu_1^0 = 0.10$，$\\mu_2^0 = 0.10$，$\\kappa_1 = 0.75$，$\\kappa_2 = 0.75$，$[\\mu_{\\min}, \\mu_{\\max}] = [10^{-4}, 50]$，$K = 60$，$t = 10^{-6}$，朴素策略振荡阈值 $= 6$。\n- 测试用例 3（高度欠定测量的边缘情况）：$n = 128$，$m = 16$，$\\sigma = 0.05$，$\\lambda_1 = 0.12$，$\\lambda_2 = 0.08$，$\\mu_1^0 = 0.10$，$\\mu_2^0 = 0.10$，$\\kappa_1 = 0.75$，$\\kappa_2 = 0.75$，$[\\mu_{\\min}, \\mu_{\\max}] = [10^{-4}, 50]$，$K = 80$，$t = 10^{-6}$，朴素策略振荡阈值 $= 8$。\n\n对于自适应策略，在所有测试用例中均使用残差平均参数 $\\alpha = 0.10$ 和阻尼参数 $\\eta = 0.25$。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，$[\\texttt{True},\\texttt{False},\\texttt{True}]$），其中每个布尔值按顺序对应测试用例 1、2 和 3 的决策。不涉及任何物理单位、角度单位或百分比；所有量均为无量纲。程序必须完全自包含，无需用户输入或外部资源即可运行，并且必须遵守指定的运行时环境。", "solution": "用户提供的问题是有效的。这是数值优化和压缩感知领域一个定义明确的任务，其基础是成熟的数学原理。问题陈述是自包含的，提供了构建唯一且可验证解所需的所有定义、方程和参数。\n\n问题的核心是使用分裂布雷格曼（SB）方法求解复合凸优化问题：\n$$\n\\min_{x \\in \\mathbb{R}^n} \\left\\{ \\frac{1}{2} \\| A x - b \\|_2^2 + \\lambda_1 \\| x \\|_1 + \\lambda_2 \\| D x \\|_1 \\right\\}\n$$\n在此问题中，$x$ 是信号，$A$ 是测量矩阵，$b$ 是测量向量，$D$ 是前向差分算子，$\\lambda_1, \\lambda_2$ 是正正则化参数。项 $\\|x\\|_1$ 促进信号系数的稀疏性，而 $\\|Dx\\|_1$（全变分）促进信号梯度的稀疏性，从而倾向于分段常数解。\n\nSB 方法通过引入辅助变量并通过迭代方案求解一个约束问题，来处理 $\\ell_1$-范数的不可微性。令 $d_1 = x$ 和 $d_2 = Dx$。该问题等价于：\n$$\n\\min_{x, d_1, d_2} \\left\\{ \\frac{1}{2} \\| A x - b \\|_2^2 + \\lambda_1 \\| d_1 \\|_1 + \\lambda_2 \\| d_2 \\|_1 \\right\\} \\quad \\text{subject to} \\quad d_1 = x, \\quad d_2 = Dx.\n$$\nSB 方法等价于交替方向乘子法（ADMM），它通过构造一个增广拉格朗日量，并针对每个变量进行迭代最小化来解决此问题。由此产生的第 $k$ 步迭代算法如下：\n1.  **$x$-子问题**：求解 $x^{k+1}$。\n    $$\n    x^{k+1} = \\arg\\min_x \\left\\{ \\frac{1}{2} \\| A x - b \\|_2^2 + \\frac{\\mu_1^k}{2} \\| d_1^k - x - b_1^k \\|_2^2 + \\frac{\\mu_2^k}{2} \\| d_2^k - Dx - b_2^k \\|_2^2 \\right\\}\n    $$\n    这是一个二次最小化问题。通过将关于 $x$ 的梯度设为零，可以找到解，从而得到以下线性系统：\n    $$\n    (A^T A + \\mu_1^k I + \\mu_2^k D^T D) x^{k+1} = A^T b + \\mu_1^k (d_1^k - b_1^k) + \\mu_2^k D^T (d_2^k - b_2^k)\n    $$\n    求解此系统以得到 $x^{k+1}$。矩阵 $H^k = A^T A + \\mu_1^k I + \\mu_2^k D^T D$ 是对称正定的（因为 $\\mu_1^k  0$），这保证了解的唯一性。\n\n2.  **$d_1, d_2$-子问题**：分别求解 $d_1^{k+1}$ 和 $d_2^{k+1}$。\n    $$\n    d_1^{k+1} = \\arg\\min_{d_1} \\left\\{ \\lambda_1 \\| d_1 \\|_1 + \\frac{\\mu_1^k}{2} \\| d_1 - (x^{k+1} + b_1^k) \\|_2^2 \\right\\}\n    $$\n    $$\n    d_2^{k+1} = \\arg\\min_{d_2} \\left\\{ \\lambda_2 \\| d_2 \\|_1 + \\frac{\\mu_2^k}{2} \\| d_2 - (Dx^{k+1} + b_2^k) \\|_2^2 \\right\\}\n    $$\n    这些是关于 $\\ell_1$-范数的邻近算子问题，其解由软阈值函数 $S_{\\tau}(y) = \\text{sign}(y) \\max(|y|-\\tau, 0)$ 给出：\n    $$\n    d_1^{k+1} = S_{\\lambda_1/\\mu_1^k}(x^{k+1} + b_1^k)\n    $$\n    $$\n    d_2^{k+1} = S_{\\lambda_2/\\mu_2^k}(Dx^{k+1} + b_2^k)\n    $$\n\n3.  **Bregman 变量更新**：更新对偶变量 $b_1, b_2$。\n    $$\n    b_1^{k+1} = b_1^k + x^{k+1} - d_1^{k+1}\n    $$\n    $$\n    b_2^{k+1} = b_2^k + Dx^{k+1} - d_2^{k+1}\n    $$\n\nSB/ADMM 算法的性能关键取决于惩罚参数 $\\mu_1, \\mu_2$ 的选择。该问题要求在每次迭代中比较两种更新这些参数的策略。\n\n令 $r_i^k = \\|\\text{Op}_i x^k - d_i^k\\|_2$ 为第 $i$ 个约束在第 $k$ 次迭代时的原始残差，其中 $\\text{Op}_1=I$ 且 $\\text{Op}_2=D$。用于下一次迭代 $k+1$ 的 $\\mu_i$ 更新规则基于第 $k$ 次迭代的残差。\n\n**朴素更新策略**：\n惩罚参数的更新值与最近一次的残差成反比，并裁剪到预定义的范围 $[\\mu_{\\min}, \\mu_{\\max}]$ 内：\n$$\n\\mu_i^{k+1} = \\operatorname{clip}\\!\\left( \\frac{\\kappa_i}{r_i^k + \\varepsilon}, \\, \\mu_{\\min}, \\, \\mu_{\\max} \\right)\n$$\n其中 $\\kappa_i  0$ 是常数，$\\varepsilon  0$ 是一个防止除零的小数。该规则可能反应过度，导致振荡。\n\n**自适应更新策略**：\n为稳定更新过程，使用指数移动平均来维持一个平滑残差 $\\bar{r}_i$：\n$$\n\\bar{r}_i^{k+1} = (1 - \\alpha) \\bar{r}_i^k + \\alpha r_i^k\n$$\n然后，惩罚参数以乘法方式进行更新，并由因子 $\\eta$ 进行阻尼：\n$$\n\\mu_i^{k+1} = \\operatorname{clip}\\!\\left( \\mu_i^k \\exp\\!\\left( \\eta \\log \\frac{r_i^k + \\varepsilon}{\\bar{r}_i^{k+1} + \\varepsilon} \\right), \\, \\mu_{\\min}, \\, \\mu_{\\max} \\right)\n$$\n此规则会平缓地调整 $\\mu_i$。如果当前残差 $r_i^k$ 大于其平滑历史值 $\\bar{r}_i^{k+1}$，则惩罚 $\\mu_i$ 会增加，反之亦然。\n\n该实现将按规定生成合成数据，对两种策略执行 SB 算法，在每次迭代中计算目标函数 $J^k$，并计算振荡次数。当目标函数的变化量 $\\Delta^k = J^{k+1} - J^k$ 的符号发生翻转，并且 $|\\Delta^k|$ 和 $|\\Delta^{k-1}|$ 都大于一个容差 $t$ 时，计为一次振荡。最终的布尔决策基于将这些振荡计数与给定阈值进行比较的结果。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to run test cases and produce the final output.\n    \"\"\"\n\n    test_cases = [\n        {'n': 64, 'm': 32, 'sigma': 0.01, 'lambda1': 0.075, 'lambda2': 0.20, 'mu1_0': 0.10, 'mu2_0': 0.10, 'kappa1': 0.75, 'kappa2': 0.75, 'mu_min': 1e-4, 'mu_max': 50, 'K': 60, 't': 1e-6, 'naive_osc_threshold': 6, 'alpha': 0.10, 'eta': 0.25},\n        {'n': 64, 'm': 64, 'sigma': 1e-6, 'lambda1': 0.25, 'lambda2': 0.60, 'mu1_0': 0.10, 'mu2_0': 0.10, 'kappa1': 0.75, 'kappa2': 0.75, 'mu_min': 1e-4, 'mu_max': 50, 'K': 60, 't': 1e-6, 'naive_osc_threshold': 6, 'alpha': 0.10, 'eta': 0.25},\n        {'n': 128, 'm': 16, 'sigma': 0.05, 'lambda1': 0.12, 'lambda2': 0.08, 'mu1_0': 0.10, 'mu2_0': 0.10, 'kappa1': 0.75, 'kappa2': 0.75, 'mu_min': 1e-4, 'mu_max': 80, 'K': 80, 't': 1e-6, 'naive_osc_threshold': 8, 'alpha': 0.10, 'eta': 0.25}\n    ]\n\n    results = []\n    for params in test_cases:\n        # The prompt is slightly ambiguous on which iteration's values to use for mu update.\n        # Standard ADMM literature updates mu for iteration k+1 based on residuals from iteration k.\n        # This implementation follows that logic. The problem says mu_i^{k+1} is based on r_i^k.\n        # But the state (x,d) is for iteration k. So r_i^k = || Op x^k - d^k ||_2.\n        # This implies we calculate residuals, then update mu for the next iteration, then run the next iteration.\n        # This is what I have implemented.\n\n        osc_naive = run_split_bregman(params, 'naive')\n        osc_adaptive = run_split_bregman(params, 'adaptive')\n\n        is_true = (osc_naive >= params['naive_osc_threshold'] and\n                   osc_adaptive  osc_naive / 3.0)\n        results.append(str(is_true))\n\n    print(f\"[{','.join(results)}]\")\n\n\ndef shrink(y, tau):\n    \"\"\"Component-wise soft-thresholding function.\"\"\"\n    return np.sign(y) * np.maximum(np.abs(y) - tau, 0.0)\n\ndef get_objective(x, A, b, D, lambda1, lambda2):\n    \"\"\"Computes the objective function value.\"\"\"\n    term1 = 0.5 * np.linalg.norm(A @ x - b)**2\n    term2 = lambda1 * np.linalg.norm(x, 1)\n    term3 = lambda2 * np.linalg.norm(D @ x, 1)\n    return term1 + term2 + term3\n\ndef run_split_bregman(params, strategy):\n    \"\"\"\n    Implements the Split Bregman algorithm for a given parameter set and update strategy.\n    Returns the count of oscillations in the objective function sequence.\n    \"\"\"\n    np.random.seed(0)  # For reproducibility\n\n    # Unpack parameters\n    n, m, sigma = params['n'], params['m'], params['sigma']\n    lambda1, lambda2 = params['lambda1'], params['lambda2']\n    mu1_0, mu2_0 = params['mu1_0'], params['mu2_0']\n    kappa1, kappa2 = params['kappa1'], params['kappa2']\n    mu_min, mu_max = params['mu_min'], params['mu_max']\n    K, t = params['K'], params['t']\n    alpha, eta = params['alpha'], params['eta']\n    eps = 1e-8\n\n    # Generate synthetic data\n    x_star = np.zeros(n)\n    x_star[n // 4: n // 2] = 5.0\n    x_star[3 * n // 4: 3 * n // 4 + n // 8] = -3.0\n    \n    A = np.random.randn(m, n)\n    A /= np.linalg.norm(A, axis=0, keepdims=True)\n    noise = np.random.randn(m) * sigma\n    b = A @ x_star + noise\n\n    D = np.eye(n - 1, n, k=1) - np.eye(n - 1, n, k=0)\n\n    # Initialize SB variables\n    x, d1, d2 = np.zeros(n), np.zeros(n), np.zeros(n - 1)\n    b1, b2 = np.zeros(n), np.zeros(n - 1)\n    mu1, mu2 = mu1_0, mu2_0\n    \n    # Initialize smoothed residuals for adaptive strategy\n    # The problem asks to calculate r_i^k = ||d_i^k - Op_i x^k||. This is the residual from the *previous* state.\n    # So r_i^0 is based on initial values, which are all zero.\n    r1_bar = np.linalg.norm(x - d1)\n    r2_bar = np.linalg.norm(D@x - d2)\n\n    # Precompute static matrices\n    AtA = A.T @ A\n    DtD = D.T @ D\n    Atb = A.T @ b\n\n    obj_sequence = [get_objective(x, A, b, D, lambda1, lambda2)]\n    \n    W = np.eye(n) # As per problem statement W=I\n\n    for k in range(K):\n        # Calculate mu for the current iteration based on the previous state (x, d1, d2)\n        r1_k = np.linalg.norm(d1 - W @ x)\n        r2_k = np.linalg.norm(d2 - D @ x)\n        \n        mu1_current = mu1\n        mu2_current = mu2\n\n        if strategy == 'naive':\n            mu1_next = np.clip(kappa1 / (r1_k + eps), mu_min, mu_max)\n            mu2_next = np.clip(kappa2 / (r2_k + eps), mu_min, mu_max)\n        elif strategy == 'adaptive':\n            r1_bar_next = (1.0 - alpha) * r1_bar + alpha * r1_k\n            r2_bar_next = (1.0 - alpha) * r2_bar + alpha * r2_k\n            \n            mu1_log_arg = (r1_k + eps) / (r1_bar_next + eps)\n            mu1_next = mu1_current * np.exp(eta * np.log(mu1_log_arg))\n            mu1_next = np.clip(mu1_next, mu_min, mu_max)\n\n            mu2_log_arg = (r2_k + eps) / (r2_bar_next + eps)\n            mu2_next = mu2_current * np.exp(eta * np.log(mu2_log_arg))\n            mu2_next = np.clip(mu2_next, mu_min, mu_max)\n\n            r1_bar, r2_bar = r1_bar_next, r2_bar_next\n            \n        # x-update\n        H = AtA + mu1_current * W.T @ W + mu2_current * DtD\n        rhs = Atb + mu1_current * W.T @ (d1 + b1) + mu2_current * D.T @ (d2 + b2)\n        x = np.linalg.solve(H, rhs)\n\n        # d-updates\n        d1 = shrink(W @ x - b1, lambda1 / mu1_current)\n        d2 = shrink(D @ x - b2, lambda2 / mu2_current)\n\n        # b-updates\n        b1 = b1 - (W @ x - d1)\n        b2 = b2 - (D @ x - d2)\n\n        # Update mu for the next iteration\n        mu1, mu2 = mu1_next, mu2_next\n        \n        obj_sequence.append(get_objective(x, A, b, D, lambda1, lambda2))\n\n    # Oscillation counting\n    diffs = np.diff(obj_sequence)\n    filtered_diffs = diffs[np.abs(diffs) > t]\n\n    if len(filtered_diffs)  2:\n        return 0\n    \n    signs = np.sign(filtered_diffs)\n    oscillation_count = np.sum(signs[:-1] * signs[1:]  0)\n    \n    return int(oscillation_count)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3480433"}]}