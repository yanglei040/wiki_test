{"hands_on_practices": [{"introduction": "卡鲁什-库恩-塔克（KKT）条件是LASSO解的最优性理论基石。本练习 [@problem_id:3488574] 将通过一个具体的小规模实例，引导你手动应用这些条件，并追踪正则化参数 $\\lambda$ 变化时完整的解路径。这有助于你直观地理解LASSO是如何通过调整 $\\lambda$ 来进行变量选择的，并为你分析更复杂的场景打下坚实的基础。", "problem": "考虑稀疏线性回归中的最小绝对收缩和选择算子 (LASSO) 问题，其中有 $n=2$ 个样本和 $p=3$ 个特征。设设计矩阵为\n$$\nX \\;=\\; \\begin{pmatrix}\n1  0  1 \\\\\n0  1  1\n\\end{pmatrix},\n$$\n响应向量为\n$$\ny \\;=\\; \\begin{pmatrix}\n1 \\\\\n0.5\n\\end{pmatrix}.\n$$\nLASSO 估计量 $\\beta \\in \\mathbb{R}^{3}$ 定义为以下最小化问题的解：\n$$\n\\min_{\\beta \\in \\mathbb{R}^{3}} \\;\\; \\frac{1}{2}\\,\\|\\,y - X\\beta\\,\\|_{2}^{2} \\;+\\; \\lambda\\,\\|\\beta\\|_{1},\n$$\n其中 $\\lambda \\ge 0$ 是正则化参数，$\\|\\cdot\\|_{1}$ 表示 $\\ell_{1}$ 范数。从凸优化和次梯度微积分的第一性原理出发，明确写出此问题及此特定 $(X,y)$ 的 Karush–Kuhn–Tucker (KKT) 最优性条件。利用这些条件，确定当 $\\lambda$ 在 $\\lambda \\ge 0$ 上变化时，支撑集（最小化子非零坐标的索引集合）保持不变的 $\\lambda$ 区间。作为最终答案，报告支撑集发生变化时的有限临界 $\\lambda$值的有序列表。所有数学实体均以标准数学符号表示。无需四舍五入，不涉及物理单位。答案必须是包含按降序排列的临界 $\\lambda$ 值的单行向量。", "solution": "所述问题是凸优化在 LASSO 回归模型中的一个适定应用。所有必要的数据和定义都已提供，问题内部一致且有科学依据。因此，我们可以着手求解。\n\nLASSO 优化问题定义为找到最小化目标函数的向量 $\\beta \\in \\mathbb{R}^{3}$：\n$$\nL(\\beta) \\;=\\; \\frac{1}{2}\\,\\|\\,y - X\\beta\\,\\|_{2}^{2} \\;+\\; \\lambda\\,\\|\\beta\\|_{1}\n$$\n在这里，最小二乘项 $f(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2}$ 是一个可微凸函数，而惩罚项 $g(\\beta) = \\lambda\\|\\beta\\|_{1}$ 是一个不可微凸函数。最小化子 $\\beta$ 必须满足 Karush–Kuhn–Tucker (KKT) 最优性条件。对于这种形式的问题，KKT 条件由次梯度最优性条件 $0 \\in \\nabla f(\\beta) + \\partial g(\\beta)$ 给出。\n\n最小二乘项的梯度是：\n$$\n\\nabla f(\\beta) = -X^T(y - X\\beta) = X^TX\\beta - X^Ty\n$$\n$\\ell_1$ 范数惩罚项的次梯度是 $\\lambda \\partial \\|\\beta\\|_{1}$，其中 $\\partial \\|\\beta\\|_{1}$ 是一个向量 $s \\in \\mathbb{R}^{3}$，其分量 $s_j$ 满足：\n$$\ns_j = \\begin{cases}\n\\text{sign}(\\beta_j)  \\text{if } \\beta_j \\neq 0 \\\\\nv_j \\in [-1, 1]  \\text{if } \\beta_j = 0\n\\end{cases}\n$$\n因此，KKT 条件要求存在一个满足上述性质的次梯度向量 $s$，使得 $\\nabla f(\\beta) + \\lambda s = 0$，可以写成：\n$$\nX^Ty - X^TX\\beta = \\lambda s\n$$\n按分量来看，设 $c=X^Ty$ 和 $H=X^TX$。对于每个分量 $j \\in \\{1, 2, 3\\}$，KKT 条件为：\n1. 如果 $\\beta_j \\neq 0$，则 $(c - H\\beta)_j = \\lambda \\, \\text{sign}(\\beta_j)$。这意味着非零系数的集合（支撑集或活动集）确定了一个线性方程组。\n2. 如果 $\\beta_j = 0$，则 $|(c - H\\beta)_j| \\le \\lambda$。此条件确保当前在活动集之外的任何系数与残差的相关性都不足以强到可以进入模型。\n\n首先，我们为给定数据计算所需的矩阵：\n$X = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix}$，$y = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix}$。\n$$\nc = X^Ty = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix}\n$$\n$$\nH = X^TX = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  2 \\end{pmatrix}\n$$\n我们现在通过从一个非常大的 $\\lambda$ 开始并减小它来追踪 LASSO 的解路径。\n\n**区域 1：第一个系数进入模型**\n对于足够大的 $\\lambda$，$\\ell_1$ 惩罚项占主导地位，迫使解为 $\\beta = (0, 0, 0)^T$。在这种情况下，非活动系数的 KKT 条件必须对所有 $j$ 成立：$|(c - H\\cdot 0)_j| \\leq \\lambda$，这可以简化为 $|c_j| \\le \\lambda$。当 $\\lambda$ 减小到该不等式首次被违反的点时，第一个系数将变为非零。这发生在：\n$$\n\\lambda_1 = \\max_{j} |c_j| = \\max\\{|1|, |\\frac{1}{2}|, |\\frac{3}{2}|\\} = \\frac{3}{2}\n$$\n因此，对于所有 $\\lambda \\ge \\frac{3}{2}$，解为 $\\beta=0$，支撑集为空集 $\\emptyset$。在 $\\lambda_1 = \\frac{3}{2}$ 时，第三个系数 $\\beta_3$ 即将进入模型。这是 $\\lambda$ 的第一个临界值。\n\n**区域 2：支撑集为 $\\{3\\}$**\n对于 $\\lambda  \\frac{3}{2}$，活动集为 $\\mathcal{A}=\\{3\\}$，因此 $\\beta_1=0$ 且 $\\beta_2=0$。$\\beta_3$ 的符号将是相应相关性的符号，即 $\\text{sign}(c_3)=+1$。活动系数 $\\beta_3$ 的 KKT 条件是：\n$$\nc_3 - (H_{31}\\beta_1 + H_{32}\\beta_2 + H_{33}\\beta_3) = \\lambda \\cdot \\text{sign}(\\beta_3)\n$$\n$$\n\\frac{3}{2} - (1 \\cdot 0 + 1 \\cdot 0 + 2 \\beta_3) = \\lambda \\cdot (+1) \\implies \\frac{3}{2} - 2\\beta_3 = \\lambda \\implies \\beta_3 = \\frac{\\frac{3}{2} - \\lambda}{2} = \\frac{3 - 2\\lambda}{4}\n$$\n只要非活动系数 $\\beta_1$ 和 $\\beta_2$ 的 KKT 条件得到满足，该解就有效：\n- 对于 $j=1$：$|c_1 - H_{13}\\beta_3| \\le \\lambda \\implies |1 - 1 \\cdot \\beta_3| \\le \\lambda$。\n代入 $\\beta_3$：$|1 - \\frac{3-2\\lambda}{4}| = |\\frac{4 - 3 + 2\\lambda}{4}| = |\\frac{1+2\\lambda}{4}|$。由于 $\\lambda \\ge 0$，这变为 $\\frac{1+2\\lambda}{4} \\le \\lambda \\implies 1+2\\lambda \\le 4\\lambda \\implies 1 \\le 2\\lambda \\implies \\lambda \\ge \\frac{1}{2}$。\n- 对于 $j=2$：$|c_2 - H_{23}\\beta_3| \\le \\lambda \\implies |\\frac{1}{2} - 1 \\cdot \\beta_3| \\le \\lambda$。\n代入 $\\beta_3$：$|\\frac{1}{2} - \\frac{3-2\\lambda}{4}| = |\\frac{2 - 3 + 2\\lambda}{4}| = |\\frac{2\\lambda-1}{4}|$。我们需要 $|\\frac{2\\lambda-1}{4}| \\le \\lambda$。\n如果 $\\lambda \\ge \\frac{1}{2}$，这是 $\\frac{2\\lambda-1}{4} \\le \\lambda \\implies 2\\lambda-1 \\le 4\\lambda \\implies -1 \\le 2\\lambda \\implies \\lambda \\ge -\\frac{1}{2}$，这是成立的。\n如果 $\\lambda  \\frac{1}{2}$，这是 $\\frac{1-2\\lambda}{4} \\le \\lambda \\implies 1-2\\lambda \\le 4\\lambda \\implies 1 \\le 6\\lambda \\implies \\lambda \\ge \\frac{1}{6}$。\n支撑集为 $\\{3\\}$ 的路径在所有条件都成立时有效，即 $\\lambda \\ge \\frac{1}{2}$ 和 $\\lambda \\ge \\frac{1}{6}$。约束性条件是 $\\lambda \\ge \\frac{1}{2}$。当这个约束变为等式时，会发生一个新的事件。\n下一个临界值是 $\\lambda_2 = \\frac{1}{2}$，此时系数 $\\beta_1$ 进入模型。\n\n**区域 3：支撑集为 $\\{1, 3\\}$**\n对于 $\\lambda  \\frac{1}{2}$，活动集为 $\\mathcal{A}=\\{1, 3\\}$，因此 $\\beta_2=0$。$\\beta_3$ 的符号保持为 $+1$。进入系数 $\\beta_1$ 的符号由进入时刻相关项的符号给出：$\\text{sign}(c_1-H_{13}\\beta_3)|_{\\lambda=1/2} = \\text{sign}(\\frac{1+2(1/2)}{4}) = \\text{sign}(\\frac{1}{2}) = +1$。\n活动集的 KKT 条件是：\n$$\nc_1 - (H_{11}\\beta_1 + H_{13}\\beta_3) = \\lambda \\cdot (+1) \\implies 1 - (\\beta_1 + \\beta_3) = \\lambda\n$$\n$$\nc_3 - (H_{31}\\beta_1 + H_{33}\\beta_3) = \\lambda \\cdot (+1) \\implies \\frac{3}{2} - (\\beta_1 + 2\\beta_3) = \\lambda\n$$\n我们求解这个关于 $\\beta_1$ 和 $\\beta_3$ 的 $2 \\times 2$ 方程组：\n$$\n\\begin{pmatrix} 1  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_3 \\end{pmatrix} = \\begin{pmatrix} 1 - \\lambda \\\\ \\frac{3}{2} - \\lambda \\end{pmatrix}\n$$\n该矩阵的逆矩阵是 $\\begin{pmatrix} 2  -1 \\\\ -1  1 \\end{pmatrix}$。\n$$\n\\begin{pmatrix} \\beta_1 \\\\ \\beta_3 \\end{pmatrix} = \\begin{pmatrix} 2  -1 \\\\ -1  1 \\end{pmatrix} \\begin{pmatrix} 1 - \\lambda \\\\ \\frac{3}{2} - \\lambda \\end{pmatrix}\n$$\n求解得到：\n$$\n\\beta_1 = 2(1-\\lambda) - (\\frac{3}{2} - \\lambda) = 2 - 2\\lambda - \\frac{3}{2} + \\lambda = \\frac{1}{2} - \\lambda\n$$\n$$\n\\beta_3 = -(1-\\lambda) + (\\frac{3}{2} - \\lambda) = -1 + \\lambda + \\frac{3}{2} - \\lambda = \\frac{1}{2}\n$$\n为使这些解有效，我们需要检查非活动系数 $\\beta_2=0$ 的 KKT 条件：\n$|c_2 - (H_{21}\\beta_1 + H_{23}\\beta_3)| \\le \\lambda$。\n$|\\frac{1}{2} - (0 \\cdot \\beta_1 + 1 \\cdot \\beta_3)| = |\\frac{1}{2} - \\beta_3| = |\\frac{1}{2} - \\frac{1}{2}| = |0| = 0$。\n条件是 $0 \\le \\lambda$，对于所有 $\\lambda > 0$ 都成立。等号在 $\\lambda=0$ 时成立。\n此外，为了使支撑集为 $\\{1,3\\}$，我们要求 $\\beta_1$ 和 $\\beta_3$ 为非零。$\\beta_3=\\frac{1}{2}$ 始终为非零。$\\beta_1 = \\frac{1}{2}-\\lambda > 0$ 要求 $\\lambda  \\frac{1}{2}$。\n对于任何 $\\lambda \\in (0, \\frac{1}{2})$，没有新系数进入模型，也没有活动系数从模型中退出。在此区间内，支撑集始终为 $\\{1,3\\}$。在 $\\lambda=0$ 时，解变为 $\\beta = (\\frac{1}{2}, 0, \\frac{1}{2})^T$，其支撑集也为 $\\{1,3\\}$。\n因此，对于 $\\lambda \\ge 0$，支撑集仅在有限个点上发生变化。\n\n支撑集保持不变的区间是：\n- 对于 $\\lambda \\in [\\frac{3}{2}, \\infty)$：支撑集为 $\\emptyset$。\n- 对于 $\\lambda \\in [\\frac{1}{2}, \\frac{3}{2})$：支撑集为 $\\{3\\}$。\n- 对于 $\\lambda \\in [0, \\frac{1}{2})$：支撑集为 $\\{1, 3\\}$。\n\n当 $\\lambda$ 穿过值 $\\frac{3}{2}$ 和 $\\frac{1}{2}$ 时，支撑集发生变化。这些是 $\\lambda$ 的有限临界值。这些值按降序排列的列表是 $(\\frac{3}{2}, \\frac{1}{2})$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{3}{2}  \\frac{1}{2}\n\\end{pmatrix}\n}\n$$", "id": "3488574"}, {"introduction": "LASSO的解路径是一个关于 $\\lambda$ 的分段线性函数，这一特性是其理论与算法的核心。本练习 [@problem_id:3488551] 让你更深入地探索解路径的几何结构，通过精确计算“节点”（即有效集发生改变的 $\\lambda$ 值）和节点间的路径方向，你将对最小角回归（LARS）等路径跟踪算法的原理有更深刻的认识。", "problem": "考虑最小绝对收缩和选择算子（LASSO）回归问题，对于给定的设计矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和响应向量 $y \\in \\mathbb{R}^{n}$，该问题旨在寻找 $ \\beta(\\lambda) \\in \\mathbb{R}^{p}$ 以最小化目标函数\n$$\n\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\n其中正则化参数 $\\lambda \\ge 0$。假设没有截距，并且 $X$ 的列具有单位欧几里得范数。解路径 $\\beta(\\lambda)$ 作为 $\\lambda$ 的函数是分段线性的，活动集的变化发生在 $\\lambda$ 的一系列离散值处，这些值被称为“结”（knots）。\n\n设 $n=4$，$p=3$，且\n$$\nX = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  0 \\\\\n0  \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}}  0  0 \\\\\n0  0  \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}, \\qquad\ny = \\begin{pmatrix}\n2 \\\\ 1 \\\\ 2 \\\\ 0\n\\end{pmatrix}.\n$$\n$X$ 的所有列确实具有单位欧几里得范数。你可以使用 LASSO 的 Karush–Kuhn–Tucker (KKT) 最优性条件和 $\\ell_{1}$ 范数的次梯度演算作为你的基本出发点。\n\n任务：\n- 确定当 $\\lambda$ 从其初始值减小时，活动集发生变化的前两个结 $\\lambda_{1}$ 和 $\\lambda_{2}$。以闭合形式精确地确定它们。\n- 在区间 $(\\lambda_{2}, \\lambda_{1})$ 上，计算 LASSO 解路径的恒定方向向量 $\\mathrm{d}\\beta/\\mathrm{d}\\lambda \\in \\mathbb{R}^{3}$。\n\n提供精确值，不要四舍五入。将你的最终答案表示为一个单行矩阵，按顺序包含 $\\lambda_{1}$、$\\lambda_{2}$，以及在 $(\\lambda_{2}, \\lambda_{1})$ 上方向向量 $\\mathrm{d}\\beta/\\mathrm{d}\\lambda$ 的三个分量。", "solution": "问题要求我们找出 LASSO 解路径的前两个结 $\\lambda_{1}$ 和 $\\lambda_{2}$，以及在区间 $(\\lambda_{2}, \\lambda_{1})$ 上解路径的方向。LASSO 的目标函数为\n$$\nL(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}\n$$\n其中 $X \\in \\mathbb{R}^{n \\times p}$，$y \\in \\mathbb{R}^{n}$，$\\beta \\in \\mathbb{R}^{p}$，且 $\\lambda \\ge 0$。给定的数据为 $n=4$，$p=3$，\n$$\nX = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  0 \\\\\n0  \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}}  0  0 \\\\\n0  0  \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}, \\qquad\ny = \\begin{pmatrix}\n2 \\\\ 1 \\\\ 2 \\\\ 0\n\\end{pmatrix}.\n$$\n\nLASSO 问题的 Karush-Kuhn-Tucker (KKT) 最优性条件由目标函数的次梯度导出。最小二乘项的梯度为 $\\nabla_{\\beta} \\left(\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2}\\right) = -X^T(y - X\\beta)$。$\\ell_1$-范数的次梯度 $\\partial \\|\\beta\\|_{1}$ 是一个集值函数，其第 $j$ 个分量在 $\\beta_j \\neq 0$ 时为 $\\text{sgn}(\\beta_j)$，在 $\\beta_j=0$ 时为区间 $[-1, 1]$。\nKKT 条件表明，一个解 $\\hat{\\beta}$ 必须满足：\n$$\n0 \\in -X^T(y - X\\hat{\\beta}) + \\lambda \\partial \\|\\hat{\\beta}\\|_{1}\n$$\n这可以按分量重写为：\n\\begin{itemize}\n    \\item 如果 $\\hat{\\beta}_j \\neq 0$，则 $x_j^T(y - X\\hat{\\beta}) = \\lambda \\cdot \\text{sgn}(\\hat{\\beta}_j)$。\n    \\item 如果 $\\hat{\\beta}_j = 0$，则 $|x_j^T(y - X\\hat{\\beta})| \\le \\lambda$。\n\\end{itemize}\n这里，$x_j$ 是 $X$ 的第 $j$ 列。\n\n**步骤1：确定第一个结 $\\lambda_1$**\n\n对于足够大的 $\\lambda$，惩罚项占主导地位，确保最优解为 $\\hat{\\beta} = 0$。在这种情况下，残差为 $y - X\\hat{\\beta} = y$。对于 $\\hat{\\beta}=0$ 的 KKT 条件简化为对所有 $j \\in \\{1, 2, \\dots, p\\}$，都有 $|x_j^T y| \\le \\lambda$。\n第一个结 $\\lambda_1$ 是 $\\lambda$ 的临界值，当 $\\lambda$ 小于此值时，解 $\\hat{\\beta}=0$ 不再是最优解。这发生在当 $\\lambda$ 减小时，条件 $|x_j^T y| \\le \\lambda$ 对某个 $j$ 首次被违反的时候。因此，$\\lambda_1$ 是这些相关性的最大值：\n$$\n\\lambda_1 = \\max_{j} |x_j^T y| = \\|X^T y\\|_{\\infty}\n$$\n我们来计算 $X^T y$：\n$$\nX^T y = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{2}}  0 \\\\\n\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  0  0 \\\\\n0  \\frac{1}{\\sqrt{2}}  0  \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n\\begin{pmatrix}\n2 \\\\ 1 \\\\ 2 \\\\ 0\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{2}{\\sqrt{2}} + \\frac{2}{\\sqrt{2}} \\\\\n\\frac{2}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{4}{\\sqrt{2}} \\\\\n\\frac{3}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n= \\begin{pmatrix}\n2\\sqrt{2} \\\\\n\\frac{3\\sqrt{2}}{2} \\\\\n\\frac{\\sqrt{2}}{2}\n\\end{pmatrix}\n$$\n最大绝对值为：\n$$\n\\lambda_1 = \\max\\left\\{|2\\sqrt{2}|, \\left|\\frac{3\\sqrt{2}}{2}\\right|, \\left|\\frac{\\sqrt{2}}{2}\\right|\\right\\} = 2\\sqrt{2}\n$$\n在 $\\lambda = \\lambda_1$ 处，第一个预测变量 $\\beta_1$ 进入非零系数的活动集。对于略小于 $\\lambda_1$ 的 $\\lambda$ 值，$\\beta_1$ 的符号将是相关性 $x_1^T y$ 的符号，即正号。所以，$\\text{sgn}(\\beta_1) = +1$。\n\n**步骤2：确定在 $(\\lambda_2, \\lambda_1)$ 上的解路径**\n\n对于 $\\lambda \\in (\\lambda_2, \\lambda_1)$，活动集为 $\\mathcal{A} = \\{1\\}$，意味着 $\\beta_1 \\neq 0$ 而 $\\beta_2 = \\beta_3 = 0$。解向量为 $\\beta(\\lambda) = (\\beta_1(\\lambda), 0, 0)^T$。对于活动变量 $\\beta_1$ 的 KKT 条件是一个等式：\n$$\nx_1^T(y - X\\beta) = \\lambda \\cdot \\text{sgn}(\\beta_1)\n$$\n代入 $\\beta = (\\beta_1, 0, 0)^T$ 和 $\\text{sgn}(\\beta_1) = +1$：\n$$\nx_1^T(y - x_1\\beta_1) = \\lambda \\implies x_1^T y - (x_1^T x_1)\\beta_1 = \\lambda\n$$\n问题陈述 $X$ 的列具有单位欧几里得范数，所以 $x_1^T x_1 = \\|x_1\\|_2^2=1$。我们已经计算出 $x_1^T y = 2\\sqrt{2}$。方程变为：\n$$\n2\\sqrt{2} - \\beta_1 = \\lambda\n$$\n解出 $\\beta_1(\\lambda)$，我们得到第一个分量的解路径：\n$$\n\\beta_1(\\lambda) = 2\\sqrt{2} - \\lambda\n$$\n因此，在区间 $(\\lambda_2, \\lambda_1)$ 上，解向量为 $\\beta(\\lambda) = (2\\sqrt{2} - \\lambda, 0, 0)^T$。\n\n**步骤3：确定第二个结 $\\lambda_2$**\n\n第二个结 $\\lambda_2$ 是 $\\lambda  \\lambda_1$ 的一个值，在该值处第二个变量进入活动集。这发生于非活动变量（$j=2$ 或 $j=3$）之一首次满足其 KKT 等式条件 $|x_j^T(y - X\\beta(\\lambda))| = \\lambda$ 时。让我们为非活动变量评估相关性：\n$$\nc_j(\\lambda) = x_j^T(y - X\\beta(\\lambda)) = x_j^T y - x_j^T(x_1\\beta_1(\\lambda)) = (X^T y)_j - (x_j^T x_1)\\beta_1(\\lambda)\n$$\n我们需要 $X$ 的列之间的点积。\n$x_1^T x_2 = (\\frac{1}{\\sqrt{2}})(\\frac{1}{\\sqrt{2}}) + (0)(\\frac{1}{\\sqrt{2}}) + (\\frac{1}{\\sqrt{2}})(0) + (0)(0) = \\frac{1}{2}$。\n$x_1^T x_3 = (\\frac{1}{\\sqrt{2}})(0) + (0)(\\frac{1}{\\sqrt{2}}) + (\\frac{1}{\\sqrt{2}})(0) + (0)(\\frac{1}{\\sqrt{2}}) = 0$。\n\n对于 $j=2$：\n$$\nc_2(\\lambda) = (X^T y)_2 - (x_2^T x_1)\\beta_1(\\lambda) = \\frac{3\\sqrt{2}}{2} - \\frac{1}{2}(2\\sqrt{2} - \\lambda) = \\frac{3\\sqrt{2}}{2} - \\sqrt{2} + \\frac{\\lambda}{2} = \\frac{\\sqrt{2}}{2} + \\frac{\\lambda}{2}\n$$\n变量 2 进入的条件是 $|c_2(\\lambda)| = \\lambda$。因为 $\\lambda > 0$ 且 $\\sqrt{2} > 0$，我们有 $c_2(\\lambda)>0$，所以条件是 $c_2(\\lambda) = \\lambda$：\n$$\n\\frac{\\sqrt{2}}{2} + \\frac{\\lambda}{2} = \\lambda \\implies \\frac{\\sqrt{2}}{2} = \\frac{\\lambda}{2} \\implies \\lambda = \\sqrt{2}\n$$\n\n对于 $j=3$：\n$$\nc_3(\\lambda) = (X^T y)_3 - (x_3^T x_1)\\beta_1(\\lambda) = \\frac{\\sqrt{2}}{2} - (0)\\beta_1(\\lambda) = \\frac{\\sqrt{2}}{2}\n$$\n变量 3 进入的条件是 $|c_3(\\lambda)| = \\lambda$：\n$$\n\\left|\\frac{\\sqrt{2}}{2}\\right| = \\lambda \\implies \\lambda = \\frac{\\sqrt{2}}{2}\n$$\n当我们从 $\\lambda_1 = 2\\sqrt{2}$ 开始减小 $\\lambda$ 时，我们遇到的这两个 $\\lambda$ 值中的第一个是较大的那个。\n$$\n\\lambda_2 = \\max\\left\\{\\sqrt{2}, \\frac{\\sqrt{2}}{2}\\right\\} = \\sqrt{2}\n$$\n在 $\\lambda_2 = \\sqrt{2}$ 处，变量 2 进入模型。\n\n**步骤4：计算在 $(\\lambda_2, \\lambda_1)$ 上的方向向量 $\\frac{\\mathrm{d}\\beta}{\\mathrm{d}\\lambda}$**\n\n在区间 $(\\lambda_2, \\lambda_1) = (\\sqrt{2}, 2\\sqrt{2})$ 上，解路径由下式给出：\n$$\n\\beta(\\lambda) = \\begin{pmatrix} \\beta_1(\\lambda) \\\\ \\beta_2(\\lambda) \\\\ \\beta_3(\\lambda) \\end{pmatrix} = \\begin{pmatrix} 2\\sqrt{2} - \\lambda \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n路径的方向向量是它关于 $\\lambda$ 的导数：\n$$\n\\frac{\\mathrm{d}\\beta}{\\mathrm{d}\\lambda} = \\frac{\\mathrm{d}}{\\mathrm{d}\\lambda} \\begin{pmatrix} 2\\sqrt{2} - \\lambda \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n这是一个恒定向量，与 LASSO 路径的分段线性性质一致。\n\n**结果总结**\n第一个结是 $\\lambda_1 = 2\\sqrt{2}$。\n第二个结是 $\\lambda_2 = \\sqrt{2}$。\n在 $(\\lambda_2, \\lambda_1)$ 上的方向向量是 $\\frac{\\mathrm{d}\\beta}{\\mathrm{d}\\lambda} = (-1, 0, 0)^T$。\n\n最终答案将这些值组合成一个单行矩阵：$(\\lambda_1, \\lambda_2, (\\frac{\\mathrm{d}\\beta}{\\mathrm{d}\\lambda})_1, (\\frac{\\mathrm{d}\\beta}{\\mathrm{d}\\lambda})_2, (\\frac{\\mathrm{d}\\beta}{\\mathrm{d}\\lambda})_3)$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2\\sqrt{2}  \\sqrt{2}  -1  0  0\n\\end{pmatrix}\n}\n$$", "id": "3488551"}, {"introduction": "在实践中，我们通常使用坐标下降法等高效算法来求解LASSO问题。本练习 [@problem_id:3488572] 引导你推导坐标下降法的更新规则，并分析一个至关重要的实践环节：特征标准化。你将理解为何标准化不仅仅是为了数值稳定性，更是保证正则化惩罚对所有特征都公平的关键，这直接关系到模型选择的公正性和最终性能。", "problem": "考虑一个线性模型，其设计矩阵为 $X \\in \\mathbb{R}^{n \\times p}$，列为 $X_j \\in \\mathbb{R}^{n}$ (其中 $j \\in \\{1,\\dots,p\\}$)，响应向量为 $y \\in \\mathbb{R}^{n}$。最小绝对值收缩和选择算子 (LASSO) 通过最小化以下凸目标函数来估计 $\\beta \\in \\mathbb{R}^{p}$\n$$\n\\frac{1}{2}\\,\\|y - X\\beta\\|_2^2 + \\lambda \\,\\|\\beta\\|_1,\n$$\n其中 $\\|\\cdot\\|_2$ 表示欧几里得范数，$\\|\\cdot\\|_1$ 表示 $\\ell_1$ 范数，$\\lambda  0$ 是正则化参数。在循环坐标下降方案中，每一步更新一个坐标 $\\beta_j$，同时保持其他坐标不变。仅从上述目标函数的定义、欧几里得范数的定义以及绝对值函数的次梯度出发，推导单个坐标更新的一维子问题，并根据排除了 $X_j$ 贡献的残差来求解该更新。然后分析每个特征都被标准化以至于对所有 $j$ 都有 $\\|X_j\\|_2^2 = n$ 的特殊情况，并将其与列未标准化的情况进行对比。根据您的推导和推理，选择以下所有正确的陈述。\n\n在全文中，令 $r^{(j)} = y - \\sum_{k \\neq j} X_k \\beta_k$ 表示排除了第 $j$ 个特征贡献的偏残差，令 $S(u,t) = \\mathrm{sign}(u)\\,(|u| - t)_+$ 表示软阈值算子。\n\nA. 在所有 $j$ 都满足 $\\|X_j\\|_2^2 = n$ 的条件下，沿坐标 $j$ 的坐标级最小化器可以写成\n$$\n\\beta_j \\leftarrow S\\!\\left(\\frac{1}{n}\\,X_j^\\top r^{(j)},\\,\\frac{\\lambda}{n}\\right),\n$$\n这表明，一个单一的全局 $\\lambda$ 对所有特征的标准化相关性 $\\frac{1}{n}\\,X_j^\\top r^{(j)}$ 起到了统一的软阈值作用。\n\nB. 在没有标准化的情况下，一个固定的单一 $\\lambda$ 会在所有坐标上引发统一水平的收縮，因为 $\\ell_1$ 惩罚项 $\\|\\beta\\|_1$ 相对于 $X$ 的列是尺度无关的，这使得对所有特征而言，调整 $\\lambda$ 具有同等的可解释性。\n\nC. 在所有 $j$ 都满足 $\\|X_j\\|_2^2 = n$ 的条件下，坐标 $j$ 上零系数的 Karush–Kuhn–Tucker (KKT) 平稳性条件简化为\n$$\n\\left|X_j^\\top r^{(j)}\\right| \\le \\lambda,\n$$\n因此，检验 $\\beta_j = 0$ 是否成立，简化为检查 $X_j$ 和 $r^{(j)}$ 之间的绝对非标准化相关性是否低于单一阈值 $\\lambda$。\n\nD. 如果列未被标准化，$\\|X_j\\|_2$ 较大的特征需要 $\\beta_j$ 的变化较小，才能在线性预测器 $X\\beta$ 中产生相同的变化，从而单位预测效果所产生的 $\\ell_1$ 惩罚更小；对于固定的 $\\lambda$，这类特征相对于 $\\|X_j\\|_2$ 较小的特征会被优先选择。\n\nE. 在 $\\|X_j\\|_2^2 = n$ 的标准化条件下，选择一个固定的单一 $\\lambda$ 会得到与样本大小 $n$ 无关的相同期望稀疏水平，因为将系数设置为零的决策规则不依赖于 $n$。\n\n选择所有正确的选项（一个或多个）。", "solution": "用户要求对 LASSO 优化问题的坐标下降更新进行详细推导和分析，然后对几个陈述进行评估。\n\n### 问题验证\n\n我将首先验证问题陈述。\n\n**第1步：提取已知信息**\n- **模型：** 线性模型。\n- **设计矩阵：** $X \\in \\mathbb{R}^{n \\times p}$，其列为 $X_j \\in \\mathbb{R}^{n}$，$j \\in \\{1,\\dots,p\\}$。\n- **响应向量：** $y \\in \\mathbb{R}^{n}$。\n- **系数向量：** $\\beta \\in \\mathbb{R}^{p}$。\n- **LASSO 目标函数：** $L(\\beta) = \\frac{1}{2}\\,\\|y - X\\beta\\|_2^2 + \\lambda \\,\\|\\beta\\|_1$。\n- **范数：** $\\|\\cdot\\|_2$ 是欧几里得范数，$\\|\\cdot\\|_1$ 是 $\\ell_1$ 范数。\n- **正则化参数：** $\\lambda  0$。\n- **算法：** 循环坐标下降，一次更新一个坐标 $\\beta_j$。\n- **要求的推导：** 推导一维子问题及其关于 $\\beta_j$ 更新的解。\n- **特殊情况：** 分析特征被标准化以至于对所有 $j$ 都有 $\\|X_j\\|_2^2 = n$ 的情况。\n- **定义：**\n    - 偏残差: $r^{(j)} = y - \\sum_{k \\neq j} X_k \\beta_k$。\n    - 软阈值算子: $S(u,t) = \\mathrm{sign}(u)\\,(|u| - t)_+$。\n\n**第2步：使用提取的已知信息进行验证**\n- **科学依据：** 该问题描述了 LASSO，这是高维统计和机器学习中一个基础且广泛使用的技术。目标函数、定义以及坐标下降的使用都是标准且科学合理的。\n- **适定性：** 该问题要求进行特定的推导和分析，这是一个明确定义的数学任务。LASSO 目标函数是凸函数，确保了全局最小值的存在，这使得优化问题是适定的。\n- **客观性：** 该问题以精确的数学语言陈述，没有歧义或主观内容。\n- **缺陷清单：**\n    1.  **科学上不合理：** 无。公式是正确的。\n    2.  **无法形式化：** 无。该问题是标准的数学推导。\n    3.  **不完整/矛盾：** 无。所有必要信息都已提供。\n    4.  **不切实际：** 无。该模型和标准化条件在实践中很常见。\n    5.  **不适定：** 无。推导会得到一个唯一的更新规则。\n    6.  **琐碎/同义反复：** 无。推导需要正确应用次梯度微积分，是一个非平凡的练习。\n    7.  **无法验证：** 无。结果在数学上是可验证的。\n\n**第3步：结论和行动**\n问题陈述有效。我将继续进行推导和评估。\n\n### 坐标级更新的推导\n\nLASSO 目标函数为：\n$$\nL(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1\n$$\n在对第 $j$ 个坐标的坐标下降步骤中，我们固定所有其他系数 $\\beta_k$（其中 $k \\neq j$），并关于 $\\beta_j$ 最小化 $L(\\beta)$。我们可以将线性预测器 $X\\beta$ 重写为：\n$$\nX\\beta = \\sum_{k=1}^p X_k \\beta_k = X_j \\beta_j + \\sum_{k \\neq j} X_k \\beta_k\n$$\n将其代入目标函数，并将不依赖于 $\\beta_j$ 的项归为常数，我们得到关于 $\\beta_j$ 的一维目标函数：\n$$\nL_j(\\beta_j) = \\frac{1}{2}\\left\\|y - \\sum_{k \\neq j} X_k \\beta_k - X_j \\beta_j\\right\\|_2^2 + \\lambda |\\beta_j| + \\lambda \\sum_{k \\neq j} |\\beta_k|\n$$\n使用偏残差的定义 $r^{(j)} = y - \\sum_{k \\neq j} X_k \\beta_k$，需要对 $\\beta_j$ 最小化的目标是：\n$$\n\\arg\\min_{\\beta_j} \\left( \\frac{1}{2}\\|r^{(j)} - X_j \\beta_j\\|_2^2 + \\lambda |\\beta_j| \\right)\n$$\n我们展开平方范数项：\n$$\n\\|r^{(j)} - X_j \\beta_j\\|_2^2 = (r^{(j)} - X_j \\beta_j)^\\top (r^{(j)} - X_j \\beta_j) = \\|r^{(j)}\\|_2^2 - 2\\beta_j X_j^\\top r^{(j)} + \\beta_j^2 \\|X_j\\|_2^2\n$$\n去掉对于 $\\beta_j$ 是常数的项 $\\|r^{(j)}\\|_2^2$，最小化问题等价于：\n$$\n\\arg\\min_{\\beta_j} \\left( \\frac{1}{2}\\beta_j^2 \\|X_j\\|_2^2 - \\beta_j X_j^\\top r^{(j)} + \\lambda |\\beta_j| \\right)\n$$\n这是一个凸函数。我们通过将其关于 $\\beta_j$ 的次梯度设置为 $0$ 来找到最小值。其次梯度为：\n$$\n\\partial_{\\beta_j} L_j(\\beta_j) = \\beta_j \\|X_j\\|_2^2 - X_j^\\top r^{(j)} + \\lambda \\cdot \\partial |\\beta_j|\n$$\n其中 $\\partial |\\beta_j|$ 是绝对值函数的次梯度：\n$$\n\\partial |\\beta_j| = \\begin{cases} \\{\\mathrm{sign}(\\beta_j)\\}  \\text{if } \\beta_j \\neq 0 \\\\ [-1, 1]  \\text{if } \\beta_j = 0 \\end{cases}\n$$\n最优性条件是 $0 \\in \\partial_{\\beta_j} L_j(\\beta_j)$。我们分析最优 $\\beta_j$ 的三种情况：\n\n**情况1：$\\beta_j > 0$**\n次梯度是单值的：$\\partial |\\beta_j| = \\{1\\}$。最优性条件变为：\n$$\n\\beta_j \\|X_j\\|_2^2 - X_j^\\top r^{(j)} + \\lambda = 0 \\implies \\beta_j = \\frac{X_j^\\top r^{(j)} - \\lambda}{\\|X_j\\|_2^2}\n$$\n为了使这个解与假设 $\\beta_j > 0$ 一致，我们必须有 $X_j^\\top r^{(j)} > \\lambda$。\n\n**情况2：$\\beta_j  0$**\n次梯度是单值的：$\\partial |\\beta_j| = \\{-1\\}$。最优性条件变为：\n$$\n\\beta_j \\|X_j\\|_2^2 - X_j^\\top r^{(j)} - \\lambda = 0 \\implies \\beta_j = \\frac{X_j^\\top r^{(j)} + \\lambda}{\\|X_j\\|_2^2}\n$$\n为了使这个解与 $\\beta_j  0$ 一致，我们必须有 $X_j^\\top r^{(j)}  -\\lambda$。\n\n**情况3：$\\beta_j = 0$**\n次梯度是区间 $[-1, 1]$。最优性条件是：\n$$\n0 \\in 0 \\cdot \\|X_j\\|_2^2 - X_j^\\top r^{(j)} + \\lambda [-1, 1] \\implies 0 \\in -X_j^\\top r^{(j)} + [-\\lambda, \\lambda]\n$$\n这意味着 $X_j^\\top r^{(j)} \\in [-\\lambda, \\lambda]$，等价于 $|X_j^\\top r^{(j)}| \\le \\lambda$。\n\n综合这三种情况，我们可以使用软阈值算子 $S(u,t) = \\mathrm{sign}(u)(|u|-t)_+$ 紧凑地写出解。令 $u = \\frac{X_j^\\top r^{(j)}}{\\|X_j\\|_2^2}$ 和 $t = \\frac{\\lambda}{\\|X_j\\|_2^2}$。那么解是：\n$$\n\\hat{\\beta}_j = S\\left(\\frac{X_j^\\top r^{(j)}}{\\|X_j\\|_2^2}, \\frac{\\lambda}{\\|X_j\\|_2^2}\\right)\n$$\n这是一般的坐标级更新规则。\n\n现在，我们分析对所有 $j=1, \\dots, p$ 都进行标准化 $\\|X_j\\|_2^2 = n$ 的特殊情况。更新规则简化为：\n$$\n\\hat{\\beta}_j = S\\left(\\frac{X_j^\\top r^{(j)}}{n}, \\frac{\\lambda}{n}\\right)\n$$\n\n### 评估选项\n\n**A. 在所有 $j$ 都满足 $\\|X_j\\|_2^2 = n$ 的条件下，沿坐标 $j$ 的坐标级最小化器可以写成 $\\beta_j \\leftarrow S\\!\\left(\\frac{1}{n}\\,X_j^\\top r^{(j)},\\,\\frac{\\lambda}{n}\\right)$, 这表明，一个单一的全局 $\\lambda$ 对所有特征的标准化相关性 $\\frac{1}{n}\\,X_j^\\top r^{(j)}$ 起到了统一的软阈值作用。**\n\n我们对标准化情况的推导证实了更新公式 $\\beta_j \\leftarrow S\\left(\\frac{X_j^\\top r^{(j)}}{n}, \\frac{\\lambda}{n}\\right)$。项 $\\frac{1}{n}X_j^\\top r^{(j)}$ 是特征 $j$ 和偏残差之间的样本协方差。因为 $\\|X_j\\|_2^2 = n$，特征 $X_j$ 被标准化为均方根值为 $1$。因此，这个量是一个标准化的相关性度量。软阈值化应用于此量，阈值为 $\\frac{\\lambda}{n}$。由于 $n$ 和 $\\lambda$ 对所有特征都是相同的，这个阈值在所有坐标 $j$ 上是统一的。这种标准化确保了惩罚公平地应用于所有特征，因为它们的“预测潜力”（相关性项）在被阈值化之前是在一个共同的尺度上测量的。该陈述准确地描述了这种情况。\n\n**结论：正确**\n\n**B. 在没有标准化的情况下，一个固定的单一 $\\lambda$ 会在所有坐标上引发统一水平的收縮，因为 $\\ell_1$ 惩罚项 $\\|\\beta\\|_1$ 相对于 $X$ 的列是尺度无关的，这使得对所有特征而言，调整 $\\lambda$ 具有同等的可解釋性。**\n\n根据我们的一般推导，没有标准化时的更新规则是 $\\beta_j \\leftarrow S\\left(\\frac{X_j^\\top r^{(j)}}{\\|X_j\\|_2^2}, \\frac{\\lambda}{\\|X_j\\|_2^2}\\right)$。应用于标准化相关性 $\\frac{X_j^\\top r^{(j)}}{\\|X_j\\|_2^2}$ 的有效阈值是 $\\frac{\\lambda}{\\|X_j\\|_2^2}$。这个阈值明确地依赖于列 $X_j$ 的范数。如果列具有不同的范数，收缩将不是统一的。范数较大的列将具有较小的有效阈值，受到的惩罚较少，反之亦然。\n此外，$\\ell_1$ 惩罚是“尺度无关”的说法是错误的。如果我们把一个列 $X_j$ 缩放一个因子 $c > 0$ 得到 $X'_j = cX_j$，为了保持模型的预测 $X_j\\beta_j$ 不变，系数必须被重新缩放为 $\\beta'_j = \\beta_j/c$。这个系数的惩罚项从 $\\lambda|\\beta_j|$ 变为 $\\lambda|\\beta_j/c|$。这表明 LASSO 解对预测变量的尺度是敏感的。因此，这个陈述中的每个前提都是错误的。\n\n**结论：不正确**\n\n**C. 在所有 $j$ 都满足 $\\|X_j\\|_2^2 = n$ 的条件下，坐标 $j$ 上零系数的 Karush–Kuhn–Tucker (KKT) 平稳性条件简化为 $|X_j^\\top r^{(j)}| \\le \\lambda$, 因此，检验 $\\beta_j = 0$ 是否成立，简化为检查 $X_j$ 和 $r^{(j)}$ 之间的绝对非标准化相关性是否低于单一阈值 $\\lambda$。**\n\n根据我们的推导（情况3），$\\beta_j=0$ 成为坐标级最小化器的条件是 $|X_j^\\top r^{(j)}| \\le \\lambda$。这是次梯度最优性条件的直接结果，并且对于一般的 LASSO 问题都成立，无论 $X$ 的列是否被标准化。在全局最小值处，这个条件必须对所有 $\\hat{\\beta}_j = 0$ 的 $j$ 成立。量 $X_j^\\top r^{(j)}$ 是特征向量 $X_j$ 和偏残差之间的点积（非标准化协方差或“非标准化相关性”）。该条件正确地指出，这个量的绝对值必须小于或等于单一的全局正则化参数 $\\lambda$。虽然提到了标准化条件 $\\|X_j\\|_2^2 = n$，但KKT条件本身是通用的。然而，该陈述的结论和公式是正确的。该条件为何时将一个特征从模型中排除提供了一个明确的规则。\n\n**结论：正确**\n\n**D. 如果列未被标准化，$\\|X_j\\|_2$ 较大的特征需要 $\\beta_j$ 的变化较小，才能在线性预测器 $X\\beta$ 中产生相同的变化，从而单位预测效果所产生的 $\\ell_1$ 惩罚更小；对于固定的 $\\lambda$，这类特征相对于 $\\|X_j\\|_2$ 较小的特征会被优先选择。**\n\n这个陈述准确地描述了特征缩放对 LASSO 的影响。让我们逐一分析。\n1.  系数 $\\Delta \\beta_j$ 的变化导致预测器中 $X_j \\Delta \\beta_j$ 的变化。该变化的幅度为 $\\|\\Delta(X\\beta)\\|_2 = |\\Delta \\beta_j|\\,\\|X_j\\|_2$。要实现一个固定大小的变化 $C$，我们需要 $|\\Delta \\beta_j| = C/\\|X_j\\|_2$。因此，如果 $\\|X_j\\|_2$ 较大，需要的 $|\\Delta \\beta_j|$ 就较小。这是正确的。\n2.  与系数 $\\beta_j$ 相关的 $\\ell_1$ 惩罚是 $\\lambda |\\beta_j|$。较小的系数绝对值 $|\\beta_j|$ 导致较小的惩罚。\n3.  综合这两点：对于给定的期望“预测效果”（$X\\beta$ 的变化），范数 $\\|X_j\\|_2$ 较大的特征 $X_j$ 可以用较小的系数 $\\beta_j$ 来实现它，从而产生较小的 $\\ell_1$ 惩罚。\n4.  因此，LASSO 优化在预测拟合和 $\\ell_1$ 惩罚之间进行权衡，会优先将范数较小的特征的系数收缩至零，并保留范数较大的特征。这个陈述正确地指出了这种偏向。\n\n**结论：正确**\n\n**E. 在 $\\|X_j\\|_2^2 = n$ 的标准化条件下，选择一个固定的单一 $\\lambda$ 会得到与样本大小 $n$ 无关的相同期望稀疏水平，因为将系数设置为零的决策规则不依赖于 $n$。**\n\n从坐标级更新导出的将系数设置为零的决策规则是检查是否 $|\\frac{1}{n}X_j^\\top r^{(j)}| \\le \\frac{\\lambda}{n}$，这等价于 $|X_j^\\top r^{(j)}| \\le \\lambda$。让我们考虑当样本大小 $n$ 增加时，项 $X_j^\\top r^{(j)} = \\sum_{i=1}^n X_{ij} r_i^{(j)}$ 的统计行为。假设数据点是独立同分布（i.i.d.）抽取的，该和的量级通常随 $n$ 增长。对于一个与响应真正相关的特征，这个和通常会以 $O(n)$ 的速率增长，而根据中心极限定理，对于一个不相关的特征，它会以 $O(\\sqrt{n})$ 的速率增长。如果 $\\lambda$ 保持固定，随着 $n$ 的增加，$|X_j^\\top r^{(j)}|$ 的值最终将对越来越多的特征超过固定的阈值 $\\lambda$。这意味着对于固定的 $\\lambda$，随着 $n$ 的增加，模型将变得更稠密（稀疏性降低）。因此，固定的 $\\lambda$ 不会产生相同的期望稀疏水平。为了保持一定的稀疏性或实现理想的统计特性（如选择一致性），$\\lambda$ 必须被选择为 $n$ 的函数（例如，$\\lambda \\sim \\sqrt{n \\log p}$）。该陈述的前提是错误的。\n\n**结论：不正确**\n\n最终的正确选项是 A、C 和 D。", "answer": "$$\\boxed{ACD}$$", "id": "3488572"}]}