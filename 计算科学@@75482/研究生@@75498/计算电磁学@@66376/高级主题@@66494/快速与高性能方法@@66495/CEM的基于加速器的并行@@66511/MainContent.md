## 引言
随着现代科学与工程问题的复杂度日益提升，对大规模、高精度[电磁仿真](@entry_id:748890)的需求变得空前迫切。传统的基于中央处理器（CPU）的计算方法在面对数以亿计的自由度或超长时间的[瞬态分析](@entry_id:262795)时，正逐渐显现其性能瓶颈。在这一背景下，以图形处理器（GPU）为代表的加速器凭借其大规模并行处理能力，为计算电磁学（CEM）领域带来了革命性的机遇。然而，要将GPU的理论峰值性能转化为实际的仿真效率，并非易事。这需要研究人员和工程师对加速器的底层架构有深刻的理解，并掌握一套针对特定C[EM算法](@entry_id:274778)的并行化设计与优化策略。

本文旨在系统性地填补这一知识鸿沟。我们将带领读者深入探索基于加速器的并行计算世界，从基本原理到高级应用，构建一个完整的知识体系。通过学习本文，您将能够：

在 **第一章：原理与机制** 中，您将掌握GPU的SIMT并行模型、多层级内存系统、[控制流](@entry_id:273851)分化处理，以及Roofline性能分析等核心概念。我们将阐明这些原理如何直接影响FDTD和FEM等算法的性能，并介绍数据布局（SoA vs. AoS）、共享内存分块等关键[优化技术](@entry_id:635438)。

在 **第二章：应用与交叉学科联系** 中，我们将把理论付诸实践，展示如何加速核心的CEM求解器（时域、[频域](@entry_id:160070)、[积分方程](@entry_id:138643)法），并探讨如何处理PML[吸收边界](@entry_id:201489)、高阶方法等高级物理模型。此外，本章还将视野拓展至多GPU扩展、多物理场[协同仿真](@entry_id:747416)以及与人工智能结合的自动调优等前沿领域。

在 **第三章：动手实践** 中，您将通过一系列精心设计的问题，亲手建立性能模型、诊断瓶颈，并评估高级[优化技术](@entry_id:635438)（如CUDA Graphs）的适用性，从而将理论知识转化为解决实际问题的能力。

本教程将为您提供必要的理论基础和实践洞察力，助您在[计算电磁学](@entry_id:265339)的研究与开发中，高效地驾驭加速器的强大算力。

## 原理与机制

在深入探讨[计算电磁学](@entry_id:265339)(CEM)中基于加速器的[并行计算](@entry_id:139241)具体应用之前，我们必须首先掌握其背后的基本原理和核心机制。这些原理不仅支配着算法在现代硬件上的性能表现，也指导着我们如何设计和优化代码以充分挖掘硬件潜力。本章将系统性地阐述加速器（尤其是图形处理器GPU）的[并行计算模型](@entry_id:163236)、内存系统特性、性能分析框架，以及这些原理如何应用于FDTD和FEM等核心C[EM算法](@entry_id:274778)，最后还将探讨[负载均衡](@entry_id:264055)和[混合精度计算](@entry_id:752019)等前沿挑战。

### 加速器的[并行计算模型](@entry_id:163236)

现代加速器通过大规模并行处理来实现其卓越的计算[吞吐量](@entry_id:271802)。理解其执行模型是高效利用这些设备的第一步。

#### SIMT与SIMD：从线程到向量

加速器并行性的核心是**[数据并行](@entry_id:172541)**（data parallelism），即同时对数据集中的多个元素执行相同的操作。两种主流的实现模型是中央处理器(CPU)中的**[单指令多数据流](@entry_id:754916)**（Single Instruction, Multiple Data, **SIMD**）和图形处理器(GPU)中的**单指令[多线程](@entry_id:752340)**（Single Instruction, Multiple Threads, **SIMT**）。

- **SIMD** 是一种指令级的并行。CPU中的向量单元（如AVX-512）包含宽度为 $v$ 的向量寄存器（例如，对于[双精度](@entry_id:636927)[浮点数](@entry_id:173316)，$v=8$）。一条[SIMD指令](@entry_id:754851)（如[向量加法](@entry_id:155045)）可以在一个时钟周期内对寄存器中的所有 $v$ 个数据元素执行相同的操作。程序员或编译器需要将数据打包成向量形式来利用这种并行性。

- **SIMT** 是NVIDIA [GPU架构](@entry_id:749972)提出的一种线程级的并行模型。它将成千上万的线程组织成称为**线程块**（thread blocks）的组，而线程块又被划分为大小固定的**线程束**（warps），典型的线程束大小为 $w=32$。在硬件层面，同一个线程束中的所有线程在同一时间执行相同的指令，这个过程称为**锁步执行**（lockstep execution）。尽管程序员编写的是看似独立的标量线程代码，但硬件会以线程束为单位进行调度和执行。

这两种模型在处理CEM中的典型计算模式时表现出显著差异。以麦克斯韦方程的数值解为例，[FDTD方法](@entry_id:263763)在[结构化网格](@entry_id:170596)上进行更新，每个网格点的场值更新依赖于其周围少数邻居，这是一种高度规则的**模板操作**（stencil operation）。而FEM方法则在[非结构化网格](@entry_id:756356)上求解，通常归结为[稀疏线性系统](@entry_id:174902) $\mathbf{K}\mathbf{x}=\mathbf{b}$ 的求解，其核心计算是稀疏矩阵向量乘积（SpMV），涉及不规则的**收集-分发**（gather-scatter）内存访问模式 [@problem_id:3287420]。

对于FDTD的规则模板操作，SIMT和SIMD都能高效执行。一个线程束的32个线程可以分别处理32个相邻的网格点，由于计算模式相同，它们可以完美地锁步执行。

#### [控制流](@entry_id:273851)分化及其应对策略

当同一个线程束内的线程需要执行不同的代码路径时（例如，基于不同的材料属性或边界条件），就会发生**控制流分化**（control-flow divergence）。这是SIMT模型的一个关键挑战。

在SIMT架构下，如果一个线程束发生分化，硬件会串行执行每个不同的代码路径，同时禁用（或屏蔽）那些不走该路径的线程。直到所有路径执行完毕，在**重汇[聚点](@entry_id:177089)**（reconvergence point）重新汇合，线程束才能恢复并行执行。这种分化和串行化会严重降低[计算效率](@entry_id:270255)。例如，在处理包含各向异性和[各向同性材料](@entry_id:170678)的FDTD模拟时，更新本构关系的[计算逻辑](@entry_id:136251)不同。如果一个线程束处理的32个单元中既有[各向异性材料](@entry_id:184874)也有[各向同性材料](@entry_id:170678)，就会触发分化 [@problem_id:3287427]。

相比之下，现代CPU的SIMD架构通过**掩码执行**（masked execution）或[谓词执行](@entry_id:753687)（predicated execution）来处理条件分支。一条向量指令会附带一个掩码（bitmask），该掩码指示哪些数据通道（lanes）应该执行该操作并更新其结果，而其他通道则保持不变。这种方式避免了指令流的串行化，但代价是部分向量通道可能被闲置，从而降低了单条指令的有效工作效率。尽管如此，对于细粒度的、数据相关的条件逻辑，掩码执行通常比SIMT的分支串行化更高效 [@problem_id:3287420]。

为了在SIMT架构上最大化性能，我们必须尽可能减少分化。一个强有力的策略是**工作重排**（work reorganization）。以上述含不同材料的FDTD模拟为例，一个高效的解决方案是首先根据材料标志 $m_i$ 对所有网格单元进行分类，创建两个索引列表：一个包含所有各向同性单元的索引($I_0$)，另一个包含所有各向异性单元的索引($I_1$)。然后，可以启动两个独立的计算核心（kernel），或者在同一个核心中连续处理这两个列表。通过这种方式，处理列表 $I_0$ 的所有线程束都将是均匀的（全部执行标量乘法），处理列表 $I_1$ 的线程束也同样均匀（全部执行张量乘法）。这种“先排序后计算”的策略可以确保绝大多数线程束（除了可能处理列表末尾剩余元素的“尾部”线程束）的[控制流](@entry_id:273851)完全一致，从而将分化降至最低，实现接近1的**控制流均匀度**（control-flow uniformity）[@problem_id:3287427]。

### 内存系统与数据访问优化

在许多CEM应用中，性能瓶颈并非处理器计算速度，而是从内存获取数据的速度。因此，理解并优化数据访问模式至关重要。

#### GPU[内存层次结构](@entry_id:163622)与合并访问

GPU拥有一个多层次的内存系统，主要包括：
- **全局内存**（Global Memory）：容量最大（通常为数GB），但延迟最高，带宽也相对有限。
- **[共享内存](@entry_id:754738)**（Shared Memory）：位于每个流式多处理器（SM）上的片上高速缓存，容量小（数十至上百KB），但延迟极低，带宽极高。
- **寄存器**（Registers）：每个线程私有的最快存储。

为了最大化全局内存的利用效率，GPU内存系统设计了**合并访问**（coalesced memory access）机制。当一个线程束中的32个线程同时访问全局内存时，如果它们访问的地址是连续的且对齐的，硬件可以将这些零散的访问请求合并成一次或少数几次大规模的内存事务（transactions）。例如，一次典型的128字节事务可以一次性满足32个线程对连续单精度浮点数（每个4字节）的请求。反之，如果访问模式是分散的、跨步的（strided）或完全随机的，硬件将需要发出多次内存事务来满足所有请求，导致内存带宽的严重浪费。

#### 数据布局：[数组结构](@entry_id:635205)体(AoS)与结构体数组(SoA)的对决

数据在内存中的布局方式直接决定了访问模式是否能够合并。对于存储矢量场（如[电场](@entry_id:194326) $\mathbf{E}=(E_x, E_y, E_z)$）的CEM应用，两种常见的数据布局是：

- **[数组结构](@entry_id:635205)体**（Array of Structures, **AoS**）：将每个数据点（如网格单元）的所有分量存储在一起。[内存布局](@entry_id:635809)类似于 `[Ex0, Ey0, Ez0, Ex1, Ey1, Ez1, ...]`。
- **结构体数组**（Structure of Arrays, **SoA**）：将所有数据点的同一个分量存储在连续的数组中。[内存布局](@entry_id:635809)类似于 `[Ex0, Ex1, ...], [Ey0, Ey1, ...], [Ez0, Ez1, ...]`。

这两种布局在GPU上的性能差异可能是巨大的。让我们通过一个具体的计算例子来说明。假设一个FDTD核心需要读取一个Yee元胞的所有六个场分量（$E_x, E_y, E_z, H_x, H_y, H_z$），每个分量为4字节。一个线程束的32个线程处理空间上连续的32个元胞。内存系统以128字节的对齐块进行事务处理。

- **在SoA布局下**：当线程束需要读取所有32个元胞的 $E_x$ 分量时，由于这些分量在内存中是连续存储的，它们的总大小为 $32 \times 4 = 128$ 字节。这恰好构成一个完美的合并访问，仅需 **1次** 内存事务即可完成。读取全部分量需要6次这样的操作，总共为 **6次** 事务。

- **在AoS布局下**：每个元胞的数据结构大小为 $6 \times 4 = 24$ 字节。当线程束尝试读取所有32个元胞的 $E_x$ 分量时，第 $t$ 个线程访问的地址与第 $t-1$ 个线程访问的地址相隔24字节（一个结构体的大小）。这种跨步访问模式破坏了合并。第一个线程访问的字节到最后一个线程访问的字节所跨越的总内存范围约为 $31 \times 24 + 4 = 748$ 字节。为了覆盖这片分散的区域，内存系统需要发出 $\lceil 748 / 128 \rceil = 6$ 次128字节的事务。由于需要读取6个分量，每个分量的读取都具有相同的跨步模式，总共需要的事务数为 $6 \times 6 = 36$ 次 [@problem_id:3287501]。

这个例子清晰地表明，对于FDTD中常见的按分量更新的算法，**SoA布局**的性能远超AoS布局，因为它能实现理想的[内存合并](@entry_id:178845)。

#### [非结构化网格](@entry_id:756356)中的不规则访问

与FDTD不同，基于[非结构化网格](@entry_id:756356)的FEM方法天生就具有不规则的内存访问模式。在执行SpMV操作 $\mathbf{y} = \mathbf{K}\mathbf{x}$ 时，计算向量 $\mathbf{y}$ 的一个元素需要从向量 $\mathbf{x}$ 中“收集”多个元素，这些元素的位置由[稀疏矩阵](@entry_id:138197) $\mathbf{K}$ 的列索引决定，而这些索引通常是不连续的。这种不规则的收集操作在GPU上会导致非合并访问，成为FEM加速的主要性能瓶颈之一。

### [性能建模](@entry_id:753340)与优化框架

为了系统地指导优化工作，我们需要一个能够量化性能并识别瓶颈的理论框架。

#### Roofline模型：性能瓶颈的诊断

**Roofline模型**是一个直观的性能模型，它揭示了计算核心的性能受限于其计算吞吐量和内存带宽的程度。该模型定义了一个关键指标：**计算强度**（Arithmetic Intensity, $I$），即程序执行的[浮点运算次数](@entry_id:749457)与从主存访问的总字节数之比。

$I = \frac{\text{浮点运算次数 (FLOPs)}}{\text{内存访问字节数 (Bytes)}}$

一个加速器的理论峰值性能 $P$ （以FLOP/s为单位）受两个“屋顶”的限制：
1.  **计算性能屋顶**：由处理器的峰值[浮点](@entry_id:749453)计算能力 $F$ (FLOP/s) 决定。性能不可能超过这个值：$P \le F$。
2.  **[内存带宽](@entry_id:751847)屋顶**：由内存系统的[峰值带宽](@entry_id:753302) $W$ (Bytes/s) 决定。执行一次计算所需的总时间至少是传输数据所需的时间。因此，性能也被限制为 $P \le W \times I$。

综合这两个限制，一个计算核心能够达到的最高性能 $P_{\max}$ 为：
$P_{\max} = \min(F, W \cdot I) = \min\left(F, W \frac{N}{B}\right)$
其中 $N$ 是每次操作的[浮点运算](@entry_id:749454)数， $B$ 是字节数 [@problem_id:3287430]。

该模型揭示了两个性能区域：
- **内存受限区**（Memory-Bound）：当计算强度 $I$ 较小时，性能受限于[内存带宽](@entry_id:751847)，$P = W \cdot I$。
- **计算受限区**（Compute-Bound）：当计算强度 $I$ 较大时，性能受限于处理器的计算能力，$P = F$。

这两个区域的交点被称为**机器[平衡点](@entry_id:272705)**（Machine Balance）或**脊点**（Ridge Point），其对应的计算强度为 $I^* = \frac{F}{W}$。如果一个算法的计算强度 $I \lt I^*$，它就是内存受限的；如果 $I \gt I^*$，它就是计算受限的。对于内存受限的应用（如大多数CEM中的模板和SpMV操作），提高性能的关键在于增加计算强度或优化内存访问。

#### 共享内存与数据重用：提升计算强度

增加计算强度 $I$ 的一个主要方法是最大化**数据重用**（data reuse）。通过将频繁访问的数据从慢速的全局内存加载到快速的片上共享内存中，可以多次使用这些数据而无需重复访问全局内存，从而减少了分母（内存访问字节数），提高了有效计算强度。

对于FDTD这类[模板计算](@entry_id:755436)，**分块**（tiling）或**分片**（tiling）是一种经典的优化策略。一个线程块负责计算一个三维数据“块”（tile）的更新。在计算开始前，整个线程块协作将这个数据块及其计算所需的**光环区域**（halo region）从全局内存加载到共享内存中。

例如，对于一个需要访问6个最近邻居的7点模板，一个大小为 $T_x \times T_y \times T_z$ 的内部计算区域需要一个宽度为 $h=1$ 的光环。假设共享内存总容量为 $S$ 个标量元素，一个线程块可以加载的总数据量为 $(T_x+2h)(T_y+2h)(T_z+2h) \le S$。我们的目标是最大化“计算-通信比”，即每个加载到共享内存的元素所能支持的计算次数。这等价于在共享内存容量固定的情况下，最大化内部计算区域的体积 $V_{compute} = T_x T_y T_z$。

通过[约束优化](@entry_id:635027)可以证明，当加载的数据块形状 $(T_x+2h) \times (T_y+2h) \times (T_z+2h)$ 为立方体时，其内部计算区域的体积最大化。因此，最优的内部计算块尺寸为各向同性的，即 $T_x^\star = T_y^\star = T_z^\star = S^{1/3} - 2h$。选择立方体形状的计算块可以最小化表面积与体积之比，从而最大化数据重用 [@problem_id:3287465]。

#### 占用率与资源管理

**占用率**（Occupancy）是衡量GPU上SM利用率的一个指标，定义为一个SM上活跃线程束的数量与该SM支持的最大线程束数量之比。更高的占用率有助于硬件更好地隐藏内存访问延迟。

占用率受到多种资源的限制，包括每个SM的寄存器数量、线程数量以及[共享内存](@entry_id:754738)容量。假设[共享内存](@entry_id:754738)是主要限制因素，每个SM的总共享内存预算为 $S$，而每个线程块需要 $n \cdot s$ 的[共享内存](@entry_id:754738)（其中 $n$ 是块内处理的元素数量， $s$ 是每个元素的开销）。那么，可以驻留在单个SM上的线程块数量为 $B = \lfloor \frac{S}{n \cdot s} \rfloor$。

一个有趣的问题是，如何选择块大小 $n$ 来最大化SM上的总活跃元素数量（即总吞吐量），$O = B \cdot n$。通过分析可以发现，$O(n) = n \cdot \lfloor \frac{S}{n \cdot s} \rfloor \le n \cdot (\frac{S}{n \cdot s}) = \frac{S}{s}$。这个理论上界 $O_{\max} = \lfloor S/s \rfloor$ 是可以达到的。为了达到这个最大值，我们需要选择一个 $n$ 使得 $n$ 是 $\lfloor S/s \rfloor$ 的一个因子。其中最大的可能选择是 $n_{\max} = \lfloor S/s \rfloor$。在这种情况下，每个SM上只驻留一个线程块（$B=1$），但这一个块就足以使SM上的活跃元素数量达到最大化。这说明，为了最大化[吞吐量](@entry_id:271802)，我们应该设计尽可能大的线程块，直到它几乎用尽SM的[共享内存](@entry_id:754738)资源 [@problem_id:3287471]。

### 核心C[EM算法](@entry_id:274778)的并行实现

现在我们将上述原理应用于FDTD和FEM这两种在CEM中广泛使用的方法。

#### [时域有限差分法 (FDTD)](@entry_id:261431) 的加速

[FDTD方法](@entry_id:263763)由于其在[结构化网格](@entry_id:170596)上的规则模板操作，是[GPU加速](@entry_id:749971)的理想候选者。其并行实现的关键在于高效的内存访问和对[CFL条件](@entry_id:178032)的深刻理解。

**CFL条件对性能的深远影响**

FDTD的[显式时间步进](@entry_id:168157)格式受到**Courant–Friedrichs–Lewy (CFL)** 稳定条件的严格限制。对于三维[Yee网格](@entry_id:756803)，最大允许时间步长 $\Delta t_{\max}$ 为：
$$ \Delta t_{\max} = \frac{1}{c_0 \sqrt{\frac{1}{\Delta x^2} + \frac{1}{\Delta y^2} + \frac{1}{\Delta z^2}}} $$
其中 $c_0$ 是光速，$\Delta x, \Delta y, \Delta z$ 是网格间距。这个条件意味着时间步长由最小的网格尺寸决定。例如，如果网格尺寸为毫米量级，如 $\Delta x = 1.25 \,\mathrm{mm}, \Delta y = 0.75 \,\mathrm{mm}, \Delta z = 2.00 \,\mathrm{mm}$，那么 $\Delta t_{\max}$ 将会是皮秒（$10^{-12}$ s）量级 [@problem_id:3287490]。

这个极小的时间步长对GPU性能有重大影响：
1.  **高频内核启动**：模拟一段有物理意义的时间（如纳秒）需要数百万个时间步。如果每个时间步都从CPU启动一个GPU内核，那么内核启动的开销（通常为微秒量级）将累积成一个巨大的性能负担，可能远超实际计算时间。
2.  **高频同步**：在多GPU或域分解的场景中，每个[子域](@entry_id:155812)边界上的数据（光环区域）需要在每个时间步进行交换和同步。极小的时间步意味着极高的同步频率，这使得计算的**通信-计算比**急剧升高，很容易使仿真变为通信受限。

为了克服这些问题，**持久化内核**（persistent kernels）策略应运而生。该策略只启动一个长期运行的内核，该内核内部包含一个循环，执行成千上万个时间步的更新。这样，昂贵的内核启动开销被分摊，而块间同步则通过更高效的设备级同步机制在GPU内部完成。

#### 有限元法 (FEM) 的加速

与FDTD相比，在GPU上加速[非结构化网格](@entry_id:756356)的FEM更具挑战性，核心在于高效实现SpMV。

**[稀疏矩阵存储格式](@entry_id:147618)的选择**

SpMV的性能高度依赖于[稀疏矩阵](@entry_id:138197)的存储格式。针对[GPU架构](@entry_id:749972)，有多种格式可供选择，每种都有其优缺点：
- **压缩稀疏行 (CSR)**：这是最通用和内存效率最高的格式之一。然而，对于GPU上“每行一个线程”的朴素实现，行长的巨大差异会导致严重的负载不平衡和非合并内存访问。更先进的“每行一个线程束”或“向量”CSR内核可以为长行实现合并访问和良好的负载平衡，使其成为处理行长[分布](@entry_id:182848)不均（如具有[重尾分布](@entry_id:142737)的FEM矩阵）的有力竞争者 [@problem_id:3287467]。
- **Ellpack-Itpack (ELL)**：此格式将所有行填充到与最长行相同的长度 $K$，从而形成一个规则的、类似稠密的结构。这使得内存访问完全合并，负载完全平衡。但其代价是巨大的填充开销，特别是对于行长[分布](@entry_id:182848)不均的矩阵，其内存和计算开销都可能高得令人无法接受。
- **[混合格式](@entry_id:167436) (HYB)**：HYB是ELL和[坐标格式(COO)](@entry_id:144564)的结合。它使用一个宽度适中（如接近行长中位数）的ELL部分来存储矩阵的“规则”主体，而将超出此宽度的“异常”长行中的元素存储在COO部分。这种方法在保持大部分数据访问规则性的同时，显著减少了填充开销，非常适合处理具有中度到高度不规则行长的FEM矩阵 [@problem_id:3287467]。

**通过重排序提升局部性**

为了改善SpMV中对输入向量 $\mathbf{x}$ 的不规则访问模式，可以对网格节点（即矩阵的行和列）进行**重排序**（reordering）。像**Reverse Cuthill–McKee (RCM)** 这样的[带宽缩减](@entry_id:746660)算法可以重新编号网格节点，使得物理上相邻的节点在索引空间中也更接近。这使得[稀疏矩阵](@entry_id:138197)的非零元素更紧密地聚集在对角线周围。其结果是，在执行SpMV时，对向量 $\mathbf{x}$ 的访问具有了更好的**[空间局部性](@entry_id:637083)**（spatial locality）。虽然这不一定能实现完美的合并访问，但它能显著提高GPU缓存和[CPU缓存](@entry_id:748001)的命中率，从而改善整体内存访问性能 [@problem_id:3287420]。

### 高级主题与现代挑战

随着CEM模拟变得越来越复杂，新的性能挑战不断出现，要求我们采用更先进的并行策略。

#### 负载不平衡与分区策略

在采用$hp$-[自适应网格](@entry_id:164379)（同时加密网格尺寸$h$和提高多项式阶数$p$）和**[局部时](@entry_id:194383)间步长**（local time-stepping, LTS）等先进技术的求解器（如[不连续伽辽金法](@entry_id:748485)DG-FEM）中，**负载不平衡**（workload imbalance）问题变得极其严重。

$hp$-自适应会产生计算量和内存需求差异极大的单元。一个高阶($p$)、小尺寸($h$)的单元，其自由度数量($N_{\text{dof},e} \propto p^3$)和本地更新次数($\nu_e \propto 1/\Delta t_e \propto p^2/h_e$)可能比一个低阶、大尺寸的单元高出几个[数量级](@entry_id:264888)。因此，每个单元的计算权重 $c_e$ 和内存权重 $m_e$ 表现出高度的异构性 [@problem_id:3287446]。

在这种情况下，若要将问题划分到多个GPU上进行并行计算，简单的几何分区或按单元数量均分任务会导致灾难性的负载不平衡——一些GPU严重过载，而另一些则大部[分时](@entry_id:274419)间处于空闲状态。因此，必须采用**加权分区**策略：

- **基于[空间填充曲线](@entry_id:161184) (SFC) 的分区**：首先，使用像希尔伯特曲线这样的SFC为所有单元生成一个一维排序，这种排序能很好地保持空间的局部性。然后，不再是按数量等分这个列表，而是根据累积的计算权重 $\sum c_e$ 将其切分为$K$段，使得每段的权重总和大致相等。同时，还需要检查并确保每段的内存权重总和 $\sum m_e$ 不超过单个GPU的内存上限 $M$。

- **基于[图分割](@entry_id:152532)的分区**：这是一种更强大的方法。将网格视为一个图，其中每个单元是一个带权重的节点（权重为$c_e$和$m_e$），单元间的邻接关系是带权重的边（权重代表通信成本）。目标是执行一个多约束的$k$-路[图分割](@entry_id:152532)，在最小化切[割边](@entry_id:266750)总权重（即最小化通信）的同时，确保每个分区内的节点计算权重总和得到平衡，并且内存权重总和满足约束。

这两种方法都能有效地解决由AMR和LTS引起的严重负载不[平衡问题](@entry_id:636409)，是实现大规模、自适应CEM模拟[可扩展性](@entry_id:636611)的关键技术 [@problem_id:3287446]。

#### [混合精度计算](@entry_id:752019)

在追求极致性能的过程中，使用较低的[浮点精度](@entry_id:138433)（如32位单精度FP32甚至16位半精度FP16）代替64位[双精度](@entry_id:636927)FP64成为一种趋势。然而，这需要在性能和[数值精度](@entry_id:173145)之间做出审慎的权衡。

**精度与[数值色散](@entry_id:145368)：FDTD中的权衡**

在FDTD中，除了由离散化方案本身引入的**截断误差**（truncation error）导致的数值色散外，有限的[浮点精度](@entry_id:138433)也会引入**舍入误差**（rounding error）。[数值色散关系](@entry_id:752786)表明，数值相速度对CFL数 $S = c\Delta t/\Delta x$ 的微小扰动是敏感的。一次舍入操作引入的相对误差由机器epsilon（[单位舍入误差](@entry_id:756332)）$u$ 限定。对于FP32， $u_{32} \approx 2^{-24}$，而对于FP64， $u_{64} \approx 2^{-53}$。这意味着，由于对CFL数和更新系数的舍入，FP32引入的累积[相位误差](@entry_id:162993)要比FP64大得多。在长时间模拟中，FP64的[舍入误差](@entry_id:162651)几乎可以忽略不计，而FP32的误差则可能变得显著，尽管通常仍小于截断误差 [@problem_id:3287496]。

**利用Tensor Core的稳定[混合精度](@entry_id:752018)方案**

现代GPU配备了如**Tensor Cores**之类的专用硬件单元，它们能以极高的[吞吐量](@entry_id:271802)执行低精度（如FP16）输入的矩阵乘加运算，同时在内部使用较高精度（如FP32）进行累加。这为设计高效且稳健的**[混合精度](@entry_id:752018)**（mixed-precision）算法提供了可能。一个用于FDTD的优秀[混合精度](@entry_id:752018)方案如下：

1.  **以FP64保证稳定性**：所有与CFL[稳定性判据](@entry_id:755304)相关的计算，如计算 $\Delta t$、CFL数 $S$ 以及最终的更新系数，都应在FP64下进行。这可以避免因[舍入误差](@entry_id:162651)导致实际的CFL数意外超过稳定极限而引发不稳定。

2.  **以FP32存储场量**：考虑到全局内存的容量和带宽限制，庞大的[电场和磁场](@entry_id:261347)数组可以存储为FP32格式。这在节省一半内存的同时，对于大多数应用场景，其精度和动态范围已经足够。

3.  **以低精度和Tensor Cores加速计算**：在执行FDTD的卷曲（curl）运算时，可以将从FP32场数组中读取的邻居值和在FP64下计算好的系数转换为FP16或TF32等低精度格式，然后送入Tensor Cores进行高速的乘加运算。由于累加是在FP32下完成的，这能有效控制累加过程中的精度损失。

这种分层使用不同精度的策略，既利用了FP64的稳健性来执行关键的稳定性检查，又利用了FP32的内存效率来存储数据，还利用了Tensor Core的强大计算能力来加速核心计算，从而在保证数值稳定性的前提下实现了性能的最大化 [@problem_id:3287496]。