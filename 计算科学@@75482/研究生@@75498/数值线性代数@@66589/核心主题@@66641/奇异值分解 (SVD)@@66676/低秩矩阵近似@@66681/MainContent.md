## 引言
在一个由[高维数据](@entry_id:138874)主导的时代，揭示其背后简洁的潜在结构变得至关重要。低秩[矩阵近似](@entry_id:149640)为此提供了一个强大的数学框架，其核心思想是，许多复杂的数据集——从用户评分到基因组数据——都可以被一个更简单的、秩更低的矩阵有效表示。本文旨在解决一个关键问题：如何将优雅的最优近似理论与大规模计算的实际挑战联系起来。它将带领读者全面了解低秩方法，从基本原理到前沿算法，再到它们在现实世界中的应用。

在接下来的章节中，您将踏上一段结构化的学习之旅。第一章 **“原理与机制”** 将奠定理论基础，深入剖析[奇异值分解](@entry_id:138057)（SVD）和著名的[Eckart-Young-Mirsky定理](@entry_id:149772)，然后转向处理海量数据所需的高效算法。第二章 **“应用与[交叉](@entry_id:147634)学科联系”** 将展示这些方法在机器学习、信号处理和[科学计算](@entry_id:143987)等不同领域的多功能性，阐明抽象概念如何转化为具体解决方案。最后，**“动手实践”** 部分将提供机会，将这些概念应用于具体问题，从而巩固您对理论和实践细节的理解。这次全面的探索将使您具备在自己的工作中理解和应用低秩[矩阵近似](@entry_id:149640)的知识。

## 原理与机制

本章深入探讨低秩[矩阵近似](@entry_id:149640)的核心数学原理与计算机制。我们将从经典的基于奇异值分解（Singular Value Decomposition, SVD）的最优近似理论出发，探索其成立的条件、唯一性与稳定性。随后，我们将转向处理大规模问题的现代计算方法，特别是随机算法的兴起，并分析其效率和理论保证。最后，我们将讨论一些前沿主题，包括在噪声下的基本性能极限和结构化约束下的近似问题。

### 基于SVD的最优低秩近似

对于低秩[矩阵近似](@entry_id:149640)问题，[奇异值分解](@entry_id:138057)（SVD）不仅是一种计算工具，更是理论的基石。它揭示了任何矩阵内在的低维结构，并为寻找最佳近似提供了直接的途径。

#### Eckart-Young-Mirsky 定理

所有低秩近似理论的出发点是 Eckart-Young-Mirsky 定理，它精确地刻画了给定矩阵的最佳低秩近似。为了理解该定理，我们首先要回顾 **[奇异值分解](@entry_id:138057) (Singular Value Decomposition, SVD)**。对于任意实矩阵 $A \in \mathbb{R}^{m \times n}$，其 SVD 分解形式为：

$A = U \Sigma V^{\top}$

其中 $U \in \mathbb{R}^{m \times m}$ 和 $V \in \mathbb{R}^{n \times n}$ 是[正交矩阵](@entry_id:169220)，它们的列向量分别称为[左奇异向量](@entry_id:751233)和[右奇异向量](@entry_id:754365)。$\Sigma \in \mathbb{R}^{m \times n}$ 是一个（伪）[对角矩阵](@entry_id:637782)，其对角线上的元素 $\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_r > 0$（其中 $r = \operatorname{rank}(A)$）被称为奇异值，其余元素均为零。

SVD 将[矩阵分解](@entry_id:139760)为一系列秩为1的成分之和：$A = \sum_{i=1}^{r} \sigma_i u_i v_i^{\top}$，其中 $u_i$ 和 $v_i$ 分别是 $U$ 和 $V$ 的第 $i$ 列。这种分解自然地引出一个想法：如果矩阵的奇异值迅速衰减，我们或许可以通过保留最大的几个[奇异值](@entry_id:152907)及对应的[奇异向量](@entry_id:143538)来构造一个很好的近似。

Eckart-Young-Mirsky 定理精确地证实了这一直觉。该定理指出，对于一类被称为 **[酉不变范数](@entry_id:185675) (unitarily invariant norms)** 的[矩阵范数](@entry_id:139520)，截断 SVD（truncated SVD）给出了最佳的 $k$ 秩近似。[酉不变范数](@entry_id:185675)是指对于任意正交矩阵 $U_0$ 和 $V_0$，范数满足 $\|U_0 A V_0\| = \|A\|$。两个最重要和最常用的[酉不变范数](@entry_id:185675)是 **[谱范数](@entry_id:143091) (spectral norm)** $\| \cdot \|_2$ 和 **[弗罗贝尼乌斯范数](@entry_id:143384) (Frobenius norm)** $\| \cdot \|_F$。

**Eckart-Young-Mirsky 定理**：令 $A = \sum_{i=1}^{r} \sigma_i u_i v_i^{\top}$ 为矩阵 $A$ 的 SVD。对于任意整数 $k  r$，在所有秩至多为 $k$ 的矩阵 $X$ 中，距离 $A$ 最近的矩阵由截断 SVD 给出，即：
$$
A_k = \sum_{i=1}^{k} \sigma_i u_i v_i^{\top}
$$
这个解 $A_k$ 在所有[酉不变范数](@entry_id:185675)下都是最优的。特别地，对于[谱范数](@entry_id:143091)和[弗罗贝尼乌斯范数](@entry_id:143384)，最小近似误差为：
$$
\min_{\operatorname{rank}(X) \le k} \|A - X\|_2 = \|A - A_k\|_2 = \sigma_{k+1}
$$
$$
\min_{\operatorname{rank}(X) \le k} \|A - X\|_F = \|A - A_k\|_F = \left( \sum_{i=k+1}^{r} \sigma_i^2 \right)^{1/2}
$$
这个结果意义非凡：它提供了一个直接的、非迭代的构造最优近似的方法，并且误差可以精确地由被舍弃的[奇异值](@entry_id:152907)量化 [@problem_id:3557713]。

#### 最优性的条件与唯一性

Eckart-Young-Mirsky 定理的普适性依赖于两个关键条件：范数的[酉不变性](@entry_id:198984)和[奇异值](@entry_id:152907)的[分布](@entry_id:182848)。

首先，**[酉不变性](@entry_id:198984)** 是至关重要的。如果[矩阵范数](@entry_id:139520)不具备此性质，那么 SVD 截断可能不再是最优的。我们可以通过一个具体的例子来说明这一点。考虑矩阵 $A = I_2 = \begin{pmatrix} 1  0 \\ 0  1 \end{pmatrix}$ 和一种非[酉不变范数](@entry_id:185675)——逐元素 $\ell_{\infty}$ 范数，定义为 $\|M\|_{\infty} = \max_{i,j} |M_{ij}|$。$A$ 的奇异值为 $\sigma_1 = 1, \sigma_2 = 1$。标准的秩1截断 SVD 近似是 $A_1 = \begin{pmatrix} 1  0 \\ 0  0 \end{pmatrix}$，其误差为 $\|A - A_1\|_{\infty} = \|\begin{pmatrix} 0  0 \\ 0  1 \end{pmatrix}\|_{\infty} = 1$。然而，我们可以构造另一个秩1矩阵 $B = \begin{pmatrix} 1/2  1/2 \\ 1/2  1/2 \end{pmatrix}$。此时，误差为 $\|A - B\|_{\infty} = \|\begin{pmatrix} 1/2  -1/2 \\ -1/2  1/2 \end{pmatrix}\|_{\infty} = 1/2$。这个误差明显小于 SVD 截断产生的误差。这表明，对于非[酉不变范数](@entry_id:185675)，寻找最优低秩近似可能需要解决一个更复杂的、依赖于特定范数结构的[优化问题](@entry_id:266749) [@problem_id:3557734]。

其次，最优解的 **唯一性** 取决于奇异值的[谱隙](@entry_id:144877)（spectral gap）。Eckart-Young-Mirsky 定理同时指出，最佳 $k$ 秩近似 $A_k$ 是唯一的，当且仅当 $\sigma_k  \sigma_{k+1}$。如果第 $k$ 个和第 $k+1$ 个[奇异值](@entry_id:152907)相等，即 $\sigma_k = \sigma_{k+1}$，那么最优解将不再唯一。

这种非唯一性源于与重复奇异值相关的奇异向量选择的自由度。考虑矩阵 $A = \operatorname{diag}(7, 4, 4, 1)$，其[奇异值](@entry_id:152907)为 $\sigma_1=7, \sigma_2=4, \sigma_3=4, \sigma_4=1$。我们寻求最佳秩2近似（$k=2$）。这里，$\sigma_2 = \sigma_3 = 4$，因此唯一性条件不满足。与[奇异值](@entry_id:152907)4相关的左、[右奇异向量](@entry_id:754365)张成一个二维[子空间](@entry_id:150286)，即 $\operatorname{span}\{e_2, e_3\}$。最佳秩2近似必须包含与 $\sigma_1=7$ 对应的部分，即 $7 e_1 e_1^{\top}$。对于第二部分，我们需要从与奇异值4相关的[子空间](@entry_id:150286)中选择一个秩1分量。我们可以选择 $4 e_2 e_2^{\top}$，得到近似 $A_{2, a} = \operatorname{diag}(7, 4, 0, 0)$。我们也可以选择 $4 e_3 e_3^{\top}$，得到另一个同样最优的近似 $A_{2, b} = \operatorname{diag}(7, 0, 4, 0)$。事实上，任何形如 $7e_1e_1^\top + 4uu^\top$ 的矩阵都是最优解，其中 $u$ 是 $\operatorname{span}\{e_2, e_3\}$ 中的任意[单位向量](@entry_id:165907)。这构成了一个无穷的最优[解集](@entry_id:154326)合 [@problem_id:3557704]。尽管最优近似矩阵不唯一，但最优近似误差是唯一的，始终等于 $\sqrt{\sigma_3^2 + \sigma_4^2} = \sqrt{4^2+1^2} = \sqrt{17}$。

#### 近似的稳定性：扰动理论

在实际应用中，我们处理的矩阵往往是带噪声的或经过某种扰动的。一个自然的问题是：当矩阵 $A$ 受到一个小的扰动 $E$ 变为 $\tilde{A} = A+E$ 时，其最佳低秩[子空间](@entry_id:150286)会发生多大变化？这个问题属于[矩阵扰动理论](@entry_id:151902)的范畴，其核心成果如 **Davis-Kahan 定理** 和 **Wedin 定理** 为我们提供了答案。

这些定理量化了奇异[子空间](@entry_id:150286)（即由奇异向量张成的空间）在扰动下的稳定性。它们给出了原始[子空间](@entry_id:150286)与扰动后[子空间](@entry_id:150286)之间 **主角度 (principal angles)** 的正弦值的[上界](@entry_id:274738)。这个上界通常取决于扰动的大小（如 $\|E\|_2$ 或 $\|E\|_F$）以及原矩阵[奇异值](@entry_id:152907)的[谱隙](@entry_id:144877)。一个大的[谱隙](@entry_id:144877)（例如 $\sigma_k - \sigma_{k+1}$ 很大）意味着对应的[子空间](@entry_id:150286)是稳定的，对扰动不敏感。反之，一个小的谱隙则意味着[子空间](@entry_id:150286)不稳定，微小的扰动可能导致[子空间](@entry_id:150286)发生剧烈变化。

我们可以通过一个简单的 $2 \times 2$ 例子来具体感受这一点 [@problem_id:3557688]。考虑原矩阵 $A(\sigma) = \begin{pmatrix} \sigma  0 \\ 0  0 \end{pmatrix}$ 和扰动后的矩阵 $\tilde{A}(t) = \begin{pmatrix} \sigma  0 \\ t  0 \end{pmatrix}$。原矩阵的秩1[子空间](@entry_id:150286)（由其最大[左奇异向量](@entry_id:751233)张成）是 $x$ 轴，即 $\operatorname{span}\{e_1\}$。通过计算 $\tilde{A}(t)\tilde{A}(t)^{\top}$ 的[特征向量](@entry_id:151813)，可以发现扰动后矩阵的秩1[子空间](@entry_id:150286)由向量 $\begin{pmatrix} \sigma \\ t \end{pmatrix}$ 张成。这两个[子空间](@entry_id:150286)之间的角度 $\theta(t)$ 满足 $\sin\theta(t) = \frac{|t|}{\sqrt{\sigma^2 + t^2}}$。Wedin 定理给出的[上界](@entry_id:274738)形式为 $\sin\theta(t) \le C \frac{\|E\|_2}{\delta}$，其中 $\|E\|_2 = |t|$，[谱隙](@entry_id:144877) $\delta = \sigma_1(A) - \sigma_2(A) = \sigma - 0 = \sigma$。在本例中，一个精确的界为 $\frac{|t|}{\sigma}$。当 $t \to 0$ 时，我们发现 $\sin\theta(t)$ 与其[上界](@entry_id:274738) $\frac{|t|}{\sigma}$ 之比趋近于1，这表明理论界是紧的，并且精确地捕捉了[子空间](@entry_id:150286)对扰动的线性响应。

### 大规模问题的计算方法

虽然SVD为低秩近似提供了完美的理论解，但对于现代数据科学中常见的超大规模矩阵，直接计算完整SVD的成本（通常为 $\mathcal{O}(\min(m, n)mn)$）是无法接受的。这催生了各种高效的近似算法，主要分为两大类：经典的迭代方法和现代的[随机化](@entry_id:198186)方法。

#### 经典迭代方法：Lanczos [双对角化](@entry_id:746789)

对于[大型稀疏矩阵](@entry_id:144372)，一类经典的方法是Krylov[子空间迭代](@entry_id:168266)法，其中 **[Golub-Kahan-Lanczos](@entry_id:749961) [双对角化](@entry_id:746789)** 是计算奇异值和[奇异向量](@entry_id:143538)的代表。该算法通过与 $A$ 和 $A^{\top}$ 的交替乘积，迭代地构建一个小的双[对角矩阵](@entry_id:637782) $B$。$B$ 的[奇异值](@entry_id:152907)（称为[Ritz值](@entry_id:145862)）能够很好地逼近 $A$ 的极端奇异值（即最大和最小的奇异值）。

Lanczos 方法的优点是它能以高精度收敛到最大的几个[奇异值](@entry_id:152907)。然而，它也存在一些计算上的挑战。首先，为了进行 $k$ 步迭代，它需要对矩阵 $A$ 进行约 $2k$ 次遍历（即矩阵-向量乘积），这在数据存储于慢速内存或分布式系统时可能成为瓶颈。其次，为了保持[数值稳定性](@entry_id:146550)，迭代过程中产生的向量必须进行（部分或完全）**[再正交化](@entry_id:754248)**，这会增加计算成本和节点间的[通信开销](@entry_id:636355) [@problem_id:3557693]。

#### 现代随机算法

近年来，随机算法为大规模低秩近似问题带来了[范式](@entry_id:161181)转变。其核心思想是，通过[随机投影](@entry_id:274693)或采样，将原矩阵的信息压缩到一个小得多的“草图”（sketch）矩阵中，然后仅对这个小矩阵进行精确的SVD计算。这类算法通常更快、更易于并行化，并能处理流式数据。

##### 随机范围搜索 (Randomized Range Finding)

随机范围搜索的基本策略分为两步：
1.  找到一个矩阵 $Q$，其列构成一个近似的[正交基](@entry_id:264024)，能够很好地张成 $A$ 的[列空间](@entry_id:156444)（即 $A$ 的值域）。
2.  通过计算 $B = Q^{\top}A$ 并对小矩阵 $B$ 进行SVD，来获得 $A$ 的近似SVD。

关键在于如何高效且可靠地构造 $Q$。一个简单而强大的方法是，生成一个随机高斯矩阵 $\Omega \in \mathbb{R}^{n \times \ell}$（其中 $\ell$ 略大于目标秩 $k$，例如 $\ell = k+p$，p为[过采样](@entry_id:270705)参数），然后形成样本矩阵 $Y = A\Omega$。理论上，如果 $A$ 的列空间是 $k$ 维的，那么 $Y$ 的 $\ell$ 个列向量很可能也张成这个空间。对 $Y$ 进行QR分解，即可得到 $Q$。

当矩阵的奇异值衰减缓慢时，为了提高近似质量，可以采用 **[幂迭代](@entry_id:141327)方案 (power scheme)** [@problem_id:3557707]。该方法不直接对 $A$ 进行采样，而是对 $(AA^{\top})^q A$ 进行采样，即构造 $Y = (AA^{\top})^q A \Omega$。$q$ 是一个小的整数（如1或2），称为[幂迭代](@entry_id:141327)次数。通过SVD分析可以发现，对 $(AA^{\top})^q A$ 采样的效果等同于对一个[奇异值](@entry_id:152907)为 $\sigma_i^{2q+1}$ 的矩阵进行采样。这会极大地放大奇异值之间的差距，使得[随机投影](@entry_id:274693)更容易捕捉到顶部[奇异向量](@entry_id:143538)，从而显著提高近似精度。例如，如果 $\sigma_k/\sigma_{k+1}$ 接近1，那么 $(\sigma_k/\sigma_{k+1})^{2q+1}$ 会随着 $q$ 的增加而快速增长。

当然，[幂迭代](@entry_id:141327)增加了对矩阵的访问次数。$q$ 次[幂迭代](@entry_id:141327)需要对 $A$ 和 $A^{\top}$ 进行总共 $2q+1$ 次矩阵-块乘积。同时，为了避免数值不稳定（中间矩阵的列向量会趋向于主[奇异向量](@entry_id:143538)而变得[线性相关](@entry_id:185830)），在实际实现中，每次与 $A$ 或 $A^{\top}$ 相乘后都应进行[再正交化](@entry_id:754248) [@problem_id:3557707]。

随机算法的性能是概率性的。**[过采样](@entry_id:270705) (oversampling)** 参数 $p$ 的作用是提供额外的“自由度”，以确保[随机投影](@entry_id:274693)能够以高概率捕获目标[子空间](@entry_id:150286)。**[测度集中](@entry_id:265372) (concentration of measure)** 现象是其理论基础，它保证了高维空间中的[随机投影](@entry_id:274693)能够很好地保持几何结构。通过一个精心设计的例子 [@problem_id:3557751] 可以看到，如果不[过采样](@entry_id:270705)（$p=0$），当[随机投影](@entry_id:274693)矩阵的某个关键子块恰好接近奇异时，算法可能会失败。[过采样](@entry_id:270705)通过增加随机矩阵的维度，极大地降低了这种情况发生的概率，从而使我们能够控制算法的失败概率。

##### 基于草图的列/行选择 (CUR 分解)

另一类随机方法是直接从原矩阵中抽取一部分列和行来构造近似，这被称为 **CUR 分解**。其形式为 $A \approx C U R$，其中 $C$ 是 $A$ 的一些列，R 是 $A$ 的一些行，而 $U$ 是一个小的连接矩阵。这种近似的优势在于其高度的可解释性，因为它保留了原始数据的部分结构。

关键问题是如何选择“重要”的列和行。均匀随机采样通常效率不高。一个更优的策略是根据 **杠杆分数 (leverage scores)** 进行概率采样 [@problem_id:3557709]。对于列采样，第 $i$ 列的（$k$秩）杠杆分数 $\ell_i$ 定义为其在 $A$ 的顶部 $k$ 个[右奇异向量](@entry_id:754365)张成的[子空间](@entry_id:150286)中的投影长度的平方，即 $\ell_i = \|e_i^{\top} V_k\|_2^2$。它量化了第 $i$ 列对于描述该主导[子空间](@entry_id:150286)的重要性。杠杆分数的一个关键性质是 $\sum_{i=1}^n \ell_i = k$。

通过按正比于杠杆分数的概率 $p_i \propto \ell_i$ 进行采样，我们可以确保高杠杆分数的列（即更“重要”的列）有更高的机会被选中。理论表明，这种[采样策略](@entry_id:188482)在某种意义上是最优的，并且它能以高概率保证近似误差 $\left\| A - CUR \right\|_F$ 接近不可避免的误差 $\left\| A - A_k \right\|_F$ [@problem_id:3557709]。在实践中，由于计算精确杠杆分数需要SVD，人们通常会先用一个快速的随机算法（如随机范围搜索）得到近似的奇异向量，然后计算近似的杠杆分数来进行采样。

#### 计算方法的比较

在[选择算法](@entry_id:637237)时，需要权衡计算成本、[通信开销](@entry_id:636355)和精度。
- **通信成本**：在[分布式计算](@entry_id:264044)或处理无法放入内存的大数据时，对矩阵的访问次数（passes）和节点间的同步次数是主要瓶颈。随机范围搜索（特别是当 $q$ 很小时）通常需要很少的 passes（例如，当 $q=0$ 时只需2次），并且其同步操作是块状的（例如对一个 $m \times \ell$ 的矩阵进行[QR分解](@entry_id:139154)），因此对高延迟网络不敏感。相比之下，Lanczos 方法需要 $2k$ 次 passes，并且每一步都需要同步（向量[内积](@entry_id:158127)），这使其对延迟更敏感 [@problem_id:3557693]。
- **计算成本**：如果矩阵乘法很快（例如，矩阵具有快速变换结构，如FFT），并且目标秩 $k$ 很小，那么 Lanczos 方法的总浮点运算量可能更低，并且能为顶部的几个[奇异值](@entry_id:152907)提供极高的精度。随机方法中的密集矩阵运算（如QR分解和小型SVD）的成本可能在此时占主导地位 [@problem_id:3557693]。
- **数据访问模式**：对于单遍（single-pass）流式数据，Lanczos 等多遍迭代算法无法使用。而随机算法（包括某些特定设计的CUR方法）天然适合这种场景 [@problem_id:3557693]。

### 前沿主题与理论极限

#### 噪声下的基本极限：尖峰[随机矩阵模型](@entry_id:196887)

前面的讨论大多假设我们能够从一个固定的矩阵 $A$ 中恢复其低秩结构。但在许多统计应用中，观测到的矩阵是“真实”低秩信号与随机噪声的混合体，即 $A = L + \sigma Z$，其中 $L$ 是秩为 $k$ 的信号矩阵，$\sigma Z$ 是噪声。一个深刻的问题是：我们能在多大程度上从 $A$ 中恢复 $L$？

**[随机矩阵理论](@entry_id:142253) (Random Matrix Theory, RMT)** 为此提供了惊人的答案。在 $m, n \to \infty$ 且 $m/n \to \gamma$ 的高维渐进极限下，存在一个 **[相变](@entry_id:147324)现象 (phase transition)** [@problem_id:3557739]。对于信号矩阵 $L$ 的一个奇异值 $s$，能否被检测到，取决于其与噪声水平 $\sigma$ 和矩阵的形状 $\gamma$ 的关系。这个临界阈值由著名的 **BBP (Baik-Ben Arous-Péché) [相变](@entry_id:147324)** 描述：
-   如果 $s  \sigma \gamma^{1/4}$（超临界），那么 $A$ 会有一个“弹出”的奇异值，它与 $s$ 相关，并且 $A$ 对应的[奇异向量](@entry_id:143538)与 $L$ 的奇异向量渐进相关（即它们的[内积](@entry_id:158127)不为零）。这意味着信号是可恢复的。
-   如果 $s \le \sigma \gamma^{1/4}$（亚临界），那么 $A$ 的顶部奇异值将“淹没”在噪声奇异值谱的边缘，并且 $A$ 的[奇异向量](@entry_id:143538)与 $L$ 的奇异向量渐进正交（[内积](@entry_id:158127)为零）。这意味着信号完全无法恢复。

这个结果揭示了一个深刻的道理：即使使用理论上最优的 SVD 方法，如果信噪比不够高（即 $s/\sigma$ 不超过临界值），低秩结构也是从根本上无法从噪声中分辨出来的。这也意味着，在亚临界区域，任何基于SVD的低秩近似（如[截断SVD](@entry_id:634824)）去估计 $L$ 都会比简单的零估计（即估计 $L$ 为零矩阵）产生更大的期望误差 [@problem_id:3557739]。

#### 结构化低秩近似

最后，值得注意的是，在许多应用中，我们寻找的低秩矩阵除了低秩之外，还需满足特定的 **结构** 约束，例如对称性、非负性、或者 **Toeplitz/Hankel 结构**。在这些情况下，无约束的最佳近似 $A_k$（来自 SVD）通常不满足这些结构。因此，问题变为在同时满足低秩和结构约束的矩阵集合中寻找最佳近似。

这是一个约束优化问题，其解通常与 $A_k$ 不同。例如，考虑寻找一个秩为1且为 Toeplitz 矩阵的最佳近似 [@problem_id:3557741]。对于矩阵 $A = \begin{pmatrix} 2  0 \\ 0  1 \end{pmatrix}$，其无约束最佳秩1近似是 $X^{\star} = \begin{pmatrix} 2  0 \\ 0  0 \end{pmatrix}$。然而，通过求解一个约束[最小二乘问题](@entry_id:164198)，可以发现最佳的秩1 Toeplitz 近似是 $T^{\star} = \begin{pmatrix} 3/4  3/4 \\ 3/4  3/4 \end{pmatrix}$。这清楚地表明，增加结构约束会改变问题的解。这类问题通常没有封闭解，需要通过交替投影、梯度下降或其他[数值优化](@entry_id:138060)技术来求解。