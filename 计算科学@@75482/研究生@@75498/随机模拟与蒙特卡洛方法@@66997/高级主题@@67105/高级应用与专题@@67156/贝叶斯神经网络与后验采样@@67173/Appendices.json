{"hands_on_practices": [{"introduction": "贝叶斯神经网络的后验分布通常是高维且非凸的，这使得直接采样极具挑战性。随机梯度朗之万动力学（SGLD）提供了一种可扩展的、基于梯度的马尔可夫链蒙特卡洛（MCMC）方法，是现代贝叶斯深度学习中的核心采样技术之一。通过从第一性原理推导SGLD的更新规则 ([@problem_id:3291187])，您将深入理解该算法如何巧妙地结合梯度信息和随机噪声，从而在复杂的权重空间中探索后验分布。", "problem": "考虑一个贝叶斯神经网络，其权重为 $w \\in \\mathbb{R}^{d}$，先验为 $p(w)=\\mathcal{N}(0,\\sigma_{p}^{2} I_{d})$，数据为 $\\mathcal{D}=\\{(x_{i},y_{i})\\}_{i=1}^{N}$。数据由网络输出 $f_{w}(x)$ 周围的加性高斯观测噪声建模，该噪声的已知方差为 $\\sigma_{y}^{2}$，因此对于 $i=1,\\dots,N$，$p(y_{i}\\mid x_{i},w)=\\mathcal{N}(f_{w}(x_{i}),\\sigma_{y}^{2})$ 且相互独立。后验满足 $p(w\\mid \\mathcal{D}) \\propto p(w)\\prod_{i=1}^{N}p(y_{i}\\mid x_{i},w)$。令 $B_{t}\\subset\\{1,\\dots,N\\}$ 是在第 $t$ 次迭代时无放回均匀采样的大小为 $m=|B_{t}|$ 的一个小批量，并令 $\\xi_{t}\\sim\\mathcal{N}(0,I_{d})$ 在不同的 $t$ 之间相互独立。从以该后验为目标的过阻尼朗之万扩散及其 Euler–Maruyama 离散化出发，使用基于 $B_{t}$ 的后验得分的无偏小批量估计量，推导出随机梯度朗之万动力学更新。在第 $t$ 次迭代的最终更新中，明确地用 $w_{t}$、$\\sigma_{p}^{2}$、$\\sigma_{y}^{2}$、$N$、$m$、$f_{w_{t}}(x)$ 和 $\\nabla_{w} f_{w_{t}}(x)$ 表示。\n\n然后，陈述步长序列 $\\{\\eta_{t}\\}_{t\\geq 1}$ 需要满足的必要条件，在这些条件下，当 $t\\to\\infty$ 时，所得到的马尔可夫链能从精确后验分布中进行渐近采样（假设保证稳定性的标准正则性条件成立，包括可测性、得分的利普希茨连续性以及小批量梯度噪声的有界二阶矩）。你的最终答案必须只包含推导出的更新和步长条件。不要包含任何解释性文本。如果你提供多个项目，请将它们表示为单个行向量。答案不需要四舍五入，也没有物理单位。", "solution": "该问题要求推导贝叶斯神经网络的随机梯度朗之万动力学（SGLD）更新规则，并陈述其收敛到后验分布的步长条件。\n\n首先，我们形式化目标分布。给定数据 $\\mathcal{D}=\\{(x_{i},y_{i})\\}_{i=1}^{N}$，权重 $w \\in \\mathbb{R}^{d}$ 上的后验分布由贝叶斯定理给出：\n$$\np(w \\mid \\mathcal{D}) \\propto p(\\mathcal{D} \\mid w) p(w)\n$$\n鉴于数据点的独立性，似然函数为 $p(\\mathcal{D} \\mid w) = \\prod_{i=1}^{N} p(y_{i} \\mid x_{i}, w)$。因此，后验分布为：\n$$\np(w \\mid \\mathcal{D}) \\propto p(w) \\prod_{i=1}^{N} p(y_{i} \\mid x_{i}, w)\n$$\n使用对数后验更为方便，其表达式为：\n$$\n\\ln p(w \\mid \\mathcal{D}) = \\ln p(w) + \\sum_{i=1}^{N} \\ln p(y_{i} \\mid x_{i}, w) + C\n$$\n其中 $C$ 是一个与 $w$ 无关的归一化常数。\n\n问题指定了高斯先验 $p(w) = \\mathcal{N}(0, \\sigma_{p}^{2} I_{d})$ 和高斯似然 $p(y_{i} \\mid x_{i}, w) = \\mathcal{N}(f_{w}(x_{i}), \\sigma_{y}^{2})$。忽略常数项，相应的对数密度为：\n$$\n\\ln p(w) = -\\frac{1}{2\\sigma_{p}^{2}} w^{T}w + C_{p} = -\\frac{1}{2\\sigma_{p}^{2}} \\|w\\|_{2}^{2} + C_{p}\n$$\n$$\n\\ln p(y_{i} \\mid x_{i}, w) = -\\frac{1}{2\\sigma_{y}^{2}} (y_{i} - f_{w}(x_{i}))^{2} + C_{y}\n$$\n将这些代入对数后验表达式，我们得到：\n$$\n\\ln p(w \\mid \\mathcal{D}) = -\\frac{1}{2\\sigma_{p}^{2}} \\|w\\|_{2}^{2} - \\frac{1}{2\\sigma_{y}^{2}} \\sum_{i=1}^{N} (y_{i} - f_{w}(x_{i}))^{2} + C'\n$$\nSGLD 算法是一个过阻尼朗之万扩散过程的离散化。以 $p(w \\mid \\mathcal{D})$ 作为其不变分布的连续时间朗之万扩散由以下随机微分方程（SDE）描述：\n$$\ndw_{t} = \\frac{1}{2} \\nabla_{w} \\ln p(w_{t} \\mid \\mathcal{D}) dt + d\\mathcal{W}_{t}\n$$\n其中 $\\mathcal{W}_{t}$ 是一个标准的 $d$ 维维纳过程。\n使用步长 $\\eta_{t}$ 对此 SDE 进行 Euler-Maruyama 离散化，得到朗之万动力学（LD）更新：\n$$\nw_{t+1} = w_{t} + \\frac{\\eta_{t}}{2} \\nabla_{w} \\ln p(w_{t} \\mid \\mathcal{D}) + \\sqrt{\\eta_{t}} \\xi_{t}\n$$\n其中 $\\xi_{t} \\sim \\mathcal{N}(0, I_{d})$ 是一个独立标准高斯噪声向量。\n\n下一步是计算对数后验的梯度，也称为得分。\n$$\n\\nabla_{w} \\ln p(w \\mid \\mathcal{D}) = \\nabla_{w} \\left( -\\frac{1}{2\\sigma_{p}^{2}} w^{T}w - \\frac{1}{2\\sigma_{y}^{2}} \\sum_{i=1}^{N} (y_{i} - f_{w}(x_{i}))^{2} \\right)\n$$\n使用链式法则，我们得到：\n$$\n\\nabla_{w} \\ln p(w \\mid \\mathcal{D}) = -\\frac{1}{\\sigma_{p}^{2}} w - \\frac{1}{2\\sigma_{y}^{2}} \\sum_{i=1}^{N} 2(y_{i} - f_{w}(x_{i}))(-\\nabla_{w} f_{w}(x_{i}))\n$$\n$$\n\\nabla_{w} \\ln p(w \\mid \\mathcal{D}) = -\\frac{1}{\\sigma_{p}^{2}} w + \\frac{1}{\\sigma_{y}^{2}} \\sum_{i=1}^{N} (y_{i} - f_{w}(x_{i})) \\nabla_{w} f_{w}(x_{i})\n$$\n对于 $N$ 很大的大型数据集，在每次迭代中计算这个完整梯度在计算上是不可行的。SGLD 使用一小批数据来近似这个梯度。令 $B_{t} \\subset \\{1, \\dots, N\\}$ 是一个大小为 $m$ 的随机均匀采样的小批量。通过对小批量上的和进行缩放，可以构造完整梯度的一个无偏估计量：\n$$\n\\widehat{\\nabla_{w} \\ln p(w \\mid \\mathcal{D})} = -\\frac{1}{\\sigma_{p}^{2}} w + \\frac{N}{m} \\sum_{i \\in B_{t}} \\frac{1}{\\sigma_{y}^{2}}(y_{i} - f_{w}(x_{i})) \\nabla_{w} f_{w}(x_{i})\n$$\n将这个随机梯度估计量代入 LD 更新方程，得到 SGLD 更新规则：\n$$\nw_{t+1} = w_{t} + \\frac{\\eta_{t}}{2} \\left[ -\\frac{1}{\\sigma_{p}^{2}} w_{t} + \\frac{N}{m\\sigma_{y}^{2}} \\sum_{i \\in B_{t}} (y_{i} - f_{w_{t}}(x_{i})) \\nabla_{w} f_{w_{t}}(x_{i}) \\right] + \\sqrt{\\eta_{t}}\\xi_{t}\n$$\n这个表达式可以重写以清楚地显示其组成部分：\n$$\nw_{t+1} = w_{t} - \\frac{\\eta_{t}}{2\\sigma_{p}^{2}} w_{t} + \\frac{\\eta_{t}N}{2m\\sigma_{y}^{2}} \\sum_{i \\in B_{t}} (y_{i} - f_{w_{t}}(x_{i})) \\nabla_{w} f_{w_{t}}(x_{i}) + \\sqrt{\\eta_{t}}\\xi_{t}\n$$\n\n为使 SGLD 迭代依分布收敛到真实后验 $p(w \\mid \\mathcal{D})$，步长策略 $\\{\\eta_{t}\\}_{t\\geq 1}$ 必须满足特定条件。这些条件是为了平衡状态空间的探索（由噪声项和梯度驱动）与离散化误差和随机梯度估计方差的减小。标准条件，类似于随机近似的条件（Robbins-Monro 条件），是：\n1. 步长不能衰减得太快，以允许过程探索整个状态空间。这通过步长之和必须发散的条件来保证：\n$$\n\\sum_{t=1}^{\\infty} \\eta_{t} = \\infty\n$$\n2. 步长必须衰减得足够快，以确保注入噪声的方差随时间减小，并且 Euler-Maruyama 方案的离散化误差在极限情况下可以忽略不计。这通过步长的平方和必须收敛的条件来保证：\n$$\n\\sum_{t=1}^{\\infty} \\eta_{t}^{2}  \\infty\n$$\n在这些条件下，并假设目标分布和网络函数具有标准正则性，由 SGLD 算法生成的 $w_{t}$ 的分布在 $t \\to \\infty$ 时弱收敛于后验分布 $p(w \\mid \\mathcal{D})$。", "answer": "$$\n\\boxed{\\begin{pmatrix} w_{t+1} = w_{t} + \\frac{\\eta_{t}}{2} \\left[ -\\frac{1}{\\sigma_{p}^{2}} w_{t} + \\frac{N}{m\\sigma_{y}^{2}} \\sum_{i \\in B_{t}} (y_{i} - f_{w_{t}}(x_{i})) \\nabla_{w} f_{w_{t}}(x_{i}) \\right] + \\sqrt{\\eta_{t}}\\xi_{t}  \\sum_{t=1}^{\\infty} \\eta_{t} = \\infty  \\sum_{t=1}^{\\infty} \\eta_{t}^{2}  \\infty \\end{pmatrix}}\n$$", "id": "3291187"}, {"introduction": "在构建实际模型时，我们常常需要超越标准假设，以捕捉数据的真实复杂性。例如，标准的回归模型通常假定噪声方差恒定（同方差性），但这在许多应用中并不成立。本练习 ([@problem_id:3291222]) 将指导您如何设计一个能够学习依赖于输入的预测不确定性（异方差性）的贝叶斯神经网络，这需要我们推导出模型对数似然函数相对于网络权重的梯度。", "problem": "考虑一个用于一维异方差回归的贝叶斯神经网络 (BNN)。对于每个输入 $x_i$，权重为 $w$ 的网络输出两个标量函数 $(\\mu_w(x_i), s_w(x_i))$，其中 $s_w(x_i) = \\log \\sigma_w^2(x_i)$ 通过 $\\sigma_w^2(x_i) = \\exp(s_w(x_i))$ 编码了预测方差。假设单个观测值 $(x_i, y_i)$ 的数据似然是高斯密度 $p(y_i \\mid x_i, w) = \\mathcal{N}(y_i; \\mu_w(x_i), \\sigma_w^2(x_i))$。从高斯概率密度函数的定义出发，使用标准的微分法则（包括链式法则），推导出单样本对数似然 $\\ell_i(w) = \\ln p(y_i \\mid x_i, w)$ 及其关于 $w$ 的梯度，并用 $y_i$、$\\mu_w(x_i)$、$s_w(x_i)$ 以及权重空间雅可比矩阵 $\\nabla_w \\mu_w(x_i)$ 和 $\\nabla_w s_w(x_i)$ 显式地表示。您的推导应仅假设高斯密度的性质以及网络输出关于 $w$ 的可微性。将您的最终答案表示为一个包含标量 $\\ell_i(w)$ 和向量 $\\nabla_w \\ell_i(w)$ 的数对。最终答案必须是单一的封闭形式解析表达式。不要包含任何单位。无需四舍五入。", "solution": "问题陈述已经过验证，被认为是合理的。这是一个在随机模拟和机器学习领域中定义明确的问题，具体涉及贝叶斯神经网络对数似然梯度的推导，这是通过变分推斷或基于梯度的 MCMC 方法训练此类模型的基本计算。所有必要的组成部分都已提供。\n\n我们首先陈述问题陈述中指定的高斯似然的概率密度函数 (PDF)。对于单个数据点 $(x_i, y_i)$，似然由下式给出：\n$$\np(y_i \\mid x_i, w) = \\mathcal{N}(y_i; \\mu_w(x_i), \\sigma_w^2(x_i))\n$$\n对于均值为 $\\mu$、方差为 $\\sigma^2$ 的变量 $y$，高斯 PDF 的显式形式为：\n$$\n\\mathcal{N}(y; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - \\mu)^2}{2\\sigma^2}\\right)\n$$\n在我们的情况下，均值为 $\\mu_w(x_i)$，方差为 $\\sigma_w^2(x_i)$。问题定义了方差通过网络输出 $s_w(x_i)$ 的特定参数化：\n$$\n\\sigma_w^2(x_i) = \\exp(s_w(x_i))\n$$\n将此代入 PDF，我们得到特定模型的似然：\n$$\np(y_i \\mid x_i, w) = \\frac{1}{\\sqrt{2\\pi\\exp(s_w(x_i))}} \\exp\\left(-\\frac{(y_i - \\mu_w(x_i))^2}{2\\exp(s_w(x_i))}\\right)\n$$\n任务的第一部分是推导单样本对数似然 $\\ell_i(w) = \\ln p(y_i \\mid x_i, w)$。我们对上述表达式取自然对数，使用性质 $\\ln(ab) = \\ln(a) + \\ln(b)$、$\\ln(a^c) = c \\ln(a)$ 和 $\\ln(\\exp(c)) = c$。\n$$\n\\ell_i(w) = \\ln\\left( \\left(2\\pi\\exp(s_w(x_i))\\right)^{-1/2} \\right) + \\ln\\left( \\exp\\left(-\\frac{(y_i - \\mu_w(x_i))^2}{2\\exp(s_w(x_i))}\\right) \\right)\n$$\n$$\n\\ell_i(w) = -\\frac{1}{2} \\ln(2\\pi\\exp(s_w(x_i))) - \\frac{(y_i - \\mu_w(x_i))^2}{2\\exp(s_w(x_i))}\n$$\n$$\n\\ell_i(w) = -\\frac{1}{2} \\left( \\ln(2\\pi) + \\ln(\\exp(s_w(x_i))) \\right) - \\frac{1}{2}(y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i))\n$$\n$$\n\\ell_i(w) = -\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}s_w(x_i) - \\frac{1}{2}(y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i))\n$$\n这就是单样本对数似然的表达式，是我们答案的第一部分。\n\n任务的第二部分是推导对数似然关于网络权重 $\\nabla_w \\ell_i(w)$ 的梯度。我们必须对 $\\ell_i(w)$ 的表达式关于向量 $w$ 进行微分。量 $\\mu_w(x_i)$ 和 $s_w(x_i)$ 是 $w$ 的函数。我们将应用多元链式法则。\n$$\n\\nabla_w \\ell_i(w) = \\nabla_w \\left( -\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}s_w(x_i) - \\frac{1}{2}(y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i)) \\right)\n$$\n第一项 $-\\frac{1}{2}\\ln(2\\pi)$ 是一个关于 $w$ 的常数，因此其梯度为零。\n$$\n\\nabla_w \\ell_i(w) = -\\frac{1}{2}\\nabla_w(s_w(x_i)) - \\frac{1}{2}\\nabla_w \\left( (y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i)) \\right)\n$$\n对于第二项，我们使用梯度的乘法法则 $\\nabla(uv) = (\\nabla u)v + u(\\nabla v)$。设 $u = (y_i - \\mu_w(x_i))^2$ 和 $v = \\exp(-s_w(x_i))$。我们使用链式法则求它们关于 $w$ 的梯度。\n\n$u$ 关于 $w$ 的梯度是：\n$$\n\\nabla_w u = \\frac{\\partial u}{\\partial \\mu_w(x_i)} \\nabla_w \\mu_w(x_i) = 2(y_i - \\mu_w(x_i)) \\cdot (-1) \\cdot \\nabla_w \\mu_w(x_i) = -2(y_i - \\mu_w(x_i)) \\nabla_w \\mu_w(x_i)\n$$\n$v$ 关于 $w$ 的梯度是：\n$$\n\\nabla_w v = \\frac{\\partial v}{\\partial s_w(x_i)} \\nabla_w s_w(x_i) = \\exp(-s_w(x_i)) \\cdot (-1) \\cdot \\nabla_w s_w(x_i) = -\\exp(-s_w(x_i)) \\nabla_w s_w(x_i)\n$$\n现在，应用乘法法则：\n$$\n\\nabla_w (uv) = \\left( -2(y_i - \\mu_w(x_i)) \\nabla_w \\mu_w(x_i) \\right) \\exp(-s_w(x_i)) + (y_i - \\mu_w(x_i))^2 \\left( -\\exp(-s_w(x_i)) \\nabla_w s_w(x_i) \\right)\n$$\n$$\n\\nabla_w (uv) = -2(y_i - \\mu_w(x_i)) \\exp(-s_w(x_i)) \\nabla_w \\mu_w(x_i) - (y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i)) \\nabla_w s_w(x_i)\n$$\n将此代回 $\\nabla_w \\ell_i(w)$ 的表达式中：\n$$\n\\nabla_w \\ell_i(w) = -\\frac{1}{2}\\nabla_w s_w(x_i) - \\frac{1}{2} \\left[ -2(y_i - \\mu_w(x_i)) \\exp(-s_w(x_i)) \\nabla_w \\mu_w(x_i) - (y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i)) \\nabla_w s_w(x_i) \\right]\n$$\n分配 $-\\frac{1}{2}$ 项：\n$$\n\\nabla_w \\ell_i(w) = -\\frac{1}{2}\\nabla_w s_w(x_i) + (y_i - \\mu_w(x_i)) \\exp(-s_w(x_i)) \\nabla_w \\mu_w(x_i) + \\frac{1}{2}(y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i)) \\nabla_w s_w(x_i)\n$$\n最后，我们按雅可比矩阵 $\\nabla_w \\mu_w(x_i)$ 和 $\\nabla_w s_w(x_i)$ 对各项进行分组：\n$$\n\\nabla_w \\ell_i(w) = \\left( (y_i - \\mu_w(x_i)) \\exp(-s_w(x_i)) \\right) \\nabla_w \\mu_w(x_i) + \\left( \\frac{1}{2}(y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i)) - \\frac{1}{2} \\right) \\nabla_w s_w(x_i)\n$$\n为了清晰起见，可以稍微分解一下：\n$$\n\\nabla_w \\ell_i(w) = (y_i - \\mu_w(x_i))\\exp(-s_w(x_i)) \\nabla_w \\mu_w(x_i) + \\frac{1}{2} \\left[ (y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i)) - 1 \\right] \\nabla_w s_w(x_i)\n$$\n这就是单样本对数似然梯度的最终表达式。要求的答案是由 $\\ell_i(w)$ 和 $\\nabla_w \\ell_i(w)$ 组成的数对。", "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{1}{2}s_w(x_i) - \\frac{1}{2}(y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i)) - \\frac{1}{2}\\ln(2\\pi)  (y_i - \\mu_w(x_i))\\exp(-s_w(x_i)) \\nabla_w \\mu_w(x_i) + \\frac{1}{2} \\left[ (y_i - \\mu_w(x_i))^2 \\exp(-s_w(x_i)) - 1 \\right] \\nabla_w s_w(x_i) \\end{pmatrix}}\n$$", "id": "3291222"}, {"introduction": "获得了后验样本之后，我们能用它们做什么？这是贝叶斯建模中一个至关重要的问题，其答案在于量化不确定性。本练习 ([@problem_id:3291210]) 聚焦于如何利用后验样本来计算一个关键指标——认知不确定性，它代表了模型自身的“知识局限”。通过推导并实现互信息的蒙特卡洛估计器，您将学会如何将抽象的不确定性概念转化为一个可计算、可应用的实用工具，例如在主动学习（Active Learning）中指导数据采集。", "problem": "考虑一个分类贝叶斯神经网络 (BNN)，它被定义为权重 $w$ 上的一个概率模型，在观测到数据集 $D$ 后具有后验分布 $p(w \\mid D)$。对于一个输入 $x^\\star$，该 BNN 通过 softmax 输出 $q_k(w, x^\\star)$ 为类别 $k \\in \\{1, \\dots, K\\}$ 生成类别概率，这些概率对于每个 $w$ 都构成一个有效的离散分布。在给定 $x^\\star$ 和 $D$ 的条件下，标签 $y^\\star$ 的预测分布是通过对权重 $w$ 的后验分布求这些类别概率的平均值来确定的。在给定 $x^\\star$ 和 $D$ 的条件下，$y^\\star$ 和 $w$ 之间的互信息量化了认知不确定性，并且是基于分歧的贝叶斯主动学习 (Bayesian Active Learning by Disagreement, BALD) 中的一个核心量。你的任务是从概率论和信息论的第一性原理出发，推导出一个可实现的蒙特卡洛 (MC) 估计器，该估计器使用独立样本 $w^{(s)} \\sim p(w \\mid D)$ 来计算此互信息。\n\n使用以下基础知识：\n- 离散标签的后验预测分布的定义：给定 $x^\\star$ 和 $D$ 时，$y^\\star = k$ 的概率是在 $p(w \\mid D)$ 分布下 $q_k(w, x^\\star)$ 的期望值。\n- 以自然单位 (nats) 计的离散分布的香农熵：对于概率 $\\{p_k\\}_{k=1}^K$，有 $H = -\\sum_{k=1}^K p_k \\log p_k$，其中 $\\log$ 表示自然对数。\n- 条件互信息的链式法则定义：对于随机变量 $A$、$B$ 和 $C$，有 $I(A; B \\mid C) = H(A \\mid C) - H(A \\mid B, C)$。\n\n从这些定义出发，推导一个估计器，其中关于 $p(w \\mid D)$ 的期望被替换为对 $S$ 个独立后验样本 $w^{(s)}$ 的经验平均，每个样本都产生一个类别概率向量 $\\big(q_1(w^{(s)}, x^\\star), \\dots, q_K(w^{(s)}, x^\\star)\\big)$。然后，实现一个程序，该程序针对一组给定的后验类别概率样本，以 nats 为单位计算此互信息的 MC 估计值。\n\n数值和实现要求：\n- 所有对数必须是自然对数，熵和互信息必须以 nats 为单位报告。\n- 为保证数值稳定性，在计算任何概率 $p$ 的 $\\log p$ 时，应将 $p$ 裁剪到闭区间 $[\\varepsilon, 1]$ 内，其中 $\\varepsilon$ 是一个小的正数（选择一个适合双精度算术的固定 $\\varepsilon$）。\n- 最终输出必须是单行，包含一个用方括号括起来的逗号分隔列表，其中每个互信息估计值四舍五入到 $6$ 位小数。\n\n测试套件：\n对于下面的每个测试用例，将每一行视为由单个独立抽取的后验样本 $w^{(s)}$ 生成的类别概率向量。在所有情况下，$S$ 表示后验样本的数量，$K$ 表示类别的数量。为每个用例计算以 nats 为单位的 MC 互信息估计值。\n\n- 用例 1（均匀、相同的预测；预期互信息接近 $0$）：$S = 5$，$K = 3$，行\n  - $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$\n  - $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$\n  - $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$\n  - $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$\n  - $(\\frac{1}{3}, \\frac{1}{3}, \\frac{1}{3})$。\n- 用例 2（样本间确定性分歧）：$S = 2$，$K = 3$，行\n  - $(1, 0, 0)$\n  - $(0, 1, 0)$。\n- 用例 3（相同的确定性预测；预期互信息为 $0$）：$S = 3$，$K = 4$，行\n  - $(0, 0, 1, 0)$\n  - $(0, 0, 1, 0)$\n  - $(0, 0, 1, 0)$。\n- 用例 4（样本间中度变异）：$S = 4$，$K = 3$，行\n  - $(0.7, 0.2, 0.1)$\n  - $(0.6, 0.3, 0.1)$\n  - $(0.3, 0.4, 0.3)$\n  - $(0.2, 0.7, 0.1)$。\n- 用例 5（极端概率和一个平衡样本；测试数值稳定性）：$S = 3$，$K = 2$，行\n  - $(10^{-12}, 1 - 10^{-12})$\n  - $(1 - 10^{-12}, 10^{-12})$\n  - $(0.5, 0.5)$。\n- 用例 6（退化的单类场景；预期互信息为 $0$）：$S = 3$，$K = 1$，行\n  - $(1)$\n  - $(1)$\n  - $(1)$。\n\n程序输入和输出规范：\n- 你的程序必须是自包含的，并且不能读取任何输入；它应在内部构建上述测试用例。\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个互信息估计值四舍五入到 $6$ 位小数（例如 $[r_1, r_2, r_3]$）。", "solution": "我们从一个分类贝叶斯神经网络 (BNN) 开始，对于任意固定的权重向量 $w$ 和输入 $x^\\star$，该网络输出一个在 $K$ 个类别上的有效离散分布，记为 $\\{q_k(w, x^\\star)\\}_{k=1}^K$，其中 $q_k(w, x^\\star) \\ge 0$ 且 $\\sum_{k=1}^K q_k(w, x^\\star) = 1$。在观测到数据集 $D$ 后，权重的后验分布为 $p(w \\mid D)$。\n\n离散标签 $y^\\star$ 的后验预测分布由 softmax 输出在后验分布下的期望给出：\n$$\np(y^\\star = k \\mid x^\\star, D) = \\mathbb{E}_{w \\sim p(w \\mid D)} \\left[ q_k(w, x^\\star) \\right], \\quad k \\in \\{1, \\dots, K\\}.\n$$\n这是 BNN 中贝叶斯模型平均的基础法则。\n\n对于任意离散分布 $\\{p_k\\}_{k=1}^K$，以自然单位 (nats) 度量的香农熵定义为\n$$\nH(\\{p_k\\}) = -\\sum_{k=1}^K p_k \\log p_k,\n$$\n其中 $\\log$ 表示自然对数。给定 $C$ 时，随机变量 $A$ 和 $B$ 之间的条件互信息由链式法则恒等式定义\n$$\nI(A; B \\mid C) = H(A \\mid C) - H(A \\mid B, C).\n$$\n\n我们将这些定义应用于 $A = y^\\star$、$B = w$ 和 $C = (x^\\star, D)$。第一项 $H(y^\\star \\mid x^\\star, D)$ 是 $y^\\star$ 的后验预测分布的香农熵。第二项 $H(y^\\star \\mid w, x^\\star, D)$ 是在给定固定权重 $w$ 和 $x^\\star$ 的条件下 $y^\\star$ 的条件熵，它仅依赖于 softmax 输出 $\\{q_k(w, x^\\star)\\}_{k=1}^K$，因为 $D$ 的信息通过 $p(w \\mid D)$ 融入。因此，将 $w$ 作为根据 $p(w \\mid D)$ 分布的随机变量进行条件化，我们得到\n$$\nI(y^\\star; w \\mid x^\\star, D) = H(y^\\star \\mid x^\\star, D) - \\mathbb{E}_{w \\sim p(w \\mid D)} \\big[ H(y^\\star \\mid w, x^\\star) \\big].\n$$\n\n为了构建蒙特卡洛 (MC) 估计器，我们从 $p(w \\mid D)$ 中抽取 $S$ 个独立样本 $w^{(s)}$，其中 $s \\in \\{1, \\dots, S\\}$。每个样本产生一个类别概率向量\n$$\n\\mathbf{q}^{(s)}(x^\\star) = \\big( q_1(w^{(s)}, x^\\star), \\dots, q_K(w^{(s)}, x^\\star) \\big),\n$$\n且满足 $\\sum_{k=1}^K q_k(w^{(s)}, x^\\star) = 1$。后验预测概率通过经验平均来近似\n$$\n\\bar{p}_k(x^\\star, D) \\approx \\frac{1}{S} \\sum_{s=1}^S q_k(w^{(s)}, x^\\star).\n$$\n预测熵则通过插件估计器来近似\n$$\n\\widehat{H}_{\\text{pred}} = -\\sum_{k=1}^K \\bar{p}_k(x^\\star, D) \\log \\bar{p}_k(x^\\star, D).\n$$\n给定单个采样权重 $w^{(s)}$ 的条件熵为\n$$\nH^{(s)} = -\\sum_{k=1}^K q_k(w^{(s)}, x^\\star) \\log q_k(w^{(s)}, x^\\star),\n$$\n其后验期望通过经验均值来近似\n$$\n\\widehat{\\mathbb{E}}[H] = \\frac{1}{S} \\sum_{s=1}^S H^{(s)}.\n$$\n因此，以 nats 为单位的互信息的 MC 估计器为\n$$\n\\widehat{I}(y^\\star; w \\mid x^\\star, D) = \\widehat{H}_{\\text{pred}} - \\widehat{\\mathbb{E}}[H].\n$$\n\n数值稳定性考虑：在计算可能极接近 $0$ 的概率 $p$ 的 $\\log p$ 时，我们将 $p$ 裁剪到 $[\\varepsilon, 1]$ 区间内，其中 $\\varepsilon$ 是一个小的正数，例如 $\\varepsilon = 10^{-12}$。这可以防止在 $p = 0$ 时出现未定义的对数，并抑制 $-\\log p$ 中的数值溢出。\n\n估计器的性质：\n- 估计器 $\\widehat{\\mathbb{E}}[H]$ 是 $\\mathbb{E}_{w \\sim p(w \\mid D)}[H(y^\\star \\mid w, x^\\star)]$ 的无偏估计器，因为它是独立同分布 (i.i.d.) 项的样本均值。\n- 由于熵的非线性，插件估计器 $\\widehat{H}_{\\text{pred}}$ 对于有限的 $S$ 通常是有偏的，但当 $S \\to \\infty$ 时它是一致的，因为 $\\bar{p}_k(x^\\star, D)$ 几乎必然收敛到 $\\mathbb{E}[q_k(w, x^\\star)]$。\n- 互信息估计器 $\\widehat{I}$ 在 $S \\to \\infty$ 时是一致的。\n\n每个测试用例的算法步骤：\n1. 令 $P$ 为一个 $S \\times K$ 矩阵，其第 $s$ 行为 $\\mathbf{q}^{(s)}(x^\\star)$。\n2. 将 $P$ 的元素裁剪到 $[\\varepsilon, 1]$ 内，并在必要时重新归一化各行以保持行和为 $1$（对于提供的测试用例，各行和已为 $1$）。\n3. 计算 $\\bar{\\mathbf{p}} = \\frac{1}{S} \\sum_{s=1}^S \\mathbf{q}^{(s)}(x^\\star)$。\n4. 计算 $\\widehat{H}_{\\text{pred}} = -\\sum_{k=1}^K \\bar{p}_k \\log \\bar{p}_k$。\n5. 对于每个 $s$，计算 $H^{(s)} = -\\sum_{k=1}^K q^{(s)}_k \\log q^{(s)}_k$，然后计算 $\\widehat{\\mathbb{E}}[H] = \\frac{1}{S} \\sum_{s=1}^S H^{(s)}$。\n6. 以 nats 为单位输出 $\\widehat{I} = \\widehat{H}_{\\text{pred}} - \\widehat{\\mathbb{E}}[H]$。\n\n该程序为指定的测试套件实现了此估计器，并按要求打印单行结果，该结果是一个用方括号括起来的逗号分隔列表，其中每个值都四舍五入到 $6$ 位小数。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef mutual_information_mc(prob_matrix: np.ndarray, eps: float = 1e-12) - float:\n    \"\"\"\n    Estimate I(y*, w | x*, D) for a classification BNN using Monte Carlo samples.\n    prob_matrix: shape (S, K), rows are class-probability vectors from posterior samples.\n    eps: small positive constant for numerical stability when taking logs.\n    Returns MI in nats as a float.\n    \"\"\"\n    # Ensure input is float64 for numerical stability\n    P = np.array(prob_matrix, dtype=np.float64)\n\n    # Clip probabilities to avoid log(0); rows should already sum to 1 in test cases.\n    P = np.clip(P, eps, 1.0)\n    # Renormalize rows in case clipping perturbed sums slightly\n    row_sums = P.sum(axis=1, keepdims=True)\n    # Avoid division by zero, though with eps clipping this shouldn't happen\n    row_sums = np.maximum(row_sums, eps)\n    P = P / row_sums\n\n    # Number of samples S and classes K\n    S, K = P.shape\n\n    # Predictive probabilities: average over samples\n    p_bar = P.mean(axis=0)\n\n    # Entropy of predictive distribution (in nats)\n    H_pred = -np.sum(p_bar * np.log(p_bar))\n\n    # Conditional entropy for each sample\n    H_cond_samples = -np.sum(P * np.log(P), axis=1)\n    H_cond_avg = H_cond_samples.mean()\n\n    # Mutual information estimate\n    MI = H_pred - H_cond_avg\n    return float(MI)\n\n\ndef solve():\n    # Define epsilon for numerical stability\n    eps = 1e-12\n\n    # Construct test cases as per the problem statement.\n    # Case 1: Uniform, identical predictions (S=5, K=3)\n    case1 = np.array([[1/3, 1/3, 1/3]] * 5, dtype=np.float64)\n\n    # Case 2: Deterministic disagreement across samples (S=2, K=3)\n    case2 = np.array([[1.0, 0.0, 0.0],\n                      [0.0, 1.0, 0.0]], dtype=np.float64)\n\n    # Case 3: Identical deterministic predictions (S=3, K=4)\n    case3 = np.array([[0.0, 0.0, 1.0, 0.0]] * 3, dtype=np.float64)\n\n    # Case 4: Moderate variation across samples (S=4, K=3)\n    case4 = np.array([[0.7, 0.2, 0.1],\n                      [0.6, 0.3, 0.1],\n                      [0.3, 0.4, 0.3],\n                      [0.2, 0.7, 0.1]], dtype=np.float64)\n\n    # Case 5: Extreme probabilities and one balanced sample (S=3, K=2)\n    tiny = 1e-12\n    case5 = np.array([[tiny, 1.0 - tiny],\n                      [1.0 - tiny, tiny],\n                      [0.5, 0.5]], dtype=np.float64)\n\n    # Case 6: Degenerate single-class scenario (S=3, K=1)\n    case6 = np.array([[1.0], [1.0], [1.0]], dtype=np.float64)\n\n    test_cases = [case1, case2, case3, case4, case5, case6]\n\n    results = []\n    for P in test_cases:\n        result = mutual_information_mc(P, eps=eps)\n        results.append(result)\n\n    # Final print statement in the exact required format with 6 decimal digits.\n    print(\"[\" + \",\".join(f\"{r:.6f}\" for r in results) + \"]\")\n\nsolve()\n```", "id": "3291210"}]}