{"hands_on_practices": [{"introduction": "对任何统计工具的深入理解都始于了解其计算方法。本练习旨在弥合柯尔莫哥洛夫-斯米尔诺夫（KS）统计量理论定义 $D_n = \\sup_{x \\in \\mathbb{R}} |F_n(x) - F_0(x)|$ 与其实际计算之间的鸿沟。我们将设计一个高效的 $O(n \\log n)$ 算法[@problem_id:3315959]，通过该练习，您将加深对经验累积分布函数（ECDF）的阶梯函数性质以及最大差异可能出现位置的理解。", "problem": "给定一个来自未知分布的独立同分布样本 $x_1,\\dots,x_n$，其目标累积分布函数 $F_0$ 已知。经验累积分布函数 $F_n$ 定义为 $F_n(x) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{x_i \\le x\\}$。单样本 Kolmogorov-Smirnov 统计量是以下量：\n$$\nD_n = \\sup_{x \\in \\mathbb{R}} \\left| F_n(x) - F_0(x) \\right|.\n$$\n您的任务是设计并实现一个 $O(n \\log n)$ 算法，在给定 $x_1,\\dots,x_n$ 和 $F_0$ 的情况下计算 $D_n$，该算法仅依赖于以下基本原理：\n- $F_n$ 和 $F_0$ 是非递减的右连续函数，有左极限，且取值范围在 $[0,1]$ 内。\n- 函数 $F_n$ 是一个阶跃函数，在观测到的样本点处有大小为 $1/n$ 的跳跃。\n\n您的算法必须遵循以下结构原则：\n- 使用排序对样本进行排列，这一步必须主导算法的渐近复杂度。\n- 对排序后的样本进行一次从左到右的扫描，以确定绝对差的上确界在何处取得。\n- 在扫描过程中遇到的相关点上计算 $F_0$ 的值。\n\n您必须将算法实现在一个单一程序中，将其应用于一个预定义的测试套件，并按下文描述的精确格式生成输出。\n\n测试套件：\n- 情况 A (一般情况): $n = 10$, $x = [0.14, 0.91, 0.22, 0.33, 0.73, 0.59, 0.01, 0.48, 0.64, 0.27]$，$F_0$ 是 $[0,1]$ 上的均匀分布，即当 $x  0$ 时 $F_0(x) = 0$，当 $0 \\le x \\le 1$ 时 $F_0(x) = x$，当 $x \\ge 1$ 时 $F_0(x) = 1$。\n- 情况 B (边界大小): $n = 1$, $x = [0.3]$，$F_0$ 为 $[0,1]$ 上的均匀分布，同情况 A。\n- 情况 C (存在相同值): $n = 6$, $x = [0.2, 0.2, 0.5, 0.5, 0.5, 0.9]$，$F_0$ 为 $[0,1]$ 上的均匀分布，同情况 A。\n- 情况 D (值在支撑集之外): $n = 6$, $x = [-1.0, -0.2, 0.0, 0.1, 1.1, 1.3]$，$F_0$ 为 $[0,1]$ 上的均匀分布，同情况 A。\n- 情况 E (非均匀目标分布): $n = 6$, $x = [0.05, 0.15, 0.7, 0.3, 0.9, 1.2]$，$F_0$ 是率 $\\lambda = 2$ 的指数分布，即当 $x  0$ 时 $F_0(x) = 0$，当 $x \\ge 0$ 时 $F_0(x) = 1 - e^{-\\lambda x}$。\n- 情况 F (无界支撑集目标分布): $n = 7$, $x = [-1.2, -0.5, 0.0, 0.2, 0.8, 1.5, 2.0]$，$F_0$ 是均值为 $\\mu = 0$、标准差为 $\\sigma = 1$ 的正态分布，即 $F_0(x) = \\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right)$，其中 $\\Phi$ 是标准正态累积分布函数。\n\n实现要求：\n- 算法必须先使用排序，然后进行单次扫描，并且必须仅在扫描逻辑所需的点上计算目标 $F_0$ 的值。\n- 程序必须能正确处理样本中的相同值。\n- 程序必须为每种情况计算 $D_n$，并按下文描述的精确格式生成最终结果。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含结果。结果是一个用方括号括起来的、以逗号分隔的小数列表，每个小数都四舍五入到小数点后六位，例如 $[0.123456,0.000000,0.500000]$。\n- 本问题不涉及物理单位。所有输出都是无量纲实数。", "solution": "用户希望我扮演一位严谨的STEM教授，解决一个与Kolmogorov-Smirnov检验相关的问题。\n\n这是我的计划：\n1.  **角色扮演**：采纳一位严谨、苛刻的STEM教授的形象。这意味着我需要做到精确、正确，并优先考虑逻辑一致性。\n2.  **问题验证**：在尝试解决问题之前，批判性地审查问题陈述。\n    *   **步骤 1：提取已知条件**：逐字列出所有提供的数据和定义。这包括经验CDF $F_n(x)$ 的定义、Kolmogorov-Smirnov统计量 $D_n$、设计 $O(n \\log n)$ 算法的任务、算法的结构原则以及具体的测试用例。\n    *   **步骤 2：验证**：根据以下标准检查问题：\n        *   **科学上可靠**：该问题是关于计算Kolmogorov-Smirnov (KS) 统计量。这是非参数统计学中的一个标准概念。$F_n(x)$ 和 $D_n$ 的定义是正确的。提到的CDF的性质（非递减、右连续、取值在[0,1]内）是标准的。关于 $F_n$ 是一个阶跃函数的陈述也是正确的。该问题牢固地植根于数理统计学。\n        *   **适定性**：目标是计算一个特定的、定义明确的数学量 $D_n$。对算法的约束（$O(n \\log n)$，排序后扫描）指导了实现，但并未使问题变得不适定。对于给定的样本和 $F_0$，$D_n$ 的存在性和唯一性由 $\\mathbb{R}$ 上有界函数的上确界的性质保证。\n        *   **客观性**：该问题使用精确的数学语言陈述。没有主观或基于观点的陈述。\n        *   **缺陷清单**：\n            1.  *科学/事实不健全*：否。定义和概念在统计学中是标准的。\n            2.  *不可形式化/不相关*：否。这是一个与指定主题直接相关的形式化数学问题。\n            3.  *不完整/矛盾*：否。为测试用例提供了所有必要信息（样本数据，目标CDF）。对算法的约束是一致的。\n            4.  *不现实/不可行*：否。测试用例涉及标准分布和小样本量。计算是完全可行的。\n            5.  *不适定/结构不良*：否。量 $D_n$ 是定义明确的。\n            6.  *伪深刻/琐碎*：否。虽然这个概念对于统计学家来说可能很熟悉，但推导高效算法需要仔细思考上确界可以在何处达到。这是一个不平凡的算法任务。\n            7.  *超出科学可验证性*：否。结果可以通过直接计算或使用标准库函数来验证（尽管提示要求我从头开始实现）。\n    *   **步骤 3：结论**：该问题是有效的。我将继续进行解答。\n\n3.  **解决阶段**：现在我将设计解决方案。\n    *   **基于原则的设计**：在展示代码之前，我需要解释算法的理论基础。\n    *   **核心思想**：问题要求计算 $D_n = \\sup_{x \\in \\mathbb{R}} |F_n(x) - F_0(x)|$。\n        *   $F_0(x)$ 是一个连续（或至少是非递减和右连续）的函数。\n        *   $F_n(x)$ 是一个阶跃函数。它在排序后的样本点之间是常数。设排序后的样本为 $x_{(1)} \\le x_{(2)} \\le \\dots \\le x_{(n)}$。\n        *   函数 $F_n(x)$ 仅在样本点 $x_i$ 处改变其值。\n        *   对于 $x  x_{(1)}$，$F_n(x) = 0$。\n        *   对于 $x_{(i)} \\le x  x_{(i+1)}$，$F_n(x) = i/n$。（暂时假设没有相同值）。\n        *   对于 $x \\ge x_{(n)}$，$F_n(x) = 1$。\n        *   差函数 $g(x) = F_n(x) - F_0(x)$ 是分段连续的。$|g(x)|$ 的上确界必须出现在 $g(x)$ 有局部极值或不连续的点上。\n        *   $g(x)$ 的不连续点在样本点 $x_i$ 处。$g(x)$ 的导数在样本点之间是 $g'(x) = -F_0'(x) = -f_0(x)$（如果 $F_0$ 可微）。由于 $f_0(x) \\ge 0$，$g(x)$ 在样本点之间是非递增的。\n        *   这意味着区间 $[x_{(i)}, x_{(i+1)})$ 中 $g(x)$ 的上确界必须在左端点 $x_{(i)}$ 处。下确界必须在右端点，即跳跃之前，即在 $\\lim_{x \\to x_{(i+1)}^-} x$ 处。\n        *   因此，我们只需要在点 $x_i$ 处检查 $|F_n(x) - F_0(x)|$ 的值。\n        *   设排序后的样本为 $x_{(1)}, \\dots, x_{(n)}$。\n        *   函数 $F_n(x)$ 在每个 $x_{(i)}$ 处跳跃。所以我们需要在跳跃前后评估差值 $|F_n(x) - F_0(x)|$。\n        *   让我们考虑一个点 $x_{(i)}$。\n            *   在跳跃之前（在 $x_{(i)} - \\epsilon$ 处，对于小的 $\\epsilon > 0$），经验CDF的值是 $F_n(x_{(i)}^-)$。如果没有相同值且所有 $x_j$ 都不同，则 $F_n(x_{(i)}^-) = (i-1)/n$。\n            *   在跳跃点及之后（在 $x_{(i)}$ 处），值是 $F_n(x_{(i)}) = i/n$。\n        *   所以，对于每个排序后的样本点 $x_{(i)}$，我们需要检查两个值：\n            1.  $|F_n(x_{(i)}^-) - F_0(x_{(i)})| = |\\frac{i-1}{n} - F_0(x_{(i)})|$\n            2.  $|F_n(x_{(i)}) - F_0(x_{(i)})| = |\\frac{i}{n} - F_0(x_{(i)})|$\n        *   上确界 $D_n$ 将是所有 $i=1, \\dots, n$ 的这些值中的最大值。\n        *   总体算法是：\n            1.  对样本 $x$ 进行排序以得到 $x_{(1)}, \\dots, x_{(n)}$。这需要 $O(n \\log n)$。\n            2.  初始化变量 `max_d` 为 $0$。\n            3.  从 $i=1$ 到 $n$ 迭代：\n                a.  获取排序后的样本点 $x_{(i)}$。\n                b.  计算 $F_0(x_{(i)})$。\n                c.  计算两个差值：\n                    *   $d_1 = |\\frac{i-1}{n} - F_0(x_{(i)})|$\n                    *   $d_2 = |\\frac{i}{n} - F_0(x_{(i)})|$\n                d.  更新 `max_d = max(max_d, d_1, d_2)`。\n            4.  返回 `max_d`。此扫描需要 $O(n)$。\n        *   总复杂度由排序主导，因此是 $O(n \\log n)$。这符合要求。\n\n    *   **处理相同值**：问题陈述明确提到了相同值（ties）。\n        *   如果存在相同值，例如 $x_{(i-1)}  x_{(i)} = x_{(i+1)} = \\dots = x_{(j)}  x_{(j+1)}$，则在该值处的跳跃大小为 $(j - (i-1))/n$。\n        *   $F_n(x)$ 的定义是 $F_n(x) = \\frac{1}{n} \\sum_{k=1}^n \\mathbf{1}\\{x_k \\le x\\}$。\n        *   如果排序后的样本是 $x_{(1)}, \\dots, x_{(n)}$，那么：\n            *   $F_n(x_{(i)}) = \\frac{1}{n} \\times (\\text{小于或等于 } x_{(i)} \\text{ 的样本数})$。\n            *   如果所有值都不同，则在 $x_{(i)}$ 处，有 $i$ 个样本小于或等于它，所以 $F_n(x_{(i)}) = i/n$。\n            *   $F_n(x_{(i)}^-) = \\lim_{\\epsilon \\to 0^+} F_n(x_{(i)}-\\epsilon) = \\frac{1}{n} \\times (\\text{严格小于 } x_{(i)} \\text{ 的样本数})$。\n            *   如果所有值都不同，这便是 $(i-1)/n$。\n        *   因此，通用逻辑应该是：\n            1.  对样本 $x$进行排序以得到 $x_{(1)}, \\dots, x_{(n)}$。\n            2.  初始化 `max_d = 0`。\n            3.  从 $i=1$ 到 $n$ 迭代：\n                a.  令 $x_{val} = x_{(i)}$。\n                b.  $F_n(x_{val}) = i/n$。（这是正确的，因为排序将所有 $\\le x_{(i)}$ 的值放在前 $i$ 个位置。）\n                c.  $F_n(x_{val}^-) = (i-1)/n$。（这是正确的，因为有 $i-1$ 个值严格小于或等于 $x_{(i-1)}$，这是严格小于 $x_{(i)}$ 的最大值，*如果没有相同值*）。\n                d.  有相同值时，此逻辑有误。对于样本数据中的一个值 $v$，假设它出现了 $k$ 次，并且有 $m$ 个值严格小于 $v$。那么对于任何 $x=v$，我们有 $F_n(x) = (m+k)/n$，而 $F_n(x^-) = m/n$。\n            4.  处理相同值的正确算法：\n                a. 排序样本：$x_{(1)}, \\dots, x_{(n)}$。\n                b. 初始化 `max_d = 0`。\n                c. 循环 $i$ 从 $1$到 $n$：\n                    i. 令 $val = x_{(i-1)}$（使用数组的0-based索引）。\n                    ii. 计算 $F_0(val)$。\n                    iii. 在此点的 $F_n$ 值为 $i/n$。所以最大差值的一个候选是 $|i/n - F_0(val)|$。\n                    iv. 在此点之前的 $F_n$ 值（来自上一步）是 $(i-1)/n$。所以另一个候选是 $|(i-1)/n - F_0(val)|$。\n                    v. `max_d = max(max_d, |i/n - F_0(val)|, |(i-1)/n - F_0(val)|)`。\n            5. 这会自动处理相同值。让我们追踪一下。\n               样本: $[0.2, 0.2, 0.5]$。$n=3$。排序后: $[0.2, 0.2, 0.5]$。\n               - i=1: $x_{(1)}=0.2$。$F_0(0.2)=0.2$。\n                 - diff1 = $|1/3 - 0.2| = |0.333 - 0.2| = 0.133$。\n                 - diff2 = $|0/3 - 0.2| = 0.2$。\n                 - 当前 max_d = 0.2。\n               - i=2: $x_{(2)}=0.2$。$F_0(0.2)=0.2$。\n                 - diff1 = $|2/3 - 0.2| = |0.667 - 0.2| = 0.467$。\n                 - diff2 = $|1/3 - 0.2| = |0.333 - 0.2| = 0.133$。\n                 - 当前 max_d = max(0.2, 0.467) = 0.467。\n               - i=3: $x_{(3)}=0.5$。$F_0(0.5)=0.5$。\n                 - diff1 = $|3/3 - 0.5| = |1.0 - 0.5| = 0.5$。\n                 - diff2 = $|2/3 - 0.5| = |0.667 - 0.5| = 0.167$。\n                 - 当前 max_d = max(0.467, 0.5) = 0.5。\n               最终结果: $D_3 = 0.5$。\n\n               让我们更仔细地分析这个结果。\n               ECDF是：\n               - 对于 $x  0.2$，$F_3(x) = 0$。\n               - 对于 $0.2 \\le x  0.5$，$F_3(x) = 2/3$。\n               - 对于 $x \\ge 0.5$，$F_3(x) = 1$。\n               差函数是 $|F_3(x) - F_0(x)|$：\n               - 对于 $x  0.2$：$|0 - x|$。在 $x \\to 0.2^-$ 时最大，为 $0.2$。（与 i=1, diff2 的计算相符）。\n               - 对于 $0.2 \\le x  0.5$：$|2/3 - x|$。在 $x=0.2$ 时最大，为 $|2/3-0.2| \\approx 0.467$。在 $x \\to 0.5^-$ 时最小，为 $|2/3-0.5| \\approx 0.167$。（与 i=2, diff1 和 i=3, diff2 的计算相符）。\n               - 对于 $x \\ge 0.5$：$|1 - x|$。在 $x=0.5$ 时最大，为 $|1-0.5| = 0.5$。（与 i=3, diff1 的计算相符）。\n               最大值确实是 $0.5$。\n\n               看起来简单的公式 $D_n = \\max_{i=1,\\dots,n} \\max( |i/n - F_0(x_{(i)})|, |(i-1)/n - F_0(x_{(i)})| )$ 即使在有相同值的情况下也是正确的，前提是 $F_0$ 是连续的。问题陈述给出的 $F_n$ 和 $F_0$ 的性质是“非递减的、具有左极限的右连续函数”。如果 $F_0$ 也有跳跃，情况会更复杂。但对于所有测试用例，$F_0$ 都是连续的。所以我将使用这个公式。\n\n    *   **算法总结**：\n        1.  输入：样本数据 $x_1, \\dots, x_n$ 和一个用于计算 $F_0$ 的函数。\n        2.  对样本数据进行排序：$x_{(1)} \\le x_{(2)} \\le \\dots \\le x_{(n)}$。\n        3.  初始化 `max_d = 0.0`。\n        4.  对于 $i$ 从 $1$ 到 $n$ 循环：\n            a. 令 $val = x_{(i-1)}$ (使用0-based索引)。\n            b. 计算 $f0\\_val = F_0(val)$。\n            c. 计算 $d1 = |(i/n) - f0\\_val|$。\n            d. 计算 $d2 = |((i-1)/n) - f0\\_val|$。\n            e. 更新 `max_d = max(max_d, d1, d2)`。\n        5.  结果是 `max_d`。\n\n    该算法由于排序是 $O(n \\log n)$，扫描是 $O(n)$。它满足所有要求。\n\n4.  **实现**：现在，我将为每个测试用例规划实现。\n    *   **主函数 `ks_test_statistic`**：这将实现上述算法。它将样本 `x` 和目标CDF函数 `f0` 作为参数。\n    *   **目标CDF函数**：我需要为每个 $F_0$ 实现一个函数。\n        *   `f0_uniform_0_1(x)`：实现 $U(0,1)$ 的CDF。\n        *   `f0_exp_lambda_2(x)`：实现 $\\text{Exp}(\\lambda=2)$ 的CDF。对于 $x \\ge 0$，$F_0(x) = 1 - e^{-2x}$。\n        *   `f0_norm_0_1(x)`：实现 $N(0,1)$ 的CDF。我可以使用 `scipy.stats.norm.cdf` 或 `math.erf`。`scipy` 是允许的。`scipy.special.erf` 也可用。$\\Phi(z) = \\frac{1}{2}(1 + \\text{erf}(z/\\sqrt{2}))$。这是可行的方法，因为没有明确列出 `scipy.stats`，但列出了 `scipy`。检查 `scipy.special.erf`。是的，它存在。\n\n    *   **测试用例**：\n        *   **情况 A**: `n=10`, `x = [0.14, ...], 'f0': f0_uniform_0_1`。\n        *   **情况 B**: `n=1`, `x=[0.3]`, `F0` 是 `U(0,1)`。\n            - 排序后的 x: `[0.3]`。n=1。\n            - i=1, x_1=0.3, F0=0.3。\n            - d_plus = |1/1 - 0.3| = 0.7。\n            - d_minus = |0/1 - 0.3| = 0.3。\n            - max_d = 0.7。\n        *   **情况 C**: `n=6`, `x=[0.2, 0.2, 0.5, 0.5, 0.5, 0.9]`, `F0` 是 `U(0,1)`。\n            - 结果应约为 0.333333。\n        *   **情况 D**: `n=6`, `x=[-1.0, -0.2, 0.0, 0.1, 1.1, 1.3]`, `F0` 是 `U(0,1)`。\n            - 结果应约为 0.566667。\n        *   **情况 E**: `n=6`, `x = [0.05, 0.15, 0.7, 0.3, 0.9, 1.2]`, `F0` 是 `Exp(lambda=2)`。\n        *   **情况 F**: `n=7`, `x = [-1.2, -0.5, 0.0, 0.2, 0.8, 1.5, 2.0]`, `F0` 是 `N(0,1)`。\n\n5.  **最终代码结构**：我在思考过程中概述的结构很好。我将实现它。\n    最终答案应四舍五入到六位小数，以逗号分隔的列表形式放在方括号内。f-string格式 `f'{r:.6f}'` 将实现这一点。\n\n我对验证、推导出的算法和实现计划充满信心。我现在将生成最终输出。", "answer": "```python\nimport numpy as np\nfrom scipy.special import erf\n\ndef solve():\n    \"\"\"\n    Solves the Kolmogorov-Smirnov test statistic problem for a suite of test cases.\n    \"\"\"\n\n    # --- Target Cumulative Distribution Functions (CDFs) ---\n\n    def f0_uniform_0_1(x):\n        \"\"\"CDF of the uniform distribution on [0,1].\"\"\"\n        if x  0:\n            return 0.0\n        elif x  1:\n            return x\n        else:\n            return 1.0\n\n    def f0_exp_lambda_2(x):\n        \"\"\"CDF of the exponential distribution with rate lambda=2.\"\"\"\n        lam = 2.0\n        if x  0:\n            return 0.0\n        else:\n            return 1.0 - np.exp(-lam * x)\n\n    def f0_norm_0_1(x, mu=0.0, sigma=1.0):\n        \"\"\"CDF of the normal distribution with mean mu and std dev sigma.\"\"\"\n        z = (x - mu) / sigma\n        return 0.5 * (1.0 + erf(z / np.sqrt(2.0)))\n\n    # --- Core Algorithm ---\n\n    def compute_dn(x_sample, f0_func):\n        \"\"\"\n        Computes the one-sample Kolmogorov-Smirnov statistic D_n.\n\n        The algorithm follows from an analysis of the statistic's definition:\n        D_n = sup |F_n(x) - F_0(x)|.\n        Since F_n(x) is a step function and F_0(x) is continuous for the\n        given cases, the supremum of the absolute difference must be achieved\n        at the sample points x_i.\n        At each sorted sample point x_(i), the empirical CDF F_n(x) can be\n        viewed from the left (value before the jump) and from the right\n        (value at and after the jump).\n        - The ECDF value for x  x_(i) corresponds to (i-1)/n.\n        - The ECDF value for x >= x_(i) corresponds to i/n.\n        Thus, for each sorted point x_(i), we check two differences against\n        the theoretical CDF F_0(x_(i)):\n        1. | (i/n) - F_0(x_(i)) |\n        2. | ((i-1)/n) - F_0(x_(i)) |\n        The maximum over all such differences for i=1,...,n is the statistic D_n.\n        This holds even with ties in the sample, as the logic effectively\n        checks the difference at the beginning and end of each step in the ECDF.\n        The complexity is O(n log n) from sorting, plus O(n) for the scan.\n        \"\"\"\n        n = len(x_sample)\n        if n == 0:\n            return 0.0\n        \n        x_sorted = np.sort(x_sample)\n        \n        max_d = 0.0\n        for i in range(1, n + 1):\n            val = x_sorted[i - 1]\n            f0_val = f0_func(val)\n            \n            # Difference between the upper ECDF step and the theoretical CDF\n            d1 = abs(i / n - f0_val)\n            \n            # Difference between the lower ECDF step and the theoretical CDF\n            d2 = abs((i - 1) / n - f0_val)\n            \n            if d1 > max_d:\n                max_d = d1\n            if d2 > max_d:\n                max_d = d2\n                \n        return max_d\n\n    # --- Test Suite Definition ---\n\n    test_cases = [\n        # Case A: General case, Uniform(0,1)\n        (np.array([0.14, 0.91, 0.22, 0.33, 0.73, 0.59, 0.01, 0.48, 0.64, 0.27]), f0_uniform_0_1),\n        \n        # Case B: Boundary size, n=1\n        (np.array([0.3]), f0_uniform_0_1),\n        \n        # Case C: Ties in the sample\n        (np.array([0.2, 0.2, 0.5, 0.5, 0.5, 0.9]), f0_uniform_0_1),\n\n        # Case D: Sample values outside the target distribution's main support\n        (np.array([-1.0, -0.2, 0.0, 0.1, 1.1, 1.3]), f0_uniform_0_1),\n\n        # Case E: Non-uniform target (Exponential)\n        (np.array([0.05, 0.15, 0.7, 0.3, 0.9, 1.2]), f0_exp_lambda_2),\n        \n        # Case F: Unbounded support target (Normal)\n        (np.array([-1.2, -0.5, 0.0, 0.2, 0.8, 1.5, 2.0]), f0_norm_0_1),\n    ]\n\n    # --- Execution and Output ---\n    \n    results = []\n    for x_sample, f0_func in test_cases:\n        dn_value = compute_dn(x_sample, f0_func)\n        results.append(dn_value)\n\n    # Format output as a comma-separated list of numbers rounded to 6 decimal places\n    output_str = \",\".join([f\"{r:.6f}\" for r in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3315959"}, {"introduction": "在学会计算KS统计量之后，理解其真正含义及局限性至关重要。这个思想实验[@problem_id:3315950]将挑战我们探索KS检验所测量的“垂直”差异与在逆变换采样等应用中至关重要的“水平”（或分位数）差异之间的区别。通过构建一个反例，我们将揭示一个小的KS距离并不总能保证两个分布在所有实际意义上都足够“接近”。", "problem": "在随机模拟和蒙特卡洛方法中，一个核心的诊断方法是通过累积分布函数 (CDF) 与分位数函数来比较分布。这是因为柯尔莫哥洛夫-斯米尔诺夫 (KS) 距离控制了 CDF 之间的最大垂直差异，而最大分位数偏差则控制了在与逆变换采样相关的固定概率水平上的最大水平差异。考虑以下候选反例族，旨在展示这两种度量在 CDF 交叉情况下的差异：\n设 $X \\sim \\mathrm{Unif}(0,1)$ 且 $Y \\sim \\mathrm{Unif}(-a,1+a)$，其中 $a>0$ 是一个固定参数。令 $F_X$ 和 $F_Y$ 表示它们的 CDF， $F_X^{-1}$ 和 $F_Y^{-1}$ 表示它们的分位数函数。在不借助任何预先制定的公式或黑箱定理的情况下，确定下列哪些陈述是正确的。\n\nA. 对于任何 $a>0$，CDF $F_X$ 和 $F_Y$ 都不交叉。\n\nB. 最大分位数偏差 $\\sup_{p \\in (0,1)} \\lvert F_X^{-1}(p)-F_Y^{-1}(p)\\rvert$ 等于 $a$，而柯尔莫哥洛夫-斯米尔诺夫距离 $\\sup_{x \\in \\mathbb{R}} \\lvert F_X(x)-F_Y(x)\\rvert$ 等于 $\\dfrac{a}{1+2a}$。\n\nC. 当 $a=3$ 时，$X$ 和 $Y$ 之间的柯尔莫哥洛夫-斯米尔诺夫统计量超过 $0.6$。\n\nD. 存在 $a>0$ 使得柯尔莫哥洛夫-斯米尔诺夫距离至多为 $0.49$，而最大分位数偏差至少为 $10$。\n\n选择所有适用项。你的推理应从 CDF、分位数函数和基于上确界的距离的定义出发，并仅使用第一性原理来验证或驳斥上述族中的每个陈述，从而评估该族是否提供了一个具有交叉 CDF 且能区分这两种度量的反例。", "solution": "我们从第一性原理出发推导每个论断，仅使用累积分布函数 (CDF)、分位数函数和基于上确界的距离的定义。\n\n步骤 1：计算并比较 CDF，并找出交叉点。\n根据定义，对于 $X \\sim \\mathrm{Unif}(0,1)$，其 CDF 为\n$$\nF_X(x) = \n\\begin{cases}\n0,  x  0,\\\\\nx,  0 \\le x \\le 1,\\\\\n1,  x > 1.\n\\end{cases}\n$$\n对于 $Y \\sim \\mathrm{Unif}(-a,1+a)$ 且 $a>0$，其 CDF 为\n$$\nF_Y(x) = \n\\begin{cases}\n0,  x  -a,\\\\\n\\dfrac{x+a}{1+2a},  -a \\le x \\le 1+a,\\\\\n1,  x > 1+a.\n\\end{cases}\n$$\n为检查交叉情况，我们在重叠区间 $[0,1]$ 上比较 $F_X(x)$ 和 $F_Y(x)$：\n$$\nF_X(x) - F_Y(x) = x - \\frac{x+a}{1+2a} = \\frac{(1+2a)x - (x+a)}{1+2a} = \\frac{2ax - a}{1+2a} = \\frac{a(2x-1)}{1+2a}.\n$$\n当 $x  \\tfrac{1}{2}$ 时，这个差值为负；当 $x=\\tfrac{1}{2}$ 时，差值为零；当 $x > \\tfrac{1}{2}$ 时，差值为正。因此，$F_X$ 和 $F_Y$ 在 $[0,1]$ 内恰好在 $x=\\tfrac{1}{2}$ 处交叉一次。在此区间之外，注意到对于 $x \\in (-a,0)$，有 $F_X(x)=0$ 和 $F_Y(x)>0$；而对于 $x \\in (1,1+a)$，有 $F_X(x)=1$ 和 $F_Y(x)1$，所以没有其他交叉点。因此，这两个 CDF 确实交叉，且仅交叉一次。\n\n步骤 2：计算柯尔莫哥洛夫-斯米尔诺夫 (KS) 距离。\n$F_X$ 和 $F_Y$ 之间的柯尔莫哥洛夫-斯米尔诺夫距离定义为\n$$\nD_{\\mathrm{KS}}(F_X,F_Y) := \\sup_{x \\in \\mathbb{R}} \\lvert F_X(x) - F_Y(x) \\rvert.\n$$\n我们分段求绝对差的最大值。\n\n- 对于 $x \\in [0,1]$，我们有\n$$\n\\left|F_X(x)-F_Y(x)\\right| = \\left|\\frac{a(2x-1)}{1+2a}\\right| \\le \\frac{a}{1+2a},\n$$\n根据表达式在端点附近的连续性，在 $x=0$ 和 $x=1$ 处取等号（也请参见下面的边界计算）。\n\n- 对于 $x \\in (-a,0)$，$F_X(x)=0$ 且 $F_Y(x)=(x+a)/(1+2a) \\in (0,a/(1+2a))$，所以\n$$\n\\left|F_X(x)-F_Y(x)\\right| = \\frac{x+a}{1+2a}  \\frac{a}{1+2a}.\n$$\n\n- 对于 $x \\in (1,1+a)$，$F_X(x)=1$ 且 $F_Y(x)=(x+a)/(1+2a)$，因此\n$$\n\\left|F_X(x)-F_Y(x)\\right| = 1 - \\frac{x+a}{1+2a} = \\frac{1+a-x}{1+2a} \\le \\frac{a}{1+2a},\n$$\n当 $x \\downarrow 1$ 时，差值趋近于上界。\n\n- 对于 $x \\le -a$ 或 $x \\ge 1+a$，两个 CDF 都在 0 或 1 处重合，差值为零。\n\n因此，\n$$\nD_{\\mathrm{KS}}(F_X,F_Y) = \\frac{a}{1+2a}.\n$$\n\n步骤 3：计算最大分位数偏差。\n$F_X$ 的分位数函数（广义逆）为\n$$\nF_X^{-1}(p) = \\inf\\{x \\in \\mathbb{R} : F_X(x) \\ge p\\} = p, \\quad p \\in (0,1).\n$$\n对于 $Y \\sim \\mathrm{Unif}(-a,1+a)$，在 $[-a,1+a]$ 上的 CDF 是仿射的：$F_Y(x)=(x+a)/(1+2a)$。反转此关系以获得分位数函数：\n$$\nF_Y^{-1}(p) = -a + (1+2a)p, \\quad p \\in (0,1).\n$$\n因此，逐点分位数差为\n$$\n\\Delta_Q(p) := F_X^{-1}(p) - F_Y^{-1}(p) = p - \\big(-a + (1+2a)p\\big) = a - 2ap.\n$$\n因此，最大绝对分位数偏差为\n$$\n\\sup_{p \\in (0,1)} \\lvert \\Delta_Q(p) \\rvert = \\sup_{p \\in (0,1)} \\lvert a - 2ap \\rvert = a,\n$$\n在极限 $p \\downarrow 0$ 或 $p \\uparrow 1$ 时达到。\n\n步骤 4：逐项分析。\n\n选项 A：“对于任何 $a>0$，CDF $F_X$ 和 $F_Y$ 都不交叉。”\n根据步骤 1，在 $[0,1]$ 上，$F_X(x)-F_Y(x) = a(2x-1)/(1+2a)$，该式在 $x=\\tfrac{1}{2}$ 处变号。因此，CDF 恰好交叉一次。结论：不正确。\n\n选项 B：“最大分位数偏差 $\\sup_{p \\in (0,1)} \\lvert F_X^{-1}(p)-F_Y^{-1}(p)\\rvert$ 等于 $a$，而柯尔莫哥洛夫-斯米尔诺夫距离 $\\sup_{x \\in \\mathbb{R}} \\lvert F_X(x)-F_Y(x)\\rvert$ 等于 $\\dfrac{a}{1+2a}$。”\n根据步骤 2 和 3，我们计算出的正是这些值。结论：正确。\n\n选项 C：“当 $a=3$ 时，$X$ 和 $Y$ 之间的柯尔莫哥洛夫-斯米尔诺夫统计量超过 $0.6$。”\n将 $a=3$ 代入 $D_{\\mathrm{KS}} = a/(1+2a)$ 得到 $D_{\\mathrm{KS}} = 3/7 \\approx 0.4286$，这不超过 $0.6$。结论：不正确。\n\n选项 D：“存在 $a>0$ 使得柯尔莫哥洛夫-斯米尔诺夫距离至多为 $0.49$，而最大分位数偏差至少为 $10$。”\n我们需要 $a/(1+2a) \\le 0.49$ 且 $a \\ge 10$。函数 $a \\mapsto a/(1+2a)$ 在 $a>0$ 上是递增的，并且当 $a \\to \\infty$ 时从下方趋近于 $1/2$。当 $a=10$ 时，$a/(1+2a) = 10/21 \\approx 0.476 \\le 0.49$，且最大分位数偏差等于 $a=10 \\ge 10$。因此，存在这样的 $a$（例如 $a=10$）。结论：正确。\n\n结论与作为交叉 CDF 反例的解释：\n族 $X \\sim \\mathrm{Unif}(0,1)$ 和 $Y \\sim \\mathrm{Unif}(-a,1+a)$ 提供了一个简洁的反例：CDF 交叉，柯尔莫哥洛夫-斯米尔诺夫距离有界（实际上等于 $a/(1+2a)$，小于 $1/2$），然而最大分位数偏差等于 $a$ 且可以通过增加 $a$ 使其任意大。这表明，较小或中等的柯尔莫哥洛夫-斯米尔诺夫距离并不能控制与逆变换蒙特卡洛方法相关的最坏情况下的分位数差异。", "answer": "$$\\boxed{BD}$$", "id": "3315950"}, {"introduction": "在许多实际应用中，我们假设的分布参数是未知的，必须从数据中进行估计。本练习将处理这种复合假设下的高级情景，此时标准的KS检验临界值不再适用。我们将实现参数自助法（parametric bootstrap）[@problem_id:3316006]，这是一种强大而通用的模拟方法，用以在复合原假设下正确地近似p值。这个练习展示了如何将经典检验方法应用于更复杂、更现实的统计问题中。", "problem": "给定一个来自未知分布的独立同分布样本 $\\{X_1,\\dots,X_n\\}$。考虑一个参数复合零假设 $H_0: X_i \\sim F(x;\\theta_0)$，其中参数 $\\theta_0$ 属于参数空间 $\\Theta$，$F(x;\\theta)$ 是一个完全指定的累积分布函数。令 $\\hat{\\theta}_n$ 表示从观测样本计算出的最大似然估计量 (MLE)。定义经验分布函数 $F_n(x) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{X_i \\le x\\}$ 和复合零假设下的单样本 Kolmogorov-Smirnov 统计量，\n$$\nD_n = \\sup_{x \\in \\mathbb{R}} \\left| F_n(x) - F(x;\\hat{\\theta}_n) \\right|.\n$$\n您的任务是设计并实现一个参数自助法 (parametric bootstrap) 算法，用以在参数 $\\theta$ 被估计的情况下，近似 $H_0$ 下 $D_n$ 的有限样本分布，并计算自助法 $p$ 值。具体而言：\n- 提出一个参数自助法程序：对于选定的自助法重复次数 $B$，该程序从 $F(x;\\hat{\\theta}_n)$ 中重复重抽样，在每个自助法重抽样样本上通过 MLE 重新拟合参数，在自助法重抽样样本上重新计算 Kolmogorov-Smirnov 统计量 $D_n^{*}$，并输出 $p$ 值的蒙特卡洛近似。$p$ 值定义为大于或等于观测到的 $D_n$ 的自助法统计量的比例。\n- 从经验过程的基本定义和 MLE 的标准正则性条件出发，为该算法的渐近有效性提供一个有理有据的论证，且不借用关于极限分布的简便公式。\n\n实现细节：\n- 在计算 $D_n$ 时，使用上述定义以及这样一个事实：对于连续的 $F(\\cdot;\\theta)$，上确界在样本点处取得。您必须推导出一个可实现的 $D_n$ 计算公式，作为解决方案的一部分。\n- 为以下参数族实现 MLE：\n  1. 正态族 $F(x;\\theta)$，其中 $\\theta=(\\mu,\\sigma)$，$\\mu \\in \\mathbb{R}$ 且 $\\sigma>0$。\n  2. 指数族 $F(x;\\theta)$，其中 $\\theta=(\\lambda)$，$\\lambda>0$，支撑集为 $[0,\\infty)$。\n\n测试协议：\n- 您的程序必须构建并分析以下测试套件。每个测试用例指定一个数据生成过程（用于生成观测样本）、用于拟合和检验的零假设模型族、样本大小 $n$、自助法重复次数 $B$ 以及一个随机种子。在所有情况下，输出都必须是一个在 $[0,1]$ 区间内的浮点数 $p$ 值。\n  1. 正态零假设，设定正确：观测数据从参数为 $\\mu=0, \\sigma=1$ 的正态分布生成；零假设模型族为正态族；$n=120$；$B=500$；种子 $20231102$。\n  2. 正态零假设，设定错误（重尾）：观测数据从自由度 $\\nu=3$ 的学生t分布生成并缩放至单位方差；零假设模型族为正态族；$n=120$；$B=500$；种子 $20231103$。\n  3. 指数零假设，设定正确：观测数据从率参数 $\\lambda=1.5$ 的指数分布生成；零假设模型族为指数族；$n=100$；$B=400$；种子 $20231104$。\n  4. 正态零假设，边界样本大小：观测数据从参数为 $\\mu=0.5, \\sigma=0.2$ 的正态分布生成；零假设模型族为正态族；$n=30$；$B=400$；种子 $20231105$。\n\n输出规范：\n- 您的程序必须产生单行输出，其中包含四个测试用例的自助法 $p$ 值，形式为逗号分隔的列表并用方括号括起，例如 $[p_1,p_2,p_3,p_4]$。\n- 每个 $p$ 值必须以浮点数形式打印，并四舍五入到六位小数。\n- 不得打印任何其他文本。\n\n注意：\n- 不涉及物理单位。\n- 不涉及角度。\n- 百分比必须以 $[0,1]$ 区间内的小数表示。\n\n您的算法必须是完全自包含的，并且必须仅使用允许的编程库。您的论证必须从经验过程和 MLE 的标准渐近性质开始，并最终得出参数自助法在复合零假设下对 Kolmogorov-Smirnov 统计量有效的结论。", "solution": "该问题是有效的。它提出了计算统计学中一个明确定义的任务，具体而言，是在分布参数由数据估计时，应用参数自助法来近似 Kolmogorov-Smirnov (KS) 统计量的零分布。其底层的统计理论是可靠的，实现细节和测试用例是完整且一致的。\n\n问题的核心在于，标准的 Kolmogorov-Smirnov 检验假设零分布是完全指定的。当零分布的参数从被检验的同一份数据中估计时，KS 统计量 $D_n = \\sup_{x \\in \\mathbb{R}} \\left| F_n(x) - F(x;\\hat{\\theta}_n) \\right|$ 将不再是分布无关的。其渐近分布依赖于零假设的真实（未知）参数，这使得标准数表不再适用。对于正态族的情况，这便是著名的 Lilliefors 检验。参数自助法提供了一种通用而强大的方法来近似正确的零分布。\n\n### 参数自助法算法的论证\n\n**1. 含估计参数的 KS 统计量的渐近分布**\n\n设零假设为 $H_0: X_i \\sim F(x; \\theta_0)$，其中 $\\theta_0 \\in \\Theta$ 是某个未知参数。检验统计量为 $D_n = \\sup_x |F_n(x) - F(x;\\hat{\\theta}_n)|$，其中 $F_n(x)$ 是经验分布函数 (EDF)，$\\hat{\\theta}_n$ 是 $\\theta$ 的最大似然估计量 (MLE)。我们分析中心化和标准化后的统计量 $\\sqrt{n} D_n$。\n\n我们可以将上确界内的项分解为：\n$$\n\\sqrt{n}(F_n(x) - F(x;\\hat{\\theta}_n)) = \\sqrt{n}(F_n(x) - F(x;\\theta_0)) - \\sqrt{n}(F(x;\\hat{\\theta}_n) - F(x;\\theta_0))\n$$\n第一项 $\\alpha_n(x) = \\sqrt{n}(F_n(x) - F(x;\\theta_0))$ 是经验过程。根据 Donsker 定理，在 $H_0$ 下，$\\alpha_n(\\cdot)$ 依分布收敛到一个高斯过程 $G(\\cdot)$，这是一个在两端都固定为 0 的布朗桥 $B(F(x; \\theta_0))$。\n\n对于第二项，在 MLE 的标准正则性条件下，我们可以将 $F(x;\\hat{\\theta}_n)$ 在真实参数 $\\theta_0$ 附近进行泰勒展开。对于 $\\theta \\in \\mathbb{R}^p$：\n$$\nF(x;\\hat{\\theta}_n) \\approx F(x;\\theta_0) + (\\hat{\\theta}_n - \\theta_0)^T \\nabla_{\\theta} F(x;\\theta_{0})\n$$\n其中 $\\nabla_{\\theta}$ 是关于 $\\theta$ 的梯度。这给出：\n$$\n\\sqrt{n}(F(x;\\hat{\\theta}_n) - F(x;\\theta_0)) \\approx (\\sqrt{n}(\\hat{\\theta}_n - \\theta_0))^T \\nabla_{\\theta} F(x;\\theta_0)\n$$\n同样，在正则性条件下，MLE 是渐近有效且服从正态分布的：\n$$\n\\sqrt{n}(\\hat{\\theta}_n - \\theta_0) \\xrightarrow{d} \\mathcal{N}(0, I(\\theta_0)^{-1})\n$$\n其中 $I(\\theta_0)$ 是在真实参数 $\\theta_0$ 处的费雪信息矩阵 (Fisher information matrix)。\n\n结合这些结果，过程 $\\sqrt{n}(F_n(x) - F(x;\\hat{\\theta}_n))$ 依分布收敛到一个极限高斯过程：\n$$\n\\mathbb{G}_n(x) = \\alpha_n(x) - (\\sqrt{n}(\\hat{\\theta}_n - \\theta_0))^T \\nabla_{\\theta} F(x;\\theta_0) + o_p(1) \\xrightarrow{d} \\mathbb{G}(x; \\theta_0)\n$$\n极限过程 $\\mathbb{G}(x; \\theta_0)$ 是一个高斯过程，其协方差结构通过梯度项 $\\nabla_{\\theta} F(x;\\theta_0)$ 和决定 MLE 方差的费雪信息 $I(\\theta_0)$ 两者依赖于 $\\theta_0$。检验统计量 $\\sqrt{n}D_n$ 依分布收敛到 $\\sup_x|\\mathbb{G}(x;\\theta_0)|$。由于这个极限分布依赖于未知的 $\\theta_0$，该统计量不是枢轴量 (pivotal)，我们无法预先制表其临界值。\n\n**2. 自助法解决方案**\n\n参数自助法通过近似 $H_0$ 下 $D_n$ 的未知分布来规避此问题。该程序基于“自助法原理”：我们将估计出的模型 $F(x;\\hat{\\theta}_n)$ 视为真实的数据生成分布。\n\n算法如下：\n1.  从观测样本 $\\{X_1, \\dots, X_n\\}$ 计算 MLE $\\hat{\\theta}_n$。\n2.  计算观测到的检验统计量 $D_n^{\\text{obs}} = \\sup_{x} |F_n(x) - F(x;\\hat{\\theta}_n)|$。\n3.  对于大量的重复次数 $B$：\n    a. 通过从分布 $F(x;\\hat{\\theta}_n)$ 中抽取独立同分布的随机变量，生成一个大小为 $n$ 的自助法样本 $\\{X_1^*, \\dots, X_n^*\\}$。\n    b. 从自助法样本 $\\{X_i^*\\}$ 计算 MLE $\\hat{\\theta}_n^*$。\n    c. 计算自助法统计量 $D_n^* = \\sup_{x} |F_n^*(x) - F(x;\\hat{\\theta}_n^*)|$，其中 $F_n^*(x)$ 是自助法样本的经验分布函数 (EDF)。\n4.  自助法统计量的集合 $\\{D_{n,1}^*, \\dots, D_{n,B}^*\\}$ 构成一个经验分布，该分布近似于 $D_n$ 的真实零分布。\n5.  $p$ 值被估计为大于或等于观测统计量的自助法统计量的比例：\n    $$\n    p\\text{-value} = \\frac{1}{B} \\sum_{b=1}^B \\mathbf{1}\\{D_{n,b}^* \\ge D_n^{\\text{obs}}\\}\n    $$\n\n**3. 渐近有效性**\n\n此程序的有效性依赖于这样一个事实：在给定原始数据的条件下，自助法统计量 $D_n^*$ 与原始统计量 $D_n$ 具有相同的极限分布，只是用 $\\hat{\\theta}_n$ 替换了 $\\theta_0$。也就是说，$\\sqrt{n}D_n^*$ 依分布收敛到 $\\sup_x|\\mathbb{G}(x; \\hat{\\theta}_n)|$。\n\n因为 MLE $\\hat{\\theta}_n$ 是 $\\theta_0$ 的一致估计量（即，当 $n \\to \\infty$ 时，$\\hat{\\theta}_n \\xrightarrow{p} \\theta_0$），并且假设高斯过程 $\\mathbb{G}(x;\\theta)$ 的协方差结构在 $\\theta$ 上是连续的，那么自助法极限分布 $\\sup_x|\\mathbb{G}(x; \\hat{\\theta}_n)|$ 依概率收敛到真实的极限分布 $\\sup_x|\\mathbb{G}(x; \\theta_0)|$。这意味着 $D_n^*$ 的分布（以数据为条件）一致地估计了 $D_n$ 的真实零分布。因此，来自自助法分布的分位数是真实分位数的一致估计，由此产生的 $p$ 值是渐近有效的。\n\n### 算法实现\n\n**MLE 公式：**\n- 对于正态分布 $\\mathcal{N}(\\mu, \\sigma^2)$，MLE 是样本均值 $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n X_i$ 和样本标准差（分母为 $n$）$\\hat{\\sigma} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(X_i-\\hat{\\mu})^2}$。\n- 对于率参数为 $\\lambda$ 的指数分布，其概率密度函数 (PDF) 为 $f(x;\\lambda) = \\lambda e^{-\\lambda x}$。$\\lambda$ 的 MLE 是样本均值的倒数：$\\hat{\\lambda} = 1/\\bar{X} = n/\\sum_{i=1}^n X_i$。\n\n**KS 统计量公式：**\n对于一个连续的累积分布函数 (CDF) $F$，$|F_n(x) - F(x;\\hat{\\theta}_n)|$ 的上确界在某个样本点 $X_{(i)}$ 处取得。令 $X_{(1)} \\le \\dots \\le X_{(n)}$ 为排序后的样本。该统计量可以高效地计算如下：\n$$\nD_n = \\max_{i=1,\\dots,n} \\max \\left( \\frac{i}{n} - F(X_{(i)};\\hat{\\theta}_n), F(X_{(i)};\\hat{\\theta}_n) - \\frac{i-1}{n} \\right)\n$$\n该公式考虑了在每个样本点处经验分布函数 (EDF) 跳跃点之前和之后的最大差异。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm, expon, t\n\ndef mle_normal(data: np.ndarray) - tuple[float, float]:\n    \"\"\"Computes the Maximum Likelihood Estimators for a Normal distribution.\"\"\"\n    mu_hat = np.mean(data)\n    sigma_hat = np.std(data)  # MLE uses n in denominator, ddof=0 is default\n    return mu_hat, sigma_hat\n    \ndef mle_exponential(data: np.ndarray) - tuple[float]:\n    \"\"\"Computes the Maximum Likelihood Estimator for an Exponential distribution.\"\"\"\n    # Note: lambda is the rate parameter.\n    if np.mean(data) == 0:\n        # Handle case with all zeros, though unlikely for continuous data.\n        # Return a large lambda to signify a distribution concentrated at 0.\n        return (np.inf,)\n    lambda_hat = 1.0 / np.mean(data)\n    return (lambda_hat,)\n\ndef calculate_ks_statistic(data: np.ndarray, model_name: str) - tuple[float, tuple]:\n    \"\"\"\n    Calculates the Kolmogorov-Smirnov statistic for a given sample and a parametric family.\n    \n    Returns the statistic D_n and the estimated parameters theta_hat.\n    \"\"\"\n    n = len(data)\n    if n == 0:\n        return 0.0, ()\n        \n    data_sorted = np.sort(data)\n    \n    if model_name == 'normal':\n        params = mle_normal(data)\n        mu_hat, sigma_hat = params\n        if sigma_hat == 0: # All data points are the same\n            return 1.0, params\n        cdf_values = norm.cdf(data_sorted, loc=mu_hat, scale=sigma_hat)\n    elif model_name == 'exponential':\n        params = mle_exponential(data)\n        lambda_hat, = params\n        if lambda_hat == np.inf: # All data points are zero\n             return 1.0, params\n        # scipy.stats.expon uses scale parameter beta = 1/lambda\n        cdf_values = expon.cdf(data_sorted, scale=1.0/lambda_hat)\n    else:\n        raise ValueError(f\"Unknown model name: {model_name}\")\n\n    i_vals = np.arange(1, n + 1)\n    \n    d_plus = np.max((i_vals / n) - cdf_values)\n    d_minus = np.max(cdf_values - ((i_vals - 1) / n))\n    \n    d_n = np.max([d_plus, d_minus])\n    \n    return d_n, params\n\ndef run_bootstrap_test(observed_data: np.ndarray, null_model: str, B: int, rng: np.random.Generator) - float:\n    \"\"\"\n    Performs a parametric bootstrap for the KS test with estimated parameters.\n    \"\"\"\n    # 1. Calculate observed statistic and parameters\n    d_observed, theta_hat_observed = calculate_ks_statistic(observed_data, null_model)\n    n = len(observed_data)\n    \n    bootstrap_stats = np.zeros(B)\n    for i in range(B):\n        # 2a. Generate a bootstrap sample from F(x; theta_hat_observed)\n        if null_model == 'normal':\n            mu_hat, sigma_hat = theta_hat_observed\n            if sigma_hat == 0: # Cannot generate from degenerate distribution\n                bootstrap_sample = np.full(n, mu_hat)\n            else:\n                bootstrap_sample = rng.normal(loc=mu_hat, scale=sigma_hat, size=n)\n        elif null_model == 'exponential':\n            lambda_hat, = theta_hat_observed\n            if lambda_hat == 0 or not np.isfinite(lambda_hat): # Cannot generate\n                bootstrap_sample = np.zeros(n)\n            else:\n                # numpy.random.exponential uses scale beta = 1/lambda\n                bootstrap_sample = rng.exponential(scale=1.0/lambda_hat, size=n)\n\n        # 2b. Calculate KS statistic for the bootstrap sample\n        d_bootstrap, _ = calculate_ks_statistic(bootstrap_sample, null_model)\n        bootstrap_stats[i] = d_bootstrap\n        \n    # 3. Calculate p-value\n    # Defined as the fraction of bootstrap statistics at least as large as the observed one.\n    count_exceeding = np.sum(bootstrap_stats >= d_observed)\n    p_value = count_exceeding / B\n    \n    return p_value\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {\n            'description': 'Normal-null, correctly specified',\n            'data_gen': {'name': 'normal', 'params': {'loc': 0.0, 'scale': 1.0}},\n            'null_model': 'normal',\n            'n': 120, 'B': 500, 'seed': 20231102\n        },\n        {\n            'description': 'Normal-null, misspecified (heavy tails)',\n            'data_gen': {'name': 't_scaled', 'params': {'df': 3}},\n            'null_model': 'normal',\n            'n': 120, 'B': 500, 'seed': 20231103\n        },\n        {\n            'description': 'Exponential-null, correctly specified',\n            'data_gen': {'name': 'exponential', 'params': {'scale': 1.0 / 1.5}},\n            'null_model': 'exponential',\n            'n': 100, 'B': 400, 'seed': 20231104\n        },\n        {\n            'description': 'Normal-null, boundary-sized sample',\n            'data_gen': {'name': 'normal', 'params': {'loc': 0.5, 'scale': 0.2}},\n            'null_model': 'normal',\n            'n': 30, 'B': 400, 'seed': 20231105\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        rng = np.random.default_rng(case['seed'])\n        n = case['n']\n        \n        # Generate the \"observed\" data for the test case\n        gen_info = case['data_gen']\n        if gen_info['name'] == 'normal':\n            observed_data = rng.normal(size=n, **gen_info['params'])\n        elif gen_info['name'] == 'exponential':\n            observed_data = rng.exponential(size=n, **gen_info['params'])\n        elif gen_info['name'] == 't_scaled':\n            df = gen_info['params']['df']\n            # Var(t(df)) = df / (df - 2). We scale to get unit variance.\n            # Scale factor = 1 / sqrt(Var(t(df))) = sqrt((df-2)/df)\n            scale_factor = np.sqrt((df - 2) / df)\n            observed_data = scale_factor * t.rvs(df=df, size=n, random_state=rng)\n        \n        p_value = run_bootstrap_test(observed_data, case['null_model'], case['B'], rng)\n        results.append(p_value)\n\n    # Format output as specified\n    formatted_results = [f\"{p:.6f}\" for p in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3316006"}]}