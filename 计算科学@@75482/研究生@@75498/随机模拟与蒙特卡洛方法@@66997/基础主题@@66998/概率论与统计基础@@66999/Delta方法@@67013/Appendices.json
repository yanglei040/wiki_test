{"hands_on_practices": [{"introduction": "深入理解一个方法的最佳途径之一是从基本原理出发构建它。本练习将引导你仅使用中心极限定理和泰勒展开，为一个常见且重要的变换（自然对数）推导其方差的近似表达式[@problem_id:3352081]。这个过程不仅能揭示德尔塔方法的核心机制，还将阐明函数曲率与统计偏差之间的深刻联系，这是超越简单公式应用的宝贵见解。", "problem": "考虑一个蒙特卡洛设定，其中一个正量通过样本均值来估计。设 $\\{X_{i}\\}_{i=1}^{n}$ 为独立同分布的随机变量，并设 $h(X)$ 是一个可测的、几乎必然为正且方差有限的被积函数。定义正参数 $\\theta \\equiv \\mathbb{E}[h(X)]  0$ 及其蒙特卡洛估计量 $\\hat{\\theta}_{n} \\equiv \\frac{1}{n}\\sum_{i=1}^{n} h(X_{i})$。记 $\\sigma_{h}^{2} \\equiv \\mathrm{Var}(h(X)) \\in (0,\\infty)$。设 $g(x) = \\ln x$ 为自然对数。\n\n仅从中心极限定理（CLT）和二阶泰勒展开出发，且不援引任何预先给出的Delta方法公式，完成以下任务：\n\n- 当 $n \\to \\infty$ 时，推导 $\\mathrm{Var}(g(\\hat{\\theta}_{n}))$ 的一阶近似的闭式解，该解仅用 $\\theta$、$\\sigma_{h}^{2}$ 和 $n$ 表示。\n- 解释对数函数的曲率（其二阶导数）如何决定 $g(\\hat{\\theta}_{n})$ 作为 $g(\\theta)$ 的估计量的偏差的符号和主阶项。\n\n你的最终答案必须是关于 $\\mathrm{Var}(g(\\hat{\\theta}_{n}))$ 一阶近似的单个闭式解析表达式，用 $\\theta$、$\\sigma_{h}^{2}$ 和 $n$ 表示。不需要进行数值计算。不要包含任何单位。", "solution": "该问题是有效的，因为它具有科学依据，问题设定良好且客观，属于统计推断和蒙特卡洛方法的标准框架内。所有必要信息均已提供，问题没有矛盾或歧义。\n\n这个问题要求完成两个主要任务：首先，为一个变换后的蒙特卡洛估计量的方差推导出一阶近似；其次，解释这个变换后估计量的偏差。\n\n将随机变量序列 $\\{h(X_{i})\\}_{i=1}^{n}$ 记为 $\\{Y_{i}\\}_{i=1}^{n}$。根据问题陈述，这些是独立同分布（i.i.d.）的随机变量，其期望为 $\\mathbb{E}[Y_{i}] = \\theta$，方差为 $\\mathrm{Var}(Y_{i}) = \\sigma_{h}^{2}$。蒙特卡洛估计量是样本均值 $\\hat{\\theta}_{n} = \\frac{1}{n}\\sum_{i=1}^{n} Y_{i}$。\n\n该估计量的期望是 $\\mathbb{E}[\\hat{\\theta}_{n}] = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n} Y_{i}\\right] = \\frac{1}{n}\\sum_{i=1}^{n} \\mathbb{E}[Y_{i}] = \\frac{1}{n}(n\\theta) = \\theta$，这表明 $\\hat{\\theta}_{n}$ 是 $\\theta$ 的一个无偏估计量。\n由于 $Y_i$ 的独立性，该估计量的方差为 $\\mathrm{Var}(\\hat{\\theta}_{n}) = \\mathrm{Var}\\left(\\frac{1}{n}\\sum_{i=1}^{n} Y_{i}\\right) = \\frac{1}{n^{2}}\\sum_{i=1}^{n} \\mathrm{Var}(Y_{i}) = \\frac{1}{n^{2}}(n\\sigma_{h}^{2}) = \\frac{\\sigma_{h}^{2}}{n}$。\n\n根据中心极限定理（CLT），当 $n \\to \\infty$ 时，标准化的样本均值的分布收敛于标准正态分布。CLT 的一个常见表述是：\n$$ \\sqrt{n}(\\hat{\\theta}_{n} - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\sigma_{h}^{2}) $$\n其中 $\\xrightarrow{d}$ 表示依分布收敛。这意味着对于大的 $n$，$\\hat{\\theta}_{n}$ 近似服从正态分布，其均值为 $\\theta$，方差为 $\\frac{\\sigma_{h}^{2}}{n}$。\n\n**第一部分：方差近似的推导**\n\n我们被要求求出 $\\mathrm{Var}(g(\\hat{\\theta}_{n}))$ 的一个近似，其中 $g(x) = \\ln x$。推导必须从第一性原理出发，即中心极限定理和泰勒展开。我们在点 $\\theta = \\mathbb{E}[\\hat{\\theta}_{n}]$ 附近对函数 $g(\\hat{\\theta}_{n})$ 进行一阶泰勒级数展开。\n一阶展开式由下式给出：\n$$ g(\\hat{\\theta}_{n}) = g(\\theta) + g'(\\theta)(\\hat{\\theta}_{n} - \\theta) + R_{1} $$\n其中 $R_{1}$ 是余项，当 $\\hat{\\theta}_{n}$ 接近 $\\theta$ 时，它的阶数比线性项更小。对于 $g(x) = \\ln x$，其一阶导数为 $g'(x) = \\frac{1}{x}$。在 $x = \\theta$ 处求值得到 $g'(\\theta) = \\frac{1}{\\theta}$。\n因此，一阶近似为：\n$$ g(\\hat{\\theta}_{n}) \\approx g(\\theta) + \\frac{1}{\\theta}(\\hat{\\theta}_{n} - \\theta) $$\n我们关心这个表达式的方差。利用方差的性质，即对于常数 $a$ 和随机变量 $Z$，有 $\\mathrm{Var}(aZ+b) = a^{2}\\mathrm{Var}(Z)$：\n$$ \\mathrm{Var}(g(\\hat{\\theta}_{n})) \\approx \\mathrm{Var}\\left(g(\\theta) + \\frac{1}{\\theta}(\\hat{\\theta}_{n} - \\theta)\\right) $$\n由于 $g(\\theta)=\\ln(\\theta)$ 和 $\\theta$ 是常数，它们对 variance 没有贡献。表达式简化为：\n$$ \\mathrm{Var}(g(\\hat{\\theta}_{n})) \\approx \\mathrm{Var}\\left(\\frac{1}{\\theta}\\hat{\\theta}_{n}\\right) = \\left(\\frac{1}{\\theta}\\right)^{2} \\mathrm{Var}(\\hat{\\theta}_{n}) $$\n代入已知的方差 $\\mathrm{Var}(\\hat{\\theta}_{n}) = \\frac{\\sigma_{h}^{2}}{n}$：\n$$ \\mathrm{Var}(g(\\hat{\\theta}_{n})) \\approx \\frac{1}{\\theta^{2}} \\left( \\frac{\\sigma_{h}^{2}}{n} \\right) = \\frac{\\sigma_{h}^{2}}{n\\theta^{2}} $$\n这就是当 $n \\to \\infty$ 时，$g(\\hat{\\theta}_{n})$ 方差所需的一阶近似。\n\n**第二部分：偏差与曲率的作用**\n\n为了分析 $g(\\hat{\\theta}_{n})$ 作为 $g(\\theta) = \\ln \\theta$ 估计量的偏差，我们必须近似其期望 $\\mathbb{E}[g(\\hat{\\theta}_{n})]$。一阶泰勒展开是不够的，因为 $\\mathbb{E}[\\hat{\\theta}_{n} - \\theta] = 0$。我们必须进行到二阶泰勒展开：\n$$ g(\\hat{\\theta}_{n}) = g(\\theta) + g'(\\theta)(\\hat{\\theta}_{n} - \\theta) + \\frac{1}{2}g''(\\theta)(\\hat{\\theta}_{n} - \\theta)^{2} + R_{2} $$\n对于 $g(x) = \\ln x$，其二阶导数为 $g''(x) = -\\frac{1}{x^{2}}$。在 $x = \\theta$ 处求值得到 $g''(\\theta) = -\\frac{1}{\\theta^{2}}$。将导数代入展开式可得：\n$$ g(\\hat{\\theta}_{n}) \\approx \\ln(\\theta) + \\frac{1}{\\theta}(\\hat{\\theta}_{n} - \\theta) - \\frac{1}{2\\theta^{2}}(\\hat{\\theta}_{n} - \\theta)^{2} $$\n对两边取期望，并利用期望的线性性质：\n$$ \\mathbb{E}[g(\\hat{\\theta}_{n})] \\approx \\mathbb{E}\\left[\\ln(\\theta) + \\frac{1}{\\theta}(\\hat{\\theta}_{n} - \\theta) - \\frac{1}{2\\theta^{2}}(\\hat{\\theta}_{n} - \\theta)^{2}\\right] $$\n$$ \\mathbb{E}[g(\\hat{\\theta}_{n})] \\approx \\ln(\\theta) + \\frac{1}{\\theta}\\mathbb{E}[\\hat{\\theta}_{n} - \\theta] - \\frac{1}{2\\theta^{2}}\\mathbb{E}[(\\hat{\\theta}_{n} - \\theta)^{2}] $$\n我们计算期望项：\n- 第一项是常数：$\\mathbb{E}[\\ln(\\theta)] = \\ln(\\theta)$。\n- 第二项为零，因为 $\\hat{\\theta}_{n}$ 是 $\\theta$ 的无偏估计量：$\\mathbb{E}[\\hat{\\theta}_{n} - \\theta] = \\mathbb{E}[\\hat{\\theta}_{n}] - \\theta = \\theta - \\theta = 0$。\n- 第三项的期望是 $\\hat{\\theta}_{n}$ 方差的定义：$\\mathbb{E}[(\\hat{\\theta}_{n} - \\theta)^{2}] = \\mathrm{Var}(\\hat{\\theta}_{n}) = \\frac{\\sigma_{h}^{2}}{n}$。\n\n将这些结果代回期望的近似式中：\n$$ \\mathbb{E}[g(\\hat{\\theta}_{n})] \\approx \\ln(\\theta) - \\frac{1}{2\\theta^{2}}\\left(\\frac{\\sigma_{h}^{2}}{n}\\right) $$\n估计量的偏差定义为 $\\mathrm{Bias}(g(\\hat{\\theta}_{n})) = \\mathbb{E}[g(\\hat{\\theta}_{n})] - g(\\theta)$。因此，偏差的主阶项为：\n$$ \\mathrm{Bias}(g(\\hat{\\theta}_{n})) \\approx -\\frac{\\sigma_{h}^{2}}{2n\\theta^{2}} $$\n偏差与函数 $g$ 的二阶导数成正比，二阶导数代表其曲率。对于一般函数 $g$，偏差的主阶项是 $\\frac{1}{2}g''(\\theta)\\mathrm{Var}(\\hat{\\theta}_{n})$。因此，偏差的符号由曲率 $g''(\\theta)$ 的符号决定。对于自然对数函数，$g''(x) = -1/x^{2}$，在其定义域 $(0, \\infty)$ 内对所有 $x$ 都是严格为负的。由于 $\\theta  0$，在 $\\theta$ 处的曲率 $g''(\\theta) = -1/\\theta^{2}$ 是负的。考虑到 $\\sigma_{h}^{2}  0$ 且 $n \\ge 1$，整个偏差项是负的。\n这表明对数函数的凹性（负曲率）导致估计量 $\\ln(\\hat{\\theta}_{n})$ 系统地低估真实值 $\\ln(\\theta)$。这个结果是 Jensen 不等式的一个量化改进。对于凹函数 $g$，Jensen 不等式表明 $\\mathbb{E}[g(Z)] \\le g(\\mathbb{E}[Z])$。这里，取 $Z=\\hat{\\theta}_{n}$，我们得到 $\\mathbb{E}[\\ln(\\hat{\\theta}_{n})] \\le \\ln(\\mathbb{E}[\\hat{\\theta}_{n}]) = \\ln(\\theta)$。泰勒展开将此不等式量化到 $1/n$ 的主阶。", "answer": "$$\\boxed{\\frac{\\sigma_{h}^{2}}{n\\theta^{2}}}$$", "id": "3352081"}, {"introduction": "在掌握了德尔塔方法的基本原理后，我们将其应用到更复杂的实际统计场景中。本练习探讨了如何对马尔可夫链蒙特卡洛（MCMC）模拟的输出使用德尔塔方法，这在现代贝叶斯推断中极为常见[@problem_id:3352103]。你需要将德尔塔方法与马尔可夫链中心极限定理相结合，正确处理由于样本相关性而产生的渐近方差，从而体会该方法在高级随机模拟中的强大功能。", "problem": "考虑一个贝叶斯模型，其中正标量参数 $\\theta$ 的后验分布为形状参数 $\\alpha0$ 和率参数 $\\beta0$ 的 $\\operatorname{Gamma}(\\alpha,\\beta)$ 分布。一个马尔可夫链蒙特卡洛 (MCMC) 过程产生一个严平稳、遍历的马尔可夫链 $\\{\\theta_t\\}_{t=1}^{\\infty}$，其不变分布等于该后验分布，且后验均值为 $\\theta=\\mathbb{E}_{\\pi}[\\theta_t]$。定义后验均值的蒙特卡洛估计量为 $\\hat{\\theta}_n = \\frac{1}{n}\\sum_{t=1}^{n}\\theta_t$。假设马尔可夫链中心极限定理 (CLT) 对 $\\hat{\\theta}_n$ 成立，并且在平稳性条件下，$\\{\\theta_t\\}$ 的自相关函数是几何的，即对所有整数 $k\\geq 0$，\n$$\n\\rho_k = \\operatorname{Corr}(\\theta_t, \\theta_{t+k}) = c^k\n$$\n对于一个固定的常数 $c\\in(0,1)$。\n\n你的任务是分析重参数化估计量 $g(\\hat{\\theta}_n)=\\log(\\hat{\\theta}_n)$。仅使用以自协方差表示的马尔可夫链中心极限定理 (CLT) 的定义以及围绕后验均值的一阶泰勒展开，推导归一化误差 $\\sqrt{n}\\,\\big(g(\\hat{\\theta}_n)-g(\\theta)\\big)$ 的渐近方差常数，并将其表示为仅含 $\\alpha$ 和 $c$ 的单一闭式解析表达式。不要引入任何辅助的数值近似。请以解析表达式的形式提供最终答案。无需四舍五入，也不涉及单位。", "solution": "问题要求解对数变换后的后验均值蒙特卡洛估计量的归一化误差的渐近方差常数。让我们用随机变量 $\\Theta$ 表示感兴趣的参数。其后验分布为 $\\pi(\\Theta) = \\operatorname{Gamma}(\\alpha, \\beta)$，其中形状参数 $\\alpha  0$，率参数 $\\beta  0$。\n\n问题陈述后验均值用 $\\theta$ 表示。对于一个 $\\operatorname{Gamma}(\\alpha, \\beta)$ 分布，其均值为：\n$$\n\\theta = \\mathbb{E}[\\Theta] = \\frac{\\alpha}{\\beta}\n$$\n该分布的方差为：\n$$\n\\operatorname{Var}(\\Theta) = \\frac{\\alpha}{\\beta^2}\n$$\n给定一个平稳遍历的马尔可夫链 $\\{\\theta_t\\}_{t=1}^{\\infty}$，其不变分布是该后验分布。后验均值 $\\theta$ 的蒙特卡洛估计量为 $\\hat{\\theta}_n = \\frac{1}{n}\\sum_{t=1}^{n}\\theta_t$。\n\n问题假设马尔可夫链中心极限定理 (CLT) 对 $\\hat{\\theta}_n$ 成立。该定理指出，归一化误差的分布收敛于一个正态分布：\n$$\n\\sqrt{n}(\\hat{\\theta}_n - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2_{asy})\n$$\n其中 $\\xrightarrow{d}$ 表示依分布收敛，$\\sigma^2_{asy}$ 是渐近方差常数。任务是找到变换后的估计量 $g(\\hat{\\theta}_n)$（其中 $g(x) = \\log(x)$）对应的渐近方差常数。我们假设 $\\log(x)$ 表示自然对数 $\\ln(x)$，这是理论统计学和微积分中的标准惯例。\n\n首先，我们必须确定 $\\sigma^2_{asy}$。对于一个平稳马尔可夫链，渐近方差由所有自协方差之和给出：\n$$\n\\sigma^2_{asy} = \\sum_{k=-\\infty}^{\\infty} \\operatorname{Cov}(\\theta_t, \\theta_{t+k})\n$$\n令 $\\gamma_k = \\operatorname{Cov}(\\theta_t, \\theta_{t+k})$ 为滞后 $k$ 阶的自协方差。由于平稳性，$\\gamma_k = \\gamma_{-k}$。因此，该表达式可以写成：\n$$\n\\sigma^2_{asy} = \\gamma_0 + 2\\sum_{k=1}^{\\infty} \\gamma_k\n$$\n其中 $\\gamma_0 = \\operatorname{Var}(\\theta_t) = \\operatorname{Var}(\\Theta) = \\frac{\\alpha}{\\beta^2}$。自相关函数 $\\rho_k$ 定义为 $\\rho_k = \\frac{\\gamma_k}{\\gamma_0}$，所以 $\\gamma_k = \\rho_k \\gamma_0$。问题给出，对于一个常数 $c \\in (0,1)$ 和所有整数 $k \\geq 0$，有 $\\rho_k = c^k$。\n\n将这些代入 $\\sigma^2_{asy}$ 的表达式中：\n$$\n\\sigma^2_{asy} = \\gamma_0 + 2\\sum_{k=1}^{\\infty} (c^k \\gamma_0) = \\gamma_0 \\left(1 + 2\\sum_{k=1}^{\\infty} c^k\\right)\n$$\n求和项是一个无穷几何级数：$\\sum_{k=1}^{\\infty} c^k = \\frac{c}{1-c}$，因为 $|c|  1$。\n将此结果代回 $\\sigma^2_{asy}$ 的表达式中：\n$$\n\\sigma^2_{asy} = \\gamma_0 \\left(1 + 2\\frac{c}{1-c}\\right) = \\frac{\\alpha}{\\beta^2} \\left(\\frac{1-c+2c}{1-c}\\right) = \\frac{\\alpha}{\\beta^2} \\left(\\frac{1+c}{1-c}\\right)\n$$\n现在，我们分析变换后的估计量 $g(\\hat{\\theta}_n) = \\ln(\\hat{\\theta}_n)$。问题要求使用 $g(\\hat{\\theta}_n)$ 在真实后验均值 $\\theta$ 附近的一阶泰勒展开。\n$$\ng(\\hat{\\theta}_n) \\approx g(\\theta) + g'(\\theta)(\\hat{\\theta}_n - \\theta)\n$$\n其中 $g'(\\theta)$ 是 $g(x)$ 在 $x=\\theta$ 处求得的导数。\n重新整理各项，我们得到：\n$$\ng(\\hat{\\theta}_n) - g(\\theta) \\approx g'(\\theta)(\\hat{\\theta}_n - \\theta)\n$$\n两边乘以 $\\sqrt{n}$：\n$$\n\\sqrt{n}\\big(g(\\hat{\\theta}_n) - g(\\theta)\\big) \\approx g'(\\theta) \\sqrt{n}(\\hat{\\theta}_n - \\theta)\n$$\n这个近似，被称为德尔塔方法 (Delta Method)，意味着左侧的渐近分布与右侧的分布相同。由于 $\\sqrt{n}(\\hat{\\theta}_n - \\theta)$ 收敛于一个均值为 $0$、方差为 $\\sigma^2_{asy}$ 的正态分布，因此右侧收敛于一个均值为 $g'(\\theta) \\times 0 = 0$、方差为 $(g'(\\theta))^2 \\sigma^2_{asy}$ 的正态分布。\n\n因此，归一化误差 $\\sqrt{n}(g(\\hat{\\theta}_n) - g(\\theta))$ 的渐近方差常数（我们称之为 $V_g$）为：\n$$\nV_g = (g'(\\theta))^2 \\sigma^2_{asy}\n$$\n函数是 $g(x) = \\ln(x)$，所以其导数是 $g'(x) = \\frac{1}{x}$。我们在后验均值 $\\theta = \\frac{\\alpha}{\\beta}$ 处计算该导数的值：\n$$\ng'(\\theta) = \\frac{1}{\\theta} = \\frac{1}{\\alpha/\\beta} = \\frac{\\beta}{\\alpha}\n$$\n现在我们可以将 $g'(\\theta)$ 和 $\\sigma^2_{asy}$ 的表达式代入 $V_g$ 的公式中：\n$$\nV_g = \\left(\\frac{\\beta}{\\alpha}\\right)^2 \\left( \\frac{\\alpha}{\\beta^2} \\left(\\frac{1+c}{1-c}\\right) \\right)\n$$\n$$\nV_g = \\frac{\\beta^2}{\\alpha^2} \\cdot \\frac{\\alpha}{\\beta^2} \\cdot \\frac{1+c}{1-c}\n$$\n分子和分母中的项 $\\beta^2$ 相互抵消。类似地，一个因子 $\\alpha$ 也被抵消。\n$$\nV_g = \\frac{1}{\\alpha} \\cdot \\frac{1+c}{1-c}\n$$\n这就是渐近方差常数的最终表达式，正如题目所要求的，仅用 $\\alpha$ 和 $c$ 表示。", "answer": "$$\n\\boxed{\\frac{1}{\\alpha}\\frac{1+c}{1-c}}\n$$", "id": "3352103"}, {"introduction": "在许多科学和工程应用中，我们感兴趣的函数 $g$ 可能是一个没有简单解析式的“黑箱”，例如一个复杂计算机模拟的输出。本练习将直面这一实际挑战，要求你实现一个完全数值化的方法来应用德尔塔方法[@problem_id:3352079]。你将学习如何使用蒙特卡洛积分来评估函数值，并结合有限差分来估计其雅可比矩阵，最终量化这种数值近似对最终方差估计带来的误差。", "problem": "设 $\\theta \\in \\mathbb{R}^2$ 表示一个参数向量，其坐标为 $\\theta = (\\mu,\\sigma)$，其中 $\\mu \\in \\mathbb{R}$ 且 $\\sigma \\in (0,\\infty)$。考虑由下式定义的映射 $g:\\mathbb{R}^2 \\to \\mathbb{R}^2$\n$$\ng(\\theta)\n=\n\\begin{bmatrix}\n\\mathbb{E}_{\\theta}[X^3] \\\\\n\\mathbb{E}_{\\theta}[\\sin(X)]\n\\end{bmatrix},\n\\quad\nX \\sim \\mathcal{N}(\\mu,\\sigma^2),\n$$\n其中正弦函数的角度以弧度为单位。\n\n假设 $\\hat{\\theta}$ 是 $\\theta$ 的一个估计量，对于大样本量，其分布近似为均值为 $\\theta$、协方差矩阵为 $V_{\\theta}$（一个给定的、固定的 $2 \\times 2$ 半正定矩阵）的多元正态分布。\n\n您将实现一个完全数值化的蒙特卡洛程序，使用有限差分法来估计雅可比矩阵 $J_g(\\theta)$（即 $g$ 在 $\\theta$ 处的 $2 \\times 2$ 一阶偏导数矩阵），其中映射 $g$ 本身通过蒙特卡洛积分进行评估。然后，您将使用这个估计的雅可比矩阵进行德尔塔方法（delta method）的方差计算，并量化雅可比矩阵近似误差对所得德尔塔方法协方差估计的影响。\n\n使用的基本原理：\n- 映射 $g(\\theta)$ 由正态定律下的期望定义，并根据大数定律，可以通过样本均值进行蒙特卡洛近似。\n- 雅可比矩阵 $J_g(\\theta)$ 可以通过步长为 $h$ 的对称有限差分进行近似。\n- 德尔塔方法源于 $g(\\hat{\\theta})$ 在 $\\theta$ 附近的一阶泰勒展开以及 $\\hat{\\theta}$ 的渐近正态性。\n\n您的任务是：\n1.  为任意 $\\theta = (\\mu,\\sigma)$ 实现一个 $g(\\theta)$ 的蒙特卡洛估计器，方法如下。抽取 $M$ 个独立标准正态样本 $Z_1,\\dots,Z_M \\sim \\mathcal{N}(0,1)$，并为 $i \\in \\{1,\\dots,M\\}$ 定义 $X_i(\\mu,\\sigma) = \\mu + \\sigma Z_i$。通过样本均值近似 $g(\\theta)$：\n$$\n\\widehat{g}_1(\\theta) = \\frac{1}{M}\\sum_{i=1}^M X_i(\\mu,\\sigma)^3, \n\\quad\n\\widehat{g}_2(\\theta) = \\frac{1}{M}\\sum_{i=1}^M \\sin\\bigl(X_i(\\mu,\\sigma)\\bigr).\n$$\n2.  使用相同的底层标准正态抽样（共同随机数）以减少方差，为 $J_g(\\theta)$ 的每一列实现对称有限差分近似。对于 $\\mu$ 方向，使用\n$$\n\\widehat{J}_{\\cdot,1}(\\theta;h,M)\n=\n\\frac{\\widehat{g}(\\mu+h,\\sigma) - \\widehat{g}(\\mu-h,\\sigma)}{2h},\n$$\n对于 $\\sigma$ 方向，使用\n$$\n\\widehat{J}_{\\cdot,2}(\\theta;h,M)\n=\n\\frac{\\widehat{g}(\\mu,\\sigma+h) - \\widehat{g}(\\mu,\\sigma-h)}{2h}.\n$$\n使用相同的 $Z_1,\\dots,Z_M$ 来计算所有四个评估值 $\\widehat{g}(\\mu\\pm h,\\sigma)$ 和 $\\widehat{g}(\\mu,\\sigma\\pm h)$。\n3.  给定 $V_{\\theta}$，构建德尔塔方法协方差估计\n$$\n\\widehat{\\Sigma}_g(\\theta;h,M) = \\widehat{J}_g(\\theta;h,M)\\, V_{\\theta}\\, \\widehat{J}_g(\\theta;h,M)^{\\top}.\n$$\n4.  为了进行评估，推导解析雅可比矩阵 $J_g(\\theta)$ 和相应的解析德尔塔方法协方差\n$$\n\\Sigma_g(\\theta) = J_g(\\theta)\\, V_{\\theta}\\, J_g(\\theta)^{\\top}.\n$$\n5.  对于下面指定的每个测试用例，计算相对弗罗贝尼乌斯（Frobenius）误差\n$$\n\\mathrm{Err}(\\theta,h,M,V_{\\theta})\n=\n\\frac{\\left\\| \\widehat{\\Sigma}_g(\\theta;h,M) - \\Sigma_g(\\theta) \\right\\|_F}{\\left\\| \\Sigma_g(\\theta) \\right\\|_F}.\n$$\n\n角度单位要求：所有三角函数参数均以弧度为单位。\n\n测试套件：\n- 情况 1：$\\mu = 0.5$，$\\sigma = 1.2$，\n$$\nV_{\\theta} =\n\\begin{bmatrix}\n0.04  0.01 \\\\\n0.01  0.09\n\\end{bmatrix},\n$$\n$h = 10^{-3}$，$M = 50000$，种子 $= 12345$。\n- 情况 2：$\\mu = 0.5$，$\\sigma = 1.2$，\n$$\nV_{\\theta} =\n\\begin{bmatrix}\n0.04  0.01 \\\\\n0.01  0.09\n\\end{bmatrix},\n$$\n$h = 10^{-1}$，$M = 50000$，种子 $= 67890$。\n- 情况 3：$\\mu = 0.5$，$\\sigma = 1.2$，\n$$\nV_{\\theta} =\n\\begin{bmatrix}\n0.04  0.01 \\\\\n0.01  0.09\n\\end{bmatrix},\n$$\n$h = 10^{-5}$，$M = 2000$，种子 $= 13579$。\n- 情况 4：$\\mu = 1.5$，$\\sigma = 0.5$，\n$$\nV_{\\theta} =\n\\begin{bmatrix}\n0.01  -0.003 \\\\\n-0.003  0.02\n\\end{bmatrix},\n$$\n$h = 5 \\times 10^{-4}$，$M = 200000$，种子 $= 24680$。\n\n要求的最终输出格式：您的程序应生成单行输出，其中包含四个测试用例的四个相对误差，形式为逗号分隔的列表并用方括号括起来，例如\n$$\n[\\mathrm{Err}_1,\\mathrm{Err}_2,\\mathrm{Err}_3,\\mathrm{Err}_4].\n$$\n每个条目必须是一个实数。", "solution": "该问题已经过验证，被认为是可靠的。这是一个定义明确、具有科学依据的计算统计学问题，没有矛盾、歧义或伪科学论断。所有用于获得唯一、可复现解的必要数据和定义均已提供。\n\n主要目标是量化当基础的雅可比矩阵通过数值方法估计时，德尔塔方法协方差矩阵近似的误差。这涉及一个双层蒙特卡洛过程：函数 $g(\\theta)$ 本身是一个通过蒙特卡洛估计的期望，然后其雅可比矩阵 $J_g(\\theta)$ 在此蒙特卡洛估计器的基础上使用有限差分进行近似。误差分析将此完全数值化的结果与解析基准进行比较。\n\n求解过程遵循以下步骤：\n1.  推导映射 $g(\\theta)$ 及其雅可比矩阵 $J_g(\\theta)$ 的解析表达式。\n2.  按照规定实现 $g(\\theta)$ 和 $J_g(\\theta)$ 的数值估计器。\n3.  计算解析和估计的德尔塔方法协方差矩阵。\n4.  为每个测试用例计算指定的相对弗罗贝尼乌斯误差。\n\n**1. 解析推导**\n\n设 $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$。参数向量为 $\\theta = (\\mu, \\sigma)$。映射为 $g(\\theta) = [\\mathbb{E}_{\\theta}[X^3], \\mathbb{E}_{\\theta}[\\sin(X)]]^{\\top}$。\n\n第一个分量，$g_1(\\theta) = \\mathbb{E}[X^3]$：\n正态分布的三阶中心矩为 $0$。原点矩 $m_k = \\mathbb{E}[X^k]$ 和中心矩之间的关系由中心化变量 $X-\\mu$ 给出。更直接的方法是使用矩生成函数 $M_X(t) = \\exp(\\mu t + \\frac{1}{2}\\sigma^2 t^2)$ 或展开 $X=\\mu+\\sigma Z$，其中 $Z \\sim \\mathcal{N}(0,1)$。\n使用后一种方法，$\\mathbb{E}[X^3] = \\mathbb{E}[(\\mu + \\sigma Z)^3] = \\mathbb{E}[\\mu^3 + 3\\mu^2\\sigma Z + 3\\mu\\sigma^2 Z^2 + \\sigma^3 Z^3]$。\n根据期望的线性性，并使用标准正态分布的矩 $\\mathbb{E}[Z]=0$，$\\mathbb{E}[Z^2]=1$ 和 $\\mathbb{E}[Z^3]=0$，我们得到：\n$g_1(\\mu, \\sigma) = \\mu^3 + 3\\mu\\sigma^2 \\mathbb{E}[Z^2] = \\mu^3 + 3\\mu\\sigma^2$。\n\n第二个分量，$g_2(\\theta) = \\mathbb{E}[\\sin(X)]$：\n我们通过考虑 $X$ 的特征函数来计算这个值，$\\phi_X(t) = \\mathbb{E}[e^{itX}] = \\exp(i\\mu t - \\frac{1}{2}\\sigma^2 t^2)$。\n使用欧拉公式 $\\sin(X) = \\mathrm{Im}(e^{iX})$ 和期望的线性性：\n$\\mathbb{E}[\\sin(X)] = \\mathbb{E}[\\mathrm{Im}(e^{iX})] = \\mathrm{Im}(\\mathbb{E}[e^{iX}])$。\n$\\mathbb{E}[e^{iX}]$ 是 $\\phi_X(1) = \\exp(i\\mu - \\frac{1}{2}\\sigma^2) = e^{-\\sigma^2/2}e^{i\\mu} = e^{-\\sigma^2/2}(\\cos(\\mu) + i\\sin(\\mu))$。\n虚部是：\n$g_2(\\mu, \\sigma) = \\sin(\\mu) e^{-\\sigma^2/2}$。\n\n解析映射为：\n$g(\\theta) = \\begin{bmatrix} \\mu^3 + 3\\mu\\sigma^2 \\\\ \\sin(\\mu) e^{-\\sigma^2/2} \\end{bmatrix}$。\n\n解析雅可比矩阵 $J_g(\\theta)$ 是 $g$ 关于 $\\mu$ 和 $\\sigma$ 的一阶偏导数矩阵：\n$J_g(\\theta) = \\begin{bmatrix} \\frac{\\partial g_1}{\\partial \\mu}  \\frac{\\partial g_1}{\\partial \\sigma} \\\\ \\frac{\\partial g_2}{\\partial \\mu}  \\frac{\\partial g_2}{\\partial \\sigma} \\end{bmatrix}$。\n偏导数是：\n$\\frac{\\partial g_1}{\\partial \\mu} = 3\\mu^2 + 3\\sigma^2$\n$\\frac{\\partial g_1}{\\partial \\sigma} = 6\\mu\\sigma$\n$\\frac{\\partial g_2}{\\partial \\mu} = \\cos(\\mu) e^{-\\sigma^2/2}$\n$\\frac{\\partial g_2}{\\partial \\sigma} = \\sin(\\mu) \\cdot e^{-\\sigma^2/2} \\cdot (-\\sigma) = -\\sigma \\sin(\\mu) e^{-\\sigma^2/2}$\n\n因此，解析雅可比矩阵是：\n$$\nJ_g(\\theta) = \\begin{bmatrix} 3\\mu^2 + 3\\sigma^2  6\\mu\\sigma \\\\ \\cos(\\mu) e^{-\\sigma^2/2}  -\\sigma \\sin(\\mu) e^{-\\sigma^2/2} \\end{bmatrix}\n$$\n\n**2. 数值估计过程**\n\n实现将遵循指定的数值方法。\n\n一个函数 `g_hat(mu, sigma, Z)`，对于给定的参数 $\\mu, \\sigma$ 和一个预先生成的包含 $M$ 个标准正态样本的数组 $Z = (Z_1, \\dots, Z_M)$，估计 $g(\\theta)$。它计算 $X_i = \\mu + \\sigma Z_i$，然后返回 $X_i^3$ 和 $\\sin(X_i)$ 的样本均值。\n\n第二个函数 `J_hat(mu, sigma, h, Z)`，估计雅可比矩阵 $J_g(\\theta)$。它使用提供的对称有限差分公式。关键是，它通过将相同的样本数组 `Z` 传递给 `g_hat` 的每次调用来采用共同随机数（CRN）技术：\n$$\n\\widehat{J}_{\\cdot,1} = \\frac{\\text{g\\_hat}(\\mu+h, \\sigma, Z) - \\text{g\\_hat}(\\mu-h, \\sigma, Z)}{2h}\n$$\n$$\n\\widehat{J}_{\\cdot,2} = \\frac{\\text{g\\_hat}(\\mu, \\sigma+h, Z) - \\text{g\\_hat}(\\mu, \\sigma-h, Z)}{2h}\n$$\n然后将这两个列向量组合成 $2 \\times 2$ 的估计雅可比矩阵 $\\widehat{J}_g(\\theta; h, M)$。\n\n**3. 德尔塔方法与误差计算**\n\n对于每个测试用例，我们计算两个协方差矩阵：\n解析德尔塔方法协方差：$\\Sigma_g(\\theta) = J_g(\\theta)\\, V_{\\theta}\\, J_g(\\theta)^{\\top}$，使用解析推导的 $J_g(\\theta)$。\n估计的德尔塔方法协方差：$\\widehat{\\Sigma}_g(\\theta;h,M) = \\widehat{J}_g(\\theta;h,M)\\, V_{\\theta}\\, \\widehat{J}_g(\\theta;h,M)^{\\top}$，使用数值估计的 $\\widehat{J}_g$。\n\n最终误差计算为这两个矩阵之差的相对弗罗贝尼乌斯范数：\n$$\n\\mathrm{Err} = \\frac{\\left\\| \\widehat{\\Sigma}_g(\\theta;h,M) - \\Sigma_g(\\theta) \\right\\|_F}{\\left\\| \\Sigma_g(\\theta) \\right\\|_F}\n$$\n其中 $\\|A\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2}$。\n\n**4. 测试用例的执行**\n\n一个循环遍历四个测试用例。在每次迭代中：\n1.  提取参数 $(\\mu, \\sigma, V_{\\theta}, h, M, \\text{seed})$。\n2.  为随机数生成器设置种子。\n3.  抽取 $M$ 个标准正态样本。\n4.  如上所述计算 $\\Sigma_g(\\theta)$ 和 $\\widehat{\\Sigma}_g(\\theta;h,M)$。\n5.  计算并存储相对误差 $\\mathrm{Err}$。\n\n最后，将四个误差值的列表按指定格式进行格式化和打印。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef g_hat_mc(mu, sigma, z_samples):\n    \"\"\"\n    Computes the Monte Carlo estimate of the mapping g(theta).\n    \n    Args:\n        mu (float): The mean parameter.\n        sigma (float): The standard deviation parameter.\n        z_samples (np.ndarray): An array of standard normal random samples.\n    \n    Returns:\n        np.ndarray: A 2-element array containing the estimates of E[X^3] and E[sin(X)].\n    \"\"\"\n    x_samples = mu + sigma * z_samples\n    g1_hat = np.mean(x_samples**3)\n    g2_hat = np.mean(np.sin(x_samples))  # np.sin uses radians\n    return np.array([g1_hat, g2_hat])\n\ndef J_g_hat_mc(mu, sigma, h, z_samples):\n    \"\"\"\n    Computes the finite-difference estimate of the Jacobian of g(theta)\n    using common random numbers.\n    \n    Args:\n        mu (float): The mean parameter.\n        sigma (float): The standard deviation parameter.\n        h (float): The finite difference step size.\n        z_samples (np.ndarray): An array of standard normal random samples.\n    \n    Returns:\n        np.ndarray: A 2x2 matrix representing the estimated Jacobian.\n    \"\"\"\n    # First column: partial derivatives with respect to mu\n    g_plus_mu = g_hat_mc(mu + h, sigma, z_samples)\n    g_minus_mu = g_hat_mc(mu - h, sigma, z_samples)\n    J_col1 = (g_plus_mu - g_minus_mu) / (2 * h)\n    \n    # Second column: partial derivatives with respect to sigma\n    g_plus_sigma = g_hat_mc(mu, sigma + h, z_samples)\n    g_minus_sigma = g_hat_mc(mu, sigma - h, z_samples)\n    J_col2 = (g_plus_sigma - g_minus_sigma) / (2 * h)\n    \n    # Assemble the Jacobian matrix from its columns\n    return np.stack([J_col1, J_col2], axis=1)\n\ndef J_g_analytic(mu, sigma):\n    \"\"\"\n    Computes the analytic Jacobian of g(theta).\n    \n    Args:\n        mu (float): The mean parameter.\n        sigma (float): The standard deviation parameter.\n    \n    Returns:\n        np.ndarray: A 2x2 matrix representing the analytic Jacobian.\n    \"\"\"\n    J11 = 3 * mu**2 + 3 * sigma**2\n    J12 = 6 * mu * sigma\n    \n    exp_term = np.exp(-0.5 * sigma**2)\n    J21 = np.cos(mu) * exp_term\n    J22 = -sigma * np.sin(mu) * exp_term\n    \n    return np.array([[J11, J12], [J21, J22]])\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and compute the errors.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Base case\n        {'mu': 0.5, 'sigma': 1.2, 'V_theta': [[0.04, 0.01], [0.01, 0.09]], 'h': 1e-3, 'M': 50000, 'seed': 12345},\n        # Case 2: Large finite-difference step h\n        {'mu': 0.5, 'sigma': 1.2, 'V_theta': [[0.04, 0.01], [0.01, 0.09]], 'h': 1e-1, 'M': 50000, 'seed': 67890},\n        # Case 3: Small Monte Carlo sample size M\n        {'mu': 0.5, 'sigma': 1.2, 'V_theta': [[0.04, 0.01], [0.01, 0.09]], 'h': 1e-5, 'M': 2000, 'seed': 13579},\n        # Case 4: Different parameter set, large M\n        {'mu': 1.5, 'sigma': 0.5, 'V_theta': [[0.01, -0.003], [-0.003, 0.02]], 'h': 5e-4, 'M': 200000, 'seed': 24680},\n    ]\n\n    results = []\n    for case in test_cases:\n        mu = case['mu']\n        sigma = case['sigma']\n        V_theta = np.array(case['V_theta'])\n        h = case['h']\n        M = case['M']\n        seed = case['seed']\n\n        # --- Analytic \"Ground Truth\" Calculation ---\n        J_analytic = J_g_analytic(mu, sigma)\n        Sigma_g_analytic = J_analytic @ V_theta @ J_analytic.T\n        \n        # --- Numerical Estimation ---\n        # 1. Set up the random number generator and draw samples\n        rng = np.random.default_rng(seed)\n        z_samples = rng.standard_normal(M)\n        \n        # 2. Estimate the Jacobian using Monte Carlo and finite differences\n        J_hat = J_g_hat_mc(mu, sigma, h, z_samples)\n        \n        # 3. Compute the estimated delta method covariance matrix\n        Sigma_g_hat = J_hat @ V_theta @ J_hat.T\n\n        # --- Error Quantification ---\n        # Compute the relative Frobenius error\n        numerator = np.linalg.norm(Sigma_g_hat - Sigma_g_analytic, 'fro')\n        denominator = np.linalg.norm(Sigma_g_analytic, 'fro')\n        \n        # Denominator should not be zero for the given test cases\n        if denominator == 0:\n            # This case is unlikely here but is good practice to handle.\n            # If numerator is also 0, error is 0. Otherwise, error is infinite.\n            relative_error = 0.0 if numerator == 0.0 else np.inf\n        else:\n            relative_error = numerator / denominator\n        \n        results.append(relative_error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3352079"}]}