{"hands_on_practices": [{"introduction": "为了掌握约束反演问题，我们从基石——直接求解 Karush-Kuhn-Tucker (KKT) 系统——开始。第一个练习 [@problem_id:3395254] 要求你将拉格朗日乘子法应用于一个经典的数据同化问题。通过推导最优性条件并找到乘子的解析解，你将为理解约束如何改变标准二次目标函数的解奠定坚实的基础。", "problem": "考虑一个在数据同化框架下建立的线性逆问题，其中状态向量 $m \\in \\mathbb{R}^{n}$ 是通过线性正演算子 $F \\in \\mathbb{R}^{p \\times n}$ 从观测值 $d \\in \\mathbb{R}^{p}$ 中估计出来的。假设存在一个正定的数据精度矩阵 $W \\in \\mathbb{R}^{p \\times p}$ 和一个正定的背景协方差矩阵 $B \\in \\mathbb{R}^{n \\times n}$，以及背景状态 $m_{b} \\in \\mathbb{R}^{n}$。在高斯假设下，$m$ 的最大后验 (MAP) 估计量是通过最小化以下二次目标函数得到的\n$$\nJ(m) = \\frac{1}{2} (F m - d)^{T} W (F m - d) + \\frac{1}{2} (m - m_{b})^{T} B^{-1} (m - m_{b}),\n$$\n该问题受线性不变约束 $s^{T} m = c$ 的限制，其中 $s \\in \\mathbb{R}^{n}$ 是一个给定的非零向量，$c \\in \\mathbb{R}$ 是一个预设的标量。\n\n从约束优化的核心定义以及针对等式约束问题的 Karush–Kuhn–Tucker (KKT) 条件出发，推导出一阶最优性系统，该系统刻画了约束 MAP 解的特征。然后，以闭式形式解析求解与约束 $s^{T} m = c$ 相关联的拉格朗日乘子，并用 $F$、$W$、$B$、$d$、$m_{b}$、$s$ 和 $c$ 表示。清晰地陈述保证该乘子唯一性所必需的任何假设。最终答案必须是该乘子的单个闭式解析表达式。无需四舍五入，也不涉及物理单位。", "solution": "所述问题是变分数据同化背景下出现的经典约束二次优化问题。我们首先将验证问题的陈述。\n\n### 问题验证\n\n**第 1 步：提取已知条件**\n- 待估计的状态向量：$m \\in \\mathbb{R}^{n}$。\n- 观测向量：$d \\in \\mathbb{R}^{p}$。\n- 线性正演算子：$F \\in \\mathbb{R}^{p \\times n}$。\n- 数据精度矩阵：$W \\in \\mathbb{R}^{p \\times p}$，正定。\n- 背景协方差矩阵：$B \\in \\mathbb{R}^{n \\times n}$，正定。\n- 背景状态向量：$m_{b} \\in \\mathbb{R}^{n}$。\n- 目标函数：$J(m) = \\frac{1}{2} (F m - d)^{T} W (F m - d) + \\frac{1}{2} (m - m_{b})^{T} B^{-1} (m - m_{b})$。\n- 线性约束：$s^{T} m = c$，其中 $s \\in \\mathbb{R}^{n}$ 是一个给定的非零向量，$c \\in \\mathbb{R}$ 是一个预设的标量。\n\n**第 2 步：使用提取的已知条件进行验证**\n- **科学依据：** 该问题是在高斯假设下，贝叶斯推断中寻找最大后验 (MAP) 估计的标准公式，在地球科学中通常称为 3D-Var 或 4D-Var。目标函数 $J(m)$ 与后验概率密度的负对数成正比。这是逆问题和数据同化领域中一个基础且成熟的方法。\n- **适定性：** 目标函数 $J(m)$ 是关于 $m$ 的二次函数。其 Hessian 矩阵为 $\\nabla_m^2 J(m) = F^T W F + B^{-1}$。由于 $B$ 是正定的，其逆矩阵 $B^{-1}$ 也是正定的。矩阵 $W$ 是正定的，这意味着对于任何向量 $v \\in \\mathbb{R}^n$，二次型 $v^T (F^T W F) v = (Fv)^T W (Fv) \\ge 0$，因此 $F^T W F$ 是半正定的。一个正定矩阵 ($B^{-1}$) 和一个半正定矩阵 ($F^T W F$) 的和是一个正定矩阵。因此，$J(m)$ 的 Hessian 矩阵是正定的，这使得 $J(m)$ 成为一个严格凸函数。约束 $s^T m = c$ 定义了一个凸集（一个仿射超平面）。在一个凸集上最小化一个严格凸函数会得到唯一的解。因此，该问题是适定的。\n- **客观性：** 该问题使用来自线性代数和优化理论的精确、无歧义的数学语言和标准符号来表述。\n- **完整性与一致性：** 所有必要的组成部分（变量、矩阵、它们的性质、目标函数和约束）都得到了明确的定义。设置中没有矛盾之处。\n\n**第 3 步：结论与行动**\n该问题是有效的，因为它具有科学依据、适定、客观且自洽。我们可以继续进行求解。\n\n### 求解推导\n\n问题在于找到向量 $m$，使得目标函数 $J(m)$ 在等式约束 $g(m) = s^T m - c = 0$ 下最小化。这个约束优化问题可以使用拉格朗日乘子法来解决。\n\n我们定义拉格朗日函数 $\\mathcal{L}(m, \\lambda)$，它包含了约束条件，其中 $\\lambda \\in \\mathbb{R}$ 是拉格朗日乘子：\n$$\n\\mathcal{L}(m, \\lambda) = J(m) + \\lambda (s^T m - c)\n$$\n代入 $J(m)$ 的表达式：\n$$\n\\mathcal{L}(m, \\lambda) = \\frac{1}{2} (F m - d)^{T} W (F m - d) + \\frac{1}{2} (m - m_{b})^{T} B^{-1} (m - m_{b}) + \\lambda (s^{T} m - c)\n$$\n一阶最优性必要条件（对于等式约束的 Karush-Kuhn-Tucker 条件）指出，在最优点 $(m^*, \\lambda^*)$，拉格朗日函数相对于 $m$ 和 $\\lambda$ 的梯度必须为零。\n\n第一个条件是关于 $m$ 的驻定性条件：$\\nabla_m \\mathcal{L}(m, \\lambda) = 0$。我们逐项计算梯度：\n$$\n\\nabla_m \\left( \\frac{1}{2} (F m - d)^{T} W (F m - d) \\right) = F^T W (Fm - d)\n$$\n$$\n\\nabla_m \\left( \\frac{1}{2} (m - m_{b})^{T} B^{-1} (m - m_{b}) \\right) = B^{-1} (m - m_b)\n$$\n$$\n\\nabla_m \\left( \\lambda (s^T m - c) \\right) = \\lambda s\n$$\n结合这些项，驻定性条件为：\n$$\n\\nabla_m \\mathcal{L}(m, \\lambda) = F^T W (Fm - d) + B^{-1} (m - m_b) + \\lambda s = 0\n$$\n第二个条件，$\\nabla_\\lambda \\mathcal{L}(m, \\lambda) = 0$，就是恢复了原始的约束条件：\n$$\ns^T m - c = 0\n$$\n这组方程构成了约束 MAP 解 $(m, \\lambda)$ 的一阶最优性系统：\n1. $(F^T W F + B^{-1}) m + \\lambda s = F^T W d + B^{-1} m_b$\n2. $s^T m = c$\n\n为了求解拉格朗日乘子 $\\lambda$，我们首先从第一个方程中解出 $m$。我们定义 Hessian 矩阵 $H = F^T W F + B^{-1}$ 和向量 $g = F^T W d + B^{-1} m_b$。该系统变为：\n1. $H m + \\lambda s = g$\n2. $s^T m = c$\n\n如在验证部分所证，矩阵 $H$ 是正定的，因此是可逆的。从方程 (1)，我们可以用 $\\lambda$ 来表示 $m$：\n$$\nH m = g - \\lambda s\n$$\n$$\nm = H^{-1} (g - \\lambda s)\n$$\n现在，将这个关于 $m$ 的表达式代入约束方程 (2)：\n$$\ns^T \\left( H^{-1} (g - \\lambda s) \\right) = c\n$$\n展开表达式：\n$$\ns^T H^{-1} g - \\lambda s^T H^{-1} s = c\n$$\n现在我们可以分离并求解 $\\lambda$。整理各项得：\n$$\n\\lambda (s^T H^{-1} s) = s^T H^{-1} g - c\n$$\n当且仅当标量系数 $s^T H^{-1} s \\neq 0$ 时，$\\lambda$ 存在唯一解。由于 $H$ 是正定的，其逆矩阵 $H^{-1}$ 也是正定的。问题陈述中指出 $s$ 是一个非零向量。对于任何正定矩阵 $A$ 和任何非零向量 $x$，二次型 $x^T A x$ 恒为正。因此，$s^T H^{-1} s  0$，这保证了乘子 $\\lambda$ 的唯一性。\n$$\n\\lambda = \\frac{s^T H^{-1} g - c}{s^T H^{-1} s}\n$$\n最后，我们将 $H$ 和 $g$ 的定义代回到这个表达式中，从而得到用原始问题数据表示的拉格朗日乘子的闭式解：\n$$\n\\lambda = \\frac{s^{T} (F^{T} W F + B^{-1})^{-1} (F^{T} W d + B^{-1} m_{b}) - c}{s^{T} (F^{T} W F + B^{-1})^{-1} s}\n$$\n这就是与该约束相关联的拉格朗日乘子的解析表达式。", "answer": "$$\n\\boxed{\\frac{s^{T} (F^{T} W F + B^{-1})^{-1} (F^{T} W d + B^{-1} m_{b}) - c}{s^{T} (F^{T} W F + B^{-1})^{-1} s}}\n$$", "id": "3395254"}, {"introduction": "虽然拉格朗日乘子为“硬”约束提供了精确解，但许多实用算法通过惩罚方法使用“软”约束。本练习 [@problem_id:3395335] 旨在探索这两种视角之间的根本联系。你将实现精确的 KKT 解法和惩罚方法，并数值验证当惩罚参数增大时，惩罚解会收敛到精确的约束解，从而将理论与常见的数值实践联系起来。", "problem": "考虑一个带等式约束的线性反演问题。设 $A \\in \\mathbb{R}^{m \\times n}$，$L \\in \\mathbb{R}^{p \\times n}$，$B \\in \\mathbb{R}^{q \\times n}$，$d \\in \\mathbb{R}^{m}$，$m_{0} \\in \\mathbb{R}^{n}$ 及 $b \\in \\mathbb{R}^{q}$。任务是通过最小化一个受线性等式约束的 Tikhonov 正则化最小二乘目标函数，来恢复一个未知的参数向量 $m \\in \\mathbb{R}^{n}$。您必须从基本原理出发：拉格朗日乘子法和针对等式约束问题的 Karush–Kuhn–Tucker (KKT) 条件，以及最小二乘的标准正规方程。不得假设任何无法从这些基础推导出的公式。\n\n对于下述每个测试用例，您的程序必须：\n- 通过构建正则化最小二乘目标函数在约束 $B m = b$ 下的 KKT 最优性系统，推导并实现使用拉格朗日乘子法的精确等式约束解。\n- 推导并实现对应的罚函数法无约束问题，即通过二次惩罚项 $\\frac{1}{2}\\,\\mu^{2}\\,\\lVert B m - b \\rVert_{2}^{2}$ 增强目标函数，并针对一系列递增的惩罚系数 $\\mu$ 求解。\n- 对于每个测试，验证随着 $\\mu$ 的增加，以下两个性质成立：\n  1. 欧几里得距离 $\\lVert m_{\\mu} - m_{\\mathrm{KKT}} \\rVert_{2}$ 在一个小的数值容差内单调非增。\n  2. 约束残差 $\\lVert B m_{\\mu} - b \\rVert_{2}$ 在一个小的数值容差内单调非增。\n- 此外，验证在最大惩罚系数下，罚函数解接近 KKT 解，且约束残差很小，两者均在指定的容差范围内。\n\n不涉及物理单位或角度单位。所有计算均在实数算术下进行。\n\n必须为惩罚序列 $\\{\\mu\\} = \\{\\,1,\\,10,\\,100,\\,1000,\\,100000\\,\\}$ 计算罚函数解。\n\n对于每个测试，您的程序必须返回一个布尔值，指示上述所有检查是否通过。\n\n提供以下四个测试用例。在每个用例中，根据给定的真实模型 $m_{\\mathrm{true}}$ 和一个加性噪声向量 $n$ 计算数据向量 $d$，具体为 $d = A\\,m_{\\mathrm{true}} + n$。所有向量和矩阵都明确给出。\n\n测试用例 1 (超定数据项，单位矩阵正则化，中等正则化权重):\n- 维度：$m = 7$，$n = 5$，$p = 5$，$q = 2$。\n- 矩阵\n$$\nA^{(1)} = \\begin{bmatrix}\n1  2  0  0  0.5 \\\\\n0  1  1  0  0 \\\\\n0  0  1  1  0 \\\\\n1  0  0  1  1 \\\\\n0  1  0  0.5  1 \\\\\n0.5  0  1  0  1 \\\\\n1  0.5  0.5  0  0\n\\end{bmatrix}.\n$$\n- 正则化算子 $L^{(1)} = I_{5}$（$5 \\times 5$ 单位矩阵）。\n- 先验模型 $m_{0}^{(1)} = \\begin{bmatrix} 0  0  0  0  0 \\end{bmatrix}^{\\mathsf{T}}$。\n- 约束矩阵\n$$\nB^{(1)} = \\begin{bmatrix}\n1  1  0  0  0 \\\\\n0  0  1  -1  0\n\\end{bmatrix},\\quad\nb^{(1)} = \\begin{bmatrix} 0.5 \\\\ 0.3 \\end{bmatrix}.\n$$\n- 真实模型和噪声：\n$m_{\\mathrm{true}}^{(1)} = \\begin{bmatrix} 1  -0.5  0.3  0  0.8 \\end{bmatrix}^{\\mathsf{T}}$，\n$n^{(1)} = \\begin{bmatrix} 0.01  -0.02  0.015  0  -0.005  0.01  -0.01 \\end{bmatrix}^{\\mathsf{T}}$。\n- 正则化权重 $\\alpha^{(1)} = 0.1$。\n\n测试用例 2 (欠定数据项，一阶差分正则化，较强正则化权重):\n- 维度：$m = 3$，$n = 6$，$p = 5$，$q = 2$。\n- 矩阵\n$$\nA^{(2)} = \\begin{bmatrix}\n1  0  0  1  0  0 \\\\\n0  1  0  0  1  0 \\\\\n0  0  1  0  0  1\n\\end{bmatrix}.\n$$\n- 正则化算子 $L^{(2)} \\in \\mathbb{R}^{5 \\times 6}$ 是一阶差分算子：\n$(L^{(2)} m)_{i} = m_{i+1} - m_{i}$ 对于 $i \\in \\{1,2,3,4,5\\}$，即，\n$$\nL^{(2)} = \\begin{bmatrix}\n-1  1  0  0  0  0 \\\\\n0  -1  1  0  0  0 \\\\\n0  0  -1  1  0  0 \\\\\n0  0  0  -1  1  0 \\\\\n0  0  0  0  -1  1\n\\end{bmatrix}.\n$$\n- 先验模型 $m_{0}^{(2)} = \\begin{bmatrix} 0  0  0  0  0  0 \\end{bmatrix}^{\\mathsf{T}}$。\n- 约束矩阵\n$$\nB^{(2)} = \\begin{bmatrix}\n1  1  1  1  1  1 \\\\\n1  0  -1  0  0  0\n\\end{bmatrix},\\quad\nb^{(2)} = \\begin{bmatrix} 0.7 \\\\ -0.1 \\end{bmatrix}.\n$$\n- 真实模型和噪声：\n$m_{\\mathrm{true}}^{(2)} = \\begin{bmatrix} 0.2  -0.1  0.3  0.4  -0.2  0.1 \\end{bmatrix}^{\\mathsf{T}}$，\n$n^{(2)} = \\begin{bmatrix} 0  0.01  -0.02 \\end{bmatrix}^{\\mathsf{T}}$。\n- 正则化权重 $\\alpha^{(2)} = 1.0$。\n\n测试用例 3 (方阵数据项，单位矩阵正则化，小正则化权重，病态但行满秩的约束):\n- 维度：$m = 5$，$n = 5$，$p = 5$，$q = 2$。\n- 矩阵\n$$\nA^{(3)} = \\begin{bmatrix}\n1  0.01  0  0  0 \\\\\n0.01  1  0.01  0  0 \\\\\n0  0.01  1  0.01  0 \\\\\n0  0  0.01  1  0.01 \\\\\n0  0  0  0.01  1\n\\end{bmatrix}.\n$$\n- 正则化算子 $L^{(3)} = I_{5}$。\n- 先验模型 $m_{0}^{(3)} = \\begin{bmatrix} 0  0  0  0  0 \\end{bmatrix}^{\\mathsf{T}}$。\n- 约束矩阵\n$$\nB^{(3)} = \\begin{bmatrix}\n1  0.0001  0  0  0 \\\\\n0  1  0.0001  0  0\n\\end{bmatrix},\\quad\nb^{(3)} = \\begin{bmatrix} 0.9999 \\\\ -0.9999 \\end{bmatrix}.\n$$\n- 真实模型和噪声：\n$m_{\\mathrm{true}}^{(3)} = \\begin{bmatrix} 1  -1  1  -1  1 \\end{bmatrix}^{\\mathsf{T}}$，\n$n^{(3)} = \\begin{bmatrix} 0  0  0  0  0.001 \\end{bmatrix}^{\\mathsf{T}}$。\n- 正则化权重 $\\alpha^{(3)} = 0.001$。\n\n测试用例 4 (超定数据项，单位矩阵正则化，强正则化权重):\n- 维度：$m = 8$，$n = 4$，$p = 4$，$q = 2$。\n- 矩阵\n$$\nA^{(4)} = \\begin{bmatrix}\n1  0  1  0 \\\\\n0  1  0  1 \\\\\n1  1  0  0 \\\\\n0  0  1  1 \\\\\n1  0  0  1 \\\\\n0  1  1  0 \\\\\n1  -1  0  0 \\\\\n0  0  1  -1\n\\end{bmatrix}.\n$$\n- 正则化算子 $L^{(4)} = I_{4}$。\n- 先验模型 $m_{0}^{(4)} = \\begin{bmatrix} 0  0  0  0 \\end{bmatrix}^{\\mathsf{T}}$。\n- 约束矩阵\n$$\nB^{(4)} = \\begin{bmatrix}\n1  0  0  1 \\\\\n0  1  1  0\n\\end{bmatrix},\\quad\nb^{(4)} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.\n$$\n- 真实模型和噪声：\n$m_{\\mathrm{true}}^{(4)} = \\begin{bmatrix} 0.5  -0.5  0.5  -0.5 \\end{bmatrix}^{\\mathsf{T}}$，\n$n^{(4)} = \\begin{bmatrix} 0.01  -0.01  0  0.005  -0.005  0  0  0.002 \\end{bmatrix}^{\\mathsf{T}}$。\n- 正则化权重 $\\alpha^{(4)} = 10$。\n\n对于每个测试用例 $i \\in \\{1,2,3,4\\}$：\n- 构建 $d^{(i)} = A^{(i)} m_{\\mathrm{true}}^{(i)} + n^{(i)}$。\n- 通过 KKT 系统构建并求解精确的等式约束问题。\n- 对指定的 $\\mu$ 值序列构建并求解罚函数问题。\n- 定义序列 $\\Delta^{(i)}(\\mu) = \\lVert m_{\\mu}^{(i)} - m_{\\mathrm{KKT}}^{(i)} \\rVert_{2}$ 和 $r^{(i)}(\\mu) = \\lVert B^{(i)} m_{\\mu}^{(i)} - b^{(i)} \\rVert_{2}$。\n- 检查 $\\Delta^{(i)}(\\mu)$ 和 $r^{(i)}(\\mu)$ 两者是否都关于 $\\mu$ 在一个小的数值容差内单调非增，并且在最大惩罚系数下，$\\Delta^{(i)}(\\mu_{\\max})$ 和 $r^{(i)}(\\mu_{\\max})$ 相对于自然尺度都很小。\n\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如，\"[True,False,True,True]\"），其中每个条目是按 $i = 1, 2, 3, 4$ 顺序排列的单个测试用例的布尔结果，指示该测试是否通过了上述所有检查。", "solution": "该问题要求解一个带等式约束的线性反演问题。任务是根据数据 $d \\in \\mathbb{R}^{m}$ 恢复参数向量 $m \\in \\mathbb{R}^{n}$，该模型被表述为一个约束优化问题的解。需要最小化的 Tikhonov 正则化最小二乘目标函数为\n$$ J(m) = \\frac{1}{2} \\lVert A m - d \\rVert_2^2 + \\frac{1}{2} \\alpha^2 \\lVert L (m - m_0) \\rVert_2^2, $$\n其中 $A \\in \\mathbb{R}^{m \\times n}$ 是正演算子，$\\alpha  0$ 是正则化权重，$L \\in \\mathbb{R}^{p \\times n}$ 是正则化算子，$m_0 \\in \\mathbb{R}^n$ 是先验模型。该最小化问题受一组线性等式约束：\n$$ B m = b, $$\n其中 $B \\in \\mathbb{R}^{q \\times n}$ 是约束矩阵，$b \\in \\mathbb{R}^q$ 是约束向量。\n\n我们将推导并实现两种解决该问题的方法：使用拉格朗日乘子法的精确方法和使用二次罚函数的近似方法。\n\n**1. 通过 Karush–Kuhn–Tucker (KKT) 条件求解精确解**\n\n拉格朗日乘子法是求解等式约束优化问题的基本工具。我们引入一个拉格朗日乘子向量 $\\lambda \\in \\mathbb{R}^q$ 并构建拉格朗日函数 $\\mathcal{L}(m, \\lambda)$：\n$$ \\mathcal{L}(m, \\lambda) = J(m) + \\lambda^\\mathsf{T} (B m - b). $$\n最优解 $(m_{\\mathrm{KKT}}, \\lambda^*)$ 必须满足一阶必要 Karush–Kuhn–Tucker (KKT) 条件，即拉格朗日函数对 $m$ 和 $\\lambda$ 的梯度都必须为零。\n\n$\\mathcal{L}$ 对 $m$ 的梯度可以通过对 $J(m)$ 和线性项 $\\lambda^\\mathsf{T} B m$ 求导得到。目标函数 $J(m)$ 可以展开为：\n$$ J(m) = \\frac{1}{2}(m^\\mathsf{T}A^\\mathsf{T}Am - 2d^\\mathsf{T}Am + d^\\mathsf{T}d) + \\frac{1}{2}\\alpha^2(m-m_0)^\\mathsf{T}L^\\mathsf{T}L(m-m_0). $$\n其梯度为：\n$$ \\nabla_m J(m) = A^\\mathsf{T}(Am - d) + \\alpha^2 L^\\mathsf{T}L(m - m_0). $$\n因此，平稳性条件 $\\nabla_m \\mathcal{L}(m, \\lambda) = 0$ 为：\n$$ \\nabla_m J(m) + B^\\mathsf{T}\\lambda = 0 $$\n$$ A^\\mathsf{T}(Am - d) + \\alpha^2 L^\\mathsf{T}L(m - m_0) + B^\\mathsf{T}\\lambda = 0. $$\n重新整理，将包含 $m$ 的项组合在左侧，得到：\n$$ (A^\\mathsf{T}A + \\alpha^2 L^\\mathsf{T}L)m + B^\\mathsf{T}\\lambda = A^\\mathsf{T}d + \\alpha^2 L^\\mathsf{T}Lm_0. $$\n\n第二个 KKT 条件是原始可行性，通过将 $\\mathcal{L}$ 对 $\\lambda$ 的梯度设为零得到：\n$$ \\nabla_\\lambda \\mathcal{L}(m, \\lambda) = Bm - b = 0. $$\n\n这两个条件构成了一个关于未知模型向量 $m$ 和拉格朗日乘子向量 $\\lambda$ 的耦合线性方程组。这就是 KKT 系统，可以写成块矩阵形式：\n$$\n\\begin{bmatrix}\nA^\\mathsf{T}A + \\alpha^2 L^\\mathsf{T}L   B^\\mathsf{T} \\\\\nB   \\boldsymbol{0}\n\\end{bmatrix}\n\\begin{bmatrix}\nm \\\\\n\\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nA^\\mathsf{T}d + \\alpha^2 L^\\mathsf{T}Lm_0 \\\\\nb\n\\end{bmatrix}.\n$$\n这是一个大小为 $(n+q) \\times (n+q)$ 的对称但非定的线性系统。假设 KKT 矩阵是可逆的，我们可以直接求解该系统，以找到精确的约束解，记为 $m_{\\mathrm{KKT}}$。\n\n**2. 罚函数法无约束近似**\n\n另一种方法是使用罚函数法将约束问题转化为一系列无约束问题。约束作为惩罚项被引入目标函数，并由一个惩罚参数 $\\mu  0$ 加权。罚函数化后的目标函数 $J_\\mu(m)$ 为：\n$$ J_\\mu(m) = J(m) + \\frac{1}{2}\\mu^2 \\lVert Bm - b \\rVert_2^2. $$\n这是一个无约束的二次最小化问题。解 $m_\\mu$ 通过将梯度 $\\nabla_m J_\\mu(m)$ 设为零来求得。惩罚项的梯度为：\n$$ \\nabla_m \\left( \\frac{1}{2}\\mu^2 \\lVert Bm-b \\rVert_2^2 \\right) = \\mu^2 B^\\mathsf{T}(Bm - b). $$\n将其加到 $J(m)$ 的梯度上，最优性条件 $\\nabla_m J_\\mu(m) = 0$ 变为：\n$$ A^\\mathsf{T}(Am - d) + \\alpha^2 L^\\mathsf{T}L(m - m_0) + \\mu^2 B^\\mathsf{T}(Bm - b) = 0. $$\n重新整理该方程，得到关于 $m_\\mu$ 的一个线性系统：\n$$ (A^\\mathsf{T}A + \\alpha^2 L^\\mathsf{T}L + \\mu^2 B^\\mathsf{T}B)m_\\mu = A^\\mathsf{T}d + \\alpha^2 L^\\mathsf{T}Lm_0 + \\mu^2 B^\\mathsf{T}b. $$\n这是一个 $n \\times n$ 大小的系统。左侧的矩阵也是对称正定的（当 $\\alpha0$ 或在 $A$ 和 $L$ 的温和条件下），这使得它可以被可靠地求解。理论上，当惩罚参数 $\\mu \\to \\infty$ 时，解 $m_\\mu$ 收敛于精确的约束解 $m_{\\mathrm{KKT}}$。\n\n**3. 数值验证**\n\n实现将通过求解 KKT 系统来计算 $m_{\\mathrm{KKT}}$，并为 $\\mu \\in \\{1,\\,10,\\,100,\\,1000,\\,100000\\}$ 计算一系列罚函数解 $m_\\mu$。对于每个测试用例，将验证以下反映罚函数法收敛性的性质：\n1.  随着 $\\mu$ 的增加，欧几里得距离序列 $\\Delta(\\mu) = \\lVert m_\\mu - m_{\\mathrm{KKT}} \\rVert_2$ 是单调非增的。\n2.  随着 $\\mu$ 的增加，约束残差序列 $r(\\mu) = \\lVert Bm_\\mu - b \\rVert_2$ 是单调非增的。\n3.  在最大惩罚系数 $\\mu_{\\max} = 100000$ 下，距离 $\\Delta(\\mu_{\\max})$ 和残差 $r(\\mu_{\\max})$ 低于指定的数值容差，这确认了罚函数解是精确约束解的一个良好近似。\n\n如果所有这些检查都通过，则认为一个测试用例是成功的。所有数值计算将使用为每个测试用例提供的矩阵和参数进行。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the constrained inverse problem using both KKT and penalty methods,\n    and verifies the convergence properties of the penalty method.\n    \"\"\"\n    # Define penalty sequence\n    mus = [1.0, 10.0, 100.0, 1000.0, 100000.0]\n    \n    # Define tolerances for checks\n    mono_tol = 1e-9\n    final_dist_tol = 1e-4\n    final_resid_tol = 1e-6\n\n    # Test Case 1\n    A1 = np.array([\n        [1, 2, 0, 0, 0.5], [0, 1, 1, 0, 0], [0, 0, 1, 1, 0], [1, 0, 0, 1, 1],\n        [0, 1, 0, 0.5, 1], [0.5, 0, 1, 0, 1], [1, 0.5, 0.5, 0, 0]\n    ])\n    L1 = np.identity(5)\n    m0_1 = np.zeros(5)\n    B1 = np.array([[1, 1, 0, 0, 0], [0, 0, 1, -1, 0]])\n    b1 = np.array([0.5, 0.3])\n    m_true1 = np.array([1, -0.5, 0.3, 0, 0.8])\n    n1 = np.array([0.01, -0.02, 0.015, 0, -0.005, 0.01, -0.01])\n    alpha1 = 0.1\n    case1 = (A1, L1, B1, b1, m0_1, m_true1, n1, alpha1)\n\n    # Test Case 2\n    A2 = np.array([[1, 0, 0, 1, 0, 0], [0, 1, 0, 0, 1, 0], [0, 0, 1, 0, 0, 1]])\n    L2 = np.array([\n        [-1, 1, 0, 0, 0, 0], [0, -1, 1, 0, 0, 0], [0, 0, -1, 1, 0, 0],\n        [0, 0, 0, -1, 1, 0], [0, 0, 0, 0, -1, 1]\n    ])\n    m0_2 = np.zeros(6)\n    B2 = np.array([[1, 1, 1, 1, 1, 1], [1, 0, -1, 0, 0, 0]])\n    b2 = np.array([0.7, -0.1])\n    m_true2 = np.array([0.2, -0.1, 0.3, 0.4, -0.2, 0.1])\n    n2 = np.array([0, 0.01, -0.02])\n    alpha2 = 1.0\n    case2 = (A2, L2, B2, b2, m0_2, m_true2, n2, alpha2)\n\n    # Test Case 3\n    A3 = np.array([\n        [1, 0.01, 0, 0, 0], [0.01, 1, 0.01, 0, 0], [0, 0.01, 1, 0.01, 0],\n        [0, 0, 0.01, 1, 0.01], [0, 0, 0, 0.01, 1]\n    ])\n    L3 = np.identity(5)\n    m0_3 = np.zeros(5)\n    B3 = np.array([[1, 0.0001, 0, 0, 0], [0, 1, 0.0001, 0, 0]])\n    b3 = np.array([0.9999, -0.9999])\n    m_true3 = np.array([1, -1, 1, -1, 1])\n    n3 = np.array([0, 0, 0, 0, 0.001])\n    alpha3 = 0.001\n    case3 = (A3, L3, B3, b3, m0_3, m_true3, n3, alpha3)\n\n    # Test Case 4\n    A4 = np.array([\n        [1, 0, 1, 0], [0, 1, 0, 1], [1, 1, 0, 0], [0, 0, 1, 1],\n        [1, 0, 0, 1], [0, 1, 1, 0], [1, -1, 0, 0], [0, 0, 1, -1]\n    ])\n    L4 = np.identity(4)\n    m0_4 = np.zeros(4)\n    B4 = np.array([[1, 0, 0, 1], [0, 1, 1, 0]])\n    b4 = np.array([0, 0])\n    m_true4 = np.array([0.5, -0.5, 0.5, -0.5])\n    n4 = np.array([0.01, -0.01, 0, 0.005, -0.005, 0, 0, 0.002])\n    alpha4 = 10.0\n    case4 = (A4, L4, B4, b4, m0_4, m_true4, n4, alpha4)\n    \n    test_cases = [case1, case2, case3, case4]\n    \n    overall_results = []\n\n    for case in test_cases:\n        A, L, B, b, m0, m_true, n_noise, alpha = case\n        n = A.shape[1]\n        q = B.shape[0]\n        \n        d = A @ m_true + n_noise\n        \n        # --- KKT Solution ---\n        H = A.T @ A + (alpha**2) * (L.T @ L)\n        g = A.T @ d + (alpha**2) * (L.T @ L @ m0)\n        \n        KKT_matrix = np.zeros((n + q, n + q))\n        KKT_matrix[:n, :n] = H\n        KKT_matrix[:n, n:] = B.T\n        KKT_matrix[n:, :n] = B\n        \n        KKT_rhs = np.concatenate([g, b])\n        \n        sol_kkt = np.linalg.solve(KKT_matrix, KKT_rhs)\n        m_kkt = sol_kkt[:n]\n        \n        # --- Penalized Solutions ---\n        m_mus = []\n        for mu in mus:\n            H_mu = H + (mu**2) * (B.T @ B)\n            g_mu = g + (mu**2) * (B.T @ b)\n            m_mu = np.linalg.solve(H_mu, g_mu)\n            m_mus.append(m_mu)\n            \n        # --- Checks ---\n        test_passed = True\n        \n        # Distances and residuals\n        deltas = [np.linalg.norm(m_mu - m_kkt) for m_mu in m_mus]\n        residuals = [np.linalg.norm(B @ m_mu - b) for m_mu in m_mus]\n        \n        # Monotonicity checks\n        for i in range(len(mus) - 1):\n            if deltas[i]  deltas[i+1] - mono_tol:\n                test_passed = False\n                break\n            if residuals[i]  residuals[i+1] - mono_tol:\n                test_passed = False\n                break\n        \n        if not test_passed:\n            overall_results.append(False)\n            continue\n            \n        # Final accuracy check\n        if deltas[-1] > final_dist_tol:\n            test_passed = False\n        if residuals[-1] > final_resid_tol:\n            test_passed = False\n            \n        overall_results.append(test_passed)\n\n    print(f\"[{','.join(map(str, overall_results))}]\")\n\nsolve()\n```", "id": "3395335"}, {"introduction": "基于增广拉格朗日量的概念，我们现在转向一个用于解决现代反演问题的强大迭代框架：交替方向乘子法 (Alternating Direction Method of Multipliers, ADMM)。这个练习 [@problem_id:3395284] 将指导你为一个包含 $\\ell_{1}$ 正则化（一种促进稀疏性的常用工具）的问题推导 ADMM 算法。该练习展示了如何通过拆分问题来处理非光滑的复杂目标函数，揭示了基于拉格朗日方法在当代数据科学中的强大威力。", "problem": "考虑一个有限维约束逆问题，该问题通过最小化目标函数来从数据 $d \\in \\mathbb{R}^{p}$ 估计参数向量 $m \\in \\mathbb{R}^{n}$ 和一个辅助变量 $z \\in \\mathbb{R}^{q}$\n$$\n\\frac{1}{2}\\|A m - d\\|_{2}^{2} + \\alpha \\|z\\|_{1}\n$$\n其满足线性一致性约束\n$$\nW m - z = 0,\n$$\n其中 $A \\in \\mathbb{R}^{p \\times n}$ 和 $W \\in \\mathbb{R}^{q \\times n}$ 是已知矩阵，$\\alpha  0$ 是给定的正则化权重。请使用拉格朗日乘子法和带有罚参数 $\\rho  0$ 的增广拉格朗日构造，从第一性原理出发，推导缩放的交替方向乘子法（ADMM）迭代过程。该迭代过程在关于 $m$ 的最小化、关于 $z$ 的最小化以及更新一个缩放对偶变量之间交替进行。您的推导必须从等式约束优化的经典拉格朗日量和增广拉格朗日量的定义开始，调用一阶最优性条件，并使用 $\\ell_{1}$ 范数的近端算子定义。假设存在保证 $m$-更新步骤具有唯一解的条件，例如对于所选的 $\\rho  0$，$A^{\\top}A + \\rho W^{\\top}W$ 具有正定性。\n\n明确求出：\n- 定义 $m$-更新步骤的线性系统，\n- 以软阈值映射表示的 $z$-更新步骤的逐元素闭式表达式，\n- 缩放对偶变量的更新步骤，\n然后将缩放对偶变量与经典拉格朗日乘子联系起来。\n\n最后，以一个单独的闭式解析表达式作为您的最终答案，该表达式表示迭代 $k+1$ 次时的经典拉格朗日乘子，且纯粹用迭代 $k+1$ 次时的缩放对偶变量和罚参数表示。您的最终答案中不应包含等号。无需进行数值计算，也不涉及任何单位。如果出现角度，请以弧度表示。最终答案必须是单个表达式，而不是方程或不等式。", "solution": "本问题旨在推导用于求解以下约束优化问题的交替方向乘子法（ADMM），以估计参数向量 $m \\in \\mathbb{R}^{n}$ 和辅助变量 $z \\in \\mathbb{R}^{q}$：\n$$\n\\min_{m,z} \\left( \\frac{1}{2}\\|A m - d\\|_{2}^{2} + \\alpha \\|z\\|_{1} \\right) \\quad \\text{subject to} \\quad W m - z = 0\n$$\n其中 $A \\in \\mathbb{R}^{p \\times n}$，$W \\in \\mathbb{R}^{q \\times n}$，$d \\in \\mathbb{R}^{p}$ 是给定的，$\\alpha  0$ 是一个正则化参数。\n\n通过识别 $x=m$，$f(m) = \\frac{1}{2}\\|A m - d\\|_{2}^{2}$，$g(z) = \\alpha \\|z\\|_{1}$，$E=W$，$F=-I$（$q \\times q$ 阶单位矩阵）和 $c=0$，该问题符合 ADMM 的标准形式 $\\min_{x,z} f(x) + g(z)$，约束条件为 $Ex + Fz = c$。\n\n推导过程首先构造增广拉格朗日量，然后推导原始变量和对偶变量的迭代更新式。\n\n对于此问题，设 $\\lambda \\in \\mathbb{R}^{q}$ 为等式约束的拉格朗日乘子，其经典拉格朗日量为：\n$$\n\\mathcal{L}(m, z, \\lambda) = \\frac{1}{2}\\|A m - d\\|_{2}^{2} + \\alpha \\|z\\|_{1} + \\lambda^{\\top}(W m - z)\n$$\n通过为约束添加一个带有罚参数 $\\rho  0$ 的二次惩罚项，构成增广拉格朗日量：\n$$\n\\mathcal{L}_{\\rho}(m, z, \\lambda) = \\frac{1}{2}\\|A m - d\\|_{2}^{2} + \\alpha \\|z\\|_{1} + \\lambda^{\\top}(W m - z) + \\frac{\\rho}{2}\\|W m - z\\|_{2}^{2}\n$$\n为简化表达式，我们使用 ADMM 的缩放形式。引入一个由关系 $\\lambda = \\rho u$ 定义的缩放对偶变量 $u \\in \\mathbb{R}^{q}$。将其代入增广拉格朗日量中，得到：\n$$\n\\mathcal{L}_{\\rho}(m, z, u) = \\frac{1}{2}\\|A m - d\\|_{2}^{2} + \\alpha \\|z\\|_{1} + (\\rho u)^{\\top}(W m - z) + \\frac{\\rho}{2}\\|W m - z\\|_{2}^{2}\n$$\n通过对包含对偶变量和约束的项进行配方，我们可以将拉格朗日量重写为更紧凑的形式：\n\\begin{align*}\n(\\rho u)^{\\top}(W m - z) + \\frac{\\rho}{2}\\|W m - z\\|_{2}^{2} = \\frac{\\rho}{2} \\left( 2u^{\\top}(W m - z) + \\|W m - z\\|_{2}^{2} \\right) \\\\\n= \\frac{\\rho}{2} \\left( \\|W m - z\\|_{2}^{2} + 2u^{\\top}(W m - z) + \\|u\\|_{2}^{2} - \\|u\\|_{2}^{2} \\right) \\\\\n= \\frac{\\rho}{2} \\left( \\|(W m - z) + u\\|_{2}^{2} - \\|u\\|_{2}^{2} \\right)\n\\end{align*}\n因此，缩放的增广拉格朗日量为：\n$$\n\\mathcal{L}_{\\rho}(m, z, u) = \\frac{1}{2}\\|A m - d\\|_{2}^{2} + \\alpha \\|z\\|_{1} + \\frac{\\rho}{2}\\|W m - z + u\\|_{2}^{2} - \\frac{\\rho}{2}\\|u\\|_{2}^{2}\n$$\nADMM 算法执行关于 $m$ 和 $z$ 的交替最小化，然后更新对偶变量 $u$。在第 $k+1$ 次迭代时，从已知值 $(z^k, u^k)$ 开始，步骤如下：\n\n$1$. $m$-更新：\n$m^{k+1}$ 是通过最小化 $\\mathcal{L}_{\\rho}(m, z^k, u^k)$ 关于 $m$ 的值得到的。我们可以舍去所有关于 $m$ 的常数项：\n$$\nm^{k+1} = \\arg\\min_{m} \\left( \\frac{1}{2}\\|A m - d\\|_{2}^{2} + \\frac{\\rho}{2}\\|W m - z^k + u^k\\|_{2}^{2} \\right)\n$$\n这是一个无约束的二次最小化问题。通过将目标函数关于 $m$ 的梯度设为零来找到解：\n$$\n\\nabla_{m} \\left( \\frac{1}{2}\\|A m - d\\|_{2}^{2} + \\frac{\\rho}{2}\\|W m - z^k + u^k\\|_{2}^{2} \\right) = 0\n$$\n$$\nA^{\\top}(A m - d) + \\rho W^{\\top}(W m - z^k + u^k) = 0\n$$\n将包含 $m$ 的项归到一侧，得到关于 $m^{k+1}$ 的线性系统：\n$$\n(A^{\\top}A + \\rho W^{\\top}W) m^{k+1} = A^{\\top}d + \\rho W^{\\top}(z^k - u^k)\n$$\n问题陈述假设矩阵 $(A^{\\top}A + \\rho W^{\\top}W)$ 是正定的，这保证了该线性系统对于 $m^{k+1}$ 有唯一解。\n\n$2$. $z$-更新：\n使用新计算出的 $m^{k+1}$，通过最小化 $\\mathcal{L}_{\\rho}(m^{k+1}, z, u^k)$ 关于 $z$ 的值来找到 $z^{k+1}$。同样，我们舍去常数项：\n$$\nz^{k+1} = \\arg\\min_{z} \\left( \\alpha \\|z\\|_{1} + \\frac{\\rho}{2}\\|W m^{k+1} - z + u^k\\|_{2}^{2} \\right)\n$$\n令向量 $v = W m^{k+1} + u^k$。最小化问题可以写为：\n$$\nz^{k+1} = \\arg\\min_{z} \\left( \\alpha \\|z\\|_{1} + \\frac{\\rho}{2}\\|z - v\\|_{2}^{2} \\right)\n$$\n这等价于：\n$$\nz^{k+1} = \\arg\\min_{z} \\left( \\frac{\\alpha}{\\rho}\\|z\\|_{1} + \\frac{1}{2}\\|z - v\\|_{2}^{2} \\right)\n$$\n这是函数 $h(z) = (\\alpha/\\rho)\\|z\\|_{1}$ 在 $v$ 处求值的近端算子的定义。$\\ell_1$-范数的近端算子是软阈值算子 $S_{\\kappa}(\\cdot)$，其中阈值为 $\\kappa = \\alpha/\\rho$。该算子逐元素应用。因此，$z$-更新的闭式表达式为：\n$$\nz^{k+1} = S_{\\alpha/\\rho}(W m^{k+1} + u^k)\n$$\n其中对于每个分量 $i$，有 $(S_{\\kappa}(x))_i = \\text{sign}(x_i) \\max(|x_i| - \\kappa, 0)$。\n\n$3$. 缩放对偶变量更新：\n经典拉格朗日乘子 $\\lambda$ 的更新遵循一个对偶上升步骤：\n$$\n\\lambda^{k+1} = \\lambda^k + \\rho(W m^{k+1} - z^{k+1})\n$$\n使用缩放关系 $\\lambda = \\rho u$，我们代入 $\\lambda^k = \\rho u^k$ 和 $\\lambda^{k+1} = \\rho u^{k+1}$：\n$$\n\\rho u^{k+1} = \\rho u^k + \\rho(W m^{k+1} - z^{k+1})\n$$\n由于 $\\rho  0$，我们可以两边除以 $\\rho$ 来获得缩放对偶变量的更新式：\n$$\nu^{k+1} = u^k + (W m^{k+1} - z^{k+1})\n$$\n项 $W m^{k+1} - z^{k+1}$ 是第 $k+1$ 次迭代时的原始残差。\n\n最后，经典拉格朗日乘子 $\\lambda^{k+1}$ 和缩放对偶变量 $u^{k+1}$ 之间的关系由缩放本身的定义给出。对迭代 $k+1$ 重新整理 $\\lambda = \\rho u$ 可得：\n$$\n\\lambda^{k+1} = \\rho u^{k+1}\n$$\n这给出了第 $k+1$ 次迭代时经典拉格朗日乘子关于相应缩放对偶变量和罚参数的表达式。", "answer": "$$\n\\boxed{\\rho u^{k+1}}\n$$", "id": "3395284"}]}