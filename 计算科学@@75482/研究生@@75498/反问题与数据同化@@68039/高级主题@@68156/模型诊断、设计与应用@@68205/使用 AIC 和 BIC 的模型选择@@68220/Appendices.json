{"hands_on_practices": [{"introduction": "贝叶斯信息准则（BIC）不仅仅是一个带惩罚项的公式，它深深植根于贝叶斯模型证据的近似。这个练习将引导你从拉普拉斯近似的理论基础出发，通过一个具体的模型比较任务，亲手计算BIC并将其与贝叶斯因子联系起来。这有助于你巩固对BIC惩罚项作为样本量函数的理解，并体会其在贝叶斯框架下的理论意义。[@problem_id:3403899]", "problem": "考虑一个逆向建模和数据同化的场景，分析师必须在两个嵌套的高斯线性观测模型中进行选择，以同化一批固定的测量数据。设 $\\mathcal{M}_1$ 是一个具有 $k_1$ 个自由参数的受限模型，$\\mathcal{M}_2$ 是一个具有 $k_2$ 个自由参数的扩展模型，其中嵌套关系意味着 $\\mathcal{M}_1$ 的参数空间是 $\\mathcal{M}_2$ 参数空间的线性子空间。数据包含 $n$ 次独立抽样，观测误差为零均值、方差未知的高斯分布。两个模型的最大似然拟合产生了最大化对数似然值 $\\ell_1$ 和 $\\ell_2$。\n\n从给定模型的贝叶斯模型证据（边际似然）的定义和标准正则性条件下的拉普拉斯近似出发，推导一个大样本准则，该准则通过一个包含样本量 $n$ 和自由参数数量 $k$ 的项来对模型复杂度进行惩罚。使用该准则计算 $\\mathcal{M}_1$ 和 $\\mathcal{M}_2$ 的值（根据报告的 $\\ell_1$ 和 $\\ell_2$），然后将这些准则值的差异与支持 $\\mathcal{M}_2$ 而非 $\\mathcal{M}_1$ 的贝叶斯因子 (Bayes factor) 的对数联系起来。\n\n您已获得最大似然拟合的以下数值输出：\n- 样本量：$n = 150$。\n- 参数数量：$\\mathcal{M}_1$ 为 $k_1 = 4$，$\\mathcal{M}_2$ 为 $k_2 = 9$。\n- 最大化对数似然：$\\mathcal{M}_1$ 为 $\\ell_1 = -210.7$，$\\mathcal{M}_2$ 为 $\\ell_2 = -195.2$。\n\n假设拉普拉斯近似的所有正则性条件均成立，模型在参数值范围内是正确设定的，先验在最大似然估计附近为正且足够平滑，并使用自然对数。计算两个模型的贝叶斯信息准则 (BIC)，然后通过将 BIC 差异与边际似然比相关联，计算支持 $\\mathcal{M}_2$ 而非 $\\mathcal{M}_1$ 的拉普拉斯近似的贝叶斯因子。将贝叶斯因子的值作为最终答案，四舍五入至四位有效数字。最终答案无需单位。", "solution": "该问题要求从贝叶斯模型证据的定义出发，推导一个大样本模型选择准则，将其应用于一个具体案例，并计算贝叶斯因子。该问题具有科学依据，是适定的，并且包含唯一解所需的所有信息。因此，它被认为是有效的。\n\n设 $\\mathcal{M}$ 是一个具有 $k$ 维参数向量 $\\boldsymbol{\\theta}$ 的模型。给定数据 $\\mathbf{y}$，模型 $\\mathcal{M}$ 的证据是边际似然，定义为似然函数在参数先验分布上的积分：\n$$ p(\\mathbf{y} | \\mathcal{M}) = \\int p(\\mathbf{y} | \\boldsymbol{\\theta}, \\mathcal{M}) p(\\boldsymbol{\\theta} | \\mathcal{M}) d\\boldsymbol{\\theta} $$\n这里，$p(\\mathbf{y} | \\boldsymbol{\\theta}, \\mathcal{M})$ 是似然函数 $L(\\boldsymbol{\\theta})$，$p(\\boldsymbol{\\theta} | \\mathcal{M})$ 是参数的先验概率分布。该积分可以重写为：\n$$ p(\\mathbf{y} | \\mathcal{M}) = \\int \\exp\\left( \\ln\\left[ L(\\boldsymbol{\\theta}) p(\\boldsymbol{\\theta} | \\mathcal{M}) \\right] \\right) d\\boldsymbol{\\theta} = \\int \\exp\\left( \\ell(\\boldsymbol{\\theta}) + \\ln p(\\boldsymbol{\\theta} | \\mathcal{M}) \\right) d\\boldsymbol{\\theta} $$\n其中 $\\ell(\\boldsymbol{\\theta}) = \\ln L(\\boldsymbol{\\theta})$ 是对数似然。\n\n我们可以使用拉普拉斯方法来近似这个积分。该方法基于指数部分在其最大值点附近的二阶泰勒展开。令 $f(\\boldsymbol{\\theta}) = \\ell(\\boldsymbol{\\theta}) + \\ln p(\\boldsymbol{\\theta} | \\mathcal{M})$。$f(\\boldsymbol{\\theta})$ 的最大值出现在最大后验 (MAP) 估计 $\\hat{\\boldsymbol{\\theta}}_{\\text{MAP}}$ 处。对于大样本量 $n$，假设先验足够分散，似然项 $\\ell(\\boldsymbol{\\theta})$ 会主导先验项 $\\ln p(\\boldsymbol{\\theta} | \\mathcal{M})$。因此，$\\hat{\\boldsymbol{\\theta}}_{\\text{MAP}}$ 可以很好地由最大化 $\\ell(\\boldsymbol{\\theta})$ 的最大似然估计 (MLE) $\\hat{\\boldsymbol{\\theta}}$ 来近似。\n\n$f(\\boldsymbol{\\theta})$ 在 $\\hat{\\boldsymbol{\\theta}}$ 附近的泰勒展开为：\n$$ f(\\boldsymbol{\\theta}) \\approx f(\\hat{\\boldsymbol{\\theta}}) + (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}})^T \\nabla f(\\hat{\\boldsymbol{\\theta}}) + \\frac{1}{2} (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}})^T \\mathbf{H}(\\hat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}}) $$\n其中 $\\mathbf{H}(\\hat{\\boldsymbol{\\theta}})$ 是在 $\\hat{\\boldsymbol{\\theta}}$ 处计算的 $f$ 的海森矩阵。根据最大似然估计的定义，$\\nabla \\ell(\\hat{\\boldsymbol{\\theta}}) = \\boldsymbol{0}$。因此，$\\nabla f(\\hat{\\boldsymbol{\\theta}}) = \\nabla \\ell(\\hat{\\boldsymbol{\\theta}}) + \\nabla \\ln p(\\hat{\\boldsymbol{\\theta}}) = \\nabla \\ln p(\\hat{\\boldsymbol{\\theta}})$。如果先验在 $\\hat{\\boldsymbol{\\theta}}$ 附近是平坦的，则该梯度项可以忽略不计。海森矩阵 $\\mathbf{H}(\\hat{\\boldsymbol{\\theta}})$ 也由对数似然的海森矩阵主导，我们可以近似 $\\mathbf{H}(\\hat{\\boldsymbol{\\theta}}) \\approx \\nabla^2 \\ell(\\hat{\\boldsymbol{\\theta}})$。该矩阵的负数是观测到的费雪信息矩阵，$\\mathcal{J}(\\hat{\\boldsymbol{\\theta}}) = -\\nabla^2 \\ell(\\hat{\\boldsymbol{\\theta}})$。\n\n证据的积分变为：\n$$ p(\\mathbf{y} | \\mathcal{M}) \\approx \\int \\exp\\left( f(\\hat{\\boldsymbol{\\theta}}) - \\frac{1}{2} (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}})^T \\mathcal{J}(\\hat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}}) \\right) d\\boldsymbol{\\theta} $$\n$$ p(\\mathbf{y} | \\mathcal{M}) \\approx \\exp(f(\\hat{\\boldsymbol{\\theta}})) \\int \\exp\\left( - \\frac{1}{2} (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}})^T \\mathcal{J}(\\hat{\\boldsymbol{\\theta}}) (\\boldsymbol{\\theta} - \\hat{\\boldsymbol{\\theta}}) \\right) d\\boldsymbol{\\theta} $$\n该积分是 $k$ 元高斯分布的非归一化形式，其值为 $(2\\pi)^{k/2} |\\mathcal{J}(\\hat{\\boldsymbol{\\theta}})|^{-1/2}$。代入此式以及 $f(\\hat{\\boldsymbol{\\theta}}) \\approx \\ell(\\hat{\\boldsymbol{\\theta}}) = \\ell_{\\text{max}}$（忽略作为低阶项的先验项 $\\ln p(\\hat{\\boldsymbol{\\theta}})$），我们得到：\n$$ p(\\mathbf{y} | \\mathcal{M}) \\approx \\exp(\\ell_{\\text{max}}) (2\\pi)^{k/2} |\\mathcal{J}(\\hat{\\boldsymbol{\\theta}})|^{-1/2} $$\n取自然对数得到对数证据：\n$$ \\ln p(\\mathbf{y} | \\mathcal{M}) \\approx \\ell_{\\text{max}} + \\frac{k}{2} \\ln(2\\pi) - \\frac{1}{2} \\ln |\\mathcal{J}(\\hat{\\boldsymbol{\\theta}})| $$\n对于大样本 $n$，费雪信息矩阵渐近地与样本量成正比，即 $|\\mathcal{J}(\\hat{\\boldsymbol{\\theta}})|$ 的阶为 $O(n^k)$。因此，$\\ln|\\mathcal{J}(\\hat{\\boldsymbol{\\theta}})| \\approx k \\ln n + C$，其中 $C$ 是一个与 $n$ 无关的常数。舍去所有不随 $n$ 增长的项，我们得到大样本近似：\n$$ \\ln p(\\mathbf{y} | \\mathcal{M}) \\approx \\ell_{\\text{max}} - \\frac{k}{2} \\ln n $$\n贝叶斯信息准则 (BIC) 通常通过将此量乘以 $-2$ 来定义，从而创建一个需要最小化的损失函数：\n$$ \\text{BIC} = -2 \\ln p(\\mathbf{y} | \\mathcal{M}) \\approx -2 \\ell_{\\text{max}} + k \\ln n $$\n这就是所需的大样本准则。较低的 BIC 值表示对模型的证据更强。\n\n现在，我们使用提供的数据将此准则应用于两个模型 $\\mathcal{M}_1$ 和 $\\mathcal{M}_2$：\n样本量：$n = 150$。\n对于 $\\mathcal{M}_1$：参数数量 $k_1 = 4$，最大化对数似然 $\\ell_1 = -210.7$。\n对于 $\\mathcal{M}_2$：参数数量 $k_2 = 9$，最大化对数似然 $\\ell_2 = -195.2$。\n\n首先，我们计算 $\\ln(150)$ 的值：\n$$ \\ln(150) \\approx 5.010635 $$\n现在我们为每个模型计算 BIC：\n$$ \\text{BIC}_1 = -2 \\ell_1 + k_1 \\ln n = -2(-210.7) + 4 \\ln(150) = 421.4 + 4(5.010635) = 421.4 + 20.04254 = 441.44254 $$\n$$ \\text{BIC}_2 = -2 \\ell_2 + k_2 \\ln n = -2(-195.2) + 9 \\ln(150) = 390.4 + 9(5.010635) = 390.4 + 45.095715 = 435.495715 $$\n由于 $\\text{BIC}_2 < \\text{BIC}_1$，BIC 支持更复杂的模型 $\\mathcal{M}_2$。\n\n最后一步是计算支持 $\\mathcal{M}_2$ 而非 $\\mathcal{M}_1$ 的贝叶斯因子 $B_{21}$。贝叶斯因子是边际似然的比值：\n$$ B_{21} = \\frac{p(\\mathbf{y} | \\mathcal{M}_2)}{p(\\mathbf{y} | \\mathcal{M}_1)} $$\n贝叶斯因子的对数与对数证据的差有关：\n$$ \\ln B_{21} = \\ln p(\\mathbf{y} | \\mathcal{M}_2) - \\ln p(\\mathbf{y} | \\mathcal{M}_1) $$\n使用我们对对数证据的大样本近似：\n$$ \\ln B_{21} \\approx \\left(\\ell_2 - \\frac{k_2}{2} \\ln n \\right) - \\left(\\ell_1 - \\frac{k_1}{2} \\ln n \\right) = (\\ell_2 - \\ell_1) - \\frac{k_2 - k_1}{2} \\ln n $$\n这也可以用 BIC 值表示：\n$$ \\ln B_{21} \\approx \\frac{1}{2} \\left[ (-2\\ell_1 + k_1 \\ln n) - (-2\\ell_2 + k_2 \\ln n) \\right] = \\frac{\\text{BIC}_1 - \\text{BIC}_2}{2} $$\n使用我们计算的 BIC 值：\n$$ \\ln B_{21} \\approx \\frac{441.44254 - 435.495715}{2} = \\frac{5.946825}{2} = 2.9734125 $$\n为了计算贝叶斯因子 $B_{21}$，我们将这个结果取指数：\n$$ B_{21} = \\exp(\\ln B_{21}) \\approx \\exp(2.9734125) \\approx 19.5583 $$\n或者，为获得更高精度，我们使用直接公式：\n$$ \\ln B_{21} \\approx (\\ell_2 - \\ell_1) - \\frac{k_2 - k_1}{2} \\ln n $$\n代入给定值：\n$$ \\ell_2 - \\ell_1 = -195.2 - (-210.7) = 15.5 $$\n$$ k_2 - k_1 = 9 - 4 = 5 $$\n$$ \\ln B_{21} \\approx 15.5 - \\frac{5}{2} \\ln(150) = 15.5 - 2.5(5.01063529) = 15.5 - 12.52658823 = 2.97341177 $$\n$$ B_{21} = \\exp(2.97341177) \\approx 19.55829 $$\n将结果四舍五入至四位有效数字，得到 $19.56$。根据贝叶斯因子的标准解释（例如，Jeffreys' 标度），这个值表明与模型 $\\mathcal{M}_1$ 相比，有非常强的证据支持模型 $\\mathcal{M}_2$。", "answer": "$$\\boxed{19.56}$$", "id": "3403899"}, {"introduction": "正确应用信息准则需要仔细思考哪些部分构成了“被估计的参数”。本练习通过一个巧妙的场景——误差方差已知与未知——来展示模型假设的微小变化如何改变参数计数，并最终影响模型选择的结果。这是一个极好的实践，可以帮助你培养对模型细节的敏锐洞察力，并理解在计算AIC时准确识别所有自由度的重要性。[@problem_id:3403909]", "problem": "考虑一个数据同化场景，其中一个预报模型通过最小化新息（残差）的平方和来对一组固定的 $n$ 个观测值进行校准。两个候选模型 $\\mathcal{M}_{1}$ 和 $\\mathcal{M}_{2}$ 在独立高斯测量误差的假设下，对相同的 $n$ 个新息进行拟合。设 $n=10$ 个同化循环的残差序列为\n$$\n\\boldsymbol{r}_{1} = \\big(1,\\,-2,\\,2,\\,1,\\,3,\\,-2,\\,1,\\,-1,\\,2,\\,-1\\big), \\quad \\boldsymbol{r}_{2} = \\big(2,\\,-1,\\,1,\\,2,\\,-2,\\,1,\\,0,\\,-1,\\,2,\\,-1\\big).\n$$\n模型 $\\mathcal{M}_{1}$ 估计一个维度为 $p_{1}=3$ 的参数向量，模型 $\\mathcal{M}_{2}$ 估计一个维度为 $p_{2}=5$ 的参数向量。假设残差来自一个方差为 $\\sigma^{2}$ 的高斯误差模型，该方差可以是已知的或未知的，具体如下所述。\n\n仅从独立误差的高斯似然和 Akaike 信息准则（AIC）的定义出发，执行以下操作：\n\n1. 计算残差平方和 $RSS_{1} = \\sum_{i=1}^{n} r_{1,i}^{2}$ 和 $RSS_{2} = \\sum_{i=1}^{n} r_{2,i}^{2}$。\n\n2. 在误差方差已知且等于 $\\sigma^{2} = 2$ 的情况下，推导每个模型的 AIC，并报告差值 $AIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1})$。\n\n3. 在误差方差 $\\sigma^{2}$ 未知并通过最大似然估计的情况下，推导每个模型的 AIC，并报告差值 $AIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1})$。\n\n基于你的计算，不使用任何预先给出的公式，简要讨论从已知 $\\sigma^{2}$ 变为未知 $\\sigma^{2}$ 如何影响模型偏好，并结合贝叶斯信息准则（BIC; Bayesian Information Criterion）总结其背后的直觉。\n\n你最终报告的答案必须是在已知和未知方差下的差值对 $\\big(AIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1})\\big)$，并使用 $\\mathrm{pmatrix}$ 环境精确表示为一个行向量。无需进行四舍五入。", "solution": "该问题是有效的，因为它具有科学依据、提法恰当且客观。它提供了在两个不同且明确定义的场景下，从第一性原理推导 Akaike 信息准则（AIC）所需的所有必要信息。\n\nAIC 的基本定义是 $AIC = -2 \\ln(\\mathcal{L}_{max}) + 2k$，其中 $\\mathcal{L}_{max}$ 是似然函数在模型参数上最大化的值，而 $k$ 是模型中估计参数的总数。\n\n对于 $n$ 个均值为 $0$、方差为 $\\sigma^2$ 的独立同分布高斯残差 $r_i$，其似然函数由下式给出：\n$$\n\\mathcal{L}(\\theta, \\sigma^2 | \\boldsymbol{r}) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{r_i^2}{2\\sigma^2}\\right) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} r_i^2\\right)\n$$\n对应的对数似然为：\n$$\n\\ln(\\mathcal{L}) = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{RSS}{2\\sigma^2}\n$$\n其中 $RSS = \\sum_{i=1}^{n} r_i^2$ 是残差平方和。\n\n**1. 计算残差平方和 (RSS)**\n\n对于模型 $\\mathcal{M}_{1}$，当 $n=10$ 且残差向量为 $\\boldsymbol{r}_{1} = \\big(1,\\,-2,\\,2,\\,1,\\,3,\\,-2,\\,1,\\,-1,\\,2,\\,-1\\big)$：\n$$\nRSS_{1} = \\sum_{i=1}^{10} r_{1,i}^{2} = 1^2 + (-2)^2 + 2^2 + 1^2 + 3^2 + (-2)^2 + 1^2 + (-1)^2 + 2^2 + (-1)^2\n$$\n$$\nRSS_{1} = 1 + 4 + 4 + 1 + 9 + 4 + 1 + 1 + 4 + 1 = 30\n$$\n对于模型 $\\mathcal{M}_{2}$，当 $n=10$ 且残差向量为 $\\boldsymbol{r}_{2} = \\big(2,\\,-1,\\,1,\\,2,\\,-2,\\,1,\\,0,\\,-1,\\,2,\\,-1\\big)$：\n$$\nRSS_{2} = \\sum_{i=1}^{10} r_{2,i}^{2} = 2^2 + (-1)^2 + 1^2 + 2^2 + (-2)^2 + 1^2 + 0^2 + (-1)^2 + 2^2 + (-1)^2\n$$\n$$\nRSS_{2} = 4 + 1 + 1 + 4 + 4 + 1 + 0 + 1 + 4 + 1 = 21\n$$\n\n**2. 已知误差方差下的 AIC**\n\n在这种情况下，误差方差已知并固定为 $\\sigma^2 = 2$。它不是一个被估计的参数。每个模型的估计参数数量仅为其内部参数向量的维度。\n- 对于模型 $\\mathcal{M}_{1}$，估计参数的数量为 $k_1 = p_1 = 3$。\n- 对于模型 $\\mathcal{M}_{2}$，估计参数的数量为 $k_2 = p_2 = 5$。\n\n模型 $j$ 的对数似然是用给定的 $RSS_j$ 和已知的 $\\sigma^2=2$ 来计算的。这个值代表了最大化的似然，因为残差是先前优化过程的结果。\n$$\n\\ln(\\mathcal{L}_{max, j}) = -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{RSS_j}{2\\sigma^2}\n$$\n模型 $j$ 的 AIC 为：\n$$\nAIC(\\mathcal{M}_{j}) = -2 \\left( -\\frac{n}{2}\\ln(2\\pi\\sigma^2) - \\frac{RSS_j}{2\\sigma^2} \\right) + 2k_j = n\\ln(2\\pi\\sigma^2) + \\frac{RSS_j}{\\sigma^2} + 2k_j\n$$\n对于 $\\mathcal{M}_{1}$：\n$$\nAIC(\\mathcal{M}_{1}) = 10\\ln(2\\pi \\cdot 2) + \\frac{30}{2} + 2(3) = 10\\ln(4\\pi) + 15 + 6 = 10\\ln(4\\pi) + 21\n$$\n对于 $\\mathcal{M}_{2}$：\n$$\nAIC(\\mathcal{M}_{2}) = 10\\ln(2\\pi \\cdot 2) + \\frac{21}{2} + 2(5) = 10\\ln(4\\pi) + \\frac{21}{2} + 10 = 10\\ln(4\\pi) + \\frac{41}{2}\n$$\n差值为：\n$$\nAIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1}) = \\left(10\\ln(4\\pi) + \\frac{41}{2}\\right) - (10\\ln(4\\pi) + 21) = \\frac{41}{2} - 21 = \\frac{41 - 42}{2} = -\\frac{1}{2}\n$$\n\n**3. 未知误差方差下的 AIC**\n\n在这种情况下，误差方差 $\\sigma^2$ 未知，必须为每个模型从数据中估计。这使得每个模型的估计参数数量增加了一个。\n- 对于模型 $\\mathcal{M}_{1}$，估计参数的数量为 $k_1 = p_1 + 1 = 3 + 1 = 4$。\n- 对于模型 $\\mathcal{M}_{2}$，估计参数的数量为 $k_2 = p_2 + 1 = 5 + 1 = 6$。\n\n为了找到最大化的对数似然，我们首先为给定的 $RSS$ 找到 $\\sigma^2$ 的最大似然估计 (MLE)。我们将对数似然对 $\\sigma^2$ 求导，并令结果为零：\n$$\n\\frac{\\partial}{\\partial (\\sigma^2)} \\ln(\\mathcal{L}) = \\frac{\\partial}{\\partial (\\sigma^2)} \\left( -\\frac{n}{2}\\ln(2\\pi) -\\frac{n}{2}\\ln(\\sigma^2) - \\frac{RSS}{2\\sigma^2} \\right) = -\\frac{n}{2\\sigma^2} + \\frac{RSS}{2(\\sigma^2)^2}\n$$\n将此式设为零可得出 $\\sigma^2$ 的 MLE：\n$$\n\\frac{n}{2\\hat{\\sigma}^2} = \\frac{RSS}{2(\\hat{\\sigma}^2)^2} \\implies \\hat{\\sigma}^2 = \\frac{RSS}{n}\n$$\n现在我们将这个 $\\hat{\\sigma}^2$ 代回到对数似然表达式中，以获得最大化后的值 $\\ln(\\mathcal{L}_{max})$：\n$$\n\\ln(\\mathcal{L}_{max}) = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln\\left(\\frac{RSS}{n}\\right) - \\frac{RSS}{2(RSS/n)} = -\\frac{n}{2}\\left( \\ln(2\\pi) + \\ln\\left(\\frac{RSS}{n}\\right) + 1 \\right)\n$$\n那么模型 $j$ 的 AIC 为：\n$$\nAIC(\\mathcal{M}_{j}) = -2\\ln(\\mathcal{L}_{max, j}) + 2k_j = n\\left(\\ln(2\\pi) + \\ln\\left(\\frac{RSS_j}{n}\\right) + 1\\right) + 2k_j\n$$\n对于 $\\mathcal{M}_{1}$：$RSS_1 = 30$, $k_1 = 4$。\n$$\nAIC(\\mathcal{M}_{1}) = 10\\left(\\ln(2\\pi) + \\ln\\left(\\frac{30}{10}\\right) + 1\\right) + 2(4) = 10(\\ln(2\\pi) + \\ln(3) + 1) + 8 = 10\\ln(6\\pi) + 10 + 8 = 10\\ln(6\\pi) + 18\n$$\n对于 $\\mathcal{M}_{2}$：$RSS_2 = 21$, $k_2 = 6$。\n$$\nAIC(\\mathcal{M}_{2}) = 10\\left(\\ln(2\\pi) + \\ln\\left(\\frac{21}{10}\\right) + 1\\right) + 2(6) = 10(\\ln(2\\pi) + \\ln(2.1) + 1) + 12 = 10\\ln(4.2\\pi) + 10 + 12 = 10\\ln\\left(\\frac{21\\pi}{5}\\right) + 22\n$$\n差值为：\n$$\nAIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1}) = \\left(10\\ln\\left(\\frac{21\\pi}{5}\\right) + 22\\right) - (10\\ln(6\\pi) + 18)\n$$\n$$\n= 10\\left(\\ln\\left(\\frac{21\\pi}{5}\\right) - \\ln(6\\pi)\\right) + 4 = 10\\ln\\left(\\frac{21\\pi/5}{6\\pi}\\right) + 4 = 10\\ln\\left(\\frac{21}{30}\\right) + 4\n$$\n$$\n= 10\\ln\\left(\\frac{7}{10}\\right) + 4\n$$\n\n**讨论**\n\n当误差方差 $\\sigma^2$ 已知时，$AIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1}) = -1/2$。较小的 AIC 值表示模型更优，因此模型 $\\mathcal{M}_{2}$ 更受青睐。\n\n当 $\\sigma^2$ 未知并被估计时，$AIC(\\mathcal{M}_{2}) - AIC(\\mathcal{M}_{1}) = 4 + 10\\ln(7/10)$。由于 $\\ln(7/10)  0$，我们可以计算这个值。$\\ln(0.7) \\approx -0.3567$，所以差值约等于 $4 - 3.567 = 0.433 > 0$。在这种情况下，模型 $\\mathcal{M}_{1}$ 更受青睐。\n\n当方差成为一个被估计的参数时，模型偏好从更复杂的模型（$\\mathcal{M}_{2}$）切换到更简单的模型（$\\mathcal{M}_{1}$）。原因在于 AIC 的拟合优度项。\n- 对于已知的 $\\sigma^2$，$\\Delta AIC$ 中对数似然项的贡献为 $(RSS_2 - RSS_1)/\\sigma^2 = (21-30)/2 = -4.5$。这是对拟合改进的线性度量。\n- 对于未知的 $\\sigma^2$，对数似然项的贡献为 $n\\ln(RSS_2/RSS_1) = 10\\ln(21/30) = 10\\ln(0.7) \\approx -3.57$。对数尺度削弱了因减小 RSS 而获得的奖励。\n\n当允许每个模型估计其自身的最优方差（$\\hat{\\sigma}^2 = RSS/n$）时，较低 $RSS$ 带来的优势部分被归因于模型具有更低的内在误差水平。与两个模型都基于一个共同的、固定的方差进行评判的情况相比，这减少了模型因其更好的拟合而获得的有效“分数”。因此，对更高复杂度的惩罚（$2k$）变得更具影响力，从而导致选择了更简单的模型 $\\mathcal{M}_{1}$。\n\n这种效应在概念上与贝叶斯信息准则（BIC）有相似之处，其定义为 $BIC = -2 \\ln(\\mathcal{L}_{max}) + k \\ln(n)$。对于 $n=10$，$\\ln(10) \\approx 2.3$，因此 BIC 的复杂度惩罚项 $k\\ln(10)$ 比 AIC 的惩罚项 $2k$ 更强。BIC 本质上比 AIC 更强烈地偏好更简单的模型。在 AIC 的未知方差情况下，偏好转向更简单的模型 $\\mathcal{M}_{1}$，因此是朝向 BIC 从一开始就可能偏好的方向移动。", "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{1}{2}  4 + 10\\ln\\left(\\frac{7}{10}\\right) \\end{pmatrix}}\n$$", "id": "3403909"}, {"introduction": "对于任何实践者来说，最重要的一课是：信息准则只是工具，而非神谕。这个概念性问题探讨了一个现实场景，其中赤池信息准则（AIC）指向一个更优的模型，但残差诊断却揭示了该模型的基本假设遭到了违背。这个练习将挑战你对统计工具有效性的批判性思维，并强调残差分析在稳健建模中不可或缺的作用。[@problem_id:3403888]", "problem": "在一个数据同化工作流的反演问题中，考虑一个正向映射 $y = \\mathcal{G}(\\theta) + \\varepsilon$ 的两个模型序列 $\\mathcal{M}_1$ 和 $\\mathcal{M}_2$。其中，$y \\in \\mathbb{R}^n$ 是观测值，$\\theta \\in \\mathbb{R}^p$ 是待推断的未知参数，$\\varepsilon$ 表示观测误差。数据同化步骤使用观测误差的似然和来自 $\\theta$ 的背景先验的正则化；此处，为了模型比较，我们仅关注观测似然分量。$\\mathcal{M}_1$ 和 $\\mathcal{M}_2$ 均假设误差是独立同分布的、具有恒定方差的高斯误差，即 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$。它们的区别在于 $\\mathcal{G}(\\theta)$ 参数形式的复杂性，其中 $\\mathcal{M}_2$ 包含额外的基分量，从而增加了自由参数的数量。\n\n在这些假设下，使用 $(\\theta, \\sigma^2)$ 的最大似然估计，从 $\\mathcal{M}_1$ 转换到 $\\mathcal{M}_2$ 时，根据拟合后的似然计算出的赤池信息准则 (Akaike Information Criterion, AIC) 减少了 $25$ 个单位，而贝叶斯信息准则 (Bayesian Information Criterion, BIC) 增加了 $5$ 个单位。然而，与 $\\mathcal{M}_1$ 相比，$\\mathcal{M}_2$ 的标准化残差诊断表现出以下变化：样本一阶自相关从 $0.20$ 增加到 $0.60$，峰度从大约 $3.0$ 增加到 $5.5$，并且 Portmanteau 白噪声检验在 $1\\%$ 的显著性水平上拒绝了 $\\mathcal{M}_2$ 的独立同分布高斯假设，但没有拒绝 $\\mathcal{M}_1$ 的。相对于 $\\mathcal{M}_1$，$\\mathcal{M}_2$ 下的数据同化新息（观测值减去模型预测值）也显示出统计上显著的序列相关性。\n\n在此反演问题和数据同化背景下，下降的 $ \\mathrm{AIC} $ 值伴随着恶化的残差诊断，可能表明模型设定错误，而非预测拟合度的真实改善。哪个选项能最好地解释这一现象？\n\nA. 下降的 $ \\mathrm{AIC} $ 是预期预测性能改善的确凿证据；残差诊断是渐近无关的，因此无论自相关和峰度如何增加，都应首选 $\\mathcal{M}_2$。\n\nB. 在 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$ 假设下计算出的下降的 $ \\mathrm{AIC} $ 值，可能是由于对均值结构 $\\mathcal{G}(\\theta)$ 进行了过拟合，以适应序列相关和重尾的误差，从而夸大了（设定错误的）似然值。恶化的残差诊断表明独立同分布高斯假设被违反，因此 $ \\mathrm{AIC} $ 的比较是不可靠的；应当对误差协方差进行建模（例如，$\\varepsilon \\sim \\mathcal{N}(0, \\Sigma)$，其中 $\\Sigma$ 非平凡），或使用根据正确设定的似然计算出的准则。$ \\mathrm{BIC} $ 的增加与在模型设定错误下对不必要的复杂性进行更强的惩罚是一致的。\n\nC. 恶化的残差诊断明确地证明了真实的数据生成过程是非高斯的，因此任何基于高斯分布的准则都必然会失败；除非存在编码错误，否则观察到的 $ \\mathrm{AIC} $ 下降是不可能的。\n\nD. 在高斯模型中，$ \\mathrm{AIC} $ 对残差自相关和尾部行为是不变的，因此如果残差自相关增加，$ \\mathrm{AIC} $ 必然会减少，这表明拟合得更好；$ \\mathrm{BIC} $ 的增加是无关紧要的，因为它依赖于任意的先验，所以应选择 $\\mathcal{M}_2$。", "solution": "用户提供了一个关于在反演问题和数据同化背景下进行模型选择的问题。任务是验证问题陈述的有效性，如果有效，则提供详细的解决方案，包括对所有选项的评估。\n\n### 问题验证\n\n**步骤 1：提取已知信息**\n\n*   **问题背景：** 反演问题，数据同化工作流。\n*   **正向模型：** $y = \\mathcal{G}(\\theta) + \\varepsilon$，其中 $y \\in \\mathbb{R}^n$ (观测值)，$\\theta \\in \\mathbb{R}^p$ (参数)，以及 $\\varepsilon$ (观测误差)。\n*   **用于比较的模型：** $\\mathcal{M}_1$ 和 $\\mathcal{M}_2$。\n*   **误差假设 (对两个模型)：** 独立同分布 (i.i.d.) 的高斯误差，$\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$。\n*   **模型复杂度：** $\\mathcal{M}_2$ 比 $\\mathcal{M}_1$ 更复杂 (拥有更多自由参数)。\n*   **信息准则变化 ($\\mathcal{M}_1 \\to \\mathcal{M}_2$)：**\n    *   赤池信息准则 (AIC) 减少了 25 个单位：$\\mathrm{AIC}(\\mathcal{M}_2) - \\mathrm{AIC}(\\mathcal{M}_1) = -25$。\n    *   贝叶斯信息准则 (BIC) 增加了 5 个单位：$\\mathrm{BIC}(\\mathcal{M}_2) - \\mathrm{BIC}(\\mathcal{M}_1) = 5$。\n*   **估计方法：** $(\\theta, \\sigma^2)$ 的最大似然估计。\n*   **残差诊断变化 ($\\mathcal{M}_1 \\to \\mathcal{M}_2$)：**\n    *   样本一阶自相关从 $0.20$ 增加到 $0.60$。\n    *   峰度从约 $3.0$ 增加到 $5.5$。\n    *   Portmanteau 白噪声检验在 $1\\%$ 的显著性水平上拒绝了 $\\mathcal{M}_2$ 的 i.i.d. 高斯假设，但没有拒绝 $\\mathcal{M}_1$ 的。\n*   **新息：** 相对于 $\\mathcal{M}_1$，$\\mathcal{M}_2$ 的数据同化新息显示出统计上显著的序列相关性。\n*   **问题：** 解释下降的 AIC 伴随恶化的残差诊断如何可能表明模型设定错误，而非真实的改善。\n\n**步骤 2：使用提取的已知信息进行验证**\n\n*   **科学上合理：** 该问题牢固地植根于已建立的模型选择和诊断的统计理论。AIC、BIC、残差分析以及 i.i.d. 高斯误差的假设是统计学、计量经济学和数据同化中的基本概念。所描述的场景是一个经典例证，说明了在不检查模型假设的情况下仅依赖信息准则的陷阱。\n*   **适定的：** 该问题提供了一个清晰、自成体系的场景，并要求进行概念性解释。所提供的定量信息（AIC、BIC、自相关、峰度的变化）是一致且充分的，足以得出一个逻辑结论。\n*   **客观的：** 该问题使用客观的、定量的度量和既有术语进行陈述。它不包含主观或基于观点的陈述。\n\n**缺陷清单：**\n1.  **科学或事实不健全：** 无。该场景是合理的，并且在实践中经常遇到，即一个更灵活的模型对数据过拟合，将噪声结构当作信号来捕捉。\n2.  **无法形式化或不相关：** 无。该问题与科学建模中的模型选择直接相关。\n3.  **不完整或矛盾的设置：** 无。AIC 结果与残差诊断之间的表面冲突是待解释问题的核心特征，而非问题陈述本身的矛盾。\n4.  **不切实际或不可行：** 无。对于一个建模问题来说，这些数值是现实的。\n5.  **不适定或结构不良：** 无。问题要求在一组选项中选出最佳解释，这是一个定义明确的任务。\n6.  **伪深刻、琐碎或同义反复：** 无。该问题探讨了应用统计学中一个微妙但关键的问题，需要理解模型选择准则背后的假设。\n7.  **超出科学可验证性范围：** 无。所涉及的原理是统计科学中的标准原理，并且是可验证的。\n\n**步骤 3：结论与行动**\n\n问题陈述是**有效的**。我们可以继续进行解答。\n\n### 推导与选项分析\n\n**理论框架**\n\n赤池信息准则 (AIC) 和贝叶斯信息准则 (BIC) 定义如下：\n$$ \\mathrm{AIC} = -2\\ln(\\hat{L}) + 2k $$\n$$ \\mathrm{BIC} = -2\\ln(\\hat{L}) + k\\ln(n) $$\n其中 $\\hat{L}$ 是模型似然函数的最大化值，$k$ 是模型中估计参数的数量，$n$ 是观测值的数量。这两个准则都在模型拟合度（由最大化对数似然 $\\ln(\\hat{L})$ 表示）和模型复杂度（由惩罚项表示）之间进行权衡。AIC 或 BIC 的值越低越好。\n\n在 i.i.d. 高斯误差 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$ 假设下，模型 $y = \\mathcal{G}(\\theta) + \\varepsilon$ 的似然为：\n$$ L(\\theta, \\sigma^2 | y) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - [\\mathcal{G}(\\theta)]_i)^2\\right) $$\n对于给定的 $\\hat{\\theta}$，方差的最大似然估计为 $\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^n (y_i - [\\mathcal{G}(\\hat{\\theta})]_i)^2 = \\frac{\\mathrm{RSS}}{n}$，其中 $\\mathrm{RSS}$ 是残差平方和。将此代入对数似然函数可得（在相差一个加法常数的情况下）：\n$$ -2\\ln(\\hat{L}) \\propto n\\ln(\\mathrm{RSS}) $$\n因此，一个能够获得更低 RSS 的模型将具有更高的最大化对数似然和更低的 AIC 和 BIC 中的 $-2\\ln(\\hat{L})$ 项的值。\n\n**场景分析**\n\n1.  **模型拟合度与复杂度：** 我们已知 $\\mathcal{M}_2$ 比 $\\mathcal{M}_1$ 更复杂，因此其参数数量 $k_2$ 大于 $k_1$。作为一个更灵活的模型，$\\mathcal{M}_2$ 在 RSS 方面至少可以达到与 $\\mathcal{M}_1$ 一样好的拟合效果。通常，它会获得一个严格更低的 RSS，从而导致一个更高的最大化似然 $\\hat{L}_2  \\hat{L}_1$。\n\n2.  **解释 AIC 和 BIC 的变化：**\n    *   $\\mathrm{AIC}(\\mathcal{M}_2)  \\mathrm{AIC}(\\mathcal{M}_1)$：减少 25 个单位表明，$\\mathcal{M}_2$ 在拟合度上的改善（$\\ln(\\hat{L})$ 的增加）足够大，足以补偿拥有更多参数的惩罚（$2(k_2-k_1)$）。简单地看，这表明 $\\mathcal{M}_2$ 是一个更好的模型。\n    *   $\\mathrm{BIC}(\\mathcal{M}_2)  \\mathrm{BIC}(\\mathcal{M}_1)$：对于任何合理的样本量（$n \\ge 8$），BIC 对复杂度的惩罚 $(k_2-k_1)\\ln(n)$ 比 AIC 的惩罚更严格。BIC 增加 5 个单位意味着，从 BIC 的角度来看，拟合度的改善并*不*能证明增加的复杂度的合理性。AIC 和 BIC 之间的这种差异是潜在过拟合的典型信号。\n\n3.  **残差诊断的作用：** 问题的关键部分就在这里。AIC 和 BIC 作为预测性能估计量的理论依据，依赖于所设定的似然函数是正确的（或至少是一个很好的近似）这一假设。问题陈述该似然是基于 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$ 的假设。而残差诊断正是为了检验这些假设而设计的。\n    *   $\\mathcal{M}_2$ 的残差表现出高自相关（$0.60$）并且未能通过白噪声检验。这直接违反了“i.i.d.”中的**独立性**假设。\n    *   $\\mathcal{M}_2$ 的残差峰度为 $5.5$，远高于高斯分布的峰度（$3.0$）。这违反了**高斯**假设。\n    *   相比之下，$\\mathcal{M}_1$ 的残差与 i.i.d. 高斯假设更加一致。\n\n4.  **综合分析：** 更灵活的模型 $\\mathcal{M}_2$ 很可能利用其额外的参数来“拟合噪声”。真实的观测误差 $\\varepsilon$ 并非 i.i.d. 高斯分布；它们似乎是序列相关且具有重尾的。通过拥有更多参数，$\\mathcal{M}_2$ 可以扭曲其均值函数 $\\mathcal{G}(\\theta)$ 以部分吸收这种结构化噪声。这导致了更小的 RSS，从而夸大了*设定错误*的似然函数的值。这个被夸大的似然值正是导致 AIC 下降的原因。\n\n然而，由于似然函数是错误的（真实的误差不服从 $\\sim \\mathcal{N}(0, \\sigma^2 I)$），因此得到的 AIC 值是不可靠的。其作为样本外预测误差的无偏估计量的理论性质也已丧失。恶化的残差诊断是这种模型设定错误的主要证据。它们揭示了 $\\mathcal{M}_2$ 的“更好拟合”是虚假的。具有更强惩罚项的 BIC 的增加，正确地惩罚了这种“不必要的复杂性”。正确的结论不是 $\\mathcal{M}_2$ 更好，而是建模框架本身（特别是 i.i.d. 高斯误差假设）存在缺陷，而 $\\mathcal{M}_2$ 只是一个对数据过拟合的、设定错误更严重的模型。\n\n**选项评估**\n\n*   **A. 下降的 $ \\mathrm{AIC} $ 是预期预测性能改善的确凿证据；残差诊断是渐近无关的，因此无论自相关和峰度如何增加，都应首选 $\\mathcal{M}_2$。**\n    这个陈述根本上是错误的。AIC 是预测性能的一个*估计*，其有效性取决于模型假设。残差诊断是验证这些假设的关键工具。忽略来自残差的强烈的设定错误证据是统计实践中的一个重大错误。声称它们“渐近无关”是错误的。\n    **结论：错误。**\n\n*   **B. 在 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$ 假设下计算出的下降的 $ \\mathrm{AIC} $ 值，可能是由于对均值结构 $\\mathcal{G}(\\theta)$ 进行了过拟合，以适应序列相关和重尾的误差，从而夸大了（设定错误的）似然值。恶化的残差诊断表明独立同分布高斯假设被违反，因此 $ \\mathrm{AIC} $ 的比较是不可靠的；应当对误差协方差进行建模（例如，$\\varepsilon \\sim \\mathcal{N}(0, \\Sigma)$，其中 $\\Sigma$ 非平凡），或使用根据正确设定的似然计算出的准则。$ \\mathrm{BIC} $ 的增加与在模型设定错误下对不必要的复杂性进行更强的惩罚是一致的。**\n    该选项对这一现象提供了完整而准确的解释。它正确地指出了对结构化噪声的过拟合是原因，并指出这会夸大一个设定错误的似然，从而正确地得出 AIC 比较不可靠的结论，提出了适当的补救措施（改进误差结构模型），并正确地将 BIC 的增加解释为惩罚过度复杂性的标志。\n    **结论：正确。**\n\n*   **C. 恶化的残差诊断明确地证明了真实的数据生成过程是非高斯的，因此任何基于高斯分布的准则都必然会失败；除非存在编码错误，否则观察到的 $ \\mathrm{AIC} $ 下降是不可能的。**\n    这个陈述是有缺陷的。首先，统计检验不能“明确地证明”任何事情；它们提供的是反对原假设的证据。其次，更关键的是，在这种情况下 AIC 的下降“不可能”发生是错误的。如上所述，对结构化噪声的过拟合会机械地减少 RSS，从而增加设定错误的对数似然，这很容易导致计算出的 AIC 下降。这是一种已知的建模病态，而不是计算错误。\n    **结论：错误。**\n\n*   **D. 在高斯模型中，$ \\mathrm{AIC} $ 对残差自相关和尾部行为是不变的，因此如果残差自相关增加，$ \\mathrm{AIC} $ 必然会减少，这表明拟合得更好；$ \\mathrm{BIC} $ 的增加是无关紧要的，因为它依赖于任意的先验，所以应选择 $\\mathcal{M}_2$。**\n    这个选项包含多个错误。AIC 并非对残差属性“不变”；其数值是通过 RSS 直接从残差计算出来的。声称自相关增加*必然*导致 AIC 减少是毫无根据的。最后，对 BIC 的摒弃是错误的。BIC 的惩罚项并不像所暗示的那样依赖于“任意的先验”，其与 AIC 的不一致是一个非常重要的信息，不应被忽略。\n    **结论：错误。**", "answer": "$$\\boxed{B}$$", "id": "3403888"}]}