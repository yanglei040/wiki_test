{"hands_on_practices": [{"introduction": "为了减少信息泄露，可信执行环境（TEE）通常会禁用某些共享的微体系结构特性。然而，这些安全措施往往会带来性能开销。本练习旨在通过一个具体场景，量化禁用硬件预取器这一安全决策对性能的影响，通过推导缓存未命中率的变化来加深对性能与安全权衡的理解。[@problem_id:3686075]", "problem": "处理器通过在飞地（enclave）内部运行代码来实现可信执行环境（TEE），飞地会限制共享的微架构状态以减少信息泄露。为飞地考虑的一种缓解措施是禁用硬件数据预取器，否则预取器可能会在内存流量中产生可观察到的模式。考虑一个飞地工作负载，它以固定的步长模式遍历一个大数组，发出对地址 $A_n = A_0 + n s$ 的内存访问，其中 $s$ 是以字节为单位的正整步长，数组基地址 $A_0$ 与缓存行大小 $L$ 对齐。假设以下条件成立：\n\n- 缓存行大小为 $L$ 字节，$s$ 是 $L$ 的一个正整数因子，因此对于某个整数 $m \\geq 1$ 有 $L = m s$。\n- 末级缓存具有足够的有效容量和关联度，以避免此遍历工作集产生容量和冲突未命中；唯一考虑的未命中是首次访问缓存行时的强制性未命中（compulsory miss）。\n- 单行流预取器（single-line stream prefetcher）在启用时，经过一个相对于总访问次数可以忽略不计的初始预热阶段后，会识别出步长，并在此后尝试提前获取下一个缓存行。设 $\\alpha \\in [0,1]$ 表示稳态预取准确率（预取的行确实是下一个需要的行的概率），设 $\\theta \\in [0,1] 表示稳态预取及时性（预取的行在其首次被请求访问前到达的概率）。将乘积 $p = \\alpha \\theta$ 视为在预热后，对缓存行的首次访问因预取而从未命中转换成命中的概率。\n\n仅使用第一性原理（缓存未命中率的定义和所述假设），推导稳态未命中率变化量\n$$\\Delta \\lambda(s) = \\lambda_{\\text{off}}(s) - \\lambda_{\\text{on}}(s),$$\n的闭式解析表达式，其中 $\\lambda_{\\text{off}}(s)$ 是在飞地内禁用预取器时的未命中率，$\\lambda_{\\text{on}}(s)$ 是在飞地内启用流预取器时的未命中率。将最终答案表示为 $s$、$L$、$\\alpha$ 和 $\\theta$ 的函数，使用最简精确形式。不需要四舍五入，也不需要单位，因为未命中率是无量纲的。", "solution": "首先验证问题，以确保其科学上合理、定义明确，并提供了所有必要信息。\n\n**步骤1：提取已知条件**\n-   内存访问模式：$A_n = A_0 + n s$，其中 $n = 0, 1, 2, \\dots$\n-   步长 $s$：正整数，单位为字节。\n-   缓存行大小 $L$：单位为字节。\n-   基地址 $A_0$：与 $L$ 对齐。\n-   $s$ 与 $L$ 的关系：$s$ 是 $L$ 的正整数因子，因此对于某个整数 $m \\geq 1$ 有 $L = m s$。\n-   缓存假设：无容量或冲突未命中；仅考虑首次访问缓存行时的强制性未命中。\n-   预取器预热：可忽略不计。\n-   预取准确率 $\\alpha$：预取的行是下一个所需行的概率，$\\alpha \\in [0,1]$。\n-   预取及时性 $\\theta$：预取的行在其首次被请求访问前到达的概率，$\\theta \\in [0,1]$。\n-   预取有效性 $p$：强制性未命中被转换为命中的概率，定义为 $p = \\alpha \\theta$。\n-   目标：推导稳态未命中率变化量 $\\Delta \\lambda(s) = \\lambda_{\\text{off}}(s) - \\lambda_{\\text{on}}(s)$ 的闭式解析表达式。\n\n**步骤2：验证**\n该问题是计算机体系结构中一个定义明确的练习，特别是在缓存性能分析方面。它使用了一个标准的、简化的内存访问和预取模型。所有术语都得到了清晰的定义，并且明确陈述了假设（例如，无容量未命中，可忽略的预热），以便能够进行基于第一性原理的解析求解。该问题具有科学依据，是客观且自洽的。没有矛盾、歧义或事实不准确之处。因此，该问题被认定为**有效**。\n\n**步骤3：求解推导**\n\n缓存未命中率 $\\lambda$ 定义为在足够长的时间段内，缓存未命中次数与总内存访问次数的比率。\n$$\\lambda = \\frac{\\text{未命中次数}}{\\text{总访问次数}}$$\n\n我们将首先推导禁用预取器时的未命中率 $\\lambda_{\\text{off}}(s)$，然后推导启用预取器时的未命中率 $\\lambda_{\\text{on}}(s)$。\n\n**$\\lambda_{\\text{off}}(s)$ 的推导（禁用预取器）**\n禁用预取器时，缓存行为完全由请求访问流（demand access stream）决定。问题指出，唯一的未命中是强制性未命中，它发生在对缓存行的首次访问时。\n\n内存访问模式是大小为 $s$ 的线性步长。由于基地址 $A_0$ 与缓存行大小 $L$ 对齐，第一个缓存行覆盖了从 $A_0$ 到 $A_0 + L - 1$ 的字节地址。\n\n步长 $s$ 是 $L$ 的一个因子，且 $L = m s$。我们可以确定有多少次连续访问落入单个缓存行。这些访问是：\n-   $A_0 = A_0 + 0 \\cdot s$\n-   $A_1 = A_0 + 1 \\cdot s$\n-   ...\n-   $A_{m-1} = A_0 + (m-1) s$\n\n所有这 $m$ 次访问都落在第一个缓存行内，因为 $(m-1)s = L - s  L$。下一次访问是：\n-   $A_m = A_0 + m s = A_0 + L$\n\n这次访问 $A_m$ 是对*下一个*缓存行的首次访问。\n模式如下：\n-   访问 $A_0$：这是对以 $A_0$ 开始的缓存行的首次访问。这是一次强制性**未命中**。\n-   访问 $A_1, A_2, \\dots, A_{m-1}$：这些是对同一缓存行的后续访问。它们都是**命中**。\n-   访问 $A_m$：这是对以 $A_0+L$ 开始的缓存行的首次访问。这是一次强制性**未命中**。\n-   访问 $A_{m+1}, \\dots, A_{2m-1}$：这些是在第二个缓存行中的命中。\n\n这种模式会重复。每 $m$ 次连续的内存访问中，恰好有 1 次强制性未命中，随后是 $m-1$ 次命中。\n因此，稳态未命中率 $\\lambda_{\\text{off}}(s)$ 是在这个重复周期中未命中次数与总访问次数的比率：\n$$\\lambda_{\\text{off}}(s) = \\frac{1}{m}$$\n因为问题给出 $L = m s$，我们可以将 $m$ 表示为 $L$ 和 $s$ 的函数，即 $m = \\frac{L}{s}$。\n将此代入未命中率的表达式中：\n$$\\lambda_{\\text{off}}(s) = \\frac{1}{L/s} = \\frac{s}{L}$$\n\n**$\\lambda_{\\text{on}}(s)$ 的推导（启用预取器）**\n启用流预取器后，它会尝试在请求访问之前提前获取下一个缓存行。问题指出，一次强制性未命中（会发生在对新缓存行的首次访问时）有 $p = \\alpha \\theta$ 的概率被转换为命中。这意味着预取既是准确的（获取了正确的行），又是及时的（在CPU需要它之前到达）。\n\n未命中的可能性仍然只在对新缓存行的首次访问时出现，这每 $m$ 次访问发生一次。\n让我们考虑这样一次访问。\n-   在没有预取器的情况下，这次访问是未命中。\n-   有了预取器，这次未命中以概率 $p$ 被避免。访问变成了命中。\n-   未命中以概率 $1-p$ *没有*被避免。在这种情况下，访问仍然是未命中。\n\n所以，在一个包含 $m$ 次访问的周期中，期望的未命中次数不再是 $1$，而是 $1 \\cdot (1-p)$。周期中的总访问次数仍然是 $m$。\n因此，新的未命中率 $\\lambda_{\\text{on}}(s)$ 是：\n$$\\lambda_{\\text{on}}(s) = \\frac{\\text{期望未命中次数}}{\\text{总访问次数}} = \\frac{1-p}{m}$$\n代入 $m = \\frac{L}{s}$ 和 $p = \\alpha \\theta$：\n$$\\lambda_{\\text{on}}(s) = \\frac{1 - \\alpha \\theta}{L/s} = \\frac{s(1 - \\alpha \\theta)}{L}$$\n\n**$\\Delta \\lambda(s)$ 的推导**\n问题要求计算未命中率的变化量 $\\Delta \\lambda(s) = \\lambda_{\\text{off}}(s) - \\lambda_{\\text{on}}(s)$。\n使用上面推导出的表达式：\n$$\\Delta \\lambda(s) = \\frac{s}{L} - \\frac{s(1 - \\alpha \\theta)}{L}$$\n我们可以提出公因式 $\\frac{s}{L}$：\n$$\\Delta \\lambda(s) = \\frac{s}{L} \\left( 1 - (1 - \\alpha \\theta) \\right)$$\n$$\\Delta \\lambda(s) = \\frac{s}{L} (1 - 1 + \\alpha \\theta)$$\n化简表达式得到最终结果：\n$$\\Delta \\lambda(s) = \\frac{s \\alpha \\theta}{L}$$\n这个结果代表了由预取器带来的未命中率的降低。它与步长 $s$（作为 $L$ 的一部分）以及预取器的有效性 $p = \\alpha \\theta$ 成正比。", "answer": "$$\\boxed{\\frac{s \\alpha \\theta}{L}}$$", "id": "3686075"}, {"introduction": "即使 TEE 提供了强大的内存隔离，不安全的代码编写方式仍可能通过缓存侧信道泄露机密信息。本练习将引导你分析一个常见的数据查找操作中存在的侧信道漏洞，并学习如何通过“恒定时间”（constant-time）编程技术来修复此漏洞，从而确保程序的执行不依赖于任何秘密数据。[@problem_id:3686131]", "problem": "一个程序在可信执行环境（Trusted Execution Environment, TEE）内运行，例如 Intel Software Guard Extensions (SGX) enclave。攻击者控制操作系统，可以对共享缓存执行高分辨率的计时和缓存观察攻击，例如 Prime+Probe (P+P)，但无法读取 enclave 内存。\n\n考虑一个大小为 $256\\,\\text{B}$ 的字节查找表 $T$，该表存储在一个 $4\\,\\text{KiB}$ 的页面中，对齐且连续。代码根据一个秘密字节 $s$ 和一个公开字节 $p$ 计算输出字节 $y$，其过程为 $y \\leftarrow T[x]$，其中 $x \\leftarrow s \\oplus p$。该实现采用了一个线性指令序列，其中不含对 $x$ 的条件分支。\n\n假设中央处理器（CPU）有一个一级数据缓存（L1D）和一个一级指令缓存（L1I），每个缓存的缓存行大小为 $64\\,\\text{B}$，命中延迟为 $4$ 个周期。CPU在稳态下每周期最多可以发出 $2$ 次加载和 $2$ 次简单整数操作（例如，按位与和相等性测试）。假设软件预取指令是建议性的，并且可能随时被硬件忽略。您不能假设有特殊的加密指令或向量扩展可用，也不能用代数重计算来替换表的语义；您必须保留查找表的语义。攻击者至少可以通过 Prime+Probe 在缓存行粒度上观察到 enclave 执行期间访问了 $T$ 的哪些缓存行。\n\n基于微架构侧信道和常数时间编程的基本原理，哪个选项最能准确地识别此场景中的主要泄露渠道，提出一个能在所述模型下消除该泄露的常数时间重写方法，并为每次查找相对于朴素实现 $y \\leftarrow T[x]$ 所增加的周期开销 $\\Delta C$ 提供一个合理的估算？\n\nA. 主要泄露来自 L1D，因为 $x$ 从 $4$ 个缓存行（$256\\,\\text{B}$ 对应 $64\\,\\text{B}$ 大小的行）中进行选择，而由于是线性代码，L1I 的足迹与 $x$ 无关。一个常数时间的重写方法是按固定顺序扫描所有 $256$ 个条目，并通过掩码选择来计算 $y$：初始化 $r \\leftarrow 0$，然后对于从 $0$ 到 $255$ 的每个 $i$，通过对 $i$ 和 $x$ 进行常数时间的相等性测试来计算掩码 $m_i \\in \\{0,255\\}$，并更新 $r \\leftarrow r | (T[i] \\ m_i)$，最后输出 $y \\leftarrow r$。数据缓存的足迹与 $x$ 无关，指令流保持不变。考虑到每次迭代有 $1$ 次加载和大约 $3$ 次简单整数操作，以及给定的发射限制，一旦预热，循环吞吐量约为每周期 $1$ 次迭代，产生大约 $256$ 个周期，而单次 L1D 命中约为 $4$ 个周期；因此，每次查找的 $\\Delta C \\approx 200$–$400$ 个周期。\n\nB. 主要泄露来自 L1I，因为不同的 $x$ 值会导致为地址计算而取不同的指令字节。一个常数时间的重写方法是将查找展开成类似 switch 的代码布局，以便 CPU 平均地取所有可能的指令块，使 L1I 足迹与 $x$ 无关，同时保留用于数据的单个索引加载。这产生的开销可以忽略不计，$\\Delta C \\approx 0$–$10$ 个周期。\n\nC. 主要泄露来自 L1D。一个常数时间的重写方法是为 $T$ 的所有 $4$ 个缓存行发出软件预取指令，然后执行单个索引加载 $T[x]$；因为这些行已被预取，所以行级足迹与 $x$ 无关。这消除了泄露，且开销可以忽略不计，$\\Delta C \\approx 0$–$10$ 个周期。\n\nD. 主要泄露来自 L1D。一个常数时间的重写方法是在 enclave 初始化时一次性加载 $T$ 的 $4$ 个缓存行，并假设它们保持驻留；后续的查找可以使用原始的 $T[x]$ 而不会产生泄露，因为所有行都已在缓存中。这使得足迹与 $x$ 无关，并且此后的开销 $\\Delta C \\approx 0$ 个周期。\n\n选择唯一的最佳选项。", "solution": "我们从关于威胁模型和微架构的核心定义和经过充分检验的事实出发。\n\n首先，可信执行环境 (TEE)，例如 Intel Software Guard Extensions (SGX)，将 enclave 内存与特权软件隔离开来，但与不受信任的代码共享包括缓存在内的微架构资源。Prime+Probe (P+P) 攻击者可以通过预填充缓存集、让 enclave 运行、然后探测驱逐来推断 enclave 接触了哪些缓存集，从而推断出哪些缓存行。常数时间编程原则要求执行的指令序列和访问的内存地址序列都必须与秘密无关，这样计时和缓存足迹就不会编码依赖于秘密的信息。\n\n在给定的代码中，指令流被说明为线性的，没有基于 $x$ 的条件分支。因此，从程序计数器值到指令获取的映射与 $x$ 无关。从基本原理的角度来看，一级指令缓存 (L1I) 的获取是静态指令序列的函数；由于沒有依赖于秘密的控制流，L1I 足迹不随 $x$ 变化。相比之下，一级数据缓存 (L1D) 的访问包括从 $T[x]$ 加载数据，其中 $x$ 来自秘密 $s$。查找表 $T$ 的大小为 $256\\,\\text{B}$，组织为 $4$ 个 $64\\,\\text{B}$ 大小的缓存行。对于任何给定的 $x \\in \\{0,\\dots,255\\}$，访问 $T[x]$ 会精确地接触这 $4$ 个行中的一个，具体是哪一个由 $x$ 的高位 $\\log_2(256/64)=2$ 位决定。因此，朴素代码所接觸的缓存行集合取决于 $x$，这违反了常数时间要求，并使得能够观察行级足迹的攻击者（例如，通过 Prime+Probe）至少可以了解到访问了 $4$ 个行中的哪一个。在多次查找中，这种泄露在密码学上下文中可能累积成部分或完整的密钥恢复。\n\n为了在所述约束（没有特殊的加密或向量指令，并保留表查找的语义）下缓解这种泄露，可以确保数据缓存足迹不依赖于 $x$。一个标准的常数时间模式是按固定顺序访问所有表项，并通过不带条件分支的掩码操作来选择所需的值。具体来说，初始化 $r \\leftarrow 0$。对于每个 $i \\in \\{0,\\dots,255\\}$，使用对 $i$ 和 $x$ 的常数时间相等性测试（例如，通过算术运算，如果相等则产生 $255$，否则产生 $0$）来计算掩码 $m_i \\in \\{0,255\\}$，并更新 $r \\leftarrow r | (T[i] \\ m_i)$。最后，$r$ 等于 $T[x]$。因为无论 $x$ 是什么，循环都以相同的顺序接触每个 $T[i]$，所以数据地址集以及因此产生的缓存行足迹是恒定的。\n\n我们使用提供的微架构参数来估计开销 $\\Delta C$。在 L1D 命中的稳态下，每次迭代大约执行 $1$ 次从 $T[i]$ 的加载和大约 $3$ 次简单整数操作（产生 $m_i$ 的相等性测试、一次按位与和一次按位或）。CPU 每周期最多可以发出 $2$ 次加载和 $2$ 次简单整数操作。因此，循环体每次迭代大约有 $4$ 个微操作，一旦流水线充满，其吞吐量受限于接近每周期 $1$ 次迭代，因为每次迭代的 $1$ 次加载和每周期最多 $2$ 次整数操作可以在连续的迭代中重叠执行；只要不同迭代之间有足够的独立工作，L1D 的 $4$ 周期命中延迟就可以被流水线隐藏，而这里确实存在这种情况，因为每次迭代读取不同的地址，数据依赖关系仅限于运行中的累加器。因此，扫描 $256$ 个条目大约需要 $256$ 个周期。相比之下，朴素查找在最佳情况下执行一次 L1D 命中，大约需要 $4$ 个周期，附加的整数运算工作可以忽略不计；因此，每次查找的额外开销是\n$$\n\\Delta C \\approx 256 - 4 \\approx 252 \\text{ cycles,}\n$$\n考虑到循环开销、前端效应以及相等性掩码实现中的差异，我们可以保守地将其框定在约 $200$–$400$ 个周期。两种版本的指令流都保持线性，因此 L1I 的行为仍然与 $x$ 无关。\n\n现在我们分析每个选项。\n\n选项 A：它正确地将 L1D 识别为主要泄露源，因为数据地址 $T[x]$ 依赖于 $x$；由于是线性指令序列，L1I 足迹与 $x$ 无关。提议的常数时间重写——带掩码选择的完全线性扫描——确保了独立于 $x$ 的固定内存访问模式，并保留了表的语义。开销估算基于给定的发射限制是合理的：$256$ 次迭代大约每周期 $1$ 次迭代，而单次 L1D 命中约为 $4$ 个周期，得出 $\\Delta C$ 在几百个周期的数量级上。结论：正确。\n\n选项 B：它断言 L1I 是主导因素，声称基于 $x$ 会取不同的指令字节。对于线性代码且没有依赖于秘密的控制流，指令获取地址不依赖于 $x$；因此 L1I 足迹不是泄露向量。此外，用类似 switch 的布局替换表并不能解决 $T[x]$ 是一个数据依赖加载的核心问题；保留那个单一的索引加载会保留 L1D 泄露。关于可忽略开销的说法无关紧要，因为在所述威胁模型下，该缓解措施是无效的。结论：错误。\n\n选项 C：它承认 L1D 泄露，但提出通过软件预取所有 $4$ 个行，然后进行数据依赖的加载来消除泄露，且开销可忽略。然而，预取指令是建议性的，可能会被忽略；问题明确指出了这一点，这意味着依赖预取来保证安全性不满足常数时间要求。如果预取被丢弃，或者如果争用导致行被驱逐，随后的数据依赖加载会再次产生一个依赖于秘密的足迹和计时。此外，即使预取被执行，代码仍然执行一个依赖于秘密的内存访问指令；一个鲁棒的常数时间准则会避免在关键路径上出现任何依赖于秘密的内存地址。结论：错误。\n\n选项 D：它建议在 enclave 初始化时一次性加载 $4$ 个缓存行，并假设它们保持驻留，之后 $T[x]$ 就是安全的。这违反了攻击者模型：操作系统可以随时调度冲突的工作负载来驱逐缓存行，并且在调用之间甚至在有争用的调用内部，都不能保证缓存驻留。在存在主动攻击者的情况下，安全性不能依赖于关于持久缓存驻留的假设。此外，即使缓存行驻留，发出 $T[x]$ 仍然构成一个依赖于秘密的数据访问；虽然足迹可能保持在 $4$ 个行的集合内，但攻击者原则上仍然可以通过 Prime+Probe 观察到每次调用的接触情况。结论：错误。\n\n因此，唯一一个正确识别泄露源，在约束条件下提供了有效的常数时间重写，并对 $\\Delta C$ 进行了合理估算的选项是 A。", "answer": "$$\\boxed{A}$$", "id": "3686131"}, {"introduction": "TEE 的安全性不仅取决于飞地（enclave）内部的代码，还受到不可信操作系统对底层系统配置的影响。本练习探讨了一个微妙的系统级漏洞：性能优化特性“大页”（huge pages）如何与共享且无标签的页表结构缓存相互作用，从而绕过其他隔离机制，意外地制造出新的侧信道。[@problem_id:3686081]", "problem": "一个 $64$ 位系统上的可信执行环境 (TEE) enclave 使用带有页表隔离的分层分页机制：该 enclave 使用其自己的控制寄存器 $CR3$ 和一个 enclave 特定的地址空间标识符 (ASID)，记为 $ASID_e$。处理器实现的转译后备缓冲器 (TLB) 条目带有进程上下文标识符 (PCID) 或 ASID 标签，因此来自非 enclave 上下文的 TLB 条目不会用于 enclave 的地址翻译。内存管理单元 (MMU) 在 $x86$-$64$ 架构上为 $4$ KB 页面执行一个穿越 $4$ 级页表的页表遍历：页映射级别 4 (PML$4$)、页目录指针表 (PDPT)、页目录 (PD) 和页表 (PT)。对于更大的页面大小 $P=2^m$，页表遍历会提前终止，并启用页面大小位，因此对于 $2$ MB 页面 ($m=21$)，遍历在 PD 级别停止；对于 $1$ GB 页面 ($m=30$)，遍历在 PDPT 级别停止。\n\n假设处理器具有微架构分页结构缓存，用于存储最近使用的分页结构条目（例如，缓存的 PML$4$E、PDPTE 和 PDE 条目）。这些分页结构缓存在地址空间和核心之间共享，并且不带 ASID 标签。enclave 外部的攻击者进程可以通过计时其自身地址空间中的地址翻译，来对这些分页结构缓存进行预置和探测攻击。操作系统是不可信的，它控制页面分配以及 enclave 是接收小页面还是大页面。enclave 的目标是最小化通过基于翻译的微架构侧信道的信息泄露，同时保持可接受的性能。\n\n从第一性原理出发，考虑以下事实：\n- 一个虚拟地址由一个页内偏移量和一系列用于在每个分页级别选择条目的索引组成。如果页面大小为 $P=2^m$，则页内偏移量占用虚拟地址的 $m$ 个低位比特。\n- 对于 $4$ KB 页面 ($m=12$)，页表遍历使用所有 $4$ 个级别 (PML$4$/PDPT/PD/PT)。对于 $2$ MB 页面 ($m=21$)，页表遍历使用 $3$ 个级别 (PML$4$/PDPT/PD)。对于 $1$ GB 页面 ($m=30$)，页表遍历使用 $2$ 个级别 (PML$4$/PDPT)。\n- 分页结构缓存，当共享且不带标签时，允许跨上下文的预置和探测攻击：如果攻击者预置一个 PDE 或 PDPTE，那么任何命中该条目的 enclave 访问都将减少翻译延迟，攻击者可以据此进行统计推断。\n\n构建并分析以下场景：enclave 在一个大小为 $P=2^m$ 的单个大页内均匀访问内存，而攻击者在共享的分页结构缓存中预置相应的上层分页条目。推导 $m$ 的选择如何影响页错误的频率（有利于减少来自操作系统的页错误侧信道）以及由于上层条目重用导致的翻译级别侧信道的放大效应。基于此分析，选择关于是否应在 enclave 中允许使用大页的合理策略。\n\n在此模型下，哪项策略最正确？\n\nA. 无条件允许大页；通过独立的 $CR3$ 和 $ASID_e$ 实现的页表隔离确保了隔离性，并且由于 TLB 带有标签，分页结构缓存无法泄露信息。\n\nB. 仅当处理器强制实施每个 enclave 的标签化或在 enclave 进入和退出时完全刷新所有与翻译相关的微架构缓存（包括分页结构缓存）时，才允许使用大页；否则，在 enclave 中禁止使用大页。\n\nC. 禁止使用大页，因为它们会导致架构别名，即同一个物理帧必须在 enclave 和非 enclave 上下文之间共享，即使 $CR3$ 和 $ASID_e$ 不同，这也会破坏隔离性。\n\nD. 允许使用大页，但在启动时随机化 enclave 的虚拟地址；仅靠随机化就能阻止攻击者跨上下文预置相关的分页结构缓存条目。\n\nE. 允许大小不超过 $P \\le 2^{21}$ 的大页，但禁止 $P=2^{30}$ 的大页；$2$ MB 的页面是安全的，而 $1$ GB 的页面本质上是不安全的，这与翻译缓存的标签化或刷新无关。", "solution": "用户希望我扮演一位严谨的STEM教授，分析一个关于在可信执行环境（TEE）中使用大页的安全影响的问题。我必须首先验证问题陈述的有效性，然后如果它有效，就从第一性原理出发推导出正确的策略，并评估给定的选项。\n\n### 问题验证\n\n**步骤1：提取已知条件**\n\n问题陈述提供了以下信息和约束：\n- **系统架构：**一个 $64$ 位系统，带有一个 TEE enclave。\n- **分页和隔离：**使用分层分页。enclave 拥有自己的页表基址寄存器 ($CR3$) 和一个 enclave 特定的地址空间标识符 ($ASID_e$)。此机制被称为页表隔离。\n- **转译后备缓冲器 (TLB)：** TLB 条目由进程上下文标识符 (PCID) 或 ASID 标记。这可以防止非 enclave 上下文的 TLB 条目被用于 enclave 地址翻译。\n- **内存管理单元 (MMU)：** MMU 在 $x86$-$64$ 上为 $4$ KB 页面执行穿越 $4$ 级页表的页表遍历。这些级别是 PML$4$、PDPT、PD 和 PT。\n- **大页：**对于更大的页面大小 $P=2^m$，页表遍历会提前终止。\n  - 对于 $2$ MB 页面 ($m=21$)，遍历使用 $3$ 个级别 (PML$4$/PDPT/PD) 并在页目录 (PD) 级别停止。\n  - 对于 $1$ GB 页面 ($m=30$)，遍历使用 $2$ 个级别 (PML$4$/PDPT) 并在页目录指针表 (PDPT) 级别停止。\n- **微架构缓存：**处理器拥有分页结构缓存，用于存储最近使用的分页结构条目（例如，PML$4$E、PDPTE、PDE）。\n- **漏洞：**这些分页结构缓存在地址空间和核心之间共享，并且**不**带 ASID 标签。\n- **威胁模型：**\n  - 在 enclave 外部运行的攻击者进程可以通过计时地址翻译来对这些共享的分页结构缓存执行预置和探测攻击。\n  - 操作系统 (OS) 是不可信的，它控制着页面分配，包括决定为 enclave 使用小页还是大页。\n- **enclave 的目标：**在保持可接受性能的同时，最小化通过基于翻译的侧信道造成的信息泄露。\n- **要分析的场景：**一个 enclave 在一个大小为 $P=2^m$ 的单个大页内均匀访问内存。攻击者在共享缓存中预置相应的上层分页条目。分析应考虑页面大小 $m$ 如何影响页错误频率和翻译级别侧信道的放大效应。\n\n**步骤2：使用提取的已知条件进行验证**\n\n- **科学上是否成立：**该问题牢固地植根于现代计算机体系结构和安全的原理。对 $x86$-$64$ 分页、ASID/PCID、TLB、大页和微架构侧信道（特别是对共享、无标签缓存如 PDE 缓存的基于争用的攻击）的描述是准确的，并反映了真实的处理器设计和已知的漏洞（例如，Foreshadow、Spectre 变体）。\n- **问题是否定义良好：**问题定义良好。它定义了一个清晰的硬件和威胁模型，指定了一个目标（平衡性能与安全），并要求从分析所提供的场景中推导出一个策略。可以通过基于给定条件的逻辑推导得出唯一且有意义的结论。\n- **客观性：**在问题设置中，语言是技术性的、精确的，并且没有主观论断。\n- **自洽性和一致性：**问题提供了所有必要的信息。核心矛盾被明确指出：TLB 是带标签的，对此攻击是安全的，但分页结构缓存是无标签的，因此是脆弱的。这是问题的核心，而非不一致之处。页表遍历终止的细节与 $x86$-$64$ 架构是一致的。\n- **不切实际或不可行：**该场景非常现实。对共享、无标签的微架构结构的攻击是安全研究的一个主要领域，所描述的条件代表了真实系统。\n- **定义不清或结构不良：**术语是计算机体系结构和系统安全领域的标准术语。问题的结构旨在引导对一个明确定义的权衡进行特定分析。\n\n**步骤3：结论与行动**\n\n问题陈述有效。它在科学上是合理的、自洽的并且定义良好。我将继续推导解决方案。\n\n### 推导正确策略\n\n问题的核心在于两种不同类型的翻译缓存的特性：TLB 和分页结构缓存。\n\n1.  **TLB 的作用：** TLB 缓存从虚拟页号到物理帧号的最终映射。问题指出 TLB 条目由 ASID 标记。这意味着如果一个拥有 $ASID_a$ 的攻击者进程访问虚拟地址 $V$，而一个拥有 $ASID_e$ 的 enclave 进程访问同一个虚拟地址 $V$，它们的翻译将在 TLB 中占据不同的条目。因此，TLB 本身不会在这些安全域之间泄露信息。使用独立的 $CR3$ 和带标签的 $ASID_e$ 在 TLB 级别有效地隔离了地址空间。\n\n2.  **分页结构缓存的作用：**在翻译结果可以被放入 TLB 之前（在 TLB 未命中时），MMU 必须执行页表遍历。这涉及从内存中的分层页表（PML$4$E、PDPTE、PDE、PTE）读取条目。为了加速此过程，处理器实现了用于这些中间页表条目的缓存。问题关键性地指出，这些**分页结构缓存跨核心共享且不带 ASID 标签**。这是根本的漏洞。\n\n3.  **攻击机制 (预置+探测)：**因为分页结构缓存是共享且无标签的，攻击者可以利用它们。\n    - **预置 (Prime)：**攻击者执行内存访问，导致其页表被遍历。其页表中的条目（例如，一个特定的 PDE）被加载到共享的分页结构缓存中。\n    - **Enclave 执行：**CPU 切换到 enclave。enclave 访问其内存。如果此访问导致页表遍历，需要一个与攻击者预置的条目映射到同一缓存组的页表条目，那么攻击者的条目将被驱逐。不可信的操作系统可以安排攻击者和 enclave 页表的物理内存布局，以确保针对特定条目的此类冲突。\n    - **探测 (Probe)：**CPU 切换回攻击者。攻击者重新访问其内存并计时翻译。较长的时间表示分页结构缓存未命中，从而揭示了 enclave 驱逐了攻击者的条目。这泄露了 enclave 访问了一个需要该特定页表条目的虚拟地址的信息。\n\n4.  **大页的放大效应：**让我们分析页面大小如何影响这种泄露。\n    - **小页 ($4$ KB, $m=12$)：**一次完整的页表遍历涉及 4 级页表。单次访问揭示了关于一条特定遍历路径的信息（一个 PML$4$E、一个 PDPTE、一个 PDE、一个 PTE）。为了覆盖一个大的内存区域，enclave 必须访问许多不同的 $4$ KB 页面，使用许多不同的 PTE，可能还有 PDE。信息泄露是细粒度的。\n    - **大页 ($2$ MB, $m=21$)：**对于一个 $2$ MB 页面的页表遍历只涉及 $3$ 个级别 (PML$4$, PDPT, PD)。PDE 条目的页面大小 (PS) 位被设置，并直接指向 $2$ MB 的物理帧。至关重要的是，*任何*在这个大页覆盖的整个 $2$ MB 虚拟地址范围内的内存访问，在进行页表遍历时，都将使用*完全相同*的 PML$4E$、PDPTE 和 PDE。这意味着攻击者只需监控与这单个 PDE 对应的缓存状态，就能检测到在广阔的 $2$ MB 区域内的*任何*活动。侧信道被放大了：许多不同的虚拟地址被折叠成一个单一的、可观察到的微架构事件。\n    - **大页 ($1$ GB, $m=30$)：**效果更加显著。页表遍历只涉及 $2$ 个级别 (PML$4$, PDPT)。PDPTE 的 PS 位被设置，并指向 $1$ GB 的帧。*任何*在这个 $1$ GB 区域内的访问都将使用相同的 PML$4$E 和 PDPTE。攻击者可以监控与单个 PDPTE 对应的缓存状态，以检测在一个巨大的 $1$ GB 区域内的活动。这提供了一个非常粗糙但信号强度很高的侧信道。\n\n5.  **性能与安全的权衡：**\n    - **大页的性能优势：**遍历的页表级别更少，减少了翻译延迟。单个 TLB 条目可以覆盖一个大区域，极大地减少了 TLB 未命中和随后的页表遍历。这也减少了需要陷入不可信操作系统的页错误数量，这是一个性能和安全上的好处（减少了在 OS 接口的攻击面）。\n    - **大页的安全成本：**如上所述，它们放大了通过共享、无标签的分页结构缓存的信息泄露。\n\n6.  **制定合理策略：**\n    根本漏洞是共享的、无标签的硬件资源（分页结构缓存）。使用大页加剧了这一漏洞。一个合理的安全策略必须解决根本原因，或者在无法解决时，避免使用放大因素。\n    - 理想的解决方案是硬件修复：像 TLB 一样，为分页结构缓存打上 ASID 标签。这就是“每个 enclave 的标签化”所指的。\n    - 一个可行的软件/微码缓解措施是在每次进出 enclave 时刷新这些共享缓存。这确保了攻击者无法为 enclave“预置”缓存，enclave 也无法为攻击者留下一个“可探测”的状态。这打破了攻击链，但会带来性能损失。\n    - 如果这两种缓解措施都不可用，那么由放大效应带来的安全风险是巨大的。审慎的策略是避免使用放大器，这意味着在 enclave 内禁止使用大页。在这种威胁模型下，大页的性能优势不足以证明其所产生的信息通道是合理的。\n\n### 逐项分析选项\n\n**A. 无条件允许大页；通过独立的 $CR3$ 和 $ASID_e$ 实现的页表隔离确保了隔离性，并且由于 TLB 带有标签，分页结构缓存无法泄露信息。**\n这个陈述是**错误的**。它犯了一个关键错误，即混淆了 TLB 和分页结构缓存。问题明确指出，虽然 TLB 由 ASID 标记并且是安全的，但分页结构缓存*没有*标记，是漏洞的来源。由独立的 $CR3$ 和带标签的 TLB 提供的隔离不足以防止这种特定的微架构侧信道。\n\n**B. 仅当处理器强制实施每个 enclave 的标签化或在 enclave 进入和退出时完全刷新所有与翻译相关的微架构缓存（包括分页结构缓存）时，才允许使用大页；否则，在 enclave 中禁止使用大页。**\n这个陈述是**正确的**。它准确地指出了这类侧信道的两种主要缓解措施：（1）对共享资源进行标记（“每个 enclave 的标签化”）以提供硬件级别的隔离，或（2）在安全域转换时刷新共享资源以防止跨域信息流。它正确地得出结论，在没有这些根本性修复的情况下，由大页引起的放大风险严重到足以保证在安全 enclave 中不使用它们。这一策略是分析的直接和逻辑结果。\n\n**C. 禁止使用大页，因为它们会导致架构别名，即同一个物理帧必须在 enclave 和非 enclave 上下文之间共享，即使 $CR3$ 和 $ASID_e$ 不同，这也会破坏隔离性。**\n这个陈述是**错误的**。其推理存在缺陷。大页在架构上并不要求 enclave 的物理内存帧与非 enclave 上下文共享。TEE 的内存加密和隔离功能的核心目的恰恰是防止这种情况。此处的漏洞是微架构层面的（缓存争用），与翻译的*元数据*（页表条目）有关，而不是内存*数据*本身共享的架构缺陷。\n\n**D. 允许使用大页，但在启动时随机化 enclave 的虚拟地址；仅靠随机化就能阻止攻击者跨上下文预置相关的分页结构缓存条目。**\n这个陈述是**错误的**。地址空间布局随机化 (ASLR) 在此威胁模型中是一种不足的防御措施。操作系统是不可信的，负责设置页表。它知道“随机化”的地址，并可以与攻击者勾结。此外，即使有一个可信的操作系统，覆盖大区域的上层页表条目数量也非常少。攻击者可以系统地预置缓存组，以找到 enclave 使用的少数几个条目，这使得随机化成为一种弱缓解措施，而不是一种预防措施。它不能“阻止”攻击。\n\n**E. 允许大小不超过 $P \\le 2^{21}$ 的大页，但禁止 $P=2^{30}$ 的大页；$2$ MB 的页面是安全的，而 $1$ GB 的页面本质上是不安全的，这与翻译缓存的标签化或刷新无关。**\n这个陈述是**错误的**。它做出了一个武断且错误的区分。分析表明，放大机制对 $2$ MB 和 $1$ GB 的页面都存在，因为它们都依赖于上层的页表条目（分别是 PDE 和 PDPTE），而问题指出这些条目可以被缓存在脆弱的分页结构缓存中。虽然 $1$ GB 页面的放大程度更大，但 $2$ MB 页面并非根本“安全”；它们只是比 $1$ GB 页面“泄露得更少”。一个稳健的安全策略不会将一个脆弱的配置宣称为“安全”。根本缺陷影响了两者。", "answer": "$$\\boxed{B}$$", "id": "3686081"}]}