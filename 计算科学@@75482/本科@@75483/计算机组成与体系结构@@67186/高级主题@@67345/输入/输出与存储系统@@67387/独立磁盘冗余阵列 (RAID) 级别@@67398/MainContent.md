## 引言
在现代计算中，[数据存储](@entry_id:141659)系统必须在三个相互竞争的目标之间取得微妙的平衡：性能、容量和可靠性。[独立磁盘冗余阵列](@entry_id:754186)（RAID）正是一套为解决这一核心挑战而生的强大技术。它通过将多个独立的物理磁盘组合成一个逻辑单元，不仅能够提供远超单个磁盘的性能和容量，还能通过[数据冗余](@entry_id:187031)来抵御磁盘故障。然而，仅仅了解“RAID 5可以容忍一块盘故障”这样的表面定义，远不足以在复杂的真实世界场景中做出明智的设计决策。本文旨在填补这一知识鸿沟，带领读者深入探索RAID的内部工作机制、性能陷阱以及其在不同应用领域中的深远影响。

为实现这一目标，本文将分为三个核心章节。首先，在“原理与机制”中，我们将解构RAID的三大基石——分条、镜像和[奇偶校验](@entry_id:165765)，并在此基础上系统地分析从RAID 0到[RAID 10](@entry_id:754026)等各个标准级别的架构、性能特征与可靠性模型，包括对写惩罚和[不可恢复读取错误](@entry_id:756341)（URE）等高级问题的定量分析。接着，在“应用与跨学科联系”中，我们将把这些理论知识置于实践的检验之下，探讨RAID如何在数据库系统、[高性能计算](@entry_id:169980)等关键领域中发挥作用，并揭示其与分布式系统、[云计算](@entry_id:747395)乃至信息论等更广阔学科的内在联系。最后，“动手实践”部分将通过一系列精心设计的问题，挑战你运用所学知识解决实际的RAID设计与分析难题，从而巩固和深化理解。通过这一结构化的学习路径，你将不仅掌握RAID是什么，更将理解如何以及为何这样使用它。

## 原理与机制

在理解了[独立磁盘冗余阵列](@entry_id:754186)（RAID）的基本目标之后，我们现在深入探讨其实现这些目标的具体原理和核心机制。本章将从构成 RAID 系统的基本[数据冗余](@entry_id:187031)技术——分条、镜像和奇偶校验——入手，系统地分析各种标准 RAID 级别的架构、性能[特征和](@entry_id:189446)可靠性权衡。最后，我们将探讨一些高级主题，包括写性能的微妙之处、降级模式下的行为，以及在大容量驱动器时代至关重要的可靠性量化分析。

### [数据冗余](@entry_id:187031)的基本构建模块

所有 RAID 级别都是由三种基本技术或其组合构建而成的：数据分条、数据镜像和[奇偶校验](@entry_id:165765)。理解这些构建模块是掌握不同 RAID 级别设计思想和行为模式的关键。

#### 数据分条 (Data Striping)

**数据分条**是一种将逻辑上连续的数据分割成大小一致的[数据块](@entry_id:748187)（称为**条带单元**或 **stripe unit**），并将这些单元交错地存储在多个物理磁盘上的技术。当多个条带单元（通常每个磁盘一个）组合在一起时，它们形成一个**条带 (stripe)**。

分条的主要目标是提升性能。通过将数据分散到多个磁盘，一个大的 I/O 请求可以被分解成多个较小的请求，并由阵列中的多个磁盘**并行**处理。例如，一个大型文件的顺序读取可以同时从所有磁盘上读取其各自的条带单元，从而使阵列的聚合[吞吐量](@entry_id:271802)接近于所有单个磁盘[吞吐量](@entry_id:271802)之和。

RAID 0 是纯粹分条的典型代表。一个由 $n$ 个容量为 $C$ 的磁盘组成的 RAID 0 阵列，其可用容量为 $nC$，并且在理想情况下，其顺序读写[吞吐量](@entry_id:271802)可以达到单个磁盘的 $n$ 倍。然而，分条本身不提供任何[数据冗余](@entry_id:187031)。阵列中任何一个磁盘的故障都会导致整个阵列的数据丢失，因为每个文件的数据都散布在所有磁盘上。因此，RAID 0 的**容错能力 (fault tolerance)** 为 0 [@problem_id:3675059]。它适用于那些追求极致性能但对数据丢失风险不敏感的应用场景，例如临时数据处理或视频编辑的暂存空间。

#### 数据镜像 (Data Mirroring)

**数据镜像**是一种简单而强大的冗余技术，它将每一份数据完整地复制到另一个或多个物理磁盘上。最常见的实现是**双路镜像 (two-way mirroring)**，其中数据同时存在于两个磁盘上，这两个磁盘互为副本。

镜像的主要目标是提供高水平的[数据冗余](@entry_id:187031)和可靠性。当一个磁盘发生故障时，系统可以无缝地从其镜像副本继续提供服务，数据不会丢失。此外，镜像对于读取操作具有显著的性能优势。由于每个数据块都有多个副本，一个读取请求可以被发送到任何一个持有该副本的磁盘上。在一个优化的系统中，控制器可以将读取请求分派到当前最空闲的磁盘，或者在读取密集型工作负载下，让所有副本磁盘并行地服务于不同的读取请求。对于一个由两块磁盘组成的镜像对，其随机读取 IOPS 理论上可以达到单个磁盘的两倍 [@problem_id:3671454]。

RAID 1 是纯镜像的实现。然而，镜像的代价是高昂的容量开销。在一个由 $n$ 个磁盘构成的双路镜像阵列中（通常组织为 $n/2$ 个镜像对），只有一半的原始物理容量可用于存储用户数据，因此其**容量效率 (capacity efficiency)** 仅为 $0.5$ 或 $\frac{1}{2}$ [@problem_id:3671463]。

#### 奇偶校验 (Parity)

**[奇偶校验](@entry_id:165765)**是一种比镜像更具空间效率的冗余技术。它利用数学函数（通常是**异或运算 (XOR)**）来生成一组[数据块](@entry_id:748187)的冗余信息。对于一组[数据块](@entry_id:748187) $D_0, D_1, \dots, D_{k-1}$，其[奇偶校验](@entry_id:165765)块 $P$ 的计算方式如下：
$$ P = D_0 \oplus D_1 \oplus \dots \oplus D_{k-1} $$
其中 $\oplus$ 代表异或运算。

[奇偶校验](@entry_id:165765)的精妙之处在于，如果这组数据块中的任何一个（包括奇偶校验块自身）丢失或损坏，它都可以通过剩余的块重建出来。例如，如果[数据块](@entry_id:748187) $D_0$ 丢失，可以通过以下方式恢复：
$$ D_0 = P \oplus D_1 \oplus \dots \oplus D_{k-1} $$
这种机制使得我们仅需一个额外磁盘的容量（用于存储奇偶校验块）就可以保护多个数据磁盘，从而大大提高了容量效率。例如，一个由 $n$ 个磁盘组成的奇偶校验阵列，其中一个磁盘容量用于存储冗余信息，其容量效率为 $\frac{n-1}{n}$，远高于镜像的 $\frac{1}{2}$。基于[奇偶校验](@entry_id:165765)的 RAID 级别（如 RAID 3、4、5、6）正是利用了这一原理来实现容错。

### 标准 RAID 级别及其权衡

通过组合上述三种基本技术，业界定义了一系列标准的 RAID 级别。每个级别都在性能、容量和可靠性这三个维度上做出了不同的权衡。

#### RAID 0：性能至上 (Striping for Performance)

如前所述，RAID 0 采用纯粹的数据分条。它将数据块[分布](@entry_id:182848)在所有 $n$ 个磁盘上，不提供任何冗余。

- **容量**：$nC$
- **容量效率**：$1$
- **[容错](@entry_id:142190)能力**：$0$
- **性能**：由于所有磁盘可以并行处理 I/O，RAID 0 提供了最高的顺序读写吞吐量。其随机 I/O 性能也因负载可以[分布](@entry_id:182848)在多个磁盘上而得到提升。

#### RAID 1：可靠性优先 (Mirroring for Reliability)

RAID 1 采用纯粹的数据镜像。在一个包含 $n$ 个磁盘的系统中，通常会配置为 $n/2$ 个独立的镜像对。

- **容量**：$\frac{nC}{2}$ (对于双路镜像)
- **容量效率**：$\frac{1}{2}$ [@problem_id:3671463]
- **[容错](@entry_id:142190)能力**：在最坏情况下为 $1$。虽然阵列可以承受多个磁盘故障（只要这些故障发生在不同的镜像对中），但只要有一个镜像对中的两个磁盘同时发生故障，数据就会丢失。因此，根据“保证能承受的最小故障数”的严格定义，其[容错](@entry_id:142190)能力为 $1$ [@problem_id:3675059]。
- **性能**：随机读取性能极佳，因为读取请求可以被分派到镜像对中的任意一个磁盘，使得一个镜像对的读取 IOPS 理论上是单个磁盘的两倍。写入性能则与单个磁盘相当，因为数据必须同时写入到所有副本。

#### RAID 3 与 RAID 4：专用[奇偶校验](@entry_id:165765)盘

RAID 3 和 RAID 4 都使用[奇偶校验](@entry_id:165765)来实现容错，并将奇偶校验信息存储在一个专用的磁盘上。它们的主要区别在于分条的粒度。

- **RAID 3** 采用**字节级分条 (byte-level striping)**，并且要求所有磁盘的**[主轴](@entry_id:172691)同步 (synchronized spindles)**。这意味着任何 I/O 操作，无论多小，都必须同时访问所有数据磁盘。这带来了两个极端后果：对于大型顺序 I/O，所有 $n-1$ 个数据磁盘可以像一个逻辑单元一样并行传输数据，实现高达 $(n-1)B$ 的[吞吐量](@entry_id:271802)（其中 $B$ 是单盘[吞吐量](@entry_id:271802)）。然而，对于小型随机 I/O，整个阵列的机械臂必须同步移动，导致阵列一次只能服务一个随机 I/O 请求，其总 IOPS 被限制在与单个磁盘相当的水平，大约为 $I$ [@problem_id:3671448]。由于这种性能特点的局限性，RAID 3 在现代[通用计算](@entry_id:275847)中已基本被淘汰。

- **RAID 4** 采用**块级分条 (block-level striping)**，放宽了主轴同步的要求。这使得小型随机读取可以只访问单个数据磁盘，性能得到改善。然而，RAID 4 继承了一个致命缺陷：**写瓶颈 (write bottleneck)**。由于所有[奇偶校验](@entry_id:165765)信息都存储在同一个专用磁盘上，任何一次写入操作（无论是小块写还是全条带写）都必须更新这个奇偶校验盘。在高并发的随机写入负载下，这个专用的奇偶校验盘会迅速饱和，成为整个阵列的性能瓶颈 [@problem_id:3671394]。

#### RAID 5：[分布](@entry_id:182848)式[奇偶校验](@entry_id:165765)

为了解决 RAID 4 的写瓶颈问题，RAID 5 应运而生。其核心思想与 RAID 4 相同，都是块级分条和单[奇偶校验](@entry_id:165765)，但关键的改进在于它将[奇偶校验](@entry_id:165765)块**旋转[分布](@entry_id:182848) (rotated/distributed)** 在阵列中的所有磁盘上。这意味着对于不同的条带，[奇偶校验](@entry_id:165765)块位于不同的磁盘上。

- **容量**：$(n-1)C$
- **容量效率**：$\frac{n-1}{n}$ [@problem_id:3671463]
- **容错能力**：$1$
- **性能**：通过将奇偶校验更新的负载分散到所有磁盘，RAID 5 有效地消除了 RAID 4 的单点写入瓶颈，提供了良好的随机 I/O 性能。然而，RAID 5 的写入操作会引入“写惩罚”。对于一次只更新部分条带的小块写入，控制器必须执行一个**读-修改-写 (read-modify-write)** 周期：读取旧数据和旧奇偶校验，计算新奇偶校验，然[后写](@entry_id:756770)入新数据和新奇偶校验。这通常需要四次磁盘 I/O 操作，对写入性能有较大影响。

#### RAID 6：双重[奇偶校验](@entry_id:165765)

随着磁盘容量的增大，重建时间变长，在重建期间发生第二次磁盘故障的风险也随之增加。RAID 5 只能容忍一次故障，这使其在关键任务和大数据环境中显得脆弱。RAID 6 通过引入第二个独立的[奇偶校验](@entry_id:165765)方案来解决这个问题，提供了双重冗余。

- **原理**：RAID 6 通常使用两种不同的数学算法（例如，一种是标准的 XOR，另一种是基于伽罗瓦域运算的 Reed-Solomon 码）来生成两个独立的[奇偶校验](@entry_id:165765)块 ($P$ 和 $Q$)。这两个奇偶校验块同样被旋转[分布](@entry_id:182848)在所有磁盘上。
- **容量**：$(n-2)C$
- **容量效率**：$\frac{n-2}{n}$ [@problem_id:3671463]
- **[容错](@entry_id:142190)能力**：$2$。RAID 6 可以承受阵列中任意两个磁盘的同时故障而不会丢失数据 [@problem_id:3675059]。
- **性能**：由于需要计算和写入两个[奇偶校验](@entry_id:165765)块，RAID 6 的写惩罚比 RAID 5 更为严重，因此其写入性能通常更低。其主要价值在于提供了比 RAID 5 高出几个[数量级](@entry_id:264888)的可靠性。

#### 嵌套 RAID 级别：[RAID 10](@entry_id:754026)

嵌套 RAID（或混合 RAID）通过组合基本的 RAID 级别来创建更复杂的阵列。最常见的嵌套级别是 **[RAID 10](@entry_id:754026)**（也称为 RAID 1+0），它是一种**镜像条带 (stripe of mirrors)**。

- **原理**：[RAID 10](@entry_id:754026) 首先将磁盘两两配对，创建多个 RAID 1 镜像组，然后在一个 RAID 0 阵列中对这些镜像组进行分条。
- **容量**：$\frac{nC}{2}$（对于 $n$ 个磁盘的双路镜像）
- **容量效率**：$\frac{1}{2}$
- **容错能力**：最坏情况下为 $1$。它可以承受每个镜像对中一个磁盘的故障，但如果任何一个镜像对中的两个磁盘都发生故障，则整个阵列失败。
- **性能**：[RAID 10](@entry_id:754026) 结合了 RAID 0 和 RAID 1 的优点。它拥有 RAID 1 卓越的随机读取性能（因为每个镜像对的读取 IOPS 可以翻倍）和较低的写入延迟（无奇偶校验计算），同时通过在多个镜像对上分条，实现了 RAID 0 的高顺序吞吐量。在理想情况下，一个由 $n$ 个磁盘组成的 [RAID 10](@entry_id:754026) 阵列，其顺序读取吞吐量可以达到 $n$ 倍单盘[吞吐量](@entry_id:271802)，与 RAID 0 相当 [@problem_id:3675059]。

#### 总结与展望

下表总结了各种主流 RAID 级别的核心特性：

| RAID 级别 | 最小磁盘数 | 容量效率 | 容错能力 | 随机读性能 | 随机写性能 | 主要优势 | 主要劣势 |
| :--- | :---: | :---: | :---: | :---: | :---: | :--- | :--- |
| **RAID 0** | 1 | $1$ | $0$ | 优 | 优 | 最高性能 | 无冗余 |
| **RAID 1** | 2 | $1/2$ | $1$ | 极优 | 中 | 高可靠性, 读性能好 | 容量成本高 |
| **RAID 5** | 3 | $(n-1)/n$ | $1$ | 良 | 中 | 容量/性能/可靠性均衡 | 写惩罚, URE风险 |
| **RAID 6** | 4 | $(n-2)/n$ | $2$ | 良 | 可 | 极高可靠性 | 写惩罚高 |
| **[RAID 10](@entry_id:754026)** | 4 | $1/2$ | $1$ | 极优 | 优 | 高性能和高可靠性 | 容量成本高 |

这些经典 RAID 级别可以被看作是更广义的**[擦除码](@entry_id:749067) (erasure codes)** 的特例。一个 $(k, m)$ [擦除码](@entry_id:749067)将数据分割成 $k$ 个片段，并生成 $m$ 个冗余片段，使得从总共 $k+m$ 个片段中任意选取 $k$ 个即可恢复原始数据。在这种框架下，RAID 5 类似于一个 $(n-1, 1)$ 码，而 RAID 6 类似于一个 $(n-2, 2)$ 码，它们的容量效率都是 $\frac{k}{k+m}$ [@problem_id:3671463]。

### 高级性能与[可靠性分析](@entry_id:192790)

除了基本的特性权衡，对 RAID 系统的深入理解还需要对其在特定条件下的行为进行定量分析。

#### 写入性能的深层分析

##### [奇偶校验](@entry_id:165765)瓶颈

RAID 5 通过[分布](@entry_id:182848)式奇偶校验解决了 RAID 4 的固定瓶颈，但这种优势能否量化？考虑一个高并发写入的场景。假设一个阵列有 $n$ 个磁盘，并发 I/O 操作数为 $\gamma$，每个操作是写的概率为 $p$。在 RAID 4 中，所有写入操作都竞争同一个专用奇偶校验盘。而在 RAID 5 中，这些写入操作的[奇偶校验](@entry_id:165765)更新会均匀地[分布](@entry_id:182848)到所有 $n$ 个磁盘上。

我们可以构建一个[概率模型](@entry_id:265150)来计算瓶颈事件的概率 [@problem_id:3671394]。对于 RAID 4，总的奇偶校验负载集中在一个磁盘上。如果每个小块写产生两次 I/O（读旧奇偶、写新奇偶），那么当写操作数 $W$ 满足 $2W > I$（其中 $I$ 是单盘 IOPS 上限）时，瓶颈就会发生。$W$ 服从参数为 $\gamma$ 和 $p$ 的二项分布。对于 RAID 5，负载被分散了。对于任意一个磁盘，它接收到一次特定写入的奇偶校验更新的概率仅为 $p/n$。该磁盘上的[奇偶校验](@entry_id:165765)写入次数 $W_j$ 服从参数为 $\gamma$ 和 $p/n$ 的二项分布。通过比较这两个模型，可以清晰地看到，将负载分散到 $n$ 个磁盘上，极大地降低了单个磁盘因[奇偶校验](@entry_id:165765)过载而成为瓶颈的概率，这是 RAID 5 相对于 RAID 4 的核心性能优势。

##### “写洞”问题 (The "Write Hole" Problem)

在基于奇偶校验的 RAID 系统（如 RAID 5）中，更新一个[数据块](@entry_id:748187)需要两个独立的写操作：写入新数据和写入新奇偶校验。在这两个操作之间存在一个微小的时间窗口。如果此时系统突然断电，就可能出现[数据块](@entry_id:748187)已更新但奇偶校验块仍是旧值的状态。这种情况被称为**“写洞”**。当系统重启后，这个条带的数据和[奇偶校验](@entry_id:165765)是不一致的，如果未来该条带中的另一个磁盘发生故障，系统将使用错误的[奇偶校验](@entry_id:165765)信息重建出损坏的数据。

为了在没有昂贵的非易失性内存（NV[RAM](@entry_id:173159)）的情况下缓解此问题，系统可以采用内存中的**写日志 (write-intent journal)**。写入意图首先被记录在日志中，然后日志被定期（例如每隔 $\delta$ 秒）或在达到一定大小时（例如 $J$ 条目）刷写到磁盘上的持久化区域。我们可以对这个过程进行建模 [@problem_id:3671489]。假设电源故障是一个泊松过程，发生率为 $\lambda$，而写操作的脆弱窗口为 $\tau$。可以推导出，在没有保护的情况下，单次写入遭遇“写洞”的概率约为 $p_w \approx \lambda\tau$。引入周期性刷新的日志后，只有当电源故障发生在脆弱窗口内，且日志尚未被刷写到磁盘时，才会发生不可恢复的不一致。通过对日志刷新的随机时间进行平均，可以计算出不一致性发生的概率，并根据此概率来选择合适的刷新间隔 $\delta$ 和日志大小 $J$，从而在成本和[数据完整性](@entry_id:167528)之间取得平衡。

#### 降级模式下的性能

当 RAID 阵列中的一个磁盘发生故障后，阵列会进入**降级模式 (degraded mode)**。在此模式下，数据仍然可用，但系统的性能和可靠性都会下降。以 RAID 5 为例，当一个读取请求的目标数据块恰好位于已失效的磁盘上时，控制器必须执行**读取时重建 (reconstruct-on-read)**。这个过程需要读取该条带中所有其他幸存磁盘上的数据块和[奇偶校验](@entry_id:165765)块，并通过异或运算来动态地生成所需的数据。

这个过程会对幸存的磁盘造成显著的额外负载。考虑一个包含 $N$ 个磁盘的 RAID 5 阵列，其中一个已失效，并且工作负载由[均匀分布](@entry_id:194597)在所有逻辑块上的大量随机读取请求组成。在正常模式下，每个磁盘平均处理 $1/N$ 的总请求。在降级模式下，一个幸存磁盘不仅要处理发往其自身数据的请求（占总请求的 $1/N$），还必须参与为发往已失效磁盘的请求（同样占总请求的 $1/N$）所做的每一次读取时重建操作。因此，该幸存磁盘的总 I/O 负载近似变为原来的两倍。这意味着，每个幸存磁盘的平均负载都翻了一番，这会导致系统响应时间显著增加，并可能使幸存磁盘因过载而更容易发生故障。

#### 平均无数据丢失时间 (MTTDL)

**平均无数据丢失时间 (Mean Time To Data Loss, MTTDL)** 是衡量存储系统长期可靠性的关键指标。我们可以使用简化的马尔可夫模型来推导它。对于一个 RAID 5 阵列，系统可以处于三种状态：健康状态（所有磁盘正常）、降级状态（一个磁盘故障，正在重建）和数据丢失状态（在重建完成前发生第二次故障）。

从健康状态到降级状态的转换率是 $n\lambda$，其中 $n$ 是磁盘数量，$\lambda$ 是单个磁盘的年化[故障率](@entry_id:264373)。一旦进入降级状态，系统面临两种竞争事件：以速率 $\mu$ 完成重建（返回健康状态），或者以速率 $(n-1)\lambda$ 发生第二次故障（进入数据丢失状态）。$\mu$ 是重建速率，通常远大于 $\lambda$。在一次降级事件中，发生数据丢失的概率是 $\frac{(n-1)\lambda}{\mu + (n-1)\lambda}$。系统的 MTTDL 近似等于平均首次故障时间除以此概率。在 $\mu \gg (n-1)\lambda$ 的常见假设下，可以推导出 RAID 5 的 MTTDL 近似公式 [@problem_id:3671474]：
$$ \text{MTTDL}_{\text{RAID 5}} \approx \frac{\mu}{n(n-1)\lambda^2} $$
这个公式揭示了几个重要事实：MTTDL 与磁盘数量的平方成反比（磁盘越多，风险越大），与单盘[故障率](@entry_id:264373)的平方成反比，与重建速率成正比。这凸显了快速重建对于维持[系统可靠性](@entry_id:274890)的重要性。通过类似的模型，我们还可以比较不同 RAID 拓扑的可靠性。例如，对于同样数量的 $2n$ 个磁盘，配置为 $n$ 个 RAID 1 镜像对的系统与配置为 $2n-1$ 个磁盘加一个热备盘的 RAID 5 系统相比，其 MTTDL 存在一个仅与 $n$ 相关的比例因子，这说明了阵列结构本身对整体可靠性的影响 [@problem_id:3671484]。

#### 大容量磁盘时代的挑战：不可恢复的读取错误 (URE)

MTTDL 模型通常假设磁盘只有两种状态：工作或完全失效。然而，现代大容量磁盘还存在另一种更微妙的故障模式：**不可恢复的读取错误 (Unrecoverable Read Error, URE)**。这是指磁盘在尝试读取某个扇区时，即使经过多次重试和内部[纠错码](@entry_id:153794)（ECC）处理，仍然无法成功恢复数据。消费级 SATA 驱动器的 URE 率通常在每读取 $10^{14}$ 到 $10^{15}$ 位发生一次，而企业级驱动器则更低。

这个看似微小的概率在 RAID 重建时会成为一个巨大的问题。考虑一个由 $n$ 个容量为 $C$ 字节的磁盘组成的 RAID 5 阵列。当一个磁盘故障后，重建过程需要读取所有 $n-1$ 个幸存磁盘的全部内容。总共需要读取的数据量为 $(n-1)C$ 字节，即 $8C(n-1)$ 位。在如此海量的数据读取过程中，遇到至少一个 URE 的概率会变得不可忽视。设单比特 URE 概率为 $u$，则重建过程中没有遇到任何 URE 的概率为 $(1-u)^{8C(n-1)}$。因此，重建失败（即遇到至少一个 URE）的概率为 [@problem_id:3671434]：
$$ P_{\text{URE}} = 1 - (1-u)^{8C(n-1)} $$
随着磁盘容量 $C$ 的增长，这个概率会迅速趋近于 1。我们可以计算一个**临界容量 (critical capacity)** $C^{\star}$，当磁盘容量达到此值时，重建失败的概率达到一个令人无法接受的阈值（例如 $0.5$）。对于典型的 URE 率（如 $10^{-14}$），这个临界容量可能只有几 TB。这解释了为什么业界普遍认为**“RAID 5 已死”**——对于现代大容量驱动器，依赖 RAID 5 进行数据保护是极其危险的。

RAID 6 的双重奇偶校验正是为了应对这一挑战。在降级模式下（一个磁盘故障），RAID 6 的重建过程可以容忍幸存磁盘上再出现一个读取错误（无论是整个磁盘故障还是一个 URE）。我们可以对 RAID 6 的重建失败概率进行建模 [@problem_id:3675135]。一个 RAID 6 条带的重建只有在幸存的 $N-1$ 个块中出现两个或更多 URE 时才会失败。这个事件的概率大约与 $(q_{\text{block}})^2$ 成正比，其中 $q_{\text{block}}$ 是单个块的 URE 概率。相比之下，RAID 5 的失败概率与 $q_{\text{block}}$ 成正比。由于 $q_{\text{block}}$ 是一个极小的值，这导致 RAID 6 的重建可靠性比 RAID 5 高出多个[数量级](@entry_id:264888)。对于由 12 TiB 驱动器组成的 8 盘阵列，计算表明 RAID 6 的可靠性可能是 RAID 5 的上亿倍。这一巨大的差异有力地证明了在今天的大数据时代，采用双重冗余（如 RAID 6 或其他形式的[擦除码](@entry_id:749067)）对于保障数据安全是必不可少的。