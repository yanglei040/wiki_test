{"hands_on_practices": [{"introduction": "要充分利用巨页带来的性能优势，仅仅在操作系统层面启用它是远远不够的。应用程序的内存分配方式必须与操作系统的要求相匹配。本练习将引导你从第一性原理出发，探讨一个关键的先决条件——内存对齐——并设计一个能满足巨页要求的对齐感知分配器，从而显著减少TLB未命中次数。[@problem_id:3684889]", "problem": "考虑一个实现了请求分页虚拟内存的系统，其基本页面大小为 $4\\,\\mathrm{KiB}$。该操作系统启用了透明大页（Transparent Huge Pages, THP），在满足某些条件时，它可以将一组基本页面提升为大小为 $2\\,\\mathrm{MiB}$ 的大页。此处相关的主要条件是：大页区域必须是单个虚拟内存区域（VMA）内的单一虚拟连续区域，起始虚拟地址必须与 $2\\,\\mathrm{MiB}$ 边界对齐，并且该 $2\\,\\mathrm{MiB}$ 区域内的所有基本页面都必须被填充且符合条件。转译后备缓冲器（TLB）缓存页表条目；当工作集远超 TLB 的覆盖范围且访问模式为流式扫描时，一个简单且常用的模型将遍历工作集一遍所产生的 TLB 未命中次数视为与遍历的不同页面数量相等。Mebibyte (MiB) 表示 $2^{20}$ 字节，Kibibyte (KiB) 表示 $2^{10}$ 字节。\n\n一个程序使用默认分配器分配了 $N = 128$ 个数组，每个数组的大小为 $S = 1\\,\\mathrm{MiB}$。该分配器返回 $16$ 字节对齐的堆块，并通过来自内核的、粒度为 $4\\,\\mathrm{KiB}$ 的独立匿名映射来服务大请求。这些数组被顺序访问，从头到尾扫描每个数组，然后处理下一个数组，并重复此模式。该程序每遍扫描的总足迹为 $W = N \\times S = 128\\,\\mathrm{MiB}$。\n\n要求您从虚拟内存和 TLB 操作的基本原理出发，解释未对齐的分配如何阻止 THP 提升，并设计一个能为此工作负载启用 THP 的对齐感知分配器。在上述流式模型下，预测从默认分配器切换到您的对齐感知分配器时，每遍扫描的 TLB 未命中次数会减少多少。\n\n选择唯一一个既正确陈述了有效分配器设计，又在给定模型和假设下对 TLB 未命中减少量做出了定量正确预测的最佳选项。\n\nA. 未对齐会阻止 THP，因为大页需要在单个 VMA 内有 $2\\,\\mathrm{MiB}$ 对齐的区域。设计一个分配器，通过系统调用（例如，请求 $4\\,\\mathrm{MiB}$）预留至少 $2\\,\\mathrm{MiB}$ 外加冗余空间，计算该预留空间内下一个 $2\\,\\mathrm{MiB}$ 对齐的基地址，通过解除映射来裁剪掉任何前导和尾随的冗余空间，然后从此 $2\\,\\mathrm{MiB}$ 对齐的超大块中子分配两个 $1\\,\\mathrm{MiB}$ 的数组，并可选择使用建议性调用来倾向于使用 THP。在对 $W = 128\\,\\mathrm{MiB}$ 进行顺序扫描时，TLB 未命中次数从 $W / 4\\,\\mathrm{KiB} = 32768$ 下降到 $W / 2\\,\\mathrm{MiB} = 64$。\n\nB. 将分配对齐到 $64\\,\\mathrm{KiB}$ 对于 THP 来说是足够的，因为硬件可以映射子大页单元。一个确保 $64\\,\\mathrm{KiB}$ 对齐的 malloc 会将 TLB 未命中次数从 $W / 4\\,\\mathrm{KiB} = 32768$ 减少到 $W / 64\\,\\mathrm{KiB} = 2048$。\n\nC. 使用标准的对齐分配调用将每个 $1\\,\\mathrm{MiB}$ 数组放置在 $2\\,\\mathrm{MiB}$ 边界上，即使每个数组都有自己的映射，也足以促进 THP 提升，因此 TLB 未命中次数从 $32768$ 减少到 $W / 1\\,\\mathrm{MiB} = 128$。\n\nD. 当内核的后台合并器运行时，THP 提升不受对齐影响，所以正确的设计是保持 malloc 不变；每遍的 TLB 未命中次数仍然是 $32768$ 次，因为在这种情况下，决定未命中次数的是 TLB 容量，而不是对齐。\n\nE. 为避免碎片化，通过建议性调用禁用 THP 并保留 $4\\,\\mathrm{KiB}$ 页面；这会改善局部性并将 TLB 未命中次数减少到 $W / 8\\,\\mathrm{KiB} = 16384$，因为在没有大页的情况下实现了更大的查找粒度。", "solution": "问题陈述是有效的。它在科学上植根于操作系统内存管理的原理，问题设定良好，有足够的信息来推导出唯一的定量答案，并且用客观、技术性的语言表达。我将开始解答。\n\n分析分为两部分：首先，计算使用默认分配器时的 TLB 未命中次数；其次，设计一个对齐感知的分配器并计算由此产生的 TLB 未命中次数。\n\n**第一部分：默认分配器分析**\n\n默认分配器通过独立的匿名映射来服务大请求，例如对 $1\\,\\mathrm{MiB}$ 数组的请求。这意味着 $N=128$ 个数组中的每一个都可能驻留在其自己的虚拟内存区域（VMA）中。透明大页（THP）机制对于将一系列基本页面提升为大页有几个严格的要求。如问题中所述，这些要求包括：\n1.  该区域必须是单个 VMA 内的一个虚拟连续区域。\n2.  起始虚拟地址必须与 $2\\,\\mathrm{MiB}$ 边界对齐。\n3.  $2\\,\\mathrm{MiB}$ 区域内的所有基本页面都必须被填充。\n\n由于每个 $S=1\\,\\mathrm{MiB}$ 的数组位于独立的 VMA 中，因此不可能通过组合两个相邻的数组来形成一个 $2\\,\\mathrm{MiB}$ 的大页，因为这将违反第一个条件。此外，默认分配器仅保证 $16$ 字节对齐，这不满足要求 $2\\,\\mathrm{MiB}$ 对齐的第二个条件。因此，使用默认分配器时，THP 提升是不可能的。内存将使用 $4\\,\\mathrm{KiB}$ 的基本页面大小进行映射。\n\n工作集的总大小为 $W = N \\times S = 128 \\times 1\\,\\mathrm{MiB} = 128\\,\\mathrm{MiB}$。\n根据所提供的流式模型，每遍扫描的 TLB 未命中次数等于遍历的不同页面的数量。\n基本页面大小为 $P_{base} = 4\\,\\mathrm{KiB}$。\n我们有 $1\\,\\mathrm{MiB} = 2^{20}$ 字节和 $1\\,\\mathrm{KiB} = 2^{10}$ 字节。\n未命中次数计算如下：\n$$ \\text{Misses}_{\\text{default}} = \\frac{W}{P_{base}} = \\frac{128\\,\\mathrm{MiB}}{4\\,\\mathrm{KiB}} = \\frac{128 \\times 2^{20}\\,\\text{bytes}}{4 \\times 2^{10}\\,\\text{bytes}} = \\frac{2^7 \\times 2^{20}}{2^2 \\times 2^{10}} = 2^{7+20-2-10} = 2^{15} = 32768 $$\n所以，使用默认分配器，每遍有 $32768$ 次 TLB 未命中。\n\n**第二部分：对齐感知分配器的设计与分析**\n\n为了启用 THP，分配器必须满足指定的条件。由于每个数组的大小为 $S=1\\,\\mathrm{MiB}$，我们可以将它们成对分组以填充一个 $2\\,\\mathrm{MiB}$ 的区域，这正是大页的大小。一个稳健的分配器设计方案如下：\n1.  对于每对 $1\\,\\mathrm{MiB}$ 的数组，分配一个大于 $2\\,\\mathrm{MiB}$ 的虚拟内存区域，以确保在其内部可以找到一个 $2\\,\\mathrm{MiB}$ 对齐的地址。例如，通过单个 `mmap` 调用分配一个 $4\\,\\mathrm{MiB}$ 的块会创建一个单一的 VMA。\n2.  在这个较大的区域内，计算出第一个与 $2\\,\\mathrm{MiB}$ 边界对齐的虚拟地址。\n3.  从分配的起始位置到这个对齐地址之间的内存是“前导冗余空间”，而所需 $2\\,\\mathrm{MiB}$ 块末端之后的内存是“尾随冗余空间”。可以使用 `munmap` 释放这些冗余区域，以避免浪费内存。\n4.  这个过程产生一个与 $2\\,\\mathrm{MiB}$ 边界对齐的 $2\\,\\mathrm{MiB}$ VMA。\n5.  然后，分配器可以从此块中为两个数组返回两个 $1\\,\\mathrm{MiB}$ 的指针。\n\n当程序顺序扫描这两个数组时，这个 $2\\,\\mathrm{MiB}$ 对齐区域内的所有组成的 $4\\,\\mathrm{KiB}$ 基本页面都将被置入并填充。由于现在所有条件（单一 VMA、 $2\\,\\mathrm{MiB}$ 对齐、完全填充）都已满足，操作系统的 THP 机制可以成功地将此区域提升为单个 $2\\,\\mathrm{MiB}$ 的大页。\n\n采用这种新的分配策略后，整个 $W = 128\\,\\mathrm{MiB}$ 的工作集都由大页映射。大页大小为 $P_{huge} = 2\\,\\mathrm{MiB}$。\n现在每遍的 TLB 未命中次数为：\n$$ \\text{Misses}_{\\text{aligned}} = \\frac{W}{P_{huge}} = \\frac{128\\,\\mathrm{MiB}}{2\\,\\mathrm{MiB}} = \\frac{128 \\times 2^{20}\\,\\text{bytes}}{2 \\times 2^{20}\\,\\text{bytes}} = \\frac{128}{2} = 64 $$\n切换到对齐感知分配器后，TLB 未命中次数从 $32768$ 减少到 $64$。\n\n**逐项分析**\n\nA. **未对齐会阻止 THP，因为大页需要在单个 VMA 内有 $2\\,\\mathrm{MiB}$ 对齐的区域。设计一个分配器，通过系统调用（例如，请求 $4\\,\\mathrm{MiB}$）预留至少 $2\\,\\mathrm{MiB}$ 外加冗余空间，计算该预留空间内下一个 $2\\,\\mathrm{MiB}$ 对齐的基地址，通过解除映射来裁剪掉任何前导和尾随的冗余空间，然后从此 $2\\,\\mathrm{MiB}$ 对齐的超大块中子分配两个 $1\\,\\mathrm{MiB}$ 的数组，并可选择使用建议性调用来倾向于使用 THP。在对 $W = 128\\,\\mathrm{MiB}$ 进行顺序扫描时，TLB 未命中次数从 $W / 4\\,\\mathrm{KiB} = 32768$ 下降到 $W / 2\\,\\mathrm{MiB} = 64$。**\n该选项正确地指出了默认分配器导致 THP 失败的原因。所提出的分配器设计是合理、实用的，并直接解决了 THP 提升的要求。定量分析也是正确的，计算出初始未命中次数为 $32768$ 次，最终未命中次数为 $64$ 次。\n**结论：正确。**\n\nB. **将分配对齐到 $64\\,\\mathrm{KiB}$ 对于 THP 来说是足够的，因为硬件可以映射子大页单元。一个确保 $64\\,\\mathrm{KiB}$ 对齐的 malloc 会将 TLB 未命中次数从 $W / 4\\,\\mathrm{KiB} = 32768$ 减少到 $W / 64\\,\\mathrm{KiB} = 2048$。**\n该选项不正确。问题明确指出大页大小为 $2\\,\\mathrm{MiB}$ 且需要 $2\\,\\mathrm{MiB}$ 对齐。$64\\,\\mathrm{KiB}$ 的对齐是不够的。关于硬件可以映射任意“子大页单元”的前提，既没有得到问题陈述的支持，也不符合典型的 x86-64 架构，该架构具有固定的页面大小（$4\\,\\mathrm{KiB}$、$2\\,\\mathrm{MiB}$、$1\\,\\mathrm{GiB}$）。最终未命中次数的计算基于一个不存在的 $64\\,\\mathrm{KiB}$ 页面大小。\n**结论：不正确。**\n\nC. **使用标准的对齐分配调用将每个 $1\\,\\mathrm{MiB}$ 数组放置在 $2\\,\\mathrm{MiB}$ 边界上，即使每个数组都有自己的映射，也足以促进 THP 提升，因此 TLB 未命中次数从 $32768$ 减少到 $W / 1\\,\\mathrm{MiB} = 128$。**\n该选项不正确，原因有二。首先，它错误地声称 THP 可以在独立的映射（VMA）之间工作，这与一个给定的条件相矛盾。其次，将单个 $1\\,\\mathrm{MiB}$ 数组放置在 $2\\,\\mathrm{MiB}$ 对齐的区域内，并不能保证整个 $2\\,\\mathrm{MiB}$ 区域都会被填充，而这是 THP 的另一个条件。其定量预测使用了一个虚构的 $1\\,\\mathrm{MiB}$ 页面大小。\n**结论：不正确。**\n\nD. **当内核的后台合并器运行时，THP 提升不受对齐影响，所以正确的设计是保持 malloc 不变；每遍的 TLB 未命中次数仍然是 $32768$ 次，因为在这种情况下，决定未命中次数的是 TLB 容量，而不是对齐。**\n该选项从根本上是错误的。对齐是 THP 的一个严格的、不可协商的前提条件。内核的合并守护进程（`khugepaged`）无法在分配后修复对齐问题；它只能提升那些已经正确对齐和结构化的区域。断言对齐无关紧要的说法是错误的。\n**结论：不正确。**\n\nE. **为避免碎片化，通过建议性调用禁用 THP 并保留 $4\\,\\mathrm{KiB}$ 页面；这会改善局部性并将 TLB 未命中次数减少到 $W / 8\\,\\mathrm{KiB} = 16384$，因为在没有大页的情况下实现了更大的查找粒度。**\n该选项不合逻辑。禁用 THP 会强制使用 $4\\,\\mathrm{KiB}$ 页面，这与现状相同。这样做不可能减少 TLB 未命中。声称这能实现 $8\\,\\mathrm{KiB}$ 的“更大查找粒度”是自相矛盾且物理上荒谬的。页面大小仍将是 $4\\,\\mathrm{KiB}$。\n**结论：不正确。**", "answer": "$$\\boxed{A}$$", "id": "3684889"}, {"introduction": "在了解了如何使应用程序代码适配巨页后，下一个关键问题是：何时应该使用它们？巨页并非万能灵药，其性能收益高度依赖于工作负载的访问模式。本练习提供了一个基于真实实验数据的场景，要求你分析不同透明巨页 (THP) 策略（`always`、`never`、`madvise`）对顺序访问和随机访问两种典型工作负载的影响，旨在让你深入理解在追求高吞吐量和低尾延迟之间的权衡。[@problem_id:3684922]", "problem": "一个运行 Linux 操作系统 (OS) 的系统支持透明大页 (Transparent Huge Pages, THP)，可通过位于 /sys/kernel/mm/transparent_hugepage/enabled 的 sysfs 旋钮进行配置，该旋钮有三种模式：always、never 和 madvise。THP 会在可能的情况下，将大小为 $p=4\\,\\mathrm{KiB}$ 的常规基页提升为大小为 $P=2\\,\\mathrm{MiB}$ 的大页（也称为巨页）。中央处理器 (CPU) 依赖转译后备缓冲器 (Translation Lookaside Buffer, TLB) 来缓存虚拟到物理地址的转换；使用更大的页面可以减少覆盖一个工作集所需的独立页面转换数量。大页尺寸与基页尺寸之比为 $r = \\dfrac{P}{p} = \\dfrac{2\\,\\mathrm{MiB}}{4\\,\\mathrm{KiB}} = 512$，因此每个大页覆盖 512 个基页。\n\n你需要设计并分析一个实验，该实验调整 THP 并测量一个内存密集型基准测试的吞吐量 $T$ 和延迟 $L$。该基准测试有两个阶段，并在具有足够随机存取存储器 (RAM) 的机器上运行，因此不会发生交换。\n\n- 阶段 1（顺序流式读取）：基准测试分配一个 $W=8\\,\\mathrm{GiB}$ 的匿名内存工作集，并以 $s=64\\,\\mathrm{B}$ 的步长在整个区域上执行顺序读取。吞吐量 $T$ 以 $\\mathrm{GiB/s}$ 为单位测量，每次访问的延迟中位数 $L_{50}$ 以 $\\mathrm{ns}$ 为单位测量。在 madvise 模式下，基准测试在分配的区域上调用建议性系统调用 madvise 并附带 MADV_HUGEPAGE；否则不调用。测量结果：\n  - always: $T_{\\mathrm{always}} = 12.0\\,\\mathrm{GiB/s}$, $L_{50,\\mathrm{always}} = 140\\,\\mathrm{ns}$\n  - madvise: $T_{\\mathrm{madvise}} = 11.3\\,\\mathrm{GiB/s}$, $L_{50,\\mathrm{madvise}} = 160\\,\\mathrm{ns}$\n  - never: $T_{\\mathrm{never}} = 9.4\\,\\mathrm{GiB/s}$, $L_{50,\\mathrm{never}} = 210\\,\\mathrm{ns}$\n\n- 阶段 2（随机指针追逐）：基准测试分配同样大小的 $W=8\\,\\mathrm{GiB}$ 工作集，并使用 $n=32$ 个线程执行指针追逐；每个操作都解引用一个随机分布在 $W$ 区域内的指针。吞吐量 $T$ 以 $\\mathrm{ops/s}$ (每秒操作数) 为单位测量，第 $99$ 百分位延迟 $L_{99}$ 以 $\\mathrm{ms}$ 为单位测量。在 madvise 模式下，基准测试再次使用 MADV_HUGEPAGE。测量结果：\n  - always: $T_{\\mathrm{always}} = 3.0 \\times 10^{6}\\,\\mathrm{ops/s}$, $L_{99,\\mathrm{always}} = 4.2\\,\\mathrm{ms}$\n  - madvise: $T_{\\mathrm{madvise}} = 3.1 \\times 10^{6}\\,\\mathrm{ops/s}$, $L_{99,\\mathrm{madvise}} = 2.6\\,\\mathrm{ms}$\n  - never: $T_{\\mathrm{never}} = 3.0 \\times 10^{6}\\,\\mathrm{ops/s}$, $L_{99,\\mathrm{never}} = 2.4\\,\\mathrm{ms}$\n\n假设有一个传统的 TLB，其对基页和大页的容量都有限（具体数量未给出），并且 always 模式下的透明大页 (THP) 会尝试进行后台内存规整 (compaction)，以便为大页创建物理上连续的内存（通过 khugepaged 和 kcompactd 等内核线程），这可能会引入偶发的停顿。交换和 I/O 不是影响因素。\n\n基于分页的基本定义和 TLB 的作用，推断大页在顺序访问和随机访问下对 TLB 未命中的预期影响，以及 THP 内存规整的潜在开销。对于此基准测试，哪项策略建议最符合测量结果和第一性原理分析，以最大化吞吐量 $T$ 同时保持较低的尾延迟 $L$？\n\nA. 对所有工作负载使用 always 模式，因为无论访问模式如何，大页总是通过减少页错误和 TLB 未命中来提高吞吐量和降低延迟。\n\nB. 对顺序流式读取使用 madvise 模式（配合 MADV_HUGEPAGE），对随机指针追逐使用 never 模式；大页在流式读取中减少了 TLB 未命中和页表遍历，但 always 模式会引入后台内存规整停顿，从而在随机工作负载中抬高 $L_{99}$。\n\nC. 对所有工作负载使用 never 模式，因为大页会增加缓存污染，从而普遍降低吞吐量。\n\nD. 仅在 RAM 面临内存压力时使用 always 模式，因为 THP 内存规整主要有利于交换行为，因此会降低随机指针追逐的延迟。", "solution": "用户要求对一个有关 Linux 操作系统中透明大页 (THP) 的问题进行批判性分析，并给出解决方案。\n\n### 问题验证\n\n首先，我必须根据指定标准验证问题陈述。\n\n#### 第 1 步：提取已知信息\n\n-   **系统：** 带透明大页 (THP) 的 Linux 操作系统。\n-   **THP 模式：** `always`、`never`、`madvise`，可通过 `/sys/kernel/mm/transparent_hugepage/enabled` 配置。\n-   **页面大小：**\n    -   基页大小：$p=4\\,\\mathrm{KiB}$。\n    -   大页大小：$P=2\\,\\mathrm{MiB}$。\n-   **页面大小比率：** $r = \\dfrac{P}{p} = \\dfrac{2\\,\\mathrm{MiB}}{4\\,\\mathrm{KiB}} = 512$。\n-   **硬件与环境：**\n    -   带转译后备缓冲器 (TLB) 的 CPU。\n    -   足够的 RAM，不会发生交换。\n-   **基准测试工作集：** $W=8\\,\\mathrm{GiB}$ 的匿名内存。\n-   **基准测试阶段 1 (顺序流式读取)：**\n    -   访问模式：步长为 $s=64\\,\\mathrm{B}$ 的顺序读取。\n    -   指标：吞吐量 $T$ ($\\mathrm{GiB/s}$)，延迟中位数 $L_{50}$ ($\\mathrm{ns}$)。\n    -   `madvise` 用法：基准测试调用 `madvise` 并附带 `MADV_HUGEPAGE`。\n    -   测量结果：\n        -   `always`: $T_{\\mathrm{always}} = 12.0\\,\\mathrm{GiB/s}$, $L_{50,\\mathrm{always}} = 140\\,\\mathrm{ns}$。\n        -   `madvise`: $T_{\\mathrm{madvise}} = 11.3\\,\\mathrm{GiB/s}$, $L_{50,\\mathrm{madvise}} = 160\\,\\mathrm{ns}$。\n        -   `never`: $T_{\\mathrm{never}} = 9.4\\,\\mathrm{GiB/s}$, $L_{50,\\mathrm{never}} = 210\\,\\mathrm{ns}$。\n-   **基准测试阶段 2 (随机指针追逐)：**\n    -   访问模式：使用 $n=32$ 个线程进行指针追逐；指针随机分布在 $W$ 区域内。\n    -   指标：吞吐量 $T$ ($\\mathrm{ops/s}$)，第 $99$ 百分位延迟 $L_{99}$ ($\\mathrm{ms}$)。\n    -   `madvise` 用法：基准测试调用 `madvise` 并附带 `MADV_HUGEPAGE`。\n    -   测量结果：\n        -   `always`: $T_{\\mathrm{always}} = 3.0 \\times 10^{6}\\,\\mathrm{ops/s}$, $L_{99,\\mathrm{always}} = 4.2\\,\\mathrm{ms}$。\n        -   `madvise`: $T_{\\mathrm{madvise}} = 3.1 \\times 10^{6}\\,\\mathrm{ops/s}$, $L_{99,\\mathrm{madvise}} = 2.6\\,\\mathrm{ms}$。\n        -   `never`: $T_{\\mathrm{never}} = 3.0 \\times 10^{6}\\,\\mathrm{ops/s}$, $L_{99,\\mathrm{never}} = 2.4\\,\\mathrm{ms}$。\n-   **假设：**\n    -   TLB 容量有限。\n    -   THP 的 `always` 模式涉及可能引入停顿的后台内存规整。\n    -   交换和 I/O 不是影响因素。\n-   **问题：** 最大化吞吐量 $T$ 同时保持低尾延迟 $L$ 的最佳策略建议是什么？\n\n#### 第 2 步：使用提取的已知信息进行验证\n\n-   **科学依据：** 该问题牢固地植根于操作系统和计算机体系结构的原理。THP、TLB、页面大小、访问模式及其性能影响都是标准主题。关于 `always`、`never` 和 `madvise` 模式的描述，以及后台内存规整（通过 `khugepaged` 和 `kcompactd`）的作用，都是准确的。\n-   **良构性：** 该问题提供了清晰的设置、具体的经验数据和一个明确定义的目标（最大化吞吐量，最小化尾延迟）。其结构允许对所涉及的权衡进行独特而有意义的分析。\n-   **客观性：** 语言技术性强且精确。数据是基于事实呈现的，问题要求基于对这些事实和第一性原理的分析提出建议。\n-   **缺陷审查：**\n    1.  **科学上不健全：** 无。概念及其相互作用的描述是正确的。\n    2.  **无法形式化/不相关：** 无。该问题是一个经典的性能分析任务，与操作系统中的内存管理主题直接相关。\n    3.  **不完整/矛盾的设置：** 无。提供的数据足以得出关于三种模式在两种工作负载下相对性能的结论。数据并不矛盾；它反映了 THP 已知的性能权衡。\n    4.  **不切实际/不可行：** 无。参数（$p=4\\,\\mathrm{KiB}$, $P=2\\,\\mathrm{MiB}$, $W=8\\,\\mathrm{GiB}$）和性能测量对于现代服务器系统来说是合理的。\n    5.  **不适定：** 无。可以得出一个独特、合理的建议。\n    6.  **伪深刻/琐碎：** 无。该问题需要对工作负载特性（访问模式）和系统级内存管理策略之间的相互作用有细致的理解，特别是平均情况和尾延迟效应之间的差异。\n    7.  **超出科学可验证性：** 无。其原理是标准的计算机科学理论，问题被构建为对一个可重复实验的分析。\n\n#### 第 3 步：结论和行动\n\n问题陈述是**有效的**。我将继续进行解答。\n\n### 推导与选项分析\n\n问题的核心在于理解使用大页与基页之间的性能权衡，这种权衡受到不同 THP 策略的调节。大页的主要好处是减少 TLB 压力。一个用于 $2\\,\\mathrm{MiB}$ 大页的 TLB 条目所覆盖的内存范围与 512 个用于 $4\\,\\mathrm{KiB}$ 基页的条目相同。TLB 未命中是昂贵的，需要进行硬件页表遍历，这可能花费数百个 CPU 周期。THP 的主要成本，尤其是在 `always` 模式下，是创建大页的开销，这需要物理上连续的内存。内核可能需要执行内存规整，这涉及到移动现有数据，并可能导致显著的、不可预测的应用程序停顿。\n\n#### 阶段 1 分析：顺序流式读取\n\n1.  **第一性原理：** 顺序流式访问模式表现出完美的空间局部性。应用程序将以线性、可预测的顺序访问内存地址。这是大页的理想用例。一旦大页的 TLB 条目被缓存，就可以访问大片内存区域（$2\\,\\mathrm{MiB}$）而无需再发生 TLB 未命中。使用基页会导致每隔几页就发生一次 TLB 未命中（取决于 TLB 覆盖范围），从而产生重复的页表遍历惩罚。因此，我们预期大页（`always` 和 `madvise` 模式）的性能会显著优于 `never` 模式。\n2.  **数据分析：**\n    -   `never`: $T_{\\mathrm{never}} = 9.4\\,\\mathrm{GiB/s}$, $L_{50,\\mathrm{never}} = 210\\,\\mathrm{ns}$。这作为低性能基线，受限于频繁的 TLB 未命中。\n    -   `madvise`: $T_{\\mathrm{madvise}} = 11.3\\,\\mathrm{GiB/s}$ (比 `never` 提升 $20\\%$)，$L_{50,\\mathrm{madvise}} = 160\\,\\mathrm{ns}$。应用程序的提示允许内核使用大页，从而大幅减少 TLB 未命中，并改善了吞吐量和延迟中位数。\n    -   `always`: $T_{\\mathrm{always}} = 12.0\\,\\mathrm{GiB/s}$ (比 `never` 提升 $28\\%$)，$L_{50,\\mathrm{always}} = 140\\,\\mathrm{ns}$。内核为创建大页所做的前瞻性工作对于这种完美匹配的工作负载更为有效，从而带来了最佳性能。与页表遍历延迟的大幅减少相比，任何后台内存规整的开销都可以忽略不计，并且没有反映在延迟中位数上。\n\n**阶段 1 结论：** 大页毫无疑问是有益的。`always` 模式是最佳选择，`madvise` 是一个非常强的次优选择。\n\n#### 阶段 2 分析：随机指针追逐\n\n1.  **第一性原理：** 在一个大的工作集（$W=8\\,\\mathrm{GiB}$）上的随机访问模式具有很差的空间局部性。然而，覆盖这个工作集所需的独立页面数量非常大：$W/p = (8 \\times 2^{30}) / (4 \\times 2^{10}) = 2,097,152$ 个基页。没有 TLB 能够缓存如此多的转换。使用大页可以将转换数量减少到 $W/P = (8 \\times 2^{30}) / (2 \\times 2^{20}) = 4096$ 个大页。这个数字足够小，现代 TLB 可以缓存其中的一大部分，因此原则上，与基页相比，大页仍应能降低 TLB 未命中率。然而，`always` 模式的后台内存规整活动会引入不可预测的停顿。这些停顿——尽管不频繁——但可能非常長，严重影响像第 $99$ 百分位 ($L_{99}$) 这样的尾延迟指标。对于延迟敏感型工作负载来说，这是一个主要问题。`madvise` 模式不那么激进，可能避免一些这种病态的尾延迟，而 `never` 则完全避免了它。\n2.  **数据分析：**\n    -   `never`: $T_{\\mathrm{never}} = 3.0 \\times 10^{6}\\,\\mathrm{ops/s}$, $L_{99,\\mathrm{never}} = 2.4\\,\\mathrm{ms}$。这是基线。性能是可预测的，具有最低的尾延迟。\n    -   `madvise`: $T_{\\mathrm{madvise}} = 3.1 \\times 10^{6}\\,\\mathrm{ops/s}$ (吞吐量有边际提升)，$L_{99,\\mathrm{madvise}} = 2.6\\,\\mathrm{ms}$ (尾延迟有边际增加)。这反映了权衡：TLB 未命中的轻微减少提高了总吞吐量，但创建大页（即使是按需创建）的开销增加了一点尾延迟。\n    -   `always`: $T_{\\mathrm{always}} = 3.0 \\times 10^{6}\\,\\mathrm{ops/s}$ (吞吐量无提升)，$L_{99,\\mathrm{always}} = 4.2\\,\\mathrm{ms}$ (尾延迟比 `never` 增加了 $75\\%$)。这是关键结果。主动的、系统范围的内存规整停顿对尾延迟是毁灭性的。在第 $99$ 百分位上，减少 TLB 未命中的好处被内核引起的停顿完全抵消了。\n\n**阶段 2 结论：** 由于其极高的尾延迟，`always` 模式是不可接受的。`never` 模式提供了最佳的可预测性和最低的尾延迟。`madvise` 以略差的尾延迟为代价提供了可忽略的吞吐量增益，使得 `never` 成为这个延迟敏感阶段的更优选择。\n\n#### 综合与策略建议\n\n目标是在保持低尾延迟的同时最大化吞吐量。必须根据工作负载选择最优策略。\n-   对于**顺序流式读取**，大页对于高吞吐量至关重要。`madvise` 或 `always` 是不错的选择。\n-   对于**随机指针追逐**，其中低尾延迟是优先事项，`always` 是有害的。`never` 在延迟指标上是最安全且性能最佳的选择。\n\n因此，一个混合策略是合适的：对受益的工作负载使用大页，对受害的工作负载禁用它们。这正是 `madvise` 设置的设计目的——应用程序引导的优化。然而，问题要求的是可能涉及设置系统级旋钮的策略建议。从测量结果来看，最佳组合是在阶段 1 使用大页（例如，通过 `madvise`），在阶段 2 不使用大页（`never`）。\n\n### 逐项选项分析\n\n**A. 对所有工作负载使用 `always` 模式，因为无论访问模式如何，大页总是通过减少页错误和 TLB 未命中来提高吞吐量和降低延迟。**\n这个陈述在事实上是不正确的。阶段 2 的数据清楚地表明，`always` 模式在没有提高吞吐量的情况下，将尾延迟 ($L_{99}$) 从 $2.4\\,\\mathrm{ms}$ 显著恶化到 $4.2\\,\\mathrm{ms}$。大页“总是提高”性能的前提是一个错误的过度概括。\n**结论：错误。**\n\n**B. 对顺序流式读取使用 `madvise` 模式（配合 MADV_HUGEPAGE），对随机指针追逐使用 `never` 模式；大页在流式读取中减少了 TLB 未命中和页表遍历，但 `always` 模式会引入后台内存规整停顿，从而在随机工作负载中抬高 $L_{99}$。**\n这个选项准确地反映了从数据和第一性原理中得出的最优策略。\n-   它正确地指出大页对流式读取有益。在此阶段使用 `madvise` 可获得高吞吐量（$11.3\\,\\mathrm{GiB/s}$），接近最佳情况的 `always` 模式。\n-   它正确地指出对于随机工作负载，`never` 模式在保持低尾延迟（$L_{99} = 2.4\\,\\mathrm{ms}$）方面更优。\n-   所提供的推理是合理的：大页在流式读取中减少了 TLB 未命中，而 `always` 模式的内存规整停顿损害了随机工作负载的尾延迟。这与我们的分析完全一致。\n**结论：正确。**\n\n**C. 对所有工作负载使用 `never` 模式，因为大页会增加缓存污染，从而普遍降低吞吐量。**\n这个陈述与阶段 1 的数据相矛盾，在阶段 1 中，`always` 和 `madvise` 模式都提供了比 `never` 高得多的吞吐量（$12.0\\,\\mathrm{GiB/s}$ 和 $11.3\\,\\mathrm{GiB/s}$ vs. $9.4\\,\\mathrm{GiB/s}$）。普遍降低吞吐量的说法是错误的。\n**结论：错误。**\n\n**D. 仅在 RAM 面临内存压力时使用 `always` 模式，因为 THP 内存规整主要有利于交换行为，因此会降低随机指针追逐的延迟。**\n这个陈述在多个方面都是不正确的。\n-   问题明确指出没有内存压力或交换。\n-   THP 内存规整的主要目的是为大页创建连续的物理内存，而不是为了有利于交换。\n-   `always` 模式会“降低随机指针追逐的延迟”的说法被实验数据直接驳斥，数据显示它显著增加了尾延迟。\n**结论：错误。**", "answer": "$$\\boxed{B}$$", "id": "3684922"}, {"introduction": "除了性能考量，采用巨页还会对内存效率产生深远影响，这是系统设计中必须权衡的另一个重要方面。高性能应用（如键值存储）常使用基于巨页的内存池（arena）来管理内存，但这可能导致内存浪费。本练习将带你量化一种常见的内存浪费形式——内部碎片，通过计算在一个具体的键值存储场景中，因分配策略导致的额外内存开销，从而更全面地评估使用巨页的利弊。[@problem_id:3684843]", "problem": "一个键值存储运行在一个支持巨页的操作系统上，它将分配器区域（arena）固定到大小为 $2\\,\\mathrm{MiB}$ 的巨页上（每个区域一个巨页）。每个区域专用于一个大小类，并被子分配为该类的大小为 $s$ 字节的固定大小槽位。槽位是对齐且不重叠的；如果 $k$ 个大小为 $s$ 的槽位能放入一个 $2\\,\\mathrm{MiB}$ 的巨页中，则该页中任何剩余的字节都将不被使用，并且不能被其他大小类借用，因为区域被固定到了一个类上。分配器将每个请求的值大小 $v$ 向上取整到满足 $s \\ge v$ 的最小槽位大小 $s$。忽略元数据和外部碎片；假设有大量分配，使得每个类的区域都密集地填充了 $k$ 个存活对象，仅留下页尾的剩余空间未使用。\n\n假设分配器使用大小类集合 $S=\\{1\\,\\mathrm{KiB},\\,1.5\\,\\mathrm{KiB},\\,3\\,\\mathrm{KiB},\\,8\\,\\mathrm{KiB},\\,48\\,\\mathrm{KiB},\\,256\\,\\mathrm{KiB},\\,1\\,\\mathrm{MiB}\\}$ 并将请求 $v$ 映射到 $s(v)=\\min\\{s \\in S \\mid s \\ge v\\}$。因此，一个大小为 $v$ 的请求所消耗的摊销已提交内存等于其所在类的区域中每个对象的份额，即巨页大小除以能容纳在巨页中的完整槽位数 $k$。\n\n设值大小 $V$（单位：字节）具有以下离散分布 $D(v)$：\n- $\\Pr[V=600]=0.25$,\n- $\\Pr[V=1100]=0.20$,\n- $\\Pr[V=2900]=0.15$,\n- $\\Pr[V=7500]=0.15$,\n- $\\Pr[V=40000]=0.10$,\n- $\\Pr[V=260000]=0.10$,\n- $\\Pr[V=900000]=0.05$.\n\n仅使用分页、取整和期望的基本定义，从第一性原理推导在上述区域固定策略下的期望内部碎片率，其定义为\n$$F \\equiv \\frac{\\text{期望已提交字节} - \\text{期望已使用字节}}{\\text{期望已提交字节}},$$\n取 $1\\,\\mathrm{MiB}=2^{20}\\,\\mathrm{bytes}$ 和 $1\\,\\mathrm{KiB}=1024\\,\\mathrm{bytes}$。将最终结果表示为一个无单位的十进制数，并四舍五入到四位有效数字。", "solution": "我们使用的基本定义如下：\n- 巨页大小为 $H = 2\\,\\mathrm{MiB} = 2 \\times 2^{20} = 2{,}097{,}152$ 字节。\n- 对于一个大小类，其槽位大小为 $s$，则一个巨页中的完整槽位数量为 $k=\\left\\lfloor \\frac{H}{s} \\right\\rfloor$，每个对象的已提交份额为 $\\frac{H}{k}$ 字节，因为 $k$ 个对象平均共享 $H$ 字节的内存。\n- 分配器将 $v$ 向上取整到 $s(v) \\in S$，其中 $s(v) \\ge v$ 且 $s(v)$ 是 $S$ 中的最小值。\n- 期望内部碎片率是\n$$F \\equiv \\frac{\\mathbb{E}[\\text{每个对象的已提交份额}] - \\mathbb{E}[V]}{\\mathbb{E}[\\text{每个对象的已提交份额}]} = 1 - \\frac{\\mathbb{E}[V]}{\\mathbb{E}[\\text{每个对象的已提交份额}]}.$$\n\n步骤1：将每个值 $v$ 映射到其槽位大小 $s(v)$，并计算 $k(v)=\\left\\lfloor \\frac{H}{s(v)} \\right\\rfloor$ 和每个对象的已提交份额 $C(v)=\\frac{H}{k(v)}$。\n\n我们有 $H=2{,}097{,}152$。\n\n- 对于 $v=600$，$s(v)=1\\,\\mathrm{KiB}=1{,}024$。那么 $k=\\left\\lfloor \\frac{2{,}097{,}152}{1{,}024} \\right\\rfloor = 2{,}048$ 且 $C(v)=\\frac{2{,}097{,}152}{2{,}048}=1{,}024$。\n- 对于 $v=1100$，$s(v)=1.5\\,\\mathrm{KiB}=1{,}536$。那么 $k=\\left\\lfloor \\frac{2{,}097{,}152}{1{,}536} \\right\\rfloor = \\left\\lfloor \\frac{4{,}096}{3} \\right\\rfloor = 1{,}365$，且 $C(v)=\\frac{2{,}097{,}152}{1{,}365} \\approx 1{,}536.37509$。\n- 对于 $v=2900$，$s(v)=3\\,\\mathrm{KiB}=3{,}072$。那么 $k=\\left\\lfloor \\frac{2{,}097{,}152}{3{,}072} \\right\\rfloor = \\left\\lfloor \\frac{2{,}048}{3} \\right\\rfloor = 682$，且 $C(v)=\\frac{2{,}097{,}152}{682} \\approx 3{,}075.00293$。\n- 对于 $v=7{,}500$，$s(v)=8\\,\\mathrm{KiB}=8{,}192$。那么 $k=\\lfloor \\frac{2{,}097{,}152}{8{,}192} \\rfloor =256$，且 $C(v)=8{,}192$。\n- 对于 $v=40{,}000$，$s(v)=48\\,\\mathrm{KiB}=49{,}152$。那么 $k=\\left\\lfloor \\frac{2{,}097{,}152}{49{,}152} \\right\\rfloor = \\left\\lfloor \\frac{43{,}690.66\\dots}{1} \\right\\rfloor = 42$，且 $C(v)=\\frac{2{,}097{,}152}{42} \\approx 49{,}932.19048$。\n- 对于 $v=260{,}000$，$s(v)=256\\,\\mathrm{KiB}=262{,}144$。那么 $k=\\lfloor \\frac{2{,}097{,}152}{262{,}144} \\rfloor =8$，且 $C(v)=262{,}144$。\n- 对于 $v=900{,}000$，$s(v)=1\\,\\mathrm{MiB}=1{,}048{,}576$。那么 $k=\\lfloor \\frac{2{,}097{,}152}{1{,}048{,}576} \\rfloor =2$，且 $C(v)=1{,}048{,}576$。\n\n步骤2：根据给定分布计算 $\\mathbb{E}[V]$。\n\n$$\n\\mathbb{E}[V] = 0.25 \\cdot 600 + 0.20 \\cdot 1{,}100 + 0.15 \\cdot 2{,}900 + 0.15 \\cdot 7{,}500 + 0.10 \\cdot 40{,}000 + 0.10 \\cdot 260{,}000 + 0.05 \\cdot 900{,}000.\n$$\n\n逐项计算：\n\n$$\n\\mathbb{E}[V] = 150 + 220 + 435 + 1{,}125 + 4{,}000 + 26{,}000 + 45{,}000 = 76{,}930.\n$$\n\n\n步骤3：通过用相同的概率对 $C(v)$ 进行加权，计算 $\\mathbb{E}[\\text{每个对象的已提交份额}] = \\mathbb{E}[C(V)]$。\n\n$$\n\\mathbb{E}[C(V)] = 0.25 \\cdot 1{,}024 + 0.20 \\cdot \\frac{2{,}097{,}152}{1{,}365} + 0.15 \\cdot \\frac{2{,}097{,}152}{682} + 0.15 \\cdot 8{,}192 + 0.10 \\cdot \\frac{2{,}097{,}152}{42} + 0.10 \\cdot 262{,}144 + 0.05 \\cdot 1{,}048{,}576.\n$$\n\n计算每一项：\n- $0.25 \\cdot 1{,}024 = 256$。\n- $0.20 \\cdot 1536.37509 \\approx 307.2750$。\n- $0.15 \\cdot 3075.00293 \\approx 461.2504$。\n- $0.15 \\cdot 8{,}192 = 1{,}228.8$。\n- $0.10 \\cdot 49932.19048 \\approx 4{,}993.2190$。\n- $0.10 \\cdot 262{,}144 = 26{,}214.4$。\n- $0.05 \\cdot 1{,}048{,}576 = 52{,}428.8$。\n\n求和，\n\n$$\n\\mathbb{E}[C(V)] \\approx 256 + 307.2750 + 461.2504 + 1{,}228.8 + 4{,}993.2190 + 26{,}214.4 + 52{,}428.8 \\approx 85{,}889.7444.\n$$\n\n\n步骤4：计算期望内部碎片率\n\n$$\nF = 1 - \\frac{\\mathbb{E}[V]}{\\mathbb{E}[C(V)]} = 1 - \\frac{76{,}930}{85{,}889.7444}.\n$$\n\n计算比率：\n\n$$\n\\frac{76{,}930}{85{,}889.7444} \\approx 0.8956832,\n$$\n\n所以\n\n$$\nF \\approx 1 - 0.8956832 = 0.1043168.\n$$\n\n\n按要求四舍五入到四位有效数字，得到 $0.1043$。", "answer": "$$\\boxed{0.1043}$$", "id": "3684843"}]}