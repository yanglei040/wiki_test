{"hands_on_practices": [{"introduction": "The core promise of the Harvard architecture is increased throughput from its parallel instruction and data memory pathways. This exercise moves beyond the qualitative concept to a quantitative analysis, allowing us to measure the real-world performance impact of this design. By calculating the overlap efficiency, you will explore how the balance between instruction and data bus widths directly impacts the degree of parallelism a processor can achieve. This practice is valuable for developing skills in modeling system performance and understanding how fundamental hardware design choices can create or alleviate bottlenecks. [@problem_id:3646906]", "problem": "A processor implements a Harvard architecture, meaning the instruction path and data path are physically separated. Specifically, it has an Instruction (I) bus and a Data (D) bus that can operate simultaneously under a common clock. Consider the execution of a single memory-dependent instruction that requires fetching the instruction of size $s_{i}$ bytes over the Instruction (I) bus and, during its execution, performing one data memory transfer of size $s_{d}$ bytes over the Data (D) bus. Assume the following:\n\n- The bus width of the Instruction (I) bus is $B_{i}$ bits, and the bus width of the Data (D) bus is $B_{d}$ bits.\n- Each bus can transfer at most its full width per clock cycle, that is, at most $B$ bits per cycle for a bus of width $B$.\n- Both transfers start at the same clock cycle on their respective buses, and transfers proceed at the maximum allowed per-cycle rate until their respective sizes are fully transferred.\n- Ignore fixed setup latencies, alignment constraints, and any additional pipeline or cache effects; consider only the raw transfer time measured in clock cycles for each bus.\n\nDefine the overlap efficiency as $\\eta = \\dfrac{\\text{parallel cycles}}{\\text{total cycles}}$, where “parallel cycles” is the number of clock cycles in which both the Instruction (I) and Data (D) buses are simultaneously active, and “total cycles” is the total number of clock cycles during which at least one of the two buses is active for this instruction’s memory activity.\n\nStarting from the fundamental definitions above of Harvard architecture concurrency and bus transfer capacity per cycle, derive an analytical expression for $\\eta$ in terms of $B_{i}$, $B_{d}$, $s_{i}$, and $s_{d}$. Then, for a scenario with instruction size $s_{i}=16$ bytes and data transfer size $s_{d}=32$ bytes, compute $\\eta$ for the following three bus width configurations (all widths in bits):\n\n- Case A: $B_{i}=64$, $B_{d}=64$.\n- Case B: $B_{i}=32$, $B_{d}=64$.\n- Case C: $B_{i}=128$, $B_{d}=32$.\n\nExpress the final overlap efficiencies as exact values. The efficiency $\\eta$ is dimensionless; do not include units in your final numerical results. No rounding is required.", "solution": "The problem asks for the overlap efficiency, $\\eta$, of two concurrent bus transfers in a Harvard architecture, defined as the ratio of parallel-active cycles to total-active cycles.\n\nFirst, we determine the number of clock cycles required for each transfer. Memory sizes are in bytes and bus widths are in bits, with $1$ byte $= 8$ bits.\n\nLet $N_i$ be the number of cycles to fetch the instruction of size $s_i$ bytes over a bus of width $B_i$ bits. The total instruction size in bits is $8s_i$. Since transfers occur in integer numbers of clock cycles, we take the ceiling of the ratio of total bits to bits per cycle.\n$$\nN_i = \\left\\lceil \\frac{8s_i}{B_i} \\right\\rceil\n$$\n\nSimilarly, let $N_d$ be the number of cycles for the data transfer of size $s_d$ bytes over a bus of width $B_d$ bits.\n$$\nN_d = \\left\\lceil \\frac{8s_d}{B_d} \\right\\rceil\n$$\n\nThe number of \"parallel cycles,\" $N_{\\text{parallel}}$, is the time both buses are active. Since they start together, this is the duration of the shorter transfer, which is the minimum of their cycle counts.\n$$\nN_{\\text{parallel}} = \\min(N_i, N_d)\n$$\n\nThe \"total cycles,\" $N_{\\text{total}}$, is the time until the longer transfer completes, which is the maximum of their cycle counts.\n$$\nN_{\\text{total}} = \\max(N_i, N_d)\n$$\n\nThe overlap efficiency $\\eta$ is the ratio:\n$$\n\\eta = \\frac{N_{\\text{parallel}}}{N_{\\text{total}}} = \\frac{\\min(N_i, N_d)}{\\max(N_i, N_d)}\n$$\n\nSubstituting the expressions for $N_i$ and $N_d$ gives the general analytical expression:\n$$\n\\eta = \\frac{\\min\\left(\\left\\lceil \\frac{8s_i}{B_i} \\right\\rceil, \\left\\lceil \\frac{8s_d}{B_d} \\right\\rceil\\right)}{\\max\\left(\\left\\lceil \\frac{8s_i}{B_i} \\right\\rceil, \\left\\lceil \\frac{8s_d}{B_d} \\right\\rceil\\right)}\n$$\n\nNow, we compute $\\eta$ for the scenario with $s_i = 16$ bytes (128 bits) and $s_d = 32$ bytes (256 bits).\n\n**Case A: $B_i = 64$ bits, $B_d = 64$ bits.**\n$$\nN_i = \\left\\lceil \\frac{128}{64} \\right\\rceil = 2 \\text{ cycles}\n$$\n$$\nN_d = \\left\\lceil \\frac{256}{64} \\right\\rceil = 4 \\text{ cycles}\n$$\n$$\n\\eta_A = \\frac{\\min(2, 4)}{\\max(2, 4)} = \\frac{2}{4} = \\frac{1}{2}\n$$\n\n**Case B: $B_i = 32$ bits, $B_d = 64$ bits.**\n$$\nN_i = \\left\\lceil \\frac{128}{32} \\right\\rceil = 4 \\text{ cycles}\n$$\n$$\nN_d = \\left\\lceil \\frac{256}{64} \\right\\rceil = 4 \\text{ cycles}\n$$\n$$\n\\eta_B = \\frac{\\min(4, 4)}{\\max(4, 4)} = \\frac{4}{4} = 1\n$$\nThis represents a perfectly balanced case with maximum efficiency.\n\n**Case C: $B_i = 128$ bits, $B_d = 32$ bits.**\n$$\nN_i = \\left\\lceil \\frac{128}{128} \\right\\rceil = 1 \\text{ cycle}\n$$\n$$\nN_d = \\left\\lceil \\frac{256}{32} \\right\\rceil = 8 \\text{ cycles}\n$$\n$$\n\\eta_C = \\frac{\\min(1, 8)}{\\max(1, 8)} = \\frac{1}{8}\n$$\n\nThe final results are $\\frac{1}{2}$, $1$, and $\\frac{1}{8}$ respectively.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2} & 1 & \\frac{1}{8}\n\\end{pmatrix}\n}\n$$", "id": "3646906"}, {"introduction": "While the separation of instruction and data paths boosts performance, it can also impose limitations, such as preventing a program from easily reading data embedded within the code itself—a common task in von Neumann machines. This exercise challenges you to devise a clever software technique to overcome this hardware restriction in a strict Harvard system. You will model a \"code-as-data\" paradigm, using control flow to access constant values stored in instruction memory, and quantify the performance trade-off by calculating a congestion factor, $\\kappa$. This problem highlights the intricate interplay between hardware architecture and software design, demonstrating how architectural constraints can inspire creative programming solutions. [@problem_id:3646951]", "problem": "A processor implements a Harvard architecture with separate instruction memory and data memory. The Instruction Set Architecture (ISA) provides fixed-width instructions of size $s$ bits. The instruction memory (instruction space, abbreviated as $I$-space) is read by the Instruction Fetch Unit (IFU) and is not directly addressable by the data load/store unit. To realize a code-as-data paradigm for small lookup tables without accessing data memory, you decide to encode the table values directly into $I$-space and access them by control flow.\n\nAssume the following base facts and definitions:\n- In a Harvard architecture, $I$-space and data space are physically separated. The IFU fetches instructions but cannot issue arbitrary data loads from $I$-space.\n- An encoded table entry of value width $v$ bits can be reconstructed by executing a sequence of immediate-carrying instructions that cumulatively materialize the constant into a register. The ISA offers an immediate-carrying instruction that writes or merges an immediate field of payload width $p$ bits into the destination without requiring extra arithmetic or logical instructions. Each such instruction still occupies a full instruction word of $s$ bits in $I$-space.\n- To access entry index $j$, the code computes a target address and performs an indirect branch (one branch instruction). At the target, it executes exactly $n$ immediate-carrying instructions (where $n$ is the minimum needed to materialize the $v$-bit value from $p$-bit payloads), and then executes a return instruction to resume at the caller.\n- Ignore branch prediction effects, pipeline bubbles, instruction cache misses, and alignment penalties; count only the bits fetched by the IFU due to the branch instruction, the $n$ immediate-carrying instructions, and the return instruction. Treat every instruction as occupying $s$ bits in $I$-space.\n\nDefine the $I$-space congestion factor $\\kappa$ for a lookup as the ratio of the total number of instruction bits fetched by the IFU during one such table access to the number of useful data bits delivered by the table (that is, the $v$ bits of the table value). Thus, $\\kappa$ is dimensionless and quantifies how many instruction bits are consumed per single delivered data bit.\n\nYou are given a concrete system with the following parameters:\n- Fixed instruction width $s = 32$ bits.\n- Immediate payload width per instruction $p = 16$ bits.\n- Value width per table entry $v = 24$ bits.\n- One indirect branch instruction and one return instruction are required per lookup, and both are $s$-bit instructions.\n\nUsing only the above foundational facts, derive $n$ from first principles, then derive an expression for $\\kappa$ in terms of $s$, $p$, $v$, and $n$, and finally compute the numerical value of $\\kappa$ for the given system. Round your final numerical result to four significant figures. Express the final answer as a pure number with no units.", "solution": "This problem requires us to derive the congestion factor, $\\kappa$, which quantifies the overhead of reading data encoded within the instruction stream in a Harvard architecture.\n\n**1. Derivation of $n$**\nFirst, we find $n$, the minimum number of immediate-carrying instructions needed to construct a value of width $v$ bits using instructions with an immediate payload of $p$ bits each. The total payload from $n$ instructions is $n \\times p$. This must be at least equal to the value width $v$.\n$$n \\times p \\geq v$$\nSince $n$ must be an integer (a count of instructions), we must use the smallest integer that satisfies this condition. This is the ceiling function.\n$$n = \\left\\lceil \\frac{v}{p} \\right\\rceil$$\nFor the given system parameters ($v = 24$ bits, $p = 16$ bits):\n$$n = \\left\\lceil \\frac{24}{16} \\right\\rceil = \\lceil 1.5 \\rceil = 2$$\nTherefore, 2 instructions are required.\n\n**2. Derivation of the Congestion Factor $\\kappa$**\nThe congestion factor $\\kappa$ is defined as the ratio of total instruction bits fetched to useful data bits delivered.\n$$\\kappa = \\frac{\\text{Total instruction bits fetched}}{\\text{Useful data bits delivered}}$$\nThe total number of instructions fetched per lookup consists of:\n- 1 indirect branch instruction\n- $n$ immediate-carrying instructions\n- 1 return instruction\nTotal instructions fetched = $1 + n + 1 = n + 2$.\nSince each instruction has a width of $s$ bits, the total instruction bits fetched is:\n$$\\text{Total instruction bits fetched} = (n + 2)s$$\nThe number of useful data bits delivered is the width of the table value, $v$.\n$$\\text{Useful data bits delivered} = v$$\nSubstituting these into the definition of $\\kappa$ gives the general expression:\n$$\\kappa = \\frac{(n + 2)s}{v}$$\nWe can also express this purely in terms of the base parameters $s$, $p$, and $v$:\n$$\\kappa = \\frac{\\left( \\left\\lceil \\frac{v}{p} \\right\\rceil + 2 \\right)s}{v}$$\n\n**3. Numerical Calculation of $\\kappa$**\nUsing the given system parameters ($s = 32$, $v = 24$) and our calculated value for $n=2$:\n$$\\kappa = \\frac{(2 + 2) \\times 32}{24} = \\frac{4 \\times 32}{24} = \\frac{128}{24}$$\nSimplifying the fraction by dividing the numerator and denominator by their greatest common divisor, 8:\n$$\\kappa = \\frac{16}{3} = 5.3333...$$\nRounding to four significant figures as requested:\n$$\\kappa \\approx 5.333$$\nThis value indicates that this technique requires fetching approximately 5.333 bits from instruction memory for every 1 bit of useful data retrieved.", "answer": "$$\n\\boxed{5.333}\n$$", "id": "3646951"}, {"introduction": "The principles of Harvard architecture extend from a single processor core to complex multicore systems, where managing shared resources becomes paramount. In this advanced scenario, the distinct instruction ($I_{bw}$) and data ($D_{bw}$) bandwidth demands of different tasks become critical factors in system-level performance. This exercise frames the concept as a resource optimization puzzle: you must schedule multiple tasks across several cores and time slots to minimize the peak load on the shared L2 cache. This practice connects the low-level architectural feature of separate I/D pathways to the high-level challenge of workload scheduling, a crucial skill in modern computer and system engineering. [@problem_id:3646991]", "problem": "A multicore processor uses a Harvard architecture with separate instruction and data pathways per core. Each core has a private instruction Level-1 cache and data Level-1 cache, and all cores share a unified Level-2 (L2) cache. In this workload model, each task is characterized by a constant-rate pair $\\left(I_{bw}, D_{bw}\\right)$ in gigabytes per second (GB/s): $I_{bw}$ is the sustained instruction fetch bandwidth demand into the core’s instruction pathway, and $D_{bw}$ is the sustained data bandwidth demand serviced by the shared Level-2 (L2) cache due to streaming data that do not hit in the Level-1 data cache. Assume that instruction fetches are fully served by the instruction Level-1 cache hierarchy without reaching Level-2 (L2), while all data requests counted in $D_{bw}$ reach Level-2 (L2).\n\nThere are $C=3$ identical cores. Time is divided into exactly $2$ non-overlapping slots. Each task must be scheduled non-preemptively into exactly one slot and run on exactly one core during that slot. At most one task can run on a core in a slot. Each core has per-core pathway limits $I_{\\max}=5$ GB/s on the instruction pathway and $D_{\\max}=8$ GB/s on the data pathway. The chip has a shared instruction fabric limit per slot of $I_{\\text{chip}}=9$ GB/s, i.e., the sum of $I_{bw}$ across all tasks scheduled in a slot cannot exceed $I_{\\text{chip}}$. The shared Level-2 (L2) cache services the aggregate data traffic $D_{bw}$ from all tasks scheduled in a slot, and the slot’s Level-2 (L2) bandwidth consumption is the sum of $D_{bw}$ for tasks in that slot.\n\nYou are given $5$ tasks with demands $(I_{bw}, D_{bw})$ in GB/s:\n- $T_{1}: \\left(4, 6\\right)$\n- $T_{2}: \\left(3, 4\\right)$\n- $T_{3}: \\left(2, 5\\right)$\n- $T_{4}: \\left(4, 3\\right)$\n- $T_{5}: \\left(1, 7\\right)$\n\nAll tasks individually satisfy $I_{bw} \\leq I_{\\max}$ and $D_{bw} \\leq D_{\\max}$, so any single task can run on any core.\n\nUsing only the foundational facts that (i) in a Harvard architecture the instruction and data pathways are independent per core, (ii) chip-level instruction fabric capacity limits the sum of instruction fetch demand per slot, and (iii) the shared Level-2 (L2) bandwidth per slot equals the sum of the scheduled tasks’ $D_{bw}$ in that slot, determine the minimal achievable peak Level-2 (L2) bandwidth across the two slots, subject to the above constraints.\n\nExpress the final answer as an exact integer in GB/s. Do not provide an inequality or an equation; provide the single minimal achievable peak value. No rounding is required; give the exact value.", "solution": "The problem is a combinatorial optimization task. We need to partition 5 tasks into two sets (for Slot 1 and Slot 2) to minimize the maximum total data bandwidth of either set, subject to several constraints.\n\n**1. Define Objective and Constraints:**\n- **Tasks:** $T_1(4,6), T_2(3,4), T_3(2,5), T_4(4,3), T_5(1,7)$.\n- **Objective:** Minimize $\\max(\\sum D_{bw,1}, \\sum D_{bw,2})$.\n- **Constraints per slot:**\n    1.  **Core Count:** At most 3 tasks per slot. Since there are 5 tasks, the partition must be (2 tasks, 3 tasks).\n    2.  **Instruction Bandwidth:** The sum of $I_{bw}$ for tasks in a slot cannot exceed $I_{\\text{chip}} = 9$ GB/s.\n\n**2. Analyze Global Constraints:**\n- **Total Instruction Bandwidth:** $\\sum I_{bw} = 4+3+2+4+1 = 14$ GB/s.\n- **Total Data Bandwidth:** $\\sum D_{bw} = 6+4+5+3+7 = 25$ GB/s.\nLet $I_{S1}$ be the sum of instruction bandwidths for tasks in Slot 1. Then for Slot 2, $I_{S2} = 14 - I_{S1}$. The constraints are $I_{S1} \\le 9$ and $I_{S2} \\le 9$. The second constraint implies $14 - I_{S1} \\le 9$, which simplifies to $I_{S1} \\ge 5$. Thus, for any valid partition, one group of tasks must have a total instruction bandwidth between 5 and 9 GB/s.\n\n**3. Enumerate and Evaluate Partitions:**\nWe must partition the 5 tasks into a group of 2 and a group of 3. We can iterate through all $\\binom{5}{2} = 10$ possible groups of 2 tasks for Slot 1 and check for validity.\n\n-   **Group {T1, T2}:**\n    -   $I_1 = 4+3 = 7$ (Valid, as $5 \\le 7 \\le 9$).\n    -   $D_1 = 6+4 = 10$. The other group has $D_2 = 25 - 10 = 15$.\n    -   Peak Bandwidth: $\\max(10, 15) = 15$.\n\n-   **Group {T1, T3}:**\n    -   $I_1 = 4+2 = 6$ (Valid).\n    -   $D_1 = 6+5 = 11$. $D_2 = 25 - 11 = 14$.\n    -   Peak Bandwidth: $\\max(11, 14) = 14$.\n\n-   **Group {T1, T4}:**\n    -   $I_1 = 4+4 = 8$ (Valid).\n    -   $D_1 = 6+3 = 9$. $D_2 = 25 - 9 = 16$.\n    -   Peak Bandwidth: $\\max(9, 16) = 16$.\n\n-   **Group {T1, T5}:**\n    -   $I_1 = 4+1 = 5$ (Valid).\n    -   $D_1 = 6+7 = 13$. $D_2 = 25 - 13 = 12$.\n    -   Peak Bandwidth: $\\max(13, 12) = 13$.\n\n-   **Group {T2, T3}:**\n    -   $I_1 = 3+2 = 5$ (Valid).\n    -   $D_1 = 4+5 = 9$. $D_2 = 25 - 9 = 16$.\n    -   Peak Bandwidth: $\\max(9, 16) = 16$.\n\n-   **Group {T2, T4}:**\n    -   $I_1 = 3+4 = 7$ (Valid).\n    -   $D_1 = 4+3 = 7$. $D_2 = 25 - 7 = 18$.\n    -   Peak Bandwidth: $\\max(7, 18) = 18$.\n\n-   **Group {T2, T5}:**\n    -   $I_1 = 3+1 = 4$ (Invalid, $4  5$).\n\n-   **Group {T3, T4}:**\n    -   $I_1 = 2+4 = 6$ (Valid).\n    -   $D_1 = 5+3 = 8$. $D_2 = 25 - 8 = 17$.\n    -   Peak Bandwidth: $\\max(8, 17) = 17$.\n\n-   **Group {T3, T5}:**\n    -   $I_1 = 2+1 = 3$ (Invalid, $3  5$).\n\n-   **Group {T4, T5}:**\n    -   $I_1 = 4+1 = 5$ (Valid).\n    -   $D_1 = 3+7 = 10$. $D_2 = 25 - 10 = 15$.\n    -   Peak Bandwidth: $\\max(10, 15) = 15$.\n\n**4. Conclusion:**\nThe peak L2 bandwidth values for all valid schedules are $\\{15, 14, 16, 13, 16, 18, 17, 15\\}$. The minimum value among these is 13.\n\nThis minimal peak bandwidth is achieved with the following schedule:\n- **Slot 1:** Tasks $T_1$ and $T_5$. Total $I_{bw}=5$ GB/s, Total $D_{bw}=13$ GB/s.\n- **Slot 2:** Tasks $T_2$, $T_3$, and $T_4$. Total $I_{bw}=9$ GB/s, Total $D_{bw}=12$ GB/s.\nThe peak L2 bandwidth is $\\max(13, 12) = 13$ GB/s.\n\nThus, the minimal achievable peak Level-2 bandwidth is 13 GB/s.", "answer": "$$\n\\boxed{13}\n$$", "id": "3646991"}]}