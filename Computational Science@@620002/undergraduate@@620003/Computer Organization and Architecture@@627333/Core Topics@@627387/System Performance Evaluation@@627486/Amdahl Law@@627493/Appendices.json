{"hands_on_practices": [{"introduction": "Real-world applications are rarely monolithic; they often consist of distinct phases—such as initialization, processing, and finalization—each with its own potential for parallelism. This practice [@problem_id:3620142] challenges you to apply Amdahl's Law to such a composite workload. By deriving the speedup for a program with multiple phases, you will learn to calculate an effective overall performance gain by correctly weighting the serial and parallel characteristics of each segment, a crucial skill for accurately modeling complex computational tasks.", "problem": "A compute-intensive workload is executed on a homogeneous multicore processor. The workload comprises three distinct phases, labeled A, B, and C. When run on a single core, the phase durations obey the ratio $2:5:3$. Let the single-core execution times for phases A, B, and C be $t_A$, $t_B$, and $t_C$, respectively, such that $t_A:t_B:t_C=2:5:3$. Each phase has a fraction of its work that is perfectly parallelizable across cores: for phase A this fraction is $p_A=0.7$, for phase B it is $p_B=0.9$, and for phase C it is $p_C=0.5$. Assume idealized parallel execution with the following properties: the parallelizable fraction of a phase scales linearly with the number of cores $N$, there is no overhead from synchronization or communication, and the non-parallelizable fraction does not accelerate with additional cores.\n\nStarting from the fundamental definition of speedup $S(N)$ as the ratio of single-core execution time to $N$-core execution time, and the assumption that within each phase the parallelizable and non-parallelizable portions contribute additively to the total phase time, derive a closed-form expression for the overall speedup $S(N)$ of the entire workload as a function of the number of cores $N$. Express your final answer as a simplified analytic expression in terms of $N$. No rounding is required.", "solution": "The overall speedup $S(N)$ for a process executed on $N$ cores is defined as the ratio of the execution time on a single core, $T(1)$, to the execution time on $N$ cores, $T(N)$.\n$$S(N) = \\frac{T(1)}{T(N)}$$\nThe total single-core execution time $T(1)$ is the sum of the execution times of the individual phases A, B, and C:\n$$T(1) = t_A + t_B + t_C$$\nThe problem states the ratio of these times is $t_A:t_B:t_C = 2:5:3$. Let the total single-core execution time be a reference value $T_1$. The fractional contribution of each phase to the total single-core execution time can be calculated from this ratio. The sum of the ratio parts is $2+5+3=10$. Therefore, the fractional times are:\n$$f_A = \\frac{t_A}{T(1)} = \\frac{2}{10} = 0.2$$\n$$f_B = \\frac{t_B}{T(1)} = \\frac{5}{10} = 0.5$$\n$$f_C = \\frac{t_C}{T(1)} = \\frac{3}{10} = 0.3$$\nSo, we can express the single-core phase times as $t_A = 0.2 T(1)$, $t_B = 0.5 T(1)$, and $t_C = 0.3 T(1)$.\n\nFor each phase $i$, a fraction $p_i$ is parallelizable and a fraction $(1-p_i)$ is serial. According to the problem statement, the execution time of the serial portion does not change with the number of cores $N$, while the execution time of the parallelizable portion is reduced by a factor of $N$. The execution time for phase $i$ on $N$ cores, $t_i(N)$, is therefore:\n$$t_i(N) = (1 - p_i)t_i + \\frac{p_i t_i}{N}$$\n\nThe total execution time on $N$ cores, $T(N)$, is the sum of the execution times of the three phases on $N$ cores:\n$$T(N) = t_A(N) + t_B(N) + t_C(N)$$\nSubstituting the expression for $t_i(N)$ for each phase:\n$$T(N) = \\left((1 - p_A)t_A + \\frac{p_A t_A}{N}\\right) + \\left((1 - p_B)t_B + \\frac{p_B t_B}{N}\\right) + \\left((1 - p_C)t_C + \\frac{p_C t_C}{N}\\right)$$\nWe can substitute the fractional times $t_i = f_i T(1)$:\n$$T(N) = \\left((1 - p_A)f_A T(1) + \\frac{p_A f_A T(1)}{N}\\right) + \\left((1 - p_B)f_B T(1) + \\frac{p_B f_B T(1)}{N}\\right) + \\left((1 - p_C)f_C T(1) + \\frac{p_C f_C T(1)}{N}\\right)$$\nFactoring out $T(1)$:\n$$T(N) = T(1) \\left[ \\left( f_A(1 - p_A) + \\frac{f_A p_A}{N} \\right) + \\left( f_B(1 - p_B) + \\frac{f_B p_B}{N} \\right) + \\left( f_C(1 - p_C) + \\frac{f_C p_C}{N} \\right) \\right]$$\nThe overall speedup $S(N)$ is then:\n$$S(N) = \\frac{T(1)}{T(N)} = \\frac{1}{ \\left( f_A(1 - p_A) + \\frac{f_A p_A}{N} \\right) + \\left( f_B(1 - p_B) + \\frac{f_B p_B}{N} \\right) + \\left( f_C(1 - p_C) + \\frac{f_C p_C}{N} \\right) }$$\nThis can be rearranged by grouping the terms that are independent of $N$ and those that are proportional to $1/N$:\n$$S(N) = \\frac{1}{ [f_A(1 - p_A) + f_B(1 - p_B) + f_C(1 - p_C)] + \\frac{1}{N}[f_A p_A + f_B p_B + f_C p_C] }$$\nNow, we substitute the given numerical values: $f_A = 0.2$, $f_B = 0.5$, $f_C = 0.3$, and $p_A = 0.7$, $p_B = 0.9$, $p_C = 0.5$.\n\nFirst, calculate the $N$-independent part of the denominator (the overall effective serial fraction):\n$$f_A(1 - p_A) + f_B(1 - p_B) + f_C(1 - p_C) = (0.2)(1 - 0.7) + (0.5)(1 - 0.9) + (0.3)(1 - 0.5)$$\n$$= (0.2)(0.3) + (0.5)(0.1) + (0.3)(0.5) = 0.06 + 0.05 + 0.15 = 0.26$$\n\nNext, calculate the coefficient of the $1/N$ term (the overall effective parallelizable fraction):\n$$f_A p_A + f_B p_B + f_C p_C = (0.2)(0.7) + (0.5)(0.9) + (0.3)(0.5)$$\n$$= 0.14 + 0.45 + 0.15 = 0.74$$\n\nSubstituting these values back into the expression for $S(N)$:\n$$S(N) = \\frac{1}{0.26 + \\frac{0.74}{N}}$$\nTo obtain a simplified rational expression, we multiply the numerator and the denominator by $N$:\n$$S(N) = \\frac{N}{N(0.26 + \\frac{0.74}{N})} = \\frac{N}{0.26N + 0.74}$$\nTo express this with integer coefficients, we can multiply the numerator and denominator by $100$:\n$$S(N) = \\frac{100N}{100(0.26N + 0.74)} = \\frac{100N}{26N + 74}$$\nThis fraction can be simplified by dividing the numerator and denominator by their greatest common divisor, which is $2$:\n$$S(N) = \\frac{50N}{13N + 37}$$\nThis is the final, closed-form expression for the overall speedup of the workload as a function of the number of cores $N$.", "answer": "$$\\boxed{\\frac{50N}{13N + 37}}$$", "id": "3620142"}, {"introduction": "Amdahl's Law is not just an analytical tool but a predictive one for guiding architectural design and making informed decisions about system upgrades. This exercise [@problem_id:3620103] presents a classic engineering trade-off: is it more effective to invest in a feature that reduces the serial execution time or to simply add more parallel processing cores? By calculating the number of cores $N^{\\ast}$ needed to match the performance gain from an improved serial fraction, you will develop a quantitative understanding of the diminishing returns of parallelism and the critical importance of optimizing serial bottlenecks.", "problem": "A workload executes on a multicore Central Processing Unit (CPU) and can be decomposed into an inherently serial fraction and a perfectly parallelizable fraction with respect to core count. Let the fraction that is inherently serial be $s$, and the fraction that is parallelizable be $1 - s$. Define speedup as the ratio of the execution time on a single core to the execution time on $N$ identical cores with negligible synchronization and communication overhead. Assume ideal load balance across cores for the parallelizable portion.\n\nA speculative prefetch mechanism is proposed that reduces the inherently serial fraction by a known amount $\\Delta s$, without changing the algorithm or the total amount of useful work. Consider the following two upgrade options applied to the same baseline system:\n- Option P: Enable speculative prefetch on a system with $N_{0}$ cores, thereby reducing the serial fraction from $s$ to $s - \\Delta s$.\n- Option C: Do not enable speculative prefetch (the serial fraction remains $s$), but increase the number of cores from the baseline to some $N^{\\ast}$ in order to match the speedup achieved by Option P.\n\nGiven $s = 0.20$, $\\Delta s = 0.05$, and $N_{0} = 12$, derive from first principles the expression for speedup as a function of $s$ and $N$ based on the decomposition of runtime into serial and parallelizable parts, and use it to determine the exact value of $N^{\\ast}$ such that the final speedup under Option C equals that under Option P. Express the final answer as a single real number with no units and do not round; provide the exact value.", "solution": "The speedup $S(N)$ of a workload on $N$ cores, given a serial fraction $s$, is described by Amdahl's Law. Let the execution time on a single core be normalized to $T(1)=1$. The execution time on $N$ cores, $T(N)$, is the sum of the serial time (which does not decrease) and the parallel time (which decreases by a factor of $N$):\n$$T(N) = s \\cdot T(1) + \\frac{(1-s) \\cdot T(1)}{N} = s + \\frac{1-s}{N}$$\nThe speedup is the ratio of single-core time to $N$-core time:\n$$S(N, s) = \\frac{T(1)}{T(N)} = \\frac{1}{s + \\frac{1-s}{N}}$$\n\nWe are asked to compare two upgrade options for a baseline system.\n\n**Option P:** The system has $N_0$ cores and an improved serial fraction of $s_P = s - \\Delta s$. The parallel fraction becomes $1 - s_P = 1 - (s - \\Delta s) = 1 - s + \\Delta s$. The speedup for this option, $S_P$, is:\n$$S_P = S(N_0, s_P) = \\frac{1}{(s - \\Delta s) + \\frac{1 - s + \\Delta s}{N_0}}$$\n\n**Option C:** The system has an increased number of cores, $N^{\\ast}$, but retains the original serial fraction $s_C = s$. The speedup for this option, $S_C$, is:\n$$S_C = S(N^{\\ast}, s_C) = \\frac{1}{s + \\frac{1-s}{N^{\\ast}}}$$\n\nThe problem requires us to find the value of $N^{\\ast}$ that makes the two speedups equal, i.e., $S_C = S_P$.\n$$\\frac{1}{s + \\frac{1-s}{N^{\\ast}}} = \\frac{1}{(s - \\Delta s) + \\frac{1 - s + \\Delta s}{N_0}}$$\nBy taking the reciprocal of both sides, we equate the denominators:\n$$s + \\frac{1-s}{N^{\\ast}} = (s - \\Delta s) + \\frac{1 - s + \\Delta s}{N_0}$$\nNow, we solve for $N^{\\ast}$. First, isolate the term containing $N^{\\ast}$:\n$$\\frac{1-s}{N^{\\ast}} = (s - \\Delta s) - s + \\frac{1 - s + \\Delta s}{N_0}$$\n$$\\frac{1-s}{N^{\\ast}} = -\\Delta s + \\frac{1 - s + \\Delta s}{N_0}$$\nCombine the terms on the right-hand side using the common denominator $N_0$:\n$$\\frac{1-s}{N^{\\ast}} = \\frac{-N_0 \\Delta s + (1 - s + \\Delta s)}{N_0} = \\frac{1 - s - \\Delta s (N_0 - 1)}{N_0}$$\nInvert both sides and multiply by $(1-s)$ to solve for $N^{\\ast}$:\n$$N^{\\ast} = (1-s) \\left( \\frac{N_0}{1 - s - \\Delta s (N_0 - 1)} \\right) = \\frac{N_0(1-s)}{1 - s - \\Delta s (N_0 - 1)}$$\nThis is the general expression for $N^{\\ast}$. We can now substitute the given numerical values: $s = 0.20$, $\\Delta s = 0.05$, and $N_0 = 12$.\nThe term $(1-s)$ is $1 - 0.20 = 0.80$.\nThe term $(N_0 - 1)$ is $12 - 1 = 11$.\nPlugging these values into the expression for $N^{\\ast}$:\n$$N^{\\ast} = \\frac{12 \\cdot (0.80)}{0.80 - 0.05 \\cdot (11)}$$\n$$N^{\\ast} = \\frac{9.6}{0.80 - 0.55}$$\n$$N^{\\ast} = \\frac{9.6}{0.25}$$\nDividing by $0.25$ is equivalent to multiplying by $4$:\n$$N^{\\ast} = 9.6 \\times 4 = 38.4$$\nTherefore, a system with $38.4$ cores and the original serial fraction would be needed to match the speedup of the 12-core system with the improved serial fraction.", "answer": "$$\n\\boxed{38.4}\n$$", "id": "3620103"}, {"introduction": "What happens when measured speedup seems to defy the predictions of Amdahl's Law, even surpassing the number of processors used? This thought-provoking scenario [@problem_id:3620139] delves into the concept of \"superlinear\" speedup, a phenomenon observable on modern hardware. This practice will help you understand that this is not a failure of the law, but a consequence of violating one of its core assumptions—that the work performed remains constant. By analyzing the profound impact of the memory hierarchy, you will learn to distinguish speedup gained from pure parallelism versus gains from secondary effects like improved cache locality.", "problem": "A program processes a large array of records using a fixed algorithm whose control flow and data accesses do not depend on the number of threads. On a single core of a dual-socket Central Processing Unit (CPU), the total runtime is $T_1 = 40$ seconds. Cycle-accurate profiling on this single-core run reports that a fraction $f_s = 0.10$ of $T_1$ is inherently serial (initialization, I/O setup, and a reduction that cannot be parallelized), and the remaining fraction is amenable to data-parallel execution with no algorithmic change. The processor has $2$ sockets; each socket has $8$ cores and a private last-level cache per socket of size $16$ MiB. Dynamic Random-Access Memory (DRAM) latency is approximately $100$ nanoseconds, whereas last-level cache hit latency is approximately $12$ nanoseconds. The working set when the program is run on one core is about $24$ MiB and exhibits a high miss rate in the last-level cache, causing the parallelizable portion to be memory-stall dominated on the single-core baseline.\n\nYou now run the exact same source code with $N = 4$ threads, pinned as $2$ threads per socket (so that each socket works on approximately half of the data). The measured parallel runtime is $T_4 = 7.5$ seconds. Assume negligible synchronization overhead and perfect load balance.\n\nUsing only fundamental definitions such as the definition of speedup $S(N) = T_1/T_N$, and the decomposition of runtime into serial and parallelizable parts under fixed per-operation cost, answer the following. Which option best explains why the measured speedup exceeds the value predicted by applying the serial/parallel decomposition to the single-core profile, and why the foundational bound on ideal parallelism still applies when improvements due to locality are excluded?\n\nA. The predicted speedup from the single-core serial fraction is about $3.08$, whereas the measured speedup is $S(4) = 40/7.5 \\approx 5.33$. This apparent “superlinear” behavior arises because partitioning the data across sockets reduces each socket’s working set from $24$ MiB to $12$ MiB, which now fits in the $16$ MiB last-level cache, drastically reducing memory-stall time in the parallelizable part relative to the single-core baseline. Amdahl’s bound on ideal parallelism still applies if we hold locality constant: for example, by using an equally cache-friendly blocked traversal on the single-core baseline (or by scaling the problem so the per-socket working set again exceeds the last-level cache), we remove the locality advantage, returning to a regime where the maximum speedup is limited solely by the serial fraction.\n\nB. The predicted speedup is about $3.08$, but the measured speedup $5.33$ proves that Amdahl’s law is false on modern hardware with caches. Because caches are part of the machine, there is no meaningful upper bound on speedup for a fixed problem size as $N$ increases.\n\nC. The predicted speedup from the single-core serial fraction is too pessimistic because the serial fraction itself decreases as $N$ grows due to overlapping memory latency across threads. If we replace $f_s$ by $f_s/N$, the prediction will match the measurement, so there is no inconsistency and no special role for cache locality.\n\nD. The observed result is an instance of weak scaling. By invoking Gustafson’s law, the speedup can legitimately exceed both the Amdahl prediction and the processor count, so no additional explanation involving cache effects or exclusion of locality gains is needed.\n\nE. The increase in aggregate cache capacity with more cores mathematically increases the parallel fraction even at fixed problem size, and therefore Amdahl’s law directly permits speedups larger than those predicted from the single-core serial fraction without altering any assumptions or baselines; no special treatment of locality is required.", "solution": "We begin from fundamental definitions. The speedup for $N$ processing elements is defined as\n$$\nS(N) \\equiv \\frac{T_1}{T_N}.\n$$\nLet the single-thread runtime decompose as $T_1 = T_s + T_p$, where $T_s$ is the time spent in code that is inherently serial and $T_p$ is the time spent in code that is parallelizable under fixed per-operation cost. The serial fraction is defined as\n$$\nf_s \\equiv \\frac{T_s}{T_1}, \\quad 0 \\le f_s \\le 1.\n$$\nIf the parallelizable portion scales ideally across $N$ threads without changing the cost per unit of work, then the parallel runtime under these fixed-cost assumptions is\n$$\nT_N = T_s + \\frac{T_p}{N} = f_s T_1 + \\frac{(1-f_s) T_1}{N},\n$$\nwhich implies the well-known bound on speedup obtained from the definition:\n$$\nS(N) = \\frac{T_1}{T_N} = \\frac{1}{f_s + \\frac{1-f_s}{N}}.\n$$\nThis bound rests on the premise that the per-operation cost in the parallelizable region is unchanged by the act of parallelization; it considers only the effect of concurrency on the parallel portion’s wall time.\n\nCompute the predicted speedup from the single-core serial fraction and the measured speedup. We are given $T_1 = 40$ seconds and $f_s = 0.10$. With $N = 4$,\n$$\nS_{\\text{pred}}(4) = \\frac{1}{0.10 + \\frac{0.90}{4}} = \\frac{1}{0.10 + 0.225} = \\frac{1}{0.325} \\approx 3.0769.\n$$\nThe measured parallel runtime is $T_4 = 7.5$ seconds, so the measured speedup is\n$$\nS_{\\text{meas}}(4) = \\frac{40}{7.5} \\approx 5.3333,\n$$\nwhich exceeds both $S_{\\text{pred}}(4)$ and the processor count $N = 4$ (a “superlinear” speedup). Why can this happen without contradicting the bound derived above?\n\nThe bound assumed that the parallel portion’s per-operation cost is unchanged by parallelization. In the given machine and workload, the single-core baseline works on a $24$ MiB working set on one socket with a $16$ MiB last-level cache. Because $24$ MiB $> 16$ MiB, the last-level cache cannot hold the working set, leading to a high miss rate and frequent Dynamic Random-Access Memory (DRAM) accesses with latency on the order of $100$ nanoseconds. The parallel run uses $N = 4$ threads pinned as $2$ per socket. The data is partitioned, so each socket processes roughly $12$ MiB. Now $12$ MiB $< 16$ MiB, so each socket’s working set fits in its last-level cache, reducing memory access latency to approximately $12$ nanoseconds for most accesses. Consequently, the per-operation cost in the parallelizable portion drops substantially compared to the single-thread baseline. This change violates the fixed-cost assumption in the above derivation and is best interpreted as a locality improvement, not as pure parallelism. As a result, $T_p$ itself shrinks when parallelized, which allows the measured $S(4)$ to exceed the bound computed under fixed-cost assumptions.\n\nWhy does the foundational bound on ideal parallelism still apply when excluding locality gains? If we hold locality constant across the comparison—by ensuring that the single-core baseline enjoys the same cache behavior as the parallel run, or equivalently that the parallel run does not alter the miss rate relative to the single-core baseline—then the per-operation cost does not change and the derivation applies. Two equivalent ways to enforce this are:\n- Use an equally cache-friendly traversal (for example, tiling or domain blocking) on the single-core baseline so that its working set during the parallelizable region also fits in the last-level cache, matching the locality of the parallel execution; or\n- Scale the problem size so that, even after partitioning across sockets, each socket’s working set still exceeds the last-level cache (iso-locality or iso-miss-rate setup), thereby preventing the per-operation cost from dropping in the parallel case.\n\nUnder such iso-locality conditions, the change in runtime from $T_1$ to $T_N$ arises from concurrency alone, and the speedup is bounded by\n$$\nS(N) = \\frac{1}{f_s + \\frac{1-f_s}{N}},\n$$\nwith the $f_s$ measured on the baseline that shares the same locality. The “extra” speedup observed in the original measurement is thus attributable to improved locality (reduced memory-stall time) and is not a violation of the bound on ideal parallelism; it reflects a different cost model rather than a breakdown of the serial/parallel decomposition.\n\nOption-by-option analysis:\n- Option A states the numerical comparison $S_{\\text{pred}}(4) \\approx 3.08$ versus $S_{\\text{meas}}(4) \\approx 5.33$, identifies the mechanism (data partitioning reduces per-socket working set to $12$ MiB, which fits in a $16$ MiB last-level cache, cutting memory-stall time), and correctly explains that Amdahl’s bound applies when locality is held constant by making the single-core baseline equally cache-friendly or by ensuring the parallel run does not change the miss rate. This is Correct.\n- Option B claims Amdahl’s law is false on modern hardware and that there is no meaningful upper bound for fixed-size problems. This misinterprets the situation: the bound is derived under fixed per-operation cost, and the observed excess speedup results from a change in per-operation cost due to locality, not from pure parallelism. This is Incorrect.\n- Option C asserts that the serial fraction $f_s$ decreases as $N$ grows due to overlapping memory latency, effectively replacing $f_s$ by $f_s/N$. The serial fraction pertains to inherently non-parallelizable work; overlapping memory latency in the parallelizable region does not convert serial work into parallel work. This is a modeling error. This is Incorrect.\n- Option D invokes weak scaling and Gustafson’s law, but the experiment is strong scaling with fixed problem size $T_1 = 40$ seconds on one core versus $T_4 = 7.5$ seconds on four threads. Gustafson’s law does not explain superlinear speedup at fixed problem size, nor does it obviate the need to account for cache effects. This is Incorrect.\n- Option E suggests that aggregate cache capacity directly and “mathematically” increases the parallel fraction so that Amdahl’s law permits larger speedups without changing assumptions. The serial/parallel fractions are defined on a baseline under fixed per-operation cost; changing cache behavior changes that cost model. One must either change the baseline or acknowledge that the speedup includes locality gains beyond ideal parallelism. This is Incorrect.\n\nTherefore, the only correct choice is Option A.", "answer": "$$\\boxed{A}$$", "id": "3620139"}]}