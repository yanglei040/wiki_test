{"hands_on_practices": [{"introduction": "In modern multicore processors, memory bandwidth is consumed not only by fetching data from DRAM but also by maintaining a consistent view of memory across private caches. This exercise explores \"false sharing,\" a common performance pitfall where logically independent variables, updated by different cores, happen to reside on the same cache line. By tracing the state transitions of the MESI coherence protocol, you will quantify the massive interconnect traffic generated, making the abstract cost of coherence tangibly clear [@problem_id:3621446].", "problem": "A shared-memory multiprocessor has $n$ symmetric cores, each with a private Level 1 (L1) write-back cache that implements the Modified, Exclusive, Shared, Invalid (MESI) protocol with write-invalidate semantics and Read For Ownership (RFO) on stores. The cache line size is $64$ bytes. Consider $n=8$ threads pinned one-to-one to the $8$ cores. Each thread repeatedly updates its own $8$-byte field inside a single shared struct whose $8$ fields are laid out contiguously and aligned so that all $8$ fields reside within the same cache line. A global barrier ensures a deterministic update order: in each iteration, thread $1$ performs one store to its field, then thread $2$ performs one store to its field, and so on up to thread $8$, after which the sequence repeats indefinitely. Assume the cache line never evicts once first touched, and ignore all traffic other than full-line data transfers on coherence events (treat invalidation messages as negligible in data volume). As a baseline, consider the same computation but with each thread’s field placed on a different cache line so that there is no false sharing; after warm-up, this baseline induces zero inter-core data movement per update.\n\nStarting from fundamental definitions of cache coherence and ownership in MESI and without invoking any pre-derived shortcut formulas, determine the steady-state average additional interconnect data volume per update that is caused solely by false sharing in the described scenario, compared to the no–false-sharing baseline. Express your final answer in bytes per update.", "solution": "The problem requires the calculation of the steady-state average additional interconnect data volume per update, resulting from false sharing. We are instructed to derive this from the fundamental principles of the Modified, Exclusive, Shared, Invalid (MESI) cache coherence protocol.\n\nLet $L_s$ be the cache line size, given as $L_s = 64$ bytes.\nLet $N$ be the number of cores and threads, given as $N=8$.\nEach thread $i$ (for $i \\in \\{1, 2, \\dots, 8\\}$) is pinned to a corresponding core, which has a private L1 cache, denoted $C_i$.\nThe $N$ threads repeatedly perform a store operation to their respective $8$-byte fields, which are all collocated on a single $64$-byte cache line, let's call it $CL$.\n\nThe execution pattern is a strict sequence: Thread $1$ stores, then Thread $2$, and so on, up to Thread $8$. This constitutes one full iteration, which then repeats. We are interested in the steady-state behavior.\n\nFirst, let's analyze the baseline case, where there is no false sharing. In this scenario, each thread's data resides on a separate cache line. After an initial \"warm-up\" store, Core $i$ will hold the cache line for its data in the **Modified (M)** state. Since no other core ever accesses this line, any subsequent stores by Thread $i$ will be local cache hits to a line in the **M** state. A write to a line in the **M** state does not generate any interconnect traffic. Thus, the interconnect data volume for the baseline case is $0$ bytes per update, as correctly stated in the problem.\n\nNow, let's analyze the false sharing scenario. We need to trace the state of the single shared cache line $CL$ across all $N=8$ caches. The MESI protocol's behavior is key. A store operation can only be performed if the core has exclusive ownership of the cache line. This corresponds to the line being in either the **Modified (M)** or **Exclusive (E)** state in that core's cache. A store to a line in the **Shared (S)** or **Invalid (I)** state will trigger a coherence event to gain ownership. Specifically, the core will issue a Read For Ownership (RFO) request on the interconnect.\n\nLet's trace one full, steady-state iteration, starting just before Thread $1$ performs its store. In the previous iteration, Thread $8$ was the last to perform a store. To do so, it must have acquired ownership of the line $CL$. This action would have brought $CL$ into its cache $C_8$ in the **M** state and simultaneously invalidated all other copies. Therefore, at the beginning of our observed iteration:\n-   The state of $CL$ in $C_8$ is **M**.\n-   The state of $CL$ in $C_1, C_2, \\dots, C_7$ is **I**.\n\nNow, the sequence of updates begins:\n1.  **Thread 1 performs a store:**\n    -   Core $1$ finds $CL$ in state **I** in its cache $C_1$. This is a write miss.\n    -   To satisfy the store, Core $1$ must gain ownership. It issues an RFO request on the interconnect.\n    -   Core $8$ snoops the interconnect and sees the RFO for $CL$, which it holds in state **M**.\n    -   According to the MESI protocol, Core $8$ must respond to the RFO. It flushes its modified version of $CL$ onto the interconnect. This constitutes a data transfer of the entire cache line. After providing the data, it changes its local state for $CL$ to **I**.\n    -   Core $1$ receives the $64$-byte cache line, performs its store operation, and sets its state for $CL$ to **M**.\n    -   **Data volume for this update:** One full cache line transfer, which is $L_s = 64$ bytes.\n    -   At the end of this step, $C_1$ has $CL$ in state **M**. All other caches ($C_2, \\dots, C_8$) have $CL$ in state **I**.\n\n2.  **Thread 2 performs a store:**\n    -   The situation is analogous to the previous step. Core $2$ finds $CL$ in state **I**.\n    -   Core $2$ issues an RFO.\n    -   Core $1$ snoops the RFO, finds it holds $CL$ in state **M**, and responds by sending its $64$-byte copy of $CL$ over the interconnect. It then transitions its state to **I**.\n    -   Core $2$ receives the line, performs its store, and transitions its state to **M**.\n    -   **Data volume for this update:** One full cache line transfer, $L_s = 64$ bytes.\n    -   At the end of this step, $C_2$ has $CL$ in state **M**. All other caches ($C_1, C_3, \\dots, C_8$) have $CL$ in state **I**.\n\nThis pattern continues for every thread in the sequence. For any thread $i \\in \\{1, 2, \\dots, 8\\}$, its store operation will find the line $CL$ in state **I**. It will issue an RFO, which will be satisfied by the previous owner (Core $i-1$, or Core $8$ if $i=1$) that holds the line in state **M**. This service always involves a cache-to-cache transfer of the entire $64$-byte line.\n\nAn iteration consists of $N=8$ updates. Each of these $8$ updates causes the migration of the entire cache line from the previous writer to the current writer.\n-   Data transfer for Update $1$: $64$ bytes (from $C_8$ to $C_1$)\n-   Data transfer for Update $2$: $64$ bytes (from $C_1$ to $C_2$)\n-   ...\n-   Data transfer for Update $8$: $64$ bytes (from $C_7$ to $C_8$)\n\nThe total interconnect data volume for one full iteration (comprising $8$ updates) is:\n$$V_{\\text{iteration}} = N \\times L_s = 8 \\times 64 \\text{ bytes} = 512 \\text{ bytes}$$\n\nThe problem asks for the *average* data volume *per update*. We can find this by dividing the total volume per iteration by the number of updates in that iteration:\n$$V_{\\text{avg}} = \\frac{V_{\\text{iteration}}}{N} = \\frac{N \\times L_s}{N} = L_s$$\nSubstituting the value of $L_s$:\n$$V_{\\text{avg}} = 64 \\text{ bytes per update}$$\n\nFinally, we need to determine the *additional* data volume compared to the no–false-sharing baseline.\nThe traffic in the false-sharing case is $V_{\\text{avg}} = 64$ bytes per update.\nThe traffic in the baseline case is given as $V_{\\text{baseline}} = 0$ bytes per update.\n\nThe additional interconnect data volume per update is:\n$$V_{\\text{additional}} = V_{\\text{avg}} - V_{\\text{baseline}} = 64 - 0 = 64 \\text{ bytes per update}$$\n\nThis result stems directly from the fundamental behavior of a write-invalidate protocol with RFO semantics: every time a core writes to a shared cache line that it does not own in a modified state, it must fetch the entire line, causing a data transfer of $L_s$. In this specific pathological scenario, this happens on every single update.", "answer": "$$\n\\boxed{64}\n$$", "id": "3621446"}, {"introduction": "Beyond hardware coherence, the way software organizes data in memory is a critical factor in bandwidth efficiency. This practice contrasts two fundamental data layout strategies: Array-of-Structures (AoS) and Structure-of-Arrays (SoA). You will analyze a common scenario where only a fraction of a data structure is accessed, allowing you to calculate the precise amount of bandwidth wasted by fetching unused data within a cache line, a key skill for performance-oriented programming [@problem_id:3621491].", "problem": "A streaming kernel traverses an array of structures and, for each structure, reads exactly two $4$-byte scalar fields: one at byte offset $0$ and one at byte offset $32$ within the structure. The array contains $N = 4096$ structures, each structure has total size $48$ bytes, and the base address of the array is aligned to a $64$-byte boundary. The memory system uses a $64$-byte cache line and, on a cache miss, transfers the entire cache line from main memory; cache line transfers are aligned to $64$-byte boundaries and cannot merge across lines. The loop is long enough and scheduled such that it saturates the sustained read bandwidth of the memory subsystem at $S = 51.2$ GB/s (decimal gigabytes per second).\n\nConsider two data layout variants for the same access pattern:\n1. Array of Structures (AoS): a single array stores all fields of each structure contiguously in memory in the $48$-byte record order described above. The kernel reads only the two fields at offsets $0$ and $32$ for every structure and never touches the other bytes.\n2. Structure of Arrays (SoA): two separate arrays store the two fields independently, each as a contiguous array of $4$-byte elements; the kernel streams through both arrays reading each element once. Both arrays are $64$-byte aligned.\n\nStarting from core definitions of memory bandwidth as bytes transferred per unit time and the behavior of cache line fills, and without invoking any pre-derived shortcut results, determine the wasted bandwidth $B$ (in GB/s) attributable specifically to the non-contiguous placement of the two fields in the AoS layout, relative to the SoA layout, under the stated assumptions. Express the final value in GB/s and round your answer to four significant figures.", "solution": "The fundamental quantities are the sustained memory bandwidth $S$ (bytes per unit time) and the bytes actually needed by the program versus the bytes that must be transferred due to $64$-byte cache line granularity. The wasted bandwidth is defined as the fraction of transferred bytes that the computation does not use, multiplied by $S$.\n\nWe analyze the two layouts.\n\nAoS analysis. Each structure has size $48$ bytes, and the two accessed fields are at offsets $0$ and $32$. The base address is aligned to a $64$-byte boundary. Let the structure index be $k \\in \\{0,1,\\dots,N-1\\}$. The byte addresses of the two fields for structure $k$ are\n$$\na_k = 48k + 0,\\qquad b_k = 48k + 32.\n$$\nLet the cache line size be $L = 64$ bytes. The cache line index containing an address $x$ is $\\ell(x) = \\left\\lfloor \\frac{x}{L} \\right\\rfloor$. The memory system transfers, for each distinct cache line index that any of the accessed addresses falls into, $L$ bytes, aligned to $64$-byte boundaries.\n\nTherefore, for the AoS pass, the total number of distinct cache lines touched is the size of the union of the sets\n$$\n\\mathcal{A} = \\left\\{ \\ell(a_k) : k = 0,1,\\dots,N-1 \\right\\},\\qquad \\mathcal{B} = \\left\\{ \\ell(b_k) : k = 0,1,\\dots,N-1 \\right\\}.\n$$\nWe first characterize $\\mathcal{A}$ and $\\mathcal{B}$. Consider the residues of $a_k$ modulo $L$:\n$$\na_k \\bmod L = (48k) \\bmod 64.\n$$\nSince $\\gcd(48,64) = 16$, the residues cycle through $0,48,32,16,0,48,32,16,\\dots$ with period $4$. The corresponding cache line indices for successive $k$ evolve as\n$$\n\\ell(a_0) = 0,\\ \\ell(a_1) = 0,\\ \\ell(a_2) = 1,\\ \\ell(a_3) = 2,\\ \\ell(a_4) = 3,\\ \\ell(a_5) = 3,\\ \\ell(a_6) = 4,\\ \\ell(a_7) = 5,\\ \\dots\n$$\nThis sequence increases by $1$ at each step except when the residue is $0$ (for $k \\equiv 0 \\ (\\text{mod }4)$), in which case the next $a_{k+1}$ remains in the same cache line. Counting increments over $N-1$ transitions, the number of zero-increment transitions is the count of $k \\in \\{0,\\dots,N-2\\}$ with $k \\equiv 0 \\ (\\text{mod }4)$, which equals $\\left\\lfloor \\frac{N-2}{4} \\right\\rfloor + 1$. Hence the total increment count is\n$$\n(N-1) - \\left( \\left\\lfloor \\frac{N-2}{4} \\right\\rfloor + 1 \\right) = N - 2 - \\left\\lfloor \\frac{N-2}{4} \\right\\rfloor,\n$$\nand the largest line index visited for $\\mathcal{A}$ is this count; therefore the number of distinct lines in $\\mathcal{A}$ is\n$$\n|\\mathcal{A}| = \\left( N - 2 - \\left\\lfloor \\frac{N-2}{4} \\right\\rfloor \\right) + 1 = N - 1 - \\left\\lfloor \\frac{N-2}{4} \\right\\rfloor.\n$$\n\nSimilarly for $b_k = 48k + 32$, the residues modulo $64$ cycle as $32,16,0,48,32,16,0,48,\\dots$ with period $4$. The sequence of line indices increases by $1$ at each step except when the residue is $0$ (which occurs for $k \\equiv 2 \\ (\\text{mod }4)$), in which case the next $b_{k+1}$ remains in the same line. The count of zero-increment transitions among $N-1$ transitions is the number of $k \\in \\{0,\\dots,N-2\\}$ with $k \\equiv 2 \\ (\\text{mod }4)$, which equals\n$$\n\\left\\lfloor \\frac{N-4}{4} \\right\\rfloor + 1 = \\left\\lfloor \\frac{N}{4} \\right\\rfloor.\n$$\nTherefore,\n$$\n|\\mathcal{B}| = \\left( (N-1) - \\left\\lfloor \\frac{N}{4} \\right\\rfloor \\right) + 1 = N - \\left\\lfloor \\frac{N}{4} \\right\\rfloor.\n$$\n\nWe now form the union. Observe that both $\\mathcal{A}$ and $\\mathcal{B}$ are contiguous sets of cache line indices starting at $0$ and extending to their respective maxima (there are duplicates but no skips). Thus the union $\\mathcal{A} \\cup \\mathcal{B}$ is also a contiguous set $[0, M]$ in terms of indices, where $M = \\max\\{\\max \\mathcal{A}, \\max \\mathcal{B}\\}$. Consequently,\n$$\n|\\mathcal{A} \\cup \\mathcal{B}| = \\max\\{|\\mathcal{A}|, |\\mathcal{B}|\\}.\n$$\nFor $N \\equiv 0$ or $1 \\ (\\text{mod }4)$, one finds $|\\mathcal{A}| = |\\mathcal{B}|$; for $N \\equiv 2$ or $3 \\ (\\text{mod }4)$, $|\\mathcal{B}| = |\\mathcal{A}| + 1$. In all cases,\n$$\n|\\mathcal{A} \\cup \\mathcal{B}| = N - \\left\\lfloor \\frac{N}{4} \\right\\rfloor.\n$$\n\nTherefore, the total bytes transferred by the AoS pass (per full traversal of the $N$ structures) is\n$$\nT_{\\text{AoS}} = L \\cdot |\\mathcal{A} \\cup \\mathcal{B}| = 64 \\left( N - \\left\\lfloor \\frac{N}{4} \\right\\rfloor \\right).\n$$\nThe bytes actually used by the computation are the sum of the two $4$-byte fields per structure:\n$$\nU = 8N.\n$$\nThe wasted fraction for AoS is then\n$$\nw_{\\text{AoS}} = 1 - \\frac{U}{T_{\\text{AoS}}} = 1 - \\frac{8N}{64 \\left( N - \\left\\lfloor \\frac{N}{4} \\right\\rfloor \\right)} = 1 - \\frac{N}{8 \\left( N - \\left\\lfloor \\frac{N}{4} \\right\\rfloor \\right)}.\n$$\n\nSoA analysis. In the Structure of Arrays (SoA) layout, the two fields reside in two separate contiguous arrays of $4$-byte elements, aligned to $64$ bytes. Streaming through each array touches a number of cache lines equal to the number of $64$-byte blocks needed to cover $N$ $4$-byte elements, i.e.,\n$$\n|\\mathcal{S}| = \\left\\lceil \\frac{4N}{64} \\right\\rceil = \\left\\lceil \\frac{N}{16} \\right\\rceil\n$$\nfor each array, and all bytes in those lines correspond to elements that are read by the loop. For $N$ a multiple of $16$, each line is fully utilized, yielding zero waste for both arrays. Hence the SoA wasted fraction is\n$$\nw_{\\text{SoA}} = 0 \\quad \\text{for } N \\text{ divisible by } 16.\n$$\n\nSince the kernel saturates the memory bandwidth at $S$, the wasted bandwidth attributable specifically to the non-contiguous AoS layout, relative to SoA, is\n$$\nB = S \\cdot \\left( w_{\\text{AoS}} - w_{\\text{SoA}} \\right) = S \\cdot w_{\\text{AoS}}.\n$$\n\nNumerical evaluation. Here $N = 4096$ is divisible by $16$, so $\\left\\lfloor \\frac{N}{4} \\right\\rfloor = \\left\\lfloor \\frac{4096}{4} \\right\\rfloor = 1024$. Therefore,\n$$\nT_{\\text{AoS}} = 64 \\left( 4096 - 1024 \\right) = 64 \\cdot 3072 = 196608,\n$$\n$$\nU = 8 \\cdot 4096 = 32768,\n$$\nand\n$$\nw_{\\text{AoS}} = 1 - \\frac{32768}{196608} = 1 - \\frac{1}{6} = \\frac{5}{6}.\n$$\nWith $S = 51.2$ GB/s,\n$$\nB = 51.2 \\cdot \\frac{5}{6} = 42.\\overline{6} \\ \\text{GB/s}.\n$$\nRounded to four significant figures and expressed in GB/s, the result is $42.67$ GB/s.", "answer": "$$\\boxed{42.67}$$", "id": "3621491"}, {"introduction": "Modern CPUs employ speculative mechanisms like hardware prefetchers to hide memory latency, but this speculation comes at a cost to bandwidth. This problem provides a model to evaluate the efficiency of a prefetcher using standard performance metrics like accuracy and coverage. By calculating the bandwidth consumed by incorrect prefetches, you will learn to reason about the trade-offs inherent in complex hardware systems and how to build simple models to quantify their impact [@problem_id:3621496].", "problem": "A multicore processor uses a stream prefetcher that is throttled by the memory controller to limit interference in the Dynamic Random Access Memory (DRAM) subsystem. The memory controller enforces a nominal prefetch bandwidth budget of $B_{\\text{pref}}$ (in GB/s). Hardware performance counters report two prefetcher-quality metrics over a long, steady-state interval for a single running application:\n- Accuracy $a$: the fraction of prefetched cache lines that are subsequently accessed by a demand instruction before eviction.\n- Coverage $c$: the fraction of the interval during which the prefetcher is actively issuing prefetches at its budgeted rate.\n\nAssume the following operational model grounded in first principles:\n- Bandwidth is data volume per unit time.\n- When the prefetcher is active, it issues prefetch requests at the enforced budget $B_{\\text{pref}}$.\n- Each prefetched byte is either useful (eventually accessed by a demand instruction) or useless (never accessed by a demand instruction).\n- Accuracy $a$ can be interpreted as the conditional probability that a prefetched byte is useful; therefore, the probability that a prefetched byte is useless is $1 - a$.\n\nUnder this model, derive from these definitions an expression for the average DRAM bandwidth wasted on incorrect prefetches (bytes that are fetched by the prefetcher but never used by demand) over the entire interval, and then compute its value for $a = 0.72$, $c = 0.60$, and $B_{\\text{pref}} = 24$ GB/s. Express the final wasted bandwidth in GB/s and round your answer to four significant figures.", "solution": "The objective is to find the average bandwidth wasted on incorrect prefetches, which we will denote as $B_{\\text{wasted}}$. We can reason about this by considering the average rate at which data is being prefetched and the fraction of that data which is useless.\n\nThe prefetcher is not always active. It is active only for a fraction $c$ of the total time interval. When it is active, it consumes bandwidth at a rate of $B_{\\text{pref}}$. When it is inactive (for a fraction $1-c$ of the time), it consumes zero prefetch bandwidth.\n\nTo find the average prefetch bandwidth over the entire interval, we can perform a time-weighted average of the instantaneous bandwidth. Let $\\bar{B}_{\\text{pref\\_total}}$ be the average total bandwidth consumed by prefetching.\n$$\n\\bar{B}_{\\text{pref\\_total}} = (c \\times B_{\\text{pref}}) + ((1-c) \\times 0) = c \\cdot B_{\\text{pref}}\n$$\nThis expression, $c \\cdot B_{\\text{pref}}$, represents the time-averaged rate at which data is being fetched by the prefetcher over the long, steady-state interval.\n\nNext, we must determine what fraction of this fetched data is wasted. The problem defines accuracy, $a$, as the fraction of prefetched data that is useful (i.e., eventually used). Consequently, the fraction of prefetched data that is useless (i.e., wasted) must be $1 - a$.\n\nWasted bandwidth is, by definition, the rate at which useless data is transferred. This can be found by multiplying the average total prefetch bandwidth by the fraction of data that is useless.\n$$\nB_{\\text{wasted}} = (\\text{fraction of prefetch that is useless}) \\times (\\text{average total prefetch bandwidth})\n$$\nSubstituting the expressions we have derived:\n$$\nB_{\\text{wasted}} = (1 - a) \\cdot \\bar{B}_{\\text{pref\\_total}}\n$$\n$$\nB_{\\text{wasted}} = (1 - a) \\cdot c \\cdot B_{\\text{pref}}\n$$\nThis is the final analytical expression for the average wasted prefetch bandwidth.\n\nNow, we compute the numerical value using the provided data:\n-   $a = 0.72$\n-   $c = 0.60$\n-   $B_{\\text{pref}} = 24$ GB/s\n\nSubstituting these values into the derived expression:\n$$\nB_{\\text{wasted}} = (1 - 0.72) \\cdot (0.60) \\cdot (24 \\frac{\\text{GB}}{\\text{s}})\n$$\n$$\nB_{\\text{wasted}} = (0.28) \\cdot (0.60) \\cdot (24 \\frac{\\text{GB}}{\\text{s}})\n$$\nPerforming the multiplication:\n$$\n0.28 \\times 0.60 = 0.168\n$$\n$$\n0.168 \\times 24 = 4.032\n$$\nThus, the numerical value for the average wasted bandwidth is $4.032$ GB/s. The problem requires the answer to be rounded to four significant figures. The calculated result, $4.032$, already has exactly four significant figures.", "answer": "$$\n\\boxed{4.032}\n$$", "id": "3621496"}]}