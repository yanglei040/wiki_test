{"hands_on_practices": [{"introduction": "One of the classic trade-offs in processor design involves choosing the right pipeline depth. While a deeper pipeline allows for a higher clock frequency by reducing the amount of logic per stage, it also increases the penalty for events like branch mispredictions that require flushing the pipeline. This exercise challenges you to model these competing effects mathematically to find the optimal pipeline depth that maximizes overall performance, measured in instructions per second [@problem_id:3664684]. By working through this, you will develop a quantitative understanding of how microarchitectural parameters interact to determine processor throughput.", "problem": "A processor designer is studying how the pipeline depth $D$ affects execution throughput for a single-issue in-order core running a branch-intensive workload. For the workload, the following are measured or assumed:\n\n- The base Cycles Per Instruction (CPI) ignoring control hazards is $C_{0} = 1.0$.\n- The dynamic fraction of branch instructions is $r_{b} = 0.20$ branches per instruction.\n- The probability of branch misprediction is $p_{m} = 0.10$ per branch.\n- The branch misprediction penalty in cycles scales proportionally with pipeline depth, modeled as $P_{\\mathrm{br}}(D) = \\beta D$ with $\\beta = 1.0$ cycles per stage.\n- The clock period $T_{\\mathrm{clk}}(D)$ is modeled by splitting the total combinational delay evenly across stages and adding a fixed per-stage overhead due to latches and clock distribution: $T_{\\mathrm{clk}}(D) = t_{\\mathrm{ov}} + \\frac{t_{\\mathrm{comb}}}{D}$ with $t_{\\mathrm{ov}} = 200\\,\\mathrm{ps}$ and $t_{\\mathrm{comb}} = 3500\\,\\mathrm{ps}$.\n\nUse the following fundamental performance model:\n\n- Instructions Per Cycle (IPC) and CPI are related by $IPC(D) = \\frac{1}{CPI(D)}$.\n- The average CPI with control hazards is $CPI(D) = C_{0} + r_{b} p_{m} P_{\\mathrm{br}}(D)$.\n- Instructions per second is $IPS(D) = \\frac{IPC(D)}{T_{\\mathrm{clk}}(D)}$.\n\nTreat $D$ as a positive real variable. Derive a parametric expression for the $D$ that maximizes $IPS(D)$ under the given model and compute its value for the specified workload parameters. Report the optimal real-valued pipeline depth $D$ (dimensionless). Round your answer to three significant figures.", "solution": "The user has requested to find the optimal pipeline depth $D$ that maximizes the processor's performance, measured in Instructions Per Second ($IPS$). The problem provides a set of models for how performance metrics depend on $D$.\n\nFirst, we must construct the objective function, $IPS(D)$, using the given relationships.\nThe average Cycles Per Instruction, $CPI(D)$, is modeled as the sum of a base CPI, $C_0$, and a penalty term for branch mispredictions:\n$$CPI(D) = C_{0} + r_{b} p_{m} P_{\\mathrm{br}}(D)$$\nThe branch misprediction penalty, $P_{\\mathrm{br}}(D)$, is proportional to the pipeline depth $D$:\n$$P_{\\mathrm{br}}(D) = \\beta D$$\nSubstituting this into the $CPI$ equation gives:\n$$CPI(D) = C_{0} + r_{b} p_{m} \\beta D$$\nThe Instructions Per Cycle, $IPC(D)$, is the reciprocal of the $CPI(D)$:\n$$IPC(D) = \\frac{1}{CPI(D)} = \\frac{1}{C_{0} + r_{b} p_{m} \\beta D}$$\nThe clock period, $T_{\\mathrm{clk}}(D)$, is also a function of the pipeline depth:\n$$T_{\\mathrm{clk}}(D) = t_{\\mathrm{ov}} + \\frac{t_{\\mathrm{comb}}}{D}$$\nFinally, the performance in Instructions Per Second, $IPS(D)$, is given by the ratio of $IPC(D)$ to the clock period $T_{\\mathrm{clk}}(D)$:\n$$IPS(D) = \\frac{IPC(D)}{T_{\\mathrm{clk}}(D)} = \\frac{\\frac{1}{C_{0} + r_{b} p_{m} \\beta D}}{t_{\\mathrm{ov}} + \\frac{t_{\\mathrm{comb}}}{D}}$$\nThis can be written as:\n$$IPS(D) = \\frac{1}{(C_{0} + r_{b} p_{m} \\beta D) (t_{\\mathrm{ov}} + \\frac{t_{\\mathrm{comb}}}{D})}$$\nTo maximize $IPS(D)$, we must minimize its denominator, which we define as a function $f(D)$. The problem specifies that $D$ should be treated as a positive real variable.\n$$f(D) = (C_{0} + r_{b} p_{m} \\beta D) \\left(t_{\\mathrm{ov}} + \\frac{t_{\\mathrm{comb}}}{D}\\right)$$\nExpanding this expression, we get:\n$$f(D) = C_{0}t_{\\mathrm{ov}} + C_{0}\\frac{t_{\\mathrm{comb}}}{D} + (r_{b} p_{m} \\beta D)t_{\\mathrm{ov}} + (r_{b} p_{m} \\beta D)\\frac{t_{\\mathrm{comb}}}{D}$$\n$$f(D) = C_{0}t_{\\mathrm{ov}} + \\frac{C_{0}t_{\\mathrm{comb}}}{D} + r_{b} p_{m} \\beta t_{\\mathrm{ov}} D + r_{b} p_{m} \\beta t_{\\mathrm{comb}}$$\nTo find the value of $D$ that minimizes $f(D)$, we take the derivative of $f(D)$ with respect to $D$ and set it to zero. First, we rearrange the terms based on their dependence on $D$:\n$$f(D) = (r_{b} p_{m} \\beta t_{\\mathrm{ov}})D + (C_{0}t_{\\mathrm{comb}})D^{-1} + (C_{0}t_{\\mathrm{ov}} + r_{b} p_{m} \\beta t_{\\mathrm{comb}})$$\nThe derivative with respect to $D$ is:\n$$\\frac{df}{dD} = r_{b} p_{m} \\beta t_{\\mathrm{ov}} - C_{0}t_{\\mathrm{comb}}D^{-2} = r_{b} p_{m} \\beta t_{\\mathrm{ov}} - \\frac{C_{0}t_{\\mathrm{comb}}}{D^2}$$\nSetting the derivative to zero finds the critical point:\n$$r_{b} p_{m} \\beta t_{\\mathrm{ov}} - \\frac{C_{0}t_{\\mathrm{comb}}}{D^2} = 0$$\n$$r_{b} p_{m} \\beta t_{\\mathrm{ov}} = \\frac{C_{0}t_{\\mathrm{comb}}}{D^2}$$\nSolving for $D^2$:\n$$D^2 = \\frac{C_{0}t_{\\mathrm{comb}}}{r_{b} p_{m} \\beta t_{\\mathrm{ov}}}$$\nSince $D$ must be positive, we take the positive square root to find the optimal pipeline depth, $D_{\\mathrm{opt}}$:\n$$D_{\\mathrm{opt}} = \\sqrt{\\frac{C_{0}t_{\\mathrm{comb}}}{r_{b} p_{m} \\beta t_{\\mathrm{ov}}}}$$\nTo confirm this is a minimum, we evaluate the second derivative:\n$$\\frac{d^2f}{dD^2} = \\frac{d}{dD}\\left(r_{b} p_{m} \\beta t_{\\mathrm{ov}} - C_{0}t_{\\mathrm{comb}}D^{-2}\\right) = 2C_{0}t_{\\mathrm{comb}}D^{-3} = \\frac{2C_{0}t_{\\mathrm{comb}}}{D^3}$$\nSince all parameters $C_{0}$, $t_{\\mathrm{comb}}$ are positive, and $D > 0$, the second derivative is positive. This confirms that the found critical point corresponds to a local minimum of $f(D)$, and thus a local maximum of $IPS(D)$. As it's the only critical point for $D > 0$, it is the global optimum.\n\nNow, we substitute the given numerical values:\n$C_{0} = 1.0$ (dimensionless)\n$r_{b} = 0.20$ (dimensionless)\n$p_{m} = 0.10$ (dimensionless)\n$\\beta = 1.0$ (cycles/stage)\n$t_{\\mathrm{ov}} = 200\\,\\mathrm{ps}$\n$t_{\\mathrm{comb}} = 3500\\,\\mathrm{ps}$\n\n$$D_{\\mathrm{opt}} = \\sqrt{\\frac{(1.0)(3500\\,\\mathrm{ps})}{(0.20)(0.10)(1.0)(200\\,\\mathrm{ps})}}$$\nThe units of picoseconds ($ps$) in the numerator and denominator cancel out, leaving a dimensionless quantity as required for $D$.\n$$D_{\\mathrm{opt}} = \\sqrt{\\frac{3500}{(0.02)(200)}} = \\sqrt{\\frac{3500}{4}}$$\n$$D_{\\mathrm{opt}} = \\sqrt{875}$$\nCalculating the numerical value:\n$$D_{\\mathrm{opt}} \\approx 29.5803989...$$\nThe problem asks for the result to be rounded to three significant figures.\n$$D_{\\mathrm{opt}} \\approx 29.6$$", "answer": "$$\\boxed{29.6}$$", "id": "3664684"}, {"introduction": "Amdahl's Law provides a crucial framework for understanding the limits of performance enhancement. In this practice, you will apply this law to a common scenario: improving the accuracy of a CPU's branch predictor, a critical component in modern pipelined processors. Your task is to derive the overall system speedup based on the improvement in prediction accuracy and the fraction of time originally lost to mispredictions, highlighting how the impact of any optimization is constrained by the proportion of time that optimization is active [@problem_id:3664724]. This exercise reinforces a fundamental principle of performance engineering.", "problem": "A pipelined Central Processing Unit (CPU) executes a benchmark workload for which cycle accounting from a hardware performance monitor or cycle-accurate simulation reports that a fraction $f$ of the baseline execution time is attributable to stalls caused by branch mispredictions. The baseline branch prediction accuracy is $p$, meaning that the baseline misprediction rate is $1-p$. A predictor upgrade increases the accuracy to $p+\\Delta p$, with $0<p<1$, $0<\\Delta p\\leq 1-p$, and $0<f<1$. Assume that:\n- the branch misprediction penalty per mispredicted branch is constant across baseline and upgraded predictor, \n- the fraction of instructions that are conditional branches remains unchanged,\n- all other sources of execution time remain unaffected,\n- stall time due to branches scales linearly with the misprediction rate.\n\nStarting from fundamental definitions of execution time decomposition and the principle of Amdahlâ€™s Law (which states that end-to-end speedup depends on the fraction of time improved and the improvement factor applied to that fraction), derive a closed-form analytic expression for the overall speedup $S$ of the workload after the accuracy improvement, expressed solely in terms of $p$, $\\Delta p$, and $f$. Provide the final expression for $S$ in its simplest form. The speedup is dimensionless and does not require units. No numerical rounding is required.", "solution": "Let $T_{base}$ be the total execution time of the workload on the baseline system. We can decompose this time into two components: the time spent on stalls due to branch mispredictions, $T_{stall, base}$, and the time spent on all other activities, $T_{other}$.\n$$T_{base} = T_{stall, base} + T_{other}$$\nAccording to the problem statement, the fraction of the baseline execution time attributable to these stalls is $f$.\n$$T_{stall, base} = f \\cdot T_{base}$$\nConsequently, the time for all other activities is:\n$$T_{other} = T_{base} - T_{stall, base} = T_{base} - f \\cdot T_{base} = (1-f) \\cdot T_{base}$$\nNow, consider the system with the upgraded branch predictor. Let the new total execution time be $T_{new}$. The problem states that all other sources of execution time remain unaffected, which means the component $T_{other}$ is unchanged. The new execution time is the sum of the new stall time, $T_{stall, new}$, and $T_{other}$.\n$$T_{new} = T_{stall, new} + T_{other}$$\nThe problem states that the stall time due to branches scales linearly with the misprediction rate. Let $m_{base}$ be the baseline misprediction rate and $m_{new}$ be the new misprediction rate.\nThe baseline prediction accuracy is $p$, so the baseline misprediction rate is:\n$$m_{base} = 1 - p$$\nThe upgraded prediction accuracy is $p + \\Delta p$, so the new misprediction rate is:\n$$m_{new} = 1 - (p + \\Delta p) = 1 - p - \\Delta p$$\nThe linear scaling implies that the ratio of stall times is equal to the ratio of their corresponding misprediction rates:\n$$\\frac{T_{stall, new}}{T_{stall, base}} = \\frac{m_{new}}{m_{base}} = \\frac{1 - p - \\Delta p}{1 - p}$$\nWe can express the new stall time, $T_{stall, new}$, in terms of the baseline stall time:\n$$T_{stall, new} = T_{stall, base} \\cdot \\left(\\frac{1 - p - \\Delta p}{1 - p}\\right)$$\nSubstituting $T_{stall, base} = f \\cdot T_{base}$:\n$$T_{stall, new} = (f \\cdot T_{base}) \\cdot \\left(\\frac{1 - p - \\Delta p}{1 - p}\\right)$$\nNow we can write the expression for the new total execution time, $T_{new}$:\n$$T_{new} = T_{stall, new} + T_{other} = (f \\cdot T_{base}) \\left(\\frac{1 - p - \\Delta p}{1 - p}\\right) + (1-f) \\cdot T_{base}$$\nFactoring out $T_{base}$:\n$$T_{new} = T_{base} \\left[ f \\left(\\frac{1 - p - \\Delta p}{1 - p}\\right) + (1-f) \\right]$$\nThe overall speedup $S$ is defined as the ratio of the baseline execution time to the new execution time:\n$$S = \\frac{T_{base}}{T_{new}}$$\nSubstituting the expression for $T_{new}$:\n$$S = \\frac{T_{base}}{T_{base} \\left[ f \\left(\\frac{1 - p - \\Delta p}{1 - p}\\right) + (1-f) \\right]} = \\frac{1}{f \\left(\\frac{1 - p - \\Delta p}{1 - p}\\right) + (1-f)}$$\nTo simplify this expression, we find a common denominator for the terms in the denominator:\n$$S = \\frac{1}{\\frac{f(1 - p - \\Delta p) + (1-f)(1 - p)}{1 - p}}$$\n$$S = \\frac{1 - p}{f(1 - p - \\Delta p) + (1-f)(1 - p)}$$\nExpanding the terms in the denominator:\n$$f(1 - p - \\Delta p) + (1-f)(1 - p) = (f - fp - f\\Delta p) + (1 - p - f + fp)$$\nThe terms $f$ and $-f$ cancel, as do $-fp$ and $fp$:\n$$f - fp - f\\Delta p + 1 - p - f + fp = 1 - p - f\\Delta p$$\nSubstituting this simplified expression back into the equation for $S$:\n$$S = \\frac{1 - p}{1 - p - f\\Delta p}$$\nThis result is consistent with Amdahl's Law, which states that speedup is given by $S = \\frac{1}{(1-f_{enh}) + \\frac{f_{enh}}{S_{enh}}}$. Here, the fraction of time enhanced is the stall time, so $f_{enh} = f$. The speedup of the enhanced portion (the stalls) is $S_{enh} = \\frac{T_{stall, base}}{T_{stall, new}} = \\frac{1-p}{1-p-\\Delta p}$. Plugging these into Amdahl's formula yields the same intermediate expression derived above, confirming the result. The final expression for the overall speedup $S$ is in its simplest form.", "answer": "$$\n\\boxed{\\frac{1-p}{1-p-f\\Delta p}}\n$$", "id": "3664724"}, {"introduction": "As we move from single-core to multicore systems, our models for performance must also evolve. This problem shifts our perspective from the fixed-problem size assumption of Amdahl's Law to the weak scaling model described by Gustafson's Law, where problem size grows with the available processing power. You will model the speedup of a parallel application, but also incorporate a realistic overhead term for cache coherence to see how practical limitations temper ideal scalability [@problem_id:3664706]. This practice demonstrates how to combine theoretical laws with empirical data to build more accurate predictive models for parallel systems.", "problem": "A memory-bound scientific kernel is evaluated under weak scaling on a shared-memory multicore system. Under weak scaling, the problem size grows proportionally with the number of cores so that each core is assigned the same amount of work as in the single-core case. Let the single-core execution time for the per-core work unit be decomposed into an inherently serial time $T_{s}$ and a perfectly parallelizable time $T_{p}$, and define the single-core serial fraction $\\alpha$ by $\\alpha = \\frac{T_{s}}{T_{s} + T_{p}}$. For this workload, profiling on a single core finds $\\alpha = 0.12$.\n\nTo account for the cost of cache coherence traffic that grows with the number of cores, the net speedup is modeled as the ideal weak-scaling speedup minus an empirical coherence-overhead loss term $\\gamma(N)$ that is a function of the core count $N$. For this kernel, empirical analysis suggests modeling $\\gamma(N)$ as linear in the number of additional cores: $\\gamma(N) = c\\,(N - 1)$, where $c$ is a workload-dependent constant to be determined.\n\nYou run a weak-scaling experiment at $N = 16$ cores and observe a net speedup of $S_{\\text{obs}} = 13.1$ relative to the single-core runtime of the $N$-times larger problem, as per the weak-scaling definition.\n\nUsing only the definitions above:\n- Derive the ideal weak-scaling speedup expression in terms of $\\alpha$ and $N$.\n- Incorporate the empirical loss term $\\gamma(N)$ to obtain a modeled net speedup $S_{\\text{model}}(N)$.\n- Use the observation at $N = 16$ to determine $c$.\n- Predict $S_{\\text{model}}(64)$.\n\nReport only the final predicted speedup for $N = 64$ as a real number rounded to four significant figures. No units are required.", "solution": "The solution proceeds in four steps as requested: deriving the ideal speedup, incorporating the loss term, determining the empirical constant, and predicting the speedup for a new core count.\n\nFirst, we derive the ideal weak-scaling speedup, $S_{\\text{ideal}}(N)$, for a system with $N$ cores. Let the execution time of the base workload on a single core be $T_1$. This time is composed of a serial part, $T_s$, and a parallelizable part, $T_p$, such that $T_1 = T_s + T_p$. The serial fraction, $\\alpha$, is defined as $\\alpha = \\frac{T_s}{T_s + T_p}$. It follows that $T_s = \\alpha T_1$ and $T_p = (1 - \\alpha) T_1$.\n\nIn weak scaling, the problem size is increased proportionally with the number of cores, $N$. The total amount of parallelizable work is scaled by a factor of $N$, while the serial work remains constant. The total work for the scaled problem, if run on a single core, would require time $T_{scaled, 1} = T_s + N \\cdot T_p$. When this scaled workload is run on $N$ cores, the serial part still takes time $T_s$ (as it cannot be parallelized), and the parallelizable part is perfectly divided among the $N$ cores, taking time $\\frac{N \\cdot T_p}{N} = T_p$. The total execution time on $N$ cores is thus $T_{scaled, N} = T_s + T_p$.\n\nThe weak-scaling speedup is defined as the ratio of the time to run the scaled problem on a single core to the time to run it on $N$ cores.\n$$S_{\\text{ideal}}(N) = \\frac{T_{scaled, 1}}{T_{scaled, N}} = \\frac{T_s + N T_p}{T_s + T_p}$$\nSubstituting the expressions for $T_s$ and $T_p$ in terms of $\\alpha$ and $T_1$:\n$$S_{\\text{ideal}}(N) = \\frac{\\alpha T_1 + N(1-\\alpha)T_1}{\\alpha T_1 + (1-\\alpha)T_1} = \\frac{T_1(\\alpha + N(1-\\alpha))}{T_1(\\alpha + 1 - \\alpha)} = \\alpha + N(1-\\alpha)$$\nThis expression can be rearranged to a more common form of Gustafson's Law:\n$$S_{\\text{ideal}}(N) = N - N\\alpha + \\alpha = N - (N-1)\\alpha$$\n\nSecond, we incorporate the empirical coherence-overhead loss term, $\\gamma(N)$, to define the modeled net speedup, $S_{\\text{model}}(N)$. The problem states that the net speedup is the ideal speedup minus the loss term.\n$$S_{\\text{model}}(N) = S_{\\text{ideal}}(N) - \\gamma(N)$$\nThe loss term is given as $\\gamma(N) = c(N-1)$, where $c$ is a constant. Substituting the expressions for $S_{\\text{ideal}}(N)$ and $\\gamma(N)$:\n$$S_{\\text{model}}(N) = [N - (N-1)\\alpha] - c(N-1)$$\n$$S_{\\text{model}}(N) = N - (N-1)(\\alpha + c)$$\n\nThird, we determine the constant $c$ using the provided experimental data. We are given that for a single core, the serial fraction is $\\alpha = 0.12$. At a core count of $N=16$, the observed speedup is $S_{\\text{obs}} = 13.1$. We equate the model to this observation: $S_{\\text{model}}(16) = 13.1$.\n$$13.1 = 16 - (16-1)(0.12 + c)$$\n$$13.1 = 16 - 15(0.12 + c)$$\nRearranging the equation to solve for the term containing $c$:\n$$15(0.12 + c) = 16 - 13.1$$\n$$15(0.12 + c) = 2.9$$\n$$0.12 + c = \\frac{2.9}{15}$$\nNow, we can solve for $c$:\n$$c = \\frac{2.9}{15} - 0.12 = \\frac{29}{150} - \\frac{12}{100} = \\frac{29}{150} - \\frac{18}{150} = \\frac{11}{150}$$\n\nFourth, we use the calibrated model to predict the net speedup for $N=64$ cores, denoted $S_{\\text{model}}(64)$.\n$$S_{\\text{model}}(64) = 64 - (64-1)(\\alpha + c) = 64 - 63(\\alpha + c)$$\nFrom the previous step, we found the value of the combined term $\\alpha + c$:\n$$\\alpha + c = \\frac{2.9}{15}$$\nSubstituting this into the equation for $S_{\\text{model}}(64)$:\n$$S_{\\text{model}}(64) = 64 - 63 \\left( \\frac{2.9}{15} \\right)$$\nWe can simplify the fraction $\\frac{63}{15}$ by dividing the numerator and denominator by $3$: $\\frac{63}{15} = \\frac{21}{5} = 4.2$.\n$$S_{\\text{model}}(64) = 64 - 4.2 \\times 2.9$$\nPerforming the multiplication:\n$$4.2 \\times 2.9 = 12.18$$\nFinally, we compute the predicted speedup:\n$$S_{\\text{model}}(64) = 64 - 12.18 = 51.82$$\nThe problem requires the answer to be a real number rounded to four significant figures. The result $51.82$ has exactly four significant figures.", "answer": "$$\n\\boxed{51.82}\n$$", "id": "3664706"}]}