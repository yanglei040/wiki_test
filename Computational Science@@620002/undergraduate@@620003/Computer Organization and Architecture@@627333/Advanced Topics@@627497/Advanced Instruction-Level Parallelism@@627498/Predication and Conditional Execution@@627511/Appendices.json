{"hands_on_practices": [{"introduction": "To build a solid foundation, we begin by quantifying the core performance trade-off between predication and traditional branching. This exercise guides you through deriving a mathematical model to compare the two techniques based on their impact on the overall Cycles Per Instruction ($CPI$) of a program. By analyzing factors like branch prediction accuracy and the cycle costs of different code paths, you will determine the critical branch misprediction rate at which predication becomes the more efficient choice, providing a powerful analytical tool for architectural decision-making [@problem_id:3667908].", "problem": "Consider a pipelined processor that implements conditional execution either by predication or by a control-flow branch. A single program region consists of two mutually exclusive blocks, a \"true\" block and a \"false\" block. Let the probability that the condition evaluates to true be $p$ and the probability that it evaluates to false be $1-p$. Outside this conditional region, the program achieves a base Cycles Per Instruction (CPI) denoted by $CPI_0$, which is the expected cycles per retired instruction contributed by all other parts of the program.\n\nUnder predication, assume an ideal annulment mechanism: instructions whose predicate evaluates to false are nullified before they consume execution resources, so they contribute negligible cycles. However, predicated instructions whose predicate evaluates to true may incur overhead due to predicate evaluation or guarding, which is included in their execution costs. Let $C_T$ denote the average cycles contributed by the \"true\" block when its predicate is true, and let $C_F$ denote the average cycles contributed by the \"false\" block when its predicate is true. All costs are nonnegative and are steady across dynamic instances.\n\nUnder a branch-based implementation, assume a dynamic branch predictor with accuracy $a$ and misprediction penalty $M$ cycles per misprediction. Let $q = 1-a$ denote the misprediction rate. When the predictor is correct and the branch is resolved without penalty, the executed block contributes $B_T$ cycles if the condition is true or $B_F$ cycles if the condition is false. All costs are nonnegative and are steady across dynamic instances.\n\nStarting from the fundamental definition that the expected cycles contributed by a region is the sum over events of the event probability times its cycle cost, and that Cycles Per Instruction (CPI) is the expected cycles per retired instruction, perform the following:\n\n1. Derive the expression for the CPI of the program under predication, in terms of $CPI_0$, $p$, $C_T$, and $C_F$.\n2. Derive the expression for the CPI of the program under branching, in terms of $CPI_0$, $p$, $B_T$, $B_F$, and $qM$.\n3. By equating the two CPI expressions, solve for the critical misprediction rate $q^{\\star}$ that makes the two CPIs equal. This $q^{\\star}$ partitions the parameter space into regions where predication decreases the CPI relative to branching versus where branching is better.\n\nExpress your final answer as a single closed-form analytic expression for $q^{\\star}$ in terms of $p$, $C_T$, $C_F$, $B_T$, $B_F$, and $M$. No numerical values are required, and no rounding is necessary. The final answer must be a single expression without units.", "solution": "The total CPI of the program is the sum of the base program's CPI and the expected cycle cost contributed by the conditional region. To find the crossover point, we can equate the expected cycle costs of the conditional region for each method.\n\nLet's denote the expected cycle cost of the conditional region under predication as $E_{pred}$ and under branching as $E_{branch}$.\n\n1. Derivation of CPI under Predication\n\nUnder predication, both blocks are fetched, but only the block whose predicate is true is executed. The annulled block contributes negligible cycles.\n- With probability $p$, the condition is true, and the \"true\" block executes, costing $C_T$ cycles.\n- With probability $1-p$, the condition is false, and the \"false\" block executes, costing $C_F$ cycles.\n\nThe expected cycle cost for the region, $E_{pred}$, is:\n$$E_{pred} = p \\cdot C_T + (1-p) \\cdot C_F$$\n\nThe total CPI of the program under predication, $CPI_{predication}$, is:\n$$CPI_{predication} = CPI_0 + p C_T + (1-p) C_F$$\n\n2. Derivation of CPI under Branching\n\nUnder branching, the total cycle cost is the sum of the execution cost of the correct path and the expected penalty from mispredictions.\n- The expected path execution cost is $p \\cdot B_T + (1-p) \\cdot B_F$.\n- The expected misprediction penalty is the misprediction rate $q$ times the penalty $M$, which is $qM$.\n\nThe total expected cycle cost for the region under branching, $E_{branch}$, is:\n$$E_{branch} = p B_T + (1-p) B_F + qM$$\n\nThe total CPI of the program under branching, $CPI_{branching}$, is:\n$$CPI_{branching} = CPI_0 + p B_T + (1-p) B_F + qM$$\n\n3. Solving for the Critical Misprediction Rate $q^{\\star}$\n\nThe critical misprediction rate, $q^{\\star}$, is the value of $q$ where the performance is identical, i.e., $CPI_{predication} = CPI_{branching}$.\n\n$$CPI_0 + p C_T + (1-p) C_F = CPI_0 + p B_T + (1-p) B_F + q^{\\star}M$$\n\nThe base $CPI_0$ term cancels out:\n$$p C_T + (1-p) C_F = p B_T + (1-p) B_F + q^{\\star}M$$\n\nNow, we solve for $q^{\\star}$ by isolating the $q^{\\star}M$ term:\n$$q^{\\star}M = p C_T + (1-p) C_F - (p B_T + (1-p) B_F)$$\n\nGrouping terms by $p$ and $(1-p)$:\n$$q^{\\star}M = p(C_T - B_T) + (1-p)(C_F - B_F)$$\n\nFinally, dividing by $M$ gives the expression for $q^{\\star}$:\n$$q^{\\star} = \\frac{p(C_T - B_T) + (1-p)(C_F - B_F)}{M}$$\n\nThis expression represents the branch misprediction rate at which the expected cycle costs of predication and branch-based control flow are equal. If the actual misprediction rate $q$ is greater than $q^{\\star}$, predication is more efficient. If $q$ is less than $q^{\\star}$, branching is more efficient.", "answer": "$$ \\boxed{\\frac{p(C_T - B_T) + (1-p)(C_F - B_F)}{M}} $$", "id": "3667908"}, {"introduction": "Beyond raw performance, ensuring program correctness after if-conversion is paramount. This practice shifts our focus to the logical integrity of predicated code, using the common pattern of saving and restoring a register via the stack. You will analyze why paired, state-modifying operations—like a `push` and a `pop`—must be governed by the exact same latched predicate to guarantee that the program state remains consistent, regardless of which conditional path is taken. This exercise reveals a fundamental principle for generating correct predicated code and avoiding subtle bugs related to state corruption [@problem_id:3667887].", "problem": "Consider an Instruction Set Architecture (ISA) that supports static predication: each instruction is associated with a boolean predicate and, when the predicate is false, the instruction produces no architecturally visible effects. The machine uses a downward-growing stack with stack pointer $SP$ and word size $w$. The semantics of push and pop are defined as follows for a value $v$: push performs $SP := SP - w$ followed by $\\mathrm{Mem}[SP] := v$, and pop performs $v := \\mathrm{Mem}[SP]$ followed by $SP := SP + w$.\n\nA control-dependent region is if-converted using predication. A boolean condition $C$ is computed before entering the region. Let $p$ denote the value of $C$ captured at region entry and stored in a predicate register. Inside the region, a sequence saves a register with a predicated push on entry to the $C$-true path and restores it with a predicated pop at reconvergence. The $C$-false path executes alternative computations. No instruction in the region other than the push and pop writes to $SP$. However, the computations in either path may modify registers and memory that were used to compute $C$, so recomputing $C$ later in the region could yield a different boolean than $p$.\n\nThe correctness requirement at the reconvergence point is that the net change in the stack pointer due to the predicated push and pop must be exactly zero relative to its value at region entry, for all initial machine states and both outcomes of $C$.\n\nWhich option ensures stack-pointer correctness under these conditions?\n\nA. Predicate the push by $p$, the true-path computations by $p$, the false-path computations by $\\neg p$, and recompute $C$ at the pop point to define the pop predicate $q := C$; use $q$ for the pop.\n\nB. Predicate the push by $p$, the true-path computations by $p$, the false-path computations by $\\neg p$, and use the same latched predicate $p$ for the pop (i.e., $q := p$), without recomputing $C$.\n\nC. Predicate the push by $p$ and the pop by $\\neg p$ so that each path performs exactly one of the two operations.\n\nD. Predicate the push by $p$ but make the pop unconditional (no predicate) to force reconvergence with a balanced number of operations across the two paths.", "solution": "Let $SP_{entry}$ be the value of the stack pointer upon entering the predicated region and $SP_{exit}$ be its value at the reconvergence point. The correctness requirement is that the net change, $\\Delta SP = SP_{exit} - SP_{entry}$, must be zero for both outcomes of the predicate $p$.\n\nThe only instructions that modify the stack pointer are the predicated `push` (which changes $SP$ by $-w$) and the predicated `pop` (which changes $SP$ by $+w$). If an instruction's predicate is false, it is nullified and does not change $SP$.\n\nFor the stack to be balanced, the `pop` operation must execute if and only if the `push` operation executed. This means their predicates must be logically equivalent. Since the `push` is part of the true path, its predicate is $p$. Therefore, the `pop` must also be controlled by the same latched predicate $p$.\n\nWe can now analyze the two possible cases for the initial condition $C$, captured in $p$.\n\n**Case 1: $p$ is true.**\n- The `push` is predicated on $p$ (true), so it executes. The change in $SP$ is $-w$.\n- To ensure $\\Delta SP = 0$, the `pop` must also execute, changing $SP$ by $+w$.\n- This requires the `pop` predicate to be true.\n\n**Case 2: $p$ is false.**\n- The `push` is predicated on $p$ (false), so it does not execute. The change in $SP$ is $0$.\n- To ensure $\\Delta SP = 0$, the `pop` must also not execute.\n- This requires the `pop` predicate to be false.\n\nIn both cases, the predicate for the `pop` must be equal to $p$.\n\nLet's evaluate the options based on this logic:\n\n**A.** Predicate the pop with a recomputed condition $q$. The problem states that side effects can cause $q \\neq p$. If $p$ is true but $q$ becomes false, a `push` occurs without a `pop`, resulting in $\\Delta SP = -w$. This is incorrect.\n\n**B.** Predicate both the push and the pop with the same latched predicate $p$. If $p$ is true, both execute, and $\\Delta SP = -w + w = 0$. If $p$ is false, neither executes, and $\\Delta SP = 0 + 0 = 0$. This is correct for all cases.\n\n**C.** Predicate the push by $p$ and the pop by $\\neg p$. If $p$ is true, only the `push` executes, resulting in $\\Delta SP = -w$. If $p$ is false, only the `pop` executes, resulting in $\\Delta SP = +w$. The stack is never balanced. This is incorrect.\n\n**D.** Predicate the push by $p$ and make the pop unconditional (predicate is always true). If $p$ is false, the `push` does not execute but the `pop` does, resulting in $\\Delta SP = +w$. The stack is unbalanced. This is incorrect.", "answer": "$$\\boxed{B}$$", "id": "3667887"}, {"introduction": "The performance benefits of predication are highly context-dependent, and this final practice explores a critical counterexample within modern GPU architectures. Moving from a simple CPU pipeline to a Single Instruction, Multiple Threads (SIMT) model, you will investigate how predication interacts with memory systems and thread activity. This scenario demonstrates that while predication avoids the penalty of control flow divergence, it can lead to significant underutilization of memory bandwidth by forcing inactive threads to participate in memory operations. This problem challenges earlier intuitions and highlights how architectural specifics, like memory coalescing and early thread exit, create a more nuanced and complex performance landscape [@problem_id:3667910].", "problem": "A graphics processing unit (GPU) with Single Instruction, Multiple Threads (SIMT) execution executes warps of size $W = 32$. Each global memory load by a thread fetches a word of size $s = 4$ bytes. The memory subsystem coalesces loads such that if $n$ threads in a warp access $n$ contiguous words lying within a single aligned cache line of size $T = 128$ bytes, the hardware issues exactly one transaction of size $T$ bytes. If multiple disjoint $128$-byte regions are touched by the active threads in an instruction, the hardware issues one transaction per region. Define the effective bandwidth utilization of a memory instruction as the ratio of algorithmically useful bytes to total bytes transferred by the corresponding memory transactions.\n\nConsider a data-dependent loop where, in the first iteration $j = 1$ of each warp, exactly $W/2$ threads need data from array $X$ and exactly $W/2$ threads need data from array $Y$. The $X$ accesses by those $W/2$ threads are contiguous and lie within one aligned $128$-byte region; the $Y$ accesses by the other $W/2$ threads are also contiguous and lie within one aligned $128$-byte region disjoint from the $X$ region. All threads that choose $X$ terminate and exit after $j = 1$. The threads that choose $Y$ continue for $K - 1$ further iterations ($j = 2, 3, \\dots, K$), and in each of these iterations their $Y$ accesses remain contiguous and lie within one aligned $128$-byte region.\n\nTwo kernel implementations are considered:\n\n- Implementation $\\mathcal{P}$ (predication via if-conversion with speculative dual loads): The compiler if-converts the conditional, issuing two unconditional global loads per iteration per thread, one from $X$ and one from $Y$, and then selects the needed value based on the thread’s predicate using a predicated move. Both loads hit global memory even when a thread’s predicate indicates the value will not be used. This approach avoids divergent control flow but may perform redundant memory work.\n\n- Implementation $\\mathcal{B}$ (divergent branching with early exit): The kernel uses a branch on the predicate. At $j = 1$, the warp executes the $X$ path for the $W/2$ threads needing $X$ and the $Y$ path for the $W/2$ threads needing $Y$, serially. Threads that take the $X$ path exit and do not participate in subsequent iterations. For $j = 2, \\dots, K$, the warp executes only the $Y$ path for the remaining $W/2$ threads.\n\nAssume no other bottlenecks (e.g., latency hiding, occupancy limits) and that addresses are perfectly coalescible as described. Which statement correctly characterizes a counterexample in which predication underutilizes memory bandwidth compared to divergent branching with early exit, by quantifying the number of $128$-byte transactions and the effective bandwidth utilization over $K$ iterations?\n\nA. Over $K$ iterations, implementation $\\mathcal{P}$ issues $2K$ transactions (two fully coalesced streams per iteration, each transferring $T$ bytes), while implementation $\\mathcal{B}$ issues $K + 1$ transactions (two transactions at $j = 1$ for the two half-warps, then one transaction per subsequent iteration for the continuing half-warp). Since only the $Y$ data are algorithmically needed after $j = 1$, $\\mathcal{P}$ transfers roughly twice as many bytes as $\\mathcal{B}$ for large $K$, yielding lower effective bandwidth utilization.\n\nB. Both implementations issue the same number of transactions over $K$ iterations because SIMT reconvergence makes predication and branching equivalent; therefore, there is no bandwidth difference.\n\nC. Implementation $\\mathcal{P}$ issues fewer transactions than $\\mathcal{B}$ because predication preserves all $W$ active threads and maximizes coalescing, while branching halves the activity and destroys coalescing, making predication strictly superior in bandwidth utilization.\n\nD. Implementation $\\mathcal{B}$ issues strictly more transactions than $\\mathcal{P}$ because early exit reduces warp occupancy and prevents coalescing; therefore, predication always achieves higher effective bandwidth utilization than branching in this scenario.", "solution": "Let's analyze the memory traffic for both implementations over $K$ iterations, using the given parameters: warp size $W=32$, word size $s=4$ bytes, and transaction size $T=128$ bytes.\n\n**Implementation $\\mathcal{P}$ (Predication)**\nIn this model, all $W=32$ threads remain active for all $K$ iterations, and each thread executes two speculative loads in each iteration.\n- **Memory Access per iteration**:\n    1.  Load from $X$: All 32 threads participate. Their accesses total $32 \\times 4 = 128$ bytes, which are contiguous and aligned. This results in one coalesced memory transaction of 128 bytes.\n    2.  Load from $Y$: Similarly, all 32 threads participate, resulting in a second 128-byte transaction.\n- **Total for $\\mathcal{P}$ over $K$ iterations**:\n    - Each iteration issues 2 transactions.\n    - Total transactions = $2K$.\n    - Total bytes transferred = $2K \\times T = 256K$ bytes.\n\n**Implementation $\\mathcal{B}$ (Divergent Branching with Early Exit)**\nIn this model, the warp's behavior changes after the first iteration.\n- **Iteration $j=1$**:\n    - The warp diverges. The hardware serially executes two paths.\n    - Path 1 ($X$ access): $16$ threads access $16 \\times 4 = 64$ contiguous bytes. This access pattern falls within a single 128-byte region, triggering one transaction of 128 bytes. These threads then exit.\n    - Path 2 ($Y$ access): The other $16$ threads access 64 contiguous bytes from $Y$, also triggering one 128-byte transaction.\n    - Total transactions for $j=1$ is $1 + 1 = 2$.\n- **Iterations $j=2, \\dots, K$**:\n    - For the remaining $K-1$ iterations, only $16$ threads are active, all on the same execution path (accessing $Y$). There is no divergence.\n    - In each of these iterations, the 16 active threads access 64 contiguous bytes from $Y$, which triggers one 128-byte transaction.\n    - Total transactions for these $K-1$ iterations is $K-1$.\n- **Total for $\\mathcal{B}$ over $K$ iterations**:\n    - Total transactions = $2$ (from $j=1$) + $(K-1)$ (from $j>1$) = $K+1$.\n    - Total bytes transferred = $(K+1) \\times T = 128(K+1)$ bytes.\n\n**Comparison**\n- **Transaction Count**: Implementation $\\mathcal{P}$ issues $2K$ transactions, while implementation $\\mathcal{B}$ issues $K+1$ transactions. For any $K>1$, $2K > K+1$, so the predicated version issues more transactions.\n- **Bandwidth Utilization**: The predicated implementation ($\\mathcal{P}$) continues to issue two full memory transactions per iteration, even after half the threads have algorithmically finished their work. The second transaction (from array $X$) after $j=1$ is entirely wasteful. The branching implementation ($\\mathcal{B}$) benefits from early thread exit, reducing the number of memory transactions in subsequent iterations to only what is necessary for the remaining active threads. For large $K$, $\\mathcal{P}$ transfers $\\frac{256K}{128(K+1)} \\approx 2$ times as much data as $\\mathcal{B}$. This demonstrates a clear case of lower effective bandwidth utilization for predication.\n\nThis analysis matches the description in option A. The other options make incorrect claims about transaction counts or the effects of coalescing and early exit.", "answer": "$$\\boxed{A}$$", "id": "3667910"}]}