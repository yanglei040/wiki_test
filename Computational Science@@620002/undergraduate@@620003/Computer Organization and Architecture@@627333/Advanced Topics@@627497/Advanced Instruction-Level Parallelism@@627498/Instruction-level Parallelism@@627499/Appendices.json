{"hands_on_practices": [{"introduction": "To begin exploring instruction-level parallelism (ILP), it is helpful to start with an idealized model. This first exercise [@problem_id:3651239] simulates how a superscalar processor executes several independent instruction chains, limited only by its issue width and true data dependencies. By analyzing this scenario, you will develop a foundational understanding of how ILP is measured through metrics like Instructions Per Cycle ($IPC$) and how the longest dependency chain, or critical path, dictates overall performance.", "problem": "Consider a basic block scheduled on a superscalar processor that exploits Instruction-Level Parallelism (ILP). The processor can issue up to $w=3$ instructions per cycle, has perfect branch prediction, performs register renaming to eliminate name hazards, and has no memory stalls or cache misses. Assume all functional unit latencies are one cycle, and an instruction that depends on a predecessor can only be issued in a cycle strictly after the predecessor is issued.\n\nThe basic block consists of three disjoint chains of true data dependencies:\n- Chain $\\mathcal{A}$: instructions $A_1 \\rightarrow A_2 \\rightarrow A_3 \\rightarrow A_4 \\rightarrow A_5$,\n- Chain $\\mathcal{B}$: instructions $B_1 \\rightarrow B_2 \\rightarrow B_3 \\rightarrow B_4$,\n- Chain $\\mathcal{C}$: instructions $C_1 \\rightarrow C_2 \\rightarrow C_3 \\rightarrow C_4 \\rightarrow C_5 \\rightarrow C_6$,\n\nand one final instruction $X$ that depends on the last instruction of each chain, that is, $X$ has true data dependencies on $A_5$, $B_4$, and $C_6$. There are no other inter-chain dependencies.\n\nDefine a “parallel stage” as any cycle during which the processor issues exactly $w$ instructions. Under an ideal out-of-order scheduler that respects the stated constraints, determine:\n1. The number of parallel stages during the execution of the basic block.\n2. The expected Instruction Per Cycle (IPC), defined as the ratio of the total number of dynamic instructions in the basic block to the total number of cycles required to issue the entire block.\n\nExpress the IPC as an exact value. No rounding is required. The final answers should be given as pure numbers without units.", "solution": "First, we calculate the total number of dynamic instructions in the basic block. This is the sum of the lengths of the three chains plus the final instruction $X$:\n$$ N_{\\text{instr}} = 5 (\\text{Chain } \\mathcal{A}) + 4 (\\text{Chain } \\mathcal{B}) + 6 (\\text{Chain } \\mathcal{C}) + 1 (X) = 16 $$\n\nNext, we simulate the execution schedule on the out-of-order processor with an issue width of $w=3$. An instruction is ready to be issued in a cycle if all its dependencies have been issued in a prior cycle.\n\n- **Cycle 1**: Instructions $A_1$, $B_1$, and $C_1$ are ready (no dependencies). All 3 are issued.\n  - Instructions Issued: $\\{A_1, B_1, C_1\\}$. Count: 3. This is a **parallel stage**.\n\n- **Cycle 2**: Instructions $A_2$, $B_2$, and $C_2$ are ready. All 3 are issued.\n  - Instructions Issued: $\\{A_2, B_2, C_2\\}$. Count: 3. This is a **parallel stage**.\n\n- **Cycle 3**: Instructions $A_3$, $B_3$, and $C_3$ are ready. All 3 are issued.\n  - Instructions Issued: $\\{A_3, B_3, C_3\\}$. Count: 3. This is a **parallel stage**.\n\n- **Cycle 4**: Instructions $A_4$, $B_4$, and $C_4$ are ready. All 3 are issued. Chain $\\mathcal{B}$ is now complete.\n  - Instructions Issued: $\\{A_4, B_4, C_4\\}$. Count: 3. This is a **parallel stage**.\n\n- **Cycle 5**: The ready instructions are $A_5$ and $C_5$. Both are issued. Chain $\\mathcal{A}$ is now complete.\n  - Instructions Issued: $\\{A_5, C_5\\}$. Count: 2.\n\n- **Cycle 6**: The only ready instruction is $C_6$. It is issued. Chain $\\mathcal{C}$ is now complete.\n  - Instructions Issued: $\\{C_6\\}$. Count: 1.\n\n- **Cycle 7**: Instruction $X$ is ready, as its dependencies ($A_5$, $B_4$, $C_6$) have all been issued (latest in cycle 6). It is issued.\n  - Instructions Issued: $\\{X\\}$. Count: 1.\n\nThe execution finishes at the end of cycle 7. Total cycles required, $N_{\\text{cycles}}$, is 7.\n\n1.  **Number of parallel stages:** A parallel stage occurs when $w=3$ instructions are issued. This happens in cycles 1, 2, 3, and 4. Thus, there are **4** parallel stages.\n\n2.  **Instruction Per Cycle (IPC):** The IPC is the ratio of total instructions to total cycles.\n$$ \\text{IPC} = \\frac{N_{\\text{instr}}}{N_{\\text{cycles}}} = \\frac{16}{7} $$", "answer": "$$\\boxed{\\begin{pmatrix} 4 & \\frac{16}{7} \\end{pmatrix}}$$", "id": "3651239"}, {"introduction": "While true data dependencies are fundamental limits on parallelism, modern processors also contend with \"false\" dependencies that arise from the reuse of register names. This practice [@problem_id:3651255] demonstrates the critical role of register renaming, a hardware technique that eliminates these artificial constraints. By comparing the performance of a code block with and without renaming, you will quantify the significant ILP gains unlocked by this essential optimization.", "problem": "You are analyzing instruction-level parallelism (ILP) in a basic block on two otherwise identical out-of-order (OoO) processors. Machine $\\mathcal{M}_0$ has no register renaming and therefore must respect all name dependences (write-after-write and write-after-read), while machine $\\mathcal{M}_1$ has perfect register renaming that removes all name dependences while preserving true data dependences (read-after-write). Assume an idealized pipeline with unbounded issue width, unbounded functional units, perfect branch prediction, and no cache or structural stalls. Each arithmetic instruction has latency $1$ cycle. Instruction-level parallelism (ILP) is defined as the average number of instructions completed per cycle under these assumptions for the basic block.\n\nConsider the following basic block of $8$ integer additions in the given static program order, where $R_x$ denotes architectural registers and each line is one instruction:\n- $I_1$: $R_1 \\leftarrow R_2 + R_3$\n- $I_2$: $R_4 \\leftarrow R_1 + R_5$\n- $I_3$: $R_4 \\leftarrow R_4 + R_7$\n- $I_4$: $R_8 \\leftarrow R_4 + R_9$\n- $I_5$: $R_1 \\leftarrow R_{10} + R_{11}$\n- $I_6$: $R_4 \\leftarrow R_1 + R_{13}$\n- $I_7$: $R_4 \\leftarrow R_4 + R_{15}$\n- $I_8$: $R_{16} \\leftarrow R_4 + R_{17}$\n\nTasks:\n- Using only the core definitions of dependence types in dynamic scheduling, identify which inter-instruction edges in this block are true data dependences (read-after-write, RAW) and which are false name dependences (write-after-read, WAR; write-after-write, WAW) on $\\mathcal{M}_0$.\n- Based on these dependences, determine the minimum completion time, measured in cycles, to execute the block on $\\mathcal{M}_0$ and on $\\mathcal{M}_1$. For $\\mathcal{M}_1$, assume perfect register renaming so that all WAR and WAW constraints are removed by assigning distinct physical registers to logically independent results while preserving all RAW dependences.\n- Let $ILP_0$ and $ILP_1$ denote the instruction-level parallelism on $\\mathcal{M}_0$ and $\\mathcal{M}_1$, respectively, computed as $ILP = \\dfrac{N}{T}$ where $N$ is the number of instructions in the block and $T$ is the minimum number of cycles to complete the block under the stated model.\n\nWhat is the numerical increase $\\Delta ILP = ILP_1 - ILP_0$? Express your answer as an exact reduced fraction. No rounding is required and no units are needed.", "solution": "The ILP is defined as $ILP = \\frac{N}{T}$, where $N$ is the number of instructions and $T$ is the minimum execution time in cycles. The block contains $N=8$ instructions, and each instruction has a latency of 1 cycle. The result of an instruction starting in cycle $c$ is available for a dependent instruction at the beginning of cycle $c+1$.\n\nFirst, we must identify all inter-instruction dependences for the processor without register renaming, $\\mathcal{M}_0$. These are categorized as true data dependences (Read-After-Write, RAW), anti-dependences (Write-After-Read, WAR), and output dependences (Write-After-Write, WAW).\n\nThe instruction sequence is:\n$I_1: R_1 \\leftarrow R_2 + R_3$\n$I_2: R_4 \\leftarrow R_1 + R_5$\n$I_3: R_4 \\leftarrow R_4 + R_7$\n$I_4: R_8 \\leftarrow R_4 + R_9$\n$I_5: R_1 \\leftarrow R_{10} + R_{11}$\n$I_6: R_4 \\leftarrow R_1 + R_{13}$\n$I_7: R_4 \\leftarrow R_4 + R_{15}$\n$I_8: R_{16} \\leftarrow R_4 + R_{17}$\n\n**1. Dependence Analysis for Machine $\\mathcal{M}_0$ (No Register Renaming)**\n\nWe identify all pairs of instructions $(I_i, I_j)$ with $j > i$ that have a dependence.\n\n- **True Data Dependences (RAW):** $I_i$ writes a register that $I_j$ reads.\n  - $(I_1, I_2)$ on $R_1$: $I_1$ writes $R_1$, which $I_2$ reads.\n  - $(I_2, I_3)$ on $R_4$: $I_2$ writes $R_4$, which $I_3$ reads.\n  - $(I_3, I_4)$ on $R_4$: $I_3$ writes $R_4$, which $I_4$ reads.\n  - $(I_5, I_6)$ on $R_1$: $I_5$ writes $R_1$, which $I_6$ reads.\n  - $(I_6, I_7)$ on $R_4$: $I_6$ writes $R_4$, which $I_7$ reads.\n  - $(I_7, I_8)$ on $R_4$: $I_7$ writes $R_4$, which $I_8$ reads.\n\n- **Anti-Dependences (WAR):** $I_i$ reads a register that $I_j$ writes.\n  - $(I_2, I_5)$ on $R_1$: $I_2$ reads $R_1$, and $I_5$ writes $R_1$.\n  - $(I_3, I_6)$ on $R_4$: $I_3$ reads $R_4$, and $I_6$ writes $R_4$.\n  - $(I_3, I_7)$ on $R_4$: $I_3$ reads $R_4$, and $I_7$ writes $R_4$.\n  - $(I_4, I_6)$ on $R_4$: $I_4$ reads $R_4$, and $I_6$ writes $R_4$.\n  - $(I_4, I_7)$ on $R_4$: $I_4$ reads $R_4$, and $I_7$ writes $R_4$.\n\n- **Output Dependences (WAW):** $I_i$ and $I_j$ both write to the same register. The processor must ensure the final value in the register corresponds to the last write in program order.\n  - $(I_1, I_5)$ on $R_1$.\n  - On $R_4$, instructions $I_2, I_3, I_6, I_7$ all write. This creates the following WAW dependences: $(I_2, I_3)$, $(I_2, I_6)$, $(I_2, I_7)$, $(I_3, I_6)$, $(I_3, I_7)$, and $(I_6, I_7)$. Note that some of these pairs also have RAW dependences (e.g., $(I_2,I_3)$), which are a stronger constraint.\n\n**2. Execution Time on Machine $\\mathcal{M}_0$**\n\nThe minimum execution time is determined by the critical path in the full dependence graph. Let $S(I_k)$ be the start cycle of instruction $I_k$ (starting from $S=0$) and $C(I_k) = S(I_k) + 1$ be its completion cycle.\n\n- $S(I_1) = 0$. $C(I_1) = 1$.\n- $S(I_2) \\geq C(I_1)$ (RAW). $\\implies S(I_2) = 1, C(I_2) = 2$.\n- $S(I_3) \\geq C(I_2)$ (RAW). $\\implies S(I_3) = 2, C(I_3) = 3$.\n- $S(I_4) \\geq C(I_3)$ (RAW). $\\implies S(I_4) = 3, C(I_4) = 4$.\n- $S(I_5) \\geq C(I_2)$ (WAR on $R_1$). Also $S(I_5) \\geq C(I_1)$ (WAW on $R_1$). Since $C(I_2) > C(I_1)$, the WAR dependence is the binding constraint. $\\implies S(I_5) = 2, C(I_5) = 3$.\n- $S(I_6)$ depends on multiple predecessors:\n    - $S(I_6) \\geq C(I_5)$ (RAW on $R_1$). So $S(I_6) \\geq 3$.\n    - $S(I_6) \\geq C(I_4)$ (WAR on $R_4$). So $S(I_6) \\geq 4$.\n    - $S(I_6) \\geq C(I_3)$ (WAW and WAR on $R_4$). So $S(I_6) \\geq 3$.\n    The most restrictive constraint is from $I_4$, so $S(I_6) \\geq \\max(3, 4, 3) = 4$. $\\implies S(I_6) = 4, C(I_6) = 5$.\n- $S(I_7) \\geq C(I_6)$ (RAW). $\\implies S(I_7) = 5, C(I_7) = 6$.\n- $S(I_8) \\geq C(I_7)$ (RAW). $\\implies S(I_8) = 6, C(I_8) = 7$.\n\nThe last instruction, $I_8$, completes at cycle $7$. The total time to execute the block on $\\mathcal{M}_0$ is $T_0 = 7$ cycles.\nThe ILP is $ILP_0 = \\frac{N}{T_0} = \\frac{8}{7}$.\n\n**3. Execution Time on Machine $\\mathcal{M}_1$ (Perfect Register Renaming)**\n\nOn machine $\\mathcal{M}_1$, perfect register renaming eliminates all WAR and WAW dependences. Only RAW dependences remain.\nThe RAW dependences partition the instructions into two independent chains:\n- Chain A: $I_1 \\rightarrow_{RAW} I_2 \\rightarrow_{RAW} I_3 \\rightarrow_{RAW} I_4$.\n- Chain B: $I_5 \\rightarrow_{RAW} I_6 \\rightarrow_{RAW} I_7 \\rightarrow_{RAW} I_8$.\n\nWith unbounded resources, these two chains execute in parallel. The total execution time is determined by the length of the longer chain.\n- The length of Chain A is the sum of latencies of its $4$ instructions, which is $1+1+1+1 = 4$ cycles.\n- The length of Chain B is the sum of latencies of its $4$ instructions, which is $1+1+1+1 = 4$ cycles.\n\nBoth chains are of equal length. The execution proceeds as follows:\n- Cycle 1: $I_1$ and $I_5$ execute.\n- Cycle 2: $I_2$ and $I_6$ execute.\n- Cycle 3: $I_3$ and $I_7$ execute.\n- Cycle 4: $I_4$ and $I_8$ execute.\n\nAll instructions complete at the end of cycle $4$. The total execution time is $T_1 = 4$ cycles.\nThe ILP is $ILP_1 = \\frac{N}{T_1} = \\frac{8}{4} = 2$.\n\n**4. Calculation of $\\Delta ILP$**\n\nThe numerical increase in ILP is $\\Delta ILP = ILP_1 - ILP_0$.\n$\\Delta ILP = 2 - \\frac{8}{7}$\nTo subtract, we find a common denominator:\n$\\Delta ILP = \\frac{14}{7} - \\frac{8}{7} = \\frac{14 - 8}{7} = \\frac{6}{7}$.\nThe result is an exact reduced fraction as required.", "answer": "$$\\boxed{\\frac{6}{7}}$$", "id": "3651255"}, {"introduction": "Our final exercise brings together the concepts of data dependency and resource limitations to model a more realistic processor. In the real world, performance is constrained not only by instruction dependencies but also by the finite number of available functional units, a condition known as a structural hazard. This problem [@problem_id:3651272] challenges you to create an optimal schedule that balances data dependencies, varying instruction latencies, and resource contention, providing a comprehensive look at the challenges of maximizing ILP in practice.", "problem": "A superscalar processor exploits Instruction-Level Parallelism (ILP) by issuing multiple independent operations in the same clock cycle, subject to functional unit availability and data-dependence constraints. Consider a single basic block to be scheduled on a processor with the following properties:\n- There are $2$ identical Integer Arithmetic Logic Units (ALU) and $1$ Floating-Point (FP) unit.\n- All units are fully pipelined such that each unit can accept at most one new operation per cycle.\n- Integer ALU operations have latency $1$ cycle.\n- Floating-point addition has latency $3$ cycles, and floating-point multiplication has latency $4$ cycles.\n- Operands produced by an operation become available exactly $d$ cycles after issue, where $d$ is the operation’s latency. A dependent consumer may be issued in the same cycle in which its last operand becomes available.\n- There are no memory operations, no control dependencies, and registers are fully renamed, eliminating false dependencies (no write-after-read or write-after-write hazards). Only true data dependencies apply.\n\nThe basic block contains the following operations. Each line lists the operation label, the operation type, and its explicit true dependencies:\n- $I_1$: integer add, depends on no prior operation.\n- $I_2$: integer add, depends on no prior operation.\n- $I_3$: integer add, depends on $I_1$.\n- $I_4$: integer add, depends on $I_1$ and $I_2$.\n- $I_5$: integer add, depends on $I_3$ and $I_4$.\n- $F_1$: floating-point multiply, depends on no prior operation.\n- $F_2$: floating-point add, depends on no prior operation.\n- $F_3$: floating-point add, depends on $F_1$ and $F_2$.\n- $F_4$: floating-point multiply, depends on $F_3$.\n- $F_5$: floating-point add, depends on $F_4$ and $F_1$.\n\nAssume out-of-order issue is allowed subject to the stated constraints, and integer operations must be assigned to Integer ALUs while floating-point operations must be assigned to the FP unit. Starting from cycle $0$, determine the minimal makespan $M$, defined as the earliest cycle by which all operations in the basic block have completed (all results ready), under an optimal schedule that maximizes ILP subject to the given constraints. Express your answer as a single integer number of cycles with no units.", "solution": "The problem requires determining the minimal makespan, $M$, for a set of operations. The makespan is the earliest cycle by which all operations have completed. The scheduling must respect both true data dependencies and resource constraints.\n\nFirst, we establish a lower bound on the makespan by finding the length of the critical path in the data dependency graph, assuming infinite resources. The ready time of an operation $O$, $Ready(O)$, is its issue time plus its latency $d(O)$. A dependent operation can be issued in the same cycle its operands become ready.\n\n**Critical Path Analysis (Infinite Resources):**\nLet $Ready(O)$ be the earliest possible ready time for an operation $O$.\nIf $O$ has no dependencies, $Ready(O) = Issue(O) + d(O) = 0 + d(O)$.\nOtherwise, $Ready(O) = \\left( \\max_{P \\in Dependencies(O)} \\{Ready(P)\\} \\right) + d(O)$.\n\n*Integer Operations:*\n- $Ready(I_1) = 0 + 1 = 1$.\n- $Ready(I_2) = 0 + 1 = 1$.\n- $Ready(I_3) = Ready(I_1) + 1 = 1 + 1 = 2$.\n- $Ready(I_4) = \\max(Ready(I_1), Ready(I_2)) + 1 = 1 + 1 = 2$.\n- $Ready(I_5) = \\max(Ready(I_3), Ready(I_4)) + 1 = 2 + 1 = 3$.\n\n*Floating-Point Operations:*\n- $Ready(F_1) = 0 + 4 = 4$.\n- $Ready(F_2) = 0 + 3 = 3$.\n- $Ready(F_3) = \\max(Ready(F_1), Ready(F_2)) + 3 = \\max(4, 3) + 3 = 7$.\n- $Ready(F_4) = Ready(F_3) + 4 = 7 + 4 = 11$.\n- $Ready(F_5) = \\max(Ready(F_4), Ready(F_1)) + 3 = \\max(11, 4) + 3 = 14$.\n\nThe longest dependency chain results in a ready time of 14 cycles. This is the critical path, so $M \\ge 14$.\n\nNow, we construct a schedule with resource constraints (2 ALUs, 1 FP unit) to see if this lower bound is achievable.\n\n- **Cycle 0:**\n  - Ready to issue: $\\{I_1, I_2, F_1, F_2\\}$.\n  - Schedule: Issue $I_1$ (ALU), $I_2$ (ALU), $F_1$ (FP). ($F_2$ must wait due to resource contention).\n  - $Ready(I_1) = 0+1=1$, $Ready(I_2) = 0+1=1$, $Ready(F_1) = 0+4=4$.\n\n- **Cycle 1:**\n  - Ready to issue: $\\{I_3, I_4, F_2\\}$. ($I_3, I_4$ ready because $I_1, I_2$ are ready).\n  - Schedule: Issue $I_3$ (ALU), $I_4$ (ALU), $F_2$ (FP).\n  - $Ready(I_3) = 1+1=2$, $Ready(I_4) = 1+1=2$, $Ready(F_2) = 1+3=4$.\n\n- **Cycle 2:**\n  - Ready to issue: $\\{I_5\\}$. ($I_5$ ready because $I_3, I_4$ are ready).\n  - Schedule: Issue $I_5$ (ALU).\n  - $Ready(I_5) = 2+1=3$.\n\n- **Cycle 3:** No operations ready.\n\n- **Cycle 4:**\n  - Ready to issue: $\\{F_3\\}$. ($F_3$ ready because $\\max(Ready(F_1), Ready(F_2)) = 4$).\n  - Schedule: Issue $F_3$ (FP).\n  - $Ready(F_3) = 4+3=7$.\n\n- **Cycles 5-6:** No operations ready.\n\n- **Cycle 7:**\n  - Ready to issue: $\\{F_4\\}$. ($F_4$ ready because $Ready(F_3)=7$).\n  - Schedule: Issue $F_4$ (FP).\n  - $Ready(F_4) = 7+4=11$.\n\n- **Cycles 8-10:** No operations ready.\n\n- **Cycle 11:**\n  - Ready to issue: $\\{F_5\\}$. ($F_5$ ready because $\\max(Ready(F_4), Ready(F_1))=11$).\n  - Schedule: Issue $F_5$ (FP).\n  - $Ready(F_5) = 11+3=14$.\n\nAll operations are scheduled. The final operation completes at cycle 14.\n$$ M = \\max_{O} \\{Ready(O)\\} = 14 $$\nSince the schedule's makespan of 14 cycles matches the lower bound from the critical path analysis, this schedule is optimal.", "answer": "$$\n\\boxed{14}\n$$", "id": "3651272"}]}