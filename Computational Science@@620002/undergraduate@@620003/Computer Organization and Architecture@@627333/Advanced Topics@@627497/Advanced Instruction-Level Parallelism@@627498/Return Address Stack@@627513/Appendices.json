{"hands_on_practices": [{"introduction": "The fundamental last-in, first-out (LIFO) behavior of a Return Address Stack is straightforward in a single-threaded processor. However, modern parallel architectures like Graphics Processing Units (GPUs) introduce new complexities. This exercise [@problem_id:3673883] explores how the Single Instruction, Multiple Threads (SIMT) execution model, particularly the phenomenon of thread divergence, challenges a simple shared RAS design and motivates the need for per-thread hardware support. You will perform a practical calculation to determine the on-chip storage cost, a critical first step in any hardware feasibility study.", "problem": "Consider a Graphics Processing Unit (GPU) that implements Single Instruction, Multiple Threads (SIMT) execution. In SIMT, a Streaming Multiprocessor (SM) schedules warps of $U$ threads, and at any given time up to $W$ warps can be resident. The control-flow mechanism supports function calls and returns in shader programs. A Return Address Stack (RAS) is a microarchitectural structure used to predict the target of a return instruction by providing the top-of-stack return address pushed on a call and popped on a return. In conventional Central Processing Unit (CPU) pipelines, a RAS improves return target prediction by leveraging the invariant that the return address equals the next program counter (PC) after the call, which is a deterministic function of the calling instruction’s location. In SIMT, threads within the same warp may diverge due to data-dependent control flow, so calls and returns can occur at different dynamic times per thread.\n\nSuppose the GPU is evaluating whether to implement a per-thread RAS for calls in shaders. The SM parameters are $U=32$ threads per warp and $W=64$ resident warps, so the maximum simultaneously resident threads per SM is $T=W \\cdot U$. Each thread’s RAS must hold up to $K=8$ nested call frames. Each RAS entry must store a return program counter of width $p=40$ bits and also $c=2$ bits for minimal context metadata, so the per-entry width is $w=p+c$ bits. Assume the SM has a reserved on-chip budget of $B=1.0 \\times 10^{6}$ bits for control-flow prediction structures.\n\nStarting from the SIMT execution model and the function of a return address stack, explain why warp-level sharing of a single RAS can be insufficient under divergence and why per-thread RAS eliminates that hazard. Then, compute the total storage required to provision a per-thread RAS for all resident threads on the SM. Express your final storage cost in bits as a single number, obtained by combining $T$, $K$, and $w$ as implied by the requirement that each thread stores $K$ entries of $w$ bits. Do not include any units inside your final boxed answer, but interpret it as bits. No rounding is required.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of computer architecture, specifically the Single Instruction, Multiple Threads (SIMT) model of GPUs. The problem is well-posed, providing all necessary parameters for a unique and meaningful calculation, and its premises are consistent and objective.\n\nThe central issue addressed is the management of function call return addresses in a SIMT execution environment characterized by thread divergence. A Return Address Stack (RAS) is a hardware stack that stores the return addresses of subroutine calls to predict the target of subsequent return instructions. In a simple, single-threaded processor, the Last-In, First-Out (LIFO) nature of function calls makes the RAS highly effective. A `CALL` instruction pushes the address of the next instruction onto the RAS, and a `RETURN` instruction pops this address and uses it as the branch target.\n\nIn a SIMT architecture, a group of threads, called a warp, nominally executes the same instruction in lockstep. Let the number of threads per warp be $U$. If all $U$ threads in a warp follow the exact same control-flow path, they would all execute a `CALL` instruction simultaneously and could, in principle, share a single warp-level RAS. They would all push the same return address and later all pop that same address upon executing a `RETURN`.\n\nHowever, the defining characteristic of SIMT that complicates this is thread divergence. Due to data-dependent conditional branches (e.g., `if-then-else` constructs), individual threads within a warp can take different execution paths. For example, consider a warp of $U$ threads where a subset of threads $S_1$ satisfies a condition and executes a `CALL` instruction within an `if` block, while the complementary subset $S_2$ does not.\n\nA single, shared warp-level RAS is insufficient in this divergent scenario for several reasons:\n$1$. **State Ambiguity:** A single RAS can only hold one stack of return addresses. It cannot simultaneously represent the state where threads in $S_1$ have made a function call and threads in $S_2$ have not. If the threads in $S_1$ push a return address, that address is now at the top of the shared stack.\n$2$. **Incorrect Prediction for Non-calling Threads:** When the threads in $S_1$ eventually execute a `RETURN` instruction, they require the return address they previously pushed. However, the `RETURN` instruction will be active for threads in $S_1$ but inactive (predicated off) for threads in $S_2$. If the hardware were to pop the shared RAS, it would provide a return address prediction that is meaningless and incorrect for the threads in $S_2$, which never made the corresponding call. This can lead to mispredictions or stalls if not handled properly.\n$3$. **Nested and Interleaved Divergence:** The problem is compounded with nested calls and further divergence. Some threads might make a call, then another nested call, while others make only the first call, and still others make no calls. A single stack cannot track these multiple, independent call histories.\n\nProvisioning a per-thread RAS resolves this hazard. By dedicating a private RAS to each thread, the control-flow state of each thread is managed independently.\n- When a thread executes a `CALL`, it pushes its unique return address onto its own private RAS.\n- When that same thread later executes a `RETURN`, it pops the address from its own RAS.\nThis operation is entirely unaffected by whether other threads in the same warp are making calls or not. The correctness of return address prediction is thus decoupled from the execution path of neighboring threads, making the mechanism robust to thread divergence.\n\nWe can now compute the total storage cost for implementing a per-thread RAS for all simultaneously resident threads on a single Streaming Multiprocessor (SM).\n\nThe given parameters are:\n- Number of threads per warp: $U = 32$\n- Number of resident warps per SM: $W = 64$\n- Maximum number of RAS entries per thread: $K = 8$\n- Width of the return program counter (PC): $p = 40$ bits\n- Width of the context metadata per entry: $c = 2$ bits\n\nFirst, we calculate the total number of resident threads per SM, denoted by $T$.\n$$T = W \\cdot U$$\nSubstituting the given values:\n$$T = 64 \\cdot 32 = 2048 \\text{ threads}$$\n\nNext, we determine the storage size of a single RAS entry, denoted by $w$. This is the sum of the PC width and the metadata width.\n$$w = p + c$$\nSubstituting the given values:\n$$w = 40 + 2 = 42 \\text{ bits}$$\n\nEach thread requires its own RAS capable of storing $K$ such entries. The total storage required for a single thread's RAS, $S_{\\text{thread}}$, is:\n$$S_{\\text{thread}} = K \\cdot w$$\n$$S_{\\text{thread}} = 8 \\cdot 42 = 336 \\text{ bits}$$\n\nFinally, the total storage required for the per-thread RAS implementation for all $T$ resident threads on the SM, $S_{\\text{total}}$, is the product of the number of threads and the storage per thread.\n$$S_{\\text{total}} = T \\cdot S_{\\text{thread}}$$\n$$S_{\\text{total}} = T \\cdot K \\cdot w$$\nSubstituting the calculated and given values:\n$$S_{\\text{total}} = 2048 \\cdot 8 \\cdot 42$$\n$$S_{\\text{total}} = 16384 \\cdot 42$$\n$$S_{\\text{total}} = 688128 \\text{ bits}$$\n\nThus, the total storage cost to provision a per-thread RAS for all resident threads on the SM is $688,128$ bits. This cost is within the specified on-chip budget of $B = 1.0 \\times 10^6$ bits.", "answer": "$$\n\\boxed{688128}\n$$", "id": "3673883"}, {"introduction": "In today's high-performance processors, instructions are executed speculatively and out of order to maximize performance, but this can lead to subtle correctness issues. This practice problem [@problem_id:3673889] delves into a realistic scenario where a wrong-path speculation corrupts the RAS, causing a misprediction much later on the correct execution path. By meticulously tracing the state of the RAS, you will uncover the source of the error and then evaluate an advanced architectural enhancement—tagging—that can make the RAS more resilient to such speculative side effects.", "problem": "You are analyzing a superscalar, out-of-order processor with a Return Address Stack (RAS) used to predict targets of return instructions. The Return Address Stack (RAS) is a last-in-first-out structure that, on decoding a call, pushes the return address (the Program Counter (PC) of the instruction immediately after the call) and, on encountering a return, predicts the return target by reading the top entry and then pops the top. The following facts hold in this machine:\n- On call decode, the machine speculatively pushes the predicted return address onto the RAS. The machine does not checkpoint the RAS, so pushes caused by wrong-path decode are not automatically undone.\n- On retirement of a return, if the predicted return target differs from the true architectural return address, a misprediction flush occurs and the RAS nevertheless pops its top entry (i.e., the machine cannot “un-pop”).\n- There are no tail calls and no exceptions; only branch mispredictions can cause wrong-path decode. The RAS depth is sufficiently large (at least $8$) that overflow does not occur in this scenario.\n\nConsider the following code structure with both recursion and mutual recursion. Let $M$ denote the main function, and $F$ and $G$ be two functions that mutually recurse as follows along the taken path:\n- $M$ calls $F$ at a call-site we label $C_{MF}$, with corresponding correct return address $R_{MF}$.\n- Early in $F$, there is a conditional branch that is mispredicted once, leading the frontend to wrong-path decode a call to an unrelated function $H$ at call-site $C_{FH}$, with predicted return address $R_{FH}$. The call to $H$ does not actually execute (the wrong-path is squashed), but due to the lack of RAS checkpointing, the push of $R_{FH}$ onto the RAS persists.\n- Still in $F$ along the correct path, $F$ calls $G$ at call-site $C_{FG1}$ with return address $R_{FG1}$.\n- In $G$, there is a call to $F$ at call-site $C_{GF1}$ with return address $R_{GF1}$.\n- The innermost $F$ reaches a base case and executes a return. Then $G$ returns. Then the outer $F$ returns back to $M$.\n\nAssume the only wrong-path event in this episode is the single spurious push of $R_{FH}$ as described above. Let the dynamic push sequence on the RAS be exactly the pushes corresponding to $R_{MF}$, then the spurious $R_{FH}$, then $R_{FG1}$, then $R_{GF1}$, in that order from bottom to top.\n\nNow consider augmenting the RAS with a small tag per entry equal to $h(ID)$, where $h(\\cdot)$ is a fixed hash function and $ID$ is some identifier associated with the call. On a return, the machine can compute a tag $t$ based only on information available at the return (before popping) and, if the top-of-stack tag does not match $t$, it is allowed to scan downward to the first entry whose tag matches $t$ and discard any entries above it before producing a prediction. Assume $h(\\cdot)$ has negligible collision probability for the identifiers under consideration.\n\nWhich option correctly describes both:\n(i) the number and position of return misprediction(s) in the described episode with the untagged RAS, and\n(ii) a feasible choice of $ID$ that allows the tagged RAS, using the compare-and-scan policy above, to avoid those mispredictions in the presence of recursion and mutual recursion?\n\nA. Exactly $1$ return is mispredicted, namely the outer return from $F$ back to $M$; choosing $ID$ as the callee’s function identity (e.g., the entry PC of the current function, so tags are $h(F)$, $h(G)$, etc.) is feasible because the callee identity is known at the return, and scanning to the nearest matching $h(F)$ skips the spurious $R_{FH}$ and restores alignment even with mutual recursion.\n\nB. No mispredictions occur because the last-in-first-out nature of calls and returns ensures that mutual recursion always keeps the RAS aligned, even with a spurious push.\n\nC. Exactly $1$ return is mispredicted, but only choosing $ID$ as the caller’s call-site identity (i.e., tags $h(C_{MF})$, $h(C_{FG1})$, etc.) can avoid it; callee-identity tags cannot disambiguate in mutual recursion.\n\nD. Exactly $2$ returns are mispredicted (the return from $F$ to $G$ and the return from $F$ to $M$); avoiding both requires choosing $ID$ as the exact return target address (tags $h(R)$), because only the return target is unique enough to disambiguate call-site contexts at return time.", "solution": "The problem statement describes a scenario involving a Return Address Stack (RAS) in an out-of-order processor and is internally consistent, scientifically grounded in the principles of computer architecture, and well-posed. The scenario is a standard-form problem for analyzing the behavior of speculative microarchitectural structures. Thus, the problem is valid and a solution can be derived.\n\nThe solution is developed in two parts: first, analyzing the behavior of the baseline, untagged RAS to determine the number of mispredictions; second, evaluating the proposed tagged RAS mechanism with different identifiers ($ID$) to assess its feasibility and effectiveness.\n\n### Part I: Analysis of the Untagged RAS\n\nThe RAS is a last-in-first-out (LIFO) stack. We will trace its state through the sequence of events described. Let the stack be represented by a list, with the bottom of the stack on the left and the top on the right.\n\n1.  **Initial State:** The RAS is empty: [].\n2.  **$M$ calls $F$:** a `call` to function $F$ from $M$ at call-site $C_{MF}$ pushes the return address $R_{MF}$.\n    RAS state: [$R_{MF}$]\n3.  **Spurious Push:** Due to a branch misprediction in $F$, a `call` to function $H$ at call-site $C_{FH}$ is decoded on the wrong path. The corresponding return address $R_{FH}$ is speculatively pushed. This push is not undone.\n    RAS state: [$R_{MF}$, $R_{FH}$]\n4.  **$F$ calls $G$:** On the correct path, the `call` to $G$ at $C_{FG1}$ pushes the return address $R_{FG1}$.\n    RAS state: [$R_{MF}$, $R_{FH}$, $R_{FG1}$]\n5.  **$G$ calls $F$:** The `call` to $F$ at $C_{GF1}$ pushes the return address $R_{GF1}$.\n    RAS state: [$R_{MF}$, $R_{FH}$, $R_{FG1}$, $R_{GF1}$]\n\nNow, we analyze the sequence of `return` instructions.\n\n1.  **Innermost $F$ returns to $G$:**\n    *   The correct architectural return address is $R_{GF1}$.\n    *   The RAS predicts the target by reading its top entry, which is $R_{GF1}$.\n    *   The prediction $R_{GF1}$ is **Correct**.\n    *   The RAS pops $R_{GF1}$.\n    *   RAS state: [$R_{MF}$, $R_{FH}$, $R_{FG1}$]\n\n2.  **$G$ returns to outer $F$:**\n    *   The correct architectural return address is $R_{FG1}$.\n    *   The RAS predicts by reading its new top entry, $R_{FG1}$.\n    *   The prediction $R_{FG1}$ is **Correct**.\n    *   The RAS pops $R_{FG1}$.\n    *   RAS state: [$R_{MF}$, $R_{FH}$]\n\n3.  **Outer $F$ returns to $M$:**\n    *   The correct architectural return address is $R_{MF}$.\n    *   The RAS predicts by reading its top entry, which is the spurious address $R_{FH}$.\n    *   Since $R_{FH}$ corresponds to a call to an unrelated function $H$, it is not equal to $R_{MF}$. The prediction $R_{FH}$ is **Incorrect**.\n    *   A misprediction flush occurs. As per the problem statement, the RAS pops its top entry $R_{FH}$ regardless of the misprediction.\n    *   RAS state: [$R_{MF}$]\n\nIn this entire episode, there is exactly **$1$** return misprediction. This misprediction occurs on the return from the outer $F$ function back to $M$. This immediately shows that options B (stating $0$ mispredictions) and D (stating $2$ mispredictions) are incorrect.\n\n### Part II: Analysis of the Tagged RAS and Feasibility of ID Choices\n\nThe goal of the tagged RAS is to detect and repair RAS corruption, such as that caused by the spurious push of $R_{FH}$. On a `return`, a tag $t$ is computed, and if the top-of-stack tag does not match, the RAS is scanned downwards to find a matching entry, discarding the non-matching entries above it. The key constraint is that $t$ must be computed `based only on information available at the return`.\n\nLet's analyze the proposed choices for the identifier $ID$, whose hash $h(ID)$ forms the tag.\n\n**Option A: $ID$ is the callee's function identity**\nA `return` instruction is part of a function's body. The identity of the function containing the `return` is the *callee* of the original `call` instruction. This identity (e.g., the function's entry Program Counter (PC)) is readily available to the processor when it executes the `return` instruction. Thus, this $ID$ choice is **feasible**.\n\nLet's trace the final, problematic return from outer $F$ to $M$ with this scheme:\n*   The RAS state just before this return is [ <$R_{MF}$, $h(F)$>, <$R_{FH}$, $h(H)$> ]. (The tag for the $M \\to F$ call is $h(F)$, and the tag for the spurious $F \\to H$ call is $h(H)$).\n*   The `return` is executed within function $F$. The processor computes the expected tag $t = h(F)$.\n*   It compares $t$ with the top-of-stack tag, which is $h(H)$. They do not match.\n*   The RAS scans downwards. The entry <$R_{FH}$, $h(H)$> is discarded. The next entry is <$R_{MF}$, $h(F)$>.\n*   The tag $h(F)$ matches the expected tag $t$.\n*   The RAS uses $R_{MF}$ as the predicted target. This is the correct target.\n*   The prediction is **Correct**, and the misprediction is avoided.\nThis scheme also works for mutual recursion: a `return` from $G$ would look for a tag $h(G)$, while a `return` from $F$ would look for a tag $h(F)$, correctly disambiguating the return contexts.\n\n**Option C: $ID$ is the caller's call-site identity**\nTo form the expected tag $t$ at a `return` instruction, the processor would need to know the address of the specific `call` instruction that invoked the current function instance. A function can be called from many different call sites. The code of the function itself (including the `return` instruction) is generic and does not have this information. It is therefore **not feasible** to compute this tag `based only on information available at the return`.\n\n**Option D: $ID$ is the exact return target address**\nTo form the expected tag $t$, the processor would need to know the correct return address $R$. However, the entire purpose of the RAS is to *predict* $R$. If the processor already knew the correct return address before the prediction, the RAS would be redundant. This is circular reasoning, and this $ID$ choice is **not feasible**.\n\n### Evaluation of Options\n\n*   **A. Exactly $1$ return is mispredicted... choosing $ID$ as the callee’s function identity... is feasible...**\n    *   The analysis of the untagged RAS confirms that exactly $1$ return is mispredicted.\n    *   The analysis of the tagged RAS confirms that using the callee's identity is a feasible and effective mechanism to prevent this misprediction, even with mutual recursion.\n    *   This option is **Correct**.\n\n*   **B. No mispredictions occur...**\n    *   This is factually incorrect based on the analysis of the untagged RAS.\n    *   This option is **Incorrect**.\n\n*   **C. Exactly $1$ return is mispredicted, but only choosing $ID$ as the caller’s call-site identity... can avoid it...**\n    *   While the number of mispredictions is correct, the proposed $ID$ choice is not feasible. The claim that *only* this $ID$ works is also false, as the callee-identity scheme works. Furthermore, the claim that callee-identity tags cannot disambiguate in mutual recursion is incorrect.\n    *   This option is **Incorrect**.\n\n*   **D. Exactly $2$ returns are mispredicted... avoiding both requires choosing $ID$ as the exact return target address...**\n    *   The number of mispredictions is incorrect.\n    *   The proposed $ID$ choice is not feasible due to circular logic.\n    *   This option is **Incorrect**.\n\nBased on a thorough analysis of both the untagged and tagged RAS behaviors, only option A is fully consistent with the principles of processor design and the details of the problem statement.", "answer": "$$\\boxed{A}$$", "id": "3673889"}, {"introduction": "Beyond understanding its functional behavior, designing a processor involves making quantitative trade-offs to balance performance, power, and area. This final exercise [@problem_id:3673912] abstracts this complex design challenge into a concrete optimization problem. Using mathematical models that capture the diminishing performance returns of a larger RAS against its increasing power and area costs, you will determine the optimal size that maximizes overall processor throughput, providing a glimpse into the analytical methods used by computer architects.", "problem": "A scalar, out-of-order processor employs a Return Address Stack (RAS) to predict function return targets. A Return Address Stack (RAS) with $K$ entries reduces return-target mispredictions, improving the effective Instructions Per Cycle (IPC). The design must respect an on-die area budget, and increasing $K$ also raises dynamic power, which reduces the achievable clock frequency under a fixed cooling solution.\n\nUse the following modeling assumptions grounded in standard computer architecture relationships:\n\n- Throughput in instructions per second is modeled as $S(K) = \\text{IPC}(K) \\cdot f(K)$, where $\\text{IPC}(K)$ is the Instructions Per Cycle (IPC) and $f(K)$ is the clock frequency.\n- Diminishing returns from larger $K$ are captured by a saturating IPC model\n  $$\\text{IPC}(K) = \\text{IPC}_{0} + \\Delta \\text{IPC}\\left(1 - \\exp(-\\alpha K)\\right),$$\n  where $\\text{IPC}_{0}$, $\\Delta \\text{IPC}$, and $\\alpha$ are positive constants.\n- A linearized thermal-power constraint relates total power to clock frequency as\n  $$f(K) = f_{0} - \\kappa \\left(P_{0} + P_{\\mathrm{e}} K\\right),$$\n  where $f_{0}$ and $\\kappa$ are positive constants, $P_{0}$ is baseline power at $K=0$, and $P_{\\mathrm{e}}$ is the marginal power per additional RAS entry. Assume the linearization is valid in the operating region and $f(K) > 0$ at the optimizer.\n- The silicon area per RAS entry is $A$ and the available area budget for the RAS is $A_{\\max}$, yielding the hard constraint $A K \\le A_{\\max}$.\n\nGiven the numeric parameters\n- $\\text{IPC}_{0} = 1.2$, $\\Delta \\text{IPC} = 0.4$, $\\alpha = 0.08$,\n- $f_{0} = 4.0$, $\\kappa = 0.05$, $P_{0} = 50$, $P_{\\mathrm{e}} = 0.02$,\n- $A = 0.02$ and $A_{\\max} = 1.5$,\n\nformulate and solve the constrained optimization problem\n$$\\max_{K \\ge 0} \\; S(K) \\quad \\text{subject to} \\quad A K \\le A_{\\max}.$$\n\nAssume $K$ can be treated as a real variable for optimization. State the optimal $K^{\\star}$ that maximizes $S(K)$ under the area constraint. Round your final numeric answer for $K^{\\star}$ to four significant figures. Do not include units in your answer.", "solution": "The problem is a constrained optimization task, which is scientifically grounded, well-posed, and based on valid modeling assumptions in computer architecture. The provided data are complete and consistent. Therefore, the problem is valid and a solution can be determined.\n\nThe objective is to maximize the throughput $S(K)$ with respect to the number of Return Address Stack (RAS) entries, $K$. The throughput is given by the product of Instructions Per Cycle, $\\text{IPC}(K)$, and clock frequency, $f(K)$.\n$$S(K) = \\text{IPC}(K) \\cdot f(K)$$\nThe constituent models are:\n$$\\text{IPC}(K) = \\text{IPC}_{0} + \\Delta \\text{IPC}\\left(1 - \\exp(-\\alpha K)\\right)$$\n$$f(K) = f_{0} - \\kappa \\left(P_{0} + P_{\\mathrm{e}} K\\right)$$\nSubstituting the given numeric parameters:\n- $\\text{IPC}_{0} = 1.2$, $\\Delta \\text{IPC} = 0.4$, $\\alpha = 0.08$\n- $f_{0} = 4.0$, $\\kappa = 0.05$, $P_{0} = 50$, $P_{\\mathrm{e}} = 0.02$\n\nThe models become:\n$$\\text{IPC}(K) = 1.2 + 0.4(1 - \\exp(-0.08 K)) = 1.6 - 0.4 \\exp(-0.08 K)$$\n$$f(K) = 4.0 - 0.05(50 + 0.02 K) = 4.0 - 2.5 - 0.001 K = 1.5 - 0.001 K$$\n\nThe objective function $S(K)$ is therefore:\n$$S(K) = \\left(1.6 - 0.4 \\exp(-0.08 K)\\right) \\left(1.5 - 0.001 K\\right)$$\n\nThe optimization is subject to the constraints $K \\ge 0$ and an area budget $A K \\le A_{\\max}$. Given the parameters $A = 0.02$ and $A_{\\max} = 1.5$, the area constraint is:\n$$0.02 K \\le 1.5 \\implies K \\le \\frac{1.5}{0.02} = 75$$\nThus, the problem is to find the value of $K$ that maximizes $S(K)$ on the closed interval $K \\in [0, 75]$. The maximum of a continuous, differentiable function on a closed interval must occur at either a boundary point ($K=0$ or $K=75$) or at a critical point $K_c$ within the interval $(0, 75)$ where the derivative $S'(K_c) = 0$.\n\nTo find the critical points, we compute the derivative of $S(K)$ with respect to $K$ using the product rule, $(uv)' = u'v + uv'$. Let $u(K) = 1.6 - 0.4 \\exp(-0.08 K)$ and $v(K) = 1.5 - 0.001 K$.\nThe derivatives are:\n$$u'(K) = \\frac{d}{dK} \\left(1.6 - 0.4 \\exp(-0.08 K)\\right) = -0.4 \\cdot (-0.08) \\exp(-0.08 K) = 0.032 \\exp(-0.08 K)$$\n$$v'(K) = \\frac{d}{dK} \\left(1.5 - 0.001 K\\right) = -0.001$$\nThe derivative of $S(K)$ is:\n$$S'(K) = u'(K)v(K) + u(K)v'(K)$$\n$$S'(K) = \\left(0.032 \\exp(-0.08 K)\\right) \\left(1.5 - 0.001 K\\right) + \\left(1.6 - 0.4 \\exp(-0.08 K)\\right) \\left(-0.001\\right)$$\nExpanding and collecting terms:\n$$S'(K) = 0.048 \\exp(-0.08 K) - 0.000032 K \\exp(-0.08 K) - 0.0016 + 0.0004 \\exp(-0.08 K)$$\n$$S'(K) = \\left(0.0484 - 0.000032 K\\right) \\exp(-0.08 K) - 0.0016$$\nSetting the derivative to zero, $S'(K) = 0$, to find critical points:\n$$\\left(0.0484 - 0.000032 K\\right) \\exp(-0.08 K) = 0.0016$$\nThis is a transcendental equation for $K$, which must be solved numerically. Let $h(K) = S'(K)$. Analysis of $h(K)$ on the interval $[0, 75]$ shows that it is a monotonically decreasing function (since its derivative, $S''(K)$, is negative throughout the interval), crossing zero at a single point.\n- At $K=0$, $h(0) = (0.0484)\\exp(0) - 0.0016 = 0.0468 > 0$.\n- At $K=75$, $h(75) = (0.0484 - 0.000032 \\cdot 75)\\exp(-0.08 \\cdot 75) - 0.0016 = 0.046 \\exp(-6) - 0.0016 \\approx 0.046(0.002479) - 0.0016 \\approx -0.001486 < 0$.\nSince $h(K)$ is continuous and changes sign from positive to negative, a unique root $K_c$ exists in $(0, 75)$. This critical point corresponds to a local maximum because $S''(K_c) < 0$.\n\nNumerical methods (such as Newton-Raphson or bisection) can be employed to find the root. Iterative evaluation yields a solution for $K_c$. For instance, testing values:\n- For $K=42$, $S'(42) \\approx 0.00002$.\n- For $K=43$, $S'(43) \\approx -0.00001$.\nThe root is between $42$ and $43$. A more precise calculation gives:\n$$K_c \\approx 42.2644$$\nThis critical point $K_c$ lies within the feasible region $[0, 75]$. To confirm it is the global maximum, we can compare the throughput $S(K)$ at $K_c$ and at the boundaries:\n$$S(0) = (1.6 - 0.4)(1.5) = 1.2 \\times 1.5 = 1.8$$\n$$S(75) = (1.6 - 0.4 \\exp(-6))(1.5 - 0.075) \\approx (1.599)(1.425) \\approx 2.279$$\n$$S(K_c \\approx 42.26) = (1.6 - 0.4 \\exp(-0.08 \\cdot 42.26))(1.5 - 0.001 \\cdot 42.26) \\approx 2.313$$\nSince $S(K_c)$ is the largest value, the optimal number of RAS entries is $K^{\\star} = K_c$.\n\nThe problem asks for a final numeric answer for $K^{\\star}$ rounded to four significant figures.\n$$K^{\\star} = 42.2644 \\dots \\approx 42.26$$\nThe optimal number of RAS entries is $42.26$.", "answer": "$$\\boxed{42.26}$$", "id": "3673912"}]}