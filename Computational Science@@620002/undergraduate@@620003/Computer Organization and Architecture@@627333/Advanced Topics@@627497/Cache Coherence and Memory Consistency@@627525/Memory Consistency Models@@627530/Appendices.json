{"hands_on_practices": [{"introduction": "We begin with a classic \"litmus test\" that reveals the profound differences between memory consistency models. This exercise explores how the intuitive model of Sequential Consistency (SC) differs from the more relaxed, real-world models like Total Store Order (TSO) and Release-Acquire (RA) by analyzing a simple two-thread program. By determining which models permit a \"surprising\" outcome and how to prevent it with memory fences, you will build a foundational understanding of why explicit synchronization is essential in concurrent programming [@problem_id:3656650].", "problem": "Consider a two-thread program running on a shared-memory multiprocessor under different memory consistency models. The shared variables are $x$ and $y$, both initially $0$. Thread $T_1$ executes the program fragment $x := 1; \\ r_1 := y$, and thread $T_2$ executes $y := 1; \\ r_2 := x$. Assume that both $T_1$ and $T_2$ run concurrently. You will analyze the outcome $r_1 = r_2 = 0$ under three memory models and determine what Operating System (OS)-level fences are required to forbid this outcome. The three models are Sequential Consistency (SC), Total Store Order (TSO), and release/acquire (RA).\n\nUse the following foundational bases:\n- The definition of Sequential Consistency (SC): all operations appear to execute in a single total order that is consistent with the program order of each individual thread.\n- The core property of Total Store Order (TSO): a thread’s own writes can be buffered, allowing subsequent reads to bypass earlier writes to different locations, while preserving write-to-write program order and read-to-read program order.\n- The semantics of release/acquire (RA): a release store orders prior operations before itself in the same thread, and an acquire load orders itself before subsequent operations in the same thread; inter-thread ordering arises only when an acquire reads from a release, thereby creating a synchronizes-with edge.\n\nFor each model, reason from these bases to determine whether $r_1 = r_2 = 0$ is possible. Then, determine the minimal OS-level fence placement needed, if any, to forbid $r_1 = r_2 = 0$. An OS-level fence is any barrier that, by specification, prevents a subsequent load from observing memory as if it were reordered before a prior store in the same thread and ensures prior stores are made globally visible before subsequent loads in that thread proceed.\n\nWhich option correctly characterizes the outcomes and the required fences?\n\nA. Under Sequential Consistency (SC), $r_1 = r_2 = 0$ is impossible; under Total Store Order (TSO), $r_1 = r_2 = 0$ is possible; under release/acquire (RA), $r_1 = r_2 = 0$ is possible; to forbid $r_1 = r_2 = 0$ under TSO and RA, a full memory fence placed between the store and the subsequent load in each thread suffices.\n\nB. Under Sequential Consistency (SC), $r_1 = r_2 = 0$ is possible; under Total Store Order (TSO), $r_1 = r_2 = 0$ is impossible; under release/acquire (RA), $r_1 = r_2 = 0$ is impossible; no fences are required in any model.\n\nC. Under Sequential Consistency (SC) and Total Store Order (TSO), $r_1 = r_2 = 0$ are both impossible, but under release/acquire (RA), $r_1 = r_2 = 0$ is possible; to forbid it under RA, using a release store followed by an acquire load in each thread suffices without additional fences.\n\nD. Under Sequential Consistency (SC), $r_1 = r_2 = 0$ is impossible; under Total Store Order (TSO), $r_1 = r_2 = 0$ is impossible; under release/acquire (RA), $r_1 = r_2 = 0$ is possible; to forbid it under RA, only placing a release fence before the store in each thread is sufficient.\n\nE. Under Sequential Consistency (SC), $r_1 = r_2 = 0$ is impossible; under Total Store Order (TSO), $r_1 = r_2 = 0$ is possible; under release/acquire (RA), $r_1 = r_2 = 0$ is impossible; to forbid it on TSO, a write-only barrier before the store in each thread suffices.", "solution": "The problem statement is analyzed for validity.\n\n### Step 1: Extract Givens\n- **Threads and Shared Memory:** Two threads, $T_1$ and $T_2$, on a shared-memory multiprocessor.\n- **Shared Variables and Initial State:** $x$ and $y$, both initially $0$.\n- **Thread Programs:**\n    - $T_1$: executes $x := 1; \\ r_1 := y$.\n    - $T_2$: executes $y := 1; \\ r_2 := x$.\n    - $r_1$ and $r_2$ are thread-local registers.\n- **Outcome Under Analysis:** The final state where $r_1 = 0$ and $r_2 = 0$.\n- **Memory Models and Definitions:**\n    1. **Sequential Consistency (SC):** All operations appear to execute in a single total order that is consistent with the program order of each individual thread.\n    2. **Total Store Order (TSO):** A thread’s own writes can be buffered, allowing subsequent reads to bypass earlier writes to different locations, while preserving write-to-write program order and read-to-read program order.\n    3. **Release/Acquire (RA):** A release store orders prior operations before itself, and an acquire load orders itself before subsequent operations. Inter-thread ordering (a \"synchronizes-with\" relationship) is established when an acquire load reads a value written by a release store. The baseline operations are simple stores and loads, which in a relaxed-memory context (the domain of RA) are not ordered with respect to each other by default.\n- **Fence Definition:** An OS-level fence prevents a subsequent load from being reordered before a prior store in the same thread and ensures prior stores are made globally visible before subsequent loads in that thread proceed.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically sound, well-posed, and objective.\n- **Scientifically Grounded:** The problem uses a classic example (a simplified mutual exclusion entry protocol, also known as the store-buffering litmus test) to differentiate fundamental memory consistency models (SC, TSO, RA). The provided definitions are standard and accurate characterizations of these models.\n- **Well-Posed:** The problem provides all necessary information: initial conditions, thread programs, and definitions of the memory models. The question is precise, asking whether a specific outcome is possible under each model and how to prevent it. A unique, meaningful solution can be derived from these premises.\n- **Objective:** The problem is stated in formal, unambiguous terms. There are no subjective or vague elements.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be derived.\n\n### Derivation and Analysis\n\nLet us denote the operations as follows:\n- $S_1$: $T_1$'s store operation, $x := 1$.\n- $L_1$: $T_1$'s load operation, $r_1 := y$.\n- $S_2$: $T_2$'s store operation, $y := 1$.\n- $L_2$: $T_2$'s load operation, $r_2 := x$.\n\nThe initial state is $x=0, y=0$. The outcome being evaluated is $r_1 = 0$ and $r_2 = 0$. This outcome requires that $L_1$ observes the initial value of $y$ and $L_2$ observes the initial value of $x$.\n\n**1. Analysis under Sequential Consistency (SC)**\nUnder SC, all operations must appear in a single global total order, and this order must respect the program order within each thread. The program orders are $S_1$ before $L_1$ (denoted $S_1 \\rightarrow_p L_1$) and $S_2$ before $L_2$ (denoted $S_2 \\rightarrow_p L_2$).\nFor the outcome $r_1 = 0$ and $r_2 = 0$ to occur:\n- $L_1$ must execute before $S_2$ is globally visible. In a total order, this means $L_1$ comes before $S_2$.\n- $L_2$ must execute before $S_1$ is globally visible. In a total order, this means $L_2$ comes before $S_1$.\nCombining these with the program order constraints, we get a cycle of dependencies for the total order:\n$S_1 \\rightarrow_p L_1$ (program order in $T_1$)\n$L_1 \\rightarrow S_2$ (for $r_1 = 0$)\n$S_2 \\rightarrow_p L_2$ (program order in $T_2$)\n$L_2 \\rightarrow S_1$ (for $r_2 = 0$)\nThis forms the cycle: $S_1 \\rightarrow L_1 \\rightarrow S_2 \\rightarrow L_2 \\rightarrow S_1$. A total order cannot contain a cycle. Therefore, it is impossible to construct a valid SC execution that results in $r_1 = 0$ and $r_2 = 0$. At least one load must observe the new value written by the other thread.\n**Conclusion for SC:** The outcome $r_1 = r_2 = 0$ is **impossible**. No fences are required.\n\n**2. Analysis under Total Store Order (TSO)**\nThe TSO model allows a thread's own writes to be buffered. A subsequent load to a *different* memory location can bypass the buffered store. This is equivalent to allowing a Store-Load reordering when the store and load are to different addresses.\n- In $T_1$, the store is to $x$ and the load is from $y$. Since $x \\neq y$, the operations can be effectively reordered. The execution can appear as if $L_1$ occurs before $S_1$.\n- In $T_2$, the store is to $y$ and the load is from $x$. Since $y \\neq x$, the operations can be effectively reordered. The execution can appear as if $L_2$ occurs before $S_2$.\nThis leads to a possible interleaving of the reordered operations:\n1. $T_1$ executes its load: $L_1 (r_1 := y)$. The value of $y$ is its initial value, $0$. So, $r_1$ becomes $0$.\n2. $T_2$ executes its load: $L_2 (r_2 := x)$. The value of $x$ is its initial value, $0$. So, $r_2$ becomes $0$.\n3. $T_1$ commits its store from its buffer: $S_1 (x := 1)$. The value of $x$ in shared memory becomes $1$.\n4. $T_2$ commits its store from its buffer: $S_2 (y := 1)$. The value of $y$ in shared memory becomes $1$.\nThis execution is valid under TSO and results in $r_1=0, r_2=0$.\n**Conclusion for TSO:** The outcome $r_1 = r_2 = 0$ is **possible**. To forbid it, the Store-Load reordering must be prevented. Placing a fence as defined in the problem between the store and the load in each thread ($S_1; \\text{fence}; L_1$ and $S_2; \\text{fence}; L_2$) enforces that the store is globally visible before the load can proceed, thus forbidding the outcome.\n\n**3. Analysis under Release/Acquire (RA)**\nThe RA model provides specific semantics for `release` stores and `acquire` loads. The problem statement gives code with plain stores and loads ($x := 1; r_1 := y; \\dots$). In the context of architectures that offer RA semantics (e.g., ARM, RISC-V), plain memory operations are \"relaxed\" or \"weakly ordered.\" This means that, by default, memory operations to different addresses can be reordered by the hardware.\nThe situation is thus analogous to TSO, but potentially allowing even more reorderings (though for this specific problem, only Store-Load reordering is relevant).\n- In $T_1$: $S_1$ and $L_1$ target different addresses, so they can be reordered.\n- In $T_2$: $S_2$ and $L_2$ target different addresses, so they can be reordered.\nThe same execution trace possible under TSO is also possible here, leading to the outcome $r_1=0, r_2=0$.\n**Conclusion for RA:** The outcome $r_1 = r_2 = 0$ is **possible**. As with TSO, forbidding this outcome requires preventing the Store-Load reordering. A full memory fence between the store and load in each thread is a sufficient mechanism to achieve this. Alternatively, one could use specific RA operations (e.g., `store-release` followed by `load-acquire`, or a `release` fence) but the most general solution described in the options is a full fence.\n\n### Option-by-Option Analysis\n\n**A. Under Sequential Consistency (SC), $r_1 = r_2 = 0$ is impossible; under Total Store Order (TSO), $r_1 = r_2 = 0$ is possible; under release/acquire (RA), $r_1 = r_2 = 0$ is possible; to forbid $r_1 = r_2 = 0$ under TSO and RA, a full memory fence placed between the store and the subsequent load in each thread suffices.**\n- SC outcome: \"impossible\". Correct.\n- TSO outcome: \"possible\". Correct.\n- RA outcome: \"possible\". Correct, under the standard interpretation of plain operations in a relaxed memory model.\n- Fencing solution: \"a full memory fence placed between the store and the subsequent load in each thread suffices\". Correct. This fence explicitly prevents the Store-Load reordering that allows the outcome.\n**Verdict: Correct.**\n\n**B. Under Sequential Consistency (SC), $r_1 = r_2 = 0$ is possible; under Total Store Order (TSO), $r_1 = r_2 = 0$ is impossible; under release/acquire (RA), $r_1 = r_2 = 0$ is impossible; no fences are required in any model.**\n- SC outcome: \"possible\". Incorrect.\n- TSO outcome: \"impossible\". Incorrect.\n- RA outcome: \"impossible\". Incorrect.\n**Verdict: Incorrect.**\n\n**C. Under Sequential Consistency (SC) and Total Store Order (TSO), $r_1 = r_2 = 0$ are both impossible, but under release/acquire (RA), $r_1 = r_2 = 0$ is possible; to forbid it under RA, using a release store followed by an acquire load in each thread suffices without additional fences.**\n- TSO outcome: \"impossible\". Incorrect. TSO is the canonical example of a model (stronger than fully relaxed) that still allows this outcome due to store buffering.\n**Verdict: Incorrect.**\n\n**D. Under Sequential Consistency (SC), $r_1 = r_2 = 0$ is impossible; under Total Store Order (TSO), $r_1 = r_2 = 0$ is impossible; under release/acquire (RA), $r_1 = r_2 = 0$ is possible; to forbid it under RA, only placing a release fence before the store in each thread is sufficient.**\n- TSO outcome: \"impossible\". Incorrect.\n- The proposed fence for RA is also not the standard or correct way to prevent Store-Load reordering. A release fence primarily orders operations *before* it with respect to operations *after* it. A `fence_release; store; load` sequence does not guarantee ordering between the `store` and `load`.\n**Verdict: Incorrect.**\n\n**E. Under Sequential Consistency (SC), $r_1 = r_2 = 0$ is impossible; under Total Store Order (TSO), $r_1 = r_2 = 0$ is possible; under release/acquire (RA), $r_1 = r_2 = 0$ is impossible; to forbid it on TSO, a write-only barrier before the store in each thread suffices.**\n- RA outcome: \"impossible\". Incorrect. This assumes plain operations in a RA model are somehow implicitly ordered, which is not the standard interpretation.\n- The proposed fence for TSO is incorrect. A write barrier (or store fence) typically orders writes with respect to other writes; it does not prevent a subsequent load from bypassing a prior write. A Store-Load barrier Is needed.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3656650"}, {"introduction": "Beyond the ordering of operations, their very indivisibility—or atomicity—is a critical factor for correctness. This practice explores the subtle but crucial distinction between memory consistency and atomicity through the dangerous phenomenon of a \"torn read\" [@problem_id:3656511]. You will investigate a scenario where a $64$-bit value is accessed on a machine that may only support $32$-bit atomic operations, learning how this can lead to corrupted data even on a machine with a strong consistency model, and clarifying the role of language-level atomics in preventing such bugs.", "problem": "Consider a shared scalar variable $x$ that occupies $64$ bits in memory. Let $x[\\text{lo}]$ denote the lower $32$ bits ($x[31:0]$) and $x[\\text{hi}]$ denote the upper $32$ bits ($x[63:32]$). The initial state is $x = 0x0000000000000000$. Three threads execute concurrently with no synchronization or fences:\n\n- Thread $T_0$: $x[\\text{lo}] := 0xAAAAAAAA$.\n- Thread $T_1$: $x[\\text{hi}] := 0xBBBBBBBB$.\n- Thread $T_2$: $r := \\text{load}_{64}(x)$, a read of the entire $64$-bit value of $x$ executed once, with $r$ local to $T_2$.\n\nAssume the machine is cache-coherent for each byte of memory and that the compiler may implement $\\text{load}_{64}(x)$ as either a single $64$-bit load or as two $32$-bit loads depending on the available atomic width. Define a “torn read” as any read of the $64$-bit object $x$ that returns a value whose lower and upper $32$-bit halves originate from different write events to $x$. For concreteness, the “mixed” value\n$$\nr_{\\text{mixed}} = (0xBBBBBBBB \\ll 32) + 0xAAAAAAAA\n$$\nis such a candidate outcome, where $0xAAAAAAAA$ was written by $T_0$ and $0xBBBBBBBB$ was written by $T_1$.\n\nYou are to reason from the following foundational bases:\n- The definition of a Memory Consistency Model (MCM): a rule set that constrains the visibility and ordering of memory operations (for example, Sequential Consistency (SC) enforces that operations appear as if interleaved in a single global order consistent with program order), assuming memory operations are atomic at the granularity modeled.\n- The definition of atomicity: an operation is atomic if, with respect to other threads, it is indivisible; no other thread can observe a partial effect of that operation.\n- The definition of cache coherence: for each byte-addressable location, there is a coherent order of stores, and a load returns the most recent store to that location in that order.\n- The C11/C++11 language memory model: non-atomic concurrent accesses to the same object without synchronization create a data race that yields undefined behavior; atomic operations on $\\text{atomic}$ objects are indivisible and forbid word tearing.\n\nBased on these definitions and the scenario above, which of the following statements are true?\n\nA. Under an abstract Sequential Consistency model where $\\text{load}_{64}(x)$ on a $32$-bit machine is implemented as two atomic $32$-bit loads (one of $x[\\text{lo}]$ and one of $x[\\text{hi}]$), the outcome $r = r_{\\text{mixed}}$ is allowed by Sequential Consistency because SC orders the individual $32$-bit loads and stores but does not require the two halves of the read to be simultaneous.\n\nB. Stronger memory consistency alone (for example, Sequential Consistency or Total Store Order) forbids $r = r_{\\text{mixed}}$ regardless of whether the $\\text{load}_{64}(x)$ is implemented atomically or as two $32$-bit loads.\n\nC. In the C11/C++11 memory model, if $x$ is a non-atomic object and the threads perform unsynchronized concurrent accesses, the program has a data race and its behavior is undefined; the model neither promises nor forbids observing $r = r_{\\text{mixed}}$.\n\nD. On a $32$-bit architecture that lacks lock-free $64$-bit atomics (so that $\\text{load}_{64}(x)$ compiles to two $32$-bit loads), $r = r_{\\text{mixed}}$ can occur even under Sequential Consistency, because SC reasons about the two $32$-bit loads separately and allows an interleaving in which the halves come from different stores.\n\nE. Cache coherence guarantees that $r$ must equal the value from a single, whole-store to $x$; therefore $r = r_{\\text{mixed}}$ is impossible.\n\nSelect all correct options.", "solution": "The problem asks us to evaluate several statements concerning memory operations on a $64$-bit shared variable $x$ by three concurrent threads. The initial value is $x = 0x0000000000000000$. The threads perform the following operations without synchronization:\n- Thread $T_0$: $x[\\text{lo}] := 0xAAAAAAAA$. Let's denote the value $0xAAAAAAAA$ as $A$. This is a $32$-bit write to the lower half of $x$. We can denote this operation as $W_A$.\n- Thread $T_1$: $x[\\text{hi}] := 0xBBBBBBBB$. Let's denote the value $0xBBBBBBBB$ as $B$. This is a $32$-bit write to the upper half of $x$. We can denote this operation as $W_B$.\n- Thread $T_2$: $r := \\text{load}_{64}(x)$. This is a read of the entire $64$-bit value.\n\nThe value in question is $r_{\\text{mixed}} = 0xBBBBBBBBAAAAAAAA$, which corresponds to observing the write from $T_1$ in the upper half and the write from $T_0$ in the lower half. The problem hinges on the distinction between memory consistency, cache coherence, and atomicity, particularly concerning whether the $64$-bit load is a single atomic operation or two separate $32$-bit loads.\n\nThe foundational principles are:\n- **Atomicity**: An operation is atomic if it appears to all other threads to occur instantaneously. A non-atomic $64$-bit load composed of two $32$-bit loads can be interrupted, meaning other threads can perform operations between the two constituent loads.\n- **Cache Coherence**: Guarantees that for any single memory location (e.g., a byte or word), all processors observe a single, consistent sequence of writes. It does *not* enforce any ordering between writes to *different* locations, nor does it guarantee atomicity for multi-word operations.\n- **Memory Consistency Model (MCM)**: Defines the allowed orderings of memory operations across different threads. Sequential Consistency (SC) is a strong model requiring that all operations appear to execute in some single global sequential order that is consistent with the program order within each thread.\n- **C11/C++11 Memory Model**: For non-atomic variables, any concurrent read and write, or two writes, to the same memory location without synchronization constitutes a data race, which results in Undefined Behavior (UB).\n\nWith these principles, we analyze each option.\n\n**A. Under an abstract Sequential Consistency model where $\\text{load}_{64}(x)$ on a $32$-bit machine is implemented as two atomic $32$-bit loads (one of $x[\\text{lo}]$ and one of $x[\\text{hi}]$), the outcome $r = r_{\\text{mixed}}$ is allowed by Sequential Consistency because SC orders the individual $32$-bit loads and stores but does not require the two halves of the read to be simultaneous.**\n\nThis statement is **Correct**. If the `load_64(x)` is not atomic and is performed as two $32$-bit loads ($R_{\\text{hi}}$ and $R_{\\text{lo}}$), then the fundamental atomic operations that SC reasons about are the $32$-bit writes ($W_A$, $W_B$) and the $32$-bit reads. SC requires a single total order of these atomic operations. A torn read is possible if the writes from $T_0$ and $T_1$ are interleaved with the reads from $T_2$.\nConsider the following global order of operations, which is a valid interleaving under SC:\n1.  $T_1$ executes its write: $W_B$ stores $0xBBBBBBBB$ to $x[\\text{hi}]$.\n2.  $T_2$ executes a read of the high part: $R_{\\text{hi}}$ reads $x[\\text{hi}]$ and gets $0xBBBBBBBB$.\n3.  $T_0$ executes its write: $W_A$ stores $0xAAAAAAAA$ to $x[\\text{lo}]$.\n4.  $T_2$ executes a read of the low part: $R_{\\text{lo}}$ reads $x[\\text{lo}]$ and gets $0xAAAAAAAA$.\nThe final value assembled in $T_2$'s register $r$ is $0xBBBBBBBBAAAAAAAA$, which is $r_{\\text{mixed}}$. Since a valid SC-compliant interleaving leads to this result, the outcome is allowed by SC. The reasoning given in the option is sound.\n\n**B. Stronger memory consistency alone (for example, Sequential Consistency or Total Store Order) forbids $r = r_{\\text{mixed}}$ regardless of whether the $\\text{load}_{64}(x)$ is implemented atomically or as two $32$-bit loads.**\n\nThis statement is **Incorrect**.\n- Case 1: The `load` is two $32$-bit loads. As demonstrated in the analysis of option A, SC *allows* the outcome $r = r_{\\text{mixed}}$.\n- Case 2: The `load` is a single atomic $64$-bit load. The final state of the $64$-bit variable $x$ after both writes $W_A$ and $W_B$ have completed is $0xBBBBBBBBAAAAAAAA$. An atomic $64$-bit load is free to execute after both writes are globally visible, in which case it will legally read $r_{\\text{mixed}}$. A strong consistency model does not forbid reading a value that the variable legitimately holds at some point in time.\nSince the claim is false in both scenarios, the statement is incorrect. Memory consistency cannot create atomicity where the hardware does not provide it, nor does it forbid reading valid, fully-written states.\n\n**C. In the C11/C++11 memory model, if $x$ is a non-atomic object and the threads perform unsynchronized concurrent accesses, the program has a data race and its behavior is undefined; the model neither promises nor forbids observing $r = r_{\\text{mixed}}$.**\n\nThis statement is **Correct**. The scenario describes multiple threads accessing the same memory object $x$ without synchronization, and at least one access is a write ($T_0$ and $T_1$ both write). This is the definition of a data race in the C11/C++11 memory model when $x$ is a standard, non-atomic type. The consequence of a data race is Undefined Behavior (UB). UB means the C++ standard imposes no requirements on the behavior of the program. The program might crash, yield an arbitrary value, or appear to work as expected. Therefore, the implementation is free to generate code that results in $r = r_{\\text{mixed}}$, but it is also free to produce any other outcome. The model \"neither promises nor forbids\" this specific result, which is a precise description of UB.\n\n**D. On a $32$-bit architecture that lacks lock-free $64$-bit atomics (so that $\\text{load}_{64}(x)$ compiles to two $32$-bit loads), $r = r_{\\text{mixed}}$ can occur even under Sequential Consistency, because SC reasons about the two $32$-bit loads separately and allows an interleaving in which the halves come from different stores.**\n\nThis statement is **Correct**. This is a more concrete restatement of the situation described in option A. On a $32$-bit machine without native $64$-bit atomic operations, a $64$-bit load on $x$ is necessarily non-atomic, implemented as two separate $32$-bit loads. As established in the analysis for option A, Sequential Consistency reasons about the sequence of these fundamental $32$-bit atomic operations. It allows an interleaving where one half of $x$ is read before a write to the other half, and the second half of $x$ is read after that write, leading to a torn read. The logic provided in the option is entirely correct.\n\n**E. Cache coherence guarantees that `r` must equal the value from a single, whole-store to $x$; therefore `r = r_{\\text{mixed}}$ is impossible.**\n\nThis statement is **Incorrect**. This confuses the guarantees of cache coherence with those of atomicity. Cache coherence ensures that all processors see a consistent serialization of writes to a *single memory location*. In this problem, $x[\\text{lo}]$ and $x[\\text{hi}]$ are distinct memory locations (or can be treated as such within a cache line). Coherence guarantees a consistent view of writes to $x[\\text{lo}]$ and a separate consistent view of writes to $x[\\text{hi}]$. It does *not* relate the timing of writes to these two different locations. The statement that a read must return a value from a \"single, whole-store\" is a property of an atomic read, which is not guaranteed by coherence. Since the $64$-bit load may not be atomic, and there is no single $64$-bit store that wrote $r_{\\text{mixed}}$, coherence does not forbid this outcome.", "answer": "$$\\boxed{ACD}$$", "id": "3656511"}, {"introduction": "Having analyzed the surprising behaviors of relaxed models, we now shift from analysis to design. This exercise challenges you to act as a systems programmer, using the precise tools of the Release-Acquire (RA) memory model to engineer correctness for a complex set of dependencies [@problem_id:3656597]. By calculating the minimum number of memory fences required to enforce a given ordering tree, you will gain hands-on experience in building correct and efficient synchronization logic for sophisticated, lock-free algorithms.", "problem": "Consider a shared-memory program running under the Release–Acquire (RA) memory model. In RA, a store annotated as release on an atomic variable creates a potential inter-thread ordering when a load annotated as acquire on the same atomic variable reads-from that release store. The happens-before (hb) relation is defined as the transitive closure of program order (po) within each thread together with synchronizes-with (sw) edges induced by matching release–acquire pairs; relaxed loads and stores do not contribute inter-thread hb. The global hb relation must be acyclic.\n\nYou are given a directed rooted tree of dependencies whose nodes are variables $x_1, \\dots, x_{10}$ and whose directed edges encode expectations of the form “a write to a parent’s variable should happen-before the first read of its child’s variable.” Specifically, the tree is:\n- Root $x_1$ has children $x_2$, $x_3$, and $x_4$.\n- Node $x_2$ has children $x_5$ and $x_6$.\n- Node $x_3$ has child $x_7$.\n- Node $x_4$ has children $x_8$, $x_9$, and $x_{10}$.\n\nEach variable $x_i$ is owned by a distinct thread $T_i$. For each directed edge $x_u \\rightarrow x_v$, thread $T_u$ performs a single write to $x_u$, and thread $T_v$ will later perform its first read of $x_v$. The expectation on edge $x_u \\rightarrow x_v$ is that the write to $x_u$ must happen-before the first read of $x_v$ in the global hb relation.\n\nYou are allowed to place “fences” only in the sense of RA annotations: a release on a store and an acquire on a load to some atomic synchronization variable. Assume each internal node $x_u$ may use a single dedicated atomic flag $f_u$ to communicate to all its children; a child $x_v$ can perform an acquire load of $f_u$ to observe that $T_u$ has completed its writes. A release–acquire pair on the same flag that is observed (the acquire reads-from the release) induces a synchronizes-with edge from $T_u$ to $T_v$. Any ordering requirement that is not supported by a release–acquire pair must be satisfied only by po within a thread and does not contribute inter-thread hb. You may choose which stores are release stores and which loads are acquire loads; each RA annotation (release on a store or acquire on a load) counts as one fence.\n\nCompute the minimal total number of such fences required to ensure that, for all edges $x_u \\rightarrow x_v$ in the tree, the expected ordering “write to $x_u$ happens-before first read of $x_v$” holds in the global hb, and the global hb remains acyclic. Express your answer as a single integer. No rounding is required and no units are to be reported.", "solution": "The problem requires finding the minimum number of \"fences\" (Release-Acquire annotations) to satisfy a set of ordering constraints in a shared-memory program. The constraints are derived from a dependency tree structure involving $10$ variables, $x_1, \\dots, x_{10}$, each owned by a distinct thread, $T_1, \\dots, T_{10}$, respectively.\n\nThe core requirement is that for every directed edge $x_u \\rightarrow x_v$ in the given tree, the operation \"write to $x_u$\" must happen-before the \"first read of $x_v$\". Let us denote the write operation to variable $x_u$ by thread $T_u$ as $W(x_u)$, and the first read operation of variable $x_v$ by thread $T_v$ as $R(x_v)$. The requirement is to ensure $W(x_u) \\text{ hb } R(x_v)$ for all edges $x_u \\rightarrow x_v$.\n\nThe happens-before relation ($hb$) is defined as the transitive closure of program order ($po$) and synchronizes-with ($sw$).\nAn $sw$ edge is established between a release store and an acquire load on the same atomic variable, where the load reads the value written by the store. Since $W(x_u)$ and $R(x_v)$ occur in different threads ($T_u$ and $T_v$), the $hb$ relation between them cannot be established by $po$ alone. It must rely on a chain of relations involving at least one $sw$ edge.\n\nLet's analyze the mechanism for a single parent-child dependency $x_u \\rightarrow x_v$. To ensure $W(x_u) \\text{ hb } R(x_v)$, the threads $T_u$ and $T_v$ can use a dedicated atomic flag, say $f_u$, as specified in the problem.\nThe sequence of operations would be:\n1.  In thread $T_u$: The write $W(x_u)$ must be ordered before a release store to the flag $f_u$. Let this store be $S(f_u)$. This gives the program order relation $W(x_u) \\text{ po } S(f_u)$. To make this a synchronization point, $S(f_u)$ must be a release store. This requires one `release` annotation, which counts as one fence.\n2.  In thread $T_v$: An acquire load from the flag $f_u$, let's call it $L(f_u)$, must be ordered before the read $R(x_v)$. This gives the program order relation $L(f_u) \\text{ po } R(x_v)$. For synchronization, $L(f_u)$ must be an acquire load. This requires one `acquire` annotation, which is another fence.\n\nIf the acquire load $L(f_u)$ in thread $T_v$ reads the value written by the release store $S(f_u)$ in thread $T_u$, a synchronizes-with edge is created: $S(f_u) \\text{ sw } L(f_u)$.\nBy transitivity of the $hb$ relation, we have:\n$$W(x_u) \\xrightarrow{po} S(f_u) \\xrightarrow{sw} L(f_u) \\xrightarrow{po} R(x_v)$$\nThis composite relation implies $W(x_u) \\text{ hb } R(x_v)$, satisfying the constraint for the edge $x_u \\rightarrow x_v$. This minimal scheme for one edge requires one release fence and one acquire fence, for a total of $2$ fences.\n\nThe problem states that an internal node $x_u$ can use a single flag $f_u$ to communicate with *all* its children. Let's consider a parent node $x_u$ with $k$ children, $\\{x_{v_1}, x_{v_2}, \\dots, x_{v_k}\\}$.\nTo satisfy all $k$ dependencies, $W(x_u) \\text{ hb } R(x_{v_i})$ for $i=1, \\dots, k$:\n-   Thread $T_u$ performs a single `release` store on its flag $f_u$ after its write $W(x_u)$. This costs $1$ fence.\n-   Each child thread $T_{v_i}$ must perform an `acquire` load on the flag $f_u$ before its read $R(x_{v_i})$. Since each child is in a distinct thread, this requires $k$ separate acquire loads, one for each child. This costs $k$ fences.\nThe total number of fences to satisfy the dependencies for a parent with $k$ children is $1+k$. This is the minimal number, as the release from the parent is necessary, and each child must individually perform an acquire to establish synchronization with the parent.\n\nTo find the total minimum number of fences, we can sum the fences required for each node based on its role in the tree. A fence is an annotation on an operation.\n-   Any node that is a **parent** (a non-leaf node) must perform a `release` store to signal its children. This requires one release fence per parent node.\n-   Any node that is a **child** (a non-root node) must perform an `acquire` load to synchronize with its parent. This requires one acquire fence per child node.\n\nLet's identify the parent and child nodes from the given tree structure:\n-   The tree has nodes $x_1, \\dots, x_{10}$.\n-   $x_1$ has children $x_2, x_3, x_4$.\n-   $x_2$ has children $x_5, x_6$.\n-   $x_3$ has child $x_7$.\n-   $x_4$ has children $x_8, x_9, x_{10}$.\n\nThe set of **parent nodes** (non-leaf nodes) is $P = \\{x_1, x_2, x_3, x_4\\}$. The number of parent nodes is $|P| = 4$. Each of these nodes corresponds to a thread that must perform one release store. Therefore, the total number of release fences is $4$.\n\nThe set of **child nodes** (non-root nodes) is $C = \\{x_2, x_3, x_4, x_5, x_6, x_7, x_8, x_9, x_{10}\\}$. The number of child nodes is $|C| = 9$. Each of these nodes corresponds to a thread that must perform one acquire load. Therefore, the total number of acquire fences is $9$.\n\nThe total minimal number of fences is the sum of all required release and acquire fences.\nTotal Fences = (Number of release fences) + (Number of acquire fences) = $|P| + |C| = 4 + 9 = 13$.\n\nThe nodes $x_2, x_3, x_4$ are both parents and children. For instance, thread $T_2$ must perform an acquire load on flag $f_1$ to synchronize with its parent $T_1$, and a release store on its own flag $f_2$ to signal its children $T_5$ and $T_6$. This requires two fences within thread $T_2$. This is consistent with our counting method.\n\nFinally, the problem requires the global `hb` relation to be acyclic. The inter-thread `hb` relations established by our scheme all follow the direction of the edges in the dependency tree ($T_{parent} \\rightarrow T_{child}$). Since the dependency structure is a tree, it is inherently acyclic. Thus, the graph of inter-thread synchronizations is also a directed acyclic graph, which guarantees that the global `hb` relation remains acyclic.\n\nThe minimal total number of fences required is $13$.", "answer": "$$\\boxed{13}$$", "id": "3656597"}]}