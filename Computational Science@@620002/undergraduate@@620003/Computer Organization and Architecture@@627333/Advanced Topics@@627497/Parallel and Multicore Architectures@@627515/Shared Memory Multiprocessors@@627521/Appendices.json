{"hands_on_practices": [{"introduction": "In any shared-memory system, a \"hot spot\" emerges when multiple processors frequently access the same piece of data, such as a shared counter. This exercise models the ultimate performance bottleneck in such a scenario: serialization. By deriving the system throughput, you will discover how cache coherence protocols, while ensuring data correctness, fundamentally limit performance to the speed at which ownership of a single cache line can be passed from one core to another [@problem_id:3675634].", "problem": "Consider a shared-memory multiprocessor with $N$ identical cores, a single shared counter variable stored in one cache line, and an invalidation-based cache coherence protocol with Modified, Exclusive, Shared, Invalid (MESI) states. The system enforces sequential consistency, meaning that all memory operations appear in a single total order consistent with program order. An increment of the shared counter is performed by an atomic read-modify-write operation that requires the core to hold the cache line in the Modified state prior to the write.\n\nDefine the cache line handoff latency $t_h$ as the end-to-end time for exclusive ownership of the cache line to transfer from the current owner to a requesting core, including coherence messages, invalidations, data movement, and any arbitration delays, but excluding local execution time of the increment once the line is held in the Modified state. Assume the following stress test configuration:\n- Each core repeatedly performs a single atomic increment of the shared counter and then immediately requests the line again only after some other core has incremented, enforcing a strict round-robin ownership pattern among the $N$ cores.\n- There is always at least one requesting core when a handoff completes, so the line is continuously contended.\n- The local execution time of the increment once the cache line is obtained in the Modified state is negligible compared to $t_h$ and may be treated as zero for the purpose of throughput analysis.\n- No other memory traffic interferes, and the interconnect does not introduce additional queuing beyond what is captured in $t_h$.\n\nUnder these assumptions, derive the steady-state system throughput $X(N, t_h)$ of completed increments per second in closed form. Your final answer must be a single analytic expression in terms of $N$ and $t_h$. Do not provide an inequality or an equation to be solved; provide the final expression. You do not need to round the result. Express the throughput as increments per second.", "solution": "The system throughput, $X$, is defined as the total number of completed increments per unit of time. It can be calculated as the reciprocal of the average time elapsed between two consecutive completed increments in the system.\n\nThe problem states that an increment is an atomic read-modify-write operation that requires the core to hold the cache line in the Modified (M) state. Because only one core can hold the line in the M state at any given time, the increments are serialized across the entire system. This means that at any point in time, at most one increment can be in progress.\n\nLet's analyze the sequence of events for the system to complete one increment.\nAssume at time $T_0$, Core $C_i$ has just completed an increment. It currently holds the cache line in the M state.\n\n1.  Due to the \"continuously contended\" and \"strict round-robin\" assumptions, the next core in the sequence, Core $C_{i+1}$ (indices are modulo $N$), has already issued a request for the cache line.\n2.  The process of transferring exclusive ownership of the cache line from Core $C_i$ to Core $C_{i+1}$ begins. This involves Core $C_i$ responding to the request, invalidating its copy (or transitioning from M), and sending the data to Core $C_{i+1}$. Core $C_{i+1}$ then receives the data and gains ownership in the M state.\n3.  The problem defines the total time for this entire transfer process as the cache line handoff latency, $t_h$.\n4.  Therefore, Core $C_{i+1}$ acquires the line in the M state at time $T_0 + t_h$.\n5.  The problem specifies that the local execution time of the increment, once the line is acquired, is negligible and can be treated as zero. Thus, Core $C_{i+1}$ completes its increment effectively at the same instant it acquires the line, at time $T_1 = T_0 + t_h$.\n\nThis analysis shows that the time elapsed between the completion of one increment (by Core $C_i$) and the completion of the very next increment in the system (by Core $C_{i+1}$) is exactly $t_h$.\n\nThe system completes one increment every $t_h$ seconds. The rate of completion, which is the system throughput $X$, is the reciprocal of the time per increment.\n$$\nX = \\frac{1 \\text{ increment}}{t_h \\text{ seconds}}\n$$\nThus, the throughput is:\n$$\nX(N, t_h) = \\frac{1}{t_h}\n$$\nThe number of cores, $N$, is a parameter of the system that establishes the round-robin pattern and ensures contention. However, in this fully serialized model where the bottleneck is the handoff of the single resource, the overall system throughput is independent of the number of contenders, $N$ (as long as $N > 1$). The throughput of any single core is $\\frac{1}{N \\cdot t_h}$, but the total system throughput, being the sum of throughputs of all $N$ cores, is $N \\times \\frac{1}{N \\cdot t_h} = \\frac{1}{t_h}$.", "answer": "$$\\boxed{\\frac{1}{t_h}}$$", "id": "3675634"}, {"introduction": "After seeing how latency can limit throughput, it's natural to ask how architects reduce that latency. This practice explores a key optimization in modern multiprocessors: the cache-to-cache transfer. You will use the fundamental concept of Average Memory Access Time ($AMAT$) to quantify the performance benefit of fetching data from a peer core's cache versus going to a slower, shared cache or main memory, a crucial technique for building high-performance systems [@problem_id:3675527].", "problem": "A symmetric shared-memory multiprocessor implements the Modified, Exclusive, Shared, Invalid (MESI) cache-coherence protocol with private level-$1$ (L1) write-back caches and an inclusive shared last-level cache (LLC). Consider a steady-state workload where each memory access is to a single cache line and the following are known, based on measurement and microarchitectural characterization:\n\n- The L1 hit latency is $t_{L1} = 4$ cycles and is identical across all cores in both designs considered below.\n- The L1 miss rate is $m_1 = 0.08$ per access.\n- Conditional on an L1 miss, the probability that the requested line is currently held in a single other coreâ€™s private cache in a state that permits direct forwarding is $p_{\\text{owner}} = 0.35$.\n- If a line is forwarded directly via a cache-to-cache transfer, the end-to-end latency (including directory lookup, coherence messages, and data transmission) is $t_{\\text{c2c}} = 80$ cycles.\n- If a line is not forwarded from another core, the request is served by the LLC or main memory with an effective average latency of $t_{\\text{LLC}} = 140$ cycles.\n\nAssume no queuing, contention, or overlap effects, and that the coherence state distribution is stationary and independent from access to access except as described above. Consider two designs:\n\n- Design A supports cache-to-cache forwarding when possible.\n- Design B does not support cache-to-cache forwarding; in B, every L1 miss is served by the LLC or main memory.\n\nUsing only the fundamental definitions of miss rate as a probability and expected latency as a probability-weighted average over mutually exclusive outcomes, derive from first principles the expected per-access latency reduction (in cycles) of Design A relative to Design B. Express your final answer in cycles and round your answer to $4$ significant figures.", "solution": "The problem asks for the expected per-access latency reduction of a multiprocessor design that supports cache-to-cache forwarding (Design A) relative to one that does not (Design B). This reduction can be found by calculating the average memory access time (AMAT) for each design and finding their difference.\n\nThe AMAT is the weighted average latency of all possible access outcomes. Let $E[T]$ be the AMAT.\n$$\nE[T] = (\\text{L1 Hit Rate}) \\times (\\text{L1 Hit Latency}) + (\\text{L1 Miss Rate}) \\times (\\text{L1 Miss Penalty})\n$$\nThe L1 hit rate is $(1 - m_1)$. The L1 miss penalty is the additional time required on a miss. A more direct formulation is:\n$$\nE[T] = (1 - m_1) \\cdot t_{L1} + m_1 \\cdot (\\text{average latency for an L1 miss})\n$$\n\n**Design B: No Cache-to-Cache Forwarding**\nFor Design B, every L1 miss is served by the LLC or main memory, with an average latency of $t_{\\text{LLC}}$.\nThe AMAT for Design B, $E[T_B]$, is:\n$$\nE[T_B] = (1 - m_1) \\cdot t_{L1} + m_1 \\cdot t_{\\text{LLC}}\n$$\n\n**Design A: With Cache-to-Cache Forwarding**\nFor Design A, an L1 miss can be resolved in two ways:\n1.  A cache-to-cache transfer from a peer core, which happens with conditional probability $p_{\\text{owner}}$ and takes $t_{\\text{c2c}}$ cycles.\n2.  A fetch from the LLC/main memory, which happens with conditional probability $(1 - p_{\\text{owner}})$ and takes $t_{\\text{LLC}}$ cycles.\n\nThe average latency for an L1 miss in Design A is the weighted average of these two outcomes:\n$$\n\\text{Avg Miss Latency}_A = p_{\\text{owner}} \\cdot t_{\\text{c2c}} + (1 - p_{\\text{owner}}) \\cdot t_{\\text{LLC}}\n$$\nThe AMAT for Design A, $E[T_A]$, is:\n$$\nE[T_A] = (1 - m_1) \\cdot t_{L1} + m_1 \\cdot (p_{\\text{owner}} \\cdot t_{\\text{c2c}} + (1 - p_{\\text{owner}}) \\cdot t_{\\text{LLC}})\n$$\n\n**Latency Reduction**\nThe latency reduction is $\\Delta T = E[T_B] - E[T_A]$.\n$$\n\\Delta T = \\left[ (1 - m_1) t_{L1} + m_1 t_{\\text{LLC}} \\right] - \\left[ (1 - m_1) t_{L1} + m_1 (p_{\\text{owner}} t_{\\text{c2c}} + (1 - p_{\\text{owner}}) t_{\\text{LLC}}) \\right]\n$$\nThe L1 hit contribution, $(1 - m_1) t_{L1}$, cancels out.\n$$\n\\Delta T = m_1 t_{\\text{LLC}} - m_1 (p_{\\text{owner}} t_{\\text{c2c}} + t_{\\text{LLC}} - p_{\\text{owner}} t_{\\text{LLC}})\n$$\n$$\n\\Delta T = m_1 t_{\\text{LLC}} - m_1 p_{\\text{owner}} t_{\\text{c2c}} - m_1 t_{\\text{LLC}} + m_1 p_{\\text{owner}} t_{\\text{LLC}}\n$$\nThe $m_1 t_{\\text{LLC}}$ terms cancel, leaving:\n$$\n\\Delta T = m_1 p_{\\text{owner}} t_{\\text{LLC}} - m_1 p_{\\text{owner}} t_{\\text{c2c}}\n$$\nFactoring out the common terms gives the final symbolic expression:\n$$\n\\Delta T = m_1 \\cdot p_{\\text{owner}} \\cdot (t_{\\text{LLC}} - t_{\\text{c2c}})\n$$\nThis result is intuitive: the total saving is the probability that an access benefits from the optimization ($m_1 \\cdot p_{\\text{owner}}$) multiplied by the amount of time saved in that case ($t_{\\text{LLC}} - t_{\\text{c2c}}$).\n\n**Numerical Calculation**\nSubstitute the given values:\n$m_1 = 0.08$\n$p_{\\text{owner}} = 0.35$\n$t_{\\text{LLC}} = 140$ cycles\n$t_{\\text{c2c}} = 80$ cycles\n\n$$\n\\Delta T = 0.08 \\times 0.35 \\times (140 - 80)\n$$\n$$\n\\Delta T = 0.08 \\times 0.35 \\times 60\n$$\n$$\n\\Delta T = 0.028 \\times 60 = 1.68\n$$\nThe latency reduction is $1.68$ cycles per access. Rounding to $4$ significant figures gives $1.680$.", "answer": "$$\\boxed{1.680}$$", "id": "3675527"}, {"introduction": "Performance issues in shared-memory systems are not always due to threads intentionally accessing the same data (\"true sharing\"). This exercise introduces the subtle but critical problem of \"false sharing,\" where independent variables happen to reside on the same cache line, causing unnecessary and expensive coherence traffic. By modeling how data is laid out across cache lines, you will gain a practical, quantitative understanding of how data structures must be designed to avoid this common performance pitfall in parallel programs [@problem_id:3675600].", "problem": "Consider a shared-memory multiprocessor with private caches and a write-invalidate cache coherence protocol. The dense result matrix $C$ of a sparse matrix multiplication is stored in row-major order. Let $C$ have $M$ rows and $N$ columns, with each element occupying $s$ bytes. The base address of $C$ is taken as $\\text{base} = 0$ for simplicity. The address of element $(i,j)$ is defined by $\\mathrm{addr}(i,j) = s \\cdot (iN + j)$, and an element maps to cache line index $$\\ell(i,j; L_c) = \\left\\lfloor \\frac{s \\cdot (iN + j)}{L_c} \\right\\rfloor,$$ where $L_c$ is the cache line size in bytes. Assume that multiple threads concurrently write to distinct elements of $C$, as induced by the nonzero distribution of the sparse multiplication. Even though threads write to different words, false sharing occurs when their words reside in the same cache line, prompting coherence invalidations.\n\nWe define the false-sharing cost for a given cache line size $L_c$ as follows. For each cache line index $\\ell$, let $k_{\\ell}$ be the number of distinct threads that write at least one element mapping to $\\ell$. The per-line false-sharing event count is $\\max(0, k_{\\ell} - 1)$, since with a write-invalidate protocol at least $k_{\\ell} - 1$ inter-thread ownership transfers are required for writers to gain exclusive access. The total false-sharing cost is $F(L_c) = \\sum_{\\ell} \\max(0, k_{\\ell} - 1)$. Your program must, for each test case, compute $F(L_c)$ for each candidate $L_c$, select the $L_c$ that minimizes $F(L_c)$, and return both the chosen $L_c$ and the minimal cost. In case of a tie, select the smallest $L_c$ among those with minimal $F(L_c)$.\n\nAll units for $L_c$ and $s$ are in bytes.\n\nUse the following test suite. In all cases, threads write to distinct elements (no true sharing). The writes are specified by integer pairs $(i,j)$ (row index and column index), and for each test case the candidate cache line sizes are given as a set in bytes.\n\n- Test case $1$:\n  - $M = 4$, $N = 16$, $s = 8$, $T = 3$, candidates $\\{32, 64, 128\\}$.\n  - Thread $0$ writes $\\{(0,0),(0,1),(0,2),(0,3),(1,8),(1,9),(1,10),(1,11)\\}$.\n  - Thread $1$ writes $\\{(0,4),(0,5),(0,6),(0,7),(2,0),(2,1)\\}$.\n  - Thread $2$ writes $\\{(1,0),(1,1),(1,2),(1,3),(2,8),(2,9)\\}$.\n- Test case $2$:\n  - $M = 3$, $N = 2$, $s = 8$, $T = 3$, candidates $\\{16, 32, 64\\}$.\n  - Thread $0$ writes $\\{(0,0),(0,1)\\}$.\n  - Thread $1$ writes $\\{(1,0),(1,1)\\}$.\n  - Thread $2$ writes $\\{(2,0),(2,1)\\}$.\n- Test case $3$:\n  - $M = 1$, $N = 16$, $s = 8$, $T = 4$, candidates $\\{64, 128\\}$.\n  - Thread $0$ writes $\\{(0,0),(0,1),(0,2),(0,3)\\}$.\n  - Thread $1$ writes $\\{(0,4),(0,5),(0,6),(0,7)\\}$.\n  - Thread $2$ writes $\\{(0,8),(0,9),(0,10),(0,11)\\}$.\n  - Thread $3$ writes $\\{(0,12),(0,13),(0,14),(0,15)\\}$.\n- Test case $4$:\n  - $M = 4$, $N = 16$, $s = 8$, $T = 3$, candidates $\\{32, 64, 128\\}$.\n  - Thread $0$ writes $\\{(0,0),(0,8)\\}$.\n  - Thread $1$ writes $\\{(1,0)\\}$.\n  - Thread $2$ writes $\\{(2,0)\\}$.\n\nYour task is to implement a program that, for each test case, computes the optimal cache line size $L_c$ (in bytes) that minimizes the total false-sharing cost $F(L_c)$, along with the minimal cost. The final output format must aggregate the results of all provided test cases into a single line, containing a comma-separated list enclosed in square brackets, where for each test case you output the chosen $L_c$ followed by the corresponding minimal $F(L_c)$ as integers. For example, the format is $[L_{c,1},F(L_{c,1}),L_{c,2},F(L_{c,2}),\\dots]$ with no spaces.", "solution": "The problem requires us to calculate a \"false-sharing cost\" for different cache line sizes and find the size that minimizes this cost. The solution involves iterating through each test case, and for each test case, iterating through all candidate cache line sizes ($L_c$).\n\nThe algorithm for a single test case is as follows:\n1.  Initialize `min_cost` to a very large number and `optimal_lc` to an invalid value.\n2.  For each candidate `Lc` in the provided set:\n    a. Create a data structure, like a hash map or dictionary, to map each cache line index to the set of threads that write to it. Let's call this `line_writers`.\n    b. For each thread `t` from $0$ to $T-1$:\n        i. For each write coordinate `(i, j)` performed by thread `t`:\n            - Calculate the memory address: `addr = s * (i * N + j)`.\n            - Calculate the cache line index: `line_idx = addr // Lc`.\n            - Add thread `t` to the set of writers for `line_idx` in `line_writers`.\n    c. After processing all writes, calculate the total cost for the current `Lc`. Initialize `current_cost = 0`.\n    d. For each `line_idx` in `line_writers`:\n        - Get the number of unique threads that wrote to this line: `k = len(line_writers[line_idx])`.\n        - If `k > 1`, add `k - 1` to `current_cost`.\n    e. Compare `current_cost` with `min_cost`.\n        - If `current_cost  min_cost`, update `min_cost = current_cost` and `optimal_lc = Lc`.\n        - If `current_cost == min_cost`, update `optimal_lc = min(optimal_lc, Lc)` to handle the tie-breaking rule.\n3.  After checking all candidate `Lc` values, the pair (`optimal_lc`, `min_cost`) is the result for the test case.\n\nThis process is repeated for all four test cases, and the results are aggregated into the required string format.\n\nFor example, let's manually verify Test Case 2 with $L_c = 32$:\n- $M=3, N=2, s=8$. Addresses: $\\mathrm{addr}(i,j) = 8 \\cdot (2i + j)$.\n- Thread 0 writes to addr(0,0)=0 and addr(0,1)=8. Both map to line index `0 // 32 = 0`.\n- Thread 1 writes to addr(1,0)=16 and addr(1,1)=24. Both map to line index `16 // 32 = 0`.\n- Thread 2 writes to addr(2,0)=32 and addr(2,1)=40. Both map to line index `32 // 32 = 1`.\n- `line_writers` will be: `{0: {Thread 0, Thread 1}, 1: {Thread 2}}`.\n- Cost for line 0: $k_0 = 2$, cost = $2 - 1 = 1$.\n- Cost for line 1: $k_1 = 1$, cost = $1 - 1 = 0$.\n- Total cost $F(32) = 1 + 0 = 1$.\n\nBy performing this calculation for all candidates in all test cases, we can determine the optimal values. A Python script is provided in the answer block to perform these calculations automatically.", "answer": "```python\nimport collections\n\ndef solve():\n    test_cases = [\n        {\n            \"params\": {\"M\": 4, \"N\": 16, \"s\": 8, \"T\": 3},\n            \"candidates\": [32, 64, 128],\n            \"writes\": [\n                [(0,0),(0,1),(0,2),(0,3),(1,8),(1,9),(1,10),(1,11)],\n                [(0,4),(0,5),(0,6),(0,7),(2,0),(2,1)],\n                [(1,0),(1,1),(1,2),(1,3),(2,8),(2,9)],\n            ]\n        },\n        {\n            \"params\": {\"M\": 3, \"N\": 2, \"s\": 8, \"T\": 3},\n            \"candidates\": [16, 32, 64],\n            \"writes\": [\n                [(0,0),(0,1)],\n                [(1,0),(1,1)],\n                [(2,0),(2,1)],\n            ]\n        },\n        {\n            \"params\": {\"M\": 1, \"N\": 16, \"s\": 8, \"T\": 4},\n            \"candidates\": [64, 128],\n            \"writes\": [\n                [(0,0),(0,1),(0,2),(0,3)],\n                [(0,4),(0,5),(0,6),(0,7)],\n                [(0,8),(0,9),(0,10),(0,11)],\n                [(0,12),(0,13),(0,14),(0,15)],\n            ]\n        },\n        {\n            \"params\": {\"M\": 4, \"N\": 16, \"s\": 8, \"T\": 3},\n            \"candidates\": [32, 64, 128],\n            \"writes\": [\n                [(0,0),(0,8)],\n                [(1,0)],\n                [(2,0)],\n            ]\n        }\n    ]\n\n    final_results = []\n\n    for case in test_cases:\n        params = case[\"params\"]\n        M, N, s = params[\"M\"], params[\"N\"], params[\"s\"]\n        \n        optimal_lc = -1\n        min_cost = float('inf')\n\n        for Lc in sorted(case[\"candidates\"]):\n            line_writers = collections.defaultdict(set)\n            \n            for thread_id, thread_writes in enumerate(case[\"writes\"]):\n                for i, j in thread_writes:\n                    addr = s * (i * N + j)\n                    line_idx = addr // Lc\n                    line_writers[line_idx].add(thread_id)\n            \n            current_cost = 0\n            for line_idx in line_writers:\n                k = len(line_writers[line_idx])\n                if k > 1:\n                    current_cost += k - 1\n            \n            if current_cost  min_cost:\n                min_cost = current_cost\n                optimal_lc = Lc\n            elif current_cost == min_cost:\n                # Tie-breaking rule: choose smallest Lc\n                optimal_lc = min(optimal_lc, Lc)\n                \n        final_results.extend([optimal_lc, min_cost])\n\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "3675600"}]}