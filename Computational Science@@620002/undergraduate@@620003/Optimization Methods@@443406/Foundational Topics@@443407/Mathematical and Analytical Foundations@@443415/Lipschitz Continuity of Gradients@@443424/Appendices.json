{"hands_on_practices": [{"introduction": "A cornerstone of analyzing gradient-based optimization algorithms is determining the function's smoothness, quantified by the Lipschitz constant $L$ of its gradient. This exercise provides fundamental practice in this skill, guiding you through the process of computing a global Lipschitz constant from first principles. By analyzing a non-convex function that saturates at large values [@problem_id:3144663], you will apply multivariable calculus to bound the spectral norm of the Hessian, a powerful and widely-used technique for establishing $L$-smoothness.", "problem": "Consider the function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ defined by $f(x)=\\frac{\\|x\\|_{2}^{2}}{1+\\|x\\|_{2}^{2}}$, where $\\|x\\|_{2}$ denotes the Euclidean norm. Starting from the definition of a Lipschitz continuous gradient with respect to the Euclidean norm, use core multivariable calculus facts to determine a global Lipschitz constant $L$ such that the gradient $\\nabla f$ satisfies $ \\|\\nabla f(x)-\\nabla f(y)\\|_{2}\\leq L\\,\\|x-y\\|_{2}$ for all $x,y\\in\\mathbb{R}^{n}$. Your derivation must be based on first principles and must be scientifically consistent. As part of your reasoning, explain how the saturating behavior of $f$ for large $\\|x\\|_{2}$ influences the bound you obtain for $L$. Provide the final value of $L$ as a single exact number. No rounding is required, and no units are involved.", "solution": "We begin with the definition: a function has a Lipschitz continuous gradient with respect to the Euclidean norm if there exists a constant $L\\geq 0$ such that for all $x,y\\in\\mathbb{R}^{n}$,\n$$\n\\|\\nabla f(x)-\\nabla f(y)\\|_{2}\\leq L\\,\\|x-y\\|_{2}.\n$$\nA well-tested fact from multivariable calculus and the mean value theorem for vector-valued functions is that, when $f$ is twice continuously differentiable and its Hessian $\\nabla^{2}f(x)$ exists for all $x$, a sufficient (and in many smooth settings, tight) global Lipschitz constant is given by the uniform bound on the spectral norm of the Hessian:\n$$\nL=\\sup_{x\\in\\mathbb{R}^{n}}\\|\\nabla^{2}f(x)\\|_{2},\n$$\nwhere $\\|\\cdot\\|_{2}$ denotes the operator norm induced by the Euclidean norm. To compute such a bound, we will derive the gradient and Hessian of $f$ and then evaluate the largest absolute eigenvalue of $\\nabla^{2}f(x)$ uniformly over $x$.\n\nLet $s=\\|x\\|_{2}^{2}=x^{\\top}x$. Define $g(s)=\\frac{s}{1+s}$. Then $f(x)=g(s)$ as a composition of $g$ with $s(x)$. By the chain rule and the known gradient $\\nabla s(x)=2x$, we have\n$$\ng'(s)=\\frac{(1+s)-s}{(1+s)^{2}}=\\frac{1}{(1+s)^{2}},\n\\quad\\text{so}\\quad\n\\nabla f(x)=g'(s)\\,\\nabla s(x)=\\frac{2x}{(1+s)^{2}}.\n$$\nTo find the Hessian, we differentiate $\\nabla f(x)$ again using the chain rule and product rule. Let $a(s) = \\frac{2}{(1+s)^2}$.\n$$\n\\nabla^{2}f(x) = \\nabla \\left( a(s) x \\right) = a(s) I + x (\\nabla a(s))^T\n$$\nwhere $\\nabla a(s) = \\frac{da}{ds} \\nabla s(x)$. We compute $\\frac{da}{ds} = -4(1+s)^{-3}$. So, $\\nabla a(s) = -4(1+s)^{-3} (2x) = -\\frac{8x}{(1+s)^3}$.\nTherefore,\n$$\n\\nabla^{2}f(x)=\\frac{2}{(1+s)^{2}}\\,I \\;-\\; \\frac{8}{(1+s)^{3}}\\,x x^{\\top}.\n$$\nThis is a symmetric matrix. To find its spectral norm, we find its eigenvalues. The matrix is a rank-one update to a scaled identity matrix.\n- For any vector $v$ orthogonal to $x$, we have $(\\nabla^2 f(x))v = \\frac{2}{(1+s)^2}v$. This gives an eigenvalue $\\lambda_{\\perp}=\\frac{2}{(1+s)^{2}}$ with multiplicity $n-1$.\n- For the vector $x$ itself, $(\\nabla^2 f(x))x = \\frac{2}{(1+s)^2}x - \\frac{8}{(1+s)^3}(x x^T)x = \\left(\\frac{2}{(1+s)^2} - \\frac{8s}{(1+s)^3}\\right)x$. This gives the eigenvalue\n$$\n\\lambda_{\\parallel}=\\frac{2(1+s) - 8s}{(1+s)^{3}} = \\frac{2 - 6s}{(1+s)^{3}}.\n$$\nThe spectral norm $\\|\\nabla^{2}f(x)\\|_{2}$ is the largest absolute value among the eigenvalues. Therefore,\n$$\n\\|\\nabla^{2}f(x)\\|_{2}=\\max\\!\\left\\{\\left|\\lambda_{\\perp}\\right|,\\left|\\lambda_{\\parallel}\\right|\\right\\}=\\max\\!\\left\\{\\frac{2}{(1+s)^{2}},\\;\\frac{|2-6s|}{(1+s)^{3}}\\right\\}, \\quad s=\\|x\\|_{2}^{2}\\ge 0.\n$$\nWe now maximize this expression over $s\\ge 0$.\n\nFirst, consider the term $\\lambda_{\\perp}(s)=\\frac{2}{(1+s)^{2}}$. It is a decreasing function of $s$ for $s \\ge 0$. Its maximum is at $s=0$:\n$$\n\\max_{s\\ge 0}\\lambda_{\\perp}(s)=\\lambda_{\\perp}(0)=\\frac{2}{(1+0)^{2}}=2.\n$$\nNext, consider $\\left|\\lambda_{\\parallel}(s)\\right|=\\frac{|2-6s|}{(1+s)^{3}}$. We analyze its maximum. At $s=0$, the value is $|2|/1^3=2$. Let's analyze the function $h(s) = \\frac{6s-2}{(1+s)^3}$ for $s \\ge 1/3$ (where $6s-2 \\ge 0$). Its derivative is $h'(s) = \\frac{6(1+s)^3 - 3(1+s)^2(6s-2)}{(1+s)^6} = \\frac{12-12s}{(1+s)^4}$. This is positive for $s \\in [1/3, 1)$ and negative for $s>1$. The maximum occurs at $s=1$, where the value is $h(1)=\\frac{6-2}{(1+1)^3} = \\frac{4}{8}=0.5$. For $s \\in [0, 1/3]$, the expression is $\\frac{2-6s}{(1+s)^3}$, which is a decreasing function (its derivative is $\\frac{12(s-1)}{(1+s)^4}  0$), so its maximum is at $s=0$, which is $2$.\nCombining the cases, the maximum value of $|\\lambda_\\parallel(s)|$ over all $s \\ge 0$ is 2.\n\nTherefore, the supremum of the spectral norm of the Hessian is\n$$\n\\sup_{x\\in\\mathbb{R}^{n}}\\|\\nabla^{2}f(x)\\|_{2}=\\sup_{s\\ge 0}\\max\\!\\left\\{\\frac{2}{(1+s)^{2}},\\;\\frac{|2-6s|}{(1+s)^{3}}\\right\\}=\\max\\{2, 2\\} = 2.\n$$\nBy the property linking the Hessian norm to the gradient's Lipschitz constant, we have $L=2$.\n\nFinally, we discuss the influence of saturation. As $\\|x\\|_{2}\\to\\infty$, the function $f(x)=\\frac{\\|x\\|_{2}^{2}}{1+\\|x\\|_{2}^{2}}$ approaches the saturating value $1$, and the gradient $\\nabla f(x)=\\frac{2x}{(1+\\|x\\|_{2}^{2})^{2}}$ tends to $0$. The Hessian $\\nabla^{2}f(x)$ also decays to the zero matrix in operator norm as $s\\to\\infty$. This means curvature is largest near the origin and diminishes as the function saturates, so the global Lipschitz constant $L$ is determined by the behavior at small $\\|x\\|_{2}$, specifically attaining its maximum at $x=0$ where $\\nabla^{2}f(0)=2I$ and $\\|\\nabla^{2}f(0)\\|_{2}=2$.", "answer": "$$\\boxed{2}$$", "id": "3144663"}, {"introduction": "While the triangle inequality provides a simple upper bound for the smoothness of a sum of functions, it can often be quite loose. This practice explores a crucial insight: when individual functions have high curvature in different, non-overlapping directions, their average can be significantly smoother than any single component. By constructing a simple yet illustrative example with quadratic functions [@problem_id:3144694], you will see how averaging can effectively reduce the overall Lipschitz constant, a principle with profound implications for distributed and large-scale optimization.", "problem": "Consider $L \\in \\mathbb{R}$ with $L0$ and define functions $\\phi_1,\\phi_2:\\mathbb{R}^2 \\to \\mathbb{R}$ by\n$$\n\\phi_1(x) \\;=\\; \\tfrac{1}{2}\\, x^{\\top} A_1 x\n\\quad\\text{and}\\quad\n\\phi_2(x) \\;=\\; \\tfrac{1}{2}\\, x^{\\top} A_2 x,\n$$\nwhere\n$$\nA_1 \\;=\\; \\begin{pmatrix} L  0 \\\\ 0  0 \\end{pmatrix}\n\\quad\\text{and}\\quad\nA_2 \\;=\\; \\begin{pmatrix} 0  0 \\\\ 0  L \\end{pmatrix}.\n$$\nLet $f:\\mathbb{R}^2 \\to \\mathbb{R}$ be the empirical average\n$$\nf(x) \\;=\\; \\tfrac{1}{2}\\big(\\phi_1(x)+\\phi_2(x)\\big).\n$$\nStarting only from the definition of gradient Lipschitz continuity and basic linear algebra facts about symmetric matrices, first justify that each $\\phi_i$ has a gradient that is $L$-Lipschitz with respect to the Euclidean norm, and then determine the smallest constant $L_f$ such that\n$$\n\\|\\nabla f(x)-\\nabla f(y)\\|_2 \\;\\le\\; L_f \\,\\|x-y\\|_2\n\\quad\\text{for all}\\quad x,y \\in \\mathbb{R}^2.\n$$\nProvide your final answer as a closed-form expression in terms of $L$. No rounding is required, and no units are involved. The answer must be a single expression.", "solution": "For a quadratic function $\\phi(x) = \\frac{1}{2}x^\\top A x$ with a symmetric matrix $A$, the gradient is $\\nabla \\phi(x) = Ax$ and the Hessian is $\\nabla^2 \\phi(x) = A$. The smallest Lipschitz constant for the gradient is the spectral norm of the constant Hessian, $\\|A\\|_2$, which for a symmetric matrix is its largest eigenvalue in absolute value.\n\n**Part 1: Lipschitz continuity of gradients of $\\phi_1$ and $\\phi_2$**\n\nFor $\\phi_1(x) = \\frac{1}{2} x^\\top A_1 x$, the Hessian is $A_1 = \\begin{pmatrix} L  0 \\\\ 0  0 \\end{pmatrix}$. The eigenvalues of $A_1$ are $L$ and $0$. Since $L>0$, the spectral norm is $\\|A_1\\|_2 = \\max(|L|, |0|) = L$. Thus, the gradient of $\\phi_1$ is $L$-Lipschitz.\n\nFor $\\phi_2(x) = \\frac{1}{2} x^\\top A_2 x$, the Hessian is $A_2 = \\begin{pmatrix} 0  0 \\\\ 0  L \\end{pmatrix}$. The eigenvalues of $A_2$ are $0$ and $L$. The spectral norm is $\\|A_2\\|_2 = \\max(|0|, |L|) = L$. Thus, the gradient of $\\phi_2$ is also $L$-Lipschitz.\n\n**Part 2: Smallest Lipschitz constant $L_f$ for the gradient of $f$**\n\nThe function $f(x)$ is the average $f(x) = \\frac{1}{2} (\\phi_1(x) + \\phi_2(x))$.\nBy linearity of differentiation, the Hessian of $f$ is the average of the Hessians of $\\phi_1$ and $\\phi_2$:\n$$\n\\nabla^2 f(x) = \\frac{1}{2} (\\nabla^2 \\phi_1(x) + \\nabla^2 \\phi_2(x)) = \\frac{1}{2} (A_1 + A_2).\n$$\nWe compute the matrix sum:\n$$\nA_1 + A_2 = \\begin{pmatrix} L  0 \\\\ 0  0 \\end{pmatrix} + \\begin{pmatrix} 0  0 \\\\ 0  L \\end{pmatrix} = \\begin{pmatrix} L  0 \\\\ 0  L \\end{pmatrix} = L I,\n$$\nwhere $I$ is the $2 \\times 2$ identity matrix.\nThe Hessian of $f$ is therefore:\n$$\n\\nabla^2 f(x) = \\frac{1}{2} (L I) = \\begin{pmatrix} L/2  0 \\\\ 0  L/2 \\end{pmatrix}.\n$$\nThe smallest Lipschitz constant for the gradient of $f$, denoted $L_f$, is the spectral norm of its Hessian matrix.\n$$\nL_f = \\|\\nabla^2 f(x)\\|_2 = \\left\\| \\frac{L}{2} I \\right\\|_2.\n$$\nThe eigenvalues of this diagonal matrix are both $L/2$. Since $L>0$, the spectral norm is the largest absolute eigenvalue:\n$$\nL_f = \\max\\left(\\left|\\frac{L}{2}\\right|, \\left|\\frac{L}{2}\\right|\\right) = \\frac{L}{2}.\n$$\nThus, the smallest Lipschitz constant for the gradient of $f$ is $L/2$.", "answer": "$$\n\\boxed{\\frac{L}{2}}\n$$", "id": "3144694"}, {"introduction": "In practice, algorithms often include heuristics that are not directly covered by standard theory. Gradient clipping is a widely used technique in training deep neural networks to prevent exploding gradients, but how does it affect the convergence guarantees derived from $L$-smoothness? This exercise [@problem_id:3144630] challenges you to rigorously analyze the interplay between a smooth function and a clipped-gradient update, determining which descent properties are preserved and which are lost. Successfully navigating this analysis will sharpen your ability to bridge the gap between optimization theory and practical application, and to debunk common misconceptions.", "problem": "Consider a differentiable function $f:\\mathbb{R}^d\\to\\mathbb{R}$ whose gradient is $L$-Lipschitz continuous (that is, $f$ is $L$-smooth): for all $x,y\\in\\mathbb{R}^d$, $\\|\\nabla f(x)-\\nabla f(y)\\|\\le L\\|x-y\\|$. Define the clipped gradient at $x$ by\n$$\n\\widetilde{g}(x)\\;=\\;\\mathrm{clip}\\big(\\nabla f(x),G\\big)\\;=\\;\\min\\!\\left(1,\\;\\frac{G}{\\|\\nabla f(x)\\|}\\right)\\,\\nabla f(x),\n$$\nwhere $G0$ is a given clipping radius, and consider the clipped-gradient update\n$$\nx_+\\;=\\;x-\\alpha\\,\\widetilde{g}(x),\n$$\nwith step size $\\alpha0$. Which of the following statements about effective smoothness or descent guarantees under clipping are true? Select all that apply.\n\nA. For any $x\\in\\mathbb{R}^d$, the $L$-smoothness of $f$ implies\n$$\nf(x_+)\\;\\le\\;f(x)\\;-\\;\\alpha\\Big(1-\\tfrac{L\\alpha}{2}\\Big)\\,\\big\\|\\widetilde{g}(x)\\big\\|^2,\n$$\nand hence $f(x_+)\\le f(x)$ whenever $0\\alpha\\tfrac{2}{L}$.\n\nB. The mapping $x\\mapsto \\mathrm{clip}\\big(\\nabla f(x),G\\big)$ is globally Lipschitz with constant at most $L$; that is, for all $x,y$,\n$$\n\\big\\|\\mathrm{clip}\\big(\\nabla f(x),G\\big)-\\mathrm{clip}\\big(\\nabla f(y),G\\big)\\big\\|\\;\\le\\;L\\,\\|x-y\\|.\n$$\n\nC. If $\\|\\nabla f(x)\\|\\ge G$, then\n$$\nf(x_+)\\;\\le\\;f(x)\\;-\\;\\alpha\\,G\\,\\|\\nabla f(x)\\|\\;+\\;\\frac{L\\alpha^2}{2}\\,G^2,\n$$\nand thus $f(x_+)\\le f(x)$ whenever $0\\alpha\\tfrac{2}{L}$.\n\nD. Clipping the gradient reduces the functionâ€™s smoothness constant from $L$ to $G$; in other words, after clipping one can treat $f$ as $G$-smooth.", "solution": "We analyze each statement based on the descent lemma, which follows from $L$-smoothness:\n$$ f(y) \\le f(x) + \\langle \\nabla f(x), y-x \\rangle + \\frac{L}{2} \\|y-x\\|^2 $$\nFor the update $x_+ = x - \\alpha \\widetilde{g}(x)$, we have $y-x = -\\alpha \\widetilde{g}(x)$. Substituting this into the lemma gives:\n$$ f(x_+) \\le f(x) - \\alpha \\langle \\nabla f(x), \\widetilde{g}(x) \\rangle + \\frac{L\\alpha^2}{2} \\|\\widetilde{g}(x)\\|^2 $$\n\n**A. Analysis of the general descent guarantee.**\nWe need to relate $\\langle \\nabla f(x), \\widetilde{g}(x) \\rangle$ to $\\|\\widetilde{g}(x)\\|^2$.\n- If $\\|\\nabla f(x)\\| \\le G$, then $\\widetilde{g}(x) = \\nabla f(x)$. The inequality becomes $f(x_+) \\le f(x) - \\alpha \\|\\nabla f(x)\\|^2 + \\frac{L\\alpha^2}{2} \\|\\nabla f(x)\\|^2$, which is $f(x_+) \\le f(x) - \\alpha(1-\\frac{L\\alpha}{2})\\|\\widetilde{g}(x)\\|^2$. The statement holds.\n- If $\\|\\nabla f(x)\\| > G$, then $\\widetilde{g}(x) = G \\frac{\\nabla f(x)}{\\|\\nabla f(x)\\|}$. This means $\\|\\widetilde{g}(x)\\| = G$ and $\\langle \\nabla f(x), \\widetilde{g}(x) \\rangle = G\\|\\nabla f(x)\\|$. The inequality we want to check is $f(x_+) \\le f(x) - \\alpha(1-\\frac{L\\alpha}{2})G^2$.\nFrom the descent lemma, we have $f(x_+) \\le f(x) - \\alpha G \\|\\nabla f(x)\\| + \\frac{L\\alpha^2}{2} G^2$. Since $\\|\\nabla f(x)\\| > G$, we have $- \\alpha G \\|\\nabla f(x)\\|  - \\alpha G^2$. Therefore, $f(x_+)  f(x) - \\alpha G^2 + \\frac{L\\alpha^2}{2}G^2 = f(x) - \\alpha(1-\\frac{L\\alpha}{2})G^2$. The inequality in the statement holds.\nThe conclusion that $f(x_+) \\le f(x)$ for $0  \\alpha  2/L$ follows because $1 - L\\alpha/2 > 0$. Thus, **A is correct.**\n\n**B. Analysis of the Lipschitz constant of the clipped gradient map.**\nThe mapping $x \\mapsto \\widetilde{g}(x)$ is a composition of two functions: $g_1(x) = \\nabla f(x)$ and $g_2(v) = \\mathrm{clip}(v, G)$.\nThe function $g_1$ is $L$-Lipschitz by definition of $L$-smoothness.\nThe function $g_2(v)$ is the projection of a vector $v$ onto the closed ball of radius $G$. Projection onto a non-empty closed convex set is a non-expansive mapping, which means it is 1-Lipschitz.\nTherefore, for any $v_1, v_2$, we have $\\|g_2(v_1) - g_2(v_2)\\| \\le \\|v_1 - v_2\\|$.\nCombining these, we analyze the composition:\n$$ \\|\\widetilde{g}(x) - \\widetilde{g}(y)\\| = \\|g_2(g_1(x)) - g_2(g_1(y))\\| \\le \\|g_1(x) - g_1(y)\\| = \\|\\nabla f(x) - \\nabla f(y)\\| \\le L\\|x-y\\|. $$\nThis shows that the clipped gradient mapping is indeed $L$-Lipschitz. **B is correct.**\n\n**C. Analysis of the descent guarantee when clipping is active.**\nThe statement assumes $\\|\\nabla f(x)\\| \\ge G$. In this case, as shown in the analysis for A, $\\widetilde{g}(x) = G \\frac{\\nabla f(x)}{\\|\\nabla f(x)\\|}$, $\\|\\widetilde{g}(x)\\| = G$, and $\\langle \\nabla f(x), \\widetilde{g}(x) \\rangle = G\\|\\nabla f(x)\\|$.\nPlugging these directly into the descent lemma gives:\n$$ f(x_+) \\le f(x) - \\alpha G\\|\\nabla f(x)\\| + \\frac{L\\alpha^2}{2} G^2 $$\nThis matches the first part of statement C exactly.\nFor the second part, to guarantee $f(x_+) \\le f(x)$, we need to show that $- \\alpha G\\|\\nabla f(x)\\| + \\frac{L\\alpha^2}{2} G^2 \\le 0$.\nSince $\\|\\nabla f(x)\\| \\ge G$, we have $-\\alpha G \\|\\nabla f(x)\\| \\le -\\alpha G^2$.\nSo, the change in $f$ is bounded by $-\\alpha G^2 + \\frac{L\\alpha^2}{2} G^2 = -\\alpha G^2(1 - \\frac{L\\alpha}{2})$.\nIf $0  \\alpha  2/L$, then $1 - L\\alpha/2 > 0$. Since $\\alpha > 0$ and $G^2>0$, the term $-\\alpha G^2(1 - L\\alpha/2)$ is negative, guaranteeing a decrease in $f$. **C is correct.**\n\n**D. Analysis of the effective smoothness constant.**\nThis statement is a common misconception. Clipping the gradient bounds its *magnitude* by $G$, i.e., $\\|\\widetilde{g}(x)\\| \\le G$. However, the smoothness relates to the gradient's *rate of change*. As shown in the analysis for B, the Lipschitz constant of the clipped gradient map $\\widetilde{g}(x)$ is $L$, not $G$. The function $f$ itself is unchanged, so it remains $L$-smooth. The descent guarantees derived in A and C also depend on $L$, not $G$. For example, the safe step size is determined by $L$ (i.e., $\\alpha  2/L$), not $G$. Therefore, one cannot simply treat the function as $G$-smooth. **D is incorrect.**", "answer": "$$\\boxed{ABC}$$", "id": "3144630"}]}