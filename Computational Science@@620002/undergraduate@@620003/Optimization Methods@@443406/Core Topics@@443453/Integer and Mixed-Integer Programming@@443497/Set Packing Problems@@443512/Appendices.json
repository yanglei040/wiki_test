{"hands_on_practices": [{"introduction": "This exercise challenges you to transform a practical course scheduling scenario into a formal set packing model. By translating requirements like non-overlapping cohorts and equity considerations into mathematical constraints, you will practice the essential skill of modeling real-world problems as integer programs. Solving this small-scale instance [@problem_id:3181267] provides a concrete foundation for understanding how optimization can be applied to resource allocation tasks.", "problem": "Consider the set packing problem for building a course roster of specialized laboratory sequences. Let the ground set of student cohorts be denoted by $U = \\{u_1, u_2, \\dots, u_n\\}$, and let the family of candidate lab sequences be denoted by $S = \\{s_1, s_2, \\dots, s_m\\}$, where each $s_j \\subseteq U$ enumerates the cohorts that would enroll together in sequence $j$. Define binary decision variables $x_j \\in \\{0,1\\}$ indicating whether sequence $j$ is selected. Let $A \\in \\{0,1\\}^{n \\times m}$ be the incidence matrix, with entry $A_{ij} = 1$ if and only if $u_i \\in s_j$, and $A_{ij} = 0$ otherwise. The non-overlap requirement between sequences means each cohort can appear in at most one selected sequence, captured by the linear system $A x \\le \\mathbf{1}$, where $x = (x_1, \\dots, x_m)^\\top$ and $\\mathbf{1}$ is the $n$-dimensional vector of ones. Each sequence $j$ has an educational value $v_j \\in \\mathbb{R}$, collected as $v \\in \\mathbb{R}^m$, and the objective is to maximize $\\sum_{j=1}^m v_j x_j = v^\\top x$.\n\nEquity side constraints are imposed to ensure representation across groups and coverage of underserved cohorts. Let the set of labels for groups be $G = \\{g_1, \\dots, g_p\\}$ and let $B \\in \\{0,1\\}^{p \\times m}$ be the group membership matrix with $B_{k j} = 1$ if sequence $j$ serves group $g_k$, and $B_{k j} = 0$ otherwise. Lower and upper bounds on selected sequences per group are captured by $B x \\ge L$ and $B x \\le U$, respectively, where $L, U \\in \\mathbb{Z}_{\\ge 0}^p$. For underserved cohorts, let $w \\in \\{0,1\\}^n$ be a mask where $w_i = 1$ indicates cohort $u_i$ is underserved; the total coverage of underserved cohorts under selection $x$ is $\\sum_{i=1}^n w_i (A x)_i$, which can be required to meet a lower bound $C_{\\min} \\in \\mathbb{Z}_{\\ge 0}$.\n\nYour task is to write a complete program that, for each provided test case, computes an optimal selection $x$ maximizing $v^\\top x$ subject to $x \\in \\{0,1\\}^m$, $A x \\le \\mathbf{1}$, and the specified equity side constraints. When multiple optimal solutions exist, break ties by the following deterministic rule: among solutions with the same objective value, prefer the one that covers a larger number of cohorts, i.e., maximize $\\sum_{i=1}^n (A x)_i$; if still tied, prefer the solution whose sorted list of selected sequence indices is lexicographically smallest. For each test case, your program should output either the sorted list of selected indices (as a list of integers) or the boolean value $False$ if the constraints are infeasible.\n\nUse the following test suite with $n = 5$ cohorts and $m = 6$ sequences:\n\nDefine the incidence matrix\n$$\nA = \\begin{bmatrix}\n1 & 0 & 0 & 1 & 0 & 0\\\\\n1 & 1 & 0 & 0 & 0 & 1\\\\\n0 & 1 & 0 & 0 & 1 & 0\\\\\n0 & 0 & 1 & 0 & 0 & 1\\\\\n0 & 0 & 0 & 1 & 1 & 0\n\\end{bmatrix}.\n$$\n\nDefine the educational values\n$$\nv = \\begin{bmatrix}\n8 & 7 & 4 & 6 & 9 & 5\n\\end{bmatrix}.\n$$\n\nDefine two groups $G = \\{g_{\\mathrm{day}}, g_{\\mathrm{evening}}\\}$ and the group membership matrix\n$$\nB = \\begin{bmatrix}\n1 & 0 & 1 & 1 & 1 & 0\\\\\n0 & 1 & 0 & 0 & 0 & 1\n\\end{bmatrix},\n$$\nwhere the first row corresponds to $g_{\\mathrm{day}}$ and the second row corresponds to $g_{\\mathrm{evening}}$.\n\nDefine the underserved cohort mask\n$$\nw = \\begin{bmatrix}\n0 & 0 & 0 & 1 & 0\n\\end{bmatrix},\n$$\nwhich marks cohort $u_4$ as underserved.\n\nThe three test cases are:\n\n- Test case $1$ (baseline set packing): no equity side constraints, i.e., $B x \\ge L$ with $L = [0, 0]^\\top$, $B x \\le U$ with $U = [\\infty, \\infty]^\\top$ (no upper bounds), and underserved coverage lower bound $C_{\\min} = 0$.\n- Test case $2$ (minimum evening representation and underserved coverage): require at least one selected evening sequence, $B x \\ge L$ with $L = [0, 1]^\\top$, no upper bounds, and underserved coverage lower bound $C_{\\min} = 1$.\n- Test case $3$ (infeasible evening quota): require at least two selected evening sequences, $B x \\ge L$ with $L = [0, 2]^\\top$, no upper bounds, and underserved coverage lower bound $C_{\\min} = 0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case’s result formatted as either a list of integers without spaces (e.g., $[0,2,4]$) or the boolean $False$. For example, the overall output format must be like $[[i_1,i_2,\\dots], [j_1,j_2,\\dots], False]$ for three test cases. No physical units or angles are involved in this problem, and values should be reported as integers or booleans exactly as specified.", "solution": "The posed problem is a constrained set packing problem, which falls into the category of Integer Linear Programming (ILP). The goal is to select a subset of laboratory sequences, represented by a binary decision vector $x \\in \\{0,1\\}^m$, to maximize a total value function while adhering to several constraints. The problem is well-defined, mathematically sound, and provides all necessary data for its resolution.\n\nThe core of the problem is to solve the following ILP for three different sets of constraints:\n-   **Maximize**: $v^\\top x$\n-   **Subject to**:\n    1.  $x_j \\in \\{0, 1\\}$ for $j = 0, \\dots, m-1$\n    2.  $A x \\le \\mathbf{1}$ (Set packing constraint)\n    3.  $B x \\ge L$ (Group lower bounds)\n    4.  $B x \\le U$ (Group upper bounds, effectively absent in this problem)\n    5.  $(w^\\top A) x \\ge C_{\\min}$ (Underserved coverage constraint)\n\nA unique solution is guaranteed by a three-level tie-breaking rule:\n1.  Primary objective: Maximize the educational value $O_1 = v^\\top x = \\sum_{j=0}^{m-1} v_j x_j$.\n2.  Secondary objective: Maximize the total number of cohorts covered, $O_2 = \\sum_{i=0}^{n-1} (A x)_i$. This can be written as $(\\mathbf{1}^\\top A) x$.\n3.  Tertiary objective: Choose the solution whose sorted list of selected sequence indices is lexicographically smallest.\n\nGiven the small number of sequences ($m=6$), the search space of possible solutions has a size of $2^6 = 64$. This is small enough to permit an exhaustive search (brute-force enumeration) over all possible binary vectors $x$. This method guarantees finding the globally optimal solution according to the hierarchical objectives.\n\nThe solution procedure is as follows:\n1.  Iterate through every possible binary vector $x$ of length $m=6$.\n2.  For each $x$, verify if it is a feasible solution by checking all active constraints for the given test case.\n3.  A solution is feasible if it satisfies $A x \\le \\mathbf{1}$, $B x \\ge L$, and $(w^\\top A) x \\ge C_{\\min}$.\n4.  All feasible solutions are collected. If none are found, the problem is infeasible.\n5.  For each feasible solution, the primary objective value $O_1$ and the secondary objective value $O_2$ are calculated.\n6.  The set of feasible solutions is then sorted based on the hierarchical criteria: first in descending order of $O_1$, then in descending order of $O_2$, and finally in ascending lexicographical order of the sorted list of selected indices.\n7.  The top entry in this sorted list is the optimal solution.\n\nThis procedure is applied to each of the three test cases.\n\n**Test Case 1**: Baseline set packing. The constraints are $L=[0, 0]^\\top$ and $C_{\\min}=0$. These are trivial and are satisfied by any selection. The only meaningful constraint is the packing constraint $A x \\le \\mathbf{1}$. An exhaustive search reveals that the selection of sequences $\\{s_0, s_2, s_4\\}$ corresponding to $x=[1,0,1,0,1,0]^\\top$ yields the unique maximum value $O_1 = v_0 + v_2 + v_4 = 8 + 4 + 9 = 21$. The sorted indices are $[0,2,4]$.\n\n**Test Case 2**: Minimum evening representation and underserved coverage. The constraints are $L=[0, 1]^\\top$ and $C_{\\min}=1$. These impose two key conditions:\n-   From $B x \\ge L$, the second row implies $\\sum_{j=0}^{m-1} B_{1j}x_j = x_1 + x_5 \\ge 1$.\n-   From $(w^\\top A)x \\ge C_{\\min}$, we get $x_2 + x_5 \\ge 1$.\nA feasible solution must therefore satisfy ($x_1=1$ or $x_5=1$) and ($x_2=1$ or $x_5=1$). We search for the best packing under these conditions. The optimal feasible solution is found to be the selection of sequences $\\{s_1, s_2, s_3\\}$, corresponding to $x=[0,1,1,1,0,0]^\\top$. This solution gives a value of $O_1 = v_1 + v_2 + v_3 = 7 + 4 + 6 = 17$. No other feasible solution achieves a higher value. The sorted indices are $[1,2,3]$.\n\n**Test Case 3**: Infeasible evening quota. The constraint is $L=[0, 2]^\\top$, which implies $x_1 + x_5 \\ge 2$. Since $x_1$ and $x_5$ are binary, this forces $x_1=1$ and $x_5=1$. However, sequences $s_1$ and $s_5$ share cohort $u_1$ (since $A_{1,1}=1$ and $A_{1,5}=1$). Selecting both violates the non-overlap constraint $A x \\le \\mathbf{1}$ for the row corresponding to cohort $u_1$, which is $x_0 + x_1 + x_5 \\le 1$. Substituting $x_1=1, x_5=1$ yields the contradiction $1+1 \\le 1$. Thus, no feasible solution exists. The result is `False`.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import product\n\ndef solve():\n    \"\"\"\n    Solves the set packing problem for three test cases using brute-force enumeration.\n    \"\"\"\n    # Define problem data based on the provided statement.\n    # n=5 cohorts, m=6 sequences\n    A = np.array([\n        [1, 0, 0, 1, 0, 0],\n        [1, 1, 0, 0, 0, 1],\n        [0, 1, 0, 0, 1, 0],\n        [0, 0, 1, 0, 0, 1],\n        [0, 0, 0, 1, 1, 0]\n    ])\n    v = np.array([8, 7, 4, 6, 9, 5])\n    B = np.array([\n        [1, 0, 1, 1, 1, 0],\n        [0, 1, 0, 0, 0, 1]\n    ])\n    w = np.array([0, 0, 0, 1, 0])\n\n    # Pre-calculate vectors for objectives and constraints to optimize the inner loop.\n    # Vector for calculating total cohort coverage (Secondary Objective O2)\n    cohort_counts_per_seq = np.ones(A.shape[0]) @ A\n    \n    # Vector for calculating underserved cohort coverage\n    underserved_coverage_per_seq = w @ A\n\n    # Define the three test cases as specified in the problem.\n    test_cases = [\n        # Test case 1: Baseline set packing\n        {'L': np.array([0, 0]), 'C_min': 0},\n        # Test case 2: Minimum evening representation and underserved coverage\n        {'L': np.array([0, 1]), 'C_min': 1},\n        # Test case 3: Infeasible evening quota\n        {'L': np.array([0, 2]), 'C_min': 0}\n    ]\n\n    results = []\n    \n    # Process each test case\n    for case in test_cases:\n        L = case['L']\n        C_min = case['C_min']\n        \n        feasible_solutions = []\n        \n        # Enumerate all 2^m possible selections using itertools.product\n        m = A.shape[1]\n        for x_tuple in product([0, 1], repeat=m):\n            x = np.array(x_tuple)\n            \n            # 1. Check non-overlap constraint: A*x <= 1\n            if not np.all(A @ x <= 1):\n                continue\n            \n            # 2. Check group quota constraint: B*x >= L\n            if not np.all(B @ x >= L):\n                continue\n            \n            # 3. Check underserved coverage constraint: (w^T * A) * x >= C_min\n            if not (underserved_coverage_per_seq @ x >= C_min):\n                continue\n            \n            # If all constraints pass, the solution is feasible.\n            # Calculate objective values for tie-breaking.\n            obj1_value = v @ x\n            obj2_cohorts = cohort_counts_per_seq @ x\n            indices = sorted([i for i, val in enumerate(x) if val == 1])\n            \n            feasible_solutions.append({\n                'obj1': obj1_value,\n                'obj2': obj2_cohorts,\n                'indices': indices\n            })\n\n        if not feasible_solutions:\n            # If no feasible solutions were found, the problem is infeasible.\n            results.append(False)\n        else:\n            # Sort solutions based on the hierarchical tie-breaking rule.\n            # Python's list.sort is stable, so we can sort by each key in reverse order of priority.\n            \n            # Tertiary: Lexicographically smallest list of indices (ascending)\n            feasible_solutions.sort(key=lambda s: s['indices'])\n            # Secondary: Number of cohorts covered (descending)\n            feasible_solutions.sort(key=lambda s: s['obj2'], reverse=True)\n            # Primary: Educational value (descending)\n            feasible_solutions.sort(key=lambda s: s['obj1'], reverse=True)\n            \n            # The first element after sorting is the optimal solution.\n            best_solution = feasible_solutions[0]\n            results.append(best_solution['indices'])\n            \n    # Format the final output string according to the specification.\n    # e.g., [[0,2,4],[1,2,3],False]\n    def format_result(item):\n        if isinstance(item, list):\n            return f\"[{','.join(map(str, item))}]\"\n        return str(item)\n\n    print(f\"[{','.join(map(format_result, results))}]\")\n\nsolve()\n\n```", "id": "3181267"}, {"introduction": "While finding the absolute best solution to a set packing problem is computationally difficult, simple greedy algorithms often provide good solutions quickly. This practice [@problem_id:3181347] delves into the limits of such heuristics by asking you to design an adversarial instance where a greedy-by-weight approach performs poorly. By analyzing the approximation ratio, you will gain a deeper appreciation for the nuances of algorithmic performance and the trade-offs between solution quality and computational speed.", "problem": "Consider the weighted set packing problem: given a finite ground set $U$ and a family $\\mathcal{F} \\subseteq 2^{U}$ of subsets with nonnegative weights $\\{w(S)\\}_{S \\in \\mathcal{F}}$, the goal is to select a subfamily $\\mathcal{P} \\subseteq \\mathcal{F}$ of pairwise-disjoint sets that maximizes $\\sum_{S \\in \\mathcal{P}} w(S)$. A simple greedy-by-weight algorithm iteratively selects a remaining set $S \\in \\mathcal{F}$ of maximum weight and deletes all sets that intersect $S$, repeating until no sets remain.\n\nDesign and analyze an adversarial instance, restricted to sets of sizes $2$ and $3$, that causes the greedy-by-weight algorithm to select a suboptimal solution. Your instance should be composed of $m \\in \\mathbb{N}$ identical gadgets, each gadget $i \\in \\{1, \\dots, m\\}$ built on six distinct elements $\\{a_{i}, b_{i}, x_{i}, y_{i}, u_{i}, v_{i}\\}$ with three sets:\n- a $2$-set $S_{i}^{(2)} = \\{a_{i}, b_{i}\\}$ of weight $w(S_{i}^{(2)}) = w + \\varepsilon$,\n- two disjoint $3$-sets $S_{i,1}^{(3)} = \\{a_{i}, x_{i}, y_{i}\\}$ and $S_{i,2}^{(3)} = \\{b_{i}, u_{i}, v_{i}\\}$, each of weight $w(S_{i,1}^{(3)}) = w(S_{i,2}^{(3)}) = w$,\nwhere $w > 0$ and $\\varepsilon \\in (0, w)$, and gadgets are disjoint across different indices $i$.\n\nStarting from the core definitions of set packing and the greedy-by-weight procedure, rigorously justify that the greedy-by-weight algorithm applied to your instance selects the $m$ sets $\\{S_{i}^{(2)}\\}_{i=1}^{m}$ and that an optimal solution selects the $2m$ sets $\\{S_{i,1}^{(3)}, S_{i,2}^{(3)}\\}_{i=1}^{m}$. Then compute the approximation ratio\n$$\\rho(m, w, \\varepsilon) = \\frac{\\text{total weight of greedy solution}}{\\text{total weight of optimal solution}}$$\nas a function of $m$, $w$, and $\\varepsilon$, and take the limit as $\\varepsilon \\to 0^{+}$ to obtain the limiting worst-case value induced by this adversarial construction. Finally, interpret how this limiting value compares to what one would expect when generalizing the same construction idea to $k$-set packing, where each set has size at most $k$.\n\nReport the limiting worst-case approximation ratio as a single exact number. No rounding is required. No physical units are involved.", "solution": "The problem statement is a valid exercise in the analysis of approximation algorithms for NP-hard optimization problems. It is well-posed, scientifically grounded in the theory of computer science, and all terms and conditions are precisely defined. We may therefore proceed with a rigorous solution.\n\nThe problem asks for an analysis of a specific adversarial instance for the weighted set packing problem under a greedy-by-weight heuristic. The instance is composed of $m \\in \\mathbb{N}$ identical and disjoint gadgets.\n\nFirst, let us formalize the problem instance. The ground set is $U = \\bigcup_{i=1}^{m} U_{i}$, where each $U_i = \\{a_{i}, b_{i}, x_{i}, y_{i}, u_{i}, v_{i}\\}$ is the set of elements for gadget $i$. Since the gadgets are disjoint, we have $|U| = 6m$. The family of sets is $\\mathcal{F} = \\bigcup_{i=1}^{m} \\mathcal{F}_i$, where $\\mathcal{F}_i = \\{S_{i}^{(2)}, S_{i,1}^{(3)}, S_{i,2}^{(3)}\\}$. The sets and their weights are given as:\n- $S_{i}^{(2)} = \\{a_{i}, b_{i}\\}$, with weight $w(S_{i}^{(2)}) = w + \\varepsilon$.\n- $S_{i,1}^{(3)} = \\{a_{i}, x_{i}, y_{i}\\}$, with weight $w(S_{i,1}^{(3)}) = w$.\n- $S_{i,2}^{(3)} = \\{b_{i}, u_{i}, v_{i}\\}$, with weight $w(S_{i,2}^{(3)}) = w$.\n\nThe parameters are constrained by $w > 0$ and $\\varepsilon \\in (0, w)$.\n\nWe begin by analyzing the behavior of the greedy-by-weight algorithm on this instance $\\mathcal{F}$. The algorithm iteratively selects a set with the maximum weight among all currently available sets.\nAt the initial step, the set of available sets is $\\mathcal{F}$. The weights of the sets in $\\mathcal{F}$ are either $w$ or $w+\\varepsilon$. Since $\\varepsilon > 0$, the maximum weight is $w+\\varepsilon$. This weight is associated with the $m$ sets of the form $S_{i}^{(2)}$.\n\nThe algorithm will select one of these maximum-weight sets. Let us assume, without loss of generality, that it selects $S_{1}^{(2)}$ in the first iteration. The weight of this set is $w(S_{1}^{(2)}) = w + \\varepsilon$. Upon selecting $S_{1}^{(2)}$, the algorithm removes $S_{1}^{(2)}$ itself and all other sets in $\\mathcal{F}$ that have a non-empty intersection with $S_{1}^{(2)} = \\{a_1, b_1\\}$. Let us identify these sets:\n- $S_{1,1}^{(3)} = \\{a_1, x_1, y_1\\}$ intersects $S_{1}^{(2)}$ because $a_1 \\in S_{1}^{(2)} \\cap S_{1,1}^{(3)}$.\n- $S_{1,2}^{(3)} = \\{b_1, u_1, v_1\\}$ intersects $S_{1}^{(2)}$ because $b_1 \\in S_{1}^{(2)} \\cap S_{1,2}^{(3)}$.\n\nBy construction, sets from different gadgets $i \\neq j$ are disjoint. Therefore, for $j \\neq 1$, $S_{j}^{(2)}$, $S_{j,1}^{(3)}$, and $S_{j,2}^{(3)}$ do not intersect $S_{1}^{(2)}$.\nAfter the first iteration, the set of remaining available sets is $\\mathcal{F}' = \\bigcup_{i=2}^{m} \\mathcal{F}_i$. The problem is now reduced to the same initial structure but with $m-1$ gadgets.\n\nBy induction, the greedy algorithm will proceed to select $S_{2}^{(2)}$, then $S_{3}^{(2)}$, and so on, until $S_{m}^{(2)}$. At each step $i$, selecting $S_{i}^{(2)}$ causes the removal of $S_{i,1}^{(3)}$ and $S_{i,2}^{(3)}$. The final solution produced by the greedy algorithm is the set of pairwise-disjoint sets $\\mathcal{P}_{G} = \\{S_{1}^{(2)}, S_{2}^{(2)}, \\dots, S_{m}^{(2)}\\}$.\nThe total weight of this greedy solution, $W_G$, is the sum of the weights of the selected sets:\n$$W_G = \\sum_{i=1}^{m} w(S_{i}^{(2)}) = \\sum_{i=1}^{m} (w + \\varepsilon) = m(w + \\varepsilon)$$\n\nNext, we must determine the optimal solution. An optimal solution is a subfamily $\\mathcal{P}_{OPT} \\subseteq \\mathcal{F}$ of pairwise-disjoint sets that maximizes total weight. Since the gadgets are disjoint, the overall optimization problem decouples into $m$ independent subproblems, one for each gadget. The total optimal weight is $m$ times the optimal weight for a single gadget.\nLet's analyze a single-gadget subproblem on $\\mathcal{F}_i$. The sets are $S_{i}^{(2)}$, $S_{i,1}^{(3)}$, and $S_{i,2}^{(3)}$. We seek a maximum-weight packing from these three sets.\nThe possible maximal packings for gadget $i$ are:\n1. $\\{S_{i}^{(2)}\\}$: We cannot include $S_{i,1}^{(3)}$ or $S_{i,2}^{(3)}$ as they both intersect $S_{i}^{(2)}$. The weight of this packing is $w(S_{i}^{(2)}) = w + \\varepsilon$.\n2. $\\{S_{i,1}^{(3)}, S_{i,2}^{(3)}\\}$: These two sets are disjoint, as $S_{i,1}^{(3)} \\cap S_{i,2}^{(3)} = \\{a_{i}, x_{i}, y_{i}\\} \\cap \\{b_{i}, u_{i}, v_{i}\\} = \\emptyset$. The weight of this packing is $w(S_{i,1}^{(3)}) + w(S_{i,2}^{(3)}) = w + w = 2w$.\n\nTo find the optimal solution for the gadget, we compare the weights of these two packings. We are given that $\\varepsilon \\in (0, w)$, which implies $\\varepsilon < w$. Therefore, $w + \\varepsilon < w + w = 2w$.\nThe optimal packing for a single gadget $i$ is $\\{S_{i,1}^{(3)}, S_{i,2}^{(3)}\\}$ with a total weight of $2w$.\nThe overall optimal solution is the union of the optimal solutions for each gadget:\n$\\mathcal{P}_{OPT} = \\bigcup_{i=1}^{m} \\{S_{i,1}^{(3)}, S_{i,2}^{(3)}\\}$.\nThe total weight of the optimal solution, $W_{OPT}$, is:\n$$W_{OPT} = \\sum_{i=1}^{m} (w(S_{i,1}^{(3)}) + w(S_{i,2}^{(3)})) = \\sum_{i=1}^{m} (w + w) = 2mw$$\n\nNow, we can compute the approximation ratio $\\rho(m, w, \\varepsilon)$ as the ratio of the greedy solution's weight to the optimal solution's weight:\n$$\\rho(m, w, \\varepsilon) = \\frac{W_G}{W_{OPT}} = \\frac{m(w + \\varepsilon)}{2mw} = \\frac{w + \\varepsilon}{2w} = \\frac{1}{2} + \\frac{\\varepsilon}{2w}$$\nAs a check, we note that the ratio is independent of $m$, as expected from the repetitive structure of the instance.\n\nThe problem requires the limiting worst-case value of this ratio as $\\varepsilon \\to 0^{+}$.\n$$\\lim_{\\varepsilon \\to 0^{+}} \\rho(m, w, \\varepsilon) = \\lim_{\\varepsilon \\to 0^{+}} \\left(\\frac{1}{2} + \\frac{\\varepsilon}{2w}\\right)$$\nSince $w > 0$ is a fixed positive constant, the term $\\frac{\\varepsilon}{2w}$ approaches $0$ as $\\varepsilon \\to 0^{+}$.\n$$\\lim_{\\varepsilon \\to 0^{+}} \\rho(m, w, \\varepsilon) = \\frac{1}{2} + 0 = \\frac{1}{2}$$\n\nFinally, we interpret this result. The instance is an example of $k$-set packing where the maximum set size is $k=3$. The greedy-by-weight algorithm is tempted by a set $S_i^{(2)}$ of size $2$ with a slightly higher weight $w+\\varepsilon$. Selecting this \"bait\" set prevents the selection of two disjoint sets, $S_{i,1}^{(3)}$ and $S_{i,2}^{(3)}$, each of size $3$ and weight $w$. The optimal solution for the gadget consists of these two sets, with total weight $2w$. The limiting ratio of greedy-to-optimal weight is $w/(2w) = 1/2$.\nThis construction is a canonical example used to establish lower bounds on the approximation ratio of greedy algorithms. For a generalized $k$-set packing problem (maximum set size $k$), a similar gadget can be constructed. One would use a \"bait\" set of size $k-1$ with weight $w+\\varepsilon$ that intersects $k-1$ disjoint sets of size $k$, each with weight $w$. The greedy algorithm would choose the bait set for a total weight of $w+\\varepsilon$. The optimal solution would consist of the $k-1$ larger sets, for a total weight of $(k-1)w$. The resulting approximation ratio would approach $\\frac{w}{(k-1)w} = \\frac{1}{k-1}$ as $\\varepsilon \\to 0^{+}$.\nIn our problem, $k=3$, so the generalized formula predicts a worst-case ratio of $\\frac{1}{3-1} = \\frac{1}{2}$. Our calculated limiting value matches this expectation perfectly.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "3181347"}, {"introduction": "Many real-world set packing problems, like the cutting stock problem, involve an astronomical number of potential variables (patterns), making standard LP methods infeasible. This advanced exercise [@problem_id:3181317] introduces column generation, a powerful technique to solve the linear programming relaxation of such massive models. You will derive and implement an algorithm where the master problem and a \"pricing\" subproblem—in this case, a knapsack problem—work together to find the optimal solution without ever enumerating all variables.", "problem": "Consider a cutting stock variant cast as a set packing model. There is a finite set of items indexed by $j \\in \\{1,\\dots,n\\}$, each item $j$ has an integer length $l_j \\in \\mathbb{Z}_{\\ge 0}$ and a profit $c_j \\in \\mathbb{R}$. A single resource (a bin or stock roll) has a fixed integer capacity $W \\in \\mathbb{Z}_{\\ge 0}$. A pattern $s$ is any subset of items $S \\subseteq \\{1,\\dots,n\\}$ whose total length respects the capacity, that is, $\\sum_{j \\in S} l_j \\le W$. Let $v_s = \\sum_{j \\in S} c_j$ denote the profit of pattern $s$. The set packing relaxation introduces one decision variable $x_s \\ge 0$ for each feasible pattern $s$, and enforces that no item appears in more than one selected pattern by the constraints $\\sum_{s: j \\in s} x_s \\le 1$ for every item $j$. The objective is to maximize the total profit $\\sum_s v_s x_s$.\n\nStarting only from the fundamental definition of the set packing relaxation and the standard linear programming duality, derive why the column generation pricing subproblem for this model is equivalent to a $0$-$1$ knapsack problem on the items. Specifically, show how the reduced cost of a pattern is constructed from the dual variables of the current restricted master problem, and why the pattern with maximum positive reduced cost can be found by selecting a subset of items whose total length does not exceed $W$ to maximize an adjusted sum of item values.\n\nThen, implement a column generation algorithm that:\n- Initializes with no patterns in the restricted master problem.\n- Iteratively solves the dual of the current restricted master problem to obtain dual variables for the item constraints.\n- Solves the pricing subproblem as a $0$-$1$ knapsack to find a single new pattern with strictly positive reduced cost. If none exists, terminates.\n- After termination, solves the final restricted master problem (the primal form) to obtain the optimal objective value of the relaxation.\n\nYour implementation must adhere to the following requirements:\n- Treat all lengths $l_j$ and capacity $W$ as integers, and solve the pricing subproblem using a dynamic programming algorithm for the $0$-$1$ knapsack.\n- Solve linear programs using a standard method compliant with linear programming duality.\n- Use a stopping tolerance for reduced cost of $\\varepsilon = 10^{-9}$.\n- When the restricted master has no columns, define the dual variables as the zero vector and proceed to pricing.\n\nTest Suite:\nProvide outputs for the following three instances, stated in purely mathematical terms:\n1. Case A: $W = 10$, item lengths $[2,3,5,7]$, item profits $[6,5,9,8]$.\n2. Case B: $W = 9$, item lengths $[2,3,4]$, item profits $[5,5,5]$.\n3. Case C: $W = 10$, item lengths $[11,2,9]$, item profits $[100,4,7]$.\n\nAnswer Specification:\n- For each test case, return the optimal objective value of the final restricted master problem (linear relaxation), rounded to six decimals.\n- Your program should produce a single line of output containing the three results as a comma-separated list enclosed in square brackets, for example, $\\texttt{[v_A,v_B,v_C]}$, where $v_A$, $v_B$, and $v_C$ are the rounded objective values corresponding to Cases A, B, and C, respectively.", "solution": "The task is to derive the pricing subproblem for a set packing formulation of a cutting stock problem and then implement a column generation algorithm to solve its linear relaxation.\n\n### Derivation of the Pricing Subproblem\n\nLet $\\mathcal{P}$ denote the set of all feasible patterns. A pattern $s$ is a subset of items, which can be represented by a binary vector $a_s \\in \\{0, 1\\}^n$, where the $j$-th component $(a_s)_j = 1$ if item $j$ is in pattern $s$, and $0$ otherwise. The feasibility condition for a pattern is $\\sum_{j=1}^n l_j (a_s)_j \\le W$. The profit of pattern $s$ is $v_s = \\sum_{j=1}^n c_j (a_s)_j$.\n\nThe full linear programming relaxation, also known as the Master Problem (MP), is formulated as follows:\n$$\n\\begin{align*}\n\\text{(Primal MP)} \\quad \\max \\quad & \\sum_{s \\in \\mathcal{P}} v_s x_s \\\\\n\\text{s.t.} \\quad & \\sum_{s \\in \\mathcal{P}} (a_s)_j x_s \\le 1, \\quad \\forall j \\in \\{1, \\dots, n\\} \\\\\n& x_s \\ge 0, \\quad \\forall s \\in \\mathcal{P}\n\\end{align*}\n$$\nThe constraint $\\sum_{s \\in \\mathcal{P}} (a_s)_j x_s \\le 1$ ensures that the total fraction of patterns containing item $j$ does not exceed $1$, effectively meaning that item $j$ is used at most once across all selected patterns.\n\nThe number of feasible patterns $|\\mathcal{P}|$ can be astronomically large, making it impossible to solve the MP directly by enumerating all variables $x_s$. Column generation is a method to solve such LPs by working with a small, manageable subset of columns (patterns) at a time. This subset of patterns, denoted $\\mathcal{P}' \\subset \\mathcal{P}$, defines the Restricted Master Problem (RMP).\n\nThe Primal RMP is:\n$$\n\\begin{align*}\n\\text{(Primal RMP)} \\quad \\max \\quad & \\sum_{s \\in \\mathcal{P}'} v_s x_s \\\\\n\\text{s.t.} \\quad & \\sum_{s \\in \\mathcal{P}'} (a_s)_j x_s \\le 1, \\quad \\forall j \\in \\{1, \\dots, n\\} \\\\\n& x_s \\ge 0, \\quad \\forall s \\in \\mathcal{P}'\n\\end{align*}\n$$\nBy standard linear programming duality, we can formulate the dual of the RMP. Let $\\pi_j \\ge 0$ be the dual variable associated with the constraint for item $j$.\n\nThe Dual RMP is:\n$$\n\\begin{align*}\n\\text{(Dual RMP)} \\quad \\min \\quad & \\sum_{j=1}^n \\pi_j \\\\\n\\text{s.t.} \\quad & \\sum_{j=1}^n (a_s)_j \\pi_j \\ge v_s, \\quad \\forall s \\in \\mathcal{P}' \\\\\n& \\pi_j \\ge 0, \\quad \\forall j \\in \\{1, \\dots, n\\}\n\\end{align*}\n$$\nAfter solving the current RMP (typically its dual form, to obtain the dual variables $\\pi_j$), we must determine if the current solution is optimal for the full MP. According to the criteria of the simplex method, the current solution is optimal if and only if all non-basic variables have non-positive reduced costs. In the context of column generation, the non-basic variables are the $x_s$ for all patterns $s \\in \\mathcal{P} \\setminus \\mathcal{P}'$.\n\nThe reduced cost, $\\bar{c}_s$, of a variable $x_s$ is given by its objective function coefficient minus the cost of its resource consumption, where the resource costs are given by the optimal dual variables $\\pi^*_j$ of the current RMP. For a pattern $s$, the resource consumption is encoded by the column $a_s$.\n\nThe reduced cost of a pattern $s$ is:\n$$\n\\bar{c}_s = v_s - \\sum_{j=1}^n (a_s)_j \\pi_j^*\n$$\nTo find a variable that could potentially improve the objective function, we must search for a pattern $s$ with a strictly positive reduced cost, $\\bar{c}_s > 0$. The column generation algorithm's pricing subproblem is the task of finding the pattern with the maximum possible reduced cost:\n$$\n\\max_{s \\in \\mathcal{P}} \\bar{c}_s = \\max_{s \\in \\mathcal{P}} \\left( v_s - \\sum_{j=1}^n (a_s)_j \\pi_j^* \\right)\n$$\nSubstituting the definition of the pattern profit $v_s = \\sum_{j=1}^n c_j (a_s)_j$:\n$$\n\\max_{s \\in \\mathcal{P}} \\left( \\sum_{j=1}^n c_j (a_s)_j - \\sum_{j=1}^n \\pi_j^* (a_s)_j \\right) = \\max_{s \\in \\mathcal{P}} \\sum_{j=1}^n (c_j - \\pi_j^*) (a_s)_j\n$$\nThe maximization is over all feasible patterns $s \\in \\mathcal{P}$, which are defined by the constraint $\\sum_{j=1}^n l_j (a_s)_j \\le W$. To operationalize this search, we introduce binary decision variables $y_j \\in \\{0, 1\\}$ for each item $j$, where $y_j=1$ signifies that item $j$ is included in the pattern we are constructing. The pattern vector $a_s$ is thus replaced by the decision vector $y$. The pricing subproblem can now be formally stated as:\n$$\n\\begin{align*}\n\\text{(Pricing Subproblem)} \\quad \\max \\quad & \\sum_{j=1}^n (c_j - \\pi_j^*) y_j \\\\\n\\text{s.t.} \\quad & \\sum_{j=1}^n l_j y_j \\le W \\\\\n& y_j \\in \\{0, 1\\}, \\quad \\forall j \\in \\{1, \\dots, n\\}\n\\end{align*}\n$$\nThis is precisely the definition of the $0$-$1$ knapsack problem. For this knapsack problem:\n-   The \"items\" to place in the knapsack are the original items $j \\in \\{1, \\dots, n\\}$.\n-   The \"weight\" of each knapsack item $j$ is its length $l_j$.\n-   The \"capacity\" of the knapsack is $W$.\n-   The \"value\" of each knapsack item $j$ is its profit adjusted by the current dual price, $c_j - \\pi_j^*$.\n\nIf the optimal objective value of this knapsack problem is greater than $0$ (or a small tolerance $\\varepsilon > 0$ for numerical stability), the pattern defined by the optimal $y^*$ vector is a column with positive reduced cost and is added to the set $\\mathcal{P}'$. The RMP is then re-solved. If the optimal value of the knapsack problem is less than or equal to $0$, no column with a positive reduced cost exists, implying that the current RMP solution is also optimal for the full Master Problem. The algorithm terminates.\n\nSince the problem specifies that the lengths $l_j$ and capacity $W$ are integers, this $0$-$1$ knapsack problem can be efficiently solved using dynamic programming.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef knapsack_dp(capacity, weights, values):\n    \"\"\"\n    Solves the 0-1 knapsack problem using dynamic programming.\n    This version is designed to handle potentially negative values.\n\n    Args:\n        capacity (int): The knapsack capacity.\n        weights (list or np.array): The weights of the items.\n        values (list or np.array): The values of the items.\n\n    Returns:\n        tuple: A tuple containing:\n            - float: The maximum value of the knapsack.\n            - list: A list of indices of the items included in the optimal solution.\n    \"\"\"\n    n = len(weights)\n    # The DP table can be initialized with 0, as not picking any item gives a value of 0.\n    # We are maximizing, so any negative values will be correctly handled.\n    dp = np.zeros((n + 1, capacity + 1))\n\n    for i in range(1, n + 1):\n        weight = weights[i-1]\n        value = values[i-1]\n        for w in range(capacity + 1):\n            # Option 1: Don't include item i-1.\n            dp[i, w] = dp[i-1, w]\n            # Option 2: Include item i-1 if it fits.\n            if w >= weight:\n                dp[i, w] = max(dp[i, w], value + dp[i-1, w - weight])\n\n    max_value = dp[n, capacity]\n\n    # Backtracking to find the items in the optimal set\n    items_included = []\n    w = capacity\n    for i in range(n, 0, -1):\n        # A simple and robust way to backtrack is to check if the value changed\n        # by excluding the current item. If it does, the item must have been\n        # included. Ties are broken by not including the item.\n        if not np.isclose(dp[i, w], dp[i-1, w]):\n            items_included.append(i-1)\n            w -= weights[i-1]\n\n    return max_value, items_included\n\ndef solve_set_packing_cg(W, l, c, tolerance=1e-9, max_iter=100):\n    \"\"\"\n    Solves the set packing relaxation using column generation.\n\n    Args:\n        W (int): The capacity of the resource.\n        l (list): The lengths of the items.\n        c (list): The profits of the items.\n        tolerance (float): The tolerance for checking if reduced cost is positive.\n        max_iter (int): A failsafe limit on iterations.\n\n    Returns:\n        float: The optimal objective value of the LP relaxation.\n    \"\"\"\n    n = len(l)\n    l_np = np.array(l, dtype=int)\n    c_np = np.array(c, dtype=float)\n\n    patterns = []  # List to store pattern vectors (as numpy arrays)\n\n    for _ in range(max_iter):\n        # Step 1: Solve the Dual of the Restricted Master Problem\n        if not patterns:\n            # Per problem statement, if RMP has no columns, duals are the zero vector.\n            pi = np.zeros(n, dtype=float)\n        else:\n            # Construct and solve the Dual RMP\n            num_patterns = len(patterns)\n            pattern_matrix = np.array(patterns) # Shape: (num_patterns, n)\n            pattern_profits = np.array([np.dot(p, c_np) for p in patterns])\n            \n            # Dual RMP: min 1^T * pi, s.t. A^T * pi >= v, pi >= 0\n            # `linprog` form: min c^T * x, s.t. A_ub * x <= b_ub\n            # So, -A^T * pi <= -v\n            c_dual = np.ones(n)\n            A_ub_dual = -pattern_matrix\n            b_ub_dual = -pattern_profits\n\n            # The 'highs' method is robust for LPs.\n            res_dual = linprog(c_dual, A_ub=A_ub_dual, b_ub=b_ub_dual, bounds=(0, None), method='highs')\n            \n            if not res_dual.success:\n                # This should not happen in a well-behaved column generation.\n                # It might indicate numerical instability or an issue with the problem.\n                # We can assume for this problem that the dual is always feasible and bounded.\n                # If it happens, we stop and report the current best solution.\n                break \n\n            pi = res_dual.x\n\n        # Step 2: Solve the Pricing Subproblem (0-1 Knapsack)\n        knapsack_values = c_np - pi\n        \n        # We only need to consider items with non-negative weights for the DP.\n        # The problem states l_j are non-negative, but 0-weight items with\n        # positive value could be problematic, but 0-1 knapsack handles them.\n        # Here all lengths are positive integers or greater than capacity.\n        max_reduced_cost, new_pattern_items = knapsack_dp(W, l_np, knapsack_values)\n\n        # Step 3: Check for Termination\n        if max_reduced_cost <= tolerance:\n            break\n\n        # Step 4: Add New Pattern\n        new_pattern = np.zeros(n, dtype=int)\n        new_pattern[new_pattern_items] = 1\n        \n        # Avoid adding a duplicate pattern to prevent cycling (though unlikely with 'highs')\n        if not any(np.array_equal(new_pattern, p) for p in patterns):\n            patterns.append(new_pattern)\n        else: # If a duplicate is generated, something may be stalled. Stop.\n            break\n\n    # Step 5: Solve the final Primal Restricted Master Problem\n    if not patterns:\n        return 0.0\n\n    num_patterns = len(patterns)\n    pattern_matrix_T = np.array(patterns).T # Shape: (n, num_patterns)\n    pattern_profits = np.array([np.dot(p, c_np) for p in patterns])\n\n    # Primal RMP: max v^T * x, s.t. A * x <= 1, x >= 0\n    # `linprog` form: min -v^T*x\n    c_primal = -pattern_profits\n    A_ub_primal = pattern_matrix_T\n    b_ub_primal = np.ones(n)\n    \n    res_primal = linprog(c_primal, A_ub=A_ub_primal, b_ub=b_ub_primal, bounds=(0, None), method='highs')\n\n    if res_primal.success:\n        return -res_primal.fun\n    else:\n        # Should not fail if the process was successful.\n        return 0.0\n\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case A: W = 10, item lengths [2,3,5,7], item profits [6,5,9,8].\n        {\"W\": 10, \"l\": [2, 3, 5, 7], \"c\": [6, 5, 9, 8]},\n        # Case B: W = 9, item lengths [2,3,4], item profits [5,5,5].\n        {\"W\": 9, \"l\": [2, 3, 4], \"c\": [5, 5, 5]},\n        # Case C: W = 10, item lengths [11,2,9], item profits [100,4,7].\n        {\"W\": 10, \"l\": [11, 2, 9], \"c\": [100, 4, 7]},\n    ]\n\n    results = []\n    for case in test_cases:\n        W, l, c = case[\"W\"], case[\"l\"], case[\"c\"]\n        optimal_value = solve_set_packing_cg(W, l, c)\n        results.append(optimal_value)\n\n    # Format output to six decimal places as specified.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3181317"}]}