{"hands_on_practices": [{"introduction": "This first exercise connects the simulated annealing algorithm directly to its physical inspiration—the process of annealing in magnetic systems. By simulating the two-dimensional ferromagnetic Ising model, you will gain hands-on experience implementing different standard cooling schedules (exponential, linear, and logarithmic). This practice is invaluable as it requires you to compare the results of your simulation against a known analytical solution, bridging the gap between computational heuristics and first-principles physics [@problem_id:2435206].", "problem": "Consider the two-dimensional Ising model on a square lattice of size $N \\times N$ with spins $s_{i,j} \\in \\{-1,+1\\}$ and energy\n$$\nE(\\mathbf{s}) \\;=\\; -J \\sum_{\\langle (i,j),(i',j') \\rangle} s_{i,j}\\, s_{i',j'} \\;-\\; B \\sum_{i=1}^{N} \\sum_{j=1}^{N} s_{i,j},\n$$\nwhere the first sum is over all unordered nearest-neighbor pairs and $B$ is an external field. In this task, take $B=0$ and $J>0$ (ferromagnetic coupling). The system is equipped with either open boundary conditions (no wraparound neighbors) or periodic boundary conditions (wraparound neighbors in both coordinate directions). All bonds are counted once in the energy.\n\nA simulated annealing process is defined as follows. Starting from an initial configuration of spins chosen independently and uniformly from $\\{-1,+1\\}$, at discrete step $k \\in \\{0,1,2,\\dots\\}$ a single site $(i,j)$ is selected uniformly at random and the proposal $s_{i,j} \\mapsto -s_{i,j}$ is made. Let $\\Delta E_k$ denote the change in energy $E$ that would result from this single-spin flip. The proposal is accepted with probability\n$$\nA_k \\;=\\; \\min\\!\\left(1, \\exp\\!\\left(-\\frac{\\Delta E_k}{T_k}\\right)\\right),\n$$\nwhere $T_k$ is the temperature at step $k$. A temperature schedule is a positive sequence $\\{T_k\\}_{k=0}^{K-1}$ of length $K$ and the process for this task uses a fixed number of sweeps per temperature. One sweep is defined as $N^2$ independent single-spin flip proposals at the same temperature. The total number of proposals equals $K$ times the number of sweeps per temperature times $N^2$. For reproducibility, the pseudo-random number generator must be initialized with seed $0$ at the beginning of each run.\n\nFor $B=0$ and $J>0$, determine the analytical ground state energy $E_{\\mathrm{gs}}(N,J,\\mathrm{bc})$ for a given lattice size $N$, coupling $J$, and boundary condition $\\mathrm{bc} \\in \\{\\text{open},\\text{periodic}\\}$ by reasoning from first principles of the model definition. Then, for each specified schedule and parameter set below, run the simulated annealing process described above and record the minimum energy $E_{\\min}$ attained over the entire run. For each test case, compute the absolute difference\n$$\nD \\;=\\; \\big| E_{\\min} \\;-\\; E_{\\mathrm{gs}}(N,J,\\mathrm{bc}) \\big|.\n$$\n\nUse $J=1$ in all cases. Use the following schedules and parameters, where $K$ is the number of temperatures, and each temperature is held constant over the stated number of sweeps per temperature. The schedules are:\n- Exponential: $T_k = T_0 \\,\\alpha^k$ with parameters $T_0>0$ and $\\alpha \\in (0,1)$.\n- Linear: $T_k = \\dfrac{T_0}{1 + c k}$ with parameters $T_0>0$ and $c>0$.\n- Logarithmic: $T_k = \\dfrac{T_0}{\\log(2 + k)}$ with parameter $T_0>0$.\n\nTest suite (each bullet is one independent run):\n- Case $1$: $N=6$, periodic boundary conditions, exponential schedule with $T_0=4.0$, $\\alpha=0.98$, $K=500$, and $1$ sweep per temperature.\n- Case $2$: $N=7$, open boundary conditions, linear schedule with $T_0=5.0$, $c=0.01$, $K=700$, and $1$ sweep per temperature.\n- Case $3$: $N=1$, open boundary conditions, exponential schedule with $T_0=1.0$, $\\alpha=0.5$, $K=5$, and $1$ sweep per temperature.\n- Case $4$: $N=3$, periodic boundary conditions, logarithmic schedule with $T_0=5.0$, $K=600$, and $2$ sweeps per temperature.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases above:\n$$\n[D_1, D_2, D_3, D_4].\n$$\nEach $D_i$ must be a floating-point number. No physical units are involved. Angles are not used. Express any fractional values as decimals.", "solution": "The problem requires the analysis and simulation of a two-dimensional ferromagnetic Ising model on an $N \\times N$ square lattice, subjected to a simulated annealing process. The objective is to compute the absolute difference between the minimum energy found during the simulation, $E_{\\min}$, and the analytically determined ground state energy, $E_{\\mathrm{gs}}$.\n\nFirst, we determine the analytical ground state energy, $E_{\\mathrm{gs}}$. The Hamiltonian of the system is given by\n$$\nE(\\mathbf{s}) = -J \\sum_{\\langle (i,j),(i',j') \\rangle} s_{i,j}\\, s_{i',j'} - B \\sum_{i=1}^{N} \\sum_{j=1}^{N} s_{i,j}\n$$\nThe problem specifies a zero external field, $B=0$, and a positive coupling constant, $J>0$ (ferromagnetic interaction). The energy expression simplifies to\n$$\nE(\\mathbf{s}) = -J \\sum_{\\langle (i,j),(i',j') \\rangle} s_{i,j}\\, s_{i',j'}\n$$\nTo minimize this energy, the sum $\\sum s_{i,j}\\,s_{i',j'}$ must be maximized. Since each spin $s_{i,j}$ can only be $+1$ or $-1$, the product $s_{i,j}\\,s_{i',j'}$ for a pair of adjacent spins is maximized when it equals $+1$, which occurs if $s_{i,j} = s_{i',j'}$. Therefore, the ground state is achieved when all spins are aligned, i.e., all $s_{i,j} = +1$ or all $s_{i,j} = -1$. In this configuration, every term in the summation contributes $+1$. The ground state energy is thus $E_{\\mathrm{gs}} = -J \\times (\\text{total number of nearest-neighbor bonds})$.\n\nThe number of bonds depends on the boundary conditions:\n1.  **Periodic Boundary Conditions**: The lattice is a torus. Each of the $N^2$ sites has four neighbors (two horizontal and two vertical). To count each bond only once, we sum the neighbors for each site and divide by two. The total number of bonds is $\\frac{1}{2} (N^2 \\times 4) = 2N^2$. The ground state energy is\n    $$E_{\\mathrm{gs}}(N,J,\\mathrm{periodic}) = -J \\cdot 2N^2$$\n2.  **Open Boundary Conditions**: The lattice has edges. There are $N$ rows, each with $N-1$ horizontal bonds, giving $N(N-1)$ horizontal bonds. Similarly, there are $N$ columns, each with $N-1$ vertical bonds, giving $N(N-1)$ vertical bonds. The total number of bonds is $N(N-1) + N(N-1) = 2N(N-1)$. The ground state energy is\n    $$E_{\\mathrm{gs}}(N,J,\\mathrm{open}) = -J \\cdot 2N(N-1)$$\n\nNext, we address the simulated annealing process. This is a probabilistic optimization algorithm used to find the global minimum of a function. The process starts from a random spin configuration. At each step, a new state is proposed by flipping a single, randomly chosen spin. The change in energy, $\\Delta E$, resulting from this flip is calculated. A flip from state $s_{i,j}$ to $-s_{i,j}$ at site $(i,j)$ changes the energy by\n$$\n\\Delta E = E_{\\text{new}} - E_{\\text{old}} = \\left(-J \\sum_{k \\in \\text{neigh}} (-s_{i,j})s_k \\right) - \\left(-J \\sum_{k \\in \\text{neigh}} s_{i,j}s_k \\right) = 2J s_{i,j} \\sum_{k \\in \\text{neigh}} s_k\n$$\nwhere the sum is over the spins $s_k$ of the neighbors of site $(i,j)$. This efficient calculation avoids re-computing the total energy of the system at each step.\n\nThe proposed flip is accepted based on the Metropolis criterion with probability $A_k = \\min(1, \\exp(-\\Delta E_k / T_k))$. Here, $T_k$ is the temperature at step $k$. If $\\Delta E_k \\le 0$, the move is always accepted. If $\\Delta E_k > 0$, the move is accepted with a probability that decreases as the temperature $T_k$ decreases.\n\nThe temperature is gradually lowered according to a schedule $\\{T_k\\}_{k=0}^{K-1}$. For each temperature $T_k$ in the schedule, a fixed number of sweeps is performed, where one sweep consists of $N^2$ spin-flip proposals. Throughout the entire simulation, the minimum energy encountered, $E_{\\min}$, is recorded.\n\nThe three temperature schedules specified are:\n-   **Exponential**: $T_k = T_0 \\alpha^k$\n-   **Linear**: $T_k = T_0 / (1 + ck)$\n-   **Logarithmic**: $T_k = T_0 / \\log(2+k)$\n\nThe implementation proceeds by first calculating $E_{\\mathrm{gs}}$ for each test case using the derived formulas with $J=1$. Then, for each case, a separate simulation is run. The pseudo-random number generator is seeded with $0$ for reproducibility. An $N \\times N$ grid of spins is initialized randomly. The simulation then iterates through the $K$ temperatures in the specified schedule. For each temperature, it performs the required number of sweeps, with each sweep comprising $N^2$ Monte Carlo steps (propose, calculate $\\Delta E$, accept/reject). The current energy and minimum energy found so far are updated accordingly. Finally, the absolute difference $D = |E_{\\min} - E_{\\mathrm{gs}}|$ is computed. This procedure is repeated for all four test cases. A special case is $N=1$ with open boundaries, which has no bonds, so its energy is always $0$, irrespective of the spin configuration or the annealing process.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_analytical_gs_energy(N, J, bc):\n    \"\"\"Calculates the analytical ground state energy for the Ising model.\"\"\"\n    if N == 0:\n        return 0.0\n    if bc == 'periodic':\n        # For a periodic lattice, each of N^2 sites has 4 neighbors.\n        # Number of bonds = (N^2 * 4) / 2 = 2 * N^2\n        return -J * 2.0 * N**2\n    elif bc == 'open':\n        # For an open lattice, there are N*(N-1) horizontal and N*(N-1) vertical bonds.\n        # Number of bonds = 2 * N * (N-1)\n        return -J * 2.0 * N * (N - 1)\n    else:\n        raise ValueError(\"Invalid boundary condition specified.\")\n\ndef calculate_total_energy(spins, N, J, bc):\n    \"\"\"Calculates the total energy of the spin configuration. Used for initialization.\"\"\"\n    energy = 0.0\n    for r in range(N):\n        for c in range(N):\n            spin = spins[r, c]\n            if bc == 'periodic':\n                # Sum over right and down neighbors to count each bond once\n                right_neighbor = spins[r, (c + 1) % N]\n                down_neighbor = spins[(r + 1) % N, c]\n                energy += -J * spin * right_neighbor\n                energy += -J * spin * down_neighbor\n            elif bc == 'open':\n                if c < N - 1:\n                    right_neighbor = spins[r, c + 1]\n                    energy += -J * spin * right_neighbor\n                if r < N - 1:\n                    down_neighbor = spins[r + 1, c]\n                    energy += -J * spin * down_neighbor\n    return energy\n\ndef get_neighbor_sum(spins, pos, N, bc):\n    \"\"\"Calculates the sum of spins of the neighbors of a given site.\"\"\"\n    r, c = pos\n    neighbor_sum = 0.0\n    if bc == 'periodic':\n        neighbor_sum += spins[(r - 1) % N, c]\n        neighbor_sum += spins[(r + 1) % N, c]\n        neighbor_sum += spins[r, (c - 1) % N]\n        neighbor_sum += spins[r, (c + 1) % N]\n    elif bc == 'open':\n        if r > 0: neighbor_sum += spins[r - 1, c]\n        if r < N - 1: neighbor_sum += spins[r + 1, c]\n        if c > 0: neighbor_sum += spins[r, c - 1]\n        if c < N - 1: neighbor_sum += spins[r, c + 1]\n    return neighbor_sum\n\ndef run_simulation(params):\n    \"\"\"Runs a single simulated annealing case.\"\"\"\n    N = params['N']\n    J = params['J']\n    bc = params['bc']\n    schedule_type = params['schedule_type']\n    schedule_params = params['schedule_params']\n    K = params['K']\n    sweeps_per_temp = params['sweeps_per_temp']\n\n    # Seed the RNG for reproducibility for each independent run\n    np.random.seed(0)\n\n    # Initialize spin configuration\n    spins = np.random.choice([-1, 1], size=(N, N)).astype(float)\n    \n    if N == 0:\n        return 0.0\n    \n    current_energy = calculate_total_energy(spins, N, J, bc)\n    min_energy = current_energy\n\n    # Generate temperature schedule\n    temps = np.zeros(K)\n    T0 = schedule_params.get('T0')\n    if schedule_type == 'exponential':\n        alpha = schedule_params['alpha']\n        for k in range(K):\n            temps[k] = T0 * (alpha**k)\n    elif schedule_type == 'linear':\n        c = schedule_params['c']\n        for k in range(K):\n            temps[k] = T0 / (1.0 + c * k)\n    elif schedule_type == 'logarithmic':\n        for k in range(K):\n            temps[k] = T0 / np.log(2.0 + k)\n    \n    steps_per_temp = sweeps_per_temp * N**2\n\n    # Simulated Annealing process\n    for k in range(K):\n        T_k = temps[k]\n        if T_k == 0: continue # Avoid division by zero\n        for _ in range(steps_per_temp):\n            # Propose a flip\n            r, c = np.random.randint(N), np.random.randint(N)\n            \n            # Calculate energy change\n            s_ij = spins[r, c]\n            sum_neighbors = get_neighbor_sum(spins, (r, c), N, bc)\n            delta_E = 2.0 * J * s_ij * sum_neighbors\n\n            # Metropolis acceptance criterion\n            if delta_E < 0 or np.random.rand() < np.exp(-delta_E / T_k):\n                spins[r, c] *= -1\n                current_energy += delta_E\n                if current_energy < min_energy:\n                    min_energy = current_energy\n    \n    return min_energy\n\ndef solve():\n    \"\"\"Main solver function to run all test cases and print results.\"\"\"\n    J_val = 1.0\n\n    test_cases = [\n        {'N': 6, 'bc': 'periodic', 'schedule_type': 'exponential', 'schedule_params': {'T0': 4.0, 'alpha': 0.98}, 'K': 500, 'sweeps_per_temp': 1},\n        {'N': 7, 'bc': 'open', 'schedule_type': 'linear', 'schedule_params': {'T0': 5.0, 'c': 0.01}, 'K': 700, 'sweeps_per_temp': 1},\n        {'N': 1, 'bc': 'open', 'schedule_type': 'exponential', 'schedule_params': {'T0': 1.0, 'alpha': 0.5}, 'K': 5, 'sweeps_per_temp': 1},\n        {'N': 3, 'bc': 'periodic', 'schedule_type': 'logarithmic', 'schedule_params': {'T0': 5.0}, 'K': 600, 'sweeps_per_temp': 2},\n    ]\n\n    results = []\n    for case in test_cases:\n        case['J'] = J_val\n        E_gs = calculate_analytical_gs_energy(case['N'], case['J'], case['bc'])\n        E_min = run_simulation(case)\n        D = abs(E_min - E_gs)\n        results.append(D)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2435206"}, {"introduction": "Real-world optimization problems often feature \"deceptive\" landscapes where many poor-quality local optima can trap an algorithm far from the true global solution. This practice challenges you to apply simulated annealing to a Quadratic Unconstrained Binary Optimization (QUBO) problem specifically designed to be deceptive. By systematically testing various cooling schedules, you will investigate their effectiveness in navigating such a landscape, deepening your understanding of the critical balance between exploration (high temperature) and exploitation (low temperature) [@problem_id:2435212].", "problem": "You are asked to implement and analyze Simulated Annealing (SA) acceptance schedules on a deliberately deceptive instance of a Quadratic Unconstrained Binary Optimization (QUBO) problem. Your program must be a complete, runnable implementation that evaluates several acceptance schedules on the same problem instance and reports success rates. The goal is to reason from first principles about why different acceptance schedules behave differently on landscapes with widely separated local and global optima.\n\nFundamental base to use:\n- The Metropolis acceptance rule arises from the Boltzmann distribution in statistical mechanics, where the probability of a configuration $x$ of energy $E(x)$ at temperature $T$ is proportional to $\\exp(-E(x)/T)$. The Metropolis rule accepts an energy-increasing move with probability $\\exp(-\\Delta E/T)$, where $\\Delta E$ is the energy difference between a proposed state and the current state. If $\\Delta E \\le 0$, the move is accepted deterministically.\n- A Markov Chain with detailed balance and an ergodic proposal kernel admits the Boltzmann distribution as a stationary distribution at fixed temperature $T$. As $T$ is decreased slowly enough, the chain concentrates near low-energy states.\n- The theoretical logarithmic cooling rate guarantees convergence in the limit under idealized conditions. Practical schedules include geometric and linear cooling, which balance exploration and exploitation under finite budgets.\n\nProblem definition:\n- Consider a QUBO with binary vector $x \\in \\{0,1\\}^n$ and symmetric matrix $Q \\in \\mathbb{R}^{n \\times n}$. The energy is $E(x) = x^\\top Q x$.\n- Use the following specific construction with dimension $n = 16$:\n  - Let $c = 1.0$ and $w = 0.2$.\n  - Define $Q_{ii} = c$ for all $i \\in \\{1,\\dots,n\\}$.\n  - Define $Q_{ij} = -\\tfrac{w}{2}$ for all $i \\ne j$.\n  - This yields $E(x) = c \\sum_{i=1}^n x_i - w \\sum_{1 \\le i &lt; j \\le n} x_i x_j$, a dense, mean-field QUBO that is known to be NP-hard to optimize in general. The landscape is deceptive in that it contains distant basins: for the given parameters, $x = \\mathbf{0}$ is a local minimum under one-bit flips and $x = \\mathbf{1}$ is the global minimum, with Hamming distance $n$ between them.\n- Initialization: For each independent SA run, draw the initial state $x$ uniformly at random from $\\{0,1\\}^n$.\n- Neighborhood: At each iteration, propose to flip one bit chosen uniformly from $\\{1,\\dots,n\\}$.\n- Acceptance: Use the Metropolis rule with temperature $T$: accept if $\\Delta E \\le 0$, otherwise accept with probability $\\exp(-\\Delta E/T)$.\n\nAcceptance schedules to implement:\n- Geometric (also called exponential): $T_k = T_0 \\,\\alpha^k$ with $0 &lt; \\alpha &lt; 1$.\n- Linear: $T_k = \\max\\{T_{\\min}, T_0 \\,(1 - k/K)\\}$ for iteration index $k \\in \\{0,\\dots,K-1\\}$ and total budget $K$.\n- Logarithmic: $T_k = \\dfrac{T_0}{\\log(b + k + 1)}$, where $\\log$ denotes the natural logarithm.\n\nSuccess criterion:\n- A run is counted as a success if it reaches the global minimum energy exactly at least once during its $K$ iterations. For this problem instance, you must compute the true global minimum energy exactly by exhaustive enumeration over all $2^n$ binary vectors $x$ before running SA. You may use any correct method to evaluate $E(x)$ for each $x$; for example, computing $E(x) = x^\\top Q x$ directly.\n\nTest suite:\nImplement the following four test cases. In each case, run $R$ independent SA runs with the schedule and parameters listed, using the specified pseudorandom seed to initialize the random number generator. Angles are not involved; there are no physical units. Report the fraction of successful runs as a decimal number for each case.\n\n- Case $1$ (geometric, sufficiently slow cooling):\n  - Schedule: geometric with $T_0 = 2.0$, $\\alpha = 0.995$.\n  - Iterations: $K = 6000$.\n  - Runs: $R = 64$.\n  - Seed: $123$.\n- Case $2$ (linear, too-rapid cooling):\n  - Schedule: linear with $T_0 = 2.0$, $T_{\\min} = 0.001$.\n  - Iterations: $K = 1500$.\n  - Runs: $R = 64$.\n  - Seed: $456$.\n- Case $3$ (logarithmic, very slow cooling):\n  - Schedule: logarithmic with $T_0 = 4.0$, $b = 2.0$.\n  - Iterations: $K = 6000$.\n  - Runs: $R = 64$.\n  - Seed: $789$.\n- Case $4$ (geometric, cold start):\n  - Schedule: geometric with $T_0 = 0.05$, $\\alpha = 0.99$.\n  - Iterations: $K = 6000$.\n  - Runs: $R = 64$.\n  - Seed: $13579$.\n\nRequired outputs:\n- For each case, compute the fraction of runs (a real number in $[0,1]$) that reached the global minimum energy exactly at least once.\n- Your program should produce a single line of output containing the four results as a comma-separated list enclosed in square brackets, with each fraction rounded to exactly three digits after the decimal point, in the order of Cases $1,2,3,4$ (for example, $\\texttt{[0.734,0.000,0.578,0.016]}$).\n\nImplementation constraints:\n- Use the Metropolis acceptance rule exactly as stated.\n- Use the neighborhood and initialization as stated.\n- Ensure reproducibility by seeding your pseudorandom number generator with the specified seed per test case; internal splitting across runs is up to you but must be deterministic.\n- Your solution must be self-contained and must not require any input. It must run within a reasonable time on a typical modern computer.", "solution": "The problem statement presented is subjected to rigorous validation and is found to be valid. It is scientifically grounded in the principles of statistical mechanics and computational optimization, specifically concerning the Simulated Annealing (SA) metaheuristic for solving Quadratic Unconstrained Binary Optimization (QUBO) problems. The problem is well-posed, with all parameters, constraints, and objectives clearly and objectively defined. It is a self-contained and numerically feasible task. Therefore, a reasoned solution will be provided.\n\nThe objective is to implement and analyze the performance of three different Simulated Annealing cooling schedules—geometric, linear, and logarithmic—on a specifically constructed deceptive QUBO problem instance. The goal is to understand from first principles how the choice of schedule impacts the ability of the algorithm to locate the global optimum on an energy landscape with a prominent local minimum far from the global minimum.\n\nFirst, we formalize the energy landscape. The problem is defined on a binary vector $x \\in \\{0, 1\\}^n$ with dimension $n=16$. The energy function is given by $E(x) = x^\\top Q x$, where $Q$ is a symmetric $n \\times n$ matrix. The matrix $Q$ is constructed with diagonal elements $Q_{ii} = c = 1.0$ and off-diagonal elements $Q_{ij} = -w/2 = -0.1$ for $i \\neq j$. For binary variables where $x_i^2 = x_i$, the energy function can be written as:\n$$\nE(x) = \\sum_{i=1}^n \\sum_{j=1}^n Q_{ij} x_i x_j = \\sum_{i=1}^n Q_{ii} x_i^2 + \\sum_{i \\neq j} Q_{ij} x_i x_j = c \\sum_{i=1}^n x_i - \\frac{w}{2} \\sum_{i \\neq j} x_i x_j\n$$\nGiven the symmetry of the sum, $\\sum_{i \\neq j} x_i x_j = 2 \\sum_{1 \\le i < j \\le n} x_i x_j$, this is equivalent to the provided expression:\n$$\nE(x) = c \\sum_{i=1}^n x_i - w \\sum_{1 \\le i < j \\le n} x_i x_j\n$$\nThe energy of a state $x$ depends only on the number of non-zero elements, let's call this Hamming weight $k = \\sum_{i=1}^n x_i$. The number of pairs $(i, j)$ with $i < j$ where both $x_i=1$ and $x_j=1$ is $\\binom{k}{2}$. Thus, the energy is a function of $k$:\n$$\nE(k) = c k - w \\frac{k(k-1)}{2}\n$$\nSubstituting the given parameters $n=16$, $c=1.0$, and $w=0.2$, we get:\n$$\nE(k) = 1.0 \\cdot k - 0.2 \\frac{k(k-1)}{2} = k - 0.1(k^2 - k) = -0.1 k^2 + 1.1 k\n$$\nThis is a downward-opening parabola in $k$. To find the minimum energy for an integer $k \\in [0, 16]$, we evaluate the function at its boundaries.\nFor $k=0$ (the state $x=\\mathbf{0}$), the energy is $E(0) = 0$.\nFor $k=16$ (the state $x=\\mathbf{1}$), the energy is $E(16) = -0.1(16^2) + 1.1(16) = -25.6 + 17.6 = -8.0$.\nThe problem states that $x=\\mathbf{0}$ is a local minimum under single-bit flips. Let's verify this. Flipping one bit from $x=\\mathbf{0}$ corresponds to changing from $k=0$ to $k=1$. The energy becomes $E(1) = -0.1(1)^2 + 1.1(1) = 1.0$. Since $E(1) > E(0)$, any single-bit flip from $x=\\mathbf{0}$ increases energy, confirming it is a local minimum. The analysis confirms that the state $x=\\mathbf{1}$ is the global minimum with energy $E_{min} = -8.0$, and $x=\\mathbf{0}$ is a local minimum with energy $E=0$. These two states are separated by the maximum possible Hamming distance of $n=16$, creating a \"deceptive\" landscape that is challenging for local search algorithms. An exhaustive search over all $2^{16}$ states is required to rigorously confirm this global minimum, which will be performed by the implementation.\n\nThe Simulated Annealing algorithm proceeds as follows. Starting from a random initial state $x$ and an initial temperature $T_0$, it iteratively proposes a new state $x'$ by flipping a single, randomly chosen bit. The change in energy, $\\Delta E = E(x') - E(x)$, is calculated. The move is accepted if $\\Delta E \\le 0$. If $\\Delta E > 0$, the move is accepted with a probability given by the Metropolis criterion, $P(\\text{accept}) = \\exp(-\\Delta E / T_k)$, where $T_k$ is the temperature at iteration $k$. The temperature is gradually lowered according to a cooling schedule. An efficient calculation of $\\Delta E$ is critical. If bit $j$ is flipped from its current state $x_j$ to $1-x_j$, the change in energy is:\n$$\n\\Delta E = ( (1-x_j) - x_j ) \\left[ c - w \\sum_{i \\neq j} x_i \\right]\n$$\nThis $O(n)$ calculation avoids the full $O(n^2)$ recalculation of the energy.\n\nWe now analyze the four specified test cases based on these principles.\nCase $1$: Geometric cooling ($T_0=2.0, \\alpha=0.995, K=6000, R=64$).\nThe initial temperature $T_0=2.0$ is sufficiently high to allow barrier-crossing; for instance, a move with $\\Delta E=1.0$ is accepted with probability $\\exp(-1.0/2.0) \\approx 0.61$. The cooling factor $\\alpha=0.995$ is close to $1$, resulting in a slow exponential decay of temperature. The number of iterations $K=6000$ is large. This combination provides a long exploratory phase at high temperatures to escape initial local minima, followed by a gradual exploitation phase as the temperature drops, allowing convergence towards the global minimum. This schedule is expected to be effective and yield a high success rate.\n\nCase $2$: Linear cooling ($T_0=2.0, T_{min}=0.001, K=1500, R=64$).\nThe initial temperature is the same as in Case $1$, but the iteration budget $K=1500$ is much smaller, and the temperature decreases linearly. The cooling is rapid, a scheme often called \"quenching\". The system will quickly lose the thermal energy needed to surmount large energy barriers. It is highly probable that the search will become trapped in the basin of attraction of the first local minimum it encounters, which depends on the random initial state. Given the deceptive landscape, the probability of finding the distant global minimum is very low. This schedule is expected to perform poorly.\n\nCase $3$: Logarithmic cooling ($T_0=4.0, b=2.0, K=6000, R=64$).\nThis schedule, $T_k = T_0 / \\log(b+k+1)$, is known to guarantee convergence to the global optimum in the limit of infinite iterations ($K \\to \\infty$). However, in a finite run, its behavior is defined by how quickly it cools. The cooling is exceptionally slow. The initial temperature is $T_0' = 4.0/\\log(3) \\approx 3.64$. At the final iteration $k=5999$, the temperature is $T_{5999} = 4.0/\\log(6002) \\approx 0.46$. The temperature remains high throughout the entire process. While this allows for extensive exploration of the state space, it hinders exploitation. The system may find the basin of the global minimum but may also have enough thermal energy to escape it. This schedule is likely to be more successful than the rapid linear quench but may be inferior to the well-tuned geometric schedule because it fails to cool down sufficiently for final convergence.\n\nCase $4$: Geometric cooling with a cold start ($T_0=0.05, \\alpha=0.99, K=6000, R=64$).\nHere, the initial temperature $T_0=0.05$ is extremely low. The probability of accepting even a small energy-increasing move is negligible. For $\\Delta E = 1.0$ (the barrier out of the $x=\\mathbf{0}$ minimum), the acceptance probability is $\\exp(-1.0/0.05) = \\exp(-20) \\approx 2 \\times 10^{-9}$. The algorithm will behave almost identically to a greedy local search, descending to the nearest local minimum from its random starting point and becoming permanently trapped. It will have virtually no capacity to explore the landscape and overcome the energy barrier to find the global minimum. This schedule is expected to have a success rate close to zero.\n\nThe implementation will execute these four test cases, count the number of successful runs for each, and report the success fractions, which should validate this theoretical analysis.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and analyzes Simulated Annealing (SA) acceptance schedules\n    on a deceptive QUBO problem instance.\n    \"\"\"\n    n = 16\n    c = 1.0\n    w = 0.2\n\n    # Construct the Q matrix for E(x) = x^T Q x\n    Q = np.full((n, n), -w / 2.0)\n    np.fill_diagonal(Q, c)\n\n    def calculate_energy(x, Q_mat):\n        # Using the sum form is faster than matrix multiplication for sparse vectors,\n        # but x.T @ Q_mat @ x is fine for dense numpy arrays.\n        return x.T @ Q_mat @ x\n\n    # Exhaustive search for the global minimum energy\n    # This is feasible for n=16 (2^16 = 65536 states)\n    global_min_energy = float('inf')\n    num_states = 1 << n\n    for i in range(num_states):\n        # Create binary vector x from integer i\n        x = np.array([int(b) for b in bin(i)[2:].zfill(n)])\n        energy = calculate_energy(x, Q)\n        if energy < global_min_energy:\n            global_min_energy = energy\n\n    test_cases = [\n        {\n            \"name\": \"Case 1\", \"schedule\": \"geometric\", \"T0\": 2.0, \"alpha\": 0.995,\n            \"K\": 6000, \"R\": 64, \"seed\": 123, \"Tmin\": None, \"b\": None\n        },\n        {\n            \"name\": \"Case 2\", \"schedule\": \"linear\", \"T0\": 2.0, \"Tmin\": 0.001,\n            \"K\": 1500, \"R\": 64, \"seed\": 456, \"alpha\": None, \"b\": None\n        },\n        {\n            \"name\": \"Case 3\", \"schedule\": \"logarithmic\", \"T0\": 4.0, \"b\": 2.0,\n            \"K\": 6000, \"R\": 64, \"seed\": 789, \"alpha\": None, \"Tmin\": None\n        },\n        {\n            \"name\": \"Case 4\", \"schedule\": \"geometric\", \"T0\": 0.05, \"alpha\": 0.99,\n            \"K\": 6000, \"R\": 64, \"seed\": 13579, \"Tmin\": None, \"b\": None\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        np.random.seed(case[\"seed\"])\n        success_count = 0\n        \n        K = case[\"K\"]\n        R = case[\"R\"]\n        T0 = case[\"T0\"]\n        \n        for r in range(R):\n            # Initialization\n            x_current = np.random.randint(0, 2, size=n)\n            e_current = calculate_energy(x_current, Q)\n            \n            found_global_min = e_current == global_min_energy\n            \n            T = T0\n            \n            for k in range(K):\n                # Propose new state by flipping one bit\n                bit_to_flip = np.random.randint(0, n)\n                \n                # Efficiently calculate energy difference (Delta E)\n                # delta_e = (1 - 2*x_current[bit_to_flip]) * (c_param - w_param * np.sum(np.delete(x_current, bit_to_flip)))\n                # A more direct form from first principles:\n                # delta_E = (x_new - x_old) @ (Q + Q.T) @ x_old + (x_new-x_old).T @ Q @ (x_new-x_old)\n                # For a single bit flip j, with x_j -> 1-x_j:\n                x_val = x_current[bit_to_flip]\n                sum_neighbors = np.sum(x_current) - x_val\n                delta_e = (1 - 2 * x_val) * (c - w * sum_neighbors)\n\n                # Metropolis acceptance criterion\n                if delta_e <= 0 or np.random.rand() < np.exp(-delta_e / T):\n                    x_current[bit_to_flip] = 1 - x_current[bit_to_flip]\n                    e_current += delta_e\n\n                if not found_global_min and np.isclose(e_current, global_min_energy):\n                    found_global_min = True\n                \n                # Update temperature based on schedule\n                if case[\"schedule\"] == \"geometric\":\n                    T = T0 * (case[\"alpha\"] ** (k + 1))\n                elif case[\"schedule\"] == \"linear\":\n                    T = max(case[\"Tmin\"], T0 * (1 - (k + 1) / K))\n                elif case[\"schedule\"] == \"logarithmic\":\n                    T = T0 / np.log(case[\"b\"] + k + 1 + 1) # k+2 for k=0..K-1\n            \n            if found_global_min:\n                success_count += 1\n                \n        success_rate = success_count / R\n        results.append(f\"{success_rate:.3f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2435212"}, {"introduction": "The performance of simulated annealing is highly sensitive to the parameters of its cooling schedule, such as the initial temperature $T_0$ and the cooling rate. This advanced exercise moves beyond simply using a given schedule to the practical engineering task of *tuning* it for optimal performance. By performing a systematic grid search to find the best parameters for an exponential schedule $T(k) = p_1 \\exp(-p_2 k)$ on the highly multimodal Rastrigin function, you will develop skills in meta-optimization, a crucial aspect of applying complex algorithms effectively [@problem_id:2435176].", "problem": "You are given a simulated annealing procedure for minimizing a multimodal objective in two dimensions. The algorithm uses the Metropolis acceptance rule derived from the Boltzmann distribution, and the temperature is controlled by a parametric cooling schedule. Your task is to treat the cooling schedule itself as the object to optimize. Specifically, consider a two-parameter exponential cooling schedule $T(k) = p_1 \\exp(-p_2 k)$, where $p_1 \\in \\mathbb{R}_{>0}$ and $p_2 \\in \\mathbb{R}_{\\ge 0}$, and determine the values of $p_1$ and $p_2$ that minimize the average terminal performance of a simulated annealing run on a fixed test objective, subject to a fixed computational budget. The test objective is the two-dimensional Rastrigin function, defined by\n$$\nf(\\mathbf{x}) = A n + \\sum_{i=1}^{n} \\left(x_i^2 - A \\cos(2 \\pi x_i)\\right),\n$$\nwith $n=2$ and $A=10$, where the cosine operates on radians. The search domain is the box $\\left[-5.12, 5.12\\right]^2$.\n\nThe simulated annealing procedure follows these fundamental components:\n- Initialization: draw $\\mathbf{x}_0$ uniformly from $\\left[-5.12, 5.12\\right]^2$.\n- Proposal at step $k$: propose $\\mathbf{x}' = \\mathbf{x} + \\boldsymbol{\\eta}$, where $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})$, and then clamp each component of $\\mathbf{x}'$ back to $\\left[-5.12, 5.12\\right]$ if it lies outside the domain.\n- Acceptance: let $\\Delta = f(\\mathbf{x}') - f(\\mathbf{x})$. If $\\Delta \\le 0$, accept $\\mathbf{x}'$. Otherwise, accept with probability $\\exp(-\\Delta/T(k))$, where $T(k) = p_1 \\exp(-p_2 k)$.\n- Best-so-far tracking: maintain $f^\\star_k = \\min\\{f(\\mathbf{x}_0), f(\\mathbf{x}_1), \\dots, f(\\mathbf{x}_k)\\}$ and report $f^\\star_{N}$ at the end of a run of length $N$ proposals.\n\nTo evaluate a schedule $(p_1,p_2)$, define the performance functional as the sample mean of the terminal best-so-far objective value over a fixed list of random seeds:\n$$\nJ(p_1,p_2) = \\frac{1}{S} \\sum_{s \\in \\mathcal{S}} f^\\star_{N}(s; p_1,p_2),\n$$\nwhere $\\mathcal{S}$ is the given seed set and $S = |\\mathcal{S}|$. For each seed $s \\in \\mathcal{S}$, initialize a reproducible independent random number generator using $s$ to drive both the initial point and all subsequent Gaussian and uniform draws. Your program must search over a specified finite grid $\\mathcal{P}_1 \\times \\mathcal{P}_2$ and return the pair $(p_1,p_2) \\in \\mathcal{P}_1 \\times \\mathcal{P}_2$ that minimizes $J(p_1,p_2)$. In case of a tie in $J$, select the pair with the smallest $p_1$; if still tied, select the smallest $p_2$.\n\nAngle units: all trigonometric functions use radians.\n\nFor numerical stability and determinism:\n- Use independent seeds as specified for each evaluation and no other source of randomness.\n- The exponential function should be evaluated in floating-point arithmetic as usual; the acceptance probability is $\\min\\{1, \\exp(-\\Delta/T(k))\\}$.\n\nYour task is to implement a program that, for each test case below, performs an exhaustive grid search over $(p_1,p_2)$ and outputs the optimal $p_1$, $p_2$, and the minimized value of $J(p_1,p_2)$, each rounded to $6$ decimal places.\n\nTest Suite (each test case defines $(N, \\sigma, \\mathcal{S}, \\mathcal{P}_1, \\mathcal{P}_2)$):\n- Case $1$ (general case):\n  - $N = 500$\n  - $\\sigma = 0.5$\n  - $\\mathcal{S} = [7, 11, 19]$\n  - $\\mathcal{P}_1 = [0.5, 1.0, 2.0, 5.0]$\n  - $\\mathcal{P}_2 = [0.001, 0.01, 0.05]$\n- Case $2$ (boundary includes constant temperature):\n  - $N = 300$\n  - $\\sigma = 0.3$\n  - $\\mathcal{S} = [101, 202, 303]$\n  - $\\mathcal{P}_1 = [0.2, 0.8, 1.5, 3.0]$\n  - $\\mathcal{P}_2 = [0.0, 0.02, 0.1]$\n- Case $3$ (longer run with larger proposals):\n  - $N = 1000$\n  - $\\sigma = 0.8$\n  - $\\mathcal{S} = [5, 6]$\n  - $\\mathcal{P}_1 = [1.0, 3.0, 6.0]$\n  - $\\mathcal{P}_2 = [0.001, 0.005, 0.02]$\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Concatenate the triplets for the cases in numerical order. For each case, append $p_1^\\star$, $p_2^\\star$, and $J^\\star$ (the minimized average terminal best value), each rounded to $6$ decimal places. For example, the final output should look like\n$[p_{1,1}^\\star,p_{2,1}^\\star,J_1^\\star,p_{1,2}^\\star,p_{2,2}^\\star,J_2^\\star,p_{1,3}^\\star,p_{2,3}^\\star,J_3^\\star]$,\nwith no additional text.", "solution": "The problem presented is a task in meta-optimization, specifically the tuning of hyperparameters for the simulated annealing algorithm. The goal is to determine the optimal parameters $(p_1, p_2)$ for an exponential cooling schedule, $T(k) = p_1 \\exp(-p_2 k)$, that minimize the average terminal performance when optimizing the two-dimensional Rastrigin function. The search for these optimal parameters is constrained to an exhaustive grid search over provided finite sets $\\mathcal{P}_1$ and $\\mathcal{P}_2$.\n\nThe core of the solution is the implementation of a deterministic procedure to evaluate a performance functional, $J(p_1,p_2)$, for any given pair of schedule parameters. This functional is defined as the sample mean of the best-found objective value, $f^\\star_N$, obtained from a set of independent simulated annealing runs, each distinguished by a unique random seed from a given set $\\mathcal{S}$.\n$$\nJ(p_1,p_2) = \\frac{1}{S} \\sum_{s \\in \\mathcal{S}} f^\\star_{N}(s; p_1,p_2)\n$$\nwhere $S = |\\mathcal{S}|$ is the number of runs. The use of fixed seeds ensures that the evaluation of $J(p_1,p_2)$ is fully deterministic and reproducible.\n\nEach individual simulated annealing run to compute $f^\\star_N(s; p_1,p_2)$ is structured as follows:\n\n$1$. **Initialization**: A random number generator is seeded with a specific value $s \\in \\mathcal{S}$. The initial state, $\\mathbf{x}_0 \\in \\mathbb{R}^2$, is drawn from a uniform distribution over the domain $[-5.12, 5.12]^2$. The objective function, which is the two-dimensional Rastrigin function with $A=10$,\n$$\nf(\\mathbf{x}) = 20 + \\sum_{i=1}^{2} \\left(x_i^2 - 10 \\cos(2 \\pi x_i)\\right)\n$$\nis evaluated at $\\mathbf{x}_0$. This initial value, $f(\\mathbf{x}_0)$, also initializes the best-so-far value, $f^\\star_0$.\n\n$2$. **Iterative Search**: The algorithm proceeds for a fixed number of steps, $N$. For each step $k$ from $0$ to $N-1$:\n    a. The temperature $T(k)$ is calculated according to the cooling schedule: $T(k) = p_1 \\exp(-p_2 k)$.\n    b. A candidate state $\\mathbf{x}'$ is generated by perturbing the current state $\\mathbf{x}_k$ with isotropic Gaussian noise: $\\mathbf{x}' = \\mathbf{x}_k + \\boldsymbol{\\eta}$, where $\\boldsymbol{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 \\mathbf{I})$. Any components of $\\mathbf{x}'$ that fall outside the domain $[-5.12, 5.12]$ are clamped to the respective boundary values.\n    c. The Metropolis acceptance criterion is applied. We compute the change in the objective function, $\\Delta = f(\\mathbf{x}') - f(\\mathbf{x}_k)$. If $\\Delta \\le 0$, the candidate state is unconditionally accepted ($\\mathbf{x}_{k+1} = \\mathbf{x}'$). If $\\Delta > 0$, it is accepted with probability $P_{\\text{accept}} = \\exp(-\\Delta / T(k))$. If not accepted, the state remains unchanged ($\\mathbf{x}_{k+1} = \\mathbf{x}_k$). This is implemented by drawing a random number $u \\sim \\mathcal{U}(0,1)$ and accepting if $u < P_{\\text{accept}}$.\n    d. The record of the best objective function value seen thus far is updated: $f^\\star_{k+1} = \\min(f^\\star_k, f(\\mathbf{x}_{k+1}))$.\n\n$3$. **Termination**: After $N$ steps, the final best-so-far value, $f^\\star_N$, is returned.\n\nThe outermost layer of the program executes this evaluation across the specified grid of parameters. For each test case, it iterates through all pairs $(p_1, p_2) \\in \\mathcal{P}_1 \\times \\mathcal{P}_2$. It computes $J(p_1, p_2)$ for each pair and keeps track of the pair $(p_1^\\star, p_2^\\star)$ that minimizes this value. To ensure a uniquely defined optimum, the problem specifies a tie-breaking rule: if two pairs yield the same minimal $J$ value, the one with the smaller $p_1$ is chosen. If $p_1$ is also tied, the one with the smaller $p_2$ is chosen.\n\nThe final program implements this complete logic for each of the three test cases, collecting the optimal triplet $(p_1^\\star, p_2^\\star, J^\\star)$ for each case. These results are then formatted and printed as a single comma-separated list. Numerical calculations are performed using the `numpy` library, adhering to the specified dependencies.", "answer": "```python\nimport numpy as np\n\n# Define problem constants\nA = 10.0\nN_DIMS = 2\nDOMAIN_MIN = -5.12\nDOMAIN_MAX = 5.12\n\ndef rastrigin(x: np.ndarray) -> float:\n    \"\"\"\n    Computes the Rastrigin function for a 2D vector.\n    f(x) = An + sum(xi^2 - A*cos(2*pi*xi))\n    \"\"\"\n    if x.shape != (N_DIMS,):\n        raise ValueError(\"Input must be a 2D vector.\")\n    \n    constant_term = A * N_DIMS\n    sum_term = np.sum(x**2 - A * np.cos(2 * np.pi * x))\n    return constant_term + sum_term\n\ndef run_simulated_annealing(N: int, sigma: float, p1: float, p2: float, seed: int) -> float:\n    \"\"\"\n    Performs one full run of the simulated annealing algorithm.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Initialization\n    x_current = rng.uniform(DOMAIN_MIN, DOMAIN_MAX, size=N_DIMS)\n    f_current = rastrigin(x_current)\n    best_f_so_far = f_current\n\n    # Simulation loop\n    for k in range(N):\n        # Calculate temperature for step k\n        T_k = p1 * np.exp(-p2 * k)\n\n        # Generate proposal state\n        eta = rng.normal(loc=0.0, scale=sigma, size=N_DIMS)\n        x_proposal = x_current + eta\n        # Clamp proposal to the domain\n        x_proposal = np.clip(x_proposal, DOMAIN_MIN, DOMAIN_MAX)\n\n        f_proposal = rastrigin(x_proposal)\n        delta_f = f_proposal - f_current\n\n        # Metropolis acceptance criterion\n        accepted = False\n        if delta_f <= 0:\n            accepted = True\n        else:\n            # Avoid division by zero, although p1>0 and exp() is always >0\n            if T_k > 1e-12:\n                prob_acceptance = np.exp(-delta_f / T_k)\n                if rng.random() < prob_acceptance:\n                    accepted = True\n\n        if accepted:\n            x_current = x_proposal\n            f_current = f_proposal\n\n        # Update the best-so-far objective value found\n        if f_current < best_f_so_far:\n            best_f_so_far = f_current\n            \n    return best_f_so_far\n\ndef evaluate_schedule(N: int, sigma: float, seeds: list[int], p1: float, p2: float) -> float:\n    \"\"\"\n    Evaluates the performance functional J(p1, p2) by averaging over runs.\n    \"\"\"\n    total_f_star = 0.0\n    for seed in seeds:\n        f_star_N = run_simulated_annealing(N, sigma, p1, p2, seed)\n        total_f_star += f_star_N\n    return total_f_star / len(seeds)\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {\n            \"N\": 500, \"sigma\": 0.5, \"S\": [7, 11, 19],\n            \"P1\": [0.5, 1.0, 2.0, 5.0], \"P2\": [0.001, 0.01, 0.05]\n        },\n        # Case 2\n        {\n            \"N\": 300, \"sigma\": 0.3, \"S\": [101, 202, 303],\n            \"P1\": [0.2, 0.8, 1.5, 3.0], \"P2\": [0.0, 0.02, 0.1]\n        },\n        # Case 3\n        {\n            \"N\": 1000, \"sigma\": 0.8, \"S\": [5, 6],\n            \"P1\": [1.0, 3.0, 6.0], \"P2\": [0.001, 0.005, 0.02]\n        }\n    ]\n\n    final_results = []\n    \n    for case in test_cases:\n        N, sigma, S_seeds, P1_grid, P2_grid = case[\"N\"], case[\"sigma\"], case[\"S\"], case[\"P1\"], case[\"P2\"]\n        \n        best_p1 = -1.0\n        best_p2 = -1.0\n        min_J = float('inf')\n\n        # Grid search over p1 and p2\n        for p1 in P1_grid:\n            for p2 in P2_grid:\n                current_J = evaluate_schedule(N, sigma, S_seeds, p1, p2)\n\n                # Check for new minimum, applying tie-breaking rules\n                is_better = False\n                if current_J < min_J:\n                    is_better = True\n                elif current_J == min_J:\n                    if p1 < best_p1:\n                        is_better = True\n                    elif p1 == best_p1 and p2 < best_p2:\n                        is_better = True\n                \n                if is_better:\n                    min_J = current_J\n                    best_p1 = p1\n                    best_p2 = p2\n        \n        # Append results for the current case\n        final_results.extend([best_p1, best_p2, min_J])\n\n    # Format the final output string\n    formatted_results = [f\"{val:.6f}\" for val in final_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```", "id": "2435176"}]}