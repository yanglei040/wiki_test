{"hands_on_practices": [{"introduction": "We begin with one of the most fundamental operations: addition. While seemingly straightforward, summing a long list of numbers with varying magnitudes can cause the finite precision of computers to fail spectacularly, leading to a significant loss of accuracy. This exercise [@problem_id:2393714] challenges you to implement and compare a naive summation with the elegant Kahan summation algorithm, a technique that compensates for rounding errors and powerfully demonstrates why associativity, a cornerstone of real arithmetic, does not hold in the floating-point world.", "problem": "You are to study the accumulation of rounding error in floating-point addition by contrasting the naive running sum with a compensated summation method. You must implement a complete, runnable program that, for a fixed test suite, computes both the naive floating-point sum and the compensated sum using the Kahan summation algorithm, and compares both against a high-precision reference computed with exact rational arithmetic. Your program must not read any input and must print a single line of output as specified below.\n\nThe fundamental base to be used is the standard rounding-error model for floating-point arithmetic with rounding to nearest: for any two real numbers $a$ and $b$, the computed floating-point addition satisfies $\\operatorname{fl}(a+b) = (a+b)(1+\\delta)$ with $|\\delta|\\le u$, where $u$ is the unit roundoff of the chosen format. You may also use the fact that the naive summation of $n$ terms can accumulate $O(nu)$ rounding error in the worst-case order, whereas compensated summation techniques are designed to reduce the leading-order accumulation by explicitly accounting for the low-order bits lost to rounding.\n\nRequirements:\n- Implement two summation routines operating on lists of real numbers in double precision:\n  - A naive summation that iteratively updates a running sum by $s \\leftarrow s + x_i$ for each term $x_i$.\n  - A Kahan compensated summation that uses a compensation variable to carry forward low-order information lost at each addition.\n- For a high-precision reference, compute the exact sum of each list using exact rational arithmetic (for example, a rational number type that represents each term $x_i$ as a fraction and sums exactly). This reference serves as the ground truth in real arithmetic.\n\nTest suite:\n- Use the following four test cases, each specified as an ordered list. Each list contains large-magnitude terms mixed with small-magnitude terms to expose catastrophic cancellation and loss of significance.\n  1. $[\\,10^{16},\\,1,\\,-10^{16}\\,]$.\n  2. $[\\,10^{16},\\,\\underbrace{1,\\,1,\\,\\dots,\\,1}_{100000\\ \\text{times}},\\,-10^{16}\\,]$. The exact mathematical sum is $100000$.\n  3. $[\\,1,\\,\\underbrace{10^{-16},\\,10^{-16},\\,\\dots,\\,10^{-16}}_{100000\\ \\text{times}},\\,-1\\,]$. The exact mathematical sum is $100000\\cdot 10^{-16}=10^{-11}$.\n  4. $[\\,10^{16},\\,\\underbrace{10^{-6},\\,10^{-6},\\,\\dots,\\,10^{-6}}_{100000\\ \\text{times}},\\,-10^{16}\\,]$. The exact mathematical sum is $100000\\cdot 10^{-6}=10^{-1}$.\n\nComputations and comparisons:\n- For each test case, compute:\n  - The naive floating-point sum $s_{\\text{naive}}\\in\\mathbb{R}$.\n  - The Kahan floating-point sum $s_{\\text{kahan}}\\in\\mathbb{R}$.\n  - The exact reference sum $s_{\\star}\\in\\mathbb{R}$ using exact rational arithmetic.\n- For each test case, compute the absolute errors $e_{\\text{naive}}=\\lvert s_{\\text{naive}}-s_{\\star}\\rvert$ and $e_{\\text{kahan}}=\\lvert s_{\\text{kahan}}-s_{\\star}\\rvert$, and determine a boolean flag $\\text{better}$ that is true if and only if $e_{\\text{kahan}}<e_{\\text{naive}}$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of four per-test-case records, in the same order as the test suite above. Each record must itself be a list of four elements in the order $[\\,s_{\\text{naive}},\\,s_{\\text{kahan}},\\,s_{\\star},\\,\\text{better}\\,]$, where $s_{\\text{naive}}$, $s_{\\text{kahan}}$, and $s_{\\star}$ are output as floating-point numbers and $\\text{better}$ is a boolean. Aggregate the four records into one list and print it as a single line with comma-separated elements enclosed in square brackets, for example\n  $[\\, [\\,\\cdot,\\cdot,\\cdot,\\cdot\\,], [\\,\\cdot,\\cdot,\\cdot,\\cdot\\,], [\\,\\cdot,\\cdot,\\cdot,\\cdot\\,], [\\,\\cdot,\\cdot,\\cdot,\\cdot\\,] \\,]$.\n\nNotes:\n- No physical units are involved.\n- Angles are not used.\n- Percentages are not used; all quantities are real numbers in $\\mathbb{R}$.\n- The implementation must be self-contained and runnable without user input. Use exact rational arithmetic or a multiple-precision facility available in your language’s standard library to compute $s_{\\star}$ so that the comparisons of $e_{\\text{naive}}$ and $e_{\\text{kahan}}$ are meaningful.", "solution": "The problem as stated is valid. It is scientifically grounded in the established principles of numerical analysis, specifically floating-point arithmetic. It is well-posed, providing a clear definition of the algorithms to be implemented, a complete set of test data, and an unambiguous output format. The problem is objective and free of contradictions or missing information. We may proceed with the solution.\n\nThe fundamental issue addressed is that computer floating-point addition is not associative and is subject to rounding error. For two real numbers $a$ and $b$, their floating-point sum is modeled as $\\operatorname{fl}(a+b) = (a+b)(1+\\delta)$, where the relative error $|\\delta|$ is bounded by the unit roundoff $u$. When summing a sequence of numbers, these small errors can accumulate. This analysis will contrast a naive summation approach with a compensated method designed to mitigate this error accumulation.\n\n**1. Naive Summation**\n\nThe most direct method is to iterate through the list of numbers $\\{x_i\\}_{i=1}^n$ and accumulate the sum in a single floating-point variable $s$. The update rule is $s \\leftarrow \\operatorname{fl}(s + x_i)$. The primary weakness of this method is **swamping**. If the magnitude of the running sum $s$ is much larger than the term $x_i$ being added, the contribution of $x_i$ may be partially or completely lost in the rounding process. For instance, in standard double-precision arithmetic, if $s \\approx 10^{16}$ and $x_i = 1$, the operation $\\operatorname{fl}(10^{16} + 1)$ evaluates to $10^{16}$, as the precision is insufficient to represent the result exactly. The term $x_i$ is effectively discarded.\n\n**2. Kahan Compensated Summation**\n\nThe Kahan summation algorithm is a technique that significantly reduces the accumulation of roundoff error. It maintains a second variable, a compensation $c$, which accumulates the errors that occur at each step. For each term $x_i$ in the input sequence, the algorithm performs the following operations:\n1.  Correct the current term: $y \\leftarrow x_i - c$. This step subtracts the accumulated error from the previous additions from the current term.\n2.  Add to the sum: $t \\leftarrow s + y$. This is the standard floating-point addition, where low-order bits of $y$ might be lost if $s$ is large.\n3.  Recover the error: $c \\leftarrow (t - s) - y$. This is the critical step. The term $(t - s)$ represents the part of $y$ that was successfully added to $s$. By subtracting the original (corrected) term $y$, we isolate the negative of the roundoff error from the addition $s+y$. This error is stored in $c$.\n4.  Update the sum: $s \\leftarrow t$.\n\nThis iterative process carries forward the lost \"change\" from each addition and incorporates it into the next, ensuring that the final accumulated error is dramatically smaller than in the naive case. The error bound for Kahan summation is of the order $O(u + N\\epsilon u)$, where $\\epsilon$ is related to machine precision, a vast improvement over the worst-case $O(Nu)$ error for naive summation.\n\n**3. Exact Rational Arithmetic**\n\nTo establish an authoritative ground truth, designated $s_{\\star}$, we must compute the sum without any floating-point error. This is achieved by using exact rational arithmetic. Each input number, which is given as a floating-point value, is first converted to its exact rational representation, i.e., a fraction $p/q$ where $p, q \\in \\mathbb{Z}$. All subsequent additions are performed using the exact rules of fraction arithmetic, e.g., $\\frac{a}{b} + \\frac{c}{d} = \\frac{ad+bc}{bd}$. This procedure is free from the representational and computational errors of floating-point systems, yielding the true mathematical sum of the input values.\n\n**Evaluation Procedure**\n\nFor each of the four specified test cases, we compute the naive sum $s_{\\text{naive}}$, the Kahan sum $s_{\\text{kahan}}$, and the exact reference sum $s_{\\star}$. The test cases are specifically designed to expose the failure modes of naive summation, particularly swamping and catastrophic cancellation, by mixing numbers of vastly different magnitudes. We then compute the absolute errors $e_{\\text{naive}} = \\lvert s_{\\text{naive}} - s_{\\star} \\rvert$ and $e_{\\text{kahan}} = \\lvert s_{\\text{kahan}} - s_{\\star} \\rvert$. The boolean flag $\\text{better}$ is set to $\\text{True}$ if and only if $e_{\\text{kahan}} < e_{\\text{naive}}$, quantitatively demonstrating the superior accuracy of the Kahan algorithm for the given input. The final output is structured as a list of records, with each record containing $[s_{\\text{naive}}, s_{\\text{kahan}}, s_{\\star}, \\text{better}]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy\nfrom fractions import Fraction\n\ndef naive_sum(numbers: list[float]) -> float:\n    \"\"\"\n    Computes the sum of a list of numbers using a naive iterative approach.\n    \"\"\"\n    s = 0.0\n    for x in numbers:\n        s += x\n    return s\n\ndef kahan_sum(numbers: list[float]) -> float:\n    \"\"\"\n    Computes the sum of a list of numbers using the Kahan summation algorithm\n    to reduce the accumulation of floating-point error.\n    \"\"\"\n    s = 0.0  # The running sum.\n    c = 0.0  # The compensation for lost low-order bits.\n    for x in numbers:\n        y = x - c    # c is the error from the previous sum.\n        t = s + y    # s is large, y is small, so low-order digits of y are lost.\n        c = (t - s) - y  # (t - s) recovers the high-order part of y.\n                         # Subtracting y recovers the low part, negated.\n        s = t        # Algebraically, c should be 0. But with rounding, it's not.\n    return s\n\ndef exact_sum(numbers: list[float]) -> float:\n    \"\"\"\n    Computes the exact sum of a list of floating-point numbers by\n    converting them to Fractions and using rational arithmetic.\n    \"\"\"\n    s = Fraction(0)\n    for x in numbers:\n        s += Fraction(x)\n    return float(s)\n\ndef solve():\n    \"\"\"\n    Runs the full test suite, comparing naive and Kahan summation against\n    an exact rational arithmetic reference, and prints the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: A simple case of catastrophic cancellation.\n        [1e16, 1.0, -1e16],\n        # Case 2: Summing many small numbers in the presence of a large one.\n        [1e16] + [1.0] * 100000 + [-1e16],\n        # Case 3: Summing many tiny numbers that are smaller than machine epsilon\n        # relative to the initial sum.\n        [1.0] + [1e-16] * 100000 + [-1.0],\n        # Case 4: A similar case to #2, but with smaller additions.\n        [1e16] + [1e-6] * 100000 + [-1e16],\n    ]\n\n    results = []\n    for case_data in test_cases:\n        # Compute the sum using all three methods.\n        s_naive = naive_sum(case_data)\n        s_kahan = kahan_sum(case_data)\n        s_star = exact_sum(case_data)\n        \n        # Calculate the absolute errors for both floating-point methods.\n        e_naive = abs(s_naive - s_star)\n        e_kahan = abs(s_kahan - s_star)\n        \n        # Determine if Kahan's method produced a smaller error.\n        better = e_kahan < e_naive\n        \n        # Store the record for this test case.\n        record = [s_naive, s_kahan, s_star, better]\n        results.append(record)\n\n    # Final print statement in the exact required format.\n    # The format template from the prompt is used: print(f\"[{','.join(map(str, results))}]\")\n    # str(list) in Python automatically includes spaces, which matches the example\n    # format diagram '[ [ . , . , . , . ], ... ]'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2393714"}, {"introduction": "Beyond the accumulation of errors across many operations, a single mathematical expression can be a hidden minefield of numerical instability. This practice [@problem_id:2393724] focuses on the classic problem of \"catastrophic cancellation,\" which occurs when subtracting two nearly-equal numbers, effectively erasing significant information. You will analytically dissect an expression involving the cosine function and use your knowledge of Taylor series and trigonometric identities to derive a numerically stable equivalent, a critical skill for translating mathematical models into robust code.", "problem": "A computational engineer must evaluate the function $f(x) = \\dfrac{\\cos(x) - 1}{x^{2}}$ for real $x$ in a numerical simulation, including values of $x$ near $x=0$. The implementation target is arithmetic conforming to the Institute of Electrical and Electronics Engineers (IEEE) $754$ double precision with rounding to nearest, where the unit roundoff is $u = 2^{-53}$. Starting only from the Taylor series of the cosine function about $x=0$ and basic trigonometric identities, do the following:\n\n- Using the Taylor series definition of $\\cos(x)$, determine whether $f(x)$ has a removable singularity at $x=0$ and find its limit as $x \\to 0$.\n- Explain why directly evaluating $f(x)$ as written can suffer catastrophic cancellation for small $|x|$ by relating the magnitude of $\\cos(x) - 1$ to the spacing of floating-point numbers near $1$.\n- Use Taylor series expansions and algebraic manipulation to derive a closed-form analytic expression for $f(x)$ that is exactly equal to $f(x)$ for all real $x$, has a well-defined finite limit at $x=0$, and avoids subtracting nearly equal numbers. Your reasoning should justify why this reformulation is numerically stable near $x=0$.\n\nReport only your final reformulated closed-form expression for $f(x)$ as your answer. No rounding is required, and no units apply.", "solution": "The problem as stated is subject to validation.\n\nGivens are extracted verbatim:\n- Function: $f(x) = \\dfrac{\\cos(x) - 1}{x^{2}}$ for real $x$.\n- Domain of interest: values of $x$ near $x=0$.\n- Arithmetic: IEEE $754$ double precision with rounding to nearest.\n- Unit roundoff: $u = 2^{-53}$.\n- Permitted methods: Taylor series of the cosine function about $x=0$ and basic trigonometric identities.\n- Task 1: Analyze the singularity at $x=0$ and find the limit $\\lim_{x \\to 0} f(x)$.\n- Task 2: Explain catastrophic cancellation for small $|x|$.\n- Task 3: Derive a numerically stable, closed-form analytic expression for $f(x)$.\n- Required output: The final reformulated closed-form expression.\n\nThe problem is scientifically grounded, concerning standard topics in numerical analysis such as function evaluation, floating-point error, and catastrophic cancellation. It is well-posed, with a clear objective and sufficient information to derive a unique, meaningful solution. The language is objective and precise. The problem is self-contained and logically consistent. Therefore, the problem is deemed valid and a solution will be provided.\n\nWe begin by analyzing the function $f(x)$ near $x=0$. The Taylor series for $\\cos(x)$ expanded about $x=0$ is given by:\n$$ \\cos(x) = \\sum_{n=0}^{\\infty} \\frac{(-1)^{n}x^{2n}}{(2n)!} = 1 - \\frac{x^{2}}{2!} + \\frac{x^{4}}{4!} - \\frac{x^{6}}{6!} + \\dots $$\nSubstituting this into the numerator of $f(x)$ yields:\n$$ \\cos(x) - 1 = \\left(1 - \\frac{x^{2}}{2!} + \\frac{x^{4}}{4!} - \\dots\\right) - 1 = - \\frac{x^{2}}{2!} + \\frac{x^{4}}{4!} - \\dots = \\sum_{n=1}^{\\infty} \\frac{(-1)^{n}x^{2n}}{(2n)!} $$\nNow, we can express $f(x)$ using this series:\n$$ f(x) = \\frac{1}{x^{2}} \\left( - \\frac{x^{2}}{2} + \\frac{x^{4}}{24} - \\frac{x^{6}}{720} + \\dots \\right) = - \\frac{1}{2} + \\frac{x^{2}}{24} - \\frac{x^{4}}{720} + \\dots $$\nTo find the limit as $x \\to 0$, we evaluate this series at $x=0$:\n$$ \\lim_{x \\to 0} f(x) = \\lim_{x \\to 0} \\left( - \\frac{1}{2} + \\frac{x^{2}}{24} - \\frac{x^{4}}{720} + \\dots \\right) = -\\frac{1}{2} $$\nSince the limit is finite, the function $f(x)$ has a removable singularity at $x=0$.\n\nNext, we explain the numerical instability of the direct evaluation of $f(x) = \\frac{\\cos(x) - 1}{x^{2}}$ for small $|x|$. For $|x| \\ll 1$, the value of $\\cos(x)$ is very close to $1$. In IEEE $754$ double precision arithmetic, the computed value of $\\cos(x)$, which we denote $\\operatorname{fl}(\\cos(x))$, will be a floating-point number near $1$. Let $\\cos(x) = 1 - \\delta$, where for small $x$, $\\delta \\approx \\frac{x^2}{2}$. The floating-point representation of $\\cos(x)$ has an inherent relative error on the order of the unit roundoff, $u = 2^{-53}$. Thus, the absolute error in $\\operatorname{fl}(\\cos(x))$ is approximately $u \\cdot |\\cos(x)| \\approx u$. The operation $\\cos(x) - 1$ is then a subtraction of two nearly equal numbers. The true result of this subtraction is $-\\delta \\approx -x^2/2$. The computed result, $\\operatorname{fl}(\\cos(x)) - 1$, will have an absolute error of approximately $u$. The relative error in the computed numerator is therefore approximately $\\frac{u}{|-x^2/2|} = \\frac{2u}{x^2}$. As $x \\to 0$, this relative error grows without bound, leading to a complete loss of significant digits. This phenomenon is known as catastrophic cancellation.\n\nTo remedy this, we must reformulate the expression to avoid the subtraction of nearly equal quantities. We use the half-angle trigonometric identity:\n$$ \\sin^{2}\\left(\\frac{\\theta}{2}\\right) = \\frac{1 - \\cos(\\theta)}{2} $$\nLetting $\\theta = x$, we have $1 - \\cos(x) = 2\\sin^{2}(x/2)$. This implies that $\\cos(x) - 1 = -2\\sin^{2}(x/2)$. This identity is exact for all real $x$. Substituting this into the original expression for $f(x)$:\n$$ f(x) = \\frac{-2\\sin^{2}(x/2)}{x^{2}} $$\nThis form still presents an indeterminate $0/0$ form, but it has eliminated the problematic subtraction. For optimal numerical stability and to make the behavior near $x=0$ explicit, we perform a further algebraic manipulation:\n$$ f(x) = \\frac{-2\\sin^{2}(x/2)}{x^{2}} = -\\frac{1}{2} \\cdot \\frac{4\\sin^{2}(x/2)}{x^{2}} = -\\frac{1}{2} \\cdot \\frac{(2\\sin(x/2))^{2}}{x^2} = -\\frac{1}{2} \\left(\\frac{2\\sin(x/2)}{x}\\right)^2 $$\nWe can rewrite the denominator inside the parenthesis to match the argument of the sine function:\n$$ f(x) = -\\frac{1}{2} \\left(\\frac{\\sin(x/2)}{x/2}\\right)^{2} $$\nThis final expression is analytically identical to the original for all $x \\neq 0$. It is numerically stable for small $|x|$ because the problematic subtraction has been replaced by operations that are well-behaved. The evaluation of $\\sin(y)$ for small $y=x/2$ is accurate, and the division $\\sin(y)/y$ approaches $1$ as $y \\to 0$. This ratio, related to the $\\operatorname{sinc}$ function, is computationally well-conditioned. The subsequent squaring and multiplication by $-1/2$ do not introduce significant relative error. The expression has the correct limit $-\\frac{1}{2}$ at $x=0$ and is well-defined, providing a robust formula for numerical implementation across all real $x$.", "answer": "$$\\boxed{-\\frac{1}{2} \\left(\\frac{\\sin(x/2)}{x/2}\\right)^{2}}$$", "id": "2393724"}, {"introduction": "Our final practice synthesizes these concepts by applying them to a cornerstone of computational science: approximating derivatives with finite differences. Here, you will discover the fundamental tension between two opposing sources of error: the truncation error of the mathematical approximation and the rounding error from floating-point arithmetic [@problem_id:2393695]. By implementing and comparing a naive formula with a stabilized one, you will uncover the classic trade-off that governs the choice of the step size $h$ and see firsthand how a clever reformulation can conquer the limitations of machine precision.", "problem": "You are asked to demonstrate and analyze the impact of floating-point cancellation on numerical differentiation using finite differences, and to implement a program that quantifies the effect for a designed function. Work within the standard model of floating-point arithmetic used in computational engineering: Institute of Electrical and Electronics Engineers (IEEE) 754 double precision (binary64). Adopt the classical floating-point rounding model that any elementary operation in floating point can be expressed as $\\operatorname{fl}(a \\,\\circ\\, b) = (a \\,\\circ\\, b)(1 + \\delta)$ with $|\\delta| \\leq u$, where $u$ is the unit roundoff.\n\nDesign a function by selecting a smooth expression that exhibits catastrophic cancellation when evaluated near a specific point. Use the following concrete design in your analysis and implementation:\n- Function: $f(x) = \\sqrt{1 + x} - 1$.\n- Point of interest: $x_0 = 0$.\n- Exact derivative at the point: $f^{\\prime}(0)$.\n- Rationale: Near $x = 0$, the two terms $\\sqrt{1 + x}$ and $1$ are nearly equal, so their subtraction loses leading significant digits.\n\nYour tasks are:\n1. Using Taylor series and the floating-point rounding model, derive from first principles the leading-order truncation error and the dominant rounding error term for the forward-difference estimate of the derivative at $x_0 = 0$ defined by\n   $$ D_h^{\\mathrm{fwd}} f(0) = \\frac{f(0 + h) - f(0)}{h}. $$\n   Express how the total error depends on $h$ and $u$, identifying the term that is amplified by cancellation. You must start from the basic definitions: the Taylor expansion of $\\sqrt{1 + x}$ around $x = 0$ and the floating-point rounding model with unit roundoff $u$.\n\n2. Show an algebraic reformulation of $f(x)$ that avoids the catastrophic cancellation and explain why it reduces the rounding error amplification. Use the identity\n   $$ f(x) = \\sqrt{1 + x} - 1 = \\frac{x}{\\sqrt{1 + x} + 1}, $$\n   and from it derive a numerically stable forward-difference formula at $x_0 = 0$ written purely in terms of $h$ with no subtractive cancellation.\n\n3. Implement a program that evaluates, for a set of decreasing step sizes $h$, the absolute error of the naive forward-difference derivative estimate and the absolute error of the stabilized estimate from the algebraic reformulation. For each $h$, compute the ratio\n   $$ R(h) = \\frac{\\big| D_h^{\\mathrm{naive}} f(0) - f^{\\prime}(0) \\big|}{\\big| D_h^{\\mathrm{stabilized}} f(0) - f^{\\prime}(0) \\big|}, $$\n   where $D_h^{\\mathrm{naive}} f(0)$ evaluates $f(h)$ as $\\sqrt{1 + h} - 1$ and $D_h^{\\mathrm{stabilized}} f(0)$ uses the algebraically equivalent cancellation-free form. Use IEEE 754 double precision and compute with the language’s default floating type.\n\nUse the following test suite of step sizes:\n- $h \\in \\{10^{-1}, 10^{-5}, 10^{-8}, 10^{-12}, 10^{-16}\\}$.\n\nSpecifications:\n- The exact derivative $f^{\\prime}(0)$ must be computed analytically and used as the ground truth in your error calculations. Express all angles, if any, in radians. There are no physical units.\n- For each $h$ in the test suite, your program must compute the single real number $R(h)$ defined above.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"). The entries must appear in the same order as the $h$ values listed above.", "solution": "The problem requires a rigorous analysis of numerical error in the computation of a derivative using a finite difference scheme. Specifically, we must dissect the total error into its constituent parts—truncation error from the mathematical approximation and rounding error from finite-precision arithmetic—and demonstrate how catastrophic cancellation in the naive evaluation exacerbates the rounding error. We will then introduce and analyze an algebraically equivalent, but numerically superior, formulation.\n\nFirst, we establish the ground truth. The function is $f(x) = \\sqrt{1 + x} - 1$. Its derivative is found using the chain rule:\n$$ f^{\\prime}(x) = \\frac{d}{dx} (\\sqrt{1 + x} - 1) = \\frac{1}{2\\sqrt{1 + x}}. $$\nAt the point of interest, $x_0 = 0$, the exact derivative is:\n$$ f^{\\prime}(0) = \\frac{1}{2\\sqrt{1 + 0}} = \\frac{1}{2}. $$\n\nThe forward-difference approximation to the derivative at $x_0 = 0$ is defined as:\n$$ D_h^{\\mathrm{fwd}} f(0) = \\frac{f(0 + h) - f(0)}{h}. $$\nSince $f(0) = \\sqrt{1+0} - 1 = 0$, this simplifies to:\n$$ D_h^{\\mathrm{fwd}} f(0) = \\frac{f(h)}{h} = \\frac{\\sqrt{1 + h} - 1}{h}. $$\n\n**1. Error Analysis of the Naive Formula**\n\nThe total error of the computed derivative is the difference between the computed value, which we denote $\\widehat{D}_h^{\\mathrm{naive}}$, and the exact value $f^{\\prime}(0)$. This error arises from two sources:\n\n- **Truncation Error ($E_{\\text{trunc}}$)**: This is the error inherent to the finite difference formula, assuming exact arithmetic. It can be quantified using a Taylor series expansion of $f(h)$ around $h=0$. The binomial series for $\\sqrt{1+h}$ is:\n$$ \\sqrt{1+h} = 1 + \\frac{1}{2}h - \\frac{1}{8}h^2 + \\frac{1}{16}h^3 - \\dots $$\nSubstituting this into the expression for $f(h)$ gives:\n$$ f(h) = \\left(1 + \\frac{1}{2}h - \\frac{1}{8}h^2 + O(h^3)\\right) - 1 = \\frac{1}{2}h - \\frac{1}{8}h^2 + O(h^3). $$\nNow, we can express the exact finite difference approximation:\n$$ D_h^{\\mathrm{fwd}} f(0) = \\frac{f(h)}{h} = \\frac{\\frac{1}{2}h - \\frac{1}{8}h^2 + O(h^3)}{h} = \\frac{1}{2} - \\frac{1}{8}h + O(h^2). $$\nThe truncation error is the difference between this approximation and the true derivative:\n$$ E_{\\text{trunc}} = D_h^{\\mathrm{fwd}} f(0) - f^{\\prime}(0) = \\left(\\frac{1}{2} - \\frac{1}{8}h + O(h^2)\\right) - \\frac{1}{2} = -\\frac{1}{8}h + O(h^2). $$\nThe leading-order truncation error is of order $O(h)$.\n\n- **Rounding Error ($E_{\\text{round}}$)**: This error arises from the use of finite-precision floating-point arithmetic. We use the model $\\operatorname{fl}(a \\circ b) = (a \\circ b)(1+\\delta)$ where $|\\delta| \\le u$, the unit roundoff. The key issue is the computation of $f(h) = \\sqrt{1+h}-1$. For small $h > 0$, $\\sqrt{1+h}$ is a number very close to $1$. The subtraction $\\sqrt{1+h}-1$ is a classic case of **catastrophic cancellation**, where the most significant digits of the two operands cancel, leaving a result dominated by the less significant, and potentially inaccurate, digits.\n\nLet's analyze the computed value of the numerator, $\\hat{f}(h) = \\operatorname{fl}(\\sqrt{1+h} - 1)$. The primary source of error is the evaluation of $\\sqrt{1+h}$. The computed value is $\\operatorname{fl}(\\sqrt{1+h}) = \\sqrt{1+h}(1+\\delta_1) \\approx \\sqrt{1+h} + \\delta_1$, where $|\\delta_1| \\le u$. The absolute error in this intermediate step is approximately $|\\delta_1| \\le u$, since $\\sqrt{1+h} \\approx 1$.\nThe subsequent subtraction from $1$ does not reduce this absolute error. Thus, the computed numerator is $\\hat{f}(h) \\approx f(h) + \\epsilon_{\\text{num}}$, where the error in the numerator, $\\epsilon_{\\text{num}}$, has a magnitude on the order of $u$.\n\nThe computed derivative is therefore:\n$$ \\widehat{D}_h^{\\mathrm{naive}} \\approx \\frac{f(h) + \\epsilon_{\\text{num}}}{h} = \\frac{f(h)}{h} + \\frac{\\epsilon_{\\text{num}}}{h}. $$\nThe rounding error component is:\n$$ E_{\\text{round}} = \\widehat{D}_h^{\\mathrm{naive}} - D_h^{\\mathrm{fwd}} f(0) \\approx \\frac{\\epsilon_{\\text{num}}}{h}. $$\nThe magnitude of the rounding error is bounded by:\n$$ |E_{\\text{round}}| \\lesssim \\frac{u}{h}. $$\nThis term is amplified by the division by a small $h$. As $h \\to 0$, the rounding error grows without bound.\n\nThe total absolute error is the sum of the magnitudes of the two error components:\n$$ |E_{\\text{total}}| \\approx |E_{\\text{trunc}}| + |E_{\\text{round}}| \\approx \\frac{h}{8} + \\frac{u}{h}. $$\nThis function of $h$ has a minimum. For large $h$, truncation error dominates. For small $h$, rounding error dominates.\n\n**2. Algebraic Reformulation for Numerical Stability**\n\nThe catastrophic cancellation can be avoided by reformulating the expression for $f(x)$. We multiply the numerator and denominator by the conjugate of the expression:\n$$ f(x) = \\sqrt{1+x} - 1 = (\\sqrt{1+x} - 1) \\times \\frac{\\sqrt{1+x} + 1}{\\sqrt{1+x} + 1} = \\frac{(1+x) - 1^2}{\\sqrt{1+x} + 1} = \\frac{x}{\\sqrt{1+x} + 1}. $$\nThis new form, which we can call $f_{\\text{stable}}(x)$, is mathematically identical to the original $f(x)$ but numerically superior. The subtraction of nearly equal numbers is replaced by an addition, $\\sqrt{1+x}+1$, which is a benign operation in floating-point arithmetic as no significant digits are lost.\n\nUsing this stable form, we derive a new finite-difference formula:\n$$ D_h^{\\mathrm{stabilized}} f(0) = \\frac{f_{\\text{stable}}(h)}{h} = \\frac{1}{h} \\left( \\frac{h}{\\sqrt{1+h} + 1} \\right) = \\frac{1}{\\sqrt{1+h} + 1}. $$\nThis formula for the derivative approximation is expressed purely in terms of $h$ and contains no subtractive cancellation.\n\nThe truncation error of this formula is identical to the naive one, since it is mathematically equivalent:\n$$ E_{\\text{trunc}} = \\frac{1}{\\sqrt{1+h}+1} - \\frac{1}{2} = -\\frac{h}{8} + O(h^2). $$\nHowever, its rounding error behavior is vastly improved. When evaluating $\\widehat{D}_h^{\\mathrm{stabilized}} = \\operatorname{fl}\\left(\\frac{1}{\\sqrt{1+h}+1}\\right)$, the rounding errors introduced in each step (square root, addition, division) are proportional to $u$. There is no amplification by a small $h$. The relative error of the final computed result is on the order of $u$, meaning $|E_{\\text{round}}| \\approx f^{\\prime}(0) \\times (\\text{small constant}) \\times u \\approx \\frac{1}{2} k u$. This rounding error is small and does not grow as $h$ decreases.\n\nTherefore, for the stabilized formula, the total error is dominated by the truncation error $|E_{\\text{trunc}}| \\approx h/8$ for all practical values of $h$.\n\n**3. Comparison and Implementation**\n\nThe program will compute the ratio $R(h) = \\frac{|\\text{error of naive formula}|}{|\\text{error of stabilized formula}|}$.\nBased on our analysis:\n$$ R(h) \\approx \\frac{\\frac{h}{8} + \\frac{u}{h}}{\\frac{h}{8}}. $$\nFor large $h$ (e.g., $h=10^{-1}$), where $\\frac{h}{8} \\gg \\frac{u}{h}$, the ratio $R(h)$ should be close to $1$.\nAs $h$ decreases, the $\\frac{u}{h}$ term in the numerator becomes significant. For $h=10^{-8}$, the two terms are of similar magnitude for double precision ($u \\approx 10^{-16}$).\nFor very small $h$ (e.g., $h=10^{-16}$), the rounding error $\\frac{u}{h} \\approx 1$ completely dominates the naive error, while the stabilized error remains small (on the order of $h/8 \\approx 10^{-17}$). Consequently, the ratio $R(h)$ is expected to become extremely large, quantifying the dramatic failure of the naive method.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes the effect of catastrophic cancellation on finite difference\n    approximations and compares a naive formula with a stabilized one.\n    \"\"\"\n\n    # The exact value of the derivative f'(0) for f(x) = sqrt(1+x) - 1.\n    # f'(x) = 1 / (2*sqrt(1+x)), so f'(0) = 1/2.\n    f_prime_at_0 = 0.5\n\n    # Define the test cases from the problem statement.\n    # The step sizes h to be evaluated.\n    test_cases = [1e-1, 1e-5, 1e-8, 1e-12, 1e-16]\n\n    results = []\n    for h in test_cases:\n        # --- Naive Forward-Difference Calculation ---\n        # This formula is prone to catastrophic cancellation for small h.\n        # f(h) = sqrt(1+h) - 1. f(0) = 0.\n        # D_naive = (f(h) - f(0)) / h = (sqrt(1+h) - 1) / h\n        f_h_naive = np.sqrt(1.0 + h) - 1.0\n        d_naive = f_h_naive / h\n        \n        # Calculate the absolute error of the naive method.\n        error_naive = abs(d_naive - f_prime_at_0)\n\n        # --- Stabilized Forward-Difference Calculation ---\n        # This formula is derived from the algebraic identity:\n        # f(h) = h / (sqrt(1+h) + 1).\n        # D_stabilized = (f(h) - f(0)) / h = 1 / (sqrt(1+h) + 1).\n        # This form avoids subtracting nearly equal numbers.\n        d_stabilized = 1.0 / (np.sqrt(1.0 + h) + 1.0)\n        \n        # Calculate the absolute error of the stabilized method.\n        # This error is primarily due to truncation for the given h values.\n        error_stabilized = abs(d_stabilized - f_prime_at_0)\n        \n        # --- Compute the Ratio of Errors ---\n        # This ratio quantifies how much worse the naive method is compared\n        # to the stabilized one.\n        # If error_stabilized is zero (unlikely for h>0), we handle it.\n        # However, due to truncation and floating point representation, it\n        # will be a small non-zero number.\n        if error_stabilized == 0.0:\n            # This case indicates either an issue or that the stabilized formula\n            # was perfectly accurate, making the naive error infinitely worse.\n            # Handle by setting a large number or np.inf if appropriate.\n            # For this problem, it will not be hit.\n            ratio = np.inf\n        else:\n            ratio = error_naive / error_stabilized\n        \n        results.append(ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2393695"}]}