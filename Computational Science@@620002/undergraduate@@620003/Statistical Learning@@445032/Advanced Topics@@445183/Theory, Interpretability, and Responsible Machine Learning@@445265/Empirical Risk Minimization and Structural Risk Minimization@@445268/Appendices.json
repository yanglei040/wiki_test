{"hands_on_practices": [{"introduction": "Empirical Risk Minimization (ERM) aims to find a model that perfectly fits the training data, but this can lead to overly complex models that \"overfit\" by capturing noise instead of the underlying trend. Structural Risk Minimization (SRM) offers a remedy by balancing this empirical fit with a penalty for model complexity. This first exercise provides a concrete, calculation-based look at this fundamental trade-off [@problem_id:3118270]. By comparing a simple linear function to a complex, oscillating polynomial, you will determine the exact point at which penalizing \"wiggliness\" makes the simpler model more desirable, building a core intuition for regularization.", "problem": "Consider the dataset consisting of $4$ ordered input-output pairs $D = \\{(x_i,y_i)\\}_{i=1}^{4}$ with $x_1=0$, $x_2=1$, $x_3=2$, $x_4=3$ and $y_1=0$, $y_2=3$, $y_3=0$, $y_4=3$. Let the hypothesis classes $\\mathcal{H}_d$ be all polynomials $f:\\mathbb{R}\\to\\mathbb{R}$ of degree at most $d$. The loss on an example $(x,y)$ is the squared loss $\\ell(f;(x,y)) = (y - f(x))^2$. The empirical risk is the sample mean squared loss\n$$\nR_{\\text{emp}}(f;D) \\;=\\; \\frac{1}{4}\\sum_{i=1}^{4} \\bigl(y_i - f(x_i)\\bigr)^2.\n$$\nDefine two learning principles:\n- Empirical Risk Minimization (ERM): choose any $\\hat{f}_{\\text{ERM},d} \\in \\arg\\min_{f\\in \\mathcal{H}_d} R_{\\text{emp}}(f;D)$.\n- Structural Risk Minimization (SRM) with a total variation penalty on the (discretized) first derivative: for a function $f$, define the discrete second difference at interior index $i$ by $\\Delta^2 f(x_i) := f(x_{i+1}) - 2 f(x_i) + f(x_{i-1})$ (using the natural ordering $x_1 < x_2 < x_3 < x_4$). The total variation penalty is\n$$\n\\mathrm{TV}(f;D) \\;=\\; \\sum_{i=2}^{3} \\left|\\, \\Delta^2 f(x_i) \\,\\right|.\n$$\nGiven a regularization weight $\\lambda > 0$, the SRM objective is\n$$\nJ_{\\lambda}(f;D) \\;=\\; R_{\\text{emp}}(f;D) \\;+\\; \\lambda\\, \\mathrm{TV}(f;D).\n$$\n\nTasks:\n1. Using only the definitions above, argue why ERM over $\\mathcal{H}_3$ can achieve an empirical risk of $0$ on $D$ yet corresponds to an oscillatory fit across the sample points.\n2. Compute the unique least-squares linear predictor $\\hat{f}_{\\text{ERM},1}(x) = a x + b$ that minimizes $R_{\\text{emp}}(f;D)$ over $\\mathcal{H}_1$, and evaluate its empirical risk $R_{\\text{emp}}(\\hat{f}_{\\text{ERM},1};D)$ and its penalty $\\mathrm{TV}(\\hat{f}_{\\text{ERM},1};D)$.\n3. Consider the cubic interpolant $\\hat{f}_{\\text{ERM},3}\\in\\mathcal{H}_3$ that achieves zero empirical risk by satisfying $\\hat{f}_{\\text{ERM},3}(x_i)=y_i$ for all $i\\in\\{1,2,3,4\\}$. Evaluate its empirical risk $R_{\\text{emp}}(\\hat{f}_{\\text{ERM},3};D)$ and its penalty $\\mathrm{TV}(\\hat{f}_{\\text{ERM},3};D)$.\n4. Let $\\lambda^{\\star}$ be the critical value of $\\lambda$ at which the SRM objective is indifferent between the linear least-squares predictor $\\hat{f}_{\\text{ERM},1}$ and the cubic interpolant $\\hat{f}_{\\text{ERM},3}$, that is, $J_{\\lambda^{\\star}}(\\hat{f}_{\\text{ERM},1};D) = J_{\\lambda^{\\star}}(\\hat{f}_{\\text{ERM},3};D)$. Compute $\\lambda^{\\star}$ exactly and express your final answer as a single reduced fraction.\n\nYour final answer must be the single exact value of $\\lambda^{\\star}$ as a fraction. Do not round.", "solution": "The problem is valid. We proceed with the solution by addressing each task in order.\n\nThe dataset is $D = \\{(x_i, y_i)\\}_{i=1}^{4}$ where the points are $(0,0)$, $(1,3)$, $(2,0)$, and $(3,3)$. The hypothesis classes $\\mathcal{H}_d$ are polynomials of degree at most $d$. The empirical risk is $R_{\\text{emp}}(f;D) = \\frac{1}{4}\\sum_{i=1}^{4} (y_i - f(x_i))^2$. The penalty term is $\\mathrm{TV}(f;D) = \\sum_{i=2}^{3} |\\Delta^2 f(x_i)|$, where $\\Delta^2 f(x_i) = f(x_{i+1}) - 2 f(x_i) + f(x_{i-1})$.\n\nTask 1: Argument for ERM over $\\mathcal{H}_3$\nA polynomial in $\\mathcal{H}_3$ has the form $f(x) = c_3 x^3 + c_2 x^2 + c_1 x + c_0$, which has $4$ free parameters (the coefficients $c_0, c_1, c_2, c_3$). To find a function $f \\in \\mathcal{H}_3$ that minimizes the empirical risk, we seek to make the squared errors $(y_i - f(x_i))^2$ as small as possible. If we can find a function that interpolates the data, i.e., $f(x_i) = y_i$ for all $i \\in \\{1, 2, 3, 4\\}$, then each squared error term will be zero. This would result in an empirical risk of $R_{\\text{emp}}(f;D) = 0$. Since the risk is a sum of non-negative terms, $0$ is the minimum possible value.\n\nThe interpolation conditions $f(x_i) = y_i$ for our $4$ data points form a system of $4$ linear equations in the $4$ unknown coefficients. The matrix of this system is a Vandermonde matrix based on the distinct input points $x_1=0, x_2=1, x_3=2, x_4=3$. Since the points are distinct, this matrix is invertible, which guarantees the existence of a unique polynomial of degree at most $3$ that passes through all four data points. Let this polynomial be $\\hat{f}_{\\text{ERM},3}$. For this polynomial, $R_{\\text{emp}}(\\hat{f}_{\\text{ERM},3};D) = \\frac{1}{4}\\sum_{i=1}^{4} (y_i - y_i)^2 = 0$. Thus, ERM over $\\mathcal{H}_3$ can achieve zero empirical risk.\n\nThe sequence of $y$-values is $0, 3, 0, 3$. A function that interpolates these points must rise from $y=0$ at $x=0$ to $y=3$ at $x=1$, then fall back to $y=0$ at $x=2$, and rise again to $y=3$ at $x=3$. This up-down-up behavior is characteristic of an oscillatory function. Such a function must have at least two turning points (extrema) in the interval $(0,3)$. This stands in contrast to a simple monotonic function and is a typical sign of overfitting, where the model is overly complex and captures not just the underlying trend but also the noise or random fluctuations in the sample.\n\nTask 2: Linear Predictor $\\hat{f}_{\\text{ERM},1}$\nWe want to find the linear function $\\hat{f}_{\\text{ERM},1}(x) = ax+b$ that minimizes $R_{\\text{emp}}(f;D)$. This is equivalent to minimizing the sum of squared errors, $S(a,b) = \\sum_{i=1}^{4} (y_i - (ax_i+b))^2$. We find the minimum by setting the partial derivatives with respect to $a$ and $b$ to zero.\n$$S(a,b) = (0 - (a \\cdot 0 + b))^2 + (3 - (a \\cdot 1 + b))^2 + (0 - (a \\cdot 2 + b))^2 + (3 - (a \\cdot 3 + b))^2$$\nThe normal equations are $\\frac{\\partial S}{\\partial a} = 0$ and $\\frac{\\partial S}{\\partial b} = 0$.\n$$\\frac{\\partial S}{\\partial b} = 2(b)(-1) + 2(3-a-b)(-1) + 2(-2a-b)(-1) + 2(3-3a-b)(-1) = 0$$\n$$-b -3+a+b + 2a+b -3+3a+b = 0 \\implies 6a+2b-6=0 \\implies 3a+b=3 \\quad (*)$$\nWait, there is a calculation error. Let us re-calculate.\n$\\frac{\\partial S}{\\partial b} = 2b(1) \\cdot (-1) \\frac{d}{db}(-b) ...$ Let us re-evaluate the sum of partial derivatives:\n$\\frac{\\partial S}{\\partial b} = \\sum_{i=1}^4 2(y_i - ax_i - b)(-1) = 0$, which simplifies to $\\sum(y_i - ax_i - b) = 0$, or $(\\sum y_i) - a(\\sum x_i) - 4b = 0$.\nWith $\\sum x_i = 0+1+2+3 = 6$ and $\\sum y_i = 0+3+0+3=6$, we have $6 - 6a - 4b = 0$, which simplifies to $3a+2b=3$.\n\n$\\frac{\\partial S}{\\partial a} = \\sum_{i=1}^4 2(y_i - ax_i - b)(-x_i) = 0$, which simplifies to $\\sum(y_i x_i - ax_i^2 - bx_i) = 0$, or $(\\sum x_i y_i) - a(\\sum x_i^2) - b(\\sum x_i) = 0$.\n$\\sum x_i = 6$.\n$\\sum x_i^2 = 0^2+1^2+2^2+3^2 = 0+1+4+9=14$.\n$\\sum x_i y_i = (0)(0) + (1)(3) + (2)(0) + (3)(3) = 0+3+0+9=12$.\nSo, $12 - 14a - 6b = 0$, which simplifies to $7a+3b=6$.\n\nWe solve the system of linear equations:\n1) $3a+2b=3$\n2) $7a+3b=6$\nMultiply (1) by $3$ and (2) by $2$:\n$9a+6b=9$\n$14a+6b=12$\nSubtracting the first from the second gives $5a=3 \\implies a = \\frac{3}{5}$.\nSubstituting into (1): $3(\\frac{3}{5}) + 2b = 3 \\implies \\frac{9}{5} + 2b = 3 \\implies 2b = 3 - \\frac{9}{5} = \\frac{6}{5} \\implies b = \\frac{3}{5}$.\nSo, the linear predictor is $\\hat{f}_{\\text{ERM},1}(x) = \\frac{3}{5}x + \\frac{3}{5}$.\n\nNow we evaluate its empirical risk, $R_{\\text{emp}}(\\hat{f}_{\\text{ERM},1};D)$:\n$f(x_1=0) = \\frac{3}{5}$, error $e_1 = 0 - \\frac{3}{5} = -\\frac{3}{5}$.\n$f(x_2=1) = \\frac{6}{5}$, error $e_2 = 3 - \\frac{6}{5} = \\frac{9}{5}$.\n$f(x_3=2) = \\frac{9}{5}$, error $e_3 = 0 - \\frac{9}{5} = -\\frac{9}{5}$.\n$f(x_4=3) = \\frac{12}{5}$, error $e_4 = 3 - \\frac{12}{5} = \\frac{3}{5}$.\nThe sum of squared errors is $\\sum e_i^2 = (-\\frac{3}{5})^2 + (\\frac{9}{5})^2 + (-\\frac{9}{5})^2 + (\\frac{3}{5})^2 = \\frac{9}{25} + \\frac{81}{25} + \\frac{81}{25} + \\frac{9}{25} = \\frac{180}{25} = \\frac{36}{5}$.\n$R_{\\text{emp}}(\\hat{f}_{\\text{ERM},1};D) = \\frac{1}{4} \\sum e_i^2 = \\frac{1}{4} \\cdot \\frac{36}{5} = \\frac{9}{5}$.\n\nNext, we evaluate its penalty, $\\mathrm{TV}(\\hat{f}_{\\text{ERM},1};D)$:\nThe inputs $x_i$ are equally spaced. For any linear function $f(x)=ax+b$ and equally spaced points $x_{i-1}, x_i, x_{i+1}$, the discrete second difference is zero:\n$\\Delta^2 f(x_i) = f(x_{i+1}) - 2f(x_i) + f(x_{i-1}) = (a x_{i+1}+b) - 2(a x_i+b) + (a x_{i-1}+b) = a(x_{i+1}-2x_i+x_{i-1}) = a((x_i+h)-2x_i+(x_i-h)) = 0$, where $h$ is the spacing.\nThus, $\\mathrm{TV}(\\hat{f}_{\\text{ERM},1};D) = |\\Delta^2 f(x_2)| + |\\Delta^2 f(x_3)| = |0| + |0| = 0$.\n\nTask 3: Cubic Interpolant $\\hat{f}_{\\text{ERM},3}$\nThis function, by definition, interpolates the data, so $\\hat{f}_{\\text{ERM},3}(x_i) = y_i$ for all $i$.\nIts empirical risk is $R_{\\text{emp}}(\\hat{f}_{\\text{ERM},3};D) = \\frac{1}{4}\\sum_{i=1}^{4} (y_i - \\hat{f}_{\\text{ERM},3}(x_i))^2 = \\frac{1}{4}\\sum_{i=1}^{4} (y_i - y_i)^2 = 0$.\n\nTo evaluate its penalty $\\mathrm{TV}(\\hat{f}_{\\text{ERM},3};D)$, we use the values $y_i$ in place of $f(x_i)$:\n$f(x_1)=y_1=0$, $f(x_2)=y_2=3$, $f(x_3)=y_3=0$, $f(x_4)=y_4=3$.\n$\\Delta^2 f(x_2) = f(x_3) - 2f(x_2) + f(x_1) = y_3 - 2y_2 + y_1 = 0 - 2(3) + 0 = -6$.\n$\\Delta^2 f(x_3) = f(x_4) - 2f(x_3) + f(x_2) = y_4 - 2y_3 + y_2 = 3 - 2(0) + 3 = 6$.\n$\\mathrm{TV}(\\hat{f}_{\\text{ERM},3};D) = |\\Delta^2 f(x_2)| + |\\Delta^2 f(x_3)| = |-6| + |6| = 6+6=12$.\n\nTask 4: Critical Value $\\lambda^{\\star}$\nThe SRM objective is $J_{\\lambda}(f;D) = R_{\\text{emp}}(f;D) + \\lambda \\mathrm{TV}(f;D)$.\nWe are looking for the value $\\lambda^{\\star}$ where the objective is equal for the two models:\n$J_{\\lambda^{\\star}}(\\hat{f}_{\\text{ERM},1};D) = J_{\\lambda^{\\star}}(\\hat{f}_{\\text{ERM},3};D)$.\n\nFor the linear model, $\\hat{f}_{\\text{ERM},1}$:\n$J_{\\lambda}(\\hat{f}_{\\text{ERM},1};D) = R_{\\text{emp}}(\\hat{f}_{\\text{ERM},1};D) + \\lambda\\mathrm{TV}(\\hat{f}_{\\text{ERM},1};D) = \\frac{9}{5} + \\lambda \\cdot 0 = \\frac{9}{5}$.\n\nFor the cubic model, $\\hat{f}_{\\text{ERM},3}$:\n$J_{\\lambda}(\\hat{f}_{\\text{ERM},3};D) = R_{\\text{emp}}(\\hat{f}_{\\text{ERM},3};D) + \\lambda\\mathrm{TV}(\\hat{f}_{\\text{ERM},3};D) = 0 + \\lambda \\cdot 12 = 12\\lambda$.\n\nSetting them equal at $\\lambda=\\lambda^{\\star}$:\n$\\frac{9}{5} = 12\\lambda^{\\star}$\nSolving for $\\lambda^{\\star}$:\n$\\lambda^{\\star} = \\frac{1}{12} \\cdot \\frac{9}{5} = \\frac{9}{60}$.\nReducing the fraction:\n$\\lambda^{\\star} = \\frac{3 \\cdot 3}{3 \\cdot 20} = \\frac{3}{20}$.\n\nThis value of $\\lambda^{\\star}$ represents the point of indifference. For $\\lambda < \\lambda^{\\star}$, the SRM objective prefers the more complex cubic model (as $12\\lambda < 9/5$). For $\\lambda > \\lambda^{\\star}$, it prefers the simpler linear model (as $12\\lambda > 9/5$), penalizing the oscillatory nature of the cubic fit more heavily than it rewards the perfect data fit.", "answer": "$$\\boxed{\\frac{3}{20}}$$", "id": "3118270"}, {"introduction": "While the previous practice introduced a penalty for complexity, the measure of \"wiggliness\" was specific to that context. A more powerful and general approach is to use a data-dependent measure of complexity. This practice introduces the empirical Rademacher complexity, a foundational concept in learning theory that quantifies a function class's capacity to fit random noise [@problem_id:3118242]. By implementing an SRM procedure to select the optimal polynomial degree, you will not only apply this theoretical tool but also discover its sensitivity to practical choices like feature scaling, a crucial lesson for any practitioner.", "problem": "Consider polynomial regression in one dimension with hypothesis classes indexed by the polynomial degree. For each integer degree $k \\in \\{0,1,2,3,4\\}$, define the hypothesis class $\\mathcal{H}_k$ consisting of functions $f_{\\boldsymbol{w}}(x) = \\sum_{j=0}^{k} w_j x^j$ where the coefficient vector $\\boldsymbol{w} \\in \\mathbb{R}^{k+1}$ satisfies the Euclidean norm constraint $\\|\\boldsymbol{w}\\|_2 \\leq B$. Use the following foundational bases: the definition of Empirical Risk Minimization (ERM), the definition of Structural Risk Minimization (SRM), and the definition of empirical Rademacher complexity. ERM selects, for each $k$, a coefficient vector $\\hat{\\boldsymbol{w}}_k$ that minimizes the empirical risk for the squared loss. SRM selects the degree $k$ by minimizing a trade-off between the empirical risk and a data-dependent complexity term.\n\nLet the dataset be constructed deterministically as follows. Let the sample size be $n = 20$. Let inputs be $x_i$ equally spaced in the interval $[-1,1]$, that is $x_i = -1 + \\frac{2(i-1)}{n-1}$ for $i = 1,2,\\dots,n$. Let outputs be generated by a noisy quadratic model: $y_i = 1 - 2 x_i + 0.5 x_i^2 + \\epsilon_i$, where $\\epsilon_i$ are independent Gaussian noise variables with $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ and $\\sigma = 0.1$. Use a fixed random seed $123$ to generate $(\\epsilon_i)_{i=1}^n$ to ensure reproducibility.\n\nFor any scaling factor $s > 0$, define scaled inputs $x_i^{(s)} = s \\cdot x_i$. For each degree $k$, define the feature map $\\boldsymbol{\\phi}_k(x) = (x^0, x^1, \\dots, x^k)^\\top \\in \\mathbb{R}^{k+1}$ and the corresponding design matrix $\\Phi_k^{(s)} \\in \\mathbb{R}^{n \\times (k+1)}$ whose $i$-th row is $\\boldsymbol{\\phi}_k(x_i^{(s)})^\\top$.\n\nFor each $k$ and scaling factor $s$, compute:\n- The empirical risk under squared loss at the ERM solution,\n  $$\\hat{R}_k^{(s)} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\hat{\\boldsymbol{w}}_k^{(s)\\top} \\boldsymbol{\\phi}_k(x_i^{(s)})\\right)^2,$$\n  where $\\hat{\\boldsymbol{w}}_k^{(s)}$ is any minimizer of the empirical risk over $\\boldsymbol{w} \\in \\mathbb{R}^{k+1}$, and ties may be broken arbitrarily among minimizers.\n- The empirical Rademacher complexity of the class $\\mathcal{H}_k$ evaluated on the scaled dataset $(x_i^{(s)})_{i=1}^n$,\n  $$\\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\|\\boldsymbol{w}\\|_2 \\leq B} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_i \\, \\boldsymbol{w}^\\top \\boldsymbol{\\phi}_k(x_i^{(s)}) \\right],$$\n  where $\\boldsymbol{\\sigma} = (\\sigma_1,\\dots,\\sigma_n)$ has independent Rademacher entries with $\\mathbb{P}(\\sigma_i = 1) = \\mathbb{P}(\\sigma_i = -1) = \\frac{1}{2}$, and $B = 1$. Approximate this expectation by Monte Carlo averaging over $M = 2000$ independent draws of $\\boldsymbol{\\sigma}$ using a fixed random seed $999$ to ensure reproducibility.\n\nDefine the SRM selection criterion for degree $k$ at scaling $s$ and trade-off parameter $c > 0$ as\n$$\\mathrm{SRM}(k; s,c) = \\hat{R}_k^{(s)} + c \\cdot \\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s).$$\nSelect the degree\n$$k^*(s,c) = \\arg\\min_{k \\in \\{0,1,2,3,4\\}} \\mathrm{SRM}(k; s,c),$$\nbreaking ties in favor of the smaller $k$.\n\nDiscuss, using the computed empirical Rademacher complexity values, how feature scaling $x \\mapsto s x$ affects $\\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s)$ through the magnitude of the feature vectors $\\boldsymbol{\\phi}_k(x_i^{(s)})$ and the resulting norms that appear in the supremum, and how this, in turn, influences the SRM-selected degree $k^*(s,c)$.\n\nImplement a complete, runnable program that:\n- Constructs the dataset $(x_i,y_i)_{i=1}^n$ as specified with $n = 20$, $x_i \\in [-1,1]$, $\\sigma = 0.1$, and random seed $123$ for the noise.\n- For each degree $k \\in \\{0,1,2,3,4\\}$, each scaling $s$ in the test suite, and each $c$ in the test suite, computes $\\hat{R}_k^{(s)}$, approximates $\\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s)$ with $M = 2000$ Monte Carlo samples using random seed $999$, and then computes $\\mathrm{SRM}(k; s,c)$ and $k^*(s,c)$.\n- Produces a single line of output containing the selected degrees $k^*(s,c)$ for all test cases as a comma-separated list enclosed in square brackets.\n\nUse the following test suite, which explores different facets:\n- Case $1$: $s = 1.0$, $c = 0.05$ (baseline scaling and moderate complexity penalty; happy path).\n- Case $2$: $s = 0.5$, $c = 0.05$ (reduced feature magnitudes; complexity decreases).\n- Case $3$: $s = 2.0$, $c = 0.05$ (increased feature magnitudes; complexity increases).\n- Case $4$: $s = 1.0$, $c = 0.5$ (strong complexity penalty; boundary pushing toward lower degrees).\n- Case $5$: $s = 0.1$, $c = 0.05$ (very small scaling; significant decrease in complexity).\n- Case $6$: $s = 1.0$, $c = 0.0$ (pure ERM with no complexity penalty; edge case).\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4,r_5,r_6]$ where each $r_i$ is an integer degree).", "solution": "### Problem Validation\n\nThe first step is a rigorous validation of the problem statement.\n\n**Step 1: Extract Givens**\n- **Hypothesis Classes**: For $k \\in \\{0,1,2,3,4\\}$, $\\mathcal{H}_k = \\{f_{\\boldsymbol{w}}(x) = \\sum_{j=0}^{k} w_j x^j : \\boldsymbol{w} \\in \\mathbb{R}^{k+1}, \\|\\boldsymbol{w}\\|_2 \\leq B\\}$.\n- **Dataset**: Sample size $n = 20$. Inputs $x_i = -1 + \\frac{2(i-1)}{n-1}$ for $i=1,\\dots,n$. Outputs $y_i = 1 - 2 x_i + 0.5 x_i^2 + \\epsilon_i$, with $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$, $\\sigma = 0.1$. Noise generation uses random seed $123$.\n- **Feature Scaling**: Scaled inputs $x_i^{(s)} = s \\cdot x_i$ for $s > 0$.\n- **Design Matrix**: $\\Phi_k^{(s)} \\in \\mathbb{R}^{n \\times (k+1)}$ with $i$-th row $\\boldsymbol{\\phi}_k(x_i^{(s)})^\\top = ((x_i^{(s)})^0, \\dots, (x_i^{(s)})^k)$.\n- **Empirical Risk (ERM)**: $\\hat{R}_k^{(s)} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{\\boldsymbol{w}}_k^{(s)\\top} \\boldsymbol{\\phi}_k(x_i^{(s)}))^2$, where $\\hat{\\boldsymbol{w}}_k^{(s)}$ minimizes this risk over $\\boldsymbol{w} \\in \\mathbb{R}^{k+1}$.\n- **Empirical Rademacher Complexity**: $\\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s) = \\mathbb{E}_{\\boldsymbol{\\sigma}} [ \\sup_{\\|\\boldsymbol{w}\\|_2 \\leq B} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_i \\, \\boldsymbol{w}^\\top \\boldsymbol{\\phi}_k(x_i^{(s)}) ]$, with $B=1$ and Rademacher variables $\\sigma_i$. The expectation is approximated by a Monte Carlo average over $M=2000$ samples using random seed $999$.\n- **Structural Risk Minimization (SRM)**: The selection criterion is $\\mathrm{SRM}(k; s,c) = \\hat{R}_k^{(s)} + c \\cdot \\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s)$.\n- **Model Selection**: $k^*(s,c) = \\arg\\min_{k \\in \\{0,1,2,3,4\\}} \\mathrm{SRM}(k; s,c)$, with ties broken by choosing the smaller $k$.\n- **Task**: Compute $k^*(s,c)$ for a given test suite of $(s, c)$ pairs.\n- **Test Suite**:\n    1. $s=1.0, c=0.05$\n    2. $s=0.5, c=0.05$\n    3. $s=2.0, c=0.05$\n    4. $s=1.0, c=0.5$\n    5. $s=0.1, c=0.05$\n    6. $s=1.0, c=0.0$\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is firmly rooted in statistical learning theory. Empirical Risk Minimization, Structural Risk Minimization, and Rademacher complexity are foundational concepts. The formulas provided are the standard definitions. The setup of a polynomial regression simulation is a canonical example used to illustrate these concepts. The problem is scientifically sound.\n- **Well-Posedness**: The problem is well-posed. The dataset is deterministically specified (up to a seeded random process). The objective functions for ERM and SRM are clearly defined. The ERM for squared loss is a standard unconstrained least-squares problem, which has a unique solution as long as the design matrix has full column rank (which is true for polynomial features on distinct points). The supremum in the Rademacher complexity definition has a well-known closed-form solution. The tie-breaking rule for $k^*$ ensures a unique answer.\n- **Objectivity**: The problem is stated in precise, objective mathematical language, free from ambiguity or subjective claims.\n- **Completeness and Consistency**: All necessary parameters ($n, k, \\sigma, B, M$, random seeds, test cases) are provided. There are no contradictions. The ERM is explicitly defined as an unconstrained optimization over $\\mathbb{R}^{k+1}$, while the hypothesis class $\\mathcal{H}_k$ used for the complexity calculation is constrained by $\\|\\boldsymbol{w}\\|_2 \\leq B$. This is a standard and consistent paradigm in learning theory where the complexity of a class is analyzed separately from the specific algorithm used for minimization.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We may proceed with the solution.\n\n### Solution Derivation\n\nThe solution requires implementing the Structural Risk Minimization (SRM) principle to select the optimal polynomial degree $k$ for a given dataset. This involves balancing the empirical risk (how well the model fits the training data) and a complexity penalty derived from the empirical Rademacher complexity of the hypothesis class. We must perform this for several scenarios defined by a feature scaling factor $s$ and a complexity trade-off parameter $c$.\n\n**1. Dataset Generation**\nFirst, we construct the dataset $(x_i, y_i)_{i=1}^n$ as specified. The sample size is $n=20$. The inputs $x_i$ are $n$ equally spaced points in the interval $[-1, 1]$. The true model is a quadratic function $f^*(x) = 1 - 2x + 0.5x^2$ corrupted by Gaussian noise.\nThe outputs are $y_i = f^*(x_i) + \\epsilon_i$, where $\\epsilon_i$ are drawn from a normal distribution $\\mathcal{N}(0, \\sigma^2)$ with $\\sigma=0.1$. The use of a fixed random seed ($123$) ensures that the noise vector $(\\epsilon_1, \\dots, \\epsilon_n)$ is reproducible.\n\n**2. Empirical Risk Minimization (ERM)**\nFor each degree $k \\in \\{0, 1, 2, 3, 4\\}$ and scaling factor $s$, we need to find the model that minimizes the empirical risk. The empirical risk is the mean squared error on the training data:\n$$ \\hat{R}(\\boldsymbol{w}) = \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_i - \\boldsymbol{w}^\\top \\boldsymbol{\\phi}_k(x_i^{(s)})\\right)^2 = \\frac{1}{n} \\|\\boldsymbol{y} - \\Phi_k^{(s)} \\boldsymbol{w}\\|_2^2 $$\nThe problem asks for the minimizer $\\hat{\\boldsymbol{w}}_k^{(s)}$ over all $\\boldsymbol{w} \\in \\mathbb{R}^{k+1}$. This is a standard unconstrained linear least-squares problem. The solution $\\hat{\\boldsymbol{w}}_k^{(s)}$ is given by the normal equations:\n$$ \\hat{\\boldsymbol{w}}_k^{(s)} = (\\Phi_k^{(s)\\top} \\Phi_k^{(s)})^{-1} \\Phi_k^{(s)\\top} \\boldsymbol{y} $$\nwhere $\\Phi_k^{(s)}$ is the design matrix whose $i$-th row is $\\boldsymbol{\\phi}_k(x_i^{(s)})^\\top = (1, sx_i, (sx_i)^2, \\dots, (sx_i)^k)$. Numerically, this is best solved using methods like QR decomposition or SVD, as implemented in `numpy.linalg.lstsq`.\nAfter finding $\\hat{\\boldsymbol{w}}_k^{(s)}$, we compute the minimized empirical risk $\\hat{R}_k^{(s)} = \\frac{1}{n} \\|\\boldsymbol{y} - \\Phi_k^{(s)} \\hat{\\boldsymbol{w}}_k^{(s)}\\|_2^2$.\n\n**3. Empirical Rademacher Complexity**\nThe empirical Rademacher complexity measures the ability of a function class to fit random noise. For the class $\\mathcal{H}_k$, it is defined on the specific (scaled) data points $x_i^{(s)}$:\n$$ \\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\sup_{\\|\\boldsymbol{w}\\|_2 \\leq B} \\frac{1}{n} \\sum_{i=1}^{n} \\sigma_i \\, \\boldsymbol{w}^\\top \\boldsymbol{\\phi}_k(x_i^{(s)}) \\right] $$\nWe can simplify the term inside the expectation. The sum can be rewritten using linear algebra:\n$$ \\sum_{i=1}^{n} \\sigma_i \\, \\boldsymbol{w}^\\top \\boldsymbol{\\phi}_k(x_i^{(s)}) = \\boldsymbol{w}^\\top \\left( \\sum_{i=1}^{n} \\sigma_i \\boldsymbol{\\phi}_k(x_i^{(s)}) \\right) = \\boldsymbol{w}^\\top (\\Phi_k^{(s)\\top} \\boldsymbol{\\sigma})$$\nwhere $\\boldsymbol{\\sigma} = (\\sigma_1, \\dots, \\sigma_n)^\\top$. By the Cauchy-Schwarz inequality, the supremum of $\\boldsymbol{w}^\\top \\boldsymbol{v}$ over the ball $\\|\\boldsymbol{w}\\|_2 \\leq B$ is $B\\|\\boldsymbol{v}\\|_2$. Thus, the supremum is:\n$$ \\sup_{\\|\\boldsymbol{w}\\|_2 \\leq B} \\boldsymbol{w}^\\top (\\Phi_k^{(s)\\top} \\boldsymbol{\\sigma}) = B \\|\\Phi_k^{(s)\\top} \\boldsymbol{\\sigma}\\|_2 $$\nSubstituting this back, we get:\n$$ \\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s) = \\mathbb{E}_{\\boldsymbol{\\sigma}} \\left[ \\frac{B}{n} \\|\\Phi_k^{(s)\\top} \\boldsymbol{\\sigma}\\|_2 \\right] $$\nThis expectation is intractable to compute exactly. We approximate it using a Monte Carlo average over $M=2000$ independent samples of the Rademacher vector $\\boldsymbol{\\sigma}^{(j)}$:\n$$ \\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s) \\approx \\frac{1}{M} \\sum_{j=1}^{M} \\frac{B}{n} \\|\\Phi_k^{(s)\\top} \\boldsymbol{\\sigma}^{(j)}\\|_2 $$\nwith $B=1$, $n=20$, and a fixed random seed ($999$) for reproducibility of the $\\boldsymbol{\\sigma}^{(j)}$ samples.\n\n**4. Structural Risk Minimization and Model Selection**\nWith the empirical risk $\\hat{R}_k^{(s)}$ and the complexity $\\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s)$ computed for each degree $k$, we evaluate the SRM criterion:\n$$ \\mathrm{SRM}(k; s, c) = \\hat{R}_k^{(s)} + c \\cdot \\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s) $$\nThis criterion balances the model's fit to the data (low $\\hat{R}_k^{(s)}$) against its complexity (low $\\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s)$). The parameter $c$ controls the strength of the penalty for complexity. The final step is to select the degree $k^*$ that minimizes this criterion for a given pair $(s, c)$:\n$$ k^*(s, c) = \\arg\\min_{k \\in \\{0,1,2,3,4\\}} \\mathrm{SRM}(k; s, c) $$\nTies are resolved by choosing the smaller degree $k$.\n\n**5. Analysis of Feature Scaling**\nThe scaling factor $s$ directly influences the magnitude of the feature vectors and, consequently, the Rademacher complexity. The scaled feature vector is $\\boldsymbol{\\phi}_k(sx) = (1, sx, (sx)^2, \\dots, (sx)^k)^\\top$. Compared to the unscaled vector $\\boldsymbol{\\phi}_k(x)$, the $j$-th component is multiplied by $s^j$. This means the Rademacher complexity term, which involves $\\|\\Phi_k^{(s)\\top} \\boldsymbol{\\sigma}\\|_2$, is affected.\n- If $s > 1$, the components of the features corresponding to higher powers are magnified. This increases the norm $\\|\\Phi_k^{(s)\\top} \\boldsymbol{\\sigma}\\|_2$, leading to a larger $\\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s)$, especially for larger $k$. A larger complexity term acts as a stronger penalty, favoring simpler models (lower $k^*$).\n- If $s < 1$, the feature components are attenuated. This decreases the norm, leading to a smaller $\\hat{\\mathfrak{R}}_n(\\mathcal{H}_k; s)$. The reduced complexity penalty makes it \"cheaper\" to use more complex models, potentially leading to the selection of a higher $k^*$ if the corresponding reduction in empirical risk $\\hat{R}_k^{(s)}$ is sufficient.\n- For $c=0$, SRM reduces to pure ERM, which selects the model with the lowest training error. For nested polynomial classes, this will always be the highest available degree, $k=4$, as it has the most freedom to fit the noise in the data.\n- For large $c$, the complexity penalty dominates the SRM criterion, strongly favoring the simplest models (low $k$).\n\nThese effects will be observed across the test cases provided in the problem statement. For instance, the true model is quadratic ($k=2$). We expect SRM with well-chosen parameters (e.g., $s=1.0, c=0.05$) to correctly identify $k=2$. Increasing $s$ or $c$ should push the selection toward $k<2$, while decreasing $s$ should make higher degrees more competitive, potentially leading to overfitting ($k>2$).", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the polynomial regression model selection problem using SRM.\n    \"\"\"\n    # Define problem parameters\n    n = 20\n    degrees = [0, 1, 2, 3, 4]\n    B = 1.0\n    sigma = 0.1\n    noise_seed = 123\n    monte_carlo_samples = 2000\n    monte_carlo_seed = 999\n\n    # Define the test suite\n    test_cases = [\n        # (s, c)\n        (1.0, 0.05),  # Case 1: Baseline\n        (0.5, 0.05),  # Case 2: Reduced feature magnitudes\n        (2.0, 0.05),  # Case 3: Increased feature magnitudes\n        (1.0, 0.5),   # Case 4: Strong complexity penalty\n        (0.1, 0.05),  # Case 5: Very small scaling\n        (1.0, 0.0),   # Case 6: Pure ERM (no penalty)\n    ]\n\n    # Generate the dataset\n    x = np.linspace(-1.0, 1.0, n)\n    noise_rng = np.random.RandomState(noise_seed)\n    epsilon = noise_rng.normal(0, sigma, n)\n    y_true = 1 - 2 * x + 0.5 * x**2\n    y = y_true + epsilon\n\n    # Prepare for Monte Carlo approximation of Rademacher complexity\n    rademacher_rng = np.random.RandomState(monte_carlo_seed)\n    sigma_matrix = rademacher_rng.choice([-1, 1], size=(monte_carlo_samples, n))\n\n    results = []\n    # Loop over all test cases\n    for s, c in test_cases:\n        srm_values = []\n        # Loop over all degrees k\n        for k in degrees:\n            # 1. Construct scaled design matrix\n            x_scaled = s * x\n            # np.vander creates columns in decreasing power order by default.\n            # a Vandermonde matrix of order N-1 for a k-th degree polynomial. so N=k+1.\n            # increasing=True puts powers as (x^0, x^1, ..., x^k)\n            phi = np.vander(x_scaled, N=k + 1, increasing=True)\n            \n            # 2. Compute Empirical Risk (ERM)\n            # Find w_hat using unconstrained least squares\n            w_hat, residuals, _, _ = np.linalg.lstsq(phi, y, rcond=None)\n            \n            # Calculate predictions and empirical risk\n            y_hat = phi @ w_hat\n            emp_risk = np.mean((y - y_hat)**2)\n            \n            # 3. Approximate Empirical Rademacher Complexity\n            # phi.T has shape (k+1, n)\n            # sigma_matrix.T has shape (n, M)\n            # The product has shape (k+1, M)\n            # Each column is phi.T @ sigma_j\n            term_inside_norm = phi.T @ sigma_matrix.T\n            # Take L2 norm over axis 0 (columns) -> shape (M,)\n            norms = np.linalg.norm(term_inside_norm, axis=0)\n            \n            rad_complexity = np.mean((B / n) * norms)\n            \n            # 4. Compute SRM criterion\n            srm = emp_risk + c * rad_complexity\n            srm_values.append(srm)\n        \n        # 5. Select best k\n        # np.argmin breaks ties by taking the first occurrence (smallest k)\n        k_star = degrees[np.argmin(srm_values)]\n        results.append(k_star)\n\n    # Print results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3118242"}, {"introduction": "The principle of Structural Risk Minimization is not limited to adding explicit penalty terms to an objective function; it can also be implemented procedurally. This advanced practice demonstrates how early stopping, a widely used technique in machine learning, is a powerful form of SRM. You will implement the AdaBoost algorithm and contrast the ERM approach of minimizing training error with the SRM approach of stopping at a point dictated by a theoretical margin-based generalization bound [@problem_id:3118279]. This exercise powerfully connects the abstract theory of generalization with a practical, state-of-the-art algorithm, showing that controlling training time is a valid and effective way to control model complexity.", "problem": "You will implement and compare Empirical Risk Minimization (ERM) and Structural Risk Minimization (SRM) for early-stopped Adaptive Boosting (AdaBoost) using the number of boosting rounds $T$ as the structure parameter. The setting is binary classification with labels in $\\{-1,+1\\}$ using the class of decision stumps on the real line. You must start from core definitions of empirical risk, boosting, margins, and a well-tested generalization bound based on the margin distribution. Your program will train AdaBoost for a fixed maximum number of rounds $T_{\\max}$, compute the normalized margin distribution on the training sample for each $T \\in \\{1,2,\\dots,T_{\\max}\\}$, and select $T$ by SRM using a margin-based generalization bound. You will also select $T$ by ERM using training error minimization and compare the two $T$ choices. All computations are purely mathematical and have no physical units.\n\nFundamental base and definitions to be used:\n- Binary classification sample $S=\\{(x_i,y_i)\\}_{i=1}^m$ with $x_i \\in \\mathbb{R}$ and $y_i \\in \\{-1,+1\\}$. Empirical risk (training error) at round $T$ is the average of the $0$-$1$ losses $\\frac{1}{m}\\sum_{i=1}^m \\mathbf{1}\\{y_i \\neq \\mathrm{sign}(F_T(x_i))\\}$, where $\\mathrm{sign}(z)=+1$ if $z \\ge 0$ and $-1$ otherwise.\n- The base hypothesis class $\\mathcal{H}$ is the set of univariate decision stumps $h_{s,p}(x) = \\mathrm{sign}(p(x-s))$, where $s \\in \\mathbb{R}$ is a threshold and $p \\in \\{-1,+1\\}$ is a polarity. The weighted empirical error of $h$ under weights $D$ on the sample is $\\sum_{i=1}^m D_i \\mathbf{1}\\{h(x_i)\\neq y_i\\}$.\n- AdaBoost constructs a weighted vote $F_T(x)=\\sum_{t=1}^T \\alpha_t h_t(x)$ by iteratively selecting $h_t \\in \\mathcal{H}$ to minimize the current weighted error and setting $\\alpha_t=\\tfrac{1}{2}\\ln\\!\\big(\\frac{1-\\varepsilon_t}{\\varepsilon_t}\\big)$, where $\\varepsilon_t$ is the weighted error of $h_t$, and updating the weights $D_i \\propto D_i \\exp(-\\alpha_t y_i h_t(x_i))$. The final classifier is $\\mathrm{sign}(F_T(x))$.\n- The normalized margin of example $i$ at round $T$ is $\\rho_i(T)=\\frac{y_i F_T(x_i)}{\\sum_{t=1}^T |\\alpha_t|}$, which lies in $[-1,+1]$ for $T \\ge 1$.\n- You will use the following well-tested margin distribution generalization bound: for any $\\theta \\in (0,1]$, with probability at least $1-\\delta$ over the draw of the sample, the true classification error of $\\mathrm{sign}(F_T)$ is at most\n$$\nB(T,\\theta) \\;=\\; \\frac{1}{m}\\sum_{i=1}^m \\mathbf{1}\\{\\rho_i(T) \\le \\theta\\} \\;+\\; \\sqrt{\\frac{ 2 d \\ln\\!\\big(\\frac{2 e m}{d}\\big) + \\ln\\!\\big(\\frac{2}{\\delta}\\big)}{m\\,\\theta^2}},\n$$\nwhere $d$ is the Vapnikâ€“Chervonenkis (VC) dimension of the base class $\\mathcal{H}$, $m$ is the sample size, $e$ is the base of the natural logarithm, and $\\delta \\in (0,1)$ is a confidence parameter. For decision stumps on the real line, you must use $d=1$. This bound serves as the SRM criterion when the structure is the number of rounds $T$.\n\nYour tasks:\n1. Implement AdaBoost using decision stumps as base learners, exactly as defined above, for a fixed maximum number of rounds $T_{\\max}$. To construct a decision stump, you must search over thresholds $s$ given by midpoints between sorted distinct training inputs and include two extreme thresholds strictly below the smallest input and strictly above the largest input. You must consider both polarities $p \\in \\{-1,+1\\}$. For any occurrence of $\\varepsilon_t=0$ or $\\varepsilon_t=1$, clip the value to stay strictly within $(0,1)$ to ensure $\\alpha_t$ is finite.\n2. For each $T \\in \\{1,2,\\dots,T_{\\max}\\}$, compute the empirical training error and the normalized margins $\\rho_i(T)$, $i \\in \\{1,\\dots,m\\}$.\n3. For the supplied grid $\\Theta$ of margin thresholds, compute $B(T,\\theta)$ for each $\\theta \\in \\Theta$ and define the SRM-selected $T_{\\mathrm{SRM}}$ as the smallest $T$ that minimizes $\\min_{\\theta \\in \\Theta} B(T,\\theta)$ over $T \\in \\{1,\\dots,T_{\\max}\\}$. Define the ERM-selected $T_{\\mathrm{ERM}}$ as the smallest $T$ that minimizes the empirical training error over $T \\in \\{1,\\dots,T_{\\max}\\}$.\n\nData and test suite:\n- Use two deterministic datasets on the real line. Define $m=20$ and inputs $x_i = -2.85 + 0.3\\, i$ for $i \\in \\{0,1,2,\\dots,19\\}$. Define labels $y_i = \\mathrm{sign}(\\sin(2.5\\, x_i))$, with the convention $\\mathrm{sign}(0)=+1$.\n- Dataset $\\mathrm{A}$ is $S_{\\mathrm{A}}=\\{(x_i,y_i)\\}_{i=1}^{20}$ as defined above.\n- Dataset $\\mathrm{B}$ is obtained by introducing label noise into $S_{\\mathrm{A}}$: flip the labels at $1$-based indices $3$ and $17$, i.e., replace $y_3$ by $-y_3$ and $y_{17}$ by $-y_{17}$, leaving the other labels unchanged.\n- Use the VC dimension $d=1$ for both datasets.\n- Use the margin threshold grid $\\Theta=\\{0.05,\\,0.1,\\,0.2,\\,0.3,\\,0.4\\}$.\n- Use the following three test cases, each specified as a tuple $(\\text{dataset}, T_{\\max}, \\delta)$:\n    - Case $1$: $(\\mathrm{A},\\,30,\\,0.1)$\n    - Case $2$: $(\\mathrm{A},\\,30,\\,0.01)$\n    - Case $3$: $(\\mathrm{B},\\,30,\\,0.1)$\n\nRequired final output format:\n- Your program must produce a single line of output containing a flat, comma-separated list of six integers enclosed in square brackets. The list must be $[T_{\\mathrm{SRM}}^{(1)}, T_{\\mathrm{ERM}}^{(1)}, T_{\\mathrm{SRM}}^{(2)}, T_{\\mathrm{ERM}}^{(2)}, T_{\\mathrm{SRM}}^{(3)}, T_{\\mathrm{ERM}}^{(3)}]$, where the superscript denotes the case index in the order listed above.\n\nNotes and constraints:\n- All mathematical computations must adhere to the definitions given above. Angles, if any, are in radians by default due to the use of $\\sin(\\cdot)$.\n- Ties for minima must always be broken by selecting the smallest $T$.\n- Your implementation must be fully deterministic.", "solution": "The user has provided a valid problem statement. All definitions, data, and parameters are self-contained, scientifically grounded in statistical learning theory, and computationally feasible. The problem is well-posed, with clear, unambiguous objectives and deterministic procedures, ensuring a unique solution exists. The minor discrepancy regarding the VC dimension of decision stumps (which is $d=2$, not $d=1$) is resolved by the problem's explicit directive to use $d=1$ in the provided formula, which is treated as a given parameter for the exercise.\n\nThe objective is to compare two strategies for model selection in the context of AdaBoost classification: Empirical Risk Minimization (ERM) and Structural Risk Minimization (SRM). The \"model\" is an AdaBoost classifier, and its complexity is determined by the number of boosting rounds, $T$. We are tasked with selecting the optimal number of rounds, $T_{\\mathrm{ERM}}$ and $T_{\\mathrm{SRM}}$, based on these two principles for three distinct test cases.\n\n**Theoretical Framework**\n\n1.  **Empirical Risk Minimization (ERM):** This principle advocates for selecting the model that best fits the training data. The \"risk\" is the training error, defined as the proportion of misclassified examples in the training sample $S=\\{(x_i,y_i)\\}_{i=1}^m$. For an AdaBoost classifier after $T$ rounds, whose output is $F_T(x) = \\sum_{t=1}^T \\alpha_t h_t(x)$, the training error is:\n    $$\n    R_{\\text{emp}}(T) = \\frac{1}{m}\\sum_{i=1}^m \\mathbf{1}\\{y_i \\neq \\mathrm{sign}(F_T(x_i))\\}\n    $$\n    where $\\mathrm{sign}(z)=+1$ if $z \\ge 0$ and $-1$ otherwise. The ERM choice, $T_{\\mathrm{ERM}}$, is the smallest integer $T \\in \\{1, \\dots, T_{\\max}\\}$ that minimizes this training error. ERM is prone to overfitting, as it may select an overly complex model that perfectly memorizes the training data, including noise, at the expense of generalization to unseen data.\n\n2.  **Structural Risk Minimization (SRM):** This principle provides a more robust approach by balancing the empirical risk with a \"structural\" complexity term. It aims to minimize a bound on the true (generalization) error, rather than just the training error. The problem provides a specific margin-based generalization bound to serve as the SRM criterion. With probability at least $1-\\delta$, the true error is bounded by:\n    $$\n    B(T,\\theta) = \\underbrace{\\frac{1}{m}\\sum_{i=1}^m \\mathbf{1}\\{\\rho_i(T) \\le \\theta\\}}_{\\text{Empirical Margin Error}} + \\underbrace{\\sqrt{\\frac{ 2 d \\ln\\!\\big(\\frac{2 e m}{d}\\big) + \\ln\\!\\big(\\frac{2}{\\delta}\\big)}{m\\,\\theta^2}}}_{\\text{Complexity Penalty}}\n    $$\n    Here, $\\rho_i(T) = \\frac{y_i F_T(x_i)}{\\sum_{t=1}^T |\\alpha_t|}$ is the normalized margin of example $i$. A positive margin indicates correct classification, and its magnitude reflects the confidence of the prediction. The first term in the bound is the fraction of training examples with a margin less than or equal to a threshold $\\theta \\in (0,1]$. A smaller margin indicates a less confident or incorrect classification. The second term is the complexity penalty, which depends on the VC dimension of the base hypothesis class ($d$, specified as $1$), the sample size ($m$), the confidence parameter ($\\delta$), and the margin threshold ($\\theta$). The SRM strategy requires a two-level minimization: first, for each $T$, we find the tightest bound by minimizing over the grid of possible margin thresholds $\\Theta$. Let this minimum be $R_{\\mathrm{SRM}}(T) = \\min_{\\theta \\in \\Theta} B(T,\\theta)$. Then, the SRM choice, $T_{\\mathrm{SRM}}$, is the smallest $T$ that minimizes $R_{\\mathrm{SRM}}(T)$ over the range $\\{1, \\dots, T_{\\max}\\}$. This procedure penalizes models that achieve low training error by placing many examples very close to the decision boundary (i.e., having small margins), thereby promoting better generalization.\n\n**Algorithmic Implementation**\n\nThe solution is implemented via a deterministic, step-by-step procedure for each test case $(\\text{dataset}, T_{\\max}, \\delta)$.\n\n1.  **Data Generation:** The two required datasets, $S_A$ (clean) and $S_B$ (noisy), are constructed based on the specified functions. The sample size is $m=20$, inputs are $x_i = -2.85 + 0.3i$ for $i \\in \\{0, \\dots, 19\\}$, and labels are $y_i = \\mathrm{sign}(\\sin(2.5 x_i))$. For Dataset B, the labels at 1-based indices $3$ and $17$ are flipped.\n\n2.  **AdaBoost Training and Analysis:** An iterative process runs for $T=1, \\dots, T_{\\max}$. In each round $t$:\n    a.  **Weak Learner Selection:** The optimal decision stump $h_t(x) = \\mathrm{sign}(p(x-s))$ is found. This involves searching over all specified thresholds $s$ (midpoints of sorted unique inputs, plus two extreme values) and both polarities $p \\in \\{-1, +1\\}$ to find the stump that minimizes the weighted empirical error $\\varepsilon_t = \\sum_{i=1}^m D_i^{(t)} \\mathbf{1}\\{h(x_i) \\neq y_i\\}$.\n    b.  **Weak Learner Weighting:** The weight $\\alpha_t$ for the selected stump $h_t$ is calculated as $\\alpha_t = \\frac{1}{2}\\ln(\\frac{1-\\varepsilon_t}{\\varepsilon_t})$. The value of $\\varepsilon_t$ is clipped to be strictly between $0$ and $1$ to ensure $\\alpha_t$ is finite.\n    c.  **Sample Weight Update:** The weights $D_i$ on the training samples are updated to prioritize misclassified points: $D_i^{(t+1)} \\propto D_i^{(t)} \\exp(-\\alpha_t y_i h_t(x_i))$, followed by normalization.\n    d.  **Incremental Analysis:** After round $t$, the cumulative classifier $F_t(x) = \\sum_{j=1}^t \\alpha_j h_j(x)$ is formed. The training error $R_{\\text{emp}}(t)$ and the SRM risk $R_{\\mathrm{SRM}}(t)$ are calculated and stored. This incremental approach avoids redundant computations.\n\n3.  **Model Selection:** After the loop completes, we have two vectors of performance metrics: `[R_emp(1), ..., R_emp(T_max)]` and `[R_SRM(1), ..., R_SRM(T_max)]`.\n    -   $T_{\\mathrm{ERM}}$ is determined as the smallest index $T$ corresponding to the minimum value in the empirical error vector.\n    -   $T_{\\mathrm{SRM}}$ is determined as the smallest index $T$ corresponding to the minimum value in the SRM risk vector.\n\nThis entire process is executed for each of the three test cases, and the resulting six integer values ($T_{\\mathrm{SRM}}$ and $T_{\\mathrm{ERM}}$ for each case) are collected for the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and compares Empirical Risk Minimization (ERM) and Structural Risk Minimization (SRM)\n    for early-stopped AdaBoost with decision stumps.\n    \"\"\"\n\n    # Define custom sign function as per problem statement: sign(z)=+1 for z >= 0, -1 otherwise.\n    def my_sign(arr: np.ndarray) -> np.ndarray:\n        return np.where(arr >= 0, 1.0, -1.0)\n\n    # Function to generate datasets as specified.\n    def generate_data(dataset_id: str, m: int) -> tuple[np.ndarray, np.ndarray]:\n        X = -2.85 + 0.3 * np.arange(m)\n        Y = my_sign(np.sin(2.5 * X))\n        if dataset_id == 'B':\n            # Flip labels at 1-based indices 3 and 17 (0-based 2 and 16).\n            Y[2] *= -1\n            Y[16] *= -1\n        return X, Y\n\n    # Function to predict using a single decision stump.\n    def predict_stump(stump: dict, X: np.ndarray) -> np.ndarray:\n        s = stump['s']\n        p = stump['p']\n        return my_sign(p * (X - s))\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        ('A', 30, 0.1),\n        ('A', 30, 0.01),\n        ('B', 30, 0.1),\n    ]\n\n    # Global parameters defined in the problem.\n    m = 20\n    # The problem mandates using d=1.\n    d = 1.0\n    theta_grid = np.array([0.05, 0.1, 0.2, 0.3, 0.4])\n    \n    final_results = []\n\n    for case in test_cases:\n        dataset_id, T_max, delta = case\n        \n        # 1. Generate data for the current case.\n        X, Y = generate_data(dataset_id, m)\n\n        # 2. Prepare for AdaBoost loop.\n        # Initialize sample weights.\n        D = np.ones(m) / m\n        \n        # Generate all possible thresholds once to optimize the inner loop.\n        sorted_unique_X = np.unique(X)\n        thresholds = (sorted_unique_X[:-1] + sorted_unique_X[1:]) / 2.0\n        # Add extreme thresholds.\n        thresholds = np.append(thresholds, [sorted_unique_X[0] - 1.0, sorted_unique_X[-1] + 1.0])\n\n        # Storage for results at each round T.\n        emp_errors_per_T = []\n        srm_risks_per_T = []\n        \n        # Cumulative classifier scores F_T(x_i) for each sample point.\n        F_T_X = np.zeros(m)\n        # Cumulative sum of |alpha_t|, used for margin normalization.\n        sum_abs_alpha = 0.0\n\n        # 3. Main AdaBoost loop. In each iteration t, we find the best weak learner,\n        #    and then compute the ERM and SRM criteria for the combined classifier of size T=t.\n        for T in range(1, T_max + 1):\n            # a. Find the best decision stump (weak learner).\n            min_error = float('inf')\n            best_stump = None\n\n            for p in [-1, 1]:\n                for s in thresholds:\n                    stump = {'s': s, 'p': p}\n                    h_preds = predict_stump(stump, X)\n                    # Calculate weighted error.\n                    error = np.sum(D[h_preds != Y])\n                    \n                    if error < min_error:\n                        min_error = error\n                        best_stump = stump\n            \n            # b. Calculate the weight alpha_t for the best stump.\n            epsilon_t = min_error\n            # Clip epsilon to avoid log(0) or division by zero, as per instructions.\n            epsilon_t = np.clip(epsilon_t, 1e-15, 1 - 1e-15)\n            alpha_t = 0.5 * np.log((1.0 - epsilon_t) / epsilon_t)\n\n            # c. Update sample weights D.\n            h_t_preds = predict_stump(best_stump, X)\n            D *= np.exp(-alpha_t * Y * h_t_preds)\n            D /= np.sum(D)\n\n            # d. Update cumulative scores and alpha sum for this round T.\n            F_T_X += alpha_t * h_t_preds\n            # alpha_t is non-negative since epsilon_t <= 0.5.\n            sum_abs_alpha += alpha_t \n\n            # e. Calculate ERM criterion for round T.\n            y_pred_T = my_sign(F_T_X)\n            emp_error_T = np.sum(y_pred_T != Y) / m\n            emp_errors_per_T.append(emp_error_T)\n\n            # f. Calculate SRM criterion for round T.\n            if sum_abs_alpha == 0:\n                srm_risks_per_T.append(float('inf'))\n                continue\n            \n            # Calculate normalized margins.\n            margins = (Y * F_T_X) / sum_abs_alpha\n\n            # Calculate the generalization bound for each theta.\n            srm_bound_numerator = 2.0 * d * np.log(2.0 * np.e * m / d) + np.log(2.0 / delta)\n            \n            bounds_for_T = []\n            for theta in theta_grid:\n                # Empirical margin error.\n                margin_violations = np.sum(margins <= theta) / m\n                # Complexity term from the bound.\n                complexity_term = np.sqrt(srm_bound_numerator / (m * theta**2))\n                bound = margin_violations + complexity_term\n                bounds_for_T.append(bound)\n            \n            # SRM risk for round T is the minimum of the bound over the theta grid.\n            srm_risk_T = np.min(bounds_for_T)\n            srm_risks_per_T.append(srm_risk_T)\n\n        # 4. Select T_ERM and T_SRM by finding the minimum of the respective criteria.\n        # np.argmin finds the first index of the minimum, satisfying the \"smallest T\" rule.\n        T_ERM = np.argmin(emp_errors_per_T) + 1\n        T_SRM = np.argmin(srm_risks_per_T) + 1\n        \n        final_results.extend([T_SRM, T_ERM])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "3118279"}]}