{"hands_on_practices": [{"introduction": "This practice dives into the core engine of many modern matrix completion methods: nuclear norm minimization solved via a proximal algorithm. You will implement the singular value thresholding (SVT) operator, which is the key step in this process, and observe firsthand how the regularization parameter $\\lambda$ acts as a \"knob\" to control the rank of the reconstructed matrix. This exercise provides a foundational understanding of how low-rank solutions are algorithmically encouraged and achieved [@problem_id:3168247].", "problem": "You are asked to design and analyze a proximal method for matrix completion that uses the nuclear norm regularizer. Your goal is to implement an iterative scheme driven by the proximal point principle and then investigate how the choice of the regularization weight affects the evolution of matrix rank across iterations. The program you produce must compute the proximal operator of the nuclear norm via Singular Value Decomposition (SVD) mechanics and evaluate rank shrinkage trajectories for different parameter values.\n\nStarting point and definitions:\n- Let $M \\in \\mathbb{R}^{m \\times n}$ be a partially observed matrix with an associated observation mask $\\Omega \\subseteq \\{1,\\dots,m\\} \\times \\{1,\\dots,n\\}$. Define the linear observation operator $P_{\\Omega}:\\mathbb{R}^{m \\times n} \\to \\mathbb{R}^{m \\times n}$ by $(P_{\\Omega}(X))_{ij} = X_{ij}$ if $(i,j)\\in\\Omega$ and $(P_{\\Omega}(X))_{ij}=0$ otherwise. Define the complementary operator $P_{\\Omega^c}(X) = X - P_{\\Omega}(X)$. The nuclear norm of a matrix $X$, denoted $\\|X\\|_*$, is the sum of its singular values.\n- The proximal operator of a proper, closed, convex function $f:\\mathbb{R}^{m \\times n}\\to\\mathbb{R}\\cup\\{+\\infty\\}$ at a point $Y$ with parameter $t0$ is defined by\n$$\n\\operatorname{prox}_{t f}(Y) = \\arg\\min_{X\\in\\mathbb{R}^{m\\times n}} \\left\\{ f(X) + \\frac{1}{2t}\\|X - Y\\|_F^2 \\right\\}.\n$$\n- Consider the composite objective $F(X) = \\frac{1}{2}\\|P_{\\Omega}(X - M)\\|_F^2 + \\lambda \\|X\\|_*$, where $\\lambda0$.\n\nTask:\n- Using only the above definitions and standard properties of convex functions and SVD, derive an iterative scheme that, at each iteration $k$, first enforces data consistency on the observed entries and then applies the proximal operator of the nuclear norm at the resulting point. Concretely, for a fixed $\\lambda$, starting from $X^0 = 0$ and for $k=0,1,2,\\dots,K-1$, perform the following two steps:\n  1. Data-consistency injection to form $Y^k$ by setting $Y^k_{ij} = M_{ij}$ if $(i,j)\\in\\Omega$ and $Y^k_{ij} = X^k_{ij}$ otherwise (i.e., $Y^k = P_{\\Omega}(M) + P_{\\Omega^c}(X^k)$).\n  2. Proximal step to obtain $X^{k+1} = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(Y^k)$, computed via singular value thresholding derived from first principles.\n- At each iteration $k$, compute the rank of $X^{k}$ as the number of singular values strictly greater than a fixed numerical tolerance (you must choose and state a sensible tolerance).\n\nInvestigation:\n- Implement the above iterative scheme and, for each choice of $\\lambda$, record the sequence of ranks $\\{\\operatorname{rank}(X^{1}), \\operatorname{rank}(X^{2}), \\dots, \\operatorname{rank}(X^{K})\\}$, where $K$ is the total number of iterations.\n\nData generation protocol (deterministic):\n- Set $m=20$ and $n=15$.\n- Construct a ground-truth low-rank matrix $X_{\\text{true}}\\in\\mathbb{R}^{20\\times 15}$ of rank $r_{\\text{true}}=3$ using orthonormal factors and singular values. Use fixed singular values $10$, $5$, and $3$, and generate orthonormal columns deterministically by applying the Gram–Schmidt procedure (via a standard QR factorization) to random Gaussian matrices with a fixed pseudorandom seed.\n- Generate the observation mask by independently sampling each entry as observed with probability $p=0.5$, using the same fixed pseudorandom seed.\n- Form the observed matrix $M$ by setting $M_{ij} = X_{\\text{true},ij} + \\varepsilon_{ij}$ for $(i,j)\\in\\Omega$, where $\\varepsilon_{ij}$ are independent Gaussian noises with mean $0$ and standard deviation $0.01$, and $M_{ij}=0$ for $(i,j)\\notin\\Omega$. Use the same fixed seed for noise generation.\n\nTest suite:\n- Use $K=12$ iterations and rank tolerance $10^{-8}$.\n- Evaluate the rank trajectory for three values of $\\lambda$: $\\lambda=0.0$ (boundary case), $\\lambda=1.0$ (moderate regularization), and $\\lambda=5.0$ (strong regularization).\n\nRequired final output format:\n- Your program must produce a single line of output containing the three rank trajectories (for the three $\\lambda$ values) as a comma-separated list of lists enclosed in square brackets, for example, $[[r_{1,1},\\dots,r_{1,12}],[r_{2,1},\\dots,r_{2,12}],[r_{3,1},\\dots,r_{3,12}]]$, where $r_{j,k}$ is an integer rank. No other text should be printed.\n\nNotes:\n- Angles and physical units are not involved in this problem.\n- The implementation must be self-contained and deterministic by using a fixed pseudorandom seed.\n- You must compute $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}$ via singular value thresholding derived from the definition of the proximal operator and properties of SVD; do not assume or use any pre-provided formula.", "solution": "The user-provided problem statement has been analyzed and validated.\n\n### Step 1: Extract Givens\n- **Matrices and Operators**:\n    - $M \\in \\mathbb{R}^{m \\times n}$: A partially observed matrix.\n    - $\\Omega \\subseteq \\{1,\\dots,m\\} \\times \\{1,\\dots,n\\}$: An observation mask.\n    - $P_{\\Omega}(X)$: Linear operator where $(P_{\\Omega}(X))_{ij} = X_{ij}$ if $(i,j)\\in\\Omega$ and $0$ otherwise.\n    - $P_{\\Omega^c}(X) = X - P_{\\Omega}(X)$.\n    - $\\|X\\|_*$: The nuclear norm of matrix $X$, which is the sum of its singular values.\n- **Proximal Operator Definition**:\n    - For a proper, closed, convex function $f$, $\\operatorname{prox}_{t f}(Y) = \\arg\\min_{X\\in\\mathbb{R}^{m \\times n}} \\left\\{ f(X) + \\frac{1}{2t}\\|X - Y\\|_F^2 \\right\\}$.\n- **Objective Function**:\n    - $F(X) = \\frac{1}{2}\\|P_{\\Omega}(X - M)\\|_F^2 + \\lambda \\|X\\|_*$, with $\\lambda0$.\n- **Iterative Scheme**:\n    - Initialization: $X^0 = 0$.\n    - For $k=0,1,2,\\dots,K-1$:\n        1. **Data-consistency injection**: $Y^k = P_{\\Omega}(M) + P_{\\Omega^c}(X^k)$.\n        2. **Proximal step**: $X^{k+1} = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(Y^k)$.\n- **Task**:\n    - Compute the sequence of ranks $\\{\\operatorname{rank}(X^{1}), \\operatorname{rank}(X^{2}), \\dots, \\operatorname{rank}(X^{K})\\}$ at each iteration. Rank is defined as the number of singular values greater than a specified tolerance.\n- **Data Generation Protocol**:\n    - Matrix dimensions: $m=20$, $n=15$.\n    - Ground-truth rank: $r_{\\text{true}}=3$.\n    - Ground-truth singular values: $\\{10, 5, 3\\}$.\n    - Ground-truth matrix construction: $X_{\\text{true}} = U \\operatorname{diag}(\\{10,5,3\\}) V^T$, where $U$ and $V$ have orthonormal columns generated via QR factorization of random Gaussian matrices using a fixed seed.\n    - Observation probability: $p=0.5$ for each entry, using a fixed seed.\n    - Observed matrix: $M = P_{\\Omega}(X_{\\text{true}} + \\varepsilon)$, where $\\varepsilon$ is Gaussian noise with mean $0$ and standard deviation $0.01$, generated using a fixed seed.\n- **Test Suite**:\n    - Number of iterations: $K=12$.\n    - Rank tolerance: $10^{-8}$.\n    - Regularization parameters: $\\lambda \\in \\{0.0, 1.0, 5.0\\}$.\n- **Output Format**:\n    - A single line `[[r_{1,1},\\dots,r_{1,12}],[r_{2,1},\\dots,r_{2,12}],[r_{3,1},\\dots,r_{3,12}]]`.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is rooted in the established field of convex optimization, specifically proximal methods for matrix completion. The concepts of nuclear norm, proximal operators, and singular value decomposition (SVD) are fundamental to this area. The algorithm described is a well-known iterative scheme for nuclear norm minimization.\n- **Well-Posed**: The problem is clearly defined. It provides all necessary parameters, an explicit iterative formula, a deterministic data generation protocol (by fixing seeds), and a precise format for the output. This ensures that a unique and meaningful solution can be computed.\n- **Objective**: The problem is stated in formal mathematical language, free of ambiguity, subjectivity, or opinion-based claims.\n\nThe problem statement is self-contained, consistent, and adheres to established scientific principles. There are no identified flaws.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution Derivation and Method\n\nThe core of the iterative scheme is the computation of the proximal operator of the nuclear norm, $X^{k+1} = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_*}(Y^k)$. The problem statement's notation suggests interpreting this as the proximal operator of the function $f(X) = \\lambda \\|X\\|_*$ with parameter $t=1$. Following the provided definition of a proximal operator, this corresponds to solving the following optimization problem:\n$$\nX^{k+1} = \\arg\\min_{X \\in \\mathbb{R}^{m \\times n}} \\left\\{ \\lambda\\|X\\|_* + \\frac{1}{2}\\|X - Y^k\\|_F^2 \\right\\}\n$$\nThis problem has a closed-form solution based on the Singular Value Decomposition (SVD) of $Y^k$. Let the SVD of $Y^k$ be $Y^k = U \\Sigma V^T$, where $U \\in \\mathbb{R}^{m \\times r}$ and $V \\in \\mathbb{R}^{n \\times r}$ are matrices with orthonormal columns, $\\Sigma = \\operatorname{diag}(\\sigma_1, \\sigma_2, \\dots, \\sigma_r)$ is a diagonal matrix of positive singular values $\\sigma_i  0$, and $r = \\operatorname{rank}(Y^k)$.\n\nThe nuclear norm and the Frobenius norm are unitarily invariant. This means for any orthonormal matrices $A$ and $B$, $\\|Z\\|_* = \\|A^T Z B\\|_*$ and $\\|Z\\|_F = \\|A^T Z B\\|_F$. Applying this property with $A=U$ and $B=V$, the optimization problem can be rewritten in terms of a transformed variable $\\hat{X} = U^T X V$:\n$$\n\\arg\\min_{\\hat{X} \\in \\mathbb{R}^{m \\times n}} \\left\\{ \\lambda\\|\\hat{X}\\|_* + \\frac{1}{2}\\|\\hat{X} - U^T Y^k V\\|_F^2 \\right\\}\n$$\nSince $Y^k = U \\Sigma V^T$, we have $U^T Y^k V = \\Sigma$. The problem simplifies to:\n$$\n\\arg\\min_{\\hat{X} \\in \\mathbb{R}^{m \\times n}} \\left\\{ \\lambda\\|\\hat{X}\\|_* + \\frac{1}{2}\\|\\hat{X} - \\Sigma\\|_F^2 \\right\\}\n$$\nIt can be shown (e.g., via von Neumann's trace inequality) that for a fixed diagonal matrix $\\Sigma$, the minimum is achieved when $\\hat{X}$ is also a diagonal matrix. Let $\\hat{X} = D = \\operatorname{diag}(d_1, d_2, \\dots, d_r)$. The singular values of $D$ are simply $|d_i|$. To minimize $\\|\\hat{X}\\|_*$, we should choose $d_i \\ge 0$. The problem then decouples into a set of independent scalar minimization problems, one for each singular value:\n$$\n\\min_{d_i \\ge 0} \\left\\{ \\lambda d_i + \\frac{1}{2}(d_i - \\sigma_i)^2 \\right\\} \\quad \\text{for } i=1, \\dots, r\n$$\nThe objective is a simple quadratic in $d_i$. Its derivative with respect to $d_i$ is $\\lambda + d_i - \\sigma_i$. Setting the derivative to zero gives a candidate solution $d_i = \\sigma_i - \\lambda$. Since we require $d_i \\ge 0$, the optimal solution is $d_i = \\max(0, \\sigma_i - \\lambda)$. This operation is known as the soft-thresholding operator, denoted $S_\\lambda(\\sigma_i) = (\\sigma_i - \\lambda)_+$.\n\nThus, the optimal diagonal matrix is $\\hat{X}_{\\text{opt}} = S_\\lambda(\\Sigma) = \\operatorname{diag}(S_\\lambda(\\sigma_1), \\dots, S_\\lambda(\\sigma_r))$. To find the final solution $X^{k+1}$, we transform back to the original coordinate system:\n$$\nX^{k+1} = U \\hat{X}_{\\text{opt}} V^T = U S_\\lambda(\\Sigma) V^T\n$$\nThis procedure is known as Singular Value Thresholding (SVT).\n\nThe complete algorithm is as follows:\n1.  **Initialize**: Generate the ground truth matrix $X_{\\text{true}}$, the observed matrix $M$, and the observation mask $\\Omega$ according to the specified deterministic protocol. Set $X^0 = 0$.\n2.  **Iterate**: For each specified value of $\\lambda$, and for $k=0, \\dots, K-1$:\n    a.  **Data Update**: Form the matrix $Y^k = M + P_{\\Omega^c}(X^k)$. This step preserves the known entries from $M$ while using the current estimate $X^k$ for the unknown entries.\n    b.  **SVD**: Compute the SVD of $Y^k_c$: $Y^k = U \\Sigma V^T$.\n    c.  **Thresholding**: Apply the soft-thresholding operator to the singular values: $\\Sigma_{\\text{thresh}} = S_\\lambda(\\Sigma)$, where the diagonal entries are $(\\Sigma_{\\text{thresh}})_{ii} = \\max(0, \\Sigma_{ii} - \\lambda)$.\n    d.  **Reconstruction**: Form the next iterate $X^{k+1} = U \\Sigma_{\\text{thresh}} V^T$.\n    e.  **Rank Calculation**: Compute $\\operatorname{rank}(X^{k+1})$ as the number of diagonal entries in $\\Sigma_{\\text{thresh}}$ that are greater than the tolerance $10^{-8}$.\n3.  **Store Results**: For each $\\lambda$, store the sequence of computed ranks $\\{\\operatorname{rank}(X^1), \\dots, \\operatorname{rank}(X^{12})\\}$. The final output will be a list containing these three sequences.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the proximal point algorithm for matrix completion and\n    evaluates rank trajectories for different regularization weights.\n    \"\"\"\n    # Parameters from the problem statement\n    m, n = 20, 15\n    r_true = 3\n    singular_values_true = np.array([10.0, 5.0, 3.0])\n    p_obs = 0.5\n    noise_std = 0.01\n    K = 12\n    rank_tol = 1e-8\n    lambda_vals = [0.0, 1.0, 5.0]\n    seed = 0\n\n    # Data generation protocol (deterministic)\n    rng = np.random.default_rng(seed)\n\n    # 1. Construct ground-truth low-rank matrix X_true\n    # Generate random matrices and use QR to get orthonormal columns\n    U_rand = rng.standard_normal((m, r_true))\n    U, _ = np.linalg.qr(U_rand)\n    \n    V_rand = rng.standard_normal((n, r_true))\n    V, _ = np.linalg.qr(V_rand)\n    V_t = V.T\n    \n    X_true = U @ np.diag(singular_values_true) @ V_t\n\n    # 2. Generate observation mask Omega\n    omega_mask = rng.random((m, n))  p_obs\n    omega_mask_c = ~omega_mask\n\n    # 3. Form the observed matrix M\n    noise = rng.normal(loc=0.0, scale=noise_std, size=(m, n))\n    M = np.zeros((m, n))\n    M[omega_mask] = (X_true + noise)[omega_mask]\n\n    all_rank_trajectories = []\n\n    # Investigation loop over lambda values\n    for lambda_val in lambda_vals:\n        rank_trajectory = []\n        X = np.zeros((m, n))\n\n        # Iterative scheme\n        for _ in range(K):\n            # 1. Data-consistency injection\n            Y = M + X * omega_mask_c\n\n            # 2. Proximal step\n            # Compute SVD of Y\n            try:\n                U_svd, s_svd, Vh_svd = np.linalg.svd(Y, full_matrices=False)\n            except np.linalg.LinAlgError:\n                # Handle cases where SVD might fail, though unlikely here\n                s_svd = np.array([])\n                U_svd = np.zeros((m, 0))\n                Vh_svd = np.zeros((0, n))\n\n            # Apply soft-thresholding to singular values\n            s_thresh = np.maximum(0, s_svd - lambda_val)\n            \n            # Reconstruct the matrix X_next\n            # Create a diagonal matrix of the correct size for multiplication\n            Sigma_thresh = np.zeros((len(s_thresh), len(s_thresh)))\n            np.fill_diagonal(Sigma_thresh, s_thresh)\n            \n            X_next = U_svd @ Sigma_thresh @ Vh_svd\n\n            # Compute rank of X_next\n            rank = np.sum(s_thresh  rank_tol)\n            rank_trajectory.append(rank)\n\n            # Update X for the next iteration\n            X = X_next\n            \n        all_rank_trajectories.append(rank_trajectory)\n\n    # Final print statement in the exact required format\n    inner_strs = [f\"[{','.join(map(str, r))}]\" for r in all_rank_trajectories]\n    final_output = f\"[{','.join(inner_strs)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3168247"}, {"introduction": "Real-world data often comes with hard constraints, such as movie ratings being between 1 and 5 stars or pixel values between 0 and 1. This practice moves beyond simple regularization to address such scenarios by introducing Projected Gradient Descent (PGD). You will implement a projection operator to enforce box constraints, comparing its performance against an unconstrained approach to understand how to effectively solve matrix completion problems while respecting known physical or logical limits of the data [@problem_id:3145715].", "problem": "You are given the task of formulating and solving a convex matrix completion objective with side constraints. Consider an unknown matrix $X \\in \\mathbb{R}^{m \\times n}$, a data matrix $M \\in \\mathbb{R}^{m \\times n}$ containing observed entries, and a binary mask $W \\in \\{0,1\\}^{m \\times n}$ indicating observed entries, where $W_{ij} = 1$ means that $M_{ij}$ is observed and $W_{ij} = 0$ otherwise. Define the objective function\n$$\nJ(X) = \\frac{1}{2}\\left\\| W \\odot (X - M) \\right\\|_F^2 \\;+\\; \\frac{\\lambda}{2}\\left\\| X \\right\\|_F^2,\n$$\nwhere $\\odot$ denotes elementwise multiplication and $\\left\\| \\cdot \\right\\|_F$ denotes the Frobenius norm. Impose the box constraints $X_{ij} \\in [0,1]$ for all $i$ and $j$.\n\nYour tasks are:\n\n- Starting from the definitions of the Frobenius norm and basic matrix calculus, derive the explicit gradient $\\nabla J(X)$ and the Euclidean projection $\\Pi_{[0,1]^{mn}}(Y)$ of an arbitrary matrix $Y \\in \\mathbb{R}^{m \\times n}$ onto the box $[0,1]^{mn}$.\n\n- Using the notion of Lipschitz continuity of the gradient for smooth convex functions, bound the gradient Lipschitz constant $L$ of $\\nabla J(X)$ in terms of $\\lambda$ and the linear operator induced by $W$. Use this bound to specify a fixed step size $\\eta = \\frac{1}{L}$ that guarantees convergence of gradient descent for the unconstrained problem and is valid for Projected Gradient Descent (PGD).\n\n- Implement two iterative schemes, both initialized at a given $X^{(0)}$:\n  - Projected Gradient Descent (PGD): $X^{(t+1)} = \\Pi_{[0,1]^{mn}}\\!\\left( X^{(t)} - \\eta \\, \\nabla J\\!\\left(X^{(t)}\\right) \\right)$.\n  - Unconstrained Gradient Descent (no projection): $\\tilde{X}^{(t+1)} = \\tilde{X}^{(t)} - \\eta \\, \\nabla J\\!\\left(\\tilde{X}^{(t)}\\right)$.\n  For both methods, use the same step size $\\eta = \\frac{1}{L}$ derived above and perform exactly $T$ iterations. For both methods, evaluate the same objective $J(\\cdot)$ as defined above, regardless of feasibility with respect to the box constraints.\n\n- Evaluate and report the final objective values $J\\!\\left(X^{(T)}\\right)$ and $J\\!\\left(\\tilde{X}^{(T)}\\right)$ for each of the test cases below. This quantifies the impact of projecting updates onto the box constraints on optimization performance under identical step sizes and iteration counts.\n\nUse the following test suite. In each case, the mask $W$ and matrices $M$ and $X^{(0)}$ are specified explicitly, along with $\\lambda$, the number of iterations $T$, and the step size rule $\\eta = \\frac{1}{L}$ using your bound for $L$.\n\n- Test case $1$ (mixed observations, interior initialization):\n  - Dimensions: $m = 2$, $n = 3$.\n  - Data:\n    $$\n    M = \\begin{bmatrix}\n    0.2  0.8  0.1 \\\\\n    0.5  0.0  0.9\n    \\end{bmatrix}, \\quad\n    W = \\begin{bmatrix}\n    1  1  0 \\\\\n    0  1  1\n    \\end{bmatrix}.\n    $$\n  - Regularization: $\\lambda = 0.1$.\n  - Initialization: $X^{(0)}$ has all entries equal to $0.5$.\n  - Iterations: $T = 50$.\n\n- Test case $2$ (all entries observed, boundary-seeking initialization to test projection effects):\n  - Dimensions: $m = 3$, $n = 3$.\n  - Data:\n    $$\n    M = \\begin{bmatrix}\n    1.0  0.0  0.7 \\\\\n    0.9  0.1  0.0 \\\\\n    1.0  0.0  0.3\n    \\end{bmatrix}, \\quad\n    W = \\mathbf{1}_{3 \\times 3}.\n    $$\n  - Regularization: $\\lambda = 0.05$.\n  - Initialization: $X^{(0)}$ has all entries equal to $1.5$.\n  - Iterations: $T = 30$.\n\n- Test case $3$ (no observations, pure regularization, feasibility enforced only by projection):\n  - Dimensions: $m = 3$, $n = 2$.\n  - Data:\n    $$\n    M = \\mathbf{0}_{3 \\times 2}, \\quad\n    W = \\mathbf{0}_{3 \\times 2}.\n    $$\n  - Regularization: $\\lambda = 0.6$.\n  - Initialization: $X^{(0)}$ has all entries equal to $0.9$.\n  - Iterations: $T = 10$.\n\nImplementation requirements:\n\n- Use $\\eta = \\frac{1}{L}$ where $L$ is your valid Lipschitz constant bound for $\\nabla J$. Use the same choice of $L$ consistently across test cases based on your bound.\n- For each test case $i \\in \\{1,2,3\\}$, run both methods for exactly $T$ iterations and record two floats: $J\\!\\left(X^{(T)}\\right)$ for PGD and $J\\!\\left(\\tilde{X}^{(T)}\\right)$ for the unprojected method.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as\n$$\n\\left[ J\\!\\left(X^{(T)}\\right)_{\\text{case }1},\\; J\\!\\left(\\tilde{X}^{(T)}\\right)_{\\text{case }1},\\; J\\!\\left(X^{(T)}\\right)_{\\text{case }2},\\; J\\!\\left(\\tilde{X}^{(T)}\\right)_{\\text{case }2},\\; J\\!\\left(X^{(T)}\\right)_{\\text{case }3},\\; J\\!\\left(\\tilde{X}^{(T)}\\right)_{\\text{case }3} \\right],\n$$\nwith each float rounded to exactly $6$ decimal places.", "solution": "We begin from first principles for smooth convex objectives and Euclidean projections.\n\n1) Gradient derivation. Write\n$$\nJ(X) = \\frac{1}{2}\\left\\| W \\odot (X - M) \\right\\|_F^2 + \\frac{\\lambda}{2}\\left\\| X \\right\\|_F^2.\n$$\nLet $A(X) \\equiv W \\odot (X - M)$. Using the identity for the Frobenius norm $\\left\\| Y \\right\\|_F^2 = \\langle Y, Y \\rangle = \\sum_{i,j} Y_{ij}^2$ and the fact that for a linear map $A$, the gradient of $\\frac{1}{2}\\left\\|A(X)\\right\\|_F^2$ is $A^{\\ast}(A(X))$, where $A^{\\ast}$ is the adjoint of $A$ under the Frobenius inner product, we compute the gradient term by term. The linear operator $X \\mapsto W \\odot X$ is self-adjoint, because for all $Y$ and $Z$, $\\langle W \\odot Y, Z \\rangle = \\sum_{i,j} W_{ij} Y_{ij} Z_{ij} = \\langle Y, W \\odot Z \\rangle$, so $A^{\\ast}(Y) = W \\odot Y$. Therefore,\n$$\n\\nabla_X \\left( \\frac{1}{2}\\left\\| W \\odot (X - M) \\right\\|_F^2 \\right) = W \\odot \\left( W \\odot (X - M) \\right) = W \\odot (X - M),\n$$\nbecause $W \\odot W = W$ elementwise with $W_{ij} \\in \\{0,1\\}$. The gradient of the Tikhonov regularization term is\n$$\n\\nabla_X \\left( \\frac{\\lambda}{2}\\left\\| X \\right\\|_F^2 \\right) = \\lambda X,\n$$\nby linearity and the identity $\\nabla_X \\left( \\frac{1}{2}\\left\\| X \\right\\|_F^2 \\right) = X$. Combining both yields\n$$\n\\nabla J(X) = W \\odot (X - M) + \\lambda X.\n$$\n\n2) Projection onto the box. The feasible set is the hyper-rectangle $[0,1]^{mn} = \\{ X \\in \\mathbb{R}^{m \\times n} : 0 \\le X_{ij} \\le 1 \\text{ for all } i,j \\}$. The Euclidean projection onto a Cartesian product of closed intervals is separable across coordinates: for each entry $Y_{ij}$, the projection minimizes $\\frac{1}{2}(x - Y_{ij})^2$ over $x \\in [0,1]$, which is achieved by clamping $Y_{ij}$ to the interval endpoints if it lies outside. Therefore,\n$$\n\\left[\\Pi_{[0,1]^{mn}}(Y)\\right]_{ij} = \\min\\!\\left\\{ 1,\\; \\max\\!\\left\\{ 0,\\; Y_{ij} \\right\\} \\right\\},\n$$\nthat is, elementwise clipping to the interval $[0,1]$.\n\n3) Lipschitz constant bound and step size. The gradient mapping can be written as\n$$\n\\nabla J(X) = \\left( \\mathcal{P}_{\\Omega} + \\lambda \\mathcal{I} \\right)(X) - \\mathcal{P}_{\\Omega}(M),\n$$\nwhere $\\mathcal{P}_{\\Omega}(Z) = W \\odot Z$ and $\\mathcal{I}$ is the identity operator. The Lipschitz constant $L$ of $\\nabla J$ equals the operator norm (spectral norm) of the linear part, namely $\\left\\| \\mathcal{P}_{\\Omega} + \\lambda \\mathcal{I} \\right\\|$. Since $\\mathcal{P}_{\\Omega}$ is a self-adjoint orthogonal projector (idempotent with eigenvalues in $\\{0,1\\}$), its operator norm is at most $1$. The identity has operator norm $1$, and by subadditivity of the operator norm for commuting self-adjoint operators,\n$$\nL \\le \\left\\| \\mathcal{P}_{\\Omega} \\right\\| + \\lambda \\left\\| \\mathcal{I} \\right\\| \\le 1 + \\lambda.\n$$\nThus, a valid and simple global bound is $L = 1 + \\lambda$, which guarantees that the fixed step size $\\eta = \\frac{1}{L}$ satisfies $\\eta \\in \\left(0, \\frac{2}{L}\\right)$ for gradient descent on this smooth convex quadratic and is admissible for Projected Gradient Descent (PGD) on the constrained problem.\n\n4) Algorithms and evaluation. With the derived components, the two updates are:\n- Projected Gradient Descent (PGD): $X^{(t+1)} = \\Pi_{[0,1]^{mn}}\\!\\left( X^{(t)} - \\eta \\left[ W \\odot (X^{(t)} - M) + \\lambda X^{(t)} \\right] \\right)$.\n- Unprojected Gradient Descent: $\\tilde{X}^{(t+1)} = \\tilde{X}^{(t)} - \\eta \\left[ W \\odot (\\tilde{X}^{(t)} - M) + \\lambda \\tilde{X}^{(t)} \\right]$.\n\nBoth methods use $\\eta = \\frac{1}{1 + \\lambda}$ for each test case’s $\\lambda$, are initialized at the specified $X^{(0)}$, and run for exactly $T$ iterations. For both, the objective value $J$ is evaluated using the same definition\n$$\nJ(Z) = \\frac{1}{2}\\left\\| W \\odot (Z - M) \\right\\|_F^2 + \\frac{\\lambda}{2}\\left\\| Z \\right\\|_F^2,\n$$\neven when $\\tilde{X}^{(t)}$ violates the box constraints. This directly quantifies the impact of projection under identical step sizes and iteration counts.\n\n5) Test suite coverage rationale.\n- Test case $1$ exercises partial observations with an interior start $X^{(0)}$ at $0.5$, demonstrating standard behavior without immediate saturation at the bounds.\n- Test case $2$ uses all observations and a boundary-seeking initialization $X^{(0)} = 1.5$ to activate projections frequently, highlighting the stabilizing effect of projection relative to unconstrained updates which may overshoot and temporarily increase error.\n- Test case $3$ removes the data fidelity term ($W = 0$), leaving pure Tikhonov regularization; PGD promptly drives entries toward the interior while respecting the box, whereas the unprojected method descends linearly toward the origin. With the conservative step choice $\\eta = \\frac{1}{1 + \\lambda}$, both converge smoothly.\n\n6) Output specification. For each test case $i \\in \\{1,2,3\\}$, record the two floats $J\\!\\left(X^{(T)}\\right)$ and $J\\!\\left(\\tilde{X}^{(T)}\\right)$, each rounded to exactly $6$ decimal places, and print a single line list ordered as\n$$\n\\left[ J\\!\\left(X^{(T)}\\right)_{\\text{case }1},\\; J\\!\\left(\\tilde{X}^{(T)}\\right)_{\\text{case }1},\\; J\\!\\left(X^{(T)}\\right)_{\\text{case }2},\\; J\\!\\left(\\tilde{X}^{(T)}\\right)_{\\text{case }2},\\; J\\!\\left(X^{(T)}\\right)_{\\text{case }3},\\; J\\!\\left(\\tilde{X}^{(T)}\\right)_{\\text{case }3} \\right].\n$$\n\nThe accompanying program implements these derivations exactly, uses the bound $L = 1 + \\lambda$ in all cases, performs the specified iterations, and prints the results in the required single-line format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef objective(X, M, W, lam):\n    # J(X) = 0.5 * || W * (X - M) ||_F^2 + 0.5 * lam * ||X||_F^2\n    R = W * (X - M)\n    return 0.5 * np.sum(R * R) + 0.5 * lam * np.sum(X * X)\n\ndef grad_J(X, M, W, lam):\n    # Gradient: W * (X - M) + lam * X\n    return W * (X - M) + lam * X\n\ndef projected_gradient_descent(M, W, lam, X0, T):\n    # Step size using bound L = 1 + lam\n    L = 1.0 + lam\n    eta = 1.0 / L\n    X = X0.copy()\n    for _ in range(T):\n        G = grad_J(X, M, W, lam)\n        X = X - eta * G\n        # Projection onto [0,1] elementwise\n        X = np.clip(X, 0.0, 1.0)\n    return X\n\ndef unconstrained_gradient_descent(M, W, lam, X0, T):\n    # Step size using bound L = 1 + lam\n    L = 1.0 + lam\n    eta = 1.0 / L\n    X = X0.copy()\n    for _ in range(T):\n        G = grad_J(X, M, W, lam)\n        X = X - eta * G\n    return X\n\ndef solve():\n    # Define the test cases from the problem statement.\n\n    # Test case 1\n    M1 = np.array([[0.2, 0.8, 0.1],\n                   [0.5, 0.0, 0.9]], dtype=float)\n    W1 = np.array([[1, 1, 0],\n                   [0, 1, 1]], dtype=float)\n    lam1 = 0.1\n    T1 = 50\n    X01 = np.full_like(M1, 0.5, dtype=float)\n\n    # Test case 2\n    M2 = np.array([[1.0, 0.0, 0.7],\n                   [0.9, 0.1, 0.0],\n                   [1.0, 0.0, 0.3]], dtype=float)\n    W2 = np.ones_like(M2, dtype=float)\n    lam2 = 0.05\n    T2 = 30\n    X02 = np.full_like(M2, 1.5, dtype=float)\n\n    # Test case 3\n    M3 = np.zeros((3, 2), dtype=float)\n    W3 = np.zeros((3, 2), dtype=float)\n    lam3 = 0.6\n    T3 = 10\n    X03 = np.full_like(M3, 0.9, dtype=float)\n\n    test_cases = [\n        (M1, W1, lam1, X01, T1),\n        (M2, W2, lam2, X02, T2),\n        (M3, W3, lam3, X03, T3),\n    ]\n\n    results = []\n    for (M, W, lam, X0, T) in test_cases:\n        X_proj = projected_gradient_descent(M, W, lam, X0, T)\n        X_unproj = unconstrained_gradient_descent(M, W, lam, X0, T)\n        J_proj = objective(X_proj, M, W, lam)\n        J_unproj = objective(X_unproj, M, W, lam)\n        results.append(f\"{J_proj:.6f}\")\n        results.append(f\"{J_unproj:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3145715"}, {"introduction": "The most powerful models often leverage domain-specific side information. This advanced practice explores how to embed knowledge of a network structure—like a road network connecting traffic sensors—directly into the matrix completion objective using a graph Laplacian regularizer. By implementing a more sophisticated alternating minimization solver, you will see how this graph-based regularization can \"borrow strength\" from neighbors to dramatically improve imputation accuracy, especially for sensors with very few observations [@problem_id:3145702].", "problem": "You are given a matrix completion task motivated by traffic sensor networks. Consider a sensor-by-time data matrix $X \\in \\mathbb{R}^{n \\times T}$ with missing entries indicated by a binary mask $M \\in \\{0,1\\}^{n \\times T}$, where $M_{i,t} = 1$ means $X_{i,t}$ is observed and $M_{i,t} = 0$ means it is missing. You will complete the matrix using a low-rank factorization model and evaluate how incorporating road-network topology through a graph regularizer affects the accuracy of peak-time predictions.\n\nFundamental base and definitions:\n- Matrix completion via low-rank factorization assumes $X \\approx U V^{\\top}$ with $U \\in \\mathbb{R}^{n \\times r}$ and $V \\in \\mathbb{R}^{T \\times r}$ for a chosen rank $r$.\n- The road network topology over sensors is encoded by an undirected weighted adjacency matrix $W \\in \\mathbb{R}_{\\ge 0}^{n \\times n}$, with $W_{i,j} = W_{j,i}$ and $W_{i,i} = 0$. The degree matrix is $D = \\mathrm{diag}(W \\mathbf{1})$, and the graph Laplacian is $L = D - W$.\n- The completion is posed as minimizing the penalized empirical risk\n$$\n\\min_{U,V} \\;\\; \\frac{1}{2} \\left\\| M \\odot \\left(X - U V^{\\top}\\right) \\right\\|_{F}^{2} + \\frac{\\lambda_{\\mathrm{U}}}{2}\\,\\mathrm{tr}\\!\\left(U^{\\top} L U\\right) + \\frac{\\lambda}{2}\\left(\\|U\\|_{F}^{2} + \\|V\\|_{F}^{2}\\right),\n$$\nwhere $\\odot$ denotes the Hadamard product, $\\lambda_{\\mathrm{U}} \\ge 0$ controls alignment with the graph topology, and $\\lambda  0$ is a ridge regularizer. When $\\lambda_{\\mathrm{U}} = 0$, this reduces to standard low-rank matrix factorization without graph regularization.\n\nTask:\n- Implement an alternating minimization solver that, for fixed $r$, $\\lambda_{\\mathrm{U}}$, and $\\lambda$, alternately updates $U$ and $V$ until a fixed number of iterations is reached. Use deterministic initialization and deterministic data generation as specified below.\n- After completing the matrix to obtain $\\widehat{X} = U V^{\\top}$, define for each sensor $i \\in \\{1,\\dots,n\\}$ its peak time index as $p_i^{\\star} = \\arg\\max_{t \\in \\{1,\\dots,T\\}} X_{i,t}$ for the ground-truth matrix, and $\\widehat{p}_i = \\arg\\max_{t \\in \\{1,\\dots,T\\}} \\widehat{X}_{i,t}$ for the completed matrix. In case of ties, take the smallest index (the default behavior of $\\arg\\max$).\n- Define the total peak-index error as $E(\\widehat{X}; X) = \\sum_{i=1}^{n} \\left| \\widehat{p}_i - p_i^{\\star} \\right|$.\n- For each test case below, compute the improvement in peak prediction obtained by graph-regularized factorization over the non-regularized baseline as an integer:\n$$\n\\Delta = E\\big(\\widehat{X}_{\\text{baseline}}; X\\big) - E\\big(\\widehat{X}_{\\text{graph}}; X\\big).\n$$\nPositive $\\Delta$ indicates that graph regularization improves peak prediction accuracy.\n\nData generation (deterministic and universal):\n- Dimensions: $n = 5$, $T = 8$, $r = 2$.\n- Ground truth factors:\n  - Sensor factors $U^{\\star} \\in \\mathbb{R}^{n \\times r}$: Let positions $p_i = \\frac{i-1}{n-1}$ for $i = 1,\\dots,n$. Set the first column as $U^{\\star}_{i,1} = p_i$ and the second column as $U^{\\star}_{i,2} = 0.5$ for all $i$.\n  - Time factors $V^{\\star} \\in \\mathbb{R}^{T \\times r}$: Let time indices $\\tau_t = t-1$ for $t = 1,\\dots,T$. Set $V^{\\star}_{t,1} = \\exp\\!\\left(-\\frac{1}{2} \\left(\\frac{\\tau_t - 5}{1.0}\\right)^{2}\\right)$ and $V^{\\star}_{t,2} = 0.2 \\cdot \\frac{\\tau_t}{T}$.\n- Ground truth matrix with small noise: $X = U^{\\star} (V^{\\star})^{\\top} + \\epsilon$, where $\\epsilon$ has independent entries drawn from a zero-mean Gaussian with standard deviation $0.01$ using a pseudo-random number generator with seed $s_{\\mathrm{truth}} = 1$.\n- Base observation mask $M^{\\mathrm{base}} \\in \\{0,1\\}^{n \\times T}$: draw independent uniform random numbers in $\\left[0,1\\right)$ with a pseudo-random number generator seed $s_{\\mathrm{mask}} = 0$, and set $M^{\\mathrm{base}}_{i,t} = 1$ if the draw is at least $0.4$ and $M^{\\mathrm{base}}_{i,t} = 0$ otherwise (so the expected observed fraction is $0.6$).\n- Deterministic initialization for the algorithm: initialize $U$ and $V$ with independent zero-mean Gaussian entries with standard deviation $0.1$ using pseudo-random number generator seed $s_{\\mathrm{init}} = 123$ at the start of each run.\n\nRoad network graphs:\n- Path graph on $n$ sensors: $W \\in \\mathbb{R}^{n \\times n}$ with $W_{i,i+1} = W_{i+1,i} = 1$ for $i \\in \\{1,\\dots,n-1\\}$ and $W_{i,j} = 0$ otherwise.\n- Null graph: $W = 0$ (so $L = 0$).\n\nAlgorithmic settings:\n- Use $\\lambda = 0.1$, iteration count $N_{\\mathrm{iter}} = 50$.\n- For the baseline, set $\\lambda_{\\mathrm{U}} = 0$; for the graph-regularized run, set $\\lambda_{\\mathrm{U}} = 5.0$.\n\nTest suite:\n- Case $1$ (happy path): $W$ equals the path graph, $M = M^{\\mathrm{base}}$.\n- Case $2$ (cold-start sensor): $W$ equals the path graph, $M$ equals $M^{\\mathrm{base}}$ but with the entire third row set to zero, i.e., $M_{3,t} = 0$ for all $t \\in \\{1,\\dots,T\\}$.\n- Case $3$ (fully observed): $W$ equals the path graph, $M$ is the all-ones matrix of shape $n \\times T$.\n- Case $4$ (no graph information): $W = 0$, $M = M^{\\mathrm{base}}$.\n\nProgram requirements:\n- Implement an alternating minimization solver that alternates between updating $V$ and $U$:\n  - The $V$-update must solve, for each time index, a ridge-regularized weighted least squares problem with weights from the corresponding column of $M$.\n  - The $U$-update must minimize the objective exactly with respect to $U$ given $V$, which entails solving a linear system coupling all sensor latent vectors via the graph Laplacian $L$.\n- For each test case, run the baseline and the graph-regularized solver starting from the same initialization, complete the matrix to obtain $\\widehat{X}$, compute the total peak-index error $E(\\widehat{X}; X)$, and report the integer improvement $\\Delta$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases $1$ through $4$, for example, $\\left[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4\\right]$.", "solution": "The problem requires the implementation of a graph-regularized matrix completion algorithm based on low-rank factorization. The goal is to evaluate the impact of a graph regularizer on the accuracy of predicting peak-time events in a simulated sensor network. The solution is found by alternating minimization.\n\nThe objective function to be minimized is the penalized empirical risk:\n$$\nJ(U, V) = \\frac{1}{2} \\left\\| M \\odot \\left(X - U V^{\\top}\\right) \\right\\|_{F}^{2} + \\frac{\\lambda_{\\mathrm{U}}}{2}\\,\\mathrm{tr}\\!\\left(U^{\\top} L U\\right) + \\frac{\\lambda}{2}\\left(\\|U\\|_{F}^{2} + \\|V\\|_{F}^{2}\\right)\n$$\nHere, $X \\in \\mathbb{R}^{n \\times T}$ is the data matrix, $M \\in \\{0,1\\}^{n \\times T}$ is the observation mask, $U \\in \\mathbb{R}^{n \\times r}$ and $V \\in \\mathbb{R}^{T \\times r}$ are the latent factors, $L \\in \\mathbb{R}^{n \\times n}$ is the graph Laplacian encoding sensor topology, and $\\lambda_{\\mathrm{U}}, \\lambda$ are regularization parameters. The term $\\mathrm{tr}\\!\\left(U^{\\top} L U\\right)$ encourages the latent factors of connected sensors to be similar.\n\nThe optimization problem is not jointly convex in $U$ and $V$. However, it is biconvex, meaning the problem is convex in $U$ for a fixed $V$, and convex in $V$ for a fixed $U$. This structure motivates an alternating minimization approach, where we iteratively solve for one factor matrix while keeping the other fixed.\n\n### Alternating Minimization Algorithm\n\nWe initialize $U$ and $V$ and then alternate between two main steps for a fixed number of iterations.\n\n#### 1. Update V (with U fixed)\nFor a fixed $U$, the objective function terms depending on $V$ are:\n$$\nJ(V) = \\frac{1}{2} \\left\\| M \\odot \\left(X - U V^{\\top}\\right) \\right\\|_{F}^{2} + \\frac{\\lambda}{2} \\|V\\|_{F}^{2} = \\frac{1}{2} \\sum_{t=1}^{T} \\sum_{i=1}^{n} M_{i,t} (X_{i,t} - \\mathbf{u}_i^{\\top} \\mathbf{v}_t)^2 + \\frac{\\lambda}{2} \\sum_{t=1}^{T} \\|\\mathbf{v}_t\\|_2^2\n$$\nwhere $\\mathbf{u}_i$ is the $i$-th row of $U$ and $\\mathbf{v}_t$ is the $t$-th row of $V$. This problem decouples by time index $t$. For each $t \\in \\{1, \\dots, T\\}$, we minimize:\n$$\nJ_t(\\mathbf{v}_t) = \\frac{1}{2} \\sum_{i=1}^{n} M_{i,t} (X_{i,t} - \\mathbf{u}_i^{\\top} \\mathbf{v}_t)^2 + \\frac{\\lambda}{2} \\|\\mathbf{v}_t\\|_2^2\n$$\nThis is a ridge-regularized weighted least squares problem. Let $M_t = \\mathrm{diag}(M_{1,t}, \\dots, M_{n,t})$ be a diagonal matrix of weights for time $t$. The problem for $\\mathbf{v}_t$ is equivalent to minimizing $\\frac{1}{2}\\|\\sqrt{M_t}(\\mathbf{x}_t - U\\mathbf{v}_t)\\|_2^2 + \\frac{\\lambda}{2}\\|\\mathbf{v}_t\\|_2^2$, where $\\mathbf{x}_t$ is the $t$-th column of $X$. Taking the gradient with respect to $\\mathbf{v}_t$ and setting it to zero gives the normal equations:\n$$\n\\left( U^{\\top} M_t U + \\lambda I_r \\right) \\mathbf{v}_t = U^{\\top} M_t \\mathbf{x}_t\n$$\nwhere $I_r$ is the $r \\times r$ identity matrix. The updated $\\mathbf{v}_t$ is found by solving this $r \\times r$ linear system. This can be expressed more explicitly as: let $\\mathcal{I}_t = \\{ i \\mid M_{i,t} = 1 \\}$ be the set of observed sensors at time $t$. Let $U_{\\mathcal{I}_t}$ be the submatrix of $U$ with rows indexed by $\\mathcal{I}_t$. The equation is:\n$$\n\\left( U_{\\mathcal{I}_t}^{\\top} U_{\\mathcal{I}_t} + \\lambda I_r \\right) \\mathbf{v}_t = U_{\\mathcal{I}_t}^{\\top} \\mathbf{x}_{\\mathcal{I}_t, t}\n$$\n\n#### 2. Update U (with V fixed)\nFor a fixed $V$, the objective function terms depending on $U$ are:\n$$\nJ(U) = \\frac{1}{2} \\left\\| M \\odot \\left(X - U V^{\\top}\\right) \\right\\|_{F}^{2} + \\frac{\\lambda_{\\mathrm{U}}}{2}\\,\\mathrm{tr}\\!\\left(U^{\\top} L U\\right) + \\frac{\\lambda}{2}\\|U\\|_{F}^{2}\n$$\nUnlike the V-update, this problem does not decouple by sensor index $i$ because the graph regularizer $\\mathrm{tr}(U^{\\top} L U) = \\sum_{i,j} L_{ij} \\mathbf{u}_i^{\\top} \\mathbf{u}_j$ couples the rows of $U$. We must solve for all rows of $U$ simultaneously. Taking the gradient with respect to a single row $\\mathbf{u}_i$ and setting it to zero yields:\n$$\n\\left( \\sum_{t=1}^{T} M_{i,t} \\mathbf{v}_t \\mathbf{v}_t^{\\top} + \\lambda I_r \\right) \\mathbf{u}_i + \\lambda_{\\mathrm{U}} \\sum_{j=1}^{n} L_{ij} \\mathbf{u}_j = \\sum_{t=1}^{T} M_{i,t} X_{i,t} \\mathbf{v}_t\n$$\nThis gives a system of $n$ coupled linear equations for the $n$ vectors $\\mathbf{u}_1, \\dots, \\mathbf{u}_n$. We can express this as a single large linear system. Let $\\mathbf{u} = \\mathrm{vec}(U) \\in \\mathbb{R}^{nr}$ be the vector obtained by stacking the rows of $U$. The system takes the form $\\mathcal{A} \\mathbf{u} = \\mathbf{b}$, where $\\mathcal{A}$ is an $nr \\times nr$ block matrix and $\\mathbf{b}$ is a vector of length $nr$. The $(i,j)$-th block of $\\mathcal{A}$ (an $r \\times r$ matrix) is given by:\n$$\n\\mathcal{A}_{ij} = \\delta_{ij} \\left( \\sum_{t=1}^{T} M_{i,t} \\mathbf{v}_t \\mathbf{v}_t^{\\top} + \\lambda I_r \\right) + \\lambda_{\\mathrm{U}} L_{ij} I_r\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta. The $i$-th block of the vector $\\mathbf{b}$ is $\\mathbf{b}_i = \\sum_{t=1}^{T} M_{i,t} X_{i,t} \\mathbf{v}_t$. The matrix $\\mathcal{A}$ is symmetric and positive definite (since $L$ is positive semi-definite and $\\lambda  0$), ensuring a unique solution for $U$. The updated $U$ is obtained by solving this $nr \\times nr$ linear system and reshaping the resulting vector.\n\n### Evaluation\nAfter running the alternating minimization for a fixed number of iterations, the completed matrix is $\\widehat{X} = UV^{\\top}$. The performance is evaluated by comparing the predicted peak time indices, $\\widehat{p}_i = \\arg\\max_t \\widehat{X}_{i,t}$, against the ground-truth peak time indices, $p_i^{\\star} = \\arg\\max_t X_{i,t}$. The total peak-index error is $E(\\widehat{X}; X) = \\sum_{i=1}^{n} | \\widehat{p}_i - p_i^{\\star} |$. The final reported metric is the improvement $\\Delta$ from graph regularization, defined as the difference in error between a baseline model ($\\lambda_{\\mathrm{U}}=0$) and the graph-regularized model ($\\lambda_{\\mathrm{U}}0$): $\\Delta = E_{\\text{baseline}} - E_{\\text{graph}}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n# language: Python\n# version: 3.12\n# libraries:\n#   - name: numpy\n#     version: 1.23.5\n#   - name: scipy\n#     version: 1.11.4\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the matrix completion task for all test cases.\n    \"\"\"\n    # Problem Parameters\n    n, T, r = 5, 8, 2\n    lambda_reg = 0.1\n    n_iter = 50\n    lambda_U_graph = 5.0\n    s_truth, s_mask, s_init = 1, 0, 123\n\n    def alternating_minimization(X, M, L, lambda_U, n, T, r, lambda_reg, n_iter, seed_init):\n        \"\"\"\n        Implements the alternating minimization solver for graph-regularized matrix completion.\n        \"\"\"\n        rng_init = np.random.default_rng(seed=seed_init)\n        U = rng_init.normal(0, 0.1, size=(n, r))\n        V = rng_init.normal(0, 0.1, size=(T, r))\n\n        for _ in range(n_iter):\n            # 1. V-update (holding U fixed)\n            # This part solves a separate ridge regression problem for each time point t.\n            for t in range(T):\n                observed_sensors = M[:, t]\n                if not np.any(observed_sensors):\n                    # No observations at this time point, impute with zero vector\n                    V[t, :] = 0.0\n                    continue\n                \n                U_obs = U[observed_sensors, :]\n                X_obs = X[observed_sensors, t]\n                \n                # A_t = U_obs.T @ U_obs + lambda * I\n                A_t = U_obs.T.dot(U_obs) + lambda_reg * np.identity(r)\n                # b_t = U_obs.T @ X_obs_t\n                b_t = U_obs.T.dot(X_obs)\n                \n                # Solve A_t v_t = b_t\n                V[t, :] = np.linalg.solve(A_t, b_t)\n\n            # 2. U-update (holding V fixed)\n            # This part solves one large linear system for all of U.\n            A_u = lambda_U * np.kron(L, np.identity(r))\n            b_u = np.zeros(n * r)\n\n            for i in range(n):\n                observed_times = M[i, :]\n                if not np.any(observed_times):\n                    # Cold-start sensor: no observations\n                    C_i = lambda_reg * np.identity(r)\n                    b_i = np.zeros(r)\n                else:\n                    V_obs = V[observed_times, :]\n                    X_obs = X[i, observed_times]\n                    \n                    # C_i = V_obs.T @ V_obs + lambda * I\n                    C_i = V_obs.T.dot(V_obs) + lambda_reg * np.identity(r)\n                    # b_i = V_obs.T @ X_obs_i\n                    b_i = V_obs.T.dot(X_obs)\n                \n                # Fill the block matrix A_u and vector b_u\n                A_u[i*r:(i+1)*r, i*r:(i+1)*r] += C_i\n                b_u[i*r:(i+1)*r] = b_i\n            \n            # Solve A_u u = b_u and reshape to get U\n            u_vec = np.linalg.solve(A_u, b_u)\n            U = u_vec.reshape((n, r))\n        \n        return U.dot(V.T)\n\n    # --- Data Generation ---\n    # Ground truth factors U_star, V_star\n    p_i = np.linspace(0, 1, n)\n    U_star = np.zeros((n, r))\n    U_star[:, 0] = p_i\n    U_star[:, 1] = 0.5\n    \n    tau_t = np.arange(T, dtype=float)\n    V_star = np.zeros((T, r))\n    V_star[:, 0] = np.exp(-0.5 * ((tau_t - 5.0) / 1.0)**2)\n    V_star[:, 1] = 0.2 * tau_t / T\n    \n    # Ground truth matrix X with Gaussian noise\n    rng_truth = np.random.default_rng(seed=s_truth)\n    epsilon = rng_truth.normal(0, 0.01, size=(n, T))\n    X_true = U_star.dot(V_star.T) + epsilon\n    \n    # Base observation mask\n    rng_mask = np.random.default_rng(seed=s_mask)\n    M_base = rng_mask.uniform(0, 1, size=(n, T)) = 0.4\n    \n    # --- Graph and Test Case Definitions ---\n    # Path graph\n    W_path = np.zeros((n, n))\n    for i in range(n - 1):\n        W_path[i, i + 1] = 1\n        W_path[i + 1, i] = 1\n    \n    # Null graph\n    W_null = np.zeros((n, n))\n\n    def get_laplacian(W):\n        D = np.diag(np.sum(W, axis=1))\n        return D - W\n\n    L_path = get_laplacian(W_path)\n    L_null = get_laplacian(W_null)\n\n    # Function to modify mask for Case 2\n    def mod_m_case2(m):\n        m_mod = m.copy()\n        m_mod[2, :] = 0  # Sensor 3 is a cold-start sensor\n        return m_mod\n\n    test_cases = [\n        {'L': L_path, 'M': M_base},\n        {'L': L_path, 'M': mod_m_case2(M_base)},\n        {'L': L_path, 'M': np.ones((n, T), dtype=bool)},\n        {'L': L_null, 'M': M_base},\n    ]\n\n    p_star = np.argmax(X_true, axis=1)\n    results = []\n\n    # --- Main Loop ---\n    for case in test_cases:\n        M, L = case['M'], case['L']\n        \n        # Run baseline model (lambda_U = 0)\n        X_hat_base = alternating_minimization(X_true, M, L, lambda_U=0.0, \n                                              n=n, T=T, r=r, lambda_reg=lambda_reg, \n                                              n_iter=n_iter, seed_init=s_init)\n        p_hat_base = np.argmax(X_hat_base, axis=1)\n        E_base = np.sum(np.abs(p_hat_base - p_star))\n\n        # Run graph-regularized model\n        X_hat_graph = alternating_minimization(X_true, M, L, lambda_U=lambda_U_graph,\n                                               n=n, T=T, r=r, lambda_reg=lambda_reg, \n                                               n_iter=n_iter, seed_init=s_init)\n        p_hat_graph = np.argmax(X_hat_graph, axis=1)\n        E_graph = np.sum(np.abs(p_hat_graph - p_star))\n\n        delta = E_base - E_graph\n        results.append(int(delta))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3145702"}]}