{"hands_on_practices": [{"introduction": "The first step after fitting a logistic regression model is understanding what its coefficients tell us. Unlike in linear regression, the coefficients in a logistic model represent the change in the log-odds of the outcome for a one-unit change in the predictor variable. This foundational exercise [@problem_id:1931449] provides direct practice in this core interpretation, translating the abstract formula into a concrete numerical answer within a realistic scenario.", "problem": "A research group is studying the factors that influence the success of students in a notoriously difficult professional certification exam. They collect data from a sample of candidates, recording the number of hours they spent studying ($x$) and whether they passed ($Y=1$) or failed ($Y=0$) the exam.\n\nThey fit a simple logistic regression model to predict the probability of passing, $p = P(Y=1|x)$. The model is given by the equation for the log-odds of passing:\n$$ \\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x $$\nAfter fitting the model to the data, the estimated coefficients are $\\hat{\\beta}_0 = -5.2$ and $\\hat{\\beta}_1 = 0.115$.\n\nA student who has been studying for some amount of time decides to study for an additional 7 hours. Based on the fitted model, what is the resulting change in the log-odds of this student passing the exam? Express your answer as a number rounded to three significant figures.", "solution": "We are given a simple logistic regression model for the log-odds:\n$$\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_{0} + \\beta_{1} x.$$\nIf the study time increases from $x$ to $x + \\Delta x$, the change in the log-odds is\n$$\\Delta \\left[\\ln\\left(\\frac{p}{1-p}\\right)\\right] = \\left[\\beta_{0} + \\beta_{1}(x+\\Delta x)\\right] - \\left[\\beta_{0} + \\beta_{1}x\\right] = \\beta_{1}\\Delta x.$$\nUsing the estimated slope $\\hat{\\beta}_{1} = 0.115$ and $\\Delta x = 7$, the change is\n$$\\Delta \\left[\\ln\\left(\\frac{p}{1-p}\\right)\\right] = \\hat{\\beta}_{1} \\cdot 7 = 0.115 \\cdot 7 = 0.805.$$\nRounded to three significant figures, this is $0.805$.", "answer": "$$\\boxed{0.805}$$", "id": "1931449"}, {"introduction": "Real-world datasets often contain categorical predictors, such as subscription tiers or geographic regions, which cannot be directly plugged into a mathematical equation. This practice [@problem_id:1931482] introduces the standard technique for handling such data: dummy variable encoding. By correctly specifying a model with a multi-level categorical variable, you will learn to interpret coefficients relative to a baseline category and avoid common pitfalls like the dummy variable trap.", "problem": "A data scientist is building a model to predict customer churn for a subscription-based software service. The outcome of interest is a binary variable, $Y$, where $Y=1$ if a customer churns (cancels their subscription) and $Y=0$ if they do not. The only predictor variable available is the customer's `Subscription Tier`, which is a categorical variable with three distinct levels: 'Basic', 'Standard', and 'Premium'.\n\nThe data scientist decides to use a logistic regression model to estimate the probability of churn, $p = P(Y=1)$. To incorporate the categorical predictor, they use dummy variable encoding, choosing the 'Basic' tier as the reference category.\n\nLet $p$ be the probability of a customer churning. Which of the following equations correctly represents the logistic regression model for the log-odds of a customer churning, $\\ln\\left(\\frac{p}{1-p}\\right)$, based on their subscription tier?\n\nA. $\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X_{\\text{Standard}} + \\beta_2 X_{\\text{Premium}}$, where $X_{\\text{Standard}} = 1$ for the 'Standard' tier and 0 otherwise, and $X_{\\text{Premium}} = 1$ for the 'Premium' tier and 0 otherwise.\n\nB. $\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 Z$, where $Z=1$ for 'Basic', $Z=2$ for 'Standard', and $Z=3$ for 'Premium'.\n\nC. $p = \\beta_0 + \\beta_1 X_{\\text{Standard}} + \\beta_2 X_{\\text{Premium}}$, where $X_{\\text{Standard}} = 1$ for the 'Standard' tier and 0 otherwise, and $X_{\\text{Premium}} = 1$ for the 'Premium' tier and 0 otherwise.\n\nD. $\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X_{\\text{Basic}} + \\beta_2 X_{\\text{Standard}} + \\beta_3 X_{\\text{Premium}}$, where $X_{\\text{level}} = 1$ for the corresponding tier and 0 otherwise.\n\nE. $\\ln\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X_{\\text{Premium}}$, where $X_{\\text{Premium}} = 1$ if the tier is 'Premium' and 0 otherwise.", "solution": "We model a binary outcome $Y \\in \\{0,1\\}$ using logistic regression, which specifies the log-odds (logit) of $p=P(Y=1)$ as a linear function of predictors:\n$$\n\\ln\\left(\\frac{p}{1-p}\\right)=\\eta=\\beta_{0}+\\text{(linear combination of predictors)}.\n$$\nFor a categorical predictor with three levels and reference coding, we introduce $L-1=2$ dummy variables when $L=3$. With 'Basic' as the reference, define\n- $X_{\\text{Standard}}=1$ if the tier is 'Standard' and $0$ otherwise,\n- $X_{\\text{Premium}}=1$ if the tier is 'Premium' and $0$ otherwise,\nso that for 'Basic' both indicators are $0$.\n\nThe correct logistic regression model is then\n$$\n\\ln\\left(\\frac{p}{1-p}\\right)=\\beta_{0}+\\beta_{1}X_{\\text{Standard}}+\\beta_{2}X_{\\text{Premium}}.\n$$\nThis yields, by substitution:\n- For 'Basic': $X_{\\text{Standard}}=0$, $X_{\\text{Premium}}=0$, so $\\ln\\left(\\frac{p}{1-p}\\right)=\\beta_{0}$.\n- For 'Standard': $X_{\\text{Standard}}=1$, $X_{\\text{Premium}}=0$, so $\\ln\\left(\\frac{p}{1-p}\\right)=\\beta_{0}+\\beta_{1}$.\n- For 'Premium': $X_{\\text{Standard}}=0$, $X_{\\text{Premium}}=1$, so $\\ln\\left(\\frac{p}{1-p}\\right)=\\beta_{0}+\\beta_{2}$.\nThis matches the standard dummy-variable encoding with 'Basic' as the reference.\n\nEvaluating the options:\n- A matches exactly the dummy-variable logistic specification above, so it is correct.\n- B uses a single numeric code $Z \\in \\{1,2,3\\}$ and assumes a linear effect across ordered codes, which is not equivalent to dummy-variable encoding with 'Basic' as the reference and imposes an unwarranted linearity across categories.\n- C models $p$ linearly in predictors, not the log-odds; this is not logistic regression.\n- D includes three dummies plus an intercept; since $X_{\\text{Basic}}+X_{\\text{Standard}}+X_{\\text{Premium}}=1$, this causes perfect multicollinearity (the dummy variable trap), so it is not a valid specification.\n- E uses only one dummy for 'Premium', collapsing 'Standard' and 'Basic' together; this does not represent a three-level factor with 'Basic' as reference via full dummy encoding.\n\nTherefore, the correct choice is A.", "answer": "$$\\boxed{A}$$", "id": "1931482"}, {"introduction": "A high-performing model must do more than just classify correctly; its predicted probabilities must be reliable. This concept, known as calibration, is a cornerstone of model evaluation. This advanced exercise [@problem_id:3142180] guides you through implementing a formal goodness-of-fit test to assess model calibration. By comparing observed outcomes to expected outcomes across bins of predicted probabilities, you will learn to diagnose systematic model failures and gain a deeper appreciation for what makes a model truly trustworthy.", "problem": "Consider a binary outcome setting modeled by logistic regression, where for each observation indexed by $i \\in \\{1,\\dots,n\\}$ the model outputs a predicted probability $\\hat{p}_i \\in (0,1)$ and the outcome $y_i \\in \\{0,1\\}$. Under the logistic regression model, the outcome $y_i$ is distributed as a Bernoulli random variable with success probability $\\hat{p}_i$, and the observations are conditionally independent given the predictors. Goodness-of-fit assessment can be framed as testing whether the observed binary outcomes are consistent with the predicted probabilities across the range of $\\hat{p}_i$, focusing on calibration rather than discrimination.\n\nStarting from the Bernoulli model and the principle that sums of independent Bernoulli random variables have a mean equal to the sum of their probabilities and a variance equal to the sum of $p_i(1-p_i)$, design a grouped residual diagnostic that compares observed counts to their expected counts within bins formed by the predicted probabilities. Then, construct a test statistic that, under the null hypothesis of correct calibration, is approximately Chi-squared distributed with degrees-of-freedom determined by the number of bins and the parameter constraints of the logistic model. Use this to compute a tail probability (a p-value) indicating lack of fit.\n\nYour task is to implement a program that:\n- Computes the grouped Chi-squared goodness-of-fit statistic for logistic regression predictions by aggregating discrepancies between observed and expected counts of successes and failures across bins of predicted probabilities.\n- Designs two binning schemes:\n  1. Equal-count bins: sort the predicted probabilities $\\hat{p}_i$ ascending and partition into $G = 10$ contiguous groups with sizes as equal as possible. If $n$ is not divisible by $G$, distribute the remainder one by one among the first few groups so that no group is empty.\n  2. High-probability emphasis bins: sort the predicted probabilities $\\hat{p}_i$ ascending; allocate the lowest $80\\%$ of the observations into $B=6$ contiguous groups and the highest $20\\%$ into $T=6$ contiguous groups (total $G = B+T = 12$). If the number of observations in a region is fewer than the number of groups allocated to that region, reduce the number of groups in that region so that each group has at least one observation and the groups cover the region contiguously.\n- For each non-empty bin $g$, compute observed counts of successes $O_{g1}$ and failures $O_{g0}$, expected counts $E_{g1}$ and $E_{g0}$ based on $\\hat{p}_i$, and aggregate a Pearson-type discrepancy across bins to form a test statistic. Use the Chi-squared distribution with degrees-of-freedom equal to $G^\\star - 2$, where $G^\\star$ is the number of non-empty bins actually formed, to compute a p-value as the upper-tail probability of the observed test statistic. If any bin yields a zero expected count for a component, exclude that componentâ€™s term from the aggregate and use the number of bins with valid contributions to determine $G^\\star$.\n\nTest Suite. Implement and evaluate the p-values under both binning schemes for the following three datasets, each with $n = 60$:\n\n- Dataset A (approximately calibrated across the range):\n  1. Predicted probabilities: define $\\hat{p}_i$ as $60$ equally spaced values from $0.05$ to $0.95$ inclusive, sorted ascending.\n  2. Deterministic outcomes designed to approximate calibration via local aggregation: partition the sorted indices into contiguous blocks of size $5$. For each block $b$, compute $S_b = \\sum_{i \\in b} \\hat{p}_i$ and set $k_b = \\operatorname{round}(S_b)$ as an integer. Within block $b$, set $y_i = 1$ for the $k_b$ observations with the largest $\\hat{p}_i$ values in that block and $y_i = 0$ for the remaining observations.\n\n- Dataset B (systematic miscalibration at high predicted probabilities):\n  1. Predicted probabilities: concatenate $40$ values equally spaced from $0.05$ to $0.75$ with $20$ values equally spaced from $0.85$ to $0.95$, then sort ascending.\n  2. Deterministic outcomes: apply the same block construction with contiguous blocks of size $5$, but treat the low and high probability regions separately. For the low region ($\\hat{p}_i \\le 0.80$), use the same rule as Dataset A. For the high region ($\\hat{p}_i > 0.80$), compute $S_b$ and $k_b = \\operatorname{round}(S_b)$ for each block $b$, then set the number of successes to $\\max(0, k_b - 3)$ (i.e., reduce by $3$ compared to the rounded expectation), assigning $1$s to the highest $\\hat{p}_i$ within the block and $0$s to the remainder. This creates systematic underperformance in the high predicted probability region.\n\n- Dataset C (edge case with extremes but not degenerate):\n  1. Predicted probabilities: concatenate $30$ values equally spaced from $0.10$ to $0.20$ with $30$ values equally spaced from $0.80$ to $0.90$, then sort ascending.\n  2. Deterministic outcomes: apply the Dataset A construction with contiguous blocks of size $5$ across the entire sorted sequence.\n\nFor each dataset, compute the p-value under the equal-count binning and the high-probability emphasis binning. Use degrees-of-freedom $G^\\star - 2$ where $G^\\star$ is the number of non-empty bins formed under the corresponding scheme. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$[\\text{pA\\_equal},\\text{pA\\_high},\\text{pB\\_equal},\\text{pB\\_high},\\text{pC\\_equal},\\text{pC\\_high}]$,\nwhere each entry is a floating-point number.", "solution": "The problem is valid as it presents a well-defined, scientifically sound task based on established principles of statistical modeling and hypothesis testing. It is complete, internally consistent, and requires the implementation of a standard, albeit nuanced, goodness-of-fit test for logistic regression models.\n\nThe problem asks for the implementation of a grouped Chi-squared goodness-of-fit test, conceptually similar to the Hosmer-Lemeshow test, to assess the calibration of a logistic regression model. Calibration refers to the agreement between predicted probabilities and observed outcome frequencies. A well-calibrated model is one where, for a collection of observations with an average predicted probability of, say, $p$, the observed proportion of positive outcomes is also approximately $p$.\n\nThe core of the test is to partition the data into groups based on the predicted probabilities, $\\hat{p}_i$, and then to compare the observed number of successes (outcomes $y_i=1$) and failures (outcomes $y_i=0$) in each group to the numbers expected under the assumption that the model is perfectly calibrated.\n\nLet the total number of observations be $n$. We first sort the observations in ascending order of their predicted probabilities $\\hat{p}_i$. The data are then partitioned into $G$ bins or groups. For each bin $g \\in \\{1, \\dots, G\\}$, we define:\n- $n_g$: the number of observations in bin $g$.\n- $O_{g1}$: the observed count of successes ($y_i=1$) in bin $g$. $O_{g1} = \\sum_{i \\in g} y_i$.\n- $O_{g0}$: the observed count of failures ($y_i=0$) in bin $g$. $O_{g0} = n_g - O_{g1}$.\n\nUnder the null hypothesis ($H_0$) that the model is correctly calibrated, the probability of success for observation $i$ is indeed $\\hat{p}_i$. Since the outcome $y_i$ is modeled as a Bernoulli($\\hat{p}_i$) random variable, its expected value is $E[y_i] = \\hat{p}_i$. The expected count of successes in bin $g$ is the sum of the individual probabilities:\n- $E_{g1}$: the expected count of successes in bin $g$. $E_{g1} = \\sum_{i \\in g} \\hat{p}_i$.\n- $E_{g0}$: the expected count of failures in bin $g$. $E_{g0} = \\sum_{i \\in g} (1 - \\hat{p}_i) = n_g - E_{g1}$.\n\nThe discrepancy between observed and expected counts is aggregated across all bins using the Pearson Chi-squared statistic. This statistic sums the squared differences between observed and expected counts, each normalized by the expected count:\n$$ \\chi^2 = \\sum_{g=1}^{G^\\star} \\left( \\frac{(O_{g0} - E_{g0})^2}{E_{g0}} + \\frac{(O_{g1} - E_{g1})^2}{E_{g1}} \\right) $$\nHere, $G^\\star$ represents the number of bins with non-zero sizes. The problem statement guarantees that $\\hat{p}_i \\in (0,1)$, which ensures that for any non-empty bin, both $E_{g0}$ and $E_{g1}$ are strictly positive, preventing division by zero.\n\nUnder $H_0$, this $\\chi^2$ statistic is approximately distributed as a chi-squared random variable. The degrees of freedom for this distribution are specified as $G^\\star - 2$. The subtraction of $2$ is a convention that accounts for the loss of degrees of freedom from estimating model parameters (typically an intercept and one slope parameter) to generate the $\\hat{p}_i$ values. The p-value, which is the probability of observing a test statistic at least as extreme as the one computed if $H_0$ were true, is calculated as the upper-tail probability of the $\\chi^2_{G^\\star-2}$ distribution: $P(\\chi^2_{df=G^\\star-2} \\ge \\chi^2_{observed})$. A small p-value (e.g., less than $0.05$) suggests a significant lack of fit, leading to rejection of the null hypothesis of good calibration.\n\nThe problem specifies two distinct binning schemes to be implemented:\n1.  **Equal-count bins**: The sorted data are partitioned into $G=10$ groups of the most equal size possible. For $n=60$, this results in $10$ groups of exactly $6$ observations each.\n2.  **High-probability emphasis bins**: This scheme aims to provide finer resolution at the higher end of the probability scale. The lowest $80\\%$ of observations (ranked by $\\hat{p}_i$) are placed into $B=6$ groups, and the highest $20\\%$ are placed into $T=6$ groups, for a total of $G=B+T=12$ groups. For $n=60$, this means the first $48$ observations are split into $6$ groups of $8$, and the final $12$ observations are split into $6$ groups of $2$.\n\nThe three test datasets are constructed to evaluate the diagnostic under different calibration scenarios:\n- **Dataset A** is designed to be well-calibrated by construction, where the number of successes in local blocks of $\\hat{p}_i$ values is set to match the rounded sum of probabilities. We expect a high p-value, indicating no significant lack of fit.\n- **Dataset B** introduces a systematic miscalibration. In the high-probability region ($\\hat{p}_i > 0.80$), the number of successes is deliberately reduced relative to the expectation. This should be detected by the test, yielding a low p-value.\n- **Dataset C** uses a bimodal distribution of probabilities but is constructed with the same calibration rule as Dataset A. We expect a high p-value, demonstrating the test's robustness to the distribution of $\\hat{p}_i$ as long as calibration is maintained.\n\nThe implementation will proceed by first generating each dataset $(\\hat{p}_i, y_i)$, then for each dataset, applying both binning schemes, calculating the $\\chi^2$ statistic, and finally computing the corresponding p-value from the $\\chi^2$ distribution with the specified degrees of freedom.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import chi2\n\ndef generate_dataset(n, block_size, p_hat_gen, y_gen_rule):\n    \"\"\"\n    Generates a dataset with predicted probabilities and deterministic outcomes.\n\n    Args:\n        n (int): Number of observations.\n        block_size (int): Size of contiguous blocks for generating outcomes.\n        p_hat_gen (function): A function that returns the array of predicted probabilities.\n        y_gen_rule (function): A function that takes a block of p_hats and returns the number of successes.\n\n    Returns:\n        tuple: A tuple containing (p_hats, outcomes).\n    \"\"\"\n    p_hats = p_hat_gen(n)\n    sorted_indices = np.argsort(p_hats)\n    p_hats_sorted = p_hats[sorted_indices]\n    \n    outcomes_sorted = np.zeros(n, dtype=int)\n    num_blocks = n // block_size\n    \n    for i in range(num_blocks):\n        start_idx = i * block_size\n        end_idx = start_idx + block_size\n        p_hat_block = p_hats_sorted[start_idx:end_idx]\n        \n        k_b = y_gen_rule(p_hat_block, p_hats_sorted, start_idx)\n\n        if k_b > 0:\n            # Assign successes to observations with the highest p_hats in the block\n            outcomes_sorted[end_idx - k_b : end_idx] = 1\n\n    # Unsort outcomes to match original p_hats order\n    outcomes = np.zeros(n, dtype=int)\n    outcomes[sorted_indices] = outcomes_sorted\n    \n    return p_hats, outcomes\n\ndef get_dataset_A(n=60):\n    \"\"\"Generates Dataset A: approximately calibrated across the range.\"\"\"\n    p_hat_gen = lambda size: np.linspace(0.05, 0.95, size)\n    \n    def y_gen_rule(p_hat_block, _, __):\n        s_b = np.sum(p_hat_block)\n        return int(np.round(s_b))\n        \n    return generate_dataset(n, 5, p_hat_gen, y_gen_rule)\n\ndef get_dataset_B(n=60):\n    \"\"\"Generates Dataset B: systematic miscalibration at high probabilities.\"\"\"\n    def p_hat_gen(size):\n        low_n = int(size * (40/60))\n        high_n = size - low_n\n        p_low = np.linspace(0.05, 0.75, low_n)\n        p_high = np.linspace(0.85, 0.95, high_n)\n        return np.concatenate((p_low, p_high))\n    \n    def y_gen_rule(p_hat_block, p_hats_sorted, start_idx):\n        # The cutoff is p_hat > 0.80. The 40th observation (index 39) is = 0.75.\n        # The 41st (index 40) is >= 0.85. So blocks 0-7 are low, 8-11 are high.\n        is_high_prob_region = p_hats_sorted[start_idx] > 0.80\n        \n        s_b = np.sum(p_hat_block)\n        k_b_expected = int(np.round(s_b))\n        \n        if is_high_prob_region:\n            return max(0, k_b_expected - 3)\n        else:\n            return k_b_expected\n\n    return generate_dataset(n, 5, p_hat_gen, y_gen_rule)\n\ndef get_dataset_C(n=60):\n    \"\"\"Generates Dataset C: edge case with extremes.\"\"\"\n    def p_hat_gen(size):\n        low_n = size // 2\n        high_n = size - low_n\n        p_low = np.linspace(0.10, 0.20, low_n)\n        p_high = np.linspace(0.80, 0.90, high_n)\n        return np.concatenate((p_low, p_high))\n    \n    def y_gen_rule(p_hat_block, _, __):\n        s_b = np.sum(p_hat_block)\n        return int(np.round(s_b))\n\n    return generate_dataset(n, 5, p_hat_gen, y_gen_rule)\n\ndef calculate_p_value(p_hats, outcomes, binning_scheme):\n    \"\"\"\n    Computes the Chi-squared goodness-of-fit p-value for a given binning scheme.\n    \"\"\"\n    n = len(p_hats)\n    \n    # Sort data by predicted probability\n    sorted_indices = np.argsort(p_hats)\n    sorted_p_hats = p_hats[sorted_indices]\n    sorted_outcomes = outcomes[sorted_indices]\n\n    bin_indices = []\n    if binning_scheme == 'equal':\n        G = 10\n        base_size = n // G\n        remainder = n % G\n        \n        current_idx = 0\n        for i in range(G):\n            bin_size = base_size + (1 if i  remainder else 0)\n            bin_indices.append(slice(current_idx, current_idx + bin_size))\n            current_idx += bin_size\n\n    elif binning_scheme == 'high_prob':\n        B = 6\n        T = 6\n        \n        n_low_region = int(0.8 * n)\n        n_high_region = n - n_low_region\n\n        # Binning for the low-probability region\n        base_size_low = n_low_region // B\n        remainder_low = n_low_region % B\n        current_idx = 0\n        for i in range(B):\n            bin_size = base_size_low + (1 if i  remainder_low else 0)\n            if bin_size > 0:\n                bin_indices.append(slice(current_idx, current_idx + bin_size))\n            current_idx += bin_size\n\n        # Binning for the high-probability region\n        base_size_high = n_high_region // T\n        remainder_high = n_high_region % T\n        for i in range(T):\n            bin_size = base_size_high + (1 if i  remainder_high else 0)\n            if bin_size > 0:\n                bin_indices.append(slice(current_idx, current_idx + bin_size))\n            current_idx += bin_size\n    \n    G_star = 0\n    chi2_stat = 0.0\n\n    for slc in bin_indices:\n        p_hats_bin = sorted_p_hats[slc]\n        outcomes_bin = sorted_outcomes[slc]\n        \n        n_g = len(outcomes_bin)\n        if n_g == 0:\n            continue\n        \n        G_star += 1\n        \n        O_g1 = np.sum(outcomes_bin)\n        O_g0 = n_g - O_g1\n        \n        E_g1 = np.sum(p_hats_bin)\n        E_g0 = n_g - E_g1\n        \n        if E_g1 > 0:\n            chi2_stat += ((O_g1 - E_g1)**2) / E_g1\n        if E_g0 > 0:\n            chi2_stat += ((O_g0 - E_g0)**2) / E_g0\n\n    df = G_star - 2\n    if df = 0:\n        # This case is not expected with the problem parameters but is a safeguard.\n        # A chi-squared test is not well-defined for non-positive degrees of freedom.\n        return 1.0 \n    \n    p_value = chi2.sf(chi2_stat, df)\n    \n    return p_value\n\ndef solve():\n    \"\"\"Main function to run the analysis and print results.\"\"\"\n    datasets = [\n        get_dataset_A(),\n        get_dataset_B(),\n        get_dataset_C()\n    ]\n    \n    test_cases = []\n    for p, y in datasets:\n        test_cases.append({'p_hats': p, 'outcomes': y, 'binning': 'equal'})\n        test_cases.append({'p_hats': p, 'outcomes': y, 'binning': 'high_prob'})\n\n    results = []\n    for case in test_cases:\n        p_val = calculate_p_value(case['p_hats'], case['outcomes'], case['binning'])\n        results.append(p_val)\n    \n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3142180"}]}