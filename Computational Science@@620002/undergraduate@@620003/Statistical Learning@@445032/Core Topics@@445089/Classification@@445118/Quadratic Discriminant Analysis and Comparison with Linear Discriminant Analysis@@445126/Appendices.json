{"hands_on_practices": [{"introduction": "To truly understand the difference between LDA and QDA, we must start with the underlying mathematics. This first practice problem guides you through a direct derivation in a simple one-dimensional setting. By calculating the decision boundaries for QDA from first principles, you will see exactly where the 'quadratic' nature emerges and how it contrasts with the linear boundary produced by LDA [@problem_id:3164341].", "problem": "Two classes $Y \\in \\{1,2\\}$ have one-dimensional features $X \\in \\mathbb{R}$ with class-conditional densities modeled as Gaussian: $X \\mid Y=k \\sim \\mathcal{N}(\\mu_{k}, \\sigma_{k}^{2})$ for $k \\in \\{1,2\\}$. The prior probabilities are $\\pi_{1} = 0.7$ and $\\pi_{2} = 0.3$. The parameters are $\\mu_{1} = 0$, $\\sigma_{1}^{2} = 1$, $\\mu_{2} = 3$, and $\\sigma_{2}^{2} = 4$. Using only Bayes’ rule and the definition of the Gaussian density, do the following:\n- Derive the closed-form posterior $p(Y=1 \\mid X=x)$ for Quadratic Discriminant Analysis (QDA) in this one-dimensional setting and show that the log-odds is a quadratic function of $x$.\n- Obtain and simplify the explicit decision thresholds for QDA by solving the equation $p(Y=1 \\mid X=x) = p(Y=2 \\mid X=x)$; provide the two thresholds in closed form.\n- Explain briefly how unequal priors and unequal variances can yield asymmetric decision regions on the real line for QDA in this setting.\n- For contrast, derive the Linear Discriminant Analysis (LDA) threshold under the assumption of a common variance and equal priors, and provide the closed-form threshold for the given means.\n\nExpress the final thresholds as exact analytic expressions with no numerical rounding. Your final answer must be a single row matrix containing the two QDA thresholds (ordered from smaller to larger) followed by the single LDA threshold.", "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, and objective.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Classes: $Y \\in \\{1,2\\}$\n- Feature space: $X \\in \\mathbb{R}$ (one-dimensional)\n- Class-conditional densities: $X \\mid Y=k \\sim \\mathcal{N}(\\mu_{k}, \\sigma_{k}^{2})$ for $k \\in \\{1,2\\}$\n- Priors for QDA: $\\pi_{1} = 0.7$, $\\pi_{2} = 0.3$\n- Parameters for QDA: $\\mu_{1} = 0$, $\\sigma_{1}^{2} = 1$, $\\mu_{2} = 3$, $\\sigma_{2}^{2} = 4$\n- Task 1 (QDA): Derive the posterior $p(Y=1 \\mid X=x)$ and show the log-odds is a quadratic function of $x$.\n- Task 2 (QDA): Find the decision thresholds by solving $p(Y=1 \\mid X=x) = p(Y=2 \\mid X=x)$.\n- Task 3 (QDA): Explain how unequal priors and variances cause asymmetric decision regions.\n- Task 4 (LDA): Derive the LDA threshold assuming a common variance and equal priors, using the given means $\\mu_1 = 0$ and $\\mu_2 = 3$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, as it deals with standard Bayesian classification models (QDA and LDA) based on Gaussian distributions, a core topic in statistical learning. The problem is well-posed; all necessary parameters for the QDA part are provided. For the LDA part, the assumption of a common variance is made, and as will be shown, its specific value is not needed as it cancels from the derivation of the threshold. The problem is objective, using clear and unambiguous mathematical definitions. It does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A complete solution will be provided.\n\n### Solution Derivation\n\n**Part 1: QDA Posterior and Log-Odds**\nThe posterior probability of class $Y=1$ given an observation $X=x$ is given by Bayes' rule:\n$$p(Y=1 \\mid X=x) = \\frac{p(X=x \\mid Y=1) p(Y=1)}{p(X=x \\mid Y=1) p(Y=1) + p(X=x \\mid Y=2) p(Y=2)}$$\nLet $\\pi_k = p(Y=k)$ be the prior probability for class $k$, and let $f_k(x) = p(X=x \\mid Y=k)$ be the class-conditional probability density. For a Gaussian distribution, this is:\n$$f_k(x) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}} \\exp\\left(-\\frac{(x-\\mu_k)^2}{2\\sigma_k^2}\\right)$$\nThe posterior for class $1$ is:\n$$p(Y=1 \\mid X=x) = \\frac{\\pi_1 f_1(x)}{\\pi_1 f_1(x) + \\pi_2 f_2(x)}$$\nThe log-odds, or logit, is the natural logarithm of the ratio of the posterior probabilities:\n$$\\text{log-odds} = \\ln\\left(\\frac{p(Y=1 \\mid X=x)}{p(Y=2 \\mid X=x)}\\right) = \\ln\\left(\\frac{\\frac{\\pi_1 f_1(x)}{\\pi_1 f_1(x) + \\pi_2 f_2(x)}}{\\frac{\\pi_2 f_2(x)}{\\pi_1 f_1(x) + \\pi_2 f_2(x)}}\\right) = \\ln\\left(\\frac{\\pi_1 f_1(x)}{\\pi_2 f_2(x)}\\right)$$\nThis can be expanded as:\n$$\\text{log-odds} = \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right) + \\ln(f_1(x)) - \\ln(f_2(x))$$\nSubstituting the expression for the Gaussian log-density, where $\\ln(f_k(x)) = -\\frac{1}{2}\\ln(2\\pi\\sigma_k^2) - \\frac{(x-\\mu_k)^2}{2\\sigma_k^2}$:\n$$\\text{log-odds} = \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right) - \\frac{1}{2}\\ln(2\\pi\\sigma_1^2) - \\frac{(x-\\mu_1)^2}{2\\sigma_1^2} - \\left(-\\frac{1}{2}\\ln(2\\pi\\sigma_2^2) - \\frac{(x-\\mu_2)^2}{2\\sigma_2^2}\\right)$$\n$$\\text{log-odds} = \\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right) + \\frac{1}{2}\\ln\\left(\\frac{\\sigma_2^2}{\\sigma_1^2}\\right) - \\frac{(x-\\mu_1)^2}{2\\sigma_1^2} + \\frac{(x-\\mu_2)^2}{2\\sigma_2^2}$$\nTo show this is a quadratic function of $x$, we expand the terms involving $x$:\n$$- \\frac{x^2 - 2x\\mu_1 + \\mu_1^2}{2\\sigma_1^2} + \\frac{x^2 - 2x\\mu_2 + \\mu_2^2}{2\\sigma_2^2}$$\nGrouping by powers of $x$, we have:\n$$x^2 \\left(\\frac{1}{2\\sigma_2^2} - \\frac{1}{2\\sigma_1^2}\\right) + x \\left(\\frac{\\mu_1}{\\sigma_1^2} - \\frac{\\mu_2}{\\sigma_2^2}\\right) + C$$\nwhere $C$ contains all constant terms. Since the variances are unequal ($\\sigma_1^2 \\neq \\sigma_2^2$), the coefficient of the $x^2$ term, $\\frac{1}{2\\sigma_2^2} - \\frac{1}{2\\sigma_1^2}$, is non-zero. Therefore, the log-odds is a quadratic function of $x$.\n\n**Part 2: QDA Decision Thresholds**\nThe decision thresholds are the values of $x$ where the posterior probabilities are equal, which is equivalent to the log-odds being zero.\n$$\\ln\\left(\\frac{\\pi_1}{\\pi_2}\\right) + \\frac{1}{2}\\ln\\left(\\frac{\\sigma_2^2}{\\sigma_1^2}\\right) - \\frac{(x-\\mu_1)^2}{2\\sigma_1^2} + \\frac{(x-\\mu_2)^2}{2\\sigma_2^2} = 0$$\nWe substitute the given parameters: $\\pi_{1} = 0.7$, $\\pi_{2} = 0.3$, $\\mu_{1} = 0$, $\\sigma_{1}^{2} = 1$, $\\mu_{2} = 3$, $\\sigma_{2}^{2} = 4$.\n$$\\ln\\left(\\frac{0.7}{0.3}\\right) + \\frac{1}{2}\\ln\\left(\\frac{4}{1}\\right) - \\frac{(x-0)^2}{2(1)} + \\frac{(x-3)^2}{2(4)} = 0$$\n$$\\ln\\left(\\frac{7}{3}\\right) + \\frac{1}{2}\\ln(4) - \\frac{x^2}{2} + \\frac{x^2 - 6x + 9}{8} = 0$$\nSince $\\frac{1}{2}\\ln(4) = \\ln(\\sqrt{4}) = \\ln(2)$, the equation becomes:\n$$\\ln\\left(\\frac{7}{3}\\right) + \\ln(2) - \\frac{x^2}{2} + \\frac{x^2 - 6x + 9}{8} = 0$$\n$$\\ln\\left(\\frac{14}{3}\\right) - \\frac{4x^2}{8} + \\frac{x^2 - 6x + 9}{8} = 0$$\nMultiply by $8$:\n$$8\\ln\\left(\\frac{14}{3}\\right) - 4x^2 + x^2 - 6x + 9 = 0$$\n$$-3x^2 - 6x + \\left(9 + 8\\ln\\left(\\frac{14}{3}\\right)\\right) = 0$$\n$$3x^2 + 6x - \\left(9 + 8\\ln\\left(\\frac{14}{3}\\right)\\right) = 0$$\nWe solve this quadratic equation $ax^2 + bx + c = 0$ with $a=3$, $b=6$, and $c = -\\left(9 + 8\\ln\\left(\\frac{14}{3}\\right)\\right)$ using the quadratic formula $x = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$x = \\frac{-6 \\pm \\sqrt{6^2 - 4(3)\\left(-\\left(9 + 8\\ln\\left(\\frac{14}{3}\\right)\\right)\\right)}}{2(3)}$$\n$$x = \\frac{-6 \\pm \\sqrt{36 + 12\\left(9 + 8\\ln\\left(\\frac{14}{3}\\right)\\right)}}{6}$$\n$$x = \\frac{-6 \\pm \\sqrt{36 + 108 + 96\\ln\\left(\\frac{14}{3}\\right)}}{6}$$\n$$x = \\frac{-6 \\pm \\sqrt{144 + 96\\ln\\left(\\frac{14}{3}\\right)}}{6}$$\nWe can factor out $16$ from the term under the square root:\n$$x = \\frac{-6 \\pm \\sqrt{16\\left(9 + 6\\ln\\left(\\frac{14}{3}\\right)\\right)}}{6} = \\frac{-6 \\pm 4\\sqrt{9 + 6\\ln\\left(\\frac{14}{3}\\right)}}{6}$$\n$$x = -1 \\pm \\frac{2}{3}\\sqrt{9 + 6\\ln\\left(\\frac{14}{3}\\right)}$$\nThe two thresholds, ordered from smaller to larger, are:\n$$x_1 = -1 - \\frac{2}{3}\\sqrt{9 + 6\\ln\\left(\\frac{14}{3}\\right)}$$\n$$x_2 = -1 + \\frac{2}{3}\\sqrt{9 + 6\\ln\\left(\\frac{14}{3}\\right)}$$\n\n**Part 3: Asymmetric Decision Regions**\nThe decision regions are asymmetric for two main reasons in this QDA setting:\n1.  **Unequal Variances ($\\sigma_1^2 \\neq \\sigma_2^2$):** This is the fundamental reason for a quadratic boundary. The log-ratio of the Gaussian densities includes an $x^2$ term, resulting in the log-odds being a parabola. A quadratic equation $ax^2+bx+c=0$ generally has two distinct roots ($x_1, x_2$). In this problem, the coefficient of $x^2$ in the log-odds is negative ($\\frac{1}{2\\sigma_2^2} - \\frac{1}{2\\sigma_1^2} = \\frac{1}{8} - \\frac{1}{2} = -\\frac{3}{8}  0$), so the parabola opens downwards. This means class $1$ is predicted for $x \\in [x_1, x_2]$, a finite interval. Class $2$ is predicted for $x \\in (-\\infty, x_1) \\cup (x_2, \\infty)$, a disconnected region composed of two infinite intervals. This inherent difference in the geometric nature of the decision regions (one is a single finite segment, the other is not) constitutes a form of asymmetry.\n2.  **Unequal Priors ($\\pi_1 \\neq \\pi_2$):** The priors influence the constant term of the log-odds through the term $\\ln(\\pi_1 / \\pi_2)$. If the priors are unequal, this term is non-zero, causing a vertical shift of the log-odds parabola. This shift moves the decision boundaries $x_1$ and $x_2$, altering the size of the decision regions. For example, since $\\pi_1=0.7 > \\pi_2=0.3$, the log-prior term is positive, shifting the parabola upwards and thus widening the interval $[x_1, x_2]$ where class $1$ is favored compared to a scenario with equal priors.\n\n**Part 4: LDA Threshold**\nLinear Discriminant Analysis (LDA) assumes a common variance, $\\sigma_1^2 = \\sigma_2^2 = \\sigma^2$. For this part, we are also told to assume equal priors, $\\pi_1 = \\pi_2$.\nThe log-odds equation simplifies significantly. The term $\\ln(\\pi_1/\\pi_2) = \\ln(1) = 0$. The terms involving variance in the log-density ratio also cancel: $\\frac{1}{2}\\ln(\\sigma_2^2/\\sigma_1^2) = \\frac{1}{2}\\ln(1) = 0$. The log-odds equation becomes:\n$$\\text{log-odds} = - \\frac{(x-\\mu_1)^2}{2\\sigma^2} + \\frac{(x-\\mu_2)^2}{2\\sigma^2} = 0$$\nMultiplying by $2\\sigma^2$ (assuming $\\sigma^2 > 0$):\n$$-(x-\\mu_1)^2 + (x-\\mu_2)^2 = 0$$\n$$(x-\\mu_2)^2 = (x-\\mu_1)^2$$\nThis is a linear equation in $x$:\n$$x^2 - 2x\\mu_2 + \\mu_2^2 = x^2 - 2x\\mu_1 + \\mu_1^2$$\n$$-2x\\mu_2 + \\mu_2^2 = -2x\\mu_1 + \\mu_1^2$$\n$$2x\\mu_1 - 2x\\mu_2 = \\mu_1^2 - \\mu_2^2$$\n$$2x(\\mu_1 - \\mu_2) = (\\mu_1 - \\mu_2)(\\mu_1 + \\mu_2)$$\nAssuming $\\mu_1 \\neq \\mu_2$, we divide by $2(\\mu_1 - \\mu_2)$:\n$$x = \\frac{\\mu_1 + \\mu_2}{2}$$\nThe common variance $\\sigma^2$ indeed cancels. The threshold is simply the midpoint of the two means. Using the given means $\\mu_1 = 0$ and $\\mu_2 = 3$:\n$$x = \\frac{0 + 3}{2} = \\frac{3}{2}$$", "answer": "$$\n\\boxed{\\begin{pmatrix} -1 - \\frac{2}{3}\\sqrt{9 + 6\\ln\\left(\\frac{14}{3}\\right)}  -1 + \\frac{2}{3}\\sqrt{9 + 6\\ln\\left(\\frac{14}{3}\\right)}  \\frac{3}{2} \\end{pmatrix}}\n$$", "id": "3164341"}, {"introduction": "Theory becomes clearer with a compelling example, especially one that reveals the limitations of a simpler model. Here, we explore a special case where Linear Discriminant Analysis fails completely, demonstrating the unique power of QDA [@problem_id:3164271]. This exercise challenges you to reason about a scenario where classes are distinguished not by their central location (mean), but by their spread (variance).", "problem": "Consider a binary classification problem in statistical learning with two classes $Y \\in \\{0,1\\}$ and a single real-valued feature $X \\in \\mathbb{R}$. The class prior probabilities satisfy $\\mathbb{P}(Y=0)=\\mathbb{P}(Y=1)=\\tfrac{1}{2}$. The class-conditional distributions are Gaussian with equal means and different variances: $X \\mid (Y=0) \\sim \\mathcal{N}(0,1)$ and $X \\mid (Y=1) \\sim \\mathcal{N}(0,9)$. Assume the true class-conditional parameters and priors are known to the classifier. The Bayes decision rule selects the class that maximizes the posterior probability $\\mathbb{P}(Y=k \\mid X=x)$, which is proportional to the product of the class prior $\\mathbb{P}(Y=k)$ and the class-conditional density $p(x \\mid Y=k)$. Quadratic Discriminant Analysis (QDA) models each class with its own covariance, while Linear Discriminant Analysis (LDA) models all classes with a shared covariance.\n\nStarting from the definitions above and the Gaussian probability density function, derive the Bayes optimal decision rule in the $1$-dimensional setting described. Then reason about what QDA and LDA will do under these conditions. Which option is correct?\n\nA. Under equal priors, the Bayes optimal decision rule is to predict class $1$ if $\\lvert x \\rvert  t$ and class $0$ otherwise, where \n$$t \\;=\\; \\sqrt{\\frac{2\\,\\sigma_0^2\\,\\sigma_1^2}{\\sigma_1^2 - \\sigma_0^2}\\,\\ln\\!\\left(\\frac{\\sigma_1}{\\sigma_0}\\right)}\\,,$$\nwith $\\sigma_0^2=1$ and $\\sigma_1^2=9$, so $t=\\sqrt{\\tfrac{9}{4}\\ln 3}$. Quadratic Discriminant Analysis (QDA) recovers this Bayes rule when the true parameters are known. Linear Discriminant Analysis (LDA), with equal means and equal priors, cannot separate the classes by $x$ and effectively predicts a single class regardless of $x$.\n\nB. The Bayes optimal decision rule predicts class $0$ when $\\lvert x \\rvert$ is large, because smaller variance implies heavier tails and thus larger likelihood for large $\\lvert x \\rvert$.\n\nC. Linear Discriminant Analysis (LDA) yields a threshold at $x=0$ when the class means are equal at $0$, thereby achieving the Bayes optimal classifier in this setting.\n\nD. Under equal priors and equal means, the Bayes decision boundary is linear and given by predicting class $1$ if $xt$ and class $0$ otherwise, where $t=\\sqrt{2\\ln(\\sigma_1/\\sigma_0)}$; both QDA and LDA recover this boundary in the described setting.", "solution": "We begin from the Bayes decision principle: predict the class $k \\in \\{0,1\\}$ that maximizes the posterior probability $\\mathbb{P}(Y=k \\mid X=x)$. By Bayes’ theorem,\n$$\n\\mathbb{P}(Y=k \\mid X=x) \\;\\propto\\; \\mathbb{P}(Y=k)\\, p(x \\mid Y=k).\n$$\nWith equal priors $\\mathbb{P}(Y=0)=\\mathbb{P}(Y=1)=\\tfrac{1}{2}$, the decision reduces to comparing the class-conditional likelihoods:\n$$\n\\text{predict } Y=1 \\text{ if } p(x \\mid Y=1)  p(x \\mid Y=0), \\text{ and } Y=0 \\text{ otherwise}.\n$$\nFor a Gaussian $\\mathcal{N}(0,\\sigma_k^2)$, the density is\n$$\np(x \\mid Y=k) \\;=\\; \\frac{1}{\\sqrt{2\\pi}\\,\\sigma_k}\\,\\exp\\!\\left(-\\frac{x^{2}}{2\\sigma_k^{2}}\\right).\n$$\nConsider the log-likelihood ratio\n$$\n\\Lambda(x) \\;=\\; \\ln\\!\\frac{p(x \\mid Y=1)}{p(x \\mid Y=0)}\n\\;=\\; -\\ln \\sigma_1 + \\ln \\sigma_0 \\;-\\; \\frac{x^2}{2}\\left(\\frac{1}{\\sigma_1^2} - \\frac{1}{\\sigma_0^2}\\right).\n$$\nWith equal priors, predict $Y=1$ if $\\Lambda(x)  0$ and $Y=0$ if $\\Lambda(x)  0$. Solving $\\Lambda(x)  0$ for $\\lvert x \\rvert$ yields a threshold of the form\n$$\n\\lvert x \\rvert \\;\\; t \\quad\\text{with}\\quad\nt \\;=\\; \\sqrt{\\frac{2\\,\\sigma_0^2\\,\\sigma_1^2}{\\sigma_1^2 - \\sigma_0^2}\\,\\ln\\!\\left(\\frac{\\sigma_1}{\\sigma_0}\\right)}.\n$$\nTo see this, observe that when $\\sigma_1  \\sigma_0$, we have $\\frac{1}{\\sigma_1^2} - \\frac{1}{\\sigma_0^2}  0$, so the quadratic term in $x^2$ flips sign and large $\\lvert x \\rvert$ favor the larger-variance class, while small $\\lvert x \\rvert$ favor the smaller-variance class. Substituting $\\sigma_0^2=1$ and $\\sigma_1^2=9$ gives\n$$\nt \\;=\\; \\sqrt{\\frac{2\\cdot 1 \\cdot 9}{9-1}\\,\\ln(3)} \\;=\\; \\sqrt{\\frac{18}{8}\\,\\ln(3)} \\;=\\; \\sqrt{\\frac{9}{4}\\,\\ln(3)}.\n$$\nTherefore, the Bayes optimal rule under equal priors and means is:\n- predict $Y=1$ (the larger-variance class) if $\\lvert x \\rvert  t$,\n- predict $Y=0$ if $\\lvert x \\rvert \\le t$.\n\nQuadratic Discriminant Analysis (QDA) uses class-specific covariance parameters to form a quadratic discriminant. In the $1$-dimensional case with known parameters, its decision boundary coincides with the Bayes optimal rule derived above, because QDA maximizes the same posterior by modeling $p(x \\mid Y=k)$ with its own $\\sigma_k^2$.\n\nLinear Discriminant Analysis (LDA) assumes a common covariance across classes. In $1$ dimension, with a shared variance $\\sigma^2$ and equal means $\\mu_0=\\mu_1=0$, the class-conditional densities become identical for all $x$, so the posterior depends only on the priors:\n$$\np(x \\mid Y=0) = p(x \\mid Y=1) \\quad\\Rightarrow\\quad \\mathbb{P}(Y=0 \\mid X=x) = \\mathbb{P}(Y=0), \\quad \\mathbb{P}(Y=1 \\mid X=x) = \\mathbb{P}(Y=1).\n$$\nWith equal priors, this yields a tie for all $x$, so LDA cannot separate based on $x$ and will effectively predict a single class or break ties arbitrarily; it does not reproduce the Bayes optimal variance-based rule.\n\nOption-by-option analysis:\n- Option A: It correctly states the Bayes decision rule under equal priors as a threshold on $\\lvert x \\rvert$, with the correct general formula for $t$ and the correct numeric specialization $t=\\sqrt{\\tfrac{9}{4}\\ln 3}$ for $\\sigma_0^2=1$, $\\sigma_1^2=9$. It also correctly describes that QDA recovers this rule when parameters are known and that LDA, with equal means and equal priors, cannot separate by $x$ and effectively predicts a single class. Verdict: Correct.\n- Option B: It claims that smaller variance implies heavier tails and thus larger likelihood for large $\\lvert x \\rvert$. This is the opposite of the truth: a larger variance yields heavier tails. For large $\\lvert x \\rvert$, the larger-variance class has higher likelihood, so the Bayes rule assigns the larger-variance class, not the smaller-variance class. Verdict: Incorrect.\n- Option C: It asserts that LDA yields a threshold at $x=0$ and achieves Bayes optimality here. With equal means and shared variance, LDA produces identical class-conditional densities; with equal priors, the posterior is identical across classes for all $x$, so there is no information in $x$ to form a threshold, and LDA is not Bayes optimal in this variance-difference scenario. Verdict: Incorrect.\n- Option D: It asserts a linear boundary in $x$ and presents an incorrect threshold formula $t=\\sqrt{2\\ln(\\sigma_1/\\sigma_0)}$, missing the necessary scaling by the variances. The correct Bayes boundary is quadratic in $x$ (a threshold on $\\lvert x \\rvert$), and LDA does not recover it under equal means and priors. Verdict: Incorrect.", "answer": "$$\\boxed{A}$$", "id": "3164271"}, {"introduction": "Moving from one dimension to two allows us to explore more subtle differences in class structure, such as correlation. This final problem presents a scenario where two classes are identical in their means and marginal variances, differing only in how their features relate to each other [@problem_id:3164346]. You will discover how QDA leverages the full covariance structure to find a non-obvious, yet powerful, decision rule that is completely invisible to LDA.", "problem": "Consider a binary classification problem in $\\mathbb{R}^2$ with feature vector $x = (x_1,x_2)^\\top$ and class label $Y \\in \\{1,2\\}$. Suppose the class-conditional distributions are multivariate normal with equal priors, equal means, and equal marginal variances but different correlations:\n- $X \\mid Y=1 \\sim \\mathcal{N}(\\mu, \\Sigma_1)$ with $\\mu = (0,0)^\\top$ and $\\Sigma_1 = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$,\n- $X \\mid Y=2 \\sim \\mathcal{N}(\\mu, \\Sigma_2)$ with the same $\\mu = (0,0)^\\top$ and $\\Sigma_2 = \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix}$,\nwhere $\\rho \\in (0,1)$ (for concreteness, you may think of $\\rho = 0.8$), and class priors $\\pi_1 = \\pi_2 = 1/2$. Thus, both classes have the same mean and the same marginal variances, and they differ only in correlation structure (the off-diagonal elements of the covariance matrices).\n\nUsing only first principles about Bayes-optimal classification under multivariate normal models, and the modeling assumptions underlying Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA), determine which of the following statements are true.\n\nA. Under Quadratic Discriminant Analysis (QDA), the Bayes decision boundary between the two classes is the set of points satisfying $x_1 x_2 = 0$, which partitions the plane into the four quadrants; points with $x_1 x_2  0$ are assigned to class $1$, and those with $x_1 x_2  0$ to class $2$ (up to a null set on the axes).\n\nB. Under Linear Discriminant Analysis (LDA) with equal priors, the pooled covariance retains the off-diagonal information and yields a nontrivial linear separator along $x_1 = x_2$.\n\nC. Because the class means and marginal variances are equal, any LDA rule with equal priors assigns identical discriminant scores to all $x$ (up to ties), producing an expected error rate of $50\\%$.\n\nD. In this setup, the pooled covariance matrix used by LDA has off-diagonal entry equal to $0$, so LDA cannot exploit the difference in correlation between classes.\n\nE. The Bayes-optimal classifier (and thus QDA under correct model specification) achieves strictly less than $50\\%$ error, because each class concentrates probability mass in the quadrants consistent with the sign of its correlation.", "solution": "We start from the definition of the Bayes classifier: for two classes with class-conditional densities $p_k(x)$ and priors $\\pi_k$, the Bayes rule assigns $x$ to the class with larger posterior probability, equivalently larger $\\pi_k p_k(x)$. When $p_k$ are multivariate normal distributions and priors are equal, comparing $\\pi_k p_k(x)$ reduces to comparing $p_k(x)$ and hence to comparing their log-densities. Quadratic Discriminant Analysis (QDA) is the plug-in rule that uses class-specific covariance matrices, while Linear Discriminant Analysis (LDA) assumes a common covariance and uses the pooled estimate.\n\nBayes/QDA boundary for the given Gaussians. For $k \\in \\{1,2\\}$, with $\\mu_k = \\mu = 0$ and $\\Sigma_k$ as given, the log-density is\n$\\log p_k(x) = -\\tfrac{1}{2}\\log |\\Sigma_k| - \\tfrac{1}{2} x^\\top \\Sigma_k^{-1} x + \\text{const}$.\nWith equal priors, the decision boundary solves $\\log p_1(x) = \\log p_2(x)$, i.e.\n$-\\tfrac{1}{2}\\log |\\Sigma_1| - \\tfrac{1}{2} x^\\top \\Sigma_1^{-1} x = -\\tfrac{1}{2}\\log |\\Sigma_2| - \\tfrac{1}{2} x^\\top \\Sigma_2^{-1} x$,\nor equivalently\n$x^\\top (\\Sigma_2^{-1} - \\Sigma_1^{-1}) x = \\log |\\Sigma_2| - \\log |\\Sigma_1|$.\nHere $|\\Sigma_1| = 1 - \\rho^2 = |\\Sigma_2|$, hence $\\log |\\Sigma_2| - \\log |\\Sigma_1| = 0$, so the boundary simplifies to\n$x^\\top (\\Sigma_2^{-1} - \\Sigma_1^{-1}) x = 0$.\n\nWe compute $\\Sigma_k^{-1}$. For $\\Sigma(\\rho) = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$ with $|\\rho|1$, the inverse is\n$\\Sigma(\\rho)^{-1} = \\dfrac{1}{1 - \\rho^2} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix}$.\nThus\n$\\Sigma_1^{-1} = \\dfrac{1}{1 - \\rho^2} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix}, \\quad \\Sigma_2^{-1} = \\dfrac{1}{1 - \\rho^2} \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$,\nand hence\n$\\Sigma_2^{-1} - \\Sigma_1^{-1} = \\dfrac{1}{1 - \\rho^2} \\begin{pmatrix} 0  2\\rho \\\\ 2\\rho  0 \\end{pmatrix} = \\dfrac{2\\rho}{1 - \\rho^2} \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}$.\nTherefore, the decision criterion is based on the sign of $x^\\top (\\Sigma_2^{-1} - \\Sigma_1^{-1}) x = \\dfrac{4\\rho}{1 - \\rho^2} x_1 x_2$.\nThe Bayes rule with equal priors is to predict class 1 if $\\log p_1(x) > \\log p_2(x)$, which simplifies to $x^\\top (\\Sigma_2^{-1} - \\Sigma_1^{-1})x > 0$. Since $\\rho \\in (0,1)$, this is equivalent to $x_1 x_2 > 0$. So class 1 (positive correlation) is predicted for $x_1 x_2 > 0$ and class 2 (negative correlation) for $x_1 x_2  0$. The boundary is $x_1 x_2 = 0$, which corresponds to the axes.\n\nLDA under a common covariance. LDA assumes a common covariance $\\Sigma_p$, typically pooled across classes. With the two class covariances given, the pooled covariance is\n$\\Sigma_p = \\dfrac{1}{2}(\\Sigma_1 + \\Sigma_2) = \\dfrac{1}{2} \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix} + \\dfrac{1}{2} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}$.\nThus, the off-diagonal entries cancel to $0$. The LDA discriminant score for class $k$ has the form (up to an additive constant common to both classes) $\\delta_k(x) = x^\\top \\Sigma_p^{-1} \\mu_k - \\tfrac{1}{2} \\mu_k^\\top \\Sigma_p^{-1} \\mu_k + \\log \\pi_k$. With $\\mu_1 = \\mu_2 = 0$ and equal priors, we obtain $\\delta_1(x) = \\delta_2(x)$ for all $x$. Hence, LDA produces a tie everywhere and cannot discriminate; under any tie-breaking that does not use $x$, the expected error is $50\\%$ given equal priors.\n\nError comparison and intuition. The Bayes/QDA classifier separates by the sign of $x_1 x_2$, which aligns with the dependence structure: for a zero-mean bivariate normal with correlation $\\rho > 0$, the probability that $x_1$ and $x_2$ have the same sign is strictly greater than $1/2$; for $\\rho  0$, it is strictly less than $1/2$. A well-known identity for zero-mean bivariate normal gives $\\mathbb{P}(x_1 x_2 > 0) = \\tfrac{1}{2} + \\tfrac{1}{\\pi} \\arcsin(\\rho)$, which is strictly larger than $\\tfrac{1}{2}$ for $\\rho > 0$ and strictly smaller than $\\tfrac{1}{2}$ for $\\rho  0$. Therefore, the Bayes/QDA rule that assigns classes according to the sign of $x_1 x_2$ achieves strictly less than $50\\%$ error, while LDA is stuck at $50\\%$ for this construction.\n\nOption-by-option analysis:\n- Option A: Our derivation shows the Bayes/QDA boundary is $x_1 x_2 = 0$ and that class 1 is assigned when $x_1 x_2 > 0$ and class 2 when $x_1 x_2  0$. This matches the statement in the option exactly. Verdict: Correct.\n- Option B: The pooled covariance is $\\Sigma_p = I_2$, which discards off-diagonal information. Therefore, LDA does not produce a nontrivial linear boundary such as $x_1 = x_2$; in fact, with equal means and priors, it produces a tie everywhere and no separation. Verdict: Incorrect.\n- Option C: With $\\mu_1 = \\mu_2 = 0$, $\\Sigma_p = I_2$, and equal priors, $\\delta_1(x) = \\delta_2(x)$ for all $x$, so LDA ties everywhere and yields an expected error of $50\\%$. Verdict: Correct.\n- Option D: We computed $\\Sigma_p = \\tfrac{1}{2}(\\Sigma_1 + \\Sigma_2) = I_2$, whose off-diagonal entry is $0$. Hence LDA cannot use the correlation difference. Verdict: Correct.\n- Option E: As argued, the Bayes/QDA rule aligns with the sign structure induced by correlation and achieves strictly less than $50\\%$ error for any $\\rho \\in (0,1)$. Verdict: Correct.", "answer": "$$\\boxed{ACDE}$$", "id": "3164346"}]}