{"hands_on_practices": [{"introduction": "The coefficients in a multiple linear regression model quantify the relationship between each predictor and the response variable. This exercise uses the concept of partitioned matrices to derive a foundational result, the Frisch-Waugh-Lovell theorem, which provides a profound interpretation of these coefficients. You will demonstrate that the coefficient for a single predictor, say $\\beta_1$, represents the effect of that predictor on the response after the linear influences of all other predictors have been partialled out from both the predictor and the response [@problem_id:3146050].", "problem": "Consider multiple linear regression in matrix form with response vector $y \\in \\mathbb{R}^{n}$ and design matrix $X \\in \\mathbb{R}^{n \\times p}$ partitioned as $X = [X_{1}\\;X_{2}]$, where $X_{1} \\in \\mathbb{R}^{n \\times 1}$ is a single predictor of interest and $X_{2} \\in \\mathbb{R}^{n \\times q}$ contains an intercept and additional predictors. The ordinary least squares estimate is defined as the minimizer of the squared residual norm $\\|y - X\\beta\\|_{2}^{2}$ over $\\beta \\in \\mathbb{R}^{p}$. Let the projection matrix onto the column space of $X_{2}$ be $P_{X_{2}} = X_{2}(X_{2}^{\\top}X_{2})^{-1}X_{2}^{\\top}$ and the associated residual maker matrix be $M_{X_{2}} = I - P_{X_{2}}$. Starting from these definitions and the normal equations implied by the least squares criterion, derive an explicit expression for the ordinary least squares coefficient associated with $X_{1}$ that depends only on $X_{1}$, $y$, and $M_{X_{2}}$.\n\nTo stress-test the derivation under overlapping covariate structure, consider $n = 5$ observations with\n$x_{1} = \\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5\\end{pmatrix}$, $z = \\begin{pmatrix}1 \\\\ 3 \\\\ 3 \\\\ 5 \\\\ 5\\end{pmatrix}$, $1_{n} = \\begin{pmatrix}1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1\\end{pmatrix}$, $X_{2} = [1_{n}\\;z]$, and $y = \\begin{pmatrix}8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12\\end{pmatrix}$.\nUse your derived expression to compute the numerical value of the coefficient on $x_{1}$. No rounding is required. Finally, explain, in terms of conditional effects, what this coefficient represents when $x_{1}$ overlaps substantially with the columns of $X_{2}$.", "solution": "The problem is first validated by extracting the given information and assessing its scientific soundness, completeness, and objectivity.\n\n**Step 1: Extract Givens**\n- Response vector: $y \\in \\mathbb{R}^{n}$\n- Design matrix: $X \\in \\mathbb{R}^{n \\times p}$, partitioned as $X = [X_{1}\\;X_{2}]$\n- Partitions: $X_{1} \\in \\mathbb{R}^{n \\times 1}$ (predictor of interest), $X_{2} \\in \\mathbb{R}^{n \\times q}$ (intercept and other predictors). The total number of predictors is $p=1+q$.\n- OLS criterion: Minimize $\\|y - X\\beta\\|_{2}^{2}$ over $\\beta \\in \\mathbb{R}^{p}$.\n- Projection matrix for $X_2$: $P_{X_{2}} = X_{2}(X_{2}^{\\top}X_{2})^{-1}X_{2}^{\\top}$\n- Residual maker for $X_2$: $M_{X_{2}} = I - P_{X_{2}}$\n- Task 1: Derive an explicit expression for the OLS coefficient of $X_1$, denoted $\\hat{\\beta}_1$, in terms of $X_1$, $y$, and $M_{X_2}$.\n- Task 2: For $n=5$, compute $\\hat{\\beta}_1$ given the data:\n  - $X_{1} = x_{1} = \\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5\\end{pmatrix}$\n  - $z = \\begin{pmatrix}1 \\\\ 3 \\\\ 3 \\\\ 5 \\\\ 5\\end{pmatrix}$\n  - $1_{n} = \\begin{pmatrix}1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1\\end{pmatrix}$\n  - $X_{2} = [1_{n}\\;z]$\n  - $y = \\begin{pmatrix}8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12\\end{pmatrix}$\n- Task 3: Explain the interpretation of the coefficient in the context of conditional effects and covariate overlap.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is well-defined and scientifically grounded. It presents a standard theoretical derivation in linear models, known as the Frisch-Waugh-Lovell theorem, followed by a direct numerical application. The provided data and definitions are self-contained and consistent. The matrices in the numerical example are specified such that the necessary matrix inverses, $(X_{2}^{\\top}X_{2})^{-1}$ and $(X^{\\top}X)^{-1}$, exist, ensuring a well-posed problem. The language is objective and formal. No flaws are identified.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n**Derivation of the Expression for $\\hat{\\beta}_1$**\n\nThe multiple linear regression model is given by $y = X\\beta + \\epsilon$. The ordinary least squares (OLS) estimate $\\hat{\\beta}$ is the vector that minimizes the sum of squared residuals, $S(\\beta) = \\|y - X\\beta\\|_{2}^{2}$. The solution to this minimization problem satisfies the normal equations:\n$$\nX^{\\top}X\\hat{\\beta} = X^{\\top}y\n$$\nWe partition the design matrix $X$ and the coefficient vector $\\beta$ according to the problem statement:\n$$\nX = [X_{1}\\;X_{2}] \\quad \\text{and} \\quad \\beta = \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\end{pmatrix}\n$$\nwhere $\\beta_1$ is a scalar corresponding to the single predictor $X_1$, and $\\beta_2 \\in \\mathbb{R}^q$ corresponds to the predictors in $X_2$. The OLS estimates are denoted $\\hat{\\beta}_1$ and $\\hat{\\beta}_2$.\n\nSubstituting the partitioned forms into the normal equations yields a block matrix system:\n$$\n\\begin{pmatrix} X_1^{\\top} \\\\ X_2^{\\top} \\end{pmatrix} [X_1\\;X_2] \\begin{pmatrix} \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\end{pmatrix} = \\begin{pmatrix} X_1^{\\top}y \\\\ X_2^{\\top}y \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} X_1^{\\top}X_1 & X_1^{\\top}X_2 \\\\ X_2^{\\top}X_1 & X_2^{\\top}X_2 \\end{pmatrix} \\begin{pmatrix} \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\end{pmatrix} = \\begin{pmatrix} X_1^{\\top}y \\\\ X_2^{\\top}y \\end{pmatrix}\n$$\nThis block system expands into two equations:\n1. $X_1^{\\top}X_1 \\hat{\\beta}_1 + X_1^{\\top}X_2 \\hat{\\beta}_2 = X_1^{\\top}y$\n2. $X_2^{\\top}X_1 \\hat{\\beta}_1 + X_2^{\\top}X_2 \\hat{\\beta}_2 = X_2^{\\top}y$\n\nOur goal is to find an expression for $\\hat{\\beta}_1$. We can achieve this by solving the second equation for $\\hat{\\beta}_2$ and substituting it into the first. From equation (2), assuming $X_2$ has full column rank so that $(X_2^{\\top}X_2)^{-1}$ exists:\n$$\nX_2^{\\top}X_2 \\hat{\\beta}_2 = X_2^{\\top}y - X_2^{\\top}X_1 \\hat{\\beta}_1\n$$\n$$\n\\hat{\\beta}_2 = (X_2^{\\top}X_2)^{-1}(X_2^{\\top}y - X_2^{\\top}X_1 \\hat{\\beta}_1)\n$$\nNow, substitute this expression for $\\hat{\\beta}_2$ into equation (1):\n$$\nX_1^{\\top}X_1 \\hat{\\beta}_1 + X_1^{\\top}X_2 (X_2^{\\top}X_2)^{-1}(X_2^{\\top}y - X_2^{\\top}X_1 \\hat{\\beta}_1) = X_1^{\\top}y\n$$\nDistribute the terms:\n$$\nX_1^{\\top}X_1 \\hat{\\beta}_1 + X_1^{\\top}X_2(X_2^{\\top}X_2)^{-1}X_2^{\\top}y - X_1^{\\top}X_2(X_2^{\\top}X_2)^{-1}X_2^{\\top}X_1 \\hat{\\beta}_1 = X_1^{\\top}y\n$$\nUsing the definition of the projection matrix $P_{X_2} = X_2(X_2^{\\top}X_2)^{-1}X_2^{\\top}$, we can simplify the equation:\n$$\nX_1^{\\top}X_1 \\hat{\\beta}_1 + X_1^{\\top}P_{X_2}y - X_1^{\\top}P_{X_2}X_1 \\hat{\\beta}_1 = X_1^{\\top}y\n$$\nGroup the terms involving $\\hat{\\beta}_1$ on the left side and the other terms on the right side:\n$$\n(X_1^{\\top}X_1 - X_1^{\\top}P_{X_2}X_1) \\hat{\\beta}_1 = X_1^{\\top}y - X_1^{\\top}P_{X_2}y\n$$\nFactor out $X_1^{\\top}$ on both sides:\n$$\nX_1^{\\top}(I - P_{X_2})X_1 \\hat{\\beta}_1 = X_1^{\\top}(I - P_{X_2})y\n$$\nUsing the definition of the residual maker matrix $M_{X_2} = I - P_{X_2}$, we arrive at:\n$$\n(X_1^{\\top}M_{X_2}X_1) \\hat{\\beta}_1 = X_1^{\\top}M_{X_2}y\n$$\nSince $X_1$ is a single column vector, $X_1^{\\top}M_{X_2}X_1$ is a scalar. As long as $X_1$ is not in the column space of $X_2$ (i.e., $M_{X_2}X_1 \\neq 0$), this scalar is non-zero, and we can solve for $\\hat{\\beta}_1$:\n$$\n\\hat{\\beta}_1 = (X_1^{\\top}M_{X_2}X_1)^{-1} (X_1^{\\top}M_{X_2}y)\n$$\nThis is the desired expression.\n\n**Numerical Calculation**\n\nWe are given:\n$X_{1} = \\begin{pmatrix}1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5\\end{pmatrix}$, $X_{2} = \\begin{pmatrix} 1 & 1 \\\\ 1 & 3 \\\\ 1 & 3 \\\\ 1 & 5 \\\\ 1 & 5 \\end{pmatrix}$, $y = \\begin{pmatrix}8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12\\end{pmatrix}$.\n\nWe need to compute the quantities $\\tilde{x}_1 = M_{X_2}X_1$ and $\\tilde{y} = M_{X_2}y$. These are the residuals from regressing $X_1$ and $y$ on $X_2$, respectively.\nFirst, compute $X_2^{\\top}X_2$ and its inverse:\n$$\nX_2^{\\top}X_2 = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\\\ 1 & 3 & 3 & 5 & 5 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ 1 & 3 \\\\ 1 & 3 \\\\ 1 & 5 \\\\ 1 & 5 \\end{pmatrix} = \\begin{pmatrix} 5 & 17 \\\\ 17 & 69 \\end{pmatrix}\n$$\nThe determinant is $\\det(X_2^{\\top}X_2) = (5)(69) - (17)(17) = 345 - 289 = 56$.\nThe inverse is:\n$$\n(X_2^{\\top}X_2)^{-1} = \\frac{1}{56} \\begin{pmatrix} 69 & -17 \\\\ -17 & 5 \\end{pmatrix}\n$$\nThe vector $\\tilde{x}_1 = M_{X_2}X_1 = X_1 - P_{X_2}X_1 = X_1 - X_2(X_2^{\\top}X_2)^{-1}X_2^{\\top}X_1$.\nFirst, calculate $X_2^{\\top}X_1$:\n$$\nX_2^{\\top}X_1 = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\\\ 1 & 3 & 3 & 5 & 5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} 15 \\\\ 61 \\end{pmatrix}\n$$\nThe OLS coefficients for regressing $X_1$ on $X_2$ are $\\hat{\\gamma}_{x_1} = (X_2^{\\top}X_2)^{-1}X_2^{\\top}X_1$:\n$$\n\\hat{\\gamma}_{x_1} = \\frac{1}{56} \\begin{pmatrix} 69 & -17 \\\\ -17 & 5 \\end{pmatrix} \\begin{pmatrix} 15 \\\\ 61 \\end{pmatrix} = \\frac{1}{56} \\begin{pmatrix} 1035 - 1037 \\\\ -255 + 305 \\end{pmatrix} = \\frac{1}{56} \\begin{pmatrix} -2 \\\\ 50 \\end{pmatrix} = \\begin{pmatrix} -1/28 \\\\ 25/28 \\end{pmatrix}\n$$\nThe fitted values are $\\hat{X}_1 = X_2\\hat{\\gamma}_{x_1}$, and the residuals are $\\tilde{x}_1 = X_1 - \\hat{X}_1$:\n$$\n\\tilde{x}_1 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix} - \\frac{1}{28}\\begin{pmatrix} 1 & 1 \\\\ 1 & 3 \\\\ 1 & 3 \\\\ 1 & 5 \\\\ 1 & 5 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 25 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{pmatrix} - \\frac{1}{28}\\begin{pmatrix} 24 \\\\ 74 \\\\ 74 \\\\ 124 \\\\ 124 \\end{pmatrix} = \\frac{1}{28}\\begin{pmatrix} 28-24 \\\\ 56-74 \\\\ 84-74 \\\\ 112-124 \\\\ 140-124 \\end{pmatrix} = \\frac{1}{28}\\begin{pmatrix} 4 \\\\ -18 \\\\ 10 \\\\ -12 \\\\ 16 \\end{pmatrix}\n$$\nNext, we compute the residuals for $y$, $\\tilde{y} = M_{X_2}y = y - X_2(X_2^{\\top}X_2)^{-1}X_2^{\\top}y$. First, $X_2^{\\top}y$:\n$$\nX_2^{\\top}y = \\begin{pmatrix} 1 & 1 & 1 & 1 & 1 \\\\ 1 & 3 & 3 & 5 & 5 \\end{pmatrix} \\begin{pmatrix} 8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12 \\end{pmatrix} = \\begin{pmatrix} 48 \\\\ 172 \\end{pmatrix}\n$$\nThe OLS coefficients for regressing $y$ on $X_2$ are $\\hat{\\gamma}_{y} = (X_2^{\\top}X_2)^{-1}X_2^{\\top}y$:\n$$\n\\hat{\\gamma}_{y} = \\frac{1}{56} \\begin{pmatrix} 69 & -17 \\\\ -17 & 5 \\end{pmatrix} \\begin{pmatrix} 48 \\\\ 172 \\end{pmatrix} = \\frac{1}{56} \\begin{pmatrix} 3312 - 2924 \\\\ -816 + 860 \\end{pmatrix} = \\frac{1}{56} \\begin{pmatrix} 388 \\\\ 44 \\end{pmatrix} = \\begin{pmatrix} 97/14 \\\\ 11/14 \\end{pmatrix}\n$$\nThe residuals are $\\tilde{y} = y - X_2\\hat{\\gamma}_y$:\n$$\n\\tilde{y} = \\begin{pmatrix} 8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12 \\end{pmatrix} - \\frac{1}{14}\\begin{pmatrix} 1 & 1 \\\\ 1 & 3 \\\\ 1 & 3 \\\\ 1 & 5 \\\\ 1 & 5 \\end{pmatrix} \\begin{pmatrix} 97 \\\\ 11 \\end{pmatrix} = \\begin{pmatrix} 8 \\\\ 8 \\\\ 10 \\\\ 10 \\\\ 12 \\end{pmatrix} - \\frac{1}{14}\\begin{pmatrix} 108 \\\\ 130 \\\\ 130 \\\\ 152 \\\\ 152 \\end{pmatrix} = \\frac{1}{14}\\begin{pmatrix} 112-108 \\\\ 112-130 \\\\ 140-130 \\\\ 140-152 \\\\ 168-152 \\end{pmatrix} = \\frac{1}{14}\\begin{pmatrix} 4 \\\\ -18 \\\\ 10 \\\\ -12 \\\\ 16 \\end{pmatrix}\n$$\nNow we compute $\\hat{\\beta}_1$:\n$$\n\\hat{\\beta}_1 = \\frac{\\tilde{x}_1^{\\top}\\tilde{y}}{\\tilde{x}_1^{\\top}\\tilde{x}_1}\n$$\nWe observe that $\\tilde{y} = \\frac{1}{14}\\begin{pmatrix} 4 \\\\ -18 \\\\ 10 \\\\ -12 \\\\ 16 \\end{pmatrix}$ and $\\tilde{x}_1 = \\frac{1}{28}\\begin{pmatrix} 4 \\\\ -18 \\\\ 10 \\\\ -12 \\\\ 16 \\end{pmatrix}$.\nThus, we have a direct relationship: $\\tilde{y} = 2 \\tilde{x}_1$.\nSubstituting this into the formula for $\\hat{\\beta}_1$:\n$$\n\\hat{\\beta}_1 = \\frac{\\tilde{x}_1^{\\top}(2\\tilde{x}_1)}{\\tilde{x}_1^{\\top}\\tilde{x}_1} = \\frac{2(\\tilde{x}_1^{\\top}\\tilde{x}_1)}{\\tilde{x}_1^{\\top}\\tilde{x}_1} = 2\n$$\nThis is valid as long as $\\tilde{x}_1$ is not a zero vector, which is true in this case. The numerical value of the coefficient is $2$.\n\n**Interpretation of the Coefficient**\n\nThe coefficient $\\hat{\\beta}_1$ represents the estimated conditional effect of the predictor $X_1$ on the response $y$. As shown in the derivation, $\\hat{\\beta}_1$ is the coefficient from a simple regression of the residuals of $y$ (after regressing on $X_2$) on the residuals of $X_1$ (after regressing on $X_2$). This means $\\hat{\\beta}_1$ quantifies the association between the part of $X_1$ that is linearly independent of the predictors in $X_2$ and the part of $y$ that is linearly independent of the predictors in $X_2$. In practical terms, it is the estimated change in the expected value of $y$ for a one-unit increase in the value of the predictor $X_1$, while holding the values of all other predictors in $X_2$ constant.\n\nWhen $X_1$ \"overlaps substantially\" with the columns of $X_2$, this implies a high degree of multicollinearity. In this situation, most of the variation in $X_1$ is already explained by the predictors in $X_2$. Consequently, the residual vector $\\tilde{x}_1 = M_{X_2}X_1$ will have components with small magnitudes, and its squared norm, $\\|M_{X_2}X_1\\|_2^2 = X_1^{\\top}M_{X_2}X_1$, which is the denominator in the final expression for $\\hat{\\beta}_1$, will be close to zero.\n\nThe variance of the estimator $\\hat{\\beta}_1$ is given by $\\text{Var}(\\hat{\\beta}_1) = \\sigma^2 (X_1^{\\top}M_{X_2}X_1)^{-1}$, where $\\sigma^2$ is the error variance. If the denominator $X_1^{\\top}M_{X_2}X_1$ is small, the variance of $\\hat{\\beta}_1$ becomes very large. This means the estimate is unstable and unreliable. A large variance implies a large standard error and a wide confidence interval, indicating great uncertainty about the true value of $\\beta_1$. While the coefficient still formally represents the conditional effect of $X_1$, the substantial overlap with $X_2$ makes it difficult for the model to disentangle the unique contribution of $X_1$ to the variation in $y$. The data simply do not contain enough information to precisely estimate this effect.", "answer": "$$\n\\boxed{2}\n$$", "id": "3146050"}, {"introduction": "The scale and units of predictors directly influence the magnitude and interpretation of regression coefficients, which can complicate model comparisons. This practice explores the impact of predictor standardization, a common data preprocessing step that sets each predictor's mean to zero and standard deviation to one. Through this exercise, you will discover how standardization transforms the matrix $X^{\\top}X$ into a matrix of pairwise correlations and learn how to convert coefficients between scaled and unscaled models, providing a scale-free way to assess predictor importance [@problem_id:3146102].", "problem": "You are given a design matrix $X \\in \\mathbb{R}^{n \\times p}$ with full column rank, whose columns are centered so that each predictor has sample mean $0$ under the averaging operator $\\frac{1}{n} \\sum_{i=1}^{n}$. The response vector $y \\in \\mathbb{R}^{n}$ is also centered. Consider the ordinary least squares (OLS) problem: find $\\hat{\\beta} \\in \\mathbb{R}^{p}$ that minimizes $\\|y - X \\beta\\|_{2}^{2}$, yielding the normal equations $X^{\\top} X \\hat{\\beta} = X^{\\top} y$ and the hat matrix $H = X (X^{\\top} X)^{-1} X^{\\top}$.\n\nDefine the diagonal scaling matrix $S = \\mathrm{diag}(s_{1}, \\dots, s_{p})$ where $s_{j}^{2} = \\frac{1}{n} x_{j}^{\\top} x_{j}$ and $x_{j}$ denotes the $j$-th column of $X$. Let the standardized predictor matrix be $Z = X S^{-1}$ so that each column of $Z$ has variance $1$ under the averaging operator $\\frac{1}{n} \\sum_{i=1}^{n}$. Let the response standard deviation be $s_{y}^{2} = \\frac{1}{n} y^{\\top} y$ and define the standardized response $z_{y} = y / s_{y}$.\n\nStarting from the OLS normal equations and using only basic linear algebra, do the following:\n\n- Derive the matrix $\\frac{1}{n} Z^{\\top} Z$ in terms of the pairwise correlations among predictors, and explain why it is a correlation-based Gram matrix.\n- Express the coefficient vector in the standardized predictor scale, $\\hat{\\gamma}$ defined by $X \\hat{\\beta} = Z \\hat{\\gamma}$, in terms of $Z^{\\top} Z$ and $Z^{\\top} y$, and then express $\\hat{\\beta}$ in terms of $\\hat{\\gamma}$ and $S$. State clearly how the units of the entries of $\\hat{\\beta}$ change under column scaling of $X$.\n- Determine whether the hat matrix changes under the transformation $X \\mapsto Z$, and justify your conclusion algebraically by deriving the hat matrix in the standardized predictor scale.\n\nLet $R \\in \\mathbb{R}^{p \\times p}$ denote the predictor correlation matrix with entries $R_{jk} = \\frac{\\frac{1}{n} x_{j}^{\\top} x_{k}}{s_{j} s_{k}}$ and let $r \\in \\mathbb{R}^{p}$ be the vector of predictorâ€“response correlations with entries $r_{j} = \\frac{\\frac{1}{n} x_{j}^{\\top} y}{s_{j} s_{y}}$. After completing your derivations, provide the closed-form expression for the coefficient vector obtained by regressing the standardized response $z_{y}$ on the standardized predictors $Z$ using OLS, expressed solely in terms of $R$ and $r$.\n\nYour final answer must be a single analytic expression. No rounding is required.", "solution": "The problem is well-posed, scientifically grounded in the principles of multiple linear regression, and contains sufficient information for a unique solution. The definitions and assumptions are standard in statistical learning. I will proceed with the derivations as requested.\n\nThe problem asks for several derivations based on the matrix formulation of ordinary least squares (OLS) regression. We are given a centered design matrix $X \\in \\mathbb{R}^{n \\times p}$ of full column rank and a centered response vector $y \\in \\mathbb{R}^{n}$.\n\n**Part 1: The matrix $\\frac{1}{n} Z^{\\top} Z$**\n\nFirst, we derive the expression for the matrix $\\frac{1}{n} Z^{\\top} Z$. The standardized predictor matrix $Z$ is defined as $Z = X S^{-1}$, where $S$ is the diagonal matrix of predictor standard deviations, $S = \\mathrm{diag}(s_1, \\dots, s_p)$ with $s_j^2 = \\frac{1}{n} x_j^{\\top} x_j$. Since $S$ is a diagonal matrix, its transpose is itself, $(S^{-1})^{\\top} = S^{-1}$.\n\nWe compute $Z^{\\top} Z$:\n$$\nZ^{\\top} Z = (X S^{-1})^{\\top} (X S^{-1}) = (S^{-1})^{\\top} X^{\\top} X S^{-1} = S^{-1} X^{\\top} X S^{-1}\n$$\nNow we scale this matrix by $\\frac{1}{n}$:\n$$\n\\frac{1}{n} Z^{\\top} Z = \\frac{1}{n} S^{-1} X^{\\top} X S^{-1}\n$$\nLet's examine the $(j,k)$-th entry of this matrix. The matrix $X^{\\top} X$ has entries $(X^{\\top} X)_{jk} = x_j^{\\top} x_k$. The matrix $S^{-1}$ is diagonal with entries $(S^{-1})_{jj} = 1/s_j$.\nThe $(j,k)$-th entry of $\\frac{1}{n} S^{-1} X^{\\top} X S^{-1}$ is:\n$$\n\\left(\\frac{1}{n} S^{-1} X^{\\top} X S^{-1}\\right)_{jk} = \\frac{1}{n} (S^{-1})_{jj} (X^{\\top} X)_{jk} (S^{-1})_{kk} = \\frac{1}{n} \\left(\\frac{1}{s_j}\\right) (x_j^{\\top} x_k) \\left(\\frac{1}{s_k}\\right) = \\frac{\\frac{1}{n} x_j^{\\top} x_k}{s_j s_k}\n$$\nThis expression is precisely the definition of the $(j,k)$-th entry of the predictor correlation matrix, $R_{jk}$. Since the columns of $X$ are centered, $\\frac{1}{n} x_j^{\\top} x_k$ is the sample covariance between predictors $x_j$ and $x_k$. Also, $s_j = \\sqrt{\\frac{1}{n} x_j^{\\top} x_j}$ and $s_k = \\sqrt{\\frac{1}{n} x_k^{\\top} x_k}$ are the respective sample standard deviations. Thus, $R_{jk}$ is the sample correlation coefficient between $x_j$ and $x_k$.\n\nTherefore, the matrix $\\frac{1}{n} Z^{\\top} Z$ is the predictor correlation matrix $R$:\n$$\n\\frac{1}{n} Z^{\\top} Z = R\n$$\nThis matrix is a Gram matrix because its entries are inner products. Specifically, if we define an inner product on the space of predictors as $\\langle u, v \\rangle = \\frac{1}{n} u^{\\top} v$, then the $(j,k)$-th entry of this matrix is $\\langle z_j, z_k \\rangle$, where $z_j$ is the $j$-th column of $Z$. Since the vectors $z_j$ are standardized to have unit variance (i.e., $\\langle z_j, z_j \\rangle = 1$), this Gram matrix is a matrix of correlations.\n\n**Part 2: Coefficients $\\hat{\\gamma}$ and $\\hat{\\beta}$**\n\nWe are asked to express the coefficient vector $\\hat{\\gamma}$ (from regressing $y$ on $Z$) and relate the original coefficients $\\hat{\\beta}$ to it. The OLS problem for the standardized predictors is to find $\\hat{\\gamma}$ that minimizes $\\|y - Z\\gamma\\|_2^2$. The corresponding normal equations are:\n$$\n(Z^{\\top} Z) \\hat{\\gamma} = Z^{\\top} y\n$$\nSince $X$ has full column rank and $S$ is invertible, $Z=XS^{-1}$ also has full column rank, which guarantees that $Z^{\\top} Z$ is invertible. Solving for $\\hat{\\gamma}$ gives:\n$$\n\\hat{\\gamma} = (Z^{\\top} Z)^{-1} Z^{\\top} y\n$$\nThe relationship between the fitted values is given by $X \\hat{\\beta} = Z \\hat{\\gamma}$. Substituting $Z = X S^{-1}$:\n$$\nX \\hat{\\beta} = (X S^{-1}) \\hat{\\gamma}\n$$\nSince $X$ has full column rank, the matrix $X^{\\top}X$ is invertible. We can left-multiply by $X^{\\top}$ to get $X^{\\top}X\\hat{\\beta} = X^{\\top}X S^{-1} \\hat{\\gamma}$, and then by $(X^{\\top}X)^{-1}$ to isolate $\\hat{\\beta}$:\n$$\n\\hat{\\beta} = (X^{\\top}X)^{-1}(X^{\\top}X) S^{-1} \\hat{\\gamma} = S^{-1} \\hat{\\gamma}\n$$\nThis means that for each component $j$, $\\hat{\\beta}_j = \\frac{1}{s_j} \\hat{\\gamma}_j$.\n\nRegarding the units of $\\hat{\\beta}_j$, let $[v]$ denote the units of a quantity $v$. The model equation is $y \\approx \\sum_j X_{ij} \\beta_j$, which implies that for each term in the sum, $[y] = [X_{ij}] [\\beta_j]$. Therefore, the units of a coefficient $\\beta_j$ are $[\\beta_j] = \\frac{[y]}{[X_{ij}]}$. If the units of a predictor column $x_j$ are changed by a scaling factor $c$ (e.g., from meters to centimeters, $c=100$), the new predictor is $x_j' = c x_j$. The new coefficient $\\beta_j'$ must satisfy $x_j' \\beta_j' = x_j \\beta_j$, so $(c x_j) \\beta_j' = x_j \\beta_j$, which implies $\\beta_j' = \\frac{1}{c} \\beta_j$. Thus, the coefficient $\\hat{\\beta}_j$ scales inversely with the scaling of its corresponding predictor column.\n\n**Part 3: Invariance of the Hat Matrix**\n\nThe hat matrix for the original model is $H = X(X^{\\top}X)^{-1}X^{\\top}$. We want to determine if this matrix changes when we use the standardized predictors $Z$. The hat matrix for the model with predictors $Z$ would be $H_Z = Z(Z^{\\top}Z)^{-1}Z^{\\top}$. We substitute $Z=XS^{-1}$ into this expression and simplify:\n$$\nH_Z = (X S^{-1}) ((X S^{-1})^{\\top} (X S^{-1}))^{-1} (X S^{-1})^{\\top}\n$$\nFirst, we simplify the inverse term in the middle:\n$$\n((X S^{-1})^{\\top} (X S^{-1}))^{-1} = (S^{-1} X^{\\top} X S^{-1})^{-1}\n$$\nUsing the property $(ABC)^{-1} = C^{-1}B^{-1}A^{-1}$ and noting that $S$ is invertible, we get:\n$$\n(S^{-1})^{-1} (X^{\\top}X)^{-1} (S^{-1})^{-1} = S (X^{\\top}X)^{-1} S\n$$\nNow, substitute this back into the expression for $H_Z$:\n$$\nH_Z = (X S^{-1}) [S (X^{\\top}X)^{-1} S] (S^{-1})^{\\top} X^{\\top}\n$$\nSince $S$ is diagonal, $(S^{-1})^{\\top} = S^{-1}$. We can now group the terms:\n$$\nH_Z = X (S^{-1} S) (X^{\\top}X)^{-1} (S S^{-1}) X^{\\top}\n$$\nSince $S^{-1}S = I$ and $SS^{-1} = I$, where $I$ is the identity matrix, the expression simplifies to:\n$$\nH_Z = X I (X^{\\top}X)^{-1} I X^{\\top} = X(X^{\\top}X)^{-1}X^{\\top} = H\n$$\nThus, the hat matrix is invariant to invertible linear transformations (such as scaling) of the columns of the design matrix.\n\n**Part 4: Final Expression for Standardized Coefficients**\n\nFinally, we are asked to find the coefficient vector for regressing the standardized response $z_y = y/s_y$ on the standardized predictors $Z$. Let us denote this coefficient vector by $\\hat{\\gamma}_{std}$. The OLS problem is to minimize $\\|z_y - Z\\gamma_{std}\\|_2^2$, for which the normal equations are:\n$$\n(Z^{\\top} Z) \\hat{\\gamma}_{std} = Z^{\\top} z_y\n$$\nSolving for $\\hat{\\gamma}_{std}$ yields:\n$$\n\\hat{\\gamma}_{std} = (Z^{\\top} Z)^{-1} (Z^{\\top} z_y)\n$$\nWe need to express this in terms of the correlation matrices $R$ and $r$. As derived in Part 1, we have $Z^{\\top}Z = nR$.\nNow, let's analyze the term $Z^{\\top} z_y$:\n$$\nZ^{\\top} z_y = Z^{\\top} \\frac{y}{s_y} = \\frac{1}{s_y} Z^{\\top} y\n$$\nLet's examine the $j$-th component of the vector $Z^{\\top} y$. It is $z_j^{\\top} y$. Since $z_j = x_j / s_j$, we have:\n$$\n(Z^{\\top} y)_j = z_j^{\\top} y = \\left(\\frac{x_j}{s_j}\\right)^{\\top} y = \\frac{1}{s_j} x_j^{\\top} y\n$$\nThe $j$-th component of the predictor-response correlation vector $r$ is given by $r_j = \\frac{\\frac{1}{n} x_j^{\\top} y}{s_j s_y}$. Rearranging this gives $x_j^{\\top} y = n s_j s_y r_j$.\nSubstituting this into the expression for $(Z^{\\top} y)_j$:\n$$\n(Z^{\\top} y)_j = \\frac{1}{s_j} (n s_j s_y r_j) = n s_y r_j\n$$\nSince this holds for each component $j$, the vector $Z^{\\top} y$ is equal to $n s_y r$.\nNow we can compute $Z^{\\top} z_y$:\n$$\nZ^{\\top} z_y = \\frac{1}{s_y} (Z^{\\top} y) = \\frac{1}{s_y} (n s_y r) = n r\n$$\nFinally, we substitute our expressions for $Z^{\\top}Z$ and $Z^{\\top}z_y$ back into the solution for $\\hat{\\gamma}_{std}$:\n$$\n\\hat{\\gamma}_{std} = (n R)^{-1} (n r) = \\frac{1}{n} R^{-1} (n r) = R^{-1} r\n$$\nThis is the closed-form expression for the coefficient vector when regressing the standardized response on the standardized predictors, expressed solely in terms of the predictor correlation matrix $R$ and the predictor-response correlation vector $r$.", "answer": "$$\\boxed{R^{-1} r}$$", "id": "3146102"}, {"introduction": "When predictors are highly correlated (a condition known as multicollinearity), the design matrix $X$ becomes ill-conditioned, and ordinary least squares (OLS) estimates can become highly sensitive to small changes in the data. This practice introduces ridge regression, a powerful regularization technique that stabilizes coefficient estimates by adding a penalty term. By leveraging the Singular Value Decomposition (SVD), you will gain a deep geometric understanding of how ridge regression systematically shrinks the model's components along unstable, low-variance directions, leading to more robust and reliable predictions [@problem_id:3146059].", "problem": "Consider the matrix formulation of multiple linear regression with an emphasis on the role of singular value decomposition (SVD) in interpreting regularization. Work in purely mathematical terms with no physical units. Begin from the definitions of the least-squares estimator as the minimizer of a squared error and the ridge estimator as the minimizer of a squared error with a squared-parameter penalty. You must not assume any closed forms a priori; instead, derive them where needed from these definitions and linear algebra facts. The goal is to express predictions under ridge regression and interpret how the penalty shrinks contributions along small singular directions via SVD, then design a concrete data matrix and response vector where small singular directions dominate ordinary least squares and observe how ridge regression stabilizes them.\n\nTask:\n1) Construct the design matrix $X \\in \\mathbb{R}^{6 \\times 3}$ as follows, where each entry is a real scalar:\n$$\nX =\n\\begin{bmatrix}\n1 & 1 + 10^{-4}\\cdot 1 & 1 + 2\\cdot 10^{-4}\\cdot (-2) \\\\\n2 & 2 + 10^{-4}\\cdot (-2) & 2 + 2\\cdot 10^{-4}\\cdot 1 \\\\\n3 & 3 + 10^{-4}\\cdot 3 & 3 + 2\\cdot 10^{-4}\\cdot (-3) \\\\\n4 & 4 + 10^{-4}\\cdot (-4) & 4 + 2\\cdot 10^{-4}\\cdot 4 \\\\\n5 & 5 + 10^{-4}\\cdot 5 & 5 + 2\\cdot 10^{-4}\\cdot (-5) \\\\\n6 & 6 + 10^{-4}\\cdot (-6) & 6 + 2\\cdot 10^{-4}\\cdot 6\n\\end{bmatrix}.\n$$\nThis matrix is full column rank but highly ill-conditioned because its columns are nearly collinear.\n\n2) Compute the singular value decomposition $X = U\\Sigma V^\\top$ with $U \\in \\mathbb{R}^{6 \\times 3}$ having orthonormal columns, $\\Sigma \\in \\mathbb{R}^{3 \\times 3}$ diagonal with nonnegative entries in nonincreasing order, and $V \\in \\mathbb{R}^{3 \\times 3}$ orthonormal. Define the response vector $y \\in \\mathbb{R}^{6}$ to be the left singular vector corresponding to the smallest singular value, i.e., the last column of $U$.\n\n3) Define the ordinary least squares estimator $\\hat{\\beta}_{\\text{OLS}} \\in \\mathbb{R}^{3}$ as any minimizer of the problem\n$$\n\\min_{\\beta \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|y - X\\beta\\|_2^2.\n$$\nDefine the ridge estimator $\\hat{\\beta}_{\\lambda} \\in \\mathbb{R}^{3}$ for any penalty strength $\\lambda \\ge 0$ as any minimizer of\n$$\n\\min_{\\beta \\in \\mathbb{R}^{3}} \\ \\frac{1}{2}\\|y - X\\beta\\|_2^2 + \\frac{\\lambda}{2}\\|\\beta\\|_2^2.\n$$\nCompute the fitted values $\\hat{y}_{\\text{OLS}} = X\\hat{\\beta}_{\\text{OLS}}$ and $\\hat{y}_{\\lambda} = X\\hat{\\beta}_{\\lambda}$.\n\n4) Interpret ridge as filtering singular directions. Using the SVD of $X$, express the predictions $\\hat{y}_{\\lambda}$ in the basis of the left singular vectors and identify the shrinkage factor applied to each singular component. Then, with $y$ chosen as in step $2$, verify computationally that the singular direction associated with the smallest singular value is the one most strongly shrunk.\n\n5) Test suite. Use the single matrix $X$ and response $y$ defined above, and evaluate the following four cases that together serve as a test suite:\n- Case A (boundary, penalty zero): $\\lambda = 0$. Verify whether the ridge predictions coincide with ordinary least squares predictions (within a numerical tolerance of $10^{-8}$). Output a boolean computed as $ \\|\\hat{y}_{\\lambda} - \\hat{y}_{\\text{OLS}}\\|_2 \\le 10^{-8}$.\n- Case B (small penalty, gentle shrinkage): $\\lambda = 10^{-6}$. Output the ratio $ \\|\\hat{\\beta}_{\\lambda}\\|_2 / \\|\\hat{\\beta}_{\\text{OLS}}\\|_2 $ as a floating-point number.\n- Case C (moderate penalty, singular-direction filter check): $\\lambda = 10^{-4}$. Let $v_{\\min}$ be the right singular vector corresponding to the smallest singular value $\\sigma_{\\min}$. Compute the component shrinkage ratio\n$$\nr_{\\text{emp}} = \\frac{|\\langle v_{\\min}, \\hat{\\beta}_{\\lambda} \\rangle|}{|\\langle v_{\\min}, \\hat{\\beta}_{\\text{OLS}} \\rangle|},\n$$\nand the theoretical filter factor\n$$\nr_{\\text{theory}} = \\frac{\\sigma_{\\min}^2}{\\sigma_{\\min}^2 + \\lambda}.\n$$\nOutput the absolute deviation $|r_{\\text{emp}} - r_{\\text{theory}}|$ as a floating-point number.\n- Case D (large penalty, heavy shrinkage): $\\lambda = 10^{2}$. Output the Euclidean norm $\\|\\hat{\\beta}_{\\lambda}\\|_2$ as a floating-point number.\n\n6) Final output format. Your program should produce a single line of output containing the four results in the order of Cases A through D as a comma-separated list enclosed in square brackets (e.g., \"[resultA,resultB,resultC,resultD]\"). The results must be, respectively, a boolean for Case A and three floating-point numbers for Cases B through D.", "solution": "The problem requires an analysis of ridge regression as a shrinkage estimator, interpreted through the singular value decomposition (SVD) of the design matrix $X$. We must first derive the estimators from their optimization definitions, then apply them to a specifically constructed ill-conditioned matrix $X$ and a response vector $y$ chosen to highlight the regularization effect.\n\nFirst, we derive the ordinary least squares (OLS) and ridge regression estimators from first principles.\n\nThe OLS estimator $\\hat{\\beta}_{\\text{OLS}}$ is defined as the minimizer of the residual sum of squares:\n$$\nL(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_2^2 = \\frac{1}{2}(y - X\\beta)^\\top(y - X\\beta)\n$$\nTo find the minimum, we compute the gradient of $L(\\beta)$ with respect to $\\beta$ and set it to zero.\n$$\n\\nabla_\\beta L(\\beta) = \\nabla_\\beta \\left( \\frac{1}{2}(y^\\top y - 2y^\\top X\\beta + \\beta^\\top X^\\top X\\beta) \\right) = -X^\\top y + X^\\top X\\beta\n$$\nSetting the gradient to zero gives the normal equations:\n$$\nX^\\top X \\hat{\\beta}_{\\text{OLS}} = X^\\top y\n$$\nSince the problem specifies that $X$ is a $6 \\times 3$ matrix of full column rank, the matrix $X^\\top X \\in \\mathbb{R}^{3 \\times 3}$ is invertible. Thus, the unique OLS solution is:\n$$\n\\hat{\\beta}_{\\text{OLS}} = (X^\\top X)^{-1} X^\\top y\n$$\n\nThe ridge estimator $\\hat{\\beta}_{\\lambda}$ is defined as the minimizer of the penalized residual sum of squares:\n$$\nL_\\lambda(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_2^2 + \\frac{\\lambda}{2}\\|\\beta\\|_2^2\n$$\nwhere $\\lambda \\ge 0$ is the regularization parameter. The gradient is:\n$$\n\\nabla_\\beta L_\\lambda(\\beta) = \\nabla_\\beta \\left( \\frac{1}{2}\\|y - X\\beta\\|_2^2 \\right) + \\nabla_\\beta \\left( \\frac{\\lambda}{2}\\beta^\\top\\beta \\right) = (-X^\\top y + X^\\top X\\beta) + \\lambda\\beta\n$$\nSetting the gradient to zero yields:\n$$\n(X^\\top X + \\lambda I)\\hat{\\beta}_{\\lambda} = X^\\top y\n$$\nFor any $\\lambda > 0$, the matrix $(X^\\top X + \\lambda I)$ is positive definite and thus invertible. For $\\lambda = 0$, it reduces to the OLS case. The unique ridge solution is:\n$$\n\\hat{\\beta}_{\\lambda} = (X^\\top X + \\lambda I)^{-1} X^\\top y\n$$\n\nNext, we interpret these estimators using the singular value decomposition (SVD) of $X$. Let $X = U\\Sigma V^\\top$, where $U \\in \\mathbb{R}^{6 \\times 3}$ has orthonormal columns, $\\Sigma = \\text{diag}(\\sigma_1, \\sigma_2, \\sigma_3)$ is a $3 \\times 3$ diagonal matrix with singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\sigma_3 > 0$, and $V \\in \\mathbb{R}^{3 \\times 3}$ is an orthonormal matrix.\nWe substitute the SVD into the expressions for the estimators. First, note that $X^\\top X = (U\\Sigma V^\\top)^\\top(U\\Sigma V^\\top) = V\\Sigma^\\top U^\\top U\\Sigma V^\\top = V\\Sigma^2 V^\\top$.\n\nFor the OLS estimator:\n$$\n\\hat{\\beta}_{\\text{OLS}} = (V\\Sigma^2 V^\\top)^{-1} (U\\Sigma V^\\top)^\\top y = (V(\\Sigma^2)^{-1}V^\\top)(V\\Sigma U^\\top y) = V\\Sigma^{-1}U^\\top y\n$$\nIf we denote the columns of $U$ and $V$ as $u_j$ and $v_j$ respectively, this can be written as a sum:\n$$\n\\hat{\\beta}_{\\text{OLS}} = \\sum_{j=1}^{3} v_j \\frac{u_j^\\top y}{\\sigma_j}\n$$\nThe OLS solution is a linear combination of the right singular vectors $v_j$, where the coefficients are the projections of $y$ onto the left singular vectors $u_j$, amplified by the inverse of the corresponding singular values $1/\\sigma_j$.\n\nFor the ridge estimator:\n$$\nX^\\top X + \\lambda I = V\\Sigma^2 V^\\top + \\lambda V V^\\top = V(\\Sigma^2 + \\lambda I)V^\\top\n$$\nThe inverse is $(X^\\top X + \\lambda I)^{-1} = V(\\Sigma^2 + \\lambda I)^{-1}V^\\top$. Thus,\n$$\n\\hat{\\beta}_{\\lambda} = (V(\\Sigma^2 + \\lambda I)^{-1}V^\\top)(V\\Sigma U^\\top y) = V(\\Sigma^2 + \\lambda I)^{-1}\\Sigma U^\\top y\n$$\nIn sum notation, this is:\n$$\n\\hat{\\beta}_{\\lambda} = \\sum_{j=1}^{3} v_j \\frac{\\sigma_j}{\\sigma_j^2 + \\lambda}(u_j^\\top y)\n$$\nComparing the coefficients of $v_j$ in the expansions for $\\hat{\\beta}_{\\text{OLS}}$ and $\\hat{\\beta}_{\\lambda}$, we see a multiplicative factor:\n$$\n\\frac{\\sigma_j/(\\sigma_j^2+\\lambda)}{1/\\sigma_j} = \\frac{\\sigma_j^2}{\\sigma_j^2 + \\lambda}\n$$\nThis factor, which is always in the interval $[0, 1]$, is the shrinkage factor. For large singular values $\\sigma_j \\gg \\sqrt{\\lambda}$, the factor is close to $1$, and there is little shrinkage. For small singular values $\\sigma_j \\ll \\sqrt{\\lambda}$, the factor is close to $0$, and the corresponding component of the solution is strongly shrunk towards zero.\n\nThe predicted values are $\\hat{y} = X\\beta$. For OLS, $\\hat{y}_{\\text{OLS}} = X\\hat{\\beta}_{\\text{OLS}} = (U\\Sigma V^\\top)(V\\Sigma^{-1}U^\\top y) = UU^\\top y$, which is the orthogonal projection of $y$ onto the column space of $X$. For ridge,\n$$\n\\hat{y}_{\\lambda} = X\\hat{\\beta}_{\\lambda} = (U\\Sigma V^\\top)(V(\\Sigma^2 + \\lambda I)^{-1}\\Sigma U^\\top y) = U \\Sigma (\\Sigma^2 + \\lambda I)^{-1} \\Sigma U^\\top y = \\sum_{j=1}^{3} u_j \\left( \\frac{\\sigma_j^2}{\\sigma_j^2+\\lambda} \\right) (u_j^\\top y)\n$$\nThis shows that the ridge predictions are formed by shrinking the components of $y$ along the left singular vectors $u_j$ by the same factors.\n\nThe problem defines $y$ to be the left singular vector corresponding to the smallest singular value, i.e., $y = u_3$ (assuming $\\sigma_3 = \\sigma_{\\min}$). By orthonormality of the columns of $U$, we have $u_j^\\top y = u_j^\\top u_3 = \\delta_{j3}$.\nThis simplifies the estimators dramatically:\n$$\n\\hat{\\beta}_{\\text{OLS}} = v_3 \\frac{1}{\\sigma_3}\n$$\n$$\n\\hat{\\beta}_{\\lambda} = v_3 \\frac{\\sigma_3}{\\sigma_3^2 + \\lambda}\n$$\nThe OLS solution's magnitude blows up as $\\sigma_3 \\to 0$, whereas the ridge solution remains bounded.\n\nWe now proceed with the specific calculations for the test suite.\nLet $X$ be the provided $6 \\times 3$ matrix. We compute its SVD $X=U\\Sigma V^\\top$ and define $y = u_3$, where $u_3$ corresponds to $\\sigma_3 = \\sigma_{\\min}$. Let $v_3 = v_{\\min}$.\n\nCase A ($\\lambda = 0$): We compare $\\hat{y}_{0}$ with $\\hat{y}_{\\text{OLS}}$. From the ridge prediction formula, for $\\lambda=0$:\n$$\n\\hat{y}_{0} = \\sum_{j=1}^{3} u_j \\left( \\frac{\\sigma_j^2}{\\sigma_j^2+0} \\right) (u_j^\\top y) = \\sum_{j=1}^{3} u_j (u_j^\\top y) = UU^\\top y = \\hat{y}_{\\text{OLS}}\n$$\nThe predictions must be identical. Numerically, $\\|\\hat{y}_{0} - \\hat{y}_{\\text{OLS}}\\|_2$ should be close to $0$.\n\nCase B ($\\lambda = 10^{-6}$): We compute the ratio of norms.\n$$\n\\frac{\\|\\hat{\\beta}_{\\lambda}\\|_2}{\\|\\hat{\\beta}_{\\text{OLS}}\\|_2} = \\frac{\\|v_3 \\frac{\\sigma_3}{\\sigma_3^2 + \\lambda}\\|_2}{\\|v_3 \\frac{1}{\\sigma_3}\\|_2} = \\frac{|\\frac{\\sigma_3}{\\sigma_3^2 + \\lambda}|}{|\\frac{1}{\\sigma_3}|} \\|v_3\\|_2 = \\frac{\\sigma_3^2}{\\sigma_3^2 + \\lambda}\n$$\nThis is the theoretical shrinkage factor for $\\sigma_{\\min}$.\n\nCase C ($\\lambda = 10^{-4}$): We test the component shrinkage ratio.\nThe empirical ratio is $r_{\\text{emp}} = \\frac{|\\langle v_{\\min}, \\hat{\\beta}_{\\lambda} \\rangle|}{|\\langle v_{\\min}, \\hat{\\beta}_{\\text{OLS}} \\rangle|}$.\nUsing $v_{\\min} = v_3$ and the derived expressions for the estimators:\n$$\n\\langle v_3, \\hat{\\beta}_{\\text{OLS}} \\rangle = \\langle v_3, v_3 \\frac{1}{\\sigma_3} \\rangle = \\frac{1}{\\sigma_3} \\|v_3\\|_2^2 = \\frac{1}{\\sigma_3}\n$$\n$$\n\\langle v_3, \\hat{\\beta}_{\\lambda} \\rangle = \\langle v_3, v_3 \\frac{\\sigma_3}{\\sigma_3^2+\\lambda} \\rangle = \\frac{\\sigma_3}{\\sigma_3^2+\\lambda}\n$$\nThus, $r_{\\text{emp}} = \\frac{|\\sigma_3/(\\sigma_3^2+\\lambda)|}{|1/\\sigma_3|} = \\frac{\\sigma_3^2}{\\sigma_3^2+\\lambda}$, which is identical to the theoretical filter factor $r_{\\text{theory}}$. Their absolute difference $|r_{\\text{emp}} - r_{\\text{theory}}|$ should be numerically zero.\n\nCase D ($\\lambda = 10^2$): A large penalty should yield a solution vector with a small norm.\n$$\n\\|\\hat{\\beta}_{\\lambda}\\|_2 = \\left\\| v_3 \\frac{\\sigma_3}{\\sigma_3^2 + \\lambda} \\right\\|_2 = \\frac{\\sigma_3}{\\sigma_3^2 + \\lambda} \\|v_3\\|_2 = \\frac{\\sigma_3}{\\sigma_3^2 + \\lambda}\n$$\nWith $\\lambda = 100$, this norm will be very small.\n\nThe numerical implementation will follow these derived formulas.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the ridge regression problem as specified.\n    1. Constructs the design matrix X.\n    2. Performs SVD on X and defines the response vector y.\n    3. Computes OLS and ridge solutions for different lambda values.\n    4. Calculates the required metrics for the four test cases.\n    \"\"\"\n\n    # 1. Construct the design matrix X\n    i_vals = np.arange(1, 7, dtype=float).reshape(-1, 1)\n    \n    # Perturbations for columns 2 and 3\n    p2_coeffs = np.array([1, -2, 3, -4, 5, -6]).reshape(-1, 1)\n    p3_coeffs = np.array([-2, 1, -3, 4, -5, 6]).reshape(-1, 1)\n    \n    col1 = i_vals\n    col2 = i_vals + 1e-4 * p2_coeffs\n    col3 = i_vals + 2e-4 * p3_coeffs\n    \n    X = np.hstack([col1, col2, col3])\n    n, p = X.shape\n\n    # 2. Compute SVD of X and define y\n    # U has shape (n, p), s has shape (p,), Vt has shape (p, p)\n    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n    \n    # y is the left singular vector for the smallest singular value\n    # which is the last column of U\n    y = U[:, -1]\n\n    # Get smallest singular value and corresponding right singular vector\n    sigma_min = s[-1]\n    # Vt is V.T, so the last row of Vt is the last column of V (as a row)\n    # v_min should be a column vector\n    v_min = Vt[-1, :].reshape(p, 1)\n    \n    # Helper to compute ridge estimator\n    def get_beta_ridge(lam, X_mat, y_vec):\n        p_dim = X_mat.shape[1]\n        XTX = X_mat.T @ X_mat\n        XTy = X_mat.T @ y_vec\n        I = np.identity(p_dim)\n        # Using np.linalg.solve for better numerical stability than inv()\n        beta = np.linalg.solve(XTX + lam * I, XTy)\n        return beta\n\n    results = []\n\n    # Case A: lambda = 0. Verify ridge prediction equals OLS prediction.\n    lam_A = 0.0\n    beta_ols = np.linalg.lstsq(X, y, rcond=None)[0]\n    y_hat_ols = X @ beta_ols\n    beta_ridge_A = get_beta_ridge(lam_A, X, y)\n    y_hat_ridge_A = X @ beta_ridge_A\n    result_A = np.linalg.norm(y_hat_ridge_A - y_hat_ols) = 1e-8\n    results.append(result_A)\n\n    # Case B: lambda = 1e-6. Compute norm ratio.\n    lam_B = 1e-6\n    # beta_ols is already computed\n    beta_ridge_B = get_beta_ridge(lam_B, X, y)\n    result_B = np.linalg.norm(beta_ridge_B) / np.linalg.norm(beta_ols)\n    results.append(result_B)\n\n    # Case C: lambda = 1e-4. Compare empirical and theoretical shrinkage.\n    lam_C = 1e-4\n    # beta_ols is already computed\n    beta_ridge_C = get_beta_ridge(lam_C, X, y)\n    \n    # Empirical ratio\n    # Inner product of a (p,) vector and a (p,1) vector results in a (1,) array\n    # We extract the scalar value with [0]\n    r_emp_num = np.abs((v_min.T @ beta_ridge_C))[0]\n    r_emp_den = np.abs((v_min.T @ beta_ols))[0]\n    r_emp = r_emp_num / r_emp_den\n\n    # Theoretical ratio\n    r_theory = (sigma_min**2) / (sigma_min**2 + lam_C)\n    \n    result_C = np.abs(r_emp - r_theory)\n    results.append(result_C)\n\n    # Case D: lambda = 1e2. Compute norm of the heavily shrunk estimator.\n    lam_D = 1e2\n    beta_ridge_D = get_beta_ridge(lam_D, X, y)\n    result_D = np.linalg.norm(beta_ridge_D)\n    results.append(result_D)\n    \n    # Final output formatting\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3146059"}]}