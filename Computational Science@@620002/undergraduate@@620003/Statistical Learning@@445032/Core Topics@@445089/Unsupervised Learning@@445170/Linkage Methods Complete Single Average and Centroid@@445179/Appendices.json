{"hands_on_practices": [{"introduction": "In hierarchical clustering, we intuitively expect the dissimilarity to increase at each merge step, creating a clean, upwardly branching dendrogram. However, not all linkage methods adhere to this property of monotonicity. This exercise provides a hands-on calculation to demonstrate how centroid linkage can produce an \"inversion,\" where the height of a later merge is lower than a previous one, a critical and sometimes counter-intuitive behavior to understand when interpreting its results. [@problem_id:3140573]", "problem": "Consider Hierarchical Agglomerative Clustering (HAC) with centroid linkage under the Euclidean metric in $\\mathbb{R}^{2}$. Centroid linkage defines the distance between two clusters as the Euclidean distance between their arithmetic means (centroids). You are given $3$ data points\n$$\n\\mathbf{x}_{1}=\\left(-\\frac{1}{2},\\,0\\right),\\quad\n\\mathbf{x}_{2}=\\left(\\frac{1}{2},\\,0\\right),\\quad\n\\mathbf{x}_{3}=\\left(0,\\,\\frac{\\sqrt{13}}{4}\\right).\n$$\nStarting from singletons $\\{\\mathbf{x}_{1}\\}$, $\\{\\mathbf{x}_{2}\\}$, and $\\{\\mathbf{x}_{3}\\}$, perform HAC with centroid linkage. At each step, merge the pair of clusters with the smallest centroid distance and record the corresponding merge height as the chosen centroid distance at that step. Compute exactly:\n- the first merge height $h_{1}$,\n- the centroid of the merged cluster after the first step,\n- the second merge height $h_{2}$,\nand verify numerically that $h_{2}<h_{1}$, thereby demonstrating that centroid linkage can violate monotonicity of merge levels. Finally, report the inversion magnitude $\\Delta=h_{2}-h_{1}$ as a single closed-form expression. Do not round; provide $\\Delta$ in exact form.", "solution": "The problem is valid as it is a well-posed and scientifically grounded exercise in applying the Hierarchical Agglomerative Clustering (HAC) algorithm with centroid linkage, a standard topic in statistical learning. It provides all necessary data and definitions to arrive at a unique and verifiable solution.\n\nThe process begins with $3$ singleton clusters, $C_{1}=\\{\\mathbf{x}_{1}\\}$, $C_{2}=\\{\\mathbf{x}_{2}\\}$, and $C_{3}=\\{\\mathbf{x}_{3}\\}$, where the data points are given as:\n$$\n\\mathbf{x}_{1}=\\left(-\\frac{1}{2},\\,0\\right),\\quad\n\\mathbf{x}_{2}=\\left(\\frac{1}{2},\\,0\\right),\\quad\n\\mathbf{x}_{3}=\\left(0,\\,\\frac{\\sqrt{13}}{4}\\right)\n$$\nIn centroid linkage, the distance between two clusters is the Euclidean distance between their centroids. For a singleton cluster, the centroid is the point itself. Let $\\mu_i$ be the centroid of cluster $C_i$. Initially, $\\mu_{1}=\\mathbf{x}_{1}$, $\\mu_{2}=\\mathbf{x}_{2}$, and $\\mu_{3}=\\mathbf{x}_{3}$.\n\nFirst, we calculate the pairwise squared Euclidean distances between the initial centroids:\nThe squared distance between $C_{1}$ and $C_{2}$ is:\n$$\nd(C_{1}, C_{2})^{2} = ||\\mu_{1} - \\mu_{2}||^{2} = ||\\mathbf{x}_{1} - \\mathbf{x}_{2}||^{2} = \\left\\|\\left(-\\frac{1}{2}-\\frac{1}{2},\\,0-0\\right)\\right\\|^{2} = \\|(-1, 0)\\|^{2} = (-1)^{2} + 0^{2} = 1\n$$\nThe squared distance between $C_{1}$ and $C_{3}$ is:\n$$\nd(C_{1}, C_{3})^{2} = ||\\mu_{1} - \\mu_{3}||^{2} = ||\\mathbf{x}_{1} - \\mathbf{x}_{3}||^{2} = \\left\\|\\left(-\\frac{1}{2}-0,\\,0-\\frac{\\sqrt{13}}{4}\\right)\\right\\|^{2} = \\left(-\\frac{1}{2}\\right)^{2} + \\left(-\\frac{\\sqrt{13}}{4}\\right)^{2} = \\frac{1}{4} + \\frac{13}{16} = \\frac{4}{16} + \\frac{13}{16} = \\frac{17}{16}\n$$\nThe squared distance between $C_{2}$ and $C_{3}$ is:\n$$\nd(C_{2}, C_{3})^{2} = ||\\mu_{2} - \\mu_{3}||^{2} = ||\\mathbf{x}_{2} - \\mathbf{x}_{3}||^{2} = \\left\\|\\left(\\frac{1}{2}-0,\\,0-\\frac{\\sqrt{13}}{4}\\right)\\right\\|^{2} = \\left(\\frac{1}{2}\\right)^{2} + \\left(-\\frac{\\sqrt{13}}{4}\\right)^{2} = \\frac{1}{4} + \\frac{13}{16} = \\frac{4}{16} + \\frac{13}{16} = \\frac{17}{16}\n$$\nThe corresponding distances are $d(C_{1}, C_{2}) = \\sqrt{1} = 1$, and $d(C_{1}, C_{3}) = d(C_{2}, C_{3}) = \\sqrt{\\frac{17}{16}} = \\frac{\\sqrt{17}}{4}$.\nTo find the smallest distance, we compare $1$ and $\\frac{\\sqrt{17}}{4}$. Squaring both values gives $1^{2}=1$ and $\\left(\\frac{\\sqrt{17}}{4}\\right)^{2}=\\frac{17}{16}$. Since $1 < \\frac{17}{16}$, we have $1 < \\frac{\\sqrt{17}}{4}$.\nThus, the smallest distance is $d(C_{1}, C_{2}) = 1$.\n\nThe first step is to merge the closest clusters, $C_{1}$ and $C_{2}$. The merge height, $h_{1}$, is this minimum distance.\n$$\nh_{1} = 1\n$$\nLet the new cluster be $C_{12} = C_{1} \\cup C_{2} = \\{\\mathbf{x}_{1}, \\mathbf{x}_{2}\\}$. The centroid of this new cluster, $\\mu_{12}$, is the arithmetic mean of its constituent points:\n$$\n\\mu_{12} = \\frac{\\mathbf{x}_{1} + \\mathbf{x}_{2}}{2} = \\frac{1}{2} \\left[ \\left(-\\frac{1}{2}, 0\\right) + \\left(\\frac{1}{2}, 0\\right) \\right] = \\frac{1}{2} (0, 0) = (0, 0)\n$$\nThe centroid of the merged cluster is the origin, $(0,0)$.\n\nThe algorithm proceeds to the second step. We now have two clusters: $C_{12}$ and $C_{3}$. The next merge must be between these two. The second merge height, $h_{2}$, is the distance between their centroids, $\\mu_{12}$ and $\\mu_{3}$.\n$$\nh_{2} = d(C_{12}, C_{3}) = ||\\mu_{12} - \\mu_{3}|| = ||(0, 0) - \\left(0, \\frac{\\sqrt{13}}{4}\\right)|| = \\left\\|\\left(0, -\\frac{\\sqrt{13}}{4}\\right)\\right\\|\n$$\nCalculating the magnitude:\n$$\nh_{2} = \\sqrt{0^{2} + \\left(-\\frac{\\sqrt{13}}{4}\\right)^{2}} = \\sqrt{\\frac{13}{16}} = \\frac{\\sqrt{13}}{4}\n$$\nNow we verify that an inversion has occurred, i.e., $h_{2} < h_{1}$.\nWe need to compare $h_2 = \\frac{\\sqrt{13}}{4}$ with $h_1 = 1$.\nThe inequality is $\\frac{\\sqrt{13}}{4} < 1$.\nMultiplying by $4$, we get $\\sqrt{13} < 4$.\nSince both sides are positive, we can square them to obtain $13 < 16$. This inequality is true, which confirms that $h_{2} < h_{1}$. The centroid linkage method has produced a non-monotonic dendrogram, which is a known characteristic of this linkage type.\n\nFinally, we compute the inversion magnitude $\\Delta$, defined as $\\Delta=h_{2}-h_{1}$.\n$$\n\\Delta = \\frac{\\sqrt{13}}{4} - 1\n$$\nTo write this as a single expression, we use a common denominator:\n$$\n\\Delta = \\frac{\\sqrt{13} - 4}{4}\n$$\nThis is the exact, closed-form expression for the inversion magnitude.", "answer": "$$\n\\boxed{\\frac{\\sqrt{13}-4}{4}}\n$$", "id": "3140573"}, {"introduction": "The choice of linkage method can dramatically alter clustering outcomes, especially when the data contains outliers. This computational practice explores the relative sensitivity of single, complete, average, and centroid linkage to noisy data points. By adding outliers and observing the changes in cluster structure, you will gain direct insight into the \"chaining\" phenomenon of single linkage and its fundamental connection to the Minimum Spanning Tree (MST), providing a practical foundation for selecting robust clustering strategies. [@problem_id:3140585]", "problem": "You are given a base dataset of two tight clusters in the plane under the Euclidean metric. The first cluster $A$ comprises the five points $(-0.2,-0.2)$, $(-0.2,0.2)$, $(0.2,-0.2)$, $(0.2,0.2)$, and $(0,0)$. The second cluster $B$ is obtained by translating every point in $A$ by $4$ units along the $x$-axis, so $B$ comprises $(3.8,-0.2)$, $(3.8,0.2)$, $(4.2,-0.2)$, $(4.2,0.2)$, and $(4,0)$. All distances are Euclidean and expressed in unitless coordinates.\n\nStarting from this base dataset, you will add outliers as follows. For a given scalar distance $R > 0$ and a given integer count $s \\ge 1$, you add $s$ outlier points placed at coordinates $(-R,y_i)$ where $y_i$ are evenly spaced offsets centered at $0$ with step $0.2$, specifically $y_i = 0.2(i - (s-1)/2)$ for $i \\in \\{0,1,\\dots,s-1\\}$. This construction ensures each added outlier is at distance approximately $R$ from the centroid of cluster $A$ and far from cluster $B$ when $R$ is large.\n\nYou must write a complete, runnable program that:\n- Constructs the base dataset and computes pairwise Euclidean distances.\n- Computes the Minimum Spanning Tree (MST; Minimum Spanning Tree) of the complete graph induced by the base dataset, using edge weights equal to the Euclidean distances. Let $W_0$ denote the maximum edge weight in this base MST.\n- For each test case, adds the specified outliers to the base dataset, recomputes the MST, and lets $W_1$ denote the maximum edge weight in the new MST. Compute the increase $\\Delta W = W_1 - W_0$. Also compute $m_{\\text{new}}$, the number of MST edges that involve at least one added outlier node.\n- For each dataset (base and with outliers), perform hierarchical agglomerative clustering with four linkage methods: single, complete, average, and centroid. For each method, extract the final merge height $H$ (the height recorded at the last merge in the linkage result). For each test case, compute the increase in the final merge height relative to the base dataset, i.e., $\\Delta H_{\\text{single}}$, $\\Delta H_{\\text{complete}}$, $\\Delta H_{\\text{average}}$, and $\\Delta H_{\\text{centroid}}$.\n\nFundamental base for reasoning: Use the definitions of linkage methods and the well-known equivalence between single linkage clustering and Kruskal’s algorithm on the Minimum Spanning Tree, under a metric space with edge weights given by pairwise distances.\n\nYour program must implement the computations and produce the results for the following test suite of $(R,s)$ values:\n- $(1.0,1)$: one outlier placed relatively close to cluster $A$.\n- $(4.0,2)$: two outliers placed at a distance comparable to the separation between the cluster centroids.\n- $(8.0,1)$: one outlier placed far from both clusters.\n- $(3.0,3)$: three outliers at a moderate distance.\n\nFor each test case, your program should output a list of six values in the following order:\n- $\\Delta H_{\\text{single}}$\n- $\\Delta H_{\\text{complete}}$\n- $\\Delta H_{\\text{average}}$\n- $\\Delta H_{\\text{centroid}}$\n- $\\Delta W$\n- $m_{\\text{new}}$\n\nAll real-valued outputs must be represented as decimal floats, and the count $m_{\\text{new}}$ must be represented as an integer.\n\nFinal output format: Your program should produce a single line of output containing the results for all test cases as a comma-separated list of the per-test-case lists, enclosed in square brackets, with no spaces. For example, it should look like $[[v_{1,1},v_{1,2},\\dots,v_{1,6}],[v_{2,1},\\dots,v_{2,6}],\\dots]$, where $v_{i,j}$ denotes the $j$-th value for the $i$-th test case.", "solution": "The problem requires a computational analysis of the effects of outliers on hierarchical clustering and Minimum Spanning Tree (MST) structures. The solution involves generating a base dataset, adding specified outliers, and then computing changes in key metrics for several test cases.\n\nThe methodological approach is as follows:\n\n1.  **Dataset Construction**:\n    The base dataset is constructed from two distinct clusters, $A$ and $B$, in a $2$-dimensional plane. Cluster $A$ is defined by the five points $\\{(-0.2,-0.2), (-0.2,0.2), (0.2,-0.2), (0.2,0.2), (0,0)\\}$. Cluster $B$ is a translation of cluster $A$ by $4$ units along the positive $x$-axis, yielding the points $\\{(3.8,-0.2), (3.8,0.2), (4.2,-0.2), (4.2,0.2), (4,0)\\}$. The total number of points in the base dataset is $10$. For each test case defined by a pair $(R, s)$, where $R > 0$ is a distance and $s \\ge 1$ is an integer count, $s$ outlier points are generated. These outliers are placed at coordinates $(-R, y_i)$, with the vertical offsets $y_i$ defined by the formula $y_i = 0.2 \\times (i - (s-1)/2)$ for $i \\in \\{0, 1, \\dots, s-1\\}$. This construction places the outliers to the left of cluster $A$.\n\n2.  **Core Analytical Procedure**:\n    For any given set of points, a standard set of metrics is computed. This procedure is applied first to the base dataset and then to each augmented dataset (base + outliers).\n    -   **Pairwise Distances**: The analysis begins by computing the matrix of pairwise Euclidean distances between all points in the dataset. This distance matrix serves as the edge weights for a complete graph where the points are vertices.\n    -   **Minimum Spanning Tree (MST)**: The MST of this complete, weighted graph is computed using a standard algorithm, such as Prim's or Kruskal's. We identify $W$, the maximum edge weight present in the MST.\n    -   **Hierarchical Agglomerative Clustering**: Hierarchical clustering is performed using four different linkage methods: single, complete, average, and centroid. For each method, the result is a linkage matrix that describes the merging process. The key metric extracted is the final merge height, $H$, which is the distance or dissimilarity value at which the last two clusters are merged to form a single cluster containing all data points.\n\n3.  **Theoretical Linkage: Single Linkage and MST**:\n    A fundamental principle from graph theory and clustering states that single-linkage hierarchical clustering is equivalent to building an MST. Specifically, the sequence of merges in single-linkage clustering corresponds to the sequence of edges added in Kruskal's algorithm for constructing an MST. A direct consequence is that the final merge height in single-linkage clustering, $H_{\\text{single}}$, must be equal to the weight of the longest edge in the MST, $W$. This provides a crucial self-consistency check for the computation, as we expect to find that $\\Delta H_{\\text{single}} = \\Delta W$ in all test cases.\n\n4.  **Differential Analysis**:\n    The core of the problem is to quantify the change in metrics due to the introduction of outliers.\n    -   **Baseline Calculation**: First, the analytical procedure is run on the base dataset to obtain baseline values for the maximum MST edge weight ($W_0$) and the final merge heights for each of the four linkage methods ($H_{\\text{single,0}}$, $H_{\\text{complete,0}}$, $H_{\\text{average,0}}$, and $H_{\\text{centroid,0}}$).\n    -   **Test Case Calculation**: For each test case $(R, s)$, the specified outliers are added to the base dataset. The same analytical procedure is then run on this new, larger dataset to compute the new values: $W_1$, $H_{\\text{single,1}}$, $H_{\\text{complete,1}}$, $H_{\\text{average,1}}$, and $H_{\\text{centroid,1}}$.\n    -   **Change Calculation**: The increases, denoted by $\\Delta$, are computed as the difference between the new and baseline values:\n        $$ \\Delta H_{\\text{method}} = H_{\\text{method,1}} - H_{\\text{method,0}} $$\n        $$ \\Delta W = W_1 - W_0 $$\n    -   **MST Edge Count**: An additional metric, $m_{\\text{new}}$, is computed. This is the count of edges in the new MST that have at least one endpoint corresponding to an added outlier point. This quantifies how the outliers are integrated into the connectivity structure of the data.\n\n5.  **Implementation**:\n    The described procedure is implemented in a Python program. The `numpy` library is used for numerical operations and array management. The `scipy` library provides the necessary high-level functions for scientific computing: `scipy.spatial.distance.pdist` and `scipy.spatial.distance.squareform` for distance calculations, `scipy.sparse.csgraph.minimum_spanning_tree` for the MST, and `scipy.cluster.hierarchy.linkage` for hierarchical clustering. It is important to note that for the 'centroid' linkage method, the `linkage` function requires the raw data points as input, whereas for 'single', 'complete', and 'average', it operates on the condensed distance matrix. The program iterates through the specified test suite, calculates the six required output values for each case, and formats them into the specified string format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.sparse.csgraph import minimum_spanning_tree\nfrom scipy.cluster.hierarchy import linkage\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing the impact of outliers on hierarchical \n    clustering and Minimum Spanning Trees (MST).\n    \"\"\"\n\n    def analyze_dataset(points):\n        \"\"\"\n        Computes clustering and MST metrics for a given set of points.\n\n        Args:\n            points (np.ndarray): A NumPy array of shape (n, 2) representing n points.\n\n        Returns:\n            A tuple containing:\n            - H_single (float): Final merge height for single linkage.\n            - H_complete (float): Final merge height for complete linkage.\n            - H_average (float): Final merge height for average linkage.\n            - H_centroid (float): Final merge height for centroid linkage.\n            - W (float): Maximum edge weight in the MST.\n            - mst (csr_matrix): The computed Minimum Spanning Tree.\n        \"\"\"\n        n_points = points.shape[0]\n        if n_points < 2:\n            return 0.0, 0.0, 0.0, 0.0, 0.0, None\n\n        # Compute pairwise Euclidean distances\n        condensed_dist = pdist(points, 'euclidean')\n        dist_matrix = squareform(condensed_dist)\n\n        # Compute Minimum Spanning Tree (MST)\n        mst = minimum_spanning_tree(dist_matrix)\n        W = mst.max()\n\n        # Perform hierarchical clustering\n        # Single linkage\n        Z_single = linkage(condensed_dist, method='single')\n        H_single = Z_single[-1, 2]\n\n        # Complete linkage\n        Z_complete = linkage(condensed_dist, method='complete')\n        H_complete = Z_complete[-1, 2]\n\n        # Average linkage\n        Z_average = linkage(condensed_dist, method='average')\n        H_average = Z_average[-1, 2]\n\n        # Centroid linkage (requires original data points, not distance matrix)\n        Z_centroid = linkage(points, method='centroid')\n        H_centroid = Z_centroid[-1, 2]\n\n        return H_single, H_complete, H_average, H_centroid, W, mst\n\n    # --- Step 1: Define and analyze the base dataset ---\n    # Cluster A\n    cluster_A = np.array([\n        [-0.2, -0.2],\n        [-0.2,  0.2],\n        [ 0.2, -0.2],\n        [ 0.2,  0.2],\n        [ 0.0,  0.0]\n    ])\n\n    # Cluster B (translation of A by 4 units on x-axis)\n    cluster_B = cluster_A + np.array([4.0, 0.0])\n\n    base_dataset = np.vstack([cluster_A, cluster_B])\n    n_base_points = base_dataset.shape[0]\n\n    # Calculate baseline metrics\n    H_single_0, H_complete_0, H_average_0, H_centroid_0, W_0, _ = analyze_dataset(base_dataset)\n\n    # --- Step 2: Process each test case ---\n    test_cases = [\n        (1.0, 1),\n        (4.0, 2),\n        (8.0, 1),\n        (3.0, 3)\n    ]\n\n    all_results = []\n    for R, s in test_cases:\n        # Generate outlier points\n        y_offsets = 0.2 * (np.arange(s) - (s - 1) / 2.0)\n        outliers = np.zeros((s, 2))\n        outliers[:, 0] = -R\n        outliers[:, 1] = y_offsets\n\n        # Create the new dataset with outliers\n        new_dataset = np.vstack([base_dataset, outliers])\n\n        # Analyze the new dataset\n        H_single_1, H_complete_1, H_average_1, H_centroid_1, W_1, new_mst = analyze_dataset(new_dataset)\n        \n        # Compute the change in metrics (delta values)\n        delta_H_single = H_single_1 - H_single_0\n        delta_H_complete = H_complete_1 - H_complete_0\n        delta_H_average = H_average_1 - H_average_0\n        delta_H_centroid = H_centroid_1 - H_centroid_0\n        delta_W = W_1 - W_0\n\n        # Compute m_new: number of MST edges involving an outlier\n        # Outlier indices start at n_base_points\n        rows, cols = new_mst.nonzero()\n        m_new = np.sum((rows >= n_base_points) | (cols >= n_base_points))\n        \n        # Collect results for the current test case\n        case_results = [\n            delta_H_single,\n            delta_H_complete,\n            delta_H_average,\n            delta_H_centroid,\n            delta_W,\n            int(m_new)\n        ]\n        all_results.append(case_results)\n\n    # --- Step 3: Format the final output string ---\n    result_strings = []\n    for res_list in all_results:\n        # Convert all items to their string representation\n        # The last value m_new is already an int.\n        stringified_list = [str(item) for item in res_list]\n        result_strings.append(f\"[{','.join(stringified_list)}]\")\n        \n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3140585"}, {"introduction": "Moving beyond simple outliers, the underlying distribution of data points significantly impacts clustering performance. This advanced coding exercise investigates how complete and average linkage behave when faced with heavy-tailed data, where extreme values are more probable. By simulating clusters from a Student's $t$-distribution, you will empirically test the hypothesis that complete linkage's \"farthest-neighbor\" criterion offers greater resistance to prematurely merging clusters whose tails intermingle, a nuanced but crucial consideration in real-world data analysis. [@problem_id:3140593]", "problem": "You are asked to implement a hierarchical agglomerative clustering experiment to compare how different linkage methods behave on heavy-tailed synthetic data. The goal is to quantify, in a reproducible way, how complete linkage resists premature merging across cluster tails compared to average linkage, which smooths distances and can merge earlier. Your program must be a complete, runnable program as specified in the final answer section.\n\nStarting from core definitions, use the following fundamental base:\n- A metric space with the Euclidean distance, where for any points $x,y \\in \\mathbb{R}^2$, the distance is $d(x,y)=\\sqrt{(x_1-y_1)^2+(x_2-y_2)^2}$.\n- Hierarchical agglomerative clustering (HAC), which iteratively merges the two closest clusters under a specified inter-cluster distance. The inter-cluster distance depends on the linkage method:\n  - Complete linkage: for clusters $A$ and $B$, the distance is $D_{\\text{complete}}(A,B)=\\max_{x\\in A,y\\in B} d(x,y)$.\n  - Average linkage: for clusters $A$ and $B$, the distance is $D_{\\text{average}}(A,B)=\\frac{1}{|A||B|}\\sum_{x\\in A}\\sum_{y\\in B} d(x,y)$.\n- Heavy-tailed synthetic data generated from the Student's $t$-distribution with degrees of freedom $\\nu$, denoted $\\text{Student-}t(\\nu)$, sampled independently in two coordinates and scaled by a factor $\\sigma$.\n\nFor each test case, generate two clusters in $\\mathbb{R}^2$ as follows:\n1. Draw $n$ points for the left cluster and $n$ points for the right cluster. Each point’s coordinates are independently sampled from $\\text{Student-}t(\\nu)$ and then multiplied by $\\sigma$.\n2. Translate the left cluster by the vector $(-\\Delta/2, 0)$ and the right cluster by $(\\Delta/2, 0)$, where $\\Delta$ is the inter-center separation.\n3. Label the left-cluster points with label $0$ and right-cluster points with label $1$.\n4. Use a fixed random seed to ensure reproducibility.\n\nFor each linkage method (complete and average), perform HAC under the Euclidean metric. Track the first dendrogram merge height at which a cluster contains both labels $0$ and $1$. Denote these heights by $h_{\\text{complete}}$ and $h_{\\text{average}}$, respectively. The quantity of interest for each test case is the float $h_{\\text{complete}} - h_{\\text{average}}$. A positive value indicates that complete linkage resists cross-label tail merges to a greater dendrogram height than average linkage.\n\nImplement the above as a program and run it on the following test suite, which specifies $n$, $\\nu$, $\\sigma$, $\\Delta$, and the random seed for each case:\n- Test case $1$ (general heavy-tailed separation): $n=80$, $\\nu=3$, $\\sigma=0.8$, $\\Delta=5.0$, seed $=2023$.\n- Test case $2$ (closer centers with heavier tails): $n=80$, $\\nu=2$, $\\sigma=1.0$, $\\Delta=2.0$, seed $=7$.\n- Test case $3$ (large separation, moderately heavy tails): $n=60$, $\\nu=5$, $\\sigma=0.8$, $\\Delta=8.0$, seed $=99$.\n- Test case $4$ (near-Gaussian tails): $n=100$, $\\nu=25$, $\\sigma=1.0$, $\\Delta=4.0$, seed $=123$.\n\nNo physical units or angle units are involved. All outputs are real numbers without units. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order of the test cases, for example, $[r_1,r_2,r_3,r_4]$, where each $r_i$ is the computed $h_{\\text{complete}} - h_{\\text{average}}$ for test case $i$. The outputs must be floats.", "solution": "The problem as stated is valid. It is a well-defined computational experiment grounded in established principles of statistical learning, specifically hierarchical agglomerative clustering (HAC). The problem is self-contained, with all necessary parameters, definitions, and procedures clearly specified. It is scientifically sound, objective, and leads to a unique, verifiable result for each test case due to the use of fixed random seeds.\n\nThe objective is to quantify the difference in behavior between complete and average linkage methods when applied to datasets with two distinct, heavy-tailed clusters. The core hypothesis is that complete linkage, being sensitive to the maximum distance between clusters, will be more robust against prematurely merging clusters whose tails intermingle, compared to average linkage, which averages out these large distances.\n\nThe solution is implemented by following a sequence of rigorously defined steps for each test case.\n\n**Step 1: Synthetic Data Generation**\n\nFor each test case, we generate a dataset in $\\mathbb{R}^2$ consisting of two distinct clusters. The total number of points is $2n$.\nThe generation process for each point $(x_1, x_2)$ is as follows:\n1.  Two independent random variates, $z_1$ and $z_2$, are drawn from the Student's $t$-distribution with $\\nu$ degrees of freedom, denoted $\\text{Student-}t(\\nu)$. This distribution is chosen for its heavy tails, especially for small values of $\\nu$. As $\\nu \\to \\infty$, the $\\text{Student-}t(\\nu)$ distribution converges to the standard normal distribution $\\mathcal{N}(0,1)$.\n2.  These variates are scaled by a factor $\\sigma$, resulting in the coordinate pair $(\\sigma z_1, \\sigma z_2)$.\n3.  We generate $n$ such points for a \"left\" cluster and $n$ points for a \"right\" cluster.\n4.  To separate the clusters, the left cluster points are translated by the vector $(-\\Delta/2, 0)$, and the right cluster points are translated by $(\\Delta/2, 0)$. This places the nominal centers of the two clusters at a distance of $\\Delta$ from each other along the first coordinate axis.\n5.  Points originating from the left cluster are assigned label $0$, and those from the right cluster are assigned label $1$. This ground truth is used to evaluate the clustering outcome.\nThis entire process is made deterministic and reproducible by initializing the random number generator with a specific seed for each test case.\n\n**Step 2: Hierarchical Agglomerative Clustering (HAC)**\n\nHAC is an iterative algorithm that builds a hierarchy of clusters. It begins by treating each of the $2n$ data points as a singleton cluster. In each subsequent step, the two closest clusters are merged into a new, larger cluster. This process continues until only one cluster, containing all data points, remains. The sequence of merges forms a binary tree structure known as a dendrogram. The \"height\" at which two clusters are merged in the dendrogram corresponds to the inter-cluster distance at which the merge occurred.\n\nThe choice of how to measure the distance between two non-singleton clusters is determined by the linkage method. The problem specifies the Euclidean distance, $d(x,y)=\\sqrt{(x_1-y_1)^2+(x_2-y_2)^2}$, as the base metric between individual points $x, y \\in \\mathbb{R}^2$.\n\n**Step 3: Linkage Methods**\n\nThe inter-cluster distance is calculated using two different linkage criteria:\n\n1.  **Complete Linkage**: The distance between two clusters, $A$ and $B$, is defined as the maximum distance between any point in $A$ and any point in $B$.\n    $$D_{\\text{complete}}(A,B)=\\max_{x\\in A,y\\in B} d(x,y)$$\n    This \"farthest neighbor\" approach is conservative. For two clusters to be considered \"close\" by this metric, all of their points must be relatively close to all points in the other cluster. This property makes complete linkage sensitive to outliers and often results in more compact, spherical clusters. It is expected to resist merging the two main clusters if their tails contain points that are far apart.\n\n2.  **Average Linkage**: The distance between clusters $A$ and $B$ is the average of all pairwise distances between points from each cluster.\n    $$D_{\\text{average}}(A,B)=\\frac{1}{|A||B|}\\sum_{x\\in A}\\sum_{y\\in B} d(x,y)$$\n    This method provides a more \"central\" measure of cluster distance, effectively averaging out the influence of a few distant points. It is less sensitive to outliers than complete linkage but may cause clusters to merge if they are close on average, even if their boundaries are not well-separated.\n\n**Step 4: Quantifying the Merging Point**\n\nFor each test case and for each linkage method (complete and average), we perform HAC. We are interested in identifying the precise moment—represented by the dendrogram merge height—at which the algorithm first merges points from the two original, ground-truth clusters (label $0$ and label $1$).\n\nThe procedure is as follows:\n1.  Initialize $2n$ clusters, one for each data point, and store their ground-truth labels ($0$ or $1$).\n2.  Iterate through the merges of the HAC algorithm, from the lowest height to the highest.\n3.  For each merge, a new cluster is formed. We determine the set of ground-truth labels present in this new cluster by taking the union of the label sets from the two smaller clusters that were merged.\n4.  The first time a newly formed cluster contains both label $0$ and label $1$, we record the corresponding merge height. Let this height be $h_{\\text{complete}}$ for the complete linkage method and $h_{\\text{average}}$ for the average linkage method.\n5.  The final quantity of interest for the test case is the difference $h_{\\text{complete}} - h_{\\text{average}}$. A positive value for this difference confirms the hypothesis that complete linkage maintains the separation between the two initial clusters up to a larger distance threshold than average linkage does on this type of data.\n\nThis entire protocol is systematically applied to each of the $4$ test cases specified, yielding $4$ numerical results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t\nfrom scipy.cluster import hierarchy\n\ndef solve():\n    \"\"\"\n    Main function to run the hierarchical clustering experiment on the test suite.\n    \"\"\"\n    # Test cases specify n (points per cluster), nu (degrees of freedom for t-dist),\n    # sigma (scaling factor), Delta (inter-center separation), and a random seed.\n    test_cases = [\n        # (n, nu, sigma, Delta, seed)\n        (80, 3, 0.8, 5.0, 2023), # Test case 1\n        (80, 2, 1.0, 2.0, 7),    # Test case 2\n        (60, 5, 0.8, 8.0, 99),   # Test case 3\n        (100, 25, 1.0, 4.0, 123),  # Test case 4\n    ]\n\n    results = []\n    for n, nu, sigma, delta, seed in test_cases:\n        # Generate the synthetic data for the current test case.\n        X, labels = generate_data(n, nu, sigma, delta, seed)\n\n        # Calculate the first cross-label merge height for complete linkage.\n        h_complete = find_first_cross_label_merge_height(X, labels, 'complete')\n\n        # Calculate the first cross-label merge height for average linkage.\n        h_average = find_first_cross_label_merge_height(X, labels, 'average')\n\n        # The quantity of interest is the difference in merge heights.\n        result = h_complete - h_average\n        results.append(result)\n\n    # Print the final results in the specified format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\ndef generate_data(n, nu, sigma, delta, seed):\n    \"\"\"\n    Generates two heavy-tailed clusters in 2D space.\n\n    Args:\n        n (int): Number of points per cluster.\n        nu (float): Degrees of freedom for the Student's t-distribution.\n        sigma (float): Scaling factor for the distribution.\n        delta (float): Separation distance between cluster centers.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]:\n            - A NumPy array of shape (2*n, 2) containing the coordinates of all points.\n            - A NumPy array of shape (2*n,) containing the ground-truth labels (0 or 1).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Generate points for the left cluster from the Student's t-distribution.\n    # The rvs function is used with a generator for reproducible randomness.\n    left_cluster = t.rvs(df=nu, size=(n, 2), random_state=rng) * sigma\n    left_cluster[:, 0] -= delta / 2\n\n    # Generate points for the right cluster.\n    right_cluster = t.rvs(df=nu, size=(n, 2), random_state=rng) * sigma\n    right_cluster[:, 0] += delta / 2\n\n    # Combine points and create corresponding labels.\n    X = np.vstack([left_cluster, right_cluster])\n    labels = np.array([0] * n + [1] * n)\n\n    return X, labels\n\ndef find_first_cross_label_merge_height(X, labels, method):\n    \"\"\"\n    Performs HAC and finds the first merge height that combines points from different labels.\n\n    Args:\n        X (np.ndarray): The data points, shape (N, 2).\n        labels (np.ndarray): The ground-truth labels, shape (N,).\n        method (str): The linkage method ('complete' or 'average').\n\n    Returns:\n        float: The dendrogram height of the first cross-label merge.\n    \"\"\"\n    N = X.shape[0]\n\n    # Perform hierarchical agglomerative clustering using the specified method.\n    # The linkage matrix Z encodes the dendrogram.\n    Z = hierarchy.linkage(X, method=method, metric='euclidean')\n\n    # `cluster_labels` will store the set of original labels for each cluster.\n    # Initially, clusters 0 to N-1 are the original points.\n    cluster_labels = {i: {labels[i]} for i in range(N)}\n\n    # Iterate through each merge recorded in the linkage matrix Z.\n    # Each row `i` represents a merge creating a new cluster with index `N + i`.\n    for i in range(Z.shape[0]):\n        # Get the indices of the two clusters being merged.\n        # Indices < N are original points, indices >= N are newly formed clusters.\n        id1, id2 = int(Z[i, 0]), int(Z[i, 1])\n\n        # Retrieve the sets of original labels for the merging clusters.\n        labels1 = cluster_labels[id1]\n        labels2 = cluster_labels[id2]\n\n        # The new cluster's labels are the union of the merged clusters' labels.\n        new_labels = labels1.union(labels2)\n\n        # The new cluster's index is N + i. We store its label set.\n        cluster_id = N + i\n        cluster_labels[cluster_id] = new_labels\n\n        # Check if the new cluster is \"impure\", i.e., contains both original labels.\n        if 0 in new_labels and 1 in new_labels:\n            # If so, this is the first cross-label merge.\n            # The height of this merge is given in the 3rd column of Z.\n            merge_height = Z[i, 2]\n            return merge_height\n\n    # This part should not be reached in a valid scenario with two labels.\n    # It would imply all points had the same label, which is not the case.\n    return np.inf\n\n# Execute the main function.\nsolve()\n```", "id": "3140593"}]}