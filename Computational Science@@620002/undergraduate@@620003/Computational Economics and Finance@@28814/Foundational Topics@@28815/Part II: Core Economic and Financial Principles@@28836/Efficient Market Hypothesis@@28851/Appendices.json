{"hands_on_practices": [{"introduction": "The cornerstone of the Efficient Market Hypothesis is the weak-form, which posits that past price data contains no information about future prices. This practice challenges you to test this hypothesis directly by simulating financial time series and applying rigorous statistical tests [@problem_id:2389249]. By implementing tools like the Ljung-Box Q-test and regression analysis, you will develop a practical understanding of how to distinguish a predictable series from an unpredictable random walk, a fundamental skill for any financial analyst.", "problem": "Consider a discrete-time series of average transaction fees per byte for a blockchain over equally spaced intervals. Let the strictly positive fee level be denoted by $f_t$ for time index $t \\in \\{0,1,2,\\dots,n\\}$, and define the log-fee $y_t = \\ln(f_t)$. The one-step log return is $r_t = y_t - y_{t-1}$ for $t \\in \\{1,2,\\dots,n\\}$. Under the weak-form Efficient Market Hypothesis (EMH), the process $\\{r_t\\}$ is a martingale difference sequence with respect to the information set generated by past values, implying $E[r_t \\mid \\mathcal{I}_{t-1}] = 0$ and zero linear predictability from $\\{r_{t-1}, r_{t-2}, \\dots\\}$. Equivalently, under the weak-form EMH, all nonzero-lag autocorrelations of $\\{r_t\\}$ are zero and the best linear one-step-ahead predictor of $r_t$ based on $r_{t-1}$ has zero slope.\n\nYour task is to write a complete program that, for each of the following test cases, simulates a fee series $\\{f_t\\}$ according to the specified data-generating process, computes the return series $\\{r_t\\}$, and then classifies the series as weak-form efficient or not at a given significance level. A series must be classified as weak-form efficient if and only if both of the following properties are supported by the data at the stated significance level $\\alpha$: (i) the joint null that all autocorrelations up to lag $m$ are zero is not rejected, and (ii) the null that the slope coefficient in the best linear predictor of $r_t$ from $r_{t-1}$ is zero is not rejected. Otherwise, classify the series as not weak-form efficient.\n\nSimulation should proceed as follows. For each test case, fix the initial level at $f_0 = 100.0$. For all stochastic simulations, use a fixed random seed equal to $123456$ so that results are reproducible. There are two process specifications:\n- Random walk in log fees (denoted RW): $y_t = y_{t-1} + \\varepsilon_t$ with $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ independently over $t$, which implies $r_t = \\varepsilon_t$.\n- Autoregressive of order one in returns (denoted AR1): $r_t = \\phi r_{t-1} + \\varepsilon_t$ with $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ independently over $t$, and $y_t = y_{t-1} + r_t$. For this case use a burn-in of $200$ steps that are discarded to approximate stationarity of $\\{r_t\\}$ before collecting the $n$ retained observations. For RW, no burn-in is needed.\n\nFor each test case, after simulating $\\{f_t\\}$ and computing $\\{r_t\\}$, apply valid statistical decision rules under the null of Gaussian martingale difference innovations to determine whether to reject each of the two properties at significance level $\\alpha$. Use the following test suite, where each case is given by the tuple $(S, n, \\sigma, \\phi, \\alpha, m)$, with $\\phi$ ignored when $S=\\text{RW}$:\n- Case A: $(\\text{RW},\\, 500,\\, 0.02,\\, 0.0,\\, 0.01,\\, 10)$\n- Case B: $(\\text{AR1},\\, 500,\\, 0.02,\\, 0.35,\\, 0.01,\\, 10)$\n- Case C: $(\\text{AR1},\\, 80,\\, 0.03,\\, 0.0,\\, 0.01,\\, 8)$\n- Case D: $(\\text{AR1},\\, 500,\\, 0.02,\\, -0.5,\\, 0.01,\\, 10)$\n\nYour program must output a single line containing the four boolean classification results in order for Cases A through D, using the exact format of a comma-separated list enclosed in square brackets with no spaces, where each entry is either True or False. For example, the required format is analogous to [True,False,True,False].", "solution": "Under the weak-form Efficient Market Hypothesis (EMH), the increments of the log-fee $r_t = y_t - y_{t-1}$ form a martingale difference sequence (MDS) with respect to the natural filtration. Formally, this is expressed as $E[r_t \\mid \\mathcal{I}_{t-1}] = 0$ for all $t$, where $\\mathcal{I}_{t-1}$ is the sigma-algebra generated by past observations. Two operational implications of this statement are: (i) the autocorrelation function at nonzero lags is zero, and (ii) the best linear predictor of $r_t$ based on $r_{t-1}$ has zero slope.\n\nData generation. For the Random Walk (RW) specification, the log-fee follows $y_t = y_{t-1} + \\varepsilon_t$ with $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ and $r_t = \\varepsilon_t$. For the Autoregressive of order one (AR1) specification, the return follows $r_t = \\phi r_{t-1} + \\varepsilon_t$ with $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$ and $y_t = y_{t-1} + r_t$. In all cases, the level is $f_t = \\exp(y_t)$ to ensure positivity. For AR1, initialization effects are mitigated by generating a burn-in of $200$ steps that are discarded, so that the retained $n$ observations approximately follow the stationary distribution when $|\\phi| < 1$. All random draws are produced using a fixed seed of $123456$ to ensure reproducibility.\n\nTesting property (i): joint zero autocorrelations up to lag $m$. Given a sample $\\{r_t\\}_{t=1}^n$, define the centered returns $\\tilde{r}_t = r_t - \\bar{r}$ with $\\bar{r} = \\frac{1}{n}\\sum_{t=1}^n r_t$. The sample autocovariance at lag $k$ is $\\hat{\\gamma}_k = \\frac{1}{n}\\sum_{t=k+1}^n \\tilde{r}_t \\tilde{r}_{t-k}$, and the lag zero autocovariance is $\\hat{\\gamma}_0 = \\frac{1}{n}\\sum_{t=1}^n \\tilde{r}_t^2$. The sample autocorrelation at lag $k$ is $\\hat{\\rho}_k = \\hat{\\gamma}_k / \\hat{\\gamma}_0$. A portmanteau statistic that aggregates these up to lag $m$ is the Ljung–Box statistic\n$$\nQ = n(n+2)\\sum_{k=1}^m \\frac{\\hat{\\rho}_k^2}{n-k}.\n$$\nUnder the null of zero autocorrelations at lags $1$ through $m$ and conditional homoskedasticity, $Q$ is asymptotically distributed as $\\chi^2_m$. The decision rule is to compute the $p$-value $p_{\\text{LB}} = 1 - F_{\\chi^2_m}(Q)$ and to reject the null if $p_{\\text{LB}} < \\alpha$.\n\nTesting property (ii): zero slope in the best linear one-step-ahead predictor. Consider the linear model\n$$\nr_t = \\beta_0 + \\beta_1 r_{t-1} + \\varepsilon_t^{\\ast}, \\quad t=2,\\dots,n,\n$$\nestimated by ordinary least squares (OLS). Let $X$ be the $(n-1)\\times 2$ matrix with a column of ones and a column of lagged returns $r_{t-1}$, and let $y$ be the $(n-1)\\times 1$ vector of contemporaneous returns $r_t$. The OLS estimator is $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$. The residual variance is $\\hat{\\sigma}^2 = \\frac{1}{n-1-2}\\sum_{t=2}^n \\hat{\\varepsilon}_t^2$, where $\\hat{\\varepsilon}_t = r_t - \\hat{\\beta}_0 - \\hat{\\beta}_1 r_{t-1}$. The estimated variance of $\\hat{\\beta}_1$ is the $(2,2)$ element of $\\hat{\\sigma}^2 (X^{\\top}X)^{-1}$, denoted $\\widehat{\\mathrm{Var}}(\\hat{\\beta}_1)$. The test statistic is\n$$\nt = \\frac{\\hat{\\beta}_1}{\\sqrt{\\widehat{\\mathrm{Var}}(\\hat{\\beta}_1)}},\n$$\nwhich under the null $H_0:\\beta_1=0$ and conditional homoskedastic normal errors follows a Student $t$ distribution with $n-3$ degrees of freedom. The two-sided $p$-value is $p_{\\text{REG}} = 2\\left(1 - F_{t_{n-3}}(|t|)\\right)$. Reject the null if $p_{\\text{REG}} < \\alpha$.\n\nClassification rule. For each case, compute both $p_{\\text{LB}}$ and $p_{\\text{REG}}$ at the provided $\\alpha$. Classify the series as weak-form efficient if and only if both nulls are not rejected, that is, if and only if $p_{\\text{LB}} \\ge \\alpha$ and $p_{\\text{REG}} \\ge \\alpha$.\n\nTest suite parameters. Each test case is specified by $(S, n, \\sigma, \\phi, \\alpha, m)$ with $\\phi$ ignored when $S=\\text{RW}$, and with $f_0=100.0$ and the fixed seed $123456$ used for all simulations:\n- Case A: $(\\text{RW},\\, 500,\\, 0.02,\\, 0.0,\\, 0.01,\\, 10)$, representing a random walk in log fees with independent and identically distributed Gaussian increments, which satisfies the weak-form EMH in expectation.\n- Case B: $(\\text{AR1},\\, 500,\\, 0.02,\\, 0.35,\\, 0.01,\\, 10)$, representing positively autocorrelated returns, which violates weak-form EMH due to linear predictability.\n- Case C: $(\\text{AR1},\\, 80,\\, 0.03,\\, 0.0,\\, 0.01,\\, 8)$, representing uncorrelated returns with a smaller sample size as a boundary-type case.\n- Case D: $(\\text{AR1},\\, 500,\\, 0.02,\\, -0.5,\\, 0.01,\\, 10)$, representing negatively autocorrelated returns, which violates weak-form EMH due to linear predictability.\n\nFinal output format. Your program should produce a single line of output containing the four boolean results for Cases A through D as a comma-separated list enclosed in square brackets and with no spaces, for example: [True,False,True,False].", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import chi2, t as student_t\n\ndef ljung_box_pvalue(returns: np.ndarray, m: int) -> float:\n    \"\"\"\n    Compute the Ljung-Box Q statistic p-value for autocorrelations up to lag m.\n    returns: 1D array of returns r_t\n    m: maximum lag to include\n    \"\"\"\n    r = returns.astype(float)\n    n = r.shape[0]\n    if n <= m + 1:\n        # Not enough data; return p-value of 0 to force non-acceptance.\n        return 0.0\n    r_centered = r - r.mean()\n    gamma0 = np.dot(r_centered, r_centered) / n\n    if gamma0 <= 0:\n        # Degenerate variance: treat as non-acceptance.\n        return 0.0\n    Q = 0.0\n    for k in range(1, m + 1):\n        num = np.dot(r_centered[k:], r_centered[:-k]) / n\n        rho_k = num / gamma0\n        Q += rho_k * rho_k / (n - k)\n    Q *= n * (n + 2)\n    p_val = 1.0 - chi2.cdf(Q, df=m)\n    # Numerical guard\n    if p_val < 0.0:\n        p_val = 0.0\n    if p_val > 1.0:\n        p_val = 1.0\n    return p_val\n\ndef ols_predictability_pvalue(returns: np.ndarray) -> float:\n    \"\"\"\n    Test H0: beta1 = 0 in the regression r_t = beta0 + beta1 * r_{t-1} + e_t.\n    Returns the two-sided p-value based on Student t with n-3 degrees of freedom.\n    \"\"\"\n    r = returns.astype(float)\n    n = r.shape[0]\n    # Need at least 3 observations to compute t-stat with df = n-3 >= 1\n    if n < 4:\n        return 0.0\n    y = r[1:]\n    x = r[:-1]\n    X = np.column_stack([np.ones_like(x), x])\n    XtX = X.T @ X\n    # Check for invertibility\n    try:\n        XtX_inv = np.linalg.inv(XtX)\n    except np.linalg.LinAlgError:\n        return 0.0\n    beta_hat = XtX_inv @ (X.T @ y)\n    residuals = y - X @ beta_hat\n    dof = y.shape[0] - X.shape[1]\n    if dof <= 0:\n        return 0.0\n    rss = float(residuals.T @ residuals)\n    sigma2_hat = rss / dof\n    var_beta = sigma2_hat * XtX_inv\n    se_beta1 = np.sqrt(max(var_beta[1, 1], 0.0))\n    if se_beta1 == 0.0:\n        # No variability; return 0 p-value to avoid false acceptance\n        return 0.0\n    t_stat = beta_hat[1] / se_beta1\n    # Two-sided p-value\n    p_val = 2.0 * (1.0 - student_t.cdf(abs(t_stat), df=dof))\n    # Numerical guard\n    if p_val < 0.0:\n        p_val = 0.0\n    if p_val > 1.0:\n        p_val = 1.0\n    return p_val\n\ndef simulate_series(case, rng: np.random.Generator):\n    \"\"\"\n    Simulate fee levels and returns based on the specified test case.\n    case: tuple (S, n, sigma, phi, alpha, m)\n    rng: numpy Generator for reproducibility\n    Returns: returns array r of length n\n    \"\"\"\n    S, n, sigma, phi, alpha, m = case\n    y0 = np.log(100.0)  # initial log-fee\n    if S == \"RW\":\n        # Random walk in log fees: y_t = y_{t-1} + e_t\n        eps = rng.normal(loc=0.0, scale=sigma, size=n)\n        r = eps  # returns are the innovations\n        # Form y and f if needed (not used directly for tests)\n        # y = y0 + np.cumsum(r)\n        return r\n    elif S == \"AR1\":\n        # AR(1) in returns with burn-in for stationarity\n        burn = 200\n        total = n + burn\n        eps = rng.normal(loc=0.0, scale=sigma, size=total)\n        r_full = np.empty(total, dtype=float)\n        r_full[0] = eps[0]\n        for t in range(1, total):\n            r_full[t] = phi * r_full[t - 1] + eps[t]\n        r = r_full[burn:]\n        return r\n    else:\n        raise ValueError(\"Unknown specification S: {}\".format(S))\n\ndef classify_efficiency(case, rng: np.random.Generator) -> bool:\n    \"\"\"\n    Classify a single test case as weak-form efficient (True) or not (False).\n    \"\"\"\n    S, n, sigma, phi, alpha, m = case\n    r = simulate_series(case, rng)\n    # Property (i): joint zero autocorrelations up to lag m\n    p_lb = ljung_box_pvalue(r, m)\n    # Property (ii): zero slope in linear predictability\n    p_reg = ols_predictability_pvalue(r)\n    # Efficient if both nulls are not rejected at level alpha\n    return (p_lb >= alpha) and (p_reg >= alpha)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\"RW\", 500, 0.02, 0.0, 0.01, 10),     # Case A\n        (\"AR1\", 500, 0.02, 0.35, 0.01, 10),   # Case B\n        (\"AR1\", 80, 0.03, 0.0, 0.01, 8),      # Case C\n        (\"AR1\", 500, 0.02, -0.5, 0.01, 10),   # Case D\n    ]\n\n    rng = np.random.default_rng(123456)\n\n    results = []\n    for case in test_cases:\n        result = classify_efficiency(case, rng)\n        results.append(result)\n\n    # Final print statement in the exact required format: booleans without spaces.\n    print(\"[\" + \",\".join(\"True\" if r else \"False\" for r in results) + \"]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2389249"}, {"introduction": "While the EMH provides a powerful theoretical baseline, empirical finance has uncovered several persistent \"anomalies.\" This exercise immerses you in one of the most famous: the momentum effect, where past winners tend to keep winning and past losers tend to keep losing [@problem_id:2389238]. You will move beyond simple statistical tests to build a basic quantitative strategy, calculate its returns, and evaluate its performance after adjusting for risk, providing insight into how market inefficiencies might be identified and measured in practice.", "problem": "You are given a formal testing task related to the Efficient Market Hypothesis (EMH) in the context of the \"momentum\" anomaly. For an asset with a time series of period-by-period simple returns $\\{r_t\\}_{t=1}^T$ expressed as decimal fractions (for example, a one percent return is written as $0.01$), define a momentum signal with lookback window $L \\in \\mathbb{N}$ and a volatility-adjusted strength metric as follows.\n\n- For each time index $t$ with $t \\in \\{L+1, \\ldots, T\\}$, define the lookback sum $b_t = \\sum_{i=1}^{L} r_{t-i}$ and the signal\n$$\ns_t = \n\\begin{cases}\n+1, & \\text{if } b_t > 0, \\\\\n-1, & \\text{if } b_t < 0, \\\\\n0, & \\text{if } b_t = 0.\n\\end{cases}\n$$\n- The realized momentum strategy return at time $t$ is $x_t = s_t \\cdot r_t$.\n- Let $N = T - L$ denote the number of non-warmup observations. Define the sample mean $\\mu = \\frac{1}{N} \\sum_{t=L+1}^{T} x_t$ and the population-standard-deviation $\\sigma = \\sqrt{\\frac{1}{N} \\sum_{t=L+1}^{T} (x_t - \\mu)^2}$. Define the volatility-adjusted strength (a per-period Sharpe-like ratio) as\n$$\nS =\n\\begin{cases}\n\\mu / \\sigma, & \\text{if } \\sigma > 0, \\\\\n0, & \\text{if } \\sigma = 0.\n\\end{cases}\n$$\n\nFor each test case, you are given two return series of equal length: one representing a cryptocurrency and one representing a traditional equity index. For each test case, compute $S_{\\text{crypto}}$ and $S_{\\text{equity}}$ using the same $L$. Using a tolerance $\\varepsilon = 10^{-12}$, classify the strength comparison as an integer:\n- Output $1$ if $S_{\\text{crypto}} > S_{\\text{equity}} + \\varepsilon$ (crypto stronger),\n- Output $0$ if $\\lvert S_{\\text{crypto}} - S_{\\text{equity}} \\rvert \\le \\varepsilon$ (equal within tolerance),\n- Output $-1$ if $S_{\\text{crypto}} < S_{\\text{equity}} - \\varepsilon$ (crypto weaker).\n\nAll returns are dimensionless and must be treated as decimals, not percentages. Angles are not used. Your program must produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, for example, \"[1,0,-1]\".\n\nTest suite:\n- Test case $1$ (happy path, stronger cryptocurrency momentum after volatility adjustment), with $L = 2$:\n    - Cryptocurrency returns $r^{(C)}$: $(0.02, 0.015, 0.018, 0.017, -0.005, 0.02, 0.019, 0.018, 0.017, 0.016, 0.015, 0.014)$,\n    - Equity returns $r^{(E)}$: $(0.005, -0.006, 0.004, -0.005, 0.003, -0.004, 0.002, -0.003, 0.001, -0.002, 0.001, -0.001)$.\n- Test case $2$ (equality by construction), with $L = 3$:\n    - Cryptocurrency returns $r^{(C)}$: $(0.01, -0.005, 0.012, 0.0, 0.008, -0.004, 0.009, -0.003, 0.007, -0.002)$,\n    - Equity returns $r^{(E)}$: $(0.01, -0.005, 0.012, 0.0, 0.008, -0.004, 0.009, -0.003, 0.007, -0.002)$.\n- Test case $3$ (crypto weaker; equity trending while crypto alternates), with $L = 2$:\n    - Cryptocurrency returns $r^{(C)}$: $(0.03, -0.025, 0.03, -0.025, 0.03, -0.025, 0.03, -0.025)$,\n    - Equity returns $r^{(E)}$: $(0.008, 0.009, 0.01, 0.011, 0.012, 0.013, 0.012, 0.011)$.\n- Test case $4$ (edge: zero volatility), with $L = 1$:\n    - Cryptocurrency returns $r^{(C)}$: $(0.0, 0.0, 0.0, 0.0)$,\n    - Equity returns $r^{(E)}$: $(0.0, 0.0, 0.0, 0.0)$.\n- Test case $5$ (boundary: minimal effective sample), with $L = 2$:\n    - Cryptocurrency returns $r^{(C)}$: $(0.01, -0.01, 0.02)$,\n    - Equity returns $r^{(E)}$: $(0.01, -0.01, 0.02)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, \"[result1,result2,result3,result4,result5]\").", "solution": "The problem as stated is subjected to rigorous validation and is found to be valid. It is scientifically grounded in the domain of computational finance, specifically concerning the testing of the Efficient Market Hypothesis. The definitions are mathematically precise, the data provided are self-contained and consistent, and the objective is clear and formalizable. There are no contradictions, ambiguities, or violations of scientific principles. We may, therefore, proceed to construct a solution.\n\nThe primary objective is to compute a volatility-adjusted momentum strength metric, denoted $S$, for two given time series of asset returns—one for a cryptocurrency and one for an equity index—and to compare them. The entire process will be encapsulated within a computational framework designed for rigorous and repeatable analysis.\n\nThe core of the solution is a function designed to calculate the strength metric $S$ for a single return series $\\{r_t\\}_{t=1}^T$ and a specified lookback window $L \\in \\mathbb{N}$. The algorithm proceeds as follows:\n\n1.  The inputs are the time series of returns, which we represent as a numerical array, and the integer lookback window size $L$. The total number of time periods is $T$.\n\n2.  The calculation is performed over the non-warmup period, for time indices $t$ from $L+1$ to $T$. For each such $t$, we first compute the lookback sum, $b_t$, defined as the sum of the previous $L$ returns:\n    $$\n    b_t = \\sum_{i=1}^{L} r_{t-i}\n    $$\n    This operation is equivalent to applying a sliding-window-summation filter to the return series.\n\n3.  Based on the sign of $b_t$, a momentum signal $s_t$ is generated. This signal directs the strategy to take a long ($+1$), short ($-1$), or neutral ($0$) position. The signal is defined as:\n    $$\n    s_t = \\text{sgn}(b_t) = \n    \\begin{cases}\n    +1, & \\text{if } b_t > 0, \\\\\n    -1, & \\text{if } b_t < 0, \\\\\n    0, & \\text{if } b_t = 0.\n    \\end{cases}\n    $$\n\n4.  The realized return of the momentum strategy at time $t$, denoted $x_t$, is the product of the signal and the actual asset return at time $t$:\n    $$\n    x_t = s_t \\cdot r_t\n    $$\n    This step effectively implements the trading strategy: profiting from continued movement in the direction of the signal.\n\n5.  This process is repeated for all $t \\in \\{L+1, \\ldots, T\\}$, generating a series of $N = T - L$ strategy returns $\\{x_t\\}$.\n\n6.  From this series of strategy returns, we compute its sample mean, $\\mu$, and its population standard deviation, $\\sigma$. These are given by the standard formulae:\n    $$\n    \\mu = \\frac{1}{N} \\sum_{t=L+1}^{T} x_t\n    $$\n    $$\n    \\sigma = \\sqrt{\\frac{1}{N} \\sum_{t=L+1}^{T} (x_t - \\mu)^2}\n    $$\n    The use of the population standard deviation ($1/N$ normalization) is explicitly required.\n\n7.  Finally, the volatility-adjusted strength $S$ is calculated. It is a Sharpe-like ratio, defined as the mean strategy return divided by its volatility. A special condition is defined for the case of zero volatility:\n    $$\n    S =\n    \\begin{cases}\n    \\mu / \\sigma, & \\text{if } \\sigma > 0, \\\\\n    0, & \\text{if } \\sigma = 0.\n    \\end{cases}\n    $$\n    The case $\\sigma=0$ arises if and only if all strategy returns $x_t$ are identical. This includes the boundary case where $N=1$, for which the standard deviation is necessarily $0$.\n\nThe main program structure iterates through each provided test case. For each case, it calls the function described above twice: once for the cryptocurrency returns $r^{(C)}$ to compute $S_{\\text{crypto}}$, and once for the equity returns $r^{(E)}$ to compute $S_{\\text{equity}}$.\n\nThe two strength metrics are then compared using a numerical tolerance $\\varepsilon = 10^{-12}$ to handle floating-point arithmetic limitations. The comparison yields an integer classification:\n-   $1$ if $S_{\\text{crypto}} > S_{\\text{equity}} + \\varepsilon$\n-   $-1$ if $S_{\\text{crypto}} < S_{\\text{equity}} - \\varepsilon$\n-   $0$ if $\\lvert S_{\\text{crypto}} - S_{\\text{equity}} \\rvert \\le \\varepsilon$\n\nThese integer results from all test cases are collected and formatted into a single output string as specified by the problem statement. The implementation will utilize routines from the `numpy` library for efficient array manipulations and statistical calculations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by calculating and comparing the volatility-adjusted\n    momentum strength for given asset return series.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1\n        {\n            \"L\": 2,\n            \"r_crypto\": np.array([0.02, 0.015, 0.018, 0.017, -0.005, 0.02, 0.019, 0.018, 0.017, 0.016, 0.015, 0.014]),\n            \"r_equity\": np.array([0.005, -0.006, 0.004, -0.005, 0.003, -0.004, 0.002, -0.003, 0.001, -0.002, 0.001, -0.001]),\n        },\n        # Test case 2\n        {\n            \"L\": 3,\n            \"r_crypto\": np.array([0.01, -0.005, 0.012, 0.0, 0.008, -0.004, 0.009, -0.003, 0.007, -0.002]),\n            \"r_equity\": np.array([0.01, -0.005, 0.012, 0.0, 0.008, -0.004, 0.009, -0.003, 0.007, -0.002]),\n        },\n        # Test case 3\n        {\n            \"L\": 2,\n            \"r_crypto\": np.array([0.03, -0.025, 0.03, -0.025, 0.03, -0.025, 0.03, -0.025]),\n            \"r_equity\": np.array([0.008, 0.009, 0.01, 0.011, 0.012, 0.013, 0.012, 0.011]),\n        },\n        # Test case 4\n        {\n            \"L\": 1,\n            \"r_crypto\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"r_equity\": np.array([0.0, 0.0, 0.0, 0.0]),\n        },\n        # Test case 5\n        {\n            \"L\": 2,\n            \"r_crypto\": np.array([0.01, -0.01, 0.02]),\n            \"r_equity\": np.array([0.01, -0.01, 0.02]),\n        },\n    ]\n\n    def calculate_strength(returns: np.ndarray, L: int) -> float:\n        \"\"\"\n        Calculates the volatility-adjusted strength (S) for a given series of returns.\n        \n        Args:\n            returns: A numpy array of asset returns.\n            L: The lookback window size.\n\n        Returns:\n            The calculated strength metric S.\n        \"\"\"\n        T = len(returns)\n        if T <= L:\n            # Not enough data for any non-warmup observation.\n            return 0.0\n\n        strategy_returns = []\n        # Loop from t = L+1 to T (using 0-based indexing for t from L to T-1)\n        for t in range(L, T):\n            # Calculate lookback sum b_t = sum of returns from t-L to t-1\n            lookback_sum = np.sum(returns[t-L:t])\n            \n            # Determine signal s_t\n            signal = np.sign(lookback_sum)\n            \n            # Calculate realized momentum return x_t\n            realized_return = signal * returns[t]\n            strategy_returns.append(realized_return)\n\n        x = np.array(strategy_returns)\n        \n        # N is the number of non-warmup observations\n        N = len(x)\n        if N == 0:\n            return 0.0\n            \n        mu = np.mean(x)\n        # sigma is the population standard deviation (ddof=0 is the default in numpy.std)\n        sigma = np.std(x)\n\n        if sigma > 0:\n            S = mu / sigma\n        else:\n            S = 0.0\n            \n        return S\n\n    results = []\n    epsilon = 1e-12\n\n    for case in test_cases:\n        L = case[\"L\"]\n        r_crypto = case[\"r_crypto\"]\n        r_equity = case[\"r_equity\"]\n\n        s_crypto = calculate_strength(r_crypto, L)\n        s_equity = calculate_strength(r_equity, L)\n\n        if s_crypto > s_equity + epsilon:\n            results.append(1)\n        elif s_crypto < s_equity - epsilon:\n            results.append(-1)\n        else:\n            results.append(0)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2389238"}, {"introduction": "To truly understand market efficiency, we must consider the behavior of the agents who constitute the market. This advanced practice guides you through building a simple Agent-Based Model (ABM) to explore how differences in forecasting ability impact trading profits [@problem_id:2389247]. By equipping agents with different volatility models—GARCH versus a simple rolling window—and simulating their trading, you will gain a deeper appreciation for how market efficiency emerges from agent interactions and how transaction costs can determine whether superior information translates into superior profits.", "problem": "You are asked to design and execute a small-scale computational experiment that probes the Efficient Market Hypothesis (EMH) using an Agent-Based Model (ABM). The core idea is to compare the realized net profitability of two groups of agents who trade a single risky asset with a positive drift but time-varying volatility. Half of the agents use a Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model to forecast volatility, while the other half use a simpler rolling-window variance estimator. You must implement a complete, runnable program that simulates the return process, produces the agents’ forecasts, computes positions, applies transaction costs, and compares cumulative profits.\n\nFundamental base:\n- Efficient Market Hypothesis (EMH): Prices fully reflect available information, implying that conditional expected excess returns are not systematically forecastable. However, volatility may be forecastable in practice and can affect risk management and transaction costs.\n- Conditional return model: The asset one-period return is modeled as $r_t = \\mu + \\sigma_t \\epsilon_t$, where $\\epsilon_t \\sim \\mathcal{N}(0,1)$ independently across $t$, $\\mu$ is a constant drift, and $\\sigma_t^2$ follows a conditional variance process.\n- GARCH(1,1) conditional variance: $\\sigma_t^2 = \\omega + \\alpha \\, (r_{t-1} - \\mu)^2 + \\beta \\, \\sigma_{t-1}^2$ with $\\omega > 0$, $\\alpha \\ge 0$, $\\beta \\ge 0$, and $\\alpha + \\beta < 1$ for covariance stationarity.\n- Rolling-window variance estimator: For a window size $W$, a simple forecast of the next-period variance at time $t$ uses the sample second moment over the last $W$ demeaned returns.\n\nModeling and trading rules to implement:\n- Data generating process (DGP): Simulate $T$ returns $\\{r_t\\}_{t=0}^{T-1}$ with $r_t = \\mu + \\sigma_t \\epsilon_t$, $\\epsilon_t \\sim \\mathcal{N}(0,1)$, and $\\sigma_t^2$ evolving as above. Initialize $\\sigma_0^2$ at the unconditional variance $\\omega/(1-\\alpha-\\beta)$ if $\\alpha + \\beta < 1$, else at $\\omega$.\n- Volatility forecasting:\n  - GARCH group (half of the agents): At each time $t$, form a one-step-ahead variance forecast $\\widehat{\\sigma}_{t+1,G}^2 = \\omega + \\alpha \\, (r_t - \\mu)^2 + \\beta \\, \\widehat{\\sigma}_{t,G}^2$, with $\\widehat{\\sigma}_{0,G}^2$ initialized to the same unconditional variance used for $\\sigma_0^2$ when $\\alpha + \\beta < 1$, else $\\omega$.\n  - Simple group (half of the agents): At each time $t \\ge W-1$, form a one-step-ahead variance forecast $\\widehat{\\sigma}_{t+1,S}^2$ as the rolling-window average of squared demeaned returns over the last $W$ observations up to time $t$, i.e., $\\widehat{\\sigma}_{t+1,S}^2 = \\frac{1}{W}\\sum_{j=0}^{W-1} (r_{t-j} - \\mu)^2$.\n- Position sizing and profit:\n  - Let the risk target be $v_{\\text{target}} > 0$. For group $g \\in \\{G,S\\}$, set the next-period position at time $t$ as $x_{t+1}^{(g)} = v_{\\text{target}} / \\widehat{\\sigma}_{t+1,g}$, using the positive drift to determine a long-only exposure. Initialize $x_0^{(g)} = 0$.\n  - Transaction costs: Moving from $x_{t}^{(g)}$ to $x_{t+1}^{(g)}$ incurs a linear cost $c \\, |x_{t+1}^{(g)} - x_{t}^{(g)}|$ with $c \\ge 0$.\n  - Per-period profit for period $t+1$ is $\\pi_{t+1}^{(g)} = x_{t+1}^{(g)} \\, r_{t+1} - c \\, |x_{t+1}^{(g)} - x_{t}^{(g)}|$.\n  - Cumulative profit for group $g$ over the trading horizon is the sum of $\\pi_{t+1}^{(g)}$ for periods where both forecasts exist. To ensure comparability, begin trading at $t = W-1$ and accumulate through $t = T-2$ so that $r_{t+1}$ is available.\n\nAgent groups:\n- There are $N$ agents with $N/2$ using the GARCH forecast and $N/2$ using the rolling-window forecast. Because all agents within a group use the same rule, the average profit per agent in a group equals the per-agent profit computed by the rules above. You should report the difference in average per-agent cumulative profit between groups, defined as $\\Pi_{\\text{diff}} = \\Pi_G - \\Pi_S$.\n\nYour task:\n- Implement the simulation and trading logic exactly as specified.\n- For reproducibility, use the provided random seeds.\n- For each test case, output the floating-point value of $\\Pi_{\\text{diff}}$.\n\nTest suite:\nFor each case, parameters are given as $(T,\\mu,\\omega,\\alpha,\\beta,W,c,v_{\\text{target}},\\text{seed})$.\n\n- Case A (GARCH world, moderate costs, typical daily persistence):\n  - $(4000, 0.0002, 0.000005, 0.05, 0.94, 30, 0.0001, 0.02, 202311)$\n- Case B (same GARCH world, high costs, very smooth simple forecast):\n  - $(4000, 0.0002, 0.000005, 0.05, 0.94, 250, 0.005, 0.02, 202311)$\n- Case C (homoskedastic world, small costs):\n  - $(4000, 0.0002, 0.0001, 0, 0, 60, 0.0001, 0.02, 202312)$\n- Case D (short horizon, more reactive GARCH, simple very noisy):\n  - $(500, 0.0002, 0.000005, 0.08, 0.90, 5, 0.0002, 0.02, 202313)$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases above. Each entry must be the floating-point value of $\\Pi_{\\text{diff}}$ rounded to $6$ decimal places. For example, a valid output format is `[val_A,val_B,val_C,val_D]` with no spaces and with each value rounded to 6 decimals.\n- The program must be self-contained and must not read any input.", "solution": "The problem statement is subjected to rigorous validation.\n\nStep 1: Extract Givens\n- **Return Model**: The asset return at time $t$ is $r_t = \\mu + \\sigma_t \\epsilon_t$, where $\\epsilon_t \\sim \\mathcal{N}(0,1)$ are independent and identically distributed standard normal random variables. $\\mu$ is a constant drift.\n- **True Volatility Process (DGP)**: The conditional variance $\\sigma_t^2$ follows a GARCH($1,1$) process: $\\sigma_t^2 = \\omega + \\alpha \\, (r_{t-1} - \\mu)^2 + \\beta \\, \\sigma_{t-1}^2$.\n- **DGP Parameters**: $\\omega > 0$, $\\alpha \\ge 0$, $\\beta \\ge 0$, and $\\alpha + \\beta < 1$ for stationarity.\n- **DGP Initialization**: The simulation generates $T$ returns, $\\{r_t\\}_{t=0}^{T-1}$. The initial variance $\\sigma_0^2$ is set to the unconditional variance, $\\sigma_{uncond}^2 = \\omega / (1 - \\alpha - \\beta)$, if $\\alpha + \\beta < 1$, and to $\\omega$ otherwise.\n- **Agent Groups**: Two groups of agents, a GARCH group ($G$) and a Simple group ($S$).\n- **GARCH Group Forecast**: The one-step-ahead variance forecast is $\\widehat{\\sigma}_{t+1,G}^2 = \\omega + \\alpha \\, (r_t - \\mu)^2 + \\beta \\, \\widehat{\\sigma}_{t,G}^2$. The initial forecast $\\widehat{\\sigma}_{0,G}^2$ is set equal to the initial true variance $\\sigma_0^2$.\n- **Simple Group Forecast**: For a window of size $W$, the one-step-ahead variance forecast at time $t \\ge W-1$ is $\\widehat{\\sigma}_{t+1,S}^2 = \\frac{1}{W}\\sum_{j=0}^{W-1} (r_{t-j} - \\mu)^2$.\n- **Position Sizing**: For group $g \\in \\{G,S\\}$, the position for period $t+1$ is $x_{t+1}^{(g)} = v_{\\text{target}} / \\widehat{\\sigma}_{t+1,g}$, where $v_{\\text{target}} > 0$ is the risk target and $\\widehat{\\sigma}_{t+1,g}$ is the forecasted standard deviation (square root of the variance forecast). Initial position is $x_0^{(g)} = 0$.\n- **Transaction Costs**: A linear cost of $c \\, |x_{t+1}^{(g)} - x_{t}^{(g)}|$ is incurred when adjusting the position, where $c \\ge 0$.\n- **Profit Calculation**: The profit for period $t+1$ is $\\pi_{t+1}^{(g)} = x_{t+1}^{(g)} \\, r_{t+1} - c \\, |x_{t+1}^{(g)} - x_{t}^{(g)}|$.\n- **Trading Horizon**: Profit accumulation begins at $t=W-1$ (for profit period $W$) and ends at $t=T-2$ (for profit period $T-1$).\n- **Performance Metric**: The final metric is the difference in cumulative profits between the two groups, $\\Pi_{\\text{diff}} = \\sum \\pi^{(G)} - \\sum \\pi^{(S)} = \\Pi_G - \\Pi_S$.\n- **Test Cases**: Four cases are specified with parameters $(T, \\mu, \\omega, \\alpha, \\beta, W, c, v_{\\text{target}}, \\text{seed})$.\n\nStep 2: Validate Using Extracted Givens\nThe problem is assessed against the required criteria.\n- **Scientifically Grounded**: The problem is based on standard, well-established models from financial econometrics, namely the GARCH model for volatility clustering and a basic asset return model. The experimental design to test aspects of the Efficient Market Hypothesis is a standard methodology in computational finance. All components are scientifically sound.\n- **Well-Posed**: The problem is mathematically and algorithmically well-posed. The data generating process, forecasting rules, trading logic, and cost structure are all specified with precise equations and parameters. The use of a fixed random seed ensures a unique and reproducible outcome for each simulation.\n- **Objective**: The problem is stated in objective, formal language, free of subjective claims or ambiguity.\n\nThe problem does not exhibit any of the invalidity flaws. It is scientifically sound, formalizable, and internally consistent. The parameters are defined, and the computational procedure is clear. The scenario is a simplified but valid representation of a quantitative trading simulation.\n\nStep 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be constructed as specified.\n\nThe solution proceeds by first implementing the data generating process (DGP) for the asset returns. A time series of $T$ returns $\\{r_t\\}_{t=0}^{T-1}$ is simulated according to the GARCH($1,1$) process. The initial variance $\\sigma_0^2$ and the GARCH agents' initial variance forecast $\\widehat{\\sigma}_{0,G}^2$ are set to the unconditional variance of the process, $\\sigma_{uncond}^2 = \\omega / (1 - \\alpha - \\beta)$, provided the stationarity condition $\\alpha + \\beta < 1$ holds.\n\nNext, the trading simulation is executed over the time series. Both groups of agents make one-step-ahead forecasts of the conditional volatility.\n1.  The GARCH ($G$) agents update their forecast recursively at each time step $t$, from $t=0$ to $t=T-2$: $\\widehat{\\sigma}_{t+1,G}^2 = \\omega + \\alpha (r_t - \\mu)^2 + \\beta \\widehat{\\sigma}_{t,G}^2$.\n2.  The Simple ($S$) agents compute a rolling-window estimate only when a full window of $W$ past returns is available, i.e., for $t \\ge W-1$. Their forecast is $\\widehat{\\sigma}_{t+1,S}^2 = \\frac{1}{W}\\sum_{j=0}^{W-1} (r_{t-j} - \\mu)^2$.\n\nTo ensure a fair comparison, trading for both groups commences at the same time. Positions are held at zero until time $t=W-1$, at which point the first non-zero positions $x_W^{(g)}$ can be determined for both groups. From $t=W-1$ up to $t=T-2$, both groups update their positions for the next period, $t+1$, based on their respective volatility forecasts. The position for group $g$ is set according to the volatility targeting rule: $x_{t+1}^{(g)} = v_{\\text{target}} / \\widehat{\\sigma}_{t+1,g}$, where $\\widehat{\\sigma}_{t+1,g}$ is the square root of the forecasted variance $\\widehat{\\sigma}_{t+1,g}^2$.\n\nFor each period from $t+1=W$ to $t+1=T-1$, the net profit $\\pi_{t+1}^{(g)}$ is calculated. This is the gross trading profit, $x_{t+1}^{(g)} r_{t+1}$, minus the transaction cost, $c |x_{t+1}^{(g)} - x_t^{(g)}|$. These per-period profits are summed to obtain the total cumulative profit, $\\Pi_G$ and $\\Pi_S$, for each group.\n\nFinally, the difference in cumulative profits, $\\Pi_{\\text{diff}} = \\Pi_G - \\Pi_S$, is computed. This entire procedure is encapsulated within a function that is executed for each set of parameters provided in the test suite, using the specified random seeds for reproducibility.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case A: (T, mu, omega, alpha, beta, W, c, v_target, seed)\n        (4000, 0.0002, 0.000005, 0.05, 0.94, 30, 0.0001, 0.02, 202311),\n        # Case B: (T, mu, omega, alpha, beta, W, c, v_target, seed)\n        (4000, 0.0002, 0.000005, 0.05, 0.94, 250, 0.005, 0.02, 202311),\n        # Case C: (T, mu, omega, alpha, beta, W, c, v_target, seed)\n        (4000, 0.0002, 0.0001, 0, 0, 60, 0.0001, 0.02, 202312),\n        # Case D: (T, mu, omega, alpha, beta, W, c, v_target, seed)\n        (500, 0.0002, 0.000005, 0.08, 0.90, 5, 0.0002, 0.02, 202313),\n    ]\n\n    results = []\n    for case in test_cases:\n        pi_diff = run_simulation(*case)\n        results.append(pi_diff)\n\n    # Format the final output as specified\n    formatted_results = ','.join([f'{res:.6f}' for res in results])\n    print(f\"[{formatted_results}]\")\n\ndef run_simulation(T, mu, omega, alpha, beta, W, c, v_target, seed):\n    \"\"\"\n    Executes a single simulation run for a given set of parameters.\n\n    Args:\n        T (int): Number of time steps.\n        mu (float): Constant drift of the asset return.\n        omega (float): Constant term in the GARCH process.\n        alpha (float): ARCH parameter.\n        beta (float): GARCH parameter.\n        W (int): Window size for the simple volatility estimator.\n        c (float): Transaction cost coefficient.\n        v_target (float): Target volatility for position sizing.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        float: The difference in cumulative profits (Pi_G - Pi_S).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # 1. Data Generating Process (DGP)\n    eps = rng.normal(loc=0.0, scale=1.0, size=T)\n    sigma2 = np.zeros(T)\n    r = np.zeros(T)\n\n    if alpha + beta < 1:\n        uncond_var = omega / (1 - alpha - beta)\n    else:\n        uncond_var = omega\n    \n    sigma2[0] = uncond_var\n    r[0] = mu + np.sqrt(sigma2[0]) * eps[0]\n\n    for t in range(1, T):\n        sigma2[t] = omega + alpha * (r[t-1] - mu)**2 + beta * sigma2[t-1]\n        r[t] = mu + np.sqrt(sigma2[t]) * eps[t]\n\n    # 2. Trading Simulation\n    # Initialize position arrays and cumulative profits\n    x_G = np.zeros(T)\n    x_S = np.zeros(T)\n    Pi_G, Pi_S = 0.0, 0.0\n\n    # Initialize GARCH forecast\n    sigma2_hat_G = np.zeros(T)\n    sigma2_hat_G[0] = uncond_var\n\n    # Main simulation loop for forecasting and trading\n    for t in range(T - 1):\n        # GARCH agent forecasts for period t+1\n        sigma2_hat_G[t+1] = omega + alpha * (r[t] - mu)**2 + beta * sigma2_hat_G[t]\n\n        # Trading starts at t = W-1 to ensure comparability\n        if t >= W - 1:\n            # Simple agent forecasts for period t+1\n            demeaned_sq_returns = (r[t-W+1 : t+1] - mu)**2\n            sigma2_hat_S = np.mean(demeaned_sq_returns)\n\n            # Position sizing for period t+1\n            # Denominator must be non-zero; GARCH variance forecast is positive by construction,\n            # simple forecast is non-negative, but can be zero in extreme cases.\n            # Add a small epsilon for stability, though unlikely needed with float precision.\n            epsilon = 1e-12\n            sigma_hat_G = np.sqrt(sigma2_hat_G[t+1])\n            sigma_hat_S = np.sqrt(sigma2_hat_S + epsilon)\n\n            x_G[t+1] = v_target / sigma_hat_G if sigma_hat_G > 0 else 0\n            x_S[t+1] = v_target / sigma_hat_S if sigma_hat_S > 0 else 0\n            \n            # Profit calculation for period t+1\n            profit_G = x_G[t+1] * r[t+1] - c * np.abs(x_G[t+1] - x_G[t])\n            profit_S = x_S[t+1] * r[t+1] - c * np.abs(x_S[t+1] - x_S[t])\n            \n            Pi_G += profit_G\n            Pi_S += profit_S\n\n    return Pi_G - Pi_S\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2389247"}]}