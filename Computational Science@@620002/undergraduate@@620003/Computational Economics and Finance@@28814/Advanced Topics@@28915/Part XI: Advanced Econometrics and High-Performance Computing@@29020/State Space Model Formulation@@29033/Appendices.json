{"hands_on_practices": [{"introduction": "Many modern macroeconomic models have solutions that can be naturally expressed in the state-space form, providing a direct path to empirical analysis and forecasting. This exercise bridges the gap between theory and practice by asking you to translate the log-linearized solution of a standard Real Business Cycle (RBC) model into its state-space representation [@problem_id:2433394]. Mastering this translation is a fundamental skill for estimating and evaluating dynamic stochastic general equilibrium (DSGE) models.", "problem": "Consider a standard Real Business Cycle (RBC) model with a single exogenous technology process. Let $z_t$ denote the log-deviation of technology from its steady state, evolving as a stationary autoregression of order one. Suppose the equilibrium conditions of the RBC model have been log-linearized around the deterministic steady state and solved for the policy and measurement relations linking next-period capital and current output to the current state variables. In particular, let the law of motion for technology be given by\n$$\nz_{t+1} = \\rho\\, z_t + \\sigma\\, \\varepsilon_{t+1},\n$$\nwhere $\\varepsilon_{t}$ is independent and identically distributed with $\\varepsilon_t \\sim \\mathcal{N}(0,1)$, and the linearized policy and output relations are\n$$\nk_{t+1} = \\phi_k\\, k_t + \\phi_z\\, z_t, \\quad y_t = \\psi_k\\, k_t + \\psi_z\\, z_t,\n$$\nwith $k_t$ the log-deviation of capital from steady state and $y_t$ the log-deviation of output from steady state. All variables are assumed covariance-stationary and the parameters satisfy the stability conditions needed for a unique, stationary solution.\n\n1. Starting from these definitions, formulate the minimal linear Gaussian state-space model with state vector $x_t = \\begin{pmatrix} k_t \\\\ z_t \\end{pmatrix}$ and measurement $y_t$. Write the state transition matrix, the shock loading matrix, and the measurement matrix explicitly in terms of $\\phi_k$, $\\phi_z$, $\\rho$, $\\sigma$, $\\psi_k$, and $\\psi_z$.\n\n2. Using the state-space representation, derive an explicit expression for the unconditional variance of $y_t$ in terms of the model parameters by solving the discrete-time Lyapunov equation for the stationary state covariance matrix of $x_t$.\n\n3. Evaluate the unconditional variance of $y_t$ numerically for the parameter values $\\phi_k = 0.85$, $\\phi_z = 0.10$, $\\rho = 0.90$, $\\sigma = 0.02$, $\\psi_k = 0.30$, and $\\psi_z = 1.00$. Round your answer to four significant figures. Express your final answer as a pure number (no units).", "solution": "The problem statement is parsed and validated. It is a standard, well-posed problem in computational macroeconomics, free of scientific or logical inconsistencies. All required definitions and parameters are provided. We may proceed directly to the solution.\n\nThe problem requires a three-part solution: first, the formulation of a linear state-space model; second, the derivation of the unconditional variance of the measurement variable; and third, the numerical evaluation of this variance for a given set of parameters.\n\nPart 1: State-Space Model Formulation\n\nA linear Gaussian state-space model is represented by a state transition equation and a measurement equation.\nThe state transition equation has the general form $x_{t+1} = A x_t + B w_{t+1}$, where $x_t$ is the state vector, $A$ is the state transition matrix, $w_{t+1}$ is a vector of i.i.d. shocks with zero mean and identity covariance matrix, and $B$ is the shock loading matrix.\nThe measurement equation has the general form $y_t = C x_t + D w_t$, where $y_t$ is the vector of observed variables and $C$ is the measurement matrix. In this problem, there is no contemporaneous shock in the measurement equation, so $D$ is a zero matrix.\n\nThe state vector is given as $x_t = \\begin{pmatrix} k_t \\\\ z_t \\end{pmatrix}$. The shock is a scalar, $w_{t+1} = \\varepsilon_{t+1}$, where $\\varepsilon_{t+1} \\sim \\mathcal{N}(0,1)$.\n\nWe construct the state transition equation by assembling the given laws of motion for the state variables $k_t$ and $z_t$:\nThe law of motion for capital, $k_{t+1}$, is given as $k_{t+1} = \\phi_k k_t + \\phi_z z_t$.\nThe law of motion for technology, $z_{t+1}$, is given as $z_{t+1} = \\rho z_t + \\sigma \\varepsilon_{t+1}$.\n\nWe can write these two equations in matrix form as:\n$$\n\\begin{pmatrix} k_{t+1} \\\\ z_{t+1} \\end{pmatrix} = \\begin{pmatrix} \\phi_k  \\phi_z \\\\ 0  \\rho \\end{pmatrix} \\begin{pmatrix} k_t \\\\ z_t \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ \\sigma \\end{pmatrix} \\varepsilon_{t+1}\n$$\nFrom this, we identify the state transition matrix $A$ and the shock loading matrix $B$:\n$$\nA = \\begin{pmatrix} \\phi_k  \\phi_z \\\\ 0  \\rho \\end{pmatrix}, \\quad B = \\begin{pmatrix} 0 \\\\ \\sigma \\end{pmatrix}\n$$\nThe measurement equation is given by the expression for output, $y_t = \\psi_k k_t + \\psi_z z_t$. In matrix form, this is:\n$$\ny_t = \\begin{pmatrix} \\psi_k  \\psi_z \\end{pmatrix} \\begin{pmatrix} k_t \\\\ z_t \\end{pmatrix}\n$$\nFrom this, we identify the measurement matrix $C$:\n$$\nC = \\begin{pmatrix} \\psi_k  \\psi_z \\end{pmatrix}\n$$\nThis completes the formulation of the minimal linear Gaussian state-space model.\n\nPart 2: Derivation of the Unconditional Variance of $y_t$\n\nThe unconditional variance of $y_t$ is derived from the unconditional covariance matrix of the state vector, $\\Gamma_x = \\mathbb{E}[x_t x_t']$. Since the variables are log-deviations from a steady state, their unconditional mean is zero, and $\\Gamma_x$ is the covariance matrix. The model is assumed to be stationary, so $\\Gamma_x$ is constant over time.\n\nThe stationary state covariance matrix $\\Gamma_x$ is the solution to the discrete-time Lyapunov equation:\n$$\n\\Gamma_x = A \\Gamma_x A' + B Q B'\n$$\nwhere $Q = \\mathbb{E}[w_{t+1} w_{t+1}'] = \\mathbb{E}[\\varepsilon_{t+1}^2] = 1$. The equation simplifies to $\\Gamma_x = A \\Gamma_x A' + B B'$.\n\nLet $\\Gamma_x = \\begin{pmatrix} \\gamma_{kk}  \\gamma_{kz} \\\\ \\gamma_{zk}  \\gamma_{zz} \\end{pmatrix}$, where $\\gamma_{ij} = \\text{Cov}(i_t, j_t)$. By symmetry, $\\gamma_{kz} = \\gamma_{zk}$. We have:\n$$\nB B' = \\begin{pmatrix} 0 \\\\ \\sigma \\end{pmatrix} \\begin{pmatrix} 0  \\sigma \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  \\sigma^2 \\end{pmatrix}\n$$\n$$\nA \\Gamma_x A' = \\begin{pmatrix} \\phi_k  \\phi_z \\\\ 0  \\rho \\end{pmatrix} \\begin{pmatrix} \\gamma_{kk}  \\gamma_{kz} \\\\ \\gamma_{kz}  \\gamma_{zz} \\end{pmatrix} \\begin{pmatrix} \\phi_k  0 \\\\ \\phi_z  \\rho \\end{pmatrix} = \\begin{pmatrix} \\phi_k^2\\gamma_{kk} + 2\\phi_k\\phi_z\\gamma_{kz} + \\phi_z^2\\gamma_{zz}  \\rho(\\phi_k\\gamma_{kz} + \\phi_z\\gamma_{zz}) \\\\ \\rho(\\phi_k\\gamma_{kz} + \\phi_z\\gamma_{zz})  \\rho^2\\gamma_{zz} \\end{pmatrix}\n$$\nSubstituting these into the Lyapunov equation gives a system of three equations for the three unknown elements of $\\Gamma_x$:\n1.  From the $(2,2)$ element: $\\gamma_{zz} = \\rho^2\\gamma_{zz} + \\sigma^2 \\implies \\gamma_{zz}(1-\\rho^2) = \\sigma^2$. This gives:\n    $$\n    \\gamma_{zz} = \\frac{\\sigma^2}{1-\\rho^2}\n    $$\n    This is the well-known variance of a stationary AR(1) process.\n2.  From the $(1,2)$ element: $\\gamma_{kz} = \\rho(\\phi_k\\gamma_{kz} + \\phi_z\\gamma_{zz}) \\implies \\gamma_{kz}(1-\\rho\\phi_k) = \\rho\\phi_z\\gamma_{zz}$. This gives:\n    $$\n    \\gamma_{kz} = \\frac{\\rho\\phi_z}{1-\\rho\\phi_k}\\gamma_{zz} = \\frac{\\rho\\phi_z\\sigma^2}{(1-\\rho\\phi_k)(1-\\rho^2)}\n    $$\n3.  From the $(1,1)$ element: $\\gamma_{kk} = \\phi_k^2\\gamma_{kk} + 2\\phi_k\\phi_z\\gamma_{kz} + \\phi_z^2\\gamma_{zz} \\implies \\gamma_{kk}(1-\\phi_k^2) = 2\\phi_k\\phi_z\\gamma_{kz} + \\phi_z^2\\gamma_{zz}$. This gives:\n    $$\n    \\gamma_{kk} = \\frac{2\\phi_k\\phi_z\\gamma_{kz} + \\phi_z^2\\gamma_{zz}}{1-\\phi_k^2}\n    $$\nThe unconditional variance of $y_t$ is given by $\\text{Var}(y_t) = \\mathbb{E}[y_t^2] = \\mathbb{E}[(C x_t)(C x_t)'] = C \\mathbb{E}[x_t x_t'] C' = C \\Gamma_x C'$.\n$$\n\\text{Var}(y_t) = \\begin{pmatrix} \\psi_k  \\psi_z \\end{pmatrix} \\begin{pmatrix} \\gamma_{kk}  \\gamma_{kz} \\\\ \\gamma_{kz}  \\gamma_{zz} \\end{pmatrix} \\begin{pmatrix} \\psi_k \\\\ \\psi_z \\end{pmatrix} = \\psi_k^2\\gamma_{kk} + 2\\psi_k\\psi_z\\gamma_{kz} + \\psi_z^2\\gamma_{zz}\n$$\nThis is the explicit expression for the unconditional variance of $y_t$ in terms of the model parameters and the elements of the state covariance matrix, which have themselves been expressed in terms of the fundamental model parameters.\n\nPart 3: Numerical Evaluation\n\nWe are given the parameter values: $\\phi_k = 0.85$, $\\phi_z = 0.10$, $\\rho = 0.90$, $\\sigma = 0.02$, $\\psi_k = 0.30$, and $\\psi_z = 1.00$. We proceed to calculate the components of $\\Gamma_x$ and then $\\text{Var}(y_t)$.\n\n1.  Calculate $\\gamma_{zz}$:\n    $$\n    \\gamma_{zz} = \\frac{(0.02)^2}{1 - (0.90)^2} = \\frac{0.0004}{1 - 0.81} = \\frac{0.0004}{0.19} \\approx 0.00210526\n    $$\n2.  Calculate $\\gamma_{kz}$:\n    $$\n    \\gamma_{kz} = \\frac{(0.90)(0.10)}{1 - (0.90)(0.85)} \\gamma_{zz} = \\frac{0.09}{1 - 0.765} \\gamma_{zz} = \\frac{0.09}{0.235} \\gamma_{zz} \\approx 0.3829787 \\times 0.00210526 \\approx 0.00080608\n    $$\n3.  Calculate $\\gamma_{kk}$:\n    $$\n    \\gamma_{kk} = \\frac{2(0.85)(0.10)\\gamma_{kz} + (0.10)^2\\gamma_{zz}}{1 - (0.85)^2} = \\frac{0.17\\gamma_{kz} + 0.01\\gamma_{zz}}{1 - 0.7225} = \\frac{0.17\\gamma_{kz} + 0.01\\gamma_{zz}}{0.2775}\n    $$\n    Substituting the intermediate values:\n    $$\n    \\gamma_{kk} \\approx \\frac{0.17(0.00080608) + 0.01(0.00210526)}{0.2775} = \\frac{0.00013703 + 0.00002105}{0.2775} = \\frac{0.00015808}{0.2775} \\approx 0.00056968\n    $$\n4.  Calculate $\\text{Var}(y_t)$:\n    $$\n    \\text{Var}(y_t) = \\psi_k^2\\gamma_{kk} + 2\\psi_k\\psi_z\\gamma_{kz} + \\psi_z^2\\gamma_{zz}\n    $$\n    $$\n    = (0.30)^2\\gamma_{kk} + 2(0.30)(1.00)\\gamma_{kz} + (1.00)^2\\gamma_{zz}\n    $$\n    $$\n    = 0.09\\gamma_{kk} + 0.6\\gamma_{kz} + \\gamma_{zz}\n    $$\n    Substituting the calculated variance and covariance terms:\n    $$\n    \\text{Var}(y_t) \\approx 0.09(0.00056968) + 0.6(0.00080608) + 0.00210526\n    $$\n    $$\n    \\approx 0.00005127 + 0.00048365 + 0.00210526 = 0.00264018\n    $$\nRounding the final result to four significant figures yields $0.002640$.", "answer": "$$\n\\boxed{0.002640}\n$$", "id": "2433394"}, {"introduction": "State-space models are also exceptionally versatile for representing complex time-series dynamics, such as the long-run equilibrium relationships described by cointegration. This problem requires you to cast a Vector Error Correction Model (VECM) for cointegrated financial assets into the state-space framework [@problem_id:2433369]. By defining the stationary cointegrating relationship as a state variable, you will gain insight into how these models elegantly handle non-stationary data.", "problem": "Consider two logarithmic stock prices $p_{1,t}$ and $p_{2,t}$ that are integrated of order one and cointegrated with cointegrating vector $\\beta = \\begin{pmatrix}1 \\\\ -1\\end{pmatrix}$. Let $p_{t} = \\begin{pmatrix}p_{1,t} \\\\ p_{2,t}\\end{pmatrix}$ and suppose their joint dynamics follow a Vector Error Correction Model (VECM), specifically\n$$\n\\Delta p_{t} = \\alpha\\, \\beta^{\\prime} p_{t-1} + \\varepsilon_{t},\n$$\nwhere $\\alpha = \\begin{pmatrix} a \\\\ b \\end{pmatrix}$ with $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$, and $\\varepsilon_{t} = \\begin{pmatrix}\\varepsilon_{1,t} \\\\ \\varepsilon_{2,t}\\end{pmatrix}$ is a zero-mean serially uncorrelated innovation process with finite second moments. Define the cointegrating relationship $c_{t} = \\beta^{\\prime} p_{t} = p_{1,t} - p_{2,t}$.\n\nConstruct a linear time-invariant state-space representation whose state vector is $x_{t} = \\begin{pmatrix} p_{1,t} \\\\ p_{2,t} \\\\ c_{t} \\end{pmatrix}$, with an observation equation that returns the vector of prices and no measurement noise. Specifically, write the state transition matrix $F$, the input (shock loading) matrix $G$ mapping $\\varepsilon_{t}$ into the state, and the observation matrix $H$ such that\n$$\nx_{t} = F x_{t-1} + G \\varepsilon_{t}, \\quad y_{t} = H x_{t}, \\quad y_{t} = p_{t}.\n$$\n\nThen, compute the determinant of the state transition matrix $F$ as a function of $a$ and $b$. Provide your final answer as a single closed-form analytic expression. Do not round.", "solution": "The problem requires the construction of a linear time-invariant state-space representation for a given Vector Error Correction Model (VECM) and the subsequent calculation of the determinant of the state transition matrix.\n\nFirst, we must validate the problem statement.\nGivens:\n1.  Logarithmic stock prices $p_{1,t}$ and $p_{2,t}$, forming a vector $p_{t} = \\begin{pmatrix}p_{1,t} \\\\ p_{2,t}\\end{pmatrix}$.\n2.  The prices are integrated of order one, I($1$), and cointegrated.\n3.  The cointegrating vector is $\\beta = \\begin{pmatrix}1 \\\\ -1\\end{pmatrix}$.\n4.  The dynamics are described by the VECM: $\\Delta p_{t} = \\alpha\\, \\beta^{\\prime} p_{t-1} + \\varepsilon_{t}$, where $\\Delta p_t = p_t - p_{t-1}$.\n5.  The adjustment vector is $\\alpha = \\begin{pmatrix} a \\\\ b \\end{pmatrix}$ for $a, b \\in \\mathbb{R}$.\n6.  $\\varepsilon_{t} = \\begin{pmatrix}\\varepsilon_{1,t} \\\\ \\varepsilon_{2,t}\\end{pmatrix}$ is a zero-mean, serially uncorrelated innovation process.\n7.  The cointegrating relationship is $c_{t} = \\beta^{\\prime} p_{t} = p_{1,t} - p_{2,t}$.\n8.  The state vector is defined as $x_{t} = \\begin{pmatrix} p_{1,t} \\\\ p_{2,t} \\\\ c_{t} \\end{pmatrix}$.\n9.  The state-space model has the form $x_{t} = F x_{t-1} + G \\varepsilon_{t}$ and $y_{t} = H x_{t}$.\n10. The output is $y_{t} = p_{t}$ with no measurement noise.\n\nThe problem is scientifically grounded, well-posed, and objective. It provides a complete and consistent setup based on standard econometric models (VECM) and their representation in state-space form. The definition of the state vector includes a redundant state ($c_t$ is a linear combination of $p_{1,t}$ and $p_{2,t}$), but this is a valid modeling choice and does not introduce a contradiction. Thus, the problem is valid and a solution can be derived.\n\nThe task is to find the matrices $F$, $G$, $H$, and then compute $\\det(F)$.\n\nWe begin by expressing the dynamics of each component of the state vector $x_t$ in terms of the components of $x_{t-1}$ and the innovation $\\varepsilon_t$. The VECM equation is the starting point:\n$$ p_t - p_{t-1} = \\alpha \\beta' p_{t-1} + \\varepsilon_t $$\nThe term $\\beta' p_{t-1}$ is precisely the lagged cointegrating relationship $c_{t-1}$:\n$$ \\beta' p_{t-1} = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} p_{1,t-1} \\\\ p_{2,t-1} \\end{pmatrix} = p_{1,t-1} - p_{2,t-1} = c_{t-1} $$\nSubstituting this into the VECM gives:\n$$ p_t - p_{t-1} = \\alpha c_{t-1} + \\varepsilon_t $$\n$$ p_t = p_{t-1} + \\alpha c_{t-1} + \\varepsilon_t $$\nLet us write this in terms of the individual price components:\n$$ \\begin{pmatrix} p_{1,t} \\\\ p_{2,t} \\end{pmatrix} = \\begin{pmatrix} p_{1,t-1} \\\\ p_{2,t-1} \\end{pmatrix} + \\begin{pmatrix} a \\\\ b \\end{pmatrix} c_{t-1} + \\begin{pmatrix} \\varepsilon_{1,t} \\\\ \\varepsilon_{2,t} \\end{pmatrix} $$\nThis yields the first two scalar equations for our state-space model:\n$$ p_{1,t} = p_{1,t-1} + a c_{t-1} + \\varepsilon_{1,t} $$\n$$ p_{2,t} = p_{2,t-1} + b c_{t-1} + \\varepsilon_{2,t} $$\n\nNext, we derive the dynamic equation for the third state variable, $c_t$. By definition, $c_t = p_{1,t} - p_{2,t}$. We substitute the expressions for $p_{1,t}$ and $p_{2,t}$ we just found:\n$$ c_t = (p_{1,t-1} + a c_{t-1} + \\varepsilon_{1,t}) - (p_{2,t-1} + b c_{t-1} + \\varepsilon_{2,t}) $$\nGrouping terms by lagged variables and innovations:\n$$ c_t = (p_{1,t-1} - p_{2,t-1}) + (a - b)c_{t-1} + (\\varepsilon_{1,t} - \\varepsilon_{2,t}) $$\nRecognizing that $p_{1,t-1} - p_{2,t-1} = c_{t-1}$, we simplify the expression:\n$$ c_t = c_{t-1} + (a-b)c_{t-1} + (\\varepsilon_{1,t} - \\varepsilon_{2,t}) $$\n$$ c_t = (1 + a - b)c_{t-1} + \\varepsilon_{1,t} - \\varepsilon_{2,t} $$\n\nWe now have the three required dynamic equations. We can assemble them into the state transition equation $x_t = F x_{t-1} + G \\varepsilon_t$:\n$$\n\\begin{pmatrix} p_{1,t} \\\\ p_{2,t} \\\\ c_t \\end{pmatrix}\n=\n\\begin{pmatrix}\n1 \\cdot p_{1,t-1} + 0 \\cdot p_{2,t-1} + a \\cdot c_{t-1} \\\\\n0 \\cdot p_{1,t-1} + 1 \\cdot p_{2,t-1} + b \\cdot c_{t-1} \\\\\n0 \\cdot p_{1,t-1} + 0 \\cdot p_{2,t-1} + (1+a-b) \\cdot c_{t-1}\n\\end{pmatrix}\n+\n\\begin{pmatrix}\n1 \\cdot \\varepsilon_{1,t} + 0 \\cdot \\varepsilon_{2,t} \\\\\n0 \\cdot \\varepsilon_{1,t} + 1 \\cdot \\varepsilon_{2,t} \\\\\n1 \\cdot \\varepsilon_{1,t} - 1 \\cdot \\varepsilon_{2,t}\n\\end{pmatrix}\n$$\nThis can be written in matrix form as:\n$$\n\\begin{pmatrix} p_{1,t} \\\\ p_{2,t} \\\\ c_t \\end{pmatrix} =\n\\begin{pmatrix} 1  0  a \\\\ 0  1  b \\\\ 0  0  1+a-b \\end{pmatrix}\n\\begin{pmatrix} p_{1,t-1} \\\\ p_{2,t-1} \\\\ c_{t-1} \\end{pmatrix} +\n\\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  -1 \\end{pmatrix}\n\\begin{pmatrix} \\varepsilon_{1,t} \\\\ \\varepsilon_{2,t} \\end{pmatrix}\n$$\nFrom this, we identify the state transition matrix $F$ and the input matrix $G$:\n$$ F = \\begin{pmatrix} 1  0  a \\\\ 0  1  b \\\\ 0  0  1+a-b \\end{pmatrix}, \\quad G = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  -1 \\end{pmatrix} $$\nFor the observation equation $y_t = H x_t$, we have $y_t = p_t = \\begin{pmatrix} p_{1,t} \\\\ p_{2,t} \\end{pmatrix}$. We require a matrix $H$ such that:\n$$ \\begin{pmatrix} p_{1,t} \\\\ p_{2,t} \\end{pmatrix} = H \\begin{pmatrix} p_{1,t} \\\\ p_{2,t} \\\\ c_t \\end{pmatrix} $$\nThe matrix that selects the first two components of the state vector is:\n$$ H = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix} $$\nThe problem asks for the determinant of the state transition matrix $F$. The matrix $F$ is an upper triangular matrix. The determinant of any triangular matrix is the product of its diagonal elements.\n$$ \\det(F) = (1) \\cdot (1) \\cdot (1 + a - b) $$\n$$ \\det(F) = 1 + a - b $$\nThis is the required closed-form analytic expression for the determinant of $F$ as a function of $a$ and $b$.", "answer": "$$\\boxed{1 + a - b}$$", "id": "2433369"}, {"introduction": "Real-world economic and financial data often come in discrete or categorical forms, which requires extending the standard linear-Gaussian state-space model. This practice introduces a model where an unobserved, continuous latent credit score is observed as a discrete credit rating, a common scenario in finance [@problem_id:2433415]. You will implement a filtering algorithm for this non-linear observation model, developing skills to tackle more complex and realistic estimation problems.", "problem": "You are asked to formulate and implement a state space model for credit ratings in which the latent state is a continuous latent credit score and the observation is a discrete letter grade. The state equation is defined on a discrete time index $t \\in \\{1,2,\\dots,T\\}$ by a linear Gaussian autoregression with Gaussian innovations, and the observation equation is an ordinal discretization of a noisy latent score.\n\nModel specification:\n- State dynamics: the latent credit score $x_t \\in \\mathbb{R}$ evolves according to\n$$\nx_t = \\phi \\, x_{t-1} + \\sigma_w \\, w_t,\n$$\nwhere $w_t \\sim \\mathcal{N}(0,1)$ are independent and identically distributed across $t$, $\\phi \\in \\mathbb{R}$ is known, and $\\sigma_w \\in \\mathbb{R}_{+}$ is known.\n- Observation: define a noisy latent score $z_t = x_t + \\sigma_v \\, v_t$, where $v_t \\sim \\mathcal{N}(0,1)$ are independent and identically distributed across $t$, and $\\sigma_v \\in \\mathbb{R}_{+}$ is known. You observe a discrete rating $y_t$ that indicates the interval of $z_t$ relative to fixed thresholds $\\{\\tau_j\\}_{j=0}^{J}$ where $\\tau_0 = -\\infty$ and $\\tau_J = +\\infty$. Ratings are the set $\\{\\text{\"BB\"}, \\text{\"BBB\"}, \\text{\"A\"}, \\text{\"AA\"}, \\text{\"AAA\"}\\}$ with the following mapping to intervals:\n    - $y_t = \\text{\"BB\"}$ if and only if $\\tau_0  z_t \\le \\tau_1$,\n    - $y_t = \\text{\"BBB\"}$ if and only if $\\tau_1  z_t \\le \\tau_2$,\n    - $y_t = \\text{\"A\"}$ if and only if $\\tau_2  z_t \\le \\tau_3$,\n    - $y_t = \\text{\"AA\"}$ if and only if $\\tau_3  z_t \\le \\tau_4$,\n    - $y_t = \\text{\"AAA\"}$ if and only if $\\tau_4  z_t \\le \\tau_5$,\n  with thresholds\n$$\n(\\tau_0,\\tau_1,\\tau_2,\\tau_3,\\tau_4,\\tau_5) = (-\\infty, -1, 0, 1, 2, +\\infty).\n$$\n- Initial condition: $x_0 \\sim \\mathcal{N}(m_0, P_0)$ with known $m_0 \\in \\mathbb{R}$ and $P_0 \\in \\mathbb{R}_{+}$.\n\nTask:\nGiven the above model, for each test case below you must compute:\n1. The log-likelihood of the observed sequence $y_{1:T}$ under the model, namely\n$$\n\\log L = \\sum_{t=1}^{T} \\log \\Pr\\left(y_t \\mid y_{1:t-1}, \\phi, \\sigma_w, \\sigma_v, \\{\\tau_j\\}, m_0, P_0\\right),\n$$\nexpressed as a real number.\n2. The filtered posterior mean $\\mathbb{E}[x_T \\mid y_{1:T}]$ and the filtered posterior variance $\\mathrm{Var}(x_T \\mid y_{1:T})$ after processing all $T$ observations.\n\nAll probabilities and expectations are with respect to the specified Gaussian state space model. No angles or physical units are involved. All numerical answers must be expressed as decimals rounded to exactly $6$ decimal places.\n\nTest suite:\nFor each case, use the specified parameters and data sequence:\n- Case A:\n    - $\\phi = 0.95$, $\\sigma_w = 0.5$, $\\sigma_v = 0.3$, $m_0 = 0.0$, $P_0 = 1.0$,\n    - $y_{1:5} = [\\text{\"A\"}, \\text{\"AA\"}, \\text{\"A\"}, \\text{\"AAA\"}, \\text{\"AA\"}]$.\n- Case B:\n    - $\\phi = 1.0$, $\\sigma_w = 0.2$, $\\sigma_v = 0.05$, $m_0 = 1.5$, $P_0 = 0.2$,\n    - $y_{1:3} = [\\text{\"AAA\"}, \\text{\"AA\"}, \\text{\"AAA\"}]$.\n- Case C:\n    - $\\phi = 0.0$, $\\sigma_w = 1.5$, $\\sigma_v = 1.0$, $m_0 = -0.5$, $P_0 = 2.0$,\n    - $y_{1:4} = [\\text{\"BB\"}, \\text{\"BBB\"}, \\text{\"A\"}, \\text{\"AA\"}]$.\n\nRequired final output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element of this outer list corresponds to one test case (in the order A, then B, then C) and must be a list of three decimal numbers rounded to exactly $6$ decimal places and ordered as $[\\log L, \\mathbb{E}[x_T \\mid y_{1:T}], \\mathrm{Var}(x_T \\mid y_{1:T})]$. For example, the overall format must be\n$[[\\text{case A triple}],[\\text{case B triple}],[\\text{case C triple}]]$\nwith no additional text.", "solution": "The problem statement is scientifically valid and well-posed. It details a state-space model for an ordinal time series, which is a standard problem in computational economics and statistics. The model consists of a linear Gaussian state equation and a non-linear observation mechanism derived from interval censoring of a noisy state variable. The task is to perform recursive Bayesian filtering to compute the log-likelihood of an observed sequence and the final filtered posterior mean and variance of the state.\n\nThe state equation is a first-order autoregressive process:\n$$x_t = \\phi x_{t-1} + \\sigma_w w_t, \\quad w_t \\sim \\mathcal{N}(0, 1)$$\nThe observation $y_t$ is determined by which interval the noisy latent score $z_t = x_t + \\sigma_v v_t$, with $v_t \\sim \\mathcal{N}(0, 1)$, falls into. This constitutes a non-linear observation model, rendering the standard Kalman filter inapplicable as it requires both state and observation models to be linear and Gaussian.\n\nThe solution requires a recursive filtering algorithm that propagates the distribution of the state $x_t$ given observations up to time $t$, denoted $y_{1:t}$. The exact posterior distribution $p(x_t \\mid y_{1:t})$ is not Gaussian due to the non-linear nature of the observation. To maintain computational tractability, we approximate the posterior at each time step with a Gaussian distribution, $\\mathcal{N}(x_t; m_{t|t}, P_{t|t})$, by matching the exact first and second moments of the true posterior. This is a form of an assumed density filter.\n\nThe filtering process for each time step $t=1, \\dots, T$ proceeds in two stages: prediction and update.\n\nLet the filtered posterior at time $t-1$ be approximated by $p(x_{t-1} \\mid y_{1:t-1}) \\approx \\mathcal{N}(x_{t-1}; m_{t-1|t-1}, P_{t-1|t-1})$. The process starts with the prior at $t=0$, $p(x_0) = \\mathcal{N}(x_0; m_0, P_0)$, so $m_{0|0} = m_0$ and $P_{0|0} = P_0$.\n\n1.  **Prediction Step:**\n    The predictive distribution for the state $x_t$ given observations $y_{1:t-1}$ is found using the state equation. Since the state transition is linear and the prior is Gaussian, the predictive distribution is also Gaussian, $p(x_t \\mid y_{1:t-1}) = \\mathcal{N}(x_t; m_{t|t-1}, P_{t|t-1})$. Its moments are:\n    $$m_{t|t-1} = \\mathbb{E}[x_t \\mid y_{1:t-1}] = \\phi m_{t-1|t-1}$$\n    $$P_{t|t-1} = \\mathrm{Var}(x_t \\mid y_{1:t-1}) = \\phi^2 P_{t-1|t-1} + \\sigma_w^2$$\n\n2.  **Update Step:**\n    The observation $y_t$ implies that $z_t \\in (\\tau_{j-1}, \\tau_j]$ for some index $j$. To update the state, we first establish the joint predictive distribution of $(x_t, z_t)$ given $y_{1:t-1}$, which is a bivariate Gaussian:\n    $$\n    \\begin{pmatrix} x_t \\\\ z_t \\end{pmatrix} \\mid y_{1:t-1} \\sim \\mathcal{N} \\left( \\begin{pmatrix} m_{t|t-1} \\\\ m_{t|t-1} \\end{pmatrix}, \\begin{pmatrix} P_{t|t-1}  P_{t|t-1} \\\\ P_{t|t-1}  P_{t|t-1} + \\sigma_v^2 \\end{pmatrix} \\right)\n    $$\n    The predictive distribution of the noisy score $z_t$ is therefore $p(z_t \\mid y_{1:t-1}) = \\mathcal{N}(z_t; \\mu_z, \\sigma_z^2)$, where $\\mu_z = m_{t|t-1}$ and $\\sigma_z^2 = P_{t|t-1} + \\sigma_v^2$.\n\n    The one-step-ahead predictive likelihood of the observation $y_t$, required for the total log-likelihood computation, is:\n    $$L_t = \\Pr(y_t \\mid y_{1:t-1}) = \\Pr(\\tau_{j-1}  z_t \\le \\tau_j \\mid y_{1:t-1}) = \\Phi\\left(\\frac{\\tau_j - \\mu_z}{\\sigma_z}\\right) - \\Phi\\left(\\frac{\\tau_{j-1} - \\mu_z}{\\sigma_z}\\right)$$\n    where $\\Phi(\\cdot)$ denotes the standard normal cumulative distribution function (CDF). The total log-likelihood is $\\log L = \\sum_{t=1}^T \\log L_t$.\n\n    The filtered posterior moments $m_{t|t} = \\mathbb{E}[x_t \\mid y_{1:t}]$ and $P_{t|t} = \\mathrm{Var}(x_t \\mid y_{1:t})$ are computed using the properties of conditional Gaussian distributions. These are expressed in terms of the moments of the predictive distribution of $z_t$ truncated to the interval $(\\tau_{j-1}, \\tau_j]$. Let $M_z = \\mathbb{E}[z_t \\mid y_{1:t}]$ and $V_z = \\mathrm{Var}(z_t \\mid y_{1:t})$. The formulas for the moments of a truncated normal variable $Y \\sim \\mathcal{N}(\\mu, \\sigma^2)$ on an interval $(a, b]$ are:\n    $$ \\mathbb{E}[Y \\mid a  Y \\le b] = \\mu + \\sigma \\frac{\\phi_{std}(\\alpha) - \\phi_{std}(\\beta)}{\\Phi_{std}(\\beta) - \\Phi_{std}(\\alpha)} $$\n    $$ \\mathrm{Var}(Y \\mid a  Y \\le b) = \\sigma^2 \\left[ 1 + \\frac{\\alpha\\phi_{std}(\\alpha) - \\beta\\phi_{std}(\\beta)}{\\Phi_{std}(\\beta) - \\Phi_{std}(\\alpha)} - \\left( \\frac{\\phi_{std}(\\alpha) - \\phi_{std}(\\beta)}{\\Phi_{std}(\\beta) - \\Phi_{std}(\\alpha)} \\right)^2 \\right] $$\n    where $\\alpha = (a-\\mu)/\\sigma$, $\\beta = (b-\\mu)/\\sigma$, and $\\phi_{std}(\\cdot)$ is the standard normal probability density function (PDF). Applying these to $z_t$ gives $M_z$ and $V_z$.\n\n    The updated moments for $x_t$ are then:\n    $$m_{t|t} = m_{t|t-1} + K_t (M_z - \\mu_z)$$\n    $$P_{t|t} = P_{t|t-1} - K_t^2 (\\sigma_z^2 - V_z)$$\n    where the gain-like term $K_t$ is given by $K_t = \\mathrm{Cov}(x_t, z_t \\mid y_{1:t-1}) / \\mathrm{Var}(z_t \\mid y_{1:t-1}) = P_{t|t-1} / \\sigma_z^2$.\n\nThis recursive procedure is repeated for all observations in the sequence. The final results for each test case are the total log-likelihood $\\log L$, the final filtered mean $m_{T|T}$, and the final filtered variance $P_{T|T}$. The implementation will follow this algorithm.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the credit rating state-space model filtering problem for a suite of test cases.\n    \"\"\"\n    \n    # Test case definitions\n    test_cases = [\n        # Case A\n        {'phi': 0.95, 'sigma_w': 0.5, 'sigma_v': 0.3, 'm0': 0.0, 'P0': 1.0, \n         'y_seq': [\"A\", \"AA\", \"A\", \"AAA\", \"AA\"]},\n        # Case B\n        {'phi': 1.0, 'sigma_w': 0.2, 'sigma_v': 0.05, 'm0': 1.5, 'P0': 0.2, \n         'y_seq': [\"AAA\", \"AA\", \"AAA\"]},\n        # Case C\n        {'phi': 0.0, 'sigma_w': 1.5, 'sigma_v': 1.0, 'm0': -0.5, 'P0': 2.0, \n         'y_seq': [\"BB\", \"BBB\", \"A\", \"AA\"]}\n    ]\n\n    # Mapping from discrete credit ratings to intervals on the latent score\n    rating_map = {\n        \"BB\": (-np.inf, -1.0),\n        \"BBB\": (-1.0, 0.0),\n        \"A\": (0.0, 1.0),\n        \"AA\": (1.0, 2.0),\n        \"AAA\": (2.0, np.inf)\n    }\n\n    final_results = []\n    \n    for case in test_cases:\n        # Unpack parameters for the current test case\n        phi = case['phi']\n        sigma_w = case['sigma_w']\n        sigma_v = case['sigma_v']\n        m_filt = case['m0']\n        P_filt = case['P0']\n        y_seq = case['y_seq']\n        \n        log_likelihood = 0.0\n\n        for y_t in y_seq:\n            # 1. Prediction step\n            m_pred = phi * m_filt\n            P_pred = phi**2 * P_filt + sigma_w**2\n\n            # 2. Update step\n            tau_lower, tau_upper = rating_map[y_t]\n\n            # Predictive distribution for the noisy latent score z_t\n            mu_z = m_pred\n            var_z = P_pred + sigma_v**2\n            sigma_z = np.sqrt(var_z)\n\n            # Standardized interval bounds for the truncated normal calculation\n            alpha = (tau_lower - mu_z) / sigma_z\n            beta = (tau_upper - mu_z) / sigma_z\n\n            # Predictive likelihood of the observation\n            cdf_beta = norm.cdf(beta)\n            cdf_alpha = norm.cdf(alpha)\n            likelihood_t = cdf_beta - cdf_alpha\n            \n            # Update total log-likelihood, with a check for numerical stability\n            if likelihood_t  1e-300:\n                log_likelihood += np.log(likelihood_t)\n            else:\n                # If likelihood is effectively zero, the observation is considered impossible\n                # under the model. The log-likelihood approaches -infinity.\n                log_likelihood += -700.0\n\n            # Calculate moments of the truncated normal distribution for z_t\n            pdf_alpha = 0.0 if np.isneginf(alpha) else norm.pdf(alpha)\n            pdf_beta = 0.0 if np.isposinf(beta) else norm.pdf(beta)\n            pdf_diff = pdf_alpha - pdf_beta\n\n            M_z, var_z_trunc = 0.0, 0.0\n            if likelihood_t  1e-300:\n                # Mean of truncated z_t\n                M_z = mu_z + sigma_z * pdf_diff / likelihood_t\n\n                # Variance of truncated z_t\n                alpha_pdf_alpha = 0.0 if np.isneginf(alpha) else alpha * pdf_alpha\n                beta_pdf_beta = 0.0 if np.isposinf(beta) else beta * pdf_beta\n                var_factor_num = alpha_pdf_alpha - beta_pdf_beta\n                \n                var_z_trunc = var_z * (1.0 + var_factor_num / likelihood_t - (pdf_diff / likelihood_t)**2)\n            else:\n                # If likelihood is zero, the update is ill-defined.\n                # We do not update the moments, effectively keeping the prediction.\n                # This scenario is not expected with the given test data.\n                M_z = mu_z\n                var_z_trunc = var_z\n            \n            # Update state mean and variance via moment matching\n            K = P_pred / var_z\n            \n            m_filt = m_pred + K * (M_z - mu_z)\n            P_filt = (1.0 - K) * P_pred + K**2 * var_z_trunc\n\n        # Store the final rounded results for this case\n        final_results.append([\n            f\"{m_filt:.6f}\",\n            f\"{P_filt:.6f}\",\n            f\"{log_likelihood:.6f}\"\n        ])\n\n    # Format the final output string to match the required specification exactly.\n    # The required order is [log L, E[x_T], Var(x_T)]. My list has [E, V, logL]. I'll reorder.\n    output_parts = [f\"[{res[2]},{res[0]},{res[1]}]\" for res in final_results]\n    print(f\"[{','.join(output_parts)}]\")\n\nsolve()\n```", "id": "2433415"}]}