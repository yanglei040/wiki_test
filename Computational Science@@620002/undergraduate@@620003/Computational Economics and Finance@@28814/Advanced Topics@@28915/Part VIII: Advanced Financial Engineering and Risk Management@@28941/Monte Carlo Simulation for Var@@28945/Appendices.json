{"hands_on_practices": [{"introduction": "Let's begin our hands-on exploration by implementing one of the most intuitive methods for estimating Value-at-Risk ($VaR$): historical simulation. This non-parametric approach avoids making strong assumptions about the underlying distribution of asset returns, instead using the empirical distribution of historical data as a model for the future. By resampling from past returns to simulate thousands of potential future price paths, you will build a practical understanding of the core Monte Carlo engine and learn to quantify potential losses at a given confidence level [@problem_id:2412304].", "problem": "You are given a price process defined on discrete time steps with an initial price $S_0 \\in \\mathbb{R}_{+}$. At each step $t \\in \\{1,2,\\dots,H\\}$, the simple return $r_t$ is an independent draw from the empirical distribution supported on a provided finite set of historical simple returns $\\{r^{(1)}, r^{(2)}, \\dots, r^{(m)}\\}$, with equal probability $1/m$ for each support point. The price evolves by simple compounding, so that $S_{t} = S_{t-1}\\,(1 + r_t)$ for $t=1,\\dots,H$. For a fixed horizon $H \\in \\mathbb{N}_0$, simulate $N \\in \\mathbb{N}$ independent terminal prices under this model to obtain the sample of terminal prices $\\{S_H^{(i)}\\}_{i=1}^N$. Define the loss on a path $i$ by $L^{(i)} = S_0 - S_H^{(i)}$ (a positive value denotes a loss). For a given probability level $q \\in [0,1]$, define the Value-at-Risk (VaR) at level $q$ as the sample $q$-quantile of the loss distribution. Specifically, if $L_{(1)} \\le \\dots \\le L_{(N)}$ are the ordered losses and $h = (N-1)\\,q + 1$, with $k = \\lfloor h \\rfloor$ and $\\gamma = h - k$, then\n$$\n\\operatorname{VaR}_q =\n\\begin{cases}\nL_{(k)} + \\gamma \\big(L_{(k+1)} - L_{(k)}\\big), & \\text{if } 1 \\le k < N, \\\\\nL_{(N)}, & \\text{if } k \\ge N.\n\\end{cases}\n$$\nIf $N=1$, then $\\operatorname{VaR}_q = L_{(1)}$. All returns must be treated as simple returns expressed in decimals (for example, $0.01$ means one one-hundredth) and the VaR must be expressed as a float in the same currency units as $S_0$ (not as a proportion). Use the specified pseudo-random seed for each test case to initialize the random number generator so that results are reproducible. If $H=0$, interpret this as a degenerate horizon with $S_H = S_0$ almost surely.\n\nYour task is to compute $\\operatorname{VaR}_q$ for each of the following test cases. The final output must be a single line containing a comma-separated list of the results, enclosed in square brackets, in the same order as the test cases. Each result must be rounded to six decimal places.\n\nTest suite:\n1) $S_0 = 100.0$, historical returns $[ -0.03,\\,-0.01,\\,0.0,\\,0.005,\\,0.01,\\,0.015,\\,0.02,\\,0.03 ]$, $H = 5$, $N = 20000$, $q = 0.99$, seed $= 202311$.\n2) $S_0 = 250.0$, historical returns $[ -0.02,\\,-0.015,\\,-0.01,\\,-0.005,\\,0.0,\\,0.005,\\,0.01,\\,0.015 ]$, $H = 10$, $N = 15000$, $q = 0.95$, seed $= 42$.\n3) $S_0 = 100.0$, historical returns $[ -0.02,\\,0.0,\\,0.02 ]$, $H = 1$, $N = 100000$, $q = 0.5$, seed $= 7$.\n4) $S_0 = 123.45$, historical returns $[ -0.01,\\,0.0,\\,0.01,\\,0.02 ]$, $H = 0$, $N = 10000$, $q = 0.99$, seed $= 99$.\n5) $S_0 = 80.0$, historical returns $[ -0.05,\\,-0.01,\\,0.0,\\,0.02,\\,0.04 ]$, $H = 3$, $N = 1$, $q = 0.975$, seed $= 555$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"), where each result is the Value-at-Risk rounded to six decimal places.", "solution": "The problem statement has been rigorously validated and is found to be scientifically sound, well-posed, and complete. It presents a standard computational finance task: the estimation of Value-at-Risk using a non-parametric Monte Carlo simulation. We will now proceed with a formal solution.\n\nThe core of the problem is to estimate the Value-at-Risk ($\\operatorname{VaR}_q$) for a financial position with initial value $S_0$. The price process $S_t$ evolves over discrete time steps $t \\in \\{1, 2, \\dots, H\\}$. The evolution is driven by simple returns $r_t$, which are modeled as independent and identically distributed random variables. Their distribution is the empirical distribution of a given set of $m$ historical returns, $\\{r^{(1)}, r^{(2)}, \\dots, r^{(m)}\\}$, where each historical return is drawn with probability $1/m$.\n\nThe price at time $t$ is given by the simple compounding rule:\n$$\nS_t = S_{t-1} (1 + r_t)\n$$\nStarting from an initial price $S_0$, the terminal price at horizon $H$ is the result of $H$ successive multiplications:\n$$\nS_H = S_0 \\prod_{t=1}^{H} (1 + r_t)\n$$\nThe loss associated with a price path is defined as the difference between the initial price and the terminal price:\n$$\nL = S_0 - S_H\n$$\nA positive value of $L$ corresponds to a monetary loss.\n\nTo estimate the $\\operatorname{VaR}_q$, which is the $q$-quantile of the loss distribution, we employ a Monte Carlo method. This involves simulating $N$ independent paths of the price process. For each simulation path $i \\in \\{1, \\dots, N\\}$, we generate a sequence of $H$ random returns $\\{r_{i,t}\\}_{t=1}^H$, each drawn from the specified empirical distribution. This yields a sample of $N$ terminal prices:\n$$\nS_H^{(i)} = S_0 \\prod_{t=1}^{H} (1 + r_{i,t}) \\quad \\text{for } i=1, \\dots, N\n$$\nFrom this sample of terminal prices, we compute the corresponding sample of losses:\n$$\nL^{(i)} = S_0 - S_H^{(i)} \\quad \\text{for } i=1, \\dots, N\n$$\nThe $\\operatorname{VaR}_q$ is then calculated as the sample $q$-quantile of the set of losses $\\{L^{(i)}\\}_{i=1}^N$. The problem specifies a precise definition for the sample quantile based on linear interpolation. Let $L_{(1)} \\le L_{(2)} \\le \\dots \\le L_{(N)}$ be the losses sorted in non-decreasing order. The quantile is computed using an index $h = (N-1)q + 1$. Let $k = \\lfloor h \\rfloor$ be the integer part and $\\gamma = h - k$ be the fractional part. The $\\operatorname{VaR}_q$ is given by:\n$$\n\\operatorname{VaR}_q = L_{(k)} + \\gamma (L_{(k+1)} - L_{(k)})\n$$\nfor $1 \\le k < N$. If $k \\ge N$ (which occurs for $q=1$), $\\operatorname{VaR}_q = L_{(N)}$. This definition is equivalent to the `'linear'` interpolation method for quantiles, which is a standard implementation available in numerical libraries such as `numpy`.\n\nThe computational algorithm is designed for efficiency using vectorized operations:\n1.  For each test case, initialize parameters $S_0$, the array of historical returns $\\mathcal{R}$, horizon $H$, number of paths $N$, quantile level $q$, and the random seed.\n2.  A pseudo-random number generator is seeded to ensure reproducibility.\n3.  The special case $H=0$ implies $S_H = S_0$, thus the loss is always $0$, and $\\operatorname{VaR}_q = 0.0$.\n4.  The special case $N=1$ requires simulating only one path. The single resulting loss $L^{(1)} = S_0 - S_H^{(1)}$ is, by definition, the $\\operatorname{VaR}_q$.\n5.  In the general case ($H > 0$, $N > 1$), an $N \\times H$ matrix of returns is generated by drawing from the historical returns array with replacement.\n6.  This matrix of returns is converted into a matrix of growth factors $(1+r)$. The product of these factors along each row (the time axis) yields a vector of total path multipliers.\n7.  Multiplying this vector by $S_0$ gives the vector of $N$ terminal prices, $\\{S_H^{(i)}\\}$.\n8.  The vector of losses $\\{L^{(i)}\\}$ is computed as $S_0 - \\{S_H^{(i)}\\}$.\n9.  Finally, the `numpy.quantile` function, configured for linear interpolation, is applied to the loss vector with the specified quantile level $q$ to compute the $\\operatorname{VaR}_q$.\n\nThis procedure is systematically applied to each test case provided in the problem statement. The final results are rounded to the specified precision.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes Value-at-Risk (VaR) for several test cases using Monte Carlo simulation.\n    \"\"\"\n    test_cases = [\n        (100.0, [-0.03, -0.01, 0.0, 0.005, 0.01, 0.015, 0.02, 0.03], 5, 20000, 0.99, 202311),\n        (250.0, [-0.02, -0.015, -0.01, -0.005, 0.0, 0.005, 0.01, 0.015], 10, 15000, 0.95, 42),\n        (100.0, [-0.02, 0.0, 0.02], 1, 100000, 0.5, 7),\n        (123.45, [-0.01, 0.0, 0.01, 0.02], 0, 10000, 0.99, 99),\n        (80.0, [-0.05, -0.01, 0.0, 0.02, 0.04], 3, 1, 0.975, 555),\n    ]\n\n    results = []\n    for case in test_cases:\n        S0, hist_returns, H, N, q, seed = case\n        \n        # Handle the degenerate case where the horizon H is 0.\n        # The terminal price is S0, so the loss is always 0.\n        if H == 0:\n            var = 0.0\n            results.append(var)\n            continue\n            \n        rng = np.random.default_rng(seed)\n        hist_returns_arr = np.array(hist_returns)\n        \n        # Handle the case where only one path is simulated (N=1).\n        # The VaR is simply the loss from that single path.\n        if N == 1:\n            # Simulate one path of H steps.\n            returns_path = rng.choice(hist_returns_arr, size=H, replace=True)\n            # The problem defines simple compounding.\n            path_multiplier = np.prod(1 + returns_path)\n            S_H = S0 * path_multiplier\n            loss = S0 - S_H\n            var = loss\n            results.append(var)\n            continue\n\n        # General case for Monte Carlo simulation (H > 0, N > 1).\n        # Generate N x H matrix of random returns by choosing from historical returns.\n        returns_matrix = rng.choice(hist_returns_arr, size=(N, H), replace=True)\n        \n        # Calculate terminal prices for all N paths using vectorized operations.\n        # The product of (1 + r_t) is taken along the time axis (axis=1).\n        path_multipliers = np.prod(1 + returns_matrix, axis=1)\n        terminal_prices = S0 * path_multipliers\n        \n        # Calculate the distribution of losses.\n        losses = S0 - terminal_prices\n        \n        # Calculate VaR, which is the sample q-quantile of the loss distribution.\n        # The problem's VaR formula corresponds to numpy's 'linear' interpolation method.\n        var = np.quantile(losses, q, method='linear')\n        \n        results.append(var)\n\n    # Format the final output as a comma-separated list string, with each\n    # result rounded to six decimal places.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2412304"}, {"introduction": "While Value-at-Risk is a widely used benchmark, it is not a \"coherent\" risk measure in all circumstances, primarily because it can violate the principle of subadditivity. This practice provides a hands-on demonstration of this critical theoretical weakness, showing that for certain types of risks, diversification can counter-intuitively increase the portfolio's $VaR$ [@problem_id:2412240]. By constructing portfolios where $VaR(A+B) > VaR(A) + VaR(B)$, you will gain a deeper appreciation for the limitations of $VaR$ and the importance of choosing appropriate risk measures.", "problem": "Construct a self-contained program that computes, from first principles, whether Value-at-Risk (VaR) is subadditive in specifically defined cases. Let a loss be a real-valued random variable. For a loss random variable $X$ with cumulative distribution function $F_X(x)$ and a level $\\alpha \\in (0,1)$, define the Value-at-Risk at level $\\alpha$ as\n$$\n\\operatorname{VaR}_{\\alpha}(X) = \\inf\\{x \\in \\mathbb{R} : F_X(x) \\ge \\alpha\\}.\n$$\nGiven an independent and identically distributed sample $L_1, \\dots, L_n$ from $X$, define the empirical distribution function $F_n(x) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{L_i \\le x\\}$ and the empirical Value-at-Risk estimator\n$$\n\\widehat{\\operatorname{VaR}}_{\\alpha}(X) = \\inf\\{x \\in \\mathbb{R} : F_n(x) \\ge \\alpha\\}.\n$$\nConsider two loss random variables $A$ and $B$ for each test case below, and define the portfolio loss $P = A + B$. For each test case $j$, compute the boolean outcome\n$$\nI_j = \\big[\\widehat{\\operatorname{VaR}}_{\\alpha}(P) > \\widehat{\\operatorname{VaR}}_{\\alpha}(A) + \\widehat{\\operatorname{VaR}}_{\\alpha}(B)\\big].\n$$\nUse the same sample size $n$ and the same fixed pseudorandom seed $s$ for all cases. All probabilities are to be interpreted as decimals in $[0,1]$. No physical units apply.\n\nYour program must implement the following test suite. In all cases, the draw size is $n = 400,000$ and the pseudorandom seed is $s = 20231407$. All Bernoulli events and standard normal variables are to be drawn according to the specified dependence structure.\n\n- Test case $1$ (independent, rare large losses): $\\alpha = 0.95$. Asset $A$ has loss $10$ with probability $0.03$ and loss $0$ otherwise. Asset $B$ has the same distribution. $A$ and $B$ are independent.\n- Test case $2$ (independent, asymmetric rare losses): $\\alpha = 0.975$. Asset $A$ has loss $8$ with probability $0.02$ and loss $0$ otherwise. Asset $B$ has loss $12$ with probability $0.02$ and loss $0$ otherwise. $A$ and $B$ are independent.\n- Test case $3$ (comonotonic rare losses): $\\alpha = 0.95$. Asset $A$ has loss $10$ with probability $0.03$ and loss $0$ otherwise. Asset $B$ has the same marginal distribution. The dependence is comonotonic: there exists a uniform $U$ on $[0,1]$ such that both $A$ and $B$ equal their loss level if and only if $U < 0.03$, and otherwise both equal $0$.\n- Test case $4$ (correlated Gaussian losses): $\\alpha = 0.99$. Asset $A$ is Gaussian with mean $0$ and standard deviation $1$. Asset $B$ is Gaussian with mean $0$ and standard deviation $1.2$. The correlation between $A$ and $B$ is $0.6$.\n- Test case $5$ (perfectly negatively dependent Gaussian losses): $\\alpha = 0.99$. Asset $A$ is Gaussian with mean $0$ and standard deviation $1$. Asset $B$ equals $-A$ almost surely (perfect negative dependence), so the portfolio loss is identically $0$.\n\nFor each case, generate exactly $n$ paired draws $(A_i,B_i)$ consistent with the stated distributions and dependence. Compute the empirical Value-at-Risk at the specified $\\alpha$ for $A$, $B$, and $P = A+B$ using the definition of $\\widehat{\\operatorname{VaR}}_{\\alpha}$ above. Then compute $I_j$ as defined.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true,false,true,true,false]\") but using capitalized Python boolean literals. That is, the exact output format must be\n\"[result1,result2,result3,result4,result5]\" with each result either \"True\" or \"False\".", "solution": "The problem requires a computational investigation into the subadditivity of the Value-at-Risk ($\\operatorname{VaR}$) measure. A risk measure $\\rho$ is defined as subadditive if for any two loss random variables $X$ and $Y$, the inequality $\\rho(X+Y) \\le \\rho(X) + \\rho(Y)$ holds. This property formalizes the principle of diversification: the risk of a combined portfolio should not be greater than the sum of the risks of its individual components. Value-at-Risk is a widely used risk measure, but it is not a coherent risk measure in the sense of Artzner et al. (1999) precisely because it can fail to be subadditive. This exercise demonstrates this failure through specific, constructed examples.\n\nThe task is to compute the boolean outcome of the inequality $\\widehat{\\operatorname{VaR}}_{\\alpha}(P) > \\widehat{\\operatorname{VaR}}_{\\alpha}(A) + \\widehat{\\operatorname{VaR}}_{\\alpha}(B)$ for five distinct test cases, where $A$ and $B$ are loss random variables, $P=A+B$ is the portfolio loss, and $\\widehat{\\operatorname{VaR}}_{\\alpha}$ is the empirical Value-at-Risk estimator at a confidence level $\\alpha$.\n\nThe empirical $\\operatorname{VaR}$ is defined as $\\widehat{\\operatorname{VaR}}_{\\alpha}(X) = \\inf\\{x \\in \\mathbb{R} : F_n(x) \\ge \\alpha\\}$, where $F_n(x)$ is the empirical cumulative distribution function from a sample of size $n$. For a sample of losses $L_1, \\dots, L_n$, let the sorted sample be $L_{(1)} \\le L_{(2)} \\le \\dots \\le L_{(n)}$. The empirical CDF $F_n(x)$ is a step function that jumps by $1/n$ at each data point. The condition $F_n(x) \\ge \\alpha$ is equivalent to saying that at least $n\\alpha$ of the sample points are less than or equal to $x$. This means $x$ must be at least as large as the $\\lceil n\\alpha \\rceil$-th order statistic. Therefore, the empirical VaR is precisely the value of this order statistic:\n$$\n\\widehat{\\operatorname{VaR}}_{\\alpha}(X) = L_{(\\lceil n\\alpha \\rceil)}\n$$\nOur general procedure for each test case is as follows:\n1.  Initialize a pseudorandom number generator with the fixed seed $s = 20231407$.\n2.  Generate a sample of $n = 400,000$ paired draws $(A_i, B_i)$ for $i=1, \\dots, n$, according to the specified marginal distributions and dependence structure.\n3.  Create the portfolio loss sample $P_i = A_i + B_i$.\n4.  For each of the three samples $\\{A_i\\}$, $\\{B_i\\}$, and $\\{P_i\\}$, compute the empirical $\\operatorname{VaR}$ at the specified level $\\alpha$. This involves sorting the sample and selecting the element at the index $k-1$ (using $0$-based indexing), where $k = \\lceil n\\alpha \\rceil$.\n5.  Evaluate the boolean expression $I_j = [\\widehat{\\operatorname{VaR}}_{\\alpha}(P) > \\widehat{\\operatorname{VaR}}_{\\alpha}(A) + \\widehat{\\operatorname{VaR}}_{\\alpha}(B)]$.\n\nThe specific generation methods for each case are:\n\nTest case $1$ (independent, rare large losses): $\\alpha = 0.95$. The losses for $A$ and $B$ are drawn from a Bernoulli distribution, representing a loss of $10$ with probability $p=0.03$ and $0$ otherwise. Independence is modeled by using two separate streams of uniform random numbers, $U_A$ and $U_B$, to generate the samples for $A$ and $B$. For each $i \\in \\{1, \\dots, n\\}$, $A_i=10$ if $U_{A,i} < 0.03$ and $A_i=0$ otherwise, and similarly for $B_i$ using $U_{B,i}$. The index for the VaR calculation is $k = \\lceil 400,000 \\times 0.95 \\rceil = 380,000$.\n\nTest case $2$ (independent, asymmetric rare losses): $\\alpha = 0.975$. This case is structurally similar to the first, but with asymmetric parameters. Asset $A$ has loss $8$ with probability $0.02$, and asset $B$ has loss $12$ with probability $0.02$. Independence is again modeled with separate random number streams. The VaR index is $k = \\lceil 400,000 \\times 0.975 \\rceil = 390,000$.\n\nTest case $3$ (comonotonic rare losses): $\\alpha = 0.95$. The marginal distributions for $A$ and $B$ are identical to case $1$. However, the dependence is comonotonic. This is modeled by using a single stream of uniform random numbers, $U$. For each $i \\in \\{1, \\dots, n\\}$, both $A_i$ and $B_i$ are set to $10$ if $U_i < 0.03$, and both are $0$ otherwise. This represents a perfect positive dependence in the tail events. The VaR index is $k = \\lceil 400,000 \\times 0.95 \\rceil = 380,000$.\n\nTest case $4$ (correlated Gaussian losses): $\\alpha = 0.99$. $A \\sim \\mathcal{N}(0, 1^2)$ and $B \\sim \\mathcal{N}(0, 1.2^2)$ with correlation $\\rho(A, B) = 0.6$. We generate paired samples from this bivariate normal distribution. This can be achieved by first generating pairs of independent standard normal variates $(Z_{1,i}, Z_{2,i})$ and then applying a linear transformation corresponding to the Cholesky decomposition of the covariance matrix $\\Sigma$:\n$$\n\\Sigma = \\begin{pmatrix} \\sigma_A^2 & \\rho \\sigma_A \\sigma_B \\\\ \\rho \\sigma_A \\sigma_B & \\sigma_B^2 \\end{pmatrix} = \\begin{pmatrix} 1 & 0.72 \\\\ 0.72 & 1.44 \\end{pmatrix}\n$$\nA simpler, equivalent construction is $A_i = Z_{1,i}$ and $B_i = \\sigma_B (\\rho Z_{1,i} + \\sqrt{1-\\rho^2} Z_{2,i})$. The VaR index is $k = \\lceil 400,000 \\times 0.99 \\rceil = 396,000$. For elliptical distributions like the multivariate normal, VaR is known to be subadditive.\n\nTest case $5$ (perfectly negatively dependent Gaussian losses): $\\alpha = 0.99$. Asset $A \\sim \\mathcal{N}(0, 1^2)$ and $B = -A$. This is a case of perfect hedging. We generate a sample of standard normal variates for $A$ and set the sample for $B$ to be its element-wise negation. The portfolio loss is $P = A+B = 0$ for all outcomes. Consequently, its empirical $\\widehat{\\operatorname{VaR}}_{\\alpha}(P)$ must be $0$. The VaR index is $k = \\lceil 400,000 \\times 0.99 \\rceil = 396,000$.", "answer": "```python\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Computes whether Value-at-Risk (VaR) is subadditive in five specific cases.\n    \"\"\"\n    \n    # Global parameters\n    N_SAMPLES = 400_000\n    SEED = 20231407\n    \n    # Initialize a single random number generator for all simulations\n    rng = np.random.default_rng(SEED)\n\n    test_cases = [\n        # Case 1: Independent, rare large losses\n        {\n            \"alpha\": 0.95,\n            \"A\": {\"type\": \"bernoulli\", \"loss\": 10.0, \"p\": 0.03},\n            \"B\": {\"type\": \"bernoulli\", \"loss\": 10.0, \"p\": 0.03},\n            \"dep\": \"independent\"\n        },\n        # Case 2: Independent, asymmetric rare losses\n        {\n            \"alpha\": 0.975,\n            \"A\": {\"type\": \"bernoulli\", \"loss\": 8.0, \"p\": 0.02},\n            \"B\": {\"type\": \"bernoulli\", \"loss\": 12.0, \"p\": 0.02},\n            \"dep\": \"independent\"\n        },\n        # Case 3: Comonotonic rare losses\n        {\n            \"alpha\": 0.95,\n            \"A\": {\"type\": \"bernoulli\", \"loss\": 10.0, \"p\": 0.03},\n            \"B\": {\"type\": \"bernoulli\", \"loss\": 10.0, \"p\": 0.03},\n            \"dep\": \"comonotonic\"\n        },\n        # Case 4: Correlated Gaussian losses\n        {\n            \"alpha\": 0.99,\n            \"A\": {\"type\": \"gaussian\", \"mean\": 0.0, \"std\": 1.0},\n            \"B\": {\"type\": \"gaussian\", \"mean\": 0.0, \"std\": 1.2},\n            \"dep\": \"correlated\", \"rho\": 0.6\n        },\n        # Case 5: Perfectly negatively dependent Gaussian losses\n        {\n            \"alpha\": 0.99,\n            \"A\": {\"type\": \"gaussian\", \"mean\": 0.0, \"std\": 1.0},\n            \"B\": {\"type\": \"gaussian\", \"mean\": 0.0, \"std\": 1.0},\n            \"dep\": \"negatively_dependent\"\n        }\n    ]\n\n    results = []\n\n    def get_empirical_var(samples, alpha):\n        \"\"\"\n        Calculates the empirical Value-at-Risk.\n        Defined as the k-th order statistic, where k = ceil(n*alpha).\n        \"\"\"\n        n = len(samples)\n        k = math.ceil(n * alpha)\n        # Using k-1 for 0-based indexing\n        sorted_samples = np.sort(samples)\n        return sorted_samples[k - 1]\n\n    for case in test_cases:\n        alpha = case[\"alpha\"]\n        \n        # --- Generate samples for A and B based on the case ---\n        if case[\"dep\"] == \"independent\":\n            u_a = rng.random(size=N_SAMPLES)\n            u_b = rng.random(size=N_SAMPLES)\n            params_a = case[\"A\"]\n            samples_a = np.where(u_a < params_a[\"p\"], params_a[\"loss\"], 0.0)\n            params_b = case[\"B\"]\n            samples_b = np.where(u_b < params_b[\"p\"], params_b[\"loss\"], 0.0)\n\n        elif case[\"dep\"] == \"comonotonic\":\n            u = rng.random(size=N_SAMPLES)\n            params_a = case[\"A\"]\n            samples_a = np.where(u < params_a[\"p\"], params_a[\"loss\"], 0.0)\n            params_b = case[\"B\"]\n            samples_b = np.where(u < params_b[\"p\"], params_b[\"loss\"], 0.0)\n\n        elif case[\"dep\"] == \"correlated\":\n            params_a = case[\"A\"]\n            params_b = case[\"B\"]\n            rho = case[\"rho\"]\n            \n            mean = [params_a[\"mean\"], params_b[\"mean\"]]\n            cov = [[params_a[\"std\"]**2, rho * params_a[\"std\"] * params_b[\"std\"]],\n                   [rho * params_a[\"std\"] * params_b[\"std\"], params_b[\"std\"]**2]]\n            \n            samples = rng.multivariate_normal(mean, cov, size=N_SAMPLES)\n            samples_a = samples[:, 0]\n            samples_b = samples[:, 1]\n            \n        elif case[\"dep\"] == \"negatively_dependent\":\n            params_a = case[\"A\"]\n            samples_a = rng.normal(loc=params_a[\"mean\"], scale=params_a[\"std\"], size=N_SAMPLES)\n            samples_b = -samples_a\n        \n        # --- Create portfolio samples ---\n        samples_p = samples_a + samples_b\n        \n        # --- Calculate VaR for A, B, and Portfolio P ---\n        var_a = get_empirical_var(samples_a, alpha)\n        var_b = get_empirical_var(samples_b, alpha)\n        var_p = get_empirical_var(samples_p, alpha)\n        \n        # --- Check for subadditivity violation ---\n        is_violated = var_p > (var_a + var_b)\n        results.append(is_violated)\n\n    # Print results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2412240"}, {"introduction": "This capstone exercise showcases the flexibility and power of the Monte Carlo method beyond standard financial assets. It challenges you to integrate a hedonic pricing model, which values assets based on their characteristics, into a risk simulation framework for an illiquid market like fine art [@problem_id:2412262]. You will model correlated auction shocks using a factor structure, demonstrating how to build a sophisticated, custom $VaR$ model from the ground up for complex, real-world valuation problems.", "problem": "You are tasked with building a complete, runnable program to estimate one-period Value at Risk (VaR) for an art collection using Monte Carlo simulation that combines a hedonic pricing model with simulated auction results. Your design must start from core definitions: the hedonic model represents log-prices as a linear function of observable characteristics, and Value at Risk at confidence level $q$ is the $q$-quantile of the portfolio loss distribution. No closed-form risk formula may be used; you must simulate the distribution directly.\n\nConsider a collection of $M$ distinct artworks. Each artwork $j$ has a feature vector $\\mathbf{x}_j \\in \\mathbb{R}^K$ (including an intercept term) and a corresponding coefficient vector $\\boldsymbol{\\beta} \\in \\mathbb{R}^K$. The hedonic log-price for item $j$ is $\\mu_j = \\mathbf{x}_j^{\\top}\\boldsymbol{\\beta}$. The current mark-to-model portfolio value is $V_0 = \\sum_{j=1}^M \\exp(\\mu_j)$ in United States dollars. Auction outcomes are modeled as multiplicative lognormal shocks applied to each item’s hedonic baseline. Specifically, future price of item $j$ is $P_j = \\exp(\\mu_j + \\varepsilon_j)$ where the vector of auction shocks $\\boldsymbol{\\varepsilon} = (\\varepsilon_1,\\dots,\\varepsilon_M)$ is multivariate normal with correlation induced by a single common factor. To maintain $\\mathbb{E}[P_j \\mid \\mu_j] = \\exp(\\mu_j)$ for each item, assume $\\mathbb{E}[\\exp(\\varepsilon_j)] = 1$, which implies a mean adjustment based on the variance of $\\varepsilon_j$. The one-period portfolio loss is $L = V_0 - V_1$ where $V_1 = \\sum_{j=1}^M P_j$. The VaR at confidence level $q \\in (0,1)$ is computed as the empirical $q$-quantile of simulated $L$.\n\nUse the following hedonic model for $M = 5$ items and $K = 5$ features (including an intercept). For each item $j$, the feature vector is $\\mathbf{x}_j = (1, \\text{size}_j, \\text{pop}_j, \\text{oil}_j, \\text{age}_j)^{\\top}$ with:\n- Size in square meters, a nonnegative real number.\n- Popularity index, a nonnegative real number on approximately the range $[0,10]$.\n- Medium indicator for oil painting as $0$ or $1$.\n- Age in decades, a nonnegative real number.\n\nThe common coefficient vector is $\\boldsymbol{\\beta} = (10.0, 0.8, 0.15, 0.3, 0.02)^{\\top}$. The five items have features:\n- Item $1$: $(1, 0.5, 8.0, 1, 5.0)$\n- Item $2$: $(1, 1.2, 5.0, 0, 2.0)$\n- Item $3$: $(1, 0.8, 9.0, 1, 10.0)$\n- Item $4$: $(1, 2.0, 3.0, 0, 1.0)$\n- Item $5$: $(1, 0.3, 6.0, 1, 3.0)$\n\nAuction shocks are correlated via a single market factor. Let $\\sigma_j \\ge 0$ denote the idiosyncratic volatility parameter for item $j$. For a chosen correlation level $\\rho \\in [0,1]$, define the auction shock for item $j$ in one simulation draw as\n$$\n\\varepsilon_j = m_j + \\sigma_j\\left( \\sqrt{\\rho}\\,Z + \\sqrt{1-\\rho}\\,Z_j \\right),\n$$\nwhere $Z \\sim \\mathcal{N}(0,1)$ is the common market factor, $Z_j \\sim \\mathcal{N}(0,1)$ are independent across $j$ and independent of $Z$, and $m_j$ is a deterministic mean adjustment that ensures $\\mathbb{E}[\\exp(\\varepsilon_j)] = 1$ in order to preserve the hedonic expectation $\\exp(\\mu_j)$. The correlation between items’ shocks is governed by $\\rho$, and the marginal variance of $\\varepsilon_j$ is $\\sigma_j^2$.\n\nYour program must:\n- Compute $\\mu_j = \\mathbf{x}_j^{\\top}\\boldsymbol{\\beta}$ for each item and $V_0 = \\sum_{j=1}^5 \\exp(\\mu_j)$ in dollars.\n- For each test case, simulate $N$ independent draws of $(\\varepsilon_1,\\dots,\\varepsilon_5)$ using the factor structure above with the specified $\\rho$ and $\\sigma_j$ values, using the mean adjustment $m_j$ that enforces $\\mathbb{E}[\\exp(\\varepsilon_j)] = 1$.\n- For each draw, compute $V_1 = \\sum_{j=1}^5 \\exp(\\mu_j + \\varepsilon_j)$, then the loss $L = V_0 - V_1$.\n- Estimate VaR at confidence level $q$ as the empirical $q$-quantile of the simulated losses.\n- Use a fixed random seed schedule for reproducibility: let the base seed be $20231120$; for test case index $i \\in \\{0,1,2,3\\}$, use seed $20231120 + 1000\\,i$.\n- Round each VaR to two decimals and report it as a floating-point number representing United States dollars (no currency symbol), i.e., in dollars.\n\nTest suite. Use the fixed hedonic model above and evaluate the following four cases:\n- Case $1$ (happy path): $\\boldsymbol{\\sigma} = (0.25, 0.20, 0.30, 0.15, 0.22)$, $\\rho = 0.0$, $q = 0.95$, $N = 100000$.\n- Case $2$ (high correlation): $\\boldsymbol{\\sigma} = (0.25, 0.20, 0.30, 0.15, 0.22)$, $\\rho = 0.9$, $q = 0.99$, $N = 100000$.\n- Case $3$ (boundary with zero volatility): $\\boldsymbol{\\sigma} = (0.0, 0.0, 0.0, 0.0, 0.0)$, $\\rho = 0.5$, $q = 0.975$, $N = 100000$.\n- Case $4$ (small sample stress): $\\boldsymbol{\\sigma} = (0.25, 0.20, 0.30, 0.15, 0.22)$, $\\rho = 0.5$, $q = 0.95$, $N = 2000$.\n\nFinal output format. Your program should produce a single line of output containing the VaR estimates for the four cases as a comma-separated list enclosed in square brackets, for example, $[v_1,v_2,v_3,v_4]$, where each $v_i$ is the rounded VaR in dollars as specified.", "solution": "We begin from core definitions. A hedonic pricing model represents the logarithm of price as a linear function of observable attributes. For item $j$ with features $\\mathbf{x}_j \\in \\mathbb{R}^K$ and coefficients $\\boldsymbol{\\beta} \\in \\mathbb{R}^K$, the log-price is $\\mu_j = \\mathbf{x}_j^{\\top}\\boldsymbol{\\beta}$. The current model-implied price is $\\exp(\\mu_j)$, so the mark-to-model portfolio value over $M$ items is $V_0 = \\sum_{j=1}^M \\exp(\\mu_j)$, in dollars.\n\nTo model auction outcomes, we require a stochastic multiplicative factor that perturbs each item’s hedonic baseline while maintaining the hedonic expected value. Let $\\varepsilon_j$ denote the auction log-return for item $j$ over the risk horizon. Assume $\\boldsymbol{\\varepsilon} = (\\varepsilon_1,\\dots,\\varepsilon_M)$ is multivariate normal; then the price is $P_j = \\exp(\\mu_j + \\varepsilon_j)$. For each $j$, we wish to maintain $\\mathbb{E}[P_j \\mid \\mu_j] = \\exp(\\mu_j)$, which is equivalent to $\\mathbb{E}[\\exp(\\varepsilon_j)] = 1$. If $\\varepsilon_j \\sim \\mathcal{N}(m_j, \\sigma_j^2)$ marginally, then $\\mathbb{E}[\\exp(\\varepsilon_j)] = \\exp(m_j + \\tfrac{1}{2}\\sigma_j^2)$. Setting this equal to $1$ yields $m_j + \\tfrac{1}{2}\\sigma_j^2 = 0$, i.e., $m_j = -\\tfrac{1}{2}\\sigma_j^2$. This mean correction ensures the hedonic expectation is preserved even when shocks are correlated.\n\nWe model cross-item dependence via a one-factor structure:\n$$\n\\varepsilon_j = m_j + \\sigma_j\\left(\\sqrt{\\rho}\\,Z + \\sqrt{1-\\rho}\\,Z_j\\right),\n$$\nwhere $Z \\sim \\mathcal{N}(0,1)$ is a common factor and $Z_j \\sim \\mathcal{N}(0,1)$ are independent idiosyncratic factors, all mutually independent. Then $\\mathbb{E}[\\varepsilon_j] = m_j$, $\\mathrm{Var}(\\varepsilon_j) = \\sigma_j^2$, and for $i \\neq j$, $\\mathrm{Cov}(\\varepsilon_i,\\varepsilon_j) = \\sigma_i \\sigma_j \\rho$. Hence, the correlation between items’ log-auction shocks is $\\rho$, and the marginal variance is $\\sigma_j^2$. For $\\sigma_j = 0$, we get $\\varepsilon_j = 0$ almost surely, leading to degenerate price dynamics for that item and, in aggregate, a degenerate portfolio when all $\\sigma_j = 0$, which yields a VaR of $0$.\n\nThe one-period portfolio loss is defined as $L = V_0 - V_1$, where $V_1 = \\sum_{j=1}^M \\exp(\\mu_j + \\varepsilon_j)$. A positive value of $L$ indicates a loss relative to the current mark-to-model value. The Value at Risk at confidence level $q \\in (0,1)$ is the $q$-quantile of the loss distribution:\n$$\n\\mathrm{VaR}_q = \\inf\\{\\ell \\in \\mathbb{R} : \\mathbb{P}(L \\le \\ell) \\ge q\\}.\n$$\nIn the absence of a closed-form expression for the distribution of $L$ under correlated lognormal components, we approximate $\\mathrm{VaR}_q$ by Monte Carlo simulation: simulate many independent draws of $(\\varepsilon_1,\\dots,\\varepsilon_M)$, compute the corresponding $L$, and take the empirical $q$-quantile.\n\nAlgorithmic design:\n1. Compute $\\mu_j = \\mathbf{x}_j^{\\top}\\boldsymbol{\\beta}$ for $j \\in \\{1,\\dots,M\\}$ and $V_0 = \\sum_{j=1}^M \\exp(\\mu_j)$.\n2. For each test case, with parameters $(\\boldsymbol{\\sigma}, \\rho, q, N)$:\n   - Set $m_j = -\\tfrac{1}{2}\\sigma_j^2$ for each $j$.\n   - Initialize a pseudorandom number generator with seed $20231120 + 1000\\,i$ for case index $i \\in \\{0,1,2,3\\}$.\n   - Draw $N$ independent common factors $Z \\sim \\mathcal{N}(0,1)$ and an $N \\times M$ array of independent idiosyncratic factors $Z_j \\sim \\mathcal{N}(0,1)$.\n   - Form $\\varepsilon$ via vectorization: $\\varepsilon_{n,j} = m_j + \\sigma_j(\\sqrt{\\rho}\\,Z_n + \\sqrt{1-\\rho}\\,Z_{n,j})$ for $n \\in \\{1,\\dots,N\\}$ and $j \\in \\{1,\\dots,M\\}$.\n   - Compute $V_{1,n} = \\sum_{j=1}^M \\exp(\\mu_j + \\varepsilon_{n,j})$ and $L_n = V_0 - V_{1,n}$ for each $n$.\n   - Estimate $\\mathrm{VaR}_q$ as the empirical $q$-quantile of $\\{L_n\\}_{n=1}^N$.\n3. Round each VaR to two decimals (dollars), output as a list.\n\nCorrectness notes:\n- The mean correction $m_j = -\\tfrac{1}{2}\\sigma_j^2$ ensures $\\mathbb{E}[\\exp(\\varepsilon_j)]=1$ for each $j$ because for any normal random variable $Y \\sim \\mathcal{N}(m,s^2)$, $\\mathbb{E}[\\exp(Y)] = \\exp(m + \\tfrac{1}{2}s^2)$. This preserves the hedonic expected price $\\exp(\\mu_j)$ under the multiplicative auction factor.\n- The factor construction delivers the intended correlation structure: for $i \\neq j$, $\\mathrm{Corr}(\\varepsilon_i,\\varepsilon_j) = \\rho$ since the shared component is $\\sqrt{\\rho}\\,\\sigma_i Z$ and $\\sqrt{\\rho}\\,\\sigma_j Z$ respectively, while idiosyncratic components are independent.\n- In the boundary case where $\\boldsymbol{\\sigma} = \\mathbf{0}$, all $\\varepsilon_j = 0$ almost surely, thus $V_1 = V_0$ and $L = 0$ almost surely, so the empirical quantile is exactly $0$.\n\nImplementation details:\n- Vectorized computation avoids constructing full covariance matrices and handles singular cases gracefully, including $\\rho \\in \\{0,1\\}$ and $\\sigma_j = 0$.\n- We employ deterministic seeding per case to ensure reproducible VaR estimates.\n- The final output is a single line in the specified list format $[v_1,v_2,v_3,v_4]$, where each $v_i$ is a float in dollars rounded to two decimals.\n\nThe program will compute the VaR for the four test cases:\n- Case $1$: $\\boldsymbol{\\sigma} = (0.25, 0.20, 0.30, 0.15, 0.22)$, $\\rho = 0.0$, $q = 0.95$, $N = 100000$.\n- Case $2$: $\\boldsymbol{\\sigma} = (0.25, 0.20, 0.30, 0.15, 0.22)$, $\\rho = 0.9$, $q = 0.99$, $N = 100000$.\n- Case $3$: $\\boldsymbol{\\sigma} = (0.0, 0.0, 0.0, 0.0, 0.0)$, $\\rho = 0.5$, $q = 0.975$, $N = 100000$.\n- Case $4$: $\\boldsymbol{\\sigma} = (0.25, 0.20, 0.30, 0.15, 0.22)$, $\\rho = 0.5$, $q = 0.95$, $N = 2000$.\n\nAll VaR outputs are expressed in dollars as floats, rounded to two decimals, with no currency symbol, aggregated as a single bracketed, comma-separated list on one line as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef hedonic_mu(X, beta):\n    \"\"\"\n    Compute hedonic log-prices mu_j = x_j^T beta for each item.\n    X: array of shape (M, K)\n    beta: array of shape (K,)\n    Returns: array mu of shape (M,)\n    \"\"\"\n    return X @ beta\n\ndef simulate_var(mu, sigmas, rho, q, N, seed):\n    \"\"\"\n    Simulate VaR at confidence level q via Monte Carlo using a one-factor\n    correlated normal model for auction shocks with mean correction.\n\n    mu: array of shape (M,), hedonic log-prices\n    sigmas: array of shape (M,), nonnegative volatilities\n    rho: float in [0,1], common correlation parameter\n    q: float in (0,1), VaR confidence level\n    N: int, number of simulations\n    seed: int, random seed for reproducibility\n    Returns: float, VaR estimate (not rounded)\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    mu = np.asarray(mu, dtype=float)\n    sigmas = np.asarray(sigmas, dtype=float)\n    M = mu.shape[0]\n\n    # Current mark-to-model portfolio value\n    V0 = np.exp(mu).sum()\n\n    # Mean correction to ensure E[exp(eps_j)] = 1\n    m = -0.5 * sigmas**2\n\n    # Draw factors\n    Z_common = rng.standard_normal(N)  # shape (N,)\n    Z_idio = rng.standard_normal((N, M))  # shape (N, M)\n\n    # Build shocks using factor structure\n    # eps_{n,j} = m_j + sigma_j*(sqrt(rho)*Z_common[n] + sqrt(1-rho)*Z_idio[n,j])\n    sqrt_rho = np.sqrt(rho)\n    sqrt_one_minus_rho = np.sqrt(1.0 - rho)\n    # Broadcast shapes: (N,1) for common, (N,M) for idio, (M,) for sigmas and m\n    eps = m + sigmas * (sqrt_rho * Z_common[:, None] + sqrt_one_minus_rho * Z_idio)\n\n    # Simulated future portfolio values V1 for each of N scenarios\n    # Compute exp(mu + eps) then sum across items\n    V1 = np.exp(mu + eps).sum(axis=1)\n\n    # Losses\n    losses = V0 - V1\n\n    # Empirical q-quantile as VaR\n    var_q = float(np.quantile(losses, q, method='linear'))\n    return var_q\n\ndef solve():\n    # Hedonic coefficients beta (K=5): (intercept, size, popularity, oil_indicator, age_in_decades)\n    beta = np.array([10.0, 0.8, 0.15, 0.3, 0.02], dtype=float)\n\n    # Features X for M=5 items (rows)\n    # Columns: [1, size, popularity, oil_indicator, age_decades]\n    X = np.array([\n        [1.0, 0.5, 8.0, 1.0, 5.0],   # Item 1\n        [1.0, 1.2, 5.0, 0.0, 2.0],   # Item 2\n        [1.0, 0.8, 9.0, 1.0, 10.0],  # Item 3\n        [1.0, 2.0, 3.0, 0.0, 1.0],   # Item 4\n        [1.0, 0.3, 6.0, 1.0, 3.0],   # Item 5\n    ], dtype=float)\n\n    mu = hedonic_mu(X, beta)\n\n    # Base seed schedule: seed_i = 20231120 + 1000*i for case index i\n    base_seed = 20231120\n\n    # Define the test cases: (sigmas, rho, q, N)\n    test_cases = [\n        # Case 1: happy path\n        (np.array([0.25, 0.20, 0.30, 0.15, 0.22], dtype=float), 0.0, 0.95, 100000),\n        # Case 2: high correlation\n        (np.array([0.25, 0.20, 0.30, 0.15, 0.22], dtype=float), 0.9, 0.99, 100000),\n        # Case 3: boundary zero volatility\n        (np.array([0.0, 0.0, 0.0, 0.0, 0.0], dtype=float), 0.5, 0.975, 100000),\n        # Case 4: small sample stress\n        (np.array([0.25, 0.20, 0.30, 0.15, 0.22], dtype=float), 0.5, 0.95, 2000),\n    ]\n\n    results = []\n    for i, (sigmas, rho, q, N) in enumerate(test_cases):\n        seed = base_seed + 1000 * i\n        var_est = simulate_var(mu, sigmas, rho, q, N, seed)\n        # Round to two decimals (dollars)\n        var_rounded = round(var_est, 2)\n        results.append(var_rounded)\n\n    # Final print statement in the exact required format: single line, bracketed, comma-separated\n    # Ensure two decimal places in output formatting\n    formatted = \"[\" + \",\".join(f\"{v:.2f}\" for v in results) + \"]\"\n    print(formatted)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2412262"}]}