{"hands_on_practices": [{"introduction": "This first practice introduces a classic exploration-exploitation problem framed within a finite-horizon setting. You will step into the role of a mining company that must decide where to drill over a fixed number of periods to maximize profit. This exercise is designed to build your intuition for solving sequential decision problems using backward induction, a cornerstone of dynamic programming where we find the optimal strategy by first solving the final step and then working our way back to the present [@problem_id:2388581]. The state in this problem is not just a physical location, but the firm's state of knowledge, a powerful concept in modeling economic decisions under uncertainty.", "problem": "A mining company considers a finite-horizon decision problem on a grid-world map with discrete cell types. Each cell is in one of three states: Unknown, Poor Deposit, or Rich Deposit. Time is discrete with a finite horizon of $T$ periods. At each period, the company may drill exactly one cell. Drilling a cell that is Unknown reveals its true type immediately and yields an immediate mining payoff at that period; drilling a cell that is already known to be Poor Deposit or Rich Deposit yields the corresponding payoff at that period. After drilling, the cell’s state persists and is available for drilling again in future periods. The company’s objective is to maximize the expected discounted sum of period payoffs.\n\nFormulation. Let the grid contain $N$ cells. For an Unknown cell, the prior probability that it is Rich Deposit is $ \\theta \\in [0,1]$, and the probability that it is Poor Deposit is $1-\\theta$. Let the per-period payoff from drilling a known Poor Deposit be $r_P \\ge 0$ and from drilling a known Rich Deposit be $r_R \\ge r_P$. Drilling an Unknown cell incurs a per-drill exploration cost $c_U \\ge 0$ in addition to the realized mining payoff. The discount factor is $\\beta \\in [0,1]$. The state of each cell is independent of others conditional on its type, and types do not change over time. The firm drills exactly one cell per period for $T$ periods. The initial state has all cells Unknown.\n\nState, action, and transitions. Define counts $(k_R, k_P, k_U)$ for the number of cells currently known to be Rich Deposit, known to be Poor Deposit, and Unknown, respectively, so that $k_R + k_P + k_U = N$. An action at a state with $(k_R, k_P, k_U)$ is to choose one available category to drill: a Rich Deposit if $k_R > 0$, a Poor Deposit if $k_P > 0$, or an Unknown if $k_U > 0$. Immediate rewards are:\n- If drilling Rich Deposit: $r_R$.\n- If drilling Poor Deposit: $r_P$.\n- If drilling Unknown: with probability $\\theta$ the cell is revealed Rich Deposit and yields $r_R - c_U$ immediately; with probability $1-\\theta$ it is revealed Poor Deposit and yields $r_P - c_U$ immediately.\n\nUpon drilling an Unknown, the next state updates to $(k_R+1, k_P, k_U-1)$ with probability $\\theta$, or $(k_R, k_P+1, k_U-1)$ with probability $1-\\theta$. Upon drilling a known type, the counts remain unchanged.\n\nObjective. Let $V_T(k_R,k_P,k_U)$ denote the optimal expected value with $T$ remaining periods from state $(k_R,k_P,k_U)$. The goal is to compute $V_T(0,0,N)$, the optimal expected total discounted payoff starting with all Unknown. The expected total payoff is the sum over periods $t = 1, \\dots, T$ of $\\beta^{t-1}$ times the immediate payoff at period $t$.\n\nTask. Write a complete, runnable program that, for each parameter set below, computes the optimal expected value $V_T(0,0,N)$ via exact dynamic programming over the count-based state space and returns the results.\n\nTest suite. Use the following parameter sets, each specified as $(N, T, \\theta, r_P, r_R, c_U, \\beta)$, where all probabilities are to be interpreted as decimals (not percentages):\n1. $(3, 3, 0.4, 1.0, 5.0, 0.5, 0.95)$\n2. $(4, 5, 0.5, 0.5, 6.0, 0.0, 0.9)$\n3. $(2, 2, 0.3, 1.0, 4.0, 2.5, 1.0)$\n4. $(1, 3, 0.5, 0.0, 10.0, 1.0, 1.0)$\n5. $(3, 3, 0.7, 1.0, 3.0, 0.2, 0.0)$\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list of floating-point numbers rounded to $6$ decimal places and enclosed in square brackets, in the same order as the test suite (e.g., $[x_1,x_2,x_3,x_4,x_5]$). No other text should be printed. There are no physical units involved in the outputs, and all probabilities must be treated as decimals rather than percentages.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Domain**: A finite-horizon decision problem on a grid-world map.\n- **Cell States**: Unknown, Poor Deposit, Rich Deposit.\n- **Time Horizon**: $T$ discrete periods.\n- **Action**: Drill exactly one cell per period.\n- **Objective**: Maximize the expected discounted sum of period payoffs, which is $\\sum_{t=1}^{T} \\beta^{t-1} \\times (\\text{payoff at period } t)$.\n- **Grid Size**: $N$ cells.\n- **Initial State**: All $N$ cells are Unknown.\n- **Parameters**:\n    - Prior probability of an Unknown cell being Rich: $\\theta \\in [0,1]$.\n    - Payoff from drilling a Poor Deposit: $r_P \\ge 0$.\n    - Payoff from drilling a Rich Deposit: $r_R \\ge r_P$.\n    - Exploration cost for drilling an Unknown cell: $c_U \\ge 0$.\n    - Discount factor: $\\beta \\in [0,1]$.\n- **State Representation**: $(k_R, k_P, k_U)$, representing the counts of Rich, Poor, and Unknown cells, where $k_R + k_P + k_U = N$.\n- **Actions from State $(k_R, k_P, k_U)$**:\n    - Drill Rich Deposit (if $k_R > 0$).\n    - Drill Poor Deposit (if $k_P > 0$).\n    - Drill Unknown (if $k_U > 0$).\n- **Immediate Rewards**:\n    - Drilling Rich: $r_R$.\n    - Drilling Poor: $r_P$.\n    - Drilling Unknown: $r_R - c_U$ with probability $\\theta$, and $r_P - c_U$ with probability $1-\\theta$.\n- **State Transitions**:\n    - Drilling Rich/Poor: State $(k_R, k_P, k_U)$ remains unchanged.\n    - Drilling Unknown: State becomes $(k_R+1, k_P, k_U-1)$ with probability $\\theta$, or $(k_R, k_P+1, k_U-1)$ with probability $1-\\theta$.\n- **Task**: Compute $V_T(0,0,N)$, the optimal expected value from the initial state, using exact dynamic programming.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed based on the specified criteria.\n- **Scientifically Grounded**: The problem is a classic finite-horizon dynamic programming problem, a fundamental model in operations research, reinforcement learning, and computational economics. Its formulation rests on established mathematical principles. It is valid.\n- **Well-Posed**: The problem is well-posed. The state space, though large, is finite and discrete. The action space is finite. The time horizon is finite. The objective function is clearly defined. These conditions guarantee that a unique, stable, and meaningful optimal value exists and can be computed via backward induction.\n- **Objective**: The problem is stated using precise, unambiguous mathematical language. All parameters are defined, and the objective is specified with a clear formula. It is free of subjective or non-scientific claims.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or infeasibility. It is a standard, solvable problem in its domain.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n---\n\nThe problem is a finite-horizon discrete-time dynamic programming problem. The solution is found by computing the value function for each state at each point in time, using the principle of optimality. This is achieved via backward induction.\n\nLet the state be defined by the tuple $(t, k_R, k_P)$, where $t$ is the number of time periods remaining, $k_R$ is the number of cells known to be Rich Deposit, and $k_P$ is the number of cells known to be Poor Deposit. The number of Unknown cells is then implicitly $k_U = N - k_R - k_P$. The value function, $V_t(k_R, k_P)$, represents the maximum expected sum of discounted future payoffs, given the state with $t$ periods remaining. Our goal is to compute $V_T(0, 0)$.\n\nThe dynamic programming recursion is defined by the Bellman equation.\nThe base case is for $t=0$ periods remaining, where no more actions can be taken, so the value is $0$.\n$$V_0(k_R, k_P) = 0 \\quad \\forall k_R, k_P \\text{ such that } k_R+k_P \\le N$$\n\nFor $t > 0$, the value function $V_t(k_R, k_P)$ is the maximum value obtainable over the set of available actions $A(k_R, k_P)$.\n$$V_t(k_R, k_P) = \\max_{a \\in A(k_R, k_P)} \\left\\{ R(k_R, k_P, a) + \\beta \\, \\mathbb{E}[V_{t-1}(k'_R, k'_P) \\mid k_R, k_P, a] \\right\\}$$\nwhere $R(k_R, k_P, a)$ is the immediate expected payoff from taking action $a$ in the current state, and the expectation is over the next state $(k'_R, k'_P)$.\n\nThe possible actions and their corresponding values are:\n$1$. **Drill a Rich Deposit** (available if $k_R > 0$):\nThe immediate payoff is $r_R$. The state does not change. The value of this action is:\n$$v_{\\text{drill_R}} = r_R + \\beta V_{t-1}(k_R, k_P)$$\n\n$2$. **Drill a Poor Deposit** (available if $k_P > 0$):\nThe immediate payoff is $r_P$. The state does not change. The value of this action is:\n$$v_{\\text{drill_P}} = r_P + \\beta V_{t-1}(k_R, k_P)$$\n\n$3$. **Drill an Unknown cell** (available if $k_U = N - k_R - k_P > 0$):\nThe immediate expected payoff is $\\theta (r_R - c_U) + (1-\\theta) (r_P - c_U) = \\theta r_R + (1-\\theta)r_P - c_U$. The next state becomes $(k_R+1, k_P)$ with probability $\\theta$, or $(k_R, k_P+1)$ with probability $1-\\theta$. The value of this action is:\n$$v_{\\text{drill_U}} = (\\theta r_R + (1-\\theta)r_P - c_U) + \\beta \\left( \\theta V_{t-1}(k_R+1, k_P) + (1-\\theta) V_{t-1}(k_R, k_P+1) \\right)$$\n\nThe value function for state $(t, k_R, k_P)$ is the maximum of the values of the available actions. For example, if $k_R > 0$, $k_P > 0$, and $k_U > 0$, then:\n$$V_t(k_R, k_P) = \\max\\{v_{\\text{drill_R}}, v_{\\text{drill_P}}, v_{\\text{drill_U}}\\}$$\n\nThe algorithm proceeds by backward induction:\n$1$. Initialize a table $V$ of dimensions $(T+1) \\times (N+1) \\times (N+1)$ to store the values of $V_t(k_R, k_P)$. Set all $V_0(k_R, k_P) = 0$.\n$2$. Iterate $t$ from $1$ to $T$.\n$3$. For each $t$, iterate over all valid states $(k_R, k_P)$ where $k_R \\in [0, N]$ and $k_P \\in [0, N-k_R]$.\n$4$. For each state, calculate the values of all available actions using the pre-computed values from the $V_{t-1}$ table.\n$5$. Store the maximum of these values in $V_t(k_R, k_P)$.\n$6$. The final result is the value at the initial state: $V_T(0, 0)$.\n\nThis procedure guarantees the computation of the optimal expected total discounted payoff.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_case(N, T, theta, r_P, r_R, c_U, beta):\n    \"\"\"\n    Computes the optimal expected value for a single set of parameters\n    using dynamic programming.\n    \"\"\"\n    # V[t, k_R, k_P] stores the max expected value with t periods remaining,\n    # given k_R known rich cells and k_P known poor cells.\n    V = np.zeros((T + 1, N + 1, N + 1))\n\n    # Iterate forwards in time remaining, from t=1 to T.\n    # This corresponds to backward induction in calendar time.\n    for t in range(1, T + 1):\n        for k_R in range(N + 1):\n            for k_P in range(N - k_R + 1):\n                k_U = N - k_R - k_P\n                \n                # A list to store the value of each possible action\n                action_values = []\n                \n                # Action: Drill a known Rich Deposit (if available)\n                if k_R > 0:\n                    # Immediate payoff is r_R. State does not change.\n                    # Continuation value depends on V[t-1] for the same state.\n                    val_drill_R = r_R + beta * V[t - 1, k_R, k_P]\n                    action_values.append(val_drill_R)\n                \n                # Action: Drill a known Poor Deposit (if available)\n                if k_P > 0:\n                    # Immediate payoff is r_P. State does not change.\n                    val_drill_P = r_P + beta * V[t - 1, k_R, k_P]\n                    action_values.append(val_drill_P)\n                \n                # Action: Drill an Unknown cell (if available)\n                if k_U > 0:\n                    # Expected immediate payoff from exploration\n                    expected_immediate_payoff = theta * r_R + (1 - theta) * r_P - c_U\n                    \n                    # Expected continuation value, averaged over possible outcomes\n                    # Outcome 1: cell is Rich (prob theta), state becomes (k_R+1, k_P)\n                    # Outcome 2: cell is Poor (prob 1-theta), state becomes (k_R, k_P+1)\n                    expected_continuation_value = beta * (\n                        theta * V[t - 1, k_R + 1, k_P] +\n                        (1 - theta) * V[t - 1, k_R, k_P + 1]\n                    )\n                    val_drill_U = expected_immediate_payoff + expected_continuation_value\n                    action_values.append(val_drill_U)\n                \n                # The value of the current state is the maximum over possible actions.\n                # If N>0 and T>0, there is always at least one action.\n                if action_values:\n                    V[t, k_R, k_P] = max(action_values)\n\n    # The result is the value at the initial state: T periods remaining,\n    # 0 known Rich, 0 known Poor.\n    return V[T, 0, 0]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    # Each test case is a tuple: (N, T, theta, r_P, r_R, c_U, beta)\n    test_cases = [\n        (3, 3, 0.4, 1.0, 5.0, 0.5, 0.95),\n        (4, 5, 0.5, 0.5, 6.0, 0.0, 0.9),\n        (2, 2, 0.3, 1.0, 4.0, 2.5, 1.0),\n        (1, 3, 0.5, 0.0, 10.0, 1.0, 1.0),\n        (3, 3, 0.7, 1.0, 3.0, 0.2, 0.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(*case)\n        # Round the result to 6 decimal places as required.\n        results.append(f\"{result:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2388581"}, {"introduction": "Building on the logic of dynamic programming, this exercise shifts our focus to an infinite-horizon problem, a common scenario in economics where decisions have long-lasting consequences without a clear endpoint. You will tackle a canonical inventory management problem, deciding on optimal order quantities to minimize long-term costs. A key feature of this practice is the discretization of a continuous state variable (inventory) into a manageable logarithmic grid, a crucial technique for applying computational methods to real-world problems [@problem_id:2388580]. You will implement the value iteration algorithm to find the optimal stationary policy that holds for all time.", "problem": "A single-product firm faces random demand in discrete time with an infinite horizon. Inventory is nonnegative and measured in units. The firm chooses a discrete order quantity each period before demand is realized. Unmet demand is lost (no backlogging). The state space for stock is discretized logarithmically, not linearly.\n\nThe model is defined as follows.\n\n- State space. The inventory state $s$ belongs to a finite grid $\\mathcal{S}$ constructed as the union of $\\{0\\}$ and a set of strictly positive points that are logarithmically spaced and rounded to integers. Given strictly positive integers $s_{\\min}^+$ and $s_{\\max}$ with $s_{\\min}^+ \\le s_{\\max}$, and a positive integer $N_+$ with $N_+ \\ge 2$, define\n$$\n\\tilde{s}_i = \\exp\\!\\left(\\log(s_{\\min}^+) + (i-1)\\frac{\\log(s_{\\max}) - \\log(s_{\\min}^+)}{N_+ - 1}\\right), \\quad i \\in \\{1,\\dots,N_+\\},\n$$\nthen round to integers $r_i = \\operatorname{round}(\\tilde{s}_i)$, drop duplicates, and set $\\mathcal{S} = \\{0\\} \\cup \\{r_i\\}_{i=1}^{N_+}$ sorted in ascending order. The grid $\\mathcal{S}$ contains $0$ and strictly positive integers up to (and including) $s_{\\max}$.\n\n- Action space. The firm chooses an order quantity $a$ from the discrete set $\\mathcal{A} = \\{0,1,2,\\dots,A_{\\max}\\}$, where $A_{\\max}$ is a given positive integer.\n\n- Demand. In each period, demand $D$ is a discrete random variable supported on a finite set $\\{d_1,\\dots,d_K\\}$ of nonnegative integers with associated probabilities $\\{p_1,\\dots,p_K\\}$ satisfying $\\sum_{k=1}^K p_k = 1$.\n\n- Timing and transition. Given current stock $s \\in \\mathcal{S}$ and action $a \\in \\mathcal{A}$, demand $D$ is realized. Sales are $\\min\\{s+a, D\\}$. End-of-period stock before discretization is\n$$\ns'_{\\text{cont}} = \\max\\{s + a - D, 0\\}.\n$$\nThe next state is obtained by projecting $s'_{\\text{cont}}$ onto the grid $\\mathcal{S}$ using the nearest-neighbor projection\n$$\n\\Pi(s'_{\\text{cont}}) = \\operatorname*{arg\\,min}_{g \\in \\mathcal{S}} |g - s'_{\\text{cont}}|,\n$$\nwith ties broken toward the lower grid point.\n\n- Per-period cost. The instantaneous cost is\n$$\ng(s,a,D) = K \\cdot \\mathbf{1}\\{a > 0\\} + c\\,a + h\\,s'_{\\text{cont}} + p\\,\\max\\{D - (s+a), 0\\},\n$$\nwhere $K$ is a fixed order cost, $c$ is a per-unit order cost, $h$ is a per-unit holding cost on end-of-period stock $s'_{\\text{cont}}$, and $p$ is a per-unit lost-sales penalty.\n\n- Objective. For discount factor $\\beta \\in (0,1)$, the firm seeks a stationary policy that minimizes the expected infinite-horizon discounted cost $\\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\beta^t g(s_t,a_t,D_t)\\right]$ over all measurable policies that choose $a_t \\in \\mathcal{A}$ as a function of $s_t \\in \\mathcal{S}$.\n\nYour task is to implement a program that, for each parameter set in the test suite below, computes the optimal stationary action at the initial stock $s_0 = 0$ under the exact discrete model defined above (including the projection $\\Pi(\\cdot)$). The required outputs are the optimal actions as integers.\n\nTest suite. For each test case, use the given parameters exactly. All quantities specified as numbers are dimensionless.\n\n- Test case $1$ (general case):\n  - Discount factor $\\beta = 0.95$.\n  - Logarithmic grid parameters $s_{\\min}^+ = 1$, $s_{\\max} = 50$, $N_+ = 10$.\n  - Action bound $A_{\\max} = 20$.\n  - Costs $K = 5$, $c = 1$, $h = 0.1$, $p = 2$.\n  - Demand support and probabilities: $\\{0,1,2,3,4\\}$ with probabilities $\\{0.1,0.2,0.3,0.25,0.15\\}$.\n\n- Test case $2$ (no fixed order cost, higher penalties):\n  - Discount factor $\\beta = 0.95$.\n  - Logarithmic grid parameters $s_{\\min}^+ = 1$, $s_{\\max} = 30$, $N_+ = 8$.\n  - Action bound $A_{\\max} = 15$.\n  - Costs $K = 0$, $c = 1$, $h = 0.5$, $p = 5$.\n  - Demand support and probabilities: $\\{0,2,5,8\\}$ with probabilities $\\{0.2,0.3,0.3,0.2\\}$.\n\n- Test case $3$ (zero demand edge case):\n  - Discount factor $\\beta = 0.9$.\n  - Logarithmic grid parameters $s_{\\min}^+ = 1$, $s_{\\max} = 20$, $N_+ = 6$.\n  - Action bound $A_{\\max} = 10$.\n  - Costs $K = 3$, $c = 1$, $h = 0.2$, $p = 4$.\n  - Demand support and probabilities: $\\{0\\}$ with probabilities $\\{1.0\\}$.\n\n- Test case $4$ (deterministic high demand, strong lost-sales penalty):\n  - Discount factor $\\beta = 0.9$.\n  - Logarithmic grid parameters $s_{\\min}^+ = 1$, $s_{\\max} = 40$, $N_+ = 12$.\n  - Action bound $A_{\\max} = 20$.\n  - Costs $K = 1$, $c = 1$, $h = 0.1$, $p = 10$.\n  - Demand support and probabilities: $\\{10\\}$ with probabilities $\\{1.0\\}$.\n\nFinal output format. Your program should produce a single line of output containing the optimal actions for the initial stock $s_0 = 0$ for test cases $1$ through $4$ in order, formatted as a comma-separated list enclosed in square brackets (for example, `[a_1,a_2,a_3,a_4]`). The required outputs are integers.", "solution": "The problem is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n\n- **State Space $\\mathcal{S}$**: A finite grid of inventory levels. It is constructed from a primary set of $N_+ \\ge 2$ strictly positive integers, $\\{r_i\\}_{i=1}^{N_+}$, and the state $\\{0\\}$. The points $r_i$ are integer-rounded values of logarithmically spaced points $\\tilde{s}_i$ between $s_{\\min}^+ > 0$ and $s_{\\max} \\ge s_{\\min}^+$. The formula for $\\tilde{s}_i$ for $i \\in \\{1,\\dots,N_+\\}$ is:\n$$\n\\tilde{s}_i = \\exp\\!\\left(\\log(s_{\\min}^+) + (i-1)\\frac{\\log(s_{\\max}) - \\log(s_{\\min}^+)}{N_+ - 1}\\right)\n$$\nThe positive part of the grid is $\\{r_i = \\operatorname{round}(\\tilde{s}_i)\\}_{i=1}^{N_+}$ with duplicates removed. The full state space is $\\mathcal{S} = \\{0\\} \\cup \\{r_i\\}$, sorted in ascending order.\n\n- **Action Space $\\mathcal{A}$**: The set of possible order quantities, given by $\\mathcal{A} = \\{0,1,2,\\dots,A_{\\max}\\}$ for a given positive integer $A_{\\max}$.\n\n- **Demand $D$**: A discrete random variable with finite support $\\{d_1,\\dots,d_K\\}$ and corresponding probabilities $\\{p_1,\\dots,p_K\\}$ where $\\sum_{k=1}^K p_k = 1$.\n\n- **State Transition**: Given state $s \\in \\mathcal{S}$ and action $a \\in \\mathcal{A}$, the end-of-period stock level before projection is $s'_{\\text{cont}} = \\max\\{s + a - D, 0\\}$. The next state $s' \\in \\mathcal{S}$ is determined by projecting $s'_{\\text{cont}}$ onto the grid $\\mathcal{S}$ using nearest-neighbor projection:\n$$\ns' = \\Pi(s'_{\\text{cont}}) = \\operatorname*{arg\\,min}_{g \\in \\mathcal{S}} |g - s'_{\\text{cont}}|\n$$\nTies are broken by selecting the smaller grid point.\n\n- **Per-Period Cost $g(s,a,D)$**: The cost incurred in a single period is:\n$$\ng(s,a,D) = K \\cdot \\mathbf{1}\\{a > 0\\} + c\\,a + h\\,s'_{\\text{cont}} + p\\,\\max\\{D - (s+a), 0\\}\n$$\nwhere $K$ is the fixed order cost, $c$ is the per-unit order cost, $h$ is the per-unit holding cost, and $p$ is the per-unit lost-sales penalty.\n\n- **Objective**: Minimize the infinite-horizon expected discounted cost $\\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\beta^t g(s_t,a_t,D_t)\\right]$ with a discount factor $\\beta \\in (0,1)$. The goal is to find a stationary optimal policy $\\pi^*: \\mathcal{S} \\to \\mathcal{A}$.\n\n- **Task**: For four distinct parameter sets (test cases), determine the optimal stationary action $a^*$ for the initial state $s_0=0$.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is a standard formulation of a single-item inventory control problem under uncertainty, which is a canonical topic in operations research and computational economics. The use of a discrete state space (specifically, a logarithmic grid) and discrete actions is a common and valid technique for making continuous-state problems computationally tractable. The model is based on well-established principles of dynamic programming. It is scientifically sound.\n\n- **Well-Posed**: The problem is a finite-state, finite-action, infinite-horizon discounted dynamic program. The per-period costs $g(s,a,D)$ are non-negative for non-negative parameters, thus they are bounded below. The discount factor $\\beta$ is strictly between $0$ and $1$. Under these conditions, the Bellman operator is a contraction mapping. Standard theorems in dynamic programming (e.g., from Blackwell) guarantee the existence of a unique bounded value function that is the fixed point of the Bellman operator, and a stationary optimal policy exists. Algorithms such as value iteration are guaranteed to converge to this unique solution. The problem is well-posed.\n\n- **Objective**: All variables, parameters, and functions are defined with mathematical precision. The language is formal and unambiguous. The problem is objective.\n\n### Step 3: Verdict and Action\n\nThe problem statement is valid. It is scientifically grounded, well-posed, and objective, with a complete and consistent setup. A solution will be provided.\n\n### Solution\n\nThe problem described is an infinite-horizon discounted cost stochastic dynamic program. The optimal policy is found by solving the Bellman equation, which characterizes the minimal expected cost, or value function $V(s)$, for each state $s \\in \\mathcal{S}$. The value function must satisfy:\n$$\nV(s) = \\min_{a \\in \\mathcal{A}} \\left\\{ \\mathbb{E}_D[g(s,a,D)] + \\beta \\mathbb{E}_D[V(\\Pi(\\max\\{s+a-D, 0\\}))] \\right\\}\n$$\nThis equation states that the value of being in state $s$ is the minimum over all possible actions $a$ of the sum of the immediate expected cost and the discounted expected future cost. The expectation $\\mathbb{E}_D[\\cdot]$ is taken over the distribution of demand $D$.\n\nLet us define the state-action value function, or $Q$-function, as the term inside the minimization:\n$$\nQ(s,a) = \\mathbb{E}_D[g(s,a,D)] + \\beta \\mathbb{E}_D[V(\\Pi(\\max\\{s+a-D, 0\\}))]\n$$\nThe Bellman equation can be written as $V(s) = \\min_{a \\in \\mathcal{A}} Q(s,a)$.\n\nThe problem is solved using the **Value Iteration** algorithm, which is guaranteed to converge to the optimal value function $V^*$ because the Bellman operator is a contraction mapping for $\\beta \\in (0,1)$. The algorithm proceeds as follows:\n\n1.  **Initialization**:\n    - Construct the discrete state space $\\mathcal{S}$ according to the specified logarithmic grid generation procedure. Let $N_s = |\\mathcal{S}|$.\n    - Initialize the value function vector $V_0 \\in \\mathbb{R}^{N_s}$, typically with $V_0(s) = 0$ for all $s \\in \\mathcal{S}$.\n    - Set an iteration counter $n=0$ and a small convergence tolerance $\\epsilon > 0$.\n\n2.  **Iteration**: Repeat for $n=0, 1, 2, \\dots$:\n    - For each state $s \\in \\mathcal{S}$, compute the updated value $V_{n+1}(s)$ by solving the one-step Bellman equation using the value function from the previous iteration, $V_n$:\n    $$\n    V_{n+1}(s) = \\min_{a \\in \\mathcal{A}} \\left\\{ \\mathbb{E}_D[g(s,a,D)] + \\beta \\sum_{k=1}^K p_k V_n\\left(\\Pi(\\max\\{s+a-d_k, 0\\})\\right) \\right\\}\n    $$\n    The expected immediate cost for a state-action pair $(s,a)$ is:\n    $$\n    \\mathbb{E}_D[g(s,a,D)] = K \\cdot \\mathbf{1}\\{a > 0\\} + c\\,a + \\sum_{k=1}^K p_k \\left( h\\,\\max\\{s+a-d_k, 0\\} + p\\,\\max\\{d_k-(s+a), 0\\} \\right)\n    $$\n    - The stationary policy $\\pi_n(s)$ at iteration $n$ is the action $a$ that achieves the minimum for each state $s$.\n\n3.  **Termination**: The iteration stops when the value function has converged, which is determined by checking if the supremum norm of the difference between successive value functions is smaller than the tolerance:\n    $$\n    \\max_{s \\in \\mathcal{S}} |V_{n+1}(s) - V_n(s)| < \\epsilon\n    $$\n    A sufficiently small tolerance, e.g., $\\epsilon=10^{-8}$, ensures the resulting policy is optimal.\n\n4.  **Result**: Upon convergence, the algorithm yields the optimal value function $V^*$ and the optimal stationary policy $\\pi^*$. The problem asks for the optimal action at the specific initial state $s_0=0$, which is $\\pi^*(0)$.\n\nThe computational implementation involves the following steps for each test case:\n\na.  A function generates the sorted state grid $\\mathcal{S}$ and a mapping from state values to their indices for efficient lookup.\nb.  A projection function $\\Pi(x)$ is implemented. Given a value $x$ and the sorted grid $\\mathcal{S}$, it finds the index of the closest grid point using `numpy.argmin(numpy.abs(S - x))`. The tie-breaking rule (lower value) is automatically handled by `argmin` returning the first index of a minimum.\nc.  The main value iteration loop is executed. Within this loop, nested loops iterate through each state $s \\in \\mathcal{S}$ and each action $a \\in \\mathcal{A}$. For each $(s,a)$ pair, a further loop over the demand outcomes $\\{d_k\\}$ calculates the full $Q(s,a)$ value.\nd.  After finding the $Q$-values for all actions for a given state, the minimum value is stored as $V_{n+1}(s)$ and the corresponding action is stored as the current best policy for that state.\ne.  After iterating over all states, the convergence criterion is checked. If not met, $V_n$ is updated to $V_{n+1}$ and the process repeats.\nf.  Once convergence is achieved, the optimal action for state $s=0$ is extracted from the final policy array. This process is repeated for all test cases provided.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the inventory management problem for all test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"beta\": 0.95, \"s_min_plus\": 1, \"s_max\": 50, \"N_plus\": 10,\n            \"A_max\": 20, \"K\": 5, \"c\": 1, \"h\": 0.1, \"p\": 2,\n            \"demand_support\": [0, 1, 2, 3, 4],\n            \"demand_probs\": [0.1, 0.2, 0.3, 0.25, 0.15]\n        },\n        {\n            \"beta\": 0.95, \"s_min_plus\": 1, \"s_max\": 30, \"N_plus\": 8,\n            \"A_max\": 15, \"K\": 0, \"c\": 1, \"h\": 0.5, \"p\": 5,\n            \"demand_support\": [0, 2, 5, 8],\n            \"demand_probs\": [0.2, 0.3, 0.3, 0.2]\n        },\n        {\n            \"beta\": 0.9, \"s_min_plus\": 1, \"s_max\": 20, \"N_plus\": 6,\n            \"A_max\": 10, \"K\": 3, \"c\": 1, \"h\": 0.2, \"p\": 4,\n            \"demand_support\": [0], \"demand_probs\": [1.0]\n        },\n        {\n            \"beta\": 0.9, \"s_min_plus\": 1, \"s_max\": 40, \"N_plus\": 12,\n            \"A_max\": 20, \"K\": 1, \"c\": 1, \"h\": 0.1, \"p\": 10,\n            \"demand_support\": [10], \"demand_probs\": [1.0]\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        optimal_action = compute_optimal_policy(**params)\n        results.append(optimal_action)\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\ndef create_state_grid(s_min_plus, s_max, N_plus):\n    \"\"\"\n    Generates the logarithmically spaced state grid.\n    \"\"\"\n    if s_min_plus > s_max:\n        raise ValueError(\"s_min_plus must be less than or equal to s_max\")\n    if N_plus < 2:\n        raise ValueError(\"N_plus must be 2 or greater\")\n\n    log_s_min = np.log(s_min_plus)\n    log_s_max = np.log(s_max)\n    \n    # Generate log-spaced points\n    grid_points = np.exp(log_s_min + (np.arange(N_plus)) * (log_s_max - log_s_min) / (N_plus - 1))\n    \n    # Round to nearest integer\n    rounded_points = np.round(grid_points).astype(int)\n    \n    # Remove duplicates and form the final grid with 0\n    positive_grid = sorted(list(set(rounded_points)))\n    \n    final_grid = np.array([0] + positive_grid)\n    \n    return final_grid\n\ndef compute_optimal_policy(beta, s_min_plus, s_max, N_plus, A_max, K, c, h, p, demand_support, demand_probs):\n    \"\"\"\n    Computes the optimal policy for a given set of parameters using value iteration.\n    \"\"\"\n    \n    # 1. Setup\n    S = create_state_grid(s_min_plus, s_max, N_plus)\n    s_to_idx = {val: i for i, val in enumerate(S)}\n    \n    A = np.arange(A_max + 1)\n    D = np.array(demand_support)\n    P_d = np.array(demand_probs)\n    \n    V = np.zeros(len(S))\n    policy = np.zeros(len(S), dtype=int)\n    tol = 1e-8\n    max_iter = 5000\n\n    def project(x_cont, grid):\n        \"\"\" Projects a continuous value onto the discrete grid S. \"\"\"\n        idx = np.argmin(np.abs(grid - x_cont))\n        return grid[idx]\n\n    # Pre-calculate components of one-period cost that depend only on (s,a,D)\n    # This can be vectorized for efficiency.\n    s_plus_a_tensor = S[:, np.newaxis] + A[np.newaxis, :] # Shape (|S|, |A|)\n    s_cont_prime_tensor = np.maximum(s_plus_a_tensor[:, :, np.newaxis] - D[np.newaxis, np.newaxis, :], 0)\n    \n    holding_costs_exp = h * np.sum(P_d * s_cont_prime_tensor, axis=2)\n    penalty_costs_exp = p * np.sum(P_d * np.maximum(D - s_plus_a_tensor[:, :, np.newaxis], 0), axis=2)\n    \n    fixed_costs = K * (A > 0)\n    unit_costs = c * A\n    \n    E_g = fixed_costs[np.newaxis, :] + unit_costs[np.newaxis, :] + holding_costs_exp + penalty_costs_exp\n    \n    # 2. Value Iteration\n    for _ in range(max_iter):\n        V_new = np.copy(V)\n        \n        # Calculate expected future value term\n        E_V = np.zeros((len(S), len(A)))\n        for s_idx, s in enumerate(S):\n            for a_idx, a in enumerate(A):\n                future_value = 0\n                for d_val, p_val in zip(D, P_d):\n                    s_cont_prime = max(s + a - d_val, 0)\n                    s_prime = project(s_cont_prime, S)\n                    s_prime_idx = s_to_idx[s_prime]\n                    future_value += p_val * V[s_prime_idx]\n                E_V[s_idx, a_idx] = future_value\n\n        Q_values = E_g + beta * E_V\n        \n        V_new = np.min(Q_values, axis=1)\n        policy = A[np.argmin(Q_values, axis=1)]\n\n        # 3. Check convergence\n        if np.max(np.abs(V - V_new)) < tol:\n            break\n        \n        V = V_new\n\n    # 4. Return optimal action for s=0\n    return policy[0]\n\nif __name__ == '__main__':\n    solve()\n```", "id": "2388580"}, {"introduction": "Our final practice takes you from a world of known models to one of learning and adaptation, which lies at the heart of modern computational finance. Here, the \"rules of the game\"—the precise probabilities and rewards of the market—are unknown. You will build a trading agent from the ground up using Q-learning, a fundamental model-free Reinforcement Learning (RL) algorithm [@problem_id:2388619]. By interacting with a simulated market environment, your agent will learn a policy to trade a stock based on a discrete representation of a technical indicator, demonstrating how optimal strategies can be discovered through trial-and-error experience alone.", "problem": "You are asked to implement a complete, runnable program that trains a tabular Reinforcement Learning (RL) agent using discrete state and action spaces to trade a single stock based solely on the Relative Strength Index (RSI) technical indicator. The program must follow a Markov Decision Process (MDP) formalization and optimize behavior using first principles derived from the Bellman optimality equation, without shortcut formulas being given in this statement.\n\nThe environment is defined as follows. The agent observes a state that is a pair composed of the RSI regime and its current position. The RSI regime is computed from the price series as a function of a lookback window $w$ using the standard definition of average gains and average losses over the most recent $w$ one-period price changes:\n- Let the price series be $\\{P_t\\}_{t=0}^{T-1}$ with $T \\geq w + 2$, and define one-step price changes $\\Delta_t = P_t - P_{t-1}$ for $t \\in \\{1, \\dots, T-1\\}$.\n- For each time $t$ with $t \\geq w$, define the average gain as the arithmetic mean of $\\max(\\Delta_k, 0)$ for $k \\in \\{t-w+1, \\dots, t\\}$, and the average loss as the arithmetic mean of $\\max(-\\Delta_k, 0)$ over the same window.\n- Define the Relative Strength as $RS_t = \\dfrac{\\text{average gain}}{\\text{average loss}}$ with the following conventions to ensure mathematical well-definedness: if the average loss is $0$ and the average gain is strictly positive, set $RS_t = +\\infty$, which implies the Relative Strength Index (RSI) $RSI_t = 100$; if the average gain is $0$ and the average loss is strictly positive, set $RSI_t = 0$; if both averages are $0$, set $RSI_t = 50$. Otherwise, when both averages are positive, set $RSI_t = 100 - \\dfrac{100}{1 + RS_t}$.\n- Discretize the RSI into regimes using thresholds $30$ and $70$ as follows: Oversold if $RSI_t \\leq 30$, Neutral if $30 < RSI_t < 70$, Overbought if $RSI_t \\geq 70$.\n\nThe full discrete state at time $t$ is $s_t = (r_t, p_t)$, where $r_t \\in \\{\\text{Oversold}, \\text{Neutral}, \\text{Overbought}\\}$ is the RSI regime and $p_t \\in \\{0,1\\}$ is the current position ($0$ for flat, $1$ for long one unit). The discrete action space is $A = \\{\\text{Hold}, \\text{Buy}, \\text{Sell}\\}$.\n\nThe within-period dynamics are:\n- At each time $t$ with $t \\in \\{w, \\dots, T-2\\}$, the agent observes $s_t$, chooses an action $a_t \\in A$, and its position updates deterministically:\n  - If $a_t = \\text{Buy}$, then $p_{t+} = 1$.\n  - If $a_t = \\text{Sell}$, then $p_{t+} = 0$.\n  - If $a_t = \\text{Hold}$, then $p_{t+} = p_t$.\n- A transaction cost $c \\geq 0$ is incurred only if $p_{t+} \\neq p_t$ (that is, only when the position actually changes).\n- The one-step reward is $r_t = p_{t+}\\cdot (P_{t+1} - P_t) - \\mathbf{1}\\{p_{t+} \\neq p_t\\}\\, c$.\n- The next state $s_{t+1}$ uses the next RSI regime and the updated position $p_{t+}$.\n\nEpisodes are defined as a single left-to-right pass over the time indices $t \\in \\{w, \\dots, T-2\\}$ starting with $p_w = 0$. The learning agent must implement tabular $Q$-learning derived from the Bellman optimality principle and sample backups. You must use an $\\epsilon$-greedy behavior policy with uniform random exploration over the three actions when exploring, and greedy action selection by choosing any action that maximizes the current action-value estimate when exploiting. Break ties deterministically by choosing the action with the smallest index. Use a fixed pseudorandom seed $12345$ for all randomization so that results are reproducible.\n\nAfter training for the specified number of episodes, evaluate the greedy policy (set $\\epsilon = 0$ during evaluation) on the same price path used for training, starting from $p_w = 0$, using the same reward construction for $t \\in \\{w, \\dots, T-2\\}$. At the end of evaluation, if the final position is $1$, force liquidation by selling and subtract a single transaction cost $c$; there is no additional price change at liquidation time. The evaluation metric for a test case is the final accumulated wealth, which equals the sum of rewards during evaluation minus the possible forced liquidation cost at the end. There are no physical units involved.\n\nYour program must implement the above and run on the following test suite. For each test case, you are given the price path $\\{P_t\\}$, the window length $w$, the learning rate $\\alpha \\in (0,1]$, the discount factor $\\gamma \\in [0,1]$, the exploration rate $\\epsilon \\in [0,1]$, the number of training episodes $E \\in \\mathbb{N}$, and the transaction cost $c \\geq 0$.\n\nTest suite:\n- Case $1$ (trend with small dips, happy path): prices $[100, 101, 102, 101, 103, 105, 104, 106, 108, 110]$, $w = 3$, $\\alpha = 0.3$, $\\gamma = 0.9$, $\\epsilon = 0.1$, $E = 200$, $c = 0.05$.\n- Case $2$ (sideways and noisy, boundary $\\gamma = 0$): prices $[100.0, 100.5, 99.8, 100.2, 99.7, 100.1, 99.9, 100.0, 99.8, 100.2]$, $w = 3$, $\\alpha = 0.5$, $\\gamma = 0.0$, $\\epsilon = 0.2$, $E = 300$, $c = 0.05$.\n- Case $3$ (greedy from start, no exploration): prices $[10, 11, 12, 13, 14, 15]$, $w = 2$, $\\alpha = 0.3$, $\\gamma = 0.9$, $\\epsilon = 0.0$, $E = 50$, $c = 0.1$.\n- Case $4$ (edge RSI regimes including $0$ and $100$): prices $[50, 49, 48, 47, 46, 47, 48, 49, 50, 49]$, $w = 3$, $\\alpha = 0.4$, $\\gamma = 0.8$, $\\epsilon = 0.15$, $E = 250$, $c = 0.05$.\n\nImplementation requirements:\n- The state space is discrete with exactly $6$ states formed by the Cartesian product of $3$ RSI regimes and $2$ position states.\n- The action space is discrete with exactly $3$ actions ordered as $\\{\\text{Hold}, \\text{Buy}, \\text{Sell}\\}$ and indexed as $\\{0, 1, 2\\}$.\n- Initialize all action-values $Q(s,a)$ to $0$.\n- Use the same fixed pseudorandom seed $12345$ for all stochasticity.\n- The evaluation produces the final wealth for each of the $4$ cases as described.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[x_1,x_2,x_3,x_4]$.\n- Each $x_i$ must be a floating-point number rounded to exactly $6$ decimal places.\n\nYour task: implement the program exactly as specified so that it runs these $4$ cases in order and prints the single line described above. No user input is required, and no external files are allowed. The implementation should be self-contained and purely algorithmic. Ensure that all randomization uses the fixed seed $12345$ so that results are reproducible.", "solution": "The problem presented is a well-posed application of Reinforcement Learning to a financial trading scenario. It is scientifically grounded, internally consistent, and specified with sufficient detail to permit a unique algorithmic solution. We shall therefore proceed with its formal analysis and implementation.\n\nThe problem is to train an agent to trade a single stock using a policy derived from the $Q$-learning algorithm. The environment is modeled as a finite-state Markov Decision Process (MDP), which is appropriate given the discrete nature of the states and actions. An MDP is formally defined by a tuple $(S, A, P, R, \\gamma)$, where:\n- $S$ is the set of states.\n- $A$ is the set of actions.\n- $P$ is the state transition probability function, $P(s'|s, a) = \\text{Pr}(S_{t+1}=s' | S_t=s, A_t=a)$.\n- $R$ is the reward function, $R(s, a, s') = \\mathbb{E}[R_{t+1} | S_t=s, A_t=a, S_{t+1}=s']$.\n- $\\gamma \\in [0, 1]$ is the discount factor for future rewards.\n\nIn this specific problem, these components are defined as follows:\n\nThe state space $S$ is the Cartesian product of the set of Relative Strength Index (RSI) regimes and the set of possible positions. Let the RSI regimes be $R_{reg} = \\{\\text{Oversold}, \\text{Neutral}, \\text{Overbought}\\}$ and the positions be $P_{pos} = \\{0, 1\\}$. The state at time $t$ is $s_t = (r_t, p_t) \\in R_{reg} \\times P_{pos}$. This results in $|S| = 3 \\times 2 = 6$ distinct states.\n\nThe action space is $A = \\{\\text{Hold}, \\text{Buy}, \\text{Sell}\\}$, a set of $3$ discrete actions.\n\nThe state transitions are governed by the agent's actions and an exogenous price series $\\{P_t\\}_{t=0}^{T-1}$. Given a state $s_t = (r_t, p_t)$ and an action $a_t$, the agent's position transitions deterministically to a new position $p_{t+}$. The next RSI regime, $r_{t+1}$, is determined by the price series up to time $t+1$. Thus, the next state is $s_{t+1} = (r_{t+1}, p_{t+})$. Since the price series is fixed, the transition dynamics are deterministic for any given action.\n\nThe reward function at time $t$, for taking action $a_t$ from state $s_t=(r_t, p_t)$, is given by the profit or loss from the change in price, adjusted for transaction costs. The position after the action, $p_{t+}$, determines the exposure to the price change $P_{t+1} - P_t$. The reward $r_t$ is:\n$$r_t = p_{t+} \\cdot (P_{t+1} - P_t) - \\mathbf{1}\\{p_{t+} \\neq p_t\\} \\cdot c$$\nwhere $c$ is the transaction cost and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n\nThe objective of the agent is to learn an optimal policy $\\pi^*: S \\to A$ that maximizes the expected cumulative discounted reward from any given state. This is achieved by learning the optimal action-value function, $Q^*(s, a)$, which represents the maximum expected discounted future reward obtainable by taking action $a$ in state $s$ and acting optimally thereafter. The $Q^*$ function satisfies the Bellman optimality equation:\n$$Q^*(s, a) = \\mathbb{E} \\left[ R_{t+1} + \\gamma \\max_{a' \\in A} Q^*(S_{t+1}, a') \\mid S_t=s, A_t=a \\right]$$\nFor this problem's deterministic transitions (given the price path), the expectation operator is redundant. For a transition from $s_t$ to $s_{t+1}$ resulting from action $a_t$, with immediate reward $r_t$, the Bellman equation simplifies to:\n$$Q^*(s_t, a_t) = r_t + \\gamma \\max_{a' \\in A} Q^*(s_{t+1}, a')$$\n$Q$-learning is a model-free temporal difference (TD) control algorithm that iteratively approximates $Q^*(s, a)$. The agent does not need to know the transition or reward functions a priori. It learns from sample transitions $(s_t, a_t, r_t, s_{t+1})$. After each such transition, the entry $Q(s_t, a_t)$ in the action-value table is updated according to the rule:\n$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left( r_t + \\gamma \\max_{a' \\in A} Q(s_{t+1}, a') - Q(s_t, a_t) \\right)$$\nHere, $\\alpha \\in (0, 1]$ is the learning rate, which controls the step size of the update. The term $r_t + \\gamma \\max_{a'} Q(s_{t+1}, a')$ is the TD target, which serves as a sample-based estimate of the right-hand side of the Bellman equation.\n\nTo ensure sufficient exploration of the state-action space, a behavior policy must be chosen that allows for non-greedy actions. The specified $\\epsilon$-greedy policy accomplishes this by selecting a random action with probability $\\epsilon$ and the greedy action (the one that maximizes the current $Q(s, \\cdot)$) with probability $1-\\epsilon$.\n\nThe implementation will proceed as follows:\n$1$. The RSI values for the given price series will be pre-calculated according to the specified formula, including the conventions for zero-valued average gains or losses. These continuous RSI values will then be mapped to the three discrete regimes.\n$2$. The state space and action space will be mapped to integer indices for efficient use with a NumPy array, which will represent the $Q$-table, initialized to all zeros. Specifically, a state $(r, p)$ where $r \\in \\{0, 1, 2\\}$ and $p \\in \\{0, 1\\}$ will be mapped to an index $i_s = r \\cdot 2 + p$. Actions will be indexed $0, 1, 2$. The $Q$-table will be of size $6 \\times 3$.\n$3$. The training process will iterate for a specified number of episodes, $E$. Each episode consists of a single pass over the valid time steps of the price series, from $t=w$ to $t=T-2$.\n$4$. Within each episode, the agent begins with zero position. At each time step $t$, it observes the state $s_t$, selects an action $a_t$ via the $\\epsilon$-greedy policy, observes the reward $r_t$ and the next state $s_{t+1}$, and updates the $Q$-table using the $Q$-learning rule.\n$5$. After training is complete, the learned policy is evaluated. The evaluation follows the same trajectory through time but with $\\epsilon=0$, meaning the agent always acts greedily with respect to the learned $Q$-values. The total wealth is accumulated over this pass. A final liquidation cost is applied if the agent holds a position at the end of the evaluation period. The final accumulated wealth is the performance metric.\n\nAll randomization is controlled by a fixed seed to ensure reproducibility, as is mandatory for scientific validation.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and runs the Q-learning trading agent problem for the given test suite.\n    \"\"\"\n    test_cases = [\n        {\n            \"prices\": [100.0, 101.0, 102.0, 101.0, 103.0, 105.0, 104.0, 106.0, 108.0, 110.0],\n            \"w\": 3, \"alpha\": 0.3, \"gamma\": 0.9, \"epsilon\": 0.1, \"E\": 200, \"c\": 0.05\n        },\n        {\n            \"prices\": [100.0, 100.5, 99.8, 100.2, 99.7, 100.1, 99.9, 100.0, 99.8, 100.2],\n            \"w\": 3, \"alpha\": 0.5, \"gamma\": 0.0, \"epsilon\": 0.2, \"E\": 300, \"c\": 0.05\n        },\n        {\n            \"prices\": [10.0, 11.0, 12.0, 13.0, 14.0, 15.0],\n            \"w\": 2, \"alpha\": 0.3, \"gamma\": 0.9, \"epsilon\": 0.0, \"E\": 50, \"c\": 0.1\n        },\n        {\n            \"prices\": [50.0, 49.0, 48.0, 47.0, 46.0, 47.0, 48.0, 49.0, 50.0, 49.0],\n            \"w\": 3, \"alpha\": 0.4, \"gamma\": 0.8, \"epsilon\": 0.15, \"E\": 250, \"c\": 0.05\n        }\n    ]\n\n    results = []\n    \n    # --- Mappings ---\n    # RSI Regimes: {0: Oversold, 1: Neutral, 2: Overbought}\n    # Positions: {0: Flat, 1: Long}\n    # States: (rsi_regime, position_state) -> rsi_regime * 2 + position_state\n    # Actions: {0: Hold, 1: Buy, 2: Sell}\n    NUM_STATES = 6\n    NUM_ACTIONS = 3\n    \n    for case in test_cases:\n        prices_np = np.array(case[\"prices\"], dtype=np.float64)\n        w = case[\"w\"]\n        alpha = case[\"alpha\"]\n        gamma = case[\"gamma\"]\n        epsilon = case[\"epsilon\"]\n        E = case[\"E\"]\n        c = case[\"c\"]\n        T = len(prices_np)\n\n        # --- 1. Pre-calculate RSI and Regimes ---\n        rsi_values = np.full(T, np.nan)\n        deltas = prices_np[1:] - prices_np[:-1]\n        \n        for t in range(w, T):\n            window_deltas = deltas[t-w:t]\n            gains = np.maximum(window_deltas, 0)\n            losses = np.maximum(-window_deltas, 0)\n            \n            avg_gain = np.mean(gains)\n            avg_loss = np.mean(losses)\n\n            if avg_loss == 0:\n                if avg_gain == 0:\n                    rsi_values[t] = 50.0\n                else:\n                    rsi_values[t] = 100.0\n            else:\n                rs = avg_gain / avg_loss\n                rsi_values[t] = 100.0 - (100.0 / (1.0 + rs))\n\n        rsi_regimes = np.full(T, -1, dtype=int)\n        rsi_regimes[rsi_values <= 30] = 0  # Oversold\n        rsi_regimes[(rsi_values > 30) & (rsi_values < 70)] = 1 # Neutral\n        rsi_regimes[rsi_values >= 70] = 2  # Overbought\n\n        # --- 2. Training Phase ---\n        q_table = np.zeros((NUM_STATES, NUM_ACTIONS))\n        rng = np.random.RandomState(12345)\n\n        for _ in range(E):\n            current_pos = 0\n            for t in range(w, T - 1):\n                # Current state\n                current_rsi_regime = rsi_regimes[t]\n                current_state_idx = current_rsi_regime * 2 + current_pos\n                \n                # Action selection (epsilon-greedy)\n                if rng.rand() < epsilon:\n                    action_idx = rng.randint(0, NUM_ACTIONS)\n                else:\n                    action_idx = np.argmax(q_table[current_state_idx, :])\n\n                # State transition and reward\n                prev_pos = current_pos\n                if action_idx == 0: # Hold\n                    next_pos = prev_pos\n                elif action_idx == 1: # Buy\n                    next_pos = 1\n                else: # Sell\n                    next_pos = 0\n                \n                transaction_cost = c if next_pos != prev_pos else 0.0\n                reward = next_pos * (prices_np[t+1] - prices_np[t]) - transaction_cost\n\n                # Next state\n                next_rsi_regime = rsi_regimes[t+1]\n                next_state_idx = next_rsi_regime * 2 + next_pos\n                \n                # Q-table update\n                old_q_value = q_table[current_state_idx, action_idx]\n                next_max_q = np.max(q_table[next_state_idx, :])\n                td_target = reward + gamma * next_max_q\n                new_q_value = old_q_value + alpha * (td_target - old_q_value)\n                q_table[current_state_idx, action_idx] = new_q_value\n\n                # Update position for next step in episode\n                current_pos = next_pos\n        \n        # --- 3. Evaluation Phase ---\n        total_wealth = 0.0\n        current_pos = 0\n        final_pos = 0\n\n        for t in range(w, T - 1):\n            # Current state\n            current_rsi_regime = rsi_regimes[t]\n            current_state_idx = current_rsi_regime * 2 + current_pos\n\n            # Action selection (greedy)\n            action_idx = np.argmax(q_table[current_state_idx, :])\n\n            # State transition and reward\n            prev_pos = current_pos\n            if action_idx == 0: # Hold\n                next_pos = prev_pos\n            elif action_idx == 1: # Buy\n                next_pos = 1\n            else: # Sell\n                next_pos = 0\n            \n            transaction_cost = c if next_pos != prev_pos else 0.0\n            reward = next_pos * (prices_np[t+1] - prices_np[t]) - transaction_cost\n            total_wealth += reward\n\n            # Update position for next step in evaluation\n            current_pos = next_pos\n        \n        final_pos = current_pos\n        \n        # Final liquidation\n        if final_pos == 1:\n            total_wealth -= c\n\n        results.append(round(total_wealth, 6))\n\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2388619"}]}