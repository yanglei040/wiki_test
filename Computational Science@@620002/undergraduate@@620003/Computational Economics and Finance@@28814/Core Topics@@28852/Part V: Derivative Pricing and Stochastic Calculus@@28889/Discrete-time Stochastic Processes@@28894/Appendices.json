{"hands_on_practices": [{"introduction": "This first practice puts the core concepts of Markov chains to work in a classic financial application: modeling credit rating transitions. You will estimate a transition matrix from a hypothetical sequence of ratings and then calculate the long-run stationary distribution. This exercise [@problem_id:2388997] is fundamental for understanding how to model and predict the long-term behavior of systems that evolve through discrete states, a common scenario in risk management and financial modeling.", "problem": "Consider a discrete-time credit rating system modeled as a first-order Markov chain on a finite state space. The state space is ordered as $S=\\{\\text{AAA},\\text{AA},\\text{A},\\text{BBB},\\text{Default}\\}$, which we index as $0,1,2,3,4$ respectively. You observe a time-ordered sequence of ratings $\\{X_t\\}_{t=0}^{T}$ taking values in $S$. Let $N_{ij}$ denote the count of one-step transitions from state $i$ to state $j$ in the observed sequence, that is, $N_{ij}=\\#\\{t \\in \\{0,\\ldots,T-1\\}\\,:\\,X_t=i,\\,X_{t+1}=j\\}$.\n\nYou must estimate the $5\\times 5$ transition probability matrix $P=\\left[P_{ij}\\right]$ using Laplace-smoothed maximum likelihood with pseudocount $\\alpha=1$. Specifically, for each $i\\in\\{0,1,2,3,4\\}$ and $j\\in\\{0,1,2,3,4\\}$, define\n$$\n\\widehat{P}_{ij}=\\frac{N_{ij}+\\alpha}{\\sum_{k=0}^{4} N_{ik}+5\\alpha},\n$$\nwith $\\alpha=1$. This yields a row-stochastic matrix with strictly positive entries.\n\nCompute the long-run (stationary) distribution $\\pi$ as the unique row vector with nonnegative entries that satisfies\n$$\n\\pi \\widehat{P}=\\pi,\\quad \\sum_{i=0}^{4}\\pi_i=1.\n$$\nReport the stationary distribution as a list of decimal numbers rounded to six decimal places in the specified state order $[\\pi_{\\text{AAA}},\\pi_{\\text{AA}},\\pi_{\\text{A}},\\pi_{\\text{BBB}},\\pi_{\\text{Default}}]$.\n\nUse the following test suite of observed sequences (each is a list of labels in $S$):\n\n- Test case $1$ (general case with upgrades and downgrades): \n  $[\\text{BBB},\\text{A},\\text{AA},\\text{AAA},\\text{AA},\\text{A},\\text{BBB},\\text{A},\\text{AA},\\text{AAA},\\text{AA},\\text{A},\\text{BBB},\\text{A},\\text{AA},\\text{AAA},\\text{AA},\\text{A},\\text{BBB},\\text{Default},\\text{A},\\text{AA},\\text{AAA},\\text{AA},\\text{A},\\text{BBB},\\text{A},\\text{AA},\\text{AAA},\\text{AA},\\text{A},\\text{BBB}]$.\n\n- Test case $2$ (boundary case with minimal information): \n  $[\\text{A},\\text{A}]$.\n\n- Test case $3$ (edge case with concentration in high ratings): \n  $[\\text{AAA},\\text{AA},\\text{AAA},\\text{AAA},\\text{AA},\\text{A},\\text{AAA},\\text{AAA},\\text{AAA},\\text{AA},\\text{A},\\text{AAA},\\text{AAA},\\text{AAA},\\text{AAA},\\text{AAA}]$.\n\nFor each test case, you must:\n1. Construct the smoothed estimator $\\widehat{P}$ using $\\alpha=1$ for all entries.\n2. Compute the stationary distribution $\\pi$ satisfying $\\pi \\widehat{P}=\\pi$ and $\\sum_i \\pi_i=1$.\n3. Round each component of $\\pi$ to six decimal places.\n\nFinal output format: Your program should produce a single line of output containing the results for all three test cases as a comma-separated list enclosed in square brackets, where each element is itself the list representation of the stationary distribution in the specified state order. For example, an admissible format is\n$[\\,[\\pi^{(1)}_{\\text{AAA}},\\ldots,\\pi^{(1)}_{\\text{Default}}],[\\pi^{(2)}_{\\text{AAA}},\\ldots,\\pi^{(2)}_{\\text{Default}}],[\\pi^{(3)}_{\\text{AAA}},\\ldots,\\pi^{(3)}_{\\text{Default}}]\\,]$,\nwith each number rounded to six decimal places. There are no physical units, angles, or percentages in this problem; all outputs must be decimals.", "solution": "The problem presented is a valid exercise in computational statistics and stochastic processes. It is scientifically grounded, well-posed, and objective. We shall proceed with a complete solution.\n\nThe problem requires us to estimate the stationary distribution of a discrete-time, first-order Markov chain. The state space is given as $S=\\{\\text{AAA}, \\text{AA}, \\text{A}, \\text{BBB}, \\text{Default}\\}$, which we shall index by integers $i \\in \\{0, 1, 2, 3, 4\\}$, respectively. We are given a time series of observations $\\{X_t\\}_{t=0}^{T}$.\n\nFirst, we must estimate the transition probability matrix, denoted by $P = [P_{ij}]$, where $P_{ij} = \\mathbb{P}(X_{t+1}=j | X_t=i)$. The problem specifies the use of a Laplace-smoothed maximum likelihood estimator. This is a Bayesian estimation method where a Dirichlet prior is placed on the rows of the transition matrix. For a given row $i$, the prior on the probability vector $[P_{i0}, \\dots, P_{i4}]$ is a symmetric Dirichlet distribution with concentration parameter $\\alpha$.\n\nLet $N_{ij}$ be the number of observed one-step transitions from state $i$ to state $j$ in the sequence. The formula for the smoothed estimator $\\widehat{P}_{ij}$ is given as:\n$$\n\\widehat{P}_{ij} = \\frac{N_{ij} + \\alpha}{\\sum_{k=0}^{4} N_{ik} + |S|\\alpha}\n$$\nwhere $|S|$ is the number of states, which is $5$. The problem specifies a pseudocount of $\\alpha=1$. Let $N_i = \\sum_{k=0}^{4} N_{ik}$ be the total number of transitions observed to originate from state $i$. The estimator simplifies to:\n$$\n\\widehat{P}_{ij} = \\frac{N_{ij} + 1}{N_i + 5}\n$$\nThis procedure ensures that every entry in the estimated transition matrix $\\widehat{P}$ is strictly positive, i.e., $\\widehat{P}_{ij} > 0$ for all $i, j \\in S$. A Markov chain with a transition matrix having all positive entries is known as a regular Markov chain. A key property of a regular Markov chain on a finite state space is that it is ergodic and possesses a unique stationary distribution.\n\nThe stationary distribution is a row vector $\\pi = [\\pi_0, \\pi_1, \\pi_2, \\pi_3, \\pi_4]$ that satisfies two conditions:\n$1.$ $\\pi \\widehat{P} = \\pi$\n$2.$ $\\sum_{i=0}^{4} \\pi_i = 1$\n\nThe first condition, $\\pi \\widehat{P} = \\pi$, means that $\\pi$ is a left eigenvector of the matrix $\\widehat{P}$ with a corresponding eigenvalue of $\\lambda=1$. This can be rewritten as a system of homogeneous linear equations:\n$$\n\\pi (\\widehat{P} - I) = \\mathbf{0}\n$$\nwhere $I$ is the $5 \\times 5$ identity matrix and $\\mathbf{0}$ is a zero row vector. In transpose form, this is $(\\widehat{P}^T - I^T) \\pi^T = \\mathbf{0}^T$, or more simply, $(\\widehat{P}^T - I) \\pi^T = \\mathbf{0}$.\n\nSince $\\lambda=1$ is an eigenvalue of any row-stochastic matrix, the matrix $(\\widehat{P}^T - I)$ is singular, and its null space is non-trivial. For a regular Markov chain, the eigenspace corresponding to $\\lambda=1$ has dimension $1$. Therefore, the system $(\\widehat{P}^T - I) \\pi^T = \\mathbf{0}$ has a solution space of dimension $1$. To find the unique stationary distribution $\\pi$, we must impose the normalization condition $\\sum_{i=0}^{4} \\pi_i = 1$.\n\nNumerically, we can solve this by constructing a system of linear equations. We take the first $|S|-1=4$ linear equations from $(\\widehat{P}^T - I) \\pi^T = \\mathbf{0}$ and add the normalization equation. Let $A$ be the matrix whose first $4$ rows are the first $4$ rows of $(\\widehat{P}^T - I)$ and whose last row is a vector of all ones. Let $b$ be a column vector with $4$ zeros followed by a one. The system to solve is:\n$$\nA \\pi^T = b\n$$\nThe solution is given by $\\pi^T = A^{-1}b$. The existence and uniqueness of the solution is guaranteed by the regularity of $\\widehat{P}$.\n\nWe will apply this procedure to each of the three test cases.\n\n**Step 1: State Mapping and Transition Counting**\nFor each test sequence, we map the string labels to integer indices $\\{0, 1, 2, 3, 4\\}$. Then, we iterate through the sequence of length $T+1$ to count the $T$ transitions and populate the $5 \\times 5$ count matrix $N = [N_{ij}]$.\n\n**Step 2: Transition Matrix Estimation**\nUsing the count matrix $N$ and $\\alpha=1$, we compute the smoothed transition matrix $\\widehat{P}$ using the formula $\\widehat{P}_{ij} = (N_{ij} + 1) / (N_i + 5)$.\n\n**Step 3: Stationary Distribution Calculation**\nWe construct the matrix $A$ from $\\widehat{P}^T$ as described above and the vector $b = [0, 0, 0, 0, 1]^T$. We then solve the linear system $A \\pi^T = b$ for $\\pi^T$ using a standard linear algebra solver.\n\n**Step 4: Formatting**\nThe components of the resulting vector $\\pi$ are rounded to six decimal places as required. The final output is a list containing the rounded stationary distribution vectors for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    \"\"\"\n\n    state_map = {'AAA': 0, 'AA': 1, 'A': 2, 'BBB': 3, 'Default': 4}\n    num_states = len(state_map)\n    alpha = 1.0\n\n    test_cases = [\n        ['BBB','A','AA','AAA','AA','A','BBB','A','AA','AAA','AA','A','BBB','A','AA','AAA','AA','A','BBB','Default','A','AA','AAA','AA','A','BBB','A','AA','AAA','AA','A','BBB'],\n        ['A','A'],\n        ['AAA','AA','AAA','AAA','AA','A','AAA','AAA','AAA','AA','A','AAA','AAA','AAA','AAA','AAA']\n    ]\n\n    all_results = []\n\n    for sequence in test_cases:\n        # Step 1: Count transitions\n        counts = np.zeros((num_states, num_states), dtype=int)\n        int_sequence = [state_map[s] for s in sequence]\n        \n        for i in range(len(int_sequence) - 1):\n            from_state = int_sequence[i]\n            to_state = int_sequence[i+1]\n            counts[from_state, to_state] += 1\n\n        # Step 2: Estimate smoothed transition matrix P_hat\n        p_hat = np.zeros((num_states, num_states), dtype=float)\n        row_totals = np.sum(counts, axis=1)\n        \n        for i in range(num_states):\n            denominator = row_totals[i] + num_states * alpha\n            for j in range(num_states):\n                numerator = counts[i, j] + alpha\n                p_hat[i, j] = numerator / denominator\n\n        # Step 3: Compute the stationary distribution pi\n        # We need to solve pi * P_hat = pi, or pi * (P_hat - I) = 0,\n        # which is equivalent to (P_hat^T - I^T) * pi^T = 0^T.\n        # Let A = P_hat^T - I. We solve for the null space of A.\n        # We replace the last equation with the normalization condition sum(pi) = 1.\n        \n        A = (p_hat.T - np.identity(num_states))\n        A[-1, :] = 1.0  # Last row is for sum(pi_i) = 1\n        \n        b = np.zeros(num_states)\n        b[-1] = 1.0  # Corresponds to sum(pi_i) = 1\n        \n        try:\n            # Solve the linear system A * pi^T = b\n            pi = np.linalg.solve(A, b)\n            \n            # Ensure non-negativity and re-normalize for robustness\n            pi[pi  0] = 0\n            pi /= np.sum(pi)\n\n        except np.linalg.LinAlgError:\n            # Fallback for singular matrix if something unexpected happens\n            # For a regular P_hat, this shouldn't be reached.\n            # We can use eigenvector method as a backup.\n            # Find the right eigenvector of P_hat.T for eigenvalue 1\n            eigenvalues, eigenvectors = np.linalg.eig(p_hat.T)\n            # Find the eigenvector corresponding to eigenvalue 1\n            idx = np.argmin(np.abs(eigenvalues - 1.0))\n            pi = np.real(eigenvectors[:, idx])\n            # Normalize to get the probability distribution\n            pi = pi / np.sum(pi)\n\n        # Step 4: Round and format the result\n        rounded_pi = np.round(pi, 6).tolist()\n        all_results.append(rounded_pi)\n\n    # Final print statement in the exact required format.\n    # The default str() representation of a list of lists works. e.g., [[...], [...]]\n    print(str(all_results).replace(\" \", \"\"))\n\n\nsolve()\n```", "id": "2388997"}, {"introduction": "We now move from discrete states to a continuous process by simulating the path of a stock price, a model central to financial engineering. This exercise uses Monte Carlo simulation to explore the first-passage time—the time it takes for a process to hit a specific target, such as a stock price doubling. By analyzing how parameters like drift ($\\mu$) and volatility ($\\sigma$) affect the probability and timing of hitting this target [@problem_id:2388969], you will build practical intuition for risk assessment and the valuation of path-dependent financial instruments.", "problem": "You are given a discrete-time geometric price process that models a stock price in computational economics and finance. Let $\\{S_n\\}_{n \\in \\mathbb{N}_0}$ denote the price at discrete times $n = 0,1,2,\\dots$ with initial level $S_0  0$. The process evolves according to\n$$\nS_{n+1} \\;=\\; S_n \\,\\exp\\!\\Big(\\big(\\mu - \\tfrac{1}{2}\\sigma^2\\big)\\,\\Delta t \\;+\\; \\sigma \\sqrt{\\Delta t}\\,\\varepsilon_{n+1}\\Big),\n$$\nwhere $\\mu \\in \\mathbb{R}$ is the drift, $\\sigma \\ge 0$ is the volatility, $\\Delta t  0$ is the time-step size, and $\\{\\varepsilon_n\\}_{n \\ge 1}$ are independent and identically distributed (i.i.d.) standard normal random variables. Define the first-passage time (also called the first-hitting time) to the doubling barrier $B = 2S_0$ by\n$$\nT \\;=\\; \\inf\\{\\, n \\in \\mathbb{N} : S_n \\ge B \\,\\},\n$$\nwith the convention that $T = +\\infty$ if the barrier is not reached.\n\nFor a fixed maximum number of steps $H \\in \\mathbb{N}$, define the following empirical quantities computed from $M$ independent realizations of the process:\n- The empirical hitting probability within the horizon $H$:\n$$\n\\widehat{p}_H \\;=\\; \\frac{1}{M}\\sum_{m=1}^M \\mathbf{1}\\{\\,T^{(m)} \\le H\\,\\},\n$$\nwhere $T^{(m)}$ is the first-passage time on the $m$-th realization.\n- The conditional empirical mean first-passage time (in steps), restricted to realizations that hit within $H$:\n$$\n\\widehat{\\mathbb{E}}[T \\mid T \\le H] \\;=\\; \\frac{\\sum_{m=1}^M T^{(m)} \\,\\mathbf{1}\\{\\,T^{(m)} \\le H\\,\\}}{\\sum_{m=1}^M \\mathbf{1}\\{\\,T^{(m)} \\le H\\,\\}}.\n$$\n- The conditional empirical minimum and maximum first-passage times (in steps) among the subset with $T \\le H$:\n$$\n\\widehat{T}_{\\min} \\;=\\; \\min\\{\\,T^{(m)} : T^{(m)} \\le H\\,\\},\\qquad\n\\widehat{T}_{\\max} \\;=\\; \\max\\{\\,T^{(m)} : T^{(m)} \\le H\\,\\}.\n$$\nIf no realization hits the barrier within $H$ (that is, $\\sum_{m=1}^M \\mathbf{1}\\{\\,T^{(m)} \\le H\\,\\} = 0$), define $\\widehat{\\mathbb{E}}[T \\mid T \\le H]$, $\\widehat{T}_{\\min}$, and $\\widehat{T}_{\\max}$ to be Not-a-Number, written as $\\mathrm{NaN}$.\n\nYour task is to produce a single program that, for each parameter set in the test suite below, computes and outputs the four-tuple\n$$\n\\big[\\,\\widehat{p}_H,\\;\\widehat{\\mathbb{E}}[T \\mid T \\le H],\\;\\widehat{T}_{\\min},\\;\\widehat{T}_{\\max}\\,\\big]\n$$\nas floating-point numbers.\n\nAll simulations must use $M = 10000$ independent realizations, initial price $S_0 = 100$, barrier $B = 2S_0$, and the fixed pseudorandom seed rule: let $s = 314159$, and for the $i$-th parameter set (with $i$ starting at $0$), use seed $s + i$ to initialize an independent pseudorandom number generator. The time-step size is $\\Delta t$ as specified per test case. Report first-passage times in units of discrete steps $n$ (integers), but output them as floating-point numbers. Do not round; output the raw floating-point values (for example, output $252$ as $252.0$). When no realization hits within $H$, output $\\mathrm{NaN}$ for the conditional quantities as specified above. Angles are not involved, and no physical units are required.\n\nTest suite (each line is one parameter set):\n- Case $0$: $\\mu = 0.08$, $\\sigma = 0.2$, $\\Delta t = 1/252$, $H = 756$, $S_0 = 100$.\n- Case $1$: $\\mu = 0.0$, $\\sigma = 0.3$, $\\Delta t = 1/252$, $H = 756$, $S_0 = 100$.\n- Case $2$: $\\mu = -0.02$, $\\sigma = 0.4$, $\\Delta t = 1/252$, $H = 756$, $S_0 = 100$.\n- Case $3$: $\\mu = \\ln 2$, $\\sigma = 0.0$, $\\Delta t = 1/252$, $H = 252$, $S_0 = 100$.\n- Case $4$: $\\mu = 0.0$, $\\sigma = 1.0$, $\\Delta t = 1/252$, $H = 252$, $S_0 = 100$.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets and ordered by the cases above. Each case’s result must itself be a list of four floating-point numbers in the order $\\big[\\,\\widehat{p}_H,\\;\\widehat{\\mathbb{E}}[T \\mid T \\le H],\\;\\widehat{T}_{\\min},\\;\\widehat{T}_{\\max}\\,\\big]$. For example, a valid output has the form\n$$\n\\big[\\,[x_{0,1},x_{0,2},x_{0,3},x_{0,4}],\\,[x_{1,1},x_{1,2},x_{1,3},x_{1,4}],\\,\\dots,\\,[x_{4,1},x_{4,2},x_{4,3},x_{4,4}]\\,\\big],\n$$\nwhere each $x_{i,j}$ is a floating-point number, with $\\mathrm{NaN}$ used where appropriate. The program must not read any input and must not print anything else.", "solution": "The problem has been subjected to rigorous validation and is determined to be valid. It is scientifically grounded in the established theory of stochastic processes, specifically the geometric Brownian motion model which is a cornerstone of modern financial mathematics. The problem statement is well-posed, providing a complete and consistent specification of all parameters, initial conditions, computational requirements, and output formats. The task constitutes a standard numerical exercise in computational finance, solvable via Monte Carlo simulation.\n\nThe problem requires the computation of first-passage time statistics for a discrete-time geometric price process $\\{S_n\\}_{n \\in \\mathbb{N}_0}$ defined by the recurrence relation:\n$$\nS_{n+1} \\;=\\; S_n \\,\\exp\\!\\Big(\\big(\\mu - \\tfrac{1}{2}\\sigma^2\\big)\\,\\Delta t \\;+\\; \\sigma \\sqrt{\\Delta t}\\,\\varepsilon_{n+1}\\Big)\n$$\nHere, $S_0  0$ is the initial price, $\\mu$ is the drift, $\\sigma$ is the volatility, $\\Delta t$ is the time step, and $\\{\\varepsilon_n\\}$ are i.i.d. standard normal random variables, $\\varepsilon_n \\sim \\mathcal{N}(0, 1)$.\n\nFor improved numerical stability and computational efficiency, we transform the multiplicative process for the price $S_n$ into an additive process for the log-price, $X_n = \\ln(S_n)$. Taking the natural logarithm of the evolution equation yields:\n$$\nX_{n+1} \\;=\\; X_n + \\underbrace{\\left(\\mu - \\frac{1}{2}\\sigma^2\\right)\\Delta t}_{\\text{drift part}} + \\underbrace{\\sigma \\sqrt{\\Delta t}\\,\\varepsilon_{n+1}}_{\\text{diffusion part}}\n$$\nThe first-passage time $T$ is the first time step $n \\in \\mathbb{N}$ at which $S_n \\ge B$, where the barrier is $B = 2S_0$. In terms of log-price, this condition is equivalent to $X_n \\ge \\ln(B) = \\ln(2S_0)$.\n\nThe required statistics are to be estimated using a Monte Carlo simulation with $M = 10000$ independent paths, each simulated up to a maximum horizon of $H$ steps. A vectorized implementation is the most efficient method for this task, as it leverages optimized `NumPy` library routines to perform operations on all paths simultaneously, thereby avoiding slow, explicit loops in Python.\n\nThe algorithm for each test case is as follows:\n$1$. Initialize the pseudorandom number generator with the specified seed, $s+i$, where $s=314159$ and $i$ is the zero-indexed case number.\n$2$. Define the model parameters $\\mu, \\sigma, \\Delta t, H, S_0$ and simulation parameters $M$. From these, calculate the barrier $B=2S_0$, the initial log-price $X_0=\\ln(S_0)$, and the log-barrier $\\ln(B)$.\n$3$. A special case exists for $\\sigma = 0$, where the process becomes deterministic. The hitting time $T$ can be calculated analytically as $T = \\lceil \\frac{\\ln(2)}{\\mu \\Delta t} \\rceil$ for $\\mu0$. If $\\mu \\le 0$, the barrier is never reached, so $T=\\infty$. This analytical solution is used directly for such cases.\n$4$. For the stochastic cases ($\\sigma  0$), we proceed with the vectorized simulation:\n    a. Pre-calculate the constant drift part of the log-price increment, $\\nu = (\\mu - \\frac{1}{2}\\sigma^2)\\Delta t$, and the diffusion scaling factor, $d = \\sigma \\sqrt{\\Delta t}$.\n    b. Generate an $M \\times H$ matrix of i.i.d. standard normal random variates, $\\boldsymbol{\\varepsilon}$.\n    c. Compute the matrix of log-price increments for all paths and steps: $\\Delta \\mathbf{X} = \\nu + d \\cdot \\boldsymbol{\\varepsilon}$.\n    d. The log-price paths at steps $n=1, \\dots, H$ are found by adding the initial log-price $X_0$ to the cumulative sum of the increments along the time axis: $\\mathbf{X}_{1:H} = X_0 + \\mathrm{cumsum}(\\Delta \\mathbf{X}, \\text{axis}=1)$.\n$5$. To find the first-passage times, we first identify all hitting events. A boolean matrix $\\mathbf{C}$ of size $M \\times H$ is created where $C_{m,n-1} = 1$ if path $m$ is at or above the log-barrier at step $n$, i.e., $X_n^{(m)} \\ge \\ln(B)$.\n$6$. The set of paths that hit the barrier at any point up to horizon $H$ is identified. The number of such paths, $N_{\\text{hit}}$, allows us to compute the empirical hitting probability $\\widehat{p}_H = N_{\\text{hit}} / M$.\n$7$. If $N_{\\text{hit}}  0$, we find the first hitting time for each of these paths. The `numpy.argmax` function is applied to each row of the relevant subset of $\\mathbf{C}$ to find the index of the first `True` value. The first-passage time in steps is this index plus one. Note that `argmax` returns $0$ for an all-`False` row, but our logic correctly restricts its use only to rows corresponding to paths that are known to have hit the barrier.\n$8$. With the collection of valid first-passage times $\\{T^{(m)} \\mid T^{(m)} \\le H\\}$, the conditional empirical mean $\\widehat{\\mathbb{E}}[T \\mid T \\le H]$, minimum $\\widehat{T}_{\\min}$, and maximum $\\widehat{T}_{\\max}$ are computed using standard `NumPy` functions (`mean`, `min`, `max`). The results from integer-valued steps are cast to floating-point numbers as required.\n$9$. If $N_{\\text{hit}}=0$, the conditional statistics $\\widehat{\\mathbb{E}}[T \\mid T \\le H]$, $\\widehat{T}_{\\min}$, and $\\widehat{T}_{\\max}$ are defined as Not-a-Number (`NaN`), per the problem specification.\n\nThis procedure is repeated for each parameter set given in the test suite, and the results are aggregated into the final specified output format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print results.\n    \"\"\"\n    \n    # Test cases: (mu, sigma, dt, H, S0)\n    test_cases_params = [\n        # Case 0: mu = 0.08, sigma = 0.2, dt = 1/252, H = 756, S0 = 100\n        (0.08, 0.2, 1.0/252.0, 756, 100.0),\n        # Case 1: mu = 0.0, sigma = 0.3, dt = 1/252, H = 756, S0 = 100\n        (0.0, 0.3, 1.0/252.0, 756, 100.0),\n        # Case 2: mu = -0.02, sigma = 0.4, dt = 1/252, H = 756, S0 = 100\n        (-0.02, 0.4, 1.0/252.0, 756, 100.0),\n        # Case 3: mu = ln(2), sigma = 0.0, dt = 1/252, H = 252, S0 = 100\n        (np.log(2.0), 0.0, 1.0/252.0, 252, 100.0),\n        # Case 4: mu = 0.0, sigma = 1.0, dt = 1/252, H = 252, S0 = 100\n        (0.0, 1.0, 1.0/252.0, 252, 100.0),\n    ]\n\n    M = 10000\n    base_seed = 314159\n\n    results = []\n    for i, params in enumerate(test_cases_params):\n        mu, sigma, dt, H, S0 = params\n        seed = base_seed + i\n        case_result = simulate_first_passage_stats(mu, sigma, dt, H, S0, M, seed)\n        results.append(case_result)\n\n    # Format the final output string as a list of lists.\n    # str() on a list produces the desired '[...]' format.\n    # Joining these string representations with a comma creates the final list of lists.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\ndef simulate_first_passage_stats(mu, sigma, dt, H, S0, M, seed):\n    \"\"\"\n    Computes first-passage time statistics for a single parameter set.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    # Barrier level B = 2*S0 -- ln(S_n) = ln(2*S0)\n    # ln(S_n) - ln(S0) = ln(2)\n    log_barrier_target = np.log(2.0)\n\n    # Handle the deterministic case (sigma = 0) analytically\n    if sigma == 0.0:\n        if mu  0.0:\n            # S_n = S0 * exp(n * mu * dt) = 2*S0\n            # n * mu * dt = ln(2) -- n = ln(2) / (mu * dt)\n            T = np.ceil(log_barrier_target / (mu * dt))\n        else:\n            # Price will not increase, so barrier is never reached.\n            T = np.inf\n        \n        if T = H:\n            # All paths hit at the same time T\n            p_H = 1.0\n            E_T = float(T)\n            T_min = float(T)\n            T_max = float(T)\n            return [p_H, E_T, T_min, T_max]\n        else:\n            # No path hits within the horizon H\n            return [0.0, np.nan, np.nan, np.nan]\n\n    # Vectorized Monte Carlo simulation for stochastic case (sigma  0)\n    # We simulate the log-price process relative to its initial value.\n    # X_n = ln(S_n/S0)\n    # X_{n+1} = X_n + (mu - 0.5*sigma^2)*dt + sigma*sqrt(dt)*epsilon\n    \n    # Pre-calculate constants for the simulation\n    drift_part = (mu - 0.5 * sigma**2) * dt\n    diffusion_part = sigma * np.sqrt(dt)\n\n    # Generate all random shocks at once\n    epsilons = rng.standard_normal(size=(M, H))\n\n    # Calculate log-price increments for all paths and steps\n    log_increments = drift_part + diffusion_part * epsilons\n\n    # Calculate the cumulative sum of increments to get the log-price path (relative to ln(S0))\n    # log_paths_relative has shape (M, H) where each element [m, n-1] is ln(S_n/S0) for path m.\n    log_paths_relative = np.cumsum(log_increments, axis=1)\n\n    # Check for barrier crossing at each step\n    crossed_barrier = log_paths_relative = log_barrier_target\n\n    # Determine which paths hit the barrier at all within the horizon H\n    path_hit_at_all = np.any(crossed_barrier, axis=1)\n    \n    num_hits = np.sum(path_hit_at_all)\n\n    if num_hits == 0:\n        return [0.0, np.nan, np.nan, np.nan]\n\n    # Calculate empirical hitting probability\n    p_H = num_hits / M\n\n    # For paths that hit, find the first hitting time\n    # We apply argmax only to the rows corresponding to paths that actually hit.\n    # argmax returns the index of the first True value.\n    hitting_indices = np.argmax(crossed_barrier[path_hit_at_all], axis=1)\n    \n    # The step number is the index + 1\n    valid_hitting_times = hitting_indices + 1\n    \n    # Calculate conditional statistics\n    E_T = np.mean(valid_hitting_times)\n    T_min = float(np.min(valid_hitting_times))\n    T_max = float(np.max(valid_hitting_times))\n\n    return [p_H, E_T, T_min, T_max]\n\nsolve()\n```", "id": "2388969"}, {"introduction": "Our final practice introduces the powerful concept of a hidden state, where the process we care about is not directly observable. Using an engaging example from sports analytics, you will apply the Kalman filter to estimate a basketball player's \"true\" shooting ability from their noisy game-to-game performance data. This exercise [@problem_id:2389012] provides a hands-on introduction to state-space models and recursive filtering, a cornerstone technique for extracting signals from noisy economic and financial time series.", "problem": "Consider a single hidden state representing a basketball player’s underlying shooting ability in a sequence of games. At each discrete time step $t \\in \\{1,2,\\dots,T\\}$, the player takes $n_t$ shots and makes $m_t$ shots. Define the observed shooting percentage as $y_t = m_t / n_t$ when $n_t \\gt 0$ and treat $y_t$ as missing when $n_t = 0$. Model the hidden ability and the observations using the following linear Gaussian state-space model:\n- Hidden state dynamics: $ \\theta_t = \\mu + \\phi \\left(\\theta_{t-1} - \\mu\\right) + w_t $, where $w_t \\sim \\mathcal{N}(0,q)$.\n- Observation equation (when $n_t \\gt 0$): $ y_t = \\theta_t + v_t $, where $v_t \\sim \\mathcal{N}(0, R_t)$.\n\nAssume $R_t$ is known and depends on $n_t$ via a binomial-sampling-inspired approximation for the variance of a sample mean: $ R_t = \\bar{p}(1-\\bar{p}) / n_t $, where $\\bar{p}$ is a fixed reference probability. When $n_t = 0$, treat the observation as missing and perform only the state prediction step (no update). All probabilities and variances must be expressed as decimals (for example, write $0.45$ instead of $45\\%$).\n\nYou are given the following fixed parameters:\n- Long-run mean: $\\mu = 0.45$.\n- Autoregressive coefficient: $\\phi = 0.90$.\n- State innovation variance: $q = 0.0005$.\n- Reference probability for the observation variance: $\\bar{p} = 0.45$ (use this in $R_t$).\n- Prior for the initial state: $\\theta_0 \\sim \\mathcal{N}(m_0, P_0)$ with $m_0 = 0.45$ and $P_0 = 0.01$.\n\nStarting from the prior $(m_0, P_0)$, implement the recursive filtering procedure that, at each time $t$, computes the one-step-ahead prediction for the state and its variance, then (if $n_t \\gt 0$) updates them with the observation $y_t$ using the linear Gaussian model above. If $n_t = 0$, skip the update and carry forward the prediction as the filtered state for that period.\n\nYour task is to write a complete program that:\n1. Implements the described time-varying-variance Kalman filter for the model specified above.\n2. Runs the filter on each of the following test suites of games (each suite is a sequence of $(m_t, n_t)$ pairs):\n   - Test case A (moderate attempts, varied outcomes):\n     - $[(5,11), (4,10), (7,12), (6,14), (2,6), (8,15), (3,8), (6,12), (7,16), (5,9)]$.\n   - Test case B (high attempts, relatively stable outcomes):\n     - $[(9,20), (10,22), (8,18), (11,24), (12,25), (9,19), (10,21), (12,26), (11,24), (13,28)]$.\n   - Test case C (some games with zero attempts; treat those as missing observations):\n     - $[(0,0), (3,5), (0,0), (4,4), (0,0), (2,10), (0,0), (5,10)]$.\n   - Test case D (volatile outcomes and attempts):\n     - $[(1,2), (0,5), (7,10), (1,12), (9,10), (0,3), (6,15), (2,2), (0,8), (10,12)]$.\n3. For each test case, outputs the final filtered posterior mean $\\hat{\\theta}_T$ and variance $P_T$ after processing the last game in the sequence.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a two-element list $[\\hat{\\theta}_T, P_T]$ with both values rounded to exactly six decimal places. For example, the overall output should look like:\n- $[[\\hat{\\theta}_T^{(A)}, P_T^{(A)}],[\\hat{\\theta}_T^{(B)}, P_T^{(B)}],[\\hat{\\theta}_T^{(C)}, P_T^{(C)}],[\\hat{\\theta}_T^{(D)}, P_T^{(D)}]]$,\nprinted as a single line.\n\nAngle units do not apply. Percentages must be expressed as decimals, not with a percentage sign. There are no physical units in this problem.", "solution": "The problem requires the implementation of a Kalman filter, a recursive algorithm for estimating the hidden state of a linear dynamical system from a series of noisy measurements. The system models a basketball player's shooting ability, $\\theta_t$, as a latent state that evolves over time.\n\nFirst, we formalize the problem within the framework of a linear Gaussian state-space model. The model consists of two equations: a state transition equation and an observation equation.\n\nThe state of the system is the player's shooting ability at time $t$, denoted by the scalar $\\theta_t$. The evolution of this state is described by an autoregressive process of order one, AR($1$), which is mean-reverting to a long-run mean $\\mu$.\n\nState Equation:\nThe hidden state dynamics are given by:\n$$ \\theta_t = \\mu + \\phi \\left(\\theta_{t-1} - \\mu\\right) + w_t $$\nThis can be rearranged into the standard linear form:\n$$ \\theta_t = \\phi \\theta_{t-1} + (1 - \\phi)\\mu + w_t $$\nwhere:\n- $\\theta_t$ is the state at time $t$.\n- $\\phi$ is the autoregressive coefficient, determining the persistence of the state. It is given as $\\phi=0.90$.\n- $\\mu$ is the long-run mean of the process, given as $\\mu=0.45$.\n- $w_t$ is the process noise, assumed to be a white noise process with $w_t \\sim \\mathcal{N}(0, q)$, where $q$ is the state innovation variance, given as $q=0.0005$.\n\nObservation Equation:\nThe observation at time $t$ is the player's observed shooting percentage, $y_t = m_t / n_t$, where $m_t$ is the number of made shots out of $n_t$ attempts. This observation is available only when $n_t  0$. The observation is modeled as a noisy measurement of the true underlying ability $\\theta_t$.\n$$ y_t = \\theta_t + v_t $$\nwhere:\n- $y_t$ is the observation at time $t$.\n- $v_t$ is the measurement noise, assumed to be a white noise process with $v_t \\sim \\mathcal{N}(0, R_t)$. The variance $R_t$ is time-dependent.\n\nThe measurement noise variance $R_t$ is approximated based on the variance of a sample proportion from a binomial distribution. Given $n_t$ trials, the variance of the sample proportion $y_t$ is approximately $p(1-p)/n_t$. The problem specifies using a fixed reference probability $\\bar{p}=0.45$ for this calculation:\n$$ R_t = \\frac{\\bar{p}(1 - \\bar{p})}{n_t} = \\frac{0.45(1 - 0.45)}{n_t} = \\frac{0.2475}{n_t} $$\nThis formulation makes $R_t$ time-varying, as it depends on the number of shot attempts $n_t$ in each game. When $n_t$ is large, $R_t$ is small, reflecting higher confidence in the observation.\n\nThe Kalman filter provides a recursive solution for estimating the posterior distribution of the state, $p(\\theta_t | y_{1:t})$. Since the model is linear and Gaussian, this posterior distribution is also Gaussian and can be fully characterized by its mean $\\hat{\\theta}_{t|t}$ and variance $P_{t|t}$.\n\nThe filtering process starts with a prior distribution for the initial state $\\theta_0 \\sim \\mathcal{N}(m_0, P_0)$, with given parameters $m_0 = 0.45$ and $P_0 = 0.01$. At each time step $t=1, 2, \\dots, T$, the algorithm performs two steps: a prediction step and an update step.\n\nLet the filtered posterior at time $t-1$ be $\\mathcal{N}(\\hat{\\theta}_{t-1|t-1}, P_{t-1|t-1})$.\n\nStep 1: Prediction (Time Update)\nIn this step, we predict the distribution of the state at time $t$ based on all information up to time $t-1$. The predicted (a priori) mean $\\hat{\\theta}_{t|t-1}$ and variance $P_{t|t-1}$ are computed.\n\nTaking the expectation of the state equation:\n$$ \\hat{\\theta}_{t|t-1} = \\mathbb{E}[\\phi \\theta_{t-1} + (1 - \\phi)\\mu + w_t | y_{1:t-1}] = \\phi \\hat{\\theta}_{t-1|t-1} + (1 - \\phi)\\mu $$\nThe variance of the prediction error is:\n$$ P_{t|t-1} = \\text{Var}(\\theta_t - \\hat{\\theta}_{t|t-1}) = \\text{Var}(\\phi(\\theta_{t-1} - \\hat{\\theta}_{t-1|t-1}) + w_t) $$\nSince the error $(\\theta_{t-1} - \\hat{\\theta}_{t-1|t-1})$ is uncorrelated with the process noise $w_t$, the variances add:\n$$ P_{t|t-1} = \\phi^2 \\text{Var}(\\theta_{t-1} - \\hat{\\theta}_{t-1|t-1}) + \\text{Var}(w_t) = \\phi^2 P_{t-1|t-1} + q $$\n\nStep 2: Update (Measurement Update)\nThis step refines the prediction using the new observation $y_t$ at time $t$. This is only performed if an observation is available (i.e., $n_t  0$).\n\nFirst, we compute the innovation, which is the discrepancy between the actual observation $y_t$ and its prediction:\n$$ \\tilde{y}_t = y_t - \\mathbb{E}[y_t | y_{1:t-1}] = y_t - \\mathbb{E}[\\theta_t + v_t | y_{1:t-1}] = y_t - \\hat{\\theta}_{t|t-1} $$\nThe variance of the innovation, or innovation covariance, is:\n$$ S_t = \\text{Var}(\\tilde{y}_t) = \\text{Var}((\\theta_t - \\hat{\\theta}_{t|t-1}) + v_t) = P_{t|t-1} + R_t $$\nThe optimal Kalman gain $K_t$ determines how much the prediction is adjusted based on the innovation. It is calculated to minimize the posterior error variance:\n$$ K_t = \\frac{\\text{Cov}(\\theta_t, \\tilde{y}_t)}{\\text{Var}(\\tilde{y}_t)} = \\frac{\\text{Cov}(\\theta_t, \\theta_t - \\hat{\\theta}_{t|t-1} + v_t)}{S_t} = \\frac{P_{t|t-1}}{S_t} = \\frac{P_{t|t-1}}{P_{t|t-1} + R_t} $$\nThe updated (a posteriori) state mean is a weighted average of the predicted mean and the observation:\n$$ \\hat{\\theta}_{t|t} = \\hat{\\theta}_{t|t-1} + K_t \\tilde{y}_t = \\hat{\\theta}_{t|t-1} + K_t (y_t - \\hat{\\theta}_{t|t-1}) $$\nThe updated (a posteriori) error variance is:\n$$ P_{t|t} = (1 - K_t) P_{t|t-1} $$\n\nHandling Missing Observations:\nIf $n_t = 0$, the observation $y_t$ is missing. In this scenario, no update can be performed. The best estimate for the state at time $t$ is simply the prediction from the previous step. Therefore, the posterior for time $t$ is set equal to the prior:\n$$ \\hat{\\theta}_{t|t} = \\hat{\\theta}_{t|t-1} $$\n$$ P_{t|t} = P_{t|t-1} $$\n\nThe overall algorithm is implemented for each test case as follows:\n1. Initialize the filter with the prior mean $\\hat{\\theta}_{0|0} = m_0 = 0.45$ and variance $P_{0|0} = P_0 = 0.01$.\n2. For each time step $t=1, \\dots, T$:\n   a. Perform the prediction step to compute $\\hat{\\theta}_{t|t-1}$ and $P_{t|t-1}$.\n   b. Check if $n_t  0$:\n      i. If true, calculate $y_t = m_t / n_t$ and $R_t = \\bar{p}(1-\\bar{p}) / n_t$. Perform the update step to compute $\\hat{\\theta}_{t|t}$ and $P_{t|t}$.\n      ii. If false, skip the update and set $\\hat{\\theta}_{t|t} = \\hat{\\theta}_{t|t-1}$ and $P_{t|t} = P_{t|t-1}$.\n3. The final values $\\hat{\\theta}_{T|T}$ and $P_{T|T}$ after processing the entire sequence are the desired outputs for each test case.", "answer": "```python\nimport numpy as np\n\ndef run_kalman_filter(data, mu, phi, q, p_bar, m0, P0):\n    \"\"\"\n    Implements the Kalman filter for the given state-space model.\n\n    Args:\n        data (list of tuples): A sequence of (m_t, n_t) pairs.\n        mu (float): Long-run mean of the state process.\n        phi (float): Autoregressive coefficient of the state process.\n        q (float): State innovation variance.\n        p_bar (float): Reference probability for observation variance.\n        m0 (float): Prior mean of the initial state.\n        P0 (float): Prior variance of the initial state.\n\n    Returns:\n        tuple: A tuple containing the final filtered posterior mean and variance.\n    \"\"\"\n    # Initialize the filtered state mean and variance with the prior\n    theta_filt = m0\n    P_filt = P0\n    \n    # Pre-calculate the numerator for the observation variance R_t\n    obs_var_numerator = p_bar * (1.0 - p_bar)\n\n    # Iterate through each time step (game)\n    for m_t, n_t in data:\n        # --- 1. Prediction Step ---\n        # Predict the next state mean\n        theta_pred = phi * theta_filt + mu * (1.0 - phi)\n        # Predict the next state variance\n        P_pred = phi**2 * P_filt + q\n\n        # --- 2. Update Step ---\n        # Check if there is an observation (n_t  0)\n        if n_t  0:\n            # Calculate the observation y_t\n            y_t = m_t / n_t\n            # Calculate the time-varying observation variance R_t\n            R_t = obs_var_numerator / n_t\n            \n            # Calculate the innovation covariance S_t\n            S_t = P_pred + R_t\n            \n            # Calculate the optimal Kalman gain K_t\n            K_t = P_pred / S_t\n            \n            # Update the state mean\n            theta_filt = theta_pred + K_t * (y_t - theta_pred)\n            \n            # Update the state variance\n            P_filt = (1.0 - K_t) * P_pred\n        else:\n            # If observation is missing (n_t = 0), the posterior is the prior\n            theta_filt = theta_pred\n            P_filt = P_pred\n            \n    return theta_filt, P_filt\n\ndef solve():\n    \"\"\"\n    Main function to define parameters, run test cases, and print results.\n    \"\"\"\n    # Fixed model parameters\n    mu = 0.45\n    phi = 0.90\n    q = 0.0005\n    p_bar = 0.45\n    \n    # Prior for the initial state\n    m0 = 0.45\n    P0 = 0.01\n\n    # Define the test cases from the problem statement\n    test_cases = [\n        # Test case A\n        [(5, 11), (4, 10), (7, 12), (6, 14), (2, 6), (8, 15), (3, 8), (6, 12), (7, 16), (5, 9)],\n        # Test case B\n        [(9, 20), (10, 22), (8, 18), (11, 24), (12, 25), (9, 19), (10, 21), (12, 26), (11, 24), (13, 28)],\n        # Test case C\n        [(0, 0), (3, 5), (0, 0), (4, 4), (0, 0), (2, 10), (0, 0), (5, 10)],\n        # Test case D\n        [(1, 2), (0, 5), (7, 10), (1, 12), (9, 10), (0, 3), (6, 15), (2, 2), (0, 8), (10, 12)]\n    ]\n\n    results = []\n    # Process each test case\n    for data in test_cases:\n        theta_T, P_T = run_kalman_filter(data, mu, phi, q, p_bar, m0, P0)\n        # Format the result for the current test case as a string\n        # with values rounded to six decimal places, enclosed in brackets.\n        results.append(f\"[{theta_T:.6f}, {P_T:.6f}]\")\n\n    # Final print statement in the exact required format.\n    # The output is a single line: a list of lists.\n    print(f\"[{','.join(results)}]\")\n\n# Execute the main function\nsolve()\n```", "id": "2389012"}]}