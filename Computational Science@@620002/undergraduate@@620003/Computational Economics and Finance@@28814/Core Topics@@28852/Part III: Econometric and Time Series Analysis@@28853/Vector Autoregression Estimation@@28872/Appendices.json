{"hands_on_practices": [{"introduction": "Vector Autoregressions are a cornerstone of multivariate time series forecasting. However, the performance of a VAR model critically depends on its specification, particularly the number of lags included. This first exercise provides a practical, hands-on opportunity to compare the out-of-sample forecasting accuracy of VAR models with different lag lengths (VAR(1) and VAR(4)) against the naive but often formidable random walk benchmark. By working through this simulation, you will gain direct experience in estimating VAR models and evaluating their forecasting power using the Root Mean Squared Forecast Error, a standard metric in the field [@problem_id:2447495].", "problem": "You are given the task of building a fully reproducible computational experiment to compare the out-of-sample one-step-ahead forecast performance of three models for multivariate exchange rates: a Vector Autoregression (VAR) of order one, a Vector Autoregression (VAR) of order four, and a random walk in levels. The experiment must be implemented as a complete program.\n\nStart from the following foundational definitions. A $k$-dimensional Vector Autoregression (VAR) of order $p$ is defined by\n$$\n\\mathbf{y}_t = \\mathbf{c} + \\sum_{i=1}^{p} \\mathbf{A}_i \\mathbf{y}_{t-i} + \\mathbf{u}_t,\n$$\nwhere $\\mathbf{y}_t \\in \\mathbb{R}^k$ is the vector of variables, $\\mathbf{c} \\in \\mathbb{R}^k$ is an intercept, $\\mathbf{A}_i \\in \\mathbb{R}^{k \\times k}$ are autoregressive coefficient matrices, and $\\mathbf{u}_t \\sim \\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$ is a zero-mean Gaussian innovation with positive definite covariance matrix $\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{k \\times k}$. The Ordinary Least Squares (OLS) estimator of the parameters in a VAR is obtained by minimizing the sum of squared residuals across all equations, which is equivalent to solving the multivariate least squares problem implied by stacking the regressors and responses.\n\nImplement and evaluate the models as follows.\n\n1. Estimation principle to be implemented. For each model with order $p \\in \\{1,4\\}$, estimate the parameters by Ordinary Least Squares (OLS) using only the training sample. Construct the regressor matrix $\\mathbf{X}$ by stacking an intercept and lagged values $\\{\\mathbf{y}_{t-1}, \\ldots, \\mathbf{y}_{t-p}\\}$ and the response matrix $\\mathbf{Y}$ by stacking $\\mathbf{y}_t$ for $t$ from the end of the training window backward by $p$ lags. Solve the least squares problem to obtain coefficient estimates. Use these fixed estimates to generate one-step-ahead forecasts over the test sample, always conditioning on actual lagged values from the realized series (no re-estimation or updating of parameters during testing).\n\n2. Random walk benchmark. The random walk forecast in levels is defined by $\\widehat{\\mathbf{y}}_{t+1|t} = \\mathbf{y}_t$ for each forecast origin in the test sample.\n\n3. Forecast loss metric. For each model, compute the Root Mean Squared Forecast Error (RMSFE) aggregated across all variables and all out-of-sample forecast origins in the test window:\n$$\n\\mathrm{RMSFE} = \\sqrt{\\frac{1}{H k} \\sum_{h=1}^{H} \\left\\|\\mathbf{y}_{T_{\\text{train}}+h} - \\widehat{\\mathbf{y}}_{T_{\\text{train}}+h|T_{\\text{train}}+h-1}\\right\\|_2^2},\n$$\nwhere $k$ is the dimension of $\\mathbf{y}_t$, $H$ is the number of out-of-sample one-step-ahead forecasts, and $\\|\\cdot\\|_2$ is the Euclidean norm.\n\n4. Data generation. Simulate three independent datasets of artificial log exchange rates, each bivariate with $k=2$, using the data-generating processes (DGPs) below with the specified parameters, training length $T_{\\text{train}}$, test length $T_{\\text{test}}$, and Gaussian innovations with the given covariance matrices. For each dataset, simulate a total length of $T_{\\text{burn}} + T_{\\text{train}} + T_{\\text{test}}$ observations and discard the first $T_{\\text{burn}}$ as burn-in. Use exactly the seeds given for reproducibility. All entries of matrices and vectors are real numbers.\n\n- Test Case $1$ (true VAR($1$), stationary):\n  - Dimension: $k=2$.\n  - Seed: $314159$.\n  - Parameters: $\\mathbf{c} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$,\n    $\\mathbf{A}_1 = \\begin{bmatrix}0.65 & 0.20 \\\\ -0.10 & 0.55\\end{bmatrix}$.\n  - Innovation covariance: $\\boldsymbol{\\Sigma} = \\begin{bmatrix}0.010 & 0.003 \\\\ 0.003 & 0.015\\end{bmatrix}$.\n  - Sample sizes: $T_{\\text{burn}} = 100$, $T_{\\text{train}} = 300$, $T_{\\text{test}} = 100$.\n\n- Test Case $2$ (true VAR($4$), stationary):\n  - Dimension: $k=2$.\n  - Seed: $271828$.\n  - Parameters: $\\mathbf{c} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$,\n    $\\mathbf{A}_1 = \\begin{bmatrix}0.55 & 0.00 \\\\ 0.05 & 0.45\\end{bmatrix}$,\n    $\\mathbf{A}_2 = \\begin{bmatrix}-0.25 & 0.06 \\\\ 0.00 & -0.15\\end{bmatrix}$,\n    $\\mathbf{A}_3 = \\begin{bmatrix}0.12 & 0.00 \\\\ 0.02 & 0.10\\end{bmatrix}$,\n    $\\mathbf{A}_4 = \\begin{bmatrix}-0.06 & 0.00 \\\\ 0.00 & -0.04\\end{bmatrix}$.\n  - Innovation covariance: $\\boldsymbol{\\Sigma} = \\begin{bmatrix}0.020 & -0.004 \\\\ -0.004 & 0.012\\end{bmatrix}$.\n  - Sample sizes: $T_{\\text{burn}} = 100$, $T_{\\text{train}} = 300$, $T_{\\text{test}} = 100$.\n\n- Test Case $3$ (true random walk in levels, nonstationary):\n  - Dimension: $k=2$.\n  - Seed: $161803$.\n  - Parameters: $\\mathbf{c} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}$,\n    $\\mathbf{A}_1 = \\mathbf{I}_2$ (the $2 \\times 2$ identity), and $\\mathbf{A}_i = \\mathbf{0}$ for all $i \\ge 2$.\n  - Innovation covariance: $\\boldsymbol{\\Sigma} = \\begin{bmatrix}0.005 & 0.0015 \\\\ 0.0015 & 0.004\\end{bmatrix}$.\n  - Sample sizes: $T_{\\text{burn}} = 100$, $T_{\\text{train}} = 300$, $T_{\\text{test}} = 100$.\n\n5. Program requirements. Your program must:\n  - Simulate each dataset exactly as specified using the given seeds.\n  - Estimate a VAR($1$) and a VAR($4$) with an intercept by Ordinary Least Squares (OLS) on the training sample only.\n  - Produce one-step-ahead forecasts over the test sample using fixed estimated parameters and actual lagged values.\n  - Compute the Root Mean Squared Forecast Error (RMSFE) for each of the three models (VAR($1$), VAR($4$), random walk) according to the definition above.\n  - For each test case, determine the index of the best model by RMSFE, using the following index convention: $0$ for VAR($1$), $1$ for VAR($4$), and $2$ for random walk.\n\n6. Final output format. Your program should produce a single line of output containing a comma-separated list of three elements (one per test case), enclosed in square brackets. Each element must itself be a list of four values in the order $\\left[\\text{best\\_index}, \\mathrm{RMSFE}_{\\text{VAR}(1)}, \\mathrm{RMSFE}_{\\text{VAR}(4)}, \\mathrm{RMSFE}_{\\text{RW}}\\right]$. Print all RMSFEs as decimal numbers rounded to six digits after the decimal point, and print best indices as integers. For example, the overall output should look like\n$[\\,[b_1,r_{1,1},r_{4,1},r_{\\mathrm{rw},1}]\\,,\\,[b_2,r_{1,2},r_{4,2},r_{\\mathrm{rw},2}]\\,,\\,[b_3,r_{1,3},r_{4,3},r_{\\mathrm{rw},3}]\\,]$,\nwhere $b_i \\in \\{0,1,2\\}$ and $r_{\\cdot,i}$ are floats rounded to six decimals.\n\nNo physical units or angles are involved in this problem. All numerical answers must be printed exactly as specified in the format above on a single line.", "solution": "The problem statement is valid. It presents a well-defined and self-contained computational experiment in time series econometrics. All parameters, models, and evaluation criteria are specified with sufficient precision to permit a unique and reproducible solution. The underlying principles are standard in the field of computational economics and finance. The task requires the implementation of a simulation study to compare the forecasting performance of Vector Autoregressive (VAR) models against a random walk benchmark.\n\nThe methodology proceeds in four distinct stages for each test case: data generation, model estimation, out-of-sample forecasting, and performance evaluation.\n\n1. Data Generation\nFor each test case, a bivariate time series $\\mathbf{y}_t \\in \\mathbb{R}^2$ of total length $T_{\\text{total}} = T_{\\text{burn}} + T_{\\text{train}} + T_{\\text{test}}$ is simulated from a Vector Autoregressive model of order $p_{\\text{true}}$, given by:\n$$\n\\mathbf{y}_t = \\mathbf{c} + \\sum_{i=1}^{p_{\\text{true}}} \\mathbf{A}_i \\mathbf{y}_{t-i} + \\mathbf{u}_t\n$$\nHere, $\\mathbf{c}$ is the intercept vector, $\\mathbf{A}_i$ are the $k \\times k$ coefficient matrices, and $\\mathbf{u}_t$ is a vector of innovations drawn from a multivariate normal distribution $\\mathcal{N}(\\mathbf{0}, \\boldsymbol{\\Sigma})$. The process is initialized with $\\mathbf{y}_t = \\mathbf{0}$ for $t < p_{\\text{true}}$. A specific seed is used for the random number generator to ensure reproducibility. The first $T_{\\text{burn}} = 100$ observations are discarded to mitigate the influence of initial conditions. The remaining data is split into a training sample of length $T_{\\text{train}} = 300$ and a testing sample of length $T_{\\text{test}} = 100$.\n\n2. Model Estimation\nTwo candidate models, a VAR($1$) and a VAR($4$), are estimated using the training sample. The estimation is performed via Ordinary Least Squares (OLS). For a generic VAR($p$) model, where $p \\in \\{1, 4\\}$, we formulate the system as a multivariate regression:\n$$\n\\mathbf{Y} = \\mathbf{X} \\mathbf{B}^\\top + \\mathbf{U}\n$$\nThe response matrix $\\mathbf{Y}$ is constructed by stacking the observation vectors $\\mathbf{y}_t^\\top$ for $t = p, \\dots, T_{\\text{train}}-1$. It has dimensions $(T_{\\text{train}} - p) \\times k$. The regressor matrix $\\mathbf{X}$ is constructed by stacking the corresponding regressor vectors $\\mathbf{x}_t^\\top = [1, \\mathbf{y}_{t-1}^\\top, \\ldots, \\mathbf{y}_{t-p}^\\top]$ for the same time indices. It has dimensions $(T_{\\text{train}} - p) \\times (1 + kp)$. The matrix $\\mathbf{B} = [\\mathbf{c}, \\mathbf{A}_1, \\ldots, \\mathbf{A}_p]$ contains all model coefficients and has dimensions $k \\times (1 + kp)$. The OLS estimate $\\widehat{\\mathbf{B}}$ is found by solving the normal equations, which can be expressed as:\n$$\n\\widehat{\\mathbf{B}}^\\top = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{Y}\n$$\nNumerically, this is solved using a more stable method like QR decomposition, as implemented in standard linear algebra libraries, which solves for $\\widehat{\\mathbf{B}}^\\top$ in the system $\\mathbf{X} \\widehat{\\mathbf{B}}^\\top = \\mathbf{Y}$. The estimated coefficients $\\widehat{\\mathbf{B}}$ are fixed and used for the entire forecasting exercise.\n\n3. Forecasting\nOne-step-ahead forecasts are generated for the duration of the test sample, from forecast origin $t = T_{\\text{train}}$ to $t = T_{\\text{train}} + T_{\\text{test}} - 1$.\n- For the VAR($p$) models, the forecast for $\\mathbf{y}_{t+1}$ made at time $t$ is:\n$$\n\\widehat{\\mathbf{y}}_{t+1|t} = \\widehat{\\mathbf{c}} + \\sum_{i=1}^{p} \\widehat{\\mathbf{A}}_i \\mathbf{y}_{t+1-i}\n$$\nThis computation uses the fixed estimated coefficients $\\widehat{\\mathbf{c}}$ and $\\widehat{\\mathbf{A}}_i$ from the training phase and the *actual* observed values of the time series for the lagged terms $\\mathbf{y}_{t}, \\mathbf{y}_{t-1}, \\dots$.\n- For the random walk (RW) benchmark model, the forecast is simply the most recent observation:\n$$\n\\widehat{\\mathbf{y}}_{t+1|t} = \\mathbf{y}_t\n$$\n\n4. Evaluation\nThe performance of each model (VAR($1$), VAR($4$), RW) is assessed using the Root Mean Squared Forecast Error (RMSFE). This metric aggregates the forecast errors across all $k$ variables and all $H = T_{\\text{test}}$ forecast horizons in the test sample:\n$$\n\\mathrm{RMSFE} = \\sqrt{\\frac{1}{H k} \\sum_{h=1}^{H} \\left\\|\\mathbf{y}_{T_{\\text{train}}+h} - \\widehat{\\mathbf{y}}_{T_{\\text{train}}+h|T_{\\text{train}}+h-1}\\right\\|_2^2}\n$$\nwhere $\\|\\cdot\\|_2^2$ is the squared Euclidean norm of the vector forecast error. The model with the lowest RMSFE is considered the best for that particular dataset. The program calculates this for each of the three models and identifies the best-performing one by its index: $0$ for VAR($1$), $1$ for VAR($4$), and $2$ for the random walk. The final output is an aggregation of these results for all three specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the computational experiment for three test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"k\": 2,\n            \"seed\": 314159,\n            \"c\": np.array([0.0, 0.0]),\n            \"A_matrices\": [\n                np.array([[0.65, 0.20], [-0.10, 0.55]])\n            ],\n            \"Sigma\": np.array([[0.010, 0.003], [0.003, 0.015]]),\n            \"T_burn\": 100, \"T_train\": 300, \"T_test\": 100,\n        },\n        {\n            \"k\": 2,\n            \"seed\": 271828,\n            \"c\": np.array([0.0, 0.0]),\n            \"A_matrices\": [\n                np.array([[0.55, 0.00], [0.05, 0.45]]),\n                np.array([[-0.25, 0.06], [0.00, -0.15]]),\n                np.array([[0.12, 0.00], [0.02, 0.10]]),\n                np.array([[-0.06, 0.00], [0.00, -0.04]]),\n            ],\n            \"Sigma\": np.array([[0.020, -0.004], [-0.004, 0.012]]),\n            \"T_burn\": 100, \"T_train\": 300, \"T_test\": 100,\n        },\n        {\n            \"k\": 2,\n            \"seed\": 161803,\n            \"c\": np.array([0.0, 0.0]),\n            \"A_matrices\": [np.identity(2)],\n            \"Sigma\": np.array([[0.005, 0.0015], [0.0015, 0.004]]),\n            \"T_burn\": 100, \"T_train\": 300, \"T_test\": 100,\n        }\n    ]\n\n    results_all_cases = []\n\n    for case in test_cases:\n        y_full = _simulate_var(\n            k=case[\"k\"],\n            p_true=len(case[\"A_matrices\"]),\n            c=case[\"c\"],\n            A_matrices=case[\"A_matrices\"],\n            Sigma=case[\"Sigma\"],\n            T_total=case[\"T_burn\"] + case[\"T_train\"] + case[\"T_test\"],\n            seed=case[\"seed\"]\n        )\n        \n        y = y_full[case[\"T_burn\"]:]\n        y_train = y[:case[\"T_train\"]]\n\n        # Estimate VAR(1) and VAR(4) models\n        B_hat_1 = _estimate_var(y_train, p=1)\n        B_hat_4 = _estimate_var(y_train, p=4)\n\n        # Evaluate performance\n        rmsfe_var1 = _forecast_and_evaluate(y, case[\"T_train\"], p=1, B_hat=B_hat_1, model_type='VAR')\n        rmsfe_var4 = _forecast_and_evaluate(y, case[\"T_train\"], p=4, B_hat=B_hat_4, model_type='VAR')\n        rmsfe_rw = _forecast_and_evaluate(y, case[\"T_train\"], p=0, B_hat=None, model_type='RW')\n        \n        rmsfes = [rmsfe_var1, rmsfe_var4, rmsfe_rw]\n        best_index = int(np.argmin(rmsfes))\n\n        # Format results for the current case\n        case_result_str = f'[{best_index},' + ','.join([f'{r:.6f}' for r in rmsfes]) + ']'\n        results_all_cases.append(case_result_str)\n\n    # Final print statement\n    print(f\"[{','.join(results_all_cases)}]\")\n\ndef _simulate_var(k, p_true, c, A_matrices, Sigma, T_total, seed):\n    \"\"\"Simulates data from a VAR(p) process.\"\"\"\n    rng = np.random.default_rng(seed)\n    y = np.zeros((T_total, k))\n    u = rng.multivariate_normal(np.zeros(k), Sigma, size=T_total)\n\n    for t in range(p_true, T_total):\n        y_t = c.copy()\n        for i in range(1, p_true + 1):\n            y_t += A_matrices[i-1] @ y[t-i]\n        y[t] = y_t + u[t]\n    return y\n\ndef _estimate_var(y_train, p):\n    \"\"\"Estimates a VAR(p) model with an intercept using OLS.\"\"\"\n    T_train, k = y_train.shape\n    num_obs = T_train - p\n    \n    Y = y_train[p:]\n    X = np.zeros((num_obs, 1 + k * p))\n    \n    for t in range(p, T_train):\n        regressor_row = [1.0]\n        for i in range(1, p + 1):\n            regressor_row.extend(y_train[t - i])\n        X[t - p, :] = regressor_row\n    \n    B_T, _, _, _ = np.linalg.lstsq(X, Y, rcond=None)\n    \n    return B_T.T\n\ndef _forecast_and_evaluate(y, T_train, p, B_hat, model_type):\n    \"\"\"Generates one-step-ahead forecasts and computes RMSFE.\"\"\"\n    T_total, k = y.shape\n    T_test = T_total - T_train\n    \n    squared_errors_sum = 0.0\n\n    for h in range(T_test):\n        # Forecast origin is t = T_train + h - 1\n        t = T_train + h - 1\n        actual_y = y[t + 1]\n\n        if model_type == 'RW':\n            forecast_y = y[t]\n        elif model_type == 'VAR':\n            x_t = [1.0]\n            for i in range(p):\n                x_t.extend(y[t - i])\n            x_t_vec = np.array(x_t)\n            forecast_y = B_hat @ x_t_vec\n        else:\n            raise ValueError(\"Unknown model_type\")\n\n        forecast_error = actual_y - forecast_y\n        squared_errors_sum += np.sum(forecast_error**2)\n    \n    rmsfe = np.sqrt(squared_errors_sum / (T_test * k))\n    return rmsfe\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "2447495"}, {"introduction": "Beyond forecasting, VAR models are frequently used to investigate causal relationships between variables using tests for Granger causality. However, a statistically significant Granger-causal relationship in an estimated model does not always imply a true causal link. This practice explores a critical pitfall: spurious causality arising from omitted variable bias [@problem_id:2447550]. You will simulate a system where two observed variables are driven by a common, unobserved (latent) factor and demonstrate how estimating a VAR without this factor can lead to the false conclusion that one observed variable Granger-causes the other.", "problem": "You are given a latent-driven trivariate linear data-generating process designed to illustrate omitted-variable bias in Vector Autoregression (VAR) estimation. A Vector Autoregression (VAR) is a system of linear stochastic difference equations describing the joint dynamics of multiple time series. Consider three processes $\\{Z_t\\}$, $\\{X_t\\}$, and $\\{Y_t\\}$ generated by\n$$Z_t = \\rho_Z Z_{t-1} + u_t,$$\n$$X_t = \\phi_X X_{t-1} + b_X Z_{t-1} + e_t,$$\n$$Y_t = \\phi_Y Y_{t-1} + b_Y Z_{t-1} + v_t,$$\nwhere $u_t$, $e_t$, and $v_t$ are mutually independent, serially independent, mean-zero, Gaussian shocks with variances $\\sigma_u^2$, $\\sigma_e^2$, and $\\sigma_v^2$, respectively. In the full-information trivariate system, $X_t$ does not Granger-cause $Y_t$ because there is no direct lag of $X_t$ entering the law of motion for $Y_t$. Granger causality is defined as follows: process $X_t$ Granger-causes process $Y_t$ if past values of $X_t$ improve the mean squared prediction of $Y_t$ at horizon $1$ beyond the information contained in the past of $Y_t$ and all other relevant processes.\n\nHowever, in practice the latent process $Z_t$ is omitted, and a bivariate VAR is estimated on $(X_t, Y_t)$ only. Your task is to demonstrate how this omission can induce spurious Granger-causal relationships in the estimated bivariate system.\n\nFundamental base:\n- Use the definition of Granger causality in terms of linear predictability, and Ordinary Least Squares (OLS) estimation as the best linear unbiased estimator under the Gauss–Markov conditions. Ordinary Least Squares (OLS) relies on solving the normal equations to minimize the sum of squared residuals.\n- For nested linear models under Gaussian disturbances, the classical F test derived from the ratio of mean squared residual sums provides a valid test for linear exclusion restrictions at finite sample sizes based on the Fisher–Snedecor distribution (F).\n\nInstructions:\n- For each test case below, simulate data from the trivariate system with the specified parameters. Use a burn-in of $B = 300$ observations that are discarded before analysis to mitigate initialization effects. Initialize $Z_0$, $X_0$, and $Y_0$ at $0$.\n- For the observed bivariate data $(X_t, Y_t)$ after burn-in, estimate a bivariate VAR of order $p = 1$ with an intercept by OLS, equation-by-equation.\n- In the $Y$-equation, test the null that $X$ does not Granger-cause $Y$ in the observed bivariate system. This null imposes that all coefficients on the $p$ lags of $X$ in the $Y$-equation are equal to $0$. Construct the standard nested-model F-statistic comparing the unrestricted $Y$-equation (including the lag(s) of $X$) to the restricted $Y$-equation (excluding the lag(s) of $X$), and compute the $p$-value using the cumulative distribution function of the Fisher–Snedecor distribution with the appropriate numerator and denominator degrees of freedom implied by the number of exclusion restrictions and the unrestricted residual degrees of freedom. Reject the null at significance level $\\alpha = 0.05$ if and only if the $p$-value is strictly less than $\\alpha$.\n- For each test case, output a boolean indicating whether the null “$X$ does not Granger-cause $Y$” is rejected in the observed bivariate VAR.\n\nTest suite:\n- Case $1$ (high latent persistence; “happy path” for spurious detection):\n  - $T = 1000$, $p = 1$, $\\alpha = 0.05$, $\\rho_Z = 0.95$, $\\phi_X = 0.2$, $\\phi_Y = 0.2$, $b_X = 1.5$, $b_Y = 1.5$, $\\sigma_u = 0.5$, $\\sigma_e = 0.5$, $\\sigma_v = 0.5$, seed $= 123456$.\n- Case $2$ (latent white noise; boundary with minimal spurious predictability from $X_{t-1}$ to $Y_t$):\n  - $T = 600$, $p = 1$, $\\alpha = 0.05$, $\\rho_Z = 0.0$, $\\phi_X = 0.4$, $\\phi_Y = 0.6$, $b_X = 1.0$, $b_Y = 1.0$, $\\sigma_u = 1.0$, $\\sigma_e = 1.0$, $\\sigma_v = 1.0$, seed $= 20231102$.\n- Case $3$ (small sample; edge case for low power and sampling variability):\n  - $T = 120$, $p = 1$, $\\alpha = 0.05$, $\\rho_Z = 0.9$, $\\phi_X = 0.6$, $\\phi_Y = 0.6$, $b_X = 1.2$, $b_Y = 0.8$, $\\sigma_u = 0.8$, $\\sigma_e = 0.8$, $\\sigma_v = 0.8$, seed $= 7$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases listed above, where each entry is a boolean corresponding to whether the null “$X$ does not Granger-cause $Y$” is rejected. For example, an output with three cases should look like $[{\\rm True},{\\rm False},{\\rm True}]$.", "solution": "The problem requires an investigation into the phenomenon of spurious Granger causality that arises from omitted variable bias in a Vector Autoregression (VAR) model. We are provided with a trivariate linear stochastic process consisting of two observed variables, $X_t$ and $Y_t$, and one unobserved (latent) common driver, $Z_t$. The task is to simulate this system under specified parameter sets and test for Granger causality from $X_t$ to $Y_t$ in a misspecified bivariate VAR that omits $Z_t$.\n\nThe data generating process (DGP) is defined by the following system of equations:\n$$Z_t = \\rho_Z Z_{t-1} + u_t$$\n$$X_t = \\phi_X X_{t-1} + b_X Z_{t-1} + e_t$$\n$$Y_t = \\phi_Y Y_{t-1} + b_Y Z_{t-1} + v_t$$\nwhere $u_t$, $e_t$, and $v_t$ are mutually independent, serially uncorrelated Gaussian white noise processes with mean $0$ and variances $\\sigma_u^2$, $\\sigma_e^2$, and $\\sigma_v^2$, respectively.\n\nIn this true structural model, the evolution of $Y_t$ depends only on its own lag, $Y_{t-1}$, and the lag of the latent process, $Z_{t-1}$. There is no term involving a lag of $X_t$ in the equation for $Y_t$. Therefore, in the context of the full trivariate system, $X_t$ does not Granger-cause $Y_t$.\n\nHowever, the econometrician does not observe $Z_t$ and proceeds to estimate a bivariate VAR model of order $p=1$ on the observed data $(X_t, Y_t)$. The equation for $Y_t$ in this misspecified model is:\n$$Y_t = c_Y + \\beta_{Y,1} Y_{t-1} + \\beta_{X,1} X_{t-1} + \\epsilon_t$$\nwhere $\\epsilon_t$ is the new error term. The test for Granger causality from $X_t$ to $Y_t$ is a test of the null hypothesis $H_0: \\beta_{X,1} = 0$.\n\nSpurious Granger causality occurs if we reject this null hypothesis, not because $X_{t-1}$ has genuine predictive content for $Y_t$ (after conditioning on $Y_{t-1}$), but because of the model's misspecification. This is a classic case of omitted variable bias. The Ordinary Least Squares (OLS) estimator for $\\beta_{X,1}$, denoted $\\hat{\\beta}_{X,1}$, will be biased if the included regressor $X_{t-1}$ is correlated with the omitted variable $Z_{t-1}$, which is part of the true error term of the misspecified regression.\n\nThe true model for $Y_t$ can be expressed as $Y_t = \\phi_Y Y_{t-1} + (b_Y Z_{t-1} + v_t)$. When we estimate the bivariate model, the term $b_Y Z_{t-1}$ is omitted and becomes part of the error term $\\epsilon_t$. For the OLS estimator $\\hat{\\beta}_{X,1}$ to be biased, two conditions must be met:\n$1$. The omitted variable $Z_{t-1}$ must be a determinant of $Y_t$. In our DGP, this holds if $b_Y \\neq 0$.\n$2$. The omitted variable $Z_{t-1}$ must be correlated with the included regressor $X_{t-1}$, i.e., $\\mathrm{Cov}(X_{t-1}, Z_{t-1}) \\neq 0$.\n\nLet us analyze this covariance, assuming the processes are stationary. From the DGP, we have $X_{t-1} = \\phi_X X_{t-2} + b_X Z_{t-2} + e_{t-1}$ and $Z_{t-1} = \\rho_Z Z_{t-2} + u_{t-1}$. Taking the covariance of $X_{t-1}$ and $Z_{t-1}$ (and assuming zero means), we find:\n$$\\mathrm{E}[X_{t-1} Z_{t-1}] = \\mathrm{E}[(\\phi_X X_{t-2} + b_X Z_{t-2} + e_{t-1})(\\rho_Z Z_{t-2} + u_{t-1})]$$\nDue to the independence of shocks, this simplifies to:\n$$\\mathrm{E}[X_{t-1} Z_{t-1}] = \\phi_X \\rho_Z \\mathrm{E}[X_{t-2}Z_{t-2}] + b_X \\rho_Z \\mathrm{E}[Z_{t-2}^2]$$\nLet $\\Gamma_{XZ} = \\mathrm{E}[X_t Z_t]$ and $\\gamma_Z(0) = \\mathrm{E}[Z_t^2]$. In steady state, we have $\\Gamma_{XZ} = \\phi_X \\rho_Z \\Gamma_{XZ} + b_X \\rho_Z \\gamma_Z(0)$, which implies $\\Gamma_{XZ} = \\frac{b_X \\rho_Z}{1 - \\phi_X \\rho_Z} \\gamma_Z(0)$.\nSince $\\gamma_Z(0) = \\sigma_u^2 / (1 - \\rho_Z^2) > 0$, the covariance $\\mathrm{Cov}(X_{t-1}, Z_{t-1})$ is non-zero if and only if $b_X \\neq 0$ and $\\rho_Z \\neq 0$.\n\nTherefore, spurious Granger causality from $X_t$ to $Y_t$ is expected to arise when $b_X \\neq 0$, $b_Y \\neq 0$, and $\\rho_Z \\neq 0$. The regressor $X_{t-1}$ acts as a proxy for the omitted variable $Z_{t-1}$, and its coefficient $\\beta_{X,1}$ spuriously picks up the influence of $Z_{t-1}$ on $Y_t$. If $\\rho_Z=0$, $Z_t$ is white noise, and $Z_{t-1}$ is uncorrelated with $X_{t-1}$, breaking the mechanism for bias.\n\nThe procedure is to perform a hypothesis test for each case. We generate $T+B$ data points and discard the first $B=300$ for burn-in. On the remaining $T$ observations, we estimate the $Y$-equation of the bivariate VAR(1) using OLS. This requires $T-1$ effective observations. The unrestricted model is $Y_t = c + \\beta_1 Y_{t-1} + \\beta_2 X_{t-1} + \\epsilon_t$, and the restricted model under $H_0: \\beta_2=0$ is $Y_t = c' + \\beta'_1 Y_{t-1} + \\epsilon'_t$.\n\nWe compute the F-statistic using the residual sums of squares ($RSS$) from both regressions:\n$$F = \\frac{(RSS_R - RSS_U) / q}{RSS_U / (N_{reg} - k)}$$\nHere, $RSS_R$ and $RSS_U$ are the RSS for the restricted and unrestricted models, respectively. The number of regression observations is $N_{reg} = T-1$. The number of restrictions is $q=1$. The number of parameters in the unrestricted model is $k=3$ (intercept, one lag of $Y$, one lag of $X$). The degrees of freedom for the F-distribution are $q=1$ for the numerator and $N_{reg} - k = (T-1) - 3 = T-4$ for the denominator.\n\nThe p-value is calculated from the cumulative distribution function (CDF) of the $F_{1, T-4}$ distribution. The null hypothesis is rejected at significance level $\\alpha=0.05$ if the p-value is less than $0.05$.\n\n- Case $1$: High persistence in the latent variable ($\\rho_Z = 0.95$) and a large sample size ($T=1000$). All conditions for spurious causality are strongly met. Rejection of the null is expected.\n- Case $2$: The latent variable is white noise ($\\rho_Z = 0.0$). The mechanism for omitted variable bias is absent. The test should not reject the null, barring a Type I error.\n- Case $3$: High latent persistence ($\\rho_Z = 0.9$) but a small sample size ($T=120$). The bias is present, but the statistical power of the test may be insufficient to detect it. The outcome demonstrates the interplay between bias magnitude and sample size.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import f as f_dist\n\ndef solve():\n    \"\"\"\n    Main function to run test cases for spurious Granger causality.\n    \"\"\"\n    test_cases = [\n        # Case 1 (high latent persistence)\n        {\n            \"T\": 1000, \"p\": 1, \"alpha\": 0.05, \"rho_Z\": 0.95, \"phi_X\": 0.2, \"phi_Y\": 0.2,\n            \"b_X\": 1.5, \"b_Y\": 1.5, \"sigma_u\": 0.5, \"sigma_e\": 0.5, \"sigma_v\": 0.5,\n            \"seed\": 123456\n        },\n        # Case 2 (latent white noise)\n        {\n            \"T\": 600, \"p\": 1, \"alpha\": 0.05, \"rho_Z\": 0.0, \"phi_X\": 0.4, \"phi_Y\": 0.6,\n            \"b_X\": 1.0, \"b_Y\": 1.0, \"sigma_u\": 1.0, \"sigma_e\": 1.0, \"sigma_v\": 1.0,\n            \"seed\": 20231102\n        },\n        # Case 3 (small sample)\n        {\n            \"T\": 120, \"p\": 1, \"alpha\": 0.05, \"rho_Z\": 0.9, \"phi_X\": 0.6, \"phi_Y\": 0.6,\n            \"b_X\": 1.2, \"b_Y\": 0.8, \"sigma_u\": 0.8, \"sigma_e\": 0.8, \"sigma_v\": 0.8,\n            \"seed\": 7\n        }\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = run_granger_test(case_params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_granger_test(params):\n    \"\"\"\n    Simulates data and performs the Granger causality F-test for a single case.\n\n    Args:\n        params (dict): A dictionary of parameters for the simulation and test.\n\n    Returns:\n        bool: True if the null hypothesis is rejected, False otherwise.\n    \"\"\"\n    # Unpack parameters\n    T = params[\"T\"]\n    p = params[\"p\"]\n    alpha = params[\"alpha\"]\n    rho_Z = params[\"rho_Z\"]\n    phi_X = params[\"phi_X\"]\n    phi_Y = params[\"phi_Y\"]\n    b_X = params[\"b_X\"]\n    b_Y = params[\"b_Y\"]\n    sigma_u = params[\"sigma_u\"]\n    sigma_e = params[\"sigma_e\"]\n    sigma_v = params[\"sigma_v\"]\n    seed = params[\"seed\"]\n    \n    B = 300  # Burn-in period\n    T_total = T + B\n\n    # Set seed for reproducibility\n    np.random.seed(seed)\n\n    # Generate shocks\n    u = np.random.normal(0, sigma_u, T_total)\n    e = np.random.normal(0, sigma_e, T_total)\n    v = np.random.normal(0, sigma_v, T_total)\n\n    # Initialize time series arrays\n    Z = np.zeros(T_total)\n    X = np.zeros(T_total)\n    Y = np.zeros(T_total)\n\n    # Simulate the trivariate system\n    for t in range(1, T_total):\n        Z[t] = rho_Z * Z[t-1] + u[t]\n        X[t] = phi_X * X[t-1] + b_X * Z[t-1] + e[t]\n        Y[t] = phi_Y * Y[t-1] + b_Y * Z[t-1] + v[t]\n\n    # Discard burn-in period\n    X_sample = X[B:]\n    Y_sample = Y[B:]\n\n    # Prepare data for regression (VAR order p=1)\n    # Effective sample size for regression is T-p\n    y_vec = Y_sample[p:]\n    N_reg = len(y_vec)\n\n    # Regressors for the unrestricted model: intercept, Y_lag1, X_lag1\n    X_unrestricted = np.vstack([\n        np.ones(N_reg),\n        Y_sample[p-1:-1],\n        X_sample[p-1:-1]\n    ]).T\n\n    # Regressors for the restricted model: intercept, Y_lag1\n    X_restricted = np.vstack([\n        np.ones(N_reg),\n        Y_sample[p-1:-1]\n    ]).T\n\n    # OLS estimation via np.linalg.lstsq\n    # lstsq returns: coefficients, residuals (sum of squares), rank, singular values\n    # We only need the residual sum of squares (RSS)\n    _, rss_unrestricted, _, _ = np.linalg.lstsq(X_unrestricted, y_vec, rcond=None)\n    _, rss_restricted, _, _ = np.linalg.lstsq(X_restricted, y_vec, rcond=None)\n    \n    # lstsq returns RSS as a one-element array, so we extract the float\n    rss_u = rss_unrestricted[0]\n    rss_r = rss_restricted[0]\n\n    # Compute the F-statistic\n    q = X_unrestricted.shape[1] - X_restricted.shape[1]\n    k_unrestricted = X_unrestricted.shape[1]\n    df_num = q\n    df_den = N_reg - k_unrestricted\n    \n    # Check for df_den > 0 to avoid division by zero\n    if df_den <= 0:\n        return False # Cannot perform test\n\n    F_statistic = ((rss_r - rss_u) / df_num) / (rss_u / df_den)\n\n    # Compute the p-value using the survival function (1 - CDF)\n    p_value = f_dist.sf(F_statistic, dfn=df_num, dfd=df_den)\n\n    # Reject null if p-value is less than the significance level\n    return p_value < alpha\n\n# Run the simulation and print the results\nsolve()\n```", "id": "2447550"}, {"introduction": "While reduced-form VARs are excellent for forecasting, they cannot identify the underlying 'structural' shocks that drive the economy, as their error terms are typically contemporaneously correlated. To perform policy analysis or test economic theories, we must move to a Structural VAR (SVAR). This advanced exercise guides you through one of the most influential methods for identifying structural shocks: imposing long-run restrictions [@problem_id:2447527]. You will implement the Blanchard-Quah identification scheme, which uses economic theory—in this case, the assumption that a nominal shock has no long-run effect on a real variable—to uncover the structural shocks from the estimated model.", "problem": "Consider a bivariate stationary Vector Autoregression (VAR) of order one, defined for the vector of variables $x_t \\in \\mathbb{R}^2$ by\n$$\nx_t = A x_{t-1} + u_t,\n$$\nwhere $A \\in \\mathbb{R}^{2 \\times 2}$ is a parameter matrix with spectral radius strictly less than $1$, and $u_t$ is a mean-zero innovation with positive-definite covariance matrix $\\Sigma_u \\in \\mathbb{R}^{2 \\times 2}$. Suppose there exists a structural representation with structural shocks $e_t \\in \\mathbb{R}^2$ satisfying $u_t = B e_t$ and $\\mathbb{E}[e_t e_t^\\top] = I_2$, where $B \\in \\mathbb{R}^{2 \\times 2}$ is the structural impact matrix and $I_2$ is the identity matrix of size $2$. Let the two components of $x_t$ be interpreted as a real variable (first component) and a nominal variable (second component), for example, real output and money supply.\n\nDefine the long-run cumulative effect matrix as\n$$\n\\Psi(1) = \\sum_{h=0}^{\\infty} A^h B,\n$$\nwhich is well-defined because the spectral radius of $A$ is strictly less than $1$. Impose the long-run restriction that a nominal structural shock (the second component of $e_t$) has no long-run effect on the real variable (the first component of $x_t$), that is,\n$$\n\\left[\\Psi(1)\\right]_{1,2} = 0.\n$$\nAdditionally, enforce that $B$ reproduces the reduced-form covariance, that is,\n$$\n\\Sigma_u = B B^\\top,\n$$\nand adopt the sign normalization\n$$\nB_{1,1} > 0 \\quad \\text{and} \\quad B_{2,2} > 0,\n$$\nto eliminate sign indeterminacy in the structural shocks.\n\nYour task is to compute, for each parameter set provided below, a matrix $B$ and the associated diagnostics implied by the above restrictions. The long-run matrix $\\Psi(1)$ must be determined from the given $A$ and your computed $B$.\n\nTest suite (each case specifies $A$ and $\\Sigma_u$):\n- Case $1$ (happy path):\n$$\nA^{(1)} = \\begin{bmatrix} 0.3 & 0.1 \\\\ 0.05 & 0.2 \\end{bmatrix}, \\quad\n\\Sigma_u^{(1)} = \\begin{bmatrix} 0.5 & 0.1 \\\\ 0.1 & 0.3 \\end{bmatrix}.\n$$\n- Case $2$ (boundary with zero dynamics):\n$\nA^{(2)} = \\begin{bmatrix} 0.0 & 0.0 \\\\ 0.0 & 0.0 \\end{bmatrix}, \\quad\n\\Sigma_u^{(2)} = \\begin{bmatrix} 1.0 & 0.2 \\\\ 0.2 & 2.0 \\end{bmatrix}.\n$\n- Case $3$ (high persistence, but stable):\n$\nA^{(3)} = \\begin{bmatrix} 0.95 & -0.1 \\\\ 0.05 & 0.9 \\end{bmatrix}, \\quad\n\\Sigma_u^{(3)} = \\begin{bmatrix} 0.2 & -0.05 \\\\ -0.05 & 0.4 \\end{bmatrix}.\n$\n\nFor each case $k \\in \\{1,2,3\\}$, compute:\n- The scalar $[\\Psi^{(k)}(1)]_{1,2}$.\n- The Frobenius norm $\\|B^{(k)} (B^{(k)})^\\top - \\Sigma_u^{(k)}\\|_F$.\n- The entries of $B^{(k)}$ in row-major order: $B^{(k)}_{1,1}$, $B^{(k)}_{1,2}$, $B^{(k)}_{2,1}$, $B^{(k)}_{2,2}$.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of three elements, one per test case. Each element must itself be a list of six floating-point numbers in the order\n$\n\\big([\\Psi(1)]_{1,2}, \\ \\|B B^\\top - \\Sigma_u\\|_F, \\ B_{1,1}, \\ B_{1,2}, \\ B_{2,1}, \\ B_{2,2}\\big),\n$\nwith each number rounded to six decimal places.\n- Concretely, the output must be a single line in the form\n$\n\\big[ [r^{(1)}_1, r^{(1)}_2, r^{(1)}_3, r^{(1)}_4, r^{(1)}_5, r^{(1)}_6], [r^{(2)}_1, \\ldots, r^{(2)}_6], [r^{(3)}_1, \\ldots, r^{(3)}_6] \\big],\n$\nwhere each $r^{(k)}_j$ is a float rounded to six decimals.\n\nNo external input is required, and no physical units are involved. Angles, where used internally, must be in radians. The program must be self-contained and runnable as provided.", "solution": "The task is to identify the structural impact matrix $B$ of a bivariate VAR($1$) model using a long-run restriction. This is a classic identification problem in structural econometrics. We proceed by first principles.\n\nThe model is given by $x_t = A x_{t-1} + u_t$, where $u_t = B e_t$ and $\\mathbb{E}[e_t e_t^\\top] = I_2$. The covariance of the reduced-form residuals is $\\Sigma_u = \\mathbb{E}[u_t u_t^\\top] = \\mathbb{E}[B e_t e_t^\\top B^\\top] = B \\mathbb{E}[e_t e_t^\\top] B^\\top = B I_2 B^\\top = B B^\\top$. This is the covariance restriction.\n\nThe vector moving average (VMA) representation of $x_t$ is obtained by recursive substitution:\n$$\nx_t = \\sum_{h=0}^{\\infty} A^h u_{t-h} = \\sum_{h=0}^{\\infty} A^h B e_{t-h}\n$$\nThe matrix of impulse responses at horizon $h$ is $A^h B$. The long-run cumulative effect matrix, $\\Psi(1)$, is the sum of all impulse response matrices:\n$$\n\\Psi(1) = \\sum_{h=0}^{\\infty} A^h B\n$$\nSince the spectral radius $\\rho(A) < 1$, the matrix geometric series converges to $\\Psi(1) = (I_2 - A)^{-1} B$. Let $C \\equiv (I_2 - A)^{-1}$. The long-run matrix is $\\Psi(1) = C B$.\n\nThe identification strategy relies on two sets of restrictions to uniquely determine $B$.\n\n1.  **Covariance Restriction**: $B B^\\top = \\Sigma_u$. This equation implies that $B$ is a matrix square root of the positive-definite matrix $\\Sigma_u$. Any matrix $B$ satisfying this relation can be parameterized as $B = P_0 Q$, where $P_0$ is a specific matrix square root of $\\Sigma_u$ (for definiteness, we choose the lower-triangular Cholesky factor, $P_0 P_0^\\top = \\Sigma_u$ with $P_{0,ii} > 0$), and $Q$ is an arbitrary $2 \\times 2$ orthogonal matrix ($Q Q^\\top = I_2$). The task reduces to finding the correct $Q$.\n\n2.  **Long-Run Restriction**: $[\\Psi(1)]_{1,2} = 0$. Substituting the expressions for $\\Psi(1)$ and $B$, we have:\n    $$\n    [C(P_0 Q)]_{1,2} = 0\n    $$\n    Let $S = C P_0$. The restriction becomes $[S Q]_{1,2} = 0$. A general $2 \\times 2$ rotation matrix is given by:\n    $$\n    Q(\\theta) = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}\n    $$\n    Let $s_{1}^\\top$ be the first row of $S$, $s_1^\\top = [S_{11}, S_{12}]$, and let $q_2$ be the second column of $Q$, $q_2 = [-\\sin\\theta, \\cos\\theta]^\\top$. The restriction is the dot product $s_1^\\top q_2 = 0$:\n    $$\n    -S_{11} \\sin\\theta + S_{12} \\cos\\theta = 0\n    $$\n    This implies $\\tan\\theta = S_{12} / S_{11}$, provided $S_{11} \\neq 0$. This equation determines the angle $\\theta$ up to multiples of $\\pi$, which yields two candidate rotation matrices, $Q(\\theta)$ and $Q(\\theta+\\pi) = -Q(\\theta)$. For computational purposes, we can uniquely determine $\\theta \\in (-\\pi, \\pi]$ using the two-argument arctangent function: $\\theta = \\operatorname{arctan2}(S_{12}, S_{11})$. This gives a unique $Q$.\n\n3.  **Sign Normalization**: $B_{1,1} > 0$ and $B_{2,2} > 0$.\n    We start with an initial solution $B_{init} = P_0 Q(\\theta)$. The covariance and long-run restrictions are satisfied by any matrix of the form $[\\pm b_1, \\pm b_2]$, where $B_{init}=[b_1, b_2]$. We must choose the column signs to satisfy the normalization. Let $B^{(0)} = P_0 Q(\\theta)$. We construct the final matrix $B$ as follows:\n    - Set the first column $B_{:,1}$. If $B^{(0)}_{1,1} < 0$, we set $B_{:,1} = -B^{(0)}_{:,1}$. Otherwise, $B_{:,1} = B^{(0)}_{:,1}$. This ensures $B_{1,1} \\geq 0$.\n    - Set the second column $B_{:,2}$. Using the (potentially sign-flipped) first column, we form a new intermediate matrix. If its $(2,2)$ element is negative, we flip the sign of the second column $B^{(0)}_{:,2}$ to form $B_{:,2}$. This ensures $B_{2,2} \\geq 0$.\n    The strict inequalities $B_{1,1} > 0, B_{2,2} > 0$ imply that neither diagonal element will be zero, which holds for the provided test cases.\n\nThe computational algorithm is as follows:\nFor each test case $(A, \\Sigma_u)$:\n1.  Calculate $C = (I_2 - A)^{-1}$.\n2.  Compute the lower-triangular Cholesky factor $P_0$ of $\\Sigma_u$ such that $P_0 P_0^\\top = \\Sigma_u$.\n3.  Calculate the matrix $S = C P_0$.\n4.  Determine the rotation angle $\\theta = \\operatorname{arctan2}(S_{0,1}, S_{0,0})$.\n5.  Construct the rotation matrix $Q(\\theta)$.\n6.  Calculate the initial candidate matrix $B_{init} = P_0 Q(\\theta)$.\n7.  Adjust column signs of $B_{init}$ to obtain the final matrix $B$ that satisfies $B_{1,1} > 0$ and $B_{2,2} > 0$.\n8.  Compute the diagnostics:\n    - The matrix of long-run effects $\\Psi(1) = C B$ and extract its $(1,2)$ element, $[\\Psi(1)]_{1,2}$.\n    - The Frobenius norm of the reconstruction error, $\\|B B^\\top - \\Sigma_u\\|_F$.\n    - The elements of $B$ in row-major order.\nBoth diagnostics $[\\Psi(1)]_{1,2}$ and $\\|B B^\\top - \\Sigma_u\\|_F$ should be numerically close to zero, serving as a check of the implementation's correctness.", "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Solves for the structural impact matrix B in a bivariate VAR(1) model\n    using Blanchard-Quah long-run restrictions for several test cases.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"A\": np.array([[0.3, 0.1], [0.05, 0.2]]),\n            \"Sigma_u\": np.array([[0.5, 0.1], [0.1, 0.3]]),\n        },\n        # Case 2 (boundary with zero dynamics)\n        {\n            \"A\": np.array([[0.0, 0.0], [0.0, 0.0]]),\n            \"Sigma_u\": np.array([[1.0, 0.2], [0.2, 2.0]]),\n        },\n        # Case 3 (high persistence, but stable)\n        {\n            \"A\": np.array([[0.95, -0.1], [0.05, 0.9]]),\n            \"Sigma_u\": np.array([[0.2, -0.05], [-0.05, 0.4]]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        Sigma_u = case[\"Sigma_u\"]\n        K = A.shape[0]\n\n        # Step 1: Compute C = (I - A)^-1\n        I = np.identity(K)\n        C = scipy.linalg.inv(I - A)\n\n        # Step 2: Compute lower Cholesky factor P0 of Sigma_u\n        # P0 P0.T = Sigma_u\n        P0 = scipy.linalg.cholesky(Sigma_u, lower=True)\n\n        # Step 3: Compute S = C * P0\n        S = C @ P0\n\n        # Step 4: Determine the rotation angle from the long-run restriction\n        # [S*Q]_12 = -S_11*sin(theta) + S_12*cos(theta) = 0\n        # tan(theta) = S_12 / S_11\n        theta = np.arctan2(S[0, 1], S[0, 0])\n\n        # Step 5: Construct the rotation matrix Q\n        c, s = np.cos(theta), np.sin(theta)\n        Q = np.array([[c, -s], [s, c]])\n\n        # Step 6: Compute initial B matrix\n        B_init = P0 @ Q\n\n        # Step 7: Apply sign normalization B_11 > 0 and B_22 > 0\n        B = B_init.copy()\n        # Ensure B[0, 0] > 0\n        if B[0, 0] < 0:\n            B[:, 0] *= -1\n        # Ensure B[1, 1] > 0\n        if B[1, 1] < 0:\n            B[:, 1] *= -1\n\n        # Step 8: Compute diagnostics\n        # Long-run effect matrix Psi(1) = C * B\n        Psi1 = C @ B\n        # Extract the restricted element [Psi(1)]_12\n        psi1_12 = Psi1[0, 1]\n\n        # Frobenius norm of the covariance reconstruction error\n        recon_error = scipy.linalg.norm(B @ B.T - Sigma_u, 'fro')\n\n        # Elements of B in row-major order\n        b_elements = B.flatten().tolist()\n\n        # Compile results for the current case\n        case_result = [psi1_12, recon_error] + b_elements\n        results.append(case_result)\n\n    # Format the final output string as specified\n    output_parts = []\n    for r in results:\n        formatted_r = [f\"{x:.6f}\" for x in r]\n        output_parts.append(f\"[{','.join(formatted_r)}]\")\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "2447527"}]}