{"hands_on_practices": [{"introduction": "A cornerstone of the Box-Jenkins methodology is transforming a non-stationary time series into a stationary one before modeling its correlation structure. This practice challenges you to determine the order of integration, denoted $I(d)$, for several canonical time series processes by implementing a sequential unit root testing procedure. Through this coding exercise [@problem_id:2378207], you will gain practical experience with the crucial first step of any ARIMA analysis.", "problem": "You are given the task of determining the smallest order of differencing that renders a given time series covariance-stationary, following the Box–Jenkins methodology in the context of Autoregressive Integrated Moving Average (ARIMA) modeling. Let $\\{y_t\\}_{t=0}^{T}$ be a real-valued discrete-time stochastic process. Define the backward difference operator $\\Delta$ by $\\Delta y_t = y_t - y_{t-1}$ and its $d$-fold iteration $\\Delta^d y_t$ by applying $\\Delta$ a total of $d$ times. A series is said to be integrated of order $d$ (denoted $I(d)$) if $\\Delta^d y_t$ is covariance-stationary and $\\Delta^{d-1} y_t$ is not, for the smallest nonnegative integer $d$.\n\nYour objective is to write a program that, for each of the following data-generating processes (DGPs), simulates a sample path and determines the smallest integer $d \\in \\{0,1,2\\}$ such that $\\Delta^d y_t$ is covariance-stationary. For economic interpretation, consider the first DGP as a stylized representation of the velocity of money (broad liquidity version, commonly labeled M2 velocity).\n\nAll processes use independent and identically distributed Gaussian innovations $\\{\\varepsilon_t\\}$ with $\\varepsilon_t \\sim \\mathcal{N}(0,\\sigma^2)$, and initial conditions as specified. Unless otherwise stated, take $y_0 = 0$. Use the stated seeds to ensure reproducibility.\n\nTest suite of DGPs to simulate and analyze:\n1. Random walk with drift (stylized M2V):\n   - Dynamics: $y_t = y_{t-1} + c + \\varepsilon_t$ for $t \\in \\{1,\\dots,T\\}$.\n   - Parameters: $T = 500$, $c = 0.002$, $\\sigma = 0.01$, seed = 1729.\n2. Stationary autoregressive process of order one with high persistence:\n   - Dynamics: $y_t = \\phi\\, y_{t-1} + \\varepsilon_t$ for $t \\in \\{1,\\dots,T\\}$.\n   - Parameters: $T = 500$, $\\phi = 0.9$, $\\sigma = 0.05$, seed = 2024.\n3. Integrated of order two:\n   - Dynamics: $\\Delta^2 y_t = \\varepsilon_t$ for $t \\in \\{1,\\dots,T\\}$ with $y_{-1} = 0$ and $y_0 = 0$.\n   - Equivalent representation: $y_t = 2 y_{t-1} - y_{t-2} + \\varepsilon_t$ for $t \\ge 1$.\n   - Parameters: $T = 500$, $\\sigma = 0.05$, seed = 7.\n4. Stationary autoregressive process of order one with moderate persistence:\n   - Dynamics: $y_t = \\phi\\, y_{t-1} + \\varepsilon_t$ for $t \\in \\{1,\\dots,T\\}$.\n   - Parameters: $T = 500$, $\\phi = 0.5$, $\\sigma = 0.05$, seed = 11.\n\nYour program must:\n- Simulate one sample path for each DGP using the stated parameters.\n- For each sample path, determine the smallest $d \\in \\{0,1,2\\}$ such that $\\Delta^d y_t$ is covariance-stationary, using a statistically sound and reproducible decision rule grounded in the definition of covariance-stationarity.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite above. For example, an output like $[1,0,2,0]$ means the first series is assessed as $I(1)$, the second as $I(0)$, the third as $I(2)$, and the fourth as $I(0)$.\n\nThere are no physical units or angles involved. All outputs must be integers. Your program must not require any input from the user and must not read from or write to any files or networks. The final output format must be exactly one line:\n- A list in the form $[\\text{d}_1,\\text{d}_2,\\text{d}_3,\\text{d}_4]$ where each $\\text{d}_i \\in \\{0,1,2\\}$ corresponds to the estimated order of integration for DGP $i$.", "solution": "The problem statement requires the determination of the order of integration, denoted $d$, for several specified discrete-time stochastic processes. The order of integration $d$ is defined as the minimum number of differences required to render a time series covariance-stationary. This is a fundamental task in time series econometrics, particularly within the Box-Jenkins framework for Autoregressive Integrated Moving Average (ARIMA) modeling.\n\nBefore proceeding to a solution, we must ascertain the validity of the problem.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- Task: For four given Data-Generating Processes (DGPs), simulate a sample path and determine the smallest integer $d \\in \\{0,1,2\\}$ such that the $d$-th difference of the series, $\\Delta^d y_t$, is covariance-stationary.\n- Definition: A series is integrated of order $d$, denoted $I(d)$, if $\\Delta^d y_t$ is stationary but $\\Delta^{d-1} y_t$ is not.\n- DGPs:\n    1. Random walk with drift: $y_t = y_{t-1} + c + \\varepsilon_t$, with $T=500$, $c=0.002$, $\\sigma=0.01$, $y_0=0$, seed=1729.\n    2. Stationary autoregressive process of order one (AR(1)): $y_t = \\phi y_{t-1} + \\varepsilon_t$, with $T=500$, $\\phi=0.9$, $\\sigma=0.05$, $y_0=0$, seed=2024.\n    3. Integrated of order two: $\\Delta^2 y_t = \\varepsilon_t$ (or $y_t = 2y_{t-1} - y_{t-2} + \\varepsilon_t$), with $T=500$, $\\sigma=0.05$, $y_{-1}=0, y_0=0$, seed=7.\n    4. Stationary AR(1): $y_t = \\phi y_{t-1} + \\varepsilon_t$, with $T=500$, $\\phi=0.5$, $\\sigma=0.05$, $y_0=0$, seed=11.\n- Innovations: $\\varepsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ are independent and identically distributed (i.i.d.) Gaussian.\n- Methodological Requirement: Use a \"statistically sound and reproducible decision rule\".\n- Output Format: A single line with a list of four integers, `[d1,d2,d3,d4]`.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is based on standard, canonical models from time series analysis (random walk, AR(1) processes) and employs well-defined concepts like covariance-stationarity, differencing, and order of integration. These are central to the specified topic of Box-Jenkins methodology. The premises are factually and mathematically sound.\n- **Well-Posed:** All DGPs are fully specified with parameters, initial conditions, and random seeds, ensuring reproducibility. The objective is clear. The phrase \"statistically sound and reproducible decision rule\" obliges the solver to select an appropriate statistical test, which is a standard expectation in this field. The established procedure is a sequence of unit root tests, such as the Augmented Dickey-Fuller (ADF) test. The problem is therefore well-posed, as a standard methodology exists to find a unique, verifiable solution.\n- **Objective:** The problem is stated in precise mathematical language, free of subjectivity or ambiguity. The DGPs are objective models.\n\n**Step 3: Verdict and Action**\nThe problem is scientifically sound, well-posed, and objective. It contains all necessary information for a unique solution. Therefore, the problem is deemed **valid**. We will now proceed with the solution.\n\n**Principle-Based Solution**\n\nThe core of the problem is to distinguish between stationary and non-stationary time series. A discrete-time stochastic process $\\{y_t\\}$ is (weakly) covariance-stationary if its statistical properties are invariant with respect to time translation. Specifically, for all integers $t$ and $k$:\n1. The mean is constant: $E[y_t] = \\mu$.\n2. The variance is finite and constant: $Var(y_t) = \\sigma_y^2 < \\infty$.\n3. The autocovariance depends only on the lag $k$: $Cov(y_t, y_{t-k}) = \\gamma_k$.\n\nProcesses that violate these properties are non-stationary. A common type of non-stationarity in economic and financial data is the presence of a unit root, where the process has infinite memory and its variance increases over time. A process with $d$ unit roots is said to be integrated of order $d$, denoted $I(d)$. Such a process can be made stationary by applying the backward difference operator $\\Delta$ a total of $d$ times, where $\\Delta y_t = y_t - y_{t-1}$.\n\nTo determine the order of integration $d$, we employ a sequential hypothesis testing procedure using the Augmented Dickey-Fuller (ADF) test. The ADF test is designed to detect the presence of a unit root. It is based on an OLS regression of the form:\n$$ \\Delta y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\sum_{i=1}^{p} \\delta_i \\Delta y_{t-i} + u_t $$\nThe null hypothesis is that the series has a unit root ($H_0: \\gamma = 0$), against the alternative that it is stationary or trend-stationary ($H_1: \\gamma < 0$). The test statistic is the t-statistic of the estimated coefficient $\\hat{\\gamma}$, which follows a non-standard Dickey-Fuller distribution under the null. We compare this statistic to pre-tabulated critical values. The terms $\\alpha$ (constant), $\\beta t$ (time trend), and the lagged differences $\\Delta y_{t-i}$ are included to respectively account for drift, deterministic trends, and higher-order autoregressive dynamics in the process, ensuring the error term $u_t$ is white noise.\n\nThe choice of including the constant and trend terms defines three main regression models:\n- No constant, no trend ('nc'): $\\Delta y_t = \\gamma y_{t-1} + \\dots$\n- Constant, no trend ('c'): $\\Delta y_t = \\alpha + \\gamma y_{t-1} + \\dots$\n- Constant and trend ('ct'): $\\Delta y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\dots$\n\nOur algorithmic approach is as follows:\n1.  **Simulate DGPs**: For each of the four test cases, we generate a sample path of length $T+1 = 501$ points ($t=0, \\dots, 500$) using the specified parameters, dynamics, initial conditions, and random seed.\n2.  **Implement ADF Test**: Since the required libraries `numpy` and `scipy` do not include a pre-built ADF test, we implement it from first principles. This involves constructing the dependent variable and the design matrix for the OLS regression, solving for the coefficients using `numpy.linalg.lstsq`, and calculating the t-statistic for the coefficient $\\gamma$. For simplicity and given the nature of the DGPs, we will use a fixed lag order of $p=1$.\n3.  **Sequential Testing Procedure**: For each simulated series, we determine the smallest $d \\in \\{0, 1, 2\\}$ via the following sequence, using a significance level of $5\\%$:\n    -   **Test for $d=0$**: We test the original series $y_t$ for a unit root. To be robust to a potential deterministic trend, we use the 'ct' model. The $5\\%$ critical value is approximately $-3.41$. If the ADF statistic is less than this value, we reject the null hypothesis of a unit root and conclude $d=0$.\n    -   **Test for $d=1$**: If the previous test fails to reject $H_0$, we test the first-differenced series $\\Delta y_t$. This series should no longer have a deterministic trend, but may have a non-zero mean (drift). We use the 'c' model. The $5\\%$ critical value is approximately $-2.86$. If the ADF statistic is less than this value, we conclude $d=1$.\n    -   **Test for $d=2$**: If both previous tests fail to reject, we test the second-differenced series $\\Delta^2 y_t$. This series is expected to be a zero-mean stationary process. We use the 'nc' model. The $5\\%$ critical value is approximately $-1.94$. If the ADF statistic is less than this value, we conclude $d=2$. If this test also fails to reject, we assign $d=2$ as the highest order specified in the problem statement.\n\nThis rigorous, principle-based procedure provides a \"statistically sound and reproducible decision rule\" as demanded by the problem statement.", "answer": "```python\nimport numpy as np\n# from scipy import linalg # numpy.linalg is sufficient and preferred.\n\ndef solve():\n    \"\"\"\n    Simulates four time series processes and determines their order of integration d.\n    \"\"\"\n\n    def adf_test(series: np.ndarray, p: int, regression_type: str) -> float:\n        \"\"\"\n        Performs an Augmented Dickey-Fuller test.\n\n        Args:\n            series: The time series to test.\n            p: The number of lagged differences to include.\n            regression_type: 'nc' (no constant), 'c' (constant), 'ct' (constant, trend).\n\n        Returns:\n            The ADF t-statistic for the lagged level term.\n        \"\"\"\n        n_obs = len(series)\n        dx = np.diff(series)\n\n        # Dependent variable (y_t - y_{t-1})\n        y = dx[p:]\n        n_reg_obs = len(y)\n\n        # Build design matrix X for OLS: y = X*beta + error\n        # Column 0: Lagged level (y_{t-1})\n        X = [series[p:-1]]\n\n        # Columns 1 to p: Lagged differences\n        if p > 0:\n            for i in range(p):\n                # Lag i is dx_{t-i}, index is p-i-1 relative to start of dx\n                X.append(dx[p - i - 1 : n_reg_obs + p - i - 1])\n\n        # Add constant and/or trend\n        if regression_type == 'c':\n            X.append(np.ones(n_reg_obs))\n        elif regression_type == 'ct':\n            X.append(np.ones(n_reg_obs))\n            # Trend starts from time p+1\n            trend = np.arange(p + 1, n_obs)\n            X.append(trend)\n\n        X = np.stack(X, axis=1)\n\n        # Perform OLS using np.linalg.lstsq\n        try:\n            coeffs, residuals_sum_sq, rank, s = np.linalg.lstsq(X, y, rcond=None)\n        except np.linalg.LinAlgError:\n            return 0.0 # Cannot solve, return value that won't reject H0\n        \n        n_regressors = X.shape[1]\n        if rank < n_regressors:\n            return 0.0 # Singular matrix, cannot compute reliably\n\n        # Extract coefficient for the lagged level term\n        gamma_hat = coeffs[0]\n\n        # Calculate standard error of gamma_hat\n        # residuals_sum_sq from lstsq is a single-element array\n        res_var = residuals_sum_sq[0] / (n_reg_obs - n_regressors)\n        xtx_inv = np.linalg.inv(X.T @ X)\n        se_gamma_hat = np.sqrt(res_var * xtx_inv[0, 0])\n\n        if se_gamma_hat == 0:\n            return 0.0\n\n        adf_stat = gamma_hat / se_gamma_hat\n        return adf_stat\n\n    def determine_d(series: np.ndarray) -> int:\n        \"\"\"\n        Determines the order of integration d for a series by sequential ADF tests.\n        \"\"\"\n        # Critical values for a 5% significance level, T~500\n        cv_ct = -3.41  # Model with constant and trend\n        cv_c = -2.86   # Model with constant\n        cv_nc = -1.94  # Model with no constant\n\n        p = 1 # Number of lags for ADF test\n\n        # Test for d=0 (I(0))\n        adf_stat_0 = adf_test(series, p, 'ct')\n        if adf_stat_0 < cv_ct:\n            return 0\n\n        # Test for d=1 (I(1))\n        diff1 = np.diff(series)\n        adf_stat_1 = adf_test(diff1, p, 'c')\n        if adf_stat_1 < cv_c:\n            return 1\n            \n        # Test for d=2 (I(2))\n        diff2 = np.diff(series, n=2)\n        adf_stat_2 = adf_test(diff2, p, 'nc')\n        if adf_stat_2 < cv_nc:\n            return 2\n        \n        # If all tests fail to reject, return highest order as per problem spec\n        return 2\n\n    # --- Test Suite---\n    \n    T = 500\n    test_cases = [\n        {'type': 'rw_drift', 'T': T, 'c': 0.002, 'sigma': 0.01, 'seed': 1729},\n        {'type': 'ar1', 'T': T, 'phi': 0.9, 'sigma': 0.05, 'seed': 2024},\n        {'type': 'i2', 'T': T, 'sigma': 0.05, 'seed': 7},\n        {'type': 'ar1', 'T': T, 'phi': 0.5, 'sigma': 0.05, 'seed': 11},\n    ]\n\n    results = []\n\n    for params in test_cases:\n        np.random.seed(params['seed'])\n        eps = np.random.normal(0, params['sigma'], params['T'] + 1)\n        y = np.zeros(params['T'] + 1)\n\n        if params['type'] == 'rw_drift':\n            # y_t = y_{t-1} + c + eps_t, with y_0 = 0\n            for t in range(1, params['T'] + 1):\n                y[t] = y[t-1] + params['c'] + eps[t]\n        \n        elif params['type'] == 'ar1':\n            # y_t = phi * y_{t-1} + eps_t, with y_0 = 0\n            for t in range(1, params['T'] + 1):\n                y[t] = params['phi'] * y[t-1] + eps[t]\n\n        elif params['type'] == 'i2':\n            # y_t = 2*y_{t-1} - y_{t-2} + eps_t, with y_{-1}=0, y_0=0\n            # Note: y array starts with y_0. y_{-1} is implicitly 0.\n            y[1] = 2 * y[0] - 0 + eps[1] # y_1 = eps_1\n            for t in range(2, params['T'] + 1):\n                y[t] = 2 * y[t-1] - y[t-2] + eps[t]\n        \n        # Determine the order of integration for the simulated series\n        d = determine_d(y)\n        results.append(d)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2378207"}, {"introduction": "Once a satisfactory time series model is identified and estimated, its primary purpose is often forecasting. This exercise [@problem_id:2378255] provides hands-on practice in computing multi-step-ahead forecasts and their corresponding prediction intervals for a simple autoregressive model. By working through these calculations, you'll develop a concrete understanding of how future uncertainty accumulates and how we can statistically bound our predictions.", "problem": "Consider the following Autoregressive of order 1 (AR(1)) model in the Box–Jenkins framework for a univariate, mean-zero, covariance-stationary time series $\\{X_t\\}$:\n$$\nX_t = 0.8\\,X_{t-1} + \\epsilon_t,\n$$\nwhere $\\{\\epsilon_t\\}$ are independent and identically distributed (i.i.d.) Gaussian innovations with $\\epsilon_t \\sim \\mathcal{N}(0,\\sigma_{\\epsilon}^{2})$ and $\\sigma_{\\epsilon}^{2} = 4$. You observe $X_T = 100$. Compute the first three step-ahead point forecasts $X_{T+1|T}$, $X_{T+2|T}$, $X_{T+3|T}$ and, under the Gaussian assumption, the corresponding two-sided $95\\%$ prediction intervals for each forecast. Use the standard normal quantile $z_{0.975} = 1.96$.\n\nAnswer format and rounding requirement:\n- Present your final answer as a single $1 \\times 9$ row matrix in the following order:\n$$\n\\big(X_{T+1|T},\\ \\text{Lower}_{T+1},\\ \\text{Upper}_{T+1},\\ X_{T+2|T},\\ \\text{Lower}_{T+2},\\ \\text{Upper}_{T+2},\\ X_{T+3|T},\\ \\text{Lower}_{T+3},\\ \\text{Upper}_{T+3}\\big).\n$$\n- Round every entry to four significant figures.\n- Do not include any units in your final answer.", "solution": "The problem presented is valid. It is a well-posed problem in time series analysis, grounded in the established Box-Jenkins methodology for autoregressive models. All necessary parameters and conditions are provided, and there are no scientific or logical contradictions. We shall proceed with the derivation of the solution.\n\nThe model is a first-order autoregressive process, AR($1$), given by:\n$$X_t = \\phi X_{t-1} + \\epsilon_t$$\nThe parameters are specified as $\\phi = 0.8$ and the innovations $\\{\\epsilon_t\\}$ are independent and identically distributed Gaussian random variables with mean $0$ and variance $\\sigma_{\\epsilon}^2 = 4$. The final observed value is $X_T = 100$.\n\nFirst, we compute the point forecasts. The $h$-step-ahead point forecast, denoted $X_{T+h|T}$, is the conditional expectation of $X_{T+h}$ given the information available up to time $T$, which we denote by $\\mathcal{F}_T$.\n$$X_{T+h|T} = E[X_{T+h} | \\mathcal{F}_T]$$\nFor an AR($1$) model, the recursive nature of the forecast is $X_{T+h|T} = \\phi X_{T+h-1|T}$. By repeated substitution, this yields the general formula for the point forecast:\n$$X_{T+h|T} = \\phi^h X_T$$\n\nWe apply this formula for $h=1, 2, 3$.\nFor $h=1$:\n$$X_{T+1|T} = \\phi X_T = 0.8 \\times 100 = 80$$\nFor $h=2$:\n$$X_{T+2|T} = \\phi^2 X_T = (0.8)^2 \\times 100 = 0.64 \\times 100 = 64$$\nFor $h=3$:\n$$X_{T+3|T} = \\phi^3 X_T = (0.8)^3 \\times 100 = 0.512 \\times 100 = 51.2$$\n\nNext, we construct the $95\\%$ prediction intervals. A two-sided $(1-\\alpha) \\times 100\\%$ prediction interval for $X_{T+h}$ is given by:\n$$X_{T+h|T} \\pm z_{1-\\alpha/2} \\sqrt{\\text{Var}(e_T(h))}$$\nwhere $e_T(h) = X_{T+h} - X_{T+h|T}$ is the $h$-step forecast error, and $z_{1-\\alpha/2}$ is the corresponding quantile of the standard normal distribution. For a $95\\%$ interval, $\\alpha=0.05$, and we are given $z_{0.975} = 1.96$.\n\nThe variance of the forecast error, $\\text{Var}(e_T(h))$, is derived from the moving-average representation of the forecast error. For an AR($1$) process, the $h$-step forecast error is:\n$$e_T(h) = \\epsilon_{T+h} + \\phi \\epsilon_{T+h-1} + \\dots + \\phi^{h-1} \\epsilon_{T+1} = \\sum_{j=0}^{h-1} \\phi^j \\epsilon_{T+h-j}$$\nSince the innovations $\\{\\epsilon_t\\}$ are independent with variance $\\sigma_{\\epsilon}^2$, the variance of the error is:\n$$\\text{Var}(e_T(h)) = \\text{Var}\\left(\\sum_{j=0}^{h-1} \\phi^j \\epsilon_{T+h-j}\\right) = \\sum_{j=0}^{h-1} (\\phi^j)^2 \\text{Var}(\\epsilon_{T+h-j}) = \\sigma_{\\epsilon}^2 \\sum_{j=0}^{h-1} \\phi^{2j}$$\nThis is the sum of a finite geometric series:\n$$\\text{Var}(e_T(h)) = \\sigma_{\\epsilon}^2 \\frac{1 - \\phi^{2h}}{1 - \\phi^2}$$\nWe have $\\sigma_{\\epsilon}^2 = 4$ and $\\phi=0.8$.\n\nFor $h=1$:\nThe forecast error variance is $\\text{Var}(e_T(1)) = \\sigma_{\\epsilon}^2 = 4$.\nThe standard deviation of the error is $\\sqrt{\\text{Var}(e_T(1))} = \\sqrt{4} = 2$.\nThe half-width of the prediction interval is $z_{0.975} \\times 2 = 1.96 \\times 2 = 3.92$.\nThe interval is $80 \\pm 3.92$, which gives:\nLower Bound: $80 - 3.92 = 76.08$\nUpper Bound: $80 + 3.92 = 83.92$\n\nFor $h=2$:\nThe forecast error variance is $\\text{Var}(e_T(2)) = \\sigma_{\\epsilon}^2 (1 + \\phi^2) = 4(1 + (0.8)^2) = 4(1 + 0.64) = 4(1.64) = 6.56$.\nThe standard deviation of the error is $\\sqrt{6.56}$.\nThe half-width of the prediction interval is $z_{0.975} \\times \\sqrt{6.56} \\approx 5.02005$.\nThe interval is $64 \\pm 5.02005$, which gives:\nLower Bound: $64 - 5.02005 = 58.97995$\nUpper Bound: $64 + 5.02005 = 69.02005$\n\nFor $h=3$:\nThe forecast error variance is $\\text{Var}(e_T(3)) = \\sigma_{\\epsilon}^2 (1 + \\phi^2 + \\phi^4) = 4(1 + (0.8)^2 + (0.8)^4) = 4(1 + 0.64 + 0.4096) = 4(2.0496) = 8.1984$.\nThe standard deviation of the error is $\\sqrt{8.1984}$.\nThe half-width of the prediction interval is $z_{0.975} \\times \\sqrt{8.1984} = 1.96 \\times \\sqrt{8.1984} \\approx 5.61195$.\nThe interval is $51.2 \\pm 5.61195$, which gives:\nLower Bound: $51.2 - 5.61195 = 45.58805$\nUpper Bound: $51.2 + 5.61195 = 56.81195$\n\nFinally, we round all results to four significant figures as required.\n$X_{T+1|T} = 80.00$\nLower$_{T+1} = 76.08$\nUpper$_{T+1} = 83.92$\n$X_{T+2|T} = 64.00$\nLower$_{T+2} = 58.98$\nUpper$_{T+2} = 69.02$\n$X_{T+3|T} = 51.20$\nLower$_{T+3} = 45.59$\nUpper$_{T+3} = 56.81$\n\nThese values are compiled into the final answer matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n80.00 & 76.08 & 83.92 & 64.00 & 58.98 & 69.02 & 51.20 & 45.59 & 56.81\n\\end{pmatrix}\n}\n$$", "id": "2378255"}, {"introduction": "The classical Box-Jenkins procedure relies on assumptions that are often violated by real-world financial data, which is frequently contaminated by extreme outliers. This problem [@problem_id:2378246] explores how such outliers can severely distort both the model identification and estimation stages, leading to spurious conclusions. Understanding these pitfalls is essential for any practitioner, and this exercise will guide you toward robust methods that ensure your models are reliable in the face of imperfect data.", "problem": "In a financial time series context, suppose you are applying the Box–Jenkins methodology to model a stationary series of daily log-returns $\\{y_t\\}_{t=1}^T$ for an asset. The series contains a small number of extreme, 1-time outliers due to market microstructure events (for example, a data reporting error or a brief trading halt). You plan to identify an AutoRegressive Integrated Moving Average (ARIMA) specification using the sample autocorrelation function (ACF) and partial autocorrelation function (PACF), and then estimate AutoRegressive Moving Average (ARMA) parameters under a Gaussian likelihood.\n\nWhich statement best characterizes how such outliers affect the identification step based on the ACF/PACF and the subsequent estimation step, and what is a suitable robust alternative procedure in this setting?\n\nA. A few extreme 1-time outliers inflate sample autocovariances across multiple lags through large cross-products, distorting both the ACF and the PACF and potentially suggesting spurious lag orders; Gaussian least-squares or Gaussian maximum likelihood estimates become biased and inefficient because the squared loss overweights the outliers. A robust alternative is to pre-adjust with impulse dummies for detected additive outliers or to use robust ACF/PACF estimators (for example, median and median absolute deviation based) for identification, and then estimate with robust M-estimation (for example, Huber loss) or a heavy-tailed likelihood (for example, Student–$t$ innovations).\n\nB. A few extreme 1-time outliers affect only the sample mean but not the ACF or PACF, and asymptotically do not affect Gaussian estimation; the recommended remedy is to difference the series additional times until the outliers vanish.\n\nC. A few extreme 1-time outliers primarily increase the magnitude of moving-average parameters while leaving the PACF essentially unaffected because it conditions out intermediate lags; the recommended remedy is to choose a higher-order AutoRegressive (AR) model so that the AR terms absorb the outliers.\n\nD. A few extreme 1-time outliers induce unit roots in the observed series, so the appropriate response is to run an augmented Dickey–Fuller test and then estimate under standard Gaussian assumptions if a unit root is rejected; no robustification is necessary.\n\nE. A few extreme 1-time outliers systematically invert ACF/PACF signatures so that AR processes look like moving-average processes and vice versa; the robust approach is to winsorize extremes prior to ACF/PACF analysis, compute Spearman rank autocorrelations, and then estimate by least absolute deviations only, which fully eliminates outlier influence.", "solution": "The problem statement must first be validated for scientific and logical integrity before a solution is attempted.\n\n### Step 1: Extract Givens\n- **Context**: Financial time series analysis using the Box–Jenkins methodology.\n- **Data**: A stationary time series of daily log-returns, denoted as $\\{y_t\\}_{t=1}^T$.\n- **Problematic Feature**: The series contains a small number of extreme, 1-time outliers. These are specified to be additive outliers (AOs) resulting from market microstructure events.\n- **Proposed Method**: Identify an ARIMA (specifically, ARMA, since the series is stationary) model using the sample autocorrelation function (ACF) and partial autocorrelation function (PACF).\n- **Proposed Estimation**: Estimate the ARMA parameters via maximization of a Gaussian likelihood.\n- **Question**: Characterize the effects of these outliers on both the identification and estimation steps, and describe a suitable robust alternative procedure.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is examined for validity.\n1.  **Scientifically Grounded**: The problem is well-grounded in the theory of time series analysis and robust statistics. The impact of outliers on classical estimation methods like least squares or Gaussian maximum likelihood, and on diagnostic tools like the sample ACF/PACF, is a canonical topic in econometrics and statistics. The scenario of outliers in financial returns data is not only plausible but ubiquitous. All concepts mentioned—Box-Jenkins, ARIMA, ACF/PACF, Gaussian likelihood, outliers—are standard. The problem is scientifically sound.\n2.  **Well-Posed**: The question asks for a characterization of a known statistical phenomenon and a description of appropriate methodological responses. It is a conceptual question with a well-established answer in the relevant literature. It is structured to have a single best answer among the choices. The problem is well-posed.\n3.  **Objective**: The terminology is precise and standard (e.g., \"stationary series,\" \"log-returns,\" \"1-time outliers,\" \"Gaussian likelihood\"). The description is free from ambiguity and subjectivity. The problem is objective.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It describes a classic, realistic, and well-understood challenge in applied time series modeling. I will now proceed to the derivation of the solution and the evaluation of the provided options.\n\nThe core of the problem addresses the non-robustness of standard time series techniques based on second-moment statistics and quadratic loss functions. Let us analyze the effect of an additive outlier on the two stages of the Box-Jenkins methodology.\n\n**Analysis of the Identification Stage (ACF and PACF)**\n\nThe sample autocovariance function at lag $k$ is defined as:\n$$\n\\hat{\\gamma}(k) = \\frac{1}{T} \\sum_{t=1}^{T-k} (y_t - \\bar{y})(y_{t+k} - \\bar{y})\n$$\nThe sample autocorrelation function (ACF) is its normalized version:\n$$\n\\hat{\\rho}(k) = \\frac{\\hat{\\gamma}(k)}{\\hat{\\gamma}(0)}\n$$\nLet us consider a single, large additive outlier at time $t_0$, such that the observed value is $y_{t_0} = y_{t_0}^* + \\omega$, where $y_{t_0}^*$ is the \"true\" unobserved value and $\\omega$ is a large constant.\n\n1.  **Effect on Sample Variance $\\hat{\\gamma}(0)$**: The term $(y_{t_0} - \\bar{y})^2$ will be exceptionally large, causing a significant positive inflation of the sample variance $\\hat{\\gamma}(0)$. The sample mean $\\bar{y}$ is also affected, but the squared term dominates.\n\n2.  **Effect on Sample Autocovariance $\\hat{\\gamma}(k)$**: For any lag $k > 0$, the outlier $y_{t_0}$ appears in two cross-product terms: $(y_{t_0-k} - \\bar{y})(y_{t_0} - \\bar{y})$ and $(y_{t_0} - \\bar{y})(y_{t_0+k} - \\bar{y})$. These products will be large in magnitude, contaminating the estimate $\\hat{\\gamma}(k)$.\n\n3.  **Effect on Sample ACF $\\hat{\\rho}(k)$**: The ACF is a ratio. The denominator, $\\hat{\\gamma}(0)$, is heavily inflated. This effect tends to push all correlations $\\hat{\\rho}(k)$ for $k>0$ toward zero. However, the numerator, $\\hat{\\gamma}(k)$, is also distorted by the outlier's cross-products. The net effect is a complex distortion of the entire ACF structure. True correlations may be masked (attenuated towards zero), and spurious correlations may appear. This makes the identification of the true ARMA orders $(p, q)$ unreliable.\n\n4.  **Effect on Sample PACF**: The partial autocorrelation function (PACF) is calculated from the ACF values (for instance, using the Durbin-Levinson algorithm, which solves the Yule-Walker equations for successively higher AR orders). Since the ACF is distorted, the resulting PACF will also be distorted, leading to misidentification of the autoregressive order $p$.\n\n**Analysis of the Estimation Stage**\n\nFor a stationary ARMA($p, q$) process, estimation is commonly performed by Maximum Likelihood Estimation (MLE). Assuming Gaussian innovations, $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$, maximizing the likelihood is equivalent to minimizing the sum of squared residuals (or errors):\n$$\n\\min_{\\boldsymbol{\\phi}, \\boldsymbol{\\theta}, \\sigma^2} \\sum_{t=1}^T \\epsilon_t^2\n$$\nwhere $\\boldsymbol{\\phi} = (\\phi_1, \\dots, \\phi_p)$ and $\\boldsymbol{\\theta} = (\\theta_1, \\dots, \\theta_q)$ are the AR and MA parameters, respectively. An outlier at $t_0$ will produce an extremely large residual, $\\epsilon_{t_0}$. Because the loss function is the square of the residual, this single term $(\\epsilon_{t_0})^2$ will dominate the sum. The optimization algorithm will adjust the parameters $\\boldsymbol{\\phi}$ and $\\boldsymbol{\\theta}$ significantly away from their true population values in a futile attempt to reduce this one massive squared residual. This results in parameter estimates that are both biased and inefficient (i.e., have high variance).\n\n**Suitable Robust Alternatives**\n\nA robust procedure must address these vulnerabilities in both identification and estimation.\n\n1.  **Robust Identification**:\n    -   **Data Cleaning/Intervention**: Detect outliers and model them explicitly, for instance by including an impulse dummy variable for each detected additive outlier. This is a form of intervention analysis. After accounting for the outliers, the ACF/PACF can be computed on the \"cleaned\" residuals.\n    -   **Robust ACF/PACF Estimators**: Replace the sample mean and autocovariance with robust counterparts. For example, using the median as a location estimator and the Median Absolute Deviation (MAD) or other robust scale estimators to construct robust correlations.\n\n2.  **Robust Estimation**:\n    -   **M-estimation**: Replace the quadratic loss function ($\\rho(x) = x^2$) with one that downweights large errors. The Huber loss function, for example, is quadratic for small errors but linear for large errors, providing a compromise between efficiency under Gaussianity and robustness to outliers.\n    -   **Heavy-Tailed Likelihood**: Abandon the Gaussian assumption for the innovations. Instead, assume they follow a heavy-tailed distribution, such as the Student's $t$-distribution. The likelihood function for a $t$-distribution naturally assigns less weight to extreme observations compared to the Gaussian likelihood, leading to more robust parameter estimates.\n\nNow, we evaluate the given options based on this analysis.\n\n**A. A few extreme 1-time outliers inflate sample autocovariances across multiple lags through large cross-products, distorting both the ACF and the PACF and potentially suggesting spurious lag orders; Gaussian least-squares or Gaussian maximum likelihood estimates become biased and inefficient because the squared loss overweights the outliers. A robust alternative is to pre-adjust with impulse dummies for detected additive outliers or to use robust ACF/PACF estimators (for example, median and median absolute deviation based) for identification, and then estimate with robust M-estimation (for example, Huber loss) or a heavy-tailed likelihood (for example, Student–$t$ innovations).**\nThis statement correctly identifies that outliers create large cross-products that inflate autocovariances and distort both the ACF and PACF. It correctly states that this leads to biased and inefficient estimates under a Gaussian likelihood due to the quadratic loss. Finally, it presents a comprehensive and accurate list of standard robust alternatives: impulse dummies (intervention analysis), robust ACF/PACF estimators for identification, and both M-estimation (Huber) and heavy-tailed likelihoods (Student-$t$) for estimation. This option is entirely consistent with established theory.\n**Verdict: Correct.**\n\n**B. A few extreme 1-time outliers affect only the sample mean but not the ACF or PACF, and asymptotically do not affect Gaussian estimation; the recommended remedy is to difference the series additional times until the outliers vanish.**\nThis statement is incorrect on multiple counts. Outliers profoundly affect the ACF and PACF, as shown above. Their influence on parameter estimates is not guaranteed to vanish asymptotically; they typically lead to inconsistent estimates. Differencing a stationary series is inappropriate and, furthermore, it propagates the effect of an additive outlier, creating two outliers from one.\n**Verdict: Incorrect.**\n\n**C. A few extreme 1-time outliers primarily increase the magnitude of moving-average parameters while leaving the PACF essentially unaffected because it conditions out intermediate lags; the recommended remedy is to choose a higher-order AutoRegressive (AR) model so that the AR terms absorb the outliers.**\nThe claim that the PACF is unaffected is false; it is derived from the ACF and is therefore also distorted. The bias does not selectively affect only MA parameters. Attempting to \"absorb\" outliers by increasing the AR order is a form of misspecification and overfitting that will produce a poor model for the majority of the data.\n**Verdict: Incorrect.**\n\n**D. A few extreme 1-time outliers induce unit roots in the observed series, so the appropriate response is to run an augmented Dickey–Fuller test and then estimate under standard Gaussian assumptions if a unit root is rejected; no robustification is necessary.**\nOutliers do not \"induce\" a unit root. A unit root is a fundamental property of the process generating the data, implying infinite memory. An outlier is a transient event. While outliers can reduce the power of unit root tests, leading to a failure to reject a false null hypothesis of a unit root, they are not the source of the unit root itself. The problem states the series is stationary. Ignoring the outlier problem even if a unit root is correctly rejected is incorrect, as estimation remains non-robust.\n**Verdict: Incorrect.**\n\n**E. A few extreme 1-time outliers systematically invert ACF/PACF signatures so that AR processes look like moving-average processes and vice versa; the robust approach is to winsorize extremes prior to ACF/PACF analysis, compute Spearman rank autocorrelations, and then estimate by least absolute deviations only, which fully eliminates outlier influence.**\nThe claim of a systematic \"inversion\" of ACF/PACF signatures is an oversimplification and not a general result; the distortion is more complex. While winsorizing and rank correlations are valid robust techniques, the statement that Least Absolute Deviations (LAD, or $L_1$) estimation \"fully eliminates\" outlier influence is an exaggeration. LAD reduces the influence relative to least squares but does not eliminate it, and it is not the only robust estimation method—M-estimators and heavy-tailed likelihoods are also central to the field.\n**Verdict: Incorrect.**\n\nBased on this rigorous evaluation, Option A provides the most complete and accurate description of the problem and its solution.", "answer": "$$\\boxed{A}$$", "id": "2378246"}]}