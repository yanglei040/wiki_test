{"hands_on_practices": [{"introduction": "To build a solid understanding of distributed source coding, we begin with a foundational exercise. This problem requires you to apply the Slepian-Wolf theorem directly to a given joint probability distribution. By calculating the necessary entropies, you will define the complete set of inequalities that characterize the achievable rate region, a cornerstone skill for analyzing correlated sources [@problem_id:1619235].", "problem": "Two correlated environmental sensors, $S_X$ and $S_Y$, are deployed to monitor a certain phenomenon. The sensor $S_X$ records a state from the set $X = \\{a, b, c\\}$, while the sensor $S_Y$ records a binary state from the set $Y = \\{0, 1\\}$. The joint behavior of these sensors is described by the following joint probability mass function, $p(x, y)$:\n\n| $p(x,y)$ |  $y=0$  |  $y=1$  |\n|:--------:|:-------:|:-------:|\n|  $x=a$   |  $1/4$  |   $0$   |\n|  $x=b$   |  $1/4$  |  $1/4$  |\n|  $x=c$   |   $0$   |  $1/4$  |\n\nThe outputs of the two sensors are to be encoded separately at rates $R_X$ and $R_Y$ (in bits per symbol), respectively. These encoded streams are then transmitted to a central decoder which must be able to reconstruct both sequences $(X^n, Y^n)$ with an arbitrarily low probability of error as the sequence length $n$ becomes large.\n\nWhich of the following sets of inequalities correctly defines the Slepian-Wolf region, representing the set of all achievable rate pairs $(R_X, R_Y)$ for lossless distributed compression of these two sources? All entropies are calculated using the base-2 logarithm.\n\nA. $R_X \\ge 1$, $R_Y \\ge 0.5$, $R_X + R_Y \\ge 2$\n\nB. $R_X \\ge 1.5$, $R_Y \\ge 1$, $R_X + R_Y \\ge 2$\n\nC. $R_X \\ge 0.5$, $R_Y \\ge 1$, $R_X + R_Y \\ge 2$\n\nD. $R_X \\ge 1$, $R_Y \\ge 0.5$, $R_X + R_Y \\ge 2.5$\n\nE. $R_X \\ge 1.5$, $R_Y \\ge 1$, $R_X + R_Y \\ge 2.5$", "solution": "For two discrete memoryless correlated sources with separate encoders and a joint decoder, the Slepian-Wolf region is the set of rate pairs satisfying\n$$\nR_{X} \\geq H(X|Y), \\quad R_{Y} \\geq H(Y|X), \\quad R_{X}+R_{Y} \\geq H(X,Y),\n$$\nwith all entropies in bits using base-2 logarithms.\n\nFrom the given joint pmf,\n$$\np(a,0)=\\frac{1}{4},\\quad p(a,1)=0,\\quad p(b,0)=\\frac{1}{4},\\quad p(b,1)=\\frac{1}{4},\\quad p(c,0)=0,\\quad p(c,1)=\\frac{1}{4}.\n$$\nMarginals are\n$$\np_{X}(a)=\\frac{1}{4},\\quad p_{X}(b)=\\frac{1}{2},\\quad p_{X}(c)=\\frac{1}{4},\\qquad p_{Y}(0)=\\frac{1}{2},\\quad p_{Y}(1)=\\frac{1}{2}.\n$$\nCompute the entropies:\n$$\nH(X)=-\\sum_{x\\in X} p_{X}(x)\\log_{2} p_{X}(x)\n= -\\left(\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}+\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}+\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}\\right)\n= \\tfrac{1}{2}+\\tfrac{1}{2}+\\tfrac{1}{2}=\\tfrac{3}{2}.\n$$\n$$\nH(Y)=-\\sum_{y\\in Y} p_{Y}(y)\\log_{2} p_{Y}(y)\n= -\\left(\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}+\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}\\right)=1.\n$$\nThe joint entropy uses the four nonzero masses of size $\\frac{1}{4}$:\n$$\nH(X,Y)=-\\sum_{x,y} p(x,y)\\log_{2} p(x,y) = -4\\cdot \\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}=2.\n$$\nThus the conditional entropies are\n$$\nH(X|Y)=H(X,Y)-H(Y)=2-1=1,\\qquad H(Y|X)=H(X,Y)-H(X)=2-\\tfrac{3}{2}=\\tfrac{1}{2}.\n$$\nEquivalently, one can verify directly that $H(X|Y=0)=1$, $H(X|Y=1)=1$ so $H(X|Y)=1$, and $H(Y|X=a)=0$, $H(Y|X=b)=1$, $H(Y|X=c)=0$ so $H(Y|X)=\\tfrac{1}{2}$.\n\nTherefore, the Slepian-Wolf region is given by\n$$\nR_{X} \\ge 1,\\quad R_{Y} \\ge \\tfrac{1}{2},\\quad R_{X}+R_{Y} \\ge 2,\n$$\nwhich matches option A.", "answer": "$$\\boxed{A}$$", "id": "1619235"}, {"introduction": "Moving beyond direct calculation from a probability table, this exercise explores a more conceptual model of correlation. Here, the relationship between two sources is defined by a shared underlying random factor, which is common in many real-world scenarios. This problem [@problem_id:1619200] will strengthen your intuition by connecting the abstract entropy calculations to a tangible generative process, showing how shared information reduces the required coding rates.", "problem": "Two remote environmental sensors, Sensor X and Sensor Y, monitor a location. At each time step, they generate a binary observation. The observations are correlated due to a shared environmental factor.\n\nLet the shared factor be represented by a random variable $U$. The specific measurements of Sensor X and Sensor Y are influenced by additional, independent local factors, represented by random variables $V$ and $W$, respectively. The variables $U$, $V$, and $W$ are all mutually independent, and each follows a fair Bernoulli distribution, i.e., they take values in $\\{0, 1\\}$ with probability $1/2$ for each outcome.\n\nThe output of Sensor X at any time step is the pair $X = (U, V)$.\nThe output of Sensor Y at the same time step is the pair $Y = (U, W)$.\n\nThe data from both sensors are to be compressed separately and transmitted to a central decoder. The decoder must be able to losslessly reconstruct both $X$ and $Y$. Let $R_X$ and $R_Y$ be the compression rates (in bits per generated pair) for the outputs of Sensor X and Sensor Y, respectively. According to the Slepian-Wolf theorem, there is a set of achievable rate pairs $(R_X, R_Y)$ for which lossless reconstruction is possible.\n\nWhich of the following options correctly describes this achievable rate region?\n\nA: The set of pairs $(R_X, R_Y)$ such that $R_X \\ge 2$ and $R_Y \\ge 2$.\n\nB: The set of pairs $(R_X, R_Y)$ such that $R_X \\ge 1$, $R_Y \\ge 1$, and $R_X + R_Y \\ge 3$.\n\nC: The set of pairs $(R_X, R_Y)$ such that $R_X \\ge 1$, $R_Y \\ge 1$, and $R_X + R_Y \\ge 2$.\n\nD: The set of pairs $(R_X, R_Y)$ such that $R_X + R_Y \\ge 3$.\n\nE: The set of pairs $(R_X, R_Y)$ such that $R_X \\ge 0$, $R_Y \\ge 0$, and $R_X + R_Y \\ge 3$.", "solution": "We apply the Slepian-Wolf theorem for two correlated discrete memoryless sources. For lossless reconstruction with separate encoders and a joint decoder, the achievable rate region is characterized by\n$$\nR_{X} \\geq H(X \\mid Y), \\quad R_{Y} \\geq H(Y \\mid X), \\quad R_{X} + R_{Y} \\geq H(X,Y).\n$$\n\nGiven $U$, $V$, and $W$ are mutually independent fair Bernoulli random variables, we compute the relevant entropies. First, identify the sources:\n$$\nX = (U,V), \\quad Y = (U,W).\n$$\nBecause $U$, $V$, and $W$ are independent and each has entropy $H(U)=H(V)=H(W)=1$ (in bits), we have\n$$\nH(X) = H(U,V) = H(U) + H(V) = 2,\n$$\n$$\nH(Y) = H(U,W) = H(U) + H(W) = 2.\n$$\nFor the joint entropy, note that $(X,Y) = (U,V,U,W)$ is equivalent to $(U,V,W)$ since $U$ appears in both:\n$$\nH(X,Y) = H(U,V,W) = H(U) + H(V) + H(W) = 3.\n$$\n\nNext, compute the conditional entropies using the chain rule and independence. For $H(X \\mid Y)$:\n$$\nH(X \\mid Y) = H(U,V \\mid U,W) = H(V \\mid U,W) + H(U \\mid U,W).\n$$\nSince $H(U \\mid U,W) = 0$ and $V$ is independent of $(U,W)$,\n$$\nH(V \\mid U,W) = H(V) = 1,\n$$\nhence\n$$\nH(X \\mid Y) = 1.\n$$\nSimilarly, for $H(Y \\mid X)$:\n$$\nH(Y \\mid X) = H(U,W \\mid U,V) = H(W \\mid U,V) + H(U \\mid U,V) = H(W \\mid U,V) = H(W) = 1.\n$$\n\nSubstituting into the Slepian-Wolf bounds yields\n$$\nR_{X} \\geq 1, \\quad R_{Y} \\geq 1, \\quad R_{X} + R_{Y} \\geq 3.\n$$\nThis set of inequalities matches option B.", "answer": "$$\\boxed{B}$$", "id": "1619200"}, {"introduction": "Finally, we transition from defining the achievable region to using it for practical assessment. This problem asks you to verify whether specific numerical and symbolic rate pairs, $(R_X, R_Y)$, are achievable for a given pair of correlated sources. This practice is crucial for any engineer or scientist looking to implement a distributed compression system, as it simulates the process of checking if a proposed design meets the fundamental limits established by the Slepian-Wolf theorem [@problem_id:1619241].", "problem": "Consider two correlated, discrete, memoryless sources, $X$ and $Y$, whose outputs are drawn from the alphabets $\\mathcal{X} = \\{0, 1\\}$ and $\\mathcal{Y} = \\{0, 1\\}$, respectively. Their joint Probability Mass Function (PMF), defined as $p(x, y) = P(X=x, Y=y)$, is given by:\n$p(0, 0) = \\frac{1}{2}$\n$p(0, 1) = \\frac{1}{4}$\n$p(1, 0) = 0$\n$p(1, 1) = \\frac{1}{4}$\n\nThese two sources are encoded separately. The first encoder compresses the sequence from source $X$ at a rate of $R_X$ bits per symbol. The second encoder compresses the sequence from source $Y$ at a rate of $R_Y$ bits per symbol. A single joint decoder receives both compressed bitstreams and must be able to losslessly reconstruct the original sequences from both $X$ and $Y$ with a probability of error that approaches zero as the sequence length increases.\n\nA rate pair $(R_X, R_Y)$ is called \"achievable\" if it allows for such lossless joint reconstruction. All entropy calculations and rates are to be expressed in bits, meaning all logarithms are base 2. Let $H(V)$ denote the Shannon entropy of a random variable $V$, and $H(V|W)$ denote the conditional entropy of $V$ given $W$.\n\nWhich of the following rate pairs $(R_X, R_Y)$ are achievable for this system? Select all that apply.\n\nA. $(H(X), H(Y|X))$\n\nB. $(H(X|Y), H(Y))$\n\nC. $(0.9, 0.65)$\n\nD. $(0.7, 0.75)$\n\nE. $(H(X), H(Y))$", "solution": "We use the Slepian–Wolf theorem for two correlated, discrete, memoryless sources with separate encoders and a joint decoder. A rate pair $(R_{X},R_{Y})$ is achievable if and only if\n$$\nR_{X} \\geq H(X|Y), \\quad R_{Y} \\geq H(Y|X), \\quad R_{X}+R_{Y} \\geq H(X,Y).\n$$\n\nFirst compute the marginals from the given joint PMF:\n$$\nP(X=0)=\\tfrac{1}{2}+\\tfrac{1}{4}=\\tfrac{3}{4}, \\quad P(X=1)=\\tfrac{1}{4},\n$$\n$$\nP(Y=0)=\\tfrac{1}{2}+0=\\tfrac{1}{2}, \\quad P(Y=1)=\\tfrac{1}{2}.\n$$\n\nEntropies:\n$$\nH(X)=-\\tfrac{3}{4}\\log_{2}\\tfrac{3}{4}-\\tfrac{1}{4}\\log_{2}\\tfrac{1}{4}\n=-\\tfrac{3}{4}(\\log_{2}3-2)-\\tfrac{1}{4}(-2)=2-\\tfrac{3}{4}\\log_{2}3 \\approx 0.811278.\n$$\n$$\nH(Y)=-\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}-\\tfrac{1}{2}\\log_{2}\\tfrac{1}{2}=1.\n$$\n\nConditional entropies:\nFor $H(X|Y)$, note $P(Y=0)=\\tfrac{1}{2}$ gives $X=0$ with certainty, and $P(Y=1)=\\tfrac{1}{2}$ gives $X$ uniform on $\\{0,1\\}$. Hence\n$$\nH(X|Y)=\\tfrac{1}{2}\\cdot 0+\\tfrac{1}{2}\\cdot 1=\\tfrac{1}{2}.\n$$\nFor $H(Y|X)$, when $X=0$ (probability $\\tfrac{3}{4}$), $(Y|X=0)$ has probabilities $\\tfrac{2}{3}$ and $\\tfrac{1}{3}$; when $X=1$ (probability $\\tfrac{1}{4}$), $(Y|X=1)$ is deterministic. Therefore\n$$\nH(Y|X)=\\tfrac{3}{4}\\Big(-\\tfrac{2}{3}\\log_{2}\\tfrac{2}{3}-\\tfrac{1}{3}\\log_{2}\\tfrac{1}{3}\\Big)\n=\\tfrac{3}{4}\\Big(-\\tfrac{2}{3}(1-\\log_{2}3)-\\tfrac{1}{3}(-\\log_{2}3)\\Big)\n=\\tfrac{3}{4}\\log_{2}3-\\tfrac{1}{2}\\approx 0.688721.\n$$\n\nBy the chain rule,\n$$\nH(X,Y)=H(X)+H(Y|X)=\\Big(2-\\tfrac{3}{4}\\log_{2}3\\Big)+\\Big(\\tfrac{3}{4}\\log_{2}3-\\tfrac{1}{2}\\Big)=\\tfrac{3}{2}.\n$$\n\nNow test each proposed pair against the Slepian–Wolf bounds:\n\nA. $(H(X),H(Y|X))$: \n$R_{X}=H(X)\\geq H(X|Y)$, $R_{Y}=H(Y|X)$, and $R_{X}+R_{Y}=H(X)+H(Y|X)=H(X,Y)$. Achievable.\n\nB. $(H(X|Y),H(Y))$: \n$R_{X}=H(X|Y)$, $R_{Y}=H(Y)\\geq H(Y|X)$, and $R_{X}+R_{Y}=H(X|Y)+H(Y)=H(X,Y)$. Achievable.\n\nC. $(0.9,0.65)$:\n$R_{X}=0.9\\geq H(X|Y)=\\tfrac{1}{2}$ and $R_{X}+R_{Y}=1.55\\geq \\tfrac{3}{2}$, but\n$$\nR_{Y}=0.65\\tfrac{3}{4}\\log_{2}3-\\tfrac{1}{2}\\approx 0.688721,\n$$\nso it violates $R_{Y}\\geq H(Y|X)$. Not achievable.\n\nD. $(0.7,0.75)$:\n$R_{X}=0.7\\geq \\tfrac{1}{2}$ and $R_{Y}=0.75\\geq \\tfrac{3}{4}\\log_{2}3-\\tfrac{1}{2}$, but\n$$\nR_{X}+R_{Y}=1.45\\tfrac{3}{2},\n$$\nso it violates $R_{X}+R_{Y}\\geq H(X,Y)$. Not achievable.\n\nE. $(H(X),H(Y))$:\n$R_{X}=H(X)\\geq H(X|Y)$, $R_{Y}=H(Y)\\geq H(Y|X)$, and $R_{X}+R_{Y}=H(X)+H(Y)\\geq H(X,Y)$. Achievable.\n\nTherefore, the achievable rate pairs among the options are A, B, and E.", "answer": "$$\\boxed{ABE}$$", "id": "1619241"}]}