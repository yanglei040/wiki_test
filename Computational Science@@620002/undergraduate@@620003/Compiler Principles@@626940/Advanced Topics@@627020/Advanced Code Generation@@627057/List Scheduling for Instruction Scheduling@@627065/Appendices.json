{"hands_on_practices": [{"introduction": "To build a solid foundation, our first practice focuses on the core mechanics of the list scheduling algorithm. You will manually trace the scheduler's operation on a simple, single-issue processor model, starting by calculating instruction priorities based on the critical path. This exercise [@problem_id:3650813] demonstrates how the scheduler's ready list evolves over time and how simple changes in the tie-breaking heuristic can lead to different performance outcomes.", "problem": "A compiler backend targets a Single Instruction Issue (SII) machine that can issue at most one instruction per clock cycle. Instructions have positive integer latencies. If an instruction $v$ is issued at cycle $t$, its result becomes available at cycle $t + l(v)$, where $l(v)$ is its latency. A successor $u$ of $v$ in the Directed Acyclic Graph (DAG) may be issued only when all of its predecessors have produced their results. A list scheduler maintains a ready list of instructions whose data dependencies are satisfied at a given cycle and chooses one instruction per cycle if available; if the list is empty, the cycle is counted as a stall. The total makespan is the completion time of the last instruction (the maximum over all issued instructions of issue time plus latency).\n\nConsider a basic block whose dependence graph is a Directed Acyclic Graph (DAG) $G=(V,E)$ with node set\n$$V=\\{C_{1},C_{2},C_{3},C_{4},C_{5}\\}\\cup \\{P_{1},P_{2},P_{3},P_{4},P_{5},P_{6},P_{7},P_{8}\\}$$\nand edge set\n$$E=\\{(C_{1},C_{2}),(C_{2},C_{3}),(C_{3},C_{4}),(C_{4},C_{5})\\}.$$\nAll $P_{j}$ nodes have no successors and no predecessors. Latencies are\n$$l(C_{i})=4 \\text{ for } i\\in\\{1,2,3,4,5\\},\\quad l(P_{j})=1 \\text{ for } j\\in\\{1,2,3,4,5,6,7,8\\}.$$\n\nUsing the definition of critical-path priority as the weighted longest-path-to-exit metric,\n$$p(v)=l(v)+\\max_{(v\\to u)\\in E}p(u),\\quad \\max \\emptyset = 0,$$\nperform the following:\n\n- Compute $p(v)$ for all $v\\in V$.\n- Construct two list schedules:\n  - Schedule $\\mathcal{S}_{A}$ uses the ready-list tie-breaker: pick the ready instruction with largest $p(v)$; on ties, pick the one with larger $l(v)$; on further ties, pick by lexicographically increasing identifier ($C$ before $P$, numerically smaller index first).\n  - Schedule $\\mathcal{S}_{B}$ uses the ready-list tie-breaker: pick the ready instruction with smallest $l(v)$; on ties, pick the one with smaller $p(v)$; on further ties, pick by lexicographically increasing identifier.\n\nFor each schedule, report the issue time of every instruction, the number of stall cycles, and the total makespan in cycles. Then, as your final numeric answer, give the difference in makespan (in cycles) defined as\n$$\\text{makespan}(\\mathcal{S}_{B})-\\text{makespan}(\\mathcal{S}_{A}).$$\nExpress your final answer as an integer number of cycles. No rounding is required.", "solution": "### 1. Priority Calculation\n\nWe first compute the critical-path priority $p(v)$ for each instruction $v$. The priority is the sum of an instruction's own latency and the maximum priority of its successors. We calculate this by starting from the exit nodes of the graph.\n\nThe instructions $P_j$ and $C_5$ have no successors.\n- For all $j \\in \\{1, \\dots, 8\\}$, $p(P_j) = l(P_j) = 1$.\n- $p(C_5) = l(C_5) = 4$.\n\nNow we work backward through the dependency chain $C_1 \\rightarrow C_2 \\rightarrow C_3 \\rightarrow C_4 \\rightarrow C_5$.\n- $p(C_4) = l(C_4) + p(C_5) = 4 + 4 = 8$.\n- $p(C_3) = l(C_3) + p(C_4) = 4 + 8 = 12$.\n- $p(C_2) = l(C_2) + p(C_3) = 4 + 12 = 16$.\n- $p(C_1) = l(C_1) + p(C_2) = 4 + 16 = 20$.\n\nSummary of priorities:\n- $p(C_1)=20, p(C_2)=16, p(C_3)=12, p(C_4)=8, p(C_5)=4$.\n- $p(P_j)=1$ for $j=1, \\dots, 8$.\n\n### 2. Schedule $\\mathcal{S}_A$ Construction\n\nHeuristic for $\\mathcal{S}_A$: Prioritize by largest $p(v)$, then largest $l(v)$, then lexicographical name.\n\n| Cycle | Ready List (Instruction, p, l) | Chosen | Issue Time | Completion Time |\n|:-----:|:-------------------------------|:------:|:----------:|:----------------:|\n| 1     | {($C_1$,20,4), ($P_1..P_8$,1,1)} | $C_1$  | 1          | $1+4=5$          |\n| 2     | {($P_1..P_8$,1,1)}              | $P_1$  | 2          | $2+1=3$          |\n| 3     | {($P_2..P_8$,1,1)}              | $P_2$  | 3          | $3+1=4$          |\n| 4     | {($P_3..P_8$,1,1)}              | $P_3$  | 4          | $4+1=5$          |\n| 5     | {($C_2$,16,4), ($P_4..P_8$,1,1)} | $C_2$  | 5          | $5+4=9$          |\n| 6     | {($P_4..P_8$,1,1)}              | $P_4$  | 6          | $6+1=7$          |\n| 7     | {($P_5..P_8$,1,1)}              | $P_5$  | 7          | $7+1=8$          |\n| 8     | {($P_6..P_8$,1,1)}              | $P_6$  | 8          | $8+1=9$          |\n| 9     | {($C_3$,12,4), ($P_7,P_8$,1,1)}  | $C_3$  | 9          | $9+4=13$         |\n| 10    | {($P_7,P_8$,1,1)}               | $P_7$  | 10         | $10+1=11$        |\n| 11    | {($P_8$,1,1)}                   | $P_8$  | 11         | $11+1=12$        |\n| 12    | {}                             | STALL  | -          | -                |\n| 13    | {($C_4$,8,4)}                   | $C_4$  | 13         | $13+4=17$        |\n| 14    | {}                             | STALL  | -          | -                |\n| 15    | {}                             | STALL  | -          | -                |\n| 16    | {}                             | STALL  | -          | -                |\n| 17    | {($C_5$,4,4)}                   | $C_5$  | 17         | $17+4=21$        |\n\n- **Makespan($\\mathcal{S}_A$)**: The last instruction, $C_5$, completes at cycle 21. The makespan is 21.\n- **Stalls**: There were 4 stall cycles (at cycles 12, 14, 15, 16).\n\n### 3. Schedule $\\mathcal{S}_B$ Construction\n\nHeuristic for $\\mathcal{S}_B$: Prioritize by smallest $l(v)$, then smaller $p(v)$, then lexicographical name.\n\n| Cycle | Ready List (Instruction, l, p) | Chosen | Issue Time | Completion Time |\n|:-----:|:-------------------------------|:------:|:----------:|:----------------:|\n| 1     | {($C_1$,4,20), ($P_1..P_8$,1,1)} | $P_1$  | 1          | $1+1=2$          |\n| 2     | {($C_1$,4,20), ($P_2..P_8$,1,1)} | $P_2$  | 2          | $2+1=3$          |\n| 3     | {($C_1$,4,20), ($P_3..P_8$,1,1)} | $P_3$  | 3          | $3+1=4$          |\n| 4     | {($C_1$,4,20), ($P_4..P_8$,1,1)} | $P_4$  | 4          | $4+1=5$          |\n| 5     | {($C_1$,4,20), ($P_5..P_8$,1,1)} | $P_5$  | 5          | $5+1=6$          |\n| 6     | {($C_1$,4,20), ($P_6..P_8$,1,1)} | $P_6$  | 6          | $6+1=7$          |\n| 7     | {($C_1$,4,20), ($P_7,P_8$,1,1)}  | $P_7$  | 7          | $7+1=8$          |\n| 8     | {($C_1$,4,20), ($P_8$,1,1)}      | $P_8$  | 8          | $8+1=9$          |\n| 9     | {($C_1$,4,20)}                  | $C_1$  | 9          | $9+4=13$         |\n| 10-12 | {}                             | STALL  | -          | -                |\n| 13    | {($C_2$,4,16)}                  | $C_2$  | 13         | $13+4=17$        |\n| 14-16 | {}                             | STALL  | -          | -                |\n| 17    | {($C_3$,4,12)}                  | $C_3$  | 17         | $17+4=21$        |\n| 18-20 | {}                             | STALL  | -          | -                |\n| 21    | {($C_4$,4,8)}                   | $C_4$  | 21         | $21+4=25$        |\n| 22-24 | {}                             | STALL  | -          | -                |\n| 25    | {($C_5$,4,4)}                   | $C_5$  | 25         | $25+4=29$        |\n\n- **Makespan($\\mathcal{S}_B$)**: The last instruction, $C_5$, completes at cycle 29. The makespan is 29.\n- **Stalls**: There were $3 \\times 4 = 12$ stall cycles.\n\n### 4. Makespan Difference\n\nThe difference in makespan is calculated as:\n$$ \\text{makespan}(\\mathcal{S}_{B}) - \\text{makespan}(\\mathcal{S}_{A}) = 29 - 21 = 8 $$\nThis example illustrates how a heuristic that does not prioritize the critical path ($\\mathcal{S}_B$) can lead to a significantly worse schedule by failing to hide the latency of long dependency chains. It prioritizes short, independent tasks, leaving the long critical path to be executed serially with many stalls.", "answer": "$$\\boxed{8}$$", "id": "3650813"}, {"introduction": "Real-world processors are superscalar, meaning they can execute multiple instructions in parallel using different functional units. This practice [@problem_id:3650798] moves beyond the single-issue model to explore how list scheduling contends with resource constraints on a dual-issue machine. By analyzing the performance impact of adding an extra ALU, you will uncover the concept of a shifting performance bottleneck and see Amdahl's Law in action, a crucial insight for any performance-aware engineer.", "problem": "Consider a basic block whose data dependences are captured by the following Directed Acyclic Graph (DAG). There is a single memory-derived chain consisting of $6$ iterations; at iteration $i \\in \\{1,\\dots,6\\}$, an Arithmetic Logic Unit (ALU) instruction $A_i$ computes an address from the value produced by the preceding load, and then a load $L_i$ reads from that address. Concretely, $A_1$ depends on an initial value $V_0$ that is available at time $0$, $L_1$ depends on $A_1$, $A_2$ depends on $L_1$, $L_2$ depends on $A_2$, and so on, ending with $L_6$ depending on $A_6$. In addition to this chain, there are $20$ independent ALU-only instructions $X_1,\\dots,X_{20}$ that are all ready at time $0$ and are not on the memory-derived chain; their results are only needed at the end of the basic block.\n\nMachine model and constraints:\n- The machine is a dual-issue processor: in each cycle at most $W=2$ instructions can be issued.\n- There is a single fully pipelined memory unit with capacity $R_M=1$ for loads, and each load has latency $L_M=3$ cycles (a load issued at cycle $t$ produces its result at cycle $t+3$).\n- There are $R_A$ fully pipelined ALU units. Each ALU instruction has latency $L_A=1$ cycle (an ALU issued at cycle $t$ produces its result at cycle $t+1$).\n- Distinct functional units can be used in the same cycle if issue width allows and operands are ready. An instruction can issue only when all its operands are available. Results become available exactly after their stated latencies.\n- All resources are exclusive-use per issue: a resource used by one instruction for issue in a cycle cannot be used by another instruction of the same type in that cycle.\n\nAssume a standard greedy list scheduling heuristic that always respects data dependences and resource capacities. You will analyze two configurations: baseline with $R_A=1$ ALU and upgraded with $R_A=2$ ALUs. Use only the definitions above and the readiness/latency semantics to determine the makespan under each configuration; do not assume any unstated optimizations.\n\nTask:\n- Compute the speedup $S$ defined as the ratio of the baseline makespan with $R_A=1$ to the upgraded makespan with $R_A=2$. Provide $S$ as a single reduced fraction. No rounding is required.\n- In your reasoning, quantify how sensitive the schedule length is to the ALU resource in this example and identify the true bottleneck that limits the realized speedup, but the only final reported result must be the single numerical value for $S$ as specified above.", "solution": "### Problem Analysis: Identifying Bottlenecks\nThe problem involves two groups of instructions: a dependent chain ($A_i \\rightarrow L_i$) and a set of independent ALU instructions ($X_j$). The total makespan will be determined by one of two lower bounds:\n1.  **Critical Path Bound**: The time required to execute the longest chain of dependent instructions, $A_1 \\rightarrow L_1 \\rightarrow \\dots \\rightarrow L_6$.\n2.  **Resource Bound**: The time required to issue all instructions of a certain type, limited by the number of functional units. In this case, the bottleneck will be the ALU instructions.\n\nLet's denote the makespan for the baseline case ($R_A=1$) as $T_1$ and for the upgraded case ($R_A=2$) as $T_2$. For makespan calculations, we assume cycles are 1-based. An instruction issued at cycle $t$ with latency $L$ completes at the end of cycle $t+L-1$.\n\n### Case 1: Baseline Schedule ($R_A=1$)\nWith one ALU and one Memory unit, let's analyze the bounds.\n- **Critical Path Timing**:\n  - An $A_i \\rightarrow L_i$ link involves an ALU instruction ($L_A=1$) followed by a dependent load ($L_M=3$).\n  - If $A_i$ issues at cycle $t$, its result is ready for cycle $t+1$.\n  - $L_i$ can then issue at cycle $t+1$, and its result is ready for cycle $(t+1)+3 = t+4$.\n  - $A_{i+1}$ can issue at cycle $t+4$.\n  - The time to traverse one $A_i \\rightarrow L_i \\rightarrow A_{i+1}$ link is 4 cycles.\n  - The issue times for the chain are: $A_1(t=1), L_1(t=2), A_2(t=5), L_2(t=6), \\dots$\n  - The last chain instruction, $L_6$, is issued at $t=22$. It completes at the end of cycle $22 + L_M - 1 = 22 + 3 - 1 = 24$.\n  - The critical path bound on the makespan is 24 cycles.\n\n- **Resource Bound Timing**:\n  - Total ALU instructions = 6 ($A_i$) + 20 ($X_j$) = 26.\n  - With a single ALU ($R_A=1$), at least $\\lceil 26/1 \\rceil = 26$ cycles are needed to issue all ALU instructions.\n  - This is a tighter lower bound than the critical path. The makespan will be at least 26 cycles.\n\nLet's confirm the schedule. A greedy scheduler prioritizes the critical path ($A_i, L_i$) and fills available slots with independent work ($X_j$).\n- When an $A_i$ is issued, the single ALU is used. No $X_j$ can be issued. (6 cycles used for $A_i$)\n- When an $L_i$ is issued (using the MEM unit), an $X_j$ can be co-issued (using the ALU). (6 $X_j$ issued)\n- In the latency gaps of the loads (e.g., between $L_1$ issuing at $t=2$ and $A_2$ issuing at $t=5$), the ALU is free during cycles 3 and 4. This gap of $L_M-L_A = 2$ cycles occurs 5 times (for $L_1$ through $L_5$). This allows $5 \\times 2 = 10$ more $X_j$ to be issued.\n- By cycle 22, a total of $6+10=16$ of the $X_j$ instructions have been issued.\n- The remaining $20-16=4$ $X_j$ instructions are issued in cycles 23, 24, 25, and 26.\n- The last instruction, $X_{20}$, is issued at cycle 26 and completes at the end of cycle $26 + L_A - 1 = 26$.\n- The makespan $T_1$ is $\\max(24, 26) = 26$ cycles.\n\n### Case 2: Upgraded Schedule ($R_A=2$)\nWith two ALUs and one Memory unit, the bounds are recalculated.\n- **Critical Path Timing**: This is determined by data dependencies, not ALU resources. The chain timing is unchanged. The critical path bound remains 24 cycles.\n- **Resource Bound Timing**:\n  - Total ALU instructions = 26.\n  - With two ALUs ($R_A=2$), the ALU resource bound is $\\lceil 26/2 \\rceil = 13$ cycles.\n  - Now, the critical path bound of 24 is the tighter lower bound.\n\nLet's confirm the schedule. With two ALUs, the scheduler can almost always find useful work.\n- When an $A_i$ is issued, an $X_j$ can be co-issued using the second ALU.\n- When an $L_i$ is issued, an $X_j$ can be co-issued using one of the ALUs.\n- In the latency gaps, two $X_j$ instructions can be issued per cycle.\n- All 20 independent $X_j$ instructions are easily completed long before the critical path finishes. Let's trace it:\n  - $t=1$: $\\{A_1, X_1\\}$\n  - $t=2$: $\\{L_1, X_2\\}$\n  - $t=3,4$: $\\{X_3, X_4\\}, \\{X_5, X_6\\}$\n  - By the time $A_2$ issues at $t=5$, 6 of the 20 $X_j$ instructions are already issued. With ample issue slots and ALUs, all 20 $X_j$ instructions are completed well before cycle 24.\n- The last instruction to finish is $L_6$ on the critical path, which completes at the end of cycle 24.\n- The makespan $T_2$ is 24 cycles.\n\n### Speedup Calculation\nThe speedup $S$ is the ratio of the makespans.\n$$ S = \\frac{T_1}{T_2} = \\frac{26}{24} $$\nReducing the fraction gives:\n$$ S = \\frac{13}{12} $$\nThe speedup is limited because the performance bottleneck shifts. In the baseline case, the bottleneck is the single ALU. After upgrading to two ALUs, the bottleneck becomes the un-parallelizable data dependency chain of memory loads (Amdahl's Law in action).", "answer": "$$\\boxed{\\frac{13}{12}}$$", "id": "3650798"}, {"introduction": "Perhaps the most subtle challenge in instruction scheduling arises from memory operations, where dependencies between loads and stores may be ambiguous. This final practice [@problem_id:3650816] confronts the problem of memory aliasing, where a compiler cannot prove whether two pointers reference the same location. You will create schedules under both optimistic and pessimistic assumptions to quantify exactly how this uncertainty can force the insertion of conservative dependencies, leading to stalls and significantly longer execution times.", "problem": "A compiler backend targets a statically scheduled Very Long Instruction Word (VLIW) architecture but emits a single-issue schedule to simplify analysis. For a single basic block, consider the following instruction sequence and machine model. The goal is to perform list scheduling twice, first under optimistic memory alias assumptions and then under pessimistic memory alias assumptions, and then to quantify how these assumptions change the schedule. Finally, compute a scalar summary of the divergence between these schedules.\n\nMachine model and scheduling policy:\n- The processor can issue exactly one instruction per cycle. Each issued instruction occupies the single issue slot at its start cycle; while its result latency elapses, the processor may issue other ready instructions in subsequent cycles.\n- Functional unit latencies are fixed: load has latency $3$, store has latency $2$, multiply has latency $3$, and add has latency $1$.\n- A consumer may not be issued until all of its producer results are available; i.e., an instruction with operands becomes ready only when each required operand has been produced and the corresponding latency has elapsed.\n- List scheduling is used with priority equal to the longest weighted path to the exit (critical-path length), computed as the node latency plus the maximum of the successor priorities. When multiple ready instructions have equal priority, break ties by earlier program order.\n- There is a single memory unit; however, because the machine is single-issue, resource conflicts are subsumed by the issue width constraint.\n\nProgram and dependencies:\n- Program order (from first to last): $L_{A}$, $L_{B}$, $M_{1}$, $S_{P}$, $L_{C}$, $M_{2}$, $A$.\n- Semantics:\n  - $L_{A}$: load $a \\leftarrow *p$.\n  - $L_{B}$: load $b \\leftarrow *q$.\n  - $M_{1}$: multiply $x \\leftarrow a \\times b$.\n  - $S_{P}$: store $*p \\leftarrow x$.\n  - $L_{C}$: load $c \\leftarrow *r$.\n  - $M_{2}$: multiply $y \\leftarrow c \\times x$.\n  - $A$: add $z \\leftarrow y + b$.\n- Proven and unknown aliasing facts from alias analysis:\n  - $q$ is proven not to alias with $p$ or $r$.\n  - $p$ may alias with $r$ (unknown alias relationship).\n- True data dependences:\n  - $L_{A} \\rightarrow M_{1}$, $L_{B} \\rightarrow M_{1}$, $M_{1} \\rightarrow S_{P}$, $L_{C} \\rightarrow M_{2}$, $M_{1} \\rightarrow M_{2}$, $M_{2} \\rightarrow A$, $L_{B} \\rightarrow A$.\n- Memory ordering dependences under the two alias assumptions:\n  - Optimistic assumption: do not introduce extra dependences beyond proven ones; in particular, do not constrain $S_{P}$ and $L_{C}$ because $p$ may alias $r$ is unknown.\n  - Pessimistic assumption: introduce a store-to-load dependence $S_{P} \\rightarrow L_{C}$ due to the may-alias between $p$ and $r$. Treat this dependence as requiring the store to complete before the load can be issued.\n\nTasks:\n1. Using the given latencies and dependences, compute the list-scheduling priorities (critical-path lengths to exit) for each instruction under both the optimistic and pessimistic alias assumptions. Then construct the single-issue schedule for each case by selecting, at each cycle starting from cycle $0$, the highest-priority ready instruction (breaking ties by earlier program order), and advancing time by $1$ per issued instruction. An instruction produces its result after its latency has elapsed.\n2. For each schedule, record the start cycle for each instruction and the finish cycle for the last instruction. Define a stall cycle as any cycle in which no instruction is issued because no instruction is ready.\n3. Define the schedule divergence $D$ as the sum over all instructions of the absolute difference between their start cycles under the optimistic and pessimistic schedules. Define the stall propagation difference $\\Delta S$ as the difference between the number of stall cycles in the pessimistic schedule and in the optimistic schedule.\n4. Compute the scalar summary $\\Gamma$ defined by $\\Gamma = \\frac{D}{\\Delta S}$. Round your answer to four significant figures.", "solution": "### 1. Optimistic Scheduling Analysis\n\nIn the optimistic case, we only consider true data dependencies. The store $S_P$ and load $L_C$ are assumed to be independent.\n\n#### Priority Calculation (Optimistic)\nWe calculate priorities by traversing the dependence graph backward from the exit nodes ($A$ and $S_P$).\n- $P(A) = \\text{latency}(A) = 1$\n- $P(S_P) = \\text{latency}(S_P) = 2$\n- $P(M_2) = \\text{latency}(M_2) + P(A) = 3 + 1 = 4$\n- $P(L_C) = \\text{latency}(L_C) + P(M_2) = 3 + 4 = 7$\n- $P(M_1) = \\text{latency}(M_1) + \\max(P(S_P), P(M_2)) = 3 + \\max(2, 4) = 7$\n- $P(L_A) = \\text{latency}(L_A) + P(M_1) = 3 + 7 = 10$\n- $P(L_B) = \\text{latency}(L_B) + \\max(P(M_1), P(A)) = 3 + \\max(7, 1) = 10$\n\n#### List Scheduling Trace (Optimistic)\nWe schedule starting from cycle 0. An instruction completes at `start_cycle + latency`.\n\n| Cycle | Ready Set & Priorities | Chosen | Start | Complete |\n|:-----:|:-----------------------|:-------|:-----:|:--------:|\n| 0     | {$L_A(10), L_B(10), L_C(7)$} | $L_A$  | 0     | 3        |\n| 1     | {$L_B(10), L_C(7)$}      | $L_B$  | 1     | 4        |\n| 2     | {$L_C(7)$}               | $L_C$  | 2     | 5        |\n| 3     | {}                     | Stall  | -     | -        |\n| 4     | {$M_1(7)$}               | $M_1$  | 4     | 7        |\n| 5     | {}                     | Stall  | -     | -        |\n| 6     | {}                     | Stall  | -     | -        |\n| 7     | {$M_2(4), S_P(2)$}       | $M_2$  | 7     | 10       |\n| 8     | {$S_P(2)$}               | $S_P$  | 8     | 10       |\n| 9     | {}                     | Stall  | -     | -        |\n| 10    | {$A(1)$}                 | $A$    | 10    | 11       |\n\n- **Optimistic Schedule Start Times**: $L_A(0), L_B(1), L_C(2), M_1(4), S_P(8), M_2(7), A(10)$.\n- **Makespan**: The schedule completes at cycle 11.\n- **Stalls**: $11$ cycles - $7$ instructions = $4$ stalls.\n\n### 2. Pessimistic Scheduling Analysis\n\nIn the pessimistic case, we add a dependence edge $S_P \\rightarrow L_C$ to prevent a potential RAW hazard through memory.\n\n#### Priority Calculation (Pessimistic)\nThe new edge changes the graph structure and priorities.\n- $P(A) = 1$\n- $P(M_2) = \\text{latency}(M_2) + P(A) = 3 + 1 = 4$\n- $P(L_C) = \\text{latency}(L_C) + P(M_2) = 3 + 4 = 7$\n- $P(S_P) = \\text{latency}(S_P) + P(L_C) = 2 + 7 = 9$\n- $P(M_1) = \\text{latency}(M_1) + \\max(P(S_P), P(M_2)) = 3 + \\max(9, 4) = 12$\n- $P(L_A) = \\text{latency}(L_A) + P(M_1) = 3 + 12 = 15$\n- $P(L_B) = \\text{latency}(L_B) + \\max(P(M_1), P(A)) = 3 + \\max(12, 1) = 15$\n\n#### List Scheduling Trace (Pessimistic)\nThe new dependence constrains $L_C$ from starting until $S_P$ is complete.\n\n| Cycle | Ready Set & Priorities | Chosen | Start | Complete |\n|:-----:|:-----------------------|:-------|:-----:|:--------:|\n| 0     | {$L_A(15), L_B(15)$}     | $L_A$  | 0     | 3        |\n| 1     | {$L_B(15)$}              | $L_B$  | 1     | 4        |\n| 2     | {}                     | Stall  | -     | -        |\n| 3     | {}                     | Stall  | -     | -        |\n| 4     | {$M_1(12)$}              | $M_1$  | 4     | 7        |\n| 5     | {}                     | Stall  | -     | -        |\n| 6     | {}                     | Stall  | -     | -        |\n| 7     | {$S_P(9)$}               | $S_P$  | 7     | 9        |\n| 8     | {}                     | Stall  | -     | -        |\n| 9     | {$L_C(7)$}               | $L_C$  | 9     | 12       |\n| 10    | {}                     | Stall  | -     | -        |\n| 11    | {}                     | Stall  | -     | -        |\n| 12    | {$M_2(4)$}               | $M_2$  | 12    | 15       |\n| 13    | {}                     | Stall  | -     | -        |\n| 14    | {}                     | Stall  | -     | -        |\n| 15    | {$A(1)$}                 | $A$    | 15    | 16       |\n\n- **Pessimistic Schedule Start Times**: $L_A(0), L_B(1), M_1(4), S_P(7), L_C(9), M_2(12), A(15)$.\n- **Makespan**: The schedule completes at cycle 16.\n- **Stalls**: $16$ cycles - $7$ instructions = $9$ stalls.\n\n### 3. Divergence and Stall Difference Calculation\n\n**Schedule Divergence, $D$**\nWe sum the absolute differences in start times for each instruction.\n- $D = |S_{opt}(L_A) - S_{pess}(L_A)| = |0-0| = 0$\n- $D += |S_{opt}(L_B) - S_{pess}(L_B)| = |1-1| = 0$\n- $D += |S_{opt}(M_1) - S_{pess}(M_1)| = |4-4| = 0$\n- $D += |S_{opt}(S_P) - S_{pess}(S_P)| = |8-7| = 1$\n- $D += |S_{opt}(L_C) - S_{pess}(L_C)| = |2-9| = 7$\n- $D += |S_{opt}(M_2) - S_{pess}(M_2)| = |7-12| = 5$\n- $D += |S_{opt}(A) - S_{pess}(A)| = |10-15| = 5$\nTotal divergence $D = 0 + 0 + 0 + 1 + 7 + 5 + 5 = 18$.\n\n**Stall Propagation Difference, $\\Delta S$**\nThis is the difference in the number of stall cycles.\n- Stalls (pessimistic) = 9\n- Stalls (optimistic) = 4\n$\\Delta S = 9 - 4 = 5$.\n\n### 4. Scalar Summary $\\Gamma$\nFinally, we compute the ratio $\\Gamma$.\n$$ \\Gamma = \\frac{D}{\\Delta S} = \\frac{18}{5} = 3.6 $$\nRounding to four significant figures gives $3.600$. This value quantifies the average schedule disruption per additional stall cycle caused by the pessimistic memory assumption.", "answer": "$$\n\\boxed{3.600}\n$$", "id": "3650816"}]}