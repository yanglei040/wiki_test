{"hands_on_practices": [{"introduction": "The bedrock of any safe program transformation is a precise understanding of data dependencies. Before a compiler can reorder, parallelize, or tile a loop, it must prove that the transformation will not change the program's output. This first exercise [@problem_id:3663286] allows you to practice the fundamental skill of dependence analysis, tasking you with determining if a simple statement reordering is legal by formally checking for Read-After-Write (RAW), Write-After-Read (WAR), and Write-After-Write (WAW) dependencies.", "problem": "Consider the single-nested loop with iteration variable $i$ and arrays $A$, $B$, and $C$ of length $N$, where $N \\geq 5$. The loop executes in strictly increasing $i$, and there is no aliasing among $A$, $B$, and $C$. The statements inside the loop are:\n$$S_1: A[i] = A[i-2] + B[i],$$\n$$S_2: A[i-2] = A[i-3] + C[i],$$\nfor all integer $i$ satisfying $3 \\leq i \\leq N-1$. Assume the original program order inside the loop body is $S_1$ followed by $S_2$ in each iteration.\n\nUsing the polyhedral loop optimization model, reason from the core definitions of iteration domains, affine access functions, program order schedules, and dependence direction vectors and distances. Determine whether reordering $S_1$ to execute after $S_2$ (that is, swapping them so the loop body becomes $S_2$ followed by $S_1$ for each $i$) is legal under sequential semantics, taking into account Read After Write (RAW), Write After Read (WAR), and Write After Write (WAW) dependences.\n\nYour final answer must be a single number: return $1$ if the reordering is legal and $0$ if the reordering is illegal. No rounding is required. Express your answer with no units.", "solution": "The problem requires us to determine if reordering two statements, $S_1$ and $S_2$, within a loop is a legal transformation according to sequential semantics. This analysis will be performed using the polyhedral model, which provides a formal framework for reasoning about loop optimizations. A transformation is legal if and only if it preserves all data dependences of the original program.\n\nFirst, we formalize the problem by defining the iteration domains, memory access functions, and program schedule.\n\nThe loop iterates with variable $i$ from $3$ to $N-1$. Both statements $S_1$ and $S_2$ are executed within this loop. The iteration domain for both statements is the set of integer points corresponding to the values of the loop iterator. Since this is a single-nested loop, the iteration vector is simply the scalar $i$.\nThe iteration domain $\\mathcal{D}$ for both $S_1$ and $S_2$ is:\n$$ \\mathcal{D} = \\{i \\in \\mathbb{Z} \\mid 3 \\leq i \\leq N-1 \\} $$\n\nThe statements are given as:\n$S_1: A[i] = A[i-2] + B[i]$\n$S_2: A[i-2] = A[i-3] + C[i]$\n\nThe memory accesses for each statement can be represented by affine functions of the iteration variable $i$. Since there is no aliasing between arrays $A$, $B$, and $C$, we only need to analyze dependences on array $A$.\nThe access functions for array $A$ are:\n- For $S_1$:\n  - Write access: $f_{W,A,1}(i) = i$\n  - Read access: $f_{R,A,1}(i) = i-2$\n- For $S_2$:\n  - Write access: $f_{W,A,2}(i) = i-2$\n  - Read access: $f_{R,A,2}(i) = i-3$\n\nThe original program order, or schedule, defines the execution sequence of all statement instances. In the original program, $S_1$ executes before $S_2$ in each iteration. We can represent the schedule of an instance of statement $S_k$ in iteration $i$ with a vector $(k, i)$. The execution order is the lexicographical order of these vectors.\n- Schedule of $S_1(i)$: $(1, i)$\n- Schedule of $S_2(i)$: $(2, i)$\nFor any given iteration $i$, $(1, i)$ is lexicographically smaller than $(2, i)$, meaning $S_1(i)$ executes before $S_2(i)$. For any two iterations $i$ and $j$ where $i  j$, any statement instance in iteration $i$ executes before any instance in iteration $j$.\n\nA data dependence exists from a statement instance $S_a(i)$ to an instance $S_b(j)$ if $S_a(i)$ executes before $S_b(j)$, they both access the same memory location, and at least one of these accesses is a write. Reordering statements $S_1$ and $S_2$ is legal only if no dependence is violated. A dependence from $S_1(i)$ to $S_2(i)$ (a loop-independent dependence) would be violated by the swap, because the new order would place the sink of the dependence ($S_2(i)$) before the source ($S_1(i)$). Therefore, the swap is illegal if and only if there exists a loop-independent dependence from $S_1$ to $S_2$.\n\nWe now search for such loop-independent dependences, which occur between statement instances within the same iteration. We test for Read After Write (RAW), Write After Read (WAR), and Write After Write (WAW) dependences from $S_1(i)$ to $S_2(i)$, where $i \\in \\mathcal{D}$.\n\n1.  **RAW Dependence ($S_1(i) \\to_{\\text{RAW}} S_2(i)$)**: $S_1$ writes to a location that $S_2$ reads in the same iteration.\n    This requires the write access function of $S_1$ to equal the read access function of $S_2$.\n    $$ f_{W,A,1}(i) = f_{R,A,2}(i) $$\n    $$ i = i - 3 $$\n    $$ 0 = -3 $$\n    This is a contradiction, so there is no loop-independent RAW dependence from $S_1$ to $S_2$.\n\n2.  **WAW Dependence ($S_1(i) \\to_{\\text{WAW}} S_2(i)$)**: $S_1$ and $S_2$ write to the same location in the same iteration.\n    This requires the write access functions to be equal.\n    $$ f_{W,A,1}(i) = f_{W,A,2}(i) $$\n    $$ i = i - 2 $$\n    $$ 0 = -2 $$\n    This is a contradiction, so there is no loop-independent WAW dependence from $S_1$ to $S_2$.\n\n3.  **WAR Dependence ($S_1(i) \\to_{\\text{WAR}} S_2(i)$)**: $S_1$ reads from a location that $S_2$ writes to in the same iteration.\n    This requires the read access function of $S_1$ to equal the write access function of $S_2$.\n    $$ f_{R,A,1}(i) = f_{W,A,2}(i) $$\n    $$ i - 2 = i - 2 $$\n    This equation is true for all values of $i$. Thus, for every iteration $i \\in \\mathcal{D}$, there is a WAR (anti-) dependence from $S_1(i)$ to $S_2(i)$ concerning the memory location $A[i-2]$.\n\nThe existence of this loop-independent WAR dependence $S_1(i) \\to_{\\text{WAR}} S_2(i)$ means that for the program semantics to be preserved, the read of $A[i-2]$ in statement $S_1$ must occur before the write to $A[i-2]$ in statement $S_2$ within the same iteration $i$. The original program order respects this dependence.\n\nIf we swap the statements, the new order within the loop body becomes $S_2$ followed by $S_1$. For any iteration $i$, statement $S_2(i)$ would execute before $S_1(i)$. This would cause the write to $A[i-2]$ to occur before the read from $A[i-2]$. Consequently, $S_1(i)$ would read the new value of $A[i-2]$ computed by $S_2(i)$, instead of the value it was supposed to read from a previous iteration. This violates the dependence and alters the program's computational result.\n\nFor completeness, we can check for other (loop-carried) dependences, though they do not affect the legality of this specific transformation. For $i  j$:\n-   $S_1(i) \\to S_2(j)$:\n    -   RAW: $f_{W,A,1}(i) = f_{R,A,2}(j) \\implies i = j-3 \\implies j-i=3$. There is a loop-carried RAW dependence with distance $3$.\n    -   WAW: $f_{W,A,1}(i) = f_{W,A,2}(j) \\implies i = j-2 \\implies j-i=2$. There is a loop-carried WAW dependence with distance $2$.\n    -   WAR: $f_{R,A,1}(i) = f_{W,A,2}(j) \\implies i-2 = j-2 \\implies i=j$. No dependence for $ij$.\n-   $S_2(i) \\to S_1(j)$:\n    -   No dependences exist since all dependence equations ($i-2=j-2$, $i-3=j$, $i-2=j$) require $i \\ge j$, but for a dependence from $S_2(i)$ to $S_1(j)$ to exist, we must have $i \\le j$. For $i=j$, we know $S_1(i)$ executes first, so no dependence is possible from $S_2(i)$ to $S_1(i)$.\n\nThese loop-carried dependences are \"forward\" in the loop nest (dependence distance $ 0$) and are not violated by swapping statements within an iteration. However, the presence of the single loop-independent WAR dependence $S_1(i) \\to S_2(i)$ is sufficient to make the reordering illegal.\n\nTherefore, the proposed transformation of swapping $S_1$ and $S_2$ is illegal. The corresponding numeric value is $0$.", "answer": "$$\\boxed{0}$$", "id": "3663286"}, {"introduction": "Once dependencies are understood, the polyhedral model uses affine schedules to legally reorder the execution of loop iterations and expose parallelism. A schedule can be thought of as a transformation that assigns a new \"execution time\" to each statement instance, and its effectiveness is judged by how it handles the program's dependencies. This exercise [@problem_id:3663326] challenges you to analyze a multidimensional schedule, determine its legality by examining how it transforms a dependence vector, and quantify the resulting parallelism by identifying parallel \"bands\" in the schedule.", "problem": "Consider the iteration domain $D=\\{(i,j)\\in\\mathbb{Z}^{2}\\mid 0\\le iN,\\ 0\\le jM\\}$ for integer parameters $N$ and $M$, and an affine multidimensional schedule $\\theta:\\mathbb{Z}^{2}\\to\\mathbb{Z}^{2}$ defined by $\\theta(i,j)=(i,\\ j-i)$. Assume a uniform flow dependence described by the dependence vector $(\\Delta i,\\Delta j)=(0,1)$, meaning an instance at $(i,j)$ must execute before the instance at $(i,j+1)$ whenever both lie in $D$. Using the core definitions of the polyhedral model—namely, that a schedule must respect dependence causality through strict lexicographic positivity of the schedule difference for flow dependences—determine:\n- The legality indicator $L\\in\\{0,1\\}$, where $L=1$ if $\\theta$ is a legal schedule that enforces $\\theta(i,j)\\prec_{\\text{lex}}\\theta(i+\\Delta i,j+\\Delta j)$ for all $(i,j)$ and $L=0$ otherwise.\n- The number $P\\in\\mathbb{N}$ of parallel schedule dimensions (bands) in $\\theta$ under the given dependence, where a schedule dimension $k$ is considered parallel if the dependence distance component along that dimension is $0$ for the given dependence, implying no dependence is carried by that dimension.\n\nReport your final answer as a row matrix $\\begin{pmatrix}L  P\\end{pmatrix}$. No rounding is required and no units should be used.", "solution": "The problem requires us to determine two values for a given polyhedral loop optimization scenario: the legality indicator $L$ and the number of parallel schedule dimensions $P$.\n\nFirst, we determine the legality of the schedule $\\theta$. A schedule is defined as legal if, for every dependence from an iteration point $p$ to an iteration point $p'$, the schedule preserves the execution order. This means the scheduled time for $p$ must be lexicographically smaller than the scheduled time for $p'$. The problem states this condition as $\\theta(p) \\prec_{\\text{lex}} \\theta(p')$. Let $p=(i,j)$ be the source iteration and $d=(\\Delta i, \\Delta j)=(0,1)$ be the uniform flow dependence vector. The destination iteration is then $p' = p+d = (i, j+1)$.\n\nThe legality condition can be restated using the dependence distance vector, $\\delta$, defined as the difference between the scheduled times of the destination and source iterations:\n$$ \\delta = \\theta(p') - \\theta(p) $$\nThe schedule is legal if and only if this distance vector is lexicographically positive, i.e., $\\delta \\succ_{\\text{lex}} 0$, for all dependences. A vector $v=(v_1, v_2, \\dots, v_n)$ is lexicographically positive if its first non-zero component is positive.\n\nThe given schedule is $\\theta:\\mathbb{Z}^{2}\\to\\mathbb{Z}^{2}$ defined by $\\theta(i,j)=(i,\\ j-i)$.\nLet's compute the scheduled times for the source and destination iterations.\nThe source iteration is $p=(i,j)$. Its scheduled time is:\n$$ \\theta(p) = \\theta(i,j) = (i,\\ j-i) $$\nThe destination iteration is $p'=(i, j+1)$. Its scheduled time is:\n$$ \\theta(p') = \\theta(i, j+1) = (i, (j+1)-i) = (i, j-i+1) $$\n\nNow, we compute the dependence distance vector $\\delta$:\n$$ \\delta = \\theta(p') - \\theta(p) = (i, j-i+1) - (i, j-i) $$\n$$ \\delta = (i-i, (j-i+1)-(j-i)) $$\n$$ \\delta = (0, 1) $$\nThe dependence distance vector is constant for this uniform dependence and affine schedule. We must check if $\\delta = (0,1)$ is lexicographically positive. The first component of $\\delta$ is $0$. We then examine the second component, which is $1$. Since $1  0$, the vector $\\delta=(0,1)$ is lexicographically positive.\n$$ (0,1) \\succ_{\\text{lex}} (0,0) $$\nThe schedule respects the dependence causality. Therefore, the schedule is legal. The legality indicator $L$ is $1$.\n$$ L=1 $$\n\nNext, we determine the number of parallel schedule dimensions, $P$. The problem defines a schedule dimension $k$ as parallel if the $k$-th component of the dependence distance vector is $0$. The number $P$ is the count of such dimensions that form the outermost parallel band. This corresponds to the number of leading zeros in the dependence distance vector.\n\nThe dependence distance vector we calculated is $\\delta = (\\delta_1, \\delta_2) = (0,1)$.\nLet's examine its components:\n1.  The first component is $\\delta_1 = 0$. According to the definition, the first schedule dimension is parallel. This means that for the given dependence, all dependent operations are mapped to the same coordinate in the first dimension of the transformed iteration space.\n2.  The second component is $\\delta_2 = 1$. Since this is non-zero, the second schedule dimension is not parallel; it is sequential. This dimension carries the dependence, ensuring correctness.\n\nThe number of leading zeros in the vector $\\delta=(0,1)$ is one. This single parallel dimension forms an outer band of parallelism. Thus, the number of parallel schedule dimensions is $1$.\n$$ P=1 $$\n\nThe problem asks for the final answer as a row matrix $\\begin{pmatrix}L  P\\end{pmatrix}$. Substituting the values we found:\n$$ \\begin{pmatrix} 1  1 \\end{pmatrix} $$\nThis indicates that the schedule is legal, and it exposes one dimension of loop-level parallelism.", "answer": "$$ \\boxed{\\begin{pmatrix} 1  1 \\end{pmatrix}} $$", "id": "3663326"}, {"introduction": "Sometimes, the dependencies within a loop nest, like those in a Gauss-Seidel-style update, form a cycle that prevents effective parallelization via scheduling alone. In such cases, a more powerful strategy is needed: transforming the program's memory access patterns to break the dependence cycle itself. This practice [@problem_id:3663289] explores the common technique of array expansion, where a temporary array is introduced to convert a sequential, in-place update into a parallel one, thereby enabling strategies like tiling that would otherwise be illegal.", "problem": "Consider the following two-dimensional Gauss–Seidel-style update written in the polyhedral model. The iteration domain is the integer polyhedron $$D = \\{(i,j)\\in \\mathbb{Z}^2 \\mid 1 \\le i \\le N,\\ 1 \\le j \\le N\\},$$ where $N \\in \\mathbb{Z}_{0}$. The single statement $S(i,j)$ executes for each $(i,j)\\in D$ and performs an in-place update of array $A$ using four-point neighbors and the current element:\n$$\n\\text{Write: } W_A(i,j) = (i,j),\\quad\n\\text{Read: } R_A(i,j) = \\{(i-1,j),\\ (i,j-1),\\ (i+1,j),\\ (i,j+1),\\ (i,j)\\},\n$$\nwith reads outside the boundary clamped to valid indices in a standard way. In a lexicographic execution order on $(i,j)$ (for example, row-major), $S(i,j)$ has flow dependences from $S(i-1,j)$ and $S(i,j-1)$ within the same sweep, which induce a cycle when rectangular tiling is applied simultaneously along $i$ and $j$.\n\nLet rectangular tiles be defined by a tiling map\n$$\n\\tau(i,j) = \\big(b_i, b_j, x_i, x_j\\big),\\quad\nb_i = \\left\\lfloor \\frac{i-1}{p} \\right\\rfloor,\\quad b_j = \\left\\lfloor \\frac{j-1}{q} \\right\\rfloor,\\quad\nx_i = i - b_i p,\\quad x_j = j - b_j q,\n$$\nfor given tile sizes $p,q \\in \\mathbb{Z}_{0}$. The tile coordinate is $(b_i,b_j)$, the intra-tile coordinate is $(x_i,x_j)$. Under the original in-place update, dependences along both $+i$ and $+j$ directions create a cycle in the tile-level dependence graph, forcing serialization (for example, via wavefront scheduling) and preventing running all tiles in parallel.\n\nA common strategy in the polyhedral model to break such a dependence cycle is to introduce a new temporary array and modify access functions so that the compute phase reads only the pre-update values and writes only to the temporary, followed by a separate copy/update phase that commits the temporary to the original array. This is sometimes described as converting a Gauss–Seidel update into a Jacobi-style two-phase update through array expansion.\n\nYou are asked to select the transformation that correctly modifies access functions to break the cycle and unlock parallel tiles under a tile schedule that orders only by $(b_i,b_j)$, i.e.,\n$$\n\\Theta_{\\text{tile}}(i,j) = (b_i, b_j),\n$$\nwith all tiles at different $(b_i,b_j)$ allowed to execute concurrently during the compute phase when no inter-tile dependences exist.\n\nWhich option precisely specifies access-function modifications and phase ordering that yield an acyclic tile-level dependence graph for the compute phase, making all tiles parallel under $\\Theta_{\\text{tile}}$, while remaining scientifically plausible?\n\nA. Replace all writes $W_A(i,j)$ by writes to a fresh temporary $B$ with $W_B(i,j) = (i,j)$, and keep all reads from $A$ unchanged: $R_A(i,j)$ as given. After completing the compute phase over the entire domain $D$, perform a separate copy phase $C(i,j)$ with write $W_A^{\\text{copy}}(i,j) = (i,j)$ and read $R_B^{\\text{copy}}(i,j) = (i,j)$, i.e., $A(i,j) \\gets B(i,j)$. Execute the compute phase for all tiles in parallel using $\\Theta_{\\text{tile}}$, then execute the copy phase element-wise (or per tile) without introducing inter-tile reads.\n\nB. Replace writes to $A$ by writes to $B$ as $W_B(i,j) = (i,j)$, but read the “earlier” neighbors from $B$ as $R_B(i,j) \\supset \\{(i-1,j),(i,j-1)\\}$ and the “later” neighbors from $A$ as $R_A(i,j) \\supset \\{(i+1,j),(i,j+1),(i,j)\\}$. Perform a per-tile copy $A \\gets B$ immediately after each tile completes.\n\nC. Keep writes to $A$ unchanged, $W_A(i,j) = (i,j)$, but redirect all neighbor reads to the temporary $B$ with $R_B(i,j) = \\{(i-1,j), (i,j-1), (i+1,j), (i,j+1), (i,j)\\}$, without an explicit initialization of $B$ from $A$. Allow tiles to run in parallel under $\\Theta_{\\text{tile}}$ because reads are from $B$.\n\nD. Write to $B$ with $W_B(i,j) = (i,j)$, then immediately copy within the same $(i,j)$ iteration by $A(i,j) \\gets B(i,j)$ (i.e., interleave compute and copy at element granularity). Maintain the original neighbor reads from $A$ and permit parallel tiles under $\\Theta_{\\text{tile}}$ because each element is committed before neighbors read it.\n\nSelect the correct option.", "solution": "The user wants me to identify the correct transformation to enable parallel execution of tiles for a Gauss-Seidel-style update.\n\n### Step 1: Extract Givens\n- **Iteration Domain**: $D = \\{(i,j)\\in \\mathbb{Z}^2 \\mid 1 \\le i \\le N,\\ 1 \\le j \\le N\\}$ for some constant $N \\in \\mathbb{Z}_{0}$.\n- **Statement**: $S(i,j)$ executes for each point $(i,j)$ in the domain $D$.\n- **Access Functions**:\n    - Write to array $A$: $W_A(i,j) = (i,j)$.\n    - Read from array $A$: $R_A(i,j) = \\{(i-1,j), (i,j-1), (i+1,j), (i,j+1), (i,j)\\}$.\n- **Execution Order and Dependences**: With a lexicographic execution order on $(i,j)$, statement $S(i,j)$ has flow dependences from $S(i-1,j)$ and $S(i,j-1)$.\n- **Tiling**: Rectangular tiles are defined by the map $\\tau(i,j) = (b_i, b_j, x_i, x_j)$, where:\n    - Tile coordinates: $b_i = \\lfloor \\frac{i-1}{p} \\rfloor$, $b_j = \\lfloor \\frac{j-1}{q} \\rfloor$ for tile sizes $p, q \\in \\mathbb{Z}_{0}$.\n    - Intra-tile coordinates: $x_i = i - b_i p$, $x_j = j - b_j q$.\n- **Problem**: The flow dependences create inter-tile dependences, which prevent the parallel execution of all tiles under the tile schedule $\\Theta_{\\text{tile}}(i,j) = (b_i, b_j)$.\n- **Goal**: Find a transformation that modifies access functions to create an acyclic tile-level dependence graph for the compute phase, allowing all tiles to be executed concurrently.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically grounded in the field of compiler optimizations, specifically the polyhedral model. It describes a standard scenario: a regular loop nest with stencil-like dependences (a Gauss-Seidel relaxation) that limits parallelism when tiled. The goal is to apply a known technique—array expansion to convert the update to a Jacobi-style one—to enable more parallelism. The terminology (iteration domain, access functions, dependence, tiling, tile schedule) is standard and used correctly. The problem is well-posed, objective, and contains sufficient information to determine the correct transformation. No scientific or logical flaws are present.\n\n### Step 3: Verdict and Action\nThe problem is valid. A rigorous derivation and evaluation of options will be performed.\n\n### Principle-Based Derivation\n\nThe core problem is the presence of flow dependences within a single computational sweep. Specifically, for a lexicographical schedule $(i,j)$, the computation of $A(i,j)$ depends on values $A(i-1,j)$ and $A(i,j-1)$ that have been updated in the same sweep. Let the update function be denoted by $f$. The update is $A(i,j) \\gets f(A(i-1,j), A(i,j-1), \\dots)$.\n\nA flow dependence exists from a statement $S_1$ to $S_2$ if $S_1$ writes to a memory location that $S_2$ subsequently reads, and $S_1$ is executed before $S_2$.\n1.  **Dependence from $S(i-1,j)$ to $S(i,j)$**: $S(i-1,j)$ writes to $A(i-1,j)$. Later, $S(i,j)$ reads $A(i-1,j)$. This is a flow dependence with dependence vector $\\mathbf{d}_1 = (i,j) - (i-1,j) = (1,0)$.\n2.  **Dependence from $S(i,j-1)$ to $S(i,j)$**: $S(i,j-1)$ writes to $A(i,j-1)$. Later, $S(i,j)$ reads $A(i,j-1)$. This is a flow dependence with dependence vector $\\mathbf{d}_2 = (i,j) - (i,j-1) = (0,1)$.\n\nThe reads from $A(i+1,j)$ and $A(i,j+1)$ are from locations that will be updated later in the sweep. Thus, these reads access values from the beginning of the sweep and do not introduce additional flow dependences.\n\nWhen tiling is applied, these dependences may cross tile boundaries.\n- For $\\mathbf{d}_1 = (1,0)$: An iteration $(i,j)$ may depend on $(i-1,j)$. If $i-1$ is a multiple of the tile width $p$, then $(i,j)$ and $(i-1,j)$ lie in different tiles. Specifically, the tile containing $(i,j)$ depends on the tile to its left.\n- For $\\mathbf{d}_2 = (0,1)$: An iteration $(i,j)$ may depend on $(i,j-1)$. If $j-1$ is a multiple of the tile height $q$, then $(i,j)$ and $(i,j-1)$ lie in different tiles. The tile containing $(i,j)$ depends on the tile \"above\" it.\n\nThese inter-tile dependences mean that tile $(b_i, b_j)$ cannot begin execution until its neighboring tiles $(b_i-1, b_j)$ and $(b_i, b_j-1)$ have produced the necessary boundary values. This prevents the simultaneous parallel execution of all tiles.\n\nTo break these dependences for the compute phase, we must ensure that no computation reads a value written during the same phase. This is achieved by converting the Gauss-Seidel update into a Jacobi-style update using a temporary array, say $B$. This technique is known as array expansion or privatization.\n\nThe transformation involves two distinct phases:\n1.  **Compute Phase**: All computations read from the original array $A$ (which holds the state from before the sweep) and write their results to the temporary array $B$.\n    - The update becomes: $B(i,j) \\gets f(A(i-1,j), A(i,j-1), A(i+1,j), A(i,j+1), A(i,j))$.\n    - For any two iterations $S'(i_1, j_1)$ and $S'(i_2, j_2)$ in this phase:\n        - All reads are from array $A$.\n        - All writes are to array $B$.\n        - Since $(i_1, j_1) \\neq (i_2, j_2)$, they write to distinct locations $B(i_1, j_1)$ and $B(i_2, j_2)$.\n        - There is no instance where one iteration writes to a location that another reads. Therefore, there are no flow, anti-, or output dependences between any two iterations in the compute phase.\n    - With no dependences, all iterations $(i,j) \\in D$ can be executed in any order, including fully in parallel. This implies that all tiles can be executed concurrently without any inter-tile dependences.\n\n2.  **Copy Phase**: After the compute phase has completed for the entire domain $D$, the results from the temporary array $B$ are copied back to the original array $A$.\n    - The copy operation is: $A(i,j) \\gets B(i,j)$ for all $(i,j) \\in D$.\n    - Each copy operation $C(i,j)$ reads from a unique location $B(i,j)$ and writes to a unique location $A(i,j)$. There are no dependences between these operations.\n    - Thus, the copy phase can also be fully parallelized.\n\nA global synchronization barrier must be placed between the compute phase and the copy phase to ensure correctness. This transformation correctly eliminates inter-tile dependences during the compute phase, achieving the stated goal.\n\n### Option-by-Option Analysis\n\n**A. Replace all writes $W_A(i,j)$ by writes to a fresh temporary $B$ with $W_B(i,j) = (i,j)$, and keep all reads from $A$ unchanged: $R_A(i,j)$ as given. After completing the compute phase over the entire domain $D$, perform a separate copy phase $C(i,j)$ with write $W_A^{\\text{copy}}(i,j) = (i,j)$ and read $R_B^{\\text{copy}}(i,j) = (i,j)$, i.e., $A(i,j) \\gets B(i,j)$. Execute the compute phase for all tiles in parallel using $\\Theta_{\\text{tile}}$, then execute the copy phase element-wise (or per tile) without introducing inter-tile reads.**\nThis option accurately describes the Jacobi-style transformation derived above. The compute phase reads only from $A$ and writes to $B$, which removes all dependences between iterations of the compute loop nest. This correctly results in an acyclic (in fact, empty) tile-level dependence graph for the compute phase, allowing all tiles to run in parallel. The subsequent copy phase correctly commits the results. This is the standard, correct method.\n**Verdict: Correct**\n\n**B. Replace writes to $A$ by writes to $B$ as $W_B(i,j) = (i,j)$, but read the “earlier” neighbors from $B$ as $R_B(i,j) \\supset \\{(i-1,j),(i,j-1)\\}$ and the “later” neighbors from $A$ as $R_A(i,j) \\supset \\{(i+1,j),(i,j+1),(i,j)\\}$. Perform a per-tile copy $A \\gets B$ immediately after each tile completes.**\nThis transformation is flawed. By reading the \"earlier\" neighbors from the temporary array $B$, it re-introduces the very flow dependences we aim to eliminate. The computation of $B(i,j)$ now depends on $B(i-1,j)$ and $B(i,j-1)$, which are computed in the same phase. This leads to dependence vectors $(1,0)$ and $(0,1)$, creating inter-tile dependences and preventing parallel execution of all tiles. The per-tile copy also introduces complex race conditions and incorrect semantics.\n**Verdict: Incorrect**\n\n**C. Keep writes to $A$ unchanged, $W_A(i,j) = (i,j)$, but redirect all neighbor reads to the temporary $B$ with $R_B(i,j) = \\{(i-1,j), (i,j-1), (i+1,j), (i,j+1), (i,j)\\}$, without an explicit initialization of $B$ from $A$. Allow tiles to run in parallel under $\\Theta_{\\text{tile}}$ because reads are from $B$.**\nThis option is scientifically unsound. It proposes reading from an uninitialized array $B$. Computations based on undefined data will produce meaningless results. In any valid implementation, $B$ would first need to be initialized with the contents of $A$. If we assume such an initialization step ($B \\gets A$), the compute phase would be $A(i,j) \\gets f(B(\\dots))$. While this allows parallel execution (read from $B$, write to $A$), the original option as stated is fundamentally flawed due to the use of uninitialized data.\n**Verdict: Incorrect**\n\n**D. Write to $B$ with $W_B(i,j) = (i,j)$, then immediately copy within the same $(i,j)$ iteration by $A(i,j) \\gets B(i,j)$ (i.e., interleave compute and copy at element granularity). Maintain the original neighbor reads from $A$ and permit parallel tiles under $\\Theta_{\\text{tile}}$ because each element is committed before neighbors read it.**\nThis transformation is functionally equivalent to the original in-place update. The sequence `temp = calculation_from_A; B(i,j) = temp; A(i,j) = B(i,j);` simplifies to `A(i,j) = calculation_from_A;`. The temporary array $B$ is completely redundant. Consequently, all original flow dependences, such as from $S(i-1,j)$ to $S(i,j)$, are preserved. The claim that this allows parallel tiles is false; the inter-tile dependences remain, and the transformation has achieved nothing.\n**Verdict: Incorrect**", "answer": "$$\\boxed{A}$$", "id": "3663289"}]}