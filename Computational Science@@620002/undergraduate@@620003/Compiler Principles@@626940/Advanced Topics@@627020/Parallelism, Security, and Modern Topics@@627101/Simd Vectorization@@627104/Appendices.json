{"hands_on_practices": [{"introduction": "Achieving peak performance with SIMD instructions is critically dependent on how data is accessed from memory. This first exercise provides a practical look at one of the most fundamental performance optimizations: ensuring memory alignment. By working through a hypothetical but realistic scenario, you will learn to apply a technique called loop peeling to align the main vectorized body of a loop and quantify the significant cycle savings that result from avoiding costly misaligned memory accesses [@problem_id:3670086].", "problem": "A compiler vectorizes the loop updating a contiguous array of single-precision values as follows: for index $i$ from $0$ to $N-1$, it performs $x[i] \\leftarrow x[i] + c$, where $x$ is an array of elements of size $E$ bytes and $c$ is a scalar constant. The machine supports Single Instruction Multiple Data (SIMD) vectors of width $W$ elements, with natural alignment $A$ bytes. The base address $p$ of $x$ is such that $p \\bmod A = 8$ bytes. Assume $E = 4$ bytes (single precision), $W = 8$, and $A = 32$ bytes, so that $W \\cdot E = A$. The total number of elements is $N = 1022$.\n\nUse the following cost model, which is throughput-limited and additive across iterations with no overlap:\n- A scalar iteration processing one element costs $c_s = 3$ cycles.\n- An aligned SIMD vector iteration processing $W$ elements costs $c_a = 2$ cycles.\n- A misaligned SIMD vector iteration (due to base address misalignment sustained across the loop) incurs an additional penalty of $d = 5$ cycles per vector iteration, so that its cost is $c_a + d$.\n\nStarting from the definitions of memory alignment and congruences, determine the minimal nonnegative number of scalar peel iterations $k$ required so that after peeling $k$ scalar iterations, the SIMD body operates on an address aligned to $A$. Then, propose the loop structure consisting of a scalar prologue of $k$ iterations, an aligned SIMD body, and a scalar epilogue for the remainder. Quantify the total number of cycles saved by performing this loop peeling compared to a naive SIMD vectorization that does not peel and thus executes all its vector iterations misaligned. Express the final result as a single integer number of cycles saved. Do not include any units in your final boxed answer.", "solution": "The user wants me to solve a compiler optimization problem regarding SIMD vectorization and memory alignment. I will first validate the problem statement.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- The loop operation is $x[i] \\leftarrow x[i] + c$ for index $i$ from $0$ to $N-1$.\n- Total number of elements: $N = 1022$.\n- Size of each element: $E = 4$ bytes.\n- SIMD vector width: $W = 8$ elements.\n- Natural alignment requirement: $A = 32$ bytes.\n- The base address $p$ of the array $x$ has the property $p \\bmod A = 8$ bytes.\n- It is stated that $W \\cdot E = A$, which is $8 \\cdot 4 = 32$, consistent with the given values.\n- Cost of a single scalar iteration: $c_s = 3$ cycles.\n- Cost of an aligned SIMD vector iteration: $c_a = 2$ cycles.\n- Cost penalty for a misaligned SIMD vector iteration: $d = 5$ cycles.\n- The cost of a misaligned SIMD vector iteration is $c_a + d$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem describes a classic scenario in computer architecture and compiler optimization, specifically loop vectorization with memory alignment considerations. The cost model, while simplified, reflects the real-world performance trade-offs between scalar and vector processing, and the penalties for misaligned memory access. The principles are sound.\n- **Well-Posed:** All necessary constants ($N$, $E$, $W$, $A$, $c_s$, $c_a$, $d$) and initial conditions ($p \\bmod A = 8$) are provided. The questions asked—to find the number of peel iterations, describe the resulting loop structure, and quantify the performance gain—are specific and lead to a unique, meaningful solution.\n- **Objective:** The problem is stated in precise, quantitative terms. The cost model is explicitly defined, leaving no room for subjective interpretation.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is scientifically sound, well-posed, and objective. I will proceed with the solution.\n\n### Solution\n\nThe solution involves three main parts:\n1.  Determining the number of scalar peel iterations, $k$, required to align the starting address of the main vector loop.\n2.  Calculating the total execution time for a naive, misaligned vectorization strategy.\n3.  Calculating the total execution time for an optimized strategy using loop peeling, and then finding the difference to quantify the cycles saved.\n\n**1. Determine the Number of Peel Iterations ($k$)**\n\nThe address of the $i$-th element of the array $x$ is given by $p + i \\cdot E$, where $p$ is the base address. For the SIMD vector loop to operate on aligned data, its starting address must be a multiple of the alignment size, $A$. If we peel $k$ scalar iterations, the vector loop will start processing at index $k$. The address of this element is $p + k \\cdot E$. The alignment condition is therefore:\n$$(p + k \\cdot E) \\bmod A = 0$$\nWe are given the following values: $p \\bmod A = 8$, $E = 4$, and $A = 32$. Substituting these values into the congruence relation:\n$$((p \\bmod A) + (k \\cdot E) \\bmod A) \\bmod A = 0$$\n$$ (8 + 4k) \\bmod 32 = 0 $$\nThis equation implies that $8 + 4k$ must be a multiple of $32$. We can write this as an equation in integers:\n$$8 + 4k = 32j$$\nwhere $j$ is a positive integer. We seek the smallest non-negative integer $k$ that satisfies this condition.\n$$4k = 32j - 8$$\n$$k = 8j - 2$$\nTo find the minimum non-negative $k$, we test integer values for $j$ starting from $1$:\n- For $j=1$: $k = 8(1) - 2 = 6$.\nThis is the smallest non-negative integer value for $k$.\nThus, $k=6$ scalar iterations must be peeled from the beginning of the loop.\n\nThe proposed loop structure is as follows:\n- **Scalar prologue:** A loop that iterates from $i=0$ to $k-1=5$, executing the operation $x[i] \\leftarrow x[i] + c$.\n- **Aligned SIMD body:** A vectorized loop that starts at index $i=k=6$. It processes $W=8$ elements per iteration. The number of elements to be processed in this loop and the subsequent epilogue is $N - k = 1022 - 6 = 1016$. The number of vector iterations is $\\lfloor \\frac{1016}{8} \\rfloor = 127$. This loop covers indices from $6$ to $6 + (127 \\cdot 8) - 1 = 1021$.\n- **Scalar epilogue:** A loop for the remaining elements. The number of remaining elements is $(N - k) \\bmod W = 1016 \\bmod 8 = 0$. Therefore, no scalar epilogue is needed.\n\n**2. Calculate the Cost of Naive (Misaligned) Vectorization**\n\nIn a naive vectorization strategy, the compiler generates a SIMD loop starting at index $0$ without peeling. Since the base address $p$ is misaligned ($p \\bmod 32 = 8$), all vector memory accesses will be misaligned, because the starting address of any subsequent vector chunk (at index $j \\cdot W$) is $p + (j \\cdot W) \\cdot E = p + j \\cdot A$, and $(p + j \\cdot A) \\bmod A = p \\bmod A = 8 \\neq 0$.\n\nThe cost of a misaligned vector iteration is $c_m = c_a + d = 2 + 5 = 7$ cycles.\nThe number of full vector iterations is $N_{\\text{vec, naive}} = \\lfloor \\frac{N}{W} \\rfloor = \\lfloor \\frac{1022}{8} \\rfloor = 127$.\nThe cost of the vector part is $T_{\\text{vec, naive}} = N_{\\text{vec, naive}} \\cdot c_m = 127 \\cdot 7 = 889$ cycles.\n\nThe number of remaining elements that must be processed by a scalar epilogue is $N_{\\text{epilogue, naive}} = N \\bmod W = 1022 \\bmod 8 = 6$.\nThe cost of the scalar epilogue is $T_{\\text{epilogue, naive}} = N_{\\text{epilogue, naive}} \\cdot c_s = 6 \\cdot 3 = 18$ cycles.\n\nThe total cost for the naive strategy is $T_{\\text{naive}} = T_{\\text{vec, naive}} + T_{\\text{epilogue, naive}} = 889 + 18 = 907$ cycles.\n\n**3. Calculate the Cost of Optimized (Peeled) Vectorization and the Savings**\n\nIn the optimized strategy, we first execute a scalar prologue of $k=6$ iterations.\nThe cost of the prologue is $T_{\\text{prologue}} = k \\cdot c_s = 6 \\cdot 3 = 18$ cycles.\n\nAfter the prologue, $N' = N - k = 1022 - 6 = 1016$ elements remain. The SIMD loop starts at index $6$, where the address is aligned. All vector iterations in this loop are aligned.\nThe number of aligned vector iterations is $N_{\\text{vec, opt}} = \\lfloor \\frac{N'}{W} \\rfloor = \\lfloor \\frac{1016}{8} \\rfloor = 127$.\nThe cost of the vector part is $T_{\\text{vec, opt}} = N_{\\text{vec, opt}} \\cdot c_a = 127 \\cdot 2 = 254$ cycles.\n\nThe number of elements for a final scalar epilogue is $N_{\\text{epilogue, opt}} = N' \\bmod W = 1016 \\bmod 8 = 0$.\nThe cost of the epilogue is $T_{\\text{epilogue, opt}} = 0$ cycles.\n\nThe total cost for the optimized strategy is $T_{\\text{opt}} = T_{\\text{prologue}} + T_{\\text{vec, opt}} + T_{\\text{epilogue, opt}} = 18 + 254 + 0 = 272$ cycles.\n\nFinally, the total number of cycles saved is the difference between the naive and optimized costs:\n$$\\text{Savings} = T_{\\text{naive}} - T_{\\text{opt}} = 907 - 272 = 635$$\n\nThe loop peeling strategy saves $635$ cycles compared to the naive misaligned vectorization.", "answer": "$$\\boxed{635}$$", "id": "3670086"}, {"introduction": "While vectorization offers substantial speedups, a compiler's primary duty is to preserve the program's original, observable behavior, a constraint known as the \"as-if\" rule. This practice explores what happens when this rule is challenged by the speculative nature of SIMD execution, particularly in loops containing operations that might raise exceptions like division-by-zero. You will model how naive vectorization can lead to incorrect results and then implement compiler safeguards that ensure semantic correctness, deepening your understanding of the fine line between aggressive optimization and correctness [@problem_id:3670137].", "problem": "You are to design and implement a complete and runnable program to detect when a compiler transformation known as Single Instruction Multiple Data (SIMD) vectorization can change observable behavior for a loop that may raise exceptions due to division by zero. The core scenario is a loop computing $c[i] = a[i] / b[i]$ for indices $i$ from $0$ to $n-1$, where $b[i]$ may equal $0$, thereby potentially changing how exceptions occur and how elements of $c[i]$ are written when the loop is vectorized.\n\nFundamental base: In a sequential execution model, the program executes operations in program order (left-to-right over the loop indices). Observable behavior includes which iterations perform writes to memory and whether exceptional conditions occur, such as division by zero. Compilers are constrained to transformations that preserve observable behavior according to the as-if rule and to sequential semantics of the source language. However, speculative and out-of-order evaluation in SIMD vectorization may cause operations that would not execute under sequential short-circuit semantics to be performed, thereby writing additional elements and altering the index at which exceptions appear to occur. These facts are widely accepted and serve as the foundational starting point of this task.\n\nDesign a detector and safeguards using the following requirements:\n\n1. Define a sequential scalar semantics for the loop: iterate $i$ from $0$ to $n-1$, compute $a[i]/b[i]$, and assign to $c[i]$ until the first index $s$ such that $b[s] = 0$, at which point the loop stops without writing $c[s]$ or any $c[j]$ for $j \\ge s$. For the purposes of this detector, treat any value $b[i]$ satisfying $b[i] = 0$ (including $-0$) as a zero.\n\n2. Define a naive SIMD-vectorized semantics model with a fixed vector width $w$: process indices in chunks of width $w$, performing all divisions $a[i+j]/b[i+j]$ for chunk lanes $j$ where $i+j  n$, and write all results to $c[i+j]$ even if some $b[i+j] = 0$. This simulates speculative evaluation across lanes, and it does not short-circuit at the first zero.\n\n3. Define two safeguards to preserve observable behavior identical to the scalar semantics:\n   - Guard $G_1$ (pre-scan and fallback): Pre-scan the entire $b[i]$ array to check whether any $b[i] = 0$. If any zero is found, skip vectorization entirely and execute the scalar loop; otherwise, use naive SIMD-vectorized semantics. This is a conservative choice ensuring identical behavior when zeros exist, and full vectorization only when safe.\n   - Guard $G_2$ (masked prefix): Compute the index $s$ of the first zero by pre-scan. Perform vectorized computation only for indices $i  s$; do not write $c[i]$ for any $i \\ge s$. This simulates a vectorized masked execution that respects short-circuit behavior and avoids speculative writes at and beyond the first exceptional index.\n\n4. Implement a detector that compares the results produced by the naive SIMD-vectorized semantics to the results produced by the scalar semantics and reports whether behavior changed. To make comparison unambiguous, initialize all $c[i]$ to not-a-number and treat any $c[i]$ that remains not-a-number as \"not written.\" A behavioral change is detected if any index $k$ has differing write status or differing value between the vectorized semantics and the scalar semantics. Use double-precision floating point arithmetic for all computations. If any division generates $\\infty$ or not-a-number under vectorized semantics, treat it as a valid write for detection purposes.\n\n5. For each safeguard ($G_1$ and $G_2$), implement its corresponding semantics and report whether the safeguard preserves identical behavior to the scalar semantics.\n\n6. Your program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a three-integer list $[x_1, x_2, x_3]$ for a test case, with $x_1$ indicating whether naive SIMD-vectorized semantics changed behavior relative to scalar semantics, $x_2$ indicating whether Guard $G_1$ preserved behavior, and $x_3$ indicating whether Guard $G_2$ preserved behavior. Use $0$ for \"no change\" or \"preserved\" and $1$ for \"changed\" or \"not preserved.\"\n\nThere are no physical units involved. Angles are not applicable. Percentages are not applicable.\n\nTest suite and coverage:\n\nUse a fixed vector width $w = 4$ for all test cases. The loop length $n$ and arrays $a$ and $b$ for each test are specified below. All values are double-precision literals.\n\n- Test case $1$ (happy path, no zeros):\n  - $n = 6$\n  - $a = [\\,1,\\,2,\\,3,\\,4,\\,5,\\,6\\,]$\n  - $b = [\\,1,\\,1,\\,1,\\,1,\\,1,\\,1\\,]$\n\n- Test case $2$ (multiple zeros, early zero):\n  - $n = 6$\n  - $a = [\\,10,\\,20,\\,30,\\,40,\\,50,\\,60\\,]$\n  - $b = [\\,2,\\,0,\\,5,\\,10,\\,0,\\,3\\,]$\n\n- Test case $3$ (zero at the first index):\n  - $n = 4$\n  - $a = [\\,7,\\,8,\\,9,\\,10\\,]$\n  - $b = [\\,0,\\,2,\\,3,\\,4\\,]$\n\n- Test case $4$ (zero at the last index):\n  - $n = 4$\n  - $a = [\\,8,\\,16,\\,24,\\,32\\,]$\n  - $b = [\\,2,\\,2,\\,2,\\,0\\,]$\n\n- Test case $5$ (signed zero):\n  - $n = 4$\n  - $a = [\\,1.0,\\,-2.0,\\,3.5,\\,-4.5\\,]$\n  - $b = [\\,1.0,\\,-0.0,\\,2.0,\\,2.0\\,]$\n\n- Test case $6$ (zero crossing vector chunk boundary):\n  - $n = 9$\n  - $a = [\\,1,\\,2,\\,3,\\,4,\\,5,\\,6,\\,7,\\,8,\\,9\\,]$\n  - $b = [\\,1,\\,1,\\,1,\\,1,\\,1,\\,0,\\,1,\\,1,\\,1\\,]$\n\nFinal output format specification:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a bracketed list of three integers $[x_1,x_2,x_3]$. For example, a valid output with two test cases would look like $[[0,0,0],[1,0,0]]$. Your program must produce exactly one line in this format and must not read any input.", "solution": "The problem requires the design and implementation of a program to analyze how SIMD (Single Instruction, Multiple Data) vectorization by a compiler can alter the observable behavior of a loop, specifically in the presence of division-by-zero exceptions. The analysis is performed by comparing the output of a well-defined sequential (scalar) loop against the output of a naive vectorized loop and two safeguarded vectorized loops.\n\nThe core principle at issue is the compiler's adherence to the `as-if` rule, which allows any transformation that does not change the observable behavior of the program as defined by the language's abstract machine. In sequential languages like C, program execution and its side effects (memory writes, exceptions) are defined by program order. A loop like `for (i=0; in; ++i) c[i] = a[i] / b[i];` is specified to execute for `i=0`, then `i=1`, and so on. If `b[k]` is zero, an exception is raised at iteration `k`, and no subsequent iterations are executed.\n\nSIMD vectorization breaks this strict sequential execution. A SIMD instruction operates on a vector of data elements simultaneously. For the given loop, a compiler might generate code that computes four divisions (`a[i]/b[i]`, `a[i+1]/b[i+1]`, ..., `a[i+3]/b[i+3]`) in a single instruction. This is a form of speculative execution: all four divisions are performed even if a scalar execution would have terminated at, say, `b[i+1] == 0`. According to the IEEE 754 floating-point standard, division by zero does not terminate the program but produces `+Infinity` or `-Infinity`. Consequently, the vectorized loop may write values to `c` for indices where the scalar loop would have written nothing, thus changing observable behavior.\n\nThis program models and detects such behavioral changes. The logic is designed around four distinct execution semantics and a comparison mechanism.\n\n1.  **Semantic Models**:\n    *   **Scalar Semantics**: This serves as the ground truth, perfectly emulating the C language's sequential execution model. A loop iterates from index $i=0$ to $n-1$. At each index, it checks if the divisor $b[i]$ is zero (which includes both $+0.0$ and $-0.0$). If it is, the loop terminates immediately, and no write to $c[i]$ or any subsequent elements occurs. Otherwise, it computes $c[i] = a[i] / b[i]$ and proceeds.\n    *   **Naive SIMD Semantics**: This model simulates a simple, aggressive vectorization strategy with a fixed vector width $w=4$. The loop is processed in chunks of size $w$. Within each chunk, all $w$ division operations are performed in parallel, irrespective of whether any divisors in that chunk are zero. The results, including any `Infinity` values from division by zero, are written to the corresponding elements of the output array $c$. This models how speculative execution across SIMD lanes can bypass the short-circuiting behavior of the scalar loop.\n    *   **Guard $G_1$ (Pre-scan and Fallback)**: This models a conservative safeguard. Before attempting vectorization, the entire divisor array $b$ is scanned for zeros. If any zero is found, vectorization is deemed unsafe, and the program falls back to the safe Scalar Semantics. If no zeros are found, the behavior of the vectorized loop is guaranteed to be identical to the scalar loop, so it proceeds with the Naive SIMD Semantics for maximum performance. This approach ensures correctness at the cost of a pre-scan and potentially forgoing vectorization.\n    *   **Guard $G_2$ (Masked Prefix)**: This models a more sophisticated safeguard that mimics modern masked vector instructions. It first finds the index $s$ of the first zero in the divisor array $b$. It then performs vectorized computation only on the \"safe prefix\" of the arrays, i.e., for all indices $i  s$. No writes are performed for any index $i \\ge s$. This preserves the exact termination behavior of the scalar loop while still benefiting from vectorization on the initial part of the computation.\n\n2.  **Behavioral Change Detection**:\n    To rigorously compare the outcomes of these semantics, the output array $c$ for each simulation is first initialized with Not-a-Number ($NaN$) values. A $NaN$ value at an index $i$ after a simulation signifies that $c[i]$ was never written. A behavioral change between two models is detected if, for any index $k$, either:\n    *   The \"write status\" differs: one model wrote a value to $c[k]$ while the other left it as $NaN$.\n    *   Both models wrote to $c[k]$, but the values are different.\n    This comparison is implemented by a function that iterates through the result arrays and returns `1` (change detected) or `0` (no change).\n\nThe overall program structure involves iterating through a suite of predefined test cases. For each case, it simulates all four semantics, producing four corresponding result arrays for $c$. It then compares the results from the Naive SIMD, Guard $G_1$, and Guard $G_2$ models against the ground-truth Scalar model to generate a triplet of flags $[x_1, x_2, x_3]$, indicating whether each model preserved the original scalar behavior. By design, the safeguards $G_1$ and $G_2$ should always preserve behavior, so their corresponding flags $x_2$ and $x_3$ serve as a validation of the simulation's correctness. The flag $x_1$ reveals the conditions under which naive vectorization is unsafe.", "answer": "```c\n// The complete and compilable C program goes here.\n// Headers must adhere to the specified restrictions.\n#include stdio.h\n#include stdlib.h\n#include string.h\n#include math.h\n\n// A struct to hold the parameters for a single test case.\ntypedef struct {\n    int n;\n    const double* a;\n    const double* b;\n    const char* description;\n} TestCase;\n\n// Forward declarations\nvoid init_c_array(double* c, int n);\nvoid run_scalar(double* c, const double* a, const double* b, int n);\nvoid run_naive_simd(double* c, const double* a, const double* b, int n, int w);\nvoid run_guard_g1(double* c, const double* a, const double* b, int n, int w);\nvoid run_guard_g2(double* c, const double* a, const double* b, int n, int w);\nint compare_results(const double* c1, const double* c2, int n);\nint find_first_zero_index(const double* b, int n);\n\n\nvoid init_c_array(double* c, int n) {\n    for (int i = 0; i  n; ++i) {\n        c[i] = NAN;\n    }\n}\n\nvoid run_scalar(double* c, const double* a, const double* b, int n) {\n    for (int i = 0; i  n; ++i) {\n        if (b[i] == 0.0) {\n            break; // Stop at first division by zero\n        }\n        c[i] = a[i] / b[i];\n    }\n}\n\nvoid run_naive_simd(double* c, const double* a, const double* b, int n, int w) {\n    for (int i = 0; i  n; i += w) {\n        for (int j = 0; j  w; ++j) {\n            int idx = i + j;\n            if (idx  n) {\n                // All operations in the vector are performed\n                c[idx] = a[idx] / b[idx];\n            }\n        }\n    }\n}\n\nint find_first_zero_index(const double* b, int n) {\n    for (int i = 0; i  n; ++i) {\n        if (b[i] == 0.0) {\n            return i;\n        }\n    }\n    return n; // Return n if no zero is found\n}\n\nvoid run_guard_g1(double* c, const double* a, const double* b, int n, int w) {\n    int has_zero = 0;\n    for (int i = 0; i  n; ++i) {\n        if (b[i] == 0.0) {\n            has_zero = 1;\n            break;\n        }\n    }\n\n    if (has_zero) {\n        run_scalar(c, a, b, n);\n    } else {\n        run_naive_simd(c, a, b, n, w);\n    }\n}\n\nvoid run_guard_g2(double* c, const double* a, const double* b, int n, int w) {\n    int first_zero_idx = find_first_zero_index(b, n);\n    // Perform vectorized computation only up to the first zero.\n    // We achieve this by running the naive SIMD model on a problem of size `first_zero_idx`.\n    run_naive_simd(c, a, b, first_zero_idx, w);\n}\n\nint compare_results(const double* c1, const double* c2, int n) {\n    for (int i = 0; i  n; ++i) {\n        int c1_is_nan = isnan(c1[i]);\n        int c2_is_nan = isnan(c2[i]);\n\n        // If one is NaN and the other is not, they differ (different write status)\n        if (c1_is_nan != c2_is_nan) {\n            return 1;\n        }\n        \n        // If both are not NaN, compare their values\n        if (!c1_is_nan  c1[i] != c2[i]) {\n            return 1;\n        }\n    }\n    return 0; // Results are identical\n}\n\nint main(void) {\n    // Define the test case data.\n    const double a1[] = {1, 2, 3, 4, 5, 6};\n    const double b1[] = {1, 1, 1, 1, 1, 1};\n\n    const double a2[] = {10, 20, 30, 40, 50, 60};\n    const double b2[] = {2, 0, 5, 10, 0, 3};\n\n    const double a3[] = {7, 8, 9, 10};\n    const double b3[] = {0, 2, 3, 4};\n\n    const double a4[] = {8, 16, 24, 32};\n    const double b4[] = {2, 2, 2, 0};\n\n    const double a5[] = {1.0, -2.0, 3.5, -4.5};\n    const double b5[] = {1.0, -0.0, 2.0, 2.0};\n\n    const double a6[] = {1, 2, 3, 4, 5, 6, 7, 8, 9};\n    const double b6[] = {1, 1, 1, 1, 1, 0, 1, 1, 1};\n\n    TestCase test_cases[] = {\n        {6, a1, b1, \"Happy path, no zeros\"},\n        {6, a2, b2, \"Multiple zeros, early zero\"},\n        {4, a3, b3, \"Zero at the first index\"},\n        {4, a4, b4, \"Zero at the last index\"},\n        {4, a5, b5, \"Signed zero\"},\n        {9, a6, b6, \"Zero crossing vector chunk boundary\"}\n    };\n    \n    int num_cases = sizeof(test_cases) / sizeof(test_cases[0]);\n    int results[num_cases][3];\n    const int w = 4;\n\n    for (int i = 0; i  num_cases; ++i) {\n        TestCase tc = test_cases[i];\n        int n = tc.n;\n\n        // Allocate result arrays for the current test case\n        double* c_scalar = malloc(n * sizeof(double));\n        double* c_naive = malloc(n * sizeof(double));\n        double* c_g1 = malloc(n * sizeof(double));\n        double* c_g2 = malloc(n * sizeof(double));\n        if (!c_scalar || !c_naive || !c_g1 || !c_g2) {\n            fprintf(stderr, \"Memory allocation failed\\n\");\n            return EXIT_FAILURE;\n        }\n\n        // Initialize all to NaN\n        init_c_array(c_scalar, n);\n        init_c_array(c_naive, n);\n        init_c_array(c_g1, n);\n        init_c_array(c_g2, n);\n        \n        // Run simulations\n        run_scalar(c_scalar, tc.a, tc.b, n);\n        run_naive_simd(c_naive, tc.a, tc.b, n, w);\n        run_guard_g1(c_g1, tc.a, tc.b, n, w);\n        run_guard_g2(c_g2, tc.a, tc.b, n, w);\n\n        // Compare results and store\n        results[i][0] = compare_results(c_scalar, c_naive, n); // Naive vs Scalar\n        results[i][1] = compare_results(c_scalar, c_g1, n);    // G1 vs Scalar\n        results[i][2] = compare_results(c_scalar, c_g2, n);    // G2 vs Scalar\n\n        free(c_scalar);\n        free(c_naive);\n        free(c_g1);\n        free(c_g2);\n    }\n    \n    // Print the results in the EXACT REQUIRED format.\n    printf(\"[\");\n    for (int i = 0; i  num_cases; ++i) {\n        printf(\"[%d,%d,%d]\", results[i][0], results[i][1], results[i][2]);\n        if (i  num_cases - 1) {\n            printf(\",\");\n        }\n    }\n    printf(\"]\");\n\n    return EXIT_SUCCESS;\n}\n```", "id": "3670137"}, {"introduction": "Not all loops are trivially vectorizable; many common algorithms, like computing a histogram, involve data dependencies that create write conflicts. This final practice tackles this challenge by introducing privatization, a powerful pattern for parallelizing reduction operations. You will implement a SIMD-style histogram computation that uses per-lane private counters to eliminate memory contention and the need for costly atomic operations, demonstrating how a change in data structure design can unlock significant parallel performance [@problem_id:3670122].", "problem": "You are given a sequence of integer observations and must compute a histogram with $k$ bins using Single Instruction Multiple Data (SIMD) principles. The goal is to transform a logically scalar histogram update into an approach that removes the need for atomic increments by privatizing per-lane bins and then merging, and to analyze contention that would arise if a single shared histogram were updated in parallel.\n\nFundamental base and definitions to use:\n- A SIMD vector of width $L$ executes the same instruction across $L$ independent lanes. In scalar code, a histogram increment to a shared array requires a read-modify-write on a single memory location. In a parallel SIMD interpretation, each of the $L$ lanes would attempt such an increment concurrently, which would necessitate atomic operations for correctness if the increments target the same bin.\n- A histogram with $k$ bins is defined by a mapping $b(x)$ from each observation $x$ to a bin index in $\\{0, 1, \\dots, k-1\\}$. In this problem, $b(x)$ is the identity mapping $b(x) = x$ assuming $x \\in \\{0, 1, \\dots, k-1\\}$.\n- A lane-privatized design allocates $k$ counters per lane and performs increments privately per lane with no atomic operations during the main pass, followed by a merge step that reduces the $L$ private histograms into a single global histogram via additions. In a single-threaded program, this merge requires no atomic operations.\n\nContention analysis to perform:\n- Let $p_i$ denote the probability that an observation maps to bin $i$, with $i \\in \\{0, \\dots, k-1\\}$. For a SIMD vector group of size $s$ (where $s \\le L$ for the last group), draw $s$ independent bin assignments according to $\\{p_i\\}$. Define a contention measure as the number of unordered lane pairs that map to the same bin within that group. Using linearity of expectation and the independence assumption, the expected number of such conflicting pairs in one group is\n$$\n\\mathbb{E}[\\text{pairs in one group of size } s] \\;=\\; \\binom{s}{2} \\sum_{i=0}^{k-1} p_i^2.\n$$\nFor a dataset of length $N$ processed in consecutive groups of size $L$ (with a final group of size $s_{\\text{last}}  L$ if $L \\nmid N$), the expected total number of conflicting pairs is the sum of this expression over all groups.\n\nProgramming tasks:\n- Implement two histogram computations on the same input:\n  1. A naive shared histogram update that logically requires atomic increments (you will simulate it sequentially in a single thread to obtain the final counts).\n  2. A lane-privatized SIMD-style histogram with $L$ lanes: maintain $k$ counters per lane, increment them privately per lane across the dataset in groups of $L$, then merge the $L$ private histograms into a single $k$-bin histogram by summing per bin. In a single-threaded program, this merge uses only ordinary additions.\n- Verify that the two resulting histograms are identical.\n- Compute:\n  1. The observed contention pairs: for each group of size $s$, count the duplicates per bin within the group, sum $\\binom{m}{2}$ over bins where $m$ is the per-bin count in that group, and accumulate across all groups.\n  2. The expected contention pairs using the empirical bin probabilities $p_i = \\frac{h_i}{N}$, where $h_i$ is the naive histogram count for bin $i$ and $N$ is the dataset length. For each group of size $s$, add $\\binom{s}{2} \\sum_i p_i^2$ to the total.\n  3. The total number of atomic increments that would be required by the naive approach during processing, which equals $N$.\n  4. The number of atomic increments required during processing by the lane-privatized approach, which equals $0$ in a single-threaded program.\n  5. The number of scalar additions performed during the merge step if you implement the merge by copying lane $0$ and then adding lanes $1$ through $L-1$ per bin, which equals $k \\cdot (L - 1)$.\n\nTest suite to exercise different behaviors:\n- Case A (happy path with minimal observed contention due to structured data):\n  - $k = 16$, $L = 8$, $N = 64$.\n  - Data: $d_j = j \\bmod 16$ for $j = 0, 1, \\dots, 63$.\n- Case B (skewed distribution causing high contention):\n  - $k = 4$, $L = 8$, $N = 40$.\n  - Data: $d_j = 0$ if $(j \\bmod 5) \\neq 0$, otherwise $d_j = j \\bmod 4$, for $j = 0, 1, \\dots, 39$.\n- Case C (degenerate distribution where all observations target the same bin):\n  - $k = 1$, $L = 8$, $N = 32$.\n  - Data: $d_j = 0$ for all $j$.\n- Case D (boundary condition with scalar width):\n  - $k = 10$, $L = 1$, $N = 37$.\n  - Data: $d_j = j \\bmod 10$ for $j = 0, 1, \\dots, 36$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each case in the order A, B, C, D, output the six values:\n  1. A boolean integer $1$ if the naive and privatized histograms are exactly equal, else $0$.\n  2. The observed contention pairs as an integer.\n  3. The expected contention pairs as a floating-point number rounded to six decimal places.\n  4. The total number of atomic increments during naive processing, which equals $N$ as an integer.\n  5. The number of atomic increments during privatized processing, which equals $0$ as an integer.\n  6. The number of scalar additions during the merge, which equals $k \\cdot (L - 1)$ as an integer.\n- Thus, the single line contains $24$ comma-separated values enclosed in square brackets, in the order: $\\text{eq}_A,\\text{obsPairs}_A,\\text{expPairs}_A,\\text{naiveAtomics}_A,\\text{privAtomics}_A,\\text{mergeAdds}_A,\\dots,\\text{eq}_D,\\text{obsPairs}_D,\\text{expPairs}_D,\\text{naiveAtomics}_D,\\text{privAtomics}_D,\\text{mergeAdds}_D$.", "solution": "The posed problem requires an analysis and comparison of two histogramming algorithms—a naive, shared-memory approach and a lane-privatized approach emulating Single Instruction, Multiple Data (SIMD) execution—along with a quantitative analysis of memory access contention. The problem is well-defined, scientifically grounded in established computer science principles, and all necessary parameters and formulas for the required computations are provided. We will proceed with a full solution.\n\nThe core task is to compute a histogram with $k$ bins from a dataset of $N$ integer observations. The bin index for an observation $x$ is given by the identity mapping $b(x) = x$.\n\n**1. Naive (Shared) Histogram Implementation**\n\nThis approach utilizes a single shared array, `hist`, of size $k$ to store the bin counts. The algorithm proceeds by iterating through the $N$ observations one by one. For each observation $d_j$, the count of the corresponding bin is incremented:\n$$ \\text{hist}[b(d_j)] \\leftarrow \\text{hist}[b(d_j)] + 1 $$\nIn a true parallel execution environment, where multiple threads or SIMD lanes could process different data elements concurrently, this read-modify-write operation on a shared memory location introduces a race condition. If two lanes simultaneously read the same counter value, both compute a new value based on the old one, and one of the updates is lost when they both write back. To ensure correctness, each increment must be performed atomically. Therefore, for a dataset of size $N$, the naive parallel approach would require $N$ atomic increments. In this single-threaded simulation, we perform standard increments but count the logical number of required atomic operations as $N$.\n\n**2. Lane-Privatized SIMD-Style Implementation**\n\nThis method is designed to eliminate the need for atomic operations during the main processing phase. The key idea is privatization. Instead of one shared histogram, we allocate $L$ private histograms, one for each of the $L$ SIMD lanes. This structure can be represented as a two-dimensional array, `private_hist`, of size $L \\times k$.\n\nThe algorithm operates in two stages:\n\n**Stage 1: Parallel Processing (Simulated)**\nThe dataset is processed in chunks of size $L$. For each chunk, lane $l$ (where $l \\in \\{0, 1, \\dots, L-1\\}$) is responsible for a single observation. It updates its own private histogram without interfering with any other lane.\n$$ \\text{private\\_hist}[l][b(d_{j+l})] \\leftarrow \\text{private\\_hist}[l][b(d_{j+l})] + 1 $$\nSince each lane writes only to its private memory, there are no data races, and no atomic operations are required. The number of atomic increments during this processing phase is $0$. If $N$ is not a multiple of $L$, the final group will be smaller, containing $N \\pmod L$ elements.\n\n**Stage 2: Merge**\nAfter all observations have been processed, the $L$ private histograms must be combined into a single final histogram. This is a reduction operation. A straightforward method is to initialize the final histogram by copying the contents of the first lane's histogram and then serially adding the contents of the remaining $L-1$ lanes. For each of the $L-1$ lanes, we perform $k$ scalar additions. Therefore, the total number of scalar additions in the merge step is:\n$$ \\text{Merge Additions} = k \\cdot (L-1) $$\nIf $L=1$, this value is correctly $0$, as no merge is needed. This final histogram must be identical to the one produced by the naive method.\n\n**3. Contention Analysis**\n\nContention occurs in the naive model when multiple lanes in a SIMD group attempt to update the same bin simultaneously. We analyze this contention both empirically and theoretically.\n\n**Observed Contention Pairs**\nWe iterate through the dataset in groups of size $s$ ($s=L$ for full groups, $s  L$ for the last group). For each group, we count the number of observations, $m_i$, that map to each bin $i$. The number of conflicting pairs for a single bin $i$ within that group is the number of ways to choose two observations that map to it, which is $\\binom{m_i}{2} = \\frac{m_i(m_i-1)}{2}$. The total observed contention for the group is the sum over all bins, $\\sum_{i=0}^{k-1} \\binom{m_i}{2}$. The overall total is the sum of these values across all groups.\n\n**Expected Contention Pairs**\nThe expected number of conflicting pairs is calculated based on the empirical probability distribution of bin assignments. First, the naive histogram counts, $h_i$, are computed. The empirical probability of an observation falling into bin $i$ is $p_i = h_i / N$. The term $\\sum_{i=0}^{k-1} p_i^2$ represents the probability that two randomly chosen, independent observations fall into the same bin.\nFor a group of $s$ independent observations, there are $\\binom{s}{2}$ pairs of observations. By linearity of expectation, the expected number of conflicting pairs in this group is:\n$$ \\mathbb{E}[\\text{pairs in group of size } s] = \\binom{s}{2} \\sum_{i=0}^{k-1} p_i^2 $$\nThe total expected contention is the sum of this quantity over all groups in the dataset.\n\nThe implementation will now proceed to calculate these metrics for the four specified test cases.", "answer": "```c\n// The complete and compilable C program goes here.\n// Headers must adhere to the specified restrictions.\n#include stdio.h\n#include stdlib.h\n#include string.h\n#include math.h\n\n// A struct to hold the parameters for a single test case.\ntypedef struct {\n    int k;\n    int L;\n    int N;\n    int *data;\n} TestCase;\n\n// Function to generate data for the test cases.\nvoid generate_data(TestCase *tc) {\n    tc-data = (int *)malloc(tc-N * sizeof(int));\n    if (!tc-data) {\n        perror(\"Failed to allocate memory for data\");\n        exit(EXIT_FAILURE);\n    }\n\n    if (tc-k == 16  tc-L == 8  tc-N == 64) { // Case A\n        for (int j = 0; j  tc-N; ++j) {\n            tc-data[j] = j % 16;\n        }\n    } else if (tc-k == 4  tc-L == 8  tc-N == 40) { // Case B\n        for (int j = 0; j  tc-N; ++j) {\n            if ((j % 5) != 0) {\n                tc-data[j] = 0;\n            } else {\n                tc-data[j] = j % 4;\n            }\n        }\n    } else if (tc-k == 1  tc-L == 8  tc-N == 32) { // Case C\n        for (int j = 0; j  tc-N; ++j) {\n            tc-data[j] = 0;\n        }\n    } else if (tc-k == 10  tc-L == 1  tc-N == 37) { // Case D\n        for (int j = 0; j  tc-N; ++j) {\n            tc-data[j] = j % 10;\n        }\n    }\n}\n\n// Helper to compute combinations C(n, 2)\nlong long combinations_2(int n) {\n    if (n  2) {\n        return 0;\n    }\n    return (long long)n * (n - 1) / 2;\n}\n\nint main(void) {\n    // Define the test cases from the problem statement.\n    TestCase test_cases[] = {\n        {16, 8, 64, NULL}, // Case A\n        {4, 8, 40, NULL},  // Case B\n        {1, 8, 32, NULL},  // Case C\n        {10, 1, 37, NULL}  // Case D\n    };\n\n    int num_cases = sizeof(test_cases) / sizeof(test_cases[0]);\n    \n    // Allocate and generate data for all test cases.\n    for (int i = 0; i  num_cases; ++i) {\n        generate_data(test_cases[i]);\n    }\n\n    // Array to store all 24 final results.\n    double results[24]; \n\n    // Calculate the result for each test case.\n    for (int i = 0; i  num_cases; ++i) {\n        int k = test_cases[i].k;\n        int L = test_cases[i].L;\n        int N = test_cases[i].N;\n        int *data = test_cases[i].data;\n\n        // --- 1. Naive Histogram ---\n        long *naive_hist = (long *)calloc(k, sizeof(long));\n        if (!naive_hist) { perror(\"malloc failed\"); return EXIT_FAILURE; }\n        for (int j = 0; j  N; ++j) {\n            naive_hist[data[j]]++;\n        }\n\n        // --- 2. Privatized Histogram ---\n        long *private_hist = (long *)calloc((size_t)L * k, sizeof(long));\n        if (!private_hist) { perror(\"malloc failed\"); free(naive_hist); return EXIT_FAILURE; }\n\n        for (int j = 0; j  N; j += L) {\n            int group_size = (j + L  N) ? (N - j) : L;\n            for (int l = 0; l  group_size; ++l) {\n                private_hist[(size_t)l * k + data[j + l]]++;\n            }\n        }\n        \n        long *merged_hist = (long *)calloc(k, sizeof(long));\n        if (!merged_hist) { perror(\"malloc failed\"); free(naive_hist); free(private_hist); return EXIT_FAILURE; }\n\n        if (L  0) {\n            memcpy(merged_hist, private_hist, k * sizeof(long));\n        }\n        for (int l = 1; l  L; ++l) {\n            for (int bin = 0; bin  k; ++bin) {\n                merged_hist[bin] += private_hist[(size_t)l * k + bin];\n            }\n        }\n\n        // --- 3. Verification ---\n        int are_equal = (memcmp(naive_hist, merged_hist, k * sizeof(long)) == 0);\n        \n        // --- 4. Contention and Metrics Calculation ---\n        // Observed contention pairs\n        long long observed_pairs = 0;\n        int *group_bin_counts = (int *)calloc(k, sizeof(int));\n        if (!group_bin_counts) { perror(\"malloc failed\"); free(naive_hist); free(private_hist); free(merged_hist); return EXIT_FAILURE; }\n        for (int j = 0; j  N; j += L) {\n            int group_size = (j + L  N) ? (N - j) : L;\n            memset(group_bin_counts, 0, k * sizeof(int));\n            for (int l = 0; l  group_size; ++l) {\n                group_bin_counts[data[j + l]]++;\n            }\n            long long group_pairs = 0;\n            for (int bin = 0; bin  k; ++bin) {\n                group_pairs += combinations_2(group_bin_counts[bin]);\n            }\n            observed_pairs += group_pairs;\n        }\n        free(group_bin_counts);\n\n        // Expected contention pairs\n        double sum_p_sq = 0.0;\n        for (int bin = 0; bin  k; ++bin) {\n            double p_i = (double)naive_hist[bin] / N;\n            sum_p_sq += p_i * p_i;\n        }\n\n        double expected_pairs = 0.0;\n        for (int j = 0; j  N; j += L) {\n            int group_size = (j + L  N) ? (N - j) : L;\n            expected_pairs += (double)combinations_2(group_size) * sum_p_sq;\n        }\n\n        // Other metrics\n        int naive_atomics = N;\n        int priv_atomics = 0;\n        int merge_adds = k * (L  1 ? L - 1 : 0);\n        \n        // Store results for this case\n        results[i * 6 + 0] = (double)are_equal;\n        results[i * 6 + 1] = (double)observed_pairs;\n        results[i * 6 + 2] = expected_pairs;\n        results[i * 6 + 3] = (double)naive_atomics;\n        results[i * 6 + 4] = (double)priv_atomics;\n        results[i * 6 + 5] = (double)merge_adds;\n\n        // Free memory for this case\n        free(naive_hist);\n        free(private_hist);\n        free(merged_hist);\n    }\n    \n    // Print the results in the EXACT REQUIRED format before the final return statement\n    printf(\"[\");\n    for (int i = 0; i  num_cases * 6; ++i) {\n        if (i % 6 == 0 || i % 6 == 1 || i % 6 = 3) {\n            printf(\"%d\", (int)results[i]);\n        } else { // expected_pairs\n            printf(\"%.6f\", results[i]);\n        }\n        if (i  num_cases * 6 - 1) {\n            printf(\",\");\n        }\n    }\n    printf(\"]\");\n    \n    // Free the generated data arrays\n    for (int i = 0; i  num_cases; ++i) {\n        free(test_cases[i].data);\n    }\n\n    return EXIT_SUCCESS;\n}\n```", "id": "3670122"}]}