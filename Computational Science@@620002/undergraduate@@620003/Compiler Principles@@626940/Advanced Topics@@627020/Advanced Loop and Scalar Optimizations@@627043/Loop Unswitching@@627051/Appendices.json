{"hands_on_practices": [{"introduction": "Loop unswitching is not merely about eliminating branch instructions; its true power often lies in creating specialized code paths that are more amenable to further optimization. This first practice delves into a fundamental consequence: the interaction with the memory hierarchy. By analyzing a matrix computation, you will quantify how unswitching can isolate memory access patterns with vastly different cache performance, a critical factor in the efficiency of high-performance code [@problem_id:3654402]. This exercise will give you a concrete understanding of how a high-level code transformation directly impacts low-level hardware behavior.", "problem": "Consider square matrices $A$, $B$, and $C$ of dimension $n \\times n$, stored in contiguous row-major layout in main memory, with each element occupying $w$ bytes. The computation to be performed is the matrix product accumulation defined by the triply nested loop order $(i,k,j)$: for each fixed $i$, for each $k$, read a scalar from $A$ according to a loop-invariant flag $t$ indicating whether $A$ was pre-transposed, and then update all entries in the $i$-th row of $C$ using the entire $k$-th row of $B$. Concretely, for each pair $(i,k)$, the code reads either $A[i][k]$ when $t$ is false (not transposed) or $A[k][i]$ when $t$ is true (transposed), binds this value to a scalar that is then reused across the inner loop over $j$, and accumulates $C[i][j] \\leftarrow C[i][j] + a \\cdot B[k][j]$ for all $j$ in $\\{0,1,\\dots,n-1\\}$.\n\nThe branch on $t$ is loop-invariant with respect to the $i$, $k$, and $j$ loops. Apply the transformation known as loop unswitching to produce two specialized versions of the computation: one for the $t=\\text{false}$ case and one for the $t=\\text{true}$ case. Use the following cache model and assumptions to estimate the cache behavior attributable solely to reads of $A$:\n\n- The processor has a Level-1 (L1) cache that is fully associative with Least Recently Used (LRU) replacement, and a cache line size of $L$ bytes.\n- Let $p = \\frac{L}{w}$ denote the number of matrix elements that fit in one cache line; assume $p$ is a positive integer and that $n$ is an exact multiple of $p$.\n- Each $A$ element read for a given $(i,k)$ is reused across the entire inner loop over $j$ without additional $A$ loads, but reuse of $A$ lines across different $(i,k)$ pairs is negligible due to streaming accesses in $B$ and updates to $C$. Therefore, count one cache line fill the first time any element from a line of $A$ is accessed within a given outer-loop iteration $i$, and do not count further cross-iteration reuse.\n- Do not include any cache behavior from $B$ or $C$; only count cache line fills caused by accesses to $A$.\n\nStarting from definitions of row-major addressing and basic properties of loop unswitching, derive the total number of L1 cache line fills caused by reads from $A$ across the entire computation in each specialized version ($t=\\text{false}$ and $t=\\text{true}$), and then provide a single closed-form analytical expression for the difference between these totals, defined as \n$$\\Delta = \\text{fills}_{t=\\text{true}} - \\text{fills}_{t=\\text{false}}.$$\n\nExpress your final answer in terms of $n$ and $p$. No numerical evaluation is required; provide the exact analytic expression.", "solution": "The problem as stated is valid. It is scientifically grounded in the principles of computer architecture and compiler optimization, specifically cache behavior analysis and loop transformations. The problem is well-posed, with all necessary parameters ($n$, $p$), a clear computational model, and a defined objective. The language is objective and the assumptions, while simplifying, are explicit and consistent.\n\nThe core task is to analyze the number of cache line fills for accesses to a matrix $A$ in two different scenarios resulting from loop unswitching. Loop unswitching is a compiler optimization that moves a loop-invariant conditional branch outside of a loop, duplicating the loop body inside each path of the branch.\n\nThe original loop structure is:\n`for i = 0 to n-1:`\n  `for k = 0 to n-1:`\n    `if t:`\n      `a = A[k][i]`\n    `else:`\n      `a = A[i][k]`\n    `for j = 0 to n-1:`\n      `C[i][j] += a * B[k][j]`\n\nApplying loop unswitching on the loop-invariant flag $t$ yields two separate code blocks:\n\nCase $1$: $t=\\text{false}$\n`for i = 0 to n-1:`\n  `for k = 0 to n-1:`\n    `a = A[i][k]`\n    `for j = 0 to n-1:`\n      `C[i][j] += a * B[k][j]`\n\nCase $2$: $t=\\text{true}$\n`for i = 0 to n-1:`\n  `for k = 0 to n-1:`\n    `a = A[k][i]`\n    `for j = 0 to n-1:`\n      `C[i][j] += a * B[k][j]`\n\nWe will now analyze the number of cache line fills for reads from matrix $A$ in each case, based on the provided cache model. The matrices are stored in a contiguous row-major layout. The memory address of an element $A[r][c]$ can be expressed as $\\text{Addr}(A[r][c]) = \\text{Addr}(A[0][0]) + (r \\cdot n + c) \\cdot w$, where $w$ is the size of each element in bytes.\n\nA cache line has a size of $L$ bytes, and contains $p = \\frac{L}{w}$ elements. The problem specifies that we must count cache fills based on the rule: \"count one cache line fill the first time any element from a line of A is accessed within a given outer-loop iteration $i$, and do not count further cross-iteration reuse.\" This implies that our analysis for each iteration of the outer loop over $i$ is independent, and we sum the fills from each such iteration.\n\n**Analysis for $t=\\text{false}$ (Not Transposed Access)**\n\nIn this case, the access pattern for matrix $A$ is $A[i][k]$. The loops are ordered with $i$ as the outer loop and $k$ as the inner loop for the access to $A$.\nFor a fixed outer loop index $i$, the inner loop `for k = 0 to n-1` accesses the elements $A[i][0], A[i][1], \\dots, A[i][n-1]$.\nThis sequence of accesses corresponds to a sequential scan of the $i$-th row of matrix $A$. Since the matrix is in row-major layout, these elements are contiguous in memory.\nWhen $A[i][0]$ is accessed, a cache miss occurs, and the cache line containing the first $p$ elements of that row ($A[i][0]$ through $A[i][p-1]$) is loaded. This accounts for $1$ cache fill.\nThe subsequent $p-1$ accesses, to $A[i][1], \\dots, A[i][p-1]$, will be cache hits.\nThe next access, to $A[i][p]$, will cause another cache miss, loading the second cache line of the row. This continues across the entire row.\nSince a row contains $n$ elements and each cache line holds $p$ elements, and $n$ is an exact multiple of $p$, the number of cache lines that comprise a single row is exactly $\\frac{n}{p}$.\nTherefore, for each iteration of the outer loop (for each fixed $i$), accessing one full row of $A$ results in $\\frac{n}{p}$ cache fills.\n\nAccording to the specified counting rule, we sum these fills over all iterations of the outer loop, from $i=0$ to $i=n-1$.\nThe total number of fills is:\n$$ \\text{fills}_{t=\\text{false}} = \\sum_{i=0}^{n-1} \\frac{n}{p} = n \\cdot \\frac{n}{p} = \\frac{n^2}{p} $$\n\n**Analysis for $t=\\text{true}$ (Transposed Access)**\n\nIn this case, the access pattern for matrix $A$ is $A[k][i]$.\nFor a fixed outer loop index $i$, the inner loop `for k = 0 to n-1` accesses the elements $A[0][i], A[1][i], \\dots, A[n-1][i]$.\nThis sequence of accesses corresponds to a scan of the $i$-th column of matrix $A$.\nLet's examine the memory locations of two consecutive accesses in this sequence: $A[k][i]$ and $A[k+1][i]$.\n$\\text{Addr}(A[k][i]) = \\text{Addr}(A[0][0]) + (k \\cdot n + i) \\cdot w$\n$\\text{Addr}(A[k+1][i]) = \\text{Addr}(A[0][0]) + ((k+1) \\cdot n + i) \\cdot w$\nThe difference in their addresses is:\n$$ \\Delta \\text{Addr} = ((k+1)n + i)w - (kn + i)w = nw $$\nThe stride between consecutive memory accesses is $n \\cdot w$ bytes.\nThe size of a cache line is $L = p \\cdot w$ bytes.\nThe problem states $n$ is a multiple of $p$, which implies $n \\ge p$. Therefore, the stride $n \\cdot w$ is greater than or equal to the cache line size $p \\cdot w$. This means that each element in the column access sequence $A[0][i], A[1][i], \\dots, A[n-1][i]$ resides in a different cache line.\n\nTo be more rigorous, let's analyze the cache line index for an element $A[k][i]$. Assuming the matrix base address is $0$ for simplicity, the cache line index for the address $(kn+i)w$ is $\\lfloor \\frac{(kn+i)w}{L} \\rfloor = \\lfloor \\frac{kn+i}{p} \\rfloor$.\nSince $n$ is a multiple of $p$, we can write $n = m \\cdot p$ for some integer $m \\ge 1$.\nThe line index for $A[k][i]$ is $\\lfloor \\frac{k(mp)+i}{p} \\rfloor = \\lfloor km + \\frac{i}{p} \\rfloor = km + \\lfloor \\frac{i}{p} \\rfloor$.\nFor a fixed $i$, as $k$ increments, the term $km$ ensures that each value of $k$ from $0$ to $n-1$ produces a unique cache line index. Specifically, the line index for $A[k_1][i]$ and $A[k_2][i]$ will be different if $k_1 \\ne k_2$.\nTherefore, each of the $n$ accesses within the inner loop over $k$ results in a cache miss.\nFor each iteration of the outer loop (for each fixed $i$), there are $n$ cache fills.\n\nSumming over all iterations of the outer loop from $i=0$ to $i=n-1$:\n$$ \\text{fills}_{t=\\text{true}} = \\sum_{i=0}^{n-1} n = n \\cdot n = n^2 $$\n\n**Calculating the Difference**\n\nThe problem asks for the difference $\\Delta = \\text{fills}_{t=\\text{true}} - \\text{fills}_{t=\\text{false}}$.\nSubstituting the derived expressions:\n$$ \\Delta = n^2 - \\frac{n^2}{p} $$\nFactoring out the $n^2$ term gives the final analytical expression:\n$$ \\Delta = n^2 \\left(1 - \\frac{1}{p}\\right) $$\nThis expression represents the increase in cache misses when switching from a spatially local access pattern (row-wise on a row-major matrix) to a non-local one (column-wise).", "answer": "$$ \\boxed{n^2 \\left(1 - \\frac{1}{p}\\right)} $$", "id": "3654402"}, {"introduction": "A key benefit of loop unswitching is its role as an enabling optimization, clearing the path for other powerful transformations that would otherwise be blocked by complex control flow. This exercise demonstrates how removing a loop-invariant branch unlocks the potential for SIMD (Single Instruction, Multiple Data) vectorization, a cornerstone of modern processor performance. By calculating the expected speedup in a hypothetical image processing kernel, you will see the dramatic performance gains achievable when the compiler can process multiple data elements in parallel [@problem_id:3654460].", "problem": "An image-processing loop iterates over an image and, for each pixel, selects a conversion routine based on a loop-invariant variable `colorSpace` that is set once before entering the loop. The two possible routines implement standardized conversions:\n- If `colorSpace` is the text string \"RGB\", the routine computes luminance $Y$ from red, green, and blue using $Y = 0.2126 R + 0.7152 G + 0.0722 B$.\n- If `colorSpace` is the text string \"YCbCr\", the routine uses the $Y$ component directly as luminance without further arithmetic.\n\nIn the baseline implementation, `colorSpace` is checked inside the loop body for every pixel, which inhibits vectorization and adds a fixed branch overhead per iteration. After applying loop unswitching, the branch is hoisted outside the loop by duplicating the loop: one specialized loop for \"RGB\" and one for \"YCbCr\". This transformation enables Single Instruction Multiple Data (SIMD) vectorization of each specialized loop.\n\nAssume the following, all consistent with a simple throughput cost model in cycles:\n- The baseline (pre-unswitch) scalar loop incurs a per-iteration branch overhead of $c_b = 1$ cycle and cannot be vectorized.\n- Scalar operation costs are $c_m = 1$ cycle per multiply, $c_a = 1$ cycle per add, $c_l = 1$ cycle per load, and $c_s = 1$ cycle per store.\n- SIMD vectorized operation costs per vector are $c^{\\text{vec}}_m = 1$ cycle per vector multiply, $c^{\\text{vec}}_a = 1$ cycle per vector add, $c^{\\text{vec}}_l = 1$ cycle per vector load, and $c^{\\text{vec}}_s = 1$ cycle per vector store.\n- The SIMD width is $w = 8$ pixels, and the image dimensions are $W = 2048$ and $H = 1024$, so the total number of pixels $N = W \\cdot H$ is divisible by $w$ and there is no remainder loop.\n- For the \"RGB\" routine per pixel: there are $n_m = 3$ multiplies and $n_a = 2$ adds, with $n_l = 3$ loads (for $R$, $G$, $B$) and $n_s = 1$ store (for $Y$).\n- For the \"YCbCr\" routine per pixel: there are $n_m = 0$ multiplies and $n_a = 0$ adds, with $n_l = 1$ load (for $Y$) and $n_s = 1$ store (for $Y$).\n- After loop unswitching, the branch is not executed inside the pixel loop; assume its one-time cost is negligible compared to $N$.\n- Across program runs, the probability that `colorSpace` equals the text string \"RGB\" is $p = 0.6$; otherwise it equals the text string \"YCbCr\" with probability $1 - p$.\n\nUsing only the definitions of loop invariance, loop unswitching, and a standard additive throughput model in cycles for loads, stores, multiplies, and adds, derive an analytic expression from first principles for the expected speedup $S$ due to loop unswitching and the resulting SIMD vectorization, defined as the ratio of the expected baseline cycles per pixel to the expected post-unswitch cycles per pixel. Evaluate $S$ numerically under the given parameters. Express the final answer as a dimensionless real number and round your answer to four significant figures.", "solution": "The problem requires the derivation of an analytic expression for the expected speedup $S$ resulting from loop unswitching and subsequent SIMD vectorization. The speedup is defined as the ratio of the expected processing time per pixel in the baseline implementation to that in the optimized implementation. We will derive this from first principles using the provided additive throughput cost model.\n\nLet $C_{\\text{baseline}}$ be the execution time in cycles per pixel for the baseline implementation, and $C_{\\text{optimized}}$ be the time for the optimized implementation. The speedup $S$ is the ratio of their expected values:\n$$S = \\frac{E[C_{\\text{baseline}}]}{E[C_{\\text{optimized}}]}$$\nThe expectation $E[\\cdot]$ is taken over the two possible code paths determined by the value of the `colorSpace` variable, with probabilities $p$ for \"RGB\" and $1-p$ for \"YCbCr\".\n\nFirst, we determine the expected cost per pixel for the baseline implementation, $E[C_{\\text{baseline}}]$. In this case, the loop contains a conditional branch executed for every pixel, which incurs a fixed overhead $c_b$ and prevents vectorization. The processing is scalar.\n\nFor the \"RGB\" path, the cost per pixel, $C_{\\text{baseline, RGB}}$, is the sum of the costs of the arithmetic and memory operations, plus the branch overhead. The number of operations are given as $n_m = 3$ multiplies, $n_a = 2$ adds, $n_l = 3$ loads, and $n_s = 1$ store. The scalar costs are $c_m = 1$, $c_a = 1$, $c_l = 1$, and $c_s = 1$. The branch overhead is $c_b=1$.\n$$C_{\\text{baseline, RGB}} = (n_m \\cdot c_m + n_a \\cdot c_a + n_l \\cdot c_l + n_s \\cdot c_s) + c_b$$\n$$C_{\\text{baseline, RGB}} = (3 \\cdot 1 + 2 \\cdot 1 + 3 \\cdot 1 + 1 \\cdot 1) + 1 = (3 + 2 + 3 + 1) + 1 = 9 + 1 = 10 \\text{ cycles/pixel}$$\n\nFor the \"YCbCr\" path, the number of operations are $n_m = 0$, $n_a = 0$, $n_l = 1$, and $n_s = 1$. The cost per pixel, $C_{\\text{baseline, YCbCr}}$, is:\n$$C_{\\text{baseline, YCbCr}} = (n_m \\cdot c_m + n_a \\cdot c_a + n_l \\cdot c_l + n_s \\cdot c_s) + c_b$$\n$$C_{\\text{baseline, YCbCr}} = (0 \\cdot 1 + 0 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1) + 1 = (0 + 0 + 1 + 1) + 1 = 2 + 1 = 3 \\text{ cycles/pixel}$$\n\nThe expected baseline cost per pixel is the weighted average of these two costs, using the probability $p = 0.6$:\n$$E[C_{\\text{baseline}}] = p \\cdot C_{\\text{baseline, RGB}} + (1-p) \\cdot C_{\\text{baseline, YCbCr}}$$\n$$E[C_{\\text{baseline}}] = 0.6 \\cdot 10 + (1-0.6) \\cdot 3 = 6.0 + 0.4 \\cdot 3 = 6.0 + 1.2 = 7.2 \\text{ cycles/pixel}$$\n\nNext, we determine the expected cost per pixel for the optimized implementation, $E[C_{\\text{optimized}}]$. After loop unswitching, the conditional branch is hoisted out of the loop. Each of the two resulting specialized loops can be vectorized using SIMD instructions with a width of $w=8$ pixels per operation. The branch overhead within the loop is eliminated.\n\nFor the \"RGB\" path, the vectorized loop processes $w$ pixels at a time. The cost per vector of $w$ pixels, $C_{\\text{vec, RGB}}$, is calculated using the vectorized operation costs, which are all given as $1$ cycle per vector operation ($c^{\\text{vec}}_m = c^{\\text{vec}}_a = c^{\\text{vec}}_l = c^{\\text{vec}}_s = 1$).\n$$C_{\\text{vec, RGB}} = n_m \\cdot c^{\\text{vec}}_m + n_a \\cdot c^{\\text{vec}}_a + n_l \\cdot c^{\\text{vec}}_l + n_s \\cdot c^{\\text{vec}}_s$$\n$$C_{\\text{vec, RGB}} = 3 \\cdot 1 + 2 \\cdot 1 + 3 \\cdot 1 + 1 \\cdot 1 = 3 + 2 + 3 + 1 = 9 \\text{ cycles/vector}$$\nThe cost per pixel, $C_{\\text{optimized, RGB}}$, is this cost divided by the SIMD width $w$:\n$$C_{\\text{optimized, RGB}} = \\frac{C_{\\text{vec, RGB}}}{w} = \\frac{9}{8} \\text{ cycles/pixel}$$\n\nFor the \"YCbCr\" path, the cost per vector of $w$ pixels, $C_{\\text{vec, YCbCr}}$, is:\n$$C_{\\text{vec, YCbCr}} = n_m \\cdot c^{\\text{vec}}_m + n_a \\cdot c^{\\text{vec}}_a + n_l \\cdot c^{\\text{vec}}_l + n_s \\cdot c^{\\text{vec}}_s$$\n$$C_{\\text{vec, YCbCr}} = 0 \\cdot 1 + 0 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1 = 0 + 0 + 1 + 1 = 2 \\text{ cycles/vector}$$\nThe cost per pixel, $C_{\\text{optimized, YCbCr}}$, is:\n$$C_{\\text{optimized, YCbCr}} = \\frac{C_{\\text{vec, YCbCr}}}{w} = \\frac{2}{8} = \\frac{1}{4} \\text{ cycles/pixel}$$\n\nThe expected optimized cost per pixel is the weighted average of these two costs:\n$$E[C_{\\text{optimized}}] = p \\cdot C_{\\text{optimized, RGB}} + (1-p) \\cdot C_{\\text{optimized, YCbCr}}$$\n$$E[C_{\\text{optimized}}] = 0.6 \\cdot \\frac{9}{8} + (1-0.6) \\cdot \\frac{2}{8} = \\frac{0.6 \\cdot 9 + 0.4 \\cdot 2}{8} = \\frac{5.4 + 0.8}{8} = \\frac{6.2}{8} = 0.775 \\text{ cycles/pixel}$$\n\nNow we can formulate the complete analytic expression for the speedup $S$:\n$$S = \\frac{p \\cdot C_{\\text{baseline, RGB}} + (1-p) \\cdot C_{\\text{baseline, YCbCr}}}{p \\cdot C_{\\text{optimized, RGB}} + (1-p) \\cdot C_{\\text{optimized, YCbCr}}}$$\n$$S = \\frac{p \\cdot ((n_{m}^{\\text{RGB}} c_m + \\dots) + c_b) + (1-p) \\cdot ((n_{m}^{\\text{YCbCr}} c_m + \\dots) + c_b)}{p \\cdot \\frac{1}{w}(n_{m}^{\\text{RGB}} c_m^{\\text{vec}} + \\dots) + (1-p) \\cdot \\frac{1}{w}(n_{m}^{\\text{YCbCr}} c_m^{\\text{vec}} + \\dots)}$$\nThis expression represents the general formula derived from first principles.\n\nFinally, we substitute the calculated expected costs to find the numerical value of $S$:\n$$S = \\frac{E[C_{\\text{baseline}}]}{E[C_{\\text{optimized}}]} = \\frac{7.2}{0.775}$$\n$$S = \\frac{7.2}{0.775} = \\frac{7200}{775} = \\frac{1440}{155} = \\frac{288}{31} \\approx 9.29032258...$$\nRounding to four significant figures, the speedup is $9.290$.", "answer": "$$\n\\boxed{9.290}\n$$", "id": "3654460"}, {"introduction": "In practice, compiler optimizations involve trade-offs. Loop unswitching, for example, improves performance by removing branches but increases code size, which can introduce new costs like instruction cache pressure and code fetch overhead. This final practice places you in the role of a compiler designer making a data-driven decision. Using a detailed cost model informed by profiling data, you will determine the precise break-even probability at which applying loop unswitching becomes profitable, illustrating how modern compilers use quantitative analysis to navigate complex optimization choices [@problem_id:3654414].", "problem": "Consider a tight inner loop of $N$ iterations in a numerical kernel. The loop body contains a loop-invariant Boolean condition whose true/false outcome is fixed for an entire invocation of the loop but varies across invocations. The baseline implementation tests the condition inside the loop on every iteration and executes either a heavier or lighter path accordingly. A compiler optimization, loop unswitching, clones the loop into two versions specialized for the true and false outcomes, moving the condition outside the loop so that no per-iteration conditional branch remains. Profiling provides a hot/cold probability $p$ that the condition is true for a given invocation.\n\nAssume the following cost model (all cycle costs are empirically measured on the target microarchitecture and are per invocation unless stated per iteration):\n- Baseline per-iteration common work cost: $C$ cycles.\n- Baseline per-iteration conditional branch overhead: $b$ cycles.\n- Heavy-path per-iteration extra work (only when the condition is true): $H$ cycles.\n- Light-path per-iteration extra work (only when the condition is false): $L$ cycles.\n- After loop unswitching, the conditional branch is removed, but there is an instruction cache (I-cache) pressure penalty of $s$ cycles per iteration in the specialized loops.\n- After loop unswitching, there is a one-time cloning overhead of $d$ cycles per invocation (e.g., due to additional code fetch and preheader duplication) that applies regardless of which specialized loop runs.\n- After loop unswitching, the heavy path is vectorized with speedup factor $v>1$, so its per-iteration extra work becomes $H/v$ cycles. The light path is unchanged.\n\nLet $N = 5 \\times 10^{5}$, $C = 2$, $b = 1.2$, $H = 12$, $L = 4$, $s = 0.3$, $d = 6.0 \\times 10^{5}$, and $v = 3$. Using the foundational definition of expected value for a Bernoulli outcome across invocations and a first-principles additive cost model, derive the break-even probability $p_{\\mathrm{crit}}$ such that for $p > p_{\\mathrm{crit}}$ loop unswitching yields a lower expected total cycles per invocation than the baseline. Compute $p_{\\mathrm{crit}}$ numerically and round your answer to four significant figures. Express your final answer as a unitless probability.", "solution": "The problem is found to be valid as it is scientifically grounded in compiler optimization principles, well-posed with a unique solution, and objective in its formulation. All necessary data and definitions are provided, and there are no contradictions.\n\nThe first step is to derive the expected total cycle cost for the baseline implementation, denoted as $E[T_{\\mathrm{base}}]$. The loop runs for $N$ iterations. In each iteration, there is a common work cost $C$ and a conditional branch overhead $b$. The condition is true with probability $p$, adding an extra work cost of $H$. The condition is false with probability $1-p$, adding an extra work cost of $L$. The total cost for a single invocation is therefore dependent on the outcome of the loop-invariant condition.\n\nIf the condition is true (probability $p$), the total cost is:\n$$T_{\\mathrm{base, true}} = N(C + b + H)$$\n\nIf the condition is false (probability $1-p$), the total cost is:\n$$T_{\\mathrm{base, false}} = N(C + b + L)$$\n\nThe expected total cost per invocation, $E[T_{\\mathrm{base}}]$, is the weighted average of these two outcomes:\n$$E[T_{\\mathrm{base}}] = p \\cdot T_{\\mathrm{base, true}} + (1-p) \\cdot T_{\\mathrm{base, false}}$$\n$$E[T_{\\mathrm{base}}] = p \\cdot N(C + b + H) + (1-p) \\cdot N(C + b + L)$$\nExpanding and collecting terms:\n$$E[T_{\\mathrm{base}}] = N(C+b)(p + (1-p)) + N(pH + (1-p)L)$$\n$$E[T_{\\mathrm{base}}] = N(C+b) + N(pH + L - pL) = N(C+b+L) + pN(H-L)$$\n\nNext, we derive the expected total cycle cost for the loop-unswitched implementation, $E[T_{\\mathrm{opt}}]$. In this version, there is a one-time cloning overhead of $d$ cycles. The conditional branch is moved outside the loop. The per-iteration branch overhead $b$ is eliminated, but an I-cache pressure penalty of $s$ cycles per iteration is introduced.\n\nIf the condition is true (probability $p$), the specialized loop for the true case runs. The heavy path work is vectorized, reducing its cost from $H$ to $H/v$. The total cost for this path is:\n$$T_{\\mathrm{opt, true}} = d + N(C + \\frac{H}{v} + s)$$\n\nIf the condition is false (probability $1-p$), the specialized loop for the false case runs. The light path is not vectorized. The total cost for this path is:\n$$T_{\\mathrm{opt, false}} = d + N(C + L + s)$$\n\nThe expected total cost per invocation for the optimized version is:\n$$E[T_{\\mathrm{opt}}] = p \\cdot T_{\\mathrm{opt, true}} + (1-p) \\cdot T_{\\mathrm{opt, false}}$$\n$$E[T_{\\mathrm{opt}}] = p \\left(d + N(C + \\frac{H}{v} + s)\\right) + (1-p) \\left(d + N(C + L + s)\\right)$$\nExpanding and collecting terms:\n$$E[T_{\\mathrm{opt}}] = d(p + (1-p)) + N(C+s)(p+(1-p)) + N\\left(p\\frac{H}{v} + (1-p)L\\right)$$\n$$E[T_{\\mathrm{opt}}] = d + N(C+s) + N\\left(p\\frac{H}{v} + L - pL\\right) = d + N(C+s+L) + pN\\left(\\frac{H}{v}-L\\right)$$\n\nThe break-even probability, $p_{\\mathrm{crit}}$, is the value of $p$ where the expected costs are equal: $E[T_{\\mathrm{base}}] = E[T_{\\mathrm{opt}}]$. Loop unswitching is beneficial for $p > p_{\\mathrm{crit}}$, as stated in the problem.\n\nSetting the two expected cost expressions equal:\n$$N(C+b+L) + p_{\\mathrm{crit}}N(H-L) = d + N(C+s+L) + p_{\\mathrm{crit}}N\\left(\\frac{H}{v}-L\\right)$$\nWe can simplify by canceling the common term $N(C+L)$ from both sides:\n$$Nb + p_{\\mathrm{crit}}N(H-L) = d + Ns + p_{\\mathrm{crit}}N\\left(\\frac{H}{v}-L\\right)$$\nNow, we group terms containing $p_{\\mathrm{crit}}$ on one side:\n$$p_{\\mathrm{crit}}N(H-L) - p_{\\mathrm{crit}}N\\left(\\frac{H}{v}-L\\right) = d + Ns - Nb$$\n$$p_{\\mathrm{crit}}N\\left((H-L) - \\left(\\frac{H}{v}-L\\right)\\right) = d + N(s-b)$$\n$$p_{\\mathrm{crit}}N\\left(H - L - \\frac{H}{v} + L\\right) = d + N(s-b)$$\n$$p_{\\mathrm{crit}}N\\left(H - \\frac{H}{v}\\right) = d + N(s-b)$$\n$$p_{\\mathrm{crit}}NH\\left(1 - \\frac{1}{v}\\right) = d + N(s-b)$$\nSolving for $p_{\\mathrm{crit}}$:\n$$p_{\\mathrm{crit}} = \\frac{d + N(s-b)}{NH\\left(1 - \\frac{1}{v}\\right)}$$\n\nNow, we substitute the given numerical values:\n$N = 5 \\times 10^{5}$\n$C = 2$\n$b = 1.2$\n$H = 12$\n$L = 4$\n$s = 0.3$\n$d = 6.0 \\times 10^{5}$\n$v = 3$\n\nFirst, calculate the numerator:\n$$d + N(s-b) = (6.0 \\times 10^{5}) + (5 \\times 10^{5})(0.3 - 1.2)$$\n$$= 6.0 \\times 10^{5} + (5 \\times 10^{5})(-0.9)$$\n$$= 6.0 \\times 10^{5} - 4.5 \\times 10^{5} = 1.5 \\times 10^{5}$$\n\nNext, calculate the denominator:\n$$NH\\left(1 - \\frac{1}{v}\\right) = (5 \\times 10^{5}) \\cdot 12 \\cdot \\left(1 - \\frac{1}{3}\\right)$$\n$$= (5 \\times 10^{5}) \\cdot 12 \\cdot \\left(\\frac{2}{3}\\right)$$\n$$= (5 \\times 10^{5}) \\cdot 8 = 40 \\times 10^{5} = 4.0 \\times 10^{6}$$\n\nFinally, compute $p_{\\mathrm{crit}}$:\n$$p_{\\mathrm{crit}} = \\frac{1.5 \\times 10^{5}}{4.0 \\times 10^{6}} = \\frac{1.5}{40} = 0.0375$$\n\nThe problem requires the answer to be rounded to four significant figures. The calculated value $0.0375$ has three significant figures. To express it with four, we add a trailing zero, which is significant in this context.\n$$p_{\\mathrm{crit}} = 0.03750$$\nThis is the break-even probability. For any probability $p > 0.03750$, loop unswitching provides a performance benefit under this cost model.", "answer": "$$\\boxed{0.03750}$$", "id": "3654414"}]}