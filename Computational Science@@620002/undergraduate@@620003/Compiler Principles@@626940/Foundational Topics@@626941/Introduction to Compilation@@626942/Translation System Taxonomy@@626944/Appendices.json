{"hands_on_practices": [{"introduction": "A foundational skill in understanding translation systems is distinguishing between those that deeply understand a program's meaning and those that merely manipulate its text. This exercise sharpens that skill by asking you to classify common web development tools based on whether they perform true semantic analysis—involving Abstract Syntax Trees and symbol tables—or rely on simpler syntactic substitution. By engaging with this problem [@problem_id:3678697], you will solidify your understanding of what defines a compiler in the formal sense and learn to identify the key architectural differences in real-world translation systems.", "problem": "A translation system in compiler principles maps strings from a source language $L_s$ to outputs in a target domain. The canonical compiler pipeline consists of phases that include lexical analysis, parsing, semantic analysis, and code generation. Semantic analysis is the phase that checks meaning-related properties of programs relative to the language definition, typically requiring an Abstract Syntax Tree (AST) and structures such as symbol tables, scope resolution, and type checking; a formalization often uses attribute grammars where attributes are computed over the AST to enforce constraints. In contrast, purely syntactic substitution operates on surface forms without building an AST for the source language or enforcing scope and type rules; examples include macro expansion and template substitution driven by pattern matching on text tokens.\n\nConsider the following four web development translation systems, each mapping some input text to an output that a browser or runtime consumes. Your task is to classify which systems necessarily perform semantic analysis (as defined above) rather than purely syntactic substitution.\n\n- System $T_1$: A Mustache-style template engine that replaces delimiters like `{{name}}` with values from a context object, supports iteration sections that repeat fragments by iterating over arrays found in the context, and inserts the resulting strings into Hypertext Markup Language (HTML) pages. It does not parse Cascading Style Sheets (CSS) or JavaScript (JS), does not build an Abstract Syntax Tree (AST) for the template language, and does not perform type checking or symbol table construction; undefined variables simply render as empty strings.\n\n- System $T_2$: A JavaScript XML (JSX) transformer implemented with regular expressions that scans JavaScript source text for angle-bracketed tags, then injects calls to a library function into the text. It does not invoke a JavaScript parser, does not track identifier bindings, and is unaware of nested syntactic contexts such as strings or comments; it fails on inputs where tags appear inside strings or template literals.\n\n- System $T_3$: A Babel transpiler plugin that takes modern JavaScript (ECMAScript (ES) version $6$) and outputs ES version $5$. It first parses the input into an Abstract Syntax Tree (AST), then transforms arrow functions to function expressions and converts block-scoped declarations to function-scoped declarations. During transformation, it performs scope analysis to avoid variable capture, maintaining a symbol table to rename identifiers when necessary.\n\n- System $T_4$: A TypeScript-to-JavaScript transpiler that fully type-checks programs according to the TypeScript specification before emitting JavaScript by erasing types. The transpiler builds a symbol table, resolves imports and namespaces, and rejects programs with type mismatches. The emitted JavaScript corresponds to the original program modulo type annotations.\n\nWhich options correctly list the systems that belong to the semantic-analysis category rather than purely syntactic substitution?\n\nA. $T_3$ and $T_4$\n\nB. $T_2$, $T_3$, and $T_4$\n\nC. $T_1$ and $T_3$\n\nD. $T_1$, $T_2$, and $T_4$", "solution": "The problem statement will first be validated to ensure it is scientifically grounded, self-contained, and objective.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n\nThe problem provides the following definitions and descriptions:\n-   **Translation System:** A system that maps strings from a source language, denoted $L_s$, to outputs in a target domain.\n-   **Canonical Compiler Pipeline:** Consists of phases including lexical analysis, parsing, semantic analysis, and code generation.\n-   **Semantic Analysis:** Defined as the phase that:\n    1.  Checks meaning-related properties of programs.\n    2.  Typically requires an Abstract Syntax Tree (AST).\n    3.  Uses structures such as symbol tables, scope resolution, and type checking.\n    4.  Can be formalized using attribute grammars.\n-   **Purely Syntactic Substitution:** Defined as a process that:\n    1.  Operates on surface forms (e.g., text tokens).\n    2.  Does not build an AST for the source language.\n    3.  Does not enforce scope and type rules.\n    4.  Examples include macro expansion and template substitution.\n\nThe problem then describes four translation systems:\n-   **System $T_1$ (Mustache-style template engine):** Replaces delimiters like `{{name}}` with context values. It explicitly \"does not build an Abstract Syntax Tree (AST) for the template language, and does not perform type checking or symbol table construction.\" Undefined variables are rendered as empty strings.\n-   **System $T_2$ (JSX transformer with regular expressions):** Scans JavaScript text for tags using regular expressions and injects function calls. It explicitly \"does not invoke a JavaScript parser, does not track identifier bindings, and is unaware of nested syntactic contexts.\"\n-   **System $T_3$ (Babel transpiler plugin):** Transforms ECMAScript (ES) version $6$ to ES version $5$. It \"parses the input into an Abstract Syntax Tree (AST),\" \"performs scope analysis to avoid variable capture,\" and \"maintain[s] a symbol table to rename identifiers when necessary.\"\n-   **System $T_4$ (TypeScript-to-JavaScript transpiler):** It \"fully type-checks programs,\" \"builds a symbol table,\" \"resolves imports and namespaces,\" and \"rejects programs with type mismatches\" before emitting JavaScript.\n\nThe task is to identify which of these systems ($T_1$, $T_2$, $T_3$, $T_4$) perform semantic analysis as defined.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem statement is evaluated against the validation criteria.\n-   **Scientifically Grounded:** The problem is firmly rooted in the established principles of compiler design, a core area of computer science. The definitions of semantic analysis, ASTs, symbol tables, scope, and type checking are standard and accurate. The examples of technologies (Mustache, JSX, Babel, TypeScript) are real-world and their described behaviors are consistent with how they (or systems like them) operate.\n-   **Well-Posed:** The problem is well-posed. It provides clear, explicit definitions for the two categories of interest (\"semantic analysis\" and \"purely syntactic substitution\"). It then provides descriptions of four systems with enough detail to classify them according to these definitions. The question is unambiguous and directs the solver to perform this classification. A unique solution exists.\n-   **Objective:** The language is technical, precise, and free of subjective or ambiguous terminology. The descriptions of the systems are factual and based on their operational mechanisms.\n\nThe problem statement has no discernible flaws. It is not scientifically unsound, incomplete, contradictory, unrealistic, or ambiguous. It is a valid, well-structured problem in the domain of compiler principles.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. The solution process will now proceed.\n\n**Solution Derivation**\n\nThe task is to classify each system ($T_1$, $T_2$, $T_3$, $T_4$) as either performing \"semantic analysis\" or \"purely syntactic substitution\" based on the provided definitions.\n\nA system performs **semantic analysis** if it checks meaning-related properties, which requires building an AST and using structures like symbol tables for scope resolution or type checking.\nA system performs **purely syntactic substitution** if it operates on surface text without building an AST and without enforcing scope or type rules.\n\n**Analysis of System $T_1$:**\nThe description for $T_1$ explicitly states it \"does not build an Abstract Syntax Tree (AST)\" and \"does not perform type checking or symbol table construction.\" Instead, it performs text replacement based on delimiters. This behavior perfectly aligns with the definition of \"purely syntactic substitution.\" Therefore, $T_1$ does not perform semantic analysis.\n\n**Analysis of System $T_2$:**\nThe description for $T_2$ states it uses \"regular expressions\" to \"scan... source text.\" It \"does not invoke a JavaScript parser\" and \"does not track identifier bindings.\" Operating on raw text with regular expressions without parsing is the hallmark of surface-level substitution, not deep structural or semantic understanding. The inability to handle nested contexts like strings further confirms its lack of syntactic, let alone semantic, awareness. This matches the definition of \"purely syntactic substitution.\" Therefore, $T_2$ does not perform semantic analysis.\n\n**Analysis of System $T_3$:**\nThe description for $T_3$ states it \"parses the input into an Abstract Syntax Tree (AST).\" Furthermore, it \"performs scope analysis\" and \"maintain[s] a symbol table.\" These actions—building an AST, managing a symbol table, and analyzing scope—are the core activities of semantic analysis as defined in the problem statement. The purpose of renaming identifiers is to preserve the program's meaning (semantics) during transformation. Therefore, $T_3$ performs semantic analysis.\n\n**Analysis of System $T_4$:**\nThe description for $T_4$ states it \"fully type-checks programs,\" \"builds a symbol table,\" and \"resolves imports and namespaces.\" Type checking is a primary and critical form of semantic analysis. Building a symbol table and resolving namespaces are also essential components of this phase, as they determine the meaning and relationships of identifiers. These activities are impossible without first parsing the code into an AST to have a structure to operate upon. Therefore, $T_4$ performs semantic analysis.\n\n**Conclusion:**\nBased on the analysis, systems $T_3$ and $T_4$ are the ones that perform semantic analysis. Systems $T_1$ and $T_2$ are examples of purely syntactic substitution. The question asks for the systems that belong to the semantic-analysis category. These are $T_3$ and $T_4$.\n\n**Option-by-Option Analysis**\n\n-   **A. $T_3$ and $T_4$**: This option lists the two systems, $T_3$ and $T_4$, which were identified as performing semantic analysis.\n    **Verdict: Correct.**\n\n-   **B. $T_2$, $T_3$, and $T_4$**: this option incorrectly includes $T_2$, which was identified as a purely syntactic substitution system.\n    **Verdict: Incorrect.**\n\n-   **C. $T_1$ and $T_3$**: This option incorrectly includes $T_1$, which was identified as a purely syntactic substitution system.\n    **Verdict: Incorrect.**\n\n-   **D. $T_1$, $T_2$, and $T_4$**: This option incorrectly includes $T_1$ and $T_2$, which were identified as purely syntactic substitution systems.\n    **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3678697"}, {"introduction": "Building on the distinction between syntactic and semantic analysis, we now explore how different translation tasks are integrated into a compiler's pipeline. This practice [@problem_id:3678689] uses the example of a macro processor with varied capabilities to demonstrate that a translation system is not a monolith, but a sequence of carefully ordered stages. You will reason about the data required by each macro type—from raw tokens to a fully typed Abstract Syntax Tree—to determine its correct placement, revealing how a compiler's architecture is dictated by the features it supports.", "problem": "Consider a source language $\\mathcal{L}$ whose translation system to a target language $\\mathcal{T}$ is organized as a sequence of phases: lexical analysis, parsing to an Abstract Syntax Tree (AST), semantic analysis including type checking, optimization, and code generation. A translation system, in the sense of compiler principles, is defined as a set of staged transformations that preserve the intended semantics of $\\mathcal{L}$ while producing $\\mathcal{T}$. A distinct pre-translation tool is external to the translator and operates without access to internal compiler artifacts such as the AST or the symbol table; it may alter the source text before the translator begins but is not itself a semantic stage of the language.\n\nA macro processor $\\mathcal{M}$ is proposed for $\\mathcal{L}$, supporting three constructs:\n\n- Construct $1$: Pattern-based hygienic macros that match parsed AST forms and expand into new ASTs, introducing new lexical bindings via generated unique identifiers tied to the compiler’s symbol table. Hygiene guarantees that newly introduced bindings do not accidentally capture or get captured by existing identifiers.\n\n- Construct $2$: Conditional inclusion directives that act on token streams, such as `ifdef` and `endif`, controlled solely by external configuration flags. These operate prior to parsing and can exclude or include token sequences without inspecting language-level semantics.\n\n- Construct $3$: Type-directed macros that select among specialized function calls based on the static types of macro arguments. Expansion requires knowledge of the resolved types after type inference and may consult the symbol table and overload resolution results.\n\nYou must categorize $\\mathcal{M}$ relative to the translation system taxonomy and place each construct in the compilation pipeline in a way that preserves the intended semantics of $\\mathcal{L}$ and is scientifically sound. Choose the option that best reflects a correct classification and placement strategy for all three constructs.\n\nA. Treat $\\mathcal{M}$ as a distinct pre-translation tool for all constructs, running $\\mathcal{M}$ entirely before lexical analysis. Rationale: expansion is purely textual and independent of the compiler; the compiler sees only the expanded text and performs its usual phases thereafter.\n\nB. Integrate only Construct $1$ within the translator, placing hygienic expansion after parsing and before semantic analysis, but treat Constructs $2$ and $3$ as external pre-translation, running entirely before lexical analysis.\n\nC. Make $\\mathcal{M}$ part of the translator. Place Construct $2$ before parsing on the token stream as an early textual gate integrated into the front end; place Construct $1$ between parsing and semantic analysis operating on the AST with access to the symbol table; and place Construct $3$ after type resolution within semantic analysis, allowing expansion decisions based on resolved static types.\n\nD. Run $\\mathcal{M}$ after code generation, transforming machine code by macro expansion. Rationale: by delaying expansion, one can leverage the target representation to guide macro decisions while avoiding interference with earlier analyses.\n\nE. Integrate all constructs after parsing but strictly before type resolution, so that the macro system sees only syntax and not any semantic information. Rationale: unifying placement simplifies the pipeline and avoids external preprocessing, while keeping macros out of lexical analysis.", "solution": "The problem requires an analysis of a proposed macro processor, $\\mathcal{M}$, for a language $\\mathcal{L}$ and its correct placement within a standard compilation pipeline. The pipeline is given as: lexical analysis $\\rightarrow$ parsing to Abstract Syntax Tree (AST) $\\rightarrow$ semantic analysis (including type checking) $\\rightarrow$ optimization $\\rightarrow$ code generation. The validity of the problem statement must be assessed first.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Compilation Pipeline**: Lexical Analysis $\\rightarrow$ Parsing (to AST) $\\rightarrow$ Semantic Analysis (with Type Checking) $\\rightarrow$ Optimization $\\rightarrow$ Code Generation.\n- **Source Language**: $\\mathcal{L}$.\n- **Target Language**: $\\mathcal{T}$.\n- **Pre-translation Tool Definition**: External to the translator, operates without access to internal compiler artifacts (AST, symbol table), alters source text pre-translation, and is not a semantic stage.\n- **Macro Processor $\\mathcal{M}$ Constructs**:\n    1.  **Construct 1 (Hygienic Macros)**: Matches parsed AST forms, expands to new ASTs, introduces lexical bindings using unique identifiers tied to the compiler's symbol table.\n    2.  **Construct 2 (Conditional Inclusion)**: Acts on token streams (`ifdef`), controlled by external flags, operates prior to parsing, does not inspect language-level semantics.\n    3.  **Construct 3 (Type-Directed Macros)**: Selects function calls based on static types of arguments, requires knowledge of resolved types from type inference, may consult the symbol table and overload resolution results.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem statement is scientifically grounded, well-posed, and objective. It uses standard terminology from the field of compiler design.\n- **Scientific Soundness**: The concepts of compilation phases (lexing, parsing, semantic analysis), internal representations (AST, symbol table), and different macro expansion strategies (textual/token-based, AST-based, type-directed) are fundamental and well-established in computer science. The descriptions of the three constructs are consistent with real-world language features (e.g., C-style preprocessor for Construct 2, Lisp/Scheme macros for Construct 1, and metaprogramming features in languages like Nim or D for Construct 3).\n- **Well-Posedness**: The properties of each construct provide specific constraints on its placement within the compilation pipeline. For instance, a construct that operates on an AST must be placed after parsing, and one that depends on resolved types must be placed after the relevant stage of semantic analysis. These constraints are clear and sufficient to determine a unique, logical arrangement.\n- **Objectivity**: The problem is stated in precise, technical terms, free of ambiguity or subjective claims. The task is to find a scientifically sound placement, which is an objective criterion within the domain of compiler engineering.\n\nThe problem does not exhibit any flaws such as factual unsoundness, incompleteness, contradiction, or vagueness. The definitions are self-contained and sufficient for a rigorous analysis.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The solution will proceed by analyzing the placement requirements for each construct.\n\n### Derivation of the Solution\n\nThe correct placement of each macro construct is determined by its dependencies on the data structures produced during compilation.\n\n1.  **Analysis of Construct 1 (Hygienic Macros)**:\n    - The description states it \"match[es] parsed AST forms and expand[s] into new ASTs\". This explicitly requires the existence of an Abstract Syntax Tree (AST). The AST is the output of the parsing phase. Therefore, this construct must operate *after* parsing.\n    - It also needs to be \"tied to the compiler’s symbol table\" to generate unique identifiers for hygiene. This confirms it is not an external tool but must be integrated with the compiler's internal state.\n    - The output is a new AST, which must subsequently be analyzed for semantic correctness (e.g., type checking). Thus, the expansion should occur *before* semantic analysis.\n    - **Conclusion for Construct 1**: Placement is between the parsing phase and the semantic analysis phase.\n\n2.  **Analysis of Construct 2 (Conditional Inclusion)**:\n    - The description specifies that it \"act[s] on token streams\" and \"operate[s] prior to parsing\". A token stream is the output of the lexical analysis phase and the input to the parsing phase.\n    - It operates based on \"external configuration flags\" and does not require \"language-level semantics\". This makes it a simple, non-semantic filtering step.\n    - This behavior is characteristic of a preprocessor that is integrated into the compiler's front end, operating on tokens before they are fed to the parser.\n    - **Conclusion for Construct 2**: Placement is after lexical analysis (which produces the token stream) and before parsing.\n\n3.  **Analysis of Construct 3 (Type-Directed Macros)**:\n    - The definition is that it expands based on the \"static types of macro arguments\" and \"requires knowledge of the resolved types after type inference\" and \"overload resolution results\".\n    - Type inference, type checking, and overload resolution are the primary responsibilities of the semantic analysis phase. The macro expander for this construct cannot run until this information is available.\n    - Therefore, this expansion must occur *during* or *after* the semantic analysis phase has computed the necessary type information. It acts upon a semantically annotated AST.\n    - **Conclusion for Construct 3**: Placement is within the semantic analysis phase, after type resolution has occurred.\n\n**Synthesis**:\nThe three constructs must be placed at three distinct stages of the compilation pipeline:\n- Construct 2: Before Parsing (on the token stream).\n- Construct 1: After Parsing, Before Semantic Analysis (on the raw AST).\n- Construct 3: During/After Semantic Analysis (on the type-annotated AST).\n\nSince the constructs require deep integration with the compiler's internal data structures (token stream, AST, symbol table, type information), the macro processor $\\mathcal{M}$ cannot be a single, monolithic \"pre-translation tool\" as defined in the problem. It must be an integrated component of the translator itself, with its functions distributed across the pipeline.\n\n### Evaluation of Options\n\n**A. Treat $\\mathcal{M}$ as a distinct pre-translation tool for all constructs, running $\\mathcal{M}$ entirely before lexical analysis. Rationale: expansion is purely textual and independent of the compiler; the compiler sees only the expanded text and performs its usual phases thereafter.**\nThis is incorrect. The rationale is falsified by the definitions of Constructs $1$ and $3$. Construct $1$ requires an AST, which is not available before parsing. Construct $3$ requires resolved types, which are not available until semantic analysis. Classifying $\\mathcal{M}$ as an external pre-translation tool contradicts its specified dependencies on internal compiler artifacts.\n**Verdict: Incorrect.**\n\n**B. Integrate only Construct $1$ within the translator, placing hygienic expansion after parsing and before semantic analysis, but treat Constructs $2$ and $3$ as external pre-translation, running entirely before lexical analysis.**\nThis is incorrect. While it correctly places Construct $1$, it incorrectly categorizes and places Construct $3$. Construct $3$ requires type information from the semantic analyzer and cannot possibly run as an external pre-translation tool.\n**Verdict: Incorrect.**\n\n**C. Make $\\mathcal{M}$ part of the translator. Place Construct $2$ before parsing on the token stream as an early textual gate integrated into the front end; place Construct $1$ between parsing and semantic analysis operating on the AST with access to the symbol table; and place Construct $3$ after type resolution within semantic analysis, allowing expansion decisions based on resolved static types.**\nThis option aligns perfectly with the derived placement for all three constructs. It correctly identifies that $\\mathcal{M}$ must be an integrated part of the translator. It places Construct $2$ on the token stream before parsing. It places Construct $1$ on the AST between parsing and semantic analysis. It places Construct $3$ after type resolution within semantic analysis. This represents a scientifically sound and internally consistent compiler architecture.\n**Verdict: Correct.**\n\n**D. Run $\\mathcal{M}$ after code generation, transforming machine code by macro expansion. Rationale: by delaying expansion, one can leverage the target representation to guide macro decisions while avoiding interference with earlier analyses.**\nThis is incorrect. The constructs are defined in terms of high-level language concepts: tokens (Construct $2$), syntactic structure via ASTs (Construct $1$), and static types (Construct $3$). These concepts do not exist in a structured way, or at all, in the final machine code output. Macro expansion is fundamentally a source-to-source or source-to-IR transformation, not a post-compilation binary modification.\n**Verdict: Incorrect.**\n\n**E. Integrate all constructs after parsing but strictly before type resolution, so that the macro system sees only syntax and not any semantic information. Rationale: unifying placement simplifies the pipeline and avoids external preprocessing, while keeping macros out of lexical analysis.**\nThis is incorrect. This placement is only correct for Construct $1$. It is incorrect for Construct $2$, which operates on a token stream before parsing. It is critically incorrect for Construct $3$, which explicitly requires semantic information (resolved types) and therefore *must* be placed after type resolution has begun, not \"strictly before\" it. The rationale that the macro system sees \"only syntax and not any semantic information\" is directly contradicted by the definition of Construct $3$.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{C}$$", "id": "3678689"}, {"introduction": "This final practice elevates our perspective from the internal pipeline to a high-level taxonomy of entire translation systems, exploring the profound link between language features and runtime architecture. You will analyze how expressive constructs like closures, exceptions, and generators can be implemented under different system constraints: a simple stack machine, a compiler with a heap-allocating library, or a full-fledged virtual machine [@problem_id:3678705]. This exercise illuminates why different translation strategies exist and demonstrates how the design of a programming language directly informs the design of its compiler and runtime environment.", "problem": "Consider the taxonomy of translation systems in compiler principles. Assume three translation system categories under the following explicit constraints, and three small language features that must be supported:\n\nTranslation system categories:\n- $T_0$: A syntax-directed translator that emits target three-address code for a conventional call/return stack machine with basic arithmetic, conditional branches, and function calls/returns. It has no heap allocation, no mechanism for dynamic stack unwinding beyond ordinary return, and no support for continuations or coroutines. The runtime system is limited to the minimal code needed to call and return; it offers no specialized control features.\n- $T_{\\text{lib}}$: A compiler that performs syntax-directed translation into a conventional low-level language and links a standard runtime library that provides heap allocation, tagged records, function pointers, and ordinary call/return. It has no built-in non-local control transfer primitives (no exceptions, no continuations) beyond what can be implemented in user code.\n- $T_{\\text{VM}}$: A bytecode Virtual Machine (VM) interpreter with built-in operations for raising exceptions (including dynamic stack unwinding across activation records) and for yielding/resuming coroutines (generator frames). The translator maps source constructs to VM bytecodes; the runtime system implements unwinding and coroutine resumption.\n\nLanguage features:\n- Closures: First-class functions with lexical scoping. A closure may escape its defining scope. A closure’s behavior is defined by its code pointer and bindings of free variables from the lexical environment.\n- Exceptions: Non-local control transfer via `raise`/`try`/`catch`. A raised exception may propagate across $k$ dynamic call frames for any $k \\ge 1$ until a matching handler is found.\n- Generators: Functions that can suspend via `yield` and later resume from the suspension point, possibly multiple times, maintaining local state between yields.\n\nDefinitions and foundations:\n- A syntax-directed translation associates rules to the Abstract Syntax Tree (AST) that map source constructs deterministically into target constructs, without requiring new runtime machinery beyond the target’s existing primitives. Its correctness is grounded in the principle that semantics preserved by translation must be realizable as compositions of available target operations.\n- Runtime support is required when a source semantic effect cannot be reduced, by local syntactic rewriting, to a sequence of existing target primitives without global instrumentation or a specialized mechanism that changes the behavior of activation records, control flow, or memory lifetimes at execution time.\n\nTask:\nUsing the above constraints and definitions, reason from first principles about lifetimes of environments, activation records, and control-transfer semantics to predict which translation system categories can handle each feature primarily via syntax-directed translation versus requiring runtime support. Validate your predictions by describing minimal prototypes that realize each feature under the stated constraints.\n\nSelect all statements that are correct under these assumptions:\n\nA. In $T_{\\text{lib}}$, first-class closures with lexical scoping can be implemented via syntax-directed translation that converts each closure into a pair consisting of a code pointer and a heap-allocated environment record capturing its free variables, using only heap allocation and ordinary function calls from the runtime library.\n\nB. In $T_0$, exceptions that propagate non-locally across $k$ frames for $k \\ge 1$ can be implemented solely by syntax-directed translation without changing any function signatures or calling conventions.\n\nC. In $T_{\\text{lib}}$, generators can be compiled by syntax-directed translation into explicit state machines, where each generator is translated into a heap-allocated record storing a finite state and locals, and a dispatcher function that advances the state on each invocation, without requiring any special VM resume operation.\n\nD. In $T_{\\text{VM}}$, exceptions can be handled by syntax-directed translation that inserts VM raise and handler setup bytecodes, while the dynamic unwinding and handler discovery are provided by the VM runtime; thus the primary mechanism is runtime support for non-local control transfer.\n\nE. In $T_0$, closures can be fully implemented on the call stack without heap allocation and still be first-class (i.e., can escape arbitrarily), using only syntax-directed translation and ordinary call/return.", "solution": "The problem statement is a valid exercise in compiler design principles. It establishes a clear taxonomy of translation systems (`$T_0$`, `$T_{\\text{lib}}$`, `$T_{\\text{VM}}$`) with well-defined constraints and capabilities. It also defines three standard language features (closures, exceptions, generators) and provides formal definitions for \"syntax-directed translation\" versus \"runtime support\". The task is to reason about which system can support which feature and through what mechanism. The problem is self-contained, scientifically grounded in established computer science theory, and objective.\n\nThe core of the analysis rests on the lifetime of activation records and the mechanisms for control transfer.\n- In a simple stack-based machine like the target of `$T_0$`, an activation record (stack frame) for a function call exists only for the dynamic extent of that call. It is created on entry and destroyed on exit (return). Control flow is limited to direct jumps within a function and the call/return sequence.\n- In a system with a heap, like the target of `$T_{\\text{lib}}$`, data can be allocated with a lifetime that is independent of the call stack. This is crucial for implementing features where state must outlive the function call that created it.\n- A Virtual Machine like `$T_{\\text{VM}}$` provides a higher-level abstraction, where the runtime system can implement complex control and data management operations (like stack unwinding or coroutine state management) as atomic primitives, simplifying the compiler's task.\n\nBased on these first principles, we evaluate each option:\n\nA. In `$T_{\\text{lib}}$`, first-class closures with lexical scoping can be implemented via syntax-directed translation that converts each closure into a pair consisting of a code pointer and a heap-allocated environment record capturing its free variables, using only heap allocation and ordinary function calls from the runtime library.\n\nThis statement describes the standard and correct implementation of first-class (escaping) closures in a language that compiles to a target with heap allocation (like C). The fundamental problem with closures is that if they can escape the scope in which they are defined (e.g., by being returned from a function), their lexical environment (the bindings of their free variables) must survive the deallocation of the defining function's stack frame. A purely stack-based environment would lead to a dangling pointer, known as the \"upward funarg problem\".\nThe system `$T_{\\text{lib}}$` provides heap allocation. A syntax-directed translation scheme can solve this problem as follows:\n1. When a closure is created, the compiler generates code to allocate a record on the heap.\n2. It then generates code to copy the values or references of all free variables from the current lexical environment into this heap-allocated record.\n3. The closure value itself is then represented as a pair of pointers: one pointer to the function's machine code, and one pointer to this heap-allocated environment record.\nWhen the closure is invoked, the environment pointer is passed as a hidden argument, giving the function code access to its captured variables. This entire process uses only heap allocation, record creation, and ordinary function calls, all of which are primitives available in the target of `$T_{\\text{lib}}$` as per its definition. This is a perfect example of realizing a complex source-level semantic (lexical closures) via a local syntactic-rewriting into a composition of available target primitives.\nVerdict: **Correct**.\n\nB. In `$T_0$`, exceptions that propagate non-locally across `$k$` frames for `$k \\ge 1$` can be implemented solely by syntax-directed translation without changing any function signatures or calling conventions.\n\nThis statement is incorrect. The system `$T_0$` is severely constrained: it has \"no mechanism for dynamic stack unwinding beyond ordinary return\". Propagating an exception across `$k$` frames requires non-local control transfer, jumping from the point of the `raise` to a handler in an ancestor activation record, and unwinding (destroying) all intermediate `$k-1$` activation records. `$T_0$` lacks any primitive for this. To implement this \"solely by syntax-directed translation\", the compiler would have to transform the code to explicitly manage the control flow. A common (though inefficient) approach is to return an error status from every function. For example, a call `y = f(x);` would be transformed into `status = f(x, &y); if (status != OK) return status;`. This, however, explicitly changes function signatures (adding a return parameter and a status return value) and calling conventions, directly violating the conditions of the statement. Without changing signatures, one might use a global flag, but every function would still need to be instrumented to check this flag after every call and manually return, which is a form of global instrumentation, not the local rewriting implied by the problem's definition of syntax-directed translation. The fundamental mechanism for stack unwinding is absent in `$T_0$` and cannot be synthesized without specialized runtime machinery or intrusive, global code transformations.\nVerdict: **Incorrect**.\n\nC. In `$T_{\\text{lib}}$`, generators can be compiled by syntax-directed translation into explicit state machines, where each generator is translated into a heap-allocated record storing a finite state and locals, and a dispatcher function that advances the state on each invocation, without requiring any special VM resume operation.\n\nThis statement is correct. A generator must preserve its local state and its point of execution across `yield` calls. An ordinary function call does not support this, as its activation record is destroyed upon return. Much like with closures, the solution is to move the state that must persist off the stack and onto the heap. `$T_{\\text{lib}}$` provides a heap.\nA syntax-directed translation can transform a generator function into:\n1. A state record, allocated on the heap, to store all local variables of the generator and an additional state variable to represent the current point of execution (i.e., which `yield` it's at).\n2. A single dispatcher function. On each invocation, this function uses the state variable (e.g., in a `switch` statement) to jump to the code segment immediately following the last `yield`. It executes until it hits the next `yield`, at which point it saves the new execution point and any updated locals back into the heap-allocated state record, and then returns the yielded value.\nThis technique, sometimes called \"generator transformation\" or related to \"continuation-passing style transformation,\" relies only on heap allocation, records, and ordinary function calls. No special `resume` or `yield` primitive is required from the runtime system. The semantics of suspension and resumption are entirely simulated by the transformed code structure.\nVerdict: **Correct**.\n\nD. In `$T_{\\text{VM}}$`, exceptions can be handled by syntax-directed translation that inserts VM raise and handler setup bytecodes, while the dynamic unwinding and handler discovery are provided by the VM runtime; thus the primary mechanism is runtime support for non-local control transfer.\n\nThis statement is correct. The definition of `$T_{\\text{VM}}$` states that it has \"built-in operations for raising exceptions (including dynamic stack unwinding across activation records)\". The problem's definition of \"runtime support\" includes \"a specialized mechanism that changes the behavior of activation records, control flow, or memory lifetimes at execution time\". Dynamic stack unwinding is precisely such a mechanism. Therefore, the compiler's task becomes a straightforward, syntax-directed mapping: a `try`/`catch` block translates to bytecodes that register a handler, and a `raise` statement translates to a `RAISE` bytecode. The VM interpreter, upon encountering the `RAISE` bytecode, executes the complex logic of searching the call stack for a handler and unwinding the stack frames. This perfectly matches the description in the option. The compiler uses a simple translation, while the runtime provides the powerful semantic implementation.\nVerdict: **Correct**.\n\nE. In `$T_0$`, closures can be fully implemented on the call stack without heap allocation and still be first-class (i.e., can escape arbitrarily), using only syntax-directed translation and ordinary call/return.\n\nThis statement is fundamentally incorrect. The term \"first-class\" implies that the closure can be treated like any other data: stored in variables, passed to functions, and, crucially, returned from functions. The ability to \"escape arbitrarily\" is the key. If a function creates a closure and returns it, the closure's lifetime must exceed that of its creating function's activation. The activation record for the creating function exists on the call stack and is destroyed upon return. If the closure's lexical environment (containing its free variables) were stored in this stack frame, any subsequent use of the returned closure would access deallocated memory, resulting in a dangling pointer and undefined behavior. Implementing first-class, escaping closures correctly requires that their captured environment has a lifetime independent of the stack, which necessitates storage with dynamic extent, i.e., heap allocation. The system `$T_0$` explicitly has \"no heap allocation\". Therefore, it can only support non-escaping closures (downward funargs), not fully first-class ones.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{ACD}$$", "id": "3678705"}]}