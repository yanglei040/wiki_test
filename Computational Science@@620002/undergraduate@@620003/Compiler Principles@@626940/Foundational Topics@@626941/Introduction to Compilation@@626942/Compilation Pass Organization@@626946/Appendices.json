{"hands_on_practices": [{"introduction": "The effectiveness of a compiler pipeline is not merely the sum of its parts; the order in which optimization passes are executed is critically important. Passes can have enabling or disabling relationships, where one pass creates opportunities for another, or conversely, eliminates them. This exercise provides a concrete, quantitative exploration of these dependencies, demonstrating how different permutations of a few standard passes can yield vastly different outcomes in code quality [@problem_id:3629247]. By methodically analyzing each sequence, you will develop a deeper appreciation for the phase-ordering problem, a classic and challenging issue in compiler design.", "problem": "A compiler pipeline consists of three optimization passes applied to a single procedure’s intermediate representation, executed exactly once each and in some order: Global Value Numbering (GVN), Loop-Invariant Code Motion (LICM), and Dead Code Elimination (DCE). The following foundational facts apply to these passes:\n\n- Global Value Numbering (GVN) identifies computations with identical value semantics and eliminates later redundant computations when an earlier computation’s value dominates the later one.\n- Loop-Invariant Code Motion (LICM) hoists computations whose operands do not change across iterations from loop bodies to a location that dominates the loop (such as a loop preheader).\n- Dead Code Elimination (DCE) removes instructions whose results are unused and which have no side effects, as determined by dataflow after previous transformations.\n\nConsider that the procedure contains independent code idioms partitioned into disjoint groups, each group contributing potential eliminated instructions subject to enabling relationships among passes. Let the group counts be:\n- Type $1$: $t_{1} = 7$ groups. Each group contains duplicate loop-invariant computations located in separate blocks that do not dominate one another initially. Only after $LICM$ hoists these computations to a common dominator can $GVN$ eliminate one redundant computation per group. Thus, elimination in these groups occurs if and only if $LICM$ precedes $GVN$ in the pipeline, and in that case exactly $1$ instruction per group is eliminated by $GVN$; otherwise $0$.\n- Type $2$: $t_{2} = 5$ groups. Each group contains a redundancy that $GVN$ can always eliminate, independent of $LICM$. Furthermore, the $GVN$ transformation exposes exactly one additional dead temporary per group, which $DCE$ can eliminate if and only if $DCE$ runs after $GVN$. Thus, each such group yields exactly $1$ instruction eliminated by $GVN$ in all orders, and yields an additional $1$ instruction eliminated by $DCE$ only when $DCE$ occurs after $GVN$.\n- Type $3$: $t_{3} = 4$ groups. Each group contains a loop-internal temporary whose uses are confined to the loop. After $LICM$ hoists the invariant producer out of the loop and rewrites uses, the temporary becomes unused. $DCE$ will eliminate this temporary only if $DCE$ runs after $LICM$. $GVN$ plays no role for these groups. Thus, elimination occurs if and only if $LICM$ precedes $DCE$, and in that case exactly $1$ instruction per group is eliminated by $DCE$; otherwise $0$.\n- Type $4$: $t_{4} = 6$ groups. Each group requires an enabling chain: $LICM$ must first hoist to establish dominance among duplicate computations; then $GVN$ eliminates one redundant computation; this in turn exposes a dead temporary that $DCE$ can eliminate only if $DCE$ runs after $GVN$. In these groups, if $LICM$ precedes $GVN$ and $DCE$ runs after $GVN$, exactly $2$ instructions per group are eliminated (one by $GVN$ and one by $DCE$). If $LICM$ precedes $GVN$ but $DCE$ runs before $GVN$, then only $1$ instruction per group is eliminated (by $GVN$). If $LICM$ does not precede $GVN$, then $0$ instructions are eliminated in the group.\n\nAssume all groups are disjoint and independent (no instruction belongs to more than one group), no side effects or undefined behavior interfere with elimination, and no pass creates new opportunities beyond the described enabling relationships. Consider all $6$ permutations of the three passes: $GVN \\rightarrow LICM \\rightarrow DCE$, $GVN \\rightarrow DCE \\rightarrow LICM$, $LICM \\rightarrow GVN \\rightarrow DCE$, $LICM \\rightarrow DCE \\rightarrow GVN$, $DCE \\rightarrow GVN \\rightarrow LICM$, and $DCE \\rightarrow LICM \\rightarrow GVN$.\n\nCompute the arithmetic mean of the total number of eliminated instructions over these $6$ permutations. Express your final answer as an exact number with no units. No rounding is required.", "solution": "The problem statement has been validated and is deemed sound. It is scientifically grounded in the principles of compiler optimization, well-posed with a clear objective and sufficient data, and free of contradictions or ambiguities. We may proceed with the solution.\n\nThe objective is to compute the arithmetic mean of the total number of eliminated instructions over all possible orderings of the three optimization passes: Global Value Numbering (GVN), Loop-Invariant Code Motion (LICM), and Dead Code Elimination (DCE). Let us denote the passes by G, L, and D respectively. There are $3! = 6$ unique permutations of these three passes.\n\nThe counts of the disjoint instruction groups are given as:\n- Type $1$: $t_{1} = 7$\n- Type $2$: $t_{2} = 5$\n- Type $3$: $t_{3} = 4$\n- Type $4$: $t_{4} = 6$\n\nLet $pos(X)$ be the position ($1$, $2$, or $3$) of a pass $X$ in a given permutation. The conditions for instruction elimination for each group type can be formalized as follows:\n\n- **Type 1 ($t_1$ groups):** $1$ instruction is eliminated per group if and only if $pos(L) < pos(G)$. This yields $t_{1} \\times 1 = 7$ instructions. Otherwise, $0$ instructions are eliminated.\n- **Type 2 ($t_2$ groups):** $1$ instruction is always eliminated by GVN. An additional $1$ instruction is eliminated by DCE if and only if $pos(D) > pos(G)$. This yields a total of $t_{2} \\times 1 = 5$ instructions if $pos(D) < pos(G)$, and $t_{2} \\times (1+1) = 10$ instructions if $pos(D) > pos(G)$.\n- **Type 3 ($t_3$ groups):** $1$ instruction is eliminated per group if and only if $pos(D) > pos(L)$. This yields $t_{3} \\times 1 = 4$ instructions. Otherwise, $0$ instructions are eliminated.\n- **Type 4 ($t_4$ groups):**\n    - If $pos(L) < pos(G)$ and $pos(G) < pos(D)$, a total of $2$ instructions are eliminated per group. This yields $t_{4} \\times 2 = 12$ instructions.\n    - If $pos(L) < pos(G)$ and $pos(D) < pos(G)$, a total of $1$ instruction is eliminated per group. This yields $t_{4} \\times 1 = 6$ instructions.\n    - If $pos(L) > pos(G)$, $0$ instructions are eliminated.\n\nWe will now systematically calculate the total number of eliminated instructions, $E_i$, for each of the $6$ permutations.\n\n1.  **Permutation $P_1: G \\rightarrow L \\rightarrow D$** ($pos(G)=1, pos(L)=2, pos(D)=3$)\n    - Type 1: $pos(L) > pos(G)$. Eliminations: $0$.\n    - Type 2: $pos(D) > pos(G)$. Eliminations: $t_2 \\times (1+1) = 5 \\times 2 = 10$.\n    - Type 3: $pos(D) > pos(L)$. Eliminations: $t_3 \\times 1 = 4$.\n    - Type 4: $pos(L) > pos(G)$. Eliminations: $0$.\n    - Total $E_1 = 0 + 10 + 4 + 0 = 14$.\n\n2.  **Permutation $P_2: G \\rightarrow D \\rightarrow L$** ($pos(G)=1, pos(D)=2, pos(L)=3$)\n    - Type 1: $pos(L) > pos(G)$. Eliminations: $0$.\n    - Type 2: $pos(D) > pos(G)$. Eliminations: $t_2 \\times (1+1) = 10$.\n    - Type 3: $pos(D) < pos(L)$. Eliminations: $0$.\n    - Type 4: $pos(L) > pos(G)$. Eliminations: $0$.\n    - Total $E_2 = 0 + 10 + 0 + 0 = 10$.\n\n3.  **Permutation $P_3: D \\rightarrow G \\rightarrow L$** ($pos(D)=1, pos(G)=2, pos(L)=3$)\n    - Type 1: $pos(L) > pos(G)$. Eliminations: $0$.\n    - Type 2: $pos(D) < pos(G)$. Eliminations: $t_2 \\times 1 = 5$.\n    - Type 3: $pos(D) < pos(L)$. Eliminations: $0$.\n    - Type 4: $pos(L) > pos(G)$. Eliminations: $0$.\n    - Total $E_3 = 0 + 5 + 0 + 0 = 5$.\n\n4.  **Permutation $P_4: L \\rightarrow G \\rightarrow D$** ($pos(L)=1, pos(G)=2, pos(D)=3$)\n    - Type 1: $pos(L) < pos(G)$. Eliminations: $t_1 \\times 1 = 7$.\n    - Type 2: $pos(D) > pos(G)$. Eliminations: $t_2 \\times (1+1) = 10$.\n    - Type 3: $pos(D) > pos(L)$. Eliminations: $t_3 \\times 1 = 4$.\n    - Type 4: $pos(L) < pos(G)$ and $pos(G) < pos(D)$. Eliminations: $t_4 \\times 2 = 6 \\times 2 = 12$.\n    - Total $E_4 = 7 + 10 + 4 + 12 = 33$.\n\n5.  **Permutation $P_5: L \\rightarrow D \\rightarrow G$** ($pos(L)=1, pos(D)=2, pos(G)=3$)\n    - Type 1: $pos(L) < pos(G)$. Eliminations: $t_1 \\times 1 = 7$.\n    - Type 2: $pos(D) < pos(G)$. Eliminations: $t_2 \\times 1 = 5$.\n    - Type 3: $pos(D) > pos(L)$. Eliminations: $t_3 \\times 1 = 4$.\n    - Type 4: $pos(L) < pos(G)$ and $pos(D) < pos(G)$. Eliminations: $t_4 \\times 1 = 6$.\n    - Total $E_5 = 7 + 5 + 4 + 6 = 22$.\n\n6.  **Permutation $P_6: D \\rightarrow L \\rightarrow G$** ($pos(D)=1, pos(L)=2, pos(G)=3$)\n    - Type 1: $pos(L) < pos(G)$. Eliminations: $t_1 \\times 1 = 7$.\n    - Type 2: $pos(D) < pos(G)$. Eliminations: $t_2 \\times 1 = 5$.\n    - Type 3: $pos(D) < pos(L)$. Eliminations: $0$.\n    - Type 4: $pos(L) < pos(G)$ and $pos(D) < pos(G)$. Eliminations: $t_4 \\times 1 = 6$.\n    - Total $E_6 = 7 + 5 + 0 + 6 = 18$.\n\nThe total number of eliminated instructions over all $6$ permutations is the sum of the totals for each permutation:\n$$ E_{\\text{total}} = E_1 + E_2 + E_3 + E_4 + E_5 + E_6 $$\n$$ E_{\\text{total}} = 14 + 10 + 5 + 33 + 22 + 18 = 102 $$\nThe arithmetic mean, $\\mu$, is this total sum divided by the number of permutations, which is $6$:\n$$ \\mu = \\frac{E_{\\text{total}}}{6} = \\frac{102}{6} $$\n$$ \\mu = 17 $$\nThe arithmetic mean of the total number of eliminated instructions is $17$.", "answer": "$$\\boxed{17}$$", "id": "3629247"}, {"introduction": "While the previous exercise showed that pass order often matters, it is equally important for a compiler engineer to know when it does not. If two passes $p$ and $q$ commute—meaning that applying $p$ then $q$ is semantically equivalent to applying $q$ then $p$—they can be reordered freely or even executed in parallel. This practice challenges you to reason about the formal property of commutativity by analyzing the interactions between several common compiler transformations [@problem_id:3629191]. Mastering this type of analysis is essential for designing flexible, efficient, and parallel compiler backends.", "problem": "A compiler organizes its pipeline as a sequence of passes, each modeled as a pure function on a program’s Intermediate Representation (IR). Consider an IR that is a directed Control Flow Graph (CFG), where each node is a basic block containing a list of instructions, and uses Static Single Assignment (SSA) form for virtual registers. Control Flow Graph (CFG) and Static Single Assignment (SSA) are defined as follows: the CFG is a graph of basic blocks connected by control-flow edges, and SSA form assigns each variable exactly once with explicit $\\phi$-nodes for merges. A pass is a total function $p$ that maps IR graphs to IR graphs. Two passes $p$ and $q$ commute if for all IR inputs $I$,\n$(p \\circ q)(I) \\equiv (q \\circ p)(I),$\nwhere $\\equiv$ denotes semantic equivalence modulo alpha-renaming of SSA names and relabeling of basic blocks (that is, graph isomorphism preserving control-flow and instruction multiset).\n\nAssume deterministic, well-defined program semantics and the following pass specifications. Use these as the fundamental base:\n\n- Variable Renaming (VR): capture-avoiding alpha-renaming of SSA virtual registers by a bijection on names. VR does not change the CFG, instruction opcodes, or def-use edges except for renaming identifiers. It preserves all dataflow facts up to renaming.\n- Block Relabeling and Layout (BRL): permutation of basic block identifiers and linear order. BRL does not change CFG edges, instruction sequences within a block, or any instruction content; it changes only labels and layout metadata.\n- Local Constant Folding (LCF): for any instruction whose operands are all literal constants, replace the instruction with the computed literal. LCF does not add or remove blocks or control-flow edges; it replaces certain instruction nodes with literals but does not propagate constants across instructions.\n- Unreachable Code Elimination (UCE): removes basic blocks not reachable from the entry block in the current CFG. UCE does not rewrite instructions in reachable blocks; it deletes unreachable blocks and any edges incident to them.\n- Constant Propagation (CP): propagates compile-time constants along def-use chains using dominance, replacing uses with literal constants when definitions are literal and intervening redefinitions do not occur. CP does not change the CFG.\n\nA scheduler wants to run commuting pass pairs in parallel without changing semantics. If $p$ and $q$ commute, any parallel execution that is equivalent to committing either $p$ then $q$ or $q$ then $p$ is considered semantics-preserving. Under the pass definitions above, which option(s) list only commuting pass pairs that can be safely scheduled in parallel in this sense?\n\nChoose all that apply.\n\n- A. The pairs $\\{$(LCF, UCE), (BRL, VR)$\\}$\n\n- B. The pair $\\{$(CP, Dead Branch Elimination)$\\}$, where Dead Branch Elimination replaces a conditional branch with a direct jump when the condition is a literal boolean and deletes the now-unreachable successor and its descendants.\n\n- C. The pairs $\\{$(LCF, CP), (UCE, Dead Branch Elimination)$\\}$\n\n- D. The pairs $\\{$(UCE, BRL), (LCF, BRL)$\\}$\n\n- E. The pairs $\\{$(VR, LCF), (VR, UCE)$\\}$\n\nYour reasoning should start from the formal definitions above and not assume undocumented behaviors. For incorrect choices, it suffices to exhibit a single counterexample IR $I$ where $(p \\circ q)(I) \\not\\equiv (q \\circ p)(I)$, or to identify a violated independence principle. For correct choices, argue from first principles (read/write footprints, invariances, and graph isomorphism) that commutativity holds for all $I$ and thus a parallel schedule that serializes to either order preserves semantics.", "solution": "### Step 1: Extract Givens\n\nThe problem statement provides the following definitions and conditions:\n- **Intermediate Representation (IR)**: A directed Control Flow Graph (CFG) where nodes are basic blocks and virtual registers are in Static Single Assignment (SSA) form.\n- **CFG**: A graph of basic blocks connected by control-flow edges.\n- **SSA**: Each variable is assigned exactly once, with explicit $\\phi$-nodes for merges.\n- **Pass**: A total function $p$ mapping an IR graph to another IR graph.\n- **Commutativity**: Two passes $p$ and $q$ commute if for all IR inputs $I$, $(p \\circ q)(I) \\equiv (q \\circ p)(I)$.\n- **Equivalence ($\\equiv$)**: Semantic equivalence defined as graph isomorphism of the CFG that preserves control-flow and the multiset of instructions in each block, modulo alpha-renaming of SSA names and relabeling of basic blocks.\n- **Pass Specifications**:\n    - **Variable Renaming (VR)**: Alpha-renames SSA virtual registers. Does not change the CFG, opcodes, or def-use edges (except for renaming).\n    - **Block Relabeling and Layout (BRL)**: Permutes basic block identifiers and their linear order. Does not change CFG edges or instructions.\n    - **Local Constant Folding (LCF)**: For an instruction with all literal constant operands, it is replaced by the computed literal result. Does not change the CFG. Does not propagate constants.\n    - **Unreachable Code Elimination (UCE)**: Removes basic blocks not reachable from the entry block, along with incident edges. Does not rewrite instructions in reachable blocks.\n    - **Constant Propagation (CP)**: Replaces uses of a variable with a literal constant if its definition is that literal constant and no redefinitions occur on the path. Does not change the CFG.\n- **Scheduling Goal**: To run commuting pass pairs in parallel.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientifically Grounded**: The problem is well-grounded in the principles of compiler construction. The concepts of IR, CFG, SSA, and compiler passes like constant folding and dead code elimination are standard material in this domain.\n- **Well-Posed**: The problem is well-posed. It provides formal definitions for passes and a precise, albeit strict, definition of equivalence ($\\equiv$). The question asks for an evaluation of specific pairs against this formal definition of commutativity. A unique set of correct options can be determined.\n- **Objective**: The problem is stated using formal, objective language without ambiguity or subjective claims.\n\nThe problem statement is self-contained and internally consistent. The definitions, while simplified, are sufficient for rigorous analysis. A new pass, Dead Branch Elimination, is introduced within the options, but its definition is provided, so it can be analyzed in that context. The validation applies to the core setup, which is sound.\n\n### Step 3: Verdict and Action\n\nThe problem statement is **valid**. I will proceed with deriving the solution.\n\n### Solution Derivation\n\nTo determine if two passes $p$ and $q$ commute, we must check if for any IR $I$, the application $(p \\circ q)(I)$ results in an IR that is equivalent to $(q \\circ p)(I)$ under the given definition of $\\equiv$. This equivalence requires that the resulting CFGs are isomorphic and that the multisets of instructions in corresponding basic blocks are identical, allowing for different SSA variable names (alpha-renaming) and basic block labels.\n\nLet's analyze the pairs in each option.\n\n**A. The pairs $\\{$(LCF, UCE), (BRL, VR)$\\}$**\n\n- **Pair ($LCF$, $UCE$)**:\n    - $LCF$ modifies instructions within basic blocks but does not alter the CFG structure (edges or blocks).\n    - $UCE$ removes blocks and edges from the CFG but does not alter instructions within the reachable blocks.\n    - Let's analyze the compositions. Let $I_r$ be the set of reachable blocks and instructions in $I$, and $I_u$ be the unreachable part.\n    - $(UCE \\circ LCF)(I)$: $LCF$ acts on $I$, producing $LCF(I)$. Since $LCF$ does not change the CFG, the set of reachable blocks in $LCF(I)$ is the same as in $I$. $UCE$ then removes the unreachable portion, resulting in an IR containing only the reachable blocks, with instructions modified by $LCF$. The result is effectively $LCF(I_r)$.\n    - $(LCF \\circ UCE)(I)$: $UCE$ acts on $I$, removing $I_u$ to produce $I_r$. $LCF$ then acts on $I_r$, producing $LCF(I_r)$.\n    - Since $(UCE \\circ LCF)(I)$ and $(LCF \\circ UCE)(I)$ both produce $LCF(I_r)$, the resulting IRs are not just equivalent but identical. Thus, $LCF$ and $UCE$ commute.\n\n- **Pair ($BRL$, $VR$)**:\n    - $BRL$ modifies basic block identifiers and layout.\n    - $VR$ modifies SSA virtual register names.\n    - These passes operate on completely disjoint aspects of the IR. $BRL$ affects CFG node metadata, while $VR$ affects instruction-level operand/definition names. The action of one does not change the set of items the other acts upon.\n    - $(VR \\circ BRL)(I)$: First, relabel blocks. Second, rename registers.\n    - $(BRL \\circ VR)(I)$: First, rename registers. Second, relabel blocks.\n    - The final IR in both cases has the same CFG structure, the same instructions, but with relabeled blocks and renamed registers. The final result is the same regardless of order. Thus, $BRL$ and $VR$ commute.\n\n- **Verdict on A**: Both pairs commute. **Correct**.\n\n\n**B. The pair $\\{$(CP, Dead Branch Elimination)$\\}$**\n\n- **Pair ($CP$, $DBE$)**:\n    - $CP$ propagates constants, which can change a variable in a conditional branch's condition to a literal constant.\n    - $DBE$ is defined as simplifying a conditional branch whose condition is a literal, and then removing the newly unreachable code. This means $CP$ can create opportunities for $DBE$.\n    - Let's construct a counterexample. Consider the IR $I$:\n    ```\n    B_1: x = true\n         goto B_2\n    B_2: if (x) goto B_3 else goto B_4\n    ```\n    - $(DBE \\circ CP)(I)$:\n        1. $CP(I)$: Propagates $x = \\text{true}$. The branch in $B_2$ becomes `if (true) goto B_3 else goto B_4`.\n        2. $DBE$ acts on this modified IR. It simplifies the branch to an unconditional `goto B_3` and deletes the now-unreachable block $B_4$ and its associated edge. The final CFG does not contain $B_4$.\n    - $(CP \\circ DBE)(I)$:\n        1. $DBE(I)$: The condition $x$ in the branch `if (x)` is not a literal constant. $DBE$ does nothing. The IR is unchanged.\n        2. $CP$ acts on the original IR. It propagates $x = \\text{true}$, so the branch becomes `if (true) goto B_3 else goto B_4`. Block $B_4$ and the edge to it remain in the CFG.\n    - The resulting CFGs are not isomorphic, as one contains block $B_4$ and the other does not. Therefore, $(DBE \\circ CP)(I) \\not\\equiv (CP \\circ DBE)(I)$. They do not commute.\n\n- **Verdict on B**: The pair does not commute. **Incorrect**.\n\n\n**C. The pairs $\\{$(LCF, CP), (UCE, Dead Branch Elimination)$\\}$**\n\n- **Pair ($LCF$, $CP$)**:\n    - $LCF$ can create new constant definitions, which $CP$ can then propagate. This suggests a dependency.\n    - Consider the IR $I$ in a single basic block:\n    ```\n    x = 2 + 3\n    y = x\n    ```\n    - $(CP \\circ LCF)(I)$:\n        1. $LCF(I)$: The instruction `x = 2 + 3` is folded into `x = 5`. The IR becomes `x = 5; y = x`.\n        2. $CP$ acts on this. The definition of $x$ is the literal $5$, so it propagates this to the use in `y = x`. The instruction becomes `y = 5`. The final instruction multiset is $\\{\\text{`x = 5`}, \\text{`y = 5`}\\}$.\n    - $(LCF \\circ CP)(I)$:\n        1. $CP(I)$: The definition of $x$ is an expression, not a literal. $CP$ does nothing. The IR is unchanged.\n        2. $LCF$ acts on this. It folds `x = 2 + 3` into `x = 5`. The instruction `y = x` is not affected. The final instruction multiset is $\\{\\text{`x = 5`}, \\text{`y = x`}\\}$.\n    - The instruction multisets are different. According to the strict definition of equivalence $\\equiv$, which requires preserving the instruction multiset, the results are not equivalent. Thus, $LCF$ and $CP$ do not commute.\n\n- **Verdict on C**: Since the first pair does not commute, the option is incorrect. We need not analyze the second pair. **Incorrect**.\n\n\n**D. The pairs $\\{$(UCE, BRL), (LCF, BRL)$\\}$**\n\n- **Pair ($UCE$, $BRL$)**:\n    - $UCE$ depends on CFG connectivity. $BRL$ changes block labels but explicitly \"does not change CFG edges\".\n    - $(BRL \\circ UCE)(I)$: $UCE$ removes unreachable blocks, resulting in a subgraph $I_r$. $BRL$ then relabels the blocks in $I_r$.\n    - $(UCE \\circ BRL)(I)$: $BRL$ first relabels all blocks in $I$, creating an isomorphic graph $I'$. $UCE$ then operates on $I'$. Since reachability is a graph-structural property, $UCE$ will remove the blocks in $I'$ that correspond to the unreachable blocks in $I$. The resulting graph is isomorphic to $I_r$, just with a different labeling.\n    - Since equivalence $\\equiv$ is defined modulo relabeling of basic blocks, the two results are equivalent. Thus, $UCE$ and $BRL$ commute.\n\n- **Pair ($LCF$, $BRL$)**:\n    - $LCF$ modifies instructions inside blocks. $BRL$ modifies the labels of blocks. These are orthogonal operations. $LCF$'s operation does not depend on block labels, and $BRL$'s operation does not depend on the content of instructions.\n    - $(BRL \\circ LCF)(I)$: Fold constants, then relabel blocks.\n    - $(LCF \\circ BRL)(I)$: Relabel blocks, then fold constants.\n    - The final instruction multisets within corresponding blocks will be identical. The final CFG structures will be identical. The only difference could be the block labels, which is permitted by the equivalence relation $\\equiv$. Thus, $LCF$ and $BRL$ commute.\n\n- **Verdict on D**: Both pairs commute. **Correct**.\n\n\n**E. The pairs $\\{$(VR, LCF), (VR, UCE)$\\}$**\n\n- **Pair ($VR$, $LCF$)**:\n    - $VR$ renames SSA registers.\n    - $LCF$ acts on instructions whose operands are \"all literal constants\". SSA registers are not literal constants, even if they hold a constant value. Therefore, the operations of $VR$ and $LCF$ are disjoint. $VR$ reads/writes register names, while $LCF$ reads/writes instructions defined on literals.\n    - $(LCF \\circ VR)(I)$: Rename registers first. $LCF$ then runs and is unaffected by the renaming.\n    - $(VR \\circ LCF)(I)$: $LCF$ runs first. $VR$ then renames registers, which is unaffected by the folding LCF may have performed.\n    - The resulting IRs are equivalent modulo alpha-renaming. They commute.\n\n- **Pair ($VR$, $UCE$)**:\n    - $VR$ renames registers. It does not change the CFG.\n    - $UCE$ removes unreachable blocks.\n    - $(UCE \\circ VR)(I)$: $VR$ renames registers throughout the program $I$, producing $I_v$. $UCE$ then runs on $I_v$. As the CFG was not changed by $VR$, $UCE$ removes the same set of blocks as it would from $I$.\n    - $(VR \\circ UCE)(I)$: $UCE$ first removes unreachable blocks from $I$, producing $I_r$. $VR$ then renames the registers in the remaining reachable code $I_r$.\n    - The result of $(UCE \\circ VR)(I)$ is the reachable subgraph with renamed registers. The result of $(VR \\circ UCE)(I)$ is the same reachable subgraph, with its registers also renamed. The two final IRs are equivalent under alpha-renaming. Thus, $VR$ and $UCE$ commute.\n\n- **Verdict on E**: Both pairs commute. **Correct**.", "answer": "$$\\boxed{ADE}$$", "id": "3629191"}, {"introduction": "The ultimate goal of many compilation passes is to generate code that runs faster on specific hardware. This exercise directly connects the abstract concept of pass organization to the concrete reality of hardware performance, specifically cache memory utilization. You will analyze how two different orderings of loop tiling and interchange transformations affect the memory access patterns of a matrix multiplication kernel [@problem_id:3629231]. By calculating the resulting number of cache misses for each sequence, you will see firsthand how strategic pass ordering is a powerful tool for bridging the gap between high-level code and low-level architectural efficiency.", "problem": "Consider the triply nested loop that computes matrix multiplication accumulation over a square domain of size $N \\times N$ with row-major layout and elements of size $8$ bytes (double precision). The abstract computation is\n$$\n\\text{for } i = 0,\\ldots,N-1 \\quad \\text{for } j = 0,\\ldots,N-1 \\quad \\text{for } k = 0,\\ldots,N-1: \\quad C[i,j] \\leftarrow C[i,j] + A[i,k] \\cdot B[k,j].\n$$\nAssume the memory system is modeled by a single-level cache model $M$ with the following properties:\n- The cache is fully associative with Least Recently Used (LRU) replacement and has capacity $Q$ cache lines.\n- The cache line size is $L$ bytes, so that each cache line holds $w = \\frac{L}{8}$ double-precision elements.\n- The cache uses write-allocate and write-back, but for the purpose of this problem you are to count only cache line fills from memory (reads of complete lines into the cache), and you may ignore write-backs.\n- All arrays $A$, $B$, and $C$ are stored in contiguous row-major order, and the cache is initially empty.\n\nTwo compilation pass organizations are applied to the original loop nest:\n\n- Pass organization $\\mathcal{P}_1$ (tiling then interchange): First apply loop tiling to the $i$ and $j$ loops with tile sizes $T_i$ and $T_j$, creating tile loops over $I$ and $J$ and point loops over $ii$ and $jj$. Then apply loop interchange so that the order inside each $(I,J)$ tile is $ii$, $jj$, $k$. Under $M$, assume that within a fixed $(I,J)$ tile, all cache lines of the corresponding $C$ tile remain resident in the cache throughout the entire traversal of the $k$ loop, i.e., the working set pressure from $A$ and $B$ never evicts any $C$ cache line during the processing of that tile.\n\n- Pass organization $\\mathcal{P}_2$ (interchange then tiling): First apply loop interchange to make the $k$ loop outermost, resulting in the order $k$, $i$, $j$. Then apply loop tiling to the inner $i$ and $j$ loops with the same tile sizes $T_i$ and $T_j$, producing the order $k$, $I$, $J$, $ii$, $jj$. Under $M$, assume that between two successive visits to the same $(I,J)$ tile across different $k$ iterations, the working set due to $A$ and $B$ exceeds the remaining cache capacity so that all cache lines of the $C$ tile are evicted before the next visit; that is, when the same $(I,J)$ tile of $C$ is reached at a new value of $k$, none of its cache lines are present in the cache.\n\nTake the specific parameters $N=256$, $T_i=32$, $T_j=32$, $L=64$ bytes, and $Q=160$ cache lines. Using model $M$ and the assumptions stated above, compute the exact ratio\n$$\nR \\;=\\; \\frac{\\text{total number of cache line fills from memory for } C \\text{ under } \\mathcal{P}_2}{\\text{total number of cache line fills from memory for } C \\text{ under } \\mathcal{P}_1}.\n$$\nExpress your answer as a single exact integer. No rounding is required and no physical units are to be included.", "solution": "The problem has been validated as scientifically grounded, well-posed, and objective. It is based on standard principles of compiler theory and computer architecture concerning cache performance optimization. The provided assumptions, while simplifying, are explicit and allow for a direct, formal analysis. I will now proceed with the solution.\n\nLet us begin by formally defining the parameters provided in the problem statement.\nThe matrix dimension is $N = 256$.\nThe element size is $8$ bytes (double precision).\nThe cache line size is $L = 64$ bytes.\nThe number of double-precision elements that fit into a single cache line is $w = \\frac{L}{8} = \\frac{64}{8} = 8$.\nThe tile sizes for the $i$ and $j$ loops are $T_i = 32$ and $T_j = 32$.\nThe matrices $A$, $B$, and $C$ are stored in contiguous row-major order. We are to count only cache line fills for matrix $C$.\n\nThe analysis will be performed for each pass organization separately.\n\n### Analysis of Pass Organization $\\mathcal{P}_1$\n\nUnder pass organization $\\mathcal{P}_1$, the loop nest is tiled on loops $i$ and $j$, and the resulting loop order is specified as having the $k$ loop innermost within the tile loops. The loop structure is as follows:\n$$\n\\begin{array}{l}\n\\text{for } I = 0 \\text{ to } N/T_i - 1 \\\\\n\\quad \\text{for } J = 0 \\text{ to } N/T_j - 1 \\\\\n\\quad \\quad \\text{for } ii = 0 \\text{ to } T_i - 1 \\\\\n\\quad \\quad \\quad \\text{for } jj = 0 \\text{ to } T_j - 1 \\\\\n\\quad \\quad \\quad \\quad \\text{for } k = 0 \\text{ to } N - 1 \\\\\n\\quad \\quad \\quad \\quad \\quad i \\leftarrow I \\cdot T_i + ii \\\\\n\\quad \\quad \\quad \\quad \\quad j \\leftarrow J \\cdot T_j + jj \\\\\n\\quad \\quad \\quad \\quad \\quad C[i,j] \\leftarrow C[i,j] + A[i,k] \\cdot B[k,j]\n\\end{array}\n$$\nThe problem states the crucial assumption for this pass: \"within a fixed $(I,J)$ tile, all cache lines of the corresponding $C$ tile remain resident in the cache throughout the entire traversal of the $k$ loop\". This is interpreted to mean that for the entire duration of the loops over $ii$, $jj$, and $k$ for a given $(I,J)$ tile block, any cache line belonging to that $C$ tile, once loaded, is not evicted. Consequently, cache fills for matrix $C$ only occur upon first access to each cache line within an $(I,J)$ tile block. Once the loops move to a new $(I,J)$ tile, we assume the previous $C$ tile's lines might have been evicted, and the new $C$ tile must be loaded.\n\nOur task is to calculate the number of cache line fills for matrix $C$ required to process one $(I,J)$ tile, and then multiply this by the total number of tiles.\n\nA tile of matrix $C$ is a submatrix of size $T_i \\times T_j$. Since $C$ is stored in row-major order, an element $C[i,j]$ is located at a memory offset proportional to $i \\cdot N + j$. A tile of $C$ is thus composed of $T_i$ separate, non-contiguous row segments. Each segment corresponds to $T_j$ consecutive elements in a row of the full matrix.\n\nLet's calculate the number of cache fills for one such row segment. Each segment has $T_j$ elements. Since elements within a row are contiguous in memory, the number of cache lines required to hold these $T_j$ elements is $\\lceil \\frac{T_j}{w} \\rceil$.\nWith the given parameters, this is $\\lceil \\frac{32}{8} \\rceil = \\lceil 4 \\rceil = 4$ cache lines per row segment.\n\nThe tile consists of $T_i$ such row segments. Because these segments are from different rows of the matrix (rows $I \\cdot T_i$ through $I \\cdot T_i + T_i - 1$), they are not contiguous in memory and must be fetched independently. The total number of cache fills for one $C$ tile is the sum of fills for each of its $T_i$ row segments.\n$$\n\\text{Fills per C tile} = T_i \\times \\left\\lceil \\frac{T_j}{w} \\right\\rceil = 32 \\times 4 = 128.\n$$\nThe total number of $(I,J)$ tiles is the product of the number of tiles in each dimension:\n$$\n\\text{Number of tiles} = \\left(\\frac{N}{T_i}\\right) \\times \\left(\\frac{N}{T_j}\\right) = \\left(\\frac{256}{32}\\right) \\times \\left(\\frac{256}{32}\\right) = 8 \\times 8 = 64.\n$$\nThe total number of cache line fills for matrix $C$ under $\\mathcal{P}_1$ is the product of the fills per tile and the number of tiles:\n$$\nF_{\\mathcal{P}_1} = (\\text{Number of tiles}) \\times (\\text{Fills per C tile}) = 64 \\times 128 = 8192.\n$$\nAlternatively, we can express this symbolically. Since $T_j$ is a multiple of $w$, the ceiling function can be dropped.\n$$\nF_{\\mathcal{P}_1} = \\left(\\frac{N}{T_i}\\right) \\left(\\frac{N}{T_j}\\right) T_i \\left(\\frac{T_j}{w}\\right) = \\frac{N^2 T_i T_j}{T_i T_j w} = \\frac{N^2}{w}.\n$$\n$$\nF_{\\mathcal{P}_1} = \\frac{256^2}{8} = \\frac{65536}{8} = 8192.\n$$\n\n### Analysis of Pass Organization $\\mathcal{P}_2$\n\nUnder pass organization $\\mathcal{P}_2$, the $k$ loop is outermost, and the $i, j$ loops are tiled. The loop structure is:\n$$\n\\begin{array}{l}\n\\text{for } k = 0 \\text{ to } N - 1 \\\\\n\\quad \\text{for } I = 0 \\text{ to } N/T_i - 1 \\\\\n\\quad \\quad \\text{for } J = 0 \\text{ to } N/T_j - 1 \\\\\n\\quad \\quad \\quad \\text{for } ii = 0 \\text{ to } T_i - 1 \\\\\n\\quad \\quad \\quad \\quad \\text{for } jj = 0 \\text{ to } T_j - 1 \\\\\n\\quad \\quad \\quad \\quad \\quad i \\leftarrow I \\cdot T_i + ii \\\\\n\\quad \\quad \\quad \\quad \\quad j \\leftarrow J \\cdot T_j + jj \\\\\n\\quad \\quad \\quad \\quad \\quad C[i,j] \\leftarrow C[i,j] + A[i,k] \\cdot B[k,j]\n\\end{array}\n$$\nFor this pass, the assumption is: \"between two successive visits to the same $(I,J)$ tile across different $k$ iterations, the working set due to $A$ and $B$ exceeds the remaining cache capacity so that all cache lines of the $C$ tile are evicted\". This means that at the beginning of each iteration of the outermost $k$ loop, the cache is effectively empty with respect to matrix $C$. Therefore, as the inner loops traverse all the tiles of $C$, the entire matrix $C$ must be read into the cache from memory.\n\nLet's calculate the number of cache fills for reading the entire matrix $C$ once. The matrix $C$ is of size $N \\times N$ and is stored in a contiguous block of memory.\nTotal number of elements in $C$ is $N^2 = 256^2 = 65536$.\nThe total number of cache lines required to hold the entire matrix is:\n$$\n\\text{Fills for C per k-iteration} = \\left\\lceil \\frac{N^2}{w} \\right\\rceil = \\left\\lceil \\frac{256^2}{8} \\right\\rceil = \\lceil 8192 \\rceil = 8192.\n$$\nThis process of loading the entire matrix $C$ occurs for each iteration of the outer $k$ loop. The $k$ loop runs from $k=0$ to $N-1$, for a total of $N$ iterations.\nSo, the total number of cache line fills for matrix $C$ under $\\mathcal{P}_2$ is:\n$$\nF_{\\mathcal{P}_2} = N \\times (\\text{Fills for C per k-iteration}) = N \\times \\left\\lceil \\frac{N^2}{w} \\right\\rceil.\n$$\nSince $N^2$ is divisible by $w$, the ceiling function can be dropped.\n$$\nF_{\\mathcal{P}_2} = N \\times \\frac{N^2}{w} = \\frac{N^3}{w}.\n$$\n$$\nF_{\\mathcal{P}_2} = \\frac{256^3}{8} = \\frac{16777216}{8} = 2097152.\n$$\n\n### Calculation of the Ratio\n\nFinally, we compute the ratio $R$ as requested:\n$$\nR = \\frac{F_{\\mathcal{P}_2}}{F_{\\mathcal{P}_1}} = \\frac{2097152}{8192}.\n$$\nUsing the symbolic expressions:\n$$\nR = \\frac{N^3/w}{N^2/w} = N.\n$$\nSubstituting the value of $N$:\n$$\nR = 256.\n$$\nThe ratio of cache line fills is exactly $256$.", "answer": "$$\\boxed{256}$$", "id": "3629231"}]}