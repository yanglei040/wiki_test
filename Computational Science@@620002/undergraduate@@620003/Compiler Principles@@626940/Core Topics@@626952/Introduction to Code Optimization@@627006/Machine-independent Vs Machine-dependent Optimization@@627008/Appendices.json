{"hands_on_practices": [{"introduction": "A cornerstone of modern compiler design is the separation of concerns: machine-independent optimizations should operate on a simple, canonical Intermediate Representation (IR), while the backend handles target-specific code generation. This practice challenges you to apply this principle to address calculations, weighing the benefits of a uniform IR against the allure of complex, target-specific instructions like x86's `LEA`. Your task is to determine the most robust and portable optimization strategy. [@problem_id:3656833]", "problem": "Consider an Intermediate Representation (IR) pass that canonicalizes arithmetic expressions involving base-plus-index addressing with a constant scale and a constant offset. Let the IR use pure functions $\\operatorname{add}$, $\\operatorname{mul}$, and $\\operatorname{shl}$ over integer-typed temporaries, with semantics defined by standard integer arithmetic. Two algebraically equivalent IR patterns for computing the same address-like value are:\n\nPattern $\\mathcal{P}$:\n- $t_1 = \\operatorname{mul}(i, 4)$\n- $t_2 = \\operatorname{add}(b, t_1)$\n- $a = \\operatorname{add}(t_2, k)$\n\nPattern $\\mathcal{Q}$:\n- $t_1 = \\operatorname{add}(b, k)$\n- $a = \\operatorname{add}(t_1, \\operatorname{shl}(i, 2))$\n\nBoth compute $a = b + 4 \\cdot i + k$ under standard integer arithmetic, assuming no overflow-sensitive semantics beyond that of the target integer type. In a backend for the family of Complex Instruction Set Computer architectures commonly referred to as $\\text{x86}$, there exists a Load Effective Address (LEA) instruction that can form $b + (i \\ll 2) + k$ in a single instruction when $k$ fits a displacement field. In a backend for Advanced RISC Machines (ARM), typical Arithmetic Logic Unit forms allow $\\operatorname{add}$ with a shifted register (e.g., $b + (i \\ll 2)$) but often require a separate instruction to add a nonzero immediate $k$, leading to two instructions for $b + (i \\ll 2) + k$ when computed into a general register.\n\nA machine-independent optimization pass encounters both $\\mathcal{P}$ and $\\mathcal{Q}$ during canonicalization. It must choose a policy that respects the separation of concerns between machine-independent optimization and machine-dependent lowering and instruction selection, while allowing machine-dependent phases to exploit LEA-like operations when available and avoiding target-specific bias in IR.\n\nWhich policy should the machine-independent pass adopt?\n\nA. Normalize to a target-agnostic canonical form that decomposes expressions using simple binary operations (e.g., separate $\\operatorname{shl}$ and $\\operatorname{add}$), relying on machine-dependent instruction selection to re-fuse into LEA-like instructions on targets that support them, even if $\\text{x86}$ can do it in $1$ instruction and $\\text{ARM}$ might require $2$.\n\nB. Preserve a fused, triadic IR operator that directly models $b + (i \\ll 2) + k$ specifically to enable $\\text{x86}$ LEA selection, accepting that this may disadvantage targets that cannot fuse the immediate $k$ and shifted index in a single instruction.\n\nC. Choose between normalization and preservation based on observed instruction-count differences ($1$ versus $2$) during machine-independent optimization, emitting target-specific IR encodings when $\\text{x86}$ is the compilation target and a different encoding when $\\text{ARM}$ is the target.\n\nD. Duplicate both $\\mathcal{P}$ and $\\mathcal{Q}$ in IR to give the backend multiple choices, ignoring the potential increase in register pressure and lost opportunities for algebraic simplification and common subexpression elimination in machine-independent phases.\n\nSelect the single best option.", "solution": "The analysis of this problem hinges on a foundational principle of compiler architecture: the separation of concerns between machine-independent and machine-dependent phases. The Intermediate Representation ($\\mathcal{IR}$) serves as the interface between these phases. A machine-independent optimization pass must, by definition, operate on the $\\mathcal{IR}$ without knowledge of the target architecture's specific instruction set, register files, or performance characteristics. Its goal is to transform the code into a generally \"better\" and canonical form. The machine-dependent backend is then responsible for translating this optimized, target-agnostic $\\mathcal{IR}$ into an efficient sequence of machine instructions for a specific target.\n\nThe problem presents two algebraically equivalent $\\mathcal{IR}$ patterns, $\\mathcal{P}$ and $\\mathcal{Q}$, for the computation $a = b + 4 \\cdot i + k$.\nPattern $\\mathcal{P}$:\n$$t_1 = \\operatorname{mul}(i, 4)$$\n$$t_2 = \\operatorname{add}(b, t_1)$$\n$$a = \\operatorname{add}(t_2, k)$$\nThis corresponds to the expression tree $(b + (i \\cdot 4)) + k$.\n\nPattern $\\mathcal{Q}$:\n$$t_1 = \\operatorname{add}(b, k)$$\n$$a = \\operatorname{add}(t_1, \\operatorname{shl}(i, 2))$$\nThis corresponds to the expression tree $(b + k) + (i \\ll 2)$.\n\nA machine-independent canonicalization pass must choose a single, standard representation for this computation. The choice of policy must respect the principle of separating concerns. The optimal policy is to normalize the expression into a simple, decomposed form built from primitive operations and then rely on the machine-dependent backend to perform instruction selection via pattern matching to generate optimal code. For instance, a multiplication by a constant power of two, such as $\\operatorname{mul}(i, 4)$, is typically canonicalized to a bit-shift, $\\operatorname{shl}(i, 2)$, as shifts are generally faster. A further canonicalization step might reorder additions based on associativity and commutativity to group variables or constants, for example, resulting in a form like $\\operatorname{add}(\\operatorname{add}(b, \\operatorname{shl}(i, 2)), k)$. This decomposed structure allows the backend to work effectively. On an $\\text{x86}$ target, a pattern matcher in the instruction selector can recognize this entire tree structure and map it to a single Load Effective Address ($\\text{LEA}$) instruction. On an $\\text{ARM}$ target, the same instruction selector would match the sub-tree $\\operatorname{add}(b, \\operatorname{shl}(i, 2))$ to an $\\operatorname{add}$-with-shift instruction and the outer $\\operatorname{add}$ to a second instruction. This division of labor is efficient and modular.\n\nWith this governing principle established, we evaluate each option.\n\nA. Normalize to a target-agnostic canonical form that decomposes expressions using simple binary operations (e.g., separate $\\operatorname{shl}$ and $\\operatorname{add}$), relying on machine-dependent instruction selection to re-fuse into LEA-like instructions on targets that support them, even if $\\text{x86}$ can do it in $1$ instruction and $\\text{ARM}$ might require $2$.\nThis policy correctly adheres to the principle of separation of concerns. The machine-independent pass creates a simple, canonical $\\mathcal{IR}$ without any bias towards a particular target. It is the responsibility of the machine-dependent backend to perform instruction selection. This process, often implemented with tree-pattern matching, can \"fuse\" a sequence of simple $\\mathcal{IR}$ operations into a single, complex machine instruction (like $\\text{LEA}$) if the target architecture supports it. This approach maintains a clean, modular compiler design where target-specific knowledge is encapsulated entirely within the backend.\n**Verdict: Correct.**\n\nB. Preserve a fused, triadic IR operator that directly models $b + (i \\ll 2) + k$ specifically to enable $\\text{x86}$ LEA selection, accepting that this may disadvantage targets that cannot fuse the immediate $k$ and shifted index in a single instruction.\nThis policy is flawed because it violates the principle of machine independence. By introducing a complex, fused operator into the $\\mathcal{IR}$ that directly models a feature of a specific architecture ($\\text{x86}$), the $\\mathcal{IR}$ is no longer target-agnostic. This complicates the backend for targets like $\\text{ARM}$ that do not have such an instruction, as they would be forced to \"legalize\" or decompose this complex operation back into simpler ones. This runs counter to the standard compiler design philosophy of moving from abstract representations to concrete machine instructions.\n**Verdict: Incorrect.**\n\nC. Choose between normalization and preservation based on observed instruction-count differences ($1$ versus $2$) during machine-independent optimization, emitting target-specific IR encodings when $\\text{x86}$ is the compilation target and a different encoding when $\\text{ARM}$ is the target.\nThis policy fundamentally misunderstands the role of a *machine-independent* pass. If the pass makes decisions based on target-specific information (like instruction counts or available opcodes) and emits different $\\mathcal{IR}$ for different targets, it is, by definition, a *machine-dependent* pass. This approach conflates the roles of the middle-end and backend, leading to a monolithic and less maintainable compiler architecture. The purpose of the phase separation is precisely to avoid having target-specific logic scattered throughout the compiler.\n**Verdict: Incorrect.**\n\nD. Duplicate both $\\mathcal{P}$ and $\\mathcal{Q}$ in IR to give the backend multiple choices, ignoring the potential increase in register pressure and lost opportunities for algebraic simplification and common subexpression elimination in machine-independent phases.\nThis policy is antithetical to the purpose of optimization and canonicalization. The goal of a canonicalizer is to reduce semantically equivalent but syntactically different expressions to a single, standard form. Doing so is critical for the effectiveness of other optimizations, most notably Common Subexpression Elimination (CSE). If two identical computations are represented by different $\\mathcal{IR}$ trees ($\\mathcal{P}$ and $\\mathcal{Q}$), a CSE pass would fail to identify them as redundant. Furthermore, duplicating code or expressions is a poor strategy that increases code size, register pressure, and compile time without a clear benefit, as a competent instruction selector can generate optimal code from a single canonical form.\n**Verdict: Incorrect.**\n\nBased on the principles of compiler design, Option A is the only sound policy. It maintains a clean separation of concerns, enabling a modular and extensible compiler architecture where machine-independent and machine-dependent tasks are correctly assigned.", "answer": "$$\\boxed{A}$$", "id": "3656833"}, {"introduction": "While algebraic identities seem like a reliable source for optimization, the world of floating-point arithmetic is notoriously tricky and does not always obey the rules of real numbers. This exercise presents a scenario where a seemingly valid simplification is incorrect under strict IEEE 754 semantics, forcing you to consider the critical role of \"fast-math\" flags. You will explore how compilers must carefully navigate the line between performance and correctness, a crucial skill in numerical computing. [@problem_id:3656736]", "problem": "A compiler is optimizing a floating-point expression in a language that adopts Institute of Electrical and Electronics Engineers (IEEE) 754 semantics by default. Consider the Intermediate Representation (IR) of the expression $$E = (a + b) - a$$ evaluated in IEEE 754 binary64 semantics, where $a$ and $b$ are real-valued program variables of type double-precision floating point. Suppose the front end does not request any relaxed floating-point behavior. A machine-independent optimization pass considers applying the algebraic simplification based on real-number identities, rewriting $$E' = b.$$ Assume the following concrete values to reason about numerical behavior: $$a = 2^{1000}, \\quad b = 2^{-1074}.$$ Recall that denormal (also called subnormal) numbers are those with magnitude between $0$ and the smallest positive normal, and that some processors run with flush-to-zero (FTZ) mode enabled by default, treating all subnormal inputs as $0$ and producing $0$ for subnormal results. Two target machines are considered: machine $M$ runs with FTZ enabled by default, and machine $M'$ enforces strict IEEE 754 semantics with subnormals preserved. The compiler has machine-independent optimization passes (target-agnostic) and machine-dependent passes (target-aware, including instruction selection and target feature configuration). The Intermediate Representation (IR) can attach per-operation relaxed floating-point (\"fast-math\") flags that explicitly permit transformations that may violate strict IEEE 754 semantics, such as reassociation, disregarding signed zero, or ignoring subnormal behavior. Using only foundational facts about IEEE 754 binary64 rounding, denormals, and semantics preservation, answer the following. Select all options that are correct.\n\nA. It is safe for a machine-independent pass to rewrite $(a + b) - a$ to $b$ whenever it can prove $b$ is subnormal, because under any target either FTZ will make the difference zero or the identity holds over the real numbers.\n\nB. The rewrite must be gated by per-operation relaxed floating-point (fast-math) flags in the Intermediate Representation; under strict IEEE 754, $(a + b) - a$ can evaluate to $0$ while $b$ is a subnormal nonzero, so the transformation is not semantics-preserving. Relying on the default FTZ mode of some targets is not sufficient justification at the machine-independent level.\n\nC. The machine-dependent backend for $M$ may freely enable FTZ and reinterpret all subnormals as zero even when the IR operations are marked strict (no fast-math), because the hardware default determines the observable behavior.\n\nD. A portable strategy is to keep IR operations strict by default and forbid the rewrite in machine-independent passes unless the operations carry relaxed flags; when the user requests fast-math, the rewrite is permitted in the machine-independent stage, and on $M$ the machine-dependent stage may enable FTZ only when all relevant operations are marked relaxed.\n\nE. The correct placement for introducing fast-math behavior is only at the very end of code generation, after instruction selection, because only then is the target known; therefore, machine-independent passes should assume real-number algebraic identities unconditionally and defer all floating-point concerns to the backend.", "solution": "The core of the problem is to determine if the algebraic simplification $(a + b) - a \\rightarrow b$ is semantics-preserving for floating-point arithmetic under strict IEEE 754 rules. We will evaluate the left-hand side, $E = (a + b) - a$, using the provided values $a = 2^{1000}$ and $b = 2^{-1074}$ with IEEE 754 binary64 arithmetic.\n\nA binary64 (double-precision) floating-point number has a 53-bit significand (including the implicit leading bit). The exponent range for normal numbers is from $E_{\\text{min}} = -1022$ to $E_{\\text{max}} = 1023$. For binary64, the smallest positive normal number is $2^{-1022}$ and the smallest positive subnormal number is $2^{-1074}$. The value $a = 2^{1000}$ is a large normal number. The value $b = 2^{-1074}$ is precisely the smallest positive subnormal number.\n\n1.  **Evaluate $a + b$**:\n    The two numbers to be added are $a = 2^{1000}$ and $b = 2^{-1074}$.\n    To perform the addition, the number with the smaller exponent must be shifted to align the binary points. Here, $b$'s exponent is much smaller.\n    In binary scientific notation, $a = 1.0 \\times 2^{1000}$. Its significand is $1.000...0$ (with 52 zeros after the binary point).\n    The value of $b$ is $2^{-1074}$. To add it to $a$, we must express it with an exponent of $1000$:\n    $$b = 2^{-1074} = 2^{-1074-1000} \\times 2^{1000} = 2^{-2074} \\times 2^{1000}$$\n    So, the sum is:\n    $$a + b = (1.0 + 2^{-2074}) \\times 2^{1000}$$\n    The significand of the result is $1.0 + 2^{-2074}$. The term $2^{-2074}$ would correspond to a 1 at the $2074^{th}$ fractional bit position. However, binary64 has only 52 fractional bits of precision. The addition of $2^{-2074}$ to $1.0$ is far too small to affect the 53-bit significand. The result is rounded back to $1.0$. This phenomenon is known as absorption or swamping.\n    Therefore, the floating-point result is:\n    $$fl(a + b) = 1.0 \\times 2^{1000} = a$$\n\n2.  **Evaluate $(a + b) - a$**:\n    Using the result from the previous step, the expression becomes:\n    $$fl(fl(a+b) - a) = fl(a - a) = 0.0$$\n\n3.  **Compare with $b$**:\n    The expression $(a + b) - a$ evaluates to $0.0$ under strict IEEE 754 semantics.\n    The value of $b$ is $2^{-1074}$, which is a non-zero subnormal number.\n    Since $0.0 \\neq 2^{-1074}$, the transformation $(a + b) - a \\rightarrow b$ is **not** semantics-preserving. A machine-independent optimization pass, which must generate correct code for all targets (including strict ones like $M'$), cannot perform this rewrite unless explicitly permitted to break strict semantics.\n\n### Option-by-Option Analysis\n\n**A. It is safe for a machine-independent pass to rewrite $(a + b) - a$ to $b$ whenever it can prove $b$ is subnormal, because under any target either FTZ will make the difference zero or the identity holds over the real numbers.**\nThis statement is flawed. A machine-independent pass must be correct for *all* targets. On machine $M'$, which follows strict IEEE 754, $(a + b) - a$ evaluates to $0.0$ while $b$ is $2^{-1074}$. The transformation is incorrect on $M'$. The argument that the identity holds over real numbers is irrelevant, as floating-point arithmetic does not follow all axioms of real numbers. The argument about FTZ only applies to machine $M$, not $M'$. Therefore, the rewrite is not \"safe\" in a machine-independent context.\n**Verdict: Incorrect.**\n\n**B. The rewrite must be gated by per-operation relaxed floating-point (fast-math) flags in the Intermediate Representation; under strict IEEE 754, $(a + b) - a$ can evaluate to $0$ while $b$ is a subnormal nonzero, so the transformation is not semantics-preserving. Relying on the default FTZ mode of some targets is not sufficient justification at the machine-independent level.**\nThis statement correctly identifies the core issue. As derived above, $(a + b) - a$ evaluates to $0.0$ while $b$ is non-zero, violating semantic preservation under strict IEEE 754. A machine-independent pass must generate code that is correct for any target, including strict ones. Therefore, it cannot perform this transformation by default. Gating such transformations with \"fast-math\" flags is the standard, correct compiler engineering practice to manage the trade-off between strict correctness and performance. The flag signals that the user has permitted such potentially value-changing optimizations. The reasoning is sound.\n**Verdict: Correct.**\n\n**C. The machine-dependent backend for $M$ may freely enable FTZ and reinterpret all subnormals as zero even when the IR operations are marked strict (no fast-math), because the hardware default determines the observable behavior.**\nThis is incorrect. The purpose of semantic information in the IR (like a \"strict\" flag) is to convey the requirements of the source program to the backend. If the program requires strict IEEE 754 semantics (the default in this problem, with no relaxation requested), a correct compiler backend must configure the target hardware to match this behavior. For machine $M$, this would mean *disabling* the default FTZ mode for operations marked \"strict\". Simply following the hardware's default mode and violating the program's required semantics is a compiler bug. The language/IR semantics dictate the required behavior, not the hardware's default state.\n**Verdict: Incorrect.**\n\n**D. A portable strategy is to keep IR operations strict by default and forbid the rewrite in machine-independent passes unless the operations carry relaxed flags; when the user requests fast-math, the rewrite is permitted in the machine-independent stage, and on $M$ the machine-dependent stage may enable FTZ only when all relevant operations are marked relaxed.**\nThis statement describes a robust and correct compiler design. \n1.  Keeping IR operations strict by default respects the language's default semantics.\n2.  Forbidding the non-preserving rewrite unless a relaxed flag is present is correct for a machine-independent pass.\n3.  Permitting the rewrite when fast-math is requested is the intended use of such flags.\n4.  Having the machine-dependent backend for $M$ use the fast-math flags to decide whether to enable FTZ correctly separates concerns and respects the semantic contract given by the IR. The backend honors \"strict\" semantics and takes advantage of hardware features like FTZ for \"relaxed\" semantics. This strategy is both portable and correct.\n**Verdict: Correct.**\n\n**E. The correct placement for introducing fast-math behavior is only at the very end of code generation, after instruction selection, because only then is the target known; therefore, machine-independent passes should assume real-number algebraic identities unconditionally and defer all floating-point concerns to the backend.**\nThis statement is fundamentally flawed. If a machine-independent pass unconditionally applies real-number identities, it will perform invalid transformations like $(a + b) - a \\rightarrow b$. This transformation changes the IR. The backend cannot reverse this. A backend for a strict target like $M'$ would receive an IR instructing it to compute $b$ and would have no way of knowing the original expression was $(a + b) - a$. It would thus generate code that produces $2^{-1074}$ instead of the correct result, $0.0$. Machine-independent passes must be aware of the fundamental rules of floating-point arithmetic (even if not target-specific costs or features) to avoid generating incorrect code.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{BD}$$", "id": "3656736"}, {"introduction": "Often, an optimization is not a clear win, creating a trade-off between different performance aspects. This practice explores a classic conflict between a machine-independent goal (improving memory locality via loop fusion) and a machine-dependent one (enabling SIMD parallelism). To resolve this, you will need to think like a compiler developer and construct a quantitative cost model that balances these competing factors to make the best decision for overall performance. [@problem_id:3656816]", "problem": "A compiler must decide whether to apply loop fusion to two loops that operate on arrays of length $N$ with element size $s$ bytes. The first loop computes a temporary array, and the second loop consumes it conditionally. The program fragment is:\nLoop $\\mathcal{L}_1$: for $i = 0$ to $N-1$: $C[i] \\leftarrow A[i] + B[i]$\nLoop $\\mathcal{L}_2$: for $i = 0$ to $N-1$: if $E[i] > 0$, then $D[i] \\leftarrow \\alpha \\cdot C[i] + E[i]$, else $D[i] \\leftarrow 0$\nAssume the fraction of indices where $E[i] > 0$ is $p \\in [0,1]$. Consider a target machine with Single Instruction Multiple Data (SIMD) width $w$ lanes, a compiler that cannot vectorize loops containing data-dependent branches (no masked operations), and sustained memory bandwidth $B$ bytes per second. The machine-independent effect of loop fusion would allow the compiler to fuse $\\mathcal{L}_1$ and $\\mathcal{L}_2$ into a single loop:\nFused loop $\\mathcal{L}_F$: for $i = 0$ to $N-1$: if $E[i] > 0$, then $t \\leftarrow A[i] + B[i]$; $D[i] \\leftarrow \\alpha \\cdot t + E[i]$, else $D[i] \\leftarrow 0$\nThis fusion eliminates the temporary $C$ from memory and avoids computing $A[i] + B[i]$ when $E[i] \\le 0$. However, on the given target, introducing the branch into $\\mathcal{L}_1$ destroys SIMD vectorization for the $A[i] + B[i]$ computation, which was otherwise a straight-line loop. You may assume that scalar addition and multiplication take constant time per operation and that a vectorized addition or multiplication processes $w$ elements per instruction in approximately the same time as one scalar instruction.\nStarting from the following fundamental bases and core definitions:\n- Loop fusion: combining loops over the same iteration space to increase temporal locality and reduce memory traffic by eliminating intermediate arrays.\n- Memory traffic: total bytes loaded and stored; moving fewer bytes generally reduces runtime on memory-bound computations.\n- SIMD vectorization: executing the same operation across $w$ lanes; straight-line loops without data-dependent branches vectorize, whereas the presence of such branches prevents vectorization on the given target.\n- Roofline-style reasoning: runtime is governed by the larger of compute time and memory transfer time, i.e., $T \\approx \\max(T_{\\text{comp}}, T_{\\text{mem}})$, with $T_{\\text{mem}}$ proportional to bytes moved and $T_{\\text{comp}}$ proportional to operations divided by effective parallelism.\nConstruct the scenario quantitatively and reason about the trade-off: without fusion, $\\mathcal{L}_1$ vectorizes and $\\mathcal{L}_2$ remains scalar due to the branch; with fusion, both computations are subject to the branch and therefore scalar, but memory traffic is reduced by eliminating $C$ and skipping the $A[i]+B[i]$ computation when $E[i] \\le 0$. Which heuristic best balances the machine-independent memory savings against the machine-dependent loss of SIMD vectorization?\n\nA. Always fuse the loops, because reducing memory traffic is universally beneficial and dominates compute effects on all machines, regardless of $w$, $B$, or $p$.\n\nB. Use a roofline-style cost model to estimate separate versus fused runtimes and fuse only if the fused estimate is lower; specifically, compare $T_{\\text{sep}} \\approx \\max\\!\\left(\\frac{6sN}{B}, \\frac{N}{w}\\cdot t_{\\text{vadd}} + pN \\cdot (t_{\\text{smul}} + t_{\\text{sadd}})\\right)$ against $T_{\\text{fus}} \\approx \\max\\!\\left(\\frac{sN(2p+2)}{B}, pN \\cdot (t_{\\text{sadd}} + t_{\\text{smul}} + t_{\\text{sadd}})\\right)$ and fuse only if $T_{\\text{fus}} < T_{\\text{sep}}$, where $t_{\\text{vadd}}$ is the time of one vector add and $t_{\\text{smul}}, t_{\\text{sadd}}$ are scalar times.\n\nC. Fuse only when $p < \\frac{1}{w}$, because low activity guarantees that scalar execution is cheap relative to vector execution, making fusion preferable independently of $B$ and $s$.\n\nD. Decide based solely on register pressure: fuse when the number of simultaneously live arrays in the fused loop is less than $\\frac{w}{2}$, and do not consider memory bandwidth $B$, activity $p$, or bytes $sN$ at all.\n\nChoose the single best option.", "solution": "The goal is to find the best heuristic for deciding whether to fuse the loops. This requires a quantitative comparison of the runtime for the two scenarios: separate loops and the fused loop. We use the provided Roofline-style model, $T \\approx \\max(T_{\\text{comp}}, T_{\\text{mem}})$.\n\n**Scenario 1: Separate Loops ($T_{\\text{sep}}$)**\n\nIn this scenario, we execute $\\mathcal{L}_1$ and then $\\mathcal{L}_2$.\n-   **Loop $\\mathcal{L}_1$:** `for i = 0 to N-1: C[i] <- A[i] + B[i]`\n    -   **Memory Traffic:** Read array $A$ ($sN$ bytes), read array $B$ ($sN$ bytes), write temporary array $C$ ($sN$ bytes). Total: $3sN$ bytes.\n    -   **Compute:** $N$ additions. The loop is vectorizable. With a SIMD width of $w$, this takes approximately $\\frac{N}{w}$ vector addition instructions. Let $t_{\\text{vadd}}$ be the time for one such instruction.\n-   **Loop $\\mathcal{L}_2$:** `for i = 0 to N-1: if E[i] > 0, then D[i] <- α · C[i] + E[i], else D[i] <- 0`\n    -   **Memory Traffic:** Read array $E$ ($sN$ bytes), read array $C$ ($sN$ bytes), write array $D$ ($sN$ bytes). Total: $3sN$ bytes.\n    -   **Compute:** The loop is scalar due to the data-dependent branch. On average, the `then` clause executes for a fraction $p$ of the $N$ iterations. This involves $pN$ multiplications and $pN$ additions. Let $t_{\\text{sadd}}$ and $t_{\\text{smul}}$ be the scalar operation times.\n\n-   **Total Runtime for Separate Loops ($T_{\\text{sep}}$):**\n    -   **Total Memory Traffic ($M_{\\text{sep}}$):** The sum of traffic from both loops is $3sN + 3sN = 6sN$ bytes. The temporary array $C$ is materialized to memory and read back, accounting for $2sN$ of this traffic.\n    -   **Total Memory Time ($T_{\\text{mem,sep}}$):** $T_{\\text{mem,sep}} = \\frac{M_{\\text{sep}}}{B} = \\frac{6sN}{B}$.\n    -   **Total Compute Time ($T_{\\text{comp,sep}}$):** The sum of compute times is $T_{\\text{comp,sep}} \\approx \\frac{N}{w} t_{\\text{vadd}} + pN (t_{\\text{sadd}} + t_{\\text{smul}})$.\n    -   **Overall Runtime ($T_{\\text{sep}}$):** Applying the Roofline model to the entire sequence:\n        $$T_{\\text{sep}} \\approx \\max\\left(T_{\\text{mem,sep}}, T_{\\text{comp,sep}}\\right) = \\max\\left(\\frac{6sN}{B}, \\frac{N}{w} t_{\\text{vadd}} + pN(t_{\\text{sadd}} + t_{\\text{smul}})\\right)$$\n\n**Scenario 2: Fused Loop ($T_{\\text{fus}}$)**\n\nIn this scenario, we execute the single fused loop $\\mathcal{L}_F$.\n-   **Loop $\\mathcal{L}_F$:** `for i = 0 to N-1: if E[i] > 0, then t <- A[i] + B[i]; D[i] <- α · t + E[i], else D[i] <- 0`\n    -   The entire loop is scalar because the computation of $A[i]+B[i]$ is now inside a data-dependent branch.\n    -   **Memory Traffic ($M_{\\text{fus}}$):**\n        -   Read $E$ to check the condition: $sN$ bytes.\n        -   Read $A$ and $B$ only when $E[i]>0$: $p s N$ bytes for $A$ and $p s N$ for $B$.\n        -   Write $D$: $sN$ bytes.\n        -   The temporary array $C$ is not written to memory; the value `t` is kept in a register.\n        -   Total: $sN + psN + psN + sN = sN(2 + 2p)$ bytes.\n    -   **Compute ($T_{\\text{comp,fus}}$):**\n        -   The operations inside the `if` execute $pN$ times. These are: one addition ($t \\leftarrow A[i] + B[i]$), one multiplication ($α \\cdot t$), and one addition ($... + E[i]$).\n        -   Total compute time: $T_{\\text{comp,fus}} \\approx pN(t_{\\text{sadd}} + t_{\\text{smul}} + t_{\\text{sadd}})$.\n\n-   **Total Runtime for Fused Loop ($T_{\\text{fus}}$):**\n    -   **Total Memory Time ($T_{\\text{mem,fus}}$):** $T_{\\text{mem,fus}} = \\frac{M_{\\text{fus}}}{B} = \\frac{sN(2+2p)}{B}$.\n    -   **Total Compute Time ($T_{\\text{comp,fus}}$):** As derived above.\n    -   **Overall Runtime ($T_{\\text{fus}}$):**\n        $$T_{\\text{fus}} \\approx \\max\\left(T_{\\text{mem,fus}}, T_{\\text{comp,fus}}\\right) = \\max\\left(\\frac{sN(2+2p)}{B}, pN(2 t_{\\text{sadd}} + t_{\\text{smul}})\\right)$$\n\n**Trade-off Analysis**\n\nThe decision to fuse should be made if $T_{\\text{fus}} < T_{\\text{sep}}$.\n\n-   **Memory:** The memory traffic is always lower with fusion. Since $p \\in [0,1]$, the multiplier $(2+2p)$ is in $[2, 4]$. Thus, $M_{\\text{fus}} = sN(2+2p) \\le 4sN$, which is always less than $M_{\\text{sep}} = 6sN$. Fusion provides a clear benefit for memory-bound cases.\n-   **Compute:** The compute time may increase or decrease. The $A[i]+B[i]$ operation is de-vectorized (a loss), but it is also executed conditionally (a gain). The change in cost for this part is roughly $pN t_{\\text{sadd}} - \\frac{N}{w} t_{\\text{vadd}}$. Assuming $t_{\\text{vadd}} \\approx t_{\\text{sadd}}$, the compute time penalty is proportional to $N(p - 1/w)$. If $p > 1/w$, compute time increases with fusion; if $p < 1/w$, it decreases.\n\nA good heuristic must balance these two competing effects based on all relevant parameters: $s, B, p, w$.\n\n### Option-by-Option Analysis\n\n**A. Always fuse the loops, because reducing memory traffic is universally beneficial and dominates compute effects on all machines, regardless of $w$, $B$, or $p$.**\n\nThis option makes an absolute claim. While fusion always reduces memory traffic in this case, this benefit may not always dominate the compute penalty. Consider a compute-bound scenario with high memory bandwidth ($B \\to \\infty$) and high conditional activity ($p=1$).\n-   $T_{\\text{sep}} \\to T_{\\text{comp,sep}} \\approx \\frac{N}{w} t_{\\text{vadd}} + N(t_{\\text{sadd}} + t_{\\text{smul}})$.\n-   $T_{\\text{fus}} \\to T_{\\text{comp,fus}} \\approx N(2 t_{\\text{sadd}} + t_{\\text{smul}})$.\nIf $w > 1$ and assuming $t_{\\text{vadd}} \\approx t_{\\text{sadd}}$, then $\\frac{N}{w}t_{\\text{sadd}} < N t_{\\text{sadd}}$. This implies $T_{\\text{comp,sep}} < T_{\\text{comp,fus}}$. In a compute-bound case, not fusing is faster. The claim that memory effects always dominate is false.\n**Verdict: Incorrect.**\n\n**B. Use a roofline-style cost model to estimate separate versus fused runtimes and fuse only if the fused estimate is lower; specifically, compare $T_{\\text{sep}} \\approx \\max\\!\\left(\\frac{6sN}{B}, \\frac{N}{w}\\cdot t_{\\text{vadd}} + pN \\cdot (t_{\\text{smul}} + t_{\\text{sadd}})\\right)$ against $T_{\\text{fus}} \\approx \\max\\!\\left(\\frac{sN(2p+2)}{B}, pN \\cdot (t_{\\text{sadd}} + t_{\\text{smul}} + t_{\\text{sadd}})\\right)$ and fuse only if $T_{\\text{fus}} < T_{\\text{sep}}$, where $t_{\\text{vadd}}$ is the time of one vector add and $t_{\\text{smul}}, t_{\\text{sadd}}$ are scalar times.**\n\nThis option proposes building a quantitative cost model and making a decision based on it. Let's examine its components.\n-   The formula for $T_{\\text{sep}}$ matches our derivation perfectly, capturing the vectorized first loop, the scalar second loop, and the total memory traffic from writing and reading the intermediate array $C$.\n-   The formula for $T_{\\text{fus}}$ also matches our derivation, capturing the reduced memory traffic (no intermediate $C$, conditional reads of $A$ and $B$) and the fully scalar compute path.\nThis approach correctly formalizes the trade-off by incorporating all the specified parameters ($s, B, w, p$) into a performance model. It is the most robust and accurate method among the choices.\n**Verdict: Correct.**\n\n**C. Fuse only when $p < \\frac{1}{w}$, because low activity guarantees that scalar execution is cheap relative to vector execution, making fusion preferable independently of $B$ and $s$.**\n\nThis heuristic focuses solely on the compute part of the trade-off. As analyzed earlier, the condition $p < 1/w$ is (approximately) the condition for the fused loop to have a lower *compute* time than the separate loops. However, it completely ignores memory bandwidth. Consider a machine with very low bandwidth $B$, making the loops strongly memory-bound.\n-   $T_{\\text{sep}} \\approx T_{\\text{mem,sep}} = \\frac{6sN}{B}$.\n-   $T_{\\text{fus}} \\approx T_{\\text{mem,fus}} = \\frac{sN(2+2p)}{B}$.\nSince $sN(2+2p) < 6sN$ for all $p \\in [0,1]$, fusion will always be faster in a sufficiently memory-bound case, regardless of the values of $p$ and $w$. This heuristic would incorrectly advise against fusion if, for example, $p=0.5$ and $w=8$ (since $0.5 \\not< 1/8$), even if the memory savings provide a huge performance win. The claim that the decision is independent of $B$ and $s$ is false.\n**Verdict: Incorrect.**\n\n**D. Decide based solely on register pressure: fuse when the number of simultaneously live arrays in the fused loop is less than $\\frac{w}{2}$, and do not consider memory bandwidth $B$, activity $p$, or bytes $sN$ at all.**\n\nThis option introduces register pressure, which is a real consideration in compiler optimizations. Fusing loops increases the number of variables live at the same time, which can lead to spilling registers to memory if the demand exceeds the available registers. However, this heuristic is flawed for several reasons. First, the threshold $\\frac{w}{2}$ arbitrarily links the number of live arrays (an integer register/pointer concern) to the SIMD vector width, which is not a standard or well-justified relationship. Second, and more critically, it completely ignores the primary performance trade-off explicitly defined in the problem: memory bandwidth versus computational parallelism (vectorization). An optimal heuristic for this problem *must* consider $B$, $p$, and $w$, as they are the key drivers of performance in the given model. Ignoring them makes for a poor decision-making process.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{B}$$", "id": "3656816"}]}