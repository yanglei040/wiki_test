{"hands_on_practices": [{"introduction": "A core function of an Intermediate Representation (IR) is to enable optimizers to make intelligent, cost-based decisions. This first exercise explores a classic trade-off at the local level: whether it is more efficient to keep a temporary value in a register or to recompute it each time it's needed. By modeling the costs associated with register pressure, memory spills, and instruction execution, you will quantitatively determine the optimal strategy for a given scenario, a process known as rematerialization analysis [@problem_id:3647567].", "problem": "A compiler backend is deciding how to represent a short-lived temporary in its Intermediate Representation (IR). Intermediate Representation (IR) here uses virtual registers, and the register allocator later maps them to machine registers. Register pressure $P$ at a program point is defined as the number of simultaneously live virtual registers at that point, and if $P$ exceeds the number of available machine registers $k$, the allocator must spill at least one live value to memory, incurring extra load and store instructions. Consider the following basic block that executes many times:\n\n- There are $k = 32$ machine registers.\n- The baseline register pressure without the temporary is $p_0 = 32$ throughout the region between all uses of the temporary.\n- A temporary $\\tau = g(a,b)$ is produced and used $U = 3$ times in the block. The cost to compute $g$ is $R = 6$ cycles per computation.\n- If $\\tau$ is retained live across the block (computed once and reused), the live range of $\\tau$ increases $P$ to $33$ across the region, which forces exactly one other live value to be spilled. The spill sequence costs a store and a reload per iteration, with total cost $C_s = 10$ cycles per iteration.\n- Each memory operation (store or load) induced directly by a choice to spill $\\tau$ itself costs $M = 5$ cycles.\n- The region of high pressure spans between all uses, so any strategy that retains $\\tau$ at any point between uses triggers the same single spill of another value, costing $C_s$ for the iteration. Recomputation of $\\tau$ at a use does not extend $\\tau$’s live range beyond that use.\n\nFrom first principles, the compiler’s objective is to minimize expected execution cycles per iteration subject to the constraint that exceeding $k$ registers forces a spill, and that recomputation increases instruction count. Which strategy minimizes the expected cycles per iteration?\n\nA. Retain $\\tau$ across the block in a virtual register and reuse it at all $U$ uses, accepting the one forced spill of another value.\n\nB. Recompute $\\tau$ at each use site, so there is no live $\\tau$ between uses and thus no additional spill due to $\\tau$.\n\nC. Compute $\\tau$ once, then spill $\\tau$ itself after its first use and reload $\\tau$ from memory before each subsequent use, avoiding spilling other values.\n\nD. Compute $\\tau$ once and retain it for the first two uses (triggering one spill), then recompute $\\tau$ for the last use to shorten the final segment of its live range.", "solution": "The objective is to find the strategy that minimizes the total execution cycle cost per iteration. We will calculate this cost for each of the four proposed strategies based on the provided parameters.\n\n**Strategy A: Retain $\\tau$ in a register and accept a spill**\n\nIn this strategy, $\\tau$ is computed once and then reused.\n-   **Computation Cost**: One computation of $\\tau$ costs $R = 6$ cycles.\n-   **Spill Cost**: Keeping $\\tau$ live increases the register pressure to $33$, which is greater than the available registers $k=32$. This forces one spill of another value, incurring a fixed cost of $C_s = 10$ cycles.\n-   **Total Cost**: $C_A = R + C_s = 6 + 10 = 16$ cycles.\n\n**Strategy B: Recompute $\\tau$ at each use**\n\nIn this strategy, $\\tau$ is recomputed before each of its $U = 3$ uses. This is known as rematerialization.\n-   **Computation Cost**: The computation is performed $U=3$ times, each costing $R=6$ cycles. The total computation cost is $U \\times R$.\n-   **Spill Cost**: Since $\\tau$ is not kept live between uses, the register pressure does not increase, and no additional spills are forced. The spill cost is $0$.\n-   **Total Cost**: $C_B = U \\times R = 3 \\times 6 = 18$ cycles.\n\n**Strategy C: Compute $\\tau$ once and spill/reload $\\tau$ itself**\n\nIn this strategy, $\\tau$ is computed once, stored to memory, and then loaded back for subsequent uses.\n-   **Computation Cost**: One initial computation costs $R = 6$ cycles.\n-   **Spill/Reload Cost**: After the first use, $\\tau$ is stored to memory (1 store). Before each of the $U-1=2$ subsequent uses, it is loaded back (2 loads). The total number of memory operations is $1 + (U-1) = U = 3$. Each memory operation costs $M = 5$ cycles. The total memory cost is $U \\times M$.\n-   By spilling $\\tau$ itself, its register is freed, avoiding an increase in register pressure and the associated spill cost $C_s$.\n-   **Total Cost**: $C_C = R + U \\times M = 6 + 3 \\times 5 = 6 + 15 = 21$ cycles.\n\n**Strategy D: Hybrid retain and recompute**\n\nThis strategy mixes retaining $\\tau$ with recomputation.\n-   **Initial Computation**: $\\tau$ is computed once at the start, costing $R=6$ cycles.\n-   **Spill Cost**: It is retained for the first two uses. As stated, retaining $\\tau$ for *any* duration between uses triggers the spill of another value, costing $C_s = 10$ cycles.\n-   **Recomputation Cost**: For the final (third) use, $\\tau$ is recomputed, costing an additional $R = 6$ cycles.\n-   **Total Cost**: $C_D = (\\text{initial computation}) + (\\text{spill cost}) + (\\text{recomputation}) = R + C_s + R = 6 + 10 + 6 = 22$ cycles.\n\n**Conclusion**\n\nComparing the total costs of each strategy:\n-   Cost(A) = 16 cycles\n-   Cost(B) = 18 cycles\n-   Cost(C) = 21 cycles\n-   Cost(D) = 22 cycles\n\nThe minimum cost is 16 cycles, which corresponds to Strategy A. Therefore, retaining $\\tau$ in a register and accepting the spill of a different value is the most efficient choice in this scenario.", "answer": "$$\\boxed{A}$$", "id": "3647567"}, {"introduction": "Expanding our scope, we now consider how IR design influences program-wide features like memory safety. This practice examines the trade-off between enforcing array bounds with dynamic runtime checks versus leveraging an advanced type system to prove safety at compile time. You will construct a cost model that incorporates the subtleties of modern hardware, such as the probabilistic cost of branch misprediction, to derive a precise condition for when the static approach becomes more performant than the dynamic one [@problem_id:3647604].", "problem": "A compiler team is designing an Intermediate Representation (IR) for array bounds safety in tight loops. Two designs are under consideration for a canonical loop with $N$ iterations and a fixed useful work cost per iteration.\n\nDesign R (runtime checks): Each iteration performs $r$ bounds checks that execute as a compare plus a conditional branch. For each check, the compare has cost $c_c$ cycles, a correctly predicted branch has overhead $c_b$ cycles, and a misprediction incurs an additional penalty of $c_m$ cycles. The branch misprediction rate is $q$, with $0 \\le q \\le 1$. Assume checks cannot be fully hoisted out of the loop, so there remain $r$ checks per iteration in the steady state.\n\nDesign T (type-encoded bounds): The IR’s type system encodes array index ranges so that bounds checks are proven statically and removed from the loop. However, this requires maintaining additional range-carrying values in Single Static Assignment (SSA) form, which adds $k$ extra integer arithmetic operations per iteration, each costing $c_a$ cycles at runtime, with no branches introduced.\n\nAssume that all other effects (such as instruction cache and memory hierarchy) are equal across designs, that the useful computation cost per iteration is $C_u$ cycles in both designs, and that branch effects are independent across checks. Using basic probability and expected value definitions, decide which condition correctly characterizes when Design T minimizes average per-iteration runtime relative to Design R.\n\nChoose the option that is correct under these assumptions.\n\nA. Prefer Design T if and only if $k \\, c_a  r \\, (c_c + c_b + q \\, c_m)$; otherwise prefer Design R.\n\nB. Prefer Design T if and only if $k \\, c_a  r \\, q \\, c_m$; otherwise prefer Design R.\n\nC. Prefer Design T only when $N  \\dfrac{k \\, c_a}{r \\, (c_c + c_b + q \\, c_m)}$; otherwise prefer Design R.\n\nD. Prefer Design R whenever the compiler can hoist checks into the loop preheader, because then the checks are $O(1)$ with respect to $N$, irrespective of $r$ and $q$.\n\nE. Prefer Design R if the hardware supports predication, because then the expected per-check overhead is approximately $c_c$ and mispredictions can be ignored for all loops.", "solution": "The goal is to determine the condition under which the average per-iteration cost of Design T is less than that of Design R. Let's model the cost for each design.\n\n**1. Average Cost per Iteration for Design T ($C_T$)**\n\nThis design replaces runtime checks with compile-time proofs, but this adds a runtime overhead of $k$ extra arithmetic operations per iteration.\n-   Useful work cost: $C_u$.\n-   Overhead cost from extra arithmetic: $k$ operations, each costing $c_a$ cycles. Total overhead is $k \\cdot c_a$.\nThe total cost per iteration for Design T is:\n$$C_T = C_u + k \\, c_a$$\n\n**2. Average Cost per Iteration for Design R ($C_R$)**\n\nThis design includes the cost of $r$ runtime bounds checks per iteration. We need to find the expected cost of a single check.\n-   Each check consists of a compare operation and a conditional branch.\n-   The cost of the compare is fixed at $c_c$.\n-   The cost of the branch depends on whether it is predicted correctly.\n    -   A correct prediction occurs with probability $(1-q)$ and costs $c_b$ cycles.\n    -   A misprediction occurs with probability $q$ and costs $c_b + c_m$ cycles (base overhead plus penalty).\n-   The expected cost of one conditional branch, $E[\\text{branch}]$, is:\n    $$E[\\text{branch}] = (1-q) \\cdot c_b + q \\cdot (c_b + c_m) = c_b - q c_b + q c_b + q c_m = c_b + q c_m$$\n-   The total expected cost of a single bounds check is the sum of the compare cost and the expected branch cost:\n    $$E[\\text{check}] = c_c + E[\\text{branch}] = c_c + c_b + q c_m$$\n-   Since there are $r$ independent checks per iteration, the total expected overhead is $r \\cdot E[\\text{check}]$.\n-   The total average cost per iteration for Design R is:\n    $$C_R = C_u + r \\cdot (c_c + c_b + q c_m)$$\n\n**3. Comparison and Derivation of the Condition**\n\nWe prefer Design T when its average per-iteration cost is lower than that of Design R, i.e., when $C_T  C_R$.\n$$C_u + k \\, c_a  C_u + r \\cdot (c_c + c_b + q c_m)$$\nThe useful work cost $C_u$ cancels out from both sides:\n$$k \\, c_a  r \\cdot (c_c + c_b + q c_m)$$\nThis inequality gives the precise condition under which Design T is more performant.\n\n**4. Evaluating the Options**\n\n-   **A. Prefer Design T if and only if $k \\, c_a  r \\, (c_c + c_b + q \\, c_m)$; otherwise prefer Design R.** This exactly matches our derived condition. It correctly compares the overhead of Design T with the full expected overhead of Design R.\n-   **B. Prefer Design T if and only if $k \\, c_a  r \\, q \\, c_m$; otherwise prefer Design R.** This is incorrect as it only considers the misprediction penalty, ignoring the base costs of the compare ($c_c$) and the branch itself ($c_b$).\n-   **C. Prefer Design T only when $N  \\dots$; otherwise prefer Design R.** This is incorrect. The comparison is based on per-iteration cost, which is independent of the total number of iterations $N$.\n-   **D. Prefer Design R whenever the compiler can hoist checks...** This is incorrect because it contradicts the problem's assumption that checks cannot be fully hoisted.\n-   **E. Prefer Design R if the hardware supports predication...** This is incorrect because it introduces external information about predication that is not part of the problem's defined cost model.\n\nTherefore, option A is the only correct characterization.", "answer": "$$\\boxed{A}$$", "id": "3647604"}, {"introduction": "Finally, we zoom out to view the compiler as an entire system, where global constraints like total compile time and peak memory usage dictate design choices. Real-world compilers are multi-stage pipelines, and the transition between different IRs is a critical point of optimization. This practice challenges you to model such a pipeline as a constrained optimization problem, deciding between fast, memory-intensive algorithms and slower, memory-efficient ones to meet a global resource budget [@problem_id:3647584].", "problem": "A compiler pipeline works on three forms of Intermediate Representation (IR): High-Level Intermediate Representation (HIR), Middle-Level Intermediate Representation (MIR), and Low-Level Intermediate Representation (LIR). Each IR form occupies memory proportional to program size, and conversions between IR forms can be performed in two modes that trade peak memory for conversion speed. The pipeline runs four passes in sequence: one pass on HIR, two passes on MIR, and one pass on LIR, with conversions HIR-to-MIR and MIR-to-LIR in between.\n\nFundamental base definitions to use:\n- Intermediate Representation (IR) is a data structure encoding the program. Compile time is the sum of all pass times and conversion times. Peak memory is the maximum over time of the total memory concurrently used by IR representations and conversion machinery.\n- For a program of size $S$ IR units, the memory occupancy of IR is linear: HIR uses $c_{1} S$ bytes, MIR uses $c_{2} S$ bytes, and LIR uses $c_{3} S$ bytes. Use $1\\,\\mathrm{GB} = 10^{9}$ bytes.\n- A conversion from IR form $X$ to IR form $Y$ can be done in:\n  - Bulk mode: both $X$ and $Y$ are held concurrently during the conversion. Peak memory includes $c_{X} S + c_{Y} S$. The bulk conversion time is $\\gamma S$ seconds.\n  - Streaming mode: only one IR is fully held at a time; peak memory is $\\max(c_{X} S, c_{Y} S)$. Streaming conversion time is $\\delta \\gamma S$ seconds with slowdown factor $\\delta  1$.\n\nThe pipeline is:\n1. Run pass $P_{1}$ on HIR.\n2. Convert HIR to MIR.\n3. Run pass $P_{2}$ on MIR.\n4. Run pass $P_{3}$ on MIR.\n5. Convert MIR to LIR.\n6. Run pass $P_{4}$ on LIR.\n\nAssume sequential execution; at any time only one pass or one conversion runs. Pass times are linear in $S$: $t_{1} = \\alpha_{1} S$, $t_{2} = \\alpha_{2} S$, $t_{3} = \\alpha_{3} S$, $t_{4} = \\alpha_{4} S$. Peak memory is determined by the IR occupancy of the active stage, with conversions needing either the sum or the maximum occupancy as above.\n\nGiven the parameters:\n- Program size $S = 10^{8}$ IR units.\n- Memory densities: $c_{1} = 16$ bytes per unit, $c_{2} = 12$ bytes per unit, $c_{3} = 8$ bytes per unit.\n- Pass time coefficients: $\\alpha_{1} = 2.5 \\times 10^{-8}$ seconds per unit, $\\alpha_{2} = 3.0 \\times 10^{-8}$ seconds per unit, $\\alpha_{3} = 4.0 \\times 10^{-8}$ seconds per unit, $\\alpha_{4} = 5.0 \\times 10^{-8}$ seconds per unit.\n- Conversion coefficients: HIR-to-MIR bulk $\\gamma_{1} = 1.5 \\times 10^{-8}$ seconds per unit with streaming slowdown $\\delta_{1} = 1.8$; MIR-to-LIR bulk $\\gamma_{2} = 1.0 \\times 10^{-8}$ seconds per unit with streaming slowdown $\\delta_{2} = 1.6$.\n- Peak memory budget $M = 2.4$ GB.\n\nTask:\n1. Derive a cost model for total compile time $T$ as a function of conversion mode choices under the peak memory constraint $M$. Your model should start from the base definitions above and explicitly account for whether each conversion is bulk or streaming, based on whether $c_{X} S + c_{Y} S \\leq M \\cdot 10^{9}$ (bulk feasible) or not (then streaming must be used).\n2. Using your cost model, determine the minimal achievable total compile time $T$ for the given $M$ by choosing the appropriate conversion modes. Express the final compile time in seconds, and round your answer to four significant figures.", "solution": "The objective is to find the minimum total compile time $T$ by choosing the fastest feasible conversion mode (bulk or streaming) for each of the two conversion steps, subject to the peak memory budget $M$.\n\n**1. Formalize Cost Model and Constraints**\n\nThe total compile time $T$ is the sum of the time for the four processing passes ($T_{\\text{passes}}$) and the two conversion steps ($T_{C1}$ for HIR-to-MIR, $T_{C2}$ for MIR-to-LIR).\n$$T = T_{\\text{passes}} + T_{C1} + T_{C2}$$\nThe time for the passes is constant:\n$$T_{\\text{passes}} = (\\alpha_1 + \\alpha_2 + \\alpha_3 + \\alpha_4)S$$\nFor each conversion, we must check if the faster bulk mode is feasible under the memory constraint.\n- Memory budget: $M_{\\text{budget}} = 2.4 \\text{ GB} = 2.4 \\times 10^9$ bytes.\n- For a conversion from IR form $X$ to $Y$:\n  - If $(c_X + c_Y)S \\le M_{\\text{budget}}$, bulk mode is feasible, and we choose it to minimize time: conversion time is $\\gamma S$.\n  - Otherwise, we must use streaming mode. We must verify that streaming is feasible: $\\max(c_X S, c_Y S) \\le M_{\\text{budget}}$. If so, the conversion time is $\\delta \\gamma S$.\n\n**2. Evaluate Pipeline Stages with Given Parameters**\n\n- **Parameters**: $S = 10^8$, $c_1=16$, $c_2=12$, $c_3=8$, $\\sum \\alpha_i = 14.5 \\times 10^{-8}$, $\\gamma_1=1.5 \\times 10^{-8}, \\delta_1=1.8$, $\\gamma_2=1.0 \\times 10^{-8}, \\delta_2=1.6$.\n\n- **Pass Memory Feasibility**: We check the peak memory for each pass type.\n  - HIR Pass: $c_1 S = 16 \\times 10^8 = 1.6 \\times 10^9$ bytes. ($1.6 \\text{ GB} \\le 2.4 \\text{ GB}$, OK).\n  - MIR Pass: $c_2 S = 12 \\times 10^8 = 1.2 \\times 10^9$ bytes. ($1.2 \\text{ GB} \\le 2.4 \\text{ GB}$, OK).\n  - LIR Pass: $c_3 S = 8 \\times 10^8 = 0.8 \\times 10^9$ bytes. ($0.8 \\text{ GB} \\le 2.4 \\text{ GB}$, OK).\n  All individual pass stages are within the memory budget.\n\n- **HIR-to-MIR Conversion ($C_1$)**:\n  - Peak memory for bulk mode: $(c_1 + c_2)S = (16 + 12) \\times 10^8 = 2.8 \\times 10^9$ bytes.\n  - Since $2.8 \\times 10^9 > 2.4 \\times 10^9$, **bulk mode is not feasible**.\n  - We must fall back to streaming mode. Peak memory for streaming is $\\max(c_1, c_2)S = 16 \\times 10^8 = 1.6 \\times 10^9$ bytes. Since $1.6 \\times 10^9 \\le 2.4 \\times 10^9$, streaming is feasible.\n  - The time for this conversion is the streaming time: $T_{C1} = \\delta_1 \\gamma_1 S = 1.8 \\times (1.5 \\times 10^{-8}) \\times 10^8 = 2.7$ seconds.\n\n- **MIR-to-LIR Conversion ($C_2$)**:\n  - Peak memory for bulk mode: $(c_2 + c_3)S = (12 + 8) \\times 10^8 = 2.0 \\times 10^9$ bytes.\n  - Since $2.0 \\times 10^9 \\le 2.4 \\times 10^9$, **bulk mode is feasible**.\n  - To minimize time, we select the faster bulk mode.\n  - The time for this conversion is the bulk time: $T_{C2} = \\gamma_2 S = 1.0 \\times 10^{-8} \\times 10^8 = 1.0$ second.\n\n**3. Calculate Total Minimal Compile Time**\n\n- **Total Pass Time**: $T_{\\text{passes}} = (\\sum \\alpha_i)S = (14.5 \\times 10^{-8}) \\times 10^8 = 14.5$ seconds.\n- **Total Compile Time**: $T = T_{\\text{passes}} + T_{C1} + T_{C2} = 14.5 + 2.7 + 1.0 = 18.2$ seconds.\n\nRounding the result to four significant figures gives $18.20$.", "answer": "$$\\boxed{18.20}$$", "id": "3647584"}]}