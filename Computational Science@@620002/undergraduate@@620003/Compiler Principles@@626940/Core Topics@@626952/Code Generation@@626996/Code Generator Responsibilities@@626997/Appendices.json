{"hands_on_practices": [{"introduction": "A core task of the code generator is \"instruction selection,\" which involves translating abstract operations into concrete machine instructions. Often, there are multiple ways to implement a single high-level operation, and the optimal choice depends on the target processor's specific features. This practice [@problem_id:3628158] challenges you to act as a code generator, evaluating two different strategies for implementing a 64-bit rotate operation and determining the fastest one by carefully scheduling instructions around hardware latencies and resource limits.", "problem": "A compiler’s code generator must lower a high-level $64$-bit rotate-right by a compile-time constant amount $k$ on a $64$-bit value $x$ into target-machine operations. Consider a target Instruction Set Architecture (ISA) that represents a $64$-bit value as an ordered pair of $32$-bit registers $(x_{\\mathrm{lo}}, x_{\\mathrm{hi}})$ and supports the following instructions and machine resources:\n\n- One Shift unit $S$ that executes $64$-bit logical shifts left or right. Each $64$-bit shift has latency $\\ell_{S} = 2$ cycles. The unit is fully pipelined with initiation interval $1$ and there is exactly one such unit.\n- One Boolean unit $B$ that executes $64$-bit bitwise $\\mathrm{OR}$. Each $64$-bit $\\mathrm{OR}$ has latency $\\ell_{B} = 1$ cycle. The unit is fully pipelined with initiation interval $1$ and there is exactly one such unit.\n- One Funnel unit $F$ that executes a $32$-bit funnel shift-right of the form $\\mathrm{FSR32}(a,b,s)$, which conceptually forms the $64$-bit value $(b:a)$ by placing $a$ in the low half and $b$ in the high half, logically shifts it right by $s$ where $0 \\le s \\le 31$, and returns the low $32$ bits. Each $\\mathrm{FSR32}$ has latency $\\ell_{F} = 1$ cycle. The unit is fully pipelined with initiation interval $1$ and there is exactly one such unit.\n\nThe code generator may choose one of the following legal lowerings for a rotate-right by constant $k$:\n\n- Shift–OR lowering: emit one $64$-bit logical right shift by $k$, one $64$-bit logical left shift by $(64 - k)$, and one $64$-bit bitwise $\\mathrm{OR}$ of the two shift results.\n- Funnel-shift lowering: emit exactly two $\\mathrm{FSR32}$ instructions to produce the new low and high halves of the rotated value, using $(x_{\\mathrm{lo}}, x_{\\mathrm{hi}})$ as inputs when $0 \\le k  32$, and swapping the input-order and using shift amount $(k - 32)$ when $32 \\le k  64$. Assume the two $\\mathrm{FSR32}$ instructions write their results into two fresh $32$-bit destination registers, so no extra moves are needed, and the ordered pair of these two destination registers is the final result.\n\nAssume the input pair $(x_{\\mathrm{lo}}, x_{\\mathrm{hi}})$ is available at cycle $0$, the constant $k$ is embedded in instructions, there are no cache or fetch effects, and scheduling is constrained only by data dependencies and the single-instance functional units described above. Use the standard notion from basic-block scheduling that an instruction’s result becomes available exactly $\\ell$ cycles after it is issued, where $\\ell$ is that instruction’s latency.\n\nFor the specific constant $k = 37$, determine the minimal possible completion time, in cycles, to compute the rotated $64$-bit result when the code generator selects the best available lowering and the scheduler issues instructions as early as possible subject to the resource and dependency constraints. Express your final answer as a single integer number of cycles (no units in the final boxed answer).", "solution": "The user has requested the minimal completion time for a $64$-bit rotate-right operation by a constant amount $k=37$. The solution requires analyzing two different implementation strategies, or \"lowerings,\" on a target architecture with specified functional units and latencies, and then selecting the faster of the two. The analysis will proceed by constructing a schedule for each lowering, respecting data dependencies and resource constraints. The \"as early as possible\" scheduling policy is applied, and all functional units are fully pipelined with an initiation interval of $1$.\n\nThe input 64-bit value $x$ is represented by a pair of $32$-bit registers $(x_{\\mathrm{lo}}, x_{\\mathrm{hi}})$, which we interpret as the value $x = (x_{\\mathrm{hi}} \\ll 32) | x_{\\mathrm{lo}}$, where $|$ denotes bitwise OR and $\\ll$ denotes logical left shift. Inputs are available at cycle $0$.\n\n### Strategy 1: Shift–OR Lowering\n\nThis strategy implements the $64$-bit rotate-right operation $\\mathrm{ROTR}(x, k)$ using its definition:\n$$ \\mathrm{ROTR}(x, k) = (x \\gg k) | (x \\ll (64 - k)) $$\nwhere $\\gg$ is logical right shift. For $k=37$, the operations are:\n1.  `Op1`: $t_1 = x \\gg 37$. This is a $64$-bit logical right shift.\n2.  `Op2`: $t_2 = x \\ll (64 - 37) = x \\ll 27$. This is a $64$-bit logical left shift.\n3.  `Op3`: $y = t_1 | t_2$. This is a $64$-bit bitwise $\\mathrm{OR}$.\n\nWe schedule these operations based on their dependencies and resource requirements.\n- **Dependencies**: `Op1` and `Op2` depend on the input $x$ (available at cycle $0$). `Op3` depends on the results of `Op1` and `Op2`.\n- **Resources**: `Op1` and `Op2` both require the Shift unit $S$. `Op3` requires the Boolean unit $B$.\n- **Latencies**: The latency of the Shift unit is $\\ell_S = 2$ cycles. The latency of the Boolean unit is $\\ell_B = 1$ cycle.\n- **Pipelining**: All units have an initiation interval of $1$, meaning a new instruction can be issued to a unit on every cycle.\n\nThe scheduling proceeds as follows:\n- **Cycle 0**: The input $x$ is available. Both `Op1` and `Op2` are ready. We can issue one of them to the Shift unit $S$. Let's issue `Op1`.\n  - Issue `Op1` ($x \\gg 37$) at cycle $0$. It uses unit $S$.\n  - The result $t_1$ will be available at cycle $0 + \\ell_S = 2$.\n- **Cycle 1**: `Op2` is ready. The Shift unit $S$ is available for a new instruction because its initiation interval is $1$.\n  - Issue `Op2` ($x \\ll 27$) at cycle $1$. It uses unit $S$.\n  - The result $t_2$ will be available at cycle $1 + \\ell_S = 3$.\n- **Cycle 2**: The result $t_1$ becomes available. `Op3` is not yet ready as it still awaits $t_2$.\n- **Cycle 3**: The result $t_2$ becomes available. Now both inputs for `Op3` ($t_1$ and $t_2$) are ready. The Boolean unit $B$ is available.\n  - Issue `Op3` ($t_1 | t_2$) at cycle $3$. It uses unit $B$.\n  - The final result $y$ will be available at cycle $3 + \\ell_B = 4$.\n\nThe completion time for the Shift–OR lowering is $4$ cycles.\n\n### Strategy 2: Funnel-shift Lowering\n\nThis strategy uses two specialized $32$-bit funnel shift instructions, $\\mathrm{FSR32}(a, b, s)$, which computes the low $32$ bits of the $64$-bit value $(b:a)$ after a logical right shift by $s$. The operation is equivalent to $(b \\ll (32-s)) | (a \\gg s)$.\nThe problem states that for $32 \\le k  64$, the input order is swapped and the shift amount is $s = k - 32$. For our specific case, $k=37$, so the shift amount is $s = 37 - 32 = 5$. The two $\\mathrm{FSR32}$ instructions must compute the new low and high halves of the rotated result, let's call them $y_{\\mathrm{lo}}$ and $y_{\\mathrm{hi}}$.\n\nLet's derive the expressions for $y_{\\mathrm{lo}}$ and $y_{\\mathrm{hi}}$ resulting from $\\mathrm{ROTR}(x, 37)$:\nThe new low half, $y_{\\mathrm{lo}}$, is composed of the low $32$ bits of $(x \\gg 37)$ and $(x \\ll 27)$.\n$$ y_{\\mathrm{lo}} = (x_{\\mathrm{lo}} \\ll 27) | (x_{\\mathrm{hi}} \\gg 5) $$\nThe new high half, $y_{\\mathrm{hi}}$, is composed of the high $32$ bits of the same expressions.\n$$ y_{\\mathrm{hi}} = (x_{\\mathrm{hi}} \\ll 27) | (x_{\\mathrm{lo}} \\gg 5) $$\nWe must match these expressions to the $\\mathrm{FSR32}$ instruction. With $s=5$:\n- To compute $y_{\\mathrm{lo}} = (x_{\\mathrm{lo}} \\ll 27) | (x_{\\mathrm{hi}} \\gg 5)$: We set $a = x_{\\mathrm{hi}}$ and $b = x_{\\mathrm{lo}}$.\n  - `OpA`: $y_{\\mathrm{lo}} = \\mathrm{FSR32}(x_{\\mathrm{hi}}, x_{\\mathrm{lo}}, 5)$.\n- To compute $y_{\\mathrm{hi}} = (x_{\\mathrm{hi}} \\ll 27) | (x_{\\mathrm{lo}} \\gg 5)$: We set $a = x_{\\mathrm{lo}}$ and $b = x_{\\mathrm{hi}}$.\n  - `OpB`: $y_{\\mathrm{hi}} = \\mathrm{FSR32}(x_{\\mathrm{lo}}, x_{\\mathrm{hi}}, 5)$.\n\nNow, we schedule `OpA` and `OpB`.\n- **Dependencies**: Both `OpA` and `OpB` depend on the inputs $x_{\\mathrm{lo}}$ and $x_{\\mathrm{hi}}$, which are available at cycle $0$. The two operations are independent of each other.\n- **Resources**: Both `OpA` and `OpB` require the Funnel unit $F$.\n- **Latencies**: The latency of the Funnel unit is $\\ell_F = 1$ cycle.\n- **Pipelining**: The unit has an initiation interval of $1$.\n\nThe scheduling proceeds as follows:\n- **Cycle 0**: Inputs are available. Both `OpA` and `OpB` are ready. We issue one to the Funnel unit $F$. Let's issue `OpA`.\n  - Issue `OpA` ($\\mathrm{FSR32}(x_{\\mathrm{hi}}, x_{\\mathrm{lo}}, 5)$) at cycle $0$. It uses unit $F$.\n  - The result $y_{\\mathrm{lo}}$ will be available at cycle $0 + \\ell_F = 1$.\n- **Cycle 1**: `OpB` is ready. Unit $F$ is available for a new instruction.\n  - Issue `OpB` ($\\mathrm{FSR32}(x_{\\mathrm{lo}}, x_{\\mathrm{hi}}, 5)$) at cycle $1$. It uses unit $F$.\n  - The result $y_{\\mathrm{hi}}$ will be available at cycle $1 + \\ell_F = 2$.\n\nThe complete $64$-bit result $(y_{\\mathrm{lo}}, y_{\\mathrm{hi}})$ is available only when both halves have been computed. The first half, $y_{\\mathrm{lo}}$, is ready at cycle $1$. The second half, $y_{\\mathrm{hi}}$, is ready at cycle $2$. Therefore, the total completion time is the maximum of these two ready times.\nCompletion time = $\\max(1, 2) = 2$ cycles.\n\n### Conclusion\n\nComparing the two strategies:\n- Shift–OR lowering completion time: $4$ cycles.\n- Funnel-shift lowering completion time: $2$ cycles.\n\nThe code generator selects the best (fastest) available lowering. In this case, the Funnel-shift lowering is superior. The minimal possible completion time is $2$ cycles.", "answer": "$$\\boxed{2}$$", "id": "3628158"}, {"introduction": "Once instructions are selected, their inputs and outputs must be assigned to a finite set of physical registers. When the number of simultaneously live values exceeds the number of available registers, some values must be temporarily \"spilled\" to memory. This exercise [@problem_id:3628173] puts you in charge of managing spill code, requiring you to make strategic decisions about which temporaries to spill to minimize performance costs and how to efficiently lay out the stack frame by reusing slots for spilled values that are not live at the same time.", "problem": "A code generator for a simple imperative language targets a machine with exactly $2$ physical registers available for temporaries. The Intermediate Representation (IR) of a single linear basic block is indexed by program points $\\{1,2,\\dots,16\\}$. Five temporaries $t_1, t_2, t_3, t_4, t_5$ have the following live segments (the notation $[a,b]$ means the temporary is live at every integer program point $p$ with $a \\leq p \\leq b$, and $\\cup$ denotes disjoint live segments due to lifetime holes), and each temporary has an associated storage width in bytes if spilled:\n\n- $t_1$: live at $[1,5] \\cup [9,11]$, width $8$.\n- $t_2$: live at $[3,7]$, width $4$.\n- $t_3$: live at $[4,5] \\cup [9,10]$, width $4$.\n- $t_4$: live at $[2,4] \\cup [10,12]$, width $16$.\n- $t_5$: live at $[8,15]$, width $4$.\n\nAt any program point $p$, at most $2$ live temporaries can occupy registers. Every live temporary not occupying a register at $p$ is considered spilled at $p$ and must occupy a stack slot of size at least its width. The code generator may change which temporaries are in registers over time, and may reuse the same stack slot for different spilled temporaries provided their spilled live segments do not overlap in time. A stack slot’s size is fixed to the maximum width of any temporary that ever occupies it. Assume no padding between slots and that the stack frame contribution from spill slots is the sum of their slot sizes. The objective is to choose, over all program points, which temporaries occupy registers and to color the spilled segments onto a minimal set of stack slots so that the total stack frame size due to spill slots is minimized.\n\nBased on the above, compute the minimal total stack frame size in bytes. Express your final answer in bytes. No rounding is required.", "solution": "The problem requires finding the minimal stack frame size needed for spilling temporaries given a machine with 2 registers. The core task is to determine, at each program point, how many temporaries must be spilled, and then to arrange these spilled segments into a minimal set of stack slots, respecting their widths.\n\nFirst, we determine the set of live temporaries $L(p)$ for each program point $p$:\n- $p=1$: $L(1) = \\{t_1\\}$\n- $p=2$: $L(2) = \\{t_1, t_4\\}$\n- $p=3$: $L(3) = \\{t_1, t_2, t_4\\}$\n- $p=4$: $L(4) = \\{t_1, t_2, t_3, t_4\\}$\n- $p=5$: $L(5) = \\{t_1, t_2, t_3\\}$\n- $p=6$: $L(6) = \\{t_2\\}$\n- $p=7$: $L(7) = \\{t_2\\}$\n- $p=8$: $L(8) = \\{t_5\\}$\n- $p=9$: $L(9) = \\{t_1, t_3, t_5\\}$\n- $p=10$: $L(10) = \\{t_1, t_3, t_4, t_5\\}$\n- $p=11$: $L(11) = \\{t_1, t_4, t_5\\}$\n- $p=12$: $L(12) = \\{t_4, t_5\\}$\n- $p=13$: $L(13) = \\{t_5\\}$\n- $p=14$: $L(14) = \\{t_5\\}$\n- $p=15$: $L(15) = \\{t_5\\}$\n- $p=16$: $L(16) = \\emptyset$\n\nAt any point $p$ where $|L(p)| > 2$, we have register pressure and must spill $|L(p)| - 2$ temporaries. To minimize the total stack frame size, the best strategy is to always spill the narrowest temporaries, as a slot's size is determined by the widest temporary it ever holds. The widths are: width($t_4$)=16, width($t_1$)=8, and width($t_2$)=width($t_3$)=width($t_5$)=4.\n\nWe analyze the points of contention:\n- At $p=3$, $L(3)=\\{t_1(8), t_2(4), t_4(16)\\}$. $|L(3)|=3$. We must spill one temporary. We spill the narrowest, $t_2$ (4 bytes).\n- At $p=4$, $L(4)=\\{t_1(8), t_2(4), t_3(4), t_4(16)\\}$. $|L(4)|=4$. We must spill two. We spill the narrowest, $t_2$ and $t_3$ (both 4 bytes).\n- At $p=5$, $L(5)=\\{t_1(8), t_2(4), t_3(4)\\}$. $|L(5)|=3$. We must spill one. We spill one of the 4-byte temporaries, e.g., $t_3$.\n- At $p=9$, $L(9)=\\{t_1(8), t_3(4), t_5(4)\\}$. $|L(9)|=3$. We must spill one. We spill one of the 4-byte temporaries, e.g., $t_3$.\n- At $p=10$, $L(10)=\\{t_1(8), t_3(4), t_4(16), t_5(4)\\}$. $|L(10)|=4$. We must spill two. We spill the narrowest, $t_3$ and $t_5$ (both 4 bytes).\n- At $p=11$, $L(11)=\\{t_1(8), t_4(16), t_5(4)\\}$. $|L(11)|=3$. We must spill one. We spill the narrowest, $t_5$ (4 bytes).\n\nFollowing this strategy, we only ever spill 4-byte temporaries. This means any spill slots we create will be 4 bytes wide. The next step is to determine the minimum number of slots needed. This is given by the maximum number of temporaries that must be spilled *simultaneously* at any single program point.\n\n- At $p=4$, we must spill $t_2$ and $t_3$ simultaneously. This requires 2 spill slots.\n- At $p=10$, we must spill $t_3$ and $t_5$ simultaneously. This also requires 2 spill slots.\n\nThe maximum number of simultaneous spills at any point is 2. Therefore, we need at least two distinct spill slots. We can check if two slots are sufficient. Let's call them Slot 1 and Slot 2.\n- At $p=3$, spill $t_2$ into Slot 1.\n- At $p=4$, spill $t_2$ into Slot 1 and $t_3$ into Slot 2.\n- At $p=5$, spill $t_3$ into Slot 2.\n- At $p=9$, spill $t_3$ into Slot 2.\n- At $p=10$, spill $t_3$ into Slot 2 and $t_5$ into Slot 1 (since $t_2$ is not live, Slot 1 is available for $t_5$).\n- At $p=11$, spill $t_5$ into Slot 1.\n\nThis assignment works. We need exactly two spill slots. Since this strategy only ever spills temporaries of width 4, both slots will be sized at 4 bytes.\nThe total stack frame size due to spill slots is the sum of the sizes of the required slots.\n$$ \\text{Total size} = \\text{size(Slot 1)} + \\text{size(Slot 2)} = 4 \\text{ bytes} + 4 \\text{ bytes} = 8 \\text{ bytes} $$\nThis is the minimal size because we established a lower bound of 2 slots, and we ensured those slots were as small as possible (4 bytes each) by never spilling the wider temporaries ($t_1$ or $t_4$).", "answer": "$$\\boxed{8}$$", "id": "3628173"}, {"introduction": "Register allocators often try to eliminate redundant copy instructions through an optimization called \"move coalescing.\" While this can reduce instruction count, the decision is not always straightforward and can have subtle, far-reaching consequences. This practice [@problem_id:3628152] explores the complex trade-offs of coalescing, asking you to reason about how it can unexpectedly increase register pressure and complicate instruction selection, demonstrating that optimal code generation requires a holistic view of interacting optimizations.", "problem": "A code generator must decide whether to coalesce a move $t \\leftarrow s$ or keep it, trading off fewer moves against potential increases in register pressure and loss of profitable addressing modes. Work with the following base model and facts.\n\nBase model and facts:\n- Register allocation seeks to assign each live temporary to one of $K$ registers so that no two simultaneously live temporaries share a register. In the interference-graph model, each temporary is a node, and an edge connects two nodes if the corresponding live ranges overlap. A $K$-register allocator corresponds to a $K$-coloring of this graph.\n- A move $t \\leftarrow s$ that is coalesced merges $s$ and $t$ into one node whose adjacency set is the union of their neighbors. This can increase the node’s degree and can change colorability properties of the interference graph.\n- A spill occurs when no register can be assigned to a live temporary and it must be stored in memory.\n- Consider a machine with $K = 2$ general-purpose registers and the following instruction forms: arithmetic requires both operands in registers; memory load/store use a base register plus immediate offset. Additionally, the machine supports a base-update addressing mode for stores of the form store-back $[\\;b\\;]^+ \\leftarrow v$ which both stores to address in base register $b$ and updates $b$ (e.g., $b \\leftarrow b + c$ for some fixed constant $c$), clobbering $b$; to use this mode, the base register must be the one that will be clobbered by the store.\n\nCase setup for analysis:\n- Interference-graph case $G$: temporaries $\\{s,t,p,q\\}$ with edges $\\{(s,p),(p,q),(q,t)\\}$, and a move $t \\leftarrow s$. Note that $s$ and $t$ do not interfere. Assume a standard, naive simplify-based graph-coloring allocator that spills when no node has degree strictly less than $K$.\n- Addressing-mode case: later code performs a store with base-update using a base equal to $t$, while $s$ is used again later in arithmetic. The code generator may either keep $t \\leftarrow s$ and use $t$ as the clobbered base, or coalesce and make $s$ serve as the clobbered base.\n\nQuestion. Which of the following statements are correct under the model above? Select all that apply.\n\nA. In interference-graph case $G$ with $K = 2$, aggressive coalescing of $t \\leftarrow s$ (merging $s$ and $t$) can transform a $2$-colorable graph into one that is not $2$-colorable, forcing a spill for a naive $K$-coloring allocator, even though the uncoalesced graph is $2$-colorable.\n\nB. On the given machine with base-update stores, keeping the copy $t \\leftarrow s$ can enable use of the base-update addressing mode without extending the live range of $s$ or reintroducing extra moves, because $t$ can be the clobbered base while $s$ remains available for later uses. Coalescing $t$ and $s$ can remove this flexibility and either block the addressing mode or require another copy later.\n\nC. If $s$ and $t$ do not interfere (their live ranges do not overlap except at the copy), then coalescing them cannot increase any other temporary’s live range and therefore cannot adversely affect allocation.\n\nD. For any machine with $K$ registers, any sequence of aggressive coalescing operations that only contract non-interfering pairs preserves $K$-colorability of the interference graph; hence coalescing never causes spills that a non-coalescing allocator would avoid.", "solution": "The core of the problem lies in analyzing the consequences of coalescing a move instruction, $t \\leftarrow s$, within the framework of graph-coloring register allocation. The decision to coalesce involves a trade-off between eliminating a move instruction and potentially complicating the register allocation problem. The provided model specifies the parameters for this analysis.\n\n-   **Register Allocation Model**: Based on an interference graph where temporaries are nodes and an edge connects two nodes if their live ranges overlap. A machine with $K$ registers requires a $K$-coloring of this graph.\n-   **Coalescing**: Merging the nodes for $s$ and $t$ (which do not interfere) into a single node, say $st$. The new node $st$ inherits the neighbors of both $s$ and $t$, i.e., $N(st) = N(s) \\cup N(t)$.\n-   **Allocator Behavior**: A naive simplify-based allocator is used. This allocator applies a `simplify` rule, removing any node with a degree strictly less than $K$. If at any point all remaining nodes have a degree greater than or equal to $K$, the allocator gives up and `spills` a temporary to memory.\n-   **Machine Details**: $K=2$ general-purpose registers. A special `store-back` instruction, `store-back $[\\;b\\;]^+ \\leftarrow v$`, uses the register $b$ for the base address and subsequently clobbers (overwrites) it.\n\nWe will now evaluate each statement based on this model and the specific cases provided.\n\n**Analysis of Statement A**\n\nStatement A claims that coalescing $t \\leftarrow s$ in interference-graph case $G$ with $K=2$ can transform a $2$-colorable graph into one that is not, forcing a spill.\n\n1.  **Analyze the uncoalesced graph $G$**:\n    -   The set of temporaries (nodes) is $\\{s, t, p, q\\}$.\n    -   The interference edges are $\\{(s,p), (p,q), (q,t)\\}$. This forms a path graph: $s-p-q-t$.\n    -   The degrees of the nodes are: $d(s)=1$, $d(t)=1$, $d(p)=2$, $d(q)=2$.\n    -   The machine has $K=2$ registers. The simple allocator spills if it cannot find a node with degree $ K$, i.e., degree $ 2$.\n    -   In graph $G$, nodes $s$ and $t$ both have degree $1$, which is strictly less than $K=2$.\n    -   The allocator can `simplify` by removing $s$ (or $t$), then continue simplifying until the graph is empty. No spill is forced. For example, a valid simplification sequence is $s, t, p, q$.\n    -   Furthermore, the graph is a path, which is a bipartite graph and therefore $2$-colorable. A possible $2$-coloring is: Color $1$ for $\\{s, q\\}$ and Color $2$ for $\\{p, t\\}$.\n    -   Conclusion for uncoalesced graph: It is $2$-colorable and the naive allocator will not spill.\n\n2.  **Analyze the coalesced graph $G'$**:\n    -   The move $t \\leftarrow s$ is coalesced. Nodes $s$ and $t$ are merged into a single node, which we can call $st$. Note that $s$ and $t$ do not interfere in $G$.\n    -   The new set of nodes is $\\{st, p, q\\}$.\n    -   The neighbors of the new node $st$ are the union of the neighbors of $s$ and $t$: $N(st) = N(s) \\cup N(t) = \\{p\\} \\cup \\{q\\} = \\{p, q\\}$.\n    -   The edges in $G'$ are the original edge $(p,q)$ plus the new edges from coalescing: $(st,p)$ and $(st,q)$. The full edge set is $\\{(st, p), (st, q), (p, q)\\}$.\n    -   This new graph $G'$ is a $3$-clique ($K_3$), or a triangle.\n    -   The degrees of the nodes in $G'$ are: $d(st)=2$, $d(p)=2$, $d(q)=2$.\n    -   Now, consider the naive allocator with $K=2$. It looks for a node with degree $ 2$.\n    -   In $G'$, every node has degree $2$. There is no node with degree strictly less than $K=2$.\n    -   According to the specified rule, the allocator must spill.\n    -   A $3$-clique is not $2$-colorable; its chromatic number is $3$. Coalescing has indeed transformed a $2$-colorable graph into one that is not.\n\nThe analysis confirms that aggressive coalescing turned a graph that was easily colorable by the naive allocator into one that forces a spill.\n\n**Verdict for A: Correct.**\n\n**Analysis of Statement B**\n\nStatement B concerns the interaction between coalescing and the availability of a special addressing mode, `store-back`, which clobbers its base register.\n\n-   **Scenario**: A `store-back` instruction needs to use $t$ as its base. Later, the value of $s$ is needed for an arithmetic operation.\n\n1.  **Case 1: Keep the copy $t \\leftarrow s$**.\n    -   The program contains the instruction $t \\leftarrow s$. This typically means $s$ and $t$ must be assigned to different registers if their live ranges overlap elsewhere (which they don't, but keeping the copy implies treating them as distinct entities whose values can be managed separately).\n    -   Let's say register $R_s$ holds the value of $s$ and register $R_t$ holds the value of $t$. After the copy, both registers hold the same value.\n    -   The instruction `store-back` uses $t$ as its base. This means it will use the register $R_t$ and subsequently clobber it.\n    -   After the `store-back` instruction, the value in $R_t$ is destroyed, but the value of $s$ is still safe in register $R_s$.\n    -   The later arithmetic use of $s$ can proceed by using the value in $R_s$.\n    -   This strategy successfully enables the use of the profitable `store-back` addressing mode without requiring any extra save/restore instructions. The live range of $s$ effectively \"goes around\" the `store-back` instruction, while the live range of $t$ ends at that instruction.\n\n2.  **Case 2: Coalesce $t$ and $s$**.\n    -   $s$ and $t$ are merged into a single temporary, let's call it $s$. This temporary is assigned to a single register, say $R_s$.\n    -   The `store-back` instruction, which was originally to use $t$ as a base, must now use $s$ as its base.\n    -   The instruction executes, using $R_s$ as the base register, and then clobbers $R_s$.\n    -   The original value of $s$, which is needed for the subsequent arithmetic operation, has now been destroyed.\n    -   This presents the code generator with a dilemma:\n        -   (a) It can avoid using the `store-back` instruction and instead use a regular `store` followed by an `add`, losing the benefit of the special addressing mode (e.g., code size, speed).\n        -   (b) It can insert additional `move` instructions to save the value of $s$ before the `store-back` and restore it afterwards. This negates the benefit of eliminating the original $t \\leftarrow s$ copy.\n    -   In either case, coalescing has introduced a complication that either blocks the use of a profitable instruction or requires reintroducing moves.\n\nThe statement accurately describes this trade-off. Keeping the copy decouples the clobbered temporary ($t$) from the one that must be preserved ($s$), whereas coalescing forces a conflict.\n\n**Verdict for B: Correct.**\n\n**Analysis of Statement C**\n\nStatement C is: \"If $s$ and $t$ do not interfere... then coalescing them cannot increase any other temporary’s live range and therefore cannot adversely affect allocation.\"\n\nThis statement contains a premise and a conclusion.\n-   **Premise**: \"coalescing them cannot increase any other temporary’s live range\". When $s$ and $t$ are coalesced into $st$, the live range of $st$ is the union of the live ranges of $s$ and $t$. This operation does not alter the definition or use points of any *other* temporary (like $p$ or $q$ in our example), so their live ranges are indeed unaffected. The premise is correct.\n-   **Conclusion**: \"and therefore cannot adversely affect allocation\". This conclusion is a non sequitur. The adverse effect of coalescing is not on the live ranges of other temporaries, but on the structure of the interference graph itself.\n-   The analysis for statement A provides a direct counterexample. Coalescing $s$ and $t$ (which do not interfere) in graph $G$ did not change the live ranges of $p$ or $q$. However, it created a new node $st$ that interfered with *both* $p$ and $q$. This increased the degree of the coalesced node and the connectivity of the graph, leading to a spill that would not have occurred otherwise.\n-   The adverse effect on allocation stems from the increase in interference constraints on the *new, merged* temporary, which can make the graph more difficult, or impossible, to $K$-color.\n\n**Verdict for C: Incorrect.**\n\n**Analysis of Statement D**\n\nStatement D makes a very strong, general claim: \"For any machine with $K$ registers, any sequence of aggressive coalescing operations that only contract non-interfering pairs preserves $K$-colorability of the interference graph; hence coalescing never causes spills that a non-coalescing allocator would avoid.\"\n\n-   This statement claims that aggressive coalescing is \"safe\" with respect to $K$-colorability.\n-   The analysis for statement A serves as a perfect counterexample to this general claim.\n-   In that analysis, we started with a machine with $K=2$ registers.\n-   We had an interference graph $G$ that was $2$-colorable.\n-   We performed a single coalescing operation on a non-interfering pair, $(s, t)$.\n-   The resulting graph, $G'$, was not $2$-colorable (it was a $K_3$, requiring $3$ colors).\n-   Thus, the operation did *not* preserve $K$-colorability for $K=2$.\n-   The second clause, \"hence coalescing never causes spills...\", is also falsified by the same example. The naive allocator did not spill on $G$ but was forced to spill on $G'$.\n-   This demonstrates that aggressive coalescing is not a safe transformation in general. Real-world compilers use more conservative heuristics (like the Briggs or George heuristics) to decide when to coalesce, precisely to avoid creating uncolorable graphs.\n\n**Verdict for D: Incorrect.**", "answer": "$$\\boxed{AB}$$", "id": "3628152"}]}