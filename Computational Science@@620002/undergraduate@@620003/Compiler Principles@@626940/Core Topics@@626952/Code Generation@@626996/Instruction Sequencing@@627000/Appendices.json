{"hands_on_practices": [{"introduction": "Instruction sequencing is fundamentally about converting a partial order of operations, defined by a dependency graph, into an optimal linear execution sequence. However, not all valid linearizations are created equal. This first exercise [@problem_id:3647127] illustrates a crucial principle: a naive scheduling approach, while correct, can be highly inefficient. By comparing a fixed topological sort to a more dynamic list scheduler, you will see how intelligently interleaving independent instructions is key to hiding latency and minimizing execution time.", "problem": "A compiler backend schedules instructions on a single-issue pipeline by respecting true data dependences represented as a Directed Acyclic Graph (DAG). A topological order is any linearization of the DAG that respects the partial order. Each instruction issues in exactly $1$ cycle, and its result becomes available $\\ell$ cycles after its issue, where $\\ell$ is the instruction’s latency. An instruction may be issued only when all of its predecessors’ results are available. If a scheduler has precommitted to a fixed topological order and the next instruction in that list is not yet eligible, it must emit a No Operation (NOP), which occupies $1$ cycle.\n\nConsider the following basic-block DAG with nodes $I_{1},\\dots,I_{8}$ and edges $I_{1}\\rightarrow I_{3}\\rightarrow I_{5}\\rightarrow I_{7}$. The other nodes $I_{2}, I_{4}, I_{6}, I_{8}$ have no incoming or outgoing edges. Latencies are:\n- $I_{1}$ is a load with latency $L_{\\text{ld}}=4$.\n- $I_{3}$ is an arithmetic operation with latency $L_{\\text{alu}}=1$.\n- $I_{5}$ is a multiply with latency $L_{\\text{mul}}=3$.\n- $I_{7}$ is an arithmetic operation with latency $L_{\\text{alu}}=1$.\n- Each of $I_{2}, I_{4}, I_{6}, I_{8}$ is an independent arithmetic operation with latency $L_{\\text{alu}}=1$.\n\nAssume time starts at cycle $0$. Define completion time $T$ as the maximum, over all instructions, of its issue time plus its latency.\n\nTwo schedulers are considered:\n- Naive topological-order scheduler: it first produces a single topological ordering by performing a depth-first traversal starting at $I_{1}$ and always following successors before exploring unrelated nodes, yielding the list $\\langle I_{1}, I_{3}, I_{5}, I_{7}, I_{2}, I_{4}, I_{6}, I_{8}\\rangle$. It then emits instructions by walking this fixed list; if the next instruction is not yet eligible, it emits a NOP and tries again on the next cycle.\n- Latency-hiding list scheduler: at each cycle it emits any eligible instruction (one per cycle), if any exists, to minimize completion time, while respecting dependences.\n\nModel instruction sequencing as the problem of producing a topological order and then placing instructions on cycles under the latency-ready constraint above. Compute the quantity $\\Delta T = T_{\\text{naive}} - T_{\\text{opt}}$, where $T_{\\text{naive}}$ is the completion time under the naive scheduler and $T_{\\text{opt}}$ is the minimum achievable completion time under the latency-hiding list scheduler. Your final answer must be a single integer giving $\\Delta T$. No rounding is required.", "solution": "The user wants me to solve the problem by following the specified instructions.\n\n### Step 1: Problem Validation\n\nI must first validate the problem statement.\n\n#### Step 1.1: Extract Givens\n- **Machine Model:** Single-issue pipeline.\n- **Instruction Scheduling:** Based on a Directed Acyclic Graph (DAG) representing true data dependences.\n- **Timing:**\n  - Each instruction issues in $1$ cycle.\n  - Result of an instruction with latency $\\ell$ is available $\\ell$ cycles after its issue.\n  - Time starts at cycle $0$.\n- **Eligibility:** An instruction can be issued at cycle $C$ only if for all its predecessors $P$, $C \\ge \\text{IssueTime}(P) + \\text{Latency}(P)$.\n- **NOPs:** A No Operation (NOP) is emitted for $1$ cycle if the scheduler is committed to a fixed order and the next instruction is not eligible.\n- **DAG Structure:**\n  - Nodes: $I_1, I_2, I_3, I_4, I_5, I_6, I_7, I_8$.\n  - Edges (dependency chain): $I_1 \\rightarrow I_3 \\rightarrow I_5 \\rightarrow I_7$.\n  - Independent nodes (no incoming or outgoing edges): $I_2, I_4, I_6, I_8$.\n- **Latencies ($\\ell$):**\n  - $I_1$ (load): $L_{\\text{ld}}=4$.\n  - $I_3$ (ALU): $L_{\\text{alu}}=1$.\n  - $I_5$ (multiply): $L_{\\text{mul}}=3$.\n  - $I_7$ (ALU): $L_{\\text{alu}}=1$.\n  - $I_2, I_4, I_6, I_8$ (independent ALU): $L_{\\text{alu}}=1$.\n- **Completion Time ($T$):** $T = \\max_{i} (\\text{IssueTime}(I_i) + \\text{Latency}(I_i))$.\n- **Schedulers:**\n  - **Naive Scheduler:** Uses a fixed topological order: $\\langle I_1, I_3, I_5, I_7, I_2, I_4, I_6, I_8 \\rangle$. Inserts NOPs if the next instruction is not ready.\n  - **Optimal Scheduler (Latency-hiding list scheduler):** At each cycle, emits any eligible instruction to achieve minimum completion time.\n- **Objective:** Compute $\\Delta T = T_{\\text{naive}} - T_{\\text{opt}}$.\n\n#### Step 1.2: Validate Using Extracted Givens\n1.  **Scientific or Factual Unsoundness:** The problem is sound. It describes a classic instruction scheduling scenario for a simple processor pipeline, a fundamental topic in computer architecture and compiler design. All concepts (DAGs, latencies, list scheduling) are standard.\n2.  **Non-Formalizable or Irrelevant:** The problem is highly formalizable and directly relevant to compiler principles.\n3.  **Incomplete or Contradictory Setup:** The problem is self-contained. It provides the graph, latencies, machine model, and scheduling algorithms. There are no contradictions.\n4.  **Unrealistic or Infeasible:** The model is a simplified but standard and plausible representation of an in-order processor pipeline. The latency values are reasonable.\n5.  **Ill-Posed or Poorly Structured:** The problem is well-posed. The rules for each scheduler are defined clearly, and the objective is a specific quantitative result. A unique solution exists.\n6.  **Pseudo-Profound, Trivial, or Tautological:** The problem is not trivial. It requires a step-by-step simulation and comparison of two distinct scheduling strategies, illustrating a core trade-off in compiler optimization.\n7.  **Outside Scientific Verifiability:** The result is verifiable by applying the rules of the simulation as stated.\n\n#### Step 1.3: Verdict and Action\nThe problem is **valid**. I will proceed with the solution.\n\n### Step 2: Solution\n\nThe problem requires us to compute the completion time for two different scheduling strategies and find their difference. Let's define the issue time of an instruction $I$ as $t_{\\text{issue}}(I)$ and its ready time as $t_{\\text{ready}}(I) = t_{\\text{issue}}(I) + \\ell(I)$, where $\\ell(I)$ is the latency of $I$. The total completion time is $T = \\max_{i} (t_{\\text{ready}}(I_i))$. An instruction $I$ is eligible to be issued at cycle $C$ if $C \\ge \\max_{P \\in \\text{preds}(I)} (t_{\\text{ready}}(P))$. If $I$ has no predecessors, it is eligible at cycle $0$.\n\n#### Analysis of the Naive Scheduler\nThe naive scheduler processes instructions in the fixed order $\\langle I_1, I_3, I_5, I_7, I_2, I_4, I_6, I_8 \\rangle$. It advances one cycle at a time, issuing the next instruction in the list if it is eligible, or a NOP otherwise.\n\n- **Cycle $0$**: The scheduler considers $I_1$. $I_1$ has no predecessors, so it is eligible.\n  - Issue $I_1$. $t_{\\text{issue}}(I_1) = 0$.\n  - $t_{\\text{ready}}(I_1) = t_{\\text{issue}}(I_1) + L_{\\text{ld}} = 0 + 4 = 4$.\n\n- **Cycle $1$**: The scheduler considers $I_3$. Its predecessor is $I_1$. $I_3$ is eligible at cycle $C$ if $C \\ge t_{\\text{ready}}(I_1) = 4$. Since the current cycle is $1$, $I_3$ is not eligible.\n  - Issue NOP.\n\n- **Cycle $2$**: The scheduler considers $I_3$ again. Eligibility requires cycle $\\ge 4$. The current cycle is $2$.\n  - Issue NOP.\n\n- **Cycle $3$**: The scheduler considers $I_3$ again. Eligibility requires cycle $\\ge 4$. The current cycle is $3$.\n  - Issue NOP.\n\n- **Cycle $4$**: The scheduler considers $I_3$. The current cycle is $4$, so it is eligible.\n  - Issue $I_3$. $t_{\\text{issue}}(I_3) = 4$.\n  - $t_{\\text{ready}}(I_3) = t_{\\text{issue}}(I_3) + L_{\\text{alu}} = 4 + 1 = 5$.\n\n- **Cycle $5$**: The scheduler considers $I_5$. Its predecessor is $I_3$. Eligibility requires cycle $\\ge t_{\\text{ready}}(I_3) = 5$. The current cycle is $5$, so it is eligible.\n  - Issue $I_5$. $t_{\\text{issue}}(I_5) = 5$.\n  - $t_{\\text{ready}}(I_5) = t_{\\text{issue}}(I_5) + L_{\\text{mul}} = 5 + 3 = 8$.\n\n- **Cycle $6$**: The scheduler considers $I_7$. Its predecessor is $I_5$. Eligibility requires cycle $\\ge t_{\\text{ready}}(I_5) = 8$. The current cycle is $6$.\n  - Issue NOP.\n\n- **Cycle $7$**: The scheduler considers $I_7$ again. Eligibility requires cycle $\\ge 8$. The current cycle is $7$.\n  - Issue NOP.\n\n- **Cycle $8$**: The scheduler considers $I_7$. The current cycle is $8$, so it is eligible.\n  - Issue $I_7$. $t_{\\text{issue}}(I_7) = 8$.\n  - $t_{\\text{ready}}(I_7) = t_{\\text{issue}}(I_7) + L_{\\text{alu}} = 8 + 1 = 9$.\n\n- **Cycle $9$**: The scheduler considers $I_2$. It has no predecessors, so it is eligible.\n  - Issue $I_2$. $t_{\\text{issue}}(I_2) = 9$.\n  - $t_{\\text{ready}}(I_2) = t_{\\text{issue}}(I_2) + L_{\\text{alu}} = 9 + 1 = 10$.\n\n- **Cycle $10$**: The scheduler considers $I_4$. It is eligible.\n  - Issue $I_4$. $t_{\\text{issue}}(I_4) = 10$.\n  - $t_{\\text{ready}}(I_4) = t_{\\text{issue}}(I_4) + L_{\\text{alu}} = 10 + 1 = 11$.\n\n- **Cycle $11$**: The scheduler considers $I_6$. It is eligible.\n  - Issue $I_6$. $t_{\\text{issue}}(I_6) = 11$.\n  - $t_{\\text{ready}}(I_6) = t_{\\text{issue}}(I_6) + L_{\\text{alu}} = 11 + 1 = 12$.\n\n- **Cycle $12$**: The scheduler considers $I_8$. It is eligible.\n  - Issue $I_8$. $t_{\\text{issue}}(I_8) = 12$.\n  - $t_{\\text{ready}}(I_8) = t_{\\text{issue}}(I_8) + L_{\\text{alu}} = 12 + 1 = 13$.\n\nThe completion time for the naive scheduler, $T_{\\text{naive}}$, is the maximum of all ready times:\n$$T_{\\text{naive}} = \\max(4, 5, 8, 9, 10, 11, 12, 13) = 13$$\n\n#### Analysis of the Optimal Scheduler\nThe optimal scheduler's goal is to minimize completion time. It can choose any eligible instruction at each cycle. A good heuristic for producing an optimal schedule is to prioritize instructions on the critical path and use independent instructions to fill stalls.\n\nThe minimum completion time is lower-bounded by the critical path length of the DAG, considering latencies. The critical path is the chain $I_1 \\rightarrow I_3 \\rightarrow I_5 \\rightarrow I_7$.\n- $I_1$ can be issued at cycle $0$. Its result is ready at $t_{\\text{ready}}(I_1) = 0 + 4 = 4$.\n- $I_3$ can be issued no earlier than cycle $4$. If issued at cycle $4$, its result is ready at $t_{\\text{ready}}(I_3) = 4 + 1 = 5$.\n- $I_5$ can be issued no earlier than cycle $5$. If issued at cycle $5$, its result is ready at $t_{\\text{ready}}(I_5) = 5 + 3 = 8$.\n- $I_7$ can be issued no earlier than cycle $8$. If issued at cycle $8$, its result is ready at $t_{\\text{ready}}(I_7) = 8 + 1 = 9$.\nThe critical path length is $9$ cycles. Thus, $T_{\\text{opt}} \\ge 9$. Let's construct a schedule that attempts to meet this bound.\n\n- **Cycle $0$**: Eligible instructions: $\\{I_1, I_2, I_4, I_6, I_8\\}$. The scheduler issues $I_1$ to start the critical path.\n  - Issue $I_1$. $t_{\\text{issue}}(I_1) = 0$, $t_{\\text{ready}}(I_1) = 4$.\n\n- **Cycle $1$**, **$2$**, **$3$**: In these cycles, $I_3$ is not yet eligible. The scheduler can issue the independent instructions to hide the latency of $I_1$.\n  - Cycle $1$: Eligible set $\\{I_2, I_4, I_6, I_8\\}$. Issue $I_2$. $t_{\\text{issue}}(I_2) = 1$, $t_{\\text{ready}}(I_2) = 1+1=2$.\n  - Cycle $2$: Eligible set $\\{I_4, I_6, I_8\\}$. Issue $I_4$. $t_{\\text{issue}}(I_4) = 2$, $t_{\\text{ready}}(I_4) = 2+1=3$.\n  - Cycle $3$: Eligible set $\\{I_6, I_8\\}$. Issue $I_6$. $t_{\\text{issue}}(I_6) = 3$, $t_{\\text{ready}}(I_6) = 3+1=4$.\n\n- **Cycle $4$**: At the start of this cycle, $t_{\\text{ready}}(I_1)=4$ is met, so $I_3$ becomes eligible.\n  - Eligible set: $\\{I_3, I_8\\}$. The scheduler prioritizes $I_3$ (critical path).\n  - Issue $I_3$. $t_{\\text{issue}}(I_3) = 4$, $t_{\\text{ready}}(I_3) = 4+1=5$.\n\n- **Cycle $5$**: $t_{\\text{ready}}(I_3)=5$ is met, so $I_5$ becomes eligible.\n  - Eligible set: $\\{I_5, I_8\\}$. The scheduler prioritizes $I_5$ (critical path).\n  - Issue $I_5$. $t_{\\text{issue}}(I_5) = 5$, $t_{\\text{ready}}(I_5) = 5+3=8$.\n\n- **Cycle $6$**: $I_7$ is not yet eligible (requires cycle $\\ge 8$).\n  - Eligible set: $\\{I_8\\}$.\n  - Issue $I_8$. $t_{\\text{issue}}(I_8) = 6$, $t_{\\text{ready}}(I_8) = 6+1=7$.\n\n- **Cycle $7$**: All independent instructions have been issued. The only remaining instruction is $I_7$, which is not eligible until cycle $8$.\n  - Eligible set: $\\emptyset$.\n  - Issue NOP.\n\n- **Cycle $8$**: $t_{\\text{ready}}(I_5)=8$ is met, so $I_7$ becomes eligible.\n  - Eligible set: $\\{I_7\\}$.\n  - Issue $I_7$. $t_{\\text{issue}}(I_7) = 8$, $t_{\\text{ready}}(I_7) = 8+1=9$.\n\nThe ready times for this schedule are: $t_{\\text{ready}}(I_1)=4$, $t_{\\text{ready}}(I_2)=2$, $t_{\\text{ready}}(I_4)=3$, $t_{\\text{ready}}(I_6)=4$, $t_{\\text{ready}}(I_3)=5$, $t_{\\text{ready}}(I_5)=8$, $t_{\\text{ready}}(I_8)=7$, $t_{\\text{ready}}(I_7)=9$.\n\nThe completion time for the optimal scheduler, $T_{\\text{opt}}$, is the maximum of these ready times:\n$$T_{\\text{opt}} = \\max(4, 2, 3, 4, 5, 8, 7, 9) = 9$$\nThis schedule achieves the lower bound of $9$ cycles, so it is optimal.\n\n#### Final Calculation\nThe quantity to compute is $\\Delta T = T_{\\text{naive}} - T_{\\text{opt}}$.\n$$ \\Delta T = 13 - 9 = 4 $$\nThe latency-hiding scheduler improves the completion time by $4$ cycles compared to the naive scheduler.", "answer": "$$\n\\boxed{4}\n$$", "id": "3647127"}, {"introduction": "Having seen the importance of latency hiding, this next practice [@problem_id:3647160] isolates this core optimization technique. We will examine a common scenario where a long-latency instruction, like division, threatens to stall the pipeline. Your task is to calculate the precise number of cycles saved by reordering independent work into this stall period, providing a concrete understanding of how compilers recover performance that would otherwise be lost to pipeline bubbles.", "problem": "A compiler targets an in-order, single-issue pipeline with the following characteristics grounded in standard pipeline semantics for dependences and latencies. The machine can issue at most $1$ instruction per cycle. An instruction that uses the result of a prior instruction can issue only when all of its source operands are available. Each instruction has a fixed producer-to-consumer latency equal to the number of cycles between the issue of the producer and the earliest cycle at which a dependent consumer can issue with zero stall. The integer Arithmetic Logic Unit (ALU) operations have latency $1$ cycle. The integer divide operation has latency $L_{\\text{div}}$ cycles. Assume structural hazards are absent and functional units are always available when operands are ready.\n\nConsider a loop body that, in a naive unscheduled form, executes the following sequence per iteration:\n- Compute $t \\leftarrow a / b$ using an integer divide with latency $L_{\\text{div}}$.\n- Immediately use $t$ in a dependent integer ALU operation $u \\leftarrow t + c$.\n- Execute $M$ integer ALU operations that are independent of both the divide result $t$ and its consumer $u$. These $M$ operations can be freely reordered relative to the divide and its consumer as long as their mutual program order among themselves is preserved, and all have latency $1$ cycle.\n\nA compiler performs local instruction scheduling to hide as much of the divide latency as possible by interleaving some of the $M$ independent integer ALU operations between the divide that produces $t$ and the consumer that uses $t$. Assume no other constraints prevent such motion, and that the machine must still issue in order.\n\nGiven $L_{\\text{div}} = 19$ and $M = 12$, derive from first principles the number of cycles per iteration saved by this scheduling relative to the naive order where the consumer of $t$ is placed immediately after the divide. Express your answer as an integer number of cycles saved per iteration. Do not round; express the final answer in cycles.", "solution": "This problem requires an analysis of pipeline stalls due to data dependencies in a simple in-order, single-issue processor. We must determine the number of cycles saved by reordering instructions to hide the long latency of a divide operation. The total number of cycles for an iteration is defined as the number of cycles from the issue of the first instruction to the issue of the last instruction, inclusive. On a single-issue machine, this is equivalent to the total number of issue slots (busy or stalled) required for the sequence.\n\nLet us first analyze the total cycles per iteration for the naive, unscheduled instruction sequence. The sequence is:\n$1$. `div`: $t \\leftarrow a / b$\n$2$. `add`: $u \\leftarrow t + c$\n$3$. $M$ independent ALU operations.\n\nThe machine is in-order and single-issue. Let the `div` instruction issue at cycle $C_0$. The `add` instruction is data-dependent on the `div` instruction, which has a latency of $L_{\\text{div}}$ cycles. By definition, this means a dependent instruction can issue no earlier than $L_{\\text{div}}$ cycles after the `div` instruction has issued. So, the `add` instruction cannot issue before cycle $C_0 + L_{\\text{div}}$.\n\nIn the naive sequence, the `add` instruction immediately follows the `div`. The pipeline will attempt to issue the `add` at cycle $C_0 + 1$. However, because the operand $t$ is not ready, the pipeline must stall. The stalls will occupy the issue slots from cycle $C_0 + 1$ up to, but not including, cycle $C_0 + L_{\\text{div}}$. The number of stall cycles is $(C_0 + L_{\\text{div}} - 1) - (C_0 + 1) + 1 = L_{\\text{div}} - 1$.\n\nThe total number of cycles, $T_{\\text{naive}}$, is the sum of cycles for issuing each instruction and any interposed stalls:\n- $1$ cycle for the `div` instruction to issue.\n- $L_{\\text{div}} - 1$ cycles of pipeline stall.\n- $1$ cycle for the dependent `add` instruction to issue.\n- $M$ cycles for the $M$ independent ALU operations to issue sequentially.\n\nSumming these, the total cycles for the naive sequence is:\n$$T_{\\text{naive}} = 1 + (L_{\\text{div}} - 1) + 1 + M = L_{\\text{div}} + M + 1$$\n\nNext, we analyze the total cycles for the optimized, scheduled instruction sequence. The compiler moves the $M$ independent operations between the `div` and the dependent `add` to hide the divide latency. The scheduled sequence is:\n$1$. `div`: $t \\leftarrow a / b$\n$2$. $M$ independent ALU operations.\n$3$. `add`: $u \\leftarrow t + c$\n\nLet the `div` instruction issue at cycle $C_0$. Since the following $M$ ALU operations are independent, they can be issued in the subsequent cycles, one per cycle.\n- The `div` instruction issues at cycle $C_0+1$.\n- The $M$ independent operations issue at cycles $C_0+1, C_0+2, \\dots, C_0+M$.\n\nAfter the last of the $M$ independent operations has issued at cycle $C_0+M$, the pipeline is ready to issue the `add` instruction at cycle $C_0+M+1$. However, the `add` is still data-dependent on the `div`. The result $t$ is available at cycle $C_0 + L_{\\text{div}}$. Therefore, the `add` instruction can only issue at the later of these two times: the time it becomes available in the instruction stream and the time its data dependency is resolved.\nThe issue cycle for the `add` is $C_{\\text{issue,add}} = \\max(C_0+M+1, C_0+L_{\\text{div}})$.\nLet's normalize by setting $C_0 = 0$. The `add` issues at cycle $\\max(M+1, L_{\\text{div}})$.\n\nThe total number of cycles for the scheduled sequence, $T_{\\text{scheduled}}$, can be calculated by counting slots. The number of slots for `div` and the $M$ ops is $M+1$. The time elapsed is $M+1$ cycles. The number of cycles passed between `div` issue and `add` being ready to issue (structurally) is $M+1$. The required time separation is $L_{\\text{div}}$. If $M+1  L_{\\text{div}}$, we need to stall for $L_{\\text{div}} - (M+1)$ cycles.\nThe total cycles are:\n$T_{\\text{scheduled}} = (\\text{cycles for div}) + (\\text{cycles for M ops}) + (\\text{stall cycles}) + (\\text{cycles for add})$\n$T_{\\text{scheduled}} = 1 + M + \\max(0, L_{\\text{div}} - (M+1)) + 1 = M+2 + \\max(0, L_{\\text{div}}-M-1)$.\nIf $M+1 \\ge L_{\\text{div}}$, $T_{\\text{scheduled}} = M+2$.\nIf $M+1  L_{\\text{div}}$, $T_{\\text{scheduled}} = M+2 + L_{\\text{div}}-M-1 = L_{\\text{div}}+1$.\nThis can be expressed as $T_{\\text{scheduled}} = \\max(M+2, L_{\\text{div}}+1)$.\n\nThe number of cycles saved, $\\Delta T$, is the difference between the naive and scheduled execution times:\n$$ \\Delta T = T_{\\text{naive}} - T_{\\text{scheduled}} $$\n$$ \\Delta T = (L_{\\text{div}} + M + 1) - \\max(M+2, L_{\\text{div}}+1) $$\n\nLet's evaluate this based on the relationship between $M$ and $L_{\\text{div}}$.\nCase 1: $M+1 \\ge L_{\\text{div}}$ (or $M \\ge L_{\\text{div}}-1$). This means there are enough independent instructions to completely hide the divide latency.\nIn this case, $\\max(M+2, L_{\\text{div}}+1) = M+2$.\n$\\Delta T = (L_{\\text{div}} + M + 1) - (M+2) = L_{\\text{div}} - 1$.\nThe number of saved cycles equals the number of stall cycles in the naive schedule, which is $L_{\\text{div}}-1$.\n\nCase 2: $M+1  L_{\\text{div}}$ (or $M  L_{\\text{div}}-1$). There are not enough independent instructions to fully hide the latency.\nIn this case, $\\max(M+2, L_{\\text{div}}+1) = L_{\\text{div}}+1$.\n$\\Delta T = (L_{\\text{div}} + M + 1) - (L_{\\text{div}}+1) = M$.\nThe number of saved cycles equals the number of independent instructions, $M$, that could be used to fill the stall slots.\n\nCombining these two cases, the number of cycles saved is the number of stall slots ($L_{\\text{div}}-1$) that can be filled by the available $M$ independent instructions. This is given by:\n$$\\Delta T = \\min(M, L_{\\text{div}}-1)$$\n\nWe are given the values $L_{\\text{div}} = 19$ and $M=12$.\nWe calculate the number of potential stall cycles:\n$L_{\\text{div}}-1 = 19-1 = 18$.\nWe have $M=12$ independent instructions.\nSince $M  L_{\\text{div}}-1$ ($12  18$), we are in the second case. We can only fill $12$ of the $18$ stall slots.\nThe number of cycles saved is:\n$$\\Delta T = \\min(12, 18) = 12$$\n\nThe number of cycles saved per iteration is $12$.", "answer": "$$\n\\boxed{12}\n$$", "id": "3647160"}, {"introduction": "Our final practice [@problem_id:3647154] brings together dependency analysis and latency hiding, applying them to a realistic, resource-constrained superscalar processor. Real hardware has finite resources, such as a limited number of arithmetic units or memory ports. This problem challenges you to construct a schedule that not only respects the critical path of data dependencies but also navigates the limits on instruction issue width and functional unit availability, mirroring the complex task faced by modern compilers.", "problem": "Consider a basic block with $n=8$ instructions to be scheduled on a single-issue core with multiple functional units and explicit resource limits. The goal is to compute a critical path for the dependence structure and to construct a legal schedule that achieves the critical-path length, if possible. The instruction set and machine model are as follows.\n\nMachine model:\n- Up to $2$ instructions may be issued per cycle.\n- There are $2$ load/store units (each can accept at most $1$ memory operation per cycle; the memory system is dual-ported, so up to $2$ memory operations can issue in the same cycle).\n- There is $1$ integer add unit for addition and subtraction.\n- There is $1$ multiply unit.\n- All functional units are fully pipelined.\n- Operation latencies (measured as cycles from issue to result availability) are: load $=$ $2$, add $=$ $1$, multiply $=$ $3$, store $=$ $1$.\n- An instruction may be issued only when all of its operands are available. If instruction $u$ has latency $\\ell(u)$ and is issued at cycle $s(u)$, then its result is available at cycle $r(u)=s(u)+\\ell(u)$.\n- A store requires its value operand to be available at issue time and occupies one load/store unit for $1$ cycle.\n\nBasic block (registers $r_i$ and memory locations $M[\\cdot]$ are architectural):\n- $I_1$: $r_1 \\leftarrow M[a]$ (load)\n- $I_2$: $r_2 \\leftarrow M[b]$ (load)\n- $I_3$: $r_3 \\leftarrow r_1 \\times r_2$ (multiply)\n- $I_4$: $r_4 \\leftarrow r_3 + r_5$ (add), where $r_5$ is already available at cycle $0$\n- $I_5$: $r_6 \\leftarrow r_4 + r_7$ (add), where $r_7$ is already available at cycle $0$\n- $I_6$: $M[c] \\leftarrow r_6$ (store)\n- $I_7$: $r_8 \\leftarrow r_2 + r_9$ (add), where $r_9$ is already available at cycle $0$\n- $I_8$: $r_{10} \\leftarrow r_8 + r_{11}$ (add), where $r_{11}$ is already available at cycle $0$\n\nData dependences:\n- $I_3$ depends on $I_1$ and $I_2$.\n- $I_4$ depends on $I_3$.\n- $I_5$ depends on $I_4$.\n- $I_6$ depends on $I_5$.\n- $I_7$ depends on $I_2$.\n- $I_8$ depends on $I_7$.\n- All other pairs are independent unless implied by the above.\n\nTasks:\n- Using the definitions of data dependence and instruction latency, determine the length (in cycles) of a critical path in the dependence structure of this basic block, ignoring resource contention but respecting data readiness.\n- Then, subject to the resource limits stated above, propose a legal cycle-by-cycle schedule that achieves this critical-path bound if it is achievable.\n- Report the minimal completion time (makespan) in cycles for this basic block under the stated model. Your final answer must be a single integer number of cycles. No rounding is required.", "solution": "The user wants me to solve an instruction scheduling problem, which falls under the purview of compiler principles and computer architecture.\n\n### Step 1: Extract Givens\n- Number of instructions: $n=8$.\n- Machine model:\n  - Issue width: Up to $2$ instructions per cycle.\n  - Functional Units: $2$ load/store units, $1$ integer add unit, $1$ multiply unit. All are fully pipelined.\n- Operation Latencies ($\\ell$):\n  - $\\ell(\\text{load}) = 2$ cycles.\n  - $\\ell(\\text{add}) = 1$ cycle.\n  - $\\ell(\\text{multiply}) = 3$ cycles.\n  - $\\ell(\\text{store}) = 1$ cycle.\n- Timing Model: An instruction $u$ issued at cycle $s(u)$ has its result available at cycle $r(u) = s(u) + \\ell(u)$. A dependent instruction can issue at or after the cycle its operands' results are available.\n- Instructions:\n  - $I_1$: $r_1 \\leftarrow M[a]$ (load)\n  - $I_2$: $r_2 \\leftarrow M[b]$ (load)\n  - $I_3$: $r_3 \\leftarrow r_1 \\times r_2$ (multiply)\n  - $I_4$: $r_4 \\leftarrow r_3 + r_5$ (add)\n  - $I_5$: $r_6 \\leftarrow r_4 + r_7$ (add)\n  - $I_6$: $M[c] \\leftarrow r_6$ (store)\n  - $I_7$: $r_8 \\leftarrow r_2 + r_9$ (add)\n  - $I_8$: $r_{10} \\leftarrow r_8 + r_{11}$ (add)\n- External Inputs: Registers $r_5, r_7, r_9, r_{11}$ are available at cycle $0$.\n- Data Dependences:\n  - $I_3$ depends on $I_1$ and $I_2$. ($I_1 \\to I_3, I_2 \\to I_3$)\n  - $I_4$ depends on $I_3$. ($I_3 \\to I_4$)\n  - $I_5$ depends on $I_4$. ($I_4 \\to I_5$)\n  - $I_6$ depends on $I_5$. ($I_5 \\to I_6$)\n  - $I_7$ depends on $I_2$. ($I_2 \\to I_7$)\n  - $I_8$ depends on $I_7$. ($I_7 \\to I_8$)\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded within computer architecture and compiler theory. It is a well-posed instruction scheduling problem with clearly defined constraints, dependencies, and objectives. The provided data is self-contained, consistent, and sufficient for deriving a solution. The model, though simplified, is a standard representation of a superscalar processor's pipeline scheduling challenge. There are no factual errors, logical contradictions, or subjective elements.\n\n### Step 3: Verdict and Action\nThe problem is valid. A solution will be constructed.\n\n### Solution Derivation\n\nThe primary goal is to find the minimal completion time (makespan) for the given basic block. This is a two-step process: first, determine the lower bound on the makespan by calculating the length of the critical path in the data dependence graph (DDG), ignoring resource constraints. Second, attempt to construct a schedule that meets this lower bound while respecting all resource constraints.\n\n#### Part 1: Critical Path Analysis\nThe critical path is the longest path through the DDG, where the length of a path is the minimum time required to execute the sequence of instructions on that path. The length is determined by the sum of latencies. Let $s(I)$ be the issue cycle of an instruction $I$ and $r(I)$ be the cycle its result is available, with $r(I) = s(I) + \\ell(I)$. For a dependency $I_A \\to I_B$, the earliest issue time for $I_B$ is $s(I_B) \\geq r(I_A)$. We assume execution starts at cycle $1$.\n\nThe dependencies form two main paths in the DDG:\n1.  Path A: $I_1 \\to I_3 \\to I_4 \\to I_5 \\to I_6$ (and the parallel path starting with $I_2$)\n2.  Path B: $I_2 \\to I_7 \\to I_8$\n\nLet's calculate the length of each path, which represents the earliest completion time if only that path existed.\n\n**Calculation for Path A ($I_1 \\to I_3 \\to I_4 \\to I_5 \\to I_6$):**\n- $I_1$ (load, $\\ell=2$) has no dependencies in the block. It can issue at cycle $1$.\n  - Earliest issue time $s(I_1) = 1$.\n  - Result available at $r(I_1) = s(I_1) + \\ell(I_1) = 1 + 2 = 3$.\n- $I_2$ (load, $\\ell=2$) can also issue at cycle $1$. Its result is also available at $r(I_2) = 1 + 2 = 3$.\n- $I_3$ (multiply, $\\ell=3$) depends on $I_1$ and $I_2$. It can issue when both results are ready.\n  - Earliest issue time $s(I_3) = \\max(r(I_1), r(I_2)) = \\max(3, 3) = 3$.\n  - Result available at $r(I_3) = s(I_3) + \\ell(I_3) = 3 + 3 = 6$.\n- $I_4$ (add, $\\ell=1$) depends on $I_3$.\n  - Earliest issue time $s(I_4) = r(I_3) = 6$.\n  - Result available at $r(I_4) = s(I_4) + \\ell(I_4) = 6 + 1 = 7$.\n- $I_5$ (add, $\\ell=1$) depends on $I_4$.\n  - Earliest issue time $s(I_5) = r(I_4) = 7$.\n  - Result available at $r(I_5) = s(I_5) + \\ell(I_5) = 7 + 1 = 8$.\n- $I_6$ (store, $\\ell=1$) depends on $I_5$.\n  - Earliest issue time $s(I_6) = r(I_5) = 8$.\n  - This instruction executes during cycle $8$ and completes at the end of cycle $8$.\nThe total time for this path is $8$ cycles.\n\n**Calculation for Path B ($I_2 \\to I_7 \\to I_8$):**\n- $I_2$ (load, $\\ell=2$) can issue at cycle $1$.\n  - Earliest issue time $s(I_2) = 1$.\n  - Result available at $r(I_2) = 1+2 = 3$.\n- $I_7$ (add, $\\ell=1$) depends on $I_2$.\n  - Earliest issue time $s(I_7) = r(I_2) = 3$.\n  - Result available at $r(I_7) = 3+1 = 4$.\n- $I_8$ (add, $\\ell=1$) depends on $I_7$.\n  - Earliest issue time $s(I_8) = r(I_7) = 4$.\n  - Result available at $r(I_8) = 4+1 = 5$.\nThe last instruction on this path completes at the end of cycle $5$.\n\nThe critical path is the longest path, which is Path A. Therefore, the minimum possible makespan for this basic block, dictated by data dependencies, is $8$ cycles.\n\n#### Part 2: Resource-Constrained Schedule\nNow, we must construct a schedule that respects the resource limits: $2$ issue slots, $2$ load/store units, $1$ add unit, and $1$ multiply unit per cycle. The goal is to achieve the makespan of $8$ cycles.\n\nWe can construct the schedule cycle by cycle:\n\n- **Cycle 1:**\n  - Ready instructions: $I_1, I_2$.\n  - Schedule: Issue $I_1$ (load) and $I_2$ (load).\n  - Resources used: $2$ issue slots, $2$ load/store units. This is valid.\n  - Results available: $r(I_1) = 1+2=3$, $r(I_2) = 1+2=3$.\n\n- **Cycle 2:**\n  - Ready instructions: None. We are waiting for $I_1$ and $I_2$ to complete.\n  - Schedule: Stall.\n  - Resources used: None.\n\n- **Cycle 3:**\n  - Ready instructions: At the start of cycle $3$, the results of $I_1$ and $I_2$ are available. This makes $I_3$ (depends on $I_1, I_2$) and $I_7$ (depends on $I_2$) ready.\n  - Schedule: Issue $I_3$ (multiply) and $I_7$ (add).\n  - Resources used: $2$ issue slots, $1$ multiply unit, $1$ add unit. This is valid.\n  - Results available: $r(I_3) = 3+3=6$, $r(I_7) = 3+1=4$.\n\n- **Cycle 4:**\n  - Ready instructions: At the start of cycle $4$, the result of $I_7$ is available. This makes $I_8$ (depends on $I_7$) ready.\n  - Schedule: Issue $I_8$ (add).\n  - Resources used: $1$ issue slot, $1$ add unit. This is valid.\n  - Result available: $r(I_8) = 4+1=5$.\n\n- **Cycle 5:**\n  - Ready instructions: None. The next instruction on the critical path, $I_4$, is waiting for $I_3$, whose result is not available until cycle $6$. The side path ($I_8$) is complete.\n  - Schedule: Stall.\n  - Resources used: None.\n\n- **Cycle 6:**\n  - Ready instructions: At the start of cycle $6$, the result of $I_3$ is available. This makes $I_4$ (depends on $I_3$) ready.\n  - Schedule: Issue $I_4$ (add).\n  - Resources used: $1$ issue slot, $1$ add unit. This is valid.\n  - Result available: $r(I_4) = 6+1=7$.\n\n- **Cycle 7:**\n  - Ready instructions: At the start of cycle $7$, the result of $I_4$ is available. This makes $I_5$ (depends on $I_4$) ready.\n  - Schedule: Issue $I_5$ (add).\n  - Resources used: $1$ issue slot, $1$ add unit. This is valid.\n  - Result available: $r(I_5) = 7+1=8$.\n\n- **Cycle 8:**\n  - Ready instructions: At the start of cycle $8$, the result of $I_5$ is available. This makes $I_6$ (depends on $I_5$) ready.\n  - Schedule: Issue $I_6$ (store).\n  - Resources used: $1$ issue slot, $1$ load/store unit. This is valid.\n  - The instruction executes in cycle $8$ and completes at the end of cycle $8$.\n\nThe constructed schedule is legal as it respects all data dependencies and resource constraints at every cycle. The final instruction, $I_6$, completes its execution at the end of cycle $8$.\n\n### Conclusion\nThe length of the critical path due to data dependencies establishes a lower bound of $8$ cycles for the makespan. A legal schedule has been constructed that achieves this makespan. Therefore, the minimal completion time for this basic block is $8$ cycles.", "answer": "$$\\boxed{8}$$", "id": "3647154"}]}