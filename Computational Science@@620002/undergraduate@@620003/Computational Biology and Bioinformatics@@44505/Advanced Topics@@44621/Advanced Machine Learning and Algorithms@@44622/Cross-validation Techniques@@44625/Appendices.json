{"hands_on_practices": [{"introduction": "In many biological studies, such as pharmacogenomics, we collect multiple measurements from the same subject over time. A standard random cross-validation split can \"leak\" information between folds by placing data from the same subject into both the training and test sets, leading to an artificially inflated performance estimate. This exercise guides you through implementing Group K-Fold cross-validation, a crucial technique that treats all data from a single subject as an indivisible group, ensuring it remains entirely within one fold. [@problem_id:2383472] By mastering this patient-aware splitting strategy, you will build robust models that provide a more realistic assessment of performance on new, unseen subjects.", "problem": "You are given a self-contained computational task to evaluate a predictive model of a continuous drug response using a fold-based partition that respects patient identity, where all time-point measurements for the same patient must be assigned to the same fold. The setting is a simplified model of temporal gene expression measurements for multiple patients in a pharmacogenomics study.\n\nThere are $6$ patients indexed by the set $\\mathcal{P}=\\{1,2,3,4,5,6\\}$. For each patient $p \\in \\mathcal{P}$, you observe measurements at specific integer time points $t \\in \\mathbb{Z}_{\\ge 0}$ given by the following sets:\n- For $p=1$: $T(1)=\\{0,1,2\\}$.\n- For $p=2$: $T(2)=\\{0,1\\}$.\n- For $p=3$: $T(3)=\\{0,1,2,3\\}$.\n- For $p=4$: $T(4)=\\{0\\}$.\n- For $p=5$: $T(5)=\\{0,1\\}$.\n- For $p=6$: $T(6)=\\{0,1,2\\}$.\n\nFor each observed pair $(p,t)$, define four features without intercept as follows:\n- $g_1 = 1 + 0.1\\,p + 0.05\\,t$,\n- $g_2 = -2 + 0.2\\,p - 0.05\\,t^2$,\n- $g_3 = 0.5\\,p + 0.1\\,t$,\n- $u = t$.\n\nLet the design vector be $x = [1, g_1, g_2, g_3, u] \\in \\mathbb{R}^5$, where the first component is the constant intercept term. The real-valued response for each $(p,t)$ is generated deterministically by\n$$\ny = x^\\top b^\\star + \\varepsilon,\n$$\nwith true coefficients\n$$\nb^\\star = \\begin{bmatrix} 0.3 \\\\ 1.2 \\\\ -0.8 \\\\ 0.5 \\\\ 0.7 \\end{bmatrix},\n$$\nand a deterministic perturbation\n$$\n\\varepsilon = 0.01\\,(p-3)\\,\\big(0.5 - 0.1\\,t\\big).\n$$\n\nFor a given fold count $K \\in \\mathbb{N}$ with $1 \\le K \\le |\\mathcal{P}|$ and a given regularization parameter $\\lambda \\in \\mathbb{R}_{\\ge 0}$, define a partition of the patient set into $K$ disjoint folds as follows. Sort patients in ascending numeric order to obtain the sequence $(1,2,3,4,5,6)$. For each patient with rank $r \\in \\{0,1,2,3,4,5\\}$ in this order, assign the patient to fold index $f = r \\bmod K$. Let $F_0,\\dots,F_{K-1}$ be the resulting disjoint patient subsets. For each fold index $j \\in \\{0,\\dots,K-1\\}$, let the test set be all rows $(p,t)$ with $p \\in F_j$, and let the training set be all remaining rows.\n\nFor each fold, let the training design matrix be $X \\in \\mathbb{R}^{n_{\\text{train}} \\times 5}$ with rows $x^\\top$ as defined above and the training response vector be $y \\in \\mathbb{R}^{n_{\\text{train}}}$. Define the parameter estimate $\\hat b^{(j)} \\in \\mathbb{R}^5$ for fold $j$ to be any minimizer of the objective\n$$\n\\frac{1}{n_{\\text{train}}}\\,\\|X\\,b - y\\|_2^2 + \\lambda\\,\\|R\\,b\\|_2^2,\n$$\nwhere $R \\in \\mathbb{R}^{5 \\times 5}$ is the diagonal matrix with $R_{00}=0$ and $R_{ii}=1$ for all $i \\in \\{1,2,3,4\\}$. For each fold, form predictions $\\hat y$ for the test rows using $\\hat b^{(j)}$ and compute the squared errors. Aggregate the squared errors from all folds and all held-out rows, and let the final score be the mean of these squared errors over the total number of held-out rows across all folds. This mean is the single real-valued result for a given pair $(K,\\lambda)$.\n\nDefine the following test suite, where each test case is a pair $(K,\\lambda)$:\n- Test $1$: $(K,\\lambda) = (3, 0.1)$.\n- Test $2$: $(K,\\lambda) = (6, 0.0)$.\n- Test $3$: $(K,\\lambda) = (2, 1.0)$.\n- Test $4$: $(K,\\lambda) = (4, 0.5)$.\n\nYour program must construct the dataset exactly as specified, perform the fold-based evaluation exactly as defined, and compute the mean of squared errors across all held-out rows for each test case. Your program should produce a single line of output containing the results for Tests $1$ through $4$ in order, formatted as a comma-separated list enclosed in square brackets, with each real number rounded to exactly $6$ digits after the decimal point (for example, $[0.123456,0.000000,1.500000,2.718282]$).", "solution": "The problem is subjected to rigorous validation and is deemed to be valid. It is a scientifically grounded, well-posed, and objective computational task. It is self-contained, with all necessary data and definitions provided. The procedure describes a standard patient-level (or leave-group-out) cross-validation for a regularized linear model, a common technique in biostatistics and bioinformatics to prevent data leakage from correlated measurements within a subject. All mathematical and procedural definitions are precise, forming a sound basis for a unique and verifiable solution.\n\nThe task is to compute the cross-validated mean squared error for a predictive model of a continuous drug response. The solution proceeds in two main stages: first, the complete dataset is generated according to the specified deterministic rules; second, a patient-aware $K$-fold cross-validation is performed for each given test case $(K, \\lambda)$.\n\nFirst, we construct the full dataset. The set of patients is $\\mathcal{P}=\\{1,2,3,4,5,6\\}$. The time points of measurement for each patient $p \\in \\mathcal{P}$ are given by the sets $T(p)$. The total number of observations is $N = \\sum_{p \\in \\mathcal{P}} |T(p)| = 3+2+4+1+2+3 = 15$. For each data point, corresponding to a pair $(p,t)$, we generate a $5$-dimensional feature vector and a scalar response. The features are:\n$$\n\\begin{aligned}\ng_1 &= 1 + 0.1\\,p + 0.05\\,t \\\\\ng_2 &= -2 + 0.2\\,p - 0.05\\,t^2 \\\\\ng_3 &= 0.5\\,p + 0.1\\,t \\\\\nu &= t\n\\end{aligned}\n$$\nThe design vector for each observation is $x = [1, g_1, g_2, g_3, u]^\\top \\in \\mathbb{R}^5$, which includes a constant intercept term.\nThe response $y$ is generated by a linear model with a deterministic perturbation term $\\varepsilon$:\n$$\ny = x^\\top b^\\star + \\varepsilon\n$$\nwhere the true coefficient vector is $b^\\star = [0.3, 1.2, -0.8, 0.5, 0.7]^\\top$ and the perturbation is $\\varepsilon = 0.01\\,(p-3)\\,(0.5 - 0.1\\,t)$. We assemble all $N=15$ observations into a full design matrix $X_{\\text{full}} \\in \\mathbb{R}^{15 \\times 5}$ and a response vector $y_{\\text{full}} \\in \\mathbb{R}^{15}$. We also maintain an array of patient identifiers corresponding to each row.\n\nSecond, for each test case $(K, \\lambda)$, we perform cross-validation. The set of patients is partitioned into $K$ folds. Patients are first sorted in ascending order of their index: $(1,2,3,4,5,6)$. The patient at rank $r \\in \\{0, 1, \\dots, 5\\}$ in this sequence is assigned to fold $f = r \\pmod K$. Let the resulting partition of patients be $F_0, \\dots, F_{K-1}$.\n\nThe cross-validation proceeds by iterating through each fold index $j \\in \\{0, \\dots, K-1\\}$. In each iteration, the data corresponding to patients in $F_j$ constitute the test set, while all remaining data form the training set. Let the training data for fold $j$ be $(X_{\\text{train}}, y_{\\text{train}})$ with $n_{\\text{train}}$ samples, and the test data be $(X_{\\text{test}}, y_{\\text{test}})$.\n\nFor each fold, we must estimate the model parameters $\\hat{b}^{(j)}$ by minimizing the specified regularized least-squares objective function:\n$$\nJ(b) = \\frac{1}{n_{\\text{train}}}\\,\\|X_{\\text{train}}\\,b - y_{\\text{train}}\\|_2^2 + \\lambda\\,\\|R\\,b\\|_2^2\n$$\nThe regularization matrix $R$ is a diagonal matrix with $R_{00}=0$ and $R_{ii}=1$ for $i>0$, implementing Ridge regression that penalizes all coefficients except the intercept. This objective function is convex, and its minimizer can be found by setting its gradient with respect to $b$ to zero:\n$$\n\\nabla_b J(b) = \\frac{2}{n_{\\text{train}}} X_{\\text{train}}^\\top (X_{\\text{train}}b - y_{\\text{train}}) + 2\\lambda R^\\top R b = 0\n$$\nSince $R$ is diagonal, $R^\\top R = R^2 = \\text{diag}(0,1,1,1,1)$. Rearranging the terms, we obtain the normal equations for this regularized problem:\n$$\n\\left(\\frac{1}{n_{\\text{train}}} X_{\\text{train}}^\\top X_{\\text{train}} + \\lambda R^2\\right) b = \\frac{1}{n_{\\text{train}}} X_{\\text{train}}^\\top y_{\\text{train}}\n$$\nMultiplying by $n_{\\text{train}}$ simplifies the system to the standard form for Ridge regression:\n$$\n(X_{\\text{train}}^\\top X_{\\text{train}} + n_{\\text{train}}\\lambda R^2) \\hat{b}^{(j)} = X_{\\text{train}}^\\top y_{\\text{train}}\n$$\nThis linear system is solved for the parameter estimate $\\hat{b}^{(j)}$. The matrix $(X_{\\text{train}}^\\top X_{\\text{train}} + n_{\\text{train}}\\lambda R^2)$ is guaranteed to be invertible for $\\lambda > 0$, and for $\\lambda=0$ it is invertible as long as $X_{\\text{train}}$ has full column rank, which is true for all training sets in this problem.\n\nWith the estimated parameters $\\hat{b}^{(j)}$, predictions for the test set are made: $\\hat{y}_{\\text{test}} = X_{\\text{test}}\\hat{b}^{(j)}$. The squared errors for all data points in the test set are calculated as $(y_{\\text{test}} - \\hat{y}_{\\text{test}})^2$. These squared errors are collected from all $K$ folds. Since each data point belongs to exactly one test set over the course of the cross-validation, we collect a total of $N=15$ squared error values. The final score for the given $(K, \\lambda)$ is the mean of these $N$ values. This procedure is repeated for all four test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_cv_score(K, lam, patient_ids, X_full, y_full):\n    \"\"\"\n    Computes the cross-validated mean squared error for a given K and lambda.\n    \n    Args:\n        K (int): The number of folds.\n        lam (float): The regularization parameter.\n        patient_ids (np.ndarray): Array of patient IDs for each row in X_full.\n        X_full (np.ndarray): The full design matrix.\n        y_full (np.ndarray): The full response vector.\n        \n    Returns:\n        float: The mean squared error across all folds.\n    \"\"\"\n    patients_sorted = sorted(list(set(patient_ids)))\n    patient_to_fold = {p: i % K for i, p in enumerate(patients_sorted)}\n    \n    all_squared_errors = []\n    \n    # Define the regularization matrix R^2\n    R_sq = np.diag([0.0, 1.0, 1.0, 1.0, 1.0])\n    \n    for j in range(K):\n        # Create boolean masks for train/test split based on patient IDs\n        test_mask = np.array([patient_to_fold[p_id] == j for p_id in patient_ids])\n        train_mask = ~test_mask\n        \n        # Split the data\n        X_train, y_train = X_full[train_mask], y_full[train_mask]\n        X_test, y_test = X_full[test_mask], y_full[test_mask]\n        \n        n_train = X_train.shape[0]\n        \n        # Solve the normal equations for Ridge regression\n        # (X_train.T @ X_train + n_train * lam * R^2) @ b = X_train.T @ y_train\n        A = X_train.T @ X_train + n_train * lam * R_sq\n        b_vec = X_train.T @ y_train\n        \n        try:\n            b_hat = np.linalg.solve(A, b_vec)\n        except np.linalg.LinAlgError:\n            # Fallback to pseudoinverse if solve fails, though not expected here\n            A_pinv = np.linalg.pinv(A)\n            b_hat = A_pinv @ b_vec\n\n        # Predict on the test set\n        y_pred = X_test @ b_hat\n        \n        # Calculate and store squared errors\n        squared_errors = (y_test - y_pred) ** 2\n        all_squared_errors.extend(squared_errors.tolist())\n        \n    # The final score is the mean of all collected squared errors\n    mean_squared_error = np.mean(all_squared_errors)\n    \n    return mean_squared_error\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate dataset generation and cross-validation evaluation.\n    \"\"\"\n    # Define test cases: (K, lambda)\n    test_cases = [\n        (3, 0.1),\n        (6, 0.0),\n        (2, 1.0),\n        (4, 0.5),\n    ]\n\n    # Patient time points\n    T = {\n        1: [0, 1, 2],\n        2: [0, 1],\n        3: [0, 1, 2, 3],\n        4: [0],\n        5: [0, 1],\n        6: [0, 1, 2],\n    }\n\n    # True coefficients b_star\n    b_star = np.array([0.3, 1.2, -0.8, 0.5, 0.7])\n\n    # Generate the full dataset\n    data_points = []\n    for p in sorted(T.keys()):\n        for t in T[p]:\n            g1 = 1 + 0.1 * p + 0.05 * t\n            g2 = -2 + 0.2 * p - 0.05 * t**2\n            g3 = 0.5 * p + 0.1 * t\n            u = float(t)\n            \n            x = np.array([1.0, g1, g2, g3, u])\n            \n            eps = 0.01 * (p - 3) * (0.5 - 0.1 * t)\n            y = x @ b_star + eps\n            \n            data_points.append({'p': p, 'x': x, 'y': y})\n\n    patient_ids_full = np.array([dp['p'] for dp in data_points])\n    X_full = np.array([dp['x'] for dp in data_points])\n    y_full = np.array([dp['y'] for dp in data_points])\n\n    results = []\n    for K, lam in test_cases:\n        score = compute_cv_score(K, lam, patient_ids_full, X_full, y_full)\n        results.append(score)\n\n    # Format the output as specified\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "2383472"}, {"introduction": "Beyond subject-level dependencies, model evaluation can be biased by confounding variables that correlate with both features and labels. In genomics, the guanine-cytosine (GC) content of a DNA sequence is a classic example of such a confounder. This practice challenges you to implement a custom stratified cross-validation scheme where the data is first binned by GC content, ensuring that each fold receives a representative sample from each bin. [@problem_id:2383457] This advanced hands-on task demonstrates how to design a CV strategy that actively controls for known confounders, yielding a more accurate and unbiased assessment of a model's true predictive power.", "problem": "A binary classifier is to be evaluated for predicting whether a deoxyribonucleic acid (DNA) sequence is an enhancer (label $1$) or not (label $0$) using a cross-validation scheme that is stratified by guanine-cytosine (GC) content. A data set of $N=12$ DNA windows is provided. For each window $i \\in \\{1,\\dots,12\\}$, the GC proportion $g_i \\in [0,1]$ and the enhancer label $y_i \\in \\{0,1\\}$ are given as follows (the indices are ordered by increasing $g_i$):\n- $i=1$: $g_1=0.30$, $y_1=0$\n- $i=2$: $g_2=0.34$, $y_2=0$\n- $i=3$: $g_3=0.36$, $y_3=0$\n- $i=4$: $g_4=0.40$, $y_4=0$\n- $i=5$: $g_5=0.45$, $y_5=0$\n- $i=6$: $g_6=0.48$, $y_6=1$\n- $i=7$: $g_7=0.52$, $y_7=1$\n- $i=8$: $g_8=0.55$, $y_8=1$\n- $i=9$: $g_9=0.60$, $y_9=1$\n- $i=10$: $g_{10}=0.62$, $y_{10}=1$\n- $i=11$: $g_{11}=0.66$, $y_{11}=1$\n- $i=12$: $g_{12}=0.70$, $y_{12}=1$\n\nThe classifier is the threshold rule $f_{\\tau}$ based solely on GC proportion:\n- For a threshold $\\tau \\in [0,1]$, the predicted label is $\\hat{y}_i = 1$ if $g_i \\ge \\tau$, and $\\hat{y}_i = 0$ otherwise.\n\nThe cross-validation (CV) splitting is defined as follows. For a given number of GC-content bins $B \\in \\mathbb{N}$ and number of folds $K \\in \\mathbb{N}$:\n1. Sort the indices by increasing $g_i$ (the list above is already sorted).\n2. Partition the sorted indices into $B$ contiguous bins whose sizes differ by at most $1$. Formally, let $q = \\left\\lfloor \\frac{N}{B} \\right\\rfloor$ and $r = N \\bmod B$. The first $r$ bins have size $q+1$, and the remaining $B-r$ bins have size $q$, preserving the sorted order.\n3. Within each bin, assign samples to folds in a round-robin manner over folds $0,1,\\dots,K-1$ in the order of the sorted indices: the $t$-th sample in a bin (with $t$ starting at $0$) is assigned to fold $t \\bmod K$.\n4. For fold $k \\in \\{0,\\dots,K-1\\}$, the test set is the union over bins of the indices assigned to fold $k$. The training set is the complement of the test set. If a fold receives an empty test set, it is excluded from the averaging described below.\n\nEvaluation on a test set $S$ uses balanced accuracy. Define the true positive rate (TPR) and true negative rate (TNR) on $S$ by\n$$\n\\mathrm{TPR}(S) = \\begin{cases}\n\\frac{\\#\\{i \\in S : y_i = 1 \\text{ and } \\hat{y}_i = 1\\}}{\\#\\{i \\in S : y_i = 1\\}}, & \\text{if } \\#\\{i \\in S : y_i = 1\\} > 0, \\\\\n0, & \\text{otherwise},\n\\end{cases}\n$$\n$$\n\\mathrm{TNR}(S) = \\begin{cases}\n\\frac{\\#\\{i \\in S : y_i = 0 \\text{ and } \\hat{y}_i = 0\\}}{\\#\\{i \\in S : y_i = 0\\}}, & \\text{if } \\#\\{i \\in S : y_i = 0\\} > 0, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nThe balanced accuracy (BA) on $S$ is\n$$\n\\mathrm{BA}(S) = \\frac{1}{2}\\left(\\mathrm{TPR}(S) + \\mathrm{TNR}(S)\\right).\n$$\nFor CV, compute $\\mathrm{BA}$ on each non-empty foldâ€™s test set and report the arithmetic mean of these fold-wise values.\n\nTest suite. For the fixed data above, evaluate $f_{\\tau}$ under the following three parameter sets $(K,B,\\tau)$:\n- Case $1$: $K=3$, $B=3$, $\\tau=0.50$.\n- Case $2$: $K=4$, $B=3$, $\\tau=0.55$.\n- Case $3$: $K=6$, $B=4$, $\\tau=0.48$.\n\nYour program must implement the binning and fold assignment exactly as specified, evaluate the classifier $f_{\\tau}$ accordingly, and compute the cross-validated balanced accuracy for each case. Final output format: produce a single line containing a comma-separated list of the three balanced accuracies in the order of the cases above, rounded to exactly four decimal places, enclosed in square brackets, for example, $[\\mathrm{result}_1,\\mathrm{result}_2,\\mathrm{result}_3]$ with no spaces.", "solution": "The problem statement is subjected to rigorous validation and is found to be scientifically grounded, well-posed, objective, and complete. All data, parameters, and algorithms are specified with sufficient precision to permit a unique and verifiable solution. The problem describes a standard, albeit specific, procedure for cross-validation in a bioinformatics context, which is a legitimate scientific task. Therefore, we proceed with the solution.\n\nThe dataset consists of $N=12$ samples, each with a guanine-cytosine (GC) proportion $g_i$ and a binary label $y_i$. The data, sorted by $g_i$, is as follows:\n$$\n\\begin{array}{r|*{12}{c}}\n\\text{Index } i & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\\\\n\\hline\n\\text{GC proportion } g_i & 0.30 & 0.34 & 0.36 & 0.40 & 0.45 & 0.48 & 0.52 & 0.55 & 0.60 & 0.62 & 0.66 & 0.70 \\\\\n\\text{Label } y_i & 0 & 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n\\end{array}\n$$\nThe classifier $f_{\\tau}$ predicts a label $\\hat{y}_i=1$ if $g_i \\ge \\tau$ and $\\hat{y}_i=0$ otherwise. The evaluation is based on the average balanced accuracy (BA) across cross-validation folds.\n\nWe analyze each test case as specified.\n\n**Case 1: $K=3$, $B=3$, $\\tau=0.50$**\n\n1.  **Binning**: For $N=12$ and $B=3$, we have $q = \\lfloor 12/3 \\rfloor = 4$ and $r = 12 \\bmod 3 = 0$. Each of the $3$ bins contains $4$ samples.\n    - Bin 1: Indices $\\{1, 2, 3, 4\\}$\n    - Bin 2: Indices $\\{5, 6, 7, 8\\}$\n    - Bin 3: Indices $\\{9, 10, 11, 12\\}$\n\n2.  **Fold Assignment**: For $K=3$, samples are assigned to folds $0, 1, 2$ in a round-robin manner within each bin.\n    - Fold $0$ comprises the $1$st and $4$th samples of each bin: $\\{1, 4, 5, 8, 9, 12\\}$.\n    - Fold $1$ comprises the $2$nd sample of each bin: $\\{2, 6, 10\\}$.\n    - Fold $2$ comprises the $3$rd sample of each bin: $\\{3, 7, 11\\}$.\n\n3.  **Evaluation**: For $\\tau=0.50$, the classifier predicts $\\hat{y}_i = 1$ if $g_i \\ge 0.50$, which corresponds to indices $i \\in \\{7, 8, \\dots, 12\\}$.\n\n    - **Fold 0 Test Set $S_0=\\{1, 4, 5, 8, 9, 12\\}$**:\n        - True negatives ($y_i=0$): $\\{1, 4, 5\\}$; count is $3$. Predictions are $\\hat{y}_1=0, \\hat{y}_4=0, \\hat{y}_5=0$. All are correct (TN). Number of TN is $3$.\n        - True positives ($y_i=1$): $\\{8, 9, 12\\}$; count is $3$. Predictions are $\\hat{y}_8=1, \\hat{y}_9=1, \\hat{y}_{12}=1$. All are correct (TP). Number of TP is $3$.\n        - $\\mathrm{TNR}(S_0) = 3/3 = 1$. $\\mathrm{TPR}(S_0) = 3/3 = 1$.\n        - $\\mathrm{BA}(S_0) = \\frac{1}{2}(1 + 1) = 1$.\n\n    - **Fold 1 Test Set $S_1=\\{2, 6, 10\\}$**:\n        - True negatives ($y_i=0$): $\\{2\\}$; count is $1$. Prediction is $\\hat{y}_2=0$. This is a TN.\n        - True positives ($y_i=1$): $\\{6, 10\\}$; count is $2$. Predictions are $\\hat{y}_6=0$ ($g_6=0.48 < 0.50$, a False Negative) and $\\hat{y}_{10}=1$ ($g_{10}=0.62 \\ge 0.50$, a TP).\n        - $\\mathrm{TNR}(S_1) = 1/1 = 1$. $\\mathrm{TPR}(S_1) = 1/2 = 0.5$.\n        - $\\mathrm{BA}(S_1) = \\frac{1}{2}(1 + 0.5) = 0.75$.\n\n    - **Fold 2 Test Set $S_2=\\{3, 7, 11\\}$**:\n        - True negatives ($y_i=0$): $\\{3\\}$; count is $1$. Prediction is $\\hat{y}_3=0$. This is a TN.\n        - True positives ($y_i=1$): $\\{7, 11\\}$; count is $2$. Predictions are $\\hat{y}_7=1, \\hat{y}_{11}=1$. Both are TP.\n        - $\\mathrm{TNR}(S_2) = 1/1 = 1$. $\\mathrm{TPR}(S_2) = 2/2 = 1$.\n        - $\\mathrm{BA}(S_2) = \\frac{1}{2}(1 + 1) = 1$.\n\n4.  **Average BA**: The mean BA is $\\frac{1}{3}(1 + 0.75 + 1) = \\frac{2.75}{3} \\approx 0.9167$.\n\n**Case 2: $K=4$, $B=3$, $\\tau=0.55$**\n\n1.  **Binning**: Identical to Case 1. Bins are $\\{1,2,3,4\\}, \\{5,6,7,8\\}, \\{9,10,11,12\\}$.\n\n2.  **Fold Assignment**: For $K=4$, samples are assigned to folds $0, 1, 2, 3$.\n    - Fold $0$: $\\{1, 5, 9\\}$. Fold $1$: $\\{2, 6, 10\\}$. Fold $2$: $\\{3, 7, 11\\}$. Fold $3$: $\\{4, 8, 12\\}$.\n\n3.  **Evaluation**: For $\\tau=0.55$, $\\hat{y}_i = 1$ if $g_i \\ge 0.55$, which is for $i \\in \\{8, 9, \\dots, 12\\}$.\n\n    - **Fold 0 Test Set $S_0=\\{1, 5, 9\\}$**:\n        - P: $\\{9\\}$ ($1$ sample), N: $\\{1, 5\\}$ ($2$ samples).\n        - Preds: $\\hat{y}_1=0$ (TN), $\\hat{y}_5=0$ (TN), $\\hat{y}_9=1$ (TP).\n        - $\\mathrm{TNR}(S_0) = 2/2 = 1$. $\\mathrm{TPR}(S_0) = 1/1 = 1$. $\\mathrm{BA}(S_0) = 1$.\n\n    - **Fold 1 Test Set $S_1=\\{2, 6, 10\\}$**:\n        - P: $\\{6, 10\\}$ ($2$ samples), N: $\\{2\\}$ ($1$ sample).\n        - Preds: $\\hat{y}_2=0$ (TN), $\\hat{y}_6=0$ (FN), $\\hat{y}_{10}=1$ (TP).\n        - $\\mathrm{TNR}(S_1) = 1/1 = 1$. $\\mathrm{TPR}(S_1) = 1/2 = 0.5$. $\\mathrm{BA}(S_1) = 0.75$.\n\n    - **Fold 2 Test Set $S_2=\\{3, 7, 11\\}$**:\n        - P: $\\{7, 11\\}$ ($2$ samples), N: $\\{3\\}$ ($1$ sample).\n        - Preds: $\\hat{y}_3=0$ (TN), $\\hat{y}_7=0$ (FN, as $g_7=0.52 < 0.55$), $\\hat{y}_{11}=1$ (TP).\n        - $\\mathrm{TNR}(S_2) = 1/1 = 1$. $\\mathrm{TPR}(S_2) = 1/2 = 0.5$. $\\mathrm{BA}(S_2) = 0.75$.\n\n    - **Fold 3 Test Set $S_3=\\{4, 8, 12\\}$**:\n        - P: $\\{8, 12\\}$ ($2$ samples), N: $\\{4\\}$ ($1$ sample).\n        - Preds: $\\hat{y}_4=0$ (TN), $\\hat{y}_8=1$ (TP), $\\hat{y}_{12}=1$ (TP).\n        - $\\mathrm{TNR}(S_3) = 1/1 = 1$. $\\mathrm{TPR}(S_3) = 2/2 = 1$. $\\mathrm{BA}(S_3) = 1$.\n\n4.  **Average BA**: The mean BA is $\\frac{1}{4}(1 + 0.75 + 0.75 + 1) = \\frac{3.5}{4} = 0.8750$.\n\n**Case 3: $K=6$, $B=4$, $\\tau=0.48$**\n\n1.  **Binning**: For $N=12$ and $B=4$, we have $q = \\lfloor 12/4 \\rfloor = 3$ and $r = 12 \\bmod 4 = 0$. Each of the $4$ bins contains $3$ samples.\n    - Bin 1: $\\{1, 2, 3\\}$. Bin 2: $\\{4, 5, 6\\}$. Bin 3: $\\{7, 8, 9\\}$. Bin 4: $\\{10, 11, 12\\}$.\n\n2.  **Fold Assignment**: For $K=6$, samples are assigned to folds $0, \\dots, 5$.\n    - Fold $0$: $\\{1, 4, 7, 10\\}$. Fold $1$: $\\{2, 5, 8, 11\\}$. Fold $2$: $\\{3, 6, 9, 12\\}$.\n    - Folds $3, 4, 5$ are empty and are excluded from averaging.\n\n3.  **Evaluation**: For $\\tau=0.48$, $\\hat{y}_i = 1$ if $g_i \\ge 0.48$, which is for $i \\in \\{6, 7, \\dots, 12\\}$.\n\n    - **Fold 0 Test Set $S_0=\\{1, 4, 7, 10\\}$**:\n        - P: $\\{7, 10\\}$ ($2$ samples), N: $\\{1, 4\\}$ ($2$ samples).\n        - Preds: $\\hat{y}_1=0$ (TN), $\\hat{y}_4=0$ (TN), $\\hat{y}_7=1$ (TP), $\\hat{y}_{10}=1$ (TP).\n        - $\\mathrm{TNR}(S_0) = 2/2 = 1$. $\\mathrm{TPR}(S_0) = 2/2 = 1$. $\\mathrm{BA}(S_0) = 1$.\n\n    - **Fold 1 Test Set $S_1=\\{2, 5, 8, 11\\}$**:\n        - P: $\\{8, 11\\}$ ($2$ samples), N: $\\{2, 5\\}$ ($2$ samples).\n        - Preds: $\\hat{y}_2=0$ (TN), $\\hat{y}_5=0$ (TN), $\\hat{y}_8=1$ (TP), $\\hat{y}_{11}=1$ (TP).\n        - $\\mathrm{TNR}(S_1) = 2/2 = 1$. $\\mathrm{TPR}(S_1) = 2/2 = 1$. $\\mathrm{BA}(S_1) = 1$.\n\n    - **Fold 2 Test Set $S_2=\\{3, 6, 9, 12\\}$**:\n        - P: $\\{6, 9, 12\\}$ ($3$ samples), N: $\\{3\\}$ ($1$ sample).\n        - Preds: $\\hat{y}_3=0$ (TN), $\\hat{y}_6=1$ (TP), $\\hat{y}_9=1$ (TP), $\\hat{y}_{12}=1$ (TP).\n        - $\\mathrm{TNR}(S_2) = 1/1 = 1$. $\\mathrm{TPR}(S_2) = 3/3 = 1$. $\\mathrm{BA}(S_2) = 1$.\n\n4.  **Average BA**: The mean BA is $\\frac{1}{3}(1 + 1 + 1) = 1.0000$.\n\nThe final results are $0.9167$, $0.8750$, and $1.0000$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the cross-validation problem for the three specified test cases.\n    \"\"\"\n    # Data as given in the problem statement.\n    # The data is 1-indexed in the problem; we use 0-indexed lists.\n    # The order is already sorted by GC content.\n    data = np.array([\n        (0.30, 0), (0.34, 0), (0.36, 0), (0.40, 0), (0.45, 0), (0.48, 1),\n        (0.52, 1), (0.55, 1), (0.60, 1), (0.62, 1), (0.66, 1), (0.70, 1)\n    ])\n    N = len(data)\n\n    test_cases = [\n        {'K': 3, 'B': 3, 'tau': 0.50},\n        {'K': 4, 'B': 3, 'tau': 0.55},\n        {'K': 6, 'B': 4, 'tau': 0.48},\n    ]\n\n    results = []\n    for params in test_cases:\n        K = params['K']\n        B = params['B']\n        tau = params['tau']\n\n        # Step 1  2: Binning\n        # Since data is already sorted, we just partition the indices.\n        q = N // B\n        r = N % B\n        \n        bins = []\n        current_idx = 0\n        for i in range(B):\n            bin_size = q + 1 if i  r else q\n            bins.append(list(range(current_idx, current_idx + bin_size)))\n            current_idx += bin_size\n            \n        # Step 3: Fold Assignment\n        folds = [[] for _ in range(K)]\n        for bin_indices in bins:\n            for t, sample_idx in enumerate(bin_indices):\n                fold_idx = t % K\n                folds[fold_idx].append(sample_idx)\n        \n        # Step 4: Evaluation\n        fold_accuracies = []\n        num_valid_folds = 0\n        \n        for k in range(K):\n            test_indices = folds[k]\n            if not test_indices:\n                continue\n            \n            num_valid_folds += 1\n            test_set = data[test_indices]\n            \n            # Get true labels and predicted labels for the test set\n            true_labels = test_set[:, 1]\n            gc_values = test_set[:, 0]\n            predicted_labels = (gc_values >= tau).astype(int)\n            \n            # Count positives and negatives in the test set\n            pos_mask = true_labels == 1\n            neg_mask = true_labels == 0\n            \n            num_pos = np.sum(pos_mask)\n            num_neg = np.sum(neg_mask)\n            \n            # Calculate True Positives (TP) and True Negatives (TN)\n            tp = np.sum((predicted_labels == 1)  pos_mask)\n            tn = np.sum((predicted_labels == 0)  neg_mask)\n            \n            # Calculate TPR and TNR with division-by-zero handling\n            tpr = tp / num_pos if num_pos > 0 else 0.0\n            tnr = tn / num_neg if num_neg > 0 else 0.0\n            \n            # Calculate Balanced Accuracy\n            ba = 0.5 * (tpr + tnr)\n            fold_accuracies.append(ba)\n            \n        # Compute mean balanced accuracy across non-empty folds\n        mean_ba = np.mean(fold_accuracies) if fold_accuracies else 0.0\n        results.append(mean_ba)\n\n    # Format the final output string as required\n    formatted_results = [f\"{res:.4f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2383457"}, {"introduction": "Choosing the number of folds, $k$, in cross-validation is a critical decision that influences the bias and variance of your generalization error estimate. Leave-One-Out Cross-Validation (LOOCV), an extreme case where $k$ equals the number of samples ($k=n$), provides a nearly unbiased estimate but can suffer from high variance, making the evaluation unstable. This simulation exercise allows you to investigate this fundamental trade-off empirically. [@problem_id:2383426] By generating multiple synthetic datasets and comparing the variance of error estimates from LOOCV versus $10$-fold CV, you will gain a practical and intuitive understanding of why a moderate $k$ is often preferred in practice.", "problem": "You are studying the reliability of Cross-Validation (CV) estimates of generalization error in a setting motivated by computational biology and bioinformatics, where gene expression values are predicted from sequence-derived features. Consider a purely synthetic data-generating process in which there is no true predictive signal from features to response, reflecting a null model often used as a calibration baseline in practice.\n\nData-generating process. For a given sample size $n$, feature dimension $p$, and noise scale $\\sigma  0$, an independent and identically distributed dataset $S = \\{(x_i, y_i)\\}_{i=1}^n$ is drawn as follows: each feature vector $x_i \\in \\mathbb{R}^p$ has coordinates sampled independently from a standard normal distribution, and each response $y_i \\in \\mathbb{R}$ is sampled independently from a normal distribution with mean $0$ and variance $\\sigma^2$, independently of $x_i$. The features may be interpreted as motif-derived or k-mer-based numeric summaries, while the response represents a gene expression measurement, all formalized as independent random variables for this synthetic setup.\n\nModel class and loss. For any training subset $T \\subset \\{1,\\dots,n\\}$, define the ordinary least squares linear model with intercept as the function $f_{\\hat{\\beta}_T}(x) = \\hat{\\beta}_{0,T} + \\sum_{j=1}^p \\hat{\\beta}_{j,T} x_j$, where the parameter vector $\\hat{\\beta}_T$ minimizes the average squared error over the training subset $T$. The loss for a prediction $f_{\\hat{\\beta}_T}(x)$ at a target $y$ is the squared error $(y - f_{\\hat{\\beta}_T}(x))^2$.\n\nCross-validated generalization error estimates. For a fixed dataset $S$, define:\n- The Leave-One-Out Cross-Validation (LOOCV) estimate of mean squared error as the average of squared errors obtained by training on the set $\\{1,\\dots,n\\} \\setminus \\{i\\}$ and validating on index $i$, averaged over all $i \\in \\{1,\\dots,n\\}$.\n- The $k$-fold Cross-Validation estimate with $k=10$ as follows. Partition $\\{1,\\dots,n\\}$ into $k$ folds that are as equal in size as possible by assigning the first $n \\bmod k$ folds an extra element, and otherwise partitioning in sequential index order. For each fold, train the model on the complement of that fold and compute squared errors on the held-out fold. The $10$-fold CV estimate is the average of all squared errors across the $k$ folds.\n\nSampling variance across independent datasets. For a fixed parameter tuple $(n,p,\\sigma)$, consider $R \\ge 2$ independent datasets $S^{(1)},\\dots,S^{(R)}$ generated from the data-generating process above. For each dataset $S^{(r)}$, compute both the LOOCV estimate and the $10$-fold CV estimate. Over these $R$ values, compute the unbiased sample variance of the LOOCV estimates and the unbiased sample variance of the $10$-fold CV estimates.\n\nTask. For each parameter tuple in the test suite below, generate $R$ independent datasets with the specified random seed (use a pseudorandom number generator initialized with the given seed for reproducibility), compute the two sample variances as defined, and output a boolean indicating whether the LOOCV variance is strictly larger than the $10$-fold CV variance by a margin of at least $10^{-12}$. Formally, output $\\mathrm{True}$ if $\\mathrm{Var}_{\\text{LOOCV}} - \\mathrm{Var}_{10\\text{-fold}} > 10^{-12}$, and $\\mathrm{False}$ otherwise.\n\nTest suite. Use the following three parameter tuples, each given as $(\\text{seed}, n, p, \\sigma, R)$:\n- $(1337, 60, 8, 1.0, 200)$,\n- $(20231102, 100, 12, 2.0, 150)$,\n- $(987654321, 20, 3, 1.5, 400)$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite listed above. Each entry must be a boolean. For example, a valid output looks like $[\\mathrm{True},\\mathrm{False},\\mathrm{True}]$.", "solution": "The problem requires a comparative analysis of the sampling variance of two cross-validation estimators for generalization error: Leave-One-Out Cross-Validation (LOOCV) and $10$-fold Cross-Validation ($10$-fold CV). This analysis is to be performed on synthetically generated data that represents a statistical null model, where features have no predictive power over the response variable. The analysis must be conducted for several specified parameter sets.\n\nThe procedural framework for solving this problem is as follows:\n\n1.  **Simulation Setup**: For each parameter tuple $(\\text{seed}, n, p, \\sigma, R)$, where $\\text{seed}$ is the random number generator seed, $n$ is the sample size, $p$ is the feature dimension, $\\sigma$ is the noise standard deviation, and $R$ is the number of independent simulation runs, we will perform $R$ distinct trials. A pseudorandom number generator initialized with the given $\\text{seed}$ ensures the reproducibility of the entire experiment.\n\n2.  **Data Generation**: In each trial $r \\in \\{1, \\dots, R\\}$, we generate a dataset $S^{(r)} = \\{(x_i, y_i)\\}_{i=1}^n$. Each feature vector $x_i \\in \\mathbb{R}^p$ is drawn such that its components $x_{ij}$ are independent and identically distributed (i.i.d.) from a standard normal distribution, $x_{ij} \\sim \\mathcal{N}(0, 1)$. Each response $y_i \\in \\mathbb{R}$ is drawn i.i.d. from a normal distribution with mean $0$ and variance $\\sigma^2$, i.e., $y_i \\sim \\mathcal{N}(0, \\sigma^2)$. The features and responses are independent of each other.\n\n3.  **Model Specification**: The model is an ordinary least squares (OLS) linear regression model with an intercept term. For a given training set, the model function is $f_{\\hat{\\beta}}(x) = \\hat{\\beta}_0 + \\sum_{j=1}^p \\hat{\\beta}_j x_j$. The coefficient vector $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_p)^T$ is estimated by minimizing the sum of squared errors on the training data. This is achieved by solving the normal equations. For an augmented design matrix $X_{\\text{aug}}$ (with a leading column of ones) and response vector $y$, the solution is $\\hat{\\beta} = (X_{\\text{aug}}^T X_{\\text{aug}})^{-1} X_{\\text{aug}}^T y$. We will use a numerically stable solver, such as one based on QR decomposition, for this computation.\n\n4.  **Leave-One-Out Cross-Validation (LOOCV) Error**: For each dataset $S^{(r)}$, we calculate the LOOCV estimate of the mean squared error (MSE). A naive implementation would involve fitting the OLS model $n$ times. However, for linear models, a highly efficient analytical shortcut exists. The LOOCV prediction for the $i$-th data point, $\\hat{y}_{(-i)}$, can be computed from the results of a single model fit on the entire dataset. Let $\\hat{y}_i$ be the prediction for the $i$-th point from the full model, and let $h_{ii}$ be the $i$-th diagonal element of the hat matrix $H = X_{\\text{aug}}(X_{\\text{aug}}^T X_{\\text{aug}})^{-1}X_{\\text{aug}}^T$. The LOOCV residual is given by:\n    $$ y_i - \\hat{y}_{(-i)} = \\frac{y_i - \\hat{y}_i}{1 - h_{ii}} $$\n    The diagonal elements $h_{ii}$, known as leverages, can be computed efficiently from the $Q$ matrix of the QR decomposition of $X_{\\text{aug}}$. If $X_{\\text{aug}} = QR$, then $H=QQ^T$, and $h_{ii}$ is the squared Euclidean norm of the $i$-th row of $Q$. The LOOCV MSE estimate is the mean of these squared residuals:\n    $$ \\text{LOOCV}(S^{(r)}) = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - \\hat{y}_i}{1 - h_{ii}} \\right)^2 $$\n    This computation is performed for each of the $R$ datasets, yielding a sequence of $R$ estimates.\n\n5.  **$10$-fold Cross-Validation Error**: For the same dataset $S^{(r)}$, we also compute the $10$-fold CV estimate. The dataset of $n$ samples is partitioned into $k=10$ folds. The problem specifies a sequential partitioning scheme where folds are as equal in size as possible. Let the folds be $F_1, \\dots, F_{10}$. For each fold $F_j$, an OLS model is trained on the data from the remaining $9$ folds ($S \\setminus F_j$). This model is then used to predict the responses for the data in the hold-out fold $F_j$. The $10$-fold CV estimate is the average of the squared prediction errors over all $n$ data points.\n    $$ 10\\text{-fold CV}(S^{(r)}) = \\frac{1}{n} \\sum_{j=1}^{10} \\sum_{(x_i, y_i) \\in F_j} (y_i - f_{\\hat{\\beta}_{S \\setminus F_j}}(x_i))^2 $$\n    This process is repeated for each of the $R$ datasets.\n\n6.  **Sampling Variance Calculation**: After $R$ trials, we have two collections of estimates: $\\{\\text{LOOCV}(S^{(r)})\\}_{r=1}^R$ and $\\{10\\text{-fold CV}(S^{(r)})\\}_{r=1}^R$. We compute the unbiased sample variance for each collection. For a generic set of estimates $\\{E_r\\}_{r=1}^R$, the unbiased sample variance is:\n    $$ \\text{Var} = \\frac{1}{R-1} \\sum_{r=1}^R (E_r - \\bar{E})^2, \\quad \\text{where } \\bar{E} = \\frac{1}{R} \\sum_{r=1}^R E_r $$\n    This yields two values: $\\mathrm{Var}_{\\text{LOOCV}}$ and $\\mathrm{Var}_{10\\text{-fold}}$.\n\n7.  **Final Comparison**: The task is to determine if the variance of the LOOCV estimator is strictly greater than that of the $10$-fold CV estimator by a small margin of $10^{-12}$. For each parameter set, we evaluate the boolean condition:\n    $$ \\mathrm{Var}_{\\text{LOOCV}} - \\mathrm{Var}_{10\\text{-fold}} > 10^{-12} $$\n    The results from all test cases are compiled into a list.\n\nTheoretically, LOOCV is expected to have a higher sampling variance than $k$-fold CV (for small $k$) because the training sets for each fold in LOOCV are nearly identical, differing by only one sample. This high overlap leads to highly correlated models and, consequently, highly correlated error estimates. The variance of an average of correlated random variables is higher than that of less correlated ones. $10$-fold CV uses training sets with less overlap, which typically results in a more stable (lower variance) estimate of the generalization error across different initial samplings of the data. Our computation will verify this heuristic for the given scenarios.", "answer": "```python\nimport numpy as np\n\ndef calculate_loocv_mse(X: np.ndarray, y: np.ndarray) -> float:\n    \"\"\"\n    Calculates the Leave-One-Out Cross-Validation Mean Squared Error for OLS\n    using the efficient analytical shortcut.\n    \"\"\"\n    n, p = X.shape\n    X_aug = np.hstack([np.ones((n, 1)), X])\n\n    # Fit OLS model on the full dataset\n    beta_hat, _, _, _ = np.linalg.lstsq(X_aug, y, rcond=None)\n    y_pred_full = X_aug @ beta_hat\n\n    # Calculate leverages (diagonal of the hat matrix) using QR decomposition\n    q, _ = np.linalg.qr(X_aug, mode='reduced')\n    leverages = np.sum(q * q, axis=1)\n\n    # Handle cases where leverage is close to 1\n    # This might happen with duplicate data points, though unlikely here.\n    # We clip to prevent division by zero or very small numbers.\n    leverages = np.clip(leverages, 0, 1 - 1e-9)\n\n    # Calculate LOOCV residuals and MSE\n    loocv_residuals = (y - y_pred_full) / (1 - leverages)\n    return np.mean(loocv_residuals**2)\n\ndef calculate_kfold_cv_mse(X: np.ndarray, y: np.ndarray, k: int) -> float:\n    \"\"\"\n    Calculates the k-fold Cross-Validation Mean Squared Error for OLS.\n    \"\"\"\n    n = X.shape[0]\n    indices = np.arange(n)\n    \n    # Define fold sizes as per the problem description\n    fold_sizes = np.full(k, n // k, dtype=int)\n    fold_sizes[:n % k] += 1\n    \n    current_idx = 0\n    all_squared_errors = []\n    \n    for fold_size in fold_sizes:\n        test_indices = indices[current_idx : current_idx + fold_size]\n        train_indices = np.setdiff1d(indices, test_indices, assume_unique=True)\n        \n        X_train, y_train = X[train_indices], y[train_indices]\n        X_test, y_test = X[test_indices], y[test_indices]\n        \n        # Augment data with intercept term\n        X_train_aug = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n        X_test_aug = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n        \n        # Fit OLS model on the training fold\n        beta_hat, _, _, _ = np.linalg.lstsq(X_train_aug, y_train, rcond=None)\n        \n        # Predict on the test fold\n        y_pred = X_test_aug @ beta_hat\n        \n        squared_errors = (y_test - y_pred)**2\n        all_squared_errors.extend(squared_errors.tolist())\n        \n        current_idx += fold_size\n        \n    return np.mean(all_squared_errors)\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation for all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        (1337, 60, 8, 1.0, 200),\n        (20231102, 100, 12, 2.0, 150),\n        (987654321, 20, 3, 1.5, 400),\n    ]\n\n    final_results = []\n\n    for seed, n, p, sigma, R in test_cases:\n        rng = np.random.default_rng(seed)\n        \n        loocv_estimates = np.zeros(R)\n        kfold_estimates = np.zeros(R)\n        \n        for r_idx in range(R):\n            # Generate dataset S = {(x_i, y_i)}\n            X = rng.normal(loc=0, scale=1, size=(n, p))\n            y = rng.normal(loc=0, scale=sigma, size=n)\n            \n            # Compute LOOCV estimate for this dataset\n            loocv_estimates[r_idx] = calculate_loocv_mse(X, y)\n            \n            # Compute 10-fold CV estimate for this dataset\n            kfold_estimates[r_idx] = calculate_kfold_cv_mse(X, y, k=10)\n            \n        # Compute unbiased sample variance over the R datasets\n        # ddof=1 for unbiased sample variance (division by R-1)\n        var_loocv = np.var(loocv_estimates, ddof=1)\n        var_kfold = np.var(kfold_estimates, ddof=1)\n        \n        # Perform the comparison and store the boolean result\n        is_larger = (var_loocv - var_kfold) > 1e-12\n        final_results.append(is_larger)\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n```", "id": "2383426"}]}