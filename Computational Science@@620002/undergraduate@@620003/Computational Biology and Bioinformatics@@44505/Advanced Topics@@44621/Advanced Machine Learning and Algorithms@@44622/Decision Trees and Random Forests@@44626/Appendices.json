{"hands_on_practices": [{"introduction": "The journey from theoretical knowledge to practical skill often begins with implementation. This first exercise challenges you to build a Random Forest classifier from the ground up, following a precise mathematical and algorithmic specification [@problem_id:2384429]. By coding the core components like Gini impurity calculation, node splitting, and tree aggregation, you will gain an intimate understanding of how these powerful models work under the hood, using a practical example from cheminformatics.", "problem": "Construct a complete, runnable program that implements a binary classifier based on an ensemble of decision trees for a cheminformatics classification task. The classifier must follow the mathematical definition of a Random Forest (RF), interpreted here as a majority vote of axis-aligned, binary, finite-depth decision trees (Decision Trees (DT)) trained on a fixed training set of compounds represented by structural descriptors. The task is to predict whether a compound is fluorescent, encoded as class label $1$ for fluorescent and $0$ for non-fluorescent.\n\nDefinitions and data:\n- Each compound is represented by a feature vector $\\mathbf{x} \\in \\mathbb{R}^d$ with $d=2$ features:\n  - Feature $0$: longest conjugated system length in alternating bonds (integer).\n  - Feature $1$: electron donor atom count (integer).\n- The labeled training set is $\\mathcal{D}=\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ with $n=8$ and labels $y_i \\in \\{0,1\\}$. The training examples are:\n  - $\\mathbf{x}_1=(3,0)$ with $y_1=0$.\n  - $\\mathbf{x}_2=(4,1)$ with $y_2=0$.\n  - $\\mathbf{x}_3=(5,0)$ with $y_3=0$.\n  - $\\mathbf{x}_4=(6,1)$ with $y_4=1$.\n  - $\\mathbf{x}_5=(7,1)$ with $y_5=1$.\n  - $\\mathbf{x}_6=(8,2)$ with $y_6=1$.\n  - $\\mathbf{x}_7=(9,1)$ with $y_7=1$.\n  - $\\mathbf{x}_8=(10,0)$ with $y_8=1$.\n- The query set for which predictions must be returned is the following list of three compounds:\n  - $\\mathbf{q}_1=(5,1)$.\n  - $\\mathbf{q}_2=(6,0)$.\n  - $\\mathbf{q}_3=(8,1)$.\n\nMathematical specification of the decision tree classifier $h$:\n- A node $v$ is associated with an index set $S_v \\subseteq \\{1,\\dots,n\\}$ of training examples that reach $v$ and a depth $\\mathrm{depth}(v)\\in \\mathbb{N}_0$ with the root having $\\mathrm{depth}=0$ and $S_{\\text{root}}=\\{1,\\dots,n\\}$.\n- For any node $v$, define the class counts $c_k(v)=|\\{i\\in S_v: y_i=k\\}|$ for $k\\in\\{0,1\\}$, the size $|S_v|$, and the empirical class probabilities $p_k(v)=c_k(v)/|S_v|$. The Gini impurity at $v$ is\n  $$G(v)=1-\\sum_{k\\in\\{0,1\\}} p_k(v)^2.$$\n- Candidate axis-aligned splits at a node $v$ are defined as follows. For each feature index $j\\in\\{0,1\\}$ and the multiset $\\{x_{i,j} : i\\in S_v\\}$, take its sorted list of unique values $z_1<z_2<\\dots<z_m$. The candidate thresholds are the midpoints\n  $$\\tau_\\ell=\\frac{z_\\ell+z_{\\ell+1}}{2}\\quad \\text{for }\\ell\\in\\{1,\\dots,m-1\\}.$$\n  A binary split is parameterized by a pair $(j,\\tau)$ and partitions $S_v$ into\n  $$S_v^{\\text{L}}(j,\\tau)=\\{i\\in S_v: x_{i,j}\\le \\tau\\},\\quad S_v^{\\text{R}}(j,\\tau)=\\{i\\in S_v: x_{i,j}>\\tau\\}.$$\n  A candidate split is valid only if $|S_v^{\\text{L}}(j,\\tau)|\\ge n_{\\min}$ and $|S_v^{\\text{R}}(j,\\tau)|\\ge n_{\\min}$ where $n_{\\min}\\in \\mathbb{N}$ is the minimum leaf size parameter.\n- For any valid split $(j,\\tau)$ at $v$, define the weighted post-split impurity\n  $$\\Phi(v;j,\\tau)=\\frac{|S_v^{\\text{L}}(j,\\tau)|}{|S_v|}G\\big(v^{\\text{L}}(j,\\tau)\\big)+\\frac{|S_v^{\\text{R}}(j,\\tau)|}{|S_v|}G\\big(v^{\\text{R}}(j,\\tau)\\big),$$\n  where $v^{\\text{L}}(j,\\tau)$ and $v^{\\text{R}}(j,\\tau)$ denote the left and right child nodes formed by the split. Among the valid candidate splits, select the one that minimizes $\\Phi(v;j,\\tau)$. If there are ties in $\\Phi$, break them by choosing the smallest threshold $\\tau$, and if still tied, by choosing the smallest feature index $j$.\n- Feature subsampling at each internal node: let $m_{\\text{try}}\\in\\{1,2\\}$ be the number of features to consider at each split. For a node $v$, select a subset $F_v\\subseteq\\{0,1\\}$ of size $m_{\\text{try}}$ uniformly at random without replacement if $m_{\\text{try}}<2$; otherwise take all features if $m_{\\text{try}}=2$. Restrict candidate splits to $j\\in F_v$.\n- Stopping rules: a node $v$ becomes a leaf if any of the following holds: $G(v)=0$, or $\\mathrm{depth}(v)\\ge D_{\\max}$ where $D_{\\max}\\in \\mathbb{N}_0$ is the maximum depth parameter, or there is no valid split due to the $n_{\\min}$ constraint or because $|S_v|<2$. A leaf $v$ predicts the class\n  $$\\hat{y}(v)=\\arg\\max_{k\\in\\{0,1\\}} c_k(v),$$\n  with ties resolved in favor of class $0$.\n- A Decision Tree $h$ maps any $\\mathbf{x}\\in\\mathbb{R}^2$ to a class in $\\{0,1\\}$ by starting at the root and repeatedly applying the rule \"go to the left child if $x_j\\le \\tau$, else go to the right child\" using the stored $(j,\\tau)$ at each internal node, until a leaf is reached; the output is the leaf’s $\\hat{y}(v)$.\n\nRandom Forest aggregation:\n- An RF with $T\\in\\mathbb{N}$ trees is the function\n  $$H(\\mathbf{x})=\\mathrm{majority}\\big(h_1(\\mathbf{x}),h_2(\\mathbf{x}),\\dots,h_T(\\mathbf{x})\\big),$$\n  where ties are resolved in favor of class $0$. Each tree $h_t$ is trained on the training set $\\mathcal{D}$ either with or without bootstrapping as specified per test case: bootstrapping \"off\" uses the full training set indices $S_{\\text{root}}=\\{1,\\dots,n\\}$; bootstrapping \"on\" uses a sample of size $n$ drawn with replacement from $\\{1,\\dots,n\\}$. For reproducibility, each test case specifies an integer seed $s\\in\\mathbb{Z}$; when bootstrapping is on, use independent replicas seeded deterministically per tree index (for example, by offsetting $s$ by the tree index). Feature subsampling follows the rule above. All other choices, splits, and tie-breaks must follow the definitions stated here to ensure determinism.\n\nTest suite:\n- Use the training data and query set listed above. Evaluate the RF on the following three parameter sets. In each tuple, the parameters are $(T, D_{\\max}, m_{\\text{try}}, n_{\\min}, s, \\text{bootstrap})$:\n  - Case A (general case): $(7, 3, 2, 1, 13, \\text{False})$.\n  - Case B (boundary depth): $(1, 1, 2, 1, 7, \\text{False})$.\n  - Case C (edge case with minimum leaf size blocking any split): $(5, 3, 2, 5, 5, \\text{False})$.\n- For each case, compute the RF predictions $H(\\mathbf{q}_1)$, $H(\\mathbf{q}_2)$, and $H(\\mathbf{q}_3)$ as integers in $\\{0,1\\}$, in that order.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all three cases as a comma-separated list of three lists, each inner list giving the three predictions in order for $(\\mathbf{q}_1,\\mathbf{q}_2,\\mathbf{q}_3)$. The exact required format is\n  $$\\texttt{[[a\\_1,a\\_2,a\\_3],[b\\_1,b\\_2,b\\_3],[c\\_1,c\\_2,c\\_3]]}$$\n  where each $a_i$, $b_i$, $c_i$ is an integer in $\\{0,1\\}$ with no spaces anywhere in the line.", "solution": "The problem statement has been rigorously validated and is determined to be **valid**. It is a self-contained, scientifically grounded, and well-posed problem in computational science, providing a complete specification for constructing a Random Forest classifier. All necessary data, mathematical definitions, hyperparameters, and tie-breaking rules are provided, ensuring a unique and deterministic solution.\n\nThe task is to build a Random Forest ($H$) classifier, which is an ensemble of $T$ decision trees ($h_t$). The prediction of the forest is the majority vote over the predictions of individual trees. The problem statement provides three test cases with specific hyperparameters. A critical observation for all three test cases is that the `bootstrap` parameter is set to `False` and the feature subsampling parameter $m_{\\text{try}}$ is set to $2$, which is the total number of features ($d=2$). This has a significant consequence:\n- **No Bootstrapping**: Each tree $h_t$ in the forest is trained on the exact same full training dataset $\\mathcal{D}$.\n- **No Feature Subsampling**: At every node of every tree, all $d=2$ features are considered for splitting.\nSince the tree construction algorithm, including all tie-breaking rules, is deterministic, all $T$ trees within a single Random Forest instance will be identical. Therefore, for any given test case, the Random Forest prediction $H(\\mathbf{x})$ simplifies to the prediction of a single decision tree $h(\\mathbf{x})$ built with the corresponding hyperparameters. The parameter $T$ (number of trees) becomes irrelevant for the final prediction.\n\nOur procedure will be to construct a single decision tree for each of the three test cases using the specified parameters ($D_{\\max}, n_{\\min}$) and then use it to classify the three query vectors.\n\nA decision tree is built recursively, starting from the root node which contains all training data indices $S_{\\text{root}} = \\{1, 2, \\dots, 8\\}$. For any node $v$ with associated training indices $S_v$, we perform the following steps:\n\n1.  **Check Stopping Criteria**: The node $v$ becomes a leaf if:\n    a. The node is pure, i.e., its Gini impurity $G(v) = 1 - \\sum_{k \\in \\{0,1\\}} p_k(v)^2$ is $0$.\n    b. The node's depth reaches the maximum allowed depth, $\\mathrm{depth}(v) \\ge D_{\\max}$.\n    c. There are no valid splits possible from this node. A split is valid if it partitions the node's data of size $|S_v|$ into two children of size $|S_v^{\\text{L}}|$ and $|S_v^{\\text{R}}|$, such that $|S_v^{\\text{L}}| \\ge n_{\\min}$ and $|S_v^{\\text{R}}| \\ge n_{\\min}$.\n\n    If a node is a leaf, its prediction $\\hat{y}(v)$ is the majority class among the samples in $S_v$. Ties are broken in favor of class $0$.\n\n2.  **Find Best Split**: If no stopping criteria are met, we search for the optimal split $(j^*, \\tau^*)$ that minimizes the weighted Gini impurity:\n    $$ \\Phi(v; j, \\tau) = \\frac{|S_v^{\\text{L}}(j,\\tau)|}{|S_v|}G(v^{\\text{L}}(j,\\tau)) + \\frac{|S_v^{\\text{R}}(j,\\tau)|}{|S_v|}G(v^{\\text{R}}(j,\\tau)) $$\n    The search is performed over all features $j \\in \\{0, 1\\}$ and all valid candidate thresholds $\\tau$. Thresholds are midpoints between sorted unique feature values. The tie-breaking rule is to choose the split with the smallest threshold $\\tau$, and then the smallest feature index $j$.\n\n3.  **Recurse**: The node $v$ is made an internal node storing $(j^*, \\tau^*)$. Two children are created by partitioning $S_v$ into $S_v^{\\text{L}}(j^*,\\tau^*)$ and $S_v^{\\text{R}}(j^*,\\tau^*)$, and the tree construction process is recursively applied to them at $\\mathrm{depth}(v)+1$.\n\nThe training data consists of $n=8$ samples with labels $y \\in \\{0,1\\}$:\n$\\mathcal{D} = \\{(\\mathbf{x}_1=(3,0), y_1=0), (\\mathbf{x}_2=(4,1), y_2=0), (\\mathbf{x}_3=(5,0), y_3=0), (\\mathbf{x}_4=(6,1), y_4=1), (\\mathbf{x}_5=(7,1), y_5=1), (\\mathbf{x}_6=(8,2), y_6=1), (\\mathbf{x}_7=(9,1), y_7=1), (\\mathbf{x}_8=(10,0), y_8=1)\\}$.\nThe root node contains indices $\\{1, \\dots, 8\\}$ with labels $\\{0,0,0,1,1,1,1,1\\}$. The class counts are $c_0=3, c_1=5$. The initial Gini impurity is $G(\\text{root}) = 1 - ((3/8)^2 + (5/8)^2) = 30/64 \\approx 0.46875$.\n\nLet us analyze each case:\n\n**Case A: $(T=7, D_{\\max}=3, m_{\\text{try}}=2, n_{\\min}=1)$ and Case B: $(T=1, D_{\\max}=1, m_{\\text{try}}=2, n_{\\min}=1)$**\n\nFor both cases, $n_{\\min}=1$, so any split that creates non-empty children is valid. We seek the best split for the root node.\n- **Feature $j=0$**: Values are $\\{3,4,5,6,7,8,9,10\\}$. A candidate split is at $\\tau=5.5$.\n  - Left child ($x_0 \\le 5.5$): Indices $\\{1,2,3\\}$, labels $\\{0,0,0\\}$. This set is pure, so $G(v^{\\text{L}})=0$. Size is $3$.\n  - Right child ($x_0 > 5.5$): Indices $\\{4,5,6,7,8\\}$, labels $\\{1,1,1,1,1\\}$. This set is also pure, $G(v^{\\text{R}})=0$. Size is $5$.\n  - The weighted impurity is $\\Phi(v; j=0, \\tau=5.5) = (3/8) \\cdot 0 + (5/8) \\cdot 0 = 0$.\nThis is a perfect split, achieving the minimum possible impurity of $0$. No other split can do better. Thus, the root node splits on feature $j=0$ at threshold $\\tau=5.5$.\n\n- **For Case B ($D_{\\max}=1$)**: The children are at depth $1$. Since $\\mathrm{depth} \\ge D_{\\max}$, both children become leaf nodes.\n  - Left Leaf: Contains samples with labels $\\{0,0,0\\}$. The majority class is $0$. Prediction $\\hat{y}=0$.\n  - Right Leaf: Contains samples with labels $\\{1,1,1,1,1\\}$. The majority class is $1$. Prediction $\\hat{y}=1$.\n\n- **For Case A ($D_{\\max}=3$)**: The children are at depth $1$. We check stopping criteria. Both children are pure nodes ($G=0$), so they become leaves immediately, without regard to $D_{\\max}$.\n  - The resulting tree structure is identical to that of Case B.\n\nTherefore, for both Case A and Case B, the decision tree is: if $x_0 \\le 5.5$, predict $0$; otherwise, predict $1$.\nPredictions for query set $\\{\\mathbf{q}_1=(5,1), \\mathbf{q}_2=(6,0), \\mathbf{q}_3=(8,1)\\}$:\n- $\\mathbf{q}_1=(5,1)$: $x_0=5 \\le 5.5 \\implies$ predict $0$.\n- $\\mathbf{q}_2=(6,0)$: $x_0=6 > 5.5 \\implies$ predict $1$.\n- $\\mathbf{q}_3=(8,1)$: $x_0=8 > 5.5 \\implies$ predict $1$.\nThe prediction vector for both cases is $[0,1,1]$.\n\n**Case C: $(T=5, D_{\\max}=3, m_{\\text{try}}=2, n_{\\min}=5)$**\n\nHere, the minimum leaf size parameter is $n_{\\min}=5$. At the root node, which contains $|S_{\\text{root}}|=8$ samples, we search for a valid split. A split is valid only if both child nodes have at least $n_{\\min}=5$ samples. If a split produces children of sizes $|S^{\\text{L}}|$ and $|S^{\\text{R}}|$, we must satisfy $|S^{\\text{L}}| \\ge 5$ and $|S^{\\text{R}}| \\ge 5$. However, $|S^{\\text{L}}| + |S^{\\text{R}}| = 8$, so this is impossible (e.g., $5+5=10>8$).\nTherefore, there are no valid splits from the root node. According to the stopping rules, the root node becomes a leaf node.\n- Leaf prediction: The data at the root has counts $c_0=3$ and $c_1=5$. The majority class is $1$.\nThe tree for Case C is simply a single leaf node that always predicts $1$.\nPredictions for query set:\n- $\\mathbf{q}_1=(5,1) \\implies$ predict $1$.\n- $\\mathbf{q}_2=(6,0) \\implies$ predict $1$.\n- $\\mathbf{q}_3=(8,1) \\implies$ predict $1$.\nThe prediction vector for Case C is $[1,1,1]$.\n\n**Summary of Predictions:**\n- Case A: $[0,1,1]$\n- Case B: $[0,1,1]$\n- Case C: $[1,1,1]$\nThe final output string is constructed from these results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\nclass Node:\n    \"\"\"Represents a node in the decision tree.\"\"\"\n    def __init__(self, depth):\n        self.depth = depth\n        self.feature_index = None\n        self.threshold = None\n        self.left = None\n        self.right = None\n        self.is_leaf = False\n        self.prediction = None\n\nclass DecisionTree:\n    \"\"\"Implements a single decision tree classifier.\"\"\"\n    def __init__(self, max_depth, min_samples_leaf):\n        self.max_depth = max_depth\n        self.min_samples_leaf = min_samples_leaf\n        self.root = None\n    \n    def _gini(self, y):\n        \"\"\"Calculates the Gini impurity for a set of labels.\"\"\"\n        n_samples = len(y)\n        if n_samples == 0:\n            return 0.0\n        _, counts = np.unique(y, return_counts=True)\n        probabilities = counts / n_samples\n        return 1.0 - np.sum(probabilities**2)\n\n    def _find_best_split(self, X, y, indices):\n        \"\"\"Finds the best split for a node.\"\"\"\n        n_samples = len(indices)\n        if n_samples < 2:\n            return None, None\n            \n        # Gini of the current node before splitting\n        parent_gini = self._gini(y[indices])\n        \n        best_gini = parent_gini\n        best_split = None\n        \n        n_features = X.shape[1]\n        \n        # As per problem, m_try=2, so we iterate through all features\n        for feature_index in range(n_features):\n            feature_values = X[indices, feature_index]\n            unique_values = np.unique(feature_values)\n            \n            if len(unique_values) < 2:\n                continue\n\n            thresholds = (unique_values[:-1] + unique_values[1:]) / 2.0\n            \n            for threshold in thresholds:\n                left_indices = indices[X[indices, feature_index] <= threshold]\n                right_indices = indices[X[indices, feature_index] > threshold]\n                \n                # Check min_samples_leaf constraint\n                if len(left_indices) < self.min_samples_leaf or len(right_indices) < self.min_samples_leaf:\n                    continue\n\n                gini_left = self._gini(y[left_indices])\n                gini_right = self._gini(y[right_indices])\n                \n                weighted_gini = (len(left_indices) / n_samples) * gini_left + \\\n                                (len(right_indices) / n_samples) * gini_right\n                \n                # Tie-breaking rules from problem: smallest tau, then smallest feature index.\n                # The loop order naturally handles this.\n                if weighted_gini < best_gini:\n                    best_gini = weighted_gini\n                    best_split = (feature_index, threshold)\n\n        if best_split is None:\n            return None, None # No valid split found\n        \n        return best_split\n\n    def _grow_tree(self, X, y, indices, depth):\n        \"\"\"Recursively grows the decision tree.\"\"\"\n        node = Node(depth)\n        \n        # Determine leaf prediction\n        labels_at_node = y[indices]\n        count_1 = np.sum(labels_at_node == 1)\n        count_0 = len(labels_at_node) - count_1\n        # Tie-breaking: favor class 0\n        node.prediction = 1 if count_1 > count_0 else 0\n\n        # Check stopping criteria\n        is_pure = self._gini(labels_at_node) == 0.0\n        if is_pure or depth >= self.max_depth or len(indices) < 2:\n            node.is_leaf = True\n            return node\n\n        feature_index, threshold = self._find_best_split(X, y, indices)\n        \n        if feature_index is None: # No valid split found\n            node.is_leaf = True\n            return node\n\n        node.feature_index = feature_index\n        node.threshold = threshold\n        \n        left_indices = indices[X[indices, feature_index] <= threshold]\n        right_indices = indices[X[indices, feature_index] > threshold]\n        \n        node.left = self._grow_tree(X, y, left_indices, depth + 1)\n        node.right = self._grow_tree(X, y, right_indices, depth + 1)\n        \n        return node\n\n    def fit(self, X, y):\n        \"\"\"Builds the decision tree.\"\"\"\n        initial_indices = np.arange(len(y))\n        self.root = self._grow_tree(X, y, initial_indices, 0)\n        \n    def _predict_one(self, x, node):\n        \"\"\"Predicts class for a single sample.\"\"\"\n        current_node = node\n        while not current_node.is_leaf:\n            if x[current_node.feature_index] <= current_node.threshold:\n                current_node = current_node.left\n            else:\n                current_node = current_node.right\n        return current_node.prediction\n\n    def predict(self, X):\n        \"\"\"Predicts classes for a set of samples.\"\"\"\n        return np.array([self._predict_one(x, self.root) for x in X])\n\nclass RandomForest:\n    \"\"\"Implements a Random Forest classifier.\"\"\"\n    def __init__(self, n_trees, max_depth, min_samples_leaf, bootstrap, seed):\n        self.n_trees = n_trees\n        self.max_depth = max_depth\n        self.min_samples_leaf = min_samples_leaf\n        self.bootstrap = bootstrap\n        self.seed = seed\n        self.trees = []\n\n    def fit(self, X, y):\n        \"\"\"Trains the Random Forest.\"\"\"\n        self.trees = []\n        n_samples = len(y)\n        \n        # The problem states m_try=2, so no feature subsampling is needed.\n        # It also sets bootstrap=False, so no data bootstrapping is needed.\n        # This means all trees will be identical for a given hyperparameter set.\n        \n        for i in range(self.n_trees):\n            tree = DecisionTree(max_depth=self.max_depth, min_samples_leaf=self.min_samples_leaf)\n            \n            if self.bootstrap:\n                # Per problem, we'd use seed + i for reproducibility\n                rng = np.random.default_rng(self.seed + i)\n                indices = rng.choice(n_samples, n_samples, replace=True)\n                tree.fit(X[indices], y[indices])\n            else:\n                tree.fit(X, y)\n                \n            self.trees.append(tree)\n            \n    def predict(self, X):\n        \"\"\"Makes predictions with the Random Forest.\"\"\"\n        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n        # tree_preds shape is (n_trees, n_samples)\n        \n        # Majority vote; ties go to class 0\n        n_ones = np.sum(tree_preds == 1, axis=0)\n        n_zeros = self.n_trees - n_ones\n        \n        return np.where(n_ones > n_zeros, 1, 0)\n\ndef solve():\n    # Define training data\n    X_train = np.array([\n        [3, 0], [4, 1], [5, 0], [6, 1], [7, 1], [8, 2], [9, 1], [10, 0]\n    ])\n    y_train = np.array([0, 0, 0, 1, 1, 1, 1, 1])\n\n    # Define query data\n    X_query = np.array([\n        [5, 1], [6, 0], [8, 1]\n    ])\n\n    # Define test cases: (T, D_max, m_try, n_min, s, bootstrap)\n    test_cases = [\n        (7, 3, 2, 1, 13, False),  # Case A\n        (1, 1, 2, 1, 7, False),   # Case B\n        (5, 3, 2, 5, 5, False)   # Case C\n    ]\n\n    all_results = []\n    for case in test_cases:\n        T, D_max, m_try, n_min, s, bootstrap = case\n        \n        # As per problem, m_try is always 2. It is not passed to the RF, as the DT handles all features.\n        rf = RandomForest(n_trees=T, max_depth=D_max, min_samples_leaf=n_min, bootstrap=bootstrap, seed=s)\n        rf.fit(X_train, y_train)\n        predictions = rf.predict(X_query)\n        all_results.append(list(predictions))\n\n    # Format the final output string exactly as required\n    inner_parts = []\n    for res_list in all_results:\n        inner_parts.append(f\"[{','.join(map(str, res_list))}]\")\n    final_output = f\"[{','.join(inner_parts)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "2384429"}, {"introduction": "While powerful, individual decision trees have a well-known vulnerability: instability. This practice provides a striking demonstration of this concept, showing how a minuscule perturbation to a single data point can cause a cascade of changes, resulting in a drastically different tree structure [@problem_id:2386935]. By exploring this phenomenon in a controlled setting, you will develop a deeper appreciation for why ensemble methods like Random Forests are essential for building robust and reliable predictive models.", "problem": "Consider binary bankruptcy prediction with a deterministic, greedy, axis-aligned decision tree that uses Gini impurity as its split criterion. You will demonstrate structural instability of a deep tree under a tiny perturbation to one company’s reported earnings. The model and data are defined as follows.\n\nModel and algorithm specification:\n- Labels are binary, with class $y \\in \\{0,1\\}$, where $y=1$ indicates bankruptcy.\n- For any set of samples $S$ with $n$ elements and empirical positive fraction $p=\\frac{1}{n}\\sum_{i\\in S}\\mathbb{1}\\{y_i=1\\}$, the Gini impurity is $G(S)=2p(1-p)$.\n- For a candidate split of a parent set $S$ into disjoint nonempty children $S_L$ and $S_R$, the impurity decrease (gain) is $\\Delta G = G(S) - \\left(\\frac{|S_L|}{|S|}G(S_L) + \\frac{|S_R|}{|S|}G(S_R)\\right)$.\n- The tree grows greedily: at each internal node, among all admissible splits, choose the split with the largest $\\Delta G$. If there are ties in $\\Delta G$, choose the split with the smaller feature index, and if still tied, the smaller threshold value. If no admissible split yields strictly positive gain, make a leaf.\n- Splits are binary of the form $x_j \\le t$ goes left, $x_j > t$ goes right.\n- Domain-knowledge-constrained thresholds: Only feature $j=0$ (earnings) and feature $j=1$ (leverage) may be used for splitting, and each only at threshold $t=0$. Features $j=2$ and $j=3$ are not used for splitting. A split is admissible only if both children are nonempty. Leaves predict the majority class in their node; ties predict class $0$.\n- The maximum depth is fixed at $D=4$ (root at depth $0$). Stop growing when a node is pure, or depth $D$ is reached, or no admissible split yields positive gain.\n\nData:\n- There are $n=16$ companies and $d=4$ features in the order: earnings (feature $0$), leverage (feature $1$), liquidity (feature $2$), and interest coverage (feature $3$).\n- Labels $y$ are\n$$\ny = [1,1,1,1,1,1,1,1,0,0,0,0,0,0,0,0].\n$$\n- Baseline (unperturbed) features $X\\in\\mathbb{R}^{16\\times 4}$ are given by rows $i=0,\\dots,15$:\n    - Earnings $x_{i,0}$:\n        - For indices $i=0,\\dots,7$ (the first eight companies with $y=1$): $[0,0,0,0,0,0,1,1]$.\n        - For indices $i=8,\\dots,15$ (the last eight companies with $y=0$): $[0,0,1,1,1,1,1,1]$.\n      Thus there are exactly $6$ bankrupt and $2$ non-bankrupt firms with $x_{i,0}\\le 0$.\n    - Leverage $x_{i,1}$:\n        - For indices $i=0,\\dots,7$: $[-1,-1,1,1,1,1,1,1]$.\n        - For indices $i=8,\\dots,15$: $[-1,-1,-1,-1,-1,-1,1,1]$.\n      Thus there are exactly $2$ bankrupt and $6$ non-bankrupt firms with $x_{i,1}\\le 0$.\n    - Liquidity $x_{i,2}$ is identically $0$ for all $i$.\n    - Interest coverage $x_{i,3}$ is identically $0$ for all $i$.\n\nPerturbation setup:\n- Choose the specific company index $i^\\star=5$ (which currently has earnings $x_{5,0}=0$ and label $y_{5}=1$).\n- Construct perturbed datasets $X^{(\\varepsilon)}$ by replacing $x_{5,0}$ with $x_{5,0}+\\varepsilon$, leaving all other entries of $X$ unchanged. Consider the test suite of perturbations\n$$\n\\varepsilon \\in \\{0.0,\\;10^{-12},\\;10^{-6},\\;10^{-2}\\}.\n$$\n\nTasks:\n1) Implement the deterministic decision tree described above and train it on the baseline data $X$ and $y$ to obtain a baseline tree $T^{(0)}$.\n\n2) For each $\\varepsilon$ in the test suite, construct $X^{(\\varepsilon)}$, train a tree $T^{(\\varepsilon)}$ under the same rules, and compute two diagnostics relative to $T^{(0)}$:\n   - Structural difference $D_{\\text{nodes}}(\\varepsilon)$: the number of internal-node mismatches between $T^{(0)}$ and $T^{(\\varepsilon)}$ when compared positionally in pre-order traversal. Formally, compare nodes recursively; if one is a leaf and the other is internal, count the entire number of internal nodes in both subtrees; if both are internal, add $1$ if their $(j,t)$ pairs differ, and recurse on aligned left and right children; if both are leaves, add $0$.\n   - Functional difference $F_{\\text{pred}}(\\varepsilon)$: the fraction of the $n=16$ training companies whose predicted labels differ between $T^{(0)}$ and $T^{(\\varepsilon)}$.\n\n3) Your program should produce a single line of output containing the aggregated results for the test suite as a comma-separated list enclosed in square brackets. For each $\\varepsilon$ in order, append first $D_{\\text{nodes}}(\\varepsilon)$ (an integer), then $F_{\\text{pred}}(\\varepsilon)$ (a float). For example, the output format must be\n\"[D_nodes(eps1),F_pred(eps1),D_nodes(eps2),F_pred(eps2),D_nodes(eps3),F_pred(eps3),D_nodes(eps4),F_pred(eps4)]\".\n\nNotes on expected behavior grounded in core definitions:\n- Using $G(S)=2p(1-p)$ and the counts above, at the root with the unperturbed data, both earnings at threshold $0$ and leverage at threshold $0$ achieve the same impurity decrease, so the tie-break selects earnings (feature $0$). After replacing $x_{5,0}$ by $x_{5,0}+\\varepsilon$ with $\\varepsilon>0$, the earnings split at $0$ strictly worsens while the leverage split at $0$ remains unchanged, so the root split switches to leverage (feature $1$). This change at the top cascades through the deep recursive construction and typically induces a radical change in the internal-node structure.\n\nYour implementation must adhere exactly to the algorithmic rules above and must not require any user input. The final results must be printed for the specified $\\varepsilon$ values in the exact format described as a single line. No physical units or angles are involved. All reported fractions must be decimals in $[0,1]$.", "solution": "The problem, as stated, is valid. It constitutes a well-posed computational exercise grounded in established principles of machine learning, specifically the construction of decision trees. The algorithmic specification, including the greedy optimization criterion, stopping conditions, and deterministic tie-breaking rules, is complete and unambiguous, permitting a unique solution. We proceed.\n\nThe task is to demonstrate the structural instability of a decision tree under a minute perturbation. This requires the implementation of a deterministic, greedy decision tree algorithm according to the provided specifications. The solution involves constructing a baseline tree $T^{(0)}$ and several perturbed trees $T^{(\\varepsilon)}$, then quantifying their differences.\n\nThe core of the algorithm is a recursive procedure for building the tree. At each node, a decision must be made: either declare it a leaf or split it into two children. A node becomes a leaf if it is pure (all samples belong to one class), if it has reached the maximum depth $D=4$, or if no admissible split can achieve a strictly positive Gini impurity decrease. An admissible split is one on an allowed feature ($j=0$ or $j=1$) at the fixed threshold $t=0$ that results in two non-empty child nodes. The prediction at a leaf is the majority class, with ties resolved to class $0$.\n\nIf a node is not a leaf, it is split. The greedy choice is the admissible split $(j, t)$ that maximizes the Gini gain, $\\Delta G$. The Gini impurity of a set $S$ with a fraction $p$ of positive samples is $G(S) = 2p(1-p)$. For a split of $S$ into $S_L$ and $S_R$, the gain is $\\Delta G = G(S) - \\left(\\frac{|S_L|}{|S|}G(S_L) + \\frac{|S_R|}{|S|}G(S_R)\\right)$. The problem specifies a strict tie-breaking rule: if multiple splits yield the same maximal gain, the one with the smaller feature index $j$ is chosen.\n\nThe instability phenomenon originates at the root node. Let us analyze the initial split for the unperturbed data ($\\varepsilon=0$). The root contains $n=16$ samples, with $n_1=8$ bankrupt ($y=1$) and $n_0=8$ non-bankrupt ($y=0$) companies. The fraction of positives is $p=8/16=0.5$, so the root's Gini impurity is $G(S_{\\text{root}}) = 2(0.5)(1-0.5) = 0.5$.\n\n1.  **Split on Earnings ($j=0, t=0$)**:\n    The left child ($x_{i,0} \\le 0$) contains $6$ bankrupt and $2$ non-bankrupt firms ($|S_L|=8, p_L=6/8=0.75$). The right child ($x_{i,0} > 0$) contains $2$ bankrupt and $6$ non-bankrupt firms ($|S_R|=8, p_R=2/8=0.25$).\n    The Gini impurities are $G(S_L) = G(S_R) = 2(0.75)(0.25) = 0.375$.\n    The gain is $\\Delta G_0 = 0.5 - \\left(\\frac{8}{16}(0.375) + \\frac{8}{16}(0.375)\\right) = 0.5 - 0.375 = 0.125$.\n\n2.  **Split on Leverage ($j=1, t=0$)**:\n    The left child ($x_{i,1} \\le 0$) contains $2$ bankrupt and $6$ non-bankrupt firms ($|S_L|=8, p_L=2/8=0.25$). The right child ($x_{i,1} > 0$) contains $6$ bankrupt and $2$ non-bankrupt firms ($|S_R|=8, p_R=6/8=0.75$).\n    The Gini impurities are again $G(S_L) = G(S_R) = 0.375$.\n    The gain is $\\Delta G_1 = 0.5 - 0.375 = 0.125$.\n\nSince $\\Delta G_0 = \\Delta G_1$, the tie-breaking rule dictates we choose the smaller feature index, $j=0$. The root of $T^{(0)}$ splits on earnings.\n\nNow consider the perturbation for $\\varepsilon > 0$. The earnings of company $i=5$ (bankrupt) changes from $x_{5,0}=0$ to $x_{5,0}=\\varepsilon>0$. This company now moves to the right child of the earnings split. The leverage split is unaffected.\nThe gain for the leverage split remains $\\Delta G_1 = 0.125$. For the earnings split, the children become imbalanced in size. The left child has $|S_L'|=7$ ($5$ bankrupt, $2$ non-bankrupt), and the right has $|S_R'|=9$ ($3$ bankrupt, $6$ non-bankrupt).\n$G(S_L') = 2(5/7)(2/7) = 20/49$. $G(S_R') = 2(3/9)(6/9) = 4/9$.\nThe new gain for the earnings split is $\\Delta G_0' = 0.5 - \\left(\\frac{7}{16}(20/49) + \\frac{9}{16}(4/9)\\right) = 0.5 - (\\frac{5}{28} + \\frac{1}{4}) = 0.5 - \\frac{12}{28} = \\frac{1}{14} \\approx 0.0714$.\nSince $\\Delta G_1 > \\Delta G_0'$, the optimal split for any $\\varepsilon > 0$ becomes leverage ($j=1$). This change at the root node propagates down the tree, leading to a completely different structure.\n\nTo quantify these differences, we compute two metrics:\n-   **Structural Difference $D_{\\text{nodes}}(\\varepsilon)$**: This is calculated via a recursive comparison of the nodes of $T^{(0)}$ and $T^{(\\varepsilon)}$ in pre-order. If two corresponding nodes are internal but have different split rules, a cost of $1$ is added. If one is a leaf and the other is internal, the cost is the total number of internal nodes in the subtree of the internal node. The total difference is the sum of these costs over the entire tree structure.\n-   **Functional Difference $F_{\\text{pred}}(\\varepsilon)$**: This measures the fraction of the $16$ training samples for which the predicted labels from $T^{(0)}$ and $T^{(\\varepsilon)}$ differ. The prediction for each company $i$ is obtained from the corresponding tree, i.e., $T^{(0)}$ predicts on $X$ and $T^{(\\varepsilon)}$ predicts on $X^{(\\varepsilon)}$.\n\nThe implementation will consist of a `DecisionTree` class encapsulating the `fit` and `predict` logic, following the specified algorithm precisely. A `Node` class represents the tree's structure. Separate functions will compute the structural and functional differences. By executing this logic for the specified suite of $\\varepsilon$ values, we generate the required output.", "answer": "```python\nimport numpy as np\n\nclass Node:\n    \"\"\"Represents a node in the decision tree.\"\"\"\n    def __init__(self, predicted_class=None):\n        self.predicted_class = predicted_class  # Used for leaf nodes\n        self.feature_index = None\n        self.threshold = None\n        self.left = None\n        self.right = None\n\n    @property\n    def is_leaf(self):\n        \"\"\"A node is a leaf if it has a predicted class.\"\"\"\n        return self.predicted_class is not None\n\nclass DecisionTree:\n    \"\"\"\n    A deterministic, greedy, axis-aligned decision tree.\n    \"\"\"\n    def __init__(self, max_depth=4):\n        self.max_depth = max_depth\n        self.root = None\n        self.split_features = [0, 1]  # Earnings, Leverage\n        self.split_threshold = 0.0\n\n    def _gini(self, y):\n        \"\"\"Calculates the Gini impurity.\"\"\"\n        n = len(y)\n        if n == 0:\n            return 0.0\n        p = np.sum(y) / n\n        return 2 * p * (1 - p)\n\n    def _make_leaf(self, y):\n        \"\"\"Creates a leaf node and determines its prediction.\"\"\"\n        leaf = Node()\n        if len(y) == 0:\n            leaf.predicted_class = 0\n            return leaf\n        \n        n_pos = np.sum(y)\n        n_neg = len(y) - n_pos\n        # Majority class prediction, with ties predicting class 0\n        leaf.predicted_class = 1 if n_pos > n_neg else 0\n        return leaf\n\n    def _find_best_split(self, X, y, indices):\n        \"\"\"Finds the best admissible split for a set of samples.\"\"\"\n        best_gain = -1.0\n        best_split = None\n        \n        current_y = y[indices]\n        if len(current_y) == 0:\n            return None, -1.0\n            \n        parent_gini = self._gini(current_y)\n        \n        for feature_index in self.split_features:\n            left_indices = indices[X[indices, feature_index] <= self.split_threshold]\n            right_indices = indices[X[indices, feature_index] > self.split_threshold]\n            \n            if len(left_indices) == 0 or len(right_indices) == 0:\n                continue\n\n            y_left, y_right = y[left_indices], y[right_indices]\n            gini_left, gini_right = self._gini(y_left), self._gini(y_right)\n            \n            n = len(current_y)\n            n_left, n_right = len(y_left), len(y_right)\n            child_gini = (n_left / n) * gini_left + (n_right / n) * gini_right\n            gain = parent_gini - child_gini\n            \n            if gain > best_gain:\n                best_gain = gain\n                best_split = (feature_index, self.split_threshold, left_indices, right_indices)\n        \n        return best_split, best_gain\n\n    def _build_tree(self, X, y, indices, depth):\n        \"\"\"Recursively builds the tree.\"\"\"\n        current_y = y[indices]\n        is_pure = (self._gini(current_y) == 0.0)\n        \n        if depth == self.max_depth or is_pure:\n            return self._make_leaf(current_y)\n            \n        best_split, best_gain = self._find_best_split(X, y, indices)\n        \n        if best_gain <= 0:\n            return self._make_leaf(current_y)\n        \n        feature_index, threshold, left_indices, right_indices = best_split\n        \n        node = Node()\n        node.feature_index = feature_index\n        node.threshold = threshold\n        node.left = self._build_tree(X, y, left_indices, depth + 1)\n        node.right = self._build_tree(X, y, right_indices, depth + 1)\n        \n        return node\n        \n    def fit(self, X, y):\n        \"\"\"Trains the decision tree.\"\"\"\n        initial_indices = np.arange(X.shape[0])\n        self.root = self._build_tree(X, y, initial_indices, depth=0)\n        \n    def _predict_one(self, x, node):\n        \"\"\"Predicts the class for a single sample.\"\"\"\n        if node.is_leaf:\n            return node.predicted_class\n        if x[node.feature_index] <= node.threshold:\n            return self._predict_one(x, node.left)\n        else:\n            return self._predict_one(x, node.right)\n\n    def predict(self, X):\n        \"\"\"Predicts classes for a set of samples.\"\"\"\n        return np.array([self._predict_one(x, self.root) for x in X])\n\ndef count_internal_nodes(node):\n    \"\"\"Recursively counts the number of internal nodes in a subtree.\"\"\"\n    if node is None or node.is_leaf:\n        return 0\n    return 1 + count_internal_nodes(node.left) + count_internal_nodes(node.right)\n\ndef compute_structural_difference(node1, node2):\n    \"\"\"Recursively computes the structural difference between two trees.\"\"\"\n    if node1.is_leaf and node2.is_leaf:\n        return 0\n    \n    if node1.is_leaf and not node2.is_leaf:\n        return count_internal_nodes(node2)\n        \n    if not node1.is_leaf and node2.is_leaf:\n        return count_internal_nodes(node1)\n        \n    # Both are internal nodes\n    cost = 0\n    if node1.feature_index != node2.feature_index or node1.threshold != node2.threshold:\n        cost = 1\n        \n    cost += compute_structural_difference(node1.left, node2.left)\n    cost += compute_structural_difference(node1.right, node2.right)\n    \n    return cost\n\ndef solve():\n    \"\"\"\n    Main function to run the experiment and produce results.\n    \"\"\"\n    # Define baseline data\n    y = np.array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int)\n    X = np.zeros((16, 4))\n    X[0:8, 0] = [0, 0, 0, 0, 0, 0, 1, 1]\n    X[8:16, 0] = [0, 0, 1, 1, 1, 1, 1, 1]\n    X[0:8, 1] = [-1, -1, 1, 1, 1, 1, 1, 1]\n    X[8:16, 1] = [-1, -1, -1, -1, -1, -1, 1, 1]\n    \n    perturbed_company_idx = 5\n    epsilons = [0.0, 1e-12, 1e-6, 1e-2]\n    \n    # Train baseline tree and get predictions\n    tree_0 = DecisionTree(max_depth=4)\n    tree_0.fit(X, y)\n    preds_0 = tree_0.predict(X)\n    \n    results = []\n    \n    for eps in epsilons:\n        if eps == 0.0:\n            # The baseline tree compared to itself has 0 difference\n            d_nodes = 0\n            f_pred = 0.0\n        else:\n            # Create perturbed data\n            X_eps = X.copy()\n            X_eps[perturbed_company_idx, 0] += eps\n            \n            # Train perturbed tree\n            tree_eps = DecisionTree(max_depth=4)\n            tree_eps.fit(X_eps, y)\n            \n            # Compute Structural Difference\n            d_nodes = compute_structural_difference(tree_0.root, tree_eps.root)\n            \n            # Compute Functional Difference\n            # Predictions are made on the data each tree was trained on\n            preds_eps = tree_eps.predict(X_eps)\n            f_pred = np.sum(preds_0 != preds_eps) / len(y)\n\n        results.extend([d_nodes, f_pred])\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2386935"}, {"introduction": "Beyond building and analyzing models, a key task for computational scientists is interpreting their predictions. This final exercise introduces you to the world of eXplainable AI (XAI) by asking you to find a \"counterfactual explanation\" for a model's decision [@problem_id:2386887]. You will work backwards from a given Random Forest's prediction to determine the smallest change in an applicant's features that would flip a \"rejection\" to an \"approval,\" a task that requires careful reasoning about the model's geometric decision boundaries.", "problem": "You are given a fixed binary classification model for credit approval built as a Random Forest (RF) of three decision trees, denoted as trees $A$, $B$, and $C$. Each tree operates on a feature vector $x \\in \\mathbb{R}^3$ with components $x = (x_1,x_2,x_3)$, defined as follows: $x_1$ is annual income measured in tens of thousands of dollars, constrained to $x_1 \\in [1.0,20.0]$; $x_2$ is the revolving credit utilization ratio, constrained to $x_2 \\in [0.0,1.0]$; and $x_3$ is a normalized credit history quality score, constrained to $x_3 \\in [0.0,1.0]$. The RF returns the label $1$ for approval and $0$ for rejection, with the RF prediction defined as the majority vote of the three trees (approval if and only if at least $2$ trees predict $1$). Each tree is a binary decision tree with axis-aligned splits of the form $x_j \\le t$ versus $x_j > t$. A tree outputs its binary label at a leaf.\n\nTree $A$ outputs approval ($1$) on any input satisfying at least one of the following two regions and outputs rejection ($0$) otherwise:\n- Region $A_1$: $x_3 > 0.55$ and $x_2 \\le 0.70$.\n- Region $A_2$: $x_3 \\le 0.55$ and $x_2 > 0.30$ and $x_1 > 8.00$.\n\nTree $B$ outputs approval ($1$) on any input satisfying at least one of the following three regions and outputs rejection ($0$) otherwise:\n- Region $B_1$: $x_1 \\le 12.00$ and $x_3 > 0.40$ and $x_2 \\le 0.50$.\n- Region $B_2$: $x_1 > 12.00$ and $x_2 \\le 0.60$.\n- Region $B_3$: $x_1 > 12.00$ and $x_2 > 0.60$ and $x_3 \\le 0.80$.\n\nTree $C$ outputs approval ($1$) on any input satisfying at least one of the following two regions and outputs rejection ($0$) otherwise:\n- Region $C_1$: $x_2 > 0.20$ and $x_3 \\le 0.50$ and $x_1 \\le 15.00$.\n- Region $C_2$: $x_2 > 0.20$ and $x_3 > 0.50$.\n\nBy construction, the RF outputs approval if and only if at least two trees output approval. For any given applicant feature vector $x$ that is currently rejected by the RF, define a counterfactual change as a vector $\\Delta x \\in \\mathbb{R}^3$ such that the modified vector $x' = x + \\Delta x$ lies within the domain box $[1.0,20.0] \\times [0.0,1.0] \\times [0.0,1.0]$ and yields RF approval. The size of a change is evaluated by the $\\ell_1$ norm $||\\Delta x||_1 = | \\Delta x_1 | + | \\Delta x_2 | + | \\Delta x_3 |$. The task is to find a smallest-change counterfactual, i.e., a vector $\\Delta x$ that minimizes $||\\Delta x||_1$ subject to $x'$ being in the domain and the RF predicting approval on $x'$.\n\nStrict inequalities in the tree regions must be interpreted with a numerical tolerance $\\varepsilon = 10^{-6}$. Specifically, for any constraint of the form $x_j > t$, feasibility requires $x_j \\ge t + \\varepsilon$, and for any constraint of the form $x_j \\le t$, feasibility requires $x_j \\le t$. For intersections of multiple constraints, all must be satisfied simultaneously. If multiple minimizers exist, you must choose among them by the following deterministic tie-breaking rule: first, minimize the Euclidean norm $||\\Delta x||_2$; if a tie persists, choose the lexicographically smallest $\\Delta x$ (compare $\\Delta x_1$, then $\\Delta x_2$, then $\\Delta x_3$).\n\nYour program must, for each test case below, compute one smallest-change counterfactual $\\Delta x$ as defined above. The final outputs for all test cases must be aggregated into a single line as a comma-separated list enclosed in square brackets, where each per-case result is a list of three floating-point values corresponding to $(\\Delta x_1,\\Delta x_2,\\Delta x_3)$. Each floating-point value must be rounded to six decimal places.\n\nTest suite (each $x$ is within the domain and is rejected by the RF):\n- Case $1$: $x = (7.00, 0.95, 0.90)$.\n- Case $2$: $x = (10.00, 0.20, 0.40)$.\n- Case $3$: $x = (19.00, 0.95, 0.85)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, where each element is the three-component list for one case, in the same order as above. For example, an acceptable format is $[[a_{11},a_{12},a_{13}],[a_{21},a_{22},a_{23}],[a_{31},a_{32},a_{33}]]$, where each $a_{ij}$ is a six-decimal-place number.", "solution": "The user has provided a problem that requires finding a minimal counterfactual change for a Random Forest (RF) classification model. The problem is well-defined, scientifically grounded, and computationally tractable. I will now proceed with a formal solution.\n\nThe problem is to find a change vector $\\Delta x = (\\Delta x_1, \\Delta x_2, \\Delta x_3)$ for a given input feature vector $x = (x_1, x_2, x_3)$ that is initially rejected by the RF model. The modified vector $x' = x + \\Delta x$ must be classified as \"approved\" and must remain within the valid feature domain $D$. The goal is to find a $\\Delta x$ that minimizes the $\\ell_1$ norm, $||\\Delta x||_1 = \\sum_{i=1}^3 |\\Delta x_i|$, subject to a specific tie-breaking rule.\n\nFirst, let us formalize the components of the problem.\nThe feature domain is a hyperrectangle (a box) in $\\mathbb{R}^3$:\n$$D = [1.0, 20.0] \\times [0.0, 1.0] \\times [0.0, 1.0]$$\nThe RF model consists of three decision trees, $T_A, T_B, T_C$. Let their respective predictions for an input $z$ be $A(z), B(z), C(z) \\in \\{0, 1\\}$. The RF prediction is approval (1) if and only if the sum of the individual tree predictions is at least $2$:\n$$RF(z) = 1 \\iff A(z) + B(z) + C(z) \\ge 2$$\nAn input $x$ is given, for which $RF(x) = 0$. We seek a new point $x' = x + \\Delta x$ such that $RF(x')=1$ and $x' \\in D$. The objective is to solve the following optimization problem:\n$$ \\min_{\\Delta x} ||\\Delta x||_1 \\quad \\text{subject to} \\quad (x + \\Delta x) \\in D \\text{ and } RF(x + \\Delta x) = 1 $$\nThe tie-breaking rules are: if multiple $\\Delta x$ yield the same minimal $\\ell_1$ norm, first select the one with the minimal Euclidean norm $||\\Delta x||_2$, and then the lexicographically smallest vector $\\Delta x$.\n\nThe condition $RF(x')=1$ means that at least two trees must approve $x'$. This can occur in three ways (which are not mutually exclusive):\n1. $A(x')=1$ and $B(x')=1$.\n2. $A(x')=1$ and $C(x')=1$.\n3. $B(x')=1$ and $C(x')=1$.\n\nLet $\\mathcal{A}_A, \\mathcal{A}_B, \\mathcal{A}_C$ denote the approval regions for trees $A$, $B$, and $C$, respectively. The total approval region for the RF, $\\mathcal{A}_{RF}$, is the union of the regions where at least two trees approve:\n$$ \\mathcal{A}_{RF} = (\\mathcal{A}_A \\cap \\mathcal{A}_B) \\cup (\\mathcal{A}_A \\cap \\mathcal{A}_C) \\cup (\\mathcal{A}_B \\cap \\mathcal{A}_C) $$\nEach tree's approval region is defined as a union of sub-regions, which are axis-aligned hyperrectangles (boxes). For example, $\\mathcal{A}_A = A_1 \\cup A_2$. The intersection of two boxes is another box (possibly empty). The union of boxes results in a set that is a union of boxes. Therefore, the total approval region $\\mathcal{A}_{RF}$ is a union of a finite number of boxes. Let us denote this set of boxes as $\\{K_k\\}$.\n\nThe optimization problem is to find a point $x'$ in the set $\\mathcal{A}_{RF} \\cap D$ that is closest to $x$ in the $\\ell_1$ norm. This is equivalent to finding the minimum distance from $x$ to any of the boxes that constitute $\\mathcal{A}_{RF} \\cap D$.\n$$ \\min_{x' \\in \\mathcal{A}_{RF} \\cap D} ||x' - x||_1 = \\min_k \\left( \\min_{x' \\in K_k \\cap D} ||x' - x||_1 \\right) $$\n\nFor any single box $K = [l_1, u_1] \\times [l_2, u_2] \\times [l_3, u_3]$, finding the point $x' \\in K$ closest to a given point $x$ is a separable problem. The $\\ell_p$ distance is minimized by projecting $x$ onto $K$. The projection $x'_{proj}$ has coordinates:\n$$ (x'_{proj})_i = \\max(l_i, \\min(x_i, u_i)) $$\nThe change vector is $\\Delta x = x'_{proj} - x$, and the $\\ell_1$ distance is $||\\Delta x||_1 = \\sum_{i=1}^3 |(x'_{proj})_i - x_i|$.\n\nThe overall algorithm is as follows:\n1. Define the approval sub-regions for each tree ($A_1, A_2, B_1, \\dots, C_2$) as boxes, incorporating the numerical tolerance $\\varepsilon = 10^{-6}$ for strict inequalities. For a constraint $x_j > t$, the boundary is $t+\\varepsilon$; for $x_j \\le t$, the boundary is $t$.\n2. Enumerate all combinations of tree approvals that result in RF approval: $(A,B), (A,C), (B,C)$.\n3. For each pair of trees, say $(T_i, T_j)$, generate all intersection boxes by taking the intersection of each approval sub-region of $T_i$ with each approval sub-region of $T_j$.\n4. For each resulting intersection box, further intersect it with the domain box $D$.\n5. If the resulting box is non-empty, calculate the projection of the initial point $x$ onto this box to find the candidate point $x'$, the change vector $\\Delta x = x' - x$, its $\\ell_1$ norm, and its $\\ell_2$ norm.\n6. Maintain a record of the best candidate solution found so far, defined as the tuple $(||\\Delta x||_1, ||\\Delta x||_2, \\Delta x)$.\n7. Iterate through all generated boxes, comparing each new candidate solution with the current best. A new candidate is better if its $\\ell_1$ norm is smaller; or if the $\\ell_1$ norms are equal and its $\\ell_2$ norm is smaller; or if both norms are equal and its $\\Delta x$ vector is lexicographically smaller.\n8. After checking all possible approval regions, the best candidate found is the solution.\n\nThis exhaustive search is guaranteed to find the optimal solution because it considers all possible ways to achieve RF approval and for each, finds the point of minimum change. The number of regions to check is finite and small in this problem. Specifically, there are $(2 \\times 3) + (2 \\times 2) + (3 \\times 2) = 6+4+6=16$ intersection boxes to evaluate for each test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the counterfactual explanation problem for all test cases.\n    \"\"\"\n    \n    # Numerical tolerance\n    EPS = 1e-6\n\n    # Feature domain box D\n    domain_box = (np.array([1.0, 0.0, 0.0]), np.array([20.0, 1.0, 1.0]))\n\n    # Define tree approval regions as boxes [low_bounds, high_bounds]\n    # np.inf is used for unbounded dimensions before intersecting with the domain.\n    inf = np.inf\n    \n    # Tree A regions\n    regions_A = [\n        # A1: x3 > 0.55, x2 <= 0.70\n        (np.array([-inf, -inf, 0.55 + EPS]), np.array([inf, 0.70, inf])),\n        # A2: x3 <= 0.55, x2 > 0.30, x1 > 8.00\n        (np.array([8.00 + EPS, 0.30 + EPS, -inf]), np.array([inf, inf, 0.55])),\n    ]\n\n    # Tree B regions\n    regions_B = [\n        # B1: x1 <= 12.00, x3 > 0.40, x2 <= 0.50\n        (np.array([-inf, -inf, 0.40 + EPS]), np.array([12.00, 0.50, inf])),\n        # B2: x1 > 12.00, x2 <= 0.60\n        (np.array([12.00 + EPS, -inf, -inf]), np.array([inf, 0.60, inf])),\n        # B3: x1 > 12.00, x2 > 0.60, x3 <= 0.80\n        (np.array([12.00 + EPS, 0.60 + EPS, -inf]), np.array([inf, inf, 0.80])),\n    ]\n\n    # Tree C regions\n    regions_C = [\n        # C1: x2 > 0.20, x3 <= 0.50, x1 <= 15.00\n        (np.array([-inf, 0.20 + EPS, -inf]), np.array([15.00, inf, 0.50])),\n        # C2: x2 > 0.20, x3 > 0.50\n        (np.array([-inf, 0.20 + EPS, 0.50 + EPS]), np.array([inf, inf, inf])),\n    ]\n\n    tree_regions = {'A': regions_A, 'B': regions_B, 'C': regions_C}\n    target_combinations = [('A', 'B'), ('A', 'C'), ('B', 'C')]\n    \n    def intersect_boxes(box1, box2):\n        \"\"\"Intersects two boxes.\"\"\"\n        low1, high1 = box1\n        low2, high2 = box2\n        intersect_low = np.maximum(low1, low2)\n        intersect_high = np.minimum(high1, high2)\n        return intersect_low, intersect_high\n\n    def get_candidate(x, box):\n        \"\"\"Calculates the change vector and norms for projecting x onto a box.\"\"\"\n        low, high = box\n        if np.any(low > high):\n            return (inf, inf, None) # Invalid box\n        \n        x_prime = np.maximum(low, np.minimum(x, high))\n        delta_x = x_prime - x\n        l1_norm = np.sum(np.abs(delta_x))\n        l2_norm = np.sqrt(np.sum(delta_x**2))\n        return (l1_norm, l2_norm, delta_x)\n\n    def solve_one_case(x_tuple):\n        \"\"\"Solves for a single test case vector x.\"\"\"\n        x = np.array(x_tuple, dtype=np.float64)\n        best_candidate = (inf, inf, None)\n\n        for combo in target_combinations:\n            regions1 = tree_regions[combo[0]]\n            regions2 = tree_regions[combo[1]]\n            \n            for r1 in regions1:\n                for r2 in regions2:\n                    # Intersection of two approval regions\n                    target_box = intersect_boxes(r1, r2)\n                    # Further intersect with domain\n                    final_box = intersect_boxes(target_box, domain_box)\n                    \n                    candidate = get_candidate(x, final_box)\n                    \n                    # Tie-breaking logic\n                    if candidate[0] < best_candidate[0]:\n                        best_candidate = candidate\n                    elif candidate[0] == best_candidate[0]:\n                        if candidate[1] < best_candidate[1]:\n                            best_candidate = candidate\n                        elif candidate[1] == best_candidate[1]:\n                            # Lexicographical comparison\n                            current_best_dx = best_candidate[2]\n                            new_dx = candidate[2]\n                            if current_best_dx is None or new_dx[0] < current_best_dx[0] or \\\n                               (new_dx[0] == current_best_dx[0] and new_dx[1] < current_best_dx[1]) or \\\n                               (new_dx[0] == current_best_dx[0] and new_dx[1] == current_best_dx[1] and new_dx[2] < current_best_dx[2]):\n                                best_candidate = candidate\n        \n        return best_candidate[2]\n\n    # Test cases from the problem statement\n    test_cases = [\n        (7.00, 0.95, 0.90),\n        (10.00, 0.20, 0.40),\n        (19.00, 0.95, 0.85),\n    ]\n\n    results_formatted = []\n    for case in test_cases:\n        delta_x = solve_one_case(case)\n        \n        # Format each component to six decimal places\n        formatted_dx = [f\"{v:.6f}\" for v in delta_x]\n        results_formatted.append(f\"[{','.join(formatted_dx)}]\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_formatted)}]\")\n\nsolve()\n\n```", "id": "2386887"}]}