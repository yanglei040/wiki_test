{"hands_on_practices": [{"introduction": "A cornerstone of modern bioinformatics is the ability to perform thousands, or even millions, of statistical tests simultaneously, such as when screening for differentially expressed genes. This massive parallelism, however, introduces a critical statistical pitfall: the inflation of false-positive findings. This exercise provides a foundational calculation to demonstrate this effect, allowing you to quantify the probability of making at least one incorrect discovery by chance alone when conducting multiple tests [@problem_id:2430505]. Understanding this concept, known as the Family-Wise Error Rate (FWER), is the first step toward appreciating the necessity of multiple testing correction.", "problem": "In a computational biology validation study of a transcription factor binding site predictor, a researcher uses Chromatin Immunoprecipitation followed by sequencing (ChIP-seq) negative control regions to estimate the behavior of the statistical testing pipeline under the absence of true signal. Suppose the researcher performs hypothesis tests on $m = 20$ independent negative control regions, where each test is conducted at a per-test significance level $\\alpha = 0.05$. Assume that all $m$ null hypotheses are true and that the tests are mutually independent. What is the probability that at least one of these tests is declared significant purely by chance? Express your answer as a decimal fraction and round your answer to $4$ significant figures.", "solution": "**Solution Formulation**\nThe problem asks for the probability of observing at least one statistically significant result when conducting $m$ independent hypothesis tests, given that all null hypotheses are true. This is a classic problem in multiple comparisons.\n\nLet $H_{0,i}$ be the null hypothesis for the $i$-th test, where $i$ ranges from $1$ to $m$. We are given that all $H_{0,i}$ are true.\nThe significance level, $\\alpha$, is the probability of committing a Type I error for a single test. A Type I error occurs when a true null hypothesis is incorrectly rejected. Therefore, for any single test $i$, the probability of declaring the result significant (i.e., rejecting $H_{0,i}$) is given by:\n$$P(\\text{test } i \\text{ is significant}) = \\alpha$$\n\nWe are given $\\alpha = 0.05$.\n\nThe complementary event is that a single test is *not* declared significant. The probability of this event is:\n$$P(\\text{test } i \\text{ is not significant}) = 1 - \\alpha$$\n\nThe problem states that all $m = 20$ tests are mutually independent. We want to find the probability that *at least one* test is significant. Let $A$ be the event that at least one of the $m$ tests is significant. It is computationally simpler to first calculate the probability of the complementary event, $A^c$, which is the event that *none* of the tests are significant.\n\nSince the tests are independent, the probability that all $m$ tests are not significant is the product of their individual probabilities:\n$$P(A^c) = P(\\text{test 1 is not significant} \\cap \\text{test 2 is not significant} \\cap \\dots \\cap \\text{test } m \\text{ is not significant})$$\n$$P(A^c) = \\prod_{i=1}^{m} P(\\text{test } i \\text{ is not significant})$$\n$$P(A^c) = (1 - \\alpha)^m$$\n\nThe probability of the event $A$ (at least one significant test) is then given by the complement rule:\n$$P(A) = 1 - P(A^c)$$\n$$P(A) = 1 - (1 - \\alpha)^m$$\n\nThis quantity is known as the Family-Wise Error Rate (FWER) under the complete null hypothesis.\n\nNow, we substitute the given values into this formula:\n- Number of tests, $m = 20$.\n- Significance level, $\\alpha = 0.05$.\n\n$$P(A) = 1 - (1 - 0.05)^{20}$$\n$$P(A) = 1 - (0.95)^{20}$$\n\nTo obtain the numerical value, we calculate $(0.95)^{20}$:\n$$(0.95)^{20} \\approx 0.358485922$$\n\nNow, we find the final probability:\n$$P(A) \\approx 1 - 0.358485922$$\n$$P(A) \\approx 0.641514078$$\n\nThe problem requires the answer to be rounded to $4$ significant figures. The first four significant figures are $6$, $4$, $1$, and $5$. The fifth digit is $1$, which is less than $5$, so we round down.\nThe final result is $0.6415$.\nThis result demonstrates a critical concept: even with a small per-test error rate of $5\\%$, the probability of making at least one false discovery inflates rapidly as the number of tests increases. For $20$ tests, there is a greater than $64\\%$ chance of at least one false positive result.", "answer": "$$\\boxed{0.6415}$$", "id": "2430505"}, {"introduction": "Beyond the straightforward issue of running many gene-level tests, statistical integrity can be compromised in more subtle ways, often termed \"researcher degrees of freedom\" or \"p-hacking\". This thought experiment explores a realistic scenario where a bioinformatician tries multiple data pre-processing pipelines but only reports the one yielding the most \"significant\" results [@problem_id:2430523]. By critiquing this workflow, you will develop the crucial skill of identifying how data-dependent analysis choices can invalidate statistical inference and learn about principled approaches to avoid such biases.", "problem": "A computational biologist analyzes a single RNA sequencing (RNA-seq) dataset to detect differential expression across $m$ genes between two conditions. The team considers $10$ different pre-processing pipelines (e.g., different aligners, normalization schemes, and filtering rules), then applies the same gene-level test downstream of each pipeline, producing a $p$-value for each gene within each pipeline. They then report only the one pipeline that yields the largest number of genes with $p$-values below a nominal threshold (for example, $p<0.05$), presenting those $p$-values as if only that pipeline had been attempted.\n\nFrom first principles, recall that under a true null hypothesis and a correctly specified continuous test, the $p$-value is uniformly distributed on $[0,1]$, and that Type I error control is defined with respect to the procedure actually employed. Consider the following critiques and remedies regarding this workflow. Assume, for concreteness, that for a given null gene the $10$ pipeline-specific $p$-values are exchangeable and approximately independent under the null.\n\nWhich of the following statements are correct?\n\nA. Selecting the pipeline that minimizes the $p$-value for each gene inflates the per-gene Type I error: under the null, the probability that the selected (minimum) $p$-value is at most $0.05$ equals approximately $0.40$ when $10$ pipelines are tried. Valid inference requires accounting for this selection, for example by adjusting across the $10$ pipelines or by separating pipeline selection from testing using a held-out dataset.\n\nB. If the chosen pipeline applies the Benjamini–Hochberg procedure to control the False Discovery Rate (FDR) at $q=0.05$, then the prior search over $10$ pipelines does not affect error rates, so the reported discoveries remain valid without further adjustment.\n\nC. If the $10$ pipelines are highly correlated in their outputs, the impact of selecting the most favorable one on Type I error is negligible and can be ignored.\n\nD. Reporting only the pipeline with the most significant results is acceptable provided the other $9$ attempted pipelines are transparently disclosed; no additional statistical correction is needed because transparency addresses the bias.\n\nE. A valid remedy is to use a resampling scheme (such as permutation under the null or the bootstrap) that repeats the entire selection process within each resample, i.e., re-running all $10$ pipelines and selecting the best within each resample, to obtain a null distribution for the selected statistic; this accounts for the data-dependent pipeline selection when computing $p$-values.", "solution": "**Analysis of the Statistical Procedure**\n\nThe core of the problem lies in the data-dependent selection of the analysis pipeline. The reported $p$-values are not from a pre-specified analysis plan but from a pipeline chosen specifically because it produced the most \"significant\" results. This constitutes a form of multiple testing, not at the level of genes, but at the level of entire analysis pipelines. This selection process invalidates the nominal statistical properties of the reported $p$-values. Specifically, for a gene where the null hypothesis is true, the reported $p$-value does not follow a $U[0,1]$ distribution. Instead, its distribution is biased towards small values, leading to a substantial inflation of the Type I error rate. The \"procedure actually employed\" is not simply \"run pipeline $k$\" but \"run pipelines $1$ through $10$, select the best one according to a criterion, and report its results.\" Any valid statistical inference must account for this entire complex procedure.\n\n### Option-by-Option Evaluation\n\n**A. Selecting the pipeline that minimizes the $p$-value for each gene inflates the per-gene Type I error: under the null, the probability that the selected (minimum) $p$-value is at most $0.05$ equals approximately $0.40$ when $10$ pipelines are tried. Valid inference requires accounting for this selection, for example by adjusting across the $10$ pipelines or by separating pipeline selection from testing using a held-out dataset.**\n\nThis statement critiques the general issue of cherry-picking. It simplifies the pipeline selection rule described in the problem to a gene-wise selection of the minimum $p$-value. Let us analyze the calculation for this simplified procedure. For a single null gene, let its $p$-values from the $10$ independent pipelines be $P_1, P_2, \\dots, P_{10}$. Under the null, each $P_i \\sim U[0,1]$. The selected $p$-value is $P_{\\min} = \\min(P_1, \\dots, P_{10})$. The probability of a Type I error at a nominal level $\\alpha = 0.05$ is:\n$$ P(P_{\\min} \\le \\alpha) = 1 - P(P_{\\min} > \\alpha) $$\nGiven independence:\n$$ P(P_{\\min} > \\alpha) = P(P_1 > \\alpha, P_2 > \\alpha, \\dots, P_{10} > \\alpha) = \\prod_{i=1}^{10} P(P_i > \\alpha) $$\nSince $P(P_i > \\alpha) = 1 - \\alpha$ for a uniform distribution:\n$$ P(P_{\\min} \\le \\alpha) = 1 - (1 - \\alpha)^{10} $$\nFor $\\alpha = 0.05$:\n$$ P(P_{\\min} \\le 0.05) = 1 - (1 - 0.05)^{10} = 1 - (0.95)^{10} \\approx 1 - 0.5987 = 0.4013 $$\nThe calculation is correct for the procedure it describes. While this procedure is not identical to the one in the problem statement, it serves as a powerful and valid illustration of the severe inflation of Type I error caused by such selection effects. The actual procedure in the problem also inflates the Type I error, as it is biased towards selecting pipelines that contain spuriously small $p$-values.\nFurthermore, the statement proposes valid remedies: \"adjusting across the $10$ pipelines\" (e.g., with a Bonferroni-type correction) or using a \"held-out dataset\" (data splitting), both of which are standard, correct approaches to mitigate this form of bias. The statement correctly identifies the pathology, illustrates its severity, and suggests appropriate classes of solutions. Thus, it represents a correct critique.\n\n**Verdict: Correct**\n\n**B. If the chosen pipeline applies the Benjamini–Hochberg procedure to control the False Discovery Rate (FDR) at $q=0.05$, then the prior search over $10$ pipelines does not affect error rates, so the reported discoveries remain valid without further adjustment.**\n\nThis statement is fundamentally incorrect. The Benjamini-Hochberg (BH) procedure for FDR control relies on the assumption that the input $p$-values are valid. Specifically, under the null hypothesis, they must be uniformly distributed on $[0,1]$ (or stochastically larger than or equal to a $U[0,1]$ variable). The selection process described in the problem invalidates this assumption. The set of $p$-values from the chosen pipeline is enriched with small values, meaning the null $p$-values are no longer uniformly distributed. Applying the BH procedure to this biased set of $p$-values will result in an actual FDR that is higher than the nominal control level $q$. The pipeline selection is part of the overall procedure and must be accounted for by any error control method.\n\n**Verdict: Incorrect**\n\n**C. If the $10$ pipelines are highly correlated in their outputs, the impact of selecting the most favorable one on Type I error is negligible and can be ignored.**\n\nThis statement is incorrect. While high correlation among the pipeline outputs *mitigates* the inflation of the Type I error, it does not render the effect \"negligible\" such that it \"can be ignored.\" In the extreme case of perfect correlation ($\\rho=1$), all pipelines would yield identical $p$-values, and the selection would be arbitrary and have no effect. However, for any correlation less than perfect, there will be some variation, allowing selection of a pipeline that is favorably biased, which in turn leads to some inflation of the error rate. For instance, an error rate inflation from a nominal $\\alpha=0.05$ to an actual $\\alpha=0.06$ is a $20\\%$ relative increase and is not considered negligible in rigorous scientific work. To claim the effect can be \"ignored\" is to endorse statistically unsound practice.\n\n**Verdict: Incorrect**\n\n**D. Reporting only the pipeline with the most significant results is acceptable provided the other $9$ attempted pipelines are transparently disclosed; no additional statistical correction is needed because transparency addresses the bias.**\n\nThis statement confuses scientific transparency with statistical validity. Transparency is a necessary component of ethical research conduct; it allows others to see what was done and critically evaluate the work. However, disclosing a flawed methodology does not correct the flaw. The reported $p$-values are still biased and do not possess their claimed statistical properties. A reader informed by the disclosure would understand that the results are likely inflated, but the numbers themselves remain uncorrected and invalid for formal inference. Statistical correction is still required to produce valid claims of significance.\n\n**Verdict: Incorrect**\n\n**E. A valid remedy is to use a resampling scheme (such as permutation under the null or the bootstrap) that repeats the entire selection process within each resample, i.e., re-running all $10$ pipelines and selecting the best within each resample, to obtain a null distribution for the selected statistic; this accounts for the data-dependent pipeline selection when computing $p$-values.**\n\nThis statement describes a statistically rigorous and valid method for addressing the selection bias. To obtain a correct null distribution for a statistic, one must simulate the entire process by which that statistic was generated. In this problem, the process includes running all $10$ pipelines and applying the selection rule. By permuting the sample labels (to generate data under the null hypothesis) and then re-running the entire procedure (all $10$ pipelines plus the selection step) many times, one can empirically construct the correct null distribution for the test statistics (e.g., the $p$-values from the \"best\" pipeline). The observed statistics from the original data can then be compared against this empirically derived null distribution to compute properly adjusted $p$-values that account for the selection bias. This is a gold-standard approach for such complex, data-dependent procedures.\n\n**Verdict: Correct**", "answer": "$$\\boxed{AE}$$", "id": "2430523"}, {"introduction": "After learning to identify issues with p-value interpretation, it is time to build a robust and valid solution from first principles. This hands-on coding exercise guides you through the implementation of a permutation test, a powerful and widely used non-parametric method for assessing statistical significance [@problem_id:2430559]. You will compute an exact p-value for the performance of a machine learning model by generating a null distribution tailored to your specific data, providing a rigorous way to determine if your model's predictive power is greater than what would be expected by random chance.", "problem": "You are given a binary classification setting from computational biology and bioinformatics in which a model assigns a real-valued risk score to each patient for a survival endpoint. Let there be $n$ patients indexed by $i \\in \\{1,\\dots,n\\}$, with model scores $s_i \\in \\mathbb{R}$ and binary labels $y_i \\in \\{0,1\\}$, where $y_i = 1$ denotes the event of interest (for example, non-survival) and $y_i = 0$ denotes the absence of the event (for example, survival). The Area Under the Receiver Operating Characteristic Curve (AUC) is defined from first principles as\n$$\n\\mathrm{AUC}(s,y) \\;=\\; \\frac{1}{n_1 n_0} \\sum_{i: y_i=1} \\sum_{j: y_j=0} \\left( \\mathbf{1}\\{s_i > s_j\\} \\;+\\; \\tfrac{1}{2}\\,\\mathbf{1}\\{s_i = s_j\\} \\right),\n$$\nwhere $n_1 = \\sum_{i=1}^n \\mathbf{1}\\{y_i=1\\}$ and $n_0 = \\sum_{i=1}^n \\mathbf{1}\\{y_i=0\\}$, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. If $n_1 = 0$ or $n_0 = 0$, define $\\mathrm{AUC}(s,y) = 0.5$ by convention.\n\nTo assess statistical significance of the observed $\\mathrm{AUC}(s,y)$, consider a label-permutation test under the null hypothesis that labels are exchangeable given the scores. Formally, let $\\mathcal{Y}$ be the set of all labelings $y' \\in \\{0,1\\}^n$ with exactly $n_1$ ones and $n_0$ zeros, and let the permutation null distribution be uniform over $\\mathcal{Y}$. The one-sided right-tail $p$-value for testing whether $\\mathrm{AUC}(s,y)$ exceeds what is expected by chance is\n$$\np \\;=\\; \\frac{1}{|\\mathcal{Y}|} \\sum_{y' \\in \\mathcal{Y}} \\mathbf{1}\\big\\{\\mathrm{AUC}(s,y') \\;\\ge\\; \\mathrm{AUC}(s,y)\\big\\}.\n$$\nIn the degenerate boundary case $n_1 = 0$ or $n_0 = 0$, define the $p$-value to be $1.0$.\n\nYour task is to write a complete program that, for each of the test cases below, computes the exact permutation $p$-value as defined above. The program must implement the definitions as stated, without assuming any approximation. For ties in scores across classes, use the $\\tfrac{1}{2}$ convention given in the AUC definition.\n\nTest suite (each case specifies $(s,y)$ as arrays in the given order):\n- Case $1$ (balanced moderate performance with $\\mathrm{AUC}$ equal to $0.6$ by construction): \n  - $s = [0.6, 0.2, 0.5, 0.1, 0.45, 0.5, 0.5]$\n  - $y = [1, 0, 0, 1, 1, 1, 1]$\n- Case $2$ (near-perfect separation):\n  - $s = [0.9, 0.8, 0.7, 0.2, 0.1]$\n  - $y = [1, 1, 1, 0, 0]$\n- Case $3$ (degenerate boundary where all labels are identical):\n  - $s = [0.3, 0.1, 0.9, 0.7]$\n  - $y = [1, 1, 1, 1]$\n- Case $4$ (interleaved ties yielding $\\mathrm{AUC}$ equal to $0.5$):\n  - $s = [0.1, 0.1, 0.2, 0.2, 0.3, 0.3]$\n  - $y = [1, 0, 1, 0, 1, 0]$\n\nFinal output specification:\n- For each case, output the $p$-value as a real number in decimal form rounded to exactly $6$ digits after the decimal point.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the cases above, for example: $[p_1,p_2,p_3,p_4]$.", "solution": "The task is to compute an exact one-sided permutation $p$-value for an observed AUC. This requires a direct implementation of the provided definitions without approximation. The algorithm consists of two principal steps: the calculation of the AUC for a given set of scores and labels, and the enumeration of the permutation space to determine the $p$-value.\n\nFirst, we define the procedure for calculating $\\mathrm{AUC}(s,y)$. Given a vector of scores $s \\in \\mathbb{R}^n$ and a vector of binary labels $y \\in \\{0,1\\}^n$, we count the number of positive labels, $n_1 = \\sum_{i=1}^n \\mathbf{1}\\{y_i=1\\}$, and negative labels, $n_0 = \\sum_{i=1}^n \\mathbf{1}\\{y_i=0\\}$. Per the problem statement, if either $n_1=0$ or $n_0=0$, the AUC is defined by convention as $0.5$. Otherwise, the AUC is computed using the formula:\n$$\n\\mathrm{AUC}(s,y) \\;=\\; \\frac{1}{n_1 n_0} \\sum_{i: y_i=1} \\sum_{j: y_j=0} \\left( \\mathbf{1}\\{s_i > s_j\\} \\;+\\; \\tfrac{1}{2}\\,\\mathbf{1}\\{s_i = s_j\\} \\right)\n$$\nThis calculation involves iterating through all pairs of positively- and negatively-labeled samples, comparing their scores, and summing the results according to the indicator function $\\mathbf{1}\\{\\cdot\\}$, which accounts for ties with a value of $\\tfrac{1}{2}$. The final sum is normalized by the total number of pairs, $n_1 n_0$.\n\nSecond, we address the computation of the $p$-value. For a given test case $(s,y)$, we first compute the observed AUC, denoted $\\mathrm{AUC}_{\\text{obs}} = \\mathrm{AUC}(s,y)$. The null hypothesis of the permutation test posits that the labels are exchangeable with respect to the scores. The set of all possible label assignments under this null hypothesis, $\\mathcal{Y}$, consists of all binary vectors of length $n$ that have the same counts of positive and negative labels as the original vector $y$, i.e., $n_1$ and $n_0$ respectively. The size of this permutation space is given by the binomial coefficient $|\\mathcal{Y}| = \\binom{n}{n_1}$.\n\nTo compute the exact $p$-value, we must enumerate every unique permutation $y' \\in \\mathcal{Y}$. For each $y'$, we calculate its corresponding $\\mathrm{AUC}(s, y')$. The one-sided right-tail $p$-value is the proportion of these permutations for which the AUC is greater than or equal to the observed AUC:\n$$\np \\;=\\; \\frac{1}{|\\mathcal{Y}|} \\sum_{y' \\in \\mathcal{Y}} \\mathbf{1}\\big\\{\\mathrm{AUC}(s,y') \\;\\ge\\; \\mathrm{AUC}_{\\text{obs}}\\big\\}\n$$\nIn the degenerate case where $n_1=0$ or $n_0=0$, the $p$-value is defined to be $1.0$.\n\nThis procedure is applied to each test case:\n- For Case $1$ ($n=7$, $n_1=5$), we compute $\\mathrm{AUC}_{\\text{obs}}$ and then iterate through all $|\\mathcal{Y}| = \\binom{7}{5} = 21$ label permutations to find the $p$-value.\n- For Case $2$ ($n=5$, $n_1=3$), we have $|\\mathcal{Y}| = \\binom{5}{3} = 10$. The observed data shows perfect separation, so $\\mathrm{AUC}_{\\text{obs}} = 1.0$. The $p$-value is the fraction of permutations that also achieve an AUC of $1.0$. Since all scores are unique, perfect separation is only possible if the three highest scores are assigned the positive label. There is only one such assignment. Thus, the $p$-value is $1/10 = 0.1$.\n- For Case $3$ ($n=4, n_1=4, n_0=0$), this is a degenerate boundary case. By definition, the $p$-value is $1.0$.\n- For Case $4$ ($n=6$, $n_1=3$), we have $|\\mathcal{Y}| = \\binom{6}{3} = 20$. The observed data yields $\\mathrm{AUC}_{\\text{obs}}=0.5$ due to the symmetric distribution of scores between the two classes. The $p$-value is the fraction of permutations where $\\mathrm{AUC} \\ge 0.5$. Due to the symmetry of the AUC statistic ($\\mathrm{AUC}(y') + \\mathrm{AUC}(1-y')=1$) when $n_1=n_0$, the distribution of AUC values under the null is symmetric around $0.5$. The number of cases where $\\mathrm{AUC} > 0.5$ must equal the number where $\\mathrm{AUC} < 0.5$. An analytical count reveals that $8$ permutations yield $\\mathrm{AUC}=0.5$, leaving $12$ permutations for which the AUC is not $0.5$. By symmetry, $6$ must be greater than $0.5$. The total count of permutations with $\\mathrm{AUC} \\ge 0.5$ is $8+6=14$. The $p$-value is therefore $14/20 = 0.7$.\n\nThe implementation will programmatically perform this exhaustive enumeration for each case, ensuring adherence to the exact definitions provided.", "answer": "```python\nimport numpy as np\nfrom itertools import combinations\nfrom scipy.special import comb\n\ndef solve():\n    \"\"\"\n    Computes exact permutation p-values for the AUC statistic for a suite of test cases.\n    \"\"\"\n\n    def calculate_auc(scores: np.ndarray, labels: np.ndarray) -> float:\n        \"\"\"\n        Calculates the Area Under the ROC Curve (AUC) based on the provided formula.\n        \"\"\"\n        n1 = np.sum(labels == 1)\n        n0 = np.sum(labels == 0)\n\n        if n1 == 0 or n0 == 0:\n            return 0.5\n        \n        pos_scores = scores[labels == 1]\n        neg_scores = scores[labels == 0]\n        \n        auc_sum = 0.0\n        for s_i in pos_scores:\n            for s_j in neg_scores:\n                if s_i > s_j:\n                    auc_sum += 1.0\n                elif s_i == s_j:\n                    auc_sum += 0.5\n        \n        return auc_sum / (n1 * n0)\n\n    def compute_exact_p_value(s: np.ndarray, y: np.ndarray) -> float:\n        \"\"\"\n        Computes the exact permutation p-value for the observed AUC.\n        \"\"\"\n        n = len(s)\n        n1 = int(np.sum(y))\n        n0 = n - n1\n\n        if n1 == 0 or n0 == 0:\n            return 1.0\n\n        auc_observed = calculate_auc(s, y)\n        \n        indices = np.arange(n)\n        # Generate all combinations of indices for the positive class\n        pos_label_indices_iter = combinations(indices, n1)\n        \n        num_permutations = comb(n, n1, exact=True)\n        count_ge_observed = 0\n\n        for pos_indices in pos_label_indices_iter:\n            y_perm = np.zeros(n, dtype=int)\n            y_perm[list(pos_indices)] = 1\n            \n            auc_permuted = calculate_auc(s, y_perm)\n            \n            # Using a small tolerance for floating point comparison is good practice,\n            # but for this problem's small denominators, direct comparison is exact.\n            if auc_permuted >= auc_observed:\n                count_ge_observed += 1\n                \n        return count_ge_observed / num_permutations\n\n    test_cases = [\n        # Case 1: balanced moderate performance\n        (np.array([0.6, 0.2, 0.5, 0.1, 0.45, 0.5, 0.5]), \n         np.array([1, 0, 0, 1, 1, 1, 1])),\n        # Case 2: near-perfect separation\n        (np.array([0.9, 0.8, 0.7, 0.2, 0.1]), \n         np.array([1, 1, 1, 0, 0])),\n        # Case 3: degenerate boundary case\n        (np.array([0.3, 0.1, 0.9, 0.7]), \n         np.array([1, 1, 1, 1])),\n        # Case 4: interleaved ties\n        (np.array([0.1, 0.1, 0.2, 0.2, 0.3, 0.3]), \n         np.array([1, 0, 1, 0, 1, 0]))\n    ]\n    \n    results = []\n    for s, y in test_cases:\n        p_value = compute_exact_p_value(s, y)\n        results.append(f\"{p_value:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "2430559"}]}