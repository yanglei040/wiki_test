{"hands_on_practices": [{"introduction": "The ability to detect structural variants (SVs) from sequencing data begins with understanding their characteristic 'footprints'. This exercise challenges you to act as a bioinformatician deciphering raw alignment signals from paired-end reads. By analyzing read pair orientation and the distance between mapped reads, you will develop a decision-making algorithm to distinguish between two of the most common types of SVs: large deletions and novel insertions [@problem_id:2431909]. This practice is fundamental to developing an intuition for how genomic rearrangements manifest in alignment data.", "problem": "You are given a decision problem grounded in paired-end sequencing for structural variant discovery. Consider a DNA sequencing library in which the distribution of physical fragment lengths (insert sizes) for properly paired reads is approximately Gaussian with mean $\\mu$ and standard deviation $\\sigma$ in base pairs (bp). At a genomic locus, you observe paired-end mappings characterized by the following two observables: (i) the orientation of each mapped read pair relative to the reference genome (categories: FR, RF, FF, RR), and (ii) the mapped distance on the reference between the two reads of a pair (the mapped insert size) for those pairs in the FR orientation. Your task is to decide, for each locus, whether the evidence is more consistent with a large deletion in the sample relative to the reference (output integer $0$) or with a novel retrotransposon insertion in the sample relative to the reference (output integer $1$), using only these two observables.\n\nDefinitions and assumptions:\n- Paired-End (PE) sequencing produces read pairs. The FR orientation denotes an inward-facing configuration that is the expected proper-pair orientation under standard Illumina protocols; RF, FF, and RR denote outward-facing or same-strand configurations regarded as discordant with respect to the library’s expected orientation.\n- A large deletion in the sample relative to the reference tends to produce an enrichment of FR pairs whose mapped insert sizes on the reference are stochastically larger than the library distribution characterized by $\\mu$ and $\\sigma$, with FR remaining the predominant orientation locally.\n- A novel retrotransposon insertion in the sample relative to the reference tends to produce an enrichment of non-FR orientations (RF, FF, RR) and FR pairs whose mapped insert sizes on the reference are stochastically smaller than the library distribution characterized by $\\mu$ and $\\sigma$.\n\nInput specification for each test case:\n- Library parameters $\\mu$ and $\\sigma$ (both in bp).\n- Orientation counts $c_{\\mathrm{FR}}$, $c_{\\mathrm{RF}}$, $c_{\\mathrm{FF}}$, $c_{\\mathrm{RR}}$ measured in a fixed window around the locus.\n- A list of mapped insert sizes (in bp) for the subset of read pairs observed in FR orientation at that locus.\n\nOutput specification:\n- For each test case, output a single integer: $0$ if the locus is classified as a deletion and $1$ if it is classified as a novel retrotransposon insertion.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets with no spaces, for example, $\"[0,1,1,0]\"$.\n\nTest suite:\n- Test case $1$:\n  - $\\mu = 350$, $\\sigma = 50$.\n  - $c_{\\mathrm{FR}} = 40$, $c_{\\mathrm{RF}} = 1$, $c_{\\mathrm{FF}} = 0$, $c_{\\mathrm{RR}} = 0$.\n  - FR insert sizes: $\\{520,540,560,590,610\\}$.\n- Test case $2$:\n  - $\\mu = 350$, $\\sigma = 50$.\n  - $c_{\\mathrm{FR}} = 10$, $c_{\\mathrm{RF}} = 25$, $c_{\\mathrm{FF}} = 5$, $c_{\\mathrm{RR}} = 3$.\n  - FR insert sizes: $\\{150,160,170,180,190,200,210\\}$.\n- Test case $3$:\n  - $\\mu = 350$, $\\sigma = 50$.\n  - $c_{\\mathrm{FR}} = 22$, $c_{\\mathrm{RF}} = 2$, $c_{\\mathrm{FF}} = 1$, $c_{\\mathrm{RR}} = 1$.\n  - FR insert sizes: $\\{410,430,420,380,370\\}$.\n- Test case $4$:\n  - $\\mu = 400$, $\\sigma = 40$.\n  - $c_{\\mathrm{FR}} = 8$, $c_{\\mathrm{RF}} = 30$, $c_{\\mathrm{FF}} = 4$, $c_{\\mathrm{RR}} = 2$.\n  - FR insert sizes: $\\{260,280,300,310\\}$.\n- Test case $5$:\n  - $\\mu = 300$, $\\sigma = 30$.\n  - $c_{\\mathrm{FR}} = 15$, $c_{\\mathrm{RF}} = 14$, $c_{\\mathrm{FF}} = 3$, $c_{\\mathrm{RR}} = 3$.\n  - FR insert sizes: $\\{200,210,190,205,195\\}$.\n\nFinal output format:\n- Your program must compute a classification for each of the above test cases and print a single line containing the results as a comma-separated list of integers enclosed in square brackets with no spaces, for example, $\"[0,1,0,1,1]\"$.", "solution": "The problem has been validated and is determined to be scientifically grounded, well-posed, and objective. It poses a standard binary classification task within the domain of computational biology, specifically the analysis of structural variation from paired-end DNA sequencing data.\n\nThe task is to classify a genomic locus as either containing a large deletion (output $0$) or a novel retrotransposon insertion (output $1$) relative to a reference genome. We are provided with two categories of evidence for this decision:\n$1$. The counts of read pairs in different orientations relative to the reference: inward-facing ($c_{\\mathrm{FR}}$) and discordant orientations ($c_{\\mathrm{RF}}$, $c_{\\mathrm{FF}}$, $c_{\\mathrm{RR}}$).\n$2$. A sample of mapped insert sizes, $\\{x_1, x_2, \\ldots, x_n\\}$, for the subset of $n$ read pairs observed in the FR orientation.\nThese observations are to be interpreted in the context of a sequencing library where properly paired reads exhibit a physical fragment length distribution that is Gaussian with mean $\\mu$ and standard deviation $\\sigma$.\n\nThe decision algorithm is derived directly from the formalization of the two mutually exclusive genomic signatures described in the problem statement.\n\n**Hypothesis $H_0$: Large Deletion**\nThe signature for a large deletion is described by two concurrent phenomena:\n- **Orientation evidence:** An enrichment of FR-oriented pairs, with FR remaining the predominant orientation. This is formalized as the count of FR pairs exceeding the total count of all non-FR (discordant) pairs.\n$$c_{\\mathrm{FR}} > c_{\\mathrm{RF}} + c_{\\mathrm{FF}} + c_{\\mathrm{RR}}$$\n- **Insert size evidence:** The mapped insert sizes for FR pairs are stochastically larger than the library's physical fragment length distribution. This is because reads spanning the deletion in the sample genome map to distant points on the reference, creating an artificially large mapped insert size. This is formalized by comparing the sample mean of the observed FR insert sizes, $\\bar{x}_{\\mathrm{FR}}$, to the library mean, $\\mu$.\n$$ \\bar{x}_{\\mathrm{FR}} = \\frac{1}{n} \\sum_{i=1}^{n} x_i > \\mu $$\n\n**Hypothesis $H_1$: Novel Retrotransposon Insertion**\nThe signature for a novel retrotransposon insertion is described by phenomena that are characteristically distinct from a deletion:\n- **Orientation evidence:** An enrichment of non-FR orientations. This occurs because read pairs spanning the insertion site have one read mapping to the flanking genomic sequence and the other mapping to a different copy of the retrotransposon elsewhere in the genome, leading to discordant orientations. This is formalized as the count of FR pairs not exceeding the count of non-FR pairs.\n$$c_{\\mathrm{FR}} \\le c_{\\mathrm{RF}} + c_{\\mathrm{FF}} + c_{\\mathrm{RR}}$$\n- **Insert size evidence:** The mapped insert sizes for FR pairs are stochastically smaller than the library's distribution. This applies to fragment pairs that are entirely contained within the novel insertion, which, when mapped back to the single-copy reference, appear to have originated from a smaller fragment than they did. This is formalized as:\n$$ \\bar{x}_{\\mathrm{FR}} < \\mu $$\n\n**Decision Rule**\nThe problem asks for a decisive classification. The description of the deletion signature implies a logical conjunction of its two evidentiary components. A locus is classified as a deletion if, and only if, both the orientation evidence and the insert size evidence are consistent with a deletion. Given that this is a binary classification task between two well-defined alternatives, any case not meeting the strict criteria for a deletion is consequently classified as an insertion. This leads to the following robust and complete decision rule:\n\nA locus is classified as a **deletion (output $0$)** if both of the following conditions are true:\n$1$. $c_{\\mathrm{FR}} > (c_{\\mathrm{RF}} + c_{\\mathrm{FF}} + c_{\\mathrm{RR}})$\n$2$. $\\bar{x}_{\\mathrm{FR}} > \\mu$\n\nOtherwise, the locus is classified as a **novel retrotransposon insertion (output $1$)**.\n\nThis logical framework is sufficient for all provided test cases, as none exhibit conflicting evidence (e.g., FR predominance with smaller-than-average inserts). The use of a more complex model incorporating the standard deviation $\\sigma$, for instance, to calculate Z-scores or likelihoods, would require additional assumptions about the relative weighting of evidence types, which are not provided in the problem statement. The chosen algorithm represents the most direct and scientifically sound interpretation of the problem as stated.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the structural variant classification problem based on paired-end sequencing data.\n    \"\"\"\n    \n    # Test cases defined in the problem statement.\n    test_cases = [\n        # (mu, sigma, c_fr, c_rf, c_ff, c_rr, fr_sizes)\n        (350, 50, 40, 1, 0, 0, [520, 540, 560, 590, 610]),\n        (350, 50, 10, 25, 5, 3, [150, 160, 170, 180, 190, 200, 210]),\n        (350, 50, 22, 2, 1, 1, [410, 430, 420, 380, 370]),\n        (400, 40, 8, 30, 4, 2, [260, 280, 300, 310]),\n        (300, 30, 15, 14, 3, 3, [200, 210, 190, 205, 195]),\n    ]\n\n    results = []\n    for case in test_cases:\n        mu, _, c_fr, c_rf, c_ff, c_rr, fr_sizes = case\n        \n        # Calculate the total count of non-FR (discordant) read pairs.\n        c_non_fr = c_rf + c_ff + c_rr\n        \n        # Calculate the sample mean of the FR insert sizes.\n        # Handle the edge case of an empty list of FR sizes, though not present in tests.\n        if not fr_sizes:\n            # If no FR sizes are available, the insert size criterion cannot be tested.\n            # The decision rule defaults to 'insertion' as the strict 'deletion' criteria cannot be met.\n            x_bar_fr = -np.inf # Ensures the comparison 'x_bar_fr > mu' is false.\n        else:\n            x_bar_fr = np.mean(fr_sizes)\n        \n        # A deletion signature requires FR orientation to be predominant AND\n        # the mean FR insert size to be larger than the library mean.\n        is_deletion_signature = (c_fr > c_non_fr) and (x_bar_fr > mu)\n        \n        if is_deletion_signature:\n            # Output 0 for a deletion.\n            results.append(0)\n        else:\n            # Otherwise, classify as an insertion (output 1).\n            # This covers cases where orientation or insert size (or both)\n            # are consistent with an insertion signature.\n            results.append(1)\n\n    # Print the final results in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2431909"}, {"introduction": "While paired-end reads provide clues about the presence of an SV, split-read alignments are critical for pinpointing the exact breakpoint locations. However, repetitive regions in the genome can often produce misleading split-read signals, known as mapping artifacts. This practice introduces a quantitative approach to build confidence in your findings by developing a scoring function that combines the sequence complexity of the read itself, measured using Shannon entropy, with its mapping quality score [@problem_id:2431938]. Mastering this skill will enable you to filter low-quality candidates and improve the precision of your SV calls.", "problem": "You are given a formal definition of a score that quantifies the reliability of a split-read alignment purported to support a genomic inversion, using the deoxyribonucleic acid (DNA) sequence complexity of the two aligned read segments and their mapping qualities. The DNA alphabet is restricted to the set of symbols {A, C, G, T}. For each split-read alignment, there are two flanks (left and right) with corresponding sequences and Phred-scaled mapping qualities.\n\nDefine, for a DNA sequence $s$ of length $n$, with mononucleotide counts $c_A$, $c_C$, $c_G$, $c_T$ satisfying $c_A + c_C + c_G + c_T = n$, the empirical base frequencies $p_b = c_b / n$ for $b \\in \\{A,C,G,T\\}$. Define the Shannon entropy in bits as\n$$\nH(s) = - \\sum_{b \\in \\{A,C,G,T\\}} p_b \\log_2 p_b,\n$$\nwith the convention that terms with $p_b = 0$ contribute $0$. Define the normalized entropy\n$$\nh(s) = \\frac{H(s)}{\\log_2 4} = \\frac{H(s)}{2},\n$$\nso that $h(s) \\in [0,1]$.\n\nLet the left and right flanks have sequences $s_L$ and $s_R$ and Phred-scale mapping qualities $Q_L$ and $Q_R$ respectively. Convert each mapping quality $Q$ to a probability of correct mapping using\n$$\nP(Q) = 1 - 10^{-Q/10}.\n$$\n\nDefine the composite reliability score\n$$\nS(s_L,s_R,Q_L,Q_R) = \\min\\{ h(s_L), h(s_R) \\} \\times \\min\\{ P(Q_L), P(Q_R) \\}.\n$$\n\nGiven a decision threshold $\\tau$, classify a split-read alignment as a true inversion-supporting signal if and only if $S \\ge \\tau$, and otherwise as a mapping artifact, yielding a boolean result.\n\nImplement a program that, for the following test suite, computes the boolean classification for each case using the threshold $\\tau = 0.7$:\n\n- Case $1$: $s_L=$ \"ACGTACGTACGT\", $s_R=$ \"TGCATGCATGCA\", $Q_L = 40$, $Q_R = 42$.\n- Case $2$: $s_L=$ \"ATATATATATAT\", $s_R=$ \"ATATATATATAT\", $Q_L = 10$, $Q_R = 10$.\n- Case $3$: $s_L=$ \"GATTACAGATTACA\", $s_R=$ \"TCGATCGATCGA\", $Q_L = 35$, $Q_R = 5$.\n- Case $4$: $s_L=$ \"AAAAAAAAAAAA\", $s_R=$ \"ACGTACGTACGT\", $Q_L = 60$, $Q_R = 60$.\n- Case $5$: $s_L=$ \"CG\", $s_R=$ \"CG\", $Q_L = 20$, $Q_R = 20$.\n- Case $6$: $s_L=$ \"ACGTACGT\", $s_R=$ \"GTACGTAC\", $Q_L = 20$, $Q_R = 20$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the cases above. For example, a valid output format is \"[True,False,True]\". The output must contain exactly six boolean values corresponding to Cases $1$ through $6$.", "solution": "The problem statement is first subjected to rigorous validation.\n\nStep 1: Extracted Givens\nThe problem provides the following definitions and data:\n- DNA alphabet: $\\Sigma = \\{A, C, G, T\\}$.\n- For a sequence $s$ of length $n$ with mononucleotide counts $c_A, c_C, c_G, c_T$ such that $\\sum_{b \\in \\Sigma} c_b = n$.\n- Empirical base frequency: $p_b = c_b / n$ for $b \\in \\Sigma$.\n- Shannon entropy in bits: $H(s) = - \\sum_{b \\in \\Sigma} p_b \\log_2 p_b$, with $0 \\log_2 0 = 0$.\n- Normalized entropy: $h(s) = H(s) / \\log_2 4 = H(s) / 2$, where $h(s) \\in [0,1]$.\n- Left and right flank sequences: $s_L, s_R$.\n- Phred-scale mapping qualities: $Q_L, Q_R$.\n- Probability of correct mapping: $P(Q) = 1 - 10^{-Q/10}$.\n- Composite reliability score: $S(s_L, s_R, Q_L, Q_R) = \\min\\{ h(s_L), h(s_R) \\} \\times \\min\\{ P(Q_L), P(Q_R) \\}$.\n- Decision threshold: $\\tau = 0.7$.\n- Classification rule: True if $S \\ge \\tau$, False otherwise.\n- Test Cases:\n    1. $s_L=$ \"ACGTACGTACGT\", $s_R=$ \"TGCATGCATGCA\", $Q_L = 40$, $Q_R = 42$.\n    2. $s_L=$ \"ATATATATATAT\", $s_R=$ \"ATATATATATAT\", $Q_L = 10$, $Q_R = 10$.\n    3. $s_L=$ \"GATTACAGATTACA\", $s_R=$ \"TCGATCGATCGA\", $Q_L = 35$, $Q_R = 5$.\n    4. $s_L=$ \"AAAAAAAAAAAA\", $s_R=$ \"ACGTACGTACGT\", $Q_L = 60$, $Q_R = 60$.\n    5. $s_L=$ \"CG\", $s_R=$ \"CG\", $Q_L = 20$, $Q_R = 20$.\n    6. $s_L=$ \"ACGTACGT\", $s_R=$ \"GTACGTAC\", $Q_L = 20$, $Q_R = 20$.\n\nStep 2: Validation Using Extracted Givens\nThe problem is evaluated against the specified criteria.\n- **Scientifically Grounded**: The problem is well-grounded. It employs standard definitions from information theory (Shannon entropy) and bioinformatics (Phred quality scores, sequence complexity). The proposed reliability score is a logical, albeit simplified, heuristic for evaluating evidence for structural variants. It is free of pseudoscience.\n- **Well-Posed**: The problem is well-posed. All mathematical relations are explicitly defined. All constants and input data for the test suite are provided. A unique boolean output is expected for each case.\n- **Objective**: The problem is stated with objective, mathematical precision. There are no subjective or ambiguous terms.\n\nThe problem exhibits no flaws such as scientific unsoundness, non-formalizability, incompleteness, contradiction, infeasibility, or ill-posed structure.\n\nStep 3: Verdict and Action\nThe problem is deemed **valid**. A solution will be furnished.\n\nThe solution requires the implementation of a classification procedure based on a composite reliability score $S$. This score must be calculated for each of the six test cases and compared against the decision threshold $\\tau = 0.7$.\n\nThe calculation of the score $S$ is composed of two primary components: the sequence complexity, measured by normalized entropy $h(s)$, and the mapping reliability, measured by the probability of correct mapping $P(Q)$.\n\nFirst, we define the calculation of the normalized entropy $h(s)$. For a given sequence $s$ of length $n$, we count the occurrences of each base $c_b$ for $b \\in \\{A, C, G, T\\}$. The frequencies are $p_b = c_b/n$. The Shannon entropy is then $H(s) = - \\sum p_b \\log_2 p_b$. The maximum entropy for a DNA sequence occurs when all four bases are equally likely ($p_b=0.25$), yielding $H_{max} = \\log_2 4 = 2$ bits. Normalizing by this maximum value gives $h(s) = H(s)/2$, which maps the complexity to the interval $[0,1]$. A value of $h(s)=0$ corresponds to a homopolymer (e.g., \"AAAA\"), representing minimum complexity. A value of $h(s)=1$ corresponds to a sequence with uniform base distribution, representing maximum complexity.\n\nSecond, we define the calculation of the mapping probability $P(Q)$. A Phred-scaled quality score $Q$ is logarithmically related to the error probability $P_{error}$. Specifically, $Q = -10 \\log_{10} P_{error}$, which implies $P_{error} = 10^{-Q/10}$. The probability of a correct mapping is thus $P(Q) = 1 - P_{error} = 1 - 10^{-Q/10}$.\n\nThe composite score $S$ combines these two metrics: $S = \\min\\{h(s_L), h(s_R)\\} \\times \\min\\{P(Q_L), P(Q_R)\\}$. This formulation is conservative; the final score is limited by the flank with the lower sequence complexity and the flank with the lower mapping quality. An alignment is classified as a true signal if $S \\ge \\tau$.\n\nWe now apply this procedure to each case with $\\tau = 0.7$.\n\nCase 1: $s_L=$ \"ACGTACGTACGT\", $s_R=$ \"TGCATGCATGCA\", $Q_L = 40$, $Q_R = 42$.\n- For both $s_L$ and $s_R$, $n=12$ and $c_A=c_C=c_G=c_T=3$. Thus, $p_b = 3/12 = 0.25$ for all bases.\n- $H(s_L) = H(s_R) = -4 \\times (0.25 \\log_2 0.25) = -4 \\times (0.25 \\times -2) = 2$.\n- $h(s_L) = h(s_R) = 2/2 = 1$. $\\min\\{h\\} = 1$.\n- $P(Q_L) = 1 - 10^{-40/10} = 1 - 10^{-4} = 0.9999$.\n- $P(Q_R) = 1 - 10^{-42/10} = 1 - 10^{-4.2} \\approx 0.999937$.\n- $\\min\\{P\\} = 0.9999$.\n- $S = 1 \\times 0.9999 = 0.9999$.\n- $0.9999 \\ge 0.7$. Classification: **True**.\n\nCase 2: $s_L=$ \"ATATATATATAT\", $s_R=$ \"ATATATATATAT\", $Q_L = 10$, $Q_R = 10$.\n- For both sequences, $n=12$, $c_A=6, c_T=6$. $p_A=p_T=0.5$.\n- $H(s_L) = H(s_R) = -2 \\times (0.5 \\log_2 0.5) = -2 \\times (0.5 \\times -1) = 1$.\n- $h(s_L) = h(s_R) = 1/2 = 0.5$. $\\min\\{h\\} = 0.5$.\n- $P(Q_L) = P(Q_R) = 1 - 10^{-10/10} = 1 - 0.1 = 0.9$. $\\min\\{P\\} = 0.9$.\n- $S = 0.5 \\times 0.9 = 0.45$.\n- $0.45 < 0.7$. Classification: **False**.\n\nCase 3: $s_L=$ \"GATTACAGATTACA\", $s_R=$ \"TCGATCGATCGA\", $Q_L = 35$, $Q_R = 5$.\n- For $s_L$, $n=14$, $c_A=6, c_C=2, c_G=2, c_T=4$. Frequencies: $p_A=6/14, p_C=2/14, p_G=2/14, p_T=4/14$.\n- $H(s_L) \\approx -(\\frac{6}{14}\\log_2\\frac{6}{14} + \\frac{2}{14}\\log_2\\frac{2}{14} + \\frac{2}{14}\\log_2\\frac{2}{14} + \\frac{4}{14}\\log_2\\frac{4}{14}) \\approx 1.8423$.\n- $h(s_L) \\approx 1.8423 / 2 \\approx 0.9212$.\n- For $s_R$, $n=12$, $p_b = 0.25$ for all bases, so $h(s_R)=1$.\n- $\\min\\{h\\} \\approx 0.9212$.\n- $P(Q_L) = 1 - 10^{-35/10} = 1 - 10^{-3.5} \\approx 0.9997$.\n- $P(Q_R) = 1 - 10^{-5/10} = 1 - 10^{-0.5} \\approx 0.6838$.\n- $\\min\\{P\\} \\approx 0.6838$.\n- $S \\approx 0.9212 \\times 0.6838 \\approx 0.6299$.\n- $0.6299 < 0.7$. Classification: **False**.\n\nCase 4: $s_L=$ \"AAAAAAAAAAAA\", $s_R=$ \"ACGTACGTACGT\", $Q_L = 60$, $Q_R = 60$.\n- For $s_L$, a homopolymer, $p_A=1$ and other $p_b=0$.\n- $H(s_L) = -(1 \\log_2 1) = 0$. $h(s_L) = 0$.\n- For $s_R$, $p_b=0.25$, so $h(s_R)=1$.\n- $\\min\\{h\\} = 0$.\n- $S = 0 \\times \\min\\{P(60), P(60)\\} = 0$.\n- $0 < 0.7$. Classification: **False**.\n\nCase 5: $s_L=$ \"CG\", $s_R=$ \"CG\", $Q_L = 20$, $Q_R = 20$.\n- For both sequences, $n=2$, $c_C=1, c_G=1$. $p_C=p_G=0.5$.\n- This is analogous to Case 2 in terms of frequencies: $H(s_{L,R})=1$, $h(s_{L,R})=0.5$.\n- $\\min\\{h\\} = 0.5$.\n- $P(Q_{L,R}) = 1 - 10^{-20/10} = 1 - 10^{-2} = 0.99$. $\\min\\{P\\}=0.99$.\n- $S = 0.5 \\times 0.99 = 0.495$.\n- $0.495 < 0.7$. Classification: **False**.\n\nCase 6: $s_L=$ \"ACGTACGT\", $s_R=$ \"GTACGTAC\", $Q_L = 20$, $Q_R = 20$.\n- For both sequences, $n=8$, $c_A=c_C=c_G=c_T=2$. $p_b=2/8=0.25$.\n- This is analogous to Case 1 in terms of frequencies: $H(s_{L,R})=2$, $h(s_{L,R})=1$.\n- $\\min\\{h\\} = 1$.\n- $P(Q_{L,R}) = 1 - 10^{-20/10} = 0.99$. $\\min\\{P\\}=0.99$.\n- $S = 1 \\times 0.99 = 0.99$.\n- $0.99 \\ge 0.7$. Classification: **True**.\n\nThe final results are: [True, False, False, False, False, True].", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom collections import Counter\n\ndef calculate_normalized_entropy(s: str) -> float:\n    \"\"\"\n    Calculates the normalized Shannon entropy for a DNA sequence.\n    The entropy is normalized by log2(4) = 2.\n    \"\"\"\n    n = len(s)\n    if n == 0:\n        return 0.0\n\n    counts = Counter(s)\n    dna_bases = {'A', 'C', 'G', 'T'}\n    entropy = 0.0\n    \n    for base in dna_bases:\n        count = counts.get(base, 0)\n        if count > 0:\n            p_b = count / n\n            entropy -= p_b * np.log2(p_b)\n    \n    # Normalize by log2(4), which is 2.\n    h_s = entropy / 2.0\n    return h_s\n\ndef calculate_mapping_probability(Q: int) -> float:\n    \"\"\"\n    Converts a Phred-scaled mapping quality score to a probability of correct mapping.\n    \"\"\"\n    # The problem specifies Q as an integer (from the test cases), so we can treat it as such.\n    return 1.0 - 10**(-float(Q) / 10.0)\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite and prints the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is (s_L, s_R, Q_L, Q_R).\n    test_cases = [\n        (\"ACGTACGTACGT\", \"TGCATGCATGCA\", 40, 42),\n        (\"ATATATATATAT\", \"ATATATATATAT\", 10, 10),\n        (\"GATTACAGATTACA\", \"TCGATCGATCGA\", 35, 5),\n        (\"AAAAAAAAAAAA\", \"ACGTACGTACGT\", 60, 60),\n        (\"CG\", \"CG\", 20, 20),\n        (\"ACGTACGT\", \"GTACGTAC\", 20, 20),\n    ]\n\n    # The decision threshold tau.\n    tau = 0.7\n    \n    results = []\n    \n    for s_L, s_R, Q_L, Q_R in test_cases:\n        # Calculate normalized entropies for left and right flanks.\n        h_L = calculate_normalized_entropy(s_L)\n        h_R = calculate_normalized_entropy(s_R)\n        \n        # Calculate mapping probabilities for left and right flanks.\n        P_L = calculate_mapping_probability(Q_L)\n        P_R = calculate_mapping_probability(Q_R)\n        \n        # Determine the minimum of each pair of metrics.\n        min_h = min(h_L, h_R)\n        min_P = min(P_L, P_R)\n        \n        # Calculate the composite reliability score S.\n        S = min_h * min_P\n        \n        # Classify the alignment based on the threshold tau.\n        classification = S >= tau\n        results.append(classification)\n\n    # Final print statement in the exact required format.\n    # The str() of a bool in Python is 'True' or 'False'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "2431938"}, {"introduction": "Beyond local signatures from individual reads, large-scale SVs like copy number variations (CNVs) are often detected by analyzing regional changes in read depth. This signal is inherently noisy, making it challenging to determine the true underlying copy number states. This advanced exercise introduces you to Hidden Markov Models (HMMs), a powerful statistical tool for decoding a sequence of hidden states—in this case, genomic copy numbers—from a series of noisy observations [@problem_id:2431910]. Implementing the Viterbi algorithm to solve this problem will provide you with direct experience in applying sophisticated probabilistic models that are central to modern computational genomics.", "problem": "You are given a generative model of read-depth signals for structural variant discovery in which the hidden state at each genomic window is the absolute copy number. The model is a Hidden Markov Model (HMM) where the hidden state space is the set of absolute copy numbers $\\{0,1,2,\\ldots,8\\}$ and the emissions are normalized read depths for contiguous genomic windows of fixed size. The task is to determine, for each provided observation sequence, the sequence of hidden states that maximizes the posterior probability under the model.\n\nModel specification:\n- Hidden state space: $\\mathcal{C}=\\{0,1,2,\\ldots,8\\}$.\n- Observation at window $t$: a real value $D_t \\in \\mathbb{R}$ representing normalized read depth.\n- Initial distribution $\\pi(c)$ over $c \\in \\mathcal{C}$:\n  $$\\pi(c) \\propto \\exp\\!\\left(-\\beta\\,(c-c_0)^2\\right), \\quad \\text{with } \\beta=1.0 \\text{ and } c_0=2,$$\n  normalized so that $\\sum_{c \\in \\mathcal{C}} \\pi(c) = 1$.\n- Transition probabilities $T(i,j)$ for transitions from $i \\in \\mathcal{C}$ to $j \\in \\mathcal{C}$ are defined by unnormalized weights\n  $$w(i,j)=\\begin{cases}\n  \\kappa & \\text{if } j=i,\\\\\n  \\exp\\!\\left(-\\gamma \\,\\lvert j-i \\rvert \\right) & \\text{if } j \\neq i,\n  \\end{cases}$$\n  with parameters $\\kappa=20.0$ and $\\gamma=\\ln 2$. The row-stochastic transition probabilities are then\n  $$T(i,j)=\\frac{w(i,j)}{\\sum\\limits_{j' \\in \\mathcal{C}} w(i,j')}.$$\n- Emission density for observation $d$ given state $c$ is Gaussian (Normal) with mean proportional to copy number:\n  $$f(d \\mid c) = \\frac{1}{\\sigma \\sqrt{2\\pi}}\\exp\\!\\left(-\\frac{(d-\\mu_c)^2}{2\\sigma^2}\\right), \\quad \\mu_c = \\frac{c}{2}, \\quad \\sigma = 0.15.$$\n\nFor an observation sequence $D_{1:n}=(D_1,D_2,\\ldots,D_n)$ and a hidden sequence $C_{1:n}=(C_1,C_2,\\ldots,C_n)$, the joint probability is\n$$P(C_{1:n},D_{1:n})=\\pi(C_1)\\left(\\prod_{t=2}^{n} T(C_{t-1},C_t)\\right)\\left(\\prod_{t=1}^{n} f(D_t \\mid C_t)\\right).$$\nYour task is: for each observation sequence below, determine a sequence $C_{1:n} \\in \\mathcal{C}^n$ that maximizes the posterior $P(C_{1:n} \\mid D_{1:n}) \\propto P(C_{1:n},D_{1:n})$. If more than one maximizing sequence exists, select the lexicographically smallest sequence under the usual order on $\\mathbb{Z}^{n}$ (that is, compare two sequences element-wise from left to right and prefer the one with the smaller value at the first position where they differ).\n\nTest suite:\n- Case A (balanced diploid with a single-copy deletion segment): $D_{1:8}=(\\,0.98,\\,1.03,\\,1.01,\\,0.51,\\,0.48,\\,0.50,\\,0.99,\\,1.02\\,)$.\n- Case B (homozygous deletion segment): $D_{1:4}=(\\,0.08,\\,0.05,\\,0.04,\\,0.07\\,)$.\n- Case C (focal high-level amplification amid diploid): $D_{1:5}=(\\,1.01,\\,1.99,\\,2.49,\\,2.51,\\,1.02\\,)$.\n- Case D (intermediate signal consistent with odd copy): $D_{1:4}=(\\,1.40,\\,1.60,\\,1.45,\\,1.55\\,)$.\n- Case E (upper boundary high copy): $D_{1:2}=(\\,3.95,\\,4.05\\,)$.\n\nRequired final output format:\n- Your program must produce a single line containing the results for all cases as a comma-separated list enclosed in square brackets, with no whitespace characters.\n- Each per-case result must be the maximizing hidden-state sequence represented as a list of integers enclosed in square brackets, also with no whitespace. For example, a valid overall output structure with two cases would be $[[0,1],[2,2,3]]$.\n- There are no physical units in this problem. All numerical results are dimensionless.\n\nYour program must compute, for each test case, the maximizing hidden-state sequence as specified above and output the five sequences in order A, B, C, D, E, aggregated into a single line as described.", "solution": "The problem presented is a valid, well-posed scientific problem from the field of computational biology. It requires finding the most probable sequence of hidden states in a Hidden Markov Model (HMM), a classic problem for which a standard algorithm exists. All model parameters and data are specified completely and consistently. I will proceed with the solution.\n\nThe objective is to find the sequence of hidden copy-number states $C_{1:n} = (C_1, C_2, \\ldots, C_n)$ that maximizes the posterior probability $P(C_{1:n} \\mid D_{1:n})$ for a given sequence of observed read-depths $D_{1:n} = (D_1, D_2, \\ldots, D_n)$. Maximizing the posterior is equivalent to maximizing the joint probability $P(C_{1:n}, D_{1:n})$, since the evidence $P(D_{1:n})$ is constant for a fixed observation sequence. The joint probability is given by:\n$$P(C_{1:n},D_{1:n})=\\pi(C_1)\\left(\\prod_{t=2}^{n} T(C_{t-1},C_t)\\right)\\left(\\prod_{t=1}^{n} f(D_t \\mid C_t)\\right)$$\nwhere $\\pi$ is the initial state distribution, $T$ is the transition probability matrix, and $f$ is the emission probability density.\n\nDirect evaluation of all possible state sequences is computationally infeasible, as there are $\\lvert\\mathcal{C}\\rvert^n$ such sequences. The Viterbi algorithm provides an efficient dynamic programming approach to find this optimal path. To prevent numerical underflow from the multiplication of many small probabilities, all calculations are performed in logarithmic space. The objective becomes maximizing the joint log-probability:\n$$\\log P(C_{1:n},D_{1:n}) = \\log\\pi(C_1) + \\sum_{t=2}^{n} \\log T(C_{t-1},C_t) + \\sum_{t=1}^{n} \\log f(D_t \\mid C_t)$$\n\nFirst, we pre-compute the logarithm of all model parameters:\n1.  **Log-Initial Probabilities**: The unnormalized initial probabilities are $\\pi'(c) = \\exp(-\\beta(c-c_0)^2)$ for $c \\in \\mathcal{C}=\\{0,1,\\ldots,8\\}$, with $\\beta=1.0$ and $c_0=2$. We compute the normalization constant $Z_\\pi = \\sum_{c \\in \\mathcal{C}} \\pi'(c)$ and then the log-probabilities $\\log\\pi(c) = \\log(\\pi'(c)) - \\log(Z_\\pi)$.\n\n2.  **Log-Transition Probabilities**: The unnormalized transition weights are $w(i,j) = \\kappa$ for $i=j$ and $w(i,j) = \\exp(-\\gamma|j-i|)$ for $i \\neq j$, with $\\kappa=20.0$ and $\\gamma=\\ln 2$. For each state $i$, we compute the normalization constant $Z_T(i) = \\sum_{j \\in \\mathcal{C}} w(i,j)$. The log-transition probabilities are then $\\log T(i,j) = \\log w(i,j) - \\log Z_T(i)$. This gives a $\\lvert\\mathcal{C}\\rvert \\times \\lvert\\mathcal{C}\\rvert$ matrix of log-probabilities.\n\n3.  **Log-Emission Probabilities**: The emission density is a Gaussian $f(d \\mid c) = \\mathcal{N}(d; \\mu_c, \\sigma^2)$ with mean $\\mu_c = c/2$ and standard deviation $\\sigma = 0.15$. The log-density is:\n    $$\\log f(d \\mid c) = -\\log(\\sigma\\sqrt{2\\pi}) - \\frac{(d - \\mu_c)^2}{2\\sigma^2}$$\n    For each observation sequence $D_{1:n}$, we compute a matrix of log-emission probabilities of size $n \\times \\lvert\\mathcal{C}\\rvert$.\n\nThe Viterbi algorithm proceeds as follows:\n\nLet $\\delta_t(j)$ be the maximum log-probability of any path of length $t$ ending in state $j$, and $\\psi_t(j)$ be the predecessor state on that path.\n\n**1. Initialization ($t=1$):**\nFor each state $j \\in \\mathcal{C}$:\n$$\\delta_1(j) = \\log \\pi(j) + \\log f(D_1 \\mid j)$$\n\n**2. Recursion (for $t=2, \\ldots, n$):**\nFor each state $j \\in \\mathcal{C}$:\n$$\\delta_t(j) = \\log f(D_t \\mid j) + \\max_{i \\in \\mathcal{C}} \\left( \\delta_{t-1}(i) + \\log T(i,j) \\right)$$\n$$\\psi_t(j) = \\arg\\max_{i \\in \\mathcal{C}} \\left( \\delta_{t-1}(i) + \\log T(i,j) \\right)$$\nIf the $\\arg\\max$ has ties, we select the predecessor $i$ with the smallest index.\n\n**3. Termination:**\nThe maximum log-probability of any complete path is $P^* = \\max_{j \\in \\mathcal{C}} \\delta_n(j)$.\n\n**4. Path Backtracking and Tie-Breaking:**\nThe problem requires selecting the lexicographically smallest sequence if multiple sequences yield the same maximum probability. A simple tie-breaking rule during the recursion is insufficient to guarantee this globally. The correct procedure is:\na. Identify all final states $J^* = \\{j \\in \\mathcal{C} \\mid \\delta_n(j) \\text{ is close to } P^*\\}$ that can end a maximally probable path. Floating point comparisons require a small tolerance.\nb. For each $j \\in J^*$, reconstruct the entire path by backtracking from $t=n$ to $t=1$: $C_n^* = j$, and for $t=n-1, \\ldots, 1$, $C_t^* = \\psi_{t+1}(C_{t+1}^*)$.\nc. This produces a set of candidate paths, all having the same maximal probability.\nd. These paths are compared lexicographically, and the smallest one is selected as the final answer.\n\nThis procedure is implemented for each provided test case to determine the unique optimal hidden state sequence.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the HMM decoding problem for structural variant discovery.\n    \"\"\"\n\n    # --- Model Specification ---\n    BETA = 1.0\n    C0 = 2.0\n    KAPPA = 20.0\n    GAMMA = np.log(2.0)\n    SIGMA = 0.15\n    STATES = np.arange(9)\n    NUM_STATES = len(STATES)\n\n    # --- Pre-compute HMM Parameters ---\n\n    # 1. Log Initial Probabilities\n    unnorm_pi = np.exp(-BETA * (STATES - C0)**2)\n    pi = unnorm_pi / np.sum(unnorm_pi)\n    log_pi = np.log(pi)\n\n    # 2. Log Transition Matrix\n    W = np.zeros((NUM_STATES, NUM_STATES))\n    for i in range(NUM_STATES):\n        for j in range(NUM_STATES):\n            if i == j:\n                W[i, j] = KAPPA\n            else:\n                W[i, j] = np.exp(-GAMMA * np.abs(i - j))\n    \n    T = W / W.sum(axis=1, keepdims=True)\n    # Avoid log(0) for impossible transitions, though not expected here\n    log_T = np.log(np.where(T > 0, T, 1e-300))\n\n    def viterbi_solver(D):\n        \"\"\"\n        Finds the most likely state sequence using the Viterbi algorithm.\n        Handles lexicographical tie-breaking.\n        \"\"\"\n        n = len(D)\n\n        # 3. Log Emission Probabilities for the given sequence D\n        mu = STATES / 2.0\n        log_em_prefactor = -np.log(SIGMA) - 0.5 * np.log(2 * np.pi)\n        inv_2_sig_sq = 1.0 / (2.0 * SIGMA**2)\n        log_E = np.zeros((n, NUM_STATES))\n        for t in range(n):\n            d_t = D[t]\n            log_E[t, :] = log_em_prefactor - ((d_t - mu)**2 * inv_2_sig_sq)\n\n        # --- Viterbi Algorithm ---\n        delta = np.zeros((n, NUM_STATES))\n        psi = np.zeros((n, NUM_STATES), dtype=int)\n\n        # Initialization (t=0)\n        delta[0, :] = log_pi + log_E[0, :]\n\n        # Recursion (t=1 to n-1)\n        for t in range(1, n):\n            for j in range(NUM_STATES):\n                scores = delta[t-1, :] + log_T[:, j]\n                # In numpy.argmax, ties are broken by returning the first index,\n                # which corresponds to the smallest state index.\n                best_prev_state = np.argmax(scores)\n                psi[t, j] = best_prev_state\n                delta[t, j] = scores[best_prev_state] + log_E[t, j]\n\n        # Termination & Tie-Breaking\n        max_logp = np.max(delta[n-1, :])\n        # Find all states that achieve the max probability within a tolerance\n        potential_last_states = np.where(np.isclose(delta[n-1, :], max_logp))[0]\n\n        optimal_paths = []\n        for last_state in potential_last_states:\n            path = [0] * n\n            path[n-1] = last_state\n            for t in range(n-2, -1, -1):\n                path[t] = psi[t+1, path[t+1]]\n            optimal_paths.append(path)\n\n        # Sort paths lexicographically and select the first one\n        optimal_paths.sort()\n        return optimal_paths[0]\n\n    # --- Test Suite ---\n    test_cases = [\n        (0.98, 1.03, 1.01, 0.51, 0.48, 0.50, 0.99, 1.02), # Case A\n        (0.08, 0.05, 0.04, 0.07),                         # Case B\n        (1.01, 1.99, 2.49, 2.51, 1.02),                   # Case C\n        (1.40, 1.60, 1.45, 1.55),                         # Case D\n        (3.95, 4.05),                                     # Case E\n    ]\n    \n    results = []\n    for D_tuple in test_cases:\n        D_array = np.array(D_tuple)\n        result_path = viterbi_solver(D_array)\n        results.append(result_path)\n\n    # --- Format Output ---\n    results_str = []\n    for path in results:\n        results_str.append(f\"[{','.join(map(str, path))}]\")\n    \n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "2431910"}]}