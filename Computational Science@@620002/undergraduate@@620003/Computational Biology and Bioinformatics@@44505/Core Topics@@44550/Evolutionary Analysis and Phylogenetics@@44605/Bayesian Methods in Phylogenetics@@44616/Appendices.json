{"hands_on_practices": [{"introduction": "At the heart of Bayesian inference lies the idea of marginalization—averaging over all unknown quantities to focus on the parameters of interest. This practice makes that abstract concept concrete by guiding you through a manual calculation of a marginal likelihood. By analytically integrating over unknown branch lengths and summing over unknown ancestral states for a simple three-taxon tree, you will gain a foundational understanding of how Bayesian methods handle uncertainty to evaluate evidence for a given model. [@problem_id:2375048]", "problem": "Consider a rooted $3$-taxon tree with leaves $A$, $B$, and $C$ attached directly to a single root by three branches of unknown lengths $\\ell_A$, $\\ell_B$, and $\\ell_C$, respectively. A single binary character is observed at the leaves with states $x_A=0$, $x_B=0$, and $x_C=1$. The unobserved root state $r \\in \\{0,1\\}$ has the stationary prior $\\pi(0)=\\pi(1)=\\frac{1}{2}$. Along a branch of length $\\ell \\ge 0$, evolution follows the symmetric two-state substitution model defined by the transition probabilities\n$$\np_{\\text{same}}(\\ell) \\;=\\; \\frac{1}{2} + \\frac{1}{2}\\exp(-2\\ell), \n\\qquad\np_{\\text{diff}}(\\ell) \\;=\\; \\frac{1}{2} - \\frac{1}{2}\\exp(-2\\ell),\n$$\nwhere $p_{\\text{same}}(\\ell)$ is the probability that the state at the end of the branch equals the state at its start, and $p_{\\text{diff}}(\\ell)$ is the probability that it differs. The branch lengths are a priori independent with $\\ell_A \\sim \\text{Exponential}(\\beta)$, $\\ell_B \\sim \\text{Exponential}(\\beta)$, and $\\ell_C \\sim \\text{Exponential}(\\beta)$, where $\\beta>0$ is known and the density is $p(\\ell \\mid \\beta)=\\beta \\exp(-\\beta \\ell)$ for $\\ell \\ge 0$.\n\nCompute the marginal likelihood of the observed data under this model,\n$$\np(x_A=0, x_B=0, x_C=1 \\mid \\beta),\n$$\nobtained by integrating over the unknown branch lengths $\\ell_A$, $\\ell_B$, $\\ell_C$ and summing over the unobserved root state $r$. Express your final answer as a single closed-form analytic expression in terms of $\\beta$. Do not round your answer.", "solution": "The problem is well-posed and scientifically grounded within the field of computational biology. All necessary information is provided, and the objective is a clear mathematical calculation. We proceed to solve it.\n\nThe objective is to compute the marginal likelihood of the observed data $D = \\{x_A=0, x_B=0, x_C=1\\}$, given the hyperparameter $\\beta$. This is obtained by integrating over the unknown branch lengths $\\boldsymbol{\\ell} = (\\ell_A, \\ell_B, \\ell_C)$ and summing over the unobserved root state $r \\in \\{0, 1\\}$. The marginal likelihood is given by the formula:\n$$\np(D \\mid \\beta) = \\int_0^\\infty \\int_0^\\infty \\int_0^\\infty p(D \\mid \\ell_A, \\ell_B, \\ell_C) \\, p(\\ell_A, \\ell_B, \\ell_C \\mid \\beta) \\, d\\ell_A \\, d\\ell_B \\, d\\ell_C\n$$\nThe prior distribution on the branch lengths is given by independent exponential distributions:\n$$\np(\\ell_A, \\ell_B, \\ell_C \\mid \\beta) = p(\\ell_A \\mid \\beta) p(\\ell_B \\mid \\beta) p(\\ell_C \\mid \\beta) = \\beta^3 \\exp(-\\beta(\\ell_A+\\ell_B+\\ell_C))\n$$\nThe likelihood of the data given the branch lengths, $p(D \\mid \\boldsymbol{\\ell})$, is found by marginalizing over the root state $r$:\n$$\np(D \\mid \\boldsymbol{\\ell}) = \\sum_{r \\in \\{0, 1\\}} p(D \\mid r, \\boldsymbol{\\ell}) \\, p(r)\n$$\nGiven the root state $r$ and the branch lengths, the character states at the leaves are conditionally independent. Thus, the likelihood conditioned on $r$ is:\n$$\np(D \\mid r, \\boldsymbol{\\ell}) = p(x_A \\mid r, \\ell_A) \\, p(x_B \\mid r, \\ell_B) \\, p(x_C \\mid r, \\ell_C)\n$$\nWe consider the two possible root states, using the transition probabilities $p_{\\text{same}}(\\ell) = \\frac{1}{2} + \\frac{1}{2}\\exp(-2\\ell)$ and $p_{\\text{diff}}(\\ell) = \\frac{1}{2} - \\frac{1}{2}\\exp(-2\\ell)$.\n\nCase $1$: Root state $r=0$.\nThe data are $x_A=0$, $x_B=0$, $x_C=1$.\n- For leaf $A$, the state is the same as the root ($0 \\to 0$). The probability is $p_{\\text{same}}(\\ell_A)$.\n- For leaf $B$, the state is the same as the root ($0 \\to 0$). The probability is $p_{\\text{same}}(\\ell_B)$.\n- For leaf $C$, the state is different from the root ($0 \\to 1$). The probability is $p_{\\text{diff}}(\\ell_C)$.\nThe joint probability for this case is $p(D \\mid r=0, \\boldsymbol{\\ell}) = p_{\\text{same}}(\\ell_A)p_{\\text{same}}(\\ell_B)p_{\\text{diff}}(\\ell_C)$.\n\nCase $2$: Root state $r=1$.\n- For leaf $A$, the state is different from the root ($1 \\to 0$). The probability is $p_{\\text{diff}}(\\ell_A)$.\n- For leaf $B$, the state is different from the root ($1 \\to 0$). The probability is $p_{\\text{diff}}(\\ell_B)$.\n- For leaf $C$, the state is the same as the root ($1 \\to 1$). The probability is $p_{\\text{same}}(\\ell_C)$.\nThe joint probability for this case is $p(D \\mid r=1, \\boldsymbol{\\ell}) = p_{\\text{diff}}(\\ell_A)p_{\\text{diff}}(\\ell_B)p_{\\text{same}}(\\ell_C)$.\n\nThe total likelihood $p(D \\mid \\boldsymbol{\\ell})$ is the sum of these possibilities, weighted by the prior probability of the root state, $p(r=0) = p(r=1) = \\frac{1}{2}$:\n$$\np(D \\mid \\boldsymbol{\\ell}) = \\frac{1}{2} p_{\\text{same}}(\\ell_A)p_{\\text{same}}(\\ell_B)p_{\\text{diff}}(\\ell_C) + \\frac{1}{2} p_{\\text{diff}}(\\ell_A)p_{\\text{diff}}(\\ell_B)p_{\\text{same}}(\\ell_C)\n$$\nSubstituting the definitions for the transition probabilities:\n$$\np(D \\mid \\boldsymbol{\\ell}) = \\frac{1}{2} \\left[ \\left(\\frac{1}{2} + \\frac{1}{2}e^{-2\\ell_A}\\right) \\left(\\frac{1}{2} + \\frac{1}{2}e^{-2\\ell_B}\\right) \\left(\\frac{1}{2} - \\frac{1}{2}e^{-2\\ell_C}\\right) + \\left(\\frac{1}{2} - \\frac{1}{2}e^{-2\\ell_A}\\right) \\left(\\frac{1}{2} - \\frac{1}{2}e^{-2\\ell_B}\\right) \\left(\\frac{1}{2} + \\frac{1}{2}e^{-2\\ell_C}\\right) \\right]\n$$\nFactor out the $\\frac{1}{2}$ from each term:\n$$\np(D \\mid \\boldsymbol{\\ell}) = \\frac{1}{2} \\cdot \\left(\\frac{1}{8}\\right) \\left[ (1+e^{-2\\ell_A})(1+e^{-2\\ell_B})(1-e^{-2\\ell_C}) + (1-e^{-2\\ell_A})(1-e^{-2\\ell_B})(1+e^{-2\\ell_C}) \\right]\n$$\nLet $u_i = \\exp(-2\\ell_i)$. The expression in the square brackets is $(1+u_A)(1+u_B)(1-u_C) + (1-u_A)(1-u_B)(1+u_C)$.\nExpanding the first term: $(1+u_A+u_B+u_Au_B)(1-u_C) = 1+u_A+u_B-u_C+u_Au_B-u_Au_C-u_Bu_C-u_Au_Bu_C$.\nExpanding the second term: $(1-u_A-u_B+u_Au_B)(1+u_C) = 1-u_A-u_B+u_C+u_Au_B-u_Au_C-u_Bu_C+u_Au_Bu_C$.\nSumming these two expansions yields $2+2u_Au_B-2u_Au_C-2u_Bu_C$.\nSubstituting back into the likelihood expression:\n$$\np(D \\mid \\boldsymbol{\\ell}) = \\frac{1}{16} \\left( 2 + 2e^{-2\\ell_A}e^{-2\\ell_B} - 2e^{-2\\ell_A}e^{-2\\ell_C} - 2e^{-2\\ell_B}e^{-2\\ell_C} \\right)\n$$\n$$\np(D \\mid \\boldsymbol{\\ell}) = \\frac{1}{8} \\left( 1 + e^{-2(\\ell_A+\\ell_B)} - e^{-2(\\ell_A+\\ell_C)} - e^{-2(\\ell_B+\\ell_C)} \\right)\n$$\nNow, we compute the marginal likelihood by integrating $p(D \\mid \\boldsymbol{\\ell})$ against the prior $p(\\boldsymbol{\\ell} \\mid \\beta)$:\n$$\np(D \\mid \\beta) = \\int_0^\\infty \\int_0^\\infty \\int_0^\\infty \\frac{1}{8} \\left( 1 + e^{-2(\\ell_A+\\ell_B)} - e^{-2(\\ell_A+\\ell_C)} - e^{-2(\\ell_B+\\ell_C)} \\right) \\beta^3 e^{-\\beta(\\ell_A+\\ell_B+\\ell_C)} \\,d\\ell_A \\,d\\ell_B \\,d\\ell_C\n$$\nThe integral separates into four terms. We use the generic definite integral:\n$$\nI(\\alpha) = \\int_0^\\infty e^{-\\alpha\\ell} \\beta e^{-\\beta\\ell} \\,d\\ell = \\beta \\int_0^\\infty e^{-(\\alpha+\\beta)\\ell} \\,d\\ell = \\beta \\left[ \\frac{-1}{\\alpha+\\beta}e^{-(\\alpha+\\beta)\\ell} \\right]_0^\\infty = \\frac{\\beta}{\\alpha+\\beta}\n$$\nThe integral for a branch with no extra exponential term corresponds to $\\alpha=0$, so $I(0) = \\frac{\\beta}{\\beta} = 1$. The integral for a branch with an $e^{-2\\ell}$ term corresponds to $\\alpha=2$, so $I(2)=\\frac{\\beta}{\\beta+2}$.\nThe total marginal likelihood is:\n$$\np(D \\mid \\beta) = \\frac{1}{8} \\left[ \\iiint_V 1 \\cdot p(\\boldsymbol{\\ell} \\mid \\beta) dV + \\iiint_V e^{-2(\\ell_A+\\ell_B)} p(\\boldsymbol{\\ell} \\mid \\beta) dV - \\iiint_V e^{-2(\\ell_A+\\ell_C)} p(\\boldsymbol{\\ell} \\mid \\beta) dV - \\iiint_V e^{-2(\\ell_B+\\ell_C)} p(\\boldsymbol{\\ell} \\mid \\beta) dV \\right]\n$$\nwhere $dV = d\\ell_A d\\ell_B d\\ell_C$ and $p(\\boldsymbol{\\ell} \\mid \\beta) = p(\\ell_A|\\beta)p(\\ell_B|\\beta)p(\\ell_C|\\beta)$.\nEach triple integral is a product of three one-dimensional integrals:\n1. First term: $\\frac{1}{8} [I(0) \\cdot I(0) \\cdot I(0)] = \\frac{1}{8} [1 \\cdot 1 \\cdot 1] = \\frac{1}{8}$.\n2. Second term: $\\frac{1}{8} [I(2) \\cdot I(2) \\cdot I(0)] = \\frac{1}{8} \\left[\\frac{\\beta}{\\beta+2} \\cdot \\frac{\\beta}{\\beta+2} \\cdot 1\\right] = \\frac{1}{8} \\left(\\frac{\\beta}{\\beta+2}\\right)^2$.\n3. Third term: $-\\frac{1}{8} [I(2) \\cdot I(0) \\cdot I(2)] = -\\frac{1}{8} \\left[\\frac{\\beta}{\\beta+2} \\cdot 1 \\cdot \\frac{\\beta}{\\beta+2}\\right] = -\\frac{1}{8} \\left(\\frac{\\beta}{\\beta+2}\\right)^2$.\n4. Fourth term: $-\\frac{1}{8} [I(0) \\cdot I(2) \\cdot I(2)] = -\\frac{1}{8} \\left[1 \\cdot \\frac{\\beta}{\\beta+2} \\cdot \\frac{\\beta}{\\beta+2}\\right] = -\\frac{1}{8} \\left(\\frac{\\beta}{\\beta+2}\\right)^2$.\n\nSumming these terms:\n$$\np(D \\mid \\beta) = \\frac{1}{8} + \\frac{1}{8}\\left(\\frac{\\beta}{\\beta+2}\\right)^2 - \\frac{1}{8}\\left(\\frac{\\beta}{\\beta+2}\\right)^2 - \\frac{1}{8}\\left(\\frac{\\beta}{\\beta+2}\\right)^2 = \\frac{1}{8} \\left[1 - \\left(\\frac{\\beta}{\\beta+2}\\right)^2\\right]\n$$\nWe simplify this expression:\n$$\np(D \\mid \\beta) = \\frac{1}{8} \\left[ \\frac{(\\beta+2)^2 - \\beta^2}{(\\beta+2)^2} \\right] = \\frac{1}{8} \\left[ \\frac{\\beta^2+4\\beta+4 - \\beta^2}{(\\beta+2)^2} \\right] = \\frac{1}{8} \\left[ \\frac{4\\beta+4}{(\\beta+2)^2} \\right]\n$$\n$$\np(D \\mid \\beta) = \\frac{4(\\beta+1)}{8(\\beta+2)^2} = \\frac{\\beta+1}{2(\\beta+2)^2}\n$$\nThis is the final closed-form expression for the marginal likelihood.", "answer": "$$\\boxed{\\frac{\\beta+1}{2(\\beta+2)^{2}}}$$", "id": "2375048"}, {"introduction": "While the previous exercise focused on the continuous integration of parameters, this practice shifts to the discrete choice between competing hypotheses—in this case, different tree topologies. You will implement a program to compute the posterior probabilities of several topologies, demonstrating the direct application of Bayes' theorem and seeing how the data (likelihood) and our initial assumptions (prior) combine to determine which tree is most plausible. This exercise also introduces a critical technique for performing these calculations on a computer without running into numerical errors, highlighting the practical side of computational phylogenetics. [@problem_id:2375013]", "problem": "You are given a discrete set of unrooted tree topologies for $4$ taxa with corresponding log-likelihoods and a prior probability mass function over the topologies. Your task is to show, by principled derivation and computation, how a strong prior can cause the Maximum Likelihood (ML) tree to have a very low posterior probability.\n\nStarting point and definitions to use:\n- Begin from Bayes’ theorem specialized to a finite hypothesis space and the standard definitions of Maximum Likelihood (ML) and Maximum A Posteriori (MAP). Treat each topology as a discrete hypothesis.\n- Let $T_i$ denote the $i$-th topology, let $D$ denote the observed sequence data, let $L_i$ denote the log-likelihood $\\log p(D \\mid T_i)$ under a fixed, time-reversible substitution model with independent sites, and let $\\pi_i$ denote the prior probability $p(T_i)$ on topology $T_i$. Assume that $L_i$ has been computed by a standard algorithm (for example, pruning) under a fixed model, and that $\\sum_i \\pi_i = 1$ with each $\\pi_i \\in (0,1)$.\n\nWhat you must do:\n1. Derive, from first principles, the expression for the posterior probability $p(T_i \\mid D)$ in terms of $\\{L_i\\}$ and $\\{\\pi_i\\}$, and then specialize this to the posterior probability of the ML topology $T_m$, where $m = \\arg\\max_i L_i$.\n2. For numerical stability, reason about how to compute the normalization constant in a way that avoids underflow when $L_i$ are large in magnitude and negative. Your implementation must operate in log-space and must be stable for the provided values.\n3. Implement a program that, for each test case below, computes the posterior probability assigned to the ML topology $T_m$ and returns it as a float rounded to six decimal places.\n\nTest suite (each case lists $\\{L_1,L_2,L_3\\}$ and $\\{\\pi_1,\\pi_2,\\pi_3\\}$):\n- Case $1$ (prior strongly favors a non-ML topology; moderate data):\n  - $\\{L_i\\} = \\{-100.0, -101.0, -102.0\\}$\n  - $\\{\\pi_i\\} = \\{0.05, 0.90, 0.05\\}$\n- Case $2$ (uniform prior; same data as Case $1$):\n  - $\\{L_i\\} = \\{-100.0, -101.0, -102.0\\}$\n  - $\\{\\pi_i\\} = \\{\\tfrac{1}{3}, \\tfrac{1}{3}, \\tfrac{1}{3}\\}$\n- Case $3$ (extremely weak data; very strong prior against the ML topology):\n  - $\\{L_i\\} = \\{-100.0, -100.001, -100.002\\}$\n  - $\\{\\pi_i\\} = \\{0.01, 0.98, 0.01\\}$\n- Case $4$ (very strong data overriding a strong prior):\n  - $\\{L_i\\} = \\{-90.0, -100.0, -110.0\\}$\n  - $\\{\\pi_i\\} = \\{0.05, 0.90, 0.05\\}$\n\nOutput specification:\n- For each case, compute the posterior probability of the ML topology $p(T_m \\mid D)$ as a float rounded to six decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[0.123456,0.234567,0.345678,0.456789]$).", "solution": "The problem as stated is valid. It is scientifically grounded, well-posed, and contains all necessary information to derive a unique, meaningful solution. It represents a a standard application of Bayesian inference in the field of computational phylogenetics. We shall proceed with the derivation and solution.\n\nThe core of the problem lies in applying Bayes' theorem to a discrete set of hypotheses, which in this context are the possible unrooted tree topologies for the given taxa. For $4$ taxa, there are $3$ such unique topologies. Let these topologies be denoted by the set $\\{T_1, T_2, T_3\\}$. We are given the observed data $D$, which typically consists of aligned molecular sequences.\n\n**1. Derivation of the Posterior Probability**\n\nWe begin from Bayes' theorem, which for a specific hypothesis $T_i$ and data $D$, is stated as:\n$$ p(T_i \\mid D) = \\frac{p(D \\mid T_i) p(T_i)}{p(D)} $$\nHere, the terms are:\n- $p(T_i \\mid D)$ is the posterior probability of topology $T_i$ given the data $D$. This is the quantity we wish to compute.\n- $p(D \\mid T_i)$ is the likelihood of observing the data $D$ if topology $T_i$ is the true topology.\n- $p(T_i)$ is the prior probability of topology $T_i$, reflecting our belief in this hypothesis before observing the data.\n- $p(D)$ is the marginal likelihood, or evidence, of the data. It acts as a normalization constant.\n\nThe marginal likelihood $p(D)$ is computed by summing over all possible hypotheses, according to the law of total probability:\n$$ p(D) = \\sum_{j=1}^{3} p(D, T_j) = \\sum_{j=1}^{3} p(D \\mid T_j) p(T_j) $$\nSubstituting this back into Bayes' theorem, we obtain the full expression for the posterior probability of topology $T_i$:\n$$ p(T_i \\mid D) = \\frac{p(D \\mid T_i) p(T_i)}{\\sum_{j=1}^{3} p(D \\mid T_j) p(T_j)} $$\nThe problem provides the log-likelihoods $L_i = \\log p(D \\mid T_i)$ and the priors $\\pi_i = p(T_i)$. To use these, we must convert the log-likelihoods back to likelihoods by exponentiation: $p(D \\mid T_i) = \\exp(L_i)$. Substituting these into our formula yields:\n$$ p(T_i \\mid D) = \\frac{\\exp(L_i) \\pi_i}{\\sum_{j=1}^{3} \\exp(L_j) \\pi_j} $$\nThis is the general expression for the posterior probability of any topology $T_i$.\n\nThe Maximum Likelihood (ML) topology, which we denote as $T_m$, is the one that maximizes the likelihood function. Since the logarithm is a strictly monotonic function, maximizing the likelihood is equivalent to maximizing the log-likelihood. Therefore, the index $m$ of the ML topology is given by:\n$$ m = \\arg\\max_i L_i $$\nThe posterior probability of this specific ML topology $T_m$ is then:\n$$ p(T_m \\mid D) = \\frac{\\exp(L_m) \\pi_m}{\\sum_{j=1}^{3} \\exp(L_j) \\pi_j} $$\n\n**2. Numerically Stable Computation**\n\nThe log-likelihood values $L_i$ provided are large negative numbers (e.g., $-100.0$). A direct computation of $\\exp(L_i)$ would result in a value so close to zero that it would be rounded down to $0.0$ by standard floating-point arithmetic, a phenomenon known as arithmetic underflow. This would make the numerator and all terms in the denominator zero, leading to an indeterminate form $0/0$.\n\nTo avoid this, we employ a standard numerical stabilization technique often called the \"log-sum-exp\" trick. We identify the maximum log-likelihood, which is $L_m$ by definition: $L_m = \\max_j\\{L_j\\}$. We then factor out $\\exp(L_m)$ from both the numerator and the denominator of the posterior probability expression. This does not change the value of the fraction.\n$$ p(T_i \\mid D) = \\frac{\\exp(L_i) \\pi_i}{\\sum_{j} \\exp(L_j) \\pi_j} = \\frac{\\exp(L_m) \\cdot \\exp(L_i - L_m) \\pi_i}{\\exp(L_m) \\cdot \\sum_{j} \\exp(L_j - L_m) \\pi_j} $$\nCanceling the $\\exp(L_m)$ term, we arrive at a numerically stable formula:\n$$ p(T_i \\mid D) = \\frac{\\exp(L_i - L_m) \\pi_i}{\\sum_{j=1}^{3} \\exp(L_j - L_m) \\pi_j} $$\nIn this expression, the exponent for each term is $L_j - L_m$. Since $L_m$ is the maximum value, this difference is always less than or equal to zero. The term for the ML topology itself has an exponent of $L_m - L_m = 0$, resulting in $\\exp(0) = 1$. All other terms $\\exp(L_j - L_m)$ for $j \\neq m$ will evaluate to a number between $0$ and $1$, thus preventing underflow.\n\nFor the posterior probability of the ML topology $T_m$ specifically, the numerator term becomes $\\exp(L_m - L_m) \\pi_m = \\exp(0) \\pi_m = \\pi_m$. The final, stable formula for the quantity of interest is:\n$$ p(T_m \\mid D) = \\frac{\\pi_m}{\\sum_{j=1}^{3} \\exp(L_j - L_m) \\pi_j} $$\nThis expression forms the basis of our computation.\n\n**3. Implementation and Final Calculation**\n\nWe will now implement a program to apply this formula to the four given test cases. The objective is to calculate $p(T_m \\mid D)$ for each case. The program will first identify the maximum log-likelihood $L_m$ and its corresponding index $m$. It will then compute the denominator (the normalization constant) by summing the terms $\\exp(L_j - L_m) \\pi_j$ over all three topologies. Finally, it will compute the posterior probability of the ML topology using the stable formula derived above. The results will demonstrate how the interplay between the likelihood signal from the data and the prior beliefs determines the final posterior probability, and how a strong prior can lead to the ML topology being deemed improbable after Bayesian analysis.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian phylogenetics problem for the given test cases.\n    It calculates the posterior probability of the Maximum Likelihood (ML) topology.\n    \"\"\"\n\n    # Test cases defined in the problem statement.\n    # Each case is a tuple of (log_likelihoods, priors).\n    test_cases = [\n        # Case 1: Prior strongly favors a non-ML topology; moderate data\n        (np.array([-100.0, -101.0, -102.0]), np.array([0.05, 0.90, 0.05])),\n        # Case 2: Uniform prior; same data as Case 1\n        (np.array([-100.0, -101.0, -102.0]), np.array([1/3, 1/3, 1/3])),\n        # Case 3: Extremely weak data; very strong prior against the ML topology\n        (np.array([-100.0, -100.001, -100.002]), np.array([0.01, 0.98, 0.01])),\n        # Case 4: Very strong data overriding a strong prior\n        (np.array([-90.0, -100.0, -110.0]), np.array([0.05, 0.90, 0.05])),\n    ]\n\n    results = []\n    for log_likelihoods, priors in test_cases:\n        result = calculate_ml_posterior(log_likelihoods, priors)\n        results.append(result)\n\n    # Format the final output as specified.\n    # The map function converts each rounded float to a string for joining.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef calculate_ml_posterior(log_likelihoods: np.ndarray, priors: np.ndarray) -> float:\n    \"\"\"\n    Calculates the posterior probability of the Maximum Likelihood (ML) topology\n    using a numerically stable method.\n\n    Args:\n        log_likelihoods: A numpy array of log-likelihoods for each topology.\n        priors: A numpy array of prior probabilities for each topology.\n\n    Returns:\n        The posterior probability of the ML topology as a float.\n    \"\"\"\n    # 1. Identify the Maximum Likelihood (ML) topology and its properties.\n    # The ML topology has the maximum log-likelihood.\n    # np.argmax returns the index of the maximum value.\n    ml_index = np.argmax(log_likelihoods)\n    \n    # Get the maximum log-likelihood value. This is L_m.\n    L_m = log_likelihoods[ml_index]\n    \n    # Get the prior probability for the ML topology. This is pi_m.\n    pi_m = priors[ml_index]\n\n    # 2. Compute the normalization constant (denominator) in a numerically stable way.\n    # The formula for the normalization constant C is:\n    # C = sum_j(exp(L_j - L_m) * pi_j)\n    \n    # Calculate the log-likelihood differences (L_j - L_m).\n    # This prevents underflow when exponentiating.\n    log_likelihood_diffs = log_likelihoods - L_m\n    \n    # Exponentiate the differences and multiply by the priors.\n    terms = np.exp(log_likelihood_diffs) * priors\n    \n    # Sum the terms to get the normalization constant.\n    normalization_constant = np.sum(terms)\n\n    # 3. Calculate the posterior probability of the ML topology.\n    # The stable formula for p(T_m | D) is:\n    # p(T_m | D) = (exp(L_m - L_m) * pi_m) / C = pi_m / C\n    # The numerator of the ML topology's posterior after stabilization is simply its prior.\n    numerator_ml = np.exp(L_m - L_m) * pi_m # which simplifies to pi_m\n    \n    posterior_ml = numerator_ml / normalization_constant\n    \n    return posterior_ml\n    \nsolve()\n```", "id": "2375013"}, {"introduction": "This final practice brings all the core concepts together in a complete simulation-to-inference workflow, demonstrating one of the most powerful properties of statistical inference: consistency. You will first act as the engine of evolution, simulating DNA sequence data on a known \"true\" tree, and then switch roles to that of a scientist, using Bayesian methods to infer the tree from that data. By repeating this process with increasing amounts of data, you will see firsthand how the posterior probability sharpens and converges on the correct topology, providing a tangible demonstration of why collecting more data leads to more confident evolutionary conclusions. [@problem_id:2375068]", "problem": "You are given a binary unrooted phylogeny on four taxa with labels $A$, $B$, $C$, and $D$. The true unrooted topology is $T_{\\text{true}} = ((A,B),(C,D))$. All branch lengths are measured in expected substitutions per site under a Continuous-Time Markov Chain (CTMC) on nucleotides with the Jukes–Cantor model. Let the pendant branch lengths be $l_A = 0.2$, $l_B = 0.2$, $l_C = 0.2$, $l_D = 0.2$, and the single internal edge length be $l_{\\text{int}} = 0.1$. Consider a rooted representation formed by placing a root at the midpoint of the internal edge so that the lengths from the root to the two internal nodes are $l_{\\text{int}}/2$ each.\n\nThe substitution process is the Jukes–Cantor model with rate $\\mu = 1$ per unit branch length. The state space is $\\{A,C,G,T\\}$. The transition probability from state $i$ to state $j$ along a branch of length $t$ is\n$$\nP_{ij}(t) =\n\\begin{cases}\n\\frac{1}{4} + \\frac{3}{4} e^{- \\frac{4}{3} t} & \\text{if } i = j, \\\\\n\\frac{1}{4} - \\frac{1}{4} e^{- \\frac{4}{3} t} & \\text{if } i \\ne j,\n\\end{cases}\n$$\nand the stationary distribution is uniform $\\pi_i = \\frac{1}{4}$ for all $i$.\n\nData generation: For each site $k \\in \\{1,\\dots,L\\}$, draw the root state $R_k$ from the stationary distribution. Propagate states down the tree independently across sites, with child states sampled from their parent states using $P(t)$ along each branch of length $t$. The observed data are the leaf states $(X_{A,k}, X_{B,k}, X_{C,k}, X_{D,k})$ at taxa $A$, $B$, $C$, and $D$ for $k = 1,\\dots,L$.\n\nInference: Consider the set of three unrooted binary topologies on the four taxa,\n$$\n\\mathcal{T} = \\{T_1=((A,B),(C,D)),\\; T_2=((A,C),(B,D)),\\; T_3=((A,D),(B,C))\\}.\n$$\nAssign a uniform prior over the three topologies, so that $\\Pr(T_i)=\\frac{1}{3}$ for each $i \\in \\{1,2,3\\}$. For any topology $T \\in \\mathcal{T}$, define its likelihood given an alignment of length $L$ as the product over sites of the site likelihoods under the Jukes–Cantor model with the same branch lengths $l_A, l_B, l_C, l_D, l_{\\text{int}}$ and the root placed at the midpoint of the internal edge (so the two edges incident to the root each have length $l_{\\text{int}}/2$). For a single site with observed leaves $(x_A, x_B, x_C, x_D)$, the site likelihood is the sum over all internal-node and root states of the product of the stationary weight at the root and the transition probabilities along all branches to the leaves. The posterior over $\\mathcal{T}$ is proportional to the prior times the likelihood and must be normalized to sum to $1$ across the three topologies.\n\nTask: Simulate sequence data under $T_{\\text{true}}$ and compute the posterior probability assigned to $T_{\\text{true}}$ for each of the following test cases. For reproducibility, before simulating each dataset, initialize the pseudo-random number generator with the specified integer seed $s$ for that test case. The test suite is the set of pairs $(L,s)$:\n$$\n\\{(1,7),\\; (25,11),\\; (100,13),\\; (400,17),\\; (1200,19)\\}.\n$$\n\nRequirements:\n- Use the Jukes–Cantor model as specified above for both simulation and likelihood calculation.\n- Use the same branch lengths $l_A = l_B = l_C = l_D = 0.2$ and $l_{\\text{int}} = 0.1$ for all topologies, with the root at the midpoint of the internal edge.\n- Assume sites are independent and identically distributed.\n- The posterior must be computed exactly from the likelihoods and the uniform prior over the three topologies.\n\nYour program should produce a single line of output containing a comma-separated list enclosed in square brackets, with the posterior probability assigned to $T_{\\text{true}}$ for each test case in the order listed above, each rounded to six decimal places. For example, the output format must be\n$$\n[\\text{p}_1,\\text{p}_2,\\text{p}_3,\\text{p}_4,\\text{p}_5],\n$$\nwhere $\\text{p}_i$ is the posterior probability of $T_{\\text{true}}$ for the $i$-th test case, rounded to six decimal places. There are no physical units to report for this problem. Angles are not involved. All final numerical answers must be decimals.", "solution": "The supplied problem statement has been subjected to rigorous validation and is found to be scientifically sound, well-posed, and objective. It poses a standard task in computational phylogenetics: the evaluation of posterior probabilities for a set of competing tree topologies given sequence data, using Bayesian inference. The problem specifies a complete generative model for the data (the Jukes-Cantor model on a fixed true topology) and a precise inference framework (Bayesian model choice with a uniform prior over topologies and fixed branch lengths). All parameters are explicitly provided. We shall, therefore, proceed with a formal solution.\n\nThe core of the problem is to compute the posterior probability of the true topology, $T_{\\text{true}} = ((A,B),(C,D))$, given a DNA sequence alignment simulated from it. This posterior probability is calculated using Bayes' theorem. The set of possible unrooted topologies for four taxa ($A, B, C, D$) is $\\mathcal{T} = \\{T_1, T_2, T_3\\}$, where $T_1 = ((A,B),(C,D))$, $T_2 = ((A,C),(B,D))$, and $T_3 = ((A,D),(B,C))$.\n\nThe posterior probability of a topology $T_i \\in \\mathcal{T}$ given the observed sequence alignment data $D$ is given by:\n$$\n\\Pr(T_i | D) = \\frac{\\Pr(D | T_i) \\Pr(T_i)}{\\sum_{j=1}^{3} \\Pr(D | T_j) \\Pr(T_j)}\n$$\nThe problem specifies a uniform prior over the topologies, $\\Pr(T_i) = 1/3$ for $i \\in \\{1, 2, 3\\}$. This simplifies the posterior probability to be proportional to the likelihood, $\\Pr(D|T_i)$, which we denote as $L(T_i|D)$:\n$$\n\\Pr(T_i | D) = \\frac{L(T_i | D)}{\\sum_{j=1}^{3} L(T_j | D)}\n$$\nThe sites in the alignment are assumed to be independent and identically distributed (i.i.d.). Thus, the total likelihood for an alignment $D$ of length $L$ is the product of the likelihoods for each site $D_k$:\n$$\nL(T_i | D) = \\prod_{k=1}^{L} L(T_i | D_k)\n$$\nwhere $D_k = (x_{A,k}, x_{B,k}, x_{C,k}, x_{D,k})$ represents the observed nucleotides at the four taxa for site $k$.\n\nThe substitution process follows the Jukes-Cantor model. The probability of a transition from state $i$ to state $j$ along a branch of length $t$ is given by the matrix $P(t)$, with elements:\n$$\nP_{ij}(t) =\n\\begin{cases}\n\\frac{1}{4} + \\frac{3}{4} e^{- \\frac{4}{3} t} & \\text{if } i = j \\\\\n\\frac{1}{4} - \\frac{1}{4} e^{- \\frac{4}{3} t} & \\text{if } i \\ne j\n\\end{cases}\n$$\nThe model assumes a uniform stationary distribution of nucleotides, $\\pi = (\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4}, \\frac{1}{4})$. For both simulation and inference, a rooted tree is required. This is formed by placing a root at the midpoint of the single internal edge of length $l_{\\text{int}} = 0.1$. This results in two internal branches of length $l_{\\text{int}}/2 = 0.05$ emanating from the root. The pendant branches to the leaves all have length $0.2$.\n\nThe site-likelihood $L(T_i | D_k)$ is calculated using Felsenstein's pruning algorithm. This algorithm efficiently sums over all possible states at the internal nodes of the tree. For a given topology $T_i$ and a site $k$, the algorithm computes partial likelihoods at each node, moving from the leaves toward the root.\nLet $L_u(s)$ be the partial likelihood at node $u$ for state $s$.\n1.  **Leaves:** At a leaf node $u \\in \\{A, B, C, D\\}$, the partial likelihood vector is initialized based on the observed data. For example, for leaf $A$, if the observed state is $x_{A,k}$, then $L_A(s) = \\delta_{s, x_{A,k}}$, where $\\delta$ is the Kronecker delta. This can be represented as a one-hot vector.\n2.  **Internal Nodes:** For an internal node $u$ with children $v$ and $w$ connected by branches of length $t_v$ and $t_w$, the partial likelihood is:\n    $$\n    L_u(s) = \\left( \\sum_{s_v} P_{s,s_v}(t_v) L_v(s_v) \\right) \\left( \\sum_{s_w} P_{s,s_w}(t_w) L_w(s_w) \\right)\n    $$\n    In matrix-vector notation, if $L_v$ and $L_w$ are column vectors of partial likelihoods for the children, this becomes:\n    $$\n    L_u = (P(t_v)^T L_v) \\odot (P(t_w)^T L_w)\n    $$\n    where `T` denotes the transpose and $\\odot$ denotes element-wise product.\n3.  **Root:** The process continues until the root $R$ is reached. The total likelihood for the site is then the sum of the partial likelihoods at the root, weighted by the stationary distribution $\\pi$:\n    $$\n    L(T_i | D_k) = \\sum_{s} \\pi_s L_R(s) = \\pi^T L_R\n    $$\nThe topology $T_i$ determines which leaves are siblings. For example, under $T_1 = ((A,B),(C,D))$, the internal nodes are parents to $(A,B)$ and $(C,D)$ respectively. Under $T_2 = ((A,C),(B,D))$, they are parents to $(A,C)$ and $(B,D)$. The branch lengths are identical for all topologies: pendant branches have length $0.2$, and the two branches descending from the root have length $0.05$.\n\nThe overall procedure is as follows:\nFor each test case $(L, s)$:\n1.  Initialize the pseudo-random number generator with seed $s$.\n2.  Simulate a sequence alignment of length $L$. For each site, a root state is drawn from $\\pi$, and states are propagated down the true rooted tree $T_1 = ((A,B),(C,D))$ using the transition probabilities $P(t)$.\n3.  Calculate the total log-likelihood for each of the three topologies, $\\log L(T_i|D) = \\sum_{k=1}^{L} \\log L(T_i | D_k)$. Using logarithms is essential to avoid numerical underflow with large $L$.\n4.  Compute the posterior probabilities from the log-likelihoods. To maintain numerical stability, we use the log-sum-exp trick. Let $\\ell_i = \\log L(T_i|D)$ and $\\ell_{\\max} = \\max(\\ell_1, \\ell_2, \\ell_3)$. The posterior for the true topology $T_1$ is:\n    $$\n    \\Pr(T_1 | D) = \\frac{e^{\\ell_1}}{e^{\\ell_1} + e^{\\ell_2} + e^{\\ell_3}} = \\frac{e^{\\ell_1 - \\ell_{\\max}}}{e^{\\ell_1 - \\ell_{\\max}} + e^{\\ell_2 - \\ell_{\\max}} + e^{\\ell_3 - \\ell_{\\max}}}\n    $$\nThe final implemented program executes this procedure for each specified test case and reports the posterior probability of $T_1$, rounded to six decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian phylogenetics problem by simulating sequence data and\n    computing the posterior probability of the true tree topology.\n    \"\"\"\n    # Define problem constants\n    N_STATES = 4\n    STATES = np.arange(N_STATES)  # 0:A, 1:C, 2:G, 3:T\n    STATIONARY_PI = np.full(N_STATES, 1.0 / N_STATES)\n\n    # Branch lengths\n    PENDANT_T = 0.2\n    INTERNAL_T = 0.1 / 2.0\n\n    # Define the three unrooted topologies for 4 taxa {A, B, C, D}\n    # These are mapped to indices {0, 1, 2, 3}\n    # T1 = ((A,B),(C,D)), T2 = ((A,C),(B,D)), T3 = ((A,D),(B,C))\n    TOPOLOGIES = [\n        (((0, 1), (2, 3))),  # T1, the true topology\n        (((0, 2), (1, 3))),  # T2\n        (((0, 3), (1, 2))),  # T3\n    ]\n\n    def get_jc_p_matrix(t):\n        \"\"\"\n        Calculates the Jukes-Cantor transition probability matrix for a branch of length t.\n        \"\"\"\n        p_ii = 0.25 + 0.75 * np.exp(-4.0 / 3.0 * t)\n        p_ij = 0.25 - 0.25 * np.exp(-4.0 / 3.0 * t)\n        P = np.full((N_STATES, N_STATES), p_ij)\n        np.fill_diagonal(P, p_ii)\n        return P\n\n    # Pre-calculate transition matrices for the given branch lengths\n    P_PENDANT = get_jc_p_matrix(PENDANT_T)\n    P_INTERNAL = get_jc_p_matrix(INTERNAL_T)\n\n    def simulate_site():\n        \"\"\"\n        Simulates a single site down the true tree topology T1=((A,B),(C,D)).\n        \"\"\"\n        # 1. Sample root state from the stationary distribution\n        root_state = np.random.choice(STATES, p=STATIONARY_PI)\n\n        # 2. Propagate states to internal nodes N1 (ancestor of A,B) and N2 (ancestor of C,D)\n        n1_state = np.random.choice(STATES, p=P_INTERNAL[root_state, :])\n        n2_state = np.random.choice(STATES, p=P_INTERNAL[root_state, :])\n\n        # 3. Propagate states to leaf nodes A, B, C, D\n        a_state = np.random.choice(STATES, p=P_PENDANT[n1_state, :])\n        b_state = np.random.choice(STATES, p=P_PENDANT[n1_state, :])\n        c_state = np.random.choice(STATES, p=P_PENDANT[n2_state, :])\n        d_state = np.random.choice(STATES, p=P_PENDANT[n2_state, :])\n\n        return [a_state, b_state, c_state, d_state]\n\n    def calculate_site_likelihood(site_data, topology):\n        \"\"\"\n        Calculates the likelihood of a single site for a given topology using \n        Felsenstein's pruning algorithm.\n        \"\"\"\n        # Initialize partial likelihoods at the leaves (one-hot vectors)\n        leaf_likelihoods = np.zeros((4, N_STATES))\n        leaf_likelihoods[0, site_data[0]] = 1.0  # Taxon A\n        leaf_likelihoods[1, site_data[1]] = 1.0  # Taxon B\n        leaf_likelihoods[2, site_data[2]] = 1.0  # Taxon C\n        leaf_likelihoods[3, site_data[3]] = 1.0  # Taxon D\n\n        # Get leaf pairings for the two internal nodes from the topology definition\n        (p1_left_idx, p1_right_idx), (p2_left_idx, p2_right_idx) = topology\n\n        # Calculate partial likelihoods for internal node 1\n        L_n1_from_left = P_PENDANT.T @ leaf_likelihoods[p1_left_idx]\n        L_n1_from_right = P_PENDANT.T @ leaf_likelihoods[p1_right_idx]\n        L_n1 = L_n1_from_left * L_n1_from_right\n\n        # Calculate partial likelihoods for internal node 2\n        L_n2_from_left = P_PENDANT.T @ leaf_likelihoods[p2_left_idx]\n        L_n2_from_right = P_PENDANT.T @ leaf_likelihoods[p2_right_idx]\n        L_n2 = L_n2_from_left * L_n2_from_right\n\n        # Propagate likelihoods to the root\n        L_root_from_n1 = P_INTERNAL.T @ L_n1\n        L_root_from_n2 = P_INTERNAL.T @ L_n2\n        L_root = L_root_from_n1 * L_root_from_n2\n\n        # Final site likelihood is the dot product with the stationary distribution\n        site_likelihood = STATIONARY_PI @ L_root\n        return site_likelihood\n\n    test_cases = [\n        (1, 7),\n        (25, 11),\n        (100, 13),\n        (400, 17),\n        (1200, 19),\n    ]\n\n    results = []\n    for L, seed in test_cases:\n        # Set the random seed for reproducibility\n        np.random.seed(seed)\n\n        # 1. Simulate sequence alignment of length L\n        alignment = [simulate_site() for _ in range(L)]\n\n        # 2. Calculate log-likelihoods for each of the three topologies\n        log_likelihoods = np.zeros(len(TOPOLOGIES))\n        for i, topo in enumerate(TOPOLOGIES):\n            total_log_lik = 0.0\n            for site in alignment:\n                site_lik = calculate_site_likelihood(site, topo)\n                if site_lik > 0:\n                    total_log_lik += np.log(site_lik)\n                else:\n                    total_log_lik = -np.inf # Should not happen in practice\n            log_likelihoods[i] = total_log_lik\n\n        # 3. Compute posterior probabilities from log-likelihoods\n        # Use log-sum-exp trick for numerical stability\n        max_log_lik = np.max(log_likelihoods)\n        shifted_log_liks = log_likelihoods - max_log_lik\n        likelihoods = np.exp(shifted_log_liks)\n        \n        # With a uniform prior, posterior is proportional to likelihood\n        sum_likelihoods = np.sum(likelihoods)\n        posteriors = likelihoods / sum_likelihoods\n\n        # The posterior for T_true is the first one in the list\n        posterior_T1 = posteriors[0]\n        results.append(posterior_T1)\n\n    # Format the results to exactly six decimal places and print\n    formatted_results = [\"{:.6f}\".format(res) for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2375068"}]}