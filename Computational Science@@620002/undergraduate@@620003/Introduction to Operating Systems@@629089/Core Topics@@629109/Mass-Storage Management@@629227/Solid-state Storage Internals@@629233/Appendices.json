{"hands_on_practices": [{"introduction": "While solid-state drives offer remarkable speed, their memory cells have a finite lifespan determined by the number of program/erase cycles they can endure. This practice builds a quantitative model from first principles to estimate an SSD's expected lifetime. By analyzing scenarios of both uniform and skewed write patterns, you will gain a concrete understanding of how workload characteristics and the resulting write amplification directly impact device longevity [@problem_id:3683908].", "problem": "A single-level cell solid-state drive is modeled at the level of erase blocks. An erase block can tolerate up to $E$ program/erase cycles before it is considered worn out. The Flash Translation Layer (FTL) performs dynamic wear leveling and garbage collection, which induces Write Amplification (WA), meaning that $D_w$ units of host data written per day cause $D_w \\times W$ units of data to be programmed on the flash per day. The device has physical capacity $C$ and erase block size $B$, so the total number of erase blocks is $N = C / B$. The device is considered to reach end-of-life when the earliest erase block reaches $E$ cycles. Assume $365$ days per year.\n\nStarting from these definitions and without introducing any additional assumptions beyond uniform random free-block selection within the set of blocks that receive writes, consider the following two traffic regimes:\n1. Uniform wear regime: all $N$ erase blocks receive erase traffic uniformly, and the steady-state Write Amplification is $W_u$.\n2. Skewed hot/cold regime: a fraction $\\beta$ of the physical erase blocks receive essentially all erase traffic (the hot set), while the remaining blocks hold cold data that is rarely moved. The steady-state Write Amplification under this regime is $W_s$.\n\nGiven the parameters $D_w = 100$ GiB/day, $E = 3000$ cycles per block, $C = 256$ GiB, $B = 256$ KiB, $W_u = 1.15$, $W_s = 2.7$, and $\\beta = 0.23$, derive the expected lifetimes in years under the two regimes from first principles and compute the ratio $R$ of uniform-wear lifetime to skewed hot/cold lifetime. Express $R$ as a pure number and round your answer to four significant figures.", "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\n- Maximum program/erase cycles per block: $E$\n- Host data written per day: $D_w$\n- Write Amplification factor: $W$\n- Device physical capacity: $C$\n- Erase block size: $B$\n- Total number of erase blocks: $N = C / B$\n- End-of-life condition: The earliest erase block reaches $E$ cycles.\n- Days per year: $365$\n\n- Regime 1 (Uniform wear):\n  - All $N$ blocks receive uniform wear.\n  - Steady-state Write Amplification: $W_u$.\n\n- Regime 2 (Skewed hot/cold):\n  - Fraction of blocks receiving erase traffic: $\\beta$.\n  - Steady-state Write Amplification: $W_s$.\n\n- Numerical values:\n  - $D_w = 100$ GiB/day\n  - $E = 3000$ cycles/block\n  - $C = 256$ GiB\n  - $B = 256$ KiB\n  - $W_u = 1.15$\n  - $W_s = 2.7$\n  - $\\beta = 0.23$\n\n- Task: Derive the expected lifetimes in years for both regimes and compute the ratio $R$ of the uniform-wear lifetime to the skewed hot/cold lifetime, rounding the result to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n- **Scientifically Grounded**: The problem uses a standard, albeit simplified, model of solid-state drive (SSD) wear. The concepts of erase blocks, program/erase (P/E) cycles, Write Amplification (WA), Flash Translation Layer (FTL), and hot/cold data are fundamental principles in the study of flash memory and storage systems. The model is scientifically sound for its intended pedagogical purpose.\n- **Well-Posed**: All necessary variables and constants are provided. The objectives are clearly stated. The assumptions of uniform wear within the specified sets of blocks allow for the derivation of a unique, stable solution.\n- **Objective**: The problem is stated in precise, objective language without subjective or biased claims.\n\nThe problem does not violate any of the specified criteria for invalidity (e.g., scientific unsoundness, incompleteness, ambiguity). The units provided ($GiB$ and $KiB$) are consistent and can be resolved during calculation. The numerical values are realistic for consumer-grade SSDs.\n\n### Step 3: Verdict and Action\nThe problem is valid and well-posed. A solution will be derived from first principles.\n\n### Derivation of Lifetime\n\nThe end-of-life for the device is defined as the moment when the most-worn erase block has accumulated $E$ program/erase cycles. The lifetime is the total time until this occurs.\n\nThe total amount of data written to the flash memory per day is the host write rate multiplied by the Write Amplification factor, which is $D_w \\times W$.\n\nThe number of block erasures (and subsequent programs) per day is the total data written to flash divided by the size of an erase block, $B$.\n$$ \\text{Cycles per day} = \\frac{D_w \\times W}{B} $$\nThese cycles are distributed among a set of active blocks. Let the number of active blocks be $N_{\\text{active}}$. The assumption of uniform wear within this set means that at the end of life, each of the $N_{\\text{active}}$ blocks will have undergone $E$ cycles.\n\nThe total number of P/E cycles the active set can endure is $N_{\\text{active}} \\times E$.\nLet $L$ be the lifetime in days. The total number of cycles consumed over this lifetime is $L \\times (\\text{Cycles per day})$.\nTherefore, we can equate the total available cycles to the total consumed cycles:\n$$ L \\times \\frac{D_w \\times W}{B} = N_{\\text{active}} \\times E $$\nSolving for the lifetime $L$ in days yields the general formula:\n$$ L = \\frac{N_{\\text{active}} \\times E \\times B}{D_w \\times W} $$\n\nWe now apply this general formula to the two specified regimes.\n\n**Regime 1: Uniform Wear Lifetime ($L_u$)**\nIn the uniform wear regime, write traffic is distributed evenly across all $N$ blocks of the drive.\n- The number of active blocks is $N_{\\text{active}} = N$.\n- The Write Amplification is $W_u$.\nSubstituting these into the general lifetime formula:\n$$ L_u = \\frac{N \\times E \\times B}{D_w \\times W_u} $$\nSince the total number of blocks $N$ is defined as the total capacity $C$ divided by the block size $B$ (i.e., $N = C/B$), we can substitute this expression for $N$:\n$$ L_u = \\frac{(C/B) \\times E \\times B}{D_w \\times W_u} = \\frac{C \\times E}{D_w \\times W_u} $$\nThis expression gives the lifetime in days. To convert to years, we divide by $365$.\n$$ L_{u, \\text{years}} = \\frac{C \\times E}{D_w \\times W_u \\times 365} $$\n\n**Regime 2: Skewed Hot/Cold Wear Lifetime ($L_s$)**\nIn the skewed wear regime, all write traffic is concentrated on a fraction $\\beta$ of the total blocks.\n- The number of active blocks is $N_{\\text{active}} = \\beta \\times N$.\n- The Write Amplification for this workload is $W_s$.\nSubstituting these into the general lifetime formula:\n$$ L_s = \\frac{(\\beta \\times N) \\times E \\times B}{D_w \\times W_s} $$\nAgain, substituting $N = C/B$:\n$$ L_s = \\frac{\\beta \\times (C/B) \\times E \\times B}{D_w \\times W_s} = \\frac{\\beta \\times C \\times E}{D_w \\times W_s} $$\nThe lifetime in years is:\n$$ L_{s, \\text{years}} = \\frac{\\beta \\times C \\times E}{D_w \\times W_s \\times 365} $$\n\n**Ratio of Lifetimes ($R$)**\nThe problem asks for the ratio $R$ of the uniform-wear lifetime to the skewed hot/cold lifetime.\n$$ R = \\frac{L_{u, \\text{years}}}{L_{s, \\text{years}}} = \\frac{L_u}{L_s} $$\nSubstituting the derived expressions for $L_u$ and $L_s$:\n$$ R = \\frac{\\frac{C \\times E}{D_w \\times W_u}}{\\frac{\\beta \\times C \\times E}{D_w \\times W_s}} $$\nThe terms $C$, $E$, and $D_w$ cancel out, simplifying the expression significantly:\n$$ R = \\frac{1/W_u}{\\beta/W_s} = \\frac{W_s}{\\beta \\times W_u} $$\n\n**Numerical Calculation**\nWe now substitute the given numerical values into the expression for $R$:\n- $W_s = 2.7$\n- $\\beta = 0.23$\n- $W_u = 1.15$\n$$ R = \\frac{2.7}{0.23 \\times 1.15} $$\nFirst, calculate the product in the denominator:\n$$ 0.23 \\times 1.15 = 0.2645 $$\nNow, compute the ratio:\n$$ R = \\frac{2.7}{0.2645} \\approx 10.2079395085... $$\nThe problem requires the answer to be rounded to four significant figures.\n$$ R \\approx 10.21 $$\nThis result indicates that the drive's expected lifetime is over $10$ times longer under a uniform wear pattern compared to a highly skewed pattern with the given parameters, highlighting the critical importance of effective wear leveling.", "answer": "$$\\boxed{10.21}$$", "id": "3683908"}, {"introduction": "Not all write workloads are created equal; some can trigger pathological behavior where an SSD spends far more time on internal maintenance (garbage collection) than on servicing host requests. This exercise presents a common but non-obvious scenario involving random updates to sparse files that leads to extremely high write amplification. Solving this puzzle reveals the critical importance of cross-layer communication, demonstrating how operating system commands like TRIM are essential for maintaining healthy SSD performance [@problem_id:3683956].", "problem": "A Solid-State Drive (SSD) stores data in pages grouped into erase blocks. A page write is out-of-place, and an erase can occur only at block granularity. The Flash Translation Layer (FTL) maps Logical Block Addresses (LBAs) to physical pages and performs garbage collection (GC) by selecting a victim erase block, copying its valid pages elsewhere, and erasing the block. The GC cost per erased block increases with the valid-page fraction, denoted by $v$, in the victim block. The host can issue range deallocations (for example, Advanced Technology Attachment TRIM or Non-Volatile Memory Express Dataset Management with deallocate) to inform the device that some LBAs are no longer needed; these LBAs can be immediately considered invalid by the FTL.\n\nConsider an SSD whose erase block contains $B=256$ pages of size $4\\,\\mathrm{KB}$ each. A file system maintains a large sparse file with logical size $1\\,\\mathrm{TB}$. An application issues a long run of small random updates: each update is a $4\\,\\mathrm{KB}$ overwrite to a uniformly random LBA within a working set of $M$ distinct logical pages, where $M$ is large. Between the time a block is filled and the time it becomes a GC candidate, the device receives $k$ of these random updates. Assume $M=25\\times 10^{6}$ pages (about $100\\,\\mathrm{GB}$ of logical space) and $k=10^{6}$ random updates in that interval. The file system does not delete the sparse file nor punch holes by default.\n\nUsing only the definitions above and basic probability reasoning about random updates over a large address space, determine which option best constructs a workload that triggers pathological garbage collection (high $v$), correctly explains why $v$ remains high in the GC victim block under that workload, and proposes an operating system (OS)-side range-deletion strategy that would decrease $v$ in future GC rounds.\n\nA. Small $4\\,\\mathrm{KB}$ updates are issued uniformly at random across a very large sparse file so that each erase block tends to contain pages whose LBAs are rarely re-written before the block is selected for GC; thus, the expected fraction of invalidated pages per block is small and the valid-page fraction $v$ remains high. The OS should periodically punch holes in large unused or obsolete extents of the sparse file and issue range deallocations aligned to erase-block boundaries to mark many pages invalid at once, thereby reducing $v$ in future GC cycles.\n\nB. Sequential appends to a log-structured file dominate the workload, so each erase block quickly accumulates only the newest data and is soon selected for GC with a high $v$. The OS should enable synchronous TRIM on every overwrite of an LBA to invalidate its old copy and reduce $v$.\n\nC. Random updates are confined to a small hot set of LBAs that fits within a few erase blocks, so updates repeatedly hit the same LBAs; therefore $v$ remains high because of locality. The OS should disable TRIM to avoid extra work in the device, which lowers $v$.\n\nD. Small random updates are issued across a very large sparse file, but pre-zeroing the entire file first will lower $v$ because zero pages are easy for the SSD to discard internally. The OS should pre-write zeros to all ranges instead of issuing range deletes, since this makes GC cheaper and reduces $v$.\n\nSelect the single best option.", "solution": "### Problem Validation\n\n**Step 1: Validate Problem Statement**\nThe problem describes a scientifically sound scenario rooted in the fundamental operating principles of NAND flash-based SSDs.\n-   **Scientifically Grounded**: The concepts of pages, erase blocks, out-of-place writes, the Flash Translation Layer (FTL), garbage collection (GC), and the critical role of the valid-page fraction ($v$) are all accurate representations of SSD internal mechanics. The reference to host commands like TRIM/deallocate is also correct.\n-   **Well-Posed**: The problem provides specific, realistic parameters for the workload ($M = 25 \\times 10^6$ pages, $k = 10^6$ updates) and the device ($B = 256$ pages/block). It asks for a qualitative explanation of the resulting behavior and a corresponding mitigation strategy, which can be derived from the provided information.\n-   **Objective**: The problem is stated using precise, objective technical language without ambiguity or bias.\n\nThe problem is valid and well-posed, representing a classic performance pathology in SSDs.\n\n### Solution Derivation\n\nThe core of the problem is to understand why the described workload leads to a high valid-page fraction ($v$) during garbage collection. A high $v$ means that when the FTL chooses a block to erase, most of its pages still contain valid data. This forces the FTL to perform a costly operation: reading and rewriting all that valid data elsewhere before it can erase the block. This leads to high write amplification.\n\nLet's analyze the workload: $k = 10^6$ random updates are spread uniformly across a large working set of $M = 25 \\times 10^6$ logical pages. Consider an erase block that has just been filled with 256 pages corresponding to 256 distinct LBAs from this working set. For a page in this block to become invalid, its LBA must be overwritten by one of the subsequent $k$ updates.\n\nThe probability that any single LBA is targeted by a single random update is $p = 1/M$. The probability that this LBA is *not* targeted is $(1 - 1/M)$.\nThe probability that this LBA is not targeted by any of the $k$ independent updates is $(1 - 1/M)^k$. This is the probability that the page remains valid.\n\nThe expected fraction of valid pages, $v$, in our victim block is therefore:\n$$ v = \\left(1 - \\frac{1}{M}\\right)^k = \\left(1 - \\frac{1}{25 \\times 10^6}\\right)^{10^6} $$\nUsing the approximation $(1-x/n)^n \\approx e^{-x}$ for large $n$:\n$$ v \\approx e^{-k/M} = e^{-10^6 / (25 \\times 10^6)} = e^{-1/25} = e^{-0.04} \\approx 0.9608 $$\nSo, approximately 96% of the pages in a victim block are still valid when it is chosen for GC. This is a pathologically high value, confirming the problem's premise. The cause is the wide distribution of random writes, which makes it very unlikely for any specific LBA to be overwritten in a short period. The FTL, lacking other information, must assume all this data is live.\n\nThe solution must be to provide the FTL with more information. The OS, which knows that the large sparse file has vast unwritten regions, can use a range deallocation command (like TRIM) to tell the FTL which LBAs are not in use. This allows the FTL to immediately mark the corresponding physical pages as invalid, dramatically reducing $v$ and making GC efficient.\n\n### Option-by-Option Analysis\n\n**A. Small $4\\,\\mathrm{KB}$ updates are issued uniformly at random across a very large sparse file so that each erase block tends to contain pages whose LBAs are rarely re-written before the block is selected for GC; thus, the expected fraction of invalidated pages per block is small and the valid-page fraction $v$ remains high. The OS should periodically punch holes in large unused or obsolete extents of the sparse file and issue range deallocations aligned to erase-block boundaries to mark many pages invalid at once, thereby reducing $v$ in future GC cycles.**\n-   **Explanation**: This perfectly matches our derivation. The writes are spread thin, so LBAs are rarely rewritten, leading to high $v$.\n-   **Solution**: \"Punching holes\" in the file and issuing \"range deallocations\" (TRIM) is the precise and correct OS-level solution to inform the drive of unused logical space.\n-   **Verdict**: **Correct.**\n\n**B. Sequential appends to a log-structured file dominate the workload, so each erase block quickly accumulates only the newest data and is soon selected for GC with a high $v$. The OS should enable synchronous TRIM on every overwrite of an LBA to invalidate its old copy and reduce $v$.**\n-   **Explanation**: This mischaracterizes the workload as sequential, whereas the problem specifies random updates.\n-   **Verdict**: **Incorrect.**\n\n**C. Random updates are confined to a small hot set of LBAs that fits within a few erase blocks, so updates repeatedly hit the same LBAs; therefore $v$ remains high because of locality.**\n-   **Explanation**: This premise is factually wrong. High locality (repeatedly updating a small set of LBAs) would cause old pages to be invalidated quickly, leading to a *low* $v$ and efficient GC. The statement that high locality causes high $v$ is the opposite of the truth.\n-   **Verdict**: **Incorrect.**\n\n**D. Small random updates are issued across a very large sparse file, but pre-zeroing the entire file first will lower $v$ because zero pages are easy for the SSD to discard internally. The OS should pre-write zeros to all ranges instead of issuing range deletes, since this makes GC cheaper and reduces $v$.**\n-   **Explanation**: Writing zeros across a 1 TB file is an extremely inefficient operation that generates massive write amplification and wear. The correct command, range delete (TRIM), is a lightweight metadata operation that achieves the goal with almost no overhead.\n-   **Verdict**: **Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3683956"}, {"introduction": "Commercial SSDs often abstract their internal geometry, but we can turn our understanding of their fundamental mechanics into a powerful diagnostic tool. This practice challenges you to think like a systems researcher by designing a microbenchmark to reverse-engineer a key internal parameter: the erase block size, $B$. By carefully controlling the write workload and observing latency discontinuities, you can uncover the device's hidden properties, demonstrating how performance signatures reveal underlying hardware characteristics [@problem_id:3683961].", "problem": "You are given a commodity Solid-State Drive (SSD) that exposes a block device interface. Internally, it uses NAND flash memory with a smallest program unit called a page of size $P$ and a smallest erase unit called an erase block of size $B$ where $B$ is an integer multiple of $P$. The device employs a Flash Translation Layer (FTL) that performs out-of-place updates, garbage collection, and wear leveling. On flash memory, a page can be programmed only if it is in an erased state, and erasure happens at the granularity of a whole erase block of size $B$. The host can issue synchronous writes with a request size $S$, aligned at multiples of $S$, and can force write-through semantics.\n\nYou want to design a timing microbenchmark to infer the unknown erase block size $B$ by sweeping the write size $S$ and observing latency discontinuities. Your experiment must isolate the internal effect of block erasure and page migration and must not be dominated by operating system caches, controller write-back caches, or internal parallelism across multiple channels or dies. Assume you can deallocate ranges (TRIM) and can run with a single outstanding input/output request, and that you can fill the device to a target utilization.\n\nFrom first principles, recall the following foundational facts:\n- Out-of-place update: An overwrite of existing logical data causes the FTL to write new physical pages and invalidate the old ones instead of updating in place.\n- Erase-before-write: A block must be erased before its pages can be programmed again; erasing affects the entire block of size $B$.\n- Garbage collection: To reclaim space, the FTL selects a victim block, copies out still-valid pages, erases the block, and returns it to the free pool. The time to service a write that triggers reclamation is approximately the sum of the time to program the newly written pages, the time to copy any valid pages that remain in the victim block, and the time to erase the block.\n- In a steady-state overwrite workload on a nearly full device, garbage collection tends to occur in the foreground and its cost becomes visible in the observed write latency.\n\nWhich of the following experimental designs and interpretations best isolates erase-block effects and produces a clear, physically justified signature that allows you to estimate $B$ from the observed per-operation latency as a function of the write size $S$?\n\nA. Precondition the device to steady state by writing it sequentially to at least $95\\%$ logical utilization. Choose a contiguous logical range of size $R$ (e.g., $R \\approx 4B$ to $64B$), and perform repeated in-place overwrites across that range with a stride equal to $S$. For each fixed $S$ in a sweep such as $S \\in \\{4\\ \\text{KiB}, 8\\ \\text{KiB}, 16\\ \\text{KiB}, \\dots, 8\\ \\text{MiB}\\}$, do the following: align the starting logical block address to $S$, disable page cache and controller write-back by using direct input/output and Force Unit Access (FUA), run with a single outstanding input/output ($Q=1$), and run long enough to trigger many garbage collection cycles. Record the distribution of per-write latency. Expect a pronounced drop in both mean latency and tail latency when $S$ equals $B$ and the writes happen to be aligned to actual erase-block boundaries, because each overwrite invalidates exactly one full block and garbage collection can erase without copying valid pages. Verify by offsetting the alignment by $S/2$, which reintroduces high tail latency. Estimate $B$ as the smallest $S$ showing this discontinuity and alignment sensitivity.\n\nB. Start from a fully trimmed device, disable FUA, and stream sequential appends at varying $S$ to a large new file using the default buffered input/output. Measure the average throughput over an entire $2$ GiB write per $S$ and identify $B$ as the $S$ where the average throughput first decreases noticeably, on the grounds that crossing an erase-block boundary causes an erase.\n\nC. Issue read-only random reads with request size $S$ over a $10$ GiB region, using the page cache for speed. Plot read throughput versus $S$ and identify the knee point as the erase block size $B$, since reading across a boundary is expected to trigger whole-block transfers.\n\nD. Generate random writes at high queue depth, e.g., $Q=32$, over the entire device using a fixed $S$ of $128$ KiB. Measure input/output operations per second versus $S$ by running separate passes for $S \\in \\{4\\ \\text{KiB}, 8\\ \\text{KiB}, \\dots, 1\\ \\text{MiB}\\}$. Identify $B$ as the $S$ where input/output operations per second changes slope, because the controller’s interleaving across channels will saturate at multiples of the erase block size.\n\nSelect the best option and be prepared to justify why the proposed method produces a discontinuity tied to $B$ rather than to the page size $P$, host buffering effects, or controller parallelism. Your answer should be a single letter.", "solution": "### Problem Validation\nThe problem asks for the design of a microbenchmark to determine an SSD's internal erase block size ($B$).\n-   **Scientifically Grounded**: The problem is based on the core principles of SSD operation: out-of-place updates, garbage collection (GC), and the latency cost of GC, which is dominated by copying valid pages. The idea of reverse-engineering internal parameters from performance signatures is a standard and valid technique in systems research.\n-   **Well-Posed**: The problem clearly states the goal, the available tools (synchronous writes, TRIM, etc.), and the constraints (must not be dominated by caches or parallelism). It provides sufficient information to reason about the design of a successful experiment.\n-   **Objective**: The problem and options are described in precise, objective technical terms.\n\nThe problem is valid and well-posed. The task is to identify the experimental design that correctly isolates the effect of the erase block size on performance.\n\n### Solution Derivation\nThe key to inferring the erase block size $B$ is to create a workload where the measured performance changes discontinuously when the write size $S$ matches $B$. The most significant performance event tied to block size is garbage collection.\n\n1.  **The Signature of Garbage Collection**: When an SSD is nearly full, a host write will often trigger foreground garbage collection. The latency of this write will be high because it includes the time to erase a block and, crucially, to copy any still-valid data from that block. The copy-work is the dominant cost.\n\n2.  **Isolating the Erase Block**: The cost of GC is minimized if there are *no* valid pages to copy. A \"zero-copy GC\" is the most efficient. This ideal scenario occurs when a host write invalidates all pages within an entire physical erase block simultaneously.\n\n3.  **Experimental Design**: To trigger a zero-copy GC, the workload must issue a write of size $S=B$ that is physically aligned with an erase block. When the host overwrites a logical range of size $B$ that maps to a full physical block, the FTL can simply mark the old block as invalid and later erase it with no copy overhead. This will result in a significantly lower write latency compared to writes of other sizes (e.g., $S  B$ or $S > B$), which would leave partial valid data in blocks and necessitate expensive copying.\n\n4.  **Required Controls**: To measure this effect cleanly, several confounding factors must be eliminated:\n    *   **High Utilization**: The device must be nearly full (e.g., >95%) to force GC to happen in the foreground, making its latency visible.\n    *   **Cache Bypass**: OS page caches and the SSD's internal write-back cache must be bypassed using Direct I/O and Force Unit Access (FUA) flags. This ensures the latency measured is that of the physical media operation.\n    *   **No Parallelism**: The experiment must use a single outstanding I/O request (queue depth $Q=1$) to prevent the device from hiding the latency of one slow operation behind other concurrent operations.\n\nThe experiment should therefore sweep the write size $S$ and measure the distribution of write latencies, looking for a sharp drop in tail latency when $S$ aligns with $B$.\n\n### Option-by-Option Analysis\n\n**A. Precondition the device to steady state by writing it sequentially to at least $95\\%$ logical utilization... perform repeated in-place overwrites... sweep... $S$... align... disable page cache and controller write-back by using direct input/output and Force Unit Access (FUA), run with a single outstanding input/output ($Q=1$)... Record the distribution of per-write latency. Expect a pronounced drop in... tail latency when $S$ equals $B$...**\n-   **Analysis**: This experimental design is flawless. It correctly implements all the necessary preconditions (high utilization) and controls (Direct I/O, FUA, Q=1) to isolate the target effect. The predicted signature—a drop in tail latency when write size $S$ matches erase block size $B$ due to zero-copy GC—is physically justified and directly tied to the parameter being measured. The verification step of using a misaligned write is also an excellent control.\n-   **Verdict**: **Correct.**\n\n**B. Start from a fully trimmed device, disable FUA, and stream sequential appends... using the default buffered input/output. Measure the average throughput...**\n-   **Analysis**: This design is incorrect for several reasons. Starting from a trimmed (empty) device means GC is not a factor. Using buffered I/O masks the device latency. Measuring average throughput smooths out the very latency spikes that provide the signature.\n-   **Verdict**: **Incorrect.**\n\n**C. Issue read-only random reads with request size $S$... using the page cache... Plot read throughput...**\n-   **Analysis**: This is fundamentally wrong. The erase block size signature is a write-side phenomenon related to garbage collection. A read-only workload will not reveal it. Using the page cache further obscures any device-level characteristics.\n-   **Verdict**: **Incorrect.**\n\n**D. Generate random writes at high queue depth, e.g., $Q=32$... Measure input/output operations per second...**\n-   **Analysis**: This design is the opposite of what is required. High queue depth is used to maximize throughput by exploiting parallelism and hiding latency. This experiment would measure the drive's parallelism capabilities, but it would completely obscure the single-operation latency spikes caused by GC, which are needed to find $B$.\n-   **Verdict**: **Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3683961"}]}