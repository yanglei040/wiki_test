{"hands_on_practices": [{"introduction": "This exercise introduces a foundational way to think about the cost of Copy-on-Write. By modeling a child process's write behavior probabilistically, we can derive the *expected* number of page copies, which is a crucial first step in understanding the average-case performance impact of COW in a system [@problem_id:3629158]. This practice helps solidify the connection between the OS mechanism and a simple, powerful analytical model using the linearity of expectation.", "problem": "An operating system implements Copy-On-Write (COW) on process creation via the system call commonly known as fork(). Immediately after fork(), the child and parent processes share the same physical pages; all shared mappings are marked read-only. On the first write to any shared page, a COW page fault occurs: the kernel allocates a new physical page, copies the original page’s contents, updates the faulting process’s page table to the new private page, and retries the write. Subsequent writes by the same process to that page do not incur further COW faults because the page is now private and writable. Reads do not cause COW faults.\n\nConsider a process whose address space consists of $n$ virtual memory pages, each of size $S$ bytes. After fork(), only the child process may write. For each of the $n$ pages, independently, the child performs exactly one write attempt to that page with probability $p \\in [0,1]$, and performs no write to that page with probability $1-p$. Assume there are no other writers and no intervening events that would break sharing except the child’s own first write. Each COW page fault copies exactly one page of size $S$ bytes.\n\nStarting from first principles in probability (Bernoulli trials, indicator random variables, and linearity of expectation) and the operating system definition of COW given above, derive an expression in terms of $n$, $p$, and $S$ for:\n- the expected number of COW page faults incurred by the child, and\n- the expected total number of bytes copied due to COW.\n\nExpress the total bytes in bytes. Provide your final answer as a two-entry row matrix using LaTeX \\texttt{pmatrix} with the first entry equal to the expected number of COW faults and the second entry equal to the expected total bytes copied. Do not include any textual commentary in the final answer.", "solution": "The problem asks for the expected number of Copy-On-Write (COW) page faults and the expected total number of bytes copied for a child process created via `fork()`. The problem is well-posed, scientifically grounded in the principles of operating systems and probability theory, and contains all necessary information for a unique solution.\n\nLet $n$ be the number of virtual memory pages in the process's address space.\nLet $S$ be the size of each page in bytes.\nLet $p$ be the probability that the child process performs a write attempt to any given page. The write attempts for each of the $n$ pages are independent events.\n\nA COW page fault occurs on a specific page if and only if the child process attempts its first write to that page. The problem states that for each page, the child performs exactly one write attempt with probability $p$. Therefore, a COW fault for a given page occurs with probability $p$.\n\nTo find the expected number of COW page faults, we can model the situation using indicator random variables. Let us define an indicator random variable $X_i$ for each page $i$, where $i \\in \\{1, 2, \\dots, n\\}$.\n\nThe indicator random variable $X_i$ is defined as:\n$$\nX_i = \\begin{cases}\n1 & \\text{if a COW fault occurs on page } i \\\\\n0 & \\text{if no COW fault occurs on page } i\n\\end{cases}\n$$\n\nA COW fault occurs on page $i$ if the child writes to it. The probability of this event is given as $p$.\nThus, the probability distribution of $X_i$ is:\n$P(X_i = 1) = p$\n$P(X_i = 0) = 1-p$\n\nThis is a Bernoulli trial for each page $i$. The expected value of a Bernoulli random variable is the probability of success. Therefore, the expected value of $X_i$ is:\n$$E[X_i] = 1 \\cdot P(X_i=1) + 0 \\cdot P(X_i=0) = 1 \\cdot p + 0 \\cdot (1-p) = p$$\n\nThe total number of COW page faults, let's call this random variable $N_{faults}$, is the sum of the indicator variables for all $n$ pages, since each $X_i$ counts as one fault if it occurs.\n$$N_{faults} = \\sum_{i=1}^{n} X_i$$\n\nTo find the expected number of total COW page faults, we compute the expectation of $N_{faults}$. We use the property of linearity of expectation, which states that the expectation of a sum of random variables is the sum of their individual expectations. This property holds regardless of whether the random variables are independent. In this problem, the events are stated to be independent, but linearity of expectation does not require this assumption.\n$$E[N_{faults}] = E\\left[\\sum_{i=1}^{n} X_i\\right] = \\sum_{i=1}^{n} E[X_i]$$\n\nSince $E[X_i] = p$ for all pages $i = 1, \\dots, n$, the sum becomes:\n$$E[N_{faults}] = \\sum_{i=1}^{n} p = n \\cdot p$$\nThis is the expected number of COW page faults incurred by the child process.\n\nNext, we need to find the expected total number of bytes copied due to COW.\nThe problem states that each COW page fault results in copying exactly one page of size $S$ bytes.\nLet $B_{copied}$ be the random variable representing the total number of bytes copied.\nThe total number of bytes copied is the number of COW faults multiplied by the number of bytes copied per fault.\n$$B_{copied} = N_{faults} \\cdot S$$\n\nWe want to find the expected value of $B_{copied}$, which is $E[B_{copied}]$.\n$$E[B_{copied}] = E[N_{faults} \\cdot S]$$\nSince $S$ is a constant, we can factor it out of the expectation:\n$$E[B_{copied}] = S \\cdot E[N_{faults}]$$\n\nWe have already derived that $E[N_{faults}] = np$. Substituting this result into the equation gives:\n$$E[B_{copied}] = S \\cdot (np) = npS$$\nThis is the expected total number of bytes copied due to COW.\n\nThe final answer requires a two-entry row matrix containing the expected number of COW faults and the expected total bytes copied.\nThe first entry is $E[N_{faults}] = np$.\nThe second entry is $E[B_{copied}] = npS$.", "answer": "$$\\boxed{\\begin{pmatrix} np & npS \\end{pmatrix}}$$", "id": "3629158"}, {"introduction": "Building on an average-case model, this practice explores the other end of the performance spectrum: the worst-case scenario. We will analyze a hypothetical situation designed to maximize COW faults and calculate the resulting demands on system resources like memory bandwidth [@problem_id:3629086]. This type of analysis is vital for understanding system bottlenecks and ensuring that performance can meet required deadlines even under heavy load.", "problem": "A multiprocessing operating system uses Copy-on-Write (COW) on process creation by the system call `fork()`. Under COW, the child process initially shares the parent's memory pages; the first write to any shared page triggers a page fault, after which the operating system allocates a new physical frame and copies the entire original page before allowing the write to proceed. Consider a parent process with a private address space size of $S$ bytes, aligned on page boundaries, where the page size is $P$ bytes. The child executes a worst-case write pattern that maximizes COW page copies by writing exactly one byte in each distinct page, sequentially, with stride equal to the page size, and never revisiting a page. Assume the following system parameters:\n- $S = 8\\,\\mathrm{GiB}$, with $1\\,\\mathrm{GiB} = 2^{30}\\,\\mathrm{bytes}$,\n- $P = 4\\,\\mathrm{KiB}$, with $1\\,\\mathrm{KiB} = 2^{10}\\,\\mathrm{bytes}$,\n- A constant page fault handling overhead of $\\tau_{\\mathrm{pf}} = 2 \\times 10^{-6}\\,\\mathrm{s}$ per fault spent in the operating system (e.g., trap handling, page frame allocation, page table update, Translation Lookaside Buffer (TLB) shootdown),\n- A sustained memory bandwidth budget available for data movement of $B_{\\mathrm{actual}} = 25\\,\\mathrm{GiB/s}$.\n\nUse the following foundational facts:\n- The number of pages is $N = S / P$.\n- A COW write to a previously shared page requires copying the entire page, which in the worst case induces two main memory transfers of size $P$ each (one read of the old page and one write of the new page), plus the actual application write of $1$ byte; caching does not eliminate these transfers in the worst case.\n- Memory bandwidth relates data movement $D$ (in bytes) to time $t$ (in seconds) by $t = D / B$ for sustained streaming transfers.\n\nStarting from these fundamentals, derive a closed-form expression for the minimal sustained memory bandwidth $B_{\\mathrm{req}}$ (in $\\mathrm{GiB/s}$) required to complete the child's one-byte-per-page write across the entire address space within a deadline of $T_{\\mathrm{goal}} = 5.0\\,\\mathrm{s}$, and compute the actual completion time $T_{\\mathrm{actual}}$ (in $\\mathrm{s}$) given the available bandwidth $B_{\\mathrm{actual}}$. Express $B_{\\mathrm{req}}$ in $\\mathrm{GiB/s}$ and $T_{\\mathrm{actual}}$ in $\\mathrm{s}$. Round both numerical results to four significant figures.", "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and objective. It presents a solvable performance analysis scenario based on established principles of operating systems, specifically the Copy-on-Write (COW) memory management technique. All necessary parameters are provided, and the objectives are clearly defined.\n\nThe core task is to determine the total time required for a series of COW operations and then use this relationship to solve for a required bandwidth and an actual completion time.\n\nFirst, let's establish the fundamental quantities from the given parameters.\nThe size of the parent process's address space is $S = 8\\,\\mathrm{GiB} = 8 \\times 2^{30}\\,\\mathrm{bytes} = 2^{3} \\times 2^{30}\\,\\mathrm{bytes} = 2^{33}\\,\\mathrm{bytes}$.\nThe page size is $P = 4\\,\\mathrm{KiB} = 4 \\times 2^{10}\\,\\mathrm{bytes} = 2^{2} \\times 2^{10}\\,\\mathrm{bytes} = 2^{12}\\,\\mathrm{bytes}$.\nThe number of pages, $N$, in the address space is the total size divided by the page size:\n$$N = \\frac{S}{P} = \\frac{2^{33}\\,\\mathrm{bytes}}{2^{12}\\,\\mathrm{bytes}} = 2^{21}$$\nNumerically, $N = 2,097,152$ pages.\n\nThe problem describes a worst-case scenario where the child process writes to each of the $N$ distinct pages exactly once. Each such write to a shared page triggers a page fault and a COW operation. The total time to complete the entire sequence is the sum of the times for each of the $N$ COW events.\n\nLet's analyze the time taken for a single COW event, $T_{\\mathrm{fault}}$. This time is composed of two parts:\n1. A constant overhead for the operating system to handle the page fault, $\\tau_{\\mathrm{pf}}$.\n2. The time required to perform the memory copy, $t_{\\mathrm{copy}}$.\n\nThe problem states that a COW operation involves reading the original page and writing it to a newly allocated frame. This corresponds to a total data transfer of $D_{\\mathrm{fault}} = P + P = 2P$ bytes per fault. The time taken for this data transfer depends on the available memory bandwidth, $B$, according to the relation $t = D/B$. Thus,\n$$t_{\\mathrm{copy}} = \\frac{2P}{B}$$\nThe total time for a single fault is the sum of the overhead and the copy time:\n$$T_{\\mathrm{fault}} = \\tau_{\\mathrm{pf}} + t_{\\mathrm{copy}} = \\tau_{\\mathrm{pf}} + \\frac{2P}{B}$$\nSince there are $N$ such independent and sequential events, the total time to complete the process, $T_{\\mathrm{total}}$, is $N$ times the time for a single event:\n$$T_{\\mathrm{total}}(B) = N \\times T_{\\mathrm{fault}} = N \\left( \\tau_{\\mathrm{pf}} + \\frac{2P}{B} \\right)$$\nThis can be expanded to:\n$$T_{\\mathrm{total}}(B) = N\\tau_{\\mathrm{pf}} + \\frac{2PN}{B}$$\nWe can simplify the second term by recognizing that $PN = (S/P)P = S$. Thus, the total data transferred is $2S$. The expression for total time becomes:\n$$T_{\\mathrm{total}}(B) = N\\tau_{\\mathrm{pf}} + \\frac{2S}{B}$$\nThis equation is the central model for this problem. The total time is the sum of the total fixed OS overhead ($N\\tau_{\\mathrm{pf}}$) and the total time spent on data transfer ($2S/B$).\n\nNow we can address the two parts of the problem.\n\nPart 1: Derive the required memory bandwidth $B_{\\mathrm{req}}$.\nWe are given a deadline $T_{\\mathrm{goal}} = 5.0\\,\\mathrm{s}$. We need to find the bandwidth $B_{\\mathrm{req}}$ that allows the process to complete in exactly this time. We set $T_{\\mathrm{total}}(B_{\\mathrm{req}}) = T_{\\mathrm{goal}}$:\n$$T_{\\mathrm{goal}} = N\\tau_{\\mathrm{pf}} + \\frac{2S}{B_{\\mathrm{req}}}$$\nSolving for $B_{\\mathrm{req}}$:\n$$T_{\\mathrm{goal}} - N\\tau_{\\mathrm{pf}} = \\frac{2S}{B_{\\mathrm{req}}}$$\n$$B_{\\mathrm{req}} = \\frac{2S}{T_{\\mathrm{goal}} - N\\tau_{\\mathrm{pf}}}$$\nThis is the closed-form expression for the required bandwidth. Now, we substitute the numerical values.\nFirst, calculate the total fixed overhead time, $N\\tau_{\\mathrm{pf}}$:\n$$N\\tau_{\\mathrm{pf}} = 2^{21} \\times (2 \\times 10^{-6}\\,\\mathrm{s}) = 2,097,152 \\times 2 \\times 10^{-6}\\,\\mathrm{s} = 4.194304\\,\\mathrm{s}$$\nThe total amount of data to be copied is $2S = 2 \\times 8\\,\\mathrm{GiB} = 16\\,\\mathrm{GiB}$.\nThe time available for this data transfer must be $T_{\\mathrm{goal}} - N\\tau_{\\mathrm{pf}}$:\n$$T_{\\mathrm{goal}} - N\\tau_{\\mathrm{pf}} = 5.0\\,\\mathrm{s} - 4.194304\\,\\mathrm{s} = 0.805696\\,\\mathrm{s}$$\nNow we can compute $B_{\\mathrm{req}}$:\n$$B_{\\mathrm{req}} = \\frac{16\\,\\mathrm{GiB}}{0.805696\\,\\mathrm{s}} \\approx 19.85981\\,\\mathrm{GiB/s}$$\nRounding to four significant figures, we get $B_{\\mathrm{req}} = 19.86\\,\\mathrm{GiB/s}$.\n\nPart 2: Calculate the actual completion time $T_{\\mathrm{actual}}$.\nWe use the same total time formula, but with the given actual bandwidth $B_{\\mathrm{actual}} = 25\\,\\mathrm{GiB/s}$.\n$$T_{\\mathrm{actual}} = T_{\\mathrm{total}}(B_{\\mathrm{actual}}) = N\\tau_{\\mathrm{pf}} + \\frac{2S}{B_{\\mathrm{actual}}}$$\nWe have already calculated the total overhead $N\\tau_{\\mathrm{pf}} = 4.194304\\,\\mathrm{s}$. The time for data transfer with the actual bandwidth is:\n$$\\frac{2S}{B_{\\mathrm{actual}}} = \\frac{16\\,\\mathrm{GiB}}{25\\,\\mathrm{GiB/s}} = 0.64\\,\\mathrm{s}$$\nThe actual total time is the sum of these two components:\n$$T_{\\mathrm{actual}} = 4.194304\\,\\mathrm{s} + 0.64\\,\\mathrm{s} = 4.834304\\,\\mathrm{s}$$\nRounding to four significant figures, we get $T_{\\mathrm{actual}} = 4.834\\,\\mathrm{s}$.\n\nThe closed-form expression for the required bandwidth is $B_{\\mathrm{req}} = \\frac{2S}{T_{\\mathrm{goal}} - (S/P)\\tau_{\\mathrm{pf}}}$. The numerical results for $B_{\\mathrm{req}}$ (in $\\mathrm{GiB/s}$) and $T_{\\mathrm{actual}}$ (in $\\mathrm{s}$) are $19.86$ and $4.834$ respectively.", "answer": "$$\\boxed{\\begin{pmatrix} 19.86 & 4.834 \\end{pmatrix}}$$", "id": "3629086"}, {"introduction": "Copy-on-Write extends beyond process creation with `fork()` and is a cornerstone of private memory-mapped files. This exercise delves into the subtle but critical interaction between COW and the file system's page cache, revealing why modifications in a private mapping are isolated from the underlying file [@problem_id:3629104]. Understanding this behavior and how to design an experiment to test it is essential for correct data management in advanced applications.", "problem": "An advanced user-space program on a Unix-like Operating System (OS) uses memory-mapped files on a kernel that implements demand paging, a unified page cache, and the standard semantics of private and shared mappings. Consider a regular file of size $N$ bytes on a conventional block-based file system (FS). A process opens the file read-write and maps it at some virtual address $X$ using a private mapping with write permission. After the mapping is established, the process stores a value into a location within the mapped region, thereby causing a write fault on the corresponding virtual page.\n\nFrom first principles, use the following foundational facts to reason about what happens and how to verify it experimentally:\n- Virtual memory maps virtual pages to physical frames via per-process page tables; each mapping is represented by a Page Table Entry (PTE). The PTE encodes access permissions and whether writes are allowed.\n- The page cache stores file-backed pages indexed by file and offset; the file system writeback machinery writes dirty file-backed cache pages to disk.\n- A private mapping of a file provides Copy-On-Write (COW) semantics: initially, the process sees the file’s page cache content; upon the first write to a mapped page, the kernel must ensure isolation between processes and from the underlying file.\n- The semantics of writeback and of data synchronization via msync and fsync are defined for shared file-backed cache pages and for data written through explicit write system calls. The Portable Operating System Interface (POSIX) does not require msync on a private mapping to make modifications visible in the file.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. In a file-backed private mapping, the first write to a clean, file-backed page triggers a Copy-On-Write (COW) fault: the kernel allocates a new anonymous page, copies the data, and updates the process’s PTE to point to this anonymous page with write permission. The original file-backed page cache page remains clean. Because the process’s dirty page is now anonymous (not in the file’s page cache), the file system writeback machinery will not write these private modifications to the file, and calls such as msync on a private mapping or fsync on the file descriptor do not propagate the private changes to the file.\n\nB. In a file-backed private mapping, the write fault marks the file’s page cache page dirty in place; therefore, background writeback and calls to msync or fsync will propagate the private changes to the file on disk.\n\nC. A conclusive user-space test can be designed as follows: compute a strong checksum $H_{0}$ of the file’s content via read on a freshly opened file descriptor; map the file with a private, writable mapping and modify some bytes in the mapping; then call msync with the MS_SYNC flag and fsync on the file descriptor; unmap; reopen the file and compute $H_{1}$ by reading the file through a path that avoids reusing any potentially stale in-memory state (for example, by using either O_DIRECT with properly aligned buffered reads or by first dropping caches if permitted by policy); finally, compare $H_{0}$ and $H_{1}$. Expect $H_{0}=H_{1}$, showing the file is unchanged, while a read from the private mapping (before unmap) returns the modified bytes.\n\nD. To make private mapping modifications reach the file, it suffices to call fsync on the file descriptor after the modifications; fsync will locate and write back the process’s private dirty pages.\n\nE. A suitable test is to map the same file with a shared mapping in another process and then, after the first process modifies its private mapping and calls msync, read through the shared mapping; the shared mapping should reflect the private mapping’s writes if the writeback mechanism works, so seeing the modified bytes would confirm that the kernel wrote back the private dirty pages to the file.", "solution": "The problem statement describes a standard scenario in a Unix-like operating system involving a private, writable memory mapping of a file. The validity of the problem statement is assessed first.\n\n**Problem Validation**\n\nThe problem statement is valid. It is scientifically grounded in well-established principles of operating system design, including virtual memory, demand paging, page caching, and the distinction between private (`MAP_PRIVATE`) and shared (`MAP_SHARED`) memory mappings. The foundational facts provided are accurate representations of how modern kernels like Linux handle these concepts. The problem is well-posed, objective, and self-contained, presenting a clear question about system behavior that can be reasoned about from the given principles. It is a non-trivial question that probes a nuanced interaction between the memory management and file system subsystems.\n\n**Derivation from First Principles**\n\nThe central concept governing the behavior of a private, writable mapping of a file is **Copy-On-Write (COW)**. This mechanism is essential for providing isolation, as stipulated in the problem's foundational facts. Let us trace the sequence of events.\n\n1.  **Mapping Establishment**: A process requests a private, writable mapping of a file. The kernel configures the process's virtual memory area (VMA) for the specified address range, noting that it is a writable, private mapping of the given file. The kernel then populates the process's page table entries (PTEs) for this range. Crucially, to enforce COW, the kernel marks these PTEs as **read-only** in the hardware page table, even though the VMA itself is marked as writable. These PTEs will initially point to the corresponding pages of the file residing in the kernel's unified page cache.\n\n2.  **Write Fault**: The process attempts to write to a byte within the mapped virtual address range. The memory management unit (MMU) of the CPU detects a write attempt to a page marked as read-only by its PTE. This violation triggers a page fault, which is a trap into the kernel.\n\n3.  **Kernel Fault Handling (COW)**: The kernel's page fault handler inspects the fault. It determines that the fault occurred at an address within a VMA that is legally writable by the process, but the fault was triggered by a write to a read-only PTE. This specific combination indicates a COW fault on a private mapping. The kernel performs the following actions:\n    a. It allocates a new, empty physical page frame. This page is **anonymous memory**; it is not associated with any file and belongs exclusively to the process.\n    b. It copies the entire content of the original file-backed page (from the page cache) to this new anonymous page.\n    c. It updates the process's PTE for the faulting virtual page. The new PTE now points to the new anonymous page, and its access permissions are changed to **read-write**.\n    d. The kernel returns from the fault handler, and the CPU re-executes the instruction that caused the fault. The write now succeeds because the page is mapped as writable.\n\n4.  **Consequences of COW**:\n    *   **Isolation**: The process now has a private copy of the page. Any modifications it makes are confined to this anonymous page and are not visible to other processes that may have mapped the same file. The original page in the file's page cache remains untouched and thus \"clean\" (not dirty).\n    *   **Loss of File-Backing**: The process's virtual page is no longer backed by the file; it is backed by anonymous memory. Like heap or stack memory, if this page needs to be paged out due to memory pressure, it will be written to the system's swap area, not to the original file.\n    *   **Synchronization Semantics**: System calls like `msync` and `fsync` are designed to synchronize the file's page cache with the on-disk storage. Since the process's modified page is anonymous and no longer part of the file's page cache, these system calls have no knowledge of it and no mechanism to write its contents to the file. The POSIX standard explicitly notes this behavior, stating that the effect of `msync` on `MAP_PRIVATE` mappings is unspecified, and in practice on systems like Linux, it has no effect on propagating private changes.\n\n**Option-by-Option Analysis**\n\n**A. In a file-backed private mapping, the first write to a clean, file-backed page triggers a Copy-On-Write (COW) fault: the kernel allocates a new anonymous page, copies the data, and updates the process’s PTE to point to this anonymous page with write permission. The original file-backed page cache page remains clean. Because the process’s dirty page is now anonymous (not in the file’s page cache), the file system writeback machinery will not write these private modifications to the file, and calls such as msync on a private mapping or fsync on the file descriptor do not propagate the private changes to the file.**\nThis statement is a precise and complete description of the COW mechanism and its consequences for private file mappings. It correctly identifies the creation of an anonymous page, the isolation of the original page cache page, and the resulting inability of `msync` or `fsync` to persist the private modifications.\n**Verdict**: **Correct**.\n\n**B. In a file-backed private mapping, the write fault marks the file’s page cache page dirty in place; therefore, background writeback and calls to msync or fsync will propagate the private changes to the file on disk.**\nThis statement is incorrect. It describes the behavior of a **shared** (`MAP_SHARED`) mapping, where modifications are made directly to the page cache page, making them visible to other processes and subject to writeback. This behavior is antithetical to the \"private\" and \"copy-on-write\" semantics of a `MAP_PRIVATE` mapping.\n**Verdict**: **Incorrect**.\n\n**C. A conclusive user-space test can be designed as follows: compute a strong checksum $H_{0}$ of the file’s content via read on a freshly opened file descriptor; map the file with a private, writable mapping and modify some bytes in the mapping; then call msync with the MS_SYNC flag and fsync on the file descriptor; unmap; reopen the file and compute $H_{1}$ by reading the file through a path that avoids reusing any potentially stale in-memory state (for example, by using either O_DIRECT with properly aligned buffered reads or by first dropping caches if permitted by policy); finally, compare $H_{0}$ and $H_{1}$. Expect $H_{0}=H_{1}$, showing the file is unchanged, while a read from the private mapping (before unmap) returns the modified bytes.**\nThis statement outlines a scientifically sound experimental procedure to verify the theoretical behavior. It correctly identifies the steps: establishing a baseline ($H_{0}$), performing the private modification, attempting synchronization, and then re-measuring the on-disk state ($H_{1}$) while carefully avoiding caching artifacts. The predicted outcome ($H_{0}=H_{1}$) is consistent with our derivation that private modifications are not written to the file. The additional check that the mapping itself holds the modified data confirms the write occurred in the process's private memory.\n**Verdict**: **Correct**.\n\n**D. To make private mapping modifications reach the file, it suffices to call fsync on the file descriptor after the modifications; fsync will locate and write back the process’s private dirty pages.**\nThis statement is incorrect. As established, the modified page is anonymous and disconnected from the file's data structures managed by the kernel's file system layer. The `fsync` system call operates on the file descriptor and its associated dirty pages in the page cache. It has no mechanism to find and write back a process's private anonymous pages.\n**Verdict**: **Incorrect**.\n\n**E. A suitable test is to map the same file with a shared mapping in another process and then, after the first process modifies its private mapping and calls msync, read through the shared mapping; the shared mapping should reflect the private mapping’s writes if the writeback mechanism works, so seeing the modified bytes would confirm that the kernel wrote back the private dirty pages to the file.**\nThis statement describes a valid experimental setup but draws an incorrect conclusion about the expected outcome. The COW mechanism ensures isolation. The first process's write creates a private copy. The second process with the shared mapping will continue to see the original, unmodified data from the page cache. Therefore, reading from the shared mapping will **not** show the modifications. The test would actually be a good demonstration of the isolation property, but the statement wrongly suggests one might see the modified bytes, which is false. The premise (\"if the writeback mechanism works\" for private pages) is itself incorrect.\n**Verdict**: **Incorrect**.", "answer": "$$\\boxed{AC}$$", "id": "3629104"}]}