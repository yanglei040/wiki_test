{"hands_on_practices": [{"introduction": "The performance of a demand-paged system is not determined by its speed during normal operation, but by how it handles the rare but costly event of a page fault. The Effective Access Time ($EAT$) provides a mathematical framework for understanding this, blending the time for a fast memory hit with the much longer time for a slow page fault. This first practice [@problem_id:3668900] challenges you to quantify this penalty by modeling a system with two different backing stores: a traditional Hard Disk Drive (HDD) and a modern Solid-State Drive (SSD), revealing how hardware choices can dramatically alter system responsiveness.", "problem": "A process uses demand paging with a single-level page table on a workstation whose main memory access time is $t_{m} = 0.1 \\,\\mu \\text{s}$. On each memory reference, a page fault occurs with probability $p = 2 \\times 10^{-4}$. When a page fault occurs, the operating system traps into the kernel, updates page tables, schedules a read of the missing page, and resumes the process. The processor overhead outside of input/output for this handling is $t_{\\text{os}} = 25 \\,\\mu \\text{s}$.\n\nThe backing store is either a Solid-State Drive (SSD) or a Hard Disk Drive (HDD). For each page fault, the storage service time comprises a fixed controller latency plus a sequential transfer of the page. Model the storage latency as a random variable $L$ with mean $\\mathbb{E}[L]$ and variance $\\operatorname{Var}(L)$; the page transfer time is given by $t_{x} = S / B$ where $S$ is the page size and $B$ is the sustained read bandwidth. The page size is $S = 4096 \\text{ bytes}$. Assume for the SSD that $\\mathbb{E}[L_{\\text{SSD}}] = 100 \\,\\mu \\text{s}$ and $\\operatorname{Var}(L_{\\text{SSD}}) = (20 \\,\\mu \\text{s})^{2}$ with sustained bandwidth $B_{\\text{SSD}} = 500 \\times 10^{6} \\text{ bytes/s}$. Assume for the HDD that $\\mathbb{E}[L_{\\text{HDD}}] = 6000 \\,\\mu \\text{s}$ and $\\operatorname{Var}(L_{\\text{HDD}}) = (3000 \\,\\mu \\text{s})^{2}$ with sustained bandwidth $B_{\\text{HDD}} = 150 \\times 10^{6} \\text{ bytes/s}$.\n\nUsing only the definition of expected value for a Bernoulli mixture of outcomes (no-fault versus page-fault) and the above parameters, derive an expression for the Effective Access Time (EAT) under SSD and under HDD. Then compute the difference $\\Delta EAT = EAT_{\\text{HDD}} - EAT_{\\text{SSD}}$ as a single real number. Finally, justify qualitatively from first principles of variance and tail probabilities why the SSD backing store reduces both the variance of access times and the tail latency of page faults relative to the HDD.\n\nExpress the final numerical value of $\\Delta EAT$ in $\\mu \\text{s}$ and round your answer to four significant figures.", "solution": "The problem asks for the derivation of an expression for the Effective Access Time (EAT), the calculation of the difference in EAT between using a Hard Disk Drive (HDD) and a Solid-State Drive (SSD) as a backing store, and a qualitative justification for the performance benefits of SSDs regarding access time variance and tail latency.\n\nFirst, we establish the model for Effective Access Time. A memory reference results in one of two outcomes: a successful access if the page is in main memory (an event with probability $1-p$), or a page fault if it is not (an event with probability $p$). The EAT is the expected value of the access time over this Bernoulli distribution of outcomes.\n\nLet $T_{hit}$ be the time for a memory access when the page is present in memory, and let $T_{fault}$ be the time to service a page fault. The problem specifies the main memory access time as $t_m$. In the absence of further information about a Translation Lookaside Buffer (TLB) or multi-level page tables, we define the non-fault access time $T_{hit}$ as $t_m$. The EAT is then given by:\n$$EAT = (1-p) \\cdot T_{hit} + p \\cdot \\mathbb{E}[T_{fault}]$$\nHere, $\\mathbb{E}[T_{fault}]$ is the expected page fault service time.\n\nThe page fault service time, $T_{fault}$, consists of several components as described: operating system overhead ($t_{os}$), storage device latency ($L$), and page transfer time ($t_x$).\n$$T_{fault} = t_{os} + L + t_x$$\nThe latency $L$ is a random variable, while $t_{os}$ and $t_x$ are constants for a given device. The expected page fault service time is found by applying the linearity of expectation:\n$$\\mathbb{E}[T_{fault}] = \\mathbb{E}[t_{os} + L + t_x] = t_{os} + \\mathbb{E}[L] + t_x$$\n\nThe page transfer time $t_x$ is determined by the page size $S$ and the device's sustained read bandwidth $B$, where $t_x = S/B$. We calculate this for both the SSD and the HDD.\n\nFor the SSD:\n$S = 4096 \\text{ bytes}$\n$B_{\\text{SSD}} = 500 \\times 10^{6} \\text{ bytes/s}$\n$$t_{x, \\text{SSD}} = \\frac{S}{B_{\\text{SSD}}} = \\frac{4096}{500 \\times 10^{6}} \\text{ s} = 8.192 \\times 10^{-6} \\text{ s} = 8.192 \\,\\mu\\text{s}$$\n\nFor the HDD:\n$S = 4096 \\text{ bytes}$\n$B_{\\text{HDD}} = 150 \\times 10^{6} \\text{ bytes/s}$\n$$t_{x, \\text{HDD}} = \\frac{S}{B_{\\text{HDD}}} = \\frac{4096}{150 \\times 10^{6}} \\text{ s} \\approx 27.3067 \\times 10^{-6} \\text{ s} = 27.3067 \\,\\mu\\text{s}$$\n\nNow, we can write the full expression for the expected page fault service time for each device, using the given parameters: $t_{os} = 25 \\,\\mu\\text{s}$, $\\mathbb{E}[L_{\\text{SSD}}] = 100 \\,\\mu\\text{s}$, and $\\mathbb{E}[L_{\\text{HDD}}] = 6000 \\,\\mu\\text{s}$.\n\nExpected page fault time for SSD:\n$$\\mathbb{E}[T_{\\text{fault, SSD}}] = t_{os} + \\mathbb{E}[L_{\\text{SSD}}] + t_{x, \\text{SSD}} = 25 \\,\\mu\\text{s} + 100 \\,\\mu\\text{s} + 8.192 \\,\\mu\\text{s} = 133.192 \\,\\mu\\text{s}$$\n\nExpected page fault time for HDD:\n$$\\mathbb{E}[T_{\\text{fault, HDD}}] = t_{os} + \\mathbb{E}[L_{\\text{HDD}}] + t_{x, \\text{HDD}} \\approx 25 \\,\\mu\\text{s} + 6000 \\,\\mu\\text{s} + 27.3067 \\,\\mu\\text{s} = 6052.3067 \\,\\mu\\text{s}$$\n\nThe Effective Access Times for each configuration are:\n$$EAT_{\\text{SSD}} = (1-p)t_m + p \\cdot \\mathbb{E}[T_{\\text{fault, SSD}}]$$\n$$EAT_{\\text{HDD}} = (1-p)t_m + p \\cdot \\mathbb{E}[T_{\\text{fault, HDD}}]$$\n\nWe are asked to compute the difference $\\Delta EAT = EAT_{\\text{HDD}} - EAT_{\\text{SSD}}$.\n$$\\Delta EAT = \\left( (1-p)t_m + p \\cdot \\mathbb{E}[T_{\\text{fault, HDD}}] \\right) - \\left( (1-p)t_m + p \\cdot \\mathbb{E}[T_{\\text{fault, SSD}}] \\right)$$\nThe term $(1-p)t_m$ cancels, simplifying the expression to:\n$$\\Delta EAT = p \\left( \\mathbb{E}[T_{\\text{fault, HDD}}] - \\mathbb{E}[T_{\\text{fault, SSD}}] \\right)$$\nSubstituting the calculated values:\n$$\\Delta EAT = (2 \\times 10^{-4}) \\left( 6052.3067 \\,\\mu\\text{s} - 133.192 \\,\\mu\\text{s} \\right)$$\n$$\\Delta EAT = (2 \\times 10^{-4}) \\left( 5919.1147 \\,\\mu\\text{s} \\right)$$\n$$\\Delta EAT \\approx 1.1838229 \\,\\mu\\text{s}$$\nRounding to four significant figures gives $1.184 \\,\\mu\\text{s}$.\n\nFinally, we provide a qualitative justification for why the SSD reduces both the variance of access times and the tail latency of page faults relative to the HDD.\n\n1.  **Variance of Access Times**: The random component in the page fault service time $T_{fault} = t_{os} + L + t_x$ is the storage latency $L$, as both $t_{os}$ and $t_x$ are deterministic constants. The variance of the page fault service time is therefore equal to the variance of the latency: $\\operatorname{Var}(T_{fault}) = \\operatorname{Var}(t_{os} + L + t_x) = \\operatorname{Var}(L)$. The problem provides these variances:\n    $$\\operatorname{Var}(L_{\\text{SSD}}) = (20 \\,\\mu\\text{s})^2 = 400 \\,(\\mu\\text{s})^2$$\n    $$\\operatorname{Var}(L_{\\text{HDD}}) = (3000 \\,\\mu\\text{s})^2 = 9 \\times 10^6 \\,(\\mu\\text{s})^2$$\n    The standard deviations are $\\sigma_{\\text{SSD}} = 20 \\,\\mu\\text{s}$ and $\\sigma_{\\text{HDD}} = 3000 \\,\\mu\\text{s}$. The variance (and standard deviation) of the HDD latency is orders of magnitude larger than that of the SSD. This is a direct consequence of their physical nature. HDD latency is dominated by mechanical operations: the seek time to move the read/write head to the correct track and the rotational latency to wait for the desired sector to spin under the head. These times are highly variable, depending on the initial and final positions of the head and disk. In contrast, SSD latency is purely electronic, involving addressing and accessing flash memory cells, which is a far more deterministic and consistent process. This fundamental difference in operation is why $\\operatorname{Var}(L_{\\text{HDD}}) \\gg \\operatorname{Var}(L_{\\text{SSD}})$, and thus the variance of the page fault service time is much lower with an SSD.\n\n2.  **Tail Latency**: Tail latency refers to the probability of observing very long service times, often characterized by high percentiles (e.g., $99$th or $99.9$th) of the service time distribution. A random variable with a larger variance will have a wider probability distribution, meaning its values are more spread out from the mean. A distribution with a larger variance and/or a larger mean will generally have a \"heavier\" or \"longer\" tail, indicating a non-trivial probability of observing outcomes far from the mean. The HDD's latency distribution has both a much larger mean ($\\mathbb{E}[L_{\\text{HDD}}] = 6000 \\,\\mu\\text{s}$ vs. $\\mathbb{E}[L_{\\text{SSD}}] = 100 \\,\\mu\\text{s}$) and a vastly larger variance. Consequently, the probability of experiencing a page fault service time significantly longer than the average (i.e., a tail latency event) is substantially higher for the HDD. For an SSD, the low mean and minuscule variance mean its latency distribution is tightly concentrated, making extremely long delays highly improbable. Therefore, the SSD provides not only faster average performance but also more predictable performance with a much lower incidence of worst-case latency spikes.", "answer": "$$\\boxed{1.184}$$", "id": "3668900"}, {"introduction": "Knowing the high cost of a page fault, the logical next step is to minimize how often they occur. This is the primary job of the page replacement algorithm, which decides which page to evict when a new page must be loaded. This exercise [@problem_id:3668817] goes beyond simple simulation to compare the effectiveness of common algorithms like FIFO, LRU, and Clock, and introduces a powerful analytical tool—reuse distance—to understand their behavior and uncover counter-intuitive performance pitfalls like Belady's Anomaly.", "problem": "A virtual memory system uses demand paging with a fully associative main memory of $M$ page frames and a single process whose reference stream exhibits distinct locality phases. Consider the following reference trace $\\mathcal{S}$ composed by concatenating two phases $\\mathcal{A}$ and $\\mathcal{B}$ twice:\n$$\n\\mathcal{A} = [\\,1,\\,2,\\,1,\\,2,\\,1,\\,2,\\,3,\\,1,\\,2,\\,1,\\,2,\\,3\\,], \\quad\n\\mathcal{B} = [\\,1,\\,2,\\,3,\\,4,\\,1,\\,2,\\,5,\\,1,\\,2,\\,3,\\,4,\\,5\\,],\n$$\nso that\n$$\n\\mathcal{S} = \\mathcal{A} \\,\\Vert\\, \\mathcal{B} \\,\\Vert\\, \\mathcal{A} \\,\\Vert\\, \\mathcal{B},\n$$\nwith $\\Vert$ denoting concatenation. The total length of $\\mathcal{S}$ is $48$ references. The page replacement algorithms to be compared are First-In First-Out (FIFO), Least Recently Used (LRU), and the Second-Chance (Clock) algorithm. Assume a single-level uniform memory access, negligible translation lookaside buffer effects, and that all page frames are initially empty with all reference bits clear before processing the first reference of $\\mathcal{S}$.\n\nYou will use the concept of reuse (stack) distance to reason about the performance of demand paging. For a reference to a page at time $t$, define its reuse distance $K$ as the number of distinct pages referenced strictly between its previous reference and time $t$; for the first reference to a page (no previous reference), let $K = \\infty$. Let $D(k)$ denote the empirical distribution of $K$ measured over the entire trace $\\mathcal{S}$, that is, for each $k \\in \\{1,2,3,\\dots\\}$ and $k=\\infty$, $D(k)$ is the fraction of references in $\\mathcal{S}$ whose reuse distance equals $k$.\n\nTasks:\n- Using only the core definitions of demand paging, reuse distance, and the qualitative stack property of LRU (that the set of pages in memory under LRU with $M$ frames is always a superset of that with $M-1$ frames on the same trace), compute the empirical distribution $D(k)$ for the given $\\mathcal{S}$.\n- From first principles, derive how to compute the expected number of page faults for LRU from $D(k)$ and the frame count $M$, and evaluate this for $M=3$ on $\\mathcal{S}$.\n- By direct reasoning from the algorithm definitions, determine the total number of page faults incurred by FIFO and by Second-Chance (Clock) when $M=3$ on $\\mathcal{S}$, and compare qualitatively to the LRU result you derived from $D(k)$.\n- Focusing on the single phase $\\mathcal{B}$ (the $12$-reference subsequence) under a cold start (empty frames at the start of $\\mathcal{B}$), discuss the risk of Belady’s anomaly for FIFO by comparing FIFO’s total page faults when $M=3$ versus when $M=4$ on that subsequence, and explain the root cause of any anomaly observed in terms of the lack of the stack property.\n\nProvide as your final answer only the expected total number of LRU page faults for $M=3$ over the entire $48$-reference trace $\\mathcal{S}$, as a single integer with no units. Do not include intermediate results in the final answer box.", "solution": "We begin from core definitions. A page fault occurs when the referenced page is not present in the set of $M$ page frames. The reuse distance $K$ for a reference is the number of distinct pages referenced strictly between its previous occurrence and the current time; for a first reference to a page, $K=\\infty$. For Least Recently Used (LRU), a reference is a hit precisely when the number of distinct pages used since the last reference to that page is strictly less than the number of frames $M$, because LRU maintains in memory the $M$ most recently used distinct pages. Therefore, under LRU with $M$ frames, a reference faults if and only if $K \\ge M$ (including $K=\\infty$). This yields, for a trace of length $N$, that the LRU fault count equals\n$$\nN \\cdot \\Big( D(\\infty) \\;+\\; \\sum_{k=M}^{\\infty} D(k) \\Big),\n$$\nwhen $D(k)$ is the empirical reuse distance distribution.\n\nStep $1$: Compute the empirical $D(k)$ for $\\mathcal{S}$.\n\nWe scan $\\mathcal{S}$ once, and for each reference, compute $K$ as the number of distinct pages encountered since the previous reference to the same page, or $K=\\infty$ if none. Tallying over the entire $48$ references yields the following counts:\n- $5$ references with $K=\\infty$,\n- $12$ references with $K=1$,\n- $16$ references with $K=2$,\n- $4$ references with $K=3$,\n- $11$ references with $K=4$,\nand $0$ for all other finite $k$.\nThese counts sum to $48$, as required. Thus,\n$$\nD(\\infty) \\;=\\; \\frac{5}{48},\\quad D(1)\\;=\\;\\frac{12}{48},\\quad D(2)\\;=\\;\\frac{16}{48},\\quad D(3)\\;=\\;\\frac{4}{48},\\quad D(4)\\;=\\;\\frac{11}{48},\n$$\nwith $D(k)=0$ for all $k \\ge 5$.\n\nStep $2$: Derive and evaluate the LRU expected fault count for $M=3$.\n\nBy the LRU stack property and the definition of reuse distance, a reference faults under LRU with $M=3$ if $K \\in \\{\\infty\\} \\cup \\{3,4,5,\\dots\\}$. Using the empirical $D(k)$ above with $N=48$,\n$$\n\\text{Faults}_{\\text{LRU},\\,M=3}\n\\;=\\;\n48 \\cdot \\Big( D(\\infty) + \\sum_{k=3}^{\\infty} D(k) \\Big)\n\\;=\\;\n48 \\cdot \\Big( \\frac{5}{48} + \\frac{4}{48} + \\frac{11}{48} \\Big)\n\\;=\\;\n48 \\cdot \\frac{20}{48}\n\\;=\\;\n20.\n$$\nEquivalently, counting directly: $5$ first-touch references plus $4$ references with $K=3$ plus $11$ with $K=4$ produce $20$ LRU faults.\n\nStep $3$: Compare FIFO and Second-Chance (Clock) for $M=3$ by direct algorithmic reasoning.\n\nWe now simulate FIFO and Clock from their definitions. For First-In First-Out (FIFO), maintain a queue of loaded frames in arrival order; on a miss with a full memory, evict the oldest frame (the head of the queue). For Second-Chance (Clock), maintain a circular list of frames with a reference bit per frame; on a hit, set that page’s bit to $1$; on a miss, advance the hand, clearing any encountered bit that is $1$ to $0$ and continuing, until a frame with bit $0$ is found; evict that frame, install the new page there with bit $1$, and advance the hand by one position.\n\nCarrying out a step-by-step simulation across the $48$-reference trace $\\mathcal{S}$ with $M=3$ (frames initially empty and, for Clock, all reference bits initially $0$ and the hand at the first frame) yields the following total page fault counts:\n- $\\text{Faults}_{\\text{FIFO},\\,M=3} = 18$,\n- $\\text{Faults}_{\\text{Clock},\\,M=3} = 18$,\nwhich are both lower than or equal to, or higher than, the LRU count depending on the trace; here, the empirical LRU count is $20$. This illustrates that, although LRU has the stack property and tends to minimize faults among stack algorithms for a given $M$ on typical locality-rich traces, FIFO and Clock are not stack algorithms; in particular, FIFO may sometimes incur fewer or more faults than LRU on a given finite trace.\n\nStep $4$: Discuss Belady’s anomaly risk for FIFO on phase $\\mathcal{B}$.\n\nBelady’s anomaly refers to the possibility that, under FIFO, increasing the number of page frames can increase the number of page faults. Consider the single phase $\\mathcal{B} = [\\,1,\\,2,\\,3,\\,4,\\,1,\\,2,\\,5,\\,1,\\,2,\\,3,\\,4,\\,5\\,]$ under a cold start (empty frames at the start of $\\mathcal{B}$). A direct FIFO simulation shows:\n- With $M=3$, the total FIFO faults over $\\mathcal{B}$ are $9$.\n- With $M=4$, the total FIFO faults over $\\mathcal{B}$ are $10$.\nThus, increasing frames from $3$ to $4$ increases FIFO faults from $9$ to $10$, an instance of Belady’s anomaly. The root cause is that FIFO lacks the stack property: the set of pages in memory with $M=4$ at a given time is not necessarily a superset of the set with $M=3$ on the same reference string, so additional capacity can perturb eviction order in a way that leads to extra misses.\n\nConclusion and required numeric result.\n\nFrom the measured $D(k)$ and the LRU stack property, the expected total number of LRU page faults on $\\mathcal{S}$ for $M=3$ equals $20$.", "answer": "$$\\boxed{20}$$", "id": "3668817"}, {"introduction": "Reactive strategies, like handling faults as they happen, are fundamental, but modern systems often employ proactive optimizations to prevent faults from occurring in the first place. This is especially true in cloud environments where predictable performance is key. In this final practice [@problem_id:3668872], you will model a common real-world scenario of \"warming up\" a new application instance by pre-loading critical pages, and you'll find the optimal balance between the upfront cost of preparation and the subsequent performance gain during execution.", "problem": "A cloud service deploys application instances into a server pool under a demand-paging virtual memory system. Before an instance begins serving traffic, the platform may optionally pre-touch $K$ virtual pages to warm the working set. The goal is to choose $K$ to minimize the expected completion time of a single request handled by one fresh instance.\n\nAssume the following, grounded in standard demand-paging behavior:\n\n- The Effective Access Time (EAT) depends on the probability of a page fault per memory reference. Each memory reference takes the baseline memory access time $m$ when it hits in memory and, if it faults, incurs an additional penalty $F$ for servicing the fault.\n- Warming $K$ pages takes time that is linear in $K$: touching each page takes $t_{\\mathrm{touch}}$ seconds and does not overlap with serving the request.\n- Warming reduces the per-reference fault probability according to a diminishing-returns model: $p(K) = p_{0}\\,\\exp(-\\alpha K)$.\n- Each fresh instance will perform exactly $N$ memory references to serve one request, after which it is torn down. The baseline memory access time $m$ does not depend on $K$.\n\nUse the following parameter values, expressed in seconds, references, and pages:\n- $m = 1.0 \\times 10^{-7}$,\n- $F = 8.0 \\times 10^{-3}$,\n- $t_{\\mathrm{touch}} = 1.0 \\times 10^{-3}$,\n- $p_{0} = 2.0 \\times 10^{-5}$,\n- $\\alpha = 5.0 \\times 10^{-2}$,\n- $N = 5.0 \\times 10^{6}$.\n\nStarting from the expected-value definition of time under demand paging and the linear warm-up model above, derive an expression for the expected total time to complete the request as a function of $K$, and determine the integer $K^{*}$ that minimizes it. Report $K^{*}$ as a single integer number of pages. No rounding rule is needed beyond choosing the integer that minimizes the expected total time. The final answer must be a single number.", "solution": "The goal is to find the integer number of pages to pre-warm, $K$, that minimizes the total expected time for a request. The total time $T(K)$ is the sum of the pre-warming time $T_{\\text{warm}}(K)$ and the request service time $T_{\\text{req}}(K)$.\n\n1.  **Model the Total Time:**\n    The pre-warming time is linear in the number of pages warmed:\n    $$T_{\\text{warm}}(K) = K \\cdot t_{\\mathrm{touch}}$$\n    The request service time is the total number of memory references $N$ multiplied by the Effective Access Time (EAT) per reference, which depends on $K$:\n    $$T_{\\text{req}}(K) = N \\cdot \\text{EAT}(K)$$\n    The EAT is the baseline memory access time $m$ plus the expected penalty from a page fault, which is the fault probability $p(K)$ times the fault penalty $F$:\n    $$\\text{EAT}(K) = m + p(K) \\cdot F$$\n    Substituting the given model for fault probability, $p(K) = p_{0} \\exp(-\\alpha K)$:\n    $$\\text{EAT}(K) = m + p_{0} F \\exp(-\\alpha K)$$\n    Combining these gives the total expected time as a function of $K$:\n    $$T(K) = T_{\\text{warm}}(K) + T_{\\text{req}}(K) = K t_{\\mathrm{touch}} + N \\left( m + p_{0} F \\exp(-\\alpha K) \\right)$$\n    $$T(K) = K t_{\\mathrm{touch}} + N m + N p_{0} F \\exp(-\\alpha K)$$\n\n2.  **Find the Optimal K:**\n    To find the value of $K$ that minimizes $T(K)$, we treat $K$ as a continuous variable and set its derivative with respect to $K$ to zero. The term $N m$ is a constant, so it does not affect the optimization.\n    $$\\frac{dT(K)}{dK} = t_{\\mathrm{touch}} - \\alpha N p_{0} F \\exp(-\\alpha K)$$\n    Setting the derivative to zero to find the optimal $K_{\\text{opt}}$:\n    $$t_{\\mathrm{touch}} = \\alpha N p_{0} F \\exp(-\\alpha K_{\\text{opt}})$$\n    $$\\exp(-\\alpha K_{\\text{opt}}) = \\frac{t_{\\mathrm{touch}}}{\\alpha N p_{0} F}$$\n    Solving for $K_{\\text{opt}}$ by taking the natural logarithm:\n    $$K_{\\text{opt}} = -\\frac{1}{\\alpha} \\ln\\left(\\frac{t_{\\mathrm{touch}}}{\\alpha N p_{0} F}\\right) = \\frac{1}{\\alpha} \\ln\\left(\\frac{\\alpha N p_{0} F}{t_{\\mathrm{touch}}}\\right)$$\n    The second derivative, $\\frac{d^2T}{dK^2} = \\alpha^2 N p_{0} F \\exp(-\\alpha K)$, is always positive, confirming that this value corresponds to a minimum.\n\n3.  **Substitute Values and Calculate:**\n    Using the provided parameters:\n    - $\\alpha = 0.05$\n    - $N = 5 \\times 10^6$\n    - $p_0 = 2 \\times 10^{-5}$\n    - $F = 8 \\times 10^{-3}$\n    - $t_{\\text{touch}} = 1 \\times 10^{-3}$\n    \n    First, calculate the argument of the logarithm:\n    $$\\frac{\\alpha N p_{0} F}{t_{\\mathrm{touch}}} = \\frac{(0.05)(5 \\times 10^6)(2 \\times 10^{-5})(8 \\times 10^{-3})}{1 \\times 10^{-3}} = \\frac{0.04}{0.001} = 40$$\n    Now, calculate $K_{\\text{opt}}$:\n    $$K_{\\text{opt}} = \\frac{1}{0.05} \\ln(40) = 20 \\ln(40)$$\n    Using $\\ln(40) \\approx 3.68888$,\n    $$K_{\\text{opt}} \\approx 20 \\times 3.68888 = 73.7776$$\n\n4.  **Determine the Integer Solution:**\n    Since $K$ must be an integer, the optimal value $K^*$ will be one of the integers adjacent to the continuous optimum, i.e., $K=73$ or $K=74$. Because the function $T(K)$ is convex, the integer minimum will be the integer closest to the continuous minimum $K_{\\text{opt}}$. As $73.7776$ is closer to $74$ than to $73$, the optimal integer number of pages is $74$.", "answer": "$$\\boxed{74}$$", "id": "3668872"}]}