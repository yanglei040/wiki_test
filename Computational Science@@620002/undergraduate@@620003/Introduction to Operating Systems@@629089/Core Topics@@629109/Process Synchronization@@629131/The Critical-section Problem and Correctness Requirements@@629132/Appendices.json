{"hands_on_practices": [{"introduction": "A common pitfall in concurrent programming is placing a lock around a shared resource write but failing to protect the logic that determines *how* that resource is accessed. This exercise [@problem_id:3687271] explores this subtle but critical error. You will analyze a flawed protocol where two threads can experience a \"torn read\" of a shared cursor, leading them to compute the same destination index and overwrite each other's data, even though the final write operation itself is protected by a mutex. This practice will sharpen your ability to identify the true, complete scope of a critical section.", "problem": "Two producer threads $T_1$ and $T_2$ append records into a shared two-dimensional buffer $B$ of size $R \\times C$, with $C = 2$ columns. A position in $B$ is addressed by a pair $(i,j)$, and the linear index of $(i,j)$ is $s(i,j) = i \\cdot C + j$. There is a shared cursor $(I,J)$ that encodes the next free slot. To reduce perceived contention, the implementation uses the following flawed protocol:\n- Each thread reads $I$ and $J$ separately, without holding the lock, into per-thread locals $(i,j)$.\n- Each thread then acquires a mutex $L$, writes its record to $B[i][j]$, and releases $L$.\n- After releasing $L$, each thread advances the shared cursor by non-atomic stores: it first sets $J := (j+1) \\bmod C$, and if the result is $0$ it later sets $I := I + 1$. These two updates to $J$ and $I$ are not performed atomically, and neither is protected by $L$.\n\nAssume the initial shared state is $(I,J) = (k,0)$ for some integer $k \\ge 0$, and both threads follow the protocol exactly as described. The system is a conventional Central Processing Unit (CPU) with sequentially consistent ordering for single-variable reads and writes, but no guarantee of atomicity across the pair $(I,J)$ as a composite state. For the purposes of this question, “duplicate entries” means two writes to the same $B[i][j]$ slot containing distinct records from $T_1$ and $T_2$.\n\nWhich of the following interleavings yields a duplicate entry in $B$ despite the presence of the mutex $L$, solely because the updates to $I$ or $J$ are non-atomic and occur outside the protection of $L$?\n\nA. Safe progression with no torn reads:\n- $T_1$: reads $I = k$; reads $J = 0$; acquires $L$; writes $B[k][0]$; releases $L$; sets $J := 1$.\n- $T_2$: reads $I = k$; reads $J = 1$; acquires $L$; writes $B[k][1]$; releases $L$; sets $J := 0$; sets $I := k + 1$.\n\nB. Torn read across non-atomic update produces duplicate:\n- $T_1$: reads $I = k$.\n- $T_2$: acquires $L$; writes $B[k][0]$; releases $L$; sets $J := 1$.\n- $T_1$: reads $J = 1$; its local is now $(i_1,j_1) = (k,1)$.\n- $T_2$: reads $I = k$; reads $J = 1$; acquires $L$; writes $B[k][1]$; releases $L$; sets $J := 0$; sets $I := k + 1$.\n- $T_1$: acquires $L$; writes $B[k][1]$; releases $L$; sets $J := 0$; sets $I := k + 1$.\n\nC. Wrap-around split across threads but still unique slots:\n- $T_1$: reads $I = k$; reads $J = 0$; acquires $L$; writes $B[k][0]$; releases $L$; sets $J := 1$.\n- $T_2$: reads $I = k$; reads $J = 1$; acquires $L$; writes $B[k][1]$; releases $L$; sets $J := 0$; sets $I := k + 1$.\n- $T_1$: later reads $I = k + 1$; reads $J = 0$; acquires $L$; writes $B[k+1][0]$; releases $L$.\n\nD. Lost-increment on $I$ but no duplicate slot:\n- $T_1$: reads $I = k$; reads $J = 0$; acquires $L$; writes $B[k][0]$; releases $L$; sets $J := 1$.\n- $T_2$: reads $I = k$; reads $J = 1$; acquires $L$; writes $B[k][1]$; releases $L$; sets $J := 0$.\n- $T_1$: sets $I := k + 1$; $T_2$ also sets $I := k + 1$; both proceed with $I = k + 1$ and $J = 0$ on their next iterations.\n\nSelect the single correct option.\n\nYour reasoning must start from the core definitions of the critical-section problem and its correctness requirements (mutual exclusion, progress, bounded waiting) and the definition of atomicity for shared-state updates. Then, deduce whether each interleaving does or does not produce a duplicate write to the same $B[i][j]$ slot, and why the mutex $L$ fails or succeeds in enforcing the necessary correctness.", "solution": "The core flaw in this protocol is that the critical section protected by the mutex $L$ is too small. It only protects the single `write` operation to the buffer $B$, but it fails to protect the logic that determines *where* to write. The reading of the shared cursor $(I, J)$ and the updating of it are performed outside the locked region, creating a race condition.\n\nLet's analyze option B to see how this race leads to a duplicate entry (a lost update on a buffer slot). Assume the shared state is $(I, J) = (k, 0)$. The trace implies that $T_2$ had previously determined its slot to be $(k,0)$.\n1.  **$T_1$ starts its turn:** It reads $I = k$.\n2.  **$T_2$ takes its turn on slot $(k,0)$:** As per the trace, $T_2$ acquires $L$, writes to $B[k][0]$, releases $L$, and then updates the shared cursor to $J=1$. The shared state becomes $(I, J) = (k, 1)$.\n3.  **$T_1$ performs a \"torn read\":** Now $T_1$ continues its turn. It reads the *new* value of $J$, which is $1$. It assembles its local destination index as $(i_1, j_1) = (k, 1)$.\n4.  **$T_2$ starts its next turn:** It now reads the current shared cursor, which is $(I, J) = (k, 1)$. It computes its local destination index as $(i_2, j_2) = (k, 1)$.\n5.  **Collision:** Both threads, $T_1$ and $T_2$, now believe the next free slot is $B[k][1]$.\n6.  **Lost Update:** The trace shows $T_2$ writes to $B[k][1]$ first, followed by $T_1$ writing to the exact same slot $B[k][1]$. The mutex $L$ serializes these writes, but it doesn't prevent the logical error: $T_1$'s write overwrites $T_2$'s write. This is a \"duplicate entry\" as per the problem's definition.\n\nThis failure happens because the read of the cursor components was not atomic with the write to the buffer. Option B correctly illustrates this specific interleaving, which is a classic example of why the entire read-modify-write sequence for shared state logic must be included within the critical section. The other options describe scenarios that are either correct executions (A, C) or describe a different type of bug (D) but do not result in a duplicate entry in the manner described.", "answer": "$$\\boxed{B}$$", "id": "3687271"}, {"introduction": "After understanding the importance of correctly defining a critical section, the next step is to explore formal locking algorithms designed for correctness. This practice [@problem_id:3687304] introduces the ticket lock, an elegant and fair mechanism that uses an atomic `fetch-and-increment` operation to serialize access. Your task is to reason about its fundamental invariants and discover how improper handling, such as re-initializing the lock while it is active, can catastrophically violate mutual exclusion. You will also see how other errors, like incorrect initial values, can lead to liveness failures like starvation, helping you distinguish between different classes of concurrency bugs.", "problem": "Consider the critical-section problem for a shared resource accessed by multiple threads on a shared-memory multiprocessor. The correctness requirements include the safety property of mutual exclusion: at most one thread is in the critical section at any time. A common solution is a fairness ticket lock that uses atomic read-modify-write to serialize entry.\n\nAssume a hardware primitive atomic fetch-and-increment, denoted by an indivisible operation that maps a shared integer variable to a pair consisting of its old value and a side effect that increases it by one. Concretely, if a shared integer variable is $x$, then an invocation returns the current value $x$ and subsequently updates the memory location to $x + 1$, all as one atomic action. Assume a sequentially consistent memory model for simplicity and that all threads agree on a single shared memory image for the lock’s state.\n\nA fairness ticket lock maintains two shared integers: a “next-ticket” counter and a “now-serving” counter. A thread that wishes to enter the critical section executes the following protocol: it performs an atomic fetch-and-increment on the next-ticket counter to obtain a private ticket $m$, and then it waits while the now-serving counter is not equal to $m$. When it observes equality, it enters the critical section. Upon exit, it increments the now-serving counter by $1$. The usual intent is that both counters start at the same value so that the first arriving thread immediately proceeds, and the others wait in first-come first-served order by ticket number.\n\nFrom first principles, base your reasoning on the following core definitions and facts:\n- Mutual exclusion (safety): at most one thread is in the critical section at any time.\n- Atomic read-modify-write operations provide uniqueness of returned “old values” across concurrent invocations.\n- Sequential consistency: the result of execution is as if all operations were interleaved in some total order consistent with the program order of each thread.\n\nSuppose the system has threads $P$, $Q$, and $R$. Consider two distinct situations of initialization of the counters:\n- Situation S1: At time $t_{0}$, thread $P$ is already in the critical section with now-serving equal to some value $s$ and next-ticket equal to $s + 1$. Thread $Q$ has already fetched its ticket and holds $m_{Q} = s + 1$, waiting for now-serving to advance. At time $t_{1}  t_{0}$, a buggy “initialization” routine erroneously writes both counters with $0$ while $P$ is still in the critical section. At time $t_{2}  t_{1}$, thread $R$ calls the lock acquire function.\n- Situation S2: Before any thread calls the lock acquire function, a developer mistakenly sets now-serving to $1$ and next-ticket to $0$ and never reinitializes afterward.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. The ticket-lock algorithm described satisfies mutual exclusion provided the two shared counters are initialized exactly once with equal values (for example, both to $0$) before any thread calls acquire, and no reinitialization occurs while any thread may be in or waiting for the lock. An appropriate initialization protocol is to perform one-time initialization guarded by a lock-free one-shot mechanism (for example, an atomic compare-and-swap on an “initialized” flag) that sets both counters to the same value and publishes this fact before any thread is allowed to acquire; reinitialization is only safe when it is known that no thread can be in or waiting for the lock.\n\nB. In Situation S1, it is possible for mutual exclusion to be violated: after the buggy writes of $0$, thread $R$ can obtain $m_{R} = 0$ from the next-ticket counter and immediately observe now-serving equal to $0$ and enter the critical section while $P$ is still inside, so two threads may be in the critical section concurrently.\n\nC. In Situation S2, the system may fail to make progress (for example, the first thread to arrive obtains ticket $0$ but will wait forever because now-serving is $1$ and no one can legitimately advance it), but mutual exclusion cannot be violated by that misinitialization alone because atomic fetch-and-increment ensures distinct tickets and the entry condition requires equality with now-serving.\n\nD. It is safe to allow “lazy initialization” in which any thread that calls acquire checks an “uninitialized” flag and, if it appears unset, it locally writes both counters to $0$ without mutual agreement with other threads; even if two or more threads race to perform this initialization, no violation of mutual exclusion can occur because tickets remain unique.", "solution": "This problem tests the correctness properties of a ticket lock under both normal and faulty conditions. A ticket lock's mutual exclusion relies on two invariants: (1) the atomic `fetch-and-increment` guarantees every thread gets a unique ticket, and (2) the `now-serving` counter is advanced one by one, ensuring only one thread (the one with the matching ticket) can be in the critical section.\n\n**A. Correctness Conditions:** This statement is **correct**. A ticket lock works if initialized correctly (e.g., `next-ticket = now-serving = 0`) and is never re-initialized while active. The initial equality ensures the first thread can enter, and the \"no re-initialization\" rule prevents the re-issuing of tickets, which would break the uniqueness guarantee and violate mutual exclusion. A one-shot atomic initialization is the standard, safe way to set up such a lock.\n\n**B. Mutual Exclusion Violation in S1:** This statement is **correct**. In Situation S1, thread $P$ is in the critical section (implying it holds ticket $s$ and `now-serving` was $s$ when it entered). The buggy re-initialization then resets both counters to 0. When thread $R$ arrives, it performs a `fetch-and-increment` on `next-ticket` (now 0) and receives ticket $m_R = 0$. It then checks if `now-serving` equals its ticket. Since `now-serving` was also reset to 0, the condition $0 == 0$ is true, and $R$ enters the critical section. At this point, $P$ is still inside, leading to a direct violation of mutual exclusion.\n\n**C. Liveness Failure in S2:** This statement is **correct**. In Situation S2, `now-serving = 1` and `next-ticket = 0`. The first thread to arrive will fetch ticket 0. It will then wait forever for `now-serving` to become 0, which will never happen because the protocol only increments `now-serving`. This is a progress failure (deadlock or starvation). However, mutual exclusion is **not** violated. The `fetch-and-increment` primitive will still dispense unique tickets (0, 1, 2, ...), and since `now-serving` can only hold one value at a time, only one thread can ever match its ticket and enter the critical section.\n\n**D. Racy Lazy Initialization:** This statement is **incorrect**. A \"lazy initialization\" without an atomic guard is unsafe. If two threads race, one might fetch a ticket, then the other might reset the counters. This is precisely the scenario in S1, where a reset allows a new ticket (e.g., 0) to be issued that matches the reset `now-serving` counter, enabling a second thread to enter the critical section and violate mutual exclusion. Initialization itself must be a critical section or use a lock-free atomic primitive to ensure it happens exactly once.\n\nTherefore, statements A, B, and C are correct.", "answer": "$$\\boxed{ABC}$$", "id": "3687304"}, {"introduction": "Many concurrency algorithms are first taught under the simplifying assumption of Sequential Consistency, where memory operations appear to occur in a single, global order. This final practice [@problem_id:3687333] confronts the reality of modern hardware, where weak memory models and optimizations like store buffering are the norm. By removing a single memory fence from a correct implementation of Peterson's algorithm, you will uncover an execution interleaving that allows two processes to enter the critical section simultaneously. This exercise demonstrates why memory fences are not optional but are a crucial part of the contract between concurrent software and the underlying multicore processor.", "problem": "Consider a two-process lock that is a fence-augmented variant of Peterson’s two-process algorithm and is known to satisfy the three critical-section correctness requirements — mutual exclusion, progress, and bounded waiting — on a weakly ordered architecture with per-core store buffers. There are two processes $P_0$ and $P_1$ and three shared variables: an array $flag[\\,]$ of booleans, indexed by process identifiers in $\\{0,1\\}$, and a shared variable $turn \\in \\{0,1\\}$. The initial state is $flag[0] = \\text{false}$, $flag[1] = \\text{false}$, and $turn = 0$. Each process $P_i$ for $i \\in \\{0,1\\}$ executes the following entry protocol before entering its critical section and an exit protocol after leaving it:\n\nEntry protocol for $P_i$:\n- Set $flag[i] \\leftarrow \\text{true}$.\n- Execute a full memory fence $F$.\n- Set $turn \\leftarrow 1 - i$.\n- Busy-wait while $(flag[1-i] \\land (turn = 1-i))$.\n- Enter the critical section.\n\nExit protocol for $P_i$:\n- Set $flag[i] \\leftarrow \\text{false}$.\n\nAssume the hardware implements per-core store buffering: a store by a process becomes immediately visible to that process but may become visible to the other process only after a nondeterministic delay. A load by a process can read from memory without waiting for that process’s earlier stores to different addresses to become visible to the other process. A full memory fence $F$ drains the issuing process’s store buffer before allowing any subsequent memory operation to proceed. There is no reordering of stores to the same address, and the system respects per-location coherence. This is a simplified and widely used operational characterization of weak memory with store buffering, sufficient for reasoning about this program.\n\nSuppose we remove the single fence $F$ from the entry protocol (so the line “execute a full memory fence $F$” is deleted) and leave all other steps unchanged. Under the model above, the modified lock may or may not satisfy the correctness requirements.\n\nYour task is to choose the interleaving below that is both feasible under the specified memory model and demonstrates a violation of mutual exclusion by admitting an execution in which both $P_0$ and $P_1$ are simultaneously in the critical section. In each option, “buffered” means the store remains in the issuing core’s store buffer (not yet visible to the other process), and “visible” means the store has propagated and can be read by the other process.\n\nA. Interleaving that exploits store buffering on both processes:\n- Step $1$: $P_0$: $flag[0] \\leftarrow \\text{true}$ (buffered).\n- Step $2$: $P_1$: $flag[1] \\leftarrow \\text{true}$ (buffered).\n- Step $3$: $P_0$: $turn \\leftarrow 1$ (visible). The earlier $flag[0]$ store remains buffered.\n- Step $4$: $P_1$: $turn \\leftarrow 0$ (visible). The earlier $flag[1]$ store remains buffered.\n- Step $5$: $P_0$: loads $flag[1]$ and reads $\\text{false}$ (since $flag[1]$ is still buffered in $P_1$), evaluates the condition $(flag[1] \\land (turn = 1))$ as $\\text{false}$, and proceeds into the critical section.\n- Step $6$: $P_1$: loads $flag[0]$ and reads $\\text{false}$ (since $flag[0]$ is still buffered in $P_0$), evaluates the condition $(flag[0] \\land (turn = 0))$ as $\\text{false}$, and proceeds into the critical section.\n- Result: both $P_0$ and $P_1$ are in the critical section concurrently.\n\nB. Interleaving that ties on $flag$ but is broken by $turn$:\n- Step $1$: $P_0$: $flag[0] \\leftarrow \\text{true}$ (visible).\n- Step $2$: $P_1$: $flag[1] \\leftarrow \\text{true}$ (visible).\n- Step $3$: $P_0$: $turn \\leftarrow 1$ (visible).\n- Step $4$: $P_1$: $turn \\leftarrow 0$ (visible).\n- Step $5$: $P_0$: loads $flag[1]$ as $\\text{true}$ and $turn$ as $0$, evaluates $(\\text{true} \\land (0 = 1)) = \\text{false}$, and enters the critical section, while $P_1$ spins.\n- Result: only one process enters the critical section.\n\nC. Interleaving that assumes write–write reordering to the same observer:\n- Step $1$: $P_0$: $flag[0] \\leftarrow \\text{true}$ (buffered).\n- Step $2$: $P_0$: $turn \\leftarrow 1$ (visible to $P_1$ before $flag[0]$).\n- Step $3$: $P_1$: loads $flag[0]$ as $\\text{true}$ (even though the $flag[0]$ store is still buffered in $P_0$) and spins.\n- Result: $P_1$ waits and mutual exclusion is preserved.\n\nD. Interleaving that deadlocks both processes in the entry loop:\n- Step $1$: $P_0$ and $P_1$ both make their $flag$ stores visible.\n- Step $2$: Both set $turn$ to the other’s identifier.\n- Step $3$: Both read $flag[\\cdot]$ as $\\text{true}$ and simultaneously read $turn$ as equal to the other’s identifier, so both conditions are $\\text{true}$, and both spin forever.\n- Result: both processes are stuck in the entry loop indefinitely.\n\nWhich option correctly describes a feasible counterexample interleaving under the stated model that breaks mutual exclusion after removing the single fence?\n\n- A\n- B\n- C\n- D", "solution": "This problem demonstrates why memory fences are essential for correctness on modern processors with weak memory models, such as those with store buffering. Without the memory fence, a process's write operations can be reordered from the perspective of another process.\n\nThe key vulnerability lies in the fact that a store to memory (e.g., $flag[0] \\leftarrow \\text{true}$) is first placed in the writing core's local store buffer. It only becomes visible to other cores after an unpredictable delay. Without a fence, the processor is allowed to execute subsequent instructions, including loads from other memory addresses, before the buffered store is made globally visible.\n\nLet's analyze option A, which presents a valid counterexample that violates mutual exclusion:\n1.  **$P_0$: $flag[0] \\leftarrow \\text{true}$ (buffered).** The write is in $P_0$'s store buffer. To $P_1$, $flag[0]$ still appears $\\text{false}$ in main memory.\n2.  **$P_1$: $flag[1] \\leftarrow \\text{true}$ (buffered).** Similarly, this write is in $P_1$'s store buffer. To $P_0$, $flag[1]$ is still $\\text{false}$.\n3.  **$P_0$ checks its wait condition.** $P_0$ executes `while ($flag[1] \\land (turn = 1)$)`. It needs to load the value of `flag[1]`. Since $P_1$'s write is still buffered, $P_0$ reads the old value `false` from main memory. The condition $(\\text{false} \\land \\dots)$ is immediately $\\text{false}$, so $P_0$ bypasses the loop and enters the critical section.\n4.  **$P_1$ checks its wait condition.** $P_1$ executes `while ($flag[0] \\land (turn = 0)$)`. It loads the value of `flag[0]`. Since $P_0$'s write is still buffered, $P_1$ also reads the old value `false` from main memory. The condition $(\\text{false} \\land \\dots)$ is `false`, so $P_1$ also enters the critical section.\n\nThe result is that both processes are in the critical section simultaneously. The writes to `turn` are irrelevant in this specific interleaving because the check on the `flag` variable is sufficient to break the lock.\n\nA memory fence placed after $flag[i] \\leftarrow \\text{true}$ would have prevented this. It would force the processor to drain its store buffer and make the `flag` write visible to all other cores *before* proceeding to check the wait condition. This ensures that if one process is about to check the loop condition, the other process's intent to enter (`flag` being `true`) is already visible.\n\nThe other options are incorrect:\n-   B describes a correct execution where mutual exclusion is preserved.\n-   C makes an impossible assumption that a buffered write is visible to another core.\n-   D incorrectly claims a deadlock occurs, whereas Peterson's algorithm is deadlock-free; it describes a liveness failure, not a mutual exclusion failure.", "answer": "$$\\boxed{A}$$", "id": "3687333"}]}