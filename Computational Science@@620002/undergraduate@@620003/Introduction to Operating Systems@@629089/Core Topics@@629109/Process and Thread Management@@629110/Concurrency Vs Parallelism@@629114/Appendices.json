{"hands_on_practices": [{"introduction": "Adding more processors to a task does not always result in a proportional decrease in execution time. This exercise introduces Amdahl's Law, a foundational principle that quantifies the maximum expected speedup from parallelization. By calculating the impact of an inherently serial critical section, you will see how even a small non-parallelizable part of a program can severely limit the benefits of adding more cores, clarifying the crucial difference between high concurrency and effective parallelism [@problem_id:3626997].", "problem": "A server application runs on a multicore Central Processing Unit (CPU) under an Operating System (OS) scheduler. The application spawns $N$ worker threads that process client requests. Each request consists of two phases: a local computation phase that does not access any shared state, followed by a critical section that updates a shared log protected by a mutual exclusion lock. Because the lock allows only one thread to execute the critical section at a time, the critical section is inherently serial even in the presence of many runnable threads. Assume the OS scheduler yields many interleavings among the $N$ threads, so concurrency (interleavings) is high, but the critical section dominates request time.\n\nSuppose the measured time per request on a single core is as follows: the local computation phase takes $c = 3$ milliseconds, and the critical section takes $s = 7$ milliseconds. Assume ideal parallel scaling for the local computation phase across $M$ identical cores, no overheads from synchronization beyond the serialization of the critical section itself, and that the critical section does not benefit from additional cores.\n\nStarting from the foundational definitions that speedup $S(M)$ is the ratio $T(1)/T(M)$, that the parallelizable fraction $p$ of the single-core execution time is the portion of work that can run concurrently across multiple cores, and that the serial fraction $1 - p$ does not accelerate with more cores, derive the expression for $S(M)$ in terms of $p$ and $M$. Then, compute the parallelizable fraction $p$ for this workload from $c$ and $s$, and evaluate the speedup for $M \\in \\{1, 2, 8\\}$. Express your final answer as a row matrix containing the three speedups corresponding to $M = 1$, $M = 2$, and $M = 8$. Do not include any units in your final numerical values.", "solution": "The problem requires the derivation of the speedup formula known as Amdahl's Law and its application to a specific workload on a multi-core processor.\n\nFirst, we perform the problem validation.\n\n**Step 1: Extract Givens**\n- Application spawns $N$ worker threads.\n- Each request has a local computation phase and a critical section phase.\n- Critical section is protected by a mutual exclusion lock, making it inherently serial.\n- Time for local computation per request on a single core: $c = 3$ milliseconds.\n- Time for critical section per request on a single core: $s = 7$ milliseconds.\n- Ideal parallel scaling is assumed for the local computation phase across $M$ cores.\n- The critical section does not benefit from additional cores.\n- No synchronization overheads beyond serialization.\n- Speedup definition: $S(M) = T(1)/T(M)$, where $T(M)$ is the execution time on $M$ cores.\n- Parallelizable fraction of single-core execution time: $p$.\n- Serial fraction of single-core execution time: $1-p$.\n- Task: Derive the expression for $S(M)$ in terms of $p$ and $M$.\n- Task: Compute $p$ from $c$ and $s$.\n- Task: Evaluate $S(M)$ for $M \\in \\{1, 2, 8\\}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is a classic application of Amdahl's Law, a fundamental principle in parallel computing used to predict the theoretical speedup when using multiple processors. All concepts (critical section, parallel fraction, speedup) are standard in computer science and operating systems.\n- **Well-Posed**: The problem provides all necessary definitions and data ($c$, $s$) to derive the formula and calculate the required values. The objectives are stated clearly. A unique solution exists.\n- **Objective**: The problem is stated in precise, technical language, free from subjective or ambiguous terms.\n\nThe problem is valid as it is scientifically sound, well-posed, and objective. We may proceed with the solution.\n\n**Derivation of Speedup $S(M)$**\n\nLet $T(1)$ be the total execution time of a task on a single processor core. This total time can be partitioned into a serial component, which cannot be parallelized, and a parallelizable component.\n\nThe serial fraction is given as $1-p$. The time spent on the serial part is therefore $(1-p)T(1)$. This time is constant regardless of the number of cores $M$.\n\nThe parallelizable fraction is given as $p$. The time spent on the parallelizable part on a single core is $pT(1)$. With ideal parallel scaling, this work can be distributed across $M$ cores, reducing the time for this portion to $\\frac{pT(1)}{M}$.\n\nThe total execution time on $M$ cores, $T(M)$, is the sum of the time for the serial part and the time for the parallelized part:\n$$T(M) = (1-p)T(1) + \\frac{pT(1)}{M}$$\nWe can factor out $T(1)$:\n$$T(M) = T(1) \\left( (1-p) + \\frac{p}{M} \\right)$$\nThe speedup $S(M)$ is defined as the ratio of the single-core execution time to the multi-core execution time:\n$$S(M) = \\frac{T(1)}{T(M)}$$\nSubstituting the expression for $T(M)$:\n$$S(M) = \\frac{T(1)}{T(1) \\left( (1-p) + \\frac{p}{M} \\right)}$$\n$$S(M) = \\frac{1}{(1-p) + \\frac{p}{M}}$$\nThis is the general expression for speedup in terms of the parallelizable fraction $p$ and the number of processors $M$, known as Amdahl's Law.\n\n**Calculation of the Parallelizable Fraction $p$**\n\nFor the given workload, the total execution time for a single request on a single core, $T(1)$, is the sum of the local computation time $c$ and the critical section time $s$.\n$$T(1) = c + s$$\nThe local computation is the part of the work that can be run in parallel. Thus, its duration $c$ corresponds to the parallelizable portion of the single-core execution time. The critical section is inherently serial, so its duration $s$ corresponds to the serial portion.\n\nThe parallelizable fraction $p$ is the ratio of the parallelizable time to the total time:\n$$p = \\frac{c}{c+s}$$\nSubstituting the given values $c=3$ and $s=7$:\n$$p = \\frac{3}{3+7} = \\frac{3}{10} = 0.3$$\nThe serial fraction is $1-p = 1 - 0.3 = 0.7$. This corresponds to $\\frac{s}{c+s} = \\frac{7}{10} = 0.7$, which is consistent.\n\n**Evaluation of Speedup for $M \\in \\{1, 2, 8\\}$**\n\nNow we use the derived speedup formula with $p=0.3$:\n$$S(M) = \\frac{1}{(1-0.3) + \\frac{0.3}{M}} = \\frac{1}{0.7 + \\frac{0.3}{M}}$$\n\nCase 1: $M=1$\n$$S(1) = \\frac{1}{0.7 + \\frac{0.3}{1}} = \\frac{1}{0.7 + 0.3} = \\frac{1}{1} = 1$$\nThis is the expected baseline speedup.\n\nCase 2: $M=2$\n$$S(2) = \\frac{1}{0.7 + \\frac{0.3}{2}} = \\frac{1}{0.7 + 0.15} = \\frac{1}{0.85}$$\nTo express this as an exact fraction:\n$$S(2) = \\frac{1}{\\frac{85}{100}} = \\frac{100}{85} = \\frac{20}{17}$$\n\nCase 3: $M=8$\n$$S(8) = \\frac{1}{0.7 + \\frac{0.3}{8}}$$\nTo express this as an exact fraction, we convert the denominator to a common base:\n$$0.7 = \\frac{7}{10} = \\frac{56}{80}$$\n$$\\frac{0.3}{8} = \\frac{\\frac{3}{10}}{8} = \\frac{3}{80}$$\nSo, the denominator is:\n$$\\frac{56}{80} + \\frac{3}{80} = \\frac{59}{80}$$\nTherefore, the speedup is:\n$$S(8) = \\frac{1}{\\frac{59}{80}} = \\frac{80}{59}$$\n\nThe three computed speedup values are $S(1)=1$, $S(2)=\\frac{20}{17}$, and $S(8)=\\frac{80}{59}$.", "answer": "$$\\boxed{\\begin{pmatrix} 1 & \\frac{20}{17} & \\frac{80}{59} \\end{pmatrix}}$$", "id": "3626997"}, {"introduction": "Real-world server applications often handle tasks that mix computation with blocking operations like network or disk I/O. This practice explores the critical design decision of sizing a thread pool to balance two distinct goals. You will derive a rule for choosing a pool size $k$ that provides enough threads for *parallelism* to keep CPU cores saturated, while also providing enough threads for *concurrency* to hide I/O latency and meet service-level targets [@problem_id:3627021].", "problem": "A server uses a fixed-size thread pool to handle independent client requests. Each request alternates between compute phases on the Central Processing Unit (CPU) and blocking Input/Output (I/O) waits. Let $C$ denote the mean CPU compute time per request and $W$ the mean blocking time per request, with $C, W > 0$, and model each thread as cycling between compute (busy on a core) and wait (blocked on I/O). The machine has $M$ identical CPU cores available to execute compute-ready threads. Requests arrive at an average rate $\\lambda$ requests per second. The system operator targets a steady-state average response time no greater than $R^\\star$ seconds. The thread pool size is $k$ threads.\n\nFrom first principles, propose a rule for choosing $k$ that simultaneously captures:\n- the concurrency requirement to cover I/O latency (so that enough requests can be in-flight to achieve the target average response time), and\n- the parallelism limit imposed by the $M$ cores (so that compute throughput is not starved, nor cores left idle due to blocking).\n\nSelect the single best option.\n\nA. Choose $k = \\lceil M \\rceil$.\n\nB. Choose $k = \\left\\lceil M \\cdot \\left(1 + \\frac{W}{C}\\right) \\right\\rceil$.\n\nC. Choose $k = \\left\\lceil \\max\\!\\left( M \\cdot \\left(1 + \\frac{W}{C}\\right),\\, \\lambda \\cdot R^\\star \\right) \\right\\rceil$.\n\nD. Choose $k = \\left\\lceil \\lambda \\cdot (C + W) \\right\\rceil$.\n\nE. Choose $k = \\left\\lceil \\frac{M}{1 + \\frac{W}{C}} \\right\\rceil$.", "solution": "The user wants me to solve a problem about sizing a thread pool.\n\n### Step 1: Validate the Problem Statement\n\nI will begin by validating the problem statement as per the instructions.\n\n#### Extract Givens\n- A server uses a fixed-size thread pool of size $k$.\n- The server handles independent client requests.\n- Each request alternates between compute phases (CPU) and blocking Input/Output (I/O) waits.\n- $C$: mean CPU compute time per request, with $C > 0$.\n- $W$: mean blocking I/O wait time per request, with $W > 0$.\n- The system has $M$ identical CPU cores.\n- $\\lambda$: average arrival rate of requests (requests per second).\n- $R^\\star$: target steady-state average response time (seconds).\n\n#### Validate Using Extracted Givens\nThe problem asks for a rule to choose the thread pool size $k$ to satisfy two requirements:\n1.  A concurrency requirement to cover I/O latency and meet the response time target.\n2.  A parallelism limit to effectively utilize the $M$ CPU cores.\n\nThe problem statement is scientifically grounded, well-posed, and objective. It describes a classic and fundamental problem in computer systems performance modeling. The parameters ($C$, $W$, $M$, $\\lambda$, $R^\\star$) are standard metrics used in queuing theory and system analysis. The model, which uses mean values for CPU and I/O time, is a common and valid simplification for first-order analysis. The problem is not ambiguous, contradictory, or based on false premises. It requires the derivation of a sizing heuristic from first principles, which is a standard engineering task.\n\n#### Verdict\nThe problem statement is **valid**. I will proceed with the derivation and solution.\n\n### Step 2: Derivation from First Principles\n\nThe problem requires a rule for determining the thread pool size, $k$, that simultaneously addresses two distinct constraints: one related to hardware utilization (parallelism) and one related to service level objectives (concurrency).\n\n**1. Parallelism Limit: Saturating the CPU Cores**\n\nThe first goal is to choose $k$ such that the $M$ CPU cores are kept busy and not left idle due to threads blocking on I/O.\n- Let's consider a single thread. In one complete cycle of its work on a request, it spends $C$ time units in a runnable state (either computing or ready to compute) and $W$ time units in a blocked state (waiting for I/O).\n- The total time for one cycle is $C+W$.\n- The fraction of time a thread is runnable (i.e., requires a CPU core) is $f_{cpu} = \\frac{C}{C+W}$.\n- The fraction of time a thread is blocked on I/O is $f_{io} = \\frac{W}{C+W}$.\n\nWith a pool of $k$ threads, the average number of threads that are in the runnable state at any given time is $k \\cdot f_{cpu} = k \\cdot \\frac{C}{C+W}$.\nTo fully saturate all $M$ cores, we need the number of runnable threads to be at least equal to the number of cores, $M$. The ideal point to aim for is where the number of runnable threads is exactly $M$.\n$$\nk \\cdot \\frac{C}{C+W} = M\n$$\nSolving for $k$ gives the number of threads required to keep the CPUs saturated:\n$$\nk_{cpu} = M \\cdot \\frac{C+W}{C} = M \\cdot \\left(1 + \\frac{W}{C}\\right)\n$$\nThis formula, often known as a utilization-based sizing rule, provides the number of threads needed to ensure that, on average, there are exactly $M$ threads ready to run, thus keeping the $M$ cores fully utilized. Any fewer threads, and cores would be idle whenever more than $k-M$ threads become blocked. This formula captures the parallelism limit.\n\n**2. Concurrency Requirement: Meeting the Response Time Target**\n\nThe second goal is to have enough threads to handle the incoming workload and achieve an average response time of $R^\\star$ or less.\n- This can be analyzed using Little's Law, a fundamental theorem in queuing theory, which states that the long-term average number of customers in a stable system, $L$, is equal to the long-term average effective arrival rate, $\\lambda$, multiplied by the average time a customer spends in the system, $R$.\n$$\nL = \\lambda \\cdot R\n$$\n- In our context, \"customers\" are the client requests. The \"system\" is the server. The time a request spends in the system is its response time.\n- The system must support an arrival rate of $\\lambda$ and aims for an average response time of $R^\\star$.\n- Applying Little's Law, the average number of concurrent requests in the system to meet this target is:\n$$\nL^\\star = \\lambda \\cdot R^\\star\n$$\n- Each request that is \"in-flight\" (being processed by the server) requires a thread from the pool. Therefore, to support an average of $L^\\star$ concurrent requests, the thread pool must have at least that many threads.\n$$\nk_{latency} \\ge \\lambda \\cdot R^\\star\n$$\nThis formula captures the concurrency requirement driven by the service level objective.\n\n**3. Combining the Requirements**\n\nThe problem asks for a single rule that captures *both* requirements. The thread pool must be large enough to satisfy the stricter of the two constraints.\n- It must be large enough to keep the CPUs busy: $k \\ge M \\cdot \\left(1 + \\frac{W}{C}\\right)$.\n- It must be large enough to support the required concurrency for the latency target: $k \\ge \\lambda \\cdot R^\\star$.\n\nTo satisfy both conditions simultaneously, $k$ must be greater than or equal to the maximum of the two required values:\n$$\nk \\ge \\max\\left( M \\cdot \\left(1 + \\frac{W}{C}\\right),\\, \\lambda \\cdot R^\\star \\right)\n$$\nSince the thread pool size $k$ must be an integer, the smallest integer value for $k$ that satisfies this condition is the ceiling of the expression.\n$$\nk = \\left\\lceil \\max\\left( M \\cdot \\left(1 + \\frac{W}{C}\\right),\\, \\lambda \\cdot R^\\star \\right) \\right\\rceil\n$$\nThis composite rule ensures the system is provisioned to handle both the hardware-level parallelism and the application-level concurrency targets.\n\n### Step 3: Option-by-Option Analysis\n\nNow, I will evaluate each option based on the derived principles.\n\n**A. Choose $k = \\lceil M \\rceil$.**\nAs $M$ (the number of cores) is an integer, this simplifies to $k=M$. This rule sets the number of threads equal to the number of available cores. This is an appropriate strategy only for workloads that are purely CPU-bound (i.e., $W=0$). For the given problem, where requests have blocking I/O waits ($W > 0$), if all $M$ threads block on I/O simultaneously, the CPU cores will become idle. This fails to satisfy the requirement of keeping the cores busy. Therefore, this option is insufficient.\n**Verdict: Incorrect.**\n\n**B. Choose $k = \\left\\lceil M \\cdot \\left(1 + \\frac{W}{C}\\right) \\right\\rceil$.**\nThis formula corresponds exactly to the $k_{cpu}$ derived to keep the $M$ cores saturated. It correctly addresses the parallelism limit. However, it completely ignores the second requirement concerning the arrival rate $\\lambda$ and the target response time $R^\\star$. If the concurrency level required to meet the service target, $\\lambda \\cdot R^\\star$, is significantly larger than $M \\cdot (1 + W/C)$, this rule would lead to a thread pool that is too small, causing requests to be rejected or to queue for excessively long times before being assigned a thread, thus failing to meet the response time target. As it only captures one of the two requirements, it is incomplete.\n**Verdict: Incorrect.**\n\n**C. Choose $k = \\left\\lceil \\max\\!\\left( M \\cdot \\left(1 + \\frac{W}{C}\\right),\\, \\lambda \\cdot R^\\star \\right) \\right\\rceil$.**\nThis formula incorporates both lower bounds that we derived from first principles. The term $M \\cdot (1 + W/C)$ ensures the pool is large enough to keep the hardware utilized. The term $\\lambda \\cdot R^\\star$ ensures the pool is large enough to support the number of concurrent requests needed to meet the performance target. By taking the maximum of these two terms, the rule guarantees that the chosen pool size $k$ is sufficient to meet both the hardware-centric requirement and the service-level-centric requirement. This rule robustly and simultaneously captures both aspects of the problem.\n**Verdict: Correct.**\n\n**D. Choose $k = \\left\\lceil \\lambda \\cdot (C + W) \\right\\rceil$.**\nThe term $C+W$ represents the total ideal service time of a request, excluding any queuing delays. The expression $\\lambda \\cdot (C+W)$ is an application of Little's Law to find the number of requests in the system if the response time were exactly equal to the service time. This is the definition of the system's offered load. While this value is related to the required concurrency, it completely ignores the number of cores $M$, failing to address the parallelism limit. Furthermore, it assumes the target response time $R^\\star$ is equal to the ideal service time $C+W$, which is generally not the case, as $R^\\star$ must typically account for queuing delays.\n**Verdict: Incorrect.**\n\n**E. Choose $k = \\left\\lceil \\frac{M}{1 + \\frac{W}{C}} \\right\\rceil$.**\nThis formula can be rewritten as $k = \\lceil M \\cdot \\frac{C}{C+W} \\rceil$. Since $C, W > 0$, the fraction $\\frac{C}{C+W}$ is always less than $1$. Therefore, this rule would suggest a thread pool size $k \\le M$. If there are fewer threads than cores, it is impossible to ever fully utilize all $M$ cores. This directly contradicts the goal of saturating the CPUs and is conceptually the inverse of the correct logic for handling I/O-bound tasks.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{C}$$", "id": "3627021"}, {"introduction": "A key skill for a systems programmer is diagnosing performance issues in concurrent software. This exercise presents a common yet counterintuitive scenario: adding more threads and running on more cores fails to improve performance. By analyzing this thought experiment, you will apply your understanding of serial bottlenecks to diagnose why parallelism is present but ineffective, reinforcing the critical link between shared-resource contention and the limits of parallel speedup [@problem_id:3627024].", "problem": "A software team runs two independent data-processing tasks, Task $T_1$ and Task $T_2$, inside a single application on a dual-core machine with $2$ identical Central Processing Unit (CPU) cores. Each task processes $K$ records and uses a pool of $N$ worker threads. Processing each record consists of two steps in order: first, a compute step that is perfectly parallelizable and takes $C$ milliseconds of pure CPU work; second, a logging step that calls into a shared third-party library routine that is guarded by a single process-wide mutual exclusion (mutex) lock $M$ and takes $L$ milliseconds of pure CPU work while holding $M$. Both tasks link against the same library and therefore contend on the same lock $M$ when logging. Assume $C \\ll L$, and that the Operating System (OS) scheduler is otherwise fair and efficient.\n\nIn experiments, as $N$ is increased from $1$ to larger values (e.g., $N=8$), the total wall-clock time to complete both tasks concurrently on the dual-core system remains approximately the same as when $N=1$. The team concludes that parallelism is present but ineffective.\n\nWhich option best explains the observation from first principles and predicts the effect of replacing the single shared lock $M$ with two disjoint locks, one used exclusively by Task $T_1$ and one used exclusively by Task $T_2$?\n\nA. The threads are not actually parallel: the OS cannot run threads from different tasks at the same time when a mutex is present. Replacing the shared lock with two locks will not help because mutexes inherently prevent parallelism across cores.\n\nB. There is parallelism: compute steps from different threads can execute simultaneously on the $2$ cores, but the single shared lock $M$ makes every logging step a single-file serial resource for all threads across both tasks. With $C \\ll L$, the serialized logging dominates, so speedup on $2$ cores vanishes. Giving each task its own lock removes cross-task contention, allowing two logging streams to proceed concurrently, restoring near $2\\times$ speedup on the dual-core for large $L$.\n\nC. The lack of speedup is caused primarily by context switching overhead that grows with $N$. Pinning all threads to a single core will reduce context switches and restore near $2\\times$ speedup on the dual-core.\n\nD. The compute step saturates memory bandwidth; a mutex cannot explain the scaling. Replacing the shared lock with per-task locks will not change performance, but upgrading to $4$ cores will guarantee near $4\\times$ speedup even if the single shared lock remains.", "solution": "### Step 1: Extract Givens\n- A dual-core machine with $2$ identical Central Processing Unit (CPU) cores.\n- Two independent data-processing tasks, Task $T_1$ and Task $T_2$.\n- Each task processes $K$ records.\n- Each task uses a pool of $N$ worker threads. The total number of threads is $2N$.\n- Processing a single record involves two sequential steps:\n    1.  A compute step of duration $C$ milliseconds, which is perfectly parallelizable and pure CPU work.\n    2.  A logging step of duration $L$ milliseconds, which is pure CPU work and requires acquiring a single process-wide mutual exclusion (mutex) lock $M$.\n- Both Task $T_1$ and Task $T_2$ contend for the same lock $M$.\n- An essential condition is given: $C \\ll L$.\n- An experimental observation is provided: as $N$ is increased from $1$ to larger values (e.g., $N=8$), the total wall-clock time to complete both tasks concurrently remains approximately constant.\n- The question asks for an explanation of this observation and a prediction of the effect of replacing the single lock $M$ with two disjoint locks, one for each task.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is assessed based on the predefined criteria.\n\n- **Scientifically Grounded**: The problem is well-grounded in the fundamental principles of operating systems, specifically concurrency, parallelism, and synchronization primitives like mutexes. The scenario described is a classic illustration of Amdahl's Law, a core concept in parallel computing. No scientific or factual unsoundness is present.\n- **Well-Posed**: The problem is clearly defined. The system (hardware, software tasks, workload), the constraints (shared lock, relative costs of compute vs. logging), and the observation are all specified. The question asks for a specific explanation and prediction, for which a logical, unique solution can be derived from the premises.\n- **Objective**: The language is precise and objective, using standard computer science terminology. It is free from subjective or ambiguous statements.\n\nThe problem does not exhibit any of the invalidity flaws:\n1.  **Scientific or Factual Unsoundness**: None. The scenario is a standard performance bottleneck problem.\n2.  **Non-Formalizable or Irrelevant**: None. The problem is directly formalizable and relevant to its stated topic.\n3.  **Incomplete or Contradictory Setup**: None. The information provided is sufficient and consistent. The condition $C \\ll L$ is key to the analysis.\n4.  **Unrealistic or Infeasible**: None. This is a highly realistic scenario in software engineering where a shared, slow logging facility becomes a major performance bottleneck.\n5.  **Ill-Posed or Poorly Structured**: None. A unique conceptual solution exists.\n6.  **Pseudo-Profound, Trivial, or Tautological**: None. The problem requires a correct application of first principles (Amdahl's law) to diagnose a non-obvious performance issue.\n7.  **Outside Scientific Verifiability**: None. The principles are well-established and the scenario is testable.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. Proceeding with the solution.\n\n### Principle-Based Derivation\n\nThe analysis is based on Amdahl's Law, which models the theoretical speedup in latency of the execution of a task at fixed workload that can be expected of a system whose resources are improved.\n\nLet's analyze the total work and its components.\n-   Total number of records to process: $2K$ (K for $T_1$ and $K$ for $T_2$).\n-   For each record, there is a compute part of duration $C$ and a logging part of duration $L$.\n-   Total CPU work for computation: $2K \\times C$. This work is \"perfectly parallelizable.\"\n-   Total CPU work for logging: $2K \\times L$. This work is serialized by the single process-wide mutex lock $M$. Only one thread in the entire application can be executing a logging step at any given moment.\n\n**Analysis of the Initial Scenario (Single Shared Lock $M$)**\n\nThe total time to complete the tasks is limited by the portion of the work that cannot be parallelized.\nThe serial portion of the workload consists of all the logging steps. Since they must be executed one after another due to the single lock $M$, the minimum time this part can take is the sum of all logging durations:\n$$T_{serial} = 2K \\times L$$\nThe parallel portion of the workload consists of all the compute steps. The total work is $2K \\times C$. With $2$ cores, this work can ideally be done in a time of:\n$$T_{parallel} = \\frac{2K \\times C}{2} = K \\times C$$\nThe total wall-clock time, $T_{total}$, is determined by how these two types of work overlap. The serial logging work forms a hard bottleneck. While one core is busy with a logging step (for a duration $L$), the other core can perform compute steps. Since we are given $C \\ll L$, one logging step takes much longer than one compute step. In the time it takes to perform one logging step of duration $L$ on one core, the other core could complete $L/C$ compute steps. Given a large number of threads ($N > 1$), it is highly likely that there will always be a thread ready to do a compute step on the free core. However, the overall progress is gated by the serial execution of the $2K$ logging operations.\nThe total execution time will be dominated by the serial part. The total time can be approximated as the time for the serial bottleneck plus the time for any parallel work that cannot be hidden behind the serial work.\nA simple and robust model is to consider the total time as approximately the sum of all serial steps:\n$$T_{total} \\approx T_{serial} = 2KL$$\nThe parallel compute work ($2KC$ total) is small compared to the serial logging work ($2KL$ total) because $C \\ll L$. Most of the compute work can be performed on one core while the other core is occupied by a logging step. Therefore, the total time is overwhelmingly dominated by the serial logging queue.\n\nThis model explains the observation: increasing the number of threads $N$ from $1$ to $8$ does not decrease the total time. With $N \\ge 1$, we already have at least $2$ threads in total (one for each task). As long as there are at least as many active threads as cores (i.e., $2N \\ge 2$), adding more threads just increases the number of threads waiting for the single lock $M$. It does not reduce the length of the serial bottleneck, which remains $2KL$. The execution time stays roughly constant, limited by this bottleneck.\n\n**Analysis of the Proposed Change (Two Disjoint Locks)**\n\nThe change involves replacing the single lock $M$ with two locks: $M_1$ exclusively for Task $T_1$ and $M_2$ exclusively for Task $T_2$.\n-   Task $T_1$'s $N$ threads now contend only for lock $M_1$. The $K$ logging steps for $T_1$ are serialized with respect to each other.\n-   Task $T_2$'s $N$ threads now contend only for lock $M_2$. The $K$ logging steps for $T_2$ are serialized with respect to each other.\n\nCrucially, a thread from $T_1$ holding $M_1$ does not prevent a thread from $T_2$ from holding $M_2$. Since the machine has $2$ cores, one core can execute a logging step for $T_1$ while the other core executes a logging step for $T_2$ in parallel.\nThe two tasks are now effectively independent and can run in parallel on the two cores. The total wall-clock time will be the time taken by the longer of the two tasks. Since they are identical, we only need to analyze one.\nFor Task $T_1$ running on its \"own\" core, the total work is $K$ records, each taking $C+L$. The logging part is serialized, taking a total time of $KL$. The compute part is parallelizable, but since it's all running on a single effective stream of execution (bottlenecked by its own lock), the total time for Task $T_1$ will be approximately the sum of all its work:\n$$T'_{T_1} \\approx K \\times C + K \\times L = K(C+L)$$\nSimilarly for Task $T_2$:\n$$T'_{T_2} \\approx K(C+L)$$\nSince both tasks run in parallel on the $2$ cores, the new total wall-clock time, $T'_{total}$, is:\n$$T'_{total} = \\max(T'_{T_1}, T'_{T_2}) \\approx K(C+L)$$\nGiven the condition $C \\ll L$, we can approximate this as:\n$$T'_{total} \\approx KL$$\nLet's calculate the speedup:\n$$Speedup = \\frac{T_{total}}{T'_{total}} \\approx \\frac{2KL}{KL} = 2$$\nTherefore, replacing the single shared lock with two per-task locks is predicted to provide a speedup of approximately $2\\times$ on the dual-core system.\n\n### Option-by-Option Analysis\n\n-   **A. The threads are not actually parallel: the OS cannot run threads from different tasks at the same time when a mutex is present. Replacing the shared lock with two locks will not help because mutexes inherently prevent parallelism across cores.**\n    This statement contains multiple falsehoods. First, the compute steps *are* parallel. Second, an OS absolutely can run threads on different cores even if one of them holds a mutex; the mutex only prevents multiple threads from entering the *same* critical section. Third, using separate locks for separate data streams is a standard technique to *increase* parallelism.\n    **Verdict: Incorrect.**\n\n-   **B. There is parallelism: compute steps from different threads can execute simultaneously on the $2$ cores, but the single shared lock $M$ makes every logging step a single-file serial resource for all threads across both tasks. With $C \\ll L$, the serialized logging dominates, so speedup on $2$ cores vanishes. Giving each task its own lock removes cross-task contention, allowing two logging streams to proceed concurrently, restoring near $2\\times$ speedup on the dual-core for large $L$.**\n    This statement correctly identifies that parallelism exists in the compute step. It correctly diagnoses the problem: the single lock $M$ on the long-duration logging step ($L$) creates a serial bottleneck that dominates the total execution time, as per Amdahl's Law, thus negating the benefit of multiple cores. It correctly predicts that two disjoint locks will unblock the two tasks from each other, allowing their logging streams to run in parallel on the two cores, which would lead to a speedup approaching $2\\times$. This matches our derivation perfectly.\n    **Verdict: Correct.**\n\n-   **C. The lack of speedup is caused primarily by context switching overhead that grows with $N$. Pinning all threads to a single core will reduce context switches and restore near $2\\times$ speedup on the dual-core.**\n    While context-switching overhead is real, the primary cause of the bottleneck here is serialization due to the lock, as established by the $C \\ll L$ condition. The suggestion to pin all threads to a single core is counterproductive; it would completely eliminate any possibility of parallelism and would significantly slow down the execution, making it impossible to achieve a $2\\times$ speedup on a dual-core machine.\n    **Verdict: Incorrect.**\n\n-   **D. The compute step saturates memory bandwidth; a mutex cannot explain the scaling. Replacing the shared lock with per-task locks will not change performance, but upgrading to $4$ cores will guarantee near $4\\times$ speedup even if the single shared lock remains.**\n    This option makes an unsupported claim about memory bandwidth. A mutex that guards a long critical section is a textbook cause for poor scaling. The prediction that replacing the lock will not help is false, as shown in our analysis. The prediction that upgrading to $4$ cores will yield a $4\\times$ speedup while keeping the single serial lock is fundamentally wrong. If the process is bottlenecked by a serial section, adding more cores yields diminishing returns, and in this extreme case, will provide almost no speedup at all.\n    **Verdict: Incorrect.**", "answer": "$$\\boxed{B}$$", "id": "3627024"}]}