{"hands_on_practices": [{"introduction": "The inode is the cornerstone of many file systems, acting as a map to a file's data blocks. This exercise provides a fundamental understanding of how an inode's structure, with its combination of direct and indirect pointers, determines the theoretical maximum size of a file. By deriving the formula from first principles, you will gain a concrete appreciation for the design trade-offs involved in balancing metadata overhead and file size scalability. [@problem_id:3635998]", "problem": "Consider a block-addressed file system that uses an index node (inode) to describe a file. The inode contains $n$ direct pointers to data blocks, $a$ single-indirect pointers, and $b$ double-indirect pointers. The disk block size is $B$ bytes. Each block address pointer stored in an indirect block has fixed size $P$ bytes. Assume $B$ is an integer multiple of $P$ so that an indirect block stores exactly $r = B/P$ block addresses, and that indirect blocks contain no additional metadata beyond these addresses. A data block contributes its full $B$ bytes to the file’s content. All pointers refer to data blocks or to blocks of pointers, and there are no triple-indirect pointers.\n\nStarting from the fundamental definitions of direct and indirect pointers in block-addressed allocation—namely, that a direct pointer addresses one data block, a single-indirect pointer addresses one block that itself contains addresses to data blocks, and a double-indirect pointer addresses one block that contains addresses to blocks of addresses to data blocks—derive a closed-form expression for the maximum file size $S_{\\max}$, in bytes, afforded by this inode structure. In your derivation, justify the multiplicative factors from first principles about how many data blocks are reachable through each pointer type.\n\nThen, briefly discuss the practical limiting factors that may reduce the usable maximum file size below $S_{\\max}$ (for example, limits due to file offset width or total number of addressable blocks on the volume), without providing any numeric estimates.\n\nProvide the final answer for $S_{\\max}$ as a single closed-form analytic expression in terms of $n$, $a$, $b$, $B$, and $P$. Express your final result in bytes. No rounding is required.", "solution": "The goal is to compute the maximum number of data blocks that can be reached through the inode’s pointer structure and then multiply that count by the block size $B$ to obtain the maximum file size $S_{\\max}$ in bytes. The derivation proceeds from the fundamental definitions of direct and indirect pointers and the geometry of how many addresses fit in an indirect block.\n\nFundamental facts and definitions to use:\n- A direct pointer references exactly one data block; that data block contributes $B$ bytes to the file.\n- A single-indirect pointer references one indirect block. An indirect block stores block addresses, each of which references one data block. If each address is $P$ bytes and the block is $B$ bytes, and $B$ is an integer multiple of $P$, then the number of addresses that fit in the indirect block is\n$$\nr = \\frac{B}{P}.\n$$\nTherefore, a single-indirect pointer can reference $r$ distinct data blocks, each contributing $B$ bytes.\n- A double-indirect pointer references one block of pointers to indirect blocks (that is, a second level of indirection). The first-level indirect block (pointed to by the double-indirect pointer) has $r$ addresses, and each of those addresses references a second-level indirect block that itself contains $r$ addresses to data blocks. Consequently, each double-indirect pointer can reference\n$$\nr \\times r = r^{2}\n$$\ndistinct data blocks, each contributing $B$ bytes.\n\nUsing these definitions, we count the total number of data blocks addressable by the inode:\n- The $n$ direct pointers contribute $n$ data blocks.\n- The $a$ single-indirect pointers contribute $a \\cdot r$ data blocks.\n- The $b$ double-indirect pointers contribute $b \\cdot r^{2}$ data blocks.\n\nSumming these contributions gives the total number of data blocks reachable through the inode:\n$$\n\\text{Total data blocks} = n + a r + b r^{2}.\n$$\nEach data block contributes $B$ bytes to the file. Therefore, the maximum file size is\n$$\nS_{\\max} = B \\left( n + a r + b r^{2} \\right).\n$$\nSubstituting $r = \\frac{B}{P}$ yields a closed-form expression purely in terms of $n$, $a$, $b$, $B$, and $P$:\n$$\nS_{\\max} = B \\left( n + a \\left( \\frac{B}{P} \\right) + b \\left( \\frac{B}{P} \\right)^{2} \\right).\n$$\n\nDiscussion of practical limits (qualitative):\n- Finite file offset width: If file offsets are stored in $w$ bits (for example, $w = 32$ or $w = 64$), then addressing within a file cannot exceed $2^{w}$ bytes. Hence, a practical limit is $\\min\\!\\left(S_{\\max}, 2^{w}\\right)$.\n- Volume capacity and addressability: If the total number of addressable blocks on the volume is limited to $N$ blocks, then a single file cannot exceed $N B$ bytes. Real systems also impose constraints on how many blocks can be allocated simultaneously to one file.\n- Implementation overhead: Some file systems reserve a small amount of metadata space within indirect blocks or maintain additional structures (for example, checksums or block maps) that may reduce the effective number of pointers per indirect block. Our derivation assumes none of this overhead.\n- Allocation policies and fragmentation: While $S_{\\max}$ counts reachable capacity, actual allocation can be constrained by fragmentation or policies (for example, maximum file size parameters in the file system superblock), potentially reducing usable size.\n\nThe closed-form expression above captures the theoretical maximum reachable through the specified pointer structure under the stated assumptions; practical constraints may make the usable maximum smaller.", "answer": "$$\\boxed{B\\left(n + a\\left(\\frac{B}{P}\\right) + b\\left(\\frac{B}{P}\\right)^{2}\\right)}$$", "id": "3635998"}, {"introduction": "The physical layout of a file on a disk directly impacts read and write performance, a concept known as fragmentation. This practice moves from theory to empirical analysis, tasking you with modeling the performance of a hard disk drive (HDD) based on hypothetical measurements. By parameterizing a linear model for read time, you will quantify the distinct costs of mechanical seeks versus sequential data transfer, revealing the tangible performance penalty of fragmented files. [@problem_id:3636040]", "problem": "An Operating System (OS) that uses extent-based allocation on a Hard Disk Drive (HDD) stores each file as a sequence of contiguous regions called extents. Reading a file that spans multiple extents requires mechanical head positioning and rotational alignment before reading each extent, followed by a sequential transfer at the disk’s sustained rate. Use the following foundational facts to model the read time: the time to transfer a quantity of data of size $\\text{size}$ at a sustained sequential rate $R$ is $\\text{size}/R$, and each extent incurs an average positioning overhead $S$ due to seek and rotational latency. Let $m$ denote the number of extents that a file occupies. Assume that the positioning overhead applies to every extent of the file, including the first.\n\nYou are given three measurement results from reading three files on the same HDD under steady-state conditions:\n- File A: $m = 4$, $\\text{size} = 500$ MiB, measured $T_{\\text{read}} = 4.048$ seconds.\n- File B: $m = 10$, $\\text{size} = 250$ MiB, measured $T_{\\text{read}} = 2.120$ seconds.\n- File C: $m = 14$, $\\text{size} = 800$ MiB, measured $T_{\\text{read}} = 6.590$ seconds.\n\nTasks:\n- Starting only from the definitions above, derive a linear model for $T_{\\text{read}}$ in terms of $m$, $\\text{size}$, $S$, and $R$.\n- Use Files A and B to parameterize the model by solving for $S$ and $R$.\n- Use your parameterized model to predict $T_{\\text{read}}$ for File C, then compare this prediction to the measured $T_{\\text{read}}$ for File C and compute the absolute error.\n\nRound your final absolute error to four significant figures. Express the final error in seconds.", "solution": "The problem statement is critically validated and found to be valid. It is scientifically grounded, employing a standard simplified model for HDD read performance. It is well-posed, providing sufficient data to determine the model parameters and verify the model's predictive accuracy. The provided data are internally consistent and physically realistic.\n\nThe problem asks for the derivation of a linear model for file read time, its parameterization using experimental data, and a verification of the model against a third data point.\n\nFirst, we derive the model for the total read time, $T_{\\text{read}}$. The process of reading a file is described as a sequence of two distinct operations for each extent: mechanical positioning and data transfer.\nThe total time is the sum of the time spent on all positioning overheads and the time spent on transferring the data.\nA file is stored in $m$ extents. For each extent, an average positioning overhead $S$ (due to seek and rotational latency) is incurred. As the overhead applies to every extent, the total time for positioning is the product of the number of extents and the per-extent overhead:\n$$T_{\\text{positioning}} = mS$$\nThe file has a total size of $\\text{size}$. The data is transferred at a sustained sequential rate $R$. The total time for transferring the data is the ratio of the total size to the transfer rate:\n$$T_{\\text{transfer}} = \\frac{\\text{size}}{R}$$\nThe total read time $T_{\\text{read}}$ is the sum of these two components:\n$$T_{\\text{read}}(m, \\text{size}) = mS + \\frac{\\text{size}}{R}$$\nThis equation is a linear model in the variables $m$ and $\\text{size}$, with parameters $S$ and $R$ to be determined.\n\nNext, we parameterize this model by determining the values of $S$ and $R$ using the data from File A and File B. All sizes are given in MiB and times in seconds, so we will determine $R$ in MiB/s and $S$ in seconds.\n\nFor File A: $m_A = 4$, $\\text{size}_A = 500$ MiB, $T_{\\text{read}, A} = 4.048$ s.\n$$4.048 = 4S + \\frac{500}{R} \\quad (1)$$\nFor File B: $m_B = 10$, $\\text{size}_B = 250$ MiB, $T_{\\text{read}, B} = 2.120$ s.\n$$2.120 = 10S + \\frac{250}{R} \\quad (2)$$\nWe have a system of two linear equations in two unknowns, $S$ and $\\frac{1}{R}$. To solve this system, we can use the method of elimination. Let's multiply equation $(2)$ by $2$ to make the term with $\\frac{1}{R}$ equal to that in equation $(1)$:\n$$2 \\times (2.120) = 2 \\times (10S + \\frac{250}{R})$$\n$$4.240 = 20S + \\frac{500}{R} \\quad (3)$$\nNow, we subtract equation $(1)$ from equation $(3)$:\n$$(20S + \\frac{500}{R}) - (4S + \\frac{500}{R}) = 4.240 - 4.048$$\n$$16S = 0.192$$\nSolving for $S$:\n$$S = \\frac{0.192}{16} = 0.012 \\text{ s}$$\nThus, the average positioning overhead is $0.012$ seconds, or $12$ milliseconds.\n\nNow we substitute the value of $S$ back into one of the original equations to solve for $R$. Using equation $(2)$:\n$$2.120 = 10(0.012) + \\frac{250}{R}$$\n$$2.120 = 0.120 + \\frac{250}{R}$$\n$$2.120 - 0.120 = \\frac{250}{R}$$\n$$2.000 = \\frac{250}{R}$$\nSolving for $R$:\n$$R = \\frac{250}{2.000} = 125 \\text{ MiB/s}$$\nThus, the sustained transfer rate is $125$ MiB/s.\n\nThe parameterized model is:\n$$T_{\\text{read}}(m, \\text{size}) = 0.012m + \\frac{\\text{size}}{125}$$\n\nFinally, we use this model to predict the read time for File C and compute the absolute error against the measured value.\nFor File C: $m_C = 14$, $\\text{size}_C = 800$ MiB, and the measured time is $T_{\\text{meas}, C} = 6.590$ s.\nThe predicted read time, $T_{\\text{pred}, C}$, is:\n$$T_{\\text{pred}, C} = 0.012 \\times 14 + \\frac{800}{125}$$\nFirst, calculate the positioning time component:\n$$0.012 \\times 14 = 0.168 \\text{ s}$$\nNext, calculate the transfer time component:\n$$\\frac{800}{125} = 6.4 \\text{ s}$$\nThe predicted total read time is the sum of these two components:\n$$T_{\\text{pred}, C} = 0.168 + 6.4 = 6.568 \\text{ s}$$\nThe absolute error, $E$, is the absolute difference between the predicted time and the measured time:\n$$E = |T_{\\text{pred}, C} - T_{\\text{meas}, C}|$$\n$$E = |6.568 - 6.590| = |-0.022| = 0.022 \\text{ s}$$\nThe problem requires the final answer to be rounded to four significant figures. The calculated error is $0.022$. To express this with four significant figures, we add trailing zeros: $0.02200$.", "answer": "$$\\boxed{0.02200}$$", "id": "3636040"}, {"introduction": "Modern file systems employ sophisticated techniques like delayed allocation to optimize disk layout and reduce fragmentation. However, these optimizations can interact with application behavior in complex ways. This exercise explores a scenario where frequent file synchronization calls (`fsync`) can undermine the benefits of delayed allocation, leading to increased fragmentation. By modeling this interaction probabilistically, you will develop a deeper understanding of the dynamic interplay between the operating system's allocation strategies and application-level demands. [@problem_id:3636029]", "problem": "You are asked to model, analyze, and implement a program that quantifies fragmentation under a specific disk space allocation scenario involving delayed allocation and file synchronization calls that force early writeback. The context is an extent-based allocator using delayed allocation. The workload must be constructed so that delayed allocation cannot coalesce writes into large extents when file synchronization (fsync) calls are frequent. You must reason from first principles and produce a program that outputs the computed fragmentation values for a set of test cases.\n\nFundamental base:\n- A file system that uses delayed allocation defers choosing on-disk locations until data must be persisted. An extent is a contiguous run of blocks allocated for a file. A file synchronization (fsync) forces dirty data for a file to be written to stable storage, triggering immediate allocation of any delayed blocks for that file.\n- Consider a plausible extent allocator that, on each allocation event, takes the currently pending logical size and allocates a single contiguous extent, advancing a global disk pointer monotonically forward (a first-fit monotonic allocator). This assumption is widely used for modeling in Operating Systems due to its analytical tractability.\n- A Bernoulli process with success probability $f$ across $T$ independent trials has an expected number of successes equal to $T f$. For a Bernoulli trial with success probability $f$, the probability that the last trial is a failure is $1 - f$.\n\nWorkload definition to ensure failure of coalescing under frequent fsync:\n- There are $K \\ge 2$ files. Time proceeds in discrete steps. At each step, exactly one file receives an append of $w$ logical blocks. The files are selected in strict round-robin order so that file $i$ is always followed by file $i+1$ modulo $K$, for $i \\in \\{1,\\dots,K\\}$.\n- Each file performs exactly $T$ appends, so each file appends a total of $B = T \\cdot w$ blocks.\n- Delayed allocation is in effect: no on-disk allocation occurs until an allocation-triggering event.\n- Immediately after each append to a given file, independently, with probability $f \\in [0,1]$, that file issues file synchronization (fsync), which triggers allocation of all of its pending delayed blocks in a single contiguous extent, using the monotonic allocator. At the very end (after all appends), any remaining delayed blocks for a file are allocated in one final extent for that file.\n- Because of the strict round-robin schedule across $K \\ge 2$ files and the monotonic allocator, any two extents of the same file that are created by different fsync events are separated on disk by at least one extent from another file allocated in-between. Therefore, extents belonging to the same file cannot become physically adjacent across fsync boundaries in this workload, and hence cannot be coalesced by the allocator. This enforces a worst-case interleaving that makes coalescing fail when fsync events are frequent.\n\nTask:\n- For a single file in this workload, derive from first principles the expected number of extents as a function of $T$, $w$, and $f$, assuming $K \\ge 2$ to enforce non-coalescing between extents separated by fsync events.\n- Using your derivation, compute the following quantities for each test case:\n  1. The expected number of extents per file, denoted by $\\mathbb{E}[E]$.\n  2. The expected average extent length in blocks, defined as $\\mathbb{E}[L] = \\dfrac{B}{\\mathbb{E}[E]}$, where $B = T \\cdot w$ is the total number of blocks written by the file.\n  3. A dimensionless fragmentation index, defined as $\\Phi = \\dfrac{\\mathbb{E}[E]}{B}$ (extents per block).\n\nConstraints and clarifications:\n- Treat the fsync decision after each append for a given file as an independent Bernoulli trial with success probability $f$.\n- Assume $T \\ge 1$, $w \\ge 1$, and $K \\ge 2$ in all test cases.\n- No physical units are involved. All counts are in integer blocks, and $f$ is a probability in the closed interval $[0,1]$.\n- Angles are not involved.\n- Percentages, if discussed, must be expressed as decimals; here, $f$ is already a decimal probability.\n\nTest suite:\nProvide results for the following test cases, each described by the tuple $(K, T, w, f)$:\n- Case A (happy path, no fsync): $(K=\\;2,\\; T=\\;100,\\; w=\\;4,\\; f=\\;0.0)$\n- Case B (worst-case, fsync on every append): $(K=\\;2,\\; T=\\;100,\\; w=\\;4,\\; f=\\;1.0)$\n- Case C (moderate fsync rate, multiple files): $(K=\\;5,\\; T=\\;100,\\; w=\\;1,\\; f=\\;0.25)$\n- Case D (boundary on small $T$): $(K=\\;3,\\; T=\\;1,\\; w=\\;16,\\; f=\\;0.5)$\n- Case E (low fsync rate, larger block size): $(K=\\;10,\\; T=\\;50,\\; w=\\;8,\\; f=\\;0.1)$\n\nRequired final output format:\n- Your program must produce a single line: a list of results corresponding to the test cases in the same order, where each result is a list with three floating-point numbers rounded to exactly six digits after the decimal point, in the order $[\\mathbb{E}[E], \\mathbb{E}[L], \\Phi]$.\n- Concretely, the output must be a single line in the form:\n  \"[[e1,l1,p1],[e2,l2,p2],[e3,l3,p3],[e4,l4,p4],[e5,l5,p5]]\"\n- Do not print any other text.", "solution": "We begin by formalizing the process. Consider a single file in the workload. The file performs $T$ appends, each append adding $w$ blocks to the file. The file synchronization (fsync) decision after each append is an independent Bernoulli trial with success probability $f \\in [0,1]$. Under delayed allocation, pending writes for a file are only allocated when an fsync occurs, at which time all currently pending blocks for that file are assigned to a single contiguous extent by the monotonic allocator. After all appends, any remaining delayed blocks are allocated in one final extent.\n\nBecause files are scheduled in strict round-robin across $K \\ge 2$ files, between any two fsync events for the same file there must be at least one allocation for another file. The monotonic allocator assigns extents in global order, and thus extents of different files placed between two extents of the target file prevent those two extents of the target file from being adjacent. Therefore, extents created at different fsync times for the same file cannot be coalesced later. This ensures a necessary condition that delayed allocation fails to coalesce when fsync happens frequently.\n\nLet $S$ denote the number of fsync events that occur for the file over its $T$ appends. Because each append independently triggers fsync with probability $f$, we have $S \\sim \\mathrm{Binomial}(T, f)$, and the expected number of fsync events is $\\mathbb{E}[S] = T f$.\n\nWe now relate $S$ to the number of extents $E$ for the file. Each fsync closes the current pending run into one extent. After the final append, if the last append did not trigger fsync, there remains a non-empty run of pending blocks that is closed by a final forced flush into one last extent. Let $I$ be the indicator random variable that the last append did not trigger fsync. Then\n$$\nE = S + I.\n$$\nBecause the final append independently triggers fsync with probability $f$, we have\n$$\n\\mathbb{E}[I] = \\Pr(\\text{no fsync on last append}) = 1 - f.\n$$\nTaking expectation,\n$$\n\\mathbb{E}[E] = \\mathbb{E}[S] + \\mathbb{E}[I] = T f + (1 - f),\n$$\nfor any $T \\ge 1$ and $f \\in [0,1]$.\n\nLet $B$ be the total number of blocks appended by the file. Since each of the $T$ appends adds $w$ blocks,\n$$\nB = T \\cdot w.\n$$\nThe expected average extent length in blocks is defined as\n$$\n\\mathbb{E}[L] = \\frac{B}{\\mathbb{E}[E]} = \\frac{T \\cdot w}{T f + (1 - f)}.\n$$\nFinally, define a dimensionless fragmentation index $\\Phi$ as the expected number of extents per block:\n$$\n\\Phi = \\frac{\\mathbb{E}[E]}{B} = \\frac{T f + (1 - f)}{T \\cdot w}.\n$$\n\nThese expressions derive directly from the Bernoulli nature of fsync events, the forced final flush, and the round-robin $K \\ge 2$ scheduling that ensures that extents separated by fsync are not coalescible.\n\nWe now compute the numerical results for the specified test suite, rounding to six digits after the decimal point.\n\n- Case A: $(K=\\;2,\\; T=\\;100,\\; w=\\;4,\\; f=\\;0.0)$\n  - $\\mathbb{E}[E] = 100 \\cdot 0.0 + (1 - 0.0) = 1.0$\n  - $B = 100 \\cdot 4 = 400$\n  - $\\mathbb{E}[L] = \\dfrac{400}{1.0} = 400.000000$\n  - $\\Phi = \\dfrac{1.0}{400} = 0.002500$\n- Case B: $(K=\\;2,\\; T=\\;100,\\; w=\\;4,\\; f=\\;1.0)$\n  - $\\mathbb{E}[E] = 100 \\cdot 1.0 + (1 - 1.0) = 100.0$\n  - $B = 100 \\cdot 4 = 400$\n  - $\\mathbb{E}[L] = \\dfrac{400}{100.0} = 4.000000$\n  - $\\Phi = \\dfrac{100.0}{400} = 0.250000$\n- Case C: $(K=\\;5,\\; T=\\;100,\\; w=\\;1,\\; f=\\;0.25)$\n  - $\\mathbb{E}[E] = 100 \\cdot 0.25 + (1 - 0.25) = 25.75$\n  - $B = 100 \\cdot 1 = 100$\n  - $\\mathbb{E}[L] = \\dfrac{100}{25.75} \\approx 3.883495$\n  - $\\Phi = \\dfrac{25.75}{100} = 0.257500$\n- Case D: $(K=\\;3,\\; T=\\;1,\\; w=\\;16,\\; f=\\;0.5)$\n  - $\\mathbb{E}[E] = 1 \\cdot 0.5 + (1 - 0.5) = 1.0$\n  - $B = 1 \\cdot 16 = 16$\n  - $\\mathbb{E}[L] = \\dfrac{16}{1.0} = 16.000000$\n  - $\\Phi = \\dfrac{1.0}{16} = 0.062500$\n- Case E: $(K=\\;10,\\; T=\\;50,\\; w=\\;8,\\; f=\\;0.1)$\n  - $\\mathbb{E}[E] = 50 \\cdot 0.1 + (1 - 0.1) = 5.9$\n  - $B = 50 \\cdot 8 = 400$\n  - $\\mathbb{E}[L] = \\dfrac{400}{5.9} \\approx 67.796610$\n  - $\\Phi = \\dfrac{5.9}{400} = 0.014750$\n\nThe program you will produce should compute these values using the formulas and output a single line:\n\"[[e1,l1,p1],[e2,l2,p2],[e3,l3,p3],[e4,l4,p4],[e5,l5,p5]]\"\nwith each number rounded to six decimal places, in the order of the test cases A through E.", "answer": "[[1.000000,400.000000,0.002500],[100.000000,4.000000,0.250000],[25.750000,3.883495,0.257500],[1.000000,16.000000,0.062500],[5.900000,67.796610,0.014750]]", "id": "3636029"}]}