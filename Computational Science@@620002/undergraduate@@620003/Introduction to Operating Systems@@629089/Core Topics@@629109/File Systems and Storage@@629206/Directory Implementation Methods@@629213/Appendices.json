{"hands_on_practices": [{"introduction": "At the heart of any file system is the directory, a structure that maps human-readable filenames to their underlying metadata. This first exercise explores the fundamental performance trade-off between a simple linear list and a more efficient hash table implementation. By calculating the time saved in a common scenario—searching for a file that does not exist—you will gain a concrete, quantitative understanding of why the choice of data structure is so critical for overall system performance [@problem_id:3634443].", "problem": "A directory is a mapping from filenames to metadata records. Consider two directory implementation methods used by an operating system: a linear list and a hash table with separate chaining. For failed metadata queries such as the Portable Operating System Interface (POSIX) system call $stat$, the cost arises entirely from searching the directory data structure in memory; assume all other overheads are identical across implementations and therefore cancel when comparing the two methods.\n\nUse the following fundamental base:\n- In a linear list implementation, to conclude a filename is absent, the system must compare the queried name against each of the $n$ directory entries, incurring a per-entry comparison cost and loop-step overhead. There is also a final end-of-list check cost.\n- In a hash table with separate chaining and $B$ buckets under the uniform hashing assumption, the expected number of elements examined in an unsuccessful search equals the load factor $\\alpha = \\frac{n}{B}$. The cost includes computing the hash over the filename, accessing the relevant bucket, traversing the expected number of chain elements (each incurring a comparison and a link-traversal overhead), and performing a chain termination check.\n\nSuppose a directory contains $n = 8192$ entries. Assume a workload of $M = 5 \\times 10^{4}$ unsuccessful $stat$ calls on filenames that do not exist in the directory; names are independent of the stored entries and uniformly distributed with respect to hashing. The average filename length examined by the hash function is $L = 24$ characters. The following microarchitectural costs apply, all measured in microseconds:\n- Per-entry average string comparison time in the directory: $t_{\\mathrm{cmp}} = 0.04$.\n- Per-iteration loop-step overhead in the linear list: $t_{\\ell} = 0.01$.\n- End-of-list check in the linear list: $t_{\\mathrm{end}} = 0.01$.\n- Number of hash buckets: $B = 4096$, so the load factor is $\\alpha = \\frac{n}{B}$.\n- Per-character hash computation time: $t_{\\mathrm{char}} = 0.01$.\n- Bucket access overhead in the hash table: $t_{\\mathrm{bucket}} = 0.02$.\n- Per-chain-element link-traversal overhead in the hash table: $t_{\\mathrm{link}} = 0.005$.\n- Chain termination check in the hash table: $t_{\\mathrm{chain}} = 0.01$.\n\nDerive from first principles an expression for the time to determine absence in each implementation and compute the average time saved per failed lookup when using the hash table instead of the linear list over the $M$ unsuccessful operations. Express your final answer in microseconds and round your answer to four significant figures.", "solution": "The problem statement has been evaluated and is deemed valid. It is scientifically grounded in the principles of algorithm analysis and data structures, specifically concerning directory implementations in operating systems. The problem is well-posed, providing all necessary parameters and a clear objective. The language is precise and objective, free from ambiguity or subjective claims. We may therefore proceed with a formal solution.\n\nThe objective is to compute the average time saved per unsuccessful lookup when using a hash table instead of a linear list. This quantity, denoted as $\\Delta T$, is the difference between the time taken for an unsuccessful search in a linear list, $T_{\\mathrm{linear}}$, and the time taken for an unsuccessful search in a hash table, $T_{\\mathrm{hash}}$.\n$$ \\Delta T = T_{\\mathrm{linear}} - T_{\\mathrm{hash}} $$\nWe will derive expressions for $T_{\\mathrm{linear}}$ and $T_{\\mathrm{hash}}$ from first principles based on the provided cost model.\n\nFirst, let us analyze the cost of an unsuccessful search in a linear list implementation. To conclude that a filename is absent, the system must search the entire list. For a directory with $n$ entries, this involves iterating through all $n$ entries. For each entry, a string comparison is performed (cost $t_{\\mathrm{cmp}}$) and a loop-step overhead is incurred (cost $t_{\\ell}$). After iterating through all $n$ entries, a final end-of-list check is performed (cost $t_{\\mathrm{end}}$). The total time for one unsuccessful lookup is the sum of these costs:\n$$ T_{\\mathrm{linear}} = n \\cdot (t_{\\mathrm{cmp}} + t_{\\ell}) + t_{\\mathrm{end}} $$\n\nNext, we analyze the cost of an unsuccessful search in a hash table with separate chaining. The process involves several steps:\n1.  Compute the hash of the queried filename. The average filename length is $L$ characters, and the per-character hashing cost is $t_{\\mathrm{char}}$. The total hash computation cost is $L \\cdot t_{\\mathrm{char}}$.\n2.  Access the appropriate bucket in the hash table, which has an overhead of $t_{\\mathrm{bucket}}$.\n3.  Traverse the linked list (chain) associated with that bucket. Under the uniform hashing assumption, the expected length of a chain for an unsuccessful search is the load factor, $\\alpha = \\frac{n}{B}$, where $n$ is the number of entries and $B$ is the number of buckets. For each of the $\\alpha$ elements in the chain, a string comparison (cost $t_{\\mathrm{cmp}}$) and a link traversal (cost $t_{\\mathrm{link}}$) are performed. The total cost for traversing the chain is $\\alpha \\cdot (t_{\\mathrm{cmp}} + t_{\\mathrm{link}})$.\n4.  After traversing the expected number of elements, a final check for the end of the chain (e.g., a null pointer) confirms the absence of the filename. This has a cost of $t_{\\mathrm{chain}}$.\n\nThe total expected time for one unsuccessful lookup in the hash table is the sum of these costs:\n$$ T_{\\mathrm{hash}} = (L \\cdot t_{\\mathrm{char}}) + t_{\\mathrm{bucket}} + \\alpha \\cdot (t_{\\mathrm{cmp}} + t_{\\mathrm{link}}) + t_{\\mathrm{chain}} $$\nSubstituting the definition of $\\alpha$:\n$$ T_{\\mathrm{hash}} = L \\cdot t_{\\mathrm{char}} + t_{\\mathrm{bucket}} + \\frac{n}{B} \\cdot (t_{\\mathrm{cmp}} + t_{\\mathrm{link}}) + t_{\\mathrm{chain}} $$\n\nThe average time saved per lookup, $\\Delta T$, is therefore:\n$$ \\Delta T = \\left[ n \\cdot (t_{\\mathrm{cmp}} + t_{\\ell}) + t_{\\mathrm{end}} \\right] - \\left[ L \\cdot t_{\\mathrm{char}} + t_{\\mathrm{bucket}} + \\frac{n}{B} \\cdot (t_{\\mathrm{cmp}} + t_{\\mathrm{link}}) + t_{\\mathrm{chain}} \\right] $$\nThe total number of unsuccessful operations, $M = 5 \\times 10^{4}$, is extraneous information as the problem asks for the *average time saved per lookup*, not the total time saved over the entire workload.\n\nWe are given the following values, all in microseconds:\n-   $n = 8192$\n-   $t_{\\mathrm{cmp}} = 0.04$\n-   $t_{\\ell} = 0.01$\n-   $t_{\\mathrm{end}} = 0.01$\n-   $L = 24$\n-   $t_{\\mathrm{char}} = 0.01$\n-   $B = 4096$\n-   $t_{\\mathrm{bucket}} = 0.02$\n-   $t_{\\mathrm{link}} = 0.005$\n-   $t_{\\mathrm{chain}} = 0.01$\n\nFirst, we substitute these values into the expression for $T_{\\mathrm{linear}}$:\n$$ T_{\\mathrm{linear}} = 8192 \\cdot (0.04 + 0.01) + 0.01 $$\n$$ T_{\\mathrm{linear}} = 8192 \\cdot (0.05) + 0.01 $$\n$$ T_{\\mathrm{linear}} = 409.6 + 0.01 = 409.61 \\text{ microseconds} $$\n\nNext, we calculate the load factor $\\alpha$:\n$$ \\alpha = \\frac{n}{B} = \\frac{8192}{4096} = 2 $$\nNow we substitute the values into the expression for $T_{\\mathrm{hash}}$:\n$$ T_{\\mathrm{hash}} = (24 \\cdot 0.01) + 0.02 + 2 \\cdot (0.04 + 0.005) + 0.01 $$\n$$ T_{\\mathrm{hash}} = 0.24 + 0.02 + 2 \\cdot (0.045) + 0.01 $$\n$$ T_{\\mathrm{hash}} = 0.24 + 0.02 + 0.09 + 0.01 $$\n$$ T_{\\mathrm{hash}} = 0.36 \\text{ microseconds} $$\n\nFinally, we compute the average time saved per lookup, $\\Delta T$:\n$$ \\Delta T = T_{\\mathrm{linear}} - T_{\\mathrm{hash}} = 409.61 - 0.36 = 409.25 \\text{ microseconds} $$\n\nThe problem requires the final answer to be rounded to four significant figures. The number $409.25$ has five significant figures. The first four are $4$, $0$, $9$, and $2$. The fifth digit is $5$, so we round up the fourth digit.\n$$ \\Delta T \\approx 409.3 \\text{ microseconds} $$", "answer": "$$\\boxed{409.3}$$", "id": "3634443"}, {"introduction": "While raw speed is important, correctness is paramount, especially in modern operating systems where multiple threads may access the same directory simultaneously. This practice moves from single-threaded performance to the challenge of concurrency, focusing on the subtle but critical Time-of-Check-to-Time-of-Use (TOCTOU) race condition. You will analyze and choose between different mitigation strategies, learning how to ensure data integrity while minimizing performance degradation caused by unnecessary retries [@problem_id:3634439].", "problem": "An operating system directory can be implemented either as a linear list of directory entries or as a hash table with $B$ buckets. Consider concurrent threads performing lookups and modifications (insertions, deletions, renames). A correctness risk in concurrent lookup is Time-of-Check-to-Time-of-Use (TOCTOU): a reader checks a property of a directory entry and then uses the entry, but the directory may have changed in between, invalidating the checked property. A sequence counter is a shared integer that is incremented by a writer at the start and end of a modification; when incremented it transitions from an even value to an odd value and back to an even value. A reader observes the counter before reading, does its read, and rechecks the counter after reading; if the counter changed or is odd at either observation, the reader retries. Writers do not wait for readers and increment the appropriate sequence counter twice per modification. Assume modifications are applied atomically to directory data structures such that the sequence counter increments bracket the logical effect.\n\nSuppose you must design a TOCTOU mitigation strategy using sequence counters for both implementations:\n\n- Linear list: A reader performs a sequential scan, following $next$ pointers between nodes. Under contention, a long scan may interleave with multiple modifications. You may choose to revalidate at different granularities (per full scan, per step).\n- Hash table: A reader computes a hash and examines the corresponding bucket. You may choose global or bucket-level versioning. Assume that bucket membership does not change without a structural bucket operation (e.g., rehashing or a per-bucket chain link update).\n\nAssume the following model for performance under contention to ground your reasoning: modifications to bucket $i$ arrive as a Poisson process with rate $\\lambda_i$ events per unit time, and modifications anywhere in the directory arrive with rate $\\lambda = \\sum_{i=1}^{B} \\lambda_i$. A reader’s time scanning a single bucket is $T_{\\text{bucket}}$, and the time to traverse one node in the linear list is $T_{\\text{step}}$. Assume read-mostly workloads with $\\lambda_i \\ll \\lambda$ for most $i$, and that writers must not be blocked by readers.\n\nWhich option identifies a correct mitigation design using sequence counters that eliminates TOCTOU for both implementations and minimizes unnecessary reader retries under the stated workload assumptions?\n\nA. For the hash table, use per-bucket sequence counters: a reader sampling bucket $j$ reads the bucket’s counter $c_j$ before traversal and checks $c_j$ again after traversal; the reader accepts the result only if both observations are equal and even. Writers that modify bucket $j$ increment $c_j$ once at the start and once at the end of the change. For the linear list, revalidate against a single directory-wide sequence counter $s$ at each step: read $s$, read the current node and move to $next$, then recheck $s$; if $s$ changed or is odd at either observation, restart from the head. Writers increment $s$ at the start and end of any structural list change. This design detects any concurrent change affecting the reader’s path while minimizing retries by localizing validation for hash buckets.\n\nB. Use one directory-wide sequence counter $s$ for all structures: readers of both the hash table and linear list sample $s$ once before any traversal and once after finishing; if $s$ changed or is odd at either observation, the reader restarts its entire traversal. Writers increment $s$ once at the start and once at the end of any modification.\n\nC. For both implementations, add per-entry sequence counters that writers increment when changing that specific entry, and have readers verify counters only on the entries they touch; do not validate at the bucket or list level. Writers do not increment any bucket-level or global counter.\n\nD. For the hash table, use per-bucket counters, but have readers validate only after traversal (post-read), accepting as long as the final counter observation is even; for the linear list, validate only once per full scan with a global counter sampled before and after the scan, ignoring intermediate steps. Writers increment counters once at the end of modifications.", "solution": "The problem requires designing a Time-of-Check-to-Time-of-Use (TOCTOU) mitigation strategy using sequence counters for two different directory implementations: a linear list and a hash table. The design must be correct (eliminate TOCTOU) and efficient (minimize unnecessary reader retries) under a read-mostly workload where modifications are distributed across the structure.\n\nFirst, let us re-state the mechanics of a sequence counter as described in the problem. A writer increments a shared counter before a modification (making it odd) and after the modification (making it even again). A reader records the counter value before reading data. After reading, it checks the counter again. If the counter has changed, or if it was odd at the start, the data is potentially inconsistent, and the reader must retry its operation. This ensures that the reader's operation does not overlap with a writer's critical section.\n\nWe will now analyze the optimal strategy for each data structure.\n\n**1. Hash Table Analysis**\n\nA hash table consists of $B$ buckets. A lookup operation involves hashing a key to identify a specific bucket, say bucket $j$, and then searching only within that bucket. The key insight is that a modification to an entry in bucket $k$ (where $k \\neq j$) has no effect on the data or structure of bucket $j$. The operations on different buckets are independent, assuming no global table-wide operations like a resize/rehash.\n\nThe problem presents two choices for versioning: global or per-bucket.\n\n-   **Global Sequence Counter:** A single counter $s$ for the entire hash table. Any modification, regardless of which bucket it affects, increments $s$. A reader accessing bucket $j$ would read $s$, traverse the bucket (taking time $T_{\\text{bucket}}$), and then re-read $s$. The total rate of modifications across the entire table is $\\lambda = \\sum_{i=1}^{B} \\lambda_i$. A reader's operation will be invalidated if any modification occurs anywhere in the table during its read. The probability of a retry is proportional to $\\lambda T_{\\text{bucket}}$.\n\n-   **Per-Bucket Sequence Counters:** Each bucket $j$ has its own counter $c_j$. A writer modifying an entry in bucket $j$ only increments $c_j$. A reader accessing bucket $j$ would read $c_j$, traverse the bucket, and re-read $c_j$. The rate of modifications relevant to this reader is only $\\lambda_j$. The probability of a retry is proportional to $\\lambda_j T_{\\text{bucket}}$.\n\nThe problem states that the workload is such that $\\lambda_i \\ll \\lambda$ for most $i$. This means that the modification rate for a single bucket is much smaller than the total modification rate for the entire directory. Therefore, $\\lambda_j T_{\\text{bucket}} \\ll \\lambda T_{\\text{bucket}}$. Using per-bucket counters significantly reduces the number of unnecessary retries, as a reader is not affected by concurrent, unrelated modifications in other buckets. This directly addresses the goal of minimizing retries.\n\nThus, for the hash table, the optimal strategy is to use **per-bucket sequence counters**.\n\n**2. Linear List Analysis**\n\nA linear list is a single, monolithic data structure. A reader must traverse it sequentially from the head by following $next$ pointers. A structural modification (insertion or deletion) at any point in the list can affect the validity of the rest of the list from the reader's perspective. For example, if a reader is at node $N_i$ and about to traverse to $N_{i+1}$, a concurrent writer could delete $N_{i+1}$. If the reader then uses its now-stale pointer to $N_{i+1}$, it results in a use-after-free error, which is a severe correctness violation far worse than stale data.\n\nThe problem presents two choices for validation granularity: per-full-scan or per-step.\n\n-   **Validate Per Full Scan:** A reader would read a single, directory-wide counter $s$ at the beginning of the scan and re-read it at the end. The time window for this check is the entire scan time, which can be long. This approach is fundamentally flawed and unsafe. It does not prevent the use-after-free scenario described above. A modification could occur mid-scan, breaking the list traversal, and the check at the end would be too late to prevent the error, even though it would correctly trigger a retry. Therefore, this strategy fails the primary requirement of correctness.\n\n-   **Validate Per Step:** A reader must re-validate the integrity of the list at each step of the traversal. A single, directory-wide counter $s$ must be used, because any modification anywhere in the list can invalidate the traversal path. The correct reader algorithm is:\n    1.  Start at the head of the list.\n    2.  In a loop:\n        a. Read $s$. Ensure it is even.\n        b. Read the current node's data and its $next$ pointer.\n        c. Re-read $s$.\n        d. If the new value of $s$ is different from the old one, or if the initial value was odd, a concurrent modification has occurred. The traversal is potentially invalid, so the reader must abort and restart the entire scan from the head.\n        e. Otherwise, the step was safe. Move to the $next$ node and continue.\n\nThis per-step validation ensures that the $next$ pointer used to advance is always valid at the moment of traversal. The validation window for each step is very short ($T_{\\text{step}}$), making the probability of a conflict per step low. While a long scan still has a significant cumulative probability of being interrupted, this fine-grained checking is the only way to guarantee correctness and prevent catastrophic traversal errors.\n\nThus, for the linear list, the correct and necessary strategy is to use a **single, directory-wide sequence counter validated at each step**.\n\n**Conclusion of Derivation**\n\nThe optimal design that is both correct and minimizes retries is:\n-   **Hash Table:** Use per-bucket sequence counters, validated before and after traversing a bucket.\n-   **Linear List:** Use a single, directory-wide sequence counter, validated at each step of the traversal.\n\nWe will now evaluate the given options against this derived solution.\n\n**Option-by-Option Analysis**\n\n**A. For the hash table, use per-bucket sequence counters... For the linear list, revalidate against a single directory-wide sequence counter s at each step... This design detects any concurrent change affecting the reader’s path while minimizing retries by localizing validation for hash buckets.**\n\n-   **Hash Table Strategy:** This matches our derived optimal strategy (per-bucket counters) which minimizes unnecessary retries.\n-   **Linear List Strategy:** This matches our derived correct strategy (global counter, per-step validation) which is necessary to prevent traversal errors like use-after-free.\n-   **Rationale:** The provided rationale accurately summarizes why this combination is optimal: it localizes validation for the hash table to reduce spurious retries and ensures traversal safety for the linear list.\n-   **Verdict:** **Correct**.\n\n**B. Use one directory-wide sequence counter s for all structures: readers of both the hash table and linear list sample s once before any traversal and once after finishing...**\n\n-   **Hash Table Strategy:** This forces readers of one bucket to retry due to unrelated modifications in other buckets. This is suboptimal and violates the goal of minimizing unnecessary retries.\n-   **Linear List Strategy:** As analyzed, validating only once per full scan is unsafe. It does not prevent a reader from following a stale pointer from a deleted node mid-traversal.\n-   **Verdict:** **Incorrect**. This solution is suboptimal for the hash table and, more critically, incorrect/unsafe for the linear list.\n\n**C. For both implementations, add per-entry sequence counters that writers increment when changing that specific entry, and have readers verify counters only on the entries they touch...**\n\n-   **Analysis:** This strategy only protects against changes to the *data within an entry*. It offers no protection against *structural* changes to the directory, such as the insertion or deletion of an entry. For a linear list, a reader would not know if the $next$ pointer it is about to follow is stale. For a hash table, a reader would not know if the entry it is examining has been unlinked from the bucket's chain. This approach fails to solve the core problem of ensuring a valid traversal path.\n-   **Verdict:** **Incorrect**.\n\n**D. For the hash table, use per-bucket counters, but have readers validate only after traversal (post-read)... for the linear list, validate only once per full scan... Writers increment counters once at the end of modifications.**\n\n-   **Analysis:** This option contains multiple fundamental errors regarding the sequence counter protocol.\n    1.  **Reader validation:** The protocol requires the reader to check the counter *before* and *after* the read. Checking only after a read is insufficient. A reader could begin while the counter is odd (writer is active), read inconsistent data, and see an even counter value at the end if the writer has finished. The read would be incorrectly accepted.\n    2.  **Writer increments:** The protocol requires the writer to increment the counter *at the start* (even $\\to$ odd) and *at the end* (odd $\\to$ even) of the modification. Incrementing only once at the end breaks the protocol; the crucial \"writer is active\" odd state is never signaled. This contradicts the problem's own definition of a sequence counter.\n    3.  **Linear list validation:** As established for option B, validating once per full scan is unsafe.\n-   **Verdict:** **Incorrect**. This option describes a broken and unsafe implementation of the sequence counter mechanism.", "answer": "$$\\boxed{A}$$", "id": "3634439"}, {"introduction": "This final practice synthesizes the concepts of algorithmic performance and concurrency by challenging you to build a working performance simulator. Theoretical analysis provides the foundation, but implementing a model offers deeper, tangible insights. By translating cycle-cost formulas into a program, you will directly observe how factors like data structure load, lock contention, and even hardware features like Dynamic Voltage and Frequency Scaling (DVFS) interact to determine real-world lookup latency [@problem_id:3634370]. This exercise bridges the gap between abstract principles and practical performance engineering.", "problem": "You are asked to design and implement a deterministic simulation of directory lookup latency under Dynamic Voltage and Frequency Scaling (DVFS) that compares two directory implementation methods: a linear list and a hash table with separate chaining and per-bucket locks. The simulation must quantify the average lookup latency per operation for each method, expressed in microseconds, and study how the latency correlates with the number of directory entries and lock contention under varying central processing unit (CPU) clock frequencies. You must implement a single program that computes the results for a fixed test suite and outputs the final answers in the required format.\n\nUse only fundamental facts as the base of your model:\n\n- The asymptotic cost of a successful search in a linear list of size $n$ under uniform random placement of the target is on average $(n+1)/2$ comparisons; the cost of an unsuccessful search is $n$ comparisons. This follows from the linear pass comparison model.\n- Under uniform hashing with separate chaining, if the number of directory entries is $n$ and the number of buckets is $m$, the load factor is $\\alpha = n/m$. The expected number of elements examined in a successful lookup is $1 + \\alpha/2$, and for an unsuccessful lookup it is $\\alpha$.\n- The time per cycle under DVFS at frequency $f$ (in hertz) is $1/f$ seconds. Therefore, a computation requiring $C$ CPU cycles takes $C/f$ seconds.\n- Lock acquisition under contention incurs additional cycles beyond a base acquisition cost. Model contention as a scalar $q \\in [0,1]$ that linearly scales a contention penalty.\n\nYou must not assume any other formulas beyond these foundational ones. You must reason from these statements to define a total cycle count for each lookup method and then convert cycles to time under DVFS.\n\nDefine the following cycle cost parameters as constants within your program:\n\n- Linear list comparison cost per element: $c_{\\text{list\\_cmp}} = 60$ cycles.\n- Linear list fixed overhead per lookup: $c_{\\text{list\\_base}} = 20$ cycles.\n- Hash computation and bucket indexing fixed cost per lookup: $c_{\\text{hash}} = 200$ cycles and $c_{\\text{bucket}} = 10$ cycles.\n- Lock acquisition base cost per lookup: $c_{\\text{lock\\_base}} = 80$ cycles.\n- Contentious lock penalty per unit contention: $c_{\\text{lock\\_contend}} = 1200$ cycles.\n- Chain traversal per examined element consists of a comparison and a small per-link overhead: $c_{\\text{chain\\_cmp}} = 50$ cycles and $c_{\\text{chain\\_probe}} = 10$ cycles.\n\nYour program must compute the average latency per lookup for both methods using the following interpretation:\n\n- Let $s \\in [0,1]$ be the fraction of lookups that are successful; the fraction of unsuccessful lookups is $1-s$.\n- For the linear list, the expected number of comparisons per lookup is $s \\cdot \\frac{n+1}{2} + (1-s) \\cdot n$. The total expected cycles per lookup are $c_{\\text{list\\_base}} + c_{\\text{list\\_cmp}} \\cdot \\big(s \\cdot \\frac{n+1}{2} + (1-s) \\cdot n\\big)$.\n- For the hash table, the load factor is $\\alpha = n/m$. The expected number of examined elements per lookup is $s \\cdot \\big(1 + \\frac{\\alpha}{2}\\big) + (1-s) \\cdot \\alpha$. The lock acquisition cost per lookup is $c_{\\text{lock\\_base}} + q \\cdot c_{\\text{lock\\_contend}}$. The total expected cycles per lookup are $c_{\\text{hash}} + c_{\\text{bucket}} + \\big(c_{\\text{lock\\_base}} + q \\cdot c_{\\text{lock\\_contend}}\\big) + \\big(c_{\\text{chain\\_cmp}} + c_{\\text{chain\\_probe}}\\big) \\cdot \\big(s \\cdot \\big(1 + \\frac{\\alpha}{2}\\big) + (1-s) \\cdot \\alpha\\big)$.\n- Convert cycles to microseconds by multiplying by $10^{6}$ and dividing by $f$, where $f$ is the CPU frequency in hertz.\n\nExpress all final latencies in microseconds, rounded to six decimal places. Angles are not involved in this problem. No physical quantities other than time are required.\n\nTest suite and parameters:\n\n- Test case $1$ (general case): directory entries $n = 1024$, buckets $m = 2048$, success fraction $s = 0.6$, number of lookups $L = 1000$, frequency $f = 2.5$ gigahertz, contention $q = 0.2$.\n- Test case $2$ (empty directory, no contention): $n = 0$, $m = 1$, $s = 0.0$, $L = 100$, $f = 3.0$ gigahertz, $q = 0.0$.\n- Test case $3$ (high load factor and moderate contention): $n = 65536$, $m = 32768$, $s = 0.7$, $L = 5000$, $f = 1.2$ gigahertz, $q = 0.5$.\n- Test case $4$ (moderate load factor and high contention, low frequency): $n = 50000$, $m = 70000$, $s = 0.4$, $L = 2000$, $f = 0.8$ gigahertz, $q = 0.9$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be a two-element list $[x,y]$ where $x$ is the average linear-list lookup latency in microseconds and $y$ is the average hash-table lookup latency in microseconds, both with six decimal places. The overall output format must be exactly:\n$[[x_1,y_1],[x_2,y_2],[x_3,y_3],[x_4,y_4]]$ with no spaces except those needed within numbers if any.\n\nNo input should be read from standard input; all parameters must be hard-coded as specified. The code must be a complete, runnable program.", "solution": "The problem statement is assessed to be valid. It is scientifically grounded, well-posed, and objective. The provided model for calculating directory lookup latency is based on established principles of algorithmic analysis and a simplified, yet reasonable, model of CPU performance and lock contention. All parameters, constants, and formulas required for the calculation are explicitly and consistently defined. The problem is formalizable and directly pertains to the specified topic in operating systems.\n\nThe core task is to compute the average lookup latency for two data structures—a linear list and a hash table—using a deterministic, cycle-based performance model. The final latency is a function of CPU frequency, which is modulated by Dynamic Voltage and Frequency Scaling (DVFS).\n\nLet us formalize the models for each data structure based on the provided principles.\n\n**1. Linear List Latency Model**\n\nThe average number of comparisons required for a lookup in a linear list depends on whether the search is successful or unsuccessful, and the probability of each case.\n\n*   Let $n$ be the number of entries in the directory.\n*   Let $s$ be the fraction of lookups that are successful. Consequently, the fraction of unsuccessful lookups is $1-s$.\n\nAccording to the problem's foundational facts:\n*   A successful search requires, on average, $\\frac{n+1}{2}$ comparisons, assuming the target element's position is uniformly distributed.\n*   An unsuccessful search requires scanning the entire list, thus making $n$ comparisons.\n\nThe expected number of comparisons, $E_{\\text{list\\_comps}}$, is the weighted average of these two cases:\n$$E_{\\text{list\\_comps}} = s \\cdot \\left(\\frac{n+1}{2}\\right) + (1-s) \\cdot n$$\n\nThe total number of CPU cycles for a single lookup, $C_{\\text{list}}$, is the sum of a fixed overhead and the cost of the comparisons:\n$$C_{\\text{list}} = c_{\\text{list\\_base}} + c_{\\text{list\\_cmp}} \\cdot E_{\\text{list\\_comps}}$$\nSubstituting the expression for $E_{\\text{list\\_comps}}$ and the given cycle cost constants ($c_{\\text{list\\_base}} = 20$, $c_{\\text{list\\_cmp}} = 60$):\n$$C_{\\text{list}} = 20 + 60 \\cdot \\left(s \\cdot \\frac{n+1}{2} + (1-s) \\cdot n\\right)$$\n\n**2. Hash Table Latency Model**\n\nThe performance of an ideal hash table with separate chaining is dictated by its load factor, $\\alpha$.\n\n*   Let $n$ be the number of entries and $m$ be the number of buckets.\n*   The load factor is defined as $\\alpha = \\frac{n}{m}$.\n\nAccording to the problem's foundational facts for uniform hashing:\n*   A successful search examines, on average, $1 + \\frac{\\alpha}{2}$ elements.\n*   An unsuccessful search examines, on average, $\\alpha$ elements (the length of the chain).\n\nThe expected number of elements examined (probed), $E_{\\text{hash\\_probes}}$, is the weighted average:\n$$E_{\\text{hash\\_probes}} = s \\cdot \\left(1 + \\frac{\\alpha}{2}\\right) + (1-s) \\cdot \\alpha$$\n\nThe total number of CPU cycles, $C_{\\text{hash}}$, is composed of several parts:\n1.  Fixed costs for hash computation and bucket indexing: $c_{\\text{hash}} + c_{\\text{bucket}}$.\n2.  Lock acquisition cost, which includes a base cost and a penalty scaled by a contention factor $q$: $c_{\\text{lock\\_base}} + q \\cdot c_{\\text{lock\\_contend}}$.\n3.  Chain traversal cost, which is the cost per element examined multiplied by the expected number of elements examined: $(c_{\\text{chain\\_cmp}} + c_{\\text{chain\\_probe}}) \\cdot E_{\\text{hash\\_probes}}$.\n\nSumming these components gives the total cycle count:\n$$C_{\\text{hash}} = (c_{\\text{hash}} + c_{\\text{bucket}}) + (c_{\\text{lock\\_base}} + q \\cdot c_{\\text{lock\\_contend}}) + (c_{\\text{chain\\_cmp}} + c_{\\text{chain\\_probe}}) \\cdot E_{\\text{hash\\_probes}}$$\nSubstituting the given cycle cost constants ($c_{\\text{hash}} = 200$, $c_{\\text{bucket}} = 10$, $c_{\\text{lock\\_base}} = 80$, $c_{\\text{lock\\_contend}} = 1200$, $c_{\\text{chain\\_cmp}} = 50$, $c_{\\text{chain\\_probe}} = 10$):\n$$C_{\\text{hash}} = (200 + 10) + (80 + q \\cdot 1200) + (50 + 10) \\cdot \\left(s \\cdot \\left(1 + \\frac{\\alpha}{2}\\right) + (1-s) \\cdot \\alpha\\right)$$\n$$C_{\\text{hash}} = 290 + 1200 \\cdot q + 60 \\cdot \\left(s \\cdot \\left(1 + \\frac{n}{2m}\\right) + (1-s) \\cdot \\frac{n}{m}\\right)$$\n\n**3. Conversion from Cycles to Latency**\n\nThe latency in seconds is the total number of cycles, $C$, divided by the CPU frequency, $f$, in Hertz. To express the latency in microseconds ($\\mu s$), we multiply by $10^6$.\n$$\\text{Latency}(\\mu s) = \\frac{C}{f} \\times 10^6$$\nGiven that the frequency $f$ is provided in gigahertz (GHz), it must be converted to Hertz by multiplying by $10^9$.\n\nIt is noted that the parameter $L$, the total number of lookups, is provided in the test suite but does not appear in the formulas for the *average latency per lookup*. Therefore, this parameter is extraneous to the required calculation and will be ignored.\n\nThe following C program will implement these precise formulas to calculate the latencies for each test case specified in the problem statement.", "answer": "```c\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <math.h>\n// #include <complex.h>\n// #include <threads.h>\n// #include <stdatomic.h>\n\n// A struct to hold the parameters for a single test case.\ntypedef struct {\n    int n;          // Number of directory entries\n    int m;          // Number of hash table buckets\n    double s;       // Fraction of successful lookups\n    double f_ghz;   // CPU frequency in GHz\n    double q;       // Lock contention factor\n} TestCase;\n\n// Function to calculate the average lookup latencies.\nvoid calculate_latencies(const TestCase* tc, double* latency_list, double* latency_hash) {\n    // Cycle cost parameters\n    const double c_list_cmp = 60.0;\n    const double c_list_base = 20.0;\n    const double c_hash = 200.0;\n    const double c_bucket = 10.0;\n    const double c_lock_base = 80.0;\n    const double c_lock_contend = 1200.0;\n    const double c_chain_cmp = 50.0;\n    const double c_chain_probe = 10.0;\n\n    // Convert frequency from GHz to Hz\n    double f_hz = tc->f_ghz * 1e9;\n\n    // ===== Linear List Calculation =====\n    double n_double = (double)tc->n;\n    // Expected number of comparisons per lookup\n    double expected_list_comps = tc->s * (n_double + 1.0) / 2.0 + (1.0 - tc->s) * n_double;\n    // Total expected cycles per lookup for the linear list\n    double total_cycles_list = c_list_base + c_list_cmp * expected_list_comps;\n    // Convert cycles to microseconds\n    *latency_list = (total_cycles_list / f_hz) * 1e6;\n\n    // ===== Hash Table Calculation =====\n    double m_double = (double)tc->m;\n    // Load factor alpha\n    double alpha = (m_double > 0) ? (n_double / m_double) : 0.0;\n    // Expected number of elements examined per lookup\n    double expected_hash_probes = tc->s * (1.0 + alpha / 2.0) + (1.0 - tc->s) * alpha;\n    // Total cost of chain traversal per lookup\n    double chain_cost = (c_chain_cmp + c_chain_probe) * expected_hash_probes;\n    // Total cost of lock acquisition per lookup\n    double lock_cost = c_lock_base + tc->q * c_lock_contend;\n    // Total expected cycles per lookup for the hash table\n    double total_cycles_hash = c_hash + c_bucket + lock_cost + chain_cost;\n    // Convert cycles to microseconds\n    *latency_hash = (total_cycles_hash / f_hz) * 1e6;\n}\n\nint main(void) {\n    // Define the test cases from the problem statement.\n    // The parameter L is omitted as it is not needed for per-operation latency calculation.\n    TestCase test_cases[] = {\n        {1024, 2048, 0.6, 2.5, 0.2}, // Test case 1\n        {0, 1, 0.0, 3.0, 0.0},       // Test case 2\n        {65536, 32768, 0.7, 1.2, 0.5},// Test case 3\n        {50000, 70000, 0.4, 0.8, 0.9} // Test case 4\n    };\n\n    // Calculate the number of test cases.\n    int num_cases = sizeof(test_cases) / sizeof(test_cases[0]);\n    double results[num_cases][2];\n\n    // Calculate the result for each test case.\n    for (int i = 0; i < num_cases; ++i) {\n        calculate_latencies(&test_cases[i], &results[i][0], &results[i][1]);\n    }\n\n    // Print the results in the EXACT REQUIRED format before the final return statement.\n    printf(\"[\");\n    for (int i = 0; i < num_cases; ++i) {\n        printf(\"[%.6f,%.6f]\", results[i][0], results[i][1]);\n        if (i < num_cases - 1) {\n            printf(\",\");\n        }\n    }\n    printf(\"]\");\n\n    // The specification requires a single line, so we do not add a newline here.\n    // If a newline is desired for shell prompt clarity, use printf(\"]\\n\");\n\n    return EXIT_SUCCESS;\n}\n```", "id": "3634370"}]}