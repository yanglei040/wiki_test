{"hands_on_practices": [{"introduction": "The architecture of a virtual memory system is a study in trade-offs, and understanding them requires thinking from the ground up. This exercise challenges you to move beyond memorized formulas and derive the size of a virtual address space from first principles, based on the structure of its multi-level page tables. By comparing two hypothetical but plausible architectures, you will gain a concrete understanding of how design choices like page size and the number of paging levels directly impact both the addressable range and the memory overhead of the system [@problem_id:3620292].", "problem": "A computer system uses demand-paged virtual memory implemented by a Memory Management Unit (MMU). The MMU translates a process’s logical (virtual) addresses to physical addresses using a uniform multi-level page table scheme. Each level of the page table is a page-sized array of fixed-size page-table entries. For a given architecture, let $L$ denote the number of page-table levels (including the leaf), let $E$ denote the number of entries per page-table page, and let $P$ denote the page size in bytes. The virtual address of a memory reference is split into a page offset and $L$ indices, one per level, that select the corresponding entry within each level.\n\nTwo architectures are proposed:\n\n- Architecture $\\mathsf{A}$: $L_{\\mathsf{A}} = 4$, $P_{\\mathsf{A}} = 4 \\times 2^{10}$ bytes, and each page-table entry is $8$ bytes, with the design choice that $E_{\\mathsf{A}} = P_{\\mathsf{A}} / 8$.\n\n- Architecture $\\mathsf{B}$: $L_{\\mathsf{B}} = 3$, $P_{\\mathsf{B}} = 16 \\times 2^{10}$ bytes, and each page-table entry is $8$ bytes, with the design choice that $E_{\\mathsf{B}} = P_{\\mathsf{B}} / 8$.\n\nAssume that a page-table entry stores enough information to locate the next-level page table (or the physical page at the leaf), and that there are no hardware-imposed constraints beyond the multi-level scheme described. Start from the core definitions of paging (page offset, page number, and multi-level indexing) and the fact that the number of bits needed to encode $N$ distinct values is $\\log_{2}(N)$, and derive, from first principles, the virtual-address width for each architecture. Use only these fundamentals and do not assume or cite any architecture-specific “shortcut” formulas.\n\nTo ground the comparison with concrete numbers, consider mapping a single contiguous region of logical memory of size $S = 256 \\times 2^{20}$ bytes for one process. For each architecture, determine the number of last-level page tables allocated, the number of upper-level page tables allocated, and the total page-table memory overhead in bytes. Briefly explain the design trade-offs observed (for example, impacts on page-table memory overhead and internal fragmentation).\n\nReport only the difference in virtual-address width between Architecture $\\mathsf{A}$ and Architecture $\\mathsf{B}$ as your final answer. Express your final answer as an integer number of bits.", "solution": "The problem is valid as it is scientifically grounded in the principles of virtual memory management, is well-posed with all necessary information provided, and is stated objectively. We can proceed with a solution derived from first principles.\n\nA virtual address is partitioned into two main components: a page number and a page offset. The number of bits required for the page offset, $w_{\\text{offset}}$, is determined by the page size, $P$. To uniquely address each byte within a page of size $P$ bytes, we require $w_{\\text{offset}} = \\log_2(P)$ bits.\n\nThe remaining bits of the virtual address constitute the page number. In a multi-level paging scheme with $L$ levels, the page number is further subdivided into $L$ indices, where each index is used to select an entry in a page table at a specific level. The problem states that each page table is itself a page-sized array containing a number of page-table entries, $E$. To select one of the $E$ entries in a page table, an index of width $w_{\\text{index}} = \\log_2(E)$ bits is required.\n\nSince the scheme is uniform, each of the $L$ indices has the same width, $w_{\\text{index}}$. The total width of the page number, $w_{\\text{pn}}$, is the sum of the widths of all indices, which is $w_{\\text{pn}} = L \\times w_{\\text{index}} = L \\times \\log_2(E)$.\n\nTherefore, the total virtual-address width, $w_{\\text{vaddr}}$, is the sum of the page number width and the page offset width:\n$$w_{\\text{vaddr}} = w_{\\text{pn}} + w_{\\text{offset}} = (L \\times \\log_2(E)) + \\log_2(P)$$\n\nWe are given that a page table is page-sized and each page-table entry (PTE) has a fixed size. Let the PTE size be $S_{\\text{PTE}}$. The number of entries per page-table, $E$, is the page size $P$ divided by the PTE size $S_{\\text{PTE}}$.\n$$E = \\frac{P}{S_{\\text{PTE}}}$$\nSubstituting this into our equation for $w_{\\text{vaddr}}$:\n$$w_{\\text{vaddr}} = L \\times \\log_2\\left(\\frac{P}{S_{\\text{PTE}}}\\right) + \\log_2(P)$$\nThis is the general formula derived from first principles. Now we apply it to each architecture.\n\nFor both architectures, the page-table entry size is $S_{\\text{PTE}} = 8$ bytes, which is $2^3$ bytes.\n\n**Architecture $\\mathsf{A}$**\nThe given parameters are:\n- Number of levels, $L_{\\mathsf{A}} = 4$.\n- Page size, $P_{\\mathsf{A}} = 4 \\times 2^{10} = 2^2 \\times 2^{10} = 2^{12}$ bytes.\n- Page-table entry size, $S_{\\text{PTE}} = 8 = 2^3$ bytes.\n\nFirst, we calculate the number of entries per page-table page for architecture $\\mathsf{A}$:\n$$E_{\\mathsf{A}} = \\frac{P_{\\mathsf{A}}}{S_{\\text{PTE}}} = \\frac{2^{12}}{2^3} = 2^9$$\nThe number of bits for an index into this page table is:\n$$w_{\\text{index, A}} = \\log_2(E_{\\mathsf{A}}) = \\log_2(2^9) = 9 \\text{ bits}$$\nThe number of bits for the page offset is:\n$$w_{\\text{offset, A}} = \\log_2(P_{\\mathsf{A}}) = \\log_2(2^{12}) = 12 \\text{ bits}$$\nThe total virtual-address width for architecture $\\mathsf{A}$ is:\n$$w_{\\text{vaddr, A}} = (L_{\\mathsf{A}} \\times w_{\\text{index, A}}) + w_{\\text{offset, A}} = (4 \\times 9) + 12 = 36 + 12 = 48 \\text{ bits}$$\n\n**Architecture $\\mathsf{B}$**\nThe given parameters are:\n- Number of levels, $L_{\\mathsf{B}} = 3$.\n- Page size, $P_{\\mathsf{B}} = 16 \\times 2^{10} = 2^4 \\times 2^{10} = 2^{14}$ bytes.\n- Page-table entry size, $S_{\\text{PTE}} = 8 = 2^3$ bytes.\n\nFirst, we calculate the number of entries per page-table page for architecture $\\mathsf{B}$:\n$$E_{\\mathsf{B}} = \\frac{P_{\\mathsf{B}}}{S_{\\text{PTE}}} = \\frac{2^{14}}{2^3} = 2^{11}$$\nThe number of bits for an index into this page table is:\n$$w_{\\text{index, B}} = \\log_2(E_{\\mathsf{B}}) = \\log_2(2^{11}) = 11 \\text{ bits}$$\nThe number of bits for the page offset is:\n$$w_{\\text{offset, B}} = \\log_2(P_{\\mathsf{B}}) = \\log_2(2^{14}) = 14 \\text{ bits}$$\nThe total virtual-address width for architecture $\\mathsf{B}$ is:\n$$w_{\\text{vaddr, B}} = (L_{\\mathsf{B}} \\times w_{\\text{index, B}}) + w_{\\text{offset, B}} = (3 \\times 11) + 14 = 33 + 14 = 47 \\text{ bits}$$\n\n**Difference in Virtual-Address Width**\nThe difference is $w_{\\text{vaddr, A}} - w_{\\text{vaddr, B}} = 48 - 47 = 1$ bit.\n\n**Analysis of Page-Table Overhead**\nWe now analyze the memory overhead for mapping a contiguous region of size $S = 256 \\times 2^{20}$ bytes.\n$S = 256 \\times 2^{20} = 2^8 \\times 2^{20} = 2^{28}$ bytes.\n\n**For Architecture $\\mathsf{A}$ ($P_{\\mathsf{A}} = 2^{12}$ bytes, $E_{\\mathsf{A}} = 2^9$ entries/table, $L_{\\mathsf{A}}=4$ levels):**\nNumber of pages required to map the region:\n$$N_{\\text{pages, A}} = \\frac{S}{P_{\\mathsf{A}}} = \\frac{2^{28}}{2^{12}} = 2^{16} = 65536 \\text{ pages}$$\nThese pages are pointed to by entries in the last-level (leaf) page tables. These are level-4 page tables in a 4-level scheme.\nNumber of last-level ($L4$) page tables:\n$$N_{L4, \\mathsf{A}} = \\left\\lceil \\frac{N_{\\text{pages, A}}}{E_{\\mathsf{A}}} \\right\\rceil = \\left\\lceil \\frac{2^{16}}{2^9} \\right\\rceil = 2^7 = 128 \\text{ tables}$$\nThese $128$ tables require $128$ pointers in the level-3 page tables.\nNumber of level-3 ($L3$) page tables:\n$$N_{L3, \\mathsf{A}} = \\left\\lceil \\frac{N_{L4, \\mathsf{A}}}{E_{\\mathsf{A}}} \\right\\rceil = \\left\\lceil \\frac{128}{2^9} \\right\\rceil = \\left\\lceil \\frac{2^7}{2^9} \\right\\rceil = 1 \\text{ table}$$\nThis single $L3$ table requires $1$ pointer in a level-2 page table.\nNumber of level-2 ($L2$) page tables:\n$$N_{L2, \\mathsf{A}} = \\left\\lceil \\frac{N_{L3, \\mathsf{A}}}{E_{\\mathsf{A}}} \\right\\rceil = \\left\\lceil \\frac{1}{2^9} \\right\\rceil = 1 \\text{ table}$$\nThis single $L2$ table requires $1$ pointer in a level-1 page table.\nNumber of level-1 ($L1$) page tables (the top-level directory):\n$$N_{L1, \\mathsf{A}} = \\left\\lceil \\frac{N_{L2, \\mathsf{A}}}{E_{\\mathsf{A}}} \\right\\rceil = \\left\\lceil \\frac{1}{2^9} \\right\\rceil = 1 \\text{ table}$$\n- Number of last-level page tables: $128$.\n- Number of upper-level page tables ($L1, L2, L3$): $1 + 1 + 1 = 3$.\n- Total page tables allocated: $128 + 3 = 131$.\n- Total page-table memory overhead: $131 \\times P_{\\mathsf{A}} = 131 \\times 2^{12} = 131 \\times 4096 = 536576$ bytes.\n\n**For Architecture $\\mathsf{B}$ ($P_{\\mathsf{B}} = 2^{14}$ bytes, $E_{\\mathsf{B}} = 2^{11}$ entries/table, $L_{\\mathsf{B}}=3$ levels):**\nNumber of pages required to map the region:\n$$N_{\\text{pages, B}} = \\frac{S}{P_{\\mathsf{B}}} = \\frac{2^{28}}{2^{14}} = 2^{14} = 16384 \\text{ pages}$$\nThese pages are pointed to by entries in the last-level (leaf) page tables. These are level-3 page tables in a 3-level scheme.\nNumber of last-level ($L3$) page tables:\n$$N_{L3, \\mathsf{B}} = \\left\\lceil \\frac{N_{\\text{pages, B}}}{E_{\\mathsf{B}}} \\right\\rceil = \\left\\lceil \\frac{2^{14}}{2^{11}} \\right\\rceil = 2^3 = 8 \\text{ tables}$$\nThese $8$ tables require $8$ pointers in the level-2 page tables.\nNumber of level-2 ($L2$) page tables:\n$$N_{L2, \\mathsf{B}} = \\left\\lceil \\frac{N_{L3, \\mathsf{B}}}{E_{\\mathsf{B}}} \\right\\rceil = \\left\\lceil \\frac{8}{2^{11}} \\right\\rceil = 1 \\text{ table}$$\nThis single $L2$ table requires $1$ pointer in a level-1 page table.\nNumber of level-1 ($L1$) page tables (the top-level directory):\n$$N_{L1, \\mathsf{B}} = \\left\\lceil \\frac{N_{L2, \\mathsf{B}}}{E_{\\mathsf{B}}} \\right\\rceil = \\left\\lceil \\frac{1}{2^{11}} \\right\\rceil = 1 \\text{ table}$$\n- Number of last-level page tables: $8$.\n- Number of upper-level page tables ($L1, L2$): $1 + 1 = 2$.\n- Total page tables allocated: $8 + 2 = 10$.\n- Total page-table memory overhead: $10 \\times P_{\\mathsf{B}} = 10 \\times 2^{14} = 10 \\times 16384 = 163840$ bytes.\n\n**Brief Explanation of Design Trade-offs**\n- **Page-Table Memory Overhead**: Architecture $\\mathsf{B}$ has a significantly lower memory overhead for its page tables ($163840$ bytes) compared to Architecture $\\mathsf{A}$ ($536576$ bytes). This is a direct consequence of its larger page size ($16 \\text{ KiB}$ vs. $4 \\text{ KiB}$) and shallower page table hierarchy ($3$ levels vs. $4$ levels). Larger pages mean fewer pages are needed to cover the same memory region, which in turn means fewer page table entries and fewer last-level page tables are required.\n- **Internal Fragmentation**: Architecture $\\mathsf{A}$ is superior in terms of internal fragmentation. With its smaller page size of $4 \\text{ KiB}$, the average wasted space at the end of a memory allocation is smaller (on average $2 \\text{ KiB}$) than with Architecture $\\mathsf{B}$'s $16 \\text{ KiB}$ pages (on average $8 \\text{ KiB}$). For processes with many small, non-page-aligned memory segments, this can lead to substantial memory waste in Architecture $\\mathsf{B}$.\nThe trade-off is classic: smaller pages reduce internal fragmentation but increase the size of the page tables and the pressure on translation lookaside buffers (TLBs), while larger pages reduce page table overhead and improve TLB effectiveness at the cost of increased internal fragmentation.\n\nThe final answer required is only the difference in virtual-address width.\n$$w_{\\text{vaddr, A}} - w_{\\text{vaddr, B}} = 48 - 47 = 1$$", "answer": "$$\\boxed{1}$$", "id": "3620292"}, {"introduction": "As memory demands grow, managing page tables efficiently becomes critical. This practice explores a key optimization in modern systems: the use of huge pages to map large memory regions, which significantly reduces the number of required page table entries [@problem_id:3620275]. By tackling a scenario with specific alignment constraints, you will calculate and directly compare the overhead of using a mixed-page-size strategy versus a traditional single-page-size approach, revealing the practical performance gains at stake.", "problem": "Consider a $64$-bit system that uses demand-paged virtual memory with support for both base pages of size $4$ KiB and huge pages of size $2$ MiB. The Memory Management Unit (MMU) translates Virtual Address (VA) to Physical Address (PA) through multi-level page tables. Each page-mapping entry (either a Page Table Entry (PTE) for a $4$ KiB page or a Page Directory Entry (PDE) with the Page Size bit set for a $2$ MiB page) maps exactly one page and is the unit of remapping. For this problem, define one \"PTE update\" to mean writing one page-mapping entry of either kind.\n\nAssume the following physical and virtual memory facts:\n- A $4$ KiB page is $2^{12}$ bytes and a $2$ MiB page is $2^{21}$ bytes. A kibibyte (KiB) is $2^{10}$ bytes and a mebibyte (MiB) is $2^{20}$ bytes.\n- A $2$ MiB mapping requires the VA base address and the PA base address to be individually aligned to $2$ MiB boundaries, and the physical frames covered by the mapping must be physically contiguous.\n- A $4$ KiB mapping requires the VA base and PA base be individually aligned to $4$ KiB boundaries, and the physical frame must be present; no larger alignment is required.\n\nYou must remap a single large contiguous VA region of size $513$ MiB to a new set of Physical Frame Numbers (PFNs) that are physically contiguous over the entire $513$ MiB span. The VA base of the region is misaligned by exactly $1$ MiB relative to the nearest $2$ MiB boundary, and the PA base of the target PFN span is also misaligned by exactly $1$ MiB relative to a $2$ MiB boundary. All addresses involved are aligned to $4$ KiB boundaries, and the multi-level page tables already exist; count only the page-mapping entry writes needed to perform the remap, not any pointer-level entries or table allocations.\n\nCompute the minimal number of page-mapping entry updates required under two independent strategies:\n- Strategy A: Use $2$ MiB pages wherever the alignment rules allow and use $4$ KiB pages for any remainder that cannot be covered by $2$ MiB pages.\n- Strategy B: Use only $4$ KiB pages for the entire $513$ MiB region.\n\nReport your final answer as a pair $(\\text{Strategy A}, \\text{Strategy B})$ in that order. The final answer must be a single entity and contain no units.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of operating system memory management, well-posed with sufficient and consistent data, and objectively stated.\n\nWe are asked to compute the number of page-mapping entry updates required to remap a contiguous virtual address (VA) region of size $513$ MiB to a contiguous physical address (PA) region of the same size, under two different paging strategies.\n\nLet's define the key parameters from the problem statement:\n-   Total size of the memory region to be remapped: $S_{total} = 513 \\text{ MiB}$.\n-   Base page size: $S_{base} = 4 \\text{ KiB} = 4 \\times 2^{10} \\text{ bytes} = 2^2 \\times 2^{10} \\text{ bytes} = 2^{12} \\text{ bytes}$.\n-   Huge page size: $S_{huge} = 2 \\text{ MiB} = 2 \\times 2^{20} \\text{ bytes} = 2^{21} \\text{ bytes}$.\n-   The VA base address of the region is misaligned by $1 \\text{ MiB}$ relative to a $2 \\text{ MiB}$ boundary.\n-   The PA base address of the target region is also misaligned by $1 \\text{ MiB}$ relative to a $2 \\text{ MiB}$ boundary.\n-   An \"update\" is defined as writing a single page-mapping entry, which can be for either a $4 \\text{ KiB}$ page or a $2 \\text{ MiB}$ page.\n\nFirst, we analyze Strategy B, as it is simpler.\n\n**Strategy B: Use only $4$ KiB pages**\n\nIn this strategy, the entire $513 \\text{ MiB}$ region is mapped using only $4 \\text{ KiB}$ pages. The number of required page-mapping entry updates, $N_B$, is the total size of the region divided by the size of a single base page. The misalignments are irrelevant here because the problem states all addresses are aligned to $4 \\text{ KiB}$ boundaries, which is the only requirement for using $4 \\text{ KiB}$ pages.\n\nWe can express the sizes in bytes to perform the calculation:\n$S_{total} = 513 \\text{ MiB} = 513 \\times 2^{20} \\text{ bytes}$.\n$S_{base} = 4 \\text{ KiB} = 2^{12} \\text{ bytes}$.\n\nThe number of updates is:\n$$N_B = \\frac{S_{total}}{S_{base}} = \\frac{513 \\times 2^{20}}{2^{12}} = 513 \\times 2^{20-12} = 513 \\times 2^8$$\n$$N_B = 513 \\times 256$$\nTo compute this, we can write $513$ as $512 + 1 = 2^9 + 1$:\n$$N_B = (2^9 + 1) \\times 2^8 = 2^9 \\times 2^8 + 1 \\times 2^8 = 2^{17} + 2^8 = 131072 + 256 = 131328$$\nSo, for Strategy B, $131,328$ updates are required.\n\n**Strategy A: Use $2$ MiB pages wherever possible**\n\nThis strategy requires careful analysis of the alignment constraints. A $2 \\text{ MiB}$ huge page can only be used if both its VA base and its corresponding PA base are aligned to a $2 \\text{ MiB}$ boundary.\n\nLet the VA base of the region be $V_{start}$ and the PA base be $P_{start}$. The problem states they are misaligned by $1 \\text{ MiB}$ relative to a $2 \\text{ MiB}$ boundary. This can be expressed mathematically:\n$$V_{start} \\pmod{2 \\text{ MiB}} = 1 \\text{ MiB}$$\n$$P_{start} \\pmod{2 \\text{ MiB}} = 1 \\text{ MiB}$$\n\nThe VA region spans from $V_{start}$ to $V_{start} + 513 \\text{ MiB} - 1$.\nBecause $V_{start}$ is not aligned to a $2 \\text{ MiB}$ boundary, we cannot use a $2 \\text{ MiB}$ page at the start of the region. We must first map a portion of the region using $4 \\text{ KiB}$ pages until we reach a VA that is $2 \\text{ MiB}$-aligned.\n\nThe first $2 \\text{ MiB}$-aligned VA address greater than $V_{start}$ is $V_{align} = V_{start} + (2 \\text{ MiB} - 1 \\text{ MiB}) = V_{start} + 1 \\text{ MiB}$. The VA segment from $V_{start}$ to $V_{align}-1$ has a size of $1 \\text{ MiB}$. This initial segment must be mapped using $4 \\text{ KiB}$ pages.\nThe number of updates for this initial $1 \\text{ MiB}$ segment, $N_{A,1}$, is:\n$$N_{A,1} = \\frac{1 \\text{ MiB}}{4 \\text{ KiB}} = \\frac{2^{20} \\text{ bytes}}{2^{12} \\text{ bytes}} = 2^{20-12} = 2^8 = 256$$\n\nNow, we consider the remaining portion of the region. The total size is $513 \\text{ MiB}$, and we have already mapped $1 \\text{ MiB}$. The remaining size is $513 \\text{ MiB} - 1 \\text{ MiB} = 512 \\text{ MiB}$.\nThis remaining region starts at VA $V_{align} = V_{start} + 1 \\text{ MiB}$. We have already established that this VA is $2 \\text{ MiB}$-aligned.\n\nNext, we must check the alignment of the corresponding PA. The physical memory is contiguous, so the PA corresponding to $V_{align}$ is $P_{align} = P_{start} + 1 \\text{ MiB}$. We check its alignment:\n$$P_{align} \\pmod{2 \\text{ MiB}} = (P_{start} + 1 \\text{ MiB}) \\pmod{2 \\text{ MiB}}$$\nSince $P_{start} \\pmod{2 \\text{ MiB}} = 1 \\text{ MiB}$, we have:\n$$(1 \\text{ MiB} + 1 \\text{ MiB}) \\pmod{2 \\text{ MiB}} = 2 \\text{ MiB} \\pmod{2 \\text{ MiB}} = 0$$\nThe PA base of the remaining region, $P_{align}$, is also $2 \\text{ MiB}$-aligned.\n\nThe remaining region has a size of $512 \\text{ MiB}$. Since both its VA and PA bases are $2 \\text{ MiB}$-aligned, and the physical memory is contiguous, we can map this entire portion using $2 \\text{ MiB}$ huge pages. The number of huge pages needed, $N_{A,2}$, is:\n$$N_{A,2} = \\frac{512 \\text{ MiB}}{2 \\text{ MiB}} = 256$$\n\nThe total number of updates for Strategy A, $N_A$, is the sum of the updates for the initial misaligned part and the main aligned part:\n$$N_A = N_{A,1} + N_{A,2} = 256 + 256 = 512$$\n\nThus, for Strategy A, $512$ updates are required.\n\nThe final answer is the pair of results for Strategy A and Strategy B.\n- Strategy A: $512$ updates.\n- Strategy B: $131,328$ updates.\nThe pair is $(512, 131328)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n512 & 131328\n\\end{pmatrix}\n}\n$$", "id": "3620275"}, {"introduction": "The theoretical speed of a processor is often limited by the practical speed of memory access, where the Translation Lookaside Buffer (TLB) plays a crucial role. This exercise delves into the dynamic behavior of the TLB, challenging you to analyze how a program's memory access pattern can lead to a performance catastrophe known as thrashing [@problem_id:3620213]. You will identify the precise conditions that cause a worst-case 100% miss rate, building a crucial intuition for writing memory-friendly code that works with the hardware, not against it.", "problem": "A single-threaded program running on a machine with a Memory Management Unit (MMU) performs a repeated, wrap-around traversal of a contiguous array using a stride access pattern. The array is a contiguous virtual region of size $(E+1)\\,p$ bytes, where $E$ is the number of entries in the Translation Lookaside Buffer (TLB) and $p$ is the page size in bytes. The base of the array is aligned to a page boundary. The TLB is fully associative with $E$ entries and uses Least Recently Used (LRU) replacement. Each virtual page of the array is mapped to a distinct physical page frame. The Central Processing Unit (CPU) generates the sequence of virtual addresses\n$$\nv_k \\equiv v_0 + (k\\,s) \\bmod \\left((E+1)\\,p\\right),\n$$\nfor integers $k \\ge 0$, where $s$ is the stride in bytes and $v_0$ is the base virtual address of the array (page-aligned). The data cache is single-level, fully associative, large enough that it never evicts a cache line due to capacity during one full cycle over the $(E+1)$ pages, and has line size $b$ bytes with $b$ dividing $p$.\n\nStarting only from the foundational definitions of virtual memory pages, the TLB as a cache of recent virtual-to-physical translations with $E$ entries under LRU, and the above-described access sequence, reason about how the induced sequence of virtual page numbers depends on the stride $s$, and how this, in turn, governs the TLB miss behavior. Also discuss, at a high level, how the cache line size $b$ interacts with the stride within a page, even though the cache does not constrain the TLB behavior under the given assumptions.\n\nDefine “maximum TLB thrash” as a steady-state TLB miss rate equal to $1$ (that is, every access after warm-up is a TLB miss). Determine the smallest positive stride $s$ (in bytes) that produces maximum TLB thrash for this workload. Express your final answer for $s$ in bytes. No rounding is required.", "solution": "The problem asks for the smallest positive stride $s$ that causes \"maximum TLB thrash,\" defined as a steady-state Translation Lookaside Buffer (TLB) miss rate of $1$. We must analyze the interaction between the strided memory access pattern and a fully associative TLB with a Least Recently Used (LRU) replacement policy.\n\nFirst, let us establish the fundamental principles. A virtual address $v$ is translated by the Memory Management Unit (MMU) into a physical address. This process involves determining the virtual page number (VPN) corresponding to $v$. Given a page size of $p$ bytes and a page-aligned base address $v_0$ for the memory region in question, the VPN for an address $v$ can be calculated as $\\text{VPN}(v) = \\lfloor (v - v_0) / p \\rfloor$. The TLB is a cache that stores recent VPN-to-physical-page-frame mappings to accelerate this translation.\n\nThe given system has a TLB with $E$ entries. It is fully associative and uses an LRU replacement policy. The program accesses a contiguous virtual array of size $(E+1)p$ bytes, which corresponds to exactly $E+1$ virtual pages. Let us label these pages with VPNs $0, 1, 2, \\dots, E$.\n\nA steady-state TLB miss rate of $1$ means that every memory access after an initial warm-up period results in a TLB miss. For a TLB with $E$ entries and an LRU policy, this condition arises when the sequence of memory accesses forces a reference to a page whose translation has just been evicted. This occurs if the access pattern cycles through a \"working set\" of at least $E+1$ distinct pages. Since our array spans exactly $E+1$ pages, maximum thrashing will occur if the access pattern systematically touches all $E+1$ pages in a repeating cycle.\n\nLet's analyze the sequence of virtual addresses generated:\n$$v_k = v_0 + ((k\\,s) \\bmod ((E+1)p))$$\nThe offset from the base of the array is $o_k = (k\\,s) \\bmod ((E+1)p)$. The VPN for the $k$-th access is:\n$$P_k = \\text{VPN}(v_k) = \\left\\lfloor \\frac{v_k - v_0}{p} \\right\\rfloor = \\left\\lfloor \\frac{(k\\,s) \\bmod ((E+1)p)}{p} \\right\\rfloor$$\n\nFor the miss rate to be $1$, every access must be to a different page than the one accessed previously (and in fact, different from the last $E$ unique pages accessed). A necessary, though not sufficient, condition for this is that the page number must change on every single access. That is, $P_k \\neq P_{k+1}$ for all $k$ in the steady state.\n\nLet's consider the effect of the stride $s$ relative to the page size $p$.\nIf the stride $s$ is less than the page size $p$ (i.e., $s < p$), then two consecutive accesses at $v_k$ and $v_{k+1}=v_k+s$ (ignoring wrap-around for simplicity) might fall within the same page. An access at an address $v$ with an in-page offset $v \\pmod p$ will be followed by an access to $v+s$. If $(v \\pmod p) + s < p$, both accesses are to the same page. This condition will be met for any address $v$ whose in-page offset is not in the last $s$ bytes of the page. This means a fraction $(p-s)/p$ of accesses will not cross a page boundary. Therefore, if $s < p$, there will be sequences of accesses to the same page, resulting in TLB hits. This violates the condition for a miss rate of $1$. Consequently, a necessary condition for maximum thrashing is $s \\ge p$.\n\nWe are looking for the *smallest positive* stride $s$ that causes maximum thrashing. Based on the reasoning above, we should start our search at $s=p$.\n\nLet's set the stride $s=p$. The sequence of VPNs becomes:\n$$P_k = \\left\\lfloor \\frac{(k\\,p) \\bmod ((E+1)p)}{p} \\right\\rfloor$$\nSince $k\\,p$ and $(E+1)p$ are both integer multiples of $p$, we can simplify this. Let $N=E+1$.\n$$(k\\,p) \\bmod (N p) = (k \\bmod N)p$$\nTherefore,\n$$P_k = \\left\\lfloor \\frac{(k \\bmod (E+1))p}{p} \\right\\rfloor = k \\bmod (E+1)$$\nThe sequence of accessed VPNs for $k=0, 1, 2, \\dots$ is:\n$$0, 1, 2, \\dots, E, 0, 1, 2, \\dots$$\nThis is a sequential scan through all $E+1$ pages of the array, wrapping around at the end. Let's trace the state of the $E$-entry LRU TLB.\n1.  Access to page $0$: Miss. TLB has $\\{0\\}$.\n2.  Access to page $1$: Miss. TLB has $\\{0, 1\\}$.\n...\nE. Access to page $E-1$: Miss. The TLB is now full: $\\{0, 1, \\dots, E-1\\}$. Page $0$ is the least recently used.\nE+1. Access to page $E$: Miss. Page $0$ is evicted. TLB contains $\\{1, 2, \\dots, E\\}$. Page $1$ becomes the LRU entry.\nE+2. Access to page $0$ (from wrap-around): Miss. Page $0$ is not in the TLB. Page $1$ is evicted. TLB contains $\\{2, 3, \\dots, E, 0\\}$. Page $2$ becomes the LRU entry.\nE+3. Access to page $1$: Miss. Page $1$ is not in the TLB. Page $2$ is evicted. TLB contains $\\{3, \\dots, E, 0, 1\\}$.\n\nThis pattern continues indefinitely. Every access is to the page whose translation was the one evicted $E$ steps prior, guaranteeing a TLB miss on every access in the steady state. Thus, a stride of $s=p$ results in a TLB miss rate of $1$.\n\nSince we established that any stride $s < p$ cannot produce a miss rate of $1$, and we have shown that $s=p$ does produce a miss rate of $1$, the smallest positive stride that produces maximum TLB thrash is $s=p$.\n\nFinally, the problem asks for a high-level discussion on the interaction between the cache line size $b$ and the stride $s$. The cache is large enough to avoid capacity misses, so we only consider compulsory misses. A memory access fetches a cache line of $b$ bytes. If the stride $s$ is smaller than $b$, consecutive accesses may fall within the same cache line. For instance, the access to $v_k$ brings in a cache line; the next access to $v_{k+1}=v_k+s$ will be a cache hit if it falls in that same line. The cache miss rate in this case would be approximately $s/b$. If $s \\ge b$, every access is to a different cache line, making every access a compulsory cache miss (miss rate of $1$). For our solution $s=p$, and since $b$ divides $p$, we have $s=p \\ge b$. This means that not only does every access cause a TLB miss, but it also causes a data cache miss, representing a worst-case scenario for both memory system components.", "answer": "$$\\boxed{p}$$", "id": "3620213"}]}