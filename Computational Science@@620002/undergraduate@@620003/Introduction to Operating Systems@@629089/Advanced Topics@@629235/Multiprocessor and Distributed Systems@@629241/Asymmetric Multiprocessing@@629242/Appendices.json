{"hands_on_practices": [{"introduction": "In an Asymmetric Multiprocessing (AMP) system where a master core handles all kernel tasks, a fundamental performance cost arises from forwarding system calls from worker cores. This exercise moves beyond simple latency measurement to dissect this overhead, using hypothetical performance counter data to calculate the Cycles Per Instruction ($CPI$) for both local and forwarded kernel paths. By comparing these $CPI$ values, we can gain a deeper, quantitative understanding of the architectural impact of the AMP model on kernel execution. [@problem_id:3621323]", "problem": "In an Asymmetric Multiprocessing (AMP) operating system, a single master core handles all operating system kernel execution while worker cores execute user processes and forward system calls to the master. You are tasked with designing microbenchmarks that differentiate kernel path lengths along two paths: a baseline path when a process runs on the master core and executes its system calls locally, and a longer path when a process runs on a worker core and forwards its system calls to the master.\n\nYou design two experiments. In the first experiment, a process pinned to the master core issues a simple system call (such as `getpid`) in a tight loop. Using performance counters on the master core, you count the average number of kernel-mode retired instructions per call along this baseline path and measure the end-to-end average latency per call. In the second experiment, an identical process is pinned to a worker core and issues the same system call; the worker forwards each call to the master. Using performance counters on the master, you count the average number of kernel-mode retired instructions per call attributable to the forwarded call on the master, and you measure the end-to-end average latency at the worker. To isolate cross-core delivery effects, you also conduct a no-operation remote procedure call that measures the forwarding and return overhead independent of payload execution on the master.\n\nAssume the following measured quantities:\n- Processor frequency is $f = 3.0 \\times 10^{9} \\, \\text{Hz}$.\n- Baseline experiment (process on master): average end-to-end latency per call is $T_b = 200 \\, \\text{ns}$; average kernel-mode retired instructions on the master per call is $I_b = 800$.\n- Forwarded experiment (process on worker): average end-to-end latency per call measured at the worker is $T_l = 500 \\, \\text{ns}$; average kernel-mode retired instructions on the master per forwarded call is $I_l = 1200$.\n- Cross-core forwarding overhead, measured via a no-operation remote procedure call with the master otherwise idle, is a constant $D = 80 \\, \\text{ns}$ per call; assume this overhead contributes to observed end-to-end latency in the forwarded experiment but does not correspond to kernel execution cycles on the master.\n\nUsing only the definitions that (i) cycles per instruction is $CPI = C / I$, where $C$ is the number of cycles and $I$ is the number of retired instructions, and (ii) time and cycles are related by $T = C / f$, compute the pair of cycle-per-instruction values $(CPI_b, CPI_l)$ for the kernel execution on the master for the baseline and forwarded paths, respectively. Ignore all user-mode contributions and any effects not explicitly stated above. Round your final values to three significant figures and report them as $(CPI_b, CPI_l)$ with no units.", "solution": "The problem requires the computation of the cycles-per-instruction ($CPI$) for kernel execution on a master core in an Asymmetric Multiprocessing (AMP) system under two distinct scenarios: a baseline local execution and a forwarded remote execution.\n\nThe fundamental relationships provided are:\n1.  Cycles per instruction: $CPI = \\frac{C}{I}$, where $C$ is the number of processor cycles and $I$ is the number of retired instructions.\n2.  Time-cycle relation: $T = \\frac{C}{f}$, where $T$ is the execution time and $f$ is the processor frequency.\n\nFrom the second relation, the number of cycles $C$ can be expressed as $C = T \\times f$. Substituting this into the definition of $CPI$, we obtain the primary formula for our calculations:\n$$CPI = \\frac{T \\times f}{I}$$\nThis formula connects the time spent on execution ($T$), the processor frequency ($f$), and the number of instructions retired ($I$) to the desired $CPI$ value. The key to solving this problem is to correctly identify the execution time ($T$) and instruction count ($I$) corresponding to the specific kernel path for each experiment.\n\nLet's analyze the baseline experiment first to compute $CPI_b$.\nIn this scenario, a process is pinned to the master core, and its system calls are executed locally. The given measurements are:\n-   Average end-to-end latency per call: $T_b = 200 \\, \\text{ns} = 200 \\times 10^{-9} \\, \\text{s}$.\n-   Average kernel-mode retired instructions on the master per call: $I_b = 800$.\n-   Processor frequency: $f = 3.0 \\times 10^{9} \\, \\text{Hz}$.\n\nThe problem specifies to ignore all user-mode contributions. Therefore, the measured end-to-end latency $T_b$ can be taken as the time the master core spends executing the kernel code for the system call. Let's denote this kernel execution time as $T_{k,b}$. Thus, $T_{k,b} = T_b$.\n\nWe can now calculate the number of cycles, $C_b$, consumed during this kernel execution:\n$$C_b = T_{k,b} \\times f = (200 \\times 10^{-9} \\, \\text{s}) \\times (3.0 \\times 10^{9} \\, \\text{s}^{-1}) = 600 \\, \\text{cycles}$$\nUsing the instruction count $I_b = 800$, we can compute $CPI_b$:\n$$CPI_b = \\frac{C_b}{I_b} = \\frac{600}{800} = \\frac{3}{4} = 0.75$$\n\nNext, let's analyze the forwarded experiment to compute $CPI_l$.\nIn this scenario, a process on a worker core issues a system call, which is forwarded to the master core for execution. The given measurements are:\n-   Average end-to-end latency per call, measured at the worker: $T_l = 500 \\, \\text{ns} = 500 \\times 10^{-9} \\, \\text{s}$.\n-   Average kernel-mode retired instructions on the master per forwarded call: $I_l = 1200$.\n-   Cross-core forwarding overhead: $D = 80 \\, \\text{ns} = 80 \\times 10^{-9} \\, \\text{s}$.\n-   Processor frequency: $f = 3.0 \\times 10^{9} \\, \\text{Hz}$.\n\nThe total latency $T_l$ measured at the worker includes the time to forward the request to the master, the kernel execution time on the master, and the time to return the result to the worker. The problem explicitly states that the overhead $D$ accounts for the forwarding and return latency and \"does not correspond to kernel execution cycles on the master.\" Therefore, to find the time spent purely on kernel execution on the master, which we denote as $T_{k,l}$, we must subtract this overhead from the total latency:\n$$T_{k,l} = T_l - D = (500 \\times 10^{-9} \\, \\text{s}) - (80 \\times 10^{-9} \\, \\text{s}) = 420 \\times 10^{-9} \\, \\text{s}$$\nNow we calculate the number of cycles, $C_l$, consumed during this kernel execution on the master:\n$$C_l = T_{k,l} \\times f = (420 \\times 10^{-9} \\, \\text{s}) \\times (3.0 \\times 10^{9} \\, \\text{s}^{-1}) = 1260 \\, \\text{cycles}$$\nUsing the instruction count $I_l = 1200$ for the forwarded path, we can compute $CPI_l$:\n$$CPI_l = \\frac{C_l}{I_l} = \\frac{1260}{1200} = \\frac{126}{120} = \\frac{21}{20} = 1.05$$\n\nThe problem requires the final values to be rounded to three significant figures.\nFor the baseline path, $CPI_b = 0.75$. To express this with three significant figures, we write it as $0.750$.\nFor the forwarded path, $CPI_l = 1.05$. This value already has three significant figures.\n\nThe pair of cycle-per-instruction values is $(CPI_b, CPI_l) = (0.750, 1.05)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.750 & 1.05\n\\end{pmatrix}\n}\n$$", "id": "3621323"}, {"introduction": "While analyzing the cost of a single remote system call is insightful, the true challenge in an AMP system is managing the collective load from all worker cores. This practice applies queueing theory to model the master core as a service center, allowing us to predict system-wide performance metrics like total latency as the load increases. By building this model, you will learn to analyze system stability and identify potential bottlenecks before they occur. [@problem_id:3621335]", "problem": "Consider an Asymmetric Multiprocessing (AMP) Operating System (OS) design used in a teaching lab. In this AMP system, there is a single dedicated master core that executes all privileged kernel system calls, and there are $n$ identical worker cores that execute user-level threads. When a user thread running on a worker core issues a system call, the worker forwards the system call to the master core. The forwarding incurs a measured software overhead $c$ on the worker to marshal arguments and trigger an inter-processor interrupt, and when the master completes the system call it sends the result back to the originating worker, incurring an additional overhead $c$ on the return path. Assume the following modeling assumptions as a fundamental base for analysis:\n\n- Each worker core generates system call requests according to an independent Poisson process with rate $\\lambda$ (requests per second).\n- The master core serves system call requests one at a time, with independent and identically distributed exponential service times having rate $\\mu$ (requests per second).\n- Superposition of independent Poisson processes yields a Poisson process with rate equal to the sum of individual rates, and the master can be modeled as an $M/M/1$ queue that is stable if and only if the total arrival rate is strictly less than the service rate.\n- All queues are First-In First-Out (FIFO), and the forwarding overheads $c$ on the forward and return paths are deterministic and do not overlap with service at the master.\n\nDefine the end-to-end latency of a system call as the elapsed time from the instant the worker initiates the system call forwarding until the instant the worker receives the return value. In the lab, the measured and configured parameters are:\n- Number of worker cores: $n = 6$.\n- Per-core system call arrival rate: $\\lambda = 800$ requests per second.\n- Master service rate: $\\mu = 6000$ requests per second.\n- One-way forwarding overhead: $c = 8$ microseconds.\n\nTasks:\n- Starting from the given modeling assumptions, derive symbolic expressions for:\n  1. The total arrival rate to the master, the traffic intensity, and the expected number of requests in the master’s queue.\n  2. The expected end-to-end system call latency as a function of $n, \\lambda, \\mu, \\text{and } c$.\n- Then, using the provided numerical values, compute the expected end-to-end system call latency. Express your final numerical answer in milliseconds and round your answer to four significant figures.", "solution": "The problem asks for an analysis of system call latency in an Asymmetric Multiprocessing (AMP) system. The system is modeled using queueing theory, where the master core is treated as an $M/M/1$ queue. We are tasked with deriving symbolic expressions for key performance metrics and then calculating the numerical value of the end-to-end latency.\n\nFirst, we will derive the symbolic expressions as requested. The system consists of $n$ worker cores, each generating system call requests according to a Poisson process with rate $\\lambda$. These requests are all directed to a single master core.\n\n1.  **Total Arrival Rate, Traffic Intensity, and Expected Queue Length**\n\nThe total arrival rate to the master core, denoted by $\\Lambda$, is the sum of the rates from the $n$ independent worker cores. Based on the superposition property of Poisson processes:\n$$ \\Lambda = \\sum_{i=1}^{n} \\lambda = n\\lambda $$\nThe master core serves these requests with an exponential service time distribution at a rate of $\\mu$. The traffic intensity, $\\rho$, is the ratio of the total arrival rate to the service rate. It represents the fraction of time the server (master core) is busy.\n$$ \\rho = \\frac{\\Lambda}{\\mu} = \\frac{n\\lambda}{\\mu} $$\nFor the queueing system to be stable, the arrival rate must be strictly less than the service rate, which means $\\rho < 1$.\n\nThe expected number of requests in the master's queue, $L_q$, for a stable $M/M/1$ system is given by the formula:\n$$ L_q = \\frac{\\rho^2}{1-\\rho} $$\nSubstituting the expression for $\\rho$, we get:\n$$ L_q = \\frac{\\left(\\frac{n\\lambda}{\\mu}\\right)^2}{1 - \\frac{n\\lambda}{\\mu}} = \\frac{(n\\lambda)^2}{\\mu(\\mu - n\\lambda)} $$\n\n2.  **Expected End-to-End System Call Latency**\n\nThe end-to-end latency of a system call, $T_{e2e}$, is defined as the time from the moment a worker initiates forwarding until it receives the result. This total time is the sum of three distinct, non-overlapping components:\n- The initial forwarding overhead, $c$.\n- The time spent in the master's system, $W$. This includes both the time waiting in the queue and the service time.\n- The return path overhead, $c$.\n\nThus, the total latency for a single request is $T_{e2e} = c + W + c = 2c + W$. We are interested in the expected value of this latency, $E[T_{e2e}]$. Since $c$ is a deterministic constant, we have:\n$$ E[T_{e2e}] = E[2c + W] = 2c + E[W] $$\n$E[W]$ is the expected time a request spends in the $M/M/1$ system (also known as the expected sojourn time). For a stable $M/M/1$ queue, this is given by:\n$$ E[W] = \\frac{1}{\\mu - \\Lambda} $$\nSubstituting $\\Lambda = n\\lambda$ into this expression gives:\n$$ E[W] = \\frac{1}{\\mu - n\\lambda} $$\nFinally, we can write the symbolic expression for the expected end-to-end latency:\n$$ E[T_{e2e}] = 2c + \\frac{1}{\\mu - n\\lambda} $$\n\nNow, we proceed to the numerical calculation using the provided parameter values:\n- Number of worker cores: $n = 6$\n- Per-core system call arrival rate: $\\lambda = 800 \\text{ s}^{-1}$\n- Master service rate: $\\mu = 6000 \\text{ s}^{-1}$\n- One-way forwarding overhead: $c = 8 \\text{ µs} = 8 \\times 10^{-6} \\text{ s}$\n\nFirst, we calculate the total arrival rate $\\Lambda$:\n$$ \\Lambda = n\\lambda = 6 \\times 800 \\text{ s}^{-1} = 4800 \\text{ s}^{-1} $$\nWe check the stability condition:\n$$ \\rho = \\frac{\\Lambda}{\\mu} = \\frac{4800}{6000} = 0.8 $$\nSince $\\rho = 0.8 < 1$, the system is stable, and our model is applicable.\n\nNext, we compute the expected time in the master's system, $E[W]$:\n$$ E[W] = \\frac{1}{\\mu - n\\lambda} = \\frac{1}{6000 - 4800} \\text{ s} = \\frac{1}{1200} \\text{ s} $$\nAs a decimal, $\\frac{1}{1200} \\approx 0.0008333... \\text{ s}$.\n\nNow, we calculate the expected end-to-end latency, $E[T_{e2e}]$:\n$$ E[T_{e2e}] = 2c + E[W] = 2 \\times (8 \\times 10^{-6} \\text{ s}) + \\frac{1}{1200} \\text{ s} $$\n$$ E[T_{e2e}] = 1.6 \\times 10^{-5} \\text{ s} + \\frac{1}{1200} \\text{ s} $$\n$$ E[T_{e2e}] = 0.000016 \\text{ s} + 0.0008333... \\text{ s} = 0.000849333... \\text{ s} $$\nThe problem requires the answer to be in milliseconds (ms). We convert the result from seconds to milliseconds:\n$$ E[T_{e2e}] = 0.000849333... \\text{ s} \\times 1000 \\frac{\\text{ms}}{\\text{s}} = 0.849333... \\text{ ms} $$\nFinally, we round the answer to four significant figures. The first four significant figures are $8, 4, 9, 3$. The following digit is $3$, which is less than $5$, so we do not round up.\n$$ E[T_{e2e}] \\approx 0.8493 \\text{ ms} $$", "answer": "$$\\boxed{0.8493}$$", "id": "3621335"}, {"introduction": "The true power of asymmetric multiprocessing lies not just in managing its overheads, but in actively exploiting its heterogeneity to boost performance. This exercise puts you in the role of an OS designer, tasking you with creating a scheduling policy that intelligently assigns tasks to specialized cores based on their behavior. You will quantify the throughput gain of your asymmetry-aware scheduler, demonstrating the significant benefits of matching workloads to the right hardware. [@problem_id:3621298]", "problem": "An operating systems laboratory explores Asymmetric Multiprocessing (AMP), in which a heterogeneous multiprocessor platform contains specialized worker cores. Consider an AMP system with $2$ worker cores and a separate dispatch core whose overhead can be neglected for steady-state throughput. One worker, denoted $W_{\\mathrm{IO}}$, is tuned for Input/Output (I/O)-bound tasks; the other, denoted $W_{\\mathrm{CPU}}$, is tuned for CPU-bound tasks. The lab asks students to design a scheduler that uses the I/O wait ratio $\\omega$ (defined as the long-run fraction of time a task is not runnable because it waits on I/O) to prefer $W_{\\mathrm{IO}}$ for I/O-bound tasks and $W_{\\mathrm{CPU}}$ for CPU-bound tasks. The system runs at a high multiprogramming level with a very large number of independent tasks so that the ready queue is effectively never empty.\n\nModel the ready CPU work as a stream of independent “compute quanta” of uniform size. Each quantum belongs to either an I/O-bound task or a CPU-bound task. Across a long observation window, the measured classification driven by $\\omega$ yields that a fraction $p$ of all ready quanta are I/O-bound and a fraction $1-p$ are CPU-bound, where $p=\\frac{4}{7}$. The per-quantum processing times (service times) on the two workers, in milliseconds, are:\n- On $W_{\\mathrm{IO}}$: $t_{\\mathrm{IO},i}=3\\,\\mathrm{ms}$ for I/O-bound quanta; $t_{\\mathrm{IO},c}=7\\,\\mathrm{ms}$ for CPU-bound quanta.\n- On $W_{\\mathrm{CPU}}$: $t_{\\mathrm{CPU},i}=6\\,\\mathrm{ms}$ for I/O-bound quanta; $t_{\\mathrm{CPU},c}=4\\,\\mathrm{ms}$ for CPU-bound quanta.\n\nAssume there is no shared bottleneck other than the two workers, no interference between workers, and that routing decisions do not introduce additional delay. Also assume perfect classification when the $\\omega$-aware policy is used: all I/O-bound quanta are identified and can be routed to $W_{\\mathrm{IO}}$, and all CPU-bound quanta to $W_{\\mathrm{CPU}}$. Under the baseline “$\\omega$-agnostic” policy, each worker receives the same mixture with I/O-bound fraction $p$.\n\nStarting only from fundamental definitions such as “throughput equals the long-run number of completed quanta divided by elapsed time” and the fact that, for a saturated single server with independent service times, the long-run throughput equals the reciprocal of the long-run average service time, derive and compute the throughput difference\n$$\\Delta = \\text{throughput}_{\\omega\\text{-aware}} - \\text{throughput}_{\\omega\\text{-agnostic}}.$$\nExpress your final answer as an exact value in quanta per second. Do not round.", "solution": "The problem requires the calculation of the throughput difference between two scheduling policies on an Asymmetric Multiprocessing (AMP) system. We begin by establishing the fundamental principles and definitions provided. The total system throughput, which we will denote as $R$, is defined as the long-run number of completed quanta divided by the elapsed time. The system consists of two worker cores, $W_{\\mathrm{IO}}$ and $W_{\\mathrm{CPU}}$, operating in parallel. In a saturated state, the throughput of a single server is the reciprocal of its long-run average service time.\n\nLet $p$ be the fraction of I/O-bound quanta, given as $p = \\frac{4}{7}$. Consequently, the fraction of CPU-bound quanta is $1 - p = 1 - \\frac{4}{7} = \\frac{3}{7}$. The service times for I/O-bound ($i$) and CPU-bound ($c$) quanta on each worker are given in milliseconds ($ms$):\n- On $W_{\\mathrm{IO}}$: $t_{\\mathrm{IO},i} = 3\\,\\mathrm{ms}$ and $t_{\\mathrm{IO},c} = 7\\,\\mathrm{ms}$.\n- On $W_{\\mathrm{CPU}}$: $t_{\\mathrm{CPU},i} = 6\\,\\mathrm{ms}$ and $t_{\\mathrm{CPU},c} = 4\\,\\mathrm{ms}$.\n\nFirst, we analyze the \"$\\omega$-agnostic\" policy.\nUnder this policy, both workers receive the same statistical mixture of quanta. Since the ready queue is never empty, both workers are saturated. The total throughput of the system is the sum of the individual throughputs of the two workers.\nFor a single worker, the throughput is the reciprocal of its average service time, $\\bar{t}$. The average service time is the weighted average of the service times for I/O-bound and CPU-bound quanta.\n\nFor worker $W_{\\mathrm{IO}}$, the average service time is:\n$$ \\bar{t}_{\\mathrm{IO}, \\text{agnostic}} = p \\cdot t_{\\mathrm{IO},i} + (1-p) \\cdot t_{\\mathrm{IO},c} = \\left(\\frac{4}{7}\\right)(3) + \\left(\\frac{3}{7}\\right)(7) = \\frac{12}{7} + \\frac{21}{7} = \\frac{33}{7}\\,\\mathrm{ms} $$\nThe throughput of worker $W_{\\mathrm{IO}}$ is:\n$$ R_{\\mathrm{IO}, \\text{agnostic}} = \\frac{1}{\\bar{t}_{\\mathrm{IO}, \\text{agnostic}}} = \\frac{1}{33/7} = \\frac{7}{33}\\,\\text{quanta/ms} $$\n\nFor worker $W_{\\mathrm{CPU}}$, the average service time is:\n$$ \\bar{t}_{\\mathrm{CPU}, \\text{agnostic}} = p \\cdot t_{\\mathrm{CPU},i} + (1-p) \\cdot t_{\\mathrm{CPU},c} = \\left(\\frac{4}{7}\\right)(6) + \\left(\\frac{3}{7}\\right)(4) = \\frac{24}{7} + \\frac{12}{7} = \\frac{36}{7}\\,\\mathrm{ms} $$\nThe throughput of worker $W_{\\mathrm{CPU}}$ is:\n$$ R_{\\mathrm{CPU}, \\text{agnostic}} = \\frac{1}{\\bar{t}_{\\mathrm{CPU}, \\text{agnostic}}} = \\frac{1}{36/7} = \\frac{7}{36}\\,\\text{quanta/ms} $$\n\nThe total throughput for the $\\omega$-agnostic policy is the sum of the individual throughputs:\n$$ \\text{throughput}_{\\omega\\text{-agnostic}} = R_{\\mathrm{IO}, \\text{agnostic}} + R_{\\mathrm{CPU}, \\text{agnostic}} = \\frac{7}{33} + \\frac{7}{36} = 7 \\left( \\frac{1}{33} + \\frac{1}{36} \\right) = 7 \\left( \\frac{12 + 11}{396} \\right) = 7 \\left( \\frac{23}{396} \\right) = \\frac{161}{396}\\,\\text{quanta/ms} $$\n\nNext, we analyze the \"$\\omega$-aware\" policy.\nUnder this policy, perfect classification routes all I/O-bound quanta to $W_{\\mathrm{IO}}$ and all CPU-bound quanta to $W_{\\mathrm{CPU}}$. We model this as two parallel processing pipelines. Consider a large number of total quanta, $N$. The number of I/O-bound quanta is $N_i = pN$, and the number of CPU-bound quanta is $N_c = (1-p)N$.\nThe time required for $W_{\\mathrm{IO}}$ to process all $N_i$ quanta is:\n$$ T_{\\mathrm{IO}} = N_i \\cdot t_{\\mathrm{IO},i} = (pN) \\cdot t_{\\mathrm{IO},i} $$\nThe time required for $W_{\\mathrm{CPU}}$ to process all $N_c$ quanta is:\n$$ T_{\\mathrm{CPU}} = N_c \\cdot t_{\\mathrm{CPU},c} = ((1-p)N) \\cdot t_{\\mathrm{CPU},c} $$\nSince the two workers operate in parallel, the total time $T$ to process the entire batch of $N$ quanta is determined by the slower of the two pipelines (the bottleneck):\n$$ T = \\max(T_{\\mathrm{IO}}, T_{\\mathrm{CPU}}) = \\max((pN) \\cdot t_{\\mathrm{IO},i}, ((1-p)N) \\cdot t_{\\mathrm{CPU},c}) $$\nThe total system throughput, based on its fundamental definition, is $R = N/T$:\n$$ \\text{throughput}_{\\omega\\text{-aware}} = \\frac{N}{\\max((pN) \\cdot t_{\\mathrm{IO},i}, ((1-p)N) \\cdot t_{\\mathrm{CPU},c})} = \\frac{1}{\\max(p \\cdot t_{\\mathrm{IO},i}, (1-p) \\cdot t_{\\mathrm{CPU},c})} $$\nWe now compute the values inside the $\\max$ function:\n$$ p \\cdot t_{\\mathrm{IO},i} = \\frac{4}{7} \\cdot 3 = \\frac{12}{7}\\,\\mathrm{ms} $$\n$$ (1-p) \\cdot t_{\\mathrm{CPU},c} = \\frac{3}{7} \\cdot 4 = \\frac{12}{7}\\,\\mathrm{ms} $$\nThe system is perfectly balanced under this policy, meaning neither worker is a bottleneck relative to the other. Therefore:\n$$ \\text{throughput}_{\\omega\\text{-aware}} = \\frac{1}{12/7} = \\frac{7}{12}\\,\\text{quanta/ms} $$\n\nFinally, we compute the throughput difference, $\\Delta$:\n$$ \\Delta = \\text{throughput}_{\\omega\\text{-aware}} - \\text{throughput}_{\\omega\\text{-agnostic}} = \\frac{7}{12} - \\frac{161}{396} $$\nTo subtract the fractions, we find a common denominator, which is $396 = 12 \\times 33$.\n$$ \\Delta = \\frac{7 \\cdot 33}{12 \\cdot 33} - \\frac{161}{396} = \\frac{231}{396} - \\frac{161}{396} = \\frac{231 - 161}{396} = \\frac{70}{396} $$\nSimplifying the fraction gives:\n$$ \\Delta = \\frac{35}{198}\\,\\text{quanta/ms} $$\nThe problem asks for the answer in quanta per second. Since there are $1000$ milliseconds in a second, we multiply our result by $1000$:\n$$ \\Delta_{\\text{quanta/s}} = \\frac{35}{198} \\times 1000 = \\frac{35000}{198} = \\frac{17500}{99} $$\nThe fraction $\\frac{17500}{99}$ is in simplest form.", "answer": "$$\\boxed{\\frac{17500}{99}}$$", "id": "3621298"}]}