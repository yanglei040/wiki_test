{"hands_on_practices": [{"introduction": "The scheduler's run queue is the heart of task management, and its efficiency is paramount. This exercise explores the classic engineering trade-off in data structure design, comparing a simple unsorted list to a more complex binary heap for managing runnable threads. By analyzing the costs of selection and reinsertion operations, you'll learn to make quantitative, evidence-based decisions that directly impact overall system performance. [@problem_id:3661227]", "problem": "A symmetric multiprocessor operating system with $p$ identical central processing unit (CPU) cores makes a scheduling decision every time a time slice expires on a core. Let the time slice be $\\tau$ seconds, so the system performs approximately $p / \\tau$ scheduling decisions per second when there is always work to run. Each scheduling decision picks the runnable thread with the highest dynamic priority from a global run queue of average length $l$ and then reinserts the preempted thread back into the run queue (assume long-running CPU-bound workloads so that each decision performs one selection and one reinsertion). The total per-decision scheduler overhead is the sum of the selection and reinsertion times. Two data-structure designs are considered for the global run queue:\n\n- Unsorted list:\n  - Selection (scan) time scales linearly in $l$: $t_{\\text{scan}}(l) = \\alpha \\, l + \\beta$.\n  - Reinsertion time is constant: $t_{\\text{enq}} = \\delta$.\n\n- Binary heap keyed by priority:\n  - Selection time scales logarithmically in $l$: $t_{\\text{sel,heap}}(l) = \\alpha_{h} \\, \\log_{2}(l) + \\beta_{h}$.\n  - Reinsertion time also scales logarithmically: $t_{\\text{enq,heap}}(l) = \\delta_{h} \\, \\log_{2}(l)$.\n\nHere $\\log_{2}(\\,\\cdot\\,)$ denotes the base-$2$ logarithm. Constants are obtained from microbenchmarks under the target platform:\n- $\\alpha = 0.03$ microseconds per element,\n- $\\beta = 0.20$ microseconds,\n- $\\delta = 0.05$ microseconds,\n- $\\alpha_{h} = 0.28$ microseconds,\n- $\\beta_{h} = 0.50$ microseconds,\n- $\\delta_{h} = 0.12$ microseconds.\n\nUsing only these definitions and facts, derive the per-decision overhead expressions for both designs and determine the run-queue length $l^{\\star}$ (treated as a real value) at which both designs have equal per-decision overhead. Report $l^{\\star}$ rounded to four significant figures. Because $l^{\\star}$ is a dimensionless count, no unit is required in the final answer.", "solution": "The objective is to determine the run-queue length $l^{\\star}$ at which the per-decision scheduler overhead of two different data-structure designs becomes equal. A scheduling decision consists of one selection and one reinsertion. The total per-decision overhead is the sum of the times for these two operations.\n\nFirst, we formulate the total per-decision overhead, $T(l)$, as a function of the average run-queue length $l$ for each design.\n\nFor the unsorted list design, the overhead $T_{\\text{list}}(l)$ is the sum of the selection (scan) time and the reinsertion (enqueue) time.\nThe selection time is given as $t_{\\text{scan}}(l) = \\alpha l + \\beta$.\nThe reinsertion time is given as a constant, $t_{\\text{enq}} = \\delta$.\nTherefore, the total overhead for the unsorted list is:\n$$T_{\\text{list}}(l) = t_{\\text{scan}}(l) + t_{\\text{enq}} = (\\alpha l + \\beta) + \\delta = \\alpha l + (\\beta + \\delta)$$\n\nFor the binary heap design, the overhead $T_{\\text{heap}}(l)$ is the sum of the selection time and the reinsertion time.\nThe selection time is given as $t_{\\text{sel,heap}}(l) = \\alpha_{h} \\log_{2}(l) + \\beta_{h}$.\nThe reinsertion time is given as $t_{\\text{enq,heap}}(l) = \\delta_{h} \\log_{2}(l)$.\nTherefore, the total overhead for the binary heap is:\n$$T_{\\text{heap}}(l) = t_{\\text{sel,heap}}(l) + t_{\\text{enq,heap}}(l) = (\\alpha_{h} \\log_{2}(l) + \\beta_{h}) + (\\delta_{h} \\log_{2}(l))$$\n$$T_{\\text{heap}}(l) = (\\alpha_{h} + \\delta_{h}) \\log_{2}(l) + \\beta_{h}$$\n\nThe problem asks for the run-queue length $l^{\\star}$ at which these two overheads are equal. We find this by setting $T_{\\text{list}}(l^{\\star}) = T_{\\text{heap}}(l^{\\star})$:\n$$\\alpha l^{\\star} + (\\beta + \\delta) = (\\alpha_{h} + \\delta_{h}) \\log_{2}(l^{\\star}) + \\beta_{h}$$\n\nNow, we substitute the provided numerical values for the constants:\n$\\alpha = 0.03$ $\\mu$s/element\n$\\beta = 0.20$ $\\mu$s\n$\\delta = 0.05$ $\\mu$s\n$\\alpha_{h} = 0.28$ $\\mu$s\n$\\beta_{h} = 0.50$ $\\mu$s\n$\\delta_{h} = 0.12$ $\\mu$s\n\nAll time constants are in microseconds ($\\mu$s), so we can treat them as dimensionless numbers for the purpose of solving the equation for the dimensionless quantity $l^{\\star}$.\n$$0.03 l^{\\star} + (0.20 + 0.05) = (0.28 + 0.12) \\log_{2}(l^{\\star}) + 0.50$$\n$$0.03 l^{\\star} + 0.25 = 0.40 \\log_{2}(l^{\\star}) + 0.50$$\nTo find the root $l^{\\star}$, we can rearrange this equation into the form $f(l) = 0$:\n$$f(l) = 0.03 l - 0.40 \\log_{2}(l) - 0.25 = 0$$\n\nThis is a transcendental equation, which cannot be solved for $l$ using elementary algebraic operations. We must find the root using numerical methods. Let's evaluate $f(l)$ at some test values to locate the root.\nFor $l=64$:\n$f(64) = 0.03(64) - 0.40 \\log_{2}(64) - 0.25 = 1.92 - 0.40(6) - 0.25 = 1.92 - 2.40 - 0.25 = -0.73$\nFor $l=128$:\n$f(128) = 0.03(128) - 0.40 \\log_{2}(128) - 0.25 = 3.84 - 0.40(7) - 0.25 = 3.84 - 2.80 - 0.25 = 0.79$\n\nSince $f(64)  0$ and $f(128) > 0$, and the function $f(l)$ is continuous for $l>0$, a root $l^{\\star}$ must exist in the interval $(64, 128)$. We can use a numerical root-finding algorithm, such as the bisection method or Newton's method, to find a more precise value.\n\nLet's use Newton's method. The derivative of $f(l)$ is required:\n$f'(l) = \\frac{d}{dl} \\left( 0.03 l - 0.40 \\frac{\\ln(l)}{\\ln(2)} - 0.25 \\right) = 0.03 - \\frac{0.40}{l \\ln(2)}$.\nThe iterative formula is $l_{n+1} = l_n - \\frac{f(l_n)}{f'(l_n)}$.\nLet's start with an initial guess of $l_0 = 100$.\n$f(100) = 0.03(100) - 0.40 \\log_{2}(100) - 0.25 \\approx 3 - 0.40(6.643856) - 0.25 \\approx 3 - 2.65754 - 0.25 = 0.09246$\n$f'(100) = 0.03 - \\frac{0.40}{100 \\ln(2)} \\approx 0.03 - \\frac{0.40}{69.3147} \\approx 0.03 - 0.00577 = 0.02423$\n$l_1 = 100 - \\frac{0.09246}{0.02423} \\approx 100 - 3.8159 = 96.1841$\n\nLet's perform a second iteration with $l_1 = 96.1841$:\n$f(96.1841) = 0.03(96.1841) - 0.40 \\log_{2}(96.1841) - 0.25 \\approx 2.885523 - 0.40(6.588079) - 0.25 = 2.885523 - 2.635232 - 0.25 \\approx 0.000291$\n$f'(96.1841) = 0.03 - \\frac{0.40}{96.1841 \\ln(2)} \\approx 0.03 - \\frac{0.40}{66.650} \\approx 0.03 - 0.00600 = 0.02400$\n$l_2 = 96.1841 - \\frac{0.000291}{0.2400} \\approx 96.1841 - 0.01213 = 96.17197$\n\nThe value has converged rapidly. Let's check the function value at $l=96.172$:\n$f(96.172) = 0.03(96.172) - 0.40 \\log_{2}(96.172) - 0.25 \\approx 2.88516 - 0.40(6.58793) - 0.25 = 2.88516 - 2.635172 - 0.25 = -0.000012$\nThis value is sufficiently close to zero. Thus, $l^{\\star} \\approx 96.172$.\n\nThe problem requires the answer to be rounded to four significant figures.\nThe value is $96.172...$. The first four significant figures are $9$, $6$, $1$, and $7$. The fifth digit is $2$, which is less than $5$, so we round down.\n$l^{\\star} \\approx 96.17$.\nThis means that for a run queue length of approximately $96$, the two designs have roughly the same overhead. For shorter queues, the unsorted list is more efficient, while for longer queues, the binary heap is superior.", "answer": "$$\n\\boxed{96.17}\n$$", "id": "3661227"}, {"introduction": "In multiprocessor systems, a fundamental tension exists between keeping all cores busy (load balancing) and keeping a thread's data nearby (locality). This practice places you in the role of an OS designer tasked with choosing a scheduling strategy for a Non-Uniform Memory Access (NUMA) machine, where memory access times vary dramatically based on location. You will analyze how different load balancing scopes affect cache performance and memory access latency, learning to reason about the critical trade-offs that maximize performance by respecting hardware topology. [@problem_id:3661196]", "problem": "A dual-socket multiprocessor has $N=8$ cores per socket. Each socket has a shared Last Level Cache (LLC) of capacity $C=16$ MiB and local Dynamic Random-Access Memory (DRAM). The machine implements Non-Uniform Memory Access (NUMA) with a first-touch allocation policy: a page is placed in the DRAM local to the socket on which it is first touched. A set of $T=16$ identical user threads executes a barrier-synchronized loop. In each iteration, a thread repeatedly traverses a private working set of size $w=1$ MiB and then participates in a barrier, after which it scans a read-mostly shared dataset of size $D=4$ MiB used by all threads in the same process. Short blocking events due to input/output cause a random subset of threads (on average $25\\%$ of them) to block for approximately $5$ ms; to maintain near-full utilization during these events, the Operating System (OS) must perform load balancing at intervals of approximately $1$ ms within whatever balancing domain is chosen.\n\nAssume the following foundational facts and definitions:\n- A cache hit occurs when the referenced data remains in the cache since its last access; capacity-induced misses occur when the aggregate actively used data exceeds the cache capacity.\n- If the aggregate working set actively contending for a given LLC during the relevant reuse window is less than the LLC capacity, the hit probability is high; if it exceeds capacity, the hit probability degrades due to evictions.\n- Under first-touch placement, migrating a thread to a different socket makes its subsequent page accesses remote until the pages are migrated; remote DRAM accesses are slower than local DRAM accesses.\n- The goal of “maximizing locality” here means maximizing the expected fraction of memory references that are either LLC hits or, on an LLC miss, are served by local DRAM rather than remote DRAM.\n\nYou are evaluating scheduler balancing scope choices $S$ to maximize locality under the stated utilization requirement. Consider these candidate policies:\n\nA. System-wide per-core balancing: a single global domain across both sockets rebalance run queues every $1$ ms with no preference for socket locality.\n\nB. Per-socket balancing: independent balancing domains per socket rebalance every $1$ ms within each socket; rare cross-socket rebalancing occurs only every $100$ ms to correct long-lived imbalance.\n\nC. No rebalancing after an initial random placement: each thread is pinned to its initially chosen core permanently.\n\nD. Cross-socket paired-core balancing: balance is performed every $1$ ms within fixed core pairs that span sockets (for each index $i$, core $i$ of socket $0$ is paired with core $i$ of socket $1$), allowing frequent cross-socket migrations within each pair.\n\nWhich choice of $S$ best maximizes locality as defined above while satisfying the utilization requirement?\n\nSelect one:\n\nA. System-wide per-core balancing\n\nB. Per-socket balancing\n\nC. No rebalancing after initial random placement\n\nD. Cross-socket paired-core balancing", "solution": "The problem describes a classic trade-off in multiprocessor scheduling on a Non-Uniform Memory Access (NUMA) machine. The goal is to maximize memory locality while satisfying a requirement for high utilization, which necessitates frequent load balancing.\n\nFirst, let's analyze the workload and memory system. The machine has $2$ sockets and $16$ total cores, with $T=16$ threads running. An initial placement of $8$ threads per socket is optimal. For each socket, the aggregate working set is the sum of the private data of its $8$ threads ($8 \\times w = 8 \\times 1 \\text{ MiB} = 8 \\text{ MiB}$) and the shared data ($D=4 \\text{ MiB}$), totaling $12 \\text{ MiB}$. This fits comfortably within the per-socket LLC capacity of $C=16 \\text{ MiB}$, ensuring high cache hit rates as long as the threads stay on their home socket. Due to the first-touch policy, each thread's private data resides in the DRAM local to its socket, maximizing NUMA locality.\n\nThe key challenge is the load balancing requirement. On average, $25\\%$ of threads ($4$ out of $16$) are blocked, creating idle cores. The OS must migrate running threads to these idle cores.\n\nWe evaluate the options based on how they handle this migration:\n\n*   **A. System-wide per-core balancing:** This policy treats all $16$ cores as a single pool. To balance load, it will frequently migrate threads between sockets (e.g., from a busy core on socket 0 to an idle core on socket 1). Such cross-socket migrations are devastating for locality. The thread's private data becomes remote, and its working set in the original socket's LLC is lost. This policy prioritizes utilization at the complete expense of locality. **This is incorrect.**\n\n*   **B. Per-socket balancing:** This policy creates two balancing domains, one for each socket. Frequent load balancing ($1$ ms) happens only *within* a socket. A thread on socket 0 can be moved to any other core on socket 0, but not to socket 1. This is the ideal approach. It preserves NUMA locality perfectly, as threads never leave their home socket. It also preserves LLC locality, as all cores on a socket share the same LLC. It balances load effectively within the socket (e.g., if 2 threads block, the remaining 6 threads can be distributed among the 8 cores). The rare cross-socket balancing handles severe, long-term imbalances without causing constant thrashing. **This is the correct approach.**\n\n*   **C. No rebalancing after an initial random placement:** This policy pins threads to cores, maximizing locality. However, it fails the problem's explicit requirement to maintain high utilization. When a thread blocks, its core becomes idle, leading to an average of $25\\%$ of cores being unused. **This is incorrect.**\n\n*   **D. Cross-socket paired-core balancing:** This policy creates rigid pairs of cores across sockets. It allows frequent migration between the two cores in a pair. This is a form of cross-socket migration and suffers from the same locality destruction as option A. A thread from socket 0 could be moved to socket 1, making its memory accesses remote and invalidating its cache state. It is also an ineffective load balancing strategy, as it cannot move work between pairs. **This is incorrect.**\n\nTherefore, per-socket balancing is the only policy that successfully meets the utilization requirement while respecting the hardware's NUMA topology to maximize memory locality.", "answer": "$$\\boxed{B}$$", "id": "3661196"}, {"introduction": "After understanding the importance of NUMA locality, a practical question arises: what should the scheduler do when a thread is already running on the \"wrong\" node, far from its data? This exercise provides a concrete model for this tactical decision, framing it as a cost-benefit analysis. You will weigh the one-time cost of migrating a thread against the ongoing penalty of slow, remote memory access, developing a quantitative framework for a core function of modern, NUMA-aware schedulers. [@problem_id:3661192]", "problem": "You are analyzing scheduling decisions for threads that are mis-placed with respect to their memory node on a Non-Uniform Memory Access (NUMA) machine. Each thread has remaining compute work that would take $w_i$ seconds if executed where its memory is local. If the thread continues to execute where memory is remote, the memory-induced slowdown factor is $\\sigma_i \\ge 1$, so its effective work becomes $\\sigma_i \\cdot w_i$ seconds. Alternatively, the operating system may migrate the thread back to its memory’s node, paying a one-time migration cost $r_i$ seconds due to cache warm-up and state transfer, after which the thread executes at its local speed, for a total effective work of $r_i + w_i$ seconds.\n\nAssume there are $P$ identical processors and the scheduler is work-conserving, preemptive, and can arbitrarily divide and interleave the execution of threads (infinitely divisible work model). There are no additional overheads beyond the migration cost $r_i$, and all threads are available at time $t = 0$. You must use these base definitions to reason from first principles.\n\nYour task is to implement a program that, for each test case, computes the difference in the total completion time (makespan in seconds) between:\n- The policy that leaves all threads where they are (no migrations at all), and\n- The policy that independently decides for each thread whether to migrate or not in order to minimize the total completion time under the given model.\n\nFor determinism, when $r_i + w_i = \\sigma_i \\cdot w_i$, break ties by choosing to leave the thread where it is (do not migrate).\n\nExpress all final results in seconds, as decimal numbers rounded to six digits after the decimal point.\n\nTest Suite:\nFor each test case, you are given the number of processors $P$, the number of threads $N$, and arrays $w$, $\\sigma$, and $r$.\n\n- Test case $1$:\n  - $P = 4$, $N = 3$.\n  - $w = (10, 5, 8)$ seconds.\n  - $\\sigma = (2.0, 1.5, 3.0)$.\n  - $r = (2.0, 1.0, 1.5)$ seconds.\n- Test case $2$:\n  - $P = 2$, $N = 4$.\n  - $w = (3, 7, 2, 6)$ seconds.\n  - $\\sigma = (1.2, 1.8, 1.0, 2.5)$.\n  - $r = (0.5, 5.0, 0.1, 10.0)$ seconds.\n- Test case $3$:\n  - $P = 8$, $N = 2$.\n  - $w = (4, 4)$ seconds.\n  - $\\sigma = (2.0, 2.0)$.\n  - $r = (4.0, 0.0)$ seconds.\n- Test case $4$:\n  - $P = 3$, $N = 3$.\n  - $w = (5, 5, 5)$ seconds.\n  - $\\sigma = (1.1, 1.1, 1.1)$.\n  - $r = (0.0, 0.0, 0.0)$ seconds.\n- Test case $5$:\n  - $P = 1$, $N = 3$.\n  - $w = (10, 10, 1)$ seconds.\n  - $\\sigma = (1.5, 1.2, 3.0)$.\n  - $r = (100.0, 3.0, 1.0)$ seconds.\n- Test case $6$:\n  - $P = 2$, $N = 2$.\n  - $w = (9, 1)$ seconds.\n  - $\\sigma = (1.0, 2.0)$.\n  - $r = (1.0, 0.2)$ seconds.\n\nFinal Output Format:\nYour program should produce a single line of output containing the differences in makespan, one per test case, as a comma-separated list enclosed in square brackets, with each number rounded to exactly six digits after the decimal point. For example: $\\text{[x\\_1,x\\_2,\\dots,x\\_6]}$.\n\nNo input should be read from standard input. All constants are embedded in the program. The only acceptable output is the single line in the specified format.", "solution": "The problem is based on a standard \"infinitely divisible work\" scheduling model. In this model, the total available processing capacity of $P$ processors is treated as a single resource that collectively works on a total pool of computational work, $W_{\\text{total}}$. The time to complete all tasks (the makespan, $T_{\\text{makespan}}$) is simply the total work divided by the number of processors.\n\n$$T_{\\text{makespan}} = \\frac{W_{\\text{total}}}{P}$$\n\nTo find the difference in makespan between the \"no-migration\" policy and the \"optimal\" policy, we must first calculate the total work under each policy. The makespan difference will then be the difference in total work, divided by $P$.\n\n**1. Work Under No-Migration Policy ($W_{\\text{total},1}$)**\nUnder this policy, every thread $i$ executes remotely, incurring a slowdown factor $\\sigma_i$. The computational work for thread $i$ is $\\sigma_i \\cdot w_i$. The total work for all $N$ threads is the sum of these individual work amounts:\n$$W_{\\text{total},1} = \\sum_{i=1}^{N} \\sigma_i \\cdot w_i$$\n\n**2. Work Under Optimal Policy ($W_{\\text{total},2}$)**\nThe optimal policy minimizes the total work by making an independent, optimal decision for each thread. For each thread $i$, we compare the work required to run remotely versus the work required to migrate and then run locally.\n- Work if remote: $W_{i,\\text{remote}} = \\sigma_i \\cdot w_i$\n- Work if migrated: $W_{i,\\text{migrated}} = r_i + w_i$\n\nThe optimal policy chooses the smaller of these two values. The tie-breaking rule states that if they are equal, the thread is not migrated. The optimal work for thread $i$, $W_{i,2}$, is:\n$$W_{i,2} = \\min(\\sigma_i \\cdot w_i, r_i + w_i)$$\nThe total work for the optimal policy is the sum of these individual minimums:\n$$W_{\\text{total},2} = \\sum_{i=1}^{N} W_{i,2}$$\n\n**3. Makespan Difference ($\\Delta T$)**\nThe required makespan difference is $\\Delta T = T_1 - T_2$.\n$$\\Delta T = \\frac{W_{\\text{total},1}}{P} - \\frac{W_{\\text{total},2}}{P} = \\frac{1}{P} (W_{\\text{total},1} - W_{\\text{total},2})$$\nThis can be rewritten as the total work saved by the optimal policy, divided by $P$. The work saved for a single thread $i$, $S_i$, is non-zero only if migrating is strictly better.\n$$S_i = \\begin{cases} (\\sigma_i \\cdot w_i) - (r_i + w_i)  \\text{if } r_i + w_i  \\sigma_i \\cdot w_i \\\\ 0  \\text{otherwise} \\end{cases}$$\nThe final formula for the makespan difference is the sum of savings for all threads, divided by the number of processors:\n$$\\Delta T = \\frac{1}{P} \\sum_{i=1}^{N} S_i$$\nThis algorithm is applied to each test case to compute the final result.", "answer": "[6.000000,0.350000,0.500000,0.500000,1.000000,0.400000]", "id": "3661192"}]}