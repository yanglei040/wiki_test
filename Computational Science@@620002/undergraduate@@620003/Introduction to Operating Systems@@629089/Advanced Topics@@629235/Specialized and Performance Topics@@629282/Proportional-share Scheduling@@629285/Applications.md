## Applications and Interdisciplinary Connections

Now that we have acquainted ourselves with the simple and beautiful machinery of proportional sharing, we might be tempted to think of it as a neat, but perhaps narrow, tool for dividing up a processor’s time. Nothing could be further from the truth. We are about to embark on a journey that will take us from the heart of a single computer to the vast, globe-spanning data centers that power our digital world, and even into domains that seem to have nothing to do with computers at all. We will discover that this one elegant idea—the simple rule of dividing a resource according to weights—is a recurring theme, a universal principle of arbitration that nature, or at least the nature of engineering, seems to favor.

To warm up our intuition, let's consider a scenario far from the world of silicon and electrons: a chess tournament [@problem_id:3659893]. Imagine a director running an event with several boards, where players have different priorities (perhaps based on their ranking). The goal is to ensure that higher-priority players get proportionally more playing time. How does the director decide who plays in each round? They could maintain a running tally of each player's total time played, $T_i$, and their priority weight, $w_i$. At the start of each round, the director simply calculates the ratio $T_i/w_i$ for every available player. This value represents the player's "normalized" playing time. To keep things fair, the director will always choose the players with the *lowest* normalized time. This simple act of always giving a chance to the most "neglected" player (relative to their importance) is the essence of proportional sharing. It's an algorithm you could run with a pencil and paper, yet as we will see, it is powerful enough to orchestrate the most complex systems on Earth.

### The Modern Computer: A Society of Programs

Let's begin our exploration inside a single computer. We no longer see it as a monolithic calculator, but as a bustling society of programs, all clamoring for the attention of the Central Processing Unit (CPU). The operating system is the government of this society, and proportional-share scheduling is one of its most fundamental laws of justice.

Consider a computer in a university lab, shared by several students. Some are running quick, interactive programs (like text editors), while others have submitted long, computationally-intensive "batch" jobs. A simple policy might be to give each *program* an equal share of the CPU. But is that fair to the *users*? One user running a single interactive session would get the same total CPU time as another user running twenty batch jobs, which doesn't seem right. A clever system administrator can do better. Using proportional-share, they can establish a social contract: every *user* should get an equal slice of the computing pie, regardless of how many programs they are running. This is achieved not by changing the scheduler's core logic, but by intelligently assigning the weights. By giving a much higher weight to an interactive session than to a batch job, the system can be tuned so that the total weighted "pull" of each user on the CPU ends up being equal [@problem_id:3673646]. The scheduler itself remains simple; the complex policy is enacted merely through the choice of weights.

This society of programs often has a hierarchy. In modern systems, it's common to group processes into containers or virtual machines, which are then grouped under users. This creates a nested, tree-like structure of resource rights, much like a set of Russian dolls. The principle of proportional sharing extends to this hierarchy with beautiful simplicity. Imagine a physical machine that runs two virtual machines, $G_1$ and $G_2$, with hypervisor-level weights $w_{G_1}$ and $w_{G_2}$. $G_1$ gets a fraction $\frac{w_{G_1}}{w_{G_1} + w_{G_2}}$ of the physical CPU. Now, inside $G_1$, two processes, $A$ and $B$, have their own weights, $w_A$ and $w_B$. Process $A$ will receive a fraction $\frac{w_A}{w_A+w_B}$ of the resources allocated to its parent, $G_1$. Its *effective* share of the total physical machine is therefore simply the product of these shares: a share of a share [@problem_id:3673601]. This same elegant, multiplicative logic applies directly to the resource-management "control groups" ([cgroups](@entry_id:747258)) in Linux, which form the foundation of container technologies like Docker and Kubernetes [@problem_id:3628619]. A container's CPU share is determined by its weight relative to other containers, and the processes inside that container then subdivide that share among themselves.

Of course, life is not always so simple. What happens if a process needs to wait for something, like data from a slow disk? A naive proportional-share scheduler would simply stop giving it CPU time, effectively punishing it for not being "ready". When it becomes ready again, it has fallen behind. This is particularly unfair to interactive applications, which often wait for user input. More sophisticated schedulers solve this with a "compensation-credit" mechanism [@problem_id:3673609]. While a high-priority task is blocked, it accrues "credit" for the CPU time it *would* have received. When it wakes up, it's allowed to temporarily "spend" this credit, running with higher priority to catch up. This ensures that a process's long-term share of the CPU truly reflects its weight, not its blocking patterns.

Proportional sharing can also be combined with other forms of control, like hard limits or quotas. A system administrator might declare that a certain group of processes can get *at most* 20% of the CPU, but within that limit, they should share the allocation proportionally to their weights. This is precisely how modern container platforms work [@problem_id:3673679]. If the group, in its fair-share allotment, would have received 30% of the CPU, it is "throttled" and gets only its 20% quota. What happens to the extra 10%? Because the scheduler is "work-conserving"—it never sits idle if there's work to do—this leftover capacity is not wasted. It is automatically and gracefully redistributed among all other unconstrained groups, again in proportion to their weights. This creates a wonderfully flexible system that both enforces hard limits and efficiently utilizes every last drop of capacity.

### Beyond the CPU: Sharing Everything

The true power of this idea becomes apparent when we realize the "resource" being shared doesn't have to be CPU time. It can be anything that is divisible.

Let's look at a disk drive. We can schedule requests to the disk using proportional-share. But what is the "quantum" of service? A natural choice is one I/O operation (an "IOP"). The scheduler can ensure that over the long run, different processes get to perform a number of I/O operations proportional to their weights. But here we encounter a profound subtlety [@problem_id:3673644]. What if one process is reading tiny 8 KB files and another is reading huge 256 KB files? Each operation has a very different cost in time. A scheduler that guarantees fairness in the *number of operations* will be grossly unfair in the amount of *time* the disk is busy serving each process. The process making small, cheap requests will consume far less device time than its weight would suggest.

This reveals the heart of the matter: fairness is defined by what you count. If you want to share *time* fairly, you must count *time*. This is the principle behind Weighted Fair Queuing (WFQ), a cousin of proportional-share where the "quantum" is not an operation, but an estimate of the actual service time of that operation.

This same issue of non-uniform service quanta appears in many places. Consider a modern Graphics Processing Unit (GPU), where applications submit "kernels" of work that may run for several milliseconds. Crucially, once a kernel starts, it often cannot be preempted—it must run to completion [@problem_id:3673688]. This non-preemptible interval is an indivisible chunk of service. An amazing result from scheduling theory shows a beautiful connection: the problem of scheduling non-preemptible GPU kernels is, in its essence, the very same problem a network router faces when sending data packets of different sizes [@problem_id:3673666]. The math doesn't know if it's a pixel shader or a video frame; it only sees an indivisible chunk of work. In such systems, perfect, instantaneous fairness is impossible. If the scheduler decides to run a long 6ms kernel, another, higher-priority process might have to wait. The theory provides a powerful guarantee: the maximum "fairness error"—the deviation from an ideal, perfectly fluid allocation—is bounded by the length of the longest non-preemptible chunk of work in the system.

### The Global Village: Clusters and Clouds

Having mastered the sharing of resources on a single machine, we can now zoom out to the scale of entire data centers. A modern cloud application is not a single program, but a collection of "pods" or containers spread across a cluster of many physical machines. The cluster orchestrator, like Kubernetes, must play the role of a global scheduler.

Suppose we have a cluster of nodes, each with a different computing capacity, and a set of pods, each with a desired resource weight. How can we place the pods on the nodes to achieve cluster-wide proportional fairness? The solution is astonishingly elegant [@problem_id:3673669]. Global fairness is achieved if, for every single node in the cluster, the ratio of the node's capacity to the sum of the weights of the pods it hosts is constant. This constant must equal the ratio of the *total cluster capacity* to the *total weight of all pods*. This provides a simple, local rule for achieving a complex global goal. An orchestrator can use this principle to find a "balanced" placement where no pod is on an "overcrowded" machine relative to its weight.

What happens when some applications don't need their full proportional share? A video streaming service might be capped by network conditions, leaving its CPU share partially unused. A simple proportional-share scheduler might just let that capacity go to waste. A better approach, often called weighted max-min fairness, addresses this directly [@problem_id:3636290]. It first satisfies the stated demands of all "modest" applications. Then, it takes all the remaining capacity and divides *that* among the "greedy" applications that can use more, once again in proportion to their weights. It's a two-step process that respects both need and entitlement.

Perhaps the most brilliant extension of proportional sharing is to the world of multiple, distinct resources. An application might need a certain amount of CPU, but also a certain amount of [memory bandwidth](@entry_id:751847), and a certain amount of disk I/O. How can we be fair in this multi-dimensional world? If we give you your fair share of CPU, but starve you of memory, you can't make progress. The answer is a beautiful idea called Dominant Resource Fairness (DRF) [@problem_id:3673677]. For each application, we look at all the resources it uses and find the one it is "hogging" the most, relative to the total system capacity. That is its "dominant resource". DRF then works to equalize the shares of that dominant resource across all applications. This prevents a user who is bottlenecked on only one resource from being unfairly penalized. It ensures that the system's "most constrained" resource is the one being shared most equitably, which is exactly what our intuition of fairness would demand.

### A Universal Principle of Arbitration

By now, we see that proportional-share is more than just a CPU [scheduling algorithm](@entry_id:636609). It's a general-purpose mechanism for arbitrating access to any scarce resource among competing entities. Consider an online advertising system [@problem_id:3673695]. When a user visits a webpage, the system must choose which ad to show. Each advertising campaign has a "budget" that functions as its weight. The goal is to show a number of impressions for each campaign that is proportional to its budget. For every single impression, the ad server must make a choice. A purely random "lottery" scheduler, where the probability of being chosen matches the budget share, is fair on average, but can have large deviations in the short term. A deterministic algorithm like [stride scheduling](@entry_id:755526), which we saw in our chess example, can make a choice for every single impression that keeps the cumulative allocation incredibly close to the ideal, providing a deterministic bound on the fairness error.

This journey, from a simple CPU to a complex, multi-resource cloud, has revealed a profound unity. The same fundamental principle of weighted sharing, refined and adapted, provides an elegant and powerful solution to a vast array of problems. It governs the microscopic dance of processes inside a single chip, orchestrates the macroscopic ballet of containers in a data center, and even decides which advertisement you see next. It is a testament to the power of simple, beautiful ideas in engineering the complex world around us.