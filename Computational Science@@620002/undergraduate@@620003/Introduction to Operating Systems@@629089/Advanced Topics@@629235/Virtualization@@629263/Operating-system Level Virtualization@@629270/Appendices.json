{"hands_on_practices": [{"introduction": "A core feature of OS-level virtualization is the ability to control how much of a shared resource, like the CPU, each container gets. This exercise moves beyond simply knowing that CPU shares exist and challenges you to derive the exact allocation behavior from the fundamental principles of Linux's Completely Fair Scheduler ($\\text{CFS}$). By working through this derivation, you will gain a quantitative understanding of performance isolation and resource guarantees [@problem_id:3665364].", "problem": "You are configuring operating-system level virtualization with Linux control groups (cgroups) on a host that exposes exactly $1$ central processing unit (CPU) of capacity. There are $k$ containers, each with a single, always-runnable, CPU-bound process, assigned control-group CPU shares $w_1, w_2, \\dots, w_k$ where each $w_i \\in \\mathbb{R}_{>0}$. The host uses the Linux Completely Fair Scheduler (CFS). Use only the following fundamental facts about CFS with cgroup shares as your starting point:\n- Each runnable entity (here, each container’s schedulable entity) maintains a virtual runtime $v_i$. While entity $i$ runs for an amount of real time $\\Delta t$, its virtual runtime increases by an amount proportional to $\\Delta t$ and inversely proportional to its weight $w_i$, so that the instantaneous rate satisfies $\\frac{dv_i}{dt} \\propto \\frac{1}{w_i}$ when $i$ is running, and $\\frac{dv_i}{dt} = 0$ when it is not running.\n- The scheduler always selects the entity with the smallest $v_i$ to run next, and over time attempts to keep all $v_i$ equalized among runnable entities.\nAssume total demand strictly exceeds capacity in steady state (that is, all $k$ entities remain runnable at all times), so the CPU is fully utilized. Over any horizon during which the set of runnable entities does not change, let $f_i$ denote the expected fraction of CPU time allocated to container $i$, so that $f_i \\in (0,1)$ and $\\sum_{i=1}^{k} f_i = 1$.\n\nTask:\n1. Starting from the two facts above and no others, derive the steady-state expression for $f_i$ in terms of $w_1, \\dots, w_k$.\n2. Express your final result as a closed-form analytic expression for the vector $\\mathbf{f} = (f_1, f_2, \\dots, f_k)$ as unitless fractions of one CPU. No numerical substitution is required.\n3. Brief discussion (no calculation needed for grading): In this idealized model without minimum-granularity effects, argue whether starvation can occur for any container with $w_i &gt; 0$, and give a bound (in terms of a fairness window of length $L &gt; 0$) on the maximum time between two runs of container $i$.\n\nProvide the final answer as the expression for $\\mathbf{f}$ only. Since your answer is symbolic, no rounding is required. The fractions are unitless, as required.", "solution": "### Part 1: Derivation of the Steady-State CPU Fraction $f_i$\n\nThe problem provides two fundamental facts governing the scheduler's behavior. We will use these, and only these, to derive the expression for $f_i$, the fraction of CPU time allocated to container $i$.\n\nThe givens are:\n- A single CPU, so total capacity is $1$.\n- $k$ containers with CPU shares $w_1, w_2, \\dots, w_k$, where each $w_i \\in \\mathbb{R}_{>0}$.\n- All $k$ containers are always runnable.\n- The total CPU time is fully utilized, so $\\sum_{i=1}^{k} f_i = 1$.\n- Fact 1: The rate of change of virtual runtime $v_i$ for a running container $i$ is $\\frac{dv_i}{dt} \\propto \\frac{1}{w_i}$. Let $c$ be the positive constant of proportionality. Thus, when container $i$ is running, its virtual runtime increases at a rate of $\\frac{dv_i}{dt} = \\frac{c}{w_i}$. When it is not running, $\\frac{dv_i}{dt} = 0$.\n- Fact 2: The scheduler always selects the runnable entity with the minimum virtual runtime $v_i$ and, over time, aims to maintain $v_i \\approx v_j$ for all runnable entities $i,j$.\n\nIn a steady-state system observed over a sufficiently long time interval of duration $T$, the scheduler's effort to equalize all virtual runtimes implies that the total accumulated virtual runtime, $\\Delta v_i$, must be the same for every container.\n$$ \\Delta v_1 = \\Delta v_2 = \\dots = \\Delta v_k $$\n\nLet $T_i$ be the total amount of real CPU time that container $i$ runs during this interval $T$. The fraction of CPU time for container $i$ is defined as $f_i$. In the context of our interval $T$, this means $f_i = \\frac{T_i}{T}$, or $T_i = f_i T$. Since the CPU is always busy, the sum of the run times for all containers must equal the total duration of the interval:\n$$ \\sum_{i=1}^{k} T_i = T $$\n\nThe accumulated virtual runtime $\\Delta v_i$ for container $i$ is the integral of its rate of change over the interval $T$. Since the rate is zero when the container is not running, we only need to consider the total time $T_i$ during which it was running at the constant rate $\\frac{c}{w_i}$.\n$$ \\Delta v_i = \\left( \\frac{c}{w_i} \\right) T_i $$\n\nUsing the steady-state condition that all total accumulated virtual runtimes are equal, we can equate the expressions for any two containers $i$ and $j$:\n$$ \\Delta v_i = \\Delta v_j $$\n$$ \\left( \\frac{c}{w_i} \\right) T_i = \\left( \\frac{c}{w_j} \\right) T_j $$\n\nSince $c > 0$, we can divide by $c$ to obtain:\n$$ \\frac{T_i}{w_i} = \\frac{T_j}{w_j} $$\nThis shows that the ratio of the total run time to the weight is a constant for all containers. Let us denote this constant by $K_T$.\n$$ \\frac{T_i}{w_i} = K_T \\implies T_i = K_T w_i $$\n\nNow, we use the constraint that the CPU is fully utilized. We substitute the expression for $T_i$ into the sum:\n$$ \\sum_{i=1}^{k} (K_T w_i) = T $$\n$$ K_T \\sum_{i=1}^{k} w_i = T $$\n\nWe can solve for the constant of proportionality $K_T$:\n$$ K_T = \\frac{T}{\\sum_{j=1}^{k} w_j} $$\nNote that the denominator $\\sum_{j=1}^{k} w_j$ is strictly positive since all $w_j > 0$.\n\nNow we substitute this expression for $K_T$ back into the equation for $T_i$:\n$$ T_i = \\left( \\frac{T}{\\sum_{j=1}^{k} w_j} \\right) w_i $$\n\nFinally, we find the fraction of CPU time $f_i$ by dividing $T_i$ by $T$:\n$$ f_i = \\frac{T_i}{T} = \\frac{1}{T} \\left( \\frac{T w_i}{\\sum_{j=1}^{k} w_j} \\right) = \\frac{w_i}{\\sum_{j=1}^{k} w_j} $$\nThis is the steady-state expression for the fraction of CPU time allocated to container $i$.\n\n### Part 2: Vector Expression for $\\mathbf{f}$\n\nThe problem requires expressing the result as a vector $\\mathbf{f} = (f_1, f_2, \\dots, f_k)$. Using the derived expression for each component $f_i$, we have:\n$$ \\mathbf{f} = \\left( \\frac{w_1}{\\sum_{j=1}^{k} w_j}, \\frac{w_2}{\\sum_{j=1}^{k} w_j}, \\dots, \\frac{w_k}{\\sum_{j=1}^{k} w_j} \\right) $$\n\n### Part 3: Discussion on Starvation and Run Intervals\n\n**Starvation:** In this idealized model, starvation for any container $i$ with an assigned weight $w_i > 0$ cannot occur. The scheduler's core principle is to always run the entity with the minimum virtual runtime $v_i$. When any other container $j \\neq i$ runs, its virtual runtime $v_j$ increases, while $v_i$ remains constant. Eventually, the virtual runtimes of all other running containers will surpass $v_i$. At that point, container $i$ will have the minimum virtual runtime among all runnable entities, and the scheduler will be forced to select it for execution. Because $w_i > 0$, the rate of increase of $v_i$ is finite, ensuring it does not remain the minimum indefinitely. This mechanism guarantees that every runnable container will eventually be scheduled.\n\n**Bound on Time Between Runs:** The maximum time between two successive executions of container $i$ can be reasoned about in terms of an idealized fairness window of length $L > 0$. In such a window, container $i$ receives a total run time of $f_i L$. The worst-case scenario for container $i$ occurs immediately after it has finished running, at which point its virtual runtime is maximal relative to the other containers. It must then wait for the other $k-1$ containers to run long enough for one of their virtual runtimes to exceed its own, or more simply, for its own virtual runtime to become the minimum again. A loose but intuitive upper bound on this wait time is the time it would take for all other containers to receive their proportional share of the CPU within the fairness window $L$. This time is the sum of the shares of all other containers, which is $(1 - f_i)L$. Substituting the derived expression for $f_i$, this maximum wait time is bounded by:\n$$ L \\left(1 - \\frac{w_i}{\\sum_{j=1}^{k} w_j}\\right) = L \\frac{\\sum_{j \\neq i} w_j}{\\sum_{j=1}^{k} w_j} $$\nIn a real CFS implementation, this period $L$ is related to the `sched_latency` parameter, which defines the target time period over which every runnable task should run at least once.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{w_1}{\\sum_{j=1}^{k} w_j} & \\frac{w_2}{\\sum_{j=1}^{k} w_j} & \\dots & \\frac{w_k}{\\sum_{j=1}^{k} w_j}\n\\end{pmatrix}\n}\n$$", "id": "3665364"}, {"introduction": "While some resources are divided, others, like the kernel's page cache, are shared to maximize efficiency. This practice explores the nuanced rules of memory sharing and accounting for containerized applications reading the same file [@problem_id:3665429]. By analyzing this scenario, you will clarify common misconceptions about memory duplication and learn how control groups (cgroups) track memory usage, which is crucial for both performance and stability.", "problem": "Two Linux containers, $C_1$ and $C_2$, run on the same host kernel with Linux control groups version $2$ (cgroups v2). Both containers have a bind mount to the same large read-only file of size $S = 2\\,\\text{GiB}$ on the host, and both perform a sequential buffered read (for example, using the system call $\\text{read}()$ in a loop) until end-of-file. The kernel page size is $p = 4\\,\\text{KiB}$. The memory controller for each container is configured as follows: for $C_1$, $\\text{memory.high} = 600\\,\\text{MiB}$ and $\\text{memory.max} = 1\\,\\text{GiB}$; for $C_2$, $\\text{memory.high} = 400\\,\\text{MiB}$ and $\\text{memory.max} = 512\\,\\text{MiB}$. Assume the file is not in the page cache initially.\n\nUse the following fundamental base to reason about page cache sharing and memory accounting behavior:\n\n- Definition: In a monolithic kernel shared by all containers, the page cache is keyed by the tuple $(\\text{inode}, \\text{offset})$. For any given $(\\text{inode}, \\text{offset})$, at most one cached page exists in the host kernel’s page cache.\n- Fact: Under cgroups v2, the memory controller charges page cache pages to the memory control group (denoted $\\text{memcg}$) that first instantiates them (for example, on the first page fault or readahead that populates the page into the cache). Subsequent accesses by tasks in other $\\text{memcg}$ groups reuse the cached page without duplicating it; the charged owner of the page remains the original $\\text{memcg}$.\n- Fact: $\\text{memory.high}$ is a soft limit. When a $\\text{memcg}$ usage exceeds $\\text{memory.high}$, the kernel attempts reclaim and applies proportional throttling (delaying further charging) rather than immediate killing; exceeding $\\text{memory.high}$ does not, by itself, guarantee an Out-Of-Memory kill.\n- Fact: $\\text{memory.max}$ is a hard limit. If a new charge would exceed $\\text{memory.max}$, the kernel first tries to reclaim within the $\\text{memcg}$; if sufficient reclaim is not possible, the charge fails, and the kernel may trigger an Out-Of-Memory kill within that $\\text{memcg}$.\n\nAssume typical sequential buffered I/O where the kernel performs readahead and can reclaim clean page cache pages behind the read cursor. Let $n = S / p$ be the total number of file pages, and let $w$ denote the approximate steady-state number of file pages charged to a $\\text{memcg}$ while streaming the file (the working set, determined by readahead and pipeline depth), with $w \\ll n$ under normal conditions.\n\nWhich of the following statements are correct in this scenario?\n\nA. The host page cache stores each $(\\text{inode}, \\text{offset})$ page only once, so $C_1$ and $C_2$ reuse the same cached pages; container boundaries do not cause duplicate cache pages for the same file content.\n\nB. Each container maintains its own separate page cache namespace, so reading the same file from both containers duplicates the cache and roughly doubles host memory usage for those pages.\n\nC. If $C_1$ reads first and populates the cache, then when $C_2$ reads the same file afterward, $C_2$’s $\\text{memory.current}$ does not increase by the size of the already cached pages, because those pages remain charged to $C_1$.\n\nD. If a container’s $\\text{memcg}$ usage exceeds $\\text{memory.high}$, the kernel immediately performs an Out-Of-Memory kill within that $\\text{memcg}$; by contrast, exceeding $\\text{memory.max}$ only causes throttling without kill.\n\nE. Because $S = 2\\,\\text{GiB}$ while $C_1$ has $\\text{memory.max} = 1\\,\\text{GiB}$, $C_1$ inevitably fails to read the file to completion: the page cache must hold all $n$ pages concurrently, and the hard limit prevents this.\n\nF. If $C_2$ reads the file after $C_1$ has populated the page cache, $C_2$ can proceed at near full speed without being throttled by its $\\text{memory.high}$, provided $C_2$ does not perform additional allocations that are charged to $C_2$ beyond minor transient kernel overhead.\n\nSelect all that apply.", "solution": "The core principles governing this scenario are as follows:\n1.  **Unified Page Cache**: The Linux kernel on the host maintains a single, global page cache. Any page of any file is uniquely identified by its inode and the offset within that file, represented by the tuple $(\\text{inode}, \\text{offset})$. Regardless of how many processes or containers access it, a given page exists at most once in this cache.\n2.  **Cgroups v2 Memory Accounting**: When a file page is first read from disk and populated into the page cache, its memory cost is \"charged\" to the memory control group ($\\text{memcg}$) of the process that triggered the read. The `memory.current` counter of that $\\text{memcg}$ is incremented. If a process from a different $\\text{memcg}$ later accesses this same page, it reuses the existing cached page, and no new charge is incurred. The memory charge remains with the original $\\text{memcg}$.\n3.  **Streaming I/O**: A sequential read of a large file does not require the entire file to be resident in memory. The kernel reads data in a \"window\" of pages, employing readahead to fetch upcoming pages and reclaiming old pages that are no longer needed (i.e., those \"behind\" the current read position). The concurrent memory usage for file data, denoted $w$, is therefore typically much smaller than the total file size $S$ (i.e., $w \\ll n$, where $n=S/p$).\n4.  **Memory Limits**: For a given $\\text{memcg}$, $\\text{memory.high}$ is a soft limit that triggers memory reclaim and throttling when exceeded. $\\text{memory.max}$ is a hard limit; an attempt to allocate memory that would push usage beyond this limit will fail, potentially leading to an Out-Of-Memory (OOM) kill.\n\nBased on these principles, we evaluate each statement.\n\n**A. The host page cache stores each $(\\text{inode}, \\text{offset})$ page only once, so $C_1$ and $C_2$ reuse the same cached pages; container boundaries do not cause duplicate cache pages for the same file content.**\n\nThis statement is a direct consequence of the fundamental design of the Linux page cache. The problem's \"Definition\" explicitly states: \"In a monolithic kernel shared by all containers, the page cache is keyed by the tuple $(\\text{inode}, \\text{offset})$. For any given $(\\text{inode}, \\text{offset})$, at most one cached page exists...\". Containers $C_1$ and $C_2$ run on the same kernel and access the same file (same inode). Therefore, they share a single instance of each file page in the host kernel's page cache. Containerization isolates many resources, but the page cache is unified to maximize memory efficiency.\n\n**Verdict: Correct**\n\n**B. Each container maintains its own separate page cache namespace, so reading the same file from both containers duplicates the cache and roughly doubles host memory usage for those pages.**\n\nThis statement contradicts the principle of a unified page cache. As established in the analysis of A, Linux does not create separate page caches for each container. Doing so would defeat a primary benefit of memory management: sharing identical data to conserve physical memory. This statement describes a behavior that is fundamentally incorrect for Linux containers.\n\n**Verdict: Incorrect**\n\n**C. If $C_1$ reads first and populates the cache, then when $C_2$ reads the same file afterward, $C_2$’s $\\text{memory.current}$ does not increase by the size of the already cached pages, because those pages remain charged to $C_1$.**\n\nThis statement accurately describes the cgroups v2 memory accounting rule for page cache. The problem \"Fact\" states: \"Under cgroups v2, the memory controller charges page cache pages to the memory control group ($\\text{memcg}$) that first instantiates them... the charged owner of the page remains the original $\\text{memcg}$.\" Since $C_1$ reads the file first, its $\\text{memcg}$ is charged for populating the pages. When $C_2$ subsequently reads the file, it finds the pages in the cache and reuses them. The charge for these pages is not transferred to $C_2$'s $\\text{memcg}$, so $C_2$'s `memory.current` usage will not increase from accessing this pre-existing cached data.\n\n**Verdict: Correct**\n\n**D. If a container’s $\\text{memcg}$ usage exceeds $\\text{memory.high}$, the kernel immediately performs an Out-Of-Memory kill within that $\\text{memcg}$; by contrast, exceeding $\\text{memory.max}$ only causes throttling without kill.**\n\nThis statement incorrectly reverses the functions of $\\text{memory.high}$ and $\\text{memory.max}$. The problem's \"Facts\" clearly define their roles:\n- $\\text{memory.high}$: A soft limit. Exceeding it causes throttling and reclaim, not an OOM kill.\n- $\\text{memory.max}$: A hard limit. Exceeding it can trigger an OOM kill if reclaim is insufficient.\nThis statement claims a kill for `high` and only throttling for `max`, which is the opposite of their actual behavior.\n\n**Verdict: Incorrect**\n\n**E. Because $S = 2\\,\\text{GiB}$ while $C_1$ has $\\text{memory.max} = 1\\,\\text{GiB}$, $C_1$ inevitably fails to read the file to completion: the page cache must hold all $n$ pages concurrently, and the hard limit prevents this.**\n\nThis statement is based on the false premise that reading a file requires holding the entire file in memory simultaneously. The problem describes a \"sequential buffered read,\" which implies a streaming operation. The kernel maintains a moving window of pages in the cache, with size $w$, where $w$ is the working set size. The problem correctly states that for such a workload, $w \\ll n$. As long as the memory for this window ($w \\times p$) plus any other memory used by the container stays below the $\\text{memory.max}$ of $1\\,\\text{GiB}$, the read will proceed. A typical readahead window is on the order of megabytes, far below the $1\\,\\text{GiB}$ limit.\n\n**Verdict: Incorrect**\n\n**F. If $C_2$ reads the file after $C_1$ has populated the page cache, $C_2$ can proceed at near full speed without being throttled by its $\\text{memory.high}$, provided $C_2$ does not perform additional allocations that are charged to $C_2$ beyond minor transient kernel overhead.**\n\nThis statement is a logical consequence of statements A and C. Since $C_1$ has already populated the page cache, $C_2$'s read requests will be served from memory (cache hits), which is extremely fast (\"near full speed\"). Furthermore, as established in C, the memory for these pages remains charged to $C_1$. Thus, $C_2$'s `memory.current` will not increase significantly from reading the file. Since its memory usage remains low, it will not approach its $\\text{memory.high}$ limit of $400\\,\\text{MiB}$. As a result, $C_2$ will not be subject to the memory-pressure-induced throttling associated with exceeding $\\text{memory.high}$.\n\n**Verdict: Correct**", "answer": "$$\\boxed{ACF}$$", "id": "3665429"}, {"introduction": "Containers are not virtual machines; they share the host kernel, which has profound implications for security and compatibility. This final practice places you in a realistic scenario where an application's system call requirements conflict with the host kernel's capabilities [@problem_id:3665412]. Analyzing this problem will reveal the critical role of security mechanisms like $\\text{seccomp}$ and the subtle but important interactions between applications, libraries, and the kernel in a sandboxed environment.", "problem": "A containerized application is built against a recent version of the GNU C Library (glibc) 2.36 and makes use of newer Linux system calls such as `openat2` and `pidfd_open`. The container runs on an x86_64 host with Linux kernel version 5.4, which does not implement those new system calls. The container runtime applies a default Secure Computing Mode (`seccomp`) profile whose default action for any non-listed system call is to return an error to the caller (that is, the filter uses an action equivalent to “return error” rather than “allow” or “kill”). Assume the filter explicitly denies `openat2` and `pidfd_open` and allows a conventional baseline such as `openat`, `fstat`, `mmap`, `mprotect`, `futex`, and `rt_sigaction`. \n\nFrom first principles, consider the following facts:\n\n- In operating-system level virtualization, processes inside containers share the same host kernel and its system call Application Binary Interface (ABI). There is no separate container kernel.\n- A system call is an invocation from user space into the kernel via a well-defined system call ABI. If the kernel does not implement the requested system call, the kernel returns $-1$ and sets the global error indicator `errno` to `ENOSYS` (function not implemented). If the kernel denies a call for sandboxing via `seccomp`, the kernel returns $-1$ and sets `errno` according to the filter’s selected error action (commonly `EPERM` for operation not permitted), or it may deliver a signal depending on the chosen action.\n- The GNU C Library (glibc) provides wrappers for system calls and may attempt fallbacks to older, widely implemented system call sequences if it detects `ENOSYS` (indicating lack of implementation). It does not perform such fallbacks when the error is `EPERM` (indicating a permission failure), because `EPERM` denotes a policy denial rather than an absent implementation.\n\nPredict the behavior when the application attempts to use `openat2` on this host under the described `seccomp` profile, and choose the option that both correctly explains the outcome and recommends a safe profile adjustment that preserves sandboxing while enabling compatibility.\n\nA. Because the container shares the host kernel, `openat2` on Linux 5.4 with a `seccomp` default “return error” action that sets `errno` = `EPERM` will return $-1$ with `EPERM`, and glibc will not fallback. A safe compatibility fix is to change the `seccomp` action for newer system calls such as `openat2` and `pidfd_open` to “return `ENOSYS`” and explicitly allow the older equivalents (for example, `openat` and related calls needed by the fallback), keeping restrictive actions for genuinely risky calls.\n\nB. The container has its own kernel separate from the host, so glibc 2.36 will see the correct implementation and the `seccomp` profile has no effect on `openat2`; the call succeeds. No profile changes are needed.\n\nC. glibc always falls back when a call fails, including on `EPERM`, so denying `openat2` is harmless. The correct safe profile is to continue returning `EPERM` for `openat2` and `pidfd_open` without allowing any older equivalents.\n\nD. Denying `openat2` via `seccomp` will deliver `SIGSYS` that glibc automatically catches and emulates with `openat`, so no specific allowlist changes are required; the safest profile is to trap all new calls and rely on glibc to emulate them in user space.\n\nSelect the single best option.", "solution": "The correct outcome is determined by tracing the execution flow of the system call from the application, through the C library and the `seccomp` filter, to the kernel.\n\n1.  **Application Call and `glibc` Wrapper:** The application is linked against `glibc` 2.36, which is a modern version of the library. When the application performs a file-opening operation, the `glibc` wrapper function attempts to use the most advanced and feature-rich system call available, which in this case is `openat2`.\n\n2.  **`seccomp` Interception:** The process, running inside a container, makes a system call to the shared host kernel. Before the kernel's dispatcher handles the `openat2` request, the `seccomp` filter associated with the process intercepts it.\n\n3.  **Filter Action and `errno`:** The problem states the `seccomp` profile denies `openat2` with a \"return error\" action that sets `errno` to `EPERM` (Operation not permitted). The system call is rejected by the filter before the kernel even checks for its implementation. The kernel therefore returns `-1` to the user-space caller and sets `errno` to `EPERM`.\n\n4.  **`glibc` Fallback Logic:** The `glibc` wrapper receives the `-1` return value and inspects `errno`. As stated in the problem's principles, `glibc` distinguishes between error codes. It interprets `ENOSYS` (Function not implemented) as a signal to try an older, fallback system call (like `openat`). However, it interprets `EPERM` as a deliberate security policy denial. In this case, `glibc` will *not* attempt a fallback. It preserves the `EPERM` error and returns failure to the application.\n\n5.  **Outcome & Fix:** The application's call fails with an \"Operation not permitted\" error. The correct fix is to adjust the `seccomp` profile to enable `glibc`'s built-in compatibility mechanism. Instead of returning `EPERM`, the action for `openat2` (and other new, unimplemented syscalls) should be to return `ENOSYS`. When `glibc` sees `ENOSYS`, it will correctly fall back to the older `openat` system call, which is both implemented by the Linux 5.4 kernel and allowed by the `seccomp` profile, allowing the operation to succeed.\n\n**Analyzing the Options:**\n\n*   **A:** This option correctly describes the entire chain of events: `seccomp` returns `EPERM`, `glibc` does not fall back, and the correct solution is to change the `seccomp` action to return `ENOSYS` to trigger the fallback. This is the correct analysis.\n*   **B:** This option is incorrect because containers share the host kernel; they do not have their own.\n*   **C:** This option is incorrect because it falsely claims `glibc` falls back on an `EPERM` error.\n*   **D:** This option is incorrect because it assumes a `trap` action (`SIGSYS`) is used, whereas the problem describes a \"return error\" action. The standard `glibc` fallback mechanism relies on `ENOSYS`, not signal handling.\n\nTherefore, option A provides the correct explanation and the appropriate, secure solution.", "answer": "$$\\boxed{A}$$", "id": "3665412"}]}