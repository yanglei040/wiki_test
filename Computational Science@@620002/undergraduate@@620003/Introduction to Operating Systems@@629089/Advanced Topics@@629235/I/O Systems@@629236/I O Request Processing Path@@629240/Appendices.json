{"hands_on_practices": [{"introduction": "To improve the performance of any system, you first need to know where the bottlenecks are. An I/O request travels through multiple layers of the operating system before reaching the hardware. This practice ([@problem_id:3648623]) challenges you to think like a performance analyst by breaking down the total end-to-end latency into the time spent in each distinct stage, from the system call to the device driver. By conceptually placing measurement probes and analyzing a time-stamped trace, you will develop a concrete understanding of the I/O path's structure and the first step in performance diagnosis.", "problem": "A single-threaded application issues a synchronous file read system call in a Linux-like operating system. The request traverses, in order, the following software and hardware layers: system call boundary, filesystem layer, block I/O layer, device driver, and the storage device. For the purposes of analysis, assume this path occurs once without retries and that the layers execute sequentially with no overlap. You are allowed to place tracing probes at kernel and driver boundaries. Your goal is to design where to measure and to use a provided trace to compute the end-to-end latency.\n\nTasks:\n- Propose concrete measurement points, one at each boundary between adjacent layers along the I/O path and at the device’s completion signal, so that the time spent in each layer is captured as a non-overlapping interval and the sum across layers equals the total time from system call entry to return to user space. Justify briefly why your chosen points make the layer durations additive and disjoint.\n- A kernel trace captured the following timestamps, in chronological order, for one such I/O. All timestamps are in microseconds relative to an arbitrary reference. Let the ordered points be $P_{0}, P_{1}, P_{2}, P_{3}, P_{4}, P_{5}, P_{6}, P_{7}, P_{8}, P_{9}$ with numeric values:\n  - $P_{0} = 0$\n  - $P_{1} = 8$\n  - $P_{2} = 33$\n  - $P_{3} = 73$\n  - $P_{4} = 85$\n  - $P_{5} = 285$\n  - $P_{6} = 300$\n  - $P_{7} = 310$\n  - $P_{8} = 319$\n  - $P_{9} = 325$\n  Assume $P_{0}$ corresponds to the instant the system call enters the kernel and $P_{9}$ corresponds to the instant the system call returns to user space. The intermediate points correspond to internal layer boundaries and the device completion, consistent with a well-instrumented I/O path as you proposed.\n- Using your proposed measurement points mapped to the given $P_{i}$ sequence, compute the end-to-end latency from system call entry to return.\n\nExpress the final latency in microseconds and round your answer to four significant figures.", "solution": "The problem statement is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\n- **Application**: A single-threaded application issues a synchronous file read system call.\n- **Operating System**: A Linux-like operating system.\n- **I/O Path Layers**: The request traverses, in order: system call boundary, filesystem layer, block I/O layer, device driver, and the storage device.\n- **Execution Model**: Layers execute sequentially with no overlap; the path occurs once without retries.\n- **Measurement Task**: Propose concrete measurement points at each boundary to capture time spent in each layer as non-overlapping intervals, such that their sum equals the total time.\n- **Trace Data**: A chronological sequence of timestamps $P_{0}, P_{1}, \\dots, P_{9}$ in microseconds ($\\mu s$).\n  - $P_{0} = 0$\n  - $P_{1} = 8$\n  - $P_{2} = 33$\n  - $P_{3} = 73$\n  - $P_{4} = 85$\n  - $P_{5} = 285$\n  - $P_{6} = 300$\n  - $P_{7} = 310$\n  - $P_{8} = 319$\n  - $P_{9} = 325$\n- **Timestamp Mapping**:\n  - $P_{0}$ corresponds to the system call entry into the kernel.\n  - $P_{9}$ corresponds to the system call return to user space.\n  - Intermediate points $P_1, \\dots, P_8$ correspond to internal layer boundaries and device completion.\n- **Calculation Task**: Compute the end-to-end latency from system call entry to return.\n- **Output Requirement**: Express the final latency in microseconds and round to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the established validation criteria.\n- **Scientifically Grounded**: The problem describes the I/O request processing path, a fundamental concept in operating systems, which is a core discipline within computer science and engineering. The specified layers (filesystem, block I/O, device driver) and the notion of tracing with probes are standard and factually correct.\n- **Well-Posed**: The problem is well-posed. It requests a conceptual design of measurement points and a specific calculation for which all necessary data ($P_0$ and $P_9$) are explicitly provided.\n- **Objective**: The problem is stated in precise, technical language, free of subjectivity or ambiguity.\n- **Flaw Analysis**:\n  1.  **Scientific or Factual Unsoundness**: None. The model is a valid simplification for analysis. The provided timestamps are plausible for a fast storage device like an SSD.\n  2.  **Non-Formalizable or Irrelevant**: The problem is formalizable and is directly relevant to its stated topic, the *I/O request processing path* in an *introduction to operating systems*.\n  3.  **Incomplete or Contradictory Setup**: None. The problem provides all necessary information to calculate the end-to-end latency. The conceptual task of defining measurement points is self-contained and does not conflict with the calculation.\n  4.  **Unrealistic or Infeasible**: None. The physical model and data are consistent with reality for modern hardware.\n  5.  **Ill-Posed or Poorly Structured**: None. The question is clear and admits a unique, stable solution.\n  6.  **Pseudo-Profound, Trivial, or Tautological**: The final calculation is straightforward, as it is derived directly from the start and end timestamps provided. However, this is embedded within a task requiring conceptual knowledge of the OS I/O stack. This structure tests both careful reading and domain knowledge, and is not considered a flaw but rather a valid pedagogical approach. The problem is not trivial in its entirety.\n  7.  **Outside Scientific Verifiability**: None. The concepts and calculations are verifiable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A reasoned solution will be provided.\n\n---\n\n### Solution\n\nThe problem requires two main tasks: first, to propose a set of measurement points that partition the total I/O latency into disjoint, additive components corresponding to different layers of the I/O stack; and second, to calculate the end-to-end latency using the provided trace data.\n\n#### Task 1: Proposing Measurement Points\n\nA synchronous I/O request involves a request path (descending through the software stack to the hardware) and a completion path (ascending from the hardware back to the user process). To capture the time spent in each distinct stage, we must place probes at the entry and exit points of each layer for both paths. Let the measurement points be denoted by $M_i$.\n\n1.  $M_0$: **System call entry**. The moment the execution context switches from user mode to kernel mode to handle the read system call. This corresponds to the given $P_0$.\n2.  $M_1$: **Filesystem entry (request path)**. The generic system call handler has dispatched the request to the virtual filesystem (VFS) layer. The interval $[M_0, M_1]$ represents the system call frontend overhead.\n3.  $M_2$: **Block I/O layer entry (request path)**. The filesystem has completed its tasks (e.g., path resolution, permission checks, translating file-offset to logical block address) and issues a request to the block I/O layer. The interval $[M_1, M_2]$ is the time spent in the filesystem on the request path.\n4.  $M_3$: **Device driver entry (request path)**. The block I/O layer has processed the request (e.g., I/O scheduling, request merging) and is dispatching it to the appropriate device driver. The interval $[M_2, M_3]$ is the time spent in the block I/O layer.\n5.  $M_4$: **Request issued to hardware**. The device driver has programmed the hardware controller with the I/O command. At this point, the CPU can be scheduled to another task while the device is busy. The interval $[M_3, M_4]$ is the time spent in the driver on the request path.\n6.  $M_5$: **Device completion interrupt**. The storage device has completed the read operation and signals the CPU via an interrupt. The interval $[M_4, M_5]$ represents the hardware service time, including any on-device queueing.\n7.  $M_6$: **Block I/O layer notified (completion path)**. The device driver's interrupt handler has finished its work (e.g., checking status, acknowledging the interrupt) and is propagating the completion up to the block I/O layer. The interval $[M_5, M_6]$ is the interrupt handling time within the driver.\n8.  $M_7$: **Filesystem notified (completion path)**. The block I/O layer has completed its post-processing (e.g., waking up waiting processes, handling request queue updates) and notifies the filesystem. The interval $[M_6, M_7]$ is the block layer's completion handling time.\n9.  $M_8$: **System call finalization**. The filesystem has completed its final tasks (e.g., copying the read data from kernel buffers to the user-space buffer, updating file metadata like access time). The system call is now ready to return. The interval $[M_7, M_8]$ is the filesystem's completion handling time.\n10. $M_9$: **System call return**. The kernel finishes the system call execution, and the context switches back to user mode, unblocking the application. This corresponds to the given $P_9$. The interval $[M_8, M_9]$ represents the system call backend and context switch overhead.\n\nThese ten measurement points, $M_0$ through $M_9$, define nine contiguous, non-overlapping time intervals. The duration of each stage $k$ is $T_k = M_k - M_{k-1}$ for $k \\in \\{1, 2, \\dots, 9\\}$. The total end-to-end latency is the sum of these durations:\n$$ \\text{Total Latency} = \\sum_{k=1}^{9} (M_k - M_{k-1}) $$\nThis forms a telescoping sum:\n$$ (M_1 - M_0) + (M_2 - M_1) + \\dots + (M_9 - M_8) = M_9 - M_0 $$\nThus, the sum of the durations of these disjoint intervals is exactly the total time elapsed from system call entry ($M_0$) to system call return ($M_9$), satisfying the problem's requirements.\n\n#### Task 2: Calculating End-to-End Latency\n\nThe problem asks for the end-to-end latency from system call entry to return. By definition, this is the total time elapsed between these two events.\n\nThe given trace data includes:\n- $P_0 = 0 \\, \\mu s$: The timestamp for system call entry.\n- $P_9 = 325 \\, \\mu s$: The timestamp for system call return.\n\nThe intermediate points $P_1$ through $P_8$ correspond to the internal measurement points $M_1$ through $M_8$ proposed above. While these points are necessary to analyze an individual layer's performance, they are not needed to calculate the *total end-to-end latency*.\n\nThe end-to-end latency, $L$, is calculated directly as the difference between the final and initial timestamps:\n$$ L = P_9 - P_0 $$\nSubstituting the given values:\n$$ L = 325 \\, \\mu s - 0 \\, \\mu s = 325 \\, \\mu s $$\nThe problem requires the answer to be rounded to four significant figures. The number $325$ has three significant figures. To express this value with four significant figures, we write it as $325.0$.\n\nTherefore, the end-to-end latency is $325.0 \\, \\mu s$.", "answer": "$$\n\\boxed{325.0}\n$$", "id": "3648623"}, {"introduction": "Not all I/O paths are created equal. The presence of a page cache introduces a critical fork in the road: a fast, memory-only path for a cache hit, and a much slower path to the storage device for a miss. This exercise ([@problem_id:3648639]) moves from analyzing a single trace to building a probabilistic performance model. You will derive the expected latency of a read request, demonstrating how the cache hit rate ($p$) determines the average performance and highlights the immense impact of caching on system responsiveness.", "problem": "A process on a general-purpose operating system issues synchronous reads for fixed-size pages. The operating system employs a page cache: for each read, either the requested page is already in memory (a cache hit) or it must be fetched from the storage device (a cache miss). Consider the Input/Output (I/O) request processing path for a single read as follows:\n- On a cache hit, the path is application $\\rightarrow$ kernel page cache $\\rightarrow$ memory copy to user space. The end-to-end latency is a constant $L_{\\text{cache}}$.\n- On a cache miss, the path is application $\\rightarrow$ virtual file system layer $\\rightarrow$ file system $\\rightarrow$ block layer $\\rightarrow$ device driver $\\rightarrow$ storage device $\\rightarrow$ completion back through the kernel and memory copy. The end-to-end latency is a constant $L_{\\text{device}}$.\n\nAssume the following scientifically reasonable and commonly used modeling assumptions for advanced analysis:\n- Each read independently has a cache-hit probability $\\Pr(\\text{hit}) = p$ and cache-miss probability $\\Pr(\\text{miss}) = 1 - p$.\n- Latencies $L_{\\text{cache}}$ and $L_{\\text{device}}$ are deterministic constants for the respective paths.\n- There is no queuing overlap or concurrency; analyze a single read in isolation.\n\nStarting from the axiomatic definition of expectation in probability and the law of total expectation, derive a general expression for the expected latency $E[L]$ of a single read in terms of $p$, $L_{\\text{cache}}$, and $L_{\\text{device}}$. Then evaluate your expression numerically for the parameters $p = 0.93$, $L_{\\text{cache}} = 0.12$ milliseconds, and $L_{\\text{device}} = 7.4$ milliseconds. Round your final numerical answer to four significant figures. Express the final latency in milliseconds.", "solution": "The problem statement is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. The model described is a standard and simplified representation of I/O latency in computer operating systems, and the given parameters are realistic. Therefore, the problem is valid, and we may proceed with the solution.\n\nLet $L$ be the random variable representing the end-to-end latency of a single read request. The problem describes two mutually exclusive and exhaustive outcomes for this read: a cache hit or a cache miss. Let $H$ denote the event of a cache hit and $M$ denote the event of a cache miss. The set of events $\\{H, M\\}$ forms a partition of the sample space.\n\nAccording to the problem statement, the probabilities of these events are given as:\n- The probability of a cache hit is $\\Pr(H) = p$.\n- The probability of a cache miss is $\\Pr(M) = 1 - p$.\n\nThe latency of the read is a deterministic constant conditional on which event occurs:\n- If a cache hit occurs (event $H$), the latency is $L | H = L_{\\text{cache}}$.\n- If a cache miss occurs (event $M$), the latency is $L | M = L_{\\text{device}}$.\n\nWe are tasked with deriving the expected latency, $E[L]$, starting from fundamental principles, specifically the law of total expectation. The law of total expectation states that for any random variable $X$ and a partition of the sample space $\\{A_i\\}$, the expected value of $X$ is given by:\n$$E[X] = \\sum_i E[X | A_i] \\Pr(A_i)$$\n\nApplying this law to our random variable $L$ and the partition $\\{H, M\\}$, we can write the expected latency as:\n$$E[L] = E[L | H] \\Pr(H) + E[L | M] \\Pr(M)$$\n\nNext, we must evaluate the conditional expectations $E[L | H]$ and $E[L | M]$. The axiomatic definition of the expectation of a discrete random variable $Y$ that takes a single value $c$ with probability $1$ is $E[Y] = c \\cdot 1 = c$.\nIn our case, the random variable 'latency, given a cache hit' ($L|H$) takes only the single, constant value $L_{\\text{cache}}$. Therefore, its expected value is:\n$$E[L | H] = L_{\\text{cache}}$$\nSimilarly, the random variable 'latency, given a cache miss' ($L|M$) takes only the single, constant value $L_{\\text{device}}$. Its expected value is:\n$$E[L | M] = L_{\\text{device}}$$\n\nSubstituting these conditional expectations and the given probabilities into the expression for $E[L]$:\n$$E[L] = (L_{\\text{cache}}) \\cdot \\Pr(H) + (L_{\\text{device}}) \\cdot \\Pr(M)$$\nThis yields the general expression for the expected latency:\n$$E[L] = p \\cdot L_{\\text{cache}} + (1 - p) \\cdot L_{\\text{device}}$$\n\nNow, we evaluate this expression numerically using the provided parameters:\n- $p = 0.93$\n- $L_{\\text{cache}} = 0.12$ milliseconds\n- $L_{\\text{device}} = 7.4$ milliseconds\n\nSubstituting these values into our derived formula:\n$$E[L] = (0.93) \\cdot (0.12) + (1 - 0.93) \\cdot (7.4)$$\n$$E[L] = (0.93) \\cdot (0.12) + (0.07) \\cdot (7.4)$$\n\nWe calculate each term separately:\n- The contribution from cache hits: $(0.93) \\cdot (0.12) = 0.1116$ milliseconds.\n- The contribution from cache misses: $(0.07) \\cdot (7.4) = 0.518$ milliseconds.\n\nSumming the two contributions gives the total expected latency:\n$$E[L] = 0.1116 + 0.518 = 0.6296 \\text{ milliseconds}$$\n\nThe problem requires the final answer to be rounded to four significant figures. The calculated value $0.6296$ already has exactly four significant figures (the digits $6$, $2$, $9$, $6$), so no further rounding is necessary.", "answer": "$$\\boxed{0.6296}$$", "id": "3648639"}, {"introduction": "An operating system rarely serves just one I/O request at a time; it constantly juggles requests from many competing processes. The I/O scheduler acts as a traffic controller, deciding which request goes next to balance fairness and efficiency. In this practice ([@problem_id:3648722]), you will model a Weighted Fair Queuing (WFQ) scheduler, a powerful mechanism for providing performance guarantees to different classes of applications. By applying principles from queuing theory, you will calculate the expected latency for each class, gaining insight into how an OS can manage a shared I/O path to deliver predictable service.", "problem": "An operating system block I/O scheduler uses Weighted Fair Queuing (WFQ) to arbitrate service among three request classes that feed into a single disk device. WFQ is modeled as an approximation to Generalized Processor Sharing (GPS): when multiple classes are backlogged, class $i$ is guaranteed a minimum fraction of the device service capacity proportional to its weight $w_i$, namely a share $\\phi_i$ given by the ratio of its weight to the total weight. The device can complete requests at an aggregate rate $C$ requests per second when driven continuously. Assume each class generates requests as a Poisson process with arrival rate $\\lambda_i$ requests per second, and per-request service times are independent and exponentially distributed. Assume the scheduler enforces minimum-rate shares continuously so that each class $i$ experiences an effective dedicated server of rate $\\mu_i = \\phi_i C$ while it is backlogged.\n\nUsing the following parameters:\n- Class weights: $w_1 = 2$, $w_2 = 1$, $w_3 = 3$.\n- Device capacity: $C = 30000$ requests per second.\n- Arrival rates: $\\lambda_1 = 9000$ requests per second, $\\lambda_2 = 3500$ requests per second, $\\lambda_3 = 12000$ requests per second.\n\nStarting from the core definitions of WFQ share mapping and standard birth–death analysis for a single-server queue with Poisson arrivals and exponential service, derive from first principles an analytic expression for the steady-state mean request latency (sojourn time) $T_i$ for each class $i$ under this scheduler model, and then compute the numerical values for $T_1$, $T_2$, and $T_3$. Express each latency in milliseconds, and round your answers to four significant figures. Report the three latencies in a single row, in the order $(T_1, T_2, T_3)$.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- Scheduler model: Weighted Fair Queuing (WFQ), modeled as an approximation to Generalized Processor Sharing (GPS).\n- Number of request classes: $3$.\n- Class weights: $w_1 = 2$, $w_2 = 1$, $w_3 = 3$.\n- Total device capacity: $C = 30000$ requests per second.\n- Arrival process: Poisson for each class $i$ with rate $\\lambda_i$.\n- Arrival rates: $\\lambda_1 = 9000$ requests per second, $\\lambda_2 = 3500$ requests per second, $\\lambda_3 = 12000$ requests per second.\n- Service time distribution: Independent and exponentially distributed for each request.\n- Effective service rate for a backlogged class $i$: $\\mu_i = \\phi_i C$, where the share $\\phi_i = \\frac{w_i}{\\sum_j w_j}$.\n- Objective: Derive the analytical expression for the steady-state mean request latency (sojourn time) $T_i$ for each class and compute the numerical values for $T_1$, $T_2$, and $T_3$ in milliseconds, rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem describes a standard scenario in the performance analysis of computer systems, specifically queuing theory applied to I/O scheduling. The model (WFQ approximated by GPS leading to independent M/M/1 queues) is a well-established and scientifically sound framework. All necessary parameters ($\\lambda_i$, $w_i$, $C$) are provided.\n\nWe must verify the stability of the system. The total weight is $W = w_1 + w_2 + w_3 = 2 + 1 + 3 = 6$.\nThe guaranteed service shares are:\n- $\\phi_1 = \\frac{w_1}{W} = \\frac{2}{6} = \\frac{1}{3}$\n- $\\phi_2 = \\frac{w_2}{W} = \\frac{1}{6}$\n- $\\phi_3 = \\frac{w_3}{W} = \\frac{3}{6} = \\frac{1}{2}$\n\nThe effective service rates for each class are:\n- $\\mu_1 = \\phi_1 C = \\frac{1}{3} \\times 30000 \\text{ s}^{-1} = 10000 \\text{ s}^{-1}$\n- $\\mu_2 = \\phi_2 C = \\frac{1}{6} \\times 30000 \\text{ s}^{-1} = 5000 \\text{ s}^{-1}$\n- $\\mu_3 = \\phi_3 C = \\frac{1}{2} \\times 30000 \\text{ s}^{-1} = 15000 \\text{ s}^{-1}$\n\nFor a queue to be stable, its arrival rate must be less than its service rate. We check the utilization $\\rho_i = \\lambda_i / \\mu_i$ for each class:\n- $\\rho_1 = \\frac{\\lambda_1}{\\mu_1} = \\frac{9000}{10000} = 0.9$. Since $0.9 < 1$, class $1$ is stable.\n- $\\rho_2 = \\frac{\\lambda_2}{\\mu_2} = \\frac{3500}{5000} = 0.7$. Since $0.7 < 1$, class $2$ is stable.\n- $\\rho_3 = \\frac{\\lambda_3}{\\mu_3} = \\frac{12000}{15000} = 0.8$. Since $0.8 < 1$, class $3$ is stable.\n\nSince all individual queues are stable, the system as described is well-posed and has a steady-state solution. The problem is complete, consistent, and scientifically sound.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe problem statement specifies that the WFQ scheduler is modeled such that each class $i$ experiences an effective dedicated server with a constant rate $\\mu_i$ while it has a backlog of requests. The arrivals for each class are a Poisson process, and the service times are exponentially distributed. This allows us to model each class $i$ as an independent M/M/1 queue with arrival rate $\\lambda_i$ and service rate $\\mu_i$. We will derive the mean sojourn time for a generic M/M/1 queue from first principles.\n\nAn M/M/1 queue can be described by a continuous-time Markov chain, specifically a birth-death process, where the state $n$ represents the number of requests in the system (in queue plus in service).\nLet $p_n$ be the steady-state probability of being in state $n$. The state transitions are governed by:\n- Birth (arrival): The rate of transition from state $n$ to $n+1$ is the constant arrival rate $\\lambda$.\n- Death (service completion): The rate of transition from state $n$ to $n-1$ is the constant service rate $\\mu$ for $n \\ge 1$.\n\nIn steady state, the rate of flow out of any state must equal the rate of flow into it. For the set of states $\\{0, 1, \\dots, n-1\\}$, the balance equation is $\\lambda p_{n-1} = \\mu p_n$ for $n \\ge 1$.\nThis yields a recursive relationship: $p_n = (\\frac{\\lambda}{\\mu}) p_{n-1}$. Let $\\rho = \\frac{\\lambda}{\\mu}$ be the utilization of the server. Then $p_n = \\rho p_{n-1} = \\rho^2 p_{n-2} = \\dots = \\rho^n p_0$.\n\nTo find $p_0$, we use the normalization condition that the sum of all probabilities must be $1$:\n$$ \\sum_{n=0}^{\\infty} p_n = 1 $$\n$$ p_0 \\sum_{n=0}^{\\infty} \\rho^n = 1 $$\nThis is a geometric series which converges for $|\\rho| < 1$, a condition for stability. The sum is $\\frac{1}{1-\\rho}$.\nTherefore, $p_0 \\left( \\frac{1}{1-\\rho} \\right) = 1$, which gives $p_0 = 1 - \\rho$.\nThe steady-state probability distribution is $p_n = (1-\\rho)\\rho^n$ for $n \\ge 0$.\n\nThe mean number of requests in the system, $L$, is the expected value of $n$:\n$$ L = E[n] = \\sum_{n=0}^{\\infty} n p_n = \\sum_{n=0}^{\\infty} n (1-\\rho)\\rho^n = (1-\\rho) \\sum_{n=0}^{\\infty} n \\rho^n $$\nWe recognize the sum $\\sum_{n=0}^{\\infty} n \\rho^n = \\rho \\frac{d}{d\\rho} \\left( \\sum_{n=0}^{\\infty} \\rho^n \\right) = \\rho \\frac{d}{d\\rho} \\left( \\frac{1}{1-\\rho} \\right) = \\rho \\frac{1}{(1-\\rho)^2}$.\nSubstituting this back into the expression for $L$:\n$$ L = (1-\\rho) \\left( \\frac{\\rho}{(1-\\rho)^2} \\right) = \\frac{\\rho}{1-\\rho} $$\n\nThe mean request latency (sojourn time), $T$, is the average time a request spends in the system. By Little's Law, the mean number of requests in a stable system is the product of the arrival rate and the mean time spent in the system: $L = \\lambda T$.\nSolving for $T$, we get:\n$$ T = \\frac{L}{\\lambda} = \\frac{\\rho/(1-\\rho)}{\\lambda} = \\frac{(\\lambda/\\mu)/(1 - \\lambda/\\mu)}{\\lambda} = \\frac{\\lambda/\\mu}{\\lambda((\\mu-\\lambda)/\\mu)} = \\frac{1}{\\mu - \\lambda} $$\nThis expression is the mean sojourn time for an M/M/1 queue.\n\nWe now apply this result to each class $i$ by substituting the specific parameters $\\lambda_i$ and $\\mu_i$:\n$$ T_i = \\frac{1}{\\mu_i - \\lambda_i} $$\n\n### Numerical Computation\nFirst, we confirm the values for $\\lambda_i$ and $\\mu_i$ for each class:\n- Class $1$: $\\lambda_1 = 9000 \\text{ s}^{-1}$, $\\mu_1 = 10000 \\text{ s}^{-1}$\n- Class $2$: $\\lambda_2 = 3500 \\text{ s}^{-1}$, $\\mu_2 = 5000 \\text{ s}^{-1}$\n- Class $3$: $\\lambda_3 = 12000 \\text{ s}^{-1}$, $\\mu_3 = 15000 \\text{ s}^{-1}$\n\nNow we compute the mean latency $T_i$ for each class in seconds:\n- $T_1 = \\frac{1}{\\mu_1 - \\lambda_1} = \\frac{1}{10000 - 9000} = \\frac{1}{1000} \\text{ s}$\n- $T_2 = \\frac{1}{\\mu_2 - \\lambda_2} = \\frac{1}{5000 - 3500} = \\frac{1}{1500} \\text{ s}$\n- $T_3 = \\frac{1}{\\mu_3 - \\lambda_3} = \\frac{1}{15000 - 12000} = \\frac{1}{3000} \\text{ s}$\n\nFinally, we convert these latencies to milliseconds ($1 \\text{ s} = 1000 \\text{ ms}$) and round to four significant figures as required:\n- $T_1 = \\frac{1}{1000} \\text{ s} \\times 1000 \\frac{\\text{ms}}{\\text{s}} = 1 \\text{ ms}$. To four significant figures, this is $1.000$ ms.\n- $T_2 = \\frac{1}{1500} \\text{ s} \\times 1000 \\frac{\\text{ms}}{\\text{s}} = \\frac{2}{3} \\text{ ms} \\approx 0.66666... \\text{ ms}$. To four significant figures, this is $0.6667$ ms.\n- $T_3 = \\frac{1}{3000} \\text{ s} \\times 1000 \\frac{\\text{ms}}{\\text{s}} = \\frac{1}{3} \\text{ ms} \\approx 0.33333... \\text{ ms}$. To four significant figures, this is $0.3333$ ms.\n\nThe resulting latencies are $(T_1, T_2, T_3) = (1.000, 0.6667, 0.3333)$ ms.", "answer": "$$\\boxed{\\begin{pmatrix} 1.000 & 0.6667 & 0.3333 \\end{pmatrix}}$$", "id": "3648722"}]}