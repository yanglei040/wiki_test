{"hands_on_practices": [{"introduction": "The two-norm condition number, $\\kappa_2(A)$, is a fundamental concept in numerical analysis that quantifies the sensitivity of a linear system's solution to input errors. While often computationally intensive to determine, its calculation simplifies dramatically for matrices with special structures. This first practice [@problem_id:3195472] challenges you to exploit the properties of a diagonal matrix to design a memory-efficient streaming algorithm, a task that highlights the translation of mathematical insight into practical, resource-aware code.", "problem": "You are given streaming access to the diagonal entries of a real diagonal matrix $D \\in \\mathbb{R}^{n \\times n}$, i.e., you receive the sequence $(d_1,d_2,\\dots,d_n)$ one element at a time. The goal is to design an algorithm that estimates the two-norm condition number $\\kappa_2(D)$ in a single pass without storing the entire sequence. You must validate the estimate against the exact value derived from first principles.\n\nFundamental base:\n- The matrix two-norm condition number is defined as $\\kappa_2(A) = \\|A\\|_2 \\,\\|A^{-1}\\|_2$ for a nonsingular matrix $A$, and $\\kappa_2(A) = +\\infty$ if $A$ is singular.\n- For a diagonal matrix $D = \\mathrm{diag}(d_1,\\dots,d_n)$, the singular values are $|d_1|,\\dots,|d_n|$.\n\nTasks:\n1. Derive, from the above definitions, the exact expression of $\\kappa_2(D)$ in terms of the diagonal entries $\\{d_i\\}_{i=1}^n$.\n2. Design a streaming algorithm that processes each $d_i$ once and uses $\\mathcal{O}(1)$ memory to produce an estimate $\\widehat{\\kappa}_2(D)$ after the final element is processed. Your algorithm should be correct for any real $d_i$, including the presence of $d_i=0$.\n3. Implement both the streaming estimator and the exact computation to validate the former against the latter on a fixed test suite. Use the following test cases, each specified by its diagonal sequence, in this exact order:\n   - Case A (general mixed magnitudes and signs): $[\\,3,-1,2,0.5\\,]$.\n   - Case B (all equal magnitude): $[\\,-5,5,-5\\,]$.\n   - Case C (contains zero, hence singular): $[\\,2,0,-7\\,]$.\n   - Case D (single-element diagonal): $[\\,-4.2\\,]$.\n   - Case E (wide dynamic range): $[\\,1\\times 10^{-12},-3.5,2\\times 10^{9},-4\\times 10^{-3}\\,]$.\n   - Case F (all zeros, singular): $[\\,0,0\\,]$.\n\nValidation metric and output:\n- For each case, compute the absolute relative error between the streaming estimate $\\widehat{\\kappa}_2$ and the exact $\\kappa_2$ as\n  $$\n  \\mathrm{err} =\n  \\begin{cases}\n  0,  \\text{if both } \\widehat{\\kappa}_2 \\text{ and } \\kappa_2 \\text{ are } +\\infty,\\\\\n  \\dfrac{\\left|\\,\\widehat{\\kappa}_2 - \\kappa_2\\,\\right|}{\\kappa_2},  \\text{if } \\kappa_2 \\text{ is finite},\\\\\n  +\\infty,  \\text{if } \\kappa_2 \\text{ is } +\\infty \\text{ but } \\widehat{\\kappa}_2 \\text{ is finite}.\n  \\end{cases}\n  $$\n- Your program must output a single line containing a list with six floating-point numbers, one per test case, equal to the six error values in the order A through F, enclosed in square brackets and separated by commas, for example: $[\\,0.0,0.0,0.0,0.0,0.0,0.0\\,]$. If any error is $+\\infty$, print it as the standard floating-point infinity.\n\nConstraints and notes:\n- Do not assume any prior structure beyond diagonality; entries may be negative and may be zero.\n- No physical units are involved.\n- Angles are not used.\n- The implementation must not read input and must print exactly one line as specified.", "solution": "The problem as stated is scientifically grounded, well-posed, and internally consistent. It presents a standard task in introductory computational science: translating a mathematical definition into a memory-efficient algorithm. All premises, definitions, and constraints are standard and unambiguous. The problem is therefore deemed valid and a solution will be provided.\n\nThe task is to derive the exact formula for the two-norm condition number, $\\kappa_2(D)$, of a diagonal matrix $D$, design a single-pass, constant-memory streaming algorithm to compute it, and validate this algorithm against an exact computation using a provided test suite.\n\n_1. Derivation of the Exact Expression for $\\kappa_2(D)$_\n\nThe two-norm condition number of a nonsingular matrix $A$ is defined as $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$. If $A$ is singular, its condition number is defined as $\\kappa_2(A) = +\\infty$.\n\nThe two-norm of a matrix, $\\|A\\|_2$, is equal to its largest singular value, $\\sigma_{\\max}(A)$. For a nonsingular matrix, the two-norm of its inverse, $\\|A^{-1}\\|_2$, is the reciprocal of the smallest singular value of $A$, $1/\\sigma_{\\min}(A)$. Therefore, for a nonsingular matrix $A$, the condition number can be expressed as the ratio of its largest to smallest singular values:\n$$ \\kappa_2(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)} $$\nThe problem states that for a diagonal matrix $D = \\mathrm{diag}(d_1, d_2, \\dots, d_n)$ with $D \\in \\mathbb{R}^{n \\times n}$, its singular values are the absolute values of its diagonal entries, $\\{|d_1|, |d_2|, \\dots, |d_n|\\}$.\n\nFrom this, the largest singular value of $D$ is $\\sigma_{\\max}(D) = \\max_{i} \\{|d_i|\\}$, and the smallest singular value is $\\sigma_{\\min}(D) = \\min_{i} \\{|d_i|\\}$.\n\nWe must consider two cases:\nCase 1: The matrix $D$ is nonsingular. This occurs if and only if all diagonal entries $d_i$ are non-zero. In this case, $\\sigma_{\\min}(D) = \\min_{i} \\{|d_i|\\}  0$. The condition number is finite and given by:\n$$ \\kappa_2(D) = \\frac{\\max_{i} \\{|d_i|\\}}{\\min_{i} \\{|d_i|\\}} $$\nCase 2: The matrix $D$ is singular. This occurs if and only if at least one diagonal entry $d_i$ is zero. In this case, $\\sigma_{\\min}(D) = \\min_{i} \\{|d_i|\\} = 0$. By definition, the condition number of a singular matrix is infinite. This is consistent with the formula, as division by zero yields infinity (assuming the matrix is not the zero matrix, for which $\\sigma_{\\max}(D)$ would also be $0$). For any singular matrix, including the zero matrix, we take $\\kappa_2(D) = +\\infty$.\n\nCombining these two cases gives the complete expression for the condition number of a diagonal matrix:\n$$\n\\kappa_2(D) =\n\\begin{cases}\n\\dfrac{\\max_{i} \\{|d_i|\\}}{\\min_{i} \\{|d_i|\\}},  \\text{if } \\min_{i} \\{|d_i|\\}  0 \\\\\n+\\infty,  \\text{if } \\min_{i} \\{|d_i|\\} = 0\n\\end{cases}\n$$\n\n_2. Design of the Streaming Algorithm_\n\nThe goal is to compute $\\kappa_2(D)$ in a single pass over the diagonal entries $(d_1, d_2, \\dots, d_n)$ using constant, i.e., $\\mathcal{O}(1)$, memory. This means we cannot store the entire sequence of entries.\n\nBased on the derived formula, the computation requires only two values from the set of absolute diagonal entries $\\{|d_i|\\}$: the maximum value and the minimum value. Both of these can be computed in a single pass with constant memory.\n\nThe algorithm proceeds as follows:\n1.  Initialize two variables: one to track the maximum absolute value seen so far, `max_abs_d`, and one to track the minimum, `min_abs_d`. A robust initialization is to set `max_abs_d` to $0$ and `min_abs_d` to $+\\infty$.\n2.  Process the stream of diagonal entries $d_i$ one by one.\n3.  For each entry $d_i$:\n    a. Compute its absolute value, $|d_i|$.\n    b. Update the maximum: `max_abs_d = max(max_abs_d, |d_i|)`.\n    c. Update the minimum: `min_abs_d = min(min_abs_d, |d_i|)`.\n4.  After the entire stream has been processed, the final condition number is computed:\n    a. If `min_abs_d` is $0$, the matrix is singular. The result is $+\\infty$.\n    b. If `min_abs_d` is greater than $0$, the matrix is nonsingular. The result is the ratio `max_abs_d / min_abs_d`.\n\nThis algorithm requires storing only two floating-point numbers (`max_abs_d` and `min_abs_d`) regardless of the size $n$ of the matrix, thus satisfying the $\\mathcal{O}(1)$ memory constraint.\n\n_3. Validation_\n\nThe problem refers to the output of the streaming algorithm as an \"estimate\" $\\widehat{\\kappa}_2(D)$. However, as designed, the algorithm computes the exact maximum and minimum of the absolute diagonal entries. Therefore, its result is not an approximation but is mathematically identical to the value computed by a \"batch\" algorithm that first stores all entries. The validation step, which compares the streaming result to the exact result, is thus expected to yield an absolute relative error of $0$ for all test cases, assuming perfect arithmetic. The provided test cases do not involve values that would cause precision issues with standard double-precision floating-point numbers.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of estimating the condition number of a diagonal matrix\n    from a stream of its diagonal entries and validating the result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (general mixed magnitudes and signs)\n        [3.0, -1.0, 2.0, 0.5],\n        # Case B (all equal magnitude)\n        [-5.0, 5.0, -5.0],\n        # Case C (contains zero, hence singular)\n        [2.0, 0.0, -7.0],\n        # Case D (single-element diagonal)\n        [-4.2],\n        # Case E (wide dynamic range)\n        [1e-12, -3.5, 2e9, -4e-3],\n        # Case F (all zeros, singular)\n        [0.0, 0.0]\n    ]\n\n    def compute_exact_kappa(diag_entries):\n        \"\"\"\n        Computes the exact condition number by storing all entries.\n        This serves as the ground truth for validation.\n        \"\"\"\n        if not diag_entries:\n            # The condition number of a 0x0 matrix is conventionally 1.\n            # This case is not in the test suite.\n            return 1.0\n\n        abs_vals = np.abs(np.array(diag_entries))\n        min_abs_val = np.min(abs_vals)\n\n        if min_abs_val == 0:\n            return np.inf\n\n        max_abs_val = np.max(abs_vals)\n        return max_abs_val / min_abs_val\n\n    def compute_streaming_kappa(diag_stream):\n        \"\"\"\n        Computes the condition number in a single pass with O(1) memory.\n        This is the \"estimator\" to be validated.\n        \"\"\"\n        if not diag_stream:\n            # Match the exact computation for the empty case.\n            return 1.0\n\n        # Initialize max_abs_d to 0 and min_abs_d to +infinity.\n        # This handles all cases, including first element, correctly.\n        max_abs_d = 0.0\n        min_abs_d = np.inf\n\n        for d in diag_stream:\n            current_abs_d = abs(d)\n            if current_abs_d  max_abs_d:\n                max_abs_d = current_abs_d\n            if current_abs_d  min_abs_d:\n                min_abs_d = current_abs_d\n        \n        # After the loop, if min_abs_d is 0, the matrix is singular.\n        if min_abs_d == 0:\n            return np.inf\n        \n        # Otherwise, the matrix is nonsingular.\n        return max_abs_d / min_abs_d\n\n    error_results = []\n    for case in test_cases:\n        kappa_2 = compute_exact_kappa(case)\n        kappa_2_hat = compute_streaming_kappa(case)\n\n        # Calculate the absolute relative error based on the problem's definition.\n        err = 0.0\n        if kappa_2 == np.inf and kappa_2_hat == np.inf:\n            err = 0.0\n        elif kappa_2 == np.inf:  # Implies kappa_2_hat is finite\n            err = np.inf \n        elif kappa_2  0: # kappa_2 is finite and positive\n            err = abs(kappa_2_hat - kappa_2) / kappa_2\n        # The remaining case is kappa_2 = 0, which is impossible for a condition number.\n        \n        error_results.append(err)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, error_results))}]\")\n\nsolve()\n```", "id": "3195472"}, {"introduction": "An essential aspect of computational science is understanding how algorithms behave in the presence of finite-precision arithmetic and stability-enhancing procedures. This practice [@problem_id:3195468] investigates the effects of LU factorization with partial pivoting, a cornerstone of numerical linear algebra, on matrices that are already triangular. You will observe and quantify the phenomenon of \"fill-in,\" where pivoting designed to control error growth can destroy the matrix's original sparse structure, providing a critical insight into the trade-offs between theory and numerical practice.", "problem": "You are given the task of studying how floating-point roundoff and row pivoting during Gaussian elimination can break or alter special triangular structures and create fill-in. The investigation will be grounded in fundamental definitions and models:\n\n- A matrix $T \\in \\mathbb{R}^{n \\times n}$ is upper triangular if $T_{ij} = 0$ for all $i  j$, lower triangular if $T_{ij} = 0$ for all $i  j$, and diagonal if $T_{ij} = 0$ for all $i \\neq j$.\n- Gaussian elimination with partial row pivoting is a process that, for a matrix $A$, computes a permutation matrix $P$, a unit lower triangular matrix $L$, and an upper triangular matrix $U$ such that $P A = L U$. This is known as the Lower-Upper (LU) factorization (LU).\n- In floating-point arithmetic, we model the effect of roundoff at each operation by the relation $\\mathrm{fl}(x \\,\\mathrm{op}\\, y) = (x \\,\\mathrm{op}\\, y)(1 + \\delta)$, where $\\mathrm{op}$ denotes a basic arithmetic operation and $|\\delta| \\le \\epsilon_{\\mathrm{mach}}$ for a small unit roundoff $\\epsilon_{\\mathrm{mach}}$.\n\nYour program must, for each test case, construct a specified special-structure matrix $T$, compute the upper triangular factor $U$ produced by LU factorization with partial pivoting applied to $T$, and then quantify structural changes and numerical differences. Use the following definitions and procedures:\n\n- Define a tolerance $\\tau = 10^{-12}$ to classify effective zeros under floating-point arithmetic.\n- Define the fill-in count $f$ as the number of indices $(i,j)$ such that $|T_{ij}| \\le \\tau$ and $|U_{ij}|  \\tau$.\n- Define the Frobenius norm difference $d_F$ as $d_F = \\|U - T\\|_F = \\left(\\sum_{i=1}^{n}\\sum_{j=1}^{n} (U_{ij} - T_{ij})^2\\right)^{1/2}$.\n- Define the maximum absolute entrywise difference $d_{\\max}$ as $d_{\\max} = \\max_{1 \\le i,j \\le n} |U_{ij} - T_{ij}|$.\n\nConstruct the following test suite of matrices, each precisely defined by $n$ and entry formulas:\n\n- Test case $1$ (lower triangular, dense subdiagonal): $n = 4$, with\n  $$\n  T_{ij} =\n  \\begin{cases}\n  1  \\text{if } i = j, \\\\\n  2  \\text{if } i  j, \\\\\n  0  \\text{if } i  j.\n  \\end{cases}\n  $$\n- Test case $2$ (upper triangular, dense superdiagonal): $n = 4$, with\n  $$\n  T_{ij} =\n  \\begin{cases}\n  1  \\text{if } i = j, \\\\\n  \\frac{1}{2}  \\text{if } i  j, \\\\\n  0  \\text{if } i  j.\n  \\end{cases}\n  $$\n- Test case $3$ (lower triangular, tiny diagonal): $n = 5$, with\n  $$\n  T_{ij} =\n  \\begin{cases}\n  10^{-16}  \\text{if } i = j, \\\\\n  1  \\text{if } i  j, \\\\\n  0  \\text{if } i  j.\n  \\end{cases}\n  $$\n- Test case $4$ (diagonal, widely varying magnitudes): $n = 5$, with diagonal entries\n  $$\n  \\operatorname{diag}(T) = [10^{-16},\\, 10^{-8},\\, 1,\\, 10^{8},\\, 10^{16}],\n  $$\n  and $T_{ij} = 0$ for all $i \\neq j$.\n\nFor each test case:\n- Compute $U$ via LU factorization with partial pivoting.\n- Compute $f$, $d_F$, and $d_{\\max}$ as defined above.\n- Round the floating-point outputs $d_F$ and $d_{\\max}$ to $10$ decimal places.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test caseâ€™s result must itself be a comma-separated list enclosed in square brackets, in the order $[f, d_F, d_{\\max}]$, and the entire output must have no spaces. For example, for two hypothetical cases, the output would look like $[[f_1,d_{F,1},d_{\\max,1}],[f_2,d_{F,2},d_{\\max,2}]]$.\n\nYour program must be self-contained, require no user input, and strictly adhere to the specified output format.", "solution": "The problem statement is assessed as valid. It presents a well-defined computational task grounded in the established principles of numerical linear algebra. The matrices, algorithms, and metrics are all standard concepts in computational science. The problem is self-contained, objective, and scientifically sound.\n\n### Introduction to the Methodology\n\nThe core of this problem is to investigate the interplay between matrix structure, numerical stability algorithms, and floating-point arithmetic. Specifically, we analyze how LU factorization with partial pivoting, a fundamental algorithm for solving linear systems, affects matrices with special structures like triangular or diagonal forms. The process involves computing the factorization $PA = LU$, where $P$ is a permutation matrix, $L$ is a unit lower triangular matrix, and $U$ is an upper triangular matrix. We will then quantify any structural or numerical deviations in the resulting factor $U$ compared to the original matrix $T$.\n\n### Principle 1: LU Factorization with Partial Pivoting\n\nGaussian elimination transforms a matrix $A$ into an upper triangular matrix $U$ by applying a sequence of elementary row operations. These operations can be represented by a unit lower triangular matrix $L$ such that $A=LU$. To ensure numerical stability, a strategy called partial pivoting is employed. At each step $k$ of the elimination, the algorithm inspects the elements in the current pivot column $k$ at or below the diagonal, i.e., $A_{ik}^{(k-1)}$ for $i \\ge k$. It identifies the row with the element of largest absolute value and swaps this row with the current pivot row $k$. This process is recorded in a permutation matrix $P$. The resulting factorization is $PA = LU$.\n\nThe purpose of pivoting is to make the multipliers $L_{ik} = A_{ik}^{(k-1)}/A_{kk}^{(k-1)}$ (for $ik$) small in magnitude (specifically, $|L_{ik}| \\le 1$). This helps control the growth of entries in the matrix during elimination, thereby mitigating the propagation of roundoff errors inherent in floating-point arithmetic.\n\n### Principle 2: Interaction with Special Matrix Structures\n\nThe effectiveness and consequences of pivoting depend heavily on the initial structure of the matrix $T$.\n\n- **Upper Triangular Matrix**: If $T$ is already upper triangular, no elimination is necessary. If, for each column $j$, the diagonal pivot candidate $T_{jj}$ has a magnitude greater than or equal to all sub-diagonal entries (which are zero), no row swaps will occur. In this ideal scenario, $P$ would be the identity matrix $I$, $L$ would also be $I$, and the resulting upper triangular factor would be the original matrix itself, $U=T$.\n- **Lower Triangular Matrix**: For a lower triangular matrix $T$, the non-zero elements are on and below the diagonal. If a sub-diagonal element $|T_{ij}|$ with $i  j$ is larger than the diagonal element $|T_{jj}|$, partial pivoting will mandate a row swap. This permutation, applied to $T$, can move non-zero elements from the lower triangle into the strict upper triangle of the matrix being processed. Subsequent elimination steps on these new non-zero entries will result in non-zero values in the strict upper triangle of the final $U$ matrix. This phenomenon, where zeros in $T$ become non-zeros in $U$, is called **fill-in**.\n- **Diagonal Matrix**: A diagonal matrix is a special case of an upper triangular matrix where all off-diagonal elements are zero. No elimination is required, and since there are no non-zero sub-diagonal elements to compete with the diagonal pivot, no pivoting occurs. The factorization is trivial: $P=I$, $L=I$, and $U=T$.\n\n### Principle 3: Quantifying Structural and Numerical Change\n\nTo measure the impact of the factorization process, we use three metrics:\n\n1.  **Fill-in count ($f$)**: Defined as the number of indices $(i,j)$ where the original matrix had a near-zero entry, $|T_{ij}| \\le \\tau$, but the computed factor has a significant non-zero entry, $|U_{ij}|  \\tau$. This metric quantifies the destruction of the zero-structure. The tolerance $\\tau = 10^{-12}$ is used to distinguish numerically significant entries from floating-point artifacts.\n2.  **Frobenius Norm Difference ($d_F$)**: $d_F = \\|U - T\\|_F$ measures the total numerical deviation between $U$ and $T$ across all entries.\n3.  **Maximum Absolute Difference ($d_{\\max}$)**: $d_{\\max} = \\max_{i,j} |U_{ij} - T_{ij}|$ identifies the single largest point of deviation, highlighting the worst-case numerical change.\n\n### Analysis of Test Cases\n\n-   **Test Case 1 (Lower Triangular)**: The matrix is $T_{ij} = \\{1 \\text{ if } i=j; 2 \\text{ if } ij; 0 \\text{ if } ij\\}$. In the first column, the pivot candidate $T_{11}=1$ is smaller than the sub-diagonal elements $T_{i1}=2$. Pivoting will occur, leading to row swaps. This will destroy the lower triangular structure and cause fill-in ($f0$), as non-zero elements are permuted into the upper part of the active submatrix. We expect significant numerical differences $d_F$ and $d_{\\max}$.\n\n-   **Test Case 2 (Upper Triangular)**: The matrix is $T_{ij} = \\{1 \\text{ if } i=j; 1/2 \\text{ if } ij; 0 \\text{ if } ij\\}$. This matrix is already upper triangular, and the diagonal entries of $1$ are the largest in magnitude in their respective columns. No pivoting is expected ($P=I$). The algorithm should ideally return $U=T$. Therefore, the fill-in $f$ should be $0$, and any numerical differences $d_F, d_{\\max}$ should be attributable only to floating-point roundoff, resulting in values close to zero.\n\n-   **Test Case 3 (Lower Triangular, Tiny Diagonal)**: The matrix has diagonal entries of $10^{-16}$ and sub-diagonal entries of $1$. The tiny diagonal elements ensure that pivoting will occur at every step of the elimination, as the sub-diagonal $1$s are much larger. This will completely scramble the initial structure, leading to a nearly dense $U$ matrix. We predict a maximal fill-in count $f$ and large values for $d_F$ and $d_{\\max}$.\n\n-   **Test Case 4 (Diagonal, Varying Magnitudes)**: The matrix is diagonal. As a special case of an upper triangular matrix with zero sub-diagonal entries, no pivoting or elimination steps are needed. The LU factorization is trivial ($P=I, L=I, U=T$), perfectly preserving the structure. We expect $f=0$ and differences $d_F, d_{\\max}$ to be zero or on the order of machine precision.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import lu\n\ndef solve():\n    \"\"\"\n    Analyzes the effect of LU factorization with partial pivoting on\n    special matrix structures.\n    \"\"\"\n    \n    # Define the tolerance for considering a value to be zero.\n    tau = 1e-12\n\n    # Container for all test case results\n    all_results = []\n\n    # --- Test Case 1: Lower triangular, dense subdiagonal ---\n    n1 = 4\n    T1 = np.zeros((n1, n1), dtype=float)\n    T1[np.tril_indices(n1, k=-1)] = 2.0\n    np.fill_diagonal(T1, 1.0)\n\n    # --- Test Case 2: Upper triangular, dense superdiagonal ---\n    n2 = 4\n    T2 = np.zeros((n2, n2), dtype=float)\n    T2[np.triu_indices(n2, k=1)] = 0.5\n    np.fill_diagonal(T2, 1.0)\n    \n    # --- Test Case 3: Lower triangular, tiny diagonal ---\n    n3 = 5\n    T3 = np.zeros((n3, n3), dtype=float)\n    T3[np.tril_indices(n3, k=-1)] = 1.0\n    np.fill_diagonal(T3, 1e-16)\n\n    # --- Test Case 4: Diagonal, widely varying magnitudes ---\n    T4 = np.diag([1e-16, 1e-8, 1.0, 1e8, 1e16])\n    \n    test_cases = [T1, T2, T3, T4]\n\n    for T in test_cases:\n        # Perform LU factorization with partial pivoting\n        # scipy.linalg.lu computes P, L, U such that P @ A = L @ U\n        # This matches the problem description.\n        _, _, U = lu(T)\n\n        # Calculate fill-in count f\n        # f is the number of indices (i,j) where |T_ij| = tau and |U_ij|  tau\n        originally_zero = np.abs(T) = tau\n        u_is_nonzero = np.abs(U)  tau\n        fill_in_mask = np.logical_and(originally_zero, u_is_nonzero)\n        f = int(np.sum(fill_in_mask))\n\n        # Calculate Frobenius norm difference d_F\n        # d_F = ||U - T||_F\n        diff = U - T\n        d_F = np.linalg.norm(diff, 'fro')\n\n        # Calculate maximum absolute entrywise difference d_max\n        # d_max = max |U_ij - T_ij|\n        d_max = np.max(np.abs(diff))\n\n        # Round results to 10 decimal places as required\n        d_F_rounded = round(d_F, 10)\n        d_max_rounded = round(d_max, 10)\n        \n        all_results.append([f, d_F_rounded, d_max_rounded])\n\n    # Format the final output string exactly as specified.\n    # e.g., [[f1,dF1,dmax1],[f2,dF2,dmax2]] with no spaces.\n    case_strings = [f\"[{r[0]},{r[1]},{r[2]}]\" for r in all_results]\n    final_output = f\"[{','.join(case_strings)}]\"\n    \n    print(final_output)\n\nsolve()\n```", "id": "3195468"}, {"introduction": "Having explored how matrix structures can simplify calculations and how algorithms can interact with them, we now turn to actively leveraging structure to accelerate computations. This exercise [@problem_id:3195493] focuses on solving large symmetric positive definite (SPD) linear systems, a ubiquitous problem in computational science. You will implement and compare the convergence of the Steepest Descent and Conjugate Gradient methods, demonstrating how a simple diagonal preconditioner, built by extracting the diagonal of the system matrix, can dramatically improve performance by effectively lowering the system's condition number.", "problem": "You are to implement, from first principles, iterative solvers for linear systems with Symmetric Positive Definite (SPD) matrices and compare the effect of diagonal preconditioning on convergence. The focus is on special matrix structures, including symmetric matrices and diagonal matrices. Consider solving the system $A\\mathbf{x} = \\mathbf{b}$ for an SPD matrix $A$ and a vector $\\mathbf{b}$ by iterative methods. You will implement four methods: unpreconditioned Steepest Descent (SD), diagonally preconditioned Steepest Descent (PSD) with $M=\\operatorname{diag}(A)$, unpreconditioned Conjugate Gradient (CG), and diagonally preconditioned Conjugate Gradient (PCG) with the same $M=\\operatorname{diag}(A)$. Use the zero vector $\\mathbf{x}_0=\\mathbf{0}$ as the initial guess in all cases, exact line search in SD and PSD, and terminate each method when the relative residual norm satisfies $\\lVert r_k \\rVert_2 / \\lVert b \\rVert_2 \\leq 10^{-8}$ or a maximum iteration cap of $5n$ iterations (where $n$ is the dimension of $A$) is reached, whichever occurs first. Do not use any linear system or Krylov-subspace solvers from external libraries; implement the algorithms directly from core definitions.\n\nFundamental base for design:\n- An SPD matrix $A$ is symmetric ($A=A^\\top$) and satisfies $\\mathbf{x}^\\top A \\mathbf{x} > 0$ for all nonzero $\\mathbf{x}$.\n- Solving $A\\mathbf{x}=\\mathbf{b}$ for SPD $A$ is equivalent to minimizing the strictly convex quadratic function $f(\\mathbf{x})=\\tfrac{1}{2} \\mathbf{x}^\\top A \\mathbf{x} - \\mathbf{b}^\\top \\mathbf{x}$.\n- Steepest Descent follows the negative gradient direction of $f(\\mathbf{x})$ with an exact line search at each iteration.\n- Conjugate Gradient constructs $A$-conjugate directions to minimize $f(\\mathbf{x})$ in a Krylov subspace.\n- Diagonal preconditioning with $M=\\operatorname{diag}(A)$ rescales the problem to improve conditioning in many cases.\n\nTest suite:\n- Test case $1$ (diagonal SPD, baseline): $A_1=\\operatorname{diag}(1,2,3,4)$ and $b_1=\\mathbf{1}$ of length $4$.\n- Test case $2$ (structured SPD, tridiagonal Poisson): $A_2 \\in \\mathbb{R}^{50\\times 50}$ is tridiagonal with $2$ on the diagonal and $-1$ on the first sub- and super-diagonal. Let $b_2=\\mathbf{1}$ of length $50$.\n- Test case $3$ (random SPD, moderate conditioning): With a fixed pseudorandom seed, let $R \\in \\mathbb{R}^{20\\times 20}$ have independent standard normal entries, define $A_3=R^\\top R + 0.1 I$, and let $b_3 \\in \\mathbb{R}^{20}$ have independent standard normal entries generated with the same seed.\n- Test case $4$ (diagonal SPD, highly ill-conditioned): $A_4=\\operatorname{diag}(10^{-6},10^{-3},1,10^{3},10^{6})$ and $b_4=\\mathbf{1}$ of length $5$.\n\nFor each test case, compute and record the iteration counts required to meet the termination criterion for the four methods in the fixed order: SD, PSD, CG, PCG. You must use the diagonal preconditioner $M=\\operatorname{diag}(A)$ for PSD and PCG, and interpret $M^{-1}$ as the elementwise reciprocal of the diagonal of $A$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results aggregated across all test cases as a comma-separated list of lists, each inner list holding four integers corresponding to the iteration counts for SD, PSD, CG, and PCG, in this order. There must be no spaces in the output, and the entire output must be enclosed in square brackets. For example, with three test cases your program would print a single line of the form $[[a_{11},a_{12},a_{13},a_{14}],[a_{21},a_{22},a_{23},a_{24}],[a_{31},a_{32},a_{33},a_{34}]]$, where each $a_{ij}$ is an integer.", "solution": "The problem of solving the linear system $A\\mathbf{x} = \\mathbf{b}$ where $A$ is a symmetric positive definite (SPD) matrix is mathematically equivalent to finding the unique vector $\\mathbf{x}$ that minimizes the strictly convex quadratic function $f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^\\top A \\mathbf{x} - \\mathbf{b}^\\top \\mathbf{x}$. The gradient of this function is $\\nabla f(\\mathbf{x}) = A\\mathbf{x} - \\mathbf{b}$, which is the negative of the residual, $\\mathbf{r}(\\mathbf{x}) = \\mathbf{b} - A\\mathbf{x}$.\n\nIterative methods for this problem generate a sequence of approximations $\\{\\mathbf{x}_k\\}$ that converge to the true solution. Starting with an initial guess $\\mathbf{x}_0$, subsequent iterates are computed as $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$, where $\\mathbf{p}_k$ is a search direction and $\\alpha_k$ is a scalar step size. The choice of $\\mathbf{p}_k$ and $\\alpha_k$ defines the specific algorithm. This implementation will explore four such algorithms.\n\n**1. Steepest Descent (SD)**\n\nThe Steepest Descent method employs the most intuitive search direction: the direction of the negative gradient of $f$ at the current iterate $\\mathbf{x}_k$. This direction corresponds to the steepest decrease in the objective function.\nThe search direction is thus $\\mathbf{p}_k = - \\nabla f(\\mathbf{x}_k) = \\mathbf{r}_k$.\nThe step size $\\alpha_k$ is determined by an exact line search, which finds the $\\alpha$ that minimizes $f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)$. The derivative of $f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)$ with respect to $\\alpha$ is zero at the minimum:\n$$ \\frac{d}{d\\alpha} f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) = \\nabla f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)^\\top \\mathbf{p}_k = (A(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) - \\mathbf{b})^\\top \\mathbf{p}_k = (A\\mathbf{x}_k - \\mathbf{b} + \\alpha A\\mathbf{p}_k)^\\top \\mathbf{p}_k = (-\\mathbf{r}_k + \\alpha A\\mathbf{p}_k)^\\top \\mathbf{p}_k = 0 $$\nSolving for $\\alpha$ gives the optimal step size: $\\alpha_k = \\frac{\\mathbf{r}_k^\\top \\mathbf{p}_k}{\\mathbf{p}_k^\\top A \\mathbf{p}_k}$. Substituting $\\mathbf{p}_k = \\mathbf{r}_k$, we get:\n$$ \\alpha_k = \\frac{\\mathbf{r}_k^\\top \\mathbf{r}_k}{\\mathbf{r}_k^\\top A \\mathbf{r}_k} $$\nThe SD algorithm updates the solution and residual at each step $k$:\n$\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{r}_k$\n$\\mathbf{r}_{k+1} = \\mathbf{b} - A\\mathbf{x}_{k+1} = \\mathbf{b} - A(\\mathbf{x}_k + \\alpha_k \\mathbf{r}_k) = (\\mathbf{b} - A\\mathbf{x}_k) - \\alpha_k A\\mathbf{r}_k = \\mathbf{r}_k - \\alpha_k A\\mathbf{r}_k$.\n\n**2. Conjugate Gradient (CG)**\n\nThe convergence of SD can be slow for ill-conditioned matrices. The Conjugate Gradient method accelerates convergence by choosing search directions $\\mathbf{p}_k$ that are A-orthogonal (or conjugate), i.e., $\\mathbf{p}_i^\\top A \\mathbf{p}_j = 0$ for all $i \\neq j$. This property ensures that the minimization performed along a new direction $\\mathbf{p}_k$ does not spoil the optimality achieved in the subspace spanned by previous directions $\\{\\mathbf{p}_0, \\dots, \\mathbf{p}_{k-1}\\}$.\n\nThe search directions are constructed iteratively. Starting with $\\mathbf{p}_0 = \\mathbf{r}_0$, subsequent directions are a combination of the current residual and the previous search direction:\n$$ \\mathbf{p}_{k+1} = \\mathbf{r}_{k+1} + \\beta_{k+1} \\mathbf{p}_k $$\nThe coefficient $\\beta_{k+1}$ is chosen to enforce A-orthogonality. A common and efficient choice is:\n$$ \\beta_{k+1} = \\frac{\\mathbf{r}_{k+1}^\\top \\mathbf{r}_{k+1}}{\\mathbf{r}_k^\\top \\mathbf{r}_k} $$\nThe step size $\\alpha_k$ is also found via an exact line search along $\\mathbf{p}_k$:\n$$ \\alpha_k = \\frac{\\mathbf{r}_k^\\top \\mathbf{p}_k}{\\mathbf{p}_k^\\top A \\mathbf{p}_k} = \\frac{\\mathbf{r}_k^\\top \\mathbf{r}_k}{\\mathbf{p}_k^\\top A \\mathbf{p}_k} $$\nThe last equality holds because of the way $\\mathbf{p}_k$ is constructed from $\\mathbf{r}_k$ and previous directions.\n\n**3. Preconditioning**\n\nThe convergence rate of iterative solvers is closely related to the condition number $\\kappa(A)$ of the matrix $A$. Preconditioning is a technique used to transform the linear system into an equivalent one that is better conditioned and thus easier to solve. Instead of solving $A\\mathbf{x}=\\mathbf{b}$, we solve the preconditioned system, for instance, $M^{-1}A\\mathbf{x} = M^{-1}\\mathbf{b}$, where $M$ is a preconditioner. A good preconditioner $M$ should approximate $A$ (so that $M^{-1}A$ is close to the identity matrix $I$), be SPD itself, and allow for the efficient solution of systems of the form $M\\mathbf{z}=\\mathbf{r}$.\n\nThis problem uses diagonal preconditioning, where $M$ is the matrix containing only the diagonal elements of $A$: $M = \\operatorname{diag}(A)$. For an SPD matrix $A$ with positive diagonal entries, $M$ is also SPD. Solving $M\\mathbf{z}=\\mathbf{r}$ is computationally trivial, requiring only an element-wise division: $\\mathbf{z}_i = \\mathbf{r}_i / M_{ii}$.\n\n**4. Preconditioned Steepest Descent (PSD)**\n\nThis method applies the steepest descent algorithm to the preconditioned problem. The equivalent quadratic form is based on the operator $M^{-1/2} A M^{-1/2}$. The gradient direction in the original variables corresponds to the preconditioned residual $\\mathbf{z}_k = M^{-1}\\mathbf{r}_k$. This is used as the search direction, $\\mathbf{p}_k = \\mathbf{z}_k$.\nThe exact line search step size that minimizes $f(\\mathbf{x}_k + \\alpha \\mathbf{z}_k)$ is:\n$$ \\alpha_k = \\frac{\\mathbf{r}_k^\\top \\mathbf{z}_k}{\\mathbf{z}_k^\\top A \\mathbf{z}_k} $$\n\n**5. Preconditioned Conjugate Gradient (PCG)**\n\nPCG is the application of the conjugate gradient method to the preconditioned system. This is the most effective method among the four. The algorithm is modified to maintain conjugacy with respect to the preconditioned matrix $M^{-1}A$. This is accomplished by incorporating a solve with $M$ at each iteration. The algorithm is as follows:\nInitialize $\\mathbf{x}_0 = \\mathbf{0}$, $\\mathbf{r}_0 = \\mathbf{b}$, solve $M\\mathbf{z}_0 = \\mathbf{r}_0$, set $\\mathbf{p}_0 = \\mathbf{z}_0$.\nFor each iteration $k$:\n$$ \\alpha_k = \\frac{\\mathbf{r}_k^\\top \\mathbf{z}_k}{\\mathbf{p}_k^\\top A \\mathbf{p}_k} $$\n$$ \\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k $$\n$$ \\mathbf{r}_{k+1} = \\mathbf{r}_k - \\alpha_k A\\mathbf{p}_k $$\nSolve $M\\mathbf{z}_{k+1} = \\mathbf{r}_{k+1}$.\n$$ \\beta_{k+1} = \\frac{\\mathbf{r}_{k+1}^\\top \\mathbf{z}_{k+1}}{\\mathbf{r}_k^\\top \\mathbf{z}_k} $$\n$$ \\mathbf{p}_{k+1} = \\mathbf{z}_{k+1} + \\beta_{k+1} \\mathbf{p}_k $$\n\n**Implementation and Termination**\n\nAll four algorithms are implemented from these first principles. The initial guess is set to the zero vector, $\\mathbf{x}_0 = \\mathbf{0}$. Iterations continue until the relative residual norm drops below the specified tolerance, $\\lVert \\mathbf{r}_k \\rVert_2 / \\lVert \\mathbf{b} \\rVert_2 \\leq 10^{-8}$, or until a maximum of $5n$ iterations is completed, where $n$ is the dimension of the matrix $A$. The number of iterations required to meet the criterion is recorded for each method and test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# from scipy import ...\n\ndef solve_sd(A, b):\n    \"\"\"\n    Implements the unpreconditioned Steepest Descent (SD) method.\n    \"\"\"\n    n = A.shape[0]\n    max_iter = 5 * n\n    tol = 1e-8\n    \n    x = np.zeros(n, dtype=np.float64)\n    r = b.copy()\n    norm_b = np.linalg.norm(b)\n\n    if norm_b == 0:\n        return 0\n\n    for k in range(max_iter):\n        r_sq_norm = r @ r\n        Ar = A @ r\n        r_Ar_dot = r @ Ar\n\n        # Avoid division by zero if r is in the null space of A\n        if r_Ar_dot == 0:\n            break\n\n        alpha = r_sq_norm / r_Ar_dot\n        x += alpha * r\n        r -= alpha * Ar\n        \n        if np.linalg.norm(r) / norm_b = tol:\n            return k + 1\n            \n    return max_iter\n\ndef solve_psd(A, b):\n    \"\"\"\n    Implements the diagonally preconditioned Steepest Descent (PSD) method.\n    \"\"\"\n    n = A.shape[0]\n    max_iter = 5 * n\n    tol = 1e-8\n\n    x = np.zeros(n, dtype=np.float64)\n    r = b.copy()\n    norm_b = np.linalg.norm(b)\n\n    if norm_b == 0:\n        return 0\n    \n    M_inv_diag = 1.0 / np.diag(A)\n    \n    for k in range(max_iter):\n        z = M_inv_diag * r\n        r_z_dot = r @ z\n        Az = A @ z\n        z_Az_dot = z @ Az\n\n        if z_Az_dot == 0:\n            break\n\n        alpha = r_z_dot / z_Az_dot\n        x += alpha * z\n        r -= alpha * Az\n\n        if np.linalg.norm(r) / norm_b = tol:\n            return k + 1\n\n    return max_iter\n\ndef solve_cg(A, b):\n    \"\"\"\n    Implements the unpreconditioned Conjugate Gradient (CG) method.\n    \"\"\"\n    n = A.shape[0]\n    max_iter = 5 * n\n    tol = 1e-8\n    \n    x = np.zeros(n, dtype=np.float64)\n    r = b.copy()\n    p = r.copy()\n    norm_b = np.linalg.norm(b)\n\n    if norm_b == 0:\n        return 0\n\n    r_sq_norm_old = r @ r\n\n    for k in range(max_iter):\n        Ap = A @ p\n        p_Ap_dot = p @ Ap\n\n        if p_Ap_dot == 0:\n            break\n        \n        alpha = r_sq_norm_old / p_Ap_dot\n        x += alpha * p\n        r -= alpha * Ap\n        \n        if np.linalg.norm(r) / norm_b = tol:\n            return k + 1\n        \n        r_sq_norm_new = r @ r\n        beta = r_sq_norm_new / r_sq_norm_old\n        p = r + beta * p\n        r_sq_norm_old = r_sq_norm_new\n\n    return max_iter\n    \ndef solve_pcg(A, b):\n    \"\"\"\n    Implements the diagonally preconditioned Conjugate Gradient (PCG) method.\n    \"\"\"\n    n = A.shape[0]\n    max_iter = 5 * n\n    tol = 1e-8\n\n    x = np.zeros(n, dtype=np.float64)\n    r = b.copy()\n    norm_b = np.linalg.norm(b)\n\n    if norm_b == 0:\n        return 0\n\n    M_inv_diag = 1.0 / np.diag(A)\n    z = M_inv_diag * r\n    p = z.copy()\n    r_z_dot_old = r @ z\n\n    for k in range(max_iter):\n        Ap = A @ p\n        p_Ap_dot = p @ Ap\n\n        if p_Ap_dot == 0:\n            break\n\n        alpha = r_z_dot_old / p_Ap_dot\n        x += alpha * p\n        r -= alpha * Ap\n\n        if np.linalg.norm(r) / norm_b = tol:\n            return k + 1\n\n        z = M_inv_diag * r\n        r_z_dot_new = r @ z\n        beta = r_z_dot_new / r_z_dot_old\n        p = z + beta * p\n        r_z_dot_old = r_z_dot_new\n\n    return max_iter\n\ndef solve():\n    # Define the test cases from the problem statement.\n    \n    # Test case 1: diagonal SPD, baseline\n    A1 = np.diag([1.0, 2.0, 3.0, 4.0])\n    b1 = np.ones(4, dtype=np.float64)\n    \n    # Test case 2: structured SPD, tridiagonal Poisson\n    n2 = 50\n    A2 = np.diag(2.0 * np.ones(n2)) + np.diag(-1.0 * np.ones(n2 - 1), 1) + np.diag(-1.0 * np.ones(n2 - 1), -1)\n    b2 = np.ones(n2, dtype=np.float64)\n    \n    # Test case 3: random SPD, moderate conditioning\n    n3 = 20\n    seed = 42\n    rng = np.random.default_rng(seed)\n    R = rng.standard_normal((n3, n3), dtype=np.float64)\n    A3 = R.T @ R + 0.1 * np.identity(n3, dtype=np.float64)\n    b3 = rng.standard_normal(n3, dtype=np.float64)\n\n    # Test case 4: diagonal SPD, highly ill-conditioned\n    A4 = np.diag([1e-6, 1e-3, 1.0, 1e3, 1e6])\n    b4 = np.ones(5, dtype=np.float64)\n\n    test_cases = [\n        (A1, b1),\n        (A2, b2),\n        (A3, b3),\n        (A4, b4),\n    ]\n\n    solvers = [solve_sd, solve_psd, solve_cg, solve_pcg]\n    \n    all_results = []\n    for A, b in test_cases:\n        case_results = []\n        for solver in solvers:\n            iterations = solver(A, b)\n            case_results.append(iterations)\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3195493"}]}