{"hands_on_practices": [{"introduction": "To master the Metropolis-Hastings algorithm, we must first understand its core engine: the acceptance probability calculation. This exercise [@problem_id:1962671] provides a concrete scenario to practice calculating this probability, $\\alpha_{i \\to j} = \\min\\left(1, \\frac{\\pi(j)q(i \\mid j)}{\\pi(i)q(j \\mid i)}\\right)$, for a discrete state space. Pay close attention to how the ratio of proposal probabilities, $q(i \\mid j) / q(j \\mid i)$, acts as a crucial \"Hastings correction\" to ensure the chain correctly samples from the target distribution $\\pi$, even when proposals are not symmetric.", "problem": "A computational simulation uses the Metropolis-Hastings algorithm to explore the conformational states of a protein. The model simplifies the protein's structure into three possible discrete states: State 1, State 2, and State 3. The goal of the simulation is to generate samples from a target stationary distribution, $\\pi$, which represents the equilibrium probability of finding the protein in each state. This target distribution is given by:\n$\\pi(\\text{State 1}) = 0.1$\n$\\pi(\\text{State 2}) = 0.3$\n$\\pi(\\text{State 3}) = 0.6$\n\nThe algorithm proceeds by proposing moves from a current state $i$ to a new state $j$ according to a proposal distribution $q(j|i)$. The proposal mechanism is defined as follows:\n- From State 1, a move to State 2 is proposed with probability $0.2$, and a move to State 3 is proposed with probability $0.8$.\n- From State 2, a move to State 1 is proposed with probability $0.5$, and a move to State 3 is proposed with probability $0.5$.\n- From State 3, a move to State 1 is proposed with probability $0.1$, and a move to State 2 is proposed with probability $0.9$.\n\nSuppose the simulation is currently in State 1 and a move to State 3 is proposed. Calculate the acceptance probability for this specific transition. Express your final answer as a simplified fraction.", "solution": "In the Metropolis-Hastings algorithm, the acceptance probability for a proposed move from a current state $i$ to a new state $j$ is\n$$\n\\alpha_{i \\to j} = \\min\\left(1, \\frac{\\pi(j)\\,q(i \\mid j)}{\\pi(i)\\,q(j \\mid i)}\\right).\n$$\nFor the proposed transition from State 1 to State 3, use the given target probabilities and proposal probabilities. Converting the given decimals to exact fractions,\n$$\n\\pi(\\text{State 3}) = \\frac{3}{5}, \\quad \\pi(\\text{State 1}) = \\frac{1}{10}, \\quad q(3 \\mid 1) = \\frac{4}{5}, \\quad q(1 \\mid 3) = \\frac{1}{10}.\n$$\nSubstitute into the acceptance ratio:\n$$\n\\frac{\\pi(3)\\,q(1 \\mid 3)}{\\pi(1)\\,q(3 \\mid 1)} = \\frac{\\left(\\frac{3}{5}\\right)\\left(\\frac{1}{10}\\right)}{\\left(\\frac{1}{10}\\right)\\left(\\frac{4}{5}\\right)} = \\frac{\\frac{3}{50}}{\\frac{4}{50}} = \\frac{3}{4}.\n$$\nTherefore,\n$$\n\\alpha_{1 \\to 3} = \\min\\left(1, \\frac{3}{4}\\right) = \\frac{3}{4}.\n$$", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "1962671"}, {"introduction": "A correctly calculated acceptance probability does not guarantee a successful simulation, as the algorithm's practical performance depends heavily on its parameters. This thought experiment [@problem_id:1962668] explores one of the most common pitfalls: poor mixing when sampling from a multimodal distribution. By considering the effect of a proposal distribution with a small step size, you will develop an intuition for why a sampler might become \"stuck\" in one region of the parameter space, failing to discover other important modes and thus leading to biased estimates.", "problem": "A data scientist is analyzing the posterior probability distribution for a parameter $\\theta$ of a complex climate model. The analysis reveals that the posterior distribution, denoted as $p(\\theta)$, is bimodal, with two distinct peaks of high probability located at $\\theta_A$ and $\\theta_B$, separated by a wide region of very low probability. To explore this distribution and estimate properties like the posterior mean, the scientist employs the Metropolis-Hastings (M-H) algorithm.\n\nThe M-H sampler is initialized with a starting value $\\theta_0$ located within the high-probability region around the first peak, $\\theta_A$. A symmetric proposal distribution $q(\\theta' | \\theta) = \\mathcal{N}(\\theta' | \\theta, \\sigma^2)$ is used, where $\\mathcal{N}$ is a normal distribution centered at the current state $\\theta$ with a standard deviation $\\sigma$, which represents the proposal step size. The scientist, aiming for a high acceptance rate, chooses a very small value for $\\sigma$ relative to the distance between the two modes, $|\\theta_A - \\theta_B|$.\n\nAfter running the M-H sampler for a very large number of iterations, which of the following descriptions most accurately characterizes the expected outcome of this simulation?\n\nA. The sampler will efficiently find the global maximum of the posterior distribution $p(\\theta)$ and remain there, thus providing an excellent point estimate for the parameter.\n\nB. The acceptance rate for proposed states will be very low, causing the chain to remain near the initial state $\\theta_0$ and explore very little of the parameter space.\n\nC. The generated chain of samples will be highly autocorrelated, and its histogram will largely represent the shape of the mode around $\\theta_A$, while failing to discover the mode around $\\theta_B$.\n\nD. The samples will alternate between the two modes in a systematic fashion, jumping from the region of $\\theta_A$ to the region of $\\theta_B$ and back again with regular frequency.\n\nE. The states of the chain will be nearly independent of one another, indicating that the sampler has successfully converged to the true bimodal posterior distribution.", "solution": "We analyze the Metropolis-Hastings (M-H) sampler with a symmetric proposal and very small proposal scale relative to the distance between two separated modes of the posterior $p(\\theta)$ at $\\theta_{A}$ and $\\theta_{B}$.\n\nFor a symmetric proposal $q(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta' \\mid \\theta, \\sigma^{2})$, the Metropolis-Hastings acceptance probability is\n$$\na(\\theta \\rightarrow \\theta') = \\min\\left\\{1, \\frac{p(\\theta')}{p(\\theta)}\\right\\}.\n$$\nThe chain is initialized at $\\theta_{0}$ in the high-probability region around $\\theta_{A}$. Because $\\sigma$ is chosen to be very small compared to the separation $|\\theta_{A} - \\theta_{B}|$, proposed moves satisfy $|\\theta' - \\theta| = O(\\sigma)$ and thus remain very close to the current state. In a high-probability region near a mode, $p(\\theta')$ is close to $p(\\theta)$ for small steps, so\n$$\n\\frac{p(\\theta')}{p(\\theta)} \\approx 1,\n$$\nimplying that $a(\\theta \\rightarrow \\theta') \\approx 1$ and the local acceptance rate is high. Hence, option B (very low acceptance rate) is contradicted by the small-step, within-mode behavior.\n\nNext, consider transitions between the modes. The probability to directly propose a state near $\\theta_{B}$ from a current state near $\\theta_{A}$ under the Gaussian proposal is\n$$\nq(\\theta_{B} \\mid \\theta) \\propto \\exp\\left(-\\frac{|\\theta_{B} - \\theta|^{2}}{2 \\sigma^{2}}\\right).\n$$\nSince $\\sigma \\ll |\\theta_{A} - \\theta_{B}|$, this proposal probability is exponentially small in $|\\theta_{A} - \\theta_{B}|^{2} / \\sigma^{2}$. Therefore, direct jumps across the low-probability valley are essentially never proposed on practical time scales. Alternatively, crossing the valley via many small steps requires repeatedly proposing moves into regions where $p(\\theta') \\ll p(\\theta)$, for which\n$$\na(\\theta \\rightarrow \\theta') = \\min\\left\\{1, \\frac{p(\\theta')}{p(\\theta)}\\right\\} \\ll 1,\n$$\nso such steps are overwhelmingly rejected. Consequently, the chain becomes effectively trapped near $\\theta_{A}$ for a very long time, failing to discover $\\theta_{B}$ in practice.\n\nBecause moves are very local and the chain stays within the same mode, successive samples are highly autocorrelated. The empirical histogram thus reflects the local shape around $\\theta_{A}$ but does not capture the separated mode near $\\theta_{B}$. This rules out option E (independence and successful convergence) and option D (regular alternation between modes). Option A is incorrect because M-H is a sampler targeting $p(\\theta)$, not an optimizer; moreover, with small steps and bimodality, it neither efficiently finds nor remains at a global maximum.\n\nTherefore, the most accurate description is that the chain exhibits high autocorrelation, predominantly samples the neighborhood of $\\theta_{A}$, and fails to discover the second mode $\\theta_{B}$.\n\nThe correct choice is C.", "answer": "$$\\boxed{C}$$", "id": "1962668"}, {"introduction": "This final practice problem [@problem_id:3160171] challenges you to synthesize your understanding in a more advanced setting common in modern Bayesian statistics, such as model selection. Here, you will design a sampler for a parameter that itself controls model complexity, like the number of clusters $K$. This involves deriving the Hastings correction for an asymmetric \"birth-and-death\" proposal mechanism and implementing the complete algorithm, reinforcing the deep connection between the underlying theory of detailed balance and robust computational practice.", "problem": "You are asked to design and implement a Metropolis–Hastings sampler over a one-dimensional discrete parameter space representing the number of clusters $K \\in \\{1,2,\\dots,K_{\\max}\\}$ using birth and death proposals that change $K$ by $\\pm 1$. Your task is to derive, from first principles, the correct acceptance probability that includes the Hastings correction for proposal asymmetry, and then implement a complete, runnable program that executes a short sampling experiment suite. The work must be done in purely mathematical and algorithmic terms.\n\nStart from the following fundamental base.\n- Construct a Markov chain with state space $\\{1,2,\\dots,K_{\\max}\\}$ that targets a given unnormalized posterior mass function $\\pi(K)$ on this discrete space.\n- Use detailed balance and the definition of the Metropolis–Hastings algorithm: the chain should have transition kernel $T(K \\to K') = q(K' \\mid K)\\,a(K \\to K')$ for $K' \\neq K$ and $T(K \\to K) = 1 - \\sum_{K' \\neq K} T(K \\to K')$, where $q$ is the proposal mass function and $a$ is the acceptance probability. Enforce detailed balance with respect to $\\pi$.\n\nProposal mechanism specification.\n- For interior states $1  K  K_{\\max}$, propose a birth $K' = K + 1$ with probability $p_{\\text{birth}} \\in (0,1)$ and a death $K' = K - 1$ with probability $p_{\\text{death}} = 1 - p_{\\text{birth}}$.\n- At the lower boundary $K = 1$, the death move is infeasible; renormalize by proposing the only feasible birth $K' = 2$ with probability $1$.\n- At the upper boundary $K = K_{\\max}$, the birth move is infeasible; renormalize by proposing the only feasible death $K' = K_{\\max} - 1$ with probability $1$.\n\nTarget distributions for the test suite.\nAll targets are defined up to a normalizing constant; your code should work solely with unnormalized log-masses to ensure numerical stability.\n- Case A (shape–rate form): for parameters $\\alpha  0$ and $\\beta \\ge 0$, let\n$$\n\\pi_{\\text{A}}(K) \\propto K^{\\beta}\\,\\exp(-\\alpha K), \\quad K \\in \\{1,2,\\dots,K_{\\max}\\}.\n$$\n- Case B (uniform): \n$$\n\\pi_{\\text{B}}(K) \\propto 1, \\quad K \\in \\{1,2,\\dots,K_{\\max}\\}.\n$$\n- Case C (strong decay): \n$$\n\\pi_{\\text{C}}(K) \\propto \\exp(-\\alpha K), \\quad K \\in \\{1,2,\\dots,K_{\\max}\\}.\n$$\n\nYour derivation requirements.\n- Starting from detailed balance, derive the general Metropolis–Hastings acceptance probability $a(K \\to K')$ that correctly accounts for the asymmetric proposal probabilities $q(K' \\mid K)$ induced by the birth–death mechanism with boundary renormalization. Your derivation must explicitly handle the interior and boundary cases $K \\in \\{1,K_{\\max}\\}$ and produce a single expression valid through the appropriate use of the Hastings correction factor.\n- You must provide the acceptance probability in a form implementable using log-masses to avoid numerical underflow.\n\nImplementation requirements.\n- Implement a function that computes the unnormalized log-mass $\\log \\pi(K)$ for a given case.\n- Implement a function that returns the Hastings correction factor $\\dfrac{q(K \\mid K')}{q(K' \\mid K)}$ implied by the birth–death proposal with boundary renormalization.\n- Implement a Metropolis–Hastings sampler that:\n  - Uses the proposal above with a fixed $p_{\\text{birth}} \\in (0,1)$ and $p_{\\text{death}} = 1 - p_{\\text{birth}}$.\n  - Computes the acceptance probability using your derived Hastings correction.\n  - Uses a fixed random seed per test case for reproducibility.\n  - Runs for a specified number of steps, includes a burn-in phase, and then computes the requested summary.\n\nTest suite.\nRun the sampler for the following parameter sets; all integers are in the set of positive integers and all reals are in the set of positive real numbers.\n- Case A:\n  - Target: $\\pi_{\\text{A}}$ with $\\alpha = 0.3$, $\\beta = 2$, $K_{\\max} = 20$.\n  - Proposal: $p_{\\text{birth}} = 0.6$.\n  - Chain: start at $K_0 = 5$, total steps $N = 100000$, burn-in $B = 10000$.\n  - Random seed: $12345$.\n  - Output for this case: the posterior mean $\\mathbb{E}[K]$ estimated by the sample average after burn-in as a float.\n- Case B:\n  - Target: $\\pi_{\\text{B}}$ with $K_{\\max} = 10$.\n  - Proposal: $p_{\\text{birth}} = 0.5$.\n  - Chain: start at $K_0 = 1$, total steps $N = 120000$, burn-in $B = 10000$.\n  - Random seed: $24680$.\n  - Output for this case: the posterior mass at $K = 1$, estimated by the post–burn-in sample proportion of time spent at $K = 1$, as a float.\n- Case C:\n  - Target: $\\pi_{\\text{C}}$ with $\\alpha = 2.0$, $K_{\\max} = 50$.\n  - Proposal: $p_{\\text{birth}} = 0.7$.\n  - Chain: start at $K_0 = 50$, total steps $N = 80000$, burn-in $B = 10000$.\n  - Random seed: $98765$.\n  - Output for this case: the acceptance rate, defined as the fraction of post–burn-in proposals that were accepted, as a float.\n\nFinal output format.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with no spaces, in the order [Case A result, Case B result, Case C result]. Each float should be rounded to $4$ decimal places, for example: $[9.5000,0.1000,0.2500]$.\n\nNotes.\n- No physical units are involved.\n- Angles are not involved.\n- All random choices must use the specified seeds to ensure reproducibility.", "solution": "The problem is valid as it is well-posed, scientifically grounded in the principles of Markov chain Monte Carlo methods, and provides a complete and consistent set of specifications for both the theoretical derivation and the computational implementation.\n\nThe core of the problem is to derive the acceptance probability for a Metropolis-Hastings sampler on a discrete state space $K \\in \\{1, 2, \\dots, K_{\\max}\\}$. The sampler must target a stationary distribution proportional to a given unnormalized mass function $\\pi(K)$. The algorithm's transitions must satisfy the detailed balance condition:\n$$\n\\pi(K) T(K \\to K') = \\pi(K') T(K' \\to K)\n$$\nfor any two states $K$ and $K'$. The transition probability $T(K \\to K')$ is defined as the product of the proposal probability $q(K' \\mid K)$ and the acceptance probability $a(K \\to K')$. Substituting this into the detailed balance equation gives:\n$$\n\\pi(K) q(K' \\mid K) a(K \\to K') = \\pi(K') q(K \\mid K') a(K' \\to K)\n$$\nThe standard Metropolis-Hastings choice for the acceptance probability $a(K \\to K')$ is:\n$$\na(K \\to K') = \\min\\left(1, \\frac{\\pi(K') q(K \\mid K')}{\\pi(K) q(K' \\mid K)}\\right)\n$$\nThis can be written as the product of the target ratio and the proposal ratio (Hastings correction):\n$$\na(K \\to K') = \\min\\left(1, \\frac{\\pi(K')}{\\pi(K)} \\times \\frac{q(K \\mid K')}{q(K' \\mid K)}\\right)\n$$\nTo complete the derivation, we must determine the proposal probabilities $q(K' \\mid K)$ based on the specified birth-death mechanism. The only allowed proposals are to adjacent states, i.e., $K' = K \\pm 1$.\n\nLet's define the proposal probabilities $q(K' \\mid K)$:\n- For an interior state $K$ where $1  K  K_{\\max}$:\n  - A birth proposal $K' = K+1$ is made with probability $p_{\\text{birth}}$. Thus, $q(K+1 \\mid K) = p_{\\text{birth}}$.\n  - A death proposal $K' = K-1$ is made with probability $p_{\\text{death}} = 1 - p_{\\text{birth}}$. Thus, $q(K-1 \\mid K) = p_{\\text{death}}$.\n- At the lower boundary $K=1$:\n  - The only possible move is a birth to $K'=2$, which is proposed with probability $1$. Thus, $q(2 \\mid 1) = 1$.\n- At the upper boundary $K=K_{\\max}$:\n  - The only possible move is a death to $K'=K_{\\max}-1$, which is proposed with probability $1$. Thus, $q(K_{\\max}-1 \\mid K_{\\max}) = 1$.\nAll other proposal probabilities $q(K' \\mid K)$ for $K' \\neq K$ are $0$.\n\nNow we derive the Hastings correction factor, $\\frac{q(K \\mid K')}{q(K' \\mid K)}$, for each possible move.\n\n**1. Birth Proposal: $K' = K+1$**\nWe are at state $K$ and propose to move to $K+1$. The reverse move is from $K+1$ to $K$.\n- Case B1: $K=1$, proposing $K'=2$.\n  - Forward proposal probability: $q(2 \\mid 1) = 1$.\n  - Reverse proposal probability (a death from $K'=2$): $q(1 \\mid 2) = p_{\\text{death}}$.\n  - Hastings correction: $\\frac{q(1 \\mid 2)}{q(2 \\mid 1)} = \\frac{p_{\\text{death}}}{1} = p_{\\text{death}}$.\n- Case B2: $1  K  K_{\\max}-1$, proposing $K' = K+1$.\n  - Forward proposal probability: $q(K+1 \\mid K) = p_{\\text{birth}}$.\n  - Reverse proposal probability (a death from interior state $K+1$): $q(K \\mid K+1) = p_{\\text{death}}$.\n  - Hastings correction: $\\frac{q(K \\mid K+1)}{q(K+1 \\mid K)} = \\frac{p_{\\text{death}}}{p_{\\text{birth}}}$.\n- Case B3: $K=K_{\\max}-1$, proposing $K'=K_{\\max}$.\n  - Forward proposal probability: $q(K_{\\max} \\mid K_{\\max}-1) = p_{\\text{birth}}$.\n  - Reverse proposal probability (a death from boundary state $K_{\\max}$): $q(K_{\\max}-1 \\mid K_{\\max}) = 1$.\n  - Hastings correction: $\\frac{q(K_{\\max}-1 \\mid K_{\\max})}{q(K_{\\max} \\mid K_{\\max}-1)} = \\frac{1}{p_{\\text{birth}}}$.\n\n**2. Death Proposal: $K' = K-1$**\nWe are at state $K$ and propose to move to $K-1$. The reverse move is from $K-1$ to $K$.\n- Case D1: $K=K_{\\max}$, proposing $K'=K_{\\max}-1$.\n  - Forward proposal probability: $q(K_{\\max}-1 \\mid K_{\\max}) = 1$.\n  - Reverse proposal probability (a birth from $K'=K_{\\max}-1$): $q(K_{\\max} \\mid K_{\\max}-1) = p_{\\text{birth}}$.\n  - Hastings correction: $\\frac{q(K_{\\max} \\mid K_{\\max}-1)}{q(K_{\\max}-1 \\mid K_{\\max})} = \\frac{p_{\\text{birth}}}{1} = p_{\\text{birth}}$.\n- Case D2: $2  K  K_{\\max}$, proposing $K'=K-1$.\n  - Forward proposal probability: $q(K-1 \\mid K) = p_{\\text{death}}$.\n  - Reverse proposal probability (a birth from interior state $K-1$): $q(K \\mid K-1) = p_{\\text{birth}}$.\n  - Hastings correction: $\\frac{q(K \\mid K-1)}{q(K-1 \\mid K)} = \\frac{p_{\\text{birth}}}{p_{\\text{death}}}$.\n- Case D3: $K=2$, proposing $K'=1$.\n  - Forward proposal probability: $q(1 \\mid 2) = p_{\\text{death}}$.\n  - Reverse proposal probability (a birth from boundary state $K'=1$): $q(2 \\mid 1) = 1$.\n  - Hastings correction: $\\frac{q(2 \\mid 1)}{q(1 \\mid 2)} = \\frac{1}{p_{\\text{death}}}$.\n\nThese cases cover all valid transitions and provide the necessary Hastings correction factors.\n\nFor numerical stability, the acceptance probability calculation is performed using logarithms. Let the acceptance ratio be $R = \\frac{\\pi(K')}{\\pi(K)} \\frac{q(K \\mid K')}{q(K' \\mid K)}$. Its logarithm is:\n$$\n\\log R = \\log \\pi(K') - \\log \\pi(K) + \\log\\left(\\frac{q(K \\mid K')}{q(K' \\mid K)}\\right)\n$$\nThe acceptance probability is then $a(K \\to K') = \\min(1, \\exp(\\log R))$. An acceptance decision is made by drawing a random number $u \\sim U(0,1)$ and accepting the proposal if $u  a(K \\to K')$.\n\nThe unnormalized log-mass functions for the specified cases are:\n- Case A: $\\log\\pi_{\\text{A}}(K) = \\beta \\log K - \\alpha K$.\n- Case B: $\\log\\pi_{\\text{B}}(K) = \\log(1) = 0$.\n- Case C: $\\log\\pi_{\\text{C}}(K) = -\\alpha K$.\n\nThe implementation will consist of a main sampler function that encapsulates the logic derived above. This function will take the log-mass function, proposal parameters, and chain parameters as input. It will perform a burn-in phase, followed by a sampling phase where states and other statistics are collected. The required summary statistics (posterior mean, posterior mass, and acceptance rate) are computed from the post-burn-in results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef get_log_pi_func(case, params):\n    \"\"\"Returns the log-mass function for a given case and parameters.\"\"\"\n    if case == 'A':\n        alpha, beta = params['alpha'], params['beta']\n        return lambda K: beta * np.log(K) - alpha * K\n    elif case == 'B':\n        return lambda K: 0.0\n    elif case == 'C':\n        alpha = params['alpha']\n        return lambda K: -alpha * K\n    else:\n        raise ValueError(\"Unknown case\")\n\ndef get_hastings_correction(K_curr, K_prop, p_birth, K_max):\n    \"\"\"\n    Computes the Hastings correction factor q(K_curr | K_prop) / q(K_prop | K_curr).\n    \"\"\"\n    p_death = 1.0 - p_birth\n    \n    if K_prop > K_curr:  # Birth proposal\n        if K_curr == 1:\n            # Move 1->2. Reverse is 2->1. q(2|1)=1, q(1|2)=p_death. Factor=p_death/1\n            return p_death\n        elif K_curr == K_max - 1:\n            # Move K_max-1 -> K_max. Reverse is K_max -> K_max-1. q(fwd)=p_birth, q(rev)=1. Factor=1/p_birth\n            return 1.0 / p_birth\n        else: # Interior birth\n            # Move K->K+1. Reverse is K+1->K. q(fwd)=p_birth, q(rev)=p_death. Factor=p_death/p_birth\n            return p_death / p_birth\n    \n    else:  # Death proposal K_prop  K_curr\n        if K_curr == K_max:\n            # Move K_max -> K_max-1. Reverse is K_max-1 -> K_max. q(fwd)=1, q(rev)=p_birth. Factor=p_birth/1\n            return p_birth\n        elif K_curr == 2:\n            # Move 2->1. Reverse is 1->2. q(fwd)=p_death, q(rev)=1. Factor=1/p_death\n            return 1.0 / p_death\n        else: # Interior death\n            # Move K->K-1. Reverse is K-1->K. q(fwd)=p_death, q(rev)=p_birth. Factor=p_birth/p_death\n            return p_birth / p_death\n\ndef run_sampler(log_pi_func, K0, N, B, K_max, p_birth, seed, task):\n    \"\"\"\n    Runs the Metropolis-Hastings sampler for a given configuration.\n    task can be 'mean', 'mass_at_1', or 'acceptance_rate'.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    current_K = K0\n    samples = []\n    \n    # Burn-in phase\n    for _ in range(B):\n        # Propose state\n        if current_K == 1:\n            proposed_K = 2\n        elif current_K == K_max:\n            proposed_K = K_max - 1\n        else:\n            if rng.random()  p_birth:\n                proposed_K = current_K + 1\n            else:\n                proposed_K = current_K - 1\n        \n        # Acceptance step\n        log_pi_curr = log_pi_func(current_K)\n        log_pi_prop = log_pi_func(proposed_K)\n        \n        hastings_corr = get_hastings_correction(current_K, proposed_K, p_birth, K_max)\n        \n        log_r = (log_pi_prop - log_pi_curr) + np.log(hastings_corr)\n        \n        acceptance_prob = min(1.0, np.exp(log_r))\n        \n        if rng.random()  acceptance_prob:\n            current_K = proposed_K\n\n    # Sampling / Measurement phase\n    num_accepted_post_burn = 0\n    num_proposals_post_burn = N - B\n    \n    for _ in range(num_proposals_post_burn):\n        # Propose state\n        if current_K == 1:\n            proposed_K = 2\n        elif current_K == K_max:\n            proposed_K = K_max - 1\n        else:\n            if rng.random()  p_birth:\n                proposed_K = current_K + 1\n            else:\n                proposed_K = current_K - 1\n        \n        # Acceptance step\n        log_pi_curr = log_pi_func(current_K)\n        log_pi_prop = log_pi_func(proposed_K)\n        \n        hastings_corr = get_hastings_correction(current_K, proposed_K, p_birth, K_max)\n        \n        log_r = (log_pi_prop - log_pi_curr) + np.log(hastings_corr)\n        \n        acceptance_prob = min(1.0, np.exp(log_r))\n\n        is_accepted = rng.random()  acceptance_prob\n        if is_accepted:\n            current_K = proposed_K\n            num_accepted_post_burn += 1\n            \n        if task in ['mean', 'mass_at_1']:\n            samples.append(current_K)\n\n    # Compute requested statistic\n    if task == 'mean':\n        return np.mean(samples)\n    elif task == 'mass_at_1':\n        return np.mean(np.array(samples) == 1)\n    elif task == 'acceptance_rate':\n        return num_accepted_post_burn / num_proposals_post_burn\n    else:\n        raise ValueError(\"Unknown task\")\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite, then prints the results.\n    \"\"\"\n    test_cases = [\n        # Case A\n        {\n            'case': 'A',\n            'params': {'alpha': 0.3, 'beta': 2, 'K_max': 20},\n            'proposal': {'p_birth': 0.6},\n            'chain': {'K0': 5, 'N': 100000, 'B': 10000, 'seed': 12345},\n            'task': 'mean'\n        },\n        # Case B\n        {\n            'case': 'B',\n            'params': {'K_max': 10},\n            'proposal': {'p_birth': 0.5},\n            'chain': {'K0': 1, 'N': 120000, 'B': 10000, 'seed': 24680},\n            'task': 'mass_at_1'\n        },\n        # Case C\n        {\n            'case': 'C',\n            'params': {'alpha': 2.0, 'K_max': 50},\n            'proposal': {'p_birth': 0.7},\n            'chain': {'K0': 50, 'N': 80000, 'B': 10000, 'seed': 98765},\n            'task': 'acceptance_rate'\n        }\n    ]\n\n    results = []\n    for cfg in test_cases:\n        K_max = cfg['params']['K_max']\n        log_pi_func = get_log_pi_func(cfg['case'], cfg['params'])\n        \n        result = run_sampler(\n            log_pi_func=log_pi_func,\n            K0=cfg['chain']['K0'],\n            N=cfg['chain']['N'],\n            B=cfg['chain']['B'],\n            K_max=K_max,\n            p_birth=cfg['proposal']['p_birth'],\n            seed=cfg['chain']['seed'],\n            task=cfg['task']\n        )\n        results.append(f\"{result:.4f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3160171"}]}