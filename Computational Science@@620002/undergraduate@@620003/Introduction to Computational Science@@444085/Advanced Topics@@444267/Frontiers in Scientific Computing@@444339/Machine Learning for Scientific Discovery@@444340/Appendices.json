{"hands_on_practices": [{"introduction": "Complex scientific models often depend on many input parameters, making them difficult to analyze. A powerful technique for taming this complexity is to find a low-dimensional \"active subspace\" that captures most of the model's variation. This exercise provides hands-on practice with the active subspace method, a tool for dimension reduction in parameter studies. You will learn to identify the most influential directions in a model's parameter space by analyzing its gradients, a fundamental step towards building more efficient surrogate models. [@problem_id:3157324]", "problem": "Consider a parameter-to-output map $f:\\mathbb{R}^d \\to \\mathbb{R}$ used in computational science to approximate a Quantity of Interest (QOI) arising from a Partial Differential Equation (PDE). In active subspaces, one seeks directions in parameter space along which $f$ varies most on average with respect to an input distribution. Starting from the first-order Taylor expansion, local variation is governed by the gradient $\\nabla_\\theta f(\\theta)$. The global average directional sensitivity along a unit vector $u$ is quantified by the quadratic form $u^\\top C u$, where the average gradient outer-product matrix is defined by\n$$\nC \\equiv \\mathbb{E}\\left[\\nabla_\\theta f(\\theta)\\,\\nabla_\\theta f(\\theta)^\\top\\right],\n$$\nwith the expectation taken over the distribution of $\\theta$. The active subspace consists of the leading eigenvectors of $C$; larger eigenvalues indicate directions with higher mean-squared directional derivative.\n\nYour task is to implement a complete program that, for a given scalar function $f$, dimension $d$, sample size $N$, threshold $\\tau$, and random seed $s$, will:\n(1) sample input parameters $\\theta_i \\sim \\mathcal{N}(0, I_d)$ independently for $i=1,\\dots,N$, \n(2) compute exact analytic gradients $\\nabla_\\theta f(\\theta_i)$ using only the fundamental rules of multivariable calculus (particularly the chain rule and linearity) and no finite-difference approximations,\n(3) estimate the matrix $C$ by the Monte Carlo average\n$$\n\\widehat{C} = \\frac{1}{N}\\sum_{i=1}^N \\left(\\nabla_\\theta f(\\theta_i)\\right)\\left(\\nabla_\\theta f(\\theta_i)\\right)^\\top,\n$$\n(4) compute the eigenvalues of $\\widehat{C}$, sort them in descending order, and determine the smallest nonnegative integer $m \\in \\{0,1,\\dots,d\\}$ such that\n$$\n\\frac{\\sum_{j=1}^m \\lambda_j}{\\sum_{j=1}^d \\lambda_j}  \\tau,\n$$\nwhere $\\lambda_1  \\lambda_2  \\dots  \\lambda_d  0$ are the eigenvalues of $\\widehat{C}$. If the denominator is zero, set $m$ to $0$ by convention. Return $m$ as the result for each test case.\n\nYou must base your derivation and design on the fundamental definition of the gradient and its role in first-order sensitivity. The program must not use any external data or user input.\n\nUse the following three scientifically plausible functions $f$ that isolate different mechanisms of variation and are amenable to analytic gradients:\n(1) A rank-$1$ function generated by a linear functional composed with a cubic nonlinearity:\n$$\nf_{\\text{rank1}}(\\theta) = \\left(w^\\top \\theta\\right)^3,\n$$\nwhere $w\\in\\mathbb{R}^d$ is a unit vector drawn from a standard normal distribution and then normalized to unit length.\n(2) An isotropic quadratic function:\n$$\nf_{\\text{iso}}(\\theta) = \\frac{1}{2}\\left\\|\\theta\\right\\|_2^2,\n$$\nwhich yields equal sensitivity in all directions under the standard normal input.\n(3) A rank-$2$ function formed by the sum of two orthogonal cubic ridge functions:\n$$\nf_{\\text{rank2}}(\\theta) = \\left(v_1^\\top \\theta\\right)^3 + \\left(v_2^\\top \\theta\\right)^3,\n$$\nwhere $v_1, v_2 \\in \\mathbb{R}^d$ are orthonormal unit vectors constructed by drawing two independent standard normal random vectors and then orthonormalizing them (for example, by Gram–Schmidt).\n\nAll random vectors ($w$, $v_1$, $v_2$) must be generated deterministically from the specified seed $s$ using a standard pseudorandom number generator, and then normalized to unit length (and orthonormalized for the pair). The input parameters must be sampled as $\\theta_i \\sim \\mathcal{N}(0, I_d)$ using the same seed $s$ for reproducibility.\n\nDesign a test suite that covers a happy path case, isotropic boundary behavior, and multi-directional activity:\nTest Case $1$: function $f_{\\text{rank1}}$, $d = 7$, $N = 4096$, $\\tau = 0.9$, seed $s = 11$.\nTest Case $2$: function $f_{\\text{iso}}$, $d = 5$, $N = 2048$, $\\tau = 0.68$, seed $s = 13$.\nTest Case $3$: function $f_{\\text{rank2}}$, $d = 10$, $N = 4096$, $\\tau = 0.9$, seed $s = 17$.\nTest Case $4$: function $f_{\\text{iso}}$, $d = 4$, $N = 1024$, $\\tau = 0.99$, seed $s = 19$.\n\nAll quantities are dimensionless; no physical units or angles are involved. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[m_1,m_2,m_3,m_4]$), where each $m_k$ is the integer active subspace dimension for the $k$-th test case computed according to the threshold definition above.", "solution": "The posed problem is valid. It is scientifically grounded in the principles of sensitivity analysis and uncertainty quantification, specifically the method of active subspaces. The problem is well-posed, with all necessary functions, parameters, and procedures clearly defined to ensure a unique, deterministic solution. It is objective and free of any non-scientific or ambiguous language. We may, therefore, proceed with a formal solution.\n\nThe central goal is to determine the dimension, $m$, of the active subspace for a given function $f(\\theta)$. This dimension represents the number of dominant directions in the parameter space $\\mathbb{R}^d$ along which the function's output exhibits the most significant variation on average. The method is based on the spectral analysis of the matrix $C = \\mathbb{E}\\left[\\nabla_\\theta f(\\theta)\\,\\nabla_\\theta f(\\theta)^\\top\\right]$, which aggregates the local, gradient-driven sensitivity information over the entire input parameter distribution. We will estimate $C$ using a Monte Carlo approach and then find $m$ by analyzing the cumulative energy of its eigenvalues.\n\nThe solution is structured as follows: First, we derive the analytic gradients for the specified functions. Second, we formalize the procedure for generating the required random vectors and input samples deterministically. Third, we detail the algorithm for estimating the matrix $C$ and computing its eigenvalues. Finally, we describe the method for calculating the active subspace dimension $m$ based on the specified energy threshold $\\tau$.\n\n**1. Analytic Gradient Derivation**\n\nThe problem requires the use of exact analytic gradients. We derive these for each of the three function families using the fundamental rules of multivariable calculus. Let $\\theta = [\\theta_1, \\dots, \\theta_d]^\\top \\in \\mathbb{R}^d$.\n\n*   **Function 1: $f_{\\text{rank1}}(\\theta) = (w^\\top \\theta)^3$**\n    This function is a composition of a linear functional $u(\\theta) = w^\\top \\theta = \\sum_{k=1}^d w_k \\theta_k$ and a cubic function $g(u) = u^3$. Applying the chain rule, the gradient is $\\nabla_\\theta f = \\frac{dg}{du} \\nabla_\\theta u$.\n    The derivative of $g(u)$ is $\\frac{dg}{du} = 3u^2 = 3(w^\\top \\theta)^2$.\n    The gradient of $u(\\theta)$ with respect to $\\theta$ is $\\nabla_\\theta (w^\\top \\theta) = w$.\n    Therefore, the gradient of $f_{\\text{rank1}}$ is:\n    $$\n    \\nabla_\\theta f_{\\text{rank1}}(\\theta) = 3(w^\\top \\theta)^2 w\n    $$\n\n*   **Function 2: $f_{\\text{iso}}(\\theta) = \\frac{1}{2}\\left\\|\\theta\\right\\|_2^2$**\n    This isotropic quadratic function can be written as $f_{\\text{iso}}(\\theta) = \\frac{1}{2} \\sum_{k=1}^d \\theta_k^2$. The partial derivative with respect to an arbitrary component $\\theta_j$ is:\n    $$\n    \\frac{\\partial f_{\\text{iso}}}{\\partial \\theta_j} = \\frac{\\partial}{\\partial \\theta_j} \\left( \\frac{1}{2} \\sum_{k=1}^d \\theta_k^2 \\right) = \\frac{1}{2} (2\\theta_j) = \\theta_j\n    $$\n    As this holds for all $j=1, \\dots, d$, the gradient vector is simply the input vector itself:\n    $$\n    \\nabla_\\theta f_{\\text{iso}}(\\theta) = \\theta\n    $$\n\n*   **Function 3: $f_{\\text{rank2}}(\\theta) = (v_1^\\top \\theta)^3 + (v_2^\\top \\theta)^3$**\n    The gradient operator is linear. Thus, we can compute the gradient of each term separately and sum the results. Each term has the same form as $f_{\\text{rank1}}$. Applying our previous result, we get:\n    $$\n    \\nabla_\\theta \\left( (v_1^\\top \\theta)^3 \\right) = 3(v_1^\\top \\theta)^2 v_1\n    $$\n    $$\n    \\nabla_\\theta \\left( (v_2^\\top \\theta)^3 \\right) = 3(v_2^\\top \\theta)^2 v_2\n    $$\n    The gradient of $f_{\\text{rank2}}$ is the sum of these two vectors:\n    $$\n    \\nabla_\\theta f_{\\text{rank2}}(\\theta) = 3(v_1^\\top \\theta)^2 v_1 + 3(v_2^\\top \\theta)^2 v_2\n    $$\n\n**2. Deterministic Generation of Random Vectors and Samples**\n\nTo ensure reproducibility, all random numbers must be generated deterministically from a given seed $s$. A pseudorandom number generator (PRNG) is initialized with $s$. This same PRNG instance is used to generate all required random vectors.\n\n*   **Parameter Vectors ($w, v_1, v_2$):**\n    For $f_{\\text{rank1}}$, a vector is drawn from the standard normal distribution $\\mathcal{N}(0, I_d)$, and then its Euclidean norm is normalized to $1$ to produce the unit vector $w$.\n    For $f_{\\text{rank2}}$, two independent vectors, say $u_1$ and $u_2$, are drawn from $\\mathcal{N}(0, I_d)$. They are then made orthonormal using the Gram-Schmidt process.\n    First, $v_1$ is obtained by normalizing $u_1$: $v_1 = u_1 / \\|u_1\\|_2$.\n    Second, the component of $u_2$ parallel to $v_1$ is removed: $u_2' = u_2 - (v_1^\\top u_2) v_1$.\n    Finally, the resulting vector $u_2'$ is normalized to produce $v_2 = u_2' / \\|u_2'\\|_2$. The event that $u_1$ and $u_2$ are linearly dependent, making $\\|u_2'\\|_2 = 0$, has probability zero for continuous distributions, but robust code should handle this unlikely case.\n\n*   **Input Samples ($\\theta_i$):**\n    For each test case, $N$ independent samples $\\theta_i$ for $i=1, \\dots, N$ are drawn from the multivariate standard normal distribution $\\mathcal{N}(0, I_d)$, where $I_d$ is the $d \\times d$ identity matrix.\n\n**3. Monte Carlo Estimation and Eigendecomposition**\n\nThe matrix $C$ is estimated by the sample mean of the gradient outer products, denoted $\\widehat{C}$:\n$$\n\\widehat{C} = \\frac{1}{N}\\sum_{i=1}^N g_i g_i^\\top\n$$\nwhere $g_i = \\nabla_\\theta f(\\theta_i)$. The algorithm proceeds as follows:\n1. Initialize a $d \\times d$ matrix $\\widehat{C}$ to all zeros.\n2. For each sample $i=1, \\dots, N$:\n   a. Draw a sample $\\theta_i \\sim \\mathcal{N}(0, I_d)$.\n   b. Compute the analytic gradient vector $g_i = \\nabla_\\theta f(\\theta_i)$ using the appropriate derived formula.\n   c. Compute the outer product $g_i g_i^\\top$, a $d \\times d$ matrix.\n   d. Add this matrix to the accumulator $\\widehat{C}$.\n3. After the loop, divide the accumulated matrix by the sample size $N$.\n\nThe matrix $\\widehat{C}$ is, by construction, symmetric and positive semidefinite. We then compute its eigenvalues. Since $\\widehat{C}$ is real and symmetric, all its eigenvalues are real and non-negative. An efficient numerical algorithm, such as the one implemented in `numpy.linalg.eigh`, is used for this purpose. The resulting eigenvalues $\\{\\lambda_j\\}_{j=1}^d$ are sorted in descending order: $\\lambda_1  \\lambda_2  \\dots  \\lambda_d  0$.\n\n**4. Active Subspace Dimension Calculation**\n\nThe final step is to find the smallest non-negative integer $m$ such that the fraction of variance captured by the leading $m$ eigenvalues meets or exceeds the threshold $\\tau$.\nThe total variance is the sum of all eigenvalues, $S = \\sum_{j=1}^d \\lambda_j$. The cumulative variance captured by the first $k$ eigenvalues is $\\sum_{j=1}^k \\lambda_j$. We seek the smallest $m \\in \\{0, 1, \\dots, d\\}$ satisfying:\n$$\n\\frac{\\sum_{j=1}^m \\lambda_j}{S}  \\tau\n$$\nThe procedure is as follows:\n1. Calculate the total sum $S$. If $S$ is effectively zero (e.g., smaller than a machine precision tolerance), this indicates a constant function with zero gradient everywhere. In this case, as prescribed, we set $m=0$.\n2. We initialize a cumulative sum to zero. We iterate through the sorted eigenvalues from $j=1$ to $d$, adding each $\\lambda_j$ to the cumulative sum.\n3. In each step of the iteration, we check if the ratio of the current cumulative sum to the total sum $S$ is greater than or equal to $\\tau$.\n4. The first index $j$ for which this condition holds gives the active subspace dimension, $m=j$. The process is then terminated for the current test case.\n5. If the loop completes without the condition being met (which could happen if $\\tau=1.0$ due to floating-point inaccuracies), the dimension is taken to be $d$.\n\nThis completes the theoretical and algorithmic framework for solving the problem. The implementation will directly follow these steps for each test case provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for active subspace dimension calculation.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'f_type': 'rank1', 'd': 7, 'N': 4096, 'tau': 0.9, 's': 11},\n        {'f_type': 'iso', 'd': 5, 'N': 2048, 'tau': 0.68, 's': 13},\n        {'f_type': 'rank2', 'd': 10, 'N': 4096, 'tau': 0.9, 's': 17},\n        {'f_type': 'iso', 'd': 4, 'N': 1024, 'tau': 0.99, 's': 19},\n    ]\n\n    results = []\n    for case in test_cases:\n        m = compute_active_subspace_dimension(**case)\n        results.append(m)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef compute_active_subspace_dimension(f_type, d, N, tau, s):\n    \"\"\"\n    Computes the active subspace dimension for a single test case.\n\n    Args:\n        f_type (str): The type of function ('rank1', 'iso', 'rank2').\n        d (int): The dimension of the parameter space.\n        N (int): The number of Monte Carlo samples.\n        tau (float): The energy threshold for the active subspace.\n        s (int): The random seed for reproducibility.\n\n    Returns:\n        int: The computed active subspace dimension 'm'.\n    \"\"\"\n    \n    # 1. Initialize deterministic random number generator\n    rng = np.random.default_rng(s)\n\n    # 2. Define gradient function based on f_type\n    grad_func = None\n    if f_type == 'rank1':\n        # Generate the fixed random vector w\n        w_raw = rng.normal(size=d)\n        w = w_raw / np.linalg.norm(w_raw)\n        \n        def grad_rank1(theta):\n            # Gradient of f(theta) = (w^T * theta)^3 is 3 * (w^T * theta)^2 * w\n            return 3 * (w.T @ theta)**2 * w\n        grad_func = grad_rank1\n\n    elif f_type == 'iso':\n        # Gradient of f(theta) = 0.5 * ||theta||^2 is theta\n        grad_func = lambda theta: theta\n\n    elif f_type == 'rank2':\n        # Generate two orthonormal vectors v1, v2 using Gram-Schmidt\n        u1_raw = rng.normal(size=d)\n        v1 = u1_raw / np.linalg.norm(u1_raw)\n        \n        u2_raw = rng.normal(size=d)\n        proj_u2_on_v1 = (v1.T @ u2_raw) * v1\n        u2_prime = u2_raw - proj_u2_on_v1\n        norm_u2_prime = np.linalg.norm(u2_prime)\n        \n        # This case is highly improbable with continuous distributions\n        if norm_u2_prime  1e-15:\n            # Regenerate until we get a linearly independent vector\n            while norm_u2_prime  1e-15:\n                u2_raw = rng.normal(size=d)\n                proj_u2_on_v1 = (v1.T @ u2_raw) * v1\n                u2_prime = u2_raw - proj_u2_on_v1\n                norm_u2_prime = np.linalg.norm(u2_prime)\n\n        v2 = u2_prime / norm_u2_prime\n        \n        def grad_rank2(theta):\n            # Gradient of f(theta) = (v1^T*theta)^3 + (v2^T*theta)^3\n            # is 3*(v1^T*theta)^2*v1 + 3*(v2^T*theta)^2*v2\n            term1 = 3 * (v1.T @ theta)**2 * v1\n            term2 = 3 * (v2.T @ theta)**2 * v2\n            return term1 + term2\n        grad_func = grad_rank2\n\n    # 3. Estimate the matrix C using Monte Carlo\n    C_hat = np.zeros((d, d))\n    \n    # Generate all samples at once for efficiency\n    thetas = rng.normal(size=(N, d))\n    \n    for i in range(N):\n        theta_i = thetas[i, :]\n        grad = grad_func(theta_i)\n        C_hat += np.outer(grad, grad)\n        \n    C_hat /= N\n    \n    # 4. Compute eigenvalues and determine the active subspace dimension m\n    \n    # C_hat is real and symmetric, so use eigh for efficiency and stability.\n    # It returns eigenvalues in ascending order.\n    eigenvalues = np.linalg.eigh(C_hat)[0]\n    \n    # Sort eigenvalues in descending order and ensure non-negativity\n    eigenvalues = np.sort(eigenvalues)[::-1]\n    eigenvalues = np.maximum(0, eigenvalues)\n\n    total_eig_sum = np.sum(eigenvalues)\n\n    # Handle the case where the function is constant (zero gradient)\n    if total_eig_sum  1e-15:\n        return 0\n    \n    # Find the smallest m such that the cumulative energy exceeds tau\n    cumulative_eig_sum = 0\n    for i in range(d):\n        cumulative_eig_sum += eigenvalues[i]\n        if cumulative_eig_sum / total_eig_sum = tau:\n            return i + 1\n\n    # If threshold is not met even with all eigenvalues (e.g., tau=1.0 and rounding)\n    return d\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3157324"}, {"introduction": "Simulating complex systems across all relevant scales is often computationally impossible, leading scientists to create simpler \"coarse-grained\" models. This practice explores the critical task of validating such a learned model against its high-resolution source. You will assess whether a simple autoregressive model replicates the long-term statistical memory of a complex stochastic process by comparing their autocorrelation functions, a key test for physical consistency in model reduction. [@problem_id:3157253]", "problem": "Consider a one-dimensional stationary stochastic process defined by the fundamental law of linear relaxation with thermal fluctuations: the Ornstein–Uhlenbeck process (OU), whose dynamics in continuous time are modeled by the stochastic differential equation (SDE) $$dx(t) = -\\kappa \\, x(t) \\, dt + \\sqrt{2 D} \\, dW(t),$$ where $x(t)$ is the state, $\\kappa  0$ is the relaxation rate, $D  0$ is the diffusion level, and $W(t)$ is a standard Wiener process with independent increments. A high-resolution numerical simulation of this process can be performed using the forward Euler–Maruyama method on a computational grid with fine time step $dt$, which is a well-tested method for simulating SDEs.\n\nIn machine learning for scientific discovery, it is common to \"learn\" a coarse-grained model from high-resolution data. A widely used coarse-grained representation for a scalar, approximately Markovian process sampled at a fixed coarse interval $\\Delta$ is the first-order autoregressive model, written in discrete time as $$y_{n+1} = a \\, y_n + \\varepsilon_n,$$ where $a$ is an unknown parameter to be learned from data and $\\varepsilon_n$ is a zero-mean Gaussian innovation with variance $\\sigma_\\varepsilon^2$ to be estimated from training residuals. The long-time statistics of interest include the normalized autocorrelation function $$\\rho(k) = \\frac{C(k)}{C(0)}, \\quad C(k) = \\mathbb{E}\\left[(z_n - \\mu)(z_{n+k} - \\mu)\\right],$$ where $z_n$ is a discrete-time stationary series, $\\mu$ is its mean, and $k$ is a nonnegative integer lag. For empirical estimation from a single realization, use the standard time-average estimator $$\\widehat{C}(k) = \\frac{1}{M - k} \\sum_{n=0}^{M-k-1} \\left(z_n - \\overline{z}\\right)\\left(z_{n+k} - \\overline{z}\\right), \\quad \\overline{z} = \\frac{1}{M} \\sum_{n=0}^{M-1} z_n,$$ and the normalized autocorrelation $$\\widehat{\\rho}(k) = \\frac{\\widehat{C}(k)}{\\widehat{C}(0)}.$$ The objective is to test whether a learned coarse-grained model preserves long-time statistics by comparing $\\widehat{\\rho}(k)$ computed from coarse samples of the full-resolution simulation to $\\widehat{\\rho}(k)$ from the learned model simulation over a range of lags.\n\nProgram requirements:\n- Implement a high-resolution simulation of the Ornstein–Uhlenbeck process via the Euler–Maruyama method over $N$ steps of size $dt$, with initial condition $x(0)$ drawn from the stationary distribution.\n- Downsample the high-resolution trajectory by a positive integer factor $m$ to obtain a coarse series of length $M = \\lfloor (N+1)/m \\rfloor$ at coarse interval $\\Delta = m \\, dt$.\n- Learn the coarse parameter $a$ from an initial training fraction $f_{\\text{train}}$ of the downsampled series using least squares under the zero-mean assumption, that is, fit $$a = \\arg\\min_{a} \\sum_{n=0}^{N_{\\text{tr}}-2} \\left(z_{n+1} - a \\, z_n\\right)^2,$$ where $N_{\\text{tr}} = \\lfloor f_{\\text{train}} \\, M \\rfloor$ and $z_n$ are the training samples. To model measurement imperfections, optionally perturb the training samples by adding independent Gaussian noise with standard deviation $\\sigma_{\\text{train}}$ before fitting.\n- Estimate the innovation variance $\\sigma_\\varepsilon^2$ from the residuals on the same training subset using $$\\widehat{\\sigma}_\\varepsilon^2 = \\frac{1}{N_{\\text{tr}}-1} \\sum_{n=0}^{N_{\\text{tr}}-2} \\left(z_{n+1} - \\widehat{a} \\, z_n\\right)^2.$$\n- Simulate the learned coarse model for $M$ steps using the estimated $\\widehat{a}$ and $\\widehat{\\sigma}_\\varepsilon^2$, with initial value equal to the first coarse sample of the full-resolution series.\n- For both the downsampled full-resolution series and the learned model series, compute the normalized autocorrelation $\\widehat{\\rho}(k)$ for integer lags $k = 0, 1, \\dots, K_{\\max}$ using the estimator above.\n- Compute the maximum absolute deviation $$\\Delta_{\\max} = \\max_{0 \\le k \\le K_{\\max}} \\left| \\widehat{\\rho}_{\\text{full}}(k) - \\widehat{\\rho}_{\\text{learned}}(k) \\right|.$$ A coarse-grained model is said to preserve long-time statistics in this test if $\\Delta_{\\max}  \\text{tol}$ for a given tolerance $\\text{tol}$.\n\nYour program must implement the above and evaluate the following test suite, which explores typical, slow, and noisy-training regimes:\n- Test case $1$: $\\kappa = 0.5$, $D = 1.0$, $dt = 0.001$, $N = 100000$, $m = 50$, $f_{\\text{train}} = 0.5$, $\\sigma_{\\text{train}} = 0.0$, $K_{\\max} = 60$, $\\text{tol} = 0.05$.\n- Test case $2$: $\\kappa = 0.05$, $D = 1.0$, $dt = 0.001$, $N = 200000$, $m = 100$, $f_{\\text{train}} = 0.3$, $\\sigma_{\\text{train}} = 0.0$, $K_{\\max} = 80$, $\\text{tol} = 0.07$.\n- Test case $3$: $\\kappa = 0.9$, $D = 1.0$, $dt = 0.001$, $N = 100000$, $m = 80$, $f_{\\text{train}} = 0.2$, $\\sigma_{\\text{train}} = 0.5$, $K_{\\max} = 50$, $\\text{tol} = 0.05$.\n\nFor reproducibility, use a fixed random seed and ensure all random draws are independent across steps but deterministic given the seed.\n\nFinal output specification:\n- For each test case, output a boolean indicating whether the learned coarse model preserves long-time statistics according to the criterion $\\Delta_{\\max}  \\text{tol}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[{\\text{result}_1},{\\text{result}_2},{\\text{result}_3}]$, where each entry is either $\\text{True}$ or $\\text{False}$.", "solution": "The problem poses a well-defined and scientifically sound task in computational science, specifically at the intersection of stochastic processes and machine learning for model discovery. The components of the problem—the Ornstein-Uhlenbeck (OU) process, the Euler-Maruyama numerical integration method, the autoregressive (AR) model, least-squares parameter estimation, and autocorrelation analysis—are all standard and rigorously defined concepts in their respective fields. The parameters provided are physically and computationally reasonable, and the specified procedure is unambiguous and algorithmically formalizable. Therefore, the problem is deemed valid and a solution can be constructed by carefully following the specified steps.\n\nThe core of the problem is to generate data from a \"true\" high-resolution physical model, coarse-grain it, learn a simplified model from this coarse data, and then validate whether the learned model accurately reproduces the long-time statistical behavior of the true system.\n\nFirst, we simulate the high-resolution dynamics of the Ornstein-Uhlenbeck process, governed by the stochastic differential equation (SDE):\n$$dx(t) = -\\kappa \\, x(t) \\, dt + \\sqrt{2 D} \\, dW(t)$$\nwhere $\\kappa$ is the relaxation rate, $D$ is the diffusion constant, and $dW(t)$ represents the increments of a Wiener process. We employ the Euler-Maruyama method for numerical integration. This method discretizes the SDE over a small time step $dt$. The update rule for the state $x$ at step $i$ is:\n$$x_{i+1} = x_i - \\kappa x_i dt + \\sqrt{2 D dt} \\, \\xi_i$$\nwhere $\\xi_i$ is a random variable drawn from a standard normal distribution, $\\mathcal{N}(0, 1)$. The simulation is initialized with $x_0$ drawn from the process's stationary distribution, which is a zero-mean Gaussian with variance $\\sigma_x^2 = D/\\kappa$, i.e., $x_0 \\sim \\mathcal{N}(0, D/\\kappa)$. The simulation is run for $N$ steps, generating a trajectory $\\{x_i\\}_{i=0}^{N}$.\n\nSecond, we perform coarse-graining. The high-resolution trajectory $\\{x_i\\}$ is downsampled by an integer factor $m$ to produce a coarse-grained time series $\\{z_n\\}$. The new time interval is $\\Delta = m \\, dt$. The resulting series has a length of $M = \\lfloor (N+1)/m \\rfloor$, as specified. This is achieved by taking every $m$-th point of the original trajectory, truncated to length $M$.\n\nThird, we learn a coarse-grained model from this data. The proposed model is a first-order autoregressive, or AR($1$), process:\n$$y_{n+1} = a \\, y_n + \\varepsilon_n$$\nwhere $a$ is the autoregressive coefficient and $\\varepsilon_n$ is a zero-mean Gaussian white noise term with variance $\\sigma_\\varepsilon^2$. Both $a$ and $\\sigma_\\varepsilon^2$ are to be learned from an initial fraction $f_{\\text{train}}$ of the coarse series $\\{z_n\\}$. The training data consists of the first $N_{\\text{tr}} = \\lfloor f_{\\text{train}} M \\rfloor$ points of $\\{z_n\\}$. Before fitting, this training data may be perturbed by adding independent Gaussian noise with standard deviation $\\sigma_{\\text{train}}$. The parameter $a$ is estimated using ordinary least squares under a zero-mean (zero-intercept) assumption. The objective is to minimize the sum of squared errors:\n$$\\min_{a} \\sum_{n=0}^{N_{\\text{tr}}-2} (z_{n+1} - a z_n)^2$$\nThe solution to this minimization problem provides the estimate $\\widehat{a}$:\n$$\\widehat{a} = \\frac{\\sum_{n=0}^{N_{\\text{tr}}-2} z_n z_{n+1}}{\\sum_{n=0}^{N_{\\text{tr}}-2} z_n^2}$$\nThe variance of the innovation term, $\\widehat{\\sigma}_\\varepsilon^2$, is then estimated from the residuals on the same training data using the provided formula:\n$$\\widehat{\\sigma}_\\varepsilon^2 = \\frac{1}{N_{\\text{tr}}-1} \\sum_{n=0}^{N_{\\text{tr}}-2} (z_{n+1} - \\widehat{a} z_n)^2$$\n\nFourth, a new time series $\\{y_n\\}_{n=0}^{M-1}$ is generated by simulating the learned AR($1$) model for $M$ steps, using the estimated parameters $\\widehat{a}$ and $\\widehat{\\sigma}_\\varepsilon = \\sqrt{\\widehat{\\sigma}_\\varepsilon^2}$. The simulation is initialized with the first value of the coarse-grained series, $y_0 = z_0$.\n\nFinally, we compare the statistical properties of the \"true\" coarse-grained series $\\{z_n\\}$ and the \"learned\" series $\\{y_n\\}$. The comparison is based on the normalized autocorrelation function (ACF), $\\widehat{\\rho}(k)$. For a given time series $\\{s_n\\}$ of length $M$, the empirical autocovariance at lag $k$ is computed as:\n$$\\widehat{C}(k) = \\frac{1}{M - k} \\sum_{n=0}^{M-k-1} (s_n - \\overline{s})(s_{n+k} - \\overline{s}), \\quad \\text{where} \\quad \\overline{s} = \\frac{1}{M}\\sum_{n=0}^{M-1}s_n$$\nThe normalized ACF is then $\\widehat{\\rho}(k) = \\widehat{C}(k) / \\widehat{C}(0)$. We compute $\\widehat{\\rho}_{\\text{full}}(k)$ from $\\{z_n\\}$ and $\\widehat{\\rho}_{\\text{learned}}(k)$ from $\\{y_n\\}$ for lags $k = 0, 1, \\dots, K_{\\max}$. The performance of the learned model is quantified by the maximum absolute deviation between these two functions:\n$$\\Delta_{\\max} = \\max_{0 \\le k \\le K_{\\max}} \\left| \\widehat{\\rho}_{\\text{full}}(k) - \\widehat{\\rho}_{\\text{learned}}(k) \\right|$$\nThe learned model is considered to have successfully preserved the long-time statistics if this deviation is within a given tolerance, i.e., $\\Delta_{\\max}  \\text{tol}$. This entire procedure is then applied to each test case defined in the problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    # A fixed random seed is used for reproducibility.\n    np.random.seed(42)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'kappa': 0.5, 'D': 1.0, 'dt': 0.001, 'N': 100000, 'm': 50, 'f_train': 0.5, 'sigma_train': 0.0, 'K_max': 60, 'tol': 0.05},\n        {'kappa': 0.05, 'D': 1.0, 'dt': 0.001, 'N': 200000, 'm': 100, 'f_train': 0.3, 'sigma_train': 0.0, 'K_max': 80, 'tol': 0.07},\n        {'kappa': 0.9, 'D': 1.0, 'dt': 0.001, 'N': 100000, 'm': 80, 'f_train': 0.2, 'sigma_train': 0.5, 'K_max': 50, 'tol': 0.05},\n    ]\n\n    results = []\n    for params in test_cases:\n        result = run_test_case(**params)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_test_case(kappa, D, dt, N, m, f_train, sigma_train, K_max, tol):\n    \"\"\"\n    Executes the full simulation and analysis for a single test case.\n    \"\"\"\n    # Step 1: High-resolution simulation of the Ornstein-Uhlenbeck process.\n    # The stationary distribution is N(0, D/kappa).\n    sigma_x = np.sqrt(D / kappa)\n    x = np.zeros(N + 1)\n    x[0] = np.random.normal(0, sigma_x)\n\n    # Wiener process increments, pre-generated for efficiency\n    dW = np.random.normal(0, np.sqrt(dt), N)\n\n    for i in range(N):\n        x[i+1] = x[i] - kappa * x[i] * dt + np.sqrt(2 * D) * dW[i]\n\n    # Step 2: Downsample the high-resolution trajectory.\n    M = int((N + 1) / m)\n    z_full = x[0::m][:M]\n\n    # Step 3: Learn the coarse parameters 'a' and 'sigma_epsilon^2'.\n    N_tr = int(f_train * M)\n    z_train_orig = z_full[:N_tr]\n    \n    # Optionally perturb training samples with measurement noise.\n    if sigma_train  0.0:\n        noise = np.random.normal(0, sigma_train, size=N_tr)\n        z_train_fit = z_train_orig + noise\n    else:\n        z_train_fit = z_train_orig\n\n    y_vec = z_train_fit[1:N_tr]\n    x_vec = z_train_fit[0:N_tr-1]\n\n    # Estimate 'a' using the least squares formula.\n    a_hat = np.dot(x_vec, y_vec) / np.dot(x_vec, x_vec)\n\n    # Estimate innovation variance from residuals on the (potentially noisy) training set.\n    residuals = y_vec - a_hat * x_vec\n    sigma_eps_sq_hat = np.sum(residuals**2) / (N_tr - 1)\n    sigma_eps_hat = np.sqrt(sigma_eps_sq_hat)\n    \n    # Step 4: Simulate the learned coarse model.\n    y_learned = np.zeros(M)\n    y_learned[0] = z_full[0]  # Initialize with the first coarse sample.\n    \n    # Generate innovation terms for the learned model simulation.\n    innovations = np.random.normal(0, sigma_eps_hat, M - 1)\n    for n in range(M - 1):\n        y_learned[n+1] = a_hat * y_learned[n] + innovations[n]\n\n    # Step 5: Compute normalized autocorrelation for both series.\n    acf_full = compute_acf(z_full, K_max)\n    acf_learned = compute_acf(y_learned, K_max)\n\n    # Step 6: Compute the maximum absolute deviation and check against tolerance.\n    delta_max = np.max(np.abs(acf_full - acf_learned))\n    \n    return delta_max = tol\n\ndef compute_acf(series, k_max):\n    \"\"\"\n    Computes the normalized autocorrelation function for a given series.\n    \"\"\"\n    M = len(series)\n    mean_series = np.mean(series)\n    centered_series = series - mean_series\n    \n    # Calculate C(0) for normalization.\n    # The estimator specifies division by (M-k), so for k=0, it's M.\n    c0 = np.dot(centered_series, centered_series) / M\n    if c0 == 0:\n        # Handle the case of a constant series to avoid division by zero.\n        return np.ones(k_max + 1)\n\n    # Calculate C(k) for k  0.\n    acf = np.zeros(k_max + 1)\n    acf[0] = 1.0 # rho(0) is always 1\n    \n    for k in range(1, k_max + 1):\n        # Slices for the dot product sum from n=0 to M-k-1.\n        term1 = centered_series[:M-k]\n        term2 = centered_series[k:]\n        ck = np.dot(term1, term2) / (M - k)\n        acf[k] = ck / c0\n        \n    return acf\n\nsolve()\n\n```", "id": "3157253"}, {"introduction": "A grand challenge in science is automatically discovering governing equations from observational data. Sparse regression offers a framework for this task by selecting a few significant terms from a large library of candidate functions. In this capstone exercise, you will implement a sophisticated sparse regression algorithm to identify the hidden partial differential equation governing a system, stepping into the role of an automated scientist and learning a core technique in modern data-driven discovery. [@problem_id:3157268]", "problem": "You will implement a sparse regression method to select relevant terms from a polynomial library designed for partial differential equation structure discovery using a convex composite optimization model based on sparse group regularization. Start from the following fundamental base: the linear empirical risk minimization principle for supervised learning that minimizes the empirical least-squares risk, the convexity of the squared loss, and the definition of the proximal operator for structured penalties. Specifically, let the feature matrix be denoted by $X \\in \\mathbb{R}^{n \\times p}$ and the target vector by $y \\in \\mathbb{R}^{n}$. Consider the convex objective that combines least squares with a sparse group penalty of the form\n$$\n\\min_{w \\in \\mathbb{R}^{p}} \\; \\frac{1}{2n}\\lVert X w - y \\rVert_2^2 \\;+\\; \\lambda \\left( (1 - \\alpha) \\sum_{g \\in \\mathcal{G}} \\lVert w_g \\rVert_2 \\;+\\; \\alpha \\lVert w \\rVert_1 \\right),\n$$\nwhere $n$ is the number of samples, $p$ is the number of features, $w$ is the coefficient vector, $\\lambda \\in \\mathbb{R}_{0}$ is the regularization strength, $\\alpha \\in (0,1)$ interpolates between group sparsity and elementwise sparsity, and $\\mathcal{G}$ is a partition of the feature indices into non-overlapping groups, with $w_g$ denoting the subvector of $w$ indexed by group $g$. The grouping $\\mathcal{G}$ you will use corresponds to terms grouped by spatial derivative order in a polynomial library for a one-dimensional scalar field. You must solve this optimization using a principled first-order proximal gradient method with a provably valid stepsize derived from the gradient’s Lipschitz constant. Do not use any prepackaged machine learning solvers.\n\nConstruct a synthetic dataset as follows, using analytic expressions to avoid numerical differentiation errors. Let $x$ and $t$ be independent variables sampled on a rectangular grid with $N_x$ equally spaced points on $[0,1]$ and $N_t$ equally spaced points on $[0,1]$; let $n = N_x \\cdot N_t$. Define the scalar field\n$$\nu(x,t) = \\sin(2\\pi x)\\, e^{-t} + \\frac{1}{2} \\cos(4\\pi x)\\, e^{-0.2\\, t},\n$$\nand its first and second spatial derivatives\n$$\nu_x(x,t) = 2\\pi \\cos(2\\pi x)\\, e^{-t} - 2\\pi \\sin(4\\pi x)\\, e^{-0.2\\, t},\n$$\n$$\nu_{xx}(x,t) = - (2\\pi)^2 \\sin(2\\pi x)\\, e^{-t} - 8\\pi^2 \\cos(4\\pi x)\\, e^{-0.2\\, t}.\n$$\nBuild a polynomial library $X \\in \\mathbb{R}^{n \\times p}$ with $p = 9$ columns in the following order:\n$[\\, 1,\\; u,\\; u^2,\\; u_x,\\; u\\,u_x,\\; u^2 u_x,\\; u_{xx},\\; u\\,u_{xx},\\; (u_x)^2 \\,]$.\nDefine the true generating mechanism for the response as a linear combination of exactly two terms:\n$$\ny = \\beta_{u u_x} \\cdot (u\\,u_x) + \\beta_{u_{xx}} \\cdot u_{xx} + \\varepsilon,\n$$\nwith $\\beta_{u u_x} = 2.0$, $\\beta_{u_{xx}} = 0.1$, and additive independent Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$ of prescribed standard deviation $\\sigma$. The ground-truth nonzero support of coefficients thus consists of exactly the indices corresponding to $u\\,u_x$ and $u_{xx}$ in the specified ordering.\n\nPartition the $p = 9$ features into $3$ non-overlapping groups by spatial derivative order:\n- Group $0$ (order $0$): indices $[0,1,2]$ for $[1,\\; u,\\; u^2]$.\n- Group $1$ (order $1$): indices $[3,4,5]$ for $[u_x,\\; u\\,u_x,\\; u^2 u_x]$.\n- Group $2$ (order $2$): indices $[6,7,8]$ for $[u_{xx},\\; u\\,u_{xx},\\; (u_x)^2]$.\n\nImplement a proximal gradient algorithm that minimizes the given objective. Derive a valid constant stepsize from a bound on the gradient Lipschitz constant, and implement the correct proximal operator for the sparse group penalty. Standardize each column of $X$ to zero mean and unit variance, and center $y$ by subtracting its mean before optimization. After optimization, determine the predicted support by thresholding the absolute value of coefficients with a user-specified threshold $\\tau  0$, counting an index as selected if $|w_j| \\ge \\tau$. Compute precision and recall as follows: precision is the number of true positives divided by the total number of predicted positives, and recall is the number of true positives divided by the total number of true nonzeros. Express these as decimals (not percentages).\n\nYour program must implement the following test suite, run all cases, and aggregate the results. Use $N_x = 64$ and $N_t = 16$ so that $n = 1024$. For each case construct a noise vector with the specified standard deviation $\\sigma$. The regularization parameter should be set as $\\lambda = \\kappa \\cdot \\Lambda(\\alpha)$, where $\\kappa$ is a provided positive scalar and $\\Lambda(\\alpha)$ is a data-dependent scale computed from standardized $(X,y)$ by\n$$\n\\Lambda(\\alpha) = \\max\\left( \\frac{1}{n}\\lVert X^\\top y \\rVert_{\\infty} \\cdot \\frac{1}{\\alpha}, \\; \\max_{g \\in \\mathcal{G}} \\frac{1}{n}\\lVert X_g^\\top y \\rVert_2 \\cdot \\frac{1}{1 - \\alpha} \\right),\n$$\nwith the conventions that division by zero is ignored by omitting the corresponding term when $\\alpha$ equals $0$ or $1$, which will not occur in the test suite. This scaling provides a robust reference magnitude for $\\lambda$ relative to the observed data.\n\nTest suite parameters, each specified as a tuple $(\\alpha, \\kappa, \\sigma, \\tau)$:\n- Case $1$: $(0.5, 0.10, 0.0, 10^{-3})$.\n- Case $2$: $(0.5, 0.20, 0.01, 10^{-3})$.\n- Case $3$: $(0.8, 0.30, 0.02, 10^{-3})$.\n- Case $4$: $(0.2, 0.60, 0.05, 10^{-3})$.\n\nFor each case, fit the model using proximal gradient descent with a principled stepsize, then compute precision and recall of the recovered support against the ground truth. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$[\\, \\text{precision}_1, \\text{recall}_1, \\text{precision}_2, \\text{recall}_2, \\text{precision}_3, \\text{recall}_3, \\text{precision}_4, \\text{recall}_4 \\,]$,\nwith each value rounded to exactly three decimal places. No other output should be printed. Angles are not used, so no angle unit is needed. There are no physical units in this problem. All computations are to be performed using real numbers. The final output must strictly adhere to the specified format.", "solution": "The user has provided a valid problem statement.\n\n### Step 1: Extract Givens\n\n- **Objective Function**: Minimize the convex objective function for $w \\in \\mathbb{R}^{p}$:\n$$\n\\frac{1}{2n}\\lVert X w - y \\rVert_2^2 \\;+\\; \\lambda \\left( (1 - \\alpha) \\sum_{g \\in \\mathcal{G}} \\lVert w_g \\rVert_2 \\;+\\; \\alpha \\lVert w \\rVert_1 \\right)\n$$\n- **Data**: Feature matrix $X \\in \\mathbb{R}^{n \\times p}$ and target vector $y \\in \\mathbb{R}^{n}$, with $n$ samples and $p$ features.\n- **Parameters**: Regularization strength $\\lambda \\in \\mathbb{R}_{0}$, interpolation parameter $\\alpha \\in (0,1)$.\n- **Groups**: $\\mathcal{G}$ is a partition of feature indices $\\{0, 1, \\dots, p-1\\}$ into non-overlapping groups. $w_g$ is the subvector of $w$ for group $g$.\n- **Synthetic Dataset**:\n    - Grid: $N_x = 64$ points on $[0,1]$ and $N_t = 16$ points on $[0,1]$, giving $n = N_x \\cdot N_t = 1024$ total sample points.\n    - Scalar field: $u(x,t) = \\sin(2\\pi x)\\, e^{-t} + \\frac{1}{2} \\cos(4\\pi x)\\, e^{-0.2\\, t}$.\n    - Spatial derivatives: $u_x(x,t) = 2\\pi \\cos(2\\pi x)\\, e^{-t} - 2\\pi \\sin(4\\pi x)\\, e^{-0.2\\, t}$ and $u_{xx}(x,t) = - (2\\pi)^2 \\sin(2\\pi x)\\, e^{-t} - 8\\pi^2 \\cos(4\\pi x)\\, e^{-0.2\\, t}$.\n    - Feature library ($p=9$): $X$ has columns in the order $[\\, 1,\\; u,\\; u^2,\\; u_x,\\; u\\,u_x,\\; u^2 u_x,\\; u_{xx},\\; u\\,u_{xx},\\; (u_x)^2 \\,]$.\n    - Target vector: $y = \\beta_{u u_x} \\cdot (u\\,u_x) + \\beta_{u_{xx}} \\cdot u_{xx} + \\varepsilon$, with coefficients $\\beta_{u u_x} = 2.0$, $\\beta_{u_{xx}} = 0.1$, and noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$.\n- **Ground Truth**: The true non-zero coefficients correspond to terms $u\\,u_x$ (index $4$) and $u_{xx}$ (index $6$).\n- **Grouping Structure**: $p=9$ features partitioned into $3$ groups by derivative order:\n    - Group $0$: indices $[0, 1, 2]$ for $[1, u, u^2]$.\n    - Group $1$: indices $[3, 4, 5]$ for $[u_x, u\\,u_x, u^2 u_x]$.\n    - Group $2$: indices $[6, 7, 8]$ for $[u_{xx}, u\\,u_{xx}, (u_x)^2]$.\n- **Method**: Proximal gradient descent with a principled constant stepsize.\n- **Preprocessing**: Standardize columns of $X$ to zero mean and unit variance. Center $y$ by subtracting its mean.\n- **Regularization Scaling**: $\\lambda = \\kappa \\cdot \\Lambda(\\alpha)$, where $\\Lambda(\\alpha) = \\max\\left( \\frac{1}{n}\\lVert X^\\top y \\rVert_{\\infty} \\cdot \\frac{1}{\\alpha}, \\; \\max_{g \\in \\mathcal{G}} \\frac{1}{n}\\lVert X_g^\\top y \\rVert_2 \\cdot \\frac{1}{1 - \\alpha} \\right)$, computed on standardized/centered data.\n- **Evaluation**:\n    - Predicted support: Indices $j$ where $|w_j| \\ge \\tau$.\n    - Metrics: Precision and Recall, defined as $\\text{precision} = \\frac{\\text{TP}}{\\text{predicted positives}}$ and $\\text{recall} = \\frac{\\text{TP}}{\\text{true nonzeros}}$.\n- **Test Suite**: $4$ cases with parameters $(\\alpha, \\kappa, \\sigma, \\tau)$:\n    1. $(0.5, 0.10, 0.0, 10^{-3})$\n    2. $(0.5, 0.20, 0.01, 10^{-3})$\n    3. $(0.8, 0.30, 0.02, 10^{-3})$\n    4. $(0.2, 0.60, 0.05, 10^{-3})$\n- **Output Format**: Single line `[precision_1, recall_1, ..., precision_4, recall_4]` with values rounded to $3$ decimal places.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is scientifically grounded, well-posed, and objective. It is based on established principles of convex optimization and sparse regression, standard in machine learning and computational science. The problem is self-contained, providing all necessary formulas, parameters, and procedures for constructing the dataset and implementing the algorithm. The setup is consistent, and the task is computationally feasible. No flaws according to the validation checklist were found.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. A solution will be provided.\n\n### Principle-Based Design of the Solution\n\nThe problem requires solving a convex composite optimization problem using a proximal gradient method. The objective function is of the form $\\min_w F(w) = f(w) + g(w)$, where $f(w)$ is a smooth, convex function and $g(w)$ is a convex, possibly non-smooth, regularizer.\n\n1.  **Decomposition of the Objective Function**:\n    The objective function is decomposed as:\n    - Smooth part (least-squares loss): $f(w) = \\frac{1}{2n}\\lVert X w - y \\rVert_2^2$.\n    - Non-smooth part (sparse group penalty): $g(w) = \\lambda \\left( (1 - \\alpha) \\sum_{g \\in \\mathcal{G}} \\lVert w_g \\rVert_2 + \\alpha \\lVert w \\rVert_1 \\right)$.\n    For the optimization procedure, we use the standardized feature matrix $\\tilde{X}$ and centered target vector $\\tilde{y}$. Thus, we minimize $f(w) = \\frac{1}{2n}\\lVert \\tilde{X} w - \\tilde{y} \\rVert_2^2$ plus the same regularizer $g(w)$.\n\n2.  **Proximal Gradient Method**:\n    The proximal gradient algorithm generates a sequence of iterates via the update rule:\n    $$\n    w^{(k+1)} = \\text{prox}_{\\gamma g}\\left(w^{(k)} - \\gamma \\nabla f(w^{(k)})\\right)\n    $$\n    where $\\gamma  0$ is the stepsize and $\\text{prox}_{\\gamma g}$ is the proximal operator of the function $\\gamma g$.\n\n3.  **Gradient of the Smooth Part**:\n    The gradient of the least-squares term $f(w)$ is:\n    $$\n    \\nabla f(w) = \\frac{1}{n} \\tilde{X}^\\top (\\tilde{X} w - \\tilde{y})\n    $$\n\n4.  **Stepsize Selection**:\n    The convergence of the proximal gradient method is guaranteed if the stepsize $\\gamma$ satisfies $0  \\gamma  1/L$, where $L$ is the Lipschitz constant of $\\nabla f(w)$. The gradient is Lipschitz continuous with constant $L = \\frac{1}{n} \\lVert \\tilde{X}^\\top \\tilde{X} \\rVert_2 = \\frac{1}{n} \\sigma_{\\max}(\\tilde{X}^\\top \\tilde{X})$, where $\\sigma_{\\max}$ denotes the maximum eigenvalue (since $\\tilde{X}^\\top \\tilde{X}$ is positive semi-definite). A principled choice is to set the stepsize to its upper bound, $\\gamma = 1/L$, which we compute from the data.\n\n5.  **Proximal Operator for Sparse Group Penalty**:\n    The proximal operator is defined as $\\text{prox}_{\\eta h}(v) = \\arg\\min_z \\left\\{ \\frac{1}{2}\\lVert z - v \\rVert_2^2 + \\eta h(z) \\right\\}$. The penalty $g(w)$ is separable across the predefined groups $\\mathcal{G}$. This means we can compute the proximal operator for each group subvector $w_g$ independently. For a single group $g$, we must solve:\n    $$\n    \\text{prox}_{\\gamma g_g}(v_g) = \\arg\\min_{w_g} \\left\\{ \\frac{1}{2}\\lVert w_g - v_g \\rVert_2^2 + \\gamma \\lambda(1-\\alpha)\\lVert w_g \\rVert_2 + \\gamma \\lambda\\alpha\\lVert w_g \\rVert_1 \\right\\}\n    $$\n    The solution to this subproblem, known as the sparse group lasso proximal operator, is a two-stage procedure:\n    a. Apply the element-wise soft-thresholding operator $S_{\\eta}(\\cdot)$ with threshold $\\eta_1 = \\gamma \\lambda \\alpha$ to the input vector $v_g$:\n    $$\n    u_g = S_{\\eta_1}(v_g) \\quad \\text{where } (S_{\\eta_1}(v_g))_j = \\text{sign}((v_g)_j) \\max(|\\,(v_g)_j| - \\eta_1, 0)\n    $$\n    b. Apply group-wise soft-thresholding (or block soft-thresholding) to the result $u_g$ with threshold $\\eta_2 = \\gamma \\lambda (1-\\alpha)$:\n    $$\n    w_g^* = u_g \\cdot \\max\\left(0, 1 - \\frac{\\eta_2}{\\lVert u_g \\rVert_2}\\right)\n    $$\n    This operation is performed for all groups $g \\in \\mathcal{G}$ to obtain the full updated vector $w^{(k+1)}$.\n\n6.  **Algorithm Summary**:\n    The implemented algorithm will proceed as follows for each test case:\n    a. Generate the data matrix $X$ and target vector $y$ according to the specified analytic functions and noise level $\\sigma$.\n    b. Standardize $X$ to get $\\tilde{X}$ and center $y$ to get $\\tilde{y}$.\n    c. Calculate the data-dependent regularization scale $\\Lambda(\\alpha)$ and the final regularization parameter $\\lambda = \\kappa \\Lambda(\\alpha)$.\n    d. Compute the Lipschitz constant $L$ and the stepsize $\\gamma = 1/L$.\n    e. Initialize the weight vector $w = \\mathbf{0}$.\n    f. Iterate the proximal gradient update rule until the change in $w$ is below a tolerance or a maximum number of iterations is reached.\n    g. On the final coefficient vector $w$, identify the non-zero support by thresholding with $\\tau$.\n    h. Compute the precision and recall of the recovered support against the known true support (indices $\\{4, 6\\}$).\n    i. Store and format the results as required.\n\nThis principled approach ensures a correct and robust implementation for solving the given scientific machine learning problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for sparse group regression.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (alpha, kappa, sigma, tau)\n        (0.5, 0.10, 0.0, 1e-3),\n        (0.5, 0.20, 0.01, 1e-3),\n        (0.8, 0.30, 0.02, 1e-3),\n        (0.2, 0.60, 0.05, 1e-3),\n    ]\n\n    results = []\n    \n    # Fixed parameters for data generation\n    Nx = 64\n    Nt = 16\n    n = Nx * Nt\n    p = 9\n    \n    # Grid generation\n    x_grid = np.linspace(0, 1, Nx)\n    t_grid = np.linspace(0, 1, Nt)\n    xx, tt = np.meshgrid(x_grid, t_grid)\n    x = xx.ravel()\n    t = tt.ravel()\n\n    # Analytic field and derivatives\n    u = np.sin(2 * np.pi * x) * np.exp(-t) + 0.5 * np.cos(4 * np.pi * x) * np.exp(-0.2 * t)\n    u_x = 2 * np.pi * np.cos(2 * np.pi * x) * np.exp(-t) - 2 * np.pi * np.sin(4 * np.pi * x) * np.exp(-0.2 * t)\n    u_xx = -(2 * np.pi)**2 * np.sin(2 * np.pi * x) * np.exp(-t) - 8 * np.pi**2 * np.cos(4 * np.pi * x) * np.exp(-0.2 * t)\n\n    # Polynomial library\n    X = np.zeros((n, p))\n    X[:, 0] = 1.0\n    X[:, 1] = u\n    X[:, 2] = u**2\n    X[:, 3] = u_x\n    X[:, 4] = u * u_x\n    X[:, 5] = u**2 * u_x\n    X[:, 6] = u_xx\n    X[:, 7] = u * u_xx\n    X[:, 8] = u_x**2\n\n    # Group definitions\n    groups = [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    true_support = {4, 6}\n\n    for i, case in enumerate(test_cases):\n        alpha, kappa, sigma, tau = case\n        \n        # Set seed for reproducible noise generation for each case\n        np.random.seed(i)\n        \n        # Generate target vector y\n        y_true = 2.0 * X[:, 4] + 0.1 * X[:, 6]\n        noise = np.random.normal(0, sigma, size=n)\n        y = y_true + noise\n\n        # Preprocessing: Standardize X and center y\n        y_mean = y.mean()\n        y_c = y - y_mean\n        \n        X_mean = X.mean(axis=0)\n        X_std = X.std(axis=0)\n        # Avoid division by zero for columns with zero variance (e.g., constant column)\n        X_std[X_std == 0] = 1.0\n        X_s = (X - X_mean) / X_std\n\n        # Calculate data-dependent regularization scale Lambda(alpha)\n        X_s_T_y_c = X_s.T @ y_c\n        \n        term1 = np.linalg.norm(X_s_T_y_c, ord=np.inf) / (n * alpha)\n        \n        term2_vals = []\n        for g_indices in groups:\n            norm_Xg_T_y_c = np.linalg.norm(X_s_T_y_c[g_indices], ord=2)\n            term2_vals.append(norm_Xg_T_y_c / (n * (1 - alpha)))\n        term2 = np.max(term2_vals)\n        \n        lambda_scale = max(term1, term2)\n        lambda_val = kappa * lambda_scale\n\n        # Proximal Gradient Descent\n        # Compute Lipschitz constant and stepsize\n        L = np.max(np.linalg.eigvalsh(X_s.T @ X_s / n))\n        if L == 0:\n            gamma = 1.0\n        else:\n            gamma = 1.0 / L\n\n        # Algorithm parameters\n        w = np.zeros(p)\n        max_iter = 20000\n        tol = 1e-7\n        lambda1 = lambda_val * (1 - alpha)\n        lambda2 = lambda_val * alpha\n\n        for _ in range(max_iter):\n            w_old = w.copy()\n            grad = X_s.T @ (X_s @ w - y_c) / n\n            v = w - gamma * grad\n            \n            w_new = np.zeros(p)\n            for g_indices in groups:\n                v_g = v[g_indices]\n                # Soft-thresholding for L1 penalty\n                u_g = np.sign(v_g) * np.maximum(np.abs(v_g) - gamma * lambda2, 0)\n                norm_u_g = np.linalg.norm(u_g)\n                # Group soft-thresholding for group L2 penalty\n                if norm_u_g  0:\n                    scale_factor = np.maximum(0, 1 - (gamma * lambda1) / norm_u_g)\n                    w_new[g_indices] = scale_factor * u_g\n            \n            w = w_new\n            if np.linalg.norm(w - w_old)  tol:\n                break\n        \n        # Evaluation\n        predicted_support = {j for j, val in enumerate(w) if np.abs(val) = tau}\n        \n        tp = len(true_support.intersection(predicted_support))\n        num_predicted_positives = len(predicted_support)\n        num_true_nonzeros = len(true_support)\n        \n        precision = tp / num_predicted_positives if num_predicted_positives  0 else 0.0\n        recall = tp / num_true_nonzeros\n        \n        results.append(precision)\n        results.append(recall)\n\n    # Format the final output string\n    output_str = \",\".join([f\"{r:.3f}\" for r in results])\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3157268"}]}