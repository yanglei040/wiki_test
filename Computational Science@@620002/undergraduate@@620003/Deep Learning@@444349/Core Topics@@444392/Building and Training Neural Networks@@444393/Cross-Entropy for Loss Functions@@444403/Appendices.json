{"hands_on_practices": [{"introduction": "A cornerstone of training neural network classifiers is understanding how the loss function's gradient drives learning. This first exercise takes you through the fundamental, first-principles derivation of the gradient for the softmax cross-entropy loss function [@problem_id:3110719]. By working through the calculus, you will uncover the remarkably simple and intuitive form of the gradient, providing a clear insight into how a model's parameters are updated to correct its errors.", "problem": "Consider a multiclass classifier that outputs a vector of logits $\\mathbf{z} \\in \\mathbb{R}^{K}$ for $K$ classes. The predicted class probabilities are obtained by applying the softmax function, defined for each class index $i \\in \\{1,2,\\dots,K\\}$ as\n$$\n\\hat{p}_{i}(\\mathbf{z}) = \\frac{\\exp(z_{i})}{\\sum_{j=1}^{K} \\exp(z_{j})}.\n$$\nLet the observed label be represented as a one-hot vector $\\mathbf{y} \\in \\{0,1\\}^{K}$ satisfying $\\sum_{i=1}^{K} y_{i} = 1$. The training objective is the Cross-Entropy (CE) loss,\n$$\n\\text{CE}(\\mathbf{z},\\mathbf{y}) = -\\sum_{i=1}^{K} y_{i} \\ln\\!\\big(\\hat{p}_{i}(\\mathbf{z})\\big).\n$$\nStarting only from these definitions, derive the gradient $\\nabla_{\\mathbf{z}} \\text{CE}(\\mathbf{z},\\mathbf{y})$ with respect to the logits $\\mathbf{z}$. Your derivation must use first principles and standard rules of calculus, clearly identifying where the product rule, chain rule, and properties of the softmax function are applied. Then interpret each term that appears in the gradient in terms of how the loss function responds to discrepancies between the predicted distribution and the one-hot label.\n\nFinally, for a concrete calculation, evaluate your derived gradient at the logits $\\mathbf{z} = [0,\\,0,\\,\\ln 2]$ and the label $\\mathbf{y} = [0,\\,1,\\,0]$. Express the final numerical result exactly; no rounding is required.", "solution": "The problem is valid. The definitions of the softmax function and cross-entropy loss are standard in the field of deep learning, and the task of deriving their gradient is a well-posed mathematical exercise.\n\nOur objective is to compute the gradient of the Cross-Entropy (CE) loss function with respect to the logits vector $\\mathbf{z}$, denoted as $\\nabla_{\\mathbf{z}} \\text{CE}(\\mathbf{z},\\mathbf{y})$. The gradient is a vector whose $k$-th component is the partial derivative of the loss with respect to the $k$-th logit, $\\frac{\\partial \\text{CE}}{\\partial z_k}$.\n\nThe loss function is given by:\n$$\n\\text{CE}(\\mathbf{z},\\mathbf{y}) = L = -\\sum_{i=1}^{K} y_{i} \\ln\\!\\big(\\hat{p}_{i}(\\mathbf{z})\\big)\n$$\nWe begin by applying the sum rule for differentiation to find the partial derivative with respect to an arbitrary logit $z_k$:\n$$\n\\frac{\\partial L}{\\partial z_k} = \\frac{\\partial}{\\partial z_k} \\left( -\\sum_{i=1}^{K} y_{i} \\ln(\\hat{p}_{i}) \\right) = -\\sum_{i=1}^{K} y_{i} \\frac{\\partial}{\\partial z_k} \\ln(\\hat{p}_{i})\n$$\nUsing the chain rule, the derivative of the natural logarithm is $\\frac{d}{dx}\\ln(u) = \\frac{1}{u}\\frac{du}{dx}$. Applying this, we get:\n$$\n\\frac{\\partial L}{\\partial z_k} = -\\sum_{i=1}^{K} \\frac{y_{i}}{\\hat{p}_{i}} \\frac{\\partial \\hat{p}_{i}}{\\partial z_k}\n$$\nThe next step is to compute the partial derivative of the softmax function output $\\hat{p}_i$ with respect to the logit $z_k$. The softmax function is:\n$$\n\\hat{p}_{i} = \\frac{\\exp(z_{i})}{\\sum_{j=1}^{K} \\exp(z_{j})}\n$$\nWe use the quotient rule for differentiation, $(\\frac{f}{g})' = \\frac{f'g - fg'}{g^2}$. Let $f(z_k) = \\exp(z_i)$ and $g(z_k) = \\sum_{j=1}^{K} \\exp(z_j)$.\nThe derivatives of the numerator and denominator with respect to $z_k$ are:\n$$\n\\frac{\\partial}{\\partial z_k} \\exp(z_i) = \\begin{cases} \\exp(z_i) & \\text{if } i=k \\\\ 0 & \\text{if } i \\neq k \\end{cases} = \\delta_{ik} \\exp(z_i)\n$$\nwhere $\\delta_{ik}$ is the Kronecker delta.\n$$\n\\frac{\\partial}{\\partial z_k} \\sum_{j=1}^{K} \\exp(z_j) = \\exp(z_k)\n$$\nApplying the quotient rule:\n$$\n\\frac{\\partial \\hat{p}_{i}}{\\partial z_k} = \\frac{(\\delta_{ik} \\exp(z_i)) \\left( \\sum_{j=1}^{K} \\exp(z_j) \\right) - \\exp(z_i) \\exp(z_k)}{\\left( \\sum_{j=1}^{K} \\exp(z_j) \\right)^2}\n$$\nWe can simplify this by splitting it into two parts:\n$$\n\\frac{\\partial \\hat{p}_{i}}{\\partial z_k} = \\frac{\\delta_{ik} \\exp(z_i)}{\\sum_{j=1}^{K} \\exp(z_j)} - \\frac{\\exp(z_i)}{\\sum_{j=1}^{K} \\exp(z_j)} \\frac{\\exp(z_k)}{\\sum_{j=1}^{K} \\exp(z_j)}\n$$\nRecognizing the definition of the softmax function, $\\hat{p}_i = \\frac{\\exp(z_i)}{\\sum_j \\exp(z_j)}$ and $\\hat{p}_k = \\frac{\\exp(z_k)}{\\sum_j \\exp(z_j)}$, we can rewrite the expression as:\n$$\n\\frac{\\partial \\hat{p}_{i}}{\\partial z_k} = \\delta_{ik} \\hat{p}_i - \\hat{p}_i \\hat{p}_k = \\hat{p}_i (\\delta_{ik} - \\hat{p}_k)\n$$\nThis general form for the derivative of the softmax function separates into two cases:\n1.  If $i = k$: $\\frac{\\partial \\hat{p}_{i}}{\\partial z_i} = \\hat{p}_i(1 - \\hat{p}_i)$\n2.  If $i \\neq k$: $\\frac{\\partial \\hat{p}_{i}}{\\partial z_k} = \\hat{p}_i(0 - \\hat{p}_k) = -\\hat{p}_i \\hat{p}_k$\n\nNow, we substitute this result back into our expression for the loss gradient:\n$$\n\\frac{\\partial L}{\\partial z_k} = -\\sum_{i=1}^{K} \\frac{y_{i}}{\\hat{p}_{i}} \\frac{\\partial \\hat{p}_{i}}{\\partial z_k} = -\\sum_{i=1}^{K} \\frac{y_{i}}{\\hat{p}_{i}} \\left( \\hat{p}_i (\\delta_{ik} - \\hat{p}_k) \\right)\n$$\nThe $\\hat{p}_i$ terms cancel out, simplifying the expression significantly:\n$$\n\\frac{\\partial L}{\\partial z_k} = -\\sum_{i=1}^{K} y_{i} (\\delta_{ik} - \\hat{p}_k)\n$$\nWe can split the summation into two parts:\n$$\n\\frac{\\partial L}{\\partial z_k} = -\\left( \\sum_{i=1}^{K} y_{i}\\delta_{ik} - \\sum_{i=1}^{K} y_{i}\\hat{p}_k \\right)\n$$\nLet's evaluate each sum:\n-  The first sum, $\\sum_{i=1}^{K} y_{i}\\delta_{ik}$, is non-zero only when $i=k$, where it becomes $y_k$. Thus, $\\sum_{i=1}^{K} y_{i}\\delta_{ik} = y_k$.\n-  In the second sum, $\\hat{p}_k$ is constant with respect to the summation index $i$. So, $\\sum_{i=1}^{K} y_{i}\\hat{p}_k = \\hat{p}_k \\sum_{i=1}^{K} y_i$. Since $\\mathbf{y}$ is a one-hot vector, we know that $\\sum_{i=1}^{K} y_i = 1$. Therefore, the sum is equal to $\\hat{p}_k$.\n\nSubstituting these results back:\n$$\n\\frac{\\partial L}{\\partial z_k} = -(y_k - \\hat{p}_k) = \\hat{p}_k - y_k\n$$\nThis remarkably simple result shows that the partial derivative of the cross-entropy loss with respect to the $k$-th logit is the difference between the predicted probability for class $k$ and the true label for class $k$. The complete gradient vector is therefore:\n$$\n\\nabla_{\\mathbf{z}} \\text{CE}(\\mathbf{z},\\mathbf{y}) = \\hat{\\mathbf{p}}(\\mathbf{z}) - \\mathbf{y}\n$$\n\nThe interpretation of the term $\\hat{p}_k - y_k$ is as follows. During training via gradient descent, the logit $z_k$ is updated according to $z_k \\leftarrow z_k - \\alpha (\\hat{p}_k - y_k)$, where $\\alpha > 0$ is the learning rate.\n- If class $k$ is the true class, then $y_k=1$. The gradient component is $\\hat{p}_k - 1$, which is negative or zero (since $\\hat{p}_k \\leq 1$). The update rule becomes $z_k \\leftarrow z_k - \\alpha(\\text{negative value})$, which increases $z_k$. This pushes the logit for the correct class higher, thereby increasing its probability $\\hat{p}_k$ toward the target of $1$.\n- If class $k$ is not the true class, then $y_k=0$. The gradient component is $\\hat{p}_k - 0 = \\hat{p}_k$, which is positive or zero (since $\\hat{p}_k \\geq 0$). The update rule becomes $z_k \\leftarrow z_k - \\alpha(\\text{positive value})$, which decreases $z_k$. This pushes the logits for incorrect classes lower, thereby decreasing their probabilities toward the target of $0$.\nEssentially, the gradient vector $\\hat{\\mathbf{p}} - \\mathbf{y}$ represents the error or residual between the predicted probability distribution and the true one-hot distribution.\n\nFinally, we evaluate this gradient for the given concrete case:\n- Logits: $\\mathbf{z} = [0, 0, \\ln 2]$. This implies $K=3$.\n- Label: $\\mathbf{y} = [0, 1, 0]$.\n\nFirst, we compute the predicted probabilities $\\hat{\\mathbf{p}}$ using the softmax function:\nThe denominator is $\\sum_{j=1}^{3} \\exp(z_j) = \\exp(0) + \\exp(0) + \\exp(\\ln 2) = 1 + 1 + 2 = 4$.\nThe probabilities are:\n$$\n\\hat{p}_1 = \\frac{\\exp(z_1)}{4} = \\frac{\\exp(0)}{4} = \\frac{1}{4}\n$$\n$$\n\\hat{p}_2 = \\frac{\\exp(z_2)}{4} = \\frac{\\exp(0)}{4} = \\frac{1}{4}\n$$\n$$\n\\hat{p}_3 = \\frac{\\exp(z_3)}{4} = \\frac{\\exp(\\ln 2)}{4} = \\frac{2}{4} = \\frac{1}{2}\n$$\nSo, the predicted probability vector is $\\hat{\\mathbf{p}} = [\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{2}]$.\n\nNow, we compute the gradient $\\nabla_{\\mathbf{z}} \\text{CE} = \\hat{\\mathbf{p}} - \\mathbf{y}$:\n$$\n\\nabla_{\\mathbf{z}} \\text{CE} = \\begin{pmatrix} \\frac{1}{4}, & \\frac{1}{4}, & \\frac{1}{2} \\end{pmatrix} - \\begin{pmatrix} 0, & 1, & 0 \\end{pmatrix}\n$$\n$$\n\\nabla_{\\mathbf{z}} \\text{CE} = \\begin{pmatrix} \\frac{1}{4} - 0, & \\frac{1}{4} - 1, & \\frac{1}{2} - 0 \\end{pmatrix}\n$$\n$$\n\\nabla_{\\mathbf{z}} \\text{CE} = \\begin{pmatrix} \\frac{1}{4}, & -\\frac{3}{4}, & \\frac{1}{2} \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{4} & -\\frac{3}{4} & \\frac{1}{2}\n\\end{pmatrix}\n}\n$$", "id": "3110719"}, {"introduction": "While accuracy tells us if a model's top prediction is correct, the cross-entropy loss provides a more nuanced picture by measuring the confidence of that prediction. This practice problem presents a concrete scenario where a model's accuracy on a validation set has plateaued, a common signal for early stopping [@problem_id:3110736]. By calculating and comparing the change in accuracy versus the change in Negative Log-Likelihood (NLL), you will gain practical insight into why monitoring the loss is often a more sensitive and informative way to track a model's learning progress.", "problem": "A multi-class classifier produces, for each input $x$, a probability vector $\\hat{\\mathbf{p}}(x)$ over classes $\\{A,B,C\\}$. Consider a fixed validation set of $n=4$ samples with true labels and the modelâ€™s predicted probabilities at epoch $t=1$ and epoch $t=2$ given below. For each sample $i$, the ground-truth class is denoted by $y_i \\in \\{A,B,C\\}$, and the model outputs a probability vector $(\\hat{p}_A,\\hat{p}_B,\\hat{p}_C)$.\n\n- Sample $1$: $y_1=A$\n  - Epoch $1$: $(0.55,\\,0.40,\\,0.05)$\n  - Epoch $2$: $(0.70,\\,0.25,\\,0.05)$\n- Sample $2$: $y_2=B$\n  - Epoch $1$: $(0.60,\\,0.35,\\,0.05)$\n  - Epoch $2$: $(0.55,\\,0.40,\\,0.05)$\n- Sample $3$: $y_3=C$\n  - Epoch $1$: $(0.24,\\,0.24,\\,0.52)$\n  - Epoch $2$: $(0.20,\\,0.20,\\,0.60)$\n- Sample $4$: $y_4=B$\n  - Epoch $1$: $(0.49,\\,0.51,\\,0.00)$\n  - Epoch $2$: $(0.49,\\,0.51,\\,0.00)$\n\nUse only the following fundamental definitions.\n\n- The validation accuracy at epoch $t$ is the fraction $\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\{\\arg\\max_{c\\in\\{A,B,C\\}}\\hat{p}_c^{(t)}(x_i)=y_i\\}$.\n- The average negative log-likelihood (NLL), equivalently the average cross-entropy (CE) for one-hot labels, at epoch $t$ is $\\frac{1}{n}\\sum_{i=1}^{n}\\big(-\\log \\hat{p}^{(t)}_{y_i}(x_i)\\big)$, where $\\log$ is the natural logarithm.\n- Define the exponential moving average (EMA) of the NLL by $\\text{EMA}_t=\\alpha L_t+(1-\\alpha)\\text{EMA}_{t-1}$, where $L_t$ is the average NLL at epoch $t$, $\\alpha\\in(0,1]$, and initialize $\\text{EMA}_1=L_1$. Take $\\alpha=0.5$.\n\nBased on these definitions and the data above, which of the following statements are true?\n\nA. From epoch $1$ to epoch $2$, the validation accuracy is unchanged, while the average negative log-likelihood strictly decreases. Therefore, an early-stopping rule that halts when accuracy plateaus for $1$ epoch would be premature compared to a rule that monitors a moving-average negative log-likelihood.\n\nB. From epoch $1$ to epoch $2$, both the validation accuracy and the average negative log-likelihood are unchanged; hence either metric yields the same stopping decision at epoch $2$.\n\nC. From epoch $1$ to epoch $2$, the validation accuracy increases, and the average negative log-likelihood increases.\n\nD. The $2$-epoch exponential moving average with smoothing parameter $\\alpha=0.5$ of the negative log-likelihood decreases from epoch $1$ to epoch $2$.", "solution": "The user has requested an analysis of a multi-class classification problem.\n\n### Problem Validation\n\nFirst, I will validate the problem statement.\n\n**Step 1: Extract Givens**\n\n-   Number of samples, $n=4$.\n-   Classes: $\\{A,B,C\\}$.\n-   Data for epoch $t=1$ and epoch $t=2$.\n-   Sample $1$: $y_1=A$.\n    -   Epoch $1$ predictions $\\hat{\\mathbf{p}}^{(1)}(x_1) = (0.55, 0.40, 0.05)$.\n    -   Epoch $2$ predictions $\\hat{\\mathbf{p}}^{(2)}(x_1) = (0.70, 0.25, 0.05)$.\n-   Sample $2$: $y_2=B$.\n    -   Epoch $1$ predictions $\\hat{\\mathbf{p}}^{(1)}(x_2) = (0.60, 0.35, 0.05)$.\n    -   Epoch $2$ predictions $\\hat{\\mathbf{p}}^{(2)}(x_2) = (0.55, 0.40, 0.05)$.\n-   Sample $3$: $y_3=C$.\n    -   Epoch $1$ predictions $\\hat{\\mathbf{p}}^{(1)}(x_3) = (0.24, 0.24, 0.52)$.\n    -   Epoch $2$ predictions $\\hat{\\mathbf{p}}^{(2)}(x_3) = (0.20, 0.20, 0.60)$.\n-   Sample $4$: $y_4=B$.\n    -   Epoch $1$ predictions $\\hat{\\mathbf{p}}^{(1)}(x_4) = (0.49, 0.51, 0.00)$.\n    -   Epoch $2$ predictions $\\hat{\\mathbf{p}}^{(2)}(x_4) = (0.49, 0.51, 0.00)$.\n-   Definition of validation accuracy at epoch $t$: $\\frac{1}{n}\\sum_{i=1}^{n}\\mathbf{1}\\{\\arg\\max_{c\\in\\{A,B,C\\}}\\hat{p}_c^{(t)}(x_i)=y_i\\}$. $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n-   Definition of average negative log-likelihood (NLL) at epoch $t$: $L_t = \\frac{1}{n}\\sum_{i=1}^{n}\\big(-\\log \\hat{p}^{(t)}_{y_i}(x_i)\\big)$, where $\\log$ is the natural logarithm.\n-   Definition of exponential moving average (EMA) of NLL: $\\text{EMA}_t=\\alpha L_t+(1-\\alpha)\\text{EMA}_{t-1}$.\n-   EMA parameter: $\\alpha=0.5$.\n-   EMA initialization: $\\text{EMA}_1=L_1$.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientific Grounding**: The problem is grounded in fundamental concepts of machine learning evaluation metrics: accuracy, cross-entropy (or negative log-likelihood for one-hot labels), and exponential moving average for smoothing time-series data like epoch-wise loss. The provided probability vectors sum to $1$ for each sample and epoch, and all values are in the range $[0, 1]$. The scenario is a standard representation of model training progress.\n-   **Well-Posed**: All definitions and data required to compute the specified metrics are provided. The questions posed in the options are answerable by direct calculation from the givens.\n-   **Objective**: The problem is stated using precise mathematical definitions and objective numerical data. There is no subjective language.\n\nThe problem statement is internally consistent, scientifically sound, and well-posed. No invalidating flaws are present.\n\n**Step 3: Verdict and Action**\n\nThe problem is **valid**. I will proceed with the solution derivation.\n\n### Solution Derivation\n\nThe solution requires calculating and comparing the validation accuracy and average negative log-likelihood (NLL) for epochs $t=1$ and $t=2$.\n\n**1. Calculation of Validation Accuracy**\n\nThe validation accuracy is the fraction of correctly classified samples. The predicted class is the one with the highest probability.\n\n**Epoch $t=1$ Accuracy ($Acc_1$)**:\n-   Sample $1$ ($y_1=A$): $\\hat{\\mathbf{p}}^{(1)}=(0.55, 0.40, 0.05)$. $\\arg\\max$ is $A$. Correct.\n-   Sample $2$ ($y_2=B$): $\\hat{\\mathbf{p}}^{(1)}=(0.60, 0.35, 0.05)$. $\\arg\\max$ is $A$. Incorrect.\n-   Sample $3$ ($y_3=C$): $\\hat{\\mathbf{p}}^{(1)}=(0.24, 0.24, 0.52)$. $\\arg\\max$ is $C$. Correct.\n-   Sample $4$ ($y_4=B$): $\\hat{\\mathbf{p}}^{(1)}=(0.49, 0.51, 0.00)$. $\\arg\\max$ is $B$. Correct.\n\nThere are $3$ correct predictions out of $4$ samples.\n$$Acc_1 = \\frac{3}{4} = 0.75$$\n\n**Epoch $t=2$ Accuracy ($Acc_2$)**:\n-   Sample $1$ ($y_1=A$): $\\hat{\\mathbf{p}}^{(2)}=(0.70, 0.25, 0.05)$. $\\arg\\max$ is $A$. Correct.\n-   Sample $2$ ($y_2=B$): $\\hat{\\mathbf{p}}^{(2)}=(0.55, 0.40, 0.05)$. $\\arg\\max$ is $A$. Incorrect.\n-   Sample $3$ ($y_3=C$): $\\hat{\\mathbf{p}}^{(2)}=(0.20, 0.20, 0.60)$. $\\arg\\max$ is $C$. Correct.\n-   Sample $4$ ($y_4=B$): $\\hat{\\mathbf{p}}^{(2)}=(0.49, 0.51, 0.00)$. $\\arg\\max$ is $B$. Correct.\n\nThere are $3$ correct predictions out of $4$ samples.\n$$Acc_2 = \\frac{3}{4} = 0.75$$\n\nTherefore, the validation accuracy is unchanged from epoch $1$ to epoch $2$: $Acc_1 = Acc_2$.\n\n**2. Calculation of Average Negative Log-Likelihood (NLL)**\n\nThe average NLL, $L_t$, is calculated using the predicted probability of the true class for each sample.\n\n**Epoch $t=1$ Average NLL ($L_1$)**:\nThe probabilities for the true classes are $\\hat{p}^{(1)}_{y_1}=0.55$, $\\hat{p}^{(1)}_{y_2}=0.35$, $\\hat{p}^{(1)}_{y_3}=0.52$, and $\\hat{p}^{(1)}_{y_4}=0.51$.\n$$L_1 = \\frac{1}{4} \\left[ -\\log(0.55) - \\log(0.35) - \\log(0.52) - \\log(0.51) \\right]$$\nNumerically, $L_1 \\approx \\frac{1}{4} [ -(-0.5978) - (-1.0498) - (-0.6539) - (-0.6733) ] = \\frac{1}{4} [2.9748] \\approx 0.7437$.\n\n**Epoch $t=2$ Average NLL ($L_2$)**:\nThe probabilities for the true classes are $\\hat{p}^{(2)}_{y_1}=0.70$, $\\hat{p}^{(2)}_{y_2}=0.40$, $\\hat{p}^{(2)}_{y_3}=0.60$, and $\\hat{p}^{(2)}_{y_4}=0.51$.\n$$L_2 = \\frac{1}{4} \\left[ -\\log(0.70) - \\log(0.40) - \\log(0.60) - \\log(0.51) \\right]$$\nNumerically, $L_2 \\approx \\frac{1}{4} [ -(-0.3567) - (-0.9163) - (-0.5108) - (-0.6733) ] = \\frac{1}{4} [2.4571] \\approx 0.6143$.\n\n**Comparing $L_1$ and $L_2$**:\nTo determine if the NLL has decreased, we can compare $L_1$ and $L_2$. A decrease means $L_2 < L_1$. We can establish this without resorting to numerical approximation.\nThe difference $L_1 - L_2$ is:\n$$L_1 - L_2 = \\frac{1}{4} \\left( [-\\log(0.55)] - [-\\log(0.70)] + [-\\log(0.35)] - [-\\log(0.40)] + [-\\log(0.52)] - [-\\log(0.60)] + [-\\log(0.51)] - [-\\log(0.51)] \\right)$$\n$$L_1 - L_2 = \\frac{1}{4} \\left( [\\log(0.70) - \\log(0.55)] + [\\log(0.40) - \\log(0.35)] + [\\log(0.60) - \\log(0.52)] \\right)$$\nSince the natural logarithm function $\\log(x)$ is strictly increasing:\n-   $0.70 > 0.55 \\implies \\log(0.70) > \\log(0.55) \\implies \\log(0.70) - \\log(0.55) > 0$.\n-   $0.40 > 0.35 \\implies \\log(0.40) > \\log(0.35) \\implies \\log(0.40) - \\log(0.35) > 0$.\n-   $0.60 > 0.52 \\implies \\log(0.60) > \\log(0.52) \\implies \\log(0.60) - \\log(0.52) > 0$.\n\nThe sum of three strictly positive terms is strictly positive. Therefore, $L_1 - L_2 > 0$, which implies $L_1 > L_2$. The average negative log-likelihood has strictly decreased.\n\n**3. Calculation of Exponential Moving Average (EMA) of NLL**\n\nThe EMA is defined by $\\text{EMA}_t=\\alpha L_t+(1-\\alpha)\\text{EMA}_{t-1}$ with $\\alpha=0.5$ and the initialization $\\text{EMA}_1=L_1$.\n\n-   At epoch $t=1$: $\\text{EMA}_1 = L_1$.\n-   At epoch $t=2$: $\\text{EMA}_2 = \\alpha L_2 + (1-\\alpha) \\text{EMA}_1 = 0.5 L_2 + 0.5 L_1$.\n\nWe need to compare $\\text{EMA}_1$ and $\\text{EMA}_2$. The change is $\\text{EMA}_2 - \\text{EMA}_1$.\n$$\\text{EMA}_2 - \\text{EMA}_1 = (0.5 L_2 + 0.5 L_1) - L_1 = 0.5 L_2 - 0.5 L_1 = 0.5 (L_2 - L_1)$$\nSince we established that $L_2 < L_1$, the term $(L_2 - L_1)$ is negative.\nTherefore, $\\text{EMA}_2 - \\text{EMA}_1 < 0$, which means $\\text{EMA}_2 < \\text{EMA}_1$. The EMA of the NLL decreases from epoch $1$ to epoch $2$.\n\n### Option-by-Option Analysis\n\n**A. From epoch $1$ to epoch $2$, the validation accuracy is unchanged, while the average negative log-likelihood strictly decreases. Therefore, an early-stopping rule that halts when accuracy plateaus for $1$ epoch would be premature compared to a rule that monitors a moving-average negative log-likelihood.**\n\n-   \"validation accuracy is unchanged\": This is true. We found $Acc_1 = Acc_2 = 0.75$.\n-   \"average negative log-likelihood strictly decreases\": This is true. We proved $L_2 < L_1$.\n-   \"Therefore, an early-stopping rule that halts when accuracy plateaus... would be premature...\": Accuracy has plateaued ($Acc_2-Acc_1=0$), which could trigger a stop. However, the NLL is improving ($L_2<L_1$), as is its EMA ($\\text{EMA}_2 < \\text{EMA}_1$). A rule monitoring the NLL or its EMA would register this improvement and not stop. The conclusion that stopping based on accuracy would be premature from the viewpoint of the loss metric is a correct interpretation of the results. This statement accurately describes a key reason why loss-based metrics can be preferred over accuracy for early stopping.\n\nVerdict for A: **Correct**.\n\n**B. From epoch $1$ to epoch $2$, both the validation accuracy and the average negative log-likelihood are unchanged; hence either metric yields the same stopping decision at epoch $2$.**\n\n-   \"average negative log-likelihood are unchanged\": This is false. The NLL strictly decreased. The entire statement is therefore false.\n\nVerdict for B: **Incorrect**.\n\n**C. From epoch $1$ to epoch $2$, the validation accuracy increases, and the average negative log-likelihood increases.**\n\n-   \"validation accuracy increases\": This is false. It was unchanged.\n-   \"average negative log-likelihood increases\": This is false. It strictly decreased.\n\nVerdict for C: **Incorrect**.\n\n**D. The $2$-epoch exponential moving average with smoothing parameter $\\alpha=0.5$ of the negative log-likelihood decreases from epoch $1$ to epoch $2$.**\n\n-   This statement claims that $\\text{EMA}_2 < \\text{EMA}_1$.\n-   As derived in section 3, $\\text{EMA}_2 - \\text{EMA}_1 = 0.5(L_2 - L_1)$.\n-   Since $L_2 < L_1$, the difference is negative. Thus, the EMA of the NLL does indeed decrease. The statement is a direct consequence of the calculations.\n\nVerdict for D: **Correct**.\n\nBoth statements A and D are true based on the provided data and definitions.", "answer": "$$\\boxed{AD}$$", "id": "3110736"}, {"introduction": "A common point of confusion is the relationship between optimizing cross-entropy and improving other popular evaluation metrics like the Receiver Operating Characteristic Area Under the Curve (ROC-AUC). While both are used to assess classifier performance, they measure fundamentally different qualities of the model's predictions. This exercise uses carefully constructed scenarios to demonstrate that minimizing cross-entropy does not necessarily maximize ROC-AUC; in fact, the two can even move in opposite directions [@problem_id:3110742]. Working through these examples will clarify the distinct roles of calibration-sensitive losses (like cross-entropy) and ranking-based metrics (like ROC-AUC) in model evaluation.", "problem": "Consider binary classification with labels $y_i \\in \\{0,1\\}$ and predicted probabilities $s_i \\in (0,1)$. The Negative Log-Likelihood (NLL) of independent Bernoulli observations with predictions $s_i$ is defined as $-\\sum_{i} \\left[ y_i \\log s_i + (1-y_i)\\log(1-s_i) \\right]$, which is equal to the empirical Cross-Entropy (CE). The Receiver Operating Characteristic (ROC) Area Under the Curve (AUC) is defined as the probability that a randomly chosen positive example is scored higher than a randomly chosen negative example, which in the empirical finite-sample setting is the fraction of positive-negative pairs for which $s_{\\text{pos}} > s_{\\text{neg}}$, with ties counted as half.\n\nYou will analyze two concrete scenarios to reason from these definitions.\n\nScenario $1$ (ranking unchanged, NLL decreases). Let the labels be $y = [1,1,0,0,0]$ and two sets of predictions be\n$s^{\\text{old}} = [0.80, 0.70, 0.40, 0.30, 0.20]$ and $s^{\\text{new1}} = [0.95, 0.85, 0.20, 0.15, 0.10]$.\nNote that in both $s^{\\text{old}}$ and $s^{\\text{new1}}$, the two positive examples have probabilities larger than all three negative examples, so the relative ranking of positive versus negative scores is preserved.\n\nScenario $2$ (ranking worsens, NLL decreases). Let the labels be $y = [1,1,1,0,0,0]$ and two sets of predictions be\n$s^{\\text{old2}} = [0.60, 0.70, 0.80, 0.40, 0.30, 0.20]$ and $s^{\\text{new2}} = [0.95, 0.70, 0.99, 0.05, 0.75, 0.01]$.\nObserve that in $s^{\\text{new2}}$ one negative example with score $0.75$ exceeds one positive example with score $0.70$, creating at least one positive-negative inversion, while the other examples move closer to their labels.\n\nUsing only the stated definitions and these data, determine which statements are true:\n\nA. Moving from $s^{\\text{old}}$ to $s^{\\text{new1}}$ reduces the NLL and leaves the ROC-AUC unchanged because the ranking of $s_i$ between positive and negative examples is preserved.\n\nB. Moving from $s^{\\text{old2}}$ to $s^{\\text{new2}}$ reduces the NLL but decreases the ROC-AUC because at least one positive-negative pair flips order, showing that optimizing CE can worsen a ranking-based metric.\n\nC. Cross-Entropy directly optimizes ROC-AUC because both depend only on the relative ordering of scores; therefore any improvement in CE necessarily increases ROC-AUC.\n\nD. Any strictly increasing transformation applied to all predicted probabilities leaves both NLL and ROC-AUC unchanged.\n\nSelect all correct options.", "solution": "The problem asks to analyze the relationship between Negative Log-Likelihood (NLL/Cross-Entropy) and ROC-AUC under two scenarios. The definitions for NLL and ROC-AUC are standard, and the problem is well-posed.\n\n### Option-by-Option Analysis\n\n**A. Moving from $s^{\\text{old}}$ to $s^{\\text{new1}}$ reduces the NLL and leaves the ROC-AUC unchanged because the ranking of $s_i$ between positive and negative examples is preserved.**\n\n- **ROC-AUC Analysis**:\n  - Labels: $y = [1,1,0,0,0]$. Positive examples are at indices 1 and 2. Negative examples are at indices 3, 4, and 5.\n  - In $s^{\\text{old}} = [0.80, 0.70, 0.40, 0.30, 0.20]$, the scores for positive examples (0.80, 0.70) are all higher than the scores for negative examples (0.40, 0.30, 0.20). Every positive-negative pair is correctly ranked. Therefore, the ROC-AUC is $1.0$.\n  - In $s^{\\text{new1}} = [0.95, 0.85, 0.20, 0.15, 0.10]$, the scores for positive examples (0.95, 0.85) are also all higher than the scores for negative examples (0.20, 0.15, 0.10). The ranking is perfect. The ROC-AUC is also $1.0$.\n  - The ROC-AUC is indeed unchanged.\n\n- **NLL Analysis**:\n  - The NLL is $-\\sum_{i} \\left[ y_i \\log s_i + (1-y_i)\\log(1-s_i) \\right]$. A lower NLL is better. We need to check if $NLL(s^{\\text{new1}})  NLL(s^{\\text{old}})$.\n  - $NLL(s^{\\text{old}}) = -[\\log(0.8) + \\log(0.7) + \\log(1-0.4) + \\log(1-0.3) + \\log(1-0.2)] = -[\\log(0.8) + \\log(0.7) + \\log(0.6) + \\log(0.7) + \\log(0.8)] \\approx 2.408$.\n  - $NLL(s^{\\text{new1}}) = -[\\log(0.95) + \\log(0.85) + \\log(1-0.2) + \\log(1-0.15) + \\log(1-0.1)] = -[\\log(0.95) + \\log(0.85) + \\log(0.8) + \\log(0.85) + \\log(0.9)] \\approx 0.887$.\n  - The NLL has strictly decreased. Each probability has moved closer to its true label (1 for positives, 0 for negatives), which reduces its contribution to the NLL.\n  - The statement is **True**.\n\n**B. Moving from $s^{\\text{old2}}$ to $s^{\\text{new2}}$ reduces the NLL but decreases the ROC-AUC because at least one positive-negative pair flips order, showing that optimizing CE can worsen a ranking-based metric.**\n\n- **ROC-AUC Analysis**:\n  - Labels: $y = [1,1,1,0,0,0]$. There are $3 \\times 3 = 9$ positive-negative pairs.\n  - In $s^{\\text{old2}} = [0.60, 0.70, 0.80, 0.40, 0.30, 0.20]$, all positive scores are greater than all negative scores. The ROC-AUC is $1.0$.\n  - In $s^{\\text{new2}} = [0.95, 0.70, 0.99, 0.05, 0.75, 0.01]$, the positive scores are $\\{0.95, 0.70, 0.99\\}$ and negative scores are $\\{0.05, 0.75, 0.01\\}$.\n  - Let's count correctly ranked pairs:\n    - Positive score 0.95 vs negatives (0.05, 0.75, 0.01): 3/3 correct.\n    - Positive score 0.70 vs negatives (0.05, 0.75, 0.01): 2/3 correct (fails for 0.75, as $0.70 \\ngtr 0.75$).\n    - Positive score 0.99 vs negatives (0.05, 0.75, 0.01): 3/3 correct.\n  - Total correctly ranked pairs = $3+2+3 = 8$. The ROC-AUC is $8/9 \\approx 0.889$.\n  - The ROC-AUC has decreased from $1.0$ to $8/9$.\n\n- **NLL Analysis**:\n  - $NLL(s^{\\text{old2}}) = -[\\log(0.6)+\\log(0.7)+\\log(0.8) + \\log(0.4)+\\log(0.3)+\\log(0.2)] \\approx 3.296$.\n  - $NLL(s^{\\text{new2}}) = -[\\log(0.95)+\\log(0.70)+\\log(0.99) + \\log(1-0.05)+\\log(1-0.75)+\\log(1-0.01)] = -[\\log(0.95)+\\log(0.70)+\\log(0.99)+\\log(0.95)+\\log(0.25)+\\log(0.99)] \\approx 1.854$.\n  - The NLL has decreased. Although one sample (the negative example with score 0.75) has a much worse loss contribution ($-\\log(0.25)$ is large), the very confident correct predictions (0.95, 0.99 for positives; 0.05, 0.01 for negatives) drastically reduce the total NLL.\n  - This demonstrates that improving the calibration of most examples (and thus the NLL) can outweigh a change that worsens the ranking.\n  - The statement is **True**.\n\n**C. Cross-Entropy directly optimizes ROC-AUC because both depend only on the relative ordering of scores; therefore any improvement in CE necessarily increases ROC-AUC.**\n\n- This statement makes two claims. First, that both CE and ROC-AUC depend only on the relative ordering of scores. This is false. NLL (CE) depends on the actual predicted probability values, not just their order. As shown in Scenario 1, the NLL changed while the ordering of positive vs. negative samples (and thus AUC) did not.\n- The second claim is that improving CE increases ROC-AUC. Scenario 2 provides a direct counterexample where NLL improved (decreased) while ROC-AUC worsened (decreased).\n- The statement is **False**.\n\n**D. Any strictly increasing transformation applied to all predicted probabilities leaves both NLL and ROC-AUC unchanged.**\n\n- **ROC-AUC**: The ROC-AUC depends only on the rank ordering of the scores. If $f$ is a strictly increasing function, then $s_i > s_j \\iff f(s_i) > f(s_j)$. Therefore, the ranking is preserved, and the ROC-AUC is unchanged.\n- **NLL**: The NLL is a function of the score values themselves, e.g., $-\\log(s_i)$ for a positive example. If we apply a transformation $f$, the new NLL term would be $-\\log(f(s_i))$. In general, $-\\log(f(s_i)) \\neq -\\log(s_i)$. For example, let $s_i = 0.8$ and $f(s) = s^2$. The new score is $0.64$. $-\\log(0.8) \\neq -\\log(0.64)$.\n- Since NLL is not unchanged, the statement is **False**.\n\nBased on this analysis, options A and B are correct.", "answer": "$$\\boxed{AB}$$", "id": "3110742"}]}