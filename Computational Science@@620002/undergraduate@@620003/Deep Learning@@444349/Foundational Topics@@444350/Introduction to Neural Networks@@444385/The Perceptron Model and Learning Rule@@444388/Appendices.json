{"hands_on_practices": [{"introduction": "Before training a model, it is crucial to understand what it is capable of representing. This foundational exercise delves into the concept of linear separability, the perceptron's essential requirement for success. You will analyze why a perceptron can easily solve certain logical functions but famously fails on others like the Exclusive OR (XOR) problem, and then discover how a simple, non-linear transformation of the input features can overcome this fundamental limitation [@problem_id:3190716].", "problem": "Consider a binary classification task with a perceptron model defined by the decision function $\\hat{y} = \\operatorname{sign}(w^{\\top} x + b)$, where $x \\in \\{0,1\\}^{d}$, $w \\in \\mathbb{R}^{d}$, and $b \\in \\mathbb{R}$. In all parts below, the class labels are $y \\in \\{-1,+1\\}$, with the positive class labeled $+1$.\n\nPart A (majority of three): Let $d = 3$ and define the target as the majority-vote logic: $y = +1$ if $x_1 + x_2 + x_3 \\geq 2$ and $y = -1$ otherwise. Assume the perceptron has equal positive weights, $w_1 = w_2 = w_3 = w$ with $w > 0$, and bias $b \\in \\mathbb{R}$. Among all separating hyperplanes satisfying correct classification for all $x \\in \\{0,1\\}^{3}$, determine the value of the ratio $b/w$ that maximizes the minimum signed distance, measured along the perceptron’s normal direction, between the decision boundary and any training point.\n\nPart B (failure on Exclusive OR (XOR)): Consider $d = 2$ with the Exclusive OR (XOR) target defined by $y = +1$ if $x_1 + x_2 = 1$ and $y = -1$ if $x_1 + x_2 \\in \\{0,2\\}$. From first principles (without appealing to pre-stated theorems), explain why no choice of $(w,b)$ yields a separating hyperplane in the original input space.\n\nPart C (restoring separability via pairwise interaction features): Augment the $d=2$ XOR input by adding the pairwise interaction to form the feature map $\\phi(x_1,x_2) = (x_1, x_2, x_1 x_2) \\in \\mathbb{R}^{3}$. Explain how an appropriate choice of weights and bias in the augmented space can correctly separate the XOR data.\n\nYour final answer should be the exact value of $b/w$ from Part A, expressed as a single reduced fraction. Do not include any units. No rounding is required.", "solution": "### Part A: Majority-of-Three Function\n\nThe task is to find the ratio $b/w$ that maximizes the minimum geometric margin for a perceptron classifying the majority-of-three function.\n\nThe input space is $x \\in \\{0,1\\}^3$. There are $2^3=8$ possible input vectors. The target function is $y = +1$ if the sum of the inputs $S = x_1+x_2+x_3$ is at least $2$, and $y=-1$ otherwise.\nThe data points and their corresponding labels $y$ and sums $S$ are:\n- $S=0$: $x=(0,0,0)$, $y=-1$\n- $S=1$: $x \\in \\{(1,0,0), (0,1,0), (0,0,1)\\}$, $y=-1$\n- $S=2$: $x \\in \\{(1,1,0), (1,0,1), (0,1,1)\\}$, $y=+1$\n- $S=3$: $x=(1,1,1)$, $y=+1$\n\nThe perceptron model has a decision boundary defined by $w^{\\top}x+b=0$. We are given that the weights are equal and positive, $w_1=w_2=w_3=w > 0$. The activation function is therefore:\n$$w^{\\top}x+b = w_1x_1+w_2x_2+w_3x_3+b = w(x_1+x_2+x_3)+b = wS+b$$\nFor a point $x$ with label $y$ to be correctly classified, we must have $y(wS+b)>0$. Let's establish the conditions for the separability of all $8$ points.\n- For $S=0, y=-1$: $(-1)(w \\cdot 0 + b) > 0 \\implies -b > 0 \\implies b < 0$.\n- For $S=1, y=-1$: $(-1)(w \\cdot 1 + b) > 0 \\implies -w-b > 0 \\implies w+b < 0$.\n- For $S=2, y=+1$: $(+1)(w \\cdot 2 + b) > 0 \\implies 2w+b > 0$.\n- For $S=3, y=+1$: $(+1)(w \\cdot 3 + b) > 0 \\implies 3w+b > 0$.\n\nFrom $w+b<0$, we get $b < -w$. From $2w+b>0$, we get $b > -2w$. Combining these, we find that any separating hyperplane of this form must satisfy $-2w < b < -w$. Since $w>0$, this implies $b<0$, consistent with our first inequality. It also implies $3w+b > 3w-2w = w > 0$, satisfying the fourth inequality. Thus, the condition for separability is $-2 < b/w < -1$.\n\nThe signed geometric distance (or margin) of a point $x_i$ from the decision boundary is given by $\\gamma_i = \\frac{y_i(w^{\\top}x_i+b)}{\\|w\\|}$. We aim to maximize the minimum such distance, $\\gamma_{\\min} = \\min_{i} \\gamma_i$.\nThe norm of the weight vector is $\\|w\\| = \\sqrt{w_1^2+w_2^2+w_3^2} = \\sqrt{w^2+w^2+w^2} = \\sqrt{3w^2} = w\\sqrt{3}$ (since $w>0$).\n\nThe margin for each group of points, distinguished by their sum $S$, is:\n- $S=0, y=-1$: $\\gamma_0 = \\frac{(-1)(w \\cdot 0 + b)}{w\\sqrt{3}} = \\frac{-b}{w\\sqrt{3}}$\n- $S=1, y=-1$: $\\gamma_1 = \\frac{(-1)(w \\cdot 1 + b)}{w\\sqrt{3}} = \\frac{-w-b}{w\\sqrt{3}}$\n- $S=2, y=+1$: $\\gamma_2 = \\frac{(+1)(w \\cdot 2 + b)}{w\\sqrt{3}} = \\frac{2w+b}{w\\sqrt{3}}$\n- $S=3, y=+1$: $\\gamma_3 = \\frac{(+1)(w \\cdot 3 + b)}{w\\sqrt{3}} = \\frac{3w+b}{w\\sqrt{3}}$\n\nThe overall margin is $\\gamma_{\\min} = \\min(\\gamma_0, \\gamma_1, \\gamma_2, \\gamma_3)$. The points that constrain the margin (the support vectors) are those closest to the decision boundary. Within the region of separability ($-2w < b < -w$), the smallest positive values of $y(wS+b)$ will come from $S=1$ and $S=2$. Let's verify this. Let $f(S) = y(wS+b)$.\n- $f(0)=-b$. $f(1)=-w-b$. Since $w>0$, $-b > -w-b$. Thus $\\gamma_0 > \\gamma_1$.\n- $f(2)=2w+b$. $f(3)=3w+b$. Since $w>0$, $2w+b < 3w+b$. Thus $\\gamma_2 < \\gamma_3$.\nThe minimum margin is therefore $\\gamma_{\\min} = \\min(\\gamma_1, \\gamma_2)$. To maximize this minimum, we must set the two arguments to be equal. This occurs for the maximal margin hyperplane.\n$$\\gamma_1 = \\gamma_2$$\n$$\\frac{-w-b}{w\\sqrt{3}} = \\frac{2w+b}{w\\sqrt{3}}$$\n$$-w-b = 2w+b$$\n$$-3w = 2b$$\n$$\\frac{b}{w} = -\\frac{3}{2}$$\nThis value lies in the interval $(-2, -1)$, confirming it corresponds to a valid separating hyperplane. The ratio $b/w$ that maximizes the minimum distance is $-3/2$.\n\n### Part B: Non-separability of XOR\n\nThe Exclusive OR (XOR) function for $d=2$ is defined by the following input-output pairs:\n- $x^{(1)}=(0,0)$, $y^{(1)}=-1$\n- $x^{(2)}=(0,1)$, $y^{(2)}=+1$\n- $x^{(3)}=(1,0)$, $y^{(3)}=+1$\n- $x^{(4)}=(1,1)$, $y^{(4)}=-1$\n\nA linear separator is a hyperplane defined by $w_1 x_1 + w_2 x_2 + b = 0$. For the data to be linearly separable, there must exist parameters $(w_1, w_2, b)$ such that $y^{(i)}(w_1 x_1^{(i)} + w_2 x_2^{(i)} + b) > 0$ for all $i \\in \\{1,2,3,4\\}$. This gives rise to a system of four linear inequalities:\n1. For $(0,0), y=-1$: $(-1)(w_1 \\cdot 0 + w_2 \\cdot 0 + b) > 0 \\implies -b > 0 \\implies b < 0$.\n2. For $(0,1), y=+1$: $(+1)(w_1 \\cdot 0 + w_2 \\cdot 1 + b) > 0 \\implies w_2 + b > 0$.\n3. For $(1,0), y=+1$: $(+1)(w_1 \\cdot 1 + w_2 \\cdot 0 + b) > 0 \\implies w_1 + b > 0$.\n4. For $(1,1), y=-1$: $(-1)(w_1 \\cdot 1 + w_2 \\cdot 1 + b) > 0 \\implies w_1 + w_2 + b < 0$.\n\nLet's demonstrate that this system has no solution.\nFrom inequality (2), we have $w_2 > -b$.\nFrom inequality (3), we have $w_1 > -b$.\nAdding these two expressions gives:\n$$w_1 + w_2 > -2b$$\nFrom inequality (1), we know $b$ is negative. Let $c = -b$, where $c > 0$. The inequalities become:\n- $w_2 > c$\n- $w_1 > c$\n- $w_1 + w_2 > 2c$\nInequality (4) can be rewritten as $w_1 + w_2 < -b$, which is $w_1 + w_2 < c$.\nWe have derived two contradictory requirements: $w_1 + w_2 > 2c$ and $w_1 + w_2 < c$. Since $c$ is a positive value, a number cannot be simultaneously greater than $2c$ and less than $c$. This contradiction proves that no single linear hyperplane can satisfy all four conditions. Therefore, the XOR function is not linearly separable.\n\n### Part C: Restoring Separability with a Feature Map\n\nWe augment the input space by mapping $x=(x_1, x_2)$ to a new feature space using $\\phi(x_1,x_2) = (x_1, x_2, x_1 x_2)$. Let's call the new coordinates $z=(z_1, z_2, z_3)$. The data points are transformed as follows:\n- $x=(0,0) \\implies z^{(1)}=(0,0,0)$, $y^{(1)}=-1$\n- $x=(0,1) \\implies z^{(2)}=(0,1,0)$, $y^{(2)}=+1$\n- $x=(1,0) \\implies z^{(3)}=(1,0,0)$, $y^{(3)}=+1$\n- $x=(1,1) \\implies z^{(4)}=(1,1,1)$, $y^{(4)}=-1$\n\nWe now seek a separating hyperplane in this new 3D space, defined by $w'^{\\top}z + b' = w'_1 z_1 + w'_2 z_2 + w'_3 z_3 + b' = 0$. The separability conditions are:\n1. For $z=(0,0,0), y=-1$: $-b' > 0 \\implies b' < 0$.\n2. For $z=(0,1,0), y=+1$: $w'_2 + b' > 0$.\n3. For $z=(1,0,0), y=+1$: $w'_1 + b' > 0$.\n4. For $z=(1,1,1), y=-1$: $-(w'_1 + w'_2 + w'_3 + b') > 0 \\implies w'_1 + w'_2 + w'_3 + b' < 0$.\n\nUnlike the original problem, this system of inequalities has solutions. Let's construct one.\nChoose $w'_1 = 1$ and $w'_2 = 1$.\nFrom conditions (2) and (3), we need $1+b' > 0$, so $b' > -1$.\nCondition (1) requires $b' < 0$. Thus we must have $-1 < b' < 0$. Let's select $b' = -1/2$.\nNow, substitute these into condition (4):\n$1 + 1 + w'_3 - \\frac{1}{2} < 0 \\implies \\frac{3}{2} + w'_3 < 0 \\implies w'_3 < -\\frac{3}{2}$.\nLet's choose $w'_3 = -2$.\n\nA valid separator is given by the parameters $w'=(1, 1, -2)$ and $b'=-1/2$. Let's verify:\n- $z^{(1)}=(0,0,0), y=-1$: $1(0)+1(0)-2(0) - 1/2 = -1/2$. $\\operatorname{sign}(-1/2)=-1$. Correct.\n- $z^{(2)}=(0,1,0), y=+1$: $1(0)+1(1)-2(0) - 1/2 = 1/2$. $\\operatorname{sign}(1/2)=+1$. Correct.\n- $z^{(3)}=(1,0,0), y=+1$: $1(1)+1(0)-2(0) - 1/2 = 1/2$. $\\operatorname{sign}(1/2)=+1$. Correct.\n- $z^{(4)}=(1,1,1), y=-1$: $1(1)+1(1)-2(1) - 1/2 = -1/2$. $\\operatorname{sign}(-1/2)=-1$. Correct.\n\nThe addition of the non-linear feature $x_1x_2$ maps the data into a higher-dimensional space where it becomes linearly separable. Geometrically, the four points in the original $x_1, x_2$ plane are not separable by a line. The feature map lifts one of the points, $(1,1)$, off the $z_3=0$ plane to $(1,1,1)$. The four points in $\\mathbb{R}^3$ are $(0,0,0)$, $(0,1,0)$, $(1,0,0)$, and $(1,1,1)$. A plane (e.g., $z_1+z_2-2z_3 - 1/2 = 0$) can now be placed to separate the positive class points $\\{(0,1,0), (1,0,0)\\}$ from the negative class points $\\{(0,0,0), (1,1,1)\\}$.\n\nThe final required answer is the specific result from Part A. As derived above, this is the ratio $b/w$.", "answer": "$$\\boxed{-\\frac{3}{2}}$$", "id": "3190716"}, {"introduction": "One of the most elegant results in early machine learning is the Perceptron Mistake Bound, which guarantees that the learning algorithm will converge on separable data in a finite number of steps. This practice provides a chance to witness this theorem in action by implementing the perceptron algorithm and comparing the empirical number of mistakes to the theoretical upper bound, which is a function of the data's geometry—specifically its radius $R$ and margin $\\gamma$. This exercise beautifully connects algorithm implementation with its underlying theory [@problem_id:3190718].", "problem": "You are asked to implement and empirically study the perceptron learning algorithm on a small set of synthetic, linearly separable datasets. The computational experiment must be grounded in fundamental definitions of the perceptron hypothesis class and the perceptron learning rule, and should not rely on any pre-derived performance formulas. Your program must compute the empirical number of mistakes made by the perceptron while training, and then compute a geometric quantity that depends on the dataset and a given reference separating vector. Finally, aggregate the results over multiple test cases and output them in a single, machine-checkable line.\n\nFundamental base and definitions to be used:\n- A binary classification dataset consists of inputs $x_i \\in \\mathbb{R}^d$ and labels $y_i \\in \\{-1, +1\\}$.\n- A linear classifier is specified by a weight vector $w \\in \\mathbb{R}^d$, which predicts $\\hat{y}_i = \\mathrm{sign}(w^\\top x_i)$, where $\\mathrm{sign}(z)$ returns $+1$ if $z > 0$, $-1$ if $z < 0$, and $0$ if $z = 0$.\n- The perceptron learning rule updates the current weight vector $w$ only on mistakes: when $y_i \\, w^\\top x_i \\le 0$, set $w \\leftarrow w + y_i x_i$.\n- For a dataset $D = \\{(x_i, y_i)\\}_{i=1}^n$, define the dataset radius $R$ by $R = \\max_i \\|x_i\\|_2$, where $\\|\\cdot\\|_2$ denotes the Euclidean norm.\n- Given any fixed reference separating vector $w^\\ast \\in \\mathbb{R}^d$ that correctly classifies all points, define the normalized geometric margin $\\gamma$ of the dataset with respect to $w^\\ast$ by $\\gamma = \\min_i \\dfrac{y_i \\, w^{\\ast\\top} x_i}{\\|w^\\ast\\|_2}$.\n\nYour tasks:\n1. For each dataset, initialize $w_0 = 0$ and run the perceptron learning rule in cyclic order over the examples until the algorithm completes a full pass with zero mistakes. To ensure termination in case of programming errors, cap the training at $T = 1000$ epochs; for the provided test cases, the algorithm will converge far before this cap.\n2. Count the total number of mistakes $M$ made during training (this equals the number of updates applied).\n3. Compute $R$ and $\\gamma$ as defined above, and then compute the quantity $B = \\left(\\dfrac{R}{\\gamma}\\right)^2$.\n4. For each dataset, produce the list $[M, B, M \\le B]$, where $M$ is an integer, $B$ is a floating-point number, and $M \\le B$ is a boolean value indicating whether the empirical mistakes do not exceed the computed quantity. Aggregate the lists from all test cases and print them as a single line in the exact format specified below.\n\nAngle units are not involved. There are no physical units in this problem.\n\nTest suite:\nProvide results for the following three datasets, each with its associated labels and a reference $w^\\ast$.\n\n- Test case $1$ (two-dimensional, well-separated):\n  - Inputs $X = [\\,(2, 2),\\; (2, 0),\\; (0, 2),\\; (-2, -1),\\; (-1, -2),\\; (-2, -2)\\,]$.\n  - Labels $Y = [\\,+1,\\; +1,\\; +1,\\; -1,\\; -1,\\; -1\\,]$.\n  - Reference vector $w^\\ast = (1, 1)$.\n\n- Test case $2$ (two-dimensional, small margin):\n  - Inputs $X = [\\,(1, 0.05),\\; (1, -0.05),\\; (1, 0.0),\\; (-1, 0.05),\\; (-1, -0.05),\\; (-1, 0.0)\\,]$.\n  - Labels $Y = [\\,+1,\\; +1,\\; +1,\\; -1,\\; -1,\\; -1\\,]$.\n  - Reference vector $w^\\ast = (1, 0.1)$.\n\n- Test case $3$ (two-dimensional, symmetry, boundary-like behavior):\n  - Inputs $X = [\\,(1, 0),\\; (0, 1),\\; (-1, 0),\\; (0, -1)\\,]$.\n  - Labels $Y = [\\,+1,\\; +1,\\; -1,\\; -1\\,]$.\n  - Reference vector $w^\\ast = (1, 1)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case must contribute a list $[M, B, M \\le B]$. For example, the final printed line should look like\n$[ [M_1,B_1,\\text{True}], [M_2,B_2,\\text{True}], [M_3,B_3,\\text{False}] ]$\nbut with the actual computed values, and with no spaces in the output, resulting in a string of the form\n$[[M_1,B_1,\\text{True}],[M_2,B_2,\\text{True}],[M_3,B_3,\\text{False}]]$.", "solution": "The problem requires an empirical investigation of the perceptron learning algorithm's performance on three synthetic, linearly separable datasets. The core of the task is to implement the algorithm, count the number of mistakes it makes during training, and compare this empirical count to a theoretical upper bound derived from the geometric properties of the dataset. This analysis is grounded in the well-known Perceptron Mistake Bound theorem.\n\nThe solution process for each test case involves the following steps:\n1.  **Simulate the Perceptron Algorithm**: The number of mistakes, $M$, is determined by running the perceptron learning algorithm.\n2.  **Compute Geometric Quantities**: The dataset radius, $R$, and the normalized geometric margin, $\\gamma$, are calculated based on the provided data and a reference separating vector, $w^\\ast$.\n3.  **Calculate the Bound**: The theoretical upper bound quantity, $B$, is computed as $B = (R/\\gamma)^2$.\n4.  **Verify the Inequality**: The empirical mistake count, $M$, is compared against the computed bound quantity, $B$, to verify the inequality $M \\le B$.\n\n**Step 1: Perceptron Algorithm Simulation**\nThe algorithm is initialized with a weight vector $w_0 = 0 \\in \\mathbb{R}^d$, where $d$ is the dimension of the input space. The algorithm then iterates through the training examples $(x_i, y_i)$ in a cyclic manner. An epoch is a complete pass through all examples in the dataset.\n\nFor each example $(x_i, y_i)$, a prediction is made. A mistake occurs if the current classifier $w$ misclassifies the point, which corresponds to the condition $y_i(w^\\top x_i) \\le 0$. The inclusion of the equality case, $w^\\top x_i = 0$, is critical, as a point lying on the decision boundary is not correctly classified to either class $+1$ or $-1$.\n\nUpon making a mistake, the weight vector is updated according to the perceptron learning rule:\n$$ w \\leftarrow w + y_i x_i $$\nThe total number of mistakes, $M$, is the cumulative count of these updates throughout the entire training process.\n\nTraining continues epoch by epoch until an entire epoch is completed with zero mistakes. At this point, the algorithm has found a weight vector $w$ that correctly classifies all training examples, and the algorithm terminates. The problem provides a safeguard of $T = 1000$ maximum epochs, though convergence is guaranteed for linearly separable data.\n\n**Step 2: Computation of Geometric Quantities**\nTwo key geometric properties of the dataset are required.\n\nFirst, the dataset radius, $R$, is defined as the maximum Euclidean norm of any input vector in the dataset:\n$$ R = \\max_{i} \\|x_i\\|_2 $$\nThis value represents the maximum distance of any data point from the origin.\n\nSecond, the normalized geometric margin, $\\gamma$, is defined with respect to a given, fixed separating vector $w^\\ast$ that correctly classifies all data points (i.e., $y_i (w^{\\ast\\top} x_i) > 0$ for all $i$). The margin $\\gamma$ is the minimum normalized signed distance from any point to the hyperplane defined by $w^\\ast$:\n$$ \\gamma = \\min_{i} \\frac{y_i (w^{\\ast\\top} x_i)}{\\|w^\\ast\\|_2} $$\nThe term $y_i(w^{\\ast\\top} x_i)$ is positive for all points, and dividing by $\\|w^\\ast\\|_2$ converts the \"functional margin\" into the true Euclidean distance, making $\\gamma$ the minimum geometric distance from a data point to the separating hyperplane.\n\n**Step 3: Calculating the Bound Quantity**\nThe Perceptron Mistake Bound theorem states that for any dataset that is linearly separable with a margin $\\gamma_{opt}$ (maximized over all separating hyperplanes), the number of mistakes $M$ made by the perceptron algorithm (starting from $w_0=0$) is bounded by $M \\le (R/\\gamma_{opt})^2$. The bound also holds for a margin $\\gamma$ defined with respect to *any* separating vector $w^\\ast$, not just the optimal one. The problem asks us to compute and verify this bound for the specific $w^\\ast$ provided for each test case. We therefore compute the quantity $B$:\n$$ B = \\left(\\frac{R}{\\gamma}\\right)^2 $$\n\n**Step 4: Assembling the Final Result**\nFor each of the three test cases, we perform the above calculations to obtain the integer mistake count $M$ and the floating-point bound $B$. We then form a list containing these two values and a boolean indicating whether the mistake bound holds: $[M, B, M \\le B]$. The final output is an aggregation of these lists for all test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the perceptron learning algorithm and computes the mistake bound quantity for three test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            # Test case 1 (two-dimensional, well-separated)\n            \"X\": np.array([[2., 2.], [2., 0.], [0., 2.], [-2., -1.], [-1., -2.], [-2., -2.]]),\n            \"Y\": np.array([1, 1, 1, -1, -1, -1]),\n            \"w_star\": np.array([1., 1.])\n        },\n        {\n            # Test case 2 (two-dimensional, small margin)\n            \"X\": np.array([[1., 0.05], [1., -0.05], [1., 0.0], [-1., 0.05], [-1., -0.05], [-1., 0.0]]),\n            \"Y\": np.array([1, 1, 1, -1, -1, -1]),\n            \"w_star\": np.array([1., 0.1])\n        },\n        {\n            # Test case 3 (two-dimensional, symmetry, boundary-like behavior)\n            \"X\": np.array([[1., 0.], [0., 1.], [-1., 0.], [0., -1.]]),\n            \"Y\": np.array([1, 1, -1, -1]),\n            \"w_star\": np.array([1., 1.])\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        X = case[\"X\"]\n        Y = case[\"Y\"]\n        w_star = case[\"w_star\"]\n        n_samples, n_features = X.shape\n\n        # Task 1  2: Run perceptron and count mistakes M\n        w = np.zeros(n_features)\n        M = 0\n        max_epochs = 1000\n\n        for _ in range(max_epochs):\n            mistakes_in_epoch = 0\n            for i in range(n_samples):\n                # Check for a mistake: y_i * (w^T * x_i) = 0\n                if Y[i] * np.dot(w, X[i]) = 0:\n                    # Apply the update rule: w - w + y_i * x_i\n                    w = w + Y[i] * X[i]\n                    M += 1\n                    mistakes_in_epoch += 1\n            \n            # If a full pass is completed with no mistakes, convergence is reached\n            if mistakes_in_epoch == 0:\n                break\n        \n        # Task 3: Compute R and gamma\n        # Compute R = max_i ||x_i||_2\n        norms = np.linalg.norm(X, axis=1)\n        R = np.max(norms)\n\n        # Compute gamma = min_i (y_i * w_star^T * x_i) / ||w_star||_2\n        numerator = Y * np.dot(X, w_star)\n        denominator = np.linalg.norm(w_star)\n        gamma = np.min(numerator) / denominator\n\n        # Compute B = (R / gamma)^2\n        B = (R / gamma)**2\n        \n        # Task 4: Aggregate the results\n        result_list = [M, B, M = B]\n        results.append(result_list)\n\n    # Final print statement in the exact required format with no spaces.\n    # Manually build the string to avoid spaces introduced by str(list).\n    output_str = \"[\"\n    for i, res in enumerate(results):\n        # res[2] is a boolean; str(res[2]) produces 'True' or 'False'.\n        output_str += f\"[{res[0]},{res[1]},{str(res[2])}]\"\n        if i  len(results) - 1:\n            output_str += \",\"\n    output_str += \"]\"\n    \n    print(output_str)\n\nsolve()\n```", "id": "3190718"}, {"introduction": "The mistake bound guarantee holds only for linearly separable data, a condition rarely met with real-world datasets. This practice explores what happens when this critical assumption is violated, demonstrating how the perceptron's updates can fail to converge and instead enter an endless cycle. You will implement two classic modifications, the pocket perceptron and the averaged perceptron, which are designed to produce a reasonable and stable classifier even in the face of non-separable data [@problem_id:3190769].", "problem": "You are asked to design and implement a complete program that constructs a near-separable dataset in two dimensions, introduces a single flipped label that renders the dataset non-separable, and then detects indefinite cycling in the Perceptron learning dynamics. You must also mitigate cycling using the pocket Perceptron and the averaged Perceptron, and report quantifiable metrics for a small test suite.\n\nThe fundamental base for this task consists of core definitions from linear classifiers. A Perceptron is a linear threshold unit that predicts a label in $\\{-1,+1\\}$ for an input vector $\\mathbf{x} \\in \\mathbb{R}^d$ via the sign of an affine score, which is implemented by augmenting $\\mathbf{x}$ with a bias term. Let $\\tilde{\\mathbf{x}} = (1, x_1, \\dots, x_d) \\in \\mathbb{R}^{d+1}$ be the bias-augmented input and let $\\mathbf{w} \\in \\mathbb{R}^{d+1}$ be the parameter vector. The prediction for an input-label pair $(\\mathbf{x}, y)$ is $\\operatorname{sign}(\\mathbf{w}^\\top \\tilde{\\mathbf{x}})$, and an example is considered correctly classified when $y \\cdot (\\mathbf{w}^\\top \\tilde{\\mathbf{x}})  0$. The Perceptron learning rule updates $\\mathbf{w}$ on a mistake as $\\mathbf{w} \\leftarrow \\mathbf{w} + y \\tilde{\\mathbf{x}}$. For linearly separable data, there exists a hyperplane such that $y_i \\cdot (\\mathbf{w}^\\top \\tilde{\\mathbf{x}}_i)  0$ for all $i$, and the Perceptron is guaranteed to find a solution in a finite number of updates. When the data are not linearly separable, the Perceptron may never reach zero training error and can continue making updates indefinitely. A principled way to certify non-separability is via convex hulls: if the convex hull of one class contains a point of the other class, then no separating hyperplane exists.\n\nYour dataset must be fully deterministic and constructed as follows in $d=2$ dimensions. Define three positive anchor points and three negative anchor points:\n- Positive anchors: $\\mathbf{p}_1 = (2,0)$, $\\mathbf{p}_2 = (0,2)$, $\\mathbf{p}_3 = (2,2)$ with labels $+1$,\n- Negative anchors: $\\mathbf{n}_1 = (-2,0)$, $\\mathbf{n}_2 = (0,-2)$, $\\mathbf{n}_3 = (-2,-2)$ with labels $-1$,\nand a single interior point $\\mathbf{q} = (1.2, 1.2)$ initially labeled $+1$. The convex hull of the positive anchors contains $\\mathbf{q}$. Flipping the label of $\\mathbf{q}$ to $-1$ makes the dataset non-separable because the convex hull of positive points contains a negative point. To explore margin effects, allow a scaling factor $s \\in \\mathbb{R}_{0}$ applied uniformly to all coordinates, producing scaled points $s \\cdot \\mathbf{p}_j$, $s \\cdot \\mathbf{n}_j$, and $s \\cdot \\mathbf{q}$ while retaining their labels. All computations must use the bias-augmented inputs $\\tilde{\\mathbf{x}} = (1, x_1, x_2)$.\n\nYou must implement:\n- A vanilla Perceptron with deterministic sample order, initializing $\\mathbf{w} = \\mathbf{0}$, and using the update $\\mathbf{w} \\leftarrow \\mathbf{w} + y \\tilde{\\mathbf{x}}$ whenever $y \\cdot (\\mathbf{w}^\\top \\tilde{\\mathbf{x}}) \\le 0$. An epoch is one full pass over the dataset in the specified order. Declare convergence if an epoch completes with zero mistakes.\n- A cycle detector that declares indefinite cycling when there are $E$ consecutive epochs with at least one update in each epoch and no improvement in the best (lowest) number of mistakes seen so far. This operationalizes the idea that the learning dynamics continue to change the parameters without reducing training error, which is consistent with non-separability.\n- A pocket mechanism that stores the parameter vector $\\mathbf{w}_{\\text{pocket}}$ which yields the lowest number of mistakes observed so far across all epochs.\n- An averaged Perceptron that returns $\\bar{\\mathbf{w}}$ equal to the arithmetic mean of the parameter vectors right after each update step. If there are zero updates, set $\\bar{\\mathbf{w}} = \\mathbf{w}$.\n\nDefine the training-set classification error rate for a parameter vector $\\mathbf{w}$ as\n$$\n\\mathcal{E}(\\mathbf{w}) \\;=\\; \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{1}\\big[\\, y_i \\cdot (\\mathbf{w}^\\top \\tilde{\\mathbf{x}}_i) \\le 0 \\,\\big],\n$$\nwhere $n$ is the total number of examples and $\\mathbf{1}[\\cdot]$ denotes the indicator function. Error rates must be expressed as decimals (unitless fractions between $0$ and $1$).\n\nImplement the following test suite of parameter values:\n- Test case $1$ (happy path, separable): flip flag $f = \\text{False}$, scale $s = 1.0$, maximum epochs $T = 50$, cycle patience $E = 5$.\n- Test case $2$ (non-separable due to single flipped label): flip flag $f = \\text{True}$, scale $s = 1.0$, maximum epochs $T = 200$, cycle patience $E = 10$.\n- Test case $3$ (separable with small margin): flip flag $f = \\text{False}$, scale $s = 0.2$, maximum epochs $T = 200$, cycle patience $E = 10$.\n\nFor each test case, run the vanilla Perceptron with the specified parameters and report five quantities in order:\n- $c$: a boolean indicating whether the vanilla Perceptron converged to zero training error,\n- $z$: a boolean indicating whether cycling was detected by the cycle detector,\n- $u$: an integer equal to the total number of updates performed,\n- $e_p$: a float equal to $\\mathcal{E}(\\mathbf{w}_{\\text{pocket}})$ on the training set,\n- $e_a$: a float equal to $\\mathcal{E}(\\bar{\\mathbf{w}})$ on the training set.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[c_1, z_1, u_1, e_{p,1}, e_{a,1}, c_2, z_2, u_2, e_{p,2}, e_{a,2}, c_3, z_3, u_3, e_{p,3}, e_{a,3}]$, where the subscript denotes the test case index. No other text should be printed.", "solution": "The task is to implement the Perceptron learning algorithm and analyze its behavior on a specially constructed dataset designed to be either linearly separable or non-separable. The analysis involves detecting convergence or cycling and evaluating the performance of the standard Perceptron, the pocket Perceptron, and the averaged Perceptron.\n\nFirst, we construct the dataset in $\\mathbb{R}^2$ as specified. The dataset consists of $n=7$ points.\nThe anchor points are defined as:\n- Positive anchors ($y=+1$): $\\mathbf{p}_1 = (2,0)$, $\\mathbf{p}_2 = (0,2)$, $\\mathbf{p}_3 = (2,2)$\n- Negative anchors ($y=-1$): $\\mathbf{n}_1 = (-2,0)$, $\\mathbf{n}_2 = (0,-2)$, $\\mathbf{n}_3 = (-2,-2)$\n- An interior point: $\\mathbf{q} = (1.2, 1.2)$\n\nA scaling factor $s  0$ is applied to all point coordinates. The label of $\\mathbf{q}$ is determined by a flip flag $f$. If $f$ is false, $\\mathbf{q}$ has label $+1$, rendering the dataset linearly separable. If $f$ is true, $\\mathbf{q}$ has label $-1$. In this case, $\\mathbf{q}$ is a negative point residing within the convex hull of the positive points (e.g., $s \\cdot \\mathbf{q} = 0.4(s \\cdot \\mathbf{p}_1) + 0.4(s \\cdot \\mathbf{p}_2) + 0.2(s \\cdot \\mathbf{p}_3)$), which makes the dataset linearly non-separable.\n\nFor computation, each input vector $\\mathbf{x} = (x_1, x_2)$ is augmented with a bias term, resulting in $\\tilde{\\mathbf{x}} = (1, x_1, x_2)$. The Perceptron's parameter vector is correspondingly $\\mathbf{w} \\in \\mathbb{R}^3$. The data points are processed in a fixed, deterministic order: $(\\mathbf{p}_1, \\dots, \\mathbf{p}_3, \\mathbf{n}_1, \\dots, \\mathbf{n}_3, \\mathbf{q})$.\n\nThe core of the implementation is a unified training function that simulates the Perceptron dynamics. The simulation proceeds for a maximum of $T$ epochs. An epoch consists of a single pass through all $n=7$ data points.\n\nThe simulation state includes:\n- The parameter vector $\\mathbf{w}$, initialized to $\\mathbf{0}$.\n- The total number of updates $u$, initialized to $0$.\n- The pocket parameter vector $\\mathbf{w}_{\\text{pocket}}$, which stores the weights that have achieved the minimum number of classification mistakes on the training set observed so far. It is initialized with the initial $\\mathbf{w}$.\n- The sum of all post-update weight vectors, $\\mathbf{w}_{\\text{sum}}$, initialized to $\\mathbf{0}$, used for the averaged Perceptron.\n\nThe main training loop iterates through epochs. Within each epoch, it iterates through the training examples $(\\tilde{\\mathbf{x}}_i, y_i)$. A mistake is identified if the condition $y_i \\cdot (\\mathbf{w}^\\top \\tilde{\\mathbf{x}}_i) \\le 0$ holds. Upon a mistake:\n1. The parameter vector is updated: $\\mathbf{w} \\leftarrow \\mathbf{w} + y_i \\tilde{\\mathbf{x}}_i$.\n2. The total update count $u$ is incremented.\n3. The new $\\mathbf{w}$ is added to the running sum: $\\mathbf{w}_{\\text{sum}} \\leftarrow \\mathbf{w}_{\\text{sum}} + \\mathbf{w}$.\n4. The pocket mechanism is engaged: The number of mistakes for the new $\\mathbf{w}$ is calculated over the entire training set. If this count is lower than the best count seen so far, $\\mathbf{w}_{\\text{pocket}}$ is updated to the new $\\mathbf{w}$, and the best mistake count is updated.\n\nAfter each epoch, two termination conditions are checked:\n- **Convergence**: If an epoch completes with zero updates, the algorithm has converged to a separating hyperplane. The flag $c$ is set to true, and the simulation halts.\n- **Cycling Detection**: A cycle is declared ($z$ is set to true) if there have been $E$ consecutive epochs where each epoch had at least one update, but the minimum number of mistakes seen so far has not improved. This is tracked by a counter that increments for each non-improving epoch (with updates) and resets whenever a new best weight vector is found by the pocket mechanism.\n\nThe simulation also terminates if the maximum number of epochs $T$ is reached.\n\nAfter the simulation terminates, the final metrics are computed:\n- $c$: Boolean, indicating if convergence was achieved.\n- $z$: Boolean, indicating if cycling was detected.\n- $u$: The total number of updates performed.\n- $e_p$: The training error rate of the final pocket vector, $\\mathcal{E}(\\mathbf{w}_{\\text{pocket}}) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{1}[y_i \\cdot (\\mathbf{w}_{\\text{pocket}}^\\top \\tilde{\\mathbf{x}}_i) \\le 0]$.\n- $e_a$: The training error rate of the averaged Perceptron vector $\\bar{\\mathbf{w}}$. $\\bar{\\mathbf{w}}$ is calculated as $\\frac{1}{u}\\mathbf{w}_{\\text{sum}}$ if $u  0$, or as the initial $\\mathbf{w}$ if $u=0$. The error is then $\\mathcal{E}(\\bar{\\mathbf{w}}) = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf{1}[y_i \\cdot (\\bar{\\mathbf{w}}^\\top \\tilde{\\mathbf{x}}_i) \\le 0]$.\n\nThis procedure is applied to each of the three test cases specified in the problem, and the five resulting metrics for each case are concatenated to produce the final output.", "answer": "```python\nimport numpy as np\n\ndef run_simulation(flip_flag: bool, scale: float, max_epochs: int, cycle_patience: int):\n    \"\"\"\n    Runs a Perceptron simulation for a given configuration.\n\n    Args:\n        flip_flag (bool): If True, flips the label of the interior point 'q'.\n        scale (float): Scaling factor for all point coordinates.\n        max_epochs (int): Maximum number of training epochs.\n        cycle_patience (int): Number of non-improving epochs to detect cycling.\n\n    Returns:\n        tuple: (converged, cycled, updates, pocket_error, avg_error)\n    \"\"\"\n    # 1. Dataset Construction\n    points = [\n        (np.array([2.0, 0.0]), 1),\n        (np.array([0.0, 2.0]), 1),\n        (np.array([2.0, 2.0]), 1),\n        (np.array([-2.0, 0.0]), -1),\n        (np.array([0.0, -2.0]), -1),\n        (np.array([-2.0, -2.0]), -1),\n        (np.array([1.2, 1.2]), -1 if flip_flag else 1)\n    ]\n    \n    n = len(points)\n    \n    # Scale and augment data\n    X = np.ones((n, 3))\n    y = np.zeros(n)\n    for i, (coord, label) in enumerate(points):\n        X[i, 1:] = coord * scale\n        y[i] = label\n\n    # 2. Initialization\n    w = np.zeros(3)\n    w_pocket = np.copy(w)\n    w_sum = np.zeros(3)\n    total_updates = 0\n    converged = False\n    cycling_detected = False\n\n    def get_mistake_count(w_vec):\n        return np.sum(y * (X @ w_vec) = 0)\n\n    min_mistakes_so_far = get_mistake_count(w)\n    epochs_at_min_mistakes = 0\n\n    # 3. Main Training Loop\n    for epoch in range(max_epochs):\n        updates_in_epoch = 0\n        new_min_found_this_epoch = False\n\n        for i in range(n):\n            if y[i] * (w @ X[i]) = 0:\n                updates_in_epoch += 1\n                total_updates += 1\n                w = w + y[i] * X[i]\n                w_sum = w_sum + w\n\n                # Pocket logic\n                current_mistakes = get_mistake_count(w)\n                if current_mistakes  min_mistakes_so_far:\n                    min_mistakes_so_far = current_mistakes\n                    w_pocket = np.copy(w)\n                    new_min_found_this_epoch = True\n        \n        # End-of-epoch checks\n        if updates_in_epoch == 0:\n            converged = True\n            break\n        \n        # Cycle detection logic\n        if new_min_found_this_epoch:\n            epochs_at_min_mistakes = 0\n        else:\n            epochs_at_min_mistakes += 1\n        \n        if epochs_at_min_mistakes = cycle_patience:\n            cycling_detected = True\n            break\n\n    # 4. Final Calculations\n    e_p = get_mistake_count(w_pocket) / n\n\n    if total_updates  0:\n        w_avg = w_sum / total_updates\n    else:\n        w_avg = w_pocket # If no updates, w_sum is zero, use initial w\n    \n    e_a = get_mistake_count(w_avg) / n\n\n    return converged, cycling_detected, total_updates, e_p, e_a\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        # (flip_flag, scale, max_epochs, cycle_patience)\n        (False, 1.0, 50, 5),    # Case 1\n        (True, 1.0, 200, 10),   # Case 2\n        (False, 0.2, 200, 10),  # Case 3\n    ]\n\n    results = []\n    for case in test_cases:\n        c, z, u, e_p, e_a = run_simulation(*case)\n        results.extend([c, z, u, round(e_p, 8), round(e_a, 8)])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3190769"}]}