{"hands_on_practices": [{"introduction": "While $k$-fold cross-validation is a cornerstone of model evaluation, a naive implementation can lead to misleading results, especially with the imbalanced datasets common in deep learning. This practice guides you to mathematically and computationally quantify the error inflation that arises when failing to stratify folds. By deriving the impact from first principles, you will gain a concrete understanding of why stratified sampling is not just a good practice, but a necessary one for reliable error estimation. [@problem_id:3134712]", "problem": "Consider a binary classification problem with severe class imbalance. Let the class label be $Y \\in \\{0,1\\}$ with population prior $\\mathbb{P}(Y=1)=\\pi$ and $\\mathbb{P}(Y=0)=1-\\pi$, where $\\pi \\ll 0.5$. Given a single real-valued feature $X$, suppose the class-conditional distributions are Gaussian with equal variance: $X \\mid Y=0 \\sim \\mathcal{N}(0,1)$ and $X \\mid Y=1 \\sim \\mathcal{N}(\\mu,1)$, with $\\mu>0$ known. Consider $k$-fold Cross-Validation (CV), performed on a dataset of size $N$, with $k$ equally sized folds.\n\nUse the following fundamental base:\n- The Bayes classifier under known class-conditional densities and prior uses the likelihood ratio test, which reduces to a threshold rule on $X$ when the variances are equal.\n- The misclassification risk is defined as the population error $\\mathbb{P}(\\hat{Y}\\neq Y)$, computed under the true data distribution.\n- A non-stratified $k$-fold split is a random partition of the $N$ samples into $k$ folds without regard to class labels.\n- A stratified $k$-fold split ensures each fold reflects the population class proportion as closely as possible. In this problem, assume exact stratification: $N\\pi$ is an integer and each fold contains exactly $N\\pi/k$ samples from the minority class and $N(1-\\pi)/k$ from the majority class.\n\nDefine the classifier trained within a fold as the likelihood-ratio threshold rule that uses the empirical class prior $\\hat{\\pi}$ estimated from the training portion of that fold, while using the true, known class-conditional densities. Specifically, the decision rule is \"predict $Y=1$ if $X \\ge t(\\hat{\\pi})$ and $Y=0$ otherwise\", where $t(\\hat{\\pi})$ is the threshold implied by the likelihood ratio test with prior $\\hat{\\pi}$. The fold’s test error is measured on data drawn from the same population.\n\nYou must:\n1. Derive from first principles the decision threshold $t(\\hat{\\pi})$ for the Gaussian equal-variance case starting from the likelihood ratio test, and the corresponding misclassification risk $R(t)$ as a function of $t$, $\\pi$, and $\\mu$.\n2. Under non-stratified $k$-fold, model the training prior $\\hat{\\pi}$ as a random variable induced by the random fold assignment. Let $M=N\\pi$ be the total number of minority samples (an integer by assumption), and $n_{\\text{train}}=N(k-1)/k$ be the training size per fold. For a given fold, the number of minority samples in the training set, $m$, follows a Hypergeometric distribution with parameters $(N, M, n_{\\text{train}})$. Therefore, $\\hat{\\pi}=m/n_{\\text{train}}$. Using this, compute the expected non-stratified CV risk as the expectation of $R\\big(t(\\hat{\\pi})\\big)$ with respect to the Hypergeometric distribution of $m$.\n3. Under stratified $k$-fold with exact per-fold class proportions, the training prior equals the population prior $\\pi$ exactly, hence the stratified CV risk equals $R\\big(t(\\pi)\\big)$.\n4. Quantify the error inflation $\\Delta$ due to non-stratification as\n$$\n\\Delta = \\mathbb{E}\\left[R\\big(t(\\hat{\\pi})\\big)\\right] - R\\big(t(\\pi)\\big).\n$$\nCompute $\\Delta$ for each test case specified below.\n\nYou must implement a complete, runnable program that, for each parameter tuple $(N,k,\\pi,\\mu)$, computes the single-fold expected non-stratified CV risk via the exact Hypergeometric distribution, the stratified CV risk, and outputs the inflation $\\Delta$ as a float. Handle the degenerate cases $\\hat{\\pi}=0$ and $\\hat{\\pi}=1$ correctly by interpreting the threshold rule limits: if $\\hat{\\pi}=0$, the classifier predicts $Y=0$ always, yielding risk $R=\\pi$; if $\\hat{\\pi}=1$, the classifier predicts $Y=1$ always, yielding risk $R=1-\\pi$.\n\nTest Suite:\n- Case $1$ (happy path): $(N,k,\\pi,\\mu) = (500,5,0.05,2.0)$.\n- Case $2$ (small data, still severely imbalanced): $(N,k,\\pi,\\mu) = (100,10,0.10,1.5)$.\n- Case $3$ (large data, extreme imbalance): $(N,k,\\pi,\\mu) = (5000,10,0.01,1.0)$.\n- Case $4$ (moderate data, strong separation): $(N,k,\\pi,\\mu) = (1200,4,0.03,3.0)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$), where each $result_i$ is the computed $\\Delta$ for Case $i$, rounded to $6$ decimal places. No other text should be printed.", "solution": "The problem requires us to quantify the error inflation, denoted by $\\Delta$, that arises from using non-stratified $k$-fold cross-validation compared to stratified $k$-fold cross-validation for a binary classifier with a severe class imbalance. The analysis is based on a specific theoretical setup involving Gaussian class-conditional distributions. We will proceed by first deriving the necessary theoretical components, then applying them to the specified cross-validation schemes.\n\n### 1. Decision Threshold and Misclassification Risk\n\nThe classifier's decision is based on the likelihood ratio test, which compares the posterior probabilities of the two classes. For a given feature value $X$, the classifier predicts class $Y=1$ if $\\mathbb{P}(Y=1|X) > \\mathbb{P}(Y=0|X)$. Using Bayes' theorem, this is equivalent to $\\frac{p(X|Y=1)\\mathbb{P}(Y=1)}{p(X)} > \\frac{p(X|Y=0)\\mathbb{P}(Y=0)}{p(X)}$, which simplifies to the likelihood ratio test:\n$$\n\\frac{p(X|Y=1)}{p(X|Y=0)} > \\frac{\\mathbb{P}(Y=0)}{\\mathbb{P}(Y=1)}\n$$\nThe problem specifies that the classifier uses the true class-conditional densities but estimates the prior probability $\\mathbb{P}(Y=1)$ with its empirical estimate $\\hat{\\pi}$ from the training data. The decision rule thus becomes:\n$$\n\\frac{p(X|Y=1)}{p(X|Y=0)} > \\frac{1-\\hat{\\pi}}{\\hat{\\pi}}\n$$\nThe class-conditional densities are given as $X \\mid Y=0 \\sim \\mathcal{N}(0,1)$ and $X \\mid Y=1 \\sim \\mathcal{N}(\\mu,1)$. Their probability density functions (PDFs) are:\n$$\np(X|Y=0) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{X^2}{2}\\right)\n$$\n$$\np(X|Y=1) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(X-\\mu)^2}{2}\\right)\n$$\nThe likelihood ratio is:\n$$\n\\frac{p(X|Y=1)}{p(X|Y=0)} = \\frac{\\exp\\left(-\\frac{(X-\\mu)^2}{2}\\right)}{\\exp\\left(-\\frac{X^2}{2}\\right)} = \\exp\\left(\\frac{X^2 - (X-\\mu)^2}{2}\\right) = \\exp\\left(\\frac{X^2 - (X^2 - 2\\mu X + \\mu^2)}{2}\\right) = \\exp\\left(\\mu X - \\frac{\\mu^2}{2}\\right)\n$$\nSubstituting this into the decision rule inequality:\n$$\n\\exp\\left(\\mu X - \\frac{\\mu^2}{2}\\right) > \\frac{1-\\hat{\\pi}}{\\hat{\\pi}}\n$$\nTaking the natural logarithm of both sides (a monotonic transformation that preserves the inequality):\n$$\n\\mu X - \\frac{\\mu^2}{2} > \\ln\\left(\\frac{1-\\hat{\\pi}}{\\hat{\\pi}}\\right)\n$$\nSince $\\mu > 0$ is given, we can solve for $X$ to find the decision threshold $t(\\hat{\\pi})$:\n$$\nX > \\frac{1}{\\mu}\\left(\\frac{\\mu^2}{2} + \\ln\\left(\\frac{1-\\hat{\\pi}}{\\hat{\\pi}}\\right)\\right)\n$$\nThus, the decision threshold as a function of the empirical prior $\\hat{\\pi}$ is:\n$$\nt(\\hat{\\pi}) = \\frac{\\mu}{2} + \\frac{1}{\\mu}\\ln\\left(\\frac{1-\\hat{\\pi}}{\\hat{\\pi}}\\right)\n$$\nThe classifier's rule is to predict $Y=1$ if $X \\geq t(\\hat{\\pi})$ and $Y=0$ otherwise.\n\nNext, we derive the misclassification risk $R(t)$ for a given threshold $t$. The risk is the population error rate, evaluated using the true population prior $\\pi$. It is the sum of the probabilities of Type I and Type II errors, weighted by the true class priors:\n$$\nR(t) = \\mathbb{P}(\\text{predict } 1 \\mid Y=0)\\mathbb{P}(Y=0) + \\mathbb{P}(\\text{predict } 0 \\mid Y=1)\\mathbb{P}(Y=1)\n$$\n$$\nR(t) = \\mathbb{P}(X \\ge t \\mid Y=0)(1-\\pi) + \\mathbb{P}(X < t \\mid Y=1)\\pi\n$$\nLet $\\Phi(\\cdot)$ be the cumulative distribution function (CDF) of the standard normal distribution $\\mathcal{N}(0,1)$.\nFor the first term, since $X \\mid Y=0 \\sim \\mathcal{N}(0,1)$, we have $\\mathbb{P}(X \\ge t \\mid Y=0) = 1 - \\Phi(t)$.\nFor the second term, since $X \\mid Y=1 \\sim \\mathcal{N}(\\mu,1)$, the variable $X-\\mu$ follows $\\mathcal{N}(0,1)$. Thus, $\\mathbb{P}(X < t \\mid Y=1) = \\mathbb{P}(X-\\mu < t-\\mu \\mid Y=1) = \\Phi(t-\\mu)$.\nThe misclassification risk is therefore:\n$$\nR(t) = (1-\\pi)(1 - \\Phi(t)) + \\pi \\Phi(t-\\mu)\n$$\n\n### 2. Stratified vs. Non-Stratified Cross-Validation Risk\n\n**Stratified CV Risk:**\nIn a stratified $k$-fold split, each fold is constructed to have the same class proportion as the overall dataset. The training set for any fold, which comprises $k-1$ folds, will therefore also have this exact proportion. Thus, the empirical prior $\\hat{\\pi}$ estimated from the training set is always equal to the population prior $\\pi$.\n$$\n\\hat{\\pi}_{\\text{strat}} = \\pi\n$$\nThe decision threshold is constant for all folds: $t_{\\text{strat}} = t(\\pi) = \\frac{\\mu}{2} + \\frac{1}{\\mu}\\ln\\left(\\frac{1-\\pi}{\\pi}\\right)$. This is the optimal Bayes threshold. The resulting risk, which we denote as $R_{\\text{strat}}$, is the Bayes error rate:\n$$\nR_{\\text{strat}} = R(t(\\pi)) = (1-\\pi)(1 - \\Phi(t(\\pi))) + \\pi \\Phi(t(\\pi)-\\mu)\n$$\n\n**Non-Stratified CV Risk:**\nIn a non-stratified split, the data is partitioned randomly into $k$ folds. The number of minority class samples in the training data for a fold is a random variable. Let $N$ be the total sample size, $M=N\\pi$ be the total number of minority samples, and $n_{\\text{train}} = N(k-1)/k$ be the training set size. The number of minority samples in the training set, $m$, follows a Hypergeometric distribution with parameters for population size ($N$), number of successes in population ($M$), and number of draws ($n_{\\text{train}}$). We write this as $m \\sim \\text{Hypergeometric}(N, M, n_{\\text{train}})$.\nThe probability mass function (PMF) is $\\mathbb{P}(m) = \\frac{\\binom{M}{m}\\binom{N-M}{n_{\\text{train}}-m}}{\\binom{N}{n_{\\text{train}}}}$.\nThe empirical prior for a given fold is $\\hat{\\pi} = m/n_{\\text{train}}$, which is a random variable. The decision threshold $t(\\hat{\\pi})$ and the corresponding risk $R(t(\\hat{\\pi}))$ are also random variables. The expected non-stratified CV risk is the expectation of this risk over the distribution of $m$:\n$$\n\\mathbb{E}\\left[R\\big(t(\\hat{\\pi})\\big)\\right] = \\sum_{m} \\mathbb{P}(m) \\cdot R\\left(t\\left(\\frac{m}{n_{\\text{train}}}\\right)\\right)\n$$\nThe summation is over the support of the Hypergeometric distribution, $m \\in [\\max(0, n_{\\text{train}} - (N-M)), \\min(n_{\\text{train}}, M)]$.\n\nThe problem specifies handling for the edge cases where $\\hat{\\pi}=0$ or $\\hat{\\pi}=1$:\n-   If $\\hat{\\pi}=0$ (i.e., $m=0$), $\\ln((1-\\hat{\\pi})/\\hat{\\pi}) \\to \\infty$, so $t(0) \\to \\infty$. The classifier always predicts $Y=0$. The risk is $R(\\infty) = (1-\\pi)(1-\\Phi(\\infty)) + \\pi \\Phi(\\infty-\\mu) = (1-\\pi) \\cdot 0 + \\pi \\cdot 1 = \\pi$.\n-   If $\\hat{\\pi}=1$ (i.e., $m=n_{\\text{train}}$), $\\ln((1-\\hat{\\pi})/\\hat{\\pi}) \\to -\\infty$, so $t(1) \\to -\\infty$. The classifier always predicts $Y=1$. The risk is $R(-\\infty) = (1-\\pi)(1-\\Phi(-\\infty)) + \\pi \\Phi(-\\infty-\\mu) = (1-\\pi) \\cdot 1 + \\pi \\cdot 0 = 1-\\pi$.\n\n### 3. Error Inflation $\\Delta$\n\nThe error inflation $\\Delta$ is defined as the difference between the expected risk under non-stratified CV and the risk under stratified CV:\n$$\n\\Delta = \\mathbb{E}\\left[R\\big(t(\\hat{\\pi})\\big)\\right] - R_{\\text{strat}}\n$$\nThis quantity captures the average increase in misclassification error due to the variability of the empirical prior estimate in non-stratified folds. This variability pushes the decision threshold away from the optimal Bayes threshold, and due to the convexity of the risk function around its minimum (the Bayes error), the expected risk is higher than the risk at the expected prior. By Jensen's inequality, since the risk function is convex, $\\mathbb{E}[R(t(\\hat{\\pi}))] \\ge R(t(\\mathbb{E}[\\hat{\\pi}]))$. Since $\\mathbb{E}[\\hat{\\pi}] = \\pi$, we expect $\\Delta \\ge 0$. Our task is to compute this value for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import hypergeom, norm\n\ndef solve():\n    \"\"\"\n    Computes the error inflation delta for a series of test cases.\n    \"\"\"\n    \n    # Test cases: tuples of (N, k, pi, mu)\n    test_cases = [\n        (500, 5, 0.05, 2.0),\n        (100, 10, 0.10, 1.5),\n        (5000, 10, 0.01, 1.0),\n        (1200, 4, 0.03, 3.0),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N, k, pi, mu = case\n        \n        # Ensure N*pi is an integer as per problem statement\n        M = int(round(N * pi))\n        n_train = int(N * (k - 1) / k)\n        \n        # --- 1. Calculate Stratified CV Risk (R_strat) ---\n        # The empirical prior is the true prior\n        pi_strat = pi\n        \n        # Calculate the Bayes optimal threshold t(pi)\n        # logit(pi) = log(pi / (1-pi)), so -log((1-pi)/pi)\n        t_strat = mu / 2.0 - (1.0 / mu) * np.log(pi_strat / (1.0 - pi_strat))\n        \n        # Calculate the risk R(t(pi)), which is the Bayes error rate\n        risk_type1_strat = 1.0 - norm.cdf(t_strat)\n        risk_type2_strat = norm.cdf(t_strat - mu)\n        R_strat = (1.0 - pi) * risk_type1_strat + pi * risk_type2_strat\n        \n        # --- 2. Calculate Expected Non-Stratified CV Risk (E_non_strat) ---\n        \n        # The number of minority samples 'm' in the training set follows a\n        # Hypergeometric distribution.\n        # Scipy's hypergeom(M, n, N) takes:\n        # M: total number of objects (our N)\n        # n: total number of type I objects (our M)\n        # N: number of draws (our n_train)\n        dist = hypergeom(M=N, n=M, N=n_train)\n        \n        # Support of the hypergeometric distribution for m\n        m_min = max(0, n_train - (N - M))\n        m_max = min(n_train, M)\n        \n        E_non_strat = 0.0\n        \n        for m in range(m_min, m_max + 1):\n            prob_m = dist.pmf(m)\n            \n            # If there's no probability mass, skip to avoid unnecessary calculation\n            if prob_m == 0:\n                continue\n\n            pi_hat = m / n_train\n            \n            # Handle edge cases as per problem statement\n            if m == 0:  # pi_hat = 0\n                risk_m = pi\n            elif m == n_train:  # pi_hat = 1\n                risk_m = 1.0 - pi\n            else:\n                # Calculate threshold t(pi_hat) for this m\n                t_m = mu / 2.0 - (1.0 / mu) * np.log(pi_hat / (1.0 - pi_hat))\n                \n                # Calculate risk R(t(pi_hat))\n                risk_type1_m = 1.0 - norm.cdf(t_m)\n                risk_type2_m = norm.cdf(t_m - mu)\n                risk_m = (1.0 - pi) * risk_type1_m + pi * risk_type2_m\n            \n            E_non_strat += prob_m * risk_m\n            \n        # --- 3. Compute Error Inflation Delta ---\n        delta = E_non_strat - R_strat\n        results.append(round(delta, 6))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3134712"}, {"introduction": "Building on the principle of maintaining data integrity across folds, this exercise tackles a common and dangerous pitfall in modern machine learning: data augmentation leakage. You will set up a computational experiment to quantify the optimistic bias introduced when augmentation is not strictly confined to the training data of each fold. This practice is essential for developing reliable evaluation pipelines, particularly in domains like computer vision where augmentation is a standard tool. [@problem_id:3134696]", "problem": "You are tasked with designing and implementing a computational experiment to analyze $k$-fold Cross-Validation (CV) in Statistical Learning under data augmentation with image rotations. The aim is to quantify how applying augmentation only to training folds compares to the scenario where augmentation leaks into validation folds. Your program must be a complete, runnable implementation that produces the requested outputs with no external input.\n\nFundamental base and definitions to be used:\n- Cross-Validation (CV): For a given dataset $D = \\{(x_i, y_i)\\}_{i=1}^n$ and a learning algorithm, the $k$-fold CV estimator of the generalization error is defined as the average validation loss over $k$ folds. Let the folds be index sets $V_1, \\dots, V_k$ that partition $\\{1,\\dots,n\\}$, and $T_j = \\{1,\\dots,n\\} \\setminus V_j$ denote the corresponding training indices. For a model $\\hat{f}_j$ trained on $T_j$, the CV estimator of accuracy is\n$$\n\\widehat{\\text{Acc}}_{\\text{CV}} = \\frac{1}{k} \\sum_{j=1}^k \\frac{1}{|V_j|} \\sum_{i \\in V_j} \\mathbf{1}\\{\\hat{f}_j(x_i) = y_i\\},\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ is the indicator function.\n- Data augmentation: A label-preserving transformation $\\mathcal{A}_\\theta$ applied to an input $x$ parameterized by a rotation angle $\\theta$ measured in degrees. In this problem, $\\theta \\in \\{90, 180, 270\\}$, and $\\mathcal{A}_\\theta$ is image rotation by $\\theta$ degrees about the image center.\n- Independence principle: Proper CV requires that validation samples are independent of training samples conditional on the data-generating process. Leakage occurs when transformed versions of the same base sample appear in both training and validation folds, violating the independence assumption and biasing the estimator.\n\nDataset construction:\n- Consider $n$ base images $\\{x_i\\}_{i=1}^n$ of size $h \\times w$ with labels $y_i \\in \\{0,1\\}$, where class $0$ is a centered \"$+$\" pattern and class $1$ is a centered \"$\\times$\" pattern. Each base image is produced by perturbing the ideal template with small random offsets in line thickness and additive Gaussian noise with standard deviation $\\sigma$, then clipping pixel intensities to the interval $[0,1]$. Rotations by $\\theta \\in \\{90,180,270\\}$ preserve class labels.\n- The classifier is the nearest-centroid classifier under squared $\\ell_2$ distance. For a training set $(X_{\\text{train}}, Y_{\\text{train}})$, compute class centroids\n$$\n\\mu_c = \\frac{1}{|\\{i: Y_{\\text{train},i} = c\\}|} \\sum_{i:Y_{\\text{train},i}=c} X_{\\text{train},i}, \\quad c \\in \\{0,1\\},\n$$\nand predict $\\hat{y}(x) = \\arg\\min_{c \\in \\{0,1\\}} \\|x - \\mu_c\\|_2^2$.\n\nTwo CV estimators to compare:\n1. Proper augmentation CV: For each fold $j \\in \\{1,\\dots,k\\}$, augment only the training images by rotations in a given angle set $S$ (angles in degrees), i.e., use $\\{x_i\\} \\cup \\{\\mathcal{A}_\\theta(x_i): \\theta \\in S\\}$ for $i \\in T_j$ to train $\\hat{f}_j$, and evaluate on the unaugmented validation images $\\{x_i: i \\in V_j\\}$.\n2. Leakage CV: First, globally augment the entire dataset by rotations in $S$ to form $D' = \\bigcup_{i=1}^n \\left(\\{x_i\\} \\cup \\{\\mathcal{A}_\\theta(x_i): \\theta \\in S\\}\\right)$, then perform $k$-fold CV on $D'$, training and validating on these augmented samples. This procedure allows augmented variants of the same base image to appear across different folds, producing dependence between training and validation samples.\n\nYour program must:\n- Generate the synthetic dataset as specified.\n- Implement the nearest-centroid classifier.\n- Compute the average $k$-fold CV accuracy for both procedures and return the difference\n$$\n\\Delta = \\widehat{\\text{Acc}}_{\\text{CV}}^{\\text{leak}} - \\widehat{\\text{Acc}}_{\\text{CV}}^{\\text{proper}}.\n$$\n\nAngle units: All angles are measured in degrees.\n\nAnswer types: All outputs must be real numbers (floats).\n\nTest suite and coverage:\nImplement the following test cases, each defined as a tuple $(n, h, w, k, S, \\sigma, \\text{seed})$ and processed in order. For every test case, compute and output $\\Delta$.\n\n- Case $1$ (happy path): $(n, h, w, k, S, \\sigma, \\text{seed}) = (60, 16, 16, 5, \\{90, 180, 270\\}, 0.10, 42)$.\n- Case $2$ (boundary, no augmentation): $(60, 16, 16, 5, \\varnothing, 0.10, 43)$, where $\\varnothing$ denotes the empty set of angles.\n- Case $3$ (few folds): $(60, 16, 16, 2, \\{90, 180, 270\\}, 0.10, 44)$.\n- Case $4$ (high noise): $(60, 16, 16, 5, \\{90, 180, 270\\}, 0.50, 45)$.\n- Case $5$ (single-angle augmentation): $(60, 16, 16, 10, \\{90\\}, 0.10, 46)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test cases. For example, an output with three results should look like $[r_1,r_2,r_3]$, where each $r_i$ is a float. Angles are in degrees. No additional text should be printed.", "solution": "We start from the core definition of $k$-fold Cross-Validation (CV) and the independence requirement. The $k$-fold CV estimator averages validation performance across disjoint folds, and its reliability as an estimator of generalization performance depends on the conditional independence between training and validation sets given the data-generating process. When augmentation is applied only to training folds, transformations $\\mathcal{A}_\\theta$ enrich the training distribution while keeping validation samples independent draws from the original data distribution, preserving the CV estimator’s integrity. When augmentation leaks into validation folds—specifically by augmenting the entire dataset before constructing folds—augmented versions of the same base sample can be assigned both to training and validation sets, violating independence and inducing optimism bias.\n\nWe formalize the two estimators:\n- Proper augmentation CV estimator:\n$$\n\\widehat{\\text{Acc}}_{\\text{proper}} = \\frac{1}{k} \\sum_{j=1}^k \\frac{1}{|V_j|} \\sum_{i \\in V_j} \\mathbf{1}\\left\\{ \\arg\\min_{c \\in \\{0,1\\}} \\|x_i - \\mu_{c}^{(j)}\\|_2^2 = y_i \\right\\},\n$$\nwith\n$$\n\\mu_{c}^{(j)} = \\frac{1}{|\\{t \\in T_j: y_t = c\\}| + |\\{(t,\\theta): t \\in T_j, y_t = c, \\theta \\in S\\}|} \\left( \\sum_{t \\in T_j, y_t=c} x_t + \\sum_{t \\in T_j, y_t=c} \\sum_{\\theta \\in S} \\mathcal{A}_\\theta(x_t) \\right).\n$$\nValidation uses the unaugmented $x_i$ for $i \\in V_j$.\n\n- Leakage CV estimator:\nConstruct $D' = \\bigcup_{i=1}^n \\left(\\{x_i\\} \\cup \\{\\mathcal{A}_\\theta(x_i): \\theta \\in S\\}\\right)$, then perform $k$-fold CV directly on $D'$. Let $V'_1, \\dots, V'_k$ be the folds and $T'_j$ be their complements. The leakage estimator is\n$$\n\\widehat{\\text{Acc}}_{\\text{leak}} = \\frac{1}{k} \\sum_{j=1}^k \\frac{1}{|V'_j|} \\sum_{(i,\\phi) \\in V'_j} \\mathbf{1}\\left\\{ \\arg\\min_{c \\in \\{0,1\\}} \\| \\mathcal{A}_\\phi (x_i) - \\tilde{\\mu}_{c}^{(j)} \\|_2^2 = y_i \\right\\},\n$$\nwith\n$$\n\\tilde{\\mu}_{c}^{(j)} = \\frac{1}{|\\{(t,\\psi) \\in T'_j: y_t = c\\}|} \\sum_{(t,\\psi) \\in T'_j, y_t=c} \\mathcal{A}_\\psi(x_t).\n$$\n\nBias mechanism:\nIn the proper procedure, $x_i$ in validation is not used (directly or via its transformed copies) in training. In the leakage procedure, for many $i$, some transformed $\\mathcal{A}_\\psi(x_i)$ can be present in training while $\\mathcal{A}_\\phi(x_i)$ is used for validation, creating high correlation between training and validation samples. For a nearest-centroid classifier, this correlation reduces $\\| \\mathcal{A}_\\phi (x_i) - \\tilde{\\mu}_{y_i}^{(j)} \\|_2^2$ and increases the probability of correct classification, hence $\\widehat{\\text{Acc}}_{\\text{leak}} \\ge \\widehat{\\text{Acc}}_{\\text{proper}}$ on average. The quantity of interest is the difference\n$$\n\\Delta = \\widehat{\\text{Acc}}_{\\text{leak}} - \\widehat{\\text{Acc}}_{\\text{proper}}.\n$$\n\nAlgorithmic design:\n1. Synthetic data generation:\n   - Create $n$ base images of size $h \\times w$. For class $0$, draw a centered \"$+$\" by setting pixels in a vertical and horizontal bar crossing the center, with random thickness (e.g., $1$ or $2$ pixels) and small random center offsets. For class $1$, draw a centered \"$\\times$\" by setting pixels along both diagonals with similar random thickness and offsets.\n   - Add independent Gaussian noise with standard deviation $\\sigma$ to each pixel and clip values to $[0,1]$.\n   - Assign labels such that $n/2$ images belong to each class (balanced), then shuffle.\n\n2. Augmentation operator:\n   - For angles $\\theta \\in S \\subseteq \\{90,180,270\\}$ measured in degrees, implement $\\mathcal{A}_\\theta$ via $90$-degree increments (i.e., $\\text{np.rot90}$ with $k = \\theta/90$), preserving labels.\n\n3. Classifier:\n   - Nearest-centroid classifier under squared $\\ell_2$ distance. Flatten images to vectors in $\\mathbb{R}^{h \\cdot w}$ for computations. Compute class centroids from the training set (including augmented samples as required) and predict the class of validation samples by nearest centroid.\n\n4. $k$-fold splitting:\n   - For proper augmentation CV, split the $n$ base samples into $k$ folds. For fold $j$, augment only the training set by angles in $S$, keep validation unaugmented, train centroids, and compute accuracy on the validation fold. Average across folds to obtain $\\widehat{\\text{Acc}}_{\\text{proper}}$.\n   - For leakage CV, first augment the entire dataset (include originals and rotations in $S$), then perform $k$-fold CV on this augmented dataset, training and validating on augmented samples to obtain $\\widehat{\\text{Acc}}_{\\text{leak}}$.\n\n5. Difference:\n   - Compute $\\Delta$ for each test case.\n\nTest suite coverage rationale:\n- Case $1$ uses moderate $k$, multiple angles, and small noise, representing a typical scenario.\n- Case $2$ uses $S = \\varnothing$ (no augmentation), where we expect $\\Delta \\approx 0$ because both procedures coincide.\n- Case $3$ uses small $k$, increasing the chance that augmented duplicates cross folds, potentially amplifying leakage effects.\n- Case $4$ increases noise $\\sigma$, examining robustness; leakage can artificially boost performance under noisy conditions.\n- Case $5$ uses $k = 10$ and a single angle, testing sensitivity to the augmentation set size.\n\nFinal output:\nA single line $[\\Delta_1,\\Delta_2,\\Delta_3,\\Delta_4,\\Delta_5]$ with floats corresponding to the cases above, in order.\n\nThis design strictly adheres to the independence principle underlying CV and exposes, through computation, how violating it via augmentation leakage produces optimistic bias in the estimated accuracy.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef make_plus_image(h, w, thickness, off_y, off_x):\n    \"\"\"\n    Create a centered plus '+' pattern with given thickness and small offsets.\n    Intensities are 1.0 on the pattern, 0.0 elsewhere.\n    \"\"\"\n    img = np.zeros((h, w), dtype=np.float64)\n    cy = h // 2 + off_y\n    cx = w // 2 + off_x\n    # Horizontal bar\n    y_start = max(cy - thickness, 0)\n    y_end = min(cy + thickness + 1, h)\n    img[y_start:y_end, :] = 1.0\n    # Vertical bar\n    x_start = max(cx - thickness, 0)\n    x_end = min(cx + thickness + 1, w)\n    img[:, x_start:x_end] = 1.0\n    return img\n\ndef make_x_image(h, w, thickness, off_y, off_x):\n    \"\"\"\n    Create a centered 'x' pattern (two diagonals) with given thickness and small offsets.\n    Intensities are 1.0 on the pattern, 0.0 elsewhere.\n    \"\"\"\n    img = np.zeros((h, w), dtype=np.float64)\n    cy = h // 2 + off_y\n    cx = w // 2 + off_x\n    # Draw diagonals with thickness by marking pixels close to diagonal lines\n    yy, xx = np.meshgrid(np.arange(h), np.arange(w), indexing='ij')\n    # Centered diagonals adjusted by offsets\n    # Line 1: slope +1 through (cy, cx)\n    # Distance from line y - cy = x - cx => y - x = cy - cx\n    d1 = np.abs((yy - xx) - (cy - cx))\n    # Line 2: slope -1 through (cy, cx)\n    # Distance from line y - cy = -(x - cx) => y + x = cy + cx\n    d2 = np.abs((yy + xx) - (cy + cx))\n    mask = (d1 <= thickness) | (d2 <= thickness)\n    img[mask] = 1.0\n    return img\n\ndef add_noise_and_clip(img, noise_std, rng):\n    noisy = img + rng.normal(loc=0.0, scale=noise_std, size=img.shape)\n    return np.clip(noisy, 0.0, 1.0)\n\ndef generate_dataset(n, h, w, noise_std, seed):\n    \"\"\"\n    Generate n base images: half '+' class (label 0), half 'x' class (label 1),\n    with random thickness and small offsets, plus Gaussian noise.\n    Returns images (n, h, w) and labels (n,).\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    images = np.zeros((n, h, w), dtype=np.float64)\n    labels = np.zeros(n, dtype=np.int64)\n    # Balanced labels: 0 then 1 alternating\n    base_labels = np.array([0, 1] * (n // 2) + ([0] if n % 2 == 1 else []), dtype=np.int64)\n    # Shuffle labels for randomness\n    rng.shuffle(base_labels)\n    for i in range(n):\n        c = base_labels[i]\n        labels[i] = c\n        thickness = rng.integers(1, 3)  # 1 or 2\n        off_y = rng.integers(-1, 2)     # -1, 0, 1\n        off_x = rng.integers(-1, 2)     # -1, 0, 1\n        if c == 0:\n            img = make_plus_image(h, w, thickness, off_y, off_x)\n        else:\n            img = make_x_image(h, w, thickness, off_y, off_x)\n        img = add_noise_and_clip(img, noise_std, rng)\n        images[i] = img\n    return images, labels\n\ndef rotate_image(img, angle_deg):\n    \"\"\"\n    Rotate the image by angle degrees (must be in {90,180,270}) using np.rot90.\n    \"\"\"\n    if angle_deg % 90 != 0 or angle_deg == 0:\n        # Only support non-zero multiples of 90 as per augmentation set; skip 0 in augmentation\n        raise ValueError(\"Only 90, 180, 270 degrees supported for augmentation\")\n    k = (angle_deg // 90) % 4\n    return np.rot90(img, k)\n\ndef augment_images(images, labels, angles):\n    \"\"\"\n    For each image, return list of (original + rotated versions) according to angles.\n    angles: list of integers from {90, 180, 270}. Original is always included when leak=True;\n            for training-only augmentation, we'll explicitly include original.\n    \"\"\"\n    augmented_imgs = []\n    augmented_labels = []\n    for img, y in zip(images, labels):\n        # include original\n        augmented_imgs.append(img)\n        augmented_labels.append(y)\n        for ang in angles:\n            aug = rotate_image(img, ang)\n            augmented_imgs.append(aug)\n            augmented_labels.append(y)\n    return np.array(augmented_imgs), np.array(augmented_labels)\n\ndef kfold_indices(n, k, seed):\n    \"\"\"\n    Return list of k folds, each a numpy array of indices.\n    Shuffle indices using seed, then split into k contiguous folds as evenly as possible.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    indices = np.arange(n, dtype=np.int64)\n    rng.shuffle(indices)\n    folds = []\n    fold_sizes = np.full(k, n // k, dtype=int)\n    fold_sizes[:(n % k)] += 1\n    start = 0\n    for fs in fold_sizes:\n        end = start + fs\n        folds.append(indices[start:end])\n        start = end\n    return folds\n\ndef compute_centroids(X_train, y_train):\n    \"\"\"\n    Compute class centroids for labels 0 and 1.\n    X_train: (m, d), y_train: (m,)\n    Returns mu0, mu1 as (d,) arrays.\n    \"\"\"\n    # In case a class is missing, handle by using zeros to avoid crash\n    c0 = (y_train == 0)\n    c1 = (y_train == 1)\n    if not np.any(c0) or not np.any(c1):\n        # If missing a class, return centroids computed from available class and zeros for missing\n        mu0 = X_train[c0].mean(axis=0) if np.any(c0) else np.zeros(X_train.shape[1], dtype=np.float64)\n        mu1 = X_train[c1].mean(axis=0) if np.any(c1) else np.zeros(X_train.shape[1], dtype=np.float64)\n        return mu0, mu1\n    mu0 = X_train[c0].mean(axis=0)\n    mu1 = X_train[c1].mean(axis=0)\n    return mu0, mu1\n\ndef predict_nearest_centroid(X_val, mu0, mu1):\n    \"\"\"\n    Predict labels for validation set X_val given centroids mu0 and mu1.\n    \"\"\"\n    # Compute squared distances to centroids\n    d0 = np.sum((X_val - mu0) ** 2, axis=1)\n    d1 = np.sum((X_val - mu1) ** 2, axis=1)\n    return (d1 < d0).astype(np.int64)  # predict 1 if distance to mu1 is smaller, else 0\n\ndef cross_val_accuracy_proper(images, labels, k, angles, seed):\n    \"\"\"\n    Proper augmentation: augment only training folds; validate on original base images.\n    \"\"\"\n    n = images.shape[0]\n    folds = kfold_indices(n, k, seed)\n    h, w = images.shape[1], images.shape[2]\n    accs = []\n    for fold in folds:\n        val_idx = fold\n        train_idx = np.setdiff1d(np.arange(n, dtype=np.int64), val_idx, assume_unique=True)\n        # Training data: original + augment rotations for training indices\n        X_train_list = []\n        y_train_list = []\n        for idx in train_idx:\n            img = images[idx]\n            y = labels[idx]\n            # include original\n            X_train_list.append(img.reshape(h * w))\n            y_train_list.append(y)\n            # include augmentations\n            for ang in angles:\n                aug = rotate_image(img, ang)\n                X_train_list.append(aug.reshape(h * w))\n                y_train_list.append(y)\n        X_train = np.stack(X_train_list, axis=0)\n        y_train = np.array(y_train_list, dtype=np.int64)\n        mu0, mu1 = compute_centroids(X_train, y_train)\n        # Validation data: original base images only\n        X_val = images[val_idx].reshape(len(val_idx), h * w)\n        y_val = labels[val_idx]\n        y_pred = predict_nearest_centroid(X_val, mu0, mu1)\n        acc = np.mean(y_pred == y_val)\n        accs.append(acc)\n    return float(np.mean(accs))\n\ndef cross_val_accuracy_leak(images, labels, k, angles, seed):\n    \"\"\"\n    Leakage augmentation: augment entire dataset first (include originals and rotations),\n    then perform k-fold CV on the augmented dataset.\n    \"\"\"\n    # Augment entire dataset: originals + rotations in angles\n    aug_imgs, aug_labels = augment_images(images, labels, angles)\n    n_aug = aug_imgs.shape[0]\n    folds = kfold_indices(n_aug, k, seed)\n    h, w = images.shape[1], images.shape[2]\n    accs = []\n    for fold in folds:\n        val_idx = fold\n        train_idx = np.setdiff1d(np.arange(n_aug, dtype=np.int64), val_idx, assume_unique=True)\n        X_train = aug_imgs[train_idx].reshape(len(train_idx), h * w)\n        y_train = aug_labels[train_idx]\n        mu0, mu1 = compute_centroids(X_train, y_train)\n        X_val = aug_imgs[val_idx].reshape(len(val_idx), h * w)\n        y_val = aug_labels[val_idx]\n        y_pred = predict_nearest_centroid(X_val, mu0, mu1)\n        acc = np.mean(y_pred == y_val)\n        accs.append(acc)\n    return float(np.mean(accs))\n\ndef compute_delta(n, h, w, k, angles, noise_std, seed):\n    images, labels = generate_dataset(n, h, w, noise_std, seed)\n    acc_proper = cross_val_accuracy_proper(images, labels, k, angles, seed)\n    acc_leak = cross_val_accuracy_leak(images, labels, k, angles, seed)\n    return acc_leak - acc_proper\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (n, h, w, k, angles_list, noise_std, seed)\n    test_cases = [\n        (60, 16, 16, 5, [90, 180, 270], 0.10, 42),  # Case 1: happy path\n        (60, 16, 16, 5, [],              0.10, 43),  # Case 2: boundary, no augmentation\n        (60, 16, 16, 2, [90, 180, 270],  0.10, 44),  # Case 3: few folds\n        (60, 16, 16, 5, [90, 180, 270],  0.50, 45),  # Case 4: high noise\n        (60, 16, 16, 10, [90],           0.10, 46),  # Case 5: single-angle augmentation\n    ]\n\n    results = []\n    for case in test_cases:\n        n, h, w, k, angles, noise_std, seed = case\n        delta = compute_delta(n, h, w, k, angles, noise_std, seed)\n        # Format with a reasonable precision\n        results.append(f\"{delta:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3134696"}, {"introduction": "Beyond its primary role in estimating generalization error, cross-validation can be ingeniously repurposed as a diagnostic tool to assess data quality. This advanced practice demonstrates how to use the stability of a model's predictions across different data splits to detect hidden issues like duplicate or near-duplicate instances. By implementing a method to measure prediction instability, you will learn how to programmatically flag problematic data points that could compromise both training and evaluation. [@problem_id:3139096]", "problem": "Given a binary classification dataset used to train a small neural network, some data instances are exact duplicates or near-duplicates of other instances with the same label. When performing $k$-fold cross-validation (CV), these duplicates or near-duplicates can cause instability in fold-wise predictions depending on whether a given test instance’s duplicates are included in the training folds. Your task is to design and implement a program that uses repeated $k$-fold cross-validation to surface such duplicate or near-duplicate instances by measuring a prediction instability statistic derived from first principles.\n\nStart from the following core definitions and principles:\n- Empirical risk minimization with binary cross-entropy: given input vectors $x \\in \\mathbb{R}^d$ and labels $y \\in \\{0,1\\}$, a model with parameters $\\theta$ maps $x$ to a predicted probability $p_\\theta(x) \\in (0,1)$ via a composition of affine maps and nonlinearities. The empirical risk over a dataset $\\{(x_i,y_i)\\}_{i=1}^n$ is\n$$\n\\mathcal{L}(\\theta) \\triangleq \\frac{1}{n} \\sum_{i=1}^n \\left[ -y_i \\log p_\\theta(x_i) - (1-y_i) \\log(1-p_\\theta(x_i)) \\right] + \\lambda \\|\\theta\\|_2^2,\n$$\nwhere $\\lambda \\ge 0$ is a regularization weight and $\\|\\cdot\\|_2$ denotes the Euclidean norm.\n- A one-hidden-layer neural network with Rectified Linear Unit (ReLU) nonlinearity and a logistic output is defined by\n$$\nh(x) = \\max(0, W_1 x + b_1), \\quad z(x) = W_2 h(x) + b_2, \\quad p_\\theta(x) = \\sigma(z(x)),\n$$\nwhere $W_1 \\in \\mathbb{R}^{d \\times m}$, $b_1 \\in \\mathbb{R}^m$, $W_2 \\in \\mathbb{R}^{m \\times 1}$, $b_2 \\in \\mathbb{R}$, $m$ is the number of hidden units, and $\\sigma(u) \\triangleq \\frac{1}{1+e^{-u}}$ is the logistic sigmoid.\n- $k$-fold cross-validation partitions indices $\\{1,\\dots,n\\}$ into $k$ disjoint folds of approximately equal size. For each fold $f \\in \\{1,\\dots,k\\}$, the model is trained on the union of the other $k-1$ folds and evaluated on fold $f$. Repeating $k$-fold cross-validation with different random fold assignments produces a distribution of predictions for each instance.\n\nDefine a neighbor relation to capture duplicates or near-duplicates as follows. Let a distance threshold $\\epsilon > 0$ and an indicator function\n$$\n\\mathbb{I}_\\epsilon(i,j) \\triangleq \\begin{cases}\n1, & \\text{if } \\|x_i - x_j\\|_2 \\le \\epsilon \\text{ and } y_i = y_j, \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n$$\nFor each instance $i$, let $N_\\epsilon(i) \\triangleq \\{ j \\ne i \\mid \\mathbb{I}_\\epsilon(i,j) = 1\\}$ be the set of its same-label neighbors within distance $\\epsilon$.\n\nConsider repeated $k$-fold cross-validation with $R$ independent random partitions. During each train-test split, for every test instance $i$, record whether its neighbor set $N_\\epsilon(i)$ intersects the training indices (i.e., whether any neighbor of $i$ is present in the training set). This yields two collections of predicted probabilities for instance $i$: the set\n$$\n\\mathcal{P}_\\mathrm{present}(i) \\triangleq \\{ p_\\theta^{(r,f)}(x_i) \\mid \\text{in repetition } r \\text{ and fold } f, \\ N_\\epsilon(i) \\cap \\text{train} \\ne \\emptyset \\}\n$$\nand the set\n$$\n\\mathcal{P}_\\mathrm{absent}(i) \\triangleq \\{ p_\\theta^{(r,f)}(x_i) \\mid \\text{in repetition } r \\text{ and fold } f, \\ N_\\epsilon(i) \\cap \\text{train} = \\emptyset \\},\n$$\nwhere $p_\\theta^{(r,f)}(x_i)$ denotes the model’s predicted probability for $x_i$ when trained on the training data of repetition $r$ and fold $f$.\n\nDefine the instability statistic for instance $i$ as\n$$\nD(i) \\triangleq \\begin{cases}\n\\left| \\overline{p}_\\mathrm{present}(i) - \\overline{p}_\\mathrm{absent}(i) \\right|, & \\text{if } |\\mathcal{P}_\\mathrm{present}(i)| \\ge 1 \\text{ and } |\\mathcal{P}_\\mathrm{absent}(i)| \\ge 1, \\\\\n0, & \\text{otherwise},\n\\end{cases}\n$$\nwhere $\\overline{p}_\\mathrm{present}(i)$ and $\\overline{p}_\\mathrm{absent}(i)$ are the arithmetic means of the respective sets. An instance is flagged as a suspected duplicate or near-duplicate if $D(i) \\ge \\tau$ for a chosen threshold $\\tau > 0$.\n\nImplement the above procedure with the following fixed design choices to ensure scientific realism and reproducibility:\n- Use a one-hidden-layer neural network as defined above with $m = 8$ hidden units, trained by full-batch gradient descent for a fixed number of epochs with learning rate $0.2$ and $\\ell_2$ regularization weight $\\lambda = 10^{-3}$. Use a fixed random seed for parameter initialization to ensure deterministic behavior of the training algorithm across runs given the same data.\n- Generate synthetic datasets for the test suite using independent fixed seeds for the data generation, consistent Gaussian clusters, and explicit duplicate construction as described below.\n\nTest Suite (three cases), each producing a list of flagged indices as integers:\n- Case $1$ (happy path with some duplicates and near-duplicates). Data generation:\n  - Draw $12$ points for label $0$ from a Gaussian with mean $(-2,-2)$ and standard deviation $0.6$ in each coordinate.\n  - Draw $12$ points for label $1$ from a Gaussian with mean $(2,2)$ and standard deviation $0.6$ in each coordinate.\n  - Create $5$ additional points as duplicates or near-duplicates by selecting $3$ points from label $1$ and $2$ points from label $0$, and perturbing each selected point by adding Gaussian noise with standard deviation $0.05$ while keeping the same label.\n  - Parameters: $k = 5$, $R = 12$, $\\epsilon = 0.25$, $\\tau = 0.10$.\n- Case $2$ (edge case with no duplicates). Data generation:\n  - Draw $10$ points for label $0$ from a Gaussian with mean $(-3,-3)$ and standard deviation $0.9$.\n  - Draw $10$ points for label $1$ from a Gaussian with mean $(3,3)$ and standard deviation $0.9$.\n  - Parameters: $k = 5$, $R = 12$, $\\epsilon = 0.15$, $\\tau = 0.10$.\n- Case $3$ (boundary case where every point is duplicated). Data generation:\n  - Draw $6$ points for label $0$ from a Gaussian with mean $(-1.5,-1.5)$ and standard deviation $0.5$.\n  - Draw $6$ points for label $1$ from a Gaussian with mean $(1.5,1.5)$ and standard deviation $0.5$.\n  - For every point, add one duplicate by perturbing with Gaussian noise with standard deviation $0.05$, preserving labels.\n  - Parameters: $k = 4$, $R = 12$, $\\epsilon = 0.25$, $\\tau = 0.07$.\n\nRequired Final Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list of three lists, one per case, enclosed in square brackets, with no spaces. For example, output of the form\n$$\n\\texttt{[[i_1,i_2,\\dots],[j_1,j_2,\\dots],[\\ell_1,\\ell_2,\\dots]]}\n$$\nwhere each inner list contains the integer indices of the flagged instances for that case, in ascending order by index.\n\nAngles and physical units are not involved; there is no unit specification. All threshold comparisons (e.g., $D(i) \\ge \\tau$) are performed using real numbers in decimal representation.", "solution": "The problem requires the design and implementation of a procedure to identify duplicate or near-duplicate data instances by measuring prediction instability under repeated $k$-fold cross-validation. The solution will be presented by first formalizing the algorithmic procedure, then detailing the neural network model and its training, and finally specifying the implementation parameters.\n\n### 1. Algorithmic Procedure\n\nThe core of the method is to quantify how the prediction for a given data instance $x_i$ changes depending on whether a very similar instance (a \"neighbor\") is included in the training set. A large change is indicative of the model \"memorizing\" the neighbor, a behavior characteristic of overfitting to redundant data points.\n\nThe overall procedure is as follows:\n\n**Step 1: Data Generation and Neighbor Identification**\nFor each test case, a synthetic dataset $(X, y)$ consisting of $n$ instances is generated according to the specified parameters. $X$ is a matrix of feature vectors $\\{x_i\\}_{i=1}^n$ and $y$ is a vector of binary labels $\\{y_i\\}_{i=1}^n$.\n\nFollowing data generation, we pre-compute the neighbor set $N_\\epsilon(i)$ for each instance $i \\in \\{1, \\dots, n\\}$. As defined in the problem, this set contains the indices of all other same-label instances within a Euclidean distance of $\\epsilon$:\n$$ N_\\epsilon(i) = \\{ j \\ne i \\mid \\|x_i - x_j\\|_2 \\le \\epsilon \\text{ and } y_i = y_j \\} $$\nThis computation is performed once for the entire dataset before commencing the cross-validation analysis.\n\n**Step 2: Repeated $k$-Fold Cross-Validation and Prediction Collection**\nThe procedure involves $R$ independent repetitions of $k$-fold cross-validation. For each instance $i$, we maintain two lists to store its out-of-sample predictions: $\\mathcal{P}_\\mathrm{present}(i)$ and $\\mathcal{P}_\\mathrm{absent}(i)$.\n\nThe process is iterated for each repetition $r \\in \\{1, \\dots, R\\}$:\n1.  A new random permutation of the data indices $\\{1, \\dots, n\\}$ is generated. This ensures that the fold compositions are different in each repetition.\n2.  The permuted indices are partitioned into $k$ disjoint folds, $F_1, \\dots, F_k$.\n3.  For each fold $f \\in \\{1, \\dots, k\\}$:\n    a. The fold $F_f$ is designated as the test set. The union of the remaining $k-1$ folds constitutes the training set, let's call its index set $T_f$.\n    b. A one-hidden-layer neural network is initialized with a fixed random seed (to ensure identical initial parameters for every training run) and trained on the training set data $\\{(x_j, y_j) \\mid j \\in T_f\\}$.\n    c. For each instance $i$ in the test fold $F_f$, the trained model is used to compute the predicted probability $p_\\theta(x_i)$.\n    d. We check if any of instance $i$'s neighbors are in the training set, i.e., if $N_\\epsilon(i) \\cap T_f \\ne \\emptyset$.\n    e. If the intersection is non-empty, the prediction $p_\\theta(x_i)$ is appended to the list $\\mathcal{P}_\\mathrm{present}(i)$.\n    f. Otherwise (if the intersection is empty), the prediction $p_\\theta(x_i)$ is appended to the list $\\mathcal{P}_\\mathrm{absent}(i)$.\n\nAfter completing all $R$ repetitions and all $k$ folds within each, we will have collected up to $R$ out-of-sample predictions for each instance $i$, categorized based on the presence or absence of its neighbors in the corresponding training sets.\n\n**Step 3: Instability Statistic Calculation and Flagging**\nFor each instance $i$, the instability statistic $D(i)$ is computed. This statistic measures the absolute difference between the average predictions across the two conditions:\n$$ D(i) = \\begin{cases} \\left| \\overline{p}_\\mathrm{present}(i) - \\overline{p}_\\mathrm{absent}(i) \\right|, & \\text{if } |\\mathcal{P}_\\mathrm{present}(i)| \\ge 1 \\text{ and } |\\mathcal{P}_\\mathrm{absent}(i)| \\ge 1 \\\\ 0, & \\text{otherwise} \\end{cases} $$\nwhere $\\overline{p}_\\mathrm{present}(i)$ and $\\overline{p}_\\mathrm{absent}(i)$ are the arithmetic means of the probabilities in the sets $\\mathcal{P}_\\mathrm{present}(i)$ and $\\mathcal{P}_\\mathrm{absent}(i)$, respectively. The condition ensures that the statistic is only computed when we have at least one prediction under each scenario, making the comparison meaningful.\n\nFinally, an instance $i$ is flagged as a suspected duplicate or near-duplicate if its instability statistic meets or exceeds a given threshold $\\tau$:\n$$ \\text{Flag}(i) \\iff D(i) \\ge \\tau $$\n\n### 2. Neural Network Model and Training\n\nThe analysis employs a one-hidden-layer feedforward neural network.\n\n**Model Architecture:**\nGiven an input feature vector $x \\in \\mathbb{R}^d$, the model computes a predicted probability $p_\\theta(x)$ as follows:\n1.  Hidden layer activation: $h(x) = \\max(0, W_1 x + b_1)$, where $W_1 \\in \\mathbb{R}^{m \\times d}$ and $b_1 \\in \\mathbb{R}^m$. The function $\\max(0, \\cdot)$ is the Rectified Linear Unit (ReLU). We use $m=8$ hidden units.\n2.  Output layer logit: $z(x) = W_2 h(x) + b_2$, where $W_2 \\in \\mathbb{R}^{1 \\times m}$ and $b_2 \\in \\mathbb{R}$.\n3.  Predicted probability: $p_\\theta(x) = \\sigma(z(x))$, where $\\sigma(u) = (1+e^{-u})^{-1}$ is the logistic sigmoid function.\nThe model parameters are $\\theta = \\{W_1, b_1, W_2, b_2\\}$. Note that for a batch of $N$ inputs arranged in a matrix $X \\in \\mathbb{R}^{N \\times d}$, this structure corresponds to $H = \\max(0, X W_1' + b_1')$ where $W_1' \\in \\mathbb{R}^{d \\times m}$ and $b_1' \\in \\mathbb{R}^{1 \\times m}$, and so forth. We will adhere to this standard batch-processing convention in implementation.\n\n**Training Procedure:**\nThe model is trained by minimizing the empirical risk, which is the binary cross-entropy loss with $\\ell_2$ regularization, using full-batch gradient descent. For a training set of size $n_{\\text{train}}$, the loss is:\n$$ \\mathcal{L}(\\theta) = \\frac{1}{n_{\\text{train}}} \\sum_{i=1}^{n_{\\text{train}}} \\left[ -y_i \\log p_\\theta(x_i) - (1-y_i) \\log(1-p_\\theta(x_i)) \\right] + \\lambda \\left( \\|W_1\\|_F^2 + \\|b_1\\|_2^2 + \\|W_2\\|_F^2 + b_2^2 \\right) $$\nwhere $\\|\\cdot\\|_F$ is the Frobenius norm for matrices and $\\|\\cdot\\|_2$ is the standard Euclidean norm for vectors. The regularization weight is fixed at $\\lambda = 10^{-3}$.\n\nThe gradients $\\nabla_\\theta \\mathcal{L}$ are computed using the backpropagation algorithm. The parameters are then updated iteratively for a fixed number of epochs:\n$$ \\theta^{(t+1)} \\leftarrow \\theta^{(t)} - \\eta \\nabla_\\theta \\mathcal{L}(\\theta^{(t)}) $$\nwhere $\\eta=0.2$ is the learning rate. We will use a fixed number of $200$ epochs, which is sufficient for convergence on the small synthetic datasets.\n\n### 3. Implementation and Parameter Choices\n\nTo ensure reproducibility, the implementation adheres strictly to the specified parameters.\n- **Reproducibility:** Separate fixed seeds are used for generating the data for each test case, for initializing the neural network parameters, and for the random shuffling in each cross-validation repetition.\n- **Test Cases:** The three test cases are implemented as described, with specified parameters for data generation, cross-validation ($k, R$), and instability analysis ($\\epsilon, \\tau$).\n- **Algorithm Execution:** For each case, the full procedure is executed: data generation, neighbor finding, repeated CV with model training and prediction collection, instability calculation, and final flagging of indices based on the threshold $\\tau$. The resulting lists of flagged indices are collected for the final output.", "answer": "```python\nimport numpy as np\nfrom scipy.special import expit as sigmoid\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the result.\n    \"\"\"\n\n    class NeuralNetwork:\n        \"\"\"\n        A one-hidden-layer neural network for binary classification.\n        \"\"\"\n        def __init__(self, input_dim, hidden_dim, lambda_reg, seed):\n            self.input_dim = input_dim\n            self.hidden_dim = hidden_dim\n            self.lambda_reg = lambda_reg\n            \n            # Initialize parameters with a fixed seed for reproducibility.\n            rng = np.random.default_rng(seed)\n            self.W1 = rng.standard_normal(size=(self.input_dim, self.hidden_dim)) * 0.1\n            self.b1 = np.zeros((1, self.hidden_dim))\n            self.W2 = rng.standard_normal(size=(self.hidden_dim, 1)) * 0.1\n            self.b2 = np.zeros((1, 1))\n\n        def _forward(self, X):\n            self.Z1 = X @ self.W1 + self.b1\n            self.A1 = np.maximum(0, self.Z1)  # ReLU activation\n            self.Z2 = self.A1 @ self.W2 + self.b2\n            P = sigmoid(self.Z2)\n            return P\n\n        def _compute_loss(self, Y, P):\n            n_samples = Y.shape[0]\n            bce_loss = -np.mean(Y * np.log(P) + (1 - Y) * np.log(1 - P))\n            reg_loss = self.lambda_reg * (np.sum(np.square(self.W1)) + np.sum(np.square(self.b1)) +\n                                          np.sum(np.square(self.W2)) + np.sum(np.square(self.b2)))\n            return bce_loss + reg_loss\n\n        def _backward(self, X, Y, P):\n            n_samples = X.shape[0]\n            \n            # Gradient of loss w.r.t. Z2 (pre-sigmoid output)\n            dZ2 = (P - Y) / n_samples\n            \n            # Gradients for W2 and b2\n            self.dW2 = self.A1.T @ dZ2 + 2 * self.lambda_reg * self.W2\n            self.db2 = np.sum(dZ2, axis=0, keepdims=True) + 2 * self.lambda_reg * self.b2\n            \n            # Backpropagate to hidden layer\n            dA1 = dZ2 @ self.W2.T\n            # Gradient of loss w.r.t Z1 (pre-ReLU)\n            dZ1 = dA1 * (self.Z1 > 0)\n            \n            # Gradients for W1 and b1\n            self.dW1 = X.T @ dZ1 + 2 * self.lambda_reg * self.W1\n            self.db1 = np.sum(dZ1, axis=0, keepdims=True) + 2 * self.lambda_reg * self.b1\n\n        def train(self, X_train, Y_train, epochs, learning_rate):\n            y_train_col = Y_train.reshape(-1, 1)\n            for _ in range(epochs):\n                P = self._forward(X_train)\n                self._backward(X_train, y_train_col, P)\n                \n                # Update parameters\n                self.W1 -= learning_rate * self.dW1\n                self.b1 -= learning_rate * self.db1\n                self.W2 -= learning_rate * self.dW2\n                self.b2 -= learning_rate * self.db2\n\n        def predict_proba(self, X):\n            return self._forward(X)\n\n    def generate_data(case_id, params):\n        \"\"\"Generates synthetic data for a given test case.\"\"\"\n        rng = np.random.default_rng(case_id)\n        \n        if case_id == 0: # Case 1\n            X0 = rng.multivariate_normal(mean=[-2, -2], cov=np.eye(2)*0.6**2, size=12)\n            Y0 = np.zeros(12)\n            X1 = rng.multivariate_normal(mean=[2, 2], cov=np.eye(2)*0.6**2, size=12)\n            Y1 = np.ones(12)\n            X = np.vstack((X0, X1))\n            Y = np.hstack((Y0, Y1))\n            \n            dupe_indices_0 = rng.choice(np.where(Y == 0)[0], size=2, replace=False)\n            dupe_indices_1 = rng.choice(np.where(Y == 1)[0], size=3, replace=False)\n            \n            X_dupes_0 = X[dupe_indices_0] + rng.normal(scale=0.05, size=(2, 2))\n            Y_dupes_0 = np.zeros(2)\n            X_dupes_1 = X[dupe_indices_1] + rng.normal(scale=0.05, size=(3, 2))\n            Y_dupes_1 = np.ones(3)\n            \n            X = np.vstack((X, X_dupes_0, X_dupes_1))\n            Y = np.hstack((Y, Y_dupes_0, Y_dupes_1))\n        \n        elif case_id == 1: # Case 2\n            X0 = rng.multivariate_normal(mean=[-3, -3], cov=np.eye(2)*0.9**2, size=10)\n            Y0 = np.zeros(10)\n            X1 = rng.multivariate_normal(mean=[3, 3], cov=np.eye(2)*0.9**2, size=10)\n            Y1 = np.ones(10)\n            X = np.vstack((X0, X1))\n            Y = np.hstack((Y0, Y1))\n\n        elif case_id == 2: # Case 3\n            X0 = rng.multivariate_normal(mean=[-1.5, -1.5], cov=np.eye(2)*0.5**2, size=6)\n            Y0 = np.zeros(6)\n            X1 = rng.multivariate_normal(mean=[1.5, 1.5], cov=np.eye(2)*0.5**2, size=6)\n            Y1 = np.ones(6)\n            X_orig = np.vstack((X0, X1))\n            Y_orig = np.hstack((Y0, Y1))\n            \n            X_dupes = X_orig + rng.normal(scale=0.05, size=X_orig.shape)\n            Y_dupes = Y_orig\n            \n            X = np.vstack((X_orig, X_dupes))\n            Y = np.hstack((Y_orig, Y_dupes))\n            \n        return X, Y\n\n    def run_analysis(case_id, params):\n        \"\"\"Runs the full analysis for a single case.\"\"\"\n        k = params['k']\n        R = params['R']\n        epsilon = params['epsilon']\n        tau = params['tau']\n        hidden_dim = 8\n        learning_rate = 0.2\n        lambda_reg = 1e-3\n        epochs = 200\n        param_init_seed = 42\n\n        X, Y = generate_data(case_id, params)\n        n_samples, input_dim = X.shape\n\n        # Step 1: Find neighbor sets for each instance\n        neighbor_sets = [set() for _ in range(n_samples)]\n        for i in range(n_samples):\n            for j in range(n_samples):\n                if i == j: continue\n                if Y[i] == Y[j]:\n                    dist = np.linalg.norm(X[i] - X[j])\n                    if dist <= epsilon:\n                        neighbor_sets[i].add(j)\n        \n        # Step 2: Repeated k-fold CV\n        preds_present = [[] for _ in range(n_samples)]\n        preds_absent = [[] for _ in range(n_samples)]\n\n        for r in range(R):\n            # Use repetition number as seed for shuffling\n            rep_rng = np.random.default_rng(r)\n            indices = rep_rng.permutation(n_samples)\n            folds = np.array_split(indices, k)\n\n            for f in range(k):\n                test_indices = folds[f]\n                train_indices_list = [folds[i] for i in range(k) if i != f]\n                train_indices = np.concatenate(train_indices_list)\n\n                train_indices_set = set(train_indices)\n                \n                X_train, Y_train = X[train_indices], Y[train_indices]\n                X_test = X[test_indices]\n                \n                # Train model\n                model = NeuralNetwork(input_dim, hidden_dim, lambda_reg, param_init_seed)\n                model.train(X_train, Y_train, epochs, learning_rate)\n\n                # Predict on test set and categorize predictions\n                predictions = model.predict_proba(X_test)\n                \n                for idx_in_fold, i in enumerate(test_indices):\n                    p_i = predictions[idx_in_fold][0]\n                    # Check if any neighbor is in the training set\n                    if not neighbor_sets[i].isdisjoint(train_indices_set):\n                        preds_present[i].append(p_i)\n                    else:\n                        preds_absent[i].append(p_i)\n        \n        # Step 3: Calculate instability and flag instances\n        flagged_indices = []\n        for i in range(n_samples):\n            D_i = 0\n            if preds_present[i] and preds_absent[i]:\n                mean_present = np.mean(preds_present[i])\n                mean_absent = np.mean(preds_absent[i])\n                D_i = np.abs(mean_present - mean_absent)\n            \n            if D_i >= tau:\n                flagged_indices.append(i)\n        \n        return sorted(flagged_indices)\n\n    test_cases = [\n        {'k': 5, 'R': 12, 'epsilon': 0.25, 'tau': 0.10},  # Case 1\n        {'k': 5, 'R': 12, 'epsilon': 0.15, 'tau': 0.10},  # Case 2\n        {'k': 4, 'R': 12, 'epsilon': 0.25, 'tau': 0.07},  # Case 3\n    ]\n\n    results = []\n    for i, params in enumerate(test_cases):\n        flagged = run_analysis(i, params)\n        results.append(flagged)\n\n    # Format the final output string\n    output_str = '[' + ','.join([str(lst).replace(' ', '') for lst in results]) + ']'\n    print(output_str)\n\nsolve()\n```", "id": "3139096"}]}