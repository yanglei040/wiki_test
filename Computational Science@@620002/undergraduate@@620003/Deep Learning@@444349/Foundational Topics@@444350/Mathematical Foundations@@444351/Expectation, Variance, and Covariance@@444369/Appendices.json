{"hands_on_practices": [{"introduction": "The training of deep neural networks relies on stochastic gradients, which are noisy estimates of the true gradient computed on mini-batches of data. This exercise treats these gradients as random variables and uses covariance to explore their statistical relationship across different layers. By analyzing a simple two-layer linear network, you will derive how the gradient noise for different parameters can be positively correlated (aligned), negatively correlated (conflicting), or uncorrelated, providing fundamental insights into the complex dynamics of deep learning optimization [@problem_id:3123312].", "problem": "Consider a scalar two-layer linear network used in deep learning with input $x \\in \\mathbb{R}$, parameters $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$, and output $\\hat{y} = b a x$. The training label is generated by the random linear model $y = \\theta x + \\varepsilon$, where $x \\sim \\mathcal{N}(0, \\sigma_x^2)$ and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^2)$ are independent zero-mean Gaussian random variables. The loss per sample is the Mean Squared Error (MSE) $L = \\frac{1}{2}(\\hat{y} - y)^2$. For a single random sample, define the stochastic gradients $g^{(1)} = \\frac{\\partial L}{\\partial a}$ and $g^{(2)} = \\frac{\\partial L}{\\partial b}$. These gradients are random variables due to the randomness in $x$ and $\\varepsilon$.\n\nYour tasks are:\n1. Starting strictly from the fundamental definitions of expectation $\\mathbb{E}$, variance $\\operatorname{Var}$, and covariance $\\operatorname{Cov}$, together with the independence of $x$ and $\\varepsilon$, derive closed-form symbolic expressions for $\\mathbb{E}[g^{(1)}]$, $\\mathbb{E}[g^{(2)}]$, $\\operatorname{Var}(g^{(1)})$, $\\operatorname{Var}(g^{(2)})$, and $\\operatorname{Cov}(g^{(1)}, g^{(2)})$ in terms of $a$, $b$, $\\theta$, $\\sigma_x^2$, and $\\sigma_{\\varepsilon}^2$. Then derive the correlation coefficient $\\rho = \\frac{\\operatorname{Cov}(g^{(1)}, g^{(2)})}{\\sqrt{\\operatorname{Var}(g^{(1)}) \\operatorname{Var}(g^{(2)})}}$.\n2. Implement a program that uses these derived formulas to compute, for each provided test case, the numerical values of $\\operatorname{Cov}(g^{(1)}, g^{(2)})$, $\\operatorname{Var}(g^{(1)})$, $\\operatorname{Var}(g^{(2)})$, and $\\rho$. Additionally, classify alignment versus conflict of gradient noise across layers via the sign of covariance: output $+1$ if $\\operatorname{Cov}(g^{(1)}, g^{(2)}) > 0$ (alignment), $-1$ if $\\operatorname{Cov}(g^{(1)}, g^{(2)})  0$ (conflict), and $0$ if $\\operatorname{Cov}(g^{(1)}, g^{(2)}) = 0$ (neutral). When either $\\operatorname{Var}(g^{(1)}) = 0$ or $\\operatorname{Var}(g^{(2)}) = 0$, define $\\rho$ by convention to be $0$.\n\nUse only the given model and probabilistic assumptions. Do not introduce any additional shortcuts or formulas in the derivation beyond well-tested facts about independent zero-mean Gaussian random variables. Express all mathematical expressions using LaTeX. No physical units or angle units are involved. Results should be numerical floats or integers.\n\nTest suite (each case is a tuple $(a, b, \\theta, \\sigma_x^2, \\sigma_{\\varepsilon}^2)$):\n- Case $1$: $(1.0, 2.0, 1.2, 1.0, 0.5)$\n- Case $2$: $(1.0, -2.0, 1.2, 1.0, 0.5)$\n- Case $3$: $(0.0, 1.5, 0.8, 1.0, 0.3)$\n- Case $4$: $(1.0, 1.5, 1.5, 1.0, 0.0)$\n- Case $5$: $(-0.5, -0.25, 0.1, 2.0, 5.0)$\n- Case $6$: $(0.3, 0.6, 0.1, 0.01, 1.0)$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with no spaces, where each test case result is the list $[\\operatorname{Cov}(g^{(1)}, g^{(2)}), \\operatorname{Var}(g^{(1)}), \\operatorname{Var}(g^{(2)}), \\rho, \\text{sign}]$. For example, the output must look like $[[c_{1},v_{1}^{(1)},v_{1}^{(2)},\\rho_{1},s_{1}],[c_{2},v_{2}^{(1)},v_{2}^{(2)},\\rho_{2},s_{2}],\\dots]$, where each $c_i$, $v_i^{(1)}$, $v_i^{(2)}$, and $\\rho_i$ are floats and each $s_i$ is an integer in $\\{-1, 0, 1\\}$.", "solution": "The problem is valid as it is scientifically grounded in probability theory and calculus, is well-posed with a unique and meaningful solution, and is expressed in objective, formal language. It is a standard theoretical exercise in analyzing the statistics of stochastic gradients in a simplified neural network model.\n\nWe begin by formally deriving the required statistical quantities.\n\n**1. Problem Setup and Gradient Derivation**\n\nThe model output is $\\hat{y} = b a x$, and the target label is generated by $y = \\theta x + \\varepsilon$. The loss for a single sample is the Mean Squared Error (MSE):\n$$L = \\frac{1}{2}(\\hat{y} - y)^2 = \\frac{1}{2}(bax - (\\theta x + \\varepsilon))^2$$\nWe can rewrite the term inside the square as:\n$$\\hat{y} - y = (ab - \\theta)x - \\varepsilon$$\nThe stochastic gradients with respect to the parameters $a$ and $b$ are the partial derivatives of the loss $L$.\n\nThe gradient with respect to $a$, denoted $g^{(1)}$, is:\n$$g^{(1)} = \\frac{\\partial L}{\\partial a} = \\frac{\\partial}{\\partial a} \\left[ \\frac{1}{2}((ab - \\theta)x - \\varepsilon)^2 \\right]$$\nUsing the chain rule:\n$$g^{(1)} = ((ab - \\theta)x - \\varepsilon) \\cdot \\frac{\\partial}{\\partial a}((ab - \\theta)x - \\varepsilon) = ((ab - \\theta)x - \\varepsilon) \\cdot (bx)$$\n$$g^{(1)} = (ab - \\theta)bx^2 - b\\varepsilon x$$\n\nThe gradient with respect to $b$, denoted $g^{(2)}$, is:\n$$g^{(2)} = \\frac{\\partial L}{\\partial b} = \\frac{\\partial}{\\partial b} \\left[ \\frac{1}{2}((ab - \\theta)x - \\varepsilon)^2 \\right]$$\nUsing the chain rule:\n$$g^{(2)} = ((ab - \\theta)x - \\varepsilon) \\cdot \\frac{\\partial}{\\partial b}((ab - \\theta)x - \\varepsilon) = ((ab - \\theta)x - \\varepsilon) \\cdot (ax)$$\n$$g^{(2)} = (ab - \\theta)ax^2 - a\\varepsilon x$$\n\n**2. Properties of Random Variables**\n\nThe random variables are $x \\sim \\mathcal{N}(0, \\sigma_x^2)$ and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\varepsilon}^2)$, and they are independent. We will use the following standard moments of zero-mean Gaussian distributions:\n- $\\mathbb{E}[x] = 0$, $\\mathbb{E}[\\varepsilon] = 0$\n- $\\mathbb{E}[x^2] = \\operatorname{Var}(x) + (\\mathbb{E}[x])^2 = \\sigma_x^2$\n- $\\mathbb{E}[\\varepsilon^2] = \\operatorname{Var}(\\varepsilon) + (\\mathbb{E}[\\varepsilon])^2 = \\sigma_{\\varepsilon}^2$\n- $\\mathbb{E}[x^3] = 0$ (all odd moments are zero)\n- $\\mathbb{E}[x^4] = 3(\\sigma_x^2)^2 = 3\\sigma_x^4$\n\nDue to independence, the expectation of a product of functions of $x$ and $\\varepsilon$ is the product of their individual expectations. For example, $\\mathbb{E}[x^k \\varepsilon^m] = \\mathbb{E}[x^k]\\mathbb{E}[\\varepsilon^m]$.\n- $\\mathbb{E}[x\\varepsilon] = \\mathbb{E}[x]\\mathbb{E}[\\varepsilon] = 0 \\cdot 0 = 0$\n- $\\mathbb{E}[x^2\\varepsilon^2] = \\mathbb{E}[x^2]\\mathbb{E}[\\varepsilon^2] = \\sigma_x^2 \\sigma_{\\varepsilon}^2$\n- $\\mathbb{E}[x^3\\varepsilon] = \\mathbb{E}[x^3]\\mathbb{E}[\\varepsilon] = 0 \\cdot 0 = 0$\n\n**3. Derivation of Expectations**\n\nUsing the linearity of the expectation operator $\\mathbb{E}$:\n$$\\mathbb{E}[g^{(1)}] = \\mathbb{E}[(ab - \\theta)bx^2 - b\\varepsilon x] = (ab - \\theta)b\\mathbb{E}[x^2] - b\\mathbb{E}[\\varepsilon x]$$\nSubstituting the moments:\n$$\\mathbb{E}[g^{(1)}] = (ab - \\theta)b\\sigma_x^2 - b(0) = (ab - \\theta)b\\sigma_x^2$$\n\nSimilarly for $g^{(2)}$:\n$$\\mathbb{E}[g^{(2)}] = \\mathbb{E}[(ab - \\theta)ax^2 - a\\varepsilon x] = (ab - \\theta)a\\mathbb{E}[x^2] - a\\mathbb{E}[\\varepsilon x]$$\n$$\\mathbb{E}[g^{(2)}] = (ab - \\theta)a\\sigma_x^2 - a(0) = (ab - \\theta)a\\sigma_x^2$$\nThese are the expected gradients, which represent the gradients of the expected loss $\\mathbb{E}[L]$.\n\n**4. Derivation of Variances and Covariance**\n\nWe use the fundamental definitions $\\operatorname{Var}(Z) = \\mathbb{E}[Z^2] - (\\mathbb{E}[Z])^2$ and $\\operatorname{Cov}(Z_1, Z_2) = \\mathbb{E}[Z_1 Z_2] - \\mathbb{E}[Z_1]\\mathbb{E}[Z_2]$.\n\nFirst, we compute the second moments of the gradients.\nFor $g^{(1)}$:\n$$(g^{(1)})^2 = ((ab - \\theta)bx^2 - b\\varepsilon x)^2 = (ab - \\theta)^2 b^2 x^4 - 2(ab - \\theta)b^2 \\varepsilon x^3 + b^2 \\varepsilon^2 x^2$$\nTaking the expectation:\n$$\\mathbb{E}[(g^{(1)})^2] = (ab - \\theta)^2 b^2 \\mathbb{E}[x^4] - 2(ab - \\theta)b^2 \\mathbb{E}[\\varepsilon x^3] + b^2 \\mathbb{E}[\\varepsilon^2 x^2]$$\nSubstituting the higher-order moments:\n$$\\mathbb{E}[(g^{(1)})^2] = (ab - \\theta)^2 b^2 (3\\sigma_x^4) - 2(ab - \\theta)b^2 (0) + b^2 (\\sigma_{\\varepsilon}^2 \\sigma_x^2)$$\n$$\\mathbb{E}[(g^{(1)})^2] = 3(ab - \\theta)^2 b^2 \\sigma_x^4 + b^2 \\sigma_{\\varepsilon}^2 \\sigma_x^2$$\nNow, we calculate the variance of $g^{(1)}$:\n$$\\operatorname{Var}(g^{(1)}) = \\mathbb{E}[(g^{(1)})^2] - (\\mathbb{E}[g^{(1)}])^2$$\n$$(\\mathbb{E}[g^{(1)}])^2 = ((ab - \\theta)b\\sigma_x^2)^2 = (ab - \\theta)^2 b^2 \\sigma_x^4$$\n$$\\operatorname{Var}(g^{(1)}) = (3(ab - \\theta)^2 b^2 \\sigma_x^4 + b^2 \\sigma_{\\varepsilon}^2 \\sigma_x^2) - (ab - \\theta)^2 b^2 \\sigma_x^4$$\n$$\\operatorname{Var}(g^{(1)}) = 2(ab - \\theta)^2 b^2 \\sigma_x^4 + b^2 \\sigma_{\\varepsilon}^2 \\sigma_x^2$$\n\nBy symmetry, the expression for $\\operatorname{Var}(g^{(2)})$ is identical with $a$ replacing the outer $b$:\n$$\\operatorname{Var}(g^{(2)}) = 2(ab - \\theta)^2 a^2 \\sigma_x^4 + a^2 \\sigma_{\\varepsilon}^2 \\sigma_x^2$$\n\nNext, for the covariance, we compute $\\mathbb{E}[g^{(1)} g^{(2)}]$:\n$$g^{(1)}g^{(2)} = ((ab - \\theta)bx^2 - b\\varepsilon x)((ab - \\theta)ax^2 - a\\varepsilon x)$$\n$$g^{(1)}g^{(2)} = (ab - \\theta)^2 ab x^4 - 2(ab - \\theta)ab \\varepsilon x^3 + ab \\varepsilon^2 x^2$$\nTaking the expectation:\n$$\\mathbb{E}[g^{(1)}g^{(2)}] = (ab - \\theta)^2 ab \\mathbb{E}[x^4] - 2(ab - \\theta)ab \\mathbb{E}[\\varepsilon x^3] + ab \\mathbb{E}[\\varepsilon^2 x^2]$$\n$$\\mathbb{E}[g^{(1)}g^{(2)}] = (ab - \\theta)^2 ab (3\\sigma_x^4) + ab (\\sigma_{\\varepsilon}^2 \\sigma_x^2) = 3(ab - \\theta)^2 ab \\sigma_x^4 + ab \\sigma_{\\varepsilon}^2 \\sigma_x^2$$\nNow, we compute the covariance:\n$$\\operatorname{Cov}(g^{(1)}, g^{(2)}) = \\mathbb{E}[g^{(1)}g^{(2)}] - \\mathbb{E}[g^{(1)}]\\mathbb{E}[g^{(2)}]$$\n$$\\mathbb{E}[g^{(1)}]\\mathbb{E}[g^{(2)}] = ((ab - \\theta)b\\sigma_x^2)((ab - \\theta)a\\sigma_x^2) = (ab - \\theta)^2 ab \\sigma_x^4$$\n$$\\operatorname{Cov}(g^{(1)}, g^{(2)}) = (3(ab - \\theta)^2 ab \\sigma_x^4 + ab \\sigma_{\\varepsilon}^2 \\sigma_x^2) - ((ab - \\theta)^2 ab \\sigma_x^4)$$\n$$\\operatorname{Cov}(g^{(1)}, g^{(2)}) = 2(ab - \\theta)^2 ab \\sigma_x^4 + ab \\sigma_{\\varepsilon}^2 \\sigma_x^2$$\n\n**5. Correlation Coefficient**\n\nLet's factor the expressions. Let $V_Z = 2(ab - \\theta)^2 \\sigma_x^4 + \\sigma_{\\varepsilon}^2 \\sigma_x^2$. This term is non-negative.\n- $\\operatorname{Var}(g^{(1)}) = b^2 V_Z$\n- $\\operatorname{Var}(g^{(2)}) = a^2 V_Z$\n- $\\operatorname{Cov}(g^{(1)}, g^{(2)}) = ab V_Z$\n\nThe correlation coefficient $\\rho$ is defined as:\n$$\\rho = \\frac{\\operatorname{Cov}(g^{(1)}, g^{(2)})}{\\sqrt{\\operatorname{Var}(g^{(1)}) \\operatorname{Var}(g^{(2)})}}$$\nSubstituting our expressions:\n$$\\rho = \\frac{ab V_Z}{\\sqrt{(b^2 V_Z) \\cdot (a^2 V_Z)}} = \\frac{ab V_Z}{\\sqrt{a^2 b^2 V_Z^2}} = \\frac{ab V_Z}{|ab| V_Z}$$\nIf $V_Z \\neq 0$ and $a,b \\neq 0$, this simplifies to:\n$$\\rho = \\frac{ab}{|ab|} = \\operatorname{sign}(ab)$$\nwhere $\\operatorname{sign}(z)$ is $1$ if $z>0$, $-1$ if $z0$, and $0$ if $z=0$. So, if $a$ and $b$ have the same sign, $\\rho=1$. If they have opposite signs, $\\rho=-1$.\n\nPer the problem statement, if either variance is zero, $\\rho$ is defined as $0$.\n$\\operatorname{Var}(g^{(1)}) = 0$ if $b=0$ or $V_Z=0$.\n$\\operatorname{Var}(g^{(2)}) = 0$ if $a=0$ or $V_Z=0$.\nSo, $\\rho=0$ if $a=0$, or $b=0$, or $V_Z=0$.\nNote that $V_Z = 0$ if and only if $\\sigma_x^2=0$, or both $\\sigma_{\\varepsilon}^2=0$ and $ab=\\theta$.\nIn all cases where one of the variances is zero, our expressions correctly give $\\operatorname{Cov}(g^{(1)}, g^{(2)})=0$, so the convention is consistent. Our general expression for $\\rho$ as $\\operatorname{sign}(ab)$ is valid when the variances are non-zero, and consistent with the $\\rho=0$ convention when $a=0$ or $b=0$.\n\n**6. Final Formulas for Implementation**\n\nLet $\\Delta = ab - \\theta$.\nLet $\\sigma_x^2$ be `sx2` and $\\sigma_{\\varepsilon}^2$ be `se2`.\nThe common factor is $V_Z = 2\\Delta^2 (\\text{sx2})^2 + \\text{se2} \\cdot \\text{sx2}$.\n- $\\operatorname{Cov}(g^{(1)}, g^{(2)}) = ab V_Z$\n- $\\operatorname{Var}(g^{(1)}) = b^2 V_Z$\n- $\\operatorname{Var}(g^{(2)}) = a^2 V_Z$\n- $\\rho$: if $\\operatorname{Var}(g^{(1)})=0$ or $\\operatorname{Var}(g^{(2)})=0$, $\\rho=0$. Otherwise, $\\rho = \\frac{\\operatorname{Cov}(g^{(1)}, g^{(2)})}{\\sqrt{\\operatorname{Var}(g^{(1)}) \\operatorname{Var}(g^{(2)})}}$.\n- sign: $\\operatorname{sign}(\\operatorname{Cov}(g^{(1)}, g^{(2)}))$, which will be $+1, -1$, or $0$.\n\nThese formulas are implemented to compute the numerical results for the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of calculating gradient statistics for a two-layer linear network.\n    \"\"\"\n    # Test suite: each case is a tuple (a, b, theta, sigma_x^2, sigma_epsilon^2)\n    test_cases = [\n        (1.0, 2.0, 1.2, 1.0, 0.5),    # Case 1\n        (1.0, -2.0, 1.2, 1.0, 0.5),   # Case 2\n        (0.0, 1.5, 0.8, 1.0, 0.3),    # Case 3\n        (1.0, 1.5, 1.5, 1.0, 0.0),    # Case 4\n        (-0.5, -0.25, 0.1, 2.0, 5.0), # Case 5\n        (0.3, 0.6, 0.1, 0.01, 1.0),   # Case 6\n    ]\n\n    results = []\n    for case in test_cases:\n        a, b, theta, sx2, se2 = case\n\n        # Let delta = ab - theta\n        delta = a * b - theta\n\n        # Common factor term V_Z = 2 * delta^2 * (sx2)^2 + se2 * sx2\n        # where V_Z = Var(((ab-theta)x - epsilon)x)\n        common_term_vz = 2 * (delta**2) * (sx2**2) + se2 * sx2\n        \n        # Cov(g1, g2) = ab * V_Z\n        cov_g1_g2 = a * b * common_term_vz\n        \n        # Var(g1) = b^2 * V_Z\n        var_g1 = b**2 * common_term_vz\n        \n        # Var(g2) = a^2 * V_Z\n        var_g2 = a**2 * common_term_vz\n\n        # Correlation coefficient rho\n        # By convention, rho = 0 if either variance is 0.\n        if var_g1 == 0 or var_g2 == 0:\n            rho = 0.0\n        else:\n            rho = cov_g1_g2 / np.sqrt(var_g1 * var_g2)\n\n        # Sign of covariance for alignment classification\n        # np.sign returns -1, 0, or 1 as a float. Convert to int.\n        sign = int(np.sign(cov_g1_g2))\n        \n        results.append([cov_g1_g2, var_g1, var_g2, rho, sign])\n\n    # Format the output string to be a list of lists with no spaces.\n    # e.g., [[c1,v11,v12,r1,s1],[c2,v21,v22,r2,s2],...]\n    result_strings = []\n    for res in results:\n        # Format each inner list to a string without spaces.\n        inner_string = \",\".join(map(str, res))\n        result_strings.append(f\"[{inner_string}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3123312"}, {"introduction": "Beyond training, statistical tools are vital for improving model performance at inference time. Test-Time Augmentation (TTA) is a popular technique for enhancing robustness by averaging a model's predictions over multiple randomly augmented versions of an input. This practice moves beyond a simple average, asking you to use the principles of variance to derive an optimal, variance-aware aggregation strategy that produces a final estimate with the minimum possible uncertainty [@problem_id:3123404].", "problem": "Consider a deterministic affine predictor in a deep learning inference context defined by $f(\\mathbf{y}) = \\mathbf{w}^\\top \\mathbf{y} + b$, where $\\mathbf{y} \\in \\mathbb{R}^d$ is an input, $\\mathbf{w} \\in \\mathbb{R}^d$ is a weight vector, and $b \\in \\mathbb{R}$ is a bias. At test time, randomized augmentations are applied to a fixed input $\\mathbf{x} \\in \\mathbb{R}^d$ via policies indexed by $i \\in \\{1,2,3\\}$ of the form $a_i(\\mathbf{x}) = \\mathbf{x} + \\boldsymbol{\\varepsilon}_i$, where $\\boldsymbol{\\varepsilon}_i$ is a zero-mean random vector with diagonal covariance matrix $\\Sigma_i \\in \\mathbb{R}^{d \\times d}$. For each policy $i$, $n_i$ independent augmentation draws are used to form a sample mean prediction for that policy. The goal is to estimate the expectation $\\mathbb{E}[f(a_i(\\mathbf{x}))]$ and the variance $\\operatorname{Var}[f(a_i(\\mathbf{x}))]$ for a given policy, and to design a variance-aware aggregation that combines the per-policy sample means into a single final estimate with minimal variance under the constraint of unbiasedness.\n\nStarting only from the core definitions of expectation, variance, and covariance, and from the independence of augmentation draws, derive the expressions required to implement:\n- A Monte Carlo (MC) estimator for $\\mathbb{E}[f(a_i(\\mathbf{x}))]$ and $\\operatorname{Var}[f(a_i(\\mathbf{x}))]$ for a given policy $i$.\n- A variance-aware aggregation rule that combines the three per-policy sample means into a single unbiased estimate of the common expectation, minimizing its variance under the assumption that all three policies share the same expectation of $f(a_i(\\mathbf{x}))$ due to $\\mathbb{E}[\\boldsymbol{\\varepsilon}_i] = \\mathbf{0}$ and identical mean input $\\mathbf{x}$. Explicitly derive the optimal weights from first principles (do not use any pre-memorized shortcut formulas), and provide the formula for the resulting minimal variance. Also derive the variance of the naive mean aggregator that simply averages all individual predictions across all samples.\n\nYour program must implement the derived formulas and produce quantitative outputs for the following test suite. In each case, the random number generator must be seeded deterministically, and the MC estimates must use the specified number of samples per policy.\n\nTest Suite:\n- Case $1$ (happy path):\n  - Dimension $d = 3$.\n  - Input $\\mathbf{x} = (0.8, -1.0, 0.5)$.\n  - Weights $\\mathbf{w} = (1.2, -0.7, 0.3)$.\n  - Bias $b = 0.1$.\n  - Policies:\n    - Policy $1$: $\\Sigma_1 = \\operatorname{diag}(0.04, 0.01, 0.09)$, $n_1 = 400$.\n    - Policy $2$: $\\Sigma_2 = \\operatorname{diag}(0.25, 0.16, 0.36)$, $n_2 = 300$.\n    - Policy $3$: $\\Sigma_3 = \\operatorname{diag}(1.00, 0.49, 0.64)$, $n_3 = 200$.\n  - Seed $42$.\n\n- Case $2$ (boundary with a zero-variance policy):\n  - Dimension $d = 3$.\n  - Input $\\mathbf{x} = (0.2, -0.3, 1.5)$.\n  - Weights $\\mathbf{w} = (0.7, 1.1, -0.4)$.\n  - Bias $b = -0.05$.\n  - Policies:\n    - Policy $1$: $\\Sigma_1 = \\operatorname{diag}(0.0, 0.0, 0.0)$, $n_1 = 50$.\n    - Policy $2$: $\\Sigma_2 = \\operatorname{diag}(0.09, 0.09, 0.09)$, $n_2 = 100$.\n    - Policy $3$: $\\Sigma_3 = \\operatorname{diag}(0.36, 0.49, 0.25)$, $n_3 = 150$.\n  - Seed $123$.\n\n- Case $3$ (heteroscedastic high-dimensional mix):\n  - Dimension $d = 5$.\n  - Input $\\mathbf{x} = (1.0, -0.5, 0.3, 0.7, -1.2)$.\n  - Weights $\\mathbf{w} = (0.6, -0.4, 0.9, 0.1, -0.2)$.\n  - Bias $b = 0.0$.\n  - Policies:\n    - Policy $1$: $\\Sigma_1 = \\operatorname{diag}(0.16, 0.04, 0.09, 0.00, 0.25)$, $n_1 = 500$.\n    - Policy $2$: $\\Sigma_2 = \\operatorname{diag}(0.49, 0.36, 0.81, 0.25, 0.16)$, $n_2 = 400$.\n    - Policy $3$: $\\Sigma_3 = \\operatorname{diag}(0.01, 0.01, 0.01, 0.01, 0.01)$, $n_3 = 600$.\n  - Seed $7$.\n\nIn each case, compute and return the following seven quantities:\n$1$) the MC estimate of $\\mathbb{E}[f(a_1(\\mathbf{x}))]$ using policy $1$,\n$2$) the MC estimate of $\\operatorname{Var}[f(a_1(\\mathbf{x}))]$ using policy $1$,\n$3$) the analytically derived value of $\\mathbb{E}[f(a_i(\\mathbf{x}))]$ (common across policies),\n$4$) the analytically derived value of $\\operatorname{Var}[f(a_1(\\mathbf{x}))]$,\n$5$) the analytically derived minimal variance of the variance-aware aggregated estimator across the three policies using $n_1$, $n_2$, and $n_3$ samples,\n$6$) the analytically derived variance of the naive mean aggregator that averages all individual predictions across all samples from all policies,\n$7$) a boolean indicating whether the variance-aware aggregation variance in item $5$ is strictly less than the naive mean variance in item $6$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the seven quantities for Case $1$, followed by the seven for Case $2$, and then the seven for Case $3$. For example, the overall format must be $[\\text{case1\\_item1},\\text{case1\\_item2},\\dots,\\text{case3\\_item7}]$. All returned values must be of type boolean, integer, float, or a list of these fundamental types. No physical units, angle units, or percentages are involved in this problem.", "solution": "The problem requires the derivation and implementation of estimators for the expectation and variance of an affine predictor under randomized input augmentations, along with the design of an optimal aggregation strategy for estimates from different augmentation policies. We will proceed by first deriving the necessary analytical expressions from fundamental principles of probability theory, and then applying them to the specific test cases.\n\nLet the affine predictor be $f(\\mathbf{y}) = \\mathbf{w}^\\top \\mathbf{y} + b$, where $\\mathbf{y} \\in \\mathbb{R}^d$ is an input, $\\mathbf{w} \\in \\mathbb{R}^d$ is a weight vector, and $b \\in \\mathbb{R}$ is a bias scalar. The augmentation policies are of the form $a_i(\\mathbf{x}) = \\mathbf{x} + \\boldsymbol{\\varepsilon}_i$ for a fixed input $\\mathbf{x} \\in \\mathbb{R}^d$, where $\\boldsymbol{\\varepsilon}_i$ is a random vector with $\\mathbb{E}[\\boldsymbol{\\varepsilon}_i] = \\mathbf{0}$ and $\\operatorname{Cov}(\\boldsymbol{\\varepsilon}_i) = \\Sigma_i$, a diagonal matrix.\n\nFirst, we define the random variable representing the predictor output for policy $i$:\n$$Z_i = f(a_i(\\mathbf{x})) = f(\\mathbf{x} + \\boldsymbol{\\varepsilon}_i) = \\mathbf{w}^\\top (\\mathbf{x} + \\boldsymbol{\\varepsilon}_i) + b = \\mathbf{w}^\\top \\mathbf{x} + \\mathbf{w}^\\top \\boldsymbol{\\varepsilon}_i + b$$\n\n**Analytical Expectation and Variance**\n\nThe expectation of $Z_i$, denoted $\\mu_i$, is found using the linearity of the expectation operator:\n$$\\mu_i = \\mathbb{E}[Z_i] = \\mathbb{E}[\\mathbf{w}^\\top \\mathbf{x} + \\mathbf{w}^\\top \\boldsymbol{\\varepsilon}_i + b]$$\nSince $\\mathbf{w}$, $\\mathbf{x}$, and $b$ are constants, we have:\n$$\\mu_i = \\mathbf{w}^\\top \\mathbf{x} + \\mathbb{E}[\\mathbf{w}^\\top \\boldsymbol{\\varepsilon}_i] + b = \\mathbf{w}^\\top \\mathbf{x} + \\mathbf{w}^\\top \\mathbb{E}[\\boldsymbol{\\varepsilon}_i] + b$$\nGiven that $\\mathbb{E}[\\boldsymbol{\\varepsilon}_i] = \\mathbf{0}$, the expectation simplifies to:\n$$\\mu_i = \\mathbf{w}^\\top \\mathbf{x} + b$$\nThis result is independent of the policy index $i$, confirming that all policies share a common expectation $\\mu = \\mathbf{w}^\\top \\mathbf{x} + b$. This is the analytical expectation required for item $3$.\n\nThe variance of $Z_i$, denoted $\\sigma_i^2$, is found using the properties of variance. Constants do not affect variance, so:\n$$\\sigma_i^2 = \\operatorname{Var}[Z_i] = \\operatorname{Var}[\\mathbf{w}^\\top \\mathbf{x} + \\mathbf{w}^\\top \\boldsymbol{\\varepsilon}_i + b] = \\operatorname{Var}[\\mathbf{w}^\\top \\boldsymbol{\\varepsilon}_i]$$\nFor a linear transformation of a random vector $\\mathbf{v}$ by a matrix $\\mathbf{A}$, the variance is $\\operatorname{Var}[\\mathbf{A}\\mathbf{v}] = \\mathbf{A}\\operatorname{Cov}(\\mathbf{v})\\mathbf{A}^\\top$. Here, $\\mathbf{A}$ is the row vector $\\mathbf{w}^\\top$ and the random vector is $\\boldsymbol{\\varepsilon}_i$.\n$$\\sigma_i^2 = \\mathbf{w}^\\top \\operatorname{Cov}(\\boldsymbol{\\varepsilon}_i) \\mathbf{w}$$\nGiven $\\operatorname{Cov}(\\boldsymbol{\\varepsilon}_i) = \\Sigma_i$, we get:\n$$\\sigma_i^2 = \\mathbf{w}^\\top \\Sigma_i \\mathbf{w}$$\nSince $\\Sigma_i$ is diagonal, with diagonal elements $\\Sigma_{i,jj}$, and $\\mathbf{w}$ has components $w_j$, this quadratic form simplifies to $\\sigma_i^2 = \\sum_{j=1}^d w_j^2 \\Sigma_{i,jj}$. This expression provides the analytical variance for policy $i$, and for $i=1$ this is the value for item $4$.\n\n**Monte Carlo Estimators**\n\nFor a given policy $i$, we generate $n_i$ independent samples $\\{ \\boldsymbol{\\varepsilon}_i^{(k)} \\}_{k=1}^{n_i}$, leading to $n_i$ predictions $\\{ z_{i,k} \\}_{k=1}^{n_i}$ where $z_{i,k} = f(\\mathbf{x} + \\boldsymbol{\\varepsilon}_i^{(k)})$.\nThe Monte Carlo estimator for the expectation $\\mu$ is the sample mean:\n$$\\hat{\\mu}_i = \\frac{1}{n_i} \\sum_{k=1}^{n_i} z_{i,k}$$\nFor policy $i=1$, this is the estimator for item $1$.\nThe unbiased Monte Carlo estimator for the variance $\\sigma_i^2$ is the sample variance:\n$$\\hat{\\sigma}_i^2 = \\frac{1}{n_i - 1} \\sum_{k=1}^{n_i} (z_{i,k} - \\hat{\\mu}_i)^2$$\nFor policy $i=1$, this is the estimator for item $2$.\n\n**Variance-Aware Aggregation**\n\nWe have three independent, unbiased estimators of $\\mu$, the sample means $\\hat{\\mu}_1, \\hat{\\mu}_2, \\hat{\\mu}_3$. The variance of each estimator is:\n$$v_i = \\operatorname{Var}[\\hat{\\mu}_i] = \\operatorname{Var}\\left[\\frac{1}{n_i} \\sum_{k=1}^{n_i} z_{i,k}\\right] = \\frac{1}{n_i^2} \\sum_{k=1}^{n_i} \\operatorname{Var}[z_{i,k}] = \\frac{n_i \\sigma_i^2}{n_i^2} = \\frac{\\sigma_i^2}{n_i}$$\nWe seek an aggregated estimator $\\hat{\\mu}_{agg} = \\sum_{i=1}^3 c_i \\hat{\\mu}_i$ that is unbiased and has minimum variance.\nUnbiasedness requires $\\mathbb{E}[\\hat{\\mu}_{agg}] = \\mu$, which implies $\\sum_{i=1}^3 c_i = 1$.\nThe variance to be minimized is $\\operatorname{Var}[\\hat{\\mu}_{agg}] = \\sum_{i=1}^3 c_i^2 \\operatorname{Var}[\\hat{\\mu}_i] = \\sum_{i=1}^3 c_i^2 v_i$, due to the independence of the estimators.\n\nWe use the method of Lagrange multipliers to minimize $V(c_1, c_2, c_3) = \\sum_{i=1}^3 c_i^2 v_i$ subject to the constraint $g(c_1, c_2, c_3) = \\sum_{i=1}^3 c_i - 1 = 0$.\nThe Lagrangian is $\\mathcal{L}(c_1, c_2, c_3, \\lambda) = \\sum_{i=1}^3 c_i^2 v_i - \\lambda(\\sum_{i=1}^3 c_i - 1)$.\nSetting the partial derivatives with respect to each $c_i$ to zero gives $2c_i v_i - \\lambda = 0$, so $c_i = \\lambda / (2v_i)$.\nSubstituting this into the constraint: $\\sum_{i=1}^3 \\frac{\\lambda}{2v_i} = 1 \\implies \\frac{\\lambda}{2} \\sum_{i=1}^3 \\frac{1}{v_i} = 1 \\implies \\lambda = 2 / (\\sum_{j=1}^3 1/v_j)$.\nThe optimal weights are therefore $c_i = \\frac{1}{v_i} / (\\sum_{j=1}^3 1/v_j)$.\n\nThe resulting minimal variance, $V_{min}$, is:\n$$V_{min} = \\sum_{i=1}^3 c_i^2 v_i = \\sum_{i=1}^3 \\left( \\frac{\\lambda}{2v_i} \\right)^2 v_i = \\frac{\\lambda^2}{4} \\sum_{i=1}^3 \\frac{1}{v_i}$$\nSubstituting $\\lambda/2 = 1 / (\\sum_j 1/v_j)$:\n$$V_{min} = \\left(\\frac{1}{\\sum_j 1/v_j}\\right)^2 \\sum_i \\frac{1}{v_i} = \\frac{1}{\\sum_{j=1}^3 1/v_j} = \\left(\\sum_{i=1}^3 \\frac{1}{v_i}\\right)^{-1} = \\left(\\frac{n_1}{\\sigma_1^2} + \\frac{n_2}{\\sigma_2^2} + \\frac{n_3}{\\sigma_3^2}\\right)^{-1}$$\nIf any $\\sigma_i^2 = 0$, then $v_i=0$ and the term $n_i/\\sigma_i^2$ becomes infinite, making the sum infinite and $V_{min}$ equal to $0$. This is correct, as a zero-variance estimator should be given full weight, resulting in an aggregated estimator with zero variance. This is the expression for item $5$.\n\n**Naive Mean Aggregator**\n\nThe naive aggregator averages all $N = n_1 + n_2 + n_3$ individual predictions. Its estimator can be written as a weighted average of the per-policy means:\n$$\\hat{\\mu}_{naive} = \\frac{1}{N} \\sum_{i=1}^3 \\sum_{k=1}^{n_i} z_{i,k} = \\sum_{i=1}^3 \\frac{n_i}{N} \\hat{\\mu}_i$$\nThis is an unbiased linear estimator with weights $c_i^{naive} = n_i/N$. Its variance, $V_{naive}$, is:\n$$V_{naive} = \\operatorname{Var}[\\hat{\\mu}_{naive}] = \\sum_{i=1}^3 (c_i^{naive})^2 v_i = \\sum_{i=1}^3 \\left(\\frac{n_i}{N}\\right)^2 \\frac{\\sigma_i^2}{n_i} = \\frac{1}{N^2} \\sum_{i=1}^3 n_i \\sigma_i^2$$\n$$V_{naive} = \\frac{n_1 \\sigma_1^2 + n_2 \\sigma_2^2 + n_3 \\sigma_3^2}{(n_1 + n_2 + n_3)^2}$$\nThis is the expression for item $6$.\n\n**Comparison of Variances**\n\nThe variance-aware aggregator is, by construction, the minimum variance unbiased linear estimator. The naive aggregator is also an unbiased linear estimator but is generally not optimal. Therefore, it must be that $V_{min} \\le V_{naive}$. Strict inequality $V_{min}  V_{naive}$ holds unless the naive weights happen to be optimal. The optimal weights are $c_i \\propto 1/v_i = n_i/\\sigma_i^2$, while the naive weights are $c_i^{naive} \\propto n_i$. These are proportional if and only if all $\\sigma_i^2$ are equal (for $\\sigma_i^2  0$). If the per-policy variances $\\sigma_i^2$ are not all equal, then $V_{min}  V_{naive}$. Item $7$ is the result of this comparison.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases specified.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {\n            \"d\": 3,\n            \"x\": np.array([0.8, -1.0, 0.5]),\n            \"w\": np.array([1.2, -0.7, 0.3]),\n            \"b\": 0.1,\n            \"policies\": [\n                {\"Sigma_diag\": np.array([0.04, 0.01, 0.09]), \"n\": 400},\n                {\"Sigma_diag\": np.array([0.25, 0.16, 0.36]), \"n\": 300},\n                {\"Sigma_diag\": np.array([1.00, 0.49, 0.64]), \"n\": 200},\n            ],\n            \"seed\": 42\n        },\n        # Case 2\n        {\n            \"d\": 3,\n            \"x\": np.array([0.2, -0.3, 1.5]),\n            \"w\": np.array([0.7, 1.1, -0.4]),\n            \"b\": -0.05,\n            \"policies\": [\n                {\"Sigma_diag\": np.array([0.0, 0.0, 0.0]), \"n\": 50},\n                {\"Sigma_diag\": np.array([0.09, 0.09, 0.09]), \"n\": 100},\n                {\"Sigma_diag\": np.array([0.36, 0.49, 0.25]), \"n\": 150},\n            ],\n            \"seed\": 123\n        },\n        # Case 3\n        {\n            \"d\": 5,\n            \"x\": np.array([1.0, -0.5, 0.3, 0.7, -1.2]),\n            \"w\": np.array([0.6, -0.4, 0.9, 0.1, -0.2]),\n            \"b\": 0.0,\n            \"policies\": [\n                {\"Sigma_diag\": np.array([0.16, 0.04, 0.09, 0.00, 0.25]), \"n\": 500},\n                {\"Sigma_diag\": np.array([0.49, 0.36, 0.81, 0.25, 0.16]), \"n\": 400},\n                {\"Sigma_diag\": np.array([0.01, 0.01, 0.01, 0.01, 0.01]), \"n\": 600},\n            ],\n            \"seed\": 7\n        },\n    ]\n\n    all_results = []\n    for case in test_cases:\n        case_results = solve_case(case)\n        all_results.extend(case_results)\n\n    # Format the final output string as specified\n    # The default str() for a boolean is 'True' or 'False' with a capital letter.\n    # The problem description does not specify lowercase.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n\ndef solve_case(case):\n    \"\"\"\n    Computes the seven required quantities for a single test case.\n    \"\"\"\n    w, x, b, seed = case[\"w\"], case[\"x\"], case[\"b\"], case[\"seed\"]\n    policies = case[\"policies\"]\n    \n    # Initialize random number generator\n    rng = np.random.default_rng(seed)\n\n    # --- Monte Carlo estimations for Policy 1 ---\n    policy1 = policies[0]\n    n1 = policy1[\"n\"]\n    Sigma1_diag = policy1[\"Sigma_diag\"]\n    \n    # Generate random noise samples for policy 1\n    std_devs1 = np.sqrt(Sigma1_diag)\n    # epsilon_samples is (n1, d)\n    epsilon_samples1 = rng.normal(0, std_devs1, size=(n1, case[\"d\"]))\n    \n    # Calculate predictions z = w^T * (x + epsilon) + b\n    # (n1, d) @ (d,) - (n1,)\n    predictions1 = epsilon_samples1 @ w + (x @ w + b)\n    \n    # Item 1: MC estimate of E[f(a_1(x))]\n    mc_mean_p1 = np.mean(predictions1)\n    \n    # Item 2: MC estimate of Var[f(a_1(x))]\n    # ddof=1 for unbiased sample variance\n    mc_var_p1 = np.var(predictions1, ddof=1)\n    \n    # --- Analytical derivations ---\n    \n    # Item 3: Analytical expectation E[f(a_i(x))]\n    analytical_mean = w @ x + b\n    \n    # Calculate analytical variances for all policies\n    sigmas_sq = []\n    for pol in policies:\n        # sigma_i^2 = w^T * Sigma_i * w\n        sigma_sq = w @ (pol[\"Sigma_diag\"] * w)  # Since Sigma is diagonal\n        sigmas_sq.append(sigma_sq)\n        \n    # Item 4: Analytical variance Var[f(a_1(x))]\n    analytical_var_p1 = sigmas_sq[0]\n    \n    # Item 5: Minimal variance of the variance-aware aggregated estimator\n    sum_inverse_v = 0\n    is_zero_var_policy = False\n    for i, pol in enumerate(policies):\n        sigma_sq_i = sigmas_sq[i]\n        n_i = pol[\"n\"]\n        if sigma_sq_i == 0:\n            is_zero_var_policy = True\n            break\n        # v_i = sigma_i^2 / n_i\n        sum_inverse_v += n_i / sigma_sq_i\n        \n    if is_zero_var_policy:\n        min_var = 0.0\n    else:\n        min_var = 1.0 / sum_inverse_v\n        \n    # Item 6: Variance of the naive mean aggregator\n    ns = np.array([p[\"n\"] for p in policies])\n    total_n = np.sum(ns)\n    weighted_sum_sigmas_sq = np.sum(ns * np.array(sigmas_sq))\n    naive_var = weighted_sum_sigmas_sq / (total_n**2)\n    \n    # Item 7: Boolean comparison\n    is_optimal_better = min_var  naive_var\n    \n    return [\n        mc_mean_p1,\n        mc_var_p1,\n        analytical_mean,\n        analytical_var_p1,\n        min_var,\n        naive_var,\n        is_optimal_better\n    ]\n\n# Run the solver\nsolve()\n\n```", "id": "3123404"}, {"introduction": "A key aspect of building trustworthy AI systems is quantifying a model's own uncertainty about its predictions, known as epistemic uncertainty. This exercise introduces Monte Carlo (MC) dropout, a powerful and practical technique for estimating this uncertainty by treating dropout as a Bayesian approximation. You will implement MC dropout to generate a distribution of outputs for a single input and use the sample variance of this distribution as a direct measure of the model's confidence, exploring how this uncertainty is affected by the dropout rate $p$ [@problem_id:3123387].", "problem": "You are given a single-hidden-layer feedforward neural network with Rectified Linear Unit (ReLU) activation and hidden-layer dropout. The network maps a two-dimensional input vector $x \\in \\mathbb{R}^2$ to a scalar output. The network parameters are fixed and known. You must use Monte Carlo dropout to approximate the predictive mean $\\mathbb{E}[f(x)]$ and predictive variance $\\operatorname{Var}[f(x)]$ of the network output $f(x)$ under the randomness induced by dropout, and then relate how epistemic uncertainty changes with the dropout rate $p$ by inspecting the variance values across test cases.\n\nFundamental base for deriving the algorithm:\n- The expectation of a real-valued random variable $Z$ is $\\mathbb{E}[Z]$.\n- The variance of a real-valued random variable $Z$ is $\\operatorname{Var}(Z) = \\mathbb{E}\\left[(Z - \\mathbb{E}[Z])^2\\right]$.\n- Monte Carlo estimation uses independent samples $Z_1, Z_2, \\dots, Z_T$ to approximate $\\mathbb{E}[Z]$ by the sample mean and $\\operatorname{Var}(Z)$ by the sample variance.\n\nNetwork definition:\n- Hidden layer of size $H=3$ with parameters\n$$\n\\mathbf{W}_1 =\n\\begin{bmatrix}\n1.0  -0.5 \\\\\n0.3  0.8 \\\\\n-0.7  0.2\n\\end{bmatrix}, \\quad\n\\mathbf{b}_1 =\n\\begin{bmatrix}\n0.1 \\\\\n-0.2 \\\\\n0.0\n\\end{bmatrix}.\n$$\n- Output layer parameters\n$$\n\\mathbf{W}_2 =\n\\begin{bmatrix}\n0.5 \\\\\n-1.0 \\\\\n0.3\n\\end{bmatrix}, \\quad\nb_2 = 0.05.\n$$\n- ReLU activation is defined as $\\operatorname{ReLU}(z) = \\max(z, 0)$ applied elementwise.\n- Hidden-layer dropout with rate $p \\in [0,1)$ is applied to the post-activation hidden vector $h = \\operatorname{ReLU}(\\mathbf{W}_1 x + \\mathbf{b}_1)$. Using inverted dropout, the dropped hidden vector is\n$$\n\\tilde{h} = \\begin{cases}\nh,  \\text{if } p = 0, \\\\\n\\frac{m \\odot h}{1-p},  \\text{if } p  0,\n\\end{cases}\n$$\nwhere $m \\in \\{0,1\\}^H$ is a random mask with independent entries $m_i \\sim \\operatorname{Bernoulli}(1-p)$, and $\\odot$ denotes elementwise multiplication. The network output is\n$$\nf(x) = \\mathbf{W}_2^\\top \\tilde{h} + b_2.\n$$\n\nMonte Carlo dropout setup:\n- For a given input $x$, dropout rate $p$, and number of Monte Carlo samples $T \\in \\mathbb{N}$, generate $T$ independent samples of $f(x)$ by independently drawing masks $m$ and computing $f(x)$.\n- Approximate $\\mathbb{E}[f(x)]$ by the sample mean of the $T$ outputs.\n- Approximate $\\operatorname{Var}[f(x)]$ by the sample variance of the $T$ outputs.\n\nTask:\n- Implement a program that performs the above Monte Carlo procedure for the specified test suite.\n- Your program must not train the network; it must use the fixed parameters given above.\n- Your outputs must be floats. No physical units are involved in this problem.\n\nTest suite:\nCompute the predictive mean and variance for the following $(x, p, T)$ triplets. Each $x$ is given as a two-dimensional vector. Use the exact numeric values shown.\n\n1. $x = \\begin{bmatrix} 0.5 \\\\ -1.0 \\end{bmatrix}$, $p = 0.0$, $T = 100$.\n2. $x = \\begin{bmatrix} 0.5 \\\\ -1.0 \\end{bmatrix}$, $p = 0.2$, $T = 1000$.\n3. $x = \\begin{bmatrix} 2.0 \\\\ 1.0 \\end{bmatrix}$, $p = 0.5$, $T = 1000$.\n4. $x = \\begin{bmatrix} 2.0 \\\\ 1.0 \\end{bmatrix}$, $p = 0.9$, $T = 2000$.\n5. $x = \\begin{bmatrix} -1.5 \\\\ 0.3 \\end{bmatrix}$, $p = 0.5$, $T = 25$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- Each test caseâ€™s result must itself be a list of two floats in the order $[\\text{mean}, \\text{variance}]$ corresponding to $\\mathbb{E}[f(x)]$ and $\\operatorname{Var}[f(x)]$ for that case.\n- For example, a valid output format for $K$ test cases is\n$$\n\\text{[}[m_1,v_1],[m_2,v_2],\\dots,[m_K,v_K]\\text{]}.\n$$\n\nDesign for coverage:\n- Case $1$ is a boundary condition with $p = 0.0$ (no dropout), which should exhibit zero variance.\n- Cases $2$ and $3$ are general cases with moderate dropout.\n- Case $4$ probes a high dropout regime $p = 0.9$ to illustrate increased epistemic uncertainty.\n- Case $5$ uses a small number of samples $T = 25$ to illustrate sampling variability in the Monte Carlo estimates.\n\nYour implementation must be deterministic across runs by using a fixed random seed for the Monte Carlo sampling.", "solution": "We begin from core definitions in probability theory. For a real-valued random variable $Z$, the expectation is $\\mathbb{E}[Z]$, and the variance is $\\operatorname{Var}(Z) = \\mathbb{E}\\left[(Z - \\mathbb{E}[Z])^2\\right]$. When the exact distribution of $Z$ is not tractable, Monte Carlo methods approximate these quantities using independent samples. Given independent samples $Z_1, Z_2, \\dots, Z_T$, the sample mean $\\hat{\\mu}$ and sample variance $\\hat{\\sigma}^2$ provide consistent estimators:\n$$\n\\hat{\\mu} = \\frac{1}{T} \\sum_{t=1}^T Z_t, \\quad\n\\hat{\\sigma}^2 = \\frac{1}{T-1} \\sum_{t=1}^T (Z_t - \\hat{\\mu})^2.\n$$\nThe unbiased sample variance uses the denominator $T-1$ and converges to $\\operatorname{Var}(Z)$ as $T \\to \\infty$.\n\nIn deep learning, dropout practices introduce stochasticity by randomly zeroing hidden units during training. Monte Carlo dropout applies this same randomness at inference time to approximate Bayesian predictions, attributing variability in outputs to epistemic uncertainty (uncertainty in the model due to limited data or parameter uncertainty). Inverted dropout rescales kept activations by $\\frac{1}{1-p}$ so that the expected activation remains unchanged. Specifically, for a hidden activation vector $h = \\operatorname{ReLU}(\\mathbf{W}_1 x + \\mathbf{b}_1)$ and a mask $m \\in \\{0,1\\}^H$ with independent entries $m_i \\sim \\operatorname{Bernoulli}(1-p)$, the dropped hidden vector is\n$$\n\\tilde{h} = \\frac{m \\odot h}{1-p} \\quad \\text{for } p > 0, \\quad \\tilde{h} = h \\quad \\text{for } p = 0.\n$$\nThe scalar output is then\n$$\nf(x) = \\mathbf{W}_2^\\top \\tilde{h} + b_2.\n$$\nUnder dropout, $f(x)$ becomes a random variable due to the randomness in $m$. We estimate $\\mathbb{E}[f(x)]$ and $\\operatorname{Var}[f(x)]$ by sampling $T$ independent masks $m^{(1)}, \\dots, m^{(T)}$ and computing the outputs $f^{(t)}(x)$, forming the sample mean and sample variance.\n\nAlgorithmic steps:\n1. Fix the network parameters $\\mathbf{W}_1$, $\\mathbf{b}_1$, $\\mathbf{W}_2$, $b_2$ as specified.\n2. For each test case $(x, p, T)$:\n   - Compute the deterministic hidden activation $h = \\operatorname{ReLU}(\\mathbf{W}_1 x + \\mathbf{b}_1)$.\n   - If $p = 0$, then $\\tilde{h} = h$ for all samples, so $f(x)$ is deterministic; $\\operatorname{Var}[f(x)] = 0$.\n   - If $p > 0$, repeat for $t = 1, \\dots, T$:\n     - Sample a mask $m^{(t)} \\in \\{0,1\\}^H$ with independent $m_i^{(t)} \\sim \\operatorname{Bernoulli}(1-p)$.\n     - Form $\\tilde{h}^{(t)} = \\frac{m^{(t)} \\odot h}{1-p}$.\n     - Compute $f^{(t)}(x) = \\mathbf{W}_2^\\top \\tilde{h}^{(t)} + b_2$.\n   - Estimate $\\mathbb{E}[f(x)]$ by $\\hat{\\mu} = \\frac{1}{T} \\sum_{t=1}^T f^{(t)}(x)$ and $\\operatorname{Var}[f(x)]$ by the unbiased sample variance $\\hat{\\sigma}^2 = \\frac{1}{T-1} \\sum_{t=1}^T (f^{(t)}(x) - \\hat{\\mu})^2$ (for $T \\geq 2$; for $p=0$, all samples are identical and variance is $0$).\n3. Aggregate the results for all test cases into a single list in the specified output format.\n\nRelation of epistemic uncertainty to the dropout rate $p$:\n- Epistemic uncertainty reflects model uncertainty. Dropout stochastically removes hidden units. As $p$ increases, the probability of zeroing units increases, which induces higher variability across different network realizations sampled via masks.\n- With inverted dropout, kept activations are scaled by $\\frac{1}{1-p}$. For large $p$ (close to $1$), this factor amplifies the contributions of the few active units when they are kept, while most samples set them to zero. This mixture (rare large contributions versus frequent zeros) increases the variance of $f(x)$.\n- Therefore, all else equal, the predictive variance $\\operatorname{Var}[f(x)]$ generally increases with $p$, illustrating greater epistemic uncertainty.\n\nTest suite coverage justification:\n- Case $1$ ($p=0$) yields zero variance since $m_i \\equiv 1$ and no randomness is present.\n- Cases $2$ and $3$ are typical Monte Carlo dropout estimates with moderate $p$ and large $T$, providing accurate estimates of $\\mathbb{E}[f(x)]$ and $\\operatorname{Var}[f(x)]$.\n- Case $4$ uses a high dropout rate $p=0.9$ and larger $T$ to stably estimate the increased variance due to strong dropout.\n- Case $5$ demonstrates the effect of small sample size $T=25$ on the estimates, emphasizing sampling variability.\n\nThe program will implement these steps using a fixed random seed to ensure deterministic results across runs and will print a single line with the list of $[\\text{mean}, \\text{variance}]$ for each test case in order.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef relu(x):\n    return np.maximum(x, 0.0)\n\ndef mc_dropout_predict(x, p, T, rng, W1, b1, W2, b2):\n    \"\"\"\n    Perform Monte Carlo dropout predictions for a single input x.\n    Returns an array of T scalar outputs.\n    \"\"\"\n    # Compute deterministic hidden activation h = ReLU(W1 x + b1)\n    a = W1 @ x + b1\n    h = relu(a)\n\n    # If p == 0, output is deterministic; replicate the same value T times\n    if p == 0.0:\n        y = float(W2 @ h + b2)\n        return np.full(T, y, dtype=float)\n\n    keep_prob = 1.0 - p\n    scale = 1.0 / keep_prob\n\n    # Sample T masks and compute outputs\n    outputs = np.empty(T, dtype=float)\n    for t in range(T):\n        # Bernoulli(keep_prob) for each hidden unit\n        m = rng.binomial(1, keep_prob, size=h.shape)\n        h_drop = (m * h) * scale\n        y = float(W2 @ h_drop + b2)\n        outputs[t] = y\n    return outputs\n\ndef solve():\n    # Fixed random seed for determinism\n    rng = np.random.default_rng(42)\n\n    # Define fixed network parameters\n    W1 = np.array([[1.0, -0.5],\n                   [0.3,  0.8],\n                   [-0.7, 0.2]], dtype=float)\n    b1 = np.array([0.1, -0.2, 0.0], dtype=float)\n    W2 = np.array([0.5, -1.0, 0.3], dtype=float)\n    b2 = 0.05\n\n    # Define the test cases from the problem statement.\n    # Each test case is (x, p, T)\n    test_cases = [\n        (np.array([0.5, -1.0], dtype=float), 0.0, 100),\n        (np.array([0.5, -1.0], dtype=float), 0.2, 1000),\n        (np.array([2.0,  1.0], dtype=float), 0.5, 1000),\n        (np.array([2.0,  1.0], dtype=float), 0.9, 2000),\n        (np.array([-1.5, 0.3], dtype=float), 0.5, 25),\n    ]\n\n    results = []\n    for x, p, T in test_cases:\n        outputs = mc_dropout_predict(x, p, T, rng, W1, b1, W2, b2)\n        mean = float(np.mean(outputs))\n        # Unbiased sample variance; for T=1, set variance to 0.0 to avoid NaN\n        var = float(np.var(outputs, ddof=1)) if T > 1 else 0.0\n        # Round for stable presentation while keeping numeric type\n        mean_rounded = round(mean, 6)\n        var_rounded = round(var, 6)\n        results.append([mean_rounded, var_rounded])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(str(res) for res in results)}]\")\n\nsolve()\n```", "id": "3123387"}]}