{"hands_on_practices": [{"introduction": "To improve a model's ability to learn disentangled representations, we first need a robust way to measure it. This exercise guides you through the process of creating a powerful diagnostic tool from scratch, based on the fundamental concept of mutual information [@problem_id:3116952]. By implementing an empirical estimator and developing a permutation-invariant score, you'll gain a concrete understanding of how we can quantify the alignment between learned latent variables and the true underlying factors of variation.", "problem": "You are given paired samples of learned latent variables and ground-truth generative factors produced by a model such as a Beta-Variational Autoencoder (Beta-VAE). Let the latent code be denoted by a random vector $z = (z_1, \\dots, z_{d_z})$ and the ground-truth generative factors by $g = (g_1, \\dots, g_{d_g})$. Your task is to construct a disentanglement diagnostic based on the mutual information between each scalar latent $z_i$ and each scalar factor $g_j$, to assemble the mutual information matrix, and to quantify diagonal dominance in a permutation-invariant way.\n\nStarting only from the foundational definitions of joint and marginal probability distributions, Shannon entropy, and mutual information (which is defined in terms of Kullback–Leibler Divergence), derive and implement an empirical estimator of the mutual information between scalar random variables using discretization of observed samples. Then, use this estimator to build a matrix of mutual informations between all pairs $(z_i, g_j)$ and compute a permutation-invariant diagonal dominance score that reflects how well each latent aligns to a unique factor.\n\nYou must follow these design requirements:\n\n- Data and discretization:\n  - For a given paired sample set $\\{(z^{(n)}, g^{(n)})\\}_{n=1}^N$ with $z^{(n)} \\in \\mathbb{R}^{d_z}$ and $g^{(n)} \\in \\mathbb{R}^{d_g}$, discretize each scalar variable into $b$ equal-width bins over its observed range (minimum to maximum in the sample). Use the same $b$ for all scalar variables. Treat the bin edges as closed on the right for the maximum so that the maximum observed value is included. Let $b \\in \\mathbb{N}$ with $b \\ge 2$.\n  - From these discretizations, form empirical joint frequency tables for each pair $(z_i, g_j)$ and normalize them to obtain empirical joint probability masses, along with their marginals.\n\n- Mutual information matrix:\n  - Using only the foundational definitions, compute the mutual information $I(z_i; g_j)$ for each pair $(i, j)$, producing a matrix $I \\in \\mathbb{R}_{\\ge 0}^{d_z \\times d_g}$. Use natural logarithms so that the units are in nats.\n\n- Permutation-invariant diagonal dominance score:\n  - Because the ordering of latent dimensions and factor dimensions is arbitrary, first compute the optimal one-to-one assignment between $\\{z_i\\}$ and $\\{g_j\\}$ that maximizes the sum of the selected entries of the matrix $I$. If $d_z \\ne d_g$, the assignment should match on $m = \\min(d_z, d_g)$ pairs without reuse of indices in either set.\n  - Let $S_{\\text{assign}}$ be the sum of the $m$ selected entries and let $S_{\\text{all}}$ be the sum of all entries of $I$. Define the diagonal dominance score as $D = S_{\\text{assign}} / S_{\\text{all}}$. If $S_{\\text{all}} = 0$ within numerical tolerance (i.e., $S_{\\text{all}} \\le 10^{-12}$), define $D = 0$.\n  - The score $D$ must satisfy $0 \\le D \\le 1$ and should be closer to $1$ when the mutual information mass concentrates on a permutation of the diagonal.\n\n- Output specification:\n  - For each test case below, your program must output a triple consisting of:\n    1) the score $D$ rounded to $4$ decimal places,\n    2) the selected assignment from latent indices to factor indices as a list of length $d_z$, where unassigned latent indices (if $d_z > d_g$) are marked with $-1$,\n    3) the mutual information matrix $I$ flattened in row-major order with each entry rounded to $4$ decimal places.\n  - Aggregate the results for all test cases into a single line containing a list of these triples. The final output must be exactly one line in the format:\n    $[ [D_1, [a_{1,1}, \\dots, a_{1,d_z}], [I_{1,1}, \\dots]], [D_2, [\\dots], [\\dots]], \\dots ]$.\n\n- Test suite and data generation:\n  - Use the following test suite. In all cases below, $N$ is the number of samples and $s$ is the pseudo-random seed for a reproducible generator. All logarithms must be natural logs, and all computations must be done in floating-point arithmetic. In all cases, set the number of bins to $b = 12$.\n\n  - Test case $1$ (near-ideal disentanglement):\n    - Dimensions: $d_z = 3$, $d_g = 3$.\n    - Parameters: $N = 5000$, $s = 13$.\n    - Sampling: For each $n \\in \\{1, \\dots, N\\}$, sample $g^{(n)} \\sim \\text{Uniform}([-1, 1]^{3})$, and noise $\\epsilon^{(n)} \\sim \\mathcal{N}(0, I_3)$.\n    - Construct $z^{(n)} = g^{(n)} + 0.05 \\cdot \\epsilon^{(n)}$.\n\n  - Test case $2$ (independent latents and factors):\n    - Dimensions: $d_z = 3$, $d_g = 3$.\n    - Parameters: $N = 5000$, $s = 7$.\n    - Sampling: For each $n$, sample $g^{(n)} \\sim \\text{Uniform}([-1, 1]^{3})$, and independently sample $z^{(n)} \\sim \\mathcal{N}(0, I_3)$.\n\n  - Test case $3$ (entangled linear mixing):\n    - Dimensions: $d_z = 3$, $d_g = 3$.\n    - Parameters: $N = 5000$, $s = 11$.\n    - Sampling: For each $n$, sample $g^{(n)} \\sim \\text{Uniform}([-1, 1]^{3})$, noise $\\epsilon^{(n)} \\sim \\mathcal{N}(0, I_3)$, and set $A \\in \\mathbb{R}^{3 \\times 3}$ to\n      $$\n      A = \\begin{bmatrix}\n      1.0 & 0.8 & 0.5 \\\\\n      0.6 & 1.0 & 0.4 \\\\\n      0.5 & 0.4 & 1.2\n      \\end{bmatrix}.\n      $$\n      Construct $z^{(n)} = A \\, g^{(n)} + 0.2 \\cdot \\epsilon^{(n)}$.\n\n  - Test case $4$ (dimension mismatch, sparse alignment):\n    - Dimensions: $d_z = 2$, $d_g = 3$.\n    - Parameters: $N = 5000$, $s = 17$.\n    - Sampling: For each $n$, sample $g^{(n)} \\sim \\text{Uniform}([-1, 1]^{3})$, noise $\\epsilon^{(n)} \\sim \\mathcal{N}(0, I_2)$, and set $B \\in \\mathbb{R}^{2 \\times 3}$ to\n      $$\n      B = \\begin{bmatrix}\n      1.0 & 0.0 & 0.0 \\\\\n      0.0 & 0.0 & 1.0\n      \\end{bmatrix}.\n      $$\n      Construct $z^{(n)} = B \\, g^{(n)} + 0.05 \\cdot \\epsilon^{(n)}$.\n\n- Final output format requirement:\n  - Your program should produce a single line of output containing the list of results for the $4$ test cases as a comma-separated list enclosed in square brackets, where each result is a list of the form $[D, \\text{assignment}, \\text{flattened\\_I}]$ as described above. All floats in the output must be rounded to $4$ decimal places, and integers must be exact.\n\nYour solution must be a complete, runnable program that performs the sampling, discretization, mutual information estimation, optimal assignment, score computation, and final formatting exactly as specified above, without requiring any external input.", "solution": "The problem requires the development of a disentanglement metric based on mutual information. We will proceed by first establishing the theoretical foundations of mutual information from first principles, then detailing the method for its empirical estimation, and finally specifying the construction of the permutation-invariant score.\n\n**1. Foundational Derivation of Mutual Information**\n\nThe cornerstone of this analysis is mutual information, which quantifies the statistical dependence between two random variables. We derive it from the more fundamental concepts of Shannon entropy and Kullback-Leibler (KL) divergence.\n\nLet $X$ and $Y$ be two discrete random variables with alphabets $\\mathcal{X}$ and $\\mathcal{Y}$, joint probability mass function (PMF) $P_{XY}(x, y)$, and marginal PMFs $P_X(x)$ and $P_Y(y)$.\n\nThe **Shannon entropy** of $X$ measures its uncertainty and is defined as:\n$$ H(X) = - \\sum_{x \\in \\mathcal{X}} P_X(x) \\log P_X(x) $$\nwhere the logarithm is the natural logarithm (base $e$), and the units are nats. By convention, $0 \\log 0 = 0$. The joint entropy of the pair $(X, Y)$ is similarly defined:\n$$ H(X, Y) = - \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} P_{XY}(x, y) \\log P_{XY}(x, y) $$\n\nThe **Kullback-Leibler (KL) divergence** measures how one probability distribution $P$ diverges from a second, expected probability distribution $Q$. For discrete distributions, it is defined as:\n$$ D_{KL}(P || Q) = \\sum_{x \\in \\mathcal{X}} P(x) \\log \\frac{P(x)}{Q(x)} $$\n\nThe problem statement requires framing **mutual information** $I(X; Y)$ in terms of KL divergence. Mutual information measures the reduction in uncertainty about one variable from knowing the other. It is formally the KL divergence between the joint distribution $P_{XY}(x, y)$ and the product of the marginals $P_X(x) P_Y(y)$, which represents the joint distribution under the assumption of independence.\n$$ I(X; Y) = D_{KL}(P_{XY} || P_X P_Y) = \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} P_{XY}(x, y) \\log \\frac{P_{XY}(x, y)}{P_X(x) P_Y(y)} $$\n\nThis definition can be expanded to relate mutual information to entropies:\n$$\n\\begin{align*}\nI(X; Y) &= \\sum_{x, y} P_{XY}(x, y) \\left[ \\log P_{XY}(x, y) - \\log P_X(x) - \\log P_Y(y) \\right] \\\\\n&= -H(X, Y) - \\sum_{x, y} P_{XY}(x, y) \\log P_X(x) - \\sum_{x, y} P_{XY}(x, y) \\log P_Y(y) \\\\\n&= -H(X, Y) - \\sum_x \\left( \\sum_y P_{XY}(x, y) \\right) \\log P_X(x) - \\sum_y \\left( \\sum_x P_{XY}(x, y) \\right) \\log P_Y(y) \\\\\n&= -H(X, Y) - \\sum_x P_X(x) \\log P_X(x) - \\sum_y P_Y(y) \\log P_Y(y) \\\\\n&= H(X) + H(Y) - H(X, Y)\n\\end{align*}\n$$\nThis final form, $I(X; Y) = H(X) + H(Y) - H(X, Y)$, provides a convenient and numerically stable way to compute mutual information from estimated entropies.\n\n**2. Empirical Estimation from Continuous Samples**\n\nThe latent variables $z_i$ and generative factors $g_j$ are continuous. To apply the discrete formulas above, we must first discretize the data.\n\nGiven $N$ samples of a scalar variable $x$, $\\{x^{(n)}\\}_{n=1}^N$, we find its minimum $x_{\\min}$ and maximum $x_{\\max}$. We divide the range $[x_{\\min}, x_{\\max}]$ into $b$ equal-width bins. The width of each bin is $\\Delta x = (x_{\\max} - x_{\\min}) / b$. The bin edges are $x_{\\min}, x_{\\min} + \\Delta x, \\dots, x_{\\max}$. The problem specifies that the last bin includes the maximum value, which is standard for histogram implementations. Each sample $x^{(n)}$ is then assigned to one of $b$ bins.\n\nFor a pair of variables $(z_i, g_j)$, we apply this process to both $\\{z_i^{(n)}\\}_{n=1}^N$ and $\\{g_j^{(n)}\\}_{n=1}^N$, using $b$ bins for each. We can then construct a $b \\times b$ joint frequency matrix, $\\text{Count}(k, l)$, where $\\text{Count}(k, l)$ is the number of samples $(z_i^{(n)}, g_j^{(n)})$ for which $z_i^{(n)}$ falls into bin $k$ and $g_j^{(n)}$ falls into bin $l$.\n\nThe empirical joint PMF, $\\hat{P}(z_i, g_j)$, is obtained by normalizing the frequency matrix by the total number of samples $N$:\n$$ \\hat{P}_{Z_i, G_j}(k, l) = \\frac{\\text{Count}(k, l)}{N} $$\nThe empirical marginal PMFs are computed by summing over the rows and columns of the joint PMF:\n$$ \\hat{P}_{Z_i}(k) = \\sum_{l=1}^b \\hat{P}_{Z_i, G_j}(k, l) \\qquad \\text{and} \\qquad \\hat{P}_{G_j}(l) = \\sum_{k=1}^b \\hat{P}_{Z_i, G_j}(k, l) $$\nWith these empirical PMFs, we can compute the empirical entropies $\\hat{H}(Z_i)$, $\\hat{H}(G_j)$, and $\\hat{H}(Z_i, G_j)$, and subsequently the empirical mutual information $\\hat{I}(z_i; g_j)$.\n\n**3. Mutual Information Matrix**\n\nBy repeating the estimation procedure for every pair of latent variable $z_i$ (for $i \\in \\{1, \\dots, d_z\\}$) and generative factor $g_j$ (for $j \\in \\{1, \\dots, d_g\\}$), we construct the mutual information matrix $I \\in \\mathbb{R}_{\\ge 0}^{d_z \\times d_g}$, where the entry $(i, j)$ is $I_{ij} = \\hat{I}(z_i; g_j)$. A well-disentangled representation would ideally have high mutual information between one specific latent $z_i$ and one specific factor $g_j$, and low mutual information otherwise. This corresponds to the matrix $I$ being a sparse matrix that is a permutation of a diagonal matrix.\n\n**4. Permutation-Invariant Diagonal Dominance Score**\n\nTo quantify the degree of disentanglement in a way that is invariant to the arbitrary ordering of latent dimensions and factors, we define a score based on optimal assignment. The task is to find a one-to-one matching between a subset of latent variables and a subset of factors that maximizes the sum of their mutual information.\n\nThis is a classic **assignment problem** (also known as maximum weight bipartite matching). Given the cost matrix $I \\in \\mathbb{R}^{d_z \\times d_g}$, we want to find an assignment $\\mathcal{A}$ of $m = \\min(d_z, d_g)$ row indices to $m$ column indices, with no index repeated, such that the sum $\\sum_{(i,j) \\in \\mathcal{A}} I_{ij}$ is maximized. Standard algorithms, such as the Hungarian algorithm, solve the minimum cost assignment problem. To maximize the sum, we can solve the minimum cost assignment on the negative of the MI matrix, $-I$.\n\nLet $(r_1, c_1), \\dots, (r_m, c_m)$ be the index pairs of the optimal assignment, where $m = \\min(d_z, d_g)$.\nThe sum of entries for this optimal assignment is:\n$$ S_{\\text{assign}} = \\sum_{k=1}^m I_{r_k, c_k} $$\nThe total sum of all entries in the matrix is:\n$$ S_{\\text{all}} = \\sum_{i=1}^{d_z} \\sum_{j=1}^{d_g} I_{ij} $$\nThe diagonal dominance score $D$ is defined as the ratio of the optimally assigned MI sum to the total MI sum:\n$$ D = \\frac{S_{\\text{assign}}}{S_{\\text{all}}} $$\nThis score is normalized, $0 \\le D \\le 1$. A value of $D$ close to $1$ indicates that most of the mutual information is concentrated along a one-to-one mapping between latents and factors, signaling good disentanglement. A value of $D$ close to $1/m$ (for square matrices) or lower might indicate entanglement or poor information capture. As per the problem, if $S_{\\text{all}} \\le 10^{-12}$, we define $D=0$. This case arises if all variables are found to be perfectly independent, resulting in an MI matrix of all zeros.\nThe final assignment list of length $d_z$ is constructed by mapping each latent index $i$ to its assigned factor index $j$ if it is part of the optimal assignment, and to $-1$ otherwise.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linear_sum_assignment\nfrom scipy.stats import entropy\n\ndef solve():\n    \"\"\"\n    Solves the disentanglement-quantification problem for all test cases.\n    \"\"\"\n    \n    # Define test cases as per the problem statement.\n    test_cases = [\n        {\n            \"d_z\": 3, \"d_g\": 3, \"N\": 5000, \"s\": 13, \"b\": 12,\n            \"type\": \"ideal\",\n        },\n        {\n            \"d_z\": 3, \"d_g\": 3, \"N\": 5000, \"s\": 7, \"b\": 12,\n            \"type\": \"independent\",\n        },\n        {\n            \"d_z\": 3, \"d_g\": 3, \"N\": 5000, \"s\": 11, \"b\": 12,\n            \"type\": \"entangled\",\n            \"A\": np.array([[1.0, 0.8, 0.5], [0.6, 1.0, 0.4], [0.5, 0.4, 1.2]]),\n        },\n        {\n            \"d_z\": 2, \"d_g\": 3, \"N\": 5000, \"s\": 17, \"b\": 12,\n            \"type\": \"dim_mismatch\",\n            \"B\": np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]]),\n        },\n    ]\n\n    all_results = []\n\n    for case in test_cases:\n        d_z, d_g, N, s, b = case[\"d_z\"], case[\"d_g\"], case[\"N\"], case[\"s\"], case[\"b\"]\n        rng = np.random.default_rng(seed=s)\n\n        # 1. Generate data\n        g_samples = rng.uniform(low=-1.0, high=1.0, size=(N, d_g))\n\n        if case[\"type\"] == \"ideal\":\n            epsilon = rng.normal(loc=0.0, scale=1.0, size=(N, d_z))\n            z_samples = g_samples + 0.05 * epsilon\n        elif case[\"type\"] == \"independent\":\n            z_samples = rng.normal(loc=0.0, scale=1.0, size=(N, d_z))\n        elif case[\"type\"] == \"entangled\":\n            epsilon = rng.normal(loc=0.0, scale=1.0, size=(N, d_z))\n            A = case[\"A\"]\n            z_samples = (A @ g_samples.T).T + 0.2 * epsilon\n        elif case[\"type\"] == \"dim_mismatch\":\n            epsilon = rng.normal(loc=0.0, scale=1.0, size=(N, d_z))\n            B = case[\"B\"]\n            z_samples = (B @ g_samples.T).T + 0.05 * epsilon\n\n        # 2. Compute Mutual Information Matrix\n        mi_matrix = np.zeros((d_z, d_g))\n\n        for i in range(d_z):\n            for j in range(d_g):\n                z_i = z_samples[:, i]\n                g_j = g_samples[:, j]\n                \n                # Discretize and get joint PMF\n                # np.histogram2d binning is [left, right) except for the last bin which is [left, right], as required.\n                hist_2d, _, _ = np.histogram2d(\n                    z_i, g_j, bins=b, \n                    range=[[z_i.min(), z_i.max()], [g_j.min(), g_j.max()]]\n                )\n                \n                p_xy = hist_2d / N\n                \n                # Compute marginal PMFs\n                p_x = np.sum(p_xy, axis=1)\n                p_y = np.sum(p_xy, axis=0)\n\n                # Compute entropies using scipy.stats.entropy which handles p*log(p) = 0 for p=0\n                # Using natural log (base=e) as required.\n                h_x = entropy(p_x, base=np.e)\n                h_y = entropy(p_y, base=np.e)\n                h_xy = entropy(p_xy.flatten(), base=np.e)\n                \n                # Mutual Information I(X;Y) = H(X) + H(Y) - H(X,Y)\n                mi = h_x + h_y - h_xy\n                mi_matrix[i, j] = mi\n\n        # 3. Compute Permutation-Invariant Score\n        s_all = np.sum(mi_matrix)\n        \n        if s_all = 1e-12:\n            d_score = 0.0\n            assignment = [-1] * d_z\n        else:\n            # We want to maximize the sum, so we find the minimum cost assignment on the negative MI matrix.\n            cost_matrix = -mi_matrix\n            row_ind, col_ind = linear_sum_assignment(cost_matrix)\n            \n            # The sum of MI values for the optimal assignment\n            s_assign = mi_matrix[row_ind, col_ind].sum()\n            \n            d_score = s_assign / s_all\n            \n            # Create the assignment list (latent index -> factor index)\n            assignment = [-1] * d_z\n            for r, c in zip(row_ind, col_ind):\n                assignment[r] = c\n\n        # 4. Format results\n        D_rounded = round(d_score, 4)\n        I_flat_rounded = [round(val, 4) for val in mi_matrix.flatten(order='C')]\n        \n        all_results.append([D_rounded, assignment, I_flat_rounded])\n\n    # Final print statement must match the required format exactly.\n    # Custom formatting to avoid spaces after commas in lists.\n    result_str = \",\".join([\n        f\"[{res[0]},{str(res[1]).replace(' ', '')},{str(res[2]).replace(' ', '')}]\" for res in all_results\n    ])\n    print(f\"[{result_str}]\")\n\n\nsolve()\n```", "id": "3116952"}, {"introduction": "The balance between reconstruction quality and regularization is at the heart of the $\\beta$-VAE, but this balance can be fragile. In this practice, we investigate a common failure mode known as posterior collapse, where the model learns to ignore the latent code entirely [@problem_id:3116830]. By analytically solving a simplified linear-Gaussian case, you will see precisely how a decoder with too much capacity can undermine the goal of learning meaningful representations.", "problem": "Consider a Variational Autoencoder (VAE) with the $\\beta$-VAE objective, where the goal is to learn disentangled representations of independent ground-truth factors. The $\\beta$-VAE objective maximizes, over encoder and decoder distributions, the expected evidence lower bound (ELBO) modified by a scaling factor $\\beta$ on the Kullback-Leibler (KL) divergence. The ground-truth data generation process is as follows: there are $d$ independent latent factors $f_i \\sim \\mathcal{N}(0,1)$, aggregated as a vector $f \\in \\mathbb{R}^d$, and observable data $x \\in \\mathbb{R}^d$ generated by a diagonal linear mapping with additive Gaussian noise, namely $x = D f + \\epsilon$, where $D = \\operatorname{diag}(\\alpha_1, \\dots, \\alpha_d)$ with $\\alpha_i \\in \\mathbb{R}_{\\ge 0}$ and $\\epsilon \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2 I_d)$. Consider two decoders for the conditional likelihood $p_\\theta(x \\mid z)$ at the same $\\beta$: a simple decoder with limited capacity, modeled as a factorized Gaussian with mean $W z$ and fixed isotropic variance (i.e., $p_\\theta(x \\mid z) = \\mathcal{N}(W z, \\sigma_x^2 I_d)$ with $W = D$), and a powerful decoder with sufficient expressivity to model the exact marginal data distribution $p(x)$ independently of $z$ (e.g., an autoregressive model), meaning it can effectively make $p_\\theta(x \\mid z) = p(x)$ for all $z$. The encoder family is restricted to diagonal Gaussian $q_\\phi(z \\mid x) = \\mathcal{N}(B x, \\sigma_e^2 I_d)$, where $B = \\operatorname{diag}(b_1,\\dots,b_d)$ and $\\sigma_e^2$ may be optimized per dimension subject to the objective.\n\nYour tasks:\n\n- Starting from the definitions of the $\\beta$-VAE objective, the properties of Gaussian distributions, and standard linear-Gaussian modeling assumptions, derive the optimal encoder parameters $b_i$ and $\\sigma_{e,i}^2$ that maximize the expected $\\beta$-VAE objective for the simple decoder $p_\\theta(x \\mid z) = \\mathcal{N}(D z, \\sigma_x^2 I_d)$, under the given ground-truth data model. Use the fact that the expectation is over the true data distribution $p_{\\text{data}}(x)$ implied by $f \\sim \\mathcal{N}(0,I_d)$ and $x = D f + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2 I_d)$, and restrict attention to solutions in the given encoder family $q_\\phi(z \\mid x) = \\mathcal{N}(B x, \\sigma_e^2 I_d)$.\n\n- Using the derived optimal encoder parameters for the simple decoder, compute for each dimension $i$ the mutual information (in natural logarithm units, i.e., nats) between the learned latent variable $z_i$ and its corresponding ground-truth factor $f_i$. Define the Mutual Information Gap (MIG) per dimension as the difference between the largest mutual information and the second largest mutual information across factors for the given latent; in the specified diagonal setting this simplifies appropriately. Report the average MIG across dimensions.\n\n- For the powerful decoder that can match the data marginal $p(x)$ independently of $z$ at the same $\\beta$, justify the optimal encoder behavior and compute the average MIG across dimensions.\n\n- Implement a program that, for each test case in the test suite below, computes the average MIG for the simple decoder and the powerful decoder at the same $\\beta$, and then outputs a single list containing, for each test case, the difference between the average MIG of the simple decoder and the average MIG of the powerful decoder. Express all mutual information quantities in nats.\n\nUse the following test suite, which varies the number of dimensions, the diagonal map $D$ entries, the encoder-decoder noise levels, and the $\\beta$ value to test different behaviors:\n\n- Test case $1$: $d = 3$, $\\alpha = [1.0, 0.8, 1.2]$, $\\sigma_\\epsilon^2 = 0.1$, $\\sigma_x^2 = 0.2$, $\\beta = 4.5$.\n\n- Test case $2$: $d = 2$, $\\alpha = [0.0, 0.5]$, $\\sigma_\\epsilon^2 = 0.2$, $\\sigma_x^2 = 0.2$, $\\beta = 4.5$.\n\n- Test case $3$: $d = 4$, $\\alpha = [1.0, 1.0, 1.0, 1.0]$, $\\sigma_\\epsilon^2 = 0.1$, $\\sigma_x^2 = 0.2$, $\\beta = 50.0$.\n\nFinal output format requirement: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3]$), where each $r_k$ is the difference between the average MIG of the simple decoder and the average MIG of the powerful decoder, in nats, for test case $k$.", "solution": "The core task is to analyze the disentangling capabilities of a $\\beta$-VAE under two different decoder assumptions (simple and powerful) by computing the Mutual Information Gap (MIG). The problem structure, with its diagonal-matrix-based linear-Gaussian models, allows for a full analytical derivation. The solution proceeds by first deriving the optimal encoder parameters for each case and then using these parameters to compute the required mutual information quantities.\n\nThe $\\beta$-VAE objective to be maximized is given by:\n$$\n\\mathcal{L}(\\phi, \\theta) = \\mathbb{E}_{p_{\\text{data}}(x)} \\left[ \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - \\beta \\text{KL}(q_\\phi(z|x) \\| p(z)) \\right]\n$$\nwhere $p(z) = \\mathcal{N}(0, I_d)$ is the latent prior. The models are defined such that all matrices are diagonal, which means the objective function decomposes into a sum of per-dimension terms, $\\mathcal{L} = \\sum_{i=1}^d \\mathcal{L}_i$. We can therefore analyze each dimension $i$ independently.\n\nThe models for dimension $i$ are:\n- Ground-truth factor: $f_i \\sim \\mathcal{N}(0, 1)$.\n- Data generation: $x_i = \\alpha_i f_i + \\epsilon_i$, where $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2)$. This implies that $x_i$ is a Gaussian random variable, $x_i \\sim \\mathcal{N}(0, \\alpha_i^2 + \\sigma_\\epsilon^2)$. We denote the variance of the data generating process for $x_i$ as $V_{x_i} = \\alpha_i^2 + \\sigma_\\epsilon^2$.\n- Encoder: $q_\\phi(z_i|x_i) = \\mathcal{N}(b_i x_i, \\sigma_{e,i}^2)$.\n- Simple Decoder: $p_\\theta(x_i|z_i) = \\mathcal{N}(\\alpha_i z_i, \\sigma_x^2)$.\n- Prior: $p(z_i) = \\mathcal{N}(0, 1)$.\n\nThe per-dimension objective is:\n$$\n\\mathcal{L}_i = \\mathbb{E}_{p(x_i)} \\left[ \\mathbb{E}_{q(z_i|x_i)}[\\log p(x_i|z_i)] - \\beta \\text{KL}(q(z_i|x_i) \\| p(z_i)) \\right]\n$$\n\n### Part 1: Simple Decoder Analysis\n\nFirst, we derive the optimal encoder parameters $b_i$ and $\\sigma_{e,i}^2$ that maximize $\\mathcal{L}_i$ for the simple decoder.\n\nThe reconstruction term's expectation is:\n$$\n\\mathbb{E}_{p(x_i)} \\mathbb{E}_{q(z_i|x_i)}[\\log p(x_i|z_i)] = \\mathbb{E}_{p(x_i), q(z_i|x_i)}\\left[-\\frac{(x_i - \\alpha_i z_i)^2}{2\\sigma_x^2} - \\frac{1}{2}\\log(2\\pi\\sigma_x^2)\\right]\n$$\nThe quadratic term expands to $\\mathbb{E}[x_i^2 - 2\\alpha_i x_i z_i + \\alpha_i^2 z_i^2]$. Taking the expectation over the joint distribution induced by $p(x_i)$ and $q(z_i|x_i)$:\n$\\mathbb{E}[(x_i - \\alpha_i z_i)^2] = \\text{Var}(x_i - \\alpha_i z_i) = \\text{Var}(x_i) - 2\\alpha_i\\text{Cov}(x_i,z_i) + \\alpha_i^2\\text{Var}(z_i)$.\nWe have $\\text{Var}(x_i) = V_{x_i}$, $\\text{Cov}(x_i, z_i) = b_i V_{x_i}$, and $\\text{Var}(z_i) = \\sigma_{e,i}^2 + b_i^2 V_{x_i}$.\nThis gives $\\mathbb{E}[(x_i - \\alpha_i z_i)^2] = V_{x_i} - 2\\alpha_i b_i V_{x_i} + \\alpha_i^2(\\sigma_{e,i}^2 + b_i^2 V_{x_i}) = V_{x_i}(1 - \\alpha_i b_i)^2 + \\alpha_i^2 \\sigma_{e,i}^2$.\nSo, the reconstruction term is $-\\frac{1}{2\\sigma_x^2}[V_{x_i}(1 - \\alpha_i b_i)^2 + \\alpha_i^2 \\sigma_{e,i}^2] + C_1$.\n\nThe KL-divergence term is:\n$$\n\\mathbb{E}_{p(x_i)}[\\text{KL}(\\mathcal{N}(b_i x_i, \\sigma_{e,i}^2) \\| \\mathcal{N}(0, 1))] = \\mathbb{E}_{p(x_i)}\\left[\\frac{1}{2}\\left(\\sigma_{e,i}^2 + (b_i x_i)^2 - 1 - \\log(\\sigma_{e,i}^2)\\right)\\right]\n$$\nThis results in $\\frac{1}{2}(\\sigma_{e,i}^2 + b_i^2 V_{x_i} - 1 - \\log(\\sigma_{e,i}^2))$.\n\nCombining these, the part of $\\mathcal{L}_i$ dependent on $b_i$ and $\\sigma_{e,i}^2$ is:\n$$\n\\mathcal{L}_i(b_i, \\sigma_{e,i}^2) = -\\frac{V_{x_i}(1 - \\alpha_i b_i)^2}{2\\sigma_x^2} - \\frac{\\alpha_i^2 \\sigma_{e,i}^2}{2\\sigma_x^2} - \\frac{\\beta}{2}(\\sigma_{e,i}^2 + b_i^2 V_{x_i} - \\log(\\sigma_{e,i}^2)) + C_2\n$$\n\nTo find the optimal parameters, we take partial derivatives and set them to zero.\n$\\frac{\\partial \\mathcal{L}_i}{\\partial b_i} = -\\frac{V_{x_i}}{2\\sigma_x^2} \\cdot 2(1-\\alpha_i b_i)(-\\alpha_i) - \\frac{\\beta}{2}(2b_i V_{x_i}) = 0$.\nAssuming $V_{x_i} = \\alpha_i^2 + \\sigma_\\epsilon^2 > 0$, we can divide by $V_{x_i}$:\n$$\n\\frac{\\alpha_i(1-\\alpha_i b_i)}{\\sigma_x^2} - \\beta b_i = 0 \\implies \\frac{\\alpha_i}{\\sigma_x^2} = b_i \\left(\\frac{\\alpha_i^2}{\\sigma_x^2} + \\beta\\right) \\implies b_i^* = \\frac{\\alpha_i}{\\alpha_i^2 + \\beta \\sigma_x^2}\n$$\n$\\frac{\\partial \\mathcal{L}_i}{\\partial \\sigma_{e,i}^2} = -\\frac{\\alpha_i^2}{2\\sigma_x^2} - \\frac{\\beta}{2} + \\frac{\\beta}{2\\sigma_{e,i}^2} = 0$.\n$$\n\\frac{\\beta}{\\sigma_{e,i}^2} = \\frac{\\alpha_i^2}{\\sigma_x^2} + \\beta = \\frac{\\alpha_i^2 + \\beta\\sigma_x^2}{\\sigma_x^2} \\implies (\\sigma_{e,i}^2)^* = \\frac{\\beta \\sigma_x^2}{\\alpha_i^2 + \\beta \\sigma_x^2}\n$$\n\nNext, we compute the mutual information $I(z_i; f_i)$. The system is a linear-Gaussian cascade: $f_i \\to x_i \\to z_i$. The mutual information for jointly Gaussian variables is $I(z_i; f_i) = \\frac{1}{2} \\log \\frac{\\text{Var}(z_i)}{\\text{Var}(z_i|f_i)}$.\nUsing the law of total variance: $\\text{Var}(z_i) = \\text{Var}(\\mathbb{E}[z_i|f_i]) + \\mathbb{E}[\\text{Var}(z_i|f_i)]$.\nThe conditional moments are $\\mathbb{E}[z_i|f_i] = b_i \\alpha_i f_i$ and $\\text{Var}(z_i|f_i) = \\sigma_{e,i}^2 + b_i^2 \\sigma_\\epsilon^2$, which is constant w.r.t $f_i$.\nSo, $\\text{Var}(z_i) = \\text{Var}(b_i \\alpha_i f_i) + (\\sigma_{e,i}^2 + b_i^2 \\sigma_\\epsilon^2) = b_i^2 \\alpha_i^2 \\text{Var}(f_i) + \\text{Var}(z_i|f_i) = b_i^2 \\alpha_i^2 + \\text{Var}(z_i|f_i)$, since $\\text{Var}(f_i)=1$.\nThe mutual information is:\n$$\nI(z_i; f_i) = \\frac{1}{2} \\log \\left(\\frac{\\text{Var}(z_i|f_i) + b_i^2 \\alpha_i^2}{\\text{Var}(z_i|f_i)}\\right) = \\frac{1}{2} \\log \\left(1 + \\frac{b_i^2 \\alpha_i^2}{\\sigma_{e,i}^2 + b_i^2 \\sigma_\\epsilon^2}\\right)\n$$\nSubstituting the optimal parameters $b_i^*$ and $(\\sigma_{e,i}^2)^*$:\nLet $A = \\alpha_i^2 + \\beta \\sigma_x^2$. Then $b_i^* = \\alpha_i/A$ and $(\\sigma_{e,i}^2)^* = \\beta\\sigma_x^2/A$.\nThe argument of the logarithm becomes:\n$1 + \\frac{(\\alpha_i/A)^2 \\alpha_i^2}{(\\beta\\sigma_x^2/A) + (\\alpha_i/A)^2 \\sigma_\\epsilon^2} = 1 + \\frac{\\alpha_i^4/A^2}{(\\beta\\sigma_x^2 A + \\alpha_i^2 \\sigma_\\epsilon^2)/A^2} = 1 + \\frac{\\alpha_i^4}{\\beta\\sigma_x^2(\\alpha_i^2 + \\beta\\sigma_x^2) + \\alpha_i^2 \\sigma_\\epsilon^2}$.\n$$\nI(z_i; f_i) = \\frac{1}{2} \\log \\left( 1 + \\frac{\\alpha_i^4}{\\alpha_i^2(\\beta\\sigma_x^2 + \\sigma_\\epsilon^2) + \\beta^2(\\sigma_x^2)^2} \\right)\n$$\nDue to the diagonal structure of all transformations in the problem setup, the only non-zero mutual information for latent variable $z_i$ will be with its corresponding factor $f_i$, i.e., $I(z_i; f_j) = 0$ for $j \\neq i$. Therefore, the MIG for latent variable $z_i$ simplifies to $I(z_i; f_i)$ itself. The average MIG for the simple decoder is thus:\n$$\n\\text{MIG}_{\\text{simple}} = \\frac{1}{d} \\sum_{i=1}^d I(z_i; f_i)\n$$\n\n### Part 2: Powerful Decoder Analysis\n\nFor the powerful decoder, $p_\\theta(x|z) = p(x)$. The objective function becomes:\n$$\n\\mathcal{L}_i = \\mathbb{E}_{p(x_i)}[\\log p(x_i)] - \\beta \\mathbb{E}_{p(x_i)}[\\text{KL}(q(z_i|x_i) \\| p(z_i))]\n$$\nThe first term is the negative entropy of the data, which is a constant with respect to the encoder parameters $(b_i, \\sigma_{e,i}^2)$. Maximizing $\\mathcal{L}_i$ is equivalent to minimizing $\\mathbb{E}_{p(x_i)}[\\text{KL}(q(z_i|x_i) \\| p(z_i))]$.\nSince KL-divergence is non-negative, the minimum is $0$, achieved when $q(z_i|x_i) = p(z_i)$ for all $x_i$.\nComparing $q(z_i|x_i) = \\mathcal{N}(b_i x_i, \\sigma_{e,i}^2)$ to $p(z_i) = \\mathcal{N}(0, 1)$, we must have $b_i x_i = 0$ for all $x_i$, which implies $b_i^*=0$, and $\\sigma_{e,i}^2 = 1$.\nThis phenomenon, where the latent variables become independent of the data to minimize the KL penalty, is known as \"posterior collapse.\"\nWith $b_i^*=0$, the mutual information formula immediately gives $I(z_i; f_i) = \\frac{1}{2}\\log(1+0) = 0$.\nTherefore, the average MIG for the powerful decoder is:\n$$\n\\text{MIG}_{\\text{powerful}} = \\frac{1}{d} \\sum_{i=1}^d 0 = 0\n$$\n\n### Part 3: Final Calculation\nThe problem asks for the difference between the average MIG of the simple decoder and that of the powerful decoder for each test case.\n$$\n\\Delta\\text{MIG} = \\text{MIG}_{\\text{simple}} - \\text{MIG}_{\\text{powerful}} = \\text{MIG}_{\\text{simple}} - 0 = \\text{MIG}_{\\text{simple}}\n$$\nThis is the quantity to be computed by the program for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test suite.\n    Derives optimal encoder parameters for a beta-VAE with a simple linear decoder,\n    computes the resulting average Mutual Information Gap (MIG), and finds the\n    difference with the MIG from a powerful decoder, which is always zero.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1: d=3, alpha=[1.0, 0.8, 1.2], sigma_eps^2=0.1, sigma_x^2=0.2, beta=4.5\n        {\n            \"d\": 3,\n            \"alpha\": [1.0, 0.8, 1.2],\n            \"sigma_eps_sq\": 0.1,\n            \"sigma_x_sq\": 0.2,\n            \"beta\": 4.5,\n        },\n        # Test case 2: d=2, alpha=[0.0, 0.5], sigma_eps^2=0.2, sigma_x^2=0.2, beta=4.5\n        {\n            \"d\": 2,\n            \"alpha\": [0.0, 0.5],\n            \"sigma_eps_sq\": 0.2,\n            \"sigma_x_sq\": 0.2,\n            \"beta\": 4.5,\n        },\n        # Test case 3: d=4, alpha=[1.0, 1.0, 1.0, 1.0], sigma_eps^2=0.1, sigma_x^2=0.2, beta=50.0\n        {\n            \"d\": 4,\n            \"alpha\": [1.0, 1.0, 1.0, 1.0],\n            \"sigma_eps_sq\": 0.1,\n            \"sigma_x_sq\": 0.2,\n            \"beta\": 50.0,\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        d = case[\"d\"]\n        alpha_vec = case[\"alpha\"]\n        sigma_eps_sq = case[\"sigma_eps_sq\"]\n        sigma_x_sq = case[\"sigma_x_sq\"]\n        beta = case[\"beta\"]\n\n        # For the simple decoder, the average MIG is the average of I(z_i; f_i)\n        # across dimensions. For the powerful decoder, the average MIG is 0.\n        # The required difference is therefore just the average MIG of the simple decoder.\n        \n        mutual_informations = []\n        for i in range(d):\n            alpha_i = alpha_vec[i]\n\n            # If alpha_i is 0, then x_i contains no information about f_i,\n            # so I(z_i; f_i) is 0.\n            if alpha_i == 0.0:\n                mi_i = 0.0\n            else:\n                alpha_i_sq = alpha_i**2\n                alpha_i_4 = alpha_i**4\n                beta_sq = beta**2\n                sigma_x_sq_sq = sigma_x_sq**2\n\n                # This is the derived formula for I(z_i; f_i) in nats.\n                # I(z_i; f_i) = 0.5 * log(1 + (alpha_i^4) /\n                # (alpha_i^2 * (beta*sigma_x^2 + sigma_eps^2) + beta^2 * (sigma_x^2)^2))\n                numerator = alpha_i_4\n                denominator = alpha_i_sq * (beta * sigma_x_sq + sigma_eps_sq) + beta_sq * sigma_x_sq_sq\n                \n                mi_i = 0.5 * np.log(1.0 + numerator / denominator)\n            \n            mutual_informations.append(mi_i)\n        \n        avg_mig_simple = np.mean(mutual_informations)\n        \n        # The average MIG for the powerful decoder is 0 due to posterior collapse.\n        avg_mig_powerful = 0.0\n        \n        difference = avg_mig_simple - avg_mig_powerful\n        results.append(difference)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.16f}' for r in results)}]\")\n\nsolve()\n```", "id": "3116830"}, {"introduction": "A static regularization weight $\\beta$ applies the same pressure throughout training, which may not be optimal for learning disentangled representations. This practice explores a more sophisticated approach: gradually increasing the pressure to regularize via an adaptive capacity schedule [@problem_id:3116925]. You will compare this dynamic technique to the standard static-$\\beta$ VAE, using a stylized model to reveal how carefully managing the information budget over time can lead to superior disentanglement.", "problem": "You are tasked with designing and evaluating an adaptive capacity schedule for a Variational Autoencoder (VAE) to encourage disentangled representations. Begin from the following foundations and use only mathematically defined quantities.\n\nFundamental base:\n- A Variational Autoencoder (VAE) optimizes the Evidence Lower Bound (ELBO). For a latent variable model with prior $p(\\mathbf{z})$ and approximate posterior $q(\\mathbf{z} \\mid \\mathbf{x})$, the negative ELBO per data point decomposes into a reconstruction term plus a regularization term:\n$$\n\\mathcal{L}_{\\text{ELBO}} = \\mathbb{E}_{q(\\mathbf{z} \\mid \\mathbf{x})}\\left[\\ell_{\\text{rec}}(\\mathbf{x}, \\mathbf{z})\\right] + \\beta \\, \\mathrm{KL}\\left(q(\\mathbf{z} \\mid \\mathbf{x}) \\, \\| \\, p(\\mathbf{z})\\right),\n$$\nwhere $\\mathrm{KL}$ denotes Kullback–Leibler divergence (KL), and $\\beta \\ge 0$ is a scalar. The reconstruction term is a well-tested surrogate (e.g., squared error under Gaussian likelihood).\n- For a standard-normal prior $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ and a diagonal Gaussian approximate posterior $q(\\mathbf{z} \\mid \\mathbf{x}) = \\mathcal{N}(\\boldsymbol{\\mu}(\\mathbf{x}), \\mathrm{diag}(\\boldsymbol{\\sigma}^2))$, the per-dimension KL is\n$$\n\\mathrm{KL}_i = \\frac{1}{2}\\left(\\mathbb{E}_{\\mathbf{x}}[\\mu_i(\\mathbf{x})^2] + \\sigma_i^2 - \\ln \\sigma_i^2 - 1\\right).\n$$\n\nStylized linear-Gaussian setting:\n- Observed data $\\mathbf{x} \\in \\mathbb{R}^2$ is generated by two independent Gaussian factors: $x_1 \\sim \\mathcal{N}(0, s_1^2)$ and $x_2 \\sim \\mathcal{N}(0, s_2^2)$ with covariance $\\mathrm{Cov}(\\mathbf{x}) = \\mathrm{diag}(s_1^2, s_2^2)$.\n- The encoder mean is linear: $\\boldsymbol{\\mu}(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}$ with $\\mathbf{A}$ constrained to be a rotation by angle $\\theta$, that is $\\mathbf{A} = \\mathbf{R}(\\theta)$, and the decoder is linear: $\\hat{\\mathbf{x}} = \\mathbf{W}\\mathbf{z}$ with $\\mathbf{W}$ as a rotation by a fixed angle $\\varphi$, that is $\\mathbf{W} = \\mathbf{R}(\\varphi)$. All angles are specified in radians.\n- The expected squared reconstruction error under this model is\n$$\nR(\\theta) = \\mathbb{E}\\left[\\|\\mathbf{x} - \\mathbf{W}\\boldsymbol{\\mu}(\\mathbf{x})\\|_2^2\\right] + \\mathrm{trace}\\!\\left(\\mathbf{W}\\,\\mathrm{Cov}(\\mathbf{z}\\mid \\mathbf{x})\\,\\mathbf{W}^\\top\\right).\n$$\nWith $\\mathbf{A} = \\mathbf{R}(\\theta)$, $\\mathbf{W} = \\mathbf{R}(\\varphi)$, and $\\mathrm{Cov}(\\mathbf{z}\\mid \\mathbf{x}) = \\mathrm{diag}(\\sigma_1^2, \\sigma_2^2)$, this reduces to\n$$\nR(\\theta) = 2\\bigl(1 - \\cos(\\theta + \\varphi)\\bigr)(s_1^2 + s_2^2) + \\sigma_1^2 + \\sigma_2^2.\n$$\n- The encoder’s per-dimension mean energies are\n$$\n\\mathbb{E}_{\\mathbf{x}}[\\mu_1(\\mathbf{x})^2] = \\cos^2\\!\\theta \\, s_1^2 + \\sin^2\\!\\theta \\, s_2^2,\\quad\n\\mathbb{E}_{\\mathbf{x}}[\\mu_2(\\mathbf{x})^2] = \\sin^2\\!\\theta \\, s_1^2 + \\cos^2\\!\\theta \\, s_2^2.\n$$\n\nAdaptive capacity schedule:\n- Replace the fixed $\\beta$ with per-dimension target KL budgets $C_1(t)$ and $C_2(t)$ that anneal in time $t$ by a linear ramp saturating at maxima:\n$$\nC_i(t) = \\min\\!\\bigl(C_{i,\\max}, \\kappa_i \\, t\\bigr),\\quad i \\in \\{1,2\\},\n$$\nwhere $\\kappa_i  0$ and $C_{i,\\max}  0$ are given constants.\n\nObjectives to compare:\n- Static-$\\beta$ objective (negative ELBO):\n$$\n\\mathcal{L}_\\beta(\\theta, \\sigma_1^2, \\sigma_2^2) = R(\\theta) + \\beta\\left(\\mathrm{KL}_1 + \\mathrm{KL}_2\\right).\n$$\n- Adaptive-capacity objective with quadratic penalty on budget deviation:\n$$\n\\mathcal{L}_{\\text{cap}}(\\theta, \\sigma_1^2, \\sigma_2^2; t) = R(\\theta) + \\gamma \\sum_{i=1}^{2}\\left(\\mathrm{KL}_i - C_i(t)\\right)^2,\n$$\nwhere $\\gamma  0$ is a given penalty weight.\n\nDisentanglement score:\n- Define a disentanglement score as a function of the encoder rotation angle $\\theta$:\n$$\nD(\\theta) = 1 - \\left|\\sin(2\\theta)\\right|.\n$$\nThis score attains $1$ when each latent primarily aligns with a single factor ($\\theta = 0$ or $\\theta = \\frac{\\pi}{2}$ up to permutation) and $0$ at maximal mixing ($\\theta = \\frac{\\pi}{4}$).\n\nTasks:\n1. Using the provided foundations, derive the expressions for $R(\\theta)$ and $\\mathrm{KL}_i$ in this setting. Then design the annealing schedule $C_i(t)$ as specified above.\n2. For the adaptive capacity objective at a given time $t$, determine the encoder angle $\\theta_{\\text{cap}}(t)$ and variances $\\sigma_1^2, \\sigma_2^2$ that minimize $\\mathcal{L}_{\\text{cap}}$ over $\\theta \\in [-\\pi, \\pi]$ and $\\sigma_i^2  0$.\n3. For the static-$\\beta$ objective, show that the optimal angle satisfies $\\theta_\\beta^* = -\\varphi$ because $\\mathrm{KL}_1 + \\mathrm{KL}_2$ is independent of $\\theta$, and derive the optimal $\\sigma_i^2$ given $\\beta$ by minimizing $\\mathcal{L}_\\beta$ with respect to $\\sigma_i^2$.\n4. For a fixed time $t$, choose $\\beta$ so that the minimized static-$\\beta$ loss equals the minimized adaptive-capacity loss, thereby ensuring a fair comparison at fixed ELBO value. Then compute and compare the disentanglement scores $D(\\theta_{\\text{cap}}(t))$ and $D(\\theta_\\beta^*)$.\n5. For each test case in the suite below, output a boolean that is true if the adaptive capacity achieves strictly higher disentanglement score than static $\\beta$ under equal minimized loss at time $t$, and false otherwise.\n\nAngle unit:\n- All angles must be in radians.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true,false,true]\"), but with Python boolean literals \"True\" and \"False\".\n\nTest suite:\n- Case 1 (happy path): $(s_1, s_2, \\varphi, \\kappa_1, \\kappa_2, C_{1,\\max}, C_{2,\\max}, \\gamma, t) = (1.5, 0.3, 0.7, 0.7, 0.1, 1.0, 0.1, 20.0, 2.0)$.\n- Case 2 (boundary: no decoder mixing): $(s_1, s_2, \\varphi, \\kappa_1, \\kappa_2, C_{1,\\max}, C_{2,\\max}, \\gamma, t) = (1.0, 0.8, 0.0, 0.3, 0.3, 0.5, 0.5, 20.0, 1.0)$.\n- Case 3 (edge: equal budgets, small decoder mixing): $(s_1, s_2, \\varphi, \\kappa_1, \\kappa_2, C_{1,\\max}, C_{2,\\max}, \\gamma, t) = (1.5, 0.3, 0.1, 0.3, 0.3, 0.2, 0.2, 20.0, 1.0)$.\n\nAnswer specification:\n- The answer for each case is a boolean in the final output list indicating whether $D(\\theta_{\\text{cap}}(t))  D(\\theta_\\beta^*)$ at matched minimized loss (fixed ELBO value).", "solution": "We proceed step by step, starting from core definitions and deriving the necessary expressions and algorithms.\n\nStep 1: Reconstruction and Kullback–Leibler divergence in the stylized model.\nThe reconstruction term under a linear decoder $\\mathbf{W} = \\mathbf{R}(\\varphi)$ and linear encoder mean $\\boldsymbol{\\mu}(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}$ with $\\mathbf{A} = \\mathbf{R}(\\theta)$ is\n$$\nR(\\theta) = \\mathbb{E}\\left[\\|\\mathbf{x} - \\mathbf{W}\\boldsymbol{\\mu}(\\mathbf{x})\\|^2\\right] + \\mathrm{trace}\\!\\left(\\mathbf{W}\\,\\mathrm{Cov}(\\mathbf{z}\\mid \\mathbf{x})\\,\\mathbf{W}^\\top\\right).\n$$\nSince both $\\mathbf{W}$ and $\\mathbf{A}$ are rotations, their product is a rotation by the sum of angles, $\\mathbf{W}\\mathbf{A} = \\mathbf{R}(\\varphi)\\mathbf{R}(\\theta) = \\mathbf{R}(\\theta + \\varphi)$. For zero-mean Gaussian $\\mathbf{x}$ with covariance $\\mathrm{diag}(s_1^2, s_2^2)$, the expected squared error term simplifies to the trace of a quadratic form:\n$$\n\\mathbb{E}\\left[\\|\\mathbf{x} - \\mathbf{W}\\boldsymbol{\\mu}(\\mathbf{x})\\|^2\\right]\n= \\mathrm{trace}\\!\\left((\\mathbf{I} - \\mathbf{R}(\\theta + \\varphi))^\\top(\\mathbf{I}-\\mathbf{R}(\\theta + \\varphi))\\,\\mathrm{diag}(s_1^2, s_2^2)\\right).\n$$\nFor a rotation by angle $\\alpha$, one can show $(\\mathbf{I}-\\mathbf{R}(\\alpha))^\\top(\\mathbf{I}-\\mathbf{R}(\\alpha))$ has both column norms equal to $2(1 - \\cos\\alpha)$, and thus the trace weighted by $\\mathrm{diag}(s_1^2, s_2^2)$ is\n$$\n2(1 - \\cos(\\theta + \\varphi))(s_1^2 + s_2^2).\n$$\nThe covariance term for $\\mathbf{W}$ orthonormal is simply $\\sigma_1^2 + \\sigma_2^2$. Therefore,\n$$\nR(\\theta) = 2\\bigl(1 - \\cos(\\theta + \\varphi)\\bigr)(s_1^2 + s_2^2) + \\sigma_1^2 + \\sigma_2^2.\n$$\n\nFor the Kullback–Leibler divergence to the standard-normal prior, per dimension, with $q(\\mathbf{z}\\mid \\mathbf{x}) = \\mathcal{N}(\\boldsymbol{\\mu}(\\mathbf{x}), \\mathrm{diag}(\\sigma_1^2, \\sigma_2^2))$, the well-tested formula is\n$$\n\\mathrm{KL}_i = \\frac{1}{2}\\left(\\mathbb{E}_{\\mathbf{x}}[\\mu_i(\\mathbf{x})^2] + \\sigma_i^2 - \\ln \\sigma_i^2 - 1\\right),\\quad i \\in \\{1,2\\}.\n$$\nThe encoder mean energies follow from linearity and independence:\n$$\n\\mathbb{E}_{\\mathbf{x}}[\\mu_1(\\mathbf{x})^2] = \\cos^2\\!\\theta \\, s_1^2 + \\sin^2\\!\\theta \\, s_2^2,\\quad\n\\mathbb{E}_{\\mathbf{x}}[\\mu_2(\\mathbf{x})^2] = \\sin^2\\!\\theta \\, s_1^2 + \\cos^2\\!\\theta \\, s_2^2.\n$$\n\nStep 2: Adaptive capacity schedule and objective.\nWe design a time-dependent budget per dimension as a linear ramp saturating at a maximum:\n$$\nC_i(t) = \\min\\!\\bigl(C_{i,\\max}, \\kappa_i \\, t\\bigr).\n$$\nThe adaptive-capacity objective at time $t$ is\n$$\n\\mathcal{L}_{\\text{cap}}(\\theta, \\sigma_1^2, \\sigma_2^2; t) =\n2\\bigl(1 - \\cos(\\theta + \\varphi)\\bigr)(s_1^2 + s_2^2) + \\sigma_1^2 + \\sigma_2^2\n+ \\gamma \\sum_{i=1}^{2}\\left(\\frac{1}{2}\\left(\\mathbb{E}_{\\mathbf{x}}[\\mu_i(\\mathbf{x})^2] + \\sigma_i^2 - \\ln \\sigma_i^2 - 1\\right) - C_i(t)\\right)^2.\n$$\nFor fixed $\\theta$, the terms in $\\sigma_1^2$ and $\\sigma_2^2$ separate, so each $\\sigma_i^2$ can be optimized independently by minimizing the scalar function\n$$\nJ_i(\\sigma_i^2;\\theta,t) = \\sigma_i^2 + \\gamma\\left(\\frac{1}{2}\\left(\\mathbb{E}_{\\mathbf{x}}[\\mu_i(\\mathbf{x})^2] + \\sigma_i^2 - \\ln \\sigma_i^2 - 1\\right) - C_i(t)\\right)^2.\n$$\nThis scalar problem has no closed form due to the logarithmic term, but it is convex for practically relevant ranges and can be efficiently minimized numerically.\n\nFinally, we minimize $\\mathcal{L}_{\\text{cap}}$ over $\\theta \\in [-\\pi,\\pi]$ by numerical one-dimensional search, yielding $(\\theta_{\\text{cap}}(t), \\sigma_1^2(t), \\sigma_2^2(t))$ and the minimized value $\\mathcal{L}_{\\text{cap}}^{\\star}(t)$.\n\nStep 3: Static-$\\beta$ objective and its minimizers.\nThe static-$\\beta$ objective is\n$$\n\\mathcal{L}_{\\beta}(\\theta, \\sigma_1^2, \\sigma_2^2) =\n2\\bigl(1 - \\cos(\\theta + \\varphi)\\bigr)(s_1^2 + s_2^2) + \\sigma_1^2 + \\sigma_2^2\n+ \\beta \\sum_{i=1}^2 \\frac{1}{2}\\left(\\mathbb{E}_{\\mathbf{x}}[\\mu_i(\\mathbf{x})^2] + \\sigma_i^2 - \\ln \\sigma_i^2 - 1\\right).\n$$\nNote that $\\mathbb{E}_{\\mathbf{x}}[\\mu_1(\\mathbf{x})^2] + \\mathbb{E}_{\\mathbf{x}}[\\mu_2(\\mathbf{x})^2] = s_1^2 + s_2^2$ is independent of $\\theta$ because $\\mathbf{A}$ is orthonormal. Therefore, the $\\theta$-dependent part of $\\mathcal{L}_\\beta$ is only the reconstruction term, which is minimized at\n$$\n\\theta_\\beta^* = -\\varphi.\n$$\nFor each $\\sigma_i^2$, the derivative of the $\\sigma_i^2$-dependent part is\n$$\n\\frac{\\partial}{\\partial \\sigma_i^2}\\left(\\sigma_i^2 + \\frac{\\beta}{2}(\\sigma_i^2 - \\ln \\sigma_i^2 - 1)\\right) = 1 + \\frac{\\beta}{2}\\left(1 - \\frac{1}{\\sigma_i^2}\\right).\n$$\nSetting to zero yields\n$$\n1 + \\frac{\\beta}{2} - \\frac{\\beta}{2}\\frac{1}{\\sigma_i^2} = 0\n\\quad\\Rightarrow\\quad\n\\sigma_i^{2\\,*}(\\beta) = \\frac{\\beta}{\\beta + 2}.\n$$\nThus the optimal posterior variances are equal and independent of $\\theta$ and the data variances in this stylized setting.\n\nThe minimized static-$\\beta$ loss is then\n$$\n\\mathcal{L}_\\beta^{\\star}(\\beta) =\n2\\bigl(1 - \\cos(0)\\bigr)(s_1^2 + s_2^2) + 2\\,\\sigma^{2\\,*}(\\beta) + \\beta\\sum_{i=1}^2 \\frac{1}{2}\\left(\\mathbb{E}_{\\mathbf{x}}[\\mu_i(\\mathbf{x})^2] + \\sigma^{2\\,*}(\\beta) - \\ln \\sigma^{2\\,*}(\\beta) - 1\\right),\n$$\nwith the reconstruction term evaluated at $\\theta_\\beta^* + \\varphi = 0$. The encoder mean energies for $\\theta_\\beta^* = -\\varphi$ are\n$$\n\\mathbb{E}_{\\mathbf{x}}[\\mu_1(\\mathbf{x})^2] = \\cos^2\\!\\varphi \\, s_1^2 + \\sin^2\\!\\varphi \\, s_2^2,\\quad\n\\mathbb{E}_{\\mathbf{x}}[\\mu_2(\\mathbf{x})^2] = \\sin^2\\!\\varphi \\, s_1^2 + \\cos^2\\!\\varphi \\, s_2^2.\n$$\n\nStep 4: Matching fixed ELBO values for fair comparison.\nAt a given time $t$, we first compute the minimized adaptive-capacity loss $\\mathcal{L}_{\\text{cap}}^{\\star}(t)$ and its minimizer $\\theta_{\\text{cap}}(t)$. Then, we select $\\beta$ so that the minimized static-$\\beta$ loss equals $\\mathcal{L}_{\\text{cap}}^{\\star}(t)$:\n$$\n\\mathcal{L}_\\beta^{\\star}(\\beta) = \\mathcal{L}_{\\text{cap}}^{\\star}(t).\n$$\nThis scalar equation in $\\beta$ is solved numerically. If an exact solution is not bracketed within a reasonable range, we choose $\\beta$ that minimizes the absolute difference $|\\mathcal{L}_\\beta^{\\star}(\\beta) - \\mathcal{L}_{\\text{cap}}^{\\star}(t)|$ over the range, ensuring the closest possible match.\n\nStep 5: Disentanglement metric and decision.\nCompute\n$$\nD(\\theta) = 1 - |\\sin(2\\theta)|\n$$\nfor both $\\theta_{\\text{cap}}(t)$ and $\\theta_\\beta^*$. The boolean result per test case is true if $D(\\theta_{\\text{cap}}(t)) > D(\\theta_\\beta^*)$ and false otherwise.\n\nAlgorithmic design:\n- Use one-dimensional bounded minimization over $\\theta \\in [-\\pi, \\pi]$ for $\\mathcal{L}_{\\text{cap}}$, with inner one-dimensional bounded minimizations over $\\sigma_i^2 \\in (0, \\text{upper}]$ per dimension to minimize $J_i(\\sigma_i^2;\\theta,t)$.\n- Use closed-form $\\sigma^{2\\,*}(\\beta) = \\frac{\\beta}{\\beta + 2}$ and $\\theta_\\beta^* = -\\varphi$ for static-$\\beta$, and solve the scalar equation for $\\beta$ using robust root finding with bracketing; if no root is bracketed, minimize the absolute difference over a grid.\n- Assemble the final booleans across the test suite into a single, comma-separated list enclosed in square brackets.\n\nAll angles are in radians; all outputs are booleans.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize_scalar, brentq\n\n# Helper functions for the stylized linear-Gaussian VAE model.\n\ndef capacity_schedule(t, k1, k2, C1max, C2max):\n    C1 = min(C1max, k1 * t)\n    C2 = min(C2max, k2 * t)\n    return C1, C2\n\ndef recon_data(theta, phi, s1, s2):\n    S = s1**2 + s2**2\n    return 2.0 * (1.0 - np.cos(theta + phi)) * S\n\ndef mu2_dim1(theta, s1, s2):\n    return (np.cos(theta) ** 2) * (s1 ** 2) + (np.sin(theta) ** 2) * (s2 ** 2)\n\ndef mu2_dim2(theta, s1, s2):\n    return (np.sin(theta) ** 2) * (s1 ** 2) + (np.cos(theta) ** 2) * (s2 ** 2)\n\ndef kl_per_dim(mu2, sigma2):\n    # KL_i = 0.5 * (E[mu_i^2] + sigma_i^2 - ln sigma_i^2 - 1)\n    return 0.5 * (mu2 + sigma2 - np.log(sigma2) - 1.0)\n\ndef disentanglement(theta):\n    # D(theta) = 1 - |sin(2 theta)|\n    return 1.0 - np.abs(np.sin(2.0 * theta))\n\ndef minimize_sigma_for_capacity(mu2, C, gamma, sigma2_upper=10.0):\n    # Minimize J(sigma2) = sigma2 + gamma*(KL(mu2, sigma2) - C)^2 over sigma2 in (0, sigma2_upper]\n    # Bound sigma2 away from 0 for numerical stability.\n    lower = 1e-8\n    upper = sigma2_upper\n    obj = lambda s: s + gamma * (kl_per_dim(mu2, s) - C) ** 2\n    res = minimize_scalar(obj, bounds=(lower, upper), method='bounded')\n    return res.x, res.fun\n\ndef capacity_optimum(s1, s2, phi, k1, k2, C1max, C2max, gamma, t):\n    C1, C2 = capacity_schedule(t, k1, k2, C1max, C2max)\n    # Outer minimization over theta in [-pi, pi].\n    def total_objective(theta):\n        mu1 = mu2_dim1(theta, s1, s2)\n        mu2 = mu2_dim2(theta, s1, s2)\n        # Inner minimizations for sigma1^2 and sigma2^2\n        sigma1_opt, J1 = minimize_sigma_for_capacity(mu1, C1, gamma)\n        sigma2_opt, J2 = minimize_sigma_for_capacity(mu2, C2, gamma)\n        # Reconstruction term includes sigma1^2 + sigma2^2\n        rec = recon_data(theta, phi, s1, s2) + sigma1_opt + sigma2_opt\n        # Replace J1 and J2 parts: they already include sigma terms and penalties.\n        # But to avoid double-counting sigma terms, we compute penalties explicitly:\n        pen = gamma * (kl_per_dim(mu1, sigma1_opt) - C1) ** 2 + gamma * (kl_per_dim(mu2, sigma2_opt) - C2) ** 2\n        # Total objective:\n        return rec + pen\n\n    res = minimize_scalar(total_objective, bounds=(-np.pi, np.pi), method='bounded')\n    theta_star = res.x\n    # Recompute sigma opts at optimal theta for reporting and final loss.\n    mu1_star = mu2_dim1(theta_star, s1, s2)\n    mu2_star = mu2_dim2(theta_star, s1, s2)\n    sigma1_star, _ = minimize_sigma_for_capacity(mu1_star, C1, gamma)\n    sigma2_star, _ = minimize_sigma_for_capacity(mu2_star, C2, gamma)\n    final_loss = recon_data(theta_star, phi, s1, s2) + sigma1_star + sigma2_star \\\n                 + gamma * (kl_per_dim(mu1_star, sigma1_star) - C1) ** 2 \\\n                 + gamma * (kl_per_dim(mu2_star, sigma2_star) - C2) ** 2\n    return theta_star, sigma1_star, sigma2_star, final_loss\n\ndef static_beta_min_loss(s1, s2, phi, beta):\n    # Optimal theta is -phi\n    theta_star = -phi\n    # Optimal sigma^2 per dimension is beta / (beta + 2)\n    sigma2_star = beta / (beta + 2.0)\n    # Reconstruction at theta_star + phi = 0\n    rec = recon_data(theta_star, phi, s1, s2) + 2.0 * sigma2_star\n    # KL terms with mu energies at theta_star:\n    mu1 = mu2_dim1(theta_star, s1, s2)\n    mu2 = mu2_dim2(theta_star, s1, s2)\n    kl_sum = kl_per_dim(mu1, sigma2_star) + kl_per_dim(mu2, sigma2_star)\n    total = rec + beta * kl_sum\n    return total, theta_star, sigma2_star\n\ndef match_beta_to_cap_loss(s1, s2, phi, target_loss):\n    # Solve L_beta_star(beta) = target_loss\n    def f(beta):\n        L, _, _ = static_beta_min_loss(s1, s2, phi, beta)\n        return L - target_loss\n    # Bracket beta in a wide interval\n    beta_lo = 1e-6\n    beta_hi = 100.0\n    flo = f(beta_lo)\n    fhi = f(beta_hi)\n    if np.isnan(flo) or np.isnan(fhi):\n        # Fallback grid search\n        grid = np.logspace(-6, 2, 200)\n        vals = [abs(f(b)) for b in grid]\n        idx = int(np.argmin(vals))\n        return grid[idx]\n    # Ensure a root is bracketed; if not, fallback to minimizing absolute difference.\n    if flo * fhi > 0:\n        grid = np.logspace(-6, 2, 200)\n        vals = [abs(f(b)) for b in grid]\n        idx = int(np.argmin(vals))\n        return grid[idx]\n    try:\n        beta_star = brentq(f, beta_lo, beta_hi, maxiter=200)\n        return beta_star\n    except Exception:\n        grid = np.logspace(-6, 2, 200)\n        vals = [abs(f(b)) for b in grid]\n        idx = int(np.argmin(vals))\n        return grid[idx]\n\ndef evaluate_case(case):\n    s1, s2, phi, k1, k2, C1max, C2max, gamma, t = case\n    # Adaptive capacity optimum at time t\n    theta_cap, sigma1_cap, sigma2_cap, L_cap_star = capacity_optimum(s1, s2, phi, k1, k2, C1max, C2max, gamma, t)\n    D_cap = disentanglement(theta_cap)\n    # Match beta so that static-beta minimized loss equals L_cap_star\n    beta_match = match_beta_to_cap_loss(s1, s2, phi, L_cap_star)\n    # Static-beta disentanglement at matched beta (note theta_beta depends only on phi)\n    _, theta_beta, _ = static_beta_min_loss(s1, s2, phi, beta_match)\n    D_beta = disentanglement(theta_beta)\n    # Boolean: adaptive capacity yields strictly higher disentanglement than static beta\n    return D_cap > D_beta\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (s1, s2, phi, k1, k2, C1max, C2max, gamma, t)\n    test_cases = [\n        (1.5, 0.3, 0.7, 0.7, 0.1, 1.0, 0.1, 20.0, 2.0),  # Case 1: happy path\n        (1.0, 0.8, 0.0, 0.3, 0.3, 0.5, 0.5, 20.0, 1.0),  # Case 2: boundary (no mixing)\n        (1.5, 0.3, 0.1, 0.3, 0.3, 0.2, 0.2, 20.0, 1.0),  # Case 3: edge (equal budgets, small mixing)\n    ]\n\n    results = []\n    for case in test_cases:\n        res = evaluate_case(case)\n        results.append(res)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3116925"}]}