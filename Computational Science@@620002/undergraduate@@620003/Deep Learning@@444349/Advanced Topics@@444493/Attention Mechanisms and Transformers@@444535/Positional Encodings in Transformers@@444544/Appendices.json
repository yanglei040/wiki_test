{"hands_on_practices": [{"introduction": "This first practice tackles a fundamental question: why use a complex sinusoidal formula for positional encodings (PEs) instead of simply learning an embedding for each position? This exercise [@problem_id:3100282] provides the answer by simulating a scenario where a model must generalize beyond the longest sequence it was trained on. By comparing the extrapolation error of a sinusoidal PE against a clamped, \"learned\" PE, you will gain a hands-on understanding of the powerful generalization capabilities inherent in the sinusoidal design.", "problem": "You are tasked with constructing a principled comparison between deterministic sinusoidal positional encodings and learned positional encodings for extrapolation beyond the training sequence length within a simplified model that isolates positional information from content. The setup models how positional encodings provide features to a linear decoder, which is a minimal abstraction of how attention-based models consume positional signals. You must derive the error behavior from first principles and implement a program that computes the extrapolation error for both encoding types.\n\nThe fundamental basis for this task is:\n- The definition of self-attention, which requires positional information to resolve order, and the notion that positional encodings inject position-dependent features into the model’s computations.\n- The well-tested method of Ordinary Least Squares (OLS), which fits a linear mapping from feature vectors to scalar targets by minimizing the sum of squared residuals.\n- The definition of Root Mean Squared Error (RMSE), which quantifies the discrepancy between predictions and targets over a set.\n\nThe sequence is indexed by discrete positions $p \\in \\{1,2,\\dots\\}$ and has a scalar target $y(p)$ defined by a superposition of sinusoidal components, with all angle computations expressed in radians. Specifically, for all positions $p$, define\n$$\ny(p) = \\sin(\\omega_1 p) + \\tfrac{1}{2}\\cos(\\omega_2 p),\n$$\nwhere $\\omega_1 = \\tfrac{2\\pi}{16}$ and $\\omega_2 = \\tfrac{2\\pi}{32}$.\n\nYou must implement two positional encoding strategies that produce feature vectors $x(p)$ for every position $p$:\n\n- Sinusoidal positional encoding: Use an even-dimensional feature vector of dimension $d$, formed by concatenating pairs of $\\sin$ and $\\cos$ functions at fixed angular frequencies. The feature vector contains pairs $(\\sin(\\omega_k p), \\cos(\\omega_k p))$ for a set of angular frequencies $\\{\\omega_k\\}$ that includes $\\omega_1$ and $\\omega_2$, along with additional distinct frequencies to ensure full column rank on the training positions. The exact construction and scaling of these features are part of your implementation, but they must be deterministic functions of $p$ and not learned from data.\n\n- Learned positional encoding: Use a one-hot feature vector of length $N_{\\text{train}}$ for positions $p \\in \\{1,\\dots,N_{\\text{train}}\\}$. For extrapolation to $p > N_{\\text{train}}$, use a clamping rule in which all positions map to the one-hot vector of index $N_{\\text{train}}$. This models the common behavior of learned embedding tables that cannot produce unseen position embeddings without explicit extension.\n\nTraining is performed using Ordinary Least Squares (OLS): given training positions $p \\in \\{1,2,\\dots,N_{\\text{train}}\\}$, assemble the design matrix $X \\in \\mathbb{R}^{N_{\\text{train}} \\times d}$ from $x(p)$ and the target vector $y \\in \\mathbb{R}^{N_{\\text{train}}}$ from $y(p)$. Fit a linear decoder $w \\in \\mathbb{R}^{d}$ by minimizing the training objective\n$$\n\\min_{w} \\sum_{p=1}^{N_{\\text{train}}} \\left( y(p) - w^\\top x(p) \\right)^2.\n$$\nUse the fitted $w$ to generate predictions $\\hat{y}(p) = w^\\top x(p)$ for positions $p > N_{\\text{train}}$.\n\nDefine the extrapolation error $E(n)$ for any integer $n \\ge N_{\\text{train}}$ as the Root Mean Squared Error (RMSE) on the positions beyond the training range:\n$$\nE(n) =\n\\begin{cases}\n0, & \\text{if } n = N_{\\text{train}},\\\\\n\\sqrt{\\dfrac{1}{n - N_{\\text{train}}} \\sum_{p=N_{\\text{train}}+1}^{n} \\left( y(p) - \\hat{y}(p) \\right)^2}, & \\text{if } n > N_{\\text{train}}.\n\\end{cases}\n$$\n\nYour program must:\n- Use training maximum $N_{\\text{train}} = 64$ and sinusoidal encoding dimension $d = 16$ (even).\n- Use the above $y(p)$ with $\\omega_1 = \\tfrac{2\\pi}{16}$ and $\\omega_2 = \\tfrac{2\\pi}{32}$, with all angle computations in radians.\n- Construct the sinusoidal positional encoding with a frequency set that includes $\\omega_1$ and $\\omega_2$ and additional distinct positive angular frequencies.\n- Construct the learned positional encoding as a one-hot vector of length $N_{\\text{train}}$ with the clamping rule for $p > N_{\\text{train}}$.\n- Fit $w$ via OLS separately for each encoding on $p \\in \\{1,2,\\dots,N_{\\text{train}}\\}$.\n\nTest suite:\n- Use the following values of $n$: $64$, $80$, $128$, $160$.\n- This suite includes the boundary $n = N_{\\text{train}}$ (yielding $E(n) = 0$ by definition), a moderate extrapolation length, and larger extrapolation lengths to probe sustained performance.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list should contain the RMSE values for sinusoidal encoding and learned encoding for each test case in order, aggregating across all tests:\n$$\n[\\;E_{\\text{sin}}(64),\\;E_{\\text{learn}}(64),\\;E_{\\text{sin}}(80),\\;E_{\\text{learn}}(80),\\;E_{\\text{sin}}(128),\\;E_{\\text{learn}}(128),\\;E_{\\text{sin}}(160),\\;E_{\\text{learn}}(160)\\;].\n$$\nAll angle computations must be in radians, and the outputs must be real-valued floats.", "solution": "The user wants to analyze and implement a comparison between sinusoidal and learned positional encodings for extrapolation in a simplified linear regression setting.\n\n### Step 1: Extract Givens\n- **Target Function**: $y(p) = \\sin(\\omega_1 p) + \\tfrac{1}{2}\\cos(\\omega_2 p)$ for integer positions $p \\in \\{1,2,\\dots\\}$.\n- **Angular Frequencies**: $\\omega_1 = \\tfrac{2\\pi}{16}$ and $\\omega_2 = \\tfrac{2\\pi}{32}$. All angle computations must be in radians.\n- **Training Range**: Positions $p \\in \\{1, 2, \\dots, N_{\\text{train}}\\}$.\n- **Training Size**: $N_{\\text{train}} = 64$.\n- **Training Method**: Ordinary Least Squares (OLS) to minimize $\\sum_{p=1}^{N_{\\text{train}}} ( y(p) - w^\\top x(p) )^2$.\n- **Sinusoidal Encoding**:\n    - Feature dimension $d = 16$.\n    - Features are pairs of $(\\sin(\\omega_k p), \\cos(\\omega_k p))$.\n    - The set of frequencies $\\{\\omega_k\\}$ must include $\\omega_1$ and $\\omega_2$.\n- **Learned Encoding**:\n    - For $p \\in \\{1, \\dots, N_{\\text{train}}\\}$, $x(p)$ is a one-hot vector of length $N_{\\text{train}}$.\n    - For $p > N_{\\text{train}}$, a clamping rule is used: $x(p) = x(N_{\\text{train}})$.\n- **Extrapolation Error Metric**: $E(n)$, the Root Mean Squared Error (RMSE) on positions $\\{N_{\\text{train}}+1, \\dots, n\\}$.\n  $$\n  E(n) =\n  \\begin{cases}\n  0, & \\text{if } n = N_{\\text{train}},\\\\\n  \\sqrt{\\dfrac{1}{n - N_{\\text{train}}} \\sum_{p=N_{\\text{train}}+1}^{n} \\left( y(p) - \\hat{y}(p) \\right)^2}, & \\text{if } n > N_{\\text{train}}.\n  \\end{cases}\n  $$\n- **Test Suite**: Evaluate $E(n)$ for $n \\in \\{64, 80, 128, 160\\}$.\n- **Required Output**: A single-line list: `[E_sin(64), E_learn(64), E_sin(80), E_learn(80), E_sin(128), E_learn(128), E_sin(160), E_learn(160)]`.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is subjected to rigorous validation.\n\n- **Scientifically Grounded**: The problem is based on well-established principles of statistical learning (Ordinary Least Squares), numerical analysis (RMSE), and concepts from modern deep learning (positional encodings in Transformer models). The target function is a simple, well-behaved mathematical function. The setup is a valid and common technique for analyzing feature representations.\n- **Well-Posed**: The problem is well-posed. The OLS solution is unique provided the design matrix has full column rank. For sinusoidal encoding, the problem explicitly instructs to choose frequencies to ensure this. For learned encoding, the design matrix on the training set is the identity matrix, which is of full rank. The extrapolation rules are unambiguously defined. A unique solution for the weights $w$ and subsequent predictions $\\hat{y}(p)$ exists for both cases.\n- **Objective**: The problem is stated in precise, objective mathematical language.\n- **Completeness**: All necessary parameters ($N_{\\text{train}}$, $d$, $\\omega_1$, $\\omega_2$) and definitions are provided. The specification for constructing the sinusoidal frequencies allows for a principled choice (e.g., a geometric progression of periods that includes the specified ones), making the problem fully determined.\n- **Consistency**: The definitions and constraints are internally consistent. Dimensionalities of vectors and matrices are compatible.\n\nThe problem does not exhibit any of the invalidity flaws. It is a formal, scientifically grounded, and well-posed problem.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be provided.\n\n### Theoretical Framework and Derivation\nThe core of the problem is to fit a linear model $\\hat{y}(p) = w^\\top x(p)$ to a target function $y(p)$ using two different feature maps $x(p)$ and to compare their extrapolation performance. The model parameters $w$ are found by Ordinary Least Squares (OLS) on the training set $p \\in \\{1, \\dots, N_{\\text{train}}\\}$.\n\nLet $X \\in \\mathbb{R}^{N_{\\text{train}} \\times d}$ be the design matrix, whose rows are the feature vectors $x(p)^\\top$ for $p=1, \\dots, N_{\\text{train}}$. Let $y_{\\text{train}} \\in \\mathbb{R}^{N_{\\text{train}}}$ be the vector of target values $y(p)$ on the training set. The OLS solution for the weight vector $w$ is given by the normal equations:\n$$\nw = (X^\\top X)^{-1} X^\\top y_{\\text{train}}\n$$\nThis solution exists and is unique if $X^\\top X$ is invertible, which is equivalent to $X$ having full column rank.\n\n#### 1. Sinusoidal Positional Encoding\nThis strategy is designed to represent positions using a basis of periodic functions.\n- **Feature Vector Construction**: The feature vector $x_{\\text{sin}}(p)$ has dimension $d=16$. It is formed by concatenating $d/2 = 8$ pairs of sine and cosine functions:\n$$\nx_{\\text{sin}}(p) = [\\sin(\\Omega_1 p), \\cos(\\Omega_1 p), \\sin(\\Omega_2 p), \\cos(\\Omega_2 p), \\dots, \\sin(\\Omega_8 p), \\cos(\\Omega_8 p)]^\\top\n$$\nThe problem requires that the set of angular frequencies $\\{\\Omega_k\\}_{k=1}^8$ contains $\\omega_1=\\frac{2\\pi}{16}$ and $\\omega_2=\\frac{2\\pi}{32}$. We select a principled set of frequencies corresponding to a geometric progression of periods, ensuring the required frequencies are included. A suitable choice for periods is $\\{8, 16, 32, 64, 128, 256, 512, 1024\\}$. The corresponding angular frequencies are $\\Omega_k = 2\\pi/T_k$. This set is distinct and spans a wide range.\n\n- **Extrapolation Behavior**: The target function is $y(p) = \\sin(\\omega_1 p) + \\frac{1}{2}\\cos(\\omega_2 p)$. Since $\\omega_1$ and $\\omega_2$ are included in our basis frequencies, the true function $y(p)$ is a linear combination of the basis functions used to construct $x_{\\text{sin}}(p)$. Specifically, it corresponds to a weight vector where the coefficient for $\\sin(\\omega_1 p)$ is $1$, the coefficient for $\\cos(\\omega_2 p)$ is $0.5$, and all other coefficients are $0$. Although the sinusoidal basis functions are not perfectly orthogonal over the finite, discrete training interval $\\{1, \\dots, 64\\}$, OLS is designed to find the best linear approximation. It will find a weight vector $w_{\\text{sin}}$ that allows the model $\\hat{y}(p) = w_{\\text{sin}}^\\top x_{\\text{sin}}(p)$ to closely approximate the true function. Because the model has the functional form of the true data-generating process, this approximation will hold not only for the training points but also for extrapolated points $p > N_{\\text{train}}$. Consequently, the extrapolation error $E_{\\text{sin}}(n)$ is expected to be very small, limited primarily by numerical precision and minor fitting errors due to non-orthogonality of the basis on the finite training set.\n\n#### 2. Learned Positional Encoding\nThis strategy models position embeddings that are learned independently for each position in the training set.\n- **Feature Vector Construction**: The feature dimension is $d = N_{\\text{train}} = 64$. For a training position $p \\in \\{1, \\dots, 64\\}$, the feature vector $x_{\\text{learn}}(p)$ is a one-hot vector where the $p$-th element is $1$ and all others are $0$. The design matrix $X_{\\text{learn}}$ is therefore the $64 \\times 64$ identity matrix, $I_{64}$.\n\n- **Training**: The OLS solution for the weights is:\n$$\nw_{\\text{learn}} = (X_{\\text{learn}}^\\top X_{\\text{learn}})^{-1} X_{\\text{learn}}^\\top y_{\\text{train}} = (I_{64}^\\top I_{64})^{-1} I_{64}^\\top y_{\\text{train}} = y_{\\text{train}}\n$$\nThis means the model simply memorizes the target value for each training position. The weight vector $w_{\\text{learn}}$ is identical to the vector of training targets.\n\n- **Extrapolation Behavior**: The problem specifies a clamping rule for extrapolation: for any $p > N_{\\text{train}}$, the feature vector is the same as for the last training position, i.e., $x_{\\text{learn}}(p) = x_{\\text{learn}}(N_{\\text{train}})$. This is the one-hot vector corresponding to position $N_{\\text{train}}=64$. The prediction for any such position is:\n$$\n\\hat{y}_{\\text{learn}}(p) = w_{\\text{learn}}^\\top x_{\\text{learn}}(N_{\\text{train}})\n$$\nSince $w_{\\text{learn}, i} = y(i)$ (using 1-based indexing for a moment) and $x_{\\text{learn}}(N_{\\text{train}})$ has a $1$ only at the $N_{\\text{train}}$-th position, the dot product picks out the last element of $w_{\\text{learn}}$.\n$$\n\\hat{y}_{\\text{learn}}(p) = y(N_{\\text{train}}) \\quad \\text{for all } p > N_{\\text{train}}\n$$\nThe model thus makes a constant prediction for all unseen positions, equal to the value at the edge of its training experience. The true function $y(p)$ continues to oscillate for $p>64$, so the error $y(p) - y(64)$ will be substantial. The RMSE, $E_{\\text{learn}}(n)$, will accumulate these large errors and is expected to be significantly higher than for the sinusoidal encoding.\n\n### Calculation of Extrapolation Error $E(n)$\nFor each test value $n > N_{\\text{train}}$:\n1. Define the set of extrapolation positions $P_{\\text{extrap}} = \\{N_{\\text{train}}+1, \\dots, n\\}$.\n2. Generate true values $y(p)$ for all $p \\in P_{\\text{extrap}}$.\n3. Generate predictions $\\hat{y}_{\\text{sin}}(p)$ and $\\hat{y}_{\\text{learn}}(p)$ for all $p \\in P_{\\text{extrap}}$.\n4. Compute the RMSE for each case:\n   $$\n   E(n) = \\sqrt{\\frac{1}{n - N_{\\text{train}}} \\sum_{p \\in P_{\\text{extrap}}} (y(p) - \\hat{y}(p))^2}\n   $$\nFor the edge case $n=N_{\\text{train}}$, the error $E(N_{\\text{train}})$ is defined to be $0$ for both methods.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and compares the extrapolation error of sinusoidal and learned\n    positional encodings within a simplified linear regression framework.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    N_train = 64\n    d_sin = 16\n    omega1 = 2 * np.pi / 16\n    omega2 = 2 * np.pi / 32\n    test_n_values = [64, 80, 128, 160]\n\n    # --- Target Function ---\n    def target_function(p):\n        \"\"\"Computes the true scalar target y(p).\"\"\"\n        return np.sin(omega1 * p) + 0.5 * np.cos(omega2 * p)\n\n    # --- Training Data ---\n    # Use 1-based indexing for positions as in the problem description\n    train_positions = np.arange(1, N_train + 1)\n    y_train = target_function(train_positions)\n\n    # --- 1. Sinusoidal Positional Encoding ---\n\n    # Define frequencies for sinusoidal encoding. The set of periods includes\n    # 16 and 32 (corresponding to omega1 and omega2) plus others to ensure\n    # full rank and represent a range of frequencies.\n    periods = np.array([8, 16, 32, 64, 128, 256, 512, 1024])\n    sin_frequencies = 2 * np.pi / periods\n    \n    def get_sinusoidal_features(positions, freqs, dim):\n        \"\"\"Generates the sinusoidal feature matrix for a given set of positions.\"\"\"\n        num_pos = len(positions)\n        num_freqs = dim // 2\n        # Ensure freqs has the right length just in case\n        if len(freqs) != num_freqs:\n            raise ValueError(f\"Expected {num_freqs} frequencies for dimension {dim}, but got {len(freqs)}\")\n            \n        # Create a matrix of p*omega values\n        p_omega = positions[:, np.newaxis] * freqs[np.newaxis, :]\n        \n        # Create the feature matrix by interleaving sin and cos\n        X = np.zeros((num_pos, dim))\n        X[:, 0::2] = np.sin(p_omega)\n        X[:, 1::2] = np.cos(p_omega)\n        return X\n\n    # Fit the linear decoder for sinusoidal encoding\n    X_sin_train = get_sinusoidal_features(train_positions, sin_frequencies, d_sin)\n    w_sin, _, _, _ = np.linalg.lstsq(X_sin_train, y_train, rcond=None)\n\n    # --- 2. Learned Positional Encoding ---\n    # For learned one-hot encoding, the weight vector w_learn is simply the\n    # target vector y_train, as X_learn is the identity matrix.\n    # The extrapolation prediction is clamped to the value at N_train.\n    y_pred_learn_extrapol_val = target_function(N_train)\n\n    # --- Calculate Errors for Test Cases ---\n    results = []\n    for n in test_n_values:\n        if n == N_train:\n            # By definition, extrapolation error at n = N_train is 0.\n            results.append(0.0)  # E_sin(64)\n            results.append(0.0)  # E_learn(64)\n            continue\n        \n        # Extrapolation positions\n        extrapol_positions = np.arange(N_train + 1, n + 1)\n        num_extrapol_points = len(extrapol_positions)\n        \n        # True values for the extrapolation range\n        y_true_extrapol = target_function(extrapol_positions)\n        \n        # Calculate error for sinusoidal encoding\n        X_sin_extrapol = get_sinusoidal_features(extrapol_positions, sin_frequencies, d_sin)\n        y_pred_sin_extrapol = X_sin_extrapol @ w_sin\n        error_sin = np.sqrt(np.mean((y_true_extrapol - y_pred_sin_extrapol)**2))\n        results.append(error_sin)\n\n        # Calculate error for learned encoding\n        # The prediction is a constant value for all extrapolation points\n        y_pred_learn_extrapol = np.full(num_extrapol_points, y_pred_learn_extrapol_val)\n        error_learn = np.sqrt(np.mean((y_true_extrapol - y_pred_learn_extrapol)**2))\n        results.append(error_learn)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3100282"}, {"introduction": "Having seen that sinusoidal encodings are effective, this practice delves into *why* they work so well by treating them as a set of signals with different frequencies. In this exercise [@problem_id:3164200], you will act as a detective, selectively removing high- or low-frequency components from the positional encoding vector to see how it affects the model's ability to represent various positional patterns. This process, known as ablation, reveals the deep connection between positional encodings and the principles of Fourier analysis.", "problem": "You are given a sequence of positions indexed by integer $p \\in \\{0,1,\\dots,N-1\\}$, and a real-valued feature representation called Positional Encoding (PE). The goal is to perform frequency band ablation on the PE and analyze how different bands contribute to reconstructing task-specific target functions over positions. You must implement a complete program that constructs a PE based on a sinusoidal basis, applies ablation masks that remove high- or low-frequency components, fits linear predictors to reconstruct target functions, and reports normalized mean squared errors (NMSE) for a specified test suite.\n\nThe fundamental basis for this problem is the Fourier representation of signals: sinusoidal functions at different angular frequencies form a basis for periodic functions sampled at uniform intervals. You must derive a PE from this basis rather than assuming a known transformer-specific formula.\n\nRequirements:\n- Construct a PE of dimensionality $D = 1 + 2K$ for sequence length $N = 128$, where the first component is a constant bias term and the remaining $2K$ components are generated from $K$ sinusoidal frequency bands using sine and cosine functions. Angles must be in radians. Frequencies must be logarithmically spaced across the interval $[\\omega_{\\min}, \\omega_{\\max}]$ with $K = 8$, $\\omega_{\\min} = \\frac{2\\pi}{N}$, and $\\omega_{\\max} = \\pi$. Denote these angular frequencies by $\\{\\omega_k\\}_{k=0}^{K-1}$.\n- Define the following target functions $h(p)$ over positions:\n    1. $h_{\\text{Lcos}}(p) = \\cos\\left(\\frac{2\\pi p}{64}\\right)$.\n    2. $h_{\\text{Hcos}}(p) = \\cos\\left(\\frac{2\\pi p}{4}\\right)$.\n    3. $h_{\\text{Window}}(p) = \\mathbb{1}\\{|p - 64| \\le 5\\}$, where $\\mathbb{1}\\{\\cdot\\}$ is the indicator function.\n    4. $h_{\\text{Square8}}(p) = \\mathbb{1}\\{(p \\bmod 8) < 4\\}$.\n- For ablation, you must apply masks to the PE components associated with specific frequency bands. Keep the constant bias term always. For low-pass ablation at cutoff $\\omega_c$, keep all frequency components with $\\omega_k \\le \\omega_c$ and remove all with $\\omega_k > \\omega_c$. For high-pass ablation at cutoff $\\omega_c$, keep all frequency components with $\\omega_k \\ge \\omega_c$ and remove all with $\\omega_k < \\omega_c$. For no ablation (\"none\"), keep all frequency components.\n- Fit a linear predictor $f(p) = \\mathbf{x}(p)^\\top \\mathbf{w}$ to minimize mean squared error, where $\\mathbf{x}(p)$ is the PE vector at position $p$ after ablation. Use the full set of positions $p \\in \\{0,\\dots,N-1\\}$ as the training set. Compute the normalized mean squared error (NMSE) defined by\n$$\n\\mathrm{NMSE} = \\frac{\\frac{1}{N}\\sum_{p=0}^{N-1}\\big(h(p) - f(p)\\big)^2}{\\mathrm{Var}\\big(h(p)\\big)},\n$$\nwhere $\\mathrm{Var}\\big(h(p)\\big)$ is the variance of $h(p)$ over $p=0,\\dots,N-1$. Report $\\mathrm{NMSE}$ as a float.\n\nAngle unit specification: All angular quantities must be in radians.\n\nTest suite:\nYou must implement the following ordered list of test cases. Each test case is a triple $(\\text{ablation\\_type}, \\omega_c, \\text{task})$ where $\\text{ablation\\_type} \\in \\{\\text{\"none\"}, \\text{\"low\\_pass\"}, \\text{\"high\\_pass\"}\\}$, $\\omega_c$ is a nonnegative real number in radians, and $\\text{task} \\in \\{\\text{\"Lcos\"}, \\text{\"Hcos\"}, \\text{\"Window\"}, \\text{\"Square8\"}\\}$ mapping to the target functions defined above.\n\n- Test case 1: $(\\text{\"none\"}, \\frac{\\pi}{2}, \\text{\"Lcos\"})$\n- Test case 2: $(\\text{\"low\\_pass\"}, 0.4\\pi, \\text{\"Hcos\"})$\n- Test case 3: $(\\text{\"high\\_pass\"}, 0.2\\pi, \\text{\"Hcos\"})$\n- Test case 4: $(\\text{\"low\\_pass\"}, 0.1\\pi, \\text{\"Window\"})$\n- Test case 5: $(\\text{\"high\\_pass\"}, 0.8\\pi, \\text{\"Lcos\"})$\n- Test case 6: $(\\text{\"low\\_pass\"}, 0.01, \\text{\"Lcos\"})$  (boundary: nearly all bands removed)\n- Test case 7: $(\\text{\"high\\_pass\"}, \\pi + 0.01, \\text{\"Hcos\"})$  (boundary: nearly all bands removed)\n- Test case 8: $(\\text{\"none\"}, \\frac{\\pi}{2}, \\text{\"Square8\"})$\n\nFinal output format:\nYour program should produce a single line of output containing the NMSE results, in order of the test cases above, as a comma-separated list enclosed in square brackets (e.g., \"[r1,r2,...,r8]\"). Each element must be a float.", "solution": "We start from the Fourier representation of signals: the set of sine and cosine functions $\\{\\sin(\\omega p), \\cos(\\omega p)\\}$ for different angular frequencies $\\omega$ forms a basis that can represent periodic signals sampled at uniform intervals. For discrete positions $p \\in \\{0,\\dots,N-1\\}$, any sufficiently smooth or periodic target function can be approximated by a linear combination of such basis functions. This is a well-tested fact from Fourier analysis.\n\nPrinciple-based design:\n1. Basis construction from first principles: Because sine and cosine functions are orthogonal over uniform samples under suitable conditions, they serve as a natural basis for representing positional information. We construct the Positional Encoding (PE) as a vector $\\mathbf{x}(p) \\in \\mathbb{R}^{D}$ comprising a constant bias term and pairs of sine and cosine components at logarithmically spaced angular frequencies $\\omega_k \\in [\\omega_{\\min}, \\omega_{\\max}]$, with $K$ bands and $D = 1 + 2K$. The constant term enables the model to represent the mean of a target function. Logarithmic spacing spans multiple orders of positional scale, providing both low- and high-frequency sensitivity.\n\n2. Tasks and frequency content:\n   - For $h_{\\text{Lcos}}(p) = \\cos\\left(\\frac{2\\pi p}{64}\\right)$, the dominant frequency is low, with angular frequency $\\omega = \\frac{2\\pi}{64}$. This is expected to be captured by low-frequency components; removing high frequencies should not harm much, but removing low frequencies will degrade reconstruction.\n   - For $h_{\\text{Hcos}}(p) = \\cos\\left(\\frac{2\\pi p}{4}\\right)$, the dominant frequency is high, with angular frequency $\\omega = \\frac{2\\pi}{4} = \\frac{\\pi}{2}$. Low-pass ablation at a cutoff below this value will remove the critical component, while high-pass ablation retaining frequencies above a lower cutoff will preserve it.\n   - For $h_{\\text{Window}}(p) = \\mathbb{1}\\{|p - 64| \\le 5\\}$, this target is localized and non-smooth (a step). High frequencies are needed to model sharp transitions; low-pass ablation will blur and increase error.\n   - For $h_{\\text{Square8}}(p) = \\mathbb{1}\\{(p \\bmod 8) < 4\\}$, this is a square wave with period $8$ and $50\\%$ duty cycle. Its Fourier series contains a constant plus odd harmonics at high frequencies. Removing high frequencies will degrade reconstruction; keeping them helps.\n\n3. Linear projection and optimality: Given a design matrix $X \\in \\mathbb{R}^{N \\times D}$ whose rows are PE vectors $\\mathbf{x}(p)$, the least squares (LS) solution $\\hat{\\mathbf{w}}$ minimizing $\\sum_p \\big(h(p) - \\mathbf{x}(p)^\\top \\mathbf{w}\\big)^2$ is the orthogonal projection of the target vector $\\mathbf{h}$ onto the column space of $X$. This projection is optimal among all linear combinations of the basis in terms of mean squared error. Therefore, ablation, which removes basis columns, restricts the subspace and can only increase the error or leave it unchanged.\n\n4. Normalized error: The normalized mean squared error (NMSE) is defined as\n$$\n\\mathrm{NMSE} = \\frac{\\frac{1}{N}\\sum_{p=0}^{N-1}\\big(h(p) - f(p)\\big)^2}{\\mathrm{Var}\\big(h(p)\\big)},\n$$\nwhich equals $1 - R^2$ for linear regression on deterministic data. It lies in $[0,1]$ when the predictor includes at least a constant term and the target has nonzero variance. If all sinusoidal components are ablated, the LS solution reduces to the constant predictor equal to the mean of $h$, yielding $\\mathrm{NMSE} \\approx 1$.\n\nAlgorithmic steps:\n- Define $N = 128$, $K = 8$, $\\omega_{\\min} = \\frac{2\\pi}{N}$, $\\omega_{\\max} = \\pi$, and generate $\\{\\omega_k\\}_{k=0}^{K-1}$ logarithmically spaced in $[\\omega_{\\min}, \\omega_{\\max}]$.\n- For each position $p$, construct the PE vector $\\mathbf{x}(p)$ with:\n  - A constant term $1$.\n  - For each $\\omega_k$, append $\\sin(\\omega_k p)$ and $\\cos(\\omega_k p)$.\n- Implement ablation:\n  - Low-pass at $\\omega_c$: keep pairs associated with $\\omega_k \\le \\omega_c$.\n  - High-pass at $\\omega_c$: keep pairs associated with $\\omega_k \\ge \\omega_c$.\n  - None: keep all pairs.\n  - Always keep the constant term.\n- For each target function $h$, build the vector $\\mathbf{h} \\in \\mathbb{R}^{N}$.\n- Solve the LS problem $\\min_{\\mathbf{w}} \\|\\mathbf{h} - X\\mathbf{w}\\|_2^2$ using a robust numerical method (for example, the least squares solver), obtain $\\hat{\\mathbf{w}}$ and predicted $\\hat{\\mathbf{h}} = X\\hat{\\mathbf{w}}$.\n- Compute $\\mathrm{NMSE}$ per the formula above.\n- Run the specified test suite and output the NMSEs.\n\nQualitative expectations for the test suite:\n- Test case 1 (none, Lcos): low-frequency cosine should be well reconstructed with the full basis; expect low NMSE.\n- Test case 2 (low-pass $0.4\\pi$, Hcos): the cutoff $\\omega_c = 0.4\\pi$ removes components above $1.2566$, while the target frequency is $\\frac{\\pi}{2} \\approx 1.5708$; expect high NMSE.\n- Test case 3 (high-pass $0.2\\pi$, Hcos): keeps frequencies above $0.6283$, including $\\frac{\\pi}{2}$; expect low NMSE.\n- Test case 4 (low-pass $0.1\\pi$, Window): keeps only very low components; sharp edges cannot be represented; expect high NMSE.\n- Test case 5 (high-pass $0.8\\pi$, Lcos): keeps only very high components, removing low ones; expect high NMSE.\n- Test case 6 (low-pass $0.01$, Lcos): cutoff below $\\omega_{\\min}$, so only the constant remains; expect NMSE near $1$.\n- Test case 7 (high-pass $\\pi + 0.01$, Hcos): cutoff above $\\omega_{\\max}$, so only the constant remains; expect NMSE near $1$.\n- Test case 8 (none, Square8): full basis allows approximation of a square wave via harmonics; expect moderate NMSE (nonzero due to finite $K$).\n\nThe program will implement these steps deterministically and output the NMSEs as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_frequencies(N: int, K: int) -> np.ndarray:\n    # Logarithmically spaced angular frequencies in radians.\n    omega_min = 2 * np.pi / N\n    omega_max = np.pi\n    return np.geomspace(omega_min, omega_max, K)\n\ndef positional_encoding_matrix(N: int, omegas: np.ndarray) -> np.ndarray:\n    # Build PE matrix with shape (N, 1 + 2*K): [bias, sin(ω_k p), cos(ω_k p)]\n    K = len(omegas)\n    D = 1 + 2 * K\n    X = np.zeros((N, D), dtype=np.float64)\n    # Bias term\n    X[:, 0] = 1.0\n    # Sin/Cos terms\n    p = np.arange(N, dtype=np.float64)\n    for idx, omega in enumerate(omegas):\n        X[:, 1 + 2 * idx] = np.sin(omega * p)\n        X[:, 1 + 2 * idx + 1] = np.cos(omega * p)\n    return X\n\ndef apply_ablation(X: np.ndarray, omegas: np.ndarray, ablation_type: str, omega_c: float) -> np.ndarray:\n    # Always keep bias column 0\n    cols = [0]\n    if ablation_type == \"none\":\n        # Keep all sin/cos pairs\n        K = len(omegas)\n        for idx in range(K):\n            cols.extend([1 + 2 * idx, 1 + 2 * idx + 1])\n    elif ablation_type == \"low_pass\":\n        for idx, omega in enumerate(omegas):\n            if omega <= omega_c:\n                cols.extend([1 + 2 * idx, 1 + 2 * idx + 1])\n    elif ablation_type == \"high_pass\":\n        for idx, omega in enumerate(omegas):\n            if omega >= omega_c:\n                cols.extend([1 + 2 * idx, 1 + 2 * idx + 1])\n    else:\n        raise ValueError(f\"Unknown ablation type: {ablation_type}\")\n    return X[:, cols]\n\ndef target_function(N: int, task: str) -> np.ndarray:\n    p = np.arange(N, dtype=np.float64)\n    if task == \"Lcos\":\n        # Low-frequency cosine, period 64\n        return np.cos(2 * np.pi * p / 64.0)\n    elif task == \"Hcos\":\n        # High-frequency cosine, period 4\n        return np.cos(2 * np.pi * p / 4.0)\n    elif task == \"Window\":\n        # Indicator of window centered at 64 with radius 5\n        return ((np.abs(p - 64) <= 5).astype(np.float64))\n    elif task == \"Square8\":\n        # Square wave of period 8, 50% duty cycle\n        return ((p % 8) < 4).astype(np.float64)\n    else:\n        raise ValueError(f\"Unknown task: {task}\")\n\ndef nmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    mse = np.mean((y_true - y_pred) ** 2)\n    var = np.var(y_true)\n    # var should be > 0 for given tasks\n    return float(mse / var)\n\ndef solve():\n    # Define constants\n    N = 128\n    K = 8\n    omegas = build_frequencies(N, K)\n    X_full = positional_encoding_matrix(N, omegas)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\"none\", np.pi / 2, \"Lcos\"),\n        (\"low_pass\", 0.4 * np.pi, \"Hcos\"),\n        (\"high_pass\", 0.2 * np.pi, \"Hcos\"),\n        (\"low_pass\", 0.1 * np.pi, \"Window\"),\n        (\"high_pass\", 0.8 * np.pi, \"Lcos\"),\n        (\"low_pass\", 0.01, \"Lcos\"),\n        (\"high_pass\", np.pi + 0.01, \"Hcos\"),\n        (\"none\", np.pi / 2, \"Square8\"),\n    ]\n\n    results = []\n    for ablation_type, omega_c, task in test_cases:\n        X = apply_ablation(X_full, omegas, ablation_type, omega_c)\n        y = target_function(N, task)\n        # Least squares fit\n        w, *_ = np.linalg.lstsq(X, y, rcond=None)\n        y_hat = X @ w\n        results.append(nmse(y, y_hat))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3164200"}, {"introduction": "Positional encodings are not just standalone features; they are a critical input to the self-attention mechanism. This final practice explores the crucial interaction between the magnitude of positional encodings and the behavior of the softmax function at the heart of attention. Through a controlled experiment [@problem_id:3180897], you will vary the amplitude of the PE vectors and observe how the resulting attention map can either diffuse into a uniform distribution or collapse into a single sharp peak, illustrating a key aspect of training stability in Transformers.", "problem": "You are tasked with building a program to study how the scaled dot-product attention mechanism depends on the magnitude of positional encodings. The goal is to quantify whether attention maps collapse (become overly concentrated) or diffuse (become overly uniform) when the amplitude of positional encodings is varied. The analysis must be expressed purely in mathematical and algorithmic terms, without relying on empirical datasets, and should be based on core definitions widely used in deep learning.\n\nStarting Point (Core Definitions):\n- Scaled Dot-Product Attention (SDPA): Given queries $Q$, keys $K$, and values $V$, the attention weights are computed by applying the softmax function row-wise to the score matrix formed by normalized inner products. Let $d_k$ denote the key dimension. The normalized score matrix is formed by dividing by $\\sqrt{d_k}$ before applying softmax.\n- Sinusoidal Positional Encoding (PE): For a sequence of length $L$ and model dimension $d$, the positional encoding matrix $PE \\in \\mathbb{R}^{L \\times d}$ assigns to each position $p \\in \\{0,\\dots,L-1\\}$ a deterministic vector that alternates between sine and cosine functions with geometrically increasing wavelengths. This matrix is fixed and does not depend on the input content.\n- Amplitude Scaling: To isolate the role of positional encodings, use content vectors $x_p \\in \\mathbb{R}^d$ that are identically zero for all positions $p$. The effective input embedding at position $p$ is $e_p = x_p + a \\cdot PE_p = a \\cdot PE_p$, where $a \\ge 0$ is a scalar amplitude. The queries $Q$ and keys $K$ are formed by linear projections of the embeddings with fixed matrices and the attention weights are computed using SDPA.\n\nYour Program Must:\n1. Construct a sinusoidal positional encoding $PE \\in \\mathbb{R}^{L \\times d}$ for a sequence of length $L$ and even model dimension $d$, using the standard alternating sine and cosine design with logarithmically spaced frequencies. Use $L = 16$ and $d = 32$.\n2. Set the content embeddings $x_p$ to the zero vector for all positions $p \\in \\{0, \\dots, L-1\\}$ so that $e_p = a \\cdot PE_p$.\n3. Fix the random seed $s = 7$ and sample two independent matrices $W_Q \\in \\mathbb{R}^{d \\times d}$ and $W_K \\in \\mathbb{R}^{d \\times d}$ with entries drawn from a standard normal distribution. Define $Q = E W_Q$ and $K = E W_K$, where $E \\in \\mathbb{R}^{L \\times d}$ stacks the embeddings $e_p$ row-wise.\n4. Compute the attention weight matrix $A \\in \\mathbb{R}^{L \\times L}$ using scaled dot-product attention: apply the softmax function row-wise to the score matrix formed by the inner products of $Q$ and $K$, divided by $\\sqrt{d_k}$ with $d_k = d$.\n5. For each row $A_{p,:}$, compute the Shannon entropy $H_p = -\\sum_{j=0}^{L-1} A_{p,j} \\log(A_{p,j})$ using the natural logarithm. Also compute the maximum attention weight $M_p = \\max_{j=0,\\dots,L-1} A_{p,j}$. Aggregate these by computing the averages $\\bar{H} = \\frac{1}{L} \\sum_{p=0}^{L-1} H_p$ and $\\bar{M} = \\frac{1}{L} \\sum_{p=0}^{L-1} M_p$.\n6. Repeat steps $2$ through $5$ for each amplitude $a$ in the following test suite:\n   - Boundary and near-boundary cases: $a = 0.0$, $a = 10^{-6}$.\n   - Low to moderate cases: $a = 0.05$, $a = 0.5$, $a = 1.0$.\n   - High-amplitude cases: $a = 5.0$, $a = 20.0$.\n7. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element should be a two-element list `[H_bar, M_bar]` for a corresponding test amplitude, in the order given above. For example, the output format must be exactly like `[[h_1,m_1],[h_2,m_2],...,[h_7,m_7]]` where each $h_i$ and $m_i$ is a floating-point number.\n\nScientific Realism and Design for Coverage:\n- The case $a = 0.0$ forces $Q = 0$ and $K = 0$, producing a uniform attention map with maximal entropy and minimal maximum weight, which serves as a boundary condition.\n- Very small $a$ values keep the scores near zero, testing numerical stability and near-uniform diffusion.\n- Large $a$ values magnify positional differences through the projections, testing softmax saturation, potential collapse in attention maps, and reduced entropy.\n\nFinal Output:\n- The program must output a single line, formatted as specified, containing seven pairs of floating-point numbers. No additional text should be printed.", "solution": "The problem statement has been critically validated and is deemed valid. It is scientifically grounded in the principles of deep learning, specifically the Transformer architecture's attention mechanism. The problem is well-posed, providing a complete and consistent set of definitions, parameters, and constraints, which allows for a unique, deterministic, and verifiable solution. The experimental design is a sound and conventional method for isolating and analyzing the functional role of a specific model component—in this case, sinusoidal positional encodings.\n\nThe core of the problem is to investigate the behavior of scaled dot-product attention (SDPA) under varying amplitudes of positional information, with content information being deliberately nullified. This is achieved by setting the input embeddings $e_p$ at position $p$ to be a scaled version of the positional encoding vector $PE_p$, i.e., $e_p = a \\cdot PE_p$, where $a \\geq 0$ is a scalar amplitude.\n\nFirst, we construct the sinusoidal positional encoding matrix $PE \\in \\mathbb{R}^{L \\times d}$ for a sequence of length $L=16$ and a model dimension $d=32$. For each position $p \\in \\{0, \\dots, L-1\\}$ and dimension index $k \\in \\{0, \\dots, d-1\\}$, the elements of $PE$ are defined by the standard alternating sine and cosine functions:\n$$\nPE_{(p, 2i)} = \\sin\\left(\\frac{p}{10000^{2i/d}}\\right) \\quad \\text{for } i \\in \\{0, \\dots, d/2-1\\}\n$$\n$$\nPE_{(p, 2i+1)} = \\cos\\left(\\frac{p}{10000^{2i/d}}\\right) \\quad \\text{for } i \\in \\{0, \\dots, d/2-1\\}\n$$\nThis formulation creates unique positional vectors that encode relative and absolute positions.\n\nThe input embeddings for the attention mechanism are formed by stacking the vectors $e_p$ into a matrix $E \\in \\mathbb{R}^{L \\times d}$. The query and key matrices, $Q \\in \\mathbb{R}^{L \\times d}$ and $K \\in \\mathbb{R}^{L \\times d}$ respectively, are then generated through linear projections of these embeddings using fixed weight matrices $W_Q \\in \\mathbb{R}^{d \\times d}$ and $W_K \\in \\mathbb{R}^{d \\times d}$. These weight matrices are sampled once from a standard normal distribution, with the random seed fixed to $s=7$ to ensure reproducibility.\n$$\nE = a \\cdot PE\n$$\n$$\nQ = E W_Q\n$$\n$$\nK = E W_K\n$$\nThe attention scores are calculated using the scaled dot-product operation. The score matrix $S \\in \\mathbb{R}^{L \\times L}$ is computed by taking the matrix product of $Q$ and the transpose of $K$, and scaling the result by the inverse square root of the key dimension, which is set to $d_k=d=32$.\n$$\nS = \\frac{Q K^T}{\\sqrt{d_k}}\n$$\nThe attention weights matrix $A \\in \\mathbb{R}^{L \\times L}$ is obtained by applying the row-wise softmax function to the score matrix $S$. For each row $p$, the attention weight $A_{p,j}$ for the pair of positions $(p, j)$ is:\n$$\nA_{p,j} = \\text{softmax}(S_{p,:})_j = \\frac{\\exp(S_{p,j})}{\\sum_{k=0}^{L-1} \\exp(S_{p,k})}\n$$\nWhen the amplitude $a$ is $0$, $E$, $Q$, and $K$ become zero matrices. Consequently, the score matrix $S$ is also a zero matrix. The softmax of a vector of zeros is a uniform distribution, so each attention weight $A_{p,j}$ becomes $1/L = 1/16$. This serves as a baseline for a perfectly diffused attention map. Conversely, as $a$ becomes large, the variance of the scores in each row of $S$ increases, causing the softmax function to saturate. This leads to a \"winner-take-all\" dynamic where one attention weight in a row approaches $1$ and the others approach $0$, signifying a collapse of attention.\n\nTo quantify this behavior, two metrics are computed for each row $p$ of the attention matrix $A$:\n1.  **Shannon Entropy**, $H_p$, which measures the uncertainty or diffusion of the attention distribution. A higher entropy corresponds to a more uniform distribution.\n    $$\n    H_p = -\\sum_{j=0}^{L-1} A_{p,j} \\log(A_{p,j})\n    $$\n2.  **Maximum Attention Weight**, $M_p$, which measures the concentration of attention. A higher maximum weight indicates a more focused, or collapsed, attention distribution.\n    $$\n    M_p = \\max_{j} A_{p,j}\n    $$\nThese metrics are then averaged over all $L$ rows to produce the final aggregate metrics, $\\bar{H}$ and $\\bar{M}$. The entire process is repeated for a specified suite of amplitudes $a$: $\\{0.0, 10^{-6}, 0.05, 0.5, 1.0, 5.0, 20.0\\}$, allowing for a systematic analysis of the impact of positional encoding magnitude. The code will implement this procedure, calculating $[\\bar{H}, \\bar{M}]$ for each amplitude.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import softmax\n\ndef solve():\n    \"\"\"\n    Computes attention metrics for varying positional encoding amplitudes.\n    \"\"\"\n    \n    # Define parameters from the problem statement.\n    L = 16  # Sequence length\n    d = 32  # Model dimension\n    d_k = d # Key dimension\n    seed = 7  # Random seed\n    amplitudes = [0.0, 1e-6, 0.05, 0.5, 1.0, 5.0, 20.0]\n\n    # Initialize random number generator and create projection matrices\n    rng = np.random.default_rng(seed)\n    W_Q = rng.standard_normal(size=(d, d))\n    W_K = rng.standard_normal(size=(d, d))\n\n    def create_pe(length, dim):\n        \"\"\"\n        Constructs a sinusoidal positional encoding matrix.\n        \"\"\"\n        if dim % 2 != 0:\n            raise ValueError(\"Model dimension d must be even.\")\n        \n        position = np.arange(length, dtype=np.float64).reshape(-1, 1)\n        # Denominator term in the PE formula\n        div_term = np.exp(np.arange(0, dim, 2, dtype=np.float64) * -(np.log(10000.0) / dim))\n        \n        pe = np.zeros((length, dim), dtype=np.float64)\n        pe[:, 0::2] = np.sin(position * div_term)\n        pe[:, 1::2] = np.cos(position * div_term)\n        return pe\n\n    # Pre-compute the positional encoding matrix\n    pe_matrix = create_pe(L, d)\n\n    results = []\n    for a in amplitudes:\n        # Step 2: Construct embeddings\n        # Content embeddings x_p are zero, so E is just scaled PE\n        E = a * pe_matrix\n\n        # Step 3: Define Q and K matrices\n        Q = E @ W_Q\n        K = E @ W_K\n\n        # Step 4: Compute attention weights\n        # Score matrix S = (Q @ K^T) / sqrt(d_k)\n        scores = (Q @ K.T) / np.sqrt(d_k)\n        \n        # Row-wise softmax to get attention weights A\n        # softmax handles the a=0 case correctly (uniform distribution)\n        attention_weights = softmax(scores, axis=1)\n\n        # Step 5: Compute metrics\n        \n        # Entropy H_p for each row p\n        # Use `where` to avoid log(0), as A_pj * log(A_pj) is 0 if A_pj is 0.\n        entropy_per_row = -np.sum(attention_weights * np.log(attention_weights, \n                                                             where=attention_weights > 0, \n                                                             out=np.zeros_like(attention_weights)), \n                                  axis=1)\n\n        # Max attention weight M_p for each row p\n        max_weight_per_row = np.max(attention_weights, axis=1)\n\n        # Aggregate by averaging over all L rows\n        avg_H = np.mean(entropy_per_row)\n        avg_M = np.mean(max_weight_per_row)\n\n        results.append([avg_H, avg_M])\n\n    # Final print statement in the exact required format.\n    # The string representation of a list is '[item1, item2]', which matches\n    # the inner list formatting requirement.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3180897"}]}