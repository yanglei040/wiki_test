{"hands_on_practices": [{"introduction": "Many real-world phenomena are described by nonlinear models. However, not all of them require complex iterative solvers. This first exercise demonstrates a powerful technique where a nonlinear model can be transformed into a linear one through a change of variables, allowing us to use the straightforward methods of linear least squares. By fitting data from an RC circuit [@problem_id:2191267], you will practice how to identify and exploit these opportunities for simplification.", "problem": "An engineering student is tasked with determining the time constant of a simple Resistor-Capacitor (RC) circuit. The capacitor is initially charged and then allowed to discharge through the resistor. The voltage $V$ across the capacitor is measured at several time points $t$ using an oscilloscope, yielding the following data set:\n- At $t = 1.0$ s, $V = 3.03$ V\n- At $t = 2.0$ s, $V = 1.84$ V\n- At $t = 3.0$ s, $V = 1.12$ V\n- At $t = 4.0$ s, $V = 0.68$ V\n\nThe theoretical model for this process is given by $V(t) = V_0 \\exp(-t/\\tau)$, where $V_0$ is the voltage at $t=0$ and $\\tau$ is the time constant of the circuit. By transforming this nonlinear model into a linear form and applying the method of linear least squares to the provided data, determine the best estimate for the time constant $\\tau$.\n\nExpress your answer for the time constant $\\tau$ in seconds, rounded to three significant figures.", "solution": "We start from the model for an RC discharge, $V(t)=V_{0}\\exp(-t/\\tau)$. Taking the natural logarithm of both sides gives a linear relation in $t$:\n$$\n\\ln V(t)=\\ln V_{0}-\\frac{1}{\\tau}t.\n$$\nDefine $y=\\ln V$ and $x=t$. Then the model is $y=b+mx$ with $b=\\ln V_{0}$ and $m=-\\frac{1}{\\tau}$. Using linear least squares for the data points $(x_{i},y_{i})$, $i=1,\\dots,n$, the slope $m$ is\n$$\nm=\\frac{n\\sum x_{i}y_{i}-\\left(\\sum x_{i}\\right)\\left(\\sum y_{i}\\right)}{n\\sum x_{i}^{2}-\\left(\\sum x_{i}\\right)^{2}},\n$$\nand the time constant is $\\tau=-\\frac{1}{m}$.\n\nFrom the measurements:\n- $(t_{i},V_{i})$: $(1.0,3.03)$, $(2.0,1.84)$, $(3.0,1.12)$, $(4.0,0.68)$.\n- Compute $y_{i}=\\ln V_{i}$:\n$$\n\\ln(3.03)\\approx 1.10856262,\\quad \\ln(1.84)\\approx 0.60976535,\\quad \\ln(1.12)\\approx 0.11332914,\\quad \\ln(0.68)\\approx -0.38566200.\n$$\nNow compute the required sums with $n=4$:\n$$\n\\sum x_{i}=1+2+3+4=10,\\quad \\sum x_{i}^{2}=1+4+9+16=30,\n$$\n$$\n\\sum y_{i}\\approx 1.10856262+0.60976535+0.11332914-0.38566200\\approx 1.44599511,\n$$\n$$\n\\sum x_{i}y_{i}\\approx 1\\cdot 1.10856262+2\\cdot 0.60976535+3\\cdot 0.11332914+4\\cdot(-0.38566200)\\approx 1.12543274.\n$$\nSubstitute into the slope formula:\n$$\nm=\\frac{4\\cdot 1.12543274-10\\cdot 1.44599511}{4\\cdot 30-10^{2}}=\\frac{4.50173096-14.4599511}{20}\\approx -0.49791101.\n$$\nTherefore,\n$$\n\\tau=-\\frac{1}{m}\\approx \\frac{1}{0.49791101}\\approx 2.00839\\ \\text{s}.\n$$\nRounded to three significant figures, the best estimate is $2.01$ s.", "answer": "$$\\boxed{2.01}$$", "id": "2191267"}, {"introduction": "When a model cannot be linearized, we turn to iterative methods like the Gauss-Newton algorithm. These methods refine an initial guess for the parameters by repeatedly solving a linear approximation of the problem. To truly understand how these solvers work, it is essential to perform the core calculations by hand, and this practice guides you through that process [@problem_id:2191241]. You will compute a single update step to see exactly how the algorithm uses residuals and the Jacobian matrix to find a better parameter estimate.", "problem": "An experimental physicist is studying the radioactive decay of a newly synthesized isotope. The activity of the sample, which is the number of decays per second, is measured at several time points. The data collected are as follows:\n\n- At time $t=0.0$ hours, the activity is $95$ Becquerels (Bq).\n- At time $t=1.0$ hour, the activity is $35$ Bq.\n- At time $t=2.0$ hours, the activity is $13$ Bq.\n\nThe theoretical model for the activity $A(t)$ as a function of time $t$ is given by the exponential decay law:\n$$ A(t; C, \\lambda) = C e^{-\\lambda t} $$\nwhere $C$ is the initial activity at $t=0$ and $\\lambda$ is the decay constant in units of inverse hours.\n\nTo estimate the parameters $C$ and $\\lambda$ from the experimental data, the physicist decides to use the Gauss-Newton method for nonlinear least squares. The goal is to find the parameter vector $\\mathbf{x} = (C, \\lambda)^T$ that minimizes the sum of the squared differences between the model's predictions and the measured data.\n\nThe Gauss-Newton algorithm iteratively refines the parameter estimates. Starting with an initial guess $\\mathbf{x}_0$, the update for the parameters, $\\Delta\\mathbf{x}$, is found by solving the linear system known as the normal equations:\n$$ (J^T J) \\Delta\\mathbf{x} = -J^T \\mathbf{f} $$\nwhere $\\mathbf{f}$ is the vector of residuals (model prediction minus measured data) and $J$ is the Jacobian matrix of the model function with respect to the parameters, both evaluated at the current guess. The new estimate is then $\\mathbf{x}_1 = \\mathbf{x}_0 + \\Delta\\mathbf{x}$.\n\nStarting with the initial guess $\\mathbf{x}_0 = (C_0, \\lambda_0) = (100.0, 0.900)$, perform exactly one iteration of the Gauss-Newton method to find the updated parameter estimates $\\mathbf{x}_1 = (C_1, \\lambda_1)$.\n\nProvide the numerical values for the updated parameters $C_1$ and $\\lambda_1$. Round your final answers to three significant figures.", "solution": "We model the activity by $A(t;C,\\lambda)=C\\exp(-\\lambda t)$. For data $(t_{i},y_{i})$ with $(t_{0},y_{0})=(0,95)$, $(t_{1},y_{1})=(1,35)$, $(t_{2},y_{2})=(2,13)$, the residuals at $(C,\\lambda)$ are\n$$\nr_{i}(C,\\lambda)=C\\exp(-\\lambda t_{i})-y_{i}.\n$$\nThe Jacobian of the residual vector with respect to $(C,\\lambda)$ has rows\n$$\n\\left[\\frac{\\partial r_{i}}{\\partial C},\\frac{\\partial r_{i}}{\\partial \\lambda}\\right]=\\left[\\exp(-\\lambda t_{i}),-Ct_{i}\\exp(-\\lambda t_{i})\\right].\n$$\nAt the initial guess $\\mathbf{x}_{0}=(C_{0},\\lambda_{0})=(100,0.9)$, define\n$$\na_{0}=\\exp(-0.9\\cdot 0)=1,\\quad a_{1}=\\exp(-0.9)=0.4065696597405991,\\quad a_{2}=\\exp(-1.8)=0.16529888822158653,\n$$\nand note $a_{1}^{2}=a_{2}$, $a_{2}^{2}=\\exp(-3.6)$, $a_{2}^{3}=\\exp(-5.4)$. The model values are\n$$\nA(t_{0})=100a_{0}=100,\\quad A(t_{1})=100a_{1}=40.65696597405991,\\quad A(t_{2})=100a_{2}=16.529888822158653,\n$$\nso the residual vector $\\mathbf{f}$ (model minus data) is\n$$\n\\mathbf{f}=\\begin{pmatrix}100-95\\\\ 100a_{1}-35\\\\ 100a_{2}-13\\end{pmatrix}=\\begin{pmatrix}5\\\\ 5.65696597405991\\\\ 3.529888822158653\\end{pmatrix}.\n$$\nThe Jacobian at $\\mathbf{x}_{0}$ is\n$$\nJ=\\begin{pmatrix}\na_{0} & -C_{0}\\cdot 0\\cdot a_{0}\\\\\na_{1} & -C_{0}\\cdot 1\\cdot a_{1}\\\\\na_{2} & -C_{0}\\cdot 2\\cdot a_{2}\n\\end{pmatrix}\n=\\begin{pmatrix}\n1 & 0\\\\\na_{1} & -100a_{1}\\\\\na_{2} & -200a_{2}\n\\end{pmatrix}.\n$$\nCompute $J^{T}J$ and $J^{T}\\mathbf{f}$. Using $a_{1}^{2}=a_{2}$,\n$$\nJ^{T}J=\\begin{pmatrix}\n1+a_{1}^{2}+a_{2}^{2} & a_{1}(-100a_{1})+a_{2}(-200a_{2})\\\\\na_{1}(-100a_{1})+a_{2}(-200a_{2}) & (100a_{1})^{2}+(200a_{2})^{2}\n\\end{pmatrix}\n=\\begin{pmatrix}\n1+a_{2}+a_{2}^{2} & -100a_{2}-200a_{2}^{2}\\\\\n-100a_{2}-200a_{2}^{2} & 10000a_{2}+40000a_{2}^{2}\n\\end{pmatrix}.\n$$\nNumerically,\n$$\nJ^{T}J=\\begin{pmatrix}\n1.1926226106688791 & -21.994633311617165\\\\\n-21.994633311617165 & 2745.937780107568\n\\end{pmatrix}.\n$$\nNext,\n$$\nJ^{T}\\mathbf{f}=\\begin{pmatrix}\na_{0}r_{0}+a_{1}r_{1}+a_{2}r_{2}\\\\\n0\\cdot r_{0}+(-100a_{1})r_{1}+(-200a_{2})r_{2}\n\\end{pmatrix}.\n$$\nUsing $r_{1}=100a_{1}-35$ and $r_{2}=100a_{2}-13$, the first component simplifies to\n$$\na_{0}r_{0}+a_{1}r_{1}+a_{2}r_{2}=5 - 35a_{1} + 87a_{2} + 100a_{2}^{2},\n$$\nand the second to\n$$\n(-100a_{1})r_{1}+(-200a_{2})r_{2}=3500a_{1}-7400a_{2}-20000a_{2}^{2}.\n$$\nNumerically,\n$$\nJ^{T}\\mathbf{f}=\\begin{pmatrix}7.883437429086315\\\\ -346.69241269349484\\end{pmatrix}.\n$$\nThe Gauss-Newton normal equations are $(J^{T}J)\\Delta\\mathbf{x}=-J^{T}\\mathbf{f}$. Let $M=J^{T}J$ and $\\mathbf{b}=-J^{T}\\mathbf{f}=\\begin{pmatrix}-7.883437429086315\\\\ 346.69241269349484\\end{pmatrix}$. Solve $M\\Delta\\mathbf{x}=\\mathbf{b}$ by Cramer’s rule. The determinant simplifies to\n$$\n\\det(M)=(1+a_{2}+a_{2}^{2})(10000a_{2}+40000a_{2}^{2})-(100a_{2}+200a_{2}^{2})^{2}=10000a_{2}+40000a_{2}^{2}+10000a_{2}^{3},\n$$\nwhich numerically is\n$$\n\\det(M)=2791.1035895336945.\n$$\nFor $\\Delta C$, the numerator is\n$$\nN_{1}=b_{1}M_{22}-b_{2}M_{12}=(10000a_{2}+40000a_{2}^{2})b_{1}+(100a_{2}+200a_{2}^{2})b_{2},\n$$\nwhich simplifies (using the expressions for $b_{1}$ and $b_{2}$) to\n$$\nN_{1}=700000\\,a_{2}^{2}a_{1}-50000\\,a_{2}-330000\\,a_{2}^{2}-1000000\\,a_{2}^{3}.\n$$\nNumerically,\n$$\nN_{1}=-14022.056184528922,\\qquad \\Delta C=\\frac{N_{1}}{\\det(M)}\\approx -5.0238394.\n$$\nFor $\\Delta\\lambda$, the numerator is\n$$\nN_{2}=M_{11}b_{2}-M_{12}b_{1}=(1+a_{2}+a_{2}^{2})b_{2}+(100a_{2}+200a_{2}^{2})b_{1},\n$$\nwhich simplifies to\n$$\nN_{2}=-3500a_{1}+3500a_{2}^{2}a_{1}+6900a_{2}+17700a_{2}^{2}.\n$$\nNumerically,\n$$\nN_{2}=240.07989483777672,\\qquad \\Delta\\lambda=\\frac{N_{2}}{\\det(M)}\\approx 0.0860161.\n$$\nUpdate the parameters:\n$$\nC_{1}=C_{0}+\\Delta C=100-5.0238394\\approx 94.9762,\\quad \\lambda_{1}=\\lambda_{0}+\\Delta\\lambda=0.9+0.0860161\\approx 0.986016.\n$$\nRounded to three significant figures,\n$$\nC_{1}\\approx 95.0,\\qquad \\lambda_{1}\\approx 0.986.\n$$", "answer": "$$\\boxed{\\begin{pmatrix}95.0 & 0.986\\end{pmatrix}}$$", "id": "2191241"}, {"introduction": "The Gauss-Newton method is powerful but can be unstable if the initial guess is poor or the problem is ill-conditioned. The Levenberg-Marquardt (LM) algorithm provides a robust alternative by adaptively blending the Gauss-Newton step with a more conservative gradient descent step. This final, comprehensive practice challenges you to implement the LM algorithm from the ground up [@problem_id:3256843], complete with adaptive damping and numerical Jacobian calculation, giving you a tool used widely across scientific and engineering fields.", "problem": "You are tasked with implementing a complete, self-contained program that solves a nonlinear least squares fitting problem using the Levenberg-Marquardt algorithm, where the damping parameter is automatically adjusted based on the ratio of actual to predicted decrease in the sum of squares. The derivation and implementation must begin from the fundamental definition of the nonlinear least squares objective and proceed to an algorithm that is scientifically and numerically sound.\n\nBegin from the core definition of the nonlinear least squares objective. Let the parameter vector be $p \\in \\mathbb{R}^m$, the independent variable vector be $x \\in \\mathbb{R}^n$, the observed data (dependent variable) be $y \\in \\mathbb{R}^n$, and the model be a function $f:\\mathbb{R}^n \\times \\mathbb{R}^m \\to \\mathbb{R}^n$ that predicts the data from parameters. Define the residual vector $r(p) \\in \\mathbb{R}^n$ by $r(p) = y - f(x,p)$ and the objective function by $F(p) = \\frac{1}{2}\\lVert r(p)\\rVert_2^2$. Define the Jacobian matrix $J(p) \\in \\mathbb{R}^{n \\times m}$ with entries $J_{ij}(p) = \\frac{\\partial r_i(p)}{\\partial p_j}$. Your program must use central finite differences to approximate the Jacobian numerically at every iteration.\n\nAt each iteration, construct a local quadratic model of $F(p)$ by a second-order approximation around a current parameter $p$ using $r(p)$ and $J(p)$, and compute a parameter increment that balances between the Gauss–Newton direction and a gradient-descent direction by solving a damped normal equation. The algorithm must decide whether to accept or reject the proposed step using a gain ratio that compares the actual decrease in $F(p)$ to the predicted decrease in the local model used to compute the step. The damping parameter must be adjusted automatically based on this gain ratio: reduce the damping when the step is productive and increase it when the step is unproductive. Use a termination criterion that stops the iterations when either the infinity norm of the gradient is small, the parameter update is small, or the change in the objective function is negligible.\n\nThe angle quantity $\\phi$ used in the sinusoidal model must be treated and computed in radians.\n\nYour program must implement the above for the following three test cases, each with a specific model, dataset, and initial parameter guess. For each test case, return the fitted parameter vector as a list of floats rounded to six decimal places.\n\nTest Case 1 (general convergence):\n- Model: $f(x,p) = p_1 \\exp(p_2 x) + p_3$, with parameters $p = [p_1,p_2,p_3]$.\n- Data: $x$ defined by $x_i = 0.0 + i\\Delta$ for $i=0,1,\\dots,30$ and $\\Delta = 0.1$, so $x \\in [0,3.0]$ with $31$ points. Observations defined by $y = 2.0\\exp(-0.7 x) + 0.5 + 0.01\\sin(5x)$.\n- Initial parameter: $p^{(0)} = [1.0,-0.2,0.0]$.\n\nTest Case 2 (steep nonlinearity):\n- Model: logistic curve $f(x,p) = \\frac{p_1}{1 + \\exp(-p_2(x - p_3))}$, with parameters $p = [p_1,p_2,p_3]$.\n- Data: $x$ defined by $x_i = -1.5 + i\\Delta$ for $i=0,1,\\dots,14$ and $\\Delta = \\frac{2.0 - (-1.5)}{14}$, so $x \\in [-1.5,2.0]$ with $15$ points. Observations defined by $y = \\frac{1.5}{1 + \\exp(-3.0(x - 0.5))} + 0.02\\cos(3x)$.\n- Initial parameter: $p^{(0)} = [1.0,1.0,0.0]$.\n\nTest Case 3 (near-degeneracy in sensitivity):\n- Model: sinusoid with fixed angular frequency $\\omega = 2.5$ radians per unit of $x$, $f(x,p) = p_1 \\cos(\\omega x + p_2) + p_3$, with parameters $p = [p_1,p_2,p_3]$ and $\\phi = p_2$ in radians.\n- Data: $x$ defined by $x_i = 0.0 + i\\Delta$ for $i=0,1,\\dots,20$ and $\\Delta = 0.1$, so $x \\in [0,2.0]$ with $21$ points. Observations defined by $y = 0.8\\cos(2.5 x + 0.4) - 0.05 + 0.01\\sin(7x)$.\n- Initial parameter: $p^{(0)} = [0.5,0.0,0.0]$.\n\nAlgorithmic requirements:\n- Use central finite differences for the Jacobian with a perturbation step proportional to $\\sqrt{\\varepsilon}(1 + |p_j|)$, where $\\varepsilon$ is machine precision for double-precision floating-point arithmetic.\n- Construct and solve a damped normal equation to obtain a parameter increment at each iteration.\n- Compute the actual decrease in the objective function and the predicted decrease in the local model used in the step computation. Use their ratio to decide acceptance of the step and to adapt the damping parameter.\n- Terminate when any standard smallness criterion is met: the infinity norm of the gradient is small, the parameter change is small, or the change in the objective function is small. Also include a maximum iteration cap to ensure the algorithm halts.\n- The angle $\\phi$ in the sinusoidal model must be computed in radians throughout.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list of the fitted parameter values rounded to six decimal places. The final output format must be exactly of the form:\n\"[[p11,p12,p13],[p21,p22,p23],[p31,p32,p33]]\"\nwhere $p1k$, $p2k$, and $p3k$ are the fitted parameters for test cases $1$, $2$, and $3$, respectively, expressed as decimal numbers.", "solution": "The user-provided problem is assessed to be **valid**. It is a well-posed, scientifically grounded, and computationally feasible task from the field of numerical methods. It requires the implementation of the Levenberg-Marquardt algorithm for nonlinear least squares, a standard and important technique in scientific computing. All necessary components, including the objective function, algorithmic procedures, test models, datasets, and initial conditions, are provided and internally consistent.\n\nHerein, a complete solution is derived from first principles.\n\n### 1. The Nonlinear Least Squares Problem\n\nThe core of the problem is to find the set of parameters that best fits a model to a set of observed data. Let the parameter vector be $p \\in \\mathbb{R}^m$, the vector of independent variables be $x \\in \\mathbb{R}^n$, and the corresponding observed data vector be $y \\in \\mathbb{R}^n$. A model function, $f(x, p)$, maps the parameters $p$ and independent variables $x$ to a set of predicted data points. The goal is to find the parameter vector $p$ that minimizes the sum of the squared differences between the observed data $y$ and the predicted data $f(x,p)$.\n\nThis is formally expressed by defining a residual vector $r(p) \\in \\mathbb{R}^n$ as:\n$$\nr(p) = y - f(x, p)\n$$\nThe objective is to minimize the scalar objective function $F(p)$, defined as one-half of the squared Euclidean norm of the residual vector:\n$$\nF(p) = \\frac{1}{2}\\lVert r(p)\\rVert_2^2 = \\frac{1}{2} \\sum_{i=1}^{n} r_i(p)^2\n$$\nSince the model $f(x,p)$ is generally a nonlinear function of the parameters $p$, this is a nonlinear least squares problem, which must be solved using iterative methods.\n\n### 2. Iterative Solution via Local Approximation\n\nThe Levenberg-Marquardt algorithm is an iterative procedure that finds a local minimum of $F(p)$. Starting from an initial guess $p^{(0)}$, the algorithm generates a sequence of parameter vectors $p^{(1)}, p^{(2)}, \\dots$ that progressively reduce the value of $F(p)$. Each iteration computes a step vector $h$ that updates the current parameter vector: $p_{k+1} = p_k + h$.\n\nThe step $h$ is found by forming a local quadratic model of the objective function $F(p)$ around the current point $p_k$. This model is constructed by first linearizing the residual vector $r(p_k+h)$ using a first-order Taylor series expansion:\n$$\nr(p_k + h) \\approx r(p_k) + J(p_k)h\n$$\nwhere $J(p_k)$ is the Jacobian matrix of the residual vector $r$ evaluated at $p_k$. The entries of the Jacobian are given by $J_{ij}(p) = \\frac{\\partial r_i(p)}{\\partial p_j}$. Since $r(p) = y - f(x,p)$, this is equivalent to $J_{ij}(p) = -\\frac{\\partial f_i(x,p)}{\\partial p_j}$.\n\nSubstituting this linear approximation of the residual into the objective function $F(p_k+h)$ yields a local quadratic model $L(h)$:\n$$\nF(p_k + h) = \\frac{1}{2}\\lVert r(p_k+h) \\rVert_2^2 \\approx \\frac{1}{2}\\lVert r(p_k) + J(p_k)h \\rVert_2^2 = L(h)\n$$\nExpanding the squared norm gives:\n$$\nL(h) = \\frac{1}{2} (r_k + J_k h)^T (r_k + J_k h) = \\frac{1}{2} (r_k^T r_k + 2h^T J_k^T r_k + h^T J_k^T J_k h)\n$$\n$$\nL(h) = F(p_k) + h^T (J_k^T r_k) + \\frac{1}{2} h^T (J_k^T J_k) h\n$$\nwhere $r_k = r(p_k)$ and $J_k = J(p_k)$.\n\n### 3. The Gauss-Newton and Levenberg-Marquardt Steps\n\nThe step $h$ is chosen to minimize this quadratic model $L(h)$. Setting the gradient of $L(h)$ with respect to $h$ to zero gives:\n$$\n\\nabla_h L(h) = J_k^T r_k + (J_k^T J_k) h = 0\n$$\nThis leads to the **Gauss-Newton normal equations**:\n$$\n(J_k^T J_k) h_{gn} = -J_k^T r_k\n$$\nThe step $h_{gn}$ is the Gauss-Newton step. This method works well when the matrix $J_k^T J_k$ is well-conditioned. However, if $J_k^T J_k$ is singular or ill-conditioned, the step can be excessively large and unproductive.\n\nThe **Levenberg-Marquardt algorithm** addresses this issue by introducing a damping parameter $\\lambda \\ge 0$. The step $h_{lm}$ is found by solving a modified, or damped, normal equation:\n$$\n(J_k^T J_k + \\lambda I) h_{lm} = -J_k^T r_k\n$$\nwhere $I$ is the identity matrix. The term $\\lambda I$ is a regularization term.\n- If $\\lambda$ is small, the step $h_{lm}$ approximates the Gauss-Newton step $h_{gn}$.\n- If $\\lambda$ is large, the term $\\lambda I$ dominates $J_k^T J_k$, and the equation approximates $\\lambda I h_{lm} \\approx -J_k^T r_k$, which means $h_{lm} \\approx -\\frac{1}{\\lambda} (J_k^T r_k)$. Since the gradient of the objective function is $\\nabla F(p_k) = J_k^T r_k$, the step becomes a small step in the direction of steepest descent.\n\nThus, the parameter $\\lambda$ adaptively blends the fast-converging Gauss-Newton method with the robust but slower gradient descent method.\n\n### 4. Algorithm Implementation Details\n\n#### Numerical Jacobian Approximation\nThe Jacobian matrix $J_k$ is computed at each iteration using **central finite differences**. For each parameter $p_j$ (the $j$-th component of the vector $p$), a small perturbation $\\delta_j$ is calculated:\n$$\n\\delta_j = \\sqrt{\\varepsilon} (1 + |p_j|)\n$$\nwhere $\\varepsilon$ is the machine precision for double-precision floating-point numbers. The $j$-th column of the Jacobian, which represents the sensitivity of the residuals to the parameter $p_j$, is then approximated as:\n$$\nJ_{\\cdot, j}(p_k) = \\frac{\\partial r(p_k)}{\\partial p_j} \\approx \\frac{r(p_k + \\delta_j e_j) - r(p_k - \\delta_j e_j)}{2\\delta_j} = -\\frac{f(x, p_k + \\delta_j e_j) - f(x, p_k - \\delta_j e_j)}{2\\delta_j}\n$$\nwhere $e_j$ is the $j$-th standard basis vector.\n\n#### Damping Parameter Adaptation\nThe success of a proposed step $h=h_{lm}$ is evaluated using a **gain ratio**, $\\rho$. This ratio compares the actual reduction in the objective function to the reduction predicted by the local quadratic model $L(h)$.\n- **Actual reduction**: $\\Delta F_{actual} = F(p_k) - F(p_k + h)$\n- **Predicted reduction**: $\\Delta F_{pred} = L(0) - L(h) = -h^T J_k^T r_k - \\frac{1}{2}h^T (J_k^T J_k) h$\n\nThe gain ratio is $\\rho = \\frac{\\Delta F_{actual}}{\\Delta F_{pred}}$. The adaptation strategy is as follows:\n1.  If $\\rho$ is positive and significant (e.g., $\\rho > 10^{-4}$), the step is \"productive\". It is accepted ($p_{k+1} = p_k + h$), and the damping $\\lambda$ is decreased to move closer to the faster Gauss-Newton method for the next iteration. A common update rule is $\\lambda \\leftarrow \\lambda \\cdot \\max(1/3, 1-(2\\rho-1)^3)$.\n2.  If $\\rho$ is small or negative, the step is \"unproductive\". It is rejected ($p_{k+1} = p_k$), and the damping $\\lambda$ is increased to move towards the more robust gradient descent direction. The algorithm then re-computes the step $h$ with the larger $\\lambda$ at the same iteration $k$. A common rule is $\\lambda \\leftarrow \\lambda \\cdot v$, where $v$ is a multiplicative factor, typically starting at $v=2$ and increasing on successive rejections.\n\nAn initial value for $\\lambda$ is chosen based on the scale of the problem, for instance, $\\lambda_0 = \\tau \\cdot \\max_{ii}((J_0^T J_0)_{ii})$ with a small constant $\\tau$ (e.g., $10^{-4}$).\n\n#### Termination Criteria\nThe iterative process is terminated when one of the following conditions is met, indicating that a satisfactory solution has been found:\n1.  **Small Gradient**: The magnitude of the gradient of the objective function is close to zero: $\\lVert J_k^T r_k \\rVert_{\\infty} < \\epsilon_g$.\n2.  **Small Parameter Update**: The relative change in the parameter vector is negligible: $\\lVert p_{k+1} - p_k \\rVert_2 < \\epsilon_p (\\lVert p_k \\rVert_2 + \\epsilon_p)$.\n3.  **Small Objective Function Change**: The relative change in the objective function value is negligible: $|F(p_{k+1}) - F(p_k)| < \\epsilon_f (F(p_k) + \\epsilon_f)$.\n4.  **Maximum Iterations**: A predefined maximum number of iterations, $k_{max}$, is reached to prevent an infinite loop.\n\nHere, $\\epsilon_g, \\epsilon_p, \\epsilon_f$ are small tolerance values (e.g., $10^{-8}$).\n\n### 5. Summary of the Levenberg-Marquardt Algorithm\nThe complete algorithm is as follows:\n\n1.  **Initialization**:\n    - Choose an initial parameter guess $p_0$.\n    - Set tolerances $\\epsilon_g, \\epsilon_p, \\epsilon_f$ and maximum iterations $k_{max}$.\n    - Compute initial residual $r_0 = y - f(x, p_0)$ and objective $F_0 = \\frac{1}{2}r_0^T r_0$.\n    - Compute initial Jacobian $J_0$ and gradient $g_0 = J_0^T r_0$.\n    - Initialize damping parameter $\\lambda_0$ and increase factor $v$.\n    - Set iteration counter $k = 0$.\n\n2.  **Main Loop**: While termination criteria are not met:\n    a. Solve the damped normal equations $(J_k^T J_k + \\lambda_k I) h = -g_k$ for the step $h$.\n    b. Evaluate the proposed new parameter vector $p_{new} = p_k + h$.\n    c. Compute the gain ratio $\\rho = \\frac{F(p_k) - F(p_{new})}{\\Delta F_{pred}}$.\n    d. **If $\\rho > \\epsilon_{\\rho}$**:\n        i. Accept the step: $p_{k+1} = p_{new}$.\n        ii. Update objective $F_{k+1}$, residual $r_{k+1}$, Jacobian $J_{k+1}$, and gradient $g_{k+1}$.\n        iii. Decrease damping: $\\lambda_{k+1} = \\lambda_k \\cdot \\max(1/3, 1-(2\\rho-1)^3)$ and reset $v=2$.\n        iv. Check termination criteria on step size and objective change.\n        v. Increment $k \\leftarrow k+1$.\n    e. **Else ($\\rho \\le \\epsilon_{\\rho}$)**:\n        i. Reject the step. Parameters are not updated.\n        ii. Increase damping: $\\lambda_k \\leftarrow \\lambda_k \\cdot v$, and then $v \\leftarrow 2v$.\n        iii. Repeat from step 2a with the new $\\lambda_k$.\n\n3.  **Termination**: Return the final parameter vector $p_k$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef levenberg_marquardt(model_func, p_init, x_data, y_data, max_iter=100, tol_g=1e-8, tol_p=1e-8, tol_f=1e-8):\n    \"\"\"\n    Implements the Levenberg-Marquardt algorithm for nonlinear least squares.\n\n    Args:\n        model_func: The model function f(x, p).\n        p_init: Initial guess for the parameters.\n        x_data: Independent variable data.\n        y_data: Dependent variable data (observations).\n        max_iter: Maximum number of iterations.\n        tol_g: Tolerance for the infinity norm of the gradient.\n        tol_p: Tolerance for the parameter update step.\n        tol_f: Tolerance for the change in the objective function.\n\n    Returns:\n        The fitted parameter vector.\n    \"\"\"\n    p_k = np.array(p_init, dtype=float)\n    m = len(p_k)\n    eps = np.finfo(np.float64).eps\n\n    # Initial evaluation\n    r_k = y_data - model_func(x_data, p_k)\n    F_k = 0.5 * np.dot(r_k, r_k)\n\n    # Jacobian calculation function\n    def calculate_jacobian(p):\n        J = np.zeros((len(x_data), m))\n        for j in range(m):\n            delta = np.sqrt(eps) * (1.0 + np.abs(p[j]))\n            p_plus = p.copy()\n            p_minus = p.copy()\n            p_plus[j] += delta\n            p_minus[j] -= delta\n            \n            # Central difference for jacobian of residual r = y - f\n            # dr/dp = -df/dp\n            # Using (f(p - d) - f(p + d)) / 2d to get -df/dp\n            f_plus = model_func(x_data, p_plus)\n            f_minus = model_func(x_data, p_minus)\n\n            J[:, j] = (f_minus - f_plus) / (2.0 * delta)\n        return J\n\n    J_k = calculate_jacobian(p_k)\n    g_k = np.dot(J_k.T, r_k)\n\n    if np.linalg.norm(g_k, np.inf) < tol_g:\n        return p_k\n\n    # Initial damping parameter\n    A = np.dot(J_k.T, J_k)\n    tau = 1e-4\n    mu = tau * np.max(np.diag(A))\n    nu = 2.0\n    \n    k = 0\n    while k < max_iter:\n        \n        while True:\n            H_lm = A + mu * np.identity(m)\n            try:\n                h = np.linalg.solve(H_lm, -g_k)\n            except np.linalg.LinAlgError:\n                mu *= nu\n                nu *= 2.0\n                continue\n\n            p_new = p_k + h\n            r_new = y_data - model_func(x_data, p_new)\n            F_new = 0.5 * np.dot(r_new, r_new)\n\n            # Predicted reduction\n            # pred_red = - (h^T * g_k + 0.5 * h^T * A * h)\n            pred_red = - (np.dot(h, g_k) + 0.5 * np.dot(h, np.dot(A, h)))\n            \n            # Gain ratio\n            actual_red = F_k - F_new\n            \n            if pred_red > 0: # Avoid division by zero or negative\n                rho = actual_red / pred_red\n            else:\n                rho = -1.0\n\n            if rho > 0:  # Step is accepted\n                # Check termination criteria on parameter and function change\n                p_norm = np.linalg.norm(p_k)\n                if np.linalg.norm(h) < tol_p * (p_norm + tol_p):\n                    return p_new\n                \n                F_norm = F_k\n                if abs(actual_red) < tol_f * (F_norm + tol_f):\n                    return p_new\n\n                p_k = p_new\n                F_k = F_new\n                r_k = r_new\n                \n                J_k = calculate_jacobian(p_k)\n                A = np.dot(J_k.T, J_k)\n                g_k = np.dot(J_k.T, r_k)\n\n                # Check termination on gradient\n                if np.linalg.norm(g_k, np.inf) < tol_g:\n                    return p_k\n\n                mu = mu * max(1/3.0, 1 - (2*rho - 1)**3)\n                nu = 2.0\n                break # Exit inner loop and start next iteration\n            else: # Step is rejected\n                mu *= nu\n                nu *= 2.0\n        \n        k += 1\n\n    return p_k\n\n\ndef solve():\n    \"\"\"\n    Defines and solves the three test cases specified in the problem.\n    \"\"\"\n    \n    # Test Case 1: Exponential Model\n    def model1(x, p):\n        return p[0] * np.exp(p[1] * x) + p[2]\n    \n    x1 = np.linspace(0.0, 3.0, 31)\n    y1 = 2.0 * np.exp(-0.7 * x1) + 0.5 + 0.01 * np.sin(5 * x1)\n    p_init1 = [1.0, -0.2, 0.0]\n    \n    # Test Case 2: Logistic Model\n    def model2(x, p):\n        return p[0] / (1.0 + np.exp(-p[1] * (x - p[2])))\n        \n    x2 = np.linspace(-1.5, 2.0, 15)\n    y2 = 1.5 / (1.0 + np.exp(-3.0 * (x2 - 0.5))) + 0.02 * np.cos(3 * x2)\n    p_init2 = [1.0, 1.0, 0.0]\n    \n    # Test Case 3: Sinusoidal Model\n    def model3(x, p):\n        omega = 2.5\n        return p[0] * np.cos(omega * x + p[1]) + p[2]\n        \n    x3 = np.linspace(0.0, 2.0, 21)\n    y3 = 0.8 * np.cos(2.5 * x3 + 0.4) - 0.05 + 0.01 * np.sin(7 * x3)\n    p_init3 = [0.5, 0.0, 0.0]\n\n    test_cases = [\n        (model1, p_init1, x1, y1),\n        (model2, p_init2, x2, y2),\n        (model3, p_init3, x3, y3),\n    ]\n\n    results = []\n    for model_func, p_init, x_data, y_data in test_cases:\n        p_fit = levenberg_marquardt(model_func, p_init, x_data, y_data)\n        rounded_p = [round(p, 6) for p in p_fit]\n        results.append(str(rounded_p).replace(\" \", \"\"))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3256843"}]}