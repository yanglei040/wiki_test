{"hands_on_practices": [{"introduction": "Many important integrals in fields like physics and statistics are defined over infinite domains, where standard numerical integration rules on finite intervals cannot be directly applied. This exercise introduces Gauss-Hermite quadrature, a powerful technique specifically designed to approximate integrals of the form $\\int_{-\\infty}^{\\infty} e^{-x^2} f(x) \\,dx$. By applying a product rule based on this method [@problem_id:3258802], you will gain hands-on experience with specialized quadrature techniques that exploit the structure of the integrand to achieve high accuracy on non-standard domains.", "problem": "Consider the two-dimensional integral of a smooth function against a Gaussian weight. Let $f(x,y)$ be a function for which the integral\n$$I = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} e^{-(x^2+y^2)} f(x,y)\\,dx\\,dy$$\nis finite. A well-tested numerical approach for such integrals uses product quadrature based on Hermite polynomials: one constructs a one-dimensional Gaussian quadrature for the weight $e^{-x^2}$ and applies it independently in each coordinate, forming a tensor-product rule for multiple integrals. This approach leverages the orthogonality of Hermite polynomials with respect to the Gaussian weight and the separability of the weight across dimensions.\n\nIn this problem, you will implement this approach to approximate\n$$I^\\star = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} e^{-(x^2+y^2)} \\cos(x+y)\\,dx\\,dy,$$\nwhere the cosine uses the radian measure for angles. The fundamental base for your design is:\n- The definition of the Riemann integral and Fubini's theorem for interchanging the order of integration when integrability conditions are met.\n- The orthogonality of the Hermite polynomial family with respect to the Gaussian weight $e^{-x^2}$ on $(-\\infty,\\infty)$, which underlies Gaussian quadrature rules that exactly integrate polynomials up to a certain degree when weighted by $e^{-x^2}$.\n- The construction of multivariate product quadrature by applying one-dimensional rules dimension-wise for separable weights.\n\nYour program must:\n- Use a product Gauss-Hermite quadrature rule to approximate $I^\\star$.\n- Treat $\\cos(\\cdot)$ in radians.\n- Produce results for the following test suite, which varies the number of quadrature nodes per dimension to assess convergence, boundary behavior, and anisotropic sampling:\n    1. $(n_x,n_y) = (1,1)$\n    2. $(n_x,n_y) = (2,3)$\n    3. $(n_x,n_y) = (4,4)$\n    4. $(n_x,n_y) = (8,16)$\n    5. $(n_x,n_y) = (32,32)$\n\nFor each test case, compute a single floating-point approximation of $I^\\star$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Express each approximation as a float rounded to ten decimal places. For example, an output with three results should look like:\n$$[r_1,r_2,r_3],$$\nwhere each $r_k$ is the ten-decimal-place rounded approximation for the $k$-th test case.\n\nNo physical units are involved; all angles are in radians. The program must be entirely self-contained, require no user input, and use only the specified libraries.", "solution": "The problem statement has been validated and is determined to be a well-posed and scientifically sound problem in the field of numerical analysis. It requests the numerical approximation of a specific two-dimensional integral using a standard, clearly defined method. All necessary data and parameters are provided.\n\nThe objective is to approximate the two-dimensional integral\n$$I^\\star = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} e^{-(x^2+y^2)} \\cos(x+y)\\,dx\\,dy$$\nThe integrand consists of a Gaussian weight function, $w(x,y) = e^{-(x^2+y^2)}$, and a function $f(x,y) = \\cos(x+y)$. The specified numerical method is a product Gauss-Hermite quadrature rule, which is particularly well-suited for integrals of this form due to the properties of Hermite polynomials and the separability of the weight function.\n\nThe foundational principle is the one-dimensional Gauss-Hermite quadrature rule. This rule approximates an integral over the real line with a Gaussian weight as a discrete weighted sum:\n$$ \\int_{-\\infty}^{\\infty} e^{-t^2} g(t)\\,dt \\approx \\sum_{k=1}^{n} w_k g(t_k) $$\nIn this formula, $n$ represents the number of quadrature nodes. The nodes $\\{t_k\\}_{k=1}^n$ are the roots of the $n$-th degree physicist's Hermite polynomial, $H_n(t)$. The associated weights $\\{w_k\\}_{k=1}^n$ are specifically calculated such that the approximation is exact if $g(t)$ is any polynomial of degree up to $2n-1$. This high degree of accuracy for polynomial integrands makes the method powerful for approximating integrals of smooth functions, which can be locally well-approximated by polynomials.\n\nFor the two-dimensional integral $I^\\star$, the separability of the weight function, $w(x,y) = e^{-x^2}e^{-y^2}$, allows for the construction of a product rule. By applying Fubini's theorem, we can express the double integral as an iterated integral:\n$$ I^\\star = \\int_{-\\infty}^{\\infty} e^{-y^2} \\left( \\int_{-\\infty}^{\\infty} e^{-x^2} \\cos(x+y)\\,dx \\right) dy $$\nThe numerical approximation is performed hierarchically. First, for a fixed value of $y$, the inner integral with respect to $x$ is approximated using an $n_x$-point Gauss-Hermite rule with nodes $\\{x_i\\}_{i=1}^{n_x}$ and weights $\\{w_i^{(x)}\\}_{i=1}^{n_x}$:\n$$ \\int_{-\\infty}^{\\infty} e^{-x^2} \\cos(x+y)\\,dx \\approx \\sum_{i=1}^{n_x} w_i^{(x)} \\cos(x_i+y) $$\nThis approximation is then substituted into the outer integral over $y$:\n$$ I^\\star \\approx \\int_{-\\infty}^{\\infty} e^{-y^2} \\left( \\sum_{i=1}^{n_x} w_i^{(x)} \\cos(x_i+y) \\right) dy $$\nThe finite nature of the sum permits the interchange of the integration and summation operators:\n$$ I^\\star \\approx \\sum_{i=1}^{n_x} w_i^{(x)} \\int_{-\\infty}^{\\infty} e^{-y^2} \\cos(x_i+y)\\,dy $$\nFinally, the remaining integral over $y$ is approximated for each fixed $x_i$ using an $n_y$-point Gauss-Hermite rule with nodes $\\{y_j\\}_{j=1}^{n_y}$ and weights $\\{w_j^{(y)}\\}_{j=1}^{n_y}$:\n$$ \\int_{-\\infty}^{\\infty} e^{-y^2} \\cos(x_i+y)\\,dy \\approx \\sum_{j=1}^{n_y} w_j^{(y)} \\cos(x_i+y_j) $$\nCombining these steps yields the final tensor-product quadrature formula for the approximation of $I^\\star$:\n$$ I^\\star \\approx \\sum_{i=1}^{n_x} \\sum_{j=1}^{n_y} w_i^{(x)} w_j^{(y)} \\cos(x_i+y_j) $$\nThis formula directs us to evaluate the function $f(x,y) = \\cos(x+y)$ on a two-dimensional grid of points $(x_i, y_j)$, and then compute a weighted sum where the weight for each point is the product of the corresponding one-dimensional quadrature weights, $w_i^{(x)} w_j^{(y)}$.\n\nThe implementation will utilize the `numpy.polynomial.hermite.hermgauss` function, which provides the necessary nodes and weights for a given number of points $n$. The double summation is computed for each test case $(n_x, n_y)$ as specified in the problem statement. The convergence of the approximation to the true value of the integral, which can be shown analytically to be $I^\\star = \\pi e^{-1/2} \\approx 1.9045137660$, is expected to be rapid as $n_x$ and $n_y$ increase, due to the smoothness of the function $\\cos(x+y)$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes approximations of a 2D integral using product Gauss-Hermite quadrature\n    for a series of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple represents (n_x, n_y), the number of quadrature nodes in\n    # the x and y dimensions, respectively.\n    test_cases = [\n        (1, 1),\n        (2, 3),\n        (4, 4),\n        (8, 16),\n        (32, 32),\n    ]\n\n    results = []\n    for nx, ny in test_cases:\n        # The integral to approximate is:\n        # I = integral from -inf to inf, integral from -inf to inf of\n        #     e^-(x^2+y^2) * cos(x+y) dx dy\n        # This is approximated by the sum:\n        # Sum_i Sum_j w_i * w_j * cos(x_i + y_j)\n        # where (x_i, w_i) and (y_j, w_j) are Gauss-Hermite nodes and weights.\n\n        # Fetch the 1D Gauss-Hermite quadrature nodes and weights.\n        # The function np.polynomial.hermite.hermgauss is designed for integrals\n        # with the weight function e^(-x^2).\n        x_nodes, x_weights = np.polynomial.hermite.hermgauss(nx)\n        y_nodes, y_weights = np.polynomial.hermite.hermgauss(ny)\n\n        # To implement the double summation efficiently, we use NumPy's broadcasting\n        # and outer products.\n\n        # 1. Create a matrix of product weights.\n        #    The element at (j, i) will be y_weights[j] * x_weights[i].\n        weights_matrix = np.outer(y_weights, x_weights)\n\n        # 2. Create a matrix of node sums for the function argument.\n        #    The element at (j, i) will be y_nodes[j] + x_nodes[i].\n        nodes_sum_matrix = np.add.outer(y_nodes, x_nodes)\n\n        # 3. Evaluate the function cos(x+y) on the grid of nodes.\n        #    The angles are in radians, as is standard for `np.cos`.\n        integrand_values = np.cos(nodes_sum_matrix)\n\n        # 4. Compute the final sum by element-wise multiplication and summation.\n        integral_approximation = np.sum(weights_matrix * integrand_values)\n\n        # Round the result to ten decimal places as specified.\n        rounded_result = round(integral_approximation, 10)\n        results.append(rounded_result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3258802"}, {"introduction": "As we move to higher dimensions, the grid-based quadrature methods become computationally infeasible due to the \"curse of dimensionality\"—the exponential growth of grid points. This practice introduces a fundamentally different approach: Monte Carlo integration. By using random sampling, this method's convergence rate is independent of the problem's dimension, making it a vital tool for high-dimensional problems in fields like financial modeling and computational physics. This exercise [@problem_id:3258918] will guide you through estimating the volume of a 4-dimensional object, providing a clear demonstration of the power and simplicity of probabilistic numerical methods.", "problem": "Consider the $4$-dimensional unit of analysis: the volume of a $4$-dimensional ball (hypersphere) of radius $R$. The volume can be expressed as a multiple integral of an indicator function over $\\mathbb{R}^4$, and estimated using Monte Carlo integration. Your task is to design and implement a complete program that numerically estimates this volume with Monte Carlo integration and compares it to the analytical result derived from a well-tested general formula.\n\nStart from the following fundamental bases:\n- The volume of a measurable region $\\Omega \\subset \\mathbb{R}^d$ equals the integral of its indicator function $I_{\\Omega}(\\mathbf{x})$, that is, $\\mathrm{Vol}(\\Omega) = \\int_{\\mathbb{R}^d} I_{\\Omega}(\\mathbf{x}) \\, d\\mathbf{x}$, where $I_{\\Omega}(\\mathbf{x}) = 1$ if $\\mathbf{x} \\in \\Omega$ and $I_{\\Omega}(\\mathbf{x}) = 0$ otherwise.\n- Monte Carlo integration estimates $\\int_{D} f(\\mathbf{x}) \\, d\\mathbf{x}$ by sampling $\\mathbf{X}_1,\\dots,\\mathbf{X}_N$ uniformly from a domain $D$ of finite measure $|D|$, and computing $\\widehat{I}_N = |D|\\cdot \\frac{1}{N}\\sum_{i=1}^{N} f(\\mathbf{X}_i)$.\n- The well-tested general formula for the volume of an $n$-dimensional ball of radius $R$ is $V_n(R) = \\dfrac{\\pi^{n/2} R^n}{\\Gamma\\!\\left(\\frac{n}{2}+1\\right)}$, where $\\Gamma$ is the Gamma function.\n\nTasks to accomplish within your program:\n1. Express the volume of the $4$-dimensional ball of radius $R$ as an integral of an indicator function over a bounding hypercube $D = [-R,R]^4$ and design a Monte Carlo estimator using $N$ independent and identically distributed samples drawn uniformly from $D$ to estimate this integral.\n2. Using the general $n$-ball formula and properties of the Gamma function, derive the analytical closed form for the exact volume in dimension $n=4$ as a function of $R$.\n3. For each test case below, compute:\n   - The Monte Carlo estimate $\\widehat{V}$ of the volume using your estimator.\n   - The exact analytical volume $V_{\\text{exact}}$.\n   - The error metric $E$ defined as follows:\n     - If $V_{\\text{exact}} > 0$, use the relative error $E = \\dfrac{|\\widehat{V} - V_{\\text{exact}}|}{V_{\\text{exact}}}$.\n     - If $V_{\\text{exact}} = 0$, use the absolute error $E = |\\widehat{V} - V_{\\text{exact}}|$.\n   - Round each error $E$ to $6$ decimal places.\n\nImplementation requirements:\n- The program must be a single, self-contained script that uses only the Python standard library and the Numerical Python (NumPy) library.\n- The program must not read any input; it must hard-code the test suite below and produce a single line of output.\n- Randomness must be reproducible by initializing a pseudorandom number generator with the provided seeds.\n\nTest suite:\n- Case $1$: $R = 1.0$, $N = 200000$, $\\text{seed} = 12345$.\n- Case $2$: $R = 0.0$, $N = 10000$, $\\text{seed} = 2023$.\n- Case $3$: $R = 1.0$, $N = 1$, $\\text{seed} = 7$.\n- Case $4$: $R = 1.5$, $N = 150000$, $\\text{seed} = 314159$.\n- Case $5$: $R = 2.0$, $N = 120000$, $\\text{seed} = 424242$.\n\nFinal output format:\n- Your program should produce a single line of output containing the errors for the test cases as a comma-separated list enclosed in square brackets, in the same order as the cases listed above, with each error rounded to $6$ decimal places, for example, $[e_1,e_2,e_3,e_4,e_5]$ where each $e_i$ is a decimal numeral.", "solution": "The problem requires the numerical estimation of the volume of a $4$-dimensional ball (a hypersphere) of radius $R$ using Monte Carlo integration. This estimate will be compared against the exact analytical volume to compute an error metric. The process involves three main steps: deriving the analytical formula for the volume, designing the Monte Carlo estimator, and implementing the computation for a given set of test cases.\n\n### 1. Analytical Volume of a 4-Dimensional Ball\n\nThe problem provides the general formula for the volume of an $n$-dimensional ball of radius $R$:\n$$V_n(R) = \\frac{\\pi^{n/2} R^n}{\\Gamma\\left(\\frac{n}{2}+1\\right)}$$\nwhere $\\Gamma(z)$ is the Gamma function.\n\nFor our specific case, the dimension is $n=4$. Substituting $n=4$ into the formula yields:\n$$V_4(R) = \\frac{\\pi^{4/2} R^4}{\\Gamma\\left(\\frac{4}{2}+1\\right)} = \\frac{\\pi^2 R^4}{\\Gamma(2+1)} = \\frac{\\pi^2 R^4}{\\Gamma(3)}$$\nThe Gamma function has the property that for any positive integer $k$, $\\Gamma(k) = (k-1)!$. Therefore, for $k=3$:\n$$\\Gamma(3) = (3-1)! = 2! = 2 \\cdot 1 = 2$$\nSubstituting this result back into the volume formula gives the exact analytical expression for the volume of a $4$-ball of radius $R$:\n$$V_{\\text{exact}} = V_4(R) = \\frac{\\pi^2 R^4}{2}$$\n\n### 2. Monte Carlo Estimator Design\n\nThe volume of a region $\\Omega \\subset \\mathbb{R}^4$ can be expressed as the integral of its indicator function $I_{\\Omega}(\\mathbf{x})$:\n$$V = \\int_{\\mathbb{R}^4} I_{\\Omega}(\\mathbf{x}) \\, d\\mathbf{x}$$\nwhere $I_{\\Omega}(\\mathbf{x}) = 1$ if $\\mathbf{x} \\in \\Omega$ and $I_{\\Omega}(\\mathbf{x}) = 0$ otherwise. For a $4$-ball of radius $R$ centered at the origin, the region is $\\Omega = \\{ \\mathbf{x} \\in \\mathbb{R}^4 : \\|\\mathbf{x}\\|_2 \\le R \\}$, or $\\Omega = \\{ (x_1, x_2, x_3, x_4) : x_1^2 + x_2^2 + x_3^2 + x_4^2 \\le R^2 \\}$.\n\nTo apply Monte Carlo integration, we choose a simple sampling domain $D$ that encloses $\\Omega$ and has a known volume $|D|$. A natural choice is the hypercube $D = [-R, R]^4$. The volume of this hypercube is:\n$$|D| = (R - (-R))^4 = (2R)^4 = 16R^4$$\nThe integral for the volume of $\\Omega$ can be rewritten over this domain $D$:\n$$V = \\int_{D} I_{\\Omega}(\\mathbf{x}) \\, d\\mathbf{x}$$\nsince $I_{\\Omega}(\\mathbf{x})=0$ for any $\\mathbf{x}$ outside of $\\Omega$ (and thus for any $\\mathbf{x}$ in $D \\setminus \\Omega$).\n\nThe Monte Carlo method estimates this integral by sampling $N$ points $\\mathbf{X}_1, \\dots, \\mathbf{X}_N$ independently and uniformly from $D$. The estimator for the integral is given by:\n$$\\widehat{V} = |D| \\cdot \\frac{1}{N} \\sum_{i=1}^N I_{\\Omega}(\\mathbf{X}_i)$$\nLet $N_{\\text{in}}$ be the number of sampled points that fall within or on the boundary of the hypersphere $\\Omega$. This count is precisely $N_{\\text{in}} = \\sum_{i=1}^N I_{\\Omega}(\\mathbf{X}_i)$. The estimator for the volume can then be written as:\n$$\\widehat{V} = |D| \\cdot \\frac{N_{\\text{in}}}{N} = 16R^4 \\cdot \\frac{N_{\\text{in}}}{N}$$\nThis is the ratio of the volume of the hypersphere to the volume of the hypercube, approximated by the ratio of \"hits\" to total samples, scaled by the volume of the hypercube.\n\nFor the special case where $R=0$, the hypersphere is a single point with $V_{\\text{exact}}=0$. The bounding box $D = [0,0]^4$ also has volume $|D|=0$, and the estimator correctly yields $\\widehat{V}=0$.\n\n### 3. Computational Procedure and Error Metric\n\nFor each test case defined by parameters $(R, N, \\text{seed})$, the following procedure is executed:\n1.  **Initialize RNG**: The pseudorandom number generator is seeded with the given `seed` to ensure reproducibility.\n2.  **Calculate Exact Volume**: $V_{\\text{exact}}$ is computed using the formula $V_4(R) = \\frac{1}{2}\\pi^2 R^4$.\n3.  **Generate Samples**: $N$ random vectors $\\mathbf{X}_i$, each with $4$ components, are generated. Each component is drawn from a uniform distribution over the interval $[-R, R]$.\n4.  **Count \"Hits\"**: For each sample $\\mathbf{X}_i = (x_{i1}, x_{i2}, x_{i3}, x_{i4})$, we check if it lies within the $4$-ball by testing the condition $x_{i1}^2 + x_{i2}^2 + x_{i3}^2 + x_{i4}^2 \\le R^2$. The number of samples satisfying this condition, $N_{\\text{in}}$, is counted.\n5.  **Compute Monte Carlo Estimate**: The estimated volume $\\widehat{V}$ is calculated using the formula $\\widehat{V} = 16R^4 \\cdot (N_{\\text{in}} / N)$. If $R=0$, this calculation is trivial as $\\widehat{V}=0$.\n6.  **Calculate Error**: The error metric $E$ is computed based on the value of $V_{\\text{exact}}$:\n    -   If $V_{\\text{exact}} > 0$: The relative error is calculated as $E = \\dfrac{|\\widehat{V} - V_{\\text{exact}}|}{V_{\\text{exact}}}$.\n    -   If $V_{\\text{exact}} = 0$: The absolute error is calculated as $E = |\\widehat{V} - V_{\\text{exact}}|$. This case occurs when $R=0$, for which both $\\widehat{V}$ and $V_{\\text{exact}}$ are $0$, resulting in an error $E=0$.\n7.  **Store Result**: The calculated error $E$ is rounded to $6$ decimal places.\n\nThis procedure is repeated for all test cases, and the resulting list of rounded errors is formatted as the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of estimating the volume of a 4D ball using Monte Carlo integration\n    and comparing it to the analytical result for a suite of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (R, N, seed)\n        (1.0, 200000, 12345),\n        (0.0, 10000, 2023),\n        (1.0, 1, 7),\n        (1.5, 150000, 314159),\n        (2.0, 120000, 424242),\n    ]\n\n    results = []\n    for R, N, seed in test_cases:\n        # Task 2: Calculate the exact analytical volume V_exact.\n        # The volume of an n-ball is V_n(R) = (pi^(n/2) * R^n) / Gamma(n/2 + 1).\n        # For n=4, this simplifies to V_4(R) = (pi^2 * R^4) / Gamma(3) = (pi^2 * R^4) / 2.\n        v_exact = (np.pi**2 * R**4) / 2.0\n\n        # Task 1: Design and run the Monte Carlo estimator.\n        \n        # Initialize the pseudorandom number generator for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        v_hat = 0.0\n        # The Monte Carlo simulation is only meaningful for R > 0.\n        # If R = 0, the volume of the ball and bounding box are both 0.\n        if R > 0.0:\n            # Generate N random points in a 4-dimensional space.\n            # The points are sampled from a hypercube D = [-R, R]^4.\n            # rng.uniform generates values in [low, high). Here, [-R, R).\n            # The shape (N, 4) creates N points, each with 4 coordinates.\n            points = rng.uniform(low=-R, high=R, size=(N, 4))\n\n            # Calculate the squared Euclidean distance from the origin for each point.\n            # This is more efficient than calculating the Euclidean distance as it avoids sqrt.\n            # np.sum(points**2, axis=1) calculates x1^2+x2^2+x3^2+x4^2 for each point.\n            squared_distances = np.sum(points**2, axis=1)\n\n            # Count the number of points that fall inside or on the surface of the 4-ball.\n            # The condition is ||x||^2 = R^2.\n            n_in = np.sum(squared_distances = R**2)\n\n            # The volume of the sampling hypercube D = [-R, R]^4 is (2R)^4.\n            volume_of_domain = (2.0 * R)**4\n\n            # The Monte Carlo estimate is the volume of the sampling domain\n            # times the ratio of points inside the hypersphere to the total number of points.\n            v_hat = volume_of_domain * (n_in / N)\n        \n        # Task 3: Compute the error metric E.\n        error = 0.0\n        if v_exact > 0:\n            # Relative error\n            error = np.abs(v_hat - v_exact) / v_exact\n        else:\n            # Absolute error (for the R=0 case)\n            error = np.abs(v_hat - v_exact)\n        \n        # Round the error to 6 decimal places.\n        rounded_error = round(error, 6)\n        results.append(str(rounded_error))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3258918"}, {"introduction": "While Monte Carlo methods are essential for very high dimensions, their convergence can be slow. For integrals in moderately high dimensions where the function exhibits different behavior along different axes (anisotropy), we can design more sophisticated deterministic methods. This advanced practice explores anisotropic sparse grids, a state-of-the-art technique that \"tames\" the curse of dimensionality by intelligently allocating computational effort. By building a sparse grid that adapts to the function's structure [@problem_id:3258801], you will see firsthand how tailoring the numerical method to the problem can lead to dramatic gains in efficiency compared to both standard grid methods and brute-force sampling.", "problem": "Consider numerical approximation of multiple integrals in three dimensions over the cube $[-1,1]^3$ using nested sparse grids. Let the integrand be the anisotropic function $f(\\mathbf{x})=\\exp(-x_1^2-100 x_2^2- x_3^2)$, where $\\mathbf{x}=(x_1,x_2,x_3)$. The goal is to design and implement an anisotropic sparse-grid quadrature based on the Smolyak construction with nested one-dimensional rules, investigate how reordering dimension weights to match the anisotropy affects the number of unique grid nodes, and quantify the savings in nodes.\n\nStart from the following foundational base: the linearity of integration, tensor-product quadrature for multiple integrals, interpolatory one-dimensional quadrature on Chebyshev extrema nodes (Clenshaw–Curtis nodes), and the definition of the Smolyak construction as a linear combination of tensor-product difference rules built from nested one-dimensional quadratures. Use these to derive an algorithm that:\n- Constructs nested one-dimensional quadrature rules on $[-1,1]$ at increasing levels $\\ell\\in\\mathbb{N}$ with $\\ell\\ge 1$, where level $\\ell$ has $N_\\ell$ nodes, and rules are nested so that nodes at level $\\ell-1$ are a subset of those at level $\\ell$.\n- Forms one-dimensional difference rules as the difference between successive nested quadrature rules at levels $\\ell$ and $\\ell-1$, exploiting telescoping to assemble a sparse-grid quadrature that is a sum over admissible multi-indices of tensor products of one-dimensional differences.\n- Selects admissible multi-indices $\\boldsymbol{\\ell}=(\\ell_1,\\ell_2,\\ell_3)$ using a weighted level-budget constraint that encodes anisotropy via nonnegative weights $\\mathbf{a}=(a_1,a_2,a_3)$:\n$$\n\\sum_{i=1}^3 a_i(\\ell_i-1)\\le L,\n$$\nwhere $L$ is a nonnegative budget parameter and $\\ell_i\\ge 1$ are integers. To avoid unbounded growth in one-dimensional rule sizes, impose a cap on levels: $\\ell_i\\le \\ell_{\\max}$ for all $i$. This cap must be applied consistently across dimensions.\n\nImplement the one-dimensional interpolatory quadrature weights by enforcing exactness for polynomials up to degree $N_\\ell-1$ on the chosen nodes, using the moment conditions $\\int_{-1}^{1} x^j\\,dx$ for $j=0,1,\\dots,N_\\ell-1$. Use the nested Clenshaw–Curtis nodes (Chebyshev extrema) for all levels. Assemble the three-dimensional sparse grid by aggregating Cartesian products of one-dimensional difference rules over the admissible multi-index set, summing repeated nodes and weights to form a single set of unique nodes with their aggregate weights. Approximate the integral $\\int_{[-1,1]^3} f(\\mathbf{x})\\,d\\mathbf{x}$ by summing the product of aggregate weights and function values at the unique nodes.\n\nInvestigate variable reordering by permuting the association between dimension weights $\\mathbf{a}$ and coordinate indices $(x_1,x_2,x_3)$. For a given $\\mathbf{a}$ and budget $L$, define two configurations:\n- A misaligned configuration where the smallest weight is assigned to a dimension with low anisotropy, and\n- An aligned configuration where the smallest weight is assigned to the most anisotropic dimension $x_2$.\n\nFor each configuration, compute:\n- The approximate integral value of $f$ over $[-1,1]^3$ in unitless terms,\n- The number of unique sparse-grid nodes,\n- The integer savings in nodes from reordering, defined as the difference between the misaligned node count and the aligned node count.\n\nDesign a test suite that exercises the boundary condition, a happy path, and an edge case in anisotropy:\n- Test case $1$: $L=0$, $\\ell_{\\max}=6$, misaligned weights $\\mathbf{a}=[0.5,1.0,1.0]$, aligned weights $\\mathbf{a}=[1.0,0.5,1.0]$.\n- Test case $2$: $L=2$, $\\ell_{\\max}=6$, misaligned weights $\\mathbf{a}=[0.5,1.0,1.0]$, aligned weights $\\mathbf{a}=[1.0,0.5,1.0]$.\n- Test case $3$: $L=4$, $\\ell_{\\max}=6$, misaligned weights $\\mathbf{a}=[0.5,1.0,1.0]$, aligned weights $\\mathbf{a}=[1.0,0.5,1.0]$.\n- Test case $4$: $L=4$, $\\ell_{\\max}=6$, misaligned weights $\\mathbf{a}=[0.3,1.0,1.0]$, aligned weights $\\mathbf{a}=[1.0,0.3,1.0]$.\n\nYour program must, for each test case, produce a list containing $[\\text{approx\\_misaligned},\\ \\text{nodes\\_misaligned},\\ \\text{approx\\_aligned},\\ \\text{nodes\\_aligned},\\ \\text{savings}]$, where $\\text{approx\\_misaligned}$ and $\\text{approx\\_aligned}$ are floats, and the remaining entries are integers. The final output must be a single line containing the results of all test cases as a comma-separated list enclosed in square brackets, for example $[[\\dots],[\\dots],[\\dots],[\\dots]]$ with no additional text. Angles do not appear, and there are no physical units; all quantities are unitless.", "solution": "The problem is a valid exercise in numerical analysis, specifically the application of anisotropic sparse-grid quadrature to approximate a three-dimensional integral. The problem statement is scientifically grounded, well-posed, and objective. It provides all necessary components to construct a unique, verifiable solution.\n\nThe task is to approximate the integral of an anisotropic function, $f(\\mathbf{x})=\\exp(-x_1^2-100 x_2^2- x_3^2)$, over the domain $[-1,1]^3$. The anisotropy arises from the large coefficient, 100, associated with the $x_2$ variable, which causes the function to vary much more rapidly in that direction. To efficiently handle this, we employ an anisotropic sparse-grid quadrature method based on the Smolyak construction. This method intelligently allocates more grid points in directions of high variation, controlled by a set of weights $\\mathbf{a}=(a_1, a_2, a_3)$.\n\n**1. One-Dimensional Quadrature Rules**\n\nThe foundation of the sparse grid is a sequence of nested one-dimensional quadrature rules. For each level $\\ell \\in \\mathbb{N}$ with $\\ell \\ge 1$, we define a quadrature operator $\\mathcal{Q}^{(\\ell)}$ that approximates the one-dimensional integral $\\int_{-1}^{1} g(x)\\,dx$.\n$$\n\\mathcal{Q}^{(\\ell)}[g] = \\sum_{j=1}^{N_\\ell} w_j^{(\\ell)} g(x_j^{(\\ell)})\n$$\nThe nodes $\\{x_j^{(\\ell)}\\}$ are chosen as the nested Clenshaw-Curtis points (Chebyshev extrema on $[-1,1]$), which ensures that the node set at level $\\ell-1$ is a subset of the node set at level $\\ell$. The number of points $N_\\ell$ and their locations are given by:\n- For level $\\ell=1$: $N_1 = 1$ node at $x_1^{(1)} = 0$.\n- For level $\\ell > 1$: $N_\\ell = 2^{\\ell-1}+1$ nodes at $x_j^{(\\ell)} = \\cos\\left(\\frac{(j-1)\\pi}{N_\\ell-1}\\right)$ for $j=1, \\dots, N_\\ell$.\n\nThe weights $\\{w_j^{(\\ell)}\\}$ are determined by enforcing that the rule is exact for all polynomials of degree up to $N_\\ell-1$. This leads to a system of $N_\\ell$ linear equations, known as the moment conditions:\n$$\n\\sum_{j=1}^{N_\\ell} w_j^{(\\ell)} (x_j^{(\\ell)})^k = \\int_{-1}^{1} x^k \\,dx \\quad \\text{for } k = 0, 1, \\dots, N_\\ell-1\n$$\nThe right-hand side evaluates to $2/(k+1)$ for even $k$ and $0$ for odd $k$. This system can be expressed in matrix form as $V\\mathbf{w}^{(\\ell)} = \\mathbf{m}$, where $V$ is the Vandermonde matrix with entries $V_{kj} = (x_j^{(\\ell)})^{k-1}$ and $\\mathbf{m}$ is the vector of moments. Solving this system yields the weights for level $\\ell$.\n\n**2. Anisotropic Smolyak Construction**\n\nThe sparse-grid approximation is constructed as a linear combination of tensor products of the one-dimensional quadrature rules. The selection of which tensor-product grids to include is governed by an anisotropic admissibility condition. A multi-index $\\boldsymbol{\\ell}=(\\ell_1, \\ell_2, \\ell_3)$, where $\\ell_i$ is the level in dimension $i$, is considered admissible if it satisfies a budget constraint:\n$$\n\\sum_{i=1}^3 a_i(\\ell_i-1) \\le L\n$$\nHere, $L \\ge 0$ is the total budget, and $\\mathbf{a}=(a_1, a_2, a_3)$ is a vector of positive weights that control the anisotropy. A smaller weight $a_i$ allows for a higher level $\\ell_i$ in dimension $i$, thus refining the grid more in that direction. Additionally, a maximum level $\\ell_{\\max}$ is imposed, so $1 \\le \\ell_i \\le \\ell_{\\max}$ for all $i$. Let $I(L, \\mathbf{a}, \\ell_{\\max})$ denote the set of all such admissible multi-indices.\n\nUsing the combination technique, the anisotropic sparse-grid quadrature operator $\\mathcal{A}_{L, \\mathbf{a}}$ is given by:\n$$\n\\mathcal{A}_{L, \\mathbf{a}}[f] = \\sum_{\\boldsymbol{\\ell} \\in I(L, \\mathbf{a}, \\ell_{\\max})} c_{\\boldsymbol{\\ell}} \\left(\\mathcal{Q}^{(\\ell_1)} \\otimes \\mathcal{Q}^{(\\ell_2)} \\otimes \\mathcal{Q}^{(\\ell_3)}\\right)[f]\n$$\nThe symbol $\\otimes$ denotes the tensor product of operators. The action of a tensor-product quadrature on a function $f(x_1, x_2, x_3)$ is:\n$$\n\\left(\\mathcal{Q}^{(\\ell_1)} \\otimes \\mathcal{Q}^{(\\ell_2)} \\otimes \\mathcal{Q}^{(\\ell_3)}\\right)[f] = \\sum_{j_1=1}^{N_{\\ell_1}} \\sum_{j_2=1}^{N_{\\ell_2}} \\sum_{j_3=1}^{N_{\\ell_3}} w_{j_1}^{(\\ell_1)} w_{j_2}^{(\\ell_2)} w_{j_3}^{(\\ell_3)} f\\left(x_{j_1}^{(\\ell_1)}, x_{j_2}^{(\\ell_2)}, x_{j_3}^{(\\ell_3)}\\right)\n$$\nThe combination coefficients $c_{\\boldsymbol{\\ell}}$ are determined by the principle of inclusion-exclusion over the lattice of multi-indices:\n$$\nc_{\\boldsymbol{\\ell}} = \\sum_{\\mathbf{j} \\in \\{0, 1\\}^3 \\text{ s.t. } \\boldsymbol{\\ell}+\\mathbf{j} \\in I(L, \\mathbf{a}, \\ell_{\\max})} (-1)^{|\\mathbf{j}|_1}\n$$\nwhere $|\\mathbf{j}|_1 = j_1+j_2+j_3$. This formula ensures that each tensor-product grid contributes with the correct multiplicity to cancel out over-counted lower-order terms.\n\n**3. Algorithmic Implementation and Anisotropy**\n\nThe overall algorithm proceeds as follows:\n1.  **Generate Admissible Indices**: For a given budget $L$, weights $\\mathbf{a}$, and cap $\\ell_{\\max}$, generate the set $I(L, \\mathbf{a}, \\ell_{\\max})$ of all multi-indices $\\boldsymbol{\\ell}=(\\ell_1, \\ell_2, \\ell_3)$ satisfying the constraints.\n2.  **Assemble Sparse Grid**: Initialize an empty dictionary to store the final unique grid points and their aggregated weights. Iterate through each $\\boldsymbol{\\ell}$ in the admissible set.\n    a. Calculate the combination coefficient $c_{\\boldsymbol{\\ell}}$.\n    b. If $c_{\\boldsymbol{\\ell}} \\neq 0$, form the tensor-product grid corresponding to $\\boldsymbol{\\ell}$. For each point $(x_{j_1}, x_{j_2}, x_{j_3})$ in this grid, calculate its composite weight $c_{\\boldsymbol{\\ell}} \\cdot w_{j_1}^{(\\ell_1)} w_{j_2}^{(\\ell_2)} w_{j_3}^{(\\ell_3)}$.\n    c. Add this composite weight to the corresponding node in the global dictionary, accumulating weights for any repeated nodes.\n3.  **Approximate Integral**: Once the final set of unique nodes and aggregated weights is assembled, the integral is approximated by summing the product of weights and function values at these nodes.\n\nThe problem investigates the effect of aligning the anisotropy of the grid with that of the function. The function $f(\\mathbf{x})=\\exp(-x_1^2-100 x_2^2- x_3^2)$ is most sensitive (most anisotropic) in the $x_2$ direction. To achieve high accuracy efficiently, we must use a finer grid (higher levels $\\ell_2$) in this direction. This is accomplished by assigning the smallest weight $a_i$ to the most anisotropic dimension.\n- **Aligned Configuration**: The small weight is assigned to $a_2$, e.g., $\\mathbf{a}=[1.0, 0.5, 1.0]$. This prioritizes refinement in the $x_2$ direction.\n- **Misaligned Configuration**: The small weight is assigned to a less sensitive dimension, e.g., $\\mathbf{a}=[0.5, 1.0, 1.0]$ assigns it to $x_1$.\n\nWe expect the aligned configuration to produce a more accurate approximation with fewer total grid points for a given budget $L$, as the computational effort is focused where it is most needed. The \"savings\" in the number of nodes quantifies this efficiency gain.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem as specified.\n    It runs through the test cases, builds the sparse grids,\n    computes the approximations and node counts, and prints the results.\n    \"\"\"\n\n    # Define the integrand function\n    def integrand(x):\n        return np.exp(-x[0]**2 - 100*x[1]**2 - x[2]**2)\n\n    # Memoization cache for 1D quadrature rules\n    rule_cache = {}\n\n    def get_1d_rule(level):\n        \"\"\"\n        Computes or retrieves from cache the nodes and weights for a 1D\n        Clenshaw-Curtis quadrature rule of a given level.\n        \"\"\"\n        if level in rule_cache:\n            return rule_cache[level]\n\n        if level == 1:\n            nodes = np.array([0.0])\n            weights = np.array([2.0])\n            rule_cache[level] = (nodes, weights)\n            return nodes, weights\n\n        n_points = 2**(level - 1) + 1\n        # Nodes are Chebyshev extrema, ordered from 1 to -1\n        nodes = np.cos(np.arange(n_points) * np.pi / (n_points - 1))\n\n        # Solve Vandermonde system for weights\n        V = np.vander(nodes, n_points, increasing=True).T\n        m = np.zeros(n_points)\n        for k in range(n_points):\n            if k % 2 == 0:\n                m[k] = 2.0 / (k + 1.0)\n        \n        weights = np.linalg.solve(V, m)\n        \n        rule_cache[level] = (nodes, weights)\n        return nodes, weights\n\n    def find_admissible_indices(L, a, l_max):\n        \"\"\"\n        Generates the set of admissible multi-indices.\n        \"\"\"\n        indices = set()\n        for l1 in range(1, l_max + 1):\n            for l2 in range(1, l_max + 1):\n                for l3 in range(1, l_max + 1):\n                    if a[0] * (l1 - 1) + a[1] * (l2 - 1) + a[2] * (l3 - 1) = L:\n                        indices.add((l1, l2, l3))\n        return indices\n\n    def build_sparse_grid(L, a, l_max):\n        \"\"\"\n        Constructs the sparse grid and computes the integral approximation.\n        \"\"\"\n        admissible_indices = find_admissible_indices(L, a, l_max)\n        sparse_grid_points = {}\n\n        # Loop over admissible multi-indices to build the grid\n        for l_vec in admissible_indices:\n            # Calculate combination coefficient\n            c_l = 0\n            for i in range(8): # Iterate through j in {0,1}^3\n                j_vec = ((i >> 2)  1, (i >> 1)  1, i  1)\n                neighbor_l_vec = (l_vec[0] + j_vec[0], l_vec[1] + j_vec[1], l_vec[2] + j_vec[2])\n                if neighbor_l_vec in admissible_indices:\n                    c_l += (-1)**sum(j_vec)\n            \n            if abs(c_l)  1e-15:\n                continue\n\n            # Get 1D rules for the current multi-index\n            nodes1, weights1 = get_1d_rule(l_vec[0])\n            nodes2, weights2 = get_1d_rule(l_vec[1])\n            nodes3, weights3 = get_1d_rule(l_vec[2])\n\n            # Add tensor product points to the sparse grid\n            for i1, n1 in enumerate(nodes1):\n                for i2, n2 in enumerate(nodes2):\n                    for i3, n3 in enumerate(nodes3):\n                        node = (n1, n2, n3)\n                        # Round nodes to handle floating point inaccuracies in dict keys\n                        key = (round(node[0], 15), round(node[1], 15), round(node[2], 15))\n                        weight = c_l * weights1[i1] * weights2[i2] * weights3[i3]\n                        \n                        sparse_grid_points[key] = sparse_grid_points.get(key, 0.0) + weight\n\n        # Approximate integral by summing over the final grid\n        integral_approx = 0.0\n        # Filter out points with effectively zero weight before counting\n        final_nodes = {k: v for k, v in sparse_grid_points.items() if abs(v) > 1e-15}\n        \n        for node, weight in final_nodes.items():\n            integral_approx += weight * integrand(node)\n                \n        num_nodes = len(final_nodes)\n        \n        return integral_approx, num_nodes\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0, 6, [0.5, 1.0, 1.0], [1.0, 0.5, 1.0]),\n        (2, 6, [0.5, 1.0, 1.0], [1.0, 0.5, 1.0]),\n        (4, 6, [0.5, 1.0, 1.0], [1.0, 0.5, 1.0]),\n        (4, 6, [0.3, 1.0, 1.0], [1.0, 0.3, 1.0]),\n    ]\n    \n    results = []\n    for case in test_cases:\n        L_val, l_max_val, a_misaligned, a_aligned = case\n        \n        # Misaligned configuration\n        approx_misaligned, nodes_misaligned = build_sparse_grid(L_val, a_misaligned, l_max_val)\n        \n        # Aligned configuration\n        approx_aligned, nodes_aligned = build_sparse_grid(L_val, a_aligned, l_max_val)\n        \n        # Savings\n        savings = nodes_misaligned - nodes_aligned\n        \n        results.append([approx_misaligned, nodes_misaligned, approx_aligned, nodes_aligned, savings])\n\n    # Final print statement in the exact required format.\n    # Convert list of lists to the specific string format\n    result_str = ','.join(str(item) for item in results)\n    print(f\"[{result_str}]\")\n\nsolve()\n\n```", "id": "3258801"}]}