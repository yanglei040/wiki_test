{"hands_on_practices": [{"introduction": "This first practice provides a complete walkthrough of analyzing a fixed-point problem from theory to implementation. We will explore the classic equation $x = \\cos x$, a simple-looking but non-trivial problem that cannot be solved algebraically. By applying the Contraction Mapping Theorem, you will not only prove that a unique solution exists but also understand the nature of the convergence, including the interesting oscillatory behavior of the sequence [@problem_id:3231288].", "problem": "Consider the fixed-point equation $x=\\cos x$ with the fixed-point iteration $x_{k+1}=\\cos x_k$ defined on the closed interval $[0,1]$. Use the following fundamental bases: the definition of a fixed point $x^\\star$ satisfying $x^\\star=g(x^\\star)$ for a function $g$, the Mean Value Theorem (MVT), and the Contraction Mapping Theorem (CMT), also known as the Banach Fixed-Point Theorem. The iteration uses the angle unit in radians. Your program must implement the iteration and verify convergence and monotonicity properties for a specified test suite.\n\nTasks:\n- Starting from the fundamental definitions, justify why the function $g(x)=\\cos x$ maps $[0,1]$ into itself. Use the derivative $g'(x)=-\\sin x$ and the bound $\\sup_{x\\in[0,1]}|\\sin x|=\\sin(1)$ with $\\sin(1)1$ to argue that $g$ is a contraction on $[0,1]$ and therefore admits a unique fixed point $x^\\star\\in[0,1]$ to which the sequence $\\{x_k\\}$ converges for any $x_0\\in[0,1]$.\n- Analyze the monotone behavior of the iterates. Because $g$ is strictly decreasing on $[0,1]$, the full sequence $\\{x_k\\}$ is generally not monotone. However, demonstrate that the even subsequence $\\{x_{2k}\\}$ and the odd subsequence $\\{x_{2k+1}\\}$ are each monotone and converge to the same limit $x^\\star$.\n- Implement a program that, for each initial value $x_0$ in the provided test suite, performs fixed-point iteration $x_{k+1}=\\cos x_k$ until either the successive-iterate difference satisfies $|x_{k+1}-x_k|10^{-12}$ or a maximum of $1000$ iterations is reached. Store the full sequence of iterates for each test case. The angle unit must be radians.\n- For each test case, compute and report:\n  1. The initial value $x_0$.\n  2. The final approximation of the fixed point after stopping, denoted by $x_{\\text{approx}}$.\n  3. The number of iterations $N$ performed.\n  4. A boolean indicating whether the full sequence $\\{x_k\\}_{k=0}^N$ is monotone (nondecreasing or nonincreasing).\n  5. A boolean indicating whether the even-indexed subsequence $\\{x_{2k}\\}$ is monotone (nondecreasing or nonincreasing).\n  6. A boolean indicating whether the odd-indexed subsequence $\\{x_{2k+1}\\}$ is monotone (nondecreasing or nonincreasing).\n- Use the following test suite of initial values (all in radians): $x_0\\in\\{0,\\,\\tfrac{1}{2},\\,1,\\,0.7390851332151607\\}$. The last value is a high-precision approximation to the unique fixed point $x^\\star$.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list in the exact form $[x_0,x_{\\text{approx}},N,\\text{is\\_monotone\\_full},\\text{is\\_monotone\\_even},\\text{is\\_monotone\\_odd}]$. For example, the overall output will look like $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot]]$ with four inner lists corresponding to the four test cases.\n\nAll numerical values mentioned above must be treated as scalars in your program. The program must not read any inputs and must print the single required line as its only output.", "solution": "We first recall the fundamental definitions and facts. A fixed point of a function $g$ is a value $x^\\star$ satisfying $x^\\star=g(x^\\star)$. A numerical fixed-point iteration constructs a sequence $\\{x_k\\}$ by $x_{k+1}=g(x_k)$ and seeks a limit $x^\\star$ such that $x_k\\to x^\\star$ and $x^\\star=g(x^\\star)$. The Contraction Mapping Theorem (CMT) states that if $g$ maps a complete metric space into itself and satisfies a Lipschitz bound $|g(x)-g(y)|\\le L|x-y|$ for all $x,y$ in the domain with a constant $L1$, then $g$ has a unique fixed point $x^\\star$ and for any initialization $x_0$ in the domain the iteration $x_{k+1}=g(x_k)$ converges to $x^\\star$.\n\nWe analyze $g(x)=\\cos x$ on the closed interval $[0,1]$. First, we show that $g$ maps $[0,1]$ into itself. For $x\\in[0,1]$, we have $\\cos x\\in[\\cos 1,\\,\\cos 0]=[\\cos 1,\\,1]$. Since $\\cos 10$, it follows that $g([0,1])\\subset[0,1]$. Next, we compute the derivative $g'(x)=-\\sin x$. On $[0,1]$, the function $\\sin x$ is nonnegative and bounded above by $\\sin 1$, so\n$$\n\\sup_{x\\in[0,1]}|g'(x)|=\\sup_{x\\in[0,1]}|\\sin x|=\\sin(1).\n$$\nBecause $\\sin(1)1$, we can use the Mean Value Theorem (MVT) to derive a Lipschitz bound: for any $x,y\\in[0,1]$,\n$$\n|g(x)-g(y)|=|\\cos x-\\cos y|=|g'(\\xi)|\\,|x-y|\\le \\sin(1)\\,|x-y|,\n$$\nfor some $\\xi$ between $x$ and $y$. Hence $g$ is a contraction on $[0,1]$ with contraction constant $L=\\sin(1)1$. By the Contraction Mapping Theorem, there exists a unique fixed point $x^\\star\\in[0,1]$, and for any $x_0\\in[0,1]$, the sequence defined by $x_{k+1}=\\cos x_k$ converges to $x^\\star$.\n\nWe next analyze monotone behavior. Note that $g$ is strictly decreasing on $[0,1]$ because $g'(x)=-\\sin x\\le 0$ and for $x\\in(0,1]$ we have $\\sin x0$. Let $x^\\star$ be the unique fixed point. Consider the error $e_k=x_k-x^\\star$. Using the MVT, there exists $\\xi_k$ between $x_k$ and $x^\\star$ such that\n$$\ne_{k+1}=x_{k+1}-x^\\star=g(x_k)-g(x^\\star)=g'(\\xi_k)\\,(x_k-x^\\star)=-\\sin(\\xi_k)\\,e_k.\n$$\nOn $[0,1]$, $\\sin(\\xi_k)\\in[0,\\sin(1)]$ and the sign of $e_{k+1}$ is the negative of the sign of $e_k$ whenever $\\sin(\\xi_k)0$. Therefore, unless $e_k=0$, the error alternates in sign, so the full sequence $\\{x_k\\}$ generally oscillates and is not monotone. However, we can infer monotone behavior of subsequences. Because $g$ is decreasing, we have for any $x\\in[0,1]$, $xx^\\star$ implies $g(x)g(x^\\star)=x^\\star$, and $xx^\\star$ implies $g(x)x^\\star$. Starting with any $x_0\\in[0,1]$, if $x_0x^\\star$ then $x_1 x^\\star$, and then $x_2=g(x_1)g(x^\\star)=x^\\star$, thus $x_2x^\\star$. The even subsequence $\\{x_{2k}\\}$ remains below $x^\\star$ and, using the contraction property,\n$$\n|x_{2k}-x^\\star|\\le L^2 |x_{2k-2}-x^\\star|,\n$$\nso $\\{x_{2k}\\}$ is monotone increasing towards $x^\\star$ (its terms get closer to $x^\\star$ from below). Symmetrically, the odd subsequence $\\{x_{2k+1}\\}$ remains above $x^\\star$ and is monotone decreasing towards $x^\\star$. If $x_0x^\\star$, the roles of the subsequences reverse, but the same alternating monotone convergence holds. In the special case $x_0=x^\\star$, the sequence is constant and trivially monotone. The contraction bound also yields a linear convergence rate:\n$$\n|x_{k}-x^\\star|\\le L^k |x_0-x^\\star|,\n$$\nwith $L=\\sin(1)$.\n\nAlgorithm design for the program:\n- For each test case initial value $x_0$, compute iterates $x_{k+1}=\\cos x_k$ (with angles in radians) until $|x_{k+1}-x_k|10^{-12}$ or a maximum of $1000$ iterations is reached. Store all iterates to check monotonicity.\n- Determine if the full sequence is monotone by checking whether all consecutive differences are nonnegative (nondecreasing) or all are nonpositive (nonincreasing), allowing equality. Determine monotonicity for the even subsequence $\\{x_{2k}\\}$ and the odd subsequence $\\{x_{2k+1}\\}$ similarly.\n- Report for each test case the list $[x_0,x_{\\text{approx}},N,\\text{is\\_monotone\\_full},\\text{is\\_monotone\\_even},\\text{is\\_monotone\\_odd}]$, where $x_{\\text{approx}}$ is the last iterate and $N$ the number of iteration steps performed.\n- Use the test suite $x_0\\in\\{0,\\,\\tfrac{1}{2},\\,1,\\,0.7390851332151607\\}$.\n\nThe contraction constant $L=\\sin(1)$ validates both the uniqueness of the fixed point and the convergence claim. The numerical results will reflect the theoretical monotone behavior: the full sequence generally oscillates but the even and odd subsequences are monotone and both converge to the same limit $x^\\star$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fixed_point_cos(x0, tol=1e-12, max_iter=1000):\n    \"\"\"\n    Perform fixed-point iteration x_{k+1} = cos(x_k) starting from x0.\n    Angles are in radians.\n    Returns:\n        x_approx: final approximation\n        iterations: number of iterations performed\n        seq: list of iterates including the initial value\n    \"\"\"\n    seq = [float(x0)]\n    x_prev = float(x0)\n    iterations = 0\n    for k in range(max_iter):\n        x_next = float(np.cos(x_prev))\n        seq.append(x_next)\n        iterations += 1\n        if abs(x_next - x_prev)  tol:\n            break\n        x_prev = x_next\n    return seq[-1], iterations, seq\n\ndef is_monotone(sequence, tol=0.0):\n    \"\"\"\n    Check if a sequence is monotone nondecreasing or monotone nonincreasing.\n    Equality is allowed.\n    tol can be used to soften comparisons, but defaults to strict.\n    \"\"\"\n    if len(sequence) = 1:\n        return True\n    diffs = [sequence[i+1] - sequence[i] for i in range(len(sequence)-1)]\n    nondecreasing = all(d = -tol for d in diffs)\n    nonincreasing = all(d = tol for d in diffs)\n    return nondecreasing or nonincreasing\n\ndef solve():\n    # Define the test cases from the problem statement (radians).\n    test_cases = [\n        0.0,\n        0.5,\n        1.0,\n        0.7390851332151607,  # high-precision approximation to the fixed point\n    ]\n\n    results = []\n    for x0 in test_cases:\n        x_approx, iters, seq = fixed_point_cos(x0, tol=1e-12, max_iter=1000)\n        # Full sequence monotonicity\n        mono_full = is_monotone(seq, tol=0.0)\n        # Even-indexed subsequence: indices 0,2,4,...\n        even_seq = seq[0::2]\n        mono_even = is_monotone(even_seq, tol=0.0)\n        # Odd-indexed subsequence: indices 1,3,5,...\n        odd_seq = seq[1::2]\n        mono_odd = is_monotone(odd_seq, tol=0.0)\n        # Assemble result for this test case\n        results.append([x0, x_approx, iters, mono_full, mono_even, mono_odd])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3231288"}, {"introduction": "A root-finding problem can often be rearranged into a fixed-point problem in multiple ways, but not all rearrangements are created equal. This exercise challenges you to compare two different fixed-point schemes for the same equation, focusing on a crucial practical question: for which starting points is convergence guaranteed? You will apply the core convergence criterion, $|g'(x)| \\lt 1$, to determine and compare the exact domains of convergence for each scheme, honing your analytical skills in assessing the viability of an iterative method [@problem_id:3231244].", "problem": "Let $f(x) = x^{3} - 2x + 2$. It is known that $f$ has a unique real root, denoted by $\\alpha$. Consider two fixed-point rearrangements $x = g(x)$ for solving $f(x) = 0$:\n- $g_{1}(x) = \\sqrt[3]{2x - 2}$,\n- $g_{2}(x) = x - \\dfrac{1}{4}\\bigl(x^{3} - 2x + 2\\bigr)$.\n\nUsing only the definition of fixed-point iteration, the mean value theorem, and the Banach Contraction Mapping Theorem (also called the Contraction Mapping Theorem), determine, for each $g_{i}$, the largest symmetric interval centered at $\\alpha$ of the form $I_{i}(r_{i}) = [\\alpha - r_{i}, \\alpha + r_{i}]$ on which the iteration $x_{n+1} = g_{i}(x_{n})$ is guaranteed to converge to $\\alpha$ for every initial guess $x_{0} \\in I_{i}(r_{i})$. The guarantee must be justified solely by verifying that $g_{i}$ is continuously differentiable on $I_{i}(r_{i})$ and that $\\sup_{x \\in I_{i}(r_{i})} |g_{i}'(x)| \\le q_{i}  1$, invoking the Contraction Mapping Theorem to conclude invariance and convergence.\n\nLet $I_{\\cap} = I_{1}(r_{1}) \\cap I_{2}(r_{2})$ be the largest symmetric interval about $\\alpha$ on which both iterations are simultaneously guaranteed to converge by this criterion. Compute the length $L$ of $I_{\\cap}$.\n\nRound your final numerical answer for $L$ to four significant figures.", "solution": "The user wants me to solve the following problem:\nLet $f(x) = x^{3} - 2x + 2$. It is known that $f$ has a unique real root, denoted by $\\alpha$. Consider two fixed-point rearrangements $x = g(x)$ for solving $f(x) = 0$:\n- $g_{1}(x) = \\sqrt[3]{2x - 2}$,\n- $g_{2}(x) = x - \\dfrac{1}{4}\\bigl(x^{3} - 2x + 2\\bigr)$.\nUsing only the definition of fixed-point iteration, the mean value theorem, and the Banach Contraction Mapping Theorem (also called the Contraction Mapping Theorem), determine, for each $g_{i}$, the largest symmetric interval centered at $\\alpha$ of the form $I_{i}(r_{i}) = [\\alpha - r_{i}, \\alpha + r_{i}]$ on which the iteration $x_{n+1} = g_{i}(x_{n})$ is guaranteed to converge to $\\alpha$ for every initial guess $x_{0} \\in I_{i}(r_{i})$. The guarantee must be justified solely by verifying that $g_{i}$ is continuously differentiable on $I_{i}(r_{i})$ and that $\\sup_{x \\in I_{i}(r_{i})} |g_{i}'(x)| \\le q_{i}  1$, invoking the Contraction Mapping Theorem to conclude invariance and convergence. Let $I_{\\cap} = I_{1}(r_{1}) \\cap I_{2}(r_{2})$ be the largest symmetric interval about $\\alpha$ on which both iterations are simultaneously guaranteed to converge by this criterion. Compute the length $L$ of $I_{\\cap}$. Round your final numerical answer for $L$ to four significant figures.\n\nFirst, I will validate the problem statement.\n\n### Step 1: Extract Givens\n-   The function is $f(x) = x^{3} - 2x + 2$.\n-   $\\alpha$ is the unique real root of $f(x) = 0$, so $\\alpha^3 - 2\\alpha + 2 = 0$.\n-   The first fixed-point iteration function is $g_{1}(x) = \\sqrt[3]{2x - 2}$.\n-   The second fixed-point iteration function is $g_{2}(x) = x - \\frac{1}{4}(x^{3} - 2x + 2)$.\n-   The task is to find the largest radius $r_i  0$ for each $g_i$ such that the iteration $x_{n+1} = g_i(x_n)$ is guaranteed to converge for any starting point $x_0$ in the symmetric interval $I_i(r_i) = [\\alpha - r_i, \\alpha + r_i]$.\n-   The condition for guaranteed convergence on $I_i(r_i)$ is that $g_i$ is continuously differentiable and $\\sup_{x \\in I_i(r_i)} |g_i'(x)| \\le q_i  1$.\n-   Then, define $I_{\\cap} = I_1(r_1) \\cap I_2(r_2)$ and compute its length $L$.\n-   The final numerical answer for $L$ must be rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Unsoundness**: The problem is based on the standard theory of fixed-point iterations and the Contraction Mapping Theorem. Let's verify that $\\alpha$ is a fixed point for both functions.\n    -   For $g_1(x)$: if $\\alpha = g_1(\\alpha)$, then $\\alpha = \\sqrt[3]{2\\alpha - 2}$, which implies $\\alpha^3 = 2\\alpha - 2$, or $\\alpha^3 - 2\\alpha + 2 = 0$. This is true by definition of $\\alpha$.\n    -   For $g_2(x)$: if $\\alpha = g_2(\\alpha)$, then $\\alpha = \\alpha - \\frac{1}{4}(\\alpha^3 - 2\\alpha + 2)$, which implies $\\frac{1}{4}(\\alpha^3 - 2\\alpha + 2) = 0$. This is also true by definition of $\\alpha$.\n    -   The statement that $f(x)$ has a unique real root is correct. $f'(x) = 3x^2 - 2$ has roots at $x = \\pm\\sqrt{2/3}$. The local maximum is $f(-\\sqrt{2/3}) = 2 + \\frac{4}{3}\\sqrt{\\frac{2}{3}}  0$ and the local minimum is $f(\\sqrt{2/3}) = 2 - \\frac{4}{3}\\sqrt{\\frac{2}{3}}  0$. Since $\\lim_{x\\to-\\infty} f(x) = -\\infty$ and the local minimum is positive, there must be exactly one real root, and it must be less than $-\\sqrt{2/3}$.\n    The premises are factually and mathematically sound.\n2.  **Non-Formalizable or Irrelevant**: The problem is a standard, formalizable problem in numerical analysis, directly related to the specified topic.\n3.  **Incomplete or Contradictory Setup**: The problem is well-defined and self-contained. The numerical value of $\\alpha$ is not given but can be computed, which is an implicit part of solving the problem.\n4.  **Unrealistic or Infeasible**: The problem is a theoretical exercise with no reference to physical reality, so this is not applicable.\n5.  **Ill-Posed or Poorly Structured**: The term \"largest symmetric interval\" is well-defined in this context. It refers to the interval $[\\alpha - r, \\alpha + r]$ where $r$ is the supremum of all possible radii for which the convergence condition holds. The problem is well-posed.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem requires non-trivial application of calculus and analysis. It is not trivial.\n7.  **Outside Scientific Verifiability**: The problem is fully verifiable through mathematical derivation and computation.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will now proceed with the solution.\n\nThe problem requires finding the largest radii $r_1$ and $r_2$ for the intervals $I_1 = [\\alpha - r_1, \\alpha + r_1]$ and $I_2 = [\\alpha - r_2, \\alpha + r_2]$ on which the fixed-point iterations for $g_1(x)$ and $g_2(x)$ are guaranteed to converge. The criterion given is that $\\sup_{x \\in I_i} |g_i'(x)|  1$. The largest such intervals will be bounded by the points where $|g_i'(x)|=1$.\n\nFirst, we need a numerical approximation of the root $\\alpha$. The equation is $f(x) = x^3-2x+2=0$. Numerical methods (like Newton's method or simply a calculator) yield $\\alpha \\approx -1.76929235$.\n\n**Analysis of $g_1(x)$**\nThe first iteration function is $g_1(x) = (2x - 2)^{1/3}$.\nIts derivative is $g_1'(x) = \\frac{1}{3}(2x-2)^{-2/3}(2) = \\frac{2}{3(2x-2)^{2/3}}$.\nThe derivative is continuous for all $x \\neq 1$. Since $\\alpha  0$, the interval $I_1(r_1)$ will not contain $1$ for any reasonable $r_1$.\nThe term $(2x-2)^{2/3} = ((2x-2)^2)^{1/3}$ is always positive. Thus, $g_1'(x)  0$.\nWe need to find the behavior of $g_1'(x)$ to find its supremum on $I_1(r_1)$. The function $h(x) = (2x-2)^2$ has its minimum at $x=1$. For $x1$, $h(x)$ is a decreasing function. Therefore, $(2x-2)^{2/3}$ is also decreasing for $x1$. This means $g_1'(x) = \\frac{2}{3(2x-2)^{2/3}}$ is an increasing function for $x1$.\nThe interval is $I_1(r_1) = [\\alpha-r_1, \\alpha+r_1]$. Since $g_1'(x)$ is increasing on $(-\\infty, 1)$, the supremum on $I_1(r_1)$ (assuming $\\alpha+r_11$) is achieved at the right endpoint, $x=\\alpha+r_1$.\nTo find the largest interval, we set the supremum of $|g_1'(x)|$ to $1$:\n$g_1'(\\alpha+r_1) = 1$\n$$ \\frac{2}{3(2(\\alpha+r_1)-2)^{2/3}} = 1 $$\n$$ (2(\\alpha+r_1-1))^{2/3} = \\frac{2}{3} $$\nRaising both sides to the power of $3/2$:\n$$ |2(\\alpha+r_1-1)| = \\left(\\frac{2}{3}\\right)^{3/2} = \\frac{2\\sqrt{2}}{3\\sqrt{3}} = \\frac{2\\sqrt{6}}{9} $$\n$$ |\\alpha+r_1-1| = \\frac{\\sqrt{6}}{9} $$\nSince we are in the domain $x1$, we have $\\alpha+r_1  1$, which means $\\alpha+r_1-1  0$. So we must take the negative case:\n$$ -(\\alpha+r_1-1) = \\frac{\\sqrt{6}}{9} $$\n$$ 1 - \\alpha - r_1 = \\frac{\\sqrt{6}}{9} $$\n$$ r_1 = 1 - \\alpha - \\frac{\\sqrt{6}}{9} $$\n\n**Analysis of $g_2(x)$**\nThe second iteration function is $g_2(x) = x - \\frac{1}{4}(x^3 - 2x + 2)$.\nIts derivative is $g_2'(x) = 1 - \\frac{1}{4}(3x^2 - 2) = 1 - \\frac{3}{4}x^2 + \\frac{1}{2} = \\frac{3}{2} - \\frac{3}{4}x^2$.\nThis function is continuously differentiable everywhere.\nWe need to find the region where $|g_2'(x)|  1$.\n$$ -1  \\frac{3}{2} - \\frac{3}{4}x^2  1 $$\nThis gives two inequalities:\n1. $\\frac{3}{2} - \\frac{3}{4}x^2  1 \\implies \\frac{1}{2}  \\frac{3}{4}x^2 \\implies x^2  \\frac{2}{3}$. This means $x \\in (-\\infty, -\\sqrt{2/3}) \\cup (\\sqrt{2/3}, \\infty)$.\n2. $-1  \\frac{3}{2} - \\frac{3}{4}x^2 \\implies \\frac{3}{4}x^2  \\frac{5}{2} \\implies x^2  \\frac{10}{3}$. This means $x \\in (-\\sqrt{10/3}, \\sqrt{10/3})$.\n\nCombining these two conditions, the set of points where the iteration is guaranteed to converge is $(-\\sqrt{10/3}, -\\sqrt{2/3}) \\cup (\\sqrt{2/3}, \\sqrt{10/3})$.\nThe root $\\alpha \\approx -1.769$ lies in the interval $(-\\sqrt{10/3}, -\\sqrt{2/3})$, since $-\\sqrt{10/3} \\approx -1.826$ and $-\\sqrt{2/3} \\approx -0.816$.\nWe need to find the largest symmetric interval $I_2(r_2) = [\\alpha - r_2, \\alpha + r_2]$ that is fully contained in $(-\\sqrt{10/3}, -\\sqrt{2/3})$.\nThe radius $r_2$ must be less than the distance from $\\alpha$ to either endpoint of the convergence interval.\nThe distance to the left endpoint $-\\sqrt{10/3}$ is $|\\alpha - (-\\sqrt{10/3})| = \\alpha + \\sqrt{10/3}$.\nThe distance to the right endpoint $-\\sqrt{2/3}$ is $|-\\sqrt{2/3} - \\alpha| = -\\alpha - \\sqrt{2/3}$ (since $\\alpha  -\\sqrt{2/3}$).\nThe largest possible radius $r_2$ is the minimum of these two distances:\n$$ r_2 = \\min\\left(\\alpha + \\sqrt{\\frac{10}{3}}, -\\alpha - \\sqrt{\\frac{2}{3}}\\right) $$\n\n**Comparison and Final Calculation**\nNow we must compare $r_1$ and $r_2$. We use the numerical value $\\alpha \\approx -1.769292$.\n$$ r_1 = 1 - \\alpha - \\frac{\\sqrt{6}}{9} \\approx 1 - (-1.769292) - \\frac{2.449490}{9} \\approx 2.769292 - 0.272166 \\approx 2.497126 $$\nFor $r_2$:\n$$ \\alpha + \\sqrt{\\frac{10}{3}} \\approx -1.769292 + 1.825742 \\approx 0.056450 $$\n$$ -\\alpha - \\sqrt{\\frac{2}{3}} \\approx 1.769292 - 0.816497 \\approx 0.952795 $$\nSo, $r_2 = \\min(0.056450, 0.952795) = 0.056450$.\nThe smaller radius is $r_2$.\nThe largest symmetric interval $I_{\\cap}$ on which both iterations are guaranteed to converge is determined by the smaller radius, $r_{\\min} = \\min(r_1, r_2) = r_2$.\n$$ I_{\\cap} = [\\alpha - r_2, \\alpha + r_2] $$\nThe length of this interval is $L = 2 r_2$.\n$$ L = 2 \\min\\left(\\alpha + \\sqrt{\\frac{10}{3}}, -\\alpha - \\sqrt{\\frac{2}{3}}\\right) = 2 \\left(\\alpha + \\sqrt{\\frac{10}{3}}\\right) $$\nUsing the numerical value:\n$$ L \\approx 2 \\times 0.056450 = 0.112900 $$\nRounding to four significant figures, we get $L=0.1129$.", "answer": "$$\\boxed{0.1129}$$", "id": "3231244"}, {"introduction": "While linear convergence guarantees that an iteration will eventually reach the solution, it can be frustratingly slow. This advanced practice introduces a powerful technique, Aitken's $\\Delta^2$ method, for accelerating the convergence of linear schemes. You will derive the acceleration formula from first principles and implement it to see a dramatic improvement in efficiency, turning a slow crawl towards the solution into a rapid jump [@problem_id:3231237].", "problem": "You are given the fixed-point iteration defined by the mapping $g(x) = \\dfrac{x + 2}{2}$ and the iteration rule $x_{k+1} = g(x_k)$. Consider the unique fixed point $x^\\star$ satisfying $x^\\star = g(x^\\star)$. You will analyze convergence from principles and implement an algorithm to accelerate convergence using Aitken's delta-squared method.\n\nTask A (theoretical foundations):\n1. Starting from the definition of a fixed point and the contraction mapping principle, establish that the equation $x = g(x)$ has a unique solution and that the fixed-point iteration $x_{k+1} = g(x_k)$ is locally convergent to that solution. Use the differentiability of $g$ and the condition $\\lvert g'(x^\\star) \\rvert  1$ as your core criterion for local linear convergence.\n2. Determine the exact fixed point $x^\\star$ by solving the algebraic equation $x = g(x)$.\n3. Using the definition of linear convergence, analyze the error sequence $e_k = x_k - x^\\star$ and identify the asymptotic rate (the linear convergence factor) for the unaccelerated fixed-point iteration.\n\nTask B (algorithmic design of acceleration):\n1. Starting from the general model of a linearly convergent sequence, derive Aitken's delta-squared acceleration in terms of forward differences of the sequence $\\{x_k\\}$. Your derivation must rely only on the assumption that the error behaves asymptotically like a geometric progression and on the basic definitions of forward differences. Do not assume or quote any pre-derived closed-form acceleration formula; instead, construct it from first principles.\n2. Design a robust algorithm that, given a starting value $x_0$ and a tolerance $\\text{tol}  0$, uses successive triplets $(x_k, x_{k+1}, x_{k+2})$ from the base iteration to produce an Aitken-accelerated estimate. Your algorithm must:\n   - Treat the computation as successful when the current approximation satisfies $\\lvert x - x^\\star \\rvert \\le \\text{tol}$.\n   - Count computational cost as the number of evaluations of $g$.\n   - Handle degeneracy when the second forward difference is zero or numerically indistinguishable from zero by detecting this condition and treating it as an indication that either the iteration is already at a fixed point or that Aitkenâ€™s update cannot be formed reliably from that triplet. In such a case, the algorithm should either terminate (if already within tolerance) or safely continue with additional base iterates to form a new triplet.\n\nTask C (programming and evaluation):\n1. Implement two procedures:\n   - A base fixed-point iterator that returns the smallest number of evaluations of $g$ required to meet $\\lvert x - x^\\star \\rvert \\le \\text{tol}$ starting from $x_0$.\n   - An Aitken-accelerated procedure that repeatedly forms Aitken updates from consecutive triples of base iterates and returns the smallest number of evaluations of $g$ required to first achieve $\\lvert x - x^\\star \\rvert \\le \\text{tol}$ in the accelerated sequence (counting every evaluation of $g$ used to generate the required iterates).\n2. Use a maximum cap of $10^6$ evaluations of $g$ to guarantee termination in pathological scenarios.\n3. Test suite. Run both procedures on the following four test cases, where each test case is a pair $(x_0, \\text{tol})$:\n   - $(0.0, 10^{-8})$\n   - $(-100.0, 10^{-12})$\n   - $(2.0, 10^{-14})$\n   - $(10.0, 10^{-10})$\n4. For each test case, compute and return a triple consisting of:\n   - The integer count of evaluations of $g$ required by the base fixed-point iteration to meet the tolerance.\n   - The integer count of evaluations of $g$ required by the Aitken-accelerated procedure to first meet the tolerance.\n   - A boolean indicating whether the Aitken-accelerated procedure used strictly fewer evaluations of $g$ than the base procedure.\n5. Final output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is a test-case triple in the order specified above. For example, a valid output format looks like:\n   - $[[n_{b,1},n_{a,1},\\text{flag}_1],[n_{b,2},n_{a,2},\\text{flag}_2],[n_{b,3},n_{a,3},\\text{flag}_3],[n_{b,4},n_{a,4},\\text{flag}_4]]$\nHere $n_{b,i}$ and $n_{a,i}$ are integers, and $\\text{flag}_i$ is a boolean. There are no physical units involved in this task. Angles are not applicable. Percentages are not used.", "solution": "The problem statement has been analyzed and is deemed valid. It is a well-posed problem in numerical analysis that is scientifically grounded, self-contained, and objective.\n\n### Task A (Theoretical Foundations): Unique Fixed Point and Linear Convergence\n\n1.  **Existence, Uniqueness, and Convergence:**\n    The fixed-point iteration is defined by the function $g(x) = \\dfrac{x + 2}{2}$. A fixed point $x^\\star$ satisfies the equation $x^\\star = g(x^\\star)$.\n    To analyze convergence, we apply the Contraction Mapping Theorem. The theorem states that if a function $g$ maps a complete metric space into itself and is a contraction, it possesses a unique fixed point, and the iteration $x_{k+1} = g(x_k)$ converges to this fixed point for any starting point $x_0$.\n    The function $g(x)$ is defined on $\\mathbb{R}$, which is a complete metric space. To check if $g$ is a contraction, we examine its derivative:\n    $$\n    g'(x) = \\frac{d}{dx}\\left(\\frac{x}{2} + 1\\right) = \\frac{1}{2}\n    $$\n    The Lipschitz constant $L$ for $g(x)$ is given by $L = \\sup_{x \\in \\mathbb{R}} |g'(x)|$. In this case, $L = |\\frac{1}{2}| = \\frac{1}{2}$.\n    Since $L = \\frac{1}{2}  1$, the function $g(x)$ is a contraction on all of $\\mathbb{R}$. Therefore, by the Contraction Mapping Theorem, there exists a unique fixed point $x^\\star \\in \\mathbb{R}$, and the iteration $x_{k+1} = g(x_k)$ converges to $x^\\star$ for any initial value $x_0 \\in \\mathbb{R}$.\n    Local convergence near a fixed point $x^\\star$ is guaranteed if $|g'(x^\\star)|  1$. As we have found, $|g'(x)| = \\frac{1}{2}$ for all $x$, including $x^\\star$. Thus, $|g'(x^\\star)| = \\frac{1}{2}  1$, which establishes that the iteration is locally linearly convergent.\n\n2.  **Determination of the Fixed Point:**\n    The fixed point $x^\\star$ is found by solving the algebraic equation $x = g(x)$:\n    $$\n    x = \\frac{x + 2}{2}\n    $$\n    $$\n    2x = x + 2\n    $$\n    $$\n    x = 2\n    $$\n    The unique fixed point is $x^\\star = 2$.\n\n3.  **Error Analysis and Convergence Rate:**\n    The error at iteration $k$ is defined as $e_k = x_k - x^\\star$. The error at the next step is $e_{k+1} = x_{k+1} - x^\\star$. Substituting the iteration rule and the fixed point definition:\n    $$\n    e_{k+1} = g(x_k) - x^\\star = g(x_k) - g(x^\\star)\n    $$\n    Using the Mean Value Theorem, there exists a value $\\xi_k$ between $x_k$ and $x^\\star$ such that $g(x_k) - g(x^\\star) = g'(\\xi_k)(x_k - x^\\star)$. This leads to the error relationship $e_{k+1} = g'(\\xi_k)e_k$.\n    As $k \\to \\infty$, we have $x_k \\to x^\\star$, and thus $\\xi_k \\to x^\\star$. The asymptotic error relationship is:\n    $$\n    \\lim_{k \\to \\infty} \\frac{|e_{k+1}|}{|e_k|} = |g'(x^\\star)|\n    $$\n    This demonstrates linear convergence. The constant $\\lambda = |g'(x^\\star)|$ is the asymptotic rate of convergence.\n    For the given function $g(x)$, the derivative $g'(x) = \\frac{1}{2}$ is constant. Therefore, the relationship is exact for all $k$:\n    $$\n    e_{k+1} = g(x_k) - g(x^\\star) = \\left(\\frac{x_k+2}{2}\\right) - \\left(\\frac{x^\\star+2}{2}\\right) = \\frac{1}{2}(x_k - x^\\star) = \\frac{1}{2}e_k\n    $$\n    The convergence rate is constant, $\\lambda = \\frac{1}{2}$, not just asymptotic.\n\n### Task B (Algorithmic Design of Acceleration): Aitken's Delta-Squared Method\n\n1.  **Derivation from First Principles:**\n    Aitken's method accelerates the convergence of a sequence $\\{x_k\\}$ that is known to converge linearly. The underlying assumption is that for large $k$, the error $e_k = x_k - x^\\star$ behaves like a geometric progression. This means the ratio of successive errors is approximately constant:\n    $$\n    \\frac{e_{k+1}}{e_k} \\approx \\lambda \\quad \\implies \\quad x_{k+1} - x^\\star \\approx \\lambda(x_k - x^\\star)\n    $$\n    To construct an accelerated estimate, we use three consecutive terms of the sequence, $(x_k, x_{k+1}, x_{k+2})$. We assume the same ratio holds for the next pair:\n    $$\n    x_{k+1} - x^\\star \\approx \\lambda(x_k - x^\\star) \\quad (1)\n    $$\n    $$\n    x_{k+2} - x^\\star \\approx \\lambda(x_{k+1} - x^\\star) \\quad (2)\n    $$\n    We can solve for the unknown limit $x^\\star$ by eliminating $\\lambda$. From $(1)$ and $(2)$, we have:\n    $$\n    \\frac{x_{k+1} - x^\\star}{x_k - x^\\star} \\approx \\lambda \\approx \\frac{x_{k+2} - x^\\star}{x_{k+1} - x^\\star}\n    $$\n    Cross-multiplying gives:\n    $$\n    (x_{k+1} - x^\\star)^2 \\approx (x_k - x^\\star)(x_{k+2} - x^\\star)\n    $$\n    Expanding the terms:\n    $$\n    x_{k+1}^2 - 2x_{k+1}x^\\star + (x^\\star)^2 \\approx x_k x_{k+2} - x_k x^\\star - x_{k+2} x^\\star + (x^\\star)^2\n    $$\n    Rearranging to solve for $x^\\star$:\n    $$\n    x_{k+1}^2 - x_k x_{k+2} \\approx (2x_{k+1} - x_k - x_{k+2})x^\\star\n    $$\n    $$\n    x^\\star \\approx \\frac{x_k x_{k+2} - x_{k+1}^2}{x_{k+2} - 2x_{k+1} + x_k}\n    $$\n    This formula gives an improved estimate of the fixed point. To express it in terms of forward differences, we define $\\Delta x_k = x_{k+1} - x_k$ and the second forward difference $\\Delta^2 x_k = \\Delta(\\Delta x_k) = (x_{k+2} - x_{k+1}) - (x_{k+1} - x_k) = x_{k+2} - 2x_{k+1} + x_k$.\n    The Aitken-accelerated estimate, denoted $\\hat{x}_k$, can be written as:\n    $$\n    \\hat{x}_k = x_k - \\frac{(\\Delta x_k)^2}{\\Delta^2 x_k} = x_k - \\frac{(x_{k+1} - x_k)^2}{x_{k+2} - 2x_{k+1} + x_k}\n    $$\n    This is the standard Aitken's delta-squared formula. For the specific linear function $g(x)$ in this problem, the ratios are exact, not approximate, meaning the formula will yield the exact fixed point $x^\\star$ from any non-trivial triplet of iterates.\n\n2.  **Algorithm Design:**\n    An algorithm is designed to compute a sequence of Aitken-accelerated estimates.\n    -   **Initialization**: Given $x_0$ and `tol`, the exact fixed point is $x^\\star = 2.0$. The number of evaluations of $g$ is the cost metric.\n    -   **Iteration**: The algorithm generates base iterates $x_{k+1} = g(x_k)$ sequentially. After at least two evaluations of $g$, we have a triplet $(x_k, x_{k+1}, x_{k+2})$ available to form an Aitken estimate.\n    -   **Aitken Update**: For each new base iterate $x_{k+2}$ generated, a new triplet is formed, and the formula $\\hat{x}_k = x_k - (\\Delta x_k)^2 / (\\Delta^2 x_k)$ is computed.\n    -   **Convergence Check**: After each Aitken update, the condition $|\\hat{x}_k - x^\\star| \\le \\text{tol}$ is checked. If it is met, the process terminates, and the total number of $g$ evaluations used to generate the required base iterates is returned.\n    -   **Degeneracy Handling**: The formula for $\\hat{x}_k$ involves division by the second difference, $\\Delta^2 x_k$. If this denominator is zero or numerically very close to zero, the update cannot be reliably computed. This typically happens when the sequence has already converged (i.e., $x_k = x_{k+1} = x_{k+2}$). The algorithm must detect this condition. If $\\Delta^2 x_k \\approx 0$, it checks if the last base iterate is within tolerance. If not, it forgoes the Aitken update for that triplet and continues generating more base iterates until $\\Delta^2 x_k$ becomes non-negligible again.\n\n### Task C (Programming and Evaluation): Implementation and Results\n\nTwo procedures are implemented: one for the base fixed-point iteration and one for the Aitken-accelerated iteration. Both functions count the number of evaluations of $g(x)$ required to get an estimate $x$ such that $|x - x^\\star| \\le \\text{tol}$.\n\n-   **Base Iterator**: This is a straightforward loop. Starting with $x_0$, it repeatedly applies $x_{k+1} = g(x_k)$, incrementing an evaluation counter, until the error tolerance is met.\n-   **Aitken Iterator**: This procedure generates base iterates $x_0, x_1, x_2, \\dots$. Starting from the first available triplet $(x_0, x_1, x_2)$ (which requires 2 evaluations of $g$), it computes an Aitken estimate $\\hat{x}_0$. If $\\hat{x}_0$ is not converged, it generates $x_3$ (3 total evaluations), forms a new Aitken estimate $\\hat{x}_1$ from $(x_1, x_2, x_3)$, and so on, until an accelerated estimate meets the tolerance.\n\nThe analysis in Task B predicts that for the given linear $g(x)$, the first Aitken estimate from any non-trivial starting point will be exact. This means the accelerated procedure should always terminate after 2 evaluations of $g$, provided $x_0 \\neq x^\\star$. The base iteration's performance depends on the initial error and tolerance.\n\nThe numerical results for the test suite are:\n1.  **$(x_0, \\text{tol}) = (0.0, 10^{-8})$**: Base iterator requires $28$ evaluations. Aitken iterator requires $2$.\n2.  **$(x_0, \\text{tol}) = (-100.0, 10^{-12})$**: Base iterator requires $47$ evaluations. Aitken iterator requires $2$.\n3.  **$(x_0, \\text{tol}) = (2.0, 10^{-14})$**: Both iterators start at the fixed point, so $0$ evaluations are needed.\n4.  **$(x_0, \\text{tol}) = (10.0, 10^{-10})$**: Base iterator requires $37$ evaluations. Aitken iterator requires $2$.\n\nIn all non-trivial cases, the Aitken-accelerated procedure is strictly faster, confirming its effectiveness for linearly convergent sequences.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the fixed-point iteration problem with base and Aitken-accelerated methods.\n    \"\"\"\n    \n    # The mapping function for the fixed-point iteration.\n    g = lambda x: (x + 2.0) / 2.0\n    \n    # The exact fixed point.\n    x_star = 2.0\n    \n    # Maximum number of evaluations to prevent infinite loops.\n    max_evals = 1000000\n    \n    # Epsilon for degeneracy check in Aitken's method.\n    degeneracy_eps = 1e-15\n\n    def base_iterator(x0, tol):\n        \"\"\"\n        Calculates the number of g evaluations for the base fixed-point method.\n        \"\"\"\n        x_k = float(x0)\n        \n        # Check if already converged at x0.\n        if abs(x_k - x_star) = tol:\n            return 0\n        \n        for num_evals in range(1, max_evals + 1):\n            x_k = g(x_k)\n            if abs(x_k - x_star) = tol:\n                return num_evals\n        \n        return max_evals\n\n    def aitken_iterator(x0, tol):\n        \"\"\"\n        Calculates the number of g evaluations for the Aitken-accelerated method.\n        \"\"\"\n        x_k = float(x0)\n\n        # Check if already converged at x0.\n        if abs(x_k - x_star) = tol:\n            return 0\n\n        x_seq = [x_k]\n\n        # Generate the first two points to form the first triplet.\n        # Eval 1:\n        x_k_plus_1 = g(x_seq[-1])\n        x_seq.append(x_k_plus_1)\n        num_evals = 1\n        # Aitken cannot be applied yet.\n\n        # Eval 2 and subsequent:\n        for num_evals in range(2, max_evals + 1):\n            x_k_plus_2 = g(x_seq[-1])\n            x_seq.append(x_k_plus_2)\n\n            # We have a triplet (x_k, x_{k+1}, x_{k+2})\n            # which are x_seq[-3], x_seq[-2], x_seq[-1]\n            xk, xk1, xk2 = x_seq[-3], x_seq[-2], x_seq[-1]\n            \n            delta_sq_x = xk2 - 2 * xk1 + xk\n\n            # Degeneracy check\n            if abs(delta_sq_x)  degeneracy_eps:\n                # This indicates convergence or numerical instability.\n                # Check if the last base iterate is converged.\n                if abs(xk2 - x_star) = tol:\n                    return num_evals\n                # Otherwise, continue generating base iterates as Aitken update is not reliable.\n                continue\n\n            delta_x = xk1 - xk\n            x_aitken = xk - (delta_x**2) / delta_sq_x\n            \n            if abs(x_aitken - x_star) = tol:\n                return num_evals\n        \n        return max_evals\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 1e-8),\n        (-100.0, 1e-12),\n        (2.0, 1e-14),\n        (10.0, 1e-10)\n    ]\n\n    results = []\n    for case in test_cases:\n        x0, tol = case\n        \n        base_evals = base_iterator(x0, tol)\n        aitken_evals = aitken_iterator(x0, tol)\n        \n        aitken_is_faster = aitken_evals  base_evals\n        \n        results.append([base_evals, aitken_evals, aitken_is_faster])\n\n    # Final print statement in the exact required format.\n    # The default string representation of a list in Python is used, as per the template.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3231237"}]}