{"hands_on_practices": [{"introduction": "Understanding algorithm stability often begins with a simple, intuitive example. This practice explores how the computational structure of an algorithm can dramatically affect its numerical stability, even when the underlying mathematics are identical. By comparing an iterative and a naive recursive implementation for computing Fibonacci numbers, you will directly observe how a massive number of redundant operations can catastrophically amplify rounding errors, providing a clear lesson on the importance of algorithmic efficiency for achieving accurate results [@problem_id:3205171].", "problem": "You are asked to study algorithm robustness and stability by comparing how rounding errors propagate in two implementations of the Fibonacci sequence under a controlled rounding model. Use the following fundamental base to frame your analysis and implementation.\n\n- Fibonacci numbers are defined by the recurrence $F_0 = 0$, $F_1 = 1$, and $F_n = F_{n-1} + F_{n-2}$ for $n \\ge 2$.\n- Adopt a standard floating-point rounding model for addition: each addition is rounded to a finite precision by an operator $\\mathrm{fl}_t(\\cdot)$ that rounds any real number to $t$ significant decimal digits using rounding-to-nearest with ties-to-even. In analysis form, one may write $\\mathrm{fl}_t(a+b) = (a+b)(1+\\delta)$ with $|\\delta| \\le u$, where $u$ (the unit roundoff) depends on $t$.\n\nYour task is to implement two algorithms that both use the same rounding operator $\\mathrm{fl}_t(\\cdot)$ after each addition:\n\n1) Iterative implementation:\n- Initialize $x_0 = \\mathrm{fl}_t(0)$, $x_1 = \\mathrm{fl}_t(1)$.\n- For $k = 2, 3, \\dots, n$, set $x_k = \\mathrm{fl}_t(x_{k-1} + x_{k-2})$.\n- Return $x_n$.\n\n2) Naive recursive implementation:\n- Define a function $\\text{fib\\_rec}(n)$ by\n  - If $n = 0$, return $\\mathrm{fl}_t(0)$.\n  - If $n = 1$, return $\\mathrm{fl}_t(1)$.\n  - Otherwise, return $\\mathrm{fl}_t(\\text{fib\\_rec}(n-1) + \\text{fib\\_rec}(n-2))$.\n\nFor a given pair $(n,t)$, compute:\n- The exact value $F_n$ as an integer using the recurrence without rounding.\n- The iteratively rounded value $X_n^{\\mathrm{iter}}$ and the recursively rounded value $X_n^{\\mathrm{rec}}$.\n- The relative errors (expressed as decimals, not percentages and without any unit):\n  - $r_{\\mathrm{iter}} = \\dfrac{|X_n^{\\mathrm{iter}} - F_n|}{|F_n|}$,\n  - $r_{\\mathrm{rec}} = \\dfrac{|X_n^{\\mathrm{rec}} - F_n|}{|F_n|}$,\n  - and their difference $s = r_{\\mathrm{rec}} - r_{\\mathrm{iter}}$.\n\nTest Suite:\nUse the following parameter pairs $(n,t)$:\n- Case A: $(n,t) = (1, 3)$,\n- Case B: $(n,t) = (12, 4)$,\n- Case C: $(n,t) = (20, 6)$,\n- Case D: $(n,t) = (24, 6)$,\n- Case E: $(n,t) = (24, 3)$.\n\nRequired final output format:\n- Your program must produce a single line of output containing a comma-separated flat list enclosed in square brackets.\n- The list must contain, in order, for each test case, the triple $[r_{\\mathrm{iter}}, r_{\\mathrm{rec}}, s]$ concatenated across all cases into one flat list:\n  - $[r_{\\mathrm{iter}}^{(A)}, r_{\\mathrm{rec}}^{(A)}, s^{(A)}, r_{\\mathrm{iter}}^{(B)}, r_{\\mathrm{rec}}^{(B)}, s^{(B)}, \\dots]$.\n- Express each number as a decimal using scientific or fixed notation with exactly $12$ significant digits.\n- There are no physical units in the answer.\n\nScientific realism and constraints:\n- You must implement $\\mathrm{fl}_t(\\cdot)$ exactly as rounding to $t$ significant decimal digits with rounding-to-nearest ties-to-even.\n- Angles are not involved; no angle units are required.\n- Ensure that the recursive implementation is the naive one described above (no memoization or dynamic programming), so that it recomputes subproblems and thus exhibits its own rounding error propagation characteristics.\n\nThe goal is to empirically compare the robustness and stability of the two methods under the same rounding model. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,...]\").", "solution": "The problem statement has been critically validated and is deemed valid. It is a well-posed problem in the field of numerical analysis, specifically concerning the study of algorithm stability and the propagation of rounding errors. The problem is scientifically grounded, internally consistent, and contains all necessary information to proceed with a solution.\n\nThe core task is to compare the numerical stability of two algorithms for computing the Fibonacci sequence under a specified rounding model. The stability of a numerical algorithm is determined by how it propagates input errors and errors introduced by floating-point arithmetic (rounding errors). A stable algorithm will not unduly magnify these errors.\n\nThe fundamental difference between the two proposed algorithms lies in the number of rounding operations performed.\n1.  The iterative algorithm computes $F_n$ with approximately $n-1$ additions, and thus $n-1$ rounding operations. The number of roundings grows linearly with $n$.\n2.  The naive recursive algorithm, which recomputes subproblems, performs a number of additions (and roundings) equal to $F_{n+1}-1$. This number grows exponentially with $n$, providing a classic example of an inefficient and numerically unstable approach for this problem. The massive number of rounding operations is expected to lead to a much larger accumulation of error compared to the iterative method.\n\nThe implementation will proceed in several steps: defining the rounding model, implementing the three required Fibonacci functions (exact, iterative, recursive), and then calculating the specified error metrics for each test case.\n\n**1. Rounding Model `fl_t(·)`**\n\nThe problem specifies a rounding operator $\\mathrm{fl}_t(\\cdot)$ that rounds a real number to $t$ significant decimal digits using the \"round-to-nearest, ties-to-even\" rule. The most robust way to implement this in Python is to use the `decimal` module from the standard library. This module is designed for decimal floating-point arithmetic and provides precise control over precision and rounding modes.\n\nThe function `fl_t(x, t)` will be implemented as follows:\n- Set the precision of the `decimal` context to $t$.\n- Set the rounding mode of the context to `ROUND_HALF_EVEN`, which corresponds to \"round-to-nearest, ties-to-even\".\n- Convert the input number $x$ (which is a standard binary float) to a `Decimal` object.\n- Apply the context's rounding rules to this `Decimal` object. The unary plus operator (`+`) on a `Decimal` object is a canonical way to do this.\n- Convert the resulting `Decimal` object back to a standard float for use in subsequent calculations.\n\n$$ \\mathrm{fl}_t(x) = \\text{float}(\\text{round}_{\\text{prec}=t, \\text{mode}=\\text{HALF\\_EVEN}}(\\text{Decimal}(x))) $$\n\n**2. Fibonacci Algorithm Implementations**\n\nThree functions are required to compute the Fibonacci numbers.\n\n- **Exact Value $F_n$**: A simple iterative algorithm using Python's arbitrary-precision integers will be used to compute the exact value $F_n$. This serves as the ground truth for error calculations.\n    - $a, b \\leftarrow 0, 1$\n    - For $k$ from $2$ to $n$: $a, b \\leftarrow b, a+b$\n    - Return $b$.\n\n- **Iterative Rounded Value $X_n^{\\mathrm{iter}}$**: This algorithm follows the same iterative structure as the exact one but applies the rounding operator $\\mathrm{fl}_t(\\cdot)$ after each addition.\n    - $x_0 \\leftarrow \\mathrm{fl}_t(0, t)$, $x_1 \\leftarrow \\mathrm{fl}_t(1, t)$\n    - For $k$ from $2$ to $n$:\n        - $s \\leftarrow x_{k-1} + x_{k-2}$\n        - $x_k \\leftarrow \\mathrm{fl}_t(s, t)$\n        - Update stored values: $x_{k-2} \\leftarrow x_{k-1}$, $x_{k-1} \\leftarrow x_k$\n    - Return $x_n$.\n\n- **Naive Recursive Rounded Value $X_n^{\\mathrm{rec}}$**: This algorithm is a direct translation of the mathematical recurrence, with rounding applied at each step. As per the problem statement, it must be \"naive,\" meaning it recomputes values for subproblems, thus exhibiting its characteristic error propagation.\n    - $\\text{fib\\_rec}(k, t)$:\n        - If $k=0$, return $\\mathrm{fl}_t(0, t)$.\n        - If $k=1$, return $\\mathrm{fl}_t(1, t)$.\n        - Otherwise, return $\\mathrm{fl}_t(\\text{fib\\_rec}(k-1, t) + \\text{fib\\_rec}(k-2, t), t)$.\n\n**3. Error Calculation**\n\nFor each test pair $(n, t)$, we compute the following quantities:\n- The exact value $F_n$.\n- The iterative rounded result $X_n^{\\mathrm{iter}}$.\n- The recursive rounded result $X_n^{\\mathrm{rec}}$.\n\nThe relative errors are then calculated. Since the test cases have $n \\ge 1$, the exact value $F_n$ is always non-zero, so division by zero is not a concern.\n- Iterative relative error: $r_{\\mathrm{iter}} = \\dfrac{|X_n^{\\mathrm{iter}} - F_n|}{|F_n|}$\n- Recursive relative error: $r_{\\mathrm{rec}} = \\dfrac{|X_n^{\\mathrm{rec}} - F_n|}{|F_n|}$\n- Difference in errors: $s = r_{\\mathrm{rec}} - r_{\\mathrm{iter}}$\n\nThese three values, for each test case, are formatted to $12$ significant digits using scientific notation and concatenated into a single list for the final output. The format string `\"{:.11e}\"` is used to achieve this, providing $1$ digit before the decimal point and $11$ after, for a total of $12$ significant digits.\n\n**4. Execution and Analysis**\n\nThe main program iterates through the provided test suite. For each $(n, t)$ pair, it invokes the three Fibonacci functions, computes the errors $r_{\\mathrm{iter}}$, $r_{\\mathrm{rec}}$, and $s$, formats the results, and appends them to a list. Finally, it joins the list elements with commas and prints them inside square brackets, adhering to the specified output format. The results are expected to show that $r_{\\mathrm{rec}}$ is significantly larger than $r_{\\mathrm{iter}}$, especially for larger $n$ or smaller $t$, demonstrating the superior numerical stability of the iterative algorithm.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext, ROUND_HALF_EVEN\nimport sys\n\n# Increase recursion limit for larger n in the naive recursive implementation.\nsys.setrecursionlimit(2000)\n\ndef solve():\n    \"\"\"\n    Solves the Fibonacci rounding error comparison problem.\n    \"\"\"\n\n    def fl_t(x: float, t: int) -> float:\n        \"\"\"\n        Rounds a number x to t significant decimal digits using\n        rounding-to-nearest with ties-to-even.\n        \"\"\"\n        if x == 0.0:\n            return 0.0\n        \n        # Set the context for decimal arithmetic\n        getcontext().prec = t\n        getcontext().rounding = ROUND_HALF_EVEN\n        \n        # Convert to Decimal, apply rounding, and convert back to float\n        # The unary '+' applies the precision and rounding from the context.\n        rounded_decimal = +Decimal(x)\n        \n        return float(rounded_decimal)\n\n    def fib_exact(n: int) -> int:\n        \"\"\"\n        Computes the exact n-th Fibonacci number using integer arithmetic.\n        \"\"\"\n        if n <= 1:\n            return n\n        a, b = 0, 1\n        for _ in range(2, n + 1):\n            a, b = b, a + b\n        return b\n\n    def fib_iterative(n: int, t: int) -> float:\n        \"\"\"\n        Computes the n-th Fibonacci number using an iterative approach\n        with rounding after each addition.\n        \"\"\"\n        if n == 0:\n            return fl_t(0.0, t)\n        if n == 1:\n            return fl_t(1.0, t)\n        \n        x0 = fl_t(0.0, t)\n        x1 = fl_t(1.0, t)\n        \n        for _ in range(2, n + 1):\n            s = x1 + x0\n            xk = fl_t(s, t)\n            x0, x1 = x1, xk\n            \n        return x1\n\n    def fib_recursive(n: int, t: int) -> float:\n        \"\"\"\n        Computes the n-th Fibonacci number using a naive recursive approach\n        with rounding after each addition.\n        \"\"\"\n        if n == 0:\n            return fl_t(0.0, t)\n        if n == 1:\n            return fl_t(1.0, t)\n        \n        # Recursive calls are made, and the sum is rounded.\n        fib_nm1 = fib_recursive(n - 1, t)\n        fib_nm2 = fib_recursive(n - 2, t)\n        \n        return fl_t(fib_nm1 + fib_nm2, t)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1, 3),   # Case A\n        (12, 4),  # Case B\n        (20, 6),  # Case C\n        (24, 6),  # Case D\n        (24, 3),  # Case E\n    ]\n\n    results = []\n    for n, t in test_cases:\n        # Calculate the exact value.\n        fn_exact = fib_exact(n)\n        \n        # Calculate the value using the iterative algorithm with rounding.\n        xn_iter = fib_iterative(n, t)\n        \n        # Calculate the value using the naive recursive algorithm with rounding.\n        xn_rec = fib_recursive(n, t)\n        \n        # Calculate relative errors. Denominator fn_exact is > 0 for all n >= 1.\n        if fn_exact != 0:\n            r_iter = abs(xn_iter - fn_exact) / abs(fn_exact)\n            r_rec = abs(xn_rec - fn_exact) / abs(fn_exact)\n        else: # This case is not hit by the provided test suite.\n            r_iter = 0.0 if xn_iter == 0.0 else float('inf')\n            r_rec = 0.0 if xn_rec == 0.0 else float('inf')\n            \n        s = r_rec - r_iter\n        \n        # Format results to 12 significant digits in scientific notation.\n        # Format specifier \".11e\" means 11 digits after the decimal point,\n        # plus one before, totaling 12 significant digits.\n        results.append(f\"{r_iter:.11e}\")\n        results.append(f\"{r_rec:.11e}\")\n        results.append(f\"{s:.11e}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3205171"}, {"introduction": "Beyond algorithmic structure, the mathematical formulation of a calculation is a critical factor in its robustness. This exercise delves into classic numerical hazards, such as catastrophic cancellation in sums and underflow in products, by examining the computation of arithmetic and geometric means for values near machine precision. You will implement and contrast these naive approaches with their robust counterparts—compensated summation and logarithmic transformation—to gain practical skills for taming numerical extremes and preserving accuracy in your computations [@problem_id:3205191].", "problem": "You will investigate the numerical robustness and stability of computing the arithmetic mean and the geometric mean for sets of positive real numbers whose magnitudes are very close to the machine epsilon of double-precision floating-point arithmetic. Use the standard rounding model for floating-point arithmetic as the foundation: for every elementary operation on real numbers $x$ and $y$, the computed floating-point result $\\mathrm{fl}(x \\,\\mathrm{op}\\, y)$ satisfies $\\mathrm{fl}(x \\,\\mathrm{op}\\, y) = (x \\,\\mathrm{op}\\, y)(1 + \\delta)$ with $|\\delta| \\le u$, where $u$ is the unit roundoff (half of machine epsilon). In double precision following the Institute of Electrical and Electronics Engineers (IEEE) $754$ standard, the machine epsilon is approximately $2^{-52}$ and the unit roundoff $u$ is approximately $2^{-53}$. \n\nThe arithmetic mean of positive numbers $x_1,\\dots,x_n$ is defined as $(x_1 + \\cdots + x_n)/n$. The geometric mean is defined as $(x_1 \\cdots x_n)^{1/n}$. You will assess two computational strategies for each mean: a straightforward approach and a numerically robust alternative justified from first principles (for example, by mitigating error accumulation or avoiding underflow via a transformation that reduces dynamic range). You will quantify robustness by computing the relative error of each strategy with respect to a high-accuracy baseline computed using a summation method that substantially reduces rounding error for arithmetic sums and a transformation that avoids underflow for products.\n\nUse the following definitions. If $a_{\\text{approx}}$ is an approximation to a positive reference value $a_{\\text{ref}}$, then the relative error is $|a_{\\text{approx}} - a_{\\text{ref}}| / a_{\\text{ref}}$. For the arithmetic mean baseline, compute the sum using a high-accuracy summation routine provided by the language standard library if available, then divide by $n$. For the geometric mean baseline, compute the sum of natural logarithms using a high-accuracy summation routine, divide by $n$, and then exponentiate.\n\nYour program must implement, for each test case:\n- A straightforward arithmetic mean and a numerically robust arithmetic mean, each compared to the arithmetic baseline via relative error.\n- A straightforward geometric mean and a numerically robust geometric mean, each compared to the geometric baseline via relative error.\n\nAll inputs are pure numbers without physical units. All angles, if any appear, must be in radians, although no angles are required here.\n\nTest suite. Let $\\varepsilon$ denote the machine epsilon of IEEE $754$ double precision. Construct the following four deterministic test cases of positive numbers:\n- Case $1$ (happy path, small $n$): $n = 5$, with $x_i = \\varepsilon\\,(1 + i \\cdot 10^{-12})$ for $i = 0,1,2,3,4$.\n- Case $2$ (product underflow stress): $n = 300$, with $x_i = \\varepsilon$ for $i = 1,2,\\dots,300$.\n- Case $3$ (large $n$, mild heterogeneity): $n = 20000$, with $x_i = \\varepsilon$ for odd $i$ and $x_i = 1.5\\,\\varepsilon$ for even $i$.\n- Case $4$ (very large $n$, tiny deterministic perturbations): $n = 100000$, with $x_i = \\varepsilon\\left(1 + \\left((i \\bmod 10) - 5\\right)\\cdot 10^{-16}\\right)$ for $i = 0,1,2,\\dots,99999$.\n\nAll $x_i$ are strictly positive in these definitions.\n\nFor each case, compute the following four quantities:\n- Relative error of the straightforward arithmetic mean versus its baseline.\n- Relative error of the robust arithmetic mean versus its baseline.\n- Relative error of the straightforward geometric mean versus its baseline.\n- Relative error of the robust geometric mean versus its baseline.\n\nFinal output format. Your program should produce a single line of output that is a comma-separated list enclosed in square brackets, containing one entry per test case, in order. Each entry must itself be a list of four floating-point numbers in the order specified above. For example, the output must have the form\n`[[a11,a12,a13,a14],[a21,a22,a23,a24],[a31,a32,a33,a34],[a41,a42,a43,a44]]`\nwith no additional text before or after this single line.", "solution": "The problem requires an investigation into the numerical robustness of computing the arithmetic and geometric means of sets of positive numbers that are close in magnitude to machine epsilon. The analysis is performed by comparing straightforward computational methods with more robust alternatives, quantifying the performance using relative error against a high-accuracy baseline.\n\nThe foundation of this analysis is the standard model of floating-point arithmetic, where for real numbers $x$ and $y$ and an elementary operation $\\mathrm{op}$, the computed result $\\mathrm{fl}(x \\,\\mathrm{op}\\, y)$ is given by $\\mathrm{fl}(x \\,\\mathrm{op}\\, y) = (x \\,\\mathrm{op}\\, y)(1 + \\delta)$, where $|\\delta| \\le u$. The term $u$ represents the unit roundoff, which for IEEE $754$ double precision is $u \\approx 2^{-53}$. The machine epsilon, $\\varepsilon$, is twice the unit roundoff, $\\varepsilon = 2u \\approx 2^{-52}$.\n\n**I. Arithmetic Mean**\n\nThe arithmetic mean (AM) of a set of $n$ numbers $\\{x_1, x_2, \\dots, x_n\\}$ is defined as $A = \\frac{1}{n} \\sum_{i=1}^{n} x_i$.\n\n**1. Straightforward Algorithm for AM**\n\nThe most direct method is to first compute the sum $S = \\sum_{i=1}^{n} x_i$ using a simple loop and then divide by $n$. In floating-point arithmetic, this sum is computed iteratively as $\\hat{S}_k = \\mathrm{fl}(\\hat{S}_{k-1} + x_k)$, where $\\hat{S}_0 = 0$. Each addition can introduce a rounding error. When summing a large number of values, these small errors can accumulate to a significant total error in the final sum. This is especially problematic when adding numbers of widely different magnitudes, but even with numbers of similar magnitude as in this problem, the accumulated error over a long sequence (e.g., $n=100000$) can degrade accuracy. This method is implemented using `numpy.sum()`.\n\n**2. Robust Algorithm for AM: Kahan Summation**\n\nTo mitigate the accumulation of rounding errors, a compensated summation algorithm, specifically the Kahan summation algorithm, is a robust alternative. This algorithm maintains a running compensation variable, $c$, which captures the low-order bits lost in each addition. The value of $c$ is then incorporated into the next step of the summation, effectively correcting the accumulated sum.\n\nThe Kahan summation algorithm proceeds as follows for a sequence of numbers $X = \\{x_1, \\dots, x_n\\}$:\n1. Initialize sum $s = 0.0$ and compensation $c = 0.0$.\n2. For each $x_i$ in $X$:\n   a. $y = x_i - c$\n   b. $t = s + y$\n   c. $c = (t - s) - y$\n   d. $s = t$\n3. Return $s$.\n\nThe key step is $c = (t - s) - y$. In exact arithmetic, this would be $c = ((s+y)-s) - y = 0$. However, in floating-point arithmetic, $t = \\mathrm{fl}(s+y)$ might lose the low-order bits of $y$ if $s$ is much larger than $y$. The term $(t-s)$ recovers the high-order part of $y$ that was successfully added to $s$, so $(t-s)-y$ isolates the negative of the part of $y$ that was lost. This lost part is stored in $c$ and subtracted from the next term $x_{i+1}$, reintroducing the lost precision into the calculation. The robust mean is then calculated as $A_{\\text{robust}} = \\text{KahanSum}(X) / n$.\n\n**3. Baseline for AM**\n\nThe reference value for the arithmetic mean, $A_{\\text{ref}}$, is computed using a high-accuracy summation routine as specified. Python's `math.fsum()` provides such a routine. It uses an algorithm by Shewchuk that tracks multiple partial sums to maintain high precision, effectively eliminating round-off error for all but the most pathological inputs. The baseline is thus $A_{\\text{ref}} = \\mathrm{math.fsum}(X) / n$.\n\n**II. Geometric Mean**\n\nThe geometric mean (GM) of a set of positive numbers $\\{x_1, x_2, \\dots, x_n\\}$ is defined as $G = (\\prod_{i=1}^{n} x_i)^{1/n}$.\n\n**1. Straightforward Algorithm for GM**\n\nThe direct implementation is to compute the product $P = \\prod_{i=1}^{n} x_i$ and then take the $n$-th root, $G_{\\text{sf}} = P^{1/n}$. This method is highly susceptible to numerical underflow or overflow. The test cases involve numbers $x_i$ on the order of machine epsilon, $\\varepsilon \\approx 2.22 \\times 10^{-16}$. The product of $n$ such numbers is approximately $\\varepsilon^n$. For even moderate $n$, like $n=300$ in Case $2$, this value becomes astronomically small ($\\approx (10^{-16})^{300} = 10^{-4800}$), far below the minimum representable positive double-precision number (which is approximately $5 \\times 10^{-324}$). The computed product underflows to $0.0$, leading to a completely incorrect geometric mean of $0.0$.\n\n**2. Robust Algorithm for GM: Logarithmic Transformation**\n\nA numerically stable approach avoids the large product by using logarithms. Based on the identity $\\ln(G) = \\frac{1}{n}\\sum_{i=1}^{n}\\ln(x_i)$, the geometric mean can be computed as:\n$G_{\\text{robust}} = \\exp\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\ln(x_i)\\right)$.\nThis transformation converts the product into a sum. For $x_i \\approx \\varepsilon$, the values of $\\ln(x_i)$ are moderate negative numbers (around $\\ln(2.22 \\times 10^{-16}) \\approx -36.0$). Summing these values is numerically stable and avoids the underflow/overflow issues inherent in direct multiplication. The sum of logarithms is computed using the standard `numpy.sum()`.\n\n**3. Baseline for GM**\n\nTo create the most accurate reference value, $G_{\\text{ref}}$, we combine the robust logarithmic transformation with the high-accuracy `math.fsum()` for the summation part:\n$G_{\\text{ref}} = \\exp\\left(\\frac{\\mathrm{math.fsum}(\\{\\ln(x_1), \\dots, \\ln(x_n)\\})}{n}\\right)$.\nThis serves as the \"true\" value against which the straightforward and robust methods are compared.\n\n**III. Error Quantification**\n\nThe robustness of each method is quantified by its relative error with respect to the corresponding baseline value. For an approximation $a_{\\text{approx}}$ and a reference $a_{\\text{ref}}$, the relative error is computed as $\\frac{|a_{\\text{approx}} - a_{\\text{ref}}|}{a_{\\text{ref}}}$. Since all $x_i$ are strictly positive, all means will be positive, and the reference values will be non-zero.\n\nThe program will execute this analysis for each of the four specified test cases, which are designed to probe different aspects of numerical stability, including the effects of large $n$ and potential underflow.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef kahan_sum(arr: np.ndarray) -> float:\n    \"\"\"\n    Computes the sum of an array of floats using Kahan's summation algorithm\n    to minimize numerical error.\n    \"\"\"\n    s = 0.0\n    c = 0.0  # A running compensation for lost low-order bits.\n    for x in arr:\n        y = x - c\n        t = s + y\n        # Algebraically, c should be 0.\n        # But in floating point, it recovers the lost low-order part of y.\n        c = (t - s) - y\n        s = t\n    return s\n\ndef relative_error(approx_val: float, ref_val: float) -> float:\n    \"\"\"\n    Computes the relative error of an approximation.\n    \"\"\"\n    if ref_val == 0:\n        # Avoid division by zero. If ref is 0, any non-zero approx is infinite error.\n        # If both are 0, error is 0.\n        return 0.0 if approx_val == 0.0 else np.inf\n    return np.abs(approx_val - ref_val) / np.abs(ref_val)\n\ndef generate_test_cases():\n    \"\"\"\n    Generates the four deterministic test cases as specified in the problem.\n    \"\"\"\n    eps = np.finfo(np.float64).eps\n\n    # Case 1 (happy path, small n)\n    n1 = 5\n    x1 = np.array([eps * (1.0 + i * 1e-12) for i in range(n1)], dtype=np.float64)\n\n    # Case 2 (product underflow stress)\n    n2 = 300\n    x2 = np.full(n2, eps, dtype=np.float64)\n\n    # Case 3 (large n, mild heterogeneity)\n    n3 = 20000\n    x3 = np.full(n3, eps, dtype=np.float64)\n    x3[1::2] = 1.5 * eps  # even indices in 1-based counting\n\n    # Case 4 (very large n, tiny deterministic perturbations)\n    n4 = 100000\n    i = np.arange(n4)\n    perturbations = ((i % 10) - 5) * 1e-16\n    x4 = eps * (1.0 + perturbations)\n\n    return [(n1, x1), (n2, x2), (n3, x3), (n4, x4)]\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n    test_cases = generate_test_cases()\n    all_results = []\n\n    for n, x in test_cases:\n        case_results = []\n\n        # --- Arithmetic Mean Analysis ---\n        # Baseline AM (high-accuracy sum)\n        am_ref = math.fsum(x) / n\n        \n        # Straightforward AM\n        am_sf = np.sum(x) / n\n        \n        # Robust AM (Kahan sum)\n        am_robust = kahan_sum(x) / n\n\n        # Relative errors for AM\n        err_am_sf = relative_error(am_sf, am_ref)\n        err_am_robust = relative_error(am_robust, am_ref)\n        case_results.extend([err_am_sf, err_am_robust])\n\n        # --- Geometric Mean Analysis ---\n        log_x = np.log(x)\n        \n        # Baseline GM (log-transform with high-accuracy sum)\n        gm_ref = np.exp(math.fsum(log_x) / n)\n        \n        # Straightforward GM (direct product)\n        # This is expected to underflow for large n\n        with np.errstate(under='ignore'): # Suppress underflow warnings for clean output\n            gm_sf = np.prod(x)**(1.0/n)\n        \n        # Robust GM (log-transform with standard sum)\n        gm_robust = np.exp(np.sum(log_x) / n)\n\n        # Relative errors for GM\n        err_gm_sf = relative_error(gm_sf, gm_ref)\n        err_gm_robust = relative_error(gm_robust, gm_ref)\n        case_results.extend([err_gm_sf, err_gm_robust])\n        \n        all_results.append(case_results)\n\n    # Format the final output string exactly as specified.\n    results_str_parts = []\n    for row in all_results:\n        # Use repr() for high-precision floating point string representation\n        # which is suitable for this numerical context.\n        row_str = \",\".join(map(repr, row))\n        results_str_parts.append(f\"[{row_str}]\")\n    \n    final_output = f\"[{','.join(results_str_parts)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3205191"}, {"introduction": "In scientific computing, we often rely on complex, \"black-box\" numerical solvers, and it is crucial to understand their limitations. This practice advances from analyzing simple formulas to the sophisticated task of designing a \"stress test\" for an Ordinary Differential Equation (ODE) solver. By using a problem with a known singularity, you will develop a suite of tests to assess a solver's robustness based on multiple criteria, including quantitative accuracy, the preservation of qualitative invariants, and overall stability, a vital skill for validating results in real-world modeling and simulation [@problem_id:3205101].", "problem": "You are asked to design and implement a stability stress test for a black-box Ordinary Differential Equation (ODE) solver within a controlled harness. The goal is to evaluate robustness and stability when integrating an ODE that has a solution singularity whose location depends on the initial condition. Use a mathematically analyzable model to define ground truth and clear pass/fail criteria.\n\nConsider the initial value problem for the scalar ODE\n$$\n\\frac{dy}{dt} = y^2,\\quad y(0)=y_0.\n$$\nStarting from separation of variables and the Fundamental Theorem of Calculus, the exact solution for any nonzero initial condition is\n$$\ny(t) = \\frac{y_0}{1 - y_0 t},\n$$\nwhich exhibits a finite-time blow-up (solution singularity) at the time\n$$\nt^\\star = \\frac{1}{y_0}\\quad \\text{when } y_0 > 0.\n$$\nFor $y_0 < 0$, the blow-up occurs at negative time $t^\\star = 1/y_0$, so no singularity lies in the forward-time interval $[0,T]$ for any $T \\ge 0$; the exact solution is monotone increasing and remains negative for all $t \\ge 0$.\n\nUse these fundamental facts to build a stress test for numerical ODE solvers. The core idea is to integrate only up to a prescribed final time $T$ that is strictly before the singularity if $y_0>0$, that is $T = \\alpha \\, t^\\star$ with $0 < \\alpha < 1$, and to check stability and robustness properties that a reliable solver should satisfy near a singularity.\n\nDefine the following pass/fail criteria for a computed numerical trajectory $\\{y_k\\}_{k=0}^N$ at uniform steps $t_k = k h$ with $N h = T$:\n- Finite arithmetic: every $y_k$ must be a finite floating-point number (no Not-a-Number or infinity).\n- Qualitative invariant: since $\\frac{dy}{dt} = y^2 \\ge 0$ for all $y$, the exact solution is monotone increasing on any interval where it is defined. A robust algorithm should have $y_{k+1} - y_k \\ge -\\tau_{\\text{mono}}$ for all $k$ with a small tolerance $\\tau_{\\text{mono}} > 0$. Moreover, the sign should be preserved on $[0,T]$: if $y_0>0$, require $y_k>0$ for all $k$, and if $y_0<0$, require $y_k<0$ for all $k$.\n- Accuracy at the terminal time: the relative error at time $T$,\n$$\n\\mathrm{RE} = \\frac{\\lvert y_N - y_{\\mathrm{exact}}(T)\\rvert}{\\max(\\lvert y_{\\mathrm{exact}}(T)\\rvert,\\varepsilon)},\n$$\nmust be below a tolerance $\\tau_{\\mathrm{rel}}$, where $y_{\\mathrm{exact}}(t) = \\dfrac{y_0}{1 - y_0 t}$ and $\\varepsilon>0$ is a small regularization constant to prevent division by zero.\n- Backward error proxy at the final step: define the discrete residual at the last step by\n$$\nr = \\frac{y_N - y_{N-1}}{h} - f(t_N, y_N),\\quad f(t,y)=y^2.\n$$\nA robust solution should satisfy the normalized residual\n$$\n\\rho = \\frac{\\lvert r \\rvert}{\\lvert f(t_N, y_N)\\rvert + \\varepsilon}\n$$\nbelow a tolerance $\\tau_{\\mathrm{res}}$.\n\nIn this assignment, treat the solver under test as a black box by wrapping two standard explicit methods as surrogates:\n- Forward Euler (FE), defined by $y_{k+1} = y_k + h f(t_k,y_k)$.\n- Classical fourth-order Runge-Kutta (RK4), defined by taking four stage evaluations per step and combining them in the standard $4$-stage formula.\n\nImplement a program that:\n- Constructs the ODE $f(t,y) = y^2$ and the exact solution $y_{\\mathrm{exact}}(t) = \\dfrac{y_0}{1 - y_0 t}$ for the cases below.\n- For each test case, integrates from $t=0$ to $t=T$ with a fixed stepsize $h$ such that $N = T/h$ is an integer, using both FE and RK4.\n- Applies the pass/fail criteria above with the following fixed tolerances: $\\tau_{\\text{mono}} = 10^{-12}$, $\\tau_{\\mathrm{rel}} = 0.2$, $\\tau_{\\mathrm{res}} = 0.5$, and $\\varepsilon = 10^{-15}$.\n- Reports, for each case and each method, a boolean indicating whether all criteria are satisfied.\n\nUse the following test suite, which targets a range of regimes from benign to near-singularity:\n- Case A (moderately close to blow-up): $y_0 = 1.0$, $\\alpha = 0.90$, $T = \\alpha \\cdot t^\\star = 0.90 \\cdot \\frac{1}{1.0} = 0.9$, $h = 0.01$.\n- Case B (closer to blow-up): $y_0 = 1.0$, $\\alpha = 0.99$, $T = 0.99$, $h = 0.01$.\n- Case C1 (very close to blow-up, coarse step): $y_0 = 4.0$, $\\alpha = 0.99$, $t^\\star = \\frac{1}{4.0} = 0.25$, $T = 0.99 \\cdot 0.25 = 0.2475$, $h = 0.0275$.\n- Case C2 (very close to blow-up, fine step): $y_0 = 4.0$, $\\alpha = 0.99$, $T = 0.2475$, $h = 0.0025$.\n- Case D (far from blow-up): $y_0 = 10^{-3}$, $\\alpha = 0.01$, $t^\\star = 1000.0$, $T = 10.0$, $h = 0.1$.\n- Case E (no forward-time singularity, negative initial condition): $y_0 = -1.0$, $T = 10.0$ (here $\\alpha$ is not used), $h = 0.1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list should contain, in order, the pass booleans for Forward Euler followed by Runge-Kutta for each case: $[\\text{FE-A},\\text{RK4-A},\\text{FE-B},\\text{RK4-B},\\text{FE-C1},\\text{RK4-C1},\\text{FE-C2},\\text{RK4-C2},\\text{FE-D},\\text{RK4-D},\\text{FE-E},\\text{RK4-E}]$. There are no physical units or angles in this problem; all quantities are dimensionless real numbers.", "solution": "The problem requires the design and implementation of a stability and robustness stress test for numerical ODE solvers. The test is based on the scalar initial value problem (IVP) given by the ordinary differential equation (ODE):\n$$\n\\frac{dy}{dt} = f(t,y) = y^2\n$$\nwith the initial condition $y(0) = y_0$. This problem is analytically tractable, providing a solid foundation for defining ground truth and objective performance metrics.\n\nThe exact solution to this IVP is given by:\n$$\ny_{\\mathrm{exact}}(t) = \\frac{y_0}{1 - y_0 t}\n$$\nFor an initial condition $y_0 > 0$, the solution exhibits a finite-time singularity (a \"blow-up\" where the solution tends to infinity) at the time $t^\\star = 1/y_0$. For $y_0 < 0$, the singularity occurs at a negative time, meaning the solution is well-behaved and remains finite for all $t \\ge 0$. This dichotomy provides a natural way to construct test cases that probe a solver's behavior in both benign and near-singular regimes.\n\nThe core of the solution is a testing harness that executes a given numerical solver and evaluates its output against a set of predefined criteria. The solvers to be tested are Forward Euler (FE) and the classical fourth-order Runge-Kutta (RK4) method, treated as black-box components.\n\nThe implementation consists of several key components:\n\n1.  **Problem Definition**: The ODE function $f(t,y) = y^2$ and the exact solution function $y_{\\mathrm{exact}}(t, y_0)$ are implemented directly from their mathematical formulas. These serve as the basis for numerical integration and error calculation.\n\n2.  **Numerical Solvers**:\n    -   **Forward Euler (FE)**: A first-order explicit method defined by the iteration $y_{k+1} = y_k + h f(t_k, y_k)$, where $h$ is the step size, $t_k = k h$, and $y_k \\approx y(t_k)$.\n    -   **Classical Fourth-Order Runge-Kutta (RK4)**: A higher-order explicit method that provides greater accuracy. A single step from $y_k$ to $y_{k+1}$ is computed via four intermediate stages:\n        $$\n        \\begin{align*}\n        k_1 &= f(t_k, y_k) \\\\\n        k_2 &= f(t_k + h/2, y_k + h/2 \\cdot k_1) \\\\\n        k_3 &= f(t_k + h/2, y_k + h/2 \\cdot k_2) \\\\\n        k_4 &= f(t_k + h, y_k + h \\cdot k_3) \\\\\n        y_{k+1} &= y_k + \\frac{h}{6}(k_1 + 2k_2 + 2k_3 + k_4)\n        \\end{align*}\n        $$\n    Both solvers integrate the ODE from $t_0=0$ to a final time $T$ using a fixed step size $h$, generating a numerical trajectory $\\{y_k\\}_{k=0}^N$.\n\n3.  **Test Harness and Pass/Fail Criteria**: A function is designed to orchestrate the test for a given solver and set of case parameters $(y_0, T, h)$. This function performs the integration and then sequentially applies four checks. A test is considered passed only if all criteria are met.\n\n    -   **Criterion 1: Finite Arithmetic**: The most basic check for robustness is that the solver does not produce non-physical values. Every point $y_k$ in the computed trajectory must be a finite floating-point number (i.e., not `infinity` or `Not-a-Number`). This guards against catastrophic failure, such as numerical overflow when approaching the singularity.\n\n    -   **Criterion 2: Qualitative Invariants**: A robust solver should preserve fundamental qualitative properties of the exact solution. For this ODE, since $\\frac{dy}{dt} = y^2 \\ge 0$, the solution is non-decreasing. We check this with a small tolerance: $y_{k+1} - y_k \\ge -\\tau_{\\text{mono}}$ for all $k$, where $\\tau_{\\text{mono}} = 10^{-12}$. Additionally, the sign of the solution should not change. If $y_0 > 0$, then $y(t) > 0$ for all $t < t^\\star$. If $y_0 < 0$, then $y(t) < 0$ for all $t \\ge 0$. The test verifies that $\\text{sign}(y_k) = \\text{sign}(y_0)$ for all $k$.\n\n    -   **Criterion 3: Accuracy at Terminal Time**: This criterion assesses the quantitative correctness of the numerical solution. The relative error (RE) at the final time $T$ is computed as:\n        $$\n        \\mathrm{RE} = \\frac{\\lvert y_N - y_{\\mathrm{exact}}(T)\\rvert}{\\max(\\lvert y_{\\mathrm{exact}}(T)\\rvert, \\varepsilon)}\n        $$\n        where $y_N$ is the computed solution at $T$, $y_{\\mathrm{exact}}(T)$ is the true value, and $\\varepsilon = 10^{-15}$ is a small constant to prevent division by zero when the exact solution is close to zero. The test passes if $\\mathrm{RE} < \\tau_{\\mathrm{rel}}$, with $\\tau_{\\mathrm{rel}} = 0.2$.\n\n    -   **Criterion 4: Backward Error Proxy**: This check evaluates how well the computed solution satisfies the ODE at the final step. The discrete residual $r$ is defined using a backward difference approximation for the derivative:\n        $$\n        r = \\frac{y_N - y_{N-1}}{h} - f(t_N, y_N)\n        $$\n        A small residual indicates that the final segment of the trajectory is consistent with the dynamics of the ODE. We check if the normalized residual $\\rho$ is below a tolerance:\n        $$\n        \\rho = \\frac{\\lvert r \\rvert}{\\lvert f(t_N, y_N)\\rvert + \\varepsilon} < \\tau_{\\mathrm{res}}\n        $$\n        with $\\tau_{\\mathrm{res}} = 0.5$.\n\nFinally, a main loop iterates through the specified test cases (A, B, C1, C2, D, E). For each case, it runs both the FE and RK4 solvers through the test harness and records the boolean pass/fail outcome. The results are collected and formatted into the specified string output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to design, implement, and run the stability stress test\n    for numerical ODE solvers.\n    \"\"\"\n    \n    # Define constants, tolerances, and problem-specific functions\n    F_ODE = lambda t, y: y**2\n    EXACT_SOL = lambda t, y0: y0 / (1 - y0 * t) if (1 - y0 * t) != 0 else np.inf\n\n    TAU_MONO = 1e-12\n    TAU_REL = 0.2\n    TAU_RES = 0.5\n    EPSILON = 1e-15\n\n    def solver_fe(f, y0, t_span, h):\n        \"\"\"\n        Integrates an ODE using the Forward Euler method.\n        \"\"\"\n        t0, T = t_span\n        # Use np.round to mitigate floating point inaccuracies in T/h\n        num_steps = int(np.round(T / h))\n        t_values = np.linspace(t0, T, num_steps + 1)\n        y_values = np.zeros(num_steps + 1, dtype=np.float64)\n        y_values[0] = y0\n        \n        for k in range(num_steps):\n            try:\n                y_values[k+1] = y_values[k] + h * f(t_values[k], y_values[k])\n            except OverflowError:\n                y_values[k+1:] = np.inf\n                break\n        return t_values, y_values\n\n    def solver_rk4(f, y0, t_span, h):\n        \"\"\"\n        Integrates an ODE using the classical 4th-order Runge-Kutta method.\n        \"\"\"\n        t0, T = t_span\n        num_steps = int(np.round(T / h))\n        t_values = np.linspace(t0, T, num_steps + 1)\n        y_values = np.zeros(num_steps + 1, dtype=np.float64)\n        y_values[0] = y0\n        \n        for k in range(num_steps):\n            t_k = t_values[k]\n            y_k = y_values[k]\n            try:\n                k1 = f(t_k, y_k)\n                k2 = f(t_k + h / 2, y_k + h / 2 * k1)\n                k3 = f(t_k + h / 2, y_k + h / 2 * k2)\n                k4 = f(t_k + h, y_k + h * k3)\n                y_values[k+1] = y_k + h / 6 * (k1 + 2*k2 + 2*k3 + k4)\n            except OverflowError:\n                y_values[k+1:] = np.inf\n                break\n        return t_values, y_values\n\n    def run_test_harness(solver, y0, T, h):\n        \"\"\"\n        Runs a single test case for a given solver and evaluates against all criteria.\n        Returns a single boolean indicating pass or fail.\n        \"\"\"\n        t_span = (0, T)\n        t_values, y_values = solver(F_ODE, y0, t_span, h)\n        \n        # Criterion 1: Finite arithmetic\n        if not np.all(np.isfinite(y_values)):\n            return False\n            \n        # Criterion 2: Qualitative invariants\n        # Monotonicity\n        if not np.all(np.diff(y_values) >= -TAU_MONO):\n            return False\n        # Sign preservation\n        if y0 > 0:\n            if not np.all(y_values > 0):\n                return False\n        elif y0 < 0:\n            if not np.all(y_values < 0):\n                return False\n                \n        # Criterion 3: Accuracy at terminal time\n        y_N = y_values[-1]\n        y_exact_T = EXACT_SOL(T, y0)\n        rel_err = np.abs(y_N - y_exact_T) / np.maximum(np.abs(y_exact_T), EPSILON)\n        if rel_err >= TAU_REL:\n            return False\n            \n        # Criterion 4: Backward error proxy\n        y_N_minus_1 = y_values[-2]\n        t_N = t_values[-1]\n        f_at_yN = F_ODE(t_N, y_N)\n        residual = (y_N - y_N_minus_1) / h - f_at_yN\n        norm_residual = np.abs(residual) / (np.abs(f_at_yN) + EPSILON)\n        if norm_residual >= TAU_RES:\n            return False\n            \n        # All criteria passed\n        return True\n\n    # Define the test suite from the problem statement.\n    test_cases = [\n        {'y0': 1.0, 'T': 0.9, 'h': 0.01},   # Case A\n        {'y0': 1.0, 'T': 0.99, 'h': 0.01},  # Case B\n        {'y0': 4.0, 'T': 0.2475, 'h': 0.0275}, # Case C1\n        {'y0': 4.0, 'T': 0.2475, 'h': 0.0025}, # Case C2\n        {'y0': 1e-3, 'T': 10.0, 'h': 0.1},  # Case D\n        {'y0': -1.0, 'T': 10.0, 'h': 0.1},  # Case E\n    ]\n\n    solvers_to_test = [solver_fe, solver_rk4]\n    results = []\n\n    for case in test_cases:\n        for solver in solvers_to_test:\n            result = run_test_harness(solver, case['y0'], case['T'], case['h'])\n            results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3205101"}]}