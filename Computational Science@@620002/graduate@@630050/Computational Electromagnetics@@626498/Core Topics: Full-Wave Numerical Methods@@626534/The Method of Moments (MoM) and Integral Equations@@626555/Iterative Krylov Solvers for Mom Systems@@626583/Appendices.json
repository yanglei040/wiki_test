{"hands_on_practices": [{"introduction": "When solving a linear system iteratively, our true goal is to reduce the error in the solution, but we can only directly measure the residual. This fundamental practice guides you through the derivation that connects these two quantities via the matrix condition number. Mastering this relationship is essential for setting a meaningful convergence tolerance $\\tau$ that guarantees your final computed solution meets a desired accuracy target $\\delta$ [@problem_id:3321315].", "problem": "Consider the Electric Field Integral Equation (EFIE) for a perfectly electrically conducting (PEC) scatterer discretized by the Method of Moments (MoM). Using Rao-Wilton-Glisson (RWG) basis functions with Galerkin testing, the discretization yields a linear system $A x = b$ in the complex field, where $A \\in \\mathbb{C}^{n \\times n}$ is the dense MoM system matrix, $x \\in \\mathbb{C}^{n}$ contains the expansion coefficients of the surface current density, and $b \\in \\mathbb{C}^{n}$ is the excitation vector formed from the incident field. An iterative Krylov method such as Generalized Minimal Residual (GMRES) is employed to compute an approximation $x_{k}$, with residual $r_{k} = b - A x_{k}$, and a stopping criterion based on the relative residual $\\|r_{k}\\|_{2}/\\|b\\|_{2} \\leq \\tau$.\n\nStarting from first principles—namely, the definitions $r_{k} = b - A x_{k}$, $e_{k} = x - x_{k}$, $x = A^{-1} b$, and the $2$-norm condition number $\\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2}$—derive a conservative tolerance $\\tau$ on the relative residual that guarantees a prescribed upper bound on the relative error in the surface current, $\\|x - x_{k}\\|_{2} / \\|x\\|_{2} \\leq \\delta$. Then evaluate this tolerance for the case $\\kappa_{2}(A) = 350$ and $\\delta = 5 \\times 10^{-3}$.\n\nExpress your final tolerance as a dimensionless number and round your answer to three significant figures.", "solution": "The objective is to derive a tolerance $\\tau$ for the relative residual, $\\|r_{k}\\|_{2}/\\|b\\|_{2} \\le \\tau$, that guarantees the relative error in the surface current, $\\|e_{k}\\|_{2}/\\|x\\|_{2} \\le \\delta$, does not exceed a prescribed value $\\delta$. The derivation will start from the fundamental definitions provided.\n\nThe key quantities are defined as:\n1.  The linear system: $A x = b$, where $x = A^{-1} b$ is the exact solution.\n2.  The error in the $k$-th iteration: $e_{k} = x - x_{k}$.\n3.  The residual in the $k$-th iteration: $r_{k} = b - A x_{k}$.\n\nOur first step is to establish a direct relationship between the error vector $e_{k}$ and the residual vector $r_{k}$. We substitute the expression $b = A x$ into the definition of the residual:\n$$r_{k} = A x - A x_{k}$$\nBy the distributive property of matrix multiplication, we can factor out the matrix $A$:\n$$r_{k} = A (x - x_{k})$$\nRecognizing the definition of the error vector $e_{k}$, we have:\n$$r_{k} = A e_{k}$$\nAssuming the matrix $A$ is invertible (a necessary condition for a unique solution $x$ to exist), we can express the error vector in terms of the residual vector by multiplying by the inverse of $A$:\n$$e_{k} = A^{-1} r_{k}$$\nThis equation links the error, which is an unknown quantity during the iteration, to the residual, which is a computable quantity.\n\nNext, we move to the vector norms to establish inequalities. The problem specifies the Euclidean $2$-norm, denoted by $\\|\\cdot\\|_{2}$. Taking the $2$-norm of both sides of the equation $e_{k} = A^{-1} r_{k}$ and applying the property of sub-multiplicative matrix norms, which states that $\\|Mv\\|_{2} \\le \\|M\\|_{2}\\|v\\|_{2}$ for any matrix $M$ and vector $v$, we obtain an upper bound on the norm of the error:\n$$\\|e_{k}\\|_{2} = \\|A^{-1} r_{k}\\|_{2} \\le \\|A^{-1}\\|_{2} \\|r_{k}\\|_{2}$$\n\nThe goal is to bound the relative error, $\\|e_{k}\\|_{2} / \\|x\\|_{2}$. To do this, we need a lower bound on the norm of the exact solution, $\\|x\\|_{2}$. We start with the original system equation, $A x = b$, and take the $2$-norm of both sides:\n$$\\|b\\|_{2} = \\|A x\\|_{2}$$\nApplying the same sub-multiplicative property, we get:\n$$\\|b\\|_{2} \\le \\|A\\|_{2} \\|x\\|_{2}$$\nAssuming the excitation vector $b$ is non-zero (which is true for any non-trivial scattering problem), we can divide by $\\|A\\|_{2}$ to establish a lower bound on $\\|x\\|_{2}$:\n$$\\|x\\|_{2} \\ge \\frac{\\|b\\|_{2}}{\\|A\\|_{2}}$$\nTaking the reciprocal of this inequality reverses the direction of the inequality, providing an upper bound for $1/\\|x\\|_{2}$:\n$$\\frac{1}{\\|x\\|_{2}} \\le \\frac{\\|A\\|_{2}}{\\|b\\|_{2}}$$\n\nNow we can combine the bound on $\\|e_{k}\\|_{2}$ and the bound on $1/\\|x\\|_{2}$ to form an upper bound on the relative error:\n$$\\frac{\\|e_{k}\\|_{2}}{\\|x\\|_{2}} = \\|e_{k}\\|_{2} \\cdot \\frac{1}{\\|x\\|_{2}} \\le \\left( \\|A^{-1}\\|_{2} \\|r_{k}\\|_{2} \\right) \\cdot \\left( \\frac{\\|A\\|_{2}}{\\|b\\|_{2}} \\right)$$\nRearranging the terms on the right-hand side yields:\n$$\\frac{\\|e_{k}\\|_{2}}{\\|x\\|_{2}} \\le \\left( \\|A\\|_{2} \\|A^{-1}\\|_{2} \\right) \\left( \\frac{\\|r_{k}\\|_{2}}{\\|b\\|_{2}} \\right)$$\nThe term $\\|A\\|_{2} \\|A^{-1}\\|_{2}$ is the definition of the $2$-norm condition number of the matrix $A$, denoted by $\\kappa_{2}(A)$. Substituting this definition gives the fundamental inequality relating the relative error to the relative residual:\n$$\\frac{\\|e_{k}\\|_{2}}{\\|x\\|_{2}} \\le \\kappa_{2}(A) \\frac{\\|r_{k}\\|_{2}}{\\|b\\|_{2}}$$\nThis inequality shows that the relative error is bounded by the relative residual, magnified by the condition number of the system matrix. This provides a worst-case estimate.\n\nThe problem requires that we guarantee the relative error is no more than a specified value $\\delta$:\n$$\\frac{\\|e_{k}\\|_{2}}{\\|x\\|_{2}} \\le \\delta$$\nThe iterative solver's stopping criterion is based on the relative residual, $\\|r_{k}\\|_{2}/\\|b\\|_{2} \\le \\tau$. To satisfy the condition on the error, we must set the tolerance $\\tau$ such that the upper bound on the relative error is less than or equal to $\\delta$:\n$$\\kappa_{2}(A) \\frac{\\|r_{k}\\|_{2}}{\\|b\\|_{2}} \\le \\kappa_{2}(A) \\tau \\le \\delta$$\nTo find the required tolerance $\\tau$, we solve the inequality for $\\tau$:\n$$\\kappa_{2}(A) \\tau \\le \\delta \\implies \\tau \\le \\frac{\\delta}{\\kappa_{2}(A)}$$\nA conservative tolerance is one that guarantees the condition. The largest possible value of $\\tau$ that provides this guarantee is given by the equality:\n$$\\tau = \\frac{\\delta}{\\kappa_{2}(A)}$$\nAny smaller value for $\\tau$ would also satisfy the guarantee but would be unnecessarily strict and potentially lead to more iterations than required.\n\nFinally, we evaluate this expression for the given numerical values: $\\kappa_{2}(A) = 350$ and $\\delta = 5 \\times 10^{-3}$.\n$$\\tau = \\frac{5 \\times 10^{-3}}{350}$$\n$$\\tau = \\frac{5}{350} \\times 10^{-3} = \\frac{1}{70} \\times 10^{-3}$$\nConverting the fraction $1/70$ to a decimal, we have $1/70 \\approx 0.0142857...$.\n$$\\tau \\approx 0.0142857 \\times 10^{-3} = 1.42857 \\times 10^{-5}$$\nThe problem asks for the result to be rounded to three significant figures.\n$$\\tau \\approx 1.43 \\times 10^{-5}$$\nThis is the required conservative tolerance on the relative residual.", "answer": "$$\\boxed{1.43 \\times 10^{-5}}$$", "id": "3321315"}, {"introduction": "The total cost of an iterative solution depends on both the number of iterations and the cost per iteration. This exercise moves beyond abstract complexity to a concrete accounting of computational resources for the workhorse GMRES algorithm. By quantifying the memory, floating-point operations, and communication costs, you will gain a crucial understanding of the performance bottlenecks that govern large-scale MoM simulations on parallel computing hardware [@problem_id:3321331].", "problem": "Consider a Method of Moments (MoM) discretization of a frequency-domain electromagnetic boundary integral equation that yields a linear system $A x = b$ of dimension $N = 10^{6}$ with complex-valued unknowns. You plan to solve this system using the restarted Generalized Minimal Residual method (GMRES) with restart parameter $m = 80$, implemented via the Arnoldi process and classical Gram–Schmidt orthogonalization, with full reorthogonalization enabled to guard against loss of orthogonality.\n\nAssume the following modeling and accounting conventions, which reflect typical high-performance implementations for complex double-precision arithmetic:\n- Each complex number is stored in double precision and occupies $16$ bytes of memory.\n- A complex inner product of two length-$N$ vectors is counted as approximately $8N$ real floating-point operations (flops). This models $6N$ flops for the elementwise complex multiplications and approximately $2N$ flops for the complex additions in the accumulation (neglecting $O(1)$ terms).\n- A scaled complex axpy of the form $y \\leftarrow y - \\alpha x$ on length-$N$ vectors with complex scalar $\\alpha$ is counted as $8N$ flops ($6N$ for the complex multiplications and $2N$ for the complex additions).\n- The restarted GMRES($m$) stores the $(m+1)$-vector Krylov basis generated by Arnoldi, the $(m+1)\\times m$ upper Hessenberg matrix, and exactly three additional length-$N$ complex work vectors used for residuals, preconditioned residuals, and temporary products.\n- For parallel execution across $P$ processes, each complex inner product induces one global reduction; full reorthogonalization doubles the number of inner products relative to a single-pass classical Gram–Schmidt.\n\nUsing only these assumptions and the algorithmic structure of GMRES($m$) with Arnoldi:\n1. Derive and compute the total memory footprint (in gibibytes, GiB; where $1\\,\\mathrm{GiB} = 2^{30}$ bytes) needed to store the Krylov basis, the upper Hessenberg matrix, and the three additional work vectors for $N = 10^{6}$ and $m = 80$.\n2. Derive and compute the additional floating-point operation count per restart cycle that full reorthogonalization imposes beyond a single-pass classical Gram–Schmidt, for $N = 10^{6}$ and $m = 80$.\n3. Determine the additional number of global dot-product reductions per restart cycle caused by full reorthogonalization for $m = 80$.\n\nDiscuss how the computed quantities influence cache behavior (e.g., streaming through vectors that exceed last-level cache) and interprocess communication (e.g., synchronization due to global reductions), grounding your discussion in the derived counts.\n\nAs your final numerical output, report a single row vector $\\begin{pmatrix} M_{\\mathrm{GiB}} & F_{\\mathrm{add}} & R_{\\mathrm{add}} \\end{pmatrix}$, where:\n- $M_{\\mathrm{GiB}}$ is the total memory footprint in GiB, rounded to three significant figures,\n- $F_{\\mathrm{add}}$ is the additional floating-point operations per restart cycle due to full reorthogonalization, rounded to three significant figures,\n- $R_{\\mathrm{add}}$ is the additional number of global dot-product reductions per restart cycle, reported as an exact integer.", "solution": "The problem requires the calculation of the memory footprint, additional floating-point operations (flops), and additional communication events for the restarted Generalized Minimal Residual method (GMRES($m$)) with full reorthogonalization. The calculations are based on the provided parameters and cost models.\n\nFirst, we establish the parameters given in the problem statement:\n- The dimension of the linear system is $N = 10^{6}$.\n- The GMRES restart parameter is $m = 80$.\n- Each complex number occupies $16$ bytes.\n- $1\\,\\mathrm{GiB} = 2^{30}$ bytes.\n- A complex inner product of length-$N$ vectors costs $8N$ flops.\n- A complex axpy operation of length-$N$ vectors costs $8N$ flops.\n\n### 1. Total Memory Footprint ($M$)\n\nThe total memory footprint is the sum of the storage required for the Krylov basis, the Hessenberg matrix, and the specified work vectors.\n\n- **Krylov basis**: The Arnoldi process generates an orthonormal basis $\\{q_1, q_2, \\ldots, q_{m+1}\\}$ for the Krylov subspace $\\mathcal{K}_{m+1}(A, r_0)$. This requires storing $m+1$ vectors, each of length $N$. The number of complex values is $(m+1)N$.\n- **Hessenberg matrix**: The Arnoldi process produces an $(m+1) \\times m$ upper Hessenberg matrix, $H_m$. The number of complex values is $(m+1)m$.\n- **Work vectors**: The problem states that $3$ additional length-$N$ complex work vectors are used. The number of complex values is $3N$.\n\nThe total number of complex values to be stored, $C_{\\mathrm{total}}$, is the sum of these components:\n$$\nC_{\\mathrm{total}} = (m+1)N + 3N + (m+1)m = (m+4)N + (m+1)m\n$$\nThe total memory footprint in bytes, $M_{\\mathrm{bytes}}$, is $C_{\\mathrm{total}}$ multiplied by the size of a complex number ($16$ bytes):\n$$\nM_{\\mathrm{bytes}} = 16 \\times \\left( (m+4)N + (m+1)m \\right)\n$$\nTo convert this to gibibytes (GiB), we divide by $2^{30}$.\n$$\nM_{\\mathrm{GiB}} = \\frac{16 \\times \\left( (m+4)N + (m+1)m \\right)}{2^{30}}\n$$\nSubstituting the given values $N = 10^{6}$ and $m = 80$:\n$$\nM_{\\mathrm{GiB}} = \\frac{16 \\times \\left( (80+4) \\times 10^{6} + (80+1) \\times 80 \\right)}{2^{30}}\n$$\n$$\nM_{\\mathrm{GiB}} = \\frac{16 \\times \\left( 84 \\times 10^{6} + 81 \\times 80 \\right)}{2^{30}} = \\frac{16 \\times \\left( 84,000,000 + 6480 \\right)}{1,073,741,824}\n$$\n$$\nM_{\\mathrm{GiB}} = \\frac{16 \\times 84,006,480}{1,073,741,824} = \\frac{1,344,103,680}{1,073,741,824} \\approx 1.25181 \\, \\mathrm{GiB}\n$$\nRounding to three significant figures, the total memory footprint is $M_{\\mathrm{GiB}} = 1.25 \\, \\mathrm{GiB}$. The storage for the Hessenberg matrix ($16 \\times 6480 \\approx 104$ kB) is negligible compared to the storage for the vectors.\n\n### 2. Additional Floating-Point Operations per Restart Cycle ($F_{\\mathrm{add}}$)\n\nThe orthogonalization in the Arnoldi process is performed using classical Gram–Schmidt (CGS). At step $j$ of the Arnoldi iteration (for $j=1, \\ldots, m$), the new vector $v = A q_j$ is orthogonalized against the existing basis vectors $\\{q_1, \\ldots, q_j\\}$. A single pass of CGS involves $j$ inner products ($h_{i,j} = q_i^H v$) and $j$ axpy operations ($v \\leftarrow v - h_{i,j}q_i$).\n\nFull reorthogonalization implies performing the Gram-Schmidt process a second time to correct for loss of orthogonality. This second pass also involves $j$ inner products and $j$ axpy operations. Therefore, the *additional* work at step $j$ from reorthogonalization consists of $j$ inner products and $j$ axpy operations.\n\nOver one full restart cycle ($m$ steps of Arnoldi), the total number of additional inner products is the sum over $j$ from $1$ to $m$:\n$$\nN_{\\mathrm{ip, add}} = \\sum_{j=1}^{m} j = \\frac{m(m+1)}{2}\n$$\nSimilarly, the total number of additional axpy operations is:\n$$\nN_{\\mathrm{axpy, add}} = \\sum_{j=1}^{m} j = \\frac{m(m+1)}{2}\n$$\nThe cost of one inner product is given as $8N$ flops, and the cost of one axpy is also $8N$ flops. The total additional flop count, $F_{\\mathrm{add}}$, is:\n$$\nF_{\\mathrm{add}} = N_{\\mathrm{ip, add}} \\times (8N) + N_{\\mathrm{axpy, add}} \\times (8N)\n$$\n$$\nF_{\\mathrm{add}} = \\frac{m(m+1)}{2} \\times 8N + \\frac{m(m+1)}{2} \\times 8N = m(m+1) \\times 8N\n$$\nSubstituting $N = 10^6$ and $m = 80$:\n$$\nF_{\\mathrm{add}} = 80 \\times (80+1) \\times 8 \\times 10^{6} = 80 \\times 81 \\times 8 \\times 10^{6}\n$$\n$$\nF_{\\mathrm{add}} = 6480 \\times 8 \\times 10^{6} = 51840 \\times 10^{6} = 5.184 \\times 10^{10}\n$$\nRounding to three significant figures, the additional flop count is $F_{\\mathrm{add}} = 5.18 \\times 10^{10}$ flops.\n\n### 3. Additional Global Reductions per Restart Cycle ($R_{\\mathrm{add}}$)\n\nThe problem states that each complex inner product induces one global reduction. Full reorthogonalization adds a second set of inner products, thus adding a corresponding number of global reductions. The number of additional global reductions, $R_{\\mathrm{add}}$, is equal to the number of additional inner products calculated previously.\n$$\nR_{\\mathrm{add}} = N_{\\mathrm{ip, add}} = \\frac{m(m+1)}{2}\n$$\nSubstituting $m = 80$:\n$$\nR_{\\mathrm{add}} = \\frac{80 \\times (80+1)}{2} = 40 \\times 81 = 3240\n$$\nThis is an exact integer value.\n\n### Discussion of Influence on Performance\n\n-   **Cache Behavior**: The total memory footprint of $M_{\\mathrm{GiB}} \\approx 1.25 \\, \\mathrm{GiB}$ is far larger than typical last-level caches (LLCs) on modern CPUs, which are on the order of tens of megabytes. A single length-$N$ vector requires $10^6 \\times 16 \\, \\text{bytes} = 16 \\, \\text{MB}$ of storage. Operations central to GMRES, such as inner products and axpy updates, involve streaming through one or more of these large vectors. For instance, in the orthogonalization step, the algorithm repeatedly accesses the basis vectors $\\{ q_1, \\ldots, q_j \\}$. As the basis size $j$ grows, the set of vectors to be accessed, $j \\times 16 \\, \\text{MB}$, quickly exceeds any on-chip cache. Consequently, these vector operations are memory-bandwidth bound; the processor spends a significant fraction of its time waiting for data to be fetched from main DRAM rather than performing computations. The high flop count, including the additional $F_{\\mathrm{add}} \\approx 5.18 \\times 10^{10}$ flops, often cannot be sustained because the arithmetic units are starved of data.\n\n-   **Interprocess Communication**: In a parallel implementation across $P$ processes, global reductions are synchronization points that require all processes to communicate. The additional $R_{\\mathrm{add}} = 3240$ global reductions per restart cycle, caused by full reorthogonalization, represent a significant communication overhead. Each of these $3240$ operations (e.g., an `MPI_Allreduce` call) introduces latency, as processes must wait for the slowest participant and for the data to travel across the network. The total wall-clock time becomes highly sensitive to network latency and bandwidth. For massively parallel systems where $P$ is large, the cost of these reductions can dominate the overall runtime, eclipsing the cost of floating-point arithmetic. This illustrates a fundamental trade-off in scalable numerical algorithms: the need for numerical robustness (achieved here via reorthogonalization) often comes at the cost of increased communication and synchronization, which can limit parallel scalability.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1.25 & 5.18 \\times 10^{10} & 3240 \\end{pmatrix}}\n$$", "id": "3321331"}, {"introduction": "A common challenge in the Method of Moments is that refining the mesh to improve accuracy can severely increase the condition number of the system matrix, leading to a dramatic rise in iteration counts. This coding exercise allows you to model and quantify this \"dense-discretization breakdown\" and then demonstrate the transformative effect of a good preconditioner. By implementing a standard convergence model, you will see firsthand how preconditioning makes solving finely detailed problems computationally feasible [@problem_id:3321343].", "problem": "Consider a fixed-frequency electromagnetic scattering problem discretized by the Method of Moments (MoM) using Rao–Wilton–Glisson basis functions, leading to a linear system $A(h)\\,x=b$ with mesh parameter $h$ equal to the average edge length normalized by the wavelength (dimensionless). Assume the following empirically validated facts form the fundamental base for the derivation.\n\n- At fixed frequency (no low-frequency scaling), the Electric Field Integral Equation (EFIE) matrix $A(h)$ exhibits dense-discretization breakdown: its smallest singular value scales like $O(h^{p_{\\alpha}})$ and its largest singular value scales like $O(h^{-p_{\\beta}})$, so that the condition number $\\kappa(h)$ scales as $O\\!\\left(h^{-(p_{\\alpha}+p_{\\beta})}\\right)$. A common case for EFIE is $p_{\\alpha}=1$ and $p_{\\beta}=0$.\n- The Generalized Minimal Residual (GMRES) method minimizes the residual over polynomials constrained by $p(0)=1$. When the field of values or spectrum of the coefficient matrix is contained in a real interval $[\\alpha,\\beta]$ on the positive real axis, the extremal properties of Chebyshev polynomials yield an optimal polynomial approximation bound on the residual norm.\n- A quasi-Helmholtz preconditioner $M$ (e.g., loop-star or loop-tree decomposition with appropriate scaling) transforms the system to $M^{-1}A(h)$ and suppresses dense-discretization breakdown, clustering the spectrum in an $h$-independent interval $[\\underline{\\alpha},\\overline{\\beta}]$ on the positive real axis. Consequently, the condition number becomes essentially independent of $h$.\n\nYour task is to predict how the GMRES iteration count changes when the mesh is refined by a factor of $2$ (that is, $h \\mapsto h/2$), both without preconditioning and with a quasi-Helmholtz preconditioner. Work in purely mathematical terms, with $h$ dimensionless. You must implement the following modeling assumptions in your program.\n\n- Model the unpreconditioned spectrum of $A(h)$ as contained in $[\\alpha(h),\\beta(h)]$ with $\\alpha(h)=c_{\\alpha}\\,h^{p_{\\alpha}}$ and $\\beta(h)=c_{\\beta}\\,h^{-p_{\\beta}}$, where $c_{\\alpha}>0$, $c_{\\beta}>0$, $p_{\\alpha}\\ge 0$, and $p_{\\beta}\\ge 0$. The resulting condition number is $\\kappa(h)=\\dfrac{\\beta(h)}{\\alpha(h)}=\\dfrac{c_{\\beta}}{c_{\\alpha}}\\,h^{-(p_{\\alpha}+p_{\\beta})}$.\n- Model the preconditioned spectrum of $M^{-1}A(h)$ as $[\\underline{\\alpha},\\overline{\\beta}]$ with $\\underline{\\alpha}>0$ and $\\overline{\\beta}>0$ independent of $h$, yielding a condition number $\\kappa_{p}=\\dfrac{\\overline{\\beta}}{\\underline{\\alpha}}$ independent of $h$.\n- Use the classical Chebyshev optimal-polynomial residual bound on a real positive interval to upper bound the GMRES residual decay. For a given tolerance $\\varepsilon\\in(0,1)$ and condition number $\\kappa\\ge 1$, define the predicted iteration count $m(\\kappa,\\varepsilon)$ to be the smallest nonnegative integer satisfying the Chebyshev-based bound for the minimax polynomial on $[\\alpha,\\beta]$. Implement this using the standard closed-form that follows from the extremal property of Chebyshev polynomials on an interval mapped to $[-1,1]$.\n\nYou must compute, for each test case below, the change in the predicted GMRES iterations when refining $h$ by a factor of $2$, that is, $\\Delta m_{\\mathrm{unprec}}=m(\\kappa(h/2),\\varepsilon)-m(\\kappa(h),\\varepsilon)$ for the unpreconditioned case, and $\\Delta m_{\\mathrm{prec}}=m(\\kappa_{p},\\varepsilon)-m(\\kappa_{p},\\varepsilon)$ for the preconditioned case (which should be $0$ if $\\kappa_{p}$ is strictly $h$-independent).\n\nImplement a program that evaluates the following test suite. In each test case, all parameters are dimensionless. The mesh refinement is always by a factor of $2$.\n\n- Test case $1$ (happy path, moderate refinement impact):\n  - $h_{0}=0.1$, $c_{\\alpha}=0.8$, $p_{\\alpha}=1$, $c_{\\beta}=2.4$, $p_{\\beta}=0$, $\\underline{\\alpha}=0.5$, $\\overline{\\beta}=2.5$, $\\varepsilon=10^{-6}$.\n- Test case $2$ (coarser initial mesh, smaller impact):\n  - $h_{0}=0.4$, $c_{\\alpha}=0.8$, $p_{\\alpha}=1$, $c_{\\beta}=2.4$, $p_{\\beta}=0$, $\\underline{\\alpha}=0.5$, $\\overline{\\beta}=2.5$, $\\varepsilon=10^{-6}$.\n- Test case $3$ (tight tolerance edge case):\n  - $h_{0}=0.05$, $c_{\\alpha}=0.8$, $p_{\\alpha}=1$, $c_{\\beta}=2.4$, $p_{\\beta}=0$, $\\underline{\\alpha}=0.5$, $\\overline{\\beta}=2.5$, $\\varepsilon=10^{-12}$.\n- Test case $4$ (variant constants, loose tolerance):\n  - $h_{0}=0.08$, $c_{\\alpha}=0.6$, $p_{\\alpha}=1$, $c_{\\beta}=1.8$, $p_{\\beta}=0$, $\\underline{\\alpha}=0.7$, $\\overline{\\beta}=2.1$, $\\varepsilon=10^{-3}$.\n\nYour program must produce the results aggregated into a single line in the following exact format: a Python-like list of lists, where each inner list corresponds to a test case and has exactly two integers $[\\Delta m_{\\mathrm{unprec}}, \\Delta m_{\\mathrm{prec}}]$ in this order. For example, the printed line should look like $[[a,b],[c,d],\\dots]$ with no extra spaces or text.\n\nNo physical units are required since $h$ is normalized by wavelength and all quantities are dimensionless. All angles, if any, must be in radians, but no angles are used here. All final numerical answers must be integers.", "solution": "The objective is to determine the change in the predicted number of GMRES iterations required to solve the MoM linear system $A(h)x=b$ when the mesh parameter $h$ is refined by a factor of $2$. We analyze both the original (unpreconditioned) system and a preconditioned system.\n\n### GMRES Iteration Count Model\nThe convergence of GMRES for a matrix whose spectrum is contained in a positive real interval $[\\alpha, \\beta]$ can be bounded using Chebyshev polynomials. A standard upper bound for the relative residual norm after $m$ iterations is given by:\n$$\n\\frac{\\|r_m\\|}{\\|r_0\\|} \\le 2 \\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right)^m\n$$\nwhere $\\kappa = \\beta/\\alpha$ is the condition number of the matrix.\n\nWe are tasked to find the predicted iteration count, $m(\\kappa, \\varepsilon)$, defined as the smallest nonnegative integer $m$ that guarantees the relative residual is below a given tolerance $\\varepsilon \\in (0, 1)$. Thus, we set the bound to be less than or equal to $\\varepsilon$:\n$$\n2 \\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right)^m \\le \\varepsilon\n$$\nSolving this inequality for $m$ proceeds as follows. First, isolate the term raised to the power of $m$:\n$$\n\\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right)^m \\le \\frac{\\varepsilon}{2}\n$$\nTaking the natural logarithm of both sides:\n$$\nm \\ln\\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right) \\le \\ln\\left( \\frac{\\varepsilon}{2} \\right)\n$$\nSince $\\kappa \\ge 1$, the term $\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}$ is between $0$ and $1$, making its logarithm negative. Therefore, dividing by it reverses the inequality sign:\n$$\nm \\ge \\frac{\\ln(\\varepsilon/2)}{\\ln\\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right)}\n$$\nUsing the property $\\ln(1/x) = -\\ln(x)$, this can be written in a more convenient form:\n$$\nm \\ge \\frac{\\ln(2/\\varepsilon)}{\\ln\\left( \\frac{\\sqrt{\\kappa} + 1}{\\sqrt{\\kappa} - 1} \\right)}\n$$\nThe predicted iteration count $m(\\kappa, \\varepsilon)$ is the smallest integer satisfying this condition. This is obtained by taking the ceiling of the right-hand side. The cases in this problem all have $\\kappa > 1$.\n$$\nm(\\kappa, \\varepsilon) = \\left\\lceil \\frac{\\ln(2/\\varepsilon)}{\\ln\\left( \\frac{\\sqrt{\\kappa} + 1}{\\sqrt{\\kappa} - 1} \\right)} \\right\\rceil\n$$\n\n### Unpreconditioned System Analysis\nFor the unpreconditioned system, the condition number $\\kappa(h)$ depends on the mesh parameter $h$:\n$$\n\\kappa(h) = \\frac{c_{\\beta}}{c_{\\alpha}} h^{-(p_{\\alpha} + p_{\\beta})}\n$$\nLet the initial mesh parameter be $h_0$. The condition number is $\\kappa_{\\text{before}} = \\kappa(h_0)$. The initial iteration count is:\n$$\nm_{\\text{before}} = m(\\kappa_{\\text{before}}, \\varepsilon)\n$$\nUpon refinement, the new mesh parameter is $h_{\\text{after}} = h_0 / 2$. The new condition number is:\n$$\n\\kappa_{\\text{after}} = \\kappa(h_0/2) = \\frac{c_{\\beta}}{c_{\\alpha}} \\left(\\frac{h_0}{2}\\right)^{-(p_{\\alpha} + p_{\\beta})} = \\kappa(h_0) \\cdot 2^{p_{\\alpha} + p_{\\beta}} = \\kappa_{\\text{before}} \\cdot 2^{p_{\\alpha} + p_{\\beta}}\n$$\nThe iteration count after refinement is:\n$$\nm_{\\text{after}} = m(\\kappa_{\\text{after}}, \\varepsilon)\n$$\nThe change in the number of iterations is the integer difference:\n$$\n\\Delta m_{\\mathrm{unprec}} = m_{\\text{after}} - m_{\\text{before}}\n$$\n\n### Preconditioned System Analysis\nThe quasi-Helmholtz preconditioner $M$ is designed to remedy the dense-discretization breakdown. The spectrum of the preconditioned matrix $M^{-1}A(h)$ is contained in an interval $[\\underline{\\alpha}, \\overline{\\beta}]$ that is independent of the mesh parameter $h$.\nConsequently, the condition number of the preconditioned system is also independent of $h$:\n$$\n\\kappa_p = \\frac{\\overline{\\beta}}{\\underline{\\alpha}}\n$$\nSince $\\kappa_p$ does not change upon mesh refinement, the predicted number of iterations $m(\\kappa_p, \\varepsilon)$ also remains constant. The change in the iteration count is therefore zero:\n$$\n\\Delta m_{\\mathrm{prec}} = m(\\kappa_p, \\varepsilon) - m(\\kappa_p, \\varepsilon) = 0\n$$\n\n### Calculation for Test Cases\nFor each test case, we will apply these formulas.\n1.  Calculate $\\kappa_{\\text{before}} = \\kappa(h_0)$ and $\\kappa_{\\text{after}} = \\kappa(h_0/2)$ for the unpreconditioned system.\n2.  Use the derived formula for $m(\\kappa, \\varepsilon)$ to find $m_{\\text{before}}$ and $m_{\\text{after}}$.\n3.  Compute $\\Delta m_{\\mathrm{unprec}} = m_{\\text{after}} - m_{\\text{before}}$.\n4.  Set $\\Delta m_{\\mathrm{prec}} = 0$.\nThe final result for each case is the integer pair $[\\Delta m_{\\mathrm{unprec}}, \\Delta m_{\\mathrm{prec}}]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the change in GMRES iteration count for unpreconditioned and\n    preconditioned systems upon mesh refinement, based on a Chebyshev\n    convergence bound model.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each tuple contains: (h0, ca, pa, cb, pb, a_bar, b_bar, eps)\n    test_cases = [\n        (0.1, 0.8, 1, 2.4, 0, 0.5, 2.5, 1e-6),\n        (0.4, 0.8, 1, 2.4, 0, 0.5, 2.5, 1e-6),\n        (0.05, 0.8, 1, 2.4, 0, 0.5, 2.5, 1e-12),\n        (0.08, 0.6, 1, 1.8, 0, 0.7, 2.1, 1e-3),\n    ]\n\n    def calculate_iterations(kappa, epsilon):\n        \"\"\"\n        Calculates the predicted GMRES iteration count based on the\n        Chebyshev optimal polynomial bound for a real positive spectrum.\n\n        Args:\n            kappa (float): The condition number of the matrix.\n            epsilon (float): The desired relative residual tolerance.\n\n        Returns:\n            int: The smallest integer number of iterations predicted by the bound.\n        \"\"\"\n        # The model is valid for kappa > 1. All test cases satisfy this.\n        if kappa = 1:\n            # A condition number of 1 implies convergence in 1 step.\n            # While this case isn't hit by the problem data, it's a good practice.\n            return 1\n        \n        # Upper bound on iterations m:\n        # m >= ln(2/epsilon) / ln((sqrt(kappa) + 1) / (sqrt(kappa) - 1))\n        sqrt_kappa = np.sqrt(kappa)\n        numerator = np.log(2.0 / epsilon)\n        denominator = np.log((sqrt_kappa + 1.0) / (sqrt_kappa - 1.0))\n        \n        # The predicted iteration count is the smallest integer satisfying the bound.\n        m = np.ceil(numerator / denominator)\n        \n        return int(m)\n\n    results = []\n    for case in test_cases:\n        h0, ca, pa, cb, pb, a_bar, b_bar, eps = case\n        \n        # --- Unpreconditioned Case ---\n        \n        # Mesh parameters before and after refinement by a factor of 2.\n        h_before = h0\n        h_after = h0 / 2.0\n        \n        # Common factors for condition number calculation.\n        kappa_ratio = cb / ca\n        power_sum = pa + pb\n        \n        # Condition numbers before and after refinement.\n        kappa_before = kappa_ratio * (h_before ** -power_sum)\n        kappa_after = kappa_ratio * (h_after ** -power_sum)\n        \n        # Predicted iteration counts.\n        m_unprec_before = calculate_iterations(kappa_before, eps)\n        m_unprec_after = calculate_iterations(kappa_after, eps)\n        \n        delta_m_unprec = m_unprec_after - m_unprec_before\n        \n        # --- Preconditioned Case ---\n        \n        # The preconditioned condition number is independent of h.\n        # Therefore, the iteration count does not change upon mesh refinement.\n        delta_m_prec = 0\n        \n        results.append([delta_m_unprec, delta_m_prec])\n\n    # Final print statement in the exact required format.\n    print(f\"[[{results[0][0]},{results[0][1]}],[{results[1][0]},{results[1][1]}],[{results[2][0]},{results[2][1]}],[{results[3][0]},{results[3][1]}]]\")\n\nsolve()\n```", "id": "3321343"}]}