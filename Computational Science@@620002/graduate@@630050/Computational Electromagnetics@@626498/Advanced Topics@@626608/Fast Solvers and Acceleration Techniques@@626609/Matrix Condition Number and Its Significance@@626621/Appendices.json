{"hands_on_practices": [{"introduction": "Theory provides bounds, but nothing builds intuition like seeing numbers go wrong on a computer. This first practice provides a direct, hands-on computational experience with the dramatic effects of ill-conditioning. By solving a linear system involving the Hilbert matrix—a classic example of a severely ill-conditioned matrix—you will empirically observe how the numerical accuracy of the solution degrades catastrophically as the problem size increases, even when using high-precision arithmetic and a stable algorithm [@problem_id:2428600]. This exercise will solidify your understanding of the relationship between the condition number, forward error, and the often misleadingly small backward error (residual).", "problem": "Write a complete program that empirically demonstrates the effect of problem conditioning on the accuracy of solving linear systems in standard floating-point arithmetic. Consider the linear system $A x = b$ where $A$ is the $n \\times n$ Hilbert matrix with entries $A_{i j} = \\dfrac{1}{i + j - 1}$ for $1 \\le i,j \\le n$. For each test case, define the exact solution vector $x_{\\text{true}} \\in \\mathbb{R}^n$ by $x_{\\text{true}} = \\mathbf{1}$ (all ones), and construct the right-hand side $b = A x_{\\text{true}}$. Then compute the numerical solution $\\hat{x}$ using standard floating-point arithmetic conforming to Institute of Electrical and Electronics Engineers (IEEE) $754$ double precision (binary$64$) semantics. Using $\\hat{x}$, compute the following quantities:\n- The $2$-norm condition number $\\kappa_2(A) = \\|A\\|_2 \\,\\|A^{-1}\\|_2$.\n- The relative forward error in the $2$-norm, $\\dfrac{\\|\\hat{x} - x_{\\text{true}}\\|_2}{\\|x_{\\text{true}}\\|_2}$.\n- The scaled residual (a normalized backward error proxy), $\\dfrac{\\|b - A \\hat{x}\\|_2}{\\|A\\|_2 \\,\\|\\hat{x}\\|_2 + \\|b\\|_2}$.\n- The estimated number of correct decimal digits in the solution, defined as $\\max\\!\\bigl(0,\\,-\\log_{10}(\\max(\\text{relative forward error}, \\varepsilon)))$, where $\\varepsilon$ denotes the smallest positive normalized IEEE $754$ double-precision number.\n\nTest Suite:\nEvaluate the above for the following problem sizes $n$:\n- $n = 3$,\n- $n = 6$,\n- $n = 10$,\n- $n = 12$.\n\nAll vector and matrix norms are the spectral norm (that is, the matrix $2$-norm and the vector $2$-norm). Angles are not involved. No physical units are involved.\n\nYour program must produce a single line of output containing all test results as a comma-separated list enclosed in square brackets, where each test result is itself a list of the form $[n,\\;\\kappa_2(A),\\;\\text{relative forward error},\\;\\text{scaled residual},\\;\\text{estimated digits}]$. The final output must therefore be a single line representing a list of lists, with no additional text. For example, the structure must be similar to $[[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],[\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],\\ldots]$ but with the actual computed numbers in place of the placeholders.", "solution": "The problem statement presented is valid. It is a well-posed, scientifically grounded exercise in numerical linear algebra, designed to demonstrate the fundamental concepts of problem conditioning, forward error, and backward error. The problem is self-contained, with all necessary definitions and data provided. It does not violate any scientific principles, logic, or contain any ambiguities. We shall proceed with the solution.\n\nThe problem requires an empirical investigation into the effects of ill-conditioning on the solution of a linear system of equations, $A x = b$. The matrix $A$ is chosen to be the $n \\times n$ Hilbert matrix, a classic example of a severely ill-conditioned matrix. Its entries are given by $A_{i j} = \\frac{1}{i + j - 1}$ for $i, j$ from $1$ to $n$.\n\nThe core of numerical analysis is not only to compute a solution but also to understand its accuracy. The accuracy of the computed solution, which we denote $\\hat{x}$, is affected by two main factors: the stability of the algorithm used and the intrinsic sensitivity of the problem itself. This sensitivity is quantified by the condition number.\n\nFor a linear system $A x = b$, the $2$-norm condition number of the matrix $A$ is defined as:\n$$ \\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2 $$\nwhere $\\| \\cdot \\|_2$ is the spectral norm (the largest singular value). A large condition number, $\\kappa_2(A) \\gg 1$, signifies an ill-conditioned problem, where small relative perturbations in the input data ($A$ or $b$) can lead to large relative changes in the solution $x$.\n\nWhen we solve $A x = b$ using floating-point arithmetic, round-off errors are inevitably introduced. A backward stable algorithm, such as the LU decomposition employed by standard solvers, produces a computed solution $\\hat{x}$ that is the exact solution to a slightly perturbed problem:\n$$ (A + \\delta A) \\hat{x} = b + \\delta b $$\nThe \"smallness\" of these perturbations $\\delta A$ and $\\delta b$ is a measure of the algorithm's backward stability. A key result in numerical analysis establishes the following bound on the relative forward error:\n$$ \\frac{\\|\\hat{x} - x_{\\text{true}}\\|_2}{\\|x_{\\text{true}}\\|_2} \\le \\kappa_2(A) \\left( \\frac{\\|\\delta A\\|_2}{\\|A\\|_2} + \\frac{\\|\\delta b\\|_2}{\\|b\\|_2} \\right) $$\nThe right-hand side parenthetical term is the relative backward error. For a backward stable algorithm operating with machine precision $\\varepsilon_{\\text{mach}}$, the backward error is typically of order $\\mathcal{O}(\\varepsilon_{\\text{mach}})$. Machine precision for IEEE $754$ double-precision is approximately $2.22 \\times 10^{-16}$. Therefore, we expect the relative forward error to be bounded by approximately $\\kappa_2(A) \\cdot \\varepsilon_{\\text{mach}}$. This demonstrates that a large condition number amplifies the unavoidable round-off errors, potentially destroying the accuracy of the solution.\n\nThe residual vector is defined as $r = b - A \\hat{x}$. The norm of the residual, $\\|r\\|_2$, is related to the backward error. The problem asks for a specific scaled residual:\n$$ \\frac{\\|b - A \\hat{x}\\|_2}{\\|A\\|_2 \\,\\|\\hat{x}\\|_2 + \\|b\\|_2} $$\nThis quantity serves as a normalized proxy for the backward error. Due to the backward stability of the solver, we expect this value to remain small, on the order of $\\varepsilon_{\\text{mach}}$, even as the condition number grows and the forward error explodes. This is a crucial distinction: a small residual does not guarantee a small forward error.\n\nFinally, we estimate the number of correct decimal digits in the solution. This is directly related to the relative forward error. If the relative error is $10^{-k}$, the solution is accurate to roughly $k$ decimal digits. The given formula, $\\max\\!\\bigl(0,\\,-\\log_{10}(\\max(\\text{relative forward error}, \\varepsilon)))$, formalizes this. The term $\\varepsilon$ is the smallest positive normalized double-precision number, approximately $2.225 \\times 10^{-308}$, which prevents a logarithm of zero and handles cases where the error is smaller than representable precision allows.\n\nWe will now perform these calculations for the specified test suite of matrix sizes $n \\in \\{3, 6, 10, 12\\}$. For each $n$:\n1.  Construct the $n \\times n$ Hilbert matrix $A$.\n2.  Define the true solution $x_{\\text{true}}$ as a vector of $n$ ones.\n3.  Compute the right-hand side $b = A x_{\\text{true}}$.\n4.  Solve for the numerical solution $\\hat{x}$ using a standard linear solver.\n5.  Compute the four specified quantities: $\\kappa_2(A)$, relative forward error, scaled residual, and estimated digits.\n\nThe results will empirically validate the theory. The condition number of the Hilbert matrix grows extremely rapidly with $n$. For small $n$ (e.g., $n=3$), $\\kappa_2(A)$ is moderate, and we expect a reasonably accurate solution. As $n$ increases to $10$ and $12$, $\\kappa_2(A)$ will become enormous ($> 10^{13}$), leading to a relative forward error of order $1$ or greater, signifying a complete loss of accuracy. Throughout this process, the scaled residual should remain small, demonstrating the backward stability of the algorithm in contrast to the poor forward accuracy for an ill-conditioned problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import hilbert\n\ndef solve():\n    \"\"\"\n    Empirically demonstrates the effect of problem conditioning on the accuracy\n    of solving linear systems Ax = b using the Hilbert matrix.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [3, 6, 10, 12]\n\n    results = []\n    \n    # Epsilon as defined in the problem: the smallest positive normalized\n    # IEEE 754 double-precision number.\n    smallest_norm_val = np.finfo(np.float64).smallest_normal\n\n    for n in test_cases:\n        # Step 1: Construct the n x n Hilbert matrix A.\n        # The problem statement indices are 1-based, but `scipy.linalg.hilbert` is 0-based,\n        # which results in the same matrix A_ij = 1 / ((i+1) + (j+1) - 1) for 0<=i,j<n.\n        A = hilbert(n)\n\n        # Step 2: Define the exact solution vector x_true (all ones).\n        x_true = np.ones(n)\n\n        # Step 3: Construct the right-hand side b = A * x_true.\n        # This ensures that b is consistent with A and x_true.\n        b = A @ x_true\n\n        # Step 4: Compute the numerical solution x_hat using a standard solver.\n        # numpy.linalg.solve uses LAPACK routines which are backward stable and\n        # operate in IEEE 754 double precision.\n        x_hat = np.linalg.solve(A, b)\n\n        # --- Calculate the required quantities ---\n\n        # The 2-norm condition number kappa_2(A).\n        kappa_2_A = np.linalg.cond(A, 2)\n\n        # The relative forward error in the 2-norm.\n        norm_x_true = np.linalg.norm(x_true, 2)\n        if norm_x_true == 0:\n            # Avoid division by zero, though not possible for x_true = 1.\n            rel_fwd_err = np.linalg.norm(x_hat - x_true, 2)\n        else:\n            rel_fwd_err = np.linalg.norm(x_hat - x_true, 2) / norm_x_true\n        \n        # The scaled residual (a normalized backward error proxy).\n        # residual = b - A @ x_hat\n        # norm_residual = ||b - A*x_hat||_2\n        # scaled_residual = norm_residual / (||A||_2 * ||x_hat||_2 + ||b||_2)\n        norm_A = np.linalg.norm(A, 2)\n        norm_x_hat = np.linalg.norm(x_hat, 2)\n        norm_b = np.linalg.norm(b, 2)\n        norm_residual = np.linalg.norm(b - A @ x_hat, 2)\n        \n        denominator = norm_A * norm_x_hat + norm_b\n        if denominator == 0:\n            # Handle potential division by zero.\n            scaled_res = norm_residual\n        else:\n            scaled_res = norm_residual / denominator\n\n        # The estimated number of correct decimal digits.\n        # est_digits = max(0, -log10(max(relative forward error, epsilon)))\n        log_val = max(rel_fwd_err, smallest_norm_val)\n        est_digits = max(0.0, -np.log10(log_val))\n        \n        # Store results for this test case.\n        results.append([n, kappa_2_A, rel_fwd_err, scaled_res, est_digits])\n\n    # Final print statement in the exact required format.\n    # The output format is a string representation of a list of lists.\n    # Example: [[3, 5.2e+02, ...], [6, 1.5e+07, ...], ...]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "2428600"}, {"introduction": "In computational electromagnetics, the condition number is not just an abstract quantity; it has direct, practical consequences on the efficiency of our simulations. This practice bridges the gap between matrix theory and solver performance, using a scenario common in Finite Element Method (FEM) modeling: a medium with highly contrasting material properties. You will calculate the condition number for a matrix representing such a heterogeneous physical domain and see how it directly dictates the convergence rate of the Conjugate Gradient (CG) method, one of the most important iterative solvers for symmetric positive definite systems [@problem_id:3328812]. This exercise demonstrates why understanding and controlling the condition number is critical for developing efficient computational tools.", "problem": "In a frequency-domain discretization of Maxwell’s equations using the Finite Element Method (FEM), a heterogeneous, anisotropic medium leads to a block-diagonal material matrix that maps field degrees of freedom in different subdomains. Consider the symmetric positive definite block-diagonal matrix $M \\in \\mathbb{R}^{6 \\times 6}$ with three $2 \\times 2$ diagonal blocks:\n$$\nM = \\mathrm{diag}(B_{1}, B_{2}, B_{3}),\n$$\nwhere\n$$\nB_{1} = \\begin{pmatrix} 10^{-6} & 0 \\\\ 0 & 4 \\times 10^{-6} \\end{pmatrix}, \\quad\nB_{2} = \\begin{pmatrix} 3 \\times 10^{-3} & 0 \\\\ 0 & 10^{-2} \\end{pmatrix}, \\quad\nB_{3} = \\begin{pmatrix} 2 \\times 10^{3} & 0 \\\\ 0 & 7 \\times 10^{3} \\end{pmatrix}.\n$$\nThis structure represents strongly contrasting material parameters (e.g., effective permittivity and permeability components) across coupled subdomains.\n\nStarting from the definition of the matrix condition number with respect to the matrix $2$-norm and from properties of symmetric positive definite matrices, compute the $2$-norm condition number $\\kappa_{2}(M)$. Then, using a standard convergence characterization for the Conjugate Gradient (CG) method applied to symmetric positive definite systems, relate $\\kappa_{2}(M)$ to the per-iteration residual contraction factor and provide that factor as an explicit expression in terms of $\\kappa_{2}(M)$, evaluated for the given $M$.\n\nExpress your final answer as a row matrix containing two entries: first $\\kappa_{2}(M)$, second the residual contraction factor. No rounding is required. The quantities are dimensionless.", "solution": "The problem statement has been validated and is deemed self-contained, scientifically grounded, and well-posed. The matrix $M$ is specified as symmetric and positive definite (SPD). The individual blocks $B_1$, $B_2$, and $B_3$ are diagonal with positive diagonal entries, confirming they are SPD. A block-diagonal matrix composed of SPD blocks is itself SPD. Therefore, the premises are consistent and valid.\n\nThe first task is to compute the $2$-norm condition number, $\\kappa_{2}(M)$, of the matrix $M$. By definition, for an invertible matrix $A$, the condition number is given by $\\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2}$. For a symmetric matrix, the $2$-norm is equal to its spectral radius, which is the maximum absolute value of its eigenvalues. Since $M$ is symmetric positive definite, all its eigenvalues are real and positive. Thus, $\\|M\\|_{2} = \\lambda_{\\max}(M)$, where $\\lambda_{\\max}(M)$ is the largest eigenvalue of $M$.\n\nThe inverse of an SPD matrix, $M^{-1}$, is also SPD. Its eigenvalues are the reciprocals of the eigenvalues of $M$. Therefore, the largest eigenvalue of $M^{-1}$ is the reciprocal of the smallest eigenvalue of $M$: $\\lambda_{\\max}(M^{-1}) = 1/\\lambda_{\\min}(M)$. Consequently, the norm of the inverse is $\\|M^{-1}\\|_{2} = 1/\\lambda_{\\min}(M)$.\n\nCombining these results, the $2$-norm condition number for an SPD matrix $M$ simplifies to the ratio of its largest to its smallest eigenvalue:\n$$\n\\kappa_{2}(M) = \\frac{\\lambda_{\\max}(M)}{\\lambda_{\\min}(M)}\n$$\nThe matrix $M$ is a block-diagonal matrix:\n$$\nM = \\mathrm{diag}(B_{1}, B_{2}, B_{3})\n$$\nThe set of eigenvalues of a block-diagonal matrix is the union of the sets of eigenvalues of its diagonal blocks. The blocks $B_1$, $B_2$, and $B_3$ are themselves diagonal matrices. The eigenvalues of a diagonal matrix are simply its diagonal entries.\nThe eigenvalues of $B_1$ are $\\{10^{-6}, 4 \\times 10^{-6}\\}$.\nThe eigenvalues of $B_2$ are $\\{3 \\times 10^{-3}, 10^{-2}\\}$.\nThe eigenvalues of $B_3$ are $\\{2 \\times 10^{3}, 7 \\times 10^{3}\\}$.\n\nThe set of all eigenvalues of $M$, denoted $\\Lambda(M)$, is the union of these sets:\n$$\n\\Lambda(M) = \\{10^{-6}, 4 \\times 10^{-6}, 3 \\times 10^{-3}, 10^{-2}, 2 \\times 10^{3}, 7 \\times 10^{3}\\}\n$$\nFrom this set, we identify the minimum and maximum eigenvalues:\n$$\n\\lambda_{\\min}(M) = 10^{-6}\n$$\n$$\n\\lambda_{\\max}(M) = 7 \\times 10^{3}\n$$\nNow, we can compute the condition number:\n$$\n\\kappa_{2}(M) = \\frac{7 \\times 10^{3}}{10^{-6}} = 7 \\times 10^{9}\n$$\nThe second part of the problem concerns the convergence of the Conjugate Gradient (CG) method. For solving a linear system $Mx=b$ where $M$ is SPD, the convergence rate of the CG method can be bounded in terms of the condition number $\\kappa_{2}(M)$. The error $e_k = x - x_k$ at the $k$-th iteration satisfies the following inequality in the $M$-norm (also known as the energy norm):\n$$\n\\|e_k\\|_{M} \\le 2 \\left( \\frac{\\sqrt{\\kappa_{2}(M)} - 1}{\\sqrt{\\kappa_{2}(M)} + 1} \\right)^{k} \\|e_0\\|_{M}\n$$\nThis bound characterizes the worst-case convergence. The per-iteration residual contraction factor, which we denote as $\\rho$, can be identified from this expression. It represents the factor by which the $M$-norm of the error is guaranteed to be reduced at each step on average. This factor is given by:\n$$\n\\rho = \\frac{\\sqrt{\\kappa_{2}(M)} - 1}{\\sqrt{\\kappa_{2}(M)} + 1}\n$$\nSubstituting the calculated value of $\\kappa_{2}(M) = 7 \\times 10^{9}$ into this expression yields the specific contraction factor for the given matrix $M$:\n$$\n\\rho = \\frac{\\sqrt{7 \\times 10^{9}} - 1}{\\sqrt{7 \\times 10^{9}} + 1}\n$$\nThe problem requires the final answer to be these two quantities, $\\kappa_2(M)$ and $\\rho$, without rounding.\nThe first value is $\\kappa_2(M) = 7 \\times 10^9$.\nThe second value is the contraction factor $\\rho = \\frac{\\sqrt{7 \\times 10^{9}} - 1}{\\sqrt{7 \\times 10^{9}} + 1}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 7 \\times 10^{9} & \\frac{\\sqrt{7 \\times 10^{9}} - 1}{\\sqrt{7 \\times 10^{9}} + 1} \\end{pmatrix}}\n$$", "id": "3328812"}, {"introduction": "Having seen the practical consequences of ill-conditioning, we now turn to a more fundamental question: what does a large condition number truly signify about a linear system? This practice explores the deep geometric meaning of conditioning by relating it to a matrix's 'distance' from being singular. You will derive the elegant and powerful result that the smallest singular value, $\\sigma_{\\min}(A)$, is precisely the smallest perturbation (in the spectral norm) needed to make a matrix non-invertible [@problem_id:3240849]. This provides the ultimate insight into why an ill-conditioned matrix is unstable: a large condition number $\\kappa_2(A) = \\sigma_{\\max} / \\sigma_{\\min}$ means the matrix is fundamentally 'close' to a problem with no unique solution.", "problem": "Consider the matrix $A \\in \\mathbb{R}^{3 \\times 3}$ given by\n$$\nA=\\begin{pmatrix}\n0 & -2 & 0 \\\\\n3 & 0 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\nA square matrix is said to be rank-deficient if its rank is strictly less than its dimension, which for a $3 \\times 3$ matrix is equivalent to being singular. The spectral (operator $2$-) norm of a matrix $M$ is defined as $\\|M\\|_{2}=\\sup_{\\|x\\|_{2}=1}\\|Mx\\|_{2}$. The singular values of a matrix are defined as the square roots of the eigenvalues of $A^{\\top}A$. \n\nUsing only these core definitions, determine the minimal value of $\\|\\Delta A\\|_{2}$ over all perturbations $\\Delta A \\in \\mathbb{R}^{3 \\times 3}$ such that $A+\\Delta A$ is rank-deficient. Your derivation should start from the definitions of rank-deficiency, the spectral norm, and singular values, and should establish the value of the minimal spectral-norm distance from $A$ to the set of rank-deficient matrices. Express your final answer as an exact real number without units.", "solution": "The problem asks for the minimal spectral norm of a perturbation $\\Delta A$ that makes the matrix $A + \\Delta A$ rank-deficient. A square matrix is rank-deficient if and only if it is singular. Thus, we seek to determine the value of $\\min \\{ \\|\\Delta A\\|_2 \\mid A + \\Delta A \\text{ is singular} \\}$. This quantity is the distance, in the spectral norm, from the matrix $A$ to the set of singular matrices. We will first establish a general result and then apply it to the specific matrix $A$ provided.\n\nLet $A$ be an $n \\times n$ matrix. The solution proceeds in two parts: first, establishing a lower bound for $\\|\\Delta A\\|_2$, and second, constructing a specific perturbation $\\Delta A$ that achieves this bound.\n\nPart 1: Derivation of the lower bound.\nSuppose $\\Delta A$ is a perturbation such that $B = A + \\Delta A$ is singular. By definition of a singular matrix, there exists a non-zero vector $x$ such that $Bx = 0$. We can normalize this vector such that its Euclidean norm is $\\|x\\|_2 = 1$.\nFrom $Bx = 0$, we have $(A + \\Delta A)x = 0$, which rearranges to $Ax = -\\Delta A x$.\nTaking the spectral norm (Euclidean vector norm) of both sides gives $\\|Ax\\|_2 = \\|-\\Delta A x\\|_2 = \\|\\Delta A x\\|_2$.\nFrom the definition of the induced matrix norm, we have $\\|\\Delta A x\\|_2 \\le \\|\\Delta A\\|_2 \\|x\\|_2$. Since we chose $\\|x\\|_2 = 1$, this simplifies to $\\|\\Delta A x\\|_2 \\le \\|\\Delta A\\|_2$.\nCombining these results, we obtain the inequality $\\|\\Delta A\\|_2 \\ge \\|Ax\\|_2$.\nThis inequality must hold for any perturbation $\\Delta A$ that renders $A+\\Delta A$ singular, where $x$ is a corresponding unit vector in the null space of $A+\\Delta A$. The minimal norm perturbation must therefore satisfy $\\|\\Delta A\\|_2 \\ge \\|Ax\\|_2$. This implies that the minimal value of $\\|\\Delta A\\|_2$ must be greater than or equal to the minimum value of $\\|Ay\\|_2$ over all possible unit vectors $y$. In other words,\n$$ \\min_{\\Delta A} \\{ \\|\\Delta A\\|_2 \\mid A + \\Delta A \\text{ is singular} \\} \\ge \\min_{\\|y\\|_2=1} \\|Ay\\|_2 $$\nWe now relate this minimum to the singular values of $A$. The singular values of $A$, denoted $\\sigma_i$, are defined as the square roots of the eigenvalues of the matrix $A^\\top A$. Let the eigenvalues of $A^\\top A$ be $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0$. Then $\\sigma_i = \\sqrt{\\lambda_i}$. The matrix $A^\\top A$ is symmetric and positive semi-definite, so its eigenvalues are real and non-negative, and it has an orthonormal basis of eigenvectors $v_1, v_2, \\dots, v_n$ corresponding to these eigenvalues.\nFor any unit vector $y$, we can express it in this basis as $y = \\sum_{i=1}^n c_i v_i$, where $\\sum_{i=1}^n c_i^2 = 1$.\nNow consider the squared norm $\\|Ay\\|_2^2$:\n$$ \\|Ay\\|_2^2 = (Ay)^\\top(Ay) = y^\\top A^\\top A y = \\left(\\sum_i c_i v_i\\right)^\\top A^\\top A \\left(\\sum_j c_j v_j\\right) $$\nSince $A^\\top A v_j = \\lambda_j v_j$, this becomes:\n$$ \\|Ay\\|_2^2 = \\left(\\sum_i c_i v_i\\right)^\\top \\left(\\sum_j c_j \\lambda_j v_j\\right) = \\sum_{i,j} c_i c_j \\lambda_j (v_i^\\top v_j) $$\nDue to the orthonormality of the eigenvectors, $v_i^\\top v_j = \\delta_{ij}$ (the Kronecker delta). The expression simplifies to:\n$$ \\|Ay\\|_2^2 = \\sum_{i=1}^n c_i^2 \\lambda_i $$\nTo find the minimum of $\\|Ay\\|_2$ over all unit vectors $y$, we need to find the minimum of $\\sum_{i=1}^n c_i^2 \\lambda_i$ subject to the constraint $\\sum_{i=1}^n c_i^2 = 1$. Since $\\lambda_1 \\ge \\dots \\ge \\lambda_n$, the minimum value is achieved when $c_n=1$ and $c_i=0$ for $i<n$. This minimum value is $\\lambda_n$. This corresponds to choosing $y = v_n$, the eigenvector associated with the smallest eigenvalue $\\lambda_n$.\nTherefore, $\\min_{\\|y\\|_2=1} \\|Ay\\|_2^2 = \\lambda_n = \\lambda_{\\min}(A^\\top A)$.\nTaking the square root, we get $\\min_{\\|y\\|_2=1} \\|Ay\\|_2 = \\sqrt{\\lambda_{\\min}(A^\\top A)} = \\sigma_{\\min}(A)$, the smallest singular value of $A$.\nCombining with our earlier result, we have established the lower bound:\n$$ \\min_{\\Delta A} \\{ \\|\\Delta A\\|_2 \\mid A + \\Delta A \\text{ is singular} \\} \\ge \\sigma_{\\min}(A) $$\n\nPart 2: Construction of a perturbation that achieves the bound.\nLet $\\sigma_n = \\sigma_{\\min}(A)$ be the smallest singular value of $A$, and let $v_n$ be the corresponding right singular vector, which is the unit eigenvector of $A^\\top A$ for the eigenvalue $\\lambda_n = \\sigma_n^2$. We have $\\|v_n\\|_2=1$. Let $u_n$ be the corresponding left singular vector, defined by the relation $Av_n = \\sigma_n u_n$. Note that $\\|u_n\\|_2=1$ as long as $\\sigma_n > 0$. If $\\sigma_n = 0$, $A$ is already singular, and the minimum perturbation is $\\Delta A = 0$ with norm $0$. Assuming $\\sigma_n > 0$, we construct the following rank-one perturbation:\n$$ \\Delta A = -\\sigma_n u_n v_n^\\top $$\nWe must verify two properties: that $A+\\Delta A$ is singular, and that $\\|\\Delta A\\|_2 = \\sigma_n$.\nFirst, let's check for singularity. We multiply $A+\\Delta A$ by the non-zero vector $v_n$:\n$$ (A+\\Delta A)v_n = Av_n + (-\\sigma_n u_n v_n^\\top)v_n = Av_n - \\sigma_n u_n (v_n^\\top v_n) $$\nSince $\\|v_n\\|_2=1$, we have $v_n^\\top v_n = 1$. The expression becomes:\n$$ (A+\\Delta A)v_n = Av_n - \\sigma_n u_n $$\nBy the definition of $u_n$, we have $Av_n = \\sigma_n u_n$. Substituting this in:\n$$ (A+\\Delta A)v_n = \\sigma_n u_n - \\sigma_n u_n = 0 $$\nSince $v_n$ is a non-zero vector, we have found a vector in the null space of $A+\\Delta A$. Thus, $A+\\Delta A$ is singular.\n\nSecond, we calculate the spectral norm of this perturbation.\n$$ \\|\\Delta A\\|_2 = \\|-\\sigma_n u_n v_n^\\top\\|_2 = \\sigma_n \\|u_n v_n^\\top\\|_2 $$\nUsing the definition of the spectral norm, $\\|u_n v_n^\\top\\|_2 = \\sup_{\\|x\\|_2=1} \\|(u_n v_n^\\top)x\\|_2$. The term $v_n^\\top x$ is a scalar.\n$$ \\|(u_n v_n^\\top)x\\|_2 = \\|u_n (v_n^\\top x)\\|_2 = |v_n^\\top x| \\|u_n\\|_2 $$\nSince $\\|u_n\\|_2=1$, this is simply $|v_n^\\top x|$. By the Cauchy-Schwarz inequality, $|v_n^\\top x| \\le \\|v_n\\|_2 \\|x\\|_2 = (1)(1) = 1$. The supremum of $|v_n^\\top x|$ over all unit vectors $x$ is $1$, achieved when $x = v_n$.\nTherefore, $\\|u_n v_n^\\top\\|_2 = 1$, and the norm of our perturbation is $\\|\\Delta A\\|_2 = \\sigma_n$.\n\nWe have shown that the minimal norm is at least $\\sigma_{\\min}(A)$ and we have constructed a perturbation of norm exactly $\\sigma_{\\min}(A)$ that makes the matrix singular. Thus, the minimal value is $\\sigma_{\\min}(A)$.\n\nCalculation for the given matrix $A$.\nWe are given the matrix\n$$\nA=\\begin{pmatrix}\n0 & -2 & 0 \\\\\n3 & 0 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\nTo find the singular values, we need the eigenvalues of $A^\\top A$.\nFirst, we compute $A^\\top$:\n$$\nA^\\top=\\begin{pmatrix}\n0 & 3 & 0 \\\\\n-2 & 0 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\nNext, we compute the product $A^\\top A$:\n$$\nA^\\top A = \\begin{pmatrix}\n0 & 3 & 0 \\\\\n-2 & 0 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n0 & -2 & 0 \\\\\n3 & 0 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n= \\begin{pmatrix}\n(0)(0)+(3)(3)+(0)(0) & (0)(-2)+(3)(0)+(0)(0) & (0)(0)+(3)(0)+(0)(1) \\\\\n(-2)(0)+(0)(3)+(0)(0) & (-2)(-2)+(0)(0)+(0)(0) & (-2)(0)+(0)(0)+(0)(1) \\\\\n(0)(0)+(0)(3)+(1)(0) & (0)(-2)+(0)(0)+(1)(0) & (0)(0)+(0)(0)+(1)(1)\n\\end{pmatrix}\n$$\n$$\nA^\\top A = \\begin{pmatrix}\n9 & 0 & 0 \\\\\n0 & 4 & 0 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\nThis is a diagonal matrix. The eigenvalues of a diagonal matrix are its diagonal entries. The eigenvalues of $A^\\top A$ are $\\lambda_1=9$, $\\lambda_2=4$, and $\\lambda_3=1$.\nThe smallest eigenvalue is $\\lambda_{\\min}(A^\\top A) = 1$.\nThe smallest singular value of $A$ is the square root of the smallest eigenvalue of $A^\\top A$:\n$$\n\\sigma_{\\min}(A) = \\sqrt{\\lambda_{\\min}(A^\\top A)} = \\sqrt{1} = 1.\n$$\nTherefore, the minimal value of $\\|\\Delta A\\|_2$ such that $A+\\Delta A$ is rank-deficient is $1$.", "answer": "$$\n\\boxed{1}\n$$", "id": "3240849"}]}