{"hands_on_practices": [{"introduction": "A core task in any integral-equation solver is assembling the system matrix, which represents interactions between all pairs of basis functions. A crucial first step in analyzing the solver's complexity is to understand the cost of the 'near-field' interactions, which are the most computationally intensive to compute accurately. This practice [@problem_id:3293963] guides you through an asymptotic analysis to show that, under standard assumptions of mesh regularity, the cost of assembling this critical part of the matrix scales linearly, $\\Theta(N)$, with the problem size, establishing a fundamental baseline for overall solver complexity.", "problem": "Consider a boundary integral equation formulation of time-harmonic electromagnetic scattering from a perfectly electrically conducting surface, discretized by the Method of Moments (MoM). The dense system matrix arises from pairwise interactions between discretization panels (elements) via the Green’s function kernel. To accelerate computation, the matrix is partitioned into near-field and far-field blocks. Near-field blocks are defined by a mesh graph neighborhood: for each panel $i$, its near-field interactions are limited to the one-ring adjacency set $\\mathcal{N}(i)$ consisting of panels that share at least one mesh edge or vertex with $i$. Assume the mesh is shape-regular and quasi-uniform so that there exists a constant $C$ independent of $N$ such that $|\\mathcal{N}(i)| \\leq C$ for all panels $i$, where $N$ is the total number of panels.\n\nEach near-field block entry corresponding to a panel pair $(i,j)$ with $j \\in \\mathcal{N}(i)$ is assembled by evaluating a fixed quadrature rule with a bounded number of points and a bounded number of basis/test function operations per panel; hence, the arithmetic cost per assembled pair $(i,j)$ is a constant $a$ that does not depend on $N$. Assume assembly is performed for all ordered panel-neighbor pairs $(i,j)$ with $j \\in \\mathcal{N}(i)$, and that symmetry is not exploited during assembly.\n\nUsing only the above assumptions and first principles of operation counting, compute the asymptotic cost, as a function of $N$, of assembling all near-field blocks. Express your final answer as a single asymptotic expression in $N$ (using standard Big-Theta notation). No numerical rounding is required, and no physical units are involved.", "solution": "The problem asks for the asymptotic computational cost of assembling the near-field blocks of a Method of Moments (MoM) matrix. The total cost is the sum of the costs of assembling each individual near-field matrix entry.\n\nLet $N$ be the total number of discretization panels on the surface.\nLet $T$ denote the total assembly cost for the near-field blocks.\nThe problem states that the near-field interactions for a panel $i$ are defined by its one-ring adjacency set, $\\mathcal{N}(i)$. The assembly is performed for all ordered panel-neighbor pairs $(i, j)$ where $i$ ranges from $1$ to $N$ and $j \\in \\mathcal{N}(i)$.\nThe cost to assemble the interaction for a single pair $(i, j)$ is given as a constant, $a$, which is independent of $N$.\n\nThe total cost $T$ can be formulated as the sum of costs over all panels $i$, where the cost for each panel is the sum of costs for its interactions with all its neighbors $j \\in \\mathcal{N}(i)$.\nMathematically, this is expressed as:\n$$ T = \\sum_{i=1}^{N} \\left( \\sum_{j \\in \\mathcal{N}(i)} (\\text{Cost for pair } (i, j)) \\right) $$\nSubstituting the given constant cost $a$ for each pair:\n$$ T = \\sum_{i=1}^{N} \\left( \\sum_{j \\in \\mathcal{N}(i)} a \\right) $$\nSince $a$ is a constant, it can be factored out of the inner summation. The inner sum then becomes a count of the number of elements in the set $\\mathcal{N}(i)$, which is its cardinality, denoted by $|\\mathcal{N}(i)|$.\n$$ \\sum_{j \\in \\mathcal{N}(i)} a = a \\sum_{j \\in \\mathcal{N}(i)} 1 = a \\cdot |\\mathcal{N}(i)| $$\nSubstituting this back into the expression for the total cost $T$:\n$$ T = \\sum_{i=1}^{N} a \\cdot |\\mathcal{N}(i)| $$\nSince $a$ is a constant with respect to the summation index $i$, we can factor it out of the total sum as well:\n$$ T = a \\sum_{i=1}^{N} |\\mathcal{N}(i)| $$\nTo determine the asymptotic cost, we need to find the asymptotic behavior of the sum $\\sum_{i=1}^{N} |\\mathcal{N}(i)|$ as a function of $N$. This sum represents the total number of near-field interactions to be computed.\n\nWe are given that the mesh is shape-regular and quasi-uniform. A key consequence of this is that there exists a constant $C$, independent of $N$, such that the size of the neighborhood of any panel $i$ is bounded from above:\n$$ |\\mathcal{N}(i)| \\leq C \\quad \\text{for all } i = 1, \\dots, N $$\nUsing this inequality, we can establish an upper bound on the total cost $T$:\n$$ T = a \\sum_{i=1}^{N} |\\mathcal{N}(i)| \\leq a \\sum_{i=1}^{N} C = a \\cdot C \\cdot N $$\nSince $a$ and $C$ are constants, the total cost $T$ is bounded above by a linear function of $N$. In asymptotic notation, this means:\n$$ T = O(N) $$\n\nTo establish the asymptotic behavior with Big-Theta notation, we must also find a lower bound. The properties of a `shape-regular` and `quasi-uniform` mesh also imply that the local connectivity is reasonably uniform across the mesh. For any panel that is not on a physical boundary of an open surface mesh, the number of neighbors is bounded from below by a positive constant, say $C_{\\min} > 0$. Let's consider the set of all panels. A small fraction of panels may lie on the boundary of an open surface and have fewer neighbors. For a two-dimensional surface discretized with $N$ panels, the number of such boundary panels, $N_{\\text{bnd}}$, is typically of a lower order than $N$, for instance, $N_{\\text{bnd}} = O(\\sqrt{N})$. The number of interior panels is $N_{\\text{int}} = N - N_{\\text{bnd}}$.\n\nFor each of the $N_{\\text{int}}$ interior panels, we can assume $|\\mathcal{N}(i)| \\geq C_{\\min}$ for some positive constant $C_{\\min}$. For the boundary panels, the number of neighbors is at least $1$ (unless the mesh is disconnected, which would be pathological).\nWe can therefore establish a lower bound on the sum:\n$$ \\sum_{i=1}^{N} |\\mathcal{N}(i)| = \\sum_{i \\in \\text{interior}} |\\mathcal{N}(i)| + \\sum_{i \\in \\text{boundary}} |\\mathcal{N}(i)| \\geq \\sum_{i \\in \\text{interior}} C_{\\min} + \\sum_{i \\in \\text{boundary}} 1 $$\n$$ \\sum_{i=1}^{N} |\\mathcal{N}(i)| \\geq N_{\\text{int}} \\cdot C_{\\min} + N_{\\text{bnd}} \\cdot 1 = (N - N_{\\text{bnd}}) C_{\\min} + N_{\\text{bnd}} $$\nFor large $N$, $N_{\\text{bnd}}$ is sublinear in $N$ (e.g., $O(\\sqrt{N})$), so $N_{\\text{int}}$ is asymptotic to $N$. The dominant term in the lower bound is $N \\cdot C_{\\min}$. Therefore, there exists a constant $C' > 0$ such that for sufficiently large $N$:\n$$ \\sum_{i=1}^{N} |\\mathcal{N}(i)| \\geq C' N $$\nThis demonstrates that the sum is bounded below by a linear function of $N$. In asymptotic notation:\n$$ \\sum_{i=1}^{N} |\\mathcal{N}(i)| = \\Omega(N) $$\nSince the total cost $T = a \\sum_{i=1}^{N} |\\mathcal{N}(i)|$ and $a$ is a positive constant, we have:\n$$ T = \\Omega(N) $$\n\nCombining the upper and lower bounds, we have $T = O(N)$ and $T = \\Omega(N)$. By the definition of Big-Theta notation, this implies:\n$$ T = \\Theta(N) $$\nThe total cost of assembling all near-field blocks grows linearly with the number of panels $N$.", "answer": "$$\\boxed{\\Theta(N)}$$", "id": "3293963"}, {"introduction": "While asymptotic analysis provides a high-level understanding, practical performance estimation requires a more detailed cost model. This is especially true for matrix assembly, where integrals for nearby elements require special, computationally expensive quadrature rules compared to those for distant elements. This exercise [@problem_id:3294042] provides a detailed cost breakdown for you to calculate the total assembly time for a dense Method of Moments matrix, giving you a concrete sense of the $\\mathcal{O}(N^2)$ cost bottleneck and the importance of optimized quadrature.", "problem": "A dense Method of Moments (MoM) assembly of the Electric Field Integral Equation (EFIE) on a perfectly electrically conducting triangulated surface uses Rao–Wilton–Glisson (RWG) basis/testing functions. The Galerkin matrix entry for RWG–RWG testing has the canonical double surface integral form\n$$\nZ_{mn} \\;=\\; \\iint_{T_m \\times T_n} G(\\mathbf{r},\\mathbf{r}')\\, \\mathbf{f}_m(\\mathbf{r}) \\cdot \\mathbf{f}_n(\\mathbf{r}') \\,\\mathrm{d}S\\,\\mathrm{d}S',\n$$\nwhere $G(\\mathbf{r},\\mathbf{r}')$ is the free-space Green’s function and $\\mathbf{f}_m$, $\\mathbf{f}_n$ are RWG functions supported on triangles $T_m$ and $T_n$. When $T_m$ and $T_n$ are identical or adjacent, near-singular behavior arises; a Duffy transformation removes the singularity and enables accurate quadrature. For non-neighbor pairs, a standard tensor-product Gaussian quadrature suffices.\n\nAssume the following computational cost model for one RWG–RWG pair $(m,n)$ evaluated with a tensor-product rule using $q$ points per triangle:\n- The number of kernel-and-basis evaluations per pair is $q^2$.\n- The per-evaluation floating-point operation count is\n$$\nc_q \\;=\\; c_k \\;+\\; 2\\,c_b \\;+\\; c_{\\mathrm{dot}},\n$$\nwith $c_k$ the Green’s function evaluation cost, $c_b$ the cost to evaluate one RWG basis function at a quadrature point, and $c_{\\mathrm{dot}}$ the cost of a $3$-dimensional dot product of the test and source RWG vectors.\n- There is an additional per-pair overhead of $c_{\\mathrm{pair}}$ floating-point operations for loop/index management and accumulation.\n\nLet the near-singular pairs be treated by a Duffy-substituted quadrature with $q_{\\mathrm{D}}$ points per triangle, while regular pairs use Gaussian quadrature with $q_{\\mathrm{G}}$ points per triangle. The total number of unknowns is $N$, and each RWG basis function interacts near-singularly with $z_{\\mathrm{self}}$ self-pairs, $z_{\\mathrm{edge}}$ edge-adjacent RWGs, and $z_{\\mathrm{vertex}}$ vertex-adjacent RWGs; all other pairs are regular. The total assembly enumerates all ordered pairs $(m,n)$.\n\nUse the following numerical values:\n- $N \\;=\\; 4000$,\n- $z_{\\mathrm{self}} \\;=\\; 1$, $z_{\\mathrm{edge}} \\;=\\; 10$, $z_{\\mathrm{vertex}} \\;=\\; 20$,\n- $q_{\\mathrm{D}} \\;=\\; 12$, $q_{\\mathrm{G}} \\;=\\; 6$,\n- $c_k \\;=\\; 50$, $c_b \\;=\\; 30$, $c_{\\mathrm{dot}} \\;=\\; 5$, $c_{\\mathrm{pair}} \\;=\\; 500$,\n- the sustained effective performance is $F_{\\mathrm{eff}} \\;=\\; 1.2 \\times 10^{11}$ floating-point operations per second.\n\nStarting from the integral form above and the stated cost model, derive an expression for the per-pair cost difference between the Duffy-substituted near-singular treatment and the Gaussian regular treatment, and then estimate the total assembly time for all $N^2$ ordered pairs as a function of the given parameters.\n\nCompute the final total assembly time numerically using the values above. Round your answer to three significant figures and express it in seconds.", "solution": "The problem asks for a derivation of the per-pair cost difference between near-singular and regular integral calculations, an expression for the total matrix assembly time, and a numerical estimate of this time. The validation indicates the problem is well-posed and scientifically sound.\n\nFirst, we formalize the computational cost for evaluating a single matrix entry $Z_{mn}$, which corresponds to the interaction of an ordered pair of basis functions $(\\mathbf{f}_m, \\mathbf{f}_n)$. The cost depends on the number of quadrature points, $q$, used for the double surface integral.\n\nThe cost model for one pair is given by the sum of the quadrature evaluation cost and a fixed per-pair overhead. The quadrature cost is the product of the number of evaluation points and the floating-point operations (FLOPS) per evaluation. For a tensor-product rule, the number of evaluation points is $q^2$. The cost per evaluation, $c_q$, is the sum of the costs for evaluating the Green's function, two basis functions, and their dot product.\n$$\nc_q = c_k + 2c_b + c_{\\mathrm{dot}}\n$$\nThus, the total cost for one pair, as a function of $q$, is:\n$$\nC(q) = q^2 \\cdot c_q + c_{\\mathrm{pair}} = q^2 (c_k + 2c_b + c_{\\mathrm{dot}}) + c_{\\mathrm{pair}}\n$$\nThere are two types of pairs: near-singular and regular.\nNear-singular pairs are treated with a Duffy-substituted quadrature using $q_{\\mathrm{D}}$ points. The cost per near-singular pair is:\n$$\nC_{\\mathrm{Duffy}} = C(q_{\\mathrm{D}}) = q_{\\mathrm{D}}^2 (c_k + 2c_b + c_{\\mathrm{dot}}) + c_{\\mathrm{pair}}\n$$\nRegular pairs are treated with a standard Gaussian quadrature using $q_{\\mathrm{G}}$ points. The cost per regular pair is:\n$$\nC_{\\mathrm{Gauss}} = C(q_{\\mathrm{G}}) = q_{\\mathrm{G}}^2 (c_k + 2c_b + c_{\\mathrm{dot}}) + c_{\\mathrm{pair}}\n$$\nThe first part of the problem asks for the per-pair cost difference between these two treatments. This is $\\Delta C$:\n$$\n\\Delta C = C_{\\mathrm{Duffy}} - C_{\\mathrm{Gauss}}\n$$\n$$\n\\Delta C = [q_{\\mathrm{D}}^2 (c_k + 2c_b + c_{\\mathrm{dot}}) + c_{\\mathrm{pair}}] - [q_{\\mathrm{G}}^2 (c_k + 2c_b + c_{\\mathrm{dot}}) + c_{\\mathrm{pair}}]\n$$\n$$\n\\Delta C = (q_{\\mathrm{D}}^2 - q_{\\mathrm{G}}^2)(c_k + 2c_b + c_{\\mathrm{dot}})\n$$\nThis is the derived expression for the per-pair cost difference.\n\nNext, we derive the total assembly time. The assembly involves computing all $N^2$ ordered pairs $(m,n)$ for a system of $N$ unknowns. We must determine the number of near-singular and regular pairs.\n\nThe problem states that for each of the $N$ basis functions, there are $z_{\\mathrm{self}}$ self-pairs, $z_{\\mathrm{edge}}$ edge-adjacent RWGs, and $z_{\\mathrm{vertex}}$ vertex-adjacent RWGs. We assume these categories are disjoint. The total number of near-singular interactions associated with a single basis function is $Z_{\\mathrm{near\\_count}} = z_{\\mathrm{self}} + z_{\\mathrm{edge}} + z_{\\mathrm{vertex}}$. Since this applies to each of the $N$ basis functions (as rows in the matrix), the total number of near-singular pairs, $N_{\\mathrm{near}}$, is:\n$$\nN_{\\mathrm{near}} = N \\cdot Z_{\\mathrm{near\\_count}} = N(z_{\\mathrm{self}} + z_{\\mathrm{edge}} + z_{\\mathrm{vertex}})\n$$\nThe total number of pairs is $N^2$. The remaining pairs are regular. Their count, $N_{\\mathrm{reg}}$, is:\n$$\nN_{\\mathrm{reg}} = N^2 - N_{\\mathrm{near}} = N^2 - N(z_{\\mathrm{self}} + z_{\\mathrm{edge}} + z_{\\mathrm{vertex}})\n$$\nThe total computational cost in FLOPS, $C_{\\mathrm{total}}$, is the sum of costs for all pairs:\n$$\nC_{\\mathrm{total}} = N_{\\mathrm{near}} \\cdot C_{\\mathrm{Duffy}} + N_{\\mathrm{reg}} \\cdot C_{\\mathrm{Gauss}}\n$$\nSubstituting the expressions for $N_{\\mathrm{near}}$, $N_{\\mathrm{reg}}$, $C_{\\mathrm{Duffy}}$, and $C_{\\mathrm{Gauss}}$ yields the general formula for the total cost.\n\nThe total assembly time, $T_{\\mathrm{assembly}}$, is the total cost divided by the sustained effective performance, $F_{\\mathrm{eff}}$:\n$$\nT_{\\mathrm{assembly}} = \\frac{C_{\\mathrm{total}}}{F_{\\mathrm{eff}}} = \\frac{N_{\\mathrm{near}} \\cdot C_{\\mathrm{Duffy}} + N_{\\mathrm{reg}} \\cdot C_{\\mathrm{Gauss}}}{F_{\\mathrm{eff}}}\n$$\nThis can be expressed as:\n$$\nT_{\\mathrm{assembly}} = \\frac{N(z_{\\mathrm{s}}+z_{\\mathrm{e}}+z_{\\mathrm{v}})[q_{\\mathrm{D}}^2(c_k+2c_b+c_{\\mathrm{dot}})+c_{\\mathrm{pair}}] + (N^2 - N(z_{\\mathrm{s}}+z_{\\mathrm{e}}+z_{\\mathrm{v}}))[q_{\\mathrm{G}}^2(c_k+2c_b+c_{\\mathrm{dot}})+c_{\\mathrm{pair}}]}{F_{\\mathrm{eff}}}\n$$\nwhere $z_s, z_e, z_v$ are used for brevity.\n\nWe now compute the numerical value using the provided data:\n- $N = 4000$\n- $z_{\\mathrm{self}} = 1$, $z_{\\mathrm{edge}} = 10$, $z_{\\mathrm{vertex}} = 20$\n- $q_{\\mathrm{D}} = 12$, $q_{\\mathrm{G}} = 6$\n- $c_k = 50$, $c_b = 30$, $c_{\\mathrm{dot}} = 5$, $c_{\\mathrm{pair}} = 500$\n- $F_{\\mathrm{eff}} = 1.2 \\times 10^{11}$ FLOPS\n\nFirst, calculate the cost components. The cost per evaluation point is:\n$$\nc_q = c_k + 2c_b + c_{\\mathrm{dot}} = 50 + 2(30) + 5 = 50 + 60 + 5 = 115 \\; \\text{FLOPS}\n$$\nThe cost per near-singular pair is:\n$$\nC_{\\mathrm{Duffy}} = q_{\\mathrm{D}}^2 \\cdot c_q + c_{\\mathrm{pair}} = 12^2 \\cdot 115 + 500 = 144 \\cdot 115 + 500 = 16560 + 500 = 17060 \\; \\text{FLOPS}\n$$\nThe cost per regular pair is:\n$$\nC_{\\mathrm{Gauss}} = q_{\\mathrm{G}}^2 \\cdot c_q + c_{\\mathrm{pair}} = 6^2 \\cdot 115 + 500 = 36 \\cdot 115 + 500 = 4140 + 500 = 4640 \\; \\text{FLOPS}\n$$\nNext, calculate the number of pairs in each category.\nThe number of near-singular interactions per basis function is:\n$$\nZ_{\\mathrm{near\\_count}} = z_{\\mathrm{self}} + z_{\\mathrm{edge}} + z_{\\mathrm{vertex}} = 1 + 10 + 20 = 31\n$$\nThe total number of near-singular pairs is:\n$$\nN_{\\mathrm{near}} = N \\cdot Z_{\\mathrm{near\\_count}} = 4000 \\cdot 31 = 124000\n$$\nThe total number of pairs is $N^2 = 4000^2 = 16,000,000$.\nThe total number of regular pairs is:\n$$\nN_{\\mathrm{reg}} = N^2 - N_{\\mathrm{near}} = 16,000,000 - 124,000 = 15,876,000\n$$\nNow, we compute the total cost, $C_{\\mathrm{total}}$:\n$$\nC_{\\mathrm{total}} = N_{\\mathrm{near}} \\cdot C_{\\mathrm{Duffy}} + N_{\\mathrm{reg}} \\cdot C_{\\mathrm{Gauss}}\n$$\n$$\nC_{\\mathrm{total}} = (124000 \\cdot 17060) + (15,876,000 \\cdot 4640)\n$$\n$$\nC_{\\mathrm{total}} = 2,115,440,000 + 73,664,640,000 = 75,780,080,000 \\; \\text{FLOPS}\n$$\nThis is equivalent to $7.578008 \\times 10^{10}$ FLOPS.\n\nFinally, we compute the total assembly time:\n$$\nT_{\\mathrm{assembly}} = \\frac{C_{\\mathrm{total}}}{F_{\\mathrm{eff}}} = \\frac{75,780,080,000}{1.2 \\times 10^{11}} = \\frac{7.578008 \\times 10^{10}}{1.2 \\times 10^{11}} \\; \\text{s}\n$$\n$$\nT_{\\mathrm{assembly}} = \\frac{7.578008}{12} \\; \\text{s} \\approx 0.63150066... \\; \\text{s}\n$$\nRounding the result to three significant figures, we get $0.632$ seconds.", "answer": "$$\n\\boxed{0.632}\n$$", "id": "3294042"}, {"introduction": "Directly assembling the dense $\\mathcal{O}(N^2)$ matrix is a major bottleneck, and iterative solvers can perform poorly without effective preconditioning. Modern solvers overcome this by using fast algorithms, like the Fast Multipole Method (FMM), to accelerate matrix-vector products and sophisticated preconditioners to ensure rapid convergence. This practice [@problem_id:3294009] delves into the complexity of such an advanced solver, guiding you to derive the $\\mathcal{O}(N \\ln N)$ costs for both setting up (building) and using (applying) a powerful Calderón preconditioner, thereby demonstrating the principles behind scalable integral equation solvers.", "problem": "Consider the Electric Field Integral Equation (EFIE) for a perfectly electrically conducting scatterer with boundary $\\Gamma$, derived from Maxwell’s equations in the frequency domain and the tangential boundary condition $\\hat{\\mathbf{n}} \\times \\mathbf{E}^{\\mathrm{tot}} = \\mathbf{0}$ on $\\Gamma$. Let the EFIE operator be denoted by $\\mathcal{T}_{k}$, built from the electromagnetic Green’s function at wavenumber $k$, and let a Galerkin discretization with Rao–Wilton–Glisson (RWG) basis functions produce a dense system matrix $Z \\in \\mathbb{C}^{N \\times N}$ for the unknown surface current expansion coefficients. The classical Calderón identity for boundary integral operators on $\\Gamma$ states that appropriate compositions of boundary integral operators yield second-kind projectors, which motivates a multiplicative Calderón preconditioner that balances the spectral properties of the EFIE. In a commonly used discrete construction, one forms the multiplicative preconditioner\n$$\nM \\;=\\; G^{-1}\\, T_{\\tilde{k}}^{\\dagger}\\, G\\, T_{\\tilde{k}},\n$$\nwhere $T_{\\tilde{k}}$ is a discrete EFIE operator at an auxiliary wavenumber $\\tilde{k}$ (chosen to avoid internal resonances), $G$ is a sparse Gram matrix that maps between compatible trial and test spaces (e.g., RWG and Buffa–Christiansen), and $T_{\\tilde{k}}^{\\dagger}$ denotes the adjoint with respect to the discrete $L^{2}$ pairing induced by $G$. Assume that $G$ has $\\mathcal{O}(N)$ nonzeros due to local support and bounded vertex degree on a shape-regular triangulation of $\\Gamma$.\n\nTo make the complexity model explicit, adopt the following assumptions for a three-dimensional Fast Multipole Method (FMM) used for all dense operator applications and near-field assembly:\n- A matrix–vector multiply with a discretized dense boundary integral operator (e.g., $T_{\\tilde{k}}$ or $T_{\\tilde{k}}^{\\dagger}$) using the FMM costs $c_{m}\\, N\\, \\ln(N)$ floating-point operations, where $c_{m} > 0$ is a constant that captures kernel translation costs and expansion orders, and the tree has $\\mathcal{O}(\\ln(N))$ levels with $\\mathcal{O}(N)$ total box interactions.\n- Building the FMM data structures (tree, interaction lists, multipole/local expansions) for one operator costs $c_{b}\\, N\\, \\ln(N)$ floating-point operations, and assembling the near-field blocks for one operator costs $c_{n}\\, N$ floating-point operations, with $c_{b}, c_{n} > 0$ constants.\n- Applying the sparse Gram matrix $G$ to a vector costs $c_{g}\\, N$ floating-point operations, and applying a precomputed sparse approximate inverse of $G$ (e.g., via incomplete Cholesky factorization followed by triangular solves) costs $c_{L}\\, N$ floating-point operations per application, with $c_{g}, c_{L} > 0$ constants.\n- Building the sparse approximate inverse of $G$ (including assembling $G$ itself) costs $c_{g}\\, N + c_{s}\\, N$ floating-point operations, where $c_{s} > 0$ captures the factorization/preconditioner setup cost and is proportional to the number of nonzeros due to bounded fill.\n\nStarting from the Calderón identity and its implication that the EFIE composed with compatible boundary operators yields a second-kind operator, explain why the multiplicative form $M = G^{-1} T_{\\tilde{k}}^{\\dagger} G T_{\\tilde{k}}$ is spectrally favorable for preconditioning $Z$. Then, using only the stated complexity assumptions and the structure of $M$, derive closed-form expressions for:\n1. The total cost to build $M$ (including constructing $T_{\\tilde{k}}$ and $T_{\\tilde{k}}^{\\dagger}$ FMM structures and their near-field blocks, as well as assembling $G$ and building its sparse approximate inverse).\n2. The total cost to apply $M$ to a vector once (i.e., one preconditioner application inside an iterative solver).\n\nExpress both results as symbolic expressions in terms of $N$, $\\ln(N)$, and the constants $c_{m}$, $c_{b}$, $c_{n}$, $c_{g}$, $c_{L}$, and $c_{s}$. Treat $T_{\\tilde{k}}$ and $T_{\\tilde{k}}^{\\dagger}$ as requiring distinct FMM data structures of identical build and apply costs. Your final answers must be closed-form analytic expressions. Express the final costs in floating-point operation counts. Do not include any physical units. If you introduce any new symbols, define them clearly. The final answers must be provided as a single row matrix using the $\\mathrm{pmatrix}$ environment, with the first entry being the build cost and the second entry being the apply cost.", "solution": "The problem statement is evaluated to be scientifically grounded, well-posed, and objective. It is based on established principles in computational electromagnetics and provides a complete, consistent set of assumptions for deriving the computational complexity of a Calderón-preconditioned integral-equation solver. The problem is valid and a solution will be provided.\n\nThe problem asks for two main components: first, an explanation for the spectral benefits of the given multiplicative Calderón preconditioner, and second, a derivation of the computational costs for building and applying this preconditioner based on a set of explicit complexity models.\n\n**Part 1: Spectral Properties of the Calderón Preconditioner**\n\nThe Electric Field Integral Equation (EFIE) is derived from a first-kind Fredholm integral equation. Discretization of such equations, for instance, using the Method of Moments (MoM) with Rao–Wilton–Glisson (RWG) basis functions, typically results in a linear system $Z \\mathbf{x} = \\mathbf{b}$ where the matrix $Z \\in \\mathbb{C}^{N \\times N}$ is dense, non-Hermitian, and severely ill-conditioned. The singular values of the continuous EFIE operator accumulate at both zero and infinity, and this poor spectral behavior is inherited by the discrete matrix $Z$. Consequently, standard iterative solvers like GMRES converge very slowly, if at all, when applied to the raw system matrix $Z$.\n\nPreconditioning aims to transform the system into an equivalent one with more favorable spectral properties. A good preconditioner $M$ for a system $Z \\mathbf{x} = \\mathbf{b}$ results in a preconditioned matrix, e.g., $M^{-1}Z$ (left preconditioning) or $ZM^{-1}$ (right preconditioning), whose spectrum is clustered, ideally around $1$. This ensures rapid convergence of iterative solvers.\n\nThe Calderón identities are fundamental properties of boundary integral operators associated with partial differential equations like the Helmholtz equation (which governs time-harmonic electromagnetics). These identities state that specific compositions of the fundamental boundary integral operators (single-layer, double-layer, adjoint double-layer, and hypersingular operators) result in operators that are compact perturbations of the identity operator. Such operators are of the Fredholm second-kind, whose spectra are ideally suited for iterative methods.\n\nThe EFIE operator, denoted $\\mathcal{T}_k$, is an example of an ill-conditioned, first-kind operator. The multiplicative preconditioner $M = G^{-1}\\, T_{\\tilde{k}}^{\\dagger}\\, G\\, T_{\\tilde{k}}$ is a discrete algebraic construction designed to mimic a continuous Calderón identity. The components serve the following purposes:\n1.  $T_{\\tilde{k}}$: This is a discretization of the EFIE operator $\\mathcal{T}_{\\tilde{k}}$ at a non-resonant auxiliary wavenumber $\\tilde{k}$. The use of $\\tilde{k} \\neq k$ ensures that the preconditioner itself does not suffer from ill-conditioning due to internal resonances of the scattering object.\n2.  $T_{\\tilde{k}}^{\\dagger}$: This represents a discrete version of a \"dual\" operator. In the context of Calderón preconditioning, this is typically related to the Magnetic Field Integral Equation (MFIE) operator, which is naturally of the second kind. The composition of the EFIE and MFIE operators yields a well-conditioned operator.\n3.  $G$ and $G^{-1}$: The matrix $G$ is a sparse Gram matrix representing the inner product between two different sets of basis functions, such as the RWG trial functions and the Buffa–Christiansen (BC) test functions. These matrices act as discrete \"change-of-basis\" operators, mapping coefficients from one function space representation to another, which is necessary to correctly form the discrete composition of operators that correspond to the continuous Calderón identities.\n\nIn essence, the operator $M$ is constructed to be a discrete approximation of a well-behaved, second-kind operator. Therefore, the preconditioned system matrix (e.g., $M^{-1}Z$) is expected to be a discrete approximation of the identity operator plus a compact term, leading to a spectrum clustered around a single point and thereby ensuring fast convergence of the iterative solver.\n\n**Part 2: Derivation of Computational Costs**\n\nThe total costs are derived by summing the costs of the individual steps as specified in the problem's assumptions.\n\n**1. Total Cost to Build the Preconditioner $M$**\n\nThe build phase involves constructing all data structures necessary to apply the operator $M = G^{-1}\\, T_{\\tilde{k}}^{\\dagger}\\, G\\, T_{\\tilde{k}}$. We do not need to assemble the full matrix $M$. The required components are the FMM structures for $T_{\\tilde{k}}$ and $T_{\\tilde{k}}^{\\dagger}$, and the sparse approximate inverse of $G$.\n\n-   **Build cost for $T_{\\tilde{k}}$**: This involves building the FMM data structures and assembling the near-field interaction blocks.\n    $$\n    C_{\\text{build}}(T_{\\tilde{k}}) = (\\text{FMM build cost}) + (\\text{Near-field assembly cost}) = c_{b}\\, N\\, \\ln(N) + c_{n}\\, N\n    $$\n-   **Build cost for $T_{\\tilde{k}}^{\\dagger}$**: The problem states that $T_{\\tilde{k}}^{\\dagger}$ requires a distinct FMM structure with a cost identical to that of $T_{\\tilde{k}}$.\n    $$\n    C_{\\text{build}}(T_{\\tilde{k}}^{\\dagger}) = c_{b}\\, N\\, \\ln(N) + c_{n}\\, N\n    $$\n-   **Build cost for the inverse of $G$**: The problem provides a direct cost for assembling the sparse matrix $G$ and constructing its sparse approximate inverse (e.g., via incomplete factorization).\n    $$\n    C_{\\text{build}}(G^{-1}) = c_{g}\\, N + c_{s}\\, N\n    $$\nThe total build cost, $C_{\\text{build}}(M)$, is the sum of these individual costs.\n$$\nC_{\\text{build}}(M) = C_{\\text{build}}(T_{\\tilde{k}}) + C_{\\text{build}}(T_{\\tilde{k}}^{\\dagger}) + C_{\\text{build}}(G^{-1})\n$$\n$$\nC_{\\text{build}}(M) = \\left( c_{b}\\, N\\, \\ln(N) + c_{n}\\, N \\right) + \\left( c_{b}\\, N\\, \\ln(N) + c_{n}\\, N \\right) + \\left( c_{g}\\, N + c_{s}\\, N \\right)\n$$\nCombining terms, we get the final expression for the total build cost:\n$$\nC_{\\text{build}}(M) = 2\\, c_{b}\\, N\\, \\ln(N) + (2\\, c_{n} + c_{g} + c_{s})\\, N\n$$\n\n**2. Total Cost to Apply the Preconditioner $M$ to a Vector**\n\nApplying the preconditioner involves computing the matrix-vector product $\\mathbf{y} = M\\mathbf{x} = G^{-1}\\, T_{\\tilde{k}}^{\\dagger}\\, G\\, T_{\\tilde{k}}\\, \\mathbf{x}$ for a given vector $\\mathbf{x}$. This is performed as a sequence of operations from right to left.\n\n-   **Step 1: Compute $\\mathbf{v}_1 = T_{\\tilde{k}}\\, \\mathbf{x}$**: This is a matrix-vector multiplication using the FMM.\n    $$\n    C_{\\text{apply}}(T_{\\tilde{k}}) = c_{m}\\, N\\, \\ln(N)\n    $$\n-   **Step 2: Compute $\\mathbf{v}_2 = G\\, \\mathbf{v}_1$**: This is a sparse matrix-vector multiplication.\n    $$\n    C_{\\text{apply}}(G) = c_{g}\\, N\n    $$\n-   **Step 3: Compute $\\mathbf{v}_3 = T_{\\tilde{k}}^{\\dagger}\\, \\mathbf{v}_2$**: This is another FMM matrix-vector multiplication. The cost is identical to applying $T_{\\tilde{k}}$.\n    $$\n    C_{\\text{apply}}(T_{\\tilde{k}}^{\\dagger}) = c_{m}\\, N\\, \\ln(N)\n    $$\n-   **Step 4: Compute $\\mathbf{y} = G^{-1}\\, \\mathbf{v}_3$**: This involves applying the precomputed sparse approximate inverse of $G$ (e.g., forward and backward substitution with incomplete factors).\n    $$\n    C_{\\text{apply}}(G^{-1}) = c_{L}\\, N\n    $$\nThe total cost to apply $M$ once, $C_{\\text{apply}}(M)$, is the sum of the costs of these four sequential steps.\n$$\nC_{\\text{apply}}(M) = C_{\\text{apply}}(T_{\\tilde{k}}) + C_{\\text{apply}}(G) + C_{\\text{apply}}(T_{\\tilde{k}}^{\\dagger}) + C_{\\text{apply}}(G^{-1})\n$$\n$$\nC_{\\text{apply}}(M) = c_{m}\\, N\\, \\ln(N) + c_{g}\\, N + c_{m}\\, N\\, \\ln(N) + c_{L}\\, N\n$$\nCombining terms, we get the final expression for the total application cost:\n$$\nC_{\\text{apply}}(M) = 2\\, c_{m}\\, N\\, \\ln(N) + (c_{g} + c_{L})\\, N\n$$\nThe two results provide the closed-form expressions for the build and apply costs in terms of the given parameters. The leading-order complexity for both is $\\mathcal{O}(N \\ln N)$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 2 c_{b} N \\ln(N) + (2 c_{n} + c_{g} + c_{s}) N & 2 c_{m} N \\ln(N) + (c_{g} + c_{L}) N \\end{pmatrix}}\n$$", "id": "3294009"}]}