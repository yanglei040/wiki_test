{"hands_on_practices": [{"introduction": "At the heart of gradient-based shape optimization lies the concept of the shape derivative, which quantifies how a cost functional changes in response to infinitesimal perturbations of the domain boundary. This practice guides you through the derivation of the celebrated Hadamard boundary integral representation for such a derivative, starting from the Reynolds transport theorem. By applying this framework to a simplified inverse scattering model, you will derive an explicit formula for the boundary sensitivity $G(\\mathbf{y})$ and numerically verify it, providing a crucial bridge between abstract shape calculus and practical gradient computation [@problem_id:3323745]. This exercise is fundamental for understanding how the shape gradient, which drives the level-set evolution, is formally defined and constructed.", "problem": "You are tasked with deriving and implementing the Hadamard boundary integral representation of the shape derivative for a data misfit functional used in computational electromagnetics, expressed via a level-set formulation.\n\nConsider a two-dimensional time-harmonic electromagnetics setting under the first Born approximation, where the far-field pattern for a compact inclusion domain $D \\subset \\mathbb{R}^2$ is modeled by\n$$\nF_D(\\hat{\\mathbf{x}}) \\;=\\; k^2 \\chi \\int_{D} e^{i k (\\mathbf{d}-\\hat{\\mathbf{x}})\\cdot \\mathbf{y}} \\, d\\mathbf{y},\n$$\nwhere $k$ is the wavenumber, $\\chi$ is a constant contrast parameter, $\\mathbf{d}$ is the unit incidence direction, and $\\hat{\\mathbf{x}} \\in \\mathbb{S}^1$ are the observation directions. Let the measured data be $\\{d_m\\}_{m=1}^M$ for directions $\\{\\hat{\\mathbf{x}}_m\\}_{m=1}^M$ and define the least-squares misfit\n$$\nJ(D) \\;=\\; \\tfrac{1}{2} \\sum_{m=1}^{M} \\alpha_m \\,\\big| F_D(\\hat{\\mathbf{x}}_m) - d_m \\big|^2,\n$$\nwith positive weights $\\{\\alpha_m\\}$.\n\nA boundary evolution is driven by a normal velocity field $\\mathbf{V}(\\mathbf{y}) = V_n(\\mathbf{y}) \\,\\mathbf{n}(\\mathbf{y})$ on $\\partial D$, encoded by a level-set function $\\phi$ which satisfies the Hamilton–Jacobi equation\n$$\n\\partial_t \\phi + V_n \\, |\\nabla \\phi| = 0.\n$$\n\nYour tasks are:\n\n$1.$ Starting from the Reynolds transport theorem for domain integrals and the chain rule for real-valued functionals of complex data, derive the Hadamard boundary integral representation of the shape derivative\n$$\ndJ(D)[\\mathbf{V}] \\;=\\; \\int_{\\partial D} V_n(\\mathbf{y}) \\, G(\\mathbf{y}) \\, ds(\\mathbf{y}),\n$$\nand provide an explicit formula for the boundary integrand $G(\\mathbf{y})$ in terms of the residuals $z_m = F_D(\\hat{\\mathbf{x}}_m) - d_m$ and the functions $f_m(\\mathbf{y}) = k^2 \\chi \\, e^{i k (\\mathbf{d}-\\hat{\\mathbf{x}}_m)\\cdot \\mathbf{y}}$.\n\n$2.$ Implement the derived boundary integral for $G(\\mathbf{y})$ and verify it numerically by comparing $dJ(D)[\\mathbf{V}]$ against a finite-difference approximation of the directional derivative for carefully chosen velocity fields $V_n$ and perturbations of $D$. Use the following concrete setup:\n\n- Use $k = 8$ (dimensionless), $\\chi = 1$, and incidence direction $\\mathbf{d} = (1,0)$.\n- Use $M = 16$ observation directions $\\hat{\\mathbf{x}}_m = (\\cos \\theta_m, \\sin \\theta_m)$ with $\\theta_m = 2\\pi m/M$ in radians.\n- Define the true domain $D^\\star$ as an ellipse centered at $\\mathbf{c}^\\star = (0.1, -0.05)$ with semi-axes $a^\\star=0.6$ and $b^\\star=0.3$. The measured data are generated by $d_m = F_{D^\\star}(\\hat{\\mathbf{x}}_m)$ using the same Born model.\n- Define the current (test) domain $D$ for evaluation of $J$ and its shape derivative as a disk centered at $\\mathbf{c}=(0,0)$ with radius $R=0.4$.\n- Use the analytic formulas for the integrals $\\int_D e^{i k \\mathbf{q}\\cdot \\mathbf{y}}\\, d\\mathbf{y}$ to compute $F_D(\\hat{\\mathbf{x}}_m)$:\n  - For a disk of radius $R$ centered at $\\mathbf{c}$, with $\\mathbf{q}=\\mathbf{d}-\\hat{\\mathbf{x}}$, use\n    $$\n    \\int_{|\\mathbf{y}-\\mathbf{c}|\\le R} e^{i k \\mathbf{q}\\cdot \\mathbf{y}} \\, d\\mathbf{y}\n    \\;=\\;\n    e^{i k \\mathbf{q}\\cdot \\mathbf{c}} \\, \\frac{2\\pi R}{k\\|\\mathbf{q}\\|} J_1\\!\\big(k \\|\\mathbf{q}\\| R\\big),\n    $$\n    with the limiting value $\\pi R^2$ when $\\|\\mathbf{q}\\|=0$.\n  - For an ellipse centered at $\\mathbf{c}$ with semi-axes $a,b$ aligned with the coordinate axes, write $\\mathbf{S}=\\mathrm{diag}(a,b)$ and use\n    $$\n    \\int_{\\mathbf{y}:\\;(\\tfrac{y_x-c_x}{a})^2 + (\\tfrac{y_y-c_y}{b})^2 \\le 1} e^{i k \\mathbf{q}\\cdot \\mathbf{y}} \\, d\\mathbf{y}\n    \\;=\\;\n    e^{i k \\mathbf{q}\\cdot \\mathbf{c}} \\, \\frac{2\\pi a b}{k \\|\\mathbf{S}^T \\mathbf{q}\\|} J_1\\!\\big(k \\|\\mathbf{S}^T \\mathbf{q}\\|\\big),\n    $$\n    with the limiting value $\\pi a b$ when $\\|\\mathbf{S}^T \\mathbf{q}\\|=0$. Here $J_1$ is the Bessel function of the first kind of order $1$.\n\n- Use the derived boundary integrand $G(\\mathbf{y})$ and verify the Hadamard formula with the following three tests. All angles are in radians, and all final reported quantities are dimensionless floats:\n\n  - Test A (uniform normal expansion of the disk): choose $V_n(\\mathbf{y}) \\equiv 1$ on $\\partial D$. Compute the predicted shape derivative $dJ(D)[\\mathbf{V}]$ via the boundary integral. Compare with a finite-difference approximation of the directional derivative using the exactly known perturbed cost $J$ for a disk of radius $R+\\varepsilon$, with $\\varepsilon = 10^{-6}$. Report the relative error $\\big|dJ_{\\text{pred}} - dJ_{\\text{fd}}\\big| / \\max\\{ |dJ_{\\text{fd}}|, 10^{-12} \\}$.\n\n  - Test B (rigid translation of the disk): choose a translation vector $\\mathbf{t}=(0.02,-0.01)$ and take $V_n(\\mathbf{y}) = \\mathbf{t}\\cdot \\mathbf{n}(\\mathbf{y})$ on $\\partial D$. Compute $dJ(D)[\\mathbf{V}]$ via the boundary integral. Compare with a finite-difference approximation using the exactly known perturbed cost $J$ for the disk translated to center $\\mathbf{c}+\\varepsilon \\mathbf{t}$ with $\\varepsilon = 10^{-6}$. Again report the relative error defined as in Test A.\n\n  - Test C (zero-residual edge case): set the current domain $D$ equal to the true ellipse $D^\\star$ and compute the boundary integrand $G(\\mathbf{y})$ on $\\partial D$. Report $\\max_{\\mathbf{y}\\in \\partial D} |G(\\mathbf{y})|$ using a uniform angular parameterization of the ellipse boundary. This value should be close to zero.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_A,r_B,r_C]$), where $r_A$ is the float from Test A, $r_B$ is the float from Test B, and $r_C$ is the float from Test C. Angles must be interpreted in radians. No physical units are required in the output. All integrals and operations are to be computed in a mathematically consistent and numerically stable way.", "solution": "The objective is to derive the explicit form of the boundary integrand $G(\\mathbf{y})$ for the shape derivative of a least-squares data misfit functional $J(D)$ and to verify the result numerically. The process is divided into two main parts: the analytical derivation and the numerical implementation.\n\n### Part 1: Derivation of the Boundary Integrand $G(\\mathbf{y})$\n\nThe analysis begins with the data misfit functional $J(D)$, defined as:\n$$\nJ(D) \\;=\\; \\tfrac{1}{2} \\sum_{m=1}^{M} \\alpha_m \\,\\big| F_D(\\hat{\\mathbf{x}}_m) - d_m \\big|^2\n$$\nwhere $\\{d_m\\}_{m=1}^M$ are the measured far-field data, $\\{\\alpha_m\\}$ are positive weights, and $F_D(\\hat{\\mathbf{x}}_m)$ is the modeled far-field pattern for a domain $D$. The modeled data is given by a domain integral:\n$$\nF_D(\\hat{\\mathbf{x}}_m) \\;=\\; \\int_{D} f_m(\\mathbf{y}) \\, d\\mathbf{y}, \\quad \\text{with} \\quad f_m(\\mathbf{y}) = k^2 \\chi \\, e^{i k (\\mathbf{d}-\\hat{\\mathbf{x}}_m)\\cdot \\mathbf{y}}\n$$\nLet $z_m(D) = F_D(\\hat{\\mathbf{x}}_m) - d_m$ be the complex-valued residual for the $m$-th observation. The functional can be written as $J(D) = \\frac{1}{2} \\sum_{m=1}^{M} \\alpha_m |z_m(D)|^2 = \\frac{1}{2} \\sum_{m=1}^{M} \\alpha_m z_m(D) \\overline{z_m(D)}$.\n\nThe shape derivative of $J(D)$ in the direction of a normal velocity field $\\mathbf{V}(\\mathbf{y}) = V_n(\\mathbf{y})\\mathbf{n}(\\mathbf{y})$ on the boundary $\\partial D$ is given by $dJ(D)[\\mathbf{V}]$. We apply the chain rule for differentiation of a real-valued functional of complex variables:\n$$\ndJ(D)[\\mathbf{V}] = \\frac{d}{d\\tau} \\left. J(D_\\tau) \\right|_{\\tau=0}\n$$\nwhere $D_\\tau$ is the domain $D$ perturbed along the velocity field $\\mathbf{V}$. Using the product rule, the derivative of each term in the sum is:\n$$\n\\frac{d}{d\\tau} \\left( z_m(D_\\tau) \\overline{z_m(D_\\tau)} \\right) = \\frac{dz_m}{d\\tau} \\overline{z_m} + z_m \\frac{d\\overline{z_m}}{d\\tau} = 2 \\, \\text{Re} \\left( \\frac{dz_m}{d\\tau} \\overline{z_m} \\right)\n$$\nThe derivative is evaluated at $\\tau=0$. The derivative of the residual is $dz_m/d\\tau = dF_D(\\hat{\\mathbf{x}}_m)[\\mathbf{V}]$, since the measured data $d_m$ are constant with respect to the domain shape. Summing over all $m$, we obtain:\n$$\ndJ(D)[\\mathbf{V}] \\;=\\; \\sum_{m=1}^{M} \\alpha_m \\, \\text{Re} \\left( dF_D(\\hat{\\mathbf{x}}_m)[\\mathbf{V}] \\cdot \\overline{z_m(D)} \\right)\n$$\nNext, we find the shape derivative of the far-field pattern $F_D(\\hat{\\mathbf{x}}_m)$. According to the Reynolds transport theorem (or specifically, the Hadamard structure theorem for domain integrals), the derivative of an integral over a domain $D$ with a smooth integrand $f_m$ is:\n$$\ndF_D(\\hat{\\mathbf{x}}_m)[\\mathbf{V}] = \\frac{d}{d\\tau} \\left. \\int_{D_\\tau} f_m(\\mathbf{y}) \\, d\\mathbf{y} \\right|_{\\tau=0} = \\int_{\\partial D} f_m(\\mathbf{y}) (\\mathbf{V}(\\mathbf{y}) \\cdot \\mathbf{n}(\\mathbf{y})) \\, ds(\\mathbf{y})\n$$\nGiven that $\\mathbf{V}(\\mathbf{y}) = V_n(\\mathbf{y})\\mathbf{n}(\\mathbf{y})$, the dot product simplifies to $\\mathbf{V}\\cdot\\mathbf{n} = V_n$. Thus:\n$$\ndF_D(\\hat{\\mathbf{x}}_m)[\\mathbf{V}] = \\int_{\\partial D} V_n(\\mathbf{y}) f_m(\\mathbf{y}) \\, ds(\\mathbf{y})\n$$\nSubstituting this integral representation back into the expression for $dJ(D)[\\mathbf{V}]$:\n$$\ndJ(D)[\\mathbf{V}] \\;=\\; \\sum_{m=1}^{M} \\alpha_m \\, \\text{Re} \\left( \\left( \\int_{\\partial D} V_n(\\mathbf{y}) f_m(\\mathbf{y}) \\, ds(\\mathbf{y}) \\right) \\overline{z_m(D)} \\right)\n$$\nSince $\\overline{z_m(D)}$ is constant with respect to the integration variable $\\mathbf{y}$, and $V_n(\\mathbf{y})$ is real, we can bring the summation and the real part operator inside the integral:\n$$\ndJ(D)[\\mathbf{V}] \\;=\\; \\int_{\\partial D} V_n(\\mathbf{y}) \\left( \\sum_{m=1}^{M} \\alpha_m \\, \\text{Re} \\left( f_m(\\mathbf{y}) \\overline{z_m(D)} \\right) \\right) ds(\\mathbf{y})\n$$\nThis expression has the desired Hadamard boundary integral form $dJ(D)[\\mathbf{V}] = \\int_{\\partial D} V_n(\\mathbf{y}) G(\\mathbf{y}) \\, ds(\\mathbf{y})$. By direct comparison, we identify the boundary integrand $G(\\mathbf{y})$:\n$$\nG(\\mathbf{y}) \\;=\\; \\sum_{m=1}^{M} \\alpha_m \\, \\text{Re} \\left( f_m(\\mathbf{y}) \\overline{z_m(D)} \\right)\n$$\nSubstituting the definitions for $f_m$ and $z_m$, the explicit formula for the boundary integrand is:\n$$\nG(\\mathbf{y}) \\;=\\; \\text{Re} \\left( \\sum_{m=1}^{M} \\alpha_m \\left( k^2 \\chi \\, e^{i k (\\mathbf{d}-\\hat{\\mathbf{x}}_m)\\cdot \\mathbf{y}} \\right) \\overline{\\left( F_D(\\hat{\\mathbf{x}}_m) - d_m \\right)} \\right)\n$$\nThis formula provides the sensitivity of the cost functional $J(D)$ to normal perturbations of its boundary $\\partial D$ at each point $\\mathbf{y} \\in \\partial D$.\n\n### Part 2: Numerical Verification Plan\n\nThe derived formula for $G(\\mathbf{y})$ is implemented and verified against finite-difference approximations for three specific test cases. For the numerical tests, the weights are set to $\\alpha_m=1$ for all $m$.\n\n**Test A (Uniform Expansion):** The velocity is $V_n(\\mathbf{y})=1$. The predicted derivative $dJ_{\\text{pred}} = \\int_{\\partial D} G(\\mathbf{y}) \\, ds(\\mathbf{y})$ is computed via numerical quadrature. This is compared to a finite-difference approximation $dJ_{\\text{fd}} = (J(D_{R+\\varepsilon}) - J(D_R))/\\varepsilon$, where $D_R$ is the initial disk of radius $R$ and $D_{R+\\varepsilon}$ is a disk of radius $R+\\varepsilon$.\n\n**Test B (Rigid Translation):** The velocity corresponds to a rigid translation by a vector $\\mathbf{t}$, so $V_n(\\mathbf{y}) = \\mathbf{t} \\cdot \\mathbf{n}(\\mathbf{y})$. The predicted derivative $dJ_{\\text{pred}} = \\int_{\\partial D} (\\mathbf{t} \\cdot \\mathbf{n}(\\mathbf{y})) G(\\mathbf{y}) \\, ds(\\mathbf{y})$ is computed and compared to $dJ_{\\text{fd}} = (J(D_{\\mathbf{c}+\\varepsilon\\mathbf{t}}) - J(D_\\mathbf{c}))/\\varepsilon$, where the disk's center is shifted from $\\mathbf{c}$ to $\\mathbf{c}+\\varepsilon\\mathbf{t}$.\n\n**Test C (Zero-Residual Case):** The test domain $D$ is set to be the true domain $D^\\star$. In this ideal case, the residuals $z_m = F_{D^\\star}(\\hat{\\mathbf{x}}_m) - d_m$ are zero. Consequently, the boundary integrand $G(\\mathbf{y})$ should also be zero everywhere on $\\partial D^\\star$. The numerical test computes $\\max_{\\mathbf{y}\\in \\partial D^\\star} |G(\\mathbf{y})|$ to verify that it is close to machine precision zero.\n\nThe numerical implementation uses the provided analytic formulas for the far-field integral over disk and elliptical domains, involving the Bessel function $J_1(x)$.", "answer": "```python\nimport numpy as np\nfrom scipy.special import j1\n\ndef solve():\n    \"\"\"\n    Derives and implements the Hadamard boundary integral for a shape derivative\n    in computational electromagnetics and verifies it numerically.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    k = 8.0\n    chi = 1.0\n    d_inc = np.array([1.0, 0.0])\n    M = 16\n    a_m = 1.0  # Weights alpha_m\n    epsilon = 1e-6\n\n    # --- Observation Directions ---\n    m_indices = np.arange(1, M + 1)\n    thetas_obs = 2 * np.pi * m_indices / M\n    obs_dirs = np.stack((np.cos(thetas_obs), np.sin(thetas_obs)), axis=-1)\n\n    # --- Domain Definitions ---\n    # True domain D_star (ellipse)\n    c_star = np.array([0.1, -0.05])\n    a_star, b_star = 0.6, 0.3\n    \n    # Current domain D (disk) for Tests A and B\n    c_disk = np.array([0.0, 0.0])\n    R_disk = 0.4\n\n    # --- Helper Functions ---\n\n    def domain_integral(q_unscaled, center, shape, params):\n        \"\"\"\n        Computes the integral of exp(i * k * q_unscaled . y) over a domain.\n        q_unscaled is (d - x_hat).\n        \"\"\"\n        if shape == 'disk':\n            R = params['R']\n            q_norm = np.linalg.norm(q_unscaled)\n            if q_norm  1e-12:\n                return np.pi * R**2\n            \n            exp_term = np.exp(1j * k * np.dot(q_unscaled, center))\n            bessel_arg = k * q_norm * R\n            val = exp_term * (2 * np.pi * R / (k * q_norm)) * j1(bessel_arg)\n            return val\n\n        elif shape == 'ellipse':\n            a, b = params['a'], params['b']\n            S = np.diag([a, b])\n            St_q = S.T @ q_unscaled\n            St_q_norm = np.linalg.norm(St_q)\n            if St_q_norm  1e-12:\n                return np.pi * a * b\n            \n            exp_term = np.exp(1j * k * np.dot(q_unscaled, center))\n            bessel_arg = k * St_q_norm\n            val = exp_term * (2 * np.pi * a * b / bessel_arg) * j1(bessel_arg)\n            return val\n        else:\n            raise ValueError(\"Unknown shape\")\n\n    def compute_far_field(obs_dirs_list, center, shape, params):\n        \"\"\"Computes the far-field pattern F_D for a list of observation directions.\"\"\"\n        F_D_vals = np.zeros(len(obs_dirs_list), dtype=np.complex128)\n        for i, x_hat in enumerate(obs_dirs_list):\n            q_unscaled = d_inc - x_hat\n            integral_val = domain_integral(q_unscaled, center, shape, params)\n            F_D_vals[i] = (k**2 * chi) * integral_val\n        return F_D_vals\n\n    def cost_functional_J(F_D, d_measured):\n        \"\"\"Computes the least-squares cost functional J.\"\"\"\n        residuals = F_D - d_measured\n        return 0.5 * np.sum(a_m * np.abs(residuals)**2)\n\n    # --- Main Calculation ---\n    \n    # 1. Generate measured data d_m from the true domain D_star\n    d_measured = compute_far_field(obs_dirs, c_star, 'ellipse', {'a': a_star, 'b': b_star})\n    \n    # 2. Compute F_D and residuals for the current domain D (disk)\n    F_D_disk = compute_far_field(obs_dirs, c_disk, 'disk', {'R': R_disk})\n    residuals = F_D_disk - d_measured\n\n    # 3. Setup for boundary integrals (common for Test A and B)\n    N_bnd = 1000  # Number of points for boundary discretization\n    theta_bnd = np.linspace(0, 2 * np.pi, N_bnd, endpoint=False)\n    y_bnd = R_disk * np.stack((np.cos(theta_bnd), np.sin(theta_bnd)), axis=-1)\n    \n    q_unscaled_vecs = d_inc - obs_dirs\n    # f_m(y) = k^2 * chi * exp(i * k * (d-x_m).y)\n    # Shape: (M, N_bnd)\n    fm_on_bnd = (k**2 * chi) * np.exp(1j * k * (q_unscaled_vecs @ y_bnd.T))\n\n    # G(y) = Re(sum_m a_m * f_m(y) * conj(z_m))\n    # Shape: (N_bnd,)\n    G_y = np.real(np.sum(a_m * fm_on_bnd.T * np.conj(residuals), axis=1))\n    \n    # --- Test A: Uniform Expansion ---\n    Vn_A = 1.0\n    ds_A = R_disk * (2 * np.pi / N_bnd) # for uniform theta sampling\n    dJ_pred_A = np.sum(Vn_A * G_y * ds_A)\n    \n    J_D = cost_functional_J(F_D_disk, d_measured)\n    F_D_eps_A = compute_far_field(obs_dirs, c_disk, 'disk', {'R': R_disk + epsilon})\n    J_D_eps_A = cost_functional_J(F_D_eps_A, d_measured)\n    dJ_fd_A = (J_D_eps_A - J_D) / epsilon\n    \n    rel_error_A = np.abs(dJ_pred_A - dJ_fd_A) / np.maximum(np.abs(dJ_fd_A), 1e-12)\n\n    # --- Test B: Rigid Translation ---\n    t_vec = np.array([0.02, -0.01])\n    normals_bnd = y_bnd / R_disk\n    Vn_B = normals_bnd @ t_vec\n    ds_B = R_disk * (2 * np.pi / N_bnd)\n    dJ_pred_B = np.sum(Vn_B * G_y * ds_B)\n    \n    F_D_eps_B = compute_far_field(obs_dirs, c_disk + epsilon * t_vec, 'disk', {'R': R_disk})\n    J_D_eps_B = cost_functional_J(F_D_eps_B, d_measured)\n    dJ_fd_B = (J_D_eps_B - J_D) / epsilon\n    \n    rel_error_B = np.abs(dJ_pred_B - dJ_fd_B) / np.maximum(np.abs(dJ_fd_B), 1e-12)\n\n    # --- Test C: Zero-Residual Edge Case ---\n    # Current domain D is now D_star\n    F_D_star = compute_far_field(obs_dirs, c_star, 'ellipse', {'a': a_star, 'b': b_star})\n    residuals_C = F_D_star - d_measured  # Should be near machine precision zero\n\n    # Evaluate G(y) on the ellipse boundary\n    theta_bnd_ellipse = np.linspace(0, 2 * np.pi, N_bnd, endpoint=False)\n    y_bnd_ellipse_x = c_star[0] + a_star * np.cos(theta_bnd_ellipse)\n    y_bnd_ellipse_y = c_star[1] + b_star * np.sin(theta_bnd_ellipse)\n    y_bnd_ellipse = np.stack((y_bnd_ellipse_x, y_bnd_ellipse_y), axis=-1)\n    \n    fm_on_bnd_ellipse = (k**2 * chi) * np.exp(1j * k * (q_unscaled_vecs @ y_bnd_ellipse.T))\n    G_y_ellipse = np.real(np.sum(a_m * fm_on_bnd_ellipse.T * np.conj(residuals_C), axis=1))\n\n    max_G_C = np.max(np.abs(G_y_ellipse))\n\n    # --- Final Output ---\n    results = [rel_error_A, rel_error_B, max_G_C]\n    print(f\"[{','.join(f'{r:.10e}' for r in results)}]\")\n\nsolve()\n\n```", "id": "3323745"}, {"introduction": "While the Hadamard formula provides theoretical insight, many real-world inverse problems involve reconstructing volumetric parameters, like permittivity $\\varepsilon_r(\\mathbf{x})$, which are governed by a partial differential equation (PDE). This practice challenges you to implement the workhorse of PDE-constrained optimization: the adjoint-state method. You will derive and code a complete gradient assembly pipeline, linking a data misfit functional to the underlying level-set function $\\phi$ through the solution of both a forward and an adjoint Helmholtz equation [@problem_id:3323749]. This hands-on task is essential for mastering how to efficiently compute gradients for complex physical systems, a core skill in computational inverse problems.", "problem": "You are asked to derive, implement, and evaluate an adjoint-state gradient assembly for a level-set parameterization of a piecewise constant relative permittivity in a scalar frequency-domain model motivated by computational electromagnetics. The goal is to reconstruct the shape of an inclusion embedded in a homogeneous background by minimizing a data misfit between simulated and observed fields, using an adjoint-state method to obtain the gradient with respect to the level-set function.\n\nStart from the following fundamental base.\n\n1) Governing equation and domain. Consider the scalar frequency-domain model on the unit square domain with homogeneous Dirichlet boundary conditions. Let the interior grid have size $N \\times N$ with grid spacing $h = \\frac{1}{N+1}$. Let the unknown complex scalar field be $u:\\Omega \\to \\mathbb{C}$ governed by the linear partial differential equation\n$$\n-\\Delta u(\\mathbf{x}) - k_0^2 \\, \\varepsilon_r(\\mathbf{x}) \\, u(\\mathbf{x}) + i \\, \\tau \\, u(\\mathbf{x}) = f(\\mathbf{x}),\n$$\nwhere $\\mathbf{x}=(x,y)\\in \\Omega=(0,1)\\times(0,1)$, $\\Delta$ is the Laplacian, $k_0$ is a given free-space wavenumber, $\\varepsilon_r(\\mathbf{x})$ is the spatially varying relative permittivity, $i$ is the imaginary unit, $\\tau0$ is a small absorption parameter, and $f$ is a point source. The homogeneous Dirichlet boundary condition enforces $u=0$ on $\\partial \\Omega$.\n\n2) Data misfit. There are $M$ detectors located at interior grid points $\\{\\mathbf{x}_m\\}_{m=1}^M$. Define the predicted data vector $\\mathbf{d}_{\\mathrm{pred}}\\in\\mathbb{C}^M$ as samples of $u$ at these detector points, and an observed data vector $\\mathbf{d}_{\\mathrm{obs}}\\in\\mathbb{C}^M$. The objective functional is the least-squares data misfit\n$$\nJ(u) = \\tfrac{1}{2} \\sum_{m=1}^M \\left| u(\\mathbf{x}_m) - d_{\\mathrm{obs},m} \\right|^2.\n$$\n\n3) Level-set parameterization of the permittivity. Introduce a level-set function $\\phi:\\Omega\\to\\mathbb{R}$ such that the inclusion is the region where $\\phi(\\mathbf{x})  0$. The relative permittivity is parameterized as a two-phase medium via a smoothed Heaviside function $H_\\beta$:\n$$\n\\varepsilon_r(\\mathbf{x}) = \\varepsilon_{\\text{in}} + \\left(\\varepsilon_{\\text{out}} - \\varepsilon_{\\text{in}}\\right) H_\\beta\\!\\left(\\phi(\\mathbf{x})\\right),\n$$\nwhere $\\varepsilon_{\\text{in}}$ and $\\varepsilon_{\\text{out}}$ are constants. The smoothed Heaviside $H_\\beta$ with width parameter $\\beta0$ is defined by\n$$\nH_\\beta(s) = \n\\begin{cases}\n0,  s \\le -\\beta,\\\\\n\\tfrac{1}{2}\\left(1 + \\tfrac{s}{\\beta} + \\tfrac{1}{\\pi}\\sin\\left(\\tfrac{\\pi s}{\\beta}\\right)\\right),  |s|  \\beta,\\\\\n1,  s \\ge \\beta.\n\\end{cases}\n$$\nIts derivative is the smoothed Dirac delta\n$$\n\\delta_\\beta(s) = H_\\beta'(s) =\n\\begin{cases}\n0,  |s| \\ge \\beta,\\\\\n\\tfrac{1}{2\\beta}\\left(1 + \\cos\\left(\\tfrac{\\pi s}{\\beta}\\right)\\right),  |s|  \\beta.\n\\end{cases}\n$$\n\n4) Discrete model. Use a standard five-point finite-difference discretization on the $N\\times N$ interior grid to approximate $-\\Delta$ with homogeneous Dirichlet boundary conditions. Let $\\mathbf{A}(\\varepsilon_r)$ be the discrete operator such that\n$$\n\\mathbf{A}(\\varepsilon_r)\\,\\mathbf{u} = \\mathbf{b},\n$$\nwhere $\\mathbf{u}\\in\\mathbb{C}^{N^2}$ is the discretized field, and $\\mathbf{b}\\in\\mathbb{C}^{N^2}$ is the discretized point source. The operator has the form\n$$\n\\mathbf{A}(\\varepsilon_r) = \\mathbf{L} + \\operatorname{diag}\\!\\left(-k_0^2 \\,\\varepsilon_r + i\\,\\tau\\right),\n$$\nwhere $\\mathbf{L}$ is the discrete approximation of $-\\Delta$.\n\n5) Adjoint-state method. Introduce the sampling operator $\\mathbf{P}:\\mathbb{C}^{N^2}\\to\\mathbb{C}^M$ that selects entries at detector indices. The functional is $J(\\mathbf{u}) = \\tfrac{1}{2}\\|\\mathbf{P}\\mathbf{u}-\\mathbf{d}_{\\mathrm{obs}}\\|_2^2$. The adjoint state $\\mathbf{p}\\in\\mathbb{C}^{N^2}$ solves the adjoint linear system\n$$\n\\mathbf{A}(\\varepsilon_r)^\\ast \\,\\mathbf{p} = \\mathbf{P}^\\ast\\!\\left(\\mathbf{P}\\mathbf{u} - \\mathbf{d}_{\\mathrm{obs}}\\right),\n$$\nwhere $(\\cdot)^\\ast$ denotes the conjugate transpose. The gradient with respect to $\\varepsilon_r$ is obtained from the adjoint-state method by differentiating the Lagrangian with respect to $\\varepsilon_r$.\n\n6) Level-set gradient assembly. The chain rule relates the derivative with respect to the level-set field to the derivative with respect to the permittivity:\n$$\n\\frac{\\partial J}{\\partial \\phi}(\\mathbf{x}) = \\frac{\\partial J}{\\partial \\varepsilon_r}(\\mathbf{x}) \\cdot \\frac{\\partial \\varepsilon_r}{\\partial \\phi}(\\mathbf{x}) = \\frac{\\partial J}{\\partial \\varepsilon_r}(\\mathbf{x}) \\cdot \\left(\\varepsilon_{\\text{out}}-\\varepsilon_{\\text{in}}\\right)\\delta_\\beta\\!\\left(\\phi(\\mathbf{x})\\right).\n$$\n\nYour tasks are:\n\na) Derive, from first principles using variational calculus and the adjoint-state method, an explicit pointwise expression for $\\frac{\\partial J}{\\partial \\varepsilon_r}(\\mathbf{x})$ in terms of the forward field $u$ and the adjoint field $p$, given that the operator is $\\mathbf{A}(\\varepsilon_r) = \\mathbf{L} + \\operatorname{diag}(-k_0^2 \\varepsilon_r + i \\tau)$.\n\nb) Assemble the level-set gradient $\\mathcal{G}(\\mathbf{x}) = \\frac{\\partial J}{\\partial \\phi}(\\mathbf{x})$ on the grid using the chain rule in item $6$.\n\nc) Implement a complete program that, for each test case in the suite below, computes the discrete $\\ell^2$-norm of the assembled gradient,\n$$\n\\left\\|\\mathcal{G}\\right\\|_{2,h} = \\left( \\sum_{j=1}^{N^2} \\left|\\mathcal{G}_j\\right|^2 \\, h^2 \\right)^{1/2},\n$$\nand outputs one real number per test case. The final output must be a single line containing a comma-separated list of these norms enclosed in square brackets.\n\nImplementation details and common setup to be used by all test cases:\n\n- Grid: use $N=30$, $h=\\frac{1}{N+1}$, and homogeneous Dirichlet boundary conditions. Use the standard five-point stencil to build $\\mathbf{L}$, so that $\\mathbf{L} = \\mathbf{I}_y\\otimes \\mathbf{T} + \\mathbf{T}\\otimes \\mathbf{I}_x$, where $\\mathbf{T}$ is the $N\\times N$ tridiagonal matrix with main diagonal $\\frac{2}{h^2}$ and off-diagonals $-\\frac{1}{h^2}$.\n\n- Absorption: set $\\tau = 1.0$.\n\n- Source: a point source located at $(x_s,y_s)=(0.2,0.5)$, injected at the nearest interior grid node with unit amplitude in the right-hand side $\\mathbf{b}$.\n\n- Detectors: $M=10$ points. Place $8$ detectors on a ring of radius $r_d=0.45$ centered at $(0.5,0.5)$ at angles $\\theta_m = \\frac{2\\pi m}{8}$ for $m=0,1,\\dots,7$, and $2$ interior detectors at $(0.5,0.3)$ and $(0.5,0.7)$. Sample at the nearest interior grid nodes.\n\n- Level-set shapes: The level-set function is a signed distance to a circle,\n$$\n\\phi(\\mathbf{x}) = \\sqrt{(x-x_0)^2 + (y-y_0)^2} - R,\n$$\nso that the inclusion is the disk of radius $R$ centered at $(x_0,y_0)$. The background is where $\\phi(\\mathbf{x}) \\ge 0$.\n\n- Relative permittivity mapping: $\\varepsilon_r(\\mathbf{x}) = \\varepsilon_{\\text{in}} + (\\varepsilon_{\\text{out}}-\\varepsilon_{\\text{in}}) H_\\beta(\\phi(\\mathbf{x}))$.\n\n- Observed data construction: For each test case, define a “true” shape with radius $R_{\\text{true}}$ to generate $\\mathbf{d}_{\\mathrm{obs}}$ by solving the forward problem with $\\varepsilon_r$ corresponding to the true shape. The “reconstruction” shape with radius $R_{\\text{rec}}$ is used to build the operator for the forward and adjoint fields whose gradient you must assemble.\n\n- Complex arithmetic: All linear solves are complex-valued. The adjoint system uses the conjugate transpose operator $\\mathbf{A}^\\ast$.\n\nTest suite:\n\n- Case $1$ (happy path, near-zero gradient): $k_0=5.0$, $\\varepsilon_{\\text{in}}=4.0$, $\\varepsilon_{\\text{out}}=1.0$, $\\beta=2h$, $R_{\\text{true}}=0.25$, $R_{\\text{rec}}=0.25$.\n\n- Case $2$ (contrast mismatch): $k_0=5.0$, $\\varepsilon_{\\text{in}}=4.0$, $\\varepsilon_{\\text{out}}=1.0$, $\\beta=2h$, $R_{\\text{true}}=0.30$, $R_{\\text{rec}}=0.20$.\n\n- Case $3$ (edge case with smaller smoothing and reduced contrast and frequency): $k_0=3.0$, $\\varepsilon_{\\text{in}}=2.0$, $\\varepsilon_{\\text{out}}=1.0$, $\\beta=0.5h$, $R_{\\text{true}}=0.30$, $R_{\\text{rec}}=0.20$.\n\nFor each case, use the common setup above and compute the discrete $\\ell^2$-norm $\\|\\mathcal{G}\\|_{2,h}$ in real units with no special physical unit required. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{result1},\\text{result2},\\text{result3}]$). Each entry must be a floating-point number corresponding to the norm for the respective test case, in the order cases $1,2,3$.", "solution": "The problem requires the derivation and implementation of an adjoint-state method to compute the gradient of a data-misfit functional with respect to a level-set function that parameterizes the shape of an inclusion in a 2D scalar wave problem. The solution proceeds in three stages: first, we validate the problem statement; second, we derive the necessary analytical expressions for the gradient; third, we detail the numerical implementation.\n\n### Problem Validation\nThe problem is well-defined and scientifically sound.\n*   **Givens Extracted**: The governing partial differential equation, boundary conditions, objective functional, level-set parameterization, discrete model, adjoint formulation, and all numerical parameters ($N=30$, $\\tau=1.0$, source/detector locations, material properties $\\varepsilon_{\\text{in}}, \\varepsilon_{\\text{out}}$, and test-case specifics) are explicitly provided.\n*   **Scientific Grounding**: The model is based on the scalar Helmholtz equation, a standard approximation in frequency-domain electromagnetics. The use of adjoint-state methods for sensitivity analysis and level-set functions for shape optimization are established and rigorous techniques in computational inverse problems.\n*   **Well-Posedness**: The inclusion of an absorption term $\\tau > 0$ ensures that the forward operator $\\mathbf{A}(\\varepsilon_r)$ is invertible, guaranteeing a unique solution for the forward field $\\mathbf{u}$ and, consequently, for the adjoint field $\\mathbf{p}$. The task of computing the gradient is a well-defined mathematical operation.\n*   **Completeness and Consistency**: The problem statement is self-contained, providing all necessary information to proceed with both the analytical derivation and the numerical implementation. There are no contradictions in the provided data or constraints.\n\nThe problem is deemed **valid**. We now proceed to the solution.\n\n### Adjoint-State Gradient Derivation\n\nThe core task is to find the gradient of the objective functional $J$ with respect to the level-set function $\\phi$. We use the chain rule: $\\frac{\\partial J}{\\partial \\phi} = \\frac{\\partial J}{\\partial \\varepsilon_r} \\frac{\\partial \\varepsilon_r}{\\partial \\phi}$. We first derive the expression for $\\frac{\\partial J}{\\partial \\varepsilon_r}$ using the adjoint-state method.\n\nThe problem is one of constrained optimization: minimize $J(\\mathbf{u})$ subject to the state equation $\\mathbf{A}(\\boldsymbol{\\varepsilon}_r)\\mathbf{u} = \\mathbf{b}$, where $\\boldsymbol{\\varepsilon}_r$ is the vector of permittivity values at each grid point. We introduce the Lagrangian functional $\\mathcal{L}$ with the adjoint state $\\mathbf{p} \\in \\mathbb{C}^{N^2}$ as the Lagrange multiplier for the state equation constraint:\n$$\n\\mathcal{L}(\\mathbf{u}, \\mathbf{p}, \\boldsymbol{\\varepsilon}_r) = J(\\mathbf{u}) - \\Re\\left[ \\mathbf{p}^\\ast (\\mathbf{A}(\\boldsymbol{\\varepsilon}_r)\\mathbf{u} - \\mathbf{b}) \\right]\n$$\nThe objective functional in discrete form is $J(\\mathbf{u}) = \\frac{1}{2}\\|\\mathbf{P}\\mathbf{u} - \\mathbf{d}_{\\mathrm{obs}}\\|_2^2 = \\frac{1}{2}(\\mathbf{P}\\mathbf{u} - \\mathbf{d}_{\\mathrm{obs}})^\\ast(\\mathbf{P}\\mathbf{u} - \\mathbf{d}_{\\mathrm{obs}})$.\n\nThe gradient of $J$ with respect to the parameter $\\boldsymbol{\\varepsilon}_r$ is given by the partial derivative of the Lagrangian, $\\frac{dJ}{d\\boldsymbol{\\varepsilon}_r} = \\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\varepsilon}_r}$, provided that the stationarity conditions $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{u}} = 0$ and $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{p}} = 0$ hold. The latter simply recovers the state equation $\\mathbf{A}\\mathbf{u}=\\mathbf{b}$. The former defines the adjoint equation.\n\nUsing complex (Wirtinger) calculus, the stationarity condition with respect to $\\mathbf{u}$ (specifically, its conjugate $\\mathbf{u}^\\ast$) is $\\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{u}^\\ast} = 0$. We have:\n$$\n\\frac{\\partial J}{\\partial \\mathbf{u}^\\ast} = \\frac{1}{2}\\mathbf{P}^\\ast(\\mathbf{P}\\mathbf{u} - \\mathbf{d}_{\\mathrm{obs}})\n$$\nThe term $\\Re[\\mathbf{p}^\\ast \\mathbf{A}\\mathbf{u}]$ can be written as $\\frac{1}{2}(\\mathbf{p}^\\ast \\mathbf{A}\\mathbf{u} + \\mathbf{u}^\\ast \\mathbf{A}^\\ast \\mathbf{p})$. Its derivative with respect to $\\mathbf{u}^\\ast$ is $\\frac{1}{2}\\mathbf{A}^\\ast\\mathbf{p}$. The stationarity condition thus becomes:\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial \\mathbf{u}^\\ast} = \\frac{1}{2}\\mathbf{P}^\\ast(\\mathbf{P}\\mathbf{u} - \\mathbf{d}_{\\mathrm{obs}}) - \\frac{1}{2}\\mathbf{A}(\\boldsymbol{\\varepsilon}_r)^\\ast\\mathbf{p} = 0\n$$\nThis yields the **adjoint equation**:\n$$\n\\mathbf{A}(\\boldsymbol{\\varepsilon}_r)^\\ast \\mathbf{p} = \\mathbf{P}^\\ast(\\mathbf{P}\\mathbf{u} - \\mathbf{d}_{\\mathrm{obs}})\n$$\nThis matches the equation given in the problem statement. With the adjoint state $\\mathbf{p}$ defined, we can now compute the gradient with respect to the permittivity vector $\\boldsymbol{\\varepsilon}_r$. The total derivative of $J$ with respect to a single component $\\varepsilon_{r,j}$ of the permittivity vector is:\n$$\n\\frac{dJ}{d\\varepsilon_{r,j}} = \\frac{\\partial \\mathcal{L}}{\\partial \\varepsilon_{r,j}} = -\\Re\\left[ \\mathbf{p}^\\ast \\frac{\\partial \\mathbf{A}(\\boldsymbol{\\varepsilon}_r)}{\\partial \\varepsilon_{r,j}} \\mathbf{u} \\right]\n$$\nThe discrete operator is $\\mathbf{A}(\\boldsymbol{\\varepsilon}_r) = \\mathbf{L} + \\operatorname{diag}(-k_0^2 \\boldsymbol{\\varepsilon}_r + i\\tau)$. Its derivative with respect to $\\varepsilon_{r,j}$ is a matrix that is zero everywhere except for the $(j,j)$ element:\n$$\n\\frac{\\partial \\mathbf{A}(\\boldsymbol{\\varepsilon}_r)}{\\partial \\varepsilon_{r,j}} = -k_0^2 \\mathbf{e}_j \\mathbf{e}_j^T\n$$\nwhere $\\mathbf{e}_j$ is the $j$-th standard basis vector. Substituting this into the gradient expression:\n$$\n\\frac{dJ}{d\\varepsilon_{r,j}} = -\\Re\\left[ \\mathbf{p}^\\ast (-k_0^2 \\mathbf{e}_j \\mathbf{e}_j^T) \\mathbf{u} \\right] = \\Re\\left[ k_0^2 \\mathbf{p}^\\ast (\\mathbf{e}_j \\mathbf{e}_j^T \\mathbf{u}) \\right] = \\Re\\left[ k_0^2 (\\bar{p}_j u_j) \\right]\n$$\nwhere $p_j$ and $u_j$ are the $j$-th components of the adjoint and forward fields, respectively. The resulting pointwise expression for the gradient with respect to permittivity is:\n$$\n\\frac{\\partial J}{\\partial \\varepsilon_r}(\\mathbf{x}) = \\Re\\left( k_0^2 \\, \\overline{p(\\mathbf{x})} \\, u(\\mathbf{x}) \\right)\n$$\nThis completes Task (a).\n\n### Level-Set Gradient Assembly\n\nFor Task (b), we assemble the gradient with respect to the level-set function $\\phi$ using the provided chain rule:\n$$\n\\mathcal{G}(\\mathbf{x}) = \\frac{\\partial J}{\\partial \\phi}(\\mathbf{x}) = \\frac{\\partial J}{\\partial \\varepsilon_r}(\\mathbf{x}) \\cdot \\frac{\\partial \\varepsilon_r}{\\partial \\phi}(\\mathbf{x})\n$$\nThe problem specifies the derivative of the permittivity parameterization:\n$$\n\\frac{\\partial \\varepsilon_r}{\\partial \\phi}(\\mathbf{x}) = (\\varepsilon_{\\text{out}} - \\varepsilon_{\\text{in}}) \\delta_\\beta(\\phi(\\mathbf{x}))\n$$\nwhere $\\delta_\\beta$ is the smoothed Dirac delta function. Combining this with the result from Task (a), we obtain the final expression for the level-set gradient at each grid point $\\mathbf{x}$:\n$$\n\\mathcal{G}(\\mathbf{x}) = \\Re\\left( k_0^2 \\, \\overline{p(\\mathbf{x})} \\, u(\\mathbf{x}) \\right) \\cdot (\\varepsilon_{\\text{out}} - \\varepsilon_{\\text{in}}) \\delta_\\beta(\\phi(\\mathbf{x}))\n$$\n\n### Algorithmic Procedure\n\nFor Task (c), we implement a program that follows these steps for each test case:\n1.  **Setup**: Define grid parameters ($N=30, h=1/31$), grid coordinates, discrete Laplacian operator $\\mathbf{L}$, source vector $\\mathbf{b}$, and detector locations/indices.\n2.  **Generate Observed Data**: For a given test case, construct the \"true\" permittivity $\\boldsymbol{\\varepsilon}_{r, \\text{true}}$ using the true radius $R_{\\text{true}}$. Assemble the true system matrix $\\mathbf{A}_{\\text{true}}$ and solve the forward problem $\\mathbf{A}_{\\text{true}}\\mathbf{u}_{\\text{true}} = \\mathbf{b}$ for $\\mathbf{u}_{\\text{true}}$. The observed data is $\\mathbf{d}_{\\mathrm{obs}} = \\mathbf{P}\\mathbf{u}_{\\text{true}}$.\n3.  **Solve Forward Problem**: Construct the \"reconstruction\" permittivity $\\boldsymbol{\\varepsilon}_{r, \\text{rec}}$ using the reconstruction radius $R_{\\text{rec}}$. Assemble the corresponding matrix $\\mathbf{A}_{\\text{rec}}$ and solve the forward problem $\\mathbf{A}_{\\text{rec}}\\mathbf{u} = \\mathbf{b}$ to find the state field $\\mathbf{u}$.\n4.  **Solve Adjoint Problem**: Calculate the data residual $\\mathbf{r} = \\mathbf{P}\\mathbf{u} - \\mathbf{d}_{\\mathrm{obs}}$. Construct the adjoint source $\\mathbf{b}_{\\text{adj}} = \\mathbf{P}^\\ast \\mathbf{r}$. Solve the adjoint system $\\mathbf{A}_{\\text{rec}}^\\ast \\mathbf{p} = \\mathbf{b}_{\\text{adj}}$ for the adjoint field $\\mathbf{p}$.\n5.  **Assemble Gradient**:\n    *   Compute the permittivity gradient term at each grid point $j$: $\\left(\\frac{\\partial J}{\\partial \\varepsilon_r}\\right)_j = \\Re(k_0^2 \\, \\bar{p}_j \\, u_j)$.\n    *   Compute the level-set function $\\phi(\\mathbf{x}_j)$ using $R_{\\text{rec}}$.\n    *   Compute the chain rule factor at each grid point $j$: $\\left(\\frac{\\partial \\varepsilon_r}{\\partial \\phi}\\right)_j = (\\varepsilon_{\\text{out}} - \\varepsilon_{\\text{in}}) \\delta_\\beta(\\phi(\\mathbf{x}_j))$.\n    *   Combine them to get the level-set gradient vector: $\\mathcal{G}_j = \\left(\\frac{\\partial J}{\\partial \\varepsilon_r}\\right)_j \\cdot \\left(\\frac{\\partial \\varepsilon_r}{\\partial \\phi}\\right)_j$.\n6.  **Compute Norm**: Calculate the discrete $\\ell^2$-norm of the gradient: $\\|\\mathcal{G}\\|_{2,h} = \\left( \\sum_{j=1}^{N^2} |\\mathcal{G}_j|^2 h^2 \\right)^{1/2}$. This value is the result for the test case.\n\nThis procedure is repeated for all test cases in the suite.", "answer": "```python\nimport numpy as np\nfrom scipy import sparse\nfrom scipy.sparse.linalg import spsolve\n\ndef solve():\n    \"\"\"\n    Solves the adjoint-state gradient assembly problem for the given test cases.\n    \"\"\"\n    # Common setup parameters\n    N = 30\n    h = 1.0 / (N + 1)\n    tau = 1.0\n    src_pos = (0.2, 0.5)\n    \n    # Detector positions\n    det_pos = []\n    # 8 detectors on a ring\n    r_d, x_c, y_c = 0.45, 0.5, 0.5\n    for m in range(8):\n        angle = 2.0 * np.pi * m / 8.0\n        det_pos.append((x_c + r_d * np.cos(angle), y_c + r_d * np.sin(angle)))\n    # 2 interior detectors\n    det_pos.extend([(0.5, 0.3), (0.5, 0.7)])\n    \n    # Test case suite\n    test_cases = [\n        # (k0, eps_in, eps_out, beta_factor, R_true, R_rec)\n        (5.0, 4.0, 1.0, 2.0, 0.25, 0.25),\n        (5.0, 4.0, 1.0, 2.0, 0.30, 0.20),\n        (3.0, 2.0, 1.0, 0.5, 0.30, 0.20),\n    ]\n\n    # --- Grid and Operator Setup ---\n    # Create 1D and 2D grid coordinates\n    grid_1d = np.linspace(h, 1.0 - h, N)\n    X, Y = np.meshgrid(grid_1d, grid_1d)\n    coords_flat = np.vstack((X.ravel(), Y.ravel())).T\n\n    # 1D Laplacian matrix T\n    T_diags = [-1.0, 2.0, -1.0]\n    T_offsets = [-1, 0, 1]\n    T = sparse.diags(T_diags, T_offsets, shape=(N, N)) / h**2\n    \n    # 2D Laplacian operator L using Kronecker sum\n    I_N = sparse.eye(N)\n    L = sparse.kronsum(T, T).tocsc()\n\n    # Source vector b\n    src_x_idx = int(round(src_pos[0] / h)) - 1\n    src_y_idx = int(round(src_pos[1] / h)) - 1\n    src_flat_idx = src_y_idx * N + src_x_idx\n    b = np.zeros(N**2, dtype=complex)\n    b[src_flat_idx] = 1.0\n\n    # Detector indices\n    det_indices = []\n    for x_d, y_d in det_pos:\n        det_x_idx = int(round(x_d / h)) - 1\n        det_y_idx = int(round(y_d / h)) - 1\n        det_indices.append(det_y_idx * N + det_x_idx)\n    det_indices = np.array(det_indices)\n\n    # --- Helper Functions ---\n    def H_beta(s, beta):\n        res = np.zeros_like(s)\n        mask_mid = np.abs(s)  beta\n        s_mid = s[mask_mid]\n        res[s >= beta] = 1.0\n        res[mask_mid] = 0.5 * (1.0 + s_mid / beta + (1.0 / np.pi) * np.sin(np.pi * s_mid / beta))\n        return res\n\n    def delta_beta(s, beta):\n        res = np.zeros_like(s)\n        mask = np.abs(s)  beta\n        s_mid = s[mask]\n        res[mask] = (0.5 / beta) * (1.0 + np.cos(np.pi * s_mid / beta))\n        return res\n    \n    def get_epsilon_r(R, beta, eps_in, eps_out):\n        phi_center = (0.5, 0.5)\n        dist = np.sqrt((coords_flat[:, 0] - phi_center[0])**2 + (coords_flat[:, 1] - phi_center[1])**2)\n        phi = dist - R\n        return eps_in + (eps_out - eps_in) * H_beta(phi, beta)\n\n    results = []\n    for case in test_cases:\n        k0, eps_in, eps_out, beta_factor, R_true, R_rec = case\n        beta = beta_factor * h\n\n        # 1. Generate observed data d_obs\n        eps_r_true = get_epsilon_r(R_true, beta, eps_in, eps_out)\n        A_true_diag = -k0**2 * eps_r_true + 1j * tau\n        A_true = L + sparse.diags(A_true_diag)\n        u_true = spsolve(A_true, b)\n        d_obs = u_true[det_indices]\n\n        # 2. Solve forward problem for reconstruction state u\n        eps_r_rec = get_epsilon_r(R_rec, beta, eps_in, eps_out)\n        A_rec_diag = -k0**2 * eps_r_rec + 1j * tau\n        A_rec = L + sparse.diags(A_rec_diag)\n        u = spsolve(A_rec, b)\n\n        # 3. Solve adjoint problem for p\n        residual = u[det_indices] - d_obs\n        b_adj = np.zeros(N**2, dtype=complex)\n        b_adj[det_indices] = residual\n        \n        A_adj = A_rec.conj().T\n        p = spsolve(A_adj, b_adj)\n\n        # 4. Assemble level-set gradient G\n        dJ_deps = np.real(k0**2 * np.conj(p) * u)\n        \n        phi_center = (0.5, 0.5)\n        dist_rec = np.sqrt((coords_flat[:, 0] - phi_center[0])**2 + (coords_flat[:, 1] - phi_center[1])**2)\n        phi_rec = dist_rec - R_rec\n        \n        deps_dphi = (eps_out - eps_in) * delta_beta(phi_rec, beta)\n        \n        G = dJ_deps * deps_dphi\n        \n        # 5. Compute the norm of the gradient\n        norm_G = np.sqrt(np.sum(np.abs(G)**2) * h**2)\n        results.append(norm_G)\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3323749"}, {"introduction": "Once the shape gradient has been computed, it provides a descent direction, which is used as a velocity field $V_n$ to update the boundary. In the level-set method, this update is achieved by solving the Hamilton-Jacobi evolution equation, $\\partial_t \\phi + V_n |\\nabla \\phi| = 0$, often augmented with regularization terms. This practice focuses on the critical task of numerically solving this PDE, where you will implement and compare a simple central-difference scheme against a more robust and stable upwind Godunov scheme [@problem_id:3323768]. This exercise provides invaluable experience with the numerical techniques required to accurately and stably evolve the level-set function, preventing the formation of oscillations and ensuring the convergence of the shape reconstruction process.", "problem": "You are tasked with deriving and implementing two numerical discretization schemes for a level-set Partial Differential Equation (PDE) used in shape reconstruction for Computational Electromagnetics. Start from fundamental principles and build the algorithm from first principles. The ultimate goal is to compare the schemes on a controlled set of synthetic velocity fields that mimic descent directions derived from shape gradients in inverse electromagnetic scattering.\n\nConsider a two-dimensional computational domain $\\Omega = [0,1]\\times[0,1]$ discretized by a uniform Cartesian grid with $N\\times N$ points (with $N$ specified in the test suite). Let the level-set function be denoted by $\\phi(x,y,t)$, with the convention that the predicted shape corresponds to the sublevel set $\\{\\phi0\\}$. The evolution equation is\n$$\n\\frac{\\partial \\phi}{\\partial t} + V(x,y)\\, \\lvert\\nabla \\phi\\rvert \\;=\\; \\mu\\,\\kappa(\\phi)\\,\\lvert\\nabla \\phi\\rvert,\n$$\nwhere $V(x,y)$ is a prescribed normal velocity field representing a descent direction proportional to the shape gradient, $\\mu0$ is a curvature regularization parameter, and $\\kappa(\\phi)$ is the mean curvature of the level set defined by\n$$\n\\kappa(\\phi) \\;=\\; \\nabla \\cdot \\left(\\frac{\\nabla \\phi}{\\lvert \\nabla \\phi \\rvert}\\right),\n$$\nwith standard $\\epsilon$-regularization in denominators to avoid division by zero.\n\nThe electromagnetics context and how $V(x,y)$ arises: In frequency-domain Maxwell equations for Transverse Electric (TE) polarization, the total electric field $E_z$ satisfies the Helmholtz equation derived from Maxwell's equations in inhomogeneous media. A typical inverse problem minimizes a misfit functional $J(\\Omega)=\\frac{1}{2}\\int_{\\Gamma}\\lvert E^{\\mathrm{comp}}(\\Omega)-E^{\\mathrm{meas}}\\rvert^2\\,\\mathrm{d}s$, where $\\Gamma$ is an observation curve. The shape derivative or topological derivative of $J$ provides a descent direction whose projection onto the normal yields a velocity $V(x,y)$ suitable for level-set evolution, i.e., interface normal motion drives $J$ downward. In this task, $V(x,y)$ will be provided as synthetic fields that are scientifically plausible proxies for such descent directions.\n\nYour tasks are:\n- Derive from first principles two consistent semi-discrete approximations for the Hamilton–Jacobi term $V(x,y)\\lvert\\nabla \\phi\\rvert$ and for the curvature term.\n    1. A monotone upwind Godunov scheme for the Hamilton–Jacobi term that respects the sign of $V(x,y)$ and uses one-sided differences to approximate $\\lvert\\nabla \\phi\\rvert$ consistently with causality.\n    2. A central-difference baseline scheme for the Hamilton–Jacobi term using centered differences for $\\nabla \\phi$ and computing $\\lvert\\nabla \\phi\\rvert$ from those centered differences.\n- For the curvature term, use a central-difference approximation of $\\nabla \\phi$, construct the unit normal $n=\\nabla\\phi/\\lvert\\nabla\\phi\\rvert$, and approximate $\\kappa(\\phi)=\\nabla\\cdot n$ with centered differences. Use an $\\epsilon$-regularized denominator $\\lvert\\nabla\\phi\\rvert_{\\epsilon}=\\sqrt{\\phi_x^2+\\phi_y^2+\\epsilon^2}$ for $\\epsilon0$.\n- Adopt homogeneous Neumann boundary conditions (zero normal derivative), implemented by setting boundary finite differences to zero where one-sided stencils would otherwise extend outside the domain.\n- Use an explicit forward Euler time integration $t^{n+1}=t^n+\\Delta t$. Choose the time step $\\Delta t$ by a Courant–Friedrichs–Lewy (CFL) condition with factor $c_{\\mathrm{CFL}}$ and grid spacing $h$, namely $\\Delta t = c_{\\mathrm{CFL}}\\frac{h}{\\max_{x,y}\\lvert V(x,y)\\rvert+\\delta}$, where $\\delta$ is a small positive number to avoid division by zero when $V\\equiv 0$.\n\nInitialization and units:\n- Initialize $\\phi(x,y,0)$ as the signed distance to a circle of radius $r_0$ centered at $(x_0,y_0)$, i.e.,\n$$\n\\phi(x,y,0) = \\sqrt{(x-x_0)^2+(y-y_0)^2}-r_0,\n$$\nwith $(x_0,y_0)=(0.5,0.5)$ and $r_0=0.25$. All quantities are dimensionless.\n\nImplement the two schemes and perform a single time step for each test case. For each test case, compute and return a quantitative discrepancy metric between the two schemes:\n$$\n\\mathrm{err} = \\left(\\frac{1}{N^2}\\sum_{i,j}\\left(\\phi^{n+1}_{\\mathrm{upwind}}(i,j)-\\phi^{n+1}_{\\mathrm{central}}(i,j)\\right)^2\\right)^{1/2}.\n$$\n\nTest suite:\n- Use $N=64$, domain $[0,1]\\times[0,1]$, grid spacing $h=1/(N-1)$, and forward Euler with a single time step. For each test case, set $(\\mu,\\epsilon,c_{\\mathrm{CFL}})$ as specified.\n    1. Happy path: $V(x,y)=\\alpha \\exp\\left(-\\frac{(x-0.5)^2+(y-0.5)^2}{\\sigma^2}\\right)$ with $\\alpha=2.0$, $\\sigma^2=0.02$, $\\mu=0.01$, $\\epsilon=10^{-6}$, $c_{\\mathrm{CFL}}=0.25$.\n    2. Boundary-driven: $V(x,y)=\\alpha$ for $x0.05$ and $V(x,y)=0$ otherwise; $\\alpha=1.5$, $\\mu=0.005$, $\\epsilon=10^{-6}$, $c_{\\mathrm{CFL}}=0.30$.\n    3. Curvature-dominated: $V(x,y)=0$ identically; $\\mu=0.02$, $\\epsilon=10^{-6}$, $c_{\\mathrm{CFL}}=0.45$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, i.e., $[\\mathrm{err}_1,\\mathrm{err}_2,\\mathrm{err}_3]$, where $\\mathrm{err}_k$ is the discrepancy for test case $k$. All outputs are unitless floating-point numbers.", "solution": "The objective is to derive and implement two distinct numerical schemes for the level-set evolution equation given by:\n$$\n\\frac{\\partial \\phi}{\\partial t} + V(x,y)\\, \\lvert\\nabla \\phi\\rvert \\;=\\; \\mu\\,\\kappa(\\phi)\\,\\lvert\\nabla \\phi\\rvert\n$$\nThis partial differential equation (PDE) models the evolution of a level-set function $\\phi(x,y,t)$. The equation can be rearranged to highlight its components: an advective Hamilton-Jacobi (HJ) term and a regularizing curvature term. The semi-discretized form is:\n$$\n\\frac{d\\phi}{dt} = -V(x,y)\\,\\lvert\\nabla \\phi\\rvert + \\mu\\,\\kappa(\\phi)\\,\\lvert\\nabla \\phi\\rvert = -H(\\phi) + C(\\phi)\n$$\nwhere $H(\\phi)$ represents the HJ term and $C(\\phi)$ the curvature term. We will develop two schemes that differ in their approximation of the Hamilton-Jacobi term, $H(\\phi)$, while utilizing a common, centrally-discretized approximation for the curvature term, $C(\\phi)$.\n\nWe consider a uniform Cartesian grid over the domain $\\Omega = [0,1]\\times[0,1]$ with $N \\times N$ points. The grid spacing is $h = 1/(N-1)$ in both $x$ and $y$ directions. The discrete level-set function at grid point $(x_i, y_j) = (ih, jh)$ and time $t_n = n\\Delta t$ is denoted by $\\phi_{i,j}^n$.\n\nThe time evolution is handled by an explicit forward Euler scheme:\n$$\n\\phi_{i,j}^{n+1} = \\phi_{i,j}^n + \\Delta t \\left( -H_{i,j}(\\phi^n) + C_{i,j}(\\phi^n) \\right)\n$$\nThe time step $\\Delta t$ is determined by a Courant-Friedrichs-Lewy (CFL) condition:\n$$\n\\Delta t = c_{\\mathrm{CFL}}\\frac{h}{\\max_{i,j}\\lvert V_{i,j}\\rvert+\\delta}\n$$\nwhere $c_{\\mathrm{CFL}}$ is the CFL factor and $\\delta$ is a small positive constant (we choose $\\delta=10^{-9}$) to prevent division by zero.\n\n### Spatial Discretization\nHomogeneous Neumann boundary conditions are imposed by setting any finite difference stencil that would require points outside the domain to zero.\n\n#### Curvature Term Discretization (Common to Both Schemes)\nThe curvature term is $C(\\phi) = \\mu\\,\\kappa(\\phi)\\,\\lvert\\nabla \\phi\\rvert$. The mean curvature is $\\kappa(\\phi) = \\nabla \\cdot \\left( \\frac{\\nabla \\phi}{\\lvert\\nabla \\phi\\rvert} \\right)$.\n\n1.  **Gradient Approximation**: We first approximate $\\nabla \\phi$ at each grid point $(i,j)$ using second-order central differences:\n    $$\n    (\\phi_x)_{i,j} = \\frac{\\phi_{i+1,j} - \\phi_{i-1,j}}{2h}, \\quad (\\phi_y)_{i,j} = \\frac{\\phi_{i,j+1} - \\phi_{i,j-1}}{2h}\n    $$\n    At the boundaries, these derivatives are set to zero per the Neumann condition. For instance, at $i=0$, $(\\phi_x)_{0,j} = 0$.\n\n2.  **Regularized Gradient Magnitude**: The magnitude of the gradient is regularized to prevent division by zero:\n    $$\n    \\lvert\\nabla\\phi\\rvert_{\\epsilon, i,j} = \\sqrt{(\\phi_x)_{i,j}^2 + (\\phi_y)_{i,j}^2 + \\epsilon^2}\n    $$\n\n3.  **Unit Normal Vector**: The components of the unit normal vector $n = (n_x, n_y)$ are then computed:\n    $$\n    (n_x)_{i,j} = \\frac{(\\phi_x)_{i,j}}{\\lvert\\nabla\\phi\\rvert_{\\epsilon, i,j}}, \\quad (n_y)_{i,j} = \\frac{(\\phi_y)_{i,j}}{\\lvert\\nabla\\phi\\rvert_{\\epsilon, i,j}}\n    $$\n\n4.  **Curvature (Divergence of Normal)**: The curvature $\\kappa = \\nabla \\cdot n$ is also approximated using central differences:\n    $$\n    \\kappa_{i,j} = \\frac{(n_x)_{i+1,j} - (n_x)_{i-1,j}}{2h} + \\frac{(n_y)_{i,j+1} - (n_y)_{i,j-1}}{2h}\n    $$\n    At boundaries, derivatives like $\\frac{\\partial n_x}{\\partial x}$ are set to zero. This requires computing $n_x$ and $n_y$ on the entire grid first, then applying the central difference operator for the divergence.\n\n5.  **Full Curvature Term**: The complete discretized curvature term is:\n    $$\n    C_{i,j}(\\phi) = \\mu \\cdot \\kappa_{i,j} \\cdot \\lvert\\nabla\\phi\\rvert_{\\epsilon, i,j}\n    $$\n\n#### Hamilton-Jacobi Term Discretization\nThis term, $H(\\phi) = V(x,y)\\lvert\\nabla\\phi\\rvert$, is where the two schemes diverge.\n\n**1. Central Difference Scheme**\nThis is a baseline, non-upwind scheme. It uses the same central difference approximations as the curvature term.\n$$\nH_{i,j}^{\\text{central}}(\\phi) = V_{i,j} \\cdot \\lvert\\nabla\\phi\\rvert_{\\epsilon, i,j}\n$$\nwhere $\\lvert\\nabla\\phi\\rvert_{\\epsilon, i,j}$ is the same regularized gradient magnitude computed previously from central differences of $\\phi$.\n\n**2. Upwind Godunov Scheme**\nThis scheme provides a more stable discretization for Hamilton-Jacobi equations by using one-sided differences selected based on the direction of information flow, which is determined by the sign of the velocity field $V(x,y)$.\n\nFirst, we define the one-sided finite difference operators:\n-   Forward differences: $D_x^+\\phi_{i,j} = \\frac{\\phi_{i+1,j}-\\phi_{i,j}}{h}$, $D_y^+\\phi_{i,j} = \\frac{\\phi_{i,j+1}-\\phi_{i,j}}{h}$\n-   Backward differences: $D_x^-\\phi_{i,j} = \\frac{\\phi_{i,j}-\\phi_{i-1,j}}{h}$, $D_y^-\\phi_{i,j} = \\frac{\\phi_{i,j}-\\phi_{i,j-1}}{h}$\nAt boundaries, stencils pointing outward are set to zero.\n\nThe numerical Hamiltonian is constructed based on an Engquist-Osher scheme. We split the velocity $V_{i,j}$ into its positive and negative parts, $V_{i,j}^+ = \\max(V_{i,j}, 0)$ and $V_{i,j}^- = \\min(V_{i,j}, 0)$.\n\n-   For $V_{i,j}  0$ (outward propagation), information flows from the interior of the shape ($\\phi  0$). The upwind gradient magnitude is:\n    $$\n    |\\nabla\\phi|_{i,j}^{+, \\text{upwind}} = \\sqrt{ \\left(\\max(D_x^-\\phi_{i,j}, 0)\\right)^2 + \\left(\\min(D_x^+\\phi_{i,j}, 0)\\right)^2 + \\left(\\max(D_y^-\\phi_{i,j}, 0)\\right)^2 + \\left(\\min(D_y^+\\phi_{i,j}, 0)\\right)^2 }\n    $$\n-   For $V_{i,j}  0$ (inward propagation), information flows from the exterior ($\\phi  0$). The upwind gradient magnitude is:\n    $$\n    |\\nabla\\phi|_{i,j}^{-, \\text{upwind}} = \\sqrt{ \\left(\\min(D_x^-\\phi_{i,j}, 0)\\right)^2 + \\left(\\max(D_x^+\\phi_{i,j}, 0)\\right)^2 + \\left(\\min(D_y^-\\phi_{i,j}, 0)\\right)^2 + \\left(\\max(D_y^+\\phi_{i,j}, 0)\\right)^2 }\n    $$\n\nThe full upwind Hamilton-Jacobi term is the sum of these contributions:\n$$\nH_{i,j}^{\\text{upwind}}(\\phi) = V_{i,j}^+ \\cdot |\\nabla\\phi|_{i,j}^{+, \\text{upwind}} + V_{i,j}^- \\cdot |\\nabla\\phi|_{i,j}^{-, \\text{upwind}}\n$$\nNote that for numerical stability, a small $\\epsilon$ term is implicitly added inside the square roots when implementing.\n\n### Final Algorithm\nFor each test case:\n1.  Initialize $\\phi^0$ as the signed distance to the circle.\n2.  Calculate the time step $\\Delta t$.\n3.  Compute the common curvature term $C(\\phi^0)$ using central differences.\n4.  Compute the central-difference HJ term $H^{\\text{central}}(\\phi^0)$.\n5.  Compute the upwind HJ term $H^{\\text{upwind}}(\\phi^0)$.\n6.  Perform one forward Euler step for each scheme:\n    $$\n    \\phi^{\\text{central}, 1} = \\phi^0 + \\Delta t \\left( -H^{\\text{central}}(\\phi^0) + C(\\phi^0) \\right)\n    $$\n    $$\n    \\phi^{\\text{upwind}, 1} = \\phi^0 + \\Delta t \\left( -H^{\\text{upwind}}(\\phi^0) + C(\\phi^0) \\right)\n    $$\n7.  Calculate the root mean square discrepancy between the two resulting fields:\n    $$\n    \\mathrm{err} = \\left(\\frac{1}{N^2}\\sum_{i,j=0}^{N-1}\\left(\\phi^{\\text{upwind}, 1}_{i,j}-\\phi^{\\text{central}, 1}_{i,j}\\right)^2\\right)^{1/2}\n    $$\nThe differences in the HJ term approximations $H^{\\text{central}}$ and $H^{\\text{upwind}}$ are the sole source of this discrepancy.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements two numerical schemes for a level-set PDE,\n    and computes the discrepancy between them for a set of test cases.\n    \"\"\"\n\n    def get_initial_phi(x, y, x0=0.5, y0=0.5, r0=0.25):\n        \"\"\"Initializes phi as a signed distance to a circle.\"\"\"\n        return np.sqrt((x - x0)**2 + (y - y0)**2) - r0\n\n    def get_velocity_field(x, y, case):\n        \"\"\"Generates the velocity field V(x,y) for a given test case.\"\"\"\n        if case == 1:\n            alpha, sigma2 = 2.0, 0.02\n            return alpha * np.exp(-((x - 0.5)**2 + (y - 0.5)**2) / sigma2)\n        elif case == 2:\n            alpha = 1.5\n            V = np.zeros_like(x)\n            V[x  0.05] = alpha\n            return V\n        elif case == 3:\n            return np.zeros_like(x)\n        return None\n\n    def compute_derivatives_central(phi, h):\n        \"\"\"Computes central differences for the gradient.\"\"\"\n        phi_x = np.zeros_like(phi)\n        phi_y = np.zeros_like(phi)\n\n        phi_x[1:-1, :] = (phi[2:, :] - phi[:-2, :]) / (2 * h)\n        phi_y[:, 1:-1] = (phi[:, 2:] - phi[:, :-2]) / (2 * h)\n        \n        return phi_x, phi_y\n\n    def compute_derivatives_one_sided(phi, h):\n        \"\"\"Computes one-sided differences.\"\"\"\n        phi_x_p = np.zeros_like(phi)\n        phi_x_m = np.zeros_like(phi)\n        phi_y_p = np.zeros_like(phi)\n        phi_y_m = np.zeros_like(phi)\n\n        phi_x_p[:-1, :] = (phi[1:, :] - phi[:-1, :]) / h\n        phi_x_m[1:, :] = (phi[1:, :] - phi[:-1, :]) / h\n        \n        phi_y_p[:, :-1] = (phi[:, 1:] - phi[:, :-1]) / h\n        phi_y_m[:, 1:] = (phi[:, 1:] - phi[:, :-1]) / h\n        \n        return phi_x_p, phi_x_m, phi_y_p, phi_y_m\n\n    def compute_rhs(phi, h, V, mu, epsilon, scheme):\n        \"\"\"Computes the right-hand side of the PDE evolution.\"\"\"\n        # Common Curvature Term Calculation\n        phi_x_c, phi_y_c = compute_derivatives_central(phi, h)\n        grad_phi_mag_central_eps = np.sqrt(phi_x_c**2 + phi_y_c**2 + epsilon**2)\n        \n        nx = phi_x_c / grad_phi_mag_central_eps\n        ny = phi_y_c / grad_phi_mag_central_eps\n\n        nx_x, _ = compute_derivatives_central(nx, h)\n        _, ny_y = compute_derivatives_central(ny, h)\n        \n        kappa = nx_x + ny_y\n        \n        curvature_term = mu * kappa * grad_phi_mag_central_eps\n\n        # Hamilton-Jacobi Term Calculation\n        if scheme == 'central':\n            hj_term = V * grad_phi_mag_central_eps\n        elif scheme == 'upwind':\n            phi_x_p, phi_x_m, phi_y_p, phi_y_m = compute_derivatives_one_sided(phi, h)\n            \n            V_pos = np.maximum(V, 0)\n            V_neg = np.minimum(V, 0)\n\n            grad_mag_sq_pos = (np.maximum(phi_x_m, 0))**2 + (np.minimum(phi_x_p, 0))**2 + \\\n                              (np.maximum(phi_y_m, 0))**2 + (np.minimum(phi_y_p, 0))**2\n            \n            grad_mag_sq_neg = (np.minimum(phi_x_m, 0))**2 + (np.maximum(phi_x_p, 0))**2 + \\\n                              (np.minimum(phi_y_m, 0))**2 + (np.maximum(phi_y_p, 0))**2\n\n            hj_term = V_pos * np.sqrt(grad_mag_sq_pos + epsilon**2) + \\\n                      V_neg * np.sqrt(grad_mag_sq_neg + epsilon**2)\n        \n        return -hj_term + curvature_term\n\n    # General parameters\n    N = 64\n    h = 1.0 / (N - 1)\n    delta = 1e-9\n    \n    # Grid setup\n    x_1d = np.linspace(0.0, 1.0, N)\n    y_1d = np.linspace(0.0, 1.0, N)\n    x, y = np.meshgrid(x_1d, y_1d, indexing='ij')\n\n    # Initial condition\n    phi_0 = get_initial_phi(x, y)\n\n    # Test suite\n    test_cases = [\n        # (case_id, mu, epsilon, cfl_factor)\n        (1, 0.01, 1e-6, 0.25),\n        (2, 0.005, 1e-6, 0.30),\n        (3, 0.02, 1e-6, 0.45)\n    ]\n    \n    results = []\n    \n    for case_id, mu, epsilon, cfl_factor in test_cases:\n        # Get velocity field for the case\n        V = get_velocity_field(x, y, case_id)\n        \n        # Calculate time step\n        max_v = np.max(np.abs(V))\n        dt = cfl_factor * h / (max_v + delta)\n\n        # Compute one step for central scheme\n        rhs_central = compute_rhs(phi_0, h, V, mu, epsilon, 'central')\n        phi_central_step = phi_0 + dt * rhs_central\n\n        # Compute one step for upwind scheme\n        rhs_upwind = compute_rhs(phi_0, h, V, mu, epsilon, 'upwind')\n        phi_upwind_step = phi_0 + dt * rhs_upwind\n\n        # Calculate discrepancy\n        err = np.sqrt(np.mean((phi_upwind_step - phi_central_step)**2))\n        results.append(err)\n\n    print(f\"[{','.join(f'{r:.10f}' for r in results)}]\")\n\nsolve()\n```", "id": "3323768"}]}