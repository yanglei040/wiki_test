{"hands_on_practices": [{"introduction": "Parallelizing stencil-based solvers like the Finite-Difference Time-Domain (FDTD) method requires dividing the computational domain among multiple processors. This decomposition inherently creates boundaries where processors must exchange data, introducing communication overhead. This exercise will build your ability to model performance by having you derive the communication cost from first principles for a \"pencil\" decomposition, a common and efficient strategy. By analyzing the scaling behavior for a large number of processors $P$, you will learn to identify the dominant communication bottlenecks that can limit the scalability of a parallel FDTD code [@problem_id:3336971].", "problem": "A three-dimensional finite-difference time-domain (FDTD) solver for Computational Electromagnetics (CEM) updates Electric and Magnetic field components on a structured grid of size $n_{x} \\times n_{y} \\times n_{z}$ per time step using an explicit stencil. The grid is distributed across $P$ processes using a pencil decomposition aligned with the $z$-axis: processes are arranged in a two-dimensional grid of size $P_{x} \\times P_{y}$, with $P = P_{x} P_{y}$, such that each process owns a subdomain of size $\\frac{n_{x}}{P_{x}} \\times \\frac{n_{y}}{P_{y}} \\times n_{z}$. The halo (ghost region) thickness required by the stencil in the $x$ and $y$ directions is $\\delta \\geq 1$ grid cells. Along the $z$ direction, no interprocess communication is required because each process spans the full $z$ extent.\n\nAssume the code explicitly exchanges the following data per time step:\n- Face halos with the four face-neighbor processes in the $x$ and $y$ directions (two in $x$, two in $y$).\n- Edge (diagonal-in-plane) halos with the four diagonal neighbor processes in the $x$-$y$ plane (combinations of $\\pm x$, $\\pm y$). These edge regions have a $\\delta \\times \\delta$ cross-section extruded along $z$.\n- Corner halos corresponding to $\\pm x$, $\\pm y$, $\\pm z$ triple neighbors are not present under this pencil decomposition, since there are no interprocess neighbors along $z$.\n\nEach grid cell carries $c$ field degrees of freedom to be communicated (for example, in a Yee scheme, $c=6$ accounting for three Electric and three Magnetic field components), and each degree of freedom is communicated in $b$ bytes. Define the total communication volume per time step as the sum, over all processes, of the bytes sent for face, edge, and corner exchanges.\n\nStarting from first principles of domain decomposition geometry and halo exchange, derive expressions for the total bytes per time step contributed by faces, edges, and corners, in terms of $n_{x}$, $n_{y}$, $n_{z}$, $P_{x}$, $P_{y}$, $\\delta$, $c$, and $b$. Then specialize to the balanced case $P_{x} = P_{y} = \\sqrt{P}$ and the near-cubic grid $n_{x} = n_{y} = n_{z} = n$, which you may denote by $n_{x} = n_{y} = n_{z} = n$ for the purpose of asymptotic comparison. In this balanced cubic case, determine which of the three contributions (faces, edges, corners) dominates as $P$ becomes large, and provide the dominant-term expression for the total communication volume per time step as a single closed-form analytic expression.\n\nExpress the final dominant-term communication volume in bytes. No numerical evaluation is required.", "solution": "The problem statement is first validated against the required criteria.\n\n### Step 1: Extract Givens\n- **Global Grid Size**: $n_{x} \\times n_{y} \\times n_{z}$ cells.\n- **Total Processes**: $P$.\n- **Process Grid**: $P_{x} \\times P_{y}$, where $P = P_{x} P_{y}$.\n- **Decomposition**: Pencil decomposition along the $z$-axis.\n- **Subdomain Size per Process**: $\\frac{n_{x}}{P_{x}} \\times \\frac{n_{y}}{P_{y}} \\times n_{z}$.\n- **Halo Thickness**: $\\delta$ grid cells in the $x$ and $y$ directions.\n- **Communication Pattern**:\n    - Face halos with four neighbors in the $x-y$ plane.\n    - Edge halos with four diagonal neighbors in the $x-y$ plane.\n    - No corner halo exchanges involving the $z$ direction.\n- **Data per Grid Cell**: $c$ degrees of freedom.\n- **Data Size**: $b$ bytes per degree of freedom.\n- **Definition of Total Communication Volume**: The sum, over all processes, of bytes sent for face, edge, and corner exchanges per time step.\n- **Special Case for Analysis**: Balanced decomposition $P_{x} = P_{y} = \\sqrt{P}$ and cubic grid $n_{x} = n_{y} = n_{z} = n$.\n- **Objective**: Derive expressions for face, edge, and corner communication volumes; then, in the special case, identify the dominant contribution for large $P$ and provide the dominant-term expression for the total volume.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is based on established concepts in high-performance computing and computational electromagnetics. Pencil domain decomposition and halo exchange are standard, fundamental techniques for parallelizing solvers for partial differential equations (like Maxwell's equations) on structured grids. The setup is scientifically and algorithmically sound.\n- **Well-Posed**: The problem is clearly defined with all necessary parameters ($n_x, n_y, n_z, P_x, P_y, \\delta, c, b$) and a precise geometric setup. The objective is to derive an analytical expression, which is a well-defined mathematical task.\n- **Objective**: The problem is stated using precise, unambiguous technical language, free of subjective content.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or ambiguity. It is a standard analysis problem in parallel computing.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Solution Derivation\n\nThe total communication volume per time step, $V_{\\text{total}}$, is the sum of the volumes for face, edge, and corner exchanges. The problem defines this as the sum of all bytes *sent* by all processes. We can calculate this by identifying all inter-process data exchanges, calculating the volume of data for each, and summing them up. An equivalent method is to count the number of distinct communication interfaces, determine the data sent across each interface (in both directions), and sum these contributions.\n\n**1. Face Communication Volume ($V_{\\text{face}}$)**\n\nFace communication occurs across the boundaries of adjacent subdomains in the $x$ and $y$ directions.\n\n- **Communication along $x$-faces**: The process grid has $P_x$ columns and $P_y$ rows. There are $(P_x - 1)$ vertical boundaries between process columns. Each of these boundaries spans $P_y$ processes in the $y$-direction. Thus, there are $(P_x - 1) P_y$ distinct interfaces in the $x$ direction. Across each interface, data is sent in both directions. The data sent is a slab of grid cells of size $\\delta \\times \\frac{n_y}{P_y} \\times n_z$.\nThe volume of data sent by one process across one $x$-face is:\n$$V_{\\text{send}, x} = \\left(\\delta \\times \\frac{n_y}{P_y} \\times n_z\\right) \\times c \\times b$$\nThe total volume sent across all $x$-faces is twice the number of interfaces multiplied by this volume (once for each direction of transfer):\n$$V_{\\text{face}, x} = 2 \\times (P_x - 1) P_y \\times \\left(\\delta \\cdot \\frac{n_y}{P_y} \\cdot n_z \\cdot c \\cdot b\\right) = 2 (P_x - 1) \\delta n_y n_z c b$$\n\n- **Communication along $y$-faces**: Similarly, there are $(P_y - 1)$ horizontal boundaries between process rows, each spanning $P_x$ processes. This gives $(P_y - 1) P_x$ interfaces. The data sent is a slab of size $\\frac{n_x}{P_x} \\times \\delta \\times n_z$.\nThe total volume sent across all $y$-faces is:\n$$V_{\\text{face}, y} = 2 \\times (P_y - 1) P_x \\times \\left(\\frac{n_x}{P_x} \\cdot \\delta \\cdot n_z \\cdot c \\cdot b\\right) = 2 (P_y - 1) \\delta n_x n_z c b$$\n\nThe total face communication volume is the sum:\n$$V_{\\text{face}} = V_{\\text{face}, x} + V_{\\text{face}, y} = 2 \\delta n_z c b \\left[ (P_x - 1) n_y + (P_y - 1) n_x \\right]$$\n\n**2. Edge Communication Volume ($V_{\\text{edge}}$)**\n\nEdge communication occurs with diagonal neighbors in the $x-y$ plane. These exchanges happen at the \"corners\" where four process subdomains meet. The number of such internal corners in the $P_x \\times P_y$ process grid is $(P_x - 1)(P_y - 1)$.\nAt each such corner, four processes meet. Let's label them by their grid indices: $(i, j)$, $(i+1, j)$, $(i, j+1)$, and $(i+1, j+1)$. The diagonal exchanges are between $(i, j)$ and $(i+1, j+1)$, and between $(i+1, j)$ and $(i, j+1)$. For each pair, communication is bidirectional. For example, process $(i,j)$ sends data to $(i+1,j+1)$, and $(i+1,j+1)$ sends to $(i,j)$. This results in a total of $4$ messages being sent per internal corner of the process grid.\nThe data sent for an edge exchange is a \"pencil\" of cells with cross-section $\\delta \\times \\delta$ and full $z$-extent $n_z$. The volume of data for one such message is:\n$$V_{\\text{send}, \\text{edge}} = (\\delta \\times \\delta \\times n_z) \\times c \\times b = \\delta^2 n_z c b$$\nThe total edge communication volume is the number of internal process corners times $4$ sends per corner, times the volume per send:\n$$V_{\\text{edge}} = (P_x - 1)(P_y - 1) \\times 4 \\times (\\delta^2 n_z c b) = 4 (P_x - 1)(P_y - 1) \\delta^2 n_z c b$$\n\n**3. Corner Communication Volume ($V_{\\text{corner}}$)**\n\nThe problem statement explicitly notes that due to the pencil decomposition (no decomposition along $z$), there are no interprocess neighbors corresponding to corners in all three dimensions ($\\pm x, \\pm y, \\pm z$). Therefore, this communication volume is zero.\n$$V_{\\text{corner}} = 0$$\n\n**4. Specialization and Asymptotic Analysis**\n\nWe now specialize these expressions for the balanced, cubic case: $n_x = n_y = n_z = n$ and $P_x = P_y = \\sqrt{P}$.\n\n- Specialized Face Volume:\n$$V_{\\text{face}} = 2 \\delta n c b \\left[ (\\sqrt{P} - 1) n + (\\sqrt{P} - 1) n \\right] = 2 \\delta n c b \\left[ 2 n (\\sqrt{P} - 1) \\right]$$\n$$V_{\\text{face}} = 4 \\delta n^2 c b (\\sqrt{P} - 1)$$\n\n- Specialized Edge Volume:\n$$V_{\\text{edge}} = 4 (\\sqrt{P} - 1)(\\sqrt{P} - 1) \\delta^2 n c b = 4 \\delta^2 n c b (\\sqrt{P} - 1)^2$$\n$$V_{\\text{edge}} = 4 \\delta^2 n c b (P - 2\\sqrt{P} + 1)$$\n\nTo determine the dominant contribution as $P$ becomes large, we compare the leading-order terms of $V_{\\text{face}}$ and $V_{\\text{edge}}$ with respect to $P$.\n- For $V_{\\text{face}}$, the leading term is proportional to $\\sqrt{P}$:\n$$V_{\\text{face}} \\approx 4 \\delta n^2 c b \\sqrt{P} \\quad (\\text{for large } P)$$\nThe scaling is $O(\\sqrt{P})$.\n\n- For $V_{\\text{edge}}$, the leading term is proportional to $P$:\n$$V_{\\text{edge}} \\approx 4 \\delta^2 n c b P \\quad (\\text{for large } P)$$\nThe scaling is $O(P)$.\n\nSince $P$ grows faster than $\\sqrt{P}$, the edge communication volume ($V_{\\text{edge}}$) is the dominant contribution for large $P$.\n\nThe total communication volume is $V_{\\text{total}} = V_{\\text{face}} + V_{\\text{edge}} + V_{\\text{corner}}$.\n$$V_{\\text{total}} = 4 \\delta n^2 c b (\\sqrt{P} - 1) + 4 \\delta^2 n c b (P - 2\\sqrt{P} + 1) + 0$$\n$$V_{\\text{total}} = (4 \\delta^2 n c b) P + (4 \\delta n^2 c b - 8 \\delta^2 n c b) \\sqrt{P} + (4 \\delta^2 n c b - 4 \\delta n^2 c b)$$\nThe problem asks for the dominant-term expression for the total communication volume. This is the term with the highest power of $P$.\nThe dominant term is the one proportional to $P$ derived from the edge communication.\n\nDominant-Term Expression: $4 \\delta^2 n c b P$.\nThis represents the total bytes sent per time step due to the dominant communication pattern (edge exchanges) in the limit of a large number of processes.", "answer": "$$\n\\boxed{4 \\delta^{2} n c b P}\n$$", "id": "3336971"}, {"introduction": "When debugging parallel code, you may have noticed that running the same simulation with a different number of threads can produce slightly different numerical results. This complicates validation and can hide more serious bugs. This phenomenon stems from a fundamental and often overlooked property of computer arithmetic: floating-point addition is not associative. This practice will challenge you to diagnose the root cause of this non-reproducibility in the context of a parallel reduction, a ubiquitous operation in iterative solvers. By critically evaluating several potential solutions, you will gain a deeper appreciation for the nuances of floating-point arithmetic and learn techniques to write robust and numerically reproducible scientific software [@problem_id:3336896].", "problem": "A three-dimensional computational electromagnetics simulation solves the linear system $A \\mathbf{x} = \\mathbf{b}$ arising from a discretization of Maxwell’s equations, where $A \\in \\mathbb{R}^{n \\times n}$ and $\\mathbf{b} \\in \\mathbb{R}^{n}$ are real-valued. An iterative Krylov method computes the residual $\\mathbf{r}^{(k)} = \\mathbf{b} - A \\mathbf{x}^{(k)}$ at iteration $k$ and uses the Euclidean norm $\\|\\mathbf{r}^{(k)}\\|_{2} = \\sqrt{\\sum_{i=1}^{n} \\left(r^{(k)}_{i}\\right)^{2}}$ for convergence and preconditioning decisions. In a shared-memory parallel implementation using Open Multi-Processing (OpenMP), the inner sum $\\sum_{i=1}^{n} \\left(r^{(k)}_{i}\\right)^{2}$ is computed with an OpenMP reduction. The hardware arithmetic conforms to the Institute of Electrical and Electronics Engineers (IEEE) Standard for Floating-Point Arithmetic (IEEE 754) with rounding to nearest ties-to-even, and double-precision unit roundoff $u$.\n\nFrom first principles, consider the floating-point rounding model $\\operatorname{fl}(a \\oplus b) = (a \\oplus b)(1 + \\delta)$, where $\\oplus$ denotes an exact arithmetic operation, $\\operatorname{fl}(\\cdot)$ denotes the floating-point result, and $|\\delta| \\le u$. In particular, for addition, $\\operatorname{fl}(a + b) = (a + b)(1 + \\delta)$ with $|\\delta| \\le u$. The parallel reduction in OpenMP may associate partial sums in implementation-defined orders depending on thread count, scheduling, and reduction tree shape.\n\nWhich of the following statements are correct about computing $\\|\\mathbf{r}^{(k)}\\|_{2}$ with OpenMP reductions and the numerical reproducibility of the resulting norm across runs and thread counts?\n\nA. Using an OpenMP reduction with the operator $+$ on double-precision ensures bitwise identical results across different numbers of threads for the same input, because $+$ is associative.\n\nB. Different parenthesizations of the sum $\\sum_{i=1}^{n} \\left(r^{(k)}_{i}\\right)^{2}$ can produce different floating-point results; however, pairwise summation in a balanced binary tree reduces the worst-case relative error from $\\mathcal{O}(n u)$ to $\\mathcal{O}(\\log n \\, u)$, and if the reduction tree is constructed deterministically (independent of thread scheduling) the computed sum is bitwise reproducible across runs that use the same tree.\n\nC. Performing Kahan compensated summation within each thread, followed by an OpenMP reduction with the operator $+$ to combine thread-local compensated sums, guarantees bitwise reproducibility across any thread count.\n\nD. Replacing reduction with atomic additions of $\\left(r^{(k)}_{i}\\right)^{2}$ into a single global accumulator guarantees both accuracy and reproducibility, because atomic operations serialize the additions.\n\nE. Using an exact accumulator based on a floating-point expansion or a superaccumulator (for example, binned summation that collects contributions into fixed bins deterministically) yields a sum independent of summation order; with a fixed binning and rounding once to nearest ties-to-even, the result is bitwise reproducible across runs and thread counts.\n\nSelect all that apply.", "solution": "The analysis of this problem requires a firm understanding of floating-point arithmetic as defined by the Institute of Electrical and Electronics Engineers (IEEE) $754$ standard and its implications for parallel computation. The core issue is that floating-point addition is not associative, which has profound consequences for reproducibility in parallel reductions.\n\nThe problem statement is first validated according to the required procedure.\n\n### Step 1: Extract Givens\n- A linear system $A \\mathbf{x} = \\mathbf{b}$ is being solved, where $A \\in \\mathbb{R}^{n \\times n}$ and $\\mathbf{b} \\in \\mathbb{R}^{n}$.\n- The context is a three-dimensional computational electromagnetics simulation.\n- The solution method is an iterative Krylov method.\n- At each iteration $k$, the residual is computed: $\\mathbf{r}^{(k)} = \\mathbf{b} - A \\mathbf{x}^{(k)}$.\n- The Euclidean norm of the residual, $\\|\\mathbf{r}^{(k)}\\|_{2} = \\sqrt{\\sum_{i=1}^{n} \\left(r^{(k)}_{i}\\right)^{2}}$, is used.\n- The inner sum, $S = \\sum_{i=1}^{n} \\left(r^{(k)}_{i}\\right)^{2}$, is computed in parallel using an OpenMP reduction.\n- Hardware arithmetic follows the IEEE $754$ standard for floating-point arithmetic with rounding to nearest ties-to-even.\n- The precision is double-precision, with unit roundoff $u$.\n- The floating-point rounding model for an exact operation $\\oplus$ is $\\operatorname{fl}(a \\oplus b) = (a \\oplus b)(1 + \\delta)$, where $|\\delta| \\le u$. Specifically, for addition, $\\operatorname{fl}(a + b) = (a + b)(1 + \\delta)$.\n- The OpenMP reduction may associate partial sums in implementation-defined orders.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem describes a standard, critical issue in high-performance scientific computing. The use of Krylov methods for CEM problems, the calculation of residuals and norms, and the challenges of parallel reductions with floating-point arithmetic are all well-established and fundamental topics in numerical analysis and computational science. The floating-point model is the standard one used in error analysis. The statement is scientifically sound.\n- **Well-Posed:** The problem provides a clear computational scenario and asks for an evaluation of specific statements concerning numerical reproducibility. It is well-defined and allows for a unique analysis based on the principles of computer arithmetic.\n- **Objective:** The language is formal, precise, and free of any subjectivity or ambiguity.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is scientifically sound, well-posed, and objective, providing a sufficient basis for a rigorous analysis of the options. The solution process may proceed.\n\n### Principle-Based Derivation and Option-by-Option Analysis\n\nThe central principle is the non-associativity of floating-point addition. While for any real numbers $a, b, c$, it holds that $(a+b)+c = a+(b+c)$, this is not generally true for their floating-point representations. Let $\\operatorname{fl}(\\cdot)$ denote the result of a floating-point operation. In general, $\\operatorname{fl}(\\operatorname{fl}(a+b)+c) \\neq \\operatorname{fl}(a+\\operatorname{fl}(b+c))$. This is because rounding errors are introduced at each step, and the magnitude of the error depends on the magnitude of the operands.\n\nAn OpenMP reduction, e.g., `#pragma omp parallel for reduction(+:sum)`, partitions the loop iterations among a set of threads. Each thread computes a partial sum. These partial sums are then combined (reduced) to form the final total sum. The OpenMP standard does not mandate a specific order for combining these partial sums. The order can depend on the number of threads, the runtime scheduler, and the specific implementation of the reduction a compiler chooses (e.g., a simple sequential reduction of partial sums, or a tree-based reduction). Since the order of operations (the \"parenthesization\" of the sum) can change from run to run, and floating-point addition is not associative, the final computed sum is not guaranteed to be bitwise identical across runs.\n\nLet's evaluate each option based on this principle.\n\n**A. Using an OpenMP reduction with the operator $+$ on double-precision ensures bitwise identical results across different numbers of threads for the same input, because $+$ is associative.**\n\n- **Analysis:** This statement contains a critical flaw in its premise. While the mathematical addition operator '$+$' is associative for real numbers, its finite-precision floating-point counterpart is **not** associative. For example, consider $a = 1.0$, $b = 10^{20}$, and $c = -10^{20}$ in double precision. Then $\\operatorname{fl}(\\operatorname{fl}(a+b)+c) = \\operatorname{fl}(1.0+10^{20}-10^{20}) = \\operatorname{fl}(10^{20}-10^{20})=0.0$. But $\\operatorname{fl}(a+\\operatorname{fl}(b+c)) = \\operatorname{fl}(1.0 + \\operatorname{fl}(10^{20}-10^{20})) = \\operatorname{fl}(1.0+0.0) = 1.0$. This demonstrates non-associativity. Since an OpenMP reduction may change the order of operations (parenthesization) when the thread count changes, it will not produce bitwise identical results.\n- **Verdict:** **Incorrect**.\n\n**B. Different parenthesizations of the sum $\\sum_{i=1}^{n} \\left(r^{(k)}_{i}\\right)^{2}$ can produce different floating-point results; however, pairwise summation in a balanced binary tree reduces the worst-case relative error from $\\mathcal{O}(n u)$ to $\\mathcal{O}(\\log n \\, u)$, and if the reduction tree is constructed deterministically (independent of thread scheduling) the computed sum is bitwise reproducible across runs that use the same tree.**\n\n- **Analysis:** This statement consists of three correct claims.\n  1. \"Different parenthesizations... can produce different floating-point results\": This is true, as explained by the non-associativity of floating-point addition.\n  2. \"pairwise summation... reduces the worst-case relative error...\": This is a classic result in numerical analysis. A simple sequential sum has a worst-case error bound proportional to the number of terms, $\\mathcal{O}(n u)$. Summing in a balanced binary tree (a form of pairwise summation) keeps the intermediate sums closer in magnitude, reducing error accumulation. The error bound for this method is indeed $\\mathcal{O}(\\log_2 n \\cdot u)$.\n  3. \"if the reduction tree is constructed deterministically... the computed sum is bitwise reproducible\": If the exact sequence of additions is fixed, and the underlying floating-point arithmetic is deterministic (as per IEEE $754$), then for the same input vector $\\mathbf{r}^{(k)}$, the result must be bitwise identical. A deterministic reduction tree fixes this sequence.\n- **Verdict:** **Correct**.\n\n**C. Performing Kahan compensated summation within each thread, followed by an OpenMP reduction with the operator $+$ to combine thread-local compensated sums, guarantees bitwise reproducibility across any thread count.**\n\n- **Analysis:** Kahan summation is a technique to improve the accuracy of a sum by tracking the rounding error in a compensation variable. While this significantly improves the accuracy of the partial sums computed within each thread, the problem of combining these partial sums remains. The final step described is \"an OpenMP reduction with the operator $+$ to combine thread-local compensated sums\". This final reduction is still subject to non-associativity. If the number of threads changes, the number of partial sums to be combined changes, and the non-deterministic reduction order will very likely lead to a different final result. The statement's claim of guaranteeing reproducibility across *any* thread count is false.\n- **Verdict:** **Incorrect**.\n\n**D. Replacing reduction with atomic additions of $\\left(r^{(k)}_{i}\\right)^{2}$ into a single global accumulator guarantees both accuracy and reproducibility, because atomic operations serialize the additions.**\n\n- **Analysis:** Atomic operations (like `omp atomic update`) ensure that updates to a shared memory location happen without interference from other threads, preventing data races. They do serialize the additions. However, they do **not** impose a specific *order* on these serial additions. The order in which threads gain access to the atomic variable is non-deterministic and depends on the thread scheduler. Since the summation order is not fixed, and floating-point addition is not associative, the final result will not be reproducible across runs. Furthermore, this approach turns a parallelizable sum into a effectively serial process with high contention overhead, destroying performance. It also does not improve accuracy over a simple sequential sum.\n- **Verdict:** **Incorrect**.\n\n**E. Using an exact accumulator based on a floating-point expansion or a superaccumulator (for example, binned summation that collects contributions into fixed bins deterministically) yields a sum independent of summation order; with a fixed binning and rounding once to nearest ties-to-even, the result is bitwise reproducible across runs and thread counts.**\n\n- **Analysis:** This statement describes a robust method for achieving reproducible sums. A superaccumulator or floating-point expansion stores the sum with enough precision to avoid any intermediate rounding errors. All inputs are added to this exact representation. Since real number addition is associative and commutative, the final exact sum is independent of the order in which the terms are added. The final step is to round this one exact sum to the target floating-point format (e.g., double precision). Since the exact sum is unique and the rounding rule (ties-to-even) is deterministic, the final floating-point result is always the same, regardless of thread count, scheduling, or summation order. Binned summation is a practical algorithm that approximates this ideal by grouping summands by exponent to minimize rounding error, and if done deterministically, it also produces reproducible results. This statement correctly describes a valid technique for achieving bitwise reproducibility.\n- **Verdict:** **Correct**.", "answer": "$$\\boxed{BE}$$", "id": "3336896"}, {"introduction": "Modern processors, especially GPUs, often provide significantly higher throughput for single-precision (binary32) arithmetic compared to double-precision (binary64). This creates a compelling opportunity to accelerate simulations, but it comes with the risk of reduced accuracy. This exercise guides you through the process of navigating this crucial trade-off. You will learn to formulate and solve an optimization problem that finds the ideal blend of precisions to maximize performance while satisfying a strict error tolerance, a key skill for developing cutting-edge computational software [@problem_id:3336885].", "problem": "Consider the time-domain integration of Maxwell’s equations in vacuum using the Finite-Difference Time-Domain (FDTD) method on a staggered Yee grid. Let the discrete field update per time step be an explicit linear map whose operator norm in the discrete electromagnetic energy norm is $1$ (energy-preserving stability under the Courant–Friedrichs–Lewy condition). The computation is performed on a Graphics Processing Unit (GPU), and arithmetic is executed either in single precision (binary32) or double precision (binary64).\n\nAssume the standard floating-point model: for any basic arithmetic operation, the computed result $\\mathrm{fl}(x \\,\\mathrm{op}\\, y)$ satisfies $\\mathrm{fl}(x \\,\\mathrm{op}\\, y) = (x \\,\\mathrm{op}\\, y)\\,(1+\\delta)$ with $|\\delta| \\le u$, where $u$ is the unit roundoff. Let $u_{32} = 2^{-24}$ for binary32 and $u_{64} = 2^{-53}$ for binary64. Suppose each per-component local field update per time step uses a fixed set of arithmetic operations; in a mixed-precision scheme, a fraction $p \\in [0,1]$ of these operations are executed in binary64 and the remainder in binary32. Let $c>0$ be a constant that captures the algorithmic sensitivity of the per-step local update to the per-operation rounding errors such that, for each field component, the magnitude of the per-step rounding error is bounded above by $c$ times an appropriate convex combination of the unit roundoffs of the participating operations.\n\nLet $N$ be the total number of time steps to be taken, and let $\\varepsilon>0$ be a target bound on the absolute error in any field component after $N$ steps. Under the energy-preserving assumption, roundoff errors do not amplify across time steps in the energy norm. The GPU delivers sustained throughput of $S$ floating-point operations per second in binary32 and $\\gamma S$ in binary64, with $0<\\gamma<1$. If the workload per time step comprises $W$ floating-point operations and the mixture is characterized by the fraction $p$ of binary64 operations, the effective throughput is modeled by\n$$\nT(p) \\;=\\; \\left(\\frac{1-p}{S} + \\frac{p}{\\gamma S}\\right)^{-1}.\n$$\nAssume that $N c u_{64}  \\varepsilon  N c u_{32}$, which guarantees that the optimal fraction lies strictly within $(0,1)$.\n\nDetermine the choice $p^{\\star}$ that minimizes the total wall-clock time to complete $N$ steps subject to the error constraint after $N$ steps being at most $\\varepsilon$, and express $p^{\\star}$ in closed form in terms of $N$, $c$, $u_{32}$, $u_{64}$, and $\\varepsilon$. Provide your final answer as an analytic expression for $p^{\\star}$.", "solution": "We begin from Maxwell’s equations in vacuum, whose FDTD discretization on the Yee grid yields an explicit, linear update of the form\n$$\n\\mathbf{u}^{n+1} \\;=\\; \\mathcal{A}\\,\\mathbf{u}^{n},\n$$\nwhere $\\mathbf{u}^{n}$ collects the discrete electric and magnetic field components at time step $n$, and $\\mathcal{A}$ is the update operator determined by the Yee scheme and material parameters. Under the Courant–Friedrichs–Lewy condition for the Yee scheme in vacuum, the update is energy-preserving in the discrete electromagnetic energy norm, so the operator norm of $\\mathcal{A}$ in that norm satisfies\n$$\n\\|\\mathcal{A}\\| \\;=\\; 1.\n$$\nConsequently, perturbations introduced at each time step do not amplify over subsequent steps in this norm; they accumulate at most linearly.\n\nFor floating-point rounding behavior, we adopt the standard model: each arithmetic operation $x \\,\\mathrm{op}\\, y$ is computed as\n$$\n\\mathrm{fl}(x \\,\\mathrm{op}\\, y) \\;=\\; (x \\,\\mathrm{op}\\, y)\\,(1+\\delta),\n$$\nwith $|\\delta| \\le u$, where $u$ is the unit roundoff of the precision used for that operation. The relevant unit roundoffs are $u_{32} = 2^{-24}$ for binary32 and $u_{64} = 2^{-53}$ for binary64.\n\nConsider one field component’s local update over a single time step. It comprises a fixed set of arithmetic operations whose rounding errors contribute to a local update error. Let the fraction of operations executed in binary64 be $p$, and the fraction in binary32 be $1-p$. Define $c0$ to encapsulate the sensitivity of the local update to the per-operation rounding errors (including both the number of operations and their conditioning). Then the magnitude of the per-step rounding error in that field component is bounded by\n$$\ne_{\\text{step}}(p) \\;\\le\\; c\\big((1-p)\\,u_{32} + p\\,u_{64}\\big),\n$$\nbecause the per-operation bounds combine as a convex combination weighted by the fractions of operations in each precision.\n\nBy the energy-preserving property, the cumulative error over $N$ steps in that component is bounded by the sum of the per-step bounds:\n$$\nE(N,p) \\;\\le\\; \\sum_{n=1}^{N} e_{\\text{step}}(p) \\;=\\; N\\,c\\big((1-p)\\,u_{32} + p\\,u_{64}\\big).\n$$\nImposing the target absolute error tolerance $\\varepsilon$ yields the constraint\n$$\nN\\,c\\big((1-p)\\,u_{32} + p\\,u_{64}\\big) \\;\\le\\; \\varepsilon.\n$$\nDivide both sides by $N\\,c$ to obtain\n$$\n(1-p)\\,u_{32} + p\\,u_{64} \\;\\le\\; \\frac{\\varepsilon}{N\\,c}.\n$$\nRewrite the left-hand side as $u_{32} + p\\,(u_{64}-u_{32})$. Hence\n$$\nu_{32} + p\\,(u_{64}-u_{32}) \\;\\le\\; \\frac{\\varepsilon}{N\\,c}.\n$$\nSince $u_{64}  u_{32}$, the difference $u_{64}-u_{32}$ is negative. Solving the linear inequality for $p$ gives\n$$\np \\;\\ge\\; \\frac{\\frac{\\varepsilon}{N\\,c} - u_{32}}{u_{64} - u_{32}}.\n$$\nUnder the stated assumption $N c u_{64}  \\varepsilon  N c u_{32}$, the right-hand side lies strictly between $0$ and $1$, guaranteeing that the admissible $p$ is within $(0,1)$.\n\nNext, consider performance. The effective throughput as a function of $p$ is modeled by\n$$\nT(p) \\;=\\; \\left(\\frac{1-p}{S} + \\frac{p}{\\gamma S}\\right)^{-1},\n$$\nwith $0\\gamma1$. Because $\\gamma1$, the term $\\frac{p}{\\gamma S}$ exceeds $\\frac{p}{S}$ for any $p0$, so increasing $p$ increases the average time per operation and strictly decreases $T(p)$. Therefore, to minimize wall-clock time subject to the error constraint, we must choose the smallest $p$ that satisfies the constraint. With the bound treated as tight (meaning the constraint is active at optimality), the optimal fraction is obtained by equality in the error bound:\n$$\np^{\\star} \\;=\\; \\frac{\\frac{\\varepsilon}{N\\,c} - u_{32}}{u_{64} - u_{32}}.\n$$\nThis expression is well-defined and lies in $(0,1)$ under the given assumption on $\\varepsilon$, and it minimizes the total run time while meeting the prescribed error tolerance.", "answer": "$$\\boxed{\\frac{\\frac{\\varepsilon}{N\\,c} - u_{32}}{u_{64} - u_{32}}}$$", "id": "3336885"}]}