{"hands_on_practices": [{"introduction": "The choice between the robust, quadratically convergent Newton-Raphson method and a simpler, linearly convergent direct (or Picard) iteration lies at the heart of nonlinear computational mechanics. While Newton's method typically requires far fewer iterations, each one is computationally expensive, demanding the assembly and solution of a large linear system involving the tangent (Jacobian) matrix. This exercise provides a practical framework for quantifying this trade-off, asking you to build a detailed cost model for a representative poromechanics problem to determine which algorithm is more efficient under a specific set of computational assumptions [@problem_id:3561408].", "problem": "Consider a nonlinear hydro-mechanical finite element model of plane-strain poroelasticity in computational geomechanics. The governing discrete equilibrium at the global level is expressed by the residual vector $\\mathbf{R}(\\mathbf{x})$, with the Jacobian (tangent stiffness) matrix $\\mathbf{J}(\\mathbf{x}) = \\frac{\\partial \\mathbf{R}}{\\partial \\mathbf{x}}$. Two nonlinear solution strategies are considered: the fixed-point (Picard) iteration and the Newton-Raphson (NR) method. The representative mesh is a structured grid of four-node quadrilateral elements on a rectangle with $n_x = 80$ elements along the horizontal direction and $n_y = 50$ elements along the vertical direction. The total number of elements is $N_e = n_x n_y$, the number of nodes is $N_n = (n_x + 1)(n_y + 1)$, and the number of degrees of freedom (DOF) is $N_d = m N_n$ with $m = 3$ DOF per node (two displacements and one pore pressure).\n\nAssume the following computational work model expressed in wall-clock time:\n1. Residual assembly per iteration is dominated by local constitutive evaluations and global vector assembly, modeled as\n$$\nT_R = N_e \\left( N_q \\, t_r + t_{ra} \\right),\n$$\nwhere $N_q = 4$ Gauss points per element, $t_r = 2.5 \\times 10^{-6} \\ \\text{s}$ is the cost per Gauss-point residual evaluation, and $t_{ra} = 1.0 \\times 10^{-6} \\ \\text{s}$ is the per-element global residual assembly overhead.\n\n2. Jacobian assembly per iteration is modeled as\n$$\nT_J = N_e \\left( N_q \\, t_j + t_{ja} \\right),\n$$\nwhere $t_j = 1.0 \\times 10^{-5} \\ \\text{s}$ is the cost per Gauss-point Jacobian contribution evaluation and $t_{ja} = 4.0 \\times 10^{-6} \\ \\text{s}$ is the per-element global matrix assembly overhead.\n\n3. The linear system is solved by a Krylov method, specifically the Preconditioned Generalized Minimal Residual (GMRES) method. Let the average number of nonzeros per row be $s = 70$, so the total number of nonzeros is $\\mathrm{nnz} = s N_d$. The cost per GMRES iteration is modeled as\n$$\nT_{\\text{lin,iter}} = \\mathrm{nnz} \\, t_{nz} + N_d \\, t_p,\n$$\nwhere $t_{nz} = 1.0 \\times 10^{-9} \\ \\text{s}$ per nonzero for the sparse matrix-vector multiply and $t_p = 5.0 \\times 10^{-9} \\ \\text{s}$ per DOF for application of a right-preconditioner. The cost to build the preconditioner once per Jacobian assembly is\n$$\nT_b = \\mathrm{nnz} \\, t_b,\n$$\nwith $t_b = 5.0 \\times 10^{-9} \\ \\text{s}$ per nonzero.\n\nFor the Picard iteration, the global tangent is fixed to an initial linearization (assembled once at the start) and $\\mathbf{R}(\\mathbf{x})$ is evaluated at every outer iteration. For the Picard solver, assume $K_P = 20$ outer iterations, and $L_P = 45$ GMRES iterations per outer iteration. For the Newton-Raphson solver, at each of $K_N = 6$ outer iterations, both $\\mathbf{R}(\\mathbf{x})$ and $\\mathbf{J}(\\mathbf{x})$ are assembled, a new preconditioner is built, and $L_N = 30$ GMRES iterations are performed.\n\nUsing the above model and parameters, compute the dimensionless ratio\n$$\n\\mathcal{R} = \\frac{T_{\\text{Picard}}}{T_{\\text{Newton}}},\n$$\nwhere $T_{\\text{Picard}}$ is the total time for Picard and $T_{\\text{Newton}}$ is the total time for Newton-Raphson, both accumulated over their respective outer iterations. Round your final answer for $\\mathcal{R}$ to four significant figures. The ratio is dimensionless; report it without units.", "solution": "The problem has been validated and is determined to be a well-posed, scientifically grounded problem in computational mechanics. The task is to compute a dimensionless ratio of computational times for two standard nonlinear solution algorithms. The solution proceeds by first calculating the model dimensions, then the costs of elementary computational tasks, and finally the total costs for each algorithm to find their ratio.\n\nThe first step is to compute the key parameters of the finite element model discretization.\nThe number of elements is given by $N_e = n_x n_y$. With $n_x = 80$ and $n_y = 50$, this is:\n$$N_e = 80 \\times 50 = 4000$$\nThe number of nodes is $N_n = (n_x + 1)(n_y + 1)$:\n$$N_n = (80 + 1)(50 + 1) = 81 \\times 51 = 4131$$\nThe total number of degrees of freedom (DOF) is $N_d = m N_n$, with $m = 3$ DOF per node:\n$$N_d = 3 \\times 4131 = 12393$$\nThe total number of nonzeros in the Jacobian matrix is $\\mathrm{nnz} = s N_d$, with an average of $s = 70$ nonzeros per row:\n$$\\mathrm{nnz} = 70 \\times 12393 = 867510$$\n\nNext, we evaluate the wall-clock time for each of the fundamental computational operations based on the provided cost models.\n\n1.  The cost of residual vector assembly per iteration, $T_R$, is:\n    $$T_R = N_e \\left( N_q \\, t_r + t_{ra} \\right)$$\n    Using the given values $N_e = 4000$, $N_q = 4$, $t_r = 2.5 \\times 10^{-6}$, and $t_{ra} = 1.0 \\times 10^{-6}$:\n    $$T_R = 4000 \\left( 4 \\times 2.5 \\times 10^{-6} + 1.0 \\times 10^{-6} \\right) = 4000 \\left( 1.0 \\times 10^{-5} + 1.0 \\times 10^{-6} \\right) = 4000 \\left( 1.1 \\times 10^{-5} \\right) = 0.044$$\n\n2.  The cost of Jacobian matrix assembly per iteration, $T_J$, is:\n    $$T_J = N_e \\left( N_q \\, t_j + t_{ja} \\right)$$\n    Using $t_j = 1.0 \\times 10^{-5}$ and $t_{ja} = 4.0 \\times 10^{-6}$:\n    $$T_J = 4000 \\left( 4 \\times 1.0 \\times 10^{-5} + 4.0 \\times 10^{-6} \\right) = 4000 \\left( 4.0 \\times 10^{-5} + 0.4 \\times 10^{-5} \\right) = 4000 \\left( 4.4 \\times 10^{-5} \\right) = 0.176$$\n\n3.  The cost to build the preconditioner, $T_b$, is:\n    $$T_b = \\mathrm{nnz} \\, t_b$$\n    Using $\\mathrm{nnz} = 867510$ and $t_b = 5.0 \\times 10^{-9}$:\n    $$T_b = 867510 \\times 5.0 \\times 10^{-9} = 0.00433755$$\n\n4.  The cost per iteration of the GMRES linear solver, $T_{\\text{lin,iter}}$, is:\n    $$T_{\\text{lin,iter}} = \\mathrm{nnz} \\, t_{nz} + N_d \\, t_p$$\n    Using $t_{nz} = 1.0 \\times 10^{-9}$ and $t_p = 5.0 \\times 10^{-9}$:\n    $$T_{\\text{lin,iter}} = 867510 \\times 1.0 \\times 10^{-9} + 12393 \\times 5.0 \\times 10^{-9} = 0.00086751 + 0.000061965 = 0.000929475$$\n\nNow, we can compute the total computational time for each of the two nonlinear solution strategies.\n\nFor the Picard iteration, the total time, $T_{\\text{Picard}}$, consists of a one-time initial setup cost (Jacobian assembly and preconditioner build) plus the cumulative cost of $K_P = 20$ iterations. Each iteration involves one residual assembly and a linear solve of $L_P = 45$ GMRES iterations.\n$$T_{\\text{Picard}} = (T_J + T_b) + K_P \\left( T_R + L_P T_{\\text{lin,iter}} \\right)$$\n$$T_{\\text{Picard}} = (0.176 + 0.00433755) + 20 \\left( 0.044 + 45 \\times 0.000929475 \\right)$$\n$$T_{\\text{Picard}} = 0.18033755 + 20 \\left( 0.044 + 0.041826375 \\right)$$\n$$T_{\\text{Picard}} = 0.18033755 + 20 \\left( 0.085826375 \\right)$$\n$$T_{\\text{Picard}} = 0.18033755 + 1.7165275 = 1.89686505$$\n\nFor the Newton-Raphson method, the total time, $T_{\\text{Newton}}$, is the cost of one iteration multiplied by the number of iterations, $K_N = 6$. Each iteration involves assembling the residual and Jacobian, building the preconditioner, and performing a linear solve of $L_N = 30$ GMRES iterations.\n$$T_{\\text{Newton}} = K_N \\left( T_R + T_J + T_b + L_N T_{\\text{lin,iter}} \\right)$$\n$$T_{\\text{Newton}} = 6 \\left( 0.044 + 0.176 + 0.00433755 + 30 \\times 0.000929475 \\right)$$\n$$T_{\\text{Newton}} = 6 \\left( 0.044 + 0.176 + 0.00433755 + 0.02788425 \\right)$$\n$$T_{\\text{Newton}} = 6 \\left( 0.2522218 \\right) = 1.5133308$$\n\nFinally, the dimensionless ratio $\\mathcal{R}$ is computed by dividing the total time for the Picard method by the total time for the Newton-Raphson method.\n$$\\mathcal{R} = \\frac{T_{\\text{Picard}}}{T_{\\text{Newton}}} = \\frac{1.89686505}{1.5133308} \\approx 1.253434$$\nRounding the result to four significant figures as requested:\n$$\\mathcal{R} \\approx 1.253$$", "answer": "$$\\boxed{1.253}$$", "id": "3561408"}, {"introduction": "While direct iteration is computationally cheap per step, its slow linear convergence can be a significant bottleneck, often rendering it impractical for challenging problems. Anderson Acceleration (AA) offers a powerful, Jacobian-free strategy to dramatically improve performance by intelligently mixing a history of past iterates to form a better prediction. This practice delves into the mechanics of AA, guiding you to derive the core least-squares problem that determines the optimal mixing coefficients and to analyze the method's computational trade-offs against both plain Picard and Newton-Raphson methods [@problem_id:3561419].", "problem": "Consider a nonlinear finite element model in computational geomechanics that couples pore pressure and deformation in a saturated soil, where the hydraulic conductivity depends on pore pressure through a state-dependent relation. Let the discrete nonlinear algebraic system for the pore pressure degrees of freedom be written as a fixed-point problem $x = G(x)$, where $x \\in \\mathbb{R}^{n}$ is the nodal vector of pore pressures and $G:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$ is the fixed-point mapping obtained by assembling the finite element residual with the conductivity evaluated at the latest iterate. The basic direct iteration is $x_{k+1} = G(x_{k})$. Define the residual $f(x) = G(x) - x$ and note that convergence of the Picard (fixed-point) iteration requires suitable contractivity in $G$.\n\nTask 1: Starting only from the definition of fixed-point iteration $x_{k+1} = G(x_{k})$ and the residual $f(x) = G(x) - x$, explain the idea of Anderson Acceleration (AA) for fixed-point iterations in this geomechanical setting as forming an affine combination of past mapped iterates, and derive the least-squares problem that determines the mixing coefficients for a history length $m \\geq 1$. Your derivation must begin from these definitions and construct, step by step, the constrained minimization problem that yields the coefficients as those minimizing the Euclidean norm of a residual combination, subject to an affine-combination constraint. Use Lagrange multipliers to derive the optimality conditions.\n\nTask 2: For the specific case $m = 1$ (using two successive iterates), suppose the residuals at iterations $k-1$ and $k$ are given by $r_{k-1} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$ and $r_{k} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$ in $\\mathbb{R}^{2}$. Compute the mixing coefficient $\\alpha_{k-1}$ associated with $r_{k-1}$ that minimizes the residual norm in the least-squares sense, subject to the affine-combination constraint $\\alpha_{k-1} + \\alpha_{k} = 1$. Express the final answer as a real number, rounded to four significant figures.\n\nTask 3: Discuss the storage and computational trade-offs of Anderson Acceleration versus Newton-Raphson and plain Picard iteration for large-scale finite element geomechanics, focusing on how memory and per-iteration arithmetic scale with the number of degrees of freedom $n$ and the history length $m$. When referring to Newton-Raphson, spell out the method as Newton-Raphson (NR) on first use, and base your comparison on the well-tested practice that NR iterations are dominated by the assembly and solution of linearized systems, whereas AA augments Picard with small dense least-squares subproblems that re-use residuals.", "solution": "The problem is valid as it is scientifically grounded in computational geomechanics and numerical analysis, well-posed with sufficient information for each task, and uses objective, formal language. It is free of the invalidating flaws listed in the instructions.\n\n### Task 1: Derivation of Anderson Acceleration\nThe objective is to accelerate the convergence of a fixed-point iteration, which in this geomechanical context models the nonlinear system for pore pressures.\n\nThe basic fixed-point iteration, also known as Picard iteration or direct iteration, is given by:\n$$ x_{k+1} = G(x_k) $$\nwhere $x_k \\in \\mathbb{R}^n$ is the vector of nodal pore pressures at iteration $k$. A solution $x^*$ is found when $x^* = G(x^*)$, or equivalently, when the residual $f(x^*) = G(x^*) - x^*$ is zero.\n\nAnderson Acceleration (AA) seeks to find a better next iterate, $x_{k+1}$, by extrapolating from a history of previous iterates. Instead of using only the last mapped point $G(x_k)$, AA constructs the next iterate as an affine combination of the $m+1$ most recent mapped points, $\\{G(x_{k-m}), \\dots, G(x_k)\\}$. Let the history length be $m \\ge 1$, and assume $k \\ge m$.\n\nThe new iterate, $x_{k+1}$, is defined as:\n$$ x_{k+1} = \\sum_{i=0}^{m} \\gamma_i^{(k)} G(x_{k-i}) $$\nwhere the coefficients $\\gamma_i^{(k)}$ must satisfy the affine-combination constraint:\n$$ \\sum_{i=0}^{m} \\gamma_i^{(k)} = 1 $$\nThe core idea of AA is to choose the coefficients $\\boldsymbol{\\gamma}^{(k)} = (\\gamma_0^{(k)}, \\dots, \\gamma_m^{(k)})^T$ in an optimal way. The ideal choice of $\\boldsymbol{\\gamma}^{(k)}$ would be one that minimizes the norm of the true residual at the new point, $\\|f(x_{k+1})\\|_2 = \\|G(x_{k+1}) - x_{k+1}\\|_2$. However, computing this would require an evaluation of the nonlinear function $G$ at the unknown point $x_{k+1}$, which is computationally prohibitive.\n\nInstead, AA makes a key simplifying approximation. It seeks to minimize the norm of an approximate residual, which is formed by applying the same affine combination to the past residuals $\\{r_{k-m}, \\dots, r_k\\}$, where $r_j = f(x_j) = G(x_j) - x_j$. This is motivated by the assumption that the mapping $G$ is approximately linear over the set of recent iterates. If $G$ were linear, the residual of the combination would equal the combination of the residuals.\n\nThe optimization problem for the coefficients is therefore formulated as a constrained linear least-squares problem:\n$$ \\min_{\\boldsymbol{\\gamma}^{(k)} \\in \\mathbb{R}^{m+1}} \\left\\| \\sum_{i=0}^{m} \\gamma_i^{(k)} r_{k-i} \\right\\|_2^2 \\quad \\text{subject to} \\quad \\sum_{i=0}^{m} \\gamma_i^{(k)} = 1 $$\nTo formalize this, let $\\mathbf{F}_k$ be the $n \\times (m+1)$ matrix whose columns are the past residual vectors:\n$$ \\mathbf{F}_k = \\begin{pmatrix} r_{k} & r_{k-1} & \\cdots & r_{k-m} \\end{pmatrix} $$\nLet $\\mathbf{1}$ be a vector of ones of size $m+1$. The optimization problem becomes:\n$$ \\min_{\\boldsymbol{\\gamma}^{(k)}} \\| \\mathbf{F}_k \\boldsymbol{\\gamma}^{(k)} \\|_2^2 \\quad \\text{subject to} \\quad \\mathbf{1}^T \\boldsymbol{\\gamma}^{(k)} = 1 $$\nThe objective function is $J(\\boldsymbol{\\gamma}^{(k)}) = (\\mathbf{F}_k \\boldsymbol{\\gamma}^{(k)})^T (\\mathbf{F}_k \\boldsymbol{\\gamma}^{(k)}) = (\\boldsymbol{\\gamma}^{(k)})^T \\mathbf{F}_k^T \\mathbf{F}_k \\boldsymbol{\\gamma}^{(k)}$.\n\nWe solve this constrained minimization problem using the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}$ is:\n$$ \\mathcal{L}(\\boldsymbol{\\gamma}^{(k)}, \\lambda) = (\\boldsymbol{\\gamma}^{(k)})^T \\mathbf{F}_k^T \\mathbf{F}_k \\boldsymbol{\\gamma}^{(k)} - \\lambda (\\mathbf{1}^T \\boldsymbol{\\gamma}^{(k)} - 1) $$\nThe optimality conditions are obtained by setting the gradients with respect to $\\boldsymbol{\\gamma}^{(k)}$ and $\\lambda$ to zero.\n$$ \\nabla_{\\boldsymbol{\\gamma}^{(k)}} \\mathcal{L} = 2 \\mathbf{F}_k^T \\mathbf{F}_k \\boldsymbol{\\gamma}^{(k)} - \\lambda \\mathbf{1} = 0 $$\n$$ \\nabla_{\\lambda} \\mathcal{L} = -(\\mathbf{1}^T \\boldsymbol{\\gamma}^{(k)} - 1) = 0 $$\nFrom the first condition, we express $\\boldsymbol{\\gamma}^{(k)}$ in terms of $\\lambda$:\n$$ 2 \\mathbf{F}_k^T \\mathbf{F}_k \\boldsymbol{\\gamma}^{(k)} = \\lambda \\mathbf{1} $$\n$$ \\boldsymbol{\\gamma}^{(k)} = \\frac{\\lambda}{2} (\\mathbf{F}_k^T \\mathbf{F}_k)^{-1} \\mathbf{1} $$\nHere, we assume the Gram matrix $\\mathbf{F}_k^T \\mathbf{F}_k$ is invertible, which is true if the residual vectors $\\{r_{k-i}\\}$ are linearly independent.\nWe substitute this expression into the second condition (the constraint $\\mathbf{1}^T \\boldsymbol{\\gamma}^{(k)} = 1$):\n$$ \\mathbf{1}^T \\left( \\frac{\\lambda}{2} (\\mathbf{F}_k^T \\mathbf{F}_k)^{-1} \\mathbf{1} \\right) = 1 $$\nThis allows us to solve for the term containing the Lagrange multiplier:\n$$ \\frac{\\lambda}{2} = \\frac{1}{\\mathbf{1}^T (\\mathbf{F}_k^T \\mathbf{F}_k)^{-1} \\mathbf{1}} $$\nSubstituting this back into the expression for $\\boldsymbol{\\gamma}^{(k)}$ gives the solution for the optimal coefficients:\n$$ \\boldsymbol{\\gamma}^{(k)} = \\frac{(\\mathbf{F}_k^T \\mathbf{F}_k)^{-1} \\mathbf{1}}{\\mathbf{1}^T (\\mathbf{F}_k^T \\mathbf{F}_k)^{-1} \\mathbf{1}} $$\nThis provides the coefficients that minimize the Euclidean norm of the combined residual. The next iterate, $x_{k+1}$, is then computed using these optimal coefficients.\n\n### Task 2: Calculation for $m=1$\nFor the specific case of $m=1$, we use a history of two successive iterates, say at steps $k-1$ and $k$. The residuals are given as $r_{k-1} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$ and $r_{k} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$ in $\\mathbb{R}^2$. The mixing coefficients are $\\alpha_{k-1}$ and $\\alpha_k$, and they must satisfy the affine-combination constraint $\\alpha_{k-1} + \\alpha_{k} = 1$. The goal is to find $\\alpha_{k-1}$.\n\nThe optimization problem is to minimize the norm of the combined residual:\n$$ \\min_{\\alpha_{k-1}, \\alpha_k} \\left\\| \\alpha_{k-1} r_{k-1} + \\alpha_k r_k \\right\\|_2^2 \\quad \\text{subject to} \\quad \\alpha_{k-1} + \\alpha_k = 1 $$\nLet's substitute the given residual vectors into the objective function:\n$$ \\left\\| \\alpha_{k-1} \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} + \\alpha_k \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} \\right\\|_2^2 = \\left\\| \\begin{pmatrix}\\alpha_{k-1} \\\\ \\alpha_k\\end{pmatrix} \\right\\|_2^2 = \\alpha_{k-1}^2 + \\alpha_k^2 $$\nNow, we use the constraint to express $\\alpha_k$ in terms of $\\alpha_{k-1}$: $\\alpha_k = 1 - \\alpha_{k-1}$. Substituting this into the objective function yields a function of a single variable, $J(\\alpha_{k-1})$:\n$$ J(\\alpha_{k-1}) = \\alpha_{k-1}^2 + (1 - \\alpha_{k-1})^2 $$\nTo find the minimum, we compute the derivative of $J$ with respect to $\\alpha_{k-1}$ and set it to zero:\n$$ \\frac{dJ}{d\\alpha_{k-1}} = 2\\alpha_{k-1} + 2(1 - \\alpha_{k-1})(-1) = 2\\alpha_{k-1} - 2 + 2\\alpha_{k-1} = 4\\alpha_{k-1} - 2 $$\nSetting the derivative to zero gives:\n$$ 4\\alpha_{k-1} - 2 = 0 \\implies \\alpha_{k-1} = \\frac{2}{4} = 0.5 $$\nThe second derivative is $\\frac{d^2J}{d\\alpha_{k-1}^2} = 4 > 0$, confirming this is a minimum. The value of the coefficient $\\alpha_{k-1}$ is $0.5$. Rounded to four significant figures, this is $0.5000$.\n\n### Task 3: Storage and Computational Trade-offs\nIn the context of large-scale finite element geomechanics, where the number of degrees of freedom $n$ is large, the choice of nonlinear solver involves critical trade-offs between memory, computational cost per iteration, and convergence rate. We compare plain Picard iteration, Anderson Acceleration (AA) with history length $m$, and the Newton-Raphson (NR) method.\n\n**Picard Iteration (Direct Iteration):**\n- **Storage:** This is the most memory-efficient method. It primarily requires storing the current iterate vector $x_k$ and the next mapped vector $G(x_k)$, both of size $n$. Therefore, its storage complexity is $O(n)$.\n- **Computation:** The cost per iteration is dominated by the evaluation of $G(x_k)$, which corresponds to assembling the finite element residual vector. For typical sparse mesh connectivities, this assembly process scales linearly with the number of degrees of freedom. Thus, the per-iteration computational cost is $O(n)$. While cheap per iteration, its linear and often slow convergence can lead to a high total number of iterations and overall long solution times, or failure to converge.\n\n**Newton-Raphson (NR) Method:**\nThe NR method solves $f(x)=0$ by iteratively solving the linear system $J_f(x_k) \\delta x_k = -f(x_k)$ where $J_f(x_k)$ is the Jacobian matrix (tangent stiffness matrix in geomechanics).\n- **Storage:** NR must store the Jacobian matrix $J_f$. While $J_f$ is an $n \\times n$ matrix, it is typically sparse in finite element models. Storing only the non-zero entries results in a storage requirement of $O(n_z)$, where $n_z$ is the number of non-zeros, often proportional to $n$. So, storage is typically $O(n)$. However, if direct solvers are used for the linear system, memory usage can grow significantly due to fill-in during factorization, potentially scaling as $O(n^{1.5})$ or worse.\n- **Computation:** This is the most computationally expensive method per iteration. The cost is dominated by two steps: (1) assembly of the Jacobian matrix $J_f$, which is significantly more work than assembling the residual vector, and (2) solution of the $n \\times n$ linear system. The cost of the linear solve is the main bottleneck. For direct solvers, it can be $O(n^2)$ for 3D problems. For iterative solvers, the cost is typically $O(k_{iter} \\cdot n)$, but the number of iterations $k_{iter}$ can be large if the Jacobian is ill-conditioned. The high per-iteration cost is offset by its quadratic convergence rate, which, if applicable, drastically reduces the total number of iterations required.\n\n**Anderson Acceleration (AA):**\nAA enhances Picard iteration by adding a \"Jacobian-free\" extrapolation step.\n- **Storage:** AA's primary storage drawback is the need to keep a history of past vectors. In the formulation derived, it requires storing the $m+1$ most recent mapped iterates $G(x_{k-i})$ (or equivalently, the iterates $x_{k-i}$ and residuals $r_{k-i}$). This results in a storage requirement of $O(m \\cdot n)$. For large-scale problems where $n$ is in the millions, this can become a significant memory burden, limiting the practical choice of the history length $m$ (typically $m \\ll n$).\n- **Computation:** The per-iteration cost of AA consists of one Picard step ($O(n)$) plus the overhead of the acceleration step. This overhead involves: (1) forming the $(m+1) \\times (m+1)$ Gram matrix $\\mathbf{F}_k^T \\mathbf{F}_k$ by computing $O(m)$ vector dot-products, which costs $O(m \\cdot n)$; (2) solving the small $(m+1) \\times (m+1)$ linear system for the coefficients, which costs $O(m^3)$ and is negligible; and (3) forming the new iterate via a linear combination of $m+1$ vectors, which costs $O(m \\cdot n)$. The dominant overhead is therefore $O(m \\cdot n)$.\n\n**Summary of Trade-offs:**\n- **Picard vs. AA:** AA adds a computational cost of $O(m \\cdot n)$ and a storage cost of $O(m \\cdot n)$ to the Picard iteration. This is a worthwhile trade-off as AA dramatically improves the convergence rate from linear to superlinear, often reducing the total solution time despite the higher per-iteration cost.\n- **AA vs. NR:** AA is a \"Jacobian-free\" method. It avoids the immense computational cost of assembling and solving the large linear system involving the Jacobian, which is the main drawback of NR. AA's per-iteration cost of $O(m \\cdot n)$ is typically much lower than NR's. However, AA's storage requirement increases with $m$, while NR's is largely independent of its iteration history. Furthermore, NR's quadratic convergence is faster than AA's superlinear rate. In practice, for many large-scale geomechanical problems where Jacobians are expensive or difficult to formulate, a well-tuned AA with a modest history $m$ often provides a more robust and efficient overall solution strategy than a full NR approach.", "answer": "$$\\boxed{0.5000}$$", "id": "3561419"}, {"introduction": "Theoretical analysis provides valuable insight, but the definitive test of an algorithm is its performance in practice. This exercise moves from analysis to implementation, challenging you to compare a monolithic Newton-Raphson solver against a splitting scheme—a block Gauss-Seidel iteration enhanced with Anderson Acceleration—for a classic poromechanics problem [@problem_id:3561443]. By running a numerical experiment, you will empirically discover how a critical parameter, the time step $\\Delta t$, can alter the balance of efficiency and determine which solution strategy is superior.", "problem": "Consider quasi-static consolidation of a single representative control volume in fluid–solid coupling under Biot poroelasticity. The unknowns at the end of a new time step are the pore fluid pressure $p^{n+1}$ and the volumetric strain $e^{n+1}$. The coupling is modeled by the following two algebraic relations that arise from the quasi-static momentum balance and the fully implicit mass balance, specialized to a zero-dimensional (no spatial flux) control volume:\n\n1. Momentum balance with drained bulk modulus $K>0$ and Biot coefficient $\\alpha \\in (0,1]$:\n$$\nK\\,e^{n+1} - \\alpha\\,p^{n+1} = s_m,\n$$\nwhere $s_m$ is a prescribed volumetric source representing the effect of external mechanical loading.\n\n2. Mass balance with nonlinear storage $S(p)$:\n$$\n\\alpha\\,e^{n+1} + S\\!\\left(p^{n+1}\\right) = \\alpha\\,e^{n} + S\\!\\left(p^{n}\\right) + \\Delta t\\,s_f,\n$$\nwhere $s_f$ is a prescribed fluid source term per unit time and $\\Delta t > 0$ is the time step size. The previous time step values $e^{n}$ and $p^{n}$ are known. The nonlinear storage is taken as a cubic polynomial\n$$\nS(p) = c_0\\,p + c_1\\,p^2 + c_2\\,p^3,\n$$\nwith $c_0>0$, $c_1 \\ge 0$, $c_2 \\ge 0$ ensuring monotonicity and physical consistency of the storage relation.\n\nYou are to solve, at each time step, for the pair $(e^{n+1}, p^{n+1})$ using two nonlinear strategies and compare their nonlinear iteration performance as the time step $\\Delta t$ varies:\n\nA. Monolithic Newton–Raphson applied to the coupled system\n$$\nF_1(e,p) = K e - \\alpha p - s_m = 0,\\quad\nF_2(e,p) = \\alpha e + S(p) - \\left(\\alpha e^{n} + S(p^{n}) + \\Delta t\\,s_f\\right) = 0,\n$$\nstarting from the initial guess $e^{0} = e^{n}$ and $p^{0} = p^{n}$, with a stopping criterion based on the Euclidean norm of the residual vector $(F_1, F_2)$ falling below a tolerance $\\varepsilon$.\n\nB. A block Gauss–Seidel Picard iteration using the fixed-stress split, in which you sequentially update $e$ and $p$ as follows:\n- Given the current $p^{k}$, update $e^{k+1}$ from momentum balance,\n$$\ne^{k+1} = \\frac{s_m + \\alpha\\,p^{k}}{K}.\n$$\n- Update $p^{k+1}$ from mass balance by a Picard linearization of $S(p)$ around $p^{k}$,\n$$\nS(p^{k+1}) \\approx \\left(c_0 + c_1\\,p^{k} + c_2\\,(p^{k})^2\\right)\\,p^{k+1},\n$$\nwhich yields the explicit mapping\n$$\np^{k+1} = \\frac{\\alpha e^{n} + S(p^{n}) + \\Delta t\\,s_f - \\alpha\\,e^{k+1}}{c_0 + c_1\\,p^{k} + c_2\\,(p^{k})^2}.\n$$\nEmbed Anderson acceleration of depth two on the scalar fixed-point map $p^{k+1} = G(p^{k})$ to obtain an accelerated iterate,\n$$\np^{k+1}_{\\text{AA}} = \\theta\\,G\\!\\left(p^{k-1}\\right) + (1-\\theta)\\,G\\!\\left(p^k\\right),\\quad\n\\theta = \\frac{G(p^{k}) - p^{k}}{G(p^{k}) - p^{k} - \\left(G(p^{k-1}) - p^{k-1}\\right)},\n$$\nwhenever the denominator is nonzero; otherwise, use $p^{k+1} = G(p^{k})$. After forming $p^{k+1}_{\\text{AA}}$, update $e^{k+1}$ from the momentum balance with $p^{k+1}_{\\text{AA}}$. Use the same residual-norm stopping criterion as in the monolithic Newton–Raphson.\n\nFrom the perspective of computational geomechanics, the fixed-stress split is a stabilization technique derived from the structure of Biot’s equations, where the mechanical subproblem is solved with stress held fixed while iteratively updating the flow subproblem. Anderson acceleration is a nonlinear sequence acceleration method that blends past fixed-point iterates to improve convergence.\n\nYour task is to implement both nonlinear solvers and empirically identify, for each parameter set below, the smallest time step $\\Delta t$ in seconds at which the block Gauss–Seidel Picard iteration with Anderson acceleration requires strictly fewer nonlinear iterations than the monolithic Newton–Raphson to reach the tolerance. If the monolithic Newton–Raphson fails due to Jacobian singularity or does not converge within a prescribed maximum iteration count while the block Gauss–Seidel Picard iteration with Anderson acceleration does converge, then the threshold is defined as the smallest $\\Delta t$ in the provided set at which this occurs. If no such $\\Delta t$ exists within the provided set, report the value $-1.0$.\n\nUse the following fixed previous-step state and tolerance across all tests:\n- Previous state: $e^{n} = 0$ (dimensionless volumetric strain), $p^{n} = 0$ (in pascals).\n- Tolerance: $\\varepsilon = 10^{-10}$.\n- Maximum nonlinear iterations per solver: $N_{\\max} = 200$.\n\nTest Suite:\nFor each case, use the list of candidate time steps $\\Delta t$ in seconds: $[0.1,\\,0.3,\\,1.0,\\,3.0,\\,10.0,\\,30.0,\\,50.0]$.\n\n- Case 1 (general coupling, moderate nonlinearity):\n  - $K = 1.0$, $\\alpha = 0.8$, $c_0 = 0.6$, $c_1 = 0.25$, $c_2 = 0.10$, $s_m = 0.0$, $s_f = 1.0$.\n- Case 2 (near-Jacobian singularity in the monolithic system):\n  - $K = 1.0$, $\\alpha = 1.0$, $c_0 = 1.0$, $c_1 = 0.0$, $c_2 = 0.0$, $s_m = 0.0$, $s_f = 1.0$.\n- Case 3 (strong nonlinearity dominating at larger pressures):\n  - $K = 2.0$, $\\alpha = 0.6$, $c_0 = 0.2$, $c_1 = 0.6$, $c_2 = 0.5$, $s_m = 0.0$, $s_f = 0.5$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results for the three cases as a comma-separated list of floating-point numbers enclosed in square brackets, where each number is the identified threshold time step in seconds for one case in the order listed above (e.g., \"[`dt_case1`,`dt_case2`,`dt_case3`]\"). Each entry must be a single float in seconds. If a threshold is not found within the provided time step set, output \"-1.0\" for that case.\n\nAll angles, if any, must be expressed in radians. All time steps must be expressed in seconds. All percentages, if any, must be expressed as decimals.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the established theory of Biot poroelasticity, mathematically well-posed, and all parameters, initial conditions, and algorithmic procedures are specified unambiguously. The problem asks for a numerical experiment comparing two standard nonlinear solution schemes, monolithic Newton-Raphson and a splitting method enhanced with Anderson acceleration, which is a legitimate inquiry in computational mechanics.\n\nThe physical model describes the coupled evolution of volumetric strain $e$ and pore fluid pressure $p$ in a representative control volume under quasi-static conditions. The governing equations for the unknowns at the new time step, $e^{n+1}$ and $p^{n+1}$, form a system of two nonlinear algebraic equations. For simplicity, we denote the unknowns as $(e, p)$.\n\nThe first equation, arising from the momentum balance, is linear:\n$$\nF_1(e, p) = K e - \\alpha p - s_m = 0\n$$\nHere, $K$ is the drained bulk modulus, $\\alpha$ is the Biot coefficient, and $s_m$ is a mechanical source term.\n\nThe second equation, from the fully implicit mass balance, contains the nonlinearity:\n$$\nF_2(e, p) = \\alpha e + S(p) - C = 0\n$$\nwhere $C = \\alpha e^n + S(p^n) + \\Delta t s_f$ is a constant within the time step, composed of known values from the previous time step $(e^n, p^n)$, the time step size $\\Delta t$, and a fluid source term $s_f$. The nonlinear storage function is given as a cubic polynomial $S(p) = c_0 p + c_1 p^2 + c_2 p^3$. Given that $e^n=0$ and $p^n=0$, we have $S(p^n)=0$ and the constant simplifies to $C = \\Delta t s_f$.\n\nThe task is to solve the system $$\\mathbf{F}(e,p) = [F_1, F_2]^T = \\mathbf{0}$$ using two methods and compare their performance by counting the number of iterations required to achieve a residual norm $\\|\\mathbf{F}\\|  \\varepsilon = 10^{-10}$.\n\n**Method A: Monolithic Newton-Raphson**\nThis method solves for both unknowns simultaneously. Starting from an initial guess $\\mathbf{x}_k = [e_k, p_k]^T$, the next iterate $\\mathbf{x}_{k+1}$ is found by solving the linear system:\n$$\nJ(\\mathbf{x}_k) (\\mathbf{x}_{k+1} - \\mathbf{x}_k) = -\\mathbf{F}(\\mathbf{x}_k)\n$$\nwhere $J$ is the Jacobian matrix of the system $\\mathbf{F}$. The components of the Jacobian are the partial derivatives of $F_1$ and $F_2$ with respect to $e$ and $p$:\n$$\nJ(e,p) = \\begin{pmatrix} \\frac{\\partial F_1}{\\partial e}  \\frac{\\partial F_1}{\\partial p} \\\\ \\frac{\\partial F_2}{\\partial e}  \\frac{\\partial F_2}{\\partial p} \\end{pmatrix} = \\begin{pmatrix} K  -\\alpha \\\\ \\alpha  S'(p) \\end{pmatrix}\n$$\nwhere $S'(p) = \\frac{dS}{dp} = c_0 + 2c_1p + 3c_2p^2$. The iteration starts from the previous time step values, $\\mathbf{x}_0 = [e^n, p^n]^T = [0, 0]^T$, and continues until the Euclidean norm of the residual vector $\\mathbf{F}(\\mathbf{x}_k)$ is below the tolerance $\\varepsilon$. A potential failure mode is the singularity of the Jacobian matrix, i.e., when its determinant $\\det(J) = K S'(p) + \\alpha^2$ approaches zero.\n\n**Method B: Block Gauss-Seidel Picard Iteration with Anderson Acceleration**\nThis approach is a splitting method, where the system is solved sequentially for $e$ and $p$. The specific scheme is the \"fixed-stress split.\" The iteration, indexed by $k$, proceeds as follows:\n1.  From the momentum balance, $e$ is expressed in terms of the current pressure iterate $p_k$:\n    $$\n    e_{k+1} = \\frac{s_m + \\alpha p_k}{K}\n    $$\n2.  This expression for $e_{k+1}$ is substituted into a linearized form of the mass balance equation to solve for $p_{k+1}$. The problem specifies a Picard linearization of $S(p)$, where $S(p_{k+1})$ is approximated as $(c_0 + c_1 p_k + c_2 p_k^2)p_{k+1}$. This leads to a fixed-point map $p_{k+1} = G(p_k)$:\n    $$\n    G(p_k) = \\frac{\\alpha e^n + S(p^n) + \\Delta t s_f - \\alpha \\left(\\frac{s_m + \\alpha p_k}{K}\\right)}{c_0 + c_1 p_k + c_2 (p_k)^2}\n    $$\n    The simple Picard iteration $p_{k+1} = G(p_k)$ may converge slowly or diverge. To improve performance, Anderson acceleration of depth two is applied. This method blends the current and previous fixed-point evaluations to generate an improved subsequent iterate. Given iterates $p_k$ and $p_{k-1}$, and their corresponding function evaluations $g_k = G(p_k)$ and $g_{k-1} = G(p_{k-1})$, the accelerated update is:\n    $$\n    p_{k+1} = \\theta g_{k-1} + (1-\\theta) g_k, \\quad \\text{where} \\quad \\theta = \\frac{g_k - p_k}{(g_k - p_k) - (g_{k-1} - p_{k-1})}\n    $$\n    This acceleration is applied for $k \\ge 1$. For the first iteration ($k=0$), a standard Picard step $p_1 = G(p_0)$ is used. After computing each new pressure iterate $p_{k+1}$, the corresponding strain $e_{k+1}$ is recomputed using the momentum balance to maintain consistency. The stopping criterion is identical to that of the Newton-Raphson method, based on the norm of the full residual vector $\\mathbf{F}(e_{k+1}, p_{k+1})$.\n\n**Computational Task**\nThe objective is to implement both solvers and, for three distinct parameter sets, find the smallest time step $\\Delta t$ from a given list for which the Anderson-accelerated solver (Method B) converges in strictly fewer iterations than the Newton-Raphson solver (Method A). If Method A fails to converge while Method B succeeds, that $\\Delta t$ also satisfies the condition. If no such $\\Delta t$ is found in the provided list, the result is $-1.0$. The comparison is performed by running both solvers for each $\\Delta t$ and comparing the resulting iteration counts.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\n# Global constants and previous step state\nE_N = 0.0\nP_N = 0.0\nTOL = 1e-10\nMAX_ITER = 200\n\ndef get_S(p, c0, c1, c2):\n    \"\"\"Computes the nonlinear storage term S(p).\"\"\"\n    return c0 * p + c1 * p**2 + c2 * p**3\n\ndef get_dSdp(p, c0, c1, c2):\n    \"\"\"Computes the derivative of the storage term dS/dp.\"\"\"\n    return c0 + 2 * c1 * p + 3 * c2 * p**2\n\ndef solve_newton(params, dt):\n    \"\"\"\n    Solves the system using monolithic Newton-Raphson (Method A).\n    Returns (number of iterations, did_converge).\n    \"\"\"\n    K, alpha, c0, c1, c2, sm, sf = params\n    e_k, p_k = E_N, P_N\n    C = alpha * E_N + get_S(P_N, c0, c1, c2) + dt * sf\n\n    for i in range(MAX_ITER):\n        f1 = K * e_k - alpha * p_k - sm\n        f2 = alpha * e_k + get_S(p_k, c0, c1, c2) - C\n        residual = np.array([f1, f2])\n        \n        if np.linalg.norm(residual)  TOL:\n            return i, True\n\n        dSdp_k = get_dSdp(p_k, c0, c1, c2)\n        J = np.array([[K, -alpha], [alpha, dSdp_k]])\n        \n        if abs(np.linalg.det(J))  1e-14:\n            return i, False # Jacobian is singular\n\n        try:\n            delta_x = np.linalg.solve(J, -residual)\n        except np.linalg.LinAlgError:\n            return i, False\n\n        e_k += delta_x[0]\n        p_k += delta_x[1]\n\n    # Final check after loop\n    f1 = K * e_k - alpha * p_k - sm\n    f2 = alpha * e_k + get_S(p_k, c0, c1, c2) - C\n    if np.linalg.norm(np.array([f1, f2]))  TOL:\n        return MAX_ITER, True\n        \n    return MAX_ITER, False\n\ndef solve_aa(params, dt):\n    \"\"\"\n    Solves the system using block Gauss-Seidel with Anderson Acceleration (Method B).\n    Returns (number of iterations, did_converge).\n    \"\"\"\n    K, alpha, c0, c1, c2, sm, sf = params\n    C = alpha * E_N + get_S(P_N, c0, c1, c2) + dt * sf\n\n    def G(p_in):\n        denom_picard = c0 + c1 * p_in + c2 * p_in**2\n        if abs(denom_picard)  1e-14:\n            return np.nan\n        e_intermediate = (sm + alpha * p_in) / K\n        p_out = (C - alpha * e_intermediate) / denom_picard\n        return p_out\n\n    p_k = P_N\n    p_hist = [None, p_k]\n    g_hist = [None, None]\n\n    for k in range(MAX_ITER):\n        e_k = (sm + alpha * p_k) / K\n        f1 = K * e_k - alpha * p_k - sm\n        f2 = alpha * e_k + get_S(p_k, c0, c1, c2) - C\n        \n        if np.linalg.norm(np.array([f1, f2]))  TOL:\n            return k, True\n\n        g_k = G(p_k)\n        if np.isnan(g_k):\n            return k, False\n\n        g_hist.append(g_k)\n        g_hist.pop(0)\n\n        if k == 0:\n            p_k_plus_1 = g_k\n        else:\n            p_k_minus_1 = p_hist[0]\n            g_k_minus_1 = g_hist[0]\n            \n            f_k = g_k - p_k\n            f_k_minus_1 = g_k_minus_1 - p_k_minus_1\n            \n            denom_aa = f_k - f_k_minus_1\n            if abs(denom_aa) > 1e-14:\n                theta = f_k / denom_aa\n                p_k_plus_1 = theta * g_k_minus_1 + (1 - theta) * g_k\n            else:\n                p_k_plus_1 = g_k\n\n        p_k = p_k_plus_1\n        p_hist.append(p_k)\n        p_hist.pop(0)\n    \n    e_k = (sm + alpha * p_k) / K\n    f1 = K * e_k - alpha * p_k - sm\n    f2 = alpha * e_k + get_S(p_k, c0, c1, c2) - C\n    if np.linalg.norm(np.array([f1, f2]))  TOL:\n        return MAX_ITER, True\n\n    return MAX_ITER, False\n\ndef solve():\n    test_cases = [\n        # Case 1: K, alpha, c0, c1, c2, sm, sf\n        (1.0, 0.8, 0.6, 0.25, 0.10, 0.0, 1.0),\n        # Case 2\n        (1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0),\n        # Case 3\n        (2.0, 0.6, 0.2, 0.6, 0.5, 0.0, 0.5),\n    ]\n\n    dt_candidates = [0.1, 0.3, 1.0, 3.0, 10.0, 30.0, 50.0]\n    results = []\n\n    for params in test_cases:\n        threshold_dt = -1.0\n        for dt in dt_candidates:\n            n_nr, conv_nr = solve_newton(params, dt)\n            n_aa, conv_aa = solve_aa(params, dt)\n\n            condition_met = False\n            if conv_aa:\n                if not conv_nr:\n                    condition_met = True\n                elif n_aa  n_nr:\n                    condition_met = True\n\n            if condition_met:\n                threshold_dt = dt\n                break\n        \n        results.append(threshold_dt)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3561443"}]}