{"hands_on_practices": [{"introduction": "Quasi-Newton methods are iterative, with each step comprised of choosing a search direction and then a step length along that direction. This first practice focuses on the second part: implementing a robust line search algorithm to determine an appropriate step length $\\alpha$. You will implement a backtracking line search that satisfies the strong Wolfe conditions, which are critical for guaranteeing the stability and convergence of BFGS-family methods by ensuring both sufficient decrease in the function value and a sufficiently shallow slope at the new point [@problem_id:3554121]. Mastering this component is the first step toward building a reliable optimization solver from the ground up.", "problem": "Consider the role of line search in quasi-Newton optimization methods, particularly in the Broyden–Fletcher–Goldfarb–Shanno (BFGS) family, as encountered in computational geomechanics when minimizing smooth objective functions derived from energy or data misfit. Given a differentiable scalar objective function $f : \\mathbb{R}^n \\to \\mathbb{R}$ with gradient $\\nabla f$, a current iterate $\\mathbf{x}_k \\in \\mathbb{R}^n$, and a search direction $\\mathbf{p}_k \\in \\mathbb{R}^n$, implement a backtracking line search that returns a step length $\\alpha > 0$ satisfying the strong Wolfe conditions:\n$$\nf(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k,\n$$\n$$\n\\left| \\nabla f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)^\\top \\mathbf{p}_k \\right| \\le c_2 \\left| \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k \\right|,\n$$\nwhere $c_1 \\in (0, 1)$ and $c_2 \\in (c_1, 1)$ are fixed constants. The algorithm must start from an initial step length $\\alpha_0$ and reduce the step length by a constant factor $\\rho \\in (0, 1)$ until both inequalities are satisfied. If $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k \\ge 0$ (i.e., $\\mathbf{p}_k$ is not a descent direction), or if no acceptable step is found within a prescribed maximum number of reductions, the algorithm must return $\\alpha = 0.0$.\n\nUse the following test suite consisting of four cases. In each case, compute and report the resulting step length $\\alpha$ as a floating-point number.\n\nCase A (convex quadratic, steepest descent, happy path):\n- Dimension: $n = 3$.\n- Objective: $f_1(\\mathbf{x}) = \\tfrac{1}{2} \\lVert \\mathbf{x} - \\mathbf{x}^\\star \\rVert_2^2$, with $\\mathbf{x}^\\star = [-1, 2, 0.5]^\\top$.\n- Gradient: $\\nabla f_1(\\mathbf{x}) = \\mathbf{x} - \\mathbf{x}^\\star$.\n- Current iterate: $\\mathbf{x}_k = [2, -1, -0.5]^\\top$.\n- Search direction: $\\mathbf{p}_k = - \\nabla f_1(\\mathbf{x}_k)$.\n- Parameters: $c_1 = 10^{-4}$, $c_2 = 0.9$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, maximum reductions $N_{\\max} = 64$.\n\nCase B (nonconvex Rosenbrock function, steepest descent):\n- Dimension: $n = 2$.\n- Objective: $f_2(\\mathbf{x}) = 100 \\left(x_2 - x_1^2\\right)^2 + \\left(1 - x_1\\right)^2$.\n- Gradient: $\\nabla f_2(\\mathbf{x}) = \\left[\\,-400 x_1 \\left(x_2 - x_1^2\\right) - 2 \\left(1 - x_1\\right),\\; 200 \\left(x_2 - x_1^2\\right)\\right]^\\top$.\n- Current iterate: $\\mathbf{x}_k = [-1.2, 1.0]^\\top$.\n- Search direction: $\\mathbf{p}_k = - \\nabla f_2(\\mathbf{x}_k)$.\n- Parameters: $c_1 = 10^{-4}$, $c_2 = 0.9$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, $N_{\\max} = 64$.\n\nCase C (computational geomechanics parameter calibration, plane strain isotropic linear elasticity, least-squares misfit):\n- Dimension: $n = 2$, parameter vector $\\boldsymbol{\\theta} = [\\lambda, \\mu]^\\top$ where $\\lambda$ and $\\mu$ are Lamé parameters.\n- For two plane strain experiments with strain vectors $\\boldsymbol{\\varepsilon}^{(1)} = [0.001, 0.0005, 0]^\\top$ and $\\boldsymbol{\\varepsilon}^{(2)} = [-0.0003, 0.0002, 0]^\\top$, predict the axial stress $\\sigma_x^{(i)}(\\boldsymbol{\\theta}) = \\lambda \\left(\\varepsilon_x^{(i)} + \\varepsilon_y^{(i)} + \\varepsilon_z^{(i)}\\right) + 2 \\mu \\varepsilon_x^{(i)}$.\n- Measured axial stresses are $\\sigma_{x,\\text{meas}}^{(1)} = 0.005$ and $\\sigma_{x,\\text{meas}}^{(2)} = -0.0008$ (dimensionless).\n- Objective: $f_3(\\boldsymbol{\\theta}) = \\tfrac{1}{2} \\sum_{i=1}^2 \\left(\\sigma_x^{(i)}(\\boldsymbol{\\theta}) - \\sigma_{x,\\text{meas}}^{(i)}\\right)^2$.\n- Gradient: $\\nabla f_3(\\boldsymbol{\\theta}) = \\left[ \\sum_{i=1}^2 r_i \\left(\\varepsilon_x^{(i)} + \\varepsilon_y^{(i)}\\right),\\; \\sum_{i=1}^2 r_i \\left(2 \\varepsilon_x^{(i)}\\right) \\right]^\\top$ where $r_i = \\sigma_x^{(i)}(\\boldsymbol{\\theta}) - \\sigma_{x,\\text{meas}}^{(i)}$.\n- Current iterate: $\\boldsymbol{\\theta}_k = [1.0, 0.5]^\\top$.\n- Search direction: $\\mathbf{p}_k = - \\nabla f_3(\\boldsymbol{\\theta}_k)$.\n- Parameters: $c_1 = 10^{-4}$, $c_2 = 0.9$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, $N_{\\max} = 64$.\n\nCase D (boundary condition, non-descent direction):\n- Use $f_1$ from Case A at the same $\\mathbf{x}_k$.\n- Set $\\mathbf{p}_k = + \\nabla f_1(\\mathbf{x}_k)$.\n- Parameters: $c_1 = 10^{-4}$, $c_2 = 0.9$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, $N_{\\max} = 64$.\n- As $\\nabla f_1(\\mathbf{x}_k)^\\top \\mathbf{p}_k \\ge 0$, the algorithm must return $\\alpha = 0.0$.\n\nYour program must evaluate these four cases and produce a single line of output containing the four resulting step lengths $\\alpha$ as a comma-separated list enclosed in square brackets, for example, $[\\alpha_A,\\alpha_B,\\alpha_C,\\alpha_D]$. No angles or physical units are involved; all quantities are dimensionless. The output values must be represented as floating-point numbers in the default string format of the programming language runtime.", "solution": "The problem requires the implementation of a backtracking line search algorithm to find a step length $\\alpha$ that satisfies the strong Wolfe conditions. This is a fundamental component of many iterative optimization algorithms, including the BFGS method mentioned, which are widely used in computational sciences such as geomechanics for parameter estimation or energy minimization.\n\nThe state of an iterative optimization algorithm at step $k$ is defined by the current iterate $\\mathbf{x}_k \\in \\mathbb{R}^n$. The goal is to find a new iterate $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$ that reduces the value of the objective function $f(\\mathbf{x})$. Here, $\\mathbf{p}_k$ is a search direction (e.g., the negative gradient $-\\nabla f(\\mathbf{x}_k)$ for steepest descent, or $-\\mathbf{B}_k^{-1} \\nabla f(\\mathbf{x}_k)$ for a quasi-Newton method) and $\\alpha_k > 0$ is the step length. The line search is the procedure for finding a suitable $\\alpha_k$.\n\nThe strong Wolfe conditions ensure that the step length $\\alpha$ leads to a sufficient decrease in the objective function and that the slope of the function along the search direction is sufficiently less steep, which is crucial for the stability and convergence of quasi-Newton methods. The two conditions are:\n\n1.  **Armijo Condition (Sufficient Decrease):**\n    $$f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k$$\n    This inequality ensures that the function value at the new point has decreased sufficiently. The constant $c_1 \\in (0, 1)$ controls how much of a decrease is required. A typical value is $c_1 = 10^{-4}$.\n\n2.  **Curvature Condition:**\n    $$ \\left| \\nabla f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)^\\top \\mathbf{p}_k \\right| \\le c_2 \\left| \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k \\right| $$\n    This inequality ensures that the new point is not too close to a region where the function is steep, preventing very small steps. The term $\\nabla f(\\mathbf{x})^\\top \\mathbf{p}_k$ is the directional derivative. The condition requires the magnitude of the new directional derivative to be a fraction of the magnitude of the original one. The constant $c_2 \\in (c_1, 1)$ is typically chosen to be close to $1$, such as $c_2 = 0.9$. For a descent direction, $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k  0$, and the condition helps to exclude points that are too far along the search direction.\n\nThe algorithm specified is a simple backtracking search:\n1.  First, it checks if the search direction $\\mathbf{p}_k$ is a descent direction by evaluating the sign of $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k$. If $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k \\ge 0$, the direction does not lead to a decrease in $f$ for small positive $\\alpha$, so the algorithm must terminate and return $\\alpha = 0.0$.\n2.  Starting with an initial trial step length $\\alpha_0 > 0$, the algorithm enters a loop.\n3.  In each iteration of the loop, it checks if the current $\\alpha$ satisfies both strong Wolfe conditions.\n4.  If both conditions are met, the algorithm has succeeded and returns the current $\\alpha$.\n5.  If not, the step length is reduced by a factor $\\rho \\in (0, 1)$, i.e., $\\alpha \\leftarrow \\rho \\alpha$, and the loop continues.\n6.  If the loop completes $N_{\\max}$ reductions without finding an acceptable step, the search fails, and the algorithm returns $\\alpha = 0.0$.\n\nWe apply this algorithm to four test cases.\n\n**Case A: Convex Quadratic Function**\n- Objective function: $f_1(\\mathbf{x}) = \\tfrac{1}{2} \\lVert \\mathbf{x} - \\mathbf{x}^\\star \\rVert_2^2$, with $\\mathbf{x}^\\star = [-1, 2, 0.5]^\\top$.\n- Gradient: $\\nabla f_1(\\mathbf{x}) = \\mathbf{x} - \\mathbf{x}^\\star$.\n- Iterate: $\\mathbf{x}_k = [2, -1, -0.5]^\\top$.\n- Search direction: $\\mathbf{p}_k = - \\nabla f_1(\\mathbf{x}_k) = - ([2, -1, -0.5]^\\top - [-1, 2, 0.5]^\\top) = -[3, -3, -1]^\\top = [-3, 3, 1]^\\top$.\n- Parameters: $c_1 = 10^{-4}$, $c_2 = 0.9$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, $N_{\\max} = 64$.\nFor this quadratic function, the exact step length that minimizes $f_1$ along $\\mathbf{p}_k$ is known, and for steepest descent, $\\alpha = 1.0$ takes us directly to the minimum $\\mathbf{x}^\\star$ in a single step. The line search algorithm correctly identifies $\\alpha_0 = 1.0$ as satisfying both Wolfe conditions.\n\n**Case B: Nonconvex Rosenbrock Function**\n- Objective function: $f_2(\\mathbf{x}) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2$.\n- Gradient: $\\nabla f_2(\\mathbf{x}) = \\left[\\,-400 x_1 (x_2 - x_1^2) - 2 (1 - x_1),\\; 200 (x_2 - x_1^2)\\right]^\\top$.\n- Iterate: $\\mathbf{x}_k = [-1.2, 1.0]^\\top$.\n- Search direction: $\\mathbf{p}_k = - \\nabla f_2(\\mathbf{x}_k)$. At $\\mathbf{x}_k$, we compute $\\nabla f_2(\\mathbf{x}_k) = [215.6, -88]^\\top$, so $\\mathbf{p}_k = [-215.6, 88]^\\top$.\n- Parameters: $c_1 = 10^{-4}$, $c_2 = 0.9$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, $N_{\\max} = 64$.\nThe Rosenbrock function has a narrow, curved valley. The initial step $\\alpha_0=1.0$ is typically too large and will fail the Armijo condition. The backtracking procedure will reduce $\\alpha$ until a suitable step is found that navigates into the valley without overshooting.\n\n**Case C: Geomechanics Parameter Inversion**\n- Objective function: $f_3(\\boldsymbol{\\theta}) = \\tfrac{1}{2} \\sum_{i=1}^2 (\\sigma_x^{(i)}(\\boldsymbol{\\theta}) - \\sigma_{x,\\text{meas}}^{(i)})^2$, where $\\boldsymbol{\\theta} = [\\lambda, \\mu]^\\top$.\n- The predicted stress is $\\sigma_x^{(i)}(\\boldsymbol{\\theta}) = \\lambda (\\varepsilon_x^{(i)} + \\varepsilon_y^{(i)}) + 2 \\mu \\varepsilon_x^{(i)}$, given plane strain conditions ($\\varepsilon_z^{(i)}=0$). Data for strains $\\boldsymbol{\\varepsilon}^{(i)}$ and measured stresses $\\sigma_{x,\\text{meas}}^{(i)}$ are provided.\n- Gradient: $\\nabla f_3(\\boldsymbol{\\theta}) = \\left[ \\sum_{i=1}^2 r_i (\\varepsilon_x^{(i)} + \\varepsilon_y^{(i)}),\\; \\sum_{i=1}^2 r_i (2 \\varepsilon_x^{(i)}) \\right]^\\top$, with $r_i = \\sigma_x^{(i)}(\\boldsymbol{\\theta}) - \\sigma_{x,\\text{meas}}^{(i)}$.\n- Iterate: $\\boldsymbol{\\theta}_k = [1.0, 0.5]^\\top$.\n- Search direction: $\\mathbf{p}_k = - \\nabla f_3(\\boldsymbol{\\theta}_k)$.\n- Parameters: $c_1 = 10^{-4}$, $c_2 = 0.9$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, $N_{\\max} = 64$.\nThis represents a realistic, albeit simplified, inverse problem. The objective function is a quadratic least-squares misfit, which is convex. As in Case A, a large step is expected. The line search algorithm will find an appropriate step length to reduce the misfit between predicted and measured stresses.\n\n**Case D: Non-Descent Direction**\n- Objective function: $f_1(\\mathbf{x})$ from Case A.\n- Iterate: $\\mathbf{x}_k = [2, -1, -0.5]^\\top$.\n- Search direction: $\\mathbf{p}_k = + \\nabla f_1(\\mathbf{x}_k) = [3, -3, -1]^\\top$.\n- Parameters: $c_1 = 10^{-4}$, $c_2 = 0.9$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, $N_{\\max} = 64$.\nHere, the search direction is an ascent direction. The directional derivative is $\\nabla f_1(\\mathbf{x}_k)^\\top \\mathbf{p}_k = \\lVert \\nabla f_1(\\mathbf{x}_k) \\rVert_2^2 > 0$. The first step of the validation algorithm is to check this condition. Since it is non-negative, the algorithm must immediately return $\\alpha = 0.0$ without performing any backtracking, as per the problem specification.", "answer": "```python\nimport numpy as np\n\ndef backtracking_wolfe_search(f, grad_f, xk, pk, c1, c2, alpha0, rho, n_max):\n    \"\"\"\n    Implements a backtracking line search to find a step length alpha\n    satisfying the strong Wolfe conditions.\n\n    Args:\n        f: The objective function.\n        grad_f: The gradient of the objective function.\n        xk: The current iterate (numpy array).\n        pk: The search direction (numpy array).\n        c1: Constant for the Armijo condition.\n        c2: Constant for the curvature condition.\n        alpha0: Initial step length.\n        rho: Backtracking factor.\n        n_max: Maximum number of backtracking reductions.\n\n    Returns:\n        The step length alpha, or 0.0 if no suitable step is found.\n    \"\"\"\n    # Evaluate function and gradient at the starting point\n    fk = f(xk)\n    grad_fk = grad_f(xk)\n\n    # Directional derivative at the starting point\n    g_k_p_k = np.dot(grad_fk, pk)\n\n    # Condition 1: Check if pk is a descent direction\n    if g_k_p_k >= 0:\n        return 0.0\n\n    alpha = alpha0\n    for _ in range(n_max):\n        x_new = xk + alpha * pk\n        f_new = f(x_new)\n\n        # Check Armijo (sufficient decrease) condition\n        armijo_satisfied = f_new = fk + c1 * alpha * g_k_p_k\n        if not armijo_satisfied:\n            alpha *= rho\n            continue\n\n        # Check curvature condition\n        grad_f_new = grad_f(x_new)\n        g_new_p_k = np.dot(grad_f_new, pk)\n        curvature_satisfied = abs(g_new_p_k) = c2 * abs(g_k_p_k)\n\n        if curvature_satisfied:\n            return alpha  # Found a suitable step length\n\n        alpha *= rho\n\n    # Condition 2: Max reductions reached\n    return 0.0\n\ndef solve():\n    \"\"\"\n    Solves the four test cases specified in the problem.\n    \"\"\"\n    results = []\n\n    # --- Case A ---\n    x_star_A = np.array([-1.0, 2.0, 0.5])\n    def f1(x):\n        d = x - x_star_A\n        return 0.5 * np.dot(d, d)\n    def grad_f1(x):\n        return x - x_star_A\n\n    xk_A = np.array([2.0, -1.0, -0.5])\n    pk_A = -grad_f1(xk_A)\n    params_A = {'c1': 1e-4, 'c2': 0.9, 'alpha0': 1.0, 'rho': 0.5, 'n_max': 64}\n    alpha_A = backtracking_wolfe_search(f1, grad_f1, xk_A, pk_A, **params_A)\n    results.append(alpha_A)\n\n    # --- Case B ---\n    def f2(x):\n        return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n    def grad_f2(x):\n        dx1 = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n        dx2 = 200 * (x[1] - x[0]**2)\n        return np.array([dx1, dx2])\n\n    xk_B = np.array([-1.2, 1.0])\n    pk_B = -grad_f2(xk_B)\n    params_B = {'c1': 1e-4, 'c2': 0.9, 'alpha0': 1.0, 'rho': 0.5, 'n_max': 64}\n    alpha_B = backtracking_wolfe_search(f2, grad_f2, xk_B, pk_B, **params_B)\n    results.append(alpha_B)\n\n    # --- Case C ---\n    eps1 = np.array([0.001, 0.0005, 0.0])\n    eps2 = np.array([-0.0003, 0.0002, 0.0])\n    sigma_meas1 = 0.005\n    sigma_meas2 = -0.0008\n\n    def sigma_x(theta, eps):\n        lam, mu = theta\n        return lam * (eps[0] + eps[1] + eps[2]) + 2 * mu * eps[0]\n\n    def f3(theta):\n        res1 = sigma_x(theta, eps1) - sigma_meas1\n        res2 = sigma_x(theta, eps2) - sigma_meas2\n        return 0.5 * (res1**2 + res2**2)\n    \n    def grad_f3(theta):\n        res1 = sigma_x(theta, eps1) - sigma_meas1\n        res2 = sigma_x(theta, eps2) - sigma_meas2\n        \n        d_dlam = res1 * (eps1[0] + eps1[1]) + res2 * (eps2[0] + eps2[1])\n        d_dmu = res1 * (2 * eps1[0]) + res2 * (2 * eps2[0])\n        \n        return np.array([d_dlam, d_dmu])\n\n    thetak_C = np.array([1.0, 0.5])\n    pk_C = -grad_f3(thetak_C)\n    params_C = {'c1': 1e-4, 'c2': 0.9, 'alpha0': 1.0, 'rho': 0.5, 'n_max': 64}\n    alpha_C = backtracking_wolfe_search(f3, grad_f3, thetak_C, pk_C, **params_C)\n    results.append(alpha_C)\n\n    # --- Case D ---\n    xk_D = np.array([2.0, -1.0, -0.5])\n    pk_D = grad_f1(xk_D) # Non-descent direction\n    params_D = {'c1': 1e-4, 'c2': 0.9, 'alpha0': 1.0, 'rho': 0.5, 'n_max': 64}\n    alpha_D = backtracking_wolfe_search(f1, grad_f1, xk_D, pk_D, **params_D)\n    results.append(alpha_D)\n    \n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3554121"}, {"introduction": "With a method for choosing a step length in hand, we now turn to the heart of the L-BFGS algorithm: calculating the search direction. This practice guides you through implementing the famous L-BFGS two-loop recursion, which efficiently computes the product of the inverse Hessian approximation and the gradient, $\\mathbf{p}_k = -\\mathbf{H}_k \\nabla f(\\boldsymbol{\\theta}_k)$, without ever forming or storing the full matrix $\\mathbf{H}_k$ [@problem_id:3554151]. This exercise is key to understanding how L-BFGS achieves its remarkable efficiency and low memory usage, making it a powerhouse for large-scale problems common in computational geomechanics.", "problem": "You are tasked with implementing one step of the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) method to compute a search direction for a parameter estimation problem in computational geomechanics. Consider a normalized three-parameter calibration problem for a constitutive soil model, where the objective is to minimize a smooth scalar function $f(\\boldsymbol{\\theta})$ that represents the sum of squares of residuals between simulated and observed responses. The computational geomechanics context supplies gradients of $f$ at the current iterate and a memory of curvature pairs from previous steps. Your implementation must compute one L-BFGS search direction using the available information only, without performing any line search or update of the parameters.\n\nStart from the following foundational base:\n- The quasi-Newton paradigm builds an approximation $\\mathbf{H}_k$ to the inverse of the true Hessian based on the secant condition that enforces expected curvature along steps. Define the step and curvature vectors as follows: $\\mathbf{s}_i = \\boldsymbol{\\theta}_{i+1} - \\boldsymbol{\\theta}_i$ and $\\mathbf{y}_i = \\nabla f(\\boldsymbol{\\theta}_{i+1}) - \\nabla f(\\boldsymbol{\\theta}_i)$, for indices $i$ corresponding to previous iterations.\n- The Broyden–Fletcher–Goldfarb–Shanno (BFGS) family constructs updates that satisfy the secant condition and maintain symmetry and positive definiteness under standard curvature conditions.\n- In the limited-memory variant, the search direction is computed by implicitly applying the inverse-Hessian approximation built from a small set of the most recent $\\left(\\mathbf{s}_i,\\mathbf{y}_i\\right)$ pairs, using an initial scaling of the inverse-Hessian approximation equal to a multiple of the identity.\n\nYou must implement the following in your program:\n- Compute the L-BFGS search direction $\\mathbf{p}_k = -\\mathbf{H}_k \\nabla f(\\boldsymbol{\\theta}_k)$ using a two-loop strategy that is consistent with the BFGS secant conditions and limited-memory aggregation of the given $\\left(\\mathbf{s}_i,\\mathbf{y}_i\\right)$ pairs.\n- Use the scaled initial inverse-Hessian $\\mathbf{H}_k^{0} = \\gamma \\mathbf{I}$, where $\\gamma$ is specified by $\\gamma = \\dfrac{\\mathbf{s}_{k-1}^{\\mathsf{T}} \\mathbf{y}_{k-1}}{\\mathbf{y}_{k-1}^{\\mathsf{T}} \\mathbf{y}_{k-1}}$.\n- Assume all vectors are non-dimensional and normalized so that no physical units are required in the output.\n\nInput is embedded in the program as a test suite. You must use the following test cases, which each provide the current gradient $\\nabla f(\\boldsymbol{\\theta}_k)$ and a list of $\\left(\\mathbf{s}_i,\\mathbf{y}_i\\right)$ pairs ordered from oldest to newest. All vectors have dimension $3$, and each pair satisfies the standard curvature condition $\\mathbf{y}_i^{\\mathsf{T}}\\mathbf{s}_i > 0$.\n\n- Test Case $1$ (happy path):\n  - Current gradient: $\\nabla f(\\boldsymbol{\\theta}_k) = \\left[0.8,\\,-1.2,\\,0.5\\right]$.\n  - Memory pairs: \n    - $\\mathbf{s}_{k-2} = \\left[0.1,\\,-0.05,\\,0.02\\right]$, $\\mathbf{y}_{k-2} = \\left[0.12,\\,-0.06,\\,0.015\\right]$,\n    - $\\mathbf{s}_{k-1} = \\left[0.08,\\,-0.02,\\,0.01\\right]$, $\\mathbf{y}_{k-1} = \\left[0.09,\\,-0.025,\\,0.008\\right]$.\n\n- Test Case $2$ (boundary memory size of $1$):\n  - Current gradient: $\\nabla f(\\boldsymbol{\\theta}_k) = \\left[-0.3,\\,0.1,\\,-0.4\\right]$.\n  - Memory pairs:\n    - $\\mathbf{s}_{k-1} = \\left[0.05,\\,0.02,\\,-0.01\\right]$, $\\mathbf{y}_{k-1} = \\left[0.06,\\,0.025,\\,-0.012\\right]$.\n\n- Test Case $3$ (edge case with nearly collinear curvature in the latest pair):\n  - Current gradient: $\\nabla f(\\boldsymbol{\\theta}_k) = \\left[0.5,\\,-0.4,\\,0.2\\right]$.\n  - Memory pairs:\n    - $\\mathbf{s}_{k-3} = \\left[0.12,\\,0.03,\\,-0.02\\right]$, $\\mathbf{y}_{k-3} = \\left[0.130,\\,0.032,\\,-0.018\\right]$,\n    - $\\mathbf{s}_{k-2} = \\left[0.04,\\,0.01,\\,0.005\\right]$, $\\mathbf{y}_{k-2} = \\left[0.039,\\,0.011,\\,0.0048\\right]$,\n    - $\\mathbf{s}_{k-1} = \\left[0.02,\\,-0.015,\\,0.007\\right]$, $\\mathbf{y}_{k-1} = \\left[0.0195,\\,-0.014,\\,0.0065\\right]$.\n\nYour program must:\n- Implement the L-BFGS two-loop strategy to compute $\\mathbf{p}_k$ for each test case using only the provided gradient and memory pairs, with the specified $\\gamma$ scaling for $\\mathbf{H}_k^{0}$.\n- Produce a single line of output containing the concatenated search direction components for Test Case $1$, then Test Case $2$, then Test Case $3$, in that order. Each component must be a decimal float rounded to $6$ decimal places.\n- The final output format must be a comma-separated list enclosed in square brackets, with the nine components ordered as $\\left[p^{(1)}_1,\\,p^{(1)}_2,\\,p^{(1)}_3,\\,p^{(2)}_1,\\,p^{(2)}_2,\\,p^{(2)}_3,\\,p^{(3)}_1,\\,p^{(3)}_2,\\,p^{(3)}_3\\right]$, where superscripts denote the test case index and subscripts denote the component index.\n\nNo user input is required; all data must be embedded in the program. The expected output type is a list of floats formatted as specified above.", "solution": "The objective is to compute one Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) search direction for a smooth least-squares calibration problem in computational geomechanics. The fundamental base begins with the quasi-Newton strategy: we approximate the inverse Hessian $\\mathbf{H}_k \\approx \\nabla^2 f(\\boldsymbol{\\theta}_k)^{-1}$ using curvature information extracted from prior iterations. The secant condition is given by\n$$\n\\mathbf{H}_{k+1}\\mathbf{y}_k = \\mathbf{s}_k,\n$$\nwhere $\\mathbf{s}_k = \\boldsymbol{\\theta}_{k+1} - \\boldsymbol{\\theta}_k$ and $\\mathbf{y}_k = \\nabla f(\\boldsymbol{\\theta}_{k+1}) - \\nabla f(\\boldsymbol{\\theta}_k)$. This condition ensures that the approximate inverse Hessian reproduces correct curvature along the new displacement. The Broyden–Fletcher–Goldfarb–Shanno (BFGS) update constructs $\\mathbf{H}_{k+1}$ to be symmetric and positive definite if $\\mathbf{y}_k^{\\mathsf{T}}\\mathbf{s}_k > 0$, which is the curvature condition satisfied by the given data.\n\nIn the limited-memory version, rather than forming $\\mathbf{H}_k$ explicitly, we apply it to the gradient via a two-loop recursion that aggregates information from the last $m$ pairs $\\left(\\mathbf{s}_i,\\mathbf{y}_i\\right)$, typically with $m$ small (for example $m \\in \\{1,2,3\\}$ in our test suite). The two-loop recursion proceeds by:\n- Starting with $\\mathbf{q} = \\nabla f(\\boldsymbol{\\theta}_k)$,\n- Looping over the memory pairs in reverse order to subtract curvature contributions using scalars that depend on $\\mathbf{s}_i$, $\\mathbf{y}_i$, and inner products,\n- Applying the initial inverse-Hessian scaling $\\mathbf{H}_k^{0} = \\gamma \\mathbf{I}$ to $\\mathbf{q}$,\n- Looping forward over the memory pairs to add back displacement contributions that ensure the secant imposition,\n- Finally yielding $\\mathbf{r} \\approx \\mathbf{H}_k \\nabla f(\\boldsymbol{\\theta}_k)$ and the search direction $\\mathbf{p}_k = -\\mathbf{r}$.\n\nThe initial inverse-Hessian scaling is set to\n$$\n\\gamma = \\frac{\\mathbf{s}_{k-1}^{\\mathsf{T}} \\mathbf{y}_{k-1}}{\\mathbf{y}_{k-1}^{\\mathsf{T}} \\mathbf{y}_{k-1}},\n$$\nwhich provides a meaningful scale for the inverse-Hessian along the most recent curvature. This choice is widely used to stabilize the recursion, especially when the limited memory does not provide a full spectrum of curvature information.\n\nAlgorithmic derivation from principles:\n1. The quasi-Newton philosophy is to approximate the action of $\\mathbf{H}_k$ on a vector without forming $\\mathbf{H}_k$. Using the BFGS update, the implicit application to a vector can be constructed from the stored pairs. The two-loop recursion is derived by repeatedly applying rank-one and rank-two update structures in a manner that respects the secant condition and symmetry.\n2. Define $\\rho_i = \\left(\\mathbf{y}_i^{\\mathsf{T}}\\mathbf{s}_i\\right)^{-1}$ for each stored pair (this uses the curvature condition and ensures positivity). Then for the reverse loop, compute\n   - $\\alpha_i = \\rho_i \\mathbf{s}_i^{\\mathsf{T}} \\mathbf{q}$,\n   - Update $\\mathbf{q} \\leftarrow \\mathbf{q} - \\alpha_i \\mathbf{y}_i$,\n   walking from the newest pair $i = k-1$ down to the oldest in memory.\n3. Apply the initial scaling: $\\mathbf{r} = \\mathbf{H}_k^{0} \\mathbf{q} = \\gamma \\mathbf{q}$.\n4. In the forward loop, for the same indices from oldest to newest, compute\n   - $\\beta_i = \\rho_i \\mathbf{y}_i^{\\mathsf{T}} \\mathbf{r}$,\n   - Update $\\mathbf{r} \\leftarrow \\mathbf{r} + \\mathbf{s}_i \\left(\\alpha_i - \\beta_i\\right)$.\n5. Set $\\mathbf{p}_k = -\\mathbf{r}$.\n\nThis procedure is equivalent to applying the limited-memory inverse-Hessian approximation to the gradient and accurately captures curvature along directions represented by the stored pairs. The use of $\\gamma$ from the last pair appropriately scales the identity component for directions not well represented by the memory.\n\nCoverage of test suite:\n- Test Case $1$ represents the general situation with two memory pairs, validating the normal behavior of the recursion and scaling.\n- Test Case $2$ uses a single memory pair, probing the boundary of limited memory where the direction should still reflect recent curvature while relying heavily on the initial scaling.\n- Test Case $3$ provides three pairs with the newest pair nearly collinear relative to the displacement, testing numerical stability and the role of $\\gamma$ when recent curvature magnitude is small but positive.\n\nImplementation details:\n- Vectors are treated as NumPy arrays of shape $3$.\n- Inner products are computed via standard vector dot products.\n- The recursion strictly follows the principle-derived steps outlined above.\n- The final output concatenates the components $\\mathbf{p}_k$ for the three test cases into a single line, formatted to six decimal places as required. Because all parameters are normalized, no physical units are attached to the numbers.\n\nThis solution integrates the quasi-Newton foundation (secant condition and curvature aggregation) with the algorithmic design of the L-BFGS two-loop recursion and initial Hessian scaling, yielding robust search directions in a computational geomechanics calibration context.", "answer": "```python\nimport numpy as np\n\ndef lbfgs_direction(grad, s_list, y_list):\n    \"\"\"\n    Compute one L-BFGS search direction p = -H_k * grad using two-loop recursion\n    with initial scaling H0 = gamma * I, where gamma is computed from the last pair.\n\n    Parameters:\n        grad: numpy array, gradient at current iterate (shape: (n,))\n        s_list: list of numpy arrays, step vectors (oldest to newest)\n        y_list: list of numpy arrays, curvature vectors (oldest to newest)\n\n    Returns:\n        p: numpy array, search direction (shape: (n,))\n    \"\"\"\n    # Ensure lists are consistent\n    assert len(s_list) == len(y_list) and len(s_list) >= 1\n    m = len(s_list)\n    n = grad.shape[0]\n\n    # Precompute rho_i = 1 / (y_i^T s_i)\n    rho = []\n    for s_i, y_i in zip(s_list, y_list):\n        ys = float(np.dot(y_i, s_i))\n        if ys = 0:\n            # Curvature condition violated; in robust implementations we might skip this pair.\n            # Here we enforce positivity per problem statement, but guard against zero.\n            ys = 1e-12\n        rho.append(1.0 / ys)\n\n    # Two-loop recursion\n    q = grad.copy()\n    alpha = [0.0] * m\n    # Loop from newest to oldest\n    for i in range(m - 1, -1, -1):\n        s_i = s_list[i]\n        y_i = y_list[i]\n        alpha[i] = rho[i] * float(np.dot(s_i, q))\n        q = q - alpha[i] * y_i\n\n    # Initial scaling H0 = gamma * I, gamma from the newest pair\n    y_last = y_list[-1]\n    s_last = s_list[-1]\n    denom = float(np.dot(y_last, y_last))\n    if denom = 0:\n        denom = 1e-12\n    gamma = float(np.dot(s_last, y_last)) / denom\n    r = gamma * q\n\n    # Forward loop from oldest to newest\n    for i in range(m):\n        s_i = s_list[i]\n        y_i = y_list[i]\n        beta = rho[i] * float(np.dot(y_i, r))\n        r = r + s_i * (alpha[i] - beta)\n\n    # Search direction\n    p = -r\n    return p\n\ndef solve():\n    # Define the test cases as per the problem statement.\n    # Each case: (grad, s_list, y_list)\n    test_cases = []\n\n    # Test Case 1\n    grad1 = np.array([0.8, -1.2, 0.5], dtype=float)\n    s1_list = [\n        np.array([0.1, -0.05, 0.02], dtype=float),\n        np.array([0.08, -0.02, 0.01], dtype=float),\n    ]\n    y1_list = [\n        np.array([0.12, -0.06, 0.015], dtype=float),\n        np.array([0.09, -0.025, 0.008], dtype=float),\n    ]\n    test_cases.append((grad1, s1_list, y1_list))\n\n    # Test Case 2\n    grad2 = np.array([-0.3, 0.1, -0.4], dtype=float)\n    s2_list = [\n        np.array([0.05, 0.02, -0.01], dtype=float),\n    ]\n    y2_list = [\n        np.array([0.06, 0.025, -0.012], dtype=float),\n    ]\n    test_cases.append((grad2, s2_list, y2_list))\n\n    # Test Case 3\n    grad3 = np.array([0.5, -0.4, 0.2], dtype=float)\n    s3_list = [\n        np.array([0.12, 0.03, -0.02], dtype=float),\n        np.array([0.04, 0.01, 0.005], dtype=float),\n        np.array([0.02, -0.015, 0.007], dtype=float),\n    ]\n    y3_list = [\n        np.array([0.130, 0.032, -0.018], dtype=float),\n        np.array([0.039, 0.011, 0.0048], dtype=float),\n        np.array([0.0195, -0.014, 0.0065], dtype=float),\n    ]\n    test_cases.append((grad3, s3_list, y3_list))\n\n    # Compute search directions and format output\n    results = []\n    for grad, s_list, y_list in test_cases:\n        p = lbfgs_direction(grad, s_list, y_list)\n        results.extend(list(p))\n\n    # Format as a single line: comma-separated floats rounded to 6 decimal places within brackets\n    formatted = [f\"{x:.6f}\" for x in results]\n    print(f\"[{','.join(formatted)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3554151"}, {"introduction": "The standard BFGS update relies on the curvature condition $y_k^\\top s_k > 0$ to ensure the Hessian approximation remains positive-definite. However, in realistic geomechanics models that include strain-softening or damage, the underlying energy function can be non-convex, leading to violations of this condition. This final practice tackles this advanced challenge by asking you to implement and compare two strategies for robustly handling negative curvature: skipping the update or applying a Powell-damping modification [@problem_id:3554083]. This exercise will deepen your understanding of the practical limitations of BFGS and equip you with techniques to build stable solvers for complex, non-convex problems.", "problem": "Consider the unconstrained minimization of a nonconvex, anisotropic stored-energy surrogate that emulates strain-softening in computational geomechanics. Let the decision vector be $x \\in \\mathbb{R}^2$. Define the anisotropic scalar $q(x)$ by\n$$\nq(x) = a_1 x_1^2 + a_2 x_2^2,\n$$\nand the stored-energy surrogate by\n$$\nf(x) = \\tfrac{1}{2} x^\\top C_0 x - \\tfrac{1}{4} c \\, q(x)^2 + \\tfrac{\\mu}{6} \\, q(x)^3,\n$$\nwhere $C_0 = \\mathrm{diag}(E_1, E_2)$ is a symmetric positive definite stiffness, and the parameters $c>0$ and $\\mu>0$ provide softening and a sixth-order stabilizer, respectively. The anisotropy in softening is controlled by $a_1>0$ and $a_2>0$. The softening term $-\\tfrac{1}{4} c \\, q(x)^2$ yields regions of negative curvature for sufficiently large $q(x)$, while the stabilizer $\\tfrac{\\mu}{6} q(x)^3$ ensures boundedness below.\n\nYou are to implement two quasi-Newton solvers for minimizing $f(x)$, both based on the Broyden–Fletcher–Goldfarb–Shanno (BFGS) method. The solvers must use a backtracking Armijo line search and must differ only in how the curvature pair $\\{s_k, y_k\\}$ is handled when softening induces $y_k^\\top s_k  0$. Here $s_k = x_{k+1} - x_k$ and $y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k)$. The solvers are:\n\n- Solver A (Powell-damped BFGS): Enforce a curvature condition by constructing a blended vector $\\tilde{y}_k$ that is a convex combination of $y_k$ and $B_k s_k$ chosen to be the smallest blend ensuring $s_k^\\top \\tilde{y}_k \\ge \\eta \\, s_k^\\top B_k s_k$ for a fixed constant $\\eta \\in (0,1)$, where $B_k$ is the current symmetric positive definite approximation of the Hessian. Update $B_k$ using the standard BFGS secant formula with $\\tilde{y}_k$.\n\n- Solver B (pair skipping): If $s_k^\\top y_k \\le \\varepsilon$ for a fixed threshold $\\varepsilon > 0$, skip the BFGS update and retain $B_{k+1} = B_k$; otherwise, use the standard BFGS update with $y_k$.\n\nBoth solvers must use a backtracking Armijo rule with sufficient decrease parameter $c_1 \\in (0,1)$ and reduction factor $\\rho \\in (0,1)$. If a step computed with the quasi-Newton direction fails to produce a descent direction (due to numerical issues), fall back to the steepest descent direction for that iteration.\n\nStart each solve from a given initial vector $x_0 \\in \\mathbb{R}^2$ and initial Hessian approximation $B_0 = I$. Terminate when $\\|\\nabla f(x)\\|_2 \\le \\mathrm{tol}$ or when the iteration count reaches $\\mathrm{max\\_iter}$. For each solve, record:\n- A convergence flag equal to $1$ if terminated by $\\|\\nabla f(x)\\|_2 \\le \\mathrm{tol}$ and $0$ otherwise.\n- The number of iterations performed (an integer).\n- The number of raw negative curvature events, defined as the count of iterations for which $s_k^\\top y_k  0$ before any damping or skipping modification is applied.\n\nYour program must implement both solvers and evaluate them on the following test suite of parameter sets. For each test case, run Solver A and Solver B and report the triplets in the order described above for each solver.\n\nUse the following fixed algorithmic constants for all test cases: Armijo parameter $c_1 = 10^{-4}$, reduction factor $\\rho = \\tfrac{1}{2}$, Powell damping parameter $\\eta = \\tfrac{1}{5}$, curvature threshold $\\varepsilon = 10^{-12}$, tolerance $\\mathrm{tol} = 10^{-6}$, and maximum iterations $\\mathrm{max\\_iter} = 200$.\n\nTest suite (each as $(E_1, E_2, a_1, a_2, c, \\mu, x_{0,1}, x_{0,2})$):\n\n- Case $1$: $(200, 100, 1.0, 0.5, 0.8, 0.08, 0.6, 0.4)$.\n- Case $2$: $(150, 80, 1.0, 0.2, 1.5, 0.10, 1.0, 0.1)$.\n- Case $3$: $(120, 60, 2.5, 0.1, 2.0, 0.12, 0.8, 0.8)$.\n- Case $4$: $(300, 50, 0.3, 2.0, 1.2, 0.20, 0.2, 1.1)$.\n\nFundamental base and modeling assumptions you must use:\n- Define the gradient $\\nabla f(x)$ from the given $f(x)$ and $q(x)$ by applying standard multivariate calculus.\n- Use the quasi-Newton secant condition and symmetry to derive the BFGS update that preserves positive definiteness under the enforced curvature condition.\n- Use the Armijo sufficient decrease condition for line search to ensure global convergence behavior in the presence of nonconvexity.\n- Recognize that softening-induced negative curvature can render the true Hessian indefinite, which is reflected in the sign of $y_k^\\top s_k$ for small steps.\n\nFinal output format:\n- For each test case $i \\in \\{1,2,3,4\\}$, construct a list \n$$\n[r^{(A)}_i, r^{(B)}_i] = [\\mathrm{conv}^{(A)}_i, \\mathrm{conv}^{(B)}_i, \\mathrm{iters}^{(A)}_i, \\mathrm{iters}^{(B)}_i, \\mathrm{neg}^{(A)}_i, \\mathrm{neg}^{(B)}_i],\n$$\nwhere the superscripts $(A)$ and $(B)$ correspond to Solver A and Solver B, respectively, and each entry is an integer.\n- Your program should produce a single line of output containing the results as a comma-separated list of the four per-case lists enclosed in square brackets (for example, $[ [\\cdot], [\\cdot], [\\cdot], [\\cdot] ]$ but without any spaces).\n\nYour task:\n- Implement the described solvers, apply them to the specified test suite, and output the aggregated list of results in the exact format specified. No external input is required, and no physical units are involved; all angles, if any, are to be considered dimensionless, but this problem does not involve angles.", "solution": "We construct a rigorous solution based on fundamental quasi-Newton principles coupled with an anisotropic softening energy surrogate. The goal is to examine, in a controlled and testable setting, how negative curvature events characterized by $y_k^\\top s_k  0$ affect the Broyden–Fletcher–Goldfarb–Shanno (BFGS) update, and to compare two standard remedies: Powell damping and pair skipping.\n\n1. Modeling and gradient. Define the anisotropic scalar\n$$\nq(x) = a_1 x_1^2 + a_2 x_2^2,\n$$\nand the energy\n$$\nf(x) = \\tfrac{1}{2} x^\\top C_0 x - \\tfrac{1}{4} c \\, q(x)^2 + \\tfrac{\\mu}{6} \\, q(x)^3,\n$$\nwith $C_0 = \\mathrm{diag}(E_1, E_2)$, $c>0$, $\\mu>0$, $a_1>0$, and $a_2>0$. The gradient is derived from multivariate calculus. First compute\n$$\n\\frac{\\partial q}{\\partial x_i} = 2 a_i x_i, \\quad i \\in \\{1,2\\}.\n$$\nApplying the chain rule to $f(x)$ yields\n$$\n\\frac{\\partial f}{\\partial x_i} = E_i x_i - \\frac{1}{2} c \\cdot 2 q(x) \\cdot \\frac{\\partial q}{\\partial x_i} + \\frac{1}{6} \\mu \\cdot 3 q(x)^2 \\cdot \\frac{\\partial q}{\\partial x_i}.\n$$\nSubstituting $\\frac{\\partial q}{\\partial x_i} = 2 a_i x_i$ gives\n$$\n\\frac{\\partial f}{\\partial x_i} = E_i x_i - c \\, q(x) \\, a_i x_i + \\mu \\, q(x)^2 \\, a_i x_i.\n$$\nTherefore,\n$$\n\\nabla f(x) = \\begin{bmatrix}\n\\left(E_1 - c \\, a_1 q(x) + \\mu \\, a_1 q(x)^2\\right) x_1 \\\\\n\\left(E_2 - c \\, a_2 q(x) + \\mu \\, a_2 q(x)^2\\right) x_2\n\\end{bmatrix}.\n$$\nIn regions where $q(x)$ is moderate and $c$ is sufficiently large relative to $E_1, E_2$, the term $- c \\, q(x)$ can dominate the local curvature, producing negative curvature in the true Hessian. This softening-induced negative curvature is reflected by instances where $y_k^\\top s_k  0$ for sufficiently small steps $s_k$ between iterates.\n\n2. Quasi-Newton foundation and BFGS update. The quasi-Newton method builds a sequence of symmetric approximations $B_k \\approx \\nabla^2 f(x_k)$ designed to satisfy the secant condition\n$$\nB_{k+1} s_k = y_k, \\quad s_k = x_{k+1} - x_k, \\quad y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k),\n$$\nand to remain symmetric. The BFGS update uniquely arises under a variational principle that minimally changes $B_k$ (in a weighted Frobenius norm) while satisfying symmetry and the secant condition. The standard BFGS update is\n$$\nB_{k+1} = B_k - \\frac{B_k s_k s_k^\\top B_k}{s_k^\\top B_k s_k} + \\frac{y_k y_k^\\top}{s_k^\\top y_k}.\n$$\nIf $s_k^\\top y_k > 0$ and $B_k$ is symmetric positive definite (SPD), then $B_{k+1}$ remains SPD. However, in softening regimes one often observes $s_k^\\top y_k \\le 0$ (or very small), breaking this guarantee. Two standard remedies address this:\n\n- Powell damping constructs $\\tilde{y}_k$ as a convex combination of $y_k$ and $B_k s_k$ that is the smallest blend satisfying the curvature condition\n$$\ns_k^\\top \\tilde{y}_k \\ge \\eta \\, s_k^\\top B_k s_k, \\quad \\eta \\in (0,1).\n$$\nThis choice preserves the secant condition in a weakened form with $\\tilde{y}_k$ and guarantees $B_{k+1}$ remains SPD. The construction of the smallest blend follows from projecting $y_k$ along $B_k s_k$ to enforce the inequality tightly when needed.\n\n- Pair skipping enforces robustness by skipping the update whenever $s_k^\\top y_k \\le \\varepsilon$ for a small threshold $\\varepsilon > 0$, keeping $B_{k+1} = B_k$; otherwise the standard BFGS update is applied. Since $B_k$ remains SPD from initialization, the search direction is a descent direction as long as the Armijo condition is enforced.\n\n3. Globalization via Armijo line search. Given a descent direction $p_k$ computed by solving\n$$\nB_k p_k = -\\nabla f(x_k),\n$$\nwe select a step length $\\alpha_k$ by backtracking to satisfy the Armijo condition\n$$\nf(x_k + \\alpha_k p_k) \\le f(x_k) + c_1 \\alpha_k \\nabla f(x_k)^\\top p_k,\n$$\nwhere $c_1 \\in (0,1)$ and $\\alpha_k$ is reduced by a fixed factor $\\rho \\in (0,1)$ until the condition holds. If numerical issues produce a non-descent direction (i.e., $\\nabla f(x_k)^\\top p_k \\ge 0$), the method falls back to the steepest descent direction $p_k = -\\nabla f(x_k)$, preserving descent for the Armijo condition.\n\n4. Failure modes and metrics. In softening-dominated regions, the true Hessian can be indefinite, and small steps can produce $y_k^\\top s_k  0$, which would cause the BFGS update to lose positive definiteness if unmodified. Powell damping remedies this by enforcing a lower bound on $s_k^\\top \\tilde{y}_k$. Pair skipping avoids unstable updates but can slow convergence by withholding curvature information, particularly if $y_k^\\top s_k \\le 0$ occurs frequently. Consequently, two metrics are insightful:\n- The convergence flag (whether $\\|\\nabla f(x)\\|_2 \\le \\mathrm{tol}$ is achieved within $\\mathrm{max\\_iter}$).\n- The number of iterations required when convergence occurs (or the cap $\\mathrm{max\\_iter}$ if not).\n- The count of raw negative curvature events where $s_k^\\top y_k  0$, indicating how often softening-induced curvature would have broken the standard BFGS update.\n\n5. Algorithmic constants and test coverage. We fix $c_1 = 10^{-4}$, $\\rho = \\tfrac{1}{2}$, $\\eta = \\tfrac{1}{5}$, $\\varepsilon = 10^{-12}$, $\\mathrm{tol} = 10^{-6}$, and $\\mathrm{max\\_iter} = 200$. The test suite spans:\n- A “happy path” with moderate softening and anisotropy (Case $1$).\n- Stronger softening with directional bias (Case $2$).\n- A boundary scenario with severe anisotropy and softening that is prone to $y_k^\\top s_k  0$ (Case $3$).\n- An edge case with pronounced directional stiffness and softening nonconvexity (Case $4$).\n\n6. Implementation and output. For each test case, initialize $x_0$ and $B_0 = I$. Run Solver A (Powell-damped BFGS) and Solver B (pair skipping). For each solver, record the convergence flag, iteration count, and raw negative curvature event count. Aggregate the per-case results into the single-line output as specified:\n$$\n\\big[ [\\mathrm{conv}^{(A)}_1, \\mathrm{conv}^{(B)}_1, \\mathrm{iters}^{(A)}_1, \\mathrm{iters}^{(B)}_1, \\mathrm{neg}^{(A)}_1, \\mathrm{neg}^{(B)}_1], \\ldots, [\\mathrm{conv}^{(A)}_4, \\mathrm{conv}^{(B)}_4, \\mathrm{iters}^{(A)}_4, \\mathrm{iters}^{(B)}_4, \\mathrm{neg}^{(A)}_4, \\mathrm{neg}^{(B)}_4] \\big],\n$$\nwith no spaces in the printed representation.\n\nThis construction directly tests the failure modes of BFGS in the presence of softening-induced negative curvature and quantitatively compares Powell-damped updates with pair skipping under anisotropic damage-like nonconvexity.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f_factory(E1, E2, a1, a2, c_soft, mu):\n    C0 = np.diag([E1, E2])\n    a = np.array([a1, a2], dtype=float)\n\n    def q(x):\n        return a[0]*x[0]**2 + a[1]*x[1]**2\n\n    def f(x):\n        qx = q(x)\n        return 0.5 * x @ (C0 @ x) - 0.25 * c_soft * (qx**2) + (mu/6.0) * (qx**3)\n\n    def grad(x):\n        qx = q(x)\n        # Gradient components as derived: g_i = E_i x_i - c*q*a_i*x_i + mu*q^2*a_i*x_i\n        g0 = (E1 - c_soft * a1 * qx + mu * a1 * (qx**2)) * x[0]\n        g1 = (E2 - c_soft * a2 * qx + mu * a2 * (qx**2)) * x[1]\n        return np.array([g0, g1], dtype=float)\n\n    return f, grad\n\ndef armijo_backtracking(f, grad, x, p, c1=1e-4, rho=0.5, max_backtracks=40):\n    g = grad(x)\n    gTp = g.dot(p)\n    if gTp >= 0.0:\n        # Not a descent direction; fallback to steepest descent\n        p = -g\n        gTp = g.dot(p)\n    alpha = 1.0\n    fx = f(x)\n    for _ in range(max_backtracks):\n        x_new = x + alpha * p\n        if f(x_new) = fx + c1 * alpha * gTp:\n            return alpha, p, False  # success, not a failure\n        alpha *= rho\n    # If failed, try steepest descent if not already doing so\n    if not np.allclose(p, -g):\n        p = -g\n        gTp = g.dot(p)\n        alpha = 1.0\n        fx = f(x)\n        for _ in range(max_backtracks):\n            x_new = x + alpha * p\n            if f(x_new) = fx + c1 * alpha * gTp:\n                return alpha, p, True  # success after fallback; mark as had a failure\n            alpha *= rho\n    return 0.0, p, True  # failed\n\ndef bfgs_minimize(E1, E2, a1, a2, c_soft, mu, x0, strategy='damped',\n                  tol=1e-6, max_iter=200, c1=1e-4, rho=0.5,\n                  eta=0.2, eps_curv=1e-12):\n    f, grad = f_factory(E1, E2, a1, a2, c_soft, mu)\n    x = np.array(x0, dtype=float)\n    B = np.eye(2, dtype=float)\n    neg_events = 0\n    iters = 0\n    converged = 0\n    for k in range(max_iter):\n        g = grad(x)\n        ng = np.linalg.norm(g)\n        if ng = tol:\n            converged = 1\n            break\n        # Solve B p = -g\n        try:\n            p = np.linalg.solve(B, -g)\n        except np.linalg.LinAlgError:\n            # If B is ill-conditioned, reset to identity and use steepest descent\n            B = np.eye(2, dtype=float)\n            p = -g\n\n        alpha, p_used, ls_failed = armijo_backtracking(f, grad, x, p, c1=c1, rho=rho)\n        if alpha == 0.0:\n            # last resort: terminate; no progress\n            iters = k + 1\n            converged = 0\n            break\n\n        s = alpha * p_used\n        x_new = x + s\n        g_new = grad(x_new)\n        y = g_new - g\n\n        sy = float(s.dot(y))\n        if sy  0.0:\n            neg_events += 1\n\n        if strategy == 'damped':\n            Bs = B @ s\n            sBs = float(s.dot(Bs))\n            # Ensure sBs > 0; if not, regularize a bit\n            if sBs = 0.0:\n                # Regularize B\n                B += 1e-8 * np.eye(2)\n                Bs = B @ s\n                sBs = float(s.dot(Bs))\n            # Powell damping: ensure s^T y_bar >= eta * s^T B s\n            if sy >= eta * sBs:\n                y_used = y\n            else:\n                # y_bar = y + tau * (B s), tau chosen so that s^T y_bar == eta * s^T B s\n                tau = (eta * sBs - sy) / sBs\n                y_used = y + tau * Bs\n            denom = float(s.dot(y_used))\n            # BFGS update with y_used\n            if denom = 0.0:\n                # As a safeguard, skip update\n                pass\n            else:\n                # Standard BFGS update on B\n                Bs = B @ s\n                sBs = float(s.dot(Bs))\n                # To avoid division by zero\n                if sBs = 1e-20:\n                    pass\n                else:\n                    B = B - np.outer(Bs, Bs) / sBs + np.outer(y_used, y_used) / denom\n        elif strategy == 'skip':\n            # Only update if s^T y > eps_curv\n            if sy > eps_curv:\n                Bs = B @ s\n                sBs = float(s.dot(Bs))\n                if sBs > 1e-20:\n                    B = B - np.outer(Bs, Bs) / sBs + np.outer(y, y) / sy\n                else:\n                    # Skip if near singular\n                    pass\n            else:\n                # Skip the update\n                pass\n        else:\n            raise ValueError(\"Unknown strategy\")\n\n        x = x_new\n        iters = k + 1\n\n    # Final convergence check at loop end\n    if converged == 0:\n        # Check gradient at final x\n        if np.linalg.norm(grad(x)) = tol:\n            converged = 1\n    return converged, iters, neg_events\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case: (E1, E2, a1, a2, c, mu, x0_1, x0_2)\n    test_cases = [\n        (200.0, 100.0, 1.0, 0.5, 0.8, 0.08, 0.6, 0.4),   # Case 1\n        (150.0, 80.0, 1.0, 0.2, 1.5, 0.10, 1.0, 0.1),    # Case 2\n        (120.0, 60.0, 2.5, 0.1, 2.0, 0.12, 0.8, 0.8),    # Case 3\n        (300.0, 50.0, 0.3, 2.0, 1.2, 0.20, 0.2, 1.1),    # Case 4\n    ]\n\n    # Algorithmic constants\n    tol = 1e-6\n    max_iter = 200\n    c1 = 1e-4\n    rho = 0.5\n    eta = 0.2\n    eps_curv = 1e-12\n\n    results = []\n    for case in test_cases:\n        E1, E2, a1, a2, c_soft, mu, x01, x02 = case\n        x0 = np.array([x01, x02], dtype=float)\n\n        convA, itA, negA = bfgs_minimize(E1, E2, a1, a2, c_soft, mu, x0,\n                                         strategy='damped', tol=tol, max_iter=max_iter,\n                                         c1=c1, rho=rho, eta=eta, eps_curv=eps_curv)\n        convB, itB, negB = bfgs_minimize(E1, E2, a1, a2, c_soft, mu, x0,\n                                         strategy='skip', tol=tol, max_iter=max_iter,\n                                         c1=c1, rho=rho, eta=eta, eps_curv=eps_curv)\n        results.append([int(convA), int(convB), int(itA), int(itB), int(negA), int(negB)])\n\n    # Since the above format_obj has a glitch in the concatenation, define a correct one:\n    def format_list_no_spaces(obj):\n        if isinstance(obj, (list, tuple)):\n            return \"[\" + \",\".join(format_list_no_spaces(e) for e in obj) + \"]\"\n        elif isinstance(obj, (np.bool_, bool)):\n            return \"1\" if obj else \"0\"\n        elif isinstance(obj, (np.integer, int)):\n            return str(int(obj))\n        elif isinstance(obj, (np.floating, float)):\n            return repr(float(obj))\n        else:\n            return str(obj)\n\n    print(format_list_no_spaces(results))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3554083"}]}