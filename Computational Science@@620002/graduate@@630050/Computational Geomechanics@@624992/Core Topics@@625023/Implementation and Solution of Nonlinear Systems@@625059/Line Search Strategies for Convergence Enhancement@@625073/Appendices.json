{"hands_on_practices": [{"introduction": "The pure Newton-Raphson method, while powerful, can diverge if the initial guess is not close to the solution. This practice addresses this by guiding you through the implementation of two fundamental globalization strategies: the Armijo and Goldstein backtracking line searches. By coding these methods from scratch for a coupled poromechanics problem [@problem_id:3538526], you will gain direct experience in their mechanics and a practical understanding of how their distinct acceptance criteria influence convergence behavior and computational cost.", "problem": "Consider the nonlinear algebraic system that arises from a minimal, dimensionless reduction of quasi-static Biot poromechanics, retaining the essential pressure–displacement coupling. Let the primary unknowns be the scalar displacement $u$ and scalar pore pressure $p$. The coupled, residual-based equilibrium is prescribed by the vector-valued map $\\mathbf{r}:\\mathbb{R}^2 \\to \\mathbb{R}^2$,\n$$\n\\mathbf{r}(u,p) = \n\\begin{bmatrix}\nr_u(u,p) \\\\\nr_p(u,p)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nk\\,u + b\\,p + \\beta\\,u^3 - f \\\\\nb\\,u + s\\,p + \\gamma\\,p^3 - g\n\\end{bmatrix},\n$$\nwhere $k>0$ is the skeleton stiffness, $s>0$ is the specific storage, $b \\in (0,1]$ is the Biot coefficient (pressure–displacement coupling), and $\\beta \\ge 0$, $\\gamma \\ge 0$ model nonlinear skeleton and fluid responses. All quantities are dimensionless. Assume a consistent, exact Jacobian\n$$\n\\mathbf{J}(u,p) = \n\\begin{bmatrix}\n\\frac{\\partial r_u}{\\partial u} & \\frac{\\partial r_u}{\\partial p} \\\\\n\\frac{\\partial r_p}{\\partial u} & \\frac{\\partial r_p}{\\partial p}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nk + 3\\beta u^2 & b \\\\\nb & s + 3\\gamma p^2\n\\end{bmatrix}.\n$$\n\nDefine the residual-norm-squared merit function $\\phi:\\mathbb{R}^2 \\to \\mathbb{R}$,\n$$\n\\phi(u,p) = \\tfrac{1}{2}\\,\\|\\mathbf{r}(u,p)\\|_2^2.\n$$\n\nYou will design two globalization strategies for a Newton–Raphson method using a backtracking line search along the exact Newton direction $\\mathbf{s}$, where $\\mathbf{J}(u,p)\\,\\mathbf{s}=-\\mathbf{r}(u,p)$. The line search strategies must be:\n\n- Armijo backtracking with parameter $c_a \\in (0,1)$, enforcing sufficient decrease of $\\phi$.\n- Goldstein backtracking with parameter $c_g \\in (0,\\tfrac{1}{2})$, enforcing a two-sided decrease window for $\\phi$.\n\nStart from the fundamental definitions (residual equilibrium, exact Jacobian, and the merit function defined above). From these, derive the directional derivative of $\\phi$ along $\\mathbf{s}$ and formulate the acceptance conditions for each line search in terms of $\\phi$, a trial step length $\\alpha \\in (0,\\infty)$, and the directional derivative at the current iterate. Your implementation must ensure that the Newton direction is a descent direction for $\\phi$ and must include a robust backtracking strategy that always terminates under the stated parameter regimes or returns a failure indicator when the acceptance conditions cannot be met within a fixed budget of trial evaluations.\n\nAlgorithmic requirements:\n\n- Use the exact Newton direction $\\mathbf{s}$ given by $\\mathbf{J}(u,p)\\,\\mathbf{s}=-\\mathbf{r}(u,p)$ at each iteration.\n- Use a backtracking factor $\\tau \\in (0,1)$ when reducing the step length $\\alpha$.\n- For Goldstein, if the step is too small relative to the two-sided decrease window, you must increase $\\alpha$ (e.g., by replacing $\\alpha$ with $\\alpha/\\tau$) subject to a finite cap on trial evaluations; if the step is too large (insufficient decrease), you must reduce $\\alpha$ by multiplying by $\\tau$.\n- Reuse any accepted residual evaluation $\\mathbf{r}(u+\\alpha s_u, p+\\alpha s_p)$ when updating the iterate, to avoid double-counting residual evaluations.\n- Terminate when $\\|\\mathbf{r}(u,p)\\|_2 \\le \\varepsilon$ for a given tolerance $\\varepsilon>0$, or when a maximum number of Newton iterations is reached. If convergence is not achieved, return the failure indicator specified below.\n\nPerformance metrics to compute for each test case and each line search strategy:\n\n- The total number of Newton iterations $N_{\\text{it}}$ taken to converge.\n- The total number of residual evaluations $N_{\\text{res}}$ incurred, counting the initial residual at the initial guess, all trial residual evaluations during line search, and the accepted residual at each iteration (if not already counted among trials).\n\nIf the solver fails to converge within the iteration cap or cannot find an acceptable step within the trial cap, report $N_{\\text{it}}=-1$ and $N_{\\text{res}}=-1$ for that strategy on that test case.\n\nTest suite (all quantities dimensionless). Use the same initial guess $(u_0,p_0)=(0,0)$, tolerance $\\varepsilon=10^{-10}$, maximum Newton iterations $N_{\\max}=50$, backtracking factor $\\tau=0.5$, and maximum line-search trial evaluations per Newton step $M_{\\max}=50$.\n\nProvide four test cases that probe different regimes:\n\n- Case $1$ (moderate coupling and mild nonlinearity; happy path): $(k,s,b,\\beta,\\gamma,f,g,c_a,c_g) = (10,1,0.8,0.2,0.1,1.0,0.2,10^{-4},0.25)$.\n- Case $2$ (strong coupling, near-incompressibility in storage; challenging conditioning): $(k,s,b,\\beta,\\gamma,f,g,c_a,c_g) = (1000,10^{-3},0.95,0.1,0.1,0.1,0.1,10^{-4},0.25)$.\n- Case $3$ (stiff nonlinearities; frequent backtracking expected): $(k,s,b,\\beta,\\gamma,f,g,c_a,c_g) = (5,1,0.6,5.0,1.0,0.5,0.1,10^{-4},0.25)$.\n- Case $4$ (sensitivity to parameter selection; stringent Armijo and narrow Goldstein window): $(k,s,b,\\beta,\\gamma,f,g,c_a,c_g) = (10,0.5,0.9,0.5,0.5,1.0,0.5,0.9,0.49)$.\n\nRequired final output format:\n\n- Your program should produce a single line of output containing a list of lists. For each test case, return a list of four integers $[N_{\\text{it}}^{A},N_{\\text{res}}^{A},N_{\\text{it}}^{G},N_{\\text{res}}^{G}]$, where superscripts $A$ and $G$ denote Armijo and Goldstein, respectively.\n- The complete output is the outer list of these per-case lists, printed as a single line with no spaces, for example, $[[1,2,3,4],[5,6,7,8],\\ldots]$.\n\nAngle units are not applicable. No physical unit conversions are required because all quantities are dimensionless. The program must be entirely self-contained and must not read any input.", "solution": "The problem requires the design and implementation of two line search strategies, Armijo and Goldstein backtracking, to globalize the Newton-Raphson method for solving a specific nonlinear system arising from quasi-static poromechanics. The solution must be derived from first principles.\n\nLet the state vector be $\\mathbf{x} = [u, p]^T \\in \\mathbb{R}^2$. The governing nonlinear algebraic system is given by finding $\\mathbf{x}$ such that the residual vector $\\mathbf{r}(\\mathbf{x}) = \\mathbf{0}$, where:\n$$\n\\mathbf{r}(\\mathbf{x}) = \\mathbf{r}(u,p) = \n\\begin{bmatrix}\nk\\,u + b\\,p + \\beta\\,u^3 - f \\\\\nb\\,u + s\\,p + \\gamma\\,p^3 - g\n\\end{bmatrix}\n$$\nThe parameters $k, s, b, \\beta, \\gamma, f, g$ are given constants. The Newton-Raphson method is an iterative procedure for solving $\\mathbf{r}(\\mathbf{x}) = \\mathbf{0}$. Given an iterate $\\mathbf{x}_k$, the next iterate $\\mathbf{x}_{k+1}$ is found by solving the linearized system:\n$$\n\\mathbf{J}(\\mathbf{x}_k)(\\mathbf{x}_{k+1} - \\mathbf{x}_k) = -\\mathbf{r}(\\mathbf{x}_k)\n$$\nwhere $\\mathbf{J}(\\mathbf{x}_k)$ is the Jacobian matrix of $\\mathbf{r}$ evaluated at $\\mathbf{x}_k$. The problem provides the exact Jacobian:\n$$\n\\mathbf{J}(u,p) = \n\\begin{bmatrix}\nk + 3\\beta u^2 & b \\\\\nb & s + 3\\gamma p^2\n\\end{bmatrix}\n$$\nDefining the search direction (the Newton step) as $\\mathbf{s}_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k$, the core of each Newton iteration is to solve the linear system $\\mathbf{J}(\\mathbf{x}_k)\\mathbf{s}_k = -\\mathbf{r}(\\mathbf{x}_k)$ for $\\mathbf{s}_k$. The full Newton-Raphson update would be $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\mathbf{s}_k$.\n\nTo ensure global convergence (convergence from an initial guess far from the solution), a line search is introduced. The update becomes $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{s}_k$, where $\\alpha_k \\in (0, \\infty)$ is a step length chosen to ensure progress towards the solution. This progress is measured using the merit function $\\phi(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{r}(\\mathbf{x})\\|_2^2$, which has a global minimum of $0$ at the solution.\n\nFirst, we must establish that the Newton direction $\\mathbf{s}_k$ is a descent direction for the merit function $\\phi$ at $\\mathbf{x}_k$. The directional derivative of $\\phi$ at $\\mathbf{x}_k$ along $\\mathbf{s}_k$ is given by $D_{\\mathbf{s}_k}\\phi(\\mathbf{x}_k) = \\nabla\\phi(\\mathbf{x}_k)^T \\mathbf{s}_k$. The gradient of $\\phi(\\mathbf{x}) = \\frac{1}{2}\\mathbf{r}(\\mathbf{x})^T\\mathbf{r}(\\mathbf{x})$ is $\\nabla\\phi(\\mathbf{x}) = \\mathbf{J}(\\mathbf{x})^T\\mathbf{r}(\\mathbf{x})$. Therefore, the directional derivative is:\n$$\nD_{\\mathbf{s}_k}\\phi(\\mathbf{x}_k) = (\\mathbf{J}(\\mathbf{x}_k)^T\\mathbf{r}(\\mathbf{x}_k))^T \\mathbf{s}_k = \\mathbf{r}(\\mathbf{x}_k)^T \\mathbf{J}(\\mathbf{x}_k) \\mathbf{s}_k\n$$\nSubstituting the definition of the Newton direction, $\\mathbf{J}(\\mathbf{x}_k)\\mathbf{s}_k = -\\mathbf{r}(\\mathbf{x}_k)$:\n$$\nD_{\\mathbf{s}_k}\\phi(\\mathbf{x}_k) = \\mathbf{r}(\\mathbf{x}_k)^T (-\\mathbf{r}(\\mathbf{x}_k)) = -\\|\\mathbf{r}(\\mathbf{x}_k)\\|_2^2\n$$\nSince $k>0$, $s>0$, $\\beta \\ge 0$, and $\\gamma \\ge 0$, the diagonal elements of the symmetric Jacobian $\\mathbf{J}$ are positive: $J_{11} = k + 3\\beta u^2 > 0$ and $J_{22} = s + 3\\gamma p^2 > 0$. If the determinant is also positive, $\\det(\\mathbf{J}) = (k + 3\\beta u^2)(s + 3\\gamma p^2) - b^2 > 0$, then $\\mathbf{J}$ is positive definite and thus invertible. Assuming invertibility, if $\\mathbf{r}(\\mathbf{x}_k) \\neq \\mathbf{0}$, then $D_{\\mathbf{s}_k}\\phi(\\mathbf{x}_k) < 0$, confirming that $\\mathbf{s}_k$ is a descent direction.\n\nThe line search algorithms determine a suitable value for $\\alpha_k$. We are to implement two such strategies. Let $\\mathbf{x}_k$ be the current iterate, $\\mathbf{s}_k$ the search direction, $\\phi_k = \\phi(\\mathbf{x}_k)$, and $D_k = -\\|\\mathbf{r}(\\mathbf{x}_k)\\|_2^2$ the directional derivative. A trial step length is denoted by $\\alpha$. The trial point is $\\mathbf{x}_{\\text{trial}} = \\mathbf{x}_k + \\alpha \\mathbf{s}_k$, and the merit function at this point is $\\phi_{\\text{trial}} = \\phi(\\mathbf{x}_{\\text{trial}})$.\n\n### Armijo Backtracking\nThe Armijo condition, or sufficient decrease condition, requires that the actual reduction in $\\phi$ is at least a fraction of the reduction predicted by the linear approximation. For a given parameter $c_a \\in (0, 1)$, we seek an $\\alpha$ such that:\n$$\n\\phi(\\mathbf{x}_k + \\alpha \\mathbf{s}_k) \\le \\phi(\\mathbf{x}_k) + c_a \\alpha D_k\n$$\nSubstituting the definitions of $\\phi$ and $D_k$:\n$$\n\\frac{1}{2}\\|\\mathbf{r}(\\mathbf{x}_k + \\alpha \\mathbf{s}_k)\\|_2^2 \\le \\frac{1}{2}\\|\\mathbf{r}(\\mathbf{x}_k)\\|_2^2 - c_a \\alpha \\|\\mathbf{r}(\\mathbf{x}_k)\\|_2^2\n$$\nThe backtracking algorithm starts with a trial $\\alpha = 1$ (the full Newton step). If the condition is not met, the step length is reduced by a factor $\\tau \\in (0,1)$, i.e., $\\alpha \\leftarrow \\tau \\alpha$, and the condition is re-checked. This process is repeated until an acceptable $\\alpha$ is found or a maximum number of trials is exceeded.\n\n### Goldstein Backtracking\nThe Goldstein conditions define a two-sided window for acceptable step lengths, ensuring both sufficient decrease and preventing steps from being excessively small. For a given parameter $c_g \\in (0, \\frac{1}{2})$, we seek an $\\alpha$ that satisfies both:\n1.  Sufficient Decrease (Upper Bound):\n    $$\n    \\phi(\\mathbf{x}_k + \\alpha \\mathbf{s}_k) \\le \\phi(\\mathbf{x}_k) + c_g \\alpha D_k\n    $$\n    $$\n    \\frac{1}{2}\\|\\mathbf{r}(\\mathbf{x}_k + \\alpha \\mathbf{s}_k)\\|_2^2 \\le \\frac{1}{2}\\|\\mathbf{r}(\\mathbf{x}_k)\\|_2^2 - c_g \\alpha \\|\\mathbf{r}(\\mathbf{x}_k)\\|_2^2\n    $$\n2.  Curvature Condition (Lower Bound):\n    $$\n    \\phi(\\mathbf{x}_k + \\alpha \\mathbf{s}_k) \\ge \\phi(\\mathbf{x}_k) + (1-c_g) \\alpha D_k\n    $$\n    $$\n    \\frac{1}{2}\\|\\mathbf{r}(\\mathbf{x}_k + \\alpha \\mathbf{s}_k)\\|_2^2 \\ge \\frac{1}{2}\\|\\mathbf{r}(\\mathbf{x}_k)\\|_2^2 - (1-c_g) \\alpha \\|\\mathbf{r}(\\mathbf{x}_k)\\|_2^2\n    $$\nThe search strategy starts with $\\alpha=1$. If Condition 1 is violated, the step is too large, and $\\alpha$ is decreased, $\\alpha \\leftarrow \\tau \\alpha$. If Condition 2 is violated, the step is considered too small, and $\\alpha$ is increased, $\\alpha \\leftarrow \\alpha / \\tau$. This continues until an $\\alpha$ satisfying both conditions is found, or a trial limit is reached.\n\n### Algorithm Implementation\nA general Newton-Raphson solver is implemented which takes a line search function as an argument. The solver performs the following steps:\n1.  Initialize $\\mathbf{x}_0 = (u_0, p_0)$, $k=0$. Compute initial residual $\\mathbf{r}_0 = \\mathbf{r}(\\mathbf{x}_0)$ and its norm $\\|\\mathbf{r}_0\\|_2$.\n2.  Increment residual evaluation count $N_{\\text{res}}$.\n3.  Loop for $k = 0, 1, \\dots, N_{\\max}-1$:\n    a. Check for convergence: If $\\|\\mathbf{r}_k\\|_2 \\le \\varepsilon$, terminate successfully.\n    b. Compute Jacobian $\\mathbf{J}_k = \\mathbf{J}(\\mathbf{x}_k)$.\n    c. Solve the linear system $\\mathbf{J}_k \\mathbf{s}_k = -\\mathbf{r}_k$ for the Newton direction $\\mathbf{s}_k$.\n    d. Call the specified line search function (Armijo or Goldstein) with $\\mathbf{x}_k, \\mathbf{s}_k, \\mathbf{r}_k$ to find an acceptable step length $\\alpha_k$ and the corresponding residual $\\mathbf{r}_{k+1} = \\mathbf{r}(\\mathbf{x}_k + \\alpha_k \\mathbf{s}_k)$.\n    e. Increment $N_{\\text{res}}$ by the number of trial evaluations performed by the line search.\n    f. If the line search fails to find a step, terminate with failure.\n    g. Update the solution: $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{s}_k$.\n4.  If the loop finishes without convergence, terminate with failure.\n\nThe performance metrics $N_{\\text{it}}$ (total Newton iterations) and $N_{\\text{res}}$ (total residual evaluations) are accumulated. $N_{\\text{res}}$ includes the initial evaluation plus all trial evaluations within the line search loops across all Newton iterations. The accepted residual evaluation is counted as one of the trials, preventing double counting. Failure is indicated by returning $N_{\\text{it}}=-1$ and $N_{\\text{res}}=-1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef r_func(u, p, params):\n    \"\"\"Computes the residual vector r(u,p).\"\"\"\n    k, s, b, beta, gamma, f, g, _, _ = params\n    r_u = k * u + b * p + beta * u**3 - f\n    r_p = b * u + s * p + gamma * p**3 - g\n    return np.array([r_u, r_p])\n\ndef J_func(u, p, params):\n    \"\"\"Computes the Jacobian matrix J(u,p).\"\"\"\n    k, s, b, beta, gamma, _, _, _, _ = params\n    J11 = k + 3.0 * beta * u**2\n    J12 = b\n    J21 = b\n    J22 = s + 3.0 * gamma * p**2\n    return np.array([[J11, J12], [J21, J22]])\n\ndef armijo_backtracking(u_k, p_k, s_k, r_k, params):\n    \"\"\"\n    Performs Armijo backtracking line search.\n    \"\"\"\n    k, s, b, beta, gamma, f, g, c_a, _ = params\n    tau = 0.5\n    M_max = 50\n\n    alpha = 1.0\n    num_trials = 0\n    \n    r_k_norm_sq = r_k[0]**2 + r_k[1]**2\n    \n    # Pre-calculate term for the Armijo condition\n    # Note: directional derivative D_k = -r_k_norm_sq\n    phi_k = 0.5 * r_k_norm_sq\n    \n    s_u, s_p = s_k\n\n    for i in range(M_max):\n        num_trials += 1\n        u_trial = u_k + alpha * s_u\n        p_trial = p_k + alpha * s_p\n        \n        r_trial = r_func(u_trial, p_trial, params)\n        phi_trial = 0.5 * (r_trial[0]**2 + r_trial[1]**2)\n\n        # Armijo condition: phi(x_k + alpha*s_k) <= phi(x_k) + c_a * alpha * grad(phi)^T * s_k\n        # which is: phi_trial <= phi_k - c_a * alpha * r_k_norm_sq\n        if phi_trial <= phi_k - c_a * alpha * r_k_norm_sq:\n            return alpha, r_trial, num_trials\n        \n        alpha *= tau\n        \n    return None, None, num_trials\n\ndef goldstein_search(u_k, p_k, s_k, r_k, params):\n    \"\"\"\n    Performs Goldstein line search.\n    \"\"\"\n    k, s, b, beta, gamma, f, g, _, c_g = params\n    tau = 0.5\n    M_max = 50\n\n    alpha = 1.0\n    num_trials = 0\n    \n    r_k_norm_sq = r_k[0]**2 + r_k[1]**2\n    \n    # Pre-calculate terms for Goldstein conditions\n    phi_k = 0.5 * r_k_norm_sq\n\n    s_u, s_p = s_k\n    \n    # Store visited alphas to prevent simple oscillations\n    # Not a full-proof solution but helps with simple cases.\n    visited_alphas = set()\n\n    for i in range(M_max):\n        if alpha in visited_alphas:\n            # Oscillating between increase/decrease, fail the search\n            return None, None, num_trials\n        visited_alphas.add(alpha)\n\n        num_trials += 1\n        u_trial = u_k + alpha * s_u\n        p_trial = p_k + alpha * s_p\n        \n        r_trial = r_func(u_trial, p_trial, params)\n        phi_trial = 0.5 * (r_trial[0]**2 + r_trial[1]**2)\n\n        # Condition 1: Sufficient decrease (upper bound)\n        cond1 = phi_trial <= phi_k - c_g * alpha * r_k_norm_sq\n        \n        # Condition 2: Curvature condition (lower bound)\n        cond2 = phi_trial >= phi_k - (1 - c_g) * alpha * r_k_norm_sq\n\n        if cond1 and cond2:\n            return alpha, r_trial, num_trials\n        elif not cond1: # Insufficient decrease, step too large\n            alpha *= tau\n        else: # not cond2, Step too small\n            alpha /= tau\n    \n    return None, None, num_trials\n\ndef newton_solver(u0, p0, line_search_func, params):\n    \"\"\"\n    Globalized Newton-Raphson solver.\n    \"\"\"\n    epsilon = 1e-10\n    N_max = 50\n    \n    u, p = u0, p0\n    N_it = 0\n    N_res = 0\n\n    r = r_func(u, p, params)\n    N_res += 1\n    r_norm = np.linalg.norm(r)\n\n    while r_norm > epsilon:\n        if N_it >= N_max:\n            return -1, -1\n\n        J = J_func(u, p, params)\n        \n        try:\n            s_k = np.linalg.solve(J, -r)\n        except np.linalg.LinAlgError:\n            # Jacobian is singular\n            return -1, -1\n        \n        alpha, r_new, trials = line_search_func(u, p, s_k, r, params)\n        N_res += trials\n        \n        if alpha is None:\n            # Line search failed to find a step\n            return -1, -1\n\n        u += alpha * s_k[0]\n        p += alpha * s_k[1]\n        \n        r = r_new\n        r_norm = np.linalg.norm(r)\n        N_it += 1\n\n    return N_it, N_res\n\ndef solve():\n    test_cases = [\n        # (k, s, b, beta, gamma, f, g, c_a, c_g)\n        (10.0, 1.0, 0.8, 0.2, 0.1, 1.0, 0.2, 1e-4, 0.25),\n        (1000.0, 1e-3, 0.95, 0.1, 0.1, 0.1, 0.1, 1e-4, 0.25),\n        (5.0, 1.0, 0.6, 5.0, 1.0, 0.5, 0.1, 1e-4, 0.25),\n        (10.0, 0.5, 0.9, 0.5, 0.5, 1.0, 0.5, 0.9, 0.49),\n    ]\n\n    u0, p0 = 0.0, 0.0\n    \n    all_results = []\n    \n    for params in test_cases:\n        case_results = []\n        \n        # Armijo\n        N_it_A, N_res_A = newton_solver(u0, p0, armijo_backtracking, params)\n        case_results.extend([N_it_A, N_res_A])\n        \n        # Goldstein\n        N_it_G, N_res_G = newton_solver(u0, p0, goldstein_search, params)\n        case_results.extend([N_it_G, N_res_G])\n        \n        all_results.append(case_results)\n\n    # Format output as a single-line string of a list of lists\n    # Example: [[1,2,3,4],[5,6,7,8]]\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3538526"}, {"introduction": "Building upon basic backtracking, this exercise introduces a more sophisticated, physically-motivated line search strategy crucial for stability analysis in geomechanics. You will implement a \"curvature-aware\" line search that rejects steps leading into regions of negative curvature, which correspond to material instability or structural failure modes. This practice [@problem_id:3538511] demonstrates how to embed physical constraints directly into the numerical solver, creating a more robust algorithm tailored to the challenges of simulating softening materials and slope failure.", "problem": "You are tasked with designing and implementing a curvature-aware backtracking line search to enhance convergence of a Newton method for solving nonlinear equilibrium equations arising in computational geomechanics. The scenario models a reduced-order representation of a slope stability finite element system with two generalized degrees of freedom. The governing discrete equilibrium equation at a state vector $u \\in \\mathbb{R}^2$ is $R(u) = f_{\\mathrm{int}}(u) - f_{\\mathrm{ext}} = 0$, where $f_{\\mathrm{ext}} \\in \\mathbb{R}^2$ is a fixed external load, and $f_{\\mathrm{int}}(u)$ is the internal force vector derived from a potential with stabilizing and destabilizing contributions.\n\nFundamental base:\n- Mechanical equilibrium requires $R(u) = 0$.\n- The Newton method for finding $u$ updates $u^{k+1} = u^k + \\alpha^k p^k$ where the search direction $p^k$ solves $K_t(u^k)p^k = -R(u^k)$ and $K_t(u)$ is the tangent stiffness matrix (the Jacobian of $R(u)$).\n- The merit function is $\\phi(u) = \\tfrac{1}{2}\\lVert R(u)\\rVert_2^2$. Its directional derivative at $u^k$ along the Newton direction $p^k$ satisfies $\\left.\\dfrac{d}{d\\alpha}\\phi(u^k+\\alpha p^k)\\right|_{\\alpha=0} = -\\lVert R(u^k)\\rVert_2^2$, provided $K_t(u^k)p^k = -R(u^k)$.\n- Curvature is diagnosed via the minimum eigenvalue $\\lambda_{\\min}(K_t(u))$: negative $\\lambda_{\\min}$ indicates negative curvature (loss of positive definiteness), which is linked to instability mechanisms in slope stability problems.\n\nModel specification:\n- The internal force is defined as $f_{\\mathrm{int}}(u) = (K_0 + C - \\gamma I)u + \\beta\\,[u_1^3,\\,u_2^3]^\\top$, where $K_0 \\in \\mathbb{R}^{2 \\times 2}$ is a symmetric baseline stiffness, $C \\in \\mathbb{R}^{2 \\times 2}$ is a symmetric coupling matrix, $\\gamma \\in \\mathbb{R}$ controls destabilization (e.g., softening), $\\beta \\in \\mathbb{R}$ controls stabilizing higher-order stiffness, and $I$ is the identity. The tangent stiffness is $K_t(u) = K_0 + C - \\gamma I + \\operatorname{diag}(3\\beta u_1^2,\\,3\\beta u_2^2)$.\n- All quantities are treated as dimensionless for this exercise.\n\nLine search requirements:\n- Given $u^k$ and $p^k$ with $K_t(u^k)p^k = -R(u^k)$, choose $\\alpha^k$ by backtracking from $\\alpha^k = 1$ with a constant contraction factor $\\rho \\in (0,1)$ until both conditions hold simultaneously:\n  1. Curvature-acceptance: $\\lambda_{\\min}(K_t(u^k + \\alpha^k p^k)) \\geq 0$.\n  2. Sufficient decrease (Armijo condition): $\\phi(u^k + \\alpha^k p^k) \\leq \\phi(u^k) - c\\,\\alpha^k\\,\\lVert R(u^k)\\rVert_2^2$, with $c \\in (0,1)$.\n- If backtracking leads to $\\alpha^k < \\alpha_{\\min}$, declare failure for the current test case and stop.\n\nNewton method termination:\n- Stop when $\\lVert R(u^k)\\rVert_2 \\leq \\varepsilon$ or when the iteration count reaches $k_{\\max}$, in which case declare failure.\n\nImplementation details:\n- Use $K_0 = \\begin{bmatrix}60 & -15\\\\ -15 & 40\\end{bmatrix}$.\n- Use $C = \\begin{bmatrix}0 & \\kappa\\\\ \\kappa & 0\\end{bmatrix}$ with $\\kappa = -10$.\n- Use the parameter sets listed below. For each set, initialize $u^0 = [0,\\,0]^\\top$, use $\\rho = 0.5$, $c = 10^{-4}$, $\\alpha_{\\min} = 10^{-6}$, $\\varepsilon = 10^{-10}$, and $k_{\\max} = 50$.\n\nTest suite:\n- Case $A$: $\\gamma = 20$, $\\beta = 3$, $f_{\\mathrm{ext}} = [50,\\,20]^\\top$.\n- Case $B$: $\\gamma = 30$, $\\beta = 4$, $f_{\\mathrm{ext}} = [80,\\,50]^\\top$.\n- Case $C$: $\\gamma = 23.05$, $\\beta = 3$, $f_{\\mathrm{ext}} = [40,\\,15]^\\top$.\n- Case $D$: $\\gamma = 60$, $\\beta = 0.05$, $f_{\\mathrm{ext}} = [70,\\,40]^\\top$.\n\nOutput specification:\n- For each case, report the integer number of Newton iterations required to converge to the tolerance $\\varepsilon$. If the algorithm fails (either by exceeding $k_{\\max}$ without convergence or by encountering $\\alpha^k < \\alpha_{\\min}$ during line search), output the integer $-1$ for that case.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[n_A,n_B,n_C,n_D]$), where each $n$ is the integer result for the corresponding case in the order $A,B,C,D$.\n\nNote: The angle unit is not applicable. There are no physical units in this reduced-order test, and all quantities should be treated as dimensionless for the purposes of this computational task.", "solution": "The user has provided a valid problem statement. The task is to implement a Newton-Raphson algorithm with a custom backtracking line search to solve a system of nonlinear equations representative of a computational geomechanics problem. The algorithm's performance will be evaluated on four distinct parameter sets.\n\nThe core of the problem is to find the state vector $u \\in \\mathbb{R}^2$ that satisfies the equilibrium condition $R(u) = 0$, where $R(u)$ is the residual vector. The residual is the difference between the internal forces, $f_{\\mathrm{int}}(u)$, and the constant external load, $f_{\\mathrm{ext}}$.\n\nThe governing equations are defined as:\n- Internal force vector: $f_{\\mathrm{int}}(u) = (K_0 + C - \\gamma I)u + \\beta\\,[u_1^3,\\,u_2^3]^\\top$.\n- Residual vector: $R(u) = f_{\\mathrm{int}}(u) - f_{\\mathrm{ext}}$.\n- Tangent stiffness matrix (Jacobian of $R(u)$): $K_t(u) = \\frac{\\partial R}{\\partial u} = K_0 + C - \\gamma I + \\operatorname{diag}(3\\beta u_1^2,\\,3\\beta u_2^2)$.\n\nThe Newton-Raphson method iteratively refines an estimate for the solution, $u^k$, using the update rule:\n$$\nu^{k+1} = u^k + \\alpha^k p^k\n$$\nHere, $k$ is the iteration number, $p^k$ is the search direction, and $\\alpha^k$ is the step length determined by a line search procedure.\n\nThe search direction $p^k$ is the classical Newton direction, obtained by solving the linear system that arises from the first-order Taylor expansion of $R(u^{k+1}) = 0$ around $u^k$:\n$$\nK_t(u^k) p^k = -R(u^k)\n$$\n\nThe key feature of this problem is the specialized backtracking line search for determining the step length $\\alpha^k$. Starting with a full step $\\alpha = 1$, the step length is repeatedly contracted by a factor $\\rho \\in (0,1)$ until two conditions are simultaneously met:\n\n1.  **Curvature-Acceptance Condition**: The material response at the trial state, $u^k + \\alpha p^k$, must be stable. This is enforced by requiring the tangent stiffness matrix at the trial state to be positive semi-definite. Mathematically, the minimum eigenvalue, $\\lambda_{\\min}$, must be non-negative:\n    $$\n    \\lambda_{\\min}(K_t(u^k + \\alpha p^k)) \\geq 0\n    $$\n    This condition prevents the algorithm from stepping into regions of material instability (where $K_t$ is not positive definite), a critical consideration in geomechanical modeling.\n\n2.  **Sufficient Decrease (Armijo) Condition**: The step must provide a sufficient reduction in the objective function. The merit function is defined as $\\phi(u) = \\frac{1}{2}\\lVert R(u)\\rVert_2^2$. The Armijo condition ensures that the new point is not just better, but sufficiently better:\n    $$\n    \\phi(u^k + \\alpha p^k) \\leq \\phi(u^k) - c\\,\\alpha\\,\\lVert R(u^k)\\rVert_2^2\n    $$\n    where $c \\in (0,1)$ is a small constant (given as $10^{-4}$). The term $-\\lVert R(u^k)\\rVert_2^2$ is the directional derivative of $\\phi$ at $u^k$ along the Newton direction $p^k$, guaranteeing that $p^k$ is a descent direction for $\\phi$.\n\nThe overall algorithm for each test case is as follows:\n\n1.  Initialize the iteration counter $k=0$ and the solution vector $u^0 = [0, 0]^\\top$. Set the constant parameters: $K_0 = \\begin{bmatrix}60 & -15\\\\ -15 & 40\\end{bmatrix}$, $C = \\begin{bmatrix}0 & -10\\\\ -10 & 0\\end{bmatrix}$, $\\rho=0.5$, $c=10^{-4}$, $\\alpha_{\\min}=10^{-6}$, $\\varepsilon=10^{-10}$, and $k_{\\max}=50$.\n\n2.  Enter the main loop, which continues as long as $k < k_{\\max}$:\n    a. Calculate the residual vector $R(u^k)$ and its squared L2-norm $\\lVert R(u^k)\\rVert_2^2$.\n    b. Check for convergence: If $\\lVert R(u^k)\\rVert_2 \\leq \\varepsilon$, the solution is found. Terminate and return the current iteration count $k$.\n    c. Compute the tangent stiffness matrix $K_t(u^k)$.\n    d. Solve the linear system $K_t(u^k)p^k = -R(u^k)$ for the search direction $p^k$.\n    e. Perform the backtracking line search:\n        i. Initialize $\\alpha = 1$.\n        ii. While $\\alpha \\geq \\alpha_{\\min}$:\n            - Compute the trial state $u_{\\text{trial}} = u^k + \\alpha p^k$.\n            - Evaluate the curvature condition: Calculate $\\lambda_{\\min}(K_t(u_{\\text{trial}}))$.\n            - Evaluate the Armijo condition: Calculate $\\phi(u_{\\text{trial}})$ and check if it satisfies the sufficient decrease inequality.\n            - If both conditions are met, accept the step by breaking the inner loop.\n            - If either condition fails, contract the step length $\\alpha \\leftarrow \\rho \\alpha$.\n    f. Check for line search failure: If the inner loop terminated because $\\alpha < \\alpha_{\\min}$, declare failure for the entire case and return $-1$.\n    g. Update the solution: $u^{k+1} = u^k + \\alpha p^k$.\n    h. Increment the iteration counter: $k \\leftarrow k+1$.\n\n3.  If the main loop completes without convergence (i.e., $k$ reaches $k_{\\max}$), declare failure and return $-1$.\n\nThis procedure is implemented for each of the four specified test cases to determine the number of iterations required for convergence or to detect failure.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the Newton-Raphson solver for all test cases.\n    \"\"\"\n\n    def solve_one_case(gamma, beta, f_ext, K0, C, u0, rho, c_armijo, alpha_min, tol, k_max):\n        \"\"\"\n        Solves a single case using a Newton-Raphson method with a curvature-aware\n        backtracking line search.\n        \"\"\"\n        # --- Pre-calculate constant matrices for efficiency ---\n        I = np.identity(2)\n        # This is the linear part of the internal force and tangent stiffness\n        K_lin_part = K0 + C - gamma * I\n\n        u = u0.copy().astype(float)\n\n        for k in range(k_max):\n            # --- 1. Calculate Residual and Check for Convergence ---\n            u_cubed = np.array([u[0]**3, u[1]**3])\n            f_int = K_lin_part @ u + beta * u_cubed\n            R_k = f_int - f_ext\n\n            norm_Rk = np.linalg.norm(R_k)\n\n            if norm_Rk <= tol:\n                return k  # Success: Converged\n\n            phi_k = 0.5 * norm_Rk**2\n            armijo_check_term = c_armijo * norm_Rk**2\n\n            # --- 2. Compute Tangent Stiffness and Search Direction ---\n            K_t_k = K_lin_part + np.diag([3.0 * beta * u[0]**2, 3.0 * beta * u[1]**2])\n\n            try:\n                p_k = np.linalg.solve(K_t_k, -R_k)\n            except np.linalg.LinAlgError:\n                return -1  # Failure: Singular matrix\n\n            # --- 3. Curvature-Aware Backtracking Line Search ---\n            alpha = 1.0\n            step_accepted = False\n            while alpha >= alpha_min:\n                u_trial = u + alpha * p_k\n\n                # 3a. Curvature-Acceptance Condition\n                K_t_trial = K_lin_part + np.diag([3.0 * beta * u_trial[0]**2, 3.0 * beta * u_trial[1]**2])\n                \n                # eigvalsh is for symmetric matrices and is more efficient\n                min_eig = np.min(np.linalg.eigvalsh(K_t_trial))\n                \n                if min_eig < 0.0:\n                    alpha *= rho\n                    continue  # Reduce alpha and re-check\n\n                # 3b. Sufficient Decrease (Armijo) Condition\n                u_trial_cubed = np.array([u_trial[0]**3, u_trial[1]**3])\n                f_int_trial = K_lin_part @ u_trial + beta * u_trial_cubed\n                R_trial = f_int_trial - f_ext\n                phi_trial = 0.5 * np.linalg.norm(R_trial)**2\n\n                if phi_trial <= phi_k - alpha * armijo_check_term:\n                    step_accepted = True\n                    break  # Both conditions met, exit line search\n\n                alpha *= rho\n\n            # --- 4. Update State or Handle Failure ---\n            if not step_accepted:\n                return -1  # Failure: Line search failed (alpha became too small)\n            \n            u = u + alpha * p_k\n\n        return -1  # Failure: Max iterations reached\n\n    # --- Problem Setup ---\n    # Global constants\n    K0 = np.array([[60.0, -15.0], [-15.0, 40.0]])\n    kappa = -10.0\n    C = np.array([[0.0, kappa], [kappa, 0.0]])\n    \n    # Algorithmic parameters\n    u0 = np.array([0.0, 0.0])\n    rho = 0.5\n    c_armijo = 1e-4\n    alpha_min = 1e-6\n    tol = 1e-10\n    k_max = 50\n\n    # Test suite from the problem statement\n    test_cases = [\n        # Case A\n        {'gamma': 20.0, 'beta': 3.0, 'f_ext': np.array([50.0, 20.0])},\n        # Case B\n        {'gamma': 30.0, 'beta': 4.0, 'f_ext': np.array([80.0, 50.0])},\n        # Case C\n        {'gamma': 23.05, 'beta': 3.0, 'f_ext': np.array([40.0, 15.0])},\n        # Case D\n        {'gamma': 60.0, 'beta': 0.05, 'f_ext': np.array([70.0, 40.0])},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_one_case(\n            gamma=case['gamma'],\n            beta=case['beta'],\n            f_ext=case['f_ext'],\n            K0=K0, C=C, u0=u0, rho=rho, c_armijo=c_armijo,\n            alpha_min=alpha_min, tol=tol, k_max=k_max\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3538511"}, {"introduction": "The effectiveness of a line search depends critically on the merit function used to measure progress. This exercise explores the nuanced, yet vital, choice between the $\\ell_2$ (Euclidean) and $\\ell_\\infty$ (maximum) norms for the residual vector. Through a targeted calculation based on a hypothetical stick-slip scenario [@problem_id:3538494], you will discover how these different norms respond to localized residual spikes, providing a key insight into designing robust solvers for non-smooth problems like contact and friction.", "problem": "A rigid block is pressed onto a rough plane and driven quasi-statically along the $x$-direction. The interface is modeled using Coulomb friction with contact constraints, discretized by the Finite Element Method (FEM). The discrete nonlinear algebraic system is written in residual form as $r(u) = 0$, where $u$ collects the nodal displacement and contact variables. In a Newton-type iteration, a line search is used on the trial update $u_{k+1} = u_k + \\alpha p_k$ to enforce convergence across stick–slip transitions. The line search accepts a trial step if a chosen merit function $\\varphi(r(u))$ exhibits a strict decrease, i.e., if $\\varphi(r(u_{k+1})) < \\varphi(r(u_k))$. Two merit functions are considered: the $\\ell_2$ norm, $\\varphi_2(r) = \\lVert r \\rVert_2$, and the $\\ell_\\infty$ norm, $\\varphi_\\infty(r) = \\lVert r \\rVert_\\infty$.\n\nDuring a stick–slip event, a local active-set change produces a spike in one contact residual component. At iteration $k$, the residual is\n$$\nr(u_k) = \\big[\\,0.10,\\;0.08,\\;0.12,\\;0.09,\\;0.11,\\;0.07,\\;0.09,\\;1.20\\,\\big]^\\mathsf{T},\n$$\nwhere the last component corresponds to a single contact node experiencing a transient spike. A Newton direction $p_k$ is computed that sharply reduces the seven smaller components but slightly increases the last (spiking) component. With a unit trial step $\\alpha = 1$, the evaluated residual at the trial state is\n$$\nr(u_{k+1}) = \\big[\\,0.02,\\;0.02,\\;0.03,\\;0.02,\\;0.02,\\;0.02,\\;0.02,\\;1.22\\,\\big]^\\mathsf{T}.\n$$\n\nAssume a monotone acceptance rule that accepts the trial step if and only if the chosen norm strictly decreases. Which statement best characterizes the acceptance outcome and the robustness of the norm choice to localized residual spikes in this contact–friction setting?\n\nA. The step is accepted under the $\\ell_2$ criterion and rejected under the $\\ell_\\infty$ criterion; the $\\ell_2$ norm is more robust to localized spikes because it aggregates residuals over all components, enabling progress across stick–slip transitions even when one component worsens slightly.\n\nB. The step is rejected under the $\\ell_2$ criterion and accepted under the $\\ell_\\infty$ criterion; the $\\ell_\\infty$ norm is more robust because it emphasizes the worst component, preventing hidden divergence.\n\nC. The step is accepted under both criteria because the total reduction in the sum of residuals dominates the slight increase in the spiking component, so any reasonable norm must strictly decrease.\n\nD. The step is rejected under both criteria because any increase in any residual component violates the monotone line search requirement, regardless of the behavior of the other components.", "solution": "The user wants me to solve this problem. I have to first validate the problem statement.\n\n**Problem Statement Validation**\n\nStep 1: Extract Givens\n\n-   **System**: A rigid block on a rough plane under quasi-static loading in the $x$-direction.\n-   **Model**: Coulomb friction and contact constraints, discretized via FEM.\n-   **Governing Equation**: A discrete nonlinear algebraic system in residual form, $r(u) = 0$.\n-   **Solution Method**: A Newton-type iterative scheme.\n-   **Globalization Strategy**: Line search on the update step $u_{k+1} = u_k + \\alpha p_k$.\n-   **Line Search Acceptance Criterion**: A strict decrease in a merit function $\\varphi(r(u))$, i.e., $\\varphi(r(u_{k+1})) < \\varphi(r(u_k))$.\n-   **Merit Functions**:\n    1.  $\\ell_2$ norm: $\\varphi_2(r) = \\lVert r \\rVert_2 = \\sqrt{\\sum_i r_i^2}$.\n    2.  $\\ell_\\infty$ norm: $\\varphi_\\infty(r) = \\lVert r \\rVert_\\infty = \\max_i |r_i|$.\n-   **Data at Iteration $k$**: The residual vector is $r(u_k) = \\big[\\,0.10,\\;0.08,\\;0.12,\\;0.09,\\;0.11,\\;0.07,\\;0.09,\\;1.20\\,\\big]^\\mathsf{T}$.\n-   **Data at Trial Step**: For a step size $\\alpha = 1$, the residual vector is $r(u_{k+1}) = \\big[\\,0.02,\\;0.02,\\;0.03,\\;0.02,\\;0.02,\\;0.02,\\;0.02,\\;1.22\\,\\big]^\\mathsf{T}$.\n-   **Rule**: The trial step is accepted if and only if the chosen norm strictly decreases.\n\nStep 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem is well-grounded in the field of computational solid mechanics, specifically nonlinear finite element analysis of contact problems. The concepts of Newton's method, line searches, merit functions (using $\\ell_2$ and $\\ell_\\infty$ norms), residual vectors, and stick-slip phenomena are standard and technically accurate.\n-   **Well-Posed**: The problem is well-posed. It provides all necessary numerical data to compute the required norms and apply the acceptance criterion. The question asks for a direct comparison and interpretation, for which a unique answer can be determined.\n-   **Objective**: The problem is stated in objective, formal language, free of subjective or ambiguous terminology.\n\n-   **Flaw Checklist**:\n    1.  **Scientific or Factual Unsoundness**: The premise is sound. The scenario of a residual spike during a stick-slip transition is a known challenge in contact simulations.\n    2.  **Non-Formalizable or Irrelevant**: The problem is directly relevant and formalizable.\n    3.  **Incomplete or Contradictory Setup**: The setup is complete and self-consistent.\n    4.  **Unrealistic or Infeasible**: The numerical values are abstract but serve to illustrate a valid numerical behavior. There are no physical inconsistencies.\n    5.  **Ill-Posed or Poorly Structured**: The problem structure is clear and leads to a unique conclusion.\n    6.  **Pseudo-Profound, Trivial, or Tautological**: The problem is not trivial; it requires calculation and an understanding of the differing properties of vector norms in the context of numerical optimization.\n    7.  **Outside Scientific Verifiability**: The claims are verifiable through direct calculation.\n\nStep 3: Verdict and Action\n\nThe problem statement is valid. I will proceed with the solution.\n\n**Solution Derivation**\n\nThe problem requires an evaluation of the line search acceptance for a trial step using two different merit functions, the $\\ell_2$ norm and the $\\ell_\\infty$ norm. The acceptance rule is a strict decrease in the value of the merit function.\n\nLet $r_k = r(u_k)$ and $r_{k+1} = r(u_{k+1})$ be the residual vectors at iteration $k$ and the trial state $k+1$, respectively.\nThe given vectors are:\n$$\nr_k = \\big[\\,0.10,\\;0.08,\\;0.12,\\;0.09,\\;0.11,\\;0.07,\\;0.09,\\;1.20\\,\\big]^\\mathsf{T}\n$$\n$$\nr_{k+1} = \\big[\\,0.02,\\;0.02,\\;0.03,\\;0.02,\\;0.02,\\;0.02,\\;0.02,\\;1.22\\,\\big]^\\mathsf{T}\n$$\nThe components of the residual vectors are non-negative, so we do not need to take absolute values when computing the $\\ell_\\infty$ norm.\n\n**1. Evaluation with the $\\ell_2$ norm criterion**\n\nThe merit function is $\\varphi_2(r) = \\lVert r \\rVert_2$. The step is accepted if $\\lVert r_{k+1} \\rVert_2 < \\lVert r_k \\rVert_2$. This inequality is equivalent to $\\lVert r_{k+1} \\rVert_2^2 < \\lVert r_k \\rVert_2^2$, which is computationally simpler to check.\n\nLet's compute the squared $\\ell_2$ norm for $r_k$:\n$$\n\\begin{align*}\n\\varphi_2(r_k)^2 = \\lVert r_k \\rVert_2^2 &= 0.10^2 + 0.08^2 + 0.12^2 + 0.09^2 + 0.11^2 + 0.07^2 + 0.09^2 + 1.20^2 \\\\\n&= 0.0100 + 0.0064 + 0.0144 + 0.0081 + 0.0121 + 0.0049 + 0.0081 + 1.4400 \\\\\n&= 0.0640 + 1.4400 \\\\\n&= 1.5040\n\\end{align*}\n$$\n\nNow, let's compute the squared $\\ell_2$ norm for $r_{k+1}$:\n$$\n\\begin{align*}\n\\varphi_2(r_{k+1})^2 = \\lVert r_{k+1} \\rVert_2^2 &= 6 \\times (0.02)^2 + 0.03^2 + 1.22^2 \\\\\n&= 6 \\times (0.0004) + 0.0009 + 1.4884 \\\\\n&= 0.0024 + 0.0009 + 1.4884 \\\\\n&= 1.4917\n\\end{align*}\n$$\n\nComparing the squared norms:\n$$\n1.4917 < 1.5040 \\implies \\lVert r_{k+1} \\rVert_2^2 < \\lVert r_k \\rVert_2^2\n$$\nSince the squared $\\ell_2$ norm has strictly decreased, the line search criterion is met. The step is **accepted** under the $\\ell_2$ criterion.\n\n**2. Evaluation with the $\\ell_\\infty$ norm criterion**\n\nThe merit function is $\\varphi_\\infty(r) = \\lVert r \\rVert_\\infty = \\max_i |r_i|$. The step is accepted if $\\lVert r_{k+1} \\rVert_\\infty < \\lVert r_k \\rVert_\\infty$.\n\nLet's compute the $\\ell_\\infty$ norm for $r_k$:\n$$\n\\varphi_\\infty(r_k) = \\lVert r_k \\rVert_\\infty = \\max\\{0.10,\\;0.08,\\;0.12,\\;0.09,\\;0.11,\\;0.07,\\;0.09,\\;1.20\\} = 1.20\n$$\n\nNow, let's compute the $\\ell_\\infty$ norm for $r_{k+1}$:\n$$\n\\varphi_\\infty(r_{k+1}) = \\lVert r_{k+1} \\rVert_\\infty = \\max\\{0.02,\\;0.02,\\;0.03,\\;0.02,\\;0.02,\\;0.02,\\;0.02,\\;1.22\\} = 1.22\n$$\n\nComparing the norms:\n$$\n1.22 \\not< 1.20\n$$\nSince the $\\ell_\\infty$ norm has not strictly decreased (it has in fact increased), the line search criterion is not met. The step is **rejected** under the $\\ell_\\infty$ criterion.\n\n**Summary and Interpretation**\n\n-   The trial step is accepted by the $\\ell_2$ criterion.\n-   The trial step is rejected by the $\\ell_\\infty$ criterion.\n\nThe physical interpretation relates to the non-smooth nature of the problem. A stick-slip transition is a discontinuity at the material point level, leading to a non-differentiable point in the system's potential energy. The Newton-Raphson method, based on a local linear (or quadratic) model, may struggle. A trial step might improve the solution \"on average\" while worsening a single component that is directly involved in the active-set change.\n\nThe $\\ell_2$ norm, by summing the squares of all residual components, aggregates the overall progress. In this case, the substantial reduction in the seven smaller components outweighed the minor increase in the single large component, resulting in a net decrease of the norm. This behavior is often considered robust for such problems, as it prevents the solver from getting stalled by localized, transient spikes that are a natural part of the physics being modeled.\n\nThe $\\ell_\\infty$ norm focuses exclusively on the worst-performing component. Because the spiking component's residual increased, this norm increased, leading to a rejection of the step. This makes the $\\ell_\\infty$ norm very conservative and sensitive to localized spikes. While this can be useful for ensuring that no part of the solution is diverging unnoticed, in non-smooth problems like contact, it can be overly restrictive and hinder convergence by rejecting steps that are otherwise globally beneficial.\n\n**Option-by-Option Analysis**\n\nA. The step is accepted under the $\\ell_2$ criterion and rejected under the $\\ell_\\infty$ criterion; the $\\ell_2$ norm is more robust to localized spikes because it aggregates residuals over all components, enabling progress across stick–slip transitions even when one component worsens slightly.\n-   The first part, \"accepted under the $\\ell_2$ criterion and rejected under the $\\ell_\\infty$ criterion,\" is consistent with our calculations.\n-   The second part, concerning robustness, correctly identifies that the aggregative nature of the $\\ell_2$ norm allows it to tolerate localized increases in the residual, which is a desirable property for making progress in non-smooth problems with active-set changes.\n-   **Verdict: Correct.**\n\nB. The step is rejected under the $\\ell_2$ criterion and accepted under the $\\ell_\\infty$ criterion; the $\\ell_\\infty$ norm is more robust because it emphasizes the worst component, preventing hidden divergence.\n-   The first part of the statement is factually incorrect based on the calculations. The acceptance/rejection outcomes are reversed.\n-   The robustness argument is weak in this context; for non-smooth problems, the hypersensitivity of the $\\ell_\\infty$ norm is often a liability, not a sign of robustness.\n-   **Verdict: Incorrect.**\n\nC. The step is accepted under both criteria because the total reduction in the sum of residuals dominates the slight increase in the spiking component, so any reasonable norm must strictly decrease.\n-   The premise \"accepted under both criteria\" is false. The step is rejected under the $\\ell_\\infty$ criterion.\n-   The claim that \"any reasonable norm must strictly decrease\" is a fallacious generalization. Different norms measure vector magnitude differently, and the $\\ell_\\infty$ norm is a standard, reasonable choice that did not decrease.\n-   **Verdict: Incorrect.**\n\nD. The step is rejected under both criteria because any increase in any residual component violates the monotone line search requirement, regardless of the behavior of the other components.\n-   The premise \"rejected under both criteria\" is false. The step is accepted under the $\\ell_2$ criterion.\n-   The reasoning provided (\"any increase in any residual component violates the monotone line search requirement\") is a misstatement of the line search rule. The rule applies to the *norm of the vector*, not to each of its individual components.\n-   **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3538494"}]}