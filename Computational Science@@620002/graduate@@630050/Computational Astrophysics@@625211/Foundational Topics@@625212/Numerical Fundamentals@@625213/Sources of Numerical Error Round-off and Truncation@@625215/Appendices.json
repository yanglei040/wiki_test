{"hands_on_practices": [{"introduction": "Choosing the step size for numerical differentiation presents a classic dilemma. A large step size, $\\Delta x$, leads to significant truncation error from approximating the derivative, while a very small step size amplifies the effect of round-off error inherent in floating-point arithmetic. This exercise [@problem_id:3536511] is a foundational practice in numerical analysis that guides you through the process of modeling both error sources to derive the optimal step size, $\\Delta x_{\\mathrm{opt}}$, that minimizes the total error. Mastering this analysis is essential for understanding the trade-offs at the heart of computational science.", "problem": "Consider a spherically symmetric, collisionless dark matter halo in equilibrium. Let $f(x)$ denote a smooth scalar field sampled along a one-dimensional coordinate $x$ through the halo, representing either the mass density, gravitational potential, or a distribution function restricted to a line-of-sight through the halo. Suppose $f(x)$ varies on a characteristic spatial scale $L$ and has a characteristic amplitude $F$, so that derivatives satisfy the dimensional scalings $|f^{(n)}(x)| \\sim F L^{-n}$ at generic locations. You wish to compute the spatial derivative $f'(x)$ at some point $x$ using the symmetric two-point central difference formula\n$$\nD_{\\Delta x} f(x) \\equiv \\frac{f(x+\\Delta x) - f(x-\\Delta x)}{2 \\Delta x}.\n$$\nAssume floating-point arithmetic obeys the standard model with unit roundoff $u$, meaning each evaluation returns $(1+\\delta) f(\\cdot)$ with $|\\delta| \\leq u$, and arithmetic operations incur relative errors of the same order. At the target $x$, define a dimensionless local shape parameter $\\alpha$ by writing the third derivative as $|f^{(3)}(x)| = \\alpha F L^{-3}$, where $\\alpha$ encodes departures from a pure exponential or power-law profile and is typically of order unity in astrophysical applications.\n\nStarting only from Taylorâ€™s theorem for $f(x\\pm\\Delta x)$ and the floating-point model described above, analyze the leading-order truncation error and round-off error in $D_{\\Delta x} f(x)$, form a total error model $E(\\Delta x)$ that adds the magnitudes of these two leading contributions, and determine the step size $\\Delta x_{\\mathrm{opt}}$ that minimizes $E(\\Delta x)$.\n\nExpress your final answer for $\\Delta x_{\\mathrm{opt}}$ as a single closed-form analytic expression in terms of $u$, $L$, and $\\alpha$. No numerical values are required, and no physical units should be included in the final expression.", "solution": "The objective is to determine the optimal step size, $\\Delta x_{\\mathrm{opt}}$, for a symmetric two-point central difference formula that minimizes the total numerical error. The total error is composed of a truncation error, which arises from the approximation of the derivative by a finite difference, and a round-off error, which is inherent to floating-point arithmetic. We will analyze each error component separately, combine them into a total error model, and then minimize this model with respect to the step size $\\Delta x$.\n\nFirst, we analyze the truncation error, $E_{\\text{trunc}}$. The central difference formula is given by:\n$$\nD_{\\Delta x} f(x) = \\frac{f(x+\\Delta x) - f(x-\\Delta x)}{2 \\Delta x}\n$$\nWe use Taylor's theorem to expand $f(x+\\Delta x)$ and $f(x-\\Delta x)$ around the point $x$. Assuming $f(x)$ is sufficiently smooth (at least $C^3$), we can write:\n$$\nf(x+\\Delta x) = f(x) + \\Delta x f'(x) + \\frac{(\\Delta x)^2}{2} f''(x) + \\frac{(\\Delta x)^3}{6} f^{(3)}(x) + O((\\Delta x)^4)\n$$\n$$\nf(x-\\Delta x) = f(x) - \\Delta x f'(x) + \\frac{(\\Delta x)^2}{2} f''(x) - \\frac{(\\Delta x)^3}{6} f^{(3)}(x) + O((\\Delta x)^4)\n$$\nSubtracting the second expansion from the first yields:\n$$\nf(x+\\Delta x) - f(x-\\Delta x) = 2 \\Delta x f'(x) + \\frac{2 (\\Delta x)^3}{6} f^{(3)}(x) + O((\\Delta x)^5)\n$$\nNote that the terms with even powers of $\\Delta x$ cancel. Substituting this result into the central difference formula:\n$$\nD_{\\Delta x} f(x) = \\frac{2 \\Delta x f'(x) + \\frac{(\\Delta x)^3}{3} f^{(3)}(x) + O((\\Delta x)^5)}{2 \\Delta x} = f'(x) + \\frac{(\\Delta x)^2}{6} f^{(3)}(x) + O((\\Delta x)^4)\n$$\nThe truncation error is the difference between the computed approximation and the true derivative, $E_{\\text{trunc}}(\\Delta x) = D_{\\Delta x} f(x) - f'(x)$. The leading-order term of this error is:\n$$\nE_{\\text{trunc}}(\\Delta x) \\approx \\frac{(\\Delta x)^2}{6} f^{(3)}(x)\n$$\nThe magnitude of the truncation error is therefore approximately $|E_{\\text{trunc}}(\\Delta x)| \\approx \\frac{(\\Delta x)^2}{6} |f^{(3)}(x)|$. Using the problem's definition for the third derivative at the point $x$, $|f^{(3)}(x)| = \\alpha F L^{-3}$, we have:\n$$\n|E_{\\text{trunc}}(\\Delta x)| \\approx \\frac{\\alpha F}{6 L^3} (\\Delta x)^2\n$$\nThis error decreases quadratically as $\\Delta x$ decreases.\n\nNext, we analyze the round-off error, $E_{\\text{round}}$. According to the floating-point model, the evaluation of $f(y)$ yields a computed value $\\tilde{f}(y) = f(y)(1+\\delta)$, where $|\\delta| \\leq u$ is the unit roundoff. Let $\\tilde{f}(x+\\Delta x)$ and $\\tilde{f}(x-\\Delta x)$ be the computed values of the function at the sample points.\n$$\n\\tilde{f}(x+\\Delta x) = f(x+\\Delta x)(1 + \\delta_{1})\n$$\n$$\n\\tilde{f}(x-\\Delta x) = f(x-\\Delta x)(1 + \\delta_{2})\n$$\nwhere $|\\delta_1| \\leq u$ and $|\\delta_2| \\leq u$. The round-off error arises from the inaccuracies in these function evaluations and subsequent arithmetic operations. The dominant source of round-off error in this formula is the catastrophic cancellation that can occur during the subtraction $\\tilde{f}(x+\\Delta x) - \\tilde{f}(x-\\Delta x)$ when $\\Delta x$ is small, as the two values are nearly equal. The absolute error in the computed numerator is:\n$$\ne_{\\text{num}} = \\tilde{f}(x+\\Delta x) - \\tilde{f}(x-\\Delta x) - (f(x+\\Delta x) - f(x-\\Delta x)) = f(x+\\Delta x)\\delta_{1} - f(x-\\Delta x)\\delta_{2}\n$$\nWe neglect the round-off error from the subtraction operation itself, as it is typically smaller than the propagated error. The magnitude of $e_{\\text{num}}$ is bounded by:\n$$\n|e_{\\text{num}}| \\leq |f(x+\\Delta x)||\\delta_1| + |f(x-\\Delta x)||\\delta_2| \\leq u (|f(x+\\Delta x)| + |f(x-\\Delta x)|)\n$$\nThe problem states that $f(x)$ has a characteristic amplitude $F$. For a small step size $\\Delta x \\ll L$, the function values $f(x\\pm\\Delta x)$ will be on the order of this amplitude. Thus, we can approximate $|f(x\\pm\\Delta x)| \\approx F$. The bound on the numerator's round-off error becomes $|e_{\\text{num}}| \\lesssim u(F+F) = 2uF$. This error is then propagated through the division by $2\\Delta x$. The magnitude of the round-off error in the final result is therefore:\n$$\n|E_{\\text{round}}(\\Delta x)| \\approx \\frac{|e_{\\text{num}}|}{2\\Delta x} \\approx \\frac{2uF}{2\\Delta x} = \\frac{uF}{\\Delta x}\n$$\nThis error increases as $\\Delta x$ decreases.\n\nThe total error, $E(\\Delta x)$, is modeled as the sum of the magnitudes of the leading-order truncation and round-off errors:\n$$\nE(\\Delta x) = |E_{\\text{trunc}}(\\Delta x)| + |E_{\\text{round}}(\\Delta x)| \\approx \\frac{\\alpha F}{6 L^3} (\\Delta x)^2 + \\frac{uF}{\\Delta x}\n$$\nTo find the optimal step size $\\Delta x_{\\mathrm{opt}}$ that minimizes this total error, we differentiate $E(\\Delta x)$ with respect to $\\Delta x$ and set the result to zero:\n$$\n\\frac{dE}{d(\\Delta x)} = \\frac{d}{d(\\Delta x)} \\left( \\frac{\\alpha F}{6 L^3} (\\Delta x)^2 + \\frac{uF}{\\Delta x} \\right) = 2 \\left( \\frac{\\alpha F}{6 L^3} \\right) \\Delta x - \\frac{uF}{(\\Delta x)^2} = 0\n$$\n$$\n\\frac{\\alpha F}{3 L^3} \\Delta x = \\frac{uF}{(\\Delta x)^2}\n$$\nThe characteristic amplitude $F$ cancels from both sides of the equation:\n$$\n\\frac{\\alpha}{3 L^3} \\Delta x = \\frac{u}{(\\Delta x)^2}\n$$\nSolving for $\\Delta x$:\n$$\n(\\Delta x)^3 = \\frac{3 u L^3}{\\alpha}\n$$\nTaking the cube root of both sides gives the optimal step size:\n$$\n\\Delta x_{\\mathrm{opt}} = \\left( \\frac{3 u L^3}{\\alpha} \\right)^{1/3} = L \\left( \\frac{3u}{\\alpha} \\right)^{1/3}\n$$\nThis result gives the step size that balances the decreasing truncation error and the increasing round-off error to yield the most accurate numerical derivative under the given model.", "answer": "$$\n\\boxed{L \\left( \\frac{3u}{\\alpha} \\right)^{1/3}}\n$$", "id": "3536511"}, {"introduction": "While round-off error is always present, some calculations are far more vulnerable than others. This exercise demonstrates how to construct a function whose numerical derivative is pathologically sensitive to round-off error, a situation caused by the \"catastrophic cancellation\" that occurs when subtracting two nearly-equal floating-point numbers. By systematically designing a function where this effect dominates the truncation error [@problem_id:3273443], you will develop a crucial instinct for identifying and understanding one of the most common and potent sources of numerical instability. This moves beyond the general scaling of errors to their underlying mechanisms.", "problem": "In the standard model of floating-point arithmetic, each basic operation is represented as $\\operatorname{fl}(z)=z(1+\\delta)$ with $|\\delta|\\leq \\varepsilon$, where $\\varepsilon$ is the unit roundoff (machine epsilon). Consider the centered-difference approximation for the derivative of a function $f(x)$ at a point $x$,\n$$\nD_{h}[f](x)=\\frac{f(x+h)-f(x-h)}{2h},\n$$\nimplemented in Institute of Electrical and Electronics Engineers (IEEE) 754 double precision with unit roundoff $\\varepsilon=2^{-53}$. You will compare truncation and round-off errors from first principles to design a function whose computed derivative is dominated by round-off over a typical step-size range.\n\nLet $x=0$ and consider the one-parameter family $f_{A}(x)=A+\\sin(x)$, where $A>0$ is a constant to be determined. Using only the Taylor series expansion of $f$ about $x$ and the floating-point model above (no pre-packaged error formulas), compare the leading-order truncation error of $D_{h}[f_{A}](0)$ with the leading-order effect of round-off in the computed quantity when $h$ ranges over $[10^{-8},\\,10^{-2}]$.\n\nDetermine the smallest $A>0$ such that, for every $h$ in the interval $[10^{-8},\\,10^{-2}]$, the magnitude of the round-off contribution to $D_{h}[f_{A}](0)$ is at least as large as the truncation contribution. Then, provide the resulting function as a single expression in $x$. Round the constant $A$ to three significant figures and express the final function as a single expression without an equality sign.", "solution": "The problem requires comparing the truncation error and the round-off error for the centered-difference approximation of the derivative of $f_A(x) = A + \\sin(x)$ at $x=0$.\n\n**1. Truncation Error Analysis**\n\nThe exact derivative of $f_A(x)$ is $f'_A(x) = \\cos(x)$. At $x=0$, the exact derivative is $f'_A(0) = \\cos(0) = 1$.\n\nThe centered-difference formula applied to $f_A(x)$ at $x=0$ gives:\n$$\nD_h[f_A](0) = \\frac{f_A(0+h) - f_A(0-h)}{2h} = \\frac{f_A(h) - f_A(-h)}{2h}\n$$\nSubstituting the definition of $f_A(x)$:\n$$\nD_h[f_A](0) = \\frac{(A + \\sin(h)) - (A + \\sin(-h))}{2h} = \\frac{\\sin(h) - (-\\sin(h))}{2h} = \\frac{2\\sin(h)}{2h} = \\frac{\\sin(h)}{h}\n$$\nThe truncation error, $E_T$, is the difference between this approximation and the exact value of the derivative:\n$$\nE_T = D_h[f_A](0) - f'_A(0) = \\frac{\\sin(h)}{h} - 1\n$$\nTo find the leading-order term, we use the Taylor series for $\\sin(h)$ around $h=0$:\n$$\n\\sin(h) = h - \\frac{h^3}{3!} + \\frac{h^5}{5!} - \\dots = h - \\frac{h^3}{6} + O(h^5)\n$$\nSubstituting this into the expression for $E_T$:\n$$\nE_T = \\frac{h - \\frac{h^3}{6} + O(h^5)}{h} - 1 = \\left(1 - \\frac{h^2}{6} + O(h^4)\\right) - 1 = -\\frac{h^2}{6} + O(h^4)\n$$\nThe magnitude of the leading-order truncation error is:\n$$\n|E_T| \\approx \\frac{h^2}{6}\n$$\n\n**2. Round-off Error Analysis**\n\nThe round-off error arises from the finite precision of floating-point arithmetic. The computation of $D_h[f_A](0)$ involves evaluating $f_A(h)$ and $f_A(-h)$, subtracting them, and dividing by $2h$.\n\nLet $\\widehat{y}_+ = \\operatorname{fl}(f_A(h))$ and $\\widehat{y}_- = \\operatorname{fl}(f_A(-h))$ be the computed values. For small $h$, $f_A(h) = A + \\sin(h) \\approx A$ and $f_A(-h) = A - \\sin(h) \\approx A$. The error in evaluating a function is bounded by the product of machine epsilon and the value of the function. Thus, we can model the computed values as:\n$$\n\\widehat{y}_+ = f_A(h) + e_+ = (A + \\sin(h)) + e_+ \\quad \\text{where } |e_+| \\lesssim \\varepsilon|f_A(h)| \\approx A\\varepsilon\n$$\n$$\n\\widehat{y}_- = f_A(-h) + e_- = (A - \\sin(h)) + e_- \\quad \\text{where } |e_-| \\lesssim \\varepsilon|f_A(-h)| \\approx A\\varepsilon\n$$\nThe primary source of round-off error in this calculation is the subtraction of two nearly equal numbers, $\\widehat{y}_+ \\approx A$ and $\\widehat{y}_- \\approx A$. This phenomenon is known as catastrophic cancellation.\n\nThe numerator of the difference quotient is computed as $\\operatorname{fl}(\\widehat{y}_+ - \\widehat{y}_-)$. The error in the numerator is dominated by the initial errors $e_+$ and $e_-$. The error in the computed numerator is approximately $e_+ - e_-$. The worst-case magnitude of this error is $|e_+| + |e_-| \\approx 2A\\varepsilon$.\n\nThe round-off error in the final result, $E_R$, is the error from the numerator propagated through the division by $2h$.\n$$\n|E_R| \\approx \\frac{|e_+ - e_-|}{2h}\n$$\nThe magnitude of the leading-order round-off contribution is bounded by its worst-case value:\n$$\n|E_R| \\approx \\frac{2A\\varepsilon}{2h} = \\frac{A\\varepsilon}{h}\n$$\nThis term dominates for small $h$, as the $1/h$ dependence causes it to grow much faster than the truncation error shrinks.\n\n**3. Determining the Constant A**\n\nThe problem requires that the round-off error be at least as large as the truncation error for all $h \\in [10^{-8}, 10^{-2}]$:\n$$\n|E_R| \\ge |E_T|\n$$\nSubstituting the expressions for the error magnitudes:\n$$\n\\frac{A\\varepsilon}{h} \\ge \\frac{h^2}{6}\n$$\nWe need to find the smallest $A$ that satisfies this inequality for the entire interval. We solve for $A$:\n$$\nA \\ge \\frac{h^3}{6\\varepsilon}\n$$\nLet $g(h) = \\frac{h^3}{6\\varepsilon}$. To ensure the inequality holds for all $h$ in the given range, $A$ must be greater than or equal to the maximum value of $g(h)$ on the interval $[10^{-8}, 10^{-2}]$.\nThe function $g(h)$ is a monotonically increasing function of $h$ for $h>0$. Therefore, its maximum value on the interval occurs at the right endpoint, $h_{max} = 10^{-2}$.\n\nThe smallest possible value for $A$ is this maximum value:\n$$\nA_{min} = \\frac{(h_{max})^3}{6\\varepsilon} = \\frac{(10^{-2})^3}{6(2^{-53})} = \\frac{10^{-6}}{6 \\cdot 2^{-53}} = \\frac{2^{53} \\times 10^{-6}}{6}\n$$\n\n**4. Numerical Calculation**\n\nWe now compute the value of $A_{min}$:\n$$\nA_{min} = \\frac{9007199254740992 \\times 10^{-6}}{6} \\approx \\frac{9.0072 \\times 10^{15} \\times 10^{-6}}{6} = \\frac{9.0072 \\times 10^9}{6}\n$$\n$$\nA_{min} \\approx 1.5012 \\times 10^9\n$$\nRounding this value to three significant figures, as requested, we get:\n$$\nA = 1.50 \\times 10^9\n$$\n\n**5. Final Function**\n\nThe resulting function is $f_A(x) = A + \\sin(x)$ with the determined value of $A$. The problem asks for the function to be expressed as a single expression.\n$$\n1.50 \\times 10^9 + \\sin(x)\n$$", "answer": "$$\\boxed{1.50 \\times 10^{9} + \\sin(x)}$$", "id": "3273443"}, {"introduction": "Adaptive Mesh Refinement (AMR) is a powerful technique in computational astrophysics that saves resources by refining the grid only where needed, based on a local error indicator. This hands-on coding practice [@problem_id:3536557] explores the practical limits of this strategy. You will discover a critical tolerance threshold, $\\tau^\\star$, below which the error indicator becomes polluted by round-off noise, leading to spurious \"false refinements.\" This exercise powerfully illustrates how the finite precision of computer arithmetic sets a fundamental limit on the achievable accuracy of even the most sophisticated algorithms.", "problem": "Consider a one-dimensional, dimensionless scalar field $f(x)$ on the interval $x \\in [0,1]$ representing a smoothly varying astrophysical quantity (for example, the density or specific internal energy in a stellar envelope model). Adaptive Mesh Refinement (AMR) is used to resolve spatial scales efficiently. Define Adaptive Mesh Refinement (AMR) decisions based on a local truncation error indicator constructed from finite-difference approximations. Your goal is to construct an AMR refinement criterion derived from truncation error indicators and then analyze thresholds below which round-off noise triggers false refinement.\n\nFundamental base and assumptions:\n- Use the definition of velocity as a spatial derivative in one dimension, namely $g(x) = \\frac{df}{dx}$, and approximate $g(x)$ at a point $x$ with a second-order centered finite difference on a uniform stencil of size $h$: $g_h(x) = \\frac{f(x+h) - f(x-h)}{2h}$.\n- Invoke Taylor expansions about $x$ for $f(x+h)$ and $f(x-h)$ and the well-tested fact that the truncation error of second-order centered differences scales like $O(h^2)$ for sufficiently smooth $f(x)$.\n- Employ Richardson-style self-consistency: form two approximations at $h$ and $h/2$ and use their discrepancy to indicate the local truncation error. You may assume floating-point arithmetic with machine round-off $\\epsilon_{\\mathrm{mach}}$ in double precision, which is approximately $2.220446049250313 \\times 10^{-16}$, although you must not rely on this exact value analytically and should treat it as an unknown parameter controlled by the floating-point implementation.\n- The angle unit for any trigonometric function is radians.\n\nRefinement criterion construction:\n- For a cell centered at $x$ with local grid spacing $h$, compute two approximations $g_h(x)$ and $g_{h/2}(x)$ using the centered difference formula above.\n- Use the difference between $g_{h/2}(x)$ and $g_h(x)$ as a truncation error indicator. Refine the cell if the indicator exceeds a user-specified tolerance $\\tau$ (unitless).\n- Iterate refinement by splitting refined cells into two equal children (each with spacing $h/2$) until either the indicator is less than or equal to $\\tau$ or a maximum level $L_{\\max}$ is reached.\n\nFalse refinement analysis:\n- Define a refinement decision as false if it is triggered when the theoretical truncation-only contribution at that level would not have exceeded the tolerance $\\tau$. Use the $O(h^2)$ truncation law for second-order centered differences with the leading term based on the third derivative: for smooth $f(x)$, the truncation-only magnitude at $x$ and spacing $h$ behaves like $\\left|\\frac{f^{(3)}(x)}{6}\\right| h^2$.\n- Assume $f(x)$ is smooth enough that $f^{(3)}(x)$ exists and is continuous on $[0,1]$.\n\nYour program must:\n- Implement the AMR decision process above on a base uniform grid with $N_0$ cells covering $[0,1]$ and recursively refine as required.\n- For a given tolerance $\\tau$, run the refinement process and count:\n  - the total number of refinement decisions, and\n  - the number of false decisions as defined by the theoretical truncation-only term $\\left|\\frac{f^{(3)}(x)}{6}\\right| h^2$ being less than or equal to $\\tau$ at the parent cell.\n- For each test case, determine the smallest tolerance $\\tau^\\star$ such that the fraction of false decisions (false decisions divided by total decisions) is less than or equal to a specified allowable false fraction $\\delta$ (express $\\delta$ as a decimal number, not a percentage).\n\nTest suite:\n- In all cases, the domain is $x \\in [0,1]$ and trigonometric functions use radians.\n- Case A (smooth, moderate amplitude): $f(x) = A \\sin(k x)$ with $A = 1.0$, $k = 2\\pi$, $N_0 = 64$, $L_{\\max} = 8$, and $\\delta = 0.05$.\n- Case B (round-off prone, tiny amplitude): $f(x) = A \\sin(k x)$ with $A = 10^{-12}$, $k = 2\\pi$, $N_0 = 64$, $L_{\\max} = 8$, and $\\delta = 0.05$.\n- Case C (high-frequency content): $f(x) = A \\sin(k x)$ with $A = 1.0$, $k = 40\\pi$, $N_0 = 128$, $L_{\\max} = 8$, and $\\delta = 0.05$.\n\nOutput specification:\n- Your program must search tolerances $\\tau$ over a reasonable logarithmic range and identify $\\tau^\\star$ per case using a robust search (for example, a binary search over the logarithmic scale).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases, namely $[\\tau^\\star_{\\text{A}}, \\tau^\\star_{\\text{B}}, \\tau^\\star_{\\text{C}}]$. Values are unitless and should be printed as decimal numbers (scientific notation is acceptable).", "solution": "The user has provided a valid problem statement. The problem asks for the construction and analysis of an Adaptive Mesh Refinement (AMR) criterion based on numerical error indicators, specifically focusing on the threshold where round-off error begins to dominate and cause spurious \"false\" refinements.\n\nThe solution will be developed in three main parts:\n1.  Derivation of the numerical error indicator and the theoretical truncation error.\n2.  Analysis of the interplay between truncation and round-off error, defining the mechanism for false refinement.\n3.  Description of the algorithm to simulate the AMR process and search for the critical tolerance threshold $\\tau^\\star$.\n\n### 1. Error Indicator and Truncation Error\n\nWe are tasked with approximating the derivative $g(x) = \\frac{df}{dx}$ of a scalar field $f(x)$. The problem specifies using a second-order centered finite difference approximation on a grid with spacing $h$:\n$$\ng_h(x) = \\frac{f(x+h) - f(x-h)}{2h}\n$$\nTo understand the error of this approximation, we perform Taylor series expansions of $f(x+h)$ and $f(x-h)$ around the point $x$, assuming $f(x)$ is sufficiently smooth for its third derivative, $f^{(3)}(x)$, to exist and be continuous.\n$$\nf(x+h) = f(x) + hf'(x) + \\frac{h^2}{2}f''(x) + \\frac{h^3}{6}f^{(3)}(x) + \\frac{h^4}{24}f^{(4)}(x) + O(h^5)\n$$\n$$\nf(x-h) = f(x) - hf'(x) + \\frac{h^2}{2}f''(x) - \\frac{h^3}{6}f^{(3)}(x) + \\frac{h^4}{24}f^{(4)}(x) - O(h^5)\n$$\nSubtracting the second expansion from the first yields:\n$$\nf(x+h) - f(x-h) = 2hf'(x) + \\frac{h^3}{3}f^{(3)}(x) + O(h^5)\n$$\nDividing by $2h$ and rearranging gives the expression for our numerical approximation $g_h(x)$ in terms of the true derivative $f'(x)$:\n$$\ng_h(x) = \\frac{f(x+h) - f(x-h)}{2h} = f'(x) + \\frac{h^2}{6}f^{(3)}(x) + O(h^4)\n$$\nThe truncation error, $E_T(h)$, is the difference between the approximation and the true value, $g(x) = f'(x)$:\n$$\nE_T(h) = g_h(x) - g(x) = \\frac{h^2}{6}f^{(3)}(x) + O(h^4)\n$$\nThe leading-order truncation error is thus $E_T(h) \\approx C(x)h^2$, where $C(x) = \\frac{f^{(3)}(x)}{6}$. This confirms the $O(h^2)$ scaling.\n\nThe AMR refinement criterion is based on a Richardson-style error indicator, which compares approximations at two different grid spacings, $h$ and $h/2$. Let's write the expressions for both:\n$$\ng_h(x) = g(x) + C(x)h^2 + O(h^4)\n$$\n$$\ng_{h/2}(x) = g(x) + C(x)\\left(\\frac{h}{2}\\right)^2 + O(h^4) = g(x) + \\frac{1}{4}C(x)h^2 + O(h^4)\n$$\nThe error indicator, $\\eta(x, h)$, is defined as the difference between these two approximations:\n$$\n\\eta(x, h) = g_{h/2}(x) - g_h(x) = \\left(g(x) + \\frac{1}{4}C(x)h^2\\right) - \\left(g(x) + C(x)h^2\\right) + O(h^4) = -\\frac{3}{4}C(x)h^2 + O(h^4)\n$$\nThe magnitude of this indicator, $|\\eta(x, h)| = |g_{h/2}(x) - g_h(x)|$, is proportional to the leading-order truncation error of the parent cell's approximation, $|E_T(h)| \\approx |C(x)|h^2$. Therefore, $|\\eta(x, h)|$ serves as a computable proxy for the local truncation error, justifying its use as a refinement criterion.\n\n### 2. False Refinement from Round-off Error\n\nThe above analysis only considers truncation error. In a real computation using floating-point arithmetic, round-off error also contributes. Let $\\tilde{g}_h(x)$ be the computed approximation. Its total error is a sum of truncation and round-off errors: $E_{total}(h) = E_T(h) + E_R(h)$.\n\nThe primary source of round-off error in the centered difference formula is the propagation of representation errors in the evaluation of $f(x)$. Let $\\tilde{f}(y) = f(y)(1+\\epsilon_y)$ where $|\\epsilon_y| \\le \\epsilon_{\\mathrm{mach}}$ is the machine epsilon. The computed difference is:\n$$\n\\tilde{g}_h(x) = \\frac{\\tilde{f}(x+h) - \\tilde{f}(x-h)}{2h} = \\frac{f(x+h)(1+\\epsilon_1) - f(x-h)(1+\\epsilon_2)}{2h}\n$$\nThe round-off error component is $E_R(h) \\approx \\frac{f(x+h)\\epsilon_1 - f(x-h)\\epsilon_2}{2h}$. Assuming $f$ is smooth, its magnitude is bounded by:\n$$\n|E_R(h)| \\lesssim \\frac{(|f(x+h)| + |f(x-h)|)\\epsilon_{\\mathrm{mach}}}{2h} \\approx \\frac{|f(x)|\\epsilon_{\\mathrm{mach}}}{h}\n$$\nAs $h \\to 0$, the truncation error $|E_T(h)| \\propto h^2$ decreases, while the round-off error $|E_R(h)| \\propto h^{-1}$ increases. The total error $|E_{total}(h)|$ has a characteristic minimum at an optimal $h$.\n\nA \"false refinement\" is defined to occur when a refinement is triggered, $|\\tilde{\\eta}(x,h)| > \\tau$, but the theoretical truncation error is already within tolerance, $|C(x)|h^2 \\le \\tau$. Here, $\\tilde{\\eta}$ is the numerically computed indicator. This situation arises when $\\tau$ is set to a value so low that it falls below the noise floor of the indicator. The computed indicator $\\tilde{\\eta}$ becomes dominated by round-off noise, i.e., $|\\tilde{\\eta}(x,h)| \\approx |E_R(h/2) - E_R(h)|$, which no longer reflects the true truncation error. If this noise level exceeds $\\tau$, refinement is triggered even if the grid is sufficiently resolved from a truncation error perspective. This leads to wasted computational effort.\n\nThe goal is to find the smallest tolerance $\\tau^\\star$ such that the fraction of these false refinements does not exceed a given threshold $\\delta$. This $\\tau^\\star$ represents the practical limit on the accuracy that can be demanded from the AMR scheme before round-off pollution becomes unacceptable.\n\n### 3. Algorithmic Implementation\n\nThe problem is solved by implementing a two-part algorithm: an AMR simulation function and a search routine to find $\\tau^\\star$.\n\n**AMR Simulation:**\nA function, `run_amr_simulation`, takes the physical parameters ($A, k$), grid parameters ($N_0, L_{\\max}$), and a tolerance $\\tau$ as input.\n1.  **Initialization**: A list of active cells is created, representing the base grid of $N_0$ cells covering the domain $[0,1]$. Each cell is defined by its center $x_i = (i+0.5)h_0$, level $l=0$, and spacing $h_0 = 1/N_0$.\n2.  **Iteration**: The algorithm iterates through the list of active cells. For each cell $(x, l, h)$:\n    a. If the cell's level $l$ has reached the maximum $L_{\\max}$, it cannot be refined further.\n    b. The numerical indicator $|\\tilde{\\eta}| = |\\tilde{g}_{h/2}(x) - \\tilde{g}_h(x)|$ is computed.\n    c. If $|\\tilde{\\eta}| > \\tau$, a refinement decision is made.\n        i. The total refinement count is incremented.\n        ii. The theoretical truncation error magnitude, $|E_{T,theory}| = |\\frac{f^{(3)}(x)}{6}|h^2$, is computed using the analytical third derivative of $f(x)$. For $f(x) = A\\sin(kx)$, $f^{(3)}(x) = -Ak^3\\cos(kx)$.\n        iii. If $|E_{T,theory}| \\le \\tau$, the decision is classified as false, and the false refinement count is incremented.\n        iv. The parent cell is replaced by two child cells at level $l+1$ with spacing $h/2$, centered at $x \\pm h/4$. These children are added to the list of cells to be processed.\n3.  **Output**: The function returns the total number of refinements and the number of false refinements.\n\n**Search for $\\tau^\\star$:**\nFor each test case, we must find the smallest tolerance $\\tau^\\star$ such that the false refinement fraction is less than or equal to $\\delta$.\n1.  **Monotonicity**: The false refinement fraction is a monotonically non-increasing function of $\\tau$. A smaller $\\tau$ leads to more refinements in total, and a higher proportion of them are likely to be \"false\" (triggered by noise).\n2.  **Binary Search**: This monotonicity allows for an efficient binary search to find the threshold $\\tau^\\star$. The search is performed on the logarithm of $\\tau$ over a wide range (e.g., from $10^{-20}$ to $10^{0}$).\n3.  **Procedure**:\n    a. Initialize a search range $[\\log \\tau_{\\min}, \\log \\tau_{\\max}]$.\n    b. In each step, select a midpoint $\\log \\tau_{\\text{mid}}$ and compute the corresponding $\\tau_{\\text{mid}} = 10^{\\log \\tau_{\\text{mid}}}$.\n    c. Run the AMR simulation with $\\tau_{\\text{mid}}$ to get the false fraction.\n    d. If the fraction is $\\le \\delta$, $\\tau_{\\text{mid}}$ is a valid tolerance. We store it as a potential answer and try to find an even smaller valid tolerance by setting $\\tau_{\\max} = \\tau_{\\text{mid}}$.\n    e. If the fraction is $>\\delta$, $\\tau_{\\text{mid}}$ is too small. We must increase it by setting $\\tau_{\\min} = \\tau_{\\text{mid}}$.\n4.  After a sufficient number of iterations, the search converges to the desired $\\tau^\\star$. This process is repeated for each of the three test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef f(x, A, k):\n    \"\"\"The scalar field function f(x) = A * sin(k*x).\"\"\"\n    return A * np.sin(k * x)\n\ndef f_ddd(x, A, k):\n    \"\"\"The analytical third derivative of f(x).\"\"\"\n    return -A * k**3 * np.cos(k * x)\n\ndef run_amr_simulation(A, k, N0, Lmax, tau):\n    \"\"\"\n    Simulates the AMR process for a given tolerance tau.\n\n    Args:\n        A (float): Amplitude of the sine function.\n        k (float): Wavenumber of the sine function.\n        N0 (int): Number of cells in the base grid.\n        Lmax (int): Maximum refinement level.\n        tau (float): Refinement tolerance.\n\n    Returns:\n        tuple: (total_refinements, false_refinements)\n    \"\"\"\n    total_refinements = 0\n    false_refinements = 0\n\n    h0 = 1.0 / N0\n    # A list of cells to process, structured as (x_center, level, h).\n    # We use a list as a queue for breadth-first processing.\n    cells_to_process = [( (i + 0.5) * h0, 0, h0) for i in range(N0)]\n\n    head = 0\n    while head  len(cells_to_process):\n        x, level, h = cells_to_process[head]\n        head += 1\n\n        if level >= Lmax:\n            continue\n\n        # Compute numerical derivatives and the error indicator\n        h_half = h / 2.0\n        \n        g_h = (f(x + h, A, k) - f(x - h, A, k)) / (2.0 * h)\n        g_h_half = (f(x + h_half, A, k) - f(x - h_half, A, k)) / (2.0 * h_half)\n\n        indicator = np.abs(g_h_half - g_h)\n\n        # Check refinement criterion\n        if indicator > tau:\n            total_refinements += 1\n\n            # Check for false refinement\n            # The theoretical truncation error for the parent cell is |f'''(x)/6| * h^2\n            theoretical_trunc_err = np.abs(f_ddd(x, A, k) / 6.0) * h**2\n\n            if theoretical_trunc_err = tau:\n                false_refinements += 1\n\n            # Add two child cells to the processing list for further refinement\n            cells_to_process.append((x - h / 4.0, level + 1, h_half))\n            cells_to_process.append((x + h / 4.0, level + 1, h_half))\n\n    return total_refinements, false_refinements\n\ndef find_tau_star(params):\n    \"\"\"\n    Finds the smallest tolerance tau_star satisfying the false fraction condition.\n    \n    Args:\n        params (tuple): A tuple containing all parameters for a test case.\n\n    Returns:\n        float: The calculated smallest tolerance tau_star.\n    \"\"\"\n    A, k, N0, Lmax, delta = params\n\n    # Binary search for tau_star on a logarithmic scale.\n    # The range is chosen to be wide enough for all expected scales.\n    log_tau_min = -30.0\n    log_tau_max = 5.0 \n    tau_star = 10**log_tau_max # Initialize with a safe upper bound.\n\n    # 100 iterations provide ample precision for a binary search.\n    for _ in range(100):\n        log_tau_mid = (log_tau_min + log_tau_max) / 2.0\n        tau_mid = 10**log_tau_mid\n\n        total_ref, false_ref = run_amr_simulation(A, k, N0, Lmax, tau_mid)\n\n        if total_ref == 0:\n            false_fraction = 0.0\n        else:\n            false_fraction = false_ref / total_ref\n\n        if false_fraction = delta:\n            # tau_mid is a valid tolerance. Store it and try for a smaller one.\n            tau_star = tau_mid\n            log_tau_max = log_tau_mid\n        else:\n            # tau_mid is too small. We need a larger tolerance.\n            log_tau_min = log_tau_mid\n\n    return tau_star\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (A, k, N0, Lmax, delta)\n    test_cases = [\n        # Case A: smooth, moderate amplitude\n        (1.0, 2.0 * np.pi, 64, 8, 0.05),\n        # Case B: round-off prone, tiny amplitude\n        (1.0e-12, 2.0 * np.pi, 64, 8, 0.05),\n        # Case C: high-frequency content\n        (1.0, 40.0 * np.pi, 128, 8, 0.05),\n    ]\n\n    results = []\n    for case in test_cases:\n        tau_star = find_tau_star(case)\n        results.append(f\"{tau_star:.6e}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "3536557"}]}