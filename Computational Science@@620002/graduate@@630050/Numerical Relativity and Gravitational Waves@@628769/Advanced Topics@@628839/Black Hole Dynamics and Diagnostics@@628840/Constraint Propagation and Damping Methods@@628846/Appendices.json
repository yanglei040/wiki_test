{"hands_on_practices": [{"introduction": "The first step in developing trustworthy scientific software is rigorous verification. This practice guides you through the method of manufactured solutions, a powerful technique to confirm that your implementation of differential operators achieves its theoretical order of accuracy by directly measuring the code's truncation error against a known analytic solution [@problem_id:3469183]. Successfully completing this exercise builds fundamental confidence in the correctness of a numerical simulation's spatial discretization.", "problem": "You are to implement and verify the scaling of the discrete constraint residuals, modeled on the Hamiltonian constraint and momentum constraints used in numerical relativity, under uniform grid refinement and optional constraint damping. Use a manufactured-solution test in a three-dimensional periodic domain to ensure that the continuum constraints vanish identically so that the discrete residuals directly measure truncation error. Your task is to compute the observed order of accuracy and to verify the expected $\\mathcal{O}(\\Delta x^{p})$ behavior, where $\\Delta x$ is the grid spacing and $p$ is the formal order of the finite-difference stencil.\n\nFundamental base:\n- The discrete truncation error of a consistent, centered, order-$p$ finite-difference approximation to a smooth derivative scales as $\\mathcal{O}(\\Delta x^{p})$.\n- The $L^{2}$ norm of a function $f$ over a domain $\\Omega$ with volume $V$ is defined as $\\lVert f \\rVert_{2} = \\left( \\int_{\\Omega} f^{2} \\, dV \\right)^{1/2}$. On a uniform grid in three dimensions with spacing $\\Delta x$ and total volume $V = 1$, the discrete $L^{2}$ norm is computed as $\\left( \\sum f^{2} \\Delta x^{3} \\right)^{1/2}$, which equals $\\left( \\mathrm{mean}(f^{2}) \\right)^{1/2}$.\n- If a constraint field $C$ satisfies a homogeneous damping law $\\partial_{t} C = - \\kappa C$, its exact solution after a fixed physical time $T$ is $C \\mapsto e^{-\\kappa T} C$, which does not alter the asymptotic order in $\\Delta x$.\n\nSet-up:\n- Domain: the periodic cube $[0,1]^{3}$ with uniform Cartesian grid spacing $\\Delta x = 1/N$ for a given integer $N \\ge 8$.\n- Angles in the trigonometric functions must be in radians.\n- Manufactured smooth analytic fields:\n  1. A scalar field $\\phi(x,y,z) = \\sin(2\\pi x)\\cos(2\\pi y) + \\sin(2\\pi z)$.\n  2. A scalar field $\\pi(x,y,z) = \\exp\\!\\big(\\sin(2\\pi x) + \\cos(2\\pi y) + \\sin(2\\pi z)\\big)$.\n- Define exact source fields so that the continuum constraints vanish:\n  1. The exact Laplacian of $\\phi$ is $\\nabla^{2}\\phi = -2(2\\pi)^{2} \\sin(2\\pi x)\\cos(2\\pi y) - (2\\pi)^{2} \\sin(2\\pi z)$. Define $S(x,y,z)$ to equal this exact Laplacian so that the continuum Hamiltonian constraint $H \\equiv \\nabla^{2}\\phi - S$ is identically zero.\n  2. The exact gradient of $\\pi$ is $\\nabla \\pi = \\big(\\partial_{x}\\pi,\\partial_{y}\\pi,\\partial_{z}\\pi\\big)$ with $\\partial_{x}\\pi = (2\\pi)\\cos(2\\pi x)\\,\\pi$, $\\partial_{y}\\pi = -(2\\pi)\\sin(2\\pi y)\\,\\pi$, and $\\partial_{z}\\pi = (2\\pi)\\cos(2\\pi z)\\,\\pi$. Define the exact vector source $J_{i}$ to equal these components so that the continuum momentum constraint $M_{i} \\equiv \\partial_{i}\\pi - J_{i}$ is identically zero.\n- Discrete operators:\n  - Let $D^{(p)}_{i}$ denote a standard centered, order-$p$ accurate finite-difference approximation to the first derivative $\\partial_{i}$ on a periodic uniform grid.\n  - Let $\\Delta^{(p)}$ denote a standard centered, order-$p$ accurate finite-difference approximation to the Laplacian $\\nabla^{2}$ on a periodic uniform grid, constructed as the sum of the second derivatives along each axis, each approximated to order $p$.\n  - You must implement the usual centered stencils of order $p \\in \\{2,4\\}$ for both first and second derivatives on a periodic grid. Do not use one-sided stencils.\n- Discrete residuals and norms:\n  - The discrete Hamiltonian residual is $H_{h} = \\Delta^{(p)} \\phi - S$.\n  - The discrete momentum residuals are $M_{i,h} = D^{(p)}_{i}\\pi - J_{i}$ for $i \\in \\{x,y,z\\}$.\n  - Define the scalar $L^{2}$ norm for $H_{h}$ as $\\lVert H_{h}\\rVert_{2} = \\left( \\mathrm{mean}(H_{h}^{2}) \\right)^{1/2}$.\n  - Define the vector $L^{2}$ norm for $M_{i,h}$ as $\\lVert \\mathbf{M}_{h}\\rVert_{2} = \\left( \\mathrm{mean}\\big(M_{x,h}^{2} + M_{y,h}^{2} + M_{z,h}^{2}\\big) \\right)^{1/2}$.\n- Optional damping:\n  - After computing $H_{h}$ and $M_{i,h}$, optionally apply homogeneous constraint damping for a fixed physical time $T = 1$ with parameter $\\kappa \\ge 0$ by the exact multiplicative update $H_{h} \\mapsto e^{-\\kappa} H_{h}$ and $M_{i,h} \\mapsto e^{-\\kappa} M_{i,h}$.\n\nScaling measurement:\n- For a fixed order $p$ and damping parameter $\\kappa$, compute $\\lVert H_{h}\\rVert_{2}$ and $\\lVert \\mathbf{M}_{h}\\rVert_{2}$ for each grid resolution $N$ in a given list. Let $\\Delta x = 1/N$.\n- Fit the observed order $p_{\\mathrm{obs}}$ by linear regression of $\\log \\lVert H_{h}\\rVert_{2}$ versus $\\log \\Delta x$ and similarly for $\\log \\lVert \\mathbf{M}_{h}\\rVert_{2}$, so that $\\lVert \\cdot \\rVert_{2} \\approx C \\, (\\Delta x)^{p_{\\mathrm{obs}}}$.\n- Verification criterion: declare success if both observed orders, $p_{\\mathrm{obs}}(H)$ and $p_{\\mathrm{obs}}(M)$, satisfy $\\lvert p_{\\mathrm{obs}} - p \\rvert \\le \\varepsilon$ for a tolerance $\\varepsilon$.\n\nTest suite to implement:\n- Use the following four parameter sets to compute and verify the observed orders. For each set, angles are in radians, and the damping update uses $T = 1$ as described above.\n  1. Case $1$: $p = 2$, $\\kappa = 0$, $N \\in \\{16, 24, 32, 48\\}$.\n  2. Case $2$: $p = 4$, $\\kappa = 0$, $N \\in \\{16, 24, 32, 48\\}$.\n  3. Case $3$: $p = 2$, $\\kappa = 20$, $N \\in \\{16, 24, 32, 48\\}$.\n  4. Case $4$: $p = 4$, $\\kappa = 10$, $N \\in \\{16, 24, 32, 48\\}$.\n- Use tolerance $\\varepsilon = 0.2$ for all cases.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the four cases. Each entry is a boolean that is $True$ if and only if both constraints meet the verification criterion for that case, and $False$ otherwise. For example, a valid output line is of the form $[b_{1},b_{2},b_{3},b_{4}]$ where each $b_{i}$ is either $True$ or $False$.", "solution": "The problem statement is valid. It describes a well-posed numerical verification task using a manufactured solution, a standard and scientifically sound method in the study of partial differential equations and numerical analysis. All parameters, equations, and procedures are defined precisely and are mutually consistent.\n\nThe solution to this problem is founded on several key principles of numerical analysis:\n\n1.  **Method of Manufactured Solutions**: To test the convergence of a numerical scheme for a differential equation, we choose a smooth, analytic function as a \"manufactured\" solution. We substitute this solution into the differential operators to compute the corresponding analytic source terms. This creates a problem where the exact solution is known. The continuum constraints, $H \\equiv \\nabla^{2}\\phi - S$ and $M_{i} \\equiv \\partial_{i}\\pi - J_{i}$, are therefore satisfied identically (equal to zero) by construction.\n\n2.  **Truncation Error as the Residual**: When the discrete finite-difference operators are applied to the analytic solution on a grid, the result will not be exactly equal to the analytic source terms. This difference, known as the discrete residual (e.g., $H_{h} = \\Delta^{(p)} \\phi - S$), is precisely the truncation error of the finite-difference scheme. For a method of order $p$, this error is expected to scale as $\\mathcal{O}(\\Delta x^p)$, where $\\Delta x$ is the grid spacing.\n\n3.  **Convergence Analysis**: By computing the discrete residual for a sequence of decreasing grid spacings $\\Delta x$, we can measure the observed order of convergence, $p_{\\mathrm{obs}}$. A robust method for determining $p_{\\mathrm{obs}}$ is to perform a linear regression on the logarithmic data. The relationship $\\lVert \\text{Error} \\rVert \\approx C(\\Delta x)^p$ becomes linear in a log-log plot: $\\log(\\lVert \\text{Error} \\rVert) \\approx p \\log(\\Delta x) + \\log(C)$. The slope of this line is the order of convergence $p$.\n\n4.  **Finite-Difference Operators on Periodic Grids**: The differential operators $\\partial_i$ and $\\nabla^2$ are approximated using centered finite-difference stencils. For a periodic domain, points required by the stencil that lie outside the grid are \"wrapped around\" from the other side. This is implemented efficiently using array-rolling operations. The specific coefficients of the stencils are chosen to cancel lower-order error terms in the Taylor series expansion, achieving a desired order of accuracy $p$.\n\nThe algorithmic procedure to solve the problem is as follows:\n\nFor each of the four test cases defined by the order $p$, damping parameter $\\kappa$, and list of resolutions $N$:\n\n1.  Initialize two lists to store the computed $L^2$ norms for the Hamiltonian residual, $\\lVert H_h \\rVert_2$, and the momentum residual, $\\lVert \\mathbf{M}_h \\rVert_2$, for each resolution. Also, prepare a list of the corresponding grid spacings, $\\Delta x = 1/N$.\n\n2.  For each resolution $N$ in the specified list:\n    a. Construct a uniform 3D Cartesian grid for the domain $[0,1]^3$. The grid coordinates are $(x_i, y_j, z_k)$ where $x_i = i/N, y_j=j/N, z_k=k/N$ for $i,j,k \\in \\{0, \\dots, N-1\\}$.\n    b. Evaluate the analytic fields $\\phi$ and $\\pi$, and the corresponding source fields $S$ and $J_i$, on every point of this grid, storing them in 3D arrays.\n    c. Implement the centered finite-difference operators for the first derivative ($D_i^{(p)}$) and the Laplacian ($\\Delta^{(p)} = D_{xx}^{(p)} + D_{yy}^{(p)} + D_{zz}^{(p)}$) for the specified order $p \\in \\{2,4\\}$. The periodicity of the grid is handled by using circular shifts (rolls) of the data arrays.\n    \n    The stencils are:\n    - $p=2$:\n      - First derivative $D_i^{(2)} f$: $\\frac{f_{i+1} - f_{i-1}}{2\\Delta x}$\n      - Second derivative $D_{ii}^{(2)} f$: $\\frac{f_{i+1} - 2f_i + f_{i-1}}{(\\Delta x)^2}$\n    - $p=4$:\n      - First derivative $D_i^{(4)} f$: $\\frac{1}{12\\Delta x}(f_{i-2} - 8f_{i-1} + 8f_{i+1} - f_{i+2})$\n      - Second derivative $D_{ii}^{(4)} f$: $\\frac{1}{12(\\Delta x)^2}(-f_{i-2} + 16f_{i-1} - 30f_i + 16f_{i+1} - f_{i+2})$\n    \n    d. Compute the discrete residuals. The Hamiltonian residual is $H_h = \\Delta^{(p)}\\phi - S$. The momentum residuals are $M_{i,h} = D_i^{(p)}\\pi - J_i$ for $i \\in \\{x,y,z\\}$.\n    e. Calculate the $L^2$ norms of the residuals as defined by the problem: $\\lVert H_{h}\\rVert_{2} = \\left( \\mathrm{mean}(H_{h}^{2}) \\right)^{1/2}$ and $\\lVert \\mathbf{M}_{h}\\rVert_{2} = \\left( \\mathrm{mean}(M_{x,h}^{2} + M_{y,h}^{2} + M_{z,h}^{2}) \\right)^{1/2}$.\n    f. If $\\kappa > 0$, apply the damping factor by multiplying the computed norms by $e^{-\\kappa T}$ with $T=1$. Note that this constant multiplicative factor does not alter the slope of the log-log error plot, hence it does not affect the observed convergence order.\n    g. Append the final norms and the grid spacing $\\Delta x$ to their respective lists.\n\n3.  After iterating through all resolutions, perform a linear regression on the logarithm of the norms versus the logarithm of the grid spacings. The slope of the resulting line for the Hamiltonian residual provides $p_{\\mathrm{obs}}(H)$, and similarly for the momentum residual, $p_{\\mathrm{obs}}(M)$.\n\n4.  Finally, verify the success of the test case. The condition for success is that both observed orders are close to the theoretical order $p$, i.e., $| p_{\\mathrm{obs}}(H) - p | \\le \\varepsilon$ and $| p_{\\mathrm{obs}}(M) - p | \\le \\varepsilon$, with the given tolerance $\\varepsilon = 0.2$. The result for the case is a boolean value (True for success, False for failure).\n\nThis entire process is repeated for all four test cases, and the final output is a list of the four boolean results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Implements and verifies the scaling of discrete constraint residuals\n    for a manufactured solution test in numerical relativity.\n    \"\"\"\n\n    # --- Finite Difference Operators ---\n\n    def diff_x_p2(f, dx):\n        \"\"\"Order-2 centered 1st derivative along axis 0.\"\"\"\n        return (np.roll(f, -1, axis=0) - np.roll(f, 1, axis=0)) / (2.0 * dx)\n\n    def diff_y_p2(f, dx):\n        \"\"\"Order-2 centered 1st derivative along axis 1.\"\"\"\n        return (np.roll(f, -1, axis=1) - np.roll(f, 1, axis=1)) / (2.0 * dx)\n\n    def diff_z_p2(f, dx):\n        \"\"\"Order-2 centered 1st derivative along axis 2.\"\"\"\n        return (np.roll(f, -1, axis=2) - np.roll(f, 1, axis=2)) / (2.0 * dx)\n\n    def diff_xx_p2(f, dx):\n        \"\"\"Order-2 centered 2nd derivative along axis 0.\"\"\"\n        return (np.roll(f, -1, axis=0) - 2.0 * f + np.roll(f, 1, axis=0)) / (dx**2)\n\n    def diff_yy_p2(f, dx):\n        \"\"\"Order-2 centered 2nd derivative along axis 1.\"\"\"\n        return (np.roll(f, -1, axis=1) - 2.0 * f + np.roll(f, 1, axis=1)) / (dx**2)\n\n    def diff_zz_p2(f, dx):\n        \"\"\"Order-2 centered 2nd derivative along axis 2.\"\"\"\n        return (np.roll(f, -1, axis=2) - 2.0 * f + np.roll(f, 1, axis=2)) / (dx**2)\n\n    def diff_x_p4(f, dx):\n        \"\"\"Order-4 centered 1st derivative along axis 0.\"\"\"\n        c = [1.0/12.0, -8.0/12.0, 8.0/12.0, -1.0/12.0]\n        return (c[0] * np.roll(f, 2, axis=0) + c[1] * np.roll(f, 1, axis=0) + \n                c[2] * np.roll(f, -1, axis=0) + c[3] * np.roll(f, -2, axis=0)) / dx\n\n    def diff_y_p4(f, dx):\n        \"\"\"Order-4 centered 1st derivative along axis 1.\"\"\"\n        c = [1.0/12.0, -8.0/12.0, 8.0/12.0, -1.0/12.0]\n        return (c[0] * np.roll(f, 2, axis=1) + c[1] * np.roll(f, 1, axis=1) + \n                c[2] * np.roll(f, -1, axis=1) + c[3] * np.roll(f, -2, axis=1)) / dx\n\n    def diff_z_p4(f, dx):\n        \"\"\"Order-4 centered 1st derivative along axis 2.\"\"\"\n        c = [1.0/12.0, -8.0/12.0, 8.0/12.0, -1.0/12.0]\n        return (c[0] * np.roll(f, 2, axis=2) + c[1] * np.roll(f, 1, axis=2) + \n                c[2] * np.roll(f, -1, axis=2) + c[3] * np.roll(f, -2, axis=2)) / dx\n\n    def diff_xx_p4(f, dx):\n        \"\"\"Order-4 centered 2nd derivative along axis 0.\"\"\"\n        c = [-1.0/12.0, 16.0/12.0, -30.0/12.0, 16.0/12.0, -1.0/12.0]\n        return (c[0] * np.roll(f, 2, axis=0) + c[1] * np.roll(f, 1, axis=0) + c[2] * f +\n                c[3] * np.roll(f, -1, axis=0) + c[4] * np.roll(f, -2, axis=0)) / (dx**2)\n\n    def diff_yy_p4(f, dx):\n        \"\"\"Order-4 centered 2nd derivative along axis 1.\"\"\"\n        c = [-1.0/12.0, 16.0/12.0, -30.0/12.0, 16.0/12.0, -1.0/12.0]\n        return (c[0] * np.roll(f, 2, axis=1) + c[1] * np.roll(f, 1, axis=1) + c[2] * f +\n                c[3] * np.roll(f, -1, axis=1) + c[4] * np.roll(f, -2, axis=1)) / (dx**2)\n\n    def diff_zz_p4(f, dx):\n        \"\"\"Order-4 centered 2nd derivative along axis 2.\"\"\"\n        c = [-1.0/12.0, 16.0/12.0, -30.0/12.0, 16.0/12.0, -1.0/12.0]\n        return (c[0] * np.roll(f, 2, axis=2) + c[1] * np.roll(f, 1, axis=2) + c[2] * f +\n                c[3] * np.roll(f, -1, axis=2) + c[4] * np.roll(f, -2, axis=2)) / (dx**2)\n\n    # --- Analytic Functions ---\n    \n    def get_analytic_fields(N):\n        dx = 1.0 / N\n        coords = np.arange(N) * dx\n        x, y, z = np.meshgrid(coords, coords, coords, indexing='ij')\n\n        pi2 = 2.0 * np.pi\n        \n        # Field phi and its Laplacian source S\n        phi = np.sin(pi2 * x) * np.cos(pi2 * y) + np.sin(pi2 * z)\n        S = -2.0 * (pi2**2) * np.sin(pi2 * x) * np.cos(pi2 * y) - (pi2**2) * np.sin(pi2 * z)\n\n        # Field pi and its gradient source J\n        pi_field = np.exp(np.sin(pi2 * x) + np.cos(pi2 * y) + np.sin(pi2 * z))\n        Jx = pi2 * np.cos(pi2 * x) * pi_field\n        Jy = -pi2 * np.sin(pi2 * y) * pi_field\n        Jz = pi2 * np.cos(pi2 * z) * pi_field\n\n        return phi, pi_field, S, Jx, Jy, Jz\n\n    def run_convergence_test(p, kappa, N_list):\n        dx_list = []\n        h_norms = []\n        m_norms = []\n\n        for N in N_list:\n            dx = 1.0 / N\n            dx_list.append(dx)\n\n            phi, pi_field, S, Jx, Jy, Jz = get_analytic_fields(N)\n\n            if p == 2:\n                # Hamiltonian residual\n                lap_phi = diff_xx_p2(phi, dx) + diff_yy_p2(phi, dx) + diff_zz_p2(phi, dx)\n                Hh = lap_phi - S\n                \n                # Momentum residuals\n                Mxh = diff_x_p2(pi_field, dx) - Jx\n                Myh = diff_y_p2(pi_field, dx) - Jy\n                Mzh = diff_z_p2(pi_field, dx) - Jz\n            elif p == 4:\n                # Hamiltonian residual\n                lap_phi = diff_xx_p4(phi, dx) + diff_yy_p4(phi, dx) + diff_zz_p4(phi, dx)\n                Hh = lap_phi - S\n\n                # Momentum residuals\n                Mxh = diff_x_p4(pi_field, dx) - Jx\n                Myh = diff_y_p4(pi_field, dx) - Jy\n                Mzh = diff_z_p4(pi_field, dx) - Jz\n            else:\n                raise ValueError(\"Unsupported order p.\")\n\n            # Calculate L2 norms\n            h_norm = np.sqrt(np.mean(Hh**2))\n            m_norm = np.sqrt(np.mean(Mxh**2 + Myh**2 + Mzh**2))\n            \n            # Apply damping\n            damping_factor = np.exp(-kappa * 1.0)\n            h_norms.append(h_norm * damping_factor)\n            m_norms.append(m_norm * damping_factor)\n\n        # Fit convergence order\n        log_dx = np.log(dx_list)\n        log_h_norm = np.log(h_norms)\n        log_m_norm = np.log(m_norms)\n\n        p_obs_h = stats.linregress(log_dx, log_h_norm).slope\n        p_obs_m = stats.linregress(log_dx, log_m_norm).slope\n        \n        return p_obs_h, p_obs_m\n\n    test_cases = [\n        {'p': 2, 'kappa': 0, 'N_list': [16, 24, 32, 48]},\n        {'p': 4, 'kappa': 0, 'N_list': [16, 24, 32, 48]},\n        {'p': 2, 'kappa': 20, 'N_list': [16, 24, 32, 48]},\n        {'p': 4, 'kappa': 10, 'N_list': [16, 24, 32, 48]},\n    ]\n    tolerance = 0.2\n    \n    final_results = []\n    for case in test_cases:\n        p_expected = case['p']\n        p_obs_h, p_obs_m = run_convergence_test(p_expected, case['kappa'], case['N_list'])\n        \n        h_success = abs(p_obs_h - p_expected) <= tolerance\n        m_success = abs(p_obs_m - p_expected) <= tolerance\n        \n        final_results.append(h_success and m_success)\n\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n\n```", "id": "3469183"}, {"introduction": "An accurate numerical scheme must also be stable to be useful for long-term evolutions, and constraint damping terms often introduce stiffness that challenges standard methods. This exercise investigates the stability constraints on time-stepping algorithms, revealing why purely explicit methods can be prohibitively inefficient for stiff systems [@problem_id:3469166]. This analysis provides the crucial motivation for adopting the Implicit-Explicit (IMEX) schemes that are a cornerstone of many modern numerical relativity codes.", "problem": "Consider the method-of-lines discretization of a constraint propagation subsystem in numerical relativity, where the dominant stiff term is a linear damping of the form $-\\kappa C$, with $C$ a constraint variable and $\\kappa > 0$ a damping coefficient. After spatial discretization, focus on the linear scalar ordinary differential equation (ODE) $\\frac{dC}{dt} = -\\kappa C$ for a single Fourier mode of $C$. In binary black hole evolutions using generalized harmonic formulations, realistic choices often make the product $\\kappa \\Delta t$ non-negligible, where $\\Delta t$ is the timestep chosen to satisfy a Courant–Friedrichs–Lewy (CFL) bound for wave propagation speeds near the speed of light.\n\nYou will compare a purely explicit fourth-order explicit Runge–Kutta method (ERK4) to an implicit–explicit (IMEX) splitting where the stiff damping part is treated implicitly via backward Euler while nonstiff terms would be treated explicitly. To ground the discussion in first principles, start from the definition of a Runge–Kutta method applied to the linear test equation $y' = \\lambda y$ with $\\lambda \\in \\mathbb{C}$, and define the associated linear stability function $R(z)$, where $z = \\lambda \\Delta t$. For the ERK4 method applied to $y'=-\\kappa y$ (so $z=-x$ lies on the negative real axis, with $x = \\kappa \\Delta t > 0$), the stability requirement is $|R(z)| \\leq 1$. For the implicit part (backward Euler) of the IMEX scheme applied solely to the stiff term, the associated stability function is defined analogously.\n\nUsing this framework:\n- Derive the ERK4 stability function $R(z)$ for the linear test equation from the Runge–Kutta stage equations and update formula, without invoking pre-memorized stability polynomials.\n- Specialize to $z=-x$ with $x=\\kappa \\Delta t > 0$ and determine the largest $x$ such that the explicit ERK4 method remains linearly stable on the negative real axis, that is, the unique positive solution $x_{\\mathrm{crit}}$ of the boundary condition $|R(-x_{\\mathrm{crit}})| = 1$ with $x_{\\mathrm{crit}} \\neq 0$.\n- Briefly explain, using the backward Euler stability function derived from first principles, why the IMEX treatment of the stiff term $-\\kappa C$ is unconditionally stable for all $x>0$ on the negative real axis.\n\nAssume code units with total mass $M$ set to $M=1$ so that realistic parameters such as $M/64 \\le \\Delta x \\le M/8$, CFL number $0.3 \\le \\nu \\le 0.5$ so $\\Delta t = \\nu \\Delta x$, and damping parameters $10 \\le \\kappa \\le 100$ are plausible in different regions of the domain. Use these to interpret qualitatively where typical $\\kappa \\Delta t$ values fall relative to the explicit ERK4 stability interval you derive.\n\nCompute $x_{\\mathrm{crit}}$ and express your final answer as a dimensionless number, rounded to four significant figures.", "solution": "This problem is valid. It is scientifically grounded in the principles of numerical analysis for differential equations, specifically within the context of numerical relativity. The problem is well-posed, objective, and contains all necessary information to proceed to a unique solution.\n\nThe primary task is to analyze the stability of numerical methods for the stiff ordinary differential equation (ODE) $\\frac{dC}{dt} = -\\kappa C$, which models a constraint damping term in numerical relativity simulations. We will compare a fully explicit method (ERK4) with an implicit-explicit (IMEX) approach.\n\nFirst, we derive the stability function for the classical fourth-order explicit Runge-Kutta (ERK4) method. A general Runge-Kutta method is defined by its stages. For the classical ERK4, these are:\n$$\nk_1 = f(t_n, y_n) \\\\\nk_2 = f(t_n + \\frac{1}{2}\\Delta t, y_n + \\frac{1}{2}\\Delta t k_1) \\\\\nk_3 = f(t_n + \\frac{1}{2}\\Delta t, y_n + \\frac{1}{2}\\Delta t k_2) \\\\\nk_4 = f(t_n + \\Delta t, y_n + \\Delta t k_3)\n$$\nThe solution is then updated via:\n$$\ny_{n+1} = y_n + \\frac{\\Delta t}{6}(k_1 + 2k_2 + 2k_3 + k_4)\n$$\nTo find the stability function $R(z)$, we apply this scheme to the linear test equation $y' = \\lambda y$, for which $f(t,y) = \\lambda y$. Let $z = \\lambda \\Delta t$. The stages become:\n$$\nk_1 = \\lambda y_n \\\\\nk_2 = \\lambda \\left(y_n + \\frac{\\Delta t}{2}(\\lambda y_n)\\right) = \\lambda y_n \\left(1 + \\frac{\\lambda \\Delta t}{2}\\right) = \\lambda y_n \\left(1 + \\frac{z}{2}\\right) \\\\\nk_3 = \\lambda \\left(y_n + \\frac{\\Delta t}{2}k_2\\right) = \\lambda \\left(y_n + \\frac{\\Delta t}{2} \\left[\\lambda y_n \\left(1 + \\frac{z}{2}\\right)\\right]\\right) = \\lambda y_n \\left(1 + \\frac{z}{2}\\left(1 + \\frac{z}{2}\\right)\\right) = \\lambda y_n \\left(1 + \\frac{z}{2} + \\frac{z^2}{4}\\right) \\\\\nk_4 = \\lambda (y_n + \\Delta t k_3) = \\lambda \\left(y_n + \\Delta t \\left[\\lambda y_n \\left(1 + \\frac{z}{2} + \\frac{z^2}{4}\\right)\\right]\\right) = \\lambda y_n \\left(1 + z\\left(1 + \\frac{z}{2} + \\frac{z^2}{4}\\right)\\right) = \\lambda y_n \\left(1 + z + \\frac{z^2}{2} + \\frac{z^3}{4}\\right)\n$$\nSubstituting these into the update formula $y_{n+1} = R(z) y_n$:\n$$\ny_{n+1} = y_n + \\frac{\\Delta t}{6} \\left[\\lambda y_n + 2\\lambda y_n \\left(1 + \\frac{z}{2}\\right) + 2\\lambda y_n \\left(1 + \\frac{z}{2} + \\frac{z^2}{4}\\right) + \\lambda y_n \\left(1 + z + \\frac{z^2}{2} + \\frac{z^3}{4}\\right)\\right]\n$$\nDividing by $y_n$ and recalling $z = \\lambda \\Delta t$:\n$$\nR(z) = 1 + \\frac{z}{6} \\left[1 + 2\\left(1 + \\frac{z}{2}\\right) + 2\\left(1 + \\frac{z}{2} + \\frac{z^2}{4}\\right) + \\left(1 + z + \\frac{z^2}{2} + \\frac{z^3}{4}\\right)\\right]\n$$\n$$\nR(z) = 1 + \\frac{z}{6} \\left[1 + 2 + z + 2 + z + \\frac{z^2}{2} + 1 + z + \\frac{z^2}{2} + \\frac{z^3}{4}\\right]\n$$\nCombining terms inside the bracket:\n$$\nR(z) = 1 + \\frac{z}{6} \\left[(1+2+2+1) + (z+z+z) + \\left(\\frac{z^2}{2}+\\frac{z^2}{2}\\right) + \\frac{z^3}{4}\\right]\n$$\n$$\nR(z) = 1 + \\frac{z}{6} \\left[6 + 3z + z^2 + \\frac{z^3}{4}\\right]\n$$\nDistributing the $z/6$ term gives the stability function for ERK4:\n$$\nR(z) = 1 + z + \\frac{z^2}{2} + \\frac{z^3}{6} + \\frac{z^4}{24}\n$$\nThis is, as expected, the fourth-order Taylor series expansion of $\\exp(z)$.\n\nNext, we determine the stability limit for this method on the negative real axis. Our ODE is $\\frac{dC}{dt} = -\\kappa C$, so $\\lambda = -\\kappa$. We define $x = \\kappa \\Delta t > 0$, which means $z = \\lambda \\Delta t = -x$. The stability condition is $|R(-x)| \\le 1$.\nThe stability function is:\n$$\nR(-x) = 1 - x + \\frac{x^2}{2} - \\frac{x^3}{6} + \\frac{x^4}{24}\n$$\nThe stability interval on the negative real axis is $[-x_{\\mathrm{crit}}, 0]$, where $x_{\\mathrm{crit}}$ is the largest positive value of $x$ for which $|R(-x)| \\le 1$ for all $x \\in [0, x_{\\mathrm{crit}}]$. The boundary of the stability region is where $|R(-x)| = 1$. This yields two conditions: $R(-x) = 1$ or $R(-x) = -1$.\nCase 1: $R(-x) = 1$\n$$\n1 - x + \\frac{x^2}{2} - \\frac{x^3}{6} + \\frac{x^4}{24} = 1\n$$\n$$\n-x + \\frac{x^2}{2} - \\frac{x^3}{6} + \\frac{x^4}{24} = 0\n$$\nFactoring out $x$ (since we seek $x_{\\mathrm{crit}} \\neq 0$):\n$$\nx\\left(-1 + \\frac{x}{2} - \\frac{x^2}{6} + \\frac{x^3}{24}\\right) = 0\n$$\nWe must find the positive root of the polynomial in the parenthesis. Multiplying by $24$:\n$$\nx^3 - 4x^2 + 12x - 24 = 0\n$$\nCase 2: $R(-x) = -1$\n$$\n\\frac{x^4}{24} - \\frac{x^3}{6} + \\frac{x^2}{2} - x + 2 = 0\n$$\nThe function $R(-x)$ starts at $R(0)=1$ and initially decreases. A plot reveals that its minimum value is positive, so it never reaches $-1$. Therefore, the equation from Case 2 has no real roots, and the stability boundary is defined entirely by the non-zero root from Case 1.\nWe need to solve $x^3 - 4x^2 + 12x - 24 = 0$ for $x > 0$. Numerical methods (e.g., Newton-Raphson) yield a single real root.\n$$\nx_{\\mathrm{crit}} \\approx 2.78528178...\n$$\nRounding to four significant figures, $x_{\\mathrm{crit}} = 2.785$. The stability interval for ERK4 on the negative real axis is approximately $[-2.785, 0]$. This means that for stability, we must have $\\kappa \\Delta t \\le 2.785$.\n\nThird, we analyze the stability of the backward Euler method, proposed for the stiff term in an IMEX scheme. The backward Euler method for $y' = f(t,y)$ is $y_{n+1} = y_n + \\Delta t f(t_{n+1}, y_{n+1})$. Applying this to the test equation $y' = \\lambda y$:\n$$\ny_{n+1} = y_n + \\Delta t (\\lambda y_{n+1}) = y_n + z y_{n+1}\n$$\nSolving for $y_{n+1}$:\n$$\ny_{n+1}(1-z) = y_n \\implies y_{n+1} = \\frac{1}{1-z}y_n\n$$\nThe stability function for backward Euler is $R_{\\mathrm{BE}}(z) = \\frac{1}{1-z}$. For the damping term, $z = -x = -\\kappa \\Delta t$.\n$$\nR_{\\mathrm{BE}}(-x) = \\frac{1}{1 - (-x)} = \\frac{1}{1+x}\n$$\nThe stability requirement is $|R_{\\mathrm{BE}}(-x)| \\le 1$. Since $x = \\kappa \\Delta t > 0$, we have $1+x > 1$. Therefore:\n$$\n0  \\frac{1}{1+x}  1\n$$\nThis inequality holds for all $x > 0$. Thus, the backward Euler method is unconditionally stable for this type of damping term. This property, known as A-stability, makes it ideal for treating stiff terms within an IMEX framework, as it imposes no stability restriction on the timestep $\\Delta t$.\n\nFinally, we interpret these results using the provided realistic parameters in code units where $M=1$: $M/64 \\le \\Delta x \\le M/8$, $0.3 \\le \\nu \\le 0.5$, and $10 \\le \\kappa \\le 100$. The parameter of interest is $x = \\kappa \\Delta t = \\kappa \\nu \\Delta x$.\nLet's find the approximate range of $x$.\nThe minimum value occurs for the smallest $\\kappa, \\nu, \\Delta x$:\n$$\nx_{\\mathrm{min}} = 10 \\times 0.3 \\times \\frac{1}{64} = \\frac{3}{64} \\approx 0.047\n$$\nThe maximum value occurs for the largest $\\kappa, \\nu, \\Delta x$:\n$$\nx_{\\mathrm{max}} = 100 \\times 0.5 \\times \\frac{1}{8} = \\frac{50}{8} = 6.25\n$$\nThe range of plausible values for $x = \\kappa \\Delta t$ is approximately $[0.047, 6.25]$.\nComparing this range to the ERK4 stability limit $x_{\\mathrm{crit}} \\approx 2.785$:\n- At the low end of the parameter range (e.g., weak damping, fine resolution), $x$ is well within the stability region of ERK4 ($0.047  2.785$).\n- At the high end of the parameter range (e.g., strong damping, coarse resolution), $x$ can significantly exceed the stability limit of ERK4 ($6.25 > 2.785$).\nThis analysis demonstrates that for many realistic simulation setups in numerical relativity, a purely explicit method like ERK4 would be forced to take a timestep much smaller than what is required by the CFL condition for wave propagation, purely to satisfy the stability of the constraint damping term. This makes the explicit approach inefficient. The IMEX method, by treating the stiff damping term implicitly, circumvents this severe timestep restriction and allows the timestep to be chosen based on the physics of the non-stiff terms (like gravitational wave propagation), leading to much more efficient computations.\n\nThe calculated critical value for $x$ is $x_{\\mathrm{crit}} \\approx 2.785$.", "answer": "$$\n\\boxed{2.785}\n$$", "id": "3469166"}, {"introduction": "Effective constraint damping is not about maximizing the decay rate, but about finding an optimal balance between benefits and costs. This practice challenges you to develop a simple performance model to determine the ideal damping parameter, considering the trade-offs between accelerated constraint reduction, numerically-generated noise, and computational overhead [@problem_id:3469179]. This exercise fosters a practical mindset, showing how to use simplified models to make informed, quantitative decisions in complex simulation environments.", "problem": "In a method-of-lines finite difference evolution of a first-order symmetric hyperbolic formulation for general relativity augmented with a linear constraint damping term, consider a single Fourier component of the constraint field $C(t,x)$ advected with characteristic speed $v$ and damped with parameter $\\gamma$. Along characteristics, model the local constraint amplitude by the ordinary differential equation\n$$\n\\frac{d C}{d t} \\;=\\; -\\,\\gamma\\, C \\;+\\; S(\\gamma),\n$$\nwhere $S(\\gamma)$ models numerically generated constraint violations. Assume a simple, physically motivated source model that includes a baseline contribution from spatial truncation error and an additional contribution proportional to the damping strength,\n$$\nS(\\gamma) \\;=\\; S_{0}\\,\\big(1 + b\\, \\gamma\\, \\Delta t\\big),\n$$\nwith $S_{0} > 0$, $b>0$ dimensionless, and $\\Delta t$ the time step. The code advances in time using the explicit forward Euler method with a fixed Courant–Friedrichs–Lewy (CFL) factor $\\lambda$, so that $\\Delta t = \\lambda\\, \\Delta x / v$, and the forward Euler stability requirement for pure decay imposes $0 \\le \\gamma\\, \\Delta t \\le 2$.\n\nAssume that the per-step computational cost increases linearly with the damping work (for example, due to operator splitting or additional evaluations), modeled by a relative cost factor\n$$\nK(\\gamma) \\;=\\; 1 + c\\, \\gamma\\, \\Delta t,\n$$\nwith $c>0$ dimensionless. As a performance objective that quantifies “constraint decay per unit computational cost,” define the steady-state constraint level per unit cost,\n$$\nJ(\\gamma) \\;=\\; K(\\gamma)\\, C_{\\mathrm{ss}}(\\gamma),\n$$\nwhere $C_{\\mathrm{ss}}(\\gamma)$ is the steady-state $C$ reached under the constant source $S(\\gamma)$.\n\nStarting only from these definitions and the forward Euler stability constraint, determine the value of the damping parameter $\\gamma$ that optimizes the objective, i.e., minimizes $J(\\gamma)$ subject to $0  \\gamma\\, \\Delta t \\le 2$. Express your final answer as a single closed-form analytic expression for the optimal $\\gamma$ in terms of the given symbols, with no numerical substitution.", "solution": "The objective is to find the value of the damping parameter $\\gamma$ that minimizes the performance metric $J(\\gamma) = K(\\gamma) C_{\\mathrm{ss}}(\\gamma)$ subject to the constraint $0  \\gamma \\Delta t \\le 2$.\n\nFirst, we determine the steady-state constraint level, $C_{\\mathrm{ss}}(\\gamma)$. The steady state is defined by the condition $\\frac{dC}{dt} = 0$. The governing ordinary differential equation for the constraint amplitude $C$ is given as:\n$$\n\\frac{d C}{d t} = -\\gamma C + S(\\gamma)\n$$\nSetting the time derivative to zero gives the steady-state condition:\n$$\n0 = -\\gamma C_{\\mathrm{ss}}(\\gamma) + S(\\gamma)\n$$\nFor a non-zero damping parameter, $\\gamma > 0$, we can solve for $C_{\\mathrm{ss}}(\\gamma)$:\n$$\nC_{\\mathrm{ss}}(\\gamma) = \\frac{S(\\gamma)}{\\gamma}\n$$\nThe problem specifies the source term as $S(\\gamma) = S_{0}(1 + b \\gamma \\Delta t)$, where $S_{0} > 0$ and $b > 0$. Substituting this into the expression for $C_{\\mathrm{ss}}(\\gamma)$ yields:\n$$\nC_{\\mathrm{ss}}(\\gamma) = \\frac{S_{0}(1 + b \\gamma \\Delta t)}{\\gamma}\n$$\nNext, we construct the objective function $J(\\gamma)$. The relative computational cost factor is given by $K(\\gamma) = 1 + c \\gamma \\Delta t$, where $c > 0$. The objective function $J(\\gamma)$ is the product of the cost and the steady-state constraint level:\n$$\nJ(\\gamma) = K(\\gamma) C_{\\mathrm{ss}}(\\gamma) = (1 + c \\gamma \\Delta t) \\left( \\frac{S_{0}(1 + b \\gamma \\Delta t)}{\\gamma} \\right)\n$$\nTo facilitate the optimization, we introduce the dimensionless variable $x = \\gamma \\Delta t$. The stability constraint $0  \\gamma \\Delta t \\le 2$ becomes $0  x \\le 2$. From the definition of $x$, we have $\\gamma = x / \\Delta t$. We can now express the objective function $J$ in terms of $x$:\n$$\nJ(x) = (1 + c x) \\left( \\frac{S_{0}(1 + b x)}{x / \\Delta t} \\right) = \\frac{S_{0} \\Delta t}{x} (1 + b x)(1 + c x)\n$$\nExpanding the product of the binomials gives:\n$$\nJ(x) = \\frac{S_{0} \\Delta t}{x} (1 + (b+c)x + bcx^2)\n$$\nDistributing the leading factor $S_0 \\Delta t / x$:\n$$\nJ(x) = S_{0} \\Delta t \\left(\\frac{1}{x} + (b+c) + bcx \\right)\n$$\nTo find the value of $x$ that minimizes $J(x)$, we compute the first derivative of $J(x)$ with respect to $x$. The factors $S_0$, $\\Delta t$, $b$, and $c$ are all positive constants.\n$$\n\\frac{dJ}{dx} = S_{0} \\Delta t \\frac{d}{dx} \\left(\\frac{1}{x} + (b+c) + bcx \\right) = S_{0} \\Delta t \\left(-\\frac{1}{x^2} + bc \\right)\n$$\nWe find the critical points by setting the derivative to zero:\n$$\n-\\frac{1}{x^2} + bc = 0 \\implies bc = \\frac{1}{x^2}\n$$\nSolving for $x^2$ gives $x^2 = \\frac{1}{bc}$. Since $x = \\gamma \\Delta t$ must be positive, we take the positive root:\n$$\nx_{\\mathrm{crit}} = \\frac{1}{\\sqrt{bc}}\n$$\nTo verify that this critical point corresponds to a minimum, we examine the second derivative:\n$$\n\\frac{d^2J}{dx^2} = S_{0} \\Delta t \\frac{d}{dx} \\left(-\\frac{1}{x^2} + bc \\right) = S_{0} \\Delta t \\left(\\frac{2}{x^3}\\right)\n$$\nSince $S_0 > 0$, $\\Delta t > 0$, and we are considering $x>0$, the second derivative is always positive. This confirms that $J(x)$ is a convex function for $x>0$ and that $x_{\\mathrm{crit}}$ is the location of its unique global minimum.\n\nThe optimization must be performed over the interval specified by the stability constraint, $x \\in (0, 2]$.\nThe behavior of $J(x)$ on this interval determines the optimal value $x_{\\mathrm{opt}}$.\nAs $x \\to 0^{+}$, the term $\\frac{1}{x}$ dominates and $J(x) \\to \\infty$.\nThe function decreases from $x=0$ until it reaches its global minimum at $x_{\\mathrm{crit}}$, and increases thereafter.\n\nWe must compare the location of the unconstrained minimum $x_{\\mathrm{crit}}$ with the allowed interval $(0, 2]$.\n\nCase 1: $x_{\\mathrm{crit}} \\le 2$. This condition is equivalent to $\\frac{1}{\\sqrt{bc}} \\le 2$, or $bc \\ge \\frac{1}{4}$. In this case, the global minimum lies within the allowed interval. Thus, the optimal value is $x_{\\mathrm{opt}} = x_{\\mathrm{crit}} = \\frac{1}{\\sqrt{bc}}$.\n\nCase 2: $x_{\\mathrm{crit}} > 2$. This condition is equivalent to $\\frac{1}{\\sqrt{bc}} > 2$, or $bc  \\frac{1}{4}$. In this case, the global minimum occurs at a value of $x$ that is larger than the allowed maximum of $2$. On the interval $(0, 2]$, the derivative $\\frac{dJ}{dx} = S_{0} \\Delta t (bc - \\frac{1}{x^2})$ is always negative, since $x^2 \\le 4 \\implies \\frac{1}{x^2} \\ge \\frac{1}{4} > bc$. Because the function is strictly decreasing on $(0, 2]$, its minimum value on this interval is attained at the right boundary, $x_{\\mathrm{opt}} = 2$.\n\nWe can express these two cases compactly using the minimum function. The optimal value for $x$ on the interval $(0, 2]$ is the smaller of the unconstrained minimum and the upper boundary of the interval:\n$$\nx_{\\mathrm{opt}} = \\min\\left(2, \\frac{1}{\\sqrt{bc}}\\right)\n$$\nFinally, we translate this result back into an expression for the optimal damping parameter, $\\gamma_{\\mathrm{opt}}$. Using the relation $x_{\\mathrm{opt}} = \\gamma_{\\mathrm{opt}} \\Delta t$, we find:\n$$\n\\gamma_{\\mathrm{opt}} = \\frac{x_{\\mathrm{opt}}}{\\Delta t} = \\frac{1}{\\Delta t} \\min\\left(2, \\frac{1}{\\sqrt{bc}}\\right)\n$$\nThis is the closed-form expression for the optimal damping parameter in terms of the given symbols.", "answer": "$$\n\\boxed{\\frac{1}{\\Delta t} \\min\\left(2, \\frac{1}{\\sqrt{bc}}\\right)}\n$$", "id": "3469179"}]}