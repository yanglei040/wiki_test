{"hands_on_practices": [{"introduction": "To effectively use Besov space priors in computational settings, we must translate their abstract definition from functional analysis into a concrete, computable penalty on coefficients. This practice establishes the fundamental connection between the $B_{1,1}^s$ Besov space quasi-norm and a weighted $\\ell^1$ norm of a function's wavelet coefficients. By explicitly deriving the scale-dependent weights, you will gain a foundational understanding of how these priors are constructed and why they take their specific form in optimization problems [@problem_id:3367773].", "problem": "Let $\\mathbb{T}^{d}$ denote the $d$-dimensional torus. Consider an orthonormal wavelet system on $\\mathbb{T}^{d}$ obtained by periodizing a compactly supported multiresolution analysis with $r$-regularity and at least $M$ vanishing moments, where $r  s$ and $M  s$ with $s \\in \\mathbb{R}$. Let $\\{\\psi_{j,k}^{m}\\}_{j \\geq 0,\\, k \\in \\{0,\\dots,2^{j}-1\\}^{d},\\, m \\in \\{1,\\dots,2^{d}-1\\}}$ denote the wavelets at scale $j$, position $k$, and type $m$, together with a finite collection of scaling functions at the coarsest scale. For $f \\in \\mathcal{D}'(\\mathbb{T}^{d})$, define the wavelet coefficients $c_{j,k}^{m} = \\langle f, \\psi_{j,k}^{m} \\rangle$, where $\\langle \\cdot,\\cdot \\rangle$ denotes the $L^{2}(\\mathbb{T}^{d})$ duality pairing.\n\nThe Besov space $B_{1,1}^{s}(\\mathbb{T}^{d})$ can be defined via the dyadic Littlewood–Paley decomposition $\\{ \\Delta_{j} \\}_{j \\geq 0}$ as the set of all $f \\in \\mathcal{D}'(\\mathbb{T}^{d})$ for which $\\sum_{j \\geq 0} 2^{j s} \\| \\Delta_{j} f \\|_{L^{1}(\\mathbb{T}^{d})}$ is finite, up to equivalence of quasi-norms. You may use the following well-tested facts about compactly supported orthonormal wavelets on $\\mathbb{T}^{d}$:\n- For each fixed $j$, the collection $\\{\\psi_{j,k}^{m}\\}_{k,m}$ forms an unconditional basis for the subspace of functions with frequencies essentially concentrated at dyadic scale $2^{j}$, and there exist constants $A_{p},B_{p} \\in (0,\\infty)$ independent of $j$ such that for every finitely supported coefficient array $\\{a_{k}^{m}\\}_{k,m}$,\n$$\nA_{p} \\, 2^{j d\\left(\\frac{1}{2}-\\frac{1}{p}\\right)} \\left\\| a \\right\\|_{\\ell^{p}} \\le \\left\\| \\sum_{k,m} a_{k}^{m} \\psi_{j,k}^{m} \\right\\|_{L^{p}(\\mathbb{T}^{d})} \\le B_{p} \\, 2^{j d\\left(\\frac{1}{2}-\\frac{1}{p}\\right)} \\left\\| a \\right\\|_{\\ell^{p}} \\quad \\text{for } p \\in [1,\\infty].\n$$\n- The Littlewood–Paley projectors $\\Delta_{j}$ can be realized (up to uniformly bounded invertible operators at each fixed scale) by the projection onto the span of $\\{\\psi_{j,k}^{m}\\}_{k,m}$.\n\nAssume throughout that $f$ has mean zero, so that the finite collection of scaling coefficients can be ignored without loss in the quasi-norm equivalence. Show that the $B_{1,1}^{s}(\\mathbb{T}^{d})$ quasi-norm of $f$ is equivalent to a weighted $\\ell^{1}$ norm of its wavelet coefficients of the form\n$$\n\\sum_{j \\geq 0} w_{j} \\sum_{k,m} |c_{j,k}^{m}|,\n$$\nand compute explicitly the weights $w_{j}$ in terms of $j$, $s$, and $d$. Your final answer must be a single closed-form analytic expression for $w_{j}$ as a function of $j$, $s$, and $d$ (no units).", "solution": "The objective is to demonstrate that the Besov quasi-norm $\\|f\\|_{B_{1,1}^{s}(\\mathbb{T}^{d})}$ is equivalent to a specific weighted $\\ell^{1}$ norm of the wavelet coefficients of $f$, and to determine the explicit form of the weights $w_{j}$.\n\nThe analysis begins with the definition of the Besov space $B_{1,1}^{s}(\\mathbb{T}^{d})$ quasi-norm, which is given in terms of the Littlewood–Paley decomposition $\\{\\Delta_{j}\\}_{j \\geq 0}$. For a function $f \\in \\mathcal{D}'(\\mathbb{T}^{d})$ with zero mean, this quasi-norm is defined as:\n$$\n\\|f\\|_{B_{1,1}^{s}(\\mathbb{T}^{d})} = \\sum_{j \\geq 0} 2^{j s} \\| \\Delta_{j} f \\|_{L^{1}(\\mathbb{T}^{d})}\n$$\nThe problem states that the Littlewood–Paley projectors $\\Delta_{j}$ can be realized by the projection of $f$ onto the span of the wavelets at scale $j$. Let us denote this projection operator by $P_{j}$. For a function $f$ with wavelet coefficients $c_{j,k}^{m} = \\langle f, \\psi_{j,k}^{m} \\rangle$, this projection is given by:\n$$\nP_{j}f = \\sum_{k \\in \\{0,\\dots,2^{j}-1\\}^{d}} \\sum_{m \\in \\{1,\\dots,2^{d}-1\\}} c_{j,k}^{m} \\psi_{j,k}^{m}\n$$\nThe problem states that $\\Delta_{j}$ is related to $P_{j}$ by uniformly bounded invertible operators. This implies that the norms of $\\Delta_{j}f$ and $P_{j}f$ are equivalent. That is, there exist constants $C_{1}, C_{2} \\in (0,\\infty)$, independent of $j$ and $f$, such that:\n$$\nC_{1} \\| P_{j}f \\|_{L^{1}(\\mathbb{T}^{d})} \\le \\| \\Delta_{j} f \\|_{L^{1}(\\mathbb{T}^{d})} \\le C_{2} \\| P_{j}f \\|_{L^{1}(\\mathbb{T}^{d})}\n$$\nSumming over $j$ and incorporating the $2^{js}$ factor, we find that the Besov quasi-norm is equivalent to a sum involving the norms of the wavelet projections:\n$$\n\\|f\\|_{B_{1,1}^{s}(\\mathbb{T}^{d})} \\approx \\sum_{j \\geq 0} 2^{j s} \\| P_{j}f \\|_{L^{1}(\\mathbb{T}^{d})}\n$$\nwhere $\\approx$ denotes equivalence of quasi-norms, meaning the ratio of the two sides is bounded above and below by positive constants.\n\nThe next step is to relate the $L^{1}(\\mathbb{T}^{d})$ norm of the wavelet projection $P_{j}f$ to the $\\ell^{1}$ norm of its corresponding coefficients $\\{c_{j,k}^{m}\\}$. The problem provides a crucial norm equivalence:\n$$\nA_{p} \\, 2^{j d\\left(\\frac{1}{2}-\\frac{1}{p}\\right)} \\left\\| a \\right\\|_{\\ell^{p}} \\le \\left\\| \\sum_{k,m} a_{k}^{m} \\psi_{j,k}^{m} \\right\\|_{L^{p}(\\mathbb{T}^{d})} \\le B_{p} \\, 2^{j d\\left(\\frac{1}{2}-\\frac{1}{p}\\right)} \\left\\| a \\right\\|_{\\ell^{p}}\n$$\nfor any finitely supported coefficient array $\\{a_{k}^{m}\\}$. This result extends to the infinite sequences of coefficients for functions in the appropriate spaces. We are interested in the $L^{1}$ norm, so we set $p=1$. The coefficients are $a_{k}^{m} = c_{j,k}^{m}$. The norm of the coefficient array is the $\\ell^{1}$ norm:\n$$\n\\|c_{j}\\|_{\\ell^{1}} = \\sum_{k,m} |c_{j,k}^{m}|\n$$\nSubstituting $p=1$ into the scaling factor exponent gives:\n$$\nj d\\left(\\frac{1}{2}-\\frac{1}{p}\\right) = j d\\left(\\frac{1}{2}-\\frac{1}{1}\\right) = j d\\left(-\\frac{1}{2}\\right) = -\\frac{jd}{2}\n$$\nApplying this to the norm equivalence for $P_{j}f = \\sum_{k,m} c_{j,k}^{m} \\psi_{j,k}^{m}$, we get:\n$$\nA_{1} \\, 2^{-jd/2} \\sum_{k,m} |c_{j,k}^{m}| \\le \\| P_{j}f \\|_{L^{1}(\\mathbb{T}^{d})} \\le B_{1} \\, 2^{-jd/2} \\sum_{k,m} |c_{j,k}^{m}|\n$$\nThis establishes the equivalence:\n$$\n\\| P_{j}f \\|_{L^{1}(\\mathbb{T}^{d})} \\approx 2^{-jd/2} \\sum_{k,m} |c_{j,k}^{m}|\n$$\nNow, we substitute this result back into the expression for the Besov quasi-norm:\n$$\n\\|f\\|_{B_{1,1}^{s}(\\mathbb{T}^{d})} \\approx \\sum_{j \\geq 0} 2^{j s} \\left( 2^{-jd/2} \\sum_{k,m} |c_{j,k}^{m}| \\right)\n$$\nCombining the exponential terms, we have:\n$$\n\\|f\\|_{B_{1,1}^{s}(\\mathbb{T}^{d})} \\approx \\sum_{j \\geq 0} 2^{js} 2^{-jd/2} \\sum_{k,m} |c_{j,k}^{m}| = \\sum_{j \\geq 0} 2^{j(s - d/2)} \\sum_{k,m} |c_{j,k}^{m}|\n$$\nThe problem asks for an expression of the form $\\sum_{j \\geq 0} w_{j} \\sum_{k,m} |c_{j,k}^{m}|$. By comparing this with our derived expression, we can directly identify the weights $w_{j}$.\n$$\nw_{j} = 2^{j(s - d/2)}\n$$\nThis shows that the $B_{1,1}^{s}(\\mathbb{T}^{d})$ quasi-norm is equivalent to the weighted $\\ell^{1}$ norm of the wavelet coefficients, with the weights given by the expression above. The conditions $rs$ and $Ms$ on the wavelet system are the standard requirements that ensure this characterization is valid.", "answer": "$$\n\\boxed{2^{j(s - \\frac{d}{2})}}\n$$", "id": "3367773"}, {"introduction": "The theoretical power of Besov priors lies in their flexibility, controlled by parameters $p$, $q$, and $s$. This hands-on coding exercise explores the practical impact of the scale-aggregation parameter $q$ by comparing reconstructions using $B^0_{2,1}$ and $B^0_{2,\\infty}$ priors. By implementing the corresponding MAP estimators and applying them to synthetic signals with features at different scales, you will develop an intuition for how $q$ governs the coupling of regularization strength across scales and influences the recovery of different signal structures [@problem_id:3367760].", "problem": "Consider a one-dimensional discrete inverse problem with identity forward model, emphasizing the role of multiscale regularization via Besov space priors. Let $N=256$ and let $u \\in \\mathbb{R}^{N}$ denote the unknown signal. Let $W:\\mathbb{R}^{N} \\to \\mathbb{R}^{N}$ be the orthonormal Haar wavelet transform that produces $J=\\log_{2}(N)$ scales of detail coefficients $\\{w_{j}\\}_{j=1}^{J}$ together with the final scaling coefficients, where $w_{j} \\in \\mathbb{R}^{N/2^{j}}$ denotes the detail coefficients at scale $j$ (with $j=1$ the finest scale). Define the discrete Besov semi-norm at smoothness $s=0$ and integrability exponent $p=2$ with scale-aggregator $q \\in \\{1,\\infty\\}$ as\n$$\n\\|u\\|_{B^{0}_{2,q}} \\;\\propto\\; \\left(\\sum_{j=1}^{J} \\left\\| w_{j} \\right\\|_{2}^{q}\\right)^{1/q}\n\\quad \\text{for } q=1,\n\\qquad\n\\|u\\|_{B^{0}_{2,\\infty}} \\;\\propto\\; \\max_{1 \\le j \\le J} \\left\\| w_{j} \\right\\|_{2},\n$$\nwhere $w = W u$ and $\\|\\cdot\\|_{2}$ denotes the Euclidean norm. Consider noisy observations $y \\in \\mathbb{R}^{N}$ generated by the model\n$$\ny \\;=\\; u_{\\text{true}} \\;+\\; \\eta, \\quad \\eta \\sim \\mathcal{N}(0, \\sigma^{2} I_{N}),\n$$\nwith $\\sigma = 0.05$. For a given regularization strength $\\lambda = 200$, define the Maximum A Posteriori (MAP) estimator for each $q \\in \\{1,\\infty\\}$ as the solution to the convex optimization problem\n$$\n\\widehat{u}_{q} \\in \\arg\\min_{u \\in \\mathbb{R}^{N}} \\; \\frac{1}{2\\sigma^{2}} \\|u - y\\|_{2}^{2} \\;+\\; \\lambda \\left(\\sum_{j=1}^{J} \\| (W u)_{j} \\|_{2}^{q}\\right)^{1/q}\n\\quad \\text{for } q=1,\n$$\nand\n$$\n\\widehat{u}_{\\infty} \\in \\arg\\min_{u \\in \\mathbb{R}^{N}} \\; \\frac{1}{2\\sigma^{2}} \\|u - y\\|_{2}^{2} \\;+\\; \\lambda \\max_{1 \\le j \\le J} \\| (W u)_{j} \\|_{2}.\n$$\nAssume the Haar transform $W$ is exactly orthonormal and the penalty excludes the final scaling coefficients (only detail coefficients are penalized).\n\nConstruct a test suite of three synthetic signals $u_{\\text{true}}$ that probe inter-scale behavior:\n\n- Test case $1$ (two-scale mixture): Define a coarse step and fine spikes. Let $u_{\\text{true}}[n]=1$ for $n \\in \\{64,65,\\dots,191\\}$ and $u_{\\text{true}}[n]=0$ otherwise. Superimpose two fine spikes of amplitudes $0.7$ at index $120$ and $-0.5$ at index $200$.\n- Test case $2$ (coarse-only): The same coarse step as in test $1$, but without any spikes.\n- Test case $3$ (fine-only): No coarse step. Two fine spikes: amplitude $1.0$ at index $50$ and amplitude $-0.8$ at index $180$.\n\nFor all tests, draw independent noise $\\eta$ with standard deviation $\\sigma=0.05$ using a fixed random seed $r_{0}=12345$ for reproducibility, with a fresh noise realization for each test case.\n\nFor each test case, compute the following quantitative outputs to compare $q=1$ versus $q=\\infty$:\n\n1. Reconstruction error ratio $R = \\text{MSE}_{\\infty} / \\text{MSE}_{1}$, where $\\text{MSE}_{q} = \\| \\widehat{u}_{q} - u_{\\text{true}} \\|_{2}^{2}/N$.\n2. Inter-scale shrinkage spread ratio $D = \\Delta_{\\infty} / \\Delta_{1}$, where for each $q$ the spread $\\Delta_{q}$ is computed from the wavelet-domain shrinkage factors at each scale,\n$$\nr^{(q)}_{j} \\;=\\; \\frac{\\| \\widehat{w}^{(q)}_{j} \\|_{2}}{\\| w^{(y)}_{j} \\|_{2} + \\epsilon},\n\\quad \\epsilon = 10^{-12},\n$$\nwith $w^{(y)} = W y$, $\\widehat{w}^{(q)} = W \\widehat{u}_{q}$, and\n$$\n\\Delta_{q} \\;=\\; \\max_{j} r^{(q)}_{j} \\;-\\; \\min_{j} r^{(q)}_{j}.\n$$\n\nProgram requirements:\n\n- Implement the orthonormal Haar wavelet transform and its inverse for $N=256$ with $J=\\log_{2}(N)$ levels. Penalize only detail coefficients in the objective; do not penalize the final scaling coefficients.\n- Solve the MAP problems exactly to global optimality for both $q=1$ and $q=\\infty$ using convex analysis. You must not rely on any external data or input; fix $\\sigma=0.05$, $\\lambda=200$, $N=256$, and the random seed $r_{0}=12345$.\n- For $q=1$, $p=2$, and $s=0$, use the canonical grouping of detail coefficients by scale.\n- For $q=\\infty$, enforce the maximum over scales of the grouped $\\ell_{2}$ norms.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$$\n[ R_{1}, D_{1}, R_{2}, D_{2}, R_{3}, D_{3} ],\n$$\nwhere the subscript indicates the test case number. Express all six numbers as floating-point values. There are no physical units in this problem. Angles are not involved. Percentages are not used. The values should be printed with at least six significant digits of precision.", "solution": "The user-provided problem is a well-posed and scientifically grounded task in computational inverse problems. It asks for the comparison of two regularization schemes based on Besov space priors for signal denoising. The problem is valid as it is self-contained, mathematically consistent, and algorithmically solvable using established principles of convex optimization and wavelet analysis.\n\nThe core of the problem lies in solving two Maximum A Posteriori (MAP) estimation problems, which are formulated as convex optimization tasks. The unknown signal is denoted by $u \\in \\mathbb{R}^{N}$, and we are given noisy observations $y = u_{\\text{true}} + \\eta$, where $\\eta$ is Gaussian noise. The objective function to minimize for an estimate $\\widehat{u}$ is:\n$$\nJ(u) = \\frac{1}{2\\sigma^{2}} \\|u - y\\|_{2}^{2} + \\lambda \\mathcal{R}(Wu)\n$$\nHere, $\\|u - y\\|_{2}^{2}$ is the data fidelity term (log-likelihood), $\\mathcal{R}(Wu)$ is the regularization term (log-prior), $\\lambda$ is the regularization parameter, and $W$ is the orthonormal Haar wavelet transform.\n\nA key principle in solving this problem is the use of the orthonormal wavelet transform $W$. Since $W$ is an orthonormal operator ($W W^T = W^T W = I$), the Euclidean norm is invariant under this transform, i.e., $\\|v\\|_{2} = \\|Wv\\|_{2}$ for any vector $v$. By changing variables to the wavelet domain with $w = Wu$ and $w_y = Wy$, the data fidelity term becomes $\\|u - y\\|_{2}^{2} = \\|W^T w - W^T w_y\\|_{2}^{2} = \\|W^T(w - w_y)\\|_{2}^{2} = \\|w - w_y\\|_{2}^{2}$. The optimization problem can then be solved more easily in the wavelet domain:\n$$\n\\widehat{w} \\in \\arg\\min_{w \\in \\mathbb{R}^{N}} \\; \\frac{1}{2\\sigma^{2}} \\|w - w_y\\|_{2}^{2} + \\lambda \\mathcal{R}(w)\n$$\nThe estimated signal in the original domain is recovered via the inverse wavelet transform, $\\widehat{u} = W^T \\widehat{w}$.\n\nThe wavelet coefficients $w$ are composed of a single scaling coefficient $w_c$ and $J=\\log_2(N)$ sets of detail coefficients $\\{w_j\\}_{j=1}^{J}$. The problem specifies that the regularization term $\\mathcal{R}(w)$ penalizes only the detail coefficients. This structure allows for decoupling the estimation of the scaling and detail coefficients.\n- The scaling coefficient $w_c$ is unpenalized, so its estimate is simply the one that minimizes the data fidelity term: $\\widehat{w}_c = w_{y,c}$.\n- The detail coefficients are estimated by solving the remaining part of the optimization problem.\n\nThe two cases for the parameter $q$ in the Besov prior $\\|u\\|_{B^0_{2,q}}$ define different structures for the penalty $\\mathcal{R}(w)$, leading to distinct solutions.\n\n**Case 1: $q=1$ (Group Sparsity Prior)**\nThe regularization term is $\\mathcal{R}(w) = \\sum_{j=1}^{J} \\|w_j\\|_2$. The optimization problem for the detail coefficients becomes:\n$$\n\\min_{\\{w_j\\}_{j=1}^J} \\sum_{j=1}^{J} \\left( \\frac{1}{2\\sigma^2} \\|w_j - w_{y,j}\\|_2^2 + \\lambda \\|w_j\\|_2 \\right)\n$$\nThis problem is fully separable, meaning we can solve for each vector of detail coefficients $w_j$ independently for each scale $j$:\n$$\n\\widehat{w}_j = \\arg\\min_{w_j} \\frac{1}{2\\sigma^2} \\|w_j - w_{y,j}\\|_2^2 + \\lambda \\|w_j\\|_2\n$$\nThis is a standard problem whose solution is given by the group soft-thresholding operator. The solution for each group $w_j$ is:\n$$\n\\widehat{w}_{1,j} = \\left(1 - \\frac{\\lambda \\sigma^2}{\\|w_{y,j}\\|_2}\\right)_+ w_{y,j}\n$$\nwhere $(x)_+ = \\max(x, 0)$. This operator shrinks the entire vector $w_{y,j}$ towards zero, setting it to zero if its norm is below the threshold $\\alpha = \\lambda \\sigma^2$.\n\n**Case 2: $q=\\infty$ (Uniform Shrinkage Prior)**\nThe regularization term is $\\mathcal{R}(w) = \\max_{1 \\le j \\le J} \\|w_j\\|_2$. The optimization problem is:\n$$\n\\min_{\\{w_j\\}_{j=1}^J} \\frac{1}{2\\sigma^2} \\sum_{j=1}^{J} \\|w_j - w_{y,j}\\|_2^2 + \\lambda \\max_{1 \\le j \\le J} \\|w_j\\|_2\n$$\nThis problem does not separate by scale. It can be formulated as a constrained optimization problem by introducing an auxiliary variable $t = \\max_j \\|w_j\\|_2$:\n$$\n\\min_{\\{w_j\\}, t \\ge 0} \\frac{1}{2\\sigma^2} \\sum_{j=1}^{J} \\|w_j - w_{y,j}\\|_2^2 + \\lambda t \\quad \\text{subject to} \\quad \\|w_j\\|_2 \\le t \\quad \\forall j=1,\\dots,J.\n$$\nFor a fixed $t$, the optimal $w_j$ is the projection of $w_{y,j}$ onto the Euclidean ball of radius $t$. Substituting this back into the objective yields a one-dimensional convex optimization problem in $t$. The derivative of this objective with respect to $t$ gives the condition $\\sum_{j=1}^J (\\|w_{y,j}\\|_2 - t)_+ = \\lambda \\sigma^2$. This equation for the optimal threshold $t_{opt}$ can be solved efficiently. The function $h(t) = \\sum_{j=1}^J (\\|w_{y,j}\\|_2 - t)_+ - \\lambda \\sigma^2$ is continuous and monotonically decreasing, so its root can be found using a bisection search. Once $t_{opt}$ is found, the estimated detail coefficients are given by:\n$$\n\\widehat{w}_{\\infty,j} = \\min\\left(1, \\frac{t_{opt}}{\\|w_{y,j}\\|_2}\\right) w_{y,j}\n$$\nThis operator provides a uniform upper bound $t_{opt}$ on the norm of the estimated detail coefficients across all scales.\n\nThe implementation will proceed by first defining the orthonormal Haar wavelet transform and its inverse. Then, for each test case, noisy data is generated, and the two MAP problems are solved in the wavelet domain using the methods derived above. Finally, the required quantitative metrics ($R$ and $D$) are computed from the results.", "answer": "```python\nimport numpy as np\n\ndef haar_dwt(x):\n    \"\"\"\n    Computes the orthonormal Haar wavelet transform of a 1D signal.\n    \n    Args:\n        x (np.ndarray): Input signal of length 2^k.\n    \n    Returns:\n        tuple: A tuple containing:\n            - np.ndarray: The final scaling coefficient (length 1).\n            - list[np.ndarray]: A list of detail coefficients, ordered from\n              finest scale (j=1) to coarsest (j=J).\n    \"\"\"\n    level = int(np.log2(len(x)))\n    a = np.copy(x).astype(float)\n    detail_coeffs_by_scale = []\n    for _ in range(level):\n        d = (a[0::2] - a[1::2]) / np.sqrt(2)\n        a = (a[0::2] + a[1::2]) / np.sqrt(2)\n        detail_coeffs_by_scale.append(d)\n    return a, detail_coeffs_by_scale\n\ndef haar_idwt(scaling_coeffs, detail_coeffs_by_scale):\n    \"\"\"\n    Computes the inverse orthonormal Haar wavelet transform.\n    \n    Args:\n        scaling_coeffs (np.ndarray): The final scaling coefficient.\n        detail_coeffs_by_scale (list[np.ndarray]): List of detail coefficients\n            from finest to coarsest scale.\n            \n    Returns:\n        np.ndarray: The reconstructed signal.\n    \"\"\"\n    level = len(detail_coeffs_by_scale)\n    a = np.copy(scaling_coeffs).astype(float)\n    for i in range(level - 1, -1, -1):\n        d = detail_coeffs_by_scale[i]\n        signal_len = len(d)\n        new_a = np.zeros(2 * signal_len, dtype=float)\n        a_plus_d = (a + d) / np.sqrt(2)\n        a_minus_d = (a - d) / np.sqrt(2)\n        new_a[0::2] = a_plus_d\n        new_a[1::2] = a_minus_d\n        a = new_a\n    return a\n\ndef find_t_opt_for_q_inf(w_y_d_list, alpha):\n    \"\"\"\n    Finds the optimal threshold t for the q=inf case by solving\n    sum_j (||w_{y,j}|| - t)_+ = alpha using bisection.\n    \"\"\"\n    z_norms = np.array([np.linalg.norm(d) for d in w_y_d_list])\n\n    def h(t, norms, alpha_val):\n        return np.sum(np.maximum(0, norms - t)) - alpha_val\n\n    if h(0, z_norms, alpha) = 0:\n        return 0.0\n\n    low_t = 0.0\n    # A safe upper bound for t is the maximum norm, as h(t) would be = 0.\n    high_t = np.max(z_norms) \n    \n    for _ in range(100): # 100 iterations for high precision\n        mid_t = (low_t + high_t) / 2.0\n        if mid_t == low_t or mid_t == high_t: # Converged\n            break\n        if h(mid_t, z_norms, alpha) > 0:\n            low_t = mid_t\n        else:\n            high_t = mid_t\n            \n    return (low_t + high_t) / 2.0\n\ndef solve():\n    \"\"\"\n    Main solver function to run the specified analysis.\n    \"\"\"\n    N = 256\n    sigma = 0.05\n    lambda_reg = 200.0\n    epsilon = 1e-12\n    J = int(np.log2(N))\n\n    # Test cases setup\n    u_true_cases = []\n    # Case 1: two-scale mixture\n    u1 = np.zeros(N, dtype=float)\n    u1[64:192] = 1.0\n    u1[120] += 0.7\n    u1[200] += -0.5\n    u_true_cases.append(u1)\n    # Case 2: coarse-only\n    u2 = np.zeros(N, dtype=float)\n    u2[64:192] = 1.0\n    u_true_cases.append(u2)\n    # Case 3: fine-only\n    u3 = np.zeros(N, dtype=float)\n    u3[50] = 1.0\n    u3[180] = -0.8\n    u_true_cases.append(u3)\n\n    rng = np.random.default_rng(12345)\n    final_results = []\n    \n    for u_true in u_true_cases:\n        noise = rng.normal(loc=0.0, scale=sigma, size=N)\n        y = u_true + noise\n\n        # Decompose noisy signal into wavelet coefficients\n        w_y_c, w_y_d_list = haar_dwt(y)\n        \n        # --- Solve for q=1 ---\n        alpha = lambda_reg * sigma**2\n        w_hat_1_d_list = []\n        for w_y_j in w_y_d_list:\n            norm_w_y_j = np.linalg.norm(w_y_j)\n            shrinkage = max(0.0, 1.0 - alpha / (norm_w_y_j + epsilon))\n            w_hat_1_d_list.append(shrinkage * w_y_j)\n        u_hat_1 = haar_idwt(w_y_c, w_hat_1_d_list)\n\n        # --- Solve for q=inf ---\n        t_opt = find_t_opt_for_q_inf(w_y_d_list, alpha)\n        w_hat_inf_d_list = []\n        for w_y_j in w_y_d_list:\n            norm_w_y_j = np.linalg.norm(w_y_j)\n            shrinkage = min(1.0, t_opt / (norm_w_y_j + epsilon))\n            w_hat_inf_d_list.append(shrinkage * w_y_j)\n        u_hat_inf = haar_idwt(w_y_c, w_hat_inf_d_list)\n\n        # --- Calculate metrics ---\n        # 1. Reconstruction error ratio R\n        mse_1 = np.mean((u_hat_1 - u_true)**2)\n        mse_inf = np.mean((u_hat_inf - u_true)**2)\n        R = mse_inf / (mse_1 + epsilon)\n\n        # 2. Inter-scale shrinkage spread ratio D\n        r1_j_vals = []\n        rinf_j_vals = []\n        for j in range(J):\n            norm_wyj = np.linalg.norm(w_y_d_list[j])\n            \n            norm_w1j_hat = np.linalg.norm(w_hat_1_d_list[j])\n            r1_j_vals.append(norm_w1j_hat / (norm_wyj + epsilon))\n\n            norm_winfj_hat = np.linalg.norm(w_hat_inf_d_list[j])\n            rinf_j_vals.append(norm_winfj_hat / (norm_wyj + epsilon))\n\n        delta_1 = np.max(r1_j_vals) - np.min(r1_j_vals)\n        delta_inf = np.max(rinf_j_vals) - np.min(rinf_j_vals)\n        D = delta_inf / (delta_1 + epsilon)\n        \n        final_results.extend([R, D])\n\n    # Format the final output string exactly as required\n    output_str = \",\".join([f\"{val:.10f}\" for val in final_results])\n    print(f\"[{output_str}]\")\n\nsolve()\n\n```", "id": "3367760"}, {"introduction": "Applying sparsity-promoting priors in large-scale systems like geophysical data assimilation requires integrating them into sophisticated optimization algorithms. This exercise challenges you to derive a core algorithmic step for a Four-Dimensional Variational (4D-Var) data assimilation problem that includes a weighted $\\ell^1$ penalty on wavelet coefficients. Successfully deriving the proximal-gradient update demonstrates how to combine the principles of convex optimization with the structure of a complex, state-of-the-art estimation framework to promote sparse solutions [@problem_id:3367744].", "problem": "Consider a Four-Dimensional Variational (4D-Var) data assimilation problem posed for an initial condition $x \\in \\mathbb{R}^{n}$ over an assimilation window with observation times $\\{t_{i}\\}_{i=1}^{m}$. Let the linearized model propagator from the initial time to time $t_{i}$ be $M_{0 \\to t_{i}} \\in \\mathbb{R}^{n \\times n}$ and the linearized observation operator at time $t_{i}$ be $H_{t_{i}} \\in \\mathbb{R}^{p_{i} \\times n}$. Observations at time $t_{i}$ are $y_{t_{i}} \\in \\mathbb{R}^{p_{i}}$ with positive definite observation-error covariance $R_{t_{i}} \\in \\mathbb{R}^{p_{i} \\times p_{i}}$. A Gaussian background (prior) for the initial condition is given with mean $x_{b} \\in \\mathbb{R}^{n}$ and positive definite covariance $B \\in \\mathbb{R}^{n \\times n}$. In addition, assume a sparsity-promoting Besov-space prior on wavelet coefficients: let $W \\in \\mathbb{R}^{n \\times n}$ be an orthonormal wavelet transform matrix and $w \\in \\mathbb{R}^{n}$ a vector of positive weights, and consider the weighted $\\ell^{1}$ penalty $\\lambda \\sum_{j=1}^{n} w_{j} |(W x)_{j}|$ with $\\lambda  0$.\n\nThe incremental 4D-Var objective for the initial condition is the sum of a smooth data-misfit plus background term and a non-smooth weighted $\\ell^{1}$ wavelet penalty:\n$$\nJ(x) \\;=\\; \\frac{1}{2}\\,(x - x_{b})^{\\top} B^{-1} (x - x_{b}) \\;+\\; \\frac{1}{2} \\sum_{i=1}^{m} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr)^{\\top} R_{t_{i}}^{-1} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr) \\;+\\; \\lambda \\sum_{j=1}^{n} w_{j} \\bigl|(W x)_{j}\\bigr| .\n$$\n\nStarting from first principles of maximum a posteriori estimation under Gaussian likelihoods, quadratic background, and a separable weighted $\\ell^{1}$ penalty, derive a single explicit analytic expression for one proximal-gradient Gauss–Newton update $x^{(k+1)}$ from a current iterate $x^{(k)}$. Use the Gauss–Newton linearization across the window (i.e., treat $M_{0 \\to t_{i}}$ and $H_{t_{i}}$ as fixed linear operators at iteration $k$) so that the gradient of the smooth part has a Lipschitz constant bounded by the spectral norm of the Gauss–Newton matrix. Denote by $\\alpha_{k}  0$ a stepsize chosen as the reciprocal of any valid global upper bound on this Lipschitz constant at iteration $k$. Express your final update in terms of $B$, $x_{b}$, $\\{M_{0 \\to t_{i}}, H_{t_{i}}, R_{t_{i}}, y_{t_{i}}\\}_{i=1}^{m}$, $\\lambda$, $w$, and $W$, and use the componentwise soft-thresholding induced by the weighted $\\ell^{1}$ penalty and the orthonormality of $W$.\n\nYour final answer must be a single closed-form analytic expression for $x^{(k+1)}$. Do not provide inequalities or intermediate equations. Do not include any units. If you introduce any auxiliary notation in your derivation, it must be fully eliminated in the final expression.", "solution": "The problem requires the derivation of a single update step for an incremental Four-Dimensional Variational (4D-Var) data assimilation problem that incorporates a sparsity-promoting Besov-space prior. The update is to be performed using a proximal-gradient method.\n\nFirst, we validate the problem statement.\nThe givens are:\n- State vector $x \\in \\mathbb{R}^{n}$.\n- Assimilation window with observation times $\\{t_{i}\\}_{i=1}^{m}$.\n- Linearized model propagator $M_{0 \\to t_{i}} \\in \\mathbb{R}^{n \\times n}$.\n- Linearized observation operator $H_{t_{i}} \\in \\mathbb{R}^{p_{i} \\times n}$.\n- Observation vector $y_{t_{i}} \\in \\mathbb{R}^{p_{i}}$.\n- Positive definite observation-error covariance matrix $R_{t_{i}} \\in \\mathbb{R}^{p_{i} \\times p_{i}}$.\n- Background (prior) mean state $x_{b} \\in \\mathbb{R}^{n}$.\n- Positive definite background-error covariance matrix $B \\in \\mathbb{R}^{n \\times n}$.\n- Orthonormal wavelet transform matrix $W \\in \\mathbb{R}^{n \\times n}$.\n- Vector of positive weights $w \\in \\mathbb{R}^{n}$.\n- Regularization parameter $\\lambda  0$.\n- The objective function to be minimized:\n$$\nJ(x) \\;=\\; \\frac{1}{2}\\,(x - x_{b})^{\\top} B^{-1} (x - x_{b}) \\;+\\; \\frac{1}{2} \\sum_{i=1}^{m} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr)^{\\top} R_{t_{i}}^{-1} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr) \\;+\\; \\lambda \\sum_{j=1}^{n} w_{j} \\bigl|(W x)_{j}\\bigr| .\n$$\n- An iterate $x^{(k)}$ and a stepsize $\\alpha_k$.\n\nThe problem is scientifically grounded, representing a standard formulation in advanced data assimilation. It is well-posed, as the objective function $J(x)$ is strictly convex (due to the positive definite matrix $B$) and thus has a unique minimizer. All terms are mathematically and physically consistent. Therefore, the problem is valid.\n\nThe objective function $J(x)$ can be interpreted from a Maximum A Posteriori (MAP) estimation perspective. The solution $x$ that minimizes $J(x)$ is the state that maximizes the posterior probability density conditioned on the observations. The posterior probability $p(x|y)$ is given by Bayes' theorem as $p(x|y) \\propto p(y|x) p(x)$, where $p(y|x)$ is the likelihood and $p(x)$ is the prior probability of the state. Minimizing $J(x)$ is equivalent to minimizing the negative logarithm of the posterior probability. The individual terms correspond to:\n$1$. The term $\\frac{1}{2} \\sum_{i=1}^{m} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr)^{\\top} R_{t_{i}}^{-1} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr)$\nis the negative log-likelihood, assuming the observation errors are independent Gaussian random variables with mean zero and covariance $R_{t_{i}}$.\n$2$. The term $\\frac{1}{2}\\,(x - x_{b})^{\\top} B^{-1} (x - x_{b})$ is the negative logarithm of a Gaussian prior on the state $x$, with mean $x_b$ and covariance $B$.\n$3$. The term $\\lambda \\sum_{j=1}^{n} w_{j} \\bigl|(W x)_{j}\\bigr|$ is the negative logarithm of a separable Laplace (or exponential) prior on the wavelet coefficients $(Wx)_j$, which promotes sparsity.\n\nThe objective function has the composite structure $J(x) = f(x) + g(x)$, where $f(x)$ is a smooth, differentiable, convex function and $g(x)$ is a convex, non-differentiable function.\nThe smooth part is:\n$$\nf(x) = \\frac{1}{2}\\,(x - x_{b})^{\\top} B^{-1} (x - x_{b}) + \\frac{1}{2} \\sum_{i=1}^{m} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr)^{\\top} R_{t_{i}}^{-1} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr).\n$$\nThe non-smooth part is the weighted $\\ell^{1}$ penalty:\n$$\ng(x) = \\lambda \\sum_{j=1}^{n} w_{j} \\bigl|(W x)_{j}\\bigr| = \\lambda \\| \\text{diag}(w) W x \\|_1.\n$$\nA proximal-gradient method is suitable for minimizing such a composite function. The update from an iterate $x^{(k)}$ to $x^{(k+1)}$ is given by:\n$$\nx^{(k+1)} = \\text{prox}_{\\alpha_k g}\\left(x^{(k)} - \\alpha_k \\nabla f(x^{(k)})\\right),\n$$\nwhere $\\alpha_k$ is the stepsize, and $\\text{prox}_{\\alpha_k g}$ is the proximal operator of the function $\\alpha_k g$.\n\nFirst, we compute the gradient of the smooth part, $\\nabla f(x)$.\nThe gradient of the background term is $\\nabla_x \\left( \\frac{1}{2}(x - x_b)^{\\top}B^{-1}(x - x_b) \\right) = B^{-1}(x - x_b)$.\nThe gradient of the observation term is $\\nabla_x \\left( \\frac{1}{2} \\sum_{i=1}^{m} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr)^{\\top} R_{t_{i}}^{-1} \\bigl(H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}\\bigr) \\right) = \\sum_{i=1}^{m} (H_{t_{i}} M_{0 \\to t_{i}})^{\\top} R_{t_{i}}^{-1} (H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}})$.\nCombining these, the full gradient of $f(x)$ is:\n$$\n\\nabla f(x) = B^{-1}(x - x_b) + \\sum_{i=1}^{m} M_{0 \\to t_{i}}^{\\top} H_{t_{i}}^{\\top} R_{t_{i}}^{-1} (H_{t_{i}} M_{0 \\to t_{i}} x - y_{t_{i}}).\n$$\nThe problem states that the stepsize $\\alpha_k$ is chosen based on the Lipschitz constant of $\\nabla f(x)$, which is given by the spectral norm of the Hessian $\\nabla^2 f(x)$. Since $f(x)$ is quadratic in $x$, its Hessian is constant:\n$$\n\\nabla^2 f(x) = B^{-1} + \\sum_{i=1}^{m} M_{0 \\to t_{i}}^{\\top} H_{t_{i}}^{\\top} R_{t_{i}}^{-1} H_{t_{i}} M_{0 \\to t_{i}}.\n$$\nThis is the Gauss-Newton matrix mentioned in the problem. The stepsize $\\alpha_k$ is the reciprocal of an upper bound on its spectral norm.\n\nNext, we derive the proximal operator, $\\text{prox}_{\\alpha_k g}(z)$, which is defined as:\n$$\n\\text{prox}_{\\alpha_k g}(z) = \\underset{u \\in \\mathbb{R}^n}{\\arg\\min} \\left( \\alpha_k g(u) + \\frac{1}{2} \\|u - z\\|_2^2 \\right).\n$$\nSubstituting $g(u) = \\lambda \\sum_{j=1}^{n} w_{j} |(W u)_{j}|$, the minimization becomes:\n$$\n\\underset{u \\in \\mathbb{R}^n}{\\arg\\min} \\left( \\alpha_k \\lambda \\sum_{j=1}^{n} w_{j} |(W u)_{j}| + \\frac{1}{2} \\|u - z\\|_2^2 \\right).\n$$\nWe use the property that the wavelet transform matrix $W$ is orthonormal ($W^{\\top}W = WW^{\\top} = I$). This implies that $W$ is a Euclidean isometry, i.e., $\\|v\\|_2 = \\|Wv\\|_2$ for any vector $v$.\nLet $\\theta = Wu$. Then $u = W^{\\top}\\theta$. The term $\\|u - z\\|_2^2$ can be rewritten as $\\|W(u-z)\\|_2^2 = \\|Wu - Wz\\|_2^2 = \\|\\theta - Wz\\|_2^2$.\nThe minimization problem can be rephrased in terms of the wavelet coefficients $\\theta$:\n$$\n\\underset{\\theta \\in \\mathbb{R}^n}{\\arg\\min} \\left( \\alpha_k \\lambda \\sum_{j=1}^{n} w_{j} |\\theta_{j}| + \\frac{1}{2} \\|\\theta - Wz\\|_2^2 \\right).\n$$\nThis problem is separable, meaning we can solve it for each component $\\theta_j$ independently:\n$$\n\\underset{\\theta_j \\in \\mathbb{R}}{\\arg\\min} \\left( \\alpha_k \\lambda w_{j} |\\theta_{j}| + \\frac{1}{2} (\\theta_j - (Wz)_j)^2 \\right).\n$$\nThe solution to this scalar problem is given by the soft-thresholding operator, which we denote by $S$. The optimal $\\theta_j$ is:\n$$\n\\theta_j^* = S_{\\alpha_k \\lambda w_j}((Wz)_j) = \\text{sign}((Wz)_j) \\max(0, |(Wz)_j| - \\alpha_k \\lambda w_j).\n$$\nIn vector form, the solution for $\\theta$ is $\\theta^* = S_{\\alpha_k \\lambda w}(Wz)$, where the operator $S$ is applied component-wise with the threshold vector $\\alpha_k \\lambda w$.\nTo find the solution for $u$, we transform back: $u^* = W^{\\top}\\theta^* = W^{\\top}S_{\\alpha_k \\lambda w}(Wz)$.\nSo, the proximal operator is:\n$$\n\\text{prox}_{\\alpha_k g}(z) = W^{\\top}S_{\\alpha_k \\lambda w}(Wz).\n$$\nNow, we can write the complete proximal-gradient update step. The argument to the proximal operator is $z = x^{(k)} - \\alpha_k \\nabla f(x^{(k)})$.\nSubstituting the expression for $\\nabla f(x^{(k)})$, we get:\n$$\nz = x^{(k)} - \\alpha_k \\left( B^{-1}(x^{(k)} - x_b) + \\sum_{i=1}^{m} M_{0 \\to t_{i}}^{\\top} H_{t_{i}}^{\\top} R_{t_{i}}^{-1} (H_{t_{i}} M_{0 \\to t_{i}} x^{(k)} - y_{t_{i}}) \\right).\n$$\nThe update for $x^{(k+1)}$ is then $\\text{prox}_{\\alpha_k g}(z)$:\n$$\nx^{(k+1)} = W^{\\top}S_{\\alpha_k \\lambda w}\\left( W \\left[ x^{(k)} - \\alpha_k \\left( B^{-1}(x^{(k)} - x_b) + \\sum_{i=1}^{m} M_{0 \\to t_{i}}^{\\top} H_{t_{i}}^{\\top} R_{t_{i}}^{-1} (H_{t_{i}} M_{0 \\to t_{i}} x^{(k)} - y_{t_{i}}) \\right) \\right] \\right).\n$$\nThis is the single explicit analytic expression for one proximal-gradient update $x^{(k+1)}$ from the current iterate $x^{(k)}$, satisfying all conditions of the problem statement. The soft-thresholding operator $S_{\\tau}(v)$ is understood as the component-wise function $(S_{\\tau}(v))_j = \\text{sign}(v_j) \\max(0, |v_j|-\\tau_j)$.", "answer": "$$\n\\boxed{\nx^{(k+1)} = W^{\\top} S_{\\alpha_k \\lambda w}\\left( W \\left( x^{(k)} - \\alpha_k \\left[ B^{-1}(x^{(k)} - x_b) + \\sum_{i=1}^{m} M_{0 \\to t_i}^{\\top} H_{t_i}^{\\top} R_{t_i}^{-1} (H_{t_i} M_{0 \\to t_i} x^{(k)} - y_{t_i}) \\right] \\right) \\right)\n}\n$$", "id": "3367744"}]}