{"hands_on_practices": [{"introduction": "A Gaussian prior is often defined by its mean and a covariance matrix $C$. But what happens when $C$ is singular, meaning it is positive semidefinite but not invertible? This scenario is not just a mathematical curiosity; it arises when prior knowledge perfectly constrains the state in certain directions. This practice [@problem_id:3385468] challenges you to explore the profound implications of a singular prior, moving beyond the standard density formula to understand the underlying geometry and the conditions required to obtain a well-posed posterior distribution.", "problem": "Consider a finite-dimensional inverse problem in which the unknown state vector $x \\in \\mathbb{R}^{n}$ is assigned a Gaussian prior with mean $m \\in \\mathbb{R}^{n}$ and covariance matrix $C \\in \\mathbb{R}^{n \\times n}$. Assume $C$ is symmetric, positive semidefinite, and has rank $r < n$. Observations $y \\in \\mathbb{R}^{p}$ are obtained through a linear forward operator $H \\in \\mathbb{R}^{p \\times n}$ and additive noise $\\varepsilon \\in \\mathbb{R}^{p}$, where $\\varepsilon$ is Gaussian with mean $0$ and covariance $R \\in \\mathbb{R}^{p \\times p}$ that is symmetric positive definite. \n\nStarting from the foundational representation of a Gaussian as a linear image of a standard normal, analyze the implications of $C$ being positive semidefinite but not full rank for the existence and nature of the prior as a probability measure on $\\mathbb{R}^{n}$ and for the existence of a proper posterior density given the linear Gaussian likelihood. Your analysis should employ the following bases:\n- The definition that a multivariate Gaussian can be represented as $x = m + A z$, where $z \\in \\mathbb{R}^{n}$ has independent standard normal components, and $A \\in \\mathbb{R}^{n \\times n}$ satisfies $C = A A^{\\top}$.\n- The interpretation of properness of a prior or posterior with respect to the Lebesgue measure on $\\mathbb{R}^{n}$ via absolute continuity and existence of a Radon–Nikodym derivative.\n- The characterization of the posterior as minimizing a quadratic objective constructed from the prior and likelihood, and the requirement of strict convexity for uniqueness and properness.\n\nWhich of the following statements are correct?\n\nA. If $C$ is positive semidefinite but singular, the prior is a degenerate Gaussian probability measure supported on the affine subspace $m + \\operatorname{range}(C)$ and does not admit a density with respect to the Lebesgue measure on $\\mathbb{R}^{n}$. For a linear Gaussian likelihood $y = H x + \\varepsilon$ with $R \\succ 0$, the posterior admits a Lebesgue density (is proper) if and only if $\\ker(C) \\cap \\ker(H) = \\{0\\}$, equivalently if the symmetric matrix $C^{\\dagger} + H^{\\top} R^{-1} H$ is positive definite, where $C^{\\dagger}$ is the Moore–Penrose pseudoinverse of $C$.\n\nB. If $C$ is singular, the Gaussian prior is not a probability measure (its total mass is not $1$) and must be regularized by replacing $C$ with $C + \\epsilon I$ for some $\\epsilon > 0$ to become proper.\n\nC. Because $R \\succ 0$, the posterior is always proper regardless of $H$ and $C$; positivity of $R$ guarantees that $H^{\\top} R^{-1} H$ is positive definite on $\\mathbb{R}^{n}$.\n\nD. A singular $C$ implies infinite variance in directions of $\\ker(C)$, so the prior is heavy-tailed but nonetheless has a Lebesgue density on $\\mathbb{R}^{n}$.\n\nE. The posterior mean exists and is unique only if $H$ has full column rank; whether the posterior covariance is proper does not depend on $C$.", "solution": "The user has requested a detailed analysis of a Bayesian inverse problem involving a singular prior covariance matrix.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   State vector: $x \\in \\mathbb{R}^{n}$.\n-   Prior distribution for $x$: Gaussian with mean $m \\in \\mathbb{R}^{n}$ and covariance $C \\in \\mathbb{R}^{n \\times n}$.\n-   Properties of $C$: Symmetric, positive semidefinite, $\\operatorname{rank}(C) = r < n$ (i.e., $C$ is singular).\n-   Observation vector: $y \\in \\mathbb{R}^{p}$.\n-   Observation model: $y = H x + \\varepsilon$, with a linear forward operator $H \\in \\mathbb{R}^{p \\times n}$.\n-   Noise distribution for $\\varepsilon$: Gaussian with mean $0$ and covariance $R \\in \\mathbb{R}^{p \\times p}$.\n-   Properties of $R$: Symmetric positive definite ($R \\succ 0$).\n-   Prescribed analysis frameworks:\n    1.  Representation $x = m + A z$ with $z \\sim \\mathcal{N}(0, I)$ and $C = A A^{\\top}$.\n    2.  Properness defined as absolute continuity with respect to the Lebesgue measure on $\\mathbb{R}^{n}$ (existence of a Radon-Nikodym derivative).\n    3.  Characterization of the posterior via minimization of a quadratic objective, linking properness to strict convexity.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem is a standard, albeit advanced, topic in Bayesian statistics, inverse problems, and data assimilation. All concepts (Gaussian distributions, singular covariance, Moore-Penrose pseudoinverse, properness) are well-defined and mathematically sound.\n-   **Well-Posed:** The problem asks for an analysis of the consequences of a singular prior and to evaluate given statements. This is a clear and solvable task.\n-   **Objective:** The language is technical and unambiguous.\n-   **Completeness and Consistency:** The problem provides all necessary information. The instruction to use the quadratic objective minimization framework provides a clear path for analysis, resolving potential ambiguities in defining the posterior. The problem is internally consistent and does not contain contradictory constraints.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The analysis will proceed by deriving the solution from the given principles.\n\n### Derivation\n\nThe analysis proceeds in two parts: characterizing the prior distribution and then the posterior distribution.\n\n**1. Analysis of the Prior Distribution**\n\nA Gaussian distribution $\\mathcal{N}(m, C)$ is defined for any symmetric positive semidefinite covariance matrix $C$. The corresponding characteristic function is $\\phi_x(t) = \\exp(i t^\\top m - \\frac{1}{2} t^\\top C t)$, which always defines a valid probability measure.\n\nThe covariance matrix $C$ is singular with $\\operatorname{rank}(C) = r < n$. Let the eigendecomposition of $C$ be $C = U \\Lambda U^\\top$, where $U$ is an orthogonal matrix of eigenvectors and $\\Lambda = \\operatorname{diag}(\\lambda_1, \\dots, \\lambda_r, 0, \\dots, 0)$ is the diagonal matrix of eigenvalues, with $\\lambda_i > 0$ for $i=1, \\dots, r$.\n\nA random vector $x \\sim \\mathcal{N}(m, C)$ can be generated as $x = m + U \\Lambda^{1/2} z$, where $z \\sim \\mathcal{N}(0, I_n)$.\nThe vector $x-m = U \\Lambda^{1/2} z$ is a linear transformation of the standard normal vector $z$. The support of the distribution of $x-m$ is the range of this linear map, which is $\\operatorname{range}(U \\Lambda^{1/2})$. This space is spanned by the first $r$ columns of $U$, which are the eigenvectors corresponding to the positive eigenvalues of $C$. This subspace is precisely the range of the covariance matrix itself, $\\operatorname{range}(C)$.\n\nTherefore, the random vector $x$ is constrained to lie on the affine subspace $m + \\operatorname{range}(C)$. This is an $r$-dimensional subspace of $\\mathbb{R}^n$. Since $r < n$, this subspace has a Lebesgue measure of zero in $\\mathbb{R}^n$. A probability measure concentrated on a set of measure zero cannot be absolutely continuous with respect to the Lebesgue measure on the larger space. Consequently, it does not admit a probability density function (Radon-Nikodym derivative) on $\\mathbb{R}^n$. Such a distribution is called a degenerate Gaussian. It is a valid probability measure, but it is not \"proper\" in the sense defined by the problem.\n\n**2. Analysis of the Posterior Distribution**\n\nThe posterior combines information from the prior and the likelihood. The likelihood of the observation $y$ given $x$ is derived from the noise model $\\varepsilon = y - Hx \\sim \\mathcal{N}(0, R)$. Since $R \\succ 0$, the likelihood has a density on $\\mathbb{R}^p$ proportional to $\\exp(-\\frac{1}{2} (y-Hx)^\\top R^{-1} (y-Hx))$.\n\nThe problem directs us to characterize the posterior by minimizing a quadratic objective function $J(x)$. For Gaussian distributions, the negative logarithm of the posterior density is proportional to $J(x)$. The standard form of $J(x)$ is:\n$$J(x) = \\frac{1}{2} (y-Hx)^\\top R^{-1} (y-Hx) + \\frac{1}{2} (x-m)^\\top C^{-1} (x-m)$$\nThis formula presumes $C$ is invertible. For a singular $C$, the prior term must be generalized. The standard generalization replaces $C^{-1}$ with the Moore-Penrose pseudoinverse $C^\\dagger$, yielding:\n$$J(x) = \\frac{1}{2} (y-Hx)^\\top R^{-1} (y-Hx) + \\frac{1}{2} (x-m)^\\top C^\\dagger (x-m)$$\nThis objective function is to be minimized over $x \\in \\mathbb{R}^n$. The posterior distribution is then formally taken to be a Gaussian whose density is proportional to $\\exp(-J(x))$. The posterior is a proper Gaussian on $\\mathbb{R}^n$ if and only if $J(x)$ is a strictly convex quadratic function, which guarantees that $\\exp(-J(x))$ is integrable over $\\mathbb{R}^n$.\n\nStrict convexity of $J(x)$ is determined by its Hessian matrix being positive definite. Let's find the Hessian:\n$J(x) = \\frac{1}{2} (x^\\top H^\\top R^{-1} H x - 2y^\\top R^{-1} H x + y^\\top R^{-1}y) + \\frac{1}{2} (x^\\top C^\\dagger x - 2m^\\top C^\\dagger x + m^\\top C^\\dagger m)$\nThis is a quadratic in $x$. The Hessian is the matrix of the quadratic term:\n$$\\nabla^2_x J(x) = C^\\dagger + H^\\top R^{-1} H$$\nThis matrix is the posterior precision matrix. For the posterior to be proper on $\\mathbb{R}^n$, this matrix must be symmetric and positive definite.\n\nLet's analyze the term $C^\\dagger + H^\\top R^{-1} H$.\n-   Since $C$ is symmetric positive semidefinite (SPSD), its pseudoinverse $C^\\dagger$ is also SPSD. The null space of $C^\\dagger$ is identical to the null space of $C$, i.e., $\\ker(C^\\dagger) = \\ker(C)$. For a vector $v \\in \\mathbb{R}^n$, $v^\\top C^\\dagger v \\ge 0$, with equality if and only if $v \\in \\ker(C)$.\n-   Since $R$ is positive definite ($R \\succ 0$), its inverse $R^{-1}$ is also positive definite. The matrix $H^\\top R^{-1} H$ is SPSD. For a vector $v \\in \\mathbb{R}^n$, $v^\\top (H^\\top R^{-1} H) v = (Hv)^\\top R^{-1} (Hv) \\ge 0$. Equality holds if and only if $Hv=0$, which means $v \\in \\ker(H)$. So, $\\ker(H^\\top R^{-1} H) = \\ker(H)$.\n-   The sum of two SPSD matrices, $A+B$, is positive definite if and only if there is no non-zero vector $v$ such that $v^\\top A v = 0$ and $v^\\top B v = 0$ simultaneously. This is equivalent to $\\ker(A) \\cap \\ker(B) = \\{0\\}$.\n\nApplying this to the posterior precision matrix, $C^\\dagger + H^\\top R^{-1} H$ is positive definite if and only if $\\ker(C^\\dagger) \\cap \\ker(H^\\top R^{-1} H) = \\{0\\}$.\nThis simplifies to:\n$$\\ker(C) \\cap \\ker(H) = \\{0\\}$$\nThis condition means that any direction for which the prior provides no information (i.e., vectors in the null space of $C$) must be a direction that is \"seen\" by the observations (i.e., not in the null space of $H$).\n\n### Evaluation of Options\n\n**A. If $C$ is positive semidefinite but singular, the prior is a degenerate Gaussian probability measure supported on the affine subspace $m + \\operatorname{range}(C)$ and does not admit a density with respect to the Lebesgue measure on $\\mathbb{R}^{n}$. For a linear Gaussian likelihood $y = H x + \\varepsilon$ with $R \\succ 0$, the posterior admits a Lebesgue density (is proper) if and only if $\\ker(C) \\cap \\ker(H) = \\{0\\}$, equivalently if the symmetric matrix $C^{\\dagger} + H^{\\top} R^{-1} H$ is positive definite, where $C^{\\dagger}$ is the Moore–Penrose pseudoinverse of $C$.**\n-   The first part of the statement, concerning the nature of the prior, is rigorously correct as shown in our analysis.\n-   The second part, concerning the posterior, is also correct under the analytical framework prescribed by the problem (minimization of the generalized quadratic objective). The condition for the posterior precision matrix $C^\\dagger + H^\\top R^{-1} H$ to be positive definite is precisely $\\ker(C) \\cap \\ker(H) = \\{0\\}$. The positive definiteness of this matrix ensures the resulting posterior Gaussian is non-degenerate and has a proper density on $\\mathbb{R}^n$.\n-   **Verdict: Correct.**\n\n**B. If $C$ is singular, the Gaussian prior is not a probability measure (its total mass is not $1$) and must be regularized by replacing $C$ with $C + \\epsilon I$ for some $\\epsilon > 0$ to become proper.**\n-   This is incorrect. A Gaussian distribution with a singular covariance matrix is a valid probability measure with a total mass of $1$. It is simply a degenerate measure. Regularization is a technique for other purposes, not to make a valid measure into one.\n-   **Verdict: Incorrect.**\n\n**C. Because $R \\succ 0$, the posterior is always proper regardless of $H$ and $C$; positivity of $R$ guarantees that $H^{\\top} R^{-1} H$ is positive definite on $\\mathbb{R}^{n}$.**\n-   This is incorrect. While $R \\succ 0$, the matrix $H^\\top R^{-1} H$ is generally only positive semidefinite. Its null space is $\\ker(H)$, which is non-trivial unless $H$ has full column rank. Therefore, the posterior properness is not guaranteed and depends on the interplay between $\\ker(C)$ and $\\ker(H)$.\n-   **Verdict: Incorrect.**\n\n**D. A singular $C$ implies infinite variance in directions of $\\ker(C)$, so the prior is heavy-tailed but nonetheless has a Lebesgue density on $\\mathbb{R}^{n}$.**\n-   This is incorrect on multiple counts. A singular $C$ implies *zero* variance in the directions of $\\ker(C)$. A vector $v \\in \\ker(C)$ corresponds to an eigenvalue of $0$, so the variance of the projection $v^\\top x$ is $v^\\top C v = 0$. This is the opposite of infinite variance. Gaussian distributions are not heavy-tailed. As established, a singular prior does not have a Lebesgue density on $\\mathbb{R}^n$.\n-   **Verdict: Incorrect.**\n\n**E. The posterior mean exists and is unique only if $H$ has full column rank; whether the posterior covariance is proper does not depend on $C$.**\n-   This is incorrect. The posterior mean's uniqueness is tied to the positive definiteness of the posterior precision $C^\\dagger + H^\\top R^{-1} H$, which requires $\\ker(C) \\cap \\ker(H) = \\{0\\}$. This condition can be met even if $H$ does not have full column rank (e.g., if $C$ is nonsingular, i.e., $\\ker(C)=\\{0\\}$). The condition for properness explicitly involves $\\ker(C)$, so it absolutely depends on $C$.\n-   **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3385468"}, {"introduction": "In Bayesian modeling, choosing the prior is a critical step, but how can we be sure our choice is appropriate for the given data? This exercise [@problem_id:3385456] introduces the powerful technique of Empirical Bayes, where hyperparameters of the prior are estimated by maximizing the marginal likelihood, or \"evidence,\" of the data. You will derive, from first principles, the condition for the optimal covariance inflation factor in a linear-Gaussian model, providing a data-driven method for tuning your prior assumptions.", "problem": "Consider a linear inverse problem in data assimilation with a Gaussian prior and a Gaussian likelihood. Let the unknown state be $x \\in \\mathbb{R}^{n}$ with prior $x \\sim \\mathcal{N}(m, \\lambda C)$, where $m \\in \\mathbb{R}^{n}$ is known, $C \\in \\mathbb{R}^{n \\times n}$ is a known symmetric positive definite matrix, and $\\lambda \\in \\mathbb{R}_{+}$ is an unknown inflation factor to be determined. Observations $y \\in \\mathbb{R}^{p}$ are related to $x$ by the linear model $y = H x + \\varepsilon$, where $H \\in \\mathbb{R}^{p \\times n}$ is known and $\\varepsilon \\sim \\mathcal{N}(0, R)$ with a known symmetric positive definite matrix $R \\in \\mathbb{R}^{p \\times p}$. The goal is to select the inflation factor $\\lambda$ by maximizing the marginal likelihood (also called the evidence) $p(y \\mid \\lambda)$.\n\nStarting from the definitions of multivariate Gaussian probability density functions and Bayes’ theorem, and using only standard matrix calculus identities that follow from these definitions, derive from first principles the analytic stationarity condition satisfied by the maximizing $\\lambda$ obtained by differentiating $\\ln p(y \\mid \\lambda)$ with respect to $\\lambda$ and setting the derivative to zero. Then, specialize your result to the scalar case $n=p=1$ with parameters $m = m_{0}$, $C = c_{0}$, $H = h$, and $R = r$, and show that the maximizing $\\lambda$ admits a closed-form expression in terms of $y$, $m_{0}$, $c_{0}$, $h$, and $r$.\n\nFinally, evaluate this expression for the concrete scalar instance with $m_{0} = 0.3$, $c_{0} = 0.2$, $h = 1.5$, $r = 0.25$, and observed value $y = 3.0$, and report the resulting $\\lambda$.\n\nRound your answer to four significant figures. The final answer must be a single real number with no units.", "solution": "The user-provided problem is assessed to be valid. It is scientifically grounded within the standard framework of Bayesian inference for linear inverse problems, is well-posed with a clear objective, and contains all necessary information without any contradictions or ambiguities.\n\nWe begin by defining the components of the Bayesian model. The state variable is $x \\in \\mathbb{R}^{n}$. The prior distribution for $x$ is Gaussian, parameterized by the inflation factor $\\lambda \\in \\mathbb{R}_{+}$:\n$$x \\mid \\lambda \\sim \\mathcal{N}(m, \\lambda C)$$\nwhere $m \\in \\mathbb{R}^{n}$ is the prior mean and $\\lambda C$ is the prior covariance matrix, with $C \\in \\mathbb{R}^{n \\times n}$ being symmetric positive definite. The probability density function (PDF) of the prior is:\n$$p(x \\mid \\lambda) = \\frac{1}{\\sqrt{(2\\pi)^{n} \\det(\\lambda C)}} \\exp\\left(-\\frac{1}{2}(x-m)^T (\\lambda C)^{-1} (x-m)\\right)$$\n\nThe observations $y \\in \\mathbb{R}^{p}$ are related to the state $x$ through a linear model with additive Gaussian noise:\n$$y = H x + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, R)$$\nwhere $H \\in \\mathbb{R}^{p \\times n}$ is the observation operator and $R \\in \\mathbb{R}^{p \\times p}$ is the symmetric positive definite covariance matrix of the noise $\\varepsilon$. This defines the likelihood function, which is the conditional probability of the data $y$ given the state $x$:\n$$p(y \\mid x) = \\mathcal{N}(y \\mid Hx, R) = \\frac{1}{\\sqrt{(2\\pi)^{p} \\det(R)}} \\exp\\left(-\\frac{1}{2}(y-Hx)^T R^{-1} (y-Hx)\\right)$$\n\nThe goal is to find the value of $\\lambda$ that maximizes the marginal likelihood, or evidence, $p(y \\mid \\lambda)$. This can be found by integrating the joint distribution $p(y, x \\mid \\lambda) = p(y \\mid x) p(x \\mid \\lambda)$ over all possible states $x$:\n$$p(y \\mid \\lambda) = \\int_{\\mathbb{R}^n} p(y \\mid x) p(x \\mid \\lambda) dx$$\nSince this is a linear-Gaussian model, the marginal distribution of $y$ is also Gaussian. We can determine its parameters as follows:\nThe mean of $y$ is:\n$$E[y \\mid \\lambda] = E[Hx + \\varepsilon \\mid \\lambda] = H E[x \\mid \\lambda] + E[\\varepsilon] = Hm + 0 = Hm$$\nThe covariance of $y$, noting that $x$ and $\\varepsilon$ are independent, is:\n$$\\text{Cov}(y \\mid \\lambda) = \\text{Cov}(Hx + \\varepsilon) = \\text{Cov}(Hx) + \\text{Cov}(\\varepsilon) = H \\text{Cov}(x) H^T + R = H(\\lambda C)H^T + R = \\lambda HCH^T + R$$\nThus, the marginal likelihood is the PDF of a multivariate normal distribution:\n$$y \\mid \\lambda \\sim \\mathcal{N}(Hm, \\lambda HCH^T + R)$$\nLet's denote the covariance matrix of $y$ as $S(\\lambda) = \\lambda HCH^T + R$. The PDF is:\n$$p(y \\mid \\lambda) = \\frac{1}{\\sqrt{(2\\pi)^{p} \\det(S(\\lambda))}} \\exp\\left(-\\frac{1}{2}(y-Hm)^T S(\\lambda)^{-1} (y-Hm)\\right)$$\n\nTo maximize $p(y \\mid \\lambda)$, it is more convenient to maximize its logarithm, the log-marginal likelihood, denoted by $\\mathcal{L}(\\lambda)$:\n$$\\mathcal{L}(\\lambda) = \\ln p(y \\mid \\lambda) = -\\frac{p}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln \\det(S(\\lambda)) - \\frac{1}{2}(y-Hm)^T S(\\lambda)^{-1} (y-Hm)$$\nWe find the optimal $\\lambda$ by differentiating $\\mathcal{L}(\\lambda)$ with respect to $\\lambda$ and setting the result to zero. We use the following standard matrix calculus identities:\n1. $\\frac{\\partial}{\\partial \\lambda} \\ln \\det(S(\\lambda)) = \\text{tr}\\left(S(\\lambda)^{-1} \\frac{\\partial S(\\lambda)}{\\partial \\lambda}\\right)$\n2. $\\frac{\\partial}{\\partial \\lambda} S(\\lambda)^{-1} = -S(\\lambda)^{-1} \\frac{\\partial S(\\lambda)}{\\partial \\lambda} S(\\lambda)^{-1}$\n\nFirst, we find the derivative of $S(\\lambda)$:\n$$\\frac{\\partial S(\\lambda)}{\\partial \\lambda} = \\frac{\\partial}{\\partial \\lambda}(\\lambda HCH^T + R) = HCH^T$$\nNow we differentiate $\\mathcal{L}(\\lambda)$:\n$$\\frac{\\partial \\mathcal{L}(\\lambda)}{\\partial \\lambda} = 0 - \\frac{1}{2} \\frac{\\partial}{\\partial \\lambda}\\ln \\det(S(\\lambda)) - \\frac{1}{2}(y-Hm)^T \\left(\\frac{\\partial}{\\partial \\lambda}S(\\lambda)^{-1}\\right) (y-Hm)$$\nSubstituting the identities:\n$$\\frac{\\partial \\mathcal{L}(\\lambda)}{\\partial \\lambda} = -\\frac{1}{2} \\text{tr}\\left(S(\\lambda)^{-1} HCH^T\\right) - \\frac{1}{2}(y-Hm)^T \\left(-S(\\lambda)^{-1} (HCH^T) S(\\lambda)^{-1}\\right) (y-Hm)$$\n$$\\frac{\\partial \\mathcal{L}(\\lambda)}{\\partial \\lambda} = -\\frac{1}{2} \\text{tr}\\left(S(\\lambda)^{-1} HCH^T\\right) + \\frac{1}{2}(y-Hm)^T S(\\lambda)^{-1} HCH^T S(\\lambda)^{-1} (y-Hm)$$\nSetting the derivative to zero gives the stationarity condition:\n$$\\text{tr}\\left(S(\\lambda)^{-1} HCH^T\\right) = (y-Hm)^T S(\\lambda)^{-1} HCH^T S(\\lambda)^{-1} (y-Hm)$$\nSubstituting back $S(\\lambda) = \\lambda HCH^T + R$, we get the final general form of the condition:\n$$\\text{tr}\\left((\\lambda HCH^T + R)^{-1} HCH^T\\right) = (y-Hm)^T (\\lambda HCH^T + R)^{-1} HCH^T (\\lambda HCH^T + R)^{-1} (y-Hm)$$\n\nNext, we specialize this result to the scalar case where $n=p=1$. The variables and matrices become scalars:\n$x \\in \\mathbb{R}$, $y \\in \\mathbb{R}$, $m \\rightarrow m_0 \\in \\mathbb{R}$, $C \\rightarrow c_0 \\in \\mathbb{R}_{++}$, $H \\rightarrow h \\in \\mathbb{R}$, $R \\rightarrow r \\in \\mathbb{R}_{++}$.\nThe terms in the stationarity condition become:\n$HCH^T \\rightarrow h c_0 h = h^2 c_0$\n$S(\\lambda) \\rightarrow \\lambda h^2 c_0 + r$\n$y-Hm \\rightarrow y - h m_0$\nThe trace of a scalar is the scalar itself. The stationarity condition simplifies to:\n$$(\\lambda h^2 c_0 + r)^{-1} (h^2 c_0) = (y - h m_0) (\\lambda h^2 c_0 + r)^{-1} (h^2 c_0) (\\lambda h^2 c_0 + r)^{-1} (y - h m_0)$$\n$$\\frac{h^2 c_0}{\\lambda h^2 c_0 + r} = \\frac{(y - h m_0)^2 h^2 c_0}{(\\lambda h^2 c_0 + r)^2}$$\nAssuming $h \\neq 0$ and $c_0 > 0$, we can divide both sides by $h^2 c_0$:\n$$\\frac{1}{\\lambda h^2 c_0 + r} = \\frac{(y - h m_0)^2}{(\\lambda h^2 c_0 + r)^2}$$\nSince $\\lambda > 0$, $c_0 > 0$, $h^2 \\ge 0$, and $r > 0$, the denominator $\\lambda h^2 c_0 + r$ is strictly positive. We can multiply by $(\\lambda h^2 c_0 + r)^2$:\n$$\\lambda h^2 c_0 + r = (y - h m_0)^2$$\nSolving for $\\lambda$ yields the closed-form expression:\n$$\\lambda = \\frac{(y - h m_0)^2 - r}{h^2 c_0}$$\nThis solution for $\\lambda \\in \\mathbb{R}_{+}$ is valid provided that the numerator is positive, i.e., $(y - h m_0)^2 > r$.\n\nFinally, we evaluate this expression for the given numerical values: $m_{0} = 0.3$, $c_{0} = 0.2$, $h = 1.5$, $r = 0.25$, and $y = 3.0$.\nFirst, calculate the terms needed for the expression:\n$$h m_0 = 1.5 \\times 0.3 = 0.45$$\n$$y - h m_0 = 3.0 - 0.45 = 2.55$$\n$$(y - h m_0)^2 = (2.55)^2 = 6.5025$$\n$$h^2 c_0 = (1.5)^2 \\times 0.2 = 2.25 \\times 0.2 = 0.45$$\nThe condition for a positive $\\lambda$ is $(y - h m_0)^2 > r$, which is $6.5025 > 0.25$. This condition is satisfied.\nNow, substitute these values into the expression for $\\lambda$:\n$$\\lambda = \\frac{6.5025 - 0.25}{0.45} = \\frac{6.2525}{0.45} \\approx 13.89444...$$\nRounding the result to four significant figures gives $13.89$.", "answer": "$$\\boxed{13.89}$$", "id": "3385456"}, {"introduction": "Moving from analytical solutions to practical algorithms is a key skill in computational science. This exercise [@problem_id:3385486] takes the concept of Empirical Bayes from the previous practice and guides you through its full implementation. You will derive an elegant expression for the gradient of the log-evidence and then use it to build a robust numerical program that automatically finds the optimal scale for your prior covariance, bridging the gap between theory and code.", "problem": "Consider the linear Gaussian inverse problem defined by the following components. Let the unknown parameter vector be $x \\in \\mathbb{R}^n$, with a Gaussian prior $x \\sim \\mathcal{N}(0, C)$ where the covariance $C$ is scaled by a positive scalar parameter $\\tau$ via $C = \\tau^{-1} \\tilde{C}$. The matrix $\\tilde{C} \\in \\mathbb{R}^{n \\times n}$ is known and symmetric positive definite (SPD). Observations $y \\in \\mathbb{R}^m$ are generated by the linear model $y = A x + \\varepsilon$, where $A \\in \\mathbb{R}^{m \\times n}$ is known, and $\\varepsilon \\sim \\mathcal{N}(0, R)$ with $R \\in \\mathbb{R}^{m \\times m}$ known and SPD. The marginal likelihood (evidence) is $p(y \\mid \\tau)$, and the goal of Empirical Bayes (EB) is to choose $\\tau$ to maximize this evidence.\n\nStarting only from core definitions of Gaussian distributions, the linear model, and Bayes’ rule, derive the expression for the gradient of the log evidence with respect to the scale $\\tau$ in terms of the posterior mean $m(\\tau)$ and posterior covariance $\\Sigma(\\tau)$ of $x \\mid y, \\tau$. Then implement a program that:\n- Computes the posterior covariance $\\Sigma(\\tau)$ and posterior mean $m(\\tau)$ using fundamental Gaussian identities for linear models.\n- Computes the derivative $\\partial_{\\tau} \\log p(y \\mid \\tau)$ using the derived expression.\n- Finds the EB maximizer $\\tau^\\star$ by solving $\\partial_{\\tau} \\log p(y \\mid \\tau) = 0$ using a robust bracketed root-finding method over a positive interval.\n\nThe fundamental base you may use includes:\n- The Gaussian density definition and its normalization, including for transformed linear models.\n- Bayes’ rule and standard formulas for conditioning in multivariate Gaussians.\n- The identity for the marginal likelihood obtained by integrating out Gaussian latent variables.\n- The property known as Fisher’s identity: for a latent variable model with prior parameter $\\tau$, $\\partial_{\\tau} \\log p(y \\mid \\tau) = \\mathbb{E}_{x \\mid y, \\tau} \\left[ \\partial_{\\tau} \\log p(x \\mid \\tau) \\right]$.\n\nYour program must compute numerical results for the following test suite. In each case, let $n$ denote the dimension of $x$, $m$ denote the dimension of $y$, and define $A$, $R$, $\\tilde{C}$, $y$, and an evaluation point $\\tau_0 > 0$. For each case, return a pair of floating-point numbers: first, the value of $\\partial_{\\tau} \\log p(y \\mid \\tau)$ evaluated at $\\tau_0$, and second, the EB maximizer $\\tau^\\star$ solving $\\partial_{\\tau} \\log p(y \\mid \\tau) = 0$.\n\nTest Suite:\n1. Case 1 (happy path, moderate dimensions):\n   - $n = 2$, $m = 2$,\n   - $A = \\begin{bmatrix} 1.0 & 0.5 \\\\ 0.2 & 1.0 \\end{bmatrix}$,\n   - $R = \\operatorname{diag}(0.1^2, 0.15^2)$,\n   - $\\tilde{C} = \\begin{bmatrix} 1.0 & 0.2 \\\\ 0.2 & 2.0 \\end{bmatrix}$,\n   - $y = \\begin{bmatrix} 1.0 \\\\ -0.5 \\end{bmatrix}$,\n   - $\\tau_0 = 1.0$.\n2. Case 2 (correlated prior, larger $n$):\n   - $n = 3$, $m = 3$,\n   - $A = \\begin{bmatrix} 1.0 & -0.3 & 0.0 \\\\ -0.2 & 0.8 & 0.5 \\\\ 0.0 & 0.4 & 1.2 \\end{bmatrix}$,\n   - $R = \\operatorname{diag}(0.2^2, 0.25^2, 0.3^2)$,\n   - $\\tilde{C} = \\begin{bmatrix} 1.5 & 0.1 & 0.0 \\\\ 0.1 & 1.0 & 0.2 \\\\ 0.0 & 0.2 & 2.0 \\end{bmatrix}$,\n   - $y = \\begin{bmatrix} 0.8 \\\\ -1.0 \\\\ 0.3 \\end{bmatrix}$,\n   - $\\tau_0 = 0.8$.\n3. Case 3 (boundary, scalar problem):\n   - $n = 1$, $m = 1$,\n   - $A = \\begin{bmatrix} 1.7 \\end{bmatrix}$,\n   - $R = \\begin{bmatrix} 0.05^2 \\end{bmatrix}$,\n   - $\\tilde{C} = \\begin{bmatrix} 0.7 \\end{bmatrix}$,\n   - $y = \\begin{bmatrix} 0.25 \\end{bmatrix}$,\n   - $\\tau_0 = 0.5$.\n\nDefinitions to use for computation:\n- The prior precision is $C^{-1} = \\tau \\tilde{C}^{-1}$.\n- The data precision is $R^{-1}$.\n- The posterior precision is $\\Lambda(\\tau) = C^{-1} + A^\\top R^{-1} A$.\n- The posterior covariance is $\\Sigma(\\tau) = \\Lambda(\\tau)^{-1}$.\n- The posterior mean is $m(\\tau) = \\Sigma(\\tau) A^\\top R^{-1} y$.\n\nTarget derivation:\n- Derive $\\partial_{\\tau} \\log p(y \\mid \\tau)$ in terms of $n$, $\\tau$, $\\tilde{C}^{-1}$, $\\Sigma(\\tau)$, and $m(\\tau)$ only, without introducing any other auxiliary variables.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5,result6]\"). The six values correspond to the three test cases in order: for each case, first $\\partial_{\\tau} \\log p(y \\mid \\tau_0)$, then $\\tau^\\star$. All outputs must be real numbers (floats). No physical units or angle units are involved in this problem, and no percentages are required.", "solution": "The problem is deemed valid upon rigorous inspection. All provided components are scientifically and mathematically sound, self-contained, and well-posed. The problem statement provides a clear objective within the established framework of Empirical Bayes for linear Gaussian models. All definitions, constraints, and test cases are formally specified and consistent.\n\nThe task is to derive the gradient of the log-marginal-likelihood (log-evidence) with respect to a prior scale parameter $\\tau$, and then to implement a numerical procedure to find the value of $\\tau$ that maximizes this evidence.\n\n### 1. Theoretical Derivation of the Log-Evidence Gradient\n\nWe are asked to find $\\partial_{\\tau} \\log p(y \\mid \\tau)$. The problem suggests starting from Fisher's identity, which is given as:\n$$\n\\partial_{\\tau} \\log p(y \\mid \\tau) = \\mathbb{E}_{x \\mid y, \\tau} \\left[ \\partial_{\\tau} \\log p(x \\mid \\tau) \\right]\n$$\nThis identity allows us to compute the gradient of the marginal likelihood by taking the expectation of the gradient of the log-prior with respect to the posterior distribution.\n\nFirst, we write the expression for the log-prior, $\\log p(x \\mid \\tau)$. The prior on $x \\in \\mathbb{R}^n$ is a zero-mean Gaussian, $x \\sim \\mathcal{N}(0, C)$, with covariance $C = \\tau^{-1} \\tilde{C}$. The corresponding prior precision matrix is $C^{-1} = \\tau \\tilde{C}^{-1}$. The probability density function is:\n$$\np(x \\mid \\tau) = \\frac{1}{(2\\pi)^{n/2} (\\det(C))^{1/2}} \\exp\\left(-\\frac{1}{2} x^\\top C^{-1} x\\right)\n$$\nThe log-prior is therefore:\n$$\n\\log p(x \\mid \\tau) = -\\frac{1}{2} x^\\top C^{-1} x - \\frac{1}{2} \\log \\det(C) - \\frac{n}{2} \\log(2\\pi)\n$$\nWe substitute $C = \\tau^{-1} \\tilde{C}$ and $C^{-1} = \\tau \\tilde{C}^{-1}$. The determinant term becomes $\\log \\det(\\tau^{-1} \\tilde{C}) = \\log(\\tau^{-n} \\det(\\tilde{C})) = -n \\log \\tau + \\log \\det(\\tilde{C})$.\n$$\n\\log p(x \\mid \\tau) = -\\frac{1}{2} x^\\top (\\tau \\tilde{C}^{-1}) x - \\frac{1}{2} (-n \\log \\tau + \\log \\det(\\tilde{C})) - \\frac{n}{2} \\log(2\\pi)\n$$\n$$\n\\log p(x \\mid \\tau) = -\\frac{\\tau}{2} x^\\top \\tilde{C}^{-1} x + \\frac{n}{2} \\log \\tau - \\frac{1}{2}\\log\\det(\\tilde{C}) - \\frac{n}{2}\\log(2\\pi)\n$$\nNext, we differentiate the log-prior with respect to $\\tau$. The terms involving $\\tilde{C}$ and $2\\pi$ are constants with respect to $\\tau$.\n$$\n\\partial_{\\tau} \\log p(x \\mid \\tau) = \\frac{\\partial}{\\partial \\tau} \\left( -\\frac{\\tau}{2} x^\\top \\tilde{C}^{-1} x + \\frac{n}{2} \\log \\tau \\right) = -\\frac{1}{2} x^\\top \\tilde{C}^{-1} x + \\frac{n}{2\\tau}\n$$\nNow, we apply Fisher's identity by taking the expectation of this expression with respect to the posterior distribution $p(x \\mid y, \\tau) = \\mathcal{N}(x \\mid m(\\tau), \\Sigma(\\tau))$.\n$$\n\\partial_{\\tau} \\log p(y \\mid \\tau) = \\mathbb{E}_{x \\mid y, \\tau} \\left[ -\\frac{1}{2} x^\\top \\tilde{C}^{-1} x + \\frac{n}{2\\tau} \\right]\n$$\nUsing the linearity of expectation, we get:\n$$\n\\partial_{\\tau} \\log p(y \\mid \\tau) = -\\frac{1}{2} \\mathbb{E}_{x \\mid y, \\tau} \\left[ x^\\top \\tilde{C}^{-1} x \\right] + \\frac{n}{2\\tau}\n$$\nThe expectation of a quadratic form $z^\\top Q z$ for a random vector $z \\sim \\mathcal{N}(\\mu, \\Sigma_z)$ is given by the identity $\\mathbb{E}[z^\\top Q z] = \\operatorname{Tr}(Q \\Sigma_z) + \\mu^\\top Q \\mu$. In our case, the random variable is $x$ from the posterior distribution, so its mean is the posterior mean $m(\\tau)$ and its covariance is the posterior covariance $\\Sigma(\\tau)$. The matrix of the quadratic form is $Q = \\tilde{C}^{-1}$.\nApplying this identity yields:\n$$\n\\mathbb{E}_{x \\mid y, \\tau} \\left[ x^\\top \\tilde{C}^{-1} x \\right] = \\operatorname{Tr}(\\tilde{C}^{-1} \\Sigma(\\tau)) + m(\\tau)^\\top \\tilde{C}^{-1} m(\\tau)\n$$\nSubstituting this back into our expression for the gradient, we arrive at the final desired form:\n$$\n\\partial_{\\tau} \\log p(y \\mid \\tau) = \\frac{n}{2\\tau} - \\frac{1}{2} \\left( \\operatorname{Tr}(\\tilde{C}^{-1} \\Sigma(\\tau)) + m(\\tau)^\\top \\tilde{C}^{-1} m(\\tau) \\right)\n$$\nThis expression depends only on $n$, $\\tau$, $\\tilde{C}^{-1}$, and the posterior statistics $m(\\tau)$ and $\\Sigma(\\tau)$, as required.\n\n### 2. Algorithmic Design and Implementation\n\nThe problem requires a Python program to perform two tasks for each test case:\n1.  Compute the gradient $\\partial_{\\tau} \\log p(y \\mid \\tau)$ at a given point $\\tau_0$.\n2.  Find the optimal hyperparameter $\\tau^\\star > 0$ that maximizes the log-evidence by solving the equation $\\partial_{\\tau} \\log p(y \\mid \\tau) = 0$.\n\nTo implement this, we design a function that calculates the gradient for any given $\\tau > 0$. This function requires the posterior mean $m(\\tau)$ and covariance $\\Sigma(\\tau)$, which are computed using the provided standard formulas for Gaussian linear models:\n- Posterior Precision: $\\Lambda(\\tau) = C^{-1} + A^\\top R^{-1} A = \\tau \\tilde{C}^{-1} + A^\\top R^{-1} A$.\n- Posterior Covariance: $\\Sigma(\\tau) = \\Lambda(\\tau)^{-1}$.\n- Posterior Mean: $m(\\tau) = \\Sigma(\\tau) A^\\top R^{-1} y$.\n\nThe numerical algorithm is as follows:\n1.  For a given test case, pre-compute the matrix inverses $\\tilde{C}^{-1}$ and $R^{-1}$.\n2.  Define a function, `gradient(tau)`, that takes a scalar `tau` and returns the value of $\\partial_{\\tau} \\log p(y \\mid \\tau)$.\n    a. Inside this function, compute $\\Lambda(\\tau)$, $\\Sigma(\\tau)$, and $m(\\tau)$. This involves matrix multiplication, addition, and inversion.\n    b. Use these posterior statistics to compute the three terms of the derived gradient formula: $\\frac{n}{2\\tau}$, $\\operatorname{Tr}(\\tilde{C}^{-1} \\Sigma(\\tau))$, and $m(\\tau)^\\top \\tilde{C}^{-1} m(\\tau)$.\n    c. Combine these terms to return the final gradient value.\n3.  Evaluate this `gradient` function at the specified $\\tau_0$ to obtain the first required result for the test case.\n4.  To find the root $\\tau^\\star$, we use a robust numerical method. The function $g(\\tau) = \\partial_{\\tau} \\log p(y \\mid \\tau)$ has the property that $g(\\tau) \\to +\\infty$ as $\\tau \\to 0^+$. For a typical, well-behaved problem, the log-evidence is concave or unimodal, meaning the gradient will cross zero from positive to negative exactly once for $\\tau > 0$. This structure is ideal for a bracketed root-finding algorithm like Brent's method, available in `scipy.optimize.brentq`.\n5.  To use `brentq`, we need to find a bracket $[a, b]$ such that $g(a) \\cdot g(b) < 0$.\n    a. We select $a$ as a small positive number (e.g., $10^{-8}$), where we expect $g(a) > 0$.\n    b. We find $b$ using an expanding search. We start with a guess, e.g., $b=1.0$, and repeatedly multiply it by a factor (e.g., $2$) until $g(b)$ becomes negative. This ensures a valid bracket for the root-finder.\n6.  Call `scipy.optimize.brentq(gradient, a, b)` to find $\\tau^\\star$.\n7.  The procedure is repeated for all test cases, and the results are collected and formatted as specified.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Solves the Empirical Bayes problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"n\": 2, \"m\": 2,\n            \"A\": np.array([[1.0, 0.5], [0.2, 1.0]]),\n            \"R\": np.diag([0.1**2, 0.15**2]),\n            \"C_tilde\": np.array([[1.0, 0.2], [0.2, 2.0]]),\n            \"y\": np.array([1.0, -0.5]),\n            \"tau_0\": 1.0\n        },\n        {\n            \"n\": 3, \"m\": 3,\n            \"A\": np.array([[1.0, -0.3, 0.0], [-0.2, 0.8, 0.5], [0.0, 0.4, 1.2]]),\n            \"R\": np.diag([0.2**2, 0.25**2, 0.3**2]),\n            \"C_tilde\": np.array([[1.5, 0.1, 0.0], [0.1, 1.0, 0.2], [0.0, 0.2, 2.0]]),\n            \"y\": np.array([0.8, -1.0, 0.3]),\n            \"tau_0\": 0.8\n        },\n        {\n            \"n\": 1, \"m\": 1,\n            \"A\": np.array([[1.7]]),\n            \"R\": np.array([[0.05**2]]),\n            \"C_tilde\": np.array([[0.7]]),\n            \"y\": np.array([0.25]),\n            \"tau_0\": 0.5\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        n, A, R, C_tilde, y, tau_0 = case['n'], case['A'], case['R'], case['C_tilde'], case['y'], case['tau_0']\n        \n        # Pre-compute matrix inverses and other constant terms\n        C_tilde_inv = np.linalg.inv(C_tilde)\n        R_inv = np.linalg.inv(R)\n        At_Rinv = A.T @ R_inv\n        At_Rinv_A = At_Rinv @ A\n        At_Rinv_y = At_Rinv @ y\n\n        def get_grad_log_evidence(tau):\n            \"\"\"\n            Computes the derivative of the log evidence wrt tau.\n            d/d(tau) log p(y|tau) = n/(2*tau) - 0.5 * (Tr(C_tilde_inv * Sigma) + m.T @ C_tilde_inv @ m)\n            \"\"\"\n            if tau <= 0:\n                return np.inf\n\n            # Posterior precision\n            # Lambda = tau * C_tilde_inv + A.T @ R_inv @ A\n            Lambda = tau * C_tilde_inv + At_Rinv_A\n            \n            # Posterior covariance and mean\n            try:\n                # Use solve for better numerical stability than inv\n                Sigma = np.linalg.solve(Lambda, np.identity(n))\n                m = Sigma @ At_Rinv_y\n            except np.linalg.LinAlgError:\n                # If Lambda is singular, gradient is ill-defined.\n                return np.nan\n\n            # Compute the terms for the gradient expression\n            trace_term = np.trace(C_tilde_inv @ Sigma)\n            quadratic_term = m.T @ C_tilde_inv @ m\n            \n            grad = n / (2 * tau) - 0.5 * (trace_term + quadratic_term)\n            return grad\n\n        # 1. Compute the gradient at tau_0\n        grad_at_tau0 = get_grad_log_evidence(tau_0)\n        \n        # 2. Find the EB maximizer tau_star by finding the root of the gradient\n        # We need a bracket [a, b] such that g(a) > 0 and g(b)  0\n        a = 1e-8\n        \n        # Check sign at lower bound\n        g_a = get_grad_log_evidence(a)\n        if g_a = 0:\n            # This is unexpected as the gradient should go to +inf at tau=0+\n            # If it happens, we cannot find a bracket in the expected way.\n            # For this problem, we assume this path is not taken.\n            tau_star = np.nan\n        else:\n            # Find an upper bound b such that get_grad_log_evidence(b)  0\n            b = 1.0 if tau_0  1.0 else tau_0 * 2.0\n            \n            # Exponentially increase b until the sign changes\n            max_iter = 100\n            for _ in range(max_iter):\n                g_b = get_grad_log_evidence(b)\n                if g_b  0:\n                    break\n                b *= 2.0\n            else: # Loop finished without break\n                # Could not find an upper bracket where the function is negative\n                tau_star = np.nan\n            \n            if not np.isnan(g_b):\n                 tau_star = brentq(get_grad_log_evidence, a, b, xtol=1e-12, rtol=1e-12)\n            else:\n                 tau_star = np.nan\n\n        results.extend([grad_at_tau0, tau_star])\n\n    # Format the final output\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3385486"}]}