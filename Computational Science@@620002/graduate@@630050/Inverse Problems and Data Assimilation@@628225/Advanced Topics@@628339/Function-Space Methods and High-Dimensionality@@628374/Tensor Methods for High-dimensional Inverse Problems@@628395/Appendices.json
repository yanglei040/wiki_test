{"hands_on_practices": [{"introduction": "A cornerstone of applying tensor methods to high-dimensional problems is the ability to efficiently represent the operators involved. This first exercise explores how a linear operator with a separable structure—specifically, a Kronecker product of smaller matrices—can be expressed in the Tensor Train (TT) format. By constructing the TT representation directly, you will uncover the remarkable data compression that is possible for this important class of operators [@problem_id:3424557].", "problem": "Consider a linear forward operator in a separable, high-dimensional inverse problem, written as the Kronecker product $A = A_1 \\otimes \\cdots \\otimes A_d$, where each $A_k \\in \\mathbb{R}^{m_k \\times n_k}$. Such operators arise in Gaussian Bayesian data assimilation with separable priors and likelihoods and in partial differential equation discretizations on tensor-product grids. You will show that this operator admits a representation in Tensor Train (TT) matrix format and quantify its TT ranks using only core definitions.\n\nStart from the following fundamental bases:\n- The definition of the Kronecker product: for $B \\in \\mathbb{R}^{p \\times q}$ and $C \\in \\mathbb{R}^{r \\times s}$, the Kronecker product $B \\otimes C \\in \\mathbb{R}^{pr \\times qs}$ satisfies $(B \\otimes C)_{(i-1)r+\\alpha,(j-1)s+\\beta} = B_{ij} C_{\\alpha \\beta}$.\n- The definition of a Tensor Train matrix (TT-matrix): a matrix $A \\in \\mathbb{R}^{(m_1 \\cdots m_d) \\times (n_1 \\cdots n_d)}$ is in TT-matrix format if there exist four-way cores $G^{(k)} \\in \\mathbb{R}^{r_{k-1} \\times m_k \\times n_k \\times r_k}$ with $r_0 = r_d = 1$ such that, for multi-indices $i = (i_1,\\dots,i_d)$ and $j = (j_1,\\dots,j_d)$,\n$$\nA_{i,j} \\;=\\; \\sum_{\\alpha_0,\\dots,\\alpha_d} G^{(1)}_{\\alpha_0,i_1,j_1,\\alpha_1} \\cdots G^{(d)}_{\\alpha_{d-1},i_d,j_d,\\alpha_d},\n$$\nwhere the sum is over $\\alpha_0 = 1$ and $\\alpha_d = 1$, and $r_k$ are the TT ranks.\n\nTasks:\n1. Using only these definitions and standard properties of matrix rank and Kronecker products, construct a TT-matrix representation for $A = A_1 \\otimes \\cdots \\otimes A_d$, and prove an a priori bound on each TT rank $r_s$ in terms of the classical matrix ranks of the factors $A_k$. Your argument must not assume any pre-existing TT representations of the factors and must proceed by identifying $A$ with a $2d$-way tensor and analyzing appropriate matricizations whose ranks equal the TT ranks.\n2. Specialize to the case where each $A_k$ is diagonal, i.e., $(A_k)_{i_k j_k} = a^{(k)}_{i_k} \\,\\delta_{i_k j_k}$ with diagonal entries $a^{(k)}_{1},\\dots,a^{(k)}_{n_k}$ and the Kronecker delta $\\delta_{i_k j_k}$. Compute the TT-matrix cores explicitly, in the sense of giving a closed-form expression for the nonzero core entries $G^{(k)}_{1,i_k,j_k,1}$ that reproduces $A$ exactly under the TT contraction.\n\nAnswer specification:\n- Provide your final answer as the single analytic expression for the TT core entries $G^{(k)}_{1,i_k,j_k,1}$ in the diagonal case, expressed in terms of $a^{(k)}_{i_k}$ and $\\delta_{i_k j_k}$.\n- No numerical rounding is required.\n- Do not include units.", "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, objective, and internally consistent. It poses a standard, verifiable problem in numerical linear algebra and tensor methods.\n\nThe problem is addressed in two parts as requested.\n\n**Part 1: General TT-Matrix Representation and Rank Bound**\n\nLet $A = A_1 \\otimes \\cdots \\otimes A_d$ be the forward operator, where each $A_k \\in \\mathbb{R}^{m_k \\times n_k}$. The matrix $A$ has dimensions $(\\prod_{k=1}^d m_k) \\times (\\prod_{k=1}^d n_k)$. An element of $A$ is indexed by a row multi-index $i = (i_1, \\dots, i_d)$ and a column multi-index $j = (j_1, \\dots, j_d)$, where $1 \\le i_k \\le m_k$ and $1 \\le j_k \\le n_k$. The definition of the Kronecker product implies that the element $A_{i,j}$ is given by the product of the corresponding elements of the factor matrices:\n$$\nA_{i,j} = \\prod_{k=1}^d (A_k)_{i_k, j_k}\n$$\nThe problem requires constructing a Tensor Train (TT) matrix representation for $A$ and finding a bound on its TT ranks. A matrix $A$ is in TT-matrix format if its elements can be expressed as\n$$\nA_{i,j} = G^{(1)}(i_1, j_1) G^{(2)}(i_2, j_2) \\cdots G^{(d)}(i_d, j_d)\n$$\nwhere $G^{(k)}(i_k, j_k)$ is a matrix of size $r_{k-1} \\times r_k$ with entries $(G^{(k)}(i_k, j_k))_{\\alpha_{k-1}, \\alpha_k} = G^{(k)}_{\\alpha_{k-1}, i_k, j_k, \\alpha_k}$. The scalars $r_k$ for $k=1, \\dots, d-1$ are the TT ranks, and the boundary ranks are fixed to $r_0=r_d=1$.\n\nThe $s$-th TT rank, $r_s$, of the matrix $A$ is defined as the rank of its $s$-th matricization, denoted $\\mathbf{A}_{(s)}$. This matricization is obtained by identifying the matrix $A$ with a $d$-th order tensor $\\mathcal{T}$ of size $(m_1 n_1) \\times \\dots \\times (m_d n_d)$ whose elements are $\\mathcal{T}_{(i_1,j_1), \\dots, (i_d,j_d)} = A_{i,j}$. The matrix $\\mathbf{A}_{(s)}$ is the unfolding of $\\mathcal{T}$ that groups the first $s$ modes against the remaining $d-s$ modes.\n\nThe matrix $\\mathbf{A}_{(s)}$ has dimensions $(\\prod_{k=1}^s m_k n_k) \\times (\\prod_{k=s+1}^d m_k n_k)$. Its rows are indexed by the multi-index $((i_1,j_1), \\dots, (i_s,j_s))$ and its columns by $((i_{s+1},j_{s+1}), \\dots, (i_d,j_d))$. An element of this matrix is given by:\n$$\n(\\mathbf{A}_{(s)})_{\\left((i_1,j_1), \\dots, (i_s,j_s)\\right), \\left((i_{s+1},j_{s+1}), \\dots, (i_d,j_d)\\right)} = A_{i,j} = \\left(\\prod_{k=1}^s (A_k)_{i_k, j_k}\\right) \\left(\\prod_{k=s+1}^d (A_k)_{i_k, j_k}\\right)\n$$\nThis structure shows that the matrix $\\mathbf{A}_{(s)}$ is the outer product of two vectors, $u \\in \\mathbb{R}^{\\prod_{k=1}^s m_k n_k}$ and $v \\in \\mathbb{R}^{\\prod_{k=s+1}^d m_k n_k}$. The components of these vectors are:\n$$\nu_{(i_1,j_1), \\dots, (i_s,j_s)} = \\prod_{k=1}^s (A_k)_{i_k, j_k}\n$$\n$$\nv_{(i_{s+1},j_{s+1}), \\dots, (i_d,j_d)} = \\prod_{k=s+1}^d (A_k)_{i_k, j_k}\n$$\nThese vectors can be identified with the vectorized forms of the corresponding partial Kronecker products, i.e., $u = \\mathrm{vec}(A_1 \\otimes \\cdots \\otimes A_s)$ and $v = \\mathrm{vec}(A_{s+1} \\otimes \\cdots \\otimes A_d)$.\nThe matrix $\\mathbf{A}_{(s)}$ is thus given by $\\mathbf{A}_{(s)} = u v^T$. The rank of an outer product of two vectors is at most $1$. If neither $u$ nor $v$ is the zero vector (i.e., if none of the matrices $A_k$ are zero matrices), the rank is exactly $1$. Therefore, the $s$-th TT rank is:\n$$\nr_s = \\mathrm{rank}(\\mathbf{A}_{(s)}) \\le 1\n$$\nThis provides the a priori bound $r_s \\le 1$ for all $s=1, \\dots, d-1$. This bound is the tightest possible and, being a constant, does not depend on the classical matrix ranks of the factors $A_k$.\n\nWith all TT ranks being $1$ (i.e., $r_0=r_1=\\dots=r_d=1$), the TT cores $G^{(k)}$ are tensors of size $1 \\times m_k \\times n_k \\times 1$. The TT-matrix product simplifies to a scalar product:\n$$\nA_{i,j} = G^{(1)}_{1,i_1,j_1,1} \\, G^{(2)}_{1,i_2,j_2,1} \\cdots G^{(d)}_{1,i_d,j_d,1}\n$$\nTo construct the representation, we can choose the core entries to be the elements of the factor matrices themselves:\n$$\nG^{(k)}_{1,i_k,j_k,1} = (A_k)_{i_k, j_k}\n$$\nWith this choice, the product becomes $\\prod_{k=1}^d (A_k)_{i_k, j_k}$, which is precisely the expression for $A_{i,j}$. This provides a constructive proof for the TT representation and confirms the rank bound.\n\n**Part 2: Specialization to Diagonal Factors**\n\nWe are given the special case where each factor matrix $A_k$ is diagonal:\n$$\n(A_k)_{i_k, j_k} = a^{(k)}_{i_k} \\delta_{i_k, j_k}\n$$\nwhere $a^{(k)}_{i_k}$ are the diagonal entries and $\\delta_{i_k,j_k}$ is the Kronecker delta.\n\nTo find the explicit form of the TT-matrix cores for this case, we use the general construction derived in Part 1. The ranks are all $1$, and the core entries are given by the elements of the factor matrices. Substituting the specific form of $(A_k)_{i_k, j_k}$ into our general core definition yields:\n$$\nG^{(k)}_{1,i_k,j_k,1} = a^{(k)}_{i_k} \\delta_{i_k,j_k}\n$$\nThis expression provides the entries for the TT-cores of size $1 \\times m_k \\times n_k \\times 1$. The core $G^{(k)}$ is a diagonal \"matrix of matrices,\" where for each $(i_k, j_k)$ with $i_k \\ne j_k$, the entry is $0$, and for $i_k = j_k$, the entry is $a^{(k)}_{i_k}$. This is the required closed-form expression.", "answer": "$$\n\\boxed{a^{(k)}_{i_k} \\delta_{i_k j_k}}\n$$", "id": "3424557"}, {"introduction": "Tensor structures are not just abstract mathematical conveniences; they arise naturally in the formulation of many scientific problems. This practice delves into the realm of Bayesian inverse problems, demonstrating how a separable Gaussian prior, modeled with a Kronecker product covariance, leads to a posterior precision matrix with a specific, computationally friendly structure. This exercise connects the algebraic properties of tensors directly to the mechanics of probabilistic inference in high dimensions [@problem_id:3424594].", "problem": "Consider a linear Gaussian Bayesian inverse problem with an unknown state vector $x \\in \\mathbb{R}^{N}$ and observations $y \\in \\mathbb{R}^{N}$ given by the separable model $y = x + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{N})$ with $\\sigma > 0$. Assume a Kronecker-structured Gaussian prior $x \\sim \\mathcal{N}(0, C)$ with covariance $C = C_{1} \\otimes \\cdots \\otimes C_{d}$, where $d \\in \\mathbb{N}$, $N = \\prod_{k=1}^{d} n_{k}$, and each factor $C_{k} \\in \\mathbb{R}^{n_{k} \\times n_{k}}$ is symmetric positive definite (SPD). You may use standard facts about multivariate Gaussian conditioning and basic Kronecker product properties, but do not assume any special structure beyond separability.\n\n1. Starting from the Bayesian linear Gaussian update for the posterior, show that the posterior precision $C_{\\text{post}}^{-1}$ is the sum of two Kronecker products. Write this posterior precision explicitly in terms of $C_{1}, \\ldots, C_{d}$ and $\\sigma^{2}$.\n\n2. Let the eigen-decomposition of each prior factor be $C_{k} = U_{k} \\Lambda_{k} U_{k}^{\\top}$ with $U_{k}$ orthogonal and $\\Lambda_{k} = \\operatorname{diag}(\\lambda_{k,1}, \\ldots, \\lambda_{k,n_{k}})$ with $\\lambda_{k,i} > 0$. Using only these eigen-decompositions and the separability of the model, derive a closed-form expression for the posterior trace $\\operatorname{tr}(C_{\\text{post}})$ in terms of $\\{\\lambda_{k,i}\\}$ and $\\sigma^{2}$. Your final answer must be a single analytic expression.\n\nExpress the final answer as a single analytic expression without units. Do not round.", "solution": "The problem asks for the derivation of the posterior precision and the posterior trace for a linear Gaussian Bayesian inverse problem with a separable (Kronecker-structured) prior.\n\nThe general linear Gaussian inverse problem is defined by a prior distribution on the unknown state $x$, $p(x) = \\mathcal{N}(x_0, C)$, and a likelihood for the observations $y$ given the state, $p(y|x) = \\mathcal{N}(Hx, R)$. Here, $x \\in \\mathbb{R}^{N}$, $y \\in \\mathbb{R}^{M}$, $x_0$ is the prior mean, $C$ is the prior covariance, $H$ is the forward operator, and $R$ is the observation noise covariance. According to Bayes' theorem, the posterior distribution $p(x|y)$ is also Gaussian, $p(x|y) = \\mathcal{N}(x_{\\text{post}}, C_{\\text{post}})$, with posterior covariance $C_{\\text{post}}$ and posterior mean $x_{\\text{post}}$ given by:\n$$C_{\\text{post}}^{-1} = C^{-1} + H^{\\top} R^{-1} H$$\n$$x_{\\text{post}} = C_{\\text{post}} (C^{-1}x_0 + H^{\\top} R^{-1} y)$$\nThese are the standard Bayesian linear Gaussian update equations.\n\nPart 1: Derivation of the posterior precision $C_{\\text{post}}^{-1}$.\n\nIn the given problem, the state vector is $x \\in \\mathbb{R}^{N}$ and the observation vector is $y \\in \\mathbb{R}^{N}$. The model is $y = x + \\varepsilon$, which corresponds to a forward operator $H=I_N$, where $I_N$ is the identity matrix of size $N \\times N$. The noise $\\varepsilon$ is distributed as $\\mathcal{N}(0, \\sigma^{2} I_{N})$, so the noise covariance is $R = \\sigma^2 I_N$. The prior on $x$ is given as $\\mathcal{N}(0, C)$, which means the prior mean is $x_0 = 0$ and the prior covariance is $C = C_1 \\otimes \\cdots \\otimes C_d$.\n\nWe can substitute these specific forms into the general formula for the posterior precision $C_{\\text{post}}^{-1}$:\n$$C_{\\text{post}}^{-1} = C^{-1} + (I_N)^{\\top} (\\sigma^2 I_N)^{-1} (I_N)$$\n$$C_{\\text{post}}^{-1} = C^{-1} + I_N \\left(\\frac{1}{\\sigma^2} I_N\\right) I_N = C^{-1} + \\frac{1}{\\sigma^2} I_N$$\nThe prior covariance is $C = C_1 \\otimes \\cdots \\otimes C_d$. Using the property of the inverse of a Kronecker product, $(A \\otimes B)^{-1} = A^{-1} \\otimes B^{-1}$, the prior precision $C^{-1}$ is:\n$$C^{-1} = (C_1 \\otimes \\cdots \\otimes C_d)^{-1} = C_1^{-1} \\otimes \\cdots \\otimes C_d^{-1}$$\nSince each factor $C_k$ is symmetric positive definite (SPD), its inverse $C_k^{-1}$ exists.\nSubstituting this into the expression for the posterior precision gives:\n$$C_{\\text{post}}^{-1} = (C_1^{-1} \\otimes \\cdots \\otimes C_d^{-1}) + \\frac{1}{\\sigma^2} I_N$$\nThe first term, $C_1^{-1} \\otimes \\cdots \\otimes C_d^{-1}$, is a Kronecker product of $d$ matrices. The second term, $\\frac{1}{\\sigma^2} I_N$, is a scaled identity matrix. Since $I_N = I_{n_1} \\otimes \\cdots \\otimes I_{n_d}$, the second term can also be viewed as a (scaled) Kronecker product. Thus, the posterior precision is the sum of two matrices that both have Kronecker product structure. The explicit expression for the posterior precision in terms of $C_1, \\ldots, C_d$ and $\\sigma^2$ is:\n$$C_{\\text{post}}^{-1} = (C_1 \\otimes \\cdots \\otimes C_d)^{-1} + \\frac{1}{\\sigma^2} I_N$$\nor equivalently, expressed with the inverses of the factors:\n$$C_{\\text{post}}^{-1} = C_1^{-1} \\otimes \\cdots \\otimes C_d^{-1} + \\frac{1}{\\sigma^2} I_N$$\n\nPart 2: Derivation of the posterior trace $\\operatorname{tr}(C_{\\text{post}})$.\n\nWe need to compute the trace of the posterior covariance, $\\operatorname{tr}(C_{\\text{post}})$. We start from the expression for the inverse posterior covariance derived above:\n$$C_{\\text{post}} = \\left( C^{-1} + \\frac{1}{\\sigma^2} I_N \\right)^{-1}$$\nThe trace of a matrix is the sum of its eigenvalues. We will find the eigenvalues of $C_{\\text{post}}$ by first finding the eigenvalues of $C_{\\text{post}}^{-1}$.\nThe problem provides the eigen-decomposition for each prior factor $C_k = U_k \\Lambda_k U_k^{\\top}$, where $U_k$ is orthogonal and $\\Lambda_k = \\operatorname{diag}(\\lambda_{k,1}, \\ldots, \\lambda_{k,n_k})$ is a diagonal matrix of positive eigenvalues.\nThe eigen-decomposition of the full prior covariance $C$ can be constructed using the properties of the Kronecker product:\n$$C = C_1 \\otimes \\cdots \\otimes C_d = (U_1 \\Lambda_1 U_1^{\\top}) \\otimes \\cdots \\otimes (U_d \\Lambda_d U_d^{\\top})$$\nUsing the mixed-product property $(A \\otimes B)(C \\otimes D) = (AC) \\otimes (BD)$, this becomes:\n$$C = (U_1 \\otimes \\cdots \\otimes U_d) (\\Lambda_1 \\otimes \\cdots \\otimes \\Lambda_d) (U_1^{\\top} \\otimes \\cdots \\otimes U_d^{\\top})$$\nLet $U = U_1 \\otimes \\cdots \\otimes U_d$ and $\\Lambda_{\\text{prior}} = \\Lambda_1 \\otimes \\cdots \\otimes \\Lambda_d$. The matrix $U$ is orthogonal, as it is a Kronecker product of orthogonal matrices. The matrix $\\Lambda_{\\text{prior}}$ is diagonal. Thus, $C = U \\Lambda_{\\text{prior}} U^{\\top}$ is the eigen-decomposition of $C$. The eigenvalues of $C$, which are the diagonal entries of $\\Lambda_{\\text{prior}}$, are the products of the eigenvalues of the factors $C_k$. An eigenvalue $\\lambda$ of $C$ has the form $\\lambda = \\prod_{k=1}^d \\lambda_{k,i_k}$ for some choice of indices $(i_1, \\ldots, i_d)$, where $i_k \\in \\{1, \\ldots, n_k\\}$.\n\nNow, we can express the posterior precision $C_{\\text{post}}^{-1}$ using this eigen-decomposition. The prior precision is $C^{-1} = (U \\Lambda_{\\text{prior}} U^{\\top})^{-1} = U \\Lambda_{\\text{prior}}^{-1} U^{\\top}$.\n$$C_{\\text{post}}^{-1} = U \\Lambda_{\\text{prior}}^{-1} U^{\\top} + \\frac{1}{\\sigma^2} I_N$$\nSince $U$ is orthogonal, $I_N = U U^{\\top}$. We can factor out $U$ and $U^{\\top}$:\n$$C_{\\text{post}}^{-1} = U \\Lambda_{\\text{prior}}^{-1} U^{\\top} + \\frac{1}{\\sigma^2} U U^{\\top} = U \\left( \\Lambda_{\\text{prior}}^{-1} + \\frac{1}{\\sigma^2} I_N \\right) U^{\\top}$$\nThis expression gives the eigen-decomposition of $C_{\\text{post}}^{-1}$. The eigenvectors are the columns of $U$, and the eigenvalues are the diagonal entries of the diagonal matrix $\\Lambda_{\\text{prior}}^{-1} + \\frac{1}{\\sigma^2} I_N$.\nThe eigenvalues of $C_{\\text{post}}^{-1}$ are therefore of the form $\\lambda^{-1} + \\frac{1}{\\sigma^2}$, where $\\lambda$ is an eigenvalue of $C$.\n\nThe posterior covariance is the inverse of the posterior precision:\n$$C_{\\text{post}} = (C_{\\text{post}}^{-1})^{-1} = \\left( U \\left( \\Lambda_{\\text{prior}}^{-1} + \\frac{1}{\\sigma^2} I_N \\right) U^{\\top} \\right)^{-1} = U \\left( \\Lambda_{\\text{prior}}^{-1} + \\frac{1}{\\sigma^2} I_N \\right)^{-1} U^{\\top}$$\nThis is the eigen-decomposition of $C_{\\text{post}}$. The eigenvalues of $C_{\\text{post}}$ are the diagonal entries of the matrix $(\\Lambda_{\\text{prior}}^{-1} + \\frac{1}{\\sigma^2} I_N)^{-1}$. These are simply the reciprocals of the eigenvalues of $C_{\\text{post}}^{-1}$. An eigenvalue of $C_{\\text{post}}$, which we denote $\\lambda_{\\text{post}}$, is given by:\n$$\\lambda_{\\text{post}} = \\left( \\lambda^{-1} + \\frac{1}{\\sigma^2} \\right)^{-1} = \\left( \\frac{1}{\\lambda} + \\frac{1}{\\sigma^2} \\right)^{-1} = \\left( \\frac{\\sigma^2 + \\lambda}{\\lambda \\sigma^2} \\right)^{-1} = \\frac{\\lambda \\sigma^2}{\\lambda + \\sigma^2}$$\nwhere $\\lambda$ is an eigenvalue of the prior covariance $C$.\n\nThe trace of $C_{\\text{post}}$ is the sum of its eigenvalues. We must sum over all $N = \\prod_{k=1}^d n_k$ eigenvalues. Each eigenvalue $\\lambda$ of $C$ is a product of the form $\\prod_{k=1}^d \\lambda_{k,i_k}$. Therefore, the trace of the posterior covariance is:\n$$\\operatorname{tr}(C_{\\text{post}}) = \\sum_{\\text{all eigenvalues } \\lambda \\text{ of } C} \\frac{\\lambda \\sigma^2}{\\lambda + \\sigma^2}$$\nSubstituting the structure of the eigenvalues of $C$, we get a sum over all combinations of indices $(i_1, \\ldots, i_d)$:\n$$\\operatorname{tr}(C_{\\text{post}}) = \\sum_{i_1=1}^{n_1} \\sum_{i_2=1}^{n_2} \\cdots \\sum_{i_d=1}^{n_d} \\frac{\\left(\\prod_{k=1}^d \\lambda_{k, i_k}\\right) \\sigma^2}{\\left(\\prod_{k=1}^d \\lambda_{k, i_k}\\right) + \\sigma^2}$$\nThis is the final closed-form expression for the posterior trace in terms of the eigenvalues of the prior factors $\\{\\lambda_{k,i}\\}$ and the noise variance $\\sigma^2$.\nRewriting the expression slightly for clarity:\n$$\\operatorname{tr}(C_{\\text{post}}) = \\sum_{i_1=1}^{n_1} \\cdots \\sum_{i_d=1}^{n_d} \\frac{\\sigma^2 \\prod_{k=1}^d \\lambda_{k, i_k}}{\\sigma^2 + \\prod_{k=1}^d \\lambda_{k, i_k}}$$\nThis completes the derivation.", "answer": "$$\n\\boxed{\\sum_{i_1=1}^{n_1} \\cdots \\sum_{i_d=1}^{n_d} \\frac{\\sigma^2 \\prod_{k=1}^d \\lambda_{k, i_k}}{\\sigma^2 + \\prod_{k=1}^d \\lambda_{k, i_k}}}\n$$", "id": "3424594"}, {"introduction": "The true power of tensor representations lies in their ability to make computations tractable that would otherwise be impossible. Building on the insight that precision matrices in Bayesian problems can be sums of Kronecker-structured operators, this final exercise asks you to quantify the computational cost of applying such an operator to a vector in the Tensor Train format [@problem_id:3424565]. The result demonstrates how these methods elegantly circumvent the curse of dimensionality, enabling efficient iterative solvers for large-scale inverse problems.", "problem": "Consider a high-dimensional Bayesian inverse problem in which the negative log-prior precision operator on a tensorized parameter field is a Kronecker sum\n$$\n\\mathcal{L} \\;=\\; \\sum_{k=1}^{d} I \\otimes \\cdots \\otimes L_k \\otimes \\cdots \\otimes I,\n$$\nwhere each $L_k \\in \\mathbb{R}^{n_k \\times n_k}$ is a dense matrix acting along mode $k$, and $I$ denotes the identity matrix of appropriate size. The parameter vector $x$ is represented in Tensor Train (TT) format with cores $\\mathcal{X}^{(k)} \\in \\mathbb{R}^{r_{k-1} \\times n_k \\times r_k}$ for $k \\in \\{1,\\ldots,d\\}$, with boundary ranks $r_0 = r_d = 1$. You may assume the standard floating point operation (flop) model in which each scalar addition or multiplication counts as one flop.\n\nStarting from the definitions of the Kronecker sum, the Tensor Train format, and the basic dense matrix-vector multiplication cost, derive a closed-form expression for the total flop count required to compute the action $y = \\mathcal{L} x$ by contracting each $L_k$ with the mode-$k$ fibers of $x$. Ignore the cost of adding the $d$ contributions and any tensor-train rounding or recompression, and ignore any costs associated with applying identities. Express your answer solely in terms of $\\{n_k\\}_{k=1}^{d}$ and $\\{r_k\\}_{k=0}^{d}$, and give the final expression as a single analytic formula. No numerical evaluation is required, and no units are involved. The final answer must be a single analytic expression.", "solution": "The problem asks for the total floating-point operation (flop) count to compute the action of a Kronecker sum operator $\\mathcal{L}$ on a vector $x$ represented in the Tensor Train (TT) format. We are given the structure of the operator and the representation of the vector, and we must derive the cost based on fundamental computational steps.\n\nFirst, we analyze the structure of the computation $y = \\mathcal{L}x$. The operator $\\mathcal{L}$ is a sum of $d$ operators:\n$$\n\\mathcal{L} \\;=\\; \\sum_{k=1}^{d} \\mathcal{L}_k, \\quad \\text{where} \\quad \\mathcal{L}_k \\;=\\; I \\otimes \\cdots \\otimes I \\otimes L_k \\otimes I \\otimes \\cdots \\otimes I\n$$\nIn the expression for $\\mathcal{L}_k$, the matrix $L_k \\in \\mathbb{R}^{n_k \\times n_k}$ is at the $k$-th position in the Kronecker product, and all other matrices are identities $I$ of appropriate sizes.\n\nThe total computation is thus $y = \\sum_{k=1}^{d} \\mathcal{L}_k x$. The problem statement instructs us to ignore the cost of adding the $d$ resulting terms. Therefore, the total flop count is the sum of the costs of computing each term $y_k = \\mathcal{L}_k x$ for $k = 1, \\ldots, d$. Let $C_k$ be the cost of computing $y_k$. The total cost $C_{\\text{total}}$ is then:\n$$\nC_{\\text{total}} = \\sum_{k=1}^{d} C_k\n$$\n\nNext, we determine the cost $C_k$ for a single term $y_k = \\mathcal{L}_k x$. The vector $x$ is given in TT format with cores $\\mathcal{X}^{(j)} \\in \\mathbb{R}^{r_{j-1} \\times n_j \\times r_j}$ for $j=1, \\ldots, d$. The operator $\\mathcal{L}_k$ acts as the identity on all modes except for the $k$-th mode, where it applies the dense matrix $L_k$. A key property of the TT format is that such a mode-$k$ operation can be performed by only modifying the $k$-th TT core, $\\mathcal{X}^{(k)}$. The resulting tensor $y_k$ will have TT cores $\\mathcal{Y}^{(j)}$ where $\\mathcal{Y}^{(j)} = \\mathcal{X}^{(j)}$ for all $j \\neq k$. The problem states to ignore costs associated with applying identities, which formalizes this observation.\n\nThe only computational work required for $y_k$ is to compute the new $k$-th core, which we denote $\\mathcal{Y}^{(k)}$. This is achieved by contracting the core $\\mathcal{X}^{(k)}$ with the matrix $L_k$ along the physical mode (the second mode of the core, of size $n_k$). The core $\\mathcal{X}^{(k)}$ is a three-dimensional tensor with elements $(\\mathcal{X}^{(k)})_{i,j,p}$, where the indices range as $i \\in \\{1, \\ldots, r_{k-1}\\}$, $j \\in \\{1, \\ldots, n_k\\}$, and $p \\in \\{1, \\ldots, r_k\\}$. The matrix $L_k$ has elements $(L_k)_{m,j}$. The elements of the resulting core $\\mathcal{Y}^{(k)}$ are given by the contraction:\n$$\n(\\mathcal{Y}^{(k)})_{i,m,p} = \\sum_{j=1}^{n_k} (L_k)_{m,j} (\\mathcal{X}^{(k)})_{i,j,p}\n$$\nThis operation must be performed for all combinations of the indices $i$, $m$, and $p$.\n\nLet's analyze the flop count for this contraction. For each fixed pair of indices $(i, p)$, the operation is equivalent to a matrix-vector multiplication. The vector is the mode-$2$ fiber $(\\mathcal{X}^{(k)})_{i,:,p}$ of length $n_k$, and the matrix is $L_k \\in \\mathbb{R}^{n_k \\times n_k}$. The cost of multiplying a dense $n_k \\times n_k$ matrix by a vector of length $n_k$ is $n_k^2$ multiplications and $n_k(n_k-1)$ additions. Following the problem's flop model (where each addition or multiplication is one flop), this gives a cost of $n_k^2 + (n_k^2 - n_k) = 2n_k^2 - n_k$ flops for each such matrix-vector product.\n\nAlternatively, we can count the flops for computing each element $(\\mathcal{Y}^{(k)})_{i,m,p}$. The sum $\\sum_{j=1}^{n_k} (L_k)_{m,j} (\\mathcal{X}^{(k)})_{i,j,p}$ involves $n_k$ multiplications and $n_k-1$ additions. This requires a total of $n_k + (n_k-1) = 2n_k - 1$ flops per element of the output core. The total number of elements in the output core $\\mathcal{Y}^{(k)}$ (which has the same dimensions $r_{k-1} \\times n_k \\times r_k$ as the input core) is $r_{k-1} n_k r_k$. Therefore, the total cost $C_k$ is:\n$$\nC_k = (r_{k-1} n_k r_k) \\times (2n_k - 1) = (2n_k^2 - n_k) r_{k-1} r_k\n$$\nThis confirms the cost derived from the matrix-vector product perspective, as there are $r_{k-1}r_k$ such products to compute.\n\nFinally, to find the total flop count $C_{\\text{total}}$, we sum the costs $C_k$ over all modes from $k=1$ to $d$:\n$$\nC_{\\text{total}} = \\sum_{k=1}^{d} C_k = \\sum_{k=1}^{d} (2n_k^2 - n_k) r_{k-1} r_k\n$$\nThe boundary ranks are given as $r_0=1$ and $r_d=1$. The summation correctly incorporates these values for the terms $k=1$ and $k=d$. This formula represents the complete flop count under the specified conditions. It can also be written as $\\sum_{k=1}^{d} n_k(2n_k-1)r_{k-1}r_k$.", "answer": "$$\n\\boxed{\\sum_{k=1}^{d} (2n_k^2 - n_k) r_{k-1} r_k}\n$$", "id": "3424565"}]}