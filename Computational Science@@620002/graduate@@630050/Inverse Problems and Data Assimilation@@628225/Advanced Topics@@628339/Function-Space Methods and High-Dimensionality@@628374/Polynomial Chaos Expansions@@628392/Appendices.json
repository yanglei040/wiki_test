{"hands_on_practices": [{"introduction": "Before applying a Polynomial Chaos Expansion (PCE), we must first understand its fundamental structure. This practice focuses on the combinatorial nature of the basis set, which is crucial for appreciating the computational cost and scalability of PCE, a challenge often referred to as the \"curse of dimensionality\". By deriving the size of the basis set from first principles, you will gain a concrete understanding of how the number of required terms grows with the problem's dimension and polynomial degree. [@problem_id:3523172]", "problem": "Consider a non-intrusive Polynomial Chaos Expansion (PCE) for a coupled thermoelastic–fluid system in a multiphysics simulation. The uncertain input is modeled by a random vector $\\boldsymbol{\\xi} \\in \\mathbb{R}^{d}$ with $d$ independent components corresponding to five material and coupling parameters ($d=5$). To construct a finite-dimensional approximation, you use a total-degree truncated multiindex set defined by\n$$\n\\Lambda_{\\mathrm{TD}}(p) \\equiv \\left\\{ \\alpha \\in \\mathbb{N}_{0}^{d} : \\|\\alpha\\|_{1} \\le p \\right\\},\n$$\nwhere $\\|\\alpha\\|_{1} = \\sum_{i=1}^{d} \\alpha_{i}$ and $p$ is the maximum polynomial degree ($p=3$). The PCE basis functions are indexed by $\\alpha \\in \\Lambda_{\\mathrm{TD}}(p)$ and orthogonal with respect to the input measure. \n\nStarting from fundamental definitions of multiindices and first principles of combinatorial counting (combinations with repetition), derive the cardinality of the index set $\\Lambda_{\\mathrm{TD}}(p)$ for $d=5$ and $p=3$ without invoking any pre-memorized shortcut formulas. Then, explicitly enumerate all multiindices $\\alpha \\in \\mathbb{N}_{0}^{5}$ satisfying $\\|\\alpha\\|_{1} \\le 3$ in lexicographic order, where lexicographic order is defined by $\\alpha \\prec \\beta$ if there exists $j \\in \\{1,\\dots,5\\}$ such that $\\alpha_{i}=\\beta_{i}$ for all $i<j$ and $\\alpha_{j}<\\beta_{j}$. \n\nProvide both the derived cardinality and the complete lexicographic list as part of your reasoning. For submission, express your final answer as the single real-valued cardinality of $\\Lambda_{\\mathrm{TD}}(3)$ for $d=5$. No rounding is required.", "solution": "The problem requires the derivation of the cardinality of a total-degree truncated multi-index set and the explicit enumeration of its elements for a specific case. The set is defined as $\\Lambda_{\\mathrm{TD}}(p) \\equiv \\left\\{ \\alpha \\in \\mathbb{N}_{0}^{d} : \\|\\alpha\\|_{1} \\le p \\right\\}$, with dimension $d=5$ and maximum polynomial degree $p=3$.\n\nFirst, we derive the cardinality of $\\Lambda_{\\mathrm{TD}}(p)$, denoted $|\\Lambda_{\\mathrm{TD}}(p)|$, from first principles of combinatorial counting, as requested. The condition is that for a multi-index $\\alpha = (\\alpha_1, \\alpha_2, \\dots, \\alpha_d)$, where each $\\alpha_i$ is a non-negative integer, the sum must satisfy $\\sum_{i=1}^{d} \\alpha_i \\le p$.\n\nThis inequality can be converted into an equality by introducing an auxiliary non-negative integer variable, $\\alpha_{d+1} \\in \\mathbb{N}_0$, often called a slack variable. We define $\\alpha_{d+1}$ such that the sum is exactly equal to $p$:\n$$\n\\alpha_1 + \\alpha_2 + \\dots + \\alpha_d + \\alpha_{d+1} = p\n$$\nFor any set of non-negative integers $(\\alpha_1, \\dots, \\alpha_d)$ satisfying the original inequality $\\sum_{i=1}^{d} \\alpha_i = k \\le p$, there exists a unique non-negative integer $\\alpha_{d+1} = p - k$ that satisfies the equality. Conversely, for any solution $(\\alpha_1, \\dots, \\alpha_d, \\alpha_{d+1})$ to the equality in non-negative integers, the first $d$ components satisfy the original inequality. Thus, the number of solutions to the inequality in $d$ variables is identical to the number of non-negative integer solutions to the equality in $d+1$ variables.\n\nThis latter problem is a classic combinatorial problem known as \"combinations with repetition,\" which can be solved using the \"stars and bars\" method. We need to find the number of ways to partition a sum of $p$ into $d+1$ non-negative integer terms. This is equivalent to arranging $p$ identical items (stars) and $(d+1)-1 = d$ identical dividers (bars) in a sequence. The total number of positions in the sequence is $p+d$. The number of ways to choose the positions for the $d$ bars (or, equivalently, the $p$ stars) determines the number of unique solutions. This count is given by the binomial coefficient:\n$$\n|\\Lambda_{\\mathrm{TD}}(p)| = \\binom{p+d}{d} = \\binom{p+d}{p}\n$$\nThis derivation from the fundamental stars and bars analogy fulfills the requirement of not using a pre-memorized shortcut formula.\n\nNow, we apply this derived formula to the specific parameters given: $d=5$ and $p=3$.\n$$\n|\\Lambda_{\\mathrm{TD}}(3)| = \\binom{3+5}{5} = \\binom{8}{5}\n$$\nThe value of the binomial coefficient is calculated as:\n$$\n\\binom{8}{5} = \\frac{8!}{5!(8-5)!} = \\frac{8!}{5!3!} = \\frac{8 \\times 7 \\times 6}{3 \\times 2 \\times 1} = 8 \\times 7 = 56\n$$\nThe cardinality of the index set $\\Lambda_{\\mathrm{TD}}(3)$ for $d=5$ is $56$.\n\nSecond, we explicitly enumerate all $56$ multi-indices $\\alpha = (\\alpha_1, \\alpha_2, \\alpha_3, \\alpha_4, \\alpha_5)$ satisfying $\\alpha_i \\in \\mathbb{N}_0$ and $\\sum_{i=1}^{5} \\alpha_i \\le 3$. The enumeration is presented in lexicographic order.\n\nThe complete list of multi-indices is as follows:\n$ (0,0,0,0,0), (0,0,0,0,1), (0,0,0,0,2), (0,0,0,0,3), (0,0,0,1,0), (0,0,0,1,1), (0,0,0,1,2), (0,0,0,2,0), (0,0,0,2,1), (0,0,0,3,0), (0,0,1,0,0), (0,0,1,0,1), (0,0,1,0,2), (0,0,1,1,0), (0,0,1,1,1), (0,0,1,2,0), (0,0,2,0,0), (0,0,2,0,1), (0,0,2,1,0), (0,0,3,0,0), (0,1,0,0,0), (0,1,0,0,1), (0,1,0,0,2), (0,1,0,1,0), (0,1,0,1,1), (0,1,0,2,0), (0,1,1,0,0), (0,1,1,0,1), (0,1,1,1,0), (0,1,2,0,0), (0,2,0,0,0), (0,2,0,0,1), (0,2,0,1,0), (0,2,1,0,0), (0,3,0,0,0), (1,0,0,0,0), (1,0,0,0,1), (1,0,0,0,2), (1,0,0,1,0), (1,0,0,1,1), (1,0,0,2,0), (1,0,1,0,0), (1,0,1,0,1), (1,0,1,1,0), (1,0,2,0,0), (1,1,0,0,0), (1,1,0,0,1), (1,1,0,1,0), (1,1,1,0,0), (1,2,0,0,0), (2,0,0,0,0), (2,0,0,0,1), (2,0,0,1,0), (2,0,1,0,0), (2,1,0,0,0), (3,0,0,0,0) $", "answer": "$$\n\\boxed{56}\n$$", "id": "3523172"}, {"introduction": "The primary payoff of constructing a PCE is the direct analytical access it provides to the statistical properties of the model output. Once the expansion coefficients are known, we can compute key statistics without resorting to expensive Monte Carlo sampling. This exercise reveals the elegant relationship between the PCE coefficients and the output's mean and variance, a powerful feature that stems directly from the orthonormality of the polynomial basis. [@problem_id:3341826]", "problem": "Consider a frequency-domain computational electromagnetics model in which a homogeneous, isotropic medium has uncertain permittivity $\\,\\epsilon(\\xi) = \\epsilon_{0}\\,(1+\\xi)\\,$, where $\\,\\epsilon_{0} > 0\\,$ is a deterministic constant and $\\,\\xi \\sim \\mathcal{N}(0,\\sigma^{2})\\,$ is a scalar Gaussian random input with variance $\\,\\sigma^{2} > 0\\,$. Let $\\,u(\\xi)\\,$ denote the nonnegative magnitude of a scalar field solution evaluated at a fixed point and frequency, obtained from the governing linear boundary value problem induced by Maxwell’s equations (e.g., the scalar Helmholtz operator with wavenumber $\\,k(\\xi) = \\omega \\sqrt{\\mu\\,\\epsilon(\\xi)}\\,$), under boundary conditions that ensure a unique solution for admissible values of $\\,\\xi\\,$. Assume $\\,u(\\xi)\\,$ belongs to the square-integrable space with respect to the Gaussian measure of $\\,\\xi\\,$, so that a Polynomial Chaos Expansion (PCE) in the Wiener–Hermite class exists.\n\nSuppose $\\,u(\\xi)\\,$ is represented by a finite-order PCE with respect to an orthonormal Hermite basis adapted to the distribution of $\\,\\xi\\,$:\n$$\nu(\\xi) \\approx \\sum_{k=0}^{p} \\hat{u}_{k}\\,\\Psi_{k}(\\xi),\n$$\nwhere $\\,p \\in \\mathbb{N}\\,$ is the truncation order, $\\,\\hat{u}_{k} \\in \\mathbb{R}\\,$ are deterministic coefficients, and $\\,\\{\\Psi_{k}(\\xi)\\}_{k=0}^{p}\\,$ is an orthonormal polynomial basis with respect to the Gaussian measure of $\\,\\xi\\,$, satisfying $\\,\\mathbb{E}[\\Psi_{m}(\\xi)\\,\\Psi_{n}(\\xi)] = \\delta_{mn}\\,$ and $\\,\\Psi_{0}(\\xi) \\equiv 1\\,$.\n\nStarting from first principles—namely, the linearity of the governing operator with respect to the field, the definition of second-order moments, and orthonormality of the Wiener–Hermite basis—derive expressions for the mean $\\,\\mathbb{E}[u(\\xi)]\\,$ and the variance $\\,\\operatorname{Var}(u(\\xi))\\,$ in terms of the coefficients $\\,\\{\\hat{u}_{k}\\}_{k=0}^{p}\\,$. Then, state mathematically precise conditions on the input $\\,\\xi\\,$, the permittivity map $\\,\\epsilon(\\xi)\\,$, and the deterministic operator that guarantee mean-square convergence of the truncated PCE $\\,\\sum_{k=0}^{p} \\hat{u}_{k}\\,\\Psi_{k}(\\xi)\\,$ to $\\,u(\\xi)\\,$ as $\\,p \\to \\infty\\,$.\n\nProvide your final answer as a single row vector containing the mean and variance in closed-form analytic expressions in terms of $\\,\\{\\hat{u}_{k}\\}\\,$ and $\\,p\\,$. No rounding is required, and no units should be included in the final answer.", "solution": "The problem asks for the derivation of the mean and variance of the random variable $u(\\xi)$ from its Polynomial Chaos Expansion (PCE), and for the conditions that guarantee the mean-square convergence of this expansion. The PCE of $u(\\xi)$ is given in a truncated form as $u_p(\\xi) = \\sum_{k=0}^{p} \\hat{u}_{k}\\,\\Psi_{k}(\\xi)$. The linearity of the governing operator with respect to the field $u$ is a foundational requirement for using spectral methods like PCE-Galerkin to determine the coefficients $\\hat{u}_k$, but the calculation of moments from the expansion itself relies on the linearity of the expectation operator and the properties of the basis polynomials.\n\nFirst, we derive the mean, $\\mathbb{E}[u(\\xi)]$. For the truncated series $u_p(\\xi)$, the mean is found by applying the expectation operator $\\mathbb{E}[\\cdot]$.\n$$\n\\mathbb{E}[u_p(\\xi)] = \\mathbb{E}\\left[\\sum_{k=0}^{p} \\hat{u}_{k}\\,\\Psi_{k}(\\xi)\\right]\n$$\nBy the linearity of the expectation operator, we can move it inside the summation. Since the coefficients $\\hat{u}_k$ are deterministic constants, they can be factored out of the expectation.\n$$\n\\mathbb{E}[u_p(\\xi)] = \\sum_{k=0}^{p} \\hat{u}_{k}\\,\\mathbb{E}[\\Psi_{k}(\\xi)]\n$$\nTo evaluate $\\mathbb{E}[\\Psi_{k}(\\xi)]$, we use the provided properties of the orthonormal basis. We are given that $\\Psi_0(\\xi) \\equiv 1$ and the orthonormality condition is $\\mathbb{E}[\\Psi_m(\\xi)\\Psi_n(\\xi)] = \\delta_{mn}$, where $\\delta_{mn}$ is the Kronecker delta. We can express the expectation of $\\Psi_k(\\xi)$ as an inner product with $\\Psi_0(\\xi)$:\n$$\n\\mathbb{E}[\\Psi_{k}(\\xi)] = \\mathbb{E}[\\Psi_k(\\xi) \\cdot 1] = \\mathbb{E}[\\Psi_k(\\xi)\\,\\Psi_0(\\xi)] = \\delta_{k0}\n$$\nThis result shows that the expectation of any basis polynomial is zero, except for the zeroth-order polynomial, for which it is one. Substituting this into the expression for the mean of $u_p(\\xi)$:\n$$\n\\mathbb{E}[u_p(\\xi)] = \\sum_{k=0}^{p} \\hat{u}_{k}\\,\\delta_{k0} = \\hat{u}_0 \\cdot 1 + \\hat{u}_1 \\cdot 0 + \\dots + \\hat{u}_p \\cdot 0 = \\hat{u}_0\n$$\nThus, the mean of the random field is exactly the zeroth-order PCE coefficient. As $p \\to \\infty$, if the expansion converges, $\\mathbb{E}[u(\\xi)] = \\hat{u}_0$.\n\nNext, we derive the variance, $\\operatorname{Var}(u(\\xi))$. The variance is defined as $\\operatorname{Var}(u_p(\\xi)) = \\mathbb{E}[u_p(\\xi)^2] - (\\mathbb{E}[u_p(\\xi)])^2$. We have already found $\\mathbb{E}[u_p(\\xi)] = \\hat{u}_0$. We now need to compute the second moment, $\\mathbb{E}[u_p(\\xi)^2]$.\n$$\n\\mathbb{E}[u_p(\\xi)^2] = \\mathbb{E}\\left[\\left(\\sum_{k=0}^{p} \\hat{u}_{k}\\,\\Psi_{k}(\\xi)\\right)^2\\right] = \\mathbb{E}\\left[\\left(\\sum_{k=0}^{p} \\hat{u}_{k}\\,\\Psi_{k}(\\xi)\\right)\\left(\\sum_{j=0}^{p} \\hat{u}_{j}\\,\\Psi_{j}(\\xi)\\right)\\right]\n$$\nExpanding the product and using the linearity of expectation:\n$$\n\\mathbb{E}[u_p(\\xi)^2] = \\mathbb{E}\\left[\\sum_{k=0}^{p}\\sum_{j=0}^{p} \\hat{u}_k \\hat{u}_j \\Psi_k(\\xi) \\Psi_j(\\xi)\\right] = \\sum_{k=0}^{p}\\sum_{j=0}^{p} \\hat{u}_k \\hat{u}_j \\mathbb{E}[\\Psi_k(\\xi) \\Psi_j(\\xi)]\n$$\nUsing the orthonormality condition $\\mathbb{E}[\\Psi_k(\\xi)\\Psi_j(\\xi)] = \\delta_{kj}$:\n$$\n\\mathbb{E}[u_p(\\xi)^2] = \\sum_{k=0}^{p}\\sum_{j=0}^{p} \\hat{u}_k \\hat{u}_j \\delta_{kj}\n$$\nThe Kronecker delta $\\delta_{kj}$ collapses the inner summation, as terms are non-zero only when $j=k$:\n$$\n\\mathbb{E}[u_p(\\xi)^2] = \\sum_{k=0}^{p} \\hat{u}_k^2\n$$\nThis is the second moment of $u_p(\\xi)$. Now, we substitute the second moment and the mean into the variance formula:\n$$\n\\operatorname{Var}(u_p(\\xi)) = \\mathbb{E}[u_p(\\xi)^2] - (\\mathbb{E}[u_p(\\xi)])^2 = \\sum_{k=0}^{p} \\hat{u}_k^2 - (\\hat{u}_0)^2\n$$\nSeparating the $k=0$ term from the summation gives:\n$$\n\\operatorname{Var}(u_p(\\xi)) = \\left(\\hat{u}_0^2 + \\sum_{k=1}^{p} \\hat{u}_k^2\\right) - \\hat{u}_0^2 = \\sum_{k=1}^{p} \\hat{u}_k^2\n$$\nThe variance is the sum of the squares of all higher-order ($k \\geq 1$) PCE coefficients.\n\nFinally, we address the conditions for mean-square convergence. The truncated PCE $\\sum_{k=0}^{p} \\hat{u}_{k}\\,\\Psi_{k}(\\xi)$ converges in the mean-square sense to $u(\\xi)$ as $p \\to \\infty$ if the mean-square error tends to zero:\n$$\n\\lim_{p \\to \\infty} \\mathbb{E}\\left[ \\left(u(\\xi) - \\sum_{k=0}^{p} \\hat{u}_{k}\\,\\Psi_{k}(\\xi)\\right)^2 \\right] = 0\n$$\nFor a Wiener-Hermite PCE, which is used for a Gaussian random variable $\\xi$, the Cameron-Martin theorem provides the necessary and sufficient condition for this convergence. The single, mathematically precise condition is that the function $u(\\xi)$ must be square-integrable with respect to the Gaussian probability measure, i.e., it must belong to the Hilbert space $L^2(\\mathbb{R}, w(\\xi)d\\xi)$, where $w(\\xi)$ is the Gaussian probability density function for $\\xi$. This is equivalent to requiring that the second moment of $u(\\xi)$ is finite:\n$$\n\\mathbb{E}[u(\\xi)^2] < \\infty\n$$\nThe problem statement explicitly assumes this condition (\"Assume $u(\\xi)$ belongs to the square-integrable space...\"). This condition on the output function $u(\\xi)$ imposes implicit conditions on the system's components:\n$1$. The input random variable $\\xi$ and the permittivity map $\\epsilon(\\xi) = \\epsilon_0(1+\\xi)$ are already defined. The Gaussian measure has support over all of $\\mathbb{R}$.\n$2$. The deterministic operator (e.g., the scalar Helmholtz operator), its associated boundary conditions, and any forcing terms must be such that the solution $u(\\xi)$ resulting from the parameterization $k(\\xi) = \\omega\\sqrt{\\mu\\epsilon(\\xi)}$ satisfies the finite-variance condition. This means that any singularities in the solution $u(\\xi)$ must be integrable in the mean-square sense. For example, in a boundary value problem, the operator may become singular at certain resonant wavenumbers. A sufficient (but not necessary) condition to guarantee square-integrability is that the range of $k(\\xi)$ for $\\xi \\in \\mathbb{R}$ does not include any of these resonant wavenumbers, ensuring a bounded solution $u(\\xi)$ and thus a finite second moment. More generally, even if $u(\\xi)$ has singularities (e.g., at $\\xi=-1$ where $\\epsilon=0$ or at resonant wavenumbers), the growth of the solution magnitude $|u(\\xi)|$ near these singularities must be slow enough for the integral $\\int_{-\\infty}^{\\infty} |u(\\xi)|^2 \\exp(-\\xi^2/(2\\sigma^2)) d\\xi$ to converge.\nIn summary, the fundamental convergence guarantee is the square-integrability of the solution, which is a combined property of the operator, boundary conditions, and the specific parameterization.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\hat{u}_{0} & \\sum_{k=1}^{p} \\hat{u}_{k}^{2} \\end{pmatrix}}\n$$", "id": "3341826"}, {"introduction": "This practice bridges the gap between PCE theory and its application to real-world problems, where we typically work with finite, noisy data. A critical task is to select a model that is \"just right\"—not too simple to be inaccurate, and not too complex to overfit the noise. This hands-on coding exercise guides you through implementing a complete, data-driven workflow for building a robust PCE surrogate, using statistical techniques like leave-one-out cross-validation to adaptively select its complexity. [@problem_id:3411033]", "problem": "Design and implement an a posteriori cross-validation estimator for the prediction error of a Polynomial Chaos Expansion (PCE) surrogate, and an adaptive selection rule for the total polynomial degree based on validation loss curves, in the context of inverse problems and data assimilation. Your implementation must be a complete, runnable program.\n\nYou must start from the following fundamental base:\n- A polynomial chaos expansion is a spectral representation of a square-integrable random quantity of interest using orthonormal polynomials adapted to the input distribution. For independent standard normal inputs, the univariate orthonormal basis is given by the probabilists' Hermite polynomials, normalized so that the multivariate basis, constructed as tensor products, is orthonormal with respect to the joint standard normal measure.\n- In least-squares regression with a fixed design matrix, the fitted predictor is a linear smoother. If the fitted values are written as a linear operator applied to the data vector, then leave-one-out cross-validation can be computed a posteriori using the diagonal of the smoothing matrix (hat matrix), without refitting the model multiple times.\n\nProblem specification:\n1. Consider a scalar forward model output represented as a function of an input random vector with independent standard normal components, denoted by $\\boldsymbol{\\Xi} \\in \\mathbb{R}^{d}$ with density $\\mathcal{N}(\\boldsymbol{0}, \\mathbf{I})$. Let the quantity of interest be $Y = g(\\boldsymbol{\\Xi}) + \\varepsilon$, where $\\varepsilon$ is an independent noise with zero mean and finite variance. The surrogate is a Polynomial Chaos Expansion of total degree $p$:\n   $$ Y \\approx \\sum_{\\boldsymbol{\\alpha} \\in \\mathbb{N}_{0}^{d}, \\lVert \\boldsymbol{\\alpha} \\rVert_{1} \\leq p} c_{\\boldsymbol{\\alpha}} \\, \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\Xi}), $$\n   where $\\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\Xi}) = \\prod_{j=1}^{d} \\psi_{\\alpha_{j}}(\\Xi_{j})$, and $\\psi_{n}$ are the univariate orthonormal probabilists' Hermite polynomials $\\mathrm{He}_{n}$ normalized by $\\sqrt{n!}$.\n\n2. Given $N$ independent and identically distributed samples $\\{\\boldsymbol{\\xi}^{(i)}, y^{(i)}\\}_{i=1}^{N}$ with $\\boldsymbol{\\xi}^{(i)} \\sim \\mathcal{N}(\\boldsymbol{0}, \\mathbf{I})$ and $y^{(i)} = g(\\boldsymbol{\\xi}^{(i)}) + \\varepsilon^{(i)}$, fit the PCE coefficients by Tikhonov-regularized least squares:\n   $$ \\widehat{\\mathbf{c}} = \\arg\\min_{\\mathbf{c}} \\big\\lVert \\mathbf{A} \\mathbf{c} - \\mathbf{y} \\big\\rVert_{2}^{2} + \\lambda \\lVert \\mathbf{c} \\rVert_{2}^{2}, $$\n   where $\\mathbf{A}_{i,m} = \\Psi_{\\boldsymbol{\\alpha}^{(m)}}(\\boldsymbol{\\xi}^{(i)})$ is the design matrix constructed from the multivariate orthonormal basis up to total degree $p$, $\\mathbf{y} = [y^{(1)}, \\dots, y^{(N)}]^{\\top}$, and $\\lambda > 0$ is a small regularization parameter.\n\n3. Derive and implement an a posteriori leave-one-out (LOO) cross-validation estimator of the root-mean-square prediction error using the smoothing matrix\n   $$ \\mathbf{S} = \\mathbf{A}(\\mathbf{A}^{\\top}\\mathbf{A} + \\lambda \\mathbf{I})^{-1}\\mathbf{A}^{\\top}, $$\n   and the residual vector $\\mathbf{r} = \\mathbf{y} - \\widehat{\\mathbf{y}}$ with $\\widehat{\\mathbf{y}} = \\mathbf{A} \\widehat{\\mathbf{c}}$, via the identity\n   $$ e_{i}^{\\mathrm{LOO}} = \\frac{r_{i}}{1 - S_{ii}}, \\quad \\mathrm{RMSE}_{\\mathrm{LOO}} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} \\left(e_{i}^{\\mathrm{LOO}}\\right)^{2}}. $$\n   For numerical stability in the implementation, if $\\lvert 1 - S_{ii} \\rvert$ is very small, you must avoid division by zero by replacing $1 - S_{ii}$ with a signed, small-magnitude surrogate.\n\n4. To make the loss scale-invariant, compute the normalized leave-one-out root-mean-square error (NRMSE) as\n   $$ \\mathrm{NRMSE}_{\\mathrm{LOO}} = \\frac{\\mathrm{RMSE}_{\\mathrm{LOO}}}{\\mathrm{std}(\\mathbf{y})}, $$\n   where $\\mathrm{std}(\\mathbf{y})$ is the sample standard deviation of the observed outputs.\n\n5. For adaptive model selection, compute $\\mathrm{NRMSE}_{\\mathrm{LOO}}(p)$ for all degrees $p \\in \\{0, 1, \\dots, p_{\\max}\\}$ and select the degree using the one-standard-error rule: let $p_{\\min}$ be a minimizer of $\\mathrm{NRMSE}_{\\mathrm{LOO}}(p)$, and let $\\mathrm{SE}(p)$ denote an estimate of the standard error of $\\mathrm{RMSE}_{\\mathrm{LOO}}(p)$. Select\n   $$ p^{\\star} = \\min \\left\\{ p \\in \\{0, \\dots, p_{\\max}\\} \\,:\\, \\mathrm{NRMSE}_{\\mathrm{LOO}}(p) \\leq \\mathrm{NRMSE}_{\\mathrm{LOO}}(p_{\\min}) + \\frac{\\mathrm{SE}(p_{\\min})}{\\mathrm{std}(\\mathbf{y})} \\right\\}. $$\n   You must estimate $\\mathrm{SE}(p)$ from the leave-one-out residuals $\\{e_{i}^{\\mathrm{LOO}}\\}_{i=1}^{N}$ via the delta method: with $Z_{i} = \\left(e_{i}^{\\mathrm{LOO}}\\right)^{2}$, $\\widehat{\\mu} = \\frac{1}{N}\\sum_{i=1}^{N} Z_{i}$, $\\widehat{\\sigma}_{Z}^{2} = \\frac{1}{N-1}\\sum_{i=1}^{N} (Z_{i} - \\widehat{\\mu})^{2}$,\n   $$ \\mathrm{SE}(\\mathrm{RMSE}_{\\mathrm{LOO}}) \\approx \\frac{\\sqrt{\\widehat{\\sigma}_{Z}^{2}/N}}{2 \\sqrt{\\widehat{\\mu}}}. $$\n\n6. Use the above to design a robust, numerically stable algorithm to:\n   - Construct the orthonormal multivariate Hermite basis up to total degree $p$ in dimension $d$.\n   - Assemble the design matrix and compute $\\widehat{\\mathbf{c}}$.\n   - Compute the a posteriori LOO NRMSE and its standard error estimate for each degree $p$.\n   - Select $p^{\\star}$ using the one-standard-error rule.\n\nTest suite:\nYou must implement the following three test cases, using a fixed random seed for reproducibility. For all trigonometric functions, use angles in radians. All inputs are independent standard normal.\n\n- Test case $1$ (smooth exponential, moderate data):\n  - Dimension $d = 2$.\n  - Sample size $N = 80$.\n  - Maximum degree $p_{\\max} = 8$.\n  - Function $g(\\boldsymbol{\\xi}) = \\exp(0.8 \\, \\xi_{1} + 0.2 \\, \\xi_{2})$.\n  - Additive noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$ with $\\sigma = 0.01$.\n\n- Test case $2$ (low data, near-saturation):\n  - Dimension $d = 3$.\n  - Sample size $N = 12$.\n  - Maximum degree $p_{\\max} = 5$.\n  - Function $g(\\boldsymbol{\\xi}) = \\xi_{1}^{2} + \\xi_{2} \\xi_{3}$.\n  - Additive noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$ with $\\sigma = 0.05$.\n\n- Test case $3$ (oscillatory with noise):\n  - Dimension $d = 2$.\n  - Sample size $N = 60$.\n  - Maximum degree $p_{\\max} = 10$.\n  - Function $g(\\boldsymbol{\\xi}) = \\sin(\\xi_{1}) + 0.1 \\cos(0.5 \\, \\xi_{2})$.\n  - Additive noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$ with $\\sigma = 0.05$.\n\nRandomness:\n- Use a fixed base seed $s_{0} = 20240513$.\n- For test case index $k \\in \\{0, 1, 2\\}$, initialize the random generator with seed $s_{k} = s_{0} + 1000 \\, k$.\n\nNumerical constraints:\n- Use a small Tikhonov regularization parameter $\\lambda = 10^{-10}$.\n- To avoid division by zero in the leave-one-out formula, replace any denominator $1 - S_{ii}$ with $\\operatorname{sign}(1 - S_{ii}) \\cdot \\max\\{\\lvert 1 - S_{ii} \\rvert, 10^{-12}\\}$.\n\nRequired program output:\n- For each test case, compute the selected degree $p^{\\star}$ and the corresponding $\\mathrm{NRMSE}_{\\mathrm{LOO}}(p^{\\star})$.\n- Your program should produce a single line of output containing the six results in order, as a comma-separated list enclosed in square brackets:\n  $$ [p^{\\star}_{1}, \\mathrm{NRMSE}_{1}, p^{\\star}_{2}, \\mathrm{NRMSE}_{2}, p^{\\star}_{3}, \\mathrm{NRMSE}_{3}] $$\n  where each $\\mathrm{NRMSE}$ must be a floating-point number. You may round each floating-point number to $6$ decimal places for readability.\n\nAdditional context for inverse problems and data assimilation:\n- In Bayesian inverse problems and data assimilation, PCE surrogates are often used to approximate forward models and their uncertainty propagation. An a posteriori cross-validation error estimator provides a data-driven way to assess and adapt surrogate complexity, preventing overfitting and stabilizing inference pipelines, especially when observational data are scarce or noisy.\n\nNo external input is allowed; your program must run as is and print the required output line.", "solution": "The objective is to design and implement a robust algorithm for the adaptive selection of the total polynomial degree $p$ for a Polynomial Chaos Expansion (PCE) surrogate model. This is accomplished within the context of inverse problems and data assimilation, where data-driven model validation is crucial. The methodology leverages a posteriori leave-one-out cross-validation (LOO-CV) for error estimation and the one-standard-error rule for model selection, ensuring a balance between model accuracy and complexity.\n\n### Principle-Based Algorithmic Design\n\nThe algorithm is constructed from a sequence of well-founded statistical and numerical principles.\n\n#### 1. Polynomial Chaos Expansion (PCE) Surrogate\n\nWe consider a quantity of interest $Y$ that is a function of a $d$-dimensional random vector $\\boldsymbol{\\Xi}$ whose components are independent standard normal random variables, i.e., $\\boldsymbol{\\Xi} \\sim \\mathcal{N}(\\boldsymbol{0}, \\mathbf{I})$. The model is given by $Y = g(\\boldsymbol{\\Xi}) + \\varepsilon$, where $\\varepsilon$ is observation noise. We approximate $g(\\boldsymbol{\\Xi})$ using a PCE of total degree $p$:\n$$ g(\\boldsymbol{\\Xi}) \\approx \\mathcal{M}_{p}(\\boldsymbol{\\Xi}) = \\sum_{\\boldsymbol{\\alpha} \\in \\mathcal{A}_{p,d}} c_{\\boldsymbol{\\alpha}} \\, \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\Xi}), $$\nwhere $\\mathcal{A}_{p,d} = \\{\\boldsymbol{\\alpha} \\in \\mathbb{N}_{0}^{d} : \\lVert \\boldsymbol{\\alpha} \\rVert_{1} \\leq p\\}$ is the set of multi-indices. The basis functions $\\Psi_{\\boldsymbol{\\alpha}}$ are multivariate orthonormal polynomials. For the standard normal measure, these are constructed as tensor products of univariate orthonormal probabilists' Hermite polynomials:\n$$ \\Psi_{\\boldsymbol{\\alpha}}(\\boldsymbol{\\Xi}) = \\prod_{j=1}^{d} \\psi_{\\alpha_{j}}(\\Xi_{j}) \\quad \\text{with} \\quad \\psi_{n}(x) = \\frac{\\mathrm{He}_{n}(x)}{\\sqrt{n!}}. $$\nThe probabilists' Hermite polynomials $\\mathrm{He}_n(x)$ are generated via the three-term recurrence relation: $\\mathrm{He}_{n+1}(x) = x \\mathrm{He}_n(x) - n \\mathrm{He}_{n-1}(x)$, with initial conditions $\\mathrm{He}_0(x)=1$ and $\\mathrm{He}_1(x)=x$.\n\n#### 2. Coefficient Estimation via Regularized Least Squares\n\nGiven a set of $N$ data pairs $\\{\\boldsymbol{\\xi}^{(i)}, y^{(i)}\\}_{i=1}^{N}$, the unknown coefficients $\\mathbf{c} = \\{c_{\\boldsymbol{\\alpha}}\\}_{\\boldsymbol{\\alpha} \\in \\mathcal{A}_{p,d}}$ are determined by solving a linear regression problem. To enhance numerical stability and prevent overfitting, particularly when the number of basis functions $P = |\\mathcal{A}_{p,d}|$ is close to or exceeds the number of samples $N$, we employ Tikhonov regularization (ridge regression). The coefficients are found by minimizing the regularized sum of squared errors:\n$$ \\widehat{\\mathbf{c}} = \\arg\\min_{\\mathbf{c}} \\left\\lVert \\mathbf{A} \\mathbf{c} - \\mathbf{y} \\right\\rVert_{2}^{2} + \\lambda \\lVert \\mathbf{c} \\rVert_{2}^{2}, $$\nwhere $\\mathbf{y} = [y^{(1)}, \\dots, y^{(N)}]^{\\top}$ is the vector of observations, $\\lambda > 0$ is the regularization parameter, and $\\mathbf{A}$ is the $N \\times P$ design matrix with entries $\\mathbf{A}_{i,m} = \\Psi_{\\boldsymbol{\\alpha}^{(m)}}(\\boldsymbol{\\xi}^{(i)})$. The analytical solution to this optimization problem is given by the normal equations:\n$$ \\widehat{\\mathbf{c}} = (\\mathbf{A}^{\\top}\\mathbf{A} + \\lambda \\mathbf{I})^{-1}\\mathbf{A}^{\\top}\\mathbf{y}. $$\n\n#### 3. A Posteriori Leave-One-Out Cross-Validation (LOO-CV)\n\nThe primary goal is to estimate the model's prediction error on unseen data. LOO-CV provides a nearly unbiased estimate of this error. For a linear smoother, such as the one resulting from ridge regression, the LOO-CV errors can be computed efficiently without refitting the model $N$ times. The vector of fitted values is $\\widehat{\\mathbf{y}} = \\mathbf{A}\\widehat{\\mathbf{c}} = \\mathbf{S}\\mathbf{y}$, where\n$$ \\mathbf{S} = \\mathbf{A}(\\mathbf{A}^{\\top}\\mathbf{A} + \\lambda \\mathbf{I})^{-1}\\mathbf{A}^{\\top} $$\nis the smoothing (or \"hat\") matrix. The LOO prediction error for the $i$-th data point, $e_{i}^{\\mathrm{LOO}} = y^{(i)} - \\widehat{y}^{(-i)}$, where $\\widehat{y}^{(-i)}$ is the prediction at $\\boldsymbol{\\xi}^{(i)}$ from a model trained on all data except the $i$-th point, is given by the identity:\n$$ e_{i}^{\\mathrm{LOO}} = \\frac{y^{(i)} - \\widehat{y}^{(i)}}{1 - S_{ii}} = \\frac{r_{i}}{1 - S_{ii}}, $$\nwhere $r_i$ is the $i$-th ordinary residual and $S_{ii}$ is the $i$-th diagonal element of the smoothing matrix. The Leave-One-Out Root-Mean-Square Error is then:\n$$ \\mathrm{RMSE}_{\\mathrm{LOO}} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} \\left(e_{i}^{\\mathrm{LOO}}\\right)^{2}}. $$\nFor numerical stability, if a denominator $1 - S_{ii}$ is close to zero, it is replaced by $\\operatorname{sign}(1 - S_{ii}) \\cdot \\max\\{\\lvert 1 - S_{ii} \\rvert, \\epsilon\\}$ for a small tolerance $\\epsilon = 10^{-12}$.\n\nTo compute the diagonal elements $S_{ii}$ efficiently, we avoid forming the full $N \\times N$ matrix $\\mathbf{S}$. Let $\\mathbf{M} = \\mathbf{A}^{\\top}\\mathbf{A} + \\lambda \\mathbf{I}$. We compute its Cholesky decomposition $\\mathbf{M} = \\mathbf{L}\\mathbf{L}^{\\top}$. Then $\\mathbf{S} = \\mathbf{A}(\\mathbf{L}\\mathbf{L}^{\\top})^{-1}\\mathbf{A}^{\\top} = (\\mathbf{A}(\\mathbf{L}^{\\top})^{-1})(\\mathbf{L}^{-1}\\mathbf{A}^{\\top}) = \\mathbf{V}\\mathbf{V}^{\\top}$, where $\\mathbf{V} = \\mathbf{A}(\\mathbf{L}^{\\top})^{-1}$. The columns of $\\mathbf{V}^{\\top}$ can be found by solving the triangular system $\\mathbf{L}\\mathbf{v}'_i = \\mathbf{a}_i$ for each column $\\mathbf{a}_i$ of $\\mathbf{A}^\\top$. Equivalently, we solve $\\mathbf{L} \\mathbf{U} = \\mathbf{A}^\\top$ for $\\mathbf{U}$, and then set $\\mathbf{V}=\\mathbf{U}^\\top$. The diagonal elements are then $S_{ii} = \\sum_{j=1}^P V_{ij}^2$, which is the sum of squares of the elements in the $i$-th row of $\\mathbf{V}$.\n\n#### 4. Adaptive Degree Selection via the One-Standard-Error Rule\n\nTo select the most appropriate model complexity, we compute the Normalized LOO-RMSE (NRMSE) for a range of polynomial degrees $p \\in \\{0, 1, \\dots, p_{\\max}\\}$:\n$$ \\mathrm{NRMSE}_{\\mathrm{LOO}}(p) = \\frac{\\mathrm{RMSE}_{\\mathrm{LOO}}(p)}{\\mathrm{std}(\\mathbf{y})}, $$\nwhere $\\mathrm{std}(\\mathbf{y})$ is the sample standard deviation of observations. This normalization makes the error metric scale-invariant.\n\nThe one-standard-error rule is a heuristic that favors parsimony. It selects the simplest model whose performance is statistically comparable to the best model. Let $p_{\\min}$ be the degree that minimizes $\\mathrm{NRMSE}_{\\mathrm{LOO}}(p)$. We select the optimal degree $p^{\\star}$ as the smallest degree $p$ whose error is within one standard error of the minimum error:\n$$ p^{\\star} = \\min \\left\\{ p \\in \\{0, \\dots, p_{\\max}\\} \\,:\\, \\mathrm{NRMSE}_{\\mathrm{LOO}}(p) \\leq \\mathrm{NRMSE}_{\\mathrm{LOO}}(p_{\\min}) + \\frac{\\mathrm{SE}(p_{\\min})}{\\mathrm{std}(\\mathbf{y})} \\right\\}. $$\nThe standard error of $\\mathrm{RMSE}_{\\mathrm{LOO}}$, denoted $\\mathrm{SE}(p)$, is estimated using the delta method. Let $Z_i = (e_{i}^{\\mathrm{LOO}})^2$. Then $\\mathrm{RMSE}_{\\mathrm{LOO}} = \\sqrt{\\widehat{\\mu}_Z}$, where $\\widehat{\\mu}_Z = \\frac{1}{N}\\sum Z_i$. The standard error of $\\sqrt{\\widehat{\\mu}_Z}$ is approximately:\n$$ \\mathrm{SE}(\\mathrm{RMSE}_{\\mathrm{LOO}}) \\approx \\frac{\\mathrm{SE}(\\widehat{\\mu}_Z)}{2\\sqrt{\\widehat{\\mu}_Z}} = \\frac{\\sqrt{\\widehat{\\sigma}_{Z}^{2}/N}}{2 \\sqrt{\\widehat{\\mu}_Z}}, $$\nwhere $\\widehat{\\sigma}_{Z}^{2} = \\frac{1}{N-1}\\sum_{i=1}^{N} (Z_i - \\widehat{\\mu}_Z)^2$ is the sample variance of the squared LOO errors.\n\nThe final algorithm proceeds by iterating through degrees $p$ from $0$ to $p_{\\max}$. For each degree, it constructs the basis, assembles the design matrix, solves the regularized system, computes the LOO NRMSE and its standard error, and finally applies the one-standard-error rule to the resulting error curve to determine $p^{\\star}$.", "answer": "```python\nimport numpy as np\nfrom scipy.special import eval_hermite, factorial\nfrom scipy.linalg import solve_triangular\nimport math\n\ndef generate_multi_indices(d, p_max):\n    \"\"\"\n    Generates all multi-indices of dimension d with L1 norm up to p_max.\n    \"\"\"\n    if d == 1:\n        return [[i] for i in range(p_max + 1)]\n    \n    indices = []\n    \n    def _recursive_gen(dim_left, p_left, current_index):\n        if dim_left == 1:\n            for i in range(p_left + 1):\n                indices.append(current_index + [i])\n            return\n\n        for i in range(p_left + 1):\n            _recursive_gen(dim_left - 1, p_left - i, current_index + [i])\n\n    _recursive_gen(d, p_max, [])\n    return sorted(indices, key=sum)\n\ndef solve_case(d, N, p_max, g_func, sigma, seed, lambda_reg, denominator_tol):\n    \"\"\"\n    Solves a single test case for PCE model selection.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    xi_samples = rng.normal(size=(N, d))\n    noise = rng.normal(0, sigma, size=N)\n    y_true = np.apply_along_axis(g_func, 1, xi_samples)\n    y = y_true + noise\n    \n    # Use sample standard deviation (ddof=1)\n    y_std = np.std(y, ddof=1)\n    if y_std < 1e-15:  # Handle constant output\n        y_std = 1.0\n\n    nrmse_loo_curve = []\n    se_rmse_curve = []\n    \n    all_multi_indices_pmax = generate_multi_indices(d, p_max)\n    \n    # Pre-compute univariate polynomial values\n    # poly_vals[i, j, k] = psi_k(xi_samples[i, j])\n    max_univariate_deg = p_max\n    univariate_poly_vals = np.zeros((N, d, max_univariate_deg + 1))\n    for k in range(max_univariate_deg + 1):\n        norm_const = np.sqrt(factorial(k))\n        univariate_poly_vals[:, :, k] = eval_hermite(k, xi_samples) / norm_const\n\n    for p in range(p_max + 1):\n        multi_indices = [idx for idx in all_multi_indices_pmax if sum(idx) <= p]\n        P = len(multi_indices)\n\n        # Assemble design matrix A\n        A = np.zeros((N, P))\n        for m, alpha in enumerate(multi_indices):\n            term = np.ones(N)\n            for j in range(d):\n                term *= univariate_poly_vals[:, j, alpha[j]]\n            A[:, m] = term\n\n        # Solve for coefficients and compute LOO error\n        try:\n            if P == 0: # Should not happen for p>=0\n                raise np.linalg.LinAlgError(\"No basis functions.\")\n            \n            # Ridge regression solution\n            # c_hat = (A.T @ A + lambda_reg * I)^-1 @ A.T @ y\n            M = A.T @ A + lambda_reg * np.identity(P)\n            c_hat = np.linalg.solve(M, A.T @ y)\n            y_hat = A @ c_hat\n            residuals = y - y_hat\n\n            # Efficiently compute diagonal of hat matrix S\n            # S = A @ (A.T @ A + lambda_reg * I)^-1 @ A.T\n            L = np.linalg.cholesky(M)\n            V = solve_triangular(L, A.T, lower=True).T\n            S_diag = np.sum(V**2, axis=1)\n\n            # Compute LOO residuals with numerical stability\n            denominators = 1.0 - S_diag\n            stable_denominators = np.sign(denominators) * np.maximum(\n                np.abs(denominators), denominator_tol\n            )\n            e_loo = residuals / stable_denominators\n            \n            # Compute RMSE_LOO and its standard error\n            Z = e_loo**2\n            mu_hat = np.mean(Z)\n            \n            if mu_hat < 1e-24: # Perfect fit, no error\n                rmse_loo = 0.0\n                se_rmse = 0.0\n            else:\n                rmse_loo = np.sqrt(mu_hat)\n                # Use sample variance (ddof=1)\n                sigma_Z_sq = np.var(Z, ddof=1)\n                se_rmse = np.sqrt(sigma_Z_sq / N) / (2.0 * rmse_loo)\n\n        except (np.linalg.LinAlgError, ValueError):\n            rmse_loo = np.inf\n            se_rmse = np.inf\n\n        nrmse_loo_curve.append(rmse_loo / y_std)\n        se_rmse_curve.append(se_rmse)\n\n    # One-standard-error rule for selecting p_star\n    nrmse_loo_curve = np.array(nrmse_loo_curve)\n    se_rmse_curve = np.array(se_rmse_curve)\n    \n    if np.all(np.isinf(nrmse_loo_curve)):\n        p_star = 0 # Default to simplest model if all fail\n    else:\n        p_min_idx = np.nanargmin(nrmse_loo_curve)\n        nrmse_min = nrmse_loo_curve[p_min_idx]\n        se_min = se_rmse_curve[p_min_idx]\n        se_nrmse_min = se_min / y_std\n    \n        threshold = nrmse_min + se_nrmse_min\n        \n        # Find simplest model (smallest p) within the threshold\n        candidate_indices = np.where(nrmse_loo_curve <= threshold)[0]\n        p_star = candidate_indices[0] if len(candidate_indices) > 0 else p_min_idx\n\n    nrmse_at_p_star = nrmse_loo_curve[p_star]\n\n    return p_star, nrmse_at_p_star\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    base_seed = 20240513\n    lambda_reg = 1e-10\n    denominator_tol = 1e-12\n\n    test_cases = [\n        {\n            \"d\": 2, \"N\": 80, \"p_max\": 8,\n            \"g_func\": lambda xi: np.exp(0.8 * xi[0] + 0.2 * xi[1]),\n            \"sigma\": 0.01,\n            \"seed\": base_seed + 1000 * 0\n        },\n        {\n            \"d\": 3, \"N\": 12, \"p_max\": 5,\n            \"g_func\": lambda xi: xi[0]**2 + xi[1] * xi[2],\n            \"sigma\": 0.05,\n            \"seed\": base_seed + 1000 * 1\n        },\n        {\n            \"d\": 2, \"N\": 60, \"p_max\": 10,\n            \"g_func\": lambda xi: np.sin(xi[0]) + 0.1 * np.cos(0.5 * xi[1]),\n            \"sigma\": 0.05,\n            \"seed\": base_seed + 1000 * 2\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        p_star, nrmse = solve_case(\n            case[\"d\"], case[\"N\"], case[\"p_max\"], case[\"g_func\"],\n            case[\"sigma\"], case[\"seed\"], lambda_reg, denominator_tol\n        )\n        results.append(p_star)\n        results.append(f\"{nrmse:.6f}\")\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3411033"}]}