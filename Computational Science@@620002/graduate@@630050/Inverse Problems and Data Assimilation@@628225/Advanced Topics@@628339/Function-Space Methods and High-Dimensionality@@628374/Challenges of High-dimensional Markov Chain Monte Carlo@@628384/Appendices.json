{"hands_on_practices": [{"introduction": "To truly grasp the challenges of high-dimensional MCMC, we must first quantify the problem. This exercise provides a foundational, first-principles derivation of the so-called \"curse of dimensionality\" for the workhorse Random Walk Metropolis algorithm [@problem_id:3370964]. By analyzing the algorithm's diffusion limit, you will calculate how its statistical efficiency, measured by the Effective Sample Size (ESS), deteriorates as the state dimension $d$ grows, revealing the mathematical roots of why simple samplers fail.", "problem": "Consider a linear-Gaussian data assimilation problem in which the posterior distribution for the discretized state is a $d$-dimensional standard normal target $\\mathcal{N}(0, I_{d})$ after appropriate whitening through prior-preconditioning and observation operator linearization. You run a Random Walk Metropolis (RWM) Markov Chain Monte Carlo (MCMC) algorithm with Gaussian proposals $y = x + \\sigma_{d} \\,\\xi$, where $\\xi \\sim \\mathcal{N}(0, I_{d})$ is independent of the current state $x$, and the proposal variance is scaled as $\\sigma_{d}^{2} = \\ell^{2}/d$ with a fixed $\\ell > 0$. Assume the chain is started in stationarity. Let $f(x) = v^{\\top} x$ be a scalar linear functional with $\\|v\\|_{2} = 1$.\n\nUse only foundational principles and well-tested facts as the starting point, including:\n- The Metropolis-Hastings acceptance probability with symmetric proposals.\n- The law of large numbers for high dimension $d$ under the above scaling.\n- The classical diffusion limit for Random Walk Metropolis targeting an independent and identically distributed product measure, which yields, under the time-rescaling by $d$, an Ornstein–Uhlenbeck (OU) process limit for any fixed coordinate or fixed unit-direction projection.\n- The definition of Effective Sample Size (ESS): for a stationary scalar time series with lag-$k$ autocorrelation $\\rho_{k}$, the integrated autocorrelation time is $\\tau_{\\mathrm{int}} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_{k}$ and the effective sample size for $n$ iterations is $\\mathrm{ESS}_{n} = n/\\tau_{\\mathrm{int}}$.\n\nStarting from these bases, derive the lag-$k$ autocorrelation of $f(x)$ and then compute $\\mathrm{ESS}_{n}$ as an explicit function of $n$, $d$, and $\\ell$. Your derivation should:\n- Determine the high-dimensional acceptance rate in terms of the standard normal cumulative distribution function $\\Phi(\\cdot)$.\n- Identify the OU limit generator coefficient $g(\\ell)$ from first principles by matching the infinitesimal drift and variance in the limit.\n- Use this to obtain a closed-form expression for the lag-$k$ autocorrelation $\\rho_{k}$ and then sum the resulting geometric series for $\\tau_{\\mathrm{int}}$.\n- Express the final $\\mathrm{ESS}_{n}(d,\\ell)$ in closed form using only $n$, $d$, $\\ell$, elementary functions, and $\\Phi(\\cdot)$.\n\nAnswer form: provide a single closed-form analytic expression for $\\mathrm{ESS}_{n}(d,\\ell)$. No numerical approximation or rounding is required. Use standard mathematical notation and define any special functions you introduce. Do not include physical units. Assume angles are dimensionless.", "solution": "The problem is rigorously defined, self-contained, and scientifically grounded in the established theory of Markov chain Monte Carlo (MCMC) methods and their diffusion limits in high dimensions. All provided data and definitions are standard within this field. The problem statement is valid.\n\nThe objective is to derive the Effective Sample Size, $\\mathrm{ESS}_{n}$, for a scalar linear functional $f(x) = v^{\\top} x$ of a state evolving under a Random Walk Metropolis (RWM) algorithm. The target distribution is a $d$-dimensional standard normal, $\\pi(x) \\propto \\exp(-\\frac{1}{2} x^{\\top}x)$.\n\nStep 1: High-dimensional acceptance rate\nThe RWM proposal is $y = x + \\sigma_{d} \\xi$, where $\\xi \\sim \\mathcal{N}(0, I_{d})$ and the proposal variance is $\\sigma_{d}^{2} = \\ell^{2}/d$. Since the proposal distribution is symmetric, $q(y|x) = q(x|y)$, the Metropolis-Hastings acceptance probability is $\\alpha(x, y) = \\min\\left(1, \\frac{\\pi(y)}{\\pi(x)}\\right)$.\n\nThe ratio of target densities is:\n$$ \\frac{\\pi(y)}{\\pi(x)} = \\frac{\\exp(-\\frac{1}{2}y^{\\top}y)}{\\exp(-\\frac{1}{2}x^{\\top}x)} = \\exp\\left(-\\frac{1}{2}(y^{\\top}y - x^{\\top}x)\\right) $$\nSubstituting $y = x + \\sigma_d \\xi$:\n$$ y^{\\top}y = (x + \\sigma_d \\xi)^{\\top}(x + \\sigma_d \\xi) = x^{\\top}x + 2\\sigma_d x^{\\top}\\xi + \\sigma_d^2 \\xi^{\\top}\\xi $$\nThus, the argument of the exponential becomes:\n$$ \\log\\left(\\frac{\\pi(y)}{\\pi(x)}\\right) = -\\sigma_d x^{\\top}\\xi - \\frac{1}{2}\\sigma_d^2 \\xi^{\\top}\\xi $$\nWe analyze the behavior of the terms in the exponent in the limit $d \\rightarrow \\infty$. The chain is assumed to be in stationarity, so $x \\sim \\mathcal{N}(0, I_{d})$.\nThe second term is $\\frac{1}{2}\\sigma_d^2 \\xi^{\\top}\\xi = \\frac{1}{2} \\frac{\\ell^2}{d} \\sum_{i=1}^{d} \\xi_i^2$. By the Law of Large Numbers, as $d \\to \\infty$, $\\frac{1}{d}\\sum_{i=1}^{d} \\xi_i^2 \\to \\mathbb{E}[\\xi_1^2] = 1$. This term converges in probability to $\\frac{1}{2}\\ell^2$.\nThe first term is $\\sigma_d x^{\\top}\\xi = \\frac{\\ell}{\\sqrt{d}} \\sum_{i=1}^{d} x_i \\xi_i$. Since $x$ and $\\xi$ are independent and standard normal, the terms $x_i \\xi_i$ are independent and identically distributed with mean $\\mathbb{E}[x_i\\xi_i]=\\mathbb{E}[x_i]\\mathbb{E}[\\xi_i]=0$ and variance $\\mathrm{Var}(x_i\\xi_i) = \\mathbb{E}[(x_i\\xi_i)^2] - (\\mathbb{E}[x_i\\xi_i])^2 = \\mathbb{E}[x_i^2]\\mathbb{E}[\\xi_i^2] = 1 \\cdot 1 = 1$. By the Central Limit Theorem, the sum $\\frac{1}{\\sqrt{d}} \\sum_{i=1}^{d} x_i \\xi_i$ converges in distribution to a standard normal variable $U \\sim \\mathcal{N}(0, 1)$. Therefore, $\\sigma_d x^{\\top}\\xi$ converges in distribution to $\\mathcal{N}(0, \\ell^2)$. Let $Z \\sim \\mathcal{N}(0, \\ell^2)$.\n\nIn the high-dimensional limit, the acceptance probability becomes independent of the specific state $x$, and the average acceptance rate $\\bar{\\alpha}(\\ell)$ is given by the expectation over the limiting distribution of the log-ratio:\n$$ \\bar{\\alpha}(\\ell) = \\mathbb{E}_{Z}\\left[\\min\\left(1, \\exp\\left(-Z - \\frac{\\ell^2}{2}\\right)\\right)\\right] $$\nwhere $Z \\sim \\mathcal{N}(0, \\ell^2)$. Let $Z = \\ell U$ where $U \\sim \\mathcal{N}(0, 1)$. Let $\\phi(u)$ be the standard normal PDF.\n$$ \\bar{\\alpha}(\\ell) = \\int_{-\\infty}^{\\infty} \\min\\left(1, \\exp\\left(-\\ell u - \\frac{\\ell^2}{2}\\right)\\right) \\phi(u) du $$\nThe term $\\exp(-\\ell u - \\ell^2/2)$ is greater than $1$ when its argument is positive, i.e., $-\\ell u - \\ell^2/2 > 0$, which implies $u < -\\ell/2$. So we split the integral:\n$$ \\bar{\\alpha}(\\ell) = \\int_{-\\infty}^{-\\ell/2} 1 \\cdot \\phi(u) du + \\int_{-\\ell/2}^{\\infty} \\exp\\left(-\\ell u - \\frac{\\ell^2}{2}\\right) \\phi(u) du $$\nThe first integral is the definition of the standard normal cumulative distribution function (CDF), $\\Phi(-\\ell/2)$.\nFor the second integral, we substitute $\\phi(u) = \\frac{1}{\\sqrt{2\\pi}}\\exp(-u^2/2)$ and complete the square in the exponent:\n$$ -\\ell u - \\frac{\\ell^2}{2} - \\frac{u^2}{2} = -\\frac{1}{2}(u^2 + 2\\ell u + \\ell^2) = -\\frac{1}{2}(u+\\ell)^2 $$\nThe second integral becomes $\\int_{-\\ell/2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{1}{2}(u+\\ell)^2) du$. Let $v=u+\\ell$, then $dv=du$. The limits of integration change from $u=-\\ell/2 \\to v=\\ell/2$ and $u \\to \\infty \\to v \\to \\infty$.\n$$ \\int_{\\ell/2}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{v^2}{2}\\right) dv = P(V > \\ell/2) = 1 - \\Phi(\\ell/2) $$\nwhere $V \\sim \\mathcal{N}(0, 1)$. Due to the symmetry of the standard normal distribution, $1 - \\Phi(z) = \\Phi(-z)$, so this integral is $\\Phi(-\\ell/2)$.\nCombining both parts, the asymptotic acceptance rate is:\n$$ \\bar{\\alpha}(\\ell) = \\Phi(-\\ell/2) + \\Phi(-\\ell/2) = 2\\Phi(-\\ell/2) $$\n\nStep 2: Ornstein-Uhlenbeck (OU) limit and autocorrelation\nThe problem states that under the time rescaling $t=k/d$, the process for any fixed unit-direction projection converges to an OU process. Let $F_t = f(x_{\\lfloor dt \\rfloor}) = v^{\\top}x_{\\lfloor dt \\rfloor}$. Since the target distribution $\\mathcal{N}(0, I_d)$ is isotropic and $\\|v\\|_2=1$, we can choose a coordinate system where $v = (1, 0, ..., 0)^{\\top}$, so $F_t$ is simply the first coordinate of the state vector.\n\nThe limiting SDE for a coordinate process is of the form $dF_t = -\\theta F_t dt + \\sigma dW_t$. The stationary distribution of this process is $\\mathcal{N}(0, \\sigma^2/(2\\theta))$. We require this to match the stationary distribution of $f(x) = v^{\\top}x$, which is $\\mathcal{N}(0, v^{\\top}I_d v) = \\mathcal{N}(0,1)$. Thus, we must have $\\sigma^2=2\\theta$.\n\nThe diffusion coefficient $\\sigma^2$ is found by calculating the rescaled expected squared jump distance:\n$$ \\sigma^2 = \\lim_{d \\to \\infty} d \\cdot \\mathbb{E}[ (f(x_{k+1}) - f(x_k))^2 | x_k=x ] $$\nThe change is $f(x_{k+1}) - f(x_k) = v^{\\top}(x_{k+1}-x_k)$. The state update is $x_{k+1} = x_k + \\sigma_d \\xi \\cdot \\mathbb{I}(\\text{accept})$, where $\\mathbb{I}$ is an indicator for acceptance.\n$$ \\mathbb{E}[(v^{\\top}(\\sigma_d \\xi \\cdot \\mathbb{I}(\\text{accept})))^2 | x] = \\sigma_d^2 \\mathbb{E}[(v^{\\top}\\xi)^2 \\alpha(x, x+\\sigma_d \\xi) | x] $$\nIn the limit $d \\to \\infty$, $\\alpha$ converges to the constant $\\bar{\\alpha}(\\ell)$. Thus:\n$$ \\sigma^2 \\approx d \\cdot \\sigma_d^2 \\mathbb{E}[(v^{\\top}\\xi)^2] \\bar{\\alpha}(\\ell) = d \\cdot \\frac{\\ell^2}{d} \\cdot \\mathrm{Var}(v^{\\top}\\xi) \\cdot \\bar{\\alpha}(\\ell) $$\nSince $\\xi \\sim \\mathcal{N}(0, I_d)$ and $\\|v\\|_2=1$, $\\mathrm{Var}(v^{\\top}\\xi) = v^{\\top}\\mathrm{Var}(\\xi)v = v^{\\top}I_d v = \\|v\\|_2^2=1$.\nTherefore, $\\sigma^2 = \\ell^2 \\bar{\\alpha}(\\ell) = 2\\ell^2 \\Phi(-\\ell/2)$.\nFrom the stationarity condition $\\sigma^2 = 2\\theta$, the drift coefficient is $\\theta = \\frac{1}{2}\\sigma^2 = \\ell^2 \\Phi(-\\ell/2)$.\nThis corresponds to the coefficient $g(\\ell)$ in the problem's implicit framing. The SDE is $dF_t = -(\\ell^2 \\Phi(-\\ell/2)) F_t dt + \\sqrt{2\\ell^2 \\Phi(-\\ell/2)} dW_t$.\n\nThe autocorrelation function for an OU process with drift coefficient $\\theta$ is $\\rho(\\tau) = \\exp(-\\theta \\tau)$. For the discrete MCMC chain, the time lag $\\tau$ corresponds to $k/d$ steps. The lag-$k$ autocorrelation is:\n$$ \\rho_k = \\rho(k/d) = \\exp(-\\theta k/d) = \\exp\\left(-k \\frac{\\ell^2 \\Phi(-\\ell/2)}{d}\\right) $$\n\nStep 3: Integrated Autocorrelation Time ($\\tau_{\\mathrm{int}}$) and ESS\nThe integrated autocorrelation time is defined as $\\tau_{\\mathrm{int}} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_{k}$. The sum is a geometric series. Let $r = \\rho_1 = \\exp\\left(-\\frac{\\ell^2 \\Phi(-\\ell/2)}{d}\\right)$.\n$$ \\sum_{k=1}^{\\infty} \\rho_k = \\sum_{k=1}^{\\infty} r^k = \\frac{r}{1-r} $$\nSubstituting this into the expression for $\\tau_{\\mathrm{int}}$:\n$$ \\tau_{\\mathrm{int}} = 1 + 2\\frac{r}{1-r} = \\frac{(1-r) + 2r}{1-r} = \\frac{1+r}{1-r} $$\nSubstituting the expression for $r$:\n$$ \\tau_{\\mathrm{int}} = \\frac{1 + \\exp\\left(-\\frac{\\ell^2 \\Phi(-\\ell/2)}{d}\\right)}{1 - \\exp\\left(-\\frac{\\ell^2 \\Phi(-\\ell/2)}{d}\\right)} $$\nThis expression can be written using the hyperbolic cotangent function, $\\coth(z) = \\frac{\\exp(z)+\\exp(-z)}{\\exp(z)-\\exp(-z)} = \\frac{\\exp(2z)+1}{\\exp(2z)-1}$. Let $z = \\frac{\\ell^2 \\Phi(-\\ell/2)}{2d}$. Then $\\tau_{\\mathrm{int}} = \\coth(z)$.\n$$ \\tau_{\\mathrm{int}} = \\coth\\left(\\frac{\\ell^2 \\Phi(-\\ell/2)}{2d}\\right) $$\nThe Effective Sample Size for $n$ iterations is $\\mathrm{ESS}_n = n/\\tau_{\\mathrm{int}}$.\n$$ \\mathrm{ESS}_{n}(d,\\ell) = \\frac{n}{\\coth\\left(\\frac{\\ell^2 \\Phi(-\\ell/2)}{2d}\\right)} = n \\tanh\\left(\\frac{\\ell^2 \\Phi(-\\ell/2)}{2d}\\right) $$\nThis provides the final closed-form expression for $\\mathrm{ESS}_{n}$ as a function of $n$, $d$, and $\\ell$, using elementary functions and the standard normal CDF $\\Phi$.", "answer": "$$\n\\boxed{n \\tanh\\left(\\frac{\\ell^{2} \\Phi(-\\ell/2)}{2d}\\right)}\n$$", "id": "3370964"}, {"introduction": "Faced with poor scaling, a natural response is to seek 'optimal' tuning rules, such as the famous target acceptance rate of $0.234$ for RWM. This practice encourages a critical perspective, revealing that such rules optimize for local exploration (Expected Squared Jump Distance) which does not always maximize the global mixing measured by ESS [@problem_id:3370957]. Through a carefully constructed counterexample, you will demonstrate that blindly applying these rules can be suboptimal, fostering a deeper understanding of what 'sampler efficiency' truly means.", "problem": "Consider sampling high-dimensional posterior distributions arising in inverse problems and data assimilation, where the dimension $d$ is large. Let $X_t \\in \\mathbb{R}^d$ denote a Markov chain with stationary distribution $\\pi$ (the posterior), generated by a Metropolis–Hastings method. Two widely cited acceptance rate targets are approximately $0.234$ for the Random-Walk Metropolis (RWM) algorithm and approximately $0.574$ for the Metropolis-Adjusted Langevin Algorithm (MALA). In this problem, you will reason from first principles to clarify why these particular acceptance rates arise, and to demonstrate that effective sample size (ESS) need not be maximized at those rates.\n\nDefinitions:\n- The expected squared jump distance (ESJD) is $J = \\mathbb{E}\\left[\\lVert X_{t+1} - X_t \\rVert^2\\right]$ under the stationary chain.\n- For a scalar functional $h:\\mathbb{R}^d \\to \\mathbb{R}$ with stationary variance $\\mathbb{V}_\\pi[h(X)] > 0$, the lag-$k$ autocorrelation is $\\rho_k = \\operatorname{Corr}\\left(h(X_t), h(X_{t+k})\\right)$, and the integrated autocorrelation time (IACT) is $\\tau_{\\mathrm{int}} = 1 + 2\\sum_{k=1}^{\\infty} \\rho_k$ when the series converges. The effective sample size over $N$ iterations is $\\mathrm{ESS}(h) = N / \\tau_{\\mathrm{int}}$.\n\nFacts base you may use:\n- In high dimensions with isotropic targets and appropriately scaled proposals, RWM and MALA admit diffusion limits. The proposal scaling that maximizes the limiting speed of the diffusion (equivalently, the ESJD) yields characteristic acceptance rates: approximately $0.234$ for RWM and approximately $0.574$ for MALA. These scalings and acceptance targets are well-tested results in the asymptotic analysis of high-dimensional Markov chains.\n- Mixtures of Markov kernels that each preserve $\\pi$ also preserve $\\pi$.\n\nConstruct the following valid Markov chain on a one-dimensional standard normal target $\\pi = \\mathcal{N}(0,1)$ to test ESS versus acceptance:\n- At each iteration, with probability $\\delta \\in (0,1)$, perform an independence resampling: draw $Y \\sim \\pi$ and set $X_{t+1} = Y$ (this kernel preserves $\\pi$ and accepts with probability $1$).\n- With probability $(1-\\delta)p$, perform a reflection: set $X_{t+1} = -X_t$ (this mapping preserves $\\pi$ exactly, so it is accepted with probability $1$).\n- With probability $(1-\\delta)(1-p)$, do nothing: set $X_{t+1} = X_t$.\nDefine the per-iteration acceptance rate as $\\alpha = \\mathbb{P}(X_{t+1} \\neq X_t) = \\delta + (1-\\delta)p$. Let $h(x) = x$.\n\nTasks embedded in the choices below include establishing why the targets $0.234$ and $0.574$ arise from ESJD optimization in diffusion limits, and showing by the above construction that $\\mathrm{ESS}(h)$ can be strictly higher at acceptance rate $\\alpha = 1$ than at acceptance rate $\\alpha \\approx 0.234$.\n\nWhich of the following statements are correct?\n\nA. The acceptance rate targets $0.234$ (RWM) and $0.574$ (MALA) arise by maximizing the limiting expected squared jump distance in high-dimensional diffusion limits, not by maximizing $\\mathrm{ESS}(h)$ directly.\n\nB. For any Metropolis–Hastings algorithm and any scalar functional $h$, maximizing ESJD is equivalent to maximizing $\\mathrm{ESS}(h)$.\n\nC. There exists a valid target and Markov kernel (specifically, the above $\\pi = \\mathcal{N}(0,1)$ with the mixture of resampling and reflections) for which $\\mathrm{ESS}(h)$ is strictly larger at acceptance rate $\\alpha=1$ (choose $p=1$) than at acceptance rate $\\alpha \\approx 0.234$ (choose $p$ so that $\\delta + (1-\\delta)p \\approx 0.234$), showing that acceptance targets such as $0.234$ do not universally maximize $\\mathrm{ESS}(h)$.\n\nD. For any posterior in inverse problems and data assimilation, tuning an RWM sampler to achieve acceptance rate $0.234$ guarantees maximal $\\mathrm{ESS}(h)$ for all scalar functionals $h$.\n\nAnswer by selecting all correct options.", "solution": "The problem statement is critically validated and found to be valid. It is scientifically grounded in established MCMC theory, well-posed, objective, and internally consistent. It presents a standard theoretical result alongside a specific, well-defined counterexample to test the limits of that result's applicability. All terms are either standard or explicitly defined.\n\nThe core of this problem rests on the distinction between two different criteria for optimizing a Markov chain Monte Carlo (MCMC) sampler:\n1.  **Local Exploration Efficiency**, often measured by the Expected Squared Jump Distance (ESJD), $J = \\mathbb{E}\\left[\\lVert X_{t+1} - X_t \\rVert^2\\right]$. This quantifies how far, on average, the sampler moves in a single step.\n2.  **Global Mixing Efficiency**, measured for a specific functional $h$ by the Effective Sample Size (ESS), which is inversely proportional to the Integrated Autocorrelation Time (IACT), $\\tau_{\\mathrm{int}}$. This quantifies how quickly the sequence of samples $h(X_t)$ becomes uncorrelated, which determines the number of \"effective\" independent samples obtained.\n\nThe problem asks us to evaluate statements that probe the relationship between these two criteria.\n\n**A. The acceptance rate targets $0.234$ (RWM) and $0.574$ (MALA) arise by maximizing the limiting expected squared jump distance in high-dimensional diffusion limits, not by maximizing $\\mathrm{ESS}(h)$ directly.**\n\nThis statement is a correct summary of a foundational result in the theory of high-dimensional MCMC, as cited in the problem's \"Facts base.\" The analysis, pioneered by Roberts, Gelman, and Rosenthal, considers a Random Walk Metropolis (RWM) or Metropolis-Adjusted Langevin (MALA) algorithm targeting a high-dimensional distribution $\\pi$, often assumed to be of the form $\\pi(x) = \\prod_{i=1}^d f(x_i)$. In the limit as the dimension $d \\to \\infty$, and with an appropriate scaling of the proposal variance with $d$, the evolution of a single component of the Markov chain $X_t$ can be described by a stochastic differential equation (a diffusion process).\n\nThe speed of this limiting diffusion process is proportional to the ESJD of the original discrete-time chain. Maximizing this speed is a proxy for optimizing the sampler's exploration of the target distribution. The calculation shows that this speed (and thus the ESJD) is maximized when the average acceptance probability of the sampler is tuned to approximately $0.234$ for RWM and $0.574$ for MALA. The optimization is performed on the ESJD, not on the IACT or ESS for any particular functional $h$. While it is often hoped that maximizing ESJD will lead to good ESS, they are not the same optimization criterion.\n\n**Verdict: Correct.**\n\n**B. For any Metropolis–Hastings algorithm and any scalar functional $h$, maximizing ESJD is equivalent to maximizing $\\mathrm{ESS}(h)$.**\n\nThis statement makes a strong, universal claim of equivalence. Such a claim is generally false. The ESJD is a single, global measure of sampler performance, averaged over the entire state space and all dimensions. In contrast, $\\mathrm{ESS}(h)$ is specific to a chosen test function $h$. It is possible to have a high ESJD—meaning the sampler is taking large steps on average—while simultaneously having poor mixing for a particular functional $h$ (i.e., high IACT and low ESS). For example, a sampler could move rapidly between regions of the state space that map to similar values of $h$, while mixing slowly between regions where $h$ differs significantly. Option C provides a concrete mathematical counterexample to this claim of equivalence.\n\n**Verdict: Incorrect.**\n\n**C. There exists a valid target and Markov kernel (specifically, the above $\\pi = \\mathcal{N}(0,1)$ with the mixture of resampling and reflections) for which $\\mathrm{ESS}(h)$ is strictly larger at acceptance rate $\\alpha=1$ (choose $p=1$) than at acceptance rate $\\alpha \\approx 0.234$ (choose $p$ so that $\\delta + (1-\\delta)p \\approx 0.234$), showing that acceptance targets such as $0.234$ do not universally maximize $\\mathrm{ESS}(h)$.**\n\nTo verify this statement, we analyze the constructed Markov chain on the target $\\pi = \\mathcal{N}(0,1)$ with the functional $h(x) = x$. The stationary distribution of the chain is standard normal, so $\\mathbb{E}_\\pi[X] = 0$ and $\\mathbb{V}_\\pi[X] = \\mathbb{E}_\\pi[X^2] = 1$. The IACT for $h(x)=x$ is given by $\\tau_{\\mathrm{int}} = 1 + 2\\sum_{k=1}^{\\infty} \\rho_k$, where $\\rho_k = \\operatorname{Corr}(X_t, X_{t+k})$.\n\nFirst, we compute the lag-$1$ autocorrelation, $\\rho_1$. Since the chain is stationary and centered, $\\rho_1 = \\mathbb{E}[X_{t+1}X_t]$.\nWe compute this using the law of total expectation: $\\mathbb{E}[X_{t+1}X_t] = \\mathbb{E}_{X_t}[\\mathbb{E}[X_{t+1}X_t | X_t]]$.\nGiven $X_t = x$, the value of $X_{t+1}$ is determined as follows:\n- With probability $\\delta$, $X_{t+1} = Y \\sim \\mathcal{N}(0,1)$. The conditional product is $xY$. $\\mathbb{E}[xY] = x\\mathbb{E}[Y] = 0$.\n- With probability $(1-\\delta)p$, $X_{t+1} = -x$. The conditional product is $x(-x) = -x^2$.\n- With probability $(1-\\delta)(1-p)$, $X_{t+1} = x$. The conditional product is $x(x) = x^2$.\n\nThe conditional expectation of the product is:\n$$ \\mathbb{E}[X_{t+1}X_t | X_t=x] = \\delta(0) + (1-\\delta)p(-x^2) + (1-\\delta)(1-p)(x^2) = (1-\\delta)(1-2p)x^2 $$\nNow, we take the expectation over $X_t \\sim \\mathcal{N}(0,1)$, where $\\mathbb{E}[X_t^2]=1$:\n$$ \\rho_1 = \\mathbb{E}[X_{t+1}X_t] = \\mathbb{E}[(1-\\delta)(1-2p)X_t^2] = (1-\\delta)(1-2p)\\mathbb{E}[X_t^2] = (1-\\delta)(1-2p) $$\nThe conditional expectation $\\mathbb{E}[X_{t+1}|X_t=x] = (1-\\delta)(1-2p)x$ is linear in $x$. This implies that the autocorrelation function decays geometrically: $\\rho_k = \\rho_1^k$. The IACT is thus:\n$$ \\tau_{\\mathrm{int}} = 1 + 2\\sum_{k=1}^\\infty \\rho_1^k = \\frac{1+\\rho_1}{1-\\rho_1} = \\frac{1 + (1-\\delta)(1-2p)}{1 - (1-\\delta)(1-2p)} $$\nTo maximize $\\mathrm{ESS}(h)$, we must minimize $\\tau_{\\mathrm{int}}$. The function $\\tau(\\rho) = (1+\\rho)/(1-\\rho)$ is monotonically increasing for $\\rho \\in (-1, 1)$. Therefore, we must minimize $\\rho_1 = (1-\\delta)(1-2p)$. Since $\\delta \\in (0,1)$, $1-\\delta > 0$. Minimizing $\\rho_1$ is achieved by maximizing $p$. The maximum value for $p$ is $1$.\n\nCase 1: Maximum ESS.\nWe set $p=1$. The acceptance rate is $\\alpha = \\delta + (1-\\delta)(1) = 1$.\nThe lag-$1$ autocorrelation is $\\rho_1 = (1-\\delta)(1-2) = -(1-\\delta)$.\nThe IACT is $\\tau_{\\mathrm{int}}(\\alpha=1) = \\frac{1-(1-\\delta)}{1+(1-\\delta)} = \\frac{\\delta}{2-\\delta}$. For this family of kernels, $\\mathrm{ESS}(h)$ is maximal when acceptance rate is $1$.\n\nCase 2: The target acceptance rate $\\alpha \\approx 0.234$.\nWe must choose $p$ such that $\\alpha = \\delta + (1-\\delta)p \\approx 0.234$. Since $\\delta \\in (0,1)$, this requires $p<1$.\nSolving for $p$: $p = (\\alpha - \\delta)/(1-\\delta)$. To be a valid probability $p \\in [0,1)$, we can choose any $\\delta \\in (0, \\alpha)$.\nFor any such choice with $p<1$, the autocorrelation $\\rho_1 = (1-\\delta)(1-2p)$ will be strictly greater than its minimum value of $-(1-\\delta)$, which was achieved at $p=1$. A larger $\\rho_1$ leads to a strictly larger $\\tau_{\\mathrm{int}}$ and thus a strictly smaller $\\mathrm{ESS}(h)$.\n\nFor a concrete example, let $\\delta=0.1$.\n- If we set $p=1$, then $\\alpha = 1$. The IACT is $\\tau_{\\mathrm{int}} = \\frac{0.1}{2-0.1} = \\frac{0.1}{1.9} \\approx 0.0526$.\n- If we set $\\alpha=0.234$, we need $p = (0.234-0.1)/0.9 = 0.134/0.9 \\approx 0.1489$. The autocorrelation is $\\rho_1 = (0.9)(1 - 2(0.1489)) \\approx 0.9 \\times 0.7022 = 0.632$. The IACT is $\\tau_{\\mathrm{int}} = \\frac{1+0.632}{1-0.632} = \\frac{1.632}{0.368} \\approx 4.43$.\n\nClearly, $\\mathrm{ESS}(h)$ is orders of magnitude larger at $\\alpha=1$ than at $\\alpha=0.234$ for this specific, valid MCMC setup. This demonstrates that the optimal acceptance rate for ESS is not universally fixed at values like $0.234$.\n\n**Verdict: Correct.**\n\n**D. For any posterior in inverse problems and data assimilation, tuning an RWM sampler to achieve acceptance rate $0.234$ guarantees maximal $\\mathrm{ESS}(h)$ for all scalar functionals $h$.**\n\nThis statement is a significant and incorrect overgeneralization.\n1.  **\"For any posterior\"**: The $0.234$ result is asymptotic ($d \\to \\infty$) and derived under idealized assumptions (e.g., i.i.d. target components). For finite-dimensional, anisotropic, or multimodal posteriors commonly found in applications, the optimal rate for ESJD itself can differ, and the link to ESS is even more tenuous.\n2.  **\"guarantees maximal $\\mathrm{ESS}(h)$\"**: As established by A and C, the $0.234$ target maximizes the limiting ESJD, not ESS. They are not equivalent criteria.\n3.  **\"for all scalar functionals $h$\"**: The optimal sampler tuning can be highly dependent on the functional of interest, $h$. A sampler that mixes well for one quantity of interest may not for another. There is no single tuning that is guaranteed to be optimal for all possible observables.\n\nThe $0.234$ result should be understood as a useful heuristic or starting point for tuning an RWM sampler in high-dimensional problems, not as a guarantee of optimality in any sense, especially not for ESS.\n\n**Verdict: Incorrect.**", "answer": "$$\\boxed{AC}$$", "id": "3370957"}, {"introduction": "Perhaps the most insidious challenge in high-dimensional MCMC is knowing when a sampler has actually converged. Standard diagnostics can be dangerously misleading, suggesting convergence when the chain is merely exploring a subspace of the posterior. This hands-on coding exercise [@problem_id:3370944] puts you in the driver's seat to simulate this exact scenario, showing how a chain can mix well in a data-informed subspace while failing to explore its vast orthogonal complement, leading to a 'false convergence' that could severely bias posterior estimates.", "problem": "Consider a linear Gaussian inverse problem in $d$ dimensions with a Gaussian prior and Gaussian observational noise. Let the unknown parameter be $x \\in \\mathbb{R}^d$ with prior $x \\sim \\mathcal{N}(0, I_d)$. Let observations satisfy $y = A x + \\eta$, where $A \\in \\mathbb{R}^{m \\times d}$ is a known matrix and $\\eta \\sim \\mathcal{N}(0, \\sigma^2 I_m)$ is independent observational noise. The negative log-likelihood is $\\Phi(x) = \\frac{1}{2 \\sigma^2} \\lVert y - A x \\rVert_2^2$. The posterior density is proportional to $\\exp\\left(-\\frac{1}{2}\\lVert x \\rVert_2^2 - \\Phi(x)\\right)$.\n\nThe likelihood-informed subspace (LIS) for this linear problem with identity prior covariance can be taken as the span of the leading eigenvectors of the symmetric matrix $H = A^\\top \\Gamma^{-1} A$ with $\\Gamma = \\sigma^2 I_m$, that is $H = \\frac{1}{\\sigma^2} A^\\top A$. Given a threshold $\\tau > 0$, define the LIS basis $U \\in \\mathbb{R}^{d \\times r}$ as the matrix of orthonormal eigenvectors of $H$ corresponding to eigenvalues strictly greater than $\\tau$, so that $r = \\mathrm{rank}_\\tau(H)$ is the number of eigenvalues above $\\tau$. The orthogonal complement subspace has dimension $d - r$.\n\nTo sample from the posterior, consider $K$ independent parallel Markov chains using a blocked preconditioned Crank–Nicolson (pCN) proposal that mixes differently along LIS and its orthogonal complement. Let $P = U U^\\top$ be the orthogonal projector onto the LIS, and $Q = I_d - P$ be the projector onto the orthogonal complement. For current state $x \\in \\mathbb{R}^d$, define the proposal\n$$\nx' = \\sqrt{1 - \\beta_{\\mathrm{LIS}}^2}\\, P x + \\beta_{\\mathrm{LIS}}\\, \\xi_{\\mathrm{LIS}} \\;+\\; \\sqrt{1 - \\beta_{\\perp}^2}\\, Q x + \\beta_{\\perp}\\, \\xi_{\\perp},\n$$\nwhere $\\xi_{\\mathrm{LIS}} \\sim \\mathcal{N}(0, P)$ and $\\xi_{\\perp} \\sim \\mathcal{N}(0, Q)$ are independent Gaussian vectors supported on their respective subspaces. Perform a Metropolis–Hastings accept/reject step with acceptance probability\n$$\n\\alpha(x, x') = \\min\\left(1, \\exp(-\\Phi(x') + \\Phi(x))\\right),\n$$\nso that the prior-invariant proposal combined with this acceptance targets the posterior.\n\nUse $n_{\\mathrm{tot}}$ steps per chain with a discarded burn-in of $n_{\\mathrm{burn}}$ steps. Denote by $\\{x_{j,t}\\}_{t=1}^{n}$, $j=1,\\dots,K$, the post burn-in samples with $n = n_{\\mathrm{tot}} - n_{\\mathrm{burn}}$. Define the LIS coordinates $z_{j,t} = U^\\top x_{j,t} \\in \\mathbb{R}^r$ and the orthogonal complement coordinates $w_{j,t} = Q_{\\perp}^\\top x_{j,t} \\in \\mathbb{R}^{d-r}$, where $Q_{\\perp} \\in \\mathbb{R}^{d \\times (d-r)}$ is any orthonormal basis of the orthogonal complement satisfying $Q_{\\perp}^\\top U = 0$ and $Q_{\\perp}^\\top Q_{\\perp} = I_{d-r}$.\n\nTo assess convergence, use a multivariate potential scale reduction factor (multivariate $\\hat{R}$) computed on the LIS coordinates and the usual univariate $\\hat{R}$ computed coordinate-wise on the orthogonal complement coordinates. Construct multivariate $\\hat{R}$ for the LIS from the within-chain covariance and between-chain covariance of the $K$ sets of $n$ samples of $z \\in \\mathbb{R}^r$, and construct univariate $\\hat{R}$ for each coordinate of $w \\in \\mathbb{R}^{d-r}$ from the within-chain variance and between-chain variance of the $K$ sets of $n$ samples. Precisely:\n- For any $p$-dimensional coordinates $v_{j,t} \\in \\mathbb{R}^p$, define for each chain $j$ the sample mean $\\bar{v}_j = \\frac{1}{n}\\sum_{t=1}^n v_{j,t}$ and the sample covariance $S_j$ (for $p=1$, this is the scalar sample variance) with divisor $n-1$. Let $\\bar{v} = \\frac{1}{K} \\sum_{j=1}^K \\bar{v}_j$. The within-chain covariance is $W = \\frac{1}{K} \\sum_{j=1}^K S_j$. The between-chain covariance is $B = n \\cdot \\mathrm{Cov}(\\bar{v}_1,\\dots,\\bar{v}_K)$, where the covariance is across the $K$ chain means; for $p=1$, $B = n$ times the sample variance of the $K$ chain means.\n- Use these matrices to form scale-comparative diagnostics. For $p=1$, the univariate $\\hat{R}$ compares a pooled variance estimator to $W$. For $p>1$, define a multivariate analogue that reduces to the univariate case when $p=1$, and is based on the relative enlargement of dispersion from within- to between-chains.\n\nDefine a stopping rule: for a threshold $t > 1$, declare convergence if and only if the LIS multivariate $\\hat{R} \\le t$. Call this a false convergence event if, simultaneously, the maximum univariate $\\hat{R}$ across coordinates of the orthogonal complement satisfies $\\max_{1 \\le k \\le d-r} \\hat{R}_k \\ge 1.1$.\n\nYour task is to write a complete, runnable program that:\n1. Synthesizes problems, runs the $K$-chain blocked pCN sampler with the given parameters, constructs the LIS and its orthogonal complement, and computes the required diagnostics as defined above from first principles of sample (co)variances.\n2. For each test case below and for each threshold $t \\in \\{1.1, 1.05, 1.01\\}$, returns a boolean indicating whether the stopping rule would produce a false convergence event.\n\nUse the following test suite. In all cases, generate $A$ with independent entries distributed as $\\mathcal{N}(0, d^{-1})$, generate a true state $x^\\star \\sim \\mathcal{N}(0, I_d)$, and generate observations $y = A x^\\star + \\eta$ with $\\eta \\sim \\mathcal{N}(0, \\sigma^2 I_m)$. Use $K=4$ chains, and set the initial state of each chain independently from the prior $\\mathcal{N}(0, I_d)$. Use a fixed random seed so the outcomes are deterministic.\n\n- Test Case 1 (happy path with poorly mixed orthogonal complement):\n  - Dimension $d = 128$, observations $m = 48$, noise standard deviation $\\sigma = 1$, LIS eigen-threshold $\\tau = 0.18$.\n  - pCN step sizes: $\\beta_{\\mathrm{LIS}} = 0.5$, $\\beta_{\\perp} = 0.02$.\n  - Total steps $n_{\\mathrm{tot}} = 1200$, burn-in $n_{\\mathrm{burn}} = 300$, so $n = 900$ kept.\n\n- Test Case 2 (good mixing in all directions):\n  - Same $d, m, \\sigma, \\tau$ as Test Case 1.\n  - pCN step sizes: $\\beta_{\\mathrm{LIS}} = 0.5$, $\\beta_{\\perp} = 0.5$.\n  - Total steps $n_{\\mathrm{tot}} = 1200$, burn-in $n_{\\mathrm{burn}} = 300$, so $n = 900$ kept.\n\n- Test Case 3 (boundary case with no orthogonal complement):\n  - Dimension $d = 32$, observations $m = 32$, noise standard deviation $\\sigma = 1$, LIS eigen-threshold $\\tau = -0.1$ (ensures $r = d$ almost surely).\n  - pCN step sizes: $\\beta_{\\mathrm{LIS}} = 0.5$, $\\beta_{\\perp} = 0.02$.\n  - Total steps $n_{\\mathrm{tot}} = 900$, burn-in $n_{\\mathrm{burn}} = 200$, so $n = 700$ kept.\n\nFor each test case and each threshold $t \\in \\{1.1, 1.05, 1.01\\}$ (in that order), output a boolean indicating whether a false convergence event occurs. Aggregate the nine booleans (three thresholds for each of the three test cases, in order) into a single list.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true,false,true,false,...]\"). The entries should be lowercase booleans (\"true\" or \"false\").", "solution": "The user has provided a problem statement that requires validation before a solution can be attempted.\n\n### Step 1: Extract Givens\n- **Problem Type**: Linear Gaussian inverse problem in $d$ dimensions.\n- **Model Parameters**: $x \\in \\mathbb{R}^d$ is the unknown parameter.\n- **Prior Distribution**: $x \\sim \\mathcal{N}(0, I_d)$, where $I_d$ is the $d \\times d$ identity matrix.\n- **Observation Model**: $y = A x + \\eta$, where $A \\in \\mathbb{R}^{m \\times d}$ is a known matrix and $\\eta \\in \\mathbb{R}^m$ is the observational noise.\n- **Noise Model**: $\\eta \\sim \\mathcal{N}(0, \\sigma^2 I_m)$, where $I_m$ is the $m \\times m$ identity matrix.\n- **Negative Log-Likelihood**: $\\Phi(x) = \\frac{1}{2 \\sigma^2} \\lVert y - A x \\rVert_2^2$.\n- **Posterior Density**: $\\pi(x|y) \\propto \\exp\\left(-\\frac{1}{2}\\lVert x \\rVert_2^2 - \\Phi(x)\\right)$.\n- **Likelihood-Informed Subspace (LIS)**: The span of eigenvectors of $H = \\frac{1}{\\sigma^2} A^\\top A$ corresponding to eigenvalues strictly greater than a threshold $\\tau > 0$. The orthonormal basis for this subspace is $U \\in \\mathbb{R}^{d \\times r}$.\n- **Projectors**: $P = U U^\\top$ (onto LIS) and $Q = I_d - P$ (onto the orthogonal complement).\n- **MCMC Sampler**: A blocked preconditioned Crank–Nicolson (pCN) algorithm with a Metropolis-Hastings correction.\n- **pCN Proposal**: $x' = \\sqrt{1 - \\beta_{\\mathrm{LIS}}^2}\\, P x + \\beta_{\\mathrm{LIS}}\\, \\xi_{\\mathrm{LIS}} \\;+\\; \\sqrt{1 - \\beta_{\\perp}^2}\\, Q x + \\beta_{\\perp}\\, \\xi_{\\perp}$.\n- **Proposal Noise**: $\\xi_{\\mathrm{LIS}} \\sim \\mathcal{N}(0, P)$ and $\\xi_{\\perp} \\sim \\mathcal{N}(0, Q)$ are independent.\n- **Acceptance Probability**: $\\alpha(x, x') = \\min\\left(1, \\exp(-\\Phi(x') + \\Phi(x))\\right)$.\n- **Simulation Parameters**: $K$ parallel chains, $n_{\\mathrm{tot}}$ total steps per chain, $n_{\\mathrm{burn}}$ burn-in steps.\n- **Coordinate Systems**: LIS coordinates $z_{j,t} = U^\\top x_{j,t} \\in \\mathbb{R}^r$ and orthogonal complement coordinates $w_{j,t} = Q_{\\perp}^\\top x_{j,t} \\in \\mathbb{R}^{d-r}$, where $Q_\\perp$ is an orthonormal basis for the complement space.\n- **Convergence Diagnostics**: Multivariate potential scale reduction factor (m$\\hat{R}$) for LIS coordinates and univariate $\\hat{R}$ for each coordinate of the orthogonal complement.\n- **Diagnostic Definitions ($p$-dim coordinates $v_{j,t}$)**:\n    - Within-chain covariance: $W = \\frac{1}{K} \\sum_{j=1}^K S_j$, where $S_j$ is the sample covariance of chain $j$ with divisor $n-1 = (n_{\\mathrm{tot}}-n_{\\mathrm{burn}})-1$.\n    - Between-chain covariance: $B = n \\cdot \\mathrm{Cov}(\\bar{v}_1,\\dots,\\bar{v}_K)$, where $\\bar{v}_j$ are the chain means.\n- **False Convergence Event**: Defined as the simultaneous occurrence of two conditions for a given threshold $t > 1$:\n    1. The LIS multivariate $\\hat{R} \\le t$.\n    2. The maximum univariate $\\hat{R}$ over the orthogonal complement coordinates, $\\max_{k} \\hat{R}_k \\ge 1.1$.\n- **Data Generation**: $A$ has entries from $\\mathcal{N}(0, d^{-1})$; true state $x^\\star \\sim \\mathcal{N}(0, I_d)$; observations $y = A x^\\star + \\eta$; initial states for MCMC chains are drawn from the prior $\\mathcal{N}(0, I_d)$. A fixed random seed must be used.\n- **Test Cases**: Three specific test cases are provided with parameters for $(d, m, \\sigma, \\tau)$, $(\\beta_{\\mathrm{LIS}}, \\beta_{\\perp})$, and $(n_{\\mathrm{tot}}, n_{\\mathrm{burn}})$.\n- **Task**: For each test case, and for each threshold $t \\in \\{1.1, 1.05, 1.01\\}$, determine if a false convergence event occurs.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is critically examined against the validation criteria.\n\n1.  **Scientific Grounding**: The problem is firmly rooted in Bayesian inverse problem theory and MCMC methods. The linear Gaussian model, pCN sampler, and LIS decomposition are standard, well-documented techniques in computational statistics and scientific computing. The convergence diagnostics ($\\hat{R}$ and its multivariate extension) are foundational tools for assessing MCMC performance. There are no scientific or factual violations.\n2.  **Formalizability**: The problem is specified with mathematical precision. All components, from the model to the sampler and the diagnostics, are formally defined, allowing for a direct and unambiguous implementation.\n3.  **Completeness and Consistency**:\n    - The problem provides all necessary parameters for each test case.\n    - The definitions for the convergence diagnostics are specified. While the exact formula for the multivariate $\\hat{R}$ is described functionally (\"reduces to the univariate case...\"), this uniquely points to a standard definition in the literature, which is based on the generalized eigenvalues of the between- and within-chain covariance matrices. This does not constitute a fatal ambiguity but requires the solver to know or derive the standard form, which is appropriate for a professor-level expert.\n    - There is a minor inconsistency where the LIS definition states $\\tau > 0$, but Test Case 3 uses $\\tau = -0.1$. However, the problem statement immediately clarifies the intent: \"ensures $r=d$ almost surely\". This resolves the inconsistency; a negative threshold on non-negative eigenvalues simply means all dimensions are included in the LIS.\n    - In Test Case 3, where $r = d$, the orthogonal complement has dimension $0$. The condition $\\max_{k} \\hat{R}_k \\ge 1.1$ must be evaluated over an empty set. Logically, this condition is false, and thus a false convergence event is impossible. This is not a flaw, but a well-defined edge case to be handled by the implementation.\n4.  **Realism and Feasibility**: The problem, while a synthetic benchmark, uses parameter values and dimensions that are computationally tractable and representative of challenges encountered in real-world applications.\n5.  **Well-Posedness**: As discussed, the standard interpretation of the multivariate $\\hat{R}$ and the logical handling of the edge case in Test Case 3 render the problem well-posed, admitting a unique and meaningful solution.\n6.  **Triviality**: The problem is non-trivial, requiring implementation of a complete MCMC simulation and analysis pipeline, including numerical linear algebra and statistical calculations. The core concept being tested—subspace-dependent convergence rates leading to misleading diagnostics—is a sophisticated topic in MCMC theory.\n7.  **Verifiability**: The requirement of a fixed random seed ensures that the entire simulation is deterministic and its results are perfectly verifiable and reproducible.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. It is well-specified, scientifically sound, and computationally feasible. The minor ambiguities are resolvable through standard interpretations in the field. I will now proceed with the solution.\n\n### Solution Design and Principles\n\nThe solution involves simulating a Bayesian inference process and analyzing its convergence properties. The core idea is to demonstrate how a sampler can appear to have converged when looking at a specific \"important\" subspace (the LIS), while still mixing poorly in the complementary \"unimportant\" subspace. This can lead to a premature declaration of convergence, which the problem terms a \"false convergence event\".\n\n**1. Posterior and Sampler**\nThe posterior distribution is a multivariate Gaussian. The pCN sampler is well-suited for this problem because its proposal is generated from the prior distribution. This property, known as \"prior-reversibility,\" simplifies the Metropolis-Hastings acceptance ratio to depend only on the likelihood, as given by $\\alpha(x, x') = \\min(1, \\exp(-\\Phi(x') + \\Phi(x)))$. The blocked nature of the proposal allows for different step sizes ($\\beta_{\\mathrm{LIS}}$, $\\beta_{\\perp}$) in the LIS and its complement, enabling us to simulate the scenario of differential mixing rates.\n\n**2. Subspace Decomposition**\nThe Likelihood-Informed Subspace is constructed from the eigenvectors of the matrix $H = \\frac{1}{\\sigma^2} A^\\top A$. This matrix is related to the data-informed part of the posterior precision (inverse covariance). Its dominant eigenvectors correspond to directions in the parameter space that are most informed by the data $y$. The algorithm will use `scipy.linalg.eigh` for stable and efficient eigendecomposition of the symmetric matrix $H$. The eigenvectors with eigenvalues greater than $\\tau$ will form the basis $U$ for the LIS, and the remaining eigenvectors will form the basis $Q_\\perp$ for its orthogonal complement.\n\n**3. Convergence Diagnostics (PSRF)**\nThe potential scale reduction factor, $\\hat{R}$, is a diagnostic that compares the variance within parallel chains to the variance between them. If chains have converged to the stationary distribution, these variances should be similar, and $\\hat{R}$ will be close to $1$.\n\n- **Univariate $\\hat{R}_k$**: For each coordinate $k$ of the orthogonal complement space, we compute the standard Gelman-Rubin diagnostic. With $n$ samples and $K$ chains, its squared value is:\n$$\n\\hat{R}_k^2 = \\frac{\\widehat{\\mathrm{Var}}_k}{W_k} = \\frac{\\frac{n-1}{n}W_k + \\frac{1}{n}B_k}{W_k} = \\frac{n-1}{n} + \\frac{B_k}{n W_k}\n$$\nwhere $W_k$ is the average within-chain variance and $B_k$ is $n$ times the between-chain variance for coordinate $k$.\n\n- **Multivariate $\\hat{R}$**: For the $r$-dimensional LIS coordinates, a multivariate generalization is required. Following the principle that it should reduce to the univariate case and represent the maximum relative variance inflation, we define it based on the generalized eigenvalue problem involving the within-chain ($W_z$) and between-chain ($B_z$) covariance matrices:\n$$\n(\\text{m}\\hat{R})^2 = \\frac{n-1}{n} + \\frac{1}{n} \\lambda_{\\max}(W_z^{-1} B_z)\n$$\nwhere $\\lambda_{\\max}(W_z^{-1} B_z)$ is the largest eigenvalue of $W_z^{-1} B_z$. This is implemented by solving the generalized eigenvalue problem $B_z v = \\lambda W_z v$ to avoid matrix inversion.\n\n**4. Implementation Strategy**\nA Python script will be developed to execute the full simulation and analysis pipeline.\n- A main function will manage the test cases and collate results.\n- A core function will handle the simulation for a single test case: generating data, performing the LIS decomposition, running the $K$ MCMC chains, and storing the samples.\n- Helper functions will calculate the multivariate and univariate $\\hat{R}$ values from the MCMC samples, encapsulating the statistical formulas.\n- For Test Case 3, where the orthogonal complement has dimension zero, the maximum univariate $\\hat{R}$ is considered not to satisfy the condition $\\ge 1.1$, correctly yielding no false convergence.\n- All random processes will be seeded to ensure deterministic output. The final result will be formatted as a list of lowercase booleans.\n\nThis systematic approach ensures correctness, adherence to the problem's specifications, and alignment with established scientific principles of MCMC diagnostics.", "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef _compute_mle_r_hat(samples, n, K):\n    \"\"\"\n    Computes the multivariate potential scale reduction factor (m-R-hat).\n    \n    Args:\n        samples (np.ndarray): MCMC samples of shape (K, n, p).\n        n (int): Number of post-burn-in samples.\n        K (int): Number of chains.\n        \n    Returns:\n        float: The multivariate R-hat value.\n    \"\"\"\n    p = samples.shape[2]\n    if p == 0:\n        return 1.0\n\n    # Chain means (shape K, p)\n    chain_means = np.mean(samples, axis=1)\n\n    # Within-chain covariance matrix W (shape p, p)\n    # S_j variable is not explicitly needed, can compute W directly\n    W = np.zeros((p, p))\n    for j in range(K):\n        # np.cov expects (p, n)\n        W += np.cov(samples[j, :, :], rowvar=False, ddof=1)\n    W /= K\n\n    # Check for singularity of W\n    if np.linalg.cond(W) > 1e12:\n        return np.inf\n\n    # Between-chain covariance matrix B (shape p, p)\n    B = n * np.cov(chain_means, rowvar=False, ddof=1)\n    \n    # Generalized eigenvalue problem: B v = lambda W v\n    # This is more stable than computing inv(W) @ B\n    try:\n        eigvals = scipy.linalg.eig(B, W, right=True, left=False)\n        # We need the largest real part of the eigenvalues\n        lambda_max = np.max(np.real(eigvals))\n    except (np.linalg.LinAlgError, ValueError):\n        return np.inf\n\n    # Multivariate R-hat squared\n    mR_hat_sq = (n - 1) / n + lambda_max / n\n    \n    if mR_hat_sq < 0: # Should not happen with well-behaved chains\n        return np.inf\n        \n    return np.sqrt(mR_hat_sq)\n\ndef _compute_uni_r_hats_max(samples, n, K):\n    \"\"\"\n    Computes the maximum of univariate R-hats over all given dimensions.\n    \n    Args:\n        samples (np.ndarray): MCMC samples of shape (K, n, p).\n        n (int): Number of post-burn-in samples.\n        K (int): Number of chains.\n        \n    Returns:\n        float: The maximum R-hat value, or -inf if p=0.\n    \"\"\"\n    d_perp = samples.shape[2]\n    if d_perp == 0:\n        return -np.inf\n\n    max_r_hat = 0.0\n    for k in range(d_perp):\n        samples_1d = samples[:, :, k]  # Shape (K, n)\n        \n        # Within-chain variances\n        within_chain_vars = np.var(samples_1d, axis=1, ddof=1)\n        W = np.mean(within_chain_vars)\n\n        # Between-chain variance\n        chain_means = np.mean(samples_1d, axis=1)\n        B = n * np.var(chain_means, ddof=1)\n\n        if W < 1e-20:\n             # Chains are stuck, if at different points, R-hat is infinite\n            r_hat = 1.0 if B < 1e-20 else np.inf\n        else:\n            r_hat_sq = (n - 1) / n + B / (n * W)\n            r_hat = np.sqrt(r_hat_sq)\n        \n        if r_hat > max_r_hat:\n            max_r_hat = r_hat\n            \n    return max_r_hat\n\ndef run_single_case(d, m, sigma, tau, beta_lis, beta_perp, n_tot, n_burn, K, rng):\n    \"\"\"\n    Runs one full test case simulation and analysis.\n    \"\"\"\n    n = n_tot - n_burn\n    \n    # 1. Synthesize problem\n    A = rng.normal(loc=0.0, scale=1.0/np.sqrt(d), size=(m, d))\n    x_star = rng.normal(loc=0.0, scale=1.0, size=d)\n    eta = rng.normal(loc=0.0, scale=sigma, size=m)\n    y = A @ x_star + eta\n\n    # 2. LIS decomposition\n    H = (1 / sigma**2) * (A.T @ A)\n    eigvals, eigvecs = scipy.linalg.eigh(H)\n    \n    lis_indices = eigvals > tau\n    r = np.sum(lis_indices)\n    U = eigvecs[:, lis_indices]\n    Q_perp = eigvecs[:, ~lis_indices]\n    d_perp = d - r\n\n    # 3. Run MCMC chains\n    x_chains = rng.normal(loc=0.0, scale=1.0, size=(K, d))\n    all_samples = np.zeros((K, n_tot, d))\n    \n    def neg_log_likelihood(x_vec):\n        return (0.5 / sigma**2) * np.linalg.norm(y - A @ x_vec)**2\n\n    for j in range(K):\n        x_current = x_chains[j]\n        phi_current = neg_log_likelihood(x_current)\n        for t in range(n_tot):\n            z_current = U.T @ x_current\n            w_current = Q_perp.T @ x_current\n\n            xi_z = rng.standard_normal(size=r)\n            xi_w = rng.standard_normal(size=d_perp)\n            \n            z_prop = np.sqrt(1 - beta_lis**2) * z_current + beta_lis * xi_z\n            w_prop = np.sqrt(1 - beta_perp**2) * w_current + beta_perp * xi_w\n            \n            x_prop = U @ z_prop + Q_perp @ w_prop\n            \n            phi_prop = neg_log_likelihood(x_prop)\n            \n            log_alpha = phi_current - phi_prop\n            \n            if np.log(rng.uniform()) < log_alpha:\n                x_current = x_prop\n                phi_current = phi_prop\n\n            all_samples[j, t, :] = x_current\n\n    # 4. Compute diagnostics\n    post_burn_samples = all_samples[:, n_burn:, :]\n    \n    z_samples = post_burn_samples @ U\n    w_samples = post_burn_samples @ Q_perp\n    \n    mR_hat_lis = _compute_mle_r_hat(z_samples, n, K)\n    max_R_hat_perp = _compute_uni_r_hats_max(w_samples, n, K)\n    \n    return mR_hat_lis, max_R_hat_perp\n\n\ndef solve():\n    # Use a fixed random seed for reproducibility\n    rng = np.random.default_rng(42)\n    \n    # Fixed parameters\n    K = 4\n    \n    test_cases = [\n        # TC 1: Poor mixing in orthogonal complement\n        dict(d=128, m=48, sigma=1.0, tau=0.18, \n             beta_lis=0.5, beta_perp=0.02, \n             n_tot=1200, n_burn=300),\n        # TC 2: Good mixing everywhere\n        dict(d=128, m=48, sigma=1.0, tau=0.18,\n             beta_lis=0.5, beta_perp=0.5,\n             n_tot=1200, n_burn=300),\n        # TC 3: No orthogonal complement\n        dict(d=32, m=32, sigma=1.0, tau=-0.1,\n             beta_lis=0.5, beta_perp=0.02,\n             n_tot=900, n_burn=200),\n    ]\n\n    thresholds = [1.1, 1.05, 1.01]\n    results = []\n\n    for case_params in test_cases:\n        mR_hat_lis, max_R_hat_perp = run_single_case(**case_params, K=K, rng=rng)\n        \n        for t in thresholds:\n            converged_lis = mR_hat_lis <= t\n            unconverged_perp = max_R_hat_perp >= 1.1\n            \n            is_false_convergence = converged_lis and unconverged_perp\n            results.append(is_false_convergence)\n\n    # Format output as a list of lowercase boolean strings\n    output_str = f\"[{','.join(map(lambda b: str(b).lower(), results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3370944"}]}