{"hands_on_practices": [{"introduction": "To build a solid foundation, we begin with an analytically tractable scenario: the linear inverse problem with Gaussian noise. This exercise [@problem_id:3362741] demystifies the active subspace matrix $C$ by revealing its direct connection to the Fisher information matrix and the structure of the forward model. By working through this problem, you will see how the dominant directions of parameter sensitivity are fundamentally linked to the singular vectors of the weighted forward operator, providing a clear bridge between abstract definitions and concrete linear algebra.", "problem": "Consider the linear inverse problem with additive Gaussian noise given by $y = A \\theta + \\eta$, where $A \\in \\mathbb{R}^{m \\times n}$ is a known forward operator, $\\theta \\in \\mathbb{R}^{n}$ is the parameter of interest, and $\\eta \\sim \\mathcal{N}(0, \\Gamma_{\\eta})$ is Gaussian noise with a known symmetric positive definite covariance matrix $\\Gamma_{\\eta} \\in \\mathbb{R}^{m \\times m}$. The negative log-likelihood (up to an additive constant independent of $\\theta$) is defined by\n$\\ell(\\theta; y) = \\frac{1}{2} (y - A \\theta)^{\\top} \\Gamma_{\\eta}^{-1} (y - A \\theta)$.\nDefine the active subspace matrix\n$C = \\mathbb{E}_{y \\mid \\theta} \\left[ \\nabla_{\\theta} \\ell(\\theta; y) \\, \\nabla_{\\theta} \\ell(\\theta; y)^{\\top} \\right]$,\nwhere the expectation is taken with respect to the conditional distribution of $y$ given $\\theta$. Use only fundamental definitions of the Gaussian likelihood and basic properties of gradients and expectations to:\n\n1. Derive $C$ explicitly in terms of $A$ and $\\Gamma_{\\eta}$, and prove that the eigenvectors of $C$ coincide with the right singular vectors of $\\Gamma_{\\eta}^{-1/2} A$ associated with its nonzero singular values. Explain the relationship between inactive directions (those associated with zero eigenvalues of $C$) and non-identifiability in the inverse problem.\n\n2. For the concrete instance with\n$A = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix}$ and $\\Gamma_{\\eta} = \\mathrm{diag}(1, 4)$,\ncompute the eigenvalues of $C$ and determine the minimal active dimension $r$ such that the sum of the largest $r$ eigenvalues is at least a fraction $\\alpha = 0.95$ of the trace of $C$; that is, find the smallest integer $r$ such that\n$\\sum_{i=1}^{r} \\lambda_{i} \\ge \\alpha \\sum_{i=1}^{n} \\lambda_{i}$,\nwhere $\\lambda_{1} \\ge \\lambda_{2} \\ge \\cdots \\ge \\lambda_{n} \\ge 0$ are the eigenvalues of $C$ in nonincreasing order.\n\nProvide your final answer as the single integer value of $r$. No units are required. No rounding is needed beyond exact arithmetic.", "solution": "The problem is divided into two parts. First, we derive the active subspace matrix $C$ for a linear Gaussian inverse problem and analyze its properties. Second, we compute $C$ for a specific instance and determine the minimal active dimension $r$ based on its eigenvalue spectrum.\n\n### Part 1: Derivation and Analysis of the Active Subspace Matrix $C$\n\nThe negative log-likelihood function is given by\n$$ \\ell(\\theta; y) = \\frac{1}{2} (y - A \\theta)^{\\top} \\Gamma_{\\eta}^{-1} (y - A \\theta) $$\nwhere $y \\in \\mathbb{R}^{m}$, $\\theta \\in \\mathbb{R}^{n}$, $A \\in \\mathbb{R}^{m \\times n}$, and $\\Gamma_{\\eta} \\in \\mathbb{R}^{m \\times m}$ is symmetric positive definite.\n\n**1. Derivation of $C$**\n\nFirst, we compute the gradient of $\\ell(\\theta; y)$ with respect to $\\theta$. Expanding the quadratic form:\n$$ \\ell(\\theta; y) = \\frac{1}{2} (y^{\\top}\\Gamma_{\\eta}^{-1}y - y^{\\top}\\Gamma_{\\eta}^{-1}A\\theta - \\theta^{\\top}A^{\\top}\\Gamma_{\\eta}^{-1}y + \\theta^{\\top}A^{\\top}\\Gamma_{\\eta}^{-1}A\\theta) $$\nSince $\\ell(\\theta; y)$ is a scalar, the term $y^{\\top}\\Gamma_{\\eta}^{-1}A\\theta$ is equal to its transpose. The matrix $\\Gamma_{\\eta}^{-1}$ is symmetric because $\\Gamma_{\\eta}$ is symmetric. Thus, $(y^{\\top}\\Gamma_{\\eta}^{-1}A\\theta)^{\\top} = \\theta^{\\top}A^{\\top}(\\Gamma_{\\eta}^{-1})^{\\top}y = \\theta^{\\top}A^{\\top}\\Gamma_{\\eta}^{-1}y$. The expression simplifies to:\n$$ \\ell(\\theta; y) = \\frac{1}{2} (y^{\\top}\\Gamma_{\\eta}^{-1}y - 2 \\theta^{\\top}A^{\\top}\\Gamma_{\\eta}^{-1}y + \\theta^{\\top}A^{\\top}\\Gamma_{\\eta}^{-1}A\\theta) $$\nUsing standard matrix calculus identities, $\\nabla_{x}(c^{\\top}x) = c$ and $\\nabla_{x}(x^{\\top}Bx) = 2Bx$ for a symmetric matrix $B$, we find the gradient with respect to $\\theta$:\n$$ \\nabla_{\\theta} \\ell(\\theta; y) = \\frac{1}{2} (-2 A^{\\top}\\Gamma_{\\eta}^{-1}y + 2 A^{\\top}\\Gamma_{\\eta}^{-1}A\\theta) = A^{\\top}\\Gamma_{\\eta}^{-1}A\\theta - A^{\\top}\\Gamma_{\\eta}^{-1}y = A^{\\top}\\Gamma_{\\eta}^{-1}(A\\theta - y) $$\nThe active subspace matrix $C$ is defined as the expectation of the outer product of this gradient:\n$$ C = \\mathbb{E}_{y \\mid \\theta} \\left[ \\nabla_{\\theta} \\ell(\\theta; y) \\, \\nabla_{\\theta} \\ell(\\theta; y)^{\\top} \\right] $$\nSubstituting the expression for the gradient:\n$$ C = \\mathbb{E}_{y \\mid \\theta} \\left[ \\left( A^{\\top}\\Gamma_{\\eta}^{-1}(A\\theta - y) \\right) \\left( A^{\\top}\\Gamma_{\\eta}^{-1}(A\\theta - y) \\right)^{\\top} \\right] $$\n$$ C = \\mathbb{E}_{y \\mid \\theta} \\left[ A^{\\top}\\Gamma_{\\eta}^{-1}(A\\theta - y)(A\\theta - y)^{\\top}\\Gamma_{\\eta}^{-1}A \\right] $$\nSince $A$ and $\\Gamma_{\\eta}$ are constant with respect to the expectation over $y$, we can move them outside:\n$$ C = A^{\\top}\\Gamma_{\\eta}^{-1} \\mathbb{E}_{y \\mid \\theta} \\left[ (A\\theta - y)(A\\theta - y)^{\\top} \\right] \\Gamma_{\\eta}^{-1}A $$\nFrom the problem statement, $y = A\\theta + \\eta$, where $\\eta \\sim \\mathcal{N}(0, \\Gamma_{\\eta})$. This implies $A\\theta - y = -\\eta$. The expectation is therefore of $(-\\eta)(-\\eta)^{\\top} = \\eta\\eta^{\\top}$.\n$$ \\mathbb{E}_{y \\mid \\theta} \\left[ (A\\theta - y)(A\\theta - y)^{\\top} \\right] = \\mathbb{E}[\\eta\\eta^{\\top}] $$\nBy definition, the covariance matrix of a zero-mean random vector $\\eta$ is $\\text{Cov}(\\eta) = \\mathbb{E}[(\\eta-\\mathbb{E}[\\eta])(\\eta-\\mathbb{E}[\\eta])^{\\top}] = \\mathbb{E}[\\eta\\eta^{\\top}]$. We are given that $\\eta \\sim \\mathcal{N}(0, \\Gamma_{\\eta})$, so $\\mathbb{E}[\\eta\\eta^{\\top}] = \\Gamma_{\\eta}$.\nSubstituting this back into the expression for $C$:\n$$ C = A^{\\top}\\Gamma_{\\eta}^{-1} \\Gamma_{\\eta} \\Gamma_{\\eta}^{-1}A = A^{\\top}\\Gamma_{\\eta}^{-1}A $$\nThis is the explicit form for $C$. This is also the Fisher information matrix for $\\theta$.\n\n**2. Eigenvectors of $C$ and Singular Vectors of $\\Gamma_{\\eta}^{-1/2} A$**\n\nLet $B = \\Gamma_{\\eta}^{-1/2} A$. Since $\\Gamma_{\\eta}$ is symmetric positive definite, its inverse $\\Gamma_{\\eta}^{-1}$ and its square root $\\Gamma_{\\eta}^{-1/2}$ exist and are also symmetric positive definite. We can express $C$ as:\n$$ C = A^{\\top}\\Gamma_{\\eta}^{-1}A = A^{\\top}(\\Gamma_{\\eta}^{-1/2})^{\\top}\\Gamma_{\\eta}^{-1/2}A = ( \\Gamma_{\\eta}^{-1/2} A )^{\\top} ( \\Gamma_{\\eta}^{-1/2} A ) = B^{\\top}B $$\nLet the Singular Value Decomposition (SVD) of $B = \\Gamma_{\\eta}^{-1/2} A$ be $B = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix of singular values $\\sigma_{i} \\ge 0$. The columns of $V$ are the right singular vectors of $B$.\nNow, substitute the SVD into the expression for $C$:\n$$ C = B^{\\top}B = (U \\Sigma V^{\\top})^{\\top} (U \\Sigma V^{\\top}) = (V \\Sigma^{\\top} U^{\\top}) (U \\Sigma V^{\\top}) $$\nSince $U$ is orthogonal, $U^{\\top}U = I_{m}$.\n$$ C = V \\Sigma^{\\top} (U^{\\top}U) \\Sigma V^{\\top} = V (\\Sigma^{\\top}\\Sigma) V^{\\top} $$\nThe matrix $V$ is orthogonal, so $V^{\\top} = V^{-1}$. The equation $C = V (\\Sigma^{\\top}\\Sigma) V^{-1}$ is the eigendecomposition of $C$. The columns of $V$ are the eigenvectors of $C$. The matrix $\\Sigma^{\\top}\\Sigma$ is an $n \\times n$ diagonal matrix whose entries are $\\sigma_i^2$ for $i=1, \\dots, \\min(m,n)$, and zero otherwise. The eigenvalues of $C$ are therefore the squared singular values of $B = \\Gamma_{\\eta}^{-1/2} A$.\nThus, the eigenvectors of $C$ are precisely the right singular vectors of $\\Gamma_{\\eta}^{-1/2} A$.\n\n**3. Inactive Directions and Non-identifiability**\n\nInactive directions are defined as the eigenvectors of $C$ associated with zero eigenvalues. Let $v$ be an eigenvector of $C$ with eigenvalue $\\lambda=0$. Then, $Cv=0$.\n$$ C v = (A^{\\top}\\Gamma_{\\eta}^{-1}A)v = 0 $$\nMultiplying from the left by $v^{\\top}$:\n$$ v^{\\top}A^{\\top}\\Gamma_{\\eta}^{-1}A v = 0 $$\nThis can be written as $(Av)^{\\top}\\Gamma_{\\eta}^{-1}(Av) = 0$. Let $w = Av$. Then $w^{\\top}\\Gamma_{\\eta}^{-1}w = 0$. Since $\\Gamma_{\\eta}$ is symmetric positive definite, $\\Gamma_{\\eta}^{-1}$ is also symmetric positive definite. By definition of positive definiteness, $w^{\\top}\\Gamma_{\\eta}^{-1}w > 0$ for any non-zero vector $w \\in \\mathbb{R}^m$. Therefore, $w^{\\top}\\Gamma_{\\eta}^{-1}w=0$ implies that $w=0$.\nSo, we must have $Av = 0$. This means that $v$ is in the null space of the forward operator $A$. Any vector in the null space of $A$ cannot be identified from the data. Specifically, if we perturb the parameter vector $\\theta$ by a multiple of $v$, i.e., $\\theta' = \\theta + k v$ for some scalar $k$, the model prediction remains unchanged:\n$$ A \\theta' = A(\\theta + k v) = A\\theta + k(Av) = A\\theta + k(0) = A\\theta $$\nSince the data $y$ depends on $\\theta$ only through $A\\theta$, the data provides no information to distinguish $\\theta$ from $\\theta'$. The parameter combinations along the direction $v$ are non-identifiable. The space spanned by eigenvectors of $C$ with zero eigenvalues is the null space of $A$, which represents the set of all non-identifiable parameter directions.\n\n### Part 2: Concrete Calculation\n\nWe are given the specific instance:\n$$ A = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} \\quad \\text{and} \\quad \\Gamma_{\\eta} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 4 \\end{pmatrix} $$\nFirst, we compute the inverse of the covariance matrix:\n$$ \\Gamma_{\\eta}^{-1} = \\begin{pmatrix} 1^{-1} & 0 \\\\ 0 & 4^{-1} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{1}{4} \\end{pmatrix} $$\nNext, we compute the active subspace matrix $C = A^{\\top}\\Gamma_{\\eta}^{-1}A$. The transpose of $A$ is:\n$$ A^{\\top} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\end{pmatrix} $$\nNow, we perform the matrix multiplication:\n$$ C = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} $$\n$$ C = \\begin{pmatrix} 2 & 0 \\\\ 0 & \\frac{1}{4} \\\\ 0 & \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 1 & 1 \\end{pmatrix} = \\begin{pmatrix} (2)(2) + (0)(0) & (2)(0) + (0)(1) & (2)(0) + (0)(1) \\\\ (0)(2) + (\\frac{1}{4})(0) & (0)(0) + (\\frac{1}{4})(1) & (0)(0) + (\\frac{1}{4})(1) \\\\ (0)(2) + (\\frac{1}{4})(0) & (0)(0) + (\\frac{1}{4})(1) & (0)(0) + (\\frac{1}{4})(1) \\end{pmatrix} $$\n$$ C = \\begin{pmatrix} 4 & 0 & 0 \\\\ 0 & \\frac{1}{4} & \\frac{1}{4} \\\\ 0 & \\frac{1}{4} & \\frac{1}{4} \\end{pmatrix} $$\nTo find the eigenvalues $\\lambda$ of $C$, we solve the characteristic equation $\\det(C - \\lambda I) = 0$:\n$$ \\det \\begin{pmatrix} 4-\\lambda & 0 & 0 \\\\ 0 & \\frac{1}{4}-\\lambda & \\frac{1}{4} \\\\ 0 & \\frac{1}{4} & \\frac{1}{4}-\\lambda \\end{pmatrix} = 0 $$\n$$ (4-\\lambda) \\left[ \\left(\\frac{1}{4}-\\lambda\\right)^2 - \\left(\\frac{1}{4}\\right)^2 \\right] = 0 $$\nThis equation gives one eigenvalue immediately: $\\lambda = 4$. The other two eigenvalues are found from the term in the brackets:\n$$ \\left(\\frac{1}{4}-\\lambda\\right)^2 - \\frac{1}{16} = 0 \\implies \\left(\\frac{1}{4}-\\lambda\\right)^2 = \\frac{1}{16} $$\n$$ \\frac{1}{4}-\\lambda = \\pm \\frac{1}{4} $$\nThis gives two possibilities:\n1. $\\frac{1}{4}-\\lambda = \\frac{1}{4} \\implies \\lambda = 0$\n2. $\\frac{1}{4}-\\lambda = -\\frac{1}{4} \\implies \\lambda = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$\nThe eigenvalues of $C$ are $\\{4, \\frac{1}{2}, 0\\}$. We order them in nonincreasing order: $\\lambda_{1} = 4$, $\\lambda_{2} = \\frac{1}{2}$, $\\lambda_{3} = 0$.\n\nThe trace of $C$ is the sum of its eigenvalues:\n$$ \\text{Tr}(C) = \\sum_{i=1}^{3} \\lambda_{i} = 4 + \\frac{1}{2} + 0 = 4.5 $$\nWe need to find the smallest integer $r$ such that the sum of the first $r$ largest eigenvalues is at least a fraction $\\alpha = 0.95$ of the total sum (the trace). The threshold is:\n$$ \\alpha \\sum_{i=1}^{3} \\lambda_{i} = 0.95 \\times 4.5 = \\frac{19}{20} \\times \\frac{9}{2} = \\frac{171}{40} = 4.275 $$\nWe check the cumulative sum of the ordered eigenvalues:\nFor $r=1$:\n$$ \\sum_{i=1}^{1} \\lambda_{i} = \\lambda_{1} = 4 $$\nSince $4 < 4.275$, the condition is not met.\nFor $r=2$:\n$$ \\sum_{i=1}^{2} \\lambda_{i} = \\lambda_{1} + \\lambda_{2} = 4 + \\frac{1}{2} = 4.5 $$\nSince $4.5 \\ge 4.275$, the condition is met.\nThe smallest integer $r$ that satisfies the condition is $2$.", "answer": "$$\\boxed{2}$$", "id": "3362741"}, {"introduction": "The true power of active subspaces lies in their ability to handle complex, nonlinear models. This practice [@problem_id:3362773] explores this by contrasting the global nature of the active subspace with a local, linearization-based method. You will analyze a carefully constructed nonlinear model where a local analysis at a single point gives a misleading picture of parameter sensitivity, while the globally-averaged active subspace correctly identifies the most influential parameter direction, highlighting the method's robustness.", "problem": "Consider a Bayesian inverse problem with a two-dimensional parameter vector $x \\in \\mathbb{R}^{2}$, a Gaussian prior $x \\sim \\mathcal{N}(0, I)$, and a Gaussian observation model with identity noise covariance. The forward model is the nonlinear map $G:\\mathbb{R}^{2} \\to \\mathbb{R}^{2}$ given by\n$$\nG(x) = \\begin{pmatrix} x_{1} \\\\ x_{2}^{3} \\end{pmatrix}.\n$$\nLet the observed data be $y = G(0) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$. Define the negative log-posterior (up to an additive constant) by\n$$\n\\Phi(x) = \\frac{1}{2}\\|G(x) - y\\|_{2}^{2} + \\frac{1}{2}\\|x\\|_{2}^{2},\n$$\nwhere $\\|\\cdot\\|_{2}$ denotes the Euclidean norm. Assume that the Maximum A Posteriori (MAP) point is $x_{\\ast} = 0$.\n\nTwo subspace constructions are of interest:\n- The Likelihood-Informed Subspace (LIS), built from the Gaussâ€“Newton approximation to the Hessian of the negative log-likelihood at the MAP point, which for identity noise covariance reduces to the matrix $J(x_{\\ast})^{\\top}J(x_{\\ast})$, where $J(x)$ is the Jacobian of $G$ at $x$.\n- The gradient-covariance active subspace for the least-squares misfit $m(x) = \\frac{1}{2}\\|G(x) - y\\|_{2}^{2}$, defined as the span of the leading eigenvector of the matrix\n$$\nC = \\mathbb{E}_{x \\sim \\mathcal{N}(0, I)}\\left[\\nabla m(x)\\,\\nabla m(x)^{\\top}\\right].\n$$\n\nCompute the principal angle between the one-dimensional LIS and the one-dimensional gradient-covariance active subspace obtained for this problem. Express your final answer in radians as an exact value. Then, explain from first principles why the two subspaces differ in this nonlinear example, identifying the source of the discrepancy between the constructions.", "solution": "The problem requires the computation of the principal angle between two one-dimensional subspaces derived from a Bayesian inverse problem setup: the Likelihood-Informed Subspace (LIS) and the gradient-covariance active subspace. It also asks for a first-principles explanation of why these subspaces differ.\n\nFirst, we validate the problem statement.\nThe given information is:\n- Parameter vector: $x = (x_1, x_2)^{\\top} \\in \\mathbb{R}^{2}$\n- Prior distribution: $x \\sim \\mathcal{N}(0, I)$, where $I$ is the $2 \\times 2$ identity matrix.\n- Forward model: $G(x) = \\begin{pmatrix} x_{1} \\\\ x_{2}^{3} \\end{pmatrix}$\n- Observation model: Gaussian with identity noise covariance.\n- Observed data: $y = G(0) = (0, 0)^{\\top}$\n- Negative log-posterior (up to a constant): $\\Phi(x) = \\frac{1}{2}\\|G(x) - y\\|_{2}^{2} + \\frac{1}{2}\\|x\\|_{2}^{2}$\n- Maximum A Posteriori (MAP) point: $x_{\\ast} = (0, 0)^{\\top}$\n- LIS definition: Span of the leading eigenvector(s) of the Gauss-Newton Hessian of the negative log-likelihood at the MAP point, $J(x_{\\ast})^{\\top}J(x_{\\ast})$.\n- Active Subspace (AS) definition: Span of the leading eigenvector(s) of the matrix $C = \\mathbb{E}_{x \\sim \\mathcal{N}(0, I)}\\left[\\nabla m(x)\\,\\nabla m(x)^{\\top}\\right]$, where $m(x) = \\frac{1}{2}\\|G(x) - y\\|_{2}^{2}$.\n\nThe problem is scientifically grounded in the fields of inverse problems and uncertainty quantification, is mathematically well-posed, internally consistent, and uses precise, objective language. All necessary information is provided. Thus, the problem is valid.\n\nWe proceed with the solution in three parts: computation of the LIS, computation of the active subspace, and calculation of the principal angle, followed by the requested explanation.\n\n**1. Computation of the Likelihood-Informed Subspace (LIS)**\n\nThe LIS is constructed from the Gauss-Newton approximation to the Hessian of the negative log-likelihood, evaluated at the MAP point $x_{\\ast} = 0$. The negative log-likelihood function is $l(x) = \\frac{1}{2}\\|G(x) - y\\|_{2}^{2}$. The Gauss-Newton Hessian is given by $H_{GN} = J(x_{\\ast})^{\\top}J(x_{\\ast})$, where $J(x)$ is the Jacobian of the forward model $G(x)$.\n\nThe forward model is $G(x) = \\begin{pmatrix} x_{1} \\\\ x_{2}^{3} \\end{pmatrix}$. Its Jacobian matrix $J(x)$ is:\n$$\nJ(x) = \\frac{\\partial G}{\\partial x} = \\begin{pmatrix} \\frac{\\partial (x_1)}{\\partial x_1} & \\frac{\\partial (x_1)}{\\partial x_2} \\\\ \\frac{\\partial (x_2^3)}{\\partial x_1} & \\frac{\\partial (x_2^3)}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 3x_{2}^{2} \\end{pmatrix}\n$$\nWe evaluate the Jacobian at the MAP point $x_{\\ast} = (0, 0)^{\\top}$:\n$$\nJ(x_{\\ast}) = J(0) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 3(0)^{2} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nNow, we compute the Gauss-Newton Hessian matrix:\n$$\nH_{GN} = J(0)^{\\top}J(0) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}^{\\top} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nThe eigenvalues of $H_{GN}$ are $\\lambda_1 = 1$ and $\\lambda_2 = 0$. The corresponding eigenvectors are $u_1 = (1, 0)^{\\top}$ and $u_2 = (0, 1)^{\\top}$, respectively. The one-dimensional LIS is the span of the eigenvector corresponding to the largest eigenvalue.\nThus, the LIS is spanned by the vector $u_1 = (1, 0)^{\\top}$.\n\n**2. Computation of the Gradient-Covariance Active Subspace (AS)**\n\nThe active subspace is defined by the eigensystem of the matrix $C = \\mathbb{E}_{x \\sim \\mathcal{N}(0, I)}\\left[\\nabla m(x)\\,\\nabla m(x)^{\\top}\\right]$.\nFirst, we find the scalar-valued function $m(x)$, which is the least-squares misfit. With $y = (0, 0)^{\\top}$, we have:\n$$\nm(x) = \\frac{1}{2}\\|G(x) - y\\|_{2}^{2} = \\frac{1}{2}\\left\\| \\begin{pmatrix} x_1 \\\\ x_2^3 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\right\\|_{2}^{2} = \\frac{1}{2}(x_1^2 + x_2^6)\n$$\nNext, we compute the gradient of $m(x)$:\n$$\n\\nabla m(x) = \\begin{pmatrix} \\frac{\\partial m}{\\partial x_1} \\\\ \\frac{\\partial m}{\\partial x_2} \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ 3x_2^5 \\end{pmatrix}\n$$\nNow we form the outer product $\\nabla m(x)\\,\\nabla m(x)^{\\top}$:\n$$\n\\nabla m(x)\\,\\nabla m(x)^{\\top} = \\begin{pmatrix} x_1 \\\\ 3x_2^5 \\end{pmatrix} \\begin{pmatrix} x_1 & 3x_2^5 \\end{pmatrix} = \\begin{pmatrix} x_1^2 & 3x_1x_2^5 \\\\ 3x_1x_2^5 & 9x_2^{10} \\end{pmatrix}\n$$\nThe matrix $C$ is the expectation of this matrix over the prior distribution $x \\sim \\mathcal{N}(0, I)$. This means $x_1$ and $x_2$ are independent standard normal random variables, $x_1, x_2 \\sim \\mathcal{N}(0, 1)$. We compute the expectation of each element:\n$$\nC = \\begin{pmatrix} \\mathbb{E}[x_1^2] & \\mathbb{E}[3x_1x_2^5] \\\\ \\mathbb{E}[3x_1x_2^5] & \\mathbb{E}[9x_2^{10}] \\end{pmatrix}\n$$\nFor a standard normal variable $Z \\sim \\mathcal{N}(0, 1)$, the moments are $\\mathbb{E}[Z^k] = 0$ for odd $k$, and $\\mathbb{E}[Z^{2n}] = (2n-1)!! = (2n-1)(2n-3)\\cdots 1$.\n- $\\mathbb{E}[x_1^2] = 1$.\n- Due to independence, $\\mathbb{E}[3x_1x_2^5] = 3\\,\\mathbb{E}[x_1]\\,\\mathbb{E}[x_2^5] = 3 \\cdot 0 \\cdot 0 = 0$.\n- $\\mathbb{E}[x_2^{10}] = (10-1)!! = 9!! = 9 \\cdot 7 \\cdot 5 \\cdot 3 \\cdot 1 = 945$.\n- $\\mathbb{E}[9x_2^{10}] = 9 \\cdot \\mathbb{E}[x_2^{10}] = 9 \\cdot 945 = 8505$.\n\nSubstituting these values back into the matrix $C$:\n$$\nC = \\begin{pmatrix} 1 & 0 \\\\ 0 & 8505 \\end{pmatrix}\n$$\nThe eigenvalues of this diagonal matrix are $\\mu_1 = 8505$ and $\\mu_2 = 1$. The corresponding eigenvectors are $v_1 = (0, 1)^{\\top}$ and $v_2 = (1, 0)^{\\top}$, respectively. The one-dimensional active subspace is the span of the eigenvector corresponding to the largest eigenvalue.\nThus, the active subspace is spanned by the vector $v_1 = (0, 1)^{\\top}$.\n\n**3. Computation of the Principal Angle**\n\nThe principal angle $\\theta$ between two one-dimensional subspaces (lines) spanned by the unit vectors $u = (1, 0)^{\\top}$ (for LIS) and $v = (0, 1)^{\\top}$ (for AS) is given by the formula:\n$$\n\\cos(\\theta) = |u^{\\top}v|\n$$\nCalculating the inner product:\n$$\nu^{\\top}v = \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = 1 \\cdot 0 + 0 \\cdot 1 = 0\n$$\nTherefore, $\\cos(\\theta) = 0$. The principal angle is $\\theta = \\arccos(0) = \\frac{\\pi}{2}$ radians. The two subspaces are orthogonal.\n\n**4. Explanation of the Discrepancy**\n\nThe fundamental reason for the discrepancy between the LIS and the active subspace lies in their construction: LIS is a *local* method, while the active subspace is a *global* one.\n\nThe **Likelihood-Informed Subspace (LIS)** is based on a linearization of the forward model $G(x)$ around a single point, the MAP estimate $x_{\\ast}$. The Gauss-Newton Hessian $J(x_{\\ast})^{\\top}J(x_{\\ast})$ measures the sensitivity of the model output to infinitesimal perturbations around $x_{\\ast}$. In this problem, $x_{\\ast}=0$. At this point, the Jacobian is $J(0) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}$. The zero in the $(2,2)$ entry arises because the derivative of the nonlinear term $x_2^3$ is $3x_2^2$, which evaluates to $0$ at $x_2=0$. Consequently, the local, linear approximation of the model around the origin, $G(\\delta x) \\approx J(0)\\delta x = (\\delta x_1, 0)^{\\top}$, is completely insensitive to perturbations in the $x_2$ direction. The LIS construction, relying solely on this local information, correctly concludes that the $x_1$ direction is the one informed by the likelihood *at the MAP point*. It is blind to any sensitivity away from this specific point.\n\nThe **Active Subspace (AS)**, in contrast, is based on a global sensitivity analysis. The matrix $C = \\mathbb{E}[\\nabla m(x) \\nabla m(x)^{\\top}]$ averages the sensitivity of the misfit function $m(x)$ over the entire parameter space, weighted by the prior probability distribution. The gradient of the misfit is $\\nabla m(x) = (x_1, 3x_2^5)^{\\top}$. The term $3x_2^5$ is zero at $x_2=0$, but it grows very rapidly as $|x_2|$ increases. The expectation integral for the $(2,2)$ component of $C$ involves $\\mathbb{E}[9x_2^{10}]$, which sums the contributions of this rapidly growing sensitivity over all possible values of $x_2$ dictated by the prior. This results in a very large value ($8505$), indicating a massive average sensitivity of the misfit function to the parameter $x_2$. The expectation for the $x_1$ component, $\\mathbb{E}[x_1^2]$, is only $1$. The active subspace construction, therefore, correctly identifies the $x_2$ direction as the one along which the misfit function varies the most on average, making it the dominant direction of parameter sensitivity from a global perspective.\n\nIn summary, the discrepancy is a direct consequence of the strong nonlinearity of the forward model in the $x_2$ variable. The LIS method, being local, is deceived by the vanishing derivative of $x_2^3$ at the linearization point $x_2=0$. The active subspace method, being global, integrates over the entire prior and correctly captures the dominant influence of the highly nonlinear $x_2$ term. The orthogonality of the two subspaces represents a complete disagreement between the local and global sensitivity analyses for this problem.", "answer": "$$\\boxed{\\frac{\\pi}{2}}$$", "id": "3362773"}, {"introduction": "Identifying an active subspace is not merely a diagnostic tool; it is a powerful guide for practical decision-making. This final hands-on exercise [@problem_id:3362746] applies the concept to the real-world problem of optimal experimental design. You will develop a strategy for sensor placement that leverages knowledge of the active subspace to maximize information gain, demonstrating a clear performance advantage over a naive, uninformed approach and cementing the practical utility of the method.", "problem": "Consider a linear Gaussian inverse problem under a sensor selection design. Let the unknown parameter be a vector $u \\in \\mathbb{R}^d$ with a standard Gaussian prior, and suppose that each potential sensor $i$ produces a linear measurement $y_i = s_i^\\top u + \\varepsilon_i$, where $s_i \\in \\mathbb{R}^d$ is the $i$-th candidate sensor row and $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ is independent Gaussian noise with variance $\\sigma^2$. If a set $\\mathcal{S}$ of $m$ sensors is selected from $p$ candidates, the information contribution (ignoring the prior for design purposes) is represented by the symmetric positive semidefinite matrix\n$$\nC(\\mathcal{S}) = \\frac{1}{\\sigma^2} \\sum_{i \\in \\mathcal{S}} s_i s_i^\\top \\in \\mathbb{R}^{d \\times d}.\n$$\nFor any unit direction $v \\in \\mathbb{S}^{d-1}$, the Rayleigh quotient $v^\\top C(\\mathcal{S}) v$ measures the concentration of information along $v$ due to the chosen sensors.\n\nActive subspace dimension reduction formalizes a preferred direction using the gradient covariance matrix for a scalar quantity of interest $q(u)$,\n$$\nC_{\\text{active}} = \\mathbb{E}\\left[ \\nabla q(u) \\nabla q(u)^\\top \\right],\n$$\nwhose leading eigenvector identifies the dominant one-dimensional active subspace. In this problem, you will work with a stylized yet scientifically consistent surrogate for $C_{\\text{active}}$ to design sensors that maximize the Rayleigh quotient along an informed direction and compare against a naive direction.\n\nFundamental bases and constraints to use:\n- The Rayleigh quotient property: for any symmetric positive semidefinite matrix $M$ and unit vector $v$, $v^\\top M v \\in [0, \\lambda_{\\max}(M)]$, where $\\lambda_{\\max}(M)$ is the largest eigenvalue of $M$.\n- For a set of sensor rows $\\{s_i\\}$ and a unit direction $v$, $v^\\top C(\\mathcal{S}) v = \\frac{1}{\\sigma^2} \\sum_{i \\in \\mathcal{S}} (s_i^\\top v)^2$.\n- The active subspace matrix is modeled as a low-rank symmetric positive semidefinite matrix that is a sum of rank-one components from unit vectors $a$ and $b$, reflecting dominant directions of $\\nabla q(u)$.\n\nDesign problem specification:\n- Active-subspace-aware design: choose the set $\\mathcal{S}_{\\text{AS}}$ of $m$ sensors that maximizes $v^\\top C(\\mathcal{S}) v$ when $v$ is the leading eigenvector of $C_{\\text{active}}$.\n- Naive design: choose the set $\\mathcal{S}_{\\text{NV}}$ of $m$ sensors that maximizes $v^\\top C(\\mathcal{S}) v$ when $v$ is the first coordinate axis $e_1 = (1,0,\\dots,0)^\\top$.\n- Comparison metric to report for each test case: the ratio\n$$\nR = \\frac{v_{\\text{true}}^\\top C(\\mathcal{S}_{\\text{AS}}) v_{\\text{true}}}{v_{\\text{true}}^\\top C(\\mathcal{S}_{\\text{NV}}) v_{\\text{true}}},\n$$\nwhere $v_{\\text{true}}$ is the leading eigenvector of $C_{\\text{active}}$, and all vectors are unit norm.\n\nConstruction of the active direction and candidate sensors:\n- The active subspace surrogate $C_{\\text{active}}$ is specified using two unit vectors $a,b \\in \\mathbb{R}^d$, defined componentwise by\n$$\na_j = \\cos(j+1), \\quad b_j = \\sin(2j+1), \\quad \\text{for } j \\in \\{0,1,\\dots,d-1\\},\n$$\nthen normalized to unit norm so that $\\|a\\|_2 = 1$ and $\\|b\\|_2 = 1$. The matrix is\n$$\nC_{\\text{active}} = a a^\\top + \\rho \\, b b^\\top,\n$$\nwhere $\\rho > 0$ is a given weight and angles are to be interpreted in radians.\n- The $p$ candidate sensor rows $s_i^\\top$ form a $p \\times d$ matrix $S$. Define the entries by\n$$\ns_{i,j} = \\sin\\!\\big((i+1)(j+1)\\big) + 0.25\\,\\cos(i + 2j),\n$$\nfor $i \\in \\{0,1,\\dots,p-1\\}$ and $j \\in \\{0,1,\\dots,d-1\\}$, with angles in radians.\n- The sensor noise variance is $\\sigma^2$.\n\nAlgorithmic requirements:\n- For any fixed unit direction $v$, the set $\\mathcal{S}$ that maximizes $v^\\top C(\\mathcal{S}) v$ subject to $|\\mathcal{S}| = m$ consists of the $m$ indices corresponding to the largest values of $(s_i^\\top v)^2$. Use this fact to construct both $\\mathcal{S}_{\\text{AS}}$ and $\\mathcal{S}_{\\text{NV}}$.\n- The evaluation direction $v_{\\text{true}}$ is the leading eigenvector of $C_{\\text{active}}$ (unit norm).\n- The matrix $C(\\mathcal{S})$ is computed exactly from the selected rows and $\\sigma^2$.\n\nTest suite:\nImplement your program to compute $R$ for each of the following test cases. All trigonometric arguments are in radians, and every vector must be normalized where specified.\n- Case 1 (happy path): $d=6$, $p=14$, $m=3$, $\\sigma^2=0.25$, $\\rho=0.25$.\n- Case 2 (boundary on $m$): $d=6$, $p=14$, $m=1$, $\\sigma^2=1.0$, $\\rho=1.0$.\n- Case 3 (higher dimension and noise): $d=8$, $p=20$, $m=4$, $\\sigma^2=4.0$, $\\rho=0.05$.\n\nYour task:\n- Write a complete, runnable program that constructs $S$, $C_{\\text{active}}$, and computes $v_{\\text{true}}$; then forms $\\mathcal{S}_{\\text{AS}}$ and $\\mathcal{S}_{\\text{NV}}$ by maximizing the Rayleigh quotient along the active direction and the naive direction, respectively; and finally computes the ratio $R$ for each case.\n- Numerical output requirements: produce three floating-point numbers corresponding to the three cases, each rounded to six decimal places using standard rounding.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[r1,r2,r3]\"), where each $r_k$ is the rounded value of $R$ for case $k$ with exactly six digits after the decimal point. No other output is allowed.", "solution": "The user-provided problem has been analyzed and is determined to be valid. It is scientifically grounded, well-posed, and objective. There are no contradictions, ambiguities, or missing information. The problem is a well-defined computational exercise in the field of optimal experimental design, specifically focusing on sensor selection informed by active subspace methods.\n\nThe problem asks for a comparison between two sensor selection strategies for a linear Gaussian inverse problem. The first strategy, termed \"active-subspace-aware\" ($\\mathcal{S}_{\\text{AS}}$), leverages information about the parameter's influence on a quantity of interest. The second, \"naive\" strategy ($\\mathcal{S}_{\\text{NV}}$), uses a simple, uninformed heuristic. The comparison is quantified by a ratio $R$ that measures the relative information content captured by each sensor set, evaluated along the true direction of maximum parametric sensitivity.\n\nThe solution proceeds systematically for each test case as follows:\n\n**Step 1: Define Problem Parameters and Core Objects**\n\nFor each test case, we are given the parameter dimension $d$, the number of candidate sensors $p$, the number of sensors to select $m$, the sensor noise variance $\\sigma^2$, and a weight $\\rho$ for constructing the active subspace matrix.\n\nThe set of $p$ candidate sensor response vectors, $\\{s_i\\}_{i=0}^{p-1}$ where $s_i \\in \\mathbb{R}^d$, are provided as rows of a $p \\times d$ matrix $S$. The components of $S$ are given by the formula:\n$$\ns_{i,j} = \\sin\\big((i+1)(j+1)\\big) + 0.25\\,\\cos(i + 2j)\n$$\nfor $i \\in \\{0, 1, \\dots, p-1\\}$ and $j \\in \\{0, 1, \\dots, d-1\\}$. The arguments to the trigonometric functions are in radians.\n\n**Step 2: Construct the Active Subspace and Identify the Target Direction**\n\nThe problem provides a surrogate for the gradient covariance matrix, $C_{\\text{active}}$, which identifies the most sensitive directions in the parameter space. This matrix is constructed from two vectors, $a$ and $b$, defined by their components:\n$$\na'_j = \\cos(j+1), \\quad b'_j = \\sin(2j+1) \\quad \\text{for } j \\in \\{0, 1, \\dots, d-1\\}\n$$\nThese vectors are then normalized to have unit Euclidean norm:\n$$\na = \\frac{a'}{\\|a'\\|_2}, \\quad b = \\frac{b'}{\\|b'\\|_2}\n$$\nThe active subspace matrix is then formed as a rank-$2$ update:\n$$\nC_{\\text{active}} = a a^\\top + \\rho \\, b b^\\top\n$$\nThe principal direction of interest, denoted $v_{\\text{true}}$, is the leading eigenvector of $C_{\\text{active}}$, i.e., the unit-norm eigenvector corresponding to its largest eigenvalue. This vector represents the one-dimensional active subspace. We find $v_{\\text{true}}$ by solving the eigenvalue problem for the symmetric positive semidefinite matrix $C_{\\text{active}}$.\n\n**Step 3: Implement Sensor Selection Strategies**\n\nThe core of the design task is to select an optimal subset of $m$ sensors from the $p$ candidates. The criterion for optimality is the maximization of the Rayleigh quotient $v^\\top C(\\mathcal{S}) v$ for a given direction $v$. As stated in the problem, this is equivalent to maximizing the sum of squared projections of the sensor vectors onto $v$:\n$$\nv^\\top C(\\mathcal{S}) v = v^\\top \\left(\\frac{1}{\\sigma^2} \\sum_{i \\in \\mathcal{S}} s_i s_i^\\top\\right) v = \\frac{1}{\\sigma^2} \\sum_{i \\in \\mathcal{S}} (v^\\top s_i)^2 = \\frac{1}{\\sigma^2} \\sum_{i \\in \\mathcal{S}} (s_i^\\top v)^2\n$$\nFor a fixed direction $v$ and size $m$, this quantity is maximized by a greedy selection: choose the $m$ sensors whose vectors $s_i$ yield the largest values for the score $(s_i^\\top v)^2$.\n\n-   **Active-Subspace-Aware Selection ($\\mathcal{S}_{\\text{AS}}$)**: The design direction is $v_{\\text{AS}} = v_{\\text{true}}$. We compute the scores $(s_i^\\top v_{\\text{true}})^2$ for all $i \\in \\{0, \\dots, p-1\\}$ and select the set $\\mathcal{S}_{\\text{AS}}$ of $m$ indices corresponding to the largest scores.\n\n-   **Naive Selection ($\\mathcal{S}_{\\text{NV}}$)**: The design direction is the first standard basis vector, $v_{\\text{NV}} = e_1 = [1, 0, \\dots, 0]^\\top$. We compute the scores $(s_i^\\top e_1)^2 = s_{i,0}^2$ for all $i \\in \\{0, \\dots, p-1\\}$ and select the set $\\mathcal{S}_{\\text{NV}}$ of $m$ indices corresponding to the largest scores.\n\n**Step 4: Evaluate an Compare the Designs**\n\nThe performance of both designs is evaluated by computing the information they provide specifically along the most important direction, $v_{\\text{true}}$. The comparison metric is the ratio $R$:\n$$\nR = \\frac{v_{\\text{true}}^\\top C(\\mathcal{S}_{\\text{AS}}) v_{\\text{true}}}{v_{\\text{true}}^\\top C(\\mathcal{S}_{\\text{NV}}) v_{\\text{true}}}\n$$\nSubstituting the expression for the Rayleigh quotient, we get:\n$$\nR = \\frac{\\frac{1}{\\sigma^2} \\sum_{i \\in \\mathcal{S}_{\\text{AS}}} (s_i^\\top v_{\\text{true}})^2}{\\frac{1}{\\sigma^2} \\sum_{i \\in \\mathcal{S}_{\\text{NV}}} (s_i^\\top v_{\\text{true}})^2} = \\frac{\\sum_{i \\in \\mathcal{S}_{\\text{AS}}} (s_i^\\top v_{\\text{true}})^2}{\\sum_{i \\in \\mathcal{S}_{\\text{NV}}} (s_i^\\top v_{\\text{true}})^2}\n$$\nNotably, the noise variance $\\sigma^2$ cancels from the final ratio. Let the evaluation scores be $\\gamma_i = (s_i^\\top v_{\\text{true}})^2$. The numerator is the sum of the $m$ largest values of $\\{\\gamma_i\\}_{i=0}^{p-1}$. The denominator is the sum of $\\gamma_i$ for those indices $i$ that were selected by the naive strategy (i.e., those that maximized $(s_i^\\top e_1)^2$). This procedure is repeated for each test case, and the resulting value of $R$ is rounded to six decimal places.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the sensor design comparison ratio R for a set of test cases.\n    \"\"\"\n    test_cases = [\n        # (d, p, m, sigma^2, rho)\n        (6, 14, 3, 0.25, 0.25),\n        (6, 14, 1, 1.0, 1.0),\n        (8, 20, 4, 4.0, 0.05),\n    ]\n\n    results = []\n    for case in test_cases:\n        d, p, m, sigma_sq, rho = case\n\n        # Step 1: Construct the candidate sensor matrix S\n        i_indices = np.arange(p).reshape(p, 1)\n        j_indices = np.arange(d).reshape(1, d)\n        S_matrix = np.sin((i_indices + 1) * (j_indices + 1)) + 0.25 * np.cos(i_indices + 2 * j_indices)\n\n        # Step 2: Construct C_active and find the true active direction v_true\n        j_vec = np.arange(d)\n        \n        # Construct unnormalized vectors a' and b'\n        a_prime = np.cos(j_vec + 1)\n        b_prime = np.sin(2 * j_vec + 1)\n\n        # Normalize to get unit vectors a and b\n        a = a_prime / np.linalg.norm(a_prime)\n        b = b_prime / np.linalg.norm(b_prime)\n        \n        # Construct C_active matrix\n        C_active = np.outer(a, a) + rho * np.outer(b, b)\n        \n        # Find the leading eigenvector (v_true) of C_active\n        eigenvalues, eigenvectors = np.linalg.eigh(C_active)\n        v_true = eigenvectors[:, -1] # Eigenvector for the largest eigenvalue\n\n        # Step 3: Implement sensor selection strategies\n        \n        # Naive design direction\n        v_naive = np.zeros(d)\n        v_naive[0] = 1.0\n\n        # Scores for AS-aware design (based on v_true)\n        scores_as_design = (S_matrix @ v_true)**2\n        \n        # Scores for naive design (based on v_naive)\n        scores_nv_design = (S_matrix @ v_naive)**2\n        \n        # Get indices for the top m sensors for each strategy\n        indices_as = np.argsort(scores_as_design)[-m:]\n        indices_nv = np.argsort(scores_nv_design)[-m:]\n\n        # Step 4: Evaluate and compare the designs\n\n        # The evaluation is performed along v_true for both sets of sensors.\n        # The scores used for evaluation are the same as those for the AS design.\n        evaluation_scores = scores_as_design\n        \n        # Numerator: sum of evaluation scores for the AS-selected sensors\n        # This is equivalent to summing the top m evaluation scores.\n        numerator = np.sum(evaluation_scores[indices_as])\n        \n        # Denominator: sum of evaluation scores for the naively-selected sensors\n        denominator = np.sum(evaluation_scores[indices_nv])\n\n        # Compute the ratio R\n        if denominator == 0:\n            # This case is unlikely given the problem construction but is handled for robustness.\n            # If the naive selection is orthogonal to the true direction, the ratio is infinite\n            # assuming the AS selection is not also orthogonal.\n            R = np.inf\n        else:\n            R = numerator / denominator\n        \n        results.append(R)\n\n    # Format the final output as specified\n    formatted_results = [f'{round(r, 6):.6f}' for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3362746"}]}