{"hands_on_practices": [{"introduction": "A cornerstone of any successful particle filter is the resampling step, which combats the inevitable problem of weight degeneracy. This exercise provides a concrete, step-by-step walkthrough of stratified resampling, a variance-reduction technique that is more efficient than naive multinomial resampling. By manually calculating the offspring counts from a given set of particle weights, you will gain a practical understanding of how the filter rejuvenates its particle set, focusing computational effort on the most probable regions of the state space [@problem_id:3409843].", "problem": "Consider a nonlinear, non-Gaussian state-space model estimated with a Particle Filter (PF) within the Sequential Monte Carlo (SMC) framework. At a given assimilation cycle, suppose you have $N$ particles with unnormalized importance weights $\\tilde{w}_{1},\\ldots,\\tilde{w}_{N}$, where $N=10$ and\n$$\n\\tilde{w}_{1}=2,\\ \\tilde{w}_{2}=1,\\ \\tilde{w}_{3}=1,\\ \\tilde{w}_{4}=1,\\ \\tilde{w}_{5}=5,\\ \\tilde{w}_{6}=2,\\ \\tilde{w}_{7}=3,\\ \\tilde{w}_{8}=1,\\ \\tilde{w}_{9}=3,\\ \\tilde{w}_{10}=1.\n$$\nIn order to mitigate weight degeneracy and reduce sampling variance, you perform the resampling step using stratified resampling, which draws one uniform variate in each stratum of equal width on $[0,1)$.\n\nStarting from the core definitions of importance sampling and resampling in SMC, normalize the weights to obtain $w_{j}=\\tilde{w}_{j}/\\sum_{k=1}^{N}\\tilde{w}_{k}$, construct the cumulative distribution function (CDF) $C_{j}=\\sum_{k=1}^{j}w_{k}$ for $j=1,\\ldots,N$, and carry out stratified resampling using the following specific uniform samples in each stratum:\n$$\nu_{i}=\\frac{i-\\tfrac{1}{2}}{N},\\quad i=1,\\ldots,N,\n$$\nso that explicitly\n$$\nu_{1}=0.05,\\ u_{2}=0.15,\\ u_{3}=0.25,\\ u_{4}=0.35,\\ u_{5}=0.45,\\ u_{6}=0.55,\\ u_{7}=0.65,\\ u_{8}=0.75,\\ u_{9}=0.85,\\ u_{10}=0.95.\n$$\nUse the inverse CDF rule to assign ancestor indices $a_{i}$, defined as the smallest $j$ such that $u_{i}\\leq C_{j}$. Then compute the offspring counts $n_{j}$, defined as the number of times index $j$ appears among $\\{a_{1},\\ldots,a_{N}\\}$.\n\nProvide as your final answer a single row matrix that concatenates first the offspring counts $(n_{1},\\ldots,n_{N})$ and then the ancestor indices $(a_{1},\\ldots,a_{N})$, in that order. No rounding is required, and no units are involved. Your reasoning should begin from foundational SMC principles rather than shortcut formulas.", "solution": "The problem asks us to perform a stratified resampling step for a Particle Filter (PF) and to report the resulting offspring counts and ancestor indices. The procedure is part of the Sequential Monte Carlo (SMC) methodology used for state estimation in nonlinear, non-Gaussian systems. The core idea of resampling is to address the weight degeneracy problem, where after a few assimilation cycles, most particles have negligible importance weights. Resampling rejuvenates the particle set by duplicating particles with high weights and discarding those with low weights, effectively focusing computational resources on more probable regions of the state space.\n\nFirst, we validate the problem statement. The problem provides a well-defined set of unnormalized importance weights, a specific resampling scheme (stratified resampling), a clear algorithm for its execution, and a precise format for the output. All definitions are standard in the field of data assimilation. The data provided are self-contained, consistent, and numerically sound. The problem is scientifically grounded, well-posed, objective, and verifiable. Therefore, the problem is valid and we may proceed with the solution.\n\nThe process begins with the given set of $N=10$ unnormalized importance weights, $\\tilde{w}_{j}$ for $j=1,\\ldots,10$:\n$$\n\\tilde{w}_{1}=2,\\ \\tilde{w}_{2}=1,\\ \\tilde{w}_{3}=1,\\ \\tilde{w}_{4}=1,\\ \\tilde{w}_{5}=5,\\ \\tilde{w}_{6}=2,\\ \\tilde{w}_{7}=3,\\ \\tilde{w}_{8}=1,\\ \\tilde{w}_{9}=3,\\ \\tilde{w}_{10}=1.\n$$\nThe first step is to normalize these weights so that they sum to $1$. The sum of the unnormalized weights is:\n$$\n\\sum_{k=1}^{10} \\tilde{w}_{k} = 2+1+1+1+5+2+3+1+3+1 = 20.\n$$\nThe normalized weights, $w_{j}$, are obtained by dividing each unnormalized weight by this sum: $w_{j} = \\tilde{w}_{j} / \\sum_{k=1}^{10} \\tilde{w}_{k}$.\n$$\n\\begin{aligned}\nw_{1} = \\frac{2}{20} = 0.1 \\\\\nw_{2} = \\frac{1}{20} = 0.05 \\\\\nw_{3} = \\frac{1}{20} = 0.05 \\\\\nw_{4} = \\frac{1}{20} = 0.05 \\\\\nw_{5} = \\frac{5}{20} = 0.25 \\\\\nw_{6} = \\frac{2}{20} = 0.1 \\\\\nw_{7} = \\frac{3}{20} = 0.15 \\\\\nw_{8} = \\frac{1}{20} = 0.05 \\\\\nw_{9} = \\frac{3}{20} = 0.15 \\\\\nw_{10} = \\frac{1}{20} = 0.05\n\\end{aligned}\n$$\nThese normalized weights define a discrete probability distribution over the particle indices. The next step is to construct the cumulative distribution function (CDF), $C_{j} = \\sum_{k=1}^{j} w_{k}$.\n$$\n\\begin{aligned}\nC_{1} = w_{1} = 0.1 \\\\\nC_{2} = C_{1} + w_{2} = 0.1 + 0.05 = 0.15 \\\\\nC_{3} = C_{2} + w_{3} = 0.15 + 0.05 = 0.20 \\\\\nC_{4} = C_{3} + w_{4} = 0.20 + 0.05 = 0.25 \\\\\nC_{5} = C_{4} + w_{5} = 0.25 + 0.25 = 0.50 \\\\\nC_{6} = C_{5} + w_{6} = 0.50 + 0.1 = 0.60 \\\\\nC_{7} = C_{6} + w_{7} = 0.60 + 0.15 = 0.75 \\\\\nC_{8} = C_{7} + w_{8} = 0.75 + 0.05 = 0.80 \\\\\nC_{9} = C_{8} + w_{9} = 0.80 + 0.15 = 0.95 \\\\\nC_{10} = C_{9} + w_{10} = 0.95 + 0.05 = 1.00\n\\end{aligned}\n$$\nThe resampling step involves drawing $N$ new particles from the current set of particles, where the probability of drawing particle $j$ is $w_{j}$. Stratified resampling partitions the interval $[0,1)$ into $N$ equal strata, $[\\frac{i-1}{N}, \\frac{i}{N})$ for $i=1,\\ldots,N$. A single uniform random sample is drawn from each stratum. The problem specifies the exact samples to be used:\n$$\nu_{i} = \\frac{i - \\frac{1}{2}}{N} \\quad \\text{for } i=1,\\ldots,10.\n$$\nThe given samples are:\n$$\nu_{1}=0.05,\\ u_{2}=0.15,\\ u_{3}=0.25,\\ u_{4}=0.35,\\ u_{5}=0.45,\\ u_{6}=0.55,\\ u_{7}=0.65,\\ u_{8}=0.75,\\ u_{9}=0.85,\\ u_{10}=0.95.\n$$\nWe use the inverse CDF method to find the ancestor index $a_i$ for each sample $u_i$. The rule is to find the smallest index $j$ such that $u_i \\leq C_j$. This is equivalent to finding which interval $(C_{j-1}, C_j]$ (with $C_0=0$) contains $u_i$.\n\n- For $u_1=0.05$: We need the smallest $j$ such that $0.05 \\le C_j$. Since $C_1=0.1$, the condition holds for $j=1$. Thus, $a_1=1$.\n- For $u_2=0.15$: We need the smallest $j$ such that $0.15 \\le C_j$. $C_1=0.1  0.15$, but $C_2=0.15 \\ge 0.15$. The smallest such $j$ is $2$. Thus, $a_2=2$.\n- For $u_3=0.25$: We need the smallest $j$ such that $0.25 \\le C_j$. $C_3=0.20  0.25$, but $C_4=0.25 \\ge 0.25$. The smallest such $j$ is $4$. Thus, $a_3=4$.\n- For $u_4=0.35$: We need the smallest $j$ such that $0.35 \\le C_j$. $C_4=0.25  0.35$, but $C_5=0.50 \\ge 0.35$. The smallest such $j$ is $5$. Thus, $a_4=5$.\n- For $u_5=0.45$: We need the smallest $j$ such that $0.45 \\le C_j$. $C_4=0.25  0.45$, but $C_5=0.50 \\ge 0.45$. The smallest such $j$ is $5$. Thus, $a_5=5$.\n- For $u_6=0.55$: We need the smallest $j$ such that $0.55 \\le C_j$. $C_5=0.50  0.55$, but $C_6=0.60 \\ge 0.55$. The smallest such $j$ is $6$. Thus, $a_6=6$.\n- For $u_7=0.65$: We need the smallest $j$ such that $0.65 \\le C_j$. $C_6=0.60  0.65$, but $C_7=0.75 \\ge 0.65$. The smallest such $j$ is $7$. Thus, $a_7=7$.\n- For $u_8=0.75$: We need the smallest $j$ such that $0.75 \\le C_j$. $C_6=0.60  0.75$, but $C_7=0.75 \\ge 0.75$. The smallest such $j$ is $7$. Thus, $a_8=7$.\n- For $u_9=0.85$: We need the smallest $j$ such that $0.85 \\le C_j$. $C_8=0.80  0.85$, but $C_9=0.95 \\ge 0.85$. The smallest such $j$ is $9$. Thus, $a_9=9$.\n- For $u_{10}=0.95$: We need the smallest $j$ such that $0.95 \\le C_j$. $C_8=0.80  0.95$, but $C_9=0.95 \\ge 0.95$. The smallest such $j$ is $9$. Thus, $a_{10}=9$.\n\nThe set of ancestor indices is $\\{a_1,\\ldots,a_{10}\\} = \\{1, 2, 4, 5, 5, 6, 7, 7, 9, 9\\}$. The new set of particles will consist of copies of these ancestor particles.\n\nFinally, we compute the offspring counts, $n_j$, which is the number of times each original particle index $j$ is selected as an ancestor. We count the occurrences of each index in the set $\\{a_1, \\ldots, a_{10}\\}$.\n- $n_1$ (count of $1$'s): $1$\n- $n_2$ (count of $2$'s): $1$\n- $n_3$ (count of $3$'s): $0$\n- $n_4$ (count of $4$'s): $1$\n- $n_5$ (count of $5$'s): $2$\n- $n_6$ (count of $6$'s): $1$\n- $n_7$ (count of $7$'s): $2$\n- $n_8$ (count of $8$'s): $0$\n- $n_9$ (count of $9$'s): $2$\n- $n_{10}$ (count of $10$'s): $0$\n\nThe vector of offspring counts is $(n_1,\\ldots,n_{10}) = (1, 1, 0, 1, 2, 1, 2, 0, 2, 0)$. As a check, the sum of offspring counts must be $N$: $1+1+0+1+2+1+2+0+2+0=10$.\nThe vector of ancestor indices is $(a_1,\\ldots,a_{10}) = (1, 2, 4, 5, 5, 6, 7, 7, 9, 9)$.\n\nThe final answer is the concatenation of the offspring counts and the ancestor indices into a single row matrix.\n$$\n(n_1,\\ldots,n_{10}, a_1,\\ldots,a_{10}) = (1, 1, 0, 1, 2, 1, 2, 0, 2, 0, 1, 2, 4, 5, 5, 6, 7, 7, 9, 9).\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  1  0  1  2  1  2  0  2  0  1  2  4  5  5  6  7  7  9  9\n\\end{pmatrix}\n}\n$$", "id": "3409843"}, {"introduction": "Particle filters are most valuable in scenarios where simpler methods, like the Extended Kalman Filter (EKF), fall short. This practice illuminates the core motivation for using particle filters by directly comparing a full numerical solution of the Bayesian update with the EKF's approximation for a nonlinear, non-Gaussian problem. By implementing both approaches, you will directly observe the impact of linearization and Gaussian assumptions, providing a clear, practical justification for the superior accuracy of particle-based methods in complex systems [@problem_id:3409807].", "problem": "Consider a scalar hidden Markov model for data assimilation with a nonlinear observation operator and non-Gaussian observation noise. Let the hidden state at time $t$ be $x_t \\in \\mathbb{R}$. Assume that the prior (also called the one-step prediction) at time $t$ is Gaussian with mean $m_t^{-}$ and variance $P_t^{-}$, that is, $p(x_t \\mid y_{1:t-1}) = \\mathcal{N}(x_t \\mid m_t^{-}, P_t^{-})$. The observation at time $t$ is $y_t \\in \\mathbb{R}$ and is related to the state through a known nonlinear function $h(\\cdot)$ with additive noise, $y_t = h(x_t) + \\varepsilon_t$, where the noise $\\varepsilon_t$ follows a Student's $t$ distribution with degrees of freedom $\\nu$ and scale $s$, so that its probability density function is \n$$\np_{\\varepsilon}(e) = \\frac{\\Gamma\\!\\left(\\frac{\\nu + 1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}\\, s} \\left(1 + \\frac{1}{\\nu}\\left(\\frac{e}{s}\\right)^2\\right)^{-\\frac{\\nu + 1}{2}}.\n$$\nThe observation operator is given by \n$$\nh(x) = x + a \\sin(b x),\n$$\nfor parameters $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}$. For this problem, assume all quantities are dimensionless and no physical units are involved.\n\nYour tasks are:\n\n1. Using Bayes' theorem and the given prior, likelihood, and observation operator, define the filtering posterior density \n$$\np(x_t \\mid y_{1:t}) \\propto p(y_t \\mid x_t) \\, p(x_t \\mid y_{1:t-1}),\n$$\nand compute numerically its first two moments, namely the posterior mean \n$$\n\\mu_t = \\mathbb{E}[x_t \\mid y_{1:t}],\n$$\nand the posterior variance \n$$\n\\sigma_t^2 = \\mathbb{V}[x_t \\mid y_{1:t}],\n$$\nfor each test case below. The numerical computation must be carried out by direct numerical integration over $x \\in \\mathbb{R}$ of the unnormalized posterior, without using any Gaussian approximation or linearization.\n\n2. Compute the moments predicted by the Extended Kalman Filter (EKF), which in this context is defined as follows. Linearize the observation operator at the predicted mean $m_t^{-}$ using a first-order Taylor expansion,\n$$\nh(x) \\approx h(m_t^{-}) + H_t (x - m_t^{-}), \\quad \\text{where } H_t = \\left.\\frac{dh}{dx}\\right|_{x = m_t^{-}} = 1 + a b \\cos(b m_t^{-}).\n$$\nApproximate the observation noise as Gaussian with the same variance as the Student's $t$ distribution, namely \n$$\nR = \\frac{\\nu}{\\nu - 2} s^2 \\quad \\text{for } \\nu  2.\n$$\nThen compute the EKF update for the mean and variance using the standard scalar Kalman filter formulas,\n$$\nS_t = H_t^2 P_t^{-} + R, \\quad K_t = \\frac{P_t^{-} H_t}{S_t}, \\quad m_t^{\\text{EKF}} = m_t^{-} + K_t \\left(y_t - h(m_t^{-})\\right), \\quad P_t^{\\text{EKF}} = \\left(1 - K_t H_t\\right) P_t^{-}.\n$$\n\nYour program must, for each test case, produce the pair of numerical posterior moments $\\mu_t$ and $\\sigma_t^2$, the EKF moments $m_t^{\\text{EKF}}$ and $P_t^{\\text{EKF}}$, and the absolute differences $\\lvert \\mu_t - m_t^{\\text{EKF}} \\rvert$ and $\\lvert \\sigma_t^2 - P_t^{\\text{EKF}} \\rvert$.\n\nUse the following test suite of parameter values $(m_t^{-}, P_t^{-}, y_t, a, b, \\nu, s)$, which covers a typical case, a small-variance regime where linearization is expected to be accurate, a highly nonlinear and dispersed prior case, and a heavy-tailed outlier case:\n\n- Test case $1$: $(0.0, 1.0, 1.0, 0.5, 1.0, 5.0, 0.5)$.\n- Test case $2$: $(0.5, 0.01, 0.2, 0.8, 2.0, 7.0, 0.3)$.\n- Test case $3$: $(-1.0, 4.0, 2.5, 1.2, 2.0, 4.0, 0.7)$.\n- Test case $4$: $(0.0, 1.5, 4.0, 0.7, 1.5, 3.1, 1.0)$.\n\nFor numerical integration, you must integrate over $x \\in (-\\infty, \\infty)$ and compute the normalization constant, the first raw moment, and the second raw moment, and then obtain the variance by subtraction. All integrals must be computed to reasonable numerical accuracy using standard adaptive quadrature; do not discretize the line coarsely.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be flattened in the following fixed order for the four test cases: for each test case in the order given above, output $[\\mu_t, \\sigma_t^2, m_t^{\\text{EKF}}, P_t^{\\text{EKF}}, \\lvert \\mu_t - m_t^{\\text{EKF}} \\rvert, \\lvert \\sigma_t^2 - P_t^{\\text{EKF}} \\rvert]$, and then concatenate these for all test cases into a single list. For example, the output should look like $[\\text{case1\\_mu}, \\text{case1\\_var}, \\text{case1\\_m\\_ekf}, \\text{case1\\_P\\_ekf}, \\text{case1\\_dmu}, \\text{case1\\_dvar}, \\text{case2\\_mu}, \\ldots, \\text{case4\\_dvar}]$, where each placeholder is a real number.", "solution": "The problem is valid as it is scientifically grounded in the principles of Bayesian filtering and data assimilation, is well-posed with all necessary information provided, and is formulated objectively. We will proceed with a full solution.\n\nThe core of the problem is to perform a Bayesian update for a scalar hidden state $x_t \\in \\mathbb{R}$ at time $t$. The update combines a prior belief about the state with new information from an observation $y_t$. The posterior probability density function (PDF) for $x_t$ given all observations up to time $t$, denoted $y_{1:t}$, is given by Bayes' theorem:\n$$\np(x_t \\mid y_{1:t}) \\propto p(y_t \\mid x_t) \\, p(x_t \\mid y_{1:t-1})\n$$\nHere, $p(x_t \\mid y_{1:t-1})$ is the prior PDF (the prediction from the previous time step), and $p(y_t \\mid x_t)$ is the likelihood function, which quantifies the probability of observing $y_t$ given a specific state $x_t$.\n\nThe problem specifies the prior distribution as Gaussian:\n$$\np(x_t \\mid y_{1:t-1}) = \\mathcal{N}(x_t \\mid m_t^{-}, P_t^{-}) = \\frac{1}{\\sqrt{2 \\pi P_t^{-}}} \\exp\\left(-\\frac{(x_t - m_t^{-})^2}{2 P_t^{-}}\\right)\n$$\nwhere $m_t^{-}$ is the prior mean and $P_t^{-}$ is the prior variance.\n\nThe observation model is $y_t = h(x_t) + \\varepsilon_t$, with a nonlinear observation operator $h(x) = x + a \\sin(b x)$ and additive noise $\\varepsilon_t$. The noise follows a Student's $t$-distribution with $\\nu$ degrees of freedom and scale parameter $s$. The likelihood function is therefore the PDF of the noise evaluated at the residual, $e = y_t - h(x_t)$:\n$$\np(y_t \\mid x_t) = p_{\\varepsilon}(y_t - h(x_t)) = \\frac{\\Gamma\\!\\left(\\frac{\\nu + 1}{2}\\right)}{\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu \\pi}\\, s} \\left(1 + \\frac{1}{\\nu}\\left(\\frac{y_t - h(x_t)}{s}\\right)^2\\right)^{-\\frac{\\nu + 1}{2}}\n$$\n\nThe unnormalized posterior density, let's call it $q(x_t)$, is the product of the prior and the likelihood. For numerical computation, we can use a version of $q(x_t)$ that is proportional to the true unnormalized posterior by dropping constant factors:\n$$\nq(x_t) \\propto \\exp\\left(-\\frac{(x_t - m_t^{-})^2}{2 P_t^{-}}\\right) \\left(1 + \\frac{1}{\\nu}\\left(\\frac{y_t - (x_t + a \\sin(b x_t))}{s}\\right)^2\\right)^{-\\frac{\\nu + 1}{2}}\n$$\n\n**1. Numerical Computation of Posterior Moments**\nTo find the exact posterior mean $\\mu_t$ and variance $\\sigma_t^2$, we must perform numerical integration. We first compute the necessary raw moments of the unnormalized posterior $q(x_t)$:\nThe normalization constant (zeroth moment):\n$$\nZ = \\int_{-\\infty}^{\\infty} q(x_t) \\, dx_t\n$$\nThe first raw moment:\n$$\nM_1 = \\int_{-\\infty}^{\\infty} x_t \\, q(x_t) \\, dx_t\n$$\nThe second raw moment:\n$$\nM_2 = \\int_{-\\infty}^{\\infty} x_t^2 \\, q(x_t) \\, dx_t\n$$\nThese integrals are computed using adaptive quadrature over the domain $(-\\infty, \\infty)$. The posterior mean and variance are then calculated as:\n$$\n\\mu_t = \\mathbb{E}[x_t \\mid y_{1:t}] = \\frac{M_1}{Z}\n$$\n$$\n\\sigma_t^2 = \\mathbb{V}[x_t \\mid y_{1:t}] = \\mathbb{E}[x_t^2 \\mid y_{1:t}] - (\\mathbb{E}[x_t \\mid y_{1:t}])^2 = \\frac{M_2}{Z} - \\mu_t^2\n$$\n\n**2. Extended Kalman Filter (EKF) Approximation**\nThe EKF simplifies the problem by making two key approximations:\n1. The nonlinear observation operator $h(x_t)$ is linearized around the prior mean $m_t^{-}$ using a first-order Taylor expansion:\n$$\nh(x_t) \\approx h(m_t^{-}) + H_t (x_t - m_t^{-})\n$$\nwhere $H_t$ is the Jacobian of $h$ evaluated at $m_t^{-}$. For the given scalar function, this is the derivative:\n$$\nH_t = \\left.\\frac{dh}{dx}\\right|_{x = m_t^{-}} = 1 + a b \\cos(b m_t^{-})\n$$\n2. The non-Gaussian observation noise $\\varepsilon_t$ is approximated by a zero-mean Gaussian distribution with the same variance as the Student's $t$-distribution. For $\\nu  2$, this variance is:\n$$\nR = \\frac{\\nu}{\\nu - 2} s^2\n$$\nWith these approximations, the problem becomes a standard linear-Gaussian update, for which the Kalman filter equations provide the exact posterior moments. These are the EKF-approximated moments:\nThe innovation covariance:\n$$\nS_t = H_t^2 P_t^{-} + R\n$$\nThe Kalman gain:\n$$\nK_t = \\frac{P_t^{-} H_t}{S_t}\n$$\nThe EKF posterior mean:\n$$\nm_t^{\\text{EKF}} = m_t^{-} + K_t \\left(y_t - h(m_t^{-})\\right)\n$$\nThe EKF posterior variance:\n$$\nP_t^{\\text{EKF}} = \\left(1 - K_t H_t\\right) P_t^{-}\n$$\nThe program below implements both the numerical integration for the exact moments and the EKF equations for the approximate moments for each of the specified test cases. The required absolute differences, $\\lvert \\mu_t - m_t^{\\text{EKF}} \\rvert$ and $\\lvert \\sigma_t^2 - P_t^{\\text{EKF}} \\rvert$, are then computed to compare the two approaches.", "answer": "```python\nimport numpy as np\nfrom scipy import integrate\nfrom scipy import stats\n\ndef solve():\n    \"\"\"\n    Computes and compares numerical Bayesian and EKF updates for a scalar HMM.\n    \"\"\"\n    test_cases = [\n        # (m_minus, P_minus, y_t, a, b, nu, s)\n        (0.0, 1.0, 1.0, 0.5, 1.0, 5.0, 0.5),  # Case 1: Typical case\n        (0.5, 0.01, 0.2, 0.8, 2.0, 7.0, 0.3),  # Case 2: Small prior variance\n        (-1.0, 4.0, 2.5, 1.2, 2.0, 4.0, 0.7), # Case 3: High nonlinearity\n        (0.0, 1.5, 4.0, 0.7, 1.5, 3.1, 1.0),   # Case 4: Heavy-tailed outlier\n    ]\n\n    results = []\n\n    def h_func(x, a, b):\n        \"\"\" The nonlinear observation operator h(x). \"\"\"\n        return x + a * np.sin(b * x)\n\n    def unnormalized_posterior(x, m_minus, P_minus, y, a, b, nu, s):\n        \"\"\"\n        Computes the unnormalized posterior density q(x) = p(y|x)p(x).\n        \"\"\"\n        # Likelihood term p(y|x)\n        residual = y - h_func(x, a, b)\n        likelihood = stats.t.pdf(residual, df=nu, loc=0, scale=s)\n        \n        # Prior term p(x)\n        prior = stats.norm.pdf(x, loc=m_minus, scale=np.sqrt(P_minus))\n        \n        return likelihood * prior\n\n    for case in test_cases:\n        m_minus, P_minus, y, a, b, nu, s = case\n        params = (m_minus, P_minus, y, a, b, nu, s)\n\n        # 1. Numerical Bayesian Update\n        \n        # Define integrands for moments\n        integrand_Z = lambda x, *p: unnormalized_posterior(x, *p)\n        integrand_M1 = lambda x, *p: x * unnormalized_posterior(x, *p)\n        integrand_M2 = lambda x, *p: x**2 * unnormalized_posterior(x, *p)\n\n        # Numerically integrate using adaptive quadrature\n        Z, _ = integrate.quad(integrand_Z, -np.inf, np.inf, args=params)\n        M1, _ = integrate.quad(integrand_M1, -np.inf, np.inf, args=params)\n        M2, _ = integrate.quad(integrand_M2, -np.inf, np.inf, args=params)\n\n        # Calculate posterior mean and variance\n        mu_t = M1 / Z\n        sigma2_t = M2 / Z - mu_t**2\n\n        # 2. Extended Kalman Filter (EKF) Update\n        \n        # Linearization\n        h_m_minus = h_func(m_minus, a, b)\n        H_t = 1.0 + a * b * np.cos(b * m_minus)\n        \n        # Observation noise variance approximation\n        R = (nu / (nu - 2.0)) * s**2\n        \n        # Standard Kalman filter update equations\n        S_t = H_t**2 * P_minus + R\n        K_t = (P_minus * H_t) / S_t\n        m_ekf = m_minus + K_t * (y - h_m_minus)\n        P_ekf = (1.0 - K_t * H_t) * P_minus\n        \n        # 3. Compute Absolute Differences\n        d_mu = np.abs(mu_t - m_ekf)\n        d_sigma2 = np.abs(sigma2_t - P_ekf)\n        \n        results.extend([mu_t, sigma2_t, m_ekf, P_ekf, d_mu, d_sigma2])\n\n    # Format and print the final output string\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3409807"}, {"introduction": "Many physical and financial systems are naturally described by continuous-time dynamics modeled with Stochastic Differential Equations (SDEs). This exercise transitions from discrete-time filters to this more advanced domain, challenging you to implement a single step of a particle filter for an SDE. You will use the Euler-Maruyama method for propagation and, crucially, apply a weight correction derived from Girsanov's theorem to account for the mismatch between the true model dynamics and a simplified proposal, a fundamental technique in advanced particle filtering [@problem_id:3409822].", "problem": "You are to implement a single time-step of a continuous-time bootstrap particle filter for a nonlinear, non-Gaussian data assimilation problem. The latent state evolves according to a Stochastic Differential Equation (SDE, stochastic differential equation) with constant diffusion and nonlinear drift. A set of particles approximate the filtering distribution at time $t$ and must be propagated to time $t + \\Delta t$ using Euler-Maruyama discretization and reweighted to account for both the model transition and an observation with heavy-tailed noise.\n\nFundamental setup:\n- The latent state $x_t \\in \\mathbb{R}$ evolves according to the SDE\n$$\n\\mathrm{d}x_t = f(x_t) \\,\\mathrm{d}t + \\sigma \\,\\mathrm{d}W_t,\n$$\nwhere $W_t$ is Brownian motion, $\\sigma  0$ is a known scalar diffusion coefficient, and $f$ is a nonlinear drift. The proposal for the particle propagation is given by a drift $g$ that may differ from the true drift $f$.\n- The Euler-Maruyama (EM, Euler-Maruyama) discretization used for the proposal computes, for each particle $x^{(i)}_t$,\n$$\nx^{(i)}_{t+\\Delta t} = x^{(i)}_t + g\\big(x^{(i)}_t\\big)\\,\\Delta t + \\sigma \\,\\Delta W^{(i)},\n$$\nwhere the Brownian increment $\\Delta W^{(i)}$ is drawn as a normal random variable with mean $0$ and variance $\\Delta t$.\n- An observation at time $t+\\Delta t$ is provided via\n$$\ny = h\\big(x_{t+\\Delta t}\\big) + v,\n$$\nwith $h$ nonlinear and $v$ drawn from a Student-$t$ distribution with degrees-of-freedom $\\nu$ and scale $s  0$. The observation noise is heavy-tailed and not Gaussian.\n\nYour task for a single time-step:\n1. Initialize $N$ particles at time $t$ from a specified distribution with uniform weights.\n2. Propagate particles to time $t+\\Delta t$ using the EM proposal with drift $g$ and diffusion $\\sigma$.\n3. Compute the importance weight correction due to mismatch between the target model transition (with drift $f$) and the proposal transition (with drift $g$) across the time-step, grounded in basic principles for change of measure in SDEs with constant diffusion. Use a principled discretization of the continuous-time Radon-Nikodym derivative for a single step: it must depend on the drift difference, the diffusion, the time-step, and the realized Brownian increments for each particle.\n4. Incorporate the observation likelihood for the nonlinear measurement $h$ with Student-$t$ noise. It is acceptable to use the likelihood up to a constant factor because normalized importance weights remove constants common to all particles.\n5. Normalize the updated weights and compute the effective sample size (ESS, effective sample size), defined as\n$$\n\\mathrm{ESS} = \\frac{1}{\\sum_{i=1}^N \\big(w^{(i)}\\big)^2},\n$$\nwhere $w^{(i)}$ are the normalized weights at time $t+\\Delta t$.\n6. Resampling is not performed; only report the ESS.\n\nModel specification:\n- Drift $f$ is\n$$\nf(x) = \\alpha \\,\\tanh(x),\n$$\nwith a known scalar parameter $\\alpha  0$.\n- Proposal drift $g$ is one of:\n  - $g(x) = f(x)$ (no mismatch),\n  - $g(x) = \\alpha x$ (linearization mismatch),\n  - $g(x) = 0$ (zero-drift mismatch).\n- Measurement function is\n$$\nh(x) = x^3.\n$$\n\nParticle initialization:\n- Number of particles $N = 250$.\n- Initial particles $x^{(i)}_t$ are independent and identically distributed as standard normal with mean $0$ and variance $1$, generated with a fixed random seed for reproducibility.\n- Initial weights are uniform $w^{(i)}_t = \\frac{1}{N}$.\n\nRandomness and reproducibility:\n- You must use the specified random seeds for initializing the particles and for generating Brownian increments in each test case.\n- Each test case uses its own seed for the Brownian increments; initial particles are generated once with a specified seed and reused across cases.\n\nTest suite and parameters:\n- Common parameters across all test cases: $\\alpha = 1.3$, $\\sigma = 0.6$, $N = 250$, initial particle seed $= 2025$, $h(x) = x^3$, degrees-of-freedom $\\nu = 5$, scale $s = 0.7$.\n- Test Case A (happy path):\n  - Time-step $\\Delta t = 0.1$,\n  - Proposal drift type: $g(x) = f(x)$ (no mismatch),\n  - Observation value $y = 1.0$,\n  - Brownian increment seed $= 10$.\n- Test Case B (significant mismatch in dynamics):\n  - Time-step $\\Delta t = 0.1$,\n  - Proposal drift type: $g(x) = 0$ (zero-drift),\n  - Observation value $y = 1.0$,\n  - Brownian increment seed $= 20$.\n- Test Case C (small time-step boundary):\n  - Time-step $\\Delta t = 10^{-3}$,\n  - Proposal drift type: $g(x) = \\alpha x$ (linearization),\n  - Observation value $y = 1.0$,\n  - Brownian increment seed $= 30$.\n- Test Case D (outlier observation):\n  - Time-step $\\Delta t = 0.1$,\n  - Proposal drift type: $g(x) = \\alpha x$ (linearization),\n  - Observation value $y = 8.0$,\n  - Brownian increment seed $= 40$.\n\nAlgorithmic constraints:\n- The importance weight correction for dynamics must be derived from first principles appropriate for constant-diffusion SDEs, ensuring scientific realism, and implemented for a single Euler-Maruyama step with the realized Brownian increments.\n- The observation likelihood must reflect the Student-$t$ noise for $y - h(x_{t+\\Delta t})$ with degrees-of-freedom $\\nu$ and scale $s$, up to a constant factor.\n- All computations must be numerically stable; for example, perform weight calculations in the logarithmic domain where appropriate.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, namely the four computed $\\mathrm{ESS}$ values for Test Cases A, B, C, and D in that order, as floating-point numbers. For example, the output format is\n$$\n[\\mathrm{ESS}_A, \\mathrm{ESS}_B, \\mathrm{ESS}_C, \\mathrm{ESS}_D].\n$$\nNo additional text should be printed.\n\nYour implementation must be a complete, runnable program, in Python, version 3.12, using only the standard library, NumPy version 1.23.5, and SciPy version 1.11.4 (no other libraries). The program must be self-contained with no external inputs or files.", "solution": "The user-provided problem statement is deemed valid. It is scientifically grounded in the theory of stochastic differential equations and particle filtering, well-posed with all necessary parameters and conditions defined, and objective in its formulation. We can therefore proceed with a full solution.\n\nThe problem requires the implementation of a single time-step of a bootstrap particle filter for a state-space model defined in continuous time. The latent state $x_t \\in \\mathbb{R}$ evolves according to a Stochastic Differential Equation (SDE), and observations are nonlinear and corrupted by heavy-tailed noise. The core of the task is to propagate an ensemble of particles and update their importance weights according to both the model dynamics and a new observation.\n\nThe overall procedure for a single time-step from $t$ to $t+\\Delta t$ is as follows:\n1.  **Propagation:** Each particle $x^{(i)}_t$ is advanced to a new state $x^{(i)}_{t+\\Delta t}$ using a proposal distribution.\n2.  **Weighting:** The importance weight of each particle is updated to reflect how well it aligns with both the true model dynamics and the observation at time $t+\\Delta t$.\n3.  **Evaluation:** The Effective Sample Size (ESS) is computed from the new, normalized weights to quantify weight degeneracy.\n\nWe now detail each step based on the provided model specification.\n\n**1. Particle Propagation**\n\nThe state of each particle $i \\in \\{1, \\dots, N\\}$ is propagated from time $t$ to $t+\\Delta t$ using the Euler-Maruyama discretization of the proposal SDE. The proposal dynamics use a drift function $g(x)$ which may differ from the true model drift $f(x)$.\n\nThe update rule for particle $i$ is:\n$$\nx^{(i)}_{t+\\Delta t} = x^{(i)}_t + g\\big(x^{(i)}_t\\big)\\,\\Delta t + \\sigma \\,\\Delta W^{(i)}\n$$\nwhere $\\Delta W^{(i)}$ is a random sample representing the Brownian increment over the interval of length $\\Delta t$. These increments are drawn independently for each particle from a normal distribution with mean $0$ and variance $\\Delta t$, i.e., $\\Delta W^{(i)} \\sim \\mathcal{N}(0, \\Delta t)$.\n\n**2. Importance Weight Update**\n\nGiven uniform weights $w^{(i)}_t = 1/N$ at time $t$, the new, unnormalized weights $\\tilde{w}^{(i)}_{t+\\Delta t}$ at time $t+\\Delta t$ are given by the product of a dynamics correction factor and an observation likelihood term:\n$$\n\\tilde{w}^{(i)}_{t+\\Delta t} \\propto w_{\\text{dyn}}^{(i)} \\times L^{(i)}\n$$\nFor numerical stability, computations are performed in the logarithmic domain:\n$$\n\\log \\tilde{w}^{(i)}_{t+\\Delta t} = \\log w_{\\text{dyn}}^{(i)} + \\log L^{(i)} + C\n$$\nwhere $C$ is an arbitrary constant.\n\n**2.1. Dynamics Correction Weight ($w_{\\text{dyn}}^{(i)}$)**\n\nThis term corrects for the discrepancy between the proposal dynamics (with drift $g$) and the target dynamics (with drift $f$). The importance weight is the ratio of the target transition probability density to the proposal transition probability density, $w_{\\text{dyn}}^{(i)} = p(x^{(i)}_{t+\\Delta t} | x^{(i)}_t) / q(x^{(i)}_{t+\\Delta t} | x^{(i)}_t)$.\n\nFor the Euler-Maruyama scheme with constant diffusion $\\sigma$, both densities are Gaussian:\n-   Target: $p(x_{t+\\Delta t} | x_t) = \\mathcal{N}(x_{t+\\Delta t} | x_t + f(x_t)\\Delta t, \\sigma^2 \\Delta t)$\n-   Proposal: $q(x_{t+\\Delta t} | x_t) = \\mathcal{N}(x_{t+\\Delta t} | x_t + g(x_t)\\Delta t, \\sigma^2 \\Delta t)$\n\nThe logarithm of the ratio of these two densities, after canceling common terms, yields the log-importance weight. This result is a first-order discretization of the Girsanov theorem's Radon-Nikodym derivative. For particle $i$, the log-weight for the dynamics correction is:\n$$\n\\log w_{\\text{dyn}}^{(i)} = \\frac{f(x_t^{(i)}) - g(x_t^{(i)})}{\\sigma^2} \\left( \\sigma \\Delta W^{(i)} \\right) - \\frac{1}{2} \\left( \\frac{f(x_t^{(i)}) - g(x_t^{(i)})}{\\sigma} \\right)^2 \\Delta t\n$$\nHere, $f(x) = \\alpha \\tanh(x)$ is the true drift, $g(x)$ is the proposal drift specified by the test case, and $\\sigma \\Delta W^{(i)} = x^{(i)}_{t+\\Delta t} - x^{(i)}_t - g(x_t^{(i)})\\Delta t$ is the stochastic component realized during the proposal step. If the proposal drift matches the true drift ($g=f$), this term is zero, and no correction is needed.\n\n**2.2. Observation Likelihood ($L^{(i)}$)**\n\nThe observation model is $y = h(x_{t+\\Delta t}) + v$, where the noise $v$ follows a Student-$t$ distribution with $\\nu$ degrees of freedom and scale parameter $s$. The probability density function (PDF) for such a distribution is:\n$$\np(v; \\nu, s) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\pi\\nu}s} \\left(1 + \\frac{1}{\\nu}\\left(\\frac{v}{s}\\right)^2\\right)^{-\\frac{\\nu+1}{2}}\n$$\nThe observation likelihood for particle $i$, given its propagated state $x^{(i)}_{t+\\Delta t}$, is the PDF evaluated at the residual $v^{(i)} = y - h(x^{(i)}_{t+\\Delta t})$. Because the final weights are normalized, any constant multiplicative factors in the likelihood are irrelevant. The log-likelihood for particle $i$ is thus:\n$$\n\\log L^{(i)} \\propto -\\frac{\\nu+1}{2} \\log\\left(1 + \\frac{1}{\\nu}\\left(\\frac{y - h\\left(x^{(i)}_{t+\\Delta t}\\right)}{s}\\right)^2\\right)\n$$\nThis expression captures the relative probability of observing $y$ for each particle's proposed state.\n\n**3. Weight Normalization and ESS Calculation**\n\nThe total unnormalized log-weights, $\\log \\tilde{w}^{(i)}_{t+\\Delta t}$, are combined and then normalized to sum to one. To prevent numerical underflow, the log-sum-exp trick is employed. First, we find the maximum log-weight, $C = \\max_i(\\log \\tilde{w}^{(i)}_{t+\\Delta t})$. Then, the normalized weights $w^{(i)}_{t+\\Delta t}$ are computed as:\n$$\nw^{(i)}_{t+\\Delta t} = \\frac{\\exp\\left(\\log \\tilde{w}^{(i)}_{t+\\Delta t} - C\\right)}{\\sum_{j=1}^{N} \\exp\\left(\\log \\tilde{w}^{(j)}_{t+\\Delta t} - C\\right)}\n$$\nFinally, the Effective Sample Size (ESS) is calculated to assess the degeneracy of the particle weights. A low ESS indicates that a few particles have very high weights, while the rest are negligible, suggesting the particle representation of the distribution is poor. The ESS is given by:\n$$\n\\mathrm{ESS} = \\frac{1}{\\sum_{i=1}^N \\left(w^{(i)}_{t+\\Delta t}\\right)^2}\n$$\nAn ESS value close to the total number of particles, $N$, indicates that the weights are nearly uniform, which is ideal. An ESS close to $1$ signifies extreme degeneracy. The analysis is performed for each of the four specified test cases.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t as student_t\nfrom scipy.special import logsumexp\n\ndef solve():\n    \"\"\"\n    Implements a single time-step of a bootstrap particle filter for a nonlinear,\n    non-Gaussian data assimilation problem and computes the Effective Sample Size (ESS)\n    for four different test cases.\n    \"\"\"\n    \n    # Common parameters across all test cases\n    alpha = 1.3\n    sigma = 0.6\n    N = 250\n    initial_particle_seed = 2025\n    nu = 5.0\n    s_scale = 0.7\n\n    # Define the core model functions\n    f_drift = lambda x: alpha * np.tanh(x)\n    h_obs = lambda x: x**3\n\n    # Define the parameters for each test case\n    test_cases_params = [\n        {'dt': 0.1, 'g_type': 'identity', 'y_obs': 1.0, 'brownian_seed': 10}, # Case A\n        {'dt': 0.1, 'g_type': 'zero', 'y_obs': 1.0, 'brownian_seed': 20},     # Case B\n        {'dt': 1e-3, 'g_type': 'linear', 'y_obs': 1.0, 'brownian_seed': 30},  # Case C\n        {'dt': 0.1, 'g_type': 'linear', 'y_obs': 8.0, 'brownian_seed': 40},   # Case D\n    ]\n\n    # --- Step 1: Initialize particles (done once for all cases) ---\n    # These are the particles at time t, x_t.\n    rng_initial = np.random.default_rng(initial_particle_seed)\n    x_t = rng_initial.normal(loc=0.0, scale=1.0, size=N)\n\n    results_ess = []\n    \n    for case_params in test_cases_params:\n        dt = case_params['dt']\n        g_type = case_params['g_type']\n        y_obs = case_params['y_obs']\n        brownian_seed = case_params['brownian_seed']\n\n        # Define the proposal drift function g(x) based on the test case\n        if g_type == 'identity':\n            g_drift = f_drift\n        elif g_type == 'zero':\n            g_drift = lambda x: np.zeros_like(x)\n        elif g_type == 'linear':\n            g_drift = lambda x: alpha * x\n        else:\n            # This path should not be reached with the given test cases\n            raise ValueError(f\"Unknown proposal drift type: {g_type}\")\n\n        # --- Step 2: Propagate particles (Prediction) ---\n        # Generate Brownian increments for this specific case\n        rng_brownian = np.random.default_rng(brownian_seed)\n        delta_W = rng_brownian.normal(loc=0.0, scale=np.sqrt(dt), size=N)\n        \n        g_val_t = g_drift(x_t)\n        # These are the propagated particles at time t+dt, x_{t+dt}\n        x_t_plus_dt = x_t + g_val_t * dt + sigma * delta_W\n\n        # --- Step 3  4: Compute unnormalized log weights (Update) ---\n        \n        # Part 1: Log importance weight correction for dynamics mismatch\n        f_val_t = f_drift(x_t)\n        drift_diff = f_val_t - g_val_t\n        \n        log_dyn_correction = (drift_diff / sigma**2) * (sigma * delta_W) - 0.5 * (drift_diff / sigma)**2 * dt\n\n        # Part 2: Log likelihood from the observation\n        h_val_t_plus_dt = h_obs(x_t_plus_dt)\n        log_obs_likelihood = student_t.logpdf(y_obs, df=nu, loc=h_val_t_plus_dt, scale=s_scale)\n\n        # Combine log weights\n        log_weights_unnorm = log_dyn_correction + log_obs_likelihood\n\n        # --- Step 5: Normalize weights and compute ESS ---\n        \n        # Use logsumexp for robust normalization\n        log_sum_weights = logsumexp(log_weights_unnorm)\n        log_normalized_weights = log_weights_unnorm - log_sum_weights\n        normalized_weights = np.exp(log_normalized_weights)\n        \n        # Compute Effective Sample Size (ESS)\n        ess = 1.0 / np.sum(normalized_weights**2)\n        results_ess.append(ess)\n\n    # Print the final results in the required format\n    print(f\"[{','.join(map(str, results_ess))}]\")\n\nsolve()\n\n```", "id": "3409822"}]}