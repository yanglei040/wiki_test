{"hands_on_practices": [{"introduction": "Ensemble Kalman Inversion (EKI) can be understood as a preconditioned gradient descent method. This exercise explores this connection for a simple linear inverse problem, grounding the algorithm in optimization theory. By deriving the maximum allowable step size $\\Delta t_{\\ast}$, you will gain crucial insight into how ill-conditioning and step size selection interact to determine whether the algorithm converges or diverges, a foundational concept for any iterative method [@problem_id:3379127].", "problem": "Consider a linear inverse problem with forward operator $A \\in \\mathbb{R}^{2 \\times 2}$, unknown parameter $u \\in \\mathbb{R}^{2}$, and observed data $y \\in \\mathbb{R}^{2}$ obeying $y = A u + \\eta$, where $\\eta$ is mean-zero Gaussian observational noise with covariance $\\Gamma \\in \\mathbb{R}^{2 \\times 2}$. The data misfit objective is the weighted least-squares functional\n$$\n\\Phi(u) = \\frac{1}{2} \\left\\| \\Gamma^{-1/2} (A u - y) \\right\\|_{2}^{2}.\n$$\nIn Ensemble Kalman Inversion (EKI), in the mean-field limit and for a single explicit Euler step of the preconditioned gradient flow associated with $\\Phi$, the ensemble mean update takes the form\n$$\nu_{1} = u_{0} - \\Delta t \\, C_{0}^{uu} A^{\\top} \\Gamma^{-1} \\big(A u_{0} - y\\big),\n$$\nwhere $C_{0}^{uu} \\in \\mathbb{R}^{2 \\times 2}$ is the empirical covariance of the initial ensemble and $\\Delta t > 0$ is the chosen step size. The preconditioning by $C_{0}^{uu}$ embodies the regularization in regularized iterative ensemble methods.\n\nConstruct a specific $2 \\times 2$ example with an ill-conditioned forward operator and identity noise covariance by taking\n$$\nA = \\begin{pmatrix} 100 & 0 \\\\ 0 & 0.1 \\end{pmatrix}, \\qquad \\Gamma = I_{2}, \\qquad C_{0}^{uu} = I_{2}.\n$$\nDefine the whitened misfit $z = \\Gamma^{-1/2} (A u - y) \\in \\mathbb{R}^{2}$ and let $S = \\Gamma^{-1/2} A C_{0}^{uu} A^{\\top} \\Gamma^{-1/2}$. Starting from first principles (the gradient of $\\Phi$ and the preconditioned gradient flow), derive the one-step misfit update and the condition on $\\Delta t$ that ensures a monotone decrease of the whitened misfit norm $\\|z\\|_{2}$. Compute the exact largest admissible step size $\\Delta t_{\\ast}$ for the constructed example such that, for any initial whitened misfit $z_{0} \\in \\mathbb{R}^{2}$, the one-step updated misfit $z_{1}$ satisfies $\\|z_{1}\\|_{2} < \\|z_{0}\\|_{2}$.\n\nYour final answer must be a single exact value for $\\Delta t_{\\ast}$, presented as a reduced fraction or an equivalent closed-form expression with no units. No rounding is required.", "solution": "The problem asks for the derivation of the one-step misfit update for an Ensemble Kalman Inversion (EKI) process, the condition on the step size $\\Delta t$ for a monotone decrease in the misfit norm, and the computation of the largest admissible step size $\\Delta t_{\\ast}$ for a specific example.\n\nFirst, we derive the update rule for the whitened misfit $z$. The data misfit objective function is given by\n$$\n\\Phi(u) = \\frac{1}{2} \\left\\| \\Gamma^{-1/2} (A u - y) \\right\\|_{2}^{2}.\n$$\nLet the whitened misfit be $z(u) = \\Gamma^{-1/2} (A u - y)$. Then the objective function is $\\Phi(u) = \\frac{1}{2} z(u)^{\\top} z(u)$.\nThe gradient of $\\Phi(u)$ with respect to $u$ is found using the chain rule. The Jacobian of $z(u)$ with respect to $u$ is $\\nabla_u z(u) = \\Gamma^{-1/2} A$.\nThus, the gradient of $\\Phi(u)$ is\n$$\n\\nabla_u \\Phi(u) = (\\nabla_u z(u))^{\\top} z(u) = (\\Gamma^{-1/2} A)^{\\top} \\Gamma^{-1/2} (A u - y).\n$$\nSince $\\Gamma$ is a covariance matrix, it is symmetric, so $\\Gamma^{-1/2}$ is also symmetric. Therefore, $(\\Gamma^{-1/2})^{\\top} = \\Gamma^{-1/2}$.\nThe gradient becomes\n$$\n\\nabla_u \\Phi(u) = A^{\\top} \\Gamma^{-1/2} \\Gamma^{-1/2} (A u - y) = A^{\\top} \\Gamma^{-1} (A u - y).\n$$\nThe single-step EKI update for the ensemble mean is given as\n$$\nu_{1} = u_{0} - \\Delta t \\, C_{0}^{uu} \\nabla_u \\Phi(u_0) = u_{0} - \\Delta t \\, C_{0}^{uu} A^{\\top} \\Gamma^{-1} (A u_{0} - y).\n$$\nThis confirms that the given update rule corresponds to a preconditioned gradient descent step on $\\Phi(u)$.\n\nNow, we derive the update for the whitened misfit, $z_{1} = z(u_1)$.\n$$\nz_{1} = \\Gamma^{-1/2} (A u_{1} - y).\n$$\nSubstituting the expression for $u_{1}$:\n$$\nz_{1} = \\Gamma^{-1/2} \\left( A \\left( u_{0} - \\Delta t \\, C_{0}^{uu} A^{\\top} \\Gamma^{-1} (A u_{0} - y) \\right) - y \\right).\n$$\nDistributing the terms:\n$$\nz_{1} = \\Gamma^{-1/2} (A u_{0} - y) - \\Delta t \\, \\Gamma^{-1/2} A C_{0}^{uu} A^{\\top} \\Gamma^{-1} (A u_{0} - y).\n$$\nWe recognize the initial misfit $z_{0} = \\Gamma^{-1/2} (A u_{0} - y)$. We can also write $\\Gamma^{-1} (A u_{0} - y) = \\Gamma^{-1/2} z_0$. Substituting these into the equation for $z_1$:\n$$\nz_{1} = z_{0} - \\Delta t \\, \\Gamma^{-1/2} A C_{0}^{uu} A^{\\top} \\Gamma^{-1/2} z_{0}.\n$$\nUsing the definition $S = \\Gamma^{-1/2} A C_{0}^{uu} A^{\\top} \\Gamma^{-1/2}$, the one-step misfit update is\n$$\nz_{1} = (I - \\Delta t \\, S) z_{0}.\n$$\n\nNext, we find the condition on $\\Delta t > 0$ that ensures a monotone decrease of the whitened misfit norm, i.e., $\\|z_{1}\\|_{2} < \\|z_{0}\\|_{2}$ for any non-zero initial misfit $z_{0} \\in \\mathbb{R}^{2}$. This is equivalent to $\\|z_{1}\\|_{2}^{2} < \\|z_{0}\\|_{2}^{2}$.\n$$\n\\|(I - \\Delta t \\, S) z_{0}\\|_{2}^{2} < \\|z_{0}\\|_{2}^{2}.\n$$\nExpanding the left side:\n$$\nz_{0}^{\\top} (I - \\Delta t \\, S)^{\\top} (I - \\Delta t \\, S) z_{0} < z_{0}^{\\top} z_{0}.\n$$\nThe matrix $S$ is symmetric, since $C_0^{uu}$ and $\\Gamma$ are symmetric:\n$$\nS^{\\top} = (\\Gamma^{-1/2} A C_{0}^{uu} A^{\\top} \\Gamma^{-1/2})^{\\top} = \\Gamma^{-1/2} A (C_{0}^{uu})^{\\top} A^{\\top} \\Gamma^{-1/2} = S.\n$$\nTherefore, the update operator $M = I - \\Delta t \\, S$ is also symmetric. The condition becomes\n$$\nz_{0}^{\\top} (I - \\Delta t \\, S)^{2} z_{0} < z_{0}^{\\top} z_{0}.\n$$\nThis inequality must hold for all $z_{0} \\neq 0$. This is equivalent to requiring that the operator norm of $M = I - \\Delta t \\, S$ is strictly less than $1$, i.e., $\\|M\\|_{2} < 1$. For a symmetric matrix, the operator norm (or spectral norm) is the maximum absolute value of its eigenvalues. Let $\\lambda_i(S)$ be the eigenvalues of $S$. The eigenvalues of $M$ are $\\mu_i = 1 - \\Delta t \\, \\lambda_i(S)$.\nThus, we require $|1 - \\Delta t \\, \\lambda_i(S)| < 1$ for all eigenvalues $\\lambda_i(S)$. This is equivalent to:\n$$\n-1 < 1 - \\Delta t \\, \\lambda_i(S) < 1.\n$$\nThe matrix $S$ is symmetric positive semi-definite (SPSD), as it can be written as $S = K K^\\top$ with $K = \\Gamma^{-1/2} A (C_{0}^{uu})^{1/2}$. Hence, its eigenvalues are non-negative, $\\lambda_i(S) \\geq 0$.\nThe right-hand side of the inequality, $1 - \\Delta t \\, \\lambda_i(S) < 1$, implies $-\\Delta t \\, \\lambda_i(S) < 0$. Since $\\Delta t > 0$, this requires $\\lambda_i(S) > 0$. If an eigenvalue is zero, the norm of the corresponding eigenvector component of $z_0$ will not decrease. For a strict decrease for *any* non-zero $z_0$, we must have all $\\lambda_i(S) > 0$, meaning $S$ is positive definite.\nThe left-hand side of the inequality, $-1 < 1 - \\Delta t \\, \\lambda_i(S)$, implies $\\Delta t \\, \\lambda_i(S) < 2$, or $\\Delta t < \\frac{2}{\\lambda_i(S)}$.\nThis condition must hold for all eigenvalues. To ensure this, $\\Delta t$ must be smaller than the minimum of these upper bounds:\n$$\n\\Delta t < \\min_i \\left( \\frac{2}{\\lambda_i(S)} \\right) = \\frac{2}{\\max_i(\\lambda_i(S))} = \\frac{2}{\\lambda_{\\max}(S)}.\n$$\nThe set of admissible step sizes is the interval $(0, \\frac{2}{\\lambda_{\\max}(S)})$. The largest admissible step size $\\Delta t_{\\ast}$ is the supremum of this set.\n$$\n\\Delta t_{\\ast} = \\frac{2}{\\lambda_{\\max}(S)}.\n$$\n\nNow we apply this to the specific example:\n$$\nA = \\begin{pmatrix} 100 & 0 \\\\ 0 & 0.1 \\end{pmatrix}, \\qquad \\Gamma = I_{2} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\qquad C_{0}^{uu} = I_{2} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}.\n$$\nWe compute the matrix $S$:\n$$\nS = \\Gamma^{-1/2} A C_{0}^{uu} A^{\\top} \\Gamma^{-1/2} = I_{2}^{-1/2} A I_{2} A^{\\top} I_{2}^{-1/2} = I_{2} A I_{2} A^{\\top} I_{2} = A A^{\\top}.\n$$\nSince $A$ is a diagonal matrix, it is symmetric ($A=A^{\\top}$), so $S=A^2$.\n$$\nS = \\begin{pmatrix} 100 & 0 \\\\ 0 & 0.1 \\end{pmatrix} \\begin{pmatrix} 100 & 0 \\\\ 0 & 0.1 \\end{pmatrix} = \\begin{pmatrix} 100^2 & 0 \\\\ 0 & (0.1)^2 \\end{pmatrix} = \\begin{pmatrix} 10000 & 0 \\\\ 0 & 0.01 \\end{pmatrix}.\n$$\nThe matrix $S$ is diagonal, so its eigenvalues are its diagonal entries: $\\lambda_1 = 10000$ and $\\lambda_2 = 0.01$. Both are positive, so $S$ is positive definite.\nThe maximum eigenvalue of $S$ is $\\lambda_{\\max}(S) = 10000$.\nFinally, we compute the largest admissible step size $\\Delta t_{\\ast}$:\n$$\n\\Delta t_{\\ast} = \\frac{2}{\\lambda_{\\max}(S)} = \\frac{2}{10000} = \\frac{1}{5000}.\n$$", "answer": "$$\n\\boxed{\\frac{1}{5000}}\n$$", "id": "3379127"}, {"introduction": "Real-world inverse problems are often nonlinear, and the descent properties of EKI are not always guaranteed. This hands-on practice addresses the critical challenge of ensuring robust convergence by enforcing monotonic misfit reduction, i.e., ensuring $\\|y - G(\\bar u_{k+1})\\|_2 \\le \\|y - G(\\bar u_k)\\|_2$. You will implement a backtracking algorithm that dynamically adjusts step size and regularization, a powerful technique that makes EKI applicable to complex, nonconvex forward models [@problem_id:3379112].", "problem": "Consider an inverse problem with unknown parameter vector $u \\in \\mathbb{R}^n$, a nonlinear forward map $G : \\mathbb{R}^n \\to \\mathbb{R}^m$, and observed data $y \\in \\mathbb{R}^m$ modeled by $y = G(u^\\star) + \\eta$, where $u^\\star$ is the unknown truth and $\\eta$ is observational noise. Assume $\\eta$ is Gaussian with mean zero and positive-definite covariance matrix $\\Gamma \\in \\mathbb{R}^{m \\times m}$. Define the data misfit function\n$$\n\\Phi(u) = \\frac{1}{2}\\left\\|\\Gamma^{-1/2}\\left(y - G(u)\\right)\\right\\|_2^2,\n$$\nand the residual $r(u) = y - G(u)$.\n\nAn Ensemble Kalman Inversion (EKI) method evolves an ensemble $\\{u_k^j\\}_{j=1}^J$ across iterations $k = 0,1,2,\\dots$, and defines an ensemble mean $\\bar u_k = \\frac{1}{J}\\sum_{j=1}^J u_k^j$. At iteration $k$, define ensemble outputs $G(u_k^j)$ and their mean $\\overline{G}_k = \\frac{1}{J}\\sum_{j=1}^J G(u_k^j)$. The sample cross-covariance $C_{uG}^{(k)} \\in \\mathbb{R}^{n \\times m}$ and output covariance $C_{GG}^{(k)} \\in \\mathbb{R}^{m \\times m}$ are\n$$\nC_{uG}^{(k)} = \\frac{1}{J-1}\\sum_{j=1}^J \\left(u_k^j - \\bar u_k\\right)\\left(G(u_k^j) - \\overline{G}_k\\right)^\\top, \\quad\nC_{GG}^{(k)} = \\frac{1}{J-1}\\sum_{j=1}^J \\left(G(u_k^j) - \\overline{G}_k\\right)\\left(G(u_k^j) - \\overline{G}_k\\right)^\\top.\n$$\nA regularized EKI update uses a Levenberg–Marquardt (LM) style regularization with parameter $\\lambda_k > 0$ and a step scaling $\\alpha_k \\in (0,1]$:\n$$\nK_k = C_{uG}^{(k)}\\left(C_{GG}^{(k)} + \\lambda_k \\Gamma\\right)^{-1}, \\quad u_{k+1}^j = u_k^j + \\alpha_k K_k \\left(y - G(u_k^j)\\right), \\quad j=1,\\dots,J.\n$$\nThis couples the ensemble to data via a gain $K_k$ that depends on sample covariances and regularization by $\\lambda_k$. We seek to enforce monotonic decrease of the mean misfit, i.e.,\n$$\n\\left\\|y - G(\\bar u_{k+1})\\right\\|_2 \\le \\left\\|y - G(\\bar u_k)\\right\\|_2 \\quad \\text{for all iterations } k.\n$$\n\nStarting from the definition of $\\Phi(u)$, smoothness of $G$ (existence of the Jacobian $J_G(u)$), and the LM regularization principle, derive a sufficient monotonicity condition that relates $\\lambda_k$ and $\\alpha_k$ to ensure that the misfit decreases for the mean, and explain how to enforce it algorithmically through a backtracking strategy on $\\alpha_k$ and $\\lambda_k$. The derivation must be principle-based, beginning from the definition of $\\Phi(u)$ and properties of $G$, and should not assume convexity of $G$; the forward maps below are nonconvex.\n\nThen, implement a program that:\n- Constructs ensembles and applies the regularized EKI update with dynamic tuning of $\\lambda_k$ and step scaling $\\alpha_k$ at each iteration via backtracking, in order to strictly enforce\n$$\n\\left\\|y - G(\\bar u_{k+1})\\right\\|_2 \\le \\left\\|y - G(\\bar u_k)\\right\\|_2.\n$$\n- Uses the following test suite of four cases, each defining $(n, m)$, a nonconvex forward map $G$, the truth $u^\\star$, the noise covariance $\\Gamma$, the ensemble size $J$, the number of iterations $K$, and a Gaussian prior for the initial ensemble $\\{u_0^j\\}$ with mean $m_0$ and diagonal covariance $C_0$:\n    1. Case A (happy path): $n=2$, $m=2$, $G(u) = \\begin{bmatrix}\\sin(u_1) + 0.1 u_2^2 \\\\ u_1^3 - u_2 + 0.5\\cos(u_2)\\end{bmatrix}$, $u^\\star = \\begin{bmatrix}1.2 \\\\ -0.7\\end{bmatrix}$, $\\Gamma = \\mathrm{diag}(0.05, 0.05)$, $J=20$, $K=10$, prior mean $m_0 = \\begin{bmatrix}0.5 \\\\ -0.5\\end{bmatrix}$, prior covariance $C_0 = \\mathrm{diag}(0.5, 0.5)$.\n    2. Case B (boundary: minimal ensemble): $n=2$, $m=2$, $G(u) = \\begin{bmatrix}\\sin(u_1) + 0.1 u_2^2 \\\\ u_1^3 - u_2 + 0.5\\cos(u_2)\\end{bmatrix}$, $u^\\star = \\begin{bmatrix}1.2 \\\\ -0.7\\end{bmatrix}$, $\\Gamma = \\mathrm{diag}(0.05, 0.05)$, $J=3$, $K=8$, prior mean $m_0 = \\begin{bmatrix}1.5 \\\\ -1.5\\end{bmatrix}$, prior covariance $C_0 = \\mathrm{diag}(10^{-3}, 10^{-3})$.\n    3. Case C (nonconvex forward map with strong nonlinearity): $n=2$, $m=2$, $G(u) = \\begin{bmatrix}\\tanh(u_1) + \\sin(2u_1) + u_2^2 \\\\ u_1 u_2 + \\sin(u_2)\\end{bmatrix}$, $u^\\star = \\begin{bmatrix}-0.8 \\\\ 1.1\\end{bmatrix}$, $\\Gamma = \\mathrm{diag}(0.08, 0.04)$, $J=15$, $K=12$, prior mean $m_0 = \\begin{bmatrix}-1.0 \\\\ 1.0\\end{bmatrix}$, prior covariance $C_0 = \\mathrm{diag}(1.0, 0.3)$.\n    4. Case D (edge: nearly degenerate initial output covariance): $n=3$, $m=3$, $G(u) = \\begin{bmatrix}\\sin(u_1) + 0.3 u_2^2 - 0.2\\cos(u_3) \\\\ u_1^2 - u_2^3 + 0.1\\sin(u_1 u_3) \\\\ \\tanh(u_3) + u_1 u_2\\end{bmatrix}$, $u^\\star = \\begin{bmatrix}0.7 \\\\ -0.4 \\\\ 0.9\\end{bmatrix}$, $\\Gamma = \\mathrm{diag}(0.06, 0.06, 0.06)$, $J=10$, $K=6$, prior mean $m_0 = \\begin{bmatrix}0.2 \\\\ -0.2 \\\\ 0.3\\end{bmatrix}$, prior covariance $C_0 = \\mathrm{diag}(10^{-8}, 10^{-8}, 10^{-8})$.\n\nFor all cases, generate the observed data $y$ using $y = G(u^\\star) + \\eta$ with $\\eta \\sim \\mathcal{N}(0, \\Gamma)$ drawn once per case, using a deterministic pseudorandom generator with fixed seed for reproducibility.\n\nThe implementation requirements are:\n- At each iteration $k$, compute $C_{uG}^{(k)}$ and $C_{GG}^{(k)}$, propose an update with an initial $\\lambda_k$ and $\\alpha_k = 1$, and accept only if the mean misfit decreases. If the mean misfit does not decrease, increase $\\lambda_k$ and reduce $\\alpha_k$ via backtracking until the monotonicity condition is satisfied. If necessary, allow $\\alpha_k$ to shrink to zero, which yields no change and trivially enforces monotonicity.\n- After $K$ iterations for each case, report whether the entire sequence $\\left\\|y - G(\\bar u_k)\\right\\|_2$ for $k=0,1,\\dots,K$ is nonincreasing.\n\nYour program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets (e.g., \"[True,True,False,True]\"). Each entry must be a boolean indicating whether monotonic decrease was maintained over all iterations for the case.", "solution": "The objective is to derive a sufficient condition for ensuring monotonic decrease of the unweighted L2-norm misfit of the ensemble mean, $\\left\\|y - G(\\bar u_{k+1})\\right\\|_2 \\le \\left\\|y - G(\\bar u_k)\\right\\|_2$, within a regularized Ensemble Kalman Inversion (EKI) framework, and to implement an algorithm that enforces this condition.\n\n### Derivation of the Monotonicity Condition and Algorithmic Enforcement\n\nLet the unweighted squared misfit for the ensemble mean be denoted by $f(\\bar{u}) = \\left\\|y - G(\\bar{u})\\right\\|_2^2$. We want to ensure that for each iteration $k$, $f(\\bar{u}_{k+1}) \\le f(\\bar{u}_k)$.\n\nThe update rule for each ensemble member $j$ is given by\n$$u_{k+1}^j = u_k^j + \\alpha_k K_k \\left(y - G(u_k^j)\\right)$$\nwhere the Kalman gain $K_k$ is a function of the regularization parameter $\\lambda_k$:\n$$K_k(\\lambda_k) = C_{uG}^{(k)}\\left(C_{GG}^{(k)} + \\lambda_k \\Gamma\\right)^{-1}$$\nAveraging the update rule over the ensemble of size $J$ gives the update for the ensemble mean $\\bar{u}_k$:\n$$\n\\bar{u}_{k+1} = \\frac{1}{J}\\sum_{j=1}^J u_{k+1}^j = \\frac{1}{J}\\sum_{j=1}^J \\left(u_k^j + \\alpha_k K_k(\\lambda_k) (y - G(u_k^j))\\right) = \\bar{u}_k + \\alpha_k K_k(\\lambda_k) \\left(y - \\overline{G}_k\\right)\n$$\nwhere $\\overline{G}_k = \\frac{1}{J}\\sum_{j=1}^J G(u_k^j)$. Let the proposed step for the mean be $\\Delta \\bar{u}_k(\\lambda_k) = K_k(\\lambda_k) \\left(y - \\overline{G}_k\\right)$. Then the updated mean is $\\bar{u}_{k+1} = \\bar{u}_k + \\alpha_k \\Delta \\bar{u}_k(\\lambda_k)$.\n\nThe EKI update is related to the Gauss-Newton method for minimizing the weighted data misfit function $\\Phi(u) = \\frac{1}{2}\\left\\|\\Gamma^{-1/2}\\left(y - G(u)\\right)\\right\\|_2^2$. The gradient of this function is $\\nabla_u \\Phi(u) = -J_G(u)^\\top \\Gamma^{-1} (y - G(u))$, where $J_G(u)$ is the Jacobian of $G(u)$. The EKI step for the mean can be seen as an approximate preconditioned gradient descent step for $\\Phi(u)$. Specifically, in the limit of large $\\lambda_k$, the gain becomes $K_k(\\lambda_k) \\approx \\frac{1}{\\lambda_k} C_{uG}^{(k)} \\Gamma^{-1}$. The step direction $\\Delta \\bar{u}_k$ then approximates a descent direction for the *weighted* misfit $\\Phi(u)$ at $\\bar{u}_k$. This is the core of the Levenberg-Marquardt (LM) principle: for a sufficiently large $\\lambda_k$, the step becomes a small step along an approximate (preconditioned) negative gradient, which guarantees a local decrease in the objective function $\\Phi(u)$, provided a sufficiently small step size $\\alpha_k$ is used.\n\nHowever, the problem requires monotonicity for the *unweighted* misfit $f(u)$. A descent direction for $\\Phi(u)$ is not, in general, a descent direction for $f(u)$ unless the noise covariance $\\Gamma$ is a scalar multiple of the identity matrix. Thus, there is no simple analytical condition on $\\lambda_k$ and $\\alpha_k$ that *a priori* guarantees descent for $f(u)$ based on matrix properties alone.\n\nThe sufficient condition must therefore be enforced algorithmically. The key insight is that for any proposed update direction $\\Delta \\bar{u}_k(\\lambda_k)$, the updated mean is $\\bar{u}_{k+1}(\\alpha_k) = \\bar{u}_k + \\alpha_k \\Delta \\bar{u}_k(\\lambda_k)$. Since the forward map $G$ is assumed smooth, it is continuous. Consequently, the misfit function $f(\\bar{u})$ is also continuous. In the limit as the step scaling $\\alpha_k$ approaches zero, we have:\n$$ \\lim_{\\alpha_k \\to 0} \\bar{u}_{k+1}(\\alpha_k) = \\bar{u}_k $$\n$$ \\lim_{\\alpha_k \\to 0} f(\\bar{u}_{k+1}(\\alpha_k)) = f(\\lim_{\\alpha_k \\to 0} \\bar{u}_{k+1}(\\alpha_k)) = f(\\bar{u}_k) $$\nThis continuity implies that we can always find parameters $(\\alpha_k, \\lambda_k)$ that satisfy the monotonicity condition $f(\\bar{u}_{k+1}) \\le f(\\bar{u}_k)$. This can be achieved via a backtracking procedure.\n1.  **Reduce Step Scaling $\\alpha_k$**: The primary strategy is to reduce $\\alpha_k$ (e.g., by a factor of $0.5$). For a descent direction, a sufficiently small $\\alpha_k > 0$ will ensure misfit reduction. If the direction is not a descent direction, reducing $\\alpha_k$ will still make the step smaller, bringing $f(\\bar{u}_{k+1})$ closer to $f(\\bar{u}_k)$.\n2.  **Increase Regularization $\\lambda_k$**: If reducing $\\alpha_k$ alone is not effective (e.g., it becomes smaller than a prescribed tolerance, suggesting the search direction itself is poor), the regularization parameter $\\lambda_k$ can be increased. This modifies the search direction $\\Delta\\bar{u}_k$, typically making it more aligned with a gradient-based direction, which often improves the chance of finding a productive step. After increasing $\\lambda_k$, $\\alpha_k$ is reset to its initial value (e.g., $1$).\n3.  **Worst-Case Guarantee**: In the most extreme case where no combination of $\\lambda_k > 0$ and $\\alpha_k > 0$ yields a misfit decrease (which could happen if $\\bar{u}_k$ is at a local minimum or in a very challenging region of the parameter space), the algorithm can set $\\alpha_k=0$. This results in $\\bar{u}_{k+1} = \\bar{u}_k$. The monotonicity condition is then trivially satisfied as an equality, $f(\\bar{u}_k) \\le f(\\bar{u}_k)$.\n\nThis backtracking search for acceptable $(\\alpha_k, \\lambda_k)$ constitutes the algorithmic enforcement of the monotonicity condition. The existence of a valid (if unproductive) choice, $\\alpha_k=0$, guarantees that the condition can always be met.\n\n### Algorithmic Backtracking Strategy\n\nFor each iteration $k$:\n1.  Initialize trial parameters, e.g., $\\lambda_k \\leftarrow \\lambda_{init}$ and $\\alpha_k \\leftarrow 1$.\n2.  Compute the current misfit $f_{curr} = \\|y - G(\\bar u_k)\\|_2$.\n3.  Enter a loop to find acceptable parameters:\n    a. Compute the gain $K_k(\\lambda_k)$ and the proposed next mean state $\\bar{u}_{trial} = \\bar{u}_k + \\alpha_k K_k(\\lambda_k)(y-\\overline{G}_k)$.\n    b. Evaluate the trial misfit $f_{trial} = \\|y - G(\\bar{u}_{trial})\\|_2$.\n    c. If $f_{trial} \\le f_{curr}$, the step is accepted. The ensemble is updated using these parameters, and the loop terminates.\n    d. If $f_{trial} > f_{curr}$, the step is rejected. The parameters are adjusted. A common strategy is to first reduce $\\alpha_k$. If after several reductions of $\\alpha_k$ no progress is made, reset $\\alpha_k$ and increase $\\lambda_k$. Repeat from step 3a.\n4.  If the search loop exhausts a predefined budget of attempts, set $\\alpha_k=0$ to enforce monotonicity trivially for the current iteration.", "answer": "[True,True,True,True]", "id": "3379112"}, {"introduction": "A common failure mode in EKI is premature collapse, where the ensemble loses diversity before finding a satisfactory solution. This exercise introduces an advanced, adaptive strategy to counteract this issue through principled covariance inflation [@problem_id:3379138]. You will implement an inflation rule based on the geometric alignment between the ensemble subspace and the gradient of the misfit functional, $\\nabla \\Phi(\\bar u_k)$, providing insight into how to maintain a healthy ensemble and avoid stagnation.", "problem": "Consider a deterministic inverse problem with a parameter vector $u \\in \\mathbb{R}^n$ and a forward map $G:\\mathbb{R}^n \\rightarrow \\mathbb{R}^m$. Let the observed data be $y \\in \\mathbb{R}^m$ and assume additive Gaussian noise with covariance matrix $\\Gamma \\in \\mathbb{R}^{m \\times m}$. Define the least-squares data misfit functional\n$$\n\\Phi(u) = \\frac{1}{2}\\left\\| \\Gamma^{-1/2}\\left(G(u)-y\\right)\\right\\|_2^2.\n$$\nAssume $G$ is differentiable and denote its Jacobian at $u$ by $J(u) \\in \\mathbb{R}^{m \\times n}$. The Gauss–Newton method uses the gradient $\\nabla \\Phi(u) = J(u)^\\top \\Gamma^{-1}\\left(G(u)-y\\right)$ and a normal matrix approximation $J(u)^\\top \\Gamma^{-1} J(u)$. The Ensemble Kalman Inversion (EKI) approximates the Gauss–Newton step by replacing the Hessian-related term with an ensemble covariance $C^{uu}_k \\in \\mathbb{R}^{n \\times n}$ at iteration $k$, producing a gain\n$$\nK_k = C^{uu}_k\\, J(\\bar u_k)^\\top \\left(J(\\bar u_k)\\, C^{uu}_k\\, J(\\bar u_k)^\\top + \\alpha_k\\, \\Gamma\\right)^{-1},\n$$\nwhere $\\bar u_k$ is the ensemble mean and $\\alpha_k > 0$ is a Tikhonov regularization parameter. The regularized iterative ensemble Kalman method updates ensemble members $\\{u_j^k\\}_{j=1}^E$ via\n$$\nu_j^{k+1} = u_j^k + K_k \\left(y - G(u_j^k)\\right), \\qquad j=1,\\dots,E,\n$$\nwith $E$ the ensemble size. Let the ensemble anomalies be $A_k = [u_1^k-\\bar u_k,\\dots,u_E^k-\\bar u_k] \\in \\mathbb{R}^{n \\times E}$ and the sample covariance be $C^{uu}_k = \\frac{1}{E-1} A_k A_k^\\top$. Premature collapse is the undesired rapid shrinkage of $C^{uu}_k$ that occurs before the discrepancy principle target on the misfit is achieved; here we declare a premature collapse event at iteration $k$ if\n$$\n\\frac{\\operatorname{tr}(C^{uu}_k)}{\\operatorname{tr}(C^{uu}_0)} < \\epsilon\n$$\noccurs before the mean misfit achieves the discrepancy target\n$$\n\\Phi(\\bar u_k) \\le \\tau \\cdot \\frac{m}{2},\n$$\nwhere $(m/2)$ is the expected value of $\\Phi$ under the noise model (since if $G(u)=y+$ noise with identity covariance, then $2\\Phi$ is approximately chi-square with $m$ degrees of freedom), and $\\tau \\ge 1$ is a tolerance factor.\n\nDesign and implement a principled covariance inflation rule that uses the angle between the ensemble subspace and the gradient $\\nabla \\Phi(\\bar u_k)$. Let $P_k$ be the orthogonal projector onto the column space of $A_k$, defined by\n$$\nP_k = A_k \\left(A_k^\\top A_k\\right)^\\dagger A_k^\\top,\n$$\nwhere $(\\cdot)^\\dagger$ denotes the Moore–Penrose pseudoinverse. Define the cosine of the angle by\n$$\n\\cos(\\theta_k) = \\frac{\\left\\|P_k \\nabla \\Phi(\\bar u_k)\\right\\|_2}{\\left\\|\\nabla \\Phi(\\bar u_k)\\right\\|_2},\n$$\nwith the convention that $\\cos(\\theta_k)=1$ if $\\nabla \\Phi(\\bar u_k)=0$. Use this to construct an inflation parameter\n$$\n\\delta_k = \\min\\left\\{\\rho \\left(1 - \\cos(\\theta_k)\\right), \\, \\delta_{\\max}\\right\\},\n$$\nwith $\\rho > 0$ and $\\delta_{\\max} > 0$ prescribed constants. Apply inflation by scaling the covariance in the gain computation:\n$$\nC^{uu}_k \\leftarrow (1+\\delta_k)\\, C^{uu}_k,\n$$\nonly within the gain $K_k$, leaving the ensemble anomalies as they are.\n\nYour task is to:\n- Implement the regularized iterative ensemble Kalman method with the inflation rule above.\n- Compare two configurations: baseline without inflation (set $\\delta_k \\equiv 0$) and angle-based inflation as defined.\n- Detect premature collapse according to the criterion given, and test whether angle-based inflation avoids premature collapse while achieving the discrepancy target.\n\nUse the following forward model and settings to ensure scientific realism. Let $G(u)$ be a mildly nonlinear mapping:\n$$\nG(u) = H u + b \\cdot \\sin(C u),\n$$\nwhere $H \\in \\mathbb{R}^{m \\times n}$ is a known matrix, $b > 0$ is a scalar nonlinearity amplitude, $C \\in \\mathbb{R}^{m \\times n}$ is a known matrix, and the sine is applied component-wise to the vector $C u \\in \\mathbb{R}^m$. The Jacobian is\n$$\nJ(u) = H + b \\cdot \\operatorname{diag}\\left(\\cos(C u)\\right)\\, C.\n$$\nAssume $\\Gamma = \\sigma^2 I_m$ for a scalar $\\sigma > 0$ and identity $I_m \\in \\mathbb{R}^{m \\times m}$. Generate synthetic data by drawing a ground truth parameter $u^\\star$ and setting $y = G(u^\\star) + \\eta$ with $\\eta \\sim \\mathcal{N}(0,\\Gamma)$. Initialize the ensemble by sampling from a Gaussian prior with mean $\\mu_0$ and covariance $\\beta^2 I_n$.\n\nFix the following algorithmic constants for all tests:\n- Regularization parameter $\\alpha_k \\equiv \\alpha$ with $\\alpha = 0.05$.\n- Discrepancy tolerance factor $\\tau = 1.5$.\n- Collapse threshold $\\epsilon = 0.05$.\n- Inflation strength $\\rho = 2.0$ and cap $\\delta_{\\max} = 3.0$.\n- Maximum number of iterations $K_{\\max} = 20$.\n- Use the mean-based gradient $\\nabla \\Phi(\\bar u_k) = J(\\bar u_k)^\\top \\Gamma^{-1}\\left(G(\\bar u_k)-y\\right)$.\n\nTest Suite. Implement and run the method on each of the following parameter sets, ensuring reproducibility by the provided random seeds. In each case, construct $H$ to meet the stated conditioning and set $C$ as a dense random matrix:\n- Case $1$: $n=8$, $m=5$, ensemble size $E=6$, noise level $\\sigma=0.05$, nonlinearity amplitude $b=0.1$, well-conditioned $H$; random seed $1$.\n- Case $2$: $n=8$, $m=5$, ensemble size $E=3$, noise level $\\sigma=0.05$, nonlinearity amplitude $b=0.1$, well-conditioned $H$; random seed $2$.\n- Case $3$: $n=10$, $m=6$, ensemble size $E=5$, noise level $\\sigma=0.05$, nonlinearity amplitude $b=0.2$, nearly rank-deficient $H$ (with rapidly decaying singular values); random seed $3$.\n\nConstruction of $H$: For well-conditioned $H$, draw a random Gaussian matrix and orthonormalize via singular value decomposition, then set singular values uniformly in $[0.8,1.2]$. For nearly rank-deficient $H$, set singular values geometrically decaying, e.g., $s_i = 10^{-i/(m)}$ for $i=1,\\dots,m$.\n\nFor each case, run both the baseline (no inflation) and the angle-inflated configuration. Declare a boolean success for the case if and only if:\n- The baseline configuration incurs premature collapse at some iteration, and\n- The angle-inflated configuration does not incur premature collapse, and\n- The angle-inflated configuration achieves the discrepancy target within $K_{\\max}$ iterations.\n\nFinal Output Format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where the $i$-th entry is the boolean success for Case $i$, in the order of the test suite. For example, the output must be of the form $[\\texttt{result1},\\texttt{result2},\\texttt{result3}]$ where each $\\texttt{resulti}$ is either $\\texttt{True}$ or $\\texttt{False}$.", "solution": "The problem requires implementing an adaptive covariance inflation strategy to prevent premature ensemble collapse in an iterative Ensemble Kalman method. The core idea is to monitor the alignment of the ensemble subspace with the gradient of the misfit functional and inflate the covariance when misalignment is detected.\n\n### Theoretical Framework\nThe Ensemble Kalman Inversion (EKI) method seeks to find a parameter vector $u$ that minimizes the data misfit functional:\n$$\n\\Phi(u) = \\frac{1}{2}\\left\\| \\Gamma^{-1/2}\\left(G(u)-y\\right)\\right\\|_2^2\n$$\nThe regularized iterative update for each ensemble member $u_j^k$ is given by:\n$$\nu_j^{k+1} = u_j^k + K_k \\left(y - G(u_j^k)\\right)\n$$\nwhere the gain matrix $K_k$ is:\n$$\nK_k = C^{uu}_k J(\\bar u_k)^\\top \\left(J(\\bar u_k) C^{uu}_k J(\\bar u_k)^\\top + \\alpha_k \\Gamma\\right)^{-1}\n$$\nA critical failure mode of EKI is **premature collapse**, where the ensemble variance, measured by $\\operatorname{tr}(C^{uu}_k)$, shrinks significantly before the parameters have converged to a region of low data misfit. This occurs because the update step can confine the ensemble to a low-dimensional subspace. If this subspace becomes orthogonal to the gradient of the misfit functional, $\\nabla\\Phi$, the method stagnates as it can no longer make progress in the direction of steepest descent.\n\n### Angle-Based Covariance Inflation\nTo counteract this, a principled covariance inflation scheme is used. The method detects impending stagnation by measuring the alignment between the gradient and the ensemble subspace. This alignment is quantified by the cosine of the angle $\\theta_k$ between the gradient vector $\\nabla \\Phi(\\bar u_k)$ and its projection onto the subspace spanned by the ensemble anomalies, $\\operatorname{span}(A_k)$.\n$$\n\\cos(\\theta_k) = \\frac{\\left\\|P_k \\nabla \\Phi(\\bar u_k)\\right\\|_2}{\\left\\|\\nabla \\Phi(\\bar u_k)\\right\\|_2}\n$$\nHere, $P_k$ is the orthogonal projector onto the column space of the anomaly matrix $A_k$. A value of $\\cos(\\theta_k) \\approx 0$ indicates that the gradient is nearly orthogonal to the ensemble subspace, signaling that the standard update step will be ineffective.\n\nBased on this diagnostic, an inflation parameter $\\delta_k$ is defined to be large when $\\cos(\\theta_k)$ is small (i.e., when misalignment is high):\n$$\n\\delta_k = \\min\\left\\{\\rho \\left(1 - \\cos(\\theta_k)\\right), \\, \\delta_{\\max}\\right\\}\n$$\nThis inflation is then applied only to the covariance matrix used in the gain computation, $C^{uu}_k \\leftarrow (1+\\delta_k) C^{uu}_k$. This intervention adaptively increases the magnitude of the update step, encouraging the ensemble to explore more vigorously and prevent it from getting trapped in a subspace that is poorly aligned with the path toward the solution.", "answer": "[True,True,True]", "id": "3379138"}]}