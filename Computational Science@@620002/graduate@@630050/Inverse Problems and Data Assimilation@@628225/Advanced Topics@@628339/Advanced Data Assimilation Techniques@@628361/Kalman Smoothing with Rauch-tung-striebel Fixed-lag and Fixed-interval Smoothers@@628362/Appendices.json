{"hands_on_practices": [{"introduction": "While the Rauch-Tung-Striebel algorithm presents a computationally efficient, recursive solution to the fixed-interval smoothing problem, it is fundamentally equivalent to solving a single, large-scale batch optimization problem. This exercise provides a crucial bridge between these two perspectives. By deriving the normal equations from the maximum a posteriori (MAP) cost function, you will uncover the underlying structure of this batch problem and see how the Markov property of the state-space model leads to a sparse, block-tridiagonal information matrix [@problem_id:3393989].", "problem": "Consider a linear, time-varying, discrete-time Gaussian state-space model posed over a fixed interval $k=0,1,\\dots,T$ with the prior, dynamics, and observations given by\n$$\nx_0 \\sim \\mathcal{N}(m_0,P_0), \\quad x_{k+1} = F_k x_k + w_k, \\quad w_k \\sim \\mathcal{N}(0,Q_k), \\quad y_k = H_k x_k + v_k, \\quad v_k \\sim \\mathcal{N}(0,R_k),\n$$\nwhere $x_k \\in \\mathbb{R}^n$, $y_k \\in \\mathbb{R}^p$, the matrices $P_0 \\in \\mathbb{R}^{n \\times n}$, $Q_k \\in \\mathbb{R}^{n \\times n}$, and $R_k \\in \\mathbb{R}^{p \\times p}$ are symmetric positive definite, and $F_k \\in \\mathbb{R}^{n \\times n}$ and $H_k \\in \\mathbb{R}^{p \\times n}$ are known. Fixed-interval smoothing over $k=0{:}T$ can be formulated as maximum a posteriori estimation by minimizing the strictly convex quadratic cost\n$$\nJ(x_{0:T}) \\;=\\; \\frac{1}{2}\\|x_0 - m_0\\|_{P_0^{-1}}^2 \\;+\\; \\sum_{k=0}^{T-1} \\frac{1}{2}\\|x_{k+1} - F_k x_k\\|_{Q_k^{-1}}^2 \\;+\\; \\sum_{k=0}^{T} \\frac{1}{2}\\|y_k - H_k x_k\\|_{R_k^{-1}}^2,\n$$\nwith the stacked decision vector $x_{0:T} \\equiv \\mathrm{col}(x_0,\\dots,x_T) \\in \\mathbb{R}^{n(T+1)}$ and the weighted norm $\\|a\\|_{M}^2 \\equiv a^{\\top} M a$. By taking first-order optimality conditions with respect to each $x_k$ and assembling terms, the normal equations can be written in the block tridiagonal form\n$$\n\\mathcal{H}\\,x_{0:T} \\;=\\; b,\n$$\nwhere $\\mathcal{H} \\in \\mathbb{R}^{n(T+1)\\times n(T+1)}$ is symmetric block tridiagonal with $n\\times n$ blocks, and $b \\in \\mathbb{R}^{n(T+1)}$ is the right-hand-side that collects prior and measurement information. Derive $\\mathcal{H}$ and $b$ from first principles using only the given probabilistic model and the definition of $J(x_{0:T})$, and identify the explicit expressions of the diagonal and off-diagonal blocks and the components of $b$.\n\nAfter you have completed this derivation, report only the closed-form analytic expression for the sub-diagonal block of $\\mathcal{H}$ coupling $x_{k}$ to $x_{k-1}$ for a generic interior index $k \\in \\{1,\\dots,T\\}$. Your final answer must be a single analytic expression. No rounding is required and no units are involved.", "solution": "The problem requires the derivation of the block tridiagonal normal equations for a fixed-interval smoothing problem and the identification of a specific block of the associated information matrix $\\mathcal{H}$. The problem is valid as it represents a standard maximum a posteriori (MAP) estimation formulation for a linear-Gaussian state-space model, which is a well-posed and fundamental problem in estimation theory.\n\nThe objective is to find the state trajectory $x_{0:T} = \\mathrm{col}(x_0, \\dots, x_T)$ that minimizes the cost function $J(x_{0:T})$. Since $J(x_{0:T})$ is a strictly convex quadratic function of $x_{0:T}$ (due to the positive definiteness of $P_0$, $Q_k$, and $R_k$), a unique minimum exists and can be found by setting the gradient of $J$ with respect to each component $x_k$ to zero. The full gradient is $\\nabla_{x_{0:T}} J = \\mathrm{col}(\\frac{\\partial J}{\\partial x_0}, \\dots, \\frac{\\partial J}{\\partial x_T})$.\n\nThe cost function is given by:\n$$\nJ(x_{0:T}) = \\frac{1}{2}\\|x_0 - m_0\\|_{P_0^{-1}}^2 + \\sum_{k=0}^{T-1} \\frac{1}{2}\\|x_{k+1} - F_k x_k\\|_{Q_k^{-1}}^2 + \\sum_{k=0}^{T} \\frac{1}{2}\\|y_k - H_k x_k\\|_{R_k^{-1}}^2\n$$\nExpanding the weighted norms, we have:\n$$\nJ(x_{0:T}) = \\frac{1}{2}(x_0 - m_0)^{\\top}P_0^{-1}(x_0 - m_0) + \\sum_{k=0}^{T-1} \\frac{1}{2}(x_{k+1} - F_k x_k)^{\\top}Q_k^{-1}(x_{k+1} - F_k x_k) + \\sum_{k=0}^{T} \\frac{1}{2}(y_k - H_k x_k)^{\\top}R_k^{-1}(y_k - H_k x_k)\n$$\nTo find the minimum, we take the partial derivative of $J$ with respect to each $x_k$ for $k = 0, \\dots, T$ and set it to zero. We utilize the matrix calculus identities $\\frac{\\partial}{\\partial z} (a-Az)^{\\top}B(a-Az) = -2A^{\\top}B(a-Az)$ for symmetric $B$.\n\nLet's first consider a generic interior time index $k \\in \\{1, \\dots, T-1\\}$. The state vector $x_k$ appears in three terms of the cost function $J$:\n1. The dynamics term from $k-1$ to $k$: $\\frac{1}{2}\\|x_k - F_{k-1} x_{k-1}\\|_{Q_{k-1}^{-1}}^2$\n2. The dynamics term from $k$ to $k+1$: $\\frac{1}{2}\\|x_{k+1} - F_k x_k\\|_{Q_k^{-1}}^2$\n3. The measurement term at time $k$: $\\frac{1}{2}\\|y_k - H_k x_k\\|_{R_k^{-1}}^2$\n\nThe partial derivative of $J$ with respect to $x_k$ is the sum of the derivatives of these three terms, as other terms in $J$ do not depend on $x_k$:\n$$\n\\frac{\\partial J}{\\partial x_k} = \\frac{\\partial}{\\partial x_k} \\left( \\frac{1}{2}(x_k - F_{k-1}x_{k-1})^{\\top}Q_{k-1}^{-1}(x_k - F_{k-1}x_{k-1}) \\right) + \\frac{\\partial}{\\partial x_k} \\left( \\frac{1}{2}(x_{k+1} - F_k x_k)^{\\top}Q_k^{-1}(x_{k+1} - F_k x_k) \\right) + \\frac{\\partial}{\\partial x_k} \\left( \\frac{1}{2}(y_k - H_k x_k)^{\\top}R_k^{-1}(y_k - H_k x_k) \\right)\n$$\nComputing each derivative:\n- $\\frac{\\partial}{\\partial x_k} \\left( \\frac{1}{2}(x_k - F_{k-1}x_{k-1})^{\\top}Q_{k-1}^{-1}(x_k - F_{k-1}x_{k-1}) \\right) = Q_{k-1}^{-1}(x_k - F_{k-1}x_{k-1})$\n- $\\frac{\\partial}{\\partial x_k} \\left( \\frac{1}{2}(x_{k+1} - F_k x_k)^{\\top}Q_k^{-1}(x_{k+1} - F_k x_k) \\right) = -F_k^{\\top}Q_k^{-1}(x_{k+1} - F_k x_k)$\n- $\\frac{\\partial}{\\partial x_k} \\left( \\frac{1}{2}(y_k - H_k x_k)^{\\top}R_k^{-1}(y_k - H_k x_k) \\right) = -H_k^{\\top}R_k^{-1}(y_k - H_k x_k)$\n\nSumming these and setting to zero gives the first-order optimality condition for $x_k$:\n$$\n\\frac{\\partial J}{\\partial x_k} = Q_{k-1}^{-1}(x_k - F_{k-1}x_{k-1}) - F_k^{\\top}Q_k^{-1}(x_{k+1} - F_k x_k) - H_k^{\\top}R_k^{-1}(y_k - H_k x_k) = 0\n$$\nTo form the normal equations $\\mathcal{H}x_{0:T}=b$, we group terms by the state vectors $x_{k-1}$, $x_k$, and $x_{k+1}$:\n$$\n(-F_k^{\\top}Q_k^{-1})x_{k+1} + (Q_{k-1}^{-1} + F_k^{\\top}Q_k^{-1}F_k + H_k^{\\top}R_k^{-1}H_k)x_k + (-Q_{k-1}^{-1}F_{k-1})^{\\top}x_{k-1} = H_k^{\\top}R_k^{-1}y_k\n$$\nThis equation corresponds to the $k$-th block row of the system $\\mathcal{H}x_{0:T}=b$. From this, we can identify the blocks of $\\mathcal{H}$ and $b$ for $k \\in \\{1, \\dots, T-1\\}$:\n- Sub-diagonal block: $\\mathcal{H}_{k,k-1} = (-Q_{k-1}^{-1}F_{k-1})^\\top = -F_{k-1}^\\top Q_{k-1}^{-1}$ (using symmetry of $\\mathcal{H}$)\n- Diagonal block: $\\mathcal{H}_{k,k} = Q_{k-1}^{-1} + F_k^{\\top}Q_k^{-1}F_k + H_k^{\\top}R_k^{-1}H_k$\n- Super-diagonal block: $\\mathcal{H}_{k,k+1} = -F_k^{\\top}Q_k^{-1}$\n- Right-hand side vector component: $b_k = H_k^{\\top}R_k^{-1}y_k$\n\nFor completeness, we examine the boundary conditions.\nFor $k=0$:\n$$\n\\frac{\\partial J}{\\partial x_0} = P_0^{-1}(x_0 - m_0) - F_0^{\\top}Q_0^{-1}(x_1 - F_0 x_0) - H_0^{\\top}R_0^{-1}(y_0 - H_0 x_0) = 0\n$$\n$$\n(P_0^{-1} + F_0^{\\top}Q_0^{-1}F_0 + H_0^{\\top}R_0^{-1}H_0)x_0 + (-F_0^{\\top}Q_0^{-1})x_1 = P_0^{-1}m_0 + H_0^{\\top}R_0^{-1}y_0\n$$\nThus, $\\mathcal{H}_{0,0} = P_0^{-1} + F_0^{\\top}Q_0^{-1}F_0 + H_0^{\\top}R_0^{-1}H_0$, $\\mathcal{H}_{0,1} = -F_0^{\\top}Q_0^{-1}$, and $b_0 = P_0^{-1}m_0 + H_0^{\\top}R_0^{-1}y_0$.\n\nFor $k=T$:\n$$\n\\frac{\\partial J}{\\partial x_T} = Q_{T-1}^{-1}(x_T - F_{T-1}x_{T-1}) - H_T^{\\top}R_T^{-1}(y_T - H_T x_T) = 0\n$$\n$$\n(-Q_{T-1}^{-1}F_{T-1})^\\top x_{T-1} + (Q_{T-1}^{-1} + H_T^{\\top}R_T^{-1}H_T)x_T = H_T^{\\top}R_T^{-1}y_T\n$$\nThus, $\\mathcal{H}_{T,T-1} = (-Q_{T-1}^{-1}F_{T-1})^\\top = -F_{T-1}^\\top Q_{T-1}^{-1}$, $\\mathcal{H}_{T,T} = Q_{T-1}^{-1} + H_T^{\\top}R_T^{-1}H_T$, and $b_T = H_T^{\\top}R_T^{-1}y_T$.\n\nThe problem asks for the sub-diagonal block of $\\mathcal{H}$ coupling $x_k$ to $x_{k-1}$. This is the block $\\mathcal{H}_{k, k-1}$ which multiplies $x_{k-1}$ in the $k$-th block row of the linear system. Based on our derivations for both the interior points ($k \\in \\{1,\\dots,T-1\\}$) and the endpoint ($k=T$), this block has the same general form. For any $k \\in \\{1, \\dots, T\\}$, the expression is:\n$$\n\\mathcal{H}_{k,k-1} = -F_{k-1}^\\top Q_{k-1}^{-1}\n$$\nThis expression represents the requested sub-diagonal block for a generic index $k$ in the specified range.", "answer": "$$\n\\boxed{-F_{k-1}^\\top Q_{k-1}^{-1}}\n$$", "id": "3393989"}, {"introduction": "This practice applies smoothing theory to a quintessential problem in robotics: Simultaneous Localization and Mapping (SLAM). You will implement both the recursive RTS smoother and a batch normal equations solver to jointly estimate a robot's path and the positions of static landmarks. This exercise not only provides a concrete numerical verification of the equivalence between recursive smoothing and batch optimization [@problem_id:3394031] but also explores the role of practical techniques like Levenberg-Marquardt damping in handling nonlinearities and improving solution robustness.", "problem": "Consider a linearized Simultaneous Localization and Mapping (SLAM) model in discrete time for a planar robot and two static landmarks. The augmented state at time index $t$ is $x_t \\in \\mathbb{R}^2$ for the robot pose and $l \\in \\mathbb{R}^4$ for the stacked landmark positions, so the augmented vector at time $t$ is $s_t = \\begin{bmatrix} x_t \\\\ l \\end{bmatrix} \\in \\mathbb{R}^6$. The robot dynamics are linear with a known control input sequence, and the landmarks are static. Measurements at each time step are linear and provide relative position observations from the robot to each landmark. All noises are zero-mean Gaussian with known covariances. You are asked to implement fixed-interval Rauch–Tung–Striebel (RTS) smoothing and compare the resulting joint posterior mean and marginal covariances to those obtained from solving the batch sparse normal equations with Levenberg–Marquardt damping. Additionally, you will implement a fixed-lag smoother and quantify its deviation from the fixed-interval smoother. The goal is to demonstrate and quantify the equivalence between fixed-interval RTS smoothing and undamped batch normal-equation solutions in the linear Gaussian case, and to analyze the effect of damping and lag on estimates and covariance recovery.\n\nUse the following scientifically consistent and numerically plausible setup:\n\n- Initial ground-truth pose at time $t=0$: $x_0^{\\text{true}} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n- Ground-truth landmark positions: $l_1^{\\text{true}} = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}$ and $l_2^{\\text{true}} = \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix}$, so $l^{\\text{true}} = \\begin{bmatrix} 2 \\\\ 0 \\\\ 3 \\\\ 1 \\end{bmatrix}$.\n- Control inputs over $t=0,1,2$: $u_0 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $u_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$, $u_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$. This yields a ground-truth trajectory $x_1^{\\text{true}} = x_0^{\\text{true}} + u_0$, $x_2^{\\text{true}} = x_1^{\\text{true}} + u_1$, $x_3^{\\text{true}} = x_2^{\\text{true}} + u_2$.\n- Process noise covariance for the robot: $Q_x = \\operatorname{diag}(0.01, 0.01)$. The landmark dynamics are static (identity), with zero process noise for the landmarks.\n- Measurement noise covariance for each landmark-relative measurement: $R = \\operatorname{diag}(0.05, 0.05)$. At each time $t \\in \\{0,1,2,3\\}$, the stacked measurement is $z_t = \\begin{bmatrix} l_1^{\\text{true}} - x_t^{\\text{true}} \\\\ l_2^{\\text{true}} - x_t^{\\text{true}} \\end{bmatrix} \\in \\mathbb{R}^4$.\n- Prior mean for the augmented state at $t=0$: $m_0 = \\begin{bmatrix} -0.5 \\\\ 0.2 \\\\ 1.5 \\\\ 0.5 \\\\ 2.5 \\\\ 2.0 \\end{bmatrix}$ corresponding to $x_0$ and $l$. Prior covariance is block-diagonal $P_0 = \\operatorname{diag}(0.5, 0.5, 0.5, 0.5, 0.5, 0.5)$.\n\nFoundational base to use:\n- Linear Gaussian state-space modeling and the Gauss–Markov theorem.\n- Bayesian estimation for linear systems with Gaussian noise.\n- Least-squares optimality and normal equations for linear models.\n- Levenberg–Marquardt (LM) damping as Tikhonov regularization in linear least squares (no need to provide a formula in the problem statement).\n\nTasks to implement from first principles:\n1. Formulate the augmented linear dynamical model and measurement model consistent with the above setup, using the identity transition for both the robot and landmarks and a deterministic control input applied to the robot component. Ensure the process noise only applies to the robot component and the measurement noise to both landmark-relative measurements at each time.\n2. Implement a fixed-interval Rauch–Tung–Striebel (RTS) smoother by first performing a Kalman filter forward pass for times $t \\in \\{0,1,2,3\\}$ and then a backward smoothing pass to obtain the smoothed posterior means and covariances at each time index. Jointly smooth the robot poses and landmarks by operating on the augmented state.\n3. Construct the batch least-squares normal equations for the unknown stacked vector $y = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\\\ l \\end{bmatrix} \\in \\mathbb{R}^{12}$ from:\n   - The prior on $x_0$ and $l$,\n   - The dynamical constraints $x_{t+1} - x_t = u_t$ for $t \\in \\{0,1,2\\}$,\n   - The measurement equations $z_t = \\begin{bmatrix} l_1 - x_t \\\\ l_2 - x_t \\end{bmatrix}$ for $t \\in \\{0,1,2,3\\}$.\n   Apply Levenberg–Marquardt damping with a scalar parameter $\\lambda \\ge 0$ by adding $\\lambda I$ to the normal-equation Hessian. Solve for the batch estimate and compute the batch posterior covariance as the inverse of the damped Hessian.\n4. Implement a fixed-lag smoother with lag $L=1$ that computes the smoothed mean at time $t=1$ using only data up to time $t+L=2$ by running a forward Kalman filter up to time $t+L$ and then performing a truncated backward pass from $t+L$ down to $t$.\n5. Quantitatively compare:\n   - The Euclidean norm of the difference between the fixed-interval RTS smoothed mean at time $t=3$ for the augmented state $\\begin{bmatrix} x_3 \\\\ l \\end{bmatrix}$ and the corresponding slice of the batch solution under three damping values $\\lambda \\in \\{0.0, 10^{-4}, 1.0\\}$.\n   - The Frobenius norm of the difference between the RTS smoothed marginal covariance of $l$ at time $t=3$ and the batch marginal covariance of $l$ for the same $\\lambda$ values.\n   - The Euclidean norm of the difference between the fixed-lag smoothed mean at time $t=1$ (with $L=1$) and the fixed-interval RTS smoothed mean at time $t=1$ for the augmented state $\\begin{bmatrix} x_1 \\\\ l \\end{bmatrix}$.\n\nYour program must implement the above models and computations and produce numerical results for the following test suite:\n- Test case $1$: $\\lambda = 0.0$ (undamped).\n- Test case $2$: $\\lambda = 10^{-4}$ (lightly damped).\n- Test case $3$: $\\lambda = 1.0$ (heavily damped).\n\nFor each test case, compute three floats in the order specified in item $5$. Aggregate all nine floats from the three test cases into a single output line. No physical units or angles are involved; all values are unitless. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,r_3,r_4,r_5,r_6,r_7,r_8,r_9]$), where each $r_i$ is a float computed by your implementation.\n\nEnsure scientific realism and internal consistency by following the given covariances and priors. Implement the algorithms from the foundational base without using any shortcut formulas in the problem statement.", "solution": "We model the augmented linear Gaussian system with state $s_t = \\begin{bmatrix} x_t \\\\ l \\end{bmatrix} \\in \\mathbb{R}^6$, where $x_t \\in \\mathbb{R}^2$ and $l \\in \\mathbb{R}^4$ stacks two landmark positions. The dynamics are\n$$\ns_{t+1} = F s_t + B u_t + w_t,\n$$\nwhere $F \\in \\mathbb{R}^{6 \\times 6}$ is the identity transition, $B \\in \\mathbb{R}^{6 \\times 2}$ injects the control $u_t \\in \\mathbb{R}^2$ into the robot pose component, and $w_t \\sim \\mathcal{N}(0, Q)$ is process noise with $Q = \\operatorname{blockdiag}(Q_x, 0_{4 \\times 4})$ and $Q_x = \\operatorname{diag}(0.01, 0.01)$. The measurements at time $t$ are stacked relative landmark positions,\n$$\nz_t = H_t s_t + v_t, \\quad v_t \\sim \\mathcal{N}(0, R_{\\text{blk}}),\n$$\nwith\n$$\nH_t = \\begin{bmatrix} -I_2 & I_2 & 0_{2 \\times 2} \\\\ -I_2 & 0_{2 \\times 2} & I_2 \\end{bmatrix} \\in \\mathbb{R}^{4 \\times 6}, \\quad R_{\\text{blk}} = \\operatorname{blockdiag}(R, R), \\quad R = \\operatorname{diag}(0.05, 0.05).\n$$\nThe prior at $t=0$ is $s_0 \\sim \\mathcal{N}(m_0, P_0)$ with $m_0 = \\begin{bmatrix} -0.5 \\\\ 0.2 \\\\ 1.5 \\\\ 0.5 \\\\ 2.5 \\\\ 2.0 \\end{bmatrix}$ and $P_0 = \\operatorname{diag}(0.5, 0.5, 0.5, 0.5, 0.5, 0.5)$.\n\nWe use foundational principles: linear Gaussian models imply the posterior is Gaussian. Bayesian filtering with Gaussian assumptions yields the Kalman filter, and fixed-interval smoothing uses backward recursion based on the Gauss–Markov theorem and conditional linear-Gaussian relationships.\n\nForward Kalman filter. At each time $t$, we compute the predicted mean and covariance:\n$$\n\\hat{m}_{t|t-1} = F m_{t-1|t-1} + B u_{t-1}, \\quad \\hat{P}_{t|t-1} = F P_{t-1|t-1} F^\\top + Q,\n$$\nwith the convention for $t=0$ that $\\hat{m}_{0| -1} = m_0$ and $\\hat{P}_{0| -1} = P_0$. The measurement update is\n$$\nS_t = H_t \\hat{P}_{t|t-1} H_t^\\top + R_{\\text{blk}}, \\quad K_t = \\hat{P}_{t|t-1} H_t^\\top S_t^{-1},\n$$\n$$\nm_{t|t} = \\hat{m}_{t|t-1} + K_t (z_t - H_t \\hat{m}_{t|t-1}), \\quad P_{t|t} = (I - K_t H_t) \\hat{P}_{t|t-1}.\n$$\n\nBackward Rauch–Tung–Striebel (RTS) smoother. For $t=T$ (here $T=3$), $m_{T|T}^s = m_{T|T}$ and $P_{T|T}^s = P_{T|T}$. For $t=T-1,\\dots,0$, the RTS gain is\n$$\nJ_t = P_{t|t} F^\\top \\hat{P}_{t+1|t}^{-1},\n$$\nand the smoothed estimates are\n$$\nm_{t|T}^s = m_{t|t} + J_t \\left(m_{t+1|T}^s - \\hat{m}_{t+1|t}\\right), \\quad P_{t|T}^s = P_{t|t} + J_t \\left(P_{t+1|T}^s - \\hat{P}_{t+1|t}\\right) J_t^\\top.\n$$\nBecause $F$ is the identity and landmarks are static, this recursion jointly smooths the robot and landmarks.\n\nBatch least squares and normal equations. We assemble a global vector of unknowns\n$$\ny = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\\\ l \\end{bmatrix} \\in \\mathbb{R}^{12}.\n$$\nWe construct linear equations:\n- Prior: $x_0 \\approx m_{0,x}$ with covariance $\\operatorname{diag}(0.5, 0.5)$, and $l \\approx m_{0,l}$ with covariance $\\operatorname{diag}(0.5, 0.5, 0.5, 0.5)$.\n- Dynamics for $t=0,1,2$: $x_{t+1} - x_t \\approx u_t$ with covariance $Q_x$.\n- Measurements for $t=0,1,2,3$: $l_1 - x_t \\approx z_t^{(1)}$ and $l_2 - x_t \\approx z_t^{(2)}$ with covariance $R$ for each $2$-vector component, yielding a $4$-vector stacked with covariance $R_{\\text{blk}}$.\n\nStacking these into $A y \\approx b$ and prewhitening by $W^{1/2}$ such that $W = \\operatorname{blockdiag}(P_0^{-1}, Q_x^{-1}, R^{-1}, \\dots)$ yields a weighted least-squares problem. The normal equations are\n$$\nH y = g, \\quad H = A^\\top W A, \\quad g = A^\\top W b.\n$$\nLevenberg–Marquardt damping with parameter $\\lambda \\ge 0$ modifies the system to\n$$\n(H + \\lambda I) y = g,\n$$\nwhich is Tikhonov regularization in the linear case. The batch posterior covariance (under the Gaussian approximation consistent with the linear model) is\n$$\n\\Sigma_{\\text{batch}} = (H + \\lambda I)^{-1}.\n$$\n\nEquivalence and comparisons. In the linear Gaussian case with $\\lambda = 0$, the fixed-interval RTS smoothed mean at $t=3$ (restricted to $\\begin{bmatrix} x_3 \\\\ l \\end{bmatrix}$) equals the corresponding slice of the batch least-squares solution $y$. The RTS smoothed covariance block for $l$ at $t=3$ equals the batch marginal covariance for $l$ recovered from the inverse Hessian restricted to the $l$ indices. With $\\lambda > 0$, damping biases the batch solution towards zero and inflates covariances relative to the undamped case, breaking exact equality with the RTS solution derived from the original probabilistic model.\n\nFixed-lag smoothing with lag $L=1$ at $t=1$ conditions only on data up to $t+L=2$. It can be implemented by running the forward filter up to $t+L$ and then applying a truncated backward RTS recursion from $t+L$ to $t$. The fixed-lag mean at $t=1$ generally differs from the fixed-interval mean at $t=1$ because the latter conditions on more data (here, measurements at $t=3$), which introduces additional information about both the robot and the landmarks.\n\nNumerical implementation details for the test suite:\n- We compute $z_t$ using the given ground-truth $x_t^{\\text{true}}$ and $l^{\\text{true}}$ exactly (noise-free realisations compatible with expected values).\n- For each $\\lambda \\in \\{0.0, 10^{-4}, 1.0\\}$, we solve the batch normal equations to obtain $y_{\\lambda}$ and $\\Sigma_{\\text{batch},\\lambda}$. We extract $\\begin{bmatrix} x_3 \\\\ l \\end{bmatrix}$ and the marginal covariances for $l$.\n- We run the forward Kalman filter and backward RTS smoother to obtain $m_{t|T}^s$ and $P_{t|T}^s$ for $t \\in \\{0,1,2,3\\}$, and then compute:\n  1. The Euclidean norm $\\left\\| \\begin{bmatrix} x_3 \\\\ l \\end{bmatrix}_{\\text{RTS}} - \\begin{bmatrix} x_3 \\\\ l \\end{bmatrix}_{\\text{batch},\\lambda} \\right\\|_2$.\n  2. The Frobenius norm $\\left\\| P_{l,\\text{RTS}}(t=3) - \\Sigma_{l,\\text{batch},\\lambda} \\right\\|_F$, where $\\Sigma_{l,\\text{batch},\\lambda}$ is the $4 \\times 4$ marginal from $\\Sigma_{\\text{batch},\\lambda}$ and $P_{l,\\text{RTS}}(t=3)$ is the $4 \\times 4$ block of $P_{3|T}^s$ corresponding to $l$.\n  3. The Euclidean norm $\\left\\| \\begin{bmatrix} x_1 \\\\ l \\end{bmatrix}_{\\text{fixed-lag},L=1} - \\begin{bmatrix} x_1 \\\\ l \\end{bmatrix}_{\\text{RTS}} \\right\\|_2$.\n\nThe final program outputs nine floats in a single line, one triplet per $\\lambda$. This demonstrates the equivalence in the undamped case ($\\lambda = 0$), the effect of damping, and the deviation introduced by fixed-lag smoothing, while correctly handling marginal covariance recovery via both RTS smoothing and the inverse Hessian from the batch formulation.", "answer": "```python\nimport numpy as np\n\ndef build_models():\n    # Dimensions\n    d = 2  # robot pose dimension\n    n_landmarks = 2\n    ldim = 2 * n_landmarks\n    aug_dim = d + ldim  # 6\n\n    # Ground-truth trajectory and landmarks\n    x0_true = np.array([0.0, 0.0])\n    u_list = [np.array([1.0, 0.0]), np.array([1.0, 0.0]), np.array([0.0, 1.0])]\n    x_true = [x0_true]\n    for u in u_list:\n        x_true.append(x_true[-1] + u)\n    # Now x_true: t=0..3\n    l_true = np.array([2.0, 0.0, 3.0, 1.0])  # [l1; l2]\n\n    # Measurements z_t = [l1 - x_t; l2 - x_t]\n    z_list = []\n    for t in range(4):\n        x_t = x_true[t]\n        l1 = l_true[0:2]\n        l2 = l_true[2:4]\n        z = np.concatenate([l1 - x_t, l2 - x_t])\n        z_list.append(z)\n\n    # Prior\n    m0 = np.array([-0.5, 0.2, 1.5, 0.5, 2.5, 2.0])  # [x0; l]\n    P0 = np.diag([0.5, 0.5, 0.5, 0.5, 0.5, 0.5])\n\n    # Process and measurement covariances\n    Qx = np.diag([0.01, 0.01])\n    Q_aug = np.block([\n        [Qx,               np.zeros((d, ldim))],\n        [np.zeros((ldim, d)), np.zeros((ldim, ldim))]\n    ])\n    R = np.diag([0.05, 0.05])\n    R_block = np.block([\n        [R,               np.zeros((d, d))],\n        [np.zeros((d, d)), R]\n    ])\n\n    # Dynamics matrices\n    F = np.eye(aug_dim)\n    B = np.zeros((aug_dim, d))\n    B[0:d, 0:d] = np.eye(d)\n\n    # Measurement matrix H_t is constant here\n    # H = [[-I, I, 0], [-I, 0, I]]\n    H = np.zeros((2 * d, aug_dim))\n    # block for l1 - x: rows 0:2\n    H[0:d, 0:d] = -np.eye(d)\n    H[0:d, d:d + d] = np.eye(d)\n    # block for l2 - x: rows 2:4\n    H[d:2 * d, 0:d] = -np.eye(d)\n    H[d:2 * d, d + d:d + d + d] = np.eye(d)\n    H_list = [H for _ in range(4)]\n\n    return {\n        \"d\": d,\n        \"ldim\": ldim,\n        \"aug_dim\": aug_dim,\n        \"x_true\": x_true,\n        \"l_true\": l_true,\n        \"z_list\": z_list,\n        \"m0\": m0,\n        \"P0\": P0,\n        \"Qx\": Qx,\n        \"Q_aug\": Q_aug,\n        \"R\": R,\n        \"R_block\": R_block,\n        \"F\": F,\n        \"B\": B,\n        \"H_list\": H_list,\n        \"u_list\": u_list\n    }\n\ndef kalman_rts(models):\n    d = models[\"d\"]\n    aug_dim = models[\"aug_dim\"]\n    F = models[\"F\"]\n    B = models[\"B\"]\n    Q_aug = models[\"Q_aug\"]\n    R_block = models[\"R_block\"]\n    H_list = models[\"H_list\"]\n    u_list = models[\"u_list\"]\n    z_list = models[\"z_list\"]\n    m0 = models[\"m0\"]\n    P0 = models[\"P0\"]\n\n    T = 3  # last time index\n    # Storage\n    m_pred = [None] * (T + 1)\n    P_pred = [None] * (T + 1)\n    m_filt = [None] * (T + 1)\n    P_filt = [None] * (T + 1)\n\n    # Initialize prediction at t=0 from prior\n    m_pred[0] = m0.copy()\n    P_pred[0] = P0.copy()\n\n    # Filter loop\n    for t in range(T + 1):\n        H = H_list[t]\n        z = z_list[t]\n        # Update\n        S = H @ P_pred[t] @ H.T + R_block\n        K = P_pred[t] @ H.T @ np.linalg.inv(S)\n        innov = z - H @ m_pred[t]\n        m_filt[t] = m_pred[t] + K @ innov\n        P_filt[t] = (np.eye(aug_dim) - K @ H) @ P_pred[t]\n        # Predict for next time if not last\n        if t  T:\n            u = u_list[t]\n            m_pred[t + 1] = F @ m_filt[t] + B @ u\n            P_pred[t + 1] = F @ P_filt[t] @ F.T + Q_aug\n\n    # RTS backward pass\n    m_smooth = [None] * (T + 1)\n    P_smooth = [None] * (T + 1)\n    m_smooth[T] = m_filt[T].copy()\n    P_smooth[T] = P_filt[T].copy()\n    for t in range(T - 1, -1, -1):\n        J = P_filt[t] @ F.T @ np.linalg.inv(P_pred[t + 1])\n        m_smooth[t] = m_filt[t] + J @ (m_smooth[t + 1] - m_pred[t + 1])\n        P_smooth[t] = P_filt[t] + J @ (P_smooth[t + 1] - P_pred[t + 1]) @ J.T\n\n    return m_filt, P_filt, m_smooth, P_smooth, m_pred, P_pred\n\ndef fixed_lag_smoother(models, t_query, L):\n    # Compute smoothed mean at time t_query using data only up to t_query + L\n    d = models[\"d\"]\n    aug_dim = models[\"aug_dim\"]\n    F = models[\"F\"]\n    B = models[\"B\"]\n    Q_aug = models[\"Q_aug\"]\n    R_block = models[\"R_block\"]\n    H_list = models[\"H_list\"]\n    u_list = models[\"u_list\"]\n    z_list = models[\"z_list\"]\n    m0 = models[\"m0\"]\n    P0 = models[\"P0\"]\n\n    T_end = t_query + L\n    # Forward filter up to T_end\n    m_pred = [None] * (T_end + 1)\n    P_pred = [None] * (T_end + 1)\n    m_filt = [None] * (T_end + 1)\n    P_filt = [None] * (T_end + 1)\n\n    m_pred[0] = m0.copy()\n    P_pred[0] = P0.copy()\n\n    for t in range(T_end + 1):\n        H = H_list[t]\n        z = z_list[t]\n        S = H @ P_pred[t] @ H.T + R_block\n        K = P_pred[t] @ H.T @ np.linalg.inv(S)\n        innov = z - H @ m_pred[t]\n        m_filt[t] = m_pred[t] + K @ innov\n        P_filt[t] = (np.eye(aug_dim) - K @ H) @ P_pred[t]\n        if t  T_end:\n            u = u_list[t]\n            m_pred[t + 1] = F @ m_filt[t] + B @ u\n            P_pred[t + 1] = F @ P_filt[t] @ F.T + Q_aug\n\n    # Backward smoothing from T_end down to t_query\n    m_smooth = [None] * (T_end + 1)\n    P_smooth = [None] * (T_end + 1)\n    m_smooth[T_end] = m_filt[T_end].copy()\n    P_smooth[T_end] = P_filt[T_end].copy()\n    for t in range(T_end - 1, t_query - 1, -1):\n        J = P_filt[t] @ F.T @ np.linalg.inv(P_pred[t + 1])\n        m_smooth[t] = m_filt[t] + J @ (m_smooth[t + 1] - m_pred[t + 1])\n        P_smooth[t] = P_filt[t] + J @ (P_smooth[t + 1] - P_pred[t + 1]) @ J.T\n\n    return m_smooth[t_query], P_smooth[t_query]\n\ndef batch_normal_equations(models, lam):\n    # Build A and b for prior, dynamics, measurements, then solve (H + lam I) y = g\n    d = models[\"d\"]\n    ldim = models[\"ldim\"]\n    x_true = models[\"x_true\"]\n    z_list = models[\"z_list\"]\n    Qx = models[\"Qx\"]\n    R = models[\"R\"]\n    m0 = models[\"m0\"]\n    P0 = models[\"P0\"]\n\n    # Unknown stacking: y = [x0, x1, x2, x3, l] of length 12\n    # Column index helpers\n    def idx_x(t):\n        return 2 * t, 2 * t + 2  # slice indices\n    idx_l1 = 2 * 4  # start at 8\n    idx_l2 = idx_l1 + 2  # start at 10\n\n    # Collect weighted rows\n    A_blocks = []\n    b_blocks = []\n\n    # Prior on x0\n    A_prior_x0 = np.zeros((2, 12))\n    s, e = idx_x(0)\n    A_prior_x0[:, s:e] = np.eye(2)\n    b_prior_x0 = m0[0:2]\n    # Whitening with P0_x inverse sqrt\n    P0_x = np.diag([P0[0, 0], P0[1, 1]])\n    L_prior_x = np.diag(1.0 / np.sqrt(np.diag(P0_x)))\n    A_blocks.append(L_prior_x @ A_prior_x0)\n    b_blocks.append(L_prior_x @ b_prior_x0)\n\n    # Prior on l1\n    A_prior_l1 = np.zeros((2, 12))\n    A_prior_l1[:, idx_l1:idx_l1 + 2] = np.eye(2)\n    b_prior_l1 = m0[2:4]\n    P0_l1 = np.diag([P0[2, 2], P0[3, 3]])\n    L_prior_l1 = np.diag(1.0 / np.sqrt(np.diag(P0_l1)))\n    A_blocks.append(L_prior_l1 @ A_prior_l1)\n    b_blocks.append(L_prior_l1 @ b_prior_l1)\n\n    # Prior on l2\n    A_prior_l2 = np.zeros((2, 12))\n    A_prior_l2[:, idx_l2:idx_l2 + 2] = np.eye(2)\n    b_prior_l2 = m0[4:6]\n    P0_l2 = np.diag([P0[4, 4], P0[5, 5]])\n    L_prior_l2 = np.diag(1.0 / np.sqrt(np.diag(P0_l2)))\n    A_blocks.append(L_prior_l2 @ A_prior_l2)\n    b_blocks.append(L_prior_l2 @ b_prior_l2)\n\n    # Dynamics: x_{t+1} - x_t = u_t, t=0..2\n    for t in range(3):\n        A_dyn = np.zeros((2, 12))\n        s0, e0 = idx_x(t)\n        s1, e1 = idx_x(t + 1)\n        A_dyn[:, s1:e1] = np.eye(2)\n        A_dyn[:, s0:e0] = -np.eye(2)\n        b_dyn = models[\"u_list\"][t]\n        L_dyn = np.diag(1.0 / np.sqrt(np.diag(Qx)))\n        A_blocks.append(L_dyn @ A_dyn)\n        b_blocks.append(L_dyn @ b_dyn)\n\n    # Measurements: for t=0..3, z_t = [l1 - x_t; l2 - x_t]\n    for t in range(4):\n        # First measurement: l1 - x_t = z_t[0:2]\n        A_meas1 = np.zeros((2, 12))\n        s, e = idx_x(t)\n        A_meas1[:, s:e] = -np.eye(2)\n        A_meas1[:, idx_l1:idx_l1 + 2] = np.eye(2)\n        b_meas1 = z_list[t][0:2]\n        L_meas = np.diag(1.0 / np.sqrt(np.diag(R)))\n        A_blocks.append(L_meas @ A_meas1)\n        b_blocks.append(L_meas @ b_meas1)\n\n        # Second measurement: l2 - x_t = z_t[2:4]\n        A_meas2 = np.zeros((2, 12))\n        A_meas2[:, s:e] = -np.eye(2)\n        A_meas2[:, idx_l2:idx_l2 + 2] = np.eye(2)\n        b_meas2 = z_list[t][2:4]\n        A_blocks.append(L_meas @ A_meas2)\n        b_blocks.append(L_meas @ b_meas2)\n\n    # Stack\n    A_w = np.vstack(A_blocks)\n    b_w = np.concatenate(b_blocks)\n\n    # Normal equations\n    H = A_w.T @ A_w\n    g = A_w.T @ b_w\n\n    # Damped solve\n    H_damped = H + lam * np.eye(H.shape[0])\n    y_hat = np.linalg.solve(H_damped, g)\n    Sigma_batch = np.linalg.inv(H_damped)\n\n    return y_hat, Sigma_batch\n\ndef compute_metrics(models, lam, m_smooth, P_smooth):\n    # Compare RTS smoothed [x3; l] vs batch solution slice\n    y_hat, Sigma_batch = batch_normal_equations(models, lam)\n    # Extract slices\n    x3_batch = y_hat[6:8]\n    l_batch = y_hat[8:12]\n    # RTS at t=3\n    x3_rts = m_smooth[3][0:2]\n    l_rts = m_smooth[3][2:6]\n    # Metric 1: Euclidean norm of mean difference\n    mean_diff = np.linalg.norm(np.concatenate([x3_rts, l_rts]) - np.concatenate([x3_batch, l_batch]))\n\n    # Metric 2: Frobenius norm of marginal covariance difference for l\n    Sigma_l_batch = Sigma_batch[8:12, 8:12]\n    P_l_rts = P_smooth[3][2:6, 2:6]\n    cov_l_diff = np.linalg.norm(P_l_rts - Sigma_l_batch, ord='fro')\n\n    # Metric 3: Fixed-lag mean deviation at t=1 (L=1) vs fixed-interval\n    m_lag_1, _ = fixed_lag_smoother(models, t_query=1, L=1)\n    mean_lag_diff = np.linalg.norm(m_lag_1 - m_smooth[1])\n\n    return mean_diff, cov_l_diff, mean_lag_diff\n\ndef solve():\n    models = build_models()\n    # Run RTS once (independent of damping)\n    _, _, m_smooth, P_smooth, _, _ = kalman_rts(models)\n\n    # Test suite lambdas\n    test_cases = [0.0, 1e-4, 1.0]\n\n    results = []\n    for lam in test_cases:\n        m1, m2, m3 = compute_metrics(models, lam, m_smooth, P_smooth)\n        results.extend([m1, m2, m3])\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3394031"}, {"introduction": "Although fixed-interval smoothing provides the most accurate estimate by using all available data, its requirement to process the entire dataset can be computationally prohibitive for real-time applications. This leads to the use of fixed-lag smoothers, which offer a trade-off between accuracy and efficiency. This practice guides you through the development of a principled, adaptive method for choosing the smoothing lag, where you will prove that the reduction in state covariance from smoothing directly corresponds to the mean-squared error introduced by truncation, providing a powerful tool for designing efficient, near-optimal smoothers [@problem_id:3393965].", "problem": "Consider a linear, time-invariant, discrete-time, Gaussian state-space model with state dimension $n$ and observation dimension $p$. The latent state $\\mathbf{x}_t \\in \\mathbb{R}^n$ and observation $\\mathbf{y}_t \\in \\mathbb{R}^p$ satisfy\n$$\n\\mathbf{x}_{t+1} = \\mathbf{A}\\,\\mathbf{x}_t + \\mathbf{w}_t,\\quad \\mathbf{w}_t \\sim \\mathcal{N}(\\mathbf{0},\\,\\mathbf{Q}),\n$$\n$$\n\\mathbf{y}_t = \\mathbf{H}\\,\\mathbf{x}_t + \\mathbf{v}_t,\\quad \\mathbf{v}_t \\sim \\mathcal{N}(\\mathbf{0},\\,\\mathbf{R}),\n$$\nwith $\\mathbf{w}_t$ and $\\mathbf{v}_t$ mutually independent and independent across time, and $\\mathbf{x}_0 \\sim \\mathcal{N}(\\mathbf{m}_0,\\,\\mathbf{P}_0)$. All matrices $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, $\\mathbf{Q} \\in \\mathbb{R}^{n \\times n}$, $\\mathbf{H} \\in \\mathbb{R}^{p \\times n}$, and $\\mathbf{R} \\in \\mathbb{R}^{p \\times p}$ are known, with $\\mathbf{Q}$ and $\\mathbf{R}$ symmetric positive definite. The Kalman Filter (KF) produces filtered estimates $\\mathbf{m}_{t|t}$ and $\\mathbf{P}_{t|t}$, and the Rauch-Tung-Striebel (RTS) fixed-interval smoother produces smoothed estimates $\\mathbf{m}_{t|T}$ and $\\mathbf{P}_{t|T}$ for $t = 0, 1, \\dots, T$, where $T$ is the terminal time index.\n\nDefine the backward information content at time $t$ as the smoothing-induced reduction in covariance\n$$\n\\Delta \\mathbf{P}_t \\triangleq \\mathbf{P}_{t|t} - \\mathbf{P}_{t|T},\\quad \\Delta \\mathbf{P}_t \\succeq \\mathbf{0}.\n$$\nWe propose an adaptive stopping criterion for the backward RTS recursion: halt the recursion at the earliest time $t^\\star$ such that\n$$\n\\mathrm{tr}\\!\\left( \\Delta \\mathbf{P}_t \\right) \\le \\varepsilon,\n$$\nfor a user-specified threshold $\\varepsilon  0$, and for all $k \\le t^\\star$ set $\\mathbf{m}_{k|T}^{\\text{stop}} \\triangleq \\mathbf{m}_{k|k}$ and $\\mathbf{P}_{k|T}^{\\text{stop}} \\triangleq \\mathbf{P}_{k|k}$ while retaining full RTS values for $k  t^\\star$. This yields an approximate smoother that uses future information only back to the stopping index.\n\nYour tasks are:\n- Starting from fundamental definitions of Gaussian conditioning, the Kalman Filter, and the RTS recursion, derive a mathematically sound criterion for adaptive lag selection based on the backward information content $\\Delta \\mathbf{P}_t$. Establish that $\\Delta \\mathbf{P}_t$ is positive semidefinite and that its trace is a scalar summary of the backward information gained from observations after time $t$.\n- Prove that the proposed stopping criterion bounds a natural notion of bias introduced by truncation in the smoothed mean. Specifically, define the truncation bias at time $t$ as $\\mathbf{b}_t \\triangleq \\mathbf{m}_{t|T} - \\mathbf{m}_{t|T}^{\\text{stop}}$ and show that the mean-squared truncation bias averaged over the joint distribution of states and observations satisfies\n$$\n\\mathbb{E}\\!\\left[\\|\\mathbf{b}_t\\|_2^2\\right] = \\mathrm{tr}\\!\\left( \\mathbf{P}_{t|T}^{\\text{stop}} - \\mathbf{P}_{t|T} \\right),\n$$\nand, in the specific case when the recursion stops at $t^\\star$, for all $t \\le t^\\star$,\n$$\n\\mathbb{E}\\!\\left[\\|\\mathbf{b}_t\\|_2^2\\right] = \\mathrm{tr}\\!\\left( \\Delta \\mathbf{P}_t \\right).\n$$\n- Implement a complete program that simulates data from a specified model and computes, for a set of thresholds $\\varepsilon$, the aggregate mean-squared truncation bias across the entire trajectory,\n$$\nB(\\varepsilon) \\triangleq \\sum_{t=0}^{T} \\mathrm{tr}\\!\\left( \\mathbf{P}_{t|T}^{\\text{stop}(\\varepsilon)} - \\mathbf{P}_{t|T} \\right),\n$$\nwhere $\\mathbf{P}_{t|T}^{\\text{stop}(\\varepsilon)}$ denotes the smoothed covariance with the adaptive stopping criterion at threshold $\\varepsilon$. The program must use a single fixed model and a single simulated data realization with fixed random seed.\n\nUse the following fixed model and simulation parameters, which must be implemented exactly as specified:\n- State dimension $n = 2$, observation dimension $p = 2$.\n- Terminal time $T = 60$.\n- System matrices\n$$\n\\mathbf{A} = \\begin{bmatrix} 0.9  0.3 \\\\ 0.0  0.7 \\end{bmatrix},\\quad\n\\mathbf{H} = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}.\n$$\n- Covariances\n$$\n\\mathbf{Q} = \\begin{bmatrix} 0.05  0.01 \\\\ 0.01  0.05 \\end{bmatrix},\\quad\n\\mathbf{R} = \\begin{bmatrix} 0.10  0.00 \\\\ 0.00  0.10 \\end{bmatrix}.\n$$\n- Initial condition\n$$\n\\mathbf{m}_0 = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix},\\quad\n\\mathbf{P}_0 = \\begin{bmatrix} 1.0  0.0 \\\\ 0.0  1.0 \\end{bmatrix}.\n$$\n- Use a single random seed `seed` = 12345 and draw $\\mathbf{x}_0$, $\\{\\mathbf{w}_t\\}_{t=0}^{T-1}$, and $\\{\\mathbf{v}_t\\}_{t=0}^{T}$ accordingly.\n\nYour test suite must evaluate the aggregate mean-squared truncation bias $B(\\varepsilon)$ for the following thresholds:\n- Thresholds $\\varepsilon \\in \\{0.0,\\;10^{-5},\\;10^{-3},\\;10^{2}\\}$.\n\nDesign for coverage:\n- The case $\\varepsilon = 0.0$ tests the boundary condition of full smoothing with no truncation.\n- The case $\\varepsilon = 10^{-5}$ tests a near-full smoothing scenario with minimal truncation.\n- The case $\\varepsilon = 10^{-3}$ tests a typical adaptive-lag truncation.\n- The case $\\varepsilon = 10^{2}$ tests the edge case of early stopping, approximating no smoothing.\n\nYour program must produce a single line of output containing the results for the test suite as a comma-separated list of Python floats enclosed in square brackets and ordered to match the thresholds given above, for example, `[r_1,r_2,r_3,r_4]`. No physical units are involved in this problem, and no angles are used, so no unit specification is required beyond the mathematical model described here. The program must be self-contained, require no input, and use only the specified runtime environment.", "solution": "The user-provided problem statement is valid. It is scientifically grounded in the well-established theory of Bayesian filtering and smoothing, specifically for linear-Gaussian state-space models. The problem is well-posed, providing all necessary parameters and a clear, albeit complex, set of tasks. The combination of theoretical derivation, proof, and numerical implementation constitutes a standard and substantive exercise in the field of statistical signal processing.\n\nWe divide the solution into two parts: the theoretical derivation and proof, and the implementation of the numerical experiment.\n\n### Part 1: Theoretical Foundation and Derivations\n\nThis part establishes the theoretical underpinnings of the proposed adaptive stopping criterion for the Rauch-Tung-Striebel (RTS) smoother. We begin by stating the standard equations for the Kalman filter and RTS smoother.\n\nLet the state-space model be given by:\n$$\n\\mathbf{x}_{t+1} = \\mathbf{A}\\,\\mathbf{x}_t + \\mathbf{w}_t,\\quad \\mathbf{w}_t \\sim \\mathcal{N}(\\mathbf{0},\\,\\mathbf{Q})\n$$\n$$\n\\mathbf{y}_t = \\mathbf{H}\\,\\mathbf{x}_t + \\mathbf{v}_t,\\quad \\mathbf{v}_t \\sim \\mathcal{N}(\\mathbf{0},\\,\\mathbf{R})\n$$\nThe posterior distribution of the state $\\mathbf{x}_t$ given observations up to time $\\tau$, denoted $\\mathbf{y}_{0:\\tau} \\triangleq \\{\\mathbf{y}_0, \\dots, \\mathbf{y}_\\tau\\}$, is Gaussian: $p(\\mathbf{x}_t|\\mathbf{y}_{0:\\tau}) = \\mathcal{N}(\\mathbf{x}_t | \\mathbf{m}_{t|\\tau}, \\mathbf{P}_{t|\\tau})$.\n\n**Kalman Filter (Forward Pass)**\nThe Kalman filter recursively computes the filtered posteriors $p(\\mathbf{x}_t|\\mathbf{y}_{0:t})$ for $t=0, \\dots, T$. Starting with the prior $\\mathbf{m}_{0|-1} = \\mathbf{m}_0$ and $\\mathbf{P}_{0|-1} = \\mathbf{P}_0$, for each time step $t=0, \\dots, T$:\n\n1.  **Prediction Step** (for $t > 0$):\n    $$ \\mathbf{m}_{t|t-1} = \\mathbf{A}\\,\\mathbf{m}_{t-1|t-1} $$\n    $$ \\mathbf{P}_{t|t-1} = \\mathbf{A}\\,\\mathbf{P}_{t-1|t-1} \\mathbf{A}^\\top + \\mathbf{Q} $$\n    For $t=0$, we use the initial prior directly: $\\mathbf{m}_{0|-1}=\\mathbf{m}_0, \\mathbf{P}_{0|-1}=\\mathbf{P}_0$.\n\n2.  **Update Step**:\n    $$ \\mathbf{S}_t = \\mathbf{H}\\,\\mathbf{P}_{t|t-1} \\mathbf{H}^\\top + \\mathbf{R} $$\n    $$ \\mathbf{K}_t = \\mathbf{P}_{t|t-1} \\mathbf{H}^\\top \\mathbf{S}_t^{-1} $$\n    $$ \\mathbf{m}_{t|t} = \\mathbf{m}_{t|t-1} + \\mathbf{K}_t (\\mathbf{y}_t - \\mathbf{H}\\,\\mathbf{m}_{t|t-1}) $$\n    $$ \\mathbf{P}_{t|t} = (\\mathbf{I} - \\mathbf{K}_t \\mathbf{H}) \\mathbf{P}_{t|t-1} $$\n\n**Rauch-Tung-Striebel (RTS) Smoother (Backward Pass)**\nThe RTS smoother computes the smoothed posteriors $p(\\mathbf{x}_t|\\mathbf{y}_{0:T})$ for $t=T-1, \\dots, 0$. It initializes with the final filtered estimate, $\\mathbf{m}_{T|T}$ and $\\mathbf{P}_{T|T}$, and recurses backward:\n\n1.  **Smoother Gain**:\n    $$ \\mathbf{G}_t = \\mathbf{P}_{t|t} \\mathbf{A}^\\top \\mathbf{P}_{t+1|t}^{-1} $$\n\n2.  **Smoother Update**:\n    $$ \\mathbf{m}_{t|T} = \\mathbf{m}_{t|t} + \\mathbf{G}_t (\\mathbf{m}_{t+1|T} - \\mathbf{m}_{t+1|t}) $$\n    $$ \\mathbf{P}_{t|T} = \\mathbf{P}_{t|t} + \\mathbf{G}_t (\\mathbf{P}_{t+1|T} - \\mathbf{P}_{t+1|t}) \\mathbf{G}_t^\\top $$\n\n**Task 1: The Backward Information Content $\\Delta \\mathbf{P}_t$**\n\nThe backward information content is defined as $\\Delta \\mathbf{P}_t \\triangleq \\mathbf{P}_{t|t} - \\mathbf{P}_{t|T}$.\n\nFirst, we establish that $\\Delta \\mathbf{P}_t$ is positive semidefinite, i.e., $\\Delta \\mathbf{P}_t \\succeq \\mathbf{0}$. The filtered covariance $\\mathbf{P}_{t|t} = \\mathrm{Cov}(\\mathbf{x}_t | \\mathbf{y}_{0:t})$ is the covariance of $\\mathbf{x}_t$ conditioned on observations up to time $t$. The smoothed covariance $\\mathbf{P}_{t|T} = \\mathrm{Cov}(\\mathbf{x}_t | \\mathbf{y}_{0:T})$ is the covariance of $\\mathbf{x}_t$ conditioned on all observations, including those from the future, $\\{\\mathbf{y}_{t+1}, \\dots, \\mathbf{y}_T\\}$. It is a fundamental property of conditioning that more information cannot increase uncertainty (variance). For any two sets of random variables $\\mathcal{D}_1$ and $\\mathcal{D}_2$ such that $\\mathcal{D}_1 \\subseteq \\mathcal{D}_2$, the conditional covariances satisfy $\\mathrm{Cov}(\\mathbf{x}|\\mathcal{D}_1) \\succeq \\mathrm{Cov}(\\mathbf{x}|\\mathcal{D}_2)$. In our context, with $\\mathbf{x} = \\mathbf{x}_t$, $\\mathcal{D}_1 = \\mathbf{y}_{0:t}$, and $\\mathcal{D}_2 = \\mathbf{y}_{0:T}$, we have $\\mathbf{P}_{t|t} \\succeq \\mathbf{P}_{t|T}$, which directly implies $\\mathbf{P}_{t|t} - \\mathbf{P}_{t|T} = \\Delta \\mathbf{P}_t \\succeq \\mathbf{0}$.\n\nThe trace of a covariance matrix gives the sum of the variances of the state components, which is the mean squared error (MSE) of the corresponding estimate. Thus, $\\mathrm{tr}(\\mathbf{P}_{t|t})$ is the MSE of the filtered estimate $\\mathbf{m}_{t|t}$, and $\\mathrm{tr}(\\mathbf{P}_{t|T})$ is the MSE of the smoothed estimate $\\mathbf{m}_{t|T}$. The quantity $\\mathrm{tr}(\\Delta \\mathbf{P}_t) = \\mathrm{tr}(\\mathbf{P}_{t|t}) - \\mathrm{tr}(\\mathbf{P}_{t|T})$ represents the total reduction in state variance (or MSE) achieved by incorporating future observations $\\{\\mathbf{y}_{t+1}, \\dots, \\mathbf{y}_T\\}$. It is therefore a natural scalar summary of the information gained from the backward pass. Using it as a stopping criterion, $\\mathrm{tr}(\\Delta \\mathbf{P}_t) \\le \\varepsilon$, formalizes the intuition that the backward recursion can be stopped when the information gain from the remaining future observations becomes negligible.\n\n**Task 2: Analysis of Truncation Bias**\n\nThe truncation bias is defined as $\\mathbf{b}_t \\triangleq \\mathbf{m}_{t|T} - \\mathbf{m}_{t|T}^{\\text{stop}}$. We are asked to show that $\\mathbb{E}[\\|\\mathbf{b}_t\\|_2^2] = \\mathrm{tr}(\\mathbf{P}_{t|T}^{\\text{stop}} - \\mathbf{P}_{t|T})$, where the expectation is over the joint distribution of states and observations.\n\nThe estimates $\\mathbf{m}_{t|T}$ and $\\mathbf{m}_{t|T}^{\\text{stop}}$ are conditional expectations of $\\mathbf{x}_t$ given different sets of observations. Let $\\mathcal{Y}^{\\text{full}} = \\mathbf{y}_{0:T}$ and $\\mathcal{Y}^{\\text{stop}}$ be the observation set used for the truncated smoother. The mean-squared norm of the difference between these two conditional expectations can be related to their respective conditional covariances. For jointly Gaussian random variables $(X, Y, Z)$, a standard result is that $\\mathbb{E}[ \\| \\mathbb{E}[X|Y,Z] - \\mathbb{E}[X|Y] \\|^2 ] = \\mathrm{tr}(\\mathrm{Cov}(X|Y) - \\mathrm{Cov}(X|Y,Z))$.\n\nIn our case, let $X = \\mathbf{x}_t$. The two sets of conditioning variables are $Y = \\mathcal{Y}^{\\text{stop}}$ and $\\{Y,Z\\} = \\mathcal{Y}^{\\text{full}}$. Since the state-space model is linear-Gaussian, the conditional covariances are independent of the specific values of the observations. We have:\n$$ \\mathrm{Cov}(\\mathbf{x}_t | \\mathcal{Y}^{\\text{stop}}) = \\mathbf{P}_{t|T}^{\\text{stop}} $$\n$$ \\mathrm{Cov}(\\mathbf{x}_t | \\mathcal{Y}^{\\text{full}}) = \\mathbf{P}_{t|T} $$\nApplying the identity, the mean-squared truncation bias becomes:\n$$ \\mathbb{E}[\\|\\mathbf{b}_t\\|_2^2] = \\mathbb{E}[\\|\\mathbf{m}_{t|T} - \\mathbf{m}_{t|T}^{\\text{stop}}\\|_2^2] = \\mathrm{tr}(\\mathrm{Cov}(\\mathbf{x}_t | \\mathcal{Y}^{\\text{stop}}) - \\mathrm{Cov}(\\mathbf{x}_t | \\mathcal{Y}^{\\text{full}})) = \\mathrm{tr}(\\mathbf{P}_{t|T}^{\\text{stop}} - \\mathbf{P}_{t|T}) $$\nThis proves the first required identity.\n\nNow, we consider the specific case where the recursion stops at $t^\\star$. According to the problem definition, for all $t \\le t^\\star$, the truncated smoother uses the filtered estimates: $\\mathbf{m}_{t|T}^{\\text{stop}} = \\mathbf{m}_{t|t}$ and $\\mathbf{P}_{t|T}^{\\text{stop}} = \\mathbf{P}_{t|t}$. The observation set for the filtered estimate is $\\mathcal{Y}^{\\text{stop}} = \\mathbf{y}_{0:t}$.\nFor $t \\le t^\\star$, substituting $\\mathbf{P}_{t|T}^{\\text{stop}} = \\mathbf{P}_{t|t}$ into the proven identity, we get:\n$$ \\mathbb{E}[\\|\\mathbf{b}_t\\|_2^2] = \\mathrm{tr}(\\mathbf{P}_{t|t} - \\mathbf{P}_{t|T}) $$\nBy definition, $\\Delta \\mathbf{P}_t = \\mathbf{P}_{t|t} - \\mathbf{P}_{t|T}$. Therefore, for all $t \\le t^\\star$:\n$$ \\mathbb{E}[\\|\\mathbf{b}_t\\|_2^2] = \\mathrm{tr}(\\Delta \\mathbf{P}_t) $$\nThis completes the proof. The aggregate bias $B(\\varepsilon)$ is the sum of these expected squared errors over the whole trajectory.\n\n### Part 2: Implementation Strategy\n\nThe implementation proceeds in four main steps:\n1.  **Data Simulation**: Generate a single realization of the state and observation trajectories $\\{\\mathbf{x}_t\\}_{t=0}^T$ and $\\{\\mathbf{y}_t\\}_{t=0}^T$ according to the specified model, parameters, and random seed.\n2.  **Kalman Filtering**: Run the standard Kalman filter forward in time from $t=0$ to $t=T$ to compute the sequences of filtered means $\\mathbf{m}_{t|t}$ and covariances $\\mathbf{P}_{t|t}$, as well as the predicted means $\\mathbf{m}_{t+1|t}$ and covariances $\\mathbf{P}_{t+1|t}$ required by the smoother.\n3.  **RTS Smoothing**: Run the standard RTS smoother backward in time from $t=T-1$ to $t=0$ to compute the sequences of fully smoothed means $\\mathbf{m}_{t|T}$ and covariances $\\mathbf{P}_{t|T}$.\n4.  **Bias Calculation**: For each specified threshold $\\varepsilon$:\n    a.  Determine the stopping time $t^\\star$. Based on the problem's descriptive text (\"full smoothing with no truncation\" for $\\varepsilon=0.0$), the correct interpretation is to find the first time from the backward pass, $t \\in \\{T-1, \\dots, 0\\}$, that satisfies the stopping criterion. Let this be $t^\\star$. If no such time exists, set $t^\\star = -1$.\n    b.  Calculate the aggregate bias $B(\\varepsilon) = \\sum_{t=0}^{T} \\mathrm{tr}(\\mathbf{P}_{t|T}^{\\text{stop}(\\varepsilon)} - \\mathbf{P}_{t|T})$. Based on our proof and the definition of the truncated smoother, this simplifies to $B(\\varepsilon) = \\sum_{t=0}^{t^\\star} \\mathrm{tr}(\\Delta \\mathbf{P}_t)$. This sum is zero if $t^\\star = -1$.\n\nThis computational procedure accurately implements the analysis requested in the problem.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Simulates a linear-Gaussian state-space model, runs a Kalman filter and RTS smoother,\n    and computes the aggregate mean-squared truncation bias for an adaptive smoother\n    based on a set of thresholds.\n    \"\"\"\n\n    # --- Model and Simulation Parameters ---\n    n = 2  # State dimension\n    p = 2  # Observation dimension\n    T = 60 # Terminal time index\n\n    A = np.array([[0.9, 0.3], [0.0, 0.7]])\n    H = np.array([[1.0, 0.0], [0.0, 1.0]])\n    Q = np.array([[0.05, 0.01], [0.01, 0.05]])\n    R = np.array([[0.10, 0.00], [0.00, 0.10]])\n    m0 = np.array([0.0, 0.0])\n    P0 = np.array([[1.0, 0.0], [0.0, 1.0]])\n    seed = 12345\n    \n    thresholds = [0.0, 1e-5, 1e-3, 100.0]\n\n    # --- 1. Data Simulation ---\n    rng = np.random.default_rng(seed)\n    \n    # State and observation arrays (T+1 steps from 0 to T)\n    states = np.zeros((T + 1, n))\n    observations = np.zeros((T + 1, p))\n    \n    # Draw initial state\n    states[0] = rng.multivariate_normal(m0, P0)\n    \n    # Generate state trajectory\n    for t in range(T):\n        w = rng.multivariate_normal(np.zeros(n), Q)\n        states[t+1] = A @ states[t] + w\n        \n    # Generate observation trajectory\n    for t in range(T + 1):\n        v = rng.multivariate_normal(np.zeros(p), R)\n        observations[t] = H @ states[t] + v\n        \n    # --- 2. Kalman Filter (Forward Pass) ---\n    m_pred = np.zeros((T + 1, n))\n    P_pred = np.zeros((T + 1, n, n))\n    m_filt = np.zeros((T + 1, n))\n    P_filt = np.zeros((T + 1, n, n))\n    \n    # Initialize with the prior for t=0\n    m_pred[0] = m0\n    P_pred[0] = P0\n    \n    # Run filter from t = 0 to T\n    for t in range(T + 1):\n        # Prediction step (for t>0, updates m_pred[t], P_pred[t])\n        if t > 0:\n            m_pred[t] = A @ m_filt[t-1]\n            P_pred[t] = A @ P_filt[t-1] @ A.T + Q\n        \n        # Update step (produces filtered estimate for time t)\n        S_t = H @ P_pred[t] @ H.T + R\n        S_t_inv = np.linalg.inv(S_t)\n        K_t = P_pred[t] @ H.T @ S_t_inv\n        \n        m_filt[t] = m_pred[t] + K_t @ (observations[t] - H @ m_pred[t])\n        P_filt[t] = (np.eye(n) - K_t @ H) @ P_pred[t]\n        \n    # --- 3. RTS Smoother (Backward Pass) ---\n    m_smooth = np.zeros((T + 1, n))\n    P_smooth = np.zeros((T + 1, n, n))\n    \n    # Initialize with the final filtered estimate\n    m_smooth[T] = m_filt[T]\n    P_smooth[T] = P_filt[T]\n    \n    # Run smoother backward from t = T-1 to 0\n    for t in range(T - 1, -1, -1):\n        # Smoother gain\n        G_t = P_filt[t] @ A.T @ np.linalg.inv(P_pred[t+1])\n        \n        # Smoother update\n        m_smooth[t] = m_filt[t] + G_t @ (m_smooth[t+1] - m_pred[t+1])\n        P_smooth[t] = P_filt[t] + G_t @ (P_smooth[t+1] - P_pred[t+1]) @ G_t.T\n        \n    # --- 4. Compute Aggregate Bias for each Threshold ---\n    results = []\n    \n    # Pre-compute backward information content for all t\n    delta_P = P_filt - P_smooth\n    trace_delta_P = np.array([np.trace(dp) for dp in delta_P])\n        \n    for eps in thresholds:\n        # Determine the stopping time t_star by searching backward\n        t_star = -1\n        for t in range(T - 1, -1, -1):\n            if trace_delta_P[t] = eps:\n                t_star = t\n                break\n        \n        # Calculate aggregate bias B(eps) = sum_{t=0 to t_star} tr(Delta_P_t)\n        # If t_star is -1, the sum is empty (0.0).\n        total_bias = 0.0\n        if t_star != -1:\n            total_bias = np.sum(trace_delta_P[:t_star + 1])\n        \n        results.append(total_bias)\n        \n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3393965"}]}