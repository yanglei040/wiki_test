{"hands_on_practices": [{"introduction": "A core component of the Primal-Dual Hybrid Gradient (PDHG) algorithm is the dual update, which requires evaluating the proximal operator of the convex conjugate function, $\\mathrm{prox}_{\\sigma g^*}$. Calculating this directly from the definition of $g^*$ can be cumbersome. This exercise [@problem_id:3413751] introduces a fundamental tool, the Moreau identity, which provides an elegant and practical way to compute this term by relating it back to the proximal operator of the original function $g$. Mastering this identity is crucial for efficiently implementing PDHG and other dual optimization methods.", "problem": "Consider the linear inverse problem with data model $z = Kx$ and a nonsmooth data-misfit potential $g(z) = \\alpha \\| z - y \\|_{2}$, where $K \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^{n}$, $z \\in \\mathbb{R}^{m}$, and $y \\in \\mathbb{R}^{m}$ is a fixed observation vector. In the Primal-Dual Hybrid Gradient (PDHG) method, the dual update requires evaluating the proximal operator of the convex conjugate $g^{*}$. Starting from the fundamental definitions of the convex conjugate and the proximal operator, derive an identity that expresses $\\mathrm{prox}_{\\sigma g^{*}}(q)$ in terms of a proximal operator of $g$ for any $\\sigma  0$ and $q \\in \\mathbb{R}^{m}$, and then use this identity to compute the dual update explicitly for the following instance:\n$\\alpha = 1$, $\\sigma = 1$, $y = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$, and $q = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}$.\n\nYour final answer must be the explicit value of $\\mathrm{prox}_{\\sigma g^{*}}(q)$ for the given numbers, expressed as a single row vector. No rounding is required.", "solution": "The problem statement has been validated and is deemed **valid**. It is scientifically grounded in the field of convex optimization, well-posed, and objective. It provides all necessary information for a unique solution.\n\nThe problem asks for two parts: first, to derive a general identity relating the proximal operator of a convex conjugate function $g^*$ to the proximal operator of the function $g$ itself; second, to apply this identity to a specific instance.\n\n**Part 1: Derivation of the Moreau Identity**\n\nWe are asked to derive an identity for $\\mathrm{prox}_{\\sigma g^{*}}(q)$. We start from the definition of the proximal operator:\n$$\n\\mathrm{prox}_{\\sigma g^{*}}(q) = \\arg\\min_{z \\in \\mathbb{R}^{m}} \\left\\{ \\frac{1}{2} \\| z - q \\|_{2}^{2} + \\sigma g^{*}(z) \\right\\}\n$$\nThe convex conjugate $g^*(z)$ is defined as:\n$$\ng^{*}(z) = \\sup_{x \\in \\mathbb{R}^{m}} \\left\\{ \\langle z, x \\rangle - g(x) \\right\\}\n$$\nSubstituting the definition of the conjugate into the proximal operator definition, we obtain a minimax problem:\n$$\n\\mathrm{prox}_{\\sigma g^{*}}(q) = \\arg\\min_{z} \\sup_{x} \\left\\{ \\frac{1}{2} \\| z - q \\|_{2}^{2} + \\sigma \\left( \\langle z, x \\rangle - g(x) \\right) \\right\\}\n$$\nLet the objective function be $L(z, x) = \\frac{1}{2} \\| z - q \\|_{2}^{2} + \\sigma \\langle z, x \\rangle - \\sigma g(x)$. The function $L(z,x)$ is convex in $z$ for any fixed $x$, and affine (thus concave) in $x$ for any fixed $z$. The domains are the entire spaces $\\mathbb{R}^m$. Therefore, by the Sion's Minimax Theorem, we can interchange the `min` and `sup` operators:\n$$\n\\sup_{x} \\min_{z} \\left\\{ \\frac{1}{2} \\| z - q \\|_{2}^{2} + \\sigma \\langle z, x \\rangle - \\sigma g(x) \\right\\}\n$$\nWe first solve the inner minimization problem with respect to $z$. The objective is a quadratic function of $z$. We find the minimum by setting the gradient with respect to $z$ to zero:\n$$\n\\nabla_{z} L(z, x) = (z - q) + \\sigma x = 0\n$$\nThis gives the unique minimizer $z^{*}(x) = q - \\sigma x$.\n\nNow, we substitute this optimal $z^{*}(x)$ back into the objective function to solve the outer maximization problem with respect to $x$:\n$$\n\\sup_{x} \\left\\{ \\frac{1}{2} \\| (q - \\sigma x) - q \\|_{2}^{2} + \\sigma \\langle q - \\sigma x, x \\rangle - \\sigma g(x) \\right\\}\n$$\n$$\n= \\sup_{x} \\left\\{ \\frac{1}{2} \\| -\\sigma x \\|_{2}^{2} + \\sigma \\langle q, x \\rangle - \\sigma^2 \\langle x, x \\rangle - \\sigma g(x) \\right\\}\n$$\n$$\n= \\sup_{x} \\left\\{ \\frac{\\sigma^2}{2} \\| x \\|_{2}^{2} + \\sigma \\langle q, x \\rangle - \\sigma^2 \\| x \\|_{2}^{2} - \\sigma g(x) \\right\\}\n$$\n$$\n= \\sup_{x} \\left\\{ \\sigma \\langle q, x \\rangle - \\frac{\\sigma^2}{2} \\| x \\|_{2}^{2} - \\sigma g(x) \\right\\}\n$$\nThe value that solves the minimax problem is the optimal $z^* = q - \\sigma x^*$, where $x^*$ is the argument that maximizes the expression above. Maximizing this expression is equivalent to minimizing its negative:\n$$\nx^{*} = \\arg\\min_{x} \\left\\{ -\\sigma \\langle q, x \\rangle + \\frac{\\sigma^2}{2} \\| x \\|_{2}^{2} + \\sigma g(x) \\right\\}\n$$\nDividing the objective by the positive constant $\\sigma$ does not change the minimizer:\n$$\nx^{*} = \\arg\\min_{x} \\left\\{ -\\langle q, x \\rangle + \\frac{\\sigma}{2} \\| x \\|_{2}^{2} + g(x) \\right\\}\n$$\nTo relate this to a proximal operator, we complete the square for the terms involving $x$:\n$$\n\\frac{\\sigma}{2} \\| x \\|_{2}^{2} - \\langle q, x \\rangle = \\frac{\\sigma}{2} \\left( \\| x \\|_{2}^{2} - 2\\langle x, \\frac{q}{\\sigma} \\rangle \\right) = \\frac{\\sigma}{2} \\left( \\left\\| x - \\frac{q}{\\sigma} \\right\\|_{2}^{2} - \\left\\| \\frac{q}{\\sigma} \\right\\|_{2}^{2} \\right)\n$$\nSubstituting this back into the minimization problem for $x^*$:\n$$\nx^{*} = \\arg\\min_{x} \\left\\{ \\frac{\\sigma}{2} \\left\\| x - \\frac{q}{\\sigma} \\right\\|_{2}^{2} - \\frac{\\sigma}{2} \\left\\| \\frac{q}{\\sigma} \\right\\|_{2}^{2} + g(x) \\right\\}\n$$\nThe term $-\\frac{\\sigma}{2} \\| q/\\sigma \\|_{2}^{2}$ is a constant with respect to $x$ and can be ignored. Multiplying the objective by $1/\\sigma$ also does not change the minimizer:\n$$\nx^{*} = \\arg\\min_{x} \\left\\{ \\frac{1}{2} \\left\\| x - \\frac{q}{\\sigma} \\right\\|_{2}^{2} + \\frac{1}{\\sigma} g(x) \\right\\}\n$$\nThis is precisely the definition of $\\mathrm{prox}_{g/\\sigma}(q/\\sigma)$. So, $x^{*} = \\mathrm{prox}_{g/\\sigma}(q/\\sigma)$.\nThe final result for the original desired quantity is $z^{*}(x^*)$:\n$$\n\\mathrm{prox}_{\\sigma g^{*}}(q) = q - \\sigma x^{*} = q - \\sigma \\mathrm{prox}_{g/\\sigma}(q/\\sigma)\n$$\nThis relationship is known as the Moreau identity (or Moreau's decomposition).\n\n**Part 2: Explicit Computation**\n\nWe are given the specific instance:\n$g(z) = \\alpha \\| z - y \\|_{2}$ with $\\alpha = 1$, so $g(z) = \\| z - y \\|_{2}$.\nThe parameters are $\\sigma = 1$, $y = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$, and $q = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}$.\n\nUsing the identity derived above with $\\sigma = 1$:\n$$\n\\mathrm{prox}_{g^{*}}(q) = q - 1 \\cdot \\mathrm{prox}_{g/1}(q/1) = q - \\mathrm{prox}_{g}(q)\n$$\nWe need to compute $\\mathrm{prox}_{g}(q)$:\n$$\n\\mathrm{prox}_{g}(q) = \\arg\\min_{z} \\left\\{ \\frac{1}{2} \\| z - q \\|_{2}^{2} + g(z) \\right\\} = \\arg\\min_{z} \\left\\{ \\frac{1}{2} \\| z - q \\|_{2}^{2} + \\| z - y \\|_{2} \\right\\}\n$$\nLet's make a change of variable $u = z - y$, which implies $z = u + y$. The minimization problem becomes:\n$$\n\\arg\\min_{u} \\left\\{ \\frac{1}{2} \\| (u + y) - q \\|_{2}^{2} + \\| u \\|_{2} \\right\\} = \\arg\\min_{u} \\left\\{ \\frac{1}{2} \\| u - (q - y) \\|_{2}^{2} + \\| u \\|_{2} \\right\\}\n$$\nThis is the definition of the proximal operator of the Euclidean norm, $\\mathrm{prox}_{\\|\\cdot\\|_2}(\\cdot)$, evaluated at the point $w = q-y$. The general solution for $\\mathrm{prox}_{\\lambda\\|\\cdot\\|_2}(w)$ is given by vector soft-thresholding:\n$$\n\\mathrm{prox}_{\\lambda\\|\\cdot\\|_2}(w) = \\max\\left(0, 1 - \\frac{\\lambda}{\\|w\\|_2}\\right) w = \\begin{cases} \\left(1 - \\frac{\\lambda}{\\|w\\|_2}\\right) w  \\text{if } \\|w\\|_2  \\lambda \\\\ 0  \\text{if } \\|w\\|_2 \\le \\lambda \\end{cases}\n$$\nIn our case, the coefficient of the norm is $1$, so $\\lambda=1$. Let's compute $w = q - y$:\n$$\nw = q - y = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix}\n$$\nThe Euclidean norm of $w$ is:\n$$\n\\| w \\|_{2} = \\sqrt{0^2 + 2^2} = 2\n$$\nSince $\\|w\\|_2 = 2  1 = \\lambda$, we use the first case of the formula. The optimal $u$ is:\n$$\nu_{\\mathrm{opt}} = \\mathrm{prox}_{\\|\\cdot\\|_2}(w) = \\left(1 - \\frac{1}{\\|w\\|_2}\\right) w = \\left(1 - \\frac{1}{2}\\right) w = \\frac{1}{2} w = \\frac{1}{2} \\begin{pmatrix} 0 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\nThe minimizer of the original problem in $z$ is $z_{\\mathrm{opt}} = u_{\\mathrm{opt}} + y$. Thus,\n$$\n\\mathrm{prox}_{g}(q) = z_{\\mathrm{opt}} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix}\n$$\nFinally, we can compute the desired dual update term:\n$$\n\\mathrm{prox}_{g^{*}}(q) = q - \\mathrm{prox}_{g}(q) = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 1-1 \\\\ 4-3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} 0  1 \\end{pmatrix}}\n$$", "id": "3413751"}, {"introduction": "Once we understand how to compute the individual steps of the PDHG algorithm, we must ensure the entire iteration converges. The algorithm's stability hinges on a critical condition relating the primal step size $\\tau$, the dual step size $\\sigma$, and the operator norm $\\|K\\|$: $\\tau\\sigma\\|K\\|^2 \\lt 1$. This practice [@problem_id:3413760] provides a hands-on coding exercise to implement the power iteration method, a standard and efficient technique for estimating the operator norm using only matrix-vector products. This allows you to select step sizes that provably satisfy the stability condition, a necessary step for any robust implementation.", "problem": "Consider a linear forward operator represented by a real matrix $K \\in \\mathbb{R}^{m \\times n}$. In Primal-Dual Hybrid Gradient (PDHG) methods for inverse problems and data assimilation, the primal step size $\\tau$ and the dual step size $\\sigma$ must satisfy the stability condition $\\tau \\sigma \\|K\\|^2  1$, where $\\|K\\|$ denotes the operator norm induced by the Euclidean norm, i.e., the spectral norm. Starting from the definition $\\|K\\| = \\sup_{\\|x\\|_2 = 1} \\|Kx\\|_2$ and the relationship between $\\|K\\|$ and the largest singular value of $K$, derive a procedure that uses only matrix-vector products with $K$ and $K^\\top$ to estimate $\\|K\\|$ for general (possibly rectangular) matrices. Use this estimate to select $\\tau$ and $\\sigma$ so that $\\tau \\sigma \\|K\\|^2  1$ holds strictly, with a tunable safety factor $s \\in (0,1)$ that controls how close $\\tau \\sigma \\|K\\|^2$ is to $1$. To ensure well-defined behavior in degenerate cases, adopt the convention that if the estimate of $\\|K\\|$ is numerically zero within a tolerance, then set $\\tau = 1$ and $\\sigma = 1$.\n\nYour task is to implement a complete program that:\n- Initializes a unit vector $v_0 \\in \\mathbb{R}^n$ deterministically as the normalized all-ones vector.\n- Applies a power iteration to the symmetric positive semidefinite matrix $K^\\top K$ using only the operations $v \\mapsto Kv$ and $u \\mapsto K^\\top u$, which estimates the largest eigenvalue of $K^\\top K$ and hence $\\|K\\| = \\sqrt{\\lambda_{\\max}(K^\\top K)}$.\n- Uses the estimate $\\widehat{\\|K\\|}$ to choose $\\tau$ and $\\sigma$ according to a rule that guarantees $\\tau \\sigma \\widehat{\\|K\\|}^2  1$ with the provided safety factor $s \\in (0,1)$, and the special-case rule $\\tau = \\sigma = 1$ if $\\widehat{\\|K\\|}$ is numerically zero.\n- Reports, for each test case, a list containing five entries: the estimated spectral norm $\\widehat{\\|K\\|}$ as a float, the chosen $\\tau$ as a float, the chosen $\\sigma$ as a float, the product $\\tau \\sigma \\widehat{\\|K\\|}^2$ as a float, and a boolean indicating whether $\\tau \\sigma \\widehat{\\|K\\|}^2  1$ is satisfied strictly.\n\nUse the following test suite, which covers a variety of matrix shapes and conditioning scenarios:\n- Test case $ 1 $ (happy path, rectangular): $K_1 = \\begin{bmatrix} 3  0  0 \\\\ 0  1  2 \\end{bmatrix}$, $s = 0.9$.\n- Test case $ 2 $ (boundary-near case, square identity): $K_2 = I_4$ with $I_4$ the $4 \\times 4$ identity, $s = 0.99$.\n- Test case $ 3 $ (degenerate zero operator, rectangular): $K_3 = \\begin{bmatrix} 0  0 \\\\ 0  0 \\\\ 0  0 \\end{bmatrix}$, $s = 0.5$.\n- Test case $ 4 $ (ill-conditioned scales, rectangular): $K_4 = \\begin{bmatrix} 10^{-3}  0  0 \\\\ 0  10  0 \\\\ 0  0  10^{-1} \\\\ 0  0  0 \\end{bmatrix}$, $s = 0.95$.\n\nImplementation requirements:\n- Use a deterministic initialization $v_0 = \\frac{1}{\\sqrt{n}} \\mathbf{1}_n$.\n- Use a relative change tolerance $10^{-12}$ and a maximum of $10000$ iterations for the power iteration.\n- The selection rule for $\\tau$ and $\\sigma$ must be $\\tau = \\frac{s}{\\widehat{\\|K\\|}}$ and $\\sigma = \\frac{s}{\\widehat{\\|K\\|}}$ whenever $\\widehat{\\|K\\|}  0$, and $\\tau = 1$, $\\sigma = 1$ otherwise.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list formatted as $[\\widehat{\\|K\\|}, \\tau, \\sigma, \\tau \\sigma \\widehat{\\|K\\|}^2, \\text{boolean}]$. For example, the overall output should look like $[[\\dots],[\\dots],[\\dots],[\\dots]]$ with no additional text.", "solution": "The problem requires the development of a procedure to select the primal step size $ \\tau $ and dual step size $ \\sigma $ for Primal-Dual Hybrid Gradient (PDHG) methods. The selection must satisfy the stability condition $ \\tau \\sigma \\|K\\|^2  1 $, where $ K \\in \\mathbb{R}^{m \\times n} $ is the linear forward operator and $ \\|K\\| $ is its spectral norm. The core of the task is to estimate $ \\|K\\| $ using an iterative numerical method that relies only on matrix-vector products involving $ K $ and its transpose $ K^\\top $.\n\n### Theoretical Foundation: Spectral Norm and Eigenvalues\n\nThe spectral norm of a matrix $ K $, denoted $ \\|K\\| $ or $ \\|K\\|_2 $, is defined as the largest factor by which $ K $ can stretch a vector $ x \\in \\mathbb{R}^n $:\n$$ \\|K\\| = \\sup_{\\|x\\|_2 = 1} \\|Kx\\|_2 $$\nwhere $ \\| \\cdot \\|_2 $ is the Euclidean norm. An equivalent and more computationally useful characterization relates the spectral norm to the singular values of $ K $. The spectral norm is precisely the largest singular value of $ K $, denoted $ \\sigma_{\\max}(K) $.\n\nThe singular values of $ K $ are, by definition, the square roots of the eigenvalues of the symmetric positive semidefinite matrix $ K^\\top K $. Let the eigenvalues of $ K^\\top K $ be $ \\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0 $. Then the singular values of $ K $ are $ \\sigma_i = \\sqrt{\\lambda_i} $. Consequently, the largest singular value corresponds to the largest eigenvalue:\n$$ \\|K\\| = \\sigma_{\\max}(K) = \\sqrt{\\lambda_{\\max}(K^\\top K)} $$\nThis relationship transforms the problem of finding the spectral norm of $ K $ into the problem of finding the largest eigenvalue of the matrix $ A = K^\\top K $.\n\n### Algorithmic Approach: The Power Method\n\nThe power method, or power iteration, is a standard algorithm for finding the eigenvalue with the largest magnitude (the dominant eigenvalue) of a matrix. For the matrix $ A = K^\\top K $, which is symmetric and positive semidefinite, all its eigenvalues are real and non-negative. Therefore, the dominant eigenvalue is simply the largest eigenvalue, $ \\lambda_{\\max}(A) $.\n\nThe power iteration algorithm proceeds as follows:\n1.  Initialize a vector $ v_0 \\in \\mathbb{R}^n $ with $ \\|v_0\\|_2 = 1 $. The problem specifies a deterministic initialization: $ v_0 = \\frac{1}{\\sqrt{n}}\\mathbf{1}_n $, where $ \\mathbf{1}_n $ is the vector of all ones.\n2.  Iterate for $ k = 0, 1, 2, \\dots $:\n    $$ v_{k+1} = \\frac{A v_k}{\\|A v_k\\|_2} $$\n    As $ k \\to \\infty $, the vector $ v_k $ converges to the eigenvector corresponding to $ \\lambda_{\\max}(A) $, provided that $ \\lambda_{\\max} $ is strictly greater than all other eigenvalues in magnitude and the initial vector $ v_0 $ has a non-zero component in the direction of this eigenvector.\n\nThe eigenvalue $ \\lambda_{\\max}(A) $ can be estimated at each step. One common estimate is the Rayleigh quotient $ \\lambda^{(k)} = v_k^\\top A v_k $. A simpler estimate, which arises directly from the iteration, is $ \\lambda^{(k+1)} = \\|A v_k\\|_2 $. As $ v_k $ approaches the true eigenvector, $ A v_k \\approx \\lambda_{\\max} v_k $, so $ \\|A v_k\\|_2 \\approx |\\lambda_{\\max}| \\|v_k\\|_2 = \\lambda_{\\max} $.\n\nA crucial requirement of the problem is to use only multiplications by $ K $ and $ K^\\top $, without explicitly forming the matrix $ A = K^\\top K $, which can be computationally expensive or infeasible for large-scale problems. The matrix-vector product $ A v_k $ is computed as a sequence of two operations:\n1.  Compute the vector $ u_k = K v_k $.\n2.  Compute the vector $ w_{k+1} = K^\\top u_k = K^\\top (K v_k) $.\n\nThe full iteration step is then:\n$ w_{k+1} = K^\\top K v_k $\n$ \\lambda_{k+1} = \\|w_{k+1}\\|_2 $ (This is our estimate of $ \\lambda_{\\max} $)\n$ v_{k+1} = \\frac{w_{k+1}}{\\lambda_{k+1}} $ (Normalization for the next step)\n\nThe loop terminates when the relative change in the eigenvalue estimate falls below a specified tolerance $ \\epsilon = 10^{-12} $, i.e., $ \\frac{|\\lambda_{k+1} - \\lambda_k|}{|\\lambda_{k+1}|}  \\epsilon $, or when a maximum number of iterations ($10000$) is reached.\n\n### Procedure for Step Size Selection\n\nThe complete procedure to estimate the spectral norm and select the step sizes $ \\tau $ and $ \\sigma $ is as follows:\n\n1.  **Estimate $ \\lambda_{\\max}(K^\\top K) $**:\n    -   Initialize $ v_0 = \\frac{1}{\\sqrt{n}}\\mathbf{1}_n $ and an initial eigenvalue estimate, e.g., $ \\lambda_0 = 0 $.\n    -   Iteratively compute $ v_{k+1} $ and $ \\lambda_{k+1} $ as described above until convergence. Let the final estimate be $ \\hat{\\lambda}_{\\max} $.\n\n2.  **Estimate $ \\|K\\| $**:\n    -   The spectral norm is estimated as $ \\widehat{\\|K\\|} = \\sqrt{\\hat{\\lambda}_{\\max}} $.\n\n3.  **Select $ \\tau $ and $ \\sigma $**:\n    -   A safety factor $ s \\in (0,1) $ is introduced to ensure the stability condition holds strictly.\n    -   **Case 1: $ \\widehat{\\|K\\|}  0 $** (the typical case). The step sizes are chosen symmetrically:\n        $$ \\tau = \\frac{s}{\\widehat{\\|K\\|}} \\quad \\text{and} \\quad \\sigma = \\frac{s}{\\widehat{\\|K\\|}} $$\n        With this choice, the stability expression becomes:\n        $$ \\tau \\sigma \\widehat{\\|K\\|}^2 = \\left(\\frac{s}{\\widehat{\\|K\\|}}\\right) \\left(\\frac{s}{\\widehat{\\|K\\|}}\\right) \\widehat{\\|K\\|}^2 = s^2 $$\n        Since $ s \\in (0,1) $, it is guaranteed that $ s^2  1 $, thus satisfying the condition $ \\tau \\sigma \\widehat{\\|K\\|}^2  1 $.\n    -   **Case 2: $ \\widehat{\\|K\\|} $ is numerically zero**. This occurs if $ K $ is the zero matrix. The problem specifies the convention:\n        $$ \\tau = 1 \\quad \\text{and} \\quad \\sigma = 1 $$\n        In this case, the product $ \\tau \\sigma \\widehat{\\|K\\|}^2 = 1 \\cdot 1 \\cdot 0^2 = 0 $, which also satisfies the stability condition $ 0  1 $.\n\nThis procedure provides a robust and standard way to configure the step sizes for PDHG algorithms, ensuring stability while using only fundamental matrix operations. The final implementation will apply this complete logic to each of the provided test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of selecting PDHG step sizes for given test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([[3.0, 0.0, 0.0], [0.0, 1.0, 2.0]]), 0.9),\n        (np.identity(4), 0.99),\n        (np.zeros((3, 2)), 0.5),\n        (np.array([[1e-3, 0.0, 0.0], [0.0, 10.0, 0.0], [0.0, 0.0, 1e-1], [0.0, 0.0, 0.0]]), 0.95),\n    ]\n\n    results = []\n    \n    # Define constants for the power iteration\n    max_iter = 10000\n    rel_tol = 1e-12\n    # Tolerance for checking if a value is numerically zero\n    zero_tol = 1e-14 \n\n    for K, s in test_cases:\n        m, n = K.shape\n\n        # Initialize the vector v0 deterministically.\n        if n  0:\n            v = np.ones(n) / np.sqrt(n)\n        else: # Handle zero-column matrix\n            v = np.array([])\n            \n        lambda_est = 0.0\n\n        # Power iteration to find the largest eigenvalue of K^T * K\n        # If K is the zero matrix, K.T @ K @ v will be zero, lambda_est will remain 0.\n        if n  0:\n            for _ in range(max_iter):\n                # Apply operator K^T * K without forming the matrix explicitly\n                K_v = K @ v\n                Kt_K_v = K.T @ K_v\n    \n                lambda_new = np.linalg.norm(Kt_K_v)\n                \n                # Check for convergence\n                if lambda_new  zero_tol:\n                    lambda_est = 0.0\n                    break\n                \n                # Normalize the vector for the next iteration\n                v = Kt_K_v / lambda_new\n    \n                # Relative change stopping criterion\n                relative_change = abs(lambda_new - lambda_est) / lambda_new\n                lambda_est = lambda_new\n    \n                if relative_change  rel_tol:\n                    break\n        \n        # Estimate the spectral norm\n        norm_K_est = np.sqrt(lambda_est)\n\n        # Select tau and sigma based on the estimated norm\n        if norm_K_est  zero_tol:\n            tau = s / norm_K_est\n            sigma = s / norm_K_est\n        else:\n            # Degenerate case rule: if norm is numerically zero\n            norm_K_est = 0.0 # Clean up tiny numerical noise\n            tau = 1.0\n            sigma = 1.0\n\n        # Calculate the stability product and check the condition\n        product = tau * sigma * norm_K_est**2\n        is_stable = product  1.0\n\n        # Append the list of results for this test case\n        results.append([norm_K_est, tau, sigma, product, is_stable])\n\n    # Final print statement in the exact required format.\n    # The string representation of a Python list includes spaces, which is\n    # consistent with the provided template's use of map(str, ...).\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3413760"}, {"introduction": "Moving from a basic implementation to one suitable for large-scale data assimilation requires exploiting problem structure for computational efficiency. In many applications, observations are collected from independent sources, leading to a separable data-misfit function $g$. This structural property can be leveraged to parallelize the algorithm. This practice [@problem_id:3413769] challenges you to analyze different implementation strategies and identify the correct, convergence-safe method for parallelizing the dual update, transforming a computationally intensive step into an embarrassingly parallel one.", "problem": "Consider a convex variational data assimilation problem formulated as minimizing a sum of a background or model-consistency term and a data-misfit term of the form\n$$\\min_{x \\in \\mathbb{R}^{n}} \\; f(x) + g(Kx),$$\nwhere $f:\\mathbb{R}^{n}\\to \\mathbb{R}\\cup\\{+\\infty\\}$ and $g:\\mathbb{R}^{m}\\to \\mathbb{R}\\cup\\{+\\infty\\}$ are proper, convex, and lower semicontinuous, and $K \\in \\mathbb{R}^{m\\times n}$ is a linear operator arising from the observation operator and possibly model linearization. Suppose that observations are grouped into $J$ blocks (for example, by sensor or time), inducing a block-row structure $K = \\begin{bmatrix} K_{1} \\\\ \\vdots \\\\ K_{J} \\end{bmatrix}$ and a corresponding decomposition of $z = Kx$ into blocks $z = (z_{1},\\dots,z_{J})$ with $z_{j} = K_{j}x \\in \\mathbb{R}^{m_{j}}$ and $\\sum_{j=1}^{J} m_{j} = m$. Assume the data-misfit is separable across observation blocks, that is,\n$$g(z) \\;=\\; \\sum_{j=1}^{J} g_{j}(z_{j}), \\qquad z = (z_{1},\\dots,z_{J}).$$\n\nThe Primal-Dual Hybrid Gradient (PDHG) method with diagonal steps uses dual and primal updates\n$$y^{k+1} \\;=\\; \\operatorname{prox}_{\\Sigma g^{\\ast}}\\!\\left(y^{k} + \\Sigma K \\bar{x}^{k}\\right), \\qquad x^{k+1} \\;=\\; \\operatorname{prox}_{T f}\\!\\left(x^{k} - T K^{\\top} y^{k+1}\\right),$$\nwith extrapolation $\\bar{x}^{k} = x^{k} + \\theta(x^{k} - x^{k-1})$, diagonal step-size operators $\\Sigma \\succeq 0$ and $T \\succeq 0$, and $g^{\\ast}$ the Fenchel conjugate of $g$. In a high-dimensional setting with many observations, $K$ is typically very sparse, and the blocks $\\{K_{j}\\}_{j=1}^{J}$ have small supports corresponding to localized or temporally isolated measurements.\n\nWhich of the following best describes a correct and convergence-safe strategy to exploit the sparsity and block structure of $K$ to parallelize the dual update across observation blocks in data assimilation?\n\nA. Partition $K$ by block rows as $K = \\begin{bmatrix} K_{1} \\\\ \\cdots \\\\ K_{J} \\end{bmatrix}$ aligned with the separable decomposition $g(z) = \\sum_{j=1}^{J} g_{j}(z_{j})$. Use a block-diagonal dual step-size $\\Sigma = \\operatorname{diag}(\\sigma_{1} I_{m_{1}},\\dots,\\sigma_{J} I_{m_{J}})$ and a primal step-size $T = \\tau I_{n}$ chosen to satisfy the standard PDHG condition $\\lVert \\Sigma^{1/2} K T^{1/2} \\rVert  1$ (for example via diagonal preconditioning based on block norms). Then compute, in parallel for all $j$, the block residuals $r_{j}^{k} = K_{j}\\bar{x}^{k}$ and the dual block updates $y_{j}^{k+1} = \\operatorname{prox}_{\\sigma_{j} g_{j}^{\\ast}}(y_{j}^{k} + \\sigma_{j} r_{j}^{k})$. Finally, form $K^{\\top} y^{k+1} = \\sum_{j=1}^{J} K_{j}^{\\top} y_{j}^{k+1}$ by a reduction and perform the primal update. This yields embarrassingly parallel dual updates with one global reduction per iteration.\n\nB. Partition $K$ by columns and parallelize the primal step by computing local $x$-updates independently while keeping a single monolithic dual vector $y$. The dual update must remain centralized because $g$ depends on all observations, so attempting to separate $y$ would violate convexity and is unnecessary for convergence.\n\nC. Replace the dual proximal operator by solving the normal equations for $y$ in each iteration using a Jacobi method applied to $K K^{\\top}$, since convexity implies equivalence between proximal ascent and solving the associated optimality conditions. This enables parallelism without needing block separability of $g$.\n\nD. Split the state $x$ into overlapping spatial subdomains and update both $x$ and all dual blocks $y_{j}$ fully asynchronously with no synchronization or global reduction, because the sparsity of $K$ implies $K^{\\top} y$ can be approximated locally without affecting convergence guarantees.\n\nE. Replace $K$ by a block-diagonal approximation $\\tilde{K} = \\operatorname{blkdiag}(K_{1},\\dots,K_{J})$ constructed via gossip among observers to eliminate cross-block couplings and then perform centralized PDHG on $(f,g,\\tilde{K})$. The loss of cross-block terms is negligible in practice and does not impact convergence to the original solution.", "solution": "The user requires a critical validation of the problem statement, followed by a derivation of the correct solution and an evaluation of all provided options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem is a convex optimization problem of the form:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; f(x) + g(Kx)\n$$\n- **Functions**: $f:\\mathbb{R}^{n}\\to \\mathbb{R}\\cup\\{+\\infty\\}$ and $g:\\mathbb{R}^{m}\\to \\mathbb{R}\\cup\\{+\\infty\\}$ are proper, convex, and lower semicontinuous.\n- **Operator**: $K \\in \\mathbb{R}^{m\\times n}$ is a linear operator.\n- **Block Structure**: The operator $K$ has a block-row structure $K = \\begin{bmatrix} K_{1} \\\\ \\vdots \\\\ K_{J} \\end{bmatrix}$, where $K_j \\in \\mathbb{R}^{m_j \\times n}$ and $\\sum_{j=1}^{J} m_{j} = m$. This induces a block structure on $z=Kx$ as $z = (z_1, \\dots, z_J)$ with $z_j = K_j x$.\n- **Separability**: The function $g$ is separable with respect to this block structure: $g(z) = \\sum_{j=1}^{J} g_{j}(z_{j})$.\n- **Algorithm**: The Primal-Dual Hybrid Gradient (PDHG) method is considered, with updates:\n  $$\n  y^{k+1} \\;=\\; \\operatorname{prox}_{\\Sigma g^{\\ast}}\\!\\left(y^{k} + \\Sigma K \\bar{x}^{k}\\right)\n  $$\n  $$\n  x^{k+1} \\;=\\; \\operatorname{prox}_{T f}\\!\\left(x^{k} - T K^{\\top} y^{k+1}\\right)\n  $$\n- **Algorithm Parameters**:\n    - $\\bar{x}^{k} = x^{k} + \\theta(x^{k} - x^{k-1})$ is an extrapolation step ($\\theta=1$ for the standard algorithm).\n    - $\\Sigma \\succeq 0$ and $T \\succeq 0$ are diagonal step-size operators.\n    - $g^{\\ast}$ is the Fenchel conjugate of $g$.\n- **Context**: The setting is high-dimensional data assimilation with a sparse operator $K$.\n- **Question**: The task is to identify a correct and convergence-safe strategy to parallelize the dual update across the $J$ observation blocks.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded**: The problem formulation is standard in the fields of inverse problems, data assimilation, and machine learning. The use of PDHG (also known as the Chambolle-Pock algorithm) to solve such problems is a cornerstone of modern convex optimization. The properties of the functions ($f, g$) and the operator ($K$) are the standard assumptions under which convergence of the algorithm is guaranteed. The problem is firmly grounded in mathematical optimization theory.\n2.  **Well-Posed**: The problem statement is well-posed. It describes a standard optimization problem and a standard algorithm to solve it. The question asks for a valid parallelization strategy, which is a well-defined computer science and numerical analysis question within this context.\n3.  **Objective**: The language is precise and objective. All terms like \"proper, convex, lower semicontinuous\", \"Fenchel conjugate\", and \"proximal operator\" have rigorous mathematical definitions.\n4.  **Completeness**: The problem provides all necessary information. The structure of the problem ($\\min f(x)+\\sum_j g_j(K_j x)$) and the form of the PDHG algorithm are explicitly stated, which is sufficient to analyze parallelization strategies.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. It is scientifically sound, well-posed, and complete. I will now proceed to derive the solution and evaluate the options.\n\n### Solution Derivation\n\nThe core of the problem lies in the structure of the dual update:\n$$\ny^{k+1} = \\operatorname{prox}_{\\Sigma g^{\\ast}}\\!\\left(y^{k} + \\Sigma K \\bar{x}^{k}\\right)\n$$\nThe possibility of parallelizing this update hinges on the separability of the function $g$ and a corresponding structure for the step-size operator $\\Sigma$.\n\nThe problem states that $g(z) = \\sum_{j=1}^{J} g_j(z_j)$, where $z = (z_1, \\dots, z_J)$ is the block decomposition of $z$. A fundamental property of the Fenchel conjugate is that for a separable function, its conjugate is also separable:\n$$\ng^{\\ast}(y) = \\left(\\sum_{j=1}^{J} g_j\\right)^{\\ast}(y) = \\sum_{j=1}^{J} g_j^{\\ast}(y_j),\n$$\nwhere $y = (y_1, \\dots, y_J)$ is the corresponding block decomposition of the dual variable.\n\nThe proximal operator is defined as:\n$$\n\\operatorname{prox}_{\\Sigma g^{\\ast}}(v) = \\arg\\min_{y} \\left\\{ g^{\\ast}(y) + \\frac{1}{2} \\| y - v \\|_{\\Sigma^{-1}}^2 \\right\\}\n$$\nLet's choose a block-diagonal dual step-size operator $\\Sigma$ conformally with the block structure of $y$:\n$$\n\\Sigma = \\begin{pmatrix} \\Sigma_1  0  \\cdots  0 \\\\ 0  \\Sigma_2  \\cdots  0 \\\\ \\vdots  \\vdots  \\ddots  \\vdots \\\\ 0  0  \\cdots  \\Sigma_J \\end{pmatrix},\n$$\nwhere $\\Sigma_j$ is a positive semidefinite operator on $\\mathbb{R}^{m_j}$. With this choice, the quadratic penalty term also becomes separable:\n$$\n\\| y - v \\|_{\\Sigma^{-1}}^2 = (y-v)^\\top \\Sigma^{-1} (y-v) = \\sum_{j=1}^{J} (y_j - v_j)^\\top \\Sigma_j^{-1} (y_j - v_j) = \\sum_{j=1}^{J} \\| y_j - v_j \\|_{\\Sigma_j^{-1}}^2.\n$$\nThe minimization problem in the proximal operator thus decouples into $J$ independent subproblems:\n$$\n\\arg\\min_{y} \\left\\{ \\sum_{j=1}^{J} g_j^{\\ast}(y_j) + \\sum_{j=1}^{J} \\frac{1}{2} \\| y_j - v_j \\|_{\\Sigma_j^{-1}}^2 \\right\\} = \\sum_{j=1}^{J} \\arg\\min_{y_j} \\left\\{ g_j^{\\ast}(y_j) + \\frac{1}{2} \\| y_j - v_j \\|_{\\Sigma_j^{-1}}^2 \\right\\}.\n$$\nThis means that the $j$-th block of the updated dual variable, $y_j^{k+1}$, can be computed as:\n$$\ny_j^{k+1} = \\operatorname{prox}_{\\Sigma_j g_j^{\\ast}}(v_j),\n$$\nwhere $v_j$ is the $j$-th block of the argument $v = y^k + \\Sigma K \\bar{x}^k$. The block $v_j$ is given by:\n$$\nv_j = y_j^k + (\\Sigma K \\bar{x}^k)_j = y_j^k + \\Sigma_j K_j \\bar{x}^k.\n$$\nTherefore, the dual update can be performed in parallel for each block $j=1, \\dots, J$:\n$$\ny_j^{k+1} = \\operatorname{prox}_{\\Sigma_j g_j^{\\ast}}\\!\\left(y_j^{k} + \\Sigma_j K_j \\bar{x}^k\\right).\n$$\nThis is an embarrassingly parallel computation, provided that the full vector $\\bar{x}^k$ is available to all processing units.\n\nNow, consider the primal update:\n$$\nx^{k+1} = \\operatorname{prox}_{T f}\\!\\left(x^{k} - T K^{\\top} y^{k+1}\\right).\n$$\nThe critical term is $K^\\top y^{k+1}$. Given the block structures of $K$ and $y$, this matrix-vector product expands to:\n$$\nK^\\top y^{k+1} = \\begin{bmatrix} K_1^\\top  K_2^\\top  \\cdots  K_J^\\top \\end{bmatrix} \\begin{bmatrix} y_1^{k+1} \\\\ y_2^{k+1} \\\\ \\vdots \\\\ y_J^{k+1} \\end{bmatrix} = \\sum_{j=1}^{J} K_j^\\top y_j^{k+1}.\n$$\nThis operation requires a synchronization step: after the parallel computation of all $y_j^{k+1}$, the products $K_j^\\top y_j^{k+1}$ must be computed (which can also be done in parallel) and then summed together. This is a classic map-reduce or reduction operation. The resulting sum is then used in the single, centralized primal update for $x^{k+1}$.\n\nThe convergence of the PDHG method with matrix step-sizes $\\Sigma, T$ is guaranteed if the primal and dual step-sizes are chosen such that the operator norm $\\|\\Sigma^{1/2} K T^{1/2}\\|  1$. This is the standard convergence condition.\n\n### Option-by-Option Analysis\n\n**A. Partition $K$ by block rows as $K = \\begin{bmatrix} K_{1} \\\\ \\dots \\\\ K_{J} \\end{bmatrix}$ aligned with the separable decomposition $g(z) = \\sum_{j=1}^{J} g_{j}(z_{j})$. Use a block-diagonal dual step-size $\\Sigma = \\operatorname{diag}(\\sigma_{1} I_{m_{1}},\\dots,\\sigma_{J} I_{m_{J}})$ and a primal step-size $T = \\tau I_{n}$ chosen to satisfy the standard PDHG condition $\\lVert \\Sigma^{1/2} K T^{1/2} \\rVert  1$ (for example via diagonal preconditioning based on block norms). Then compute, in parallel for all $j$, the block residuals $r_{j}^{k} = K_{j}\\bar{x}^{k}$ and the dual block updates $y_{j}^{k+1} = \\operatorname{prox}_{\\sigma_{j} g_{j}^{\\ast}}(y_{j}^{k} + \\sigma_{j} r_{j}^{k})$. Finally, form $K^{\\top} y^{k+1} = \\sum_{j=1}^{J} K_{j}^{\\top} y_{j}^{k+1}$ by a reduction and perform the primal update. This yields embarrassingly parallel dual updates with one global reduction per iteration.**\n\nThis option precisely matches the derivation above. It correctly identifies that block-row partitioning of $K$ combined with the separability of $g$ and a block-diagonal step-size $\\Sigma$ decouples the dual update. The specific form $\\Sigma_j = \\sigma_j I_{m_j}$ is a common practical choice. The dual update formula $y_{j}^{k+1} = \\operatorname{prox}_{\\sigma_{j} g_{j}^{\\ast}}(y_{j}^{k} + \\sigma_{j} K_j \\bar{x}^{k})$ is correct. The correct form of the reduction step for the primal update, $K^{\\top} y^{k+1} = \\sum_{j=1}^{J} K_{j}^{\\top} y_{j}^{k+1}$, is also stated. The cited convergence condition is the correct one for this setting. The description of the computational pattern as \"embarrassingly parallel dual updates with one global reduction\" is accurate.\n**Verdict: Correct**\n\n**B. Partition $K$ by columns and parallelize the primal step by computing local $x$-updates independently while keeping a single monolithic dual vector $y$. The dual update must remain centralized because $g$ depends on all observations, so attempting to separate $y$ would violate convexity and is unnecessary for convergence.**\n\nThis option is flawed on multiple grounds. First, it proposes parallelizing the primal step by partitioning $K$ by columns, which is a different strategy (a form of primal decomposition). Second, the justification is incorrect. The problem explicitly states that $g$ is separable ($g(z) = \\sum_j g_j(z_j)$), which contradicts the claim that it \"depends on all observations\" in a non-separable way. This separability is precisely what allows for a parallel dual update. The claim that separating $y$ \"would violate convexity\" is baseless; the separability of $g^*$ is a direct consequence of the convexity and separability of $g$.\n**Verdict: Incorrect**\n\n**C. Replace the dual proximal operator by solving the normal equations for $y$ in each iteration using a Jacobi method applied to $K K^{\\top}$, since convexity implies equivalence between proximal ascent and solving the associated optimality conditions. This enables parallelism without needing block separability of $g$.**\n\nThis option is incorrect. Replacing the proximal operator with a linear \"normal equations\" solve is only valid if $g$ (and thus $g^*$) is a quadratic function (specifically, an $\\ell_2$-norm squared). For a general convex function $g$, the proximal operator is a non-linear map. The statement that \"convexity implies equivalence\" is a gross oversimplification; while the prox solves an optimality condition, this condition is generally not a linear system. Furthermore, it incorrectly suggests this works without separability of $g$. If $g$ is not separable, its proximal operator is not separable, and parallelization of the dual update is not straightforward, regardless of the method used to compute the prox.\n**Verdict: Incorrect**\n\n**D. Split the state $x$ into overlapping spatial subdomains and update both $x$ and all dual blocks $y_{j}$ fully asynchronously with no synchronization or global reduction, because the sparsity of $K$ implies $K^{\\top} y$ can be approximated locally without affecting convergence guarantees.**\n\nThis option proposes a fully asynchronous scheme and makes strong, generally false claims. While asynchronous optimization algorithms exist, they come with strict theoretical requirements and often different convergence guarantees than their synchronous counterparts. The claim that $K^\\top y$ can be \"approximated locally without affecting convergence guarantees\" is false. Sparsity of $K$ means that any component of the sum $K^\\top y = \\sum_j K_j^\\top y_j$ has few non-zero terms, making it efficient to compute. It does *not* mean one can ignore non-local contributions and still converge to the correct solution. Such an approximation changes the algorithm to one that solves a different problem. This strategy is not \"convergence-safe\" as requested.\n**Verdict: Incorrect**\n\n**E. Replace $K$ by a block-diagonal approximation $\\tilde{K} = \\operatorname{blkdiag}(K_{1},\\dots,K_{J})$ constructed via gossip among observers to eliminate cross-block couplings and then perform centralized PDHG on $(f,g,\\tilde{K})$. The loss of cross-block terms is negligible in practice and does not impact convergence to the original solution.**\n\nThis option is fundamentally flawed because it proposes solving a different problem. By replacing $K$ with an approximation $\\tilde{K}$, the algorithm will converge to the minimizer of $f(x) + g(\\tilde{K}x)$, not the original objective function. The claim that this \"does not impact convergence to the original solution\" is false. Such an approximation may be a valid heuristic in some applications, but it is not a correct strategy for solving the original problem. The construction of $\\tilde{K} = \\operatorname{blkdiag}(K_{1},\\dots,K_{J})$ is also ill-defined, as each $K_j$ is a \"fat\" matrix of size $m_j \\times n$; a block diagonal arrangement would require a partitioning of the primal space $\\mathbb{R}^n$, which is not given.\n**Verdict: Incorrect**", "answer": "$$\\boxed{A}$$", "id": "3413769"}]}