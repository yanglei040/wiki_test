{"hands_on_practices": [{"introduction": "A common challenge in numerical optimization is adapting existing solvers to new but related problems. This practice demonstrates a powerful \"augmentation trick\" that recasts the Elastic Net problem into an equivalent LASSO problem. By skillfully manipulating the quadratic penalty term, you will learn how to augment the forward operator and data vector, making it possible to solve the Elastic Net using standard LASSO algorithms [@problem_id:3377866].", "problem": "Consider a linear inverse problem in which an unknown state vector $x \\in \\mathbb{R}^{n}$ is related to observed data $y \\in \\mathbb{R}^{m}$ by a known linear forward operator $A \\in \\mathbb{R}^{m \\times n}$ through $y = A x + \\varepsilon$, where $\\varepsilon$ represents additive noise. A common approach in inverse problems and data assimilation is to estimate $x$ by minimizing a composite objective that balances fidelity to the data with regularization. Suppose the objective function is\n$$\nJ(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + g(x),\n$$\nwhere the analysis elastic net penalty is defined by\n$$\ng(x) = \\lambda_{1}\\|W x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2},\n$$\nwith $W \\in \\mathbb{R}^{p \\times n}$ a fixed analysis operator (for example, a discrete gradient or wavelet transform), $\\lambda_{1} > 0$, and $\\lambda_{2} \\ge 0$. Starting from the fundamental properties of the Euclidean norm and standard linear algebra identities (specifically, that for any vectors $u$ and $v$ of compatible dimensions, $\\|[u; v]\\|_{2}^{2} = \\|u\\|_{2}^{2} + \\|v\\|_{2}^{2}$), derive an equivalent formulation of the minimization problem in which the quadratic part of $g(x)$ is absorbed into the data misfit via an augmentation of the forward operator and observation vector. More precisely, show that there exist an augmented operator $\\tilde{A} \\in \\mathbb{R}^{(m+n) \\times n}$ and an augmented observation vector $\\tilde{b} \\in \\mathbb{R}^{m+n}$ such that\n$$\nJ(x) = \\frac{1}{2}\\|\\tilde{A} x - \\tilde{b}\\|_{2}^{2} + \\lambda_{1}\\|W x\\|_{1},\n$$\nand write $\\tilde{A}$ and $\\tilde{b}$ explicitly in terms of $A$, $y$, $\\lambda_{2}$, and $I \\in \\mathbb{R}^{n \\times n}$, the identity matrix. Assume $\\lambda_{2} \\ge 0$ and take the principal square root when forming any scalar square roots. Your final answer must be the explicit expressions for $\\tilde{A}$ and $\\tilde{b}$ as a single analytical expression. No numerical approximation or rounding is required.", "solution": "The problem is valid as it is scientifically grounded in standard optimization and linear algebra principles, is well-posed with a unique answer, and is stated using objective, formal mathematical language.\n\nThe objective function to be minimized is given by:\n$$\nJ(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + g(x)\n$$\nwhere a mixed elastic net penalty is defined as:\n$$\ng(x) = \\lambda_{1}\\|W x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}\n$$\nThe parameters $\\lambda_{1}$ and $\\lambda_{2}$ are non-negative, with $\\lambda_{1} > 0$ and $\\lambda_{2} \\ge 0$. Substituting the expression for $g(x)$ into $J(x)$, we have:\n$$\nJ(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda_{1}\\|W x\\|_{1} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}\n$$\nThe goal is to show that this can be reformulated into the form:\n$$\nJ(x) = \\frac{1}{2}\\|\\tilde{A} x - \\tilde{b}\\|_{2}^{2} + \\lambda_{1}\\|W x\\|_{1}\n$$\nfor some augmented operator $\\tilde{A}$ and augmented vector $\\tilde{b}$. This requires us to combine the two quadratic terms, $\\frac{1}{2}\\|A x - y\\|_{2}^{2}$ and $\\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}$, into a single squared Euclidean norm. The term $\\lambda_{1}\\|W x\\|_{1}$ is to be left unchanged.\n\nLet's focus on the sum of the quadratic terms:\n$$\nQ(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}\n$$\nWe can rewrite the second term, $\\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}$, by bringing the scalar coefficient inside the norm. Since $\\lambda_{2} \\ge 0$, its principal square root, $\\sqrt{\\lambda_{2}}$, is a real, non-negative number. The squared Euclidean norm of a vector $v$ is $\\|v\\|_{2}^{2} = v^{T}v$. Thus, $\\|x\\|_{2}^{2}$ can be manipulated as follows:\n$$\n\\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} = \\frac{1}{2} (\\sqrt{\\lambda_{2}})^{2} \\|x\\|_{2}^{2} = \\frac{1}{2} \\|\\sqrt{\\lambda_{2}} x\\|_{2}^{2}\n$$\nWe can express the vector $\\sqrt{\\lambda_{2}}x$ as a matrix-vector product using the identity matrix $I \\in \\mathbb{R}^{n \\times n}$, which gives $\\sqrt{\\lambda_{2}}x = (\\sqrt{\\lambda_{2}}I)x$. To match the structure of the first data-fitting term, we can introduce a zero vector $0_n \\in \\mathbb{R}^n$:\n$$\n\\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} = \\frac{1}{2} \\|(\\sqrt{\\lambda_{2}}I)x\\|_{2}^{2} = \\frac{1}{2} \\|(\\sqrt{\\lambda_{2}}I)x - 0_n\\|_{2}^{2}\n$$\nNow, substitute this back into the expression for $Q(x)$:\n$$\nQ(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\frac{1}{2}\\|(\\sqrt{\\lambda_{2}}I)x - 0_n\\|_{2}^{2}\n$$\nFactoring out the $\\frac{1}{2}$ term, we get:\n$$\nQ(x) = \\frac{1}{2} \\left( \\|A x - y\\|_{2}^{2} + \\|(\\sqrt{\\lambda_{2}}I)x - 0_n\\|_{2}^{2} \\right)\n$$\nThe problem statement provides the identity for the squared norm of a vertically concatenated vector: for any vectors $u$ and $v$ of compatible dimensions, $\\|[u; v]\\|_{2}^{2} = \\|u\\|_{2}^{2} + \\|v\\|_{2}^{2}$. Let's apply this identity in reverse.\nLet $u = Ax - y$ and $v = (\\sqrt{\\lambda_{2}}I)x - 0_n$. The vector $u \\in \\mathbb{R}^m$ and the vector $v \\in \\mathbb{R}^n$. The sum of their squared norms is equal to the squared norm of their vertical concatenation:\n$$\nQ(x) = \\frac{1}{2} \\left\\| \\begin{pmatrix} Ax - y \\\\ (\\sqrt{\\lambda_{2}}I)x - 0_n \\end{pmatrix} \\right\\|_{2}^{2}\n$$\nThis concatenated vector can be rewritten by separating the terms involving $x$ from the constant terms using block matrix algebra:\n$$\n\\begin{pmatrix} Ax - y \\\\ (\\sqrt{\\lambda_{2}}I)x - 0_n \\end{pmatrix} = \\begin{pmatrix} Ax \\\\ (\\sqrt{\\lambda_{2}}I)x \\end{pmatrix} - \\begin{pmatrix} y \\\\ 0_n \\end{pmatrix}\n$$\nFactoring the vector $x$ out of the first block vector gives:\n$$\n\\begin{pmatrix} A \\\\ \\sqrt{\\lambda_{2}}I \\end{pmatrix} x - \\begin{pmatrix} y \\\\ 0_n \\end{pmatrix}\n$$\nBy comparing this expression with the target form $\\tilde{A}x - \\tilde{b}$, we can identify the augmented operator $\\tilde{A}$ and the augmented observation vector $\\tilde{b}$.\nThe augmented operator $\\tilde{A}$ is a block matrix formed by stacking $A$ on top of $\\sqrt{\\lambda_{2}}I$. Since $A \\in \\mathbb{R}^{m \\times n}$ and $I \\in \\mathbb{R}^{n \\times n}$, the resulting matrix $\\tilde{A}$ has dimensions $(m+n) \\times n$:\n$$\n\\tilde{A} = \\begin{pmatrix} A \\\\ \\sqrt{\\lambda_{2}}I \\end{pmatrix}\n$$\nThe augmented vector $\\tilde{b}$ is formed by stacking the original observation vector $y$ on top of the zero vector $0_n$. Since $y \\in \\mathbb{R}^m$ and $0_n \\in \\mathbb{R}^n$, the resulting vector $\\tilde{b}$ has dimensions $(m+n) \\times 1$:\n$$\n\\tilde{b} = \\begin{pmatrix} y \\\\ 0_n \\end{pmatrix}\n$$\nSubstituting these definitions back, we have successfully shown that:\n$$\n\\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} = \\frac{1}{2} \\left\\| \\begin{pmatrix} A \\\\ \\sqrt{\\lambda_{2}}I \\end{pmatrix} x - \\begin{pmatrix} y \\\\ 0_n \\end{pmatrix} \\right\\|_{2}^{2} = \\frac{1}{2}\\|\\tilde{A} x - \\tilde{b}\\|_{2}^{2}\n$$\nTherefore, the original objective function $J(x)$ is equivalent to:\n$$\nJ(x) = \\frac{1}{2}\\|\\tilde{A} x - \\tilde{b}\\|_{2}^{2} + \\lambda_{1}\\|W x\\|_{1}\n$$\nThe expressions for $\\tilde{A}$ and $\\tilde{b}$ are thus derived as required.", "answer": "$$\n\\boxed{\\tilde{A} = \\begin{pmatrix} A \\\\ \\sqrt{\\lambda_2} I \\end{pmatrix}, \\quad \\tilde{b} = \\begin{pmatrix} y \\\\ 0 \\end{pmatrix}}\n$$", "id": "3377866"}, {"introduction": "Building on the augmentation technique, this problem explores its profound computational consequences. You will analyze how this reformulation affects the problem's mathematical properties, such as convexity and conditioning, and how those changes influence the efficiency and implementation of first-order optimization algorithms [@problem_id:3377906]. This exercise bridges the gap between algebraic theory and practical algorithmic performance.", "problem": "Consider a linear inverse problem in data assimilation where observations $b \\in \\mathbb{R}^{m}$ are related to a state vector $x \\in \\mathbb{R}^{n}$ via a known linear forward operator $A \\in \\mathbb{R}^{m \\times n}$ with $m \\leq n$. To stabilize the estimation of $x$ and promote both sparsity and stability, one employs the Elastic Net (EN) penalty, leading to the following composite convex optimization problem:\nminimize over $x \\in \\mathbb{R}^{n}$ the objective $J_{\\mathrm{EN}}(x) = \\tfrac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda_{1}\\|x\\|_{1} + \\tfrac{\\lambda_{2}}{2}\\|x\\|_{2}^{2}$, where $\\lambda_{1} \\geq 0$ and $\\lambda_{2} \\geq 0$.\nIt is often desirable to leverage a robust solver for the Least Absolute Shrinkage and Selection Operator (LASSO), that is, the problem minimize over $x \\in \\mathbb{R}^{n}$ the objective $\\tfrac{1}{2}\\|\\tilde{A} x - \\tilde{b}\\|_{2}^{2} + \\lambda_{1}\\|x\\|_{1}$ for some appropriately chosen $\\tilde{A} \\in \\mathbb{R}^{\\tilde{m} \\times n}$ and $\\tilde{b} \\in \\mathbb{R}^{\\tilde{m}}$, to solve the EN-regularized problem. A common strategy is to recast the EN objective as a LASSO on an augmented system via a suitable choice of $(\\tilde{A}, \\tilde{b})$, a construction sometimes referred to as the “augmentation trick.” The goal of this problem is to reason from first principles to determine a correct augmentation and to analyze how this transformation influences conditioning and algorithmic performance in first-order methods frequently used in inverse problems and data assimilation.\nSelect all statements that are true.\n\nA. The EN problem with $\\lambda_{2} \\geq 0$ is exactly equivalent to a LASSO problem on an augmented system obtained by choosing $\\tilde{A} = \\begin{bmatrix} A \\\\ \\sqrt{\\lambda_{2}} I_{n} \\end{bmatrix}$ and $\\tilde{b} = \\begin{bmatrix} b \\\\ 0 \\end{bmatrix}$, while keeping the $\\ell_{1}$ weight $\\lambda_{1}$ unchanged.\n\nB. An alternative equivalence is obtained by scaling the forward operator to $\\tilde{A} = \\sqrt{1+\\lambda_{2}}\\,A$ and keeping $\\tilde{b} = b$, which yields a LASSO whose minimizer coincides with that of the EN problem for any $\\lambda_{2} > 0$.\n\nC. Under the augmentation, the smooth part’s Hessian changes from $A^{\\top}A$ to $A^{\\top}A + \\lambda_{2} I_{n}$. This strictly increases strong convexity and cannot worsen the spectral condition number; moreover, as $\\lambda_{2} \\to \\infty$, the spectral condition number tends to $1$.\n\nD. For first-order proximal gradient methods such as Iterative Shrinkage-Thresholding Algorithm (ISTA) and Fast Iterative Shrinkage-Thresholding Algorithm (FISTA), increasing $\\lambda_{2}$ necessarily increases the number of iterations to reach a given accuracy because the Lipschitz constant of the gradient increases with $\\lambda_{2}$.\n\nE. A naive implementation that explicitly forms the augmented matrix increases per-iteration cost, but one can avoid materializing it by computing products with $\\tilde{A}$ via two separate operations $x \\mapsto A x$ and $x \\mapsto \\sqrt{\\lambda_{2}}\\,x$, which yields a cost equal to applying $A$ plus an additional $O(n)$ term per product.\n\nF. If $A$ is rank-deficient, taking any $\\lambda_{2} > 0$ renders the EN objective strictly convex, and hence the minimizer is unique.\n\nG. Safe screening rules designed for LASSO can be applied to the augmented system without modification, and the spectral norm of the augmented matrix satisfies $\\|\\tilde{A}\\|_{2} = \\sqrt{\\sigma_{\\max}(A)^{2} + \\lambda_{2}}$, where $\\sigma_{\\max}(A)$ denotes the largest singular value of $A$.", "solution": "This problem requires analyzing several statements about the \"augmentation trick\" that recasts an Elastic Net problem into a LASSO problem.\n\nFirst, let's verify the augmentation proposed in **Option A**. The augmented LASSO objective is:\n$$ J_{LASSO}(\\tilde{A}, \\tilde{b}) = \\frac{1}{2}\\|\\tilde{A} x - \\tilde{b}\\|_{2}^{2} + \\lambda_{1}\\|x\\|_{1} $$\nSubstituting $\\tilde{A} = \\begin{bmatrix} A \\\\ \\sqrt{\\lambda_{2}} I_{n} \\end{bmatrix}$ and $\\tilde{b} = \\begin{bmatrix} b \\\\ 0 \\end{bmatrix}$:\n$$ J_{LASSO} = \\frac{1}{2}\\left\\| \\begin{bmatrix} A \\\\ \\sqrt{\\lambda_{2}} I_{n} \\end{bmatrix} x - \\begin{bmatrix} b \\\\ 0 \\end{bmatrix} \\right\\|_{2}^{2} + \\lambda_{1}\\|x\\|_{1} = \\frac{1}{2}\\left\\| \\begin{bmatrix} Ax - b \\\\ \\sqrt{\\lambda_{2}}x \\end{bmatrix} \\right\\|_{2}^{2} + \\lambda_{1}\\|x\\|_{1} $$\nUsing the property $\\|[u;v]\\|_2^2 = \\|u\\|_2^2 + \\|v\\|_2^2$, this expands to:\n$$ = \\frac{1}{2}\\left( \\|Ax - b\\|_{2}^{2} + \\|\\sqrt{\\lambda_{2}}x\\|_{2}^{2} \\right) + \\lambda_{1}\\|x\\|_{1} = \\frac{1}{2}\\|Ax - b\\|_{2}^{2} + \\frac{\\lambda_{2}}{2}\\|x\\|_{2}^{2} + \\lambda_{1}\\|x\\|_{1} $$\nThis is exactly the Elastic Net objective $J_{\\mathrm{EN}}(x)$. Therefore, **Option A is true**.\n\nNow let's analyze the other options:\n\n*   **B**: The proposed alternative scaling $\\tilde{A} = \\sqrt{1+\\lambda_{2}}\\,A$ is incorrect. The objective would become $\\frac{1}{2}\\|\\sqrt{1+\\lambda_{2}} A x - b\\|_{2}^{2} + \\lambda_{1}\\|x\\|_{1}$. The quadratic term is $\\frac{1+\\lambda_2}{2}\\|Ax-b\\|_2^2$, which is not equivalent to the EN objective. The minimizers will not be the same. Thus, **Option B is false**.\n\n*   **C**: The Hessian of the smooth part of the original EN objective is $A^\\top A + \\lambda_2 I_n$. The Hessian of the augmented LASSO's smooth part is $\\tilde{A}^\\top\\tilde{A}$.\n    $$ \\tilde{A}^\\top\\tilde{A} = \\begin{bmatrix} A^\\top & \\sqrt{\\lambda_2}I_n \\end{bmatrix} \\begin{bmatrix} A \\\\ \\sqrt{\\lambda_2}I_n \\end{bmatrix} = A^\\top A + \\lambda_2 I_n $$\n    This confirms the Hessian. For $\\lambda_2 > 0$, this strictly increases the eigenvalues of the Hessian, which increases the strong convexity parameter. The condition number of this Hessian is $\\kappa = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{\\sigma_{\\max}(A)^2 + \\lambda_2}{\\sigma_{\\min}(A)^2 + \\lambda_2}$. As $\\lambda_2 \\to \\infty$, this ratio approaches 1. An improved condition number (closer to 1) is better. Thus, **Option C is true**.\n\n*   **D**: The convergence rate of first-order methods depends on properties of the smooth part's gradient. The Lipschitz constant of the gradient is $L = \\lambda_{\\max}(\\tilde{A}^\\top\\tilde{A}) = \\sigma_{\\max}(A)^2 + \\lambda_2$. Increasing $\\lambda_2$ does increase $L$. However, for strongly convex problems, the convergence rate is often dictated by the condition number $\\kappa = L/\\mu$, where $\\mu = \\lambda_{\\min}(\\tilde{A}^\\top\\tilde{A}) = \\sigma_{\\min}(A)^2 + \\lambda_2$. As shown in C, $\\kappa$ decreases towards 1 as $\\lambda_2$ increases. A smaller condition number typically leads to *faster* convergence (fewer iterations). The statement that it *necessarily increases* the number of iterations is incorrect. Thus, **Option D is false**.\n\n*   **E**: Explicitly forming $\\tilde{A} \\in \\mathbb{R}^{(m+n)\\times n}$ can be memory-intensive if $n$ is large. A matrix-vector product $\\tilde{A}x$ can be computed without forming $\\tilde{A}$ by calculating $Ax$ and $\\sqrt{\\lambda_2}x$ (a vector scaling, cost $O(n)$) and concatenating the results. Similarly, $\\tilde{A}^\\top y$ can be computed. This \"matrix-free\" approach is standard and efficient. Thus, **Option E is true**.\n\n*   **F**: If $A$ is rank-deficient, $A^\\top A$ has at least one zero eigenvalue, so the smooth part of the LASSO objective ($\\lambda_2=0$) is not strictly convex. Adding the term $\\frac{\\lambda_2}{2}\\|x\\|_2^2$ for any $\\lambda_2 > 0$ makes the total objective strictly convex because the Hessian of the smooth part, $A^\\top A + \\lambda_2 I_n$, becomes positive definite. A strictly convex function has a unique minimizer. Thus, **Option F is true**.\n\n*   **G**: Since the EN problem is transformed into an equivalent LASSO problem on $(\\tilde{A}, \\tilde{b})$, any algorithm applicable to LASSO, such as safe screening rules, can be directly applied to the augmented system. The spectral norm of $\\tilde{A}$ is $\\|\\tilde{A}\\|_2 = \\sigma_{\\max}(\\tilde{A})$. The squared singular values of $\\tilde{A}$ are the eigenvalues of $\\tilde{A}^\\top \\tilde{A} = A^\\top A + \\lambda_2 I_n$. The largest eigenvalue is $\\sigma_{\\max}(A)^2 + \\lambda_2$. Therefore, $\\|\\tilde{A}\\|_2 = \\sqrt{\\sigma_{\\max}(A)^2 + \\lambda_2}$. Thus, **Option G is true**.\n\nIn summary, options A, C, E, F, and G are correct.", "answer": "$$\\boxed{ACEFG}$$", "id": "3377906"}, {"introduction": "This exercise moves from algorithmic mechanics to the fundamental statistical motivation for regularization. By deriving the bias-variance decomposition for Ridge regression, a special case of the Elastic Net, you will quantify how the regularization parameter $\\lambda_2$ controls the trade-off between model complexity and stability [@problem_id:3377910]. This provides a concrete understanding of why regularization is a cornerstone of solving ill-posed inverse problems.", "problem": "Consider a linear inverse problem in two dimensions with an identity observation operator. Let the data model be $y = x^{\\star} + \\varepsilon$, where $x^{\\star} \\in \\mathbb{R}^{2}$ is the unknown state, $y \\in \\mathbb{R}^{2}$ is the observed data, and $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{2})$ is additive Gaussian noise with zero mean and covariance $\\sigma^{2} I_{2}$. We perform estimation using the Elastic Net with the mixed penalty specialized to isolate the effect of the quadratic penalty by setting the $\\ell_{1}$ parameter to zero, yielding the ridge-regularized estimator\n$$\n\\widehat{x}_{\\lambda_{2}} \\in \\arg\\min_{x \\in \\mathbb{R}^{2}} \\left\\{ \\frac{1}{2} \\| y - x \\|_{2}^{2} + \\frac{\\lambda_{2}}{2} \\| x \\|_{2}^{2} \\right\\}, \\quad \\lambda_{2} \\ge 0.\n$$\nDefine the in-sample prediction as $\\widehat{y}_{\\lambda_{2}} = \\widehat{x}_{\\lambda_{2}}$ and the noiseless truth as $y^{\\star} = x^{\\star}$. Predictive stability is quantified by the variance term in the error decomposition of the expected in-sample prediction error $\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right]$ into its squared bias and variance components. \n\nStarting from the data model and the definition of $\\widehat{x}_{\\lambda_{2}}$, derive the bias-variance decomposition of $\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right]$ as an explicit function of $\\lambda_{2}$, $x^{\\star}$, and $\\sigma^{2}$. Then, instantiate the example with $x^{\\star} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$ and $\\sigma^{2} = 1$ to demonstrate that increasing $\\lambda_{2}$ from $0$ to $1$ improves predictive stability (variance reduction) at the cost of increased bias. Quantify this tradeoff by computing the ratio\n$$\nR \\equiv \\frac{\\text{reduction in variance when } \\lambda_{2}: 0 \\to 1}{\\text{increase in squared bias when } \\lambda_{2}: 0 \\to 1}.\n$$\nProvide the value of $R$ as a single exact number. Do not include units. Do not provide intermediate values. The final answer must be a single real number.", "solution": "The analysis begins by validating the problem statement.\n\n### Step 1: Extract Givens\n- Data model: $y = x^{\\star} + \\varepsilon$, where $x^{\\star} \\in \\mathbb{R}^{2}$ is the unknown true state, $y \\in \\mathbb{R}^{2}$ is the observation.\n- Noise model: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{2})$, where $I_{2}$ is the $2 \\times 2$ identity matrix.\n- Estimator: $\\widehat{x}_{\\lambda_{2}} \\in \\arg\\min_{x \\in \\mathbb{R}^{2}} \\left\\{ \\frac{1}{2} \\| y - x \\|_{2}^{2} + \\frac{\\lambda_{2}}{2} \\| x \\|_{2}^{2} \\right\\}$, for $\\lambda_{2} \\ge 0$.\n- In-sample prediction: $\\widehat{y}_{\\lambda_{2}} = \\widehat{x}_{\\lambda_{2}}$.\n- Noiseless truth: $y^{\\star} = x^{\\star}$.\n- Quantity to analyze: The expected in-sample prediction error, $\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right]$, and its bias-variance decomposition.\n- Specific parameters for instantiation: $x^{\\star} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$, $\\sigma^{2} = 1$.\n- Required calculation: The ratio $R \\equiv \\frac{\\text{reduction in variance when } \\lambda_{2}: 0 \\to 1}{\\text{increase in squared bias when } \\lambda_{2}: 0 \\to 1}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, being a standard formulation of ridge regression (a special case of the elastic net) in statistical learning theory. It is well-posed, as the cost function is strictly convex, guaranteeing a unique minimum. The problem is objective, using precise mathematical language. All data and definitions required for a unique solution are provided, and there are no contradictions. The problem asks for a rigorous derivation and a specific calculation, which is a standard exercise in the field of inverse problems. It is not trivial and is scientifically verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the solution.\n\nThe first step is to find an explicit closed-form expression for the estimator $\\widehat{x}_{\\lambda_{2}}$. The cost function to be minimized is\n$$\nJ(x) = \\frac{1}{2} \\| y - x \\|_{2}^{2} + \\frac{\\lambda_{2}}{2} \\| x \\|_{2}^{2}\n$$\nThis function is strictly convex for $\\lambda_{2} \\ge 0$. The minimum is found by setting the gradient with respect to $x$ to zero.\n$$\n\\nabla_{x} J(x) = \\nabla_{x} \\left( \\frac{1}{2} (y - x)^{T}(y - x) + \\frac{\\lambda_{2}}{2} x^{T}x \\right) = -(y - x) + \\lambda_{2} x\n$$\nSetting the gradient to zero:\n$$\n-(y - \\widehat{x}_{\\lambda_{2}}) + \\lambda_{2} \\widehat{x}_{\\lambda_{2}} = 0\n$$\n$$\n-y + \\widehat{x}_{\\lambda_{2}} + \\lambda_{2} \\widehat{x}_{\\lambda_{2}} = 0\n$$\n$$\n(1 + \\lambda_{2}) \\widehat{x}_{\\lambda_{2}} = y\n$$\nSolving for $\\widehat{x}_{\\lambda_{2}}$ yields:\n$$\n\\widehat{x}_{\\lambda_{2}} = \\frac{1}{1 + \\lambda_{2}} y\n$$\nThe in-sample prediction is $\\widehat{y}_{\\lambda_{2}} = \\widehat{x}_{\\lambda_{2}}$. The noiseless truth is $y^{\\star} = x^{\\star}$. The expected prediction error is $\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right]$. We decompose this error into its squared bias and variance components:\n$$\n\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right] = \\underbrace{\\left\\| \\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] - y^{\\star} \\right\\|_{2}^{2}}_{\\text{Squared Bias}} + \\underbrace{\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - \\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] \\|_{2}^{2}\\right]}_{\\text{Variance}}\n$$\nFirst, we compute the expected value of the prediction. The expectation is taken over the distribution of the noise $\\varepsilon$.\n$$\n\\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] = \\mathbb{E}\\left[ \\frac{1}{1 + \\lambda_{2}} y \\right] = \\frac{1}{1 + \\lambda_{2}} \\mathbb{E}[y]\n$$\nUsing the data model $y = x^{\\star} + \\varepsilon$ and the fact that $\\mathbb{E}[\\varepsilon] = 0$:\n$$\n\\mathbb{E}[y] = \\mathbb{E}[x^{\\star} + \\varepsilon] = x^{\\star} + \\mathbb{E}[\\varepsilon] = x^{\\star}\n$$\nThus, the expected prediction is:\n$$\n\\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] = \\frac{1}{1 + \\lambda_{2}} x^{\\star}\n$$\nNow, we calculate the squared bias, denoted $B^2(\\lambda_2)$:\n$$\nB^2(\\lambda_2) = \\left\\| \\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] - y^{\\star} \\right\\|_{2}^{2} = \\left\\| \\frac{1}{1 + \\lambda_{2}} x^{\\star} - x^{\\star} \\right\\|_{2}^{2} = \\left\\| \\left(\\frac{1}{1 + \\lambda_{2}} - 1\\right) x^{\\star} \\right\\|_{2}^{2} = \\left\\| \\frac{-\\lambda_{2}}{1 + \\lambda_{2}} x^{\\star} \\right\\|_{2}^{2}\n$$\n$$\nB^2(\\lambda_2) = \\left( \\frac{\\lambda_{2}}{1 + \\lambda_{2}} \\right)^{2} \\|x^{\\star}\\|_{2}^{2}\n$$\nNext, we compute the variance, denoted $V(\\lambda_2)$:\n$$\nV(\\lambda_2) = \\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - \\mathbb{E}[\\widehat{y}_{\\lambda_{2}}] \\|_{2}^{2}\\right] = \\mathbb{E}\\left[ \\left\\| \\frac{1}{1 + \\lambda_{2}} y - \\frac{1}{1 + \\lambda_{2}} x^{\\star} \\right\\|_{2}^{2} \\right]\n$$\n$$\nV(\\lambda_2) = \\mathbb{E}\\left[ \\left\\| \\frac{1}{1 + \\lambda_{2}} (y - x^{\\star}) \\right\\|_{2}^{2} \\right] = \\mathbb{E}\\left[ \\left\\| \\frac{1}{1 + \\lambda_{2}} \\varepsilon \\right\\|_{2}^{2} \\right] = \\frac{1}{(1 + \\lambda_{2})^{2}} \\mathbb{E}[\\|\\varepsilon\\|_{2}^{2}]\n$$\nThe noise vector is $\\varepsilon = (\\varepsilon_1, \\varepsilon_2)^T$, where $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$ are independent. The expected squared norm of $\\varepsilon$ is:\n$$\n\\mathbb{E}[\\|\\varepsilon\\|_{2}^{2}] = \\mathbb{E}[\\varepsilon_1^2 + \\varepsilon_2^2] = \\mathbb{E}[\\varepsilon_1^2] + \\mathbb{E}[\\varepsilon_2^2]\n$$\nFor any random variable $Z$ with mean $\\mu_Z$ and variance $\\sigma_Z^2$, $\\mathbb{E}[Z^2] = \\text{Var}(Z) + (\\mathbb{E}[Z])^2$. Here, $\\mathbb{E}[\\varepsilon_i] = 0$ and $\\text{Var}(\\varepsilon_i) = \\sigma^2$. So, $\\mathbb{E}[\\varepsilon_i^2] = \\sigma^2$.\n$$\n\\mathbb{E}[\\|\\varepsilon\\|_{2}^{2}] = \\sigma^2 + \\sigma^2 = 2\\sigma^2\n$$\nSubstituting this back into the variance expression:\n$$\nV(\\lambda_2) = \\frac{2\\sigma^2}{(1 + \\lambda_{2})^{2}}\n$$\nThe full bias-variance decomposition is:\n$$\n\\mathbb{E}\\left[\\| \\widehat{y}_{\\lambda_{2}} - y^{\\star} \\|_{2}^{2}\\right] = \\left( \\frac{\\lambda_{2}}{1 + \\lambda_{2}} \\right)^{2} \\|x^{\\star}\\|_{2}^{2} + \\frac{2\\sigma^2}{(1 + \\lambda_{2})^{2}}\n$$\nNow, we instantiate this with the given parameters: $x^{\\star} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$ and $\\sigma^{2} = 1$.\nFirst, calculate $\\|x^{\\star}\\|_{2}^{2}$:\n$$\n\\|x^{\\star}\\|_{2}^{2} = 2^2 + 1^2 = 4 + 1 = 5\n$$\nWe evaluate the squared bias and variance for $\\lambda_{2} = 0$ and $\\lambda_{2} = 1$.\n\nFor $\\lambda_{2} = 0$:\nSquared Bias: $B^2(0) = \\left( \\frac{0}{1 + 0} \\right)^{2} \\times 5 = 0$.\nVariance: $V(0) = \\frac{2(1)}{(1 + 0)^{2}} = 2$.\n\nFor $\\lambda_{2} = 1$:\nSquared Bias: $B^2(1) = \\left( \\frac{1}{1 + 1} \\right)^{2} \\times 5 = \\left(\\frac{1}{2}\\right)^{2} \\times 5 = \\frac{5}{4}$.\nVariance: $V(1) = \\frac{2(1)}{(1 + 1)^{2}} = \\frac{2}{4} = \\frac{1}{2}$.\n\nThe increase in squared bias when $\\lambda_2$ goes from $0$ to $1$ is:\n$$\n\\Delta B^2 = B^2(1) - B^2(0) = \\frac{5}{4} - 0 = \\frac{5}{4}\n$$\nThe reduction in variance when $\\lambda_2$ goes from $0$ to $1$ is:\n$$\n\\Delta V = V(0) - V(1) = 2 - \\frac{1}{2} = \\frac{3}{2}\n$$\nThe problem asks for the ratio $R$:\n$$\nR = \\frac{\\text{reduction in variance}}{\\text{increase in squared bias}} = \\frac{\\Delta V}{\\Delta B^2} = \\frac{3/2}{5/4}\n$$\n$$\nR = \\frac{3}{2} \\times \\frac{4}{5} = \\frac{12}{10} = \\frac{6}{5}\n$$", "answer": "$$\n\\boxed{\\frac{6}{5}}\n$$", "id": "3377910"}]}