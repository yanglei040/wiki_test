{"hands_on_practices": [{"introduction": "Bound constraints are one of the simplest yet most common types of constraints in inverse problems, ensuring that model parameters remain within physically plausible ranges. The projected gradient method is an intuitive and effective way to solve such problems, extending standard gradient descent by projecting iterates back into the feasible set. This practice will solidify your understanding of this core algorithm by guiding you through a hands-on implementation, including the critical backtracking line search needed for robust convergence [@problem_id:3371700].", "problem": "Consider the bound-constrained quadratic inverse problem in finite dimensions. Let $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, and $\\lambda  0$. Define the Tikhonov-regularized least-squares objective\n$$\nf(x) \\equiv \\tfrac{1}{2}\\|A x - b\\|_2^2 + \\tfrac{\\lambda}{2}\\|x\\|_2^2, \\quad x \\in \\mathbb{R}^n.\n$$\nImpose simple bound constraints $l \\le x \\le u$ componentwise, where $l, u \\in \\mathbb{R}^n$ with $l_i \\le u_i$ for all $i$. The Euclidean projection onto the feasible set $[l,u]$ is defined componentwise by\n$$\n\\Pi_{[l,u]}(y)_i \\equiv \\min\\{\\max\\{y_i, l_i\\}, u_i\\}, \\quad i = 1,\\dots,n.\n$$\nYou are asked to implement one iteration of the projected gradient method with backtracking line search that respects the projection mapping. Start from a current iterate $x \\in [l,u]$, compute a projected trial point\n$$\nx^+(t) \\equiv \\Pi_{[l,u]}\\bigl(x - t \\nabla f(x)\\bigr),\n$$\nand choose a step length $t$ via Armijo backtracking so that the sufficient decrease condition holds while respecting the projection:\n$$\nf\\bigl(x^+(t)\\bigr) \\le f(x) + \\sigma \\, \\nabla f(x)^{\\top} \\bigl(x^+(t) - x\\bigr).\n$$\nUse the following base rules and definitions only:\n- The objective $f$ is as given above.\n- The Euclidean projection $\\Pi_{[l,u]}$ is as defined above.\n- The gradient $\\nabla f(x)$ must be derived from first principles for the given $f$.\n- The Armijo backtracking must start at initial step $t_0 = 1$ and reduce the step by a fixed factor $\\beta \\in (0,1)$, here $\\beta = 0.5$, until the sufficient decrease condition is met or until the step size falls below a minimum threshold $t_{\\min} = 10^{-12}$.\n- The Armijo parameter is $\\sigma = 10^{-4}$.\n\nImplement a program that, for each test case below, performs exactly one such projected gradient iteration from the provided $x$:\n- Compute $\\nabla f(x)$.\n- Starting from $t = t_0$, repeatedly evaluate the Armijo condition using $x^+(t)$ and reduce $t \\leftarrow \\beta t$ if it is not satisfied, always maintaining $x^+(t)$ via the projection $\\Pi_{[l,u]}$.\n- Terminate the backtracking when the condition holds or $t  t_{\\min}$.\n- Return the accepted $t$ and the corresponding objective value $f\\bigl(x^+(t)\\bigr)$.\n\nTest suite (each case specifies $(A,b,\\lambda,l,u,x)$):\n1. Happy-path, interior iterate:\n   - $A = \\begin{bmatrix} 3  0  1 \\\\ 0  2  -1 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix}$, $\\lambda = 0.1$,\n   - $l = \\begin{bmatrix} -5 \\\\ -5 \\\\ -5 \\end{bmatrix}$, $u = \\begin{bmatrix} 5 \\\\ 5 \\\\ 5 \\end{bmatrix}$, $x = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 1.0 \\end{bmatrix}$.\n2. Lower-bound activation:\n   - $A = \\begin{bmatrix} 2  -1 \\\\ -1  2 \\end{bmatrix}$, $b = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$, $\\lambda = 0.01$,\n   - $l = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $u = \\begin{bmatrix} 10 \\\\ 10 \\end{bmatrix}$, $x = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}$.\n3. Upper-bound activation:\n   - $A = \\begin{bmatrix} 1  3 \\\\ 2  0 \\end{bmatrix}$, $b = \\begin{bmatrix} 4 \\\\ -1 \\end{bmatrix}$, $\\lambda = 0.05$,\n   - $l = \\begin{bmatrix} -1 \\\\ -1 \\end{bmatrix}$, $u = \\begin{bmatrix} 0.2 \\\\ 0.3 \\end{bmatrix}$, $x = \\begin{bmatrix} 0.19 \\\\ 0.29 \\end{bmatrix}$.\n4. Zero-gradient edge case:\n   - $A = \\begin{bmatrix} 2 \\end{bmatrix}$, $b = \\begin{bmatrix} 4 \\end{bmatrix}$, $\\lambda = 1.0$,\n   - $l = \\begin{bmatrix} 0 \\end{bmatrix}$, $u = \\begin{bmatrix} 10 \\end{bmatrix}$, $x = \\begin{bmatrix} 1.6 \\end{bmatrix}$.\n5. Backtracking shrink in a stiff one-dimensional problem:\n   - $A = \\begin{bmatrix} 3 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\end{bmatrix}$, $\\lambda = 1.0$,\n   - $l = \\begin{bmatrix} -5 \\end{bmatrix}$, $u = \\begin{bmatrix} 5 \\end{bmatrix}$, $x = \\begin{bmatrix} 0.0 \\end{bmatrix}$.\n\nAngle units are not applicable. No physical units are involved.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be the list $[t,f^+]$, where $t$ is the accepted step length and $f^+ = f\\bigl(x^+(t)\\bigr)$ is the objective at the projected point. Both $t$ and $f^+$ must be printed as decimal numbers rounded to six decimal places. The final output format is thus a single line like\n\"[ [t_1,f_1], [t_2,f_2], [t_3,f_3], [t_4,f_4], [t_5,f_5] ]\"\nwith each $t_i$ and $f_i$ rounded to six decimal places and no extra text.\n\nConstants for the backtracking must be fixed as $t_0 = 1$, $\\beta = 0.5$, $\\sigma = 10^{-4}$, $t_{\\min} = 10^{-12}$, identical for all test cases.\n\nDerive all needed expressions from first principles and implement the method accordingly.", "solution": "The problem requires implementing a single iteration of the projected gradient method for a bound-constrained quadratic objective function.\n\nFirst, we derive the gradient of the objective function, $f(x)$:\n$$\nf(x) \\equiv \\tfrac{1}{2}\\|A x - b\\|_2^2 + \\tfrac{\\lambda}{2}\\|x\\|_2^2\n$$\nExpressing the norms using inner products gives $f(x) = \\tfrac{1}{2}(A x - b)^{\\top}(A x - b) + \\tfrac{\\lambda}{2} x^{\\top}x$. Expanding this yields:\n$$\nf(x) = \\tfrac{1}{2}(x^{\\top}A^{\\top}A x - 2 b^{\\top}A x + b^{\\top}b) + \\tfrac{\\lambda}{2} x^{\\top}x\n$$\nDifferentiating with respect to the vector $x$ using standard matrix calculus rules gives the gradient:\n$$\n\\nabla f(x) = A^{\\top}(A x - b) + \\lambda x\n$$\nThe algorithm for one iteration starting from a feasible point $x$ is as follows:\n1.  **Compute Gradient**: Calculate $g = \\nabla f(x)$.\n2.  **Backtracking Line Search**: Find a step length $t$ that satisfies the Armijo-type condition adapted for projections. Start with $t = t_0=1$.\n    -   Compute a trial point by stepping in the negative gradient direction and projecting onto the feasible set $[l, u]$:\n        $$\n        x^+(t) = \\Pi_{[l,u]}(x - t g)\n        $$\n    -   The projection is defined as $(\\Pi_{[l,u]}(y))_i = \\min\\{\\max\\{y_i, l_i\\}, u_i\\}$.\n    -   Check the sufficient decrease condition:\n        $$\n        f(x^+(t)) \\le f(x) + \\sigma g^{\\top}(x^+(t) - x)\n        $$\n    -   If the condition is met, accept the step $t$.\n    -   If not, reduce the step size $t \\leftarrow \\beta t$ (with $\\beta=0.5$) and repeat, until the condition is met or $t$ falls below $t_{\\min}$.\n\nThe implementation will apply this logic to each test case, returning the accepted step length $t$ and the new objective value $f(x^+(t))$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for the projected gradient method.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            # Happy-path, interior iterate\n            \"A\": np.array([[3.0, 0.0, 1.0], [0.0, 2.0, -1.0]]),\n            \"b\": np.array([1.0, -2.0]),\n            \"lambda\": 0.1,\n            \"l\": np.array([-5.0, -5.0, -5.0]),\n            \"u\": np.array([5.0, 5.0, 5.0]),\n            \"x\": np.array([0.5, -0.5, 1.0]),\n        },\n        {\n            # Lower-bound activation\n            \"A\": np.array([[2.0, -1.0], [-1.0, 2.0]]),\n            \"b\": np.array([0.0, 1.0]),\n            \"lambda\": 0.01,\n            \"l\": np.array([0.0, 0.0]),\n            \"u\": np.array([10.0, 10.0]),\n            \"x\": np.array([0.1, 0.2]),\n        },\n        {\n            # Upper-bound activation\n            \"A\": np.array([[1.0, 3.0], [2.0, 0.0]]),\n            \"b\": np.array([4.0, -1.0]),\n            \"lambda\": 0.05,\n            \"l\": np.array([-1.0, -1.0]),\n            \"u\": np.array([0.2, 0.3]),\n            \"x\": np.array([0.19, 0.29]),\n        },\n        {\n            # Zero-gradient edge case\n            \"A\": np.array([[2.0]]),\n            \"b\": np.array([4.0]),\n            \"lambda\": 1.0,\n            \"l\": np.array([0.0]),\n            \"u\": np.array([10.0]),\n            \"x\": np.array([1.6]),\n        },\n        {\n            # Backtracking shrink in a stiff one-dimensional problem\n            \"A\": np.array([[3.0]]),\n            \"b\": np.array([1.0]),\n            \"lambda\": 1.0,\n            \"l\": np.array([-5.0]),\n            \"u\": np.array([5.0]),\n            \"x\": np.array([0.0]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        t, f_plus = projected_gradient_iteration(\n            case[\"A\"], case[\"b\"], case[\"lambda\"],\n            case[\"l\"], case[\"u\"], case[\"x\"]\n        )\n        results.append(f\"[{t:.6f}, {f_plus:.6f}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\ndef objective_function(A, b, lam, x):\n    \"\"\"Computes the Tikhonov-regularized objective function.\"\"\"\n    residual = A @ x - b\n    loss = 0.5 * np.linalg.norm(residual)**2\n    reg = (lam / 2.0) * np.linalg.norm(x)**2\n    return loss + reg\n\ndef gradient(A, b, lam, x):\n    \"\"\"Computes the gradient of the objective function.\"\"\"\n    return A.T @ (A @ x - b) + lam * x\n\ndef projection(y, l, u):\n    \"\"\"Projects a vector y onto the box [l, u].\"\"\"\n    return np.minimum(np.maximum(y, l), u)\n\ndef projected_gradient_iteration(A, b, lam, l, u, x):\n    \"\"\"\n    Performs one iteration of projected gradient descent with Armijo backtracking.\n    \"\"\"\n    # Backtracking parameters\n    t_0 = 1.0\n    beta = 0.5\n    sigma = 1e-4\n    t_min = 1e-12\n\n    # Compute values at current point x\n    f_x = objective_function(A, b, lam, x)\n    grad_f_x = gradient(A, b, lam, x)\n    \n    t = t_0\n    while True:\n        # Compute projected trial point\n        x_plus_t = projection(x - t * grad_f_x, l, u)\n        \n        # Compute objective at trial point\n        f_x_plus_t = objective_function(A, b, lam, x_plus_t)\n        \n        # Check Armijo condition\n        armijo_rhs = f_x + sigma * np.dot(grad_f_x, x_plus_t - x)\n        \n        if f_x_plus_t = armijo_rhs:\n            # Step size is accepted\n            return t, f_x_plus_t\n        \n        # If condition is not met, check termination for small step\n        if t  t_min:\n            # Terminate because step size is too small, returning last tried values\n            return t, f_x_plus_t\n            \n        # Reduce step size\n        t *= beta\n\nsolve()\n```", "id": "3371700"}, {"introduction": "Beyond simple bounds, many inverse problems involve more complex geometric constraints, such as those defined by second-order cones, which are fundamental in areas like robust optimization. The key to applying projection-based algorithms is having an efficient way to compute the projection onto the constraint set. This exercise will guide you through the derivation of the projection onto the second-order cone from first principles, providing a deep understanding of how Karush–Kuhn–Tucker (KKT) conditions are used to find explicit solutions for these crucial building blocks [@problem_id:3371711].", "problem": "Consider a projected-gradient step that arises in a constrained formulation of an inverse problem in data assimilation, where a slack variable $t \\in \\mathbb{R}$ and a state increment $u \\in \\mathbb{R}^{n-1}$ are constrained by the Second-Order Cone (SOC) $\\mathcal{K} = \\{(s,x) \\in \\mathbb{R} \\times \\mathbb{R}^{n-1} : \\|x\\|_{2} \\le s\\}$. The Euclidean projection onto a nonempty, closed, and convex set is defined as the minimizer of the squared Euclidean distance. Starting from this definition, and using only first principles of convex optimization (existence and uniqueness of projections onto closed convex sets, convexity of norms, and Karush–Kuhn–Tucker optimality conditions), derive the explicit formula for the Euclidean projection $\\Pi_{\\mathcal{K}}(t,u)$ of an arbitrary point $(t,u) \\in \\mathbb{R} \\times \\mathbb{R}^{n-1}$ onto $\\mathcal{K}$. Your derivation must systematically characterize and justify all three regimes $t  \\|u\\|_{2}$, $t  -\\|u\\|_{2}$, and the remaining case, including the value of the Lagrange multiplier and the structure of the optimizer.\n\nFinally, for the specific instance with $n = 4$ and $(t,u) = (1,(2,-1,2))$, compute the projection $\\Pi_{\\mathcal{K}}(t,u)$ exactly. Do not round. Express your final answer as a single explicit vector expression with four entries, corresponding to the projected scalar followed by the projected $(n-1)$-dimensional vector, with no units.", "solution": "The problem asks for the derivation of the formula for the Euclidean projection of a point $(t,u) \\in \\mathbb{R} \\times \\mathbb{R}^{n-1}$ onto the second-order cone $\\mathcal{K} = \\{(s,x) \\in \\mathbb{R} \\times \\mathbb{R}^{n-1} : \\|x\\|_{2} \\le s\\}$.\n\nThe projection $\\Pi_{\\mathcal{K}}(t,u)$ is the unique solution $(\\hat{s}, \\hat{x})$ to the convex optimization problem:\n$$\n\\begin{aligned}\n \\underset{s,x}{\\text{minimize}}   \\frac{1}{2} ((s-t)^2 + \\|x-u\\|_2^2) \\\\\n \\text{subject to}   \\|x\\|_2 - s \\le 0\n\\end{aligned}\n$$\nWe use the Karush–Kuhn–Tucker (KKT) conditions with Lagrange multiplier $\\lambda \\ge 0$. The stationarity conditions are:\n$$\n(\\hat{s}-t) - \\lambda = 0 \\quad \\implies \\quad \\hat{s} = t + \\lambda\n$$\n$$\n(\\hat{x}-u) + \\lambda \\nabla (\\|\\hat{x}\\|_2) = 0\n$$\nThe complementary slackness condition is $\\lambda (\\|\\hat{x}\\|_2 - \\hat{s}) = 0$. This gives two main cases.\n\n**Regime 1: The point $(t,u)$ is inside the cone, $t \\ge \\|u\\|_2$.**\nIf the constraint is inactive, we can set $\\lambda=0$. The KKT conditions are satisfied if $(\\hat{s}, \\hat{x}) = (t,u)$ and $\\|u\\|_2 - t \\le 0$, which is the assumption for this case. Thus, if $t \\ge \\|u\\|_2$, the point is its own projection.\n\n**Regime 2: The point $(t,u)$ is outside the cone.**\nThe projection must lie on the boundary, so $\\|\\hat{x}\\|_2 = \\hat{s}$, which allows $\\lambda > 0$. We can further split this into two sub-cases.\n-   **Projection to the apex**: If the projection is $(\\hat{s}, \\hat{x})=(0,0)$, KKT stationarity implies $\\lambda=-t$ and $u = \\lambda z$ for some subgradient $z$ with $\\|z\\|_2 \\le 1$. Dual feasibility requires $\\lambda \\ge 0$, so $t \\le 0$. Taking norms, $\\|u\\|_2 = |-t|\\|z\\|_2 \\le -t$, since $t \\le 0$. This case holds if $t \\le -\\|u\\|_2$.\n-   **Projection to the side**: If $-\\|u\\|_2  t  \\|u\\|_2$, the projection is on the boundary but not the apex. Stationarity gives $u = \\hat{x}(1 + \\lambda/\\|\\hat{x}\\|_2)$, implying $\\hat{x}$ and $u$ are collinear. Combining this with the boundary condition $\\|\\hat{x}\\|_2 = \\hat{s}$ and the stationarity condition $\\hat{s} = t+\\lambda$, we can solve for $\\lambda$, $\\hat{s}$, and $\\hat{x}$:\n    $$\n    \\lambda = \\frac{\\|u\\|_2-t}{2}, \\quad \\hat{s} = \\frac{t+\\|u\\|_2}{2}, \\quad \\hat{x} = \\frac{t+\\|u\\|_2}{2\\|u\\|_2} u\n    $$\n\n**Summary of Projection Formula:**\n$$\n\\Pi_{\\mathcal{K}}(t,u) = \n\\begin{cases} \n(t,u)  \\text{if } t \\ge \\|u\\|_2 \\\\\n(0,0)  \\text{if } t \\le -\\|u\\|_2 \\\\\n\\left(\\frac{t+\\|u\\|_2}{2}, \\frac{t+\\|u\\|_2}{2\\|u\\|_2} u\\right)  \\text{if } -\\|u\\|_2  t  \\|u\\|_2\n\\end{cases}\n$$\n\n**Specific Instance Calculation**\nWe are given the point $(t,u) = (1, (2,-1,2))$.\nFirst, compute the norm of $u$:\n$$\n\\|u\\|_2 = \\sqrt{2^2+(-1)^2+2^2} = \\sqrt{4+1+4} = \\sqrt{9} = 3\n$$\nWe have $t=1$. Since $-3  1  3$, we are in the third regime.\nThe projected scalar component $\\hat{s}$ is:\n$$\n\\hat{s} = \\frac{t+\\|u\\|_2}{2} = \\frac{1+3}{2} = 2\n$$\nThe projected vector component $\\hat{x}$ is:\n$$\n\\hat{x} = \\left(\\frac{t+\\|u\\|_2}{2\\|u\\|_2}\\right)u = \\left(\\frac{1+3}{2 \\cdot 3}\\right)(2, -1, 2) = \\frac{4}{6}(2, -1, 2) = \\frac{2}{3}(2, -1, 2) = \\left(\\frac{4}{3}, -\\frac{2}{3}, \\frac{4}{3}\\right)\n$$\nThe projected point $(\\hat{s}, \\hat{x})$ is therefore $\\left(2, \\frac{4}{3}, -\\frac{2}{3}, \\frac{4}{3}\\right)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2  \\frac{4}{3}  -\\frac{2}{3}  \\frac{4}{3}\n\\end{pmatrix}\n}\n$$", "id": "3371711"}, {"introduction": "Many modern inverse problems leverage non-smooth regularizers like Total Variation (TV) to promote properties such as piecewise-constant solutions, which are often tackled with sophisticated primal-dual methods. Primal-dual algorithms transform the original minimization problem into a saddle-point problem, which can then be solved by iterating on both primal and dual variables using simpler proximal steps. This practice provides a concrete, step-by-step calculation of one iteration of the Chambolle–Pock algorithm, demystifying its mechanics and showing how convex conjugacy and projections are used to handle the non-smooth TV term [@problem_id:3371670].", "problem": "Consider the one-dimensional total variation regularized least-squares inverse problem on a uniform grid with three unknowns. Let the primal variable be $u \\in \\mathbb{R}^{3}$, the data be $y \\in \\mathbb{R}^{3}$, and the discrete forward difference operator $D \\in \\mathbb{R}^{2 \\times 3}$ be\n$$\nD \\;=\\; \\begin{pmatrix}\n-1  1  0\\\\\n0  -1  1\n\\end{pmatrix}.\n$$\nConsider the composite convex objective\n$$\n\\min_{u \\in \\mathbb{R}^{3}} \\; h(u) \\;+\\; g(Du),\n$$\nwith the data fidelity $h(u) = \\tfrac{1}{2}\\|u - y\\|_{2}^{2}$ and the regularizer $g(z) = \\lambda \\|z\\|_{1}$ for $z \\in \\mathbb{R}^{2}$ and $\\lambda  0$. Work with the saddle-point formulation derived from convex conjugacy of $g$ and use a primal-dual splitting scheme in which the dual variable is updated by Euclidean projection onto an $\\ell_{\\infty}$ ball and the primal variable is updated by a single explicit gradient descent step on $h$ plus the linear coupling term.\n\nUse the following concrete data and parameters:\n- $y = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\end{pmatrix}$,\n- $\\lambda = 1.5$,\n- primal step size $\\tau = 0.2$,\n- dual step size $\\sigma = 1.0$,\n- extrapolation parameter $\\theta = 1$ for the over-relaxed primal variable,\n- initialization $u^{0} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix}$, $u^{-1} = u^{0}$, and $p^{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\nStarting from the definitions of convex conjugate, saddle-point formulation, and projection onto a closed convex set, derive the single-iteration update $(u^{0}, p^{0}) \\mapsto (u^{1}, p^{1})$ of the Primal-Dual Hybrid Gradient (PDHG) method (also known as the Chambolle–Pock scheme) specialized to this problem, where the primal update is taken as one explicit gradient descent step on $h$ and the dual update is the Euclidean projection of an affine argument onto the $\\ell_{\\infty}$ ball associated with $g^{\\ast}$. Carry out all calculations explicitly for the given data and parameters.\n\nReport as your final answer the exact value of the second component of $u^{1}$, namely $u^{1}_{2}$. No rounding is required, and no units are needed.", "solution": "The problem is to find the next iterate $(u^1, p^1)$ of a Primal-Dual Hybrid Gradient (PDHG) algorithm for the optimization problem $\\min_{u} \\frac{1}{2}\\|u - y\\|_{2}^{2} + \\lambda\\|Du\\|_{1}$.\nThis is an instance of the general problem $\\min_u h(u) + g(Du)$, which can be reformulated as a saddle-point problem:\n$$\n\\min_{u} \\max_{p} \\left( h(u) + \\langle Du, p \\rangle - g^{\\ast}(p) \\right),\n$$\nwhere $g^{\\ast}(p)$ is the convex conjugate of $g(z) = \\lambda \\|z\\|_{1}$. The conjugate is the indicator function of the set $C = \\{p \\in \\mathbb{R}^2 \\mid \\|p\\|_{\\infty} \\le \\lambda\\}$.\nThe PDHG iteration proceeds as follows, starting from $(u^k, u^{k-1}, p^k)$:\n\n1.  **Extrapolation**: Compute $\\bar{u}^k = u^k + \\theta(u^k - u^{k-1})$.\n2.  **Dual Update**: $p^{k+1} = \\text{prox}_{\\sigma g^{\\ast}}(p^k + \\sigma D \\bar{u}^k) = \\Pi_C(p^k + \\sigma D \\bar{u}^k)$.\n3.  **Primal Update**: $u^{k+1} = u^k - \\tau (\\nabla h(u^k) + D^T p^{k+1}) = u^k - \\tau(u^k - y + D^T p^{k+1})$.\n\nWe apply these steps for the first iteration ($k=0$) with the given data: $\\lambda = 1.5$, $\\tau=0.2$, $\\sigma=1.0$, $\\theta=1.0$, $y = (0, 1, 2)^T$, $u^0 = u^{-1} = (1, -2, 3)^T$, and $p^0 = (0, 0)^T$.\n\n**Step 1: Extrapolation**\n$\\bar{u}^0 = u^0 + 1.0(u^0 - u^0) = u^0 = \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix}$.\n\n**Step 2: Dual Update for $p^1$**\nFirst, compute the argument for the projection:\n$$\np^0 + \\sigma D \\bar{u}^0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + 1.0 \\begin{pmatrix} -1  1  0\\\\ 0  -1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} -1 - 2 \\\\ 2 + 3 \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ 5 \\end{pmatrix}\n$$\nNext, project this vector onto the $\\ell_{\\infty}$-ball $C = \\{ p \\in \\mathbb{R}^2 \\mid \\|p\\|_{\\infty} \\le 1.5 \\}$:\n$$\np^1 = \\Pi_C\\begin{pmatrix} -3 \\\\ 5 \\end{pmatrix} = \\begin{pmatrix} \\text{sign}(-3) \\min(|-3|, 1.5) \\\\ \\text{sign}(5) \\min(|5|, 1.5) \\end{pmatrix} = \\begin{pmatrix} -1.5 \\\\ 1.5 \\end{pmatrix} = \\begin{pmatrix} -3/2 \\\\ 3/2 \\end{pmatrix}.\n$$\n\n**Step 3: Primal Update for $u^1$**\nFirst, compute the term in the parentheses for the update:\n$$\nu^0 - y + D^T p^1 = \\begin{pmatrix} 1 \\\\ -3 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} -1  0 \\\\ 1  -1 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} -3/2 \\\\ 3/2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -3 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 3/2 \\\\ -3 \\\\ 3/2 \\end{pmatrix} = \\begin{pmatrix} 5/2 \\\\ -6 \\\\ 5/2 \\end{pmatrix}\n$$\nNow, complete the update for $u^1$ with $\\tau = 1/5$:\n$$\nu^1 = u^0 - \\tau \\begin{pmatrix} 5/2 \\\\ -6 \\\\ 5/2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 3 \\end{pmatrix} - \\frac{1}{5} \\begin{pmatrix} 5/2 \\\\ -6 \\\\ 5/2 \\end{pmatrix} = \\begin{pmatrix} 1 - 1/2 \\\\ -2 - (-6/5) \\\\ 3 - 1/2 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ -4/5 \\\\ 5/2 \\end{pmatrix}\n$$\nThe problem asks for the second component of $u^1$, which is $u^1_2$.\n$$\nu^1_2 = -\\frac{4}{5}\n$$", "answer": "$$\\boxed{-\\frac{4}{5}}$$", "id": "3371670"}]}