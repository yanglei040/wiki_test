{"hands_on_practices": [{"introduction": "This exercise provides a foundational look at how Total Variation (TV) regularization approximates smooth signals. By finding the optimal two-step approximation to a simple linear ramp, you will derive from first principles how the staircasing effect naturally emerges as the best compromise between fitting the data and minimizing variation. This problem is a cornerstone for understanding the piecewise-constant nature of TV-regularized solutions [@problem_id:3420945].", "problem": "Consider a one-dimensional data assimilation setting on the interval $[0,1]$ with a deterministic observation $f(x) = x$. We seek a staircasing solution with two plateaus, meaning a piecewise-constant function $u$ that takes the form\n$$\nu(x) = \\begin{cases}\nc_{1}  \\text{for } x \\in [0,s) \\\\\nc_{2}  \\text{for } x \\in [s,1]\n\\end{cases}\n$$\nfor some break point $s \\in (0,1)$ and plateau values $c_{1}, c_{2} \\in \\mathbb{R}$. The estimator is selected by minimizing the standard Tikhonov-type $L^{2}$ fidelity with Total Variation (TV) regularization used in inverse problems:\n$$\n\\min_{c_{1},c_{2},s} \\ \\frac{1}{2} \\int_{0}^{1} \\big(u(x)-f(x)\\big)^{2} \\,\\mathrm{d}x \\ + \\ \\lambda \\, TV(u),\n$$\nwhere $TV$ denotes the Total Variation, and for such a piecewise-constant $u$ with a single jump at $x=s$, one has $TV(u) = |c_{2} - c_{1}|$. The constant $\\lambda0$ is the regularization parameter.\n\nStarting only from the fundamental definitions of the $L^{2}$ norm and Total Variation, derive the optimal plateau values $c_{1}^{\\star}$ and $c_{2}^{\\star}$ and the optimal break point $s^{\\star}$ that jointly minimize\n$$\n\\frac{1}{2}\\left(\\int_{0}^{s} (c_{1}-x)^{2}\\,\\mathrm{d}x + \\int_{s}^{1} (c_{2}-x)^{2}\\,\\mathrm{d}x\\right) + \\lambda |c_{2}-c_{1}|,\n$$\nunder the assumption $0\\lambda\\frac{1}{8}$ so that the minimizer exhibits two distinct plateaus. Your final answer must be a single closed-form analytic expression giving $c_{1}^{\\star}$, $c_{2}^{\\star}$, and $s^{\\star}$.", "solution": "The problem is to find the optimal parameters $c_1$, $c_2$, and $s$ that minimize the objective functional $J(c_1, c_2, s)$. The functional consists of an $L^2$ data fidelity term and a Total Variation (TV) regularization term. The observation is $f(x) = x$ on the interval $[0,1]$. The sought-after solution $u(x)$ is a piecewise-constant function with two plateaus, $c_1$ and $c_2$, with a break point at $s$.\n\nThe objective functional is given by:\n$$\nJ(c_1, c_2, s) = \\frac{1}{2} \\int_{0}^{1} \\big(u(x)-f(x)\\big)^{2} \\,\\mathrm{d}x \\ + \\ \\lambda \\, TV(u)\n$$\nSubstituting the given forms for $u(x)$, $f(x)$, and $TV(u)$, we have:\n$$\nJ(c_1, c_2, s) = \\frac{1}{2}\\left(\\int_{0}^{s} (c_{1}-x)^{2}\\,\\mathrm{d}x + \\int_{s}^{1} (c_{2}-x)^{2}\\,\\mathrm{d}x\\right) + \\lambda |c_{2}-c_{1}|\n$$\nThis minimization is performed with respect to $c_1 \\in \\mathbb{R}$, $c_2 \\in \\mathbb{R}$, and $s \\in (0,1)$. The problem statement specifies we seek a solution with two distinct plateaus, which implies $c_1 \\neq c_2$. The data function $f(x)=x$ is strictly increasing. It is reasonable to expect the optimal approximation $u(x)$ to be non-decreasing, which means we can assume $c_1 \\le c_2$. The condition $0  \\lambda  1/8$ ensures that $c_1  c_2$, so we can replace $|c_2 - c_1|$ with $c_2 - c_1$. The objective functional becomes:\n$$\nJ(c_1, c_2, s) = \\frac{1}{2}\\left(\\int_{0}^{s} (c_{1}-x)^{2}\\,\\mathrm{d}x + \\int_{s}^{1} (c_{2}-x)^{2}\\,\\mathrm{d}x\\right) + \\lambda (c_{2}-c_{1})\n$$\nTo find the optimal values $(c_1^{\\star}, c_2^{\\star}, s^{\\star})$, we use the first-order necessary conditions for a minimum. This involves setting the partial derivatives of $J$ with respect to $c_1$, $c_2$, and $s$ to zero.\n\nFirst, for a fixed value of $s$, we find the optimal $c_1$ and $c_2$. We compute the partial derivatives of $J$ with respect to $c_1$ and $c_2$:\n$$\n\\frac{\\partial J}{\\partial c_1} = \\frac{\\partial}{\\partial c_1} \\left[ \\frac{1}{2} \\int_{0}^{s} (c_{1}-x)^{2}\\,\\mathrm{d}x - \\lambda c_1 \\right] = \\int_{0}^{s} (c_1 - x)\\,\\mathrm{d}x - \\lambda\n$$\n$$\n\\frac{\\partial J}{\\partial c_2} = \\frac{\\partial}{\\partial c_2} \\left[ \\frac{1}{2} \\int_{s}^{1} (c_{2}-x)^{2}\\,\\mathrm{d}x + \\lambda c_2 \\right] = \\int_{s}^{1} (c_2 - x)\\,\\mathrm{d}x + \\lambda\n$$\nSetting these derivatives to zero yields:\n$$\n\\int_{0}^{s} (c_1 - x)\\,\\mathrm{d}x = \\lambda \\implies \\left[ c_1 x - \\frac{x^2}{2} \\right]_0^s = \\lambda \\implies c_1 s - \\frac{s^2}{2} = \\lambda\n$$\n$$\n\\int_{s}^{1} (c_2 - x)\\,\\mathrm{d}x = -\\lambda \\implies \\left[ c_2 x - \\frac{x^2}{2} \\right]_s^1 = -\\lambda \\implies (c_2 - \\frac{1}{2}) - (c_2 s - \\frac{s^2}{2}) = -\\lambda\n$$\nFrom the first equation, we solve for $c_1$ as a function of $s$:\n$$\nc_1(s) = \\frac{s}{2} + \\frac{\\lambda}{s}\n$$\nFrom the second equation, we solve for $c_2$ as a function of $s$:\n$$\nc_2(1-s) - \\frac{1-s^2}{2} = -\\lambda \\implies c_2(1-s) = \\frac{(1-s)(1+s)}{2} - \\lambda \\implies c_2(s) = \\frac{1+s}{2} - \\frac{\\lambda}{1-s}\n$$\nThese expressions give the optimal plateau values for any given break point $s$.\n\nNext, we find the optimal break point $s^{\\star}$. We substitute $c_1(s)$ and $c_2(s)$ into $J$ to obtain a function of $s$ alone, and then minimize it. A more direct approach is to find the partial derivative of $J(c_1, c_2, s)$ with respect to $s$ and set it to zero, using the values of $c_1$ and $c_2$ we just found. According to the envelope theorem, for the optimal functions $c_1(s)$ and $c_2(s)$, the total derivative $\\frac{dJ}{ds}$ simplifies to the partial derivative $\\frac{\\partial J}{\\partial s}$.\n$$\n\\frac{dJ}{ds} = \\frac{\\partial J}{\\partial c_1}\\frac{dc_1}{ds} + \\frac{\\partial J}{\\partial c_2}\\frac{dc_2}{ds} + \\frac{\\partial J}{\\partial s}\n$$\nSince $c_1(s)$ and $c_2(s)$ are defined such that $\\frac{\\partial J}{\\partial c_1} = 0$ and $\\frac{\\partial J}{\\partial c_2} = 0$, this simplifies to $\\frac{dJ}{ds} = \\frac{\\partial J}{\\partial s}$.\n\nUsing the Leibniz integral rule, the partial derivative of $J$ with respect to $s$ is:\n$$\n\\frac{\\partial J}{\\partial s} = \\frac{1}{2}(c_1-s)^2 - \\frac{1}{2}(c_2-s)^2\n$$\nSetting this to zero to find the optimal $s$:\n$$\n\\frac{1}{2}(c_1-s)^2 - \\frac{1}{2}(c_2-s)^2 = 0 \\implies (c_1-s)^2 = (c_2-s)^2\n$$\nThis implies $c_1-s = \\pm(c_2-s)$.\nCase 1: $c_1-s = c_2-s \\implies c_1=c_2$. This would mean there is no jump, a single plateau solution. This occurs when $\\lambda \\ge 1/8$, but the problem specifies $\\lambda  1/8$. So we discard this case.\nCase 2: $c_1-s = -(c_2-s) \\implies c_1-s = s-c_2 \\implies c_1+c_2 = 2s$.\nThis condition means the optimal break point $s$ is the arithmetic mean of the plateau values $c_1$ and $c_2$.\n\nWe now have a system of three equations for the three unknowns $c_1, c_2, s$:\n1. $c_1 = \\frac{s}{2} + \\frac{\\lambda}{s}$\n2. $c_2 = \\frac{1+s}{2} - \\frac{\\lambda}{1-s}$\n3. $c_1+c_2 = 2s$\n\nSubstitute (1) and (2) into (3):\n$$\n\\left(\\frac{s}{2} + \\frac{\\lambda}{s}\\right) + \\left(\\frac{1+s}{2} - \\frac{\\lambda}{1-s}\\right) = 2s\n$$\n$$\ns + \\frac{1}{2} + \\lambda\\left(\\frac{1}{s} - \\frac{1}{1-s}\\right) = 2s\n$$\n$$\n\\frac{1}{2} - s + \\lambda\\left(\\frac{1-s-s}{s(1-s)}\\right) = 0\n$$\n$$\n\\frac{1-2s}{2} + \\lambda\\frac{1-2s}{s(1-s)} = 0\n$$\n$$\n(1-2s)\\left(\\frac{1}{2} + \\frac{\\lambda}{s(1-s)}\\right) = 0\n$$\nThis equation has two possible solutions.\nPossibility A: $1-2s=0 \\implies s = 1/2$. This solution lies in the valid range $(0,1)$.\nPossibility B: $\\frac{1}{2} + \\frac{\\lambda}{s(1-s)} = 0 \\implies s(1-s) = -2\\lambda$. This is the quadratic equation $s^2-s-2\\lambda=0$. The solutions are $s = \\frac{1 \\pm \\sqrt{1+8\\lambda}}{2}$. For $\\lambda0$, $\\sqrt{1+8\\lambda}1$. Thus, the root $\\frac{1+\\sqrt{1+8\\lambda}}{2}  1$, and the root $\\frac{1-\\sqrt{1+8\\lambda}}{2}  0$. Neither root is in the interval $(0,1)$.\n\nTherefore, the only valid optimal break point is $s^{\\star} = 1/2$.\n\nFinally, we find the optimal plateau values $c_1^{\\star}$ and $c_2^{\\star}$ by substituting $s^{\\star}=1/2$ into their expressions:\n$$\nc_1^{\\star} = c_1(1/2) = \\frac{1/2}{2} + \\frac{\\lambda}{1/2} = \\frac{1}{4} + 2\\lambda\n$$\n$$\nc_2^{\\star} = c_2(1/2) = \\frac{1+1/2}{2} - \\frac{\\lambda}{1-1/2} = \\frac{3/2}{2} - \\frac{\\lambda}{1/2} = \\frac{3}{4} - 2\\lambda\n$$\nWe check our initial assumption that $c_1  c_2$:\n$$\nc_2^{\\star} - c_1^{\\star} = \\left(\\frac{3}{4} - 2\\lambda\\right) - \\left(\\frac{1}{4} + 2\\lambda\\right) = \\frac{1}{2} - 4\\lambda\n$$\nThe condition $c_2^{\\star}  c_1^{\\star}$ requires $\\frac{1}{2} - 4\\lambda  0 \\implies \\lambda  \\frac{1}{8}$. This is consistent with the problem's assumption, which justifies our replacement of $|c_2-c_1|$ with $c_2-c_1$.\n\nThe optimal parameters are:\n$c_1^{\\star} = \\frac{1}{4} + 2\\lambda$\n$c_2^{\\star} = \\frac{3}{4} - 2\\lambda$\n$s^{\\star} = \\frac{1}{2}$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{4} + 2\\lambda  \\frac{3}{4} - 2\\lambda  \\frac{1}{2} \\end{pmatrix}}\n$$", "id": "3420945"}, {"introduction": "While staircasing can be an unwanted artifact, Total Variation regularization is prized for its ability to preserve sharp discontinuities. This practice problem illustrates the fundamental trade-off by analyzing a signal with both a strong edge and gentle slopes [@problem_id:3420951]. You will quantify both the impressive edge preservation and the simultaneous creation of spurious plateaus, gaining insight into the dual behavior of the TV penalty.", "problem": "Consider one-dimensional Total Variation (TV) regularization in the context of state estimation for inverse problems and data assimilation. Let the unknown signal be a vector $u \\in \\mathbb{R}^{6}$ reconstructed from observations $f \\in \\mathbb{R}^{6}$ by minimizing the Rudin–Osher–Fatemi (ROF) functional\n$$\nJ(u) \\;=\\; \\frac{1}{2}\\sum_{i=1}^{6}\\big(u_{i}-f_{i}\\big)^{2} \\;+\\; \\lambda\\sum_{i=1}^{5}\\big|u_{i+1}-u_{i}\\big|,\n$$\nwhere $\\lambda0$ is the regularization parameter and the one-dimensional discrete Total Variation (TV) is $\\sum_{i=1}^{5}\\big|u_{i+1}-u_{i}\\big|$. This framework is widely used to preserve sharp features while suppressing oscillations.\n\nLet the observation vector encode a strong edge with gentle ramps away from it:\n$$\nf \\;=\\; \\big(0,\\;\\varepsilon,\\;2\\varepsilon,\\;M,\\;M+\\varepsilon,\\;M+2\\varepsilon\\big),\n$$\nwith parameters $M0$ and $\\varepsilon0$. Assume the regularization parameter obeys\n$$\n3\\varepsilon \\;\\; \\lambda \\;\\; \\frac{3}{2}M,\n$$\nand adopt the discrete ROF model specified above.\n\nStarting from first principles of convex optimization and the subdifferential characterization of the discrete Total Variation, derive the structure of the minimizer that preserves the strong edge at the interface between indices $3$ and $4$ while producing spurious plateaus away from that interface. Use these principles to compute, in closed form, the following two quantities:\n\n- The edge-preservation factor defined by\n$$\nE \\;=\\; \\frac{\\text{jump of the reconstructed signal at the edge}}{\\text{jump in the observations at the edge}} \\;=\\; \\frac{u_{4}-u_{3}}{f_{4}-f_{3}}.\n$$\n\n- A quantitative measure of staircasing away from the edge defined as the mean squared flattening on the left plateau,\n$$\nP \\;=\\; \\frac{1}{3}\\sum_{i=1}^{3}\\big(u_{i}-f_{i}\\big)^{2}.\n$$\n\nExpress your final answer as a single closed-form expression in terms of $M$, $\\varepsilon$, and $\\lambda$. No numerical approximation is required, and no units are involved. Present both $E$ and $P$ together as a single row matrix.", "solution": "The problem asks for the derivation of the minimizer of a one-dimensional Rudin–Osher–Fatemi (ROF) functional and the computation of two related quantities: an edge-preservation factor and a staircasing measure.\n\nFirst, we perform the validation of the problem statement.\n\n### Step 1: Extract Givens\n-   The unknown signal is a vector $u \\in \\mathbb{R}^{6}$.\n-   The observation vector is $f \\in \\mathbb{R}^{6}$, given by $f = (0, \\varepsilon, 2\\varepsilon, M, M+\\varepsilon, M+2\\varepsilon)$, with $M0$ and $\\varepsilon0$.\n-   The objective functional to be minimized is $J(u) = \\frac{1}{2}\\sum_{i=1}^{6}(u_{i}-f_{i})^{2} + \\lambda\\sum_{i=1}^{5}|u_{i+1}-u_{i}|$.\n-   The regularization parameter $\\lambda$ is positive and satisfies the condition $3\\varepsilon  \\lambda  \\frac{3}{2}M$.\n-   The quantities to compute are:\n    1.  The edge-preservation factor $E = \\frac{u_{4}-u_{3}}{f_{4}-f_{3}}$.\n    2.  The mean squared flattening on the left plateau $P = \\frac{1}{3}\\sum_{i=1}^{3}(u_{i}-f_{i})^{2}$.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is formulated using the standard ROF model for Total Variation (TV) regularization, a cornerstone of modern signal and image processing for solving inverse problems. The formulation is mathematically and physically sound.\n-   **Well-Posed**: The functional $J(u)$ is the sum of a strictly convex, differentiable function (the data fidelity term $\\frac{1}{2}\\|u-f\\|_2^2$) and a convex, non-differentiable function (the TV-seminorm). The sum $J(u)$ is strictly convex, which guarantees the existence and uniqueness of a minimizer.\n-   **Objective**: The problem is stated in precise mathematical terms, with all variables and objectives clearly defined.\n-   **Completeness**: The problem provides all necessary information, including the explicit form of the data vector $f$ and a specific range for the regularization parameter $\\lambda$, which is crucial for determining the structure of the solution. The setup is self-contained and consistent.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-defined, self-contained, and scientifically grounded optimization problem. We proceed to solve it.\n\n### Solution Derivation\n\nThe minimizer $u$ of the convex functional $J(u)$ is characterized by the first-order optimality condition $0 \\in \\partial J(u)$, where $\\partial J(u)$ is the subdifferential of $J(u)$. The functional is $J(u) = F(u) + \\lambda G(u)$, where $F(u) = \\frac{1}{2}\\sum_{i=1}^{6}(u_i-f_i)^2$ and $G(u) = \\sum_{i=1}^{5}|u_{i+1}-u_i|$.\nThe subdifferential of $J(u)$ is $\\partial J(u) = \\nabla F(u) + \\lambda \\partial G(u)$.\nThe gradient of $F(u)$ is $\\nabla F(u) = u-f$.\nThe subdifferential of $G(u)$ is more complex. For each component $u_k$, the optimality condition is:\n$$ (u_k - f_k) + \\lambda \\partial_{u_k}G = 0 $$\nLet $p_i \\in \\partial|u_{i+1}-u_i|$, which means $p_i = \\text{sgn}(u_{i+1}-u_i)$ if $u_{i+1} \\neq u_i$ and $p_i \\in [-1, 1]$ if $u_{i+1}=u_i$. The subgradient components of $G(u)$ are:\n$\\partial_{u_1}G \\ni -p_1$\n$\\partial_{u_k}G \\ni p_{k-1} - p_k$ for $k=2, \\ldots, 5$\n$\\partial_{u_6}G \\ni p_5$\nThis gives the system of optimality conditions for $k=1, \\ldots, 6$:\n\\begin{align*} u_1 - f_1 - \\lambda p_1 = 0 \\\\ u_2 - f_2 + \\lambda p_1 - \\lambda p_2 = 0 \\\\ u_3 - f_3 + \\lambda p_2 - \\lambda p_3 = 0 \\\\ u_4 - f_4 + \\lambda p_3 - \\lambda p_4 = 0 \\\\ u_5 - f_5 + \\lambda p_4 - \\lambda p_5 = 0 \\\\ u_6 - f_6 + \\lambda p_5 = 0 \\end{align*}\nThe problem statement suggests a solution with plateaus away from the edge at index $3$. We hypothesize that the solution has the structure $u_1=u_2=u_3=c_1$ and $u_4=u_5=u_6=c_2$ for some constants $c_1$ and $c_2$.\nThis hypothesis implies $u_2-u_1=0$, $u_3-u_2=0$, $u_5-u_4=0$, and $u_6-u_5=0$. Thus, the subgradient variables $p_1, p_2, p_4, p_5$ must lie in the interval $[-1, 1]$.\nThe data $f$ has a large jump $f_4-f_3 = M-2\\varepsilon$. The given condition $\\lambda  \\frac{3}{2}M$ suggests this jump will be preserved, so we assume $u_4  u_3$. This implies $p_3 = \\text{sgn}(u_4-u_3) = 1$.\n\nWe can find $c_1$ and $c_2$ by summing the optimality equations over the hypothesized plateaus.\nSumming the first three equations:\n$$ (u_1-f_1) + (u_2-f_2) + (u_3-f_3) - \\lambda p_3 = 0 $$\nSubstituting $u_1=u_2=u_3=c_1$, $p_3=1$, and the values for $f_1, f_2, f_3$:\n$$ 3c_1 - (f_1+f_2+f_3) - \\lambda = 0 $$\n$$ 3c_1 - (0+\\varepsilon+2\\varepsilon) - \\lambda = 0 \\implies 3c_1 - 3\\varepsilon - \\lambda = 0 $$\n$$ c_1 = \\varepsilon + \\frac{\\lambda}{3} $$\nSumming the last three equations:\n$$ (u_4-f_4) + (u_5-f_5) + (u_6-f_6) + \\lambda p_3 = 0 $$\nSubstituting $u_4=u_5=u_6=c_2$, $p_3=1$, and the values for $f_4, f_5, f_6$:\n$$ 3c_2 - (f_4+f_5+f_6) + \\lambda = 0 $$\n$$ 3c_2 - (M+M+\\varepsilon+M+2\\varepsilon) + \\lambda = 0 \\implies 3c_2 - (3M+3\\varepsilon) + \\lambda = 0 $$\n$$ c_2 = M + \\varepsilon - \\frac{\\lambda}{3} $$\nWith $c_1$ and $c_2$ found, the proposed minimizer is $u = (c_1, c_1, c_1, c_2, c_2, c_2)$. We must verify that this solution is consistent with all subgradient conditions.\n1.  Check $u_4  u_3$: $u_4-u_3 = c_2 - c_1 = (M+\\varepsilon-\\frac{\\lambda}{3}) - (\\varepsilon+\\frac{\\lambda}{3}) = M - \\frac{2\\lambda}{3}$. For this to be positive, we need $M  \\frac{2\\lambda}{3}$, or $\\lambda  \\frac{3}{2}M$. This is given in the problem statement. So, $p_3=1$ is correct.\n2.  Find $p_1, p_2, p_4, p_5$ and check if they are in $[-1, 1]$. From the individual optimality equations:\n    - $p_1 = \\frac{u_1-f_1}{\\lambda} = \\frac{c_1-0}{\\lambda} = \\frac{\\varepsilon + \\lambda/3}{\\lambda} = \\frac{1}{3} + \\frac{\\varepsilon}{\\lambda}$.\n    - $p_2-p_1 = \\frac{u_2-f_2}{\\lambda} = \\frac{c_1-\\varepsilon}{\\lambda} = \\frac{\\lambda/3}{\\lambda} = \\frac{1}{3} \\implies p_2 = p_1+\\frac{1}{3} = \\frac{2}{3} + \\frac{\\varepsilon}{\\lambda}$.\n    - $p_4-p_3 = \\frac{u_4-f_4}{\\lambda} = \\frac{c_2-M}{\\lambda} = \\frac{\\varepsilon-\\lambda/3}{\\lambda} = \\frac{\\varepsilon}{\\lambda}-\\frac{1}{3}$. With $p_3=1$, $p_4=1+\\frac{\\varepsilon}{\\lambda}-\\frac{1}{3} = \\frac{2}{3}+\\frac{\\varepsilon}{\\lambda}$.\n    - $p_5-p_4 = \\frac{u_5-f_5}{\\lambda} = \\frac{c_2-(M+\\varepsilon)}{\\lambda} = \\frac{-\\lambda/3}{\\lambda} = -\\frac{1}{3} \\implies p_5 = p_4-\\frac{1}{3} = \\frac{1}{3}+\\frac{\\varepsilon}{\\lambda}$.\nThe conditions $|p_i| \\le 1$ for $i \\in \\{1,2,4,5\\}$ must hold. Since $\\varepsilon0, \\lambda0$, we only need to check the upper bound.\n$|p_2| = |p_4| = \\frac{2}{3} + \\frac{\\varepsilon}{\\lambda} \\le 1 \\implies \\frac{\\varepsilon}{\\lambda} \\le \\frac{1}{3} \\implies \\lambda \\ge 3\\varepsilon$. The problem states the stricter condition $\\lambda  3\\varepsilon$, so we have $|p_2|1$ and $|p_4|1$.\n$|p_1| = |p_5| = \\frac{1}{3} + \\frac{\\varepsilon}{\\lambda}$. Since $\\lambda  3\\varepsilon$, we have $\\frac{\\varepsilon}{\\lambda}  \\frac{1}{3}$, so $|p_1|  \\frac{1}{3}+\\frac{1}{3} = \\frac{2}{3}  1$.\nAll conditions are satisfied, so our hypothesized structure is correct. The unique minimizer is $u_1=u_2=u_3=\\varepsilon+\\frac{\\lambda}{3}$ and $u_4=u_5=u_6=M+\\varepsilon-\\frac{\\lambda}{3}$.\n\nNow, we compute the required quantities $E$ and $P$.\n\n1.  **Edge-preservation factor $E$**:\n    $E = \\frac{u_{4}-u_{3}}{f_{4}-f_{3}}$\n    The jump in the reconstructed signal is $u_4-u_3 = c_2 - c_1 = M - \\frac{2\\lambda}{3}$.\n    The jump in the observations is $f_4-f_3 = M - 2\\varepsilon$.\n    $$ E = \\frac{M - \\frac{2\\lambda}{3}}{M - 2\\varepsilon} $$\n\n2.  **Staircasing measure $P$**:\n    $P = \\frac{1}{3}\\sum_{i=1}^{3}(u_{i}-f_{i})^{2}$\n    Since $u_1=u_2=u_3=c_1=\\varepsilon+\\frac{\\lambda}{3}$, we compute the residuals:\n    $u_1-f_1 = c_1 - 0 = \\varepsilon + \\frac{\\lambda}{3}$\n    $u_2-f_2 = c_1 - \\varepsilon = \\frac{\\lambda}{3}$\n    $u_3-f_3 = c_1 - 2\\varepsilon = \\frac{\\lambda}{3} - \\varepsilon$\n    Substituting these into the expression for $P$:\n    $$ P = \\frac{1}{3}\\left[ \\left(\\varepsilon + \\frac{\\lambda}{3}\\right)^2 + \\left(\\frac{\\lambda}{3}\\right)^2 + \\left(\\frac{\\lambda}{3} - \\varepsilon\\right)^2 \\right] $$\n    $$ P = \\frac{1}{3}\\left[ \\left(\\varepsilon^2 + \\frac{2\\varepsilon\\lambda}{3} + \\frac{\\lambda^2}{9}\\right) + \\frac{\\lambda^2}{9} + \\left(\\frac{\\lambda^2}{9} - \\frac{2\\varepsilon\\lambda}{3} + \\varepsilon^2\\right) \\right] $$\n    $$ P = \\frac{1}{3}\\left[ 2\\varepsilon^2 + 3 \\frac{\\lambda^2}{9} \\right] = \\frac{1}{3}\\left[ 2\\varepsilon^2 + \\frac{\\lambda^2}{3} \\right] $$\n    $$ P = \\frac{2}{3}\\varepsilon^2 + \\frac{\\lambda^2}{9} $$\nThe final answer is the pair $(E, P)$ presented as a row matrix.", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{M - \\frac{2\\lambda}{3}}{M - 2\\varepsilon}  \\frac{2}{3}\\varepsilon^{2} + \\frac{\\lambda^{2}}{9} \\end{pmatrix} } $$", "id": "3420951"}, {"introduction": "The severity of the staircasing effect is directly controlled by the regularization parameter $\\lambda$. This hands-on problem delves into the mechanics of this control by asking you to trace the evolution of a denoised signal as $\\lambda$ increases [@problem_id:3420941]. Using the dual formulation or the \"taut string\" characterization, you will determine the exact thresholds at which distinct signal levels merge into larger plateaus, revealing the discrete process of information simplification driven by TV regularization.", "problem": "Consider a one-dimensional discrete denoising problem on the interval $[0,1]$ with a uniform grid of $n=4$ points at locations $t_{i} \\in \\{0, \\tfrac{1}{3}, \\tfrac{2}{3}, 1\\}$. The observed data are noisy step values\n$$\nf_{1} = 0.02,\\quad f_{2} = 0.08,\\quad f_{3} = 0.95,\\quad f_{4} = 1.10.\n$$\nFor a regularization parameter $\\lambda \\ge 0$, consider the one-dimensional Rudin–Osher–Fatemi (ROF) total variation (TV) regularization problem\n$$\n\\min_{u \\in \\mathbb{R}^{4}} \\ \\frac{1}{2}\\sum_{i=1}^{4} (u_{i} - f_{i})^{2} \\ + \\ \\lambda \\sum_{i=1}^{3} |u_{i+1} - u_{i}|.\n$$\nIt is known from the taut string characterization of one-dimensional total variation denoising that the solution can be obtained by taking the discrete derivative of a shortest path that lies within a strip of half-width $\\lambda$ around the cumulative sum of the data, connecting the endpoints of that cumulative curve. Using only fundamental principles (convex optimality and subgradient conditions for the TV term) and the taut string characterization as a definition, do the following:\n\n- Explicitly construct, for each regime of $\\lambda$, the taut string corresponding to the cumulative data $F_{k} := \\sum_{i=1}^{k} f_{i}$, $k=0,1,2,3,4$, with $F_{0} := 0$, and hence obtain the TV-denoised solution $u_{\\lambda}$.\n- Determine, as a function of $\\lambda$, the number of plateau regions (maximal contiguous index sets on which $u_{\\lambda}$ is constant) and their locations (which indices belong to each plateau).\n\nFinally, let $N(\\lambda)$ denote the number of plateau regions in $u_{\\lambda}$. Provide $N(\\lambda)$ as a single piecewise-defined analytic expression in $\\lambda$. No rounding is required.", "solution": "We begin from the convex optimization problem\n$$\n\\min_{u \\in \\mathbb{R}^{4}} \\ J_{\\lambda}(u) := \\frac{1}{2}\\sum_{i=1}^{4} (u_{i} - f_{i})^{2} + \\lambda \\sum_{i=1}^{3} |u_{i+1} - u_{i}|.\n$$\nThe total variation (TV) term is the $\\ell^{1}$ norm of first differences. A standard characterization of the optimality conditions uses the discrete difference operator $D \\in \\mathbb{R}^{3 \\times 4}$, with $(Du)_{i} = u_{i+1} - u_{i}$ for $i \\in \\{1,2,3\\}$, and its transpose $D^{\\top}$. The subdifferential of the TV term is given via $D^{\\top}s$, where $s \\in \\mathbb{R}^{3}$ satisfies $s_{i} \\in \\partial |(Du)_{i}|$, i.e., $s_{i} \\in [-1,1]$ with $s_{i} = \\operatorname{sign}((Du)_{i})$ whenever $(Du)_{i} \\neq 0$. The first-order optimality condition is\n$$\nu - f + \\lambda D^{\\top} s = 0.\n$$\nA dual and equivalent formulation, sometimes presented as the discrete taut string method, introduces $p \\in \\mathbb{R}^{3}$ with $|p_{i}| \\le \\lambda$ and $p_{0} := 0$, $p_{4} := 0$, so that the unique minimizer $u_{\\lambda}$ satisfies\n$$\nu = f + D^{\\top} p, \\quad \\text{that is,} \\quad\n\\begin{cases}\nu_{1} = f_{1} + p_{0} - p_{1} = f_{1} - p_{1},\\\\\nu_{2} = f_{2} + p_{1} - p_{2},\\\\\nu_{3} = f_{3} + p_{2} - p_{3},\\\\\nu_{4} = f_{4} + p_{3} - p_{4} = f_{4} + p_{3},\n\\end{cases}\n$$\ntogether with the complementarity condition that $|p_{i}|  \\lambda$ if and only if $u_{i+1} = u_{i}$ (a plateau between indices $i$ and $i+1$), and $|p_{i}| = \\lambda$ with $p_{i} = \\lambda \\operatorname{sign}(u_{i+1} - u_{i})$ when $u_{i+1} \\neq u_{i}$. The dual variable $p$ minimizes the strictly convex quadratic\n$$\n\\min_{|p_{i}| \\le \\lambda} \\ \\frac{1}{2}\\sum_{i=1}^{4} (f_{i} + p_{i-1} - p_{i})^{2}.\n$$\nThe unconstrained minimizer $p^{\\star}$ solves the linear system obtained by setting the gradient to zero. Writing $g_{i}(p) := \\partial/\\partial p_{i} \\ \\tfrac{1}{2}\\sum_{k=1}^{4} (f_{k} + p_{k-1} - p_{k})^{2}$ and setting $g_{i}(p) = 0$ yields\n$$\n\\begin{aligned}\n2 p_{1} - p_{2} + (f_{2} - f_{1}) = 0,\\\\\n-p_{1} + 2 p_{2} - p_{3} + (f_{3} - f_{2}) = 0,\\\\\n-p_{2} + 2 p_{3} + (f_{4} - f_{3}) = 0.\n\\end{aligned}\n$$\nWith the given data $f_{1} = 0.02$, $f_{2} = 0.08$, $f_{3} = 0.95$, $f_{4} = 1.10$, these become\n$$\n\\begin{aligned}\n2 p_{1} - p_{2} + 0.06 = 0,\\\\\n-p_{1} + 2 p_{2} - p_{3} + 0.87 = 0,\\\\\n-p_{2} + 2 p_{3} + 0.15 = 0.\n\\end{aligned}\n$$\nSolving this tridiagonal system gives the unconstrained solution\n$$\np^{\\star} = \\begin{pmatrix} -0.5175 \\\\ -0.975 \\\\ -0.5625 \\end{pmatrix}.\n$$\nThe box constraints $|p_{i}| \\le \\lambda$ define regimes as $\\lambda$ increases. At each $\\lambda$, the Karush–Kuhn–Tucker (KKT) conditions are:\n- If $p_{i} \\in (-\\lambda,\\lambda)$, then $g_{i}(p) = 0$ (interior).\n- If $p_{i} = -\\lambda$, then $g_{i}(p) \\ge 0$.\n- If $p_{i} = \\lambda$, then $g_{i}(p) \\le 0$.\nGiven $p^{\\star}_{i}  0$ for all $i$, the lower bound $p_{i} = -\\lambda$ is the relevant active face. We now track which coordinates are at the bound as $\\lambda$ grows.\n\nDefine the gradients explicitly:\n$$\ng_{1}(p) = 2 p_{1} - p_{2} + 0.06,\\quad\ng_{2}(p) = -p_{1} + 2 p_{2} - p_{3} + 0.87,\\quad\ng_{3}(p) = -p_{2} + 2 p_{3} + 0.15.\n$$\n\nRegime I: $0 \\le \\lambda \\le 0.06$.\nAssume $p = (-\\lambda,-\\lambda,-\\lambda)$. Then\n$$\ng_{1} = -\\lambda + 0.06 \\ge 0,\\quad g_{2} = 0.87 \\ge 0,\\quad g_{3} = -\\lambda + 0.15 \\ge 0,\n$$\nwhich holds for $\\lambda \\le 0.06$. Thus $p = (-\\lambda,-\\lambda,-\\lambda)$. The solution\n$$\n\\begin{aligned}\nu_{1} = f_{1} - p_{1} = 0.02 + \\lambda,\\\\\nu_{2} = f_{2} + p_{1} - p_{2} = 0.08,\\\\\nu_{3} = f_{3} + p_{2} - p_{3} = 0.95,\\\\\nu_{4} = f_{4} + p_{3} = 1.10 - \\lambda.\n\\end{aligned}\n$$\nThe differences are $u_{2} - u_{1} = 0.06 - \\lambda \\ge 0$, $u_{3} - u_{2} = 0.87  0$, $u_{4} - u_{3} = 0.15 - \\lambda \\ge 0$, with strict inequality for $0 \\le \\lambda  0.06$. Hence there are four distinct levels (no plateaus across indices) for $0 \\le \\lambda  0.06$.\n\nThe taut string is the cumulative sum $U_{k} := \\sum_{i=1}^{k} u_{i}$:\n$$\nU_{0} = 0,\\quad U_{1} = 0.02 + \\lambda,\\quad U_{2} = 0.10 + \\lambda,\\quad U_{3} = 1.05 + \\lambda,\\quad U_{4} = 2.15,\n$$\nwhich lies within a strip of half-width $\\lambda$ around $F_{k} := \\sum_{i=1}^{k} f_{i}$, touching the lower wall at each step.\n\nRegime II: $0.06 \\le \\lambda \\le 0.15$.\nHere $p_{1}$ leaves the bound while $p_{2} = -\\lambda$, $p_{3} = -\\lambda$ remain on the bound. Set $g_{1} = 0$ to solve for $p_{1}$:\n$$\n2 p_{1} - (-\\lambda) + 0.06 = 0 \\ \\Rightarrow \\ p_{1} = -\\frac{\\lambda + 0.06}{2}.\n$$\nCheck $|p_{1}|  \\lambda$ for $\\lambda  0.06$, and for the bound coordinates:\n$$\ng_{2} = -p_{1} - \\lambda + 0.87 = -\\left(-\\frac{\\lambda + 0.06}{2}\\right) - \\lambda + 0.87 = -\\frac{\\lambda}{2} + 0.90 \\ge 0,\n$$\nwhich holds in this regime, and\n$$\ng_{3} = -(-\\lambda) + 2(-\\lambda) + 0.15 = -\\lambda + 0.15 \\ge 0,\n$$\nvalid for $\\lambda \\le 0.15$. Thus\n$$\n\\begin{aligned}\nu_{1} = 0.02 - p_{1} = 0.05 + 0.5 \\lambda,\\\\\nu_{2} = 0.08 + p_{1} - p_{2} = 0.05 + 0.5 \\lambda,\\\\\nu_{3} = 0.95,\\\\\nu_{4} = 1.10 - \\lambda.\n\\end{aligned}\n$$\nHence a plateau forms on indices $\\{1,2\\}$, with singleton plateaus at $\\{3\\}$ and $\\{4\\}$. The taut string cumulative sums are\n$$\nU_{0} = 0,\\quad U_{1} = 0.05 + 0.5\\lambda,\\quad U_{2} = 0.10 + \\lambda,\\quad U_{3} = 1.05 + \\lambda,\\quad U_{4} = 2.15.\n$$\n\nRegime III: $0.15 \\le \\lambda \\le 0.975$.\nNow $p_{3}$ also leaves the bound while $p_{2} = -\\lambda$ stays on the bound. Set $g_{1} = 0$, $g_{3} = 0$:\n$$\np_{1} = -\\frac{\\lambda + 0.06}{2},\\qquad p_{3} = -\\frac{\\lambda + 0.15}{2}.\n$$\nCheck the bound KKT for $p_{2}$:\n$$\ng_{2} = -p_{1} + 2(-\\lambda) - p_{3} + 0.87 = -\\lambda + 0.975 \\ge 0 \\quad \\text{for} \\quad \\lambda \\le 0.975.\n$$\nThus\n$$\n\\begin{aligned}\nu_{1} = 0.05 + 0.5 \\lambda,\\\\\nu_{2} = 0.05 + 0.5 \\lambda,\\\\\nu_{3} = 0.95 - \\frac{\\lambda}{2} + 0.075 = 1.025 - 0.5 \\lambda,\\\\\nu_{4} = 1.10 - \\frac{\\lambda + 0.15}{2} = 1.025 - 0.5 \\lambda.\n\\end{aligned}\n$$\nHence there are two plateau regions: $\\{1,2\\}$ and $\\{3,4\\}$. The taut string cumulative sums are\n$$\nU_{0} = 0,\\quad U_{1} = 0.05 + 0.5\\lambda,\\quad U_{2} = 0.10 + \\lambda,\\quad U_{3} = 1.125 + 0.5\\lambda,\\quad U_{4} = 2.15.\n$$\n\nRegime IV: $\\lambda \\ge 0.975$.\nAll coordinates become interior; the minimizer coincides with the unconstrained $p^{\\star}$. Since $|p^{\\star}_{i}| \\le \\lambda$ for $\\lambda \\ge 0.975$, we have $p = p^{\\star}$ and\n$$\nu = f + D^{\\top} p^{\\star} = \\begin{pmatrix} 0.5375 \\\\ 0.5375 \\\\ 0.5375 \\\\ 0.5375 \\end{pmatrix}.\n$$\nThis is a single plateau across all indices, equal to the sample mean $\\bar{f} = \\tfrac{1}{4}\\sum_{i=1}^{4} f_{i} = 0.5375$. The taut string is the straight line $U_{k} = 0.5375 \\, k$.\n\nSummary of plateau counts and locations:\n- For $0 \\le \\lambda  0.06$: four distinct levels (no merged plateaus), indices $\\{1\\}$, $\\{2\\}$, $\\{3\\}$, $\\{4\\}$.\n- For $0.06 \\le \\lambda  0.15$: three plateaus, indices $\\{1,2\\}$, $\\{3\\}$, $\\{4\\}$.\n- For $0.15 \\le \\lambda  0.975$: two plateaus, indices $\\{1,2\\}$, $\\{3,4\\}$.\n- For $\\lambda \\ge 0.975$: one plateau, indices $\\{1,2,3,4\\}$.\n\nTherefore, the number of plateau regions $N(\\lambda)$ is the piecewise-defined function\n$$\nN(\\lambda) =\n\\begin{cases}\n4,  0 \\le \\lambda  0.06,\\\\\n3,  0.06 \\le \\lambda  0.15,\\\\\n2,  0.15 \\le \\lambda  0.975,\\\\\n1,  \\lambda \\ge 0.975.\n\\end{cases}\n$$\nThis piecewise characterization is consistent with the taut string construction: as $\\lambda$ grows, the taut string detaches from the tube walls at contact points corresponding to interfaces between plateaus, decreasing the number of plateaus monotonically from four to one.", "answer": "$$\\boxed{N(\\lambda)=\\begin{cases}\n4,  0 \\le \\lambda  0.06,\\\\\n3,  0.06 \\le \\lambda  0.15,\\\\\n2,  0.15 \\le \\lambda  0.975,\\\\\n1,  \\lambda \\ge 0.975.\n\\end{cases}}$$", "id": "3420941"}]}