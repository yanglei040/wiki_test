{"hands_on_practices": [{"introduction": "Before we can devise algorithms to find a solution, we must first be able to recognize one. This practice focuses on defining the characteristics of a minimizer for a bound-constrained least-squares problem, using the powerful Karush-Kuhn-Tucker (KKT) conditions [@problem_id:3369395]. By translating the abstract KKT theory into a concrete checklist for the gradient at free and active boundaries, you will develop the essential skill of certifying optimality, which forms the stopping criteria for many sophisticated solvers.", "problem": "Consider the bound-constrained linear least squares problem in inverse problems and data assimilation: minimize the differentiable convex objective $$\\phi(x) = \\tfrac{1}{2}\\|A x - b\\|_2^2$$ subject to the box constraints $$\\ell \\le x \\le u,$$ where $$A \\in \\mathbb{R}^{m \\times n}, \\quad b \\in \\mathbb{R}^m, \\quad \\ell,u \\in \\mathbb{R}^n, \\quad \\ell_i \\le u_i \\text{ for all } i.$$ Let $g = \\nabla \\phi(x) = A^\\top (A x - b)$ be the gradient at a feasible candidate $x \\in \\mathbb{R}^n$ with $\\ell \\le x \\le u$. Define the index sets $$\\mathcal{F} = \\{ i \\in \\{1,\\dots,n\\} : \\ell_i < x_i < u_i \\}, \\quad \\mathcal{L} = \\{ i : x_i = \\ell_i < u_i \\}, \\quad \\mathcal{U} = \\{ i : x_i = u_i > \\ell_i \\}, \\quad \\mathcal{E} = \\{ i : \\ell_i = u_i \\}.$$ Using the Karush-Kuhn-Tucker (KKT) optimality conditions for convex problems with linear inequality constraints, one can certify optimality by inspecting the signs of $g_i$ at active bounds and the zeroing of $g_i$ on free components. Which of the following is a correct necessary and sufficient test, expressed purely in terms of $x$, $g$, and the bounds $\\ell,u$, that certifies optimality of $x$ for the above problem?\n\nA. $x$ is optimal if and only if $\\ell \\le x \\le u$ and $g_i = 0$ for all $i \\in \\mathcal{F},$ $g_i \\ge 0$ for all $i \\in \\mathcal{L},$ $g_i \\le 0$ for all $i \\in \\mathcal{U},$ with no additional condition on indices $i \\in \\mathcal{E}.$\n\nB. $x$ is optimal if and only if $\\ell \\le x \\le u$ and $g_i = 0$ for all $i \\in \\mathcal{F},$ $g_i > 0$ for all $i \\in \\mathcal{L},$ $g_i < 0$ for all $i \\in \\mathcal{U},$ and $g_i = 0$ for all $i \\in \\mathcal{E}.$\n\nC. $x$ is optimal if and only if $\\ell \\le x \\le u$ and the residual components satisfy $\\big(A x - b\\big)_i = 0$ for all $i \\in \\mathcal{F},$ $\\big(A x - b\\big)_i \\ge 0$ for all $i \\in \\mathcal{L},$ $\\big(A x - b\\big)_i \\le 0$ for all $i \\in \\mathcal{U},$ with no condition on $\\mathcal{E}.$\n\nD. $x$ is optimal if and only if $\\ell \\le x \\le u$ and $g_i \\le 0$ for all $i \\in \\mathcal{L},$ $g_i \\ge 0$ for all $i \\in \\mathcal{U},$ and $g_i = 0$ for all $i \\in \\mathcal{F}.$\n\nE. $x$ is optimal if and only if $\\ell \\le x \\le u$ and $g_i = 0$ for all $i \\in \\{1,\\dots,n\\}$ (that is, the unconstrained stationary condition holds componentwise).", "solution": "We derive the correct KKT-based test from first principles. The fundamental base is the Karush-Kuhn-Tucker (KKT) conditions for convex optimization with linear inequality constraints, combined with the definition of the least squares objective gradient. The problem is to minimize $\\phi(x) = \\tfrac{1}{2}\\|A x - b\\|_2^2$ subject to $\\ell \\le x \\le u$. The function $\\phi$ is convex and differentiable with gradient $\\nabla \\phi(x) = A^\\top(Ax - b) = g$. The feasible set is a hyperrectangle (a box), defined by linear inequalities $x_i \\ge \\ell_i$ and $x_i \\le u_i$. The KKT conditions are necessary and sufficient for optimality because the problem is convex and the constraints are linear.\n\nIntroduce Lagrange multipliers for the inequality constraints: $\\lambda^L \\in \\mathbb{R}^n, \\quad \\lambda^U \\in \\mathbb{R}^n$, associated with $x_i \\ge \\ell_i$ and $x_i \\le u_i$, respectively. The Lagrangian is\n$$\n\\mathcal{L}(x,\\lambda^L,\\lambda^U) = \\tfrac{1}{2}\\|A x - b\\|_2^2 \\;-\\; (\\lambda^L)^\\top (x - \\ell) \\;+\\; (\\lambda^U)^\\top (x - u).\n$$\nThe KKT conditions are:\n\n1. Primal feasibility: $\\ell \\le x \\le u$.\n\n2. Dual feasibility: $\\lambda^L \\ge 0, \\quad \\lambda^U \\ge 0$, componentwise.\n\n3. Stationarity: $\\nabla \\phi(x) - \\lambda^L + \\lambda^U = 0$, i.e.,\n$$\ng - \\lambda^L + \\lambda^U = 0 \\quad \\Longleftrightarrow \\quad g_i - \\lambda^L_i + \\lambda^U_i = 0 \\text{ for each } i.\n$$\n\n4. Complementary slackness: $\\lambda^L_i (x_i - \\ell_i) = 0, \\quad \\lambda^U_i (u_i - x_i) = 0 \\text{ for each } i$.\n\nWe now extract conditions on $g_i$ based on the activity of bounds at $x$. Consider each index $i$:\n\n- Free component $i \\in \\mathcal{F}:$ This means $\\ell_i < x_i < u_i$. Complementary slackness implies $\\lambda^L_i = 0$ and $\\lambda^U_i = 0$. Stationarity reduces to $g_i = 0$. Thus, for free components, the gradient must vanish componentwise.\n\n- Lower-bound active component $i \\in \\mathcal{L}:$ Here $x_i = \\ell_i < u_i$. Complementary slackness implies $\\lambda^L_i \\ge 0$ can be nonzero, but $\\lambda^U_i = 0$ because $u_i - x_i > 0$. Stationarity gives $g_i - \\lambda^L_i + 0 = 0$, so $g_i = \\lambda^L_i \\ge 0$. Hence, at a lower-active bound, the gradient component is nonnegative.\n\n- Upper-bound active component $i \\in \\mathcal{U}:$ Here $x_i = u_i > \\ell_i$. Complementary slackness implies $\\lambda^U_i \\ge 0$ can be nonzero, but $\\lambda^L_i = 0$ because $x_i - \\ell_i > 0$. Stationarity gives $g_i - 0 + \\lambda^U_i = 0$, so $g_i = -\\lambda^U_i \\le 0$. Hence, at an upper-active bound, the gradient component is nonpositive.\n\n- Equality-bound component $i \\in \\mathcal{E}:$ Here $\\ell_i = u_i$ and thus $x_i = \\ell_i = u_i$. Complementary slackness imposes $\\lambda^L_i (x_i - \\ell_i) = 0$ and $\\lambda^U_i (u_i - x_i) = 0$, which are identically satisfied for any nonnegative $\\lambda^L_i, \\lambda^U_i$. Stationarity imposes $g_i - \\lambda^L_i + \\lambda^U_i = 0$, which can be satisfied by some choice of nonnegative $\\lambda^L_i, \\lambda^U_i$ for any real $g_i$ (e.g., set $\\lambda^L_i = \\max\\{g_i,0\\}$ and $\\lambda^U_i = \\max\\{-g_i,0\\}$). Therefore, when $\\ell_i = u_i$, there is no sign restriction on $g_i$ beyond the existence of feasible multipliers, and the certificate requires only feasibility at such indices.\n\nCombining these cases, the KKT conditions translate to the test:\n\n- Primal feasibility $\\ell \\le x \\le u$.\n- For all $i \\in \\mathcal{F},$ $g_i = 0$.\n- For all $i \\in \\mathcal{L},$ $g_i \\ge 0$.\n- For all $i \\in \\mathcal{U},$ $g_i \\le 0$.\n- No additional requirement on $g_i$ for $i \\in \\mathcal{E}$.\n\nBecause $\\phi$ is convex and the constraints are linear, these KKT conditions are necessary and sufficient, so they form a complete certificate of optimality.\n\nWe now evaluate each option:\n\nOption A: States primal feasibility and exactly the sign/zero conditions derived above for $\\mathcal{F}, \\mathcal{L}, \\mathcal{U}$, and explicitly notes no extra condition for $\\mathcal{E}$. This matches the KKT-based derivation. Verdict: Correct.\n\nOption B: Requires strict inequalities $g_i > 0$ at $\\mathcal{L}$ and $g_i  0$ at $\\mathcal{U}$, and forces $g_i = 0$ at $\\mathcal{E}$. The strict inequalities are unnecessary and incorrect: KKT allows $g_i = 0$ at active bounds (with zero multipliers or degenerate cases), and for $\\mathcal{E}$ there is no restriction on $g_i$ as argued, since one can choose $\\lambda^L_i, \\lambda^U_i$ to satisfy stationarity. Verdict: Incorrect.\n\nOption C: Uses the residual components $\\big(A x - b\\big)_i$ rather than the gradient $g_i = A^\\top(Ax - b)_i$. The KKT stationarity condition involves $g$, not the raw residual, and equating residual components to zero on free indices is generally wrong unless $A$ is the identity. Thus this test is not necessary nor sufficient for general $A$. Verdict: Incorrect.\n\nOption D: Reverses the sign conditions at active bounds: it demands $g_i \\le 0$ at lower bounds and $g_i \\ge 0$ at upper bounds, which contradicts the stationarity with nonnegative multipliers as derived. Verdict: Incorrect.\n\nOption E: Demands $g_i = 0$ for all components, i.e., unconstrained stationarity. This is stronger than needed and fails to certify optimality at active bounds, where nonzero gradient components are admissible and required to be of appropriate sign to balance multipliers. It may only hold when the solution lies strictly in the interior or when special degeneracy occurs; it is not necessary in general. Verdict: Incorrect.\n\nTherefore, the correct test is given in Option A.", "answer": "$$\\boxed{A}$$", "id": "3369395"}, {"introduction": "A cornerstone of many iterative algorithms for constrained optimization is the projection operator, which enforces feasibility after taking an exploratory step outside the allowed region. This problem provides concrete, hands-on experience with this fundamental tool by asking you to compute the Euclidean projection of a point onto a simple box, the unique point in the box closest to the given point [@problem_id:3369454]. Mastering this simple, component-wise operation is a key building block for implementing methods like projected gradient descent.", "problem": "Let $y \\in \\mathbb{R}^{6}$ be given by $y = (3.7,\\,-5.2,\\,-3.0,\\,9.1,\\,-2.3,\\,14.8)^{\\top}$. Consider the bound-constrained least-squares projection of $y$ onto the box $[l,u] := \\{x \\in \\mathbb{R}^{6} : l \\le x \\le u\\}$, where the bounds are specified componentwise by\n$l = (0,\\,-\\infty,\\,-1,\\,4,\\,-\\infty,\\,10)^{\\top}$ and $u = (2,\\,0,\\,\\infty,\\,\\infty,\\,1,\\,12)^{\\top}$. Here $-\\infty$ and $\\infty$ denote the absence of a lower or upper bound, respectively.\n\nThe Euclidean projection $P_{[l,u]}(y)$ is defined as the unique minimizer of the strictly convex problem\n$\\min\\{ \\tfrac{1}{2}\\|x - y\\|_{2}^{2} : l \\le x \\le u \\}$.\n\nTasks:\n1. Compute $P_{[l,u]}(y)$.\n2. Starting from the definition of the projection as the unique minimizer of the above problem and using only fundamental properties of convexity and first-order optimality, verify that the variational inequality characterization holds: for all $z \\in [l,u]$,\n$\\langle y - P_{[l,u]}(y),\\, z - P_{[l,u]}(y) \\rangle \\le 0$.\n3. Report the exact value of the squared Euclidean distance $\\|y - P_{[l,u]}(y)\\|_{2}^{2}$.\n\nProvide your final numerical answer to Task 3 as an exact value (no rounding).", "solution": "The problem asks for three tasks to be completed. Let $x^* = P_{[l,u]}(y)$. The vector $y \\in \\mathbb{R}^{6}$ is given as $y = (3.7,\\,-5.2,\\,-3.0,\\,9.1,\\,-2.3,\\,14.8)^{\\top}$. The bounds are $l = (0,\\,-\\infty,\\,-1,\\,4,\\,-\\infty,\\,10)^{\\top}$ and $u = (2,\\,0,\\,\\infty,\\,\\infty,\\,1,\\,12)^{\\top}$.\n\n**Task 1: Compute $P_{[l,u]}(y)$**\n\nThe objective function to be minimized is $L(x) = \\frac{1}{2} \\|x - y\\|_{2}^{2} = \\frac{1}{2} \\sum_{i=1}^{6} (x_i - y_i)^2$. The constraints are $l_i \\le x_i \\le u_i$ for each component $i=1, \\dots, 6$. Since both the objective function and the constraints are separable, the $6$-dimensional optimization problem decouples into $6$ independent $1$-dimensional problems:\n$$\n\\min_{x_i} \\frac{1}{2} (x_i - y_i)^2 \\quad \\text{subject to} \\quad l_i \\le x_i \\le u_i\n$$\nfor each $i \\in \\{1, 2, 3, 4, 5, 6\\}$.\n\nThe solution to each $1$D problem is the projection of the scalar $y_i$ onto the interval $[l_i, u_i]$. This is given by the formula:\n$$\nx_i^* = \\max(l_i, \\min(u_i, y_i))\n$$\n\nWe apply this formula for each component:\n- For $i=1$: $y_1=3.7$, $[l_1, u_1] = [0, 2]$.\n  $x_1^* = \\max(0, \\min(2, 3.7)) = \\max(0, 2) = 2$.\n- For $i=2$: $y_2=-5.2$, $[l_2, u_2] = [-\\infty, 0]$.\n  $x_2^* = \\max(-\\infty, \\min(0, -5.2)) = \\max(-\\infty, -5.2) = -5.2$.\n- For $i=3$: $y_3=-3.0$, $[l_3, u_3] = [-1, \\infty]$.\n  $x_3^* = \\max(-1, \\min(\\infty, -3.0)) = \\max(-1, -3.0) = -1$.\n- For $i=4$: $y_4=9.1$, $[l_4, u_4] = [4, \\infty]$.\n  $x_4^* = \\max(4, \\min(\\infty, 9.1)) = \\max(4, 9.1) = 9.1$.\n- For $i=5$: $y_5=-2.3$, $[l_5, u_5] = [-\\infty, 1]$.\n  $x_5^* = \\max(-\\infty, \\min(1, -2.3)) = \\max(-\\infty, -2.3) = -2.3$.\n- For $i=6$: $y_6=14.8$, $[l_6, u_6] = [10, 12]$.\n  $x_6^* = \\max(10, \\min(12, 14.8)) = \\max(10, 12) = 12$.\n\nCombining these components, the projection is:\n$$\nx^* = P_{[l,u]}(y) = (2, -5.2, -1, 9.1, -2.3, 12)^{\\top}\n$$\n\n**Task 2: Verify the variational inequality**\n\nThe problem is to minimize a differentiable convex function $f(x) = \\frac{1}{2}\\|x-y\\|_2^2$ over a convex set $C = [l,u]$. A fundamental result in convex analysis states that a point $x^* \\in C$ is a minimizer of $f$ over $C$ if and only if it satisfies the first-order optimality condition:\n$$\n\\langle \\nabla f(x^*), z - x^* \\rangle \\ge 0, \\quad \\forall z \\in C\n$$\nThe gradient of $f(x)$ is $\\nabla f(x) = x - y$. Substituting this into the optimality condition, we get:\n$$\n\\langle x^* - y, z - x^* \\rangle \\ge 0, \\quad \\forall z \\in C\n$$\nMultiplying the inner product by $-1$ reverses the inequality:\n$$\n\\langle -(x^* - y), z - x^* \\rangle \\le 0\n$$\n$$\n\\langle y - x^*, z - x^* \\rangle \\le 0, \\quad \\forall z \\in C\n$$\nThis is precisely the variational inequality characterization of the projection. This derivation from first-order principles verifies that the characterization holds for any projection onto a convex set.\n\nWe can also verify this component-wise for the specific projection onto a box. The inequality is $\\sum_{i=1}^{6} (y_i - x_i^*)(z_i - x_i^*) \\le 0$. We show that each term in the sum is non-positive. For any $i$, consider the term $(y_i - x_i^*)(z_i - x_i^*)$, where $z_i \\in [l_i, u_i]$.\n- Case 1: $l_i  y_i  u_i$. The point $y_i$ is in the interior of the interval. The projection is $x_i^* = y_i$. The term is $(y_i - y_i)(z_i - y_i) = 0$.\n- Case 2: $y_i \\le l_i$. The projection is $x_i^* = l_i$. The factor $y_i - x_i^* = y_i - l_i \\le 0$. For any $z_i \\in [l_i, u_i]$, we have $z_i \\ge l_i$, so the factor $z_i - x_i^* = z_i - l_i \\ge 0$. The product is non-positive.\n- Case 3: $y_i \\ge u_i$. The projection is $x_i^* = u_i$. The factor $y_i - x_i^* = y_i - u_i \\ge 0$. For any $z_i \\in [l_i, u_i]$, we have $z_i \\le u_i$, so the factor $z_i - x_i^* = z_i - u_i \\le 0$. The product is non-positive.\nIn all cases, $(y_i - x_i^*)(z_i - x_i^*) \\le 0$. Summing over all $i$ confirms the variational inequality.\n\n**Task 3: Report the squared Euclidean distance**\n\nWe need to compute $\\|y - x^*\\|_{2}^{2}$. First, we compute the difference vector $d = y - x^*$:\n$d = (3.7 - 2, -5.2 - (-5.2), -3.0 - (-1), 9.1 - 9.1, -2.3 - (-2.3), 14.8 - 12)^{\\top}$\n$d = (1.7, 0, -2.0, 0, 0, 2.8)^{\\top}$\n\nThe squared Euclidean distance is the sum of the squares of the components of $d$:\n$$\n\\|y - x^*\\|_{2}^{2} = d_1^2 + d_2^2 + d_3^2 + d_4^2 + d_5^2 + d_6^2\n$$\n$$\n\\|y - x^*\\|_{2}^{2} = (1.7)^2 + (0)^2 + (-2.0)^2 + (0)^2 + (0)^2 + (2.8)^2\n$$\nWe compute the individual squared terms:\n- $(1.7)^2 = 2.89$\n- $(0)^2 = 0$\n- $(-2.0)^2 = 4.0$\n- $(2.8)^2 = 7.84$\n\nSumming these values:\n$$\n\\|y - x^*\\|_{2}^{2} = 2.89 + 0 + 4.0 + 0 + 0 + 7.84 = 14.73\n$$\nThe exact value of the squared Euclidean distance is $14.73$.", "answer": "$$\n\\boxed{14.73}\n$$", "id": "3369454"}, {"introduction": "Now, we integrate the concepts of a descent direction and a feasibility-enforcing projection into a single, powerful algorithmic step. This practice challenges you to perform one complete iteration of the Projected Gradient method, a workhorse algorithm for bound-constrained problems [@problem_id:3369390]. You will calculate the gradient to find the direction of steepest descent, use an Armijo backtracking line search to determine an appropriate step size, and apply the projection operator to ensure the next iterate remains within the bounds, demonstrating the dynamic process at the heart of constrained optimization.", "problem": "Consider a linear data assimilation setting with a state vector $\\boldsymbol{x} \\in \\mathbb{R}^{3}$ and a linear observation operator $\\boldsymbol{A} \\in \\mathbb{R}^{3 \\times 3}$, where the observations are modeled as $\\boldsymbol{y} \\approx \\boldsymbol{A}\\boldsymbol{x}$ with observed data $\\boldsymbol{b} \\in \\mathbb{R}^{3}$. We seek to minimize the least-squares misfit subject to bound constraints. The objective function is\n$$\nf(\\boldsymbol{x}) = \\frac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{b}\\|^{2},\n$$\nand the feasible set is the box $\\mathcal{B} = \\{\\boldsymbol{x} \\in \\mathbb{R}^{3} : \\boldsymbol{l} \\le \\boldsymbol{x} \\le \\boldsymbol{u}\\}$ with componentwise bounds.\n\nStarting strictly from the definition of $f(\\boldsymbol{x})$ and the rules of differentiation for compositions of linear maps and quadratic forms, derive the gradient $\\nabla f(\\boldsymbol{x})$. Then, using the Projected Gradient (PG) method with an Armijo backtracking line search, perform one iteration from the current state $\\boldsymbol{x}^{(0)}$ with the following data:\n$$\n\\boldsymbol{A} = \\begin{pmatrix} 2  0  0 \\\\ 0  1  0 \\\\ 0  0  3 \\end{pmatrix}, \\quad\n\\boldsymbol{b} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 3 \\end{pmatrix}, \\quad\n\\boldsymbol{x}^{(0)} = \\begin{pmatrix} 0.8 \\\\ -0.5 \\\\ 1.8 \\end{pmatrix},\n$$\nand bounds\n$$\n\\boldsymbol{l} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 0.5 \\end{pmatrix}, \\quad\n\\boldsymbol{u} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix}.\n$$\nDefine the projection onto the box $\\mathcal{B}$ componentwise by $P_{[\\boldsymbol{l},\\boldsymbol{u}]}(\\boldsymbol{z}) = \\min(\\max(\\boldsymbol{z}, \\boldsymbol{l}), \\boldsymbol{u})$, where the minimum and maximum are taken componentwise. Use an initial step size $\\alpha_{0} = 1$, a backtracking factor $\\gamma = \\frac{1}{2}$, and an Armijo constant $\\sigma = \\frac{1}{10}$. At a current iterate $\\boldsymbol{x}$ with gradient $\\boldsymbol{g} = \\nabla f(\\boldsymbol{x})$, define the trial point for step size $\\alpha$ by\n$$\n\\boldsymbol{y}(\\alpha) = P_{[\\boldsymbol{l},\\boldsymbol{u}]}\\big(\\boldsymbol{x} - \\alpha \\boldsymbol{g}\\big).\n$$\nAccept a step size $\\alpha$ if the Armijo condition\n$$\nf\\big(\\boldsymbol{y}(\\alpha)\\big) \\le f(\\boldsymbol{x}) + \\sigma\\, \\nabla f(\\boldsymbol{x})^{\\top}\\big(\\boldsymbol{y}(\\alpha) - \\boldsymbol{x}\\big)\n$$\nis satisfied; otherwise replace $\\alpha \\leftarrow \\gamma \\alpha$ and repeat until acceptance. Carry out this procedure for one iteration starting from $\\boldsymbol{x}^{(0)}$ with the numerical values above and report the resulting next iterate $\\boldsymbol{x}^{(1)} = \\boldsymbol{y}(\\alpha)$ as your final answer. No rounding is required. Express the final answer as a row vector.", "solution": "The problem asks for one iteration of the Projected Gradient (PG) method for a bound-constrained least-squares problem. The process involves first deriving the gradient of the objective function, then applying the PG algorithm with an Armijo backtracking line search.\n\nFirst, we derive the gradient of the objective function $f(\\boldsymbol{x}) = \\frac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{b}\\|^{2}$.\nThe squared Euclidean norm can be written as an inner product:\n$$\nf(\\boldsymbol{x}) = \\frac{1}{2} (\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{b})^{\\top}(\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{b})\n$$\nExpanding this expression gives:\n$$\nf(\\boldsymbol{x}) = \\frac{1}{2} \\left( (\\boldsymbol{A}\\boldsymbol{x})^{\\top} - \\boldsymbol{b}^{\\top} \\right) (\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{b}) = \\frac{1}{2} \\left( \\boldsymbol{x}^{\\top}\\boldsymbol{A}^{\\top}\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{x}^{\\top}\\boldsymbol{A}^{\\top}\\boldsymbol{b} - \\boldsymbol{b}^{\\top}\\boldsymbol{A}\\boldsymbol{x} + \\boldsymbol{b}^{\\top}\\boldsymbol{b} \\right)\n$$\nSince $\\boldsymbol{b}^{\\top}\\boldsymbol{A}\\boldsymbol{x}$ is a scalar, it is equal to its own transpose, $(\\boldsymbol{b}^{\\top}\\boldsymbol{A}\\boldsymbol{x})^{\\top} = \\boldsymbol{x}^{\\top}\\boldsymbol{A}^{\\top}\\boldsymbol{b}$. Thus, the expression simplifies to:\n$$\nf(\\boldsymbol{x}) = \\frac{1}{2} \\boldsymbol{x}^{\\top}\\boldsymbol{A}^{\\top}\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{b}^{\\top}\\boldsymbol{A}\\boldsymbol{x} + \\frac{1}{2} \\boldsymbol{b}^{\\top}\\boldsymbol{b}\n$$\nTo find the gradient $\\nabla f(\\boldsymbol{x})$, we differentiate with respect to $\\boldsymbol{x}$. Using the rules for vector calculus, specifically $\\nabla_{\\boldsymbol{x}}(\\boldsymbol{x}^{\\top}\\boldsymbol{Q}\\boldsymbol{x}) = (\\boldsymbol{Q} + \\boldsymbol{Q}^{\\top})\\boldsymbol{x}$ and $\\nabla_{\\boldsymbol{x}}(\\boldsymbol{c}^{\\top}\\boldsymbol{x}) = \\boldsymbol{c}$, we get:\n$$\n\\nabla f(\\boldsymbol{x}) = \\frac{1}{2} (\\boldsymbol{A}^{\\top}\\boldsymbol{A} + (\\boldsymbol{A}^{\\top}\\boldsymbol{A})^{\\top})\\boldsymbol{x} - (\\boldsymbol{b}^{\\top}\\boldsymbol{A})^{\\top}\n$$\nThe matrix $\\boldsymbol{A}^{\\top}\\boldsymbol{A}$ is symmetric, so $(\\boldsymbol{A}^{\\top}\\boldsymbol{A})^{\\top} = \\boldsymbol{A}^{\\top}\\boldsymbol{A}$. Also, $(\\boldsymbol{b}^{\\top}\\boldsymbol{A})^{\\top} = \\boldsymbol{A}^{\\top}\\boldsymbol{b}$. The gradient is therefore:\n$$\n\\nabla f(\\boldsymbol{x}) = \\frac{1}{2} (2\\boldsymbol{A}^{\\top}\\boldsymbol{A})\\boldsymbol{x} - \\boldsymbol{A}^{\\top}\\boldsymbol{b} = \\boldsymbol{A}^{\\top}\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{A}^{\\top}\\boldsymbol{b} = \\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{x} - \\boldsymbol{b})\n$$\nThis is the required expression for the gradient.\n\nNow, we perform one iteration of the PG method starting from $\\boldsymbol{x}^{(0)}$.\nThe current iterate is $\\boldsymbol{x}^{(0)} = \\begin{pmatrix} 0.8 \\\\ -0.5 \\\\ 1.8 \\end{pmatrix}$. The matrices and vectors are:\n$$\n\\boldsymbol{A} = \\begin{pmatrix} 2  0  0 \\\\ 0  1  0 \\\\ 0  0  3 \\end{pmatrix}, \\quad \\boldsymbol{b} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 3 \\end{pmatrix}, \\quad \\boldsymbol{l} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 0.5 \\end{pmatrix}, \\quad \\boldsymbol{u} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix}\n$$\nFirst, we compute the gradient $\\boldsymbol{g}^{(0)} = \\nabla f(\\boldsymbol{x}^{(0)})$.\nThe residual is $\\boldsymbol{A}\\boldsymbol{x}^{(0)} - \\boldsymbol{b}$:\n$$\n\\boldsymbol{A}\\boldsymbol{x}^{(0)} - \\boldsymbol{b} = \\begin{pmatrix} 2  0  0 \\\\ 0  1  0 \\\\ 0  0  3 \\end{pmatrix} \\begin{pmatrix} 0.8 \\\\ -0.5 \\\\ 1.8 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 1.6 \\\\ -0.5 \\\\ 5.4 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0.6 \\\\ 0.5 \\\\ 2.4 \\end{pmatrix}\n$$\nThe matrix $\\boldsymbol{A}$ is diagonal and thus symmetric, so $\\boldsymbol{A}^{\\top} = \\boldsymbol{A}$.\n$$\n\\boldsymbol{g}^{(0)} = \\boldsymbol{A}^{\\top}(\\boldsymbol{A}\\boldsymbol{x}^{(0)} - \\boldsymbol{b}) = \\begin{pmatrix} 2  0  0 \\\\ 0  1  0 \\\\ 0  0  3 \\end{pmatrix} \\begin{pmatrix} 0.6 \\\\ 0.5 \\\\ 2.4 \\end{pmatrix} = \\begin{pmatrix} 1.2 \\\\ 0.5 \\\\ 7.2 \\end{pmatrix}\n$$\nNext, we perform the Armijo backtracking line search, starting with $\\alpha_0 = 1$. The trial point is $\\boldsymbol{y}(\\alpha) = P_{[\\boldsymbol{l},\\boldsymbol{u}]}(\\boldsymbol{x}^{(0)} - \\alpha \\boldsymbol{g}^{(0)})$. We must check if the Armijo condition is satisfied for $\\alpha = 1$:\n$$\nf\\big(\\boldsymbol{y}(1)\\big) \\le f(\\boldsymbol{x}^{(0)}) + \\sigma\\, (\\boldsymbol{g}^{(0)})^{\\top}\\big(\\boldsymbol{y}(1) - \\boldsymbol{x}^{(0)}\\big)\n$$\nLet's evaluate each term for $\\alpha = 1$. First, the trial point $\\boldsymbol{y}(1)$:\nThe unprojected point is $\\boldsymbol{x}^{(0)} - 1 \\cdot \\boldsymbol{g}^{(0)}$:\n$$\n\\begin{pmatrix} 0.8 \\\\ -0.5 \\\\ 1.8 \\end{pmatrix} - \\begin{pmatrix} 1.2 \\\\ 0.5 \\\\ 7.2 \\end{pmatrix} = \\begin{pmatrix} -0.4 \\\\ -1.0 \\\\ -5.4 \\end{pmatrix}\n$$\nProjecting this point onto the box $\\mathcal{B}$ defined by $\\boldsymbol{l}$ and $\\boldsymbol{u}$:\n$$\n\\boldsymbol{y}(1) = P_{[\\boldsymbol{l},\\boldsymbol{u}]}\\left(\\begin{pmatrix} -0.4 \\\\ -1.0 \\\\ -5.4 \\end{pmatrix}\\right) = \\begin{pmatrix} \\min(\\max(-0.4, 0), 1) \\\\ \\min(\\max(-1.0, -1), 2) \\\\ \\min(\\max(-5.4, 0.5), 2) \\end{pmatrix} = \\begin{pmatrix} \\min(0, 1) \\\\ \\min(-1, 2) \\\\ \\min(0.5, 2) \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 0.5 \\end{pmatrix}\n$$\nNow, we evaluate the two sides of the Armijo inequality.\nFor the left-hand side (LHS), we compute $f(\\boldsymbol{y}(1))$:\n$$\n\\boldsymbol{A}\\boldsymbol{y}(1) - \\boldsymbol{b} = \\begin{pmatrix} 2  0  0 \\\\ 0  1  0 \\\\ 0  0  3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -1 \\\\ 0.5 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 1.5 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ -1 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0 \\\\ -1.5 \\end{pmatrix}\n$$\n$$\nf(\\boldsymbol{y}(1)) = \\frac{1}{2}\\left\\| \\begin{pmatrix} -1 \\\\ 0 \\\\ -1.5 \\end{pmatrix} \\right\\|^2 = \\frac{1}{2}((-1)^2 + 0^2 + (-1.5)^2) = \\frac{1}{2}(1 + 2.25) = \\frac{1}{2}(3.25) = 1.625\n$$\nFor the right-hand side (RHS), we first need $f(\\boldsymbol{x}^{(0)})$ and the directional derivative term.\n$$\nf(\\boldsymbol{x}^{(0)}) = \\frac{1}{2}\\|\\boldsymbol{A}\\boldsymbol{x}^{(0)} - \\boldsymbol{b}\\|^2 = \\frac{1}{2}\\left\\| \\begin{pmatrix} 0.6 \\\\ 0.5 \\\\ 2.4 \\end{pmatrix} \\right\\|^2 = \\frac{1}{2}(0.6^2 + 0.5^2 + 2.4^2) = \\frac{1}{2}(0.36 + 0.25 + 5.76) = \\frac{1}{2}(6.37) = 3.185\n$$\nThe directional term is $\\sigma\\, (\\boldsymbol{g}^{(0)})^{\\top}(\\boldsymbol{y}(1) - \\boldsymbol{x}^{(0)})$. The vector difference is:\n$$\n\\boldsymbol{y}(1) - \\boldsymbol{x}^{(0)} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 0.5 \\end{pmatrix} - \\begin{pmatrix} 0.8 \\\\ -0.5 \\\\ 1.8 \\end{pmatrix} = \\begin{pmatrix} -0.8 \\\\ -0.5 \\\\ -1.3 \\end{pmatrix}\n$$\nThe dot product with the gradient is:\n$$\n(\\boldsymbol{g}^{(0)})^{\\top}(\\boldsymbol{y}(1) - \\boldsymbol{x}^{(0)}) = \\begin{pmatrix} 1.2  0.5  7.2 \\end{pmatrix} \\begin{pmatrix} -0.8 \\\\ -0.5 \\\\ -1.3 \\end{pmatrix} = (1.2)(-0.8) + (0.5)(-0.5) + (7.2)(-1.3) = -0.96 - 0.25 - 9.36 = -10.57\n$$\nThe RHS of the Armijo inequality is, with $\\sigma = \\frac{1}{10} = 0.1$:\n$$\nf(\\boldsymbol{x}^{(0)}) + \\sigma\\, (\\boldsymbol{g}^{(0)})^{\\top}(\\boldsymbol{y}(1) - \\boldsymbol{x}^{(0)}) = 3.185 + 0.1(-10.57) = 3.185 - 1.057 = 2.128\n$$\nNow we check the condition: Is $f(\\boldsymbol{y}(1)) \\le 2.128$?\n$$\n1.625 \\le 2.128\n$$\nThe inequality holds. Therefore, the step size $\\alpha=1$ is accepted. The next iterate is $\\boldsymbol{x}^{(1)} = \\boldsymbol{y}(1)$.\n$$\n\\boldsymbol{x}^{(1)} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 0.5 \\end{pmatrix}\n$$\nThe problem asks for the result as a row vector.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0  -1  0.5 \\end{pmatrix}}\n$$", "id": "3369390"}]}