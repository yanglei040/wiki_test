{"hands_on_practices": [{"introduction": "To truly understand an algorithm, there is no substitute for working through its mechanics by hand. This first exercise provides a concrete, pencil-and-paper calculation of a single Alternating Direction Method of Multipliers (ADMM) iteration applied to the celebrated LASSO problem [@problem_id:3476975]. By explicitly deriving and computing the updates for the primal and dual variables, you will gain a foundational intuition for how the algorithm decomposes a complex problem into a sequence of simpler steps.", "problem": "Consider the sparse linear regression problem with an $\\ell_{1}$ penalty (least absolute shrinkage and selection operator), which seeks $\\beta \\in \\mathbb{R}^{p}$ to minimize the objective $f(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$ for data $X \\in \\mathbb{R}^{m \\times p}$ and $y \\in \\mathbb{R}^{m}$. To enforce variable splitting for the $\\ell_{1}$ term, introduce an auxiliary variable $z \\in \\mathbb{R}^{p}$ and the constraint $z = \\beta$, yielding the equivalent constrained optimization problem $\\min_{\\beta, z} \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|z\\|_{1}$ subject to $z = \\beta$. The Alternating Direction Method of Multipliers (ADMM) applies to such problems via the augmented Lagrangian and alternating updates of the primal variables and scaled dual variable.\n\nUsing the foundational definitions of convex optimization and proximal operators, derive the single-iteration updates for the Alternating Direction Method of Multipliers (ADMM) on this constrained formulation and compute one iteration explicitly for the following toy instance. The data matrix and response vector are\n$$\nX = \\begin{pmatrix} 1 & 2 \\\\ 2 & 1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix},\n$$\nwith regularization parameter $\\lambda = 1$, penalty parameter $\\rho = 2$, and initial iterate\n$$\n\\beta^{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad z^{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad u^{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n$$\nCompute the updated variables $\\beta^{k+1}$, $z^{k+1}$, and $u^{k+1}$ exactly, in closed form, by performing one complete Alternating Direction Method of Multipliers (ADMM) iteration starting from the given iterate. Report your final answer as a single row matrix listing the six scalar entries in the order $\\beta^{k+1}_{1}$, $\\beta^{k+1}_{2}$, $z^{k+1}_{1}$, $z^{k+1}_{2}$, $u^{k+1}_{1}$, $u^{k+1}_{2}$. Do not round; express all rational values in simplest fractional form. No physical units are involved.", "solution": "The user has asked for the derivation and computation of one iteration of the Alternating Direction Method of Multipliers (ADMM) for a sparse linear regression problem.\n\n### Step 1: Extract Givens\nThe problem is to minimize the objective function $f(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$.\nThis is reformulated as an equivalent constrained problem:\n$$\n\\min_{\\beta, z} \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|z\\|_{1} \\quad \\text{subject to} \\quad z = \\beta\n$$\nThe data and parameters for the specific instance are:\n- Data matrix: $X = \\begin{pmatrix} 1 & 2 \\\\ 2 & 1 \\end{pmatrix}$\n- Response vector: $y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$\n- Regularization parameter: $\\lambda = 1$\n- ADMM penalty parameter: $\\rho = 2$\n- Initial iterates (at step $k$): $\\beta^{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, $z^{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, $u^{k} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n\nThe task is to compute the next iterates $\\beta^{k+1}$, $z^{k+1}$, and $u^{k+1}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is scientifically sound, well-posed, and objective.\n- **Scientifically Grounded:** The problem uses the LASSO objective, a standard technique in high-dimensional statistics and machine learning. Variable splitting and ADMM are established methods for solving such problems. The formulation is mathematically correct.\n- **Well-Posed:** The problem provides all necessary data and initial conditions to compute a unique solution for one iteration. The objective function is convex, ensuring that the subproblems in ADMM are well-posed. The matrix $X^T X + \\rho I$ is invertible for $\\rho > 0$, ensuring a unique solution for the $\\beta$-update.\n- **Objective:** The problem is stated using precise mathematical language without any ambiguity or subjective elements.\n\nThe problem is valid.\n\n### Step 3: Derivation of ADMM Updates\nThe ADMM algorithm is applied to the constrained formulation. The constraint is written as $\\beta - z = 0$.\nThe augmented Lagrangian in the scaled dual variable form is given by:\n$$\nL_{\\rho}(\\beta, z, u) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|z\\|_{1} + \\frac{\\rho}{2}\\|\\beta - z + u\\|_{2}^{2}\n$$\nHere, $u$ is the scaled dual variable. The ADMM iteration consists of three steps:\n1. Minimization with respect to $\\beta$: $\\beta^{k+1} = \\arg\\min_{\\beta} L_{\\rho}(\\beta, z^k, u^k)$\n2. Minimization with respect to $z$: $z^{k+1} = \\arg\\min_{z} L_{\\rho}(\\beta^{k+1}, z, u^k)$\n3. Update of the dual variable: $u^{k+1} = u^k + \\beta^{k+1} - z^{k+1}$\n\n**Derivation of the $\\beta$-update:**\nThe $\\beta$-minimization step involves solving:\n$$\n\\beta^{k+1} = \\arg\\min_{\\beta} \\left( \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\rho}{2}\\|\\beta - z^k + u^k\\|_{2}^{2} \\right)\n$$\nThis is a quadratic objective function in $\\beta$. We find the minimum by setting its gradient with respect to $\\beta$ to zero:\n$$\n\\nabla_{\\beta} \\left( \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\frac{\\rho}{2}\\|\\beta - (z^k - u^k)\\|_{2}^{2} \\right) = 0\n$$\n$$\n-X^T(y - X\\beta) + \\rho(\\beta - z^k + u^k) = 0\n$$\n$$\n(X^T X)\\beta - X^T y + \\rho\\beta - \\rho(z^k - u^k) = 0\n$$\n$$\n(X^T X + \\rho I)\\beta = X^T y + \\rho(z^k - u^k)\n$$\nThe update for $\\beta$ is a linear system solve:\n$$\n\\beta^{k+1} = (X^T X + \\rho I)^{-1} (X^T y + \\rho(z^k - u^k))\n$$\n\n**Derivation of the $z$-update:**\nThe $z$-minimization step involves solving:\n$$\nz^{k+1} = \\arg\\min_{z} \\left( \\lambda\\|z\\|_{1} + \\frac{\\rho}{2}\\|\\beta^{k+1} - z + u^k\\|_{2}^{2} \\right)\n$$\nThis can be rewritten as:\n$$\nz^{k+1} = \\arg\\min_{z} \\left( \\frac{1}{2}\\|z - (\\beta^{k+1} + u^k)\\|_{2}^{2} + \\frac{\\lambda}{\\rho}\\|z\\|_{1} \\right)\n$$\nThis is the definition of the proximal operator of the $\\ell_1$-norm. The solution is given by the soft-thresholding operator $S_{\\kappa}(\\cdot)$ with threshold $\\kappa = \\lambda/\\rho$:\n$$\nz^{k+1} = S_{\\lambda/\\rho}(\\beta^{k+1} + u^k)\n$$\nThe operator is applied element-wise: $(S_{\\kappa}(v))_i = \\text{sign}(v_i) \\max(|v_i| - \\kappa, 0)$.\n\n**The dual variable update:**\nThe dual update step is a simple addition:\n$$\nu^{k+1} = u^k + \\beta^{k+1} - z^{k+1}\n$$\n\n### Step 4: Computation of One Iteration\nWe now compute one iteration with the given values:\n$X = \\begin{pmatrix} 1 & 2 \\\\ 2 & 1 \\end{pmatrix}$, $y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$, $\\lambda = 1$, $\\rho = 2$, and initial values $\\beta^k = \\mathbf{0}$, $z^k = \\mathbf{0}$, $u^k = \\mathbf{0}$. The indices are $k=0$ and $k+1=1$.\n\n**1. Compute $\\beta^{1}$:**\nUsing the derived formula with $z^0 = u^0 = \\mathbf{0}$:\n$$\n\\beta^{1} = (X^T X + \\rho I)^{-1} (X^T y)\n$$\nFirst, we compute the matrices involved:\n$$\nX^T X = \\begin{pmatrix} 1 & 2 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 2 \\\\ 2 & 1 \\end{pmatrix} = \\begin{pmatrix} 1(1)+2(2) & 1(2)+2(1) \\\\ 2(1)+1(2) & 2(2)+1(1) \\end{pmatrix} = \\begin{pmatrix} 5 & 4 \\\\ 4 & 5 \\end{pmatrix}\n$$\n$$\nX^T X + \\rho I = \\begin{pmatrix} 5 & 4 \\\\ 4 & 5 \\end{pmatrix} + 2 \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 7 & 4 \\\\ 4 & 7 \\end{pmatrix}\n$$\nThe inverse of this matrix is:\n$$\n(X^T X + \\rho I)^{-1} = \\frac{1}{7(7) - 4(4)} \\begin{pmatrix} 7 & -4 \\\\ -4 & 7 \\end{pmatrix} = \\frac{1}{33} \\begin{pmatrix} 7 & -4 \\\\ -4 & 7 \\end{pmatrix}\n$$\nNext, we compute $X^T y$:\n$$\nX^T y = \\begin{pmatrix} 1 & 2 \\\\ 2 & 1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1(3)+2(1) \\\\ 2(3)+1(1) \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 7 \\end{pmatrix}\n$$\nFinally, we compute $\\beta^1$:\n$$\n\\beta^{1} = \\frac{1}{33} \\begin{pmatrix} 7 & -4 \\\\ -4 & 7 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ 7 \\end{pmatrix} = \\frac{1}{33} \\begin{pmatrix} 7(5) - 4(7) \\\\ -4(5) + 7(7) \\end{pmatrix} = \\frac{1}{33} \\begin{pmatrix} 35 - 28 \\\\ -20 + 49 \\end{pmatrix} = \\frac{1}{33} \\begin{pmatrix} 7 \\\\ 29 \\end{pmatrix}\n$$\nSo, $\\beta^1 = \\begin{pmatrix} 7/33 \\\\ 29/33 \\end{pmatrix}$.\n\n**2. Compute $z^{1}$:**\nWe use the soft-thresholding operator with threshold $\\kappa = \\lambda/\\rho = 1/2$. The argument is $\\beta^1 + u^0 = \\beta^1$ since $u^0 = \\mathbf{0}$.\n$$\nz^{1} = S_{1/2}(\\beta^{1}) = S_{1/2}\\begin{pmatrix} 7/33 \\\\ 29/33 \\end{pmatrix}\n$$\nWe apply this for each component:\n- For the first component, $z_1^1 = S_{1/2}(7/33)$. Since $|7/33| = 7/33$ and $1/2 = 16.5/33$, we have $7/33 < 1/2$. Therefore, $z_1^1 = 0$.\n- For the second component, $z_2^1 = S_{1/2}(29/33)$. Since $|29/33| = 29/33$ and $1/2 = 16.5/33$, we have $29/33 > 1/2$. Therefore, $z_2^1 = 29/33 - 1/2 = \\frac{58 - 33}{66} = \\frac{25}{66}$.\nSo, $z^1 = \\begin{pmatrix} 0 \\\\ 25/66 \\end{pmatrix}$.\n\n**3. Compute $u^{1}$:**\nWe use the dual update rule:\n$$\nu^{1} = u^0 + \\beta^1 - z^1 = \\mathbf{0} + \\begin{pmatrix} 7/33 \\\\ 29/33 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 25/66 \\end{pmatrix}\n$$\nComputing each component:\n- $u_1^1 = 7/33 - 0 = 7/33$.\n- $u_2^1 = 29/33 - 25/66 = 58/66 - 25/66 = 33/66 = 1/2$.\nSo, $u^1 = \\begin{pmatrix} 7/33 \\\\ 1/2 \\end{pmatrix}$.\n\nThe results for the first full ADMM iteration are:\n$\\beta^{1} = \\begin{pmatrix} 7/33 \\\\ 29/33 \\end{pmatrix}$, $z^{1} = \\begin{pmatrix} 0 \\\\ 25/66 \\end{pmatrix}$, $u^{1} = \\begin{pmatrix} 7/33 \\\\ 1/2 \\end{pmatrix}$.\n\nThe requested final answer is a row matrix containing the six scalar values in the order $(\\beta^1_1, \\beta^1_2, z^1_1, z^1_2, u^1_1, u^1_2)$.\nThese values are $(\\frac{7}{33}, \\frac{29}{33}, 0, \\frac{25}{66}, \\frac{7}{33}, \\frac{1}{2})$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{7}{33} & \\frac{29}{33} & 0 & \\frac{25}{66} & \\frac{7}{33} & \\frac{1}{2}\n\\end{pmatrix}\n}\n$$", "id": "3476975"}, {"introduction": "While ADMM's convergence is theoretically guaranteed for convex problems, its practical performance hinges critically on the choice of the penalty parameter $\\rho$. This coding exercise moves from theory to practice by tasking you with implementing a residual balancing scheme, a popular and effective heuristic for adapting $\\rho$ on the fly [@problem_id:3364422]. By comparing the performance of adaptive $\\rho$ to a fixed choice, you will see firsthand how this technique can dramatically accelerate convergence, especially for ill-conditioned problems.", "problem": "Consider the Alternating Direction Method of Multipliers (ADMM) applied to a consensus splitting of a convex, twice continuously differentiable quadratic objective in two dimensions. The goal is to design a residual balancing rule that adapts the augmented Lagrangian penalty parameter to equilibrate the norms of the primal and dual residuals and to analyze its effect on convergence for a two-dimensional quadratic test problem. The program you will write must be fully self-contained and compute the number of iterations until convergence for several test cases.\n\nStart from the following base. Let the optimization problem be stated as minimizing a separable sum of convex functions with an equality constraint expressed in the consensus form:\n$$\n\\min_{\\boldsymbol{x} \\in \\mathbb{R}^2,\\, \\boldsymbol{z} \\in \\mathbb{R}^2} \\; f(\\boldsymbol{x}) + g(\\boldsymbol{z}) \\quad \\text{subject to} \\quad \\boldsymbol{x} = \\boldsymbol{z},\n$$\nwhere\n$$\nf(\\boldsymbol{x}) = \\frac{1}{2}\\boldsymbol{x}^\\top \\boldsymbol{Q}\\,\\boldsymbol{x} + \\boldsymbol{q}^\\top \\boldsymbol{x}, \\quad g(\\boldsymbol{z}) = \\frac{1}{2}\\boldsymbol{z}^\\top \\boldsymbol{R}\\,\\boldsymbol{z} + \\boldsymbol{r}^\\top \\boldsymbol{z},\n$$\nand the matrices $\\boldsymbol{Q}$ and $\\boldsymbol{R}$ are symmetric positive definite and the vectors $\\boldsymbol{q}$ and $\\boldsymbol{r}$ are in $\\mathbb{R}^2$. Define the scaled ADMM iterate $(\\boldsymbol{x}^{k+1}, \\boldsymbol{z}^{k+1}, \\boldsymbol{u}^{k+1})$ for penalty parameter $\\rho > 0$ with scaled dual variable $\\boldsymbol{u}^k \\in \\mathbb{R}^2$. The primal residual is\n$$\n\\boldsymbol{r}^{k+1} = \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1},\n$$\nand the dual residual is\n$$\n\\boldsymbol{s}^{k+1} = \\rho\\left(\\boldsymbol{z}^{k+1} - \\boldsymbol{z}^{k}\\right).\n$$\n\nYour task is to:\n- Derive, from first principles and the first-order optimality conditions of the subproblems, the explicit closed-form updates for $\\boldsymbol{x}^{k+1}$ and $\\boldsymbol{z}^{k+1}$ when $f$ and $g$ are the stated convex quadratics and the constraint is $\\boldsymbol{x} = \\boldsymbol{z}$.\n- Implement ADMM with two modes: a fixed penalty parameter mode and an adaptive residual balancing mode. In the adaptive mode, adjust $\\rho$ during iterations using the following residual balancing rule with parameters $\\mu > 0$ and $\\kappa > 1$:\n$$\n\\text{if } \\|\\boldsymbol{r}^{k+1}\\|_2 > \\mu \\|\\boldsymbol{s}^{k+1}\\|_2 \\text{ then set } \\rho \\leftarrow \\kappa \\rho \\text{ and } \\boldsymbol{u}^{k+1} \\leftarrow \\boldsymbol{u}^{k+1}/\\kappa;\n$$\n$$\n\\text{else if } \\|\\boldsymbol{s}^{k+1}\\|_2 > \\mu \\|\\boldsymbol{r}^{k+1}\\|_2 \\text{ then set } \\rho \\leftarrow \\rho/\\kappa \\text{ and } \\boldsymbol{u}^{k+1} \\leftarrow \\boldsymbol{u}^{k+1}\\kappa;\n$$\nand otherwise leave $\\rho$ unchanged. The scaling of $\\boldsymbol{u}^{k+1}$ must preserve the unscaled Lagrange multiplier $\\boldsymbol{y}^{k+1} = \\rho \\boldsymbol{u}^{k+1}$ across changes of $\\rho$.\n- Use stopping criteria based on absolute and relative tolerances. Let $n = 2$. Define\n$$\n\\varepsilon_{\\mathrm{pri}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\max\\left(\\|\\boldsymbol{x}^{k+1}\\|_2, \\|\\boldsymbol{z}^{k+1}\\|_2\\right),\n$$\n$$\n\\varepsilon_{\\mathrm{dual}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\,\\|\\rho\\,\\boldsymbol{u}^{k+1}\\|_2,\n$$\nand terminate when both $\\|\\boldsymbol{r}^{k+1}\\|_2 \\le \\varepsilon_{\\mathrm{pri}}$ and $\\|\\boldsymbol{s}^{k+1}\\|_2 \\le \\varepsilon_{\\mathrm{dual}}$. Apply any penalty parameter adaptation only after checking these stopping criteria.\n\nImplement the algorithm with the following fixed inputs and test suite. All vectors are column vectors in $\\mathbb{R}^2$ and all matrices are $2 \\times 2$.\n\nCommon initial conditions:\n- $\\boldsymbol{x}^0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $\\boldsymbol{z}^0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $\\boldsymbol{u}^0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n- Tolerances: $\\varepsilon_{\\mathrm{abs}} = 10^{-6}$, $\\varepsilon_{\\mathrm{rel}} = 10^{-6}$.\n- Maximum iterations: $50000$.\n- Euclidean norms must be used for all residuals and threshold computations.\n\nTest suite parameter sets:\n- Case $1$ (well-conditioned, fixed $\\rho$):\n  - $\\boldsymbol{Q} = \\begin{bmatrix} 4 & 1 \\\\ 1 & 2 \\end{bmatrix}$, $\\boldsymbol{R} = \\begin{bmatrix} 3 & 0 \\\\ 0 & 1 \\end{bmatrix}$,\n  - $\\boldsymbol{q} = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}$, $\\boldsymbol{r} = \\begin{bmatrix} 0.5 \\\\ -1 \\end{bmatrix}$,\n  - Fixed $\\rho = 1$, adaptive mode disabled.\n- Case $2$ (well-conditioned, adaptive with small initial $\\rho$):\n  - Same $\\boldsymbol{Q}$, $\\boldsymbol{R}$, $\\boldsymbol{q}$, $\\boldsymbol{r}$ as Case $1$,\n  - Initial $\\rho = 10^{-4}$, adaptive mode enabled with $\\mu = 10$, $\\kappa = 2$.\n- Case $3$ (well-conditioned, adaptive with large initial $\\rho$):\n  - Same $\\boldsymbol{Q}$, $\\boldsymbol{R}$, $\\boldsymbol{q}$, $\\boldsymbol{r}$ as Case $1$,\n  - Initial $\\rho = 10^{2}$, adaptive mode enabled with $\\mu = 2$, $\\kappa = 2$.\n- Case $4$ (ill-conditioned, fixed $\\rho$):\n  - $\\boldsymbol{Q} = \\begin{bmatrix} 1000 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $\\boldsymbol{R} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 100 \\end{bmatrix}$,\n  - $\\boldsymbol{q} = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}$, $\\boldsymbol{r} = \\begin{bmatrix} 0.5 \\\\ -1 \\end{bmatrix}$,\n  - Fixed $\\rho = 1$, adaptive mode disabled.\n- Case $5$ (ill-conditioned, adaptive with very small initial $\\rho$):\n  - Same $\\boldsymbol{Q}$, $\\boldsymbol{R}$, $\\boldsymbol{q}$, $\\boldsymbol{r}$ as Case $4$,\n  - Initial $\\rho = 10^{-6}$, adaptive mode enabled with $\\mu = 3$, $\\kappa = 2$.\n\nFor each case, compute and return the total number of iterations required to satisfy the stopping criteria. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $\\left[ \\text{result}_1, \\text{result}_2, \\text{result}_3, \\text{result}_4, \\text{result}_5 \\right]$, where each $\\text{result}_i$ is the integer number of iterations for Case $i$.", "solution": "The user-provided problem is valid. It is a well-posed numerical optimization task based on established principles of the Alternating Direction Method of Multipliers (ADMM). All data, parameters, and conditions are provided, making the problem self-contained and free of contradictions or ambiguities.\n\nThe problem asks for the implementation of ADMM to solve a separable, convex quadratic optimization problem in consensus form:\n$$\n\\min_{\\boldsymbol{x} \\in \\mathbb{R}^2,\\, \\boldsymbol{z} \\in \\mathbb{R}^2} \\; f(\\boldsymbol{x}) + g(\\boldsymbol{z}) \\quad \\text{subject to} \\quad \\boldsymbol{x} = \\boldsymbol{z},\n$$\nwhere $f(\\boldsymbol{x}) = \\frac{1}{2}\\boldsymbol{x}^\\top \\boldsymbol{Q}\\,\\boldsymbol{x} + \\boldsymbol{q}^\\top \\boldsymbol{x}$ and $g(\\boldsymbol{z}) = \\frac{1}{2}\\boldsymbol{z}^\\top \\boldsymbol{R}\\,\\boldsymbol{z} + \\boldsymbol{r}^\\top \\boldsymbol{z}$. The matrices $\\boldsymbol{Q}$ and $\\boldsymbol{R}$ are symmetric positive definite.\n\nThe solution proceeds by first deriving the explicit update equations for the ADMM algorithm and then describing the algorithmic implementation, including the adaptive penalty parameter scheme.\n\n**ADMM Formulation and Update Derivations**\n\nThe scaled-form augmented Lagrangian $L_\\rho$ for this problem, using a scaled dual variable $\\boldsymbol{u}$, is:\n$$\nL_\\rho(\\boldsymbol{x}, \\boldsymbol{z}, \\boldsymbol{u}) = f(\\boldsymbol{x}) + g(\\boldsymbol{z}) + \\frac{\\rho}{2} \\|\\boldsymbol{x} - \\boldsymbol{z} + \\boldsymbol{u}\\|_2^2 - \\frac{\\rho}{2} \\|\\boldsymbol{u}\\|_2^2\n$$\nThe ADMM algorithm consists of three sequential update steps per iteration $k$, for a given penalty parameter $\\rho > 0$:\n$$\n\\begin{align*}\n\\boldsymbol{x}^{k+1} &:= \\arg\\min_{\\boldsymbol{x}} L_\\rho(\\boldsymbol{x}, \\boldsymbol{z}^k, \\boldsymbol{u}^k) \\\\\n\\boldsymbol{z}^{k+1} &:= \\arg\\min_{\\boldsymbol{z}} L_\\rho(\\boldsymbol{x}^{k+1}, \\boldsymbol{z}, \\boldsymbol{u}^k) \\\\\n\\boldsymbol{u}^{k+1} &:= \\boldsymbol{u}^k + \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1}\n\\end{align*}\n$$\nWe derive the closed-form solutions for the $\\boldsymbol{x}$ and $\\boldsymbol{z}$ subproblems.\n\n**1. $\\boldsymbol{x}$-update Derivation**\nThe minimization subproblem for $\\boldsymbol{x}$ is:\n$$\n\\boldsymbol{x}^{k+1} = \\arg\\min_{\\boldsymbol{x}} \\left( f(\\boldsymbol{x}) + \\frac{\\rho}{2} \\|\\boldsymbol{x} - \\boldsymbol{z}^k + \\boldsymbol{u}^k\\|_2^2 \\right)\n$$\nSubstituting the quadratic form of $f(\\boldsymbol{x})$:\n$$\n\\boldsymbol{x}^{k+1} = \\arg\\min_{\\boldsymbol{x}} \\left( \\frac{1}{2}\\boldsymbol{x}^\\top \\boldsymbol{Q}\\,\\boldsymbol{x} + \\boldsymbol{q}^\\top \\boldsymbol{x} + \\frac{\\rho}{2} \\|\\boldsymbol{x} - (\\boldsymbol{z}^k - \\boldsymbol{u}^k)\\|_2^2 \\right)\n$$\nThis is an unconstrained convex quadratic minimization. The minimizer is found by setting the gradient with respect to $\\boldsymbol{x}$ to zero. The gradient is:\n$$\n\\nabla_{\\boldsymbol{x}} (\\cdot) = \\boldsymbol{Q}\\boldsymbol{x} + \\boldsymbol{q} + \\rho(\\boldsymbol{x} - \\boldsymbol{z}^k + \\boldsymbol{u}^k)\n$$\nSetting the gradient to zero and solving for $\\boldsymbol{x}$:\n$$\n\\boldsymbol{Q}\\boldsymbol{x}^{k+1} + \\rho\\boldsymbol{I}\\boldsymbol{x}^{k+1} = \\rho(\\boldsymbol{z}^k - \\boldsymbol{u}^k) - \\boldsymbol{q}\n$$\n$$\n(\\boldsymbol{Q} + \\rho\\boldsymbol{I})\\boldsymbol{x}^{k+1} = \\rho(\\boldsymbol{z}^k - \\boldsymbol{u}^k) - \\boldsymbol{q}\n$$\nSince $\\boldsymbol{Q}$ is positive definite and $\\rho > 0$, the matrix $(\\boldsymbol{Q} + \\rho\\boldsymbol{I})$ is symmetric positive definite and thus invertible. The explicit update is:\n$$\n\\boldsymbol{x}^{k+1} = (\\boldsymbol{Q} + \\rho\\boldsymbol{I})^{-1} \\left( \\rho(\\boldsymbol{z}^k - \\boldsymbol{u}^k) - \\boldsymbol{q} \\right)\n$$\n\n**2. $\\boldsymbol{z}$-update Derivation**\nThe minimization subproblem for $\\boldsymbol{z}$ uses the newly computed $\\boldsymbol{x}^{k+1}$:\n$$\n\\boldsymbol{z}^{k+1} = \\arg\\min_{\\boldsymbol{z}} \\left( g(\\boldsymbol{z}) + \\frac{\\rho}{2} \\|\\boldsymbol{x}^{k+1} - \\boldsymbol{z} + \\boldsymbol{u}^k\\|_2^2 \\right)\n$$\nSubstituting the quadratic form of $g(\\boldsymbol{z})$:\n$$\n\\boldsymbol{z}^{k+1} = \\arg\\min_{\\boldsymbol{z}} \\left( \\frac{1}{2}\\boldsymbol{z}^\\top \\boldsymbol{R}\\,\\boldsymbol{z} + \\boldsymbol{r}^\\top \\boldsymbol{z} + \\frac{\\rho}{2} \\|\\boldsymbol{z} - (\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k)\\|_2^2 \\right)\n$$\nAgain, we set the gradient with respect to $\\boldsymbol{z}$ to zero. The gradient is:\n$$\n\\nabla_{\\boldsymbol{z}} (\\cdot) = \\boldsymbol{R}\\boldsymbol{z} + \\boldsymbol{r} + \\rho(\\boldsymbol{z} - (\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k)) = \\boldsymbol{R}\\boldsymbol{z} + \\boldsymbol{r} - \\rho(\\boldsymbol{x}^{k+1} - \\boldsymbol{z} + \\boldsymbol{u}^k)\n$$\nSetting the gradient to zero and solving for $\\boldsymbol{z}$:\n$$\n\\boldsymbol{R}\\boldsymbol{z}^{k+1} + \\rho\\boldsymbol{I}\\boldsymbol{z}^{k+1} = \\rho(\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k) - \\boldsymbol{r}\n$$\n$$\n(\\boldsymbol{R} + \\rho\\boldsymbol{I})\\boldsymbol{z}^{k+1} = \\rho(\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k) - \\boldsymbol{r}\n$$\nSince $\\boldsymbol{R}$ is positive definite, $(\\boldsymbol{R} + \\rho\\boldsymbol{I})$ is invertible. The explicit update is:\n$$\n\\boldsymbol{z}^{k+1} = (\\boldsymbol{R} + \\rho\\boldsymbol{I})^{-1} \\left( \\rho(\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k) - \\boldsymbol{r} \\right)\n$$\n\n**3. $\\boldsymbol{u}$-update**\nThe dual variable update is given by:\n$$\n\\boldsymbol{u}^{k+1} = \\boldsymbol{u}^k + \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1}\n$$\n\n**Algorithm Implementation**\nThe algorithm iterates these updates until convergence. The process for iteration $k$ (to compute iteration $k+1$ values) is as follows:\n1.  Initialize $\\boldsymbol{x}^0, \\boldsymbol{z}^0, \\boldsymbol{u}^0$ to $\\boldsymbol{0}$, and $\\rho_0$ to its initial value.\n2.  For $k=0, 1, \\dots, \\text{max\\_iter}-1$:\n    a.  Compute $\\boldsymbol{x}^{k+1}$ using the derived formula with $\\boldsymbol{z}^k, \\boldsymbol{u}^k, \\rho_k$.\n    b.  Compute $\\boldsymbol{z}^{k+1}$ using the derived formula with $\\boldsymbol{x}^{k+1}, \\boldsymbol{u}^k, \\rho_k$.\n    c.  Compute the pre-scaled dual update: $\\boldsymbol{u}^{k+1}_{\\text{pre}} = \\boldsymbol{u}^k + \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1}$.\n    d.  Calculate residuals:\n        -   Primal residual: $\\boldsymbol{r}^{k+1} = \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1}$.\n        -   Dual residual: $\\boldsymbol{s}^{k+1} = \\rho_k(\\boldsymbol{z}^{k+1} - \\boldsymbol{z}^{k})$.\n    e.  Compute stopping thresholds based on absolute tolerance $\\varepsilon_{\\mathrm{abs}}$ and relative tolerance $\\varepsilon_{\\mathrm{rel}}$:\n        $$\n        \\varepsilon_{\\mathrm{pri}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\max\\left(\\|\\boldsymbol{x}^{k+1}\\|_2, \\|\\boldsymbol{z}^{k+1}\\|_2\\right)\n        $$\n        The unscaled Lagrange multiplier is $\\boldsymbol{y}^{k+1} = \\rho_k \\boldsymbol{u}^{k+1}_{\\text{pre}}$. The dual threshold is:\n        $$\n        \\varepsilon_{\\mathrm{dual}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\,\\|\\boldsymbol{y}^{k+1}\\|_2\n        $$\n    f. Check for convergence: if $\\|\\boldsymbol{r}^{k+1}\\|_2 \\le \\varepsilon_{\\mathrm{pri}}$ and $\\|\\boldsymbol{s}^{k+1}\\|_2 \\le \\varepsilon_{\\mathrm{dual}}$, terminate and return $k+1$.\n    g. If adaptive mode is enabled, update $\\rho$ for the next iteration, $\\rho_{k+1}$, and scale $\\boldsymbol{u}^{k+1}$ accordingly. Let $\\boldsymbol{u}^{k+1}_{\\text{post}}$ be the updated dual variable for the next iteration.\n        - Set $\\rho_{k+1} = \\rho_k$ and $\\boldsymbol{u}^{k+1}_{\\text{post}} = \\boldsymbol{u}^{k+1}_{\\text{pre}}$ by default.\n        - If $\\|\\boldsymbol{r}^{k+1}\\|_2 > \\mu \\|\\boldsymbol{s}^{k+1}\\|_2$:\n            $\\rho_{k+1} \\leftarrow \\kappa \\rho_k$ and $\\boldsymbol{u}^{k+1}_{\\text{post}} \\leftarrow \\boldsymbol{u}^{k+1}_{\\text{pre}} / \\kappa$.\n        - Else if $\\|\\boldsymbol{s}^{k+1}\\|_2 > \\mu \\|\\boldsymbol{r}^{k+1}\\|_2$:\n            $\\rho_{k+1} \\leftarrow \\rho_k / \\kappa$ and $\\boldsymbol{u}^{k+1}_{\\text{post}} \\leftarrow \\boldsymbol{u}^{k+1}_{\\text{pre}} \\times \\kappa$.\n    h. Prepare for the next iteration: update state variables $(\\boldsymbol{z}^k, \\boldsymbol{u}^k, \\rho_k)$ to $(\\boldsymbol{z}^{k+1}, \\boldsymbol{u}^{k+1}_{\\text{post}}, \\rho_{k+1})$.\n3.  If the loop completes without convergence, return the maximum number of iterations.\nThe Python code in the final answer implements this logic.", "answer": "```python\nimport numpy as np\n\ndef admm_solver(Q, R, q, r, rho_init, adaptive_params, max_iter, eps_abs, eps_rel):\n    \"\"\"\n    Solves a 2D quadratic consensus ADMM problem.\n\n    Args:\n        Q (np.array): 2x2 symmetric positive definite matrix for f(x).\n        R (np.array): 2x2 symmetric positive definite matrix for g(z).\n        q (np.array): 2x1 vector for f(x).\n        r (np.array): 2x1 vector for g(z).\n        rho_init (float): Initial penalty parameter.\n        adaptive_params (tuple): (is_adaptive, mu, kappa) for rho adaptation.\n        max_iter (int): Maximum number of iterations.\n        eps_abs (float): Absolute tolerance for stopping criteria.\n        eps_rel (float): Relative tolerance for stopping criteria.\n\n    Returns:\n        int: Number of iterations to converge, or max_iter.\n    \"\"\"\n    is_adaptive, mu, kappa = adaptive_params\n\n    # Initialization\n    n = 2  # Dimension\n    x_k = np.zeros((n, 1))\n    z_k = np.zeros((n, 1))\n    u_k = np.zeros((n, 1))\n    rho_k = float(rho_init)\n\n    I = np.identity(n)\n\n    for k in range(max_iter):\n        # Precompute matrix inverses, which depend on rho\n        try:\n            inv_Q_rhoI = np.linalg.inv(Q + rho_k * I)\n            inv_R_rhoI = np.linalg.inv(R + rho_k * I)\n        except np.linalg.LinAlgError:\n            # In case of numerical issues, though unlikely with SPD matrices\n            return max_iter\n\n        # x-update\n        x_k1 = inv_Q_rhoI @ (rho_k * (z_k - u_k) - q)\n\n        # z-update\n        z_k1 = inv_R_rhoI @ (rho_k * (x_k1 + u_k) - r)\n\n        # u-update (before potential scaling)\n        u_k1_prescale = u_k + x_k1 - z_k1\n\n        # Calculate residuals\n        r_k1 = x_k1 - z_k1\n        s_k1 = rho_k * (z_k1 - z_k)\n\n        # Calculate residual norms\n        norm_r = np.linalg.norm(r_k1)\n        norm_s = np.linalg.norm(s_k1)\n\n        # Calculate stopping thresholds\n        eps_pri = np.sqrt(n) * eps_abs + eps_rel * max(np.linalg.norm(x_k1), np.linalg.norm(z_k1))\n        \n        # y^{k+1} = rho_k * u_{k+1}, where u_{k+1} = u_k + r_{k+1}\n        y_k1 = rho_k * u_k1_prescale\n        eps_dual = np.sqrt(n) * eps_abs + eps_rel * np.linalg.norm(y_k1)\n\n        # Check for convergence\n        if norm_r = eps_pri and norm_s = eps_dual:\n            return k + 1\n\n        # Penalty parameter and dual variable adaptation for the next iteration\n        rho_k1 = rho_k\n        u_k1 = u_k1_prescale\n        if is_adaptive:\n            if norm_r > mu * norm_s:\n                rho_k1 = kappa * rho_k\n                u_k1 = u_k1_prescale / kappa\n            elif norm_s > mu * norm_r:\n                rho_k1 = rho_k / kappa\n                u_k1 = u_k1_prescale * kappa\n        \n        # Update states for next iteration\n        # x_k is not needed, will be recomputed\n        z_k = z_k1\n        u_k = u_k1\n        rho_k = rho_k1\n    \n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Common parameters\n    eps_abs = 1e-6\n    eps_rel = 1e-6\n    max_iter = 50000\n    \n    # Common vectors for specified cases\n    q_vec = np.array([[-1.0], [2.0]])\n    r_vec = np.array([[0.5], [-1.0]])\n\n    # Case 1 (well-conditioned, fixed rho)\n    Q1 = np.array([[4.0, 1.0], [1.0, 2.0]])\n    R1 = np.array([[3.0, 0.0], [0.0, 1.0]])\n    rho1 = 1.0\n    params1 = (False, 0, 0)\n    \n    # Case 2 (well-conditioned, adaptive small rho)\n    rho2 = 1e-4\n    params2 = (True, 10.0, 2.0)\n\n    # Case 3 (well-conditioned, adaptive large rho)\n    rho3 = 1e2\n    params3 = (True, 2.0, 2.0)\n    \n    # Case 4 (ill-conditioned, fixed rho)\n    Q4 = np.array([[1000.0, 0.0], [0.0, 1.0]])\n    R4 = np.array([[1.0, 0.0], [0.0, 100.0]])\n    rho4 = 1.0\n    params4 = (False, 0, 0)\n    \n    # Case 5 (ill-conditioned, adaptive very small rho)\n    rho5 = 1e-6\n    params5 = (True, 3.0, 2.0)\n\n    test_cases = [\n        (Q1, R1, q_vec, r_vec, rho1, params1),\n        (Q1, R1, q_vec, r_vec, rho2, params2),\n        (Q1, R1, q_vec, r_vec, rho3, params3),\n        (Q4, R4, q_vec, r_vec, rho4, params4),\n        (Q4, R4, q_vec, r_vec, rho5, params5),\n    ]\n\n    results = []\n    for case in test_cases:\n        iter_count = admm_solver(*case, max_iter, eps_abs, eps_rel)\n        results.append(iter_count)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3364422"}, {"introduction": "The remarkable convergence guarantees of ADMM apply to the two-block case, but famously, they do not hold for a naive Gauss-Seidel extension to three or more blocks. This advanced practice explores this critical limitation, a common pitfall for newcomers to the method [@problem_id:3364446]. You will derive the linear iteration matrix for a three-block quadratic problem and analyze its spectral radius to demonstrate how and why the iteration can diverge, solidifying your understanding of ADMM's theoretical boundaries.", "problem": "Consider the linearly constrained three-block quadratic optimization problem in one dimension per block, defined by the variables $x_1 \\in \\mathbb{R}$, $x_2 \\in \\mathbb{R}$, $x_3 \\in \\mathbb{R}$ and the constraint coefficient scalars $A_1, A_2, A_3 \\in \\mathbb{R}$:\n$$\n\\min_{x_1,x_2,x_3} \\;\\; \\frac{1}{2} q_1 x_1^2 + \\frac{1}{2} q_2 x_2^2 + \\frac{1}{2} q_3 x_3^2 \\quad \\text{subject to} \\quad A_1 x_1 + A_2 x_2 + A_3 x_3 = c,\n$$\nwhere $q_1, q_2, q_3 \\ge 0$, and $c \\in \\mathbb{R}$ is a given constant.\n\nThe Alternating Direction Method of Multipliers (ADMM) applied naively in Gauss-Seidel fashion to more than two blocks is known to possibly diverge. In the three-block case above, consider the scaled-dual ADMM iteration with penalty parameter $\\rho  0$ and scaled dual variable $u$:\n1. Update $x_1$ by minimizing the augmented Lagrangian with respect to $x_1$ holding $x_2$, $x_3$, $u$ fixed.\n2. Update $x_2$ by minimizing the augmented Lagrangian with respect to $x_2$ holding $x_1$ (just updated), $x_3$, $u$ fixed.\n3. Update $x_3$ by minimizing the augmented Lagrangian with respect to $x_3$ holding $x_1$, $x_2$ (just updated), $u$ fixed.\n4. Update the scaled dual variable $u$ with a single Gauss-Seidel step.\n\nStarting from the augmented Lagrangian definition\n$$\n\\mathcal{L}_\\rho(x_1,x_2,x_3,u) \\;=\\; \\frac{1}{2} q_1 x_1^2 + \\frac{1}{2} q_2 x_2^2 + \\frac{1}{2} q_3 x_3^2 \\;+\\; \\frac{\\rho}{2}\\left(A_1 x_1 + A_2 x_2 + A_3 x_3 - c + u\\right)^2,\n$$\nthe first-order optimality conditions for each block update yield closed-form linear updates. Denote\n$$\n\\theta_i \\;=\\; \\frac{\\rho A_i}{q_i + \\rho A_i^2}, \\quad i \\in \\{1,2,3\\},\n$$\nassuming $A_i \\neq 0$ or $q_i  0$ so all denominators are positive. Then, for $c=0$ (which isolates the homogeneous linear iteration), the naive Gauss-Seidel ADMM block updates are:\n$$\nx_1^{k+1} = - \\theta_1\\left(A_2 x_2^k + A_3 x_3^k + u^k\\right),\n$$\n$$\nx_2^{k+1} = - \\theta_2\\left(A_1 x_1^{k+1} + A_3 x_3^k + u^k\\right),\n$$\n$$\nx_3^{k+1} = - \\theta_3\\left(A_1 x_1^{k+1} + A_2 x_2^{k+1} + u^k\\right),\n$$\n$$\nu^{k+1} = u^k + \\left(A_1 x_1^{k+1} + A_2 x_2^{k+1} + A_3 x_3^{k+1}\\right).\n$$\n\nThese four equations constitute a linear iteration on the state vector $z^k = [x_1^k, x_2^k, x_3^k, u^k]^\\top$, which can be written as\n$$\nz^{k+1} = J z^k,\n$$\nfor an iteration matrix $J \\in \\mathbb{R}^{4 \\times 4}$ determined solely by $(q_1,q_2,q_3)$, $(A_1,A_2,A_3)$, and $\\rho$. The mechanism of divergence is governed by the spectral radius of $J$, namely\n$$\n\\varrho(J) = \\max\\{|\\lambda| : \\lambda \\text{ is an eigenvalue of } J\\}.\n$$\nIf $\\varrho(J)  1$, the iteration is linearly unstable and diverges along the eigenvector associated with the unstable eigenvalue; if $\\varrho(J)  1$, the iteration contracts and converges linearly to the fixed point (the optimal solution); and if $\\varrho(J) = 1$, the iteration is at best marginally stable and can fail to converge.\n\nTasks:\n- Derive the explicit iteration matrix $J$ consistent with the above Gauss-Seidel ADMM steps for the homogeneous case $c=0$.\n- Explain the mechanism of divergence in terms of $\\varrho(J)$.\n- Implement a program that constructs $J$ for given parameters, computes $\\varrho(J)$, and reports it for a test suite of parameter sets that exercise convergence, marginal stability, and divergence.\n\nYour program must:\n1. Build the iteration matrix $J$ by applying the linear update mapping to each canonical basis vector of $\\mathbb{R}^4$.\n2. Compute the spectral radius as the maximum magnitude of the eigenvalues of $J$.\n3. For each test case, return the spectral radius as a floating-point number rounded to six decimal places.\n\nTest suite to cover different regimes:\n- Case 1 (strongly convex, well-conditioned, expected contraction): $(q_1,q_2,q_3) = (10.0,10.0,10.0)$, $(A_1,A_2,A_3) = (1.0,1.0,1.0)$, $\\rho = 1.0$, $c = 0.0$.\n- Case 2 (no objective quadratics, identity coefficients, marginal stability): $(q_1,q_2,q_3) = (0.0,0.0,0.0)$, $(A_1,A_2,A_3) = (1.0,1.0,1.0)$, $\\rho = 1.0$, $c = 0.0$.\n- Case 3 (low-magnitude and sign-indefinite coefficients producing large block gains, expected divergence): $(q_1,q_2,q_3) = (0.01,0.01,0.01)$, $(A_1,A_2,A_3) = (0.2,-0.2,0.2)$, $\\rho = 1.0$, $c = 0.0$.\n- Case 4 (mixed signs with moderate scaling, stress test): $(q_1,q_2,q_3) = (0.0,0.0,0.0)$, $(A_1,A_2,A_3) = (0.5,0.5,-0.5)$, $\\rho = 1.0$, $c = 0.0$.\n\nFinal output format:\nYour program should produce a single line of output containing the spectral radii for the four test cases as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places (for example, \"[0.732101,1.000000,1.284557,0.998340]\").\nNo physical units are involved in this problem, and there are no angles; all quantities are dimensionless real numbers.", "solution": "The problem statement has been validated and is deemed to be scientifically grounded, well-posed, objective, and complete. All parameters and update rules are provided, allowing for a direct and unambiguous derivation and analysis. The problem addresses a standard, non-trivial topic in numerical optimization—the convergence of the Alternating Direction Method of Multipliers (ADMM) for more than two blocks—and is structured as a solvable mathematical and computational task.\n\nThe analysis of the three-block ADMM iteration begins by formulating the given Gauss-Seidel update steps as a single linear transformation on a state vector. The state at iteration $k$ is represented by the vector $z^k = [x_1^k, x_2^k, x_3^k, u^k]^\\top \\in \\mathbb{R}^4$. The iteration process maps this state to the next state, $z^{k+1}$, via a linear operator represented by an iteration matrix $J \\in \\mathbb{R}^{4 \\times 4}$, such that $z^{k+1} = J z^k$.\n\nTo derive the matrix $J$, we first express the update equations for the homogeneous case, where $c=0$:\n$$x_1^{k+1} = - \\theta_1(A_2 x_2^k + A_3 x_3^k + u^k)$$\n$$x_2^{k+1} = - \\theta_2(A_1 x_1^{k+1} + A_3 x_3^k + u^k)$$\n$$x_3^{k+1} = - \\theta_3(A_1 x_1^{k+1} + A_2 x_2^{k+1} + u^k)$$\n$$u^{k+1} = u^k + A_1 x_1^{k+1} + A_2 x_2^{k+1} + A_3 x_3^{k+1}$$\n\nThese equations constitute a coupled system. The variables with superscript $k+1$ are the unknowns, and they depend on variables with superscript $k$ (the knowns from the previous iteration) and on other variables that have already been updated within the current iteration (the Gauss-Seidel dependency). To form a standard matrix equation, we rearrange these equations by moving all terms with superscript $k+1$ to the left-hand side and all terms with superscript $k$ to the right-hand side.\n\n1. $1 \\cdot x_1^{k+1} = 0 \\cdot x_1^k - \\theta_1 A_2 x_2^k - \\theta_1 A_3 x_3^k - \\theta_1 u^k$\n2. $\\theta_2 A_1 x_1^{k+1} + 1 \\cdot x_2^{k+1} = 0 \\cdot x_1^k + 0 \\cdot x_2^k - \\theta_2 A_3 x_3^k - \\theta_2 u^k$\n3. $\\theta_3 A_1 x_1^{k+1} + \\theta_3 A_2 x_2^{k+1} + 1 \\cdot x_3^{k+1} = 0 \\cdot x_1^k + 0 \\cdot x_2^k + 0 \\cdot x_3^k - \\theta_3 u^k$\n4. $-A_1 x_1^{k+1} - A_2 x_2^{k+1} - A_3 x_3^{k+1} + 1 \\cdot u^{k+1} = 0 \\cdot x_1^k + 0 \\cdot x_2^k + 0 \\cdot x_3^k + 1 \\cdot u^k$\n\nThis system of linear equations can be written in the matrix form $L z^{k+1} = R z^k$, where $L$ and $R$ are $4 \\times 4$ matrices. The matrix $L$ contains the coefficients of the $z^{k+1}$ terms, and the matrix $R$ contains the coefficients of the $z^k$ terms.\n\nFrom the equations above, we can explicitly construct $L$ and $R$:\n$$\nL = \\begin{pmatrix}\n1  0  0  0 \\\\\n\\theta_2 A_1  1  0  0 \\\\\n\\theta_3 A_1  \\theta_3 A_2  1  0 \\\\\n-A_1  -A_2  -A_3  1\n\\end{pmatrix}\n$$\nThe matrix $L$ is lower triangular with ones on its diagonal, which reflects the sequential, feed-forward nature of the Gauss-Seidel updates. This structure guarantees that $L$ is invertible.\n\n$$\nR = \\begin{pmatrix}\n0  -\\theta_1 A_2  -\\theta_1 A_3  -\\theta_1 \\\\\n0  0  -\\theta_2 A_3  -\\theta_2 \\\\\n0  0  0  -\\theta_3 \\\\\n0  0  0  1\n\\end{pmatrix}\n$$\nThe matrix $R$ is upper triangular. The first column is zero because $x_1^k$ does not appear on the right-hand side of any update equation.\n\nThe iteration matrix $J$ is then found by solving for $z^{k+1}$:\n$$z^{k+1} = L^{-1} R z^k \\implies J = L^{-1} R$$\nNumerically, this is computed by solving the linear system $LJ = R$ for the matrix $J$, which is more stable and efficient than computing the inverse of $L$ directly.\n\nThe convergence of the iteration $z^{k+1} = J z^k$ is determined by the spectral radius of $J$, denoted $\\varrho(J)$. The spectral radius is defined as the maximum magnitude of the eigenvalues of $J$: $\\varrho(J) = \\max \\{|\\lambda_i|\\}$, where $\\lambda_i$ are the eigenvalues of $J$.\nThe mechanism of divergence is understood by considering the long-term behavior of the iteration. Any initial state $z^0$ can, in general, be expressed as a linear combination of the eigenvectors of $J$ (assuming $J$ is diagonalizable for simplicity of explanation). Let $J v_i = \\lambda_i v_i$ for eigenvectors $v_i$ and eigenvalues $\\lambda_i$. If $z^0 = \\sum_i c_i v_i$, then after $k$ iterations, the state is $z^k = J^k z^0 = \\sum_i c_i \\lambda_i^k v_i$.\n- If $\\varrho(J)  1$, then $|\\lambda_i|  1$ for all $i$. Consequently, $\\lambda_i^k \\to 0$ as $k \\to \\infty$, and the iteration converges to the zero vector, which is the fixed point (and optimal solution for $c=0$).\n- If $\\varrho(J)  1$, there exists at least one eigenvalue $\\lambda_j$ with $|\\lambda_j|  1$. The corresponding term $c_j \\lambda_j^k v_j$ will grow in magnitude, causing the state vector $z^k$ to diverge, provided the initial state $z^0$ has a non-zero component along $v_j$ (i.e., $c_j \\neq 0$). This is the condition for linear instability.\n- If $\\varrho(J) = 1$, the iteration is marginally stable. Some components of the state may not decay, leading to oscillations or slow, sublinear convergence, or even divergence if the geometric multiplicity of an eigenvalue with magnitude $1$ is less than its algebraic multiplicity.\n\nThe program below will implement this derivation. For each test case, it will:\n1. Define the parameters $q_i, A_i, \\rho$.\n2. Calculate the intermediate variables $\\theta_i = \\frac{\\rho A_i}{q_i + \\rho A_i^2}$.\n3. Construct the matrices $L$ and $R$ according to the formulas derived above.\n4. Solve the linear matrix equation $LJ=R$ to find the iteration matrix $J$.\n5. Compute the eigenvalues of $J$.\n6. Determine the spectral radius $\\varrho(J)$ by finding the maximum absolute value among the eigenvalues.\n7. Report this value, rounded to six decimal places.\nThis process will be repeated for all provided test cases to explore the different convergence regimes.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the spectral radius of the 3-block ADMM iteration matrix\n    for a suite of test cases.\n    \"\"\"\n\n    # Test cases defined in the problem statement.\n    # Each case is a tuple: ((q1, q2, q3), (A1, A2, A3), rho)\n    test_cases = [\n        # Case 1: strongly convex, well-conditioned, expected contraction\n        ((10.0, 10.0, 10.0), (1.0, 1.0, 1.0), 1.0),\n        # Case 2: no objective quadratics, identity coefficients, marginal stability\n        ((0.0, 0.0, 0.0), (1.0, 1.0, 1.0), 1.0),\n        # Case 3: low-magnitude coefficients producing large gains, expected divergence\n        ((0.01, 0.01, 0.01), (0.2, -0.2, 0.2), 1.0),\n        # Case 4: mixed signs with moderate scaling, stress test\n        ((0.0, 0.0, 0.0), (0.5, 0.5, -0.5), 1.0),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        (q1, q2, q3), (A1, A2, A3), rho = case\n        \n        # Calculate theta_i values\n        # theta_i = (rho * A_i) / (q_i + rho * A_i^2)\n        # Denominators are positive as per problem constraints.\n        theta1 = (rho * A1) / (q1 + rho * A1**2)\n        theta2 = (rho * A2) / (q2 + rho * A2**2)\n        theta3 = (rho * A3) / (q3 + rho * A3**2)\n        \n        # Construct the L matrix from the equation L * z^{k+1} = R * z^k\n        # L = [[1, 0, 0, 0],\n        #      [theta2*A1, 1, 0, 0],\n        #      [theta3*A1, theta3*A2, 1, 0],\n        #      [-A1, -A2, -A3, 1]]\n        L = np.array([\n            [1.0, 0.0, 0.0, 0.0],\n            [theta2 * A1, 1.0, 0.0, 0.0],\n            [theta3 * A1, theta3 * A2, 1.0, 0.0],\n            [-A1, -A2, -A3, 1.0]\n        ])\n        \n        # Construct the R matrix\n        # R = [[0, -theta1*A2, -theta1*A3, -theta1],\n        #      [0, 0, -theta2*A3, -theta2],\n        #      [0, 0, 0, -theta3],\n        #      [0, 0, 0, 1]]\n        R = np.array([\n            [0.0, -theta1 * A2, -theta1 * A3, -theta1],\n            [0.0, 0.0, -theta2 * A3, -theta2],\n            [0.0, 0.0, 0.0, -theta3],\n            [0.0, 0.0, 0.0, 1.0]\n        ])\n\n        # The iteration matrix J is defined by z^{k+1} = J * z^k,\n        # where J = inv(L) * R.\n        # This is best solved as L*J = R.\n        J = np.linalg.solve(L, R)\n        \n        # Compute the eigenvalues of the iteration matrix J.\n        eigenvalues = np.linalg.eigvals(J)\n        \n        # The spectral radius is the maximum magnitude of the eigenvalues.\n        spectral_radius = np.max(np.abs(eigenvalues))\n        \n        results.append(spectral_radius)\n\n    # Format the final output as a comma-separated list in brackets,\n    # with each value rounded to six decimal places.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3364446"}]}