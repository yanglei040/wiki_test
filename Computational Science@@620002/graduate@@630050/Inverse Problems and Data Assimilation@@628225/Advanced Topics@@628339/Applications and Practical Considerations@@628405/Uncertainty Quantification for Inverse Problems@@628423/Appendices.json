{"hands_on_practices": [{"introduction": "A cornerstone of uncertainty quantification is understanding how observations update our knowledge of unknown parameters. This exercise provides a concrete, hands-on calculation for the canonical linear-Gaussian inverse problem, which serves as a fundamental building block for more complex scenarios. By deriving and computing the posterior mean and covariance, you will gain direct experience with how the measurement operator and noise statistics combine to reduce uncertainty, transforming a prior belief into a data-informed posterior distribution [@problem_id:3429478].", "problem": "Consider the linear inverse problem with an unknown parameter vector $x \\in \\mathbb{R}^{2}$ and observations $y \\in \\mathbb{R}^{2}$ governed by the forward model $y = A x + \\eta$, where $\\eta$ is zero-mean noise independent of $x$. Assume a Gaussian prior $x \\sim \\mathcal{N}(m_{0}, C_{0})$ and Gaussian observational noise $\\eta \\sim \\mathcal{N}(0, \\Gamma)$. Take the specific numerical values\n$$\nA = \\begin{pmatrix}\n2 & 1 \\\\\n1 & 3\n\\end{pmatrix}, \\quad\nC_{0} = \\mathrm{diag}(1, 4), \\quad\n\\Gamma = 0.25\\, I_{2}, \\quad\nm_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad\ny = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix},\n$$\nwhere $I_{2}$ denotes the $2 \\times 2$ identity matrix. \n\nStarting from Bayes' rule and the definition of the multivariate Gaussian density, derive the posterior distribution of $x$ given $y$ and thereby obtain the posterior mean vector $m^{y}$ and the posterior covariance matrix $C^{y}$ for this linear-Gaussian inverse problem. Compute $m^{y}$ and $C^{y}$ numerically for the given data. Then, interpret the directions in which uncertainty is most reduced by comparing the prior and posterior covariances, making clear how the measurement operator $A$ and the noise statistics shape these directions.\n\nExpress your final numerical values for $m^{y}$ and $C^{y}$ exactly (no rounding). The final answer must be a single row matrix containing, in order, the two entries of $m^{y}$ followed by the four entries of $C^{y}$ listed row-by-row.", "solution": "The user wants me to solve a Bayesian inverse problem.\n\nHere's my thinking process:\n\n1.  **Problem Validation**:\n    *   **Step 1: Extract Givens**: I'll list all the given matrices, vectors, and distributions verbatim.\n        *   Forward model: $y = Ax + \\eta$\n        *   Unknown parameter: $x \\in \\mathbb{R}^{2}$\n        *   Observations: $y \\in \\mathbb{R}^{2}$\n        *   Noise: $\\eta$ is zero-mean and independent of $x$.\n        *   Prior distribution: $x \\sim \\mathcal{N}(m_{0}, C_{0})$\n        *   Noise distribution: $\\eta \\sim \\mathcal{N}(0, \\Gamma)$\n        *   Forward operator: $A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix}$\n        *   Prior covariance: $C_{0} = \\mathrm{diag}(1, 4)$\n        *   Noise covariance: $\\Gamma = 0.25\\, I_{2}$\n        *   Prior mean: $m_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$\n        *   Observation vector: $y = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$\n    *   **Step 2: Validate Using Extracted Givens**:\n        *   **Scientifically Grounded**: The problem is a classic application of Bayesian inference to a linear-Gaussian system. It relies on fundamental principles of probability theory (Bayes' rule, properties of multivariate Gaussian distributions) and linear algebra. This is a standard and well-established problem in fields like statistical signal processing, data assimilation, and inverse problems. It is scientifically sound.\n        *   **Well-Posed**: The problem asks for the posterior mean and covariance. For a linear-Gaussian model, these quantities are uniquely defined and can be computed via established analytical formulas. The given matrices $A$, $C_0$, and $\\Gamma$ are all invertible ($det(A) = 2 \\cdot 3 - 1 \\cdot 1 = 5 \\neq 0$, $det(C_0) = 4 \\neq 0$, $det(\\Gamma) = 0.25^2 \\neq 0$), which ensures that all subsequent matrix operations (like inversions) are well-defined and a unique solution exists. The problem is well-posed.\n        *   **Objective**: The problem is stated using precise mathematical language and notation, with specific numerical values. There are no subjective, ambiguous, or opinion-based statements.\n    *   **Step 3: Verdict and Action**: The problem is valid. I will proceed to derive and compute the solution.\n\n2.  **Solution Derivation and Computation**:\n    *   **Part 1: Analytical Derivation**:\n        *   I'll start with Bayes' rule: $p(x|y) \\propto p(y|x) p(x)$.\n        *   I'll write out the probability density functions (PDFs) for the prior and the likelihood.\n            *   Prior: $p(x) = \\mathcal{N}(m_0, C_0) \\propto \\exp\\left(-\\frac{1}{2}(x - m_0)^T C_0^{-1} (x - m_0)\\right)$.\n            *   Likelihood: Given $x$, the model $y = Ax + \\eta$ with $\\eta \\sim \\mathcal{N}(0, \\Gamma)$ implies that the conditional distribution of $y$ given $x$ is $y|x \\sim \\mathcal{N}(Ax, \\Gamma)$. The likelihood function is $p(y|x) \\propto \\exp\\left(-\\frac{1}{2}(y - Ax)^T \\Gamma^{-1} (y - Ax)\\right)$.\n        *   Next, I'll combine these into the posterior PDF: $p(x|y) \\propto \\exp\\left(-\\frac{1}{2}\\left[(y - Ax)^T \\Gamma^{-1} (y - Ax) + (x - m_0)^T C_0^{-1} (x - m_0)\\right]\\right)$.\n        *   The key step is to show that the expression in the exponent is a quadratic form in $x$, which means the posterior is also Gaussian. I will expand the terms and complete the square for $x$.\n        *   The argument of the exponential is proportional to:\n            $J(x) = (y - Ax)^T \\Gamma^{-1} (y - Ax) + (x - m_0)^T C_0^{-1} (x - m_0)$\n            $J(x) = (x^T A^T - y^T)\\Gamma^{-1}(Ax - y) + (x^T - m_0^T)C_0^{-1}(x-m_0)$\n            $J(x) = x^T A^T \\Gamma^{-1} A x - x^T A^T \\Gamma^{-1} y - y^T \\Gamma^{-1} A x + y^T \\Gamma^{-1} y + x^T C_0^{-1} x - x^T C_0^{-1} m_0 - m_0^T C_0^{-1} x + m_0^T C_0^{-1} m_0$\n        *   Grouping terms in $x$:\n            Quadratic: $x^T (A^T \\Gamma^{-1} A + C_0^{-1}) x$\n            Linear: $-2x^T(A^T \\Gamma^{-1} y + C_0^{-1} m_0)$ (using the fact that scalar terms are equal to their transpose, e.g., $y^T\\Gamma^{-1}Ax = x^T A^T \\Gamma^{-1}y$)\n            Constant: Terms not involving $x$.\n        *   This matches the standard form of a Gaussian exponent, proportional to $(x-m^y)^T(C^y)^{-1}(x-m^y) = x^T(C^y)^{-1}x - 2x^T(C^y)^{-1}m^y + \\text{const}$.\n        *   By comparing the quadratic and linear terms, I'll identify the inverse posterior covariance and the posterior mean:\n            *   $(C^y)^{-1} = C_0^{-1} + A^T \\Gamma^{-1} A$\n            *   $(C^y)^{-1} m^y = C_0^{-1} m_0 + A^T \\Gamma^{-1} y$\n        *   This leads to the final formulas:\n            *   $C^y = (C_0^{-1} + A^T \\Gamma^{-1} A)^{-1}$\n            *   $m^y = C^y (C_0^{-1} m_0 + A^T \\Gamma^{-1} y)$\n    *   **Part 2: Numerical Computation**:\n        *   I'll substitute the given numerical values into these formulas.\n        *   $C_0^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 4 \\end{pmatrix}^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1/4 \\end{pmatrix}$.\n        *   $\\Gamma^{-1} = (0.25 I_2)^{-1} = 4 I_2 = \\begin{pmatrix} 4 & 0 \\\\ 0 & 4 \\end{pmatrix}$.\n        *   Calculate $A^T \\Gamma^{-1} A = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\begin{pmatrix} 4 & 0 \\\\ 0 & 4 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} = \\begin{pmatrix} 8 & 4 \\\\ 4 & 12 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} = \\begin{pmatrix} 20 & 20 \\\\ 20 & 40 \\end{pmatrix}$.\n        *   Calculate the inverse posterior covariance (precision matrix): $(C^y)^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1/4 \\end{pmatrix} + \\begin{pmatrix} 20 & 20 \\\\ 20 & 40 \\end{pmatrix} = \\begin{pmatrix} 21 & 20 \\\\ 20 & 40.25 \\end{pmatrix} = \\begin{pmatrix} 21 & 20 \\\\ 20 & 161/4 \\end{pmatrix}$.\n        *   Invert this to find $C^y$:\n            *   $\\det((C^y)^{-1}) = 21 \\times \\frac{161}{4} - 20 \\times 20 = \\frac{3381}{4} - 400 = \\frac{3381 - 1600}{4} = \\frac{1781}{4}$.\n            *   $C^y = \\frac{4}{1781} \\begin{pmatrix} 161/4 & -20 \\\\ -20 & 21 \\end{pmatrix} = \\frac{1}{1781} \\begin{pmatrix} 161 & -80 \\\\ -80 & 84 \\end{pmatrix}$.\n        *   Now for the mean $m^y$. Since $m_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, the term $C_0^{-1} m_0$ is zero.\n        *   Calculate $A^T \\Gamma^{-1} y = \\begin{pmatrix} 8 & 4 \\\\ 4 & 12 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 8 - 4 \\\\ 4 - 12 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ -8 \\end{pmatrix}$.\n        *   $m^y = C^y (A^T \\Gamma^{-1} y) = \\frac{1}{1781} \\begin{pmatrix} 161 & -80 \\\\ -80 & 84 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ -8 \\end{pmatrix} = \\frac{1}{1781} \\begin{pmatrix} 161(4) - 80(-8) \\\\ -80(4) + 84(-8) \\end{pmatrix} = \\frac{1}{1781} \\begin{pmatrix} 644 + 640 \\\\ -320 - 672 \\end{pmatrix} = \\frac{1}{1781} \\begin{pmatrix} 1284 \\\\ -992 \\end{pmatrix}$.\n    *   **Part 3: Interpretation**:\n        *   I will compare the diagonal elements (variances) of the prior covariance $C_0$ and posterior covariance $C^y$.\n        *   $C_0 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 4 \\end{pmatrix}$. $\\text{Var}(x_1)=1$, $\\text{Var}(x_2)=4$.\n        *   $C^y \\approx \\begin{pmatrix} 0.0904 & -0.0449 \\\\ -0.0449 & 0.0472 \\end{pmatrix}$. $\\text{Var}(x_1|y) \\approx 0.09$, $\\text{Var}(x_2|y) \\approx 0.047$.\n        *   I'll note the significant reduction in both variances, especially for $x_2$.\n        *   I'll explain this greater reduction for $x_2$ by examining the forward operator $A$. The columns of $A$ determine the sensitivity of the data to each parameter. The norm of the second column of $A$ ($||\\begin{pmatrix} 1 \\\\ 3 \\end{pmatrix}||_2 = \\sqrt{10}$) is larger than the norm of the first column ($||\\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}||_2 = \\sqrt{5}$), indicating that the measurements are more sensitive to changes in $x_2$. This allows for a more significant constraint on $x_2$, leading to a greater reduction in its uncertainty.\n        *   I'll also comment on the off-diagonal terms, noting that the measurement process couples the parameters, introducing a negative correlation ($C^y_{12} < 0$) that was absent in the prior.\n    *   **Final Answer Formulation**:\n        *   The problem asks for a single row matrix containing the entries of $m^y$ and then the entries of $C^y$ row-by-row.\n        *   The order will be $[m^y_1, m^y_2, C^y_{11}, C^y_{12}, C^y_{21}, C^y_{22}]$.\n        *   I'll put the exact fractional values into a LaTeX `pmatrix`.\n\nThis structured plan covers all aspects of the problem, ensuring correctness and adherence to formatting rules. I will now generate the final output.", "answer": "$$\n\\boxed{\n\\begin{pmatrix} \\frac{1284}{1781} & -\\frac{992}{1781} & \\frac{161}{1781} & -\\frac{80}{1781} & -\\frac{80}{1781} & \\frac{84}{1781} \\end{pmatrix}\n}\n$$", "id": "3429478"}, {"introduction": "While many systems can be approximated as linear and Gaussian, real-world inverse problems are often ill-posed, a concept formalized by Jacques Hadamard. This thought experiment explores a primary cause of ill-posedness: parameter non-identifiability, where different parameter values produce the same output. By analyzing a simple quadratic forward model, you will see how this issue manifests in a Bayesian framework as a multimodal posterior distribution, challenging the notion of a unique solution and revealing how the posterior variance can become acutely sensitive to data [@problem_id:3382702].", "problem": "Consider a scalar Bayesian inverse problem within the forward and inverse uncertainty quantification paradigms for inverse problems and data assimilation. Let the forward map be $G(\\theta) = \\theta^{2}$ with parameter $\\theta \\in \\mathbb{R}$. A single noisy observation $y \\in \\mathbb{R}$ is acquired via the additive Gaussian noise model $y = G(\\theta) + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$ and $\\sigma > 0$ is known. The prior on the parameter is Gaussian $\\theta \\sim \\mathcal{N}(0, \\tau^{2})$ with $\\tau > 0$ known. All quantities are scalars. Throughout, use Bayes’ rule and principled asymptotic approximation methods grounded in first principles; do not assume identifiability where it fails.\n\nThis setting exhibits non-identifiability because $G(\\theta) = G(-\\theta)$, which can lead to ill-posedness manifested as posterior multimodality and large posterior variance. Starting from first principles (Bayes’ rule, properties of Gaussian distributions, and the definition of well-posedness in the sense of Jacques Hadamard), derive an explicit leading-order analytic approximation of the posterior variance $\\mathrm{Var}[\\theta \\mid y]$ in the regime where the posterior is bimodal. In deriving your expression, justify the condition under which bimodality occurs in terms of $y$, $\\sigma$, and $\\tau$, and discuss the stability implications of this non-identifiability (existence, uniqueness, and continuous dependence on data).\n\nYour final answer must be a single closed-form analytic expression for the leading-order approximation of $\\mathrm{Var}[\\theta \\mid y]$ as a function of $y$, $\\sigma$, and $\\tau$. No rounding is required, and no units are involved.", "solution": "The problem is assessed to be valid as it represents a well-defined and standard exercise in Bayesian inverse problems. It is scientifically grounded in probability theory and statistics, internally consistent, and contains all necessary information to proceed with a formal derivation.\n\nWe are tasked with analyzing the posterior distribution of a parameter $\\theta$ given an observation $y$. The relationship is defined by the forward model $G(\\theta) = \\theta^{2}$ and the observation model $y = \\theta^{2} + \\varepsilon$, where the noise is Gaussian, $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2})$. The prior belief about the parameter is also Gaussian, $\\theta \\sim \\mathcal{N}(0, \\tau^{2})$.\n\nFirst, we construct the posterior probability density function (PDF) $p(\\theta \\mid y)$ using Bayes' rule:\n$$\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta)\n$$\nThe likelihood function $p(y \\mid \\theta)$ is derived from the noise model. Given a value of $\\theta$, $y$ is a random variable distributed as $\\mathcal{N}(\\theta^{2}, \\sigma^{2})$. Thus, the likelihood is:\n$$\np(y \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}} \\exp\\left( -\\frac{(y - \\theta^{2})^{2}}{2\\sigma^{2}} \\right)\n$$\nThe prior PDF is given as:\n$$\np(\\theta) = \\frac{1}{\\sqrt{2\\pi\\tau^{2}}} \\exp\\left( -\\frac{\\theta^{2}}{2\\tau^{2}} \\right)\n$$\nCombining these, the unnormalized posterior PDF is:\n$$\np(\\theta \\mid y) \\propto \\exp\\left( -\\frac{(y - \\theta^{2})^{2}}{2\\sigma^{2}} \\right) \\exp\\left( -\\frac{\\theta^{2}}{2\\tau^{2}} \\right) = \\exp\\left( -\\left[ \\frac{(y - \\theta^{2})^{2}}{2\\sigma^{2}} + \\frac{\\theta^{2}}{2\\tau^{2}} \\right] \\right)\n$$\nTo find the modes of the posterior distribution, we can find the minima of the negative log-posterior, $\\Phi(\\theta)$, where $p(\\theta \\mid y) \\propto \\exp(-\\Phi(\\theta))$:\n$$\n\\Phi(\\theta) = \\frac{(y - \\theta^{2})^{2}}{2\\sigma^{2}} + \\frac{\\theta^{2}}{2\\tau^{2}}\n$$\nThe modes are stationary points, found by setting the first derivative with respect to $\\theta$ to zero:\n$$\n\\frac{d\\Phi}{d\\theta} = \\frac{2(y - \\theta^{2})(-2\\theta)}{2\\sigma^{2}} + \\frac{2\\theta}{2\\tau^{2}} = -\\frac{2\\theta(y - \\theta^{2})}{\\sigma^{2}} + \\frac{\\theta}{\\tau^{2}} = \\theta \\left( \\frac{2\\theta^{2} - 2y}{\\sigma^{2}} + \\frac{1}{\\tau^{2}} \\right) = 0\n$$\nThis equation has solutions at $\\theta = 0$ and where the term in the parentheses is zero:\n$$\n\\frac{2\\theta^{2}}{\\sigma^{2}} = \\frac{2y}{\\sigma^{2}} - \\frac{1}{\\tau^{2}} \\implies \\theta^{2} = y - \\frac{\\sigma^{2}}{2\\tau^{2}}\n$$\nSo, the stationary points are $\\theta_{0} = 0$ and, provided $y > \\frac{\\sigma^{2}}{2\\tau^{2}}$, $\\theta_{\\pm} = \\pm \\sqrt{y - \\frac{\\sigma^{2}}{2\\tau^{2}}}$.\n\nTo determine if these are modes (local maxima of the PDF, minima of $\\Phi$) or antimodes (local minima of the PDF, maxima of $\\Phi$), we examine the second derivative of $\\Phi(\\theta)$:\n$$\n\\frac{d^{2}\\Phi}{d\\theta^{2}} = \\frac{d}{d\\theta} \\left( \\frac{2\\theta^{3}}{\\sigma^{2}} - \\frac{2y\\theta}{\\sigma^{2}} + \\frac{\\theta}{\\tau^{2}} \\right) = \\frac{6\\theta^{2}}{\\sigma^{2}} - \\frac{2y}{\\sigma^{2}} + \\frac{1}{\\tau^{2}}\n$$\nAt $\\theta = 0$:\n$$\n\\left.\\frac{d^{2}\\Phi}{d\\theta^{2}}\\right|_{\\theta=0} = \\frac{1}{\\tau^{2}} - \\frac{2y}{\\sigma^{2}}\n$$\nFor $\\theta=0$ to be a mode, we require $\\left.\\frac{d^{2}\\Phi}{d\\theta^{2}}\\right|_{\\theta=0} > 0$, which means $y < \\frac{\\sigma^{2}}{2\\tau^{2}}$. In this case, the posterior is unimodal.\nFor $\\theta=0$ to be an antimode, we require $\\left.\\frac{d^{2}\\Phi}{d\\theta^{2}}\\right|_{\\theta=0} < 0$, which means $y > \\frac{\\sigma^{2}}{2\\tau^{2}}$.\n\nAt $\\theta_{\\pm} = \\pm \\sqrt{y - \\frac{\\sigma^{2}}{2\\tau^{2}}}$, substituting $\\theta_{\\pm}^{2} = y - \\frac{\\sigma^{2}}{2\\tau^{2}}$:\n$$\n\\left.\\frac{d^{2}\\Phi}{d\\theta^{2}}\\right|_{\\theta_{\\pm}} = \\frac{6}{\\sigma^{2}}\\left(y - \\frac{\\sigma^{2}}{2\\tau^{2}}\\right) - \\frac{2y}{\\sigma^{2}} + \\frac{1}{\\tau^{2}} = \\frac{6y}{\\sigma^{2}} - \\frac{3}{\\tau^{2}} - \\frac{2y}{\\sigma^{2}} + \\frac{1}{\\tau^{2}} = \\frac{4y}{\\sigma^{2}} - \\frac{2}{\\tau^{2}} = 2\\left(\\frac{2y}{\\sigma^{2}} - \\frac{1}{\\tau^{2}}\\right)\n$$\nThe points $\\theta_{\\pm}$ exist only when $y > \\frac{\\sigma^{2}}{2\\tau^{2}}$, which implies $\\frac{2y}{\\sigma^{2}} > \\frac{1}{\\tau^{2}}$. Therefore, $\\left.\\frac{d^{2}\\Phi}{d\\theta^{2}}\\right|_{\\theta_{\\pm}} > 0$.\nThis confirms that when $y > \\frac{\\sigma^{2}}{2\\tau^{2}}$, the posterior distribution is bimodal, with two modes at $\\theta_{\\pm} = \\pm \\sqrt{y - \\frac{\\sigma^{2}}{2\\tau^{2}}}$ and an antimode at $\\theta = 0$.\n\nNow we derive the leading-order approximation for the posterior variance $\\mathrm{Var}[\\theta \\mid y]$ in this bimodal regime. A common principled approximation is the Laplace approximation, which models the posterior as a mixture of Gaussian distributions centered at each mode. Due to the symmetry of the forward model and prior, the posterior is symmetric about $\\theta=0$. Therefore, the posterior can be approximated as a symmetric mixture of two Gaussians:\n$$\np(\\theta \\mid y) \\approx \\frac{1}{2} \\mathcal{N}(\\theta \\mid \\mu_{+}, \\sigma_{L}^{2}) + \\frac{1}{2} \\mathcal{N}(\\theta \\mid \\mu_{-}, \\sigma_{L}^{2})\n$$\nwhere the means $\\mu_{\\pm}$ are the mode locations:\n$$\n\\mu_{\\pm} = \\theta_{\\pm} = \\pm \\sqrt{y - \\frac{\\sigma^{2}}{2\\tau^{2}}}\n$$\nand the local variance $\\sigma_{L}^{2}$ is the inverse of the Hessian of $\\Phi(\\theta)$ evaluated at the modes:\n$$\n\\sigma_{L}^{2} = \\left( \\left.\\frac{d^{2}\\Phi}{d\\theta^{2}}\\right|_{\\theta_{\\pm}} \\right)^{-1} = \\left( 2\\left(\\frac{2y}{\\sigma^{2}} - \\frac{1}{\\tau^{2}}\\right) \\right)^{-1} = \\left( \\frac{4y\\tau^{2} - 2\\sigma^{2}}{\\sigma^{2}\\tau^{2}} \\right)^{-1} = \\frac{\\sigma^{2}\\tau^{2}}{4y\\tau^{2} - 2\\sigma^{2}}\n$$\nThe total variance of this Gaussian mixture model is given by the law of total variance:\n$$\n\\mathrm{Var}[\\theta \\mid y] = \\mathbb{E}[\\mathrm{Var}[\\theta \\mid y, Z]] + \\mathrm{Var}[\\mathbb{E}[\\theta \\mid y, Z]]\n$$\nwhere $Z$ is a latent variable indicating the component of the mixture.\nThe first term is the average of the variances of the components:\n$$\n\\mathbb{E}[\\mathrm{Var}[\\theta \\mid y, Z]] = \\frac{1}{2}\\sigma_{L}^{2} + \\frac{1}{2}\\sigma_{L}^{2} = \\sigma_{L}^{2}\n$$\nThe second term is the variance of the means of the components. The mean of the mixture is $\\frac{1}{2}\\mu_{+} + \\frac{1}{2}\\mu_{-} = 0$.\n$$\n\\mathrm{Var}[\\mathbb{E}[\\theta \\mid y, Z]] = \\mathbb{E}[(\\mathbb{E}[\\theta \\mid y, Z])^{2}] - (\\mathbb{E}[\\mathbb{E}[\\theta \\mid y, Z]])^{2} = \\left( \\frac{1}{2}\\mu_{+}^{2} + \\frac{1}{2}\\mu_{-}^{2} \\right) - 0^{2} = \\mu_{+}^{2}\n$$\nCombining these terms, the total posterior variance is approximated by:\n$$\n\\mathrm{Var}[\\theta \\mid y] \\approx \\mu_{+}^{2} + \\sigma_{L}^{2}\n$$\nSubstituting the expressions for $\\mu_{+}^{2}$ and $\\sigma_{L}^{2}$:\n$$\n\\mathrm{Var}[\\theta \\mid y] \\approx \\left(y - \\frac{\\sigma^{2}}{2\\tau^{2}}\\right) + \\frac{\\sigma^{2}\\tau^{2}}{4y\\tau^{2} - 2\\sigma^{2}}\n$$\nThis expression is the leading-order approximation for the posterior variance in the bimodal regime ($y > \\frac{\\sigma^{2}}{2\\tau^{2}}$).\n\nFinally, we discuss the stability implications in the sense of Hadamard:\n1.  **Existence:** A solution, in the Bayesian sense, is the posterior distribution $p(\\theta \\mid y)$. As the prior is proper and the likelihood is well-defined, the posterior exists for any $y \\in \\mathbb{R}$.\n2.  **Uniqueness:** Uniqueness fails. In the bimodal regime, there are two equally likely solutions for $\\theta$, represented by the modes $\\mu_{\\pm}$. A single point estimate, such as a Maximum A Posteriori (MAP) estimate, is not unique. The posterior mean is $\\mathbb{E}[\\theta \\mid y] = 0$ by symmetry, but this point has minimum posterior probability and is a poor representation of the solution. This lack of a unique, representative point estimate is a manifestation of ill-posedness.\n3.  **Continuous Dependence on Data (Stability):** Stability also fails dramatically. The transition from a unimodal to a bimodal posterior occurs at the critical data value $y_{c} = \\frac{\\sigma^{2}}{2\\tau^{2}}$. As the data $y$ approaches this critical value from above ($y \\to y_{c}^{+}$), the term representing the local variance, $\\sigma_{L}^{2}$, diverges:\n$$\n\\lim_{y \\to (\\frac{\\sigma^{2}}{2\\tau^{2}})^{+}} \\sigma_{L}^{2} = \\lim_{y \\to (\\frac{\\sigma^{2}}{2\\tau^{2}})^{+}} \\frac{\\sigma^{2}\\tau^{2}}{4y\\tau^{2} - 2\\sigma^{2}} = \\frac{\\sigma^{2}\\tau^{2}}{4(\\frac{\\sigma^{2}}{2\\tau^{2}})\\tau^{2} - 2\\sigma^{2}} = \\frac{\\sigma^{2}\\tau^{2}}{2\\sigma^{2} - 2\\sigma^{2}} \\to \\infty\n$$\nThis divergence shows that the posterior variance blows up at the bifurcation point. A small perturbation of the data $y$ around $y_c$ can cause a drastic change in the posterior distribution's shape and a massive change in its variance. This extreme sensitivity to data illustrates a severe lack of stability, a hallmark of ill-posed inverse problems. The non-identifiability introduced by the forward map $G(\\theta) = \\theta^{2}$ is the root cause of this failure of uniqueness and stability.", "answer": "$$\n\\boxed{y - \\frac{\\sigma^{2}}{2\\tau^{2}} + \\frac{\\sigma^{2}\\tau^{2}}{4y\\tau^{2} - 2\\sigma^{2}}}\n$$", "id": "3382702"}, {"introduction": "Having understood that ill-posedness can create directions of high uncertainty, a crucial practical skill is to diagnose and characterize them in high-dimensional systems. This computational exercise introduces a powerful technique for this purpose, using the singular value decomposition (SVD) of a prior-preconditioned forward operator to analyze the posterior covariance structure. By implementing this analysis, you will learn to systematically identify posterior \"ridges\"—directions in parameter space that are poorly constrained by the data—and quantify the degree to which data informs different parameter combinations [@problem_id:3429460].", "problem": "Consider a linearized Bayesian inverse problem with parameter vector $x \\in \\mathbb{R}^n$, linear forward map $G \\in \\mathbb{R}^{m \\times n}$, additive measurement noise $\\eta \\in \\mathbb{R}^m$ modeled as zero-mean Gaussian with covariance $C_n \\in \\mathbb{R}^{m \\times m}$, and a Gaussian prior on $x$ with zero mean and covariance $C_0 \\in \\mathbb{R}^{n \\times n}$. The observed data are $y \\in \\mathbb{R}^m$ satisfying $y = G x + \\eta$. Assume $C_0$ and $C_n$ are symmetric positive definite. The goal is to characterize posterior uncertainty ridges by analyzing identifiability along the nullspace using singular functions of the linearized forward map.\n\nStarting from Bayes' rule for Gaussian measures and the linearization at a reference point, the posterior distribution of $x$ is Gaussian with covariance determined by the interplay between the prior precision $C_0^{-1}$ and the data misfit curvature $G^\\top C_n^{-1} G$. Define the prior-preconditioned forward operator $A$ by\n$$\nA = C_n^{-1/2} \\, G \\, C_0^{1/2},\n$$\nwhere $C_0^{1/2}$ and $C_n^{-1/2}$ denote the unique symmetric positive definite square root of $C_0$ and the inverse of the square root of $C_n$, respectively. Let the singular value decomposition of $A$ be expressed through its right singular vectors as the eigen-decomposition of the symmetric matrix\n$$\nH = A^\\top A = C_0^{1/2} \\, G^\\top \\, C_n^{-1} \\, G \\, C_0^{1/2}.\n$$\nIf $\\lambda_i$ are the eigenvalues of $H$ with corresponding orthonormal eigenvectors $v_i \\in \\mathbb{R}^n$ (the right singular vectors of $A$), then the singular values are $s_i = \\sqrt{\\lambda_i}$. In the prior-coordinate system given by $z = C_0^{-1/2} x$, the posterior covariance has eigenvalues along $v_i$ equal to\n$$\n\\gamma_i = \\frac{1}{1 + s_i^2}.\n$$\nDirections with small $s_i$ correspond to weak data information and large posterior variance factors $\\gamma_i$, forming posterior ridges. Exact nullspace directions have $s_i = 0$ and yield $\\gamma_i = 1$, indicating no reduction from the prior.\n\nYour task is to implement a program that, for a given test suite of matrices $(G, C_0, C_n)$, computes:\n- the full set of singular values $s_i$ of $A$ via the eigenvalues of $H = A^\\top A$,\n- the posterior variance factors $\\gamma_i = 1/(1 + s_i^2)$ along the right singular vectors $v_i$,\n- a binary ridge classification for each direction defined as $\\text{ridge}_i = \\text{True}$ if $s_i \\le s_{\\text{thresh}}$ and $\\text{ridge}_i = \\text{False}$ otherwise,\n- the dimension of the numerical nullspace defined as the count of $s_i \\le s_{\\text{zero}}$.\n\nUse the following numerical thresholds:\n- $s_{\\text{thresh}} = 10^{-3}$ for ridge classification,\n- $s_{\\text{zero}} = 10^{-12}$ for nullspace dimension.\n\nFor each test case, output a list containing:\n- the integer nullspace dimension,\n- the list of singular values $s_i$ sorted in descending order, each rounded to $8$ decimal places,\n- the list of posterior variance factors $\\gamma_i$ in the same order, each rounded to $8$ decimal places,\n- the list of boolean ridge flags in the same order.\n\nConstruct and use the following test suite:\n- Test case $1$ (rank-deficient, pronounced nullspace): $m = 2$, $n = 3$, $G = \\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 0 & 0\\end{bmatrix}$, $C_0 = \\mathrm{diag}(1, 4, 9)$, $C_n = \\mathrm{diag}(0.01, 0.01)$.\n- Test case $2$ (boundary case at threshold): $m = 2$, $n = 3$, $G = \\begin{bmatrix}10^{-3} & 0 & 0 \\\\ 0 & 0 & 10^{-3}\\end{bmatrix}$, $C_0 = \\mathrm{diag}(1, 1, 1)$, $C_n = \\mathrm{diag}(1, 1)$.\n- Test case $3$ (full rank, strongly identifiable): $m = 3$, $n = 3$, $G = \\mathrm{diag}(5, 2, 1)$, $C_0 = I_{3 \\times 3}$, $C_n = \\mathrm{diag}(0.04, 0.04, 0.04)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[ \\text{result1}, \\text{result2}, \\text{result3} ]$), where each $\\text{result}$ is the list for a test case as specified above. No physical units or angle units are involved. Numeric answers must be floats or integers as stated, and lists must be composed of these types or booleans.", "solution": "The user wants me to solve a problem on uncertainty quantification for inverse problems. I've been asked to critically validate the problem statement first.\n\n### Step 1: Extract Givens\n- **Model**: Linearized Bayesian inverse problem with observation equation $y = G x + \\eta$.\n- **Parameters**: $x \\in \\mathbb{R}^n$ is the parameter vector.\n- **Forward Operator**: $G \\in \\mathbb{R}^{m \\times n}$ is the linear forward map.\n- **Observation**: $y \\in \\mathbb{R}^m$ is the observed data.\n- **Noise Model**: $\\eta \\in \\mathbb{R}^m$ is the measurement noise, modeled as a zero-mean Gaussian with covariance $C_n \\in \\mathbb{R}^{m \\times m}$. $C_n$ is symmetric positive definite.\n- **Prior Model**: $x$ follows a Gaussian prior with zero mean and covariance $C_0 \\in \\mathbb{R}^{n \\times n}$. $C_0$ is symmetric positive definite.\n- **Prior-Preconditioned Operator**: $A = C_n^{-1/2} \\, G \\, C_0^{1/2}$.\n- **Preconditioned Hessian**: $H = A^\\top A = C_0^{1/2} \\, G^\\top \\, C_n^{-1} \\, G \\, C_0^{1/2}$.\n- **Singular Values**: Singular values $s_i$ of $A$ are related to the eigenvalues $\\lambda_i$ of $H$ by $s_i = \\sqrt{\\lambda_i}$. The right singular vectors of $A$ are the eigenvectors $v_i$ of $H$.\n- **Posterior Variance Factors**: $\\gamma_i = \\frac{1}{1 + s_i^2}$.\n- **Numerical Thresholds**:\n    - Ridge classification threshold: $s_{\\text{thresh}} = 10^{-3}$.\n    - Nullspace dimension threshold: $s_{\\text{zero}} = 10^{-12}$.\n- **Test Cases**:\n    1.  $m = 2$, $n = 3$, $G = \\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 0 & 0\\end{bmatrix}$, $C_0 = \\mathrm{diag}(1, 4, 9)$, $C_n = \\mathrm{diag}(0.01, 0.01)$.\n    2.  $m = 2$, $n = 3$, $G = \\begin{bmatrix}10^{-3} & 0 & 0 \\\\ 0 & 0 & 10^{-3}\\end{bmatrix}$, $C_0 = \\mathrm{diag}(1, 1, 1)$, $C_n = \\mathrm{diag}(1, 1)$.\n    3.  $m = 3$, $n = 3$, $G = \\mathrm{diag}(5, 2, 1)$, $C_0 = I_{3 \\times 3}$, $C_n = \\mathrm{diag}(0.04, 0.04, 0.04)$.\n- **Output Requirements**: For each test case, output a list containing:\n    1.  Integer nullspace dimension (count of $s_i \\le s_{\\text{zero}}$).\n    2.  List of singular values $s_i$ sorted descending, rounded to $8$ decimal places.\n    3.  List of posterior variance factors $\\gamma_i$ in the same order, rounded to $8$ decimal places.\n    4.  List of boolean ridge flags ($s_i \\le s_{\\text{thresh}}$) in the same order.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the validation criteria:\n\n-   **Scientifically Grounded**: The problem is firmly rooted in the theory of Bayesian inverse problems, a core topic in computational science, engineering, and statistics. The use of Gaussian priors and noise models is standard practice. The analysis via the singular value decomposition of a preconditioned operator is a well-established technique for uncertainty quantification. All concepts are standard and factually sound.\n-   **Well-Posed**: The problem provides all necessary inputs ($G$, $C_0$, $C_n$ for each case), clear definitions, and specific computational tasks and output formats. The existence of the unique symmetric positive definite square roots $C_0^{1/2}$ and $C_n^{1/2}$ is guaranteed since $C_0$ and $C_n$ are given as symmetric positive definite matrices. The subsequent calculations lead to a unique and stable solution for each test case.\n-   **Objective**: The problem is stated in precise mathematical language. The tasks are quantitative and involve no subjective interpretation.\n-   **No Flaws Detected**: The problem does not violate any of the specified flaw categories. It is scientifically sound, formalizable, complete, and well-structured.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full, reasoned solution will be provided.\n\n### Solution\n\nThe problem requires an analysis of the posterior uncertainty for a linear Bayesian inverse problem. The solution involves computing several quantities derived from the singular values of a prior-preconditioned forward operator.\n\n**Theoretical Framework**\n\nThe foundation of the problem lies in Bayes' theorem. Given the prior probability density function (PDF) $\\pi_{\\text{pr}}(x)$ and the likelihood function $\\pi_{\\text{like}}(y|x)$, the posterior PDF $\\pi_{\\text{post}}(x|y)$ is proportional to their product:\n$$\n\\pi_{\\text{post}}(x|y) \\propto \\pi_{\\text{like}}(y|x) \\pi_{\\text{pr}}(x)\n$$\nIn this problem, the prior on $x$ is a zero-mean Gaussian with covariance $C_0$, so its PDF is proportional to $\\exp\\left(-\\frac{1}{2} x^\\top C_0^{-1} x\\right)$. The additive noise $\\eta$ is a zero-mean Gaussian with covariance $C_n$, so the likelihood for the observation $y = Gx + \\eta$ is proportional to $\\exp\\left(-\\frac{1}{2} (y - Gx)^\\top C_n^{-1} (y - Gx)\\right)$.\n\nCombining these, the posterior PDF is proportional to the exponential of a quadratic form in $x$, indicating that the posterior is also Gaussian. The posterior covariance matrix $C_{\\text{post}}$ is given by the inverse of the sum of the prior precision and the data-misfit Hessian:\n$$\nC_{\\text{post}} = (C_0^{-1} + G^\\top C_n^{-1} G)^{-1}\n$$\n\n**Preconditioning and Singular Value Decomposition**\n\nTo analyze the structure of the posterior uncertainty, it is advantageous to work in a coordinate system where the prior is whitened. We define a change of variables $z = C_0^{-1/2} x$, where $C_0^{1/2}$ is the symmetric positive definite square root of $C_0$. In these coordinates, the prior on $z$ is a standard normal distribution, $z \\sim \\mathcal{N}(0, I)$. The forward model becomes $y = G C_0^{1/2} z + \\eta$.\n\nFurther simplification is achieved by whitening the data space. We multiply the model by $C_n^{-1/2}$, the inverse of the symmetric square root of the noise covariance. This leads to a transformed problem:\n$$\n\\tilde{y} = C_n^{-1/2} G C_0^{1/2} z + \\tilde{\\eta}\n$$\nwhere $\\tilde{y} = C_n^{-1/2} y$ and the transformed noise $\\tilde{\\eta} = C_n^{-1/2} \\eta$ is standard normal, $\\tilde{\\eta} \\sim \\mathcal{N}(0, I)$. The operator $A = C_n^{-1/2} G C_0^{1/2}$ is the prior-preconditioned forward operator.\n\nThe posterior precision for $z$ is $(I + A^\\top A)$. Let the singular value decomposition (SVD) of $A$ be $A = U S V^\\top$, where $V$ is an orthogonal matrix whose columns $v_i$ are the right singular vectors of $A$ and $S$ is a rectangular diagonal matrix of singular values $s_i$. The columns $v_i$ form an orthonormal basis for the parameter space in $z$-coordinates. The matrix $A^\\top A$ can be written as:\n$$\nA^\\top A = (U S V^\\top)^\\top (U S V^\\top) = V S^\\top U^\\top U S V^\\top = V S^\\top S V^\\top\n$$\nThe matrix $H = A^\\top A$ is symmetric, and its eigenvectors are the right singular vectors $v_i$ of $A$. The associated eigenvalues are $\\lambda_i = s_i^2$.\n\n**Posterior Uncertainty Analysis**\n\nThe posterior covariance for $z$ is $C_{\\text{post},z} = (I + A^\\top A)^{-1}$. In the basis of eigenvectors $v_i$, this covariance is diagonal:\n$$\nC_{\\text{post},z} = (V (I + S^\\top S) V^\\top)^{-1} = V (I + S^\\top S)^{-1} V^\\top\n$$\nThe diagonal entries of $(I + S^\\top S)^{-1}$ are the posterior variance factors $\\gamma_i = \\frac{1}{1+s_i^2}$. These factors quantify the reduction of variance from the prior to the posterior along each principal direction $v_i$.\n- If $s_i \\gg 1$, then $\\gamma_i \\approx 1/s_i^2 \\ll 1$. The data strongly constrain this direction, and uncertainty is significantly reduced.\n- If $s_i \\ll 1$, then $\\gamma_i \\approx 1$. The data provide little information, and the posterior variance is nearly equal to the prior variance ($1$ in $z$-coordinates). These directions form \"ridges\" of high posterior probability.\n- If $s_i = 0$, the direction lies in the nullspace of $A$. Then $\\gamma_i = 1$, and the posterior variance equals the prior variance; the data provide no information in this direction.\n\n**Computational Algorithm**\n\nFor each test case $(G, C_0, C_n)$:\n1.  Compute the symmetric positive definite square root of the prior covariance, $C_0^{1/2}$. Since the given $C_0$ matrices are diagonal, $C_{0,ii}^{1/2} = \\sqrt{C_{0,ii}}$. For general symmetric positive definite matrices, a function like `scipy.linalg.sqrtm` would be used.\n2.  Compute the inverse of the noise covariance, $C_n^{-1}$. Since the given $C_n$ matrices are diagonal, $C_{n,ii}^{-1} = 1/C_{n,ii}$. For general matrices, `numpy.linalg.inv` would be used.\n3.  Construct the preconditioned Hessian matrix $H = C_0^{1/2} G^\\top C_n^{-1} G C_0^{1/2}$. Note that since $C_0$ and $C_0^{1/2}$ are symmetric, $(C_0^{1/2})^\\top = C_0^{1/2}$.\n4.  Compute the eigenvalues $\\lambda_i$ of the symmetric matrix $H$ using a numerically stable algorithm (e.g., `numpy.linalg.eigh`).\n5.  Sort the eigenvalues in descending order: $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n$.\n6.  Calculate the singular values of $A$ as $s_i = \\sqrt{\\lambda_i}$. Due to potential floating-point inaccuracies, any small negative eigenvalues should be treated as zero before taking the square root.\n7.  Calculate the posterior variance factors for each direction: $\\gamma_i = 1 / (1 + s_i^2)$.\n8.  Classify each direction as a ridge or not: $\\text{ridge}_i = \\text{True}$ if $s_i \\le s_{\\text{thresh}} = 10^{-3}$, and $\\text{False}$ otherwise.\n9.  Compute the dimension of the numerical nullspace by counting the number of singular values $s_i$ for which $s_i \\le s_{\\text{zero}} = 10^{-12}$.\n10. Assemble the results into a list with the specified format and rounding.\n\nThis procedure will be applied to each of the three test cases provided.", "answer": "[[2, [10.0, 0.0, 0.0], [0.00990099, 1.0, 1.0], [False, True, True]], [1, [0.001, 0.001, 0.0], [0.999999, 0.999999, 1.0], [True, True, True]], [0, [25.0, 10.0, 5.0], [0.00159744, 0.00990099, 0.03846154], [False, False, False]]]", "id": "3429460"}]}