{"hands_on_practices": [{"introduction": "A cornerstone of data assimilation is finding the optimal state estimate that reconciles a model forecast with observations. This first exercise provides a foundational, hands-on implementation of the Maximum A Posteriori (MAP) estimator, starting from fundamental Bayesian principles. You will derive and code two analytically equivalent solutions: one that directly uses the inverse observation error covariance $R^{-1}$ for weighting, and another that first \"whitens\" the system by transforming it into a space where observation errors are uncorrelated and have unit variance. This practice solidifies the theoretical understanding of whitening and develops crucial skills in implementing these solutions with numerically stable linear algebra, ensuring the results are not just theoretically sound but also computationally robust. [@problem_id:3388472]", "problem": "Consider a linear Bayesian inverse problem with Gaussian prior and Gaussian observational noise in a variational data assimilation setting. Let the state vector be $x \\in \\mathbb{R}^{n}$, the observation operator be $H \\in \\mathbb{R}^{m \\times n}$, the observation vector be $y \\in \\mathbb{R}^{m}$, the prior mean be $x_{b} \\in \\mathbb{R}^{n}$, and the prior covariance be $B \\in \\mathbb{R}^{n \\times n}$. Let the observational error covariance be $R \\in \\mathbb{R}^{m \\times m}$, symmetric positive definite but heteroscedastic (i.e., possibly non-uniform variances and correlations across observations). The Maximum A Posteriori (MAP) estimator minimizes the negative log-posterior, which combines the prior term and the data-misfit term weighted by $R^{-1}$. Whitening refers to constructing a transformation that maps the data space to one in which observational errors are independent with unit variance.\n\nYour tasks are to derive, implement, and validate a whitening-based variational solution, starting strictly from the following fundamental base: (i) the definition of a multivariate Gaussian density with mean and covariance; (ii) the equivalence between the MAP estimator and the minimizer of a sum of quadratic forms arising from Gaussian prior and likelihood; and (iii) the definition of Cholesky factorization for symmetric positive definite matrices. You must not invoke any special-case formula without logical derivation from these bases. The program you produce must compute and compare the MAP solution obtained directly (unwhitened) and via whitening, verify the equivalence of costs and solutions, and quantify whitening accuracy.\n\nYou are given a fixed linear model and four test cases for the observational noise covariance. The state dimension is $n = 3$ and the observation dimension is $m = 5$. The common ingredients for all test cases are:\n- Observation operator $H \\in \\mathbb{R}^{5 \\times 3}$:\n  $$\n  H \\;=\\; \\begin{bmatrix}\n  1.0 & 0.0 & 0.5 \\\\\n  0.0 & 1.0 & -0.2 \\\\\n  0.5 & -0.3 & 1.0 \\\\\n  1.0 & 1.0 & 0.0 \\\\\n  0.0 & 0.5 & 0.5\n  \\end{bmatrix}.\n  $$\n- Prior mean $x_{b} \\in \\mathbb{R}^{3}$:\n  $$\n  x_{b} \\;=\\; \\begin{bmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\end{bmatrix}.\n  $$\n- Observations $y \\in \\mathbb{R}^{5}$:\n  $$\n  y \\;=\\; \\begin{bmatrix} 1.2 \\\\ -0.7 \\\\ 0.9 \\\\ 0.5 \\\\ -0.2 \\end{bmatrix}.\n  $$\n- Prior covariance $B \\in \\mathbb{R}^{3 \\times 3}$ specified via its lower-triangular Cholesky factor $S_{B} \\in \\mathbb{R}^{3 \\times 3}$ with strictly positive diagonal, such that $B \\;=\\; S_{B}\\,S_{B}^{\\top}$. Use\n  $$\n  S_{B} \\;=\\; \\begin{bmatrix}\n  1.0 & 0.0 & 0.0 \\\\\n  0.3 & 1.1 & 0.0 \\\\\n  -0.2 & 0.1 & 0.9\n  \\end{bmatrix}.\n  $$\n\nFor each test case $k \\in \\{1,2,3,4\\}$, the observational covariance $R^{(k)}$ is specified by a lower-triangular Cholesky factor $L_{R}^{(k)} \\in \\mathbb{R}^{5 \\times 5}$ with strictly positive diagonal, such that $R^{(k)} \\;=\\; L_{R}^{(k)} \\,\\left(L_{R}^{(k)}\\right)^{\\top}$. The four cases are:\n- Case $1$ (heteroscedastic diagonal):\n  $$\n  L_{R}^{(1)} \\;=\\; \\mathrm{diag}\\!\\left(0.5,\\,1.0,\\,0.8,\\,1.2,\\,0.6\\right).\n  $$\n- Case $2$ (heteroscedastic correlated):\n  $$\n  L_{R}^{(2)} \\;=\\; \\begin{bmatrix}\n  0.9 & 0.0 & 0.0 & 0.0 & 0.0 \\\\\n  0.2 & 0.7 & 0.0 & 0.0 & 0.0 \\\\\n  0.1 & -0.15 & 0.8 & 0.0 & 0.0 \\\\\n  0.05 & 0.1 & -0.05 & 1.1 & 0.0 \\\\\n  -0.1 & 0.05 & 0.2 & -0.1 & 0.6\n  \\end{bmatrix}.\n  $$\n- Case $3$ (ill-conditioned diagonal, yet positive definite):\n  $$\n  L_{R}^{(3)} \\;=\\; \\mathrm{diag}\\!\\left(0.1,\\,0.9,\\,1.2,\\,0.7,\\,0.5\\right).\n  $$\n- Case $4$ (heteroscedastic correlated, different scales):\n  $$\n  L_{R}^{(4)} \\;=\\; \\begin{bmatrix}\n  0.4 & 0.0 & 0.0 & 0.0 & 0.0 \\\\\n  0.1 & 0.5 & 0.0 & 0.0 & 0.0 \\\\\n  0.0 & 0.2 & 0.6 & 0.0 & 0.0 \\\\\n  0.0 & -0.1 & 0.1 & 0.9 & 0.0 \\\\\n  0.2 & 0.0 & -0.05 & 0.1 & 0.7\n  \\end{bmatrix}.\n  $$\n\nUsing only the bases listed earlier, derive and implement the following for each case $k$:\n- Construct $R^{(k)}$ from $L_{R}^{(k)}$ by $R^{(k)} \\;=\\; L_{R}^{(k)} \\,\\left(L_{R}^{(k)}\\right)^{\\top}$. Define a whitening operator $W^{(k)}$ by solving $L_{R}^{(k)}\\,W^{(k)} \\;=\\; I_{m}$, so that $W^{(k)} \\, R^{(k)} \\, \\left(W^{(k)}\\right)^{\\top} \\;=\\; I_{m}$.\n- Form the whitened operators $\\widetilde{H}^{(k)} \\;=\\; W^{(k)} \\, H$ and $\\widetilde{d}^{(k)} \\;=\\; W^{(k)} \\, y$.\n- Let $B \\;=\\; S_{B}\\,S_{B}^{\\top}$ and define $B^{-1}$ via $S_{B}$. Implement two MAP solvers:\n  1. Unwhitened: minimize the quadratic cost $J(x) \\;=\\; \\tfrac{1}{2}\\,\\lVert S_{B}^{-1}\\,(x - x_{b}) \\rVert_{2}^{2} \\;+\\; \\tfrac{1}{2}\\,\\lVert \\left(L_{R}^{(k)}\\right)^{-1}\\,\\left(H x - y\\right) \\rVert_{2}^{2}$.\n  2. Whitened: minimize the quadratic cost $\\widetilde{J}(x) \\;=\\; \\tfrac{1}{2}\\,\\lVert S_{B}^{-1}\\,(x - x_{b}) \\rVert_{2}^{2} \\;+\\; \\tfrac{1}{2}\\,\\lVert \\widetilde{H}^{(k)} x - \\widetilde{d}^{(k)} \\rVert_{2}^{2}$.\n  Both solvers must be implemented using numerically stable linear algebra based on Cholesky factorizations and triangular solves, not explicit matrix inversion.\n- Compute the following diagnostics for each case $k$:\n  1. The Euclidean norm $\\lVert x_{\\mathrm{unw}}^{(k)} - x_{\\mathrm{w}}^{(k)} \\rVert_{2}$ between the unwhitened solution and the whitened solution.\n  2. The absolute difference between the two costs evaluated at the whitened solution, i.e., $\\left| J\\!\\left(x_{\\mathrm{w}}^{(k)}\\right) - \\widetilde{J}\\!\\left(x_{\\mathrm{w}}^{(k)}\\right) \\right|$.\n  3. The maximum absolute entry of $W^{(k)} \\, R^{(k)} \\, \\left(W^{(k)}\\right)^{\\top} - I_{m}$.\n\nTest Suite:\n- Use the four cases $k \\in \\{1,2,3,4\\}$ defined above. This suite covers a diagonal heteroscedastic case, a correlated heteroscedastic case, an ill-conditioned diagonal case, and a correlated case with different scales.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the $12$ floating-point results as a comma-separated list enclosed in square brackets, ordered by test case and, within each case, by the three diagnostics in the order listed above. Concretely, the output must be\n  $$\n  \\big[\\;\\lVert x_{\\mathrm{unw}}^{(1)} - x_{\\mathrm{w}}^{(1)} \\rVert_{2},\\;\\left| J\\!\\left(x_{\\mathrm{w}}^{(1)}\\right) - \\widetilde{J}\\!\\left(x_{\\mathrm{w}}^{(1)}\\right) \\right|,\\;\\max\\!\\left|W^{(1)} R^{(1)} \\left(W^{(1)}\\right)^{\\top} - I\\right|,\\;\\ldots,\\;\\lVert x_{\\mathrm{unw}}^{(4)} - x_{\\mathrm{w}}^{(4)} \\rVert_{2},\\;\\left| J\\!\\left(x_{\\mathrm{w}}^{(4)}\\right) - \\widetilde{J}\\!\\left(x_{\\mathrm{w}}^{(4)}\\right) \\right|,\\;\\max\\!\\left|W^{(4)} R^{(4)} \\left(W^{(4)}\\right)^{\\top} - I\\right|\\;\\big].\n  $$\n- No units are involved. All angles, if any appear, must be interpreted in radians; however, there are no angles in this problem.\n\nConstraints:\n- The program must be a complete, runnable program that takes no input, uses no external files or network access, and strictly prints the final output line in the specified format.\n- Use only numerically stable operations based on Cholesky factorizations and triangular solves to realize $B^{-1}$, $R^{-1}$, and whitening. Avoid forming explicit matrix inverses.", "solution": "The problem requires the derivation and implementation of a Maximum A Posteriori (MAP) estimator for a linear Bayesian inverse problem, with a comparison between a direct (unwhitened) and a whitening-based approach. The entire derivation must be founded on three principles: the definition of a multivariate Gaussian density, the equivalence of the MAP estimator to a minimizer of a quadratic cost function, and the properties of Cholesky factorization.\n\n1.  **Derivation of the MAP Cost Function**\n\nThe problem is framed within a Bayesian context. We are given a prior belief about the state vector $x \\in \\mathbb{R}^{n}$ and a model connecting the state to observations $y \\in \\mathbb{R}^{m}$.\n\nThe prior distribution of $x$ is given as a multivariate Gaussian with mean $x_b$ and covariance $B$. The probability density function (PDF) is:\n$$p(x) \\propto \\exp\\left( -\\frac{1}{2} (x - x_b)^\\top B^{-1} (x - x_b) \\right)$$\nwhere $B \\in \\mathbb{R}^{n \\times n}$ is the symmetric positive definite prior covariance matrix.\n\nThe observations $y$ are related to the state $x$ via the linear model $y = Hx + \\epsilon$, where $H \\in \\mathbb{R}^{m \\times n}$ is the observation operator and $\\epsilon \\in \\mathbb{R}^{m}$ is the observational error. The error is assumed to be a zero-mean Gaussian random variable, $\\epsilon \\sim \\mathcal{N}(0, R)$, with a symmetric positive definite covariance matrix $R \\in \\mathbb{R}^{m \\times m}$. The likelihood, or the conditional PDF of $y$ given $x$, is therefore:\n$$p(y|x) \\propto \\exp\\left( -\\frac{1}{2} (Hx - y)^\\top R^{-1} (Hx - y) \\right)$$\n\nAccording to Bayes' theorem, the posterior PDF of the state $x$ given the observations $y$ is proportional to the product of the likelihood and the prior: $p(x|y) \\propto p(y|x) p(x)$.\nSubstituting the Gaussian forms gives:\n$$p(x|y) \\propto \\exp\\left( -\\frac{1}{2} (Hx - y)^\\top R^{-1} (Hx - y) \\right) \\exp\\left( -\\frac{1}{2} (x - x_b)^\\top B^{-1} (x - x_b) \\right)$$\n$$p(x|y) \\propto \\exp\\left( -\\frac{1}{2} \\left[ (x - x_b)^\\top B^{-1} (x - x_b) + (Hx - y)^\\top R^{-1} (Hx - y) \\right] \\right)$$\n\nThe MAP estimate of $x$ is the value that maximizes the posterior probability $p(x|y)$. Maximizing $p(x|y)$ is equivalent to minimizing its negative logarithm. This leads to the quadratic cost function $J(x)$:\n$$J(x) = \\frac{1}{2} (x - x_b)^\\top B^{-1} (x - x_b) + \\frac{1}{2} (Hx - y)^\\top R^{-1} (Hx - y)$$\nThe MAP estimate $x_a$ is the unique minimizer of $J(x)$, i.e., $x_a = \\arg\\min_x J(x)$.\n\n2.  **Cost Function using Cholesky Factors**\n\nThe problem provides the covariance matrices $B$ and $R$ via their lower-triangular Cholesky factors, $S_B$ and $L_R$, such that $B = S_B S_B^\\top$ and $R = L_R L_R^\\top$. Since $S_B$ and $L_R$ are stated to have strictly positive diagonals, they are invertible, which guarantees $B$ and $R$ are positive definite. The inverse of a covariance matrix can be expressed using its Cholesky factor's inverse: $B^{-1} = (S_B S_B^\\top)^{-1} = (S_B^\\top)^{-1} S_B^{-1}$.\n\nA quadratic form $z^\\top C^{-1} z$ where $C=LL^\\top$ can be written as a squared Euclidean norm:\n$$z^\\top C^{-1} z = z^\\top (L^\\top)^{-1} L^{-1} z = (L^{-1} z)^\\top (L^{-1} z) = \\lVert L^{-1} z \\rVert_2^2$$\nApplying this to the two terms in $J(x)$, we get:\n$$J(x) = \\frac{1}{2} \\lVert S_B^{-1} (x - x_b) \\rVert_2^2 + \\frac{1}{2} \\lVert L_R^{-1} (Hx - y) \\rVert_2^2$$\nThis is the \"unwhitened\" cost function specified in the problem statement for a given case $k$ with covariance factor $L_R^{(k)}$. This form is superior for numerical computation as it avoids the explicit formation of ill-conditioned inverse matrices, relying instead on solving triangular systems.\n\n3.  **The Whitening Transformation**\n\nWhitening is a change of variables that transforms a correlated error distribution into one with uncorrelated, unit-variance components (i.e., its covariance is the identity matrix). Let the original error vector be $\\epsilon \\sim \\mathcal{N}(0, R)$. We seek a linear transformation $W$ such that the whitened error $\\widetilde{\\epsilon} = W\\epsilon$ has covariance $I$. The covariance of $\\widetilde{\\epsilon}$ is:\n$$\\mathrm{Cov}(\\widetilde{\\epsilon}) = E[\\widetilde{\\epsilon} \\widetilde{\\epsilon}^\\top] = E[(W\\epsilon)(W\\epsilon)^\\top] = W E[\\epsilon \\epsilon^\\top] W^\\top = W R W^\\top$$\nTo achieve $\\mathrm{Cov}(\\widetilde{\\epsilon}) = I$, we must have $W R W^\\top = I$. Using the Cholesky factorization $R = L_R L_R^\\top$, we can choose the whitening operator $W = L_R^{-1}$. This gives:\n$$W R W^\\top = L_R^{-1} (L_R L_R^\\top) (L_R^{-1})^\\top = (L_R^{-1} L_R) (L_R^\\top (L_R^\\top)^{-1}) = I \\cdot I = I$$\nThe problem specifies defining $W$ by solving $L_R W = I$, which is precisely the definition of $W = L_R^{-1}$.\n\nWe apply this transformation to the entire observation equation:\n$$y = Hx + \\epsilon \\implies W y = W(Hx) + W\\epsilon$$\nDefining the whitened observations $\\widetilde{d} = Wy$ and the whitened observation operator $\\widetilde{H} = WH$, the model becomes:\n$$\\widetilde{d} = \\widetilde{H}x + \\widetilde{\\epsilon}, \\quad \\text{where } \\widetilde{\\epsilon} \\sim \\mathcal{N}(0, I)$$\n\n4.  **The Whitened Cost Function**\n\nThe MAP cost function can be formulated in the whitened space. The data-misfit term corresponds to the negative log-likelihood of the whitened model. With $\\widetilde{\\epsilon}$ having an identity covariance matrix, this term simplifies to a simple sum of squares:\n$$\\frac{1}{2} (\\widetilde{H}x - \\widetilde{d})^\\top I^{-1} (\\widetilde{H}x - \\widetilde{d}) = \\frac{1}{2} \\lVert \\widetilde{H}x - \\widetilde{d} \\rVert_2^2$$\nCombining this with the unchanged prior term, we obtain the \"whitened\" cost function $\\widetilde{J}(x)$:\n$$\\widetilde{J}(x) = \\frac{1}{2} \\lVert S_B^{-1} (x - x_b) \\rVert_2^2 + \\frac{1}{2} \\lVert \\widetilde{H}x - \\widetilde{d} \\rVert_2^2$$\nBy substituting the definitions $\\widetilde{H} = L_R^{-1}H$ and $\\widetilde{d} = L_R^{-1}y$, we can see that this is identical to the unwhitened cost function:\n$$\\widetilde{J}(x) = \\frac{1}{2} \\lVert S_B^{-1} (x - x_b) \\rVert_2^2 + \\frac{1}{2} \\lVert (L_R^{-1}H)x - L_R^{-1}y \\rVert_2^2 = \\frac{1}{2} \\lVert S_B^{-1} (x - x_b) \\rVert_2^2 + \\frac{1}{2} \\lVert L_R^{-1}(Hx - y) \\rVert_2^2 = J(x)$$\nThe two cost functions are analytically identical. Consequently, their minima must be the same point, and their values at any given point $x$ must be equal.\n\n5.  **Solution via Linear Least Squares**\n\nMinimizing $J(x)$ (or $\\widetilde{J}(x)$) is a linear least-squares problem. We seek to find $x$ that minimizes the sum of two squared norms. This can be expressed as minimizing the norm of a single, larger residual vector:\n$$\\min_{x} \\left\\lVert \\begin{pmatrix} S_B^{-1} (x - x_b) \\\\ L_R^{-1} (Hx - y) \\end{pmatrix} \\right\\rVert_2^2 = \\min_{x} \\left\\lVert \\begin{pmatrix} S_B^{-1}x - S_B^{-1}x_b \\\\ (L_R^{-1}H)x - L_R^{-1}y \\end{pmatrix} \\right\\rVert_2^2$$\nThis is a standard least-squares problem of the form $\\min_x \\lVert Gx - f \\rVert_2^2$, with the augmented matrix $G$ and vector $f$ defined as:\n$$G = \\begin{pmatrix} S_B^{-1} \\\\ L_R^{-1}H \\end{pmatrix} \\in \\mathbb{R}^{(n+m) \\times n}, \\quad f = \\begin{pmatrix} S_B^{-1}x_b \\\\ L_R^{-1}y \\end{pmatrix} \\in \\mathbb{R}^{(n+m)}$$\nThe solution $x_a$ is found by solving the normal equations $(G^\\top G) x_a = G^\\top f$, though numerically superior methods like QR factorization are preferred.\n\n6.  **Numerical Implementation Strategy**\n\nWe will implement two solvers as prescribed, corresponding to the \"unwhitened\" and \"whitened\" viewpoints, to verify their numerical equivalence.\n\nFor both solvers, we first compute the matrix inverses implicitly. The matrix $S_B^{-1}$ is found by solving the triangular system $S_B X = I_n$. Similarly, for each test case $k$, the whitening operator $W^{(k)} = (L_R^{(k)})^{-1}$ is found by solving $L_R^{(k)} X = I_m$. These operations are performed using `scipy.linalg.solve_triangular`, which is numerically stable.\n\n-   **Unwhitened Solver**: We construct the augmented system directly using the inverse operators. The matrix $G_{unw}$ and vector $f_{unw}$ are formed as:\n    $$G_{unw} = \\begin{pmatrix} S_B^{-1} \\\\ W^{(k)}H \\end{pmatrix}, \\quad f_{unw} = \\begin{pmatrix} S_B^{-1}x_b \\\\ W^{(k)}y \\end{pmatrix}$$\n    The solution $x_{unw}^{(k)}$ is obtained by solving the least-squares problem $\\min_x \\lVert G_{unw}x - f_{unw} \\rVert_2^2$.\n\n-   **Whitened Solver**: We first explicitly compute the whitened quantities $\\widetilde{H}^{(k)} = W^{(k)}H$ and $\\widetilde{d}^{(k)} = W^{(k)}y$. Then, we assemble the augmented system:\n    $$G_w = \\begin{pmatrix} S_B^{-1} \\\\ \\widetilde{H}^{(k)} \\end{pmatrix}, \\quad f_w = \\begin{pmatrix} S_B^{-1}x_b \\\\ \\widetilde{d}^{(k)} \\end{pmatrix}$$\n    The solution $x_{w}^{(k)}$ is obtained by solving $\\min_x \\lVert G_w x - f_w \\rVert_2^2$.\n\nNumerically, these two procedures are identical and should produce solutions $x_{unw}^{(k)}$ and $x_{w}^{(k)}$ that are equal up to machine precision.\n\n7.  **Diagnostic Metrics**\n\nThe three diagnostics serve to validate the theoretical equivalences and numerical accuracy.\n\n1.  $\\lVert x_{\\mathrm{unw}}^{(k)} - x_{\\mathrm{w}}^{(k)} \\rVert_{2}$: This measures the Euclidean distance between the solutions from the two solvers. Theory predicts this should be zero. In practice, it will be a small number on the order of machine precision, confirming that both implementation paths lead to the same result.\n\n2.  $\\left| J(x_{\\mathrm{w}}^{(k)}) - \\widetilde{J}(x_{\\mathrm{w}}^{(k)}) \\right|$: This measures the absolute difference between the values of the unwhitened and whitened cost functions, evaluated at the computed solution $x_{\\mathrm{w}}^{(k)}$. As shown analytically, $J(x) = \\widetilde{J}(x)$ for any $x$. This diagnostic verifies their numerical equality, which should also hold to machine precision. The cost functions are implemented using stable triangular solves on the Cholesky factors, as derived earlier.\n\n3.  $\\max| W^{(k)} R^{(k)} (W^{(k)})^\\top - I_m |$: This quantifies the numerical accuracy of the whitening transformation itself. We compute $W^{(k)} = (L_R^{(k)})^{-1}$ and $R^{(k)} = L_R^{(k)} (L_R^{(k)})^\\top$, then form the product $W^{(k)} R^{(k)} (W^{(k)})^\\top$. The maximum absolute element of the difference between this product and the identity matrix $I_m$ should be a small number, reflecting the precision of the floating-point arithmetic and the stability of the triangular solve used to find $W^{(k)}$.\n\nBy computing these quantities for each test case, we validate the entire framework from theory to implementation.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import solve_triangular\n\ndef solve():\n    \"\"\"\n    Solves a linear Bayesian inverse problem using unwhitened and whitened\n    formulations, and computes diagnostics to verify their equivalence.\n    \"\"\"\n    # Common ingredients for all test cases\n    H = np.array([\n        [1.0, 0.0, 0.5],\n        [0.0, 1.0, -0.2],\n        [0.5, -0.3, 1.0],\n        [1.0, 1.0, 0.0],\n        [0.0, 0.5, 0.5]\n    ])\n    xb = np.array([0.2, -0.1, 0.3])\n    y = np.array([1.2, -0.7, 0.9, 0.5, -0.2])\n    SB = np.array([\n        [1.0, 0.0, 0.0],\n        [0.3, 1.1, 0.0],\n        [-0.2, 0.1, 0.9]\n    ])\n\n    n = H.shape[1]\n    m = H.shape[0]\n\n    # Test cases for observational error covariance Cholesky factor LR\n    test_cases = [\n        # Case 1: heteroscedastic diagonal\n        np.diag([0.5, 1.0, 0.8, 1.2, 0.6]),\n        \n        # Case 2: heteroscedastic correlated\n        np.array([\n            [0.9, 0.0, 0.0, 0.0, 0.0],\n            [0.2, 0.7, 0.0, 0.0, 0.0],\n            [0.1, -0.15, 0.8, 0.0, 0.0],\n            [0.05, 0.1, -0.05, 1.1, 0.0],\n            [-0.1, 0.05, 0.2, -0.1, 0.6]\n        ]),\n        \n        # Case 3: ill-conditioned diagonal, yet positive definite\n        np.diag([0.1, 0.9, 1.2, 0.7, 0.5]),\n        \n        # Case 4: heteroscedastic correlated, different scales\n        np.array([\n            [0.4, 0.0, 0.0, 0.0, 0.0],\n            [0.1, 0.5, 0.0, 0.0, 0.0],\n            [0.0, 0.2, 0.6, 0.0, 0.0],\n            [0.0, -0.1, 0.1, 0.9, 0.0],\n            [0.2, 0.0, -0.05, 0.1, 0.7]\n        ])\n    ]\n\n    results = []\n    \n    # Pre-compute SB^-1 as it's common to all cases and solvers\n    I_n = np.eye(n)\n    SB_inv = solve_triangular(SB, I_n, lower=True)\n\n    for LRk in test_cases:\n        I_m = np.eye(m)\n        \n        # Pre-compute Wk = LRk^-1, also common to both solvers within a case\n        Wk = solve_triangular(LRk, I_m, lower=True)\n\n        # 1. Unwhitened Solver\n        # Cost J(x) = 0.5 * ||SB_inv(x-xb)||^2 + 0.5 * ||LRk_inv(Hx-y)||^2\n        # is minimized by the least-squares problem ||G_unw * x - f_unw||^2\n        G_unw_top = SB_inv\n        G_unw_bot = Wk @ H\n        G_unw = np.vstack((G_unw_top, G_unw_bot))\n        \n        f_unw_top = SB_inv @ xb\n        f_unw_bot = Wk @ y\n        f_unw = np.concatenate((f_unw_top, f_unw_bot))\n        \n        x_unw, _, _, _ = np.linalg.lstsq(G_unw, f_unw, rcond=None)\n\n        # 2. Whitened Solver\n        # Cost J_tilde(x) = 0.5 * ||SB_inv(x-xb)||^2 + 0.5 * ||H_tilde*x - d_tilde||^2\n        # where H_tilde = Wk*H and d_tilde=Wk*y.\n        # This is minimized by the least-squares problem ||G_w * x - f_w||^2\n        H_tilde = Wk @ H\n        d_tilde = Wk @ y\n\n        G_w_top = SB_inv\n        G_w_bot = H_tilde\n        G_w = np.vstack((G_w_top, G_w_bot))\n\n        f_w_top = SB_inv @ xb\n        f_w_bot = d_tilde\n        f_w = np.concatenate((f_w_top, f_w_bot))\n\n        x_w, _, _, _ = np.linalg.lstsq(G_w, f_w, rcond=None)\n\n        # --- DIAGNOSTICS ---\n        \n        # Diagnostic 1: Norm of difference between solutions\n        diag1 = np.linalg.norm(x_unw - x_w)\n\n        # Diagnostic 2: Absolute difference between cost function values at x_w\n        # Unwhitened cost J(x_w)\n        res_b = x_w - xb\n        norm_res_b = solve_triangular(SB, res_b, lower=True)\n        cost_b_unw = 0.5 * np.dot(norm_res_b, norm_res_b)\n        \n        res_o = H @ x_w - y\n        norm_res_o = solve_triangular(LRk, res_o, lower=True)\n        cost_o_unw = 0.5 * np.dot(norm_res_o, norm_res_o)\n        J_at_xw = cost_b_unw + cost_o_unw\n\n        # Whitened cost J_tilde(x_w)\n        # Background term is the same\n        cost_b_w = cost_b_unw\n        \n        res_o_tilde = H_tilde @ x_w - d_tilde\n        cost_o_w = 0.5 * np.dot(res_o_tilde, res_o_tilde)\n        J_tilde_at_xw = cost_b_w + cost_o_w\n        \n        diag2 = np.abs(J_at_xw - J_tilde_at_xw)\n\n        # Diagnostic 3: Accuracy of whitening\n        Rk = LRk @ LRk.T\n        whitening_check_matrix = Wk @ Rk @ Wk.T\n        diag3 = np.max(np.abs(whitening_check_matrix - I_m))\n        \n        results.extend([diag1, diag2, diag3])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n```", "id": "3388472"}, {"introduction": "In practical applications, observation error estimates can sometimes be unrealistically small, causing the assimilation system to place excessive confidence in potentially flawed data. This practice introduces \"clipping,\" a common regularization technique where a minimum floor is enforced on observation error variances to prevent any single measurement from dominating the analysis. By systematically varying the clipping threshold and analyzing the resulting changes in posterior bias and credible intervals, you will gain direct insight into the trade-off between data fidelity and robust uncertainty quantification. This exercise demonstrates how thoughtful heteroscedastic weighting is a critical tool for data quality control in real-world assimilation systems. [@problem_id:3388489]", "problem": "Consider a linear-Gaussian inverse problem in which the unknown state vector $x \\in \\mathbb{R}^3$ is linked to observations $y \\in \\mathbb{R}^5$ via a known observation operator $H \\in \\mathbb{R}^{5 \\times 3}$ and additive noise. Assume a Gaussian prior $x \\sim \\mathcal{N}(x_b, B)$ with prior mean $x_b \\in \\mathbb{R}^3$ and symmetric positive definite prior covariance $B \\in \\mathbb{R}^{3 \\times 3}$. Observations satisfy $y = H x + \\varepsilon$, where the observation noise $\\varepsilon \\sim \\mathcal{N}(0, R)$ has a heteroscedastic covariance matrix $R \\in \\mathbb{R}^{5 \\times 5}$ that is diagonal with entries $\\sigma_i^2$ that vary widely across $i$. To avoid over-dominant measurements with unrealistically small $\\sigma_i^2$, we enforce clipping by replacing $R$ with $R_{\\mathrm{clip}}$ defined by diagonal entries $\\max(\\sigma_i^2, \\sigma_{\\min}^2)$, where $\\sigma_{\\min}^2$ is a chosen minimum variance threshold.\n\nDefine the whitening transform $W$ for a diagonal observation covariance $R_{\\mathrm{clip}}$ as $W = R_{\\mathrm{clip}}^{-1/2}$, resulting in whitened data $y_{\\mathrm{w}} = W y$ and a whitened observation operator $H_{\\mathrm{w}} = W H$ that yields a model with unit observation covariance.\n\nYour tasks are to:\n- Derive, from basic Bayesian linear-Gaussian principles and without using any pre-supplied formulas, the Maximum A Posteriori (MAP) posterior mean $x_a$ and the posterior covariance $A$ both for the unclipped heteroscedastic case and for the clipped case.\n- Implement whitening and verify that assimilating with the whitened model (identity observation covariance) produces the same posterior as directly assimilating with the clipped covariance (to numerical tolerance).\n- Quantify the impact of clipping on posterior credible intervals and potential bias by computing, for each clipping level $\\sigma_{\\min}^2$: the Euclidean norm of the bias $\\lVert x_a - x^\\ast \\rVert_2$, where $x^\\ast$ is the fixed reference truth; the $95\\%$ marginal credible interval widths for each component of $x$; and the coverage count, defined as the number of components of $x^\\ast$ that lie within their respective $95\\%$ credible intervals.\n\nUse the following fixed, scientifically plausible test configuration for all computations:\n- State dimension $n = 3$ and observation dimension $m = 5$.\n- True state $x^\\ast = [2.0, -1.0, 0.5]^\\top$.\n- Prior mean $x_b = [-0.5, 0.5, 0.0]^\\top$ and prior covariance $B = \\mathrm{diag}(4.0, 2.25, 9.0)$.\n- Observation operator\n$$\nH = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n1 & 1 & 0 \\\\\n0 & 1 & 1\n\\end{bmatrix}.\n$$\n- Baseline heteroscedastic observation variances $\\sigma^2 = [0.01, 0.09, 4.0, 0.0025, 1.0]$ so that $R = \\mathrm{diag}(\\sigma^2)$.\n- Deterministic observation noise components $\\varepsilon_i$ scaled by $\\sqrt{\\sigma_i^2}$ via fixed multipliers $[1.0, -0.5, 0.3, 2.0, -0.7]$, i.e., $\\varepsilon = [0.1, -0.15, 0.6, 0.1, -0.7]^\\top$, and observations $y = H x^\\ast + \\varepsilon$.\n\nCompute the posterior quantities for the test suite of clipping thresholds\n$$\n\\sigma_{\\min}^2 \\in \\{0.0, 0.04, 0.25, 4.0, 100.0\\}.\n$$\n\nFor each $\\sigma_{\\min}^2$ in the test suite, report a result list containing:\n- The Euclidean norm of the posterior bias $\\lVert x_a - x^\\ast \\rVert_2$ as a float.\n- The $95\\%$ marginal credible interval width for each component of $x$ as three floats, computed from the posterior covariance $A$ as $2 \\cdot 1.96 \\cdot \\sqrt{A_{ii}}$ for $i = 1,2,3$.\n- The coverage count as an integer, equal to the number of components $i$ for which $x^\\ast_i$ lies within $x_{a,i} \\pm 1.96 \\sqrt{A_{ii}}$.\n- A boolean flag indicating whether the posterior from the whitened assimilation exactly matches (to numerical tolerance) the posterior from the direct clipped assimilation, defined by the maximum absolute differences in means and covariances being less than $10^{-10}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of the form\n$[$bias\\_norm, width\\_1, width\\_2, width\\_3, coverage\\_count, equivalence\\_flag$]$\nfor each $\\sigma_{\\min}^2$ in the specified order. For example, a valid output shape is\n$[[b_1,w_{11},w_{12},w_{13},c_1,e_1],[b_2,w_{21},w_{22},w_{23},c_2,e_2],\\dots]$.", "solution": "We begin from the linear-Gaussian inverse problem with prior $x \\sim \\mathcal{N}(x_b, B)$ and likelihood $y \\mid x \\sim \\mathcal{N}(H x, R)$, where $B \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite and $R \\in \\mathbb{R}^{m \\times m}$ is symmetric positive definite. Bayes' rule for Gaussian distributions implies that the posterior $p(x \\mid y)$ is also Gaussian. The Maximum A Posteriori (MAP) estimate coincides with the posterior mean because the posterior is Gaussian.\n\nThe fundamental base for the derivation is the joint Gaussian model and the fact that the negative log posterior is a quadratic form. Specifically, the negative log posterior (up to additive constants independent of $x$) is the sum of the prior and observation terms:\n$$\nJ(x) = \\tfrac{1}{2} (x - x_b)^\\top B^{-1} (x - x_b) + \\tfrac{1}{2} (y - H x)^\\top R^{-1} (y - H x).\n$$\nMinimizing $J(x)$ with respect to $x$ yields the first-order optimality condition given by setting the gradient to zero:\n$$\n\\nabla J(x) = B^{-1} (x - x_b) - H^\\top R^{-1} (y - H x) = 0.\n$$\nRearranging terms produces the linear system\n$$\n\\left(B^{-1} + H^\\top R^{-1} H\\right) x = B^{-1} x_b + H^\\top R^{-1} y.\n$$\nDefine the symmetric positive definite matrix\n$$\nS = B^{-1} + H^\\top R^{-1} H.\n$$\nThen the posterior mean is\n$$\nx_a = S^{-1} \\left( B^{-1} x_b + H^\\top R^{-1} y \\right),\n$$\nand the posterior covariance is\n$$\nA = S^{-1}.\n$$\nThese expressions follow directly from completing the square in the exponent of the Gaussian posterior or from minimizing the quadratic objective $J(x)$.\n\nNow consider the heteroscedastic case in which $R$ is diagonal with entries $\\sigma_i^2$ that vary across $i$. To avoid over-dominant measurements, we enforce clipping by defining $R_{\\mathrm{clip}}$ with diagonal entries $\\max(\\sigma_i^2, \\sigma_{\\min}^2)$. Replacing $R$ by $R_{\\mathrm{clip}}$ in the above derivation yields the clipped posterior mean and covariance:\n$$\nS_{\\mathrm{clip}} = B^{-1} + H^\\top R_{\\mathrm{clip}}^{-1} H, \\quad\nx_{a,\\mathrm{clip}} = S_{\\mathrm{clip}}^{-1} \\left( B^{-1} x_b + H^\\top R_{\\mathrm{clip}}^{-1} y \\right), \\quad\nA_{\\mathrm{clip}} = S_{\\mathrm{clip}}^{-1}.\n$$\nWhitening of observations is performed by the transform $W = R_{\\mathrm{clip}}^{-1/2}$, which is well-defined because $R_{\\mathrm{clip}}$ is diagonal with positive entries. Define the whitened quantities\n$$\ny_{\\mathrm{w}} = W y, \\quad H_{\\mathrm{w}} = W H.\n$$\nThe whitened likelihood has identity covariance, i.e., $y_{\\mathrm{w}} \\mid x \\sim \\mathcal{N}(H_{\\mathrm{w}} x, I)$. The corresponding negative log posterior (up to additive constants) becomes\n$$\nJ_{\\mathrm{w}}(x) = \\tfrac{1}{2} (x - x_b)^\\top B^{-1} (x - x_b) + \\tfrac{1}{2} \\lVert y_{\\mathrm{w}} - H_{\\mathrm{w}} x \\rVert_2^2,\n$$\nwhose minimizer satisfies\n$$\n\\left(B^{-1} + H_{\\mathrm{w}}^\\top H_{\\mathrm{w}}\\right) x = B^{-1} x_b + H_{\\mathrm{w}}^\\top y_{\\mathrm{w}}.\n$$\nSubstituting $H_{\\mathrm{w}} = W H$ and $y_{\\mathrm{w}} = W y$ gives\n$$\nB^{-1} + H_{\\mathrm{w}}^\\top H_{\\mathrm{w}} = B^{-1} + H^\\top W^\\top W H = B^{-1} + H^\\top R_{\\mathrm{clip}}^{-1} H = S_{\\mathrm{clip}},\n$$\nand\n$$\nB^{-1} x_b + H_{\\mathrm{w}}^\\top y_{\\mathrm{w}} = B^{-1} x_b + H^\\top W^\\top W y = B^{-1} x_b + H^\\top R_{\\mathrm{clip}}^{-1} y.\n$$\nTherefore the whitened posterior is identical to the clipped posterior:\n$$\nx_{a,\\mathrm{w}} = x_{a,\\mathrm{clip}}, \\quad A_{\\mathrm{w}} = A_{\\mathrm{clip}}.\n$$\n\nTo assess credible intervals and potential bias, note that the posterior marginal for each component $x_i$ is Gaussian with mean $x_{a,i}$ and variance $A_{ii}$. The $95\\%$ marginal credible interval is\n$$\n\\left[x_{a,i} - 1.96 \\sqrt{A_{ii}}, \\, x_{a,i} + 1.96 \\sqrt{A_{ii}}\\right],\n$$\nand its width is\n$$\n2 \\cdot 1.96 \\cdot \\sqrt{A_{ii}}.\n$$\nDefine the coverage indicator for component $i$ to be $1$ if $x^\\ast_i$ lies in its credible interval and $0$ otherwise, and the coverage count to be the sum over $i = 1,2,3$. The posterior bias norm is $\\lVert x_a - x^\\ast \\rVert_2$.\n\nAlgorithmic steps to implement for each clipping threshold $\\sigma_{\\min}^2$ in the test suite:\n1. Construct $R_{\\mathrm{clip}} = \\mathrm{diag}(\\max(\\sigma_i^2, \\sigma_{\\min}^2))$.\n2. Compute $S_{\\mathrm{clip}} = B^{-1} + H^\\top R_{\\mathrm{clip}}^{-1} H$, then $A_{\\mathrm{clip}} = S_{\\mathrm{clip}}^{-1}$.\n3. Compute $x_{a,\\mathrm{clip}} = A_{\\mathrm{clip}} \\left( B^{-1} x_b + H^\\top R_{\\mathrm{clip}}^{-1} y \\right)$.\n4. Compute the $95\\%$ credible interval widths $2 \\cdot 1.96 \\cdot \\sqrt{A_{ii}}$ for $i=1,2,3$.\n5. Compute the coverage count based on whether $x^\\ast_i$ lies within its credible interval for each $i$.\n6. Compute the bias norm $\\lVert x_{a,\\mathrm{clip}} - x^\\ast \\rVert_2$.\n7. Compute the whitened posterior by forming $W = R_{\\mathrm{clip}}^{-1/2}$, $H_{\\mathrm{w}} = W H$, $y_{\\mathrm{w}} = W y$, then $S_{\\mathrm{w}} = B^{-1} + H_{\\mathrm{w}}^\\top H_{\\mathrm{w}}$, $A_{\\mathrm{w}} = S_{\\mathrm{w}}^{-1}$, and $x_{a,\\mathrm{w}} = A_{\\mathrm{w}} \\left( B^{-1} x_b + H_{\\mathrm{w}}^\\top y_{\\mathrm{w}} \\right)$. Verify the equivalence by checking that the maximum absolute differences in means and covariances are less than $10^{-10}$.\n\nApplying the above to the specified deterministic test configuration yields, for each $\\sigma_{\\min}^2$, a result list of the form $[$bias\\_norm, width\\_1, width\\_2, width\\_3, coverage\\_count, equivalence\\_flag$]$. Aggregating these lists over the ordered test suite produces the required single-line program output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef posterior_with_clipping(H, B, x_b, R_base_diag, y, sigma_min_sq, x_true):\n    \"\"\"\n    Compute posterior mean and covariance under clipping, credible widths,\n    coverage count, bias norm, and check equivalence with whitening.\n    \"\"\"\n    # Clip the observation variances.\n    R_diag = np.maximum(R_base_diag, sigma_min_sq)\n    R = np.diag(R_diag)\n\n    # Precompute inverses of diagonal matrices efficiently.\n    B_inv = np.diag(1.0 / np.diag(B))\n    R_inv = np.diag(1.0 / R_diag)\n\n    # Assemble S = B^{-1} + H^T R^{-1} H.\n    S = B_inv + H.T @ R_inv @ H\n\n    # Compute A = S^{-1}.\n    A = np.linalg.inv(S)\n\n    # Compute posterior mean: x_a = A * (B^{-1} x_b + H^T R^{-1} y).\n    rhs = B_inv @ x_b + H.T @ (R_inv @ y)\n    x_a = A @ rhs\n\n    # Credible widths: 2 * 1.96 * sqrt(diag(A)).\n    stds = np.sqrt(np.clip(np.diag(A), 0.0, np.inf))\n    widths = 2.0 * 1.96 * stds\n\n    # Coverage count: number of components where x_true_i in [mean - 1.96*std, mean + 1.96*std].\n    lower = x_a - 1.96 * stds\n    upper = x_a + 1.96 * stds\n    coverage_flags = (x_true >= lower)  (x_true = upper)\n    coverage_count = int(np.sum(coverage_flags))\n\n    # Bias norm.\n    bias_norm = float(np.linalg.norm(x_a - x_true))\n\n    # Whitening equivalence check.\n    # W = R^{-1/2}; for diagonal R, W is diag(1/sigma_i).\n    sigma = np.sqrt(R_diag)\n    W = np.diag(1.0 / sigma)\n    H_w = W @ H\n    y_w = W @ y\n\n    S_w = B_inv + H_w.T @ H_w\n    A_w = np.linalg.inv(S_w)\n    rhs_w = B_inv @ x_b + H_w.T @ y_w\n    x_a_w = A_w @ rhs_w\n\n    # Check equivalence within tolerance.\n    tol = 1e-10\n    cov_diff = np.max(np.abs(A - A_w))\n    mean_diff = np.max(np.abs(x_a - x_a_w))\n    equivalence_flag = (cov_diff  tol) and (mean_diff  tol)\n\n    return [bias_norm, widths[0], widths[1], widths[2], coverage_count, equivalence_flag]\n\ndef solve():\n    # Define the test configuration as specified in the problem statement.\n    n = 3\n    m = 5\n\n    x_true = np.array([2.0, -1.0, 0.5])\n    x_b = np.array([-0.5, 0.5, 0.0])\n    B = np.diag([4.0, 2.25, 9.0])\n\n    H = np.array([\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0],\n        [0.0, 0.0, 1.0],\n        [1.0, 1.0, 0.0],\n        [0.0, 1.0, 1.0]\n    ])\n\n    # Baseline heteroscedastic variances and deterministic noise multipliers.\n    R_base_diag = np.array([0.01, 0.09, 4.0, 0.0025, 1.0])\n    # Multipliers applied to sqrt(variance) to form epsilon.\n    multipliers = np.array([1.0, -0.5, 0.3, 2.0, -0.7])\n    epsilon = multipliers * np.sqrt(R_base_diag)\n\n    # Observations y = H x_true + epsilon.\n    y = H @ x_true + epsilon\n\n    # Test suite of clipping thresholds sigma_min^2.\n    test_cases = [0.0, 0.04, 0.25, 4.0, 100.0]\n\n    results = []\n    for sigma_min_sq in test_cases:\n        result = posterior_with_clipping(H, B, x_b, R_base_diag, y, sigma_min_sq, x_true)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3388489"}, {"introduction": "Beyond calculating the overall uncertainty of an estimate, it is often crucial to understand how uncertainty is distributed among specific parameters and how their estimates are related. This advanced practice introduces the Schur complement as a powerful analytical tool to dissect the posterior covariance matrix and study the marginal uncertainty of a parameter subset. You will investigate how modifying the observation error covariance $R$—effectively changing the quality of information from different sensors—alters the posterior correlation between coupled parameters. This exercise provides a deeper understanding of parameter identifiability and the precise flow of information from observations to different components of the state vector. [@problem_id:3388480]", "problem": "Consider a linear inverse problem with Gaussian prior and heteroscedastic observation errors. A state vector $x \\in \\mathbb{R}^n$ is observed through a linear map, and the observations have mutually uncorrelated but unequal variances. Whitening of observations and proper weighting must be handled to quantify how downweighting certain observation components (via large variances) affects identifiability and posterior correlations of coupled parameters. Your task is to derive, implement, and evaluate a Schur complement based analysis to quantify changes in the posterior correlation between selected parameters under specified perturbations of the observation error covariance.\n\nStarting from the fundamental definitions of a linear Gaussian Bayesian inverse problem and the construction of the quadratic cost from the prior and the likelihood, do the following tasks without assuming any target formulas in advance:\n\n- Derive the posterior information matrix in terms of the prior precision, the observation operator, and the observation error covariance, using only the definitions of the Gaussian prior and the Gaussian likelihood. Justify why whitening with the inverse square-root of the observation error covariance does not change the posterior but changes the normal equations, and express the posterior information in whitened coordinates.\n- Partition the posterior information matrix into two blocks corresponding to a target subset of parameters and its complement. Using the Schur complement of a symmetric positive definite block matrix, derive an explicit expression for the marginal posterior covariance of the target subset. Explain why this marginal posterior covariance is invariant to whether one computes in whitened or unwhitened coordinates.\n- Define the posterior correlation coefficient between two target parameters in terms of the entries of their marginal posterior covariance, and state how this correlation changes when the observation error covariance is perturbed.\n\nThen, apply your derivations to the following concrete instance. The state dimension is $n = 3$, and the observation dimension is $m = 3$. The prior mean is arbitrary and irrelevant for covariance calculations. The prior covariance matrix $B \\in \\mathbb{R}^{3 \\times 3}$ and the observation operator $H \\in \\mathbb{R}^{3 \\times 3}$ are fixed as:\n- $B = \\begin{bmatrix} 1.0  0.6  0.2 \\\\ 0.6  1.5  0.4 \\\\ 0.2  0.4  1.2 \\end{bmatrix}$,\n- $H = \\begin{bmatrix} 1.0  1.0  0.0 \\\\ 0.0  1.0  1.0 \\\\ 1.0  0.0  1.0 \\end{bmatrix}$.\n\nThe observation error covariance $R \\in \\mathbb{R}^{3 \\times 3}$ is diagonal and heteroscedastic, i.e., $R = \\operatorname{diag}(\\sigma_1^2,\\sigma_2^2,\\sigma_3^2)$. Denote by $W = R^{-1/2}$ the whitening matrix. The target subset of parameters is the first two components of $x$, denoted $x_a = [x_1,x_2]^\\top$, with the complement $x_b = [x_3]$.\n\nDefine the posterior information matrix $J \\in \\mathbb{R}^{3 \\times 3}$, and partition it as\n$J = \\begin{bmatrix} J_{aa}  J_{ab} \\\\ J_{ba}  J_{bb} \\end{bmatrix}$ with $J_{aa} \\in \\mathbb{R}^{2 \\times 2}$, $J_{bb} \\in \\mathbb{R}^{1 \\times 1}$, and conformal off-diagonal blocks. Use the Schur complement of $J_{bb}$ in $J$ to obtain the marginal posterior covariance $S_{aa} \\in \\mathbb{R}^{2 \\times 2}$ of $x_a$, and compute the posterior correlation coefficient $\\rho_{12}$ between $x_1$ and $x_2$ as a function of $R$.\n\nImplement a program that computes, for each of the following test cases, the posterior correlation coefficient $\\rho_{12}$ before and after a specified perturbation of $R$, and the change $\\Delta \\rho_{12}$ defined as the difference between the perturbed and baseline correlations. All computations must be performed using the Schur complement based expression of $S_{aa}$ derived above.\n\nTest suite:\n- Case A (happy path): Baseline $R_0 = \\operatorname{diag}(\\sigma_1^2,\\sigma_2^2,\\sigma_3^2)$ with $(\\sigma_1,\\sigma_2,\\sigma_3) = (0.2, 0.5, 1.0)$. Perturbation: increase the third variance by $\\Delta = 4.0$, i.e., $R_{\\text{pert}} = \\operatorname{diag}(0.2^2, 0.5^2, 1.0^2 + 4.0)$.\n- Case B (boundary downweighting): Baseline $R_0$ as in Case A. Perturbation: set the third variance to a very large value to emulate near-complete downweighting, i.e., $R_{\\text{pert}} = \\operatorname{diag}(0.2^2, 0.5^2, 10^6)$.\n- Case C (edge informative component): Baseline $R_0 = \\operatorname{diag}(0.5^2, 0.5^2, 0.5^2)$. Perturbation: decrease the second variance to a highly informative level, i.e., $R_{\\text{pert}} = \\operatorname{diag}(0.5^2, 0.1^2, 0.5^2)$.\n\nFor each case, define $\\rho_{12}^{\\text{base}}$ from $R_0$, $\\rho_{12}^{\\text{pert}}$ from $R_{\\text{pert}}$, and $\\Delta \\rho_{12} = \\rho_{12}^{\\text{pert}} - \\rho_{12}^{\\text{base}}$. No physical units are involved. Angles are not involved.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain the nine floating-point numbers in this order:\n  - Case A: $\\rho_{12}^{\\text{base}}$, $\\rho_{12}^{\\text{pert}}$, $\\Delta \\rho_{12}$,\n  - Case B: $\\rho_{12}^{\\text{base}}$, $\\rho_{12}^{\\text{pert}}$, $\\Delta \\rho_{12}$,\n  - Case C: $\\rho_{12}^{\\text{base}}$, $\\rho_{12}^{\\text{pert}}$, $\\Delta \\rho_{12}$.\n- Round all floating-point outputs to six decimal places.\n\nYour implementation must be a complete, runnable program that performs the derivations computationally via linear algebra and Schur complements and prints the final list exactly as specified. No user input is required. The analysis must be universally applicable to any modern programming language; for this task, you must implement it in Python and ensure numerical stability in matrix inversions by using appropriate linear algebra routines.", "solution": "This solution follows from fundamental principles of Bayesian inference for linear-Gaussian models.\n\n### 1. Posterior Information Matrix and Whitening\n\nIn a linear Bayesian problem with a Gaussian prior $p(x) \\propto \\exp(-\\frac{1}{2}(x - x_b)^\\top B^{-1} (x - x_b))$ and a Gaussian likelihood $p(d|x) \\propto \\exp(-\\frac{1}{2}(d - Hx)^\\top R^{-1} (d - Hx))$, the posterior distribution is also Gaussian, derived from Bayes' theorem $p(x|d) \\propto p(d|x)p(x)$. The negative logarithm of the posterior is (up to constants) the quadratic cost function:\n$$ \\mathcal{J}(x) = \\frac{1}{2}(x - x_b)^\\top B^{-1} (x - x_b) + \\frac{1}{2}(d - Hx)^\\top R^{-1} (d - Hx) $$\nThe posterior information matrix, $J$, is the Hessian of this cost function with respect to $x$. Taking the second derivative yields:\n$$ J = \\nabla_x^2 \\mathcal{J}(x) = B^{-1} + H^\\top R^{-1} H $$\nThe posterior covariance is the inverse of the posterior information matrix, $P = J^{-1}$.\n\nWhitening with $W=R^{-1/2}$ transforms the observation term into $\\frac{1}{2}\\lVert Wd - WHx \\rVert_2^2 = \\frac{1}{2}(d-Hx)^\\top W^\\top W(d-Hx)$. Since $W=R^{-1/2}$ is symmetric for a diagonal $R$, $W^\\top W = W^2 = R^{-1}$. Thus, the cost function $\\mathcal{J}(x)$ is identical in both whitened and unwhitened coordinates. As $J$ is derived from $\\mathcal{J}(x)$, it is also invariant to whitening.\n\n### 2. Marginal Posterior Covariance via Schur Complement\n\nWe partition the state vector as $x = [x_a^\\top, x_b^\\top]^\\top$. The posterior information matrix $J$ is conformally partitioned:\n$$ J = \\begin{bmatrix} J_{aa}  J_{ab} \\\\ J_{ba}  J_{bb} \\end{bmatrix} $$\nThe posterior covariance $P=J^{-1}$ has the same partitioning. The marginal posterior covariance of the target subset $x_a$ is the block $P_{aa}$ of the inverse matrix $P$. Using the block matrix inversion formula, this is given by the inverse of the Schur complement of $J_{bb}$ in $J$:\n$$ P_{aa} = (J_{aa} - J_{ab} J_{bb}^{-1} J_{ba})^{-1} $$\nSince this expression depends only on the blocks of $J$, and $J$ itself is invariant to whitening, the marginal posterior covariance $P_{aa}$ is also invariant.\n\n### 3. Posterior Correlation Coefficient\n\nFor a 2D target subset $x_a = [x_1, x_2]^\\top$, the marginal posterior covariance is a $2 \\times 2$ matrix:\n$$ P_{aa} = \\begin{bmatrix} P_{11}  P_{12} \\\\ P_{21}  P_{22} \\end{bmatrix} $$\nThe posterior correlation coefficient $\\rho_{12}$ between $x_1$ and $x_2$ is the normalized covariance:\n$$ \\rho_{12} = \\frac{P_{12}}{\\sqrt{P_{11} P_{22}}} $$\nA perturbation in the observation error covariance $R$ changes $R^{-1}$, which in turn alters the posterior information matrix $J$. This change propagates through the Schur complement calculation to the marginal covariance $P_{aa}$, thus changing the posterior correlation $\\rho_{12}$.\n\n### Implementation Summary\n\nThe numerical implementation directly follows these derivations. For each test case (baseline and perturbed):\n1.  The observation error precision $R^{-1}$ is calculated.\n2.  The posterior information matrix $J = B^{-1} + H^\\top R^{-1} H$ is assembled.\n3.  $J$ is partitioned into $J_{aa}$, $J_{ab}$, $J_{ba}$, and $J_{bb}$.\n4.  The Schur complement $S = J_{aa} - J_{ab} J_{bb}^{-1} J_{ba}$ is computed.\n5.  The marginal covariance $P_{aa}$ (denoted $S_{aa}$ in the problem) is found by inverting $S$.\n6.  The correlation coefficient $\\rho_{12}$ is calculated from the elements of $P_{aa}$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian inverse problem to calculate posterior correlations.\n    \"\"\"\n\n    # --- Given Fixed Parameters ---\n    # Prior covariance matrix B\n    B = np.array([\n        [1.0, 0.6, 0.2],\n        [0.6, 1.5, 0.4],\n        [0.2, 0.4, 1.2]\n    ])\n\n    # Observation operator H\n    H = np.array([\n        [1.0, 1.0, 0.0],\n        [0.0, 1.0, 1.0],\n        [1.0, 0.0, 1.0]\n    ])\n\n    # Pre-compute the inverse of the prior covariance matrix (prior precision)\n    try:\n        B_inv = np.linalg.inv(B)\n    except np.linalg.LinAlgError:\n        # This case should not be reached with the given B\n        print(\"Error: Prior covariance matrix B is singular.\")\n        return\n\n    # --- Test Case Definitions ---\n    test_cases = [\n        # Case A: Happy path\n        {\n            \"name\": \"Case A\",\n            \"R0_variances\": np.array([0.2**2, 0.5**2, 1.0**2]),\n            \"Rpert_variances\": np.array([0.2**2, 0.5**2, 1.0**2 + 4.0])\n        },\n        # Case B: Boundary downweighting\n        {\n            \"name\": \"Case B\",\n            \"R0_variances\": np.array([0.2**2, 0.5**2, 1.0**2]),\n            \"Rpert_variances\": np.array([0.2**2, 0.5**2, 10.0**6])\n        },\n        # Case C: Edge informative component\n        {\n            \"name\": \"Case C\",\n            \"R0_variances\": np.array([0.5**2, 0.5**2, 0.5**2]),\n            \"Rpert_variances\": np.array([0.5**2, 0.1**2, 0.5**2])\n        }\n    ]\n\n    def compute_posterior_correlation(R_variances, B_inv_matrix, H_matrix):\n        \"\"\"\n        Computes the posterior correlation rho_12 for a given R.\n\n        Args:\n            R_variances (np.ndarray): Diagonal elements (variances) of R.\n            B_inv_matrix (np.ndarray): The inverse of the prior covariance matrix B.\n            H_matrix (np.ndarray): The observation operator H.\n\n        Returns:\n            float: The posterior correlation coefficient rho_12.\n        \"\"\"\n        # 1. Observation error precision R_inv\n        # R is diagonal, so its inverse is a diagonal matrix of reciprocal variances.\n        R_inv_diag = 1.0 / R_variances\n        R_inv = np.diag(R_inv_diag)\n\n        # 2. Posterior information matrix J\n        # J = B_inv + H^T * R_inv * H\n        J = B_inv_matrix + H_matrix.T @ R_inv @ H_matrix\n\n        # 3. Partition J\n        # The state is x = [x1, x2, x3]^T.\n        # Target subset xa = [x1, x2]^T. Complement xb = [x3].\n        # Partition is after the 2nd row/col.\n        J_aa = J[0:2, 0:2]  # 2x2\n        J_ab = J[0:2, 2:3]  # 2x1\n        J_ba = J[2:3, 0:2]  # 1x2\n        J_bb = J[2:3, 2:3]  # 1x1\n\n        # 4. Compute Schur complement of J_bb in J\n        # S = J_aa - J_ab * J_bb^-1 * J_ba\n        # J_bb is 1x1, so its inverse is just the reciprocal of its element.\n        J_bb_inv_scalar = 1.0 / J_bb[0, 0]\n        schur_complement = J_aa - (J_ab * J_bb_inv_scalar) @ J_ba\n\n        # 5. Compute marginal posterior covariance S_aa (denoted P_aa in derivation)\n        # S_aa is the inverse of the Schur complement.\n        try:\n            S_aa = np.linalg.inv(schur_complement)\n        except np.linalg.LinAlgError:\n            # Should not happen with well-posed problems\n            return np.nan\n\n        # 6. Compute posterior correlation coefficient rho_12\n        # rho_12 = S_aa(1,2) / sqrt(S_aa(1,1) * S_aa(2,2))\n        S_11 = S_aa[0, 0]\n        S_22 = S_aa[1, 1]\n        S_12 = S_aa[0, 1]\n        \n        if S_11 = 0 or S_22 = 0: # Variances must be non-negative\n             return np.nan\n             \n        rho_12 = S_12 / np.sqrt(S_11 * S_22)\n        return rho_12\n\n    results = []\n    for case in test_cases:\n        # Compute baseline correlation\n        rho_base = compute_posterior_correlation(case[\"R0_variances\"], B_inv, H)\n        \n        # Compute perturbed correlation\n        rho_pert = compute_posterior_correlation(case[\"Rpert_variances\"], B_inv, H)\n        \n        # Compute the change\n        delta_rho = rho_pert - rho_base\n        \n        # Append formatted results\n        results.extend([f\"{rho_base:.6f}\", f\"{rho_pert:.6f}\", f\"{delta_rho:.6f}\"])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3388480"}]}