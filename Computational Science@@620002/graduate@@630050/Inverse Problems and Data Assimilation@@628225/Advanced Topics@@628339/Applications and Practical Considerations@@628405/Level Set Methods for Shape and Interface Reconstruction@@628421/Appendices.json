{"hands_on_practices": [{"introduction": "A cornerstone of modern shape optimization is the ability to efficiently compute the gradient of an objective function that depends on the solution of a partial differential equation (PDE). This exercise guides you through the derivation and implementation of the powerful adjoint-state method, which allows us to compute this gradient at a cost independent of the number of optimization parameters. By working through the adjoint derivation for a variable-coefficient Poisson problem, you will gain hands-on experience with the core computational engine used in many large-scale inverse problems and verify your implementation with a gradient check, an essential skill for practical application [@problem_id:3396641].", "problem": "Consider a two-dimensional square domain $\\Omega = [0,1] \\times [0,1]$ with homogeneous Dirichlet boundary conditions. A piecewise-constant diffusion coefficient $a(\\phi)$ depends on a level set function $\\phi:\\Omega \\to \\mathbb{R}$ that represents an unknown shape $\\mathcal{D} = \\{ x \\in \\Omega \\mid \\phi(x)  0 \\}$. The forward model is the variable-coefficient Poisson equation\n$$\n-\\nabla \\cdot \\big(a(\\phi)\\,\\nabla u\\big) = f \\quad \\text{in } \\Omega, \\qquad u = 0 \\quad \\text{on } \\partial \\Omega,\n$$\nwhere $f:\\Omega \\to \\mathbb{R}$ is a known source. The data assimilation objective is to reconstruct the shape $\\mathcal{D}$ by optimizing the level set function $\\phi$ to minimize a data misfit functional\n$$\nJ(\\phi) = \\tfrac{1}{2}\\int_{\\Omega} \\big(u(\\phi) - u_{\\mathrm{obs}}\\big)^2 \\,\\mathrm{d}x,\n$$\nwhere $u_{\\mathrm{obs}}:\\Omega \\to \\mathbb{R}$ is an observed state generated from a different, true shape $\\phi_{\\mathrm{true}}$. The coefficient $a(\\phi)$ is defined using a smoothed Heaviside function to ensure differentiability,\n$$\na(\\phi) = a_{\\mathrm{out}} + \\big(a_{\\mathrm{in}} - a_{\\mathrm{out}}\\big)\\,H_{\\varepsilon}(-\\phi),\n$$\nwith\n$$\nH_{\\varepsilon}(s) = \\tfrac{1}{2} + \\frac{1}{\\pi}\\arctan\\!\\Big(\\frac{s}{\\varepsilon}\\Big),\n$$\nwhere $a_{\\mathrm{in}}  0$ and $a_{\\mathrm{out}}  0$ are constants, and $\\varepsilon  0$ is a smoothing parameter. The smoothed Dirac delta associated with $H_{\\varepsilon}$ is\n$$\n\\delta_{\\varepsilon}(\\phi) = \\frac{1}{\\pi}\\frac{\\varepsilon}{\\varepsilon^2 + \\phi^2}.\n$$\nThe goal is to derive, from first principles, an adjoint-state formulation and the shape gradient with respect to $\\phi$ under the level set parameterization. Starting only from the strong form of the forward model, the definition of the data misfit functional, and standard calculus of variations, derive the adjoint equation and a pointwise expression for the shape gradient $G(\\phi)$ such that for any suitably smooth perturbation $\\eta:\\Omega \\to \\mathbb{R}$, the first-order directional derivative satisfies\n$$\n\\mathrm{D}J(\\phi)[\\eta] = \\int_{\\Omega} G(\\phi)\\,\\eta \\,\\mathrm{d}x.\n$$\nYou must then implement a program that:\n- Discretizes $\\Omega$ on a uniform Cartesian grid with $N \\times N$ nodes and grid spacing $h = 1/(N-1)$.\n- Assembles the discrete operator in flux form for $-\\nabla \\cdot \\big(a(\\phi)\\,\\nabla (\\cdot)\\big)$ using face-centered arithmetic averages of $a$ and homogeneous Dirichlet boundary conditions.\n- Solves the forward problem to compute $u(\\phi)$.\n- Constructs $u_{\\mathrm{obs}}$ by solving the same forward problem with the true shape $\\phi_{\\mathrm{true}}$.\n- Solves the adjoint problem driven by the data misfit residual to obtain the adjoint variable.\n- Computes the shape gradient $G(\\phi)$ at grid nodes using the derived pointwise expression that is consistent with the level set parameterization and the chain rule for $a(\\phi)$.\n- Verifies the adjoint-state identity by comparing the predicted directional derivative $\\int_{\\Omega} G(\\phi)\\,\\eta \\,\\mathrm{d}x$ to a finite-difference approximation $\\big(J(\\phi + t\\,\\eta) - J(\\phi)\\big)/t$ for a small step size $t$.\n\nUse the following deterministic specifications:\n- Source term $f(x,y) = 1$ for all $(x,y) \\in \\Omega$.\n- Observed state $u_{\\mathrm{obs}}$ is computed with a true shape given by a circle: $\\phi_{\\mathrm{true}}(x,y) = \\sqrt{(x - 0.5)^2 + (y - 0.5)^2} - r_{\\mathrm{true}}$ with $r_{\\mathrm{true}} = 0.25$.\n- The perturbation is $\\eta(x,y) = \\sin(2\\pi x)\\sin(2\\pi y)$.\n- The initial level set function $\\phi$ is either a circle $\\phi(x,y) = \\sqrt{(x - 0.5)^2 + (y - 0.5)^2} - r_0$ or a constant function $\\phi(x,y) \\equiv c_0$ as specified by each test case.\n- The smoothed Heaviside is $H_{\\varepsilon}(-\\phi)$ and its derivative with respect to $\\phi$ is $-\\delta_{\\varepsilon}(\\phi)$, with $\\delta_{\\varepsilon}(\\phi)$ as defined above.\n- The inner products and integrals must be approximated by their standard Riemann sums on the grid using weight $h^2$ per node.\n\nYour program must compute, for each test case, the relative consistency error between the adjoint-predicted directional derivative and the finite-difference approximation,\n$$\nE = \\frac{\\left|\\int_{\\Omega} G(\\phi)\\,\\eta \\,\\mathrm{d}x - \\frac{J(\\phi + t\\,\\eta) - J(\\phi)}{t}\\right|}{\\max\\!\\Big(\\lvert \\int_{\\Omega} G(\\phi)\\,\\eta \\,\\mathrm{d}x \\rvert + \\left\\lvert \\frac{J(\\phi + t\\,\\eta) - J(\\phi)}{t} \\right\\rvert, \\,10^{-12}\\Big)}.\n$$\n\nTest suite:\n- Case $1$: $N = 64$, $\\varepsilon = 0.02$, $t = 10^{-3}$, $a_{\\mathrm{in}} = 5$, $a_{\\mathrm{out}} = 1$, initial $\\phi$ is a circle with $r_0 = 0.35$.\n- Case $2$: $N = 64$, $\\varepsilon = 0.005$, $t = 10^{-3}$, $a_{\\mathrm{in}} = 5$, $a_{\\mathrm{out}} = 1$, initial $\\phi$ is a circle with $r_0 = 0.35$.\n- Case $3$: $N = 16$, $\\varepsilon = 0.02$, $t = 10^{-3}$, $a_{\\mathrm{in}} = 5$, $a_{\\mathrm{out}} = 1$, initial $\\phi$ is a circle with $r_0 = 0.35$.\n- Case $4$: $N = 64$, $\\varepsilon = 0.02$, $t = 10^{-4}$, $a_{\\mathrm{in}} = 5$, $a_{\\mathrm{out}} = 1$, initial $\\phi$ is the constant $c_0 = 0.5$ (so $a(\\phi)$ is uniform), which should yield a near-zero directional derivative for sufficiently small $t$.\n\nFinal output format:\n- Your program should produce a single line of output containing the four relative errors for the test suite as a comma-separated list enclosed in square brackets (for example, $\\texttt{[e1,e2,e3,e4]}$). No additional text should be printed.", "solution": "The user-provided problem is a well-posed and scientifically sound task in the field of PDE-constrained optimization and inverse problems. It requests the derivation of the adjoint-state equations for a shape optimization problem governed by a variable-coefficient Poisson equation, followed by a numerical implementation to verify the derived gradient. The problem is self-contained, with all parameters, governing equations, and numerical specifications clearly defined. It adheres to established principles of calculus of variations and numerical analysis. Therefore, the problem is deemed **valid**.\n\n### Derivation of the Shape Gradient using the Adjoint Method\n\nThe objective is to find the gradient of the functional $J(\\phi)$ with respect to the level set function $\\phi$. The functional is\n$$\nJ(\\phi) = \\frac{1}{2}\\int_{\\Omega} \\big(u(\\phi) - u_{\\mathrm{obs}}\\big)^2 \\,\\mathrm{d}x,\n$$\nwhere the state variable $u(\\phi)$ is the solution to the forward problem:\n$$\n-\\nabla \\cdot \\big(a(\\phi)\\,\\nabla u\\big) = f \\quad \\text{in } \\Omega, \\qquad u = 0 \\quad \\text{on } \\partial \\Omega.\n$$\nThe weak form of the forward problem is to find $u \\in H^1_0(\\Omega)$ such that for all test functions $v \\in H^1_0(\\Omega)$:\n$$\n\\int_{\\Omega} a(\\phi)\\,\\nabla u \\cdot \\nabla v \\,\\mathrm{d}x = \\int_{\\Omega} f\\,v \\,\\mathrm{d}x.\n$$\n\nWe seek the Gâteaux derivative of $J(\\phi)$ in an arbitrary direction $\\eta$, which is defined as\n$$\n\\mathrm{D}J(\\phi)[\\eta] = \\lim_{t \\to 0} \\frac{J(\\phi + t\\eta) - J(\\phi)}{t}.\n$$\nApplying the chain rule to the definition of $J(\\phi)$, we get:\n$$\n\\mathrm{D}J(\\phi)[\\eta] = \\int_{\\Omega} \\big(u(\\phi) - u_{\\mathrm{obs}}\\big)\\,\\dot{u} \\,\\mathrm{d}x,\n$$\nwhere $\\dot{u} = \\frac{\\mathrm{d}u}{\\mathrm{d}\\phi}[\\eta]$ is the sensitivity of the state $u$ with respect to a change in $\\phi$ in the direction $\\eta$.\n\nTo find an equation for $\\dot{u}$, we differentiate the weak form of the state equation with respect to $\\phi$ in the direction $\\eta$. The right-hand side is independent of $\\phi$, so its derivative is zero. Differentiating the left-hand side gives:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}t} \\left( \\int_{\\Omega} a(\\phi+t\\eta)\\,\\nabla u(\\phi+t\\eta) \\cdot \\nabla v \\,\\mathrm{d}x \\right) \\bigg|_{t=0} = 0.\n$$\nApplying the product rule, we obtain the tangent equation:\n$$\n\\int_{\\Omega} \\dot{a}\\,\\nabla u \\cdot \\nabla v \\,\\mathrm{d}x + \\int_{\\Omega} a(\\phi)\\,\\nabla\\dot{u} \\cdot \\nabla v \\,\\mathrm{d}x = 0,\n$$\nwhere $\\dot{a} = \\frac{\\mathrm{d}a}{\\mathrm{d}\\phi}[\\eta]$. The term $\\dot{a}$ is found by applying the chain rule to the definition of $a(\\phi)$:\n$$\na(\\phi) = a_{\\mathrm{out}} + \\big(a_{\\mathrm{in}} - a_{\\mathrm{out}}\\big)\\,H_{\\varepsilon}(-\\phi).\n$$\nThe derivative of the smoothed Heaviside function $H_{\\varepsilon}(s)$ is the smoothed Dirac delta $\\delta_{\\varepsilon}(s)$. As specified, the derivative of $H_{\\varepsilon}(-\\phi)$ with respect to $\\phi$ is $-\\delta_{\\varepsilon}(\\phi)$, where $\\delta_{\\varepsilon}(\\phi) = \\frac{1}{\\pi}\\frac{\\varepsilon}{\\varepsilon^2 + \\phi^2}$. Thus,\n$$\n\\frac{\\mathrm{d}a(\\phi)}{\\mathrm{d}\\phi} = (a_{\\mathrm{in}} - a_{\\mathrm{out}}) \\frac{\\mathrm{d}}{\\mathrm{d}\\phi} H_{\\varepsilon}(-\\phi) = -(a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi).\n$$\nThis implies $\\dot{a} = \\frac{\\mathrm{d}a(\\phi)}{\\mathrm{d}\\phi} \\eta = -(a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,\\eta$.\nSubstituting this into the tangent equation gives:\n$$\n\\int_{\\Omega} a(\\phi)\\,\\nabla\\dot{u} \\cdot \\nabla v \\,\\mathrm{d}x = \\int_{\\Omega} (a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,\\eta\\,(\\nabla u \\cdot \\nabla v) \\,\\mathrm{d}x.\n$$\n\nThe adjoint method is introduced to avoid explicitly solving for $\\dot{u}$. We define an adjoint state $p \\in H^1_0(\\Omega)$ as the solution to the following adjoint equation: find $p$ such that for all test functions $w \\in H^1_0(\\Omega)$,\n$$\n\\int_{\\Omega} a(\\phi)\\,\\nabla p \\cdot \\nabla w \\,\\mathrm{d}x = \\int_{\\Omega} (u - u_{\\mathrm{obs}})\\,w \\,\\mathrm{d}x.\n$$\nThe strong form of the adjoint equation is:\n$$\n-\\nabla \\cdot \\big(a(\\phi)\\,\\nabla p\\big) = u - u_{\\mathrm{obs}} \\quad \\text{in } \\Omega, \\qquad p = 0 \\quad \\text{on } \\partial\\Omega.\n$$\nNote that the differential operator is self-adjoint. The source term for the adjoint equation is the residual of the data misfit.\n\nNow, we strategically choose the test function $w$ in the adjoint equation's weak form to be the state sensitivity, $w=\\dot{u}$. This gives:\n$$\n\\int_{\\Omega} a(\\phi)\\,\\nabla p \\cdot \\nabla \\dot{u} \\,\\mathrm{d}x = \\int_{\\Omega} (u - u_{\\mathrm{obs}})\\,\\dot{u} \\,\\mathrm{d}x.\n$$\nThe right-hand side is exactly the expression for the directional derivative $\\mathrm{D}J(\\phi)[\\eta]$. Therefore,\n$$\n\\mathrm{D}J(\\phi)[\\eta] = \\int_{\\Omega} a(\\phi)\\,\\nabla p \\cdot \\nabla \\dot{u} \\,\\mathrm{d}x.\n$$\nBecause the bilinear form is symmetric, we can write $\\int_{\\Omega} a(\\phi)\\,\\nabla p \\cdot \\nabla \\dot{u} \\,\\mathrm{d}x = \\int_{\\Omega} a(\\phi)\\,\\nabla \\dot{u} \\cdot \\nabla p \\,\\mathrm{d}x$.\nWe can now use the tangent equation, with the test function chosen as the adjoint state, $v=p$:\n$$\n\\int_{\\Omega} a(\\phi)\\,\\nabla \\dot{u} \\cdot \\nabla p \\,\\mathrm{d}x = \\int_{\\Omega} (a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,\\eta\\,(\\nabla u \\cdot \\nabla p) \\,\\mathrm{d}x.\n$$\nBy equating the expressions for the directional derivative, we arrive at the final form:\n$$\n\\mathrm{D}J(\\phi)[\\eta] = \\int_{\\Omega} \\Big( (a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,(\\nabla u \\cdot \\nabla p) \\Big)\\,\\eta \\,\\mathrm{d}x.\n$$\nThis expression is in the required form $\\mathrm{D}J(\\phi)[\\eta] = \\int_{\\Omega} G(\\phi)\\,\\eta \\,\\mathrm{d}x$. We can therefore identify the pointwise shape gradient $G(\\phi)$ as:\n$$\nG(\\phi) = (a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,(\\nabla u \\cdot \\nabla p).\n$$\n\n### Summary of the Adjoint-State System for Gradient Computation\n1.  **Solve the Forward Problem for $u$**: Given $\\phi$, solve for $u$:\n    $$-\\nabla \\cdot (a(\\phi)\\nabla u) = f, \\quad u|_{\\partial\\Omega}=0.$$\n2.  **Solve the Adjoint Problem for $p$**: Using the solution $u$ from step 1, solve for $p$:\n    $$-\\nabla \\cdot (a(\\phi)\\nabla p) = u - u_{\\mathrm{obs}}, \\quad p|_{\\partial\\Omega}=0.$$\n3.  **Compute the Gradient $G(\\phi)$**: Using $u$ and $p$, compute the gradient:\n    $$G(\\phi) = (a_{\\mathrm{in}} - a_{\\mathrm{out}})\\,\\delta_{\\varepsilon}(\\phi)\\,(\\nabla u \\cdot \\nabla p).$$\nThis completes the derivation. The subsequent Python code implements this three-step procedure numerically.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.sparse import lil_matrix\nfrom scipy.sparse.linalg import spsolve\n\ndef solve():\n    \"\"\"\n    Main driver function to run all test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # Case 1\n        {'N': 64, 'eps': 0.02, 't': 1e-3, 'a_in': 5.0, 'a_out': 1.0, 'phi_type': 'circle', 'phi_param': 0.35},\n        # Case 2\n        {'N': 64, 'eps': 0.005, 't': 1e-3, 'a_in': 5.0, 'a_out': 1.0, 'phi_type': 'circle', 'phi_param': 0.35},\n        # Case 3\n        {'N': 16, 'eps': 0.02, 't': 1e-3, 'a_in': 5.0, 'a_out': 1.0, 'phi_type': 'circle', 'phi_param': 0.35},\n        # Case 4\n        {'N': 64, 'eps': 0.02, 't': 1e-4, 'a_in': 5.0, 'a_out': 1.0, 'phi_type': 'const', 'phi_param': 0.5},\n    ]\n\n    results = []\n    for params in test_cases:\n        error = run_case(**params)\n        results.append(error)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.10e}' for r in results)}]\")\n\ndef run_case(N, eps, t, a_in, a_out, phi_type, phi_param):\n    \"\"\"\n    Executes a single test case for the adjoint verification.\n    \"\"\"\n    h = 1.0 / (N - 1)\n    x = np.linspace(0, 1, N)\n    y = np.linspace(0, 1, N)\n    X, Y = np.meshgrid(x, y)\n\n    # Helper functions for level set, Heaviside, and delta\n    def get_phi(X, Y, type, param):\n        if type == 'circle':\n            radius = param\n            return np.sqrt((X - 0.5)**2 + (Y - 0.5)**2) - radius\n        elif type == 'const':\n            c0 = param\n            return np.full_like(X, c0)\n        \n    def H_eps(s, eps_val):\n        return 0.5 + (1.0 / np.pi) * np.arctan(s / eps_val)\n\n    def delta_eps(s, eps_val):\n        return (1.0 / np.pi) * eps_val / (eps_val**2 + s**2)\n\n    def get_a(phi, a_in_val, a_out_val, eps_val):\n        return a_out_val + (a_in_val - a_out_val) * H_eps(-phi, eps_val)\n\n    # --- Finite Difference Poisson Solver ---\n    def solve_poisson(a_coeff, rhs, h_val, N_val):\n        num_interior = (N_val - 2)**2\n        A = lil_matrix((num_interior, num_interior), dtype=np.float64)\n        \n        # Assemble matrix A for -div(a grad(u)) using a 5-point stencil\n        for i in range(1, N_val - 1):\n            for j in range(1, N_val - 1):\n                k = (i - 1) * (N_val - 2) + (j - 1)\n\n                # Face-centered arithmetic means for 'a'\n                a_south = (a_coeff[i-1, j] + a_coeff[i, j]) / 2.0\n                a_north = (a_coeff[i+1, j] + a_coeff[i, j]) / 2.0\n                a_west  = (a_coeff[i, j-1] + a_coeff[i, j]) / 2.0\n                a_east  = (a_coeff[i, j+1] + a_coeff[i, j]) / 2.0\n                \n                A[k, k] = a_north + a_south + a_east + a_west\n\n                if i > 1:\n                    A[k, k - (N_val - 2)] = -a_south # South neighbor\n                if i  N_val - 2:\n                    A[k, k + (N_val - 2)] = -a_north # North neighbor\n                if j > 1:\n                    A[k, k - 1] = -a_west           # West neighbor\n                if j  N_val - 2:\n                    A[k, k + 1] = -a_east           # East neighbor\n\n        A /= h_val**2\n        \n        # Flatten the right-hand side for interior nodes\n        b = rhs[1:-1, 1:-1].flatten()\n        \n        # Solve the linear system\n        u_interior = spsolve(A.tocsr(), b)\n        \n        # Embed solution back into the full grid with boundary conditions\n        u_full = np.zeros((N_val, N_val))\n        u_full[1:-1, 1:-1] = u_interior.reshape((N_val - 2, N_val - 2))\n        return u_full\n\n    # Function to compute the objective functional J\n    def compute_J(u, u_obs, h_val):\n        return 0.5 * np.sum((u - u_obs)**2) * h_val**2\n\n    # --- Main Calculation Steps ---\n    \n    # 1. Compute observed state u_obs\n    f_source = np.ones((N, N))\n    r_true = 0.25\n    phi_true = get_phi(X, Y, 'circle', r_true)\n    a_true = get_a(phi_true, a_in, a_out, eps)\n    u_obs = solve_poisson(a_true, f_source, h, N)\n    \n    # 2. Define initial state phi and perturbation eta\n    phi = get_phi(X, Y, phi_type, phi_param)\n    eta = np.sin(2 * np.pi * X) * np.sin(2 * np.pi * Y)\n\n    # 3. Solve forward problem for u(phi) and compute J(phi)\n    a_phi = get_a(phi, a_in, a_out, eps)\n    u_phi = solve_poisson(a_phi, f_source, h, N)\n    J_phi = compute_J(u_phi, u_obs, h)\n    \n    # 4. Compute finite difference approximation of the directional derivative\n    phi_pert = phi + t * eta\n    a_pert = get_a(phi_pert, a_in, a_out, eps)\n    u_pert = solve_poisson(a_pert, f_source, h, N)\n    J_pert = compute_J(u_pert, u_obs, h)\n    \n    DJ_fd = (J_pert - J_phi) / t\n\n    # 5. Compute adjoint-based directional derivative\n    # 5a. Solve adjoint equation: -div(a grad(p)) = u - u_obs\n    adjoint_rhs = u_phi - u_obs\n    p = solve_poisson(a_phi, adjoint_rhs, h, N)\n\n    # 5b. Compute gradients of u and p\n    grad_u_y, grad_u_x = np.gradient(u_phi, h)\n    grad_p_y, grad_p_x = np.gradient(p, h)\n    \n    # 5c. Compute the shape gradient G(phi)\n    grad_u_dot_grad_p = grad_u_x * grad_p_x + grad_u_y * grad_p_y\n    G_phi = (a_in - a_out) * delta_eps(phi, eps) * grad_u_dot_grad_p\n    \n    # 5d. Compute the directional derivative via inner product\n    DJ_adj = np.sum(G_phi * eta) * h**2\n    \n    # 6. Calculate the relative consistency error\n    numerator = np.abs(DJ_adj - DJ_fd)\n    denominator = max(np.abs(DJ_adj) + np.abs(DJ_fd), 1e-12)\n    error = numerator / denominator\n\n    return error\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3396641"}, {"introduction": "Once we can compute gradients for an optimization routine, the success of Tikhonov regularization hinges on a critical choice: the value of the regularization parameter, $\\alpha$. This parameter balances the competing demands of fitting the observed data and satisfying prior constraints on the shape, such as smoothness. This practice introduces a classic and principled method for this task, the Morozov discrepancy principle, which links the optimal $\\alpha$ to the known noise level in the data. You will implement a complete workflow that combines a gradient-based level set evolution with a search algorithm to automatically determine the regularization parameter, a crucial step in moving from theory to practical application [@problem_id:3396597].", "problem": "Consider a shape reconstruction inverse problem in which an unknown subset of a rectangular domain is represented implicitly as the zero level set of a scalar field. Let the domain be a discrete grid of size $N \\times N$ with $N = 48$. A shape is represented by a level set function $\\phi : \\{1,\\dots,N\\} \\times \\{1,\\dots,N\\} \\to \\mathbb{R}$ such that the reconstructed indicator field is $u = H_{\\varepsilon}(\\phi)$, where $H_{\\varepsilon}$ is a smoothed Heaviside function with smoothing parameter $\\varepsilon  0$. Observations are generated by a known linear, symmetric forward operator $A$ applied to $u$, contaminated by additive independent Gaussian noise with known standard deviation $\\sigma$. The reconstruction is posed as the minimization of a Tikhonov-like functional with a perimeter prior expressed in the level set framework.\n\nUse the following fundamental base:\n- The level set representation of a binary shape via a Heaviside function $H_{\\varepsilon}(\\phi)$ and its smoothed Dirac delta $\\delta_{\\varepsilon}(\\phi) = H_{\\varepsilon}'(\\phi)$.\n- The Morozov discrepancy principle: choose the regularization parameter so that the data misfit equals the noise level in the appropriate norm.\n- The gradient flow obtained from the variational derivative of the energy functional, using standard calculus of variations.\n\nLet the forward operator $A$ be a Gaussian blur with standard deviation $\\sigma_{b} = 1.2$ grid units and reflective boundary conditions. Let the smoothed Heaviside and smoothed Dirac delta be defined by\n$$\nH_{\\varepsilon}(x) = \\tfrac{1}{2} + \\tfrac{1}{\\pi} \\arctan\\!\\left(\\tfrac{x}{\\varepsilon}\\right), \\quad\n\\delta_{\\varepsilon}(x) = \\tfrac{1}{\\pi} \\tfrac{\\varepsilon}{\\varepsilon^{2} + x^{2}},\n$$\nwith $\\varepsilon = 1.0$.\n\nLet the true shape be a disk of radius $r_{\\mathrm{true}} = 12$ pixels centered at $(c_x,c_y) = (24,30)$, represented by the signed distance level set $\\phi_{\\mathrm{true}}(i,j) = \\sqrt{(i-c_x)^2 + (j-c_y)^2} - r_{\\mathrm{true}}$. The initial guess is a disk of radius $r_{0} = 10$ pixels centered at $(c_{0x},c_{0y}) = (16,18)$, i.e., $\\phi_{0}(i,j) = \\sqrt{(i-c_{0x})^2 + (j-c_{0y})^2} - r_{0}$. The noise-free data are $y_{\\mathrm{clean}} = A(H_{\\varepsilon}(\\phi_{\\mathrm{true}}))$ and the observed data are $y = y_{\\mathrm{clean}} + \\eta$, where $\\eta$ has independent Gaussian entries with zero mean and standard deviation $\\sigma$. The number of data points is $m = N^{2}$.\n\nFor a given regularization parameter $\\alpha  0$, consider the energy\n$$\nJ(\\phi;\\alpha) = \\tfrac{1}{2} \\lVert A(H_{\\varepsilon}(\\phi)) - y \\rVert_{2}^{2} + \\alpha \\int_{\\Omega} \\delta_{\\varepsilon}(\\phi) \\, \\lvert \\nabla \\phi \\rvert \\, \\mathrm{d}x,\n$$\nwhere $\\Omega$ is the discrete grid and $\\lVert \\cdot \\rVert_{2}$ is the Euclidean norm on $\\mathbb{R}^{m}$. Using the calculus of variations and the chain rule, a gradient descent for $\\phi$ with time step $\\Delta t$ can be written in discrete form as\n$$\n\\phi^{k+1} = \\phi^{k} + \\Delta t \\left( - \\delta_{\\varepsilon}(\\phi^{k}) \\cdot A^{\\top}\\!\\left( A(H_{\\varepsilon}(\\phi^{k})) - y \\right) + \\alpha \\, \\delta_{\\varepsilon}(\\phi^{k}) \\, \\kappa(\\phi^{k}) \\right),\n$$\nwhere $A^{\\top} = A$ (symmetry of the Gaussian blur) and $\\kappa(\\phi)$ is the curvature of the zero level set given by\n$$\n\\kappa(\\phi) = \\nabla \\cdot \\left( \\frac{\\nabla \\phi}{\\sqrt{\\lvert \\nabla \\phi \\rvert^{2} + \\beta}} \\right),\n$$\nwith a small stabilizer $\\beta = 10^{-8}$. The spatial derivatives are approximated with central differences and reflective boundary conditions. Use $\\Delta t = 0.2$ and perform $K = 80$ iterations per evaluation of $J$ for a given $\\alpha$.\n\nThe Morozov discrepancy principle prescribes choosing $\\alpha$ such that the residual norm equals the noise level up to a multiplicative factor: select $\\alpha$ so that\n$$\n\\lVert A(H_{\\varepsilon}(\\phi_{\\alpha}^{\\star})) - y \\rVert_{2} \\approx \\tau \\, \\sigma \\, \\sqrt{m},\n$$\nwhere $\\phi_{\\alpha}^{\\star}$ is the result of the gradient descent after $K$ iterations starting from $\\phi_{0}$. Implement a bracketing and bisection procedure on $\\alpha$ using an initial bracket $[\\alpha_{\\min},\\alpha_{\\max}] = [10^{-6}, 1.0]$, doubling $\\alpha_{\\max}$ until the residual at $\\alpha_{\\max}$ is not smaller than the target discrepancy or until $\\alpha_{\\max}$ exceeds $10^{6}$. Use a relative tolerance $\\rho = 10^{-2}$ on the discrepancy in the bisection; if the target cannot be bracketed, choose the endpoint that yields the closest residual to the target.\n\nTest suite. For each case below, generate the noise with a fixed pseudo-random seed equal to $s_{0} + i$, where $s_{0} = 12345$ and $i$ is the $0$-based index of the test case. For each case, compute the selected regularization parameter $\\alpha^{\\star}$ by the Morozov discrepancy principle as described. The cases are:\n- Case $1$: $(\\sigma,\\tau) = (0.05, 1.0)$.\n- Case $2$: $(\\sigma,\\tau) = (0.01, 1.0)$.\n- Case $3$: $(\\sigma,\\tau) = (0.10, 1.1)$.\n- Case $4$: $(\\sigma,\\tau) = (0.08, 0.9)$.\n\nFinal output format. Your program should produce a single line of output containing the selected $\\alpha^{\\star}$ for each case as a comma-separated list enclosed in square brackets, with each number rounded to $6$ decimal places (e.g., \"[0.123456,0.234567,0.345678,0.456789]\"). No additional text should be printed. All angles, if any, are measured in radians. There are no physical units beyond the discrete grid spacing; report the requested values as dimensionless numbers. Ensure the program is fully deterministic and does not require any user input.", "solution": "The user-provided problem is a well-posed and self-contained exercise in computational inverse problems, specifically focusing on shape reconstruction using a level set-based method. The problem is scientifically sound, with all components—the physical model, mathematical formulation, and numerical algorithms—being standard and clearly defined. The task is to determine an optimal regularization parameter, $\\alpha$, for four different scenarios using the Morozov discrepancy principle. This will be achieved by implementing a gradient descent optimization for the level set function and a bisection search for the parameter $\\alpha$.\n\nFirst, we formalize the problem. The unknown shape is represented by the zero level set of a function $\\phi(x,y)$, defined on a discrete grid $\\Omega$ of size $N \\times N$, where $N=48$. The shape's indicator function, $u$, which is $1$ inside the shape and $0$ outside, is approximated by a smoothed Heaviside function $H_{\\varepsilon}(\\phi)$. The problem specifies:\n$$\nH_{\\varepsilon}(x) = \\tfrac{1}{2} + \\tfrac{1}{\\pi} \\arctan\\!\\left(\\tfrac{x}{\\varepsilon}\\right)\n$$\nwith a smoothing width of $\\varepsilon = 1.0$. The derivative of this function is the smoothed Dirac delta, $\\delta_{\\varepsilon}(x) = H_{\\varepsilon}'(x)$, given by:\n$$\n\\delta_{\\varepsilon}(x) = \\tfrac{1}{\\pi} \\frac{\\varepsilon}{\\varepsilon^{2} + x^{2}}\n$$\nThis function localizes computations to the vicinity of the interface (where $\\phi \\approx 0$).\n\nThe data $y$ are generated by applying a linear forward operator $A$ to the true indicator function $u_{\\mathrm{true}} = H_{\\varepsilon}(\\phi_{\\mathrm{true}})$ and adding Gaussian noise $\\eta$. The operator $A$ is a Gaussian blur with standard deviation $\\sigma_b = 1.2$ grid units and is symmetric ($A=A^\\top$). The true shape is a disk of radius $r_{\\mathrm{true}} = 12$ centered at $(c_x, c_y) = (24, 30)$, and the initial guess for the reconstruction is a disk of radius $r_0 = 10$ centered at $(c_{0x}, c_{0y}) = (16, 18)$. The noise $\\eta$ is drawn from a Gaussian distribution with mean $0$ and standard deviation $\\sigma$.\n\nThe reconstruction $\\phi$ is found by minimizing the Tikhonov-like energy functional $J(\\phi;\\alpha)$:\n$$\nJ(\\phi;\\alpha) = \\underbrace{\\tfrac{1}{2} \\lVert A(H_{\\varepsilon}(\\phi)) - y \\rVert_{2}^{2}}_{\\text{Data Fidelity Term}} + \\underbrace{\\alpha \\int_{\\Omega} \\delta_{\\varepsilon}(\\phi) \\, \\lvert \\nabla \\phi \\rvert \\, \\mathrm{d}x}_{\\text{Perimeter Regularization}}\n$$\nThe first term ensures the reconstructed shape, when blurred, matches the observed data $y$. The second term is a perimeter penalty, which regularizes the problem by favoring shapes with shorter boundary lengths, thus promoting smoother and more compact reconstructions. $\\alpha0$ is the regularization parameter that balances these two competing objectives.\n\nTo minimize $J(\\phi;\\alpha)$, we use a gradient descent algorithm. The evolution of $\\phi$ over a fictitious time $t$ follows the negative gradient of the energy. The problem provides the discrete update rule:\n$$\n\\phi^{k+1} = \\phi^{k} + \\Delta t \\left( - \\frac{\\delta J}{\\delta \\phi}(\\phi^k) \\right)\n$$\nwhere a standard, simplified form of the variational derivative $\\frac{\\delta J}{\\delta \\phi}$ is used, leading to the update equation:\n$$\n\\phi^{k+1} = \\phi^{k} + \\Delta t \\left( -\\delta_{\\varepsilon}(\\phi^{k}) A^{\\top}\\!\\left( A(H_{\\varepsilon}(\\phi^{k})) - y \\right) + \\alpha \\delta_{\\varepsilon}(\\phi^{k}) \\kappa(\\phi^{k}) \\right)\n$$\nHere, $\\Delta t = 0.2$ is the time step. The first term inside the parentheses is the data-driven force, pulling the level set to match the data. The second term is the geometric regularization force, which moves the level set according to its mean curvature $\\kappa(\\phi)$, effectively smoothing the boundary. The curvature is given by:\n$$\n\\kappa(\\phi) = \\nabla \\cdot \\left( \\frac{\\nabla \\phi}{\\sqrt{\\lvert \\nabla \\phi \\rvert^{2} + \\beta}} \\right)\n$$\nwith a small stabilizer $\\beta=10^{-8}$. All spatial derivatives (gradient $\\nabla$ and divergence $\\nabla \\cdot$) are approximated using central differences with reflective boundary conditions, which is implemented by padding the grid before differentiation.\n\nThe core of the problem is selecting the parameter $\\alpha$. The Morozov discrepancy principle is employed for this purpose. It dictates that the regularization parameter $\\alpha$ should be chosen such that the Euclidian norm of the data residual for the optimal solution $\\phi^{\\star}_{\\alpha}$ matches the expected norm of the noise. The target is formulated as:\n$$\n\\lVert A(H_{\\varepsilon}(\\phi_{\\alpha}^{\\star})) - y \\rVert_{2} = \\tau \\, \\sigma \\, \\sqrt{m}\n$$\nwhere $m = N^2 = 48^2$ is the total number of data points (pixels) and $\\tau$ is a given factor close to $1$. The term $\\sigma\\sqrt{m}$ is the expected $L_2$-norm of the noise vector. For each given $(\\sigma, \\tau)$, we must find the corresponding $\\alpha^{\\star}$.\n\nTo find $\\alpha^{\\star}$, we define a function $D(\\alpha) = \\lVert A(H_{\\varepsilon}(\\phi_{\\alpha}^{\\star})) - y \\rVert_{2}$, where $\\phi_{\\alpha}^{\\star}$ is the result of running the gradient descent for $K=80$ iterations with parameter $\\alpha$. The function $D(\\alpha)$ is generally monotonic increasing with $\\alpha$. The problem of finding $\\alpha^{\\star}$ reduces to solving the scalar equation $D(\\alpha) = \\tau \\sigma \\sqrt{m}$. This is solved numerically using a bracketing and bisection algorithm:\n1.  **Bracketing**: An initial search interval $[\\alpha_{\\min}, \\alpha_{\\max}] = [10^{-6}, 1.0]$ is established. If the discrepancy at $\\alpha_{\\max}$ is less than the target, $\\alpha_{\\max}$ is repeatedly doubled until the target is bracketed (i.e., $D(\\alpha_{\\min})  \\text{target}  D(\\alpha_{\\max})$) or $\\alpha_{\\max}$ exceeds a cap of $10^6$. If bracketing fails, the endpoint of the search range yielding the discrepancy closest to the target is chosen.\n2.  **Bisection**: Once a bracket is found, the bisection method is used to narrow down the interval, iteratively halving it until the discrepancy at the midpoint $\\alpha_m$ is within a relative tolerance $\\rho=10^{-2}$ of the target value.\n\nThe overall algorithm for each test case is as follows:\n1.  Generate the true signed distance function $\\phi_{\\mathrm{true}}$, the true indicator $u_{\\mathrm{true}}$, and the clean data $y_{\\mathrm{clean}} = A(u_{\\mathrm{true}})$.\n2.  Generate the noise vector $\\eta$ with the specified standard deviation $\\sigma$ and random seed, and form the noisy data $y = y_{\\mathrm{clean}} + \\eta$.\n3.  Calculate the target discrepancy $T = \\tau \\sigma \\sqrt{m}$.\n4.  Execute the bracketing-and-bisection search for $\\alpha$ to find the value $\\alpha^{\\star}$ that solves $D(\\alpha) = T$ to the specified tolerance.\n5.  Store the resulting $\\alpha^{\\star}$ and repeat for the next test case.\nFinally, the computed values of $\\alpha^{\\star}$ for all four cases are reported in the specified format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import ndimage\n\ndef solve():\n    \"\"\"\n    Solves the level set shape reconstruction problem by finding the optimal\n    regularization parameter alpha using the Morozov discrepancy principle.\n    \"\"\"\n    \n    # Constants from the problem statement\n    N = 48\n    EPSILON = 1.0\n    SIGMA_B = 1.2\n    R_TRUE = 12.0\n    CX_TRUE, CY_TRUE = 23, 29  # 0-based from 1-based (24, 30)\n    R0 = 10.0\n    CX0, CY0 = 15, 17  # 0-based from 1-based (16, 18)\n    BETA = 1e-8\n    DT = 0.2\n    K = 80\n    ALPHA_MIN_INIT = 1e-6\n    ALPHA_MAX_INIT = 1.0\n    ALPHA_MAX_CAP = 1e6\n    RHO = 1e-2\n    S0 = 12345\n    M = N * N\n\n    # 1. Mathematical Functions\n    def smoothed_heaviside(x):\n        \"\"\" Smoothed Heaviside function H_epsilon(x). \"\"\"\n        return 0.5 + (1.0 / np.pi) * np.arctan(x / EPSILON)\n\n    def smoothed_delta(x):\n        \"\"\" Smoothed Dirac delta function delta_epsilon(x). \"\"\"\n        return (1.0 / np.pi) * (EPSILON / (EPSILON**2 + x**2))\n\n    def create_sdf_disk(n_grid, cx, cy, r):\n        \"\"\" Creates a signed distance function for a disk. \"\"\"\n        y_coords, x_coords = np.mgrid[0:n_grid, 0:n_grid]\n        return np.sqrt((x_coords - cx)**2 + (y_coords - cy)**2) - r\n\n    # 2. Operators and Derivatives\n    def forward_operator_A(u):\n        \"\"\" Forward operator A: Gaussian blur with reflective boundaries. \"\"\"\n        return ndimage.gaussian_filter(u, sigma=SIGMA_B, mode='reflect')\n\n    def gradient_cd_reflect(f):\n        \"\"\" Computes gradient (fy, fx) using central differences and reflective padding. \"\"\"\n        f_padded = np.pad(f, pad_width=1, mode='reflect')\n        fy = (f_padded[2:, 1:-1] - f_padded[:-2, 1:-1]) / 2.0\n        fx = (f_padded[1:-1, 2:] - f_padded[1:-1, :-2]) / 2.0\n        return fy, fx\n\n    def divergence_cd_reflect(fy, fx):\n        \"\"\" Computes divergence of a vector field (fy, fx) using central differences. \"\"\"\n        fy_padded = np.pad(fy, pad_width=1, mode='reflect')\n        fx_padded = np.pad(fx, pad_width=1, mode='reflect')\n        fyy = (fy_padded[2:, 1:-1] - fy_padded[:-2, 1:-1]) / 2.0\n        fxx = (fx_padded[1:-1, 2:] - fx_padded[1:-1, :-2]) / 2.0\n        return fxx + fyy\n\n    def curvature(phi):\n        \"\"\" Computes curvature kappa(phi) of the level set. \"\"\"\n        phi_y, phi_x = gradient_cd_reflect(phi)\n        grad_phi_norm = np.sqrt(phi_x**2 + phi_y**2 + BETA)\n        g_x = phi_x / grad_phi_norm\n        g_y = phi_y / grad_phi_norm\n        return divergence_cd_reflect(g_y, g_x)\n\n    # 3. Optimization and Parameter Search\n    def run_gradient_descent(phi_init, y, alpha):\n        \"\"\" Performs K iterations of gradient descent for a given alpha. \"\"\"\n        phi = phi_init.copy()\n        for _ in range(K):\n            u_k = smoothed_heaviside(phi)\n            delta_k = smoothed_delta(phi)\n            residual = forward_operator_A(u_k) - y\n            data_term_grad = -delta_k * forward_operator_A(residual)\n            kappa_k = curvature(phi)\n            reg_term_grad = alpha * delta_k * kappa_k\n            phi += DT * (data_term_grad + reg_term_grad)\n        return phi\n\n    def get_discrepancy(alpha, phi_init, y, memo):\n        \"\"\" Calculates the residual norm for a given alpha, with memoization. \"\"\"\n        if alpha in memo:\n            return memo[alpha]\n        phi_star = run_gradient_descent(phi_init, y, alpha)\n        u_star = smoothed_heaviside(phi_star)\n        discrepancy = np.linalg.norm(forward_operator_A(u_star) - y)\n        memo[alpha] = discrepancy\n        return discrepancy\n\n    def find_alpha_morozov(y, phi_0, sigma, tau):\n        \"\"\" Finds the optimal alpha using Morozov's principle with bisection. \"\"\"\n        target_discrepancy = tau * sigma * np.sqrt(M)\n        memo = {}\n        \n        alpha_min, alpha_max = ALPHA_MIN_INIT, ALPHA_MAX_INIT\n        d_min = get_discrepancy(alpha_min, phi_0, y, memo)\n        \n        if d_min > target_discrepancy:\n            return alpha_min\n\n        d_max = get_discrepancy(alpha_max, phi_0, y, memo)\n\n        while d_max  target_discrepancy:\n            alpha_max *= 2.0\n            if alpha_max > ALPHA_MAX_CAP:\n                return ALPHA_MAX_CAP\n            d_max = get_discrepancy(alpha_max, phi_0, y, memo)\n            \n        alpha_l, alpha_u = alpha_min, alpha_max\n        for _ in range(50):\n            alpha_m = (alpha_l + alpha_u) / 2.0\n            if alpha_m == alpha_l or alpha_m == alpha_u:\n                break\n            d_m = get_discrepancy(alpha_m, phi_0, y, memo)\n            if abs(d_m - target_discrepancy)  RHO * target_discrepancy:\n                return alpha_m\n            if d_m  target_discrepancy:\n                alpha_l = alpha_m\n            else:\n                alpha_u = alpha_m\n        \n        d_l = get_discrepancy(alpha_l, phi_0, y, memo)\n        d_u = get_discrepancy(alpha_u, phi_0, y, memo)\n        return alpha_l if abs(d_l - target_discrepancy)  abs(d_u - target_discrepancy) else alpha_u\n\n    # 4. Main Execution Loop\n    phi_true = create_sdf_disk(N, CX_TRUE, CY_TRUE, R_TRUE)\n    u_true = smoothed_heaviside(phi_true)\n    y_clean = forward_operator_A(u_true)\n    phi_0 = create_sdf_disk(N, CX0, CY0, R0)\n    \n    test_cases = [\n        (0.05, 1.0), (0.01, 1.0), (0.10, 1.1), (0.08, 0.9)\n    ]\n    \n    results = []\n    for i, (sigma, tau) in enumerate(test_cases):\n        seed = S0 + i\n        rng = np.random.default_rng(seed)\n        noise = rng.normal(0, sigma, (N, N))\n        y = y_clean + noise\n        \n        alpha_star = find_alpha_morozov(y, phi_0, sigma, tau)\n        results.append(\"{:.6f}\".format(alpha_star))\n        \n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3396597"}, {"introduction": "The choice of a data misfit function is a fundamental modeling decision that implicitly makes an assumption about the statistical properties of the measurement noise. While the standard least-squares misfit is computationally convenient, its underlying assumption of Gaussian noise makes it highly sensitive to outliers, which are common in real-world data. This exercise delves into the Bayesian connection between noise models and objective functions, tasking you with implementing and comparing robust misfits derived from the Laplace and Student's $t$ distributions. Through a concrete shape reconstruction problem, you will directly observe how robust statistical methods can dramatically improve inversion results when data is contaminated with outliers [@problem_id:3396667].", "problem": "Consider a two-dimensional shape reconstruction problem framed in the Level Set Method, where a binary shape is represented by a level set function. Let the unknown shape be parameterized by the circle parameters $\\theta = (c_x, c_y, r)$, and let the level set function be defined by $\\phi_\\theta(x,y)$ such that the zero level set delineates the interface. The forward operator maps $\\phi_\\theta$ to a predicted observation image by smoothing the binary indicator associated with $\\phi_\\theta$ and convolving it with a known point spread function. The observations are corrupted by noise that is not necessarily Gaussian, and may include sparse outliers.\n\nStarting from: (i) the Bayesian paradigm linking likelihood to negative log-likelihood misfit, (ii) the definition of the level set function for geometric shapes, and (iii) the convolution with a known Gaussian Point Spread Function (PSF), derive robust negative log-likelihood misfits corresponding to three noise models: Gaussian, Laplace, and Student’s $t$ with fixed degrees of freedom. Your program must implement these derived misfits and perform a discrete search over the shape parameter space to estimate the radius $r$ for each test case and each noise model.\n\nThe program must follow these modeling and algorithmic specifications:\n\n- Level set representation:\n  - Represent a circle by a level set function $\\phi_\\theta(x,y)$ defined on a uniform grid of size $N \\times N$ with pixel coordinates $(x,y) \\in \\{0,1,\\dots,N-1\\}^2$.\n  - Use a smooth approximation of the binary indicator via a smooth Heaviside function $H_\\epsilon(\\phi_\\theta)$ with smoothing parameter $\\epsilon  0$.\n\n- Forward operator:\n  - The predicted image is obtained by convolving $H_\\epsilon(\\phi_\\theta)$ with a Gaussian PSF of standard deviation $\\sigma_{\\text{psf}}$.\n\n- Noise models and robust misfits:\n  - Derive, from first principles, the negative log-likelihood misfit for the three noise models:\n    - Gaussian with standard deviation $\\sigma$.\n    - Laplace with scale parameter $b$.\n    - Student’s $t$ with degrees of freedom $\\nu$ and scale parameter $\\sigma_t$.\n  - Implement these misfits as functions of the image residual between the observed and predicted images.\n\n- Estimation strategy:\n  - For each test case, generate synthetic observations by evaluating the forward operator at the true parameter $\\theta^\\star = (c_x^\\star, c_y^\\star, r^\\star)$, adding zero-mean independent Gaussian noise with the stated standard deviation, and then injecting a specified number of sparse outliers with specified amplitude.\n  - For each noise model, perform a discrete search over the parameter space\n    $$\n    c_x \\in \\{c_x^\\star - 2, c_x^\\star - 1, c_x^\\star, c_x^\\star + 1, c_x^\\star + 2\\},\\quad\n    c_y \\in \\{c_y^\\star - 2, c_y^\\star - 1, c_y^\\star, c_y^\\star + 1, c_y^\\star + 2\\},\\quad\n    r \\in \\{r^\\star - 2, r^\\star - 1, r^\\star, r^\\star + 1, r^\\star + 2\\},\n    $$\n    clipping indices to the valid image bounds and restricting $r$ to positive integers, and select the parameter that minimizes the corresponding negative log-likelihood misfit.\n  - Report only the estimated radius $r$ for each noise model in each test case.\n\nTest suite and parameters:\n\n- All angles, if any, are to be treated in radians. No physical units are involved, and all outputs are dimensionless.\n\n- Use the following three test cases (all integer values are exact and must be treated as such):\n\n  1. Case $1$ (baseline, near-ideal):\n     - $N = 64$, $c_x^\\star = 32$, $c_y^\\star = 32$, $r^\\star = 12$, $\\epsilon = 1.0$, $\\sigma_{\\text{psf}} = 1.2$,\n     - Observation noise standard deviation $\\sigma_{\\text{obs}} = 0.05$, number of outliers $n_{\\text{out}} = 0$, outlier amplitude $a_{\\text{out}} = 0.0$.\n\n  2. Case $2$ (heavy-tailed contamination):\n     - $N = 64$, $c_x^\\star = 32$, $c_y^\\star = 32$, $r^\\star = 12$, $\\epsilon = 1.0$, $\\sigma_{\\text{psf}} = 1.2$,\n     - Observation noise standard deviation $\\sigma_{\\text{obs}} = 0.05$, number of outliers $n_{\\text{out}} = 300$, outlier amplitude $a_{\\text{out}} = 0.5$.\n\n  3. Case $3$ (near-boundary geometry):\n     - $N = 64$, $c_x^\\star = 54$, $c_y^\\star = 10$, $r^\\star = 9$, $\\epsilon = 1.0$, $\\sigma_{\\text{psf}} = 1.2$,\n     - Observation noise standard deviation $\\sigma_{\\text{obs}} = 0.04$, number of outliers $n_{\\text{out}} = 40$, outlier amplitude $a_{\\text{out}} = 0.3$.\n\n- For misfit parameterization use:\n  - Gaussian: $\\sigma = \\sigma_{\\text{obs}}$ for each case.\n  - Laplace: $b = \\sigma_{\\text{obs}} / \\sqrt{2}$ for each case.\n  - Student’s $t$: $\\nu = 3$ (fixed for all cases), $\\sigma_t = \\sigma_{\\text{obs}}$ for each case.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets, where each inner list corresponds to one test case and contains the three estimated integer radii in the order [Gaussian, Laplace, Student-$t$]. For example: \"[[r1G,r1L,r1T],[r2G,r2L,r2T],[r3G,r3L,r3T]]\".", "solution": "We begin with the Bayesian framework for inverse problems. Let the observed image be denoted by $y \\in \\mathbb{R}^{N \\times N}$ and the forward operator mapping the parameters $\\theta$ to a predicted image $f(\\theta) \\in \\mathbb{R}^{N \\times N}$. Under the likelihood-based approach, the posterior density is proportional to the product of the prior density and the likelihood. In the absence of an informative prior, the Maximum A Posteriori (MAP) estimator reduces to the Maximum Likelihood (ML) estimator, and minimizing the negative log-likelihood is equivalent to maximizing the likelihood.\n\nThe residual image is defined by $r(\\theta) = y - f(\\theta)$. The negative log-likelihood misfit is derived from the Probability Density Function (PDF) of the noise model applied pixelwise under the assumption of independent and identically distributed noise. We derive three misfits:\n\n1. Gaussian noise model:\n   The Gaussian noise model with standard deviation $\\sigma$ has PDF for each residual $r_i$ given by\n   $$\n   p(r_i \\mid \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma} \\exp\\left(-\\frac{r_i^2}{2\\sigma^2}\\right).\n   $$\n   The negative log-likelihood aggregated over pixels yields, up to an additive constant independent of $\\theta$,\n   $$\n   \\mathcal{J}_{\\text{Gaussian}}(\\theta) = \\sum_i \\frac{r_i(\\theta)^2}{2\\sigma^2}.\n   $$\n   This is the familiar least-squares misfit scaled by the variance, and is sensitive to outliers due to the quadratic penalty on large residuals.\n\n2. Laplace (double-exponential) noise model:\n   The Laplace noise model with scale parameter $b$ has PDF\n   $$\n   p(r_i \\mid b) = \\frac{1}{2b} \\exp\\left(-\\frac{|r_i|}{b}\\right).\n   $$\n   The negative log-likelihood, ignoring constants, becomes\n   $$\n   \\mathcal{J}_{\\text{Laplace}}(\\theta) = \\sum_i \\frac{|r_i(\\theta)|}{b}.\n   $$\n   This is an $\\ell_1$-type misfit, which is more robust to outliers than the Gaussian model because the penalty grows linearly with the residual magnitude.\n\n3. Student’s $t$ noise model:\n   The Student’s $t$ distribution with degrees of freedom $\\nu$ and scale $\\sigma_t$ has PDF\n   $$\n   p(r_i \\mid \\nu, \\sigma_t) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\Gamma\\left(\\frac{\\nu}{2}\\right)\\sqrt{\\nu\\pi}\\sigma_t} \\left(1 + \\frac{r_i^2}{\\nu \\sigma_t^2}\\right)^{-\\frac{\\nu+1}{2}}.\n   $$\n   The negative log-likelihood, up to an additive constant independent of $\\theta$, is\n   $$\n   \\mathcal{J}_{\\text{Student}}(\\theta) = \\sum_i \\frac{\\nu+1}{2} \\log\\left(1 + \\frac{r_i(\\theta)^2}{\\nu \\sigma_t^2}\\right).\n   $$\n   This misfit is strongly robust to outliers because the effective penalty transitions from quadratic near zero to subquadratic for large residuals.\n\nForward operator construction via Level Set Method:\n\n- We represent a circle by the level set function\n  $$\n  \\phi_\\theta(x,y) = r - \\sqrt{(x - c_x)^2 + (y - c_y)^2},\n  $$\n  where $(x,y)$ are pixel coordinates on the grid $\\{0,1,\\dots,N-1\\} \\times \\{0,1,\\dots,N-1\\}$ and $\\theta = (c_x, c_y, r)$.\n\n- To obtain a differentiable indicator, we use a smooth Heaviside function $H_\\epsilon(\\cdot)$ with smoothing parameter $\\epsilon  0$. A commonly used smooth approximation is based on the hyperbolic tangent:\n  $$\n  H_\\epsilon(\\phi) = \\frac{1}{2} \\left( 1 + \\tanh\\left(\\frac{\\phi}{\\epsilon}\\right) \\right),\n  $$\n  which transitions smoothly from $0$ to $1$ across the interface region of width proportional to $\\epsilon$.\n\n- The predicted image is modeled by a convolution of $H_\\epsilon(\\phi_\\theta)$ with a Gaussian Point Spread Function (PSF) of standard deviation $\\sigma_{\\text{psf}}$. Denoting the convolution by $\\ast$, we have\n  $$\n  f(\\theta) = \\text{PSF}_{\\sigma_{\\text{psf}}} \\ast H_\\epsilon(\\phi_\\theta).\n  $$\n  In practice, we compute this via a Gaussian filter over the image.\n\nSynthetic observations:\n\n- For each test case, we compute $f(\\theta^\\star)$ at the true parameters $\\theta^\\star$ to obtain a noise-free predicted image.\n\n- We then add independent zero-mean Gaussian noise with standard deviation $\\sigma_{\\text{obs}}$ to each pixel. Next, we add $n_{\\text{out}}$ outliers by selecting $n_{\\text{out}}$ random pixel locations uniformly over the image and adding a fixed amplitude $a_{\\text{out}}$ with random sign to those pixels. This produces the observed image $y$.\n\nDiscrete search and estimation:\n\n- For each noise model, we compute the misfit $\\mathcal{J}(\\theta)$ for all candidates in the discrete set\n  $$\n  c_x \\in \\{c_x^\\star - 2, c_x^\\star - 1, c_x^\\star, c_x^\\star + 1, c_x^\\star + 2\\},\\quad\n  c_y \\in \\{c_y^\\star - 2, c_y^\\star - 1, c_y^\\star, c_y^\\star + 1, c_y^\\star + 2\\},\\quad\n  r \\in \\{r^\\star - 2, r^\\star - 1, r^\\star, r^\\star + 1, r^\\star + 2\\},\n  $$\n  clipped to valid bounds for $c_x$ and $c_y$ and restricted to positive integer $r$. We select the parameter $\\theta$ that minimizes $\\mathcal{J}(\\theta)$.\n\n- We then report only the estimated radius $r$ for each noise model in each test case.\n\nNoise model parameters:\n\n- For the Gaussian misfit, set $\\sigma = \\sigma_{\\text{obs}}$.\n\n- For the Laplace misfit, set $b = \\sigma_{\\text{obs}} / \\sqrt{2}$, which aligns scales so that the Laplace variance $2b^2$ matches the Gaussian variance approximately.\n\n- For the Student’s $t$ misfit, fix $\\nu = 3$ and set $\\sigma_t = \\sigma_{\\text{obs}}$.\n\nOutput:\n\n- The program must output a single line in the format\n  $$\n  [[r_{1,\\text{G}}, r_{1,\\text{L}}, r_{1,\\text{T}}], [r_{2,\\text{G}}, r_{2,\\text{L}}, r_{2,\\text{T}}], [r_{3,\\text{G}}, r_{3,\\text{L}}, r_{3,\\text{T}}]],\n  $$\n  where $r_{i,\\cdot}$ are integers representing the estimated radii for test case $i \\in \\{1,2,3\\}$ and the Gaussian (G), Laplace (L), and Student’s $t$ (T) noise models, respectively.\n\nThis design tests the following: (a) correct derivation and implementation of robust misfits from their PDFs, (b) correct level set forward modeling, (c) robustness differences under clean, outlier-contaminated, and near-boundary scenarios, and (d) a consistent final result aggregation in the specified format.", "answer": "```python\nimport numpy as np\nfrom scipy.ndimage import gaussian_filter\n\ndef smooth_heaviside(phi, eps):\n    # Smooth Heaviside using tanh\n    return 0.5 * (1.0 + np.tanh(phi / eps))\n\ndef level_set_circle(N, cx, cy, r):\n    # Create level set phi = r - distance to center\n    y_indices, x_indices = np.meshgrid(np.arange(N), np.arange(N), indexing='ij')\n    dist = np.sqrt((x_indices - cx)**2 + (y_indices - cy)**2)\n    phi = r - dist\n    return phi\n\ndef forward_operator(N, theta, eps, psf_sigma):\n    cx, cy, r = theta\n    phi = level_set_circle(N, cx, cy, r)\n    H = smooth_heaviside(phi, eps)\n    pred = gaussian_filter(H, psf_sigma, mode='reflect')\n    return pred\n\ndef generate_observation(N, true_theta, eps, psf_sigma, sigma_obs, n_outliers, outlier_amp, rng):\n    clean = forward_operator(N, true_theta, eps, psf_sigma)\n    noise = rng.normal(0.0, sigma_obs, size=(N, N))\n    obs = clean + noise\n    # Inject sparse outliers with random sign\n    if n_outliers > 0 and outlier_amp > 0.0:\n        total_pixels = N * N\n        idx = rng.choice(total_pixels, size=min(n_outliers, total_pixels), replace=False)\n        signs = rng.choice(np.array([-1.0, 1.0]), size=idx.shape[0])\n        obs.flat[idx] += signs * outlier_amp\n    return obs\n\ndef nll_gaussian(residual, sigma):\n    # Sum of 0.5 * (r/sigma)^2 over pixels\n    return 0.5 * np.sum((residual / sigma) ** 2)\n\ndef nll_laplace(residual, b):\n    # Sum of |r|/b over pixels\n    return np.sum(np.abs(residual) / b)\n\ndef nll_student_t(residual, nu, sigma_t):\n    # Sum of 0.5*(nu+1)*log(1 + r^2/(nu*sigma_t^2)) over pixels\n    # ignoring additive constants\n    return 0.5 * (nu + 1.0) * np.sum(np.log1p((residual ** 2) / (nu * (sigma_t ** 2))))\n\ndef clip_center(N, c):\n    return int(np.clip(c, 0, N - 1))\n\ndef candidate_grid(cx_true, cy_true, r_true, N):\n    cx_candidates = [clip_center(N, cx_true + d) for d in (-2, -1, 0, 1, 2)]\n    cy_candidates = [clip_center(N, cy_true + d) for d in (-2, -1, 0, 1, 2)]\n    r_candidates = [ri for ri in (r_true - 2, r_true - 1, r_true, r_true + 1, r_true + 2) if ri > 0]\n    return cx_candidates, cy_candidates, r_candidates\n\ndef estimate_radius_for_models(N, obs, eps, psf_sigma, cx_true, cy_true, r_true, sigma_obs):\n    cx_cand, cy_cand, r_cand = candidate_grid(cx_true, cy_true, r_true, N)\n    # Misfit parameters\n    sigma_gauss = sigma_obs\n    b_laplace = sigma_obs / np.sqrt(2.0)\n    nu = 3\n    sigma_t = sigma_obs\n\n    best_gauss = (np.inf, None)\n    best_laplace = (np.inf, None)\n    best_student = (np.inf, None)\n\n    # Precompute nothing; iterate candidates\n    for cx in cx_cand:\n        for cy in cy_cand:\n            for r in r_cand:\n                pred = forward_operator(N, (cx, cy, r), eps, psf_sigma)\n                residual = obs - pred\n\n                jg = nll_gaussian(residual, sigma_gauss)\n                if jg  best_gauss[0]:\n                    best_gauss = (jg, r)\n\n                jl = nll_laplace(residual, b_laplace)\n                if jl  best_laplace[0]:\n                    best_laplace = (jl, r)\n\n                jt = nll_student_t(residual, nu, sigma_t)\n                if jt  best_student[0]:\n                    best_student = (jt, r)\n\n    return [int(best_gauss[1]), int(best_laplace[1]), int(best_student[1])]\n\ndef solve():\n    rng = np.random.default_rng(12345)\n    test_cases = [\n        # Case 1: Baseline, near-ideal\n        {\n            \"N\": 64, \"cx_true\": 32, \"cy_true\": 32, \"r_true\": 12,\n            \"eps\": 1.0, \"psf_sigma\": 1.2,\n            \"sigma_obs\": 0.05, \"n_outliers\": 0, \"outlier_amp\": 0.0\n        },\n        # Case 2: Heavy-tailed contamination\n        {\n            \"N\": 64, \"cx_true\": 32, \"cy_true\": 32, \"r_true\": 12,\n            \"eps\": 1.0, \"psf_sigma\": 1.2,\n            \"sigma_obs\": 0.05, \"n_outliers\": 300, \"outlier_amp\": 0.5\n        },\n        # Case 3: Near-boundary geometry\n        {\n            \"N\": 64, \"cx_true\": 54, \"cy_true\": 10, \"r_true\": 9,\n            \"eps\": 1.0, \"psf_sigma\": 1.2,\n            \"sigma_obs\": 0.04, \"n_outliers\": 40, \"outlier_amp\": 0.3\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        cx_true = case[\"cx_true\"]\n        cy_true = case[\"cy_true\"]\n        r_true = case[\"r_true\"]\n        eps = case[\"eps\"]\n        psf_sigma = case[\"psf_sigma\"]\n        sigma_obs = case[\"sigma_obs\"]\n        n_outliers = case[\"n_outliers\"]\n        outlier_amp = case[\"outlier_amp\"]\n\n        obs = generate_observation(\n            N,\n            (cx_true, cy_true, r_true),\n            eps,\n            psf_sigma,\n            sigma_obs,\n            n_outliers,\n            outlier_amp,\n            rng\n        )\n        est_radii = estimate_radius_for_models(\n            N, obs, eps, psf_sigma, cx_true, cy_true, r_true, sigma_obs\n        )\n        results.append(est_radii)\n\n    # Print single line in the required format\n    # Nested lists with integers\n    def format_nested(lst):\n        return \"[\" + \",\".join(\"[\" + \",\".join(str(x) for x in inner) + \"]\" for inner in lst) + \"]\"\n\n    print(format_nested(results))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3396667"}]}