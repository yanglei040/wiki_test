{"hands_on_practices": [{"introduction": "At the heart of Gaussian process regression lies the multivariate normal distribution, and its probability density is the foundation for model fitting and prediction. This exercise provides hands-on practice with the core numerical operations required to evaluate the GP's log-marginal likelihood. By explicitly calculating the log-determinant and Cholesky factorization of a covariance matrix [@problem_id:3423975], you will solidify your understanding of the essential computational building blocks that underpin nearly all GP-based methods.", "problem": "Consider a linear observation model in a data assimilation setting where the latent state is endowed with a Gaussian Process (GP) prior. In Gaussian Process regression (GPR), the covariance of two noisily observed function values at two input locations is the sum of the prior covariance and an independent observation noise variance. Specifically, let the prior covariance matrix be $K=\\begin{pmatrix}1 & 0.9\\\\ 0.9 & 1\\end{pmatrix}$ and let the observation noise variance be $\\sigma^{2}=0.1$, so the total covariance of the observed data is $A=K+\\sigma^{2}I$. For the purposes of surrogate modeling and hyperparameter inference in inverse problems, two computational primitives are required: the natural logarithm of the determinant $\\ln\\det(A)$ and the lower-triangular Cholesky factor $L$ with positive diagonal entries such that $A=L\\,L^{\\top}$.\n\nStarting from the definitions and elementary properties of determinants and the Cholesky factorization of symmetric positive definite matrices, compute $\\ln\\det(K+\\sigma^{2}I)$ and the Cholesky factor $L$ explicitly for the given $K$ and $\\sigma^{2}$. Use the natural logarithm and provide exact algebraic expressions (do not round). Your final answer must contain both quantities, with the Cholesky factor specified by its nonzero entries.", "solution": "The user has provided a valid, well-posed problem statement from the field of data assimilation and machine learning.\n\nThe problem requires the computation of two essential quantities for Gaussian Process regression: the natural logarithm of the determinant of a covariance matrix, $\\ln\\det(A)$, and the Cholesky factor $L$ of that matrix.\n\nThe given quantities are:\nThe prior covariance matrix $K = \\begin{pmatrix} 1 & 0.9 \\\\ 0.9 & 1 \\end{pmatrix}$.\nThe observation noise variance $\\sigma^{2}=0.1$.\nThe total covariance of the observed data is given by the model $A=K+\\sigma^{2}I$, where $I$ is the $2 \\times 2$ identity matrix.\n\nFirst, we construct the matrix $A$.\n$$\nI = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\n$$\nA = K + \\sigma^{2}I = \\begin{pmatrix} 1 & 0.9 \\\\ 0.9 & 1 \\end{pmatrix} + 0.1 \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\n$$\nA = \\begin{pmatrix} 1 & 0.9 \\\\ 0.9 & 1 \\end{pmatrix} + \\begin{pmatrix} 0.1 & 0 \\\\ 0 & 0.1 \\end{pmatrix} = \\begin{pmatrix} 1.1 & 0.9 \\\\ 0.9 & 1.1 \\end{pmatrix}\n$$\nTo maintain exactness, we can express these decimal values as fractions:\n$$\nA = \\begin{pmatrix} \\frac{11}{10} & \\frac{9}{10} \\\\ \\frac{9}{10} & \\frac{11}{10} \\end{pmatrix}\n$$\n\nNext, we compute the first required quantity, $\\ln\\det(A)$.\nThe determinant of a $2 \\times 2$ matrix $\\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$ is given by the formula $ad - bc$.\n$$\n\\det(A) = \\left(\\frac{11}{10}\\right) \\left(\\frac{11}{10}\\right) - \\left(\\frac{9}{10}\\right) \\left(\\frac{9}{10}\\right) = \\frac{121}{100} - \\frac{81}{100} = \\frac{121 - 81}{100} = \\frac{40}{100} = \\frac{2}{5}\n$$\nThe natural logarithm of the determinant is therefore:\n$$\n\\ln\\det(A) = \\ln\\left(\\frac{2}{5}\\right)\n$$\n\nSecond, we compute the Cholesky factor $L$. $A$ is a symmetric positive definite matrix, so a unique lower-triangular matrix $L$ with positive diagonal entries exists such that $A = L L^{\\top}$. Let $L$ be:\n$$\nL = \\begin{pmatrix} L_{11} & 0 \\\\ L_{21} & L_{22} \\end{pmatrix}\n$$\nThe condition $A=LL^{\\top}$ gives:\n$$\n\\begin{pmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{pmatrix} = \\begin{pmatrix} L_{11} & 0 \\\\ L_{21} & L_{22} \\end{pmatrix} \\begin{pmatrix} L_{11} & L_{21} \\\\ 0 & L_{22} \\end{pmatrix} = \\begin{pmatrix} L_{11}^2 & L_{11}L_{21} \\\\ L_{21}L_{11} & L_{21}^2 + L_{22}^2 \\end{pmatrix}\n$$\nWe solve for the entries of $L$ by equating the elements of the matrices:\nThe $(1,1)$ element gives $A_{11} = L_{11}^2$:\n$$\nL_{11}^2 = \\frac{11}{10} \\implies L_{11} = \\sqrt{\\frac{11}{10}}\n$$\nWe take the positive root as required for the Cholesky factorization.\n\nThe $(2,1)$ element gives $A_{21} = L_{21}L_{11}$:\n$$\nL_{21} = \\frac{A_{21}}{L_{11}} = \\frac{9/10}{\\sqrt{11/10}} = \\frac{9}{10} \\frac{\\sqrt{10}}{\\sqrt{11}} = \\frac{9}{\\sqrt{10}\\sqrt{11}} = \\frac{9}{\\sqrt{110}}\n$$\n\nThe $(2,2)$ element gives $A_{22} = L_{21}^2 + L_{22}^2$:\n$$\nL_{22}^2 = A_{22} - L_{21}^2 = \\frac{11}{10} - \\left(\\frac{9}{\\sqrt{110}}\\right)^2 = \\frac{11}{10} - \\frac{81}{110}\n$$\nTo subtract, we find a common denominator:\n$$\nL_{22}^2 = \\frac{11 \\times 11}{110} - \\frac{81}{110} = \\frac{121 - 81}{110} = \\frac{40}{110} = \\frac{4}{11}\n$$\nTaking the positive square root:\n$$\nL_{22} = \\sqrt{\\frac{4}{11}} = \\frac{2}{\\sqrt{11}}\n$$\nThus, the Cholesky factor is:\n$$\nL = \\begin{pmatrix} \\sqrt{\\frac{11}{10}} & 0 \\\\ \\frac{9}{\\sqrt{110}} & \\frac{2}{\\sqrt{11}} \\end{pmatrix}\n$$\nThe problem requires the two computed quantities: $\\ln\\det(A)$ and the nonzero entries of the Cholesky factor $L$, which are $L_{11}$, $L_{21}$, and $L_{22}$.\n\nThe final values are:\n$\\ln\\det(A) = \\ln(\\frac{2}{5})$\n$L_{11} = \\sqrt{\\frac{11}{10}}$\n$L_{21} = \\frac{9}{\\sqrt{110}}$\n$L_{22} = \\frac{2}{\\sqrt{11}}$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\ln\\left(\\frac{2}{5}\\right) & \\sqrt{\\frac{11}{10}} & \\frac{9}{\\sqrt{110}} & \\frac{2}{\\sqrt{11}} \\end{pmatrix}}\n$$", "id": "3423975"}, {"introduction": "A Gaussian process model is defined by its kernel and hyperparameters, which must be tuned to best explain the observed data. This exercise moves from static calculation to dynamic optimization by tackling the crucial task of hyperparameter estimation. You will derive the first-order condition for the maximum likelihood estimate (MLE) of the observation noise variance and implement a numerical solver to find its value [@problem_id:3423961], bridging the gap between the theoretical principles of likelihood maximization and the practical application of fitting a GP surrogate model.", "problem": "Consider a one-dimensional Gaussian Process (GP) regression surrogate model for an expensive forward model in an inverse problem and data assimilation context. You are given a small dataset of size $n=5$ with input locations and corresponding scalar outputs. The covariance between outputs is modeled by the Squared Exponential (SE) kernel $k_{\\mathrm{SE}}$ with fixed signal variance and length-scale hyperparameters. Assume additive independent and identically distributed Gaussian observation noise with variance $\\sigma^2$, which is unknown. Your goal is to obtain the Maximum Likelihood Estimation (MLE) of $\\sigma^2$ by differentiating the exact marginal likelihood and solving the First-Order Condition (FOC) numerically.\n\nStarting from the fundamental definitions of Gaussian Process regression and the exact marginal likelihood of a multivariate normal distribution, derive the first-order optimality condition for $\\sigma^2$ using only well-established properties of matrix calculus, such as the derivative of a log-determinant and the derivative of a matrix inverse with respect to a scalar parameter. Do not assume any shortcut formulas that skip such derivations. Then, implement a numerical solver to find the MLE $\\hat{\\sigma}^2$ that satisfies the derived FOC. If the FOC has no solution in the open interval $\\sigma^2>0$, determine the maximizer by correctly analyzing boundary behavior.\n\nUse the following fixed kernel and data specification. For each test case, the covariance matrix is defined by\n$$\nK_{ij} = k_{\\mathrm{SE}}(x_i, x_j) = \\alpha^2 \\exp\\left(-\\frac{1}{2}\\left(\\frac{|x_i - x_j|}{\\ell}\\right)^2\\right),\n$$\nwhere $\\alpha^2$ is the signal variance and $\\ell$ is the length-scale. The observation model is $y \\sim \\mathcal{N}(0, K + \\sigma^2 I)$, where $I$ is the $n \\times n$ identity matrix.\n\nImplement a program that, for each test case, computes the MLE $\\hat{\\sigma}^2$ by:\n- Building the covariance matrix $K$ using $k_{\\mathrm{SE}}$ and the given $(\\alpha^2, \\ell)$,\n- Forming the exact log marginal likelihood of $y$ under $K + \\sigma^2 I$,\n- Differentiating with respect to $\\sigma^2$ to obtain the FOC,\n- Solving the FOC numerically for $\\sigma^2 > 0$; if no interior solution exists, selecting the correct boundary maximizer.\n\nReport each $\\hat{\\sigma}^2$ as a floating-point number rounded to six decimal places.\n\nTest suite:\n- Case $1$ (general case): $x = [0.0, 0.25, 0.5, 0.75, 1.0]$, $y = [0.05, 0.6, 0.9, 0.6, 0.0]$, $\\alpha^2 = 1.2$, $\\ell = 0.3$.\n- Case $2$ (boundary case with near-zero signal): $x = [0.0, 0.1, 0.4, 0.8, 1.2]$, $y = [0.0, 0.0, 0.0, 0.0, 0.0]$, $\\alpha^2 = 0.8$, $\\ell = 0.5$.\n- Case $3$ (different scale and curvature): $x = [0.0, 0.2, 0.5, 0.9, 1.4]$, $y = [1.0, 0.9, 0.4, -0.2, -0.7]$, $\\alpha^2 = 1.5$, $\\ell = 0.4$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{result1},\\text{result2},\\text{result3}]$), with each $\\hat{\\sigma}^2$ rounded to six decimal places. No physical units apply in this problem, and all angles, if any, are implicitly dimensionless real numbers. The final outputs must be floats.", "solution": "The user wants to find the Maximum Likelihood Estimate (MLE) for the noise variance ($\\sigma^2$) in a Gaussian Process (GP) regression model. The process involves two main stages: first, a formal derivation of the first-order condition (FOC) for the MLE of $\\sigma^2$ from fundamental principles; second, the implementation of a numerical solver to find the value of $\\sigma^2$ that satisfies this condition for given datasets.\n\n### Part 1: Derivation of the First-Order Condition\n\nThe problem specifies a GP model for a set of observations $y$ at input locations $X$. The observations are assumed to follow a multivariate normal distribution with a zero mean:\n$$\ny \\sim \\mathcal{N}(0, K_y)\n$$\nwhere $K_y$ is the covariance matrix of the noisy observations. It is defined as the sum of the signal covariance matrix $K$ and the noise covariance matrix. The signal covariance is given by the Squared Exponential (SE) kernel, and the noise is assumed to be independent and identically distributed Gaussian noise with variance $\\sigma^2$. Thus, the total covariance matrix is:\n$$\nK_y = K + \\sigma^2 I\n$$\nwhere $K_{ij} = k_{\\mathrm{SE}}(x_i, x_j) = \\alpha^2 \\exp\\left(-\\frac{1}{2}\\left(\\frac{|x_i - x_j|}{\\ell}\\right)^2\\right)$ and $I$ is the $n \\times n$ identity matrix, with $n$ being the number of data points.\n\nThe probability density function (PDF) of this multivariate normal distribution is:\n$$\np(y | X, \\alpha^2, \\ell, \\sigma^2) = \\frac{1}{(2\\pi)^{n/2} (\\det(K_y))^{1/2}} \\exp\\left(-\\frac{1}{2} y^T K_y^{-1} y\\right)\n$$\nThe log marginal likelihood, denoted by $\\mathcal{L}(\\sigma^2)$, is the natural logarithm of this PDF. We treat $\\sigma^2$ as the variable to be optimized, while other parameters are fixed.\n$$\n\\mathcal{L}(\\sigma^2) = \\log p(y | \\sigma^2) = -\\frac{n}{2}\\log(2\\pi) - \\frac{1}{2}\\log(\\det(K_y)) - \\frac{1}{2} y^T K_y^{-1} y\n$$\nTo find the MLE for $\\sigma^2$, we must maximize $\\mathcal{L}(\\sigma^2)$. This is equivalent to maximizing the expression without the constant term $-\\frac{n}{2}\\log(2\\pi)$. Let's define $\\theta = \\sigma^2$ to simplify notation. The objective function becomes:\n$$\n\\mathcal{L}(\\theta) = -\\frac{1}{2}\\log(\\det(K + \\theta I)) - \\frac{1}{2} y^T (K + \\theta I)^{-1} y\n$$\nWe find the maximum by taking the derivative of $\\mathcal{L}(\\theta)$ with respect to $\\theta$ and setting it to zero. This is the First-Order Condition (FOC).\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = -\\frac{1}{2} \\frac{\\partial}{\\partial \\theta} \\left[ \\log(\\det(K + \\theta I)) \\right] - \\frac{1}{2} \\frac{\\partial}{\\partial \\theta} \\left[ y^T (K + \\theta I)^{-1} y \\right] = 0\n$$\nTo evaluate this derivative, we use two standard identities from matrix calculus. For a symmetric matrix $A(t)$ that is a function of a scalar $t$:\n1.  Derivative of the log-determinant: $\\frac{\\partial}{\\partial t} \\log(\\det(A(t))) = \\text{tr}\\left(A(t)^{-1} \\frac{\\partial A(t)}{\\partial t}\\right)$\n2.  Derivative of the inverse: $\\frac{\\partial}{\\partial t} A(t)^{-1} = -A(t)^{-1} \\frac{\\partial A(t)}{\\partial t} A(t)^{-1}$\n\nIn our problem, the matrix is $A(\\theta) = K + \\theta I$. Its derivative with respect to $\\theta$ is:\n$$\n\\frac{\\partial A(\\theta)}{\\partial \\theta} = \\frac{\\partial}{\\partial \\theta}(K + \\theta I) = I\n$$\nsince $K$ is constant with respect to $\\theta$.\n\nNow, we apply the identities to each term of $\\frac{\\partial \\mathcal{L}}{\\partial \\theta}$:\n\nFor the first term (log-determinant):\n$$\n\\frac{\\partial}{\\partial \\theta} \\log(\\det(K + \\theta I)) = \\text{tr}\\left((K + \\theta I)^{-1} \\frac{\\partial(K + \\theta I)}{\\partial \\theta}\\right) = \\text{tr}\\left((K + \\theta I)^{-1} I\\right) = \\text{tr}((K + \\theta I)^{-1})\n$$\n\nFor the second term (quadratic form):\n$$\n\\frac{\\partial}{\\partial \\theta} \\left[ y^T (K + \\theta I)^{-1} y \\right] = y^T \\left( \\frac{\\partial}{\\partial \\theta} (K + \\theta I)^{-1} \\right) y\n$$\nUsing the identity for the derivative of an inverse:\n$$\n\\frac{\\partial}{\\partial \\theta} (K + \\theta I)^{-1} = -(K + \\theta I)^{-1} \\left(\\frac{\\partial(K + \\theta I)}{\\partial \\theta}\\right) (K + \\theta I)^{-1} = -(K + \\theta I)^{-1} I (K + \\theta I)^{-1} = -(K + \\theta I)^{-2}\n$$\nSubstituting this back, the derivative of the quadratic term is:\n$$\ny^T \\left( -(K + \\theta I)^{-2} \\right) y = -y^T (K + \\theta I)^{-2} y\n$$\n\nCombining the derivatives of both terms, we get the derivative of the log marginal likelihood:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = -\\frac{1}{2} \\left[ \\text{tr}((K + \\theta I)^{-1}) \\right] - \\frac{1}{2} \\left[ -y^T (K + \\theta I)^{-2} y \\right]\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\frac{1}{2} \\left[ y^T (K + \\theta I)^{-2} y - \\text{tr}((K + \\theta I)^{-1}) \\right]\n$$\nSetting this derivative to zero gives the FOC for the MLE $\\hat{\\theta} = \\hat{\\sigma}^2$:\n$$\ny^T (K + \\hat{\\sigma}^2 I)^{-2} y = \\text{tr}((K + \\hat{\\sigma}^2 I)^{-1})\n$$\nThis is a nonlinear scalar equation in $\\hat{\\sigma}^2$ that must be solved numerically.\n\n### Part 2: Numerical Solution Strategy\n\nThe FOC requires finding a root $\\hat{\\sigma}^2 > 0$ for the function:\n$$\nf(\\sigma^2) = y^T (K + \\sigma^2 I)^{-2} y - \\text{tr}((K + \\sigma^2 I)^{-1})\n$$\nThe search for a solution must be handled carefully, as the maximizer may lie on the boundary of the valid domain, i.e., $\\hat{\\sigma}^2 = 0$. The behavior of the log-likelihood function $\\mathcal{L}(\\sigma^2)$ is determined by the sign of its derivative, $\\frac{\\partial\\mathcal{L}}{\\partial\\sigma^2} = \\frac{1}{2}f(\\sigma^2)$.\n\nWe analyze the sign of the derivative at the boundary $\\sigma^2 \\to 0^+$:\n$$\n\\lim_{\\sigma^2 \\to 0^+} \\frac{\\partial\\mathcal{L}}{\\partial\\sigma^2} = \\frac{1}{2} \\left[ y^T K^{-2} y - \\text{tr}(K^{-1}) \\right]\n$$\n(This assumes $K$ is invertible, which is true for the SE kernel with distinct inputs).\n\n1.  If $\\frac{\\partial\\mathcal{L}}{\\partial\\sigma^2} \\le 0$ at $\\sigma^2 = 0^+$, the log-likelihood function is initially non-increasing. As $\\mathcal{L}(\\sigma^2) \\to -\\infty$ for large $\\sigma^2$, the function is maximized at the boundary. Thus, the MLE is $\\hat{\\sigma}^2 = 0$.\n\n2.  If $\\frac{\\partial\\mathcal{L}}{\\partial\\sigma^2} > 0$ at $\\sigma^2 = 0^+$, the log-likelihood is initially increasing. Since it must eventually decrease to $-\\infty$, a maximum must exist in the interior of the domain, i.e., at some $\\hat{\\sigma}^2 > 0$. This interior maximum corresponds to a root of the FOC, $f(\\hat{\\sigma}^2)=0$.\n\nThe numerical algorithm is as follows:\n1.  For a given test case, construct the kernel matrix $K$.\n2.  Numerically evaluate the sign of $f(\\sigma^2)$ for a small positive value, e.g., $\\sigma^2 = 10^{-9}$, to approximate the behavior at the boundary.\n3.  If $f(10^{-9}) \\le 0$, the MLE is $\\hat{\\sigma}^2 = 0$.\n4.  If $f(10^{-9}) > 0$, an interior root exists. We can find this root using a numerical solver like `scipy.optimize.brentq`. This requires bracketing the root. We know $f(\\sigma^2)$ is positive near zero. For large $\\sigma^2$, the term $-\\text{tr}((K + \\sigma^2 I)^{-1}) \\approx -n/\\sigma^2$ dominates the term $y^T (K + \\sigma^2 I)^{-2} y \\approx (y^T y)/\\sigma^4$, so $f(\\sigma^2)$ becomes negative. This guarantees that a bracketing interval $[a, b]$ with $f(a) > 0$ and $f(b) < 0$ can be found, allowing the root-finder to converge to the unique interior maximum.\n\nThe following Python code implements this logic.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Computes the Maximum Likelihood Estimate (MLE) for the noise variance sigma^2\n    in a Gaussian Process regression model for several test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"x\": np.array([0.0, 0.25, 0.5, 0.75, 1.0]),\n            \"y\": np.array([0.05, 0.6, 0.9, 0.6, 0.0]),\n            \"alpha_sq\": 1.2,\n            \"ell\": 0.3\n        },\n        {\n            \"x\": np.array([0.0, 0.1, 0.4, 0.8, 1.2]),\n            \"y\": np.array([0.0, 0.0, 0.0, 0.0, 0.0]),\n            \"alpha_sq\": 0.8,\n            \"ell\": 0.5\n        },\n        {\n            \"x\": np.array([0.0, 0.2, 0.5, 0.9, 1.4]),\n            \"y\": np.array([1.0, 0.9, 0.4, -0.2, -0.7]),\n            \"alpha_sq\": 1.5,\n            \"ell\": 0.4\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        mle_sigma_sq = compute_mle_for_case(\n            case[\"x\"], case[\"y\"], case[\"alpha_sq\"], case[\"ell\"]\n        )\n        results.append(f\"{mle_sigma_sq:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\ndef build_se_kernel(x, alpha_sq, ell):\n    \"\"\"Constructs the Squared Exponential kernel matrix K.\"\"\"\n    n = len(x)\n    K = np.zeros((n, n))\n    l_sq = ell**2\n    for i in range(n):\n        for j in range(n):\n            dist_sq = (x[i] - x[j])**2\n            K[i, j] = alpha_sq * np.exp(-0.5 * dist_sq / l_sq)\n    return K\n\ndef foc_function(sigma_sq, K, y_col):\n    \"\"\"\n    Computes the value of the First-Order Condition (FOC) function for a given sigma^2.\n    The function is f(sigma^2) = y^T(K+sigma^2*I)^(-2)y - tr((K+sigma^2*I)^(-1)).\n    The MLE is a root of this function.\n    \n    Args:\n        sigma_sq (float): The noise variance parameter sigma^2.\n        K (np.ndarray): The signal covariance matrix.\n        y_col (np.ndarray): The observation vector (as a column vector).\n\n    Returns:\n        float: The value of the FOC function.\n    \"\"\"\n    if sigma_sq  0:\n        return np.inf  # Enforce non-negativity constraint.\n    \n    n = K.shape[0]\n    I = np.eye(n)\n    K_y = K + sigma_sq * I\n    \n    try:\n        inv_K_y = np.linalg.inv(K_y)\n    except np.linalg.LinAlgError:\n        # If matrix is singular, return a large value to repel the solver.\n        return np.inf\n        \n    # Efficient computation of the quadratic form term\n    # alpha = inv(K_y) @ y\n    # y.T @ inv(K_y)^2 @ y = y.T @ inv(K_y).T @ inv(K_y) @ y = (inv(K_y) @ y).T @ (inv(K_y) @ y)\n    alpha = inv_K_y @ y_col\n    quadratic_term = alpha.T @ alpha\n    \n    # Trace term\n    trace_term = np.trace(inv_K_y)\n    \n    return quadratic_term - trace_term\n\ndef compute_mle_for_case(x, y, alpha_sq, ell):\n    \"\"\"\n    Determines the MLE of sigma^2 for a single case.\n    \n    It first checks the derivative of the log-likelihood at the boundary (sigma^2=0).\n    If it's non-positive, the maximum is at the boundary (sigma^2=0).\n    If it's positive, an interior maximum exists and is found using a numerical root-finder.\n    \"\"\"\n    K = build_se_kernel(x, alpha_sq, ell)\n    y_col = y.reshape(-1, 1)\n\n    # The sign of the FOC function f(sigma^2) is the sign of the derivative of the log-likelihood.\n    # Check the sign at a small positive value to determine behavior at the boundary.\n    epsilon = 1e-9\n    \n    try:\n        foc_at_zero = foc_function(epsilon, K, y_col)\n    except np.linalg.LinAlgError:\n        # This handles the rare case where K itself is singular.\n        # This implies likelihood at sigma^2=0 is -inf, so derivative must be positive.\n        foc_at_zero = np.inf\n\n    # If the derivative is non-positive at the boundary, max is at sigma^2 = 0.\n    if foc_at_zero = 0:\n        return 0.0\n    else:\n        # Interior solution exists. Find a bracketing interval for the root-finder.\n        # The FOC function is positive near zero and becomes negative for large sigma^2.\n        lower_bound = epsilon\n        upper_bound = 1.0\n        \n        # Search for an upper bound where the function is negative.\n        for _ in range(10): # Limit iterations to prevent infinite loop\n            if foc_function(upper_bound, K, y_col)  0:\n                break\n            upper_bound *= 10\n        else:\n            # This should not be reached in a well-behaved problem.\n            # It indicates the derivative doesn't become negative within a reasonable range.\n            # Returning a sentinel value would be appropriate in a general library,\n            # but for this specific problem, the root is expected to be found.\n            # Assume it will be found for these test cases.\n            pass\n\n        # Use Brent's method to find the root in the established bracket.\n        mle_sigma_sq = brentq(foc_function, lower_bound, upper_bound, args=(K, y_col))\n        return mle_sigma_sq\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3423961"}, {"introduction": "Surrogate models are powerful tools, but their performance in real-world applications depends on their robustness to imperfect data. This advanced practice explores how to handle outliers in training data by comparing a standard Gaussian process with a more resilient Student-$t$ process surrogate. By implementing both models and deploying them within a Bayesian inverse problem [@problem_id:3423948], you will learn a practical technique for building robust surrogates and quantify how model choice can significantly impact the accuracy and reliability of posterior inference.", "problem": "You are given a one-dimensional Bayesian inverse problem with an expensive forward model surrogate. The latent parameter is denoted by $\\theta \\in \\mathbb{R}$. The deterministic forward model is $G(\\theta) = \\theta + 0.5 \\sin(3 \\theta)$, and the data model is $y_{\\text{obs}} = G(\\theta_{\\text{true}}) + \\eta$, where $\\eta$ is an additive noise term that is modeled as Gaussian with zero mean and known variance. The prior on $\\theta$ is Gaussian with zero mean and known variance. You will construct two surrogate models for $G(\\theta)$ using simulation data with potential outliers and compare their impact on Bayesian posterior inference, including posterior tails and the robustness of the maximum a posteriori (MAP) estimator.\n\nThe fundamental base for the surrogate modeling is the definition of a Gaussian process prior and the Student-$t$ distribution as a scale mixture of Gaussians. Specifically, a Gaussian process prior on a latent function $f(\\cdot)$ with a squared exponential covariance kernel is defined by the rule that any finite collection of evaluations has a multivariate normal distribution with covariances given by the kernel. The Student-$t$ likelihood can be represented as a Gaussian scale mixture with a Gamma-distributed precision, which is a well-tested representation enabling robust estimation by downweighting outliers.\n\nYou will use the following fixed, known, and shared settings across all test cases:\n- Forward model: $G(\\theta) = \\theta + 0.5 \\sin(3 \\theta)$.\n- Prior on $\\theta$: $\\theta \\sim \\mathcal{N}(0, \\sigma_{\\theta}^{2})$ with $\\sigma_{\\theta} = 2.0$.\n- True parameter: $\\theta_{\\text{true}} = 0.7$.\n- Observed data: $y_{\\text{obs}} = G(\\theta_{\\text{true}})$.\n- Observational noise standard deviation: $\\sigma_{y} = 0.2$.\n- Surrogate input domain for $\\theta$: the closed interval $[-3, 3]$.\n- Training inputs: $n = 15$ equally spaced points in $[-3, 3]$.\n- Kernel for both surrogates: squared exponential $k(\\theta, \\theta') = \\sigma_{f}^{2} \\exp\\!\\left(-\\frac{(\\theta - \\theta')^{2}}{2 \\ell^{2}}\\right)$ with $\\sigma_{f} = 1.0$ and $\\ell = 0.8$.\n- Surrogate training noise scale parameter: $\\sigma_{n} = 0.1$.\n- For the Student-$t$ process surrogate, use degrees of freedom $\\nu = 3.0$ and the Gaussian scale-mixture representation with $\\sigma_{n}$ as above.\n- Posterior over $\\theta$ must be computed on a uniform grid of $1001$ points over $[-3, 3]$ and normalized numerically by Riemann sum. The grid spacing must be used as the integration step for normalization and for computing tail probabilities.\n\nTwo surrogates must be fit on the same training set in each test case:\n- Gaussian process regression surrogate: latent function $f$ with Gaussian likelihood $y \\mid f \\sim \\mathcal{N}(f, \\sigma_{n}^{2} I)$ and zero-mean prior $f \\sim \\mathcal{N}(0, K)$ with covariance matrix entries $K_{ij} = k(\\theta_{i}, \\theta_{j})$.\n- Student-$t$ process surrogate implemented as a Gaussian process prior with Student-$t$ observation noise via the Gaussian scale-mixture representation: $y_{i} \\mid f_{i}, \\lambda_{i} \\sim \\mathcal{N}(f_{i}, \\sigma_{n}^{2} / \\lambda_{i})$ with independent latent precisions $\\lambda_{i} \\sim \\text{Gamma}(\\nu/2, \\nu/2)$, and $f \\sim \\mathcal{N}(0, K)$. The posterior mode under this model can be computed by an expectation-maximization fixed-point iteration where the conditional expectations $\\mathbb{E}[\\lambda_{i} \\mid f, y]$ provide weights that downweight outliers.\n\nFor each fitted surrogate, define the surrogate-induced likelihood for the inverse problem as follows. Let $m(\\theta)$ and $s^{2}(\\theta)$ denote the predictive mean and variance of the latent forward-model value $f(\\theta)$ under the surrogate given the training data. Then the marginal likelihood of the observed data at parameter $\\theta$ is modeled as a Gaussian density $y_{\\text{obs}} \\mid \\theta \\sim \\mathcal{N}(m(\\theta), s^{2}(\\theta) + \\sigma_{y}^{2})$. The posterior density for $\\theta$ on the specified grid is proportional to this likelihood multiplied by the Gaussian prior density, and must be normalized numerically using the grid spacing. The MAP estimator $\\hat{\\theta}_{\\text{MAP}}$ is the grid point with maximal posterior density.\n\nYou must quantify the posterior tail probability for the event $\\{|\\theta|  2.5\\}$ by numerical integration of the normalized posterior over the subset of grid points satisfying $|\\theta|  2.5$. You must report both the posterior tail probability and the MAP estimator for each surrogate and each test case.\n\nTest suite and training data with outliers: In all cases, the noise-free simulator outputs at the $n$ training inputs are $y_{i}^{\\text{sim}} = G(\\theta_{i})$. Outliers are introduced by additive contamination at selected indices. For a given test case with contamination index set $\\mathcal{I}$ and magnitude $A$, the observed training outputs are $y_{i} = y_{i}^{\\text{sim}} + \\delta_{i}$ where $\\delta_{i} = A$ for $i \\in \\mathcal{I}$ and $\\delta_{i} = 0$ otherwise. Use the following four cases to cover a clean scenario, moderate contamination, severe contamination, and a boundary stress case near the true parameter:\n- Case $1$: $\\mathcal{I} = \\{\\}$, $A = 0.0$.\n- Case $2$: $\\mathcal{I} = \\{3, 11\\}$, $A = 4.0$.\n- Case $3$: $\\mathcal{I} = \\{2, 7, 12, 13\\}$, $A = 8.0$.\n- Case $4$: $\\mathcal{I} = \\{9\\}$, $A = 6.0$.\n\nFor each case and for each surrogate, you must compute:\n- The posterior MAP estimator $\\hat{\\theta}_{\\text{MAP}}$.\n- The posterior tail probability $P(|\\theta|  2.5 \\mid y_{\\text{obs}})$.\n- The absolute MAP error $|\\hat{\\theta}_{\\text{MAP}} - \\theta_{\\text{true}}|$.\n\nAdditionally, for each case, define a boolean indicator that is true if the Student-$t$ process surrogate yields a strictly smaller absolute MAP error than the Gaussian process surrogate.\n\nFinal output format: Your program should produce a single line of output containing a single list that aggregates, in order for Cases $1$ through $4$, the following values for each case:\n$[\\hat{\\theta}_{\\text{MAP}}^{\\text{GP}}, \\hat{\\theta}_{\\text{MAP}}^{\\text{T}}, \\text{Tail}^{\\text{GP}}, \\text{Tail}^{\\text{T}}, |\\hat{\\theta}_{\\text{MAP}}^{\\text{GP}} - \\theta_{\\text{true}}|, |\\hat{\\theta}_{\\text{MAP}}^{\\text{T}} - \\theta_{\\text{true}}|, \\text{Improved}]$, where $\\text{Improved}$ is the boolean indicator described above. Concatenate these $7$ values for each of the $4$ cases in order into one flat list, and print it exactly as a comma-separated list enclosed in square brackets, for example $[\\text{result}_{1}, \\ldots, \\text{result}_{28}]$. There are no physical units or angles in this problem, so no unit conversion is required. All floating-point values must be printed in ordinary decimal form without rounding to a specified number of significant figures.", "solution": "The problem requires the comparison of two surrogate models, a standard Gaussian Process (GP) and a robust Student-$t$ Process (tP), within the context of a Bayesian inverse problem. We aim to infer a latent parameter $\\theta$ given an observation $y_{\\text{obs}}$. The forward model $G(\\theta)$ that connects $\\theta$ to the data is computationally expensive, necessitating the use of a surrogate. The robustness of these surrogates is tested against training data contaminated with outliers.\n\nThe core of the inverse problem is Bayes' theorem, which states that the posterior probability density of the parameter $\\theta$ given the observed data $y_{\\text{obs}}$ is proportional to the product of the likelihood and the prior:\n$$\np(\\theta | y_{\\text{obs}}) \\propto p(y_{\\text{obs}} | \\theta) p(\\theta)\n$$\nThe prior on the latent parameter is given as a Gaussian distribution:\n$$\np(\\theta) = \\mathcal{N}(\\theta | 0, \\sigma_{\\theta}^{2})\n$$\nwhere the standard deviation is $\\sigma_{\\theta} = 2.0$.\n\nThe likelihood $p(y_{\\text{obs}} | \\theta)$ depends on the forward model $G(\\theta) = \\theta + 0.5 \\sin(3 \\theta)$. The data-generating process is $y_{\\text{obs}} = G(\\theta_{\\text{true}}) + \\eta$, where $\\theta_{\\text{true}} = 0.7$ and $\\eta$ is a noise term with distribution $\\mathcal{N}(0, \\sigma_{y}^{2})$ and $\\sigma_{y} = 0.2$.\n\nSince $G(\\theta)$ is assumed to be expensive to evaluate, we construct a surrogate model based on a set of $n=15$ training points $\\{\\theta_i, y_i\\}_{i=1}^n$. The training inputs $\\theta_i$ are equally spaced in the interval $[-3, 3]$. The training outputs $y_i$ are generated from the true forward model, potentially with added outliers.\n\nA surrogate model, trained on these data, provides a predictive distribution for the value of the forward model $f(\\theta) \\approx G(\\theta)$ at any new point $\\theta$. This distribution is Gaussian, characterized by a predictive mean $m(\\theta)$ and a predictive variance $s^2(\\theta)$. The uncertainty from the surrogate model is propagated into the likelihood for the inverse problem. The marginal likelihood of $y_{\\text{obs}}$ at a given $\\theta$ is derived by integrating over the surrogate's predictive distribution for $f(\\theta)$:\n$$\np(y_{\\text{obs}} | \\theta) = \\int p(y_{\\text{obs}} | f(\\theta)) p(f(\\theta)|\\text{training data}) df(\\theta)\n$$\nGiven the data model $p(y_{\\text{obs}} | f(\\theta)) = \\mathcal{N}(y_{\\text{obs}} | f(\\theta), \\sigma_{y}^{2})$ and the surrogate's prediction $p(f(\\theta)|\\text{training data}) = \\mathcal{N}(f(\\theta) | m(\\theta), s^2(\\theta))$, the resulting marginal likelihood is:\n$$\np(y_{\\text{obs}} | \\theta) = \\mathcal{N}(y_{\\text{obs}} | m(\\theta), s^2(\\theta) + \\sigma_{y}^{2})\n$$\nWe will now detail the construction of the two surrogate models. Both use a zero-mean Gaussian process prior on the latent function $f$ and a squared exponential kernel:\n$$\nk(\\theta, \\theta') = \\sigma_{f}^{2} \\exp\\!\\left(-\\frac{(\\theta - \\theta')^{2}}{2 \\ell^{2}}\\right)\n$$\nwith hyperparameters $\\sigma_{f} = 1.0$ and $\\ell = 0.8$. Let $K$ be the $n \\times n$ kernel matrix with entries $K_{ij} = k(\\theta_i, \\theta_j)$ for the training inputs.\n\n**1. Gaussian Process (GP) Surrogate**\nThe GP surrogate assumes a Gaussian likelihood for the training data: $y_i \\sim \\mathcal{N}(f(\\theta_i), \\sigma_{n}^{2})$, where $\\sigma_n = 0.1$. The predictive distribution for $f(\\theta_*)$ at a test point $\\theta_*$ is a Gaussian with mean $m_{\\text{GP}}(\\theta_*)$ and variance $s^2_{\\text{GP}}(\\theta_*)$ given by:\n$$\nm_{\\text{GP}}(\\theta_*) = K_*^T (K + \\sigma_{n}^{2}I)^{-1} y_{\\text{train}}\n$$\n$$\ns^2_{\\text{GP}}(\\theta_*) = k(\\theta_*, \\theta_*) - K_*^T (K + \\sigma_{n}^{2}I)^{-1} K_*\n$$\nwhere $y_{\\text{train}}$ is the vector of training outputs, $K_*$ is the vector of kernel covariances between $\\theta_*$ and the training inputs, and $I$ is the identity matrix.\n\n**2. Student-$t$ Process (tP) Surrogate**\nThe tP surrogate provides robustness to outliers by assuming a Student-$t$ distribution for the observation noise. This is implemented using a Gaussian scale-mixture representation. Each observation $y_i$ is modeled as:\n$$\ny_i | f_i, \\lambda_i \\sim \\mathcal{N}(f_i, \\sigma_{n}^{2} / \\lambda_i)\n$$\nwhere each data point has its own latent precision parameter $\\lambda_i$, drawn from a Gamma distribution:\n$$\n\\lambda_i \\sim \\text{Gamma}(\\nu/2, \\nu/2)\n$$\nwith degrees of freedom $\\nu = 3.0$. This hierarchical structure results in marginalizing over $\\lambda_i$ to a Student-$t$ likelihood.\nUnlike the GP case, finding the posterior of $f$ is not analytically tractable. We seek the maximum a posteriori (MAP) estimate of the function values at the training points, $\\hat{f}_{\\text{MAP}}$, using an Expectation-Maximization (EM) algorithm.\n\nThe EM algorithm iteratively updates the latent precisions $\\lambda_i$ and the function estimate $f$. Given a current estimate $f^{(t)}$:\n- **E-Step**: Compute the expected value of each precision $\\lambda_i$ given $f^{(t)}$ and the data. The posterior for $\\lambda_i$ is a Gamma distribution, and its expectation is:\n$$\n\\hat{\\lambda}_i^{(t+1)} = \\mathbb{E}[\\lambda_i | y_i, f^{(t)}_i] = \\frac{\\nu + 1}{\\nu + (y_i - f^{(t)}_i)^2 / \\sigma_{n}^{2}}\n$$\nOutliers (large residuals $|y_i - f^{(t)}_i|$) will result in small values of $\\hat{\\lambda}_i$, effectively down-weighting their influence.\n- **M-Step**: Update the function estimate to $f^{(t+1)}$ by maximizing the expected log-posterior, which is equivalent to solving a weighted GP regression problem where the noise variance for point $i$ is $\\sigma_{n}^{2} / \\hat{\\lambda}_i^{(t+1)}$. The MAP estimate is:\n$$\nf^{(t+1)} = K(K + \\sigma_{n}^{2} \\text{diag}(\\hat{\\lambda}^{(t+1)})^{-1})^{-1} y_{\\text{train}}\n$$\nThe algorithm is iterated until convergence of $f^{(t)}$. After convergence, we obtain the final weights $\\hat{\\lambda}$ and use them to define the predictive mean $m_{\\text{tP}}(\\theta_*)$ and variance $s^2_{\\text{tP}}(\\theta_*)$:\n$$\nm_{\\text{tP}}(\\theta_*) = K_*^T (K + \\sigma_{n}^{2} \\text{diag}(\\hat{\\lambda})^{-1})^{-1} y_{\\text{train}}\n$$\n$$\ns^2_{\\text{tP}}(\\theta_*) = k(\\theta_*, \\theta_*) - K_*^T (K + \\sigma_{n}^{2} \\text{diag}(\\hat{\\lambda})^{-1})^{-1} K_*\n$$\n\n**Posterior Inference and Analysis**\nFor each surrogate (GP and tP), the posterior density for $\\theta$ is computed on a grid of $1001$ points spanning $[-3, 3]$. The unnormalized posterior is $\\tilde{p}(\\theta_j) = p(y_{\\text{obs}} | \\theta_j) p(\\theta_j)$ for each grid point $\\theta_j$. The posterior is normalized using a Riemann sum: $Z = \\sum_j \\tilde{p}(\\theta_j) \\Delta\\theta$, where $\\Delta\\theta$ is the grid spacing. The normalized posterior is $p(\\theta_j) = \\tilde{p}(\\theta_j) / Z$.\n\nFrom the normalized posterior, we calculate:\n- The MAP estimator $\\hat{\\theta}_{\\text{MAP}}$, which is the grid point maximizing $p(\\theta_j)$.\n- The absolute MAP error $|\\hat{\\theta}_{\\text{MAP}} - \\theta_{\\text{true}}|$.\n- The posterior tail probability $P(|\\theta|  2.5 | y_{\\text{obs}})$, computed by summing $p(\\theta_j) \\Delta\\theta$ over all grid points where $|\\theta_j|  2.5$.\n\nThis entire procedure is repeated for four test cases, each with a different set of outliers added to the training data, allowing for a quantitative comparison of the robustness and performance of the GP and tP surrogate models. An indicator variable will report whether the tP model yields a strictly smaller MAP error than the GP model.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky, cho_solve\nfrom scipy.stats import norm\n\n# Set a fixed random seed for reproducibility, although not strictly necessary as there's no sampling.\nnp.random.seed(42)\n\ndef solve():\n    # --- Problem Constants and Setup ---\n    # Prior on theta\n    sigma_theta = 2.0\n    \n    # True parameter and observation\n    theta_true = 0.7\n    \n    # Forward model\n    def G(theta):\n        return theta + 0.5 * np.sin(3.0 * theta)\n\n    y_obs = G(theta_true)\n    sigma_y = 0.2\n    \n    # Surrogate model parameters\n    theta_domain = [-3.0, 3.0]\n    n_train = 15\n    sigma_f = 1.0\n    length_scale = 0.8\n    sigma_n = 0.1\n    \n    # Student-t process parameters\n    nu = 3.0\n    \n    # Posterior computation grid\n    n_grid = 1001\n    \n    # Training and posterior evaluation grids\n    theta_train = np.linspace(theta_domain[0], theta_domain[1], n_train)\n    theta_grid = np.linspace(theta_domain[0], theta_domain[1], n_grid)\n    grid_spacing = theta_grid[1] - theta_grid[0]\n\n    # --- Kernel Function ---\n    def kernel(t1, t2, sf, l):\n        t1_col = t1[:, np.newaxis]\n        t2_row = t2[np.newaxis, :]\n        sqdist = (t1_col - t2_row)**2\n        return sf**2 * np.exp(-0.5 * sqdist / l**2)\n\n    # Pre-compute kernel matrices\n    K_train_train = kernel(theta_train, theta_train, sigma_f, length_scale)\n    K_grid_train = kernel(theta_grid, theta_train, sigma_f, length_scale)\n    k_diag_grid = np.full(n_grid, sigma_f**2)\n\n    # --- Test Cases ---\n    test_cases = [\n        {'I': [], 'A': 0.0},                  # Case 1: Clean data\n        {'I': [3, 11], 'A': 4.0},             # Case 2: Moderate contamination\n        {'I': [2, 7, 12, 13], 'A': 8.0},      # Case 3: Severe contamination\n        {'I': [9], 'A': 6.0}                  # Case 4: Boundary stress case\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        # --- Generate Training Data for the Case ---\n        y_sim = G(theta_train)\n        y_train = y_sim.copy()\n        if case['I']:\n            y_train[case['I']] += case['A']\n        \n        # --- 1. Gaussian Process (GP) Surrogate ---\n        # Covariance matrix with noise\n        C_gp = K_train_train + np.eye(n_train) * sigma_n**2\n        L_gp = cholesky(C_gp, lower=True)\n        \n        # Predictive mean\n        alpha_gp = cho_solve((L_gp, True), y_train)\n        m_gp = K_grid_train @ alpha_gp\n        \n        # Predictive variance\n        V_gp = cho_solve((L_gp, True), K_grid_train.T)\n        s2_gp = k_diag_grid - np.sum(K_grid_train * V_gp.T, axis=1)\n\n        # --- 2. Student-t Process (tP) Surrogate ---\n        # EM Algorithm\n        max_iter = 100\n        tol = 1e-6\n        \n        # Initialize f with y_train\n        f_t = y_train.copy()\n        \n        for _ in range(max_iter):\n            # E-step: Compute weights\n            residuals_sq = (y_train - f_t)**2\n            lambdas = (nu + 1.0) / (nu + residuals_sq / sigma_n**2)\n            \n            # M-step: Update f\n            D_noise_tP = np.diag(sigma_n**2 / lambdas)\n            C_tP_iter = K_train_train + D_noise_tP\n            L_tP_iter = cholesky(C_tP_iter, lower=True)\n            alpha_tP_iter = cho_solve((L_tP_iter, True), y_train)\n            f_new = K_train_train @ alpha_tP_iter\n            \n            # Check for convergence\n            if np.linalg.norm(f_new - f_t) / np.linalg.norm(f_t)  tol:\n                break\n            f_t = f_new\n        \n        # Final weights and covariance for prediction\n        final_lambdas = (nu + 1.0) / (nu + (y_train - f_t)**2 / sigma_n**2)\n        D_noise_tP_final = np.diag(sigma_n**2 / final_lambdas)\n        C_tP = K_train_train + D_noise_tP_final\n        L_tP = cholesky(C_tP, lower=True)\n\n        # Predictive mean\n        alpha_tP = cho_solve((L_tP, True), y_train)\n        m_tP = K_grid_train @ alpha_tP\n        \n        # Predictive variance\n        V_tP = cho_solve((L_tP, True), K_grid_train.T)\n        s2_tP = k_diag_grid - np.sum(K_grid_train * V_tP.T, axis=1)\n\n        # --- 3. Posterior Inference and Analysis ---\n        def compute_posterior_stats(m, s2):\n            # Surrogate-induced likelihood\n            log_likelihood = norm.logpdf(y_obs, loc=m, scale=np.sqrt(s2 + sigma_y**2))\n            \n            # Prior\n            log_prior = norm.logpdf(theta_grid, loc=0, scale=sigma_theta)\n            \n            # Unnormalized log posterior\n            log_posterior = log_likelihood + log_prior\n            \n            # Normalize posterior (avoiding underflow by shifting)\n            posterior = np.exp(log_posterior - np.max(log_posterior))\n            Z = np.sum(posterior) * grid_spacing\n            normalized_posterior = posterior / Z\n            \n            # MAP estimator\n            map_idx = np.argmax(normalized_posterior)\n            theta_map = theta_grid[map_idx]\n            \n            # Tail probability\n            tail_indices = np.abs(theta_grid) > 2.5\n            tail_prob = np.sum(normalized_posterior[tail_indices]) * grid_spacing\n            \n            return theta_map, tail_prob\n\n        # Calculate stats for both surrogates\n        theta_map_gp, tail_prob_gp = compute_posterior_stats(m_gp, s2_gp)\n        theta_map_tP, tail_prob_tP = compute_posterior_stats(m_tP, s2_tP)\n        \n        # Absolute MAP errors\n        error_gp = abs(theta_map_gp - theta_true)\n        error_tP = abs(theta_map_tP - theta_true)\n        \n        # Improvement indicator\n        improved = error_tP  error_gp\n        \n        all_results.extend([\n            theta_map_gp, theta_map_tP,\n            tail_prob_gp, tail_prob_tP,\n            error_gp, error_tP,\n            improved\n        ])\n\n    # --- Final Output ---\n    # Convert bools to strings 'True'/'False' and format numbers\n    formatted_results = [str(r) for r in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3423948"}]}