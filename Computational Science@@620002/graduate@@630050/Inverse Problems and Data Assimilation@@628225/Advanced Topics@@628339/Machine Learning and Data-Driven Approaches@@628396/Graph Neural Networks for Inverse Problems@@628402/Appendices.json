{"hands_on_practices": [{"introduction": "This first practice grounds the abstract concept of a Graph Neural Network in a tangible physical system. By modeling a simple electrical circuit as a graph, you will derive the forward map—the function that predicts observations from system parameters. The core task is to compute the Jacobian of this map, a fundamental skill required for training any GNN with gradient-based optimization and for linearizing models in data assimilation [@problem_id:3386851].", "problem": "Consider a one-dimensional ($1$D) chain graph with nodes indexed $i \\in \\{0,1,\\dots,N\\}$ and edges $(i,i+1)$ for $i \\in \\{0,1,\\dots,N-1\\}$. Each edge $(i,i+1)$ has a strictly positive conductance $w_{i,i+1} \\in \\mathbb{R}_{>0}$. Define the boundary (Dirichlet) potentials by $v_0 = V_L$ and $v_N = V_R$, where $V_L, V_R \\in \\mathbb{R}$ are fixed and known. Interior node voltages $\\{v_i\\}_{i=1}^{N-1}$ are determined by Kirchhoff’s current law and Ohm’s law on the graph. Let the input parameterization be the log-conductances $x \\in \\mathbb{R}^N$ with components $x_i = \\ln w_{i,i+1}$ for $i \\in \\{0,1,\\dots,N-1\\}$, and define the output $y \\in \\mathbb{R}^2$ as the boundary current vector $y = [I_0, I_N]^{\\top}$, where $I_0$ is the net current injected at node $0$ (positive into the network) and $I_N$ is the net current injected at node $N$ (positive into the network). This defines a forward map $y = H(x)$, which is the graph-theoretic Dirichlet-to-Neumann map for this chain.\n\nYou are designing a Graph Neural Network (GNN) surrogate for $H(x)$ as part of an inverse problem in data assimilation, where gradients with respect to $x$ are required for backpropagation and for Gauss–Newton regularized inversion. Starting only from the core definitions of Ohm’s law on edges, Kirchhoff’s current law at nodes, and the positivity enforced by the log-parameterization, derive the discrete forward map $y = H(x)$ explicitly in terms of $\\{w_{i,i+1}\\}$ and $\\{V_L,V_R\\}$, and then compute the Jacobian $\\frac{\\partial y}{\\partial x} \\in \\mathbb{R}^{2 \\times N}$ in closed form at an arbitrary $x$. Express your final Jacobian using only $\\{w_{i,i+1}\\}_{i=0}^{N-1}$, $V_L$, and $V_R$.\n\nGive your final Jacobian as a single closed-form analytic expression. Do not approximate. No units are required. Angles are not involved. The final answer must be the requested expression only.", "solution": "The problem statement is first validated against the required criteria.\n\n### Problem Validation\n\n#### Step 1: Extract Givens\n-   **Graph Structure:** A one-dimensional ($1$D) chain graph with nodes indexed $i \\in \\{0,1,\\dots,N\\}$ and edges $(i,i+1)$ for $i \\in \\{0,1,\\dots,N-1\\}$.\n-   **Edge Property:** Each edge $(i,i+1)$ has a strictly positive conductance $w_{i,i+1} \\in \\mathbb{R}_{>0}$.\n-   **Boundary Conditions:** Dirichlet boundary potentials are specified as $v_0 = V_L$ and $v_N = V_R$, with $V_L, V_R \\in \\mathbb{R}$ being fixed and known.\n-   **Interior Node Physics:** The interior node voltages $\\{v_i\\}_{i=1}^{N-1}$ are governed by Kirchhoff’s current law and Ohm’s law.\n-   **Input Parameterization:** The input parameters are the log-conductances $x \\in \\mathbb{R}^N$ with components $x_i = \\ln w_{i,i+1}$ for $i \\in \\{0,1,\\dots,N-1\\}$.\n-   **Output Definition:** The output is the boundary current vector $y = [I_0, I_N]^{\\top} \\in \\mathbb{R}^2$.\n-   **Current Definitions:** $I_0$ is the net current injected at node $0$ (positive into the network), and $I_N$ is the net current injected at node $N$ (positive into the network).\n-   **Forward Map:** The relationship between input and output is denoted by the map $y = H(x)$.\n-   **Objective:**\n    1.  Derive the explicit forward map $y = H(x)$ in terms of $\\{w_{i,i+1}\\}$ and $\\{V_L, V_R\\}$.\n    2.  Compute the Jacobian matrix $\\frac{\\partial y}{\\partial x} \\in \\mathbb{R}^{2 \\times N}$ in closed form.\n    3.  Express the final Jacobian using only $\\{w_{i,i+1}\\}_{i=0}^{N-1}$, $V_L$, and $V_R$.\n\n#### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded:** The problem is based on fundamental principles of electrical circuits, namely Ohm's law and Kirchhoff's current law. These are well-established laws of physics. The log-parameterization is a standard and valid technique in optimization and inverse problems to enforce positivity constraints. The problem is scientifically sound.\n-   **Well-Posed:** The problem describes a linear resistive network with fixed Dirichlet boundary conditions. Since all conductances $w_{i,i+1}$ are strictly positive, the resistances $1/w_{i,i+1}$ are finite and positive. This guarantees that the system of linear equations for the interior node potentials has a unique solution. Consequently, the forward map $H(x)$ is well-defined, and its Jacobian is computable. The problem is well-posed.\n-   **Objective:** The problem is stated using precise mathematical language, with all terms clearly defined. It is free from subjective or ambiguous statements.\n\nThe problem does not violate any of the invalidity criteria. It is a standard, well-posed problem in mathematical physics and is directly relevant to computational methods in data assimilation and machine learning.\n\n#### Step 3: Verdict and Action\nThe problem is valid. The solution process will proceed.\n\n### Solution Derivation\n\nThe solution is derived in two parts: first, the forward map $y=H(x)$ is determined, and second, its Jacobian $\\frac{\\partial y}{\\partial x}$ is computed.\n\n#### Part 1: Derivation of the Forward Map $y = H(x)$\n\nThe electrical network is a simple $1$D series circuit. According to Ohm's law, the current flowing from node $i$ to node $i+1$ is given by $I_{i \\to i+1} = w_{i,i+1} (v_i - v_{i+1})$, where $v_i$ is the voltage at node $i$ and $w_{i,i+1}$ is the conductance of the edge $(i, i+1)$.\n\nFor a $1$D chain with sources only at the boundaries (nodes $0$ and $N$), Kirchhoff's current law at any interior node $j \\in \\{1, \\dots, N-1\\}$ implies that the current flowing in equals the current flowing out. This means the current is constant throughout the entire chain. Let this constant current be denoted by $I_{\\text{chain}}$.\n$$ I_{\\text{chain}} = I_{0 \\to 1} = I_{1 \\to 2} = \\dots = I_{N-1 \\to N} $$\nThe total voltage drop across the chain is $v_0 - v_N = V_L - V_R$.\nThe resistance of an individual edge $(i, i+1)$ is the reciprocal of its conductance: $R_{i,i+1} = 1/w_{i,i+1}$.\nThe total resistance of the series circuit, $R_{\\text{total}}$, is the sum of the individual resistances:\n$$ R_{\\text{total}} = \\sum_{k=0}^{N-1} R_{k,k+1} = \\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}} $$\nThe constant chain current $I_{\\text{chain}}$ can now be found using Ohm's law for the entire circuit:\n$$ I_{\\text{chain}} = \\frac{V_L - V_R}{R_{\\text{total}}} = \\frac{V_L - V_R}{\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}} $$\nThe output vector $y = [I_0, I_N]^{\\top}$ consists of the boundary currents.\n-   $I_0$ is the net current injected at node $0$. This current must flow into the chain, so it is equal to the current flowing from node $0$ to node $1$: $I_0 = I_{0 \\to 1} = I_{\\text{chain}}$.\n-   $I_N$ is the net current injected at node $N$. The current flowing from the chain into node $N$ is $I_{N-1 \\to N} = I_{\\text{chain}}$. By the convention \"positive into the network\", $I_N$ must be the negative of the current exiting the network at node $N$. The current exiting the network is $I_{\\text{chain}}$, so $I_N = -I_{\\text{chain}}$.\n\nThus, the forward map is:\n$$ y = H(x) = \\begin{pmatrix} I_0 \\\\ I_N \\end{pmatrix} = \\begin{pmatrix} I_{\\text{chain}} \\\\ -I_{\\text{chain}} \\end{pmatrix} = \\frac{V_L - V_R}{\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\nThis expression gives the output $y$ in terms of the conductances $\\{w_{i,i+1}\\}$.\n\n#### Part 2: Computation of the Jacobian $\\frac{\\partial y}{\\partial x}$\n\nThe Jacobian of $y$ with respect to $x$ is a $2 \\times N$ matrix whose elements are $\\frac{\\partial y_m}{\\partial x_j}$, where $y_m$ is the $m$-th component of $y$ ($m \\in \\{1,2\\}$) and $x_j$ is the $j$-th component of $x$ for $j \\in \\{0, \\dots, N-1\\}$. The components are $y_1 = I_0$ and $y_2 = I_N$.\n\nGiven that $I_N = -I_0$, the second row of the Jacobian is simply the negative of the first row: $\\frac{\\partial I_N}{\\partial x_j} = -\\frac{\\partial I_0}{\\partial x_j}$. We only need to compute the first row, which corresponds to the derivatives $\\frac{\\partial I_0}{\\partial x_j}$.\n\nWe use the chain rule to differentiate $I_0$ with respect to $x_j$:\n$$ \\frac{\\partial I_0}{\\partial x_j} = \\frac{\\partial I_0}{\\partial w_{j,j+1}} \\frac{\\partial w_{j,j+1}}{\\partial x_j} $$\nThe problem defines the parameterization $x_j = \\ln w_{j,j+1}$, which implies $w_{j,j+1} = \\exp(x_j)$. Therefore, the derivative is:\n$$ \\frac{\\partial w_{j,j+1}}{\\partial x_j} = \\exp(x_j) = w_{j,j+1} $$\nNext, we compute the derivative of $I_0$ with respect to an arbitrary conductance $w_{j,j+1}$. Let $S = \\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}$. Then $I_0 = (V_L - V_R)S^{-1}$.\n$$ \\frac{\\partial I_0}{\\partial w_{j,j+1}} = (V_L - V_R) \\frac{d(S^{-1})}{dS} \\frac{\\partial S}{\\partial w_{j,j+1}} $$\nThe derivatives are:\n$$ \\frac{d(S^{-1})}{dS} = -S^{-2} $$\n$$ \\frac{\\partial S}{\\partial w_{j,j+1}} = \\frac{\\partial}{\\partial w_{j,j+1}} \\left( \\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}} \\right) = -\\frac{1}{w_{j,j+1}^2} $$\nCombining these results:\n$$ \\frac{\\partial I_0}{\\partial w_{j,j+1}} = (V_L - V_R) (-S^{-2}) \\left( -\\frac{1}{w_{j,j+1}^2} \\right) = \\frac{V_L - V_R}{S^2 w_{j,j+1}^2} = \\frac{V_L - V_R}{\\left(\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}\\right)^2 w_{j,j+1}^2} $$\nNow we can find $\\frac{\\partial I_0}{\\partial x_j}$:\n$$ \\frac{\\partial I_0}{\\partial x_j} = \\frac{\\partial I_0}{\\partial w_{j,j+1}} \\frac{\\partial w_{j,j+1}}{\\partial x_j} = \\left( \\frac{V_L - V_R}{\\left(\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}\\right)^2 w_{j,j+1}^2} \\right) w_{j,j+1} = \\frac{V_L - V_R}{w_{j,j+1} \\left(\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}\\right)^2} $$\nThis expression is the $(1, j+1)$-th entry of the Jacobian matrix (using $j$ from $0$ to $N-1$ to index columns). The full Jacobian matrix $\\frac{\\partial y}{\\partial x}$ is then constructed by assembling these components for $j=0, 1, \\dots, N-1$:\n$$ \\frac{\\partial y}{\\partial x} = \\begin{pmatrix} \\frac{\\partial I_0}{\\partial x_0} & \\frac{\\partial I_0}{\\partial x_1} & \\cdots & \\frac{\\partial I_0}{\\partial x_{N-1}} \\\\ \\frac{\\partial I_N}{\\partial x_0} & \\frac{\\partial I_N}{\\partial x_1} & \\cdots & \\frac{\\partial I_N}{\\partial x_{N-1}} \\end{pmatrix} $$\nSubstituting the derived expressions:\n$$ \\frac{\\partial y}{\\partial x} = \\frac{V_L - V_R}{\\left(\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}\\right)^2} \\begin{pmatrix} \\frac{1}{w_{0,1}} & \\frac{1}{w_{1,2}} & \\cdots & \\frac{1}{w_{N-1,N}} \\\\ -\\frac{1}{w_{0,1}} & -\\frac{1}{w_{1,2}} & \\cdots & -\\frac{1}{w_{N-1,N}} \\end{pmatrix} $$\nThis is the final closed-form expression for the Jacobian, expressed in terms of the required variables.", "answer": "$$ \\boxed{\\frac{V_L - V_R}{\\left(\\sum_{k=0}^{N-1} \\frac{1}{w_{k,k+1}}\\right)^2} \\begin{pmatrix} \\frac{1}{w_{0,1}} & \\frac{1}{w_{1,2}} & \\cdots & \\frac{1}{w_{N-1,N}} \\\\ -\\frac{1}{w_{0,1}} & -\\frac{1}{w_{1,2}} & \\cdots & -\\frac{1}{w_{N-1,N}} \\end{pmatrix}} $$", "id": "3386851"}, {"introduction": "Many advanced GNNs for inverse problems are designed as \"unrolled\" iterative solvers, where each network layer corresponds to one step of an optimization algorithm. This practice delves into the crucial topic of numerical stability for such architectures. You will derive the convergence condition for a learned gradient-descent-like iteration, connecting the learnable step size to the spectral properties of the underlying graph operator to ensure the solver is well-behaved and converges correctly [@problem_id:3386845].", "problem": "In a learned unrolled Graph Neural Network (GNN) for a linear inverse problem on a graph, consider the data-fidelity function $f(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{H}\\mathbf{x} - \\mathbf{y}\\|_{2}^{2}$ with $\\mathbf{H} \\in \\mathbb{R}^{n \\times n}$ symmetric positive definite. One layer of the unrolled architecture implements the affine iteration $\\mathbf{x}_{k+1} = (\\mathbf{I} - \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{H})\\mathbf{x}_{k} + \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{y}$, where $\\alpha > 0$ is a constant step size. Using only the spectral theorem for real symmetric matrices, the definition of the spectral radius, and basic properties of graph Laplacians, derive the necessary and sufficient condition on $\\alpha$ such that the linear operator $\\mathbf{T}(\\alpha) = \\mathbf{I} - \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{H}$ is a strict contraction in the Euclidean norm.\n\nSpecialize to the case $n = 3$ where $\\mathbf{H} = \\mathbf{I} + \\mathbf{L}$ and $\\mathbf{L}$ is the combinatorial Laplacian of the path graph on $3$ nodes with unit edge weights:\n$$\n\\mathbf{L} = \\begin{pmatrix}\n1 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 1\n\\end{pmatrix}.\n$$\nCompute the largest admissible constant step size $\\alpha$ that guarantees $\\rho(\\mathbf{I} - \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{H}) < 1$. Report the final $\\alpha$ as an exact number. Do not round.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of numerical linear algebra and optimization, specifically the convergence analysis of iterative methods for solving linear systems. The problem is well-posed, objective, and contains all necessary information for a unique solution. The premises are consistent; the specified matrix $\\mathbf{H} = \\mathbf{I} + \\mathbf{L}$ is indeed symmetric and positive definite, as the combinatorial Laplacian $\\mathbf{L}$ is symmetric and positive semidefinite, meaning its eigenvalues $\\lambda_i(\\mathbf{L}) \\ge 0$, which in turn implies that the eigenvalues of $\\mathbf{H}$, given by $1 + \\lambda_i(\\mathbf{L})$, are all greater than or equal to $1$, thus positive.\n\nThe problem asks for two main results: first, a general condition on the step size $\\alpha$ for an iterative method to converge, and second, the computation of the maximum step size for a specific case.\n\nFirst, we derive the necessary and sufficient condition for the iteration to be a strict contraction. The iterative scheme is given by\n$$\n\\mathbf{x}_{k+1} = (\\mathbf{I} - \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{H})\\mathbf{x}_{k} + \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{y}\n$$\nThis is an affine iteration of the form $\\mathbf{x}_{k+1} = \\mathbf{T}\\mathbf{x}_k + \\mathbf{c}$, where the iteration matrix is $\\mathbf{T}(\\alpha) = \\mathbf{I} - \\alpha\\,\\mathbf{H}^{\\top}\\mathbf{H}$. The iteration converges to a unique fixed point for any starting vector $\\mathbf{x}_0$ if and only if the operator $\\mathbf{T}(\\alpha)$ is a strict contraction, which in a finite-dimensional vector space is equivalent to its spectral radius being less than $1$. That is, $\\rho(\\mathbf{T}(\\alpha)) < 1$.\n\nThe spectral radius of a matrix is the maximum of the absolute values of its eigenvalues. Let $\\mu_i$ be the eigenvalues of $\\mathbf{T}(\\alpha)$. The condition is $\\max_i |\\mu_i| < 1$.\n\nThe eigenvalues of $\\mathbf{T}(\\alpha) = \\mathbf{I} - \\alpha\\mathbf{H}^{\\top}\\mathbf{H}$ are related to the eigenvalues of $\\mathbf{H}^{\\top}\\mathbf{H}$. Let $\\nu_i$ be the eigenvalues of $\\mathbf{H}^{\\top}\\mathbf{H}$. Then, the eigenvalues of $\\mathbf{T}(\\alpha)$ are $\\mu_i = 1 - \\alpha\\nu_i$.\n\nThe problem states that $\\mathbf{H}$ is a real symmetric matrix. Therefore, $\\mathbf{H}^{\\top} = \\mathbf{H}$, and the matrix $\\mathbf{H}^{\\top}\\mathbf{H}$ simplifies to $\\mathbf{H}^2$. Let $\\lambda_i$ be the eigenvalues of $\\mathbf{H}$. The eigenvalues of $\\mathbf{H}^2$ are then $\\nu_i = \\lambda_i^2$. Because $\\mathbf{H}$ is also positive definite, all its eigenvalues $\\lambda_i$ are strictly positive real numbers. Consequently, the eigenvalues $\\nu_i = \\lambda_i^2$ of $\\mathbf{H}^2$ are also strictly positive.\n\nThe convergence condition $\\rho(\\mathbf{T}(\\alpha)) < 1$ becomes $\\max_i |1 - \\alpha \\lambda_i^2| < 1$. This must hold for all eigenvalues $\\lambda_i$ of $\\mathbf{H}$. This inequality is equivalent to:\n$$\n-1 < 1 - \\alpha \\lambda_i^2 < 1\n$$\nWe analyze the two inequalities separately for any given eigenvalue $\\lambda_i > 0$.\n1.  $1 - \\alpha \\lambda_i^2 < 1 \\implies -\\alpha \\lambda_i^2 < 0$. Since $\\alpha > 0$ is given and $\\lambda_i^2 > 0$, this inequality is always satisfied.\n2.  $-1 < 1 - \\alpha \\lambda_i^2 \\implies 2 > \\alpha \\lambda_i^2 \\implies \\alpha < \\frac{2}{\\lambda_i^2}$.\n\nThis condition must be true for all eigenvalues $\\lambda_i$ of $\\mathbf{H}$. To ensure this, $\\alpha$ must be smaller than the minimum of all the upper bounds $\\frac{2}{\\lambda_i^2}$.\n$$\n\\alpha < \\min_i \\left(\\frac{2}{\\lambda_i^2}\\right) = \\frac{2}{\\max_i (\\lambda_i^2)} = \\frac{2}{(\\max_i \\lambda_i)^2}\n$$\nSince for a positive definite matrix $\\max_i \\lambda_i = \\lambda_{\\max}(\\mathbf{H})$, which is also its spectral radius $\\rho(\\mathbf{H})$, the necessary and sufficient condition on $\\alpha$ is:\n$$\n0 < \\alpha < \\frac{2}{(\\lambda_{\\max}(\\mathbf{H}))^2}\n$$\n\nNext, we specialize to the case where $n=3$ and $\\mathbf{H} = \\mathbf{I} + \\mathbf{L}$, with $\\mathbf{L}$ being the given graph Laplacian matrix:\n$$\n\\mathbf{L} = \\begin{pmatrix} 1 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 1 \\end{pmatrix}\n$$\nThe eigenvalues of $\\mathbf{H}$ are given by $\\lambda_i(\\mathbf{H}) = 1 + \\lambda_i(\\mathbf{L})$, where $\\lambda_i(\\mathbf{L})$ are the eigenvalues of $\\mathbf{L}$. We find the eigenvalues of $\\mathbf{L}$ by solving the characteristic equation $\\det(\\mathbf{L} - \\lambda \\mathbf{I}) = 0$.\n$$\n\\det \\begin{pmatrix} 1-\\lambda & -1 & 0 \\\\ -1 & 2-\\lambda & -1 \\\\ 0 & -1 & 1-\\lambda \\end{pmatrix} = 0\n$$\nExpanding the determinant along the first row:\n$$\n(1-\\lambda) \\det \\begin{pmatrix} 2-\\lambda & -1 \\\\ -1 & 1-\\lambda \\end{pmatrix} - (-1) \\det \\begin{pmatrix} -1 & -1 \\\\ 0 & 1-\\lambda \\end{pmatrix} = 0\n$$\n$$\n(1-\\lambda)[(2-\\lambda)(1-\\lambda) - 1] + [(-1)(1-\\lambda) - 0] = 0\n$$\n$$\n(1-\\lambda)[\\lambda^2 - 3\\lambda + 2 - 1] - (1-\\lambda) = 0\n$$\n$$\n(1-\\lambda)[\\lambda^2 - 3\\lambda + 1] - (1-\\lambda) = 0\n$$\nFactoring out $(1-\\lambda)$:\n$$\n(1-\\lambda)[(\\lambda^2 - 3\\lambda + 1) - 1] = 0\n$$\n$$\n(1-\\lambda)(\\lambda^2 - 3\\lambda) = 0\n$$\n$$\n\\lambda(1-\\lambda)(\\lambda-3) = 0\n$$\nThe eigenvalues of $\\mathbf{L}$ are $\\lambda(\\mathbf{L}) \\in \\{0, 1, 3\\}$.\n\nThe eigenvalues of $\\mathbf{H} = \\mathbf{I} + \\mathbf{L}$ are therefore:\n$$\n\\lambda_1(\\mathbf{H}) = 1 + 0 = 1\n$$\n$$\n\\lambda_2(\\mathbf{H}) = 1 + 1 = 2\n$$\n$$\n\\lambda_3(\\mathbf{H}) = 1 + 3 = 4\n$$\nThe maximum eigenvalue of $\\mathbf{H}$ is $\\lambda_{\\max}(\\mathbf{H}) = 4$.\n\nFinally, we compute the largest admissible step size $\\alpha$. Using the derived condition, the interval of admissible values for $\\alpha$ is:\n$$\n0 < \\alpha < \\frac{2}{(\\lambda_{\\max}(\\mathbf{H}))^2} = \\frac{2}{4^2} = \\frac{2}{16} = \\frac{1}{8}\n$$\nThe problem asks for the largest admissible constant step size $\\alpha$ that guarantees convergence. This value is the supremum of the open interval $(0, \\frac{1}{8})$. Any step size $\\alpha$ within this interval will result in a strict contraction, while at the boundary value $\\alpha = \\frac{1}{8}$, the spectral radius of the iteration matrix becomes $\\rho(\\mathbf{T}(\\frac{1}{8}))=1$, which does not guarantee convergence to the correct solution for all initial vectors. The largest value that *guarantees* the strict inequality $\\rho < 1$ is the limit of the interval. Therefore, the largest admissible value is the supremum.\n\nThe largest admissible constant step size is $\\frac{1}{8}$.", "answer": "$$\\boxed{\\frac{1}{8}}$$", "id": "3386845"}, {"introduction": "This practice applies the principles of GNN-based modeling to a dynamic inverse problem using the framework of data assimilation. Given a forecast state and covariance, which could be the output of a GNN-based dynamics model, you will perform the \"analysis\" step of a Kalman filter. This involves computing the Kalman gain and updating the state estimate to incorporate new, noisy observations from the system, a core task in sequential state estimation [@problem_id:3386871].", "problem": "Consider a dynamic inverse problem on a graph, where the latent state $x_t \\in \\mathbb{R}^3$ is defined on the nodes of a path graph with three nodes and edges $\\{(1,2),(2,3)\\}$. A Graph Neural Network (GNN) prior induces a linearized one-step rollout operator $F \\in \\mathbb{R}^{3 \\times 3}$ and a process noise covariance $Q \\in \\mathbb{R}^{3 \\times 3}$, modeled here using the graph Laplacian $L \\in \\mathbb{R}^{3 \\times 3}$ with \n$$\nL \\;=\\; \\begin{bmatrix}\n1 & -1 & 0 \\\\\n-1 & 2 & -1 \\\\\n0 & -1 & 1\n\\end{bmatrix}, \\qquad \nF \\;=\\; I - \\alpha L \\;\\;\\text{with}\\;\\; \\alpha \\;=\\; \\frac{1}{2}, \\qquad \nQ \\;=\\; \\sigma_q^2 I,\\;\\; \\sigma_q^2 \\;=\\; \\frac{1}{10}.\n$$\nAt time $t$, before assimilating data $y_t$, the forecast mean and covariance (obtained from the previous analysis step and the GNN-induced dynamics) are given as\n$$\nx_{t|t-1} \\;=\\; \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix}, \n\\qquad \nP_{t|t-1} \\;=\\; \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{bmatrix}.\n$$\nNoisy observations are collected on nodes $1$ and $3$ via a linear observation operator $H \\in \\mathbb{R}^{2 \\times 3}$ and additive Gaussian noise $\\varepsilon_t \\sim \\mathcal{N}(0,R)$ with\n$$\nH \\;=\\; \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}, \n\\qquad \nR \\;=\\; \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}, \n\\qquad \ny_t \\;=\\; \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}.\n$$\nStarting from the linear-Gaussian data assimilation model and the conditioning rules for jointly Gaussian random variables, derive the analysis step that fuses the forecast $(x_{t|t-1}, P_{t|t-1})$ with $y_t$ to obtain the Kalman gain and the posterior mean. Then evaluate these expressions for the given numerical instance.\n\nReport the final answer as a single row matrix that first lists the entries of the Kalman gain $K_t \\in \\mathbb{R}^{3 \\times 2}$ in column-major order, followed by the three entries of the posterior mean $x_{t|t} \\in \\mathbb{R}^3$. Concretely, report\n$$\n\\big[\\, K_{t,(1,1)},\\, K_{t,(2,1)},\\, K_{t,(3,1)},\\, K_{t,(1,2)},\\, K_{t,(2,2)},\\, K_{t,(3,2)},\\, x_{t|t,(1)},\\, x_{t|t,(2)},\\, x_{t|t,(3)} \\,\\big].\n$$\nUse exact values; no rounding is required. There are no physical units in this problem. The angle unit specification is not applicable here.", "solution": "The user wants me to solve a data assimilation problem based on a Kalman filter framework.\n\n### Step 1: Extract Givens\nThe problem provides the following data and definitions for a dynamic inverse problem on a graph at a specific time $t$:\n\nLatent state vector: $x_t \\in \\mathbb{R}^3$\nForecast mean: $x_{t|t-1} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix}$\nForecast covariance: $P_{t|t-1} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{bmatrix}$\nLinear observation operator: $H = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$\nObservation noise covariance: $R = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$\nObservation vector: $y_t = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$\n\nThe problem also provides context regarding a GNN-prior in the form of a one-step rollout operator $F = I - \\alpha L$ and process noise covariance $Q = \\sigma_q^2 I$, with specific values for $L$, $\\alpha$, and $\\sigma_q^2$. This information pertains to the forecast step (propagating the state from time $t-1$ to $t$), which has already been performed to yield the given forecast quantities $x_{t|t-1}$ and $P_{t|t-1}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem describes a standard linear-Gaussian data assimilation scenario, which is a core concept in state estimation, control theory, and inverse problems. The formulation is based on the Kalman filter, a fundamental and widely-used algorithm in engineering and science. All definitions and models are standard and scientifically sound.\n2.  **Well-Posed**: The problem is well-posed. It asks for the posterior mean and Kalman gain, which are uniquely determined by the given forecast state and the observation model. The provided matrices are well-defined, and the necessary matrix inverses exist (as will be shown in the solution).\n3.  **Objective**: The problem is stated in precise, objective mathematical language. It provides all necessary numerical values and definitions, leaving no room for subjective interpretation.\n4.  **Incomplete or Contradictory Setup**: The problem is self-contained. While it provides extra information about the dynamic model ($F, Q, L$), this information is context for how the forecast state ($x_{t|t-1}, P_{t|t-1}$) was obtained and is not needed for the requested analysis step. This does not constitute a contradiction but rather presents a complete picture of a single time step in a filtering cycle. All required components for the analysis step are provided.\n5.  **Unrealistic or Infeasible**: The dimensions of the matrices and vectors are consistent: $x_{t|t-1}$ is $3 \\times 1$, $P_{t|t-1}$ is $3 \\times 3$, $y_t$ is $2 \\times 1$, $H$ is $2 \\times 3$, and $R$ is $2 \\times 2$. All operations are mathematically valid. The values are standard and do not represent any physical impossibility.\n6.  **Ill-Posed or Poorly Structured**: The problem is clearly structured and asks for a specific, calculable result derived from established formulas.\n7.  **All other checks**: The problem is formalizable, relevant to the specified fields, not pseudo-profound, and scientifically verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\nThe problem requires the derivation of the analysis step update equations from the conditioning rules for jointly Gaussian random variables, and then the evaluation of the Kalman gain $K_t$ and the posterior mean $x_{t|t}$.\n\nThe analysis step combines the forecast information with new observations. We assume a linear-Gaussian model.\nThe prior (forecast) distribution for the state $x_t$ is Gaussian:\n$$ x_t \\sim \\mathcal{N}(x_{t|t-1}, P_{t|t-1}) $$\nThe observation model is linear with additive Gaussian noise:\n$$ y_t = H x_t + \\varepsilon_t, \\quad \\text{where} \\quad \\varepsilon_t \\sim \\mathcal{N}(0, R) $$\nThe state $x_t$ and the noise $\\varepsilon_t$ are assumed to be uncorrelated.\n\nWe form a joint random vector $\\begin{pmatrix} x_t \\\\ y_t \\end{pmatrix}$. Its distribution is also Gaussian.\nThe mean of the joint vector is:\n$$ E\\left[ \\begin{pmatrix} x_t \\\\ y_t \\end{pmatrix} \\right] = \\begin{pmatrix} E[x_t] \\\\ E[H x_t + \\varepsilon_t] \\end{pmatrix} = \\begin{pmatrix} x_{t|t-1} \\\\ H E[x_t] + E[\\varepsilon_t] \\end{pmatrix} = \\begin{pmatrix} x_{t|t-1} \\\\ H x_{t|t-1} \\end{pmatrix} $$\nThe covariance of the joint vector is:\n$$ \\text{Cov}\\left( \\begin{pmatrix} x_t \\\\ y_t \\end{pmatrix} \\right) = \\begin{pmatrix} \\text{Cov}(x_t, x_t) & \\text{Cov}(x_t, y_t) \\\\ \\text{Cov}(y_t, x_t) & \\text{Cov}(y_t, y_t) \\end{pmatrix} $$\nLet's compute the blocks:\n- $\\text{Cov}(x_t, x_t) = P_{t|t-1}$\n- $\\text{Cov}(x_t, y_t) = E[(x_t - x_{t|t-1})(y_t - H x_{t|t-1})^T] = E[(x_t - x_{t|t-1})(H x_t + \\varepsilon_t - H x_{t|t-1})^T] = E[(x_t - x_{t|t-1})(H(x_t - x_{t|t-1}) + \\varepsilon_t)^T] = E[(x_t - x_{t|t-1})(x_t - x_{t|t-1})^T H^T] + E[(x_t - x_{t|t-1})\\varepsilon_t^T] = P_{t|t-1} H^T$, since $x_t$ and $\\varepsilon_t$ are uncorrelated.\n- $\\text{Cov}(y_t, x_t) = (\\text{Cov}(x_t, y_t))^T = H P_{t|t-1}$\n- $\\text{Cov}(y_t, y_t) = E[(y_t - H x_{t|t-1})(y_t - H x_{t|t-1})^T] = E[(H(x_t - x_{t|t-1}) + \\varepsilon_t)(H(x_t - x_{t|t-1}) + \\varepsilon_t)^T] = H E[(x_t - x_{t|t-1})(x_t - x_{t|t-1})^T] H^T + E[\\varepsilon_t \\varepsilon_t^T] = H P_{t|t-1} H^T + R$.\n\nSo the joint distribution is:\n$$ \\begin{pmatrix} x_t \\\\ y_t \\end{pmatrix} \\sim \\mathcal{N}\\left( \\begin{pmatrix} x_{t|t-1} \\\\ H x_{t|t-1} \\end{pmatrix}, \\begin{pmatrix} P_{t|t-1} & P_{t|t-1} H^T \\\\ H P_{t|t-1} & H P_{t|t-1} H^T + R \\end{pmatrix} \\right) $$\n\nThe analysis step requires finding the conditional distribution of $x_t$ given the observation $y_t$. For a general partitioned Gaussian vector $\\begin{pmatrix} z_1 \\\\ z_2 \\end{pmatrix}$ with mean $\\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}$ and covariance $\\begin{pmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{pmatrix}$, the conditional distribution of $z_1$ given $z_2$ is Gaussian with mean $\\mu_{1|2} = \\mu_1 + \\Sigma_{12}\\Sigma_{22}^{-1}(z_2 - \\mu_2)$ and covariance $\\Sigma_{1|2} = \\Sigma_{11} - \\Sigma_{12}\\Sigma_{22}^{-1}\\Sigma_{21}$.\n\nApplying this to our case ($z_1=x_t$, $z_2=y_t$), we find the posterior (analysis) mean $x_{t|t}$ and covariance $P_{t|t}$:\n$$ x_{t|t} = x_{t|t-1} + (P_{t|t-1} H^T)(H P_{t|t-1} H^T + R)^{-1}(y_t - H x_{t|t-1}) $$\nThe term $K_t = (P_{t|t-1} H^T)(H P_{t|t-1} H^T + R)^{-1}$ is the Kalman gain.\nThe posterior mean is then given by the update equation:\n$$ x_{t|t} = x_{t|t-1} + K_t (y_t - H x_{t|t-1}) $$\n\nNow, let's evaluate these expressions for the given numerical instance.\nThe given matrices and vectors are:\n$$ P_{t|t-1} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{bmatrix}, \\quad H = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}, \\quad R = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} $$\n$$ x_{t|t-1} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix}, \\quad y_t = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $$\n\nFirst, we calculate the components needed for the Kalman gain $K_t$.\nThe term $P_{t|t-1} H^T$:\n$$ P_{t|t-1} H^T = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\\\ 0 & 3 \\end{bmatrix} $$\nThe innovation covariance $S_t = H P_{t|t-1} H^T + R$:\n$$ H P_{t|t-1} H^T = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 3 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 3 \\end{bmatrix} $$\n$$ S_t = \\begin{bmatrix} 1 & 0 \\\\ 0 & 3 \\end{bmatrix} + \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 2 & 0 \\\\ 0 & 4 \\end{bmatrix} $$\nThe inverse of the innovation covariance $S_t^{-1}$:\n$$ S_t^{-1} = \\begin{bmatrix} 2 & 0 \\\\ 0 & 4 \\end{bmatrix}^{-1} = \\begin{bmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{4} \\end{bmatrix} $$\nNow, we can compute the Kalman gain $K_t = P_{t|t-1} H^T S_t^{-1}$:\n$$ K_t = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\\\ 0 & 3 \\end{bmatrix} \\begin{bmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{4} \\end{bmatrix} = \\begin{bmatrix} (1)(\\frac{1}{2})+(0)(0) & (1)(0)+(0)(\\frac{1}{4}) \\\\ (0)(\\frac{1}{2})+(0)(0) & (0)(0)+(0)(\\frac{1}{4}) \\\\ (0)(\\frac{1}{2})+(3)(0) & (0)(0)+(3)(\\frac{1}{4}) \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} & 0 \\\\ 0 & 0 \\\\ 0 & \\frac{3}{4} \\end{bmatrix} $$\n\nNext, we calculate the posterior mean $x_{t|t}$.\nWe need the innovation term, $y_t - H x_{t|t-1}$:\n$$ H x_{t|t-1} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 2 \\end{bmatrix} $$\n$$ y_t - H x_{t|t-1} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 0 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} $$\nFinally, we compute the posterior mean $x_{t|t} = x_{t|t-1} + K_t (y_t - H x_{t|t-1})$:\n$$ x_{t|t} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix} + \\begin{bmatrix} \\frac{1}{2} & 0 \\\\ 0 & 0 \\\\ 0 & \\frac{3}{4} \\end{bmatrix} \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix} + \\begin{bmatrix} (\\frac{1}{2})(1)+(0)(-1) \\\\ (0)(1)+(0)(-1) \\\\ (0)(1)+(\\frac{3}{4})(-1) \\end{bmatrix} $$\n$$ x_{t|t} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\end{bmatrix} + \\begin{bmatrix} \\frac{1}{2} \\\\ 0 \\\\ -\\frac{3}{4} \\end{bmatrix} = \\begin{bmatrix} 0+\\frac{1}{2} \\\\ 1+0 \\\\ 2-\\frac{3}{4} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ 1 \\\\ \\frac{5}{4} \\end{bmatrix} $$\n\nThe problem asks for the result as a single row matrix containing the entries of the Kalman gain $K_t$ in column-major order, followed by the entries of the posterior mean $x_{t|t}$.\nThe Kalman gain is $K_t = \\begin{bmatrix} K_{t,(1,1)} & K_{t,(1,2)} \\\\ K_{t,(2,1)} & K_{t,(2,2)} \\\\ K_{t,(3,1)} & K_{t,(3,2)} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} & 0 \\\\ 0 & 0 \\\\ 0 & \\frac{3}{4} \\end{bmatrix}$.\nIn column-major order, the entries are: $K_{t,(1,1)}=\\frac{1}{2}$, $K_{t,(2,1)}=0$, $K_{t,(3,1)}=0$, $K_{t,(1,2)}=0$, $K_{t,(2,2)}=0$, $K_{t,(3,2)}=\\frac{3}{4}$.\nThe posterior mean is $x_{t|t} = \\begin{bmatrix} x_{t|t,(1)} \\\\ x_{t|t,(2)} \\\\ x_{t|t,(3)} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ 1 \\\\ \\frac{5}{4} \\end{bmatrix}$.\n\nThe final reported vector is $\\big[\\, \\frac{1}{2},\\, 0,\\, 0,\\, 0,\\, 0,\\, \\frac{3}{4},\\, \\frac{1}{2},\\, 1,\\, \\frac{5}{4} \\,\\big]$.", "answer": "$$\n\\boxed{\n\\begin{bmatrix}\n\\frac{1}{2} & 0 & 0 & 0 & 0 & \\frac{3}{4} & \\frac{1}{2} & 1 & \\frac{5}{4}\n\\end{bmatrix}\n}\n$$", "id": "3386871"}]}