{"hands_on_practices": [{"introduction": "A core challenge in solving inverse problems is ensuring stability: the solution should not be overly sensitive to small perturbations or noise in the measurement data. This practice [@problem_id:3399532] explores how to enforce this stability in a learned inverse map by controlling its global Lipschitz constant, a measure of its maximum amplification of input changes. You will connect modern deep learning techniques like spectral normalization to the classical singular value analysis of the forward operator, providing a principled way to build reliable and well-behaved reconstruction networks.", "problem": "You are given a linear forward model with observations defined by $y = A x$, where $A \\in \\mathbb{R}^{m \\times n}$ is known. Consider learning an inverse mapping $\\Phi: \\mathbb{R}^{m} \\to \\mathbb{R}^{n}$ using a $K$-layer feedforward neural network of the form\n$$\n\\Phi(y) \\;=\\; W_{K} \\,\\rho\\!\\left(W_{K-1} \\,\\rho\\!\\left(\\cdots \\rho\\!\\left(W_{1} y\\right)\\cdots\\right)\\right),\n$$\nwhere each $W_{k} \\in \\mathbb{R}^{d_{k} \\times d_{k-1}}$ is a linear layer and $\\rho$ is a pointwise nonlinearity that is $1$-Lipschitz (for example, the Rectified Linear Unit (ReLU)). Spectral normalization is enforced so that the spectral norm (operator $2$-norm) of each layer obeys $\\|W_{k}\\|_{2} \\le s_{k}$ for known positive scalars $s_{k}$.\n\n1. Using only the definitions of Lipschitz continuity, operator norms, and their behavior under composition, derive a tight upper bound $L_{\\mathrm{sn}}$ such that for all $y_{1},y_{2} \\in \\mathbb{R}^{m}$,\n$$\n\\|\\Phi(y_{1})-\\Phi(y_{2})\\|_{2} \\;\\le\\; L_{\\mathrm{sn}} \\,\\|y_{1}-y_{2}\\|_{2}.\n$$\nExpress $L_{\\mathrm{sn}}$ in terms of the $\\{s_{k}\\}_{k=1}^{K}$.\n\n2. Let $A$ have Singular Value Decomposition (SVD) $A = U \\Sigma V^{\\top}$ with singular values $\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge \\sigma_{r} > 0$ and rank $r$. Suppose that identifiability is restricted to the subspace spanned by the right singular vectors associated with singular values at least a threshold $\\tau>0$, that is, to the subspace $\\mathcal{V}_{\\mathrm{id}} = \\mathrm{span}\\{v_{i}:\\sigma_{i}\\ge\\tau\\}$. Using only properties of the SVD and the Moore-Penrose pseudoinverse (MPP), determine the smallest Lipschitz constant of any exact right-inverse on $\\mathcal{V}_{\\mathrm{id}}$, and express it as a function of the smallest singular value $\\sigma_{\\min}^{\\mathrm{id}}$ within $\\mathcal{V}_{\\mathrm{id}}$.\n\n3. In practice, to ensure conditional stability while not exceeding the amplification that is unavoidable due to the forward operator $A$, one enforces the global Lipschitz bound of $\\Phi$ at the level\n$$\nL_{\\star} \\;=\\; \\min\\!\\big\\{L_{\\mathrm{sn}},\\, L_{\\mathrm{id}}\\big\\},\n$$\nwhere $L_{\\mathrm{id}}$ is the minimal Lipschitz constant of an exact right-inverse on $\\mathcal{V}_{\\mathrm{id}}$ found in part 2. Given the following concrete values:\n- $K=3$, $s_{1}=2$, $s_{2}=1.5$, $s_{3}=1.2$,\n- $A$ has singular values $\\{10,\\, 2,\\, 0.5,\\, 0.01\\}$, and the identifiability threshold is $\\tau=0.4$,\ncompute $L_{\\star}$.\n\nExpress your final answer as a pure number (no units), rounded to four significant figures.", "solution": "The problem is validated as being self-contained, scientifically grounded in the mathematics of linear algebra and neural network theory, and well-posed. The solution process proceeds in three parts as requested.\n\n### Part 1: Derivation of the Lipschitz bound $L_{\\mathrm{sn}}$\n\nThe goal is to find a tight upper bound on the Lipschitz constant of the neural network $\\Phi(y)$. The network is a composition of functions. Let us define the function for each layer. For $k \\in \\{1, 2, \\dots, K-1\\}$, let the $k$-th layer transformation be $f_k(z) = \\rho(W_k z)$. For the final layer, let $f_K(z) = W_K z$. The network is then the composition $\\Phi(y) = f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1(y)$.\n\nThe Lipschitz constant of a composition of functions is bounded by the product of their individual Lipschitz constants. That is, $L(g \\circ h) \\le L(g)L(h)$. We will apply this property recursively.\n\nFirst, let's determine the Lipschitz constant for each layer function $f_k$.\n\nFor an intermediate layer $k \\in \\{1, 2, \\dots, K-1\\}$, we have $f_k(z) = \\rho(W_k z)$. Let $z_1, z_2$ be two vectors in the domain of $f_k$. We analyze the norm of the difference:\n$$\n\\|f_k(z_1) - f_k(z_2)\\|_2 = \\|\\rho(W_k z_1) - \\rho(W_k z_2)\\|_2\n$$\nThe problem states that the nonlinearity $\\rho$ is a pointwise $1$-Lipschitz function. For any two vectors $u, v$, a pointwise $1$-Lipschitz function satisfies $\\|\\rho(u) - \\rho(v)\\|_2 \\le \\|u - v\\|_2$. Let $u = W_k z_1$ and $v = W_k z_2$. Applying this property, we get:\n$$\n\\|\\rho(W_k z_1) - \\rho(W_k z_2)\\|_2 \\le \\|W_k z_1 - W_k z_2\\|_2\n$$\nUsing the linearity of $W_k$ and the definition of the operator $2$-norm (spectral norm), we have:\n$$\n\\|W_k z_1 - W_k z_2\\|_2 = \\|W_k (z_1 - z_2)\\|_2 \\le \\|W_k\\|_2 \\|z_1 - z_2\\|_2\n$$\nThe problem specifies that spectral normalization is applied such that $\\|W_k\\|_2 \\le s_k$. Therefore, the Lipschitz constant $L_k$ of the function $f_k$ is bounded by $s_k$:\n$$\n\\|f_k(z_1) - f_k(z_2)\\|_2 \\le s_k \\|z_1 - z_2\\|_2 \\implies L_k \\le s_k\n$$\n\nFor the final layer, $k=K$, the function is $f_K(z) = W_K z$. This is a linear map. Its Lipschitz constant is precisely its operator norm, $L_K = \\|W_K\\|_2$. We are given the bound $\\|W_K\\|_2 \\le s_K$, so $L_K \\le s_K$.\n\nNow, we compose the layers to find the Lipschitz constant of the entire network $\\Phi$.\n$$\nL_{\\Phi} = L(f_K \\circ f_{K-1} \\circ \\cdots \\circ f_1) \\le L_K \\cdot L_{K-1} \\cdots L_1\n$$\nUsing the bounds we derived for each layer:\n$$\nL_{\\Phi} \\le s_K \\cdot s_{K-1} \\cdots s_1 = \\prod_{k=1}^{K} s_k\n$$\nThis upper bound is designated as $L_{\\mathrm{sn}}$. The bound is considered tight because it is achievable for specific choices of weights $W_k$ (where $\\|W_k\\|_2=s_k$) and specific inputs (e.g., if $\\rho$ is the identity and the dominant singular vectors of the weight matrices are aligned).\n$$\nL_{\\mathrm{sn}} = \\prod_{k=1}^{K} s_k\n$$\n\n### Part 2: Derivation of the minimal Lipschitz constant $L_{\\mathrm{id}}$\n\nWe are asked for the smallest Lipschitz constant of any exact right-inverse on the subspace $\\mathcal{V}_{\\mathrm{id}} = \\mathrm{span}\\{v_{i}:\\sigma_{i}\\ge\\tau\\}$. Let such an inverse map be $\\mathcal{G}: \\mathbb{R}^m \\to \\mathbb{R}^n$. The condition of being an exact right-inverse on $\\mathcal{V}_{\\mathrm{id}}$ means that for any vector $x \\in \\mathcal{V}_{\\mathrm{id}}$, if $y = Ax$, then $\\mathcal{G}(y) = x$.\n\nLet $L_{\\mathcal{G}}$ be the Lipschitz constant of such a map $\\mathcal{G}$. Let $\\sigma_{\\min}^{\\mathrm{id}} = \\min\\{\\sigma_i : \\sigma_i \\ge \\tau\\}$, and let $v_j$ be a right singular vector corresponding to this singular value, so $Av_j = \\sigma_j u_j$ with $\\sigma_j = \\sigma_{\\min}^{\\mathrm{id}}$. Since $\\sigma_j \\ge \\tau$, $v_j \\in \\mathcal{V}_{\\mathrm{id}}$.\nLet us test the Lipschitz condition with two specific points. Let $x_1 = v_j$ and $x_2=0$. Both are in $\\mathcal{V}_{\\mathrm{id}}$. The corresponding observations are $y_1 = Ax_1 = Av_j = \\sigma_j u_j$ and $y_2 = Ax_2 = 0$.\nThe right-inverse property demands $\\mathcal{G}(y_1) = x_1 = v_j$ and $\\mathcal{G}(y_2) = x_2 = 0$.\nThe definition of the Lipschitz constant requires:\n$$\n\\|\\mathcal{G}(y_1) - \\mathcal{G}(y_2)\\|_2 \\le L_{\\mathcal{G}} \\|y_1 - y_2\\|_2\n$$\nSubstituting the vectors:\n$$\n\\|v_j - 0\\|_2 \\le L_{\\mathcal{G}} \\|\\sigma_j u_j - 0\\|_2\n$$\nThe singular vectors are orthonormal, so $\\|v_j\\|_2 = 1$ and $\\|u_j\\|_2 = 1$. The inequality becomes:\n$$\n1 \\le L_{\\mathcal{G}} |\\sigma_j| \\cdot \\|u_j\\|_2 = L_{\\mathcal{G}} \\sigma_j\n$$\nSince $\\sigma_j = \\sigma_{\\min}^{\\mathrm{id}}$, we have a lower bound on the Lipschitz constant for *any* such inverse map $\\mathcal{G}$:\n$$\nL_{\\mathcal{G}} \\ge \\frac{1}{\\sigma_{\\min}^{\\mathrm{id}}}\n$$\nThis shows that the minimal possible Lipschitz constant is at least $1/\\sigma_{\\min}^{\\mathrm{id}}$. To show that this minimum is achievable, we must construct a map that satisfies the right-inverse property and has this exact Lipschitz constant.\n\nConsider the truncated singular value decomposition (TSVD) inverse, defined as:\n$$\n\\mathcal{G}_{\\mathrm{TSVD}} = \\sum_{i : \\sigma_i \\ge \\tau} \\frac{1}{\\sigma_i} v_i u_i^{\\top}\n$$\nThis is a linear map from $\\mathbb{R}^m$ to $\\mathbb{R}^n$. Its operator norm (which is its Lipschitz constant) is given by the largest singular value of the map, which is $\\max_{i:\\sigma_i \\ge \\tau} (1/\\sigma_i) = 1/\\min_{i:\\sigma_i \\ge \\tau} (\\sigma_i) = 1/\\sigma_{\\min}^{\\mathrm{id}}$.\nLet's verify it is an exact right-inverse on $\\mathcal{V}_{\\mathrm{id}}$. Any $x \\in \\mathcal{V}_{\\mathrm{id}}$ can be written as $x = \\sum_{j : \\sigma_j \\ge \\tau} c_j v_j$. Then $y = Ax = \\sum_{j : \\sigma_j \\ge \\tau} c_j A v_j = \\sum_{j : \\sigma_j \\ge \\tau} c_j \\sigma_j u_j$.\nApplying $\\mathcal{G}_{\\mathrm{TSVD}}$ to $y$:\n$$\n\\mathcal{G}_{\\mathrm{TSVD}}(y) = \\mathcal{G}_{\\mathrm{TSVD}}\\left(\\sum_{j : \\sigma_j \\ge \\tau} c_j \\sigma_j u_j\\right) = \\sum_{j : \\sigma_j \\ge \\tau} c_j \\sigma_j \\mathcal{G}_{\\mathrm{TSVD}}(u_j)\n$$\nBy definition of $\\mathcal{G}_{\\mathrm{TSVD}}$ and orthogonality of $\\{u_i\\}$, $\\mathcal{G}_{\\mathrm{TSVD}}(u_j) = (1/\\sigma_j) v_j$.\n$$\n\\mathcal{G}_{\\mathrm{TSVD}}(y) = \\sum_{j : \\sigma_j \\ge \\tau} c_j \\sigma_j \\left(\\frac{1}{\\sigma_j} v_j\\right) = \\sum_{j : \\sigma_j \\ge \\tau} c_j v_j = x\n$$\nThe map fulfills the condition. Since we found a map that achieves the lower bound, this is the minimal possible Lipschitz constant.\n$$\nL_{\\mathrm{id}} = \\frac{1}{\\sigma_{\\min}^{\\mathrm{id}}}\n$$\n\n### Part 3: Computation of $L_{\\star}$\n\nWe are given the concrete values and need to compute $L_{\\star} = \\min\\{L_{\\mathrm{sn}}, L_{\\mathrm{id}}\\}$.\n\nFirst, compute $L_{\\mathrm{sn}}$:\n- $K=3$\n- $s_1=2$, $s_2=1.5$, $s_3=1.2$\n- Using the formula from Part 1:\n$$\nL_{\\mathrm{sn}} = \\prod_{k=1}^{3} s_k = s_1 \\cdot s_2 \\cdot s_3 = 2 \\times 1.5 \\times 1.2 = 3 \\times 1.2 = 3.6\n$$\n\nNext, compute $L_{\\mathrm{id}}$:\n- Singular values of $A$: $\\{10, 2, 0.5, 0.01\\}$\n- Identifiability threshold: $\\tau = 0.4$\n- We must find $\\sigma_{\\min}^{\\mathrm{id}} = \\min\\{\\sigma_i : \\sigma_i \\ge \\tau\\}$.\n- The singular values greater than or equal to $\\tau=0.4$ are $\\{10, 2, 0.5\\}$.\n- The minimum among these is $\\sigma_{\\min}^{\\mathrm{id}} = 0.5$.\n- Using the formula from Part 2:\n$$\nL_{\\mathrm{id}} = \\frac{1}{\\sigma_{\\min}^{\\mathrm{id}}} = \\frac{1}{0.5} = 2\n$$\n\nFinally, compute $L_{\\star}$:\n$$\nL_{\\star} = \\min\\{L_{\\mathrm{sn}}, L_{\\mathrm{id}}\\} = \\min\\{3.6, 2\\} = 2\n$$\nThe problem asks for the answer to be rounded to four significant figures.\n$$\nL_{\\star} = 2.000\n$$", "answer": "$$\\boxed{2.000}$$", "id": "3399532"}, {"introduction": "Instead of designing a generic network architecture, a powerful approach in scientific machine learning is to \"unroll\" a classical iterative optimization algorithm into a deep neural network. This endows the network with a principled structure, enhancing interpretability and performance. This exercise [@problem_id:3399544] guides you through the analysis of an unrolled proximal-gradient network, where you will derive how the network layers perform iterative refinement and demonstrate that the network depth acts as an implicit regularization parameter, bridging the gap between deep learning and classical spectral filter theory.", "problem": "Consider the linear inverse problem $y = A x^{\\star} + \\eta$ with $A \\in \\mathbb{R}^{m \\times n}$, unknown target $x^{\\star} \\in \\mathbb{R}^{n}$, and additive noise $\\eta \\in \\mathbb{R}^{m}$ satisfying a known bound $\\|\\eta\\|_{2} \\leq \\delta$. Assume a data-fidelity objective derived from the squared residual and a quadratic penalty, so that the regularized cost is $J(x) = \\frac{1}{2}\\|Ax - y\\|_{2}^{2} + \\mu \\|x\\|_{2}^{2}$ with a regularization weight $\\mu > 0$. Consider an unrolled Proximal Gradient (PG) network with $L$ layers, whose $k$-th layer performs the update\n$$\nx^{k+1} = \\mathrm{prox}_{\\alpha_{k} \\mu \\|\\cdot\\|_{2}^{2}}\\!\\left(x^{k} - \\alpha_{k} A^{\\top}(Ax^{k} - y)\\right), \\quad x^{0} = 0,\n$$\nwhere $\\alpha_{k} > 0$ is a trainable step size and $\\mathrm{prox}_{\\gamma R}(v)$ denotes the proximal operator of $\\gamma R$ evaluated at $v$. You may take as given the definition of the proximal operator and standard properties of the Singular Value Decomposition (SVD). The goal is to analyze the implicit spectral filtering induced by the trainable parameters and to derive a principled early-stopping rule.\n\nYour tasks:\n- Derive, from first principles of proximal calculus and linear algebra, the layer-wise update in terms of the symmetric matrix $M = A^{\\top} A$, and show that the network defines a linear mapping from $A^{\\top} y$ to $x^{L}$.\n- Using the eigen-decomposition of $M$ and the linearity of the network, derive the scalar spectral filter $g_{L}(\\lambda)$ such that for any eigenpair $(\\lambda, u)$ of $M$ with $M u = \\lambda u$, the component of $x^{L}$ along $u$ is $g_{L}(\\lambda)\\,(u^{\\top} A^{\\top} y)$. Assume for this part that the trainable parameters are chosen layer-constant, $\\alpha_{k} \\equiv \\alpha$ and $\\mu$ fixed across layers.\n- Starting from the data model and the spectral representation of the iterates, formulate a discrepancy principle for early stopping. That is, propose a rule to select the smallest $L$ such that the residual $\\|A x^{L} - y\\|_{2}$ approximately matches the noise level multiplied by a safety factor $\\tau > 1$, and explain why this guarantees stability with respect to perturbations in $y$.\n- Provide your final answer as the closed-form expression for $g_{L}(\\lambda)$ under the layer-constant choice $\\alpha_{k} \\equiv \\alpha$ and fixed $\\mu$. No numerical evaluation is required.\n\nExpress all mathematical expressions in LaTeX. The final answer must be a single closed-form analytic expression. Do not include any units.", "solution": "The problem is analyzed and solved in four parts as requested.\n\nFirst, we derive the layer-wise update in terms of the matrix $M = A^{\\top} A$ and demonstrate the linearity of the network map.\nThe $k$-th layer update is given by\n$$\nx^{k+1} = \\mathrm{prox}_{\\alpha_{k} \\mu \\|\\cdot\\|_{2}^{2}}\\!\\left(x^{k} - \\alpha_{k} A^{\\top}(Ax^{k} - y)\\right)\n$$\nThe proximal operator $\\mathrm{prox}_{\\gamma R}(v)$ is defined as the solution to an optimization problem:\n$$\n\\mathrm{prox}_{\\gamma R}(v) = \\arg\\min_{z} \\left\\{ \\gamma R(z) + \\frac{1}{2}\\|z-v\\|_{2}^{2} \\right\\}\n$$\nIn our case, the regularization function is $R(x) = \\|x\\|_{2}^{2}$ and the parameter is $\\gamma = \\alpha_k \\mu$. The minimization objective is\n$$\nf(z) = \\alpha_{k} \\mu \\|z\\|_{2}^{2} + \\frac{1}{2}\\|z-v\\|_{2}^{2}\n$$\nThis is a strictly convex function. We find the minimum by setting the gradient with respect to $z$ to zero:\n$$\n\\nabla_{z} f(z) = 2 \\alpha_{k} \\mu z + (z - v) = 0\n$$\n$$\n(1 + 2 \\alpha_{k} \\mu) z = v \\implies z = \\frac{1}{1 + 2 \\alpha_{k} \\mu} v\n$$\nThus, the proximal operator is a simple scaling: $\\mathrm{prox}_{\\alpha_{k} \\mu \\|\\cdot\\|_{2}^{2}}(v) = (1 + 2 \\alpha_{k} \\mu)^{-1} v$.\nSubstituting this back into the update rule for $x^{k+1}$, we get:\n$$\nx^{k+1} = \\frac{1}{1 + 2 \\alpha_{k} \\mu} \\left(x^{k} - \\alpha_{k} (Mx^{k} - A^{\\top}y)\\right)\n$$\nRearranging the terms, we obtain the update rule expressed in terms of $M$:\n$$\nx^{k+1} = \\frac{1}{1 + 2 \\alpha_{k} \\mu} \\left( (I - \\alpha_{k} M)x^{k} + \\alpha_{k} A^{\\top}y \\right)\n$$\nThis is an affine recurrence relation. To show that the final output $x^L$ is a linear function of $A^{\\top}y$, we unroll the recursion starting from the initial condition $x^0 = 0$:\nFor $k=0$:\n$$\nx^{1} = \\frac{1}{1 + 2 \\alpha_{0} \\mu} \\left( (I - \\alpha_{0} M) x^{0} + \\alpha_{0} A^{\\top}y \\right) = \\frac{\\alpha_{0}}{1 + 2 \\alpha_{0} \\mu} A^{\\top}y\n$$\nFor $k=1$:\n$$\nx^{2} = \\frac{1}{1 + 2 \\alpha_{1} \\mu} \\left( (I - \\alpha_{1} M) x^{1} + \\alpha_{1} A^{\\top}y \\right) = \\frac{I - \\alpha_{1} M}{1 + 2 \\alpha_{1} \\mu} \\left(\\frac{\\alpha_{0}}{1 + 2 \\alpha_{0} \\mu} A^{\\top}y\\right) + \\frac{\\alpha_{1}}{1 + 2 \\alpha_{1} \\mu} A^{\\top}y\n$$\nThis can be written as $x^{2} = (Q_1 + Q_2) A^{\\top}y$, where $Q_1$ and $Q_2$ are matrices. Continuing this process up to layer $L$, $x^L$ will be a sum of terms, where each term is a product of matrices involving $M$ and $\\alpha_k$ applied to a vector proportional to $A^{\\top}y$. The final expression for $x^L$ is of the form:\n$$\nx^L = G_L(\\alpha_0, \\dots, \\alpha_{L-1}, \\mu, M) A^{\\top}y\n$$\nwhere $G_L$ is a matrix-valued function of the parameters. This demonstrates that for fixed parameters, the unrolled network defines a linear mapping from $A^{\\top}y$ to $x^L$.\n\nSecond, we derive the scalar spectral filter $g_{L}(\\lambda)$ for the case of layer-constant parameters, i.e., $\\alpha_k \\equiv \\alpha$.\nThe update rule simplifies to:\n$$\nx^{k+1} = \\frac{1}{1 + 2 \\alpha \\mu} \\left( (I - \\alpha M)x^{k} + \\alpha A^{\\top}y \\right)\n$$\nLet $C = \\frac{1}{1 + 2 \\alpha \\mu}(I - \\alpha M)$ and $d = \\frac{\\alpha}{1 + 2 \\alpha \\mu} A^{\\top}y$. The update is $x^{k+1} = Cx^{k} + d$.\nStarting with $x^0=0$, we have $x^L = \\left(\\sum_{k=0}^{L-1} C^k\\right) d$.\nLet $(\\lambda, u)$ be an eigenpair of $M$, so $Mu = \\lambda u$. The vector $u$ is also an eigenvector of $C$:\n$$\nCu = \\frac{1}{1 + 2 \\alpha \\mu}(I - \\alpha M)u = \\frac{1}{1 + 2 \\alpha \\mu}(u - \\alpha \\lambda u) = \\left(\\frac{1 - \\alpha \\lambda}{1 + 2 \\alpha \\mu}\\right) u\n$$\nLet's denote the eigenvalue of $C$ corresponding to $\\lambda$ as $\\gamma(\\lambda) = \\frac{1 - \\alpha \\lambda}{1 + 2 \\alpha \\mu}$.\nThe operator $\\sum_{k=0}^{L-1} C^k$ is a geometric series of matrices. Its action on the eigenvector $u$ is multiplication by the scalar $\\sum_{k=0}^{L-1} \\gamma(\\lambda)^k = \\frac{1 - \\gamma(\\lambda)^L}{1 - \\gamma(\\lambda)}$.\nLet's project the equation onto the eigenvector $u$. The component of $x^L$ along $u$ is $u^{\\top}x^L$.\n$$\nu^{\\top}x^L = \\left(\\frac{1 - \\gamma(\\lambda)^L}{1 - \\gamma(\\lambda)}\\right) u^{\\top}d\n$$\nSubstituting the expressions for $d$ and $\\gamma(\\lambda)$:\n$$\nu^{\\top}d = u^{\\top} \\left(\\frac{\\alpha}{1 + 2 \\alpha \\mu} A^{\\top}y\\right) = \\frac{\\alpha}{1 + 2 \\alpha \\mu} (u^{\\top} A^{\\top}y)\n$$\n$$\n1 - \\gamma(\\lambda) = 1 - \\frac{1 - \\alpha \\lambda}{1 + 2 \\alpha \\mu} = \\frac{(1 + 2 \\alpha \\mu) - (1 - \\alpha \\lambda)}{1 + 2 \\alpha \\mu} = \\frac{\\alpha \\lambda + 2 \\alpha \\mu}{1 + 2 \\alpha \\mu} = \\frac{\\alpha(\\lambda + 2\\mu)}{1 + 2 \\alpha \\mu}\n$$\nCombining these, the component of $x^L$ along $u$ is:\n$$\nu^{\\top}x^L = \\left(\\frac{1 - \\gamma(\\lambda)^L}{\\frac{\\alpha(\\lambda + 2\\mu)}{1 + 2 \\alpha \\mu}}\\right) \\left(\\frac{\\alpha}{1 + 2 \\alpha \\mu} (u^{\\top} A^{\\top}y)\\right) = (1 - \\gamma(\\lambda)^L) \\frac{1}{\\lambda + 2\\mu} (u^{\\top} A^{\\top}y)\n$$\nThe problem asks for the scalar filter $g_L(\\lambda)$ such that the component of $x^L$ along $u$ is $g_L(\\lambda) (u^{\\top}A^{\\top}y)$. From the derived expression, we can identify the filter as:\n$$\ng_L(\\lambda) = \\frac{1}{\\lambda + 2\\mu} \\left(1 - \\left(\\frac{1 - \\alpha\\lambda}{1 + 2\\alpha\\mu}\\right)^L\\right)\n$$\n\nThird, we formulate a discrepancy principle for early stopping and explain the stability it provides.\nThe Morozov discrepancy principle is a well-established method for choosing a regularization parameter. In the context of iterative methods, the number of iterations $L$ acts as the regularization parameter.\nThe proposed rule is: Given the data $y$, the noise bound $\\delta$, and a safety factor $\\tau > 1$, choose the number of layers (iterations) $L$ to be the smallest integer $k \\geq 1$ such that the residual norm satisfies:\n$$\n\\|Ax^{k} - y\\|_{2} \\leq \\tau \\delta\n$$\nThis rule is implemented by computing the residual at each layer of the network and stopping at the first layer that meets this criterion.\n\nThis principle guarantees stability for the following reasons:\nThe inverse problem is ill-posed, meaning that small singular values of $A$ cause noise amplification in a naive inversion. The iterative process of the unrolled network acts as a regularization scheme. The number of iterations $L$ controls the degree of regularization.\nIn the initial iterations (small $L$), the method reconstructs the components of the solution $x^{\\star}$ corresponding to large singular values of $A$, which are less affected by noise. During this phase, the residual $\\|Ax^{k} - y\\|_{2}$ is large and dominated by the yet-to-be-reconstructed part of the signal, $A(x^{\\star}-x^k)$.\nAs $k$ increases, $x^k$ gets closer to $x^{\\star}$, and the residual norm decreases, eventually approaching the norm of the noise, since $Ax^k - y = A(x^k - x^{\\star}) - \\eta$. The discrepancy principle stops the iteration when the residual becomes comparable to the noise level $\\delta$.\nIf the iteration were to continue past this point, the algorithm would attempt to decrease the residual further. Since the residual is already dominated by noise $\\eta$, this would mean fitting the model to the noise. Fitting noise requires the algorithm to reconstruct components corresponding to small singular values, which are heavily contaminated. This leads to a large amplification of the noise components in the solution $x^k$, destroying the reconstruction and causing instability.\nBy stopping early, the discrepancy principle prevents the algorithm from entering this noise-fitting regime. The effective spectral filter $g_L(\\lambda)$ for the chosen $L$ appropriately dampens the components corresponding to small singular values (where $\\lambda = \\sigma^2$ is small), thus preventing the amplification of noise. This ensures that the mapping from the perturbed data $y$ to the solution $x^L$ is a bounded operator, which is the definition of stability.\n\nFinally, the closed-form expression for the spectral filter $g_{L}(\\lambda)$ is provided as the final answer.", "answer": "$$\n\\boxed{\\frac{1}{\\lambda + 2\\mu}\\left(1 - \\left(\\frac{1 - \\alpha\\lambda}{1 + 2\\alpha\\mu}\\right)^L\\right)}\n$$", "id": "3399544"}, {"introduction": "The standard least-squares approach to data fitting is optimal for Gaussian noise but notoriously fragile in the presence of outliers. Robust statistics, a field with deep connections to machine learning, provides tools to design estimators that are resilient to such data contamination. In this problem [@problem_id:3399542], you will work with robust loss functions like the Huber and Tukey bisquare losses to build more reliable data-fidelity terms, using the formal tool of the influence function to quantify and compare their robustness to extreme observations.", "problem": "Consider a linear inverse problem with additive noise in which observations $y_i \\in \\mathbb{R}$, for $i \\in \\{1,\\dots,n\\}$, are generated by a known forward model $y_i = a\\,\\theta^{\\star} + \\varepsilon_i$, where $a \\in \\mathbb{R}\\setminus\\{0\\}$ is known, $\\theta^{\\star} \\in \\mathbb{R}$ is the unknown parameter to be estimated, and $\\{\\varepsilon_i\\}_{i=1}^{n}$ are independent and identically distributed noise terms with a symmetric distribution about zero and known scale $s>0$. You are asked to design robust machine-learning-inspired data-fidelity terms suitable for outlier-contaminated data, analyze their robustness via influence functions, and relate these to breakdown points in inversion.\n\nA robust-data-fidelity estimator $\\widehat{\\theta}$ is defined as any minimizer of\n$$\nJ(\\theta) \\triangleq \\sum_{i=1}^{n} \\rho\\!\\left(\\frac{y_i - a\\,\\theta}{s}\\right),\n$$\nwhere $\\rho$ is a robust loss. Two widely used robust losses are the Huber loss with tuning constant $c>0$,\n$$\n\\rho_H(t) \\triangleq \\begin{cases}\n\\frac{1}{2}\\,t^2, & |t| \\le c,\\\\\nc\\,|t| - \\frac{1}{2}\\,c^2, & |t| > c,\n\\end{cases}\n$$\nand Tukey’s bisquare (biweight) loss with tuning constant $c>0$,\n$$\n\\rho_T(t) \\triangleq \\begin{cases}\n\\frac{c^2}{6}\\left[1 - \\left(1 - \\left(\\frac{t}{c}\\right)^2\\right)^3\\right], & |t| \\le c,\\\\\n\\frac{c^2}{6}, & |t| > c.\n\\end{cases}\n$$\nLet $\\psi(t) \\triangleq \\frac{d}{dt}\\rho(t)$ denote the score function associated with $\\rho$.\n\nTasks:\n- Starting from the definition of an $M$-estimator as a minimizer of an empirical expected loss and the definition of the influence function (IF) of a statistical functional, derive the influence function of the estimator of $\\theta$ induced by $J(\\theta)$ under the model distribution. Express the result in terms of $a$, $s$, $\\psi$, and an expectation involving $\\psi'$ under the noise distribution.\n- For the Huber loss $\\rho_H$ and Tukey’s bisquare loss $\\rho_T$, compute the associated score functions $\\psi_H$ and $\\psi_T$, their derivatives $\\psi_H'$ and $\\psi_T'$, and the supremum of $|\\psi_H(t)|$ and $|\\psi_T(t)|$ over $t \\in \\mathbb{R}$.\n- Using the derived influence functions, discuss which of the two losses has redescending influence and explain, in the setting of this one-parameter inverse problem with known scale $s$, how the breakdown point behaves for each choice of $\\rho$. Contrast this with the case where $s$ is estimated robustly and jointly with $\\theta$.\n- Assume now that the standardized noise $T \\triangleq \\varepsilon/s$ is standard normal. Define the gross-error sensitivity $\\gamma^{\\ast}$ as the supremum over all possible contamination points $y \\in \\mathbb{R}$ of the absolute value of the influence function of the Huber-$M$ estimator of $\\theta$. What is the closed-form expression of $\\gamma^{\\ast}$ as a function of $a$, $s$, $c$, and the standard normal cumulative distribution function $\\Phi$? Your final answer must be a single analytic expression. Do not include units in the final answer.", "solution": "The problem is valid as it is scientifically grounded in the theory of robust statistics and M-estimation, is well-posed with a clear objective, and provides all necessary information for a rigorous mathematical derivation.\n\n### Task 1: Derivation of the Influence Function\n\nAn $M$-estimator $\\widehat{\\theta}$ for the parameter $\\theta$ is defined as a minimizer of the objective function $J(\\theta) = \\sum_{i=1}^{n} \\rho\\left(\\frac{y_i - a\\,\\theta}{s}\\right)$. The first-order condition for a minimum is obtained by setting the derivative of $J(\\theta)$ with respect to $\\theta$ to zero. Let $\\psi(t) = \\frac{d}{dt}\\rho(t)$.\n$$\n\\frac{d J(\\theta)}{d\\theta} = \\sum_{i=1}^{n} \\psi\\left(\\frac{y_i - a\\,\\theta}{s}\\right) \\cdot \\left(-\\frac{a}{s}\\right) = 0.\n$$\nAssuming $a \\neq 0$ and $s > 0$, this is equivalent to the estimating equation:\n$$\n\\sum_{i=1}^{n} \\psi\\left(\\frac{y_i - a\\,\\theta}{s}\\right) = 0.\n$$\nThe estimator $\\widehat{\\theta}$ can be viewed as a statistical functional $T$ evaluated at the empirical distribution $F_n$ of the data $\\{y_i\\}_{i=1}^n$, i.e., $\\widehat{\\theta} = T(F_n)$. For a general distribution $F$ of the data $Y$, the functional $T(F)$ is defined implicitly by the population version of the estimating equation:\n$$\n\\mathbb{E}_F\\left[\\psi\\left(\\frac{Y - a\\,T(F)}{s}\\right)\\right] = 0.\n$$\nLet $F_0$ be the true data distribution, where $Y = a\\theta^{\\star} + \\varepsilon$. Then $T(F_0) = \\theta^{\\star}$, since $\\mathbb{E}_{F_0}\\left[\\psi\\left(\\frac{Y - a\\theta^{\\star}}{s}\\right)\\right] = \\mathbb{E}_{\\varepsilon}\\left[\\psi\\left(\\frac{\\varepsilon}{s}\\right)\\right] = 0$. The last equality holds because the noise distribution is symmetric about zero and $\\psi$ is an odd function (as it is the derivative of an even function $\\rho$).\n\nThe influence function (IF) of the estimator $T$ at the distribution $F_0$ for a contamination point $y$ is defined as:\n$$\n\\text{IF}(y; T, F_0) = \\left. \\frac{d}{d\\epsilon} T((1-\\epsilon)F_0 + \\epsilon \\delta_y) \\right|_{\\epsilon=0},\n$$\nwhere $\\delta_y$ is the Dirac measure at $y$. Let $F_{\\epsilon} = (1-\\epsilon)F_0 + \\epsilon \\delta_y$ and $\\theta_{\\epsilon} = T(F_{\\epsilon})$. The defining equation for $\\theta_{\\epsilon}$ is:\n$$\n\\mathbb{E}_{F_{\\epsilon}}\\left[\\psi\\left(\\frac{Y - a\\,\\theta_{\\epsilon}}{s}\\right)\\right] = (1-\\epsilon)\\mathbb{E}_{F_0}\\left[\\psi\\left(\\frac{Y - a\\,\\theta_{\\epsilon}}{s}\\right)\\right] + \\epsilon \\psi\\left(\\frac{y - a\\,\\theta_{\\epsilon}}{s}\\right) = 0.\n$$\nDifferentiating this expression with respect to $\\epsilon$ and applying the chain rule gives:\n$$\n-\\mathbb{E}_{F_0}\\left[\\psi\\left(\\frac{Y - a\\,\\theta_{\\epsilon}}{s}\\right)\\right] + (1-\\epsilon)\\mathbb{E}_{F_0}\\left[\\psi'\\left(\\frac{Y - a\\,\\theta_{\\epsilon}}{s}\\right)\\left(-\\frac{a}{s}\\right)\\frac{d\\theta_{\\epsilon}}{d\\epsilon}\\right] + \\psi\\left(\\frac{y - a\\,\\theta_{\\epsilon}}{s}\\right) + \\epsilon \\frac{d}{d\\epsilon}\\left[\\dots\\right] = 0.\n$$\nWe now evaluate this at $\\epsilon = 0$. At this point, $\\theta_{\\epsilon} = \\theta^{\\star}$ and $\\left.\\frac{d\\theta_{\\epsilon}}{d\\epsilon}\\right|_{\\epsilon=0} = \\text{IF}(y; T, F_0)$. The first term is $\\mathbb{E}_{F_0}[\\psi((\\cdot)/s)] = 0$. The equation simplifies to:\n$$\n\\mathbb{E}_{F_0}\\left[\\psi'\\left(\\frac{Y - a\\,\\theta^{\\star}}{s}\\right)\\left(-\\frac{a}{s}\\right)\\text{IF}(y; T, F_0)\\right] + \\psi\\left(\\frac{y - a\\,\\theta^{\\star}}{s}\\right) = 0.\n$$\nRearranging to solve for the influence function, we get:\n$$\n\\frac{a}{s} \\text{IF}(y; T, F_0) \\mathbb{E}_{F_0}\\left[\\psi'\\left(\\frac{Y - a\\,\\theta^{\\star}}{s}\\right)\\right] = \\psi\\left(\\frac{y - a\\,\\theta^{\\star}}{s}\\right).\n$$\nThe expectation can be taken with respect to the noise distribution, as $Y - a\\theta^\\star = \\varepsilon$. Let $T_{noise} = \\varepsilon/s$ be the standardized noise.\n$$\n\\text{IF}(y; T, F_0) = \\frac{s \\cdot \\psi\\left(\\frac{y - a\\theta^{\\star}}{s}\\right)}{a \\cdot \\mathbb{E}_{T_{noise}}\\left[\\psi'(T_{noise})\\right]}.\n$$\n\n### Task 2: Analysis of Score Functions for Huber and Tukey Losses\n\nFor the Huber loss $\\rho_H(t)$:\nThe score function $\\psi_H(t) = \\frac{d}{dt}\\rho_H(t)$ is:\n$$\n\\psi_H(t) = \\begin{cases} t, & |t| \\le c, \\\\ c \\cdot \\mathrm{sgn}(t), & |t| > c. \\end{cases}\n$$\nThis can be written as $\\psi_H(t) = \\mathrm{clip}(t, -c, c) = \\max(-c, \\min(c, t))$.\nThe derivative $\\psi'_H(t)$ (defined for $t \\neq \\pm c$) is:\n$$\n\\psi'_H(t) = \\begin{cases} 1, & |t| < c, \\\\ 0, & |t| > c. \\end{cases}\n$$\nThe supremum of the absolute value of the score function is $\\sup_{t \\in \\mathbb{R}} |\\psi_H(t)| = c$.\n\nFor Tukey's bisquare loss $\\rho_T(t)$:\nThe score function $\\psi_T(t) = \\frac{d}{dt}\\rho_T(t)$ for $|t| \\le c$ is:\n$$\n\\psi_T(t) = \\frac{d}{dt} \\left(\\frac{c^2}{6}\\left[1 - \\left(1 - \\frac{t^2}{c^2}\\right)^3\\right]\\right) = \\frac{c^2}{6} \\left(-3\\left(1 - \\frac{t^2}{c^2}\\right)^2 \\left(-\\frac{2t}{c^2}\\right)\\right) = t\\left(1 - \\frac{t^2}{c^2}\\right)^2.\n$$\nSo, the full score function is:\n$$\n\\psi_T(t) = \\begin{cases} t\\left(1 - \\left(\\frac{t}{c}\\right)^2\\right)^2, & |t| \\le c, \\\\ 0, & |t| > c. \\end{cases}\n$$\nThe derivative $\\psi'_T(t)$ for $|t| < c$ is:\n$$\n\\psi'_T(t) = \\frac{d}{dt} \\left[t\\left(1 - \\frac{t^2}{c^2}\\right)^2\\right] = \\left(1 - \\frac{t^2}{c^2}\\right)^2 + t \\cdot 2\\left(1 - \\frac{t^2}{c^2}\\right)\\left(-\\frac{2t}{c^2}\\right) = \\left(1 - \\frac{t^2}{c^2}\\right)\\left(1 - \\frac{5t^2}{c^2}\\right).\n$$\nFor $|t|>c$, $\\psi'_T(t)=0$. To find the supremum of $|\\psi_T(t)|$, we find the extrema of $\\psi_T(t)$ on $[-c, c]$ by setting $\\psi'_T(t)=0$. The critical points are at $t^2=c^2$ and $t^2=c^2/5$. At $t=\\pm c$, $\\psi_T(t)=0$. The maximum value occurs at $t = \\pm c/\\sqrt{5}$, which is $|\\psi_T(\\pm c/\\sqrt{5})| = \\frac{c}{\\sqrt{5}}\\left(1-\\frac{1}{5}\\right)^2 = \\frac{c}{\\sqrt{5}}\\left(\\frac{4}{5}\\right)^2 = \\frac{16c}{25\\sqrt{5}}$.\nThus, $\\sup_{t \\in \\mathbb{R}} |\\psi_T(t)| = \\frac{16c}{25\\sqrt{5}}$.\n\n### Task 3: Redescending Influence and Breakdown Point\n\nThe influence function is proportional to the score function $\\psi$.\nAn estimator has a redescending influence function if $\\text{IF}(y) \\to 0$ as the contamination point $|y| \\to \\infty$. This implies that very large outliers are completely rejected by the estimator.\n\nFor the Huber loss, $|\\psi_H(t)|$ becomes constant ($c$) for $|t|>c$. Therefore, its influence function is bounded but does not redescend to zero. This means large outliers still have a constant, non-zero influence on the estimate.\n\nFor Tukey's bisquare loss, $\\psi_T(t) = 0$ for $|t|>c$. This means that if an observation $y_i$ is sufficiently far from the current estimate (i.e., $|y_i - a\\theta|/s > c$), it is completely ignored. The influence function for the Tukey estimator is therefore redescending.\n\nThe breakdown point (BDP) is the smallest fraction of contaminated data that can cause the estimator to take on an arbitrarily large or nonsensical value. In this one-parameter estimation problem with a known scale $s$, both the Huber and Tukey M-estimators possess the maximal BDP of $50\\%$. The contribution of any single data point to the estimating equation is bounded by $\\sup|\\psi|$. To make the sum $\\sum_i \\psi(\\dots)$ uncontrollably large or small, one must corrupt at least half of the data points to \"outvote\" the uncorrupted half.\n\nIf the scale $s$ were unknown and estimated jointly with $\\theta$, the situation changes significantly. A single large outlier $y_k \\to \\infty$ could cause the scale estimate $s$ to inflate towards infinity to accommodate it.\nFor the Huber estimator, if $s \\to \\infty$, all standardized residuals $(y_i - a\\theta)/s$ would approach zero. In the region near zero, $\\rho_H(t) \\propto t^2$, meaning the Huber estimator would behave like a standard least-squares estimator, which has a BDP of $0$ (or $1/n$). Thus, its robustness would be lost.\nFor a redescending estimator like Tukey's, an inflating scale estimate can also be problematic, as it moves all residuals into the central region where $\\rho_T(t) \\propto t^2$, again losing robustness. Furthermore, the non-convexity introduced by the redescending nature of $\\psi_T$ can lead to multiple local minima, making the joint estimation of $\\theta$ and $s$ numerically unstable. To maintain a high BDP when scale is unknown, one must use a robust scale estimator (like the Median Absolute Deviation, MAD) either separately or as part of a more complex estimation scheme (e.g., S-estimation).\n\n### Task 4: Gross-Error Sensitivity for the Huber Estimator\n\nThe gross-error sensitivity $\\gamma^{\\ast}$ is defined as the supremum of the absolute value of the influence function over all possible contamination points $y$:\n$$\n\\gamma^{\\ast} = \\sup_{y \\in \\mathbb{R}} |\\text{IF}(y; T, F_0)|.\n$$\nUsing the derived IF for the Huber estimator:\n$$\n\\gamma^{\\ast} = \\sup_{y \\in \\mathbb{R}} \\left| \\frac{s \\cdot \\psi_H\\left(\\frac{y - a\\theta^{\\star}}{s}\\right)}{a \\cdot \\mathbb{E}_{T_{noise}}\\left[\\psi'_H(T_{noise})\\right]} \\right| = \\frac{s \\cdot \\sup_{t \\in \\mathbb{R}}|\\psi_H(t)|}{|a| \\cdot \\mathbb{E}_{T_{noise}}\\left[\\psi'_H(T_{noise})\\right]}.\n$$\nFrom Task 2, we have $\\sup_{t \\in \\mathbb{R}}|\\psi_H(t)| = c$.\n\nThe denominator requires calculating the expectation of $\\psi'_H(T_{noise})$, where the standardized noise $T_{noise}=\\varepsilon/s$ is assumed to be standard normal, $T_{noise} \\sim \\mathcal{N}(0, 1)$. The function $\\psi'_H(t)$ is $1$ for $|t| < c$ and $0$ otherwise. Thus, it is the indicator function $I(|t|<c)$.\n$$\n\\mathbb{E}_{T_{noise}}\\left[\\psi'_H(T_{noise})\\right] = \\mathbb{E}_{\\mathcal{N}(0,1)}[I(|T_{noise}| < c)] = P(-c < T_{noise} < c).\n$$\nLet $\\Phi$ be the cumulative distribution function (CDF) of the standard normal distribution. Then,\n$$\nP(-c < T_{noise} < c) = \\Phi(c) - \\Phi(-c).\n$$\nDue to the symmetry of the normal distribution, $\\Phi(-c) = 1 - \\Phi(c)$. Therefore,\n$$\n\\mathbb{E}_{T_{noise}}\\left[\\psi'_H(T_{noise})\\right] = \\Phi(c) - (1 - \\Phi(c)) = 2\\Phi(c) - 1.\n$$\nSubstituting these results back into the expression for $\\gamma^{\\ast}$:\n$$\n\\gamma^{\\ast} = \\frac{s \\cdot c}{|a|(2\\Phi(c) - 1)}.\n$$", "answer": "$$\n\\boxed{\\frac{s c}{|a|(2\\Phi(c) - 1)}}\n$$", "id": "3399542"}]}