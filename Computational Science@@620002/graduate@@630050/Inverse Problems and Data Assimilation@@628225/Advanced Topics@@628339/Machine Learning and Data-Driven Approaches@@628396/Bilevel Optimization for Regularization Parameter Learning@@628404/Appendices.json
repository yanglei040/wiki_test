{"hands_on_practices": [{"introduction": "To build a solid foundation, we begin with a simplified yet highly instructive analytical exercise. By reducing a linear inverse problem to a scalar case, we can bypass the complexities of linear algebra and derive a closed-form solution for the optimal regularization parameter. This practice [@problem_id:3368781] illuminates the fundamental mechanics of bilevel optimization, showing precisely how the balance between training data fit and validation performance dictates the ideal level of regularization.", "problem": "Consider a scalar linear inverse problem where the forward operator is represented by a positive scalar $a > 0$, and the regularization operator is represented by a positive scalar $l > 0$. Given a training datum $y_{\\mathrm{tr}} > 0$ and a validation datum $y_{\\mathrm{val}} > 0$, with the assumption that $\\frac{y_{\\mathrm{tr}}}{y_{\\mathrm{val}}} > 1$, we aim to learn the regularization parameter $\\alpha \\geq 0$ via a bilevel optimization framework.\n\nThe lower-level problem is the Tikhonov-regularized least-squares estimator, which for any fixed $\\alpha \\geq 0$ solves\n$$\n\\min_{x \\in \\mathbb{R}} \\; \\frac{1}{2}\\left(a x - y_{\\mathrm{tr}}\\right)^{2} + \\frac{1}{2} \\alpha \\left(l x\\right)^{2}.\n$$\nThe upper-level problem chooses the parameter $\\alpha$ to minimize the validation data misfit,\n$$\n\\min_{\\alpha \\geq 0} \\; \\frac{1}{2}\\left(a x_{\\alpha} - y_{\\mathrm{val}}\\right)^{2},\n$$\nwhere $x_{\\alpha}$ denotes the unique minimizer of the lower-level problem for the given $\\alpha$.\n\nStarting from the definitions of least-squares estimation and Tikhonov regularization, and using first-order optimality conditions for the lower-level problem together with differentiation through the solution map of the lower-level problem, derive a closed-form analytic expression for the bilevel-optimal regularization parameter $\\alpha^{\\star}$ in terms of $a$, $l$, $y_{\\mathrm{tr}}$, and $y_{\\mathrm{val}}$. Express your final answer as a single analytic expression. No rounding is required, and no physical units apply.", "solution": "The problem statement is first validated against the required criteria.\n\n### Step 1: Extract Givens\n- Forward operator: a positive scalar $a > 0$.\n- Regularization operator: a positive scalar $l > 0$.\n- Training datum: $y_{\\mathrm{tr}} > 0$.\n- Validation datum: $y_{\\mathrm{val}} > 0$.\n- A-priori condition: $\\frac{y_{\\mathrm{tr}}}{y_{\\mathrm{val}}} > 1$.\n- Lower-level problem: $\\min_{x \\in \\mathbb{R}} \\; \\frac{1}{2}\\left(a x - y_{\\mathrm{tr}}\\right)^{2} + \\frac{1}{2} \\alpha \\left(l x\\right)^{2}$, for a fixed $\\alpha \\geq 0$.\n- Upper-level problem: $\\min_{\\alpha \\geq 0} \\; \\frac{1}{2}\\left(a x_{\\alpha} - y_{\\mathrm{val}}\\right)^{2}$, where $x_{\\alpha}$ is the solution to the lower-level problem.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem describes a standard bilevel optimization framework for learning a hyperparameter (the regularization parameter $\\alpha$) in the context of Tikhonov regularization for a linear inverse problem. This is a well-established and fundamental topic in numerical optimization, inverse problems, and machine learning.\n- **Well-Posed:** The lower-level problem is the minimization of a strictly convex quadratic function for any $\\alpha \\geq 0$ (since $a > 0, l > 0$), which guarantees a unique minimizer $x_{\\alpha}$ exists. The upper-level problem is then well-defined. The condition $\\frac{y_{\\mathrm{tr}}}{y_{\\mathrm{val}}} > 1$ is crucial, as will be shown, for ensuring the optimal $\\alpha$ is positive, thus making the problem well-structured.\n- **Objective:** The problem is expressed in precise mathematical language, with no subjective or ambiguous terms.\n- **Completeness and Consistency:** All necessary variables, constants, and conditions are provided to derive a closed-form solution. There are no internal contradictions.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\n### Solution Derivation\n\nThe derivation proceeds in three main steps:\n1.  Solve the lower-level problem to find the optimal state $x_{\\alpha}$ as a function of the regularization parameter $\\alpha$.\n2.  Substitute this solution into the upper-level problem to obtain an objective function that depends only on $\\alpha$.\n3.  Solve the upper-level problem by minimizing this objective function with respect to $\\alpha$.\n\n**Step 1: Solve the Lower-Level Problem**\n\nThe lower-level objective function is given by\n$$\nJ_{\\alpha}(x) = \\frac{1}{2}(ax - y_{\\mathrm{tr}})^2 + \\frac{1}{2}\\alpha (lx)^2.\n$$\nThis function is differentiable and convex. The unique minimizer $x_{\\alpha}$ can be found by setting the first derivative with respect to $x$ to zero (first-order optimality condition):\n$$\n\\frac{dJ_{\\alpha}}{dx} = 0.\n$$\nComputing the derivative, we get\n$$\n\\frac{dJ_{\\alpha}}{dx} = a(ax - y_{\\mathrm{tr}}) + \\alpha l^2 x = a^2 x - ay_{\\mathrm{tr}} + \\alpha l^2 x.\n$$\nSetting the derivative to zero yields\n$$\n(a^2 + \\alpha l^2)x - ay_{\\mathrm{tr}} = 0.\n$$\nSince we are given $a > 0$ and $l > 0$, the term $(a^2 + \\alpha l^2)$ is strictly positive for any $\\alpha \\geq 0$. We can therefore solve for $x$ to obtain the solution map $x_{\\alpha}$:\n$$\nx_{\\alpha} = \\frac{ay_{\\mathrm{tr}}}{a^2 + \\alpha l^2}.\n$$\n\n**Step 2: Formulate the Upper-Level Problem**\n\nThe upper-level problem is to find the value of $\\alpha$ that minimizes the validation error. The upper-level objective function is\n$$\nE(\\alpha) = \\frac{1}{2}(ax_{\\alpha} - y_{\\mathrm{val}})^2.\n$$\nSubstituting the expression for $x_{\\alpha}$ from Step 1 into $E(\\alpha)$, we obtain an objective function that depends solely on $\\alpha$:\n$$\nE(\\alpha) = \\frac{1}{2} \\left( a \\left( \\frac{ay_{\\mathrm{tr}}}{a^2 + \\alpha l^2} \\right) - y_{\\mathrm{val}} \\right)^2 = \\frac{1}{2} \\left( \\frac{a^2 y_{\\mathrm{tr}}}{a^2 + \\alpha l^2} - y_{\\mathrm{val}} \\right)^2.\n$$\n\n**Step 3: Solve the Upper-Level Problem**\n\nWe seek to find $\\alpha^{\\star}$ such that\n$$\n\\alpha^{\\star} = \\arg\\min_{\\alpha \\geq 0} E(\\alpha).\n$$\nSince $E(\\alpha)$ is non-negative, its minimum value is $0$. This minimum is achieved if we can find an $\\alpha \\geq 0$ that makes the term inside the square zero. Let's find such an $\\alpha$ by solving the equation:\n$$\n\\frac{a^2 y_{\\mathrm{tr}}}{a^2 + \\alpha l^2} - y_{\\mathrm{val}} = 0.\n$$\nRearranging the terms, we have\n$$\n\\frac{a^2 y_{\\mathrm{tr}}}{a^2 + \\alpha l^2} = y_{\\mathrm{val}}.\n$$\nAssuming $y_{\\mathrm{val}} > 0$, we can cross-multiply:\n$$\na^2 y_{\\mathrm{tr}} = y_{\\mathrm{val}} (a^2 + \\alpha l^2).\n$$\n$$\na^2 y_{\\mathrm{tr}} = a^2 y_{\\mathrm{val}} + \\alpha l^2 y_{\\mathrm{val}}.\n$$\nNow, we isolate the term containing $\\alpha$:\n$$\n\\alpha l^2 y_{\\mathrm{val}} = a^2 y_{\\mathrm{tr}} - a^2 y_{\\mathrm{val}} = a^2(y_{\\mathrm{tr}} - y_{\\mathrm{val}}).\n$$\nSince $l > 0$ and $y_{\\mathrm{val}} > 0$, we can divide by $l^2 y_{\\mathrm{val}}$ to solve for $\\alpha$:\n$$\n\\alpha^{\\star} = \\frac{a^2(y_{\\mathrm{tr}} - y_{\\mathrm{val}})}{l^2 y_{\\mathrm{val}}}.\n$$\nThis expression can be rewritten as\n$$\n\\alpha^{\\star} = \\frac{a^2}{l^2} \\left(\\frac{y_{\\mathrm{tr}} - y_{\\mathrm{val}}}{y_{\\mathrm{val}}}\\right) = \\frac{a^2}{l^2} \\left(\\frac{y_{\\mathrm{tr}}}{y_{\\mathrm{val}}} - 1\\right).\n$$\nWe must verify that this solution satisfies the constraint $\\alpha \\geq 0$. We are given that $a > 0$ and $l > 0$, so the term $\\frac{a^2}{l^2}$ is positive. The problem statement includes the explicit condition $\\frac{y_{\\mathrm{tr}}}{y_{\\mathrm{val}}} > 1$, which implies that the term $\\left(\\frac{y_{\\mathrm{tr}}}{y_{\\mathrm{val}}} - 1\\right)$ is strictly positive. Therefore, $\\alpha^{\\star} > 0$.\n\nThis value $\\alpha^{\\star}$ yields $E(\\alpha^{\\star}) = 0$. Since $E(\\alpha) \\geq 0$ for all $\\alpha$, this is the global minimum of the function. Therefore, this is the bilevel-optimal regularization parameter.\n\nThe final analytic expression for the optimal regularization parameter $\\alpha^{\\star}$ is:\n$$\n\\alpha^{\\star} = \\frac{a^2}{l^2} \\left(\\frac{y_{\\mathrm{tr}}}{y_{\\mathrm{val}}} - 1\\right).\n$$", "answer": "$$\n\\boxed{\\frac{a^{2}}{l^{2}} \\left( \\frac{y_{\\mathrm{tr}}}{y_{\\mathrm{val}}} - 1 \\right)}\n$$", "id": "3368781"}, {"introduction": "Moving from analytical insight to practical application, this next exercise guides you through the numerical implementation of a bilevel learning framework for a classic Tikhonov-regularized inverse problem. You will translate the theoretical concepts into a working program, solving the lower-level problem via normal equations and employing a numerical optimizer to find the ideal regularization parameter for the upper-level objective. This hands-on problem [@problem_id:3368812] is crucial for developing the core computational skills needed to apply bilevel optimization to real-world scientific models.", "problem": "Consider a linear inverse problem with a deterministic observation model and quadratic regularization. Let $A \\in \\mathbb{R}^{m \\times n}$ denote a known forward operator, $x \\in \\mathbb{R}^{n}$ the unknown state, and $y \\in \\mathbb{R}^{m}$ a given observation. The lower-level reconstruction for a given regularization parameter $\\lambda \\in \\mathbb{R}_{>0}$ is defined as the unique minimizer $x_{\\lambda}$ of the Tikhonov functional\n$$\n\\Phi_{\\lambda}(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\frac{1}{2}\\lambda \\|L x\\|_{2}^{2},\n$$\nwhere $L \\in \\mathbb{R}^{p \\times n}$ is a known linear operator that encodes prior structure (for example, a discrete derivative operator). The bilevel optimization problem aims to learn the regularization parameter $\\lambda$ by minimizing a validation objective with access to a known reference state $x^{\\star} \\in \\mathbb{R}^{n}$:\n$$\nJ(\\lambda) = \\frac{1}{2}\\|x_{\\lambda} - x^{\\star}\\|_{2}^{2}.\n$$\nYou must design and implement a complete, runnable program that, for each specified test case, numerically solves the bilevel optimization problem in the scalar variable $\\lambda$ to produce an optimal value $\\lambda^{\\star}$ within prescribed bounds. The program shall solve the lower-level problem exactly for each candidate $\\lambda$ and then minimize the upper-level objective over the given interval, using scientifically sound numerical methods.\n\nBase assumptions and core definitions to use:\n- The observation physics are modeled as $y = A x^{\\star} + \\varepsilon$ with deterministic $\\varepsilon \\in \\mathbb{R}^{m}$.\n- The regularization operator $L$ encodes smoothness via discrete first differences unless otherwise specified.\n- The lower-level minimizer $x_{\\lambda}$ is uniquely defined by the strict convexity of $\\Phi_{\\lambda}$ when $\\lambda > 0$ and $A^{\\top}A + \\lambda L^{\\top}L$ is positive definite.\n- All computations are in real-valued Euclidean spaces with the standard $2$-norm.\n\nYour program must implement the following, from first principles:\n1. Construct each matrix $A$ and operator $L$ exactly as specified.\n2. For any given $\\lambda$, compute $x_{\\lambda}$ by solving the linear system derived from the first-order optimality condition of $\\Phi_{\\lambda}$.\n3. Define the upper-level objective $J(\\lambda)$ as stated, evaluate it for any $\\lambda$, and carry out a robust one-dimensional minimization over the specified interval to obtain $\\lambda^{\\star}$ for each test case.\n\nTest suite and data specification:\n- For all test cases, the search interval for the regularization parameter is $\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]$ with $\\lambda_{\\min} = 10^{-6}$ and $\\lambda_{\\max} = 10^{2}$.\n\n- Test Case $1$ (general well-posed blur with mild noise; happy path):\n  - Dimensions: $n = 12$, $m = 18$.\n  - Forward operator construction: define a Gaussian blur matrix $A \\in \\mathbb{R}^{m \\times n}$ with entries\n    $$\n    A_{ij} = \\exp\\left(-\\frac{\\left(\\frac{i-1}{m-1} - \\frac{j-1}{n-1}\\right)^{2}}{2\\sigma^{2}}\\right), \\quad \\sigma = 0.8,\n    $$\n    followed by row-wise normalization: for each $i \\in \\{1,\\dots,m\\}$, replace the $i$-th row by $A_{i\\cdot}/\\left(\\sum_{j=1}^{n} A_{ij}\\right)$.\n  - Regularization operator: $L \\in \\mathbb{R}^{(n-1) \\times n}$ is the first-difference matrix with entries $L_{k,k} = -1$, $L_{k,k+1} = 1$ for $k \\in \\{1,\\dots,n-1\\}$, and all other entries $0$.\n  - Reference state: $x^{\\star} \\in \\mathbb{R}^{n}$ with components\n    $$\n    x^{\\star}_{j} = \\sin\\left(\\frac{2\\pi (j-1)}{n}\\right) + 0.2 \\sin\\left(\\frac{4\\pi (j-1)}{n}\\right), \\quad j \\in \\{1,\\dots,n\\}.\n    $$\n  - Observation: $y = A x^{\\star} + \\varepsilon$ with deterministic noise $\\varepsilon \\in \\mathbb{R}^{m}$ given by $\\varepsilon_{i} = 0.01 \\cos\\left(\\frac{2\\pi (i-1)}{m}\\right)$.\n\n- Test Case $2$ (stronger blur and higher noise; tests sensitivity):\n  - Dimensions: $n = 12$, $m = 18$.\n  - Forward operator construction: $A \\in \\mathbb{R}^{m \\times n}$ defined as in Test Case $1$ but with $\\sigma = 1.5$, then apply the same row-wise normalization.\n  - Regularization operator: same first-difference $L \\in \\mathbb{R}^{(n-1) \\times n}$ as in Test Case $1$.\n  - Reference state: the same $x^{\\star} \\in \\mathbb{R}^{n}$ as in Test Case $1$.\n  - Observation: $y = A x^{\\star} + \\varepsilon$ with noise $\\varepsilon_{i} = 0.05 \\cos\\left(\\frac{2\\pi (i-1)}{m}\\right)$.\n\n- Test Case $3$ (identity forward model and identity regularization; boundary-behavior check):\n  - Dimensions: $n = 6$, $m = 6$.\n  - Forward operator: $A = I_{n}$, the $n \\times n$ identity matrix.\n  - Regularization operator: $L = I_{n}$.\n  - Reference state: $x^{\\star} = [1, 0.5, -0.3, 0.2, -0.1, 0]^{\\top}$.\n  - Observation: $y = [1.2, 0.4, -0.25, 0.1, -0.15, 0.05]^{\\top}$.\n\n- Test Case $4$ (rank-deficient forward operator; tests stability from regularization):\n  - Dimensions: $n = 10$, $m = 10$.\n  - Forward operator: $A = \\operatorname{diag}(d) \\in \\mathbb{R}^{n \\times n}$ with\n    $$\n    d = [1, 1, 1, 1, 1, 0, 0, 0.5, 0.5, 0.5]^{\\top}.\n    $$\n  - Regularization operator: first-difference $L \\in \\mathbb{R}^{(n-1) \\times n}$ as in Test Case $1$.\n  - Reference state: $x^{\\star}_{j} = \\sin\\left(\\frac{\\pi (j-1)}{n-1}\\right)$ for $j \\in \\{1,\\dots,n\\}$.\n  - Observation: $y = A x^{\\star} + \\varepsilon$ with $\\varepsilon_{i} = 0.02 (-1)^{i-1}$.\n\nComputational requirements:\n- For each test case, minimize $J(\\lambda)$ over $\\lambda \\in [10^{-6}, 10^{2}]$.\n- For any trial $\\lambda$, compute $x_{\\lambda}$ exactly by solving the linear system implied by the first-order condition of $\\Phi_{\\lambda}$, using numerically stable linear algebra.\n- Produce the final learned parameter values $\\lambda^{\\star}$ as floating-point numbers rounded to six decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[r_{1},r_{2},r_{3},r_{4}]$, where each $r_{k}$ is the learned $\\lambda^{\\star}$ for the $k$-th test case rounded to six decimal places.\n- No additional text or lines are permitted in the output.", "solution": "We begin from the core definitions for linear inverse problems and quadratic regularization, and derive both the lower-level solution for the Tikhonov functional and the upper-level bilevel formulation for learning the regularization parameter.\n\nThe lower-level objective is given by\n$$\n\\Phi_{\\lambda}(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\frac{1}{2}\\lambda \\|L x\\|_{2}^{2},\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $L \\in \\mathbb{R}^{p \\times n}$, $x \\in \\mathbb{R}^{n}$, $y \\in \\mathbb{R}^{m}$, and $\\lambda \\in \\mathbb{R}_{>0}$. The functional $\\Phi_{\\lambda}$ is strictly convex in $x$ under the condition that $A^{\\top}A + \\lambda L^{\\top}L$ is positive definite. This is guaranteed when $\\lambda > 0$ and $L$ penalizes the nullspace components of $A$ sufficiently to regularize the problem.\n\nTo compute the minimizer $x_{\\lambda}$, we invoke first principles in convex optimization: at the minimizer, the gradient with respect to $x$ vanishes. The gradient of $\\Phi_{\\lambda}$ is\n$$\n\\nabla_{x} \\Phi_{\\lambda}(x) = A^{\\top}(A x - y) + \\lambda L^{\\top}(L x).\n$$\nSetting $\\nabla_{x} \\Phi_{\\lambda}(x) = 0$ yields the normal equations\n$$\n\\left(A^{\\top}A + \\lambda L^{\\top}L\\right) x = A^{\\top} y.\n$$\nThus, the unique minimizer $x_{\\lambda}$ is obtained by solving the linear system\n$$\nH(\\lambda) x_{\\lambda} = A^{\\top} y, \\quad \\text{where } H(\\lambda) = A^{\\top}A + \\lambda L^{\\top}L.\n$$\nBecause $H(\\lambda)$ is symmetric positive definite for $\\lambda > 0$ given the problem structure, numerical linear algebra methods such as Cholesky factorization or a stable direct solver may be used to compute $x_{\\lambda}$.\n\nThe bilevel optimization defines an upper-level objective that measures the discrepancy between the reconstructed state and a known reference state $x^{\\star}$:\n$$\nJ(\\lambda) = \\frac{1}{2}\\|x_{\\lambda} - x^{\\star}\\|_{2}^{2}.\n$$\nOur goal is to compute\n$$\n\\lambda^{\\star} = \\operatorname{argmin}_{\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]} J(\\lambda),\n$$\nwhere the interval $[\\lambda_{\\min}, \\lambda_{\\max}]$ is specified for each test case as $[10^{-6}, 10^{2}]$.\n\nA principle-based algorithm for this bilevel problem uses the following steps:\n- For any candidate $\\lambda$, form the matrix $H(\\lambda) = A^{\\top} A + \\lambda L^{\\top} L$ and the right-hand side $b = A^{\\top} y$.\n- Solve $H(\\lambda) x_{\\lambda} = b$ exactly using a numerically stable linear solver.\n- Evaluate the upper-level objective $J(\\lambda) = \\frac{1}{2}\\|x_{\\lambda} - x^{\\star}\\|_{2}^{2}$.\n- Minimize $J(\\lambda)$ over $\\lambda \\in [10^{-6}, 10^{2}]$ by robust one-dimensional numerical optimization (for example, bracketing and golden-section search or bounded scalar minimization), exploiting the continuity and differentiability of $J$ in $\\lambda$ induced by the implicit dependence of $x_{\\lambda}$ on $\\lambda$.\n\nFor completeness, we outline the implicit differentiation that would yield a gradient for $J(\\lambda)$, even though a derivative-free bounded minimization is sufficient and numerically robust here. Differentiating the normal equations with respect to $\\lambda$:\n$$\n\\frac{d}{d\\lambda}\\left(H(\\lambda) x_{\\lambda}\\right) = \\frac{d}{d\\lambda}\\left(A^{\\top}y\\right) = 0,\n$$\nwhich gives\n$$\n\\left(\\frac{dH}{d\\lambda}\\right) x_{\\lambda} + H(\\lambda) \\frac{d x_{\\lambda}}{d\\lambda} = 0.\n$$\nSince $\\frac{dH}{d\\lambda} = L^{\\top}L$, we obtain\n$$\nH(\\lambda) \\frac{d x_{\\lambda}}{d\\lambda} = - L^{\\top}L\\, x_{\\lambda},\n$$\nand therefore\n$$\n\\frac{d x_{\\lambda}}{d\\lambda} = - H(\\lambda)^{-1} L^{\\top}L\\, x_{\\lambda}.\n$$\nLet $r(\\lambda) = x_{\\lambda} - x^{\\star}$. The gradient of the upper-level objective is\n$$\n\\frac{dJ}{d\\lambda} = r(\\lambda)^{\\top} \\frac{d x_{\\lambda}}{d\\lambda} = - r(\\lambda)^{\\top} H(\\lambda)^{-1} L^{\\top}L\\, x_{\\lambda}.\n$$\nThis expression follows from the chain rule and the linearity of the quadratic forms. In practice, one can compute $H(\\lambda)^{-1} L^{\\top}L\\, x_{\\lambda}$ by solving a linear system with the same matrix $H(\\lambda)$ and right-hand side $L^{\\top}L\\, x_{\\lambda}$, reusing factorizations for efficiency.\n\nHowever, given the scalar nature of $\\lambda$ and the smoothness of $J(\\lambda)$, a bounded, derivative-free scalar minimization suffices and avoids potential numerical subtleties with gradient accuracy. We therefore proceed with a bounded minimization strategy that evaluates $J(\\lambda)$ across the interval and converges to $\\lambda^{\\star}$.\n\nImplementation details per test case follow the scientific construction mandated:\n- For the Gaussian blur operator in Test Case $1$ and Test Case $2$, we construct $A$ using the prescribed formula:\n  $$\n  A_{ij} = \\exp\\left(-\\frac{\\left(\\frac{i-1}{m-1} - \\frac{j-1}{n-1}\\right)^{2}}{2\\sigma^{2}}\\right),\n  $$\n  and then normalize each row $i$ by dividing by $\\sum_{j=1}^{n} A_{ij}$ to maintain consistent scaling across measurements. This models a smooth convolution-like forward model between continuous grids mapped to discrete matrices, consistent with well-posed numerical discretizations.\n- The first-difference operator $L$ is constructed as $L_{k,k} = -1$ and $L_{k,k+1} = 1$ for $k \\in \\{1,\\dots,n-1\\}$, yielding a discrete gradient penalty that enforces smoothness.\n- The reference states and deterministic noise are defined by trigonometric sequences as specified, ensuring a reproducible, scientifically meaningful test suite.\n\nNumerical algorithm summary for each test case:\n- Define $A$, $L$, $x^{\\star}$, and $y$ exactly.\n- For $\\lambda \\in [10^{-6}, 10^{2}]$, compute $x_{\\lambda}$ by solving $H(\\lambda) x_{\\lambda} = A^{\\top} y$.\n- Evaluate $J(\\lambda) = \\frac{1}{2}\\|x_{\\lambda} - x^{\\star}\\|_{2}^{2}$.\n- Minimize $J(\\lambda)$ with a bounded scalar optimization method to obtain $\\lambda^{\\star}$.\n- Round $\\lambda^{\\star}$ to six decimal places.\n\nThis approach integrates foundational principles from inverse problems and bilevel optimization: strict convexity of the lower-level problem, linear system solution via normal equations, and scalar upper-level parameter learning by minimizing a smooth validation objective. The final program adheres to the specified execution environment and produces a single-line output containing the learned parameters for all test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import minimize_scalar\n\ndef first_difference_matrix(n: int) -> np.ndarray:\n    \"\"\"Construct the (n-1) x n first-difference matrix L.\"\"\"\n    L = np.zeros((n - 1, n))\n    for k in range(n - 1):\n        L[k, k] = -1.0\n        L[k, k + 1] = 1.0\n    return L\n\ndef gaussian_blur_matrix(m: int, n: int, sigma: float) -> np.ndarray:\n    \"\"\"Construct an m x n Gaussian blur matrix with row-wise normalization.\"\"\"\n    A = np.zeros((m, n))\n    # Grid points scaled to [0,1]\n    for i in range(m):\n        xi = i / (m - 1) if m > 1 else 0.0\n        for j in range(n):\n            xj = j / (n - 1) if n > 1 else 0.0\n            diff = xi - xj\n            A[i, j] = np.exp(- (diff * diff) / (2.0 * sigma * sigma))\n    # Row-wise normalization to keep consistent measurement scale\n    row_sums = A.sum(axis=1, keepdims=True)\n    # Avoid division by zero; if a row sum is zero, keep the row as zeros\n    row_sums[row_sums == 0.0] = 1.0\n    A = A / row_sums\n    return A\n\ndef solve_lower_level(A: np.ndarray, L: np.ndarray, y: np.ndarray, lam: float) -> np.ndarray:\n    \"\"\"Solve (A^T A + lam L^T L) x = A^T y for x.\"\"\"\n    AtA = A.T @ A\n    LtL = L.T @ L\n    H = AtA + lam * LtL\n    b = A.T @ y\n    # Use a stable solver; for small sizes, numpy.linalg.solve is sufficient\n    x = np.linalg.solve(H, b)\n    return x\n\ndef upper_objective(lam: float, A: np.ndarray, L: np.ndarray, y: np.ndarray, x_star: np.ndarray) -> float:\n    \"\"\"Compute J(lam) = 0.5 * ||x_lam - x_star||^2.\"\"\"\n    x_lam = solve_lower_level(A, L, y, lam)\n    r = x_lam - x_star\n    return 0.5 * float(r.T @ r)\n\ndef learn_lambda(A: np.ndarray, L: np.ndarray, y: np.ndarray, x_star: np.ndarray,\n                 lam_min: float = 1e-6, lam_max: float = 1e2) -> float:\n    \"\"\"Minimize the upper-level objective over [lam_min, lam_max] to learn lambda.\"\"\"\n    # Use bounded scalar minimization for robust 1D search\n    res = minimize_scalar(\n        lambda lam: upper_objective(lam, A, L, y, x_star),\n        bounds=(lam_min, lam_max),\n        method='bounded',\n        options={'xatol': 1e-12, 'maxiter': 500}\n    )\n    return float(res.x)\n\ndef test_case_1():\n    # n = 12, m = 18, sigma = 0.8\n    n, m, sigma = 12, 18, 0.8\n    A = gaussian_blur_matrix(m, n, sigma)\n    L = first_difference_matrix(n)\n    # x_star[j] = sin(2*pi*(j-1)/n) + 0.2 * sin(4*pi*(j-1)/n)\n    j_idx = np.arange(n)\n    x_star = np.sin(2.0 * np.pi * (j_idx) / n) + 0.2 * np.sin(4.0 * np.pi * (j_idx) / n)\n    # y = A x_star + eps, eps[i] = 0.01 * cos(2*pi*(i-1)/m)\n    i_idx = np.arange(m)\n    eps = 0.01 * np.cos(2.0 * np.pi * (i_idx) / m)\n    y = A @ x_star + eps\n    lam = learn_lambda(A, L, y, x_star)\n    return lam\n\ndef test_case_2():\n    # n = 12, m = 18, sigma = 1.5\n    n, m, sigma = 12, 18, 1.5\n    A = gaussian_blur_matrix(m, n, sigma)\n    L = first_difference_matrix(n)\n    # x_star as in test case 1\n    j_idx = np.arange(n)\n    x_star = np.sin(2.0 * np.pi * (j_idx) / n) + 0.2 * np.sin(4.0 * np.pi * (j_idx) / n)\n    # y = A x_star + eps, eps[i] = 0.05 * cos(2*pi*(i-1)/m)\n    i_idx = np.arange(m)\n    eps = 0.05 * np.cos(2.0 * np.pi * (i_idx) / m)\n    y = A @ x_star + eps\n    lam = learn_lambda(A, L, y, x_star)\n    return lam\n\ndef test_case_3():\n    # n = 6, m = 6, A = I, L = I\n    n = 6\n    A = np.eye(n)\n    L = np.eye(n)\n    x_star = np.array([1.0, 0.5, -0.3, 0.2, -0.1, 0.0], dtype=float)\n    y = np.array([1.2, 0.4, -0.25, 0.1, -0.15, 0.05], dtype=float)\n    lam = learn_lambda(A, L, y, x_star)\n    return lam\n\ndef test_case_4():\n    # n = 10, m = 10, A = diag(d) with rank deficiency\n    n = 10\n    d = np.array([1, 1, 1, 1, 1, 0, 0, 0.5, 0.5, 0.5], dtype=float)\n    A = np.diag(d)\n    L = first_difference_matrix(n)\n    j_idx = np.arange(n)\n    x_star = np.sin(np.pi * (j_idx) / (n - 1))\n    i_idx = np.arange(n)\n    eps = 0.02 * ((-1.0) ** i_idx)\n    y = A @ x_star + eps\n    lam = learn_lambda(A, L, y, x_star)\n    return lam\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        test_case_1,\n        test_case_2,\n        test_case_3,\n        test_case_4,\n    ]\n\n    results = []\n    for case_fn in test_cases:\n        lam_opt = case_fn()\n        results.append(lam_opt)\n\n    # Format results rounded to six decimals\n    formatted = [f\"{r:.6f}\" for r in results]\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted)}]\")\n\nsolve()\n```", "id": "3368812"}, {"introduction": "This advanced practice addresses a central feature of modern inverse problems: nonsmooth regularization. We will explore how to learn the parameter for an $\\ell_1$-regularized problem, which is widely used to promote sparsity in solutions. The core of this exercise [@problem_id:3368774] is to implement a hypergradient computation based on the Karush–Kuhn–Tucker (KKT) conditions and an active-set strategy, providing you with a powerful technique to handle a broad class of nonsmooth optimization challenges common in machine learning and data assimilation.", "problem": "You are given a bilevel optimization setup for learning a regularization parameter in a linear inverse problem with a nonsmooth penalty. The inner problem seeks the unique minimizer $x^\\star(\\lambda)$ of the strongly convex composite objective\n$$\n\\min_{x \\in \\mathbb{R}^n} \\;\\; \\frac{1}{2}\\|A x - y_{\\mathrm{tr}}\\|_2^2 + \\frac{\\gamma}{2}\\|x\\|_2^2 + \\lambda \\|x\\|_1,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ is a known forward operator, $y_{\\mathrm{tr}} \\in \\mathbb{R}^m$ is given training data, $\\gamma \\in \\mathbb{R}_{>0}$ is a positive parameter ensuring strong convexity, and $\\lambda \\in \\mathbb{R}_{\\ge 0}$ is the regularization weight to be learned. The outer objective evaluates the learned estimator on validation data via\n$$\nJ(\\lambda) \\;=\\; \\frac{1}{2}\\|B x^\\star(\\lambda) - y_{\\mathrm{val}}\\|_2^2,\n$$\nwith $B \\in \\mathbb{R}^{p \\times n}$ and $y_{\\mathrm{val}} \\in \\mathbb{R}^p$ given.\n\nYour task is to implement a program that computes the derivative $\\frac{dJ}{d\\lambda}$ using Karush–Kuhn–Tucker (KKT)-based differentiation under a fixed active-set assumption for the nonsmooth $\\ell_1$ term. The procedure must be scientifically founded on the following principles:\n- The composite inner objective is strongly convex, hence $x^\\star(\\lambda)$ is unique for each $\\lambda$.\n- The subdifferential of the $\\ell_1$ norm is given by $\\partial \\|x\\|_1 = \\{ z \\in \\mathbb{R}^n \\,|\\, z_i = \\operatorname{sign}(x_i) \\text{ if } x_i \\neq 0; \\, z_i \\in [-1,1] \\text{ if } x_i = 0 \\}$.\n- On regions of $\\lambda$ where the support and signs of $x^\\star(\\lambda)$ are constant, the KKT conditions yield a differentiable implicit relation that can be used to compute $\\frac{d x^\\star}{d\\lambda}$ by solving a linear system restricted to the active set.\n- If the support is empty, then $x^\\star(\\lambda) = 0$ and the derivative $\\frac{d x^\\star}{d\\lambda} = 0$.\n\nAlgorithmic requirements:\n1. Solve the inner problem for a given $\\lambda$ using a first-order method appropriate for a nonsmooth penalty, such as the Iterative Shrinkage-Thresholding Algorithm (ISTA), to obtain $x^\\star(\\lambda)$.\n2. Identify the active set $S = \\{ i \\in \\{1,\\dots,n\\} \\,|\\, x^\\star_i(\\lambda) \\neq 0\\}$ and the corresponding sign vector $s_S = \\operatorname{sign}(x^\\star_S(\\lambda))$.\n3. Assuming the active set and signs are fixed locally in $\\lambda$, compute $\\frac{d x^\\star}{d\\lambda}$ on $S$ by solving the appropriate linear system implied by the KKT conditions restricted to $S$. Set $\\frac{d x^\\star_i}{d\\lambda} = 0$ for $i \\notin S$. Use this sensitivity and the chain rule to obtain $\\frac{dJ}{d\\lambda}$.\n4. In the special case $S = \\emptyset$, set $\\frac{dJ}{d\\lambda} = 0$.\n\nUse the following fixed data for all test cases:\n- Dimensions: $m=5$, $n=4$, $p=4$.\n- Matrix $A$:\n$$\nA = \\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n1 & 1 & 1 & 1\n\\end{bmatrix}.\n$$\n- Training data $y_{\\mathrm{tr}}$:\n$$\ny_{\\mathrm{tr}} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 2 \\\\ 0.5 \\\\ 0 \\end{bmatrix}.\n$$\n- Strong convexity parameter $\\gamma$:\n$$\n\\gamma = 0.2.\n$$\n- Matrix $B$ (identity in $\\mathbb{R}^{4 \\times 4}$):\n$$\nB = I_4.\n$$\n- Validation data $y_{\\mathrm{val}}$:\n$$\ny_{\\mathrm{val}} = \\begin{bmatrix} 0.9 \\\\ -1.2 \\\\ 1.8 \\\\ 0.4 \\end{bmatrix}.\n$$\n\nTest suite for $\\lambda$:\n- Case 1 (happy path, mixed support): $\\lambda = 0.3$.\n- Case 2 (boundary, empty support expected): $\\lambda = 100.0$.\n- Case 3 (low regularization, full support likely): $\\lambda = 0.01$.\n\nYour program must:\n- Implement the above algorithm, including the inner solver and KKT-based differentiation.\n- Produce a single line of output containing the derivative values for the three test cases, formatted as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\").\n- Each result must be a floating-point number (unitless).\n\nNo user input is required. Your implementation must be deterministic and self-contained.", "solution": "The problem is valid as it is scientifically grounded in the established field of bilevel optimization for inverse problems, is mathematically well-posed, and all provided data and conditions are complete, consistent, and objective.\n\nThe task is to compute the derivative of an outer objective function, $J(\\lambda)$, with respect to a regularization parameter, $\\lambda$. The parameter $\\lambda$ is part of an inner optimization problem that defines an intermediate variable $x^\\star(\\lambda)$.\n\nThe inner optimization problem is to find the unique minimizer $x^\\star(\\lambda)$ of a composite objective function $F(x, \\lambda)$:\n$$\nx^\\star(\\lambda) = \\arg\\min_{x \\in \\mathbb{R}^n} F(x, \\lambda)\n$$\nwhere\n$$\nF(x, \\lambda) = \\underbrace{\\frac{1}{2}\\|A x - y_{\\mathrm{tr}}\\|_2^2 + \\frac{\\gamma}{2}\\|x\\|_2^2}_{f(x)} + \\underbrace{\\lambda \\|x\\|_1}_{g(x, \\lambda)}.\n$$\nThe function $f(x)$ is smooth and strongly convex, as $\\gamma > 0$, and $g(x, \\lambda)$ is convex but nonsmooth. The strong convexity of $F(x, \\lambda)$ guarantees that the minimizer $x^\\star(\\lambda)$ is unique for any given $\\lambda \\ge 0$.\n\nThe outer objective function $J(\\lambda)$ is defined as:\n$$\nJ(\\lambda) = \\frac{1}{2}\\|B x^\\star(\\lambda) - y_{\\mathrm{val}}\\|_2^2.\n$$\nWe aim to compute the derivative $\\frac{dJ}{d\\lambda}$.\n\nThe overall procedure involves three main steps:\n1.  Solve the inner optimization problem to find $x^\\star(\\lambda)$ for a given $\\lambda$.\n2.  Compute the sensitivity of the solution, $\\frac{dx^\\star}{d\\lambda}$, by differentiating the optimality conditions of the inner problem.\n3.  Use the chain rule to compute the final derivative $\\frac{dJ}{d\\lambda}$.\n\n**Step 1: Solving the Inner Problem**\n\nThe inner objective is a sum of a smooth, differentiable function $f(x)$ and a nonsmooth, proximable function $g(x, \\lambda)$. This structure is ideal for a proximal gradient method, such as the Iterative Shrinkage-Thresholding Algorithm (ISTA).\n\nThe gradient of the smooth part $f(x)$ is:\n$$\n\\nabla f(x) = A^T(Ax - y_{\\mathrm{tr}}) + \\gamma x.\n$$\nThe proximal operator of the nonsmooth part $g(x, \\lambda) = \\lambda \\|x\\|_1$ is the soft-thresholding operator, $S_{\\alpha\\lambda}(\\cdot)$, defined as:\n$$\n\\text{prox}_{\\alpha g(\\cdot, \\lambda)}(v) = S_{\\alpha\\lambda}(v) = \\text{sign}(v) \\odot \\max(|v| - \\alpha\\lambda, 0),\n$$\nwhere operations are element-wise. The parameter $\\alpha$ is a step size.\n\nThe ISTA update rule is given by:\n$$\nx_{k+1} = S_{\\alpha\\lambda}(x_k - \\alpha \\nabla f(x_k)).\n$$\nFor convergence, the step size $\\alpha$ must satisfy $0 < \\alpha < 2/L$, where $L$ is the Lipschitz constant of $\\nabla f(x)$. A common and safe choice is $\\alpha = 1/L$. The Lipschitz constant $L$ is the largest eigenvalue of the Hessian of $f(x)$, which is:\n$$\nH = \\nabla^2 f(x) = A^T A + \\gamma I.\n$$\nSince $H$ is a constant, positive-definite matrix, $L = \\lambda_{\\max}(H)$, which can be pre-computed. We iterate the ISTA update rule starting from an initial guess (e.g., $x_0 = 0$) until convergence to obtain $x^\\star(\\lambda)$.\n\n**Step 2: Differentiating the Solution Map $x^\\star(\\lambda)$**\n\nThe Karush-Kuhn-Tucker (KKT) optimality conditions for the inner problem state that at the solution $x^\\star = x^\\star(\\lambda)$, there must exist a vector $z \\in \\partial \\|x^\\star\\|_1$ such that:\n$$\n\\nabla f(x^\\star) + \\lambda z = 0.\n$$\nThe subdifferential of the $\\ell_1$-norm is $\\partial \\|x\\|_1 = \\{ z \\in \\mathbb{R}^n \\,|\\, z_i = \\operatorname{sign}(x_i) \\text{ if } x_i \\neq 0; \\, z_i \\in [-1,1] \\text{ if } x_i = 0 \\}$.\n\nLet $S = \\{i \\mid x^\\star_i \\neq 0\\}$ be the active set and $N = \\{i \\mid x^\\star_i = 0\\}$ be the inactive set. For indices $i \\in S$, the KKT condition becomes an equality: $(\\nabla f(x^\\star))_i + \\lambda \\operatorname{sign}(x_i^\\star) = 0$.\n\nWe assume that for a small perturbation in $\\lambda$, the active set $S$ and the signs of its components, $s_S = \\operatorname{sign}(x^\\star_S)$, remain constant. This implies that for $i \\in N$, $x_i^\\star$ remains zero, so $\\frac{dx^\\star_i}{d\\lambda} = 0$. We can differentiate the KKT equality condition for $i \\in S$ with respect to $\\lambda$:\n$$\n\\frac{d}{d\\lambda} \\left[ (\\nabla f(x^\\star(\\lambda)))_i + \\lambda s_i \\right] = 0, \\quad \\forall i \\in S.\n$$\nUsing the chain rule, this becomes:\n$$\n\\sum_{j=1}^n \\frac{\\partial (\\nabla f(x^\\star))_i}{\\partial x_j} \\frac{dx^\\star_j}{d\\lambda} + s_i = 0.\n$$\nThe term $\\frac{\\partial (\\nabla f(x^\\star))_i}{\\partial x_j}$ is simply the $(i, j)$-th entry of the Hessian matrix $H = A^TA + \\gamma I$. Since we assume $\\frac{dx^\\star_j}{d\\lambda} = 0$ for $j \\notin S$, the sum reduces to indices in $S$:\n$$\n\\sum_{j \\in S} H_{ij} \\frac{dx^\\star_j}{d\\lambda} + s_i = 0, \\quad \\forall i \\in S.\n$$\nThis forms a linear system of equations for the derivative components on the active set, which we denote by the vector $\\frac{dx^\\star_S}{d\\lambda}$:\n$$\nH_{SS} \\frac{dx^\\star_S}{d\\lambda} = -s_S,\n$$\nwhere $H_{SS}$ is the submatrix of $H$ with rows and columns corresponding to the active set $S$. Since $H$ is positive definite, $H_{SS}$ is also positive definite and thus invertible. We can solve this system to find $\\frac{dx^\\star_S}{d\\lambda}$. The full derivative vector $\\frac{dx^\\star}{d\\lambda}$ is then formed by placing these solution components at the active indices and zeros at the inactive indices.\n\nIf the active set $S$ is empty (i.e., $x^\\star(\\lambda)=0$), then $\\frac{dx^\\star}{d\\lambda} = 0$.\n\n**Step 3: Computing the Outer Objective Gradient**\n\nFinally, we apply the chain rule to the outer objective $J(\\lambda)$:\n$$\n\\frac{dJ}{d\\lambda} = \\nabla_x J(x^\\star(\\lambda))^T \\frac{dx^\\star}{d\\lambda}.\n$$\nThe gradient of $J$ with respect to $x$ is:\n$$\n\\nabla_x J(x) = \\nabla_x \\left( \\frac{1}{2}\\|Bx - y_{\\mathrm{val}}\\|_2^2 \\right) = B^T(Bx - y_{\\mathrm{val}}).\n$$\nSubstituting $x = x^\\star(\\lambda)$ and the computed derivative $\\frac{dx^\\star}{d\\lambda}$, we obtain the final result:\n$$\n\\frac{dJ}{d\\lambda} = (B^T(Bx^\\star(\\lambda) - y_{\\mathrm{val}}))^T \\frac{dx^\\star}{d\\lambda}.\n$$\nIf the active set is empty, $\\frac{dx^\\star}{d\\lambda}$ is the zero vector, which correctly implies $\\frac{dJ}{d\\lambda}=0$. This completes the derivation of the algorithm.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the derivative of the outer objective J with respect to lambda\n    for a bilevel optimization problem using KKT-based differentiation.\n    \"\"\"\n    \n    # Define the fixed data from the problem statement\n    m, n, p = 5, 4, 4\n    A = np.array([\n        [1, 0, 0, 0],\n        [0, 1, 0, 0],\n        [0, 0, 1, 0],\n        [0, 0, 0, 1],\n        [1, 1, 1, 1]\n    ], dtype=np.float64)\n    \n    y_train = np.array([1, -1, 2, 0.5, 0], dtype=np.float64)\n    gamma = 0.2\n    \n    # B is the 4x4 identity matrix\n    B = np.eye(p, dtype=np.float64)\n    y_val = np.array([0.9, -1.2, 1.8, 0.4], dtype=np.float64)\n\n    # Test cases for lambda\n    test_lambdas = [0.3, 100.0, 0.01]\n\n    # --- Pre-computation ---\n    # Hessian of the smooth part of the inner objective\n    H = A.T @ A + gamma * np.eye(n, dtype=np.float64)\n    # Lipschitz constant for ISTA step size\n    L = np.max(np.linalg.eigvalsh(H))\n    # ISTA step size\n    alpha = 1.0 / L\n\n    # Helper function for the soft-thresholding operator\n    def soft_threshold(v, t):\n        return np.sign(v) * np.maximum(np.abs(v) - t, 0.0)\n\n    # Helper function for the ISTA solver\n    def run_ista(lambda_val, max_iter=10000):\n        x = np.zeros(n, dtype=np.float64)\n        y_tr = y_train # Use consistent variable name inside function\n        for _ in range(max_iter):\n            grad_f = A.T @ (A @ x - y_tr) + gamma * x\n            x_update = x - alpha * grad_f\n            x = soft_threshold(x_update, alpha * lambda_val)\n        return x\n\n    results = []\n    for lambda_val in test_lambdas:\n        # Step 1: Solve the inner problem to find x_star(lambda)\n        x_star = run_ista(lambda_val)\n\n        # Step 2: Identify the active set\n        # Use a small tolerance to account for floating-point inaccuracies\n        tol = 1e-9\n        active_set_indices = np.where(np.abs(x_star) > tol)[0]\n\n        # Step 3 & 4: Compute the derivative dJ/dlambda\n        if active_set_indices.size == 0:\n            # If the active set is empty, x_star is 0, so dx_star/dlambda is 0.\n            # This makes dJ/dlambda = 0.\n            dJ_dlambda = 0.0\n        else:\n            # Get signs of the active components\n            s_S = np.sign(x_star[active_set_indices])\n            \n            # Extract the submatrix H_SS from the Hessian H\n            H_SS = H[np.ix_(active_set_indices, active_set_indices)]\n            \n            # Solve the linear system: H_SS * dx_S/dlambda = -s_S\n            dx_star_S_dlambda = np.linalg.solve(H_SS, -s_S)\n            \n            # Construct the full derivative vector dx_star/dlambda\n            dx_star_dlambda = np.zeros(n, dtype=np.float64)\n            dx_star_dlambda[active_set_indices] = dx_star_S_dlambda\n            \n            # Compute the gradient of the outer objective J with respect to x\n            grad_J_x = B.T @ (B @ x_star - y_val)\n            \n            # Compute dJ/dlambda using the chain rule\n            dJ_dlambda = grad_J_x.T @ dx_star_dlambda\n        \n        results.append(dJ_dlambda)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3368774"}]}