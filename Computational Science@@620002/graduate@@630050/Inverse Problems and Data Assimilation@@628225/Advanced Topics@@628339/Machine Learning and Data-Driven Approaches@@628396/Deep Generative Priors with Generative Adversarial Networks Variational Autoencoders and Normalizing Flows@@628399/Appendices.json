{"hands_on_practices": [{"introduction": "One of the most direct ways to incorporate a generative prior into an inverse problem is through Maximum A Posteriori (MAP) estimation, which involves minimizing an objective function that balances a data-fit term with a regularization term encoding the prior. This exercise [@problem_id:3374821] provides a hands-on analytical workout to build intuition for this trade-off, focusing on the role of the regularization parameter $\\lambda$. By analyzing a simplified yet insightful case, you will derive the regularization path and use the L-curve method to identify an optimal balance between data fidelity and prior conformity.", "problem": "Consider an inverse problem with observation model $y = A x + \\varepsilon$, where $A \\in \\mathbb{R}^{m \\times n}$ is a known forward operator, $x \\in \\mathbb{R}^{n}$ is the unknown state, and $\\varepsilon$ is additive noise. Assume a deep generative prior $x = G(z)$ where $G : \\mathbb{R}^{p} \\to \\mathbb{R}^{n}$ is a differentiable generator mapping (e.g., the decoder of a Generative Adversarial Network (GAN), a Variational Autoencoder (VAE), or a Normalizing Flow (NF) expressed in latent coordinates). We pose the latent Maximum A Posteriori (MAP) estimator as the solution to\n$$\n\\min_{z \\in \\mathbb{R}^{p}} \\ \\|A G(z) - y\\|_{2}^{2} + \\lambda \\|z\\|_{2}^{2},\n$$\nwith regularization strength $\\lambda > 0$. Assume a Gaussian likelihood and an isotropic Gaussian prior on $z$ so that the above criterion is the negative log-posterior up to an additive constant. To analyze the dependence of the latent MAP solution on $\\lambda$, proceed as follows.\n\nAssume that $G$ is linearized around a reference latent code $z = 0$ (shift the reference so that the linearization bias is zero, i.e., $G(0) = 0$) and write $G(z) \\approx B z$, where $B \\in \\mathbb{R}^{n \\times p}$ is the Jacobian of $G$ at $z = 0$. Let $C := A B \\in \\mathbb{R}^{m \\times p}$. Consider the scientifically meaningful and interpretable special case where $C$ has numerical rank one with Singular Value Decomposition (SVD) $C = \\sigma u v^{\\top}$, where $\\sigma > 0$ is the unique nonzero singular value, $u \\in \\mathbb{R}^{m}$ and $v \\in \\mathbb{R}^{p}$ are unit left and right singular vectors, and the data are aligned with the measurement mode, $y = y_{0} u$ with $y_{0} > 0$.\n\nTasks:\n- Derive the regularization path $z^{\\star}(\\lambda)$, i.e., the exact minimizer of $\\|C z - y\\|_{2}^{2} + \\lambda \\|z\\|_{2}^{2}$ for each $\\lambda > 0$.\n- Compute the data misfit $r(\\lambda) := \\|C z^{\\star}(\\lambda) - y\\|_{2}$ and the latent norm $s(\\lambda) := \\|z^{\\star}(\\lambda)\\|_{2}$, and parameterize the L-curve in latent versus image space via $(\\log r(\\lambda), \\log s(\\lambda))$.\n- From first principles, derive the curvature $\\kappa(\\lambda)$ of the parametric curve $\\lambda \\mapsto (\\log r(\\lambda), \\log s(\\lambda))$.\n- Determine the value $\\lambda^{\\star} > 0$ that maximizes $\\kappa(\\lambda)$.\n\nAnswer specification:\n- Provide the final answer as the single closed-form analytic expression for $\\lambda^{\\star}$ in terms of $\\sigma$. No numerical approximation is required, and no rounding is needed.", "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, objective, and contains sufficient information for a unique solution. We proceed with the derivation.\n\nThe objective is to find the minimizer $z^{\\star}(\\lambda)$ of the functional\n$$\nf(z) = \\|C z - y\\|_{2}^{2} + \\lambda \\|z\\|_{2}^{2}\n$$\nfor $\\lambda > 0$. The problem provides the specific forms $C = \\sigma u v^{\\top}$ and $y = y_{0} u$, where $\\sigma > 0$, $y_{0} > 0$, and $u, v$ are unit vectors.\n\nFirst, we derive the regularization path $z^{\\star}(\\lambda)$. The solution $z^{\\star}(\\lambda)$ must lie in the span of the columns of $C^{\\top}$, which in this rank-one case is the span of $v$. Thus, we can posit a solution of the form $z = \\alpha v$ for some scalar $\\alpha \\in \\mathbb{R}$. Substituting this into the objective function:\n$$\nf(\\alpha v) = \\|C (\\alpha v) - y\\|_{2}^{2} + \\lambda \\|\\alpha v\\|_{2}^{2}\n$$\nUsing the given forms for $C$ and $y$, and the fact that $v$ and $u$ are unit vectors ($v^{\\top}v = 1$, $\\|u\\|_{2}=1$):\n$$\nf(\\alpha v) = \\|\\sigma u v^{\\top} (\\alpha v) - y_{0} u\\|_{2}^{2} + \\lambda \\alpha^2 \\|v\\|_{2}^{2}\n$$\n$$\nf(\\alpha v) = \\|\\alpha \\sigma u (v^{\\top}v) - y_{0} u\\|_{2}^{2} + \\lambda \\alpha^2\n$$\n$$\nf(\\alpha v) = \\|(\\alpha \\sigma - y_{0}) u\\|_{2}^{2} + \\lambda \\alpha^2 = (\\alpha \\sigma - y_{0})^2 \\|u\\|_{2}^{2} + \\lambda \\alpha^2\n$$\n$$\nf(\\alpha v) = (\\alpha \\sigma - y_{0})^2 + \\lambda \\alpha^2\n$$\nTo find the minimum, we differentiate with respect to $\\alpha$ and set the derivative to zero:\n$$\n\\frac{df}{d\\alpha} = 2(\\alpha \\sigma - y_{0})\\sigma + 2\\lambda \\alpha = 0\n$$\n$$\n\\alpha \\sigma^2 - y_{0} \\sigma + \\lambda \\alpha = 0\n$$\n$$\n\\alpha(\\sigma^2 + \\lambda) = y_{0} \\sigma \\implies \\alpha = \\frac{y_{0} \\sigma}{\\sigma^2 + \\lambda}\n$$\nThe minimizer is therefore:\n$$\nz^{\\star}(\\lambda) = \\frac{\\sigma y_{0}}{\\sigma^2 + \\lambda} v\n$$\n\nNext, we compute the data misfit $r(\\lambda) = \\|C z^{\\star}(\\lambda) - y\\|_{2}$ and the latent norm $s(\\lambda) = \\|z^{\\star}(\\lambda)\\|_{2}$.\nFor the latent norm $s(\\lambda)$:\n$$\ns(\\lambda) = \\left\\| \\frac{\\sigma y_{0}}{\\sigma^2 + \\lambda} v \\right\\|_{2} = \\left| \\frac{\\sigma y_{0}}{\\sigma^2 + \\lambda} \\right| \\|v\\|_{2} = \\frac{\\sigma y_{0}}{\\sigma^2 + \\lambda}\n$$\nsince $\\sigma, y_0, \\lambda$ are positive.\nFor the data misfit $r(\\lambda)$:\n$$\nC z^{\\star}(\\lambda) - y = \\sigma u v^{\\top} \\left( \\frac{\\sigma y_{0}}{\\sigma^2 + \\lambda} v \\right) - y_{0} u = \\frac{\\sigma^2 y_{0}}{\\sigma^2 + \\lambda} u (v^{\\top}v) - y_{0} u\n$$\n$$\nC z^{\\star}(\\lambda) - y = \\left( \\frac{\\sigma^2 y_{0}}{\\sigma^2 + \\lambda} - y_{0} \\right) u = y_{0} \\left( \\frac{\\sigma^2 - (\\sigma^2 + \\lambda)}{\\sigma^2 + \\lambda} \\right) u = \\frac{-\\lambda y_{0}}{\\sigma^2 + \\lambda} u\n$$\n$$\nr(\\lambda) = \\left\\| \\frac{-\\lambda y_{0}}{\\sigma^2 + \\lambda} u \\right\\|_{2} = \\left| \\frac{-\\lambda y_{0}}{\\sigma^2 + \\lambda} \\right| \\|u\\|_{2} = \\frac{\\lambda y_{0}}{\\sigma^2 + \\lambda}\n$$\n\nThe L-curve is parameterized in log-log scale by $(\\rho(\\lambda), \\eta(\\lambda))$, where $\\rho(\\lambda) = \\log r(\\lambda)$ and $\\eta(\\lambda) = \\log s(\\lambda)$.\n$$\n\\rho(\\lambda) = \\log\\left(\\frac{\\lambda y_{0}}{\\sigma^2 + \\lambda}\\right) = \\log(y_0) + \\log(\\lambda) - \\log(\\sigma^2 + \\lambda)\n$$\n$$\n\\eta(\\lambda) = \\log\\left(\\frac{\\sigma y_{0}}{\\sigma^2 + \\lambda}\\right) = \\log(\\sigma y_0) - \\log(\\sigma^2 + \\lambda)\n$$\nTo find the curvature $\\kappa(\\lambda)$, we use the formula for a parametric curve $(x(t), y(t))$: $\\kappa(t) = \\frac{|x' y'' - y' x''|}{((x')^2 + (y')^2)^{3/2}}$. Here, the parameter is $\\lambda$. We need the first and second derivatives of $\\rho$ and $\\eta$ with respect to $\\lambda$.\n\nFirst derivatives:\n$$\n\\rho'(\\lambda) = \\frac{d\\rho}{d\\lambda} = \\frac{1}{\\lambda} - \\frac{1}{\\sigma^2 + \\lambda} = \\frac{\\sigma^2}{\\lambda(\\sigma^2 + \\lambda)}\n$$\n$$\n\\eta'(\\lambda) = \\frac{d\\eta}{d\\lambda} = -\\frac{1}{\\sigma^2 + \\lambda}\n$$\n\nSecond derivatives:\n$$\n\\rho''(\\lambda) = \\frac{d^2\\rho}{d\\lambda^2} = -\\frac{1}{\\lambda^2} + \\frac{1}{(\\sigma^2 + \\lambda)^2} = \\frac{-\\sigma^2(2\\lambda + \\sigma^2)}{\\lambda^2(\\sigma^2 + \\lambda)^2}\n$$\n$$\n\\eta''(\\lambda) = \\frac{d^2\\eta}{d\\lambda^2} = \\frac{1}{(\\sigma^2 + \\lambda)^2}\n$$\n\nNow we compute the terms for the curvature formula.\nNumerator term: $|\\rho'\\eta'' - \\eta'\\rho''|$\n$$\n\\rho'\\eta'' - \\eta'\\rho'' = \\frac{\\sigma^2}{\\lambda(\\sigma^2+\\lambda)} \\frac{1}{(\\sigma^2+\\lambda)^2} - \\left(-\\frac{1}{\\sigma^2+\\lambda}\\right) \\frac{-\\sigma^2(2\\lambda + \\sigma^2)}{\\lambda^2(\\sigma^2 + \\lambda)^2}\n$$\n$$\n= \\frac{\\sigma^2}{\\lambda(\\sigma^2+\\lambda)^3} - \\frac{\\sigma^2(2\\lambda + \\sigma^2)}{\\lambda^2(\\sigma^2+\\lambda)^3} = \\frac{\\sigma^2}{\\lambda^2(\\sigma^2+\\lambda)^3} \\left[ \\lambda - (2\\lambda + \\sigma^2) \\right]\n$$\n$$\n= \\frac{\\sigma^2(-\\lambda - \\sigma^2)}{\\lambda^2(\\sigma^2+\\lambda)^3} = -\\frac{\\sigma^2}{\\lambda^2(\\sigma^2+\\lambda)^2}\n$$\nThe absolute value is $\\frac{\\sigma^2}{\\lambda^2(\\sigma^2+\\lambda)^2}$.\n\nDenominator term: $((\\rho')^2 + (\\eta')^2)^{3/2}$\n$$\n(\\rho')^2 + (\\eta')^2 = \\left( \\frac{\\sigma^2}{\\lambda(\\sigma^2 + \\lambda)} \\right)^2 + \\left( -\\frac{1}{\\sigma^2 + \\lambda} \\right)^2 = \\frac{\\sigma^4}{\\lambda^2(\\sigma^2 + \\lambda)^2} + \\frac{\\lambda^2}{\\lambda^2(\\sigma^2 + \\lambda)^2} = \\frac{\\sigma^4 + \\lambda^2}{\\lambda^2(\\sigma^2 + \\lambda)^2}\n$$\nTaking this to the power of $3/2$:\n$$\n\\left( \\frac{\\sigma^4 + \\lambda^2}{\\lambda^2(\\sigma^2 + \\lambda)^2} \\right)^{3/2} = \\frac{(\\sigma^4 + \\lambda^2)^{3/2}}{\\lambda^3(\\sigma^2 + \\lambda)^3}\n$$\n\nCombining these to get the curvature $\\kappa(\\lambda)$:\n$$\n\\kappa(\\lambda) = \\frac{\\frac{\\sigma^2}{\\lambda^2(\\sigma^2+\\lambda)^2}}{\\frac{(\\sigma^4 + \\lambda^2)^{3/2}}{\\lambda^3(\\sigma^2 + \\lambda)^3}} = \\frac{\\sigma^2 \\lambda (\\sigma^2 + \\lambda)}{(\\sigma^4 + \\lambda^2)^{3/2}}\n$$\n\nFinally, we find the value $\\lambda^{\\star}$ that maximizes $\\kappa(\\lambda)$. Maximizing $\\kappa(\\lambda)$ is equivalent to maximizing $\\log(\\kappa(\\lambda)^2)$ for $\\lambda > 0$.\nLet $L(\\lambda) = \\log(\\kappa(\\lambda)^2) = \\log\\left(\\frac{\\sigma^4 \\lambda^2 (\\sigma^2 + \\lambda)^2}{(\\sigma^4 + \\lambda^2)^3}\\right)$.\n$$\nL(\\lambda) = 4\\log(\\sigma) + 2\\log(\\lambda) + 2\\log(\\sigma^2 + \\lambda) - 3\\log(\\sigma^4 + \\lambda^2)\n$$\nDifferentiating with respect to $\\lambda$ and setting to zero:\n$$\n\\frac{dL}{d\\lambda} = \\frac{2}{\\lambda} + \\frac{2}{\\sigma^2 + \\lambda} - \\frac{3(2\\lambda)}{\\sigma^4 + \\lambda^2} = 0\n$$\n$$\n\\frac{2(\\sigma^2 + \\lambda) + 2\\lambda}{\\lambda(\\sigma^2 + \\lambda)} = \\frac{6\\lambda}{\\sigma^4 + \\lambda^2}\n$$\n$$\n\\frac{2\\sigma^2 + 4\\lambda}{\\lambda(\\sigma^2 + \\lambda)} = \\frac{6\\lambda}{\\sigma^4 + \\lambda^2}\n$$\n$$\n(2\\sigma^2 + 4\\lambda)(\\sigma^4 + \\lambda^2) = 6\\lambda^2(\\sigma^2 + \\lambda)\n$$\n$$\n(\\sigma^2 + 2\\lambda)(\\sigma^4 + \\lambda^2) = 3\\lambda^2(\\sigma^2 + \\lambda)\n$$\nExpanding both sides:\n$$\n\\sigma^6 + \\sigma^2\\lambda^2 + 2\\sigma^4\\lambda + 2\\lambda^3 = 3\\sigma^2\\lambda^2 + 3\\lambda^3\n$$\nRearranging the terms gives a cubic equation in $\\lambda$:\n$$\n\\lambda^3 + 2\\sigma^2\\lambda^2 - 2\\sigma^4\\lambda - \\sigma^6 = 0\n$$\nTo solve this, we introduce the substitution $\\lambda = k \\sigma^2$.\n$$\n(k\\sigma^2)^3 + 2\\sigma^2(k\\sigma^2)^2 - 2\\sigma^4(k\\sigma^2) - \\sigma^6 = 0\n$$\n$$\nk^3\\sigma^6 + 2k^2\\sigma^6 - 2k\\sigma^6 - \\sigma^6 = 0\n$$\nDividing by $\\sigma^6$ (since $\\sigma > 0$):\n$$\nk^3 + 2k^2 - 2k - 1 = 0\n$$\nBy the rational root theorem, we test $k=1$: $1+2-2-1=0$. So $k=1$ is a root.\nThis means $\\lambda = 1 \\cdot \\sigma^2 = \\sigma^2$ is a critical point.\nFactoring the cubic polynomial: $(k-1)(k^2+3k+1) = 0$.\nThe other roots are solutions to $k^2+3k+1=0$, which are $k = \\frac{-3 \\pm \\sqrt{9-4}}{2} = \\frac{-3 \\pm \\sqrt{5}}{2}$.\nBoth of these roots are negative. Since $\\lambda > 0$ and $\\sigma^2 > 0$, we must have $k > 0$.\nThus, the only valid positive solution is $k=1$, which corresponds to $\\lambda^{\\star} = \\sigma^2$.\nThe second derivative test confirms this is a maximum. Therefore, the value of $\\lambda$ that maximizes the curvature of the L-curve is $\\sigma^2$.", "answer": "$$\\boxed{\\sigma^2}$$", "id": "3374821"}, {"introduction": "While point estimates provide a single solution, a full Bayesian treatment allows us to quantify the uncertainty that remains after assimilating data. This practice [@problem_id:3374817] casts the reconstruction of satellite imagery with missing spectral bands as a data assimilation problem. You will implement the core equations of Bayesian inference for a linear-Gaussian model to compute how posterior uncertainty in the latent space propagates to the final image, and observe quantitatively how this uncertainty decreases as more measurements become available.", "problem": "You are tasked with constructing a mathematically precise and computationally verifiable data assimilation pipeline for multispectral satellite remote sensing under missing spectral bands, using a Variational Autoencoder (VAE) prior. The pipeline must be framed as a latent-variable inverse problem equipped with a deep generative prior and a linear observation operator reflecting missing-band selection. Your program must implement the following setup and computations exactly, and produce a single-line output in the specified format.\n\nAssume the following generative model for a single multispectral pixel with $d$ spectral bands:\n1. The latent variable is $z \\in \\mathbb{R}^L$ with prior $p(z) = \\mathcal{N}(0, I_L)$, where $I_L$ is the $L \\times L$ identity matrix.\n2. The VAE decoder mapping is locally approximated by a first-order linearization with a mean and Jacobian around a nominal point, yielding $x \\in \\mathbb{R}^d$ as\n$$\nx = g(z) \\approx \\mu + W z,\n$$\nwhere $\\mu \\in \\mathbb{R}^d$ is a fixed decoder mean and $W \\in \\mathbb{R}^{d \\times L}$ is the decoder Jacobian evaluated at a linearization point. This approximation is consistent with the standard methodology in inverse problems for deep generative priors.\n\nObservations under missing spectral bands are modeled as:\n$$\ny = H x + \\epsilon,\n$$\nwhere $H \\in \\mathbb{R}^{m \\times d}$ is the observation operator selecting the $m$ observed bands ($m \\le d$) and $\\epsilon \\sim \\mathcal{N}(0, R)$ with $R \\in \\mathbb{R}^{m \\times m}$ symmetric positive definite (diagonal).\n\nYour tasks:\n- Derive the observation operator $H$ as a binary selection matrix based on the set of observed band indices. The construction must be logically deduced from the definition that $H$ selects rows of the identity matrix corresponding to observed indices, resulting in a linear projection of $x$ onto the observed subspace.\n- From first principles (Bayesâ€™ rule and Gaussian models), derive the latent posterior $p(z \\mid y)$ as a Gaussian distribution $\\mathcal{N}(\\mu_{\\text{post}}, \\Sigma_{\\text{post}})$ with mean $\\mu_{\\text{post}} \\in \\mathbb{R}^L$ and covariance $\\Sigma_{\\text{post}} \\in \\mathbb{R}^{L \\times L}$, consistent with the linear-Gaussian data assimilation paradigm using a deep generative prior.\n- Quantify the uncertainty propagation from the latent posterior to the data space through the decoder linearization via the propagated covariance\n$$\n\\Sigma_x = W \\Sigma_{\\text{post}} W^\\top \\in \\mathbb{R}^{d \\times d}.\n$$\n\nImplement the above computations for the following test suite. All matrices and vectors are to be constructed exactly as given. Every number is a real scalar. The decoder dimension is $d = 5$ and the latent dimension is $L = 3$.\n\nDecoder parameters:\n$$\nW = \\begin{bmatrix}\n0.90 & -0.10 & 0.20 \\\\\n0.30 & 0.80 & -0.50 \\\\\n-0.40 & 0.10 & 0.70 \\\\\n0.00 & -0.60 & 0.40 \\\\\n0.50 & 0.20 & -0.30\n\\end{bmatrix}, \\quad\n\\mu = \\begin{bmatrix}\n0.05 \\\\ -0.10 \\\\ 0.20 \\\\ -0.05 \\\\ 0.10\n\\end{bmatrix}.\n$$\n\nGround-truth latent variable (used to generate a noise-free observation for each case):\n$$\nz^\\star = \\begin{bmatrix} 0.60 \\\\ -0.40 \\\\ 0.80 \\end{bmatrix}.\n$$\nCompute $x^\\star = \\mu + W z^\\star$ and then $y$ in each case as $y = H x^\\star$ (set $\\epsilon = 0$ for deterministic evaluation; uncertainty enters only through $R$).\n\nTest cases:\n- Case $1$: Observed band indices $\\Omega = \\{0, 2, 4\\}$, so $m = 3$. Noise covariance $R = \\operatorname{diag}(0.05, 0.10, 0.08)$.\n- Case $2$: Observed band indices $\\Omega = \\{0, 1, 2, 3, 4\\}$, so $m = 5$. Noise covariance $R = \\operatorname{diag}(0.05, 0.05, 0.05, 0.05, 0.05)$.\n- Case $3$: Observed band indices $\\Omega = \\varnothing$, so $m = 0$. Noise covariance $R$ is an empty matrix of shape $0 \\times 0$ (no observations).\n- Case $4$: Observed band indices $\\Omega = \\{1, 3\\}$, so $m = 2$. Noise covariance $R = \\operatorname{diag}(1.00, 2.00)$.\n\nRequired outputs:\n- For each case, compute the latent posterior covariance $\\Sigma_{\\text{post}}$ and the propagated covariance $\\Sigma_x$.\n- From these, extract the scalar metrics:\n$$\n\\tau_z = \\operatorname{tr}(\\Sigma_{\\text{post}}), \\quad \\tau_x = \\operatorname{tr}(\\Sigma_x).\n$$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order\n$$\n[\\tau_z^{(1)}, \\tau_x^{(1)}, \\tau_z^{(2)}, \\tau_x^{(2)}, \\tau_z^{(3)}, \\tau_x^{(3)}, \\tau_z^{(4)}, \\tau_x^{(4)}],\n$$\nwhere the superscript $(k)$ denotes Case $k$ for $k \\in \\{1, 2, 3, 4\\}$.\n- All outputs must be real numbers (floats). No physical units are required.\n\nYour implementation must be self-contained and must not require any input. Construct all quantities exactly as specified, and perform the required computations for each case. The program must be written in the Python language and use only the libraries listed in the final answer specification.", "solution": "The problem requires the derivation and computation of uncertainty metrics for a data assimilation pipeline. The pipeline uses a linearized Variational Autoencoder (VAE) decoder as a deep generative prior to reconstruct a multispectral satellite pixel from incomplete observations. The analysis will proceed from first principles of Bayesian inference for linear-Gaussian models.\n\n### 1. Problem Formulation\n\nThe system is described by a set of linear-Gaussian models.\nThe latent variable $z \\in \\mathbb{R}^L$ has a standard normal prior distribution:\n$$\np(z) = \\mathcal{N}(z \\mid 0, I_L)\n$$\nwhere $I_L$ is the $L \\times L$ identity matrix. Here, the prior mean is $\\mu_{\\text{prior}} = 0$ and the prior covariance is $\\Sigma_{\\text{prior}} = I_L$.\n\nThe VAE decoder, which maps the latent variable $z$ to the data space vector $x \\in \\mathbb{R}^d$, is approximated by a linear function:\n$$\nx \\approx \\mu + W z\n$$\nwhere $\\mu \\in \\mathbb{R}^d$ is the decoder mean and $W \\in \\mathbb{R}^{d \\times L}$ is the decoder Jacobian.\n\nThe observation $y \\in \\mathbb{R}^m$ is a linear projection of the full data vector $x$, corrupted by additive Gaussian noise $\\epsilon$:\n$$\ny = H x + \\epsilon\n$$\nwhere $H \\in \\mathbb{R}^{m \\times d}$ is the observation operator that selects the $m$ observed spectral bands, and the noise is distributed as $\\epsilon \\sim \\mathcal{N}(0, R)$.\n\nBy substituting the expression for $x$ into the observation model, we obtain a direct relationship between the latent variable $z$ and the observation $y$:\n$$\ny = H(\\mu + Wz) + \\epsilon = H\\mu + HWz + \\epsilon\n$$\nThis can be rearranged into the standard form for a linear inverse problem:\n$$\ny - H\\mu = (HW)z + \\epsilon\n$$\nHere, $(HW)$ is the forward operator mapping the latent variable $z$ to the (mean-corrected) observation space, and $\\epsilon$ is the observation noise.\n\n### 2. Derivation of the Latent Posterior Covariance\n\nWe seek the posterior distribution of the latent variable given the observation, $p(z \\mid y)$. Since the prior $p(z)$ and the likelihood $p(y \\mid z)$ are both Gaussian, the posterior will also be a Gaussian distribution, which we denote as $\\mathcal{N}(\\mu_{\\text{post}}, \\Sigma_{\\text{post}})$.\n\nAccording to Bayes' rule for linear-Gaussian models, the inverse of the posterior covariance matrix (the posterior precision matrix) is the sum of the prior precision matrix and the precision matrix from the likelihood term.\n\nThe prior precision is $\\Sigma_{\\text{prior}}^{-1} = I_L^{-1} = I_L$.\n\nThe likelihood function is derived from the observation model: $p(y \\mid z) = \\mathcal{N}(y \\mid H\\mu + HWz, R)$. The corresponding negative log-likelihood, ignoring constants, is proportional to:\n$$\n\\frac{1}{2} (y - H\\mu - HWz)^\\top R^{-1} (y - H\\mu - HWz)\n$$\nThe term that is quadratic in $z$ is $\\frac{1}{2} z^\\top (HW)^\\top R^{-1} (HW) z$. The coefficient of this quadratic term defines the data precision matrix, which is $(HW)^\\top R^{-1} (HW) = W^\\top H^\\top R^{-1} H W$.\n\nSumming the prior and data precision matrices gives the posterior precision matrix:\n$$\n\\Sigma_{\\text{post}}^{-1} = \\Sigma_{\\text{prior}}^{-1} + W^\\top H^\\top R^{-1} H W = I_L + W^\\top H^\\top R^{-1} H W\n$$\nThe posterior covariance is therefore the inverse of this matrix:\n$$\n\\Sigma_{\\text{post}} = (I_L + W^\\top H^\\top R^{-1} H W)^{-1}\n$$\nNote that this expression for the posterior covariance does not depend on the specific observation value $y$, but only on the structure of the model ($W, H$) and the noise statistics ($R$).\n\n### 3. Construction of the Observation Operator $H$\n\nThe observation operator $H \\in \\mathbb{R}^{m \\times d}$ is defined as a binary selection matrix based on the set of observed band indices $\\Omega = \\{i_0, i_1, \\dots, i_{m-1}\\}$. For $H$ to select the specified bands from a vector $x \\in \\mathbb{R}^d$, each row of $H$ must be a standard basis vector of $\\mathbb{R}^d$. Specifically, the $k$-th row of $H$ corresponds to the basis vector $e_{i_k}^\\top$, which has a $1$ at index $i_k$ and zeros elsewhere. This construction is equivalent to taking the rows of the $d \\times d$ identity matrix $I_d$ corresponding to the indices in $\\Omega$.\n\n### 4. Propagated Data Space Covariance\n\nThe posterior uncertainty in the latent space, characterized by $\\Sigma_{\\text{post}}$, propagates through the linearized decoder to the data space. The posterior distribution of the reconstructed data vector $x$ is approximately Gaussian. Given $x \\approx \\mu + Wz$ and $z \\sim \\mathcal{N}(\\mu_{\\text{post}}, \\Sigma_{\\text{post}})$, the covariance of $x$ is given by the standard formula for linear transformation of a random variable:\n$$\n\\Sigma_x = W \\Sigma_{\\text{post}} W^\\top\n$$\n\n### 5. Uncertainty Metrics and Special Case\n\nThe total uncertainty in the latent and data spaces is quantified by the trace of their respective covariance matrices.\nThe latent space uncertainty is $\\tau_z = \\operatorname{tr}(\\Sigma_{\\text{post}})$.\nThe propagated data space uncertainty is $\\tau_x = \\operatorname{tr}(\\Sigma_x)$.\n\nA crucial special case is when no observations are made, i.e., $m=0$ and $\\Omega = \\varnothing$. In this scenario, the observation provides no new information about $z$. Consequently, the posterior distribution must be identical to the prior distribution.\n$$\np(z \\mid y_{\\text{no-data}}) = p(z) \\implies \\Sigma_{\\text{post}} = \\Sigma_{\\text{prior}} = I_L\n$$\nThis is consistent with our derived formula, as for $m=0$, the observation matrix $H$ has dimensions $0 \\times d$, making the term $W^\\top H^\\top R^{-1} H W$ an $L \\times L$ zero matrix. Thus, $\\Sigma_{\\text{post}} = (I_L + 0)^{-1} = I_L$.\nFor this case:\n$\\tau_z = \\operatorname{tr}(I_L) = L$.\n$\\tau_x = \\operatorname{tr}(W I_L W^\\top) = \\operatorname{tr}(W W^\\top)$.\n\n### 6. Computational Procedure\n\nFor each test case, the following steps are performed:\n1. Define the dimensions $d=5$, $L=3$ and the matrices $W$ and $I_L$.\n2. For a given set of observed indices $\\Omega$ and noise variances (diagonal of $R$), determine the dimension of the observation space, $m = |\\Omega|$.\n3. If $m=0$:\n   - $\\Sigma_{\\text{post}} = I_L$.\n4. If $m>0$:\n   - Construct the observation operator $H \\in \\mathbb{R}^{m \\times d}$.\n   - Construct the noise covariance matrix $R \\in \\mathbb{R}^{m \\times m}$ and compute its inverse $R^{-1}$.\n   - Compute the posterior precision $\\Sigma_{\\text{post}}^{-1} = I_L + W^\\top H^\\top R^{-1} H W$.\n   - Invert to find the posterior covariance $\\Sigma_{\\text{post}} = (\\Sigma_{\\text{post}}^{-1})^{-1}$.\n5. Compute the propagated data space covariance $\\Sigma_x = W \\Sigma_{\\text{post}} W^\\top$.\n6. Compute the uncertainty metrics $\\tau_z = \\operatorname{tr}(\\Sigma_{\\text{post}})$ and $\\tau_x = \\operatorname{tr}(\\Sigma_x)$.\n7. Collect the results for all cases and format them as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the data assimilation problem under a linearized VAE prior.\n    \n    This function implements the derivation for the posterior covariance in a\n    linear-Gaussian model, computes the propagated covariance in the data space,\n    and calculates their traces as uncertainty metrics for four distinct test cases.\n    \"\"\"\n    # Decoder and latent space parameters\n    d = 5  # Data space dimension\n    L = 3  # Latent space dimension\n\n    # Decoder Jacobian W\n    W = np.array([\n        [0.90, -0.10, 0.20],\n        [0.30,  0.80, -0.50],\n        [-0.40, 0.10,  0.70],\n        [0.00, -0.60,  0.40],\n        [0.50,  0.20, -0.30]\n    ])\n\n    # Identity matrix in the latent space (prior covariance)\n    I_L = np.eye(L)\n\n    # Note: The decoder mean mu and ground-truth latent z_star are not needed for\n    # computing the posterior covariance, which only depends on the model structure\n    # (W, H) and noise statistics (R).\n\n    # Test cases defined by observed indices and diagonal of noise covariance R.\n    test_cases = [\n        # Case 1: m=3 observed bands\n        {'indices': [0, 2, 4], 'R_diag': [0.05, 0.10, 0.08]},\n        # Case 2: m=5 (all) observed bands\n        {'indices': [0, 1, 2, 3, 4], 'R_diag': [0.05, 0.05, 0.05, 0.05, 0.05]},\n        # Case 3: m=0 (no) observed bands\n        {'indices': [], 'R_diag': []},\n        # Case 4: m=2 observed bands\n        {'indices': [1, 3], 'R_diag': [1.00, 2.00]}\n    ]\n\n    results = []\n    for case in test_cases:\n        indices = case['indices']\n        R_diag = np.array(case['R_diag'])\n        m = len(indices)\n\n        if m == 0:\n            # Special case: no observations.\n            # Posterior covariance equals prior covariance.\n            Sigma_post = I_L\n        else:\n            # Construct observation operator H\n            # H is an m x d matrix that selects the observed bands.\n            H = np.zeros((m, d))\n            H[np.arange(m), indices] = 1.0\n\n            # Compute R^-1. Since R is diagonal, its inverse is a diagonal\n            # matrix with the reciprocals of the diagonal elements.\n            R_inv = np.diag(1.0 / R_diag)\n            \n            # Compute posterior precision matrix: I_L + W^T H^T R^-1 H W\n            # This is the sum of the prior and data-likelihood precisions.\n            data_precision = W.T @ H.T @ R_inv @ H @ W\n            Sigma_post_inv = I_L + data_precision\n\n            # Invert to get posterior covariance\n            Sigma_post = np.linalg.inv(Sigma_post_inv)\n\n        # Propagate covariance to data space: Sigma_x = W * Sigma_post * W^T\n        Sigma_x = W @ Sigma_post @ W.T\n\n        # Calculate uncertainty metrics (traces of covariance matrices)\n        tau_z = np.trace(Sigma_post)\n        tau_x = np.trace(Sigma_x)\n\n        results.extend([tau_z, tau_x])\n\n    # Format the output as a single comma-separated string in brackets\n    # Using a high precision format to avoid rounding issues in verification\n    formatted_results = [f\"{r:.15f}\".rstrip('0').rstrip('.') for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3374817"}, {"introduction": "A generative prior is only as good as the data it was trained on, and a critical failure mode occurs when the true signal lies outside the model's expressive range; this is the challenge of model misspecification. This hands-on exercise [@problem_id:3374820] guides you to numerically simulate this exact scenario by deliberately \"corrupting\" a prior to exclude the true solution. By visualizing the resulting posterior distribution, you will discover how such prior-data conflict can lead to pathological outcomes like spurious or multiple solutions, a crucial diagnostic skill for any practitioner.", "problem": "Consider an inverse problem with a deep generative prior specified by a Variational Autoencoder (VAE) that induces a latent-variable model. Let the latent variable be one-dimensional, denoted by $z \\in \\mathbb{R}$. The decoder is deterministic, $x = g(z)$, so the induced prior over $x$ is the pushforward of the latent prior $p(z)$ through $g$. Observations are generated by a measurement operator $h$ acting on the decoded state, with additive Gaussian noise. In this setup, the generative model is\n$$\nz \\sim \\mathcal{N}(0,1), \\quad x = g(z), \\quad y = h(x) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0,\\sigma_y^2).\n$$\nBy Bayes' rule, the posterior over the latent variable $z$ given the observation $y$ obeys\n$$\n\\pi(z \\mid y) \\propto p(y \\mid z) \\, p(z),\n$$\nwhere $p(y \\mid z)$ is the Gaussian likelihood induced by $h \\circ g$ and $p(z)$ is the latent prior.\n\nTo deliberately corrupt the training distribution of the VAE prior and quantify model misspecification, suppose that the latent prior is truncated to remove mass in a specific interval, thereby omitting the true state $x^\\dagger$ from the induced prior support. Specifically, define the corrupted latent prior as\n$$\np_{\\mathrm{corr}}(z) \\propto \\exp\\left(-\\frac{z^2}{2}\\right) \\cdot \\mathbf{1}\\left(\\lvert z - z_c \\rvert \\ge \\delta\\right),\n$$\nwhere $z_c \\in \\mathbb{R}$ is the center of the removed interval and $\\delta > 0$ is its radius. This corruption models a training distribution that omits the latent neighborhood containing the true latent $z^\\dagger$ and hence omits $x^\\dagger = g(z^\\dagger)$ from the prior support when the decoder is deterministic.\n\nAssume a one-dimensional observation model so that the posterior can be analyzed over $z \\in \\mathbb{R}$ numerically on a fixed grid. The tasks are:\n- Derive from first principles the expression for the unnormalized posterior density $\\pi(z \\mid y)$ for a general measurement function $h \\circ g$, a Gaussian observation noise with variance $\\sigma_y^2$, and either the standard normal latent prior or the corrupted latent prior defined above.\n- Implement a numerical approximation on a uniform grid $z \\in [-5,5]$ with $20001$ equispaced points to evaluate the unnormalized posterior, normalize it, and compute:\n  1. The number of modes (local maxima) of the posterior over $z$ restricted to the grid. A mode is defined as an index $i$ in the interior of the grid such that the normalized posterior density at $z_i$ is strictly greater than its immediate neighbors and exceeds a relative threshold of $10^{-4}$ times the maximum normalized posterior density.\n  2. The posterior standard deviation $\\sqrt{\\mathrm{Var}[z \\mid y]}$, approximated by the normalized discrete distribution on the grid.\n  3. The posterior mass within a radius $\\rho = 0.15$ of the true latent $z^\\dagger$, i.e., $\\sum_{i: \\lvert z_i - z^\\dagger \\rvert \\le \\rho} \\pi(z_i \\mid y) \\Delta z$, where $\\Delta z$ is the grid spacing. This must be reported as a decimal.\n\nUse the following test suite, which explores well-specified and misspecified regimes, including the emergence of spurious modes when prior support omits the true $x^\\dagger$:\n- Case A (misspecified, corrupted prior, small noise): $g(z) = z$, $h(x) = x$, $y = 1.0$, $\\sigma_y = 0.05$, $z^\\dagger = 1.0$, $z_c = 1.0$, $\\delta = 0.3$.\n- Case B (misspecified, corrupted prior, larger noise): $g(z) = z$, $h(x) = x$, $y = 1.0$, $\\sigma_y = 0.5$, $z^\\dagger = 1.0$, $z_c = 1.0$, $\\delta = 0.3$.\n- Case C (well-specified, standard prior): $g(z) = z$, $h(x) = x$, $y = 1.0$, $\\sigma_y = 0.05$, $z^\\dagger = 1.0$, no corruption ($\\delta = 0.0$ and any $z_c$ ignored).\n- Case D (non-injective decoder-induced observation, standard prior): $g(z) = z$, $h(x) = x^2$, $y = 1.0$, $\\sigma_y = 0.05$, $z^\\dagger = 1.0$, no corruption ($\\delta = 0.0$ and any $z_c$ ignored).\n\nFor each case, compute the three quantities listed above:\n1. The integer number of modes.\n2. The posterior standard deviation as a float.\n3. The posterior mass within radius $\\rho$ of $z^\\dagger$ as a float.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets with one sublist per case in the order A, B, C, D, where each sublist is of the form $[n, s, m]$ with $n$ the integer number of modes, $s$ the posterior standard deviation, and $m$ the posterior mass near $z^\\dagger$, for example, $[[n_A,s_A,m_A],[n_B,s_B,m_B],[n_C,s_C,m_C],[n_D,s_D,m_D]]$. No physical units are involved. Angles do not appear. All decimals must be printed in standard decimal notation.", "solution": "The problem requires the analysis of a Bayesian posterior distribution for a one-dimensional latent variable $z$ in the context of an inverse problem regularized by a deep generative prior. We will first derive the mathematical expression for the posterior distribution, then detail the numerical methods for its characterization, and finally apply these methods to the four specified test cases.\n\n### 1. Derivation of the Posterior Distribution\n\nThe problem is structured as a hierarchical Bayesian model. The generative process is given by:\n1.  A latent variable $z$ is drawn from a prior distribution $p(z)$.\n2.  A state $x$ is deterministically generated by a decoder function $g$, so $x = g(z)$.\n3.  An observation $y$ is obtained through a measurement operator $h$ with additive Gaussian noise $\\varepsilon$, i.e., $y = h(x) + \\varepsilon = (h \\circ g)(z) + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_y^2)$.\n\nOur goal is to find the posterior distribution of the latent variable $z$ given an observation $y$, denoted $\\pi(z \\mid y)$. By Bayes' theorem, the posterior is proportional to the product of the likelihood and the prior:\n$$\n\\pi(z \\mid y) \\propto p(y \\mid z) \\, p(z)\n$$\n\nThe likelihood function $p(y \\mid z)$ describes the probability of observing $y$ given a specific latent code $z$. From the measurement model $y = (h \\circ g)(z) + \\varepsilon$, it follows that the conditional distribution of $y$ given $z$ is a Gaussian centered at $(h \\circ g)(z)$ with variance $\\sigma_y^2$:\n$$\np(y \\mid z) = \\mathcal{N}(y; (h \\circ g)(z), \\sigma_y^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_y^2}} \\exp\\left( -\\frac{(y - (h \\circ g)(z))^2}{2\\sigma_y^2} \\right)\n$$\n\nThe prior distribution $p(z)$ is specified in two forms:\n1.  **Standard Prior**: A standard normal distribution, $p(z) = \\mathcal{N}(z; 0, 1) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{z^2}{2}\\right)$.\n2.  **Corrupted Prior**: A truncated standard normal distribution, $p_{\\mathrm{corr}}(z) \\propto \\exp\\left(-\\frac{z^2}{2}\\right) \\cdot \\mathbf{1}\\left(\\lvert z - z_c \\rvert \\ge \\delta\\right)$, where $\\mathbf{1}(\\cdot)$ is the indicator function. This prior is zero within the interval $(z_c - \\delta, z_c + \\delta)$.\n\nCombining the likelihood and prior, the unnormalized posterior density $\\pi_{\\text{unnorm}}(z \\mid y)$ is given by the product. For numerical stability, it is advantageous to work with the log-posterior:\n$$\n\\ln \\pi(z \\mid y) = \\ln p(y \\mid z) + \\ln p(z) + C\n$$\nwhere $C$ is a constant. Dropping constant terms that do not depend on $z$, the expression for the unnormalized log-posterior becomes:\n$$\n\\ln \\pi_{\\text{unnorm}}(z \\mid y) = -\\frac{(y - (h \\circ g)(z))^2}{2\\sigma_y^2} - \\frac{z^2}{2}\n$$\nfor the standard prior. For the corrupted prior, we have:\n$$\n\\ln \\pi_{\\text{unnorm}}(z \\mid y) = \n\\begin{cases} \n-\\frac{(y - (h \\circ g)(z))^2}{2\\sigma_y^2} - \\frac{z^2}{2} & \\text{if } \\lvert z - z_c \\rvert \\ge \\delta \\\\\n-\\infty & \\text{if } \\lvert z - z_c \\rvert < \\delta\n\\end{cases}\n$$\n\n### 2. Numerical Approximation\n\nWe approximate the continuous posterior distribution on a discrete, uniform grid of points $z_i$. The problem specifies a grid for $z \\in [-5, 5]$ with $N = 20001$ points. The grid spacing is $\\Delta z = (5 - (-5))/(N - 1) = 10 / 20000 = 5 \\times 10^{-4}$.\n\nFor each point $z_i$ on the grid, we evaluate the unnormalized log-posterior $\\ln \\pi_{\\text{unnorm}}(z_i \\mid y)$. To obtain the unnormalized posterior values $\\pi_{\\text{unnorm}}(z_i \\mid y)$, we compute $\\exp(\\ln \\pi_{\\text{unnorm}}(z_i \\mid y))$. To prevent numerical underflow or overflow, we use the identity $\\exp(a) = \\exp(a - \\max(a_j))\\exp(\\max(a_j))$ and drop the constant factor $\\exp(\\max(a_j))$. We define a stabilized unnormalized posterior:\n$$\n\\tilde{\\pi}_{\\text{unnorm}}(z_i \\mid y) = \\exp\\left( \\ln \\pi_{\\text{unnorm}}(z_i \\mid y) - \\max_{j} \\{ \\ln \\pi_{\\text{unnorm}}(z_j \\mid y) \\} \\right)\n$$\nThe normalized posterior density $\\pi(z_i \\mid y)$ is then found by approximating the integral of the unnormalized posterior using a sum (equivalent to the trapezoidal rule for a fine grid):\n$$\n\\pi(z_i \\mid y) = \\frac{\\tilde{\\pi}_{\\text{unnorm}}(z_i \\mid y)}{\\sum_{j=1}^{N} \\tilde{\\pi}_{\\text{unnorm}}(z_j \\mid y) \\Delta z}\n$$\n\n### 3. Computation of Required Quantities\n\nWith the normalized discrete posterior probability density $\\pi(z_i \\mid y)$ and the corresponding probability masses $P_i = \\pi(z_i \\mid y) \\Delta z$, we compute the three required quantities for each test case.\n\n**1. Number of Modes:**\nA mode is identified at a grid point $z_i$ (for $i$ in the grid interior, i.e., $i \\in \\{1, \\dots, N-2\\}$) if the posterior density at that point is strictly greater than its immediate neighbors: $\\pi(z_i \\mid y) > \\pi(z_{i-1} \\mid y)$ and $\\pi(z_i \\mid y) > \\pi(z_{i+1} \\mid y)$. Additionally, to filter out insignificant numerical fluctuations, a mode must satisfy $\\pi(z_i \\mid y) > 10^{-4} \\cdot \\max_{j} \\{ \\pi(z_j \\mid y) \\}$. We count the number of grid points that satisfy these two conditions.\n\n**2. Posterior Standard Deviation:**\nThe posterior standard deviation $\\sigma_z = \\sqrt{\\mathrm{Var}[z \\mid y]}$ is computed from the discrete approximation.\nFirst, the posterior mean $\\mu_z = \\mathrm{E}[z \\mid y]$ is approximated:\n$$\n\\mu_z \\approx \\sum_{i=1}^{N} z_i \\cdot \\pi(z_i \\mid y) \\Delta z\n$$\nThen, the posterior variance $\\mathrm{Var}[z \\mid y] = \\mathrm{E}[(z - \\mu_z)^2 \\mid y]$ is approximated:\n$$\n\\mathrm{Var}[z \\mid y] \\approx \\sum_{i=1}^{N} (z_i - \\mu_z)^2 \\cdot \\pi(z_i \\mid y) \\Delta z\n$$\nThe standard deviation is the square root of this variance.\n\n**3. Posterior Mass near $z^\\dagger$:**\nThe posterior mass within a radius $\\rho = 0.15$ of the true latent value $z^\\dagger$ is the sum of the probability masses of all grid points $z_i$ that fall within the interval $[z^\\dagger - \\rho, z^\\dagger + \\rho]$:\n$$\nM = \\sum_{i: \\lvert z_i - z^\\dagger \\rvert \\le \\rho} \\pi(z_i \\mid y) \\Delta z\n$$\n\n### 4. Analysis of Test Cases\n\n*   **Case A (Misspecified, small noise):** $h \\circ g(z) = z$, $y = 1.0$, $\\sigma_y = 0.05$, corrupted prior with hole centered at $z_c = 1.0$ of radius $\\delta=0.3$. The likelihood is a sharp Gaussian peak at $z = 1.0$. The prior is zero in the interval $(0.7, 1.3)$. The product of these two functions will result in a posterior with two sharp peaks located at the edges of the excluded region, i.e., at $z=0.7$ and $z=1.3$. This is a classic signature of prior-data conflict, thus we expect $2$ modes. The mass near $z^\\dagger=1.0$ will be zero.\n\n*   **Case B (Misspecified, larger noise):** Same as Case A, but with $\\sigma_y = 0.5$. The likelihood is much broader. While the prior is still zero in $(0.7, 1.3)$, the broad likelihood might \"spill over\" this gap. The posterior will still exhibit a bimodal shape, but the peaks will be less sharp and potentially shifted outwards. Symmetrically, the distribution might be saddle-shaped but still have two local maxima. The increased variance from the likelihood will increase the posterior standard deviation compared to Case A.\n\n*   **Case C (Well-specified):** $h \\circ g(z) = z$, $y = 1.0$, $\\sigma_y = 0.05$, standard normal prior. The posterior is the product of two Gaussians (likelihood $\\mathcal{N}(z; 1.0, 0.05^2)$ and prior $\\mathcal{N}(z; 0, 1)$), which results in a Gaussian posterior. This posterior is unimodal. Due to the small noise variance $\\sigma_y^2$, the likelihood will dominate, so the posterior will be sharply peaked very close to $z = 1.0$ with a small standard deviation close to $0.05$. Most of the posterior mass will be concentrated around $z^\\dagger=1.0$.\n\n*   **Case D (Non-injective model):** $h \\circ g(z) = z^2$, $y = 1.0$, $\\sigma_y = 0.05$, standard normal prior. The likelihood term $\\exp(-(1.0 - z^2)^2 / (2\\sigma_y^2))$ is maximized when $z^2 = 1.0$, i.e., at both $z = 1.0$ and $z = -1.0$. The standard normal prior $\\exp(-z^2/2)$ is symmetric. The resulting posterior will therefore be bimodal and symmetric, with peaks near $z = 1.0$ and $z = -1.0$. A significant portion of the posterior mass will be near $z=1.0$, but also near $z=-1.0$.\n\nThese analyses guide the implementation and verification of the numerical results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing posterior statistics for four test cases.\n    \"\"\"\n\n    def compute_posterior_stats(g_func, h_func, y, sigma_y, z_dagger, use_corrupted_prior, z_c, delta):\n        \"\"\"\n        Computes posterior stats for a given set of parameters.\n        \"\"\"\n        # 1. Setup grid\n        z_grid = np.linspace(-5.0, 5.0, 20001)\n        dz = z_grid[1] - z_grid[0]\n        \n        # 2. Compute log posterior\n        # Log-likelihood term\n        h_of_g = h_func(g_func(z_grid))\n        log_likelihood = -0.5 * ((y - h_of_g) / sigma_y)**2\n        \n        # Log-prior term\n        log_prior = -0.5 * z_grid**2\n        if use_corrupted_prior:\n            hole_indices = np.abs(z_grid - z_c) < delta\n            log_prior[hole_indices] = -np.inf\n            \n        log_posterior = log_prior + log_likelihood\n        \n        # 3. Normalize posterior\n        # Stabilize by subtracting max before exponentiation\n        log_posterior -= np.nanmax(log_posterior[np.isfinite(log_posterior)])\n        \n        unnorm_posterior = np.exp(log_posterior)\n        \n        # Normalize using numerical integration (sum * dz)\n        integral = np.sum(unnorm_posterior) * dz\n        if integral == 0:\n            # Handle cases where posterior is zero everywhere (e.g., if z_dagger is in a hole with zero noise)\n            norm_posterior = np.zeros_like(unnorm_posterior)\n        else:\n            norm_posterior = unnorm_posterior / integral\n            \n        # 4. Compute required quantities\n        \n        # 4.1. Number of modes\n        num_modes = 0\n        max_density = np.max(norm_posterior)\n        threshold = 1e-4 * max_density\n        \n        if max_density > 0: # Proceed only if there's a non-zero posterior\n            for i in range(1, len(z_grid) - 1):\n                is_peak = norm_posterior[i] > norm_posterior[i-1] and norm_posterior[i] > norm_posterior[i+1]\n                if is_peak and norm_posterior[i] > threshold:\n                    num_modes += 1\n\n        # 4.2. Posterior standard deviation\n        mean_z = np.sum(z_grid * norm_posterior) * dz\n        var_z = np.sum(((z_grid - mean_z)**2) * norm_posterior) * dz\n        std_dev = np.sqrt(var_z)\n        \n        # 4.3. Posterior mass near z_dagger\n        rho = 0.15\n        mass_indices = np.abs(z_grid - z_dagger) <= rho\n        mass = np.sum(norm_posterior[mass_indices]) * dz\n        \n        return [num_modes, std_dev, mass]\n\n    # Define test cases from the problem statement\n    test_cases = [\n        # Case A: misspecified, corrupted prior, small noise\n        {'g': lambda z: z, 'h': lambda x: x, 'y': 1.0, 'sigma_y': 0.05, 'z_dagger': 1.0, \n         'use_corrupted': True, 'z_c': 1.0, 'delta': 0.3},\n        # Case B: misspecified, corrupted prior, larger noise\n        {'g': lambda z: z, 'h': lambda x: x, 'y': 1.0, 'sigma_y': 0.5, 'z_dagger': 1.0, \n         'use_corrupted': True, 'z_c': 1.0, 'delta': 0.3},\n        # Case C: well-specified, standard prior\n        {'g': lambda z: z, 'h': lambda x: x, 'y': 1.0, 'sigma_y': 0.05, 'z_dagger': 1.0, \n         'use_corrupted': False, 'z_c': 1.0, 'delta': 0.0},\n        # Case D: non-injective decoder-induced observation, standard prior\n        {'g': lambda z: z, 'h': lambda x: x**2, 'y': 1.0, 'sigma_y': 0.05, 'z_dagger': 1.0, \n         'use_corrupted': False, 'z_c': 1.0, 'delta': 0.0},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = compute_posterior_stats(\n            g_func=case['g'], \n            h_func=case['h'], \n            y=case['y'], \n            sigma_y=case['sigma_y'], \n            z_dagger=case['z_dagger'],\n            use_corrupted_prior=case['use_corrupted'],\n            z_c=case['z_c'],\n            delta=case['delta']\n        )\n        results.append(result)\n\n    # Format the final output string to ensure standard decimal notation\n    sublist_strs = []\n    for res in results:\n        n, s, m = res\n        # Use f-string formatting to avoid scientific notation\n        sublist_strs.append(f\"[{n},{s:.8f},{m:.8f}]\")\n    \n    # Final print statement in the exact required format\n    print(f\"[{','.join(sublist_strs)}]\")\n\nsolve()\n```", "id": "3374820"}]}