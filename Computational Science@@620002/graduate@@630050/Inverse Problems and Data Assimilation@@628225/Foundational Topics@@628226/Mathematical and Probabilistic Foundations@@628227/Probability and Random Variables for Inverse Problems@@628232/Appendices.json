{"hands_on_practices": [{"introduction": "This first exercise grounds the abstract theory of Bayesian inference in a concrete calculation. By working through a simple one-dimensional linear problem, you will derive the posterior distribution from first principles and compute the posterior mean, which represents the updated estimate of the unknown parameter. This practice reinforces the core mechanism of how a prior belief is updated by observational data in a conjugate Gaussian model [@problem_id:3414496].", "problem": "Consider a one-dimensional linear inverse problem with an unknown parameter $U \\in \\mathbb{R}$ modeled as a random variable with a Gaussian prior distribution $U \\sim \\mathcal{N}(m_{0}, s_{0}^{2})$. An observed datum $y \\in \\mathbb{R}$ is generated by the forward model $y = a u + \\eta$, where $u$ denotes a realization of $U$, $a \\in \\mathbb{R}$ is a known scalar, and $\\eta$ is independent additive noise with distribution $\\eta \\sim \\mathcal{N}(0, \\sigma^{2})$. The associated negative log-likelihood is defined by\n$$\n\\Phi(u; y) = \\frac{1}{2 \\sigma^{2}} \\left( y - a u \\right)^{2},\n$$\nand the unnormalized likelihood is given by $L(y \\mid u) = \\exp\\left( - \\Phi(u; y) \\right)$. Let $\\mu_{0}$ denote the prior measure of $U$ on $\\mathbb{R}$ and $\\mu^{y}$ the Bayesian posterior measure given the observation $y$.\n\nStarting from fundamental measure-theoretic Bayes principles and core properties of Gaussian integrals, derive the Radon–Nikodym derivative (RND) $\\frac{d \\mu^{y}}{d \\mu_{0}}(u)$ and its normalizing constant $Z(y)$ defined implicitly by the requirement that $\\mu^{y}$ is a probability measure. Then, using the prior-weighted averaging identity,\n$$\n\\mathbb{E}_{\\mu^{y}}[f(U)] = \\frac{ \\int_{\\mathbb{R}} f(u) \\, L(y \\mid u) \\, d\\mu_{0}(u) }{ \\int_{\\mathbb{R}} L(y \\mid u) \\, d\\mu_{0}(u) },\n$$\nevaluate the posterior expectation for the choice $f(u) = u$, with the specific parameter values\n$$\nm_{0} = 0.8, \\quad s_{0}^{2} = 0.49, \\quad a = 1.5, \\quad \\sigma^{2} = 0.36, \\quad y = 1.9.\n$$\nExpress the final numerical value of $\\mathbb{E}_{\\mu^{y}}[U]$ and round your answer to four significant figures. No physical units are associated with this quantity.", "solution": "The user-provided problem is valid as it is scientifically grounded in Bayesian statistics and inverse problem theory, is well-posed with a unique solution, objective, and self-contained with all necessary parameters and definitions.\n\nThe problem requires the derivation of the Radon–Nikodym derivative (RND) of the posterior measure with respect to the prior measure, its normalizing constant, and the subsequent calculation of the posterior expectation $\\mathbb{E}_{\\mu^{y}}[U]$.\n\nThe relationship between the posterior measure $\\mu^{y}$ and the prior measure $\\mu_{0}$ is given by Bayes' theorem. The provided expectation identity,\n$$\n\\mathbb{E}_{\\mu^{y}}[f(U)] = \\frac{ \\int_{\\mathbb{R}} f(u) \\, L(y \\mid u) \\, d\\mu_{0}(u) }{ \\int_{\\mathbb{R}} L(y \\mid u) \\, d\\mu_{0}(u) }\n$$\ncan be compared to the definition of expectation with respect to the posterior measure, $\\mathbb{E}_{\\mu^{y}}[f(U)] = \\int_{\\mathbb{R}} f(u) \\, d\\mu^{y}(u)$.\nBy the properties of measures, this implies that the Radon–Nikodym derivative of $\\mu^{y}$ with respect to $\\mu_{0}$ is given by\n$$\n\\frac{d \\mu^{y}}{d \\mu_{0}}(u) = \\frac{L(y \\mid u)}{Z(y)}\n$$\nwhere $Z(y)$ is the normalizing constant, also known as the evidence or marginal likelihood, defined as\n$$\nZ(y) = \\int_{\\mathbb{R}} L(y \\mid u) \\, d\\mu_{0}(u)\n$$\nsuch that $\\int_{\\mathbb{R}} d\\mu^{y}(u) = 1$.\n\nThe prior on $U$ is Gaussian, $U \\sim \\mathcal{N}(m_{0}, s_{0}^{2})$, so its probability density function (PDF) is $p_{0}(u) = \\frac{1}{\\sqrt{2\\pi s_{0}^{2}}} \\exp\\left(-\\frac{(u-m_{0})^{2}}{2s_{0}^{2}}\\right)$. The prior measure is $d\\mu_{0}(u) = p_{0}(u)du$.\nThe likelihood is given by $L(y \\mid u) = \\exp(-\\Phi(u;y)) = \\exp\\left(-\\frac{(y-au)^{2}}{2\\sigma^{2}}\\right)$.\n\nThe posterior PDF, $p(u \\mid y)$, is proportional to the product of the likelihood and the prior PDF:\n$$\np(u \\mid y) \\propto L(y \\mid u) p_0(u)\n$$\nSubstituting the expressions for the likelihood and prior density, we get:\n$$\np(u \\mid y) \\propto \\exp\\left(-\\frac{(y-au)^{2}}{2\\sigma^{2}}\\right) \\frac{1}{\\sqrt{2\\pi s_{0}^{2}}} \\exp\\left(-\\frac{(u-m_{0})^{2}}{2s_{0}^{2}}\\right)\n$$\nThe posterior is proportional to the exponential of a sum of quadratic terms in $u$. We can ignore constant factors for now and focus on the exponent, denoted as $J(u)$:\n$$\nJ(u) = -\\frac{1}{2} \\left[ \\frac{(y-au)^{2}}{\\sigma^{2}} + \\frac{(u-m_{0})^{2}}{s_{0}^{2}} \\right]\n$$\nExpanding the quadratic terms inside the brackets:\n$$\nJ(u) = -\\frac{1}{2} \\left[ \\frac{y^2-2ayu+a^2u^2}{\\sigma^{2}} + \\frac{u^2-2um_{0}+m_{0}^2}{s_{0}^{2}} \\right]\n$$\nGrouping terms by powers of $u$:\n$$\nJ(u) = -\\frac{1}{2} \\left[ u^2 \\left(\\frac{a^2}{\\sigma^2} + \\frac{1}{s_0^2}\\right) - 2u \\left(\\frac{ay}{\\sigma^2} + \\frac{m_0}{s_0^2}\\right) + \\left(\\frac{y^2}{\\sigma^2} + \\frac{m_0^2}{s_0^2}\\right) \\right]\n$$\nSince $J(u)$ is a quadratic function of $u$, the posterior distribution for $U$ given $y$ is also Gaussian, say $U|y \\sim \\mathcal{N}(m_{\\text{post}}, s_{\\text{post}}^2)$. The PDF of this posterior is of the form $p(u \\mid y) \\propto \\exp\\left(-\\frac{(u-m_{\\text{post}})^2}{2s_{\\text{post}}^2}\\right)$.\nComparing the form of $J(u)$ to the exponent of a Gaussian PDF, we can identify the posterior variance $s_{\\text{post}}^2$ and mean $m_{\\text{post}}$. The term multiplying $u^2$ gives the inverse of the posterior variance (the precision):\n$$\n\\frac{1}{s_{\\text{post}}^2} = \\frac{a^2}{\\sigma^2} + \\frac{1}{s_0^2}\n$$\nSo, the posterior variance is $s_{\\text{post}}^2 = \\left(\\frac{a^2}{\\sigma^2} + \\frac{1}{s_0^2}\\right)^{-1}$.\nThe term multiplying $-2u$ gives the ratio of the posterior mean to the posterior variance:\n$$\n\\frac{m_{\\text{post}}}{s_{\\text{post}}^2} = \\frac{ay}{\\sigma^2} + \\frac{m_0}{s_0^2}\n$$\nSolving for the posterior mean $m_{\\text{post}}$:\n$$\nm_{\\text{post}} = s_{\\text{post}}^2 \\left(\\frac{ay}{\\sigma^2} + \\frac{m_0}{s_0^2}\\right) = \\left(\\frac{a^2}{\\sigma^2} + \\frac{1}{s_0^2}\\right)^{-1} \\left(\\frac{ay}{\\sigma^2} + \\frac{m_0}{s_0^2}\\right)\n$$\nThe posterior expectation is $\\mathbb{E}_{\\mu^{y}}[U] = m_{\\text{post}}$.\n\nThe normalizing constant $Z(y)$ can be found by evaluating the integral $Z(y) = \\int_{\\mathbb{R}} L(y|u) p_0(u) du$. Recognizing that the forward model $y = aU + \\eta$ implies $y$ is a sum of Gaussian random variables, $y$ itself is Gaussian with mean $\\mathbb{E}[y] = a m_0$ and variance $\\text{Var}(y) = a^2 s_0^2 + \\sigma^2$. The marginal PDF of $y$ is $p(y) = \\frac{1}{\\sqrt{2\\pi(a^2s_0^2+\\sigma^2)}} \\exp\\left(-\\frac{(y-am_0)^2}{2(a^2s_0^2+\\sigma^2)}\\right)$.\nFrom Bayes' rule for densities, $p(y) = \\int p(y|u)p(u)du = \\int \\frac{1}{\\sqrt{2\\pi\\sigma^2}} L(y|u) p_0(u) du = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}Z(y)$.\nTherefore, $Z(y) = \\sqrt{2\\pi\\sigma^2}p(y) = \\sqrt{\\frac{\\sigma^2}{a^2s_0^2+\\sigma^2}} \\exp\\left(-\\frac{(y-am_0)^2}{2(a^2s_0^2+\\sigma^2)}\\right)$.\nThe RND is then $\\frac{d\\mu^y}{d\\mu_0}(u) = \\frac{L(y|u)}{Z(y)}$.\n\nWe now proceed with the numerical calculation of the posterior expectation $\\mathbb{E}_{\\mu^{y}}[U] = m_{\\text{post}}$ using the given values:\n$m_{0} = 0.8$, $s_{0}^{2} = 0.49$, $a = 1.5$, $\\sigma^{2} = 0.36$, $y = 1.9$.\n\nFirst, we calculate the components of the posterior precision $\\frac{1}{s_{\\text{post}}^2}$:\n$$\n\\frac{1}{s_{0}^{2}} = \\frac{1}{0.49} = \\frac{100}{49}\n$$\n$$\n\\frac{a^2}{\\sigma^2} = \\frac{(1.5)^2}{0.36} = \\frac{2.25}{0.36} = \\frac{225}{36} = \\frac{25}{4}\n$$\nThe posterior precision is:\n$$\n\\frac{1}{s_{\\text{post}}^2} = \\frac{25}{4} + \\frac{100}{49} = \\frac{25 \\times 49 + 100 \\times 4}{4 \\times 49} = \\frac{1225 + 400}{196} = \\frac{1625}{196}\n$$\nSo, the posterior variance is $s_{\\text{post}}^2 = \\frac{196}{1625}$.\n\nNext, we calculate the terms for the numerator of $m_{\\text{post}}$:\n$$\n\\frac{m_0}{s_0^2} = \\frac{0.8}{0.49} = \\frac{80}{49}\n$$\n$$\n\\frac{ay}{\\sigma^2} = \\frac{1.5 \\times 1.9}{0.36} = \\frac{2.85}{0.36} = \\frac{285}{36} = \\frac{95}{12}\n$$\nThe sum is:\n$$\n\\frac{ay}{\\sigma^2} + \\frac{m_0}{s_0^2} = \\frac{95}{12} + \\frac{80}{49} = \\frac{95 \\times 49 + 80 \\times 12}{12 \\times 49} = \\frac{4655 + 960}{588} = \\frac{5615}{588}\n$$\nFinally, we compute the posterior mean $m_{\\text{post}}$:\n$$\nm_{\\text{post}} = s_{\\text{post}}^2 \\left(\\frac{ay}{\\sigma^2} + \\frac{m_0}{s_0^2}\\right) = \\frac{196}{1625} \\times \\frac{5615}{588}\n$$\nRecognizing that $588 = 3 \\times 196$:\n$$\nm_{\\text{post}} = \\frac{196}{1625} \\times \\frac{5615}{3 \\times 196} = \\frac{5615}{3 \\times 1625}\n$$\nSimplify the fraction:\n$$\n\\frac{5615}{1625} = \\frac{5 \\times 1123}{5 \\times 325} = \\frac{1123}{325}\n$$\nSo,\n$$\nm_{\\text{post}} = \\frac{1}{3} \\times \\frac{1123}{325} = \\frac{1123}{975}\n$$\nPerforming the division to get a numerical value:\n$$\nm_{\\text{post}} = \\frac{1123}{975} \\approx 1.15179487...\n$$\nRounding to four significant figures, we get $1.152$.\n\nThus, the posterior expectation of $U$ is $\\mathbb{E}_{\\mu^{y}}[U] \\approx 1.152$.", "answer": "$$\\boxed{1.152}$$", "id": "3414496"}, {"introduction": "Effective scientific modeling requires not just estimating parameters, but also quantifying the uncertainty in our conclusions. This practice [@problem_id:3414519] explores the crucial distinction between uncertainty about a model's output (a credible interval) and uncertainty in a future observation (a prediction interval). Through analysis and implementation, you will decompose the total predictive variance into its constituent parts, revealing the contributions from parameter uncertainty, model error, and observation noise.", "problem": "You are asked to formalize, analyze, and implement a scalar linear-Gaussian data assimilation problem with explicit model error, focusing on the distinction between credible intervals and prediction intervals, and on a variance decomposition into parametric, discrepancy, and observation components.\n\nConsider a scalar parameter $ \\theta $ representing an unknown system state. You are given:\n- A prior distribution $ \\theta \\sim \\mathcal{N}(\\mu_0,\\tau_0^2) $.\n- A forward model $ m = a\\,\\theta + b $ with known coefficients $ a $ and $ b $.\n- An additive model discrepancy random variable $ \\delta \\sim \\mathcal{N}(0,\\sigma_\\delta^2) $.\n- An additive observation noise random variable $ \\epsilon \\sim \\mathcal{N}(0,\\sigma_\\epsilon^2) $.\n- A single observed datum $ z $ related to $ \\theta $ via the observation equation\n  $$ z = a\\,\\theta + b + \\delta + \\epsilon. $$\nAssume that $ \\theta $, $ \\delta $, and $ \\epsilon $ are mutually independent.\n\nYour tasks are:\n1) Starting only from the core definitions of the normal distribution, Bayes’ rule, and the law of total expectation and the law of total variance, derive the posterior distribution of $ \\theta $ given $ z $. Then derive the conditional distribution of $ m \\mid z $ and of a future replicate observation $ \\tilde z \\mid z $, where $ \\tilde z = m + \\delta' + \\epsilon' $ with $ \\delta' \\sim \\mathcal{N}(0,\\sigma_\\delta^2) $ and $ \\epsilon' \\sim \\mathcal{N}(0,\\sigma_\\epsilon^2) $ independent of everything else.\n2) Define the central credible interval for $ m \\mid z $ at level $ 0.95 $ and the central prediction interval for $ \\tilde z \\mid z $ at level $ 0.95 $, both based on the normal quantile of order $ 1 - \\alpha/2 $ with $ \\alpha = 0.05 $.\n3) Use the law of total variance to obtain a decomposition of the total predictive variance into three nonnegative components attributable to posterior parametric uncertainty, model discrepancy, and observation noise. That is, express\n   $$ \\operatorname{Var}(\\tilde z \\mid z) = v_{\\mathrm{par}} + v_{\\mathrm{disc}} + v_{\\mathrm{obs}}, $$\n   and identify $ v_{\\mathrm{par}} $, $ v_{\\mathrm{disc}} $, and $ v_{\\mathrm{obs}} $ explicitly in terms of $ a $, $ b $, $ \\mu_0 $, $ \\tau_0 $, $ \\sigma_\\delta $, $ \\sigma_\\epsilon $, and $ z $.\n\nImplementation requirements:\n- Implement a program that, for each parameter set in the test suite below, computes:\n  - the lower and upper endpoints of the $ 0.95 $ credible interval for $ m \\mid z $,\n  - the lower and upper endpoints of the $ 0.95 $ prediction interval for $ \\tilde z \\mid z $,\n  - the triple $ (v_{\\mathrm{par}}, v_{\\mathrm{disc}}, v_{\\mathrm{obs}}) $ from the variance decomposition in item $ 3 $.\n- For numerical stability and cross-language reproducibility, compute the standard normal quantile using a numerically stable routine. Round every floating-point number in the final output to $ 6 $ decimal places.\n\nTest suite (each tuple is $ (a,b,\\mu_0,\\tau_0,\\sigma_\\delta,\\sigma_\\epsilon,z) $):\n- Case A (general): $ (2.0,\\,1.0,\\,0.0,\\,1.5,\\,0.7,\\,0.5,\\,3.2) $.\n- Case B (no model discrepancy): $ (1.2,\\,-0.5,\\,0.8,\\,0.7,\\,0.0,\\,0.3,\\,0.4) $.\n- Case C (zero sensitivity): $ (0.0,\\,2.0,\\,-1.0,\\,1.0,\\,0.4,\\,0.4,\\,2.5) $.\n- Case D (weakly informative prior): $ (1.5,\\,0.2,\\,0.0,\\,10000.0,\\,0.6,\\,0.4,\\,1.7) $.\n\nFinal output format:\n- Your program must produce a single line of output containing a list of results, one per test case, preserving the order A, B, C, D.\n- The result for each case must be a list of $ 7 $ floats in the fixed order\n  $ [\\text{CI}_{\\mathrm{low}}, \\text{CI}_{\\mathrm{high}}, \\text{PI}_{\\mathrm{low}}, \\text{PI}_{\\mathrm{high}}, v_{\\mathrm{par}}, v_{\\mathrm{disc}}, v_{\\mathrm{obs}}] $,\n  all rounded to $ 6 $ decimal places.\n- The overall output must be a single line with no spaces anywhere, for example:\n  $$ [ [\\dots], [\\dots], [\\dots], [\\dots] ] $$\n  but with no spaces, i.e., use the exact format\n  $$ [[x_{1,1},x_{1,2},\\dots,x_{1,7}],[x_{2,1},\\dots,x_{2,7}],[x_{3,1},\\dots,x_{3,7}],[x_{4,1},\\dots,x_{4,7}]]. $$\n\nYour program must not read any input and must use only the parameter values provided above. It must be implemented in Python and use only the specified libraries. The final print must produce exactly one line in the specified format.", "solution": "The problem is well-posed, scientifically grounded, and self-contained, presenting a standard problem in Bayesian data assimilation for a linear-Gaussian model. We proceed with the derivation and solution.\n\n### 1. Derivation of Distributions\n\n**1.1. Posterior Distribution of $ \\theta \\mid z $**\n\nThe problem defines the prior distribution for the parameter $ \\theta $ and the observation model.\nThe prior is given as $ \\theta \\sim \\mathcal{N}(\\mu_0, \\tau_0^2) $. The probability density function (PDF) is:\n$$ p(\\theta) \\propto \\exp\\left(-\\frac{1}{2\\tau_0^2}(\\theta - \\mu_0)^2\\right) $$\n\nThe observation equation is $ z = a\\theta + b + \\delta + \\epsilon $. The terms $ \\delta \\sim \\mathcal{N}(0, \\sigma_\\delta^2) $ and $ \\epsilon \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2) $ are independent noise sources. Their sum, let's call it $ \\eta = \\delta + \\epsilon $, is also a normal random variable. By the properties of independent normal variables:\n$ E[\\eta] = E[\\delta] + E[\\epsilon] = 0 + 0 = 0 $\n$ \\operatorname{Var}(\\eta) = \\operatorname{Var}(\\delta) + \\operatorname{Var}(\\epsilon) = \\sigma_\\delta^2 + \\sigma_\\epsilon^2 $\nLet $ \\sigma_{\\text{tot}}^2 = \\sigma_\\delta^2 + \\sigma_\\epsilon^2 $. Thus, $ \\eta \\sim \\mathcal{N}(0, \\sigma_{\\text{tot}}^2) $.\n\nThe observation equation can be rewritten as $ z - (a\\theta + b) = \\eta $. This defines the likelihood of the observation $ z $ given the parameter $ \\theta $, which is $ z \\mid \\theta \\sim \\mathcal{N}(a\\theta + b, \\sigma_{\\text{tot}}^2) $. The likelihood function is:\n$$ p(z \\mid \\theta) \\propto \\exp\\left(-\\frac{1}{2\\sigma_{\\text{tot}}^2}(z - (a\\theta + b))^2\\right) $$\n\nAccording to Bayes' rule, the posterior distribution of $ \\theta $ given $ z $ is proportional to the product of the likelihood and the prior:\n$$ p(\\theta \\mid z) \\propto p(z \\mid \\theta) p(\\theta) $$\n$$ p(\\theta \\mid z) \\propto \\exp\\left(-\\frac{1}{2\\sigma_{\\text{tot}}^2}(z - a\\theta - b)^2\\right) \\exp\\left(-\\frac{1}{2\\tau_0^2}(\\theta - \\mu_0)^2\\right) $$\nThe posterior is proportional to the exponential of a sum of quadratic terms in $ \\theta $. This implies the posterior is also a normal distribution, say $ \\theta \\mid z \\sim \\mathcal{N}(\\mu_1, \\tau_1^2) $. We identify its parameters by analyzing the exponent, focusing on terms involving $ \\theta $:\n$$ -\\frac{1}{2} \\left[ \\frac{(a\\theta - (z-b))^2}{\\sigma_{\\text{tot}}^2} + \\frac{(\\theta - \\mu_0)^2}{\\tau_0^2} \\right] $$\n$$ = -\\frac{1}{2} \\left[ \\frac{a^2\\theta^2 - 2a\\theta(z-b)}{\\sigma_{\\text{tot}}^2} + \\frac{\\theta^2 - 2\\theta\\mu_0}{\\tau_0^2} + \\text{const} \\right] $$\n$$ = -\\frac{1}{2} \\left[ \\left(\\frac{a^2}{\\sigma_{\\text{tot}}^2} + \\frac{1}{\\tau_0^2}\\right)\\theta^2 - 2\\left(\\frac{a(z-b)}{\\sigma_{\\text{tot}}^2} + \\frac{\\mu_0}{\\tau_0^2}\\right)\\theta + \\text{const} \\right] $$\nFor a normal distribution $ \\mathcal{N}(\\mu_1, \\tau_1^2) $, the exponent is $ -\\frac{(\\theta - \\mu_1)^2}{2\\tau_1^2} = -\\frac{1}{2} \\left( \\frac{\\theta^2}{\\tau_1^2} - \\frac{2\\mu_1\\theta}{\\tau_1^2} + \\text{const} \\right) $.\nBy comparing coefficients of $ \\theta^2 $ and $ \\theta $, we find the posterior precision ($ 1/\\tau_1^2 $) and mean ($ \\mu_1 $):\n$$ \\frac{1}{\\tau_1^2} = \\frac{1}{\\tau_0^2} + \\frac{a^2}{\\sigma_{\\text{tot}}^2} \\implies \\tau_1^2 = \\left(\\frac{1}{\\tau_0^2} + \\frac{a^2}{\\sigma_{\\text{tot}}^2}\\right)^{-1} $$\n$$ \\frac{\\mu_1}{\\tau_1^2} = \\frac{\\mu_0}{\\tau_0^2} + \\frac{a(z-b)}{\\sigma_{\\text{tot}}^2} \\implies \\mu_1 = \\tau_1^2 \\left(\\frac{\\mu_0}{\\tau_0^2} + \\frac{a(z-b)}{\\sigma_{\\text{tot}}^2}\\right) $$\nThese are the parameters of the posterior distribution $ \\theta \\mid z \\sim \\mathcal{N}(\\mu_1, \\tau_1^2) $.\n\n**1.2. Conditional Distribution of $ m \\mid z $**\n\nThe forward model is $ m = a\\theta + b $. Since $ m $ is a linear function of $ \\theta $, its conditional distribution $ m \\mid z $ is also normal. We find its mean and variance using properties of expectation and variance:\n$ E[m \\mid z] = E[a\\theta + b \\mid z] = a E[\\theta \\mid z] + b = a\\mu_1 + b $.\n$ \\operatorname{Var}(m \\mid z) = \\operatorname{Var}(a\\theta + b \\mid z) = a^2 \\operatorname{Var}(\\theta \\mid z) = a^2 \\tau_1^2 $.\nThus, $ m \\mid z \\sim \\mathcal{N}(a\\mu_1 + b, a^2\\tau_1^2) $.\n\n**1.3. Conditional Distribution of $ \\tilde z \\mid z $**\n\nA future replicate observation is given by $ \\tilde z = m + \\delta' + \\epsilon' = a\\theta + b + \\delta' + \\epsilon' $, where $ \\delta' \\sim \\mathcal{N}(0, \\sigma_\\delta^2) $ and $ \\epsilon' \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2) $ are independent of all other variables. The distribution of $ \\tilde z $ given $ z $ is a predictive distribution. Given $ z $, the variables $ \\theta $ (whose distribution is $ p(\\theta|z) $), $ \\delta' $, and $ \\epsilon' $ are all independent. $ \\tilde z \\mid z $ is a linear combination of normal random variables, so it is also normal. We find its parameters using the law of total expectation and law of total variance.\n\nThe mean is:\n$ E[\\tilde z \\mid z] = E[a\\theta + b + \\delta' + \\epsilon' \\mid z] = a E[\\theta \\mid z] + b + E[\\delta' \\mid z] + E[\\epsilon' \\mid z] $.\nSince $ \\delta' $ and $ \\epsilon' $ are independent of $ z $, $ E[\\delta' \\mid z] = E[\\delta'] = 0 $ and $ E[\\epsilon' \\mid z] = E[\\epsilon'] = 0 $.\n$ E[\\tilde z \\mid z] = a\\mu_1 + b $.\n\nThe variance is, by independence of $ (\\theta \\mid z) $, $ \\delta' $, and $ \\epsilon' $:\n$ \\operatorname{Var}(\\tilde z \\mid z) = \\operatorname{Var}(a\\theta + b + \\delta' + \\epsilon' \\mid z) = \\operatorname{Var}(a\\theta \\mid z) + \\operatorname{Var}(\\delta' \\mid z) + \\operatorname{Var}(\\epsilon' \\mid z) $.\n$ \\operatorname{Var}(\\tilde z \\mid z) = a^2 \\operatorname{Var}(\\theta \\mid z) + \\operatorname{Var}(\\delta') + \\operatorname{Var}(\\epsilon') = a^2\\tau_1^2 + \\sigma_\\delta^2 + \\sigma_\\epsilon^2 $.\nThus, $ \\tilde z \\mid z \\sim \\mathcal{N}(a\\mu_1 + b, a^2\\tau_1^2 + \\sigma_\\delta^2 + \\sigma_\\epsilon^2) $.\n\n### 2. Credible and Prediction Intervals\n\nA central interval at level $ 1-\\alpha $ for a normal distribution $ \\mathcal{N}(\\mu, \\sigma^2) $ is given by $ [\\mu - q \\sigma, \\mu + q \\sigma] $, where $ q = \\Phi^{-1}(1 - \\alpha/2) $ is the $ (1 - \\alpha/2) $-quantile of the standard normal distribution $ \\mathcal{N}(0, 1) $. For this problem, $ \\alpha = 0.05 $, so we use the quantile $ q_{0.975} = \\Phi^{-1}(0.975) \\approx 1.959964 $.\n\nA **credible interval** is an interval in the domain of a posterior probability distribution. The $ 95\\% $ central credible interval for the model output $ m \\mid z $ is based on its posterior distribution $ \\mathcal{N}(a\\mu_1 + b, a^2\\tau_1^2) $.\nThe endpoints are:\n$ \\text{CI}_{\\text{low/high}} = (a\\mu_1 + b) \\mp q_{0.975} \\sqrt{a^2\\tau_1^2} = (a\\mu_1 + b) \\mp q_{0.975} |a|\\tau_1 $.\n\nA **prediction interval** is an interval in which a future observation will fall with a certain probability. The $ 95\\% $ central prediction interval for the future observation $ \\tilde z \\mid z $ is based on its predictive distribution $ \\mathcal{N}(a\\mu_1 + b, a^2\\tau_1^2 + \\sigma_\\delta^2 + \\sigma_\\epsilon^2) $.\nThe endpoints are:\n$ \\text{PI}_{\\text{low/high}} = (a\\mu_1 + b) \\mp q_{0.975} \\sqrt{a^2\\tau_1^2 + \\sigma_\\delta^2 + \\sigma_\\epsilon^2} $.\nThe prediction interval is always wider than the credible interval (for $ \\sigma_\\delta^2 + \\sigma_\\epsilon^2 > 0 $) as it accounts for additional sources of uncertainty from model discrepancy and observation noise a future measurement will be subject to.\n\n### 3. Variance Decomposition\n\nThe problem asks for a decomposition of the total predictive variance $ \\operatorname{Var}(\\tilde z \\mid z) $ using the law of total variance: $ \\operatorname{Var}(X) = E[\\operatorname{Var}(X \\mid Y)] + \\operatorname{Var}(E[X \\mid Y]) $. Here, $ X = \\tilde z \\mid z $ and the variable we condition on is the parameter $ \\theta $ (whose uncertainty is described by $ p(\\theta \\mid z) $).\n$ \\operatorname{Var}(\\tilde z \\mid z) = E[\\operatorname{Var}(\\tilde z \\mid \\theta, z) \\mid z] + \\operatorname{Var}(E[\\tilde z \\mid \\theta, z] \\mid z) $.\n\nFirst, we compute the inner terms, conditional on a specific value of $ \\theta $.\n$ E[\\tilde z \\mid \\theta, z] = E[a\\theta + b + \\delta' + \\epsilon' \\mid \\theta, z] = a\\theta + b $, since $ \\theta $ is fixed and $ \\delta', \\epsilon' $ are zero-mean and independent of $ z $.\n$ \\operatorname{Var}(\\tilde z \\mid \\theta, z) = \\operatorname{Var}(a\\theta + b + \\delta' + \\epsilon' \\mid \\theta, z) = \\operatorname{Var}(\\delta' + \\epsilon') = \\operatorname{Var}(\\delta') + \\operatorname{Var}(\\epsilon') = \\sigma_\\delta^2 + \\sigma_\\epsilon^2 $, since $ \\theta $ is fixed and $ \\delta', \\epsilon' $ are independent.\n\nNow, we substitute these back into the law of total variance formula:\nFirst term: $ E[\\operatorname{Var}(\\tilde z \\mid \\theta, z) \\mid z] = E[\\sigma_\\delta^2 + \\sigma_\\epsilon^2 \\mid z] = \\sigma_\\delta^2 + \\sigma_\\epsilon^2 $, since the variances are constants.\nSecond term: $ \\operatorname{Var}(E[\\tilde z \\mid \\theta, z] \\mid z) = \\operatorname{Var}(a\\theta + b \\mid z) = a^2 \\operatorname{Var}(\\theta \\mid z) = a^2\\tau_1^2 $.\n\nCombining them, we get the total predictive variance:\n$ \\operatorname{Var}(\\tilde z \\mid z) = a^2\\tau_1^2 + \\sigma_\\delta^2 + \\sigma_\\epsilon^2 $.\nThis expression naturally decomposes into three non-negative components:\n1.  **Parametric variance ($ v_{\\mathrm{par}} $):** $ v_{\\mathrm{par}} = a^2\\tau_1^2 $. This component represents the uncertainty in the model prediction due to the posterior uncertainty of the parameter $ \\theta $.\n2.  **Discrepancy variance ($ v_{\\mathrm{disc}} $):** $ v_{\\mathrm{disc}} = \\sigma_\\delta^2 $. This component is the uncertainty contribution from the inherent mismatch between the model and the true system (model error).\n3.  **Observation variance ($ v_{\\mathrm{obs}} $):** $ v_{\\mathrm{obs}} = \\sigma_\\epsilon^2 $. This component represents the uncertainty arising from the measurement noise in a future observation.\n\nThe decomposition is $ \\operatorname{Var}(\\tilde z \\mid z) = v_{\\mathrm{par}} + v_{\\mathrm{disc}} + v_{\\mathrm{obs}} $.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Solves the scalar linear-Gaussian data assimilation problem for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each tuple is (a, b, mu0, tau0, sigma_delta, sigma_epsilon, z).\n    test_cases = [\n        (2.0, 1.0, 0.0, 1.5, 0.7, 0.5, 3.2),      # Case A (general)\n        (1.2, -0.5, 0.8, 0.7, 0.0, 0.3, 0.4),      # Case B (no model discrepancy)\n        (0.0, 2.0, -1.0, 1.0, 0.4, 0.4, 2.5),      # Case C (zero sensitivity)\n        (1.5, 0.2, 0.0, 10000.0, 0.6, 0.4, 1.7),  # Case D (weakly informative prior)\n    ]\n\n    results = []\n    # Quantile for 95% central interval\n    q_975 = norm.ppf(0.975)\n\n    for case in test_cases:\n        a, b, mu0, tau0, sigma_delta, sigma_epsilon, z = case\n\n        # 1. Calculate total noise variance from model discrepancy and observation error\n        sigma_tot_sq = sigma_delta**2 + sigma_epsilon**2\n\n        # 2. Compute posterior distribution of theta: theta|z ~ N(mu1, tau1_sq).\n        # We work with precisions for numerical stability, especially for large tau0.\n        tau0_sq = tau0**2\n        \n        # In a case with zero total noise, the posterior would be a point mass.\n        # This is not in the test suite, but a robust implementation would check for it.\n        # The logic here is stable for all test cases provided.\n        prec0 = 1.0 / tau0_sq\n        \n        # If sigma_tot_sq is zero, the likelihood precision is infinite. We handle a=0 separately.\n        if a == 0.0:\n            # If a=0, data z gives no info on theta, so posterior = prior\n            mu1 = mu0\n            tau1_sq = tau0_sq\n        else:\n             # This branch is safe if sigma_tot_sq is not zero\n            prec_like = (a**2) / sigma_tot_sq\n            prec1 = prec0 + prec_like\n            tau1_sq = 1.0 / prec1\n            # Posterior mean is the precision-weighted average of prior and data\n            mu1 = tau1_sq * (prec0 * mu0 + a * (z - b) / sigma_tot_sq)\n\n        # 3. Compute posterior distribution of model output m = a*theta + b\n        # m|z ~ N(mu_m, var_m)\n        mu_m = a * mu1 + b\n        var_m = a**2 * tau1_sq\n\n        # 4. Compute 95% Credible Interval (CI) for m|z\n        std_m = np.sqrt(var_m)\n        ci_half_width = q_975 * std_m\n        ci_low = mu_m - ci_half_width\n        ci_high = mu_m + ci_half_width\n\n        # 5. Compute predictive distribution of a future observation z_tilde\n        # z_tilde|z ~ N(mu_m, var_z_tilde)\n        var_z_tilde = var_m + sigma_delta**2 + sigma_epsilon**2\n        std_z_tilde = np.sqrt(var_z_tilde)\n        \n        # 6. Compute 95% Prediction Interval (PI) for z_tilde|z\n        pi_half_width = q_975 * std_z_tilde\n        pi_low = mu_m - pi_half_width\n        pi_high = mu_m + pi_half_width\n\n        # 7. Compute the variance decomposition\n        v_par = var_m\n        v_disc = sigma_delta**2\n        v_obs = sigma_epsilon**2\n        \n        # 8. Store the 7 required floating point numbers for this case\n        case_results = [ci_low, ci_high, pi_low, pi_high, v_par, v_disc, v_obs]\n        results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # The format is [[...],[...],...] with no spaces and 6 decimal places.\n    output_str = \"[\" + \",\".join([\n        \"[\" + \",\".join([f'{val:.6f}' for val in res]) + \"]\" \n        for res in results\n    ]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3414519"}, {"introduction": "While analytical solutions are instructive, most real-world inverse problems require computational methods. This exercise introduces the Gibbs sampler, a foundational Markov Chain Monte Carlo (MCMC) algorithm, for an augmented linear-Gaussian model [@problem_id:3414544]. You will derive the necessary conditional distributions that drive the sampler and analyze its efficiency by computing the spectral radius of the mean-field update operator, linking algorithmic implementation to theoretical convergence rates.", "problem": "Consider a linear inverse problem with Gaussian structure. Let the unknown state be a vector $U \\in \\mathbb{R}^n$, and let the observed data be a vector $y \\in \\mathbb{R}^m$ related through a known linear forward operator $A \\in \\mathbb{R}^{m \\times n}$. Assume a Gaussian prior $U \\sim \\mathcal{N}(0, \\Lambda^{-1})$ where $\\Lambda \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite precision matrix. Introduce an auxiliary variable $Z \\in \\mathbb{R}^n$ with the Gaussian conditional $Z \\mid U \\sim \\mathcal{N}(U, \\eta^{-1} I_n)$ for a given scalar precision parameter $\\eta > 0$, and model the data likelihood as $y \\mid Z \\sim \\mathcal{N}(A Z, \\Gamma^{-1})$ where $\\Gamma \\in \\mathbb{R}^{m \\times m}$ is a symmetric positive definite precision matrix. This construction yields a conjugate linear Gaussian augmented model for $(U, Z)$ given $y$.\n\nStarting from fundamental definitions of multivariate normal distributions and Bayes' rule for conjugate Gaussian models, derive the full conditional distributions $U \\mid Z, y$ and $Z \\mid U, y$, demonstrating that each is Gaussian with mean given by a linear function of the conditioning variable and covariance given by the inverse of a sum of precisions. Then, implement a Gibbs sampler that alternates sampling from these two conditionals:\n- Sample $Z^{k+1} \\sim p(Z \\mid U^k, y)$.\n- Sample $U^{k+1} \\sim p(U \\mid Z^{k+1}, y)$.\n\nFinally, derive the deterministic linear update operator for the conditional expectation map of the $U$-marginal under one full Gibbs sweep, and compute its convergence rate in terms of the spectral radius. Specifically, show that there exists a matrix $B \\in \\mathbb{R}^{n \\times n}$ such that\n$$\n\\mathbb{E}[U^{k+1} \\mid U^k] \\;=\\; B \\, U^k \\;+\\; c,\n$$\nfor some vector $c \\in \\mathbb{R}^n$, and that the asymptotic linear convergence rate of the mean is controlled by the spectral radius $\\rho(B)$ defined as the maximum modulus of the eigenvalues of $B$.\n\nYour program must:\n- Implement the Gibbs sampler described above (you may use it internally to verify correctness, but the final reported quantity is the spectral radius of the update operator $B$).\n- For each test case parameters provided below, construct the matrices and vectors, compute the conditional covariance matrices, compute the operator $B$ for the $U$-update, and output the spectral radius $\\rho(B)$ as a floating-point number.\n\nUse the following test suite of parameter values. Each test case is specified by $(n, m, A, \\Lambda, \\Gamma, \\eta, y)$:\n1. Happy path: $n = 3$, $m = 3$, $A = \\operatorname{diag}(1, 2, 3)$, $\\Lambda = \\operatorname{diag}(2, 3, 4)$, $\\Gamma = 2 I_3$, $\\eta = 1$, $y = (1, -1, 0.5)^\\top$.\n2. Slow mixing boundary: $n = 3$, $m = 3$, $A = \\operatorname{diag}(1, 2, 3)$, $\\Lambda = \\operatorname{diag}(2, 3, 4)$, $\\Gamma = 2 I_3$, $\\eta = 100$, $y = (1, -1, 0.5)^\\top$.\n3. Anisotropic prior and correlated forward map: $n = 2$, $m = 2$, $A = \\begin{bmatrix} 1 & 0.5 \\\\ 0.5 & 1 \\end{bmatrix}$, $\\Lambda = \\operatorname{diag}(0.1, 10)$, $\\Gamma = \\operatorname{diag}(5, 1)$, $\\eta = 0.2$, $y = (0.5, -0.2)^\\top$.\n4. Fast mixing boundary: $n = 3$, $m = 3$, $A = \\operatorname{diag}(1, 2, 3)$, $\\Lambda = \\operatorname{diag}(2, 3, 4)$, $\\Gamma = 50 I_3$, $\\eta = 1$, $y = (1, -1, 0.5)^\\top$.\n\nThe final output format must be a single line containing the list of spectral radii for the test cases in the order listed above, as a comma-separated list enclosed in square brackets, for example, $[r_1, r_2, r_3, r_4]$, where each $r_i$ is a floating-point number. No physical units are involved. Angles are not used. Percentages are not used. The program must be self-contained and runnable without any external input.", "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in Bayesian statistics and linear algebra, well-posed, objective, and internally consistent. All necessary parameters for the required computation are provided. I will therefore proceed with a full solution.\n\nThe specified probabilistic model is:\n1.  Prior: $U \\sim \\mathcal{N}(0, \\Lambda^{-1})$, with probability density function (PDF) $p(U) \\propto \\exp\\left(-\\frac{1}{2} U^\\top \\Lambda U\\right)$.\n2.  Auxiliary Conditional: $Z \\mid U \\sim \\mathcal{N}(U, \\eta^{-1} I_n)$, with PDF $p(Z|U) \\propto \\exp\\left(-\\frac{\\eta}{2} (Z-U)^\\top(Z-U)\\right)$.\n3.  Likelihood: $y \\mid Z \\sim \\mathcal{N}(AZ, \\Gamma^{-1})$, with PDF $p(y|Z) \\propto \\exp\\left(-\\frac{1}{2} (y-AZ)^\\top \\Gamma (y-AZ)\\right)$.\n\nThe joint posterior distribution of the unobserved variables $(U, Z)$ given the data $y$ is given by Bayes' rule:\n$$p(U, Z \\mid y) \\propto p(y \\mid Z, U) p(Z \\mid U) p(U)$$\nDue to the specified model structure, which forms a Markov chain $U \\to Z \\to y$, the likelihood term simplifies to $p(y \\mid Z, U) = p(y \\mid Z)$. Thus,\n$$p(U, Z \\mid y) \\propto p(y \\mid Z) p(Z \\mid U) p(U)$$\nThe negative log-posterior, ignoring constants, is a quadratic function of $U$ and $Z$:\n$$\\mathcal{L}(U, Z) \\propto \\frac{1}{2} U^\\top \\Lambda U + \\frac{\\eta}{2} (Z - U)^\\top (Z - U) + \\frac{1}{2} (y - AZ)^\\top \\Gamma (y - AZ)$$\nThis confirms that the joint posterior $p(U, Z \\mid y)$ is a multivariate Gaussian distribution. A Gibbs sampler is constructed by alternately sampling from the full conditional distributions $p(U \\mid Z, y)$ and $p(Z \\mid U, y)$.\n\n### Derivation of the full conditional $p(U \\mid Z, y)$\n\nTo find the distribution of $U$ conditional on $Z$ and $y$, we examine the posterior PDF as a function of $U$, treating $Z$ and $y$ as fixed. The term $p(y \\mid Z)$ does not depend on $U$. Therefore,\n$$p(U \\mid Z, y) \\propto p(Z \\mid U) p(U)$$\nThe negative log-conditional is:\n$$-\\log p(U \\mid Z, y) \\propto \\frac{1}{2} U^\\top \\Lambda U + \\frac{\\eta}{2} (Z - U)^\\top (Z - U)$$\nExpanding the terms involving $U$:\n$$-\\log p(U \\mid Z, y) \\propto \\frac{1}{2} U^\\top \\Lambda U + \\frac{\\eta}{2} (U^\\top U - 2U^\\top Z + Z^\\top Z)$$\nCollecting terms quadratic and linear in $U$:\n$$-\\log p(U \\mid Z, y) \\propto \\frac{1}{2} U^\\top (\\Lambda + \\eta I_n) U - (\\eta Z)^\\top U + \\mathrm{const.}$$\nThis is the kernel of a multivariate Gaussian distribution for $U$. The precision matrix of this conditional distribution is the coefficient of the quadratic term, and the mean can be found by completing the square. The precision matrix is $\\Sigma_{U|Z,y}^{-1} = \\Lambda + \\eta I_n$. The mean $\\mu_{U|Z,y}$ satisfies $\\Sigma_{U|Z,y}^{-1} \\mu_{U|Z,y} = \\eta Z$.\nThus, $U \\mid Z, y \\sim \\mathcal{N}(\\mu_{U|Z,y}, \\Sigma_{U|Z,y})$ with:\n-   Precision: $\\Sigma_{U|Z,y}^{-1} = \\Lambda + \\eta I_n$\n-   Covariance: $\\Sigma_{U|Z,y} = (\\Lambda + \\eta I_n)^{-1}$\n-   Mean: $\\mu_{U|Z,y} = \\mathbb{E}[U \\mid Z, y] = (\\Lambda + \\eta I_n)^{-1} (\\eta Z) = \\eta (\\Lambda + \\eta I_n)^{-1} Z$\n\nThe mean is a linear function of the conditioning variable $Z$.\n\n### Derivation of the full conditional $p(Z \\mid U, y)$\n\nTo find the distribution of $Z$ conditional on $U$ and $y$, we examine the joint posterior PDF as a function of $Z$, treating $U$ and $y$ as fixed. The term $p(U)$ does not depend on $Z$. Therefore,\n$$p(Z \\mid U, y) \\propto p(y \\mid Z) p(Z \\mid U)$$\nThe negative log-conditional is:\n$$-\\log p(Z \\mid U, y) \\propto \\frac{\\eta}{2} (Z - U)^\\top(Z - U) + \\frac{1}{2} (y - AZ)^\\top \\Gamma (y - AZ)$$\nExpanding the terms involving $Z$:\n$$-\\log p(Z \\mid U, y) \\propto \\frac{\\eta}{2} (Z^\\top Z - 2Z^\\top U) + \\frac{1}{2} (Z^\\top A^\\top \\Gamma A Z - 2Z^\\top A^\\top \\Gamma y)$$\nCollecting terms quadratic and linear in $Z$:\n$$-\\log p(Z \\mid U, y) \\propto \\frac{1}{2} Z^\\top(A^\\top \\Gamma A + \\eta I_n) Z - (A^\\top \\Gamma y + \\eta U)^\\top Z + \\mathrm{const.}$$\nThis is the kernel of a Gaussian distribution for $Z$. The precision matrix is $\\Sigma_{Z|U,y}^{-1} = A^\\top \\Gamma A + \\eta I_n$. The mean $\\mu_{Z|U,y}$ satisfies $\\Sigma_{Z|U,y}^{-1} \\mu_{Z|U,y} = A^\\top \\Gamma y + \\eta U$.\nThus, $Z \\mid U, y \\sim \\mathcal{N}(\\mu_{Z|U,y}, \\Sigma_{Z|U,y})$ with:\n-   Precision: $\\Sigma_{Z|U,y}^{-1} = A^\\top \\Gamma A + \\eta I_n$\n-   Covariance: $\\Sigma_{Z|U,y} = (A^\\top \\Gamma A + \\eta I_n)^{-1}$\n-   Mean: $\\mu_{Z|U,y} = \\mathbb{E}[Z \\mid U, y] = (A^\\top \\Gamma A + \\eta I_n)^{-1} (A^\\top \\Gamma y + \\eta U)$\n\nThe mean is a linear function of the conditioning variable $U$.\n\n### Derivation of the Gibbs Sampler Mean Update Operator\n\nThe Gibbs sampler proceeds by drawing samples $Z^{k+1} \\sim p(Z \\mid U^k, y)$ and $U^{k+1} \\sim p(U \\mid Z^{k+1}, y)$. We are interested in the evolution of the conditional expectation of $U$.\nLet $U^k$ be the sample at iteration $k$. We want to find the relationship $\\mathbb{E}[U^{k+1} \\mid U^k] = B U^k + c$.\nUsing the law of total expectation:\n$$\\mathbb{E}[U^{k+1} \\mid U^k] = \\mathbb{E}_{Z^{k+1} \\mid U^k, y} \\left[ \\mathbb{E}[U^{k+1} \\mid Z^{k+1}, U^k, y] \\right]$$\nFrom the Markov structure of the sampler, $U^{k+1}$ depends only on $Z^{k+1}$ (and data $y$), so $\\mathbb{E}[U^{k+1} \\mid Z^{k+1}, U^k, y] = \\mathbb{E}[U \\mid Z^{k+1}, y] = \\mu_{U|Z,y}(Z^{k+1})$.\n$$\\mathbb{E}[U^{k+1} \\mid U^k] = \\mathbb{E}_{Z^{k+1} \\mid U^k, y} \\left[ \\mu_{U|Z,y}(Z^{k+1}) \\right]$$\nSubstituting the expression for the mean $\\mu_{U|Z,y}(Z) = \\eta (\\Lambda + \\eta I_n)^{-1} Z$:\n$$\\mathbb{E}[U^{k+1} \\mid U^k] = \\mathbb{E}_{Z^{k+1} \\mid U^k, y} \\left[ \\eta (\\Lambda + \\eta I_n)^{-1} Z^{k+1} \\right]$$\nSince expectation is a linear operator:\n$$\\mathbb{E}[U^{k+1} \\mid U^k] = \\eta (\\Lambda + \\eta I_n)^{-1} \\mathbb{E}[Z^{k+1} \\mid U^k, y]$$\nThe expectation $\\mathbb{E}[Z^{k+1} \\mid U^k, y]$ is the mean of the distribution $p(Z \\mid U^k, y)$, which is $\\mu_{Z|U,y}(U^k)$:\n$$\\mathbb{E}[Z^{k+1} \\mid U^k, y] = (A^\\top \\Gamma A + \\eta I_n)^{-1} (A^\\top \\Gamma y + \\eta U^k)$$\nSubstituting this back gives:\n$$\\mathbb{E}[U^{k+1} \\mid U^k] = \\eta (\\Lambda + \\eta I_n)^{-1} \\left[ (A^\\top \\Gamma A + \\eta I_n)^{-1} (\\eta U^k + A^\\top \\Gamma y) \\right]$$\nRearranging into the form $B U^k + c$:\n$$\\mathbb{E}[U^{k+1} \\mid U^k] = \\left( \\eta^2 (\\Lambda + \\eta I_n)^{-1} (A^\\top \\Gamma A + \\eta I_n)^{-1} \\right) U^k + \\left( \\eta (\\Lambda + \\eta I_n)^{-1} (A^\\top \\Gamma A + \\eta I_n)^{-1} A^\\top \\Gamma y \\right)$$\nFrom this, we identify the linear update operator $B$:\n$$B = \\eta^2 (\\Lambda + \\eta I_n)^{-1} (A^\\top \\Gamma A + \\eta I_n)^{-1}$$\nThe asymptotic linear convergence rate of the mean of the Gibbs sampler is determined by the spectral radius of $B$, denoted $\\rho(B)$, which is the maximum modulus of its eigenvalues.\n\n### Implementation and Computation\n\nThe provided program implements the calculation of the matrix $B$ and its spectral radius $\\rho(B)$ for each of the test cases. For each case, the parameters $(n, m, A, \\Lambda, \\Gamma, \\eta)$ are used to construct the matrices $A$, $\\Lambda$, and $\\Gamma$ as `numpy` arrays. The identity matrix $I_n$ is also constructed. The formula for $B$ derived above is then implemented using `numpy`'s linear algebra functions for matrix inversion (`np.linalg.inv`) and matrix multiplication (`@`). Finally, the eigenvalues of the resulting matrix $B$ are computed using `np.linalg.eigvals`, and the spectral radius is found by taking the maximum of their absolute values with `np.max(np.abs(...))`. The process is repeated for all test cases, and the results are collected and printed in the specified format. The data vector $y$ is not required for the computation of $B$ and its spectral radius.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the spectral radius of the Gibbs sampler mean update operator B for\n    a series of test cases based on a linear Gaussian augmented model.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # The data vector 'y' is not needed for the calculation of matrix B.\n    test_cases = [\n        {\n            'n': 3, 'm': 3,\n            'A': np.diag([1.0, 2.0, 3.0]),\n            'Lambda': np.diag([2.0, 3.0, 4.0]),\n            'Gamma': 2.0 * np.eye(3),\n            'eta': 1.0\n        },\n        {\n            'n': 3, 'm': 3,\n            'A': np.diag([1.0, 2.0, 3.0]),\n            'Lambda': np.diag([2.0, 3.0, 4.0]),\n            'Gamma': 2.0 * np.eye(3),\n            'eta': 100.0\n        },\n        {\n            'n': 2, 'm': 2,\n            'A': np.array([[1.0, 0.5], [0.5, 1.0]]),\n            'Lambda': np.diag([0.1, 10.0]),\n            'Gamma': np.diag([5.0, 1.0]),\n            'eta': 0.2\n        },\n        {\n            'n': 3, 'm': 3,\n            'A': np.diag([1.0, 2.0, 3.0]),\n            'Lambda': np.diag([2.0, 3.0, 4.0]),\n            'Gamma': 50.0 * np.eye(3),\n            'eta': 1.0\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        n = params['n']\n        A = params['A']\n        Lambda = params['Lambda']\n        Gamma = params['Gamma']\n        eta = params['eta']\n\n        # Identity matrix of size n x n\n        In = np.eye(n)\n\n        # The operator B is given by B = M_U * M_Z, where\n        # M_U = eta * (Lambda + eta * I)^-1\n        # M_Z = eta * (A^T * Gamma * A + eta * I)^-1\n        # So, B = eta^2 * (Lambda + eta * I)^-1 * (A^T * Gamma * A + eta * I)^-1\n\n        # Calculate the first inverse term related to U\n        inv_U_term = np.linalg.inv(Lambda + eta * In)\n\n        # Calculate the second inverse term related to Z\n        At_Gamma_A = A.T @ Gamma @ A\n        inv_Z_term = np.linalg.inv(At_Gamma_A + eta * In)\n\n        # Construct the matrix B\n        B = (eta**2) * inv_U_term @ inv_Z_term\n\n        # Compute the eigenvalues of B\n        eigenvalues = np.linalg.eigvals(B)\n\n        # The spectral radius is the maximum absolute eigenvalue\n        spectral_radius = np.max(np.abs(eigenvalues))\n        results.append(spectral_radius)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3414544"}]}