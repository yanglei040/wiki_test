{"hands_on_practices": [{"introduction": "This first practice is a critical thought experiment that goes to the heart of ill-conditioning. We will investigate how observational noise $\\eta$, even with a fixed budget $\\|\\eta\\|_2 = \\delta$, can be maximally amplified by the Moore-Penrose pseudoinverse $A^\\dagger$. By deriving the structure of this \"worst-case\" noise and its effect on the solution, you will gain a deep, intuitive understanding of how the geometry of the forward operator $A$, as described by its singular vectors, governs the stability of the inverse problem [@problem_id:3404452].", "problem": "Consider a linearized observation operator in a variational Data Assimilation (DA) setting, represented by a real matrix $A \\in \\mathbb{R}^{m \\times n}$ with singular value decomposition (SVD) $A = U \\Sigma V^{\\mathsf{T}}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with nonnegative diagonal entries $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{r} > 0$ and $\\sigma_{i} = 0$ for $i > r$, with $r = \\operatorname{rank}(A) \\geq 1$. The Moore–Penrose pseudoinverse is $A^{\\dagger} = V \\Sigma^{\\dagger} U^{\\mathsf{T}}$, where $\\Sigma^{\\dagger}$ is obtained by inverting the nonzero singular values and leaving zeros in place.\n\nAssume the true state $x_{\\star} \\in \\mathbb{R}^{n}$ generates noise-contaminated observations $y = A x_{\\star} + \\eta$, where the observation error $\\eta \\in \\mathbb{R}^{m}$ is deterministic but unknown and satisfies the noise budget $\\|\\eta\\|_{2} = \\delta$ with a known $\\delta > 0$. The DA reconstructor computes $\\hat{x} = A^{\\dagger} y$. The reconstruction error attributable to the observation error is $e = \\hat{x} - A^{\\dagger} A x_{\\star} = A^{\\dagger} \\eta$.\n\nUsing only the definitions above and fundamental properties of orthogonal transformations and Euclidean norms, determine the exact maximum possible value of $\\|e\\|_{2}$ over all $\\eta$ with $\\|\\eta\\|_{2} = \\delta$, expressed in closed form in terms of $\\delta$ and the singular values of $A$. You should not assume any result beyond the definitions given and standard properties of orthogonal matrices and Euclidean norms. In your reasoning, indicate which direction in data space and which direction in state space realize this maximum, and explain the relationship to the right singular vector associated with the smallest positive singular value. Interpret the maximizing configuration in the context of DA as the worst-case noise-induced analysis error mechanism.\n\nYour final answer must be a single closed-form analytic expression. Do not include any units in your final answer. Do not provide the maximizing directions or any explanatory text in the final answer.", "solution": "The problem statement has been validated and is deemed valid. It is a well-posed, scientifically grounded problem in the field of inverse problems and data assimilation.\n\nOur objective is to find the maximum possible value of the Euclidean norm of the reconstruction error, $\\|e\\|_{2}$, given that $e = A^{\\dagger} \\eta$ and the observation error $\\eta$ satisfies $\\|\\eta\\|_{2} = \\delta$. The matrix $A$ has the singular value decomposition (SVD) $A = U \\Sigma V^{\\mathsf{T}}$, and its Moore-Penrose pseudoinverse is $A^{\\dagger} = V \\Sigma^{\\dagger} U^{\\mathsf{T}}$.\n\nThe norm of the error vector $e$ is given by:\n$$\n\\|e\\|_{2} = \\|A^{\\dagger} \\eta\\|_{2}\n$$\nSubstituting the SVD expression for $A^{\\dagger}$, we get:\n$$\n\\|e\\|_{2} = \\|V \\Sigma^{\\dagger} U^{\\mathsf{T}} \\eta\\|_{2}\n$$\nThe problem states that $V \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix. A fundamental property of orthogonal transformations is that they preserve the Euclidean norm. That is, for any vector $z \\in \\mathbb{R}^{n}$, $\\|Vz\\|_{2} = \\|z\\|_{2}$. Applying this property, we can remove the orthogonal matrix $V$ from the norm calculation:\n$$\n\\|e\\|_{2} = \\|\\Sigma^{\\dagger} U^{\\mathsf{T}} \\eta\\|_{2}\n$$\nNow, let us define a new vector $\\tilde{\\eta} \\in \\mathbb{R}^{m}$ as the projection of the noise vector $\\eta$ onto the basis of left singular vectors (the columns of $U$):\n$$\n\\tilde{\\eta} = U^{\\mathsf{T}} \\eta\n$$\nSince $U \\in \\mathbb{R}^{m \\times m}$ is also an orthogonal matrix, its transpose $U^{\\mathsf{T}}$ is orthogonal as well. Therefore, this transformation also preserves the Euclidean norm:\n$$\n\\|\\tilde{\\eta}\\|_{2} = \\|U^{\\mathsf{T}} \\eta\\|_{2} = \\|\\eta\\|_{2} = \\delta\n$$\nThe problem is now transformed into finding the maximum of $\\|\\Sigma^{\\dagger} \\tilde{\\eta}\\|_{2}$ subject to the constraint $\\|\\tilde{\\eta}\\|_{2} = \\delta$.\n\nLet's analyze the term $\\|\\Sigma^{\\dagger} \\tilde{\\eta}\\|_{2}$. The matrix $\\Sigma^{\\dagger}$ is a diagonal matrix of size $n \\times m$. Its first $r$ diagonal entries are the reciprocals of the positive singular values of $A$, i.e., $1/\\sigma_1, 1/\\sigma_2, \\dots, 1/\\sigma_r$. All other entries are zero. Let $\\tilde{\\eta} = (\\tilde{\\eta}_1, \\tilde{\\eta}_2, \\dots, \\tilde{\\eta}_m)^{\\mathsf{T}}$. The product $\\Sigma^{\\dagger} \\tilde{\\eta}$ is an $n$-dimensional vector:\n$$\n\\Sigma^{\\dagger} \\tilde{\\eta} = \\begin{pmatrix}\n1/\\sigma_1 & 0 & \\dots & 0 & \\dots & 0 \\\\\n0 & 1/\\sigma_2 & \\dots & 0 & \\dots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & & \\vdots \\\\\n0 & 0 & \\dots & 1/\\sigma_r & \\dots & 0 \\\\\n\\vdots & \\vdots & & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 0 & \\dots & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n\\tilde{\\eta}_1 \\\\ \\tilde{\\eta}_2 \\\\ \\vdots \\\\ \\tilde{\\eta}_r \\\\ \\vdots \\\\ \\tilde{\\eta}_m\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\tilde{\\eta}_1 / \\sigma_1 \\\\\n\\tilde{\\eta}_2 / \\sigma_2 \\\\\n\\vdots \\\\\n\\tilde{\\eta}_r / \\sigma_r \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{pmatrix}\n$$\nThe squared norm of this vector is:\n$$\n\\|e\\|_{2}^{2} = \\|\\Sigma^{\\dagger} \\tilde{\\eta}\\|_{2}^{2} = \\sum_{i=1}^{r} \\left(\\frac{\\tilde{\\eta}_i}{\\sigma_i}\\right)^2 = \\sum_{i=1}^{r} \\frac{1}{\\sigma_i^2} \\tilde{\\eta}_i^2\n$$\nOur goal is to maximize this quantity subject to the constraint $\\|\\tilde{\\eta}\\|_{2}^2 = \\sum_{i=1}^{m} \\tilde{\\eta}_i^2 = \\delta^2$. Since the sum for $\\|e\\|_2^2$ only involves components up to $i=r$, any component $\\tilde{\\eta}_i$ for $i > r$ does not contribute to the error norm. To maximize the sum, we should set $\\tilde{\\eta}_i = 0$ for $i>r$. The constraint becomes $\\sum_{i=1}^{r} \\tilde{\\eta}_i^2 = \\delta^2$.\n\nWe have the singular values ordered as $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{r} > 0$. Consequently, their squared reciprocals are ordered as:\n$$\n0 < \\frac{1}{\\sigma_1^2} \\leq \\frac{1}{\\sigma_2^2} \\leq \\cdots \\leq \\frac{1}{\\sigma_r^2}\n$$\nTo maximize the weighted sum $\\sum_{i=1}^{r} (1/\\sigma_i^2) \\tilde{\\eta}_i^2$ subject to $\\sum_{i=1}^{r} \\tilde{\\eta}_i^2 = \\delta^2$, we must allocate the entire \"energy\" $\\delta^2$ of $\\tilde{\\eta}$ to the component $\\tilde{\\eta}_i$ that is multiplied by the largest coefficient. The largest coefficient is $1/\\sigma_r^2$.\n\nTherefore, the maximum is achieved by choosing $\\tilde{\\eta}_r^2 = \\delta^2$ and $\\tilde{\\eta}_i^2 = 0$ for all $i \\neq r$. This configuration satisfies the constraint. For this choice, the squared error norm is:\n$$\n\\|e\\|_{2, \\text{max}}^{2} = \\frac{1}{\\sigma_r^2} \\tilde{\\eta}_r^2 = \\frac{\\delta^2}{\\sigma_r^2}\n$$\nTaking the square root, we find the maximum possible value of the error norm:\n$$\n\\|e\\|_{2, \\text{max}} = \\frac{\\delta}{\\sigma_r}\n$$\n\nTo realize this maximum, we chose $\\tilde{\\eta}$ such that $\\tilde{\\eta}_r = \\pm\\delta$ and all other components are zero. Let's denote the canonical basis vector in $\\mathbb{R}^m$ with a $1$ in the $i$-th position as $e'_i$. Then, the maximizing transformed noise vector is $\\tilde{\\eta}_{\\text{max}} = \\delta e'_r$.\nThe noise in the original data space, $\\eta \\in \\mathbb{R}^m$, is found by transforming back:\n$$\n\\eta_{\\text{max}} = U \\tilde{\\eta}_{\\text{max}} = U (\\delta e'_r) = \\delta (U e'_r) = \\delta u_r\n$$\nwhere $u_r$ is the $r$-th column of $U$, which is the left singular vector of $A$ corresponding to the singular value $\\sigma_r$. Thus, the worst-case noise is aligned with the left singular vector corresponding to the smallest positive singular value $\\sigma_r$.\n\nThe resulting reconstruction error vector $e \\in \\mathbb{R}^n$ is:\n$$\ne_{\\text{max}} = A^{\\dagger} \\eta_{\\text{max}} = V \\Sigma^{\\dagger} U^{\\mathsf{T}} (\\delta u_r)\n$$\nSince the columns of $U$ are orthonormal, $U^{\\mathsf{T}} u_r = e'_r$. The expression becomes:\n$$\ne_{\\text{max}} = V \\Sigma^{\\dagger} (\\delta e'_r) = \\delta (V (\\Sigma^{\\dagger} e'_r))\n$$\nThe vector $\\Sigma^{\\dagger} e'_r$ is a vector in $\\mathbb{R}^n$ with its $r$-th component equal to $1/\\sigma_r$ and all other components zero. Let's call this vector $(1/\\sigma_r) e''_r$, where $e''_r$ is the $r$-th canonical basis vector in $\\mathbb{R}^n$.\n$$\ne_{\\text{max}} = \\delta V \\left(\\frac{1}{\\sigma_r} e''_r\\right) = \\frac{\\delta}{\\sigma_r} (V e''_r) = \\frac{\\delta}{\\sigma_r} v_r\n$$\nwhere $v_r$ is the $r$-th column of $V$, i.e., the right singular vector of $A$ corresponding to $\\sigma_r$.\nThis shows that the worst-case reconstruction error $e_{\\text{max}}$ is aligned with the right singular vector associated with the smallest positive singular value $\\sigma_r$.\n\nIn the context of data assimilation, this result illustrates the mechanism of error amplification in ill-conditioned inverse problems. The right singular vector $v_r$ represents a direction in the state space that is \"least observable,\" because its mapping into the observation space, $A v_r = \\sigma_r u_r$, is scaled by the small singular value $\\sigma_r$. The worst-case observation error $\\eta_{\\text{max}} = \\delta u_r$ has a spatial structure that perfectly mimics the faint observational signature of this poorly-observed state component. The reconstruction process, which involves the inverse operator $A^{\\dagger}$, misinterprets this noise as a very large signal component in the direction of $v_r$. The noise with magnitude $\\delta$ is amplified by the factor $1/\\sigma_r$, leading to a large reconstruction error of magnitude $(\\delta/\\sigma_r)$ aligned with the state space direction $v_r$. This amplification factor $1/\\sigma_r$ is precisely the operator norm $\\|A^{\\dagger}\\|_2$, which quantifies the sensitivity of the solution to perturbations in the data.", "answer": "$$\n\\boxed{\\frac{\\delta}{\\sigma_{r}}}\n$$", "id": "3404452"}, {"introduction": "Having seen the danger of uncontrolled noise amplification, our next step is to implement a practical remedy. This exercise guides you through coding the Truncated Singular Value Decomposition (TSVD), a foundational regularization method. You will apply the L-curve criterion—a powerful heuristic for choosing the regularization parameter $k$—to balance the trade-off between fitting the data and controlling the norm of the solution, thereby ensuring a stable and meaningful result [@problem_id:3404450].", "problem": "You are given the singular value decomposition context where a real matrix $A \\in \\mathbb{R}^{m \\times n}$ has a singular value decomposition $A = U \\Sigma V^\\top$, with orthonormal columns $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$, and a diagonal matrix of singular values $\\Sigma \\in \\mathbb{R}^{m \\times n}$ whose diagonal entries are nonnegative and nonincreasing. In many inverse problems for data assimilation, computing the Moore-Penrose pseudoinverse solution $x = A^+ b$ can be unstable when $A$ is ill-conditioned due to very small singular values. A common stabilization is the truncated singular value decomposition (TSVD), where only the largest $k$ singular values are used to compute the regularized approximate solution $x_k$.\n\nFundamental base:\n- Singular value decomposition: $A = U \\Sigma V^\\top$ with singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n \\ge 0$, left singular vectors $u_i$ (columns of $U$), and right singular vectors $v_i$ (columns of $V$).\n- For any vector $b \\in \\mathbb{R}^m$, the data coefficients with respect to the left singular vectors are $c_i = u_i^\\top b$.\n- The truncated singular value decomposition (TSVD) solution using the first $k$ singular components is defined in the right-singular-vector basis by coefficients $a_i^{(k)} = c_i / \\sigma_i$ for $i \\le k$ and $a_i^{(k)} = 0$ for $i > k$, so that $x_k = \\sum_{i=1}^k (c_i / \\sigma_i) v_i$.\n\nFrom these definitions, for $k \\in \\{1,2,\\dots,n\\}$, the residual norm and solution norm are\n- Residual norm: $r_k = \\lVert A x_k - b \\rVert_2 = \\sqrt{\\sum_{i=k+1}^n c_i^2}$, under the assumption that $b$ lies in the span of the first $n$ left singular vectors.\n- Solution norm: $s_k = \\lVert x_k \\rVert_2 = \\sqrt{\\sum_{i=1}^k \\left(\\frac{c_i}{\\sigma_i}\\right)^2}$.\n\nThe L-curve criterion selects $k$ that corresponds to the corner of the parametric curve $(\\log r_k, \\log s_k)$ as $k$ varies, trading off fitting the data (small residual) and controlling solution norm (stability). To operationalize this, define a discrete curvature at interior indices using the Menger curvature of three consecutive points. For three points $p_{j-1}, p_j, p_{j+1} \\in \\mathbb{R}^2$, the Menger curvature is\n$$\n\\kappa_j = \\frac{4 \\Delta(p_{j-1}, p_j, p_{j+1})}{\\lVert p_{j-1} - p_j \\rVert_2 \\, \\lVert p_j - p_{j+1} \\rVert_2 \\, \\lVert p_{j+1} - p_{j-1} \\rVert_2},\n$$\nwhere the triangle area $\\Delta$ is\n$$\n\\Delta(p_{j-1}, p_j, p_{j+1}) = \\frac{1}{2} \\left| (x_j - x_{j-1})(y_{j+1} - y_{j-1}) - (x_{j+1} - x_{j-1})(y_j - y_{j-1}) \\right|\n$$\nwith $p_\\ell = (x_\\ell, y_\\ell)$. The selected $k$ is the index $j$ that maximizes $\\kappa_j$ over all $j$ where it is defined, with ties broken by choosing the smallest such $j$. If no curvature value can be computed (for example, due to degenerate points), select the smallest $k$ that minimizes $r_k$.\n\nImplement a program that, for each test case, computes $x_k$ for all $k \\in \\{1,\\dots,n\\}$ in the right-singular-vector coordinate basis (that is, the coefficients $a_i^{(k)}$), evaluates $r_k$ and $s_k$, constructs the L-curve points $(\\log r_k, \\log s_k)$, computes the discrete Menger curvature $\\kappa_k$ for valid interior indices, and selects $k$ according to the criterion above.\n\nTest suite:\n- Case A (moderately ill-conditioned):\n  - Singular values: $\\sigma = (1.0, 0.6065306597126334, 0.36787944117144233, 0.22313016014842982, 0.1353352832366127, 0.0820849986238988, 0.049787068367863944, 0.0301973834223185, 0.01831563888873418, 0.011108996538242306)$.\n  - Data coefficients: $c = (1.1, 0.95, 0.85, 0.75, 0.65, 0.58, 0.52, 0.47, 0.43, 0.4)$.\n- Case B (highly ill-conditioned):\n  - Singular values: $\\sigma = (1.0, 0.36787944117144233, 0.1353352832366127, 0.049787068367863944, 0.01831563888873418, 0.006737946999085467, 0.0024787521766663585, 0.0009118819655545162, 0.00033546262790251185, 0.00012340980408667956)$.\n  - Data coefficients: $c = (0.7, 0.68, 0.67, 0.65, 0.64, 0.63, 0.62, 0.62, 0.62, 0.62)$.\n- Case C (plateau then sharp drop, with tail energy):\n  - Singular values: $\\sigma = (1.2, 1.1, 1.0, 0.95, 0.9, 0.85, 0.01, 0.005, 0.0025, 0.00125)$.\n  - Data coefficients: $c = (1.5, 1.0, 0.6, 0.3, 0.15, 0.08, 0.2, 0.2, 0.2, 0.2)$.\n\nAngle units are not applicable. There are no physical units involved. All numerical results must be real numbers. The final output for each test case is the selected integer $k$.\n\nFinal output format:\nYour program should produce a single line of output containing the selected $k$ values for the three test cases, as a comma-separated list enclosed in square brackets, for example, $[k_A,k_B,k_C]$.", "solution": "The problem requires the implementation of the L-curve criterion using Menger curvature to select the optimal regularization parameter $k$ for a Truncated Singular Value Decomposition (TSVD) solution. The solution methodology proceeds through a sequence of rigorous, well-defined steps.\n\nFirst, we must correctly interpret the problem statement to devise an algorithm. The problem defines the residual norm $r_k = \\lVert A x_k - b \\rVert_2 = \\sqrt{\\sum_{i=k+1}^n c_i^2}$ and the solution norm $s_k = \\lVert x_k \\rVert_2 = \\sqrt{\\sum_{i=1}^k (c_i/\\sigma_i)^2}$ for $k \\in \\{1, 2, \\dots, n\\}$. The L-curve is a parametric plot of $(\\log r_k, \\log s_k)$. A critical observation is that for $k=n$, the residual norm $r_n = \\sqrt{\\sum_{i=n+1}^n c_i^2} = 0$. Consequently, $\\log r_n$ is undefined ($-\\infty$), which means the point $p_n = (\\log r_n, \\log s_n)$ is not a finite point in the Cartesian plane $\\mathbb{R}^2$.\n\nThe selection of $k$ is based on the Menger curvature $\\kappa_j$ of three consecutive points $p_{j-1}, p_j, p_{j+1}$, maximized over \"valid interior indices\". A valid index $j$ requires that all points used to compute $\\kappa_j$ are themselves valid, i.e., finite. Since $p_n$ is not a finite point, any curvature calculation that depends on it (namely, $\\kappa_{n-1}$, which uses $p_{n-2}, p_{n-1}, p_n$) is ill-defined in this discrete context. Therefore, the set of valid, finite L-curve points is $\\{p_1, p_2, \\dots, p_{n-1}\\}$. For this sequence of $n-1$ points, the interior points are $p_2, \\dots, p_{n-2}$. The Menger curvature $\\kappa_j$ can thus only be calculated for the indices $j \\in \\{2, 3, \\dots, n-2\\}$. The optimal parameter $k_{opt}$ is found by maximizing the curvature over this range:\n$$\nk_{opt} = \\arg\\max_{k \\in \\{2, \\dots, n-2\\}} \\kappa_k\n$$\nIn case of a tie for the maximum curvature, the problem specifies that the smallest index $k$ should be selected. The fallback rule, to be used if no curvature can be computed (e.g., if $n \\le 3$), is to choose the smallest $k$ that minimizes $r_k$; since $r_k$ is a non-increasing function of $k$, its minimum is at $k=n$.\n\nThe algorithmic procedure is as follows:\n1.  For each test case, given the vectors of singular values $\\sigma = (\\sigma_1, \\dots, \\sigma_n)$ and data coefficients $c = (c_1, \\dots, c_n)$, we first compute the squared norms for $k \\in \\{1, \\dots, n\\}$.\n    -   Solution norm squared: $s_k^2 = \\sum_{i=1}^k \\left(\\frac{c_i}{\\sigma_i}\\right)^2$. This can be computed efficiently for all $k$ using a cumulative sum.\n    -   Residual norm squared: $r_k^2 = \\sum_{i=k+1}^n c_i^2$. This is computed for $k=1, \\dots, n-1$; $r_n^2=0$.\n\n2.  Next, we construct the set of $n-1$ finite L-curve points, $\\{p_k\\}_{k=1}^{n-1}$, where $p_k = (\\rho_k, \\eta_k) = (\\log r_k, \\log s_k)$. These coordinates are calculated as $\\rho_k = \\frac{1}{2}\\log(r_k^2)$ and $\\eta_k = \\frac{1}{2}\\log(s_k^2)$.\n\n3.  With the sequence of points $p_1, \\dots, p_{n-1}$ established, we compute the Menger curvature $\\kappa_k$ for each interior index $k \\in \\{2, \\dots, n-2\\}$. The curvature at point $p_k$ is a function of $p_{k-1}, p_k$, and $p_{k+1}$:\n    $$\n    \\kappa_k = \\frac{4 \\Delta(p_{k-1}, p_k, p_{k+1})}{\\lVert p_{k-1} - p_k \\rVert_2 \\, \\lVert p_k - p_{k+1} \\rVert_2 \\, \\lVert p_{k+1} - p_{k-1} \\rVert_2}\n    $$\n    The area of the triangle formed by these three points is given by:\n    $$\n    \\Delta(p_{k-1}, p_k, p_{k+1}) = \\frac{1}{2} \\left| (\\rho_k - \\rho_{k-1})(\\eta_{k+1} - \\eta_{k-1}) - (\\rho_{k+1} - \\rho_{k-1})(\\eta_k - \\eta_{k-1}) \\right|\n    $$\n    A small threshold is used to handle the denominator becoming zero in the case of collinear points, where the curvature is correctly defined as $0$.\n\n4.  Finally, we iterate through the computed curvatures $\\{\\kappa_k\\}_{k=2}^{n-2}$ to find the maximum value. The index $k$ corresponding to the first occurrence of this maximum value is selected as the optimal regularization parameter for the given test case. This finds the \"corner\" of the L-curve as intended by the criterion. This procedure is applied independently to each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite. It computes the\n    optimal TSVD truncation parameter k for each case and prints the results.\n    \"\"\"\n    \n    # The test cases are provided as tuples of (sigma, c) lists.\n    test_cases = [\n        (\n            # Case A: Moderately ill-conditioned\n            [1.0, 0.6065306597126334, 0.36787944117144233, 0.22313016014842982, 0.1353352832366127, 0.0820849986238988, 0.049787068367863944, 0.0301973834223185, 0.01831563888873418, 0.011108996538242306],\n            [1.1, 0.95, 0.85, 0.75, 0.65, 0.58, 0.52, 0.47, 0.43, 0.4]\n        ),\n        (\n            # Case B: Highly ill-conditioned\n            [1.0, 0.36787944117144233, 0.1353352832366127, 0.049787068367863944, 0.01831563888873418, 0.006737946999085467, 0.0024787521766663585, 0.0009118819655545162, 0.00033546262790251185, 0.00012340980408667956],\n            [0.7, 0.68, 0.67, 0.65, 0.64, 0.63, 0.62, 0.62, 0.62, 0.62]\n        ),\n        (\n            # Case C: Plateau then sharp drop\n            [1.2, 1.1, 1.0, 0.95, 0.9, 0.85, 0.01, 0.005, 0.0025, 0.00125],\n            [1.5, 1.0, 0.6, 0.3, 0.15, 0.08, 0.2, 0.2, 0.2, 0.2]\n        )\n    ]\n\n    results = []\n    for sigma, c in test_cases:\n        k_opt = find_optimal_k(sigma, c)\n        results.append(k_opt)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef menger_curvature(p1: np.ndarray, p2: np.ndarray, p3: np.ndarray) -> float:\n    \"\"\"Computes the Menger curvature for three points in a 2D plane.\"\"\"\n    x1, y1 = p1\n    x2, y2 = p2\n    x3, y3 = p3\n\n    # Triangle area formula from the problem statement\n    area = 0.5 * np.abs((x2 - x1) * (y3 - y1) - (x3 - x1) * (y2 - y1))\n\n    # Calculate side lengths\n    d12 = np.linalg.norm(p1 - p2)\n    d23 = np.linalg.norm(p2 - p3)\n    d31 = np.linalg.norm(p3 - p1)\n\n    denominator = d12 * d23 * d31\n    if denominator < 1e-30:  # Avoid division by zero for (near) collinear points\n        return 0.0\n\n    return 4.0 * area / denominator\n\ndef find_optimal_k(sigma, c) -> int:\n    \"\"\"\n    Finds the optimal truncation parameter k using the L-curve Menger curvature criterion.\n    sigma: list of singular values.\n    c: list of data coefficients.\n    \"\"\"\n    sigma_arr = np.array(sigma)\n    c_arr = np.array(c)\n    n = len(sigma_arr)\n\n    # 1. Calculate squared residual norm (r_k^2) and solution norm (s_k^2) for k=1..n\n    r_sq = np.zeros(n)\n    \n    c_sq = c_arr**2\n    # s_k^2 = sum_{i=1 to k} (c_i/sigma_i)^2\n    s_sq = np.cumsum((c_arr / sigma_arr)**2)\n\n    # r_k^2 = sum_{i=k+1 to n} c_i^2\n    # In 0-based index k_idx = k-1: r_sq[k_idx] = sum(c_sq[k_idx+1:])\n    for k_idx in range(n - 1):\n        r_sq[k_idx] = np.sum(c_sq[k_idx + 1:])\n    r_sq[n - 1] = 0.0\n            \n    # 2. Construct L-curve points (log r_k, log s_k)\n    # The point p_n for k=n is not finite because r_n=0 -> log(r_n)=-inf.\n    # We form a set of finite points for k=1,...,n-1.\n    num_finite_points = n - 1\n    \n    # rho_k = log(r_k), eta_k = log(s_k)\n    # Using log of squares to avoid sqrt, then halving: log(x) = 0.5*log(x^2)\n    rho = 0.5 * np.log(r_sq[:num_finite_points])\n    eta = 0.5 * np.log(s_sq[:num_finite_points])\n    \n    # points[i] corresponds to k=i+1\n    points = np.vstack((rho, eta)).T\n    \n    # 3. Compute discrete Menger curvature kappa_k\n    # Curvature is defined for interior indices. For n-1 points, these are k=2, ..., n-2.\n    k_min_curv = 2\n    k_max_curv = n - 2\n    \n    if k_min_curv > k_max_curv:\n        # Not enough points to compute curvature (e.g., n<=3).\n        # Fallback rule: select smallest k that minimizes r_k.\n        # r_k is non-increasing, minimum is r_n = 0 at k=n. Smallest such k is n.\n        min_r_val = np.min(np.sqrt(r_sq))\n        k_opt = np.where(np.sqrt(r_sq) == min_r_val)[0][0] + 1\n        return int(k_opt)\n\n    curvatures = {}\n    for k in range(k_min_curv, k_max_curv + 1):\n        # We need points p_{k-1}, p_k, p_{k+1}.\n        # In 0-based `points` array, these are indices k-2, k-1, k.\n        p_prev = points[k - 2]\n        p_curr = points[k - 1]\n        p_next = points[k]\n        \n        curvatures[k] = menger_curvature(p_prev, p_curr, p_next)\n\n    if not curvatures:\n        # Fallback rule, same logic as above.\n        min_r_val = np.min(np.sqrt(r_sq))\n        k_opt = np.where(np.sqrt(r_sq) == min_r_val)[0][0] + 1\n        return int(k_opt)\n\n    # 4. Select k that maximizes curvature, with ties broken by smallest k.\n    max_curvature = -1.0\n    k_opt = -1\n    \n    # Iterate through sorted keys to ensure tie-breaking rule (smallest k) is met.\n    for k in sorted(curvatures.keys()):\n        if curvatures[k] > max_curvature:\n            max_curvature = curvatures[k]\n            k_opt = k\n            \n    return k_opt\n\n# Execute the main function to produce the final output.\nsolve()\n```", "id": "3404450"}, {"introduction": "A mathematically optimal solution is not always a physically meaningful one. This final practice confronts a common challenge where the unconstrained minimum-norm solution $x^\\dagger$ violates fundamental physical bounds, such as non-negativity or conservation laws. You will first compute such an unphysical solution and then apply a crucial corrective step: projecting the result onto a physically feasible set, a technique essential for producing realistic state estimates in many scientific applications [@problem_id:3404374].", "problem": "Consider a linear data assimilation model with state vector $x \\in \\mathbb{R}^{3}$ representing mixing fractions of three chemical species. Physical feasibility requires the bounds $x_{i} \\ge 0$ for all $i$ and conservation $\\sum_{i=1}^{3} x_{i} = 1$. Let the forward operator be the matrix\n$$\nA(\\varepsilon) \\;=\\; \\begin{pmatrix}\n1 & 1 & 1 \\\\\n0 & \\varepsilon & 2\\varepsilon\n\\end{pmatrix},\n$$\nwith a small parameter $\\varepsilon > 0$ (so the second row is much weaker than the first), and suppose we assimilate the observation vector\n$$\ny \\;=\\; \\begin{pmatrix} a \\\\ b \\end{pmatrix},\n$$\nwhere $a = \\frac{102}{100}$ and $b = \\frac{1}{4000}$, and $\\varepsilon = \\frac{1}{1000}$. Start from the defining characterization of the Moore–Penrose pseudoinverse as the unique minimum-Euclidean-norm solution,\n$$\nx^{\\dagger} \\;=\\; \\underset{x \\in \\mathbb{R}^{3}}{\\arg\\min}\\;\\|x\\|_{2} \\quad \\text{subject to} \\quad A(\\varepsilon)\\,x = y.\n$$\nUsing only this definition and first principles, derive $x^{\\dagger}$ exactly and identify which physical constraints it violates, if any. As a remedy, enforce feasibility by projecting $x^{\\dagger}$ onto the probability simplex $\\mathcal{S} = \\{x \\in \\mathbb{R}^{3} : x_{i} \\ge 0,\\; \\sum_{i=1}^{3} x_{i} = 1\\}$ via the Euclidean projection\n$$\nx^{\\mathrm{proj}} \\;=\\; \\underset{x \\in \\mathcal{S}}{\\arg\\min}\\;\\|x - x^{\\dagger}\\|_{2}.\n$$\nCompute, in exact arithmetic, the squared Euclidean correction magnitude\n$$\n\\Delta^{2} \\;=\\; \\bigl\\|x^{\\dagger} - x^{\\mathrm{proj}}\\bigr\\|_{2}^{2}.\n$$\nProvide your final answer as a single exact number. No units are required. If you choose to approximate, round your answer to four significant figures.", "solution": "The problem will be validated against the specified criteria before a solution is attempted.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- State vector: $x \\in \\mathbb{R}^{3}$.\n- Physical constraints on the state vector: $x_{i} \\ge 0$ for all $i \\in \\{1, 2, 3\\}$, and $\\sum_{i=1}^{3} x_{i} = 1$.\n- Forward operator: $A(\\varepsilon) = \\begin{pmatrix} 1 & 1 & 1 \\\\ 0 & \\varepsilon & 2\\varepsilon \\end{pmatrix}$.\n- Small parameter: $\\varepsilon > 0$, with specific value $\\varepsilon = \\frac{1}{1000}$.\n- Observation vector: $y = \\begin{pmatrix} a \\\\ b \\end{pmatrix}$, with specific values $a = \\frac{102}{100}$ and $b = \\frac{1}{4000}$.\n- Definition of the minimum-norm solution: $x^{\\dagger} = \\underset{x \\in \\mathbb{R}^{3}}{\\arg\\min}\\;\\|x\\|_{2} \\quad \\text{subject to} \\quad A(\\varepsilon)\\,x = y$.\n- Definition of the projected feasible solution: $x^{\\mathrm{proj}} = \\underset{x \\in \\mathcal{S}}{\\arg\\min}\\;\\|x - x^{\\dagger}\\|_{2}$, where the feasible set is the probability simplex $\\mathcal{S} = \\{x \\in \\mathbb{R}^{3} : x_{i} \\ge 0,\\; \\sum_{i=1}^{3} x_{i} = 1\\}$.\n- Quantity to be computed: The squared Euclidean correction magnitude, $\\Delta^{2} = \\bigl\\|x^{\\dagger} - x^{\\mathrm{proj}}\\bigr\\|_{2}^{2}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is a standard exercise in linear inverse theory and data assimilation. It deals with finding a state vector from observations using a linear forward model, identifying the unphysical nature of the unconstrained solution, and projecting it onto a physically meaningful set. These are core concepts in the field.\n- **Well-Posed**: The problem is mathematically well-posed. The forward operator $A(\\varepsilon)$ for $\\varepsilon > 0$ is a $2 \\times 3$ matrix with rank $2$ (full row rank), which guarantees the existence and uniqueness of the minimum-Euclidean-norm solution $x^{\\dagger}$. The probability simplex $\\mathcal{S}$ is a closed and convex set, which guarantees the existence and uniqueness of the Euclidean projection $x^{\\mathrm{proj}}$. All necessary parameters are specified.\n- **Objective**: The problem is stated using precise mathematical language without any ambiguity or subjective claims.\n\n**Step 3: Verdict and Action**\nThe problem is valid. It is scientifically sound, well-posed, objective, and complete. A solution will be derived.\n\n### Derivation of the Solution\n\nThe first step is to compute the minimum-Euclidean-norm solution $x^{\\dagger}$ that satisfies the linear system $A(\\varepsilon)x = y$. This is a constrained optimization problem: minimize $f(x) = \\|x\\|_{2}^{2} = x^{T}x$ subject to the constraint $A(\\varepsilon)x - y = 0$. We use the method of Lagrange multipliers. The Lagrangian is:\n$$\n\\mathcal{L}(x, \\lambda) = x^{T}x + \\lambda^{T}(A(\\varepsilon)x - y)\n$$\nwhere $\\lambda \\in \\mathbb{R}^{2}$ is the vector of Lagrange multipliers. To find the minimum, we set the gradient with respect to $x$ to zero:\n$$\n\\nabla_{x} \\mathcal{L} = 2x + A(\\varepsilon)^{T}\\lambda = 0 \\quad \\implies \\quad x = -\\frac{1}{2}A(\\varepsilon)^{T}\\lambda\n$$\nSubstituting this into the constraint equation $A(\\varepsilon)x = y$:\n$$\nA(\\varepsilon) \\left(-\\frac{1}{2}A(\\varepsilon)^{T}\\lambda\\right) = y \\quad \\implies \\quad -\\frac{1}{2}A(\\varepsilon)A(\\varepsilon)^{T}\\lambda = y\n$$\nSince $A(\\varepsilon)$ has full row rank for $\\varepsilon > 0$, the matrix $A(\\varepsilon)A(\\varepsilon)^{T}$ is invertible. We can solve for $\\lambda$:\n$$\n\\lambda = -2 \\left(A(\\varepsilon)A(\\varepsilon)^{T}\\right)^{-1}y\n$$\nSubstituting $\\lambda$ back into the expression for $x$ gives the solution $x^{\\dagger}$:\n$$\nx^{\\dagger} = -\\frac{1}{2}A(\\varepsilon)^{T} \\left(-2 \\left(A(\\varepsilon)A(\\varepsilon)^{T}\\right)^{-1}y\\right) = A(\\varepsilon)^{T}\\left(A(\\varepsilon)A(\\varepsilon)^{T}\\right)^{-1}y\n$$\nThis is the standard formula for the Moore-Penrose pseudoinverse $A(\\varepsilon)^{+}$ applied to $y$. Let's compute the matrices:\n$$\nA(\\varepsilon)A(\\varepsilon)^{T} = \\begin{pmatrix} 1 & 1 & 1 \\\\ 0 & \\varepsilon & 2\\varepsilon \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 1 & \\varepsilon \\\\ 1 & 2\\varepsilon \\end{pmatrix} = \\begin{pmatrix} 3 & 3\\varepsilon \\\\ 3\\varepsilon & 5\\varepsilon^{2} \\end{pmatrix}\n$$\nThe determinant is $\\det(A(\\varepsilon)A(\\varepsilon)^{T}) = (3)(5\\varepsilon^{2}) - (3\\varepsilon)(3\\varepsilon) = 15\\varepsilon^{2} - 9\\varepsilon^{2} = 6\\varepsilon^{2}$. The inverse is:\n$$\n\\left(A(\\varepsilon)A(\\varepsilon)^{T}\\right)^{-1} = \\frac{1}{6\\varepsilon^{2}} \\begin{pmatrix} 5\\varepsilon^{2} & -3\\varepsilon \\\\ -3\\varepsilon & 3 \\end{pmatrix}\n$$\nNow we compute $x^{\\dagger}$:\n$$\nx^{\\dagger} = \\begin{pmatrix} 1 & 0 \\\\ 1 & \\varepsilon \\\\ 1 & 2\\varepsilon \\end{pmatrix} \\frac{1}{6\\varepsilon^{2}} \\begin{pmatrix} 5\\varepsilon^{2} & -3\\varepsilon \\\\ -3\\varepsilon & 3 \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix}\n$$\n$$\nx^{\\dagger} = \\frac{1}{6\\varepsilon^{2}} \\begin{pmatrix} 1 & 0 \\\\ 1 & \\varepsilon \\\\ 1 & 2\\varepsilon \\end{pmatrix} \\begin{pmatrix} 5\\varepsilon^{2}a - 3\\varepsilon b \\\\ -3\\varepsilon a + 3b \\end{pmatrix}\n$$\n$$\nx^{\\dagger} = \\frac{1}{6\\varepsilon^{2}} \\begin{pmatrix} 5\\varepsilon^{2}a - 3\\varepsilon b \\\\ (5\\varepsilon^{2}a - 3\\varepsilon b) + \\varepsilon(-3\\varepsilon a + 3b) \\\\ (5\\varepsilon^{2}a - 3\\varepsilon b) + 2\\varepsilon(-3\\varepsilon a + 3b) \\end{pmatrix} = \\frac{1}{6\\varepsilon^{2}} \\begin{pmatrix} 5\\varepsilon^{2}a - 3\\varepsilon b \\\\ 2\\varepsilon^{2}a \\\\ -\\varepsilon^{2}a + 3\\varepsilon b \\end{pmatrix}\n$$\nSimplifying the components of $x^{\\dagger} = (x_{1}^{\\dagger}, x_{2}^{\\dagger}, x_{3}^{\\dagger})^{T}$:\n$$\nx_{1}^{\\dagger} = \\frac{5}{6}a - \\frac{1}{2\\varepsilon}b, \\quad x_{2}^{\\dagger} = \\frac{1}{3}a, \\quad x_{3}^{\\dagger} = -\\frac{1}{6}a + \\frac{1}{2\\varepsilon}b\n$$\nWe substitute the given values $a = \\frac{102}{100}$, $b = \\frac{1}{4000}$, and $\\varepsilon = \\frac{1}{1000}$.\nNote that $\\frac{1}{2\\varepsilon} = \\frac{1}{2(1/1000)} = 500$.\n$$\nx_{1}^{\\dagger} = \\frac{5}{6}\\left(\\frac{102}{100}\\right) - 500\\left(\\frac{1}{4000}\\right) = \\frac{17}{20} - \\frac{1}{8} = \\frac{34}{40} - \\frac{5}{40} = \\frac{29}{40}\n$$\n$$\nx_{2}^{\\dagger} = \\frac{1}{3}\\left(\\frac{102}{100}\\right) = \\frac{34}{100} = \\frac{17}{50}\n$$\n$$\nx_{3}^{\\dagger} = -\\frac{1}{6}\\left(\\frac{102}{100}\\right) + 500\\left(\\frac{1}{4000}\\right) = -\\frac{17}{100} + \\frac{1}{8} = -\\frac{34}{200} + \\frac{25}{200} = -\\frac{9}{200}\n$$\nSo, the unconstrained solution is $x^{\\dagger} = \\left(\\frac{29}{40}, \\frac{17}{50}, -\\frac{9}{200}\\right)^{T}$.\n\nNext, we check this solution against the physical constraints.\n1. Non-negativity ($x_{i} \\ge 0$): $x_{1}^{\\dagger} = \\frac{29}{40} > 0$, $x_{2}^{\\dagger} = \\frac{17}{50} > 0$, but $x_{3}^{\\dagger} = -\\frac{9}{200} < 0$. The non-negativity constraint is violated for the third component.\n2. Conservation ($\\sum x_i = 1$): The first row of the constraint $A(\\varepsilon)x=y$ is $\\sum x_{i} = a$. With $a = \\frac{102}{100} = 1.02$, we have $\\sum x_i^{\\dagger} = 1.02 \\neq 1$. The conservation constraint is violated.\n\nTo enforce feasibility, we project $x^{\\dagger}$ onto the probability simplex $\\mathcal{S}$. We need to compute $x^{\\mathrm{proj}} = \\underset{x \\in \\mathcal{S}}{\\arg\\min}\\;\\|x - x^{\\dagger}\\|_{2}$. The standard algorithm for this projection involves sorting the components of $x^{\\dagger}$ and finding a threshold $\\tau$.\nThe components of $x^{\\dagger}$ are $\\frac{29}{40} = 0.725$, $\\frac{17}{50} = 0.34$, and $-\\frac{9}{200} = -0.045$.\nLet $z = x^{\\dagger}$. The components in descending order are $z_{(1)} = \\frac{29}{40}$, $z_{(2)} = \\frac{17}{50}$, $z_{(3)} = -\\frac{9}{200}$.\nWe find $\\rho = \\max\\left\\{ j \\in \\{1, 2, 3\\} \\, \\Big| \\, z_{(j)} - \\frac{1}{j}\\left(\\sum_{k=1}^{j} z_{(k)} - 1\\right) > 0 \\right\\}$.\nFor $j=1$: $z_{(1)} - (z_{(1)} - 1) = 1 > 0$.\nFor $j=2$: Let's calculate the term.\n$\\sum_{k=1}^{2} z_{(k)} = \\frac{29}{40} + \\frac{17}{50} = \\frac{145}{200} + \\frac{68}{200} = \\frac{213}{200}$.\n$z_{(2)} - \\frac{1}{2}\\left(\\frac{213}{200} - 1\\right) = \\frac{17}{50} - \\frac{1}{2}\\left(\\frac{13}{200}\\right) = \\frac{136}{400} - \\frac{13}{400} = \\frac{123}{400} > 0$.\nFor $j=3$:\n$\\sum_{k=1}^{3} z_{(k)} = \\sum_{i=1}^{3} x_{i}^{\\dagger} = a = \\frac{102}{100} = \\frac{204}{200}$.\n$z_{(3)} - \\frac{1}{3}\\left(\\frac{204}{200} - 1\\right) = -\\frac{9}{200} - \\frac{1}{3}\\left(\\frac{4}{200}\\right) = -\\frac{27}{600} - \\frac{4}{600} = -\\frac{31}{600} < 0$.\nThe maximum such $j$ is $\\rho = 2$.\nThe threshold $\\tau$ is given by $\\tau = \\frac{1}{\\rho}\\left(\\sum_{k=1}^{\\rho} z_{(k)} - 1\\right)$.\n$\\tau = \\frac{1}{2}\\left(\\frac{213}{200} - 1\\right) = \\frac{13}{400}$.\nThe projected solution $x^{\\mathrm{proj}}$ has components $x_{i}^{\\mathrm{proj}} = \\max(x_{i}^{\\dagger} - \\tau, 0)$.\n$x_{1}^{\\mathrm{proj}} = \\max\\left(\\frac{29}{40} - \\frac{13}{400}, 0\\right) = \\max\\left(\\frac{290-13}{400}, 0\\right) = \\frac{277}{400}$.\n$x_{2}^{\\mathrm{proj}} = \\max\\left(\\frac{17}{50} - \\frac{13}{400}, 0\\right) = \\max\\left(\\frac{136-13}{400}, 0\\right) = \\frac{123}{400}$.\n$x_{3}^{\\mathrm{proj}} = \\max\\left(-\\frac{9}{200} - \\frac{13}{400}, 0\\right) = \\max\\left(\\frac{-18-13}{400}, 0\\right) = \\max\\left(-\\frac{31}{400}, 0\\right) = 0$.\nThe projected solution is $x^{\\mathrm{proj}} = \\left(\\frac{277}{400}, \\frac{123}{400}, 0\\right)^{T}$.\n\nFinally, we compute the squared Euclidean correction magnitude $\\Delta^{2} = \\|x^{\\dagger} - x^{\\mathrm{proj}}\\|_{2}^{2}$.\nThe difference vector is $x^{\\dagger} - x^{\\mathrm{proj}}$:\n$$\nx^{\\dagger} - x^{\\mathrm{proj}} = \\begin{pmatrix} 29/40 \\\\ 17/50 \\\\ -9/200 \\end{pmatrix} - \\begin{pmatrix} 277/400 \\\\ 123/400 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 290/400 - 277/400 \\\\ 136/400 - 123/400 \\\\ -18/400 - 0 \\end{pmatrix} = \\begin{pmatrix} 13/400 \\\\ 13/400 \\\\ -18/400 \\end{pmatrix}\n$$\nThe squared norm of this vector is:\n$$\n\\Delta^{2} = \\left(\\frac{13}{400}\\right)^{2} + \\left(\\frac{13}{400}\\right)^{2} + \\left(-\\frac{18}{400}\\right)^{2}\n$$\n$$\n\\Delta^{2} = \\frac{13^{2} + 13^{2} + 18^{2}}{400^{2}} = \\frac{169 + 169 + 324}{160000} = \\frac{338 + 324}{160000} = \\frac{662}{160000}\n$$\nSimplifying the fraction by dividing the numerator and denominator by $2$:\n$$\n\\Delta^{2} = \\frac{331}{80000}\n$$\nThe numerator $331$ is a prime number, so this fraction is in its simplest form. This is the exact value for the squared correction magnitude.", "answer": "$$\\boxed{\\frac{331}{80000}}$$", "id": "3404374"}]}