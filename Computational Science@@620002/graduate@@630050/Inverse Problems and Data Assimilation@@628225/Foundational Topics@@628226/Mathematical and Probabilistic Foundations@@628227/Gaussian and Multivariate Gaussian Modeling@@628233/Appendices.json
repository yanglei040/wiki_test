{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we will start with the bedrock of Bayesian data assimilation: the analytical solution for the linear-Gaussian inverse problem. This exercise requires you to derive the posterior distribution by combining a Gaussian prior with a Gaussian likelihood, a process that culminates in the celebrated formulas for the posterior mean and covariance. By performing this derivation and then applying it to a concrete numerical example, you will solidify your understanding of how information from observations updates our prior knowledge and reduces uncertainty [@problem_id:3384489].", "problem": "Consider a linear inverse problem under the linear-Gaussian data assimilation framework. Let the unknown parameter vector be $m \\in \\mathbb{R}^{2}$ with a Gaussian prior $m \\sim \\mathcal{N}(m_{0}, C_{0})$, and let the observation model be $y = G m + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\Gamma)$ is independent of $m$. You are given\n$$\nm_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad\nC_{0} = \\begin{pmatrix} 1.2  0 \\\\ 0  0.8 \\end{pmatrix}, \\quad\nG = \\begin{pmatrix} 1  -0.5 \\\\ 0.2  1.5 \\end{pmatrix}, \\quad\n\\Gamma = \\begin{pmatrix} 0.5  0.1 \\\\ 0.1  0.2 \\end{pmatrix}, \\quad\ny = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nStarting from Bayes’ theorem and the quadratic form of Gaussian densities, derive the posterior distribution $m \\mid y \\sim \\mathcal{N}(m_{\\text{post}}, C_{\\text{post}})$ by completing the square in the negative log-posterior. Then, using the provided numerical matrices, compute $m_{\\text{post}}$ and $C_{\\text{post}}$ explicitly. Verify that $C_{\\text{post}}$ is symmetric positive definite (SPD), for example by checking positivity of its leading principal minors. Finally, report the scalar\n$$\ns \\equiv \\ln\\!\\big(\\det(C_{\\text{post}})\\big).\n$$\nRound your final reported value of $s$ to $4$ significant figures. Express the final answer as a pure number with no units.", "solution": "### Step 1: Extract Givens\nThe problem provides the following information:\n- The unknown parameter vector is $m \\in \\mathbb{R}^{2}$.\n- The prior distribution for $m$ is Gaussian: $m \\sim \\mathcal{N}(m_{0}, C_{0})$.\n- The observation model is linear: $y = G m + \\epsilon$.\n- The observation noise $\\epsilon$ is Gaussian, independent of $m$: $\\epsilon \\sim \\mathcal{N}(0, \\Gamma)$.\n- The specific numerical values for the matrices and vectors are:\n$$\nm_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad\nC_{0} = \\begin{pmatrix} 1.2  0 \\\\ 0  0.8 \\end{pmatrix}, \\quad\nG = \\begin{pmatrix} 1  -0.5 \\\\ 0.2  1.5 \\end{pmatrix}, \\quad\n\\Gamma = \\begin{pmatrix} 0.5  0.1 \\\\ 0.1  0.2 \\end{pmatrix}, \\quad\ny = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\n- The task is to derive the posterior distribution $m \\mid y \\sim \\mathcal{N}(m_{\\text{post}}, C_{\\text{post}})$, compute $m_{\\text{post}}$ and $C_{\\text{post}}$, verify that $C_{\\text{post}}$ is symmetric positive definite (SPD), and compute the scalar $s \\equiv \\ln(\\det(C_{\\text{post}}))$, rounding it to $4$ significant figures.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is a standard application of Bayesian inference in a linear-Gaussian model, a fundamental topic in data assimilation, inverse problems, and statistics. It is firmly based on established mathematical and statistical principles.\n- **Well-Posed:** The problem is well-posed. The given covariance matrices $C_0$ and $\\Gamma$ are required to be symmetric positive definite. $C_0$ is a diagonal matrix with positive entries ($1.2 > 0$, $0.8 > 0$), so it is SPD. For $\\Gamma$, the leading principal minors are $M_1 = 0.5 > 0$ and $M_2 = \\det(\\Gamma) = (0.5)(0.2) - (0.1)^2 = 0.1 - 0.01 = 0.09 > 0$. Thus, $\\Gamma$ is also SPD. All matrix and vector dimensions are consistent for the defined operations. The problem asks for a unique solution based on the provided data.\n- **Objective:** The problem is stated in precise, objective mathematical language, free of any subjectivity or ambiguity.\n- **Completeness:** All necessary information ($m_0, C_0, G, \\Gamma, y$) is provided to solve the problem.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid as it is scientifically grounded, well-posed, objective, and complete. I will proceed with a full solution.\n\n### Derivation of the Posterior Distribution\nAccording to Bayes' theorem, the posterior probability density function (PDF) $p(m|y)$ is proportional to the product of the likelihood $p(y|m)$ and the prior $p(m)$:\n$$p(m|y) \\propto p(y|m) p(m)$$\nThe prior is given as $m \\sim \\mathcal{N}(m_0, C_0)$, so its PDF has the form:\n$$p(m) \\propto \\exp\\left(-\\frac{1}{2}(m-m_0)^T C_0^{-1} (m-m_0)\\right)$$\nThe observation model $y = Gm + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0, \\Gamma)$ implies that the conditional distribution of $y$ given $m$ is $y|m \\sim \\mathcal{N}(Gm, \\Gamma)$. The likelihood function is thus:\n$$p(y|m) \\propto \\exp\\left(-\\frac{1}{2}(y-Gm)^T \\Gamma^{-1} (y-Gm)\\right)$$\nCombining these, the posterior PDF is:\n$$p(m|y) \\propto \\exp\\left(-\\frac{1}{2}(y-Gm)^T \\Gamma^{-1} (y-Gm) - \\frac{1}{2}(m-m_0)^T C_0^{-1} (m-m_0)\\right)$$\nThe posterior is also Gaussian, so its PDF will be of the form $p(m|y) \\propto \\exp\\left(-\\frac{1}{2}(m-m_{\\text{post}})^T C_{\\text{post}}^{-1} (m-m_{\\text{post}})\\right)$. The negative log-posterior, up to an additive constant, is the quadratic form $J(m)$:\n$$J(m) = \\frac{1}{2}(y-Gm)^T \\Gamma^{-1} (y-Gm) + \\frac{1}{2}(m-m_0)^T C_0^{-1} (m-m_0)$$\nWe expand the terms in $J(m)$ and collect terms quadratic and linear in $m$:\n$$2J(m) = (y^T - m^T G^T) \\Gamma^{-1} (y - Gm) + (m^T - m_0^T) C_0^{-1} (m - m_0)$$\n$$2J(m) = y^T \\Gamma^{-1} y - y^T \\Gamma^{-1} Gm - m^T G^T \\Gamma^{-1} y + m^T G^T \\Gamma^{-1} Gm + m^T C_0^{-1} m - m^T C_0^{-1} m_0 - m_0^T C_0^{-1} m + m_0^T C_0^{-1} m_0$$\nRecognizing that the scalar terms $y^T \\Gamma^{-1} Gm$ and $m^T C_0^{-1} m_0$ are equal to their transposes, and grouping terms:\n$$2J(m) = m^T (G^T \\Gamma^{-1} G + C_0^{-1}) m - 2 m^T (G^T \\Gamma^{-1} y + C_0^{-1} m_0) + \\text{const.}$$\nBy completing the square, we can identify the posterior precision matrix $C_{\\text{post}}^{-1}$ and the posterior mean $m_{\\text{post}}$. Comparing with the general quadratic form $ (m-m_{\\text{post}})^T C_{\\text{post}}^{-1} (m-m_{\\text{post}}) = m^T C_{\\text{post}}^{-1} m - 2m^T C_{\\text{post}}^{-1} m_{\\text{post}} + \\text{const.}$, we find:\n$$C_{\\text{post}}^{-1} = C_0^{-1} + G^T \\Gamma^{-1} G$$\n$$C_{\\text{post}}^{-1} m_{\\text{post}} = C_0^{-1} m_0 + G^T \\Gamma^{-1} y$$\nFrom these, we obtain the posterior covariance matrix and mean vector:\n$$C_{\\text{post}} = (C_0^{-1} + G^T \\Gamma^{-1} G)^{-1}$$\n$$m_{\\text{post}} = C_{\\text{post}} (C_0^{-1} m_0 + G^T \\Gamma^{-1} y)$$\n\n### Numerical Computation\nWe are given:\n$m_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, $C_{0} = \\begin{pmatrix} 1.2  0 \\\\ 0  0.8 \\end{pmatrix}$, $G = \\begin{pmatrix} 1  -0.5 \\\\ 0.2  1.5 \\end{pmatrix}$, $\\Gamma = \\begin{pmatrix} 0.5  0.1 \\\\ 0.1  0.2 \\end{pmatrix}$, $y = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\nFirst, we compute the inverse matrices $C_0^{-1}$ and $\\Gamma^{-1}$:\n$$C_0^{-1} = \\begin{pmatrix} \\frac{1}{1.2}  0 \\\\ 0  \\frac{1}{0.8} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{6}  0 \\\\ 0  \\frac{5}{4} \\end{pmatrix}$$\n$$\\det(\\Gamma) = (0.5)(0.2) - (0.1)^2 = 0.1 - 0.01 = 0.09 = \\frac{9}{100}$$\n$$\\Gamma^{-1} = \\frac{1}{0.09} \\begin{pmatrix} 0.2  -0.1 \\\\ -0.1  0.5 \\end{pmatrix} = \\frac{100}{9} \\begin{pmatrix} \\frac{2}{10}  -\\frac{1}{10} \\\\ -\\frac{1}{10}  \\frac{5}{10} \\end{pmatrix} = \\frac{10}{9} \\begin{pmatrix} 2  -1 \\\\ -1  5 \\end{pmatrix} = \\begin{pmatrix} \\frac{20}{9}  -\\frac{10}{9} \\\\ -\\frac{10}{9}  \\frac{50}{9} \\end{pmatrix}$$\nNext, we compute the Hessian term $G^T\\Gamma^{-1}G$:\n$$G^T = \\begin{pmatrix} 1  0.2 \\\\ -0.5  1.5 \\end{pmatrix} = \\begin{pmatrix} 1  \\frac{1}{5} \\\\ -\\frac{1}{2}  \\frac{3}{2} \\end{pmatrix}$$\n$$G^T \\Gamma^{-1} = \\begin{pmatrix} 1  \\frac{1}{5} \\\\ -\\frac{1}{2}  \\frac{3}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{20}{9}  -\\frac{10}{9} \\\\ -\\frac{10}{9}  \\frac{50}{9} \\end{pmatrix} = \\begin{pmatrix} \\frac{20}{9} - \\frac{2}{9}  -\\frac{10}{9} + \\frac{10}{9} \\\\ -\\frac{10}{9} - \\frac{15}{9}  \\frac{5}{9} + \\frac{75}{9} \\end{pmatrix} = \\begin{pmatrix} \\frac{18}{9}  0 \\\\ -\\frac{25}{9}  \\frac{80}{9} \\end{pmatrix} = \\begin{pmatrix} 2  0 \\\\ -\\frac{25}{9}  \\frac{80}{9} \\end{pmatrix}$$\n$$G^T \\Gamma^{-1} G = \\begin{pmatrix} 2  0 \\\\ -\\frac{25}{9}  \\frac{80}{9} \\end{pmatrix} \\begin{pmatrix} 1  -\\frac{1}{2} \\\\ \\frac{1}{5}  \\frac{3}{2} \\end{pmatrix} = \\begin{pmatrix} 2  -1 \\\\ -\\frac{25}{9} + \\frac{16}{9}  \\frac{25}{18} + \\frac{120}{9} \\end{pmatrix} = \\begin{pmatrix} 2  -1 \\\\ -\\frac{9}{9}  \\frac{25}{18} + \\frac{240}{18} \\end{pmatrix} = \\begin{pmatrix} 2  -1 \\\\ -1  \\frac{265}{18} \\end{pmatrix}$$\nNow we compute the posterior precision matrix $C_{\\text{post}}^{-1}$:\n$$C_{\\text{post}}^{-1} = C_0^{-1} + G^T \\Gamma^{-1} G = \\begin{pmatrix} \\frac{5}{6}  0 \\\\ 0  \\frac{5}{4} \\end{pmatrix} + \\begin{pmatrix} 2  -1 \\\\ -1  \\frac{265}{18} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{6} + \\frac{12}{6}  -1 \\\\ -1  \\frac{45}{36} + \\frac{530}{36} \\end{pmatrix} = \\begin{pmatrix} \\frac{17}{6}  -1 \\\\ -1  \\frac{575}{36} \\end{pmatrix}$$\nTo find $C_{\\text{post}}$, we invert $C_{\\text{post}}^{-1}$:\n$$\\det(C_{\\text{post}}^{-1}) = \\left(\\frac{17}{6}\\right)\\left(\\frac{575}{36}\\right) - (-1)^2 = \\frac{9775}{216} - 1 = \\frac{9775 - 216}{216} = \\frac{9559}{216}$$\n$$C_{\\text{post}} = \\frac{1}{\\det(C_{\\text{post}}^{-1})} \\begin{pmatrix} \\frac{575}{36}  1 \\\\ 1  \\frac{17}{6} \\end{pmatrix} = \\frac{216}{9559} \\begin{pmatrix} \\frac{575}{36}  1 \\\\ 1  \\frac{17}{6} \\end{pmatrix} = \\frac{1}{9559} \\begin{pmatrix} 6 \\cdot 575  216 \\\\ 216  36 \\cdot 17 \\end{pmatrix}$$\n$$C_{\\text{post}} = \\frac{1}{9559} \\begin{pmatrix} 3450  216 \\\\ 216  612 \\end{pmatrix}$$\nNow we compute $m_{\\text{post}}$. Since $m_0 = 0$, the formula simplifies to $m_{\\text{post}} = C_{\\text{post}} (G^T \\Gamma^{-1} y)$.\n$$G^T \\Gamma^{-1} y = \\begin{pmatrix} 2  0 \\\\ -\\frac{25}{9}  \\frac{80}{9} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -\\frac{25}{9} \\end{pmatrix}$$\n$$m_{\\text{post}} = C_{\\text{post}} \\begin{pmatrix} 2 \\\\ -\\frac{25}{9} \\end{pmatrix} = \\frac{1}{9559} \\begin{pmatrix} 3450  216 \\\\ 216  612 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -\\frac{25}{9} \\end{pmatrix} = \\frac{1}{9559} \\begin{pmatrix} 3450(2) + 216(-\\frac{25}{9}) \\\\ 216(2) + 612(-\\frac{25}{9}) \\end{pmatrix}$$\n$$m_{\\text{post}} = \\frac{1}{9559} \\begin{pmatrix} 6900 - 24(25) \\\\ 432 - 68(25) \\end{pmatrix} = \\frac{1}{9559} \\begin{pmatrix} 6900 - 600 \\\\ 432 - 1700 \\end{pmatrix} = \\frac{1}{9559} \\begin{pmatrix} 6300 \\\\ -1268 \\end{pmatrix}$$\nIn decimal form, this is $m_{\\text{post}} \\approx \\begin{pmatrix} 0.65906 \\\\ -0.13265 \\end{pmatrix}$.\n\n### Verification of $C_{\\text{post}}$\n- **Symmetry:** The matrix $C_{\\text{post}} = \\frac{1}{9559} \\begin{pmatrix} 3450  216 \\\\ 216  612 \\end{pmatrix}$ is clearly symmetric as its off-diagonal entries are equal.\n- **Positive Definiteness:** We check the leading principal minors.\nThe first minor is $C_{\\text{post},11} = \\frac{3450}{9559} > 0$.\nThe second minor is $\\det(C_{\\text{post}})$. We know that $\\det(C_{\\text{post}}) = (\\det(C_{\\text{post}}^{-1}))^{-1}$. Since $\\det(C_{\\text{post}}^{-1}) = \\frac{9559}{216} > 0$, we have $\\det(C_{\\text{post}}) = \\frac{216}{9559} > 0$.\nSince all leading principal minors are positive, $C_{\\text{post}}$ is symmetric positive definite.\n\n### Calculation of $s$\nThe final scalar to compute is $s \\equiv \\ln(\\det(C_{\\text{post}}))$.\n$$s = \\ln\\left(\\frac{216}{9559}\\right)$$\nUsing a calculator:\n$$s \\approx \\ln(0.0225965059...) \\approx -3.78994114...$$\nRounding to $4$ significant figures, we get:\n$$s \\approx -3.790$$", "answer": "$$\\boxed{-3.790}$$", "id": "3384489"}, {"introduction": "Having learned to compute the posterior estimate, a critical next step for any practitioner is to assess the quality of the model fit. A model that is inconsistent with the data it is trying to explain is of little use. This practice introduces a fundamental diagnostic tool: misfit analysis, which quantifies the discrepancy between the observations and the model's posterior prediction, properly weighted by the observation error covariance [@problem_id:3384530]. By comparing this observed misfit to its theoretically expected value, you will learn to diagnose common modeling pathologies such as underfitting and overfitting.", "problem": "Consider the linear inverse problem in a data assimilation setting where the goal is to infer an unknown parameter vector $m \\in \\mathbb{R}^p$ from observations $y \\in \\mathbb{R}^n$. Assume a linear observation operator $G \\in \\mathbb{R}^{n \\times p}$, a Gaussian prior $m \\sim \\mathcal{N}(m_{\\text{prior}}, C_{\\text{prior}})$ with mean $m_{\\text{prior}} \\in \\mathbb{R}^p$ and covariance $C_{\\text{prior}} \\in \\mathbb{R}^{p \\times p}$, and a Gaussian likelihood model $y \\mid m \\sim \\mathcal{N}(G m, \\Gamma)$ with observation noise covariance $\\Gamma \\in \\mathbb{R}^{n \\times n}$. All covariance matrices are Symmetric Positive Definite (SPD).\n\nThe normalized misfit is defined as the squared norm of the posterior residual in the observation noise metric, namely\n$$\n\\|y - G m_{\\text{post}}\\|_{\\Gamma^{-1}}^2 = (y - G m_{\\text{post}})^\\top \\Gamma^{-1} (y - G m_{\\text{post}}),\n$$\nwhere $m_{\\text{post}}$ denotes the posterior mean of $m$ under the model assumptions above.\n\nThe expected value of the normalized misfit under the linear Gaussian model provides a reference to assess model fit quality. Comparing the observed normalized misfit to its expected value allows one to flag potential overfitting or underfitting. Your task is to derive the posterior mean $m_{\\text{post}}$ and the expected value of the normalized misfit using accepted identities of multivariate Gaussian distributions and linear operators, starting only from the model assumptions stated above. Then, implement a program that:\n\n1. Computes $m_{\\text{post}}$.\n2. Computes the normalized misfit $\\|y - G m_{\\text{post}}\\|_{\\Gamma^{-1}}^2$.\n3. Computes the expected value of the normalized misfit predicted by the model.\n4. Classifies the fit as:\n   - $-1$ for overfitting (observed misfit significantly below the expected level),\n   - $0$ for adequate fit,\n   - $1$ for underfitting (observed misfit significantly above the expected level).\n   Use a two-standard-deviation band around the expected value as the decision threshold, where the standard deviation is derived under the same linear Gaussian assumptions.\n\nNo physical units are involved in this problem.\n\nUse the following test suite of three cases, each specified by $(G, C_{\\text{prior}}, m_{\\text{prior}}, \\Gamma, y)$:\n\n- Test Case 1 (balanced, happy path):\n  - $n = 4$, $p = 3$,\n  - $G = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\\\ 1  1  1 \\end{bmatrix}$,\n  - $C_{\\text{prior}} = \\operatorname{diag}(1.0, 0.5, 2.0)$,\n  - $m_{\\text{prior}} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n  - $\\Gamma = \\operatorname{diag}(0.5, 0.5, 0.5, 0.5)$,\n  - $y = \\begin{bmatrix} 1.0 \\\\ -1.0 \\\\ 0.5 \\\\ 0.5 \\end{bmatrix}$.\n\n- Test Case 2 (boundary, zero innovation leading to potential overfitting flag):\n  - $n = 4$, $p = 3$,\n  - $G = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\\\ 1  1  1 \\end{bmatrix}$,\n  - $C_{\\text{prior}} = \\operatorname{diag}(1.0, 1.0, 1.0)$,\n  - $m_{\\text{prior}} = \\begin{bmatrix} 0.2 \\\\ -0.3 \\\\ 0.1 \\end{bmatrix}$,\n  - $\\Gamma = \\operatorname{diag}(1.0, 1.0, 1.0, 1.0)$,\n  - $y = \\begin{bmatrix} 0.2 \\\\ -0.3 \\\\ 0.1 \\\\ 0.0 \\end{bmatrix}$.\n\n- Test Case 3 (edge case, strong prior and moderate noise leading to potential underfitting flag):\n  - $n = 4$, $p = 3$,\n  - $G = \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\\\ 1  1  1 \\end{bmatrix}$,\n  - $C_{\\text{prior}} = \\operatorname{diag}(10^{-3}, 10^{-3}, 10^{-3})$,\n  - $m_{\\text{prior}} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$,\n  - $\\Gamma = \\operatorname{diag}(0.3, 0.3, 0.3, 0.3)$,\n  - $y = \\begin{bmatrix} 5.0 \\\\ -4.0 \\\\ 3.0 \\\\ 10.0 \\end{bmatrix}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one entry per test case. Each entry must be a triple $[\\text{misfit}, \\text{expected}, \\text{classification}]$, where $\\text{misfit}$ and $\\text{expected}$ are real numbers (floats) and $\\text{classification}$ is an integer in $\\{-1, 0, 1\\}$. For example, the output format must be of the form:\n$$\n[[\\text{misfit}_1,\\text{expected}_1,\\text{classification}_1],[\\text{misfit}_2,\\text{expected}_2,\\text{classification}_2],[\\text{misfit}_3,\\text{expected}_3,\\text{classification}_3]].\n$$", "solution": "The user-provided problem is a standard exercise in Bayesian inference for linear-Gaussian models, a cornerstone of data assimilation and inverse problems. The problem is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. Therefore, the problem is deemed valid and a full solution is provided below.\n\nThe solution proceeds in two main parts. First, we provide a theoretical derivation of all required quantities. Second, we implement these derivations in a Python program to solve the specific test cases.\n\n### Theoretical Derivation\n\nThe modeling framework is defined by a prior distribution on the parameters $m$ and a likelihood function for the observations $y$ given $m$.\n\n1.  **Prior Model**: The parameter vector $m \\in \\mathbb{R}^p$ is assumed to follow a Gaussian distribution, $m \\sim \\mathcal{N}(m_{\\text{prior}}, C_{\\text{prior}})$. The probability density function (PDF) is given by:\n    $$\n    p(m) \\propto \\exp\\left(-\\frac{1}{2} (m - m_{\\text{prior}})^\\top C_{\\text{prior}}^{-1} (m - m_{\\text{prior}})\\right)\n    $$\n    where $m_{\\text{prior}} \\in \\mathbb{R}^p$ is the prior mean and $C_{\\text{prior}} \\in \\mathbb{R}^{p \\times p}$ is the symmetric positive definite (SPD) prior covariance matrix.\n\n2.  **Likelihood Model**: The observations $y \\in \\mathbb{R}^n$ are related to the parameters $m$ through a linear operator $G \\in \\mathbb{R}^{n \\times p}$ and are corrupted by additive Gaussian noise. The conditional distribution of $y$ given $m$ is $y \\mid m \\sim \\mathcal{N}(G m, \\Gamma)$. The likelihood function is:\n    $$\n    p(y \\mid m) \\propto \\exp\\left(-\\frac{1}{2} (y - G m)^\\top \\Gamma^{-1} (y - G m)\\right)\n    $$\n    where $\\Gamma \\in \\mathbb{R}^{n \\times n}$ is the SPD observation noise covariance matrix.\n\n#### 1. Posterior Distribution and Posterior Mean\n\nAccording to Bayes' theorem, the posterior distribution of $m$ given $y$ is proportional to the product of the likelihood and the prior, $p(m \\mid y) \\propto p(y \\mid m) p(m)$.\nThe posterior PDF is therefore proportional to the exponential of a quadratic form in $m$, which implies that the posterior is also Gaussian, $m \\mid y \\sim \\mathcal{N}(m_{\\text{post}}, C_{\\text{post}})$. The negative logarithm of the posterior PDF is (up to an additive constant):\n$$\nJ(m) = \\frac{1}{2} (m - m_{\\text{prior}})^\\top C_{\\text{prior}}^{-1} (m - m_{\\text{prior}}) + \\frac{1}{2} (y - G m)^\\top \\Gamma^{-1} (y - G m)\n$$\nThe posterior mean, $m_{\\text{post}}$, is the value of $m$ that maximizes the posterior PDF, which is equivalent to minimizing the quadratic cost function $J(m)$. We find this minimum by setting the gradient of $J(m)$ with respect to $m$ to zero:\n$$\n\\nabla_m J(m) = C_{\\text{prior}}^{-1} (m - m_{\\text{prior}}) - G^\\top \\Gamma^{-1} (y - G m) = 0\n$$\n$$\nC_{\\text{prior}}^{-1} m - C_{\\text{prior}}^{-1} m_{\\text{prior}} - G^\\top \\Gamma^{-1} y + G^\\top \\Gamma^{-1} G m = 0\n$$\nRearranging the terms to solve for $m$:\n$$\n(C_{\\text{prior}}^{-1} + G^\\top \\Gamma^{-1} G) m = C_{\\text{prior}}^{-1} m_{\\text{prior}} + G^\\top \\Gamma^{-1} y\n$$\nThe term $(C_{\\text{prior}}^{-1} + G^\\top \\Gamma^{-1} G)$ is the Hessian of $J(m)$, which is the inverse of the posterior covariance matrix, $C_{\\text{post}}^{-1}$. Since $C_{\\text{prior}}$ and $\\Gamma$ are SPD, $C_{\\text{post}}^{-1}$ is also SPD and thus invertible.\n$$\nC_{\\text{post}} = (C_{\\text{prior}}^{-1} + G^\\top \\Gamma^{-1} G)^{-1}\n$$\nThe posterior mean $m_{\\text{post}}$ is then:\n$$\nm_{\\text{post}} = C_{\\text{post}} (C_{\\text{prior}}^{-1} m_{\\text{prior}} + G^\\top \\Gamma^{-1} y)\n$$\nThis expression is computationally efficient when the parameter dimension $p$ is small, as it requires the inversion of a $p \\times p$ matrix.\n\n#### 2. Expected Value of the Normalized Misfit\n\nThe normalized misfit is defined as $\\chi^2 = \\|y - G m_{\\text{post}}\\|_{\\Gamma^{-1}}^2$. We seek its expected value, $\\mathbb{E}[\\chi^2]$, under the prior predictive distribution of the data. The data $y$ is a random variable whose distribution is obtained by marginalizing the joint distribution $p(y, m) = p(y|m)p(m)$.\nThe mean of $y$ is $\\mathbb{E}[y] = \\mathbb{E}[\\mathbb{E}[y|m]] = \\mathbb{E}[Gm] = G m_{\\text{prior}}$.\nThe covariance of $y$ is $\\text{Cov}(y) = \\mathbb{E}[\\text{Cov}(y|m)] + \\text{Cov}(\\mathbb{E}[y|m]) = \\Gamma + G C_{\\text{prior}} G^\\top$.\nLet us define the innovation vector $d = y - G m_{\\text{prior}}$. Under the model, $d$ is a zero-mean Gaussian random variable, $d \\sim \\mathcal{N}(0, S)$, where $S = \\Gamma + G C_{\\text{prior}} G^\\top$ is the innovation covariance matrix.\n\nWe can express the posterior mean using the Kalman gain formulation: $m_{\\text{post}} = m_{\\text{prior}} + K d$, where $K = C_{\\text{prior}} G^\\top S^{-1}$ is the Kalman gain.\nThe posterior residual is $r = y - G m_{\\text{post}} = (d + G m_{\\text{prior}}) - G(m_{\\text{prior}} + K d) = (I_n - GK)d$, where $I_n$ is the $n \\times n$ identity matrix.\nThe misfit becomes a quadratic form in $d$:\n$$\n\\chi^2 = r^\\top \\Gamma^{-1} r = d^\\top (I_n - GK)^\\top \\Gamma^{-1} (I_n - GK) d\n$$\nThe expected value of a quadratic form $x^\\top A x$ for a random vector $x$ with mean $\\mu_x$ and covariance $\\Sigma_x$ is $\\mathbb{E}[x^\\top A x] = \\text{Tr}(A \\Sigma_x) + \\mu_x^\\top A \\mu_x$. Since $\\mu_d = 0$, we have:\n$$\n\\mathbb{E}[\\chi^2] = \\text{Tr}\\left( (I_n - GK)^\\top \\Gamma^{-1} (I_n - GK) S \\right)\n$$\nUsing the identity $(I_n - GK)S = \\Gamma$ (which can be verified by substituting the expressions for $K$ and $S$), the expression simplifies:\n$$\n\\mathbb{E}[\\chi^2] = \\text{Tr}\\left( (I_n - GK)^\\top \\Gamma^{-1} \\Gamma \\right) = \\text{Tr}\\left( (I_n - GK)^\\top \\right) = \\text{Tr}(I_n - GK)\n$$\n$$\n\\mathbb{E}[\\chi^2] = n - \\text{Tr}(GK)\n$$\nFor computational purposes, we can express $GK$ using $C_{\\text{post}}$:\n$K = C_{\\text{post}} G^\\top \\Gamma^{-1}$, which gives $GK = G C_{\\text{post}} G^\\top \\Gamma^{-1}$.\nThus, the expected misfit is:\n$$\n\\mathbb{E}[\\chi^2] = n - \\text{Tr}(G C_{\\text{post}} G^\\top \\Gamma^{-1})\n$$\n\n#### 3. Standard Deviation of the Normalized Misfit\n\nTo establish the classification threshold, we need the variance of the misfit, $\\text{Var}(\\chi^2)$. For a zero-mean Gaussian vector $d \\sim \\mathcal{N}(0, S)$, the variance of the quadratic form $\\chi^2 = d^\\top A d$ where $A$ is a symmetric matrix is $\\text{Var}(\\chi^2) = 2 \\text{Tr}((AS)^2)$.\nThe matrix of our quadratic form is $A_0 = (I_n - GK)^\\top \\Gamma^{-1} (I_n - GK)$. Symmetrizing it yields $A = (A_0 + A_0^\\top)/2$.\nAs shown before, $(I_n - GK)S = \\Gamma$, so $(I_n-GK) = \\Gamma S^{-1}$.\n$A_0 = (\\Gamma S^{-1})^\\top \\Gamma^{-1} (\\Gamma S^{-1}) = (S^{-1})^\\top \\Gamma^\\top \\Gamma^{-1} \\Gamma S^{-1} = S^{-1} \\Gamma S^{-1}$ (since $\\Gamma$ and $S$ are symmetric). This matrix $A$ is already symmetric.\nTherefore, $A = S^{-1}\\Gamma S^{-1}$.\nThe term $AS$ becomes $AS = (S^{-1} \\Gamma S^{-1}) S = S^{-1} \\Gamma$.\nThe variance is then:\n$$\n\\text{Var}(\\chi^2) = 2 \\text{Tr}((S^{-1} \\Gamma)^2)\n$$\nFor computation, we express $S^{-1}\\Gamma$ in terms of $C_{\\text{post}}$. Using the Woodbury matrix identity, $S^{-1} = (\\Gamma + G C_{\\text{prior}} G^\\top)^{-1} = \\Gamma^{-1} - \\Gamma^{-1} G C_{\\text{post}} G^\\top \\Gamma^{-1}$.\nThen, $S^{-1}\\Gamma = (\\Gamma^{-1} - \\Gamma^{-1} G C_{\\text{post}} G^\\top \\Gamma^{-1})\\Gamma = I_n - \\Gamma^{-1} G C_{\\text{post}} G^\\top$.\nLet $M = I_n - \\Gamma^{-1} G C_{\\text{post}} G^\\top$. The variance is:\n$$\n\\text{Var}(\\chi^2) = 2 \\text{Tr}(M^2)\n$$\nThe standard deviation is $\\sigma_{\\chi^2} = \\sqrt{\\text{Var}(\\chi^2)}$.\n\n#### 4. Classification Criterion\n\nThe classification of the model fit is based on comparing the observed misfit $\\chi^2_{\\text{obs}}$ with a confidence interval around its expected value:\n- Overfitting ($-1$): $\\chi^2_{\\text{obs}}  \\mathbb{E}[\\chi^2] - 2\\sigma_{\\chi^2}$\n- Underfitting ($1$): $\\chi^2_{\\text{obs}}  \\mathbb{E}[\\chi^2] + 2\\sigma_{\\chi^2}$\n- Adequate fit ($0$): Otherwise.\n\nThis completes the theoretical framework needed to solve the problem.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the linear inverse problem for three test cases, computing\n    the posterior mean, normalized misfit, its expected value, and a\n    fit classification.\n    \"\"\"\n\n    test_cases = [\n        # Test Case 1: balanced, happy path\n        {\n            \"G\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]]),\n            \"C_prior\": np.diag([1.0, 0.5, 2.0]),\n            \"m_prior\": np.array([0.0, 0.0, 0.0]),\n            \"Gamma\": np.diag([0.5, 0.5, 0.5, 0.5]),\n            \"y\": np.array([1.0, -1.0, 0.5, 0.5]),\n        },\n        # Test Case 2: boundary, zero innovation leading to potential overfitting flag\n        {\n            \"G\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]]),\n            \"C_prior\": np.diag([1.0, 1.0, 1.0]),\n            \"m_prior\": np.array([0.2, -0.3, 0.1]),\n            \"Gamma\": np.diag([1.0, 1.0, 1.0, 1.0]),\n            \"y\": np.array([0.2, -0.3, 0.1, 0.0]),\n        },\n        # Test Case 3: edge case, strong prior and moderate noise - underfitting\n        {\n            \"G\": np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]]),\n            \"C_prior\": np.diag([1e-3, 1e-3, 1e-3]),\n            \"m_prior\": np.array([0.0, 0.0, 0.0]),\n            \"Gamma\": np.diag([0.3, 0.3, 0.3, 0.3]),\n            \"y\": np.array([5.0, -4.0, 3.0, 10.0]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        G = case[\"G\"]\n        C_prior = case[\"C_prior\"]\n        m_prior = case[\"m_prior\"]\n        Gamma = case[\"Gamma\"]\n        y = case[\"y\"]\n\n        n, p = G.shape\n\n        # Matrix inversions. For diagonal matrices, this is trivial, but\n        # the general formulas are implemented for correctness.\n        try:\n            C_prior_inv = np.linalg.inv(C_prior)\n            Gamma_inv = np.linalg.inv(Gamma)\n        except np.linalg.LinAlgError:\n            # Handle singular matrices if they occurred, though not expected here.\n            results.append([float('nan'), float('nan'), 0])\n            continue\n\n        # 1. Compute posterior covariance C_post\n        # C_post_inv = C_prior_inv + G^T Gamma_inv G\n        # This involves a p x p inversion, which is efficient for p  n.\n        C_post_inv = C_prior_inv + G.T @ Gamma_inv @ G\n        C_post = np.linalg.inv(C_post_inv)\n\n        # 2. Compute posterior mean m_post\n        # m_post = C_post (C_prior_inv m_prior + G^T Gamma_inv y)\n        m_post = C_post @ (C_prior_inv @ m_prior + G.T @ Gamma_inv @ y)\n\n        # 3. Compute the observed normalized misfit\n        # misfit = (y - G m_post)^T Gamma_inv (y - G m_post)\n        residual = y - G @ m_post\n        misfit = residual.T @ Gamma_inv @ residual\n        \n        # 4. Compute the expected value of the misfit\n        # E[misfit] = n - Tr(G C_post G^T Gamma_inv)\n        trace_matrix_exp = G @ C_post @ G.T @ Gamma_inv\n        expected_misfit = n - np.trace(trace_matrix_exp)\n\n        # 5. Compute the standard deviation of the misfit\n        # Var[misfit] = 2 * Tr((I - Gamma_inv G C_post G^T)^2)\n        M = np.identity(n) - Gamma_inv @ G @ C_post @ G.T\n        var_misfit = 2 * np.trace(M @ M)\n        std_misfit = np.sqrt(var_misfit)\n\n        # 6. Classify the fit\n        lower_bound = expected_misfit - 2 * std_misfit\n        upper_bound = expected_misfit + 2 * std_misfit\n\n        classification = 0\n        if misfit  lower_bound:\n            classification = -1  # Overfitting\n        elif misfit  upper_bound:\n            classification = 1   # Underfitting\n\n        results.append([misfit, expected_misfit, classification])\n    \n    # Format the final output string as per the problem specification\n    # [[misfit_1,expected_1,classification_1],[misfit_2,expected_2,classification_2],...]\n    list_of_strings = []\n    for res in results:\n        list_of_strings.append(f\"[{res[0]},{res[1]},{res[2]}]\")\n    final_output = f\"[{','.join(list_of_strings)}]\"\n    \n    print(final_output)\n\n\nsolve()\n\n```", "id": "3384530"}, {"introduction": "While direct matrix algebra is instructive for small-scale problems, real-world applications in fields like weather prediction or medical imaging involve state spaces of enormous dimension, making the explicit construction of covariance matrices computationally infeasible. This advanced practice challenges you to step into the world of large-scale inverse problems by implementing matrix-free algorithms. You will use the Preconditioned Conjugate Gradient (PCG) method to compute the action of the posterior covariance and the Lanczos method to draw approximate samples from the posterior, all without ever forming a dense matrix [@problem_id:3384561]. These techniques are the workhorses of modern computational data assimilation.", "problem": "You are given a linear Gaussian inverse problem on a one-dimensional grid with an unknown field modeled as a zero-mean Gaussian random field and noisy pointwise observations. The prior model is specified by a symmetric positive definite precision operator. The likelihood arises from pointwise restriction, with independent Gaussian noise. Your task is to design and implement a matrix-free algorithm to compute the posterior covariance action on a vector and to draw approximate samples from the posterior using Krylov subspace methods without forming dense matrices.\n\nLet the unknown field live on a uniform grid with $n$ points. Denote the unknown by $x \\in \\mathbb{R}^n$ with prior $x \\sim \\mathcal{N}(0, C_{\\text{prior}})$ and precision $Q = C_{\\text{prior}}^{-1}$. The observations are modeled by $y = A x + \\eta$, where $A \\in \\mathbb{R}^{m \\times n}$ is a restriction operator sampling $m$ grid positions, and $\\eta \\sim \\mathcal{N}(0, \\Gamma)$ with $\\Gamma = \\sigma^2 I_m$.\n\nFrom the law of Gaussian posterior in linear inverse problems, the posterior is Gaussian with precision given by\n$$\nH \\equiv Q + A^\\top \\Gamma^{-1} A,\n$$\nand posterior covariance $C_{\\text{post}} = H^{-1}$. The required outputs in this task rely on computing, in a matrix-free manner, the following two operations:\n1. The posterior covariance action on a vector $v \\in \\mathbb{R}^n$, i.e., compute $C_{\\text{post}} v$, which is the solution $w$ to the symmetric positive definite system $H w = v$.\n2. An approximate posterior sample $s \\approx C_{\\text{post}}^{1/2} z$ for a given $z \\in \\mathbb{R}^n$ with $z \\sim \\mathcal{N}(0, I_n)$, via Lanczos approximation of the operator function $H^{-1/2}$ applied to $z$.\n\nYou must adhere to the following modeling details and algorithmic constraints:\n- The prior precision $Q$ is the one-dimensional discrete negative Laplacian with Neumann boundary conditions plus a pointwise mass term. On a uniform grid with spacing $h$, define for each component $i \\in \\{0, 1, \\dots, n-1\\}$,\n$$\n(Q x)_i = \\lambda \\left( \\frac{2 x_i - x_{i-1} - x_{i+1}}{h^2} + \\beta x_i \\right),\n$$\nwith mirrored boundary values $x_{-1} = x_0$ and $x_{n} = x_{n-1}$, and parameters $\\lambda  0$ and $\\beta  0$.\n- The observation operator $A$ is pointwise restriction at a fixed set of indices $\\mathcal{I} \\subset \\{0, 1, \\dots, n-1\\}$ of cardinality $m$, so $A x \\in \\mathbb{R}^m$ extracts the entries $x_i$ for $i \\in \\mathcal{I}$. Consequently,\n$$\nA^\\top \\Gamma^{-1} A x = \\sum_{i \\in \\mathcal{I}} \\frac{1}{\\sigma^2} x_i e_i,\n$$\nwhere $e_i$ is the $i$-th canonical basis vector of $\\mathbb{R}^n$.\n- You must not form the dense matrix $H$ or any dense matrix representing $Q$, $A$, or $A^\\top \\Gamma^{-1} A$. Instead, implement matrix-free routines to apply $Q$ and $A^\\top \\Gamma^{-1} A$ to a vector and compose these to apply $H$ to a vector.\n- To compute $C_{\\text{post}} v$, use Preconditioned Conjugate Gradients (PCG). The preconditioner must be Jacobi (diagonal), i.e., $M = \\operatorname{diag}(H)$, and must be applied as $M^{-1}$ via elementwise division. The diagonal of $Q$ is given by\n$$\n\\operatorname{diag}(Q)_i = \\lambda \\left( \\beta + d_i \\right), \\quad d_i = \\begin{cases} \\dfrac{1}{h^2},  i \\in \\{0, n-1\\} \\\\ \\dfrac{2}{h^2},  \\text{otherwise} \\end{cases}\n$$\nand the diagonal of $A^\\top \\Gamma^{-1} A$ contributes $\\dfrac{1}{\\sigma^2}$ on indices in $\\mathcal{I}$ and $0$ elsewhere.\n- To draw an approximate sample $s \\approx H^{-1/2} z$, implement the Lanczos method with at most $k$ iterations to build an orthonormal basis $Q_k = [q_1, \\dots, q_k]$ of the Krylov subspace $\\mathcal{K}_k(H, z)$ and the corresponding tridiagonal matrix $T_k$. Then use the standard Lanczos functional approximation\n$$\nH^{-1/2} z \\approx \\| z \\|_2 \\, Q_k f(T_k) e_1, \\quad f(\\cdot) = (\\cdot)^{-1/2},\n$$\nby computing the symmetric eigenvalue decomposition of $T_k$, applying $f$ to the eigenvalues, and transforming back. Use no reorthogonalization, and assume $H$ is symmetric positive definite. The stopping criterion for Lanczos is a fixed number of iterations $k$.\n\nFor validation and test coverage, compute the following two scalar diagnostics for each test case:\n- The residual norm for $w = C_{\\text{post}} v$,\n$$\nr_{\\text{norm}} = \\| H w - v \\|_2,\n$$\nas a floating-point number.\n- The energy identity error for $s \\approx H^{-1/2} z$,\n$$\n\\varepsilon_{\\text{energy}} = \\left| s^\\top H s - \\| z \\|_2^2 \\right|,\n$$\nas a floating-point number. The identity $s^\\top H s = \\| z \\|_2^2$ holds exactly for $s = H^{-1/2} z$; your approximate sample should yield a small error.\n\nYou must implement the above for the following test suite, which specifies parameters and inputs:\n- Case $1$ (happy path): $n = 50$, $m = 15$, $\\lambda = 1.0$, $\\beta = 1.0$, $\\sigma = 0.2$, Lanczos iterations $k = 20$, observation indices $\\mathcal{I}$ chosen as $m$ approximately evenly spaced grid indices. The grid spacing is $h = \\dfrac{1}{n-1}$. Use the deterministic probe vector $v \\in \\mathbb{R}^{n}$ with entries $v_i = \\sin\\left( 2 \\pi \\left( \\dfrac{i + 0.5}{n} \\right) \\right)$ for $i = 0, \\dots, n-1$. For sampling, use a standard normal $z \\sim \\mathcal{N}(0, I_n)$ generated with a fixed seed equal to the case index ($1$).\n- Case $2$ (high-noise boundary): $n = 50$, $m = 25$, $\\lambda = 1.0$, $\\beta = 0.5$, $\\sigma = 5.0$, $k = 25$, observation indices chosen as above. Use the same deterministic $v$ definition. For sampling, use $z \\sim \\mathcal{N}(0, I_n)$ with seed $2$.\n- Case $3$ (no-data edge): $n = 50$, $m = 0$, $\\lambda = 2.0$, $\\beta = 0.1$, $\\sigma = 1.0$, $k = 20$, no observation indices. Use the same deterministic $v$. For sampling, use $z \\sim \\mathcal{N}(0, I_n)$ with seed $3$.\n- Case $4$ (low-noise, dense observations): $n = 50$, $m = 40$, $\\lambda = 1.0$, $\\beta = 1.0$, $\\sigma = 0.01$, $k = 30$, observation indices chosen as above. Use the same deterministic $v$. For sampling, use $z \\sim \\mathcal{N}(0, I_n)$ with seed $4$.\n\nAlgorithmic requirements:\n- Implement a matrix-free routine to apply $Q$, $A^\\top \\Gamma^{-1} A$, and $H$ to a vector for arbitrary $n$, $m$, $\\lambda$, $\\beta$, $\\sigma$, and observation index set $\\mathcal{I}$.\n- Implement Preconditioned Conjugate Gradients (PCG) with the Jacobi preconditioner $M = \\operatorname{diag}(H)$ to compute $w$ solving $H w = v$, with tolerance $10^{-8}$ on the relative residual and maximum iterations $200$.\n- Implement Lanczos with a fixed iteration count $k$ to approximate $H^{-1/2} z$ via tridiagonal projection and spectral mapping.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the four test cases as a comma-separated list of pairs of floating-point numbers enclosed in square brackets, with no spaces. Each pair corresponds to $[r_{\\text{norm}}, \\varepsilon_{\\text{energy}}]$ for a test case. For example, the output should look like\n$$\n[\\,[r_1,\\varepsilon_1],[r_2,\\varepsilon_2],[r_3,\\varepsilon_3],[r_4,\\varepsilon_4]\\,]\n$$\nbut printed as a single line with no spaces as\n\"[[r1,eps1],[r2,eps2],[r3,eps3],[r4,eps4]]\".", "solution": "The problem requires the design and implementation of matrix-free algorithms to analyze the posterior distribution in a linear Gaussian inverse problem. Specifically, we need to compute the action of the posterior covariance operator on a vector and to draw an approximate sample from the posterior distribution. The problem is well-posed and scientifically grounded in the field of Bayesian inverse problems and data assimilation. The methods specified—Preconditioned Conjugate Gradients (PCG) and the Lanczos method for function approximation—are standard and appropriate for this context.\n\nThe posterior precision operator $H$ is the sum of the prior precision $Q$ and the data precision term $A^\\top \\Gamma^{-1} A$:\n$$\nH = Q + A^\\top \\Gamma^{-1} A\n$$\nThis operator is symmetric and positive definite, as it is the sum of a symmetric positive definite operator $Q$ (since $\\lambda  0, \\beta  0$) and a symmetric positive semi-definite operator $A^\\top \\Gamma^{-1} A$. This property is fundamental for the stability and convergence of the chosen numerical methods.\n\nWe will structure the solution by first defining the matrix-free applications of the operators, then detailing the PCG algorithm for solving the linear system, and finally explaining the Lanczos-based method for approximating the operator square root matrix-vector product.\n\n**1. Matrix-Free Operator Implementation**\n\nA central requirement is to avoid forming dense matrices. We implement functions that compute the action of operators $Q$, $A^\\top \\Gamma^{-1} A$, and $H$ on an arbitrary vector $x \\in \\mathbb{R}^n$.\n\nThe prior precision operator $Q$ is defined as:\n$$\n(Q x)_i = \\lambda \\left( \\frac{2 x_i - x_{i-1} - x_{i+1}}{h^2} + \\beta x_i \\right)\n$$\nwith Neumann boundary conditions $x_{-1} = x_0$ and $x_{n} = x_{n-1}$. These conditions can be implemented efficiently using array slicing. For a vector $x$ of length $n$, the action $(Qx)$ can be computed as follows:\n- For the interior points $i \\in \\{1, \\dots, n-2\\}$: $(Qx)_i = \\lambda \\left( \\frac{2x_i - x_{i-1} - x_{i+1}}{h^2} + \\beta x_i \\right)$.\n- For the boundary point $i=0$: $(Qx)_0 = \\lambda \\left( \\frac{2x_0 - x_0 - x_1}{h^2} + \\beta x_0 \\right) = \\lambda \\left( \\frac{x_0 - x_1}{h^2} + \\beta x_0 \\right)$.\n- For the boundary point $i=n-1$: $(Qx)_{n-1} = \\lambda \\left( \\frac{2x_{n-1} - x_{n-2} - x_{n-1}}{h^2} + \\beta x_{n-1} \\right) = \\lambda \\left( \\frac{x_{n-1} - x_{n-2}}{h^2} + \\beta x_{n-1} \\right)$.\n\nThe data precision operator $A^\\top \\Gamma^{-1} A$ is a diagonal operator that acts on a vector $x$ by scaling its components corresponding to the observation locations. Given the observation noise model $\\eta \\sim \\mathcal{N}(0, \\sigma^2 I_m)$, the inverse noise covariance is $\\Gamma^{-1} = \\frac{1}{\\sigma^2} I_m$. The action of this operator is:\n$$\n(A^\\top \\Gamma^{-1} A x)_i = \\begin{cases} \\frac{1}{\\sigma^2} x_i,  i \\in \\mathcal{I} \\\\ 0,  i \\notin \\mathcal{I} \\end{cases}\n$$\nwhere $\\mathcal{I}$ is the set of observation indices.\n\nThe action of the posterior precision operator $H$ on a vector $x$ is then simply the sum of the actions of its components:\n$$\nH x = Q x + (A^\\top \\Gamma^{-1} A) x\n$$\n\n**2. Computing the Posterior Covariance Action**\n\nThe task of computing the posterior covariance action $w = C_{\\text{post}} v = H^{-1} v$ is equivalent to solving the linear system $H w = v$. Since $H$ is symmetric and positive definite, the Conjugate Gradient (CG) method is an ideal choice. We use its preconditioned variant (PCG) to improve convergence.\n\nThe chosen preconditioner is the Jacobi (or diagonal) preconditioner, $M = \\operatorname{diag}(H)$. The inverse of the preconditioner, $M^{-1}$, is a diagonal matrix whose entries are the reciprocals of the diagonal entries of $H$. Its application to a vector is a simple element-wise division. The diagonal of $H$ is the sum of the diagonals of $Q$ and $A^\\top \\Gamma^{-1} A$.\nThe diagonal of $Q$ is given by:\n$$\n\\operatorname{diag}(Q)_i = \\lambda \\left( \\beta + \\frac{1}{h^2} \\right) \\text{ for } i \\in \\{0, n-1\\}, \\quad \\operatorname{diag}(Q)_i = \\lambda \\left( \\beta + \\frac{2}{h^2} \\right) \\text{ otherwise.}\n$$\nThe diagonal of $A^\\top \\Gamma^{-1} A$ is non-zero only at the observation indices:\n$$\n\\operatorname{diag}(A^\\top \\Gamma^{-1} A)_i = \\begin{cases} \\frac{1}{\\sigma^2},  i \\in \\mathcal{I} \\\\ 0,  i \\notin \\mathcal{I} \\end{cases}\n$$\nThe PCG algorithm iteratively refines a solution $w$ starting from an initial guess (e.g., $w_0=0$) by generating a sequence of search directions that are $H$-orthogonal. The algorithm terminates when the relative residual norm $\\|Hw - v\\|_2 / \\|v\\|_2$ falls below a specified tolerance of $10^{-8}$ or a maximum number of iterations ($200$) is reached. The final computed vector $w$ is the desired result $C_{\\text{post}}v$. The residual norm $\\|Hw - v\\|_2$ is computed as a diagnostic.\n\n**3. Approximate Posterior Sampling**\n\nThe second task is to generate an approximate sample $s$ from the posterior distribution $\\mathcal{N}(0, C_{\\text{post}})$. This is equivalent to computing $s \\approx C_{\\text{post}}^{1/2} z = H^{-1/2} z$, where $z \\sim \\mathcal{N}(0, I_n)$. We use the Lanczos method to approximate this matrix function-vector product.\n\nThe Lanczos algorithm is an iterative method that, for a symmetric operator $H$ and a starting vector $z$, constructs an orthonormal basis $\\{q_1, \\dots, q_k\\}$ for the Krylov subspace $\\mathcal{K}_k(H, z) = \\operatorname{span}\\{z, Hz, H^2z, \\dots, H^{k-1}z\\}$. In this basis, the operator $H$ is represented by a small $k \\times k$ symmetric tridiagonal matrix $T_k = Q_k^\\top H Q_k$.\n\nThe approximation relies on the property that for a function $f$, $f(H)z \\approx \\|z\\|_2 Q_k f(T_k) e_1$, where $e_1 = [1, 0, \\dots, 0]^\\top$. In our case, $f(\\cdot) = (\\cdot)^{-1/2}$.\nThe algorithm proceeds as follows:\n1.  Run $k$ iterations of the Lanczos algorithm on $H$ with starting vector $z$ to obtain the orthonormal basis matrix $Q_k = [q_1, \\dots, q_k]$ and the tridiagonal matrix $T_k$. No reorthogonalization is performed.\n2.  Compute the spectral decomposition of the small symmetric matrix $T_k = V \\Lambda V^\\top$, where $\\Lambda$ is a diagonal matrix of eigenvalues and $V$ is the orthogonal matrix of eigenvectors. This is efficiently done using specialized numerical routines like `scipy.linalg.eigh_tridiagonal`.\n3.  Compute $f(T_k) = V f(\\Lambda) V^\\top = V \\Lambda^{-1/2} V^\\top$.\n4.  The final approximation for the sample $s$ is calculated as:\n    $$\n    s \\approx \\|z\\|_2 \\, Q_k \\left( V \\Lambda^{-1/2} V^\\top \\right) e_1\n    $$\n    This computation is performed efficiently without forming full matrices, by calculating the action of each term on the vector $e_1$.\n\nThe quality of this approximation is assessed via the energy identity error, $\\varepsilon_{\\text{energy}} = \\left| s^\\top H s - \\|z\\|_2^2 \\right|$. For an exact sample $s = H^{-1/2} z$, we have $s^\\top H s = (H^{-1/2}z)^\\top H (H^{-1/2}z) = z^\\top H^{-1/2} H H^{-1/2} z = z^\\top z = \\|z\\|_2^2$. The error $\\varepsilon_{\\text{energy}}$ measures the deviation from this identity due to the Lanczos approximation.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import eigh_tridiagonal\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and produce the final output.\n    \"\"\"\n    test_cases = [\n        # Case 1: happy path\n        {'n': 50, 'm': 15, 'lam': 1.0, 'beta': 1.0, 'sigma': 0.2, 'k': 20, 'seed': 1},\n        # Case 2: high-noise boundary\n        {'n': 50, 'm': 25, 'lam': 1.0, 'beta': 0.5, 'sigma': 5.0, 'k': 25, 'seed': 2},\n        # Case 3: no-data edge\n        {'n': 50, 'm': 0, 'lam': 2.0, 'beta': 0.1, 'sigma': 1.0, 'k': 20, 'seed': 3},\n        # Case 4: low-noise, dense observations\n        {'n': 50, 'm': 40, 'lam': 1.0, 'beta': 1.0, 'sigma': 0.01, 'k': 30, 'seed': 4},\n    ]\n\n    # --- Matrix-Free Operator Implementations ---\n    \n    def apply_Q(x, n, h, lam, beta):\n        \"\"\"Matrix-free application of the prior precision operator Q.\"\"\"\n        h_sq = h**2\n        lap_x = np.zeros(n)\n        \n        # Interior points\n        if n  2:\n            lap_x[1:-1] = (2 * x[1:-1] - x[:-2] - x[2:]) / h_sq\n        \n        # Boundary points (Neumann)\n        if n  1:\n            lap_x[0] = (x[0] - x[1]) / h_sq\n            lap_x[-1] = (x[-1] - x[-2]) / h_sq\n        elif n == 1:\n            lap_x[0] = 0 # No neighbors\n            \n        return lam * (lap_x + beta * x)\n\n    def apply_A_T_Gamma_inv_A(x, obs_indices, sigma_sq_inv):\n        \"\"\"Matrix-free application of the data precision operator.\"\"\"\n        result = np.zeros_like(x)\n        if obs_indices.size  0:\n            result[obs_indices] = x[obs_indices] * sigma_sq_inv\n        return result\n\n    # --- Preconditioner  PCG ---\n\n    def compute_diag_H(n, h, lam, beta, obs_indices, sigma_sq_inv):\n        \"\"\"Computes the diagonal of the posterior precision operator H.\"\"\"\n        h_sq = h**2\n        \n        # Diagonal of Q\n        diag_Q = np.full(n, lam * (beta + 2.0 / h_sq))\n        if n  1:\n            diag_Q[0] = lam * (beta + 1.0 / h_sq)\n            diag_Q[-1] = lam * (beta + 1.0 / h_sq)\n        elif n == 1:\n             diag_Q[0] = lam * beta\n\n        # Diagonal of A^T*Gamma^-1*A\n        diag_A_part = np.zeros(n)\n        if obs_indices.size  0:\n            diag_A_part[obs_indices] = sigma_sq_inv\n            \n        return diag_Q + diag_A_part\n\n    def pcg(apply_A, b, apply_M_inv, tol=1e-8, maxiter=200):\n        \"\"\"Preconditioned Conjugate Gradients algorithm.\"\"\"\n        x = np.zeros_like(b)\n        r = b - apply_A(x)\n        if np.linalg.norm(b) == 0:\n            return x\n            \n        norm_b = np.linalg.norm(b)\n        if norm_b == 0: norm_b = 1.0 # Handle b=0 case\n\n        z = apply_M_inv(r)\n        p = z\n        rs_old = np.dot(r, z)\n\n        for i in range(maxiter):\n            Ap = apply_A(p)\n            alpha = rs_old / np.dot(p, Ap)\n            x += alpha * p\n            r -= alpha * Ap\n            \n            if np.linalg.norm(r) / norm_b  tol:\n                break\n                \n            z = apply_M_inv(r)\n            rs_new = np.dot(r, z)\n            p = z + (rs_new / rs_old) * p\n            rs_old = rs_new\n            \n        return x\n\n    # --- Lanczos Algorithm  Function Approximation ---\n    \n    def lanczos(apply_H, z, k):\n        \"\"\"Lanczos algorithm to generate T_k and Q_k.\"\"\"\n        n = len(z)\n        Q = np.zeros((n, k))\n        alphas = np.zeros(k)\n        betas_sub = np.zeros(k - 1)\n\n        z_norm = np.linalg.norm(z)\n        if z_norm == 0:\n            return alphas, betas_sub, Q, z_norm\n        \n        q = z / z_norm\n        \n        for j in range(k):\n            Q[:, j] = q\n            u = apply_H(q)\n            alphas[j] = np.dot(q, u)\n            \n            if j  k - 1:\n                u = u - alphas[j] * q\n                if j  0:\n                    u = u - betas_sub[j-1] * Q[:, j-1]\n                \n                beta_j = np.linalg.norm(u)\n                betas_sub[j] = beta_j\n                if beta_j  1e-14: # (Near) breakdown\n                    # Resize and return what we have\n                    return alphas[:j+1], betas_sub[:j], Q[:, :j+1], z_norm\n                q = u / beta_j\n                \n        return alphas, betas_sub, Q, z_norm\n\n    results = []\n    for case in test_cases:\n        n, m, lam, beta, sigma, k, seed = case.values()\n\n        # --- Setup for the current test case ---\n        h = 1.0 / (n - 1) if n  1 else 1.0\n        sigma_sq_inv = 1.0 / sigma**2\n        obs_indices = np.unique(np.linspace(0, n - 1, m, dtype=int)) if m  0 else np.array([], dtype=int)\n        \n        # Generate input vectors\n        v = np.sin(2 * np.pi * (np.arange(n) + 0.5) / n)\n        rng = np.random.default_rng(seed)\n        z = rng.normal(size=n)\n        \n        # Define operators for this case\n        def apply_H(x):\n            return apply_Q(x, n, h, lam, beta) + \\\n                   apply_A_T_Gamma_inv_A(x, obs_indices, sigma_sq_inv)\n                   \n        # --- Part 1: PCG for C_post * v ---\n        diag_H = compute_diag_H(n, h, lam, beta, obs_indices, sigma_sq_inv)\n        def apply_M_inv(r):\n            return r / diag_H\n            \n        w = pcg(apply_H, v, apply_M_inv, tol=1e-8, maxiter=200)\n        r_norm = np.linalg.norm(apply_H(w) - v)\n\n        # --- Part 2: Lanczos for approximate sample ---\n        alphas, betas_sub, Q_k, z_norm = lanczos(apply_H, z, k)\n        \n        # Adjust k if early breakdown occurred\n        actual_k = Q_k.shape[1]\n        \n        if actual_k  0:\n            # Eigendecomposition of T_k\n            eivals, eivecs = eigh_tridiagonal(alphas, betas_sub, tol=1e-15)\n        \n            # Compute H^{-1/2} z approximation via spectral mapping\n            f_eivals = 1.0 / np.sqrt(eivals)\n            y = eivecs @ (f_eivals * eivecs[0, :])\n            s = z_norm * (Q_k @ y)\n\n            # Compute energy error\n            sHs = np.dot(s, apply_H(s))\n            z_norm_sq = z_norm**2\n            eps_energy = np.abs(sHs - z_norm_sq)\n        else: # Handle z=0 case\n            s = np.zeros(n)\n            eps_energy = 0.0\n\n        results.append(f\"[{r_norm},{eps_energy}]\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3384561"}]}