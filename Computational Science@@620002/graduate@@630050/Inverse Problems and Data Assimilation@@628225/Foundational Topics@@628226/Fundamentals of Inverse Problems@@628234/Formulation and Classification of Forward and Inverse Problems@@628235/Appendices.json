{"hands_on_practices": [{"introduction": "The stability of a linear inverse problem is fundamentally linked to the properties of its forward operator. The condition number serves as a crucial metric, quantifying the worst-case amplification of noise from the data to the solution. This exercise guides you through calculating the condition number from an operator's singular values and interpreting its profound implications for the reliability of a naive inversion [@problem_id:3382306].", "problem": "Consider a linear forward model $y = A x$ between finite-dimensional Hilbert spaces endowed with the Euclidean norm, where $A:\\mathbb{R}^{3}\\to\\mathbb{R}^{3}$ is a bounded, invertible linear operator. The corresponding inverse problem is to recover $x$ from noisy data $y^{\\delta} = y + \\eta$, with an unknown perturbation $\\eta$. The singular value decomposition (SVD) of $A$ has singular values $\\sigma_{1}$, $\\sigma_{2}$, and $\\sigma_{3}$, ordered so that $\\sigma_{1} \\ge \\sigma_{2} \\ge \\sigma_{3} > 0$. Suppose that for this operator one has $\\sigma_{1} = 18$, $\\sigma_{2} = 5$, and $\\sigma_{3} = 0.09$.\n\nUsing only core definitions from linear operator theory on Euclidean spaces, compute the condition number of the forward operator with respect to the Euclidean norm. Then, interpret its epistemic meaning for the reliability of solutions to the inverse problem in terms of worst-case amplification of data perturbations by the data-to-solution map. Your final reported result must be the computed condition number as a single real number without units. No rounding is necessary.", "solution": "The user has provided a valid problem statement.\n\nThe problem asks for two things: first, the computation of the condition number of a linear operator $A$, and second, an interpretation of its meaning in the context of an associated inverse problem. The operator $A$ maps from $\\mathbb{R}^{3}$ to $\\mathbb{R}^{3}$, which are finite-dimensional Hilbert spaces when endowed with the standard inner product and the induced Euclidean norm, denoted as $\\|\\cdot\\|$. The Euclidean norm is equivalent to the $L^2$ norm, so we will use the notation $\\|\\cdot\\|_2$.\n\nThe condition number of a linear operator $A$ with respect to a specific norm, denoted $\\kappa(A)$, is defined as the product of the norm of the operator and the norm of its inverse:\n$$\n\\kappa(A) = \\|A\\| \\|A^{-1}\\|\n$$\nThe problem specifies the use of the Euclidean norm. For a matrix or linear operator $A$, the induced $2$-norm, $\\|A\\|_2$, is defined as:\n$$\n\\|A\\|_2 = \\sup_{x \\neq 0} \\frac{\\|Ax\\|_2}{\\|x\\|_2}\n$$\nA fundamental result from linear algebra states that the $2$-norm of an operator $A$ is equal to its largest singular value, $\\sigma_{\\max}$. The singular values of $A$ are the square roots of the eigenvalues of the self-adjoint operator $A^*A$, where $A^*$ is the adjoint of $A$.\n\nThe problem provides the singular values of $A$ as $\\sigma_{1} = 18$, $\\sigma_{2} = 5$, and $\\sigma_{3} = 0.09$, ordered such that $\\sigma_{1} \\ge \\sigma_{2} \\ge \\sigma_{3} > 0$. Therefore, the largest singular value is $\\sigma_{\\max} = \\sigma_{1} = 18$.\nThe $2$-norm of the operator $A$ is thus:\n$$\n\\|A\\|_2 = \\sigma_{1} = 18\n$$\nNext, we must find the norm of the inverse operator, $\\|A^{-1}\\|_2$. The operator $A$ is stated to be invertible, which is consistent with the fact that all its singular values are strictly positive ($\\sigma_3 = 0.09 > 0$). If the singular values of $A$ are $\\sigma_i$, then the singular values of its inverse, $A^{-1}$, are $\\sigma_i^{-1}$.\nThe norm of the inverse operator, $\\|A^{-1}\\|_2$, is equal to the largest singular value of $A^{-1}$. Since the singular values of $A$ are ordered $\\sigma_1 \\ge \\sigma_2 \\ge \\sigma_3 > 0$, their reciprocals will be ordered as $\\sigma_3^{-1} \\ge \\sigma_2^{-1} \\ge \\sigma_1^{-1} > 0$.\nTherefore, the largest singular value of $A^{-1}$ is $\\sigma_3^{-1}$.\n$$\n\\|A^{-1}\\|_2 = \\sigma_{\\max}(A^{-1}) = (\\sigma_{\\min}(A))^{-1} = \\frac{1}{\\sigma_3} = \\frac{1}{0.09}\n$$\nWe can now compute the condition number with respect to the $2$-norm, $\\kappa_2(A)$:\n$$\n\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2 = \\sigma_{1} \\cdot \\frac{1}{\\sigma_{3}} = \\frac{\\sigma_{1}}{\\sigma_{3}}\n$$\nSubstituting the given values:\n$$\n\\kappa_2(A) = \\frac{18}{0.09} = \\frac{18}{\\frac{9}{100}} = 18 \\cdot \\frac{100}{9} = 2 \\cdot 100 = 200\n$$\n\nThe second part of the problem requires an interpretation of this result for the reliability of solutions to the inverse problem. The inverse problem is to find $x$ given noisy data $y^{\\delta}$. The \"true\" noise-free data is $y = Ax$, and the corresponding true solution is $x = A^{-1}y$. The measured data is $y^{\\delta} = y + \\eta$, where $\\eta$ is an unknown perturbation. A naive solution to the inverse problem would be to directly apply the inverse operator to the noisy data:\n$$\nx^{\\delta} = A^{-1}y^{\\delta} = A^{-1}(y + \\eta) = A^{-1}y + A^{-1}\\eta = x + A^{-1}\\eta\n$$\nThe error in the solution is $\\Delta x = x^{\\delta} - x = A^{-1}\\eta$. We are interested in how the relative error in the data, $\\frac{\\|\\eta\\|_2}{\\|y\\|_2}$, propagates to the relative error in the solution, $\\frac{\\|\\Delta x\\|_2}{\\|x\\|_2}$.\nWe have the following inequalities:\n$$\n\\|\\Delta x\\|_2 = \\|A^{-1}\\eta\\|_2 \\le \\|A^{-1}\\|_2 \\|\\eta\\|_2\n$$\nAnd from the forward model $y=Ax$:\n$$\n\\|y\\|_2 = \\|Ax\\|_2 \\le \\|A\\|_2 \\|x\\|_2 \\implies \\|x\\|_2 \\ge \\frac{\\|y\\|_2}{\\|A\\|_2}\n$$\nCombining these two inequalities, we can bound the relative error in the solution:\n$$\n\\frac{\\|\\Delta x\\|_2}{\\|x\\|_2} \\le \\frac{\\|A^{-1}\\|_2 \\|\\eta\\|_2}{\\frac{\\|y\\|_2}{\\|A\\|_2}} = \\|A\\|_2 \\|A^{-1}\\|_2 \\frac{\\|\\eta\\|_2}{\\|y\\|_2} = \\kappa_2(A) \\frac{\\|\\eta\\|_2}{\\|y\\|_2}\n$$\nThis inequality demonstrates the epistemic meaning of the condition number. It represents the worst-case amplification factor for the relative error. Specifically, the relative error in the computed solution can be up to $\\kappa_2(A)$ times the relative error in the measurement data.\nIn this case, $\\kappa_2(A) = 200$. This indicates that the inverse problem is ill-conditioned. A value of $200$ means that a small relative error in the data can be amplified by a factor of up to $200$ in the solution. For instance, if the measurement data $y^{\\delta}$ has a relative noise level of $1\\%$ (i.e., $\\frac{\\|\\eta\\|_2}{\\|y\\|_2} = 0.01$), the relative error in the solution $\\frac{\\|\\Delta x\\|_2}{\\|x\\|_2}$ could be as large as $200 \\times 0.01 = 2$, which corresponds to a $200\\%$ error. This signifies that the naive solution $x^{\\delta} = A^{-1}y^{\\delta}$ is highly unreliable and potentially meaningless, as the error can be larger than the solution itself. Such a high condition number necessitates the use of regularization techniques to obtain a stable and meaningful approximate solution to the inverse problem.", "answer": "$$\\boxed{200}$$", "id": "3382306"}, {"introduction": "We now transition from discrete matrices to continuous operators, exploring deconvolution as a classic example of an ill-posed problem. Using the powerful lens of Fourier analysis, this practice reveals how a smoothing forward operator leads to an unbounded inverse, which catastrophically amplifies high-frequency noise. You will also see how restricting the problem to a band-limited space provides a form of regularization, making the problem solvable, albeit with a finite amplification factor [@problem_id:3382283].", "problem": "Consider the linear time-invariant forward model given by convolution, where the data $y(t)$ is generated from a latent signal $x(t)$ via $y(t) = (k * x)(t)$ with smoothing kernel $k(t)$. Suppose $k(t)$ is the Gaussian kernel $k(t) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\big(-\\frac{t^{2}}{2\\sigma^{2}}\\big)$ for a fixed $\\sigma > 0$. The inverse problem seeks to recover $x(t)$ from noisy measurements $y_{\\delta}(t) = y(t) + \\delta y(t)$, where $\\delta y \\in L^{2}(\\mathbb{R})$ is a small perturbation. Let $\\widehat{f}(\\omega)$ denote the Fourier transform defined by $\\widehat{f}(\\omega) = \\int_{-\\infty}^{\\infty} f(t)\\,\\exp(-\\mathrm{i}\\,\\omega\\,t)\\,\\mathrm{d}t$. Consider the band-limited subspace $\\mathcal{S}_{\\Omega} = \\{f \\in L^{2}(\\mathbb{R}) : \\widehat{f}(\\omega) = 0 \\text{ for } |\\omega| > \\Omega\\}$ for some fixed cutoff $\\Omega > 0$.\n\nYour tasks:\n- Formulate the forward and inverse problems in this setting and classify them within the framework of Hadamard well-posedness (existence, uniqueness, and stability) for the operator that maps $x$ to $y$.\n- On the subspace $\\mathcal{S}_{\\Omega}$, compute the $L^{2}$-operator norm of the deconvolution map that sends a data perturbation $\\delta y \\in \\mathcal{S}_{\\Omega}$ to the corresponding reconstruction perturbation $\\delta x$ obtained by exact inversion of the convolution model. Interpret this norm as the worst-case amplification factor of perturbations from data space to solution space.\n\nExpress the final answer as a closed-form analytic expression in terms of $\\sigma$ and $\\Omega$. No rounding is required, and no physical units are involved.", "solution": "The user has provided a problem from the field of inverse problems, specifically concerning the analysis of a deconvolution problem. The tasks are to classify the problem's well-posedness and to compute the operator norm of the inverse map on a restricted function space.\n\n### Step 1: Problem Formulation and Classification\n\nThe forward problem describes how a latent signal $x(t) \\in L^{2}(\\mathbb{R})$ is transformed into observed data $y(t) \\in L^{2}(\\mathbb{R})$. The transformation is a convolution with a Gaussian kernel $k(t)$:\n$$y(t) = (k * x)(t) = \\int_{-\\infty}^{\\infty} k(t-\\tau)x(\\tau)\\,\\mathrm{d}\\tau$$\nwhere $k(t) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\big(-\\frac{t^{2}}{2\\sigma^{2}}\\big)$ for a fixed $\\sigma > 0$.\n\nWe can define a linear forward operator $A: L^{2}(\\mathbb{R}) \\to L^{2}(\\mathbb{R})$ such that $y = A(x)$. The inverse problem is to recover $x$ given data $y$. In practice, we have noisy data $y_{\\delta} = y + \\delta y$, and we seek to recover $x$ from $y_{\\delta}$.\n\nTo analyze the properties of the operator $A$, it is most convenient to work in the Fourier domain. Let $\\widehat{f}(\\omega)$ denote the Fourier transform of a function $f(t)$. Using the convolution theorem, the forward model becomes:\n$$\\widehat{y}(\\omega) = \\widehat{k}(\\omega)\\,\\widehat{x}(\\omega)$$\nThe Fourier transform of the Gaussian kernel $k(t)$ is a standard result:\n$$\\widehat{k}(\\omega) = \\exp\\left(-\\frac{\\sigma^{2}\\omega^{2}}{2}\\right)$$\nThus, the forward problem in the frequency domain is:\n$$\\widehat{y}(\\omega) = \\exp\\left(-\\frac{\\sigma^{2}\\omega^{2}}{2}\\right)\\widehat{x}(\\omega)$$\n\nWe now assess the forward problem against Hadamard's criteria for well-posedness:\n1.  **Existence**: A solution $y$ must exist for any $x \\in L^{2}(\\mathbb{R})$. Using Plancherel's theorem, the $L^{2}$-norm of $y$ is related to the $L^{2}$-norm of its Fourier transform:\n    $$\\|y\\|_{L^{2}}^{2} = \\frac{1}{2\\pi}\\|\\widehat{y}\\|_{L^{2}}^{2} = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} |\\widehat{y}(\\omega)|^{2}\\,\\mathrm{d}\\omega = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} \\left|\\exp\\left(-\\frac{\\sigma^{2}\\omega^{2}}{2}\\right)\\widehat{x}(\\omega)\\right|^{2}\\,\\mathrm{d}\\omega$$\n    $$\\|y\\|_{L^{2}}^{2} = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} \\exp(-\\sigma^{2}\\omega^{2})|\\widehat{x}(\\omega)|^{2}\\,\\mathrm{d}\\omega$$\n    Since $\\sigma > 0$, the term $\\exp(-\\sigma^{2}\\omega^{2}) \\le 1$ for all $\\omega \\in \\mathbb{R}$. Therefore:\n    $$\\|y\\|_{L^{2}}^{2} \\le \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} |\\widehat{x}(\\omega)|^{2}\\,\\mathrm{d}\\omega = \\|x\\|_{L^{2}}^{2}$$\n    If $x \\in L^{2}(\\mathbb{R})$, then $\\|x\\|_{L^{2}} < \\infty$, which implies $\\|y\\|_{L^{2}} < \\infty$. So, $y \\in L^{2}(\\mathbb{R})$ and a solution exists.\n\n2.  **Uniqueness**: For any given $y$, the corresponding $x$ must be unique. From the Fourier domain relation, if $A(x_{1}) = A(x_{2}) = y$, then $\\widehat{k}(\\omega)\\widehat{x_{1}}(\\omega) = \\widehat{k}(\\omega)\\widehat{x_{2}}(\\omega)$. This implies $\\widehat{k}(\\omega)(\\widehat{x_{1}}(\\omega) - \\widehat{x_{2}}(\\omega)) = 0$. The term $\\widehat{k}(\\omega) = \\exp(-\\frac{\\sigma^{2}\\omega^{2}}{2})$ is strictly positive for all finite $\\omega \\in \\mathbb{R}$. Thus, we must have $\\widehat{x_{1}}(\\omega) - \\widehat{x_{2}}(\\omega) = 0$ for all $\\omega$. By the injectivity of the Fourier transform, this means $x_{1}(t) - x_{2}(t) = 0$, so $x_{1} = x_{2}$. The solution is unique.\n\n3.  **Stability**: The solution $x$ must depend continuously on the data $y$. This means the inverse operator $A^{-1}$ must be a continuous (bounded) operator. The inverse problem is formally solved by:\n    $$\\widehat{x}(\\omega) = \\frac{1}{\\widehat{k}(\\omega)}\\widehat{y}(\\omega) = \\exp\\left(\\frac{\\sigma^{2}\\omega^{2}}{2}\\right)\\widehat{y}(\\omega)$$\n    Let's consider the effect of a small perturbation $\\delta y$ in the data, leading to a perturbation $\\delta x$ in the solution: $\\widehat{\\delta x}(\\omega) = \\exp(\\frac{\\sigma^{2}\\omega^{2}}{2})\\widehat{\\delta y}(\\omega)$. The amplification factor $\\exp(\\frac{\\sigma^{2}\\omega^{2}}{2})$ grows without bound as the frequency $|\\omega| \\to \\infty$. This means that high-frequency components of any noise $\\delta y$, even if they have very small amplitude, will be amplified to arbitrarily large magnitudes in the reconstruction $\\delta x$. The inverse operator $A^{-1}$ is therefore unbounded. The solution does not depend continuously on the data.\n\n**Conclusion on Classification**: The forward problem satisfies existence and uniqueness but fails the stability criterion. Therefore, the problem is **ill-posed** in the sense of Hadamard.\n\n### Step 2: Operator Norm on the Subspace $\\mathcal{S}_{\\Omega}$\n\nWe are asked to compute the $L^{2}$-operator norm of the deconvolution map, which we denote as $A^{-1}$, restricted to the subspace $\\mathcal{S}_{\\Omega}$. This subspace consists of functions whose Fourier transforms vanish for frequencies $|\\omega| > \\Omega$. The operator maps a data perturbation $\\delta y \\in \\mathcal{S}_{\\Omega}$ to a solution perturbation $\\delta x = A^{-1}(\\delta y)$. The operator norm is defined as:\n$$\\|A^{-1}\\|_{\\mathcal{S}_{\\Omega}} = \\sup_{\\substack{\\delta y \\in \\mathcal{S}_{\\Omega} \\\\ \\delta y \\neq 0}} \\frac{\\|\\delta x\\|_{L^{2}}}{\\|\\delta y\\|_{L^{2}}} = \\sup_{\\substack{\\delta y \\in \\mathcal{S}_{\\Omega} \\\\ \\delta y \\neq 0}} \\frac{\\|A^{-1}(\\delta y)\\|_{L^{2}}}{\\|\\delta y\\|_{L^{2}}}$$\nWe will again use Plancherel's theorem to express the norms in the Fourier domain:\n$$\\|\\delta x\\|_{L^{2}}^{2} = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} |\\widehat{\\delta x}(\\omega)|^{2}\\,\\mathrm{d}\\omega = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} \\left|\\exp\\left(\\frac{\\sigma^{2}\\omega^{2}}{2}\\right)\\widehat{\\delta y}(\\omega)\\right|^{2}\\,\\mathrm{d}\\omega = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} \\exp(\\sigma^{2}\\omega^{2})|\\widehat{\\delta y}(\\omega)|^{2}\\,\\mathrm{d}\\omega$$\nThe condition $\\delta y \\in \\mathcal{S}_{\\Omega}$ implies that $\\widehat{\\delta y}(\\omega) = 0$ for all $|\\omega| > \\Omega$. Consequently, the integrals can be restricted to the interval $[-\\Omega, \\Omega]$:\n$$\\|\\delta x\\|_{L^{2}}^{2} = \\frac{1}{2\\pi}\\int_{-\\Omega}^{\\Omega} \\exp(\\sigma^{2}\\omega^{2})|\\widehat{\\delta y}(\\omega)|^{2}\\,\\mathrm{d}\\omega$$\nSimilarly, for $\\delta y$:\n$$\\|\\delta y\\|_{L^{2}}^{2} = \\frac{1}{2\\pi}\\int_{-\\Omega}^{\\Omega} |\\widehat{\\delta y}(\\omega)|^{2}\\,\\mathrm{d}\\omega$$\nWe need to find the supremum of the ratio $\\frac{\\|\\delta x\\|_{L^{2}}}{\\|\\delta y\\|_{L^{2}}}$, which is equivalent to finding the supremum of its square:\n$$\\left(\\frac{\\|\\delta x\\|_{L^{2}}}{\\|\\delta y\\|_{L^{2}}}\\right)^{2} = \\frac{\\int_{-\\Omega}^{\\Omega} \\exp(\\sigma^{2}\\omega^{2})|\\widehat{\\delta y}(\\omega)|^{2}\\,\\mathrm{d}\\omega}{\\int_{-\\Omega}^{\\Omega} |\\widehat{\\delta y}(\\omega)|^{2}\\,\\mathrm{d}\\omega}$$\nLet $g(\\omega) = |\\widehat{\\delta y}(\\omega)|^{2}$. The expression is a weighted average of the function $\\exp(\\sigma^{2}\\omega^{2})$ over the interval $[-\\Omega, \\Omega]$, with the non-negative weight function $g(\\omega)$. The supremum of such a ratio is the supremum of the function itself over the interval of integration.\n$$\\sup_{g \\ge 0, g \\not\\equiv 0} \\frac{\\int_{-\\Omega}^{\\Omega} \\exp(\\sigma^{2}\\omega^{2})g(\\omega)\\,\\mathrm{d}\\omega}{\\int_{-\\Omega}^{\\Omega} g(\\omega)\\,\\mathrm{d}\\omega} = \\sup_{\\omega \\in [-\\Omega, \\Omega]} \\exp(\\sigma^{2}\\omega^{2})$$\nThe function $f(\\omega) = \\exp(\\sigma^{2}\\omega^{2})$ is an even function of $\\omega$ and is strictly increasing for $\\omega > 0$. Its maximum value on the compact interval $[-\\Omega, \\Omega]$ occurs at the endpoints, $\\omega = \\pm\\Omega$.\nThe maximum value is $\\exp(\\sigma^{2}\\Omega^{2})$.\nTherefore:\n$$\\sup_{\\substack{\\delta y \\in \\mathcal{S}_{\\Omega} \\\\ \\delta y \\neq 0}} \\left(\\frac{\\|\\delta x\\|_{L^{2}}}{\\|\\delta y\\|_{L^{2}}}\\right)^{2} = \\exp(\\sigma^{2}\\Omega^{2})$$\nTaking the square root gives the operator norm:\n$$\\|A^{-1}\\|_{\\mathcal{S}_{\\Omega}} = \\sqrt{\\exp(\\sigma^{2}\\Omega^{2})} = \\exp\\left(\\frac{\\sigma^{2}\\Omega^{2}}{2}\\right)$$\nThis norm represents the worst-case amplification factor. For any perturbation $\\delta y$ containing only frequencies up to $\\Omega$, the corresponding error $\\delta x$ in the reconstructed signal is amplified by at most a factor of $\\exp(\\frac{\\sigma^{2}\\Omega^{2}}{2})$. By restricting the frequency content of the noise, the ill-posed problem is regularized, and the amplification of error becomes finite, although it can still be very large if $\\sigma$ or $\\Omega$ are large. This value is the condition number of the truncated inverse problem.", "answer": "$$\\boxed{\\exp\\left(\\frac{\\sigma^{2}\\Omega^{2}}{2}\\right)}$$", "id": "3382283"}, {"introduction": "In practical applications, how can we determine if our noisy data permits a stable solution? The Discrete Picard Condition offers a powerful diagnostic tool for just this purpose. This exercise provides a scenario where you will compare the decay rate of data coefficients against the operator's singular values to distinguish between signal-dominated and noise-dominated components, ultimately assessing the feasibility of a non-regularized solution [@problem_id:3382319].", "problem": "Consider a linear forward model $y = A x_{\\text{true}} + e$ where $A \\in \\mathbb{R}^{m \\times n}$ is a compact operator arising from a linearized observation operator in data assimilation, $x_{\\text{true}} \\in \\mathbb{R}^{n}$ is the unknown state, $y \\in \\mathbb{R}^{m}$ is the data, and $e \\in \\mathbb{R}^{m}$ is additive measurement noise. Let $A = U \\Sigma V^\\top$ denote the Singular Value Decomposition (SVD), with singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\cdots \\ge \\sigma_r > 0$ where $r = \\operatorname{rank}(A)$, and left and right singular vectors $\\{u_i\\}_{i=1}^r$ and $\\{v_i\\}_{i=1}^r$.\n\nSuppose $m = n = 200$ and the singular values of $A$ are known to follow a polynomial decay $\\sigma_i \\approx i^{-2}$ for $i = 1,2,\\ldots,200$. A measurement campaign produces $y$ such that the empirical coefficients $|u_i^\\top y|$ satisfy the following behavior:\n- For $1 \\le i \\le 30$, $|u_i^\\top y| \\approx 0.5\\, i^{-3}$.\n- For $31 \\le i \\le 200$, $|u_i^\\top y| \\approx 2 \\times 10^{-5}$ (a noise floor, independent of $i$ within the measurement precision).\n\nYou are asked to assess the Picard condition by comparing the decay of $|u_i^\\top y|$ with the decay of $\\sigma_i$, and from this comparison deduce whether solving the naive least squares (LS) problem $\\min_x \\|A x - y\\|_2^2$ without regularization is feasible in the sense of producing a stable estimate of $x_{\\text{true}}$ from the noisy data.\n\nWhich of the following statements is most accurate?\n\nA. The Picard condition holds for all $i$ because $|u_i^\\top y|$ decays faster than $\\sigma_i$, so naive LS without regularization yields a stable solution.\n\nB. The Picard condition holds only for the low-index range where the data coefficients decay ($i \\le 30$), but is violated in the noise-dominated tail ($i \\ge 31$) where $|u_i^\\top y|$ stops decaying relative to $\\sigma_i$; consequently, naive LS amplifies noise and is not feasible without regularization.\n\nC. The Picard condition compares $|u_i^\\top y|$ to $\\sigma_i^2$, and because $|u_i^\\top y|$ decays faster than $\\sigma_i^2$, naive LS is stable.\n\nD. The decay of $|u_i^\\top y|$ is irrelevant to stability; feasibility of naive LS depends only on the behavior of $|v_i^\\top x_{\\text{true}}|$, so naive LS is feasible in this setting.", "solution": "The problem statement is a valid and well-posed scenario in the field of inverse problems and data assimilation. It asks for an assessment of the stability of a naive least squares solution based on the decay properties of singular values and measured data coefficients.\n\nWe begin by recalling the formal solution to the linear least squares problem $\\min_x \\|A x - y\\|_2^2$. For a matrix $A$ with Singular Value Decomposition (SVD) $A = U \\Sigma V^\\top$, where $U$ and $V$ are orthogonal matrices and $\\Sigma$ is a diagonal matrix of singular values $\\sigma_i$, the solution is given by $x_{\\text{LS}} = A^{\\dagger}y$, where $A^{\\dagger} = V \\Sigma^{\\dagger} U^\\top$ is the Moore-Penrose pseudoinverse. Since the problem specifies $m = n = 200$ and provides singular values for all $i$ from $1$ to $200$, we assume the matrix $A$ has full rank, so $A^{\\dagger} = A^{-1} = V \\Sigma^{-1} U^\\top$.\n\nThe solution can be written as a sum over the singular components:\n$$\nx_{\\text{LS}} = \\sum_{i=1}^{n} \\frac{u_i^\\top y}{\\sigma_i} v_i\n$$\nwhere $\\{u_i\\}$ and $\\{v_i\\}$ are the columns of $U$ and $V$ respectively (the left and right singular vectors).\n\nThe data $y$ is given by $y = A x_{\\text{true}} + e$, where $x_{\\text{true}}$ is the true unknown state and $e$ is measurement noise. Substituting this into the solution formula yields:\n$$\nx_{\\text{LS}} = \\sum_{i=1}^{n} \\frac{u_i^\\top (A x_{\\text{true}} + e)}{\\sigma_i} v_i\n$$\nWe can split this into two parts. The first part involves the true signal:\n$u_i^\\top A x_{\\text{true}} = u_i^\\top (U \\Sigma V^\\top) x_{\\text{true}} = (\\text{row } i \\text{ of } U^\\top)(U \\Sigma V^\\top) x_{\\text{true}} = e_i^\\top \\Sigma V^\\top x_{\\text{true}} = \\sigma_i v_i^\\top x_{\\text{true}}$.\nHere, $e_i$ is the $i$-th standard basis vector.\nThe second part is the projection of the noise: $u_i^\\top e$.\nThus, the solution becomes:\n$$\nx_{\\text{LS}} = \\sum_{i=1}^{n} \\frac{\\sigma_i v_i^\\top x_{\\text{true}} + u_i^\\top e}{\\sigma_i} v_i = \\sum_{i=1}^{n} (v_i^\\top x_{\\text{true}}) v_i + \\sum_{i=1}^{n} \\frac{u_i^\\top e}{\\sigma_i} v_i\n$$\nThe first summation, $\\sum_{i=1}^{n} (v_i^\\top x_{\\text{true}}) v_i$, is the projection of $x_{\\text{true}}$ onto the basis $\\{v_i\\}$, which reconstructs $x_{\\text{true}}$ itself. Therefore:\n$$\nx_{\\text{LS}} = x_{\\text{true}} + \\sum_{i=1}^{n} \\frac{u_i^\\top e}{\\sigma_i} v_i\n$$\nThe stability of the solution is determined by the magnitude of the error term, $\\sum_{i=1}^{n} \\frac{u_i^\\top e}{\\sigma_i} v_i$. The problem is ill-posed because the singular values $\\sigma_i$ decay to zero. If the noise coefficients $u_i^\\top e$ do not decay at least as fast, the ratio $\\frac{u_i^\\top e}{\\sigma_i}$ will be large for large $i$, leading to the amplification of noise and an unstable solution.\n\nThe Discrete Picard Condition provides a practical way to diagnose this instability. It requires that for a stable reconstruction, the coefficients $|u_i^\\top y|$ must decay faster than the singular values $\\sigma_i$. If this condition is violated, it indicates that the noise component $u_i^\\top e$ is dominating the signal component $\\sigma_i v_i^\\top x_{\\text{true}}$, and the division by the small $\\sigma_i$ will amplify this noise.\n\nLet's analyze the given data with this condition in mind:\nGiven: $\\sigma_i \\approx i^{-2}$.\n\nCase 1: $1 \\le i \\le 30$\nWe have $|u_i^\\top y| \\approx 0.5\\, i^{-3}$.\nLet's examine the ratio that appears in the solution sum:\n$$\n\\frac{|u_i^\\top y|}{\\sigma_i} \\approx \\frac{0.5\\, i^{-3}}{i^{-2}} = 0.5\\, i^{-1}\n$$\nIn this range of indices, the ratio decays. The coefficients of the solution expansion are well-behaved. This suggests that for $i \\le 30$, the data is dominated by the true signal, and the Picard condition is satisfied.\n\nCase 2: $31 \\le i \\le 200$\nWe have $|u_i^\\top y| \\approx 2 \\times 10^{-5}$, which is a constant noise floor.\nNow, the ratio becomes:\n$$\n\\frac{|u_i^\\top y|}{\\sigma_i} \\approx \\frac{2 \\times 10^{-5}}{i^{-2}} = (2 \\times 10^{-5}) i^2\n$$\nIn this range, the ratio *grows* quadratically with $i$. This is a clear violation of the Picard condition. The constant value of $|u_i^\\top y|$ indicates that any underlying signal component has fallen below the noise level, so $|u_i^\\top y| \\approx |u_i^\\top e|$. The division by the rapidly decaying $\\sigma_i$ will cause extreme amplification of these noise components in the solution $x_{\\text{LS}}$.\n\nConsequently, the naive least squares solution, which sums over all $i$ up to $200$, will be dominated by large, erroneous, high-frequency components coming from the $i \\ge 31$ terms. This makes the solution highly unstable and physically meaningless. Regularization techniques (such as Truncated SVD, where one would cut off the sum at $i \\approx 30$) are required to obtain a stable and meaningful estimate of $x_{\\text{true}}$.\n\nNow we evaluate the given options:\n\nA. The Picard condition holds for all $i$ because $|u_i^\\top y|$ decays faster than $\\sigma_i$, so naive LS without regularization yields a stable solution.\nThis is **Incorrect**. The statement that $|u_i^\\top y|$ decays faster than $\\sigma_i$ is only true for $i \\le 30$. For $i \\ge 31$, $|u_i^\\top y|$ is approximately constant, hence decaying much slower than $\\sigma_i$. The conclusion that naive LS is stable is false.\n\nB. The Picard condition holds only for the low-index range where the data coefficients decay ($i \\le 30$), but is violated in the noise-dominated tail ($i \\ge 31$) where $|u_i^\\top y|$ stops decaying relative to $\\sigma_i$; consequently, naive LS amplifies noise and is not feasible without regularization.\nThis is **Correct**. This statement accurately describes the situation derived from our analysis. It correctly identifies the two regimes, correctly assesses the Picard condition in each, and draws the correct conclusion about the feasibility of the naive LS solution.\n\nC. The Picard condition compares $|u_i^\\top y|$ to $\\sigma_i^2$, and because $|u_i^\\top y|$ decays faster than $\\sigma_i^2$, naive LS is stable.\nThis is **Incorrect**. The stability of the LS solution $x_{\\text{LS}} = \\sum (u_i^\\top y / \\sigma_i) v_i$ depends on the behavior of the coefficients $u_i^\\top y / \\sigma_i$. Thus, the relevant comparison is between $|u_i^\\top y|$ and $\\sigma_i$, not $\\sigma_i^2$. The fundamental premise of the comparison is wrong.\n\nD. The decay of $|u_i^\\top y|$ is irrelevant to stability; feasibility of naive LS depends only on the behavior of $|v_i^\\top x_{\\text{true}}|$, so naive LS is feasible in this setting.\nThis is **Incorrect**. The stability of the solution computed from noisy data $y$ is critically dependent on the properties of $y$, which includes the noise $e$. As shown in the derivation of the error term, the instability arises directly from the amplification of noise coefficients $u_i^\\top e$, which are a component of $u_i^\\top y$. Claiming that the decay of $|u_i^\\top y|$ is irrelevant is a fundamental misunderstanding of how noise affects inverse problems. While the properties of $x_{\\text{true}}$ (reflected in $|v_i^\\top x_{\\text{true}}|$) determine the signal component, it is the interplay between signal, noise, and the operator's singular values that dictates stability.", "answer": "$$\\boxed{B}$$", "id": "3382319"}]}