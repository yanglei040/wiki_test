{"hands_on_practices": [{"introduction": "The Jacobian matrix, which contains the sensitivities of model outputs to parameter changes, is the cornerstone of local identifiability analysis. A powerful, modern approach is to analyze its structure using Singular Value Decomposition (SVD), which reveals combinations of parameters that are difficult or impossible to estimate independently. This computational exercise [@problem_id:3426732] provides direct, hands-on experience in implementing this technique, allowing you to diagnose how different experimental conditions create \"sloppy\" or unidentifiable directions in the parameter space.", "problem": "You are given a parametric measurement model in a data assimilation setting, where the scalar observation at time $t$ is modeled by $y(t) = \\theta_1 e^{-\\theta_2 t} + \\theta_3$, with the parameter vector $\\theta = (\\theta_1, \\theta_2, \\theta_3) \\in \\mathbb{R}^3$. The goal is to perform a local, theoretical identifiability analysis around a nominal parameter vector by computing the Jacobian matrix and analyzing its singular structure.\n\nUse the following fundamental base:\n- The Jacobian matrix $J(\\theta)$ of a vector-valued function with respect to its parameters is defined entrywise by $J_{k,i}(\\theta) = \\frac{\\partial y(t_k)}{\\partial \\theta_i}$, where $\\{t_k\\}_{k=1}^m$ is a sampling schedule.\n- Local identifiability at a nominal parameter vector is implied by the Jacobian $J(\\theta)$ having full column rank at that vector; near-non-identifiable directions are associated with small singular values in the Singular Value Decomposition (SVD) of $J(\\theta)$.\n\nYour task:\n1. Starting from the definitions above and the model $y(t) = \\theta_1 e^{-\\theta_2 t} + \\theta_3$, derive the Jacobian $J(\\theta)$ for a given sampling schedule $\\{t_k\\}$, evaluated at a nominal parameter vector $\\theta$.\n2. Compute the SVD $J(\\theta) = U \\Sigma V^\\top$ and determine near-null directions as those right singular vectors corresponding to singular values $\\sigma_j$ satisfying $\\sigma_j \\leq \\tau \\, \\sigma_{\\max}$, where $\\sigma_{\\max}$ is the largest singular value and $\\tau = 10^{-3}$ (dimensionless).\n3. For each test case defined below, compute the integer count of near-null directions (i.e., the number of singular values classified as near-null as per the threshold above). The time variable $t$ must be interpreted in seconds and provided in $\\mathrm{s}$. No angles are involved. No physical units are required in the output.\n\nTest suite:\n- Case $1$ (general case): $\\theta = (2.0,\\,0.5,\\,0.1)$, sampling schedule $t = [0.0,\\,0.5,\\,1.0,\\,1.5,\\,2.0]\\,\\mathrm{s}$.\n- Case $2$ (degeneracy at $t=0$): $\\theta = (1.5,\\,0.4,\\,-0.2)$, sampling schedule $t = [0.0,\\,0.0,\\,0.0]\\,\\mathrm{s}$.\n- Case $3$ (fast decay, late sampling): $\\theta = (1.0,\\,5.0,\\,0.0)$, sampling schedule $t = [2.0,\\,3.0,\\,4.0,\\,5.0]\\,\\mathrm{s}$.\n- Case $4$ (minimal sampling, well-conditioned): $\\theta = (1.0,\\,0.7,\\,0.2)$, sampling schedule $t = [0.2,\\,0.8,\\,1.7]\\,\\mathrm{s}$.\n\nFinal output specification:\n- Your program should compute the count of near-null directions for each test case and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of Cases $1$ through $4$ (for example, $[n_1,n_2,n_3,n_4]$). The entries $n_i$ must be integers.", "solution": "The problem requires a local, theoretical identifiability analysis for a given parametric model. The analysis is to be performed by computing the Jacobian matrix of a set of measurements with respect to the model parameters and then analyzing its singular value spectrum. A valid problem statement is provided, and the solution proceeds as follows.\n\n### Step 1: Derivation of the Jacobian Matrix\n\nThe measurement model is given as a scalar function of time $t$ and a parameter vector $\\theta = (\\theta_1, \\theta_2, \\theta_3) \\in \\mathbb{R}^3$:\n$$ y(t) = \\theta_1 e^{-\\theta_2 t} + \\theta_3 $$\nWe consider a set of $m$ measurements taken at discrete time points specified by the sampling schedule $\\{t_k\\}_{k=1}^m$. The measurement at time $t_k$ is $y(t_k)$.\n\nThe Jacobian matrix $J(\\theta)$ quantifies the sensitivity of the measurements to infinitesimal changes in the parameters. Its entries are defined as $J_{k,i}(\\theta) = \\frac{\\partial y(t_k)}{\\partial \\theta_i}$. The matrix has $m$ rows (one for each time sample) and $3$ columns (one for each parameter).\n\nTo construct the Jacobian, we compute the partial derivatives of $y(t)$ with respect to each parameter $\\theta_i$:\n\n1.  **Partial derivative with respect to $\\theta_1$**:\n    $$ \\frac{\\partial y(t)}{\\partial \\theta_1} = \\frac{\\partial}{\\partial \\theta_1} \\left( \\theta_1 e^{-\\theta_2 t} + \\theta_3 \\right) = e^{-\\theta_2 t} $$\n\n2.  **Partial derivative with respect to $\\theta_2$**:\n    $$ \\frac{\\partial y(t)}{\\partial \\theta_2} = \\frac{\\partial}{\\partial \\theta_2} \\left( \\theta_1 e^{-\\theta_2 t} + \\theta_3 \\right) = \\theta_1 \\frac{\\partial}{\\partial \\theta_2} \\left( e^{-\\theta_2 t} \\right) = \\theta_1 ( -t e^{-\\theta_2 t} ) = -t \\theta_1 e^{-\\theta_2 t} $$\n\n3.  **Partial derivative with respect to $\\theta_3$**:\n    $$ \\frac{\\partial y(t)}{\\partial \\theta_3} = \\frac{\\partial}{\\partial \\theta_3} \\left( \\theta_1 e^{-\\theta_2 t} + \\theta_3 \\right) = 1 $$\n\nAssembling these partial derivatives, the $k$-th row of the Jacobian matrix, corresponding to the time sample $t_k$, is given by the vector $\\left[ e^{-\\theta_2 t_k}, -t_k \\theta_1 e^{-\\theta_2 t_k}, 1 \\right]$. The full $m \\times 3$ Jacobian matrix $J(\\theta)$ is therefore:\n$$\nJ(\\theta) =\n\\begin{pmatrix}\n\\frac{\\partial y(t_1)}{\\partial \\theta_1} & \\frac{\\partial y(t_1)}{\\partial \\theta_2} & \\frac{\\partial y(t_1)}{\\partial \\theta_3} \\\\\n\\frac{\\partial y(t_2)}{\\partial \\theta_1} & \\frac{\\partial y(t_2)}{\\partial \\theta_2} & \\frac{\\partial y(t_2)}{\\partial \\theta_3} \\\\\n\\vdots & \\vdots & \\vdots \\\\\n\\frac{\\partial y(t_m)}{\\partial \\theta_1} & \\frac{\\partial y(t_m)}{\\partial \\theta_2} & \\frac{\\partial y(t_m)}{\\partial \\theta_3}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\ne^{-\\theta_2 t_1} & -t_1 \\theta_1 e^{-\\theta_2 t_1} & 1 \\\\\ne^{-\\theta_2 t_2} & -t_2 \\theta_1 e^{-\\theta_2 t_2} & 1 \\\\\n\\vdots & \\vdots & \\vdots \\\\\ne^{-\\theta_2 t_m} & -t_m \\theta_1 e^{-\\theta_2 t_m} & 1\n\\end{pmatrix}\n$$\n\n### Step 2: Identifiability Analysis via Singular Value Decomposition (SVD)\n\nLocal parameter identifiability at a nominal parameter vector $\\theta$ is contingent on the Jacobian matrix $J(\\theta)$ having full column rank. If $J(\\theta)$ is rank-deficient, its columns are linearly dependent. This implies that there exists a non-zero linear combination of parameter changes, $\\Delta\\theta$, such that $J(\\theta) \\Delta\\theta = 0$. This means that a combination of parameter perturbations results in no change to the model output, rendering the parameters indistinguishable from data.\n\nThe Singular Value Decomposition (SVD) is a robust numerical tool for analyzing the rank and near-rank-deficiency of a matrix. The SVD of the $m \\times 3$ Jacobian is $J(\\theta) = U \\Sigma V^\\top$, where:\n- $U$ is an $m \\times m$ orthogonal matrix whose columns are the left singular vectors.\n- $\\Sigma$ is an $m \\times 3$ diagonal matrix containing the non-negative singular values $\\sigma_1 \\ge \\sigma_2 \\ge \\sigma_3 \\ge 0$.\n- $V^\\top$ is a $3 \\times 3$ orthogonal matrix whose rows are the transposes of the right singular vectors.\n\nThe number of non-zero singular values equals the rank of the matrix. Small singular values indicate that the matrix is close to being rank-deficient. The corresponding right singular vectors (columns of $V$) identify the directions in the parameter space $(\\theta_1, \\theta_2, \\theta_3)$ along which the model output is insensitive. These are termed \"near-null directions\".\n\nThe problem defines a quantitative criterion for identifying such directions: a singular value $\\sigma_j$ is considered small enough to indicate a near-null direction if it satisfies the inequality:\n$$ \\sigma_j \\le \\tau \\sigma_{\\max} $$\nwhere $\\sigma_{\\max}$ is the largest singular value and the threshold factor is given as $\\tau = 10^{-3}$. The task is to count how many of the $3$ singular values satisfy this condition.\n\n### Step 3: Algorithmic Procedure for Test Cases\n\nThe solution is implemented by applying the derived formulas to each specified test case. The algorithm is as follows:\n\n1.  For a given test case, consisting of a nominal parameter vector $\\theta = (\\theta_1, \\theta_2, \\theta_3)$ and a sampling schedule $t = [t_1, t_2, \\ldots, t_m]$, construct the numerical Jacobian matrix $J$ using the expressions derived in Step 1.\n2.  Compute the singular values of the matrix $J$. For an $m \\times 3$ matrix, this will yield $\\min(m, 3)=3$ singular values (since $m \\ge 3$ in all test cases). Let these be denoted by the vector $s = [\\sigma_1, \\sigma_2, \\sigma_3]$.\n3.  If all singular values are zero, all directions are null directions. The count is $3$.\n4.  Otherwise, find the maximum singular value, $\\sigma_{\\max} = \\max(s)$.\n5.  Calculate the threshold $T = \\tau \\sigma_{\\max} = 10^{-3} \\sigma_{\\max}$.\n6.  Count the number of singular values $\\sigma_j$ in $s$ such that $\\sigma_j \\le T$. This integer count is the result for the test case.\n\nThis procedure is systematically applied to all four test cases provided in the problem statement.\n- **Case 1** is a general, well-posed scenario where full rank is expected, resulting in $0$ near-null directions.\n- **Case 2** presents a degenerate sampling schedule where all $t_k=0$. This makes all rows of the Jacobian identical: $[1, 0, 1]$. The resulting $3 \\times 3$ matrix has rank $1$, and its nullity is $3-1=2$. Thus, $2$ singular values will be exactly zero, leading to a count of $2$.\n- **Case 3** involves a fast decay rate $(\\theta_2 = 5.0)$ and late sampling times. The term $e^{-\\theta_2 t_k}$ becomes negligible. The first two columns of the Jacobian, which depend on this term, become nearly zero vectors. The Jacobian is approximately a matrix with two zero columns and one column of ones, giving it a rank of $1$. This practical non-identifiability of $\\theta_1$ and $\\theta_2$ will manifest as two very small singular values, resulting in a count of $2$.\n- **Case 4** uses the minimum number of samples ($m=3$) required to identify $3$ parameters. The sampling times and parameter values are chosen such that the Jacobian is a well-conditioned square matrix, so no near-null directions are expected (count of $0$).\n\nThe following Python program implements this logic to calculate the final answer.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the theoretical identifiability analysis problem for four test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'theta': np.array([2.0, 0.5, 0.1]), 't': np.array([0.0, 0.5, 1.0, 1.5, 2.0])},\n        {'theta': np.array([1.5, 0.4, -0.2]), 't': np.array([0.0, 0.0, 0.0])},\n        {'theta': np.array([1.0, 5.0, 0.0]), 't': np.array([2.0, 3.0, 4.0, 5.0])},\n        {'theta': np.array([1.0, 0.7, 0.2]), 't': np.array([0.2, 0.8, 1.7])}\n    ]\n\n    tau = 1e-3\n    results = []\n\n    for case in test_cases:\n        theta = case['theta']\n        t_samples = case['t']\n        \n        theta1, theta2, _ = theta\n        \n        # 1. Derive the Jacobian matrix J(theta)\n        # J_ki = dy(t_k)/d(theta_i)\n        # The columns of the Jacobian are the partial derivatives with respect to each parameter,\n        # evaluated at each time sample t_k.\n        \n        # Column 1: Partial derivative with respect to theta_1\n        # d/d(theta_1) [theta_1 * exp(-theta_2*t) + theta_3] = exp(-theta_2*t)\n        col1 = np.exp(-theta2 * t_samples)\n        \n        # Column 2: Partial derivative with respect to theta_2\n        # d/d(theta_2) [theta_1 * exp(-theta_2*t) + theta_3] = -t * theta_1 * exp(-theta_2*t)\n        col2 = -t_samples * theta1 * np.exp(-theta2 * t_samples)\n        \n        # Column 3: Partial derivative with respect to theta_3\n        # d/d(theta_3) [theta_1 * exp(-theta_2*t) + theta_3] = 1\n        col3 = np.ones_like(t_samples)\n        \n        jacobian = np.c_[col1, col2, col3]\n        \n        # 2. Compute the SVD and determine near-null directions\n        \n        # The number of singular values is min(m, n) where J is m x n.\n        # Here n = 3 and m = len(t_samples) >= 3, so there are always 3 singular values.\n        singular_values = np.linalg.svd(jacobian, compute_uv=False)\n        \n        if singular_values.size == 0:\n            # This case shouldn't happen with the given problem constraints\n            # but is included for robustness.\n            count = 0\n        else:\n            sigma_max = np.max(singular_values)\n            \n            # If sigma_max is 0, the matrix is a zero matrix. All directions are null.\n            if sigma_max > 0:\n                threshold = tau * sigma_max\n                count = np.sum(singular_values = threshold)\n            else:\n                # All singular values are 0, so all are counted.\n                count = len(singular_values)\n                \n        results.append(int(count))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3426732"}, {"introduction": "Building upon the concept of sensitivity, the Fisher Information Matrix (FIM) provides a crucial link between the model's Jacobian and the statistical uncertainty of its parameter estimates. The conditioning of the FIM dictates practical identifiabilityâ€”a nearly singular FIM implies that parameters cannot be estimated with meaningful precision. This practice [@problem_id:3426673] delves into how experimental design choices, such as the sampling interval, directly impact the FIM's structure, offering fundamental insights into why even a structurally identifiable model can become practically non-identifiable with poorly collected data.", "problem": "Consider a scalar observable model with an unknown amplitude parameter $A \\in \\mathbb{R}$ and unknown decay rate parameter $\\theta  0$ given by $y(t) = A e^{-\\theta t}$. Suppose we collect $K$ discrete-time observations at $t_k = k \\Delta t$ for $k = 1, \\dots, K$ with independent and identically distributed (i.i.d.) additive Gaussian noise of variance $\\sigma^2$, so that $y_k = A e^{-\\theta t_k} + \\varepsilon_k$ with $\\varepsilon_k \\sim \\mathcal{N}(0, \\sigma^2)$ and $t_k = k \\Delta t$. In a local (first-order) analysis of parameter identifiability, the sensitivity matrix $S \\in \\mathbb{R}^{K \\times 2}$ collects partial derivatives of the observation model with respect to the parameters, evaluated along the discrete sampling schedule. That is, its two columns are $s_A(k) = \\partial y_k / \\partial A$ and $s_{\\theta}(k) = \\partial y_k / \\partial \\theta$. The Fisher Information Matrix (FIM) for $(A, \\theta)$ under Gaussian noise is, up to the factor $1/\\sigma^2$, given by $S^\\top S$.\n\nStarting from these definitions and the measurement model, reason from first principles to determine how the choice of sampling interval $\\Delta t$ interacts with the decay governed by $\\theta$ to affect the geometry (e.g., near-collinearity) of the sensitivity columns and the conditioning of the Fisher Information Matrix. In particular, assess the implications of coarse sampling (large $\\Delta t$) for the practical identifiability of $\\theta$.\n\nWhich of the following statements are correct? Choose all that apply.\n\nA. If we define weights $w_k = e^{-2 \\theta k \\Delta t}$ and the associated normalized discrete distribution $p_k = w_k \\big/ \\sum_{j=1}^K w_j$, then the absolute Pearson correlation between the sensitivity columns $s_A$ and $s_{\\theta}$ is $|\\rho| = \\mu_1 \\big/ \\sqrt{\\mu_2}$, where $\\mu_1 = \\sum_{k=1}^K k \\, p_k$ and $\\mu_2 = \\sum_{k=1}^K k^2 \\, p_k$. For large $\\theta \\Delta t$ (coarse sampling on a fast decay), $p_k$ concentrates on small $k$ and $|\\rho| \\to 1$, indicating near-collinearity.\n\nB. With $w_k = e^{-2 \\theta k \\Delta t}$, the Fisher Information Matrix determinant satisfies $\\det(S^\\top S) = A^2 \\Delta t^2 \\left(\\sum_{k=1}^K w_k\\right)^2 \\operatorname{Var}_{p}(k)$, where $\\operatorname{Var}_{p}(k) = \\sum_{k=1}^K k^2 p_k - \\left(\\sum_{k=1}^K k p_k\\right)^2$. As $\\Delta t$ grows with fixed $\\theta$, $p_k$ concentrates and $\\operatorname{Var}_{p}(k) \\to 0$, making the FIM nearly singular and rendering $\\theta$ practically non-identifiable.\n\nC. Discretization cannot affect identifiability because identifiability is a structural property of the continuous-time model; if $\\theta$ is identifiable in continuous time, it remains equally identifiable under any discrete sampling.\n\nD. Refining the sampling (decreasing $\\Delta t$) always increases the correlation between the sensitivity columns $s_A$ and $s_{\\theta}$, thereby worsening the identifiability of $\\theta$.\n\nE. If an additional constant offset parameter $B$ is included so that $y_k = A e^{-\\theta k \\Delta t} + B + \\varepsilon_k$, then coarse sampling makes the sensitivity to $B$, namely $s_B(k) = 1$, orthogonal to $s_A(k)$, thereby improving the identifiability of $\\theta$.", "solution": "The user wants a critical validation of the problem statement, followed by a first-principles derivation and evaluation of all options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n*   **Model:** Scalar observable $y(t) = A e^{-\\theta t}$.\n*   **Parameters:** Amplitude $A \\in \\mathbb{R}$, decay rate $\\theta  0$.\n*   **Discrete Observations:** $y_k = A e^{-\\theta t_k} + \\varepsilon_k$ for $k = 1, \\dots, K$.\n*   **Sampling Schedule:** $t_k = k \\Delta t$.\n*   **Noise Model:** $\\varepsilon_k \\sim \\mathcal{N}(0, \\sigma^2)$, independent and identically distributed (i.i.d.).\n*   **Sensitivity Matrix:** $S \\in \\mathbb{R}^{K \\times 2}$ with columns $s_A$ and $s_{\\theta}$.\n*   **Sensitivity Column Definitions:** The $k$-th element of the columns are $s_A(k) = \\partial y_k / \\partial A$ and $s_{\\theta}(k) = \\partial y_k / \\partial \\theta$.\n*   **Fisher Information Matrix (FIM):** FIM for $(A, \\theta)$ under Gaussian noise is stated to be given by $S^\\top S$ up to a factor of $1/\\sigma^2$. The analysis should focus on $S^\\top S$.\n*   **Objective:** Analyze the effect of the sampling interval $\\Delta t$ and decay rate $\\theta$ on the collinearity of the sensitivity columns and the conditioning of the FIM, particularly for a large sampling interval $\\Delta t$.\n\n**Step 2: Validate Using Extracted Givens**\n\n1.  **Scientifically Grounded:** The problem is based on the standard exponential decay model, a cornerstone of modeling in many STEM fields. The analysis framework using sensitivity analysis and the Fisher Information Matrix is a standard and rigorous method for local parameter identifiability analysis and optimal experimental design. All concepts are well-established.\n2.  **Well-Posed:** The problem provides a clear, self-contained mathematical setup. The goal is to analyze specific properties of the resulting mathematical objects ($S$ and $S^\\top S$), which is a well-defined task. A unique analysis based on the provided definitions is possible.\n3.  **Objective:** The language is precise and mathematical. Terms like \"sensitivity matrix,\" \"collinearity,\" and \"Fisher Information Matrix\" have unambiguous definitions in this context.\n4.  **Completeness:** The problem statement includes all necessary information: the model function, parameter definitions, sampling scheme, noise characteristics, and the definitions of the mathematical objects to be analyzed.\n5.  **No Other Flaws:** The problem is free from contradictions, unrealistic conditions, or vagueness. It presents a standard, non-trivial problem in parameter estimation theory.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. The analysis can proceed.\n\n### Principle-Based Derivation and Solution\n\n**1. Derivation of Sensitivity Vectors**\nThe model for the $k$-th observation is $y_k$, which is based on the model function $f(t_k; A, \\theta) = A e^{-\\theta t_k}$. The sensitivity vectors, $\\mathbf{s}_A$ and $\\mathbf{s}_{\\theta}$, are vectors in $\\mathbb{R}^K$ whose components are the partial derivatives of the model function with respect to each parameter, evaluated at each sampling time $t_k = k \\Delta t$.\n\n*   **Sensitivity to A:** The $k$-th component is\n    $$ s_A(k) = \\frac{\\partial}{\\partial A} \\left( A e^{-\\theta k \\Delta t} \\right) = e^{-\\theta k \\Delta t} $$\n*   **Sensitivity to $\\theta$:** The $k$-th component is\n    $$ s_{\\theta}(k) = \\frac{\\partial}{\\partial \\theta} \\left( A e^{-\\theta k \\Delta t} \\right) = A e^{-\\theta k \\Delta t} (-k \\Delta t) = -A k \\Delta t e^{-\\theta k \\Delta t} $$\n\nThus, the two sensitivity column vectors are:\n$$ \\mathbf{s}_A = \\begin{pmatrix} e^{-\\theta \\Delta t} \\\\ e^{-2\\theta \\Delta t} \\\\ \\vdots \\\\ e^{-K\\theta \\Delta t} \\end{pmatrix}, \\quad \\mathbf{s}_{\\theta} = -A \\Delta t \\begin{pmatrix} 1 \\cdot e^{-\\theta \\Delta t} \\\\ 2 \\cdot e^{-2\\theta \\Delta t} \\\\ \\vdots \\\\ K \\cdot e^{-K\\theta \\Delta t} \\end{pmatrix} $$\n\n**2. Derivation of the Fisher Information Matrix ($S^\\top S$)**\nThe matrix $S^\\top S$ is a $2 \\times 2$ Gram matrix whose elements are the dot products of the sensitivity vectors.\n$$ S^\\top S = \\begin{pmatrix} \\mathbf{s}_A^\\top \\mathbf{s}_A  \\mathbf{s}_A^\\top \\mathbf{s}_{\\theta} \\\\ \\mathbf{s}_{\\theta}^\\top \\mathbf{s}_A  \\mathbf{s}_{\\theta}^\\top \\mathbf{s}_{\\theta} \\end{pmatrix} $$\nLet's compute the elements:\n*   $(S^\\top S)_{11} = \\mathbf{s}_A^\\top \\mathbf{s}_A = \\sum_{k=1}^K (e^{-\\theta k \\Delta t})^2 = \\sum_{k=1}^K e^{-2\\theta k \\Delta t}$\n*   $(S^\\top S)_{12} = \\mathbf{s}_A^\\top \\mathbf{s}_{\\theta} = \\sum_{k=1}^K (e^{-\\theta k \\Delta t})(-A k \\Delta t e^{-\\theta k \\Delta t}) = -A \\Delta t \\sum_{k=1}^K k e^{-2\\theta k \\Delta t}$\n*   $(S^\\top S)_{21} = (S^\\top S)_{12}$\n*   $(S^\\top S)_{22} = \\mathbf{s}_{\\theta}^\\top \\mathbf{s}_{\\theta} = \\sum_{k=1}^K (-A k \\Delta t e^{-\\theta k \\Delta t})^2 = A^2 (\\Delta t)^2 \\sum_{k=1}^K k^2 e^{-2\\theta k \\Delta t}$\n\n**3. Analysis of Collinearity**\nCollinearity is assessed by the cosine of the angle $\\phi$ between the vectors $\\mathbf{s}_A$ and $\\mathbf{s}_{\\theta}$, where $\\cos(\\phi) = \\frac{\\mathbf{s}_A^\\top \\mathbf{s}_{\\theta}}{\\|\\mathbf{s}_A\\| \\|\\mathbf{s}_{\\theta}\\|}$. Near-collinearity corresponds to $|\\cos(\\phi)| \\approx 1$.\n$$ \\cos(\\phi) = \\frac{-A \\Delta t \\sum_{k=1}^K k e^{-2\\theta k \\Delta t}}{\\sqrt{\\left(\\sum_{k=1}^K e^{-2\\theta k \\Delta t}\\right) \\left(A^2 (\\Delta t)^2 \\sum_{k=1}^K k^2 e^{-2\\theta k \\Delta t}\\right)}} = -\\mathrm{sgn}(A) \\frac{\\sum_{k=1}^K k e^{-2\\theta k \\Delta t}}{\\sqrt{\\left(\\sum_{k=1}^K e^{-2\\theta k \\Delta t}\\right)\\left(\\sum_{k=1}^K k^2 e^{-2\\theta k \\Delta t}\\right)}} $$\nLet's define weights $w_k = e^{-2\\theta k \\Delta t}$ and the total weight $W = \\sum_{j=1}^K w_j$. We can define a discrete probability distribution $p_k = w_k/W$. The expression for the absolute value of the cosine becomes:\n$$ |\\cos(\\phi)| = \\frac{\\sum k w_k}{\\sqrt{(\\sum w_k)(\\sum k^2 w_k)}} = \\frac{W \\sum k p_k}{\\sqrt{W \\cdot W \\sum k^2 p_k}} = \\frac{\\sum k p_k}{\\sqrt{\\sum k^2 p_k}} $$\nFollowing the notation in the problem options, let $\\mu_1 = \\sum_{k=1}^K k p_k$ and $\\mu_2 = \\sum_{k=1}^K k^2 p_k$. Then, $|\\cos(\\phi)| = \\mu_1 / \\sqrt{\\mu_2}$. While the Pearson correlation coefficient is technically different as it involves de-meaning the vectors, the cosine similarity is a direct measure of collinearity and is often what is contextually meant by \"correlation\" in this type of analysis.\n\n### Option-by-Option Analysis\n\n**A. If we define weights $w_k = e^{-2 \\theta k \\Delta t}$ and the associated normalized discrete distribution $p_k = w_k \\big/ \\sum_{j=1}^K w_j$, then the absolute Pearson correlation between the sensitivity columns $s_A$ and $s_{\\theta}$ is $|\\rho| = \\mu_1 \\big/ \\sqrt{\\mu_2}$, where $\\mu_1 = \\sum_{k=1}^K k \\, p_k$ and $\\mu_2 = \\sum_{k=1}^K k^2 \\, p_k$. For large $\\theta \\Delta t$ (coarse sampling on a fast decay), $p_k$ concentrates on small $k$ and $|\\rho| \\to 1$, indicating near-collinearity.**\n\n- **Formula Evaluation:** As derived above, the formula $|\\rho| = \\mu_1 / \\sqrt{\\mu_2}$ represents the magnitude of the cosine similarity between the sensitivity vectors, not strictly the Pearson correlation coefficient. However, it is a valid and direct measure of their geometric alignment (collinearity).\n- **Limit Analysis:** Consider the case of \"coarse sampling,\" where the product $\\theta \\Delta t$ is large. Let $x = \\theta \\Delta t \\gg 1$. The weights are $w_k = e^{-2kx}$. The ratio of successive weights is $w_{k+1}/w_k = e^{-2x} \\ll 1$. This means the weights decay extremely rapidly: $w_1 \\gg w_2 \\gg w_3 \\dots$. The sum $W = \\sum_j w_j$ is dominated by the first term, $w_1$.\n- The probability mass function $p_k = w_k / W$ will therefore be heavily concentrated at the smallest index, $k=1$. In the limit as $\\theta \\Delta t \\to \\infty$, we have $p_1 \\to 1$ and $p_k \\to 0$ for $k  1$.\n- We can now evaluate the limits of the moments:\n    - $\\mu_1 = \\sum_{k=1}^K k p_k \\longrightarrow 1 \\cdot p_1 + 2 \\cdot p_2 + \\dots \\longrightarrow 1 \\cdot 1 = 1$.\n    - $\\mu_2 = \\sum_{k=1}^K k^2 p_k \\longrightarrow 1^2 \\cdot p_1 + 2^2 \\cdot p_2 + \\dots \\longrightarrow 1^2 \\cdot 1 = 1$.\n- Thus, the collinearity measure becomes $|\\rho| = \\mu_1 / \\sqrt{\\mu_2} \\longrightarrow 1 / \\sqrt{1} = 1$.\n- **Conclusion:** A value of $|\\rho| \\to 1$ indicates that the vectors $\\mathbf{s}_A$ and $\\mathbf{s}_{\\theta}$ become nearly parallel. This makes it statistically difficult to distinguish the effect of changing $A$ from the effect of changing $\\theta$, which is the essence of poor practical identifiability. The statement is conceptually and mathematically correct, despite a minor terminological ambiguity.\n- **Verdict on A:** **Correct**.\n\n**B. With $w_k = e^{-2 \\theta k \\Delta t}$, the Fisher Information Matrix determinant satisfies $\\det(S^\\top S) = A^2 \\Delta t^2 \\left(\\sum_{k=1}^K w_k\\right)^2 \\operatorname{Var}_{p}(k)$, where $\\operatorname{Var}_{p}(k) = \\sum_{k=1}^K k^2 p_k - \\left(\\sum_{k=1}^K k p_k\\right)^2$. As $\\Delta t$ grows with fixed $\\theta$, $p_k$ concentrates and $\\operatorname{Var}_{p}(k) \\to 0$, making the FIM nearly singular and rendering $\\theta$ practically non-identifiable.**\n\n- **Formula Evaluation:** Let's compute the determinant of $S^\\top S$:\n    $$ \\det(S^\\top S) = (\\mathbf{s}_A^\\top \\mathbf{s}_A)(\\mathbf{s}_{\\theta}^\\top \\mathbf{s}_{\\theta}) - (\\mathbf{s}_A^\\top \\mathbf{s}_{\\theta})^2 $$\n    $$ \\det(S^\\top S) = \\left(\\sum w_k\\right) \\left(A^2 (\\Delta t)^2 \\sum k^2 w_k\\right) - \\left(-A \\Delta t \\sum k w_k\\right)^2 $$\n    $$ \\det(S^\\top S) = A^2 (\\Delta t)^2 \\left[ \\left(\\sum w_k\\right)\\left(\\sum k^2 w_k\\right) - \\left(\\sum k w_k\\right)^2 \\right] $$\n    Using $p_k=w_k/W$ and $W=\\sum w_k$: $\\sum k w_k = W \\mu_1$ and $\\sum k^2 w_k = W \\mu_2$.\n    $$ \\det(S^\\top S) = A^2 (\\Delta t)^2 \\left[ W(W \\mu_2) - (W \\mu_1)^2 \\right] = A^2 (\\Delta t)^2 W^2 (\\mu_2 - \\mu_1^2) $$\n    Since $W = \\sum w_k$ and $\\operatorname{Var}_p(k) = \\mu_2 - \\mu_1^2$, the formula is $\\det(S^\\top S) = A^2 (\\Delta t)^2 \\left(\\sum w_k\\right)^2 \\operatorname{Var}_p(k)$. The formula is correct.\n- **Limit Analysis:** As $\\Delta t$ grows, $\\theta \\Delta t \\to \\infty$. As established for option A, the distribution $p_k$ concentrates on $k=1$. For a random variable that takes a single value with probability $1$, its variance is zero.\n    - $\\operatorname{Var}_p(k) = \\mu_2 - \\mu_1^2 \\longrightarrow 1 - 1^2 = 0$.\n- **Conclusion:** A determinant of the FIM that approaches zero indicates that the matrix is nearly singular. This is a hallmark of poor practical identifiability, as the volume of the parameter confidence region (which is inversely related to $\\sqrt{\\det(\\text{FIM})}$) becomes infinitely large. The statement provides a correct formula and a correct conclusion about the consequences for identifiability.\n- **Verdict on B:** **Correct**.\n\n**C. Discretization cannot affect identifiability because identifiability is a structural property of the continuous-time model; if $\\theta$ is identifiable in continuous time, it remains equally identifiable under any discrete sampling.**\n\n- This statement conflates two distinct concepts: *structural identifiability* and *practical identifiability*.\n- Structural identifiability asks if parameters are unique given perfect, continuous data. It is indeed a property of the model structure. For $y(t) = A e^{-\\theta t}$, the parameters are structurally identifiable.\n- Practical identifiability asks if parameters can be estimated with finite precision from a finite set of noisy, discrete data. It depends critically on the experimental design, including the sampling schedule ($\\Delta t, K$) and noise level ($\\sigma^2$).\n- As demonstrated in the analysis of A and B, choosing a poor sampling scheme (e.g., large $\\Delta t$) can lead to a nearly singular FIM, making it impossible to estimate parameters reliably. Therefore, discretization and the choice of sampling have a profound effect on practical identifiability.\n- **Verdict on C:** **Incorrect**.\n\n**D. Refining the sampling (decreasing $\\Delta t$) always increases the correlation between the sensitivity columns $s_A$ and $s_{\\theta}$, thereby worsening the identifiability of $\\theta$.**\n\n- This statement is the opposite of the truth. As shown in A and B, coarse sampling (large $\\Delta t$) increases the correlation and worsens identifiability. Let's consider the limit of refining the sampling, $\\Delta t \\to 0$.\n- As $\\Delta t \\to 0$, we have $\\theta k \\Delta t \\to 0$, so $w_k = e^{-2\\theta k \\Delta t} \\to 1$ for all $k$.\n- The distribution $p_k = w_k / \\sum w_j$ approaches the discrete uniform distribution, $p_k \\to 1/K$.\n- The correlation measure $|\\rho|$ approaches a value less than $1$ (for $K1$), which implies less collinearity than the coarse sampling case. For example, as $\\Delta t \\to 0$, $|\\rho| \\to \\sqrt{\\frac{3(K+1)}{2(2K+1)}}$, which for $K \\gg 1$ is approximately $\\sqrt{3/4} \\approx 0.866$.\n- Since coarse sampling leads to $|\\rho| \\to 1$, refining the sampling *decreases* the correlation (moves it away from $1$) and thus *improves* practical identifiability. The statement is false.\n- **Verdict on D:** **Incorrect**.\n\n**E. If an additional constant offset parameter $B$ is included so that $y_k = A e^{-\\theta k \\Delta t} + B + \\varepsilon_k$, then coarse sampling makes the sensitivity to $B$, namely $s_B(k) = 1$, orthogonal to $s_A(k)$, thereby improving the identifiability of $\\theta$.**\n\n- The new model is $y_k = A e^{-\\theta k \\Delta t} + B$. The sensitivity vector for $B$ has components $s_B(k) = \\partial y_k / \\partial B = 1$. The vector $\\mathbf{s}_B$ is a vector of ones.\n- The sensitivity vector $\\mathbf{s}_A$ is unchanged, with components $s_A(k) = e^{-\\theta k \\Delta t}$.\n- Orthogonality requires the dot product to be zero: $\\mathbf{s}_A^\\top \\mathbf{s}_B = \\sum_{k=1}^K s_A(k) s_B(k) = \\sum_{k=1}^K e^{-\\theta k \\Delta t}$.\n- Since $\\theta  0$ and $\\Delta t  0$, every term in the sum is strictly positive. The dot product is always positive, so the vectors are never orthogonal.\n- Let's consider the limit of coarse sampling ($\\theta \\Delta t \\to \\infty$). In this limit, $e^{-\\theta k \\Delta t} \\to 0$ for all $k \\ge 1$. So, the vector $\\mathbf{s}_A \\to \\mathbf{0}$. The zero vector is technically orthogonal to any other vector. However, if a sensitivity vector becomes zero, it means the observations have no sensitivity to that parameter. This makes the parameter ($A$) completely unidentifiable. The identifiability of $\\theta$ also vanishes, since $\\mathbf{s}_{\\theta}$ is proportional to components like $k e^{-\\theta k \\Delta t}$, which also go to zero.\n- The conclusion that this \"improves the identifiability of $\\theta$\" is incorrect. Vanishing sensitivity is the worst possible scenario for identifiability.\n- **Verdict on E:** **Incorrect**.", "answer": "$$\\boxed{AB}$$", "id": "3426673"}, {"introduction": "While Jacobian-based methods analyze all parameters locally, the profile likelihood technique offers a powerful way to investigate the identifiability of one parameter at a time while accounting for its interactions with the others. This is achieved by exploring the landscape of the cost function along a single parameter dimension, after optimizing over all others. This analytical exercise [@problem_id:3426692] guides you through the from-scratch derivation of a profile cost function and its curvature, providing a deeper understanding of the geometric properties of the likelihood surface that ultimately determine a parameter's uncertainty.", "problem": "Consider data assimilation for a scalar output model with independent and identically distributed (IID) Gaussian measurement noise. You observe data $\\{(t_i,y_i)\\}_{i=1}^{n}$ at known times $t_i \\in \\mathbb{R}_{0}$ from the parametric model $y(t)=\\theta_1 \\exp(-\\theta_2 t)$, with observation noise modeled as $y_i = \\theta_1 \\exp(-\\theta_2 t_i) + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ are IID and $\\sigma^20$ is known. Consider the negative log-likelihood (up to an additive constant independent of parameters), equivalently the least-squares cost\n$$\nJ(\\theta_1,\\theta_2) \\;=\\; \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\big(y_i - \\theta_1 \\exp(-\\theta_2 t_i)\\big)^2.\n$$\nIn theoretical identifiability analysis, one studies the profile cost for $\\theta_2$ by minimizing $J(\\theta_1,\\theta_2)$ over $\\theta_1$ for each fixed $\\theta_2$, and then analyzing the curvature of this one-dimensional profile cost at its minimizer to assess local practical identifiability.\n\nStarting only from the definitions of least squares and basic calculus, and without invoking any pre-packaged identifiability formulas, do the following:\n- Derive explicitly the optimal $\\theta_1^{\\star}(\\theta_2)$ that minimizes $J(\\theta_1,\\theta_2)$ for a fixed $\\theta_2$.\n- Substitute $\\theta_1^{\\star}(\\theta_2)$ back into $J$ to obtain a fully explicit, closed-form expression for the profile cost $J_{\\mathrm{p}}(\\theta_2) \\equiv J\\big(\\theta_1^{\\star}(\\theta_2),\\theta_2\\big)$ in terms of the data $\\{(t_i,y_i)\\}$, the noise variance $\\sigma^2$, and $\\theta_2$.\n- Let $\\widehat{\\theta}_2$ be any minimizer of $J_{\\mathrm{p}}(\\theta_2)$ over $\\theta_2 \\in \\mathbb{R}_{0}$. Compute the exact second derivative $\\frac{d^2}{d\\theta_2^2} J_{\\mathrm{p}}(\\theta_2)$ evaluated at $\\theta_2=\\widehat{\\theta}_2$, expressing the result entirely in terms of $\\{(t_i,y_i)\\}$, $\\sigma^2$, and $\\widehat{\\theta}_2$. Your derivation must be explicit and self-contained.\n\nProvide your final answer as a single analytic expression, consisting of a row with two entries: first the explicit $J_{\\mathrm{p}}(\\theta_2)$ and second the curvature $\\left.\\frac{d^2}{d\\theta_2^2} J_{\\mathrm{p}}(\\theta_2)\\right|_{\\theta_2=\\widehat{\\theta}_2}$. No numerical evaluation is required, and no rounding is needed. Do not include units in your final answer.", "solution": "The user wants to solve a problem related to the theoretical identifiability analysis for a given parametric model. The problem is divided into three parts: finding the optimal value of a parameter $\\theta_1$ as a function of another parameter $\\theta_2$, deriving the profile cost function for $\\theta_2$, and calculating the curvature of this profile cost at its minimum.\n\n### Step 1: Extract Givens\n-   **Data:** A set of $n$ observations $\\{(t_i, y_i)\\}_{i=1}^{n}$ with known times $t_i \\in \\mathbb{R}_{0}$.\n-   **Parametric Model:** $y(t) = \\theta_1 \\exp(-\\theta_2 t)$.\n-   **Observation Model:** $y_i = \\theta_1 \\exp(-\\theta_2 t_i) + \\varepsilon_i$.\n-   **Noise Model:** Measurement errors $\\varepsilon_i$ are independent and identically distributed (IID) as a Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$, with known variance $\\sigma^2  0$.\n-   **Cost Function:** The negative log-likelihood or least-squares cost function is given by:\n    $$\n    J(\\theta_1, \\theta_2) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\big(y_i - \\theta_1 \\exp(-\\theta_2 t_i)\\big)^2.\n    $$\n-   **Profile Cost:** $J_{\\mathrm{p}}(\\theta_2) \\equiv J\\big(\\theta_1^{\\star}(\\theta_2), \\theta_2\\big)$, where $\\theta_1^{\\star}(\\theta_2)$ minimizes $J(\\theta_1, \\theta_2)$ for a fixed $\\theta_2$.\n-   **Minimizer of Profile Cost:** $\\widehat{\\theta}_2$ is a minimizer of $J_{\\mathrm{p}}(\\theta_2)$ over $\\theta_2 \\in \\mathbb{R}_{0}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria:\n-   **Scientifically Grounded:** The problem is a standard exercise in nonlinear regression and parameter estimation, a fundamental topic in statistics, data assimilation, and inverse problems. The model is a simple exponential decay, and the cost function is the standard least-squares objective derived from a Gaussian noise assumption. It is firmly based on established mathematical and statistical principles.\n-   **Well-Posed:** The problem statement is clear and the tasks are mathematically well-defined. The existence of minimizers is assumed, which is typical for such problems. The objective is to derive analytical expressions, which is a well-posed mathematical task.\n-   **Objective:** The language is precise, using standard mathematical terminology. There are no subjective or opinion-based statements.\n-   **Completeness:** The problem provides all necessary information (model, cost function, definitions) to perform the required derivations.\n-   **No other flaws detected:** The problem does not violate any of the other invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full, reasoned solution will be provided.\n\n### Solution Derivation\n\nThe solution will be derived in three parts as requested.\n\n**Part 1: Derivation of the optimal $\\theta_1^{\\star}(\\theta_2)$**\n\nFor a fixed value of $\\theta_2$, the cost function $J(\\theta_1, \\theta_2)$ is a quadratic function of $\\theta_1$. To find the minimizer $\\theta_1^{\\star}(\\theta_2)$, we compute the partial derivative of $J$ with respect to $\\theta_1$ and set it to zero.\n\n$$\n\\frac{\\partial J}{\\partial \\theta_1} = \\frac{\\partial}{\\partial \\theta_1} \\left[ \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\big(y_i - \\theta_1 \\exp(-\\theta_2 t_i)\\big)^2 \\right]\n$$\n$$\n\\frac{\\partial J}{\\partial \\theta_1} = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} 2 \\big(y_i - \\theta_1 \\exp(-\\theta_2 t_i)\\big) \\cdot \\big(-\\exp(-\\theta_2 t_i)\\big)\n$$\nSetting this derivative to zero:\n$$\n-\\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\big(y_i \\exp(-\\theta_2 t_i) - \\theta_1 \\exp(-2\\theta_2 t_i)\\big) = 0\n$$\nSince $\\sigma^2  0$, we can multiply by $-\\sigma^2$:\n$$\n\\sum_{i=1}^{n} y_i \\exp(-\\theta_2 t_i) - \\theta_1 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) = 0\n$$\nSolving for $\\theta_1$ yields the optimal value $\\theta_1^{\\star}(\\theta_2)$:\n$$\n\\theta_1^{\\star}(\\theta_2) = \\frac{\\sum_{i=1}^{n} y_i \\exp(-\\theta_2 t_i)}{\\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)}\n$$\nTo confirm this is a minimum, we check the second partial derivative:\n$$\n\\frac{\\partial^2 J}{\\partial \\theta_1^2} = \\frac{\\partial}{\\partial \\theta_1} \\left[ \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\big(\\theta_1 \\exp(-2\\theta_2 t_i) - y_i \\exp(-\\theta_2 t_i)\\big) \\right] = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)\n$$\nSince $t_i  0$ and $\\theta_2$ is sought in $\\mathbb{R}_{0}$, $\\exp(-2\\theta_2 t_i)  0$. The sum is therefore positive, and so is $\\frac{\\partial^2 J}{\\partial \\theta_1^2}$. This confirms that $\\theta_1^{\\star}(\\theta_2)$ is indeed a minimizer.\n\n**Part 2: Derivation of the profile cost $J_{\\mathrm{p}}(\\theta_2)$**\n\nWe substitute $\\theta_1^{\\star}(\\theta_2)$ back into the cost function $J(\\theta_1, \\theta_2)$:\n$$\nJ_{\\mathrm{p}}(\\theta_2) = J\\big(\\theta_1^{\\star}(\\theta_2), \\theta_2\\big) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\big(y_i - \\theta_1^{\\star}(\\theta_2) \\exp(-\\theta_2 t_i)\\big)^2\n$$\nExpanding the square:\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} \\left( y_i^2 - 2y_i \\theta_1^{\\star}(\\theta_2) \\exp(-\\theta_2 t_i) + (\\theta_1^{\\star}(\\theta_2))^2 \\exp(-2\\theta_2 t_i) \\right)\n$$\nDistributing the summation:\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - 2\\theta_1^{\\star}(\\theta_2) \\sum_{i=1}^{n} y_i \\exp(-\\theta_2 t_i) + (\\theta_1^{\\star}(\\theta_2))^2 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) \\right)\n$$\nFrom the expression for $\\theta_1^{\\star}(\\theta_2)$, we have $\\sum_{i=1}^{n} y_i \\exp(-\\theta_2 t_i) = \\theta_1^{\\star}(\\theta_2) \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)$. Substituting this into the middle term:\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - 2\\theta_1^{\\star}(\\theta_2) \\left(\\theta_1^{\\star}(\\theta_2) \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)\\right) + (\\theta_1^{\\star}(\\theta_2))^2 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) \\right)\n$$\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - 2(\\theta_1^{\\star}(\\theta_2))^2 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) + (\\theta_1^{\\star}(\\theta_2))^2 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) \\right)\n$$\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - (\\theta_1^{\\star}(\\theta_2))^2 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) \\right)\n$$\nSubstituting the full expression for $\\theta_1^{\\star}(\\theta_2)$:\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - \\left(\\frac{\\sum_{j=1}^{n} y_j \\exp(-\\theta_2 t_j)}{\\sum_{k=1}^{n} \\exp(-2\\theta_2 t_k)}\\right)^2 \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i) \\right)\n$$\nWhich simplifies to the explicit closed-form expression for the profile cost:\n$$\nJ_{\\mathrm{p}}(\\theta_2) = \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - \\frac{\\left(\\sum_{i=1}^{n} y_i \\exp(-\\theta_2 t_i)\\right)^2}{\\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)} \\right)\n$$\n\n**Part 3: Derivation of the curvature $\\left.\\frac{d^2}{d\\theta_2^2} J_{\\mathrm{p}}(\\theta_2)\\right|_{\\theta_2=\\widehat{\\theta}_2}$**\n\nTo find the second derivative of the profile cost, we can use a result based on the envelope theorem. The first derivative of the profile cost is given by the partial derivative of the original cost function with respect to $\\theta_2$, evaluated at the optimal $\\theta_1^{\\star}(\\theta_2)$:\n$$\n\\frac{d J_{\\mathrm{p}}}{d\\theta_2} = \\left. \\frac{\\partial J(\\theta_1, \\theta_2)}{\\partial \\theta_2} \\right|_{\\theta_1=\\theta_1^{\\star}(\\theta_2)}\n$$\nDifferentiating this again with respect to $\\theta_2$ using the chain rule gives the second derivative:\n$$\n\\frac{d^2 J_{\\mathrm{p}}}{d\\theta_2^2} = \\frac{\\partial^2 J}{\\partial \\theta_2^2} + \\frac{\\partial^2 J}{\\partial \\theta_1 \\partial \\theta_2} \\frac{d\\theta_1^{\\star}}{d\\theta_2}\n$$\nwhere all partial derivatives are evaluated at $(\\theta_1^{\\star}(\\theta_2), \\theta_2)$. The derivative $\\frac{d\\theta_1^{\\star}}{d\\theta_2}$ is found by differentiating the first-order optimality condition $\\frac{\\partial J}{\\partial \\theta_1}(\\theta_1^{\\star}(\\theta_2), \\theta_2) = 0$ with respect to $\\theta_2$:\n$$\n\\frac{\\partial^2 J}{\\partial \\theta_1^2} \\frac{d\\theta_1^{\\star}}{d\\theta_2} + \\frac{\\partial^2 J}{\\partial \\theta_2 \\partial \\theta_1} = 0 \\implies \\frac{d\\theta_1^{\\star}}{d\\theta_2} = - \\left(\\frac{\\partial^2 J}{\\partial \\theta_1^2}\\right)^{-1} \\frac{\\partial^2 J}{\\partial \\theta_1 \\partial \\theta_2}\n$$\nSubstituting this back, we get the formula for the curvature in terms of the Hessian components of $J$:\n$$\n\\frac{d^2 J_{\\mathrm{p}}}{d\\theta_2^2} = \\frac{\\partial^2 J}{\\partial \\theta_2^2} - \\frac{\\left(\\frac{\\partial^2 J}{\\partial \\theta_1 \\partial \\theta_2}\\right)^2}{\\frac{\\partial^2 J}{\\partial \\theta_1^2}}\n$$\nWe need to evaluate this at $\\theta_2 = \\widehat{\\theta}_2$. The point $(\\widehat{\\theta}_1, \\widehat{\\theta}_2)$ where $\\widehat{\\theta}_1 = \\theta_1^{\\star}(\\widehat{\\theta}_2)$ is a minimizer of the full cost function $J(\\theta_1, \\theta_2)$. Thus, the gradient of $J$ vanishes at this point. In particular, $\\left.\\frac{\\partial J}{\\partial \\theta_2}\\right|_{(\\widehat{\\theta}_1, \\widehat{\\theta}_2)} = 0$, which implies:\n$$\n\\sum_{i=1}^{n} (y_i - \\widehat{\\theta}_1 \\exp(-\\widehat{\\theta}_2 t_i)) t_i \\exp(-\\widehat{\\theta}_2 t_i) = 0 \\implies \\sum_{i=1}^{n} y_i t_i \\exp(-\\widehat{\\theta}_2 t_i) = \\widehat{\\theta}_1 \\sum_{i=1}^{n} t_i \\exp(-2\\widehat{\\theta}_2 t_i)\n$$\nWe now compute the necessary second partial derivatives of $J$:\n$$\nJ_{\\theta_1\\theta_1} = \\frac{\\partial^2 J}{\\partial \\theta_1^2} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)\n$$\n$$\nJ_{\\theta_1\\theta_2} = \\frac{\\partial^2 J}{\\partial \\theta_1 \\partial \\theta_2} = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (y_i t_i \\exp(-\\theta_2 t_i) - 2\\theta_1 t_i \\exp(-2\\theta_2 t_i))\n$$\nAt $(\\widehat{\\theta}_1, \\widehat{\\theta}_2)$, using the optimality condition derived above:\n$$\n\\left.J_{\\theta_1\\theta_2}\\right|_{(\\widehat{\\theta}_1, \\widehat{\\theta}_2)} = \\frac{1}{\\sigma^2} \\left( \\widehat{\\theta}_1 \\sum_{i=1}^{n} t_i \\exp(-2\\widehat{\\theta}_2 t_i) - 2\\widehat{\\theta}_1 \\sum_{i=1}^{n} t_i \\exp(-2\\widehat{\\theta}_2 t_i) \\right) = -\\frac{\\widehat{\\theta}_1}{\\sigma^2} \\sum_{i=1}^{n} t_i \\exp(-2\\widehat{\\theta}_2 t_i)\n$$\n$$\nJ_{\\theta_2\\theta_2} = \\frac{\\partial^2 J}{\\partial \\theta_2^2} = \\frac{\\partial}{\\partial \\theta_2} \\left( \\frac{\\theta_1}{\\sigma^2} \\sum_{i=1}^n (y_i t_i e^{-\\theta_2 t_i} - \\theta_1 t_i e^{-2\\theta_2 t_i}) \\right) = -\\frac{\\theta_1}{\\sigma^2} \\sum_{i=1}^n y_i t_i^2 e^{-\\theta_2 t_i} + \\frac{2\\theta_1^2}{\\sigma^2} \\sum_{i=1}^n t_i^2 e^{-2\\theta_2 t_i}\n$$\nNow, assembling the curvature expression and evaluating at $\\theta_2 = \\widehat{\\theta}_2$ (so $\\theta_1 = \\widehat{\\theta}_1$):\n$$\n\\left.\\frac{d^2 J_{\\mathrm{p}}}{d\\theta_2^2}\\right|_{\\widehat{\\theta}_2} = \\left( -\\frac{\\widehat{\\theta}_1}{\\sigma^2} \\sum_{i=1}^n y_i t_i^2 e^{-\\widehat{\\theta}_2 t_i} + \\frac{2\\widehat{\\theta}_1^2}{\\sigma^2} \\sum_{i=1}^n t_i^2 e^{-2\\widehat{\\theta}_2 t_i} \\right) - \\frac{\\left(-\\frac{\\widehat{\\theta}_1}{\\sigma^2} \\sum_{i=1}^n t_i e^{-2\\widehat{\\theta}_2 t_i}\\right)^2}{\\frac{1}{\\sigma^2} \\sum_{i=1}^n e^{-2\\widehat{\\theta}_2 t_i}}\n$$\n$$\n= \\frac{1}{\\sigma^2} \\left( -\\widehat{\\theta}_1 \\sum_{i=1}^n y_i t_i^2 e^{-\\widehat{\\theta}_2 t_i} + 2\\widehat{\\theta}_1^2 \\sum_{i=1}^n t_i^2 e^{-2\\widehat{\\theta}_2 t_i} - \\widehat{\\theta}_1^2 \\frac{\\left( \\sum_{i=1}^n t_i e^{-2\\widehat{\\theta}_2 t_i} \\right)^2}{\\sum_{i=1}^n e^{-2\\widehat{\\theta}_2 t_i}} \\right)\n$$\nFinally, we substitute $\\widehat{\\theta}_1 = \\theta_1^{\\star}(\\widehat{\\theta}_2) = \\frac{\\sum_{j=1}^n y_j \\exp(-\\widehat{\\theta}_2 t_j)}{\\sum_{k=1}^n \\exp(-2\\widehat{\\theta}_2 t_k)}$ to express the result entirely in terms of the data, $\\sigma^2$, and $\\widehat{\\theta}_2$:\n$$\n\\frac{1}{\\sigma^2} \\left[ \\left(\\frac{\\sum_{j=1}^n y_j e^{-\\widehat{\\theta}_2 t_j}}{\\sum_{k=1}^n e^{-2\\widehat{\\theta}_2 t_k}}\\right) \\left(-\\sum_{i=1}^n y_i t_i^2 e^{-\\widehat{\\theta}_2 t_i}\\right) + \\left(\\frac{\\sum_{j=1}^n y_j e^{-\\widehat{\\theta}_2 t_j}}{\\sum_{k=1}^n e^{-2\\widehat{\\theta}_2 t_k}}\\right)^2 \\left(2\\sum_{i=1}^n t_i^2 e^{-2\\widehat{\\theta}_2 t_i} - \\frac{(\\sum_{i=1}^n t_i e^{-2\\widehat{\\theta}_2 t_i})^2}{\\sum_{i=1}^n e^{-2\\widehat{\\theta}_2 t_i}}\\right) \\right]\n$$\nThis is the required explicit expression.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^{n} y_i^2 - \\frac{\\left(\\sum_{i=1}^{n} y_i \\exp(-\\theta_2 t_i)\\right)^2}{\\sum_{i=1}^{n} \\exp(-2\\theta_2 t_i)} \\right)  \\frac{1}{\\sigma^2} \\left[ \\left(\\frac{\\sum_{j=1}^n y_j \\exp(-\\widehat{\\theta}_2 t_j)}{\\sum_{k=1}^n \\exp(-2\\widehat{\\theta}_2 t_k)}\\right)^2 \\left(2\\sum_{i=1}^n t_i^2 \\exp(-2\\widehat{\\theta}_2 t_i) - \\frac{\\left(\\sum_{i=1}^n t_i \\exp(-2\\widehat{\\theta}_2 t_i)\\right)^2}{\\sum_{i=1}^n \\exp(-2\\widehat{\\theta}_2 t_i)}\\right) - \\left(\\frac{\\sum_{j=1}^n y_j \\exp(-\\widehat{\\theta}_2 t_j)}{\\sum_{k=1}^n \\exp(-2\\widehat{\\theta}_2 t_k)}\\right) \\sum_{i=1}^n y_i t_i^2 \\exp(-\\widehat{\\theta}_2 t_i) \\right] \\end{pmatrix}}\n$$", "id": "3426692"}]}