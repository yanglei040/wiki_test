{"hands_on_practices": [{"introduction": "A crucial first step in applying Hamiltonian Monte Carlo is defining a smooth, differentiable potential energy function, which is typically the negative log-posterior. Many real-world inverse problems involve parameters with physical constraints, such as positivity or box constraints, which violate this requirement. This exercise demonstrates how to handle such boundaries elegantly using a logarithmic barrier potential, transforming a constrained problem into an unconstrained one that is suitable for HMC's gradient-based exploration. Mastering this technique [@problem_id:3388143] is essential for applying HMC to a wide range of practical models.", "problem": "Consider a Bayesian inverse problem in which the unknown parameter vector $u \\in \\mathbb{R}^{n}$ is subject to box constraints $a_{i}  u_{i}  b_{i}$ for each component $i=1,\\dots,n$, with known bounds $a_{i} \\in \\mathbb{R}$ and $b_{i} \\in \\mathbb{R}$ satisfying $a_{i}  b_{i}$. In Hamiltonian Monte Carlo (HMC), the potential energy is typically the negative log posterior. To encode the constraints in a differentiable manner suitable for HMC, a standard approach is to add a logarithmic barrier potential\n$$\nU_{\\mathrm{bar}}(u) = \\sum_{i=1}^{n} \\left( -\\ln\\big(b_{i} - u_{i}\\big) - \\ln\\big(u_{i} - a_{i}\\big) \\right),\n$$\ndefined on the feasible set $\\{u \\in \\mathbb{R}^{n} : a_{i}  u_{i}  b_{i} \\ \\text{for all } i\\}$.\n\nStarting from calculus fundamentals and the definitions of the gradient and Hessian,\n$$\n\\nabla U_{\\mathrm{bar}}(u) = \\left( \\frac{\\partial U_{\\mathrm{bar}}}{\\partial u_{1}}, \\dots, \\frac{\\partial U_{\\mathrm{bar}}}{\\partial u_{n}} \\right)^{\\top}, \\qquad \\nabla^{2} U_{\\mathrm{bar}}(u) = \\left[ \\frac{\\partial^{2} U_{\\mathrm{bar}}}{\\partial u_{i} \\partial u_{j}} \\right]_{i,j=1}^{n},\n$$\nderive explicit closed-form expressions for the gradient vector and the Hessian matrix of $U_{\\mathrm{bar}}(u)$ on the feasible set. Ensure your derivation is fully justified from first principles (e.g., rules for derivatives of compositions and the natural logarithm). Then, state the final expressions for the gradient and the Hessian.\n\nYour final answer must be a single closed-form analytic expression and must consist only of the gradient and Hessian expressions. If you provide multiple expressions, arrange them as a single row using the LaTeX `pmatrix` environment. No numerical evaluation or rounding is required.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in standard multivariable calculus and its application to Bayesian statistics, specifically in the context of Hamiltonian Monte Carlo methods. The problem is well-posed, self-contained, and all terms are formally and unambiguously defined. There are no contradictions, factual errors, or infeasible conditions. We may therefore proceed with the derivation.\n\nThe objective is to derive the gradient vector $\\nabla U_{\\mathrm{bar}}(u)$ and the Hessian matrix $\\nabla^{2} U_{\\mathrm{bar}}(u)$ for the logarithmic barrier potential $U_{\\mathrm{bar}}(u)$. The potential is defined for a parameter vector $u \\in \\mathbb{R}^{n}$ with components $u_i$ constrained by $a_{i}  u_{i}  b_{i}$ for $i=1, \\dots, n$. The function is given by:\n$$\nU_{\\mathrm{bar}}(u) = \\sum_{i=1}^{n} \\left( -\\ln\\big(b_{i} - u_{i}\\big) - \\ln\\big(u_{i} - a_{i}\\big) \\right)\n$$\nThe constraints $a_{i}  u_{i}  b_{i}$ ensure that the arguments of the natural logarithm, $b_{i} - u_{i}$ and $u_{i} - a_{i}$, are strictly positive, guaranteeing that the function is well-defined and real-valued on the feasible set.\n\n**1. Derivation of the Gradient Vector**\n\nThe gradient vector $\\nabla U_{\\mathrm{bar}}(u)$ is a vector whose components are the partial derivatives of $U_{\\mathrm{bar}}(u)$ with respect to each variable $u_k$. The $k$-th component of the gradient is given by $\\frac{\\partial U_{\\mathrm{bar}}}{\\partial u_{k}}$ for $k \\in \\{1, \\dots, n\\}$.\n\nWe apply the partial derivative operator $\\frac{\\partial}{\\partial u_k}$ to the expression for $U_{\\mathrm{bar}}(u)$:\n$$\n\\frac{\\partial U_{\\mathrm{bar}}}{\\partial u_{k}} = \\frac{\\partial}{\\partial u_{k}} \\left[ \\sum_{i=1}^{n} \\left( -\\ln\\big(b_{i} - u_{i}\\big) - \\ln\\big(u_{i} - a_{i}\\big) \\right) \\right]\n$$\nBy the sum rule of differentiation, the derivative of a sum is the sum of the derivatives. We can move the partial derivative operator inside the summation:\n$$\n\\frac{\\partial U_{\\mathrm{bar}}}{\\partial u_{k}} = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial u_{k}} \\left( -\\ln\\big(b_{i} - u_{i}\\big) - \\ln\\big(u_{i} - a_{i}\\big) \\right)\n$$\nThe term inside the summation depends only on the variable $u_i$. Therefore, its partial derivative with respect to $u_k$ is zero for all $i \\neq k$. The only non-zero contribution to the sum comes from the term where $i=k$.\n$$\n\\frac{\\partial U_{\\mathrm{bar}}}{\\partial u_{k}} = \\frac{\\partial}{\\partial u_{k}} \\left( -\\ln\\big(b_{k} - u_{k}\\big) - \\ln\\big(u_{k} - a_{k}\\big) \\right)\n$$\nWe differentiate each term separately. From first principles of calculus, the derivative of the natural logarithm function $\\ln(x)$ is $\\frac{1}{x}$. We use the chain rule, which states that for a composite function $f(g(x))$, its derivative is $f'(g(x))g'(x)$.\n\nFor the first term, $-\\ln(b_{k} - u_{k})$, let $f(v) = -\\ln(v)$ and $v(u_k) = b_k - u_k$. Then $f'(v) = -\\frac{1}{v}$ and $v'(u_k) = -1$. Applying the chain rule:\n$$\n\\frac{\\partial}{\\partial u_{k}} \\left( -\\ln\\big(b_{k} - u_{k}\\big) \\right) = \\left( -\\frac{1}{b_{k} - u_{k}} \\right) \\cdot (-1) = \\frac{1}{b_{k} - u_{k}}\n$$\nFor the second term, $-\\ln(u_{k} - a_{k})$, let $f(v) = -\\ln(v)$ and $v(u_k) = u_k - a_k$. Then $f'(v) = -\\frac{1}{v}$ and $v'(u_k) = 1$. Applying the chain rule:\n$$\n\\frac{\\partial}{\\partial u_{k}} \\left( -\\ln\\big(u_{k} - a_{k}\\big) \\right) = \\left( -\\frac{1}{u_{k} - a_{k}} \\right) \\cdot (1) = -\\frac{1}{u_{k} - a_{k}}\n$$\nCombining these results gives the $k$-th component of the gradient:\n$$\n\\frac{\\partial U_{\\mathrm{bar}}}{\\partial u_{k}} = \\frac{1}{b_{k} - u_{k}} - \\frac{1}{u_{k} - a_{k}}\n$$\nThe full gradient vector $\\nabla U_{\\mathrm{bar}}(u)$ is thus a column vector where the $k$-th entry is the expression above.\n\n**2. Derivation of the Hessian Matrix**\n\nThe Hessian matrix $\\nabla^{2} U_{\\mathrm{bar}}(u)$ is an $n \\times n$ matrix whose entry at the $j$-th row and $k$-th column is the second-order partial derivative $\\frac{\\partial^{2} U_{\\mathrm{bar}}}{\\partial u_{j} \\partial u_{k}}$. We can find this by differentiating the components of the gradient we just derived:\n$$\n\\left[ \\nabla^{2} U_{\\mathrm{bar}}(u) \\right]_{jk} = \\frac{\\partial^{2} U_{\\mathrm{bar}}}{\\partial u_{j} \\partial u_{k}} = \\frac{\\partial}{\\partial u_{j}} \\left( \\frac{\\partial U_{\\mathrm{bar}}}{\\partial u_{k}} \\right) = \\frac{\\partial}{\\partial u_{j}} \\left( \\frac{1}{b_{k} - u_{k}} - \\frac{1}{u_{k} - a_{k}} \\right)\n$$\nWe consider two cases for the indices $j$ and $k$.\n\nCase 1: Off-diagonal elements ($j \\neq k$).\nThe expression for $\\frac{\\partial U_{\\mathrm{bar}}}{\\partial u_{k}}$ depends only on the variable $u_k$. Therefore, its partial derivative with respect to any other variable $u_j$ (where $j \\neq k$) is zero.\n$$\n\\left[ \\nabla^{2} U_{\\mathrm{bar}}(u) \\right]_{jk} = 0 \\quad \\text{for } j \\neq k\n$$\nThis implies that the Hessian matrix is a diagonal matrix.\n\nCase 2: Diagonal elements ($j = k$).\nWe need to compute the second partial derivative with respect to $u_k$:\n$$\n\\left[ \\nabla^{2} U_{\\mathrm{bar}}(u) \\right]_{kk} = \\frac{\\partial^{2} U_{\\mathrm{bar}}}{\\partial u_{k}^{2}} = \\frac{\\partial}{\\partial u_{k}} \\left( \\frac{1}{b_{k} - u_{k}} - \\frac{1}{u_{k} - a_{k}} \\right)\n$$\nWe rewrite the terms using negative exponents to apply the power rule, $(x^p)' = p x^{p-1}$, in conjunction with the chain rule.\n$$\n\\frac{\\partial^{2} U_{\\mathrm{bar}}}{\\partial u_{k}^{2}} = \\frac{\\partial}{\\partial u_{k}} \\left( (b_k - u_k)^{-1} - (u_k - a_k)^{-1} \\right)\n$$\nDifferentiating the first term:\n$$\n\\frac{\\partial}{\\partial u_{k}} \\left( (b_{k} - u_{k})^{-1} \\right) = -1 \\cdot (b_{k} - u_{k})^{-2} \\cdot \\frac{\\partial}{\\partial u_k}(b_k - u_k) = -1 \\cdot (b_{k} - u_{k})^{-2} \\cdot (-1) = \\frac{1}{(b_{k} - u_{k})^{2}}\n$$\nDifferentiating the second term:\n$$\n\\frac{\\partial}{\\partial u_{k}} \\left( -(u_{k} - a_{k})^{-1} \\right) = - \\left( -1 \\cdot (u_{k} - a_{k})^{-2} \\cdot \\frac{\\partial}{\\partial u_k}(u_k - a_k) \\right) = (u_{k} - a_{k})^{-2} \\cdot (1) = \\frac{1}{(u_{k} - a_{k})^{2}}\n$$\nCombining these results gives the diagonal elements of the Hessian:\n$$\n\\left[ \\nabla^{2} U_{\\mathrm{bar}}(u) \\right]_{kk} = \\frac{1}{(b_{k} - u_{k})^{2}} + \\frac{1}{(u_{k} - a_{k})^{2}}\n$$\nThe Hessian matrix is therefore a diagonal matrix with these entries along its main diagonal.\n\n**Summary of Results**\n\nThe gradient of $U_{\\mathrm{bar}}(u)$ is a a vector whose components are functions of the corresponding components of $u$:\n$$\n\\nabla U_{\\mathrm{bar}}(u) = \\begin{pmatrix}\n\\frac{1}{b_{1} - u_{1}} - \\frac{1}{u_{1} - a_{1}} \\\\\n\\vdots \\\\\n\\frac{1}{b_{n} - u_{n}} - \\frac{1}{u_{n} - a_{n}}\n\\end{pmatrix}\n$$\nThe Hessian of $U_{\\mathrm{bar}}(u)$ is a diagonal matrix, which can be denoted using the $\\mathrm{diag}(\\cdot)$ operator:\n$$\n\\nabla^2 U_{\\mathrm{bar}}(u) = \\mathrm{diag}\\left( \\frac{1}{(b_{1} - u_{1})^{2}} + \\frac{1}{(u_{1} - a_{1})^{2}}, \\dots, \\frac{1}{(b_{n} - u_{n})^{2}} + \\frac{1}{(u_{n} - a_{n})^{2}} \\right)\n$$\nThese are the explicit closed-form expressions for the gradient and the Hessian of the given logarithmic barrier potential.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\nabla U_{\\mathrm{bar}}(u)  \\nabla^2 U_{\\mathrm{bar}}(u)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\left( \\frac{1}{b_{k} - u_{k}} - \\frac{1}{u_{k} - a_{k}} \\right)_{k=1}^{n}\n\n\\mathrm{diag}\\left( \\frac{1}{(b_{k} - u_{k})^{2}} + \\frac{1}{(u_{k} - a_{k})^{2}} \\right)_{k=1}^{n}\n\\end{pmatrix}\n}\n$$", "id": "3388143"}, {"introduction": "An effective HMC sampler does more than just follow Hamiltonian dynamics; it navigates the potentially complex geometry of the posterior distribution efficiently. The mass matrix $M$ in the kinetic energy term is a powerful tuning parameter that can be adapted to the local curvature of the potential energy surface. This practice [@problem_id:3388124] delves into the art of preconditioning by choosing an appropriate mass matrix to improve sampling performance, particularly for navigating the challenging saddle points that often separate modes in a multimodal distribution.", "problem": "Consider a posterior distribution in an inverse problem for data assimilation with state vector $q \\in \\mathbb{R}^d$ and negative log posterior $U(q)$, so that the target density is proportional to $\\exp(-U(q))$. Hamiltonian Monte Carlo (HMC) with position $q$ and momentum $p$ uses the Hamiltonian $H(q,p) = U(q) + \\tfrac{1}{2} p^\\top M^{-1} p$, where $M \\succ 0$ is the mass matrix. The continuous-time Hamiltonian dynamics are $\\dot{q} = M^{-1} p$ and $\\dot{p} = - \\nabla U(q)$. Assume that $U$ has two modes separated by a saddle point $q_\\star$, and that the Hessian $H_\\star = \\nabla^2 U(q_\\star)$ is symmetric and indefinite with eigen-decomposition $H_\\star = Q \\Lambda Q^\\top$, where $\\Lambda = \\operatorname{diag}(\\lambda_1,\\dots,\\lambda_d)$ has at least one negative eigenvalue and at least one positive eigenvalue.\n\nFocus on the local behavior of HMC trajectories near $q_\\star$. Linearize the dynamics around $q_\\star$ by writing $x = q - q_\\star$ and approximating $\\nabla U(q) \\approx H_\\star x$. For a symmetric positive definite mass matrix $M$, the resulting linearized equation for $x$ is $\\ddot{x} = - M^{-1} H_\\star x$. Suppose you are allowed to choose $M$ using only information at $q_\\star$ to improve the ability of HMC to cross the saddle and move between the modes while maintaining good numerical stability for a symplectic integrator with step size $h$.\n\nWhich of the following mass matrix choices best facilitates crossing the saddle by equalizing local time scales across the eigen-directions of $H_\\star$ and keeping the linearized dynamics well-conditioned, and why?\n\nA. Choose $M = H_\\star^{-1}$ so that all linearized modes are purely oscillatory with equal frequency, which stabilizes integration and avoids exponential growth in unstable directions.\n\nB. Choose $M = |H_\\star| := Q \\, \\operatorname{diag}(|\\lambda_1|,\\dots,|\\lambda_d|) \\, Q^\\top$ so that in the eigen-basis of $H_\\star$ each coordinate obeys either $\\ddot{z}_i = - z_i$ if $\\lambda_i  0$ or $\\ddot{z}_i = + z_i$ if $\\lambda_i  0$, thereby equalizing local oscillation and growth rates to unit magnitude and reducing stiffness.\n\nC. Keep $M = I$ because the Hamiltonian flow preserves energy and will cross the saddle whenever the total energy exceeds the barrier height, making preconditioning unnecessary.\n\nD. Choose $M = |H_\\star|^{-1} := Q \\, \\operatorname{diag}(|\\lambda_1|^{-1},\\dots,|\\lambda_d|^{-1}) \\, Q^\\top$ so that the linearized rates $\\sqrt{\\lambda_i^2}$ are large in high-curvature directions, increasing momentum in those directions and accelerating transitions across the saddle.\n\nProvide the single best choice and justify it by deriving the local decoupled dynamics in an eigen-basis and analyzing the resulting time scales for oscillatory ($\\lambda_i  0$) and unstable ($\\lambda_i  0$) directions. Your justification should start from the definitions of HMC dynamics and the linearization near $q_\\star$, without relying on pre-stated specialized formulas. Assume a fixed small integrator step size $h$ must be chosen to control discretization error across all directions simultaneously, and recall that a symplectic integrator for a harmonic oscillator with frequency $\\omega$ is stable only if $h \\, \\omega$ is not too large. Answer by selecting one option among A, B, C, and D.", "solution": "The problem asks for the best choice of a mass matrix $M$ to precondition HMC dynamics near a saddle point $q_\\star$. The goal is to equalize the time scales of the motion along different directions to improve numerical stability and efficiency.\n\n**1. Linearized Dynamics in an Eigen-basis**\n\nThe linearized dynamics for a small displacement $x = q - q_\\star$ are given by $\\ddot{x} = - M^{-1} H_\\star x$. To analyze this system, we change to the coordinate system defined by the eigenvectors of the Hessian $H_\\star = Q \\Lambda Q^\\top$. Let $x = Qz$, where $z$ represents the displacement in the eigen-basis. Substituting this into the equation of motion gives:\n$$ Q\\ddot{z} = - M^{-1} (Q \\Lambda Q^\\top) (Qz) = -M^{-1} Q \\Lambda z $$\nMultiplying by $Q^\\top$ from the left yields:\n$$ \\ddot{z} = - (Q^\\top M^{-1} Q) \\Lambda z $$\nFor the dynamics to decouple along the eigen-directions $z_i$, the matrix $Q^\\top M^{-1} Q$ must be diagonal. All proposed choices for $M$ (except the identity) are of the form $M = Q D Q^\\top$ for some diagonal matrix $D$. For such a choice, $M^{-1} = Q D^{-1} Q^\\top$, and thus $Q^\\top M^{-1} Q = D^{-1}$. The decoupled dynamics for each component $z_i$ become:\n$$ \\ddot{z}_i = - (D^{-1})_{ii} \\lambda_i z_i $$\n\n**2. Analysis of Options**\n\nWe now analyze each choice for $M$ based on the resulting dynamics.\n\n*   **A. Choose $M = H_\\star^{-1}$**: This choice is invalid. The mass matrix $M$ must be symmetric positive definite. Since $H_\\star$ is the Hessian at a saddle point, it is indefinite (has both positive and negative eigenvalues $\\lambda_i$). Therefore, $H_\\star^{-1}$ is also indefinite and cannot be used as a mass matrix.\n\n*   **B. Choose $M = |H_\\star| := Q \\operatorname{diag}(|\\lambda_1|,\\dots,|\\lambda_d|) Q^\\top$**: This choice is a valid positive definite mass matrix (assuming no zero eigenvalues). Here, the diagonal matrix $D$ is $\\operatorname{diag}(|\\lambda_1|,\\dots,|\\lambda_d|)$, so its inverse is $D^{-1} = \\operatorname{diag}(|\\lambda_1|^{-1},\\dots,|\\lambda_d|^{-1})$. The dynamics for each component become:\n    $$ \\ddot{z}_i = - (|\\lambda_i|^{-1}) \\lambda_i z_i $$\n    - If $\\lambda_i > 0$ (stable direction): $\\ddot{z}_i = - (|\\lambda_i|^{-1}) |\\lambda_i| z_i = -z_i$. This is a harmonic oscillator with unit frequency $\\omega_i = 1$.\n    - If $\\lambda_i  0$ (unstable direction): $\\ddot{z}_i = - (|\\lambda_i|^{-1}) (-|\\lambda_i|) z_i = +z_i$. This corresponds to exponential growth/decay with a unit rate $\\gamma_i = 1$.\n    This choice successfully normalizes all time scales (frequencies and growth rates) to unity. This eliminates stiffness, making the system well-conditioned for a numerical integrator with a fixed step size. This is the correct approach.\n\n*   **C. Keep $M = I$**: This is the un-preconditioned case. The dynamics are $\\ddot{z}_i = - \\lambda_i z_i$. The time scales are $\\sqrt{|\\lambda_i|}$. If the eigenvalues $|\\lambda_i|$ span a large range, the system is stiff, forcing a very small integration step size to resolve the fastest mode, which is inefficient for the slower modes. This does not meet the goal of equalizing time scales.\n\n*   **D. Choose $M = |H_\\star|^{-1} := Q \\operatorname{diag}(|\\lambda_1|^{-1},\\dots,|\\lambda_d|^{-1}) Q^\\top$**: This is a valid positive definite mass matrix. Here, $D = \\operatorname{diag}(|\\lambda_1|^{-1},\\dots,|\\lambda_d|^{-1})$, so $D^{-1} = \\operatorname{diag}(|\\lambda_1|,\\dots,|\\lambda_d|)$. The dynamics become:\n    $$ \\ddot{z}_i = - |\\lambda_i| \\lambda_i z_i $$\n    The time scales become proportional to $|\\lambda_i|$ (frequency or growth rate). This would amplify any existing disparity in the magnitudes of the eigenvalues, making the stiffness problem worse, not better.\n\n**Conclusion**\n\nChoice B is the only one that correctly preconditions the dynamics by transforming all local time scales to unity, thus reducing stiffness and improving the stability and efficiency of the HMC sampler near the saddle point.", "answer": "$$\\boxed{B}$$", "id": "3388124"}, {"introduction": "This final practice serves as a capstone, guiding you through the end-to-end implementation of HMC for a realistic inverse problem in tomographic imaging. Starting from a complex, non-Gaussian prior designed for edge preservation (the Student-$t$ distribution), you will derive the necessary gradients and Hessians and then analyze the stability of the leapfrog integrator to intelligently set its step size $h$. By translating theory into running code [@problem_id:3388099], this exercise solidifies your understanding of how HMC's core components—the potential, the mass matrix, and the integrator—work together to solve challenging scientific problems.", "problem": "Consider a Bayesian inverse problem for one-dimensional edge-preserving tomography, modeled as a linear forward map with additive Gaussian noise. Let $u \\in \\mathbb{R}^n$ denote the discretized image (voxel intensities), $A \\in \\mathbb{R}^{n \\times n}$ a known linear operator representing the tomography forward map, and $y \\in \\mathbb{R}^n$ the observed data. The additive noise in the data is modeled as independent and identically distributed Gaussian with zero mean and variance $\\sigma_\\ell^2$. The likelihood is the Gaussian $\\mathcal{N}(A u, \\sigma_\\ell^2 I)$, and the negative log-likelihood is\n$$\nU_{\\text{like}}(u) = \\frac{1}{2 \\sigma_\\ell^2} \\|A u - y\\|_2^2,\n$$\nwhere $\\|\\cdot\\|_2$ denotes the Euclidean norm.\n\nTo promote edge preservation in the reconstruction and robustness to outliers, we place a heavy-tailed Student-$t$ prior on the discrete spatial gradients of $u$. Let $G \\in \\mathbb{R}^{(n-1)\\times n}$ denote the one-dimensional forward difference matrix, so that $g = G u \\in \\mathbb{R}^{n-1}$ are the nearest-neighbor differences of $u$. The Student-$t$ prior is applied to a smoothed absolute-magnitude of these gradients. Define\n$$\nr_i = \\sqrt{g_i^2 + \\varepsilon^2}, \\quad \\text{for } i = 1,\\dots,n-1,\n$$\nwhere $\\varepsilon  0$ is a small smoothing parameter controlling differentiability near zero. For degrees of freedom $\\nu  0$ and scale $s  0$, define the prior negative log-density\n$$\nU_{\\text{prior}}(u) = \\alpha \\sum_{i=1}^{n-1} \\log\\!\\left(1 + \\frac{r_i}{\\nu s}\\right), \\quad \\text{with } \\alpha = \\frac{\\nu + 1}{2}.\n$$\nThe posterior negative log-density (potential energy) is then\n$$\nU(u) = U_{\\text{like}}(u) + U_{\\text{prior}}(u).\n$$\n\nHamiltonian Monte Carlo (HMC) introduces an auxiliary momentum $p \\in \\mathbb{R}^n$ with a standard Gaussian kinetic energy $K(p) = \\frac{1}{2}\\|p\\|_2^2$ and simulates approximate Hamiltonian dynamics for the Hamiltonian $H(u,p) = U(u) + K(p)$ using the symplectic Störmer–Verlet (leapfrog) integrator. For the leapfrog scheme with step size $h  0$ and unit mass matrix, one step maps $(u^{(0)}, p^{(0)})$ to $(u^{(1)}, p^{(1)})$ according to\n- Half-step momentum update: $p^{(1/2)} = p^{(0)} - \\frac{h}{2} \\nabla U(u^{(0)})$,\n- Full-step position update: $u^{(1)} = u^{(0)} + h \\, p^{(1/2)}$,\n- Half-step momentum update: $p^{(1)} = p^{(1/2)} - \\frac{h}{2} \\nabla U(u^{(1)})$.\nThe leapfrog is second-order, time-reversible, and volume-preserving. For a quadratic potential energy $U(u) = \\frac{1}{2} u^\\top H u$ with symmetric positive semidefinite $H$, the leapfrog method is linearly stable if $h \\sqrt{\\lambda_{\\max}}  2$, where $\\lambda_{\\max}$ is the largest eigenvalue of $H$. For a general smooth $U(u)$, a principled local linearization about a state $u$ leads to an approximate quadratic potential with Hessian $H(u) = \\nabla^2 U(u)$. A conservative local stability bound is then given by\n$$\nh_{\\max}(u) = \\begin{cases}\n\\frac{2}{\\sqrt{\\lambda_{\\max}^+(u)}},  \\text{if } \\lambda_{\\max}^+(u)  0, \\\\\n+\\infty,  \\text{if } \\lambda_{\\max}^+(u) = 0,\n\\end{cases}\n$$\nwhere $\\lambda_{\\max}^+(u)$ is the largest nonnegative eigenvalue of the symmetric Hessian $H(u)$, and the presence of any negative eigenvalues indicates local hyperbolic directions (non-oscillatory growth) making leapfrog nonlinearly unstable in those directions for any step size.\n\nStarting from fundamental rules:\n- Bayes’ rule defines the posterior density up to proportionality as the product of the likelihood and the prior.\n- The negative log-posterior $U(u)$ is the sum of the negative log-likelihood and negative log-prior.\n- The gradient and Hessian of $U(u)$ govern local dynamics and stability under HMC via leapfrog discretization.\n- For Gaussian likelihood with linear map $A$ and Student-$t$ prior as specified, the gradient and Hessian can be derived using the chain rule and properties of matrix calculus.\n\nTasks:\n1. Derive the gradient $\\nabla U(u)$ and Hessian $H(u) = \\nabla^2 U(u)$ of the posterior potential $U(u)$ in terms of $A$, $y$, $G$, $\\sigma_\\ell$, $\\nu$, $s$, and $\\varepsilon$. Your derivation must clearly show the contribution from the likelihood and the prior, starting from the above definitions.\n2. From the local linearization principle for Hamiltonian dynamics with unit mass matrix, justify the local leapfrog stability bound $h_{\\max}(u) = 2/\\sqrt{\\lambda_{\\max}^+(u)}$ in terms of the largest positive eigenvalue of the Hessian $H(u)$.\n3. Design an algorithm that, for a given $u$, computes:\n   - The posterior potential $U(u)$,\n   - Its gradient $\\nabla U(u)$,\n   - Its Hessian $H(u)$,\n   - The largest eigenvalue $\\lambda_{\\max}^+(u)$ and a boolean indicating whether $H(u)$ is positive semidefinite,\n   - The local stability bound $h_{\\max}(u)$,\n   - The one-step leapfrog energy error $\\Delta H = H(u^{(1)}, p^{(1)}) - H(u^{(0)}, p^{(0)})$ using initial momentum $p^{(0)} = 0$ and step size $h = 0.9 \\, h_{\\max}(u)$ if $h_{\\max}(u)$ is finite; otherwise set $h = 1$.\n4. Implement the algorithm as a complete, runnable program. Use $n = 32$, construct $A$ as a tridiagonal blur with main diagonal $0.6$ and first off-diagonals $0.2$, and use zero boundary conditions. Let $G$ be the forward difference operator: $(G u)_i = u_{i+1} - u_i$. Define a ground truth $u_{\\text{true}}$ as a step function with $u_{\\text{true},i} = 0$ for $i \\le 16$ and $u_{\\text{true},i} = 1$ for $i  16$. Define $y = A u_{\\text{true}}$. Let $u_0$ be the zero vector and $u_{\\text{tail}} = 2 \\, u_{\\text{true}}$.\n5. Using the following test suite of parameter sets $(\\nu, s, \\varepsilon, \\sigma_\\ell)$, compute, for each parameter set, the six outputs:\n   - $h_{\\max}(u_0)$,\n   - $h_{\\max}(u_{\\text{tail}})$,\n   - a boolean indicating whether $H(u_0)$ is positive semidefinite,\n   - a boolean indicating whether $H(u_{\\text{tail}})$ is positive semidefinite,\n   - the one-step leapfrog energy error at $u_0$ with step size as specified,\n   - the one-step leapfrog energy error at $u_{\\text{tail}}$ with step size as specified.\n   The test suite is:\n   - Case 1 (happy path, moderately heavy-tailed): $(\\nu, s, \\varepsilon, \\sigma_\\ell) = (3.0, 0.2, 10^{-2}, 0.05)$,\n   - Case 2 (near non-smooth tails, heavier tails): $(\\nu, s, \\varepsilon, \\sigma_\\ell) = (1.5, 0.1, 10^{-6}, 0.2)$,\n   - Case 3 (extreme heavy-tail behavior, weak data curvature): $(\\nu, s, \\varepsilon, \\sigma_\\ell) = (1.0, 0.05, 10^{-8}, 20.0)$.\n6. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the six outputs for Case 1, then Case 2, then Case 3. All outputs must be real numbers or booleans, with energy errors expressed as floats. For angles, no angles are involved. For physical units, none are involved. For any ratio or fraction, express as a decimal.\n\nConstraints and derivation requirements:\n- Begin derivations from the definitions provided, using Bayes’ rule and standard facts of matrix calculus and calculus of variations.\n- Do not use shortcut formulas without justification; show how each term arises from the fundamental definitions.\n- Ensure all quantities are scientifically sound and numerically plausible.", "solution": "The problem is assessed to be valid. It is scientifically grounded in Bayesian statistics and numerical methods for Hamiltonian systems, well-posed, and self-contained. All necessary definitions, parameters, and test cases are provided for a complete analysis.\n\n### 1. Derivation of the Gradient and Hessian\n\nThe posterior negative log-density, or potential energy, $U(u)$, is the sum of the negative log-likelihood $U_{\\text{like}}(u)$ and the negative log-prior $U_{\\text{prior}}(u)$.\n$$\nU(u) = U_{\\text{like}}(u) + U_{\\text{prior}}(u)\n$$\nThe gradient $\\nabla U(u)$ and Hessian $H(u) = \\nabla^2 U(u)$ are similarly decomposed:\n$$\n\\nabla U(u) = \\nabla U_{\\text{like}}(u) + \\nabla U_{\\text{prior}}(u) \\\\\nH(u) = \\nabla^2 U_{\\text{like}}(u) + \\nabla^2 U_{\\text{prior}}(u)\n$$\nWe derive the contributions from the likelihood and the prior separately.\n\n#### Likelihood Contribution\nThe negative log-likelihood is given as:\n$$\nU_{\\text{like}}(u) = \\frac{1}{2 \\sigma_\\ell^2} \\|A u - y\\|_2^2 = \\frac{1}{2 \\sigma_\\ell^2} (A u - y)^\\top(A u - y)\n$$\nTo find the gradient, we use standard matrix calculus rules. Let $f(u) = A u - y$. Then $U_{\\text{like}}(u) = \\frac{1}{2 \\sigma_\\ell^2} f(u)^\\top f(u)$. The gradient is:\n$$\n\\nabla U_{\\text{like}}(u) = \\frac{1}{2 \\sigma_\\ell^2} \\left( \\left(\\frac{\\partial f}{\\partial u}\\right)^\\top f(u) + \\left(\\frac{\\partial f}{\\partial u}\\right)^\\top f(u) \\right) = \\frac{1}{\\sigma_\\ell^2} \\left(\\frac{\\partial f}{\\partial u}\\right)^\\top f(u)\n$$\nSince $\\frac{\\partial f}{\\partial u} = A$, the gradient is:\n$$\n\\nabla U_{\\text{like}}(u) = \\frac{1}{\\sigma_\\ell^2} A^\\top(A u - y)\n$$\nTo find the Hessian, we differentiate the gradient with respect to $u$:\n$$\nH_{\\text{like}}(u) = \\nabla^2 U_{\\text{like}}(u) = \\frac{\\partial}{\\partial u} \\left( \\frac{1}{\\sigma_\\ell^2} (A^\\top A u - A^\\top y) \\right) = \\frac{1}{\\sigma_\\ell^2} A^\\top A\n$$\nThe Hessian of the likelihood term is constant and positive semidefinite, as $A^\\top A$ is always positive semidefinite.\n\n#### Prior Contribution\nThe negative log-prior is given by:\n$$\nU_{\\text{prior}}(u) = \\alpha \\sum_{i=1}^{n-1} \\log\\left(1 + \\frac{r_i}{\\nu s}\\right), \\quad \\text{where } \\alpha = \\frac{\\nu + 1}{2}, \\ g = Gu, \\ r_i = \\sqrt{g_i^2 + \\varepsilon^2}\n$$\nWe use the chain rule to find the gradient $\\nabla_u U_{\\text{prior}}(u)$. Let's find the gradient with respect to the gradients $g = Gu$ first.\n$$\n\\frac{\\partial U_{\\text{prior}}}{\\partial g_j} = \\sum_{i=1}^{n-1} \\frac{\\partial U_{\\text{prior}}}{\\partial r_i} \\frac{\\partial r_i}{\\partial g_j}\n$$\nSince $r_i$ only depends on $g_i$, $\\frac{\\partial r_i}{\\partial g_j} = \\delta_{ij} \\frac{\\partial r_i}{\\partial g_i}$.\n$$\n\\frac{\\partial U_{\\text{prior}}}{\\partial r_i} = \\frac{\\alpha}{1 + r_i/(\\nu s)} \\cdot \\frac{1}{\\nu s} = \\frac{\\alpha}{\\nu s + r_i}\n$$\n$$\n\\frac{\\partial r_i}{\\partial g_i} = \\frac{1}{2\\sqrt{g_i^2 + \\varepsilon^2}} (2 g_i) = \\frac{g_i}{r_i}\n$$\nCombining these, the gradient with respect to $g_i$ is:\n$$\n\\frac{\\partial U_{\\text{prior}}}{\\partial g_i} = \\frac{\\alpha}{\\nu s + r_i} \\cdot \\frac{g_i}{r_i} = \\frac{\\alpha g_i}{r_i(\\nu s + r_i)}\n$$\nUsing the relationship $g = Gu$, the gradient with respect to $u$ is found by applying the transpose of the operator $G$:\n$$\n\\nabla U_{\\text{prior}}(u) = G^\\top \\nabla_g U_{\\text{prior}}(Gu)\n$$\nLet $v(Gu)$ be a vector with components $v_i = \\frac{\\alpha g_i}{r_i(\\nu s + r_i)}$. Then the prior gradient is:\n$$\n\\nabla U_{\\text{prior}}(u) = G^\\top v(Gu)\n$$\nTo find the Hessian of the prior term, we differentiate the gradient $\\nabla U_{\\text{prior}}(u)$ with respect to $u$:\n$$\nH_{\\text{prior}}(u) = \\nabla^2 U_{\\text{prior}}(u) = \\frac{\\partial}{\\partial u} \\left( G^\\top v(Gu) \\right)\n$$\nApplying the chain rule again, the Jacobian of $v(Gu)$ with respect to $u$ is $(\\nabla_g v(Gu)) G$. Thus:\n$$\nH_{\\text{prior}}(u) = G^\\top (\\nabla_g v(Gu)) G\n$$\nThe term $\\nabla_g v(Gu)$ is a matrix. Its $(i,j)$ component is $\\frac{\\partial v_i}{\\partial g_j}$. Since $v_i$ depends only on $g_i$, this matrix is diagonal. Let's call it $D_H$. The diagonal entries are $v_i'(g_i) = \\frac{d v_i}{d g_i}$.\n$$\nv_i'(g_i) = \\frac{d}{d g_i} \\left( \\frac{\\alpha g_i}{r_i(\\nu s + r_i)} \\right)\n$$\nUsing the quotient rule and $\\frac{d r_i}{d g_i} = \\frac{g_i}{r_i}$:\n$$\nv_i'(g_i) = \\alpha \\frac{1 \\cdot (r_i(\\nu s + r_i)) - g_i \\cdot \\left(\\frac{d r_i}{d g_i}(\\nu s + r_i) + r_i \\frac{d r_i}{d g_i}\\right)}{(r_i(\\nu s + r_i))^2}\n$$\n$$\nv_i'(g_i) = \\alpha \\frac{r_i(\\nu s + r_i) - g_i \\frac{g_i}{r_i}(\\nu s + 2r_i)}{r_i^2(\\nu s + r_i)^2} = \\alpha \\frac{r_i^2(\\nu s + r_i) - g_i^2(\\nu s + 2r_i)}{r_i^3(\\nu s + r_i)^2}\n$$\nSubstituting $r_i^2 = g_i^2 + \\varepsilon^2$:\n$$\nv_i'(g_i) = \\alpha \\frac{(g_i^2 + \\varepsilon^2)(\\nu s + r_i) - g_i^2(\\nu s + 2r_i)}{r_i^3(\\nu s + r_i)^2} = \\alpha \\frac{g_i^2\\nu s + g_i^2 r_i + \\varepsilon^2(\\nu s + r_i) - g_i^2\\nu s - 2g_i^2 r_i}{r_i^3(\\nu s + r_i)^2}\n$$\n$$\nv_i'(g_i) = \\frac{\\alpha (\\varepsilon^2(\\nu s + r_i) - g_i^2 r_i)}{r_i^3(\\nu s + r_i)^2}\n$$\nThe Hessian of the prior is $H_{\\text{prior}}(u) = G^\\top D_H G$, where $D_H$ is the diagonal matrix with entries $v_i'(g_i)$.\n\n#### Total Gradient and Hessian\nCombining the terms, we get the final expressions:\n$$\n\\nabla U(u) = \\frac{1}{\\sigma_\\ell^2} A^\\top(A u - y) + G^\\top v(Gu), \\quad \\text{with } v_i = \\frac{\\alpha g_i}{r_i(\\nu s + r_i)}\n$$\n$$\nH(u) = \\nabla^2 U(u) = \\frac{1}{\\sigma_\\ell^2} A^\\top A + G^\\top D_H G, \\quad \\text{with } (D_H)_{ii} = \\frac{\\alpha (\\varepsilon^2(\\nu s + r_i) - g_i^2 r_i)}{r_i^3(\\nu s + r_i)^2}\n$$\n\n### 2. Justification of the Local Leapfrog Stability Bound\n\nThe Hamiltonian system is described by $\\dot{u} = p$ and $\\dot{p} = -\\nabla U(u)$. For a general potential $U(u)$, the equations of motion are nonlinear. To analyze the stability of a numerical integrator like leapfrog, we linearize the system around a state $(u, p)$. The evolution of a small perturbation $\\delta u$ is given by the second-order ODE:\n$$\n\\delta \\ddot{u} = -\\nabla^2 U(u) \\delta u = -H(u) \\delta u\n$$\nwhere $H(u) = \\nabla^2 U(u)$ is the Hessian of the potential at $u$. This equation describes a system of coupled harmonic oscillators. The stability of the leapfrog integrator for this linear system depends on the eigenvalues of the matrix $H(u)$.\n\nIf $H(u)$ is symmetric positive definite with eigenvalues $\\lambda_k = \\omega_k^2  0$, the solutions are oscillatory. The leapfrog method, applied to the single oscillator equation $\\ddot{x} = -\\omega^2 x$, is stable if and only if the step size $h$ satisfies $h\\omega \\leq 2$. For the system, this condition must hold for all eigenfrequencies $\\omega_k$. The most restrictive constraint comes from the largest frequency, corresponding to the largest eigenvalue $\\lambda_{\\max}$ of $H(u)$. Thus, stability requires:\n$$\nh \\sqrt{\\lambda_{\\max}} \\leq 2 \\implies h \\leq \\frac{2}{\\sqrt{\\lambda_{\\max}}}\n$$\nIf $H(u)$ has a negative eigenvalue $\\lambda = -k^2  0$, the corresponding mode evolves according to $\\delta \\ddot{u} = k^2 \\delta u$. This equation has exponentially growing solutions ($\\exp(kt)$), indicating that the fixed point is unstable (a saddle point). Any symplectic integrator, including leapfrog, will diverge along this direction for any step size $h  0$.\n\nTherefore, for the leapfrog method to be locally stable in an oscillatory sense, the Hessian $H(u)$ must be positive semidefinite. The maximum allowable step size is determined by the largest positive eigenvalue, $\\lambda_{\\max}^+(u) = \\max(\\{0\\} \\cup \\{\\lambda_i \\mid \\lambda_i \\text{ is an eigenvalue of } H(u)\\})$. If $\\lambda_{\\max}^+(u)  0$, the stability bound is $h_{\\max}(u) = 2/\\sqrt{\\lambda_{\\max}^+(u)}$. If all eigenvalues are non-positive, any step size is stable for the oscillatory modes (of which there are none if all eigenvalues are negative), so $h_{\\max}(u) = +\\infty$. The presence of any negative eigenvalues signifies local non-linear instability regardless of the step size.\n\n### 3. Algorithm Design\n\nThe algorithm computes the required diagnostic quantities for a given state $u$ and model parameters.\n\n1.  **Inputs**: State vector $u \\in \\mathbb{R}^n$, parameters $(\\nu, s, \\varepsilon, \\sigma_\\ell)$, and constant problem data $(A, y, G)$. Let $\\alpha = (\\nu+1)/2$.\n2.  **Compute Potential, Gradient, and Hessian**:\n    a.  Compute intermediate values: $g = Gu$ and $r_i = \\sqrt{g_i^2 + \\varepsilon^2}$ for $i=1,\\dots,n-1$.\n    b.  **Potential $U(u)$**:\n        i.  $U_{\\text{like}} = \\frac{1}{2\\sigma_\\ell^2} \\|Au - y\\|_2^2$.\n        ii. $U_{\\text{prior}} = \\alpha \\sum_i \\log(1 + r_i / (\\nu s))$.\n        iii. $U(u) = U_{\\text{like}} + U_{\\text{prior}}$.\n    c.  **Gradient $\\nabla U(u)$**:\n        i.  $\\nabla U_{\\text{like}} = \\frac{1}{\\sigma_\\ell^2} A^\\top(Au - y)$.\n        ii. Compute vector $v$ with components $v_i = \\frac{\\alpha g_i}{r_i(\\nu s + r_i)}$.\n        iii. $\\nabla U_{\\text{prior}} = G^\\top v$.\n        iv. $\\nabla U(u) = \\nabla U_{\\text{like}} + \\nabla U_{\\text{prior}}$.\n    d.  **Hessian $H(u)$**:\n        i.  $H_{\\text{like}} = \\frac{1}{\\sigma_\\ell^2} A^\\top A$.\n        ii. Compute diagonal matrix $D_H$ with entries $(D_H)_{ii} = \\frac{\\alpha (\\varepsilon^2(\\nu s + r_i) - g_i^2 r_i)}{r_i^3(\\nu s + r_i)^2}$.\n        iii. $H_{\\text{prior}} = G^\\top D_H G$.\n        iv. $H(u) = H_{\\text{like}} + H_{\\text{prior}}$.\n3.  **Stability Analysis**:\n    a.  Compute the eigenvalues of the symmetric matrix $H(u)$, e.g., using `eigvalsh`.\n    b.  Find the minimum eigenvalue $\\lambda_{\\min}$. The Hessian is positive semidefinite if $\\lambda_{\\min} \\ge 0$ (within a small numerical tolerance). Store this boolean result.\n    c.  Find the largest non-negative eigenvalue $\\lambda_{\\max}^+(u)$. If all eigenvalues are negative, $\\lambda_{\\max}^+(u) = 0$.\n    d.  Compute the stability bound $h_{\\max}(u)$. If $\\lambda_{\\max}^+(u)  0$, $h_{\\max}(u) = 2/\\sqrt{\\lambda_{\\max}^+(u)}$. Otherwise, $h_{\\max}(u) = \\infty$.\n4.  **Leapfrog Energy Error**:\n    a.  Initialize $(u^{(0)}, p^{(0)}) = (u, 0)$. The initial energy is $H(u^{(0)}, p^{(0)}) = U(u) + 0 = U(u)$.\n    b.  Set the step size $h = 0.9 \\cdot h_{\\max}(u)$ if $h_{\\max}(u)$ is finite, otherwise $h = 1$.\n    c.  Perform one leapfrog step:\n        i.   $p^{(1/2)} = p^{(0)} - \\frac{h}{2} \\nabla U(u^{(0)})$. The gradient $\\nabla U(u^{(0)})$ was computed in step 2c.\n        ii.  $u^{(1)} = u^{(0)} + h p^{(1/2)}$.\n        iii. Compute the potential $U(u^{(1)})$ and gradient $\\nabla U(u^{(1)})$ at the new position $u^{(1)}$ by re-applying steps 2a, 2b, and 2c with $u=u^{(1)}$.\n        iv.  $p^{(1)} = p^{(1/2)} - \\frac{h}{2} \\nabla U(u^{(1)})$.\n    d.  Compute the final energy $H(u^{(1)}, p^{(1)}) = U(u^{(1)}) + \\frac{1}{2}\\|p^{(1)}\\|_2^2$.\n    e.  The energy error is $\\Delta H = H(u^{(1)}, p^{(1)}) - H(u^{(0)}, p^{(0)})$.\n5.  **Outputs**: Return $h_{\\max}(u)$, the positive semidefiniteness boolean, and the energy error $\\Delta H$.", "answer": "```python\nimport numpy as np\nfrom typing import Dict, Any, Tuple, List\n\ndef setup_problem(n: int) - Dict[str, np.ndarray]:\n    \"\"\"\n    Sets up the constant matrices and vectors for the problem.\n    \"\"\"\n    # Blurring operator A\n    A = np.diag(0.6 * np.ones(n)) + \\\n        np.diag(0.2 * np.ones(n - 1), 1) + \\\n        np.diag(0.2 * np.ones(n - 1), -1)\n\n    # Forward difference operator G\n    G = np.diag(-1 * np.ones(n)) + np.diag(np.ones(n - 1), 1)\n    G = G[:n - 1, :]\n\n    # Ground truth and data\n    u_true = np.zeros(n)\n    u_true[16:] = 1.0\n    y = A @ u_true\n\n    # Evaluation points\n    u_0 = np.zeros(n)\n    u_tail = 2.0 * u_true\n    \n    return {'A': A, 'G': G, 'y': y, 'u_0': u_0, 'u_tail': u_tail, 'n': n}\n\ndef compute_potential_and_derivatives(\n    u: np.ndarray, \n    params: Tuple[float, float, float, float],\n    consts: Dict[str, Any]\n) - Tuple[float, np.ndarray, np.ndarray]:\n    \"\"\"\n    Computes the potential U(u), gradient gradU(u), and Hessian H(u).\n    \"\"\"\n    nu, s, eps, sigma_l = params\n    A, G, y, n = consts['A'], consts['G'], consts['y'], consts['n']\n    alpha = (nu + 1.0) / 2.0\n\n    # Intermediate quantities for the prior\n    g = G @ u\n    r_sq = g**2 + eps**2\n    r = np.sqrt(r_sq)\n\n    # --- Potential U(u) ---\n    res = A @ u - y\n    U_like = 0.5 / (sigma_l**2) * np.dot(res, res)\n    U_prior = alpha * np.sum(np.log(1.0 + r / (nu * s)))\n    U = U_like + U_prior\n\n    # --- Gradient gradU(u) ---\n    grad_U_like = 1.0 / (sigma_l**2) * A.T @ res\n    v_vec = alpha * g / (r * (nu * s + r))\n    grad_U_prior = G.T @ v_vec\n    grad_U = grad_U_like + grad_U_prior\n\n    # --- Hessian H(u) ---\n    H_like = 1.0 / (sigma_l**2) * A.T @ A\n    \n    # Diagonal of D_H matrix for prior Hessian\n    num = alpha * (eps**2 * (nu * s + r) - g**2 * r)\n    den = r**3 * (nu * s + r)**2\n    # Add a small constant to denominator to avoid division by zero if r is extremely small\n    # (though eps  0 should prevent this)\n    den[den == 0] = 1e-30\n    d_H_diag = num / den\n    \n    D_H = np.diag(d_H_diag)\n    H_prior = G.T @ D_H @ G\n    H = H_like + H_prior\n\n    return U, grad_U, H\n\ndef compute_hmc_diagnostics(\n    u: np.ndarray,\n    params: Tuple[float, float, float, float],\n    consts: Dict[str, Any]\n) - Dict[str, Any]:\n    \"\"\"\n    Computes all required diagnostic quantities for a given state u.\n    \"\"\"\n    n = consts['n']\n    \n    # 1. Compute U, gradU, H\n    U0, gradU0, H0 = compute_potential_and_derivatives(u, params, consts)\n\n    # 2. Stability Analysis\n    eigvals = np.linalg.eigvalsh(H0)\n    lambda_min = np.min(eigvals)\n    is_psd = lambda_min = -1e-9 # Tolerance for floating point errors\n\n    non_neg_eigvals = eigvals[eigvals = 0]\n    lambda_max_plus = np.max(non_neg_eigvals) if non_neg_eigvals.size  0 else 0.0\n\n    if lambda_max_plus  1e-12: # Check against small tolerance\n        h_max = 2.0 / np.sqrt(lambda_max_plus)\n    else:\n        h_max = np.inf\n\n    # 3. Leapfrog Energy Error\n    if np.isinf(h_max):\n        h = 1.0\n    else:\n        h = 0.9 * h_max\n    \n    p0 = np.zeros(n)\n    H_initial = U0 # K(p0) is 0\n\n    # One leapfrog step\n    p_half = p0 - (h / 2.0) * gradU0\n    u1 = u + h * p_half\n    \n    U1, gradU1, _ = compute_potential_and_derivatives(u1, params, consts)\n    \n    p1 = p_half - (h / 2.0) * gradU1\n\n    K1 = 0.5 * np.dot(p1, p1)\n    H_final = U1 + K1\n    \n    delta_H = H_final - H_initial\n\n    return {\n        'h_max': h_max,\n        'is_psd': is_psd,\n        'delta_H': delta_H\n    }\n\ndef solve():\n    \"\"\"\n    Main solver function to run test cases and print results.\n    \"\"\"\n    n = 32\n    consts = setup_problem(n)\n    \n    u_0 = consts['u_0']\n    u_tail = consts['u_tail']\n\n    test_cases = [\n        # (nu, s, epsilon, sigma_ell)\n        (3.0, 0.2, 1e-2, 0.05),\n        (1.5, 0.1, 1e-6, 0.2),\n        (1.0, 0.05, 1e-8, 20.0),\n    ]\n\n    results: List[Any] = []\n    \n    for case_params in test_cases:\n        # Run for u_0\n        diag_u0 = compute_hmc_diagnostics(u_0, case_params, consts)\n        \n        # Run for u_tail\n        diag_utail = compute_hmc_diagnostics(u_tail, case_params, consts)\n\n        results.extend([\n            diag_u0['h_max'],\n            diag_utail['h_max'],\n            diag_u0['is_psd'],\n            diag_utail['is_psd'],\n            diag_u0['delta_H'],\n            diag_utail['delta_H'],\n        ])\n\n    # Format output as specified\n    formatted_results = []\n    for res in results:\n        if isinstance(res, bool):\n            formatted_results.append(str(res).lower())\n        else:\n            formatted_results.append(f\"{res:.8e}\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3388099"}]}